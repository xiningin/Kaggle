{"cell_type":{"dfa8462a":"code","f83b1966":"code","3fb56f45":"code","dc35ab7a":"code","350b4199":"code","af7ebfa3":"code","622a89ec":"code","0997f2e8":"code","d14b5b98":"code","019b6de1":"code","54cea430":"code","c903e12d":"code","26250157":"code","d5c6ce98":"code","55a0e73d":"code","62310750":"code","8b5c28c5":"code","5fbc735b":"code","a8471b9d":"code","bd64685b":"code","b7221c56":"code","fb774d19":"code","909cfb31":"code","335fde56":"code","b21a3a24":"code","12b176bf":"code","eeba0fbe":"code","c538f148":"code","add6c568":"code","2df566d3":"code","54dcaa56":"code","900e7adf":"code","fa72628b":"code","01799d7b":"code","23f7697c":"code","6bd50934":"code","f48218d7":"code","7d440c25":"code","d013aea0":"code","baaed247":"code","a40e7271":"code","cbc205f7":"code","75b01d0e":"code","cadcf301":"code","64135336":"code","e0e3e4d7":"code","d05d5f5b":"code","535209b1":"code","a031f0a6":"code","687a0acc":"code","24a2af7e":"code","263750b0":"code","25a7de26":"code","e3e7ffc3":"code","5747b14e":"code","79f1039a":"code","b94e04c6":"code","0cb61a62":"code","c959c80c":"code","2dc2f923":"code","affda656":"code","7938de8d":"code","73304b53":"code","3b3b0e36":"code","0227dcb2":"code","4d4f994a":"code","35c74f2d":"code","48a322ef":"code","7542e4b9":"code","b50a4720":"code","0b4cf617":"markdown","26b1e24b":"markdown","6aa703a3":"markdown","50bb42ff":"markdown","231e4f73":"markdown","a624f363":"markdown","808c12db":"markdown","febf7f36":"markdown","52117cbd":"markdown","b7c529a8":"markdown","1e4545fe":"markdown","7ae94df0":"markdown","ad73dec4":"markdown","12734c63":"markdown"},"source":{"dfa8462a":"import numpy as np \nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f83b1966":"# Load data\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ntrain['train_test'] = 1\ntest['train_test'] = 0\n# Concatinate the data. This makes it more convenient to do EDA and feature engineering on the whole data set\nall_data = pd.concat([train,test], ignore_index = True)","3fb56f45":"all_data.info() ","dc35ab7a":"all_data.describe() ","350b4199":"# Figure 1: Distribution of continuous features of all_data\nplt.figure(figsize=[14,7])\nnum_columns = ['Fare','Age','Parch','SibSp']\n\nfor i in range(len(num_columns)):\n    cur_subplot = 221 + i\n    cur_feature = num_columns[i]\n    plt.subplot(cur_subplot)\n    plt.hist(x = all_data[cur_feature])\n#     plt.title(cur_feature + ' Histogram by Survival')\n    plt.xlabel(cur_feature)\n    plt.ylabel('# of Passengers')\n    plt.legend()","af7ebfa3":"# Figure 2: Distribution of categorical features of all_data\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\ncat_columns = ['Sex','Pclass','Embarked']\nfor i in range(len(cat_columns)):\n    curr_feature = cat_columns[i]\n    all_data.groupby(curr_feature)['PassengerId'].count().plot(kind = 'bar', stacked = False, ax = axes[i])","622a89ec":"# Figure 3: Distribution \/ relation of continuous features with Survived\nnum_columns = ['Fare','Age','Parch','SibSp']\nplt.figure(figsize=[14,7])\nfor i in range(len(num_columns)):\n    cur_subplot = 221 + i\n    cur_feature = num_columns[i]\n    plt.subplot(cur_subplot)\n    plt.hist(x = [train[train['Survived']==0][cur_feature], train[train['Survived']==1][cur_feature]], \n             stacked=False,label = ['Dead','Survived'])\n    plt.xlabel(cur_feature)\n    plt.ylabel('# of Passengers')\n    plt.legend()","0997f2e8":"# Figure 4: Distribution \/ relation of categorical features with Survived\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\ncat_columns = ['Sex','Pclass','Embarked']\nfor i in range(len(cat_columns)):\n    curr_feature = cat_columns[i]\n    train.groupby([curr_feature, 'Survived'])['PassengerId'].count().unstack().plot(kind = 'bar', stacked = False, ax = axes[i])\n    axes[i].legend(['Dead','Survived'])","d14b5b98":"# Figure 5: Ditribution comparison of continuous features between train and test\nplt.figure(figsize=[14,7])\nnum_columns = ['Fare','Age','Parch','SibSp']\nfor i in range(len(num_columns)):\n    cur_subplot = 221 + i\n    cur_feature = num_columns[i]\n    plt.subplot(cur_subplot)\n    plt.hist(x = [test[cur_feature], train[cur_feature]], stacked=False,label = ['Test','Train'], color = ['skyblue','orange'])\n    plt.xlabel(cur_feature)\n    plt.ylabel('# of Passengers')\n    plt.legend()","019b6de1":"# Figure 6: Ditribution comparison of categorical features between train and test\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\ncat_columns = ['Sex','Pclass','Embarked']\nfor i in range(len(cat_columns)):\n    curr_feature = cat_columns[i]\n    all_data.groupby([curr_feature, 'train_test'])['PassengerId'].count().unstack().plot(\n    kind = 'bar', stacked = False, ax = axes[i], color = ['skyblue','orange'])\n    axes[i].legend(['Test','Train'])","54cea430":"# Figure 7: Deeper relation between Embarked, Pclass and Survived\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\nEmbarked_features = ['C','Q','S']\n\nfor i in range(len(Embarked_features)):\n    curr_feature = cat_columns[i]\n    temp = train[train.Embarked == Embarked_features[i]]\n    temp.groupby(['Pclass', 'Survived'])['PassengerId'].count().unstack().plot(\n    kind = 'bar', stacked = False, ax = axes[i], title = ('Embarked_' + Embarked_features[i]))\n\n    axes[i].legend(['Dead','Survived'])","c903e12d":"# Figure 8: Deeper relation between Age, Sex and Pclass\nplt.figure(figsize=[14,7])\nfor i in range(3):\n    cur_subplot = 131 + i\n    train_temp_pclass_sex = train[(train['Pclass'] == (i+1)) & (train['Sex'] == 'female')]\n    plt.subplot(cur_subplot)\n    plt.hist(x = [train_temp_pclass_sex[train_temp_pclass_sex['Survived']==0]['Age'], train_temp_pclass_sex[train_temp_pclass_sex['Survived']==1]['Age']], \n             stacked=False,label = ['Dead','Survived'], bins = np.arange(0,80,5))\n    plt.xlabel('Pclass' + str(i+1) + '_female')\n    plt.ylabel('# of Passengers')\n    plt.legend()","26250157":"# Figure 9: Deeper relation between Age, Sex and Pclass\nplt.figure(figsize=[14,7])\nfor i in range(3):\n    cur_subplot = 131 + i\n    cur_feature = num_columns[i]\n    train_temp_pclass_sex = train[(train['Pclass'] == (i+1)) & (train['Sex'] == 'male')]\n    plt.subplot(cur_subplot)\n    plt.hist(x = [train_temp_pclass_sex[train_temp_pclass_sex['Survived']==0]['Age'], train_temp_pclass_sex[train_temp_pclass_sex['Survived']==1]['Age']], \n             stacked=False,label = ['Dead','Survived'], bins = np.arange(0,90,7.5))\n    plt.xlabel('Pclass' + str(i+1) + '_male')\n    plt.ylabel('# of Passengers')\n    plt.legend()","d5c6ce98":"all_data['Age_isMissing'] = 0\nall_data.loc[all_data.Age.isnull(), 'Age_isMissing'] = 1","55a0e73d":"# Figure 1: Age_isMissing against continuous features, to check if any obvious distribution difference between people with\/without age.\nnum_columns = ['Fare','Parch','SibSp']\nplt.figure(figsize=[14,7])\ntemp = all_data.copy()\nfor i in range(len(num_columns)):\n    cur_subplot = 131 + i\n    cur_feature = num_columns[i]\n    plt.subplot(cur_subplot)\n    plt.hist(x = [temp[temp['Age_isMissing']==0][cur_feature], temp[temp['Age_isMissing']==1][cur_feature]], \n             stacked=False,label = ['Age','NoAge'])\n#     plt.title(cur_feature + ' Histogram by Survival')\n    plt.xlabel(cur_feature)\n    plt.ylabel('# of Passengers')\n    plt.legend()","62310750":"# Figure 2: Age_isMissing against categorical features, to check if any obvious distribution difference between people with\/without age.\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\ncat_columns = ['Sex','Pclass','Embarked']\ntemp = all_data.copy()\nfor i in range(len(cat_columns)):\n    curr_feature = cat_columns[i]\n    temp.groupby([curr_feature, 'Age_isMissing'])['PassengerId'].count().unstack().plot(\n    kind = 'bar', stacked = False, ax = axes[i])\n\n    axes[i].legend(['Age','NoAge'])","8b5c28c5":"# Figure 3: Check statistical difference between people with\/without age.\nall_data.loc[all_data.Age_isMissing == 0, ['Survived', 'Pclass', 'SibSp', 'Parch', 'Fare', 'train_test']].describe()","5fbc735b":"all_data.loc[all_data.Age_isMissing == 1, ['Survived', 'Pclass', 'SibSp', 'Parch', 'Fare', 'train_test']].describe()","a8471b9d":"# Get the Title from the name\nall_data['Name_title'] = all_data.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())","bd64685b":"print(\"Max age of title Master:\" + str(all_data[all_data['Name_title'] == 'Master'].Age.max()))","b7221c56":"print('Titles of children:')\nall_data[all_data['Age'] < 15].Name_title.value_counts()","fb774d19":"# I am curious about the exceptions.\n# By checking Last name and Ticket, passengerId 684 and 687 traveled with their family. They were probably mistaken as Mr instead of Master\n# PassengerId7 732 traveled with a 26 years old male and the last name is different. Maybe from one family, and the woman is nanny.\nall_data[(all_data['Age'] < 15) & (all_data['Name_title'] == 'Mr')]","909cfb31":"# By checking Ticket and last name, PassengerId 10 is a 14 years old girl, and she is married with a 32.5 years old guy. \n# Not sure if it was legal at the time.\nall_data[all_data['Ticket'] == '237736']","335fde56":"# Check the Parch distribution, when Age < 15\nall_data[all_data['Age'] < 15].Parch.value_counts()","b21a3a24":"# Make a categorical label for child. It would be convenient later to fill the missing value on Age.\nall_data['isChild'] = 0\nall_data.loc[all_data['Age'] < 15, 'isChild'] = 1","12b176bf":"all_data.loc[(all_data.Age_isMissing == 1) & ((all_data.Name_title == 'Miss') | \n                                            (all_data.Name_title == 'Master')) & (all_data.Parch > 0),'isChild'] = 1","eeba0fbe":"# Print out all the candidates\nall_data.loc[(all_data.Age_isMissing == 1) & ((all_data.Name_title == 'Miss') | \n                                            (all_data.Name_title == 'Master')) & (all_data.Parch > 0)]","c538f148":"all_data[all_data.Ticket == '2661']","add6c568":"all_data[all_data.Ticket == 'CA. 2343']","2df566d3":"all_data['Age_isMissing_mean_temp'] = all_data.groupby(['Pclass', 'Sex', 'isChild']).Age.transform('median')\nall_data.loc[all_data.Age.isnull(), 'Age'] = all_data.loc[all_data.Age.isnull(), 'Age_isMissing_mean_temp']\nall_data.drop('Age_isMissing_mean_temp', axis = 1, inplace = True)","54dcaa56":"all_data['Cabin_initial'] = all_data.Cabin.apply(lambda x: str(x)[0])","900e7adf":"# Figure 1: print the total number of passengers regarding Pclass and Cabin deck\nall_data.groupby(['Cabin_initial', 'Pclass']).PassengerId.count()","fa72628b":"# Figure 2: left: number of passenger of each Pclass distribution in each Cabin\n#           right: percentage of the Pclass distribution in each \nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n_ = all_data.groupby(['Cabin_initial', 'Pclass']).PassengerId.count().unstack(fill_value=0).plot.bar(ax = axes[0])\n\n_ = (all_data.groupby(['Cabin_initial', 'Pclass']).PassengerId.count()\/ \n     all_data.groupby(['Cabin_initial']).PassengerId.count()).unstack(fill_value=0).plot.bar(ax = axes[1])","01799d7b":"# Figure 3: left: number of survived\/not survived passengers in each Cabin\n# right: percentage of survived\/not survived in each Cabin\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\ntrain_temp = all_data[all_data.train_test == 1] \n_ = train_temp.groupby(['Cabin_initial', 'Survived']).PassengerId.count().unstack(fill_value=0).plot.bar(ax = axes[0])\n\n_ = (train_temp.groupby(['Cabin_initial', 'Survived']).PassengerId.count()\/ \n     train_temp.groupby(['Cabin_initial']).PassengerId.count()).unstack(fill_value=0).plot.bar(ax = axes[1])","23f7697c":"all_data['Sex_label'] = 0\nall_data.loc[all_data.Sex == 'male', 'Sex_label'] = 1","6bd50934":"# # Figure 4: The higher the Sex_label.mean, the more male than female\nall_data.groupby('Cabin_initial').Sex_label.mean()","f48218d7":"# # Figure 5: print the average age of each deck\nall_data.groupby('Cabin_initial').Age.mean()","7d440c25":"def group_deck(deck):\n    if(deck in ['A','B','C','T']):\n        return \"ABC\"\n    elif(deck in ['D','E']):\n        return \"DE\"\n    elif (deck in ['F', 'G']):\n        return \"FG\"\n    else:\n        return \"N\"\n\nall_data['Cabin_deck'] = all_data['Cabin_initial'].apply(group_deck)","d013aea0":"# Figure 1: plot the distribution of Fare in each Pclass\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\nfor i in range(3):\n    ax = all_data[all_data.Pclass == (i+1)].Fare.plot.hist(ax = axes[i], title='Plcass = ' + str(i+1))\n    ax.set_xlabel(\"Fare\")","baaed247":"# Figure 2: plot the distribution of Fare in each Pclass with more specific bins\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\n_ = all_data[all_data.Pclass == 1].Fare.plot.hist(ax = axes[0], bins = np.arange(0,30,1)).set_xlabel(\"Fare\")\n_ = all_data[all_data.Pclass == 2].Fare.plot.hist(ax = axes[1], bins = np.arange(0,20,1)).set_xlabel(\"Fare\")\n_ = all_data[all_data.Pclass == 3].Fare.plot.hist(ax = axes[2], bins = np.arange(0,10,1)).set_xlabel(\"Fare\")\n","a40e7271":"ticket_count = all_data.groupby('Ticket').PassengerId.count()\nall_data['Ticket_count'] = all_data['Ticket'].map(ticket_count)\nall_data['Fare_individual'] = all_data['Fare']\/all_data['Ticket_count']","cbc205f7":"# Figure 3: plot the distribution of Fare_individual in each Pclass with more specific bins\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\nfor i in range(3):\n    ax = all_data[all_data.Pclass == (i+1)].Fare_individual.plot.hist(ax = axes[i], title='Plcass = ' + str(i+1))\n    ax.set_xlabel(\"Fare\")","75b01d0e":"all_data.loc[all_data.Fare_individual.isnull(), 'Fare_individual'] = all_data[all_data['Pclass'] == 3].Fare_individual.median()\nall_data.loc[all_data.Fare_individual.isnull(), 'Fare'] = all_data[all_data['Pclass'] == 3].Fare_individual.median()","cadcf301":"all_data['Pclass_startingPrice'] = 0\nall_data.loc[all_data.Pclass==1, 'Pclass_startingPrice'] = 25\nall_data.loc[all_data.Pclass==2, 'Pclass_startingPrice'] = 10\nall_data.loc[all_data.Pclass==3, 'Pclass_startingPrice'] = 7\nall_data['Fare_differenceFromStart'] = all_data.Fare_individual - all_data.Pclass_startingPrice","64135336":"# Figure 4: plot the Fare_differenceFromStart in each Pclass\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\nfor i in range(3):\n    _ = all_data[all_data.Pclass == (i+1)].Fare_differenceFromStart.plot.hist(ax = axes[i], title='Plcass = ' + str(i+1))\n    ax.set_xlabel(\"Fare\")","e0e3e4d7":"# Figure 5: plot Fare_differenceFromStart against Pclass and Survived\nnum_columns = ['Fare','Parch','SibSp']\nplt.figure(figsize=[14,7])\nfor i in range(len(num_columns)):\n    cur_subplot = 131 + i\n    cur_feature = num_columns[i]\n    plt.subplot(cur_subplot)\n    temp = all_data[all_data.Pclass == (i+1)]\n    plt.hist(x = [temp[temp['Survived']==0]['Fare_differenceFromStart'], temp[temp['Survived']==1]['Fare_differenceFromStart']], \n             stacked=False, label = ['Dead','Survived'])\n    plt.title('Fare_differenceFromStart')\n    plt.xlabel('Pclass=' + str(i+1))\n    plt.ylabel('# of Passengers')\n    plt.legend()","d05d5f5b":"# The survival rate of people who paid near zero is extremly low, no matter the Pclass\nall_data[(all_data.Fare_individual < 2) & (all_data.train_test == 1)].Survived.mean()","535209b1":"# The most frequent Embarked value for 1st class female is 'C'. Uncomment to check.\n# all_data.groupby(['Pclass', 'Sex', 'Embarked']).size()\nall_data.loc[all_data.Embarked.isnull(), 'Embarked'] = 'C'","a031f0a6":"all_data['FamilySize'] = all_data.Parch + all_data.SibSp + 1","687a0acc":"all_data['GroupSize'] = all_data[['Ticket_count', 'FamilySize']].max(axis = 1)","24a2af7e":"from sklearn.preprocessing import StandardScaler\n# Set the feature set\nfeatures_to_use = ['Pclass', 'Sex_label', 'Age', 'Embarked', 'train_test', 'Cabin_deck', \n                   'Fare_individual', 'Fare_differenceFromStart', 'GroupSize']\nall_features_train = all_data[features_to_use].copy()\n# Set the Pclass to str, so we can get one hot encoding of it\nall_features_train.Pclass = all_features_train.Pclass.astype(str)\n# one hot encoding\nall_features_train = pd.get_dummies(all_features_train, columns = ['Pclass', 'Embarked','Cabin_deck'])\n# standard scale the numerical features, as logitstic regresstion is sensitive to it\nscale = StandardScaler()\nto_be_scaled = ['Age', 'Fare_individual', 'Fare_differenceFromStart','GroupSize']\nall_features_train[to_be_scaled]= scale.fit_transform(all_features_train[to_be_scaled])","263750b0":"# get the training and test set\nx_train = all_features_train[all_features_train.train_test == 1].copy()\nx_train.drop('train_test', axis = 1, inplace = True)\nx_test = all_features_train[all_features_train.train_test == 0].copy()\nx_test.drop('train_test', axis = 1, inplace = True)\n\ny_train = all_data[all_data.train_test == 1].Survived","25a7de26":"#simple performance reporting function\ndef clf_performance(classifier, model_name):\n    print(model_name)\n    print('Best Score: ' + str(classifier.best_score_))\n    print('Best Parameters: ' + str(classifier.best_params_))","e3e7ffc3":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn import metrics\n# Gridsearch the parameters. Uncomment if needed.\n# lr = LogisticRegression()\n# param_grid = {'max_iter' : [2000],\n#               'penalty' : ['l1', 'l2', 'elasticnet'],\n#               'C' : np.logspace(-4, 4, 20),\n#               'solver' : ['liblinear', 'newton-cg', 'saga']}\n                                  \n# clf_lr = GridSearchCV(lr, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\n# best_clf_lr = clf_lr.fit(x_train,y_train)\n# clf_performance(best_clf_lr,'Logistic Regression')","5747b14e":"# Logistic regression score: 0.784\nlr = LogisticRegression(C = 0.1)\nlr.fit(x_train, y_train)\ny_pred_test = lr.predict(x_test)\n\nlr_submission = {'PassengerId': all_data[all_data.train_test == 0].PassengerId, 'Survived': y_pred_test.astype(int)}\nsubmission_lr = pd.DataFrame(data=lr_submission)\nsubmission_lr.to_csv('lr_submission_1.csv', index=False)","79f1039a":"from sklearn.ensemble import RandomForestClassifier\n# Gridsearch the parameters. Uncomment if needed.\n# rf = RandomForestClassifier(random_state = 1)\n# param_grid =  {'n_estimators': [100,300,500], \n#                                   'bootstrap': [True,False],\n#                                   'max_depth': [3,5,10,20,None],\n#                                   'max_features': ['auto','sqrt'],\n#                                   'min_samples_leaf': [1,2,4,10],\n#                                   'min_samples_split': [2,5,10]}\n                                  \n# clf_rf_rnd = GridSearchCV(rf, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\n# best_clf_rf_rnd = clf_rf_rnd.fit(x_train,y_train)\n# clf_performance(best_clf_rf_rnd,'Random Forest')","b94e04c6":"# Randome forest score: 0.784\nrf = RandomForestClassifier(random_state = 1, n_estimators = 100, bootstrap = False, max_depth = 5, max_features = 'auto',\n                            min_samples_leaf = 1, min_samples_split = 5)\nrf.fit(x_train, y_train)\ny_pred_test = rf.predict(x_test)\n\nrf_submission = {'PassengerId': all_data[all_data.train_test == 0].PassengerId, 'Survived': y_pred_test.astype(int)}\nsubmission_rf = pd.DataFrame(data=rf_submission)\nsubmission_rf.to_csv('rf_submission_1.csv', index=False)","0cb61a62":"# get the list of people who are in test set only\ntrain_temp = all_data[all_data.train_test == 1].copy()\ntest_temp = all_data[all_data.train_test == 0].copy()\n\nticket_train_list = train_temp.Ticket.value_counts().index.tolist()\nticket_test_list = test_temp.Ticket.value_counts().index.tolist()\nticket_only_test_list = list(set(ticket_test_list) - set(ticket_train_list))","c959c80c":"# if in the list, set Ticket_survivalRate_NA = 1\nall_data['Ticket_survivalRate_NA'] = 0\nall_data['Ticket_survivalRate_NA'] = all_data['Ticket'].apply(lambda x: 1 if x in ticket_only_test_list else 0)\nall_data.loc[all_data.Ticket_count<2, 'Ticket_survivalRate_NA'] = 1","2dc2f923":"# map the Ticket group survival rate.\nall_data['Ticket_survivalRate'] = all_data['Ticket'].map(all_data[all_data.Ticket_survivalRate_NA == 0].groupby('Ticket').Survived.mean())\n# fill NA by zero\nall_data['Ticket_survivalRate'] = all_data['Ticket_survivalRate'].fillna(0)","affda656":"# get first name as the indicator of family\nall_data['FirstName'] = all_data.Name.apply(lambda x: x.split(',')[0].strip())\n\ntrain_temp = all_data[all_data.train_test == 1].copy()\ntest_temp = all_data[all_data.train_test == 0].copy()\n\n# do the similar mapping as we did on Ticket\nFirstName_train_list = train_temp.FirstName.value_counts().index.tolist()\nFirstName_test_list = test_temp.FirstName.value_counts().index.tolist()\nFirstName_only_test_list = list(set(FirstName_test_list) - set(FirstName_train_list))\n\nall_data['Family_survivalRate_NA'] = 0\nall_data['Family_survivalRate_NA'] = all_data['FirstName'].apply(lambda x: 1 if x in FirstName_only_test_list else 0)\nall_data.loc[all_data.FamilySize<2, 'Family_survivalRate_NA'] = 1\n\nall_data['Family_survivalRate'] = all_data['FirstName'].map(all_data[all_data.Family_survivalRate_NA == 0].groupby('FirstName').Survived.mean())\nall_data['Family_survivalRate'] = all_data['Family_survivalRate'].fillna(0)","7938de8d":"# Average the survival rate of the family group and ticket group\nall_data['SurvivalRate'] = (all_data['Family_survivalRate'] + all_data['Ticket_survivalRate'])\/2","73304b53":"# Just need to simply add the new feature into the train and test set,\nx_train['SurvivalRate'] = all_data[all_data.train_test == 1]['SurvivalRate']\nx_test['SurvivalRate'] = all_data[all_data.train_test == 0]['SurvivalRate']","3b3b0e36":"# print out the correlation.\ntemp = all_data[all_data.train_test == 1][['Survived', 'SurvivalRate','Family_survivalRate','Ticket_survivalRate']]\ntemp.corr()","0227dcb2":"# Gridsearch the parameters. Uncomment if needed.\n# lr = LogisticRegression()\n# param_grid = {'max_iter' : [2000],\n#               'penalty' : ['l1', 'l2', 'elasticnet'],\n#               'C' : np.logspace(-4, 4, 20),\n#               'solver' : ['liblinear', 'newton-cg', 'saga']}\n                                  \n# clf_lr = GridSearchCV(lr, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\n# best_clf_lr = clf_lr.fit(x_train,y_train))\n# clf_performance(best_clf_lr,'Logistic Regression')","4d4f994a":"# Score: 0.7894736842105263\nlr = LogisticRegression(C = 0.033, max_iter = 2000, penalty = 'l2', solver = 'liblinear')\nlr.fit(x_train, y_train)\ny_pred_test_lr = lr.predict(x_test)\nlr_submission = {'PassengerId': all_data[all_data.train_test == 0].PassengerId, 'Survived': y_pred_test_lr.astype(int)}\nsubmission_lr = pd.DataFrame(data=lr_submission)\nsubmission_lr.to_csv('lr_submission_2.csv', index=False)","35c74f2d":"# Gridsearch the parameters. Uncomment if needed.\n# rf = RandomForestClassifier(random_state = 1)\n# param_grid =  {'n_estimators': [300,500,1000], \n#                                   'bootstrap': [True,False],\n#                                   'max_depth': [2,3,4,5,None],\n#                                   'max_features': ['auto','sqrt'],\n#                                   'min_samples_leaf': [1,2,4],\n#                                   'min_samples_split': [1,2,3,5]}\n                                  \n# clf_rf_rnd = GridSearchCV(rf, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\n# best_clf_rf_rnd = clf_rf_rnd.fit(x_train,y_train)\n# clf_performance(best_clf_rf_rnd,'Random Forest')","48a322ef":"# Score: 0.7918660287081339\nrf = RandomForestClassifier(random_state = 1, bootstrap = True, \n                            max_depth = 3, max_features = 'auto', min_samples_leaf = 1, min_samples_split = 2, n_estimators = 300)\nrf.fit(x_train, y_train)\ny_pred_test_rf = rf.predict(x_test)\nrf_submission = {'PassengerId': all_data[all_data.train_test == 0].PassengerId, 'Survived': y_pred_test_rf.astype(int)}\nsubmission_rf = pd.DataFrame(data=rf_submission)\nsubmission_rf.to_csv('rf_submission_2.csv', index=False)","7542e4b9":"from sklearn import svm\n# Gridsearch the parameters. Uncomment if needed.\n# param_grid = tuned_parameters = [{'kernel': ['rbf'], 'gamma': [.1,.5,1,2,5,10],\n#                                   'C': [.1, 1, 10, 100, 1000]},\n#                                  {'kernel': ['linear'], 'C': [.1, 1, 10, 100, 1000]},\n#                                  {'kernel': ['poly'], 'degree' : [2,3,4,5], 'C': [.1, 1, 10, 100, 1000]}]\n# clf_svc = GridSearchCV(svc, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\n# best_clf_svc = clf_svc.fit(x_train,y_train)\n# clf_performance(best_clf_svc,'SVC')","b50a4720":"svc = svm.SVC(random_state=1, C = 1, degree = 2, kernel = 'poly')\nsvc.fit(x_train, y_train)\ny_pred_test_svc = svc.predict(x_test)\nsvc_submission = {'PassengerId': all_data[all_data.train_test == 0].PassengerId, 'Survived': y_pred_test_svc.astype(int)}\nsubmission_svc = pd.DataFrame(data=svc_submission)\nsubmission_svc.to_csv('svc_submission_1.csv', index=False)","0b4cf617":"Fill the missing **Age** values by **Pclass**, **Sex** and **isChild**.","26b1e24b":"## **5. Fare: Feature Engineering and Completing**\n`Fare`: Basing on common sense, we can assume that **Fare** should be related to **Pclass**.\n* Figure 1,2: By plotting the **Fare** within different **Pclass** with bins, we see that the **starting** price for each **Pclass** is:\n     - Plcass 1: 25\n     - Pclass 2: 10\n     - Pclass 3: 7\n2. Why some 3rd class passenger paid higher fare than 2nd class passenger and even higher than 1st class\n    - During EDA, we saw that one ticket can have more than one person included, so we do **frenquency encoding** on **Ticket** to find the number of occurance of each Ticket (**Tiket_count**), and devide **Fare** by **Tiket_count**, get **Fare_individual**. \n    \n    Once plot the Fare_individual, we can see that the distribution is **centered** at the **starting price** of each Pclass (Figure 3). For the Fare_individual that is less than the starting price, we can assume that it is due to the family discount (family ticket is cheaper, or ticket of children is cheaper).\n    - Some people are willing to pay more to get better service within the same **Pclass**, we can create a new feature **Fare_differenceFromStart** to indicate if someone paid more or less for the ticket.\n        \n        **Fare_differenceFromStart = Fare_individual - StartingPrice of the Pclass**\n        \n3. Completing: since the one missing value of Fare has **Ticket_count** = 1, we can just complete it by fill the median of Fare_individual in the corresponding Pclass.\n\n3. lastly, in the last plot, we see that no matter the Pclass, people who paid much less than the starting price (paid zero or near zeor) mostly did not survive (survived 1 out of 15).\n    ","6aa703a3":"## **1. EDA: First Glance**\n\n* `PassengerId` is the unique id of the row and it doesn't have any effect on target\n* `Survived` is the target variable we are trying to predict (**0** or **1**):\n    - **1 = Survived**\n    - **0 = Not Survived**\n* `Pclass` (Passenger Class) is the socio-economic status of the passenger and it is a categorical ordinal feature which has **3** unique values (**1**,  **2 **or **3**):\n    - **1 = Upper Class**\n    - **2 = Middle Class**\n    - **3 = Lower Class**\n    **Note**: We should check the relation of Pclass and Survived.\n* `Name` is the passengers's name.\n* `Sex` is the gender of the passengers..\n* `Age` is the age of passengers.\n    - About 20% of Are values are missing.\n* `SibSp` is the total number of the passengers' siblings and spouse.\n* `Parch` is the total number of the passengers' parents and children.\n* `Ticket` is the ticket number of the passenger.\n* `Fare` is the passenger fare.\n    - 1 Fare missing value in test set\n* `Cabin` is the cabin number of the passenger.\n    - About 78% of Cabin values are missing\n* `Embarked` is port of embarkation and it is a categorical feature which has **3** unique values (**C**, **Q** or **S**):\n    - **C = Cherbourg**\n    - **Q = Queenstown**\n    - **S = Southampton**\n    - 2 Embarked missing values in train set. ","50bb42ff":"## **9. Target Encoding and Training**\nAs both logistic regression and random forest reached the same score 0.784, I probabaly have reached the limit of the feature set.\nTypically, at the later stage of feature engineering, I would add target encoding. \n\nTarget encoding usually does **NOT** help **understanding** the data better, but more help to **boost** prediction **score**. For this feature set, target encoding Survival rate on **Family** or **Ticket** group would be a good choice, as if some people **survived** in the training set, then the rest of group would have **similar chance** to survive in the test set. \n\nWhen applying target encoding on **Family** group and **Ticket** group, we need to solve the following problems:\n\n1. For both Family and Ticket group, some group exist only in **test** set.\n2. Lots of people are traveling alone. \n    * For 1 and 2, I created a feature SurvivalRate_NA to indicate if someone is traveling alone, or the group only exist in test set. If so, (Group)SurvivalRate_NA = 1. For people with SurvivalRate_NA = 1, their family survival rate or ticket group survival rate would be 0.\n3. Do we need to combine Family group survival rate and Ticket group survival rate, or keep them both in feature set?\n4. If combine, which method? Min, max, mean?\n    * There are 3 cases for Family-Ticket group difference:\n        1. Some people travel with friends. Their family survival rate would be 0.\n        2. Some people travel with family but not on same ticket, so their Ticket survival rate would be closer to 0.\n        3. Majority of families are on the same Ticket.\n    \n    In case A and B, the group bonding is **weaker** than the one of C, because **friends** bonding is **weaker** than **family**, and as ticket could be relative to Cabin posotion, the bonding of family members with different tickets is **weaker** than the one of family of same ticket. By averaging, the result survival rate would be lower in case A and B, but in case C it won't be affected, which represents the actual bonding better.\n\n**Note**: sometimes people who have the same last name do not come from the same family, but such \"mistake\" may actually improve the model. One of the weak point of **target encoding** is that it can make the model **overfit** easily. The \"mistake\" and the averaging of Ticket group and Family group survival rate can help to **reduce overfitting**.","231e4f73":"## **4. Cabin: Completing and Feature Engineering**\n    \n`Cabin`: Unlike Age, 78% percent of Cabin values are missing, which means it is more likely missing for an actual reason, instead of being wiped on purpose. Typically, grouping values and assigning a value isMissing are good methods to complete such feature. \n\nFigure 1,2: by plotting the relation between **Cabin** against **Pclass** we see that:\n* A, B, C, T: 100% Pclass 1\n* D, E: Majority are Pclass 1,\n* F: 62% Pclass 2 and 38% Pclass 3.\n* G: 100% Pclass 3.\n* n: People with missing Cabin value. Majority of n is Pclass 3.\n\nFigure 3, 4, 5: By combining the relation between **Cabin**, **Pclass**, **Survived**, **Age** and **Sex**, we can explain some uncommon situations:\n* A: Although A has only 1st class, but the majority is **male** and the mean **age** is older, so **survival** rate is **lower**.\n* G: Deck G has only **female** in **3rd** class, which is probably why its **survival** rate is **higher** than the average one of Pclass 3.\n* n: people who have **missing** Cabin values are mostly from 3rd class, and thus the **survival** rate is **lower**. Or the other way around: these 3rd class passengers mostly did not survive, so their Cabin information could not be collected.\n\nFor the rest, the survival rate falls in expectations. Since the uncommon survival rate in Cabin A can be explained by differernt Age and Sex distribution, we can safely group the Cabin_initial values basing on Pclass (Figure 2):\n* A, B, C, T --> 'ABC'\n* D, E --> 'DE'\n* F, G --> 'FG'\n* n --> 'N'","a624f363":"## **3. Age: Advanced EDA and Completing**\n\n`Age`: After checking through the correlation between Age and (Sex, Pclass and Embarked, in Figure 1, 2, 3), no obvious distribution difference was found. This means that most likely the 20% of Age was wiped out on purpose. My **assumption** is that Kaggle did this on **purpose** to imitate missing values in real life. \n\nHowever, there are hidden relations between **Name**, **SibSp**, **Parch** and **Age**:\n* All the people with title Master are under age 14.5.\n* Most of children have tile Master(boy) or Miss(girl). Exceptions 6 out of 109, 5 boys and 1 girl (married).\n* Most of children have Parch > 0. Exception 11 out of 109.\n\n**Assumption**: People are most likely to be child if they have title **Master** or **Miss**, and **Parch  > 0**.\n\nThis assumption turns out to be true. We can verify it by checking the Ticket, last name, Parch and SibSp. The steps are at the end of section 1.3.1.\n\nI created a feature **isChild** to show if someone is likely to be the **young, unmarried and travling with parents** in the family. Then the missing **Age** values are filled by the median of corresponding **Pclass**, **Sex** and **isChild**.","808c12db":"## **2. EDA: Feature Distribution and Correlation with the Target Value**\n\nFigure 1, 2, 3, 4: Plot the distribution of each feature(all_data, and train vs test), and the correlation between each feature and Survived.\n\n* `Age`: Children have higher chance to survive.\n* `Pclass`: People in Pclass 1 and 2 have higher chance to survive.\n* `Fare`: People who paid more have higher chance to survive.\n* `Sex`: Female have higher chance to survive.\n\nFigure 5, 6: No significant distribution difference is found between training set and test set.\n\nFigure 7, 8 ,9: Deeper relation between `Embarked`, `Age`, `Sex` and `Pclass`:\n* `Embarked`: People embarked from C have higher chance to survive, but this seems be relative to Pclass and Sex.\n* `Pclass`:\n    - Pclass 1: Most of women and children survived. Man's survival rate is lower than woman and child, but higher than the one in Pclass 3.\n    - Pclass 2: Most of women and children survived. However man's survival rate is lower than the one in Pclass 3.\n    - Pclass 3: Women and children have higher survival rate than men. Overall the survival rate is lower than Pclass 1 and 2.","febf7f36":"### **Proof** of the **Assumption**: \n\nPeople are most likely to be child if they have title **Master** or **Miss**, and **Parch  > 0**.","52117cbd":"## **9. Thank you**\nYou made it all the way here!\n\nAfter tunning SVC, we reached 80% accuracy. This also means that this data set has more **potential** and can be used for more advanced modeling such as **stacking**. You are more than welcome to try it out yourself.\n\nAny constructive input is welcome!\n\nIf you find this helpful, please consider up vote :)","b7c529a8":"Another more complex example:\n\nID 1234 and 1257 are one male and one female, they are married (same last name), have same SibSp = 1 and Parch = 9. All the rest of family have SibSp = 8 and Parch = 2. So ID 1234 and 1257 are parents and the rest are their children. These children could be older than 15, but the family relationship can be confirmed.\n\nSince there are not many candidates, I manully went through all of them and confirmed that such family relationship stays true for all candidates.\n\nFor male candidates, title Master indicates that they are children.\n\nFor female candidates, the chance of misassumption on their age is higher, because the title Miss could be used for adults. However, since they are female, they already fall into the other priority group: Sex = female, which have similar survival rate as children in each Pclass, so such possible misassumption will not hurt our model much.","1e4545fe":"## **6. Others: Feature Engineering and Completing**\n`Embarked`: There are only 2 missing values in **Embarked**. We can just fill it with the most frequent value within the same **Pclass** and **Sex**.\n\n`FamilySize`: We can create a new feature **FamilySize = Parch + SibSp + 1**, to indicate the size of the family.\n\n`GroupSize`: Then we can create another feature **GroupSize = max(Ticket_count, FamilySize)**, to include friends, nanny and family members.","7ae94df0":"## **0. Introduction**\nWelcome to my first kernel! \n\nI wrote this kernel **Titanic: Machine Learning from Disaster** as my first data science blog on Kaggle. \n\nIn my opinion, **better understanding of the data is more important than higher prediction score**. \n\nIn this kernel, I will focus more on **Exploratory Data Analysis** and **Feature Engineering**, and less on model tuning. I hope this will help to discover the insight of this data set. At the end I am able to use **logistic regression** to achieve score **78.2** with basic feature set, and achieve **0.791** with **target encoding** by **Random Forest** and **0.801** by **SVC**.","ad73dec4":"## **7. First Training**\n\nHere I only used **Logistic Regression** and **Random Forest**. \n\nAfter fine tunning and poking leader board, both models reached **score 0.784**, which indicates that I probabaly have reached the limit of the current feature set.","12734c63":"For the first candidate, check Ticket 2661:\n\nThe female has title Mrs (married), and has Parch = 2. The two 'Master' both have SibSp = 1 and Parch = 2. \n\nWe can now assume that ID 1117 is the mom, and ID 66 and 710 are two sons."}}