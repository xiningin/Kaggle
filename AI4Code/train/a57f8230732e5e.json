{"cell_type":{"336fde81":"code","431a2bda":"code","b752a3c3":"code","bbaf4295":"code","c78bcdc8":"code","d7a3c42c":"code","e93c9b6b":"code","e1350aa5":"code","e5fd41c4":"code","5e613505":"code","a4a92a95":"code","71daf521":"code","d0a736a7":"code","91148dca":"code","09e248d1":"code","daf6951c":"code","07d3a4b8":"code","4cd7c095":"markdown","3e8cfb2d":"markdown","19c37ea5":"markdown","6421764e":"markdown","1ceca46f":"markdown","52a25354":"markdown","e27fbc97":"markdown","117ef4b2":"markdown"},"source":{"336fde81":"import numpy as np \nimport pandas as pd\n\nfrom fastai.vision import *\nimport seaborn as sns\nimport cv2","431a2bda":"from pathlib import Path\n\nshape = (1600, 256)\n\ndata_folder = Path('\/kaggle\/input\/severstal-steel-defect-detection\/')\n\ntrain_data = pd.read_csv(data_folder \/ 'train.csv')\ntrain_data = pd.concat([train_data, train_data.ImageId_ClassId.str.split('_', expand=True).rename(columns={0: 'ImageId', 1: 'ClassId'})], axis=1)\n\ntrain_data.head()","b752a3c3":"train_df = train_data[['ImageId', 'ClassId', 'EncodedPixels']]\nmask_map = train_df.set_index(['ImageId', 'ClassId'])\n\nmask_map.head()","bbaf4295":"imgs_with_masks = train_data.ImageId.drop_duplicates().isin(train_data.dropna().ImageId.drop_duplicates()).sum()\ntotal_images = train_data.ImageId.drop_duplicates().shape[0]\n\nprint(f'{imgs_with_masks}\/{total_images} --- {imgs_with_masks \/ total_images * 100} %')\n","c78bcdc8":"sns.distplot(train_data.dropna()[['ImageId', 'ClassId']].groupby('ImageId').count())","d7a3c42c":"def mask2rle(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels= img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    \n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n \ndef rle2mask(mask_rle, shape=(1600,256)):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (width,height) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n        \n    return img.reshape(shape).T\n\ndef decode_image_mask(img_id, shape=(1600,256)):\n    masks = []\n    for cl in range(1,5):\n        cl_str = str(cl)\n\n        cl_rle = mask_map.loc[(img_id, cl_str)].values[0]\n\n        if not isinstance(cl_rle, str):\n            cl_mask = np.zeros(shape).T\n        else:\n            cl_mask = rle2mask(cl_rle, shape)\n\n        cl_mask = cl_mask[np.newaxis,:,:] * cl\n        masks.append(cl_mask)\n\n    masks = np.concatenate(masks, axis=0)\n    masks = np.sum(masks, axis=0)\n\n    return masks\n\ndef encode_image_mask(mask_tensor, shape=(1600,256)):\n    mask_array = np.array(mask_tensor)\n    mask_array = mask_array.argmax(axis=0)  \n        \n    rles = []\n    for i in range(1,5):\n        mask = (mask_array == i).astype(np.uint8)\n        mask = cv2.resize(mask, shape, cv2.INTER_NEAREST)\n        \n        rle = mask2rle(mask)\n        rle = np.nan if rle == '' else rle\n        rles.append(rle)\n    \n    return rles","e93c9b6b":"import fastai\n\ndef transform(self, tfms:Optional[Tuple[TfmList,TfmList]]=(None,None), **kwargs):\n    if not tfms: tfms=(None,None)\n    assert is_listy(tfms) and len(tfms) == 2\n    self.train.transform(tfms[0], **kwargs)\n    self.valid.transform(tfms[1], **kwargs)\n    kwargs['tfm_y'] = False # Test data has no labels\n    if self.test: self.test.transform(tfms[1], **kwargs)\n    return self\n\nfastai.data_block.ItemLists.transform = transform\n\nclass ServerstalSegmentationLabelList(SegmentationLabelList):\n    \n    def open(self, image_id:str):\n        mask = decode_image_mask(image_id, shape)[np.newaxis,:,:]\n        mask = torch.tensor(mask, dtype=torch.float)\n        \n        return ImageSegment(mask)\n    \n    ","e1350aa5":"from sklearn.model_selection import train_test_split\n\n_, valid_ids = train_test_split(train_df.ImageId.drop_duplicates(), test_size=0.2)\n\ntrain_df['is_valid'] = train_df.ImageId.isin(valid_ids)","e5fd41c4":"bs = 16\nsize = (128,800)\ndata = (\n    SegmentationItemList.from_df(train_df, data_folder, cols='ImageId', folder='train_images')\n    .split_from_df('is_valid')\n    .label_from_df('ImageId', label_cls=ServerstalSegmentationLabelList, classes=[0,1,2,3,4])\n    .add_test_folder(data_folder \/ 'test_images', label=None)\n    .transform(get_transforms(flip_vert=True), size=size, tfm_y=True)\n    .databunch(bs=bs)\n    .normalize(imagenet_stats)\n)\n\ndata.show_batch(5)","5e613505":"import math\nimport torch\nfrom torch.optim import Adam\n\n\nclass RAdam(Adam):\n    \n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n                 weight_decay=0, amsgrad=False):\n        super(RAdam, self).__init__(params, lr=lr, betas=betas, eps=eps, \n                                   weight_decay=weight_decay)\n        \n\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n\n                state = self.state[p]\n\n                beta1, beta2 = group['betas']\n                \n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n                    # Max SMA\n                    state['max_sma'] = 2\/(1 - beta2) - 1\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                \n                state['step'] += 1\n\n                if group['weight_decay'] != 0:\n                    grad.add_(group['weight_decay'], p.data)\n                \n                max_sma = state['max_sma']\n                \n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                \n                bias_correction1 = 1 - beta1 ** state['step']\n                \n                beta2_t = beta2 ** state['step']\n                approx_sma = max_sma - 2 * state['step'] * beta2_t \/ beta2_t\n                \n                if approx_sma > 4:\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n                    bias_correction2 = 1 - beta2 ** state['step']\n                    var_rectification = math.sqrt(\n                                                    ((approx_sma - 4) * (approx_sma - 2) * max_sma) \n                                                    \/ ((max_sma - 4) * (max_sma - 2) * approx_sma)\n                                                 )\n                    step_size = group['lr'] * var_rectification * math.sqrt(bias_correction2) \/ bias_correction1\n\n                    p.data.addcdiv_(-step_size, exp_avg, denom)\n                else:\n                    p.data.add_(-group['lr'] * exp_avg \/ bias_correction1)\n\n        return loss","a4a92a95":"base_model = models.resnet34\n\nlearn = unet_learner(data, base_model, model_dir='\/kaggle\/working', opt_func=RAdam, metrics=[dice])\nlearn.path = Path('\/kaggle\/working')","71daf521":"lr_find(learn)\n\nlearn.recorder.plot()","d0a736a7":"learn.fit_one_cycle(9, 1e-3)","91148dca":"learn.unfreeze()\n\nlr_find(learn)\n\nlearn.recorder.plot()","09e248d1":"learn.fit_one_cycle(8, slice(4e-6\/20, 4e-6))","daf6951c":"learn.export()","07d3a4b8":"# load_learner('\/kaggle\/working')","4cd7c095":"### Create Validation Data","3e8cfb2d":"## Quick Look at the Data","19c37ea5":"### How many images have masks?","6421764e":"### How many masks per image?","1ceca46f":"## Utilities","52a25354":"## Train Utils","e27fbc97":"## Train on All Images","117ef4b2":"**Changelog**\n\n* RAdam optimizer (practice implementation by inheriting torch Adam and overriding step method)"}}