{"cell_type":{"715fcc9f":"code","645f2f6e":"code","f160e11c":"code","52bd1aaa":"code","b44658ae":"code","21cf183b":"code","70680ccd":"code","4fba2d25":"code","84f58372":"code","ece99d6a":"code","26868872":"code","feb95152":"code","29c07c79":"code","45844f34":"code","e07b900b":"code","80dfa11e":"code","8ca5b3d1":"code","a9a02716":"code","4b1a7aa5":"code","5da6a8cd":"code","bd2c9d9b":"code","d47c5920":"code","7aa48aaf":"code","8bc91d6e":"code","17271011":"code","becbef6d":"code","f164b9a4":"code","c52bd9eb":"code","4de0c2e9":"code","6a6394a0":"code","21ca92a4":"code","2d589760":"code","8e711e4b":"code","83e04557":"code","9314482a":"code","1d1ef431":"code","7ddc0f90":"code","57a3e6ba":"code","158673ed":"code","2164cebc":"code","1a46ae76":"code","7b76c24e":"code","2db14317":"code","6fbb7253":"code","11ef4869":"code","951ef49d":"code","8f5ee359":"code","2189e74e":"code","5500d5c6":"code","5f79c9ee":"code","230b84e4":"code","ccb4359d":"code","6edfbdc9":"code","dbfdaaa9":"code","212b1ef3":"code","9abfc20c":"code","b7058270":"code","a2972f7d":"code","327e5a26":"code","e0a3da32":"code","fd6a01cc":"code","cd2f8f21":"code","b456e84e":"code","fc994cb4":"code","f9b2064f":"code","d61558a2":"code","260eda56":"code","e2ef3a5b":"code","6c322923":"code","936a7e27":"code","7ff0f968":"code","9089b61f":"code","e4669ecc":"code","ef2cfa71":"code","27cd9fd9":"code","d465ba89":"code","71cbfbde":"code","57be47a5":"code","2d5bc970":"code","6013ad69":"code","ce913816":"code","bb96c88f":"code","9403d17c":"code","90bf32fe":"code","71295cb8":"code","d1e7a8e5":"code","b5366a74":"code","daa09342":"code","3b2ec378":"code","a444aa0e":"code","e55273dd":"code","de1c9419":"code","7ec87907":"markdown","6610fabc":"markdown","f99d156c":"markdown","2e6f09e3":"markdown","264774a5":"markdown","777340cf":"markdown","20e593ae":"markdown","edba41c8":"markdown","9a3f0716":"markdown","d9b34aee":"markdown","26146c27":"markdown","ae260656":"markdown","dc923d2d":"markdown","9e3c7dd2":"markdown","a162bf6e":"markdown","68ddd453":"markdown","53d01568":"markdown","1c6ba0a3":"markdown","25370201":"markdown","fe379af2":"markdown","6f08a053":"markdown","5cd7b46b":"markdown","ae4a1296":"markdown","9b5364e2":"markdown","2ddc175b":"markdown","d06cc9a4":"markdown","a8cf78a6":"markdown","606170c0":"markdown","051e4b98":"markdown","177cd1eb":"markdown","9ebbeaab":"markdown","49c4233d":"markdown","14fdee14":"markdown","df540409":"markdown","2b5af361":"markdown","85acdf36":"markdown","d74fdc82":"markdown","995f4725":"markdown","e028a412":"markdown","bf9cea48":"markdown","c36a7f79":"markdown","4002c5d3":"markdown","7f779ef7":"markdown","d7029eb7":"markdown","69ac852c":"markdown","31983d42":"markdown","4e5c3b78":"markdown","472e0318":"markdown","024f36a1":"markdown","6ad2d42f":"markdown","7070aaa3":"markdown","b0935d16":"markdown","407788ca":"markdown","c28dc717":"markdown","567dd8a7":"markdown","ec95ecbf":"markdown","fb7946f6":"markdown","dabb5f47":"markdown","6177d064":"markdown","a0173358":"markdown","6fcad251":"markdown","8bcd6a90":"markdown","c2512573":"markdown","4f29e1f8":"markdown","a50f33b5":"markdown","74518c42":"markdown","5433c481":"markdown","53f7e80c":"markdown","3aacabe6":"markdown","d81b2c81":"markdown","7ee944df":"markdown","296fa8a4":"markdown","f47c8459":"markdown","8e767eb3":"markdown","e4fda53a":"markdown","296cc1be":"markdown","3b69a34d":"markdown","9a030d80":"markdown","5fc0d5bd":"markdown"},"source":{"715fcc9f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","645f2f6e":"#load libraries\nfrom __future__ import division\nimport pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime, nltk, warnings\nimport matplotlib.cm as cm\nimport itertools\nfrom pathlib import Path\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom sklearn import preprocessing, model_selection, metrics, feature_selection\nfrom sklearn.model_selection import GridSearchCV, learning_curve\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import neighbors, linear_model, svm, tree, ensemble\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.decomposition import PCA\nfrom IPython.display import display, HTML\n%matplotlib inline","f160e11c":"#read dataset\ndf_initial = pd.read_csv('..\/input\/customer-segmentation-of-online-retail\/data.csv',encoding=\"ISO-8859-1\",\n                         dtype={'CustomerID': str,'InvoiceID': str})\nprint('Dataframe dimensions:', df_initial.shape)\ndf_initial['InvoiceDate'] = pd.to_datetime(df_initial['InvoiceDate'])","52bd1aaa":"# show head\ndisplay(df_initial[:5])","b44658ae":"#identify and clean null values\n# gives some infomation on columns types and number of null values\ntab_info=pd.DataFrame(df_initial.dtypes).T.rename(index={0:'column type'})\ntab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()).T.rename(index={0:'null values (nb)'}))\ntab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()\/df_initial.shape[0]*100).T.\n                         rename(index={0:'null values (%)'}))\nprint ('-' * 10 + \" Display information about column types and number of null values \" + '-' * 10 )\nprint \ndisplay(tab_info)\n","21cf183b":"#remove null data entries\ndf_initial.dropna(axis = 0, subset = ['CustomerID'], inplace = True)\nprint('Dataframe dimensions:', df_initial.shape)\n# gives some information on columns types and number of null values\ntab_info=pd.DataFrame(df_initial.dtypes).T.rename(index={0:'column type'})\ntab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()).T.rename(index={0:'null values (nb)'}))\ntab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()\/df_initial.shape[0]*100).T.\n                         rename(index={0:'null values (%)'}))\ndisplay(tab_info)","70680ccd":"print('Duplicate data entries: {}'.format(df_initial.duplicated().sum()))\ndf_initial.drop_duplicates(inplace = True)","4fba2d25":"#explore data countrywise\ntemp = df_initial[['CustomerID', 'InvoiceNo', 'Country']].groupby(\n    ['CustomerID', 'InvoiceNo', 'Country']).count()\ntemp = temp.reset_index(drop = False)\ncountries = temp['Country'].value_counts()\nprint('No. of cuntries in dataframe: {}'.format(len(countries)))\n\ntemp_no_of_order_per_count = df_initial[['CustomerID','Country']].groupby(['Country']).count()\ntemp_no_of_order_per_count = temp_no_of_order_per_count.reset_index(drop = False)\n\nprint('-' * 10 + \" Country-wise order calculation \"+ '-' * 10)\nprint\nprint (temp_no_of_order_per_count.sort_values(\n    by='CustomerID', ascending=False).rename(index=str,\n                                        columns={\"CustomerID\": \"Country wise number of order\"}))","84f58372":"#explore data \"customers and products\"wise\npd.DataFrame([{'products': len(df_initial['StockCode'].value_counts()),    \n               'transactions': len(df_initial['InvoiceNo'].value_counts()),\n               'customers': len(df_initial['CustomerID'].value_counts()),  \n              }], columns = ['products', 'transactions', 'customers'], \n              index = ['quantity'])","ece99d6a":"temp = df_initial.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['InvoiceDate'].count()\nnb_products_per_basket = temp.rename(columns = {'InvoiceDate':'Number of products'})\nnb_products_per_basket[:10].sort_values('CustomerID')","26868872":"nb_products_per_basket['order_cancelled'] = nb_products_per_basket['InvoiceNo'].apply(\n    lambda x:int('C' in x))\ndisplay(nb_products_per_basket[:5])\n\n\nn1 = nb_products_per_basket['order_cancelled'].sum()\nn2 = nb_products_per_basket.shape[0]\npercentage = (n1\/n2)*100\nprint('Number of orders cancelled: {}\/{} ({:.2f}%) '.format(n1, n2, percentage))","feb95152":"display(df_initial.sort_values('CustomerID')[:5])","29c07c79":"df_check = df_initial[df_initial['Quantity'] < 0][['CustomerID','Quantity',\n                                                   'StockCode','Description','UnitPrice']]\nfor index, col in  df_check.iterrows():\n    if df_initial[(df_initial['CustomerID'] == col[0]) & (df_initial['Quantity'] == -col[1]) \n                & (df_initial['Description'] == col[2])].shape[0] == 0: \n        print(df_check.loc[index])\n        print(15*'-'+'>'+' HYPOTHESIS NOT FULFILLED')\n        break","45844f34":"f_check = df_initial[(df_initial['Quantity'] < 0) & (df_initial['Description'] != 'Discount')][\n                                 ['CustomerID','Quantity','StockCode',\n                                  'Description','UnitPrice']]\n\nfor index, col in  df_check.iterrows():\n    if df_initial[(df_initial['CustomerID'] == col[0]) & (df_initial['Quantity'] == -col[1]) \n                & (df_initial['Description'] == col[2])].shape[0] == 0: \n        print(index, df_check.loc[index])\n        print(15*'-'+'>'+' HYPOTHESIS NOT FULFILLED')\n        break","e07b900b":"df_cleaned = df_initial.copy(deep = True)\ndf_cleaned['QuantityCanceled'] = 0\n\nentry_to_remove = [] ; doubtfull_entry = []\n\nfor index, col in  df_initial.iterrows():\n    if (col['Quantity'] > 0) or col['Description'] == 'Discount': continue        \n    df_test = df_initial[(df_initial['CustomerID'] == col['CustomerID']) &\n                         (df_initial['StockCode']  == col['StockCode']) & \n                         (df_initial['InvoiceDate'] < col['InvoiceDate']) & \n                         (df_initial['Quantity']   > 0)].copy()\n\n    # Cancelation WITHOUT counterpart\n    if (df_test.shape[0] == 0): \n        doubtfull_entry.append(index)\n   \n    # Cancelation WITH a counterpart\n    elif (df_test.shape[0] == 1): \n        index_order = df_test.index[0]\n        df_cleaned.loc[index_order, 'QuantityCanceled'] = -col['Quantity']\n        entry_to_remove.append(index)        \n   \n    # Various counterparts exist in orders: we delete the last one\n    elif (df_test.shape[0] > 1): \n        df_test.sort_index(axis=0 ,ascending=False, inplace = True)        \n        for ind, val in df_test.iterrows():\n            if val['Quantity'] < -col['Quantity']: continue\n            df_cleaned.loc[ind, 'QuantityCanceled'] = -col['Quantity']\n            entry_to_remove.append(index) \n            break            ","80dfa11e":"print(\"entry_to_remove: {}\".format(len(entry_to_remove)))\nprint(\"doubtfull_entry: {}\".format(len(doubtfull_entry)))","8ca5b3d1":"df_cleaned.drop(entry_to_remove, axis = 0, inplace = True)\ndf_cleaned.drop(doubtfull_entry, axis = 0, inplace = True)\nremaining_entries = df_cleaned[(df_cleaned['Quantity'] < 0) & (df_cleaned['StockCode'] != 'D')]\nprint(\"nb of entries to delete: {}\".format(remaining_entries.shape[0]))\nremaining_entries[:5]","a9a02716":"df_cleaned[(df_cleaned['CustomerID'] == 14048) & (df_cleaned['StockCode'] == '22464')]","4b1a7aa5":"list_special_codes = df_cleaned[df_cleaned['StockCode'].str.contains('^[a-zA-Z]+', regex=True)]['StockCode'].unique()\nlist_special_codes\n\nfor code in list_special_codes:\n    print(\"{:<15} -> {:<30}\".format(code, df_cleaned[df_cleaned['StockCode'] == code]['Description'].unique()[0]))","5da6a8cd":"df_cleaned['TotalPrice'] = df_cleaned['UnitPrice'] * (df_cleaned['Quantity'] - df_cleaned['QuantityCanceled'])\ndf_cleaned.sort_values('CustomerID')[:5]","bd2c9d9b":"\n# sum of purchases \/ user & order\ntemp = df_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['TotalPrice'].sum()\nbasket_price = temp.rename(columns = {'TotalPrice':'Basket Price'})\n\n# date of the order\ndf_cleaned['InvoiceDate_int'] = df_cleaned['InvoiceDate'].astype('int64')\ntemp = df_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['InvoiceDate_int'].mean()\ndf_cleaned.drop('InvoiceDate_int', axis = 1, inplace = True)\nbasket_price.loc[:, 'InvoiceDate'] = pd.to_datetime(temp['InvoiceDate_int'])\n\n# selection of significant entries\nbasket_price = basket_price[basket_price['Basket Price'] > 0]\nbasket_price.sort_values('CustomerID')[:6]","d47c5920":"# Purchase count\nprice_range = [0, 50, 100, 200, 500, 1000, 5000, 50000]\ncount_price = []\nfor i, price in enumerate(price_range):\n    if i == 0: continue\n    val = basket_price[(basket_price['Basket Price'] < price) &\n                       (basket_price['Basket Price'] > price_range[i-1])]['Basket Price'].count()\n    count_price.append(val)\n\n# Representation of the number of purchases \/ amount       \nplt.rc('font', weight='bold')\nf, ax = plt.subplots(figsize=(11, 6))\ncolors = ['yellowgreen', 'gold', 'wheat', 'c', 'violet', 'royalblue','firebrick']\nlabels = [ '{}<.<{}'.format(price_range[i-1], s) for i,s in enumerate(price_range) if i != 0]\nsizes  = count_price\nexplode = [0.0 if sizes[i] < 100 else 0.0 for i in range(len(sizes))]\nax.pie(sizes, explode = explode, labels=labels, colors = colors,\n       autopct = lambda x:'{:1.0f}%'.format(x) if x > 1 else '',\n       shadow = False, startangle=0)\nax.axis('equal')\nf.text(0.5, 1.01, \"Distribution of order amounts\", ha='center', fontsize = 18);","7aa48aaf":"is_noun = lambda pos: pos[:2] == 'NN'\n\ndef keywords_inventory(dataframe, colonne = 'Description'):\n    stemmer = nltk.stem.SnowballStemmer(\"english\")\n    keywords_roots  = dict()  # collect the words \/ root\n    keywords_select = dict()  # association: root <-> keyword\n    category_keys   = []\n    count_keywords  = dict()\n    icount = 0\n    for s in dataframe[colonne]:\n        if pd.isnull(s): continue\n        lines = s.lower()\n        tokenized = nltk.word_tokenize(lines)\n        nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] \n        \n        for t in nouns:\n            t = t.lower() ; racine = stemmer.stem(t)\n            if racine in keywords_roots:                \n                keywords_roots[racine].add(t)\n                count_keywords[racine] += 1                \n            else:\n                keywords_roots[racine] = {t}\n                count_keywords[racine] = 1\n    \n    for s in keywords_roots.keys():\n        if len(keywords_roots[s]) > 1:  \n            min_length = 1000\n            for k in keywords_roots[s]:\n                if len(k) < min_length:\n                    clef = k ; min_length = len(k)            \n            category_keys.append(clef)\n            keywords_select[s] = clef\n        else:\n            category_keys.append(list(keywords_roots[s])[0])\n            keywords_select[s] = list(keywords_roots[s])[0]\n                   \n    print(\"number of keywords in variable '{}': {}\".format(colonne,len(category_keys)))\n    return category_keys, keywords_roots, keywords_select, count_keywords","8bc91d6e":"df_produits = pd.DataFrame(df_initial['Description'].unique()).rename(columns = {0:'Description'})","17271011":"keywords, keywords_roots, keywords_select, count_keywords = keywords_inventory(df_produits)","becbef6d":"list_products = []\nfor k,v in count_keywords.items():\n    list_products.append([keywords_select[k],v])\nlist_products.sort(key = lambda x:x[1], reverse = True)","f164b9a4":"liste = sorted(list_products, key = lambda x:x[1], reverse = True)\n\nplt.rc('font', weight='normal')\nfig, ax = plt.subplots(figsize=(7, 25))\ny_axis = [i[1] for i in liste[:125]]\nx_axis = [k for k,i in enumerate(liste[:125])]\nx_label = [i[0] for i in liste[:125]]\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 13)\nplt.yticks(x_axis, x_label)\nplt.xlabel(\"Number of occurences\", fontsize = 18, labelpad = 10)\nax.barh(x_axis, y_axis, align = 'center')\nax = plt.gca()\nax.invert_yaxis()\n\nplt.title(\"Words occurence\",bbox={'facecolor':'k', 'pad':5}, color='w',fontsize = 25)\nplt.show()","c52bd9eb":"list_products = []\nfor k,v in count_keywords.items():\n    word = keywords_select[k]\n    if word in ['pink', 'blue', 'tag', 'green', 'orange']: continue\n    if len(word) < 3 or v < 13: continue\n    if ('+' in word) or ('\/' in word): continue\n    list_products.append([word, v])\n \nlist_products.sort(key = lambda x:x[1], reverse = True)\nprint('Preserved words:', len(list_products))","4de0c2e9":"liste_produits = df_cleaned['Description'].unique()\n#print(liste_produits[0:2])\nX = pd.DataFrame()\nfor key, occurence in list_products:\n    X.loc[:, key] = list(map(lambda x:int(key.upper() in x), liste_produits))\n#print(X[0:1])","6a6394a0":"threshold = [0, 1, 2, 3, 5, 10]\nlabel_col = []\nfor i in range(len(threshold)):\n    if i == len(threshold)-1:\n        col = '.>{}'.format(threshold[i])\n    else:\n        col = '{}<.<{}'.format(threshold[i],threshold[i+1])\n    #print(i)\n    #print(col)\n    label_col.append(col)\n    X.loc[:, col] = 0\n\nfor i, prod in enumerate(liste_produits):\n    prix = df_cleaned[ df_cleaned['Description'] == prod]['UnitPrice'].mean()\n    #print (prix)\n    j = 0\n    while prix > threshold[j]:\n        j+=1\n        if j == len(threshold): break\n    X.loc[i, label_col[j-1]] = 1","21ca92a4":"print(\"{:<8} {:<20} \\n\".format('range', 'number of products') + 20*'-')\nfor i in range(len(threshold)):\n    if i == len(threshold)-1:\n        col = '.>{}'.format(threshold[i])\n    else:\n        col = '{}<.<{}'.format(threshold[i],threshold[i+1])    \n    print(\"{:<10}  {:<20}\".format(col, X.loc[:, col].sum()))","2d589760":"matrix = X.values\nfor n_clusters in range(3,10):\n    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30)\n    kmeans.fit(matrix)\n    clusters = kmeans.predict(matrix)\n    silhouette_avg = silhouette_score(matrix, clusters)\n    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)","8e711e4b":"n_clusters = 5\nsilhouette_avg = -1\nwhile silhouette_avg < 0.145:\n    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30)\n    kmeans.fit(matrix)\n    clusters = kmeans.predict(matrix)\n    silhouette_avg = silhouette_score(matrix, clusters)\n    \n    #km = kmodes.KModes(n_clusters = n_clusters, init='Huang', n_init=2, verbose=0)\n    #clusters = km.fit_predict(matrix)\n    #silhouette_avg = silhouette_score(matrix, clusters)\n    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)","83e04557":"pd.Series(clusters).value_counts()","9314482a":"def graph_component_silhouette(n_clusters, lim_x, mat_size, sample_silhouette_values, clusters):\n    #plt.rcParams[\"patch.force_edgecolor\"] = True\n    plt.style.use('fivethirtyeight')\n    mpl.rc('patch', edgecolor = 'dimgray', linewidth=1)\n    \n    fig, ax1 = plt.subplots(1, 1)\n    fig.set_size_inches(8, 8)\n    ax1.set_xlim([lim_x[0], lim_x[1]])\n    ax1.set_ylim([0, mat_size + (n_clusters + 1) * 10])\n    y_lower = 10\n    for i in range(n_clusters):\n        \n        # Aggregate the silhouette scores for samples belonging to cluster i, and sort them\n        ith_cluster_silhouette_values = sample_silhouette_values[clusters == i]\n        ith_cluster_silhouette_values.sort()\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n        #color = cm.spectral(float(i) \/ n_clusters) facecolor=color, edgecolor=color,       \n        ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, alpha=0.8)\n        \n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.03, y_lower + 0.5 * size_cluster_i, str(i), color = 'red', fontweight = 'bold',\n                bbox=dict(facecolor='white', edgecolor='black', boxstyle='round, pad=0.3'))\n       \n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10 ","1d1ef431":"# define individual silouhette scores\nsample_silhouette_values = silhouette_samples(matrix, clusters)\n\n# and do the graph\ngraph_component_silhouette(n_clusters, [-0.07, 0.33], len(X), sample_silhouette_values, clusters)","7ddc0f90":"liste = pd.DataFrame(liste_produits)\nliste_words = [word for (word, occurence) in list_products]\n\noccurence = [dict() for _ in range(n_clusters)]\n\nfor i in range(n_clusters):\n    liste_cluster = liste.loc[clusters == i]\n    for word in liste_words:\n        if word in ['art', 'set', 'heart', 'pink', 'blue', 'tag']: continue\n        occurence[i][word] = sum(liste_cluster.loc[:, 0].str.contains(word.upper()))","57a3e6ba":"#word clouds\nef random_color_func(word=None, font_size=None, position=None,\n                      orientation=None, font_path=None, random_state=None):\n    h = int(360.0 * tone \/ 255.0)\n    s = int(100.0 * 255.0 \/ 255.0)\n    l = int(100.0 * float(random_state.randint(70, 120)) \/ 255.0)\n    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n\ndef make_wordcloud(liste, increment):\n    ax1 = fig.add_subplot(4,2,increment)\n    words = dict()\n    trunc_occurences = liste[0:150]\n    for s in trunc_occurences:\n        words[s[0]] = s[1]\n    \n    wordcloud = WordCloud(width=1000,height=400, background_color='lightgrey', \n                          max_words=1628,relative_scaling=1,\n                          color_func = random_color_func,\n                          normalize_plurals=False)\n    wordcloud.generate_from_frequencies(words)\n    ax1.imshow(wordcloud, interpolation=\"bilinear\")\n    ax1.axis('off')\n    plt.title('cluster n{}'.format(increment-1))\n\nfig = plt.figure(1, figsize=(14,14))\ncolor = [0, 160, 130, 95, 280, 40, 330, 110, 25]\nfor i in range(n_clusters):\n    list_cluster_occurences = occurence[i]\n\n    tone = color[i] # define the color of the words\n    liste = []\n    for key, value in list_cluster_occurences.items():\n        liste.append([key, value])\n    liste.sort(key = lambda x:x[1], reverse = True)\n    make_wordcloud(liste, i+1)            ","158673ed":"pca = PCA()\npca.fit(matrix)\npca_samples = pca.transform(matrix)\n\n","2164cebc":"fig, ax = plt.subplots(figsize=(14, 5))\nsns.set(font_scale=1)\nplt.step(range(matrix.shape[1]), pca.explained_variance_ratio_.cumsum(), where='mid',\n         label='cumulative explained variance')\nsns.barplot(np.arange(1,matrix.shape[1]+1), pca.explained_variance_ratio_, alpha=0.5, color = 'g',\n            label='individual explained variance')\nplt.xlim(0, 100)\n\nax.set_xticklabels([s if int(s.get_text())%2 == 0 else '' for s in ax.get_xticklabels()])\n\nplt.ylabel('Explained variance', fontsize = 14)\nplt.xlabel('Principal components', fontsize = 14)\nplt.legend(loc='upper left', fontsize = 13);","1a46ae76":"pca = PCA(n_components=50)\nmatrix_9D = pca.fit_transform(matrix)\nmat = pd.DataFrame(matrix_9D)\nmat['cluster'] = pd.Series(clusters)","7b76c24e":"import matplotlib.patches as mpatches\n\nsns.set_style(\"white\")\nsns.set_context(\"notebook\", font_scale=1, rc={\"lines.linewidth\": 2.5})\n\nLABEL_COLOR_MAP = {0:'r', 1:'gold', 2:'b', 3:'k', 4:'c', 5:'g'}\nlabel_color = [LABEL_COLOR_MAP[l] for l in mat['cluster']]\n\nfig = plt.figure(figsize = (12,10))\nincrement = 0\nfor ix in range(4):\n    for iy in range(ix+1, 4):    \n        increment += 1\n        ax = fig.add_subplot(3,3,increment)\n        ax.scatter(mat[ix], mat[iy], c= label_color, alpha=0.4) \n        plt.ylabel('PCA {}'.format(iy+1), fontsize = 12)\n        plt.xlabel('PCA {}'.format(ix+1), fontsize = 12)\n        ax.yaxis.grid(color='lightgray', linestyle=':')\n        ax.xaxis.grid(color='lightgray', linestyle=':')\n        ax.spines['right'].set_visible(False)\n        ax.spines['top'].set_visible(False)\n        \n        if increment == 9: break\n    if increment == 9: break\n        \n\ncomp_handler = []\nfor i in range(5):\n    comp_handler.append(mpatches.Patch(color = LABEL_COLOR_MAP[i], label = i))\n\nplt.legend(handles=comp_handler, bbox_to_anchor=(1.1, 0.97), \n           title='Cluster',\n           shadow = True, frameon = True, framealpha = 1,fontsize = 13, \n           bbox_transform = plt.gcf().transFigure) #facecolor = 'lightgrey',\n\nplt.tight_layout()","2db14317":"corresp = dict()\nfor key, val in zip (liste_produits, clusters):\n    corresp[key] = val \n\ndf_cleaned['categ_product'] = df_cleaned.loc[:, 'Description'].map(corresp)\ndf_cleaned[['InvoiceNo', 'Description', \n            'categ_product']][:10]","6fbb7253":"for i in range(5):\n    col = 'categ_{}'.format(i)        \n    df_temp = df_cleaned[df_cleaned['categ_product'] == i]\n    price_temp = df_temp['UnitPrice'] * (df_temp['Quantity'] - df_temp['QuantityCanceled'])\n    price_temp = price_temp.apply(lambda x:x if x > 0 else 0)\n    df_cleaned.loc[:, col] = price_temp\n    df_cleaned[col].fillna(0, inplace = True)\n\n\ndf_cleaned[['InvoiceNo', 'Description', \n            'categ_product', 'categ_0', 'categ_1', 'categ_2', 'categ_3','categ_4']][:10]","11ef4869":"# sum of purchases \/ user & order\ntemp = df_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['TotalPrice'].sum()\nbasket_price = temp.rename(columns = {'TotalPrice':'Basket Price'})\n\n# percentage of the price of the order \/ product category\nfor i in range(5):\n    col = 'categ_{}'.format(i) \n    temp = df_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)[col].sum()\n    basket_price.loc[:, col] = temp \n\n# date of the order\n\ndf_cleaned['InvoiceDate_int'] = df_cleaned['InvoiceDate'].astype('int64')\ntemp = df_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['InvoiceDate_int'].mean()\ndf_cleaned.drop('InvoiceDate_int', axis = 1, inplace = True)\nbasket_price.loc[:, 'InvoiceDate'] = pd.to_datetime(temp['InvoiceDate_int'])\n\n# selection of significant entries:\nbasket_price = basket_price[basket_price['Basket Price'] > 0]\nbasket_price.sort_values('CustomerID', ascending = True)[:5]","951ef49d":"print(basket_price['InvoiceDate'].min(), '->',  basket_price['InvoiceDate'].max())\n\nset_entrainement = basket_price[basket_price['InvoiceDate'] < datetime.date(2011,10,1)]\nset_test         = basket_price[basket_price['InvoiceDate'] >= datetime.date(2011,10,1)]\nbasket_price = set_entrainement.copy(deep = True)","8f5ee359":"# of visits and stats on cart amount \/ users\ntransactions_per_user=basket_price.groupby(by=['CustomerID'])['Basket Price'].agg(['count','min',\n                                                                                   'max','mean','sum'])\nfor i in range(5):\n    col = 'categ_{}'.format(i)\n    transactions_per_user.loc[:,col] = basket_price.groupby(by=['CustomerID'])[col].sum() \/\\\n                                            transactions_per_user['sum']*100\n\ntransactions_per_user.reset_index(drop = False, inplace = True)\nbasket_price.groupby(by=['CustomerID'])['categ_0'].sum()\ntransactions_per_user.sort_values('CustomerID', ascending = True)[:5]","2189e74e":"last_date = basket_price['InvoiceDate'].max().date()\n\nfirst_registration = pd.DataFrame(basket_price.groupby(by=['CustomerID'])['InvoiceDate'].min())\nlast_purchase      = pd.DataFrame(basket_price.groupby(by=['CustomerID'])['InvoiceDate'].max())\n\ntest  = first_registration.applymap(lambda x:(last_date - x.date()).days)\ntest2 = last_purchase.applymap(lambda x:(last_date - x.date()).days)\n\ntransactions_per_user.loc[:, 'LastPurchase'] = test2.reset_index(drop = False)['InvoiceDate']\ntransactions_per_user.loc[:, 'FirstPurchase'] = test.reset_index(drop = False)['InvoiceDate']\n\ntransactions_per_user[:5]","5500d5c6":"n1 = transactions_per_user[transactions_per_user['count'] == 1].shape[0]\nn2 = transactions_per_user.shape[0]\nprint(\"No. customers with single purchase: {:<2}\/{:<5} ({:<2.2f}%)\".format(n1,n2,n1\/n2*100))","5f79c9ee":"list_cols = ['count','min','max','mean','categ_0','categ_1','categ_2','categ_3','categ_4']\n#_____________________________________________________________\nselected_customers = transactions_per_user.copy(deep = True)\nmatrix = selected_customers[list_cols].as_matrix()","230b84e4":"scaler = StandardScaler()\nscaler.fit(matrix)\nprint('variables mean values: \\n' + 90*'-' + '\\n' , scaler.mean_)\nscaled_matrix = scaler.transform(matrix)","ccb4359d":"pca = PCA()\npca.fit(scaled_matrix)\npca_samples = pca.transform(scaled_matrix)","6edfbdc9":"#variance of each components\n\nig, ax = plt.subplots(figsize=(14, 5))\nsns.set(font_scale=1)\nplt.step(range(matrix.shape[1]), pca.explained_variance_ratio_.cumsum(), where='mid',\n         label='cumulative explained variance')\nsns.barplot(np.arange(1,matrix.shape[1]+1), pca.explained_variance_ratio_, alpha=0.5, color = 'g',\n            label='individual explained variance')\nplt.xlim(0, 10)\n\nax.set_xticklabels([s if int(s.get_text())%2 == 0 else '' for s in ax.get_xticklabels()])\n\nplt.ylabel('Explained variance', fontsize = 14)\nplt.xlabel('Principal components', fontsize = 14)\nplt.legend(loc='best', fontsize = 13);","dbfdaaa9":"n_clusters = 11\nkmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=100)\nkmeans.fit(scaled_matrix)\nclusters_clients = kmeans.predict(scaled_matrix)\nsilhouette_avg = silhouette_score(scaled_matrix, clusters_clients)\nprint('silhouette score: {:<.3f}'.format(silhouette_avg))","212b1ef3":"pd.DataFrame(pd.Series(clusters_clients).value_counts(), columns = ['number of clients']).T","9abfc20c":"pca = PCA(n_components=6)\nmatrix_3D = pca.fit_transform(scaled_matrix)\nmat = pd.DataFrame(matrix_3D)\nmat['cluster'] = pd.Series(clusters_clients)","b7058270":"import matplotlib.patches as mpatches\n\nsns.set_style(\"white\")\nsns.set_context(\"notebook\", font_scale=1, rc={\"lines.linewidth\": 2.5})\n\nLABEL_COLOR_MAP = {0:'r', 1:'tan', 2:'b', 3:'k', 4:'c', 5:'g', 6:'deeppink', 7:'skyblue', 8:'darkcyan',\n                   9:'orange',\n                   10:'yellow', 11:'tomato', 12:'seagreen'}\nlabel_color = [LABEL_COLOR_MAP[l] for l in mat['cluster']]\n\nfig = plt.figure(figsize = (12,10))\nincrement = 0\nfor ix in range(6):\n    for iy in range(ix+1, 6):   \n        increment += 1\n        ax = fig.add_subplot(4,3,increment)\n        ax.scatter(mat[ix], mat[iy], c= label_color, alpha=0.5) \n        plt.ylabel('PCA {}'.format(iy+1), fontsize = 12)\n        plt.xlabel('PCA {}'.format(ix+1), fontsize = 12)\n        ax.yaxis.grid(color='lightgray', linestyle=':')\n        ax.xaxis.grid(color='lightgray', linestyle=':')\n        ax.spines['right'].set_visible(False)\n        ax.spines['top'].set_visible(False)\n        \n        if increment == 12: break\n    if increment == 12: break\n        \n#_______________________________________________\n# I set the legend: abreviation -> airline name\ncomp_handler = []\nfor i in range(n_clusters):\n    comp_handler.append(mpatches.Patch(color = LABEL_COLOR_MAP[i], label = i))\n\nplt.legend(handles=comp_handler, bbox_to_anchor=(1.1, 0.9), \n           title='Cluster', \n           shadow = True, frameon = True, framealpha = 1,\n           fontsize = 13, bbox_transform = plt.gcf().transFigure) #facecolor = 'lightgrey',\n\nplt.tight_layout()","a2972f7d":"sample_silhouette_values = silhouette_samples(scaled_matrix, clusters_clients)\n#____________________________________\n# define individual silhouette scores \nsample_silhouette_values = silhouette_samples(scaled_matrix, clusters_clients)\n#__________________\n# and do the graph\ngraph_component_silhouette(n_clusters, [-0.15, 0.55], len(scaled_matrix), sample_silhouette_values, \n                           clusters_clients)","327e5a26":"selected_customers.loc[:, 'cluster'] = clusters_clients","e0a3da32":"merged_df = pd.DataFrame()\nfor i in range(n_clusters):\n    test = pd.DataFrame(selected_customers[selected_customers['cluster'] == i].mean())\n    test = test.T.set_index('cluster', drop = True)\n    test['size'] = selected_customers[selected_customers['cluster'] == i].shape[0]\n    merged_df = pd.concat([merged_df, test])\n#_____________________________________________________\nmerged_df.drop('CustomerID', axis = 1, inplace = True)\nprint('number of customers:', merged_df['size'].sum())\n\nmerged_df = merged_df.sort_values('sum')","fd6a01cc":"liste_index = []\nfor i in range(5):\n    column = 'categ_{}'.format(i)\n    liste_index.append(merged_df[merged_df[column] > 45].index.values[0])\n\nliste_index_reordered = liste_index\nliste_index_reordered += [ s for s in merged_df.index if s not in liste_index]\n\nmerged_df = merged_df.reindex(index = liste_index_reordered)\nmerged_df = merged_df.reset_index(drop = False)\ndisplay(merged_df[['cluster', 'count', 'min', 'max', 'mean', 'sum', 'categ_0',\n                   'categ_1', 'categ_2', 'categ_3', 'categ_4', 'size']])","cd2f8f21":"def _scale_data(data, ranges):\n    (x1, x2) = ranges[0]\n    d = data[0]\n    return [(d - y1) \/ (y2 - y1) * (x2 - x1) + x1 for d, (y1, y2) in zip(data, ranges)]\n\nclass RadarChart():\n    def __init__(self, fig, location, sizes, variables, ranges, n_ordinate_levels = 6):\n\n        angles = np.arange(0, 360, 360.\/len(variables))\n\n        ix, iy = location[:] ; size_x, size_y = sizes[:]\n        \n        axes = [fig.add_axes([ix, iy, size_x, size_y], polar = True, \n        label = \"axes{}\".format(i)) for i in range(len(variables))]\n\n        _, text = axes[0].set_thetagrids(angles, labels = variables)\n        \n        for txt, angle in zip(text, angles):\n            if angle > -1 and angle < 181:\n                txt.set_rotation(angle - 90)\n            else:\n                txt.set_rotation(angle - 270)\n        \n        for ax in axes[1:]:\n            ax.patch.set_visible(False)\n            ax.xaxis.set_visible(False)\n            ax.grid(\"off\")\n        \n        for i, ax in enumerate(axes):\n            grid = np.linspace(*ranges[i],num = n_ordinate_levels)\n            grid_label = [\"\"]+[\"{:.0f}\".format(x) for x in grid[1:-1]]\n            ax.set_rgrids(grid, labels = grid_label, angle = angles[i])\n            ax.set_ylim(*ranges[i])\n        \n        self.angle = np.deg2rad(np.r_[angles, angles[0]])\n        self.ranges = ranges\n        self.ax = axes[0]\n                \n    def plot(self, data, *args, **kw):\n        sdata = _scale_data(data, self.ranges)\n        self.ax.plot(self.angle, np.r_[sdata, sdata[0]], *args, **kw)\n\n    def fill(self, data, *args, **kw):\n        sdata = _scale_data(data, self.ranges)\n        self.ax.fill(self.angle, np.r_[sdata, sdata[0]], *args, **kw)\n\n    def legend(self, *args, **kw):\n        self.ax.legend(*args, **kw)\n        \n    def title(self, title, *args, **kw):\n        self.ax.text(0.9, 1, title, transform = self.ax.transAxes, *args, **kw)\n","b456e84e":"#globa view of the content of each cluster\nfig = plt.figure(figsize=(50,50))\n\nattributes = ['count', 'mean', 'sum', 'categ_0', 'categ_1', 'categ_2', 'categ_3', 'categ_4']\nranges = [[0.01, 10], [0.01, 1500], [0.01, 10000], [0.01, 75], [0.01, 75], [0.01, 75], [0.01, 75], [0.01, 75]]\nindex  = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n\nn_groups = n_clusters ; i_cols = 3\ni_rows = n_groups\/\/i_cols\nsize_x, size_y = (1\/i_cols), (1\/i_rows)\n\nfor ind in range(n_clusters):\n    ix = ind%3 ; iy = i_rows - ind\/\/3\n    pos_x = ix*(size_x + 0.05) ; pos_y = iy*(size_y + 0.05)            \n    location = [pos_x, pos_y]  ; sizes = [size_x, size_y] \n    #______________________________________________________\n    data = np.array(merged_df.loc[index[ind], attributes])  \n    #print (data)\n    radar = RadarChart(fig, location, sizes, attributes, ranges)\n    radar.plot(data, color = 'b', linewidth=5.0)\n    radar.fill(data, alpha = 0.2, color = 'b')\n    radar.title(title = 'cluster n{}'.format(index[ind]), color = 'r')\n    ind += 1 ","fc994cb4":"class Class_Fit(object):\n    def __init__(self, clf, params=None):\n        if params:            \n            self.clf = clf(**params)\n        else:\n            self.clf = clf()\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def grid_search(self, parameters, Kfold):\n        self.grid = GridSearchCV(estimator = self.clf, param_grid = parameters, cv = Kfold)\n        \n    def grid_fit(self, X, Y):\n        self.grid.fit(X, Y)\n        \n    def grid_predict(self, X, Y):\n        self.predictions = self.grid.predict(X)\n        print(\"Precision: {:.2f} % \".format(100*metrics.accuracy_score(Y, self.predictions)))\n        ","f9b2064f":"selected_customers.head()\n\ncolumns = ['mean', 'categ_0', 'categ_1', 'categ_2', 'categ_3', 'categ_4' ]\nX = selected_customers[columns]\nY = selected_customers['cluster']","d61558a2":"X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, train_size = 0.8)","260eda56":"svc = Class_Fit(clf = svm.LinearSVC)\nsvc.grid_search(parameters = [{'C':np.logspace(-2,2,10)}], Kfold = 5)","e2ef3a5b":"#train the model\nsvc.grid_fit(X = X_train, Y = Y_train)","6c322923":"#prediction\nsvc.grid_predict(X_test, Y_test)","936a7e27":"def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.yticks(tick_marks, classes)\n   \n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    #_________________________________________________\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","7ff0f968":"class_names = [i for i in range(11)]\ncnf_matrix = confusion_matrix(Y_test, svc.predictions) \nnp.set_printoptions(precision=2)\nplt.figure(figsize = (8,8))\nplot_confusion_matrix(cnf_matrix, classes=class_names, normalize = False, title='Confusion matrix')","9089b61f":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 10)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","e4669ecc":"g = plot_learning_curve(svc.grid.best_estimator_,\n                        \"SVC learning curves\", X_train, Y_train, ylim = [1.01, 0.6],\n                        cv = 5,  train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5,\n                                                0.6, 0.7, 0.8, 0.9, 1])","ef2cfa71":"lr = Class_Fit(clf = linear_model.LogisticRegression)\nlr.grid_search(parameters = [{'C':np.logspace(-2,2,20)}], Kfold = 5)\nlr.grid_fit(X = X_train, Y = Y_train)\nlr.grid_predict(X_test, Y_test)","27cd9fd9":"#plot the curve\ng = plot_learning_curve(lr.grid.best_estimator_, \"Logistic Regression learning curves\", X_train, Y_train,\n                        ylim = [1.01, 0.7], cv = 5, \n                        train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])","d465ba89":"knn = Class_Fit(clf = neighbors.KNeighborsClassifier)\nknn.grid_search(parameters = [{'n_neighbors': np.arange(1,50,1)}], Kfold = 5)\nknn.grid_fit(X = X_train, Y = Y_train)\nknn.grid_predict(X_test, Y_test)\n\n","71cbfbde":"g = plot_learning_curve(knn.grid.best_estimator_, \"Nearest Neighbors learning curves\", X_train, Y_train,\n                        ylim = [1.01, 0.7], cv = 5, \n                        train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])","57be47a5":"tr = Class_Fit(clf = tree.DecisionTreeClassifier)\ntr.grid_search(parameters = [{'criterion' : ['entropy', 'gini'], 'max_features' :['sqrt', 'log2']}], Kfold = 5)\ntr.grid_fit(X = X_train, Y = Y_train)\ntr.grid_predict(X_test, Y_test)\n\ng = plot_learning_curve(tr.grid.best_estimator_, \"Decision tree learning curves\", X_train, Y_train,\n                        ylim = [1.01, 0.7], cv = 5, \n                        train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])","2d5bc970":"rf = Class_Fit(clf = ensemble.RandomForestClassifier)\nparam_grid = {'criterion' : ['entropy', 'gini'], 'n_estimators' : [20, 40, 60, 80, 100],\n               'max_features' :['sqrt', 'log2']}\nrf.grid_search(parameters = param_grid, Kfold = 5)\nrf.grid_fit(X = X_train, Y = Y_train)\nrf.grid_predict(X_test, Y_test)\n\n\ng = plot_learning_curve(rf.grid.best_estimator_, \"Random Forest learning curves\", X_train, Y_train,\n                        ylim = [1.01, 0.7], cv = 5, \n                        train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])","6013ad69":"ada = Class_Fit(clf = AdaBoostClassifier)\nparam_grid = {'n_estimators' : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]}\nada.grid_search(parameters = param_grid, Kfold = 5)\nada.grid_fit(X = X_train, Y = Y_train)\nada.grid_predict(X_test, Y_test)\n\ng = plot_learning_curve(ada.grid.best_estimator_, \"AdaBoost learning curves\", X_train, Y_train,\n                        ylim = [1.01, 0.4], cv = 5, \n                        train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])","ce913816":"gb = Class_Fit(clf = ensemble.GradientBoostingClassifier)\nparam_grid = {'n_estimators' : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]}\ngb.grid_search(parameters = param_grid, Kfold = 5)\ngb.grid_fit(X = X_train, Y = Y_train)\ngb.grid_predict(X_test, Y_test)\n\ng = plot_learning_curve(gb.grid.best_estimator_, \"Gradient Boosting learning curves\", X_train, Y_train,\n                        ylim = [1.01, 0.7], cv = 5, \n                        train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])","bb96c88f":"rf_best  = ensemble.RandomForestClassifier(**rf.grid.best_params_)\ngb_best  = ensemble.GradientBoostingClassifier(**gb.grid.best_params_)\nsvc_best = svm.LinearSVC(**svc.grid.best_params_)\ntr_best  = tree.DecisionTreeClassifier(**tr.grid.best_params_)\nknn_best = neighbors.KNeighborsClassifier(**knn.grid.best_params_)\nlr_best  = linear_model.LogisticRegression(**lr.grid.best_params_)","9403d17c":"votingC = ensemble.VotingClassifier(estimators=[('rf', rf_best),('gb', gb_best),\n                                                ('knn', knn_best)], voting='soft')  ","90bf32fe":"#lets train the model\nvotingC = votingC.fit(X_train, Y_train)","71295cb8":"#lets make prediction now\npredictions = votingC.predict(X_test)\nprint(\"Precision: {:.2f} % \".format(100*metrics.accuracy_score(Y_test, predictions)))","d1e7a8e5":"basket_price = set_test.copy(deep = True)","b5366a74":"transactions_per_user=basket_price.groupby(by=['CustomerID'])['Basket Price'].agg(['count','min','max','mean','sum'])\nfor i in range(5):\n    col = 'categ_{}'.format(i)\n    transactions_per_user.loc[:,col] = basket_price.groupby(by=['CustomerID'])[col].sum() \/\\\n                                            transactions_per_user['sum']*100\n\ntransactions_per_user.reset_index(drop = False, inplace = True)\nbasket_price.groupby(by=['CustomerID'])['categ_0'].sum()\n\n#_______________________\n# Correcting time range\ntransactions_per_user['count'] = 5 * transactions_per_user['count']\ntransactions_per_user['sum']   = transactions_per_user['count'] * transactions_per_user['mean']\n\ntransactions_per_user.sort_values('CustomerID', ascending = True)[:5]","daa09342":"list_cols = ['count','min','max','mean','categ_0','categ_1','categ_2','categ_3','categ_4']\n#_____________________________________________________________\nmatrix_test = transactions_per_user[list_cols].as_matrix()\nscaled_test_matrix = scaler.transform(matrix_test)","3b2ec378":"Y = kmeans.predict(scaled_test_matrix)","a444aa0e":"columns = ['mean', 'categ_0', 'categ_1', 'categ_2', 'categ_3', 'categ_4' ]\nX = transactions_per_user[columns]","e55273dd":"classifiers = [(svc, 'Support Vector Machine'),\n                (lr, 'Logostic Regression'),\n                (knn, 'k-Nearest Neighbors'),\n                (tr, 'Decision Tree'),\n                (rf, 'Random Forest'),\n                (gb, 'Gradient Boosting')]\n#______________________________\nfor clf, label in classifiers:\n    print(30*'_', '\\n{}'.format(label))\n    clf.grid_predict(X, Y)","de1c9419":"predictions = votingC.predict(X)\nprint(\"Precision: {:.2f} % \".format(100*metrics.accuracy_score(Y, predictions)))","7ec87907":"Finally, as anticipated in revised approach, it is possible to improve the quality of the classifier by combining their respective predictions. At this level, we chose to mix Random Forest, Gradient Boosting and k-Nearest Neighbors predictions because this leads to a slight improvement in predictions:","6610fabc":"In order to have a global view of the type of order performed in this dataset, I determine how the purchases are divided according to total prizes:\n\n","f99d156c":"## Defining product categories\n\nThe list that was obtained contains more than 1400 keywords and the most frequent ones appear in more than 200 products. However, while examining the content of the list, I note that some names are useless. Others are do not carry information, like colors. Therefore, I discard these words from the analysis that follows and also, I decide to consider only the words that appear more than 13 times.","2e6f09e3":"Note:\n\n1. If you are looking to the CustomerID column then there are  \u223c 25% data entries are\nnull.\n2. That means there are  \u223c 25% of data entries which aren't assigned to the any customer(s).\n3. It is impossible for us to map values for the customer and these entries. These is usless for the current exercise.\n4. Because of all the above points we are deleting these data entries.","264774a5":"Using it, Lets create a representation of the most common keywords:","777340cf":"It can be seen, for example, that the first 5 clusters correspond to a strong preponderance of purchases in a particular category of products. Other clusters will differ from basket averages (** mean ), the total sum spent by the clients ( sum ) or the total number of visits made ( count **).","20e593ae":"___\n#### Confusion matrix\n\nThe accuracy of the results seems to be correct. Nevertheless, let us remember that when the different classes were defined, there was an imbalance in size between the classes obtained. In particular, one class contains around 40% of the clients. It is therefore interesting to look at how the predictions and real values compare to the breasts of the different classes. This is the subject of the confusion matrices and to represent them, I use the code of the [sklearn documentation](http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html):","edba41c8":"-----\n#### Creating customer categories\n\nAt this point, I define clusters of clients from the standardized matrix that was defined earlier and using the `k-means` algorithm from` scikit-learn`. I choose the number of clusters based on the silhouette score and I find that the best score is obtained with 11 clusters:","9a3f0716":"Then, lets define a classifier that merges the results of the various classifiers:","d9b34aee":" check for the amount of variance explained by each component:","26146c27":"\n\nIn order to have an insight on the quality of the classification, we can represent the silhouette scores of each element of the different clusters. This is the purpose of the next figure which is taken from the [sklearn documentation](http:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_kmeans_silhouette_analysis.html):","ae260656":"In the following, I will create clusters of customers. In practice, before creating these clusters, it is interesting to define a base of smaller dimension allowing to describe the scaled_matrix matrix. In this case, I will use this base in order to create a representation of the different clusters and thus verify the quality of the separation of the different groups. I therefore perform a PCA beforehand","dc923d2d":"Finally, I define two additional variables that give the number of days elapsed since the first purchase (** FirstPurchase ) and the number of days since the last purchase ( LastPurchase **):","9e3c7dd2":"The execution of this function returns three variables:\n\nkeywords: the list of extracted keywords\n\nkeywords_roots: a dictionary where the keys are the keywords roots and the values are the lists of words associated with those roots\n\ncount_keywords: dictionary listing the number of times every word is used\n\nAt this point, Lets convert the count_keywords dictionary into a list, to sort the keywords according to their occurrence:","a162bf6e":"Note that the number of cancelled transactions are quite large (  \u223c 16% of the total number of transactions).\n\nNow, let's look at the first few lines of the dataframe}","68ddd453":"Then, we convert the dataframe into a matrix and retain only variables that define the category to which consumers belong. At this level, I recall the method of normalization that had been used on the training set:","53d01568":"### which one is the best model?","1c6ba0a3":"## Analysis of the product categories\n\nIn the data-frame, products are uniquely identified through the StockCode variable. A short description of the products is given in the Description variable. In this section, I intend to use the content of this latter variable in order to group the products into different categories.\n\n## Products Description\n\nAs a first step, I extract from the Description variable the information that will prove useful. To do this, I use the following function:","25370201":"## word clouds\nNow we can have a look at the type of objects that each cluster represents. In order to obtain a global view of their contents, I determine which keywords are the most frequent in each of them","fe379af2":"Then, I average the contents of this dataframe by first selecting the different groups of clients. This gives access to, for example, the average baskets price, the number of visits or the total sums spent by the clients of the different clusters. I also determine the number of clients in each group (variable ** size **):","6f08a053":"From this representation, it can be seen, for example, that the first principal component allow to separate the tiniest clusters from the rest. More generally, we see that there is always a representation in which two clusters will appear to be distinct.\n\n** b\/ _Score of the silhouette intra-cluster_ **\n\nAs with product categories, another way to look at the quality of the separation is to look at silouhette scores within different clusters:","5cd7b46b":"Once more, we find that the initial hypothesis is not verified. Hence, cancellations do not necessarily correspond to orders that would have been made beforehand.\n\nAt this point, I decide to create a new variable in the dataframe that indicate if part of the command has been canceled. For the cancellations without counterparts, a few of them are probably due to the fact that the buy orders were performed before December 2010 (the point of entry of the database). Below, I make a census of the cancel orders and check for the existence of counterparts:","ae4a1296":"#### Grouping products\n\nIn a second step, I decide to create the **categ_N** variables (with $ N \\in [0: 4]$) that contains the amount spent in each product category:","9b5364e2":"Once this list is created, I use the function I previously defined in order to analyze the description of the various products:","2ddc175b":"## Data encoding\nNow we will use these keywords to create groups of product. Firstly, lets define the  X  matrix as:\n\n\n \n|   | word  1  |  ...  | word j  | ...  | word N  |\n|:-:|---|---|---|---|---|\n| product 1  | $a_{1,1}$  |     |   |   | $a_{1,N}$  |\n| ...        |            |     | ...  |   |   |\n|product i   |    ...     |     | $a_{i,j}$    |   | ...  |\n|...         |            |     |  ... |   |   |\n| product M  | $a_{M,1}$  |     |   |   | $a_{M,N}$   |\n\n\n\nwhere the $a_ {i, j}$ coefficient  is 1 if the description of the product $i$ contains the word $j$, and 0 otherwise.","d06cc9a4":"We see that the initial hypothesis is not fulfilled because of the existence of a 'Discount' entry. I check again the hypothesis but this time discarding the 'Discount' entries:","a8cf78a6":"## PCA\n\nFrom this representation, we can see that for example, one of the clusters contains objects that could be associated with gifts (keywords: Christmas, packaging, card, ...). Another cluster would rather contain luxury items and jewelry (keywords: necklace, bracelet, lace, silver, ...). Nevertheless, it can also be observed that many words appear in various clusters and it is therefore difficult to clearly distinguish them.\n\n** c: Principal Component Analysis **\n\nIn order to ensure that these clusters are truly distinct, I look at their composition. Given the large number of variables of the initial matrix, I first perform a PCA:","606170c0":"If one looks, for example, at the purchases of the consumer of one of the above entries and corresponding to the same product as that of the cancellation, one observes:","051e4b98":"As you can see that this dataset contain the recods of 4372 users who bought 3684 different items. There are  \u223c 22,000 transactions which are carried out.\n\nNow we need to explore the number of products purchased in every transaction","177cd1eb":"### Formatting data\n\nIn the previous section, the different products were grouped in five clusters. In order to prepare the rest of the analysis, a first step consists in introducing this information into the dataframe. To do this, I create the categorical variable **categ_product** where I indicate the cluster of each product :","9ebbeaab":"Since the goal is to define the class to which a client belongs and this, as soon as its first visit, I only keep the variables that describe the content of the basket, and do not take into account the variables related to the frequency of visits or variations of the basket price over time:","49c4233d":"In the above function, I checked the two cases:\n\n1. a cancel order exists without counterpart\nthere's at least one counterpart with the exact same quantity\n2. The index of the corresponding cancel order are respectively kept in the doubtfull_entry and entry_to_remove lists whose sizes are:","14fdee14":"We see that the number of components required to explain the data is extremely important: we need more than 100 components to explain 90% of the variance of the data. In practice, we decide to keep only a limited number of components since this decomposition is only performed to visualize the data:","df540409":"** d \/ _Customers morphology garphical representation_ **\n\nFinally, I created a representation of the different morphotypes. To do this, I define a class to create \"Radar Charts\" (which has been adapted from this [kernel](https:\/\/www.kaggle.com\/yassineghouzam\/don-t-know-why-employees-leave -read-this)):","2b5af361":"## Revised approach\n\n### Logistic Regression\n\nI now consider the logistic regression classifier. As before, I create an instance of the `Class_Fit` class, adjust the model on the training data and see how the predictions compare to the real values:","85acdf36":"We see that the quantity canceled is greater than the sum of the previous purchases.\n\n## Analysis of the StockCode\n\nbove, it has been seen that some values of the ** StockCode ** variable indicate a particular transaction (i.e. D for Discount). I check the contents of this variable by looking for the set of codes that would contain only letters:","d74fdc82":"Each entry of the dataframe indicates prizes for a single kind of product. Hence, orders are split on several lines. I collect all the purchases made during a single order to recover the total order prize:","995f4725":"On this curve, we can see that the train and cross-validation curves converge towards the same limit when the sample size increases. This is typical of modeling with low variance and proves that the model does not suffer from overfitting. Also, we can see that the accuracy of the training curve is correct which is synonymous of a low bias. Hence the model does not underfit the data.","e028a412":"### Random Forest","bf9cea48":"Finally, in order to prepare the execution of the classifier, it is sufficient to select the variables on which it acts:","c36a7f79":"## Finally, lets split the dataset in train and test sets:","4002c5d3":"###  Gradient Boosting Classifier","7f779ef7":"The  X  matrix indicates the words contained in the description of the products using the one-hot-encoding principle.\n\nIn practice, I have found that introducing the price range results in more balanced groups in terms of element numbers.\n\nHence, I add 6 extra columns to this matrix, where I indicate the price range of the products:","d7029eb7":"Points to look carefully here:\n\n1. There are some users who bought only comes one time on the E-commerce platform and purchased one\nitem. The example of this kind of user is customerID 12346.\n\n2. There are some users who frequently buy large number of items per order. The example of this kind of user is customerID 12347.\n\n3. If you notice Invoiceno data attribute then you can find out that there is prefix 'C' for one invoice. This 'C' indicates that the particular transaction has been cancelled.","69ac852c":"This function takes as input the dataframe and analyzes the content of the Description column by performing the following operations:\n\n-extract the names (proper, common) appearing in the products description\n\n-for each name, I extract the root of the word and aggregate the set of names associated with this particular root\n\n-count the number of times each root appears in the dataframe\n\n-when several words are listed for the same root, I consider that the keyword associated with this root is the shortest name (this systematically selects the singular when there are singular\/plural variants\n\n\nThe first step of the analysis is to retrieve the list of products:","31983d42":"In practice, the different variables I selected have quite different ranges of variation and before continuing the analysis, I create a matrix where these data are standardized","4e5c3b78":"At first, lets look at the number of customers in each cluster:","472e0318":"#### Splitting of data over time\n\nThe dataframe `basket_price` contains information for a period of 12 months. Later, one of the objectives will be to develop a model capable of characterizing and anticipating the habits of the customers visiting the site and this, from their first visit. In order to be able to test the model in a realistic way, I split the data set by retaining the first 10 months to develop the model and the following two months to test it:","024f36a1":"Up to now, the information related to a single order was split over several lines of the dataframe (one line per product). I decide to collect the information related to a particular order and put in in a single entry. I therefore create a new dataframe that contains, for each order, the amount of the basket, as well as the way it is distributed over the 5 categories of products:","6ad2d42f":"Finally, the results of the different classifiers presented in the previous sections can be combined to improve the classification model. This can be achieved by selecting the customer category as the one indicated by the majority of classifiers. To do this, I use the VotingClassifier method of the sklearn package. As a first step, I adjust the parameters of the various classifiers using the best parameters previously found:","7070aaa3":"# Customer categories\n\nSteps for generatin\n\n* Formatting data \n  * Grouping products  \n  * Splitting of the dataset \n  * Grouping orders  \n* Creating customer categories \n  * Data encoding \n  * Creating categories ","b0935d16":"In practice, the scores obtained above can be considered equivalent since, depending on the run, scores of $ 0.1 \\pm 0.05 $ will be obtained for all clusters with `n_clusters` $> $ 3 (we obtain slightly lower scores for the first cluster). On the other hand, I found that beyond 5 clusters, some clusters contained very few elements. I therefore choose to separate the dataset into 5 clusters. In order to ensure a good classification at every run of the notebook, I iterate untill we obtain the best possible silhouette score, which is, in the present case, around 0.15:","407788ca":"We see that there are several types of peculiar transactions, connected e.g. to port charges or bank charges.\n\n## Analysis of Basket Price\n\nLets create a new variable that indicates the total price of every purchase:","c28dc717":"t can be seen that the vast majority of orders concern relatively large purchases given that  \u223c 65% of purchases give prizes in excess of \u00a3 200.","567dd8a7":"\n----------------\n#### Learning curve\n\nA typical way to test the quality of a fit is to draw a learning curve. In particular, this type of curves allow to detect possible drawbacks in the model, linked for example to over- or under-fitting. This also shows to which extent the mode could benefit from a larger data sample. In order to draw this curve, I use the [scikit-learn documentation code again](http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_learning_curve.html#sphx-glr- self-examples-model-selection-pad-learning-curve-py)","ec95ecbf":"----\n### k-Nearest Neighbors","fb7946f6":"A customer category of particular interest is that of customers who make only one purchase. One of the objectives may be, for example, to target these customers in order to retain them. In part, I find that this type of customer represents 1\/3 of the customers listed:","dabb5f47":"### AdaBoost Classifier","6177d064":"## Characterizing the content of clusters\nWe check the number of elements in every class:","a0173358":"Each line in this matrix contains a consumer's buying habits. At this stage, it is a question of using these habits in order to define the category to which the consumer belongs. These categories have been established in Section 4. ** At this stage, it is important to bear in mind that this step does not correspond to the classification stage itself**. Here, we prepare the test data by defining the category to which the customers belong. However, this definition uses data obtained over a period of 2 months (via the variables ** count **, ** min **, ** max ** and ** sum **). The classifier defined in Section 5 uses a more restricted set of variables that will be defined from the first purchase of a client.\n\nHere it is a question of using the available data over a period of two months and using this data to define the category to which the customers belong. Then, the classifier can be tested by comparing its predictions with these categories. In order to define the category to which the clients belong, I recall the instance of the `kmeans` method used in section 4. The` predict` method of this instance calculates the distance of the consumers from the centroids of the 11 client classes and the smallest distance will define the belonging to the different categories:","6fcad251":"Among these entries, the lines listed in the doubtfull_entry list correspond to the entries indicating a cancellation but for which there is no command beforehand. In practice, I decide to delete all of these entries, which count respectively for  \u223c  1.4% and 0.2% of the dataframe entries.\n\nNow I check the number of entries that correspond to cancellations and that have not been deleted with the previous filter:","8bcd6a90":"In a first step, we regroup reformattes these data according to the same procedure as used on the training set. However, I am correcting the data to take into account the difference in time between the two datasets and weights the variables ** count ** and ** sum ** to obtain an equivalence with the training set:","c2512573":"** a \/ _Report via the PCA_ **\n\nThere is a certain disparity in the sizes of different groups that have been created. Hence I will now try to understand the content of these clusters in order to validate (or not) this particular separation. At first, I use the result of the PCA:","4f29e1f8":"#### Consumer Order Combinations\n\nIn a second step, I group together the different entries that correspond to the same user. I thus determine the number of purchases made by the user, as well as the minimum, maximum, average amounts and the total amount spent during all the visits:","a50f33b5":"#### Data encoding\n\nThe dataframe `transactions_per_user` contains a summary of all the commands that were made. Each entry in this dataframe corresponds to a particular client. I use this information to characterize the different types of customers and only keep a subset of variables:","74518c42":"We need to count the number of transactions corresponding to cancelled orders","5433c481":"From the above output, we see that when an order is canceled, we have another transactions in the dataframe, mostly identical except for the Quantity and InvoiceDate variables. I decide to check if this is true for all the entries. To do this, I decide to locate the entries that indicate a negative quantity and check if there is systematically an order indicating the same quantity (but positive), with the same description (CustomerID, Description and UnitPrice):","53f7e80c":"and to choose the appropriate ranges, I check the number of products in the different groups:","3aacabe6":"___\n### Support Vector Machine Classifier (SVC)\n\nThe first classifier I use is the SVC classifier. In order to use it, I create an instance of the `Class_Fit` class and then call` grid_search()`. When calling this method, I provide as parameters:\n- the hyperparameters for which I will seek an optimal value\n- the number of folds to be used for cross-validation","d81b2c81":"Note that when defining the votingC classifier, I only used a sub-sample of the whole set of classifiers defined above and only retained the Random Forest, the k-Nearest Neighbors and the Gradient Boosting classifiers. In practice, this choice has been done with respect to the performance of the classification carried out in the next section.","7ee944df":"** c\/ Customers morphotype**\n\nAt this stage, I have verified that the different clusters are indeed disjoint (at least, in a global way). It remains to understand the habits of the customers in each cluster. To do so, I start by adding to the selected_customers dataframe a variable that defines the cluster to which each client belongs:","296fa8a4":"in order to create a representation of the various clusters:","f47c8459":"### Decision Tree","8e767eb3":"Finally, I re-organize the content of the dataframe by ordering the different clusters: first, in relation to the amount wpsent in each product category and then, according to the total amount spent:","e4fda53a":"It can be seen, for example, that the first 5 clusters correspond to a strong preponderance of purchases in a particular category of products. Other clusters will differ from basket averages (** mean **), the total sum spent by the clients (** sum **) or the total number of visits made (** count **).\n\n____\n\n## Classification of customers\n\nIn this part, the objective will be to adjust a classifier that will classify consumers in the different client categories that were established in the previous section. The objective is to make this classification possible at the first visit. To fulfill this objective, I will test several classifiers implemented in `scikit-learn`. First, in order to simplify their use, I define a class that allows to interface several of the functionalities common to these different classifiers: ","296cc1be":"## Creating clusters of products\n\nIn this section, we will group the products into different classes. In the case of matrices with binary encoding, the most suitable metric for the calculation of distances is the Hamming's metric. Note that the kmeans method of sklearn uses a Euclidean distance that can be used, but it is not to the best choice in the case of categorical variables. However, in order to use the Hamming's metric, we need to use the kmodes package which is not available on the current plateform. Hence, we use the kmeans method even if this is not the best choice.\n\nIn order to define (approximately) the number of clusters that best represents the data, we use the silhouette score:","3b69a34d":"---\n### Creation of customers categories","9a030d80":"Note that when defining the `votingC` classifier, I only used a sub-sample of the whole set of classifiers defined above and only retained the *Random Forest*, the *k-Nearest Neighbors* and the *Gradient Boosting* classifiers. In practice, this choice has been done with respect to the performance of the classification carried out in the next section.\n\n___\n### Testing predictions\n\nIn the previous section, a few classifiers were trained in order to categorize customers. Until that point, the whole analysis was based on the data of the first 10 months. In this section, we test the model  the last two months of the dataset, that has been stored in the `set_test` dataframe:","5fc0d5bd":"It remains only to examine the predictions of the different classifiers that have been trained earlier"}}