{"cell_type":{"4c2193e0":"code","637db80e":"code","fa5bd0f5":"code","f36eda25":"code","72a19e7e":"code","d60a580b":"code","f6bda64b":"code","a65b53b6":"code","7c6f56be":"code","6d53266a":"code","44247330":"code","98b3085a":"code","64951755":"code","dfe75919":"code","3ad8442a":"code","7d651613":"code","47123fc4":"code","39ff42f8":"code","a4e1c10e":"code","7d49253d":"code","80ad4ddd":"code","c029e3ab":"code","1ab506e5":"code","89a545da":"code","bd1402fa":"markdown"},"source":{"4c2193e0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","637db80e":"import gc\nimport warnings\nwarnings.filterwarnings('ignore')","fa5bd0f5":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import models\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import optimizers","f36eda25":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df\n","72a19e7e":"train = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/test.csv')","d60a580b":"train = reduce_memory_usage(train)\ntest = reduce_memory_usage(test)","f6bda64b":"# train.drop(['id', 'target'], axis=1).info()","a65b53b6":"x_data = train.drop(['id', 'target'], axis=1)\ny_data = train.target\nx_test = test.drop('id', axis=1)\n\nx_data = x_data.values\nx_test = x_test.values\n\n\ndel train, test\ngc.collect()","7c6f56be":"# tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# # instantiate a distribution strategy\n# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","6d53266a":"from sklearn.decomposition import PCA","44247330":"pca = PCA(n_components=256)\n\nx_data_image = pca.fit_transform(x_data)\nx_test_image = pca.transform(x_test)","98b3085a":"x_data_image = x_data_image.reshape((-1, 16, 16, 1))\nx_test_image = x_test_image.reshape((-1, 16, 16, 1))","64951755":"del x_data, x_test\ngc.collect()","dfe75919":"model = 0\ndef model_create():\n    \n    model = models.Sequential()\n    model.add(layers.Conv2D(512, (3, 3), activation='relu', input_shape=(16, 16, 1), padding='same'))\n    # model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n    model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n    # model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(32, activation='relu'))\n    model.add(layers.Dense(1, activation='sigmoid'))\n    model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model \n    \n# model.summary()","3ad8442a":"# # with tpu_strategy.scope():\n# model = models.Sequential()\n# model.add(layers.Dense(1000, activation='relu', input_shape=(285,)))\n# model.add(layers.Dense(900, activation='relu'))\n# model.add(layers.Dense(800, activation='relu'))\n# model.add(layers.Dense(700, activation='relu'))\n# model.add(layers.Dense(600, activation='relu'))\n# model.add(layers.Dense(500, activation='relu'))\n# model.add(layers.Dense(100, activation='relu'))\n# model.add(layers.Dense(1, activation='sigmoid'))\n# model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])","7d651613":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\n\nn_split = 10\nkfold = KFold(n_split)\n\nval_pred = np.zeros(y_data.shape)\ny_test = np.zeros((x_test_image.shape[0],))\n\nfor i, (train_index, val_index) in enumerate(kfold.split(x_data_image)):\n    gc.collect()\n    # train model\n    print(\"fold {} training\".format(i))\n    model = model_create()\n    model.fit(x_data_image[train_index], y_data.iloc[train_index], batch_size=512, epochs=5)\n    \n    # predict val and test\n    val_pred[val_index] = model.predict(x_data_image[val_index]).reshape((-1,))\n    vla_score = roc_auc_score(y_data.iloc[val_index], val_pred[val_index])\n    print(\"fold {} validation auc score {}\".format(i, vla_score))\n    \n    y_test += model.predict(x_test_image).reshape((-1,)) \/ n_split\n    \n# evaluate validation score    \nprint(\"val auc score :\", roc_auc_score(y_data, val_pred))","47123fc4":"# from sklearn.model_selection import train_test_split\n# x_train, x_val, y_train, y_val = train_test_split(x_data_image, y_data, test_size=0.3, random_state=50)","39ff42f8":"# print(x_train.shape)","a4e1c10e":"# model.fit(x_train, y_train, validation_data = (x_val, y_val), batch_size=512, epochs=5)","7d49253d":"from sklearn.metrics import roc_auc_score","80ad4ddd":"# y_val_pred = model.predict(x_val)\n# roc_auc_score(y_val, y_val_pred)","c029e3ab":"# y_pred = model.predict(x_test_image)","1ab506e5":"submission = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv')\nsubmission.target = y_test\nsubmission.to_csv('submission.csv', index=False)","89a545da":"import seaborn as sns\nsns.histplot(y_test)","bd1402fa":"### cnn"}}