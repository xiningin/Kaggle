{"cell_type":{"bbe184a6":"code","f819c2aa":"code","dce4dec3":"code","7a222424":"code","e8f60938":"code","1e11955d":"code","5bd9a190":"code","0517be60":"code","3cd26e88":"code","bfc193fe":"code","3c71e99f":"code","2f4fde08":"code","777b3d74":"code","4a2832ba":"code","4a4b2588":"code","f5cb6ee8":"code","708b5fc6":"code","05f86594":"code","648ff891":"code","9851c3bd":"code","1d3ea90b":"code","8b6a1f5a":"code","300c95f4":"code","429eabcf":"code","8e24659e":"code","7712962f":"code","629a7022":"code","84f2c3a3":"code","14f8eefa":"code","935ec5af":"code","6faef4e1":"code","9ec4a823":"code","9278ab56":"code","877d18c0":"code","381bd882":"code","6740e458":"code","914a086c":"code","6fb0cf48":"code","f749e51e":"code","639e90b2":"code","01f30dfe":"code","d9b12db9":"code","4df9811b":"code","6433a355":"code","736f6aac":"markdown","912c8401":"markdown","7db42807":"markdown","4bb1e361":"markdown","808458bd":"markdown","0e400232":"markdown","6741a79a":"markdown","276bfb61":"markdown","0fa7b053":"markdown","a2166243":"markdown","a27d15b5":"markdown","a7c30249":"markdown","f8b30d04":"markdown","113da0ec":"markdown","26a4c483":"markdown","83968f7b":"markdown","018c0e33":"markdown","fc6323bc":"markdown","49236eee":"markdown","c8bc564c":"markdown","253055fa":"markdown","dfb64ecc":"markdown"},"source":{"bbe184a6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import scale\nfrom tensorflow.keras import backend as K, callbacks\nfrom sklearn.metrics import accuracy_score as accuracy, f1_score, mean_absolute_error as mae\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D\nfrom tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\nfrom keras.utils.vis_utils import plot_model\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f819c2aa":"dataset_NASDAQ = pd.read_csv(\"\/kaggle\/input\/cnnpred-stock-market-prediction\/Processed_NASDAQ.csv\", parse_dates=['Date'])\ndataset_NYSE = pd.read_csv(\"\/kaggle\/input\/cnnpred-stock-market-prediction\/Processed_NYSE.csv\", parse_dates=['Date'])\ndataset_SP = pd.read_csv(\"\/kaggle\/input\/cnnpred-stock-market-prediction\/Processed_SP.csv\", parse_dates=['Date'])\ndataset_DJI = pd.read_csv(\"\/kaggle\/input\/cnnpred-stock-market-prediction\/Processed_DJI.csv\", parse_dates=['Date'])\ndataset_RUSSELL = pd.read_csv(\"\/kaggle\/input\/cnnpred-stock-market-prediction\/Processed_RUSSELL.csv\", parse_dates=['Date'])","dce4dec3":"dataset_NASDAQ.head(5)","7a222424":"dataset_NASDAQ['Date']","e8f60938":"dataset_NASDAQ.index = dataset_NASDAQ['Date']\ndataset_NYSE.index = dataset_NYSE['Date']\ndataset_SP.index = dataset_SP['Date']\ndataset_DJI.index = dataset_DJI['Date']\ndataset_RUSSELL.index = dataset_RUSSELL['Date']","1e11955d":"dataset_NASDAQ.index","5bd9a190":"dataset_NASDAQ['Date']","0517be60":"dataset_NASDAQ.columns","3cd26e88":"# del dataset_NASDAQ['Name'] # Deletes \"Name\" column","bfc193fe":"dataset_NASDAQ.columns","3c71e99f":"dataset_NASDAQ","2f4fde08":"print(dataset_NASDAQ.shape)\nprint(dataset_NYSE.shape)\nprint(dataset_SP.shape)\nprint(dataset_DJI.shape)\nprint(dataset_RUSSELL.shape)","777b3d74":"whole_data = dataset_NASDAQ.append(dataset_NYSE, ignore_index=True)","4a2832ba":"whole_data","4a4b2588":"whole_data = whole_data.append(dataset_SP, ignore_index=True)\nwhole_data = whole_data.append(dataset_DJI, ignore_index=True)\nwhole_data = whole_data.append(dataset_RUSSELL, ignore_index=True)\nwhole_data","f5cb6ee8":"whole_data[\"Close\"]","708b5fc6":"print(len(dataset_NASDAQ ))\nprint(len(dataset_NYSE))\nprint(len(dataset_SP))\nprint(len(dataset_DJI ))\nprint(len(dataset_RUSSELL))","05f86594":"whole_data.isnull().sum()","648ff891":"whole_data.info()","9851c3bd":"import matplotlib.pyplot as plt\nwhole_data.hist(figsize=(30,30))\nplt.show()","1d3ea90b":"def cnn_data_sequence(data, target, seque_len):\n    print ('sequencing data ...')\n    new_train = []\n    new_target = []\n\n    for index in range(data.shape[1] - seque_len + 1):\n        new_train.append(data[:, index: index + seque_len])\n        new_target.append(target[index + seque_len - 1])\n\n    new_train = np.array(new_train)\n    new_target = np.array(new_target)\n\n    return new_train, new_target","8b6a1f5a":"# Remember: we defined our indexes as our dates.\nprint(dataset_NASDAQ.Close[0])\nprint(\"--------\")\ndataset_NASDAQ.Close[0:5]","300c95f4":"# (data['Close'][predict_day:] \/ data['Close'][:-predict_day].values).astype(int)\n(dataset_NASDAQ['Close'][1:] \/ dataset_NASDAQ['Close'][:-1].values).astype(int) # -> Bu k\u0131sma bir daha bak","429eabcf":"predict_index = \"RUT\"  #  -> Change this part to see the results. # RUT, S&P, NYA, NASDAQ, DJI\nnumber_of_stocks = 0\norder_stocks = []\npredict_day = 1\n\n\ndef prepare_for_CNN():\n    global number_of_stocks\n    global samples_in_each_stock\n    global number_feature\n    #global predict_index\n    global order_stocksw\n    tottal_train_data = np.empty((0,82))  # column number is 82\n    tottal_train_target = np.empty((0))\n    tottal_test_data = np.empty((0,82))\n    tottal_test_target = np.empty((0))\n    \n    for data in [dataset_DJI, dataset_NASDAQ, dataset_NYSE, dataset_RUSSELL, dataset_SP]:\n        \n        number_of_stocks += 1\n        \n        df_name = data['Name'][0]\n        order_stocks.append(df_name)\n        del data['Name']                # deletes \"Name\" column\n\n        target = (data['Close'][predict_day:] \/ data['Close'][:-predict_day].values).astype(int)\n        print(target)   # (0's or 1's.)\n        print(\"*****\")\n        data = data[:-predict_day]\n        target.index = data.index\n        # Becasue of using 200 days Moving Average as one of the features\n        data = data[200:]                         # Bu k\u0131sma bir daha bak\n        data = data.fillna(0)\n        data['target'] = target\n        target = data['target']\n        del data['target']\n        del data['Date']\n        # data['Date'] = data['Date'].apply(lambda x: x.weekday())\n\n        number_feature = data.shape[1]\n        samples_in_each_stock = data.shape[0]\n\n        train_data = data[data.index < '2016-04-21']\n        train_data = scale(train_data)\n        \n        print(\"train data:\", train_data)\n        \n        if df_name == predict_index:\n            tottal_train_target = target[target.index < '2016-04-21']\n            tottal_test_target = target[target.index >= '2016-04-21']\n\n        data = pd.DataFrame(scale(data.values), columns=data.columns)\n        data.index = target.index\n        test_data = data[data.index >= '2016-04-21']\n\n        tottal_train_data = np.concatenate((tottal_train_data, train_data))\n        print(tottal_train_data.shape)\n        tottal_test_data = np.concatenate((tottal_test_data, test_data))\n        print(tottal_test_data.shape)\n        \n    print(\"order_stocks:\", order_stocks)\n        \n    train_size = int(tottal_train_data.shape[0]\/number_of_stocks)\n    print(\"Train size:\", train_size)\n    test_size = int(tottal_test_data.shape[0] \/ number_of_stocks)\n    print(\"Test size:\", test_size)\n    \n    tottal_train_data = tottal_train_data.reshape(number_of_stocks, train_size, number_feature)\n    print(\"Total train data shape:\", tottal_train_data.shape)\n    tottal_test_data = tottal_test_data.reshape(number_of_stocks, test_size, number_feature) \n    print(\"Total test data shape:\", tottal_test_data.shape)\n    \n    print(\"tottal_train_target:\", tottal_train_target, \"shape :\", tottal_train_target.shape)\n    print(\"tottal_test_target: \", tottal_test_target, \"shape:\", tottal_test_target.shape)\n\n    return tottal_train_data, tottal_test_data, tottal_train_target, tottal_test_target","8e24659e":"def f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    precision_pos = precision(y_true, y_pred)\n    recall_pos = recall(y_true, y_pred)\n    precision_neg = precision((K.ones_like(y_true)-y_true), (K.ones_like(y_pred)-K.clip(y_pred, 0, 1)))\n    recall_neg = recall((K.ones_like(y_true)-y_true), (K.ones_like(y_pred)-K.clip(y_pred, 0, 1)))\n    f_posit = 2*((precision_pos*recall_pos)\/(precision_pos+recall_pos+K.epsilon()))\n    f_neg = 2 * ((precision_neg * recall_neg) \/ (precision_neg + recall_neg + K.epsilon()))\n\n    return (f_posit + f_neg) \/ 2\n\ndef sklearn_acc(model, test_data, test_target):\n    overall_results = model.predict(test_data)\n    test_pred = (overall_results > 0.5).astype(int)\n    acc_results = [mae(overall_results, test_target), accuracy(test_pred, test_target),\n                   f1_score(test_pred, test_target, average='macro')]\n\n    return acc_results","7712962f":"number_filter = [8,8,8]\n\ndef CNN(train_data, test_data, train_target, test_target):\n    # hisory of data in each sample\n    seq_len = 60\n    epoc = 100\n    drop = 0.1\n    \n    cnn_train_data, cnn_train_target = cnn_data_sequence(train_data, train_target, seq_len)\n    #print(\"cnn_train_data:\", cnn_train_data.shape)\n    #print(\"cnn_train_target:\", cnn_train_target.shape)\n    \n    cnn_test_data, cnn_test_target = cnn_data_sequence(test_data, test_target, seq_len)\n    #print(\"cnn_test_data:\", cnn_test_data.shape)\n    #print(\"cnn_test_target:\", cnn_test_target.shape)\n    \n    result = []\n    \n    for i in range(1,5):   \n        K.clear_session()\n        print ('i: ', i)\n        \n        print('fitting model')\n        \n        model = Sequential()\n                                            # M0 -> inputs are given (60,5,82)\n        #layer 1                            # M1 -> (60,5,8) (# past days, # markets, # features)\n        model.add(Conv2D(number_filter[0], (1, 1), activation='relu', input_shape=(number_of_stocks,seq_len, number_feature), data_format='channels_last'))\n        model.add(Dropout(0.1)) # added     # +++ Dropout \n        \n        #layer 2\n        model.add(BatchNormalization())     # added   +++ Batch Normalization\n        model.add(Conv2D(number_filter[1], (number_of_stocks, 3), activation='relu'))\n        model.add(MaxPool2D(pool_size=(1, 2)))\n        model.add(Dropout(0.2)) # added\n\n        \n        #layer 3\n        model.add(Conv2D(number_filter[2], (1, 3), activation='relu'))\n        model.add(MaxPool2D(pool_size=(1, 2)))\n        \n        # Flattening Layer:\n        model.add(Flatten())\n        model.add(Dropout(0.01))  # added\n        \n        # Last Layer:\n        model.add(Dense(1, activation='sigmoid'))\n\n        model.compile(optimizer='Adam', loss='mae', metrics=['acc',f1])\n\n        #best_model = callbacks.ModelCheckpoint(filepath, monitor='val_f1', verbose=0, save_best_only=True,\n                                               #save_weights_only=False, mode='max', period=1)\n\n        model.fit(cnn_train_data, cnn_train_target, epochs=epoc, batch_size=128, verbose=0, validation_split=0.25) #callbacks=[best_model],\n\n    #   model = load_model(filepath, custom_objects={'f1': f1})\n        test_pred = sklearn_acc(model,cnn_test_data, cnn_test_target)\n        print (test_pred)\n        result.append(test_pred)\n                \n        model.summary() # added \n        \n        plot_model(model)    \n    \n    print('saving results')\n    results = pd.DataFrame(result , columns=['MAE', 'Accuracy', 'F-score'])\n    results = results.append([results.mean(), results.max(), results.std()], ignore_index=True)\n    #results.to_csv(join(Base_dir, '3D-models\/{}\/new results.csv'.format(predict_index)), index=False)\n    return results\n    ","629a7022":"train_data, test_data, train_target, test_target = prepare_for_CNN()\n\nCNN(train_data, test_data, train_target, test_target)","84f2c3a3":"def cnn_data_sequence_for_intermediate(data, target, seque_len):\n    print ('sequencing data ...')\n    new_train = []\n    new_target = []\n\n    for index in range(data.shape[1] - seque_len + 1):\n        new_train.append(data[:, index: index + seque_len])\n        new_target.append(target[index + seque_len - 1])\n\n    new_train = np.array(new_train)\n    new_target = np.array(new_target)\n\n    return new_train, new_target","14f8eefa":"from tensorflow.python.keras import Model\n\ndef intermediate_CNN(train_data, test_data, train_target, test_target):\n    # hisory of data in each sample\n    seq_len = 60\n    epoc = 100\n    drop = 0.1\n    \n    cnn_train_data, cnn_train_target = cnn_data_sequence(train_data, train_target, seq_len)\n    print(\"cnn_train_data:\", cnn_train_data.shape)\n    print(\"cnn_train_target:\", cnn_train_target.shape)\n    \n    cnn_test_data, cnn_test_target = cnn_data_sequence(test_data, test_target, seq_len)\n    print(\"cnn_test_data:\", cnn_test_data.shape)\n    print(\"cnn_test_target:\", cnn_test_target.shape)\n    \n    result = []\n    \n           \n    print('fitting model')\n    \n    model = Sequential()\n                                        # M0 -> inputs are given (60,5,82)\n    #layer 1                            # M1 -> (60,5,8) (# past days, # markets, # features)\n    model.add(Conv2D(number_filter[0], (1, 1), activation='relu', input_shape=(number_of_stocks,seq_len, number_feature), data_format='channels_last'))\n    model.add(Dropout(0.1)) # added     # +++ Dropout \n    \n    #layer 2\n    model.add(BatchNormalization())     # added   +++ Batch Normalization\n    model.add(Conv2D(number_filter[1], (number_of_stocks, 3), activation='relu'))\n    model.add(MaxPool2D(pool_size=(1, 2)))\n    model.add(Dropout(0.2)) # added\n\n    \n    #layer 3\n    model.add(Conv2D(number_filter[2], (1, 3), activation='relu'))\n    model.add(MaxPool2D(pool_size=(1, 2)))\n    \n    # Flattening Layer:\n    model.add(Flatten())\n    outputlayer = Dropout(0.01)\n    model.add(outputlayer)  # added\n        \n    return model,outputlayer","935ec5af":"new_train, new_target = cnn_data_sequence_for_intermediate(train_data, train_target, 60)\nmodel, outputlayer = intermediate_CNN(train_data, test_data, train_target, test_target)\nintermediate_model = Model(model.input, outputlayer.output)\ncnn_features = intermediate_model.predict(new_train)","6faef4e1":"new_test, new_target_test = cnn_data_sequence_for_intermediate(test_data, test_target, 60)\ntest_cnn_features = intermediate_model.predict(new_test)","9ec4a823":"from xgboost import XGBClassifier","9278ab56":"xgbmodel = XGBClassifier()","877d18c0":"xgbmodel.fit(cnn_features, new_target)","381bd882":"pred = xgbmodel.predict(test_cnn_features)","6740e458":"from sklearn.metrics import accuracy_score\n\naccuracy = accuracy_score(new_target_test, pred)\nprint(\"Accuracy:\", accuracy)","914a086c":"print(\"f1 score:\", f1_score(new_target_test, pred, average=\"macro\"))","6fb0cf48":"print(\"MAE:\", mae(new_target_test, pred))","f749e51e":"best_SP_results = []\nSP_CNN_XGBoost = [0.5532544378698225, 0.5518382901750042, 0.4467455621301775] # Accuracy, F1, MAE","639e90b2":"best_DJI_results = []\nDJI_CNN_XGBoost = [0.4970414201183432, 0.48484848484848486, 0.5029585798816568]","01f30dfe":"best_NASDAQ_results = []\nNASDAQ_CNN_XGBoost = [0.5355029585798816, 0.5219063921798279, 0.46449704142011833]","d9b12db9":"best_NYSE_results = []\nNYSE_CNN_XGBoost = [0.5325443786982249, 0.45984545049965614, 0.46745562130177515]","4df9811b":"best_RUSSELL_results = []\nRUSSELL_CNN_XGBoost = [0.5266272189349113, 0.5255640944660842, 0.47337278106508873]","6433a355":"# Tablo yap\u0131lacak","736f6aac":"# NYSE Results:","912c8401":"# XGBoost","7db42807":"![image.png](attachment:4b8fb5e9-d3db-419f-a586-9cfbad869b00.png)","4bb1e361":"![image.png](attachment:13c42fb9-ab65-4b0f-bbd7-10c4d0b90973.png)","808458bd":"![image.png](attachment:36357dba-a589-4cfa-97d1-372796ee33b8.png)","0e400232":"![image.png](attachment:87cce2fb-0699-48d2-bfef-3d348912cbf1.png)\n![image.png](attachment:b1d4aa43-6eb0-489f-b744-435f8c82000d.png)\n![image.png](attachment:993fcfde-b2b8-4f80-ad05-813207bf6f3f.png)","6741a79a":"![image.png](attachment:cc7e78f6-bc70-4e94-876b-953d25b663c8.png)\n![image.png](attachment:4025b31c-59c0-4483-81ac-8ebf83e0d520.png)\n![image.png](attachment:6541f186-bf7c-46eb-bf11-a5e0c2f290d3.png)","276bfb61":"![image.png](attachment:e05688a1-603b-4f87-925a-b11c2a140977.png)","0fa7b053":"![image.png](attachment:66fbdff2-e2be-4283-8069-50801b883d48.png)","a2166243":"# NASDAQ Results:","a27d15b5":"# DJI Results:","a7c30249":"![image.png](attachment:40ef2ea6-6c1b-45f2-a1fe-e94a457499eb.png)","f8b30d04":"![image.png](attachment:c4e62991-62ae-4770-8ce9-55045ddd5a90.png)","113da0ec":"# S&P 500 Results:","26a4c483":"![image.png](attachment:eb92e190-7ae1-45df-a694-79a72ca9a29d.png)","83968f7b":"![image.png](attachment:9f2b007a-5b4b-44c5-b1c4-eb7dce423c21.png)\n![image.png](attachment:a141bf18-89e5-40b5-a42d-2361925d6f9b.png)\n![image.png](attachment:75306e14-0418-4bfa-9617-c0bf03d4d46d.png)","018c0e33":"## Data Preparation","fc6323bc":"![image.png](attachment:742c6c67-dac8-428e-920c-21748aff14c6.png)","49236eee":"# RUSSELL Results:","c8bc564c":"![image.png](attachment:df7b0e18-a1be-4e0e-a39c-bad3e9704eb8.png)","253055fa":"![image.png](attachment:fba87088-d3ec-44de-91b7-c9c47d0d2a38.png)\n![image.png](attachment:7f67fbf9-88bd-49f0-a333-5db70fcabebe.png)\n![image.png](attachment:199dc984-7f6b-4997-89c6-a55a77cc199a.png)","dfb64ecc":"![image.png](attachment:dc5b0891-05e0-43da-a732-8b1540e20795.png)\n![image.png](attachment:a8e4995d-4317-4522-a860-da8c21d9913d.png)\n![image.png](attachment:d205bc03-1c75-42a6-bb03-28debd3ee7fe.png)"}}