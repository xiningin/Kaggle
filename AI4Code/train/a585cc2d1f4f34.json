{"cell_type":{"5c5ed4dd":"code","5e10e232":"code","c1465886":"code","90929d6e":"code","3e4b1a49":"code","aefa4e1e":"code","5b9cacaf":"code","5587e5b4":"code","4655b363":"code","4076bfcd":"code","540911a9":"code","94df2ec0":"markdown","d1635cc8":"markdown","e6780e46":"markdown","e5d07b7f":"markdown","6a772b8d":"markdown","b42b7e5e":"markdown","2dd52876":"markdown","08dfbf2f":"markdown"},"source":{"5c5ed4dd":"import math \nfrom tqdm.notebook import tqdm \nimport matplotlib.pyplot as plt \nimport numpy as np ","5e10e232":"# generating data\ndata = np.random.uniform(-1, 1, (1500, 2)) ","c1465886":"plt.figure(figsize=(8, 8))\nplt.plot(data, 'bo')","90929d6e":"# dimensions of map\nx, y = 10, 10\n\nsigma = 1.\nlearning_rate = 0.5\nepochs = 50000\ndecay_parameter = epochs \/ 2","3e4b1a49":"# Activation map and Assigning Weights \n# using random number generation  \nactivation_map = np.zeros((x, y)) \nweights = 2 * (np.random.ranf((x, y, data.shape[1])) - 0.5) ","aefa4e1e":"# Neighborhood regions\nneighbour_x = np.arange(x)\nneighbour_y = np.arange(y)","5b9cacaf":"def decay_learning_rate_sigma(iteration): \n    learning_rate_ = learning_rate \/ (1 + iteration \/ decay_parameter) \n    sigma_ = sigma \/ (1 + iteration \/ decay_parameter) \n  \n    return learning_rate_, sigma_ ","5587e5b4":"def get_winner_neuron(x): \n    s = np.subtract(x, weights) # x - w \n    it = np.nditer(activation_map, flags =['multi_index']) \n    while not it.finished: \n        # || x - w || \n        activation_map[it.multi_index] = np.linalg.norm(s[it.multi_index])   \n        it.iternext() \n\n    return np.unravel_index(activation_map.argmin(), activation_map.shape) ","4655b363":"def update_weights(win_neuron, inputx, iteration): \n    # decay learning rate and sigma \n    learning_rate_, sigma_ = decay_learning_rate_sigma(iteration) \n\n    # get neighborhood about winning neuron (Mexican hat function) \n    d = 2 * np.pi * (sigma_**2) \n    ax = np.exp(-1 * np.square(neighbour_x - win_neuron[0]) \/ d) \n    ay = np.exp(-1 * np.square(neighbour_y - win_neuron[1]) \/ d) \n\n    neighborhood = np.outer(ax, ay) \n\n    it = np.nditer(neighborhood, flags = ['multi_index']) \n    while not it.finished: \n        weights[it.multi_index] += learning_rate_ * neighborhood[it.multi_index] * (inputx - weights[it.multi_index]) \n        it.iternext() ","4076bfcd":"for epoch in tqdm(range(1, epochs + 1)): \n    np.random.shuffle(data) \n    idx = np.random.randint(0, data.shape[0]) \n    win_neuron = get_winner_neuron(data[idx]) \n    update_weights(win_neuron, data[idx], epoch) \n\n    if epoch == 1 or epoch == 100 == 0 or epoch == 1000 or epoch == 10000 or epoch == 50000: \n        plot_x = [] \n        plot_y = []\n\n        for i in range(weights.shape[0]): \n            for j in range(weights.shape[1]): \n                plot_x.append(weights[i][j][0]) \n                plot_y.append(weights[i][j][1]) \n\n        plt.title('After ' + str(epoch) + ' iterations') \n        plt.scatter(plot_x, plot_y, c='b') \n        plt.show() \n        plt.close() ","540911a9":"test_inputs = np.array([[-0.6, 0.9]]) \n\n# The plots below depict the working of this Kohonen Network on \n# given test inputs [-0.6, 0.9] \nfor i in range(test_inputs.shape[0]): \n    test_input = test_inputs[i, :] \n    win_neuron = get_winner_neuron(test_input) \n\n    plot_x = np.arange(-1, 1, 0.1) \n    plot_y = np.arange(-1, 1, 0.1) \n    xx, yy = np.meshgrid(plot_x, plot_y) \n\n    coordx, coordy = weights[win_neuron[0]][win_neuron[1]][0],weights[win_neuron[0]][win_neuron[1]][1] \n    dist = math.sqrt((coordx-test_input[0])**2 + (coordy - test_input[1])**2) \n    coordx = round(coordx, 1) \n    coordy = round(coordy, 1) \n\n    plt.figure(figsize =(20, 20)) \n    plt.title(\"Distance between activated neuron and input = \" + str(dist), fontsize = 30) \n    plt.scatter(xx, yy, c='g') \n    plt.scatter(coordx, coordy, c='r') \n    plt.show() \n    plt.close() ","94df2ec0":"# Hyperparameters","d1635cc8":"# Function to retrive winner neuron","e6780e46":"# Function to update weights","e5d07b7f":"# Decay Learning Rate","6a772b8d":"# Prepare Data","b42b7e5e":"# Training","2dd52876":"# Test","08dfbf2f":"# Self Organizing Neural Network (SONN)"}}