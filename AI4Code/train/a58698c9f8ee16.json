{"cell_type":{"907a7673":"code","ecf038a3":"code","4ab490a8":"code","6c129f4d":"code","55415b86":"code","b3b8539b":"code","9c03b285":"code","dbfff545":"code","e39de449":"code","97a648a4":"code","71b0589e":"code","bc24b3ef":"code","319d19db":"code","f7e9e1e9":"code","ba92d549":"code","14168f19":"code","cd434210":"code","2da4a093":"code","6a0a4c8d":"code","ed2e4555":"code","06e0aead":"code","14bdd131":"code","4716fd47":"code","217b4493":"code","da272715":"code","75105726":"code","c0dace6c":"code","9626cf1c":"code","c6224cef":"code","40b114a4":"code","e1651cf3":"code","4f4eed95":"code","a41b7e38":"code","9b9115fd":"code","c2e996c9":"code","a44fc947":"code","e8d83a8d":"code","0dcf84a9":"code","31dfb8c5":"code","c94128e6":"code","f4efa305":"code","6c7800c0":"markdown","a8972861":"markdown","08f78f3d":"markdown","2a4f2114":"markdown","6ad36b53":"markdown","dda8f393":"markdown","c727bbd4":"markdown","f653c76e":"markdown","9e3f2dda":"markdown","ca846b91":"markdown","b9719827":"markdown","3430aa7d":"markdown","faac958f":"markdown","b45c6e6a":"markdown","c37120b9":"markdown"},"source":{"907a7673":"import numpy as np\nimport pandas as pd \n\nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\nimport os \nimport cv2\nimport random\n \nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split \n\nimport warnings \nwarnings.filterwarnings(\"ignore\")","ecf038a3":"print(\"TensorFlow version: {}\".format(tf.__version__))\nprint(\"Eager execution: {}\".format(tf.executing_eagerly()))","4ab490a8":"sns.set(style= 'darkgrid', \n       color_codes=True,\n       font = 'Arial',\n       font_scale= 1.5,\n       rc={'figure.figsize':(12,8)})","6c129f4d":"from IPython.core.display import display, HTML\ndisplay(HTML(\"<style>div.output_scroll { height: 70em; }<\/style>\"))","55415b86":"data_dir = \"..\/input\/petfinder-pawpularity-score\/train\/\"\ntest_dir = \"..\/input\/petfinder-pawpularity-score\/test\/\"\n\ndata = pd.read_csv('..\/input\/petfinder-pawpularity-score\/train.csv')\ntest = pd.read_csv('..\/input\/petfinder-pawpularity-score\/test.csv')\nss = pd.read_csv('..\/input\/petfinder-pawpularity-score\/sample_submission.csv')","b3b8539b":"print(data.shape)\nprint(test.shape)\nprint(ss.shape)","9c03b285":"data.head()","dbfff545":"_, axs = plt.subplots( 2, 2, figsize=(15, 12))\n\naxs = axs.flatten()\ncol = data.columns.tolist() \n\nfor a, ax in zip(data.sample(4).iterrows(), axs):\n    img = cv2.imread(data_dir + f'{a[1][0]}.jpg')\n    img = cv2.resize(img, (600, 600))\n    other_info = [col[i] for i in range(13) if a[1][i] == 1 ]\n    ax.grid(False)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.imshow(img)\n    ax.set_title(f'Id: {a[0]}, Pawpularity : {a[1][13]}, ' + \", \".join(other_info), fontsize= 12, fontweight='bold' )\n    \nplt.show()","e39de449":"sns.distplot(data[\"Pawpularity\"])\nplt.title(\"Distribution of Pawpularity\")","97a648a4":"train,val  = train_test_split( data, test_size=0.2)  ","71b0589e":"train.shape","bc24b3ef":"val.shape","319d19db":"train.columns","f7e9e1e9":"filenames = tf.constant(train.Id.map(lambda x : data_dir + f'{x}.jpg' ).tolist())\nfeaturs = tf.constant(train[['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory','Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']])\nlabels = tf.constant( train.Pawpularity.tolist())\n\ndataset = tf.data.Dataset.from_tensor_slices(( {\"input_1\": filenames, \"input_2\": featurs }, labels))","ba92d549":"\nval_filenames = tf.constant(val.Id.map(lambda x : data_dir + f'{x}.jpg' ).tolist())\nval_featurs = tf.constant(val[['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory','Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']])\nval_labels = tf.constant( val.Pawpularity.tolist() )\n\n\nval_dataset = tf.data.Dataset.from_tensor_slices(({\"input_1\": val_filenames, \"input_2\": val_featurs }, val_labels))","14168f19":"list(dataset.as_numpy_iterator())[:5]","cd434210":"### Hyperparams \n\nBATCH_SIZE = 64\nIMG_SIZE = ( 224 ,  224) ","2da4a093":"def _parse_function( inputs,  output):\n\n    filename = inputs[\"input_1\"]\n\n    image_string = tf.io.read_file(filename)\n    image_decoded = tf.image.decode_jpeg(image_string)\n    image_resized = tf.image.resize(image_decoded, IMG_SIZE)\n\n    inputs[\"input_1\"] = image_resized\n\n    return  inputs, output","6a0a4c8d":"dataset = dataset.map(_parse_function)\nval_dataset = val_dataset.map(_parse_function)","ed2e4555":"# Print one key val pair \ndef print_pair( input, output):\n\n    image = input[\"input_1\"]\n    feature = input[\"input_2\"]\n\n    print(feature.numpy())\n    print(feature.numpy().shape)\n\n    plt.figure()\n    plt.imshow((image.numpy()).astype(np.uint8))\n    plt.title( output.numpy())\n    plt.axis('off')\n    plt.show()\n    print(\"\\n\\n\\n\")\n\n\nfor input, output  in dataset.take(2):\n    print_pair(input, output)\n","06e0aead":"dataset = dataset.batch(BATCH_SIZE) \nval_dataset = val_dataset.batch(BATCH_SIZE) ","14bdd131":"AUTOTUNE = tf.data.AUTOTUNE\ndataset = dataset.cache().shuffle(500).prefetch(buffer_size=AUTOTUNE)\nval_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE) ## We dont need to shuffel the validation data ","4716fd47":"base_image_model = tf.keras.applications.DenseNet121( \n                                               include_top=False,\n                                               weights='imagenet'\n                                               )","217b4493":"base_image_model.trainable = False","da272715":"class ProcessImageBlock(tf.keras.Model):\n\n    def __init__(self):\n\n        super(ProcessImageBlock, self).__init__()\n\n        self.input_l = tf.keras.layers.InputLayer( input_shape = IMG_SIZE + (3,)  ) \n        self.base_model = base_image_model\n        self.preprocess_input = tf.keras.applications.densenet.preprocess_input \n        \n        self.data_augmentation = tf.keras.Sequential([\n                                tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n                                tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n                                ])\n        self.rescale = tf.keras.layers.experimental.preprocessing.Rescaling(1.\/127.5, offset= -1)\n        self.gap = tf.keras.layers.GlobalAveragePooling2D() ##  ( batch_size , 2048 )\n\n        self.activation = tf.keras.layers.ReLU()\n        self.dense = tf.keras.layers.Dense(512, activation= self.activation )\n        self.final = tf.keras.layers.Dense(64, activation= self.activation )\n\n        \n    def call(self, input_tensor):\n\n        x = self.input_l(input_tensor)\n        x = self.data_augmentation(x)\n        x = self.preprocess_input(x)\n        x = self.base_model(x, training=False)\n        x = self.gap(x)\n\n        x = self.dense(x)\n        x = self.final(x)\n \n        return  x\n\nclass ProcessTabBlock(tf.keras.Model):\n\n    def __init__(self):\n\n        super(ProcessTabBlock, self).__init__()\n\n        self.input_l = tf.keras.layers.InputLayer( input_shape = (12,)  ) \n        self.layer_1 = tf.keras.layers.Dense(32, activation='relu')\n        self.layer_2 = tf.keras.layers.Dense(64, activation='relu')\n        \n    def call(self, input_tensor ):\n        \n        x = self.input_l(input_tensor)\n        x = self.layer_1(x)\n        x = self.layer_2(x)\n\n        return x\n","75105726":"class MyCustomModel(tf.keras.Model):\n\n    def __init__(self):\n\n        super(MyCustomModel, self).__init__()\n\n        self.process_image_data = ProcessImageBlock()\n        self.process_tabular_data = ProcessTabBlock()\n\n        self.activation_1 = tf.keras.layers.LeakyReLU( alpha=0.3)\n        self.activation_2 = tf.keras.layers.ReLU()\n        self.activation_final = tf.keras.layers.ReLU(max_value = 100 )\n        self.dropout = tf.keras.layers.Dropout(0.2) \n\n        self.dense_1 =   tf.keras.layers.Dense(64,activation= self.activation_1  )\n        self.dense_2 =   tf.keras.layers.Dense(8,activation=  self.activation_2  )\n        self.final =   tf.keras.layers.Dense(1, activation=  self.activation_final )\n    \n    def call(self, inputs ): \n\n        image = inputs[\"input_1\"]\n        feature = inputs[\"input_2\"]\n\n        x1 = self.process_image_data(image)\n        x2 = self.process_tabular_data(feature)\n\n        x = tf.keras.layers.concatenate([x1, x2])## ( batch_size, 128 )\n\n        x = self.dense_1(x)\n        x = self.dropout(x)\n        x = self.dense_2(x)\n\n        x = self.final(x)\n   \n        return  x","c0dace6c":"def create_model():\n    \n    model = MyCustomModel()\n    \n    model.compile(\n        optimizer='adam', \n        loss=\"mse\", # Mean squared error \n        metrics=[\"mae\"] # Mean Absolute Error\n      )\n    \n    return model ","9626cf1c":"model = create_model()","c6224cef":"epochs = 20\n\ncheckpoint_path = \"cp.ckpt\"\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\n\n# Create a callback that saves the model's weights\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                 save_weights_only=True,\n                                                 verbose=1)\n\nes_callback = tf.keras.callbacks.EarlyStopping(\n                                monitor='val_mae',\n                                patience=3,\n                                verbose=1,\n                                restore_best_weights=True)\n\nhistory = model.fit(\n                    dataset,\n                    validation_data = val_dataset, \n                    epochs=epochs,\n                    callbacks = [cp_callback , es_callback ] ,\n                    )","40b114a4":"saved_checkpoint_path = \"cp.ckpt\"","e1651cf3":"# Create a basic model instance\nnew_model = create_model()\n\n# Loads the weights\nnew_model.load_weights(saved_checkpoint_path)","4f4eed95":"def _parse_function_test( inputs):\n\n    filename = inputs[\"input_1\"]\n\n    image_string = tf.io.read_file(filename)\n    image_decoded = tf.image.decode_jpeg(image_string)\n    image_resized = tf.image.resize(image_decoded, IMG_SIZE)\n\n    inputs[\"input_1\"] = image_resized\n\n    return  inputs","a41b7e38":"\ntest_filenames = tf.constant(test.Id.map(lambda x : test_dir + f'{x}.jpg' ).tolist())\ntest_featurs = tf.constant(test[['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory','Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']])\n \ntest_dataset = tf.data.Dataset.from_tensor_slices(({\"input_1\": test_filenames, \"input_2\": test_featurs }))","9b9115fd":"test_dataset = test_dataset.map(_parse_function_test)","c2e996c9":"test_dataset = test_dataset.batch(len(test))","a44fc947":"predictions = new_model.predict(test_dataset)","e8d83a8d":"ss.head()","0dcf84a9":"submission = pd.DataFrame()\nsubmission[\"Id\"] = test[\"Id\"]\nsubmission[\"Pawpularity\"]= predictions","31dfb8c5":"submission.head()","c94128e6":"ss.columns.equals(submission.columns)","f4efa305":"submission.to_csv('submission.csv', index=False)","6c7800c0":"## Predict on test data","a8972861":"## Model","08f78f3d":"### Perfomence Optimization ","2a4f2114":"### Compile and Train","6ad36b53":"### Analyze the data","dda8f393":"## Conclusion:\n\n> #### In the [*last notebook*](https:\/\/www.kaggle.com\/vivmankar\/cnn-regressor-using-transfer-learning-tf-data) we saw the approach that uses only image data, the test_mae was 15.1742, \n> #### while in this approach we have achieved the test_mae of **13.5925** on the same test split, which is an improvement.","c727bbd4":"### Download base model \n\nWe will use DenseNet121 as a base model, other opctions for pretrained models can be found [here](https:\/\/keras.io\/api\/applications\/)","f653c76e":"## Overview of the Notebook\n\nIn this notebook we will discuss the Transfer Learning + Multi-input custom model approch to the problem \n\n#### Data preprocessing\n\n>   1. Analyze the datset \n>   2. Create the dataset with two inputs and one output (tf.data.dataset ) \n>   2. Batching ( To speedup the treaning ) \n>   3. Configure the dataset for performance ( To speedup the treaning )\n \n#### Model Building \n\n>   1. Load the base model ( DenseNet ) \n>   2. Develope image and tabular data models\n>   2. Develope a final custom model class  \n>   3. Update last layer activation to ReLU(max_value = 100 ) \/\/ this helps improving performence \n>   4. Compile model and add callbacks ( Save-Checkpoint, Early Stopping ) \n>   5. Train Model ( MAE on validation split : 13.5925  ) \n","9e3f2dda":"## Set up","ca846b91":"### Data preprocessing","b9719827":" ## Problem description\n\nPetFinder.my uses a basic Cuteness Meter to rank pet photos. It analyzes picture composition and other factors compared to the performance of thousands of pet profiles. \n\nWhile this basic tool is helpful, it's still in an experimental stage and the algorithm could be improved. The participants needs to build an AI model using provided data to help make the tool better.  \n\n**Task** \n\nThe task is to predict engagement with a pet's profile( **Pawpularity** ) based on the photograph for that profile. \n\n**Data** \n\nThe dataset for this competition comprises both images and tabular data(hand-labelled metadata for each photo). \n\nThe train set contains 9912 pet photos \n\nThe test set contains 8 pet photos\n> NOTE: The actual test data comprises about **6800** pet photos similar to the training set photos. \n\n\n####  **Previous Notebooks**: \n1. [*Understanding the problem & EDA*](https:\/\/www.kaggle.com\/vivmankar\/understanding-the-problem-eda) \n2. [*ML RandomForestRegressor*](https:\/\/www.kaggle.com\/vivmankar\/ml-randomforestregressor)\n3. [*CNN_Regressor_using_Transfer_Learning_+_tf.data*](https:\/\/www.kaggle.com\/vivmankar\/cnn-regressor-using-transfer-learning-tf-data)","3430aa7d":"### File names to images","faac958f":" \n* **Dataset.cache** keeps the images in memory after they're loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache.\n* **Dataset.prefetch** overlaps data preprocessing and model execution while training.\n ","b45c6e6a":"## Data ","c37120b9":"### Load the saved model "}}