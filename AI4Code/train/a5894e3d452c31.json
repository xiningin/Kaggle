{"cell_type":{"50c6c4c7":"code","c09ff21e":"code","d765df6b":"code","dcc46deb":"code","2fbea4a3":"code","84858457":"code","80054f6a":"code","eb8952b6":"code","b86b663f":"code","ad817dd4":"code","955a6f3f":"code","306b183e":"code","e9936fdc":"code","781b3583":"code","47ae35db":"code","0ded488b":"markdown","e48e0dbc":"markdown","cacba12a":"markdown","863bf9ef":"markdown"},"source":{"50c6c4c7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","c09ff21e":"from numpy.random import permutation\nfrom sklearn import metrics\nfrom fastsr.estimators.symbolic_regression import SymbolicRegression","d765df6b":"train = pd.read_csv('..\/input\/train.csv')\nmagnetic_shielding_tensors = pd.read_csv('..\/input\/magnetic_shielding_tensors.csv')\ntrain = pd.merge(train, magnetic_shielding_tensors, how='left',\n              left_on=['molecule_name', 'atom_index_0'],\n              right_on=['molecule_name', 'atom_index'])\n\ntrain.rename(columns = { \"XX\": \"XX_0\", \"YX\": \"YX_0\", \"ZX\": \"ZX_0\",\n                         \"XY\": \"XY_0\", \"YY\": \"YY_0\", \"ZY\": \"ZY_0\",\n                         \"XZ\": \"XZ_0\", \"YZ\": \"YZ_0\", \"ZZ\": \"ZZ_0\" }, inplace=True)\n\ntrain = pd.merge(train, magnetic_shielding_tensors, how='left',\n              left_on=['molecule_name', 'atom_index_1'],\n              right_on=['molecule_name', 'atom_index'])\n\ntrain.rename(columns = { \"XX\": \"XX_1\", \"YX\": \"YX_1\", \"ZX\": \"ZX_1\",\n                         \"XY\": \"XY_1\", \"YY\": \"YY_1\", \"ZY\": \"ZY_1\",\n                         \"XZ\": \"XZ_1\", \"YZ\": \"YZ_1\", \"ZZ\": \"ZZ_1\" }, inplace=True)\ndel magnetic_shielding_tensors","dcc46deb":"# one-hot-encode type\ntrain = pd.concat([train, pd.get_dummies(train['type'], prefix='type')], axis=1)","2fbea4a3":"# features for predicting scalar coupling constant on train set from magnetic shielding tensors\npred_vars = ['type_1JHC', 'type_1JHN', 'type_2JHC', 'type_2JHH', 'type_2JHN',\n             'type_3JHC', 'type_3JHH', 'type_3JHN', 'XX_0', 'YX_0', 'ZX_0', 'XY_0', 'YY_0', 'ZY_0',\n             'XZ_0', 'YZ_0', 'ZZ_0', 'XX_1', 'YX_1', 'ZX_1', 'XY_1',\n             'YY_1', 'ZY_1', 'XZ_1', 'YZ_1', 'ZZ_1']","84858457":"# train-val split by molecule_name\nmolecule_names = pd.DataFrame(permutation(train['molecule_name'].unique()),columns=['molecule_name'])\nnm = molecule_names.shape[0]\nntrn = int(0.9*nm)\nnval = int(0.1*nm)\n\ntmp_train = pd.merge(train, molecule_names[0:ntrn], how='right', on='molecule_name')\ntmp_val = pd.merge(train, molecule_names[ntrn:nm], how='right', on='molecule_name')\n\nX_train = tmp_train[pred_vars]\nX_val = tmp_val[pred_vars]\ny_train = tmp_train['scalar_coupling_constant']\ny_val = tmp_val['scalar_coupling_constant']\ny_type = tmp_val['type']\ndel tmp_train, tmp_val","80054f6a":"# fit symbolic regression on train\nsr = SymbolicRegression(ngen=100, pop_size=10)\nsr.fit(X_train.values, y_train.values)","eb8952b6":"# evaluation metric for validation\n# https:\/\/www.kaggle.com\/abhishek\/competition-metric\ndef metric(df, preds):\n    df[\"prediction\"] = preds\n    maes = []\n    for t in df.type.unique():\n        y_true = df[df.type==t].scalar_coupling_constant.values\n        y_pred = df[df.type==t].prediction.values\n        mae = np.log(metrics.mean_absolute_error(y_true, y_pred))\n        maes.append(mae)\n    return np.mean(maes)","b86b663f":"# performance on val\npreds = sr.predict(X_val.values)\nmetric(pd.concat([X_val, y_val, y_type], axis=1), preds)","ad817dd4":"scc = pd.read_csv('..\/input\/scalar_coupling_contributions.csv')\ntrain = pd.merge(train, scc, how='left',\n                 on=['molecule_name', 'atom_index_0', 'atom_index_1', 'type'])\ndel scc","955a6f3f":"# features for predicting scalar coupling constant on train set from its contributions\npred_vars = ['type_1JHC', 'type_1JHN', 'type_2JHC', 'type_2JHH', 'type_2JHN',\n             'type_3JHC', 'type_3JHH', 'type_3JHN', 'fc', 'sd', 'pso', 'dso']","306b183e":"# train-val split by molecule_name\nmolecule_names = pd.DataFrame(permutation(train['molecule_name'].unique()),columns=['molecule_name'])\nnm = molecule_names.shape[0]\nntrn = int(0.9*nm)\nnval = int(0.1*nm)\n\ntmp_train = pd.merge(train, molecule_names[0:ntrn], how='right', on='molecule_name')\ntmp_val = pd.merge(train, molecule_names[ntrn:nm], how='right', on='molecule_name')\n\nX_train = tmp_train[pred_vars]\nX_val = tmp_val[pred_vars]\ny_train = tmp_train['scalar_coupling_constant']\ny_val = tmp_val['scalar_coupling_constant']\ny_type = tmp_val['type']\ndel tmp_train, tmp_val","e9936fdc":"# fit symbolic regression on train\nsr = SymbolicRegression(ngen=100, pop_size=10)\nsr.fit(X_train.values, y_train.values)","781b3583":"# performance on val\npreds = sr.predict(X_val.values)\nmetric(pd.concat([X_val, y_val, y_type], axis=1), preds)","47ae35db":"sr.print_best_individuals()","0ded488b":"There are several datasets provided only for the train data and not the test data. In other kernels I have shown that imputing the dipole moment and potential energy on the test set marginally improved the LB score over a simple model using only structural information:\nhttps:\/\/www.kaggle.com\/robertburbidge\/imputing-molecular-features\n\nAnd that estimated Mulliken charges can be used to further improve the LB score:\nhttps:\/\/www.kaggle.com\/robertburbidge\/using-estimated-mulliken-charges\n\nIn another notebook I tried to do the same for the magnetic shielding tensor:\nhttps:\/\/www.kaggle.com\/robertburbidge\/imputing-magnetic-shielding-tensor\/\n\nHowever, I can't get that kernel to complete on Kaggle compute so I ran it on a VM with 72 CPUs and 144 GB of RAM and found that I couldn't do much better than predicting the median tensor for all atoms. Rather than waste time pursuing this I decided to investigate the relationship between the magnetic shielding tensor of two atoms in a molecule and their scalar coupling constant.\n\nSince I have no idea what the relationship may be, I'm using symbolic regression. It shows no relationship. As a sanity check, I show that symbolic regression correctly identifies the scalar coupling constant contribution `fc` as being predictive.","e48e0dbc":"This suggests that there is no relationship between the magnetic shielding tensors and the scalar coupling constant.\n\nAs a sanity check, in the following I check that symbolic regression can find a relationship when it exists by regressing the scalar coupling constant on its contributions.","cacba12a":"The best individuals are `fc` with minor random variations (introduced by the stochastic search), which is known to be the major contributing factor.\n\nConclusion: I am not going to bother with the magnetic shielding tensor.","863bf9ef":"Symbolic regression has picked up a strong relationship. Let's see what it is."}}