{"cell_type":{"720a1421":"code","bbdedbfc":"code","ada5a244":"code","696d1b94":"code","5a5dcabb":"code","aa9fd0ff":"code","b62d4bc8":"code","d665ef46":"code","fbff667c":"code","6d345fa4":"code","4b3d9df1":"code","f4151077":"code","4b471f51":"code","accd8c3f":"code","da30902a":"code","fa6acfc5":"code","386de481":"code","12e0f7ab":"code","1dbbb174":"code","c6dfe519":"code","a2cfe541":"code","c85cca71":"code","5860a3a0":"code","4eaff3bb":"code","9152e799":"code","1796b70f":"code","1532d09a":"code","1bd3e788":"code","b85c54f4":"code","6e91dce6":"code","93d7e56f":"code","b657a843":"code","1291d6f8":"code","62805cfa":"code","963bdf71":"code","308e17c2":"code","36b67d1a":"code","8158d7c1":"code","6b2d7996":"code","c9afa0be":"code","c4a0058e":"code","0dea486f":"code","a5a0db53":"code","0ef78cfa":"code","ae0aaf5e":"code","7dfd5fc3":"code","fcb4a40a":"code","974e30ea":"code","fcb619f9":"code","28f65678":"code","889047b1":"code","db62124b":"code","153613a9":"code","80a78a42":"code","fa72353a":"code","8865a7eb":"code","27ebab9e":"code","e2c684b5":"code","9257d47e":"code","447c1f1f":"code","e20d1a16":"code","1c964283":"code","933a3ac6":"code","4582e4f8":"code","37649e21":"code","091fe2b1":"code","0a8b30a2":"code","7a8cb383":"code","dbf30ff9":"code","f6b50269":"code","2a352df9":"code","d44cfbe2":"code","1beecf00":"code","29807ab6":"code","0bf8eba2":"code","ce45b0e3":"code","0aec3db6":"code","84d7ade1":"code","6cdf59c5":"code","3d5d1574":"code","ce7e576f":"code","a14d1cd8":"code","c8b50e4a":"code","7abf0f07":"code","538608b4":"code","f4c1b41f":"code","3e31914f":"code","ca1ab5dd":"code","51c0123e":"code","18de17e7":"markdown","340d9216":"markdown","0695750e":"markdown","9c0ee6c9":"markdown","de44507d":"markdown","167af529":"markdown","59210dba":"markdown","317d029c":"markdown","d1e11551":"markdown","9aede78e":"markdown","9ae2f4f6":"markdown","93bb8397":"markdown","23abd348":"markdown"},"source":{"720a1421":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport graphviz\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","bbdedbfc":"df = pd.read_csv(\"..\/input\/testset.csv\")","ada5a244":"df.head()","696d1b94":"# We can see all the column name has space in there names, lets assign new names with removed space.\ndf.columns","5a5dcabb":"df.columns = map(lambda x: x.strip(), df.columns)","aa9fd0ff":"df.columns # Space removed","b62d4bc8":"# Lets check the usual whether codition. We can see usually delhi's weather is either Haze, Smoke. Not good\n# for health. :()\ndf._conds.value_counts(ascending=False)","d665ef46":"# Lets plot top 10 weather condition in delhi.\nplt.figure(figsize=(15, 10));\ndf._conds.value_counts().head(10).plot(kind='bar');\nplt.title(\"Top 10 most common weather condition\")\nplt.plot();\n# We can clearly see that haze and smoe are the most commo weather condition in delhi.","fbff667c":"# Lets see top 10 least condition\nplt.figure(figsize=(15, 10));\ndf._conds.value_counts(ascending=True).head(10).plot(kind=\"bar\");\nplt.title(\"Top 10 least whether condition in delhi\");\nplt.plot();","6d345fa4":"# common wind direction\ndf._wdire.value_counts()","4b3d9df1":"plt.figure(figsize=(15, 10));\nplt.title(\"Common wind direction in delhi\");\ndf._wdire.value_counts().plot(kind=\"bar\");\nplt.plot();","f4151077":"# Average temprature\nprint(\"average temprature in delhi:\", round(df._tempm.mean(axis=0),2))","4b471f51":"# As we can see there is datetime column, We can extract year from it. Year can ve an important feature\n# for us to calculate how temprature is changing according to year\ndef extract_year(value):\n    return (value[0:4])","accd8c3f":"df.head()","da30902a":"# function to get month\ndef extract_month(value):\n    return (value[4:6])","fa6acfc5":"# Lets check our method\ndf[\"year\"] = df[\"datetime_utc\"].apply(lambda x:extract_year(x))\ndf[\"month\"] = df[\"datetime_utc\"].apply(lambda x:extract_month(x))","386de481":"df.head() # So we can see a new column with year added","12e0f7ab":"# lets check out data range\nprint(\"max, min: \", df.year.max(), \",\", df.year.min())","1dbbb174":"# So our given data is from 1996 to 2017. ","c6dfe519":"# Number of records for paticular year\ndf.year.value_counts()","a2cfe541":"df.groupby(\"year\")._tempm.mean()","c85cca71":"df_mean = df.groupby(\"year\")._tempm.mean().reset_index().sort_values('_tempm', ascending=True)","5860a3a0":"df_mean.dtypes","4eaff3bb":"df_mean.year = df_mean.year.astype(\"float\")","9152e799":"df_mean.dtypes","1796b70f":"\ndf_mean.plot(kind=\"scatter\", x=\"year\", y=\"_tempm\", figsize=(15, 10))\n\nplt.xticks(df_mean.year);\nplt.title(\"Average temprature change\");\nplt.plot();","1532d09a":"df.isnull().sum()","1bd3e788":"df.columns","b85c54f4":"df_filtered = df[['datetime_utc', '_conds', '_dewptm', '_fog', '_hail',\n       '_hum', '_pressurem', '_rain', '_snow', '_tempm',\n       '_thunder', '_tornado', '_vism', '_wdird', '_wdire'\n       , '_wspdm', 'year', \"month\"]]","6e91dce6":"# Lets replace missing values in _dewptm. We can take an avrgae of that year\ndf_filtered[df_filtered._dewptm.isnull()]","93d7e56f":"# We will try to replace value with average value of that year\nfor index,row in df_filtered[df_filtered._dewptm.isnull()].iterrows():\n    mean_val = df_filtered[df_filtered[\"year\"] == row[\"year\"]]._dewptm.mean()\n    df_filtered.at[index, \"_dewptm\"] = mean_val\n    ","b657a843":"df_filtered[df_filtered._dewptm.isnull()] # We replaced null values fof _dewtmp","1291d6f8":"df_filtered.shape","62805cfa":"df_filtered.isnull().sum()\n# so now we have only relevant columns. Lets handle them one by one.","963bdf71":"# Handle _hum column.\ndf_filtered[df_filtered._hum.isnull()]","308e17c2":"# We will use the same logic o replace as we did before.\n# We will try to replace value with average value of that year\nfor index,row in df_filtered[df_filtered._hum.isnull()].iterrows():\n    mean_val = df_filtered[df_filtered[\"year\"] == row[\"year\"]]._hum.mean()\n    df_filtered.at[index, \"_hum\"] = mean_val\n    ","36b67d1a":"df_filtered[df_filtered._hum.isnull()] # replaced","8158d7c1":"df_filtered.isnull().sum() # Now lets handle _pressurem","6b2d7996":"df_filtered[df_filtered._pressurem.isnull()]","c9afa0be":"df_filtered.head()","c4a0058e":"# if you see pressure column, there are few -9999 values. Which is obviously bad values and it can affect your\n# calculations very badly. So we will consider this also missing values. Lets convert them first to the nan\ndf_filtered._pressurem.replace(-9999.0, np.nan, inplace=True)\n","0dea486f":"df_filtered.head() # so now -9999.0 is Nan.Lets again get the number of missing values in _pressurem","a5a0db53":"df_filtered._pressurem.isnull().sum() # So u can see previsously it was 232 and now its 983. \n# We need to check the data for this kin of errors.\n# So we will use the same idea as before We will replace missing values with the mean values of _hum column\n# for that partcular year.","0ef78cfa":"\nfor index,row in df_filtered[df_filtered._pressurem.isnull()].iterrows():\n    mean_val = df_filtered[df_filtered[\"year\"] == row[\"year\"]]._pressurem.mean()\n    df_filtered.at[index, \"_pressurem\"] = mean_val\n    ","ae0aaf5e":"df_filtered.isnull().sum() # pressurem is also resolved. Lets apply same for other columns. I will make\n# it quickly. Process will be the same as above.","7dfd5fc3":"for index,row in df_filtered[df_filtered._tempm.isnull()].iterrows():\n    mean_val = df_filtered[df_filtered[\"year\"] == row[\"year\"]]._tempm.mean()\n    df_filtered.at[index, \"_tempm\"] = mean_val\n    ","fcb4a40a":"for index,row in df_filtered[df_filtered._vism.isnull()].iterrows():\n    mean_val = df_filtered[df_filtered[\"year\"] == row[\"year\"]]._vism.mean()\n    df_filtered.at[index, \"_vism\"] = mean_val","974e30ea":"for index,row in df_filtered[df_filtered._wdird.isnull()].iterrows():\n    mean_val = df_filtered[df_filtered[\"year\"] == row[\"year\"]]._wdird.mean()\n    df_filtered.at[index, \"_wdird\"] = mean_val","fcb619f9":"for index,row in df_filtered[df_filtered._wspdm.isnull()].iterrows():\n    mean_val = df_filtered[df_filtered[\"year\"] == row[\"year\"]]._wspdm.mean()\n    df_filtered.at[index, \"_wspdm\"] = mean_val","28f65678":"df_filtered.isnull().sum()","889047b1":"# As we can see _wdire is a categorical feature so we can not apply mean here. We have to get the most frequent\n# value of _wdire for a year and then replace missing value with the most frequent value.\nfor index,row in df_filtered[df_filtered._wdire.isnull()].iterrows():\n    most_frequent = df_filtered[df_filtered[\"year\"] == row[\"year\"]]._wdire.value_counts().idxmax()\n    df_filtered.at[index, \"_wdire\"] = most_frequent","db62124b":"df_filtered.isnull().sum()","153613a9":"# now we can see,  _conds which is again acategorical feature.\n# so we will apply again the same strategy as above(_wdire)\nfor index,row in df_filtered[df_filtered._conds.isnull()].iterrows():\n    most_frequent = df_filtered[df_filtered[\"year\"] == row[\"year\"]]._conds.value_counts().idxmax()\n    df_filtered.at[index, \"_conds\"] = most_frequent","80a78a42":"df_filtered.isnull().sum()\n","fa72353a":"## So finally ..... WE HAVE REPLACED ALL THE MISSING VALUES. Phew... Thats a whole big task.\n","8865a7eb":"df_filtered.year = df_filtered.year.astype(\"object\")\ndf_filtered.month = df_filtered.month.astype(\"object\")","27ebab9e":"df_filtered.dtypes","e2c684b5":"pd.crosstab(df_filtered.year, [df_filtered.month], values=df_filtered._tempm, aggfunc=\"mean\")","9257d47e":"# Heatmap for year and average temprature across the month. More red more heat, more blue less heat\nplt.figure(figsize=(15, 10));\nsns.heatmap(pd.crosstab(df_filtered.year, [df_filtered.month], values=df_filtered._tempm, aggfunc=\"mean\"),\n            cmap=\"coolwarm\", annot=True, cbar=True);\nplt.title(\"Average Temprature 1996-2016\")\nplt.plot();","447c1f1f":"df_filtered._conds.value_counts()","e20d1a16":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder","1c964283":"df_filtered.columns","933a3ac6":"feature_columns = ['_wdire', '_dewptm', '_fog', '_hail', '_hum',\n       '_pressurem', '_rain', '_snow', '_tempm', '_thunder', '_tornado',\n       '_vism', '_wdird', '_wspdm', 'year', 'month', '_conds']","4582e4f8":"# Lets create a new dataset, so that we dont change in our filtered dataset\n# We will create dataset in such a way, _wdire(categorical feature in starting position & target variable\n# at last which is _conds\ndf_final = df_filtered[feature_columns]","37649e21":"df_final.head()","091fe2b1":"df_final.dtypes","0a8b30a2":"df_final._wdire.value_counts()","7a8cb383":"wdire_dummies = pd.get_dummies(df_final[\"_wdire\"])","dbf30ff9":"df_final = pd.concat([wdire_dummies, df_final], axis=1)","f6b50269":"df_final.head()","2a352df9":"df_final.columns","d44cfbe2":"df_final.drop(\"_wdire\", inplace=True, axis=1)","1beecf00":"df_final.columns","29807ab6":"X = df_final.iloc[:, 0:-1].values\nX.shape","0bf8eba2":"y = df_final.iloc[:, -1].values","ce45b0e3":"label_encoder = LabelEncoder()","0aec3db6":"y = label_encoder.fit_transform(y)","84d7ade1":"y.shape","6cdf59c5":"# SO now our Feature Matrix(X) and target matrix y is ready","3d5d1574":"from sklearn.model_selection import train_test_split","ce7e576f":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=0) # test size =0.25 or 25%","a14d1cd8":"print(\"Shape of X_train\", X_train.shape)\nprint(\"Shape of X_test\", X_test.shape)\nprint(\"Shape of y_train\", y_train.shape)\nprint(\"Shape of y_test\", y_test.shape)","c8b50e4a":"from sklearn.tree import DecisionTreeClassifier","7abf0f07":"clf = DecisionTreeClassifier(criterion=\"entropy\", random_state=0)","538608b4":"clf.fit(X_train, y_train)","f4c1b41f":"y_pred = clf.predict(X_test)","3e31914f":"y_pred","ca1ab5dd":"from sklearn import metrics\nimport matplotlib.pyplot as plt\nprint(\"DecisionTrees's Accuracy: \", metrics.accuracy_score(y_test, y_pred))","51c0123e":"# Congrats your model is ready wth 78% of accuracy, Please provide your suggestions to increase the accuracy of the\n# model","18de17e7":"# Train & Test Split","340d9216":"<a href=\"https:\/\/www.facebook.com\/codemakerz\"><img src=\"https:\/\/scontent.ffjr1-4.fna.fbcdn.net\/v\/t1.0-9\/36189148_736466693143793_2172101683281133568_n.png?_nc_cat=107&_nc_eui2=AeHzxv3SUcQBOfijLP-cEnHkX4z9XQXdeau__2MlErWZ1x07aZ1zx1PzJUDDxL6cpr7oPqYiifggXDptgtP8W5iCoDRjcdILDBYZ5Ig40dqi8Q&_nc_oc=AQmMCNXdzelFB2rdtpk8wN8nC410Wm2yKupYfYS1FxHNejTF0Jhr1G3WIZORKRF3TvFpohMB8Puw29Txxan8CW05&_nc_ht=scontent.ffjr1-4.fna&oh=7b13627e991a4d1b508923041bd7bc22&oe=5D8A7B03\" \/>\n<\/a>\nFollow Us:\nFacebook: https:\/\/www.facebook.com\/codemakerz\n\n<h1>Delhi Weather Classification Using Decision Tree Classification<\/h1>\n<h3>Help us to increase the accuracy of model. Contact us to post your code.<\/h3>","0695750e":"# Create Model","9c0ee6c9":"# Feature & Target Matrix","de44507d":"# EDA & Data Munging","167af529":"# Problem Set","59210dba":"Given dataset provides the weather data for city Delhi, India. We will try to predict the weather _conds field. Like weather will be smoke, Haze, Clear.\n\nFor this we will use Decision Tree Classifier. You can use any other classifier to compare its accuracy.","317d029c":"We will make copy of original dataset and will take only relevant columns.","d1e11551":"# Missing Values","9aede78e":"So u can see there was a big change in year 1996-1997. It may be because of many reasons:\n1. New industries started in the city.\n2. People started purchasing more vehicles.\nor any other reasons.","9ae2f4f6":"# Accuracy","93bb8397":"# Train Model","23abd348":"Now our dataset doesn;t have any missing values in it. Now we should observe one thing. That our _windre is\na categorical column and it is also important to predict a whether but the thing is your model does not understand a text value. So we need to encode this categorical column so that we can change it to integer"}}