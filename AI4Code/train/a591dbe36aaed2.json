{"cell_type":{"46683072":"code","29c2ca88":"code","65194ac7":"code","1c0c3bf6":"code","6d048324":"code","0b032cb5":"code","899f8b7b":"code","7b16ad56":"code","405671cb":"code","e04d90ae":"code","689c1028":"code","1cdb7f52":"code","46ad46b4":"code","c5371705":"code","703d0c0f":"code","1ac34dc8":"code","30e0cd5c":"code","e30440e8":"code","ed34ab04":"code","40a7e5d1":"code","25f76b26":"code","ae5b7839":"code","a1e418c3":"code","8d043fd4":"code","6d2d909c":"code","eeeda014":"code","46c67a13":"markdown","8b18eb46":"markdown","527af315":"markdown","9ff27195":"markdown","c4ad5ff4":"markdown","ae49e382":"markdown","af09a2e7":"markdown","75e81e17":"markdown","76ac9800":"markdown","7beb504d":"markdown","d898c00a":"markdown","c605beee":"markdown","1ccaef5b":"markdown","500c8882":"markdown","b78add85":"markdown","62a35d8a":"markdown","c112857b":"markdown"},"source":{"46683072":"pip install torchsummary","29c2ca88":"import os\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\nfrom torchvision.utils import save_image\nfrom torchvision.utils import make_grid\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchsummary import summary\nimport datetime\nimport matplotlib.pyplot as plt","65194ac7":"os.environ['CUDA_VISIBLE_DEVICES'] = '0'","1c0c3bf6":"torch.manual_seed(1)","6d048324":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nbatch_size = 128","0b032cb5":"train_transform = transforms.Compose([transforms.Resize((64, 64)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\ntrain_dataset = datasets.ImageFolder(root='..\/input\/selfie2anime', transform=train_transform)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)","899f8b7b":"def show_images(images):\n    fig, ax = plt.subplots(figsize=(20, 20))\n    ax.set_xticks([]); ax.set_yticks([])\n    ax.imshow(make_grid(images.detach(), nrow=22).permute(1, 2, 0))\n\ndef show_batch(dl):\n    for images, _ in dl:\n        show_images(images)\n        break","7b16ad56":"show_batch(train_loader)","405671cb":"image_shape = (3, 64, 64)\nimage_dim = int(np.prod(image_shape))\nlatent_dim = 100","e04d90ae":"def weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        torch.nn.init.normal_(m.weight, 1.0, 0.02)\n        torch.nn.init.zeros_(m.bias)","689c1028":"class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.main = nn.Sequential(\n            nn.ConvTranspose2d(latent_dim, 64 * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(64 * 8),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(64 * 8, 64 * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64 * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(64 * 4, 64 * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64 * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(64 * 2, 64, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n\n    def forward(self, input):\n        output = self.main(input)\n        return output    ","1cdb7f52":"generator = Generator().to(device)\ngenerator.apply(weights_init)\nprint(generator)","46ad46b4":"summary(generator, (100,1,1))","c5371705":"class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64 * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64 * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64 * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64 * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid(),\n            nn.Flatten()\n        )\n\n    def forward(self, input):\n        output = self.main(input)\n        return output\n","703d0c0f":"discriminator = Discriminator().to(device)\ndiscriminator.apply(weights_init)\nprint(discriminator)","1ac34dc8":"summary(discriminator, (3,64,64))","30e0cd5c":"adversarial_loss = nn.BCELoss() ","e30440e8":"def generator_loss(fake_output, label):\n    gen_loss = adversarial_loss(fake_output, label)\n    return gen_loss","ed34ab04":"def discriminator_loss(output, label):\n    disc_loss = adversarial_loss(output, label)\n    return disc_loss","40a7e5d1":"fixed_noise = torch.randn(128, latent_dim, 1, 1, device=device)\nreal_label = 1\nfake_label = 0","25f76b26":"learning_rate = 0.0002 \nG_optimizer = optim.Adam(generator.parameters(), lr = learning_rate, betas=(0.5, 0.999))\nD_optimizer = optim.Adam(discriminator.parameters(), lr = learning_rate, betas=(0.5, 0.999))","ae5b7839":"!mkdir t_weights\n!mkdir images","a1e418c3":"num_epochs = 500\nD_loss_plot, G_loss_plot = [], []\nfor epoch in range(1, num_epochs+1): \n\n    D_loss_list, G_loss_list = [], []\n   \n    for index, (real_images, _) in enumerate(train_loader):\n        D_optimizer.zero_grad()\n        real_images = real_images.to(device)\n      \n        real_target = Variable(torch.ones(real_images.size(0)).to(device))\n        fake_target = Variable(torch.zeros(real_images.size(0)).to(device))\n        \n        real_target = real_target.unsqueeze(1)\n        fake_target = fake_target.unsqueeze(1)\n\n        D_real_loss = discriminator_loss(discriminator(real_images), real_target)\n        D_real_loss.backward()\n    \n        noise_vector = torch.randn(real_images.size(0), latent_dim, 1, 1, device=device)  \n        noise_vector = noise_vector.to(device)\n    \n        generated_image = generator(noise_vector)\n        output = discriminator(generated_image.detach())\n        D_fake_loss = discriminator_loss(output,  fake_target)\n\n        D_fake_loss.backward()\n      \n        D_total_loss = D_real_loss + D_fake_loss\n        D_loss_list.append(D_total_loss)\n\n        D_optimizer.step()\n\n        \n        G_optimizer.zero_grad()\n        G_loss = generator_loss(discriminator(generated_image), real_target)\n        G_loss_list.append(G_loss)\n\n        G_loss.backward()\n        G_optimizer.step()\n\n\n    print('Epoch: [%d\/%d]: D_loss: %.3f, G_loss: %.3f' % (\n            (epoch), num_epochs, torch.mean(torch.FloatTensor(D_loss_list)),\\\n             torch.mean(torch.FloatTensor(G_loss_list))))\n    \n    D_loss_plot.append(torch.mean(torch.FloatTensor(D_loss_list)))\n    G_loss_plot.append(torch.mean(torch.FloatTensor(G_loss_list)))\n    save_image(generated_image.data[:50], '.\/images\/sample_%d'%epoch + '.png', nrow=5, normalize=True)\n     \n    torch.save(generator.state_dict(), '.\/t_weights\/generator_epoch_%d.pth' % (epoch))\n    torch.save(discriminator.state_dict(), '.\/t_weights\/discriminator_epoch_%d.pth' % (epoch))","8d043fd4":"def getImagePaths(path):\n    image_names = []\n    for dirname, _, filenames in os.walk(path):\n        for filename in filenames:\n            fullpath = os.path.join(dirname, filename)\n            image_names.append(fullpath)\n    return image_names","6d2d909c":"import cv2\ndef display_multiple_img(images_paths, rows, cols):\n    figure, ax = plt.subplots(nrows=rows,ncols=cols,figsize=(500,100) )\n    for ind,image_path in enumerate(images_paths):\n        image=cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n        try:\n            ax.ravel()[ind].imshow(image)\n            ax.ravel()[ind].set_axis_off()\n        except:\n            continue;\n    plt.tight_layout()\n    plt.show()","eeeda014":"display_multiple_img(getImagePaths('.\/images'),num_epochs\/\/2 , 2)","46c67a13":"<div style=\"color:white;\n           display:fill;\n           border-radius:10px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:#ff05da;\n           color:Black;\n           font-family:cursive;\n            padding:5px 5px 5px 5px;\n           \">\n<h1 style=\"text-align:center;font-weight: bold;\">Exploratory Data Analysis<\/h1>\n\n\n\n<\/div>\n\n\n\n","8b18eb46":"<div style=\"color:white;\n           display:fill;\n           border-radius:15px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:#ff05da;\n           color:Black;\n           font-family:cursive;\n            padding:5px 5px 5px 5px;\n           \">\n<h1 style=\"text-align:center;font-weight: bold\">Outputing Results<\/h1>\n\n<\/div>\n\n","527af315":"- All the convolution-layer weights are initialized from a zero-centered normal distribution, with a standard deviation of 0.02. \n- The batch-normalization layer weights are initialized with a normal distribution, having mean 1 and a standard deviation of 0.02. The bias is initialized with zeros.","9ff27195":"<div style=\"color:white;\n           display:fill;\n           border-radius:10px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:#ff05da;\n           color:Black;\n           font-family:cursive;\n            padding:5px 5px 5px 5px;\n           \">\n<h1 style=\"text-align:center;font-weight: bold;\">Creating DCGAN<\/h1>\n\n\n\n<\/div>\n\n\n\n","c4ad5ff4":"<div style=\"color:white;\n           display:fill;\n           border-radius:10px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:#ff05da;\n           color:Black;\n           font-family:cursive;\n            padding:5px 5px 5px 5px;\n           \">\n<h1 style=\"text-align:center;font-weight: bold\">About the Dataset<\/h1>\n    <h3 style=\"text-align:left;font-weight: bold\">Pictures of the dataset selfie2anime use in Gan work and found in this <a href =https:\/\/github.com\/znxlwm\/UGATIT-pytorch>github<\/a><\/h3>\n\n<ul>\n\n<li>3400 women faces in trainA<\/li>\n<li>3400 anime faces in trainB<\/li>\n<li>100 women faces in testA\/li>\n<li>100 anime faces in testB<\/li>\n   \n<\/ul>\n<\/div>","ae49e382":"## The generator_loss function is fed two parameters:\n\n- fake_output: Output predictions from the discriminator, when fed generator-produced images.\n- label: Ground truth labels (1), for you would like the generator to fool the discriminator and produce real images. Hence, the labels would be one.","af09a2e7":"<div style=\"color:white;\n           display:fill;\n           border-radius:10px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:#ff05da;\n           color:Black;\n           font-family:cursive;\n            padding:5px 5px 5px 5px;\n           \">\n<h2 style=\"text-align:center;font-weight: bold;\">Training our network<\/h2>\n\n\n\n<\/div>\n\n\n\n","75e81e17":"![Image1](https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/gans_gfg.jpg)\n![](https:\/\/miro.medium.com\/max\/1400\/1*TKr1dtcNgJCA8uYY1OhmSg.png)","76ac9800":"<div style=\"color:white;\n           display:fill;\n           border-radius:10px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:#ff05da;\n           color:Black;\n           font-family:cursive;\n            padding:5px 5px 5px 5px;\n           \">\n<h2 style=\"text-align:center;font-weight: bold;\">Descriminator Network<\/h2>\n\n\n\n<\/div>\n\n\n\n","7beb504d":"<div style=\"color:white;\n           display:fill;\n           border-radius:10px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:#ff05da;\n           color:Black;\n           font-family:cursive;\n            padding:5px 5px 5px 5px;\n           \">\n<h2 style=\"text-align:center;font-weight: bold;\">Generator Network<\/h2>\n\n\n\n<\/div>\n\n\n\n","d898c00a":"## The discriminator loss has:\n\n- real (original images) output predictions, ground truth label as 1\n- fake (generated images) output predictions, ground truth label as 0.","c605beee":"<div style=\"color:white;\n           display:fill;\n           border-radius:10px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:#ff05da;\n           color:Black;\n           font-family:cursive;\n            padding:5px 5px 5px 5px;\n           \">\n<h1 style=\"text-align:center;font-weight: bold;\">Importing Dataset<\/h1>\n\n\n\n<\/div>\n\n\n\n","1ccaef5b":"<div style=\"color:white;\n           display:fill;\n           border-radius:10px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:#ff05da;\n           color:Black;\n           font-family:cursive;\n            padding:5px 5px 5px 5px;\n           \">\n<h2 style=\"text-align:center;font-weight: bold;\">Initializing Weights<\/h2>\n\n\n\n<\/div>\n\n\n\n","500c8882":"<div style=\"color:white;\n           display:fill;\n           border-radius:10px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:#ff05da;\n           color:Black;\n           font-family:cursive;\n            padding:5px 5px 5px 5px;\n           \">\n<h1 style=\"text-align:center;font-weight: bold\">GAN<\/h1>\n    <h3 style=\"text-align:left;font-weight: bold\">A generative adversarial network is a class of machine learning frameworks designed by Ian Goodfellow and his colleagues in 2014. Two neural networks contest with each other in a game. Given a training set, this technique learns to generate new data with the same statistics as the training set.<\/h3>\n\n   \n\n<\/div>","b78add85":"<div style=\"color:white;\n           display:fill;\n           border-radius:10px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:#ff05da;\n           color:Black;\n           font-family:cursive;\n            padding:5px 5px 5px 5px;\n           \">\n<h1 style=\"text-align:center;font-weight: bold;\">Importing Libraries<\/h1>\n\n\n\n<\/div>\n\n\n\n","62a35d8a":"<div style=\"color:white;\n           display:fill;\n           border-radius:15px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:#ff05da;\n           color:Black;\n           font-family:cursive;\n            padding:5px 5px 5px 5px;\n           \">\n<h1 style=\"text-align:center;font-weight: bold\">Please share your feedback in the comment section. I'll be more than happy to hear ya'll and If you liked my work and learned from it Please Consider Upvoting.<\/h1>\n\n<\/div>\n\n","c112857b":"<div style=\"color:white;\n           display:fill;\n           border-radius:10px;\n           font-size:110%;\n           font-family:cursive;\n           letter-spacing:0.5px;\n           background-color:#ff05da;\n           color:Black;\n           font-family:cursive;\n            padding:5px 5px 5px 5px;\n           \">\n<h1 style=\"text-align:center;font-weight: bold;\">Introduction<\/h1>\n\n\n\n<\/div>\n\n\n\n"}}