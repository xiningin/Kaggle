{"cell_type":{"3e570edd":"code","9af39526":"code","29911e04":"code","687fb6d0":"code","44200fa4":"code","1a185557":"code","60271601":"code","aad57962":"code","c764d7be":"code","e5a5873d":"code","c9aa0c43":"code","14376207":"code","33e73866":"code","2bd70372":"code","5081e72b":"code","a4b32e6a":"code","537aebb8":"markdown"},"source":{"3e570edd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9af39526":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport cv2\nimport glob\nimport random\nfrom wordcloud import WordCloud, STOPWORDS\n","29911e04":"train_file = pd.read_csv(\"..\/input\/shopee-product-matching\/train.csv\")\ntrain_file.head()","687fb6d0":"test_file = pd.read_csv(\"..\/input\/shopee-product-matching\/test.csv\")\ntest_file.head()","44200fa4":"# Let's find out how many images are under the directory\ntotal_train_files = glob.glob(\"..\/input\/shopee-product-matching\/train_images\/*.jpg\")\ntotal_test_files = glob.glob(\"..\/input\/shopee-product-matching\/test_images\/*.jpg\")\n\nprint(f\"Total Training Images: {len(total_train_files)}\")\nprint(f\"Total Testing Images: {len(total_test_files)}\")","1a185557":"#Let's figure out the unique number for each columns.\nfor col in train_file.columns:\n    print(col + \":\" + str(len(train_file[col].unique())))","60271601":"train_file.info()","aad57962":"train_file.nunique().to_frame().rename(columns={0:\"Unique Values\"}).style.background_gradient(cmap=\"plasma\")","c764d7be":"# Check for missing values in the training data\ntrain_file.isnull().sum()","e5a5873d":"#Image Label Groups by No. of Images\ntop10_names = train_file['label_group'].value_counts().index.tolist()[:15]\ntop10_values = train_file['label_group'].value_counts().tolist()[:15]\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=top10_names, y=top10_values)\nplt.xticks(rotation=45)\nplt.xlabel(\"Label Group\")\nplt.ylabel(\"Image Count\")\nplt.title(\"Top-15 Label Groups by Image Count\")\nplt.show()","c9aa0c43":"stopwords = set(STOPWORDS) \nwordcloud = WordCloud(width = 1000, \n                      height = 500,\n                      background_color ='white',\n                      min_font_size = 10,\n                      stopwords = stopwords,).generate(' '.join(train_file['title'])) \n\n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n\nplt.show() ","14376207":"def plot(num):\n    IMG_PATHS = \"..\/input\/shopee-product-matching\/train_images\/\"\n    sq_num = np.sqrt(num)\n    assert sq_num == int(sq_num), \"Number of Images must be a perfect Square!\"\n\n    sq_num = int(sq_num)\n    image_ids = os.listdir(IMG_PATHS)\n    random.shuffle(image_ids)\n    fig, ax = plt.subplots(nrows=sq_num, ncols=sq_num, figsize=(10, 10))\n\n    for i in range(sq_num):\n        for j in range(sq_num):\n            idx = i*sq_num + j\n            ax[i, j].axis('off')\n            img = cv2.imread(IMG_PATHS + '\/' + image_ids[idx])\n            img = img[:, :, ::-1]\n            ax[i, j].imshow(img); ax[i, j].set_title(f'{image_ids[idx]}', fontsize=6.5)\n\n    plt.show()\n    \n    \ndef plot_from_label(group):\n    IMG_PATHS = \"..\/input\/shopee-product-matching\/train_images\/\"\n    image_list = train_file[train_file['label_group'] == group]\n    image_list = image_list['image'].tolist()\n    num = len(image_list)\n    \n    sq_num = np.sqrt(num)\n\n    sq_num = int(sq_num)\n    image_ids = os.listdir(IMG_PATHS)\n    random.shuffle(image_ids)\n    fig, ax = plt.subplots(nrows=sq_num, ncols=sq_num, figsize=(10, 10))\n    \n    path = [os.path.join(IMG_PATHS, x) for x in image_list]\n    \n    for i in range(sq_num):\n        for j in range(sq_num):\n            idx = i*sq_num + j\n            ax[i, j].axis('off')\n            img = cv2.imread(path[idx])\n            img = img[:, :, ::-1]\n            ax[i, j].imshow(img)\n\n    plt.show()\n\ndef plot_from_title(title):\n    IMG_PATHS = \"..\/input\/shopee-product-matching\/train_images\/\"\n    image_list = train_file[train_file['title'] == title]\n    image_list = image_list['image'].tolist()\n    num = len(image_list)\n    \n    sq_num = np.sqrt(num)\n    sq_num = int(sq_num)\n    \n    image_ids = os.listdir(IMG_PATHS)\n    random.shuffle(image_ids)\n    fig, ax = plt.subplots(nrows=sq_num, ncols=sq_num, figsize=(10, 10))\n    fig.suptitle(f\"Product Name: {title}\")\n    path = [os.path.join(IMG_PATHS, x) for x in image_list]\n    \n    for i in range(sq_num):\n        for j in range(sq_num):\n            idx = i*sq_num + j\n            ax[i, j].axis('off')\n            img = cv2.imread(path[idx])\n            img = img[:, :, ::-1]\n            ax[i, j].imshow(img)\n            \n    plt.show()","33e73866":"plot(16)","2bd70372":"plot_from_label(994676122)","5081e72b":"plot_from_title(\"Koko syubbanul muslimin koko azzahir koko baju\")","a4b32e6a":"plot_from_title(\"Monde Boromon Cookies 1 tahun+ 120gr\")","537aebb8":"# How does the Data look like? \ud83d\uddc3\nSo, the data provided to us in this competition consists of 3 .csv files and 2 folders (training_images and testing_images).\n\nBelow is the breakdown of the .csv files and image folders;\n\n* **\ud83d\udcc4 train.csv** - This is the Training set metadata. Each row contains the data for a single posting. Multiple postings might have the exact same image ID, but with different titles or vice versa.\n* **\ud83d\udcc4 test.csv** - Same as train.csv except the label_group column. This file will be what we are going to use at inference time. Currently it only consists of 3 samples but it will be replaced by a bigger private test set at submission time.\n* **\ud83d\udcc4 sample_submission.csv **- The Sample submission file in the format we are expected to follow.\n* **\ud83d\udcc2 train_images\/** - Folder with all the training images.\n* **\ud83d\udcc2 test_images\/ **- Folder with all the testing images (again, only 4 images for now, but will be around ~70,000 images during submission)"}}