{"cell_type":{"347c0f9b":"code","953a4de9":"code","fd149c47":"code","14fa8d7d":"code","02ab50a6":"code","8fa2144b":"code","b3e79928":"code","71445594":"code","33980741":"code","573d2b69":"code","9178b620":"code","aa691b1d":"code","030aadc3":"code","df6bb436":"code","503b218e":"code","5a682d09":"code","815c536f":"code","a0281d28":"code","1a77f7a5":"code","5ed79dd3":"code","c34cf424":"code","301de18d":"code","13b78f76":"code","8bf3b2e6":"markdown","6a33a88e":"markdown","fca542cc":"markdown","441c7c11":"markdown","9c46dca9":"markdown","927451dd":"markdown","1e622cec":"markdown","7fe3807f":"markdown","3a2e60f7":"markdown"},"source":{"347c0f9b":"# clone YOLOv5 repository\n!git clone https:\/\/github.com\/ultralytics\/yolov5  # clone repo\n%cd yolov5\n!git reset --hard 886f1c03d839575afecb059accf74296fad395b6","953a4de9":"%cd \/kaggle\/working\/yolov5","fd149c47":"# install dependencies as necessary\n!pip install -qr requirements.txt  # install dependencies (ignore errors)\nimport torch\n\nfrom IPython.display import Image, clear_output  # to display images\n# from utils.google_utils import gdrive_download  # to download models\/datasets\n\n# clear_output()\nprint('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))","14fa8d7d":"!ls \/kaggle\/working\/yolov5\/utils","02ab50a6":"#follow the link below to get your download code from from Roboflow\n!pip install -q roboflow\nfrom roboflow import Roboflow\nrf = Roboflow(model_format=\"yolov6\", notebook=\"roboflow-yolov5\")","8fa2144b":"!pip install torch==1.8.1 torchvision==0.9.1","b3e79928":"%cd \/content\/yolov5\nfrom roboflow import Roboflow\nrf = Roboflow(api_key=\"3GlzkMLGOnKjnaHm1e2L\")\nproject = rf.workspace().project(\"cots-ewpgn\")\ndataset = project.version(6).download(\"yolov5\")","71445594":"# this is the YAML file Roboflow wrote for us that we're loading into this notebook with our data\n%cat {dataset.location}\/data.yaml","33980741":"# define number of classes based on YAML\nimport yaml\nwith open(dataset.location + \"\/data.yaml\", 'r') as stream:\n    num_classes = str(yaml.safe_load(stream)['nc'])","573d2b69":"#this is the model configuration we will use for our tutorial \n%cat .\/yolov5\/models\/yolov5l.yaml","9178b620":"#customize iPython writefile so we can write variables\nfrom IPython.core.magic import register_line_cell_magic\n\n@register_line_cell_magic\ndef writetemplate(line, cell):\n    with open(line, 'w') as f:\n        f.write(cell.format(**globals()))","aa691b1d":"%%writetemplate .\/models\/custom_yolov5l.yaml\n\nnc: {num_classes}  # number of classes\ndepth_multiple: 1.0  # model depth multiple\nwidth_multiple: 1.0  # layer channel multiple\n\n# anchors\nanchors:\n  - [10,13, 16,30, 33,23]  # P3\/8\n  - [30,61, 62,45, 59,119]  # P4\/16\n  - [116,90, 156,198, 373,326]  # P5\/32\n\n# YOLOv5 backbone\nbackbone:\n  # [from, number, module, args]\n  [[-1, 1, Focus, [64, 3]],  # 0-P1\/2\n   [-1, 1, Conv, [128, 3, 2]],  # 1-P2\/4\n   [-1, 3, C3, [128]],\n   [-1, 1, Conv, [256, 3, 2]],  # 3-P3\/8\n   [-1, 9, C3, [256]],\n   [-1, 1, Conv, [512, 3, 2]],  # 5-P4\/16\n   [-1, 9, C3, [512]],\n   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5\/32\n   [-1, 1, SPP, [1024, [5, 9, 13]]],\n   [-1, 3, C3, [1024, False]],  # 9\n  ]\n\n# YOLOv5 head\nhead:\n  [[-1, 1, Conv, [512, 1, 1]],\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n   [-1, 3, C3, [512, False]],  # 13\n\n   [-1, 1, Conv, [256, 1, 1]],\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n   [-1, 3, C3, [256, False]],  # 17 (P3\/8-small)\n\n   [-1, 1, Conv, [256, 3, 2]],\n   [[-1, 14], 1, Concat, [1]],  # cat head P4\n   [-1, 3, C3, [512, False]],  # 20 (P4\/16-medium)\n\n   [-1, 1, Conv, [512, 3, 2]],\n   [[-1, 10], 1, Concat, [1]],  # cat head P5\n   [-1, 3, C3, [1024, False]],  # 23 (P5\/32-large)\n\n   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n  ]","030aadc3":"# Weights & Biases  (optional)\n%pip install -q wandb\nimport wandb","df6bb436":"wandb.login()","503b218e":"!ls models","5a682d09":"%%time\n!python train.py --img 640 --batch 16 --epochs 60 --data {dataset.location}\/data.yaml --cfg .\/models\/custom_yolov5l.yaml --weights yolov5l.pt --name yolov5s_results  --cache ","815c536f":"# Start tensorboard\n# Launch after you have started training\n# logs save in the folder \"runs\"\n%load_ext tensorboard\n%tensorboard --logdir \/content\/drive\/MyDrive\/reef_1\/yolo","a0281d28":"# trained weights are saved by default in our weights folder\n%ls runs\/","1a77f7a5":"%ls runs\/detect","5ed79dd3":"# when we ran this, we saw .007 second inference time. That is 140 FPS on a TESLA P100!\n# use the best weights!\n# %cd \/content\/yolov5\n!python detect.py --weights runs\/train\/yolov5s_results\/weights\/best.pt --img 640 --conf 0.4 --source COTS-6\/valid\/images","c34cf424":"#display inference on ALL test images\n#this looks much better with longer training above\n\nimport glob\nfrom IPython.display import Image, display\n\nfor imageName in glob.glob('runs\/detect\/exp3\/*.jpg'): #assuming JPG\n    display(Image(filename=imageName))\n    print(\"\\n\")","301de18d":"from google.colab import drive\ndrive.mount('\/content\/gdrive')","13b78f76":"%cp \/content\/yolov5\/runs\/train\/yolov5s_results\/weights\/best.pt \/content\/gdrive\/My\\ Drive","8bf3b2e6":"Training losses and performance metrics are saved to Tensorboard and also to a logfile defined above with the **--name** flag when we train. In our case, we named this `yolov5s_results`. (If given no name, it defaults to `results.txt`.) The results file is plotted as a png after training completes.\n\nNote from Glenn: Partially completed `results.txt` files can be plotted with `from utils.utils import plot_results; plot_results()`.","6a33a88e":"#Run Inference  With Trained Weights\nRun inference with a pretrained checkpoint on contents of `test\/images` folder downloaded from Roboflow.","fca542cc":"#Install Dependencies\n\n_(Remember to choose GPU in Runtime if not already selected. Runtime --> Change Runtime Type --> Hardware accelerator --> GPU)_","441c7c11":"# Evaluate Custom YOLOv5 Detector Performance","9c46dca9":"### Curious? Visualize Our Training Data with Labels\n\nAfter training starts, view `train*.jpg` images to see training images, labels and augmentation effects.\n\nNote a mosaic dataloader is used for training (shown below), a new dataloading concept developed by Glenn Jocher and first featured in [YOLOv4](https:\/\/arxiv.org\/abs\/2004.10934).","927451dd":"# Export Trained Weights for Future Inference\n\nNow that you have trained your custom detector, you can export the trained weights you have made here for inference on your device elsewhere","1e622cec":"## Congrats!\n\nHope you enjoyed this!\n\n--Team [Roboflow](https:\/\/roboflow.ai)","7fe3807f":"# Define Model Configuration and Architecture\n\nWe will write a yaml script that defines the parameters for our model like the number of classes, anchors, and each layer.\n\nYou do not need to edit these cells, but you may.","3a2e60f7":"# Train Custom YOLOv5 Detector\n\n### Next, we'll fire off training!\n\n\nHere, we are able to pass a number of arguments:\n- **img:** define input image size\n- **batch:** determine batch size\n- **epochs:** define the number of training epochs. (Note: often, 3000+ are common here!)\n- **data:** set the path to our yaml file\n- **cfg:** specify our model configuration\n- **weights:** specify a custom path to weights. (Note: you can download weights from the Ultralytics Google Drive [folder](https:\/\/drive.google.com\/open?id=1Drs_Aiu7xx6S-ix95f9kNsA6ueKRpN2J))\n- **name:** result names\n- **nosave:** only save the final checkpoint\n- **cache:** cache images for faster training"}}