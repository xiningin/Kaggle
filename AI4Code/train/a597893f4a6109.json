{"cell_type":{"5733182a":"code","d6693933":"code","370a4bed":"code","70fb8dc7":"code","33e8ea74":"code","ab56b184":"code","5d9ec7ec":"code","908858d7":"code","c013addf":"code","88480f0b":"code","f8b5f5cc":"code","aac337fa":"code","654c018c":"code","daa5ee16":"code","a76d1e03":"code","520f72a4":"code","7b68d7fc":"code","408fe302":"code","b7ec06c3":"code","e25f6035":"code","54d7795a":"code","8d80f6c9":"code","a26dd636":"code","753e3bc4":"code","b978a8a8":"code","e17b7d4b":"code","de6a8d8d":"code","7cdd4835":"code","fd8f4616":"code","5cc290d0":"code","5e86d567":"code","00c498a0":"code","9ad9ad36":"code","3e4f29eb":"markdown","cb8b28ce":"markdown","56eb07ea":"markdown","e2fcb51e":"markdown","d075d6e5":"markdown","10cd044b":"markdown","69d4fdb6":"markdown","875aeb3e":"markdown","3f45a496":"markdown","fad720d4":"markdown","245dfe4f":"markdown","7383dbd7":"markdown","4f637e27":"markdown","e78f6a9e":"markdown","c2d33c7e":"markdown","c352479f":"markdown"},"source":{"5733182a":"import json\nimport math\nimport os\n\nimport cv2\nfrom PIL import Image\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score\nimport scipy\nfrom tqdm import tqdm\nimport gc\nfrom functools import partial\nfrom sklearn import metrics\nfrom collections import Counter\nimport json\nimport itertools\n\n%matplotlib inline","d6693933":"!pip install efficientnet\nimport efficientnet.tfkeras as efn ","370a4bed":"SEED = 42\nnp.random.seed(SEED)\ntf.set_random_seed(SEED)","70fb8dc7":"from skimage.filters import rank, threshold_otsu\nfrom skimage.color import rgb2grey\nfrom sklearn.cluster import KMeans\nfrom skimage.morphology import closing, square, disk","33e8ea74":"def plot_any(arr, title = ''):\n    \"\"\"\n    plot multiple pictures\n    \"\"\"\n    plt.figure(figsize = (15, 25))\n    for i in range(len(arr)):\n        plt.subplot(1,len(arr),i + 1)\n        plt.title(title)\n        plt.imshow(arr[i]);\n\n        \ndef d2Kmeans(img, k):\n    \"\"\"\n    Apply 2 dimensional KMeans algorithm on pictures\n    \"\"\"\n    return KMeans(n_jobs=-1, \n                  random_state=1, \n                  n_clusters = k, \n                  init='k-means++'\n    ).fit(img.reshape((-1,1))).labels_.reshape(img.shape)\n\n\ndef merge_segmented_mask_ROI(uri_img, img_kluster):\n    \"\"\"\n    Merge original pricture and segmented picture\n    \"\"\"\n    new_img = uri_img.copy()\n    for ch in range(3):\n        new_img[:,:, ch] *= img_kluster\n    return new_img\n\n\ndef mean_filter(image, radius):\n    \"\"\"\n    Create smooth boundaries of segmenation thourgh applying a gaussian mean blur\n    \"\"\"\n    return rank.mean_percentile(image, selem = disk(radius))\n\n\ndef binary(image):\n    \"\"\"\n    Get round boundaries of the image when segmenting\n    \"\"\"\n    return image > threshold_otsu(image)\n\n\ndef select_cluster_index(clusters):\n    \"\"\"\n    Chooose the right cluster index, which is the smallest, as the rest is background\n    \"\"\"\n    minx = clusters[0].mean()\n    index = 0\n    for i in clusters:\n        if i.mean() < minx:\n            minx = i.mean()\n            index += 1\n    return index\n\n\ndef segment_image(img, k = 2):\n    \"\"\"\n    segment the image in skin mole versus background\n    \"\"\"\n    # Cluster the image\n    result_gray = d2Kmeans(rgb2grey(img), k)\n    \n    # Select the correct cluster\n    clusters_gray = [result_gray == i for i in range(k)]\n    selected_index = select_cluster_index(clusters_gray)\n    results_gray = clusters_gray[selected_index]\n    \n    # Apply smoothing of the boundaries\n    image_mean_filter = mean_filter(results_gray, 20)\n    test_binary = binary(image_mean_filter)\n    \n    # Create segmented picture with black background\n    new_img = merge_segmented_mask_ROI(img, test_binary)\n    \n    return new_img","ab56b184":"#Transfer 'jpg' images to an array IMG\ndef Dataset_loader(DIR, RESIZE):\n    IMG = []\n    read = lambda imname: np.asarray(Image.open(imname).convert(\"RGB\"))\n    for IMAGE_NAME in tqdm(os.listdir(DIR)):\n        PATH = os.path.join(DIR,IMAGE_NAME)\n        _, ftype = os.path.splitext(PATH)\n        if ftype == \".jpg\":\n            img = read(PATH)\n            img = cv2.resize(img, (RESIZE,RESIZE))\n            img = segment_image(img)\n            IMG.append(np.array(img))\n    return IMG\n\nbenign_train = np.array(Dataset_loader('..\/input\/skin-cancer-malignant-vs-benign\/data\/train\/benign',224))\nmalign_train = np.array(Dataset_loader('..\/input\/skin-cancer-malignant-vs-benign\/data\/train\/malignant',224))\nbenign_test = np.array(Dataset_loader('..\/input\/skin-cancer-malignant-vs-benign\/data\/test\/benign',224))\nmalign_test = np.array(Dataset_loader('..\/input\/skin-cancer-malignant-vs-benign\/data\/test\/malignant',224))","5d9ec7ec":"plot_any(benign_train[:5], title = \"Benign Train\")\nplot_any(malign_train[:5], title = \"Malignant Train\")\nplot_any(benign_test[:5], title = \"Benign Test\")\nplot_any(malign_test[:5], title = \"Malignant Test\")","908858d7":"# Skin Cancer: Malignant vs. Benign\n# Create labels\nbenign_train_label = np.zeros(len(benign_train))\nmalign_train_label = np.ones(len(malign_train))\nbenign_test_label = np.zeros(len(benign_test))\nmalign_test_label = np.ones(len(malign_test))\n\n# Merge data \nX_train = np.concatenate((benign_train, malign_train), axis = 0)\nY_train = np.concatenate((benign_train_label, malign_train_label), axis = 0)\nX_test = np.concatenate((benign_test, malign_test), axis = 0)\nY_test = np.concatenate((benign_test_label, malign_test_label), axis = 0)\n\n# Shuffle train data\ns = np.arange(X_train.shape[0])\nnp.random.shuffle(s)\nX_train = X_train[s]\nY_train = Y_train[s]\n\n# Shuffle test data\ns = np.arange(X_test.shape[0])\nnp.random.shuffle(s)\nX_test = X_test[s]\nY_test = Y_test[s]\n\n# To categorical\nY_train = tf.keras.utils.to_categorical(Y_train, num_classes= 2)\nY_test = tf.keras.utils.to_categorical(Y_test, num_classes= 2)","c013addf":"x_train, x_val, y_train, y_val = train_test_split(\n    X_train, Y_train, \n    test_size=0.2, \n    random_state=SEED\n)","88480f0b":"\n# # Display first 15 images of moles, and how they are classified\nw=60\nh=40\nfig=plt.figure(figsize=(15, 15))\ncolumns = 4\nrows = 3\n\nfor i in range(1, columns*rows +1):\n    ax = fig.add_subplot(rows, columns, i)\n    if np.argmax(Y_train[i]) == 0:\n        ax.title.set_text('Benign')\n    else:\n        ax.title.set_text('Malignant')\n    plt.imshow(x_train[i], interpolation='nearest')\nplt.show()","f8b5f5cc":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nBATCH_SIZE = 32\n\n# Add Image augmentation to our generator\ntrain_datagen = ImageDataGenerator(rescale=1.\/255,\n                                   rotation_range=360,\n                                   horizontal_flip=True,\n                                   vertical_flip=True,\n                                   width_shift_range=0.1,\n                                   height_shift_range=0.1,\n                                   zoom_range=(0.75,1),\n                                   brightness_range=(0.75,1.25)\n                                  )\n\nval_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntrain_generator = train_datagen.flow(x_train, y_train, batch_size=BATCH_SIZE)\nval_generator = train_datagen.flow(x_val, y_val, batch_size=BATCH_SIZE, shuffle= False)\ntest_generator = test_datagen.flow(X_test, Y_test, batch_size=BATCH_SIZE, shuffle= False)","aac337fa":"from skimage import io\n\ndef imshow(image_RGB):\n    io.imshow(image_RGB)\n    io.show()\n\nx1, y1 = train_generator[0]\nx2, y2 = val_generator[0]\nx3, y3 = test_generator[0]\n\nimshow(x1[0])\nimshow(x2[0])\nimshow(x3[0])","654c018c":"def build_model(backbone, lr=1e-4):\n    model = tf.keras.Sequential()\n    model.add(backbone)\n    model.add(tf.keras.layers.GlobalAveragePooling2D())\n    model.add(tf.keras.layers.Dropout(0.5))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Dense(2, activation='softmax'))\n    \n    return model","daa5ee16":"from tensorflow.python.keras.optimizer_v2.optimizer_v2 import OptimizerV2\nfrom tensorflow.python import ops, math_ops, state_ops, control_flow_ops\nfrom tensorflow.python.keras import backend as K\n\n__all__ = ['RAdam']\n\n\nclass RAdam(OptimizerV2):\n    \"\"\"RAdam optimizer.\n    According to the paper\n    [On The Variance Of The Adaptive Learning Rate And Beyond](https:\/\/arxiv.org\/pdf\/1908.03265v1.pdf).\n    \"\"\"\n\n    def __init__(self,\n                 learning_rate=0.001,\n                 beta_1=0.9,\n                 beta_2=0.999,\n                 epsilon=1e-7,\n                 weight_decay=0.,\n                 amsgrad=False,\n                 total_steps=0,\n                 warmup_proportion=0.1,\n                 min_lr=0.,\n                 name='RAdam',\n                 **kwargs):\n        r\"\"\"Construct a new Adam optimizer.\n        Args:\n            learning_rate: A Tensor or a floating point value.    The learning rate.\n            beta_1: A float value or a constant float tensor. The exponential decay\n                rate for the 1st moment estimates.\n            beta_2: A float value or a constant float tensor. The exponential decay\n                rate for the 2nd moment estimates.\n            epsilon: A small constant for numerical stability. This epsilon is\n                \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n                Section 2.1), not the epsilon in Algorithm 1 of the paper.\n            weight_decay: A floating point value. Weight decay for each param.\n            amsgrad: boolean. Whether to apply AMSGrad variant of this algorithm from\n                the paper \"On the Convergence of Adam and beyond\".\n            total_steps: An integer. Total number of training steps.\n                Enable warmup by setting a positive value.\n            warmup_proportion: A floating point value. The proportion of increasing steps.\n            min_lr: A floating point value. Minimum learning rate after warmup.\n            name: Optional name for the operations created when applying gradients.\n                Defaults to \"Adam\".    @compatibility(eager) When eager execution is\n                enabled, `learning_rate`, `beta_1`, `beta_2`, and `epsilon` can each be\n                a callable that takes no arguments and returns the actual value to use.\n                This can be useful for changing these values across different\n                invocations of optimizer functions. @end_compatibility\n            **kwargs: keyword arguments. Allowed to be {`clipnorm`, `clipvalue`, `lr`,\n                `decay`}. `clipnorm` is clip gradients by norm; `clipvalue` is clip\n                gradients by value, `decay` is included for backward compatibility to\n                allow time inverse decay of learning rate. `lr` is included for backward\n                compatibility, recommended to use `learning_rate` instead.\n        \"\"\"\n\n        super(RAdam, self).__init__(name, **kwargs)\n        self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))\n        self._set_hyper('beta_1', beta_1)\n        self._set_hyper('beta_2', beta_2)\n        self._set_hyper('decay', self._initial_decay)\n        self._set_hyper('weight_decay', weight_decay)\n        self._set_hyper('total_steps', float(total_steps))\n        self._set_hyper('warmup_proportion', warmup_proportion)\n        self._set_hyper('min_lr', min_lr)\n        self.epsilon = epsilon or K.epsilon()\n        self.amsgrad = amsgrad\n        self._initial_weight_decay = weight_decay\n        self._initial_total_steps = total_steps\n\n    def _create_slots(self, var_list):\n        for var in var_list:\n            self.add_slot(var, 'm')\n        for var in var_list:\n            self.add_slot(var, 'v')\n        if self.amsgrad:\n            for var in var_list:\n                self.add_slot(var, 'vhat')\n\n    def set_weights(self, weights):\n        params = self.weights\n        num_vars = int((len(params) - 1) \/ 2)\n        if len(weights) == 3 * num_vars + 1:\n            weights = weights[:len(params)]\n        super(RAdam, self).set_weights(weights)\n\n    def _resource_apply_dense(self, grad, var):\n        var_dtype = var.dtype.base_dtype\n        lr_t = self._decayed_lr(var_dtype)\n        m = self.get_slot(var, 'm')\n        v = self.get_slot(var, 'v')\n        beta_1_t = self._get_hyper('beta_1', var_dtype)\n        beta_2_t = self._get_hyper('beta_2', var_dtype)\n        epsilon_t = ops.convert_to_tensor(self.epsilon, var_dtype)\n        local_step = math_ops.cast(self.iterations + 1, var_dtype)\n        beta_1_power = math_ops.pow(beta_1_t, local_step)\n        beta_2_power = math_ops.pow(beta_2_t, local_step)\n\n        if self._initial_total_steps > 0:\n            total_steps = self._get_hyper('total_steps', var_dtype)\n            warmup_steps = total_steps * self._get_hyper('warmup_proportion', var_dtype)\n            min_lr = self._get_hyper('min_lr', var_dtype)\n            decay_steps = K.maximum(total_steps - warmup_steps, 1)\n            decay_rate = (min_lr - lr_t) \/ decay_steps\n            lr_t = tf.where(\n                local_step <= warmup_steps,\n                lr_t * (local_step \/ warmup_steps),\n                lr_t + decay_rate * K.minimum(local_step - warmup_steps, decay_steps),\n            )\n\n        sma_inf = 2.0 \/ (1.0 - beta_2_t) - 1.0\n        sma_t = sma_inf - 2.0 * local_step * beta_2_power \/ (1.0 - beta_2_power)\n\n        m_t = state_ops.assign(m,\n                               beta_1_t * m + (1.0 - beta_1_t) * grad,\n                               use_locking=self._use_locking)\n        m_corr_t = m_t \/ (1.0 - beta_1_power)\n\n        v_t = state_ops.assign(v,\n                               beta_2_t * v + (1.0 - beta_2_t) * math_ops.square(grad),\n                               use_locking=self._use_locking)\n        if self.amsgrad:\n            vhat = self.get_slot(var, 'vhat')\n            vhat_t = state_ops.assign(vhat,\n                                      math_ops.maximum(vhat, v_t),\n                                      use_locking=self._use_locking)\n            v_corr_t = math_ops.sqrt(vhat_t \/ (1.0 - beta_2_power))\n        else:\n            vhat_t = None\n            v_corr_t = math_ops.sqrt(v_t \/ (1.0 - beta_2_power))\n\n        r_t = math_ops.sqrt((sma_t - 4.0) \/ (sma_inf - 4.0) *\n                            (sma_t - 2.0) \/ (sma_inf - 2.0) *\n                            sma_inf \/ sma_t)\n\n        var_t = tf.where(sma_t >= 5.0, r_t * m_corr_t \/ (v_corr_t + epsilon_t), m_corr_t)\n\n        if self._initial_weight_decay > 0.0:\n            var_t += self._get_hyper('weight_decay', var_dtype) * var\n\n        var_update = state_ops.assign_sub(var,\n                                          lr_t * var_t,\n                                          use_locking=self._use_locking)\n\n        updates = [var_update, m_t, v_t]\n        if self.amsgrad:\n            updates.append(vhat_t)\n        return control_flow_ops.group(*updates)\n\n    def _resource_apply_sparse(self, grad, var, indices):\n        var_dtype = var.dtype.base_dtype\n        lr_t = self._decayed_lr(var_dtype)\n        beta_1_t = self._get_hyper('beta_1', var_dtype)\n        beta_2_t = self._get_hyper('beta_2', var_dtype)\n        epsilon_t = ops.convert_to_tensor(self.epsilon, var_dtype)\n        local_step = math_ops.cast(self.iterations + 1, var_dtype)\n        beta_1_power = math_ops.pow(beta_1_t, local_step)\n        beta_2_power = math_ops.pow(beta_2_t, local_step)\n\n        if self._initial_total_steps > 0:\n            total_steps = self._get_hyper('total_steps', var_dtype)\n            warmup_steps = total_steps * self._get_hyper('warmup_proportion', var_dtype)\n            min_lr = self._get_hyper('min_lr', var_dtype)\n            decay_steps = K.maximum(total_steps - warmup_steps, 1)\n            decay_rate = (min_lr - lr_t) \/ decay_steps\n            lr_t = tf.where(\n                local_step <= warmup_steps,\n                lr_t * (local_step \/ warmup_steps),\n                lr_t + decay_rate * K.minimum(local_step - warmup_steps, decay_steps),\n            )\n\n        sma_inf = 2.0 \/ (1.0 - beta_2_t) - 1.0\n        sma_t = sma_inf - 2.0 * local_step * beta_2_power \/ (1.0 - beta_2_power)\n\n        m = self.get_slot(var, 'm')\n        m_scaled_g_values = grad * (1 - beta_1_t)\n        m_t = state_ops.assign(m, m * beta_1_t, use_locking=self._use_locking)\n        with ops.control_dependencies([m_t]):\n            m_t = self._resource_scatter_add(m, indices, m_scaled_g_values)\n        m_corr_t = m_t \/ (1.0 - beta_1_power)\n\n        v = self.get_slot(var, 'v')\n        v_scaled_g_values = (grad * grad) * (1 - beta_2_t)\n        v_t = state_ops.assign(v, v * beta_2_t, use_locking=self._use_locking)\n        with ops.control_dependencies([v_t]):\n            v_t = self._resource_scatter_add(v, indices, v_scaled_g_values)\n\n        if self.amsgrad:\n            vhat = self.get_slot(var, 'vhat')\n            vhat_t = state_ops.assign(vhat,\n                                      math_ops.maximum(vhat, v_t),\n                                      use_locking=self._use_locking)\n            v_corr_t = math_ops.sqrt(vhat_t \/ (1.0 - beta_2_power))\n        else:\n            vhat_t = None\n            v_corr_t = math_ops.sqrt(v_t \/ (1.0 - beta_2_power))\n\n        r_t = math_ops.sqrt((sma_t - 4.0) \/ (sma_inf - 4.0) *\n                            (sma_t - 2.0) \/ (sma_inf - 2.0) *\n                            sma_inf \/ sma_t)\n\n        var_t = tf.where(sma_t >= 5.0, r_t * m_corr_t \/ (v_corr_t + epsilon_t), m_corr_t)\n\n        if self._initial_weight_decay > 0.0:\n            var_t += self._get_hyper('weight_decay', var_dtype) * var\n\n        var_update = self._resource_scatter_add(var, indices, tf.gather(-lr_t * var_t, indices))\n\n        updates = [var_update, m_t, v_t]\n        if self.amsgrad:\n            updates.append(vhat_t)\n        return control_flow_ops.group(*updates)\n\n    def get_config(self):\n        config = super(RAdam, self).get_config()\n        config.update({\n            'learning_rate': self._serialize_hyperparameter('learning_rate'),\n            'beta_1': self._serialize_hyperparameter('beta_1'),\n            'beta_2': self._serialize_hyperparameter('beta_2'),\n            'decay': self._serialize_hyperparameter('decay'),\n            'weight_decay': self._serialize_hyperparameter('weight_decay'),\n            'epsilon': self.epsilon,\n            'amsgrad': self.amsgrad,\n            'total_steps': self._serialize_hyperparameter('total_steps'),\n            'warmup_proportion': self._serialize_hyperparameter('warmup_proportion'),\n            'min_lr': self._serialize_hyperparameter('min_lr'),\n        })\n        return config","a76d1e03":"efficientnetb3 = efn.EfficientNetB0(\n        weights='imagenet',\n        input_shape=(224,224,3),\n        include_top=False\n                   )\n\nmodel = build_model(efficientnetb3)\nmodel.summary()\n","520f72a4":"model.compile(\n        loss='categorical_crossentropy',\n        optimizer = RAdam(learning_rate=1e-3, \n                          min_lr=1e-7,\n                          warmup_proportion=0.15),\n        metrics=['accuracy']\n    )","7b68d7fc":"# Learning Rate Reducer\nlearn_control = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', \n                                  patience=5,\n                                  verbose=1,\n                                  factor=0.2, \n                                  min_lr=1e-7)\n\n# Checkpoint\nfilepath=\"weights.best.hdf5\"\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')","408fe302":"history = model.fit_generator(\n    train_generator,\n    steps_per_epoch=x_train.shape[0] \/\/ BATCH_SIZE,\n    epochs=30,\n    validation_data=val_generator,\n    validation_steps = x_val.shape[0] \/\/ BATCH_SIZE,\n    callbacks=[learn_control, checkpoint]\n)","b7ec06c3":"with open('history.json', 'w') as f:\n    json.dump(str(history.history), f)","e25f6035":"history_df = pd.DataFrame(history.history)\nhistory_df[['acc', 'val_acc']].plot()","54d7795a":"history_df = pd.DataFrame(history.history)\nhistory_df[['loss', 'val_loss']].plot()","8d80f6c9":"model.load_weights(\"weights.best.hdf5\")","a26dd636":"val_generator.reset()\nY_val_pred = model.predict_generator(val_generator, steps=np.ceil(x_val.shape[0]\/BATCH_SIZE))\naccuracy_score(np.argmax(y_val, axis=1), np.argmax(Y_val_pred, axis=1))","753e3bc4":"test_generator.reset()\nY_pred = model.predict_generator(test_generator, steps=np.ceil(X_test.shape[0]\/BATCH_SIZE))\naccuracy_score(np.argmax(Y_test, axis=1), np.argmax(Y_pred, axis=1))","b978a8a8":"tta_steps = 10\npredictions = []\n\nfor i in tqdm(range(tta_steps)):\n    test_generator = train_datagen.flow(X_test, Y_test, batch_size=BATCH_SIZE, shuffle= False)\n    preds = model.predict_generator(test_generator, steps=np.ceil(X_test.shape[0]\/BATCH_SIZE))\n    predictions.append(preds)\n\n    del test_generator\n    gc.collect()\n    \nY_pred_tta = np.mean(predictions, axis=0)","e17b7d4b":"accuracy_score(np.argmax(Y_test, axis=1), np.argmax(Y_pred, axis=1))","de6a8d8d":"accuracy_score(np.argmax(Y_test, axis=1), np.argmax(Y_pred_tta, axis=1))","7cdd4835":"from sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=55)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n\ncm = confusion_matrix(np.argmax(Y_test, axis=1), np.argmax(Y_pred, axis=1))\n\ncm_plot_label =['benign', 'malignant']\nplot_confusion_matrix(cm, cm_plot_label, title ='Confusion Metrix for Skin Cancer')","fd8f4616":"cm = confusion_matrix(np.argmax(Y_test, axis=1), np.argmax(Y_pred_tta, axis=1))\n\ncm_plot_label =['benign', 'malignant']\nplot_confusion_matrix(cm, cm_plot_label, title ='Confusion Metrix for Skin Cancer')","5cc290d0":"from sklearn.metrics import classification_report\nclassification_report( np.argmax(Y_test, axis=1), np.argmax(Y_pred_tta, axis=1))","5e86d567":"from sklearn.metrics import roc_auc_score, auc\nfrom sklearn.metrics import roc_curve\nroc_log = roc_auc_score(np.argmax(Y_test, axis=1), np.argmax(Y_pred_tta, axis=1))\nfalse_positive_rate, true_positive_rate, threshold = roc_curve(np.argmax(Y_test, axis=1), np.argmax(Y_pred_tta, axis=1))\narea_under_curve = auc(false_positive_rate, true_positive_rate)\n\nplt.plot([0, 1], [0, 1], 'r--')\nplt.plot(false_positive_rate, true_positive_rate, label='AUC = {:.3f}'.format(area_under_curve))\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc='best')\nplt.show()\n#plt.savefig(ROC_PLOT_FILE, bbox_inches='tight')\nplt.close()","00c498a0":"i=0\nprop_class=[]\nmis_class=[]\n\nfor i in range(len(Y_test)):\n    if(np.argmax(Y_test[i])==np.argmax(Y_pred_tta[i])):\n        prop_class.append(i)\n    if(len(prop_class)==8):\n        break\n\ni=0\nfor i in range(len(Y_test)):\n    if(not np.argmax(Y_test[i])==np.argmax(Y_pred_tta[i])):\n        mis_class.append(i)\n    if(len(mis_class)==8):\n        break\n\n# # Display first 8 images of benign\nw=60\nh=40\nfig=plt.figure(figsize=(18, 10))\ncolumns = 4\nrows = 2\n\ndef Transfername(namecode):\n    if namecode==0:\n        return \"Benign\"\n    else:\n        return \"Malignant\"\n    \nfor i in range(len(prop_class)):\n    ax = fig.add_subplot(rows, columns, i+1)\n    ax.set_title(\"Predicted result:\"+ Transfername(np.argmax(Y_pred_tta[prop_class[i]]))\n                       +\"\\n\"+\"Actual result: \"+ Transfername(np.argmax(Y_test[prop_class[i]])))\n    plt.imshow(X_test[prop_class[i]], interpolation='nearest')\nplt.show()","9ad9ad36":"# save model\n# serialize model to JSON\nmodel_json = model.to_json()\n\nwith open(\"efficientnetb0.json\", \"w\") as json_file:\n    json_file.write(model_json)\n    \n# serialize weights to HDF5\nmodel.save_weights(\"efficientnet.h5\")\nprint(\"Saved model to disk\")","3e4f29eb":"# Loading & Preprocessing","cb8b28ce":"# Save Model","56eb07ea":"# Data Generator","e2fcb51e":"## Image Segmentation with KMeans","d075d6e5":"# Training & Evaluation","10cd044b":"### Confusion Matrix","69d4fdb6":"# Prediction","875aeb3e":"# Create Label","3f45a496":"# Skin Cancer Detection Model\n### Special Thanks to:\n- KonG for the Data Loader and Display functions: https:\/\/www.kaggle.com\/kocayinana\/skin-cancer-recognition-with-resnet-50\n- Martin Kondor for the Metrics Plots: https:\/\/www.kaggle.com\/martinkondor\/skin-cancer-detection-with-cnn","fad720d4":"# Display Some Images","245dfe4f":"# Model:","7383dbd7":"# Test Time Augmentation\nA Method I have seen in Kernels of the APTOS blindness challenge, were the final prediction is the mean of predictions which went through image generators","4f637e27":"# Evaluation\n### Accuracy Score","e78f6a9e":"### ROC and AUC","c2d33c7e":"### Classification Report","c352479f":"# Train and Evalutation split\nNow we can split it into a training and validation set."}}