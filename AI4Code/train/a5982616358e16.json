{"cell_type":{"1b6d7e10":"code","3412e604":"code","277c696b":"code","7d6e0217":"code","9f3432ae":"code","15175458":"code","4caaa8fb":"code","998e3131":"code","c89b1be3":"code","35822546":"code","c9679d37":"code","943acce7":"code","106d7313":"markdown","7f5b2f8b":"markdown","dab68ced":"markdown","c24b6c02":"markdown","a5d70d08":"markdown"},"source":{"1b6d7e10":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3412e604":"human=pd.read_table('\/kaggle\/input\/humandata\/human_data.txt')","277c696b":"def getKmers(sequence, size=6):\n    return [sequence[x:x+size].lower() for x in range(len(sequence) - size + 1)]","7d6e0217":"human['words'] = human.apply(lambda x: getKmers(x['sequence']), axis=1)\nhuman = human.drop('sequence', axis=1)","9f3432ae":"human.head()","15175458":"human_text=list(human['words'])\nprint(human_text[0])\nfor item in range(len(human_text)):\n    human_text[item]=\" \".join(human_text[item])\ny_data=human.iloc[:,0].values","4caaa8fb":"from sklearn.feature_extraction.text import CountVectorizer\ncv=CountVectorizer(ngram_range=(4,4))\nX=cv.fit_transform(human_text)","998e3131":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline  \nhuman['class'].value_counts().sort_index().plot.bar()","c89b1be3":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y_data, \n                                                    test_size = 0.20, \n                                                    random_state=42)","35822546":"from sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB(alpha=0.1)\nclassifier.fit(X_train, y_train)","c9679d37":"y_pred = classifier.predict(X_test)\n","943acce7":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nprint(\"Confusion matrix\\n\")\nprint(pd.crosstab(pd.Series(y_test, name='Actual'), pd.Series(y_pred, name='Predicted')))\ndef get_metrics(y_test, y_predicted):\n    accuracy = accuracy_score(y_test, y_predicted)\n    precision = precision_score(y_test, y_predicted, average='weighted')\n    recall = recall_score(y_test, y_predicted, average='weighted')\n    f1 = f1_score(y_test, y_predicted, average='weighted')\n    return accuracy, precision, recall, f1\naccuracy, precision, recall, f1 = get_metrics(y_test, y_pred)\nprint(\"accuracy = %.3f \\nprecision = %.3f \\nrecall = %.3f \\nf1 = %.3f\" % (accuracy, precision, recall, f1))","106d7313":"**APPLYING K-MERS FUCTIONALITY OF NLP **\nas DNA sequences are huge string we are breaking them in in strings of every k length and using them as vectors.for more information you can google it","7f5b2f8b":"**so we have an accuracy of 98%.**","dab68ced":"please upvote the kernel if you find it useful.","c24b6c02":"So now that we know how to transform our DNA sequences into uniform length numerical vectors in the form of k-mer counts and ngrams, we can now go ahead and build a classification model that can predict the DNA sequence function based only on the sequence itself.","a5d70d08":"*checking for imbalanced  data*\nif there will be then we will be using downsampling or oversampling"}}