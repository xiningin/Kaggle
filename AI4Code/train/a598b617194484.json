{"cell_type":{"ce978998":"code","3259d2b0":"code","de7994db":"code","f1a46300":"code","6426b718":"code","63055380":"code","51eaa633":"code","966fcd71":"code","8feb53f3":"code","0425d60c":"code","0e36c352":"code","e9e9a49b":"code","4bb74be5":"code","7676e87c":"code","d961d1af":"code","bf7b2eb9":"code","89d6aaae":"code","1d0361db":"code","2de293ca":"code","2809f652":"code","4385af8f":"code","998b54cc":"code","1a17be43":"code","af32669c":"code","7a8741c3":"code","0045f2f5":"code","f07142d1":"code","d112ba6b":"code","80d23a30":"code","4c82168a":"code","fd002749":"code","b45760d3":"code","b94e69ef":"code","b51e3c24":"code","fd737309":"code","42458a43":"code","6b07cb96":"code","a9bc2766":"code","a68baf7c":"code","1bdc3f9b":"code","b3d958d8":"code","adaba698":"code","30cea435":"code","3e3ae849":"code","9f420486":"code","d9a8ebd7":"code","38ab5c31":"code","68d6367d":"code","6c973a70":"code","050512d8":"code","ef4dc6fd":"code","d78f5720":"markdown","b180e43e":"markdown","89254011":"markdown","20b88ed1":"markdown","73964b70":"markdown","70a9d21e":"markdown","2ad8e3e4":"markdown","5061dbd8":"markdown","e54b62c7":"markdown","c3ae8c45":"markdown","c1a1025d":"markdown","9d91a135":"markdown","26ac0a0d":"markdown","f31528bf":"markdown","4c72efba":"markdown","e042520d":"markdown","032ef6bf":"markdown","b61487db":"markdown","09e2fe52":"markdown","d7d96db6":"markdown","8b0f8f92":"markdown","17581399":"markdown","0ca6659c":"markdown","166151b9":"markdown","0d772b2a":"markdown","4c2de73e":"markdown","4b7691aa":"markdown","cfe36a55":"markdown","77f8df1d":"markdown","941644a7":"markdown","b075eb1f":"markdown","368e6caa":"markdown","cdc4a392":"markdown","b8db710b":"markdown","4058a6b5":"markdown","d0f7b25f":"markdown","aeb67ec5":"markdown","bc3857fc":"markdown","ae966ee5":"markdown","bfc71322":"markdown","aa59ef56":"markdown","8716a652":"markdown","31b9e865":"markdown","7f291b61":"markdown","66388f2b":"markdown","b2bb5493":"markdown","fb6cf066":"markdown","f492898c":"markdown","6939e126":"markdown","74dda090":"markdown","9b24278d":"markdown","14230b31":"markdown","c3fd20dc":"markdown","72702518":"markdown","4b5a8a0d":"markdown","1868ba0d":"markdown","c4911696":"markdown","1161c011":"markdown","49ef1ae5":"markdown","df33b2c9":"markdown","c088739b":"markdown","2fbdd21c":"markdown","a43838cd":"markdown","25d49b79":"markdown","df16df87":"markdown","079144e5":"markdown","b1699a76":"markdown","e7bc340e":"markdown","28a87d99":"markdown","5781083e":"markdown","a7c73136":"markdown","f140593e":"markdown","32bcadd6":"markdown"},"source":{"ce978998":"import pandas as pd\nimport numpy as np\n","3259d2b0":"df=pd.read_csv('..\/input\/Iris.csv')","de7994db":"df.head(10)","f1a46300":"# When we have categorical values in the data set, we can create a table and sumarize it\ndf.describe(include=['O'])","6426b718":"missing_data=df.isnull()","63055380":"missing_data.head(5)","51eaa633":"missing_data.sum()","966fcd71":"for column in missing_data.columns.values.tolist():\n    print(column)\n    print(missing_data[column].value_counts())\n    print(\"--------------------------------\")","8feb53f3":"#Finding the porcentage of  missing data\nround(((missing_data.sum()\/len(missing_data))*100), 4)","0425d60c":"df.shape","0e36c352":"df.info()","e9e9a49b":"#On classification problems you need to know how balanced the class values are.( This is an example)\ndf.groupby('Species').size() ","4bb74be5":"#Lets check the types \ndf.dtypes\n","7676e87c":"#Apply only when we have categorical values in our X\n\n#df = pd.get_dummies(df)\n","d961d1af":"# Apply when our target is categorical and we need to calculate the correlation \nfrom sklearn.preprocessing import LabelEncoder\n\nlb_make = LabelEncoder()\ndf[\"Target\"] = lb_make.fit_transform(df[\"Species\"])\ndf[[\"Species\", \"Target\"]].head(1)","bf7b2eb9":"# We can analyze all the data set \ndf.describe()","89d6aaae":"#We can analyze any colums separate\ndf['SepalLengthCm'].describe()","1d0361db":"# When we have categorical values in the data set, we can create a table and sumarize it\ndf.describe(include=['O'])","2de293ca":"df.corr()","2809f652":"corr_matrix= df.corr()","4385af8f":"#To check a correlation with our target\n\ncorr_matrix['Target'].sort_values(ascending=False)\n","998b54cc":"import seaborn as sns","1a17be43":"sns.heatmap(df.corr(), vmin=-1, vmax=1.0, annot=True)","af32669c":"df.skew()","7a8741c3":"from matplotlib import pyplot as plt\ndf.hist(bins=10, figsize=(20,15))\nplt.show()","0045f2f5":"df.plot(kind='box', subplots=True, layout=(3,3), sharex=False, sharey=False, figsize=(10,15))\nplt.show()","f07142d1":"from pandas.plotting import scatter_matrix\n","d112ba6b":"\nscatter_matrix(df,figsize=(20,20))\nplt.show()","80d23a30":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nX = df.drop(['Species','Target','Id'],axis=1)  #independent columns\ny = df['Species']    #target column\n#apply SelectKBest class to extract top 10 best features\nbestfeatures = SelectKBest(score_func=chi2, k=4)\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(10,'Score'))  #print 10 best features","4c82168a":"import pandas as pd\nimport numpy as np\n\n\nX =  df.drop(['Species','Target','Id'],axis=1) #independent columns\ny = df['Species']     #target column i.e price range\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()","fd002749":"import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom matplotlib import pyplot\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","b45760d3":"X_train=df.drop(['Id','Target','Species'],axis=1).values\ny_train=df['Species'].values","b94e69ef":"models = []\nmodels.append(('PER', Perceptron( max_iter=1000,tol=1e-3)))\nmodels.append(('XGB', XGBClassifier(objective=\"binary:logistic\", random_state=42)))\nmodels.append(('LSVC',  LinearSVC(max_iter=10000)))\nmodels.append(('SGDC',  SGDClassifier(max_iter=1000,tol=1e-3)))\nmodels.append(('LR', LogisticRegression(solver='lbfgs',max_iter=10000, multi_class='auto')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='scale',max_iter=1000)))\nmodels.append(('AB', AdaBoostClassifier()))\nmodels.append(('GBM', GradientBoostingClassifier()))\nmodels.append(('RF', RandomForestClassifier(n_estimators=100)))\nmodels.append(('ET', ExtraTreesClassifier(n_estimators=100)))\n\n# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=42)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","b51e3c24":"fig = pyplot.figure()\nfig.suptitle( ' Algorithm Comparison ' )\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()","fd737309":"X=df.drop(['Id','Target','Species'],axis=1).values\ny=df['Species'].values","42458a43":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,\ntest_size=0.20, random_state=42)","6b07cb96":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nlogreg_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, y_train) * 100, 2)\nacc_log\nprint(accuracy_score(y_test, logreg_pred))\nprint(confusion_matrix(y_test, logreg_pred))\nprint(classification_report(y_test, logreg_pred))","a9bc2766":"# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(X_train, y_train)\nsvc_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, y_train) * 100, 2)\nacc_svc\nprint(accuracy_score(y_test, svc_pred ))\nprint(confusion_matrix(y_test, svc_pred ))\nprint(classification_report(y_test, svc_pred ))","a68baf7c":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\nknn_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, y_train) * 100, 2)\nacc_knn\nprint(accuracy_score(y_test, knn_pred))\nprint(confusion_matrix(y_test, knn_pred ))\nprint(classification_report(y_test, knn_pred ))","1bdc3f9b":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\ngaussian_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, y_train) * 100, 2)\nacc_gaussian\nprint(accuracy_score(y_test, gaussian_pred))\nprint(confusion_matrix(y_test, gaussian_pred ))\nprint(classification_report(y_test,gaussian_pred ))\n#GaussianNB(priors=None, var_smoothing=1e-09)","b3d958d8":"# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, y_train)\nperceptron_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, y_train) * 100, 2)\nacc_perceptron\nprint(accuracy_score(y_test, perceptron_pred))\nprint(confusion_matrix(y_test, perceptron_pred))\nprint(classification_report(y_test,perceptron_pred))","adaba698":"# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, y_train)\nlsvc_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, y_train) * 100, 2)\nacc_linear_svc\nprint(accuracy_score(y_test, lsvc_pred ))\nprint(confusion_matrix(y_test, lsvc_pred ))\nprint(classification_report(y_test,lsvc_pred ))","30cea435":"# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train, y_train)\nsgd_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, y_train) * 100, 2)\nacc_sgd\nprint(accuracy_score(y_test, sgd_pred  ))\nprint(confusion_matrix(y_test, sgd_pred  ))\nprint(classification_report(y_test,sgd_pred  ))","3e3ae849":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, y_train)\ndetree_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, y_train) * 100, 2)\nacc_decision_tree\nprint(accuracy_score(y_test, detree_pred ))\nprint(confusion_matrix(y_test, detree_pred ))\nprint(classification_report(y_test,detree_pred ))","9f420486":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, y_train)\nrforest_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, y_train)\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\nacc_random_forest\nprint(accuracy_score(y_test, rforest_pred  ))\nprint(confusion_matrix(y_test, rforest_pred  ))\nprint(classification_report(y_test,rforest_pred  ))","d9a8ebd7":"#Linear Discriminant Analysis\nclf = LinearDiscriminantAnalysis()\nclf.fit(X_train, y_train)\nclf_pred= clf.predict(X_test)\nacc_clf = round(clf.score(X_train, y_train) * 100, 2)\nacc_clf\nprint(accuracy_score(y_test, clf_pred ))\nprint(confusion_matrix(y_test, clf_pred  ))\nprint(classification_report(y_test,clf_pred  ))\n#LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None, solver='svd', store_covariance=False, tol=0.0001)\n","38ab5c31":"# Ada Boost Classifier\nAB = AdaBoostClassifier()\nAB.fit(X_train, y_train)\nAB_pred= AB.predict(X_test)\nacc_AB = round(AB.score(X_train, y_train) * 100, 2)\nacc_AB\nprint(accuracy_score(y_test, AB_pred))\nprint(confusion_matrix(y_test, AB_pred  ))\nprint(classification_report(y_test,AB_pred ))\n#AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm=\u2019SAMME.R\u2019, random_state=None)\n","68d6367d":"# Gradient Boosting Classifier\nGBC = GradientBoostingClassifier()\nGBC.fit(X_train, y_train)\nGBC_pred= GBC.predict(X_test)\nacc_GBC = round(GBC.score(X_train, y_train) * 100, 2)\nacc_GBC\nprint(accuracy_score(y_test, GBC_pred))\nprint(confusion_matrix(y_test,GBC_pred ))\nprint(classification_report(y_test,GBC_pred ))\n#GradientBoostingClassifier(loss=\u2019deviance\u2019, learning_rate=0.1, n_estimators=100, subsample=1.0, criterion=\u2019friedman_mse\u2019, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, presort=\u2019auto\u2019, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001)","6c973a70":"#ExtraTreesClassifier\nETC=ExtraTreesClassifier(n_estimators=100)\nETC.fit(X_train, y_train)\nETC_pred= ETC.predict(X_test)\nacc_ETC = round(ETC.score(X_train, y_train) * 100, 2)\nacc_ETC\nprint(accuracy_score(y_test, ETC_pred))\nprint(confusion_matrix(y_test,ETC_pred))\nprint(classification_report(y_test,ETC_pred ))\n#ExtraTreesClassifier(n_estimators=\u2019warn\u2019, criterion=\u2019gini\u2019, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=\u2019auto\u2019, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=False, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None)","050512d8":"#XGBClassifier(objective\nxgbs = XGBClassifier(objective=\"binary:logistic\", random_state=42)\nxgbs.fit(X_train, y_train)\nxgbs_pred= xgbs.predict(X_test)\nacc_xgbs = round(xgbs.score(X_train, y_train) * 100, 2)\nacc_xgbs\nprint(accuracy_score(y_test, xgbs_pred))\nprint(confusion_matrix(y_test,xgbs_pred))\nprint(classification_report(y_test,xgbs_pred ))","ef4dc6fd":"models = pd.DataFrame({\n    'Model': ['LogisticRegression', 'Support Vector Machines',\n              'KNeighbors', 'Gaussian Naive Bayes', 'Perceptron', 'LinearSVC', \n              'Stochastic Gradient Descent', 'Decision Tree', 'RandomForest', 'Linear Discriminant Analysis', 'AdaBoostClassifier', 'GradientBoostingClassifier', 'ExtraTreesClassifier', 'XGBClassifier'],\n    'Score': [acc_log,acc_svc,acc_knn,acc_gaussian,acc_perceptron,acc_linear_svc,acc_sgd,acc_decision_tree,acc_random_forest,acc_clf,acc_AB,acc_GBC,acc_ETC,acc_xgbs\n]})\nmodels.sort_values(by='Score', ascending=False)","d78f5720":"Scatter Plot Matrix","b180e43e":"Load Clean Data ready for Machine Learning algorithms ","89254011":"Support Vector Machines","20b88ed1":"Most Machine Learning algorithms cannot work with missing features. We can do three things:\n\n\u2022 Get rid of the corresponding districts.\n\n\u2022 Get rid of the whole attribute.\n\n\u2022 Set the values to some value (zero, the mean, the median, etc.).\n\nYou can accomplish these easily using DataFrame\u2019s dropna(), drop(), and fillna()\nmethods:\n\nhousing.dropna(subset=[\"total_bedrooms\"]) # option 1\n\nhousing.drop(\"total_bedrooms\", axis=1) # option 2\n\nmedian = housing[\"total_bedrooms\"].median()\n\nhousing[\"total_bedrooms\"].fillna(median) # option 3\n\nIf you choose option 3, you should compute the median value on the training set, and\nuse it to fill the missing values in the training set, but also don\u2019t forget to save the\nmedian value that you have computed. You will need it later to replace missing values\nin the test set when you want to evaluate your system, and also once the system goes\nlive to replace missing values in new data.","73964b70":"-------------------------------------------","70a9d21e":"--------------------------","2ad8e3e4":"Class Distribution (Classification Only)","5061dbd8":"2- Data Cleaning ","e54b62c7":"Random Forest","c3ae8c45":"---------------------------------","c1a1025d":"6-Correlation Between Attributes","9d91a135":"1- If we don't have X_test\n","26ac0a0d":"------------------------------------------------------","f31528bf":"One Hot Encoding ","4c72efba":"Firts Import the lybraries and data sets","e042520d":"---------------------------------------------------------------------------------------------------","032ef6bf":"Label Enconder","b61487db":"KNeighbors","09e2fe52":"----------------","d7d96db6":"----------------------------------------------------","8b0f8f92":"----------------------------------------------------------------------","17581399":" Take a peek at your row data","0ca6659c":"-------------------------------","166151b9":"It is better to use some data visualization to get a better idea.","0d772b2a":"Gaussian Naive Bayes","4c2de73e":"This is useful to know, because some machine learning algorithms like linear and logistic regression can have poor performance if there are highly correlated input variables in your data.","4b7691aa":"----------------------------------------------------","cfe36a55":"The skew result can show us a positive(right) or negative (left) skew. Values closer to zero show less skew","77f8df1d":"Examples:\n\n1- age_median= df['Age'].median()\n\n1 - df['Age']=df['Age'].fillna(age_median)\n\n2- df['Embarked']=df['Embarked'].fillna('S')\n\n3-df.drop(['Name','Cabin','Ticket'],axis=1,inplace=True)","941644a7":"Summarize the insights  from the results.","b075eb1f":"XGBClassifier","368e6caa":"ExtraTreesClassifier","cdc4a392":"------------------\n","b8db710b":"Linear SVC","4058a6b5":"Select the atribites and save the data frame that is ready for use with the machine learning model.","d0f7b25f":"----------------------------------------------------------------------------","aeb67ec5":"Load libraries ","bc3857fc":"Define  X_train, y_train, X_test","ae966ee5":"Descriptive Statistics","bfc71322":"It is better to use some data visualization to get a better idea.","aa59ef56":"----------------------","8716a652":"Predictions using the algorithms ","31b9e865":"Ada Boost Classifier","7f291b61":"Perceptron","66388f2b":"Define X_test, y_train, X_train","b2bb5493":"Logistic Regression","fb6cf066":"---------------------","f492898c":"-------------------------------------------------------------------------","6939e126":"Handling Text and Categorical Attributes ","74dda090":"-----------------------------------------------------------------","9b24278d":"Stochastic Gradient Descent","14230b31":"It Needs to be done only with numerical values, in case that we have categorical values we can create dummies values or transform the data frame.\n","c3fd20dc":"-----------------------------------------","72702518":"-------------------------------------","4b5a8a0d":"Check for missing data ","1868ba0d":"Compare Machine Learning Algorithms ","c4911696":"Feature Selection For Machine Learning ","1161c011":"Boxplot algorithm comparison ","49ef1ae5":"Compare Machine Learning Algorithms","df33b2c9":"Understand Your Data With Descriptive Statistics and Visualizations","c088739b":"The 10-fold cross validation procedure is used to\nevaluate each algorithm, importantly configured with the same random seed to ensure that the\nsame splits to the training data are performed and that each algorithm is evaluated in precisely\nthe same way.","2fbdd21c":"1-Univariete Selection","a43838cd":"Linear Discriminant Analysis","25d49b79":"Work in proccess, changes coming soon.","df16df87":"Decision Tree","079144e5":"Choose the model that you want to use and do  parameter tuning to get a better score ","b1699a76":"---------------------------------------------------------","e7bc340e":"---------------------------------------------------------------------","28a87d99":"(2,3) -Dimmension of Your Data and Data Types","5781083e":"Gradient Boosting Classifier","a7c73136":"2-Feature Importance","f140593e":"Heatmap","32bcadd6":"7-Skew of Univariate Distributions"}}