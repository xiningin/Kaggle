{"cell_type":{"7025c90f":"code","0c4be8a3":"code","dab3a85f":"code","b972eccb":"code","8eea0644":"code","52f2bf0b":"code","7c1bcd5c":"code","b177ed93":"code","2e60b3a2":"code","912177e2":"code","21d8aa3a":"code","4ad797a2":"code","46cb7660":"code","ae9802e5":"code","f3fe0f78":"code","48b11c96":"code","556555e0":"code","38269073":"code","44c6fc3f":"code","d5c02759":"code","6e5515f9":"code","6271825c":"code","91e74e9b":"code","9c0870d1":"code","7fc78a27":"code","c3910f82":"code","783d1490":"code","aa3b90d0":"code","d85f6070":"code","193c5725":"code","5cd0cf61":"code","fe8aea7d":"code","b5577d3f":"code","2e8ed1a6":"code","5e52cf5d":"code","e7975ee7":"code","1ed992d7":"code","82e42967":"code","60100975":"code","003c4734":"code","a3e61780":"code","c2062448":"code","24f2207b":"code","429db483":"code","99be8d82":"code","6ebda1c9":"code","742deb7c":"code","c7acd718":"code","9195b6cf":"code","1fc60241":"code","e25e6854":"code","ca644028":"code","50f3319c":"code","04d897d9":"code","ee169068":"code","40813465":"code","ebe39c57":"code","d0acf430":"code","7ea77bcc":"code","8a456ad2":"code","bcb7b568":"code","5337118a":"code","1411cb51":"code","ac4d3d5d":"code","86395afd":"markdown","b3226d67":"markdown","f35bcea0":"markdown","bbdb622a":"markdown","31553ad2":"markdown","d2f876a7":"markdown","ba96f401":"markdown","2714dabc":"markdown","721261f9":"markdown","ea206e88":"markdown","81a0b5b7":"markdown"},"source":{"7025c90f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/eng-translations'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0c4be8a3":"categories = pd.read_csv(\"..\/input\/eng-translations\/categories_eng.csv\")\nitems = pd.read_csv(\"..\/input\/eng-translations\/items_eng.csv\")\nsales = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\ntest = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/test.csv\")\nshops = pd.read_csv(\"..\/input\/eng-translations\/shops_eng.csv\")\nsubmission = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv\")","dab3a85f":"def downcast1(df, verbose=True):\n    \n    \"\"\"\n    Funciton to reduce the memory used of a particular dataframe by downcasting to a less memory-intensive data type.\n    \"\"\"\n    \n    start_mem = df.memory_usage().sum() \/ 1024**2\n    \n    for col in df.columns:\n        dtype_name = df[col].dtype.name\n        if dtype_name == 'object':\n            pass\n        elif dtype_name == 'bool':\n            df[col] = df[col].astype('int8')\n        elif dtype_name.startswith('int') or (df[col].round() == df[col]).all():\n            df[col] = pd.to_numeric(df[col], downcast='integer')\n        else:\n            df[col] = pd.to_numeric(df[col], downcast='float')\n    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    \n    if verbose:\n        print('{:.1f}% compressed'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","b972eccb":"all_df = [sales, shops, items, categories, test]\nfor df in all_df:\n    df = downcast1(df)","8eea0644":"shops.sample(10)","52f2bf0b":"import re\n\ndef cleans(i):\n    \n    \"\"\"\n    Function to clean strings, removing non-alphanumeric characters.\n    \"\"\"\n    \n    pattern = r'[A-Za-z0-9]+'\n    \n    finds = re.findall(pattern, str(i))\n\n    stringy = \"\"\n    \n    for j in finds:\n        \n        stringy += f\" {j}\"\n        \n    return stringy","7c1bcd5c":"shops[\"clean\"] = shops[\"shop_name\"].apply(cleans)\nshops.head()","b177ed93":"# Deal with obsolete shop_ids\n\nsales.loc[sales[\"shop_id\"]==0, \"shop_id\"] = 57\nsales.loc[sales[\"shop_id\"]==1, \"shop_id\"] = 58\nsales.loc[sales[\"shop_id\"]==10, \"shop_id\"] = 11\nsales.loc[sales[\"shop_id\"]==39, \"shop_id\"] = 40\n\ntest.loc[test['shop_id'] == 0, 'shop_id'] = 57\ntest.loc[test['shop_id'] == 1, 'shop_id'] = 58\ntest.loc[test['shop_id'] == 10, 'shop_id'] = 11\ntest.loc[test['shop_id'] == 39, 'shop_id'] = 40","2e60b3a2":"# Only use shops in train data that are in test data\n\nunique_test_shops = test[\"shop_id\"].unique()\nsales = sales[sales[\"shop_id\"].isin(unique_test_shops)]\n\nprint(f\"Number of Unique Shops in Test Data:{len(unique_test_shops)}\\nNumber of Unique Shops in Sales Data:{len(sales['shop_id'].unique())}\")","912177e2":"shops.drop(\"shop_name\", axis=1, inplace=True)","21d8aa3a":"shops[\"city\"] = shops[\"clean\"].apply(lambda x: x.split()[0])","4ad797a2":"# Use LabelEncoder to convert categorical variables into numerical variables\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\nshops[\"city\"] = le.fit_transform(shops[\"city\"])\nshops.drop(\"clean\", axis=1, inplace=True)\n","46cb7660":"# Final shops dataframe\nshops.sample(10)","ae9802e5":"items[\"item_name\"] = items[\"item_name\"].str.lower()\nitems[\"item_name_clean\"] = items[\"item_name\"].apply(cleans)\nitems.drop(\"item_name\", axis=1, inplace=True)","f3fe0f78":"# Take the first five characters of the item_name string\n\nitems[\"item_name_five\"] = [x[:5] for x in items[\"item_name_clean\"]]\nitems[\"item_name_five\"] = le.fit_transform(items[\"item_name_five\"])\nitems.drop(\"item_name_clean\", axis=1, inplace=True)","48b11c96":"# Create first_sale_date feature\n\nitems[\"first_sale_date\"] = sales.groupby(\"item_id\").agg({\"date_block_num\":\"min\"})[\"date_block_num\"]\nitems","556555e0":"# As the NaN values in this table are for items first sold in the test period, replace them with 34 (the date_block_num for the test period)\n\nitems[items[\"first_sale_date\"].isna()]\nitems[\"first_sale_date\"] = items[\"first_sale_date\"].fillna(34)","38269073":"categories[\"category\"] = categories[\"category_name\"].apply(lambda x: x.split()[0])\ncategories","44c6fc3f":"categories[\"category\"].value_counts()","d5c02759":"# Cleaning \n\ncategories.loc[categories[\"category\"] == \"Game\"] = \"Games\"","6e5515f9":"def make_misc(x):\n    \n    \"\"\"\n    Function to change the name of low frequency categories to 'Misc'\n    \"\"\"\n    \n    if len(categories[categories['category']==x]) >= 5:\n        return x\n    else:\n        return 'Misc'\n    \ncategories[\"cats\"] = categories[\"category\"].apply(make_misc)\n\ncategories","6271825c":"categories.drop([\"category\", \"category_name\"], axis=1, inplace=True)","91e74e9b":"# Encode the 'cats' feature and delete\n\ncategories[\"cats_le\"] = le.fit_transform(categories[\"cats\"])\n\ncategories.drop(\"cats\", inplace=True, axis=1)","9c0870d1":"sales = sales[sales[\"item_price\"] > 0]\nsales = sales[sales[\"item_price\"] < 50000]\nsales = sales[sales[\"item_cnt_day\"] > 0]\nsales = sales[sales[\"item_cnt_day\"] < 1000]\nsales[\"item_price\"] = sales[\"item_price\"].apply(lambda x: round(x,2))\nsales","7fc78a27":"# Create a dataframe of the Cartesian Product of the unique shops and unique items for each month\n\nfrom itertools import product\n\ntrain = []\n\nfor i in range(0,34):\n    \n    cur_shops = sales.loc[sales[\"date_block_num\"] == i, \"shop_id\"].unique()\n    \n    cur_items = sales.loc[sales[\"date_block_num\"] == i, \"item_id\"].unique()\n    \n    train.append(np.array(list(product(*[[i],cur_shops, cur_items]))))\n    \nindex_feats = [\"date_block_num\", \"shop_id\", \"item_id\"]\n\ntrain = pd.DataFrame(np.vstack(train), columns=index_feats)\n    ","c3910f82":"# Create the column showing how many of each item have been sold in each month. This is the form the target variable will take.\n\ngroup = sales.groupby(index_feats).agg({\"item_cnt_day\": \"sum\"})\ngroup = group.reset_index()\ngroup = group.rename(columns={\"item_cnt_day\": \"item_cnt_month\"})\n\ntrain = pd.merge(train, group, on=index_feats, how=\"left\")\ntrain","783d1490":"# Use garbage collection to minimise memory usage\n\nimport gc\n\ndel group\n\ngc.collect()","aa3b90d0":"# Add column for count of items sold.\n\ngroup = sales.groupby(index_feats).agg({\"item_cnt_day\":\"count\"})\ngroup = group.reset_index()\ngroup = group.rename(columns={\"item_cnt_day\":\"item_cnt\"})\n\ntrain = pd.merge(train, group, on=index_feats, how=\"left\")\n\ntrain.sample(5)","d85f6070":"del group, sales\ngc.collect()","193c5725":"test[\"date_block_num\"] = 34\n\nall_data = pd.concat([train, test.drop(\"ID\", axis=1)], ignore_index=True, keys=index_feats)\n\nall_data = all_data.fillna(0)\n\nall_data.sample(10)","5cd0cf61":"# Merge all dataframes \n\nall_data = pd.merge(all_data, shops, on=\"shop_id\", how=\"left\")\nall_data = pd.merge(all_data, items, on=\"item_id\", how=\"left\")\nall_data = pd.merge(all_data, categories, on=\"category_id\", how=\"left\")\n\nall_data.sample(10)","fe8aea7d":"all_data = downcast1(all_data)\n\ndel shops, items, categories\n\ngc.collect()","b5577d3f":"def add_mean_feats(df, mean_feats, index_features, agg_col=\"item_cnt_month\", agg_func=\"mean\"):\n    \n    \"\"\"\n    Function to automatically create new features showing the mean item_cnt_month grouped by the specified columns.\n    \"\"\"\n    \n    if len(index_features) == 2:\n        feature_name = index_features[1] + f\"_{agg_col}_{agg_func}\"\n    else: \n        feature_name = index_features[1] + \"_\" + index_features[2] + f\"_{agg_col}_{agg_func}\"\n        \n    group = df.groupby(index_features).agg({agg_col:agg_func}).reset_index().rename(columns={agg_col:feature_name})\n    \n    df = pd.merge(df, group, on=index_features, how=\"left\")\n    \n    df = downcast1(df)\n    \n    mean_feats.append(feature_name)\n    \n    del group\n    gc.collect()\n    \n    return df, mean_feats\n","2e8ed1a6":"item_mean_features = []\n\nall_data, item_mean_features = add_mean_feats(all_data, item_mean_features, [\"date_block_num\", \"item_id\"])\n\nall_data","5e52cf5d":"all_data, item_mean_features = add_mean_feats(all_data, item_mean_features, [\"date_block_num\", \"item_id\", \"city\"])","e7975ee7":"shop_mean_features = []\n\nall_data, shop_mean_features = add_mean_feats(all_data, shop_mean_features, [\"date_block_num\", \"shop_id\", \"category_id\"])","1ed992d7":"cat_mean_features = []\n\nall_data, cat_mean_features = add_mean_feats(all_data, cat_mean_features, [\"date_block_num\", \"category_id\"])","82e42967":"all_data, cat_mean_features = add_mean_feats(all_data, cat_mean_features, [\"date_block_num\", \"cats_le\"])","60100975":"all_data.sample(10)","003c4734":"def add_lags(df, lag_features, index_features, lag_feature, lags=[1,2,3], clip=False):\n    \n    \"\"\"\n    Function to automatically create lag features based on the columns specified.\n    \"\"\"\n    \n    df_temp = df[index_features + [lag_feature]].copy()\n    \n    for i in lags:\n        \n        feat_name = lag_feature + \"_lag\" + str(i)\n        df_temp.columns = index_features + [feat_name]\n        df_temp[\"date_block_num\"] += i\n        df = pd.merge(df, df_temp.drop_duplicates(), on=index_features, how=\"left\")\n        df[feat_name] = df[feat_name].fillna(0)\n        \n        if clip:\n            lag_feats_to_clip.append(feat_name)\n            \n    df = downcast1(df)\n    del df_temp\n    gc.collect()\n    \n    return df, lag_feats_to_clip","a3e61780":"lag_feats_to_clip = []\nindex_features = [\"date_block_num\", \"shop_id\", \"item_id\"]\n\nall_data, lag_feats_to_clip = add_lags(all_data, lag_feats_to_clip, index_features, \"item_cnt_month\", clip=True)\nall_data, lag_feats_to_clip = add_lags(all_data, lag_feats_to_clip, index_features, \"item_cnt\", clip=True)\n\nall_data.sample(10)","c2062448":"# Check there is no data that has leaked into test set\n\nX_test_temp = all_data[all_data[\"date_block_num\"]==34]\nX_test_temp[item_mean_features].sum()","24f2207b":"# Now use the lists that have saved previously in creating the mean features to create additional lags\n\nfor item in item_mean_features:\n    \n    all_data, lag_feats_to_clip = add_lags(all_data, lag_feats_to_clip, index_features, item, clip=True)","429db483":"for shop in shop_mean_features:\n    \n    all_data, lag_feats_to_clip = add_lags(all_data, lag_feats_to_clip, [\"date_block_num\", \"shop_id\", \"category_id\"], \n                                           shop, clip=True)\n    ","99be8d82":"for cat in cat_mean_features:\n    \n    all_data, lag_feats_to_clip = add_lags(all_data, lag_feats_to_clip, [\"date_block_num\", \"category_id\"], cat, lags=[1,2,3], clip=True)\n    ","6ebda1c9":"all_data = all_data.drop(item_mean_features, axis=1)\nall_data = all_data.drop(shop_mean_features, axis=1)\nall_data = all_data.drop(cat_mean_features, axis=1)\n\nall_data = all_data.drop(all_data[all_data[\"date_block_num\"]<3].index)","742deb7c":"all_data.sample(10)","c7acd718":"del X_test_temp\ngc.collect()","9195b6cf":"# Create feature showing mean of the three lags\n\nall_data[\"item_cnt_month_3lag_mean\"] = all_data[[\"item_cnt_month_lag1\", \"item_cnt_month_lag2\", \"item_cnt_month_lag3\"]].mean(axis=1)","1fc60241":"all_data[lag_feats_to_clip + [\"item_cnt_month_3lag_mean\", \n                                 \"item_cnt_month\"]] =  all_data[lag_feats_to_clip + [\"item_cnt_month_3lag_mean\", \n                                                                                        \"item_cnt_month\"]].clip(0,20)","e25e6854":"# Create lag gradient features\n\nall_data[\"lag_grad_1\"] = all_data[\"item_cnt_month_lag1\"] \/ all_data[\"item_cnt_month_lag2\"]\nall_data[\"lag_grad_1\"] = all_data[\"lag_grad_1\"].replace([np.inf, -np.inf], np.nan).fillna(0)\n\nall_data[\"lag_grad_2\"] = all_data[\"item_cnt_month_lag2\"] \/ all_data[\"item_cnt_month_lag3\"]\nall_data[\"lag_grad_2\"] = all_data[\"lag_grad_2\"].replace([np.inf, -np.inf], np.nan).fillna(0)","ca644028":"all_data[\"new_items\"] = all_data[\"first_sale_date\"] == all_data[\"date_block_num\"]","50f3319c":"all_data[\"time_since_first_sale\"] = all_data[\"date_block_num\"] - all_data[\"first_sale_date\"]\n\nall_data.drop(\"first_sale_date\", inplace=True, axis=1)","04d897d9":"all_data[\"month\"] = all_data[\"date_block_num\"] % 12","ee169068":"all_data.drop([\"item_cnt\"], axis=1, inplace=True)","40813465":"all_data = downcast1(all_data)\nall_data.info()","ebe39c57":"# Change category_id from object datatype to int8\n\nall_data[\"category_id\"] = all_data[\"category_id\"].astype(\"int8\")","d0acf430":"X_train = all_data[all_data[\"date_block_num\"]<33]\ny_train = X_train[\"item_cnt_month\"]\nX_train = X_train.drop(\"item_cnt_month\", axis=1)\n\nX_val = all_data[all_data[\"date_block_num\"] == 33]\ny_val = X_val[\"item_cnt_month\"]\nX_val = X_val.drop(\"item_cnt_month\", axis=1)\n\nX_test = all_data[all_data[\"date_block_num\"]==34]\nX_test = X_test.drop(\"item_cnt_month\", axis=1)\n\ndel all_data\ngc.collect()\n","7ea77bcc":"def preds(model, test, name):\n    \n    \"\"\"\n    Function to use the chosen model to make predictions using the chosen test set, format the\n    predictions and save these as a .csv file ready for upload to Kaggle.\n    \"\"\"\n    \n    prediction = model.predict(test)\n    \n    df_sub = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv\")\n    \n    df_sub[\"item_cnt_month\"] = prediction.clip(0,20)\n    \n    df_sub.to_csv(f\"{name}.csv\", index=False)\n    \n    print(\"Complete.\")","8a456ad2":"# Try Light Gradient Boosting Machine, parameters can be altered for further accuracy.\n\nimport lightgbm as lgb\n\nparams = {'metric': 'rmse',\n          'num_leaves': 255,\n          'learning_rate': 0.005,\n          'feature_fraction': 0.75,\n          'bagging_fraction': 0.75,\n          'bagging_freq': 5,\n          'force_col_wise' : True,\n          'random_state': 10,\n         'num_rounds':1500,\n         'early_stopping':150}\n\nlgb_train = lgb.Dataset(X_train, y_train)\n\nlgb_val = lgb.Dataset(X_val, y_val)\n\nmodel = lgb.train(params=params, train_set=lgb_train, valid_sets=(lgb_train, lgb_val), verbose_eval=50)","bcb7b568":"preds(model, X_test, \"lgb_model\")","5337118a":"del lgb_train, lgb_val\ngc.collect()","1411cb51":"def plot_features(booster, figsize):\n    \n    \"\"\"\n    Function to create a feature importance plot\n    \"\"\"\n    \n    fig, ax = plt.subplots(1, 1, figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)","ac4d3d5d":"# Try an Extreme Gradient Boosting model\n\nfrom xgboost import XGBRegressor, plot_importance\nimport matplotlib.pyplot as plt\n\n\nxgb_model = XGBRegressor(\n    max_depth=8,\n    n_estimators=100,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.1,    \n    seed=42)\n\nxgb_model.fit(X_train, y_train, eval_metric=\"rmse\", eval_set=[(X_train, y_train), (X_val, y_val)])\n\npreds(xgb_model, X_test, f\"xgb_{i}\")\n\nplot_features(xgb_model, (10, 14))","86395afd":"## Add Test Data to Overall Dataframe","b3226d67":"## Remove Outliers from the Sales Dataframe","f35bcea0":"# Data Preparation and Cleaning","bbdb622a":"## Shops","31553ad2":"## Create Mean Features","d2f876a7":"## Items","ba96f401":"## Categories","2714dabc":"## Data Combinations","721261f9":"## Creating the Machine Learning Models","ea206e88":"## Create Lag Features","81a0b5b7":"## Additional Features"}}