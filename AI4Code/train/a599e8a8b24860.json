{"cell_type":{"d562b982":"code","5521645a":"markdown","071b5cab":"markdown"},"source":{"d562b982":"import pandas  as pd\n\n#===========================================================================\n# read in the data\n#===========================================================================\ntrain_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_data  = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\n#===========================================================================\n# select some features of interest (\"ay, there's the rub\", Shakespeare)\n#===========================================================================\nfeatures = ['OverallQual' , 'GrLivArea' , 'TotalBsmtSF' , 'BsmtFinSF1' ,\n            '2ndFlrSF'    , 'GarageArea', '1stFlrSF'    , 'YearBuilt'  ]\n\n#===========================================================================\n#===========================================================================\nX_train       = train_data[features]\ny_train       = train_data[\"SalePrice\"]\nfinal_X_test  = test_data[features]\n\n#===========================================================================\n# pre-processing: imputation; substitute any 'NaN' with mean value\n#===========================================================================\nX_train      = X_train.fillna(X_train.mean())\nfinal_X_test = final_X_test.fillna(final_X_test.mean())\n\n#===========================================================================\n# create the kernel \n#===========================================================================\nfrom sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\nkernel = DotProduct() + WhiteKernel()\n\n#===========================================================================\n# perform the regression \n#===========================================================================\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nregressor = GaussianProcessRegressor(kernel=kernel)\n\n#===========================================================================\n# and the fit \n#===========================================================================\nregressor.fit(X_train, y_train)\n\n#===========================================================================\n# now use the model to predict the prices for the test data\n#===========================================================================\npredictions = regressor.predict(final_X_test)\n\n#===========================================================================\n# and finally write out CSV submission file\n#===========================================================================\noutput = pd.DataFrame({\"Id\":test_data.Id, \"SalePrice\":predictions})\noutput.to_csv('submission.csv', index=False)","5521645a":"## Gaussian process regression\nHere is a very simple example script that applies [Gaussian process](https:\/\/scikit-learn.org\/stable\/modules\/gaussian_process.html) regression to the House Prices data set. \nIn [Gaussian process regression](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.GaussianProcessRegressor.html) a [kernel function](https:\/\/scikit-learn.org\/stable\/modules\/gaussian_process.html#gp-kernels) (or [covariance](https:\/\/en.wikipedia.org\/wiki\/Covariance) function) is used to help to mould the shape of prior and posterior values. It is assumed that any uncertainty, or noise, is Gaussian in nature (i.e. can be sampled from a [normal distribution](https:\/\/en.wikipedia.org\/wiki\/Normal_distribution)), hence the name.\nFor feature selection I have simply used the top eight features obtained from my Scikit-learn [recursive feature elimination script](https:\/\/www.kaggle.com\/carlmcbrideellis\/recursive-feature-elimination-rfe-example). ","071b5cab":"### Kernel functions available in scikit-learn are:\n* [CompoundKernel](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.kernels.CompoundKernel.html): Kernel which is composed of a set of other kernels.\n* [ConstantKernel](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.kernels.ConstantKernel.html): Constant kernel.\n* [DotProduct](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.kernels.DotProduct.html): Dot-Product kernel.\n* [Exponentiation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.kernels.Exponentiation.html): The Exponentiation kernel takes one base kernel and a scalar parameter and combines them.\n* [ExpSineSquared](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.kernels.ExpSineSquared.html): Exp-Sine-Squared kernel (aka periodic kernel).\n* [Hyperparameter](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.kernels.Hyperparameter.html): A kernel hyperparameter\u2019s specification in form of a namedtuple.\n* [Kernel](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.kernels.Kernel.html): Base class for all kernels.\n* [Matern](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.kernels.Matern.html): The class of Matern kernels is a generalization of the radial-basis function kernel (aka squared-exponential kernel).\n* [PairwiseKernel](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.kernels.PairwiseKernel.html): Wrapper for kernels in sklearn.metrics.pairwise.\n* [Product](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.kernels.Product.html): The Product kernel takes two kernels and combines them.\n* [RationalQuadratic](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.kernels.RationalQuadratic.html): The RationalQuadratic kernel can be seen as a scale mixture (an infinite sum) of radial-basis function kernels with different characteristic length scales.\n* [RBF](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.kernels.RBF.html): Radial-basis function kernel (aka squared-exponential kernel).\n* [Sum](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.kernels.Sum.html): The Sum kernel takes two kernels and combines them.\n* [WhiteKernel](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.kernels.WhiteKernel.html): The main use-case of the White kernel is as part of a sum-kernel where it explains the noise of the signal as independently and identically normally-distributed.\n\nFor more information on which choice to make see [The Kernel Cookbook: 'Advice on Covariance functions'](https:\/\/www.cs.toronto.edu\/~duvenaud\/cookbook\/), by David Duvenaud.\n\n## Related reading\n* [Gaussian process](https:\/\/en.wikipedia.org\/wiki\/Gaussian_process) on Wikipedia\n* [Gaussian Processes](https:\/\/scikit-learn.org\/stable\/modules\/gaussian_process.html) on Scikit-learn\n* [GaussianProcessRegressor](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.GaussianProcessRegressor.html) on Scikit-learn\n* [Carl Edward Rasmussen and Christopher K. I. Williams \"Gaussian Processes for Machine Learning\"](http:\/\/www.gaussianprocess.org\/gpml\/) (book website)\n* [Gaussian Process Regression Models](https:\/\/www.mathworks.com\/help\/stats\/gaussian-process-regression-models.html) on MathWorks\n\n### Related notebooks:\n* [Gaussian process regression and classification](https:\/\/www.kaggle.com\/residentmario\/gaussian-process-regression-and-classification) by Aleksey Bilogur\n* [Feature Engineering with Gaussian Process](https:\/\/www.kaggle.com\/kenmatsu4\/feature-engineering-with-gaussian-process) by kenmatsu4"}}