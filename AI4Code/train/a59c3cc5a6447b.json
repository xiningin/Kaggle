{"cell_type":{"2895aa6c":"code","a3c1c9e7":"code","5db3f49f":"code","cda7879e":"code","6280e782":"code","4a50d77d":"code","a12d2aed":"code","befa6b41":"code","2cc4e7f6":"code","c69ed59a":"code","c781bc55":"code","f86b45fb":"code","c64094fd":"code","e0b823ed":"code","9f54a5b5":"code","ee6dd80a":"code","1224f3a3":"code","70e35286":"code","94849c91":"code","d807cd02":"code","605dacf3":"markdown","f694423b":"markdown"},"source":{"2895aa6c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sys\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nnp.random.seed(2018)\nimport nltk\nimport re\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom nltk.corpus import stopwords \nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport string\nimport gensim\nfrom gensim import corpora\nfrom time import time\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF\n\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/department-of-justice-20092018-press-releases\/\"))\ndata=os.listdir(\"..\/input\/20-newsgroup-original\/20_newsgroup\/20_newsgroup\/soc.religion.christian\/\")\n# Any results you write to the current directory are saved as output.","a3c1c9e7":"TEXT_DATA_DIR = r'..\/input\/20-newsgroup-original\/20_newsgroup\/20_newsgroup\/'\ntexts = []  # list of text samples\nlabels_index = {}  # dictionary mapping label name to numeric id\nlabels = []  # list of label ids\nfor name in sorted(os.listdir(TEXT_DATA_DIR)):\n    path = os.path.join(TEXT_DATA_DIR, name)\n    if os.path.isdir(path):\n        label_id = len(labels_index)\n        labels_index[name] = label_id\n        for fname in sorted(os.listdir(path)):\n            if fname.isdigit():\n                fpath = os.path.join(path, fname)\n                if sys.version_info < (3,):\n                    f = open(fpath)\n                else:\n                    f = open(fpath, encoding='latin-1')\n                t = f.read()\n                i = t.find('\\n\\n')  # skip header\n                if 0 < i:\n                    t = t[i:]\n                texts.append(t)\n                f.close()\n                labels.append(label_id)","5db3f49f":"print('Original Topics-----------------------------------------------------')\nfor index,ele in labels_index.items():\n    print(ele,\" \",index)","cda7879e":"def remove(list1): \n    pattern = '[_:.)><@!(\\\\n\\[\\]\/0-9]'\n    list1 = [re.sub(pattern, ' ', i) for i in list1] \n    list1=[re.sub(r'\\b\\w{1,3}\\b', ' ',i) for i in list1]\n    return list1\ntexts=remove(texts)","6280e782":"tfidf = TfidfVectorizer(\n    min_df = 0.002,\n    max_df = 0.1,\n    max_features = 10000,\n    stop_words = 'english',\n)\ntfidf.fit(texts)\ntext = tfidf.transform(texts)\ndf = pd.DataFrame(text.toarray(),columns=[tfidf.get_feature_names()])\ndf.iloc[:5]","4a50d77d":"#Finding the optimal cluster through Elbow method\ndef find_optimal_clusters(data, max_k):\n    iters = range(2, max_k+1, 2)\n    \n    sse = []\n    for k in iters:\n        sse.append(MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=20).fit(data).inertia_)\n        print('Fit {} clusters'.format(k))\n        \n    f, ax = plt.subplots(1, 1)\n    ax.plot(iters, sse, marker='o')\n    ax.set_xlabel('Cluster Centers')\n    ax.set_xticks(iters)\n    ax.set_xticklabels(iters)\n    ax.set_ylabel('SSE')\n    ax.set_title('SSE by Cluster Center Plot')\n    \nfind_optimal_clusters(text, 20)","a12d2aed":"#To see the sum of squared error for given cluster size\nMiniBatchKMeans(n_clusters=20, init_size=1024, batch_size=2048, random_state=20).fit(text).inertia_","befa6b41":"clusters = MiniBatchKMeans(n_clusters=20, init_size=1024, batch_size=2048, random_state=20).fit_predict(text)","2cc4e7f6":"def get_top_keywords(data, clusters, labels, n_terms):\n    df = pd.DataFrame(data.todense()).groupby(clusters).mean()\n    topic_df=pd.DataFrame()\n    for i,r in df.iterrows():\n        print('\\nTopic {}'.format(i))\n        print(','.join([labels[t] for t in np.argsort(r)[-n_terms:]]))\n        return topic_df[i]=[labels[t] for t in np.argsort(r)[-n_terms:]]    \nget_top_keywords(text, clusters, tfidf.get_feature_names(), 10)\n\n#To join the clusters in original data\n","c69ed59a":"def get_top_keywords(data, clusters, labels, n_terms):\n    df = pd.DataFrame(data.todense()).groupby(clusters).mean()\n    topic_df=pd.DataFrame()\n    for i,r in df.iterrows():\n        topic_df[i]=[labels[t] for t in np.argsort(r)[-n_terms:]]    \n    return topic_df\ndf1=(get_top_keywords(text, clusters, tfidf.get_feature_names(), 10))","c781bc55":"print('topics from K-Means')\ndf1","f86b45fb":"stop = set(stopwords.words('english'))\nexclude = set(string.punctuation) \nlemma = WordNetLemmatizer()\ndef clean(doc):\n    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n    return normalized\n\ndoc_clean = [clean(doc).split() for doc in texts] ","c64094fd":"# Creating the term dictionary of our courpus, where every unique term is assigned an index. \ndictionary = corpora.Dictionary(doc_clean)\n\n# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\ndoc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]","e0b823ed":"# Creating the object for LDA model using gensim library\nLda = gensim.models.ldamodel.LdaModel\n\n# Running and Trainign LDA model on the document term matrix.\nldamodel = Lda(doc_term_matrix, num_topics=20, id2word = dictionary, passes=20)","9f54a5b5":"def get_lda_topics(model, num_topics):\n    word_dict = {}\n    for i in range(num_topics):\n        words = model.show_topic(i, topn = 10)\n        word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words]\n    return pd.DataFrame(word_dict)","ee6dd80a":"get_lda_topics(ldamodel, 20)","1224f3a3":"n_samples = 2000\nn_features = 8000\nn_components = 20\nn_top_words = 10\n\n\ndef print_top_words(model, feature_names, n_top_words):\n    topic_df=pd.DataFrame()\n    for topic_idx, topic in enumerate(model.components_):\n#         message = \"Topic #%d: \" % topic_idx\n#         message += \" \".join([feature_names[i]\n#                              for i in topic.argsort()[:-n_top_words - 1:-1]])\n#         print(message)\n        topic_df[topic_idx]=[feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n    return topic_df\n    print()","70e35286":"# Use tf-idf features for NMF.\nprint(\"Extracting tf-idf features for NMF...\")\ntfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n                                   max_features=n_features,\n                                   stop_words='english')\n\ntfidf = tfidf_vectorizer.fit_transform(texts)\n","94849c91":"# Fit the NMF model\nnmf = NMF(n_components=n_components, random_state=1,\n          alpha=.1, l1_ratio=.5).fit(tfidf)\n\ntfidf_feature_names = tfidf_vectorizer.get_feature_names()\ndf2=print_top_words(nmf, tfidf_feature_names, n_top_words)","d807cd02":"print(\"\\nTopics in NMF model (Frobenius norm):\")\ndf2","605dacf3":"### K means clustering","f694423b":"**LDA**"}}