{"cell_type":{"04b9e542":"code","acd24ddd":"code","72b74576":"code","55226fed":"code","a30f0df7":"code","8c189875":"code","4ff39d86":"code","4ae189f3":"code","c17d368d":"code","d89ff209":"code","a85be939":"code","1deba456":"code","87600dcb":"code","daadb9dc":"code","eda90f52":"code","74509725":"code","b18f9416":"code","00ab004f":"code","d0759da9":"code","c25b9dd8":"code","576fff4a":"code","6c8153a3":"code","a6e299a7":"code","1cdc38d7":"code","3303fb66":"code","037da2d3":"code","1f09f780":"code","74b5a5c1":"code","b19ef0eb":"code","e127e39e":"code","7a607682":"code","56b0928d":"code","5a8d2bcf":"code","df4ae109":"code","33acce7b":"code","eeb2d0d1":"code","612c16aa":"code","3b1306e9":"code","85c67703":"code","08a89be0":"code","33c4d03d":"code","092f525a":"code","fc012140":"code","a0679c49":"code","77e880a1":"code","b67edab9":"code","6874eb2f":"code","ade502bc":"code","6524974c":"code","76c075e0":"code","ee87736e":"code","b18b3a05":"code","6d029bb4":"code","263aac01":"markdown","0e09d2a3":"markdown","6986546f":"markdown","fa848da1":"markdown","6e51ced3":"markdown","fb9d4512":"markdown","3b14ba74":"markdown","b0a06fb2":"markdown","a84e7f42":"markdown","063ac726":"markdown","c4295398":"markdown","38d158a4":"markdown","fd232e26":"markdown","a2cc5541":"markdown","d089fcc1":"markdown","735471ec":"markdown","ff0ac039":"markdown","4c5a23fc":"markdown","bdced637":"markdown","d8a8bddf":"markdown","dcff4591":"markdown","aecb7201":"markdown","c69a7cf8":"markdown","63de375f":"markdown","dfff7bad":"markdown","8faf4928":"markdown","f3246526":"markdown","f47e60d2":"markdown","b9cc7f64":"markdown","d5718080":"markdown","6ca5e075":"markdown","3089d65c":"markdown","b28e468a":"markdown","01e0e3ea":"markdown","1d37c594":"markdown","9fb37298":"markdown","370c6988":"markdown","a2df35c0":"markdown","59a1189e":"markdown"},"source":{"04b9e542":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","acd24ddd":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","72b74576":"train.info()\ntrain.shape\ntest.shape","55226fed":"#check whether the data has missing values \ntrain.isnull().values.any()\n#print how many missing values data has\ntrain.isnull().values.sum()","a30f0df7":"#We can plot the missing values with following library\nimport missingno as msno\nmsno.matrix(train)","8c189875":"#drop the two rows in which there are two missing values in the variable `Embarked'\ntrain['Embarked'].isnull()\ntrain[train['Embarked'].isna()]\ntrain_dropped = train.drop([61,829])\ntrain_dropped.shape","4ff39d86":"#Drop the variable 'cabin' since it does not give any meaningful information (just for simplification)\ntrain_dropped['Cabin']","4ae189f3":"train_dropped1 = train_dropped.drop('Cabin',axis=1)\ntrain_dropped1.shape","c17d368d":"train_dropped1['Age'].fillna(train_dropped1['Age'].mean(),inplace=True)","d89ff209":"train_dropped1.isnull().values.any()","a85be939":"test.info()","1deba456":"test['Age'].fillna(test['Age'].mean(),inplace=True)","87600dcb":"test = test.drop('Cabin',axis=1)","daadb9dc":"test['Fare'].fillna(test['Fare'].mean(),inplace=True)","eda90f52":"test.info()","74509725":"train = train_dropped1","b18f9416":"train.shape","00ab004f":"test.shape","d0759da9":"train.head()","c25b9dd8":"attributes = ['Survived','Sex','Age','SibSp','Parch','Fare','Embarked']\ntrain[attributes].hist(figsize=(10,10))","576fff4a":"train[['Survived','Sex']].value_counts()","6c8153a3":"pd.crosstab(train.Survived,train.Sex)","a6e299a7":"import seaborn as sns\n#draw a bar plot of survival by sex\nsns.barplot(x=\"Sex\", y=\"Survived\", data=train)","1cdc38d7":"sns.barplot(x=\"Pclass\", y=\"Survived\", data=train)","3303fb66":"sns.barplot(x=\"Parch\", y=\"Survived\", data=train)","037da2d3":"sns.barplot(x=\"Embarked\", y=\"Survived\", data=train)","1f09f780":"sns.barplot(x=\"SibSp\", y=\"Survived\", data=train)","74b5a5c1":"age_count = pd.cut(train.Age,bins=[0,2,17,50,80],labels=['Toddler\/Baby','Child','Adult','Elderly'])\ntrain.insert(10,'Age_Group',age_count)\nsns.barplot(x=\"Age_Group\", y=\"Survived\", data=train)","b19ef0eb":"train = train.drop('Age_Group',axis=1)","e127e39e":"#check whether the Age_Group variable is removed from dataset.\ntrain.info()","7a607682":"#Converting Categorical variables into numeric\ntrain['Sex'] = train['Sex'].map({'male':1, 'female':0})\ntest['Sex'] = test['Sex'].map({'male':1, 'female':0})\ntrain['Embarked'] = train['Embarked'].map({'Q':2, 'S':1, 'C':0})\ntest['Embarked'] = test['Embarked'].map({'Q':2, 'S':1, 'C':0})","56b0928d":"X_train = train.drop([\"Name\", \"Survived\", \"PassengerId\",\"Ticket\"], axis=1)\nY_train = train[\"Survived\"]\nX_test  = test.drop(['Name',\"PassengerId\",\"Ticket\"], axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","5a8d2bcf":"X_train.head()","df4ae109":"X_test.head()","33acce7b":"# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import SGDClassifier\nimport xgboost as xgb\n!pip install pygam\nfrom pygam import LogisticGAM\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score","eeb2d0d1":"# Logistic Regression\nlogreg = LogisticRegression(max_iter=200)\nlogreg.fit(X_train,Y_train)\nlogreg_Y_pred=logreg.predict(X_test)\nlogreg_probs = logreg.predict_proba(X_train)[:,1] #predicted probabilities on training data\nlogreg_accuracy = logreg.score(X_train, Y_train) #accuracy on training data\nlogreg_accuracy","612c16aa":"#Ridge Classifier\n#Method 1:\nridge = LogisticRegression(penalty=\"l2\",max_iter=200)\nridge.fit(X_train,Y_train)\nridge_Y_pred=ridge.predict(X_test)\nridge_probs = ridge.predict_proba(X_train)[:,1] #predicted probabilities on training data\nridge_accuracy = ridge.score(X_train, Y_train) #accuracy on training data\nridge_accuracy\n\n#Method 2:\nsgd = SGDClassifier(penalty=\"l2\",random_state=7)\nsgd.fit(X_train,Y_train)\nsgd_Y_pred=sgd.predict(X_test)\nsgd_accuracy = sgd.score(X_train, Y_train)\nsgd_accuracy","3b1306e9":"ridge_accuracy","85c67703":"# Logistic GAM\ngam = LogisticGAM()\ngam.fit(X_train,Y_train)\ngam_Y_pred=gam.predict(X_test)\ngam_Y_pred = gam_Y_pred*1\ngam_probs = gam.predict_proba(X_train) #predicted probabilities on training data\ngam_accuracy = gam.accuracy(X_train, Y_train) #accuracy on training data\ngam_accuracy","08a89be0":"# Support Vector Machine\nsvm = SVC(probability=True)\nsvm.fit(X_train, Y_train)\nsvm_Y_pred = svm.predict(X_test)\nsvm_probs = svm.predict_proba(X_train)[:,1] #predicted probabilities on training data\nsvm_accuracy = svm.score(X_train, Y_train) #accuracy on training data\nsvm_accuracy","33c4d03d":"# k-nearest neighbor\nknn = KNeighborsClassifier(n_neighbors = 1)\nknn.fit(X_train, Y_train)\nknn_Y_pred = knn.predict(X_test)\nknn_probs = knn.predict_proba(X_train)[:,1] #predicted probabilities on training data\nknn_accuracy = knn.score(X_train,Y_train) #accuracy on training data\nknn_accuracy","092f525a":"# Gaussian Naive Bayes\nnb = GaussianNB()\nnb.fit(X_train, Y_train)\nnb_Y_pred = nb.predict(X_test)\nnb_probs = nb.predict_proba(X_train)[:,1] #predicted probabilities on training data\nnb_accuracy = nb.score(X_train, Y_train) #accuracy on training data\nnb_accuracy","fc012140":"# Decision Tree\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\ndecision_tree_Y_pred = decision_tree.predict(X_test)\ndecision_tree_probs = decision_tree.predict_proba(X_train)[:,1] #predicted probabilities on training data\ndecision_tree_accuracy = decision_tree.score(X_train, Y_train) #accuracy on training data\ndecision_tree_accuracy","a0679c49":"# Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nrandom_forest_Y_pred = random_forest.predict(X_test)\nrandom_forest_probs = random_forest.predict_proba(X_train)[:,1] #predicted probabilities on training data\nrandom_forest.score(X_train, Y_train)\nrandom_forest_accuracy = random_forest.score(X_train, Y_train) #accuracy on training data\nrandom_forest_accuracy","77e880a1":"#XGBOOST\ndtrain = xgb.DMatrix(data=X_train,label=Y_train)\ndtest = xgb.DMatrix(X_test)\nparam = {'max_depth':11, 'eta':0.9, 'objective':'binary:logistic' }\nnum_round = 2\nmodel = xgb.train(param, dtrain, num_round)\n# make prediction for test data\npreds = model.predict(dtest)\nxgboost_Y_pred = [round(value) for value in preds]\n# make predictions for training data\npreds = model.predict(dtrain)\nY_pred = [round(value) for value in preds]\nxgboost_accuracy = accuracy_score(Y_train,Y_pred) #accuracy on training data\nxgboost_probs = preds #predicted probabilities on training data\nxgboost_accuracy\n","b67edab9":"#MLP\nmlp = MLPClassifier(solver='adam', alpha=1e-5,hidden_layer_sizes=(10, 5), random_state=1,max_iter=1000)\nmlp.fit(X_train, Y_train)\nmlp_Y_pred = mlp.predict(X_test)\nmlp_probs = mlp.predict_proba(X_train)[:,1] #predicted probabilities on training data\nmlp_accuracy = mlp.score(X_train, Y_train) #accuracy on training data\nmlp_accuracy","6874eb2f":"all_predicted = pd.DataFrame({\"Logistic\": logreg_Y_pred,\"Ridge\":ridge_Y_pred,\"GAM\": gam_Y_pred,\"SVM\": svm_Y_pred,\"KNN\": knn_Y_pred,\"Naive_Bayes\":nb_Y_pred,\"Tree\":decision_tree_Y_pred,\"RF\": random_forest_Y_pred,\"XGBOOST\": xgboost_Y_pred,\"MLP\":mlp_Y_pred})\n\nall_predicted","ade502bc":"# This creates an object final_pred which gives the final predicted class from all classifiers using majority votes method.\n\nfinal_pred = list(range(418))\nfor i in range(418):\n    if sum(all_predicted.loc[i])< 5:\n        final_pred[i] = 0\n    else : \n        final_pred[i] = 1","6524974c":"# submission file for a single classifier \nsubmission = pd.DataFrame({\"PassengerId\": test[\"PassengerId\"], \"Survived\": random_forest_Y_pred})\nsubmission.to_csv('submission.csv', index=False)\n\n# submission file for ensemble of all classifiers\nsubmission = pd.DataFrame({\"PassengerId\": test[\"PassengerId\"], \"Survived\": final_pred})\nsubmission.to_csv('submission.csv', index=False)","76c075e0":"from sklearn.metrics import roc_auc_score, roc_curve\n\nclassifiers = [\"Logistic\",\"Ridge\",\"GAM\",\"SVM\",\"KNN\",\"NB\",\"Decision Tree\",\"RF\",\"xgboost\",\"MLP\"]\n\npredicted_probs = pd.DataFrame({\"Logistic\": logreg_probs,\"Ridge\":ridge_probs,\"GAM\": gam_probs,\"SVM\": svm_probs,\"KNN\": knn_probs,\"Naive_Bayes\":nb_probs,\"Tree\":decision_tree_probs,\"RF\": random_forest_probs,\"XGBOOST\": xgboost_probs,\"MLP\":mlp_probs})\n\npredicted_probs","ee87736e":"#Draw ROC Curves\nplt.figure(figsize=(10,7))\nplt.plot([0,1], [0,1], linestyle='--')\nfor i in range(len(classifiers)):\n    fpr , tpr, thresholds = roc_curve(Y_train, predicted_probs.iloc[:,i])\n    plt.plot(fpr, tpr, label= classifiers[i])\n\nplt.legend()\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")\nplt.title('Receiver Operating Characteristic')\nplt.show()   ","b18b3a05":"#Obtain AUC values for all the cassifiers for the training data\nauc = list(range(len(classifiers)))\nfor i in range(len(classifiers)):\n    auc[i] = roc_auc_score(Y_train, predicted_probs.iloc[:,i])\n  \npd.DataFrame({\"Classifier\":classifiers,\"AUC\":auc})","6d029bb4":"accuracy = [logreg_accuracy,ridge_accuracy,gam_accuracy,svm_accuracy,knn_accuracy,nb_accuracy,decision_tree_accuracy,random_forest_accuracy,xgboost_accuracy,mlp_accuracy]\n\npd.DataFrame({\"Classifier\":classifiers,\"Accuracy\":accuracy})","263aac01":"# Exploratory Data Analysis (EDA)","0e09d2a3":"# Importing all the Supervised machine learning models","6986546f":"# 1. Logistic Regression","fa848da1":" # Note : \n* Feature engineering is not performed in this notebook since the focus of this notebook is to give general idea of how to implement different classifiers and compare them with metrics such as accuracy and AUC.\n* Before using these models for your data, make sure to optimize them according to your dataset.","6e51ced3":"# 10. Multilayer Perceptron (MLP) : Neural network model","fb9d4512":"# 9. Extreme Gradient Boosting (xgboost)","3b14ba74":"# 2. Accuracy Metric","b0a06fb2":"**The features in the dataset are as follows :**\n\n1.survival =\tSurvival (0 = No, 1 = Yes)\n\n2.pclass =\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd\n          A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\n3.Name = Name of the passanger\n\n4.sex =\tSex\t\n5.Age = \tAge in years (Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5)\n\n6.sibsp =\t# of siblings \/ spouses aboard the Titanic\t\nThe dataset defines family relations in this way...\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n7.parch =\t# of parents \/ children aboard the Titanic\t\nThe dataset defines family relations in this way...\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.\n\n8.ticket =\tTicket number\t\n9.fare =\tPassenger fare\t\n10.cabin =\tCabin number\t\n11.embarked =\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton","a84e7f42":"# 8. Random Forest (RF)","063ac726":"# Dataset Description","c4295398":"We can see clearly that class status has effect on the survival. Upper class people had more chances of survival.\nSimilarly, we can see the effect of other variables on survival.","38d158a4":"# This notebook will give an idea of how to implement following 10 classifiers on the very popular titanic dataset and also assess them using ROC and Accuracy analysis.\n<div class=\"alert alert-block alert-success\">  \n<ol>\n<li>  Logistic Regression <\/li>\n<li>  Regularization : Ridge Regression <\/li>\n<li>  Generalized Additive Model (GAM)<\/li>\n<li>  Support Vector Machine (SVM)<\/li>\n<li>  K-Nearest Neighbour (KNN)<\/li>\n<li>  Naive Bayes (NB)<\/li>\n<li>  Decision Tree<\/li>\n<li>  Random Forest (RF)<\/li>\n<li>  Extreme Gradient Boosting (xgboost)<\/li>\n<li> Multilayer Perceptron : Neural Network Model<\/li>\n<\/ol>\n<\/div>","fd232e26":"We substitute the missing values in the variable 'Age' with mean values. (You may use median values as well).","a2cc5541":"# If we compare all the ten classifiers based on both accuracy and AUC score then Decision tree and random forest perform better than other models.","d089fcc1":"# Create data for machine learning algorithms","735471ec":"Since, the test data labels are not available, we use performance metrics on training data predictions.","ff0ac039":"We perform the same data cleaning on the test data as well.","4c5a23fc":"# 6. Naive Bayes (NB)","bdced637":"#  Thank You!","d8a8bddf":"# I hope this notebook helps to answer questions regarding classiication.","dcff4591":"# Submission file (.csv file)","aecb7201":"# 3. Generalized Additive Model (GAM) ","c69a7cf8":"# Cleaned training and test data","63de375f":"# We plot all the categorical variables vs Survived to see the pattern","dfff7bad":"# Combining the predicted class from all classifiers","8faf4928":"Next, we drop this categorical variable from the dataset to get the original dataset.","f3246526":"# Missing Value Imputation","f47e60d2":"# 1. ROC analysis","b9cc7f64":"# Convert the continuous variable `Age' into categorical variable and see its effect on survival. ","d5718080":"It seems females were survived in much bigger number than that of males.","6ca5e075":"# Obtain the cross-table from two categorical variables","3089d65c":"# 5. K- Nearest Neighbour (KNN)","b28e468a":"# Accuracy and ROC assessment of all the classifiers","01e0e3ea":"# 4. Support Vector Machine (SVM)","1d37c594":"# 7. Decision Tree","9fb37298":"# Train and predict survival using 10 different classifiers.","370c6988":"There are missing values in three variables - Age,Cabin,Embarked.","a2df35c0":"# 2. Regularization : Ridge Regression","59a1189e":"# Importing important libraries"}}