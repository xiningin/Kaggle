{"cell_type":{"2d1db48a":"code","31065bf5":"code","5db88e7e":"code","58c2f2e6":"code","38620ba6":"code","8968e681":"code","09780ceb":"code","433f4c06":"code","083ce570":"code","7f20fec4":"code","7d10ff1f":"code","864acf06":"code","2bad0e4f":"code","a2242e1f":"code","a313f322":"code","06457430":"code","be10754f":"code","e5a55913":"code","3413b2c2":"code","a966b905":"code","79fbb294":"code","c41e71b2":"code","a6b5091b":"code","5715ec5f":"code","dbaef1ea":"code","876a7e3d":"code","d4ccf32c":"code","46344cbd":"code","10e2d0c3":"code","888a8ec9":"code","dc4503cd":"code","4d5b4178":"code","5dc751c5":"code","5cd3c243":"code","1d6e1fc6":"code","d589d600":"code","7dfecdd7":"code","d32e8456":"code","282f4cbe":"markdown","cbab1b01":"markdown","7e5f679b":"markdown","992b7b8d":"markdown","f0f519b0":"markdown","f83e07d7":"markdown","225bcf28":"markdown","d71de1f6":"markdown","a8b9f334":"markdown","e383428f":"markdown","075a8bba":"markdown","9a8f2f8d":"markdown","b779bd59":"markdown","ec02b9f2":"markdown","77b80442":"markdown"},"source":{"2d1db48a":"from IPython.display import Image\nImage(\"..\/input\/image-house-prices-advanced-regression\/image.jpeg\")\n","31065bf5":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nimport xgboost","5db88e7e":"df_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_train.head()","58c2f2e6":"df_train.shape","38620ba6":"df_train.dtypes","8968e681":"df_train.describe()","09780ceb":"df_train.isna().sum()","433f4c06":"df_train.corr()","083ce570":"# Skewness\ndf_train.skew()","7f20fec4":"class_counts = df_train['SalePrice'].value_counts()\nclass_counts","7d10ff1f":"# Histogram\ndf_train.hist(figsize=(20, 10))","864acf06":"# Scatter plot\nfig, ax = plt.subplots()\nax.scatter(df_train['Id'], df_train['SalePrice'])","2bad0e4f":"def preprocess_data(df):\n    \"\"\"\n    Performs transformations on df and returns transformed df.\n    \"\"\"\n    # Fill the numeric rows with median\n    for label, content in df.items():\n        if pd.api.types.is_numeric_dtype(content):\n            if pd.isnull(content).sum():\n                # Add a binary column which tells us if the data was missing or not\n                df[label+\"_is_missing\"] = pd.isnull(content)\n                # Fill missing numeric values with median\n                df[label] = content.fillna(content.median())\n    \n        # Filled categorical missing data and turn categories into numbers\n        if not pd.api.types.is_numeric_dtype(content):\n            df[label+\"_is_missing\"] = pd.isnull(content)\n            # We add +1 to the category code because pandas encodes missing categories as -1\n            df[label] = pd.Categorical(content).codes+1\n    \n    return df","a2242e1f":"# Process the test data \ndf_train = preprocess_data(df_train)\ndf_train.head()","a313f322":"# Process the test data \ndf_train = preprocess_data(df_train)\ndf_train.head()","06457430":"df_train.info()","be10754f":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression, Lasso\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error, mean_absolute_error, r2_score\n\nX = df_train.drop('SalePrice', axis=1)\ny = df_train['SalePrice']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2)\n\n# Put models in a dictionary\nmodels = {'Logistic Regression': LogisticRegression(),\n          'linear_regression' : LinearRegression(),\n          'Random Forest': RandomForestRegressor(),\n          'linear_model' : linear_model.Lasso(alpha=0.1),\n          'XGBoost' : XGBRegressor()\n           \n          }\n\n# Create a function to fit and score models\ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n    \"\"\"\n    Fits and evaluates given machine learning models.\n    models : a dict of differetn Scikit-Learn machine learning models\n    X_train : training data (no labels)\n    X_test : validation data (no labels)\n    y_train : training labels\n    y_test : validation labels\n    \"\"\"\n    # Set random seed\n    np.random.seed(42)\n    # Make a dictionary to keep model scores\n    model_scores = {}\n    # Loop through models\n    for name, model in models.items():\n        # Fit the model to the data\n        model.fit(X_train, y_train)\n        # Evaluate the model and append its score to model_scores\n        model_scores[name] = model.score(X_test, y_test)\n    return model_scores\n    ","e5a55913":"model_scores = fit_and_score(models=models,\n                             X_train=X_train,\n                             X_test= X_test,\n                             y_train=y_train,\n                             y_test=y_test)\n\nmodel_scores\n\n","3413b2c2":"models = {'Logistic Regression': LogisticRegression(),\n          'linear_regression' : LinearRegression(),\n          'Random Forest': RandomForestRegressor(),\n          'linear_model' : linear_model.Lasso(alpha=0.1),\n          'XGBoost' : XGBRegressor()\n           \n          }\n\n# Logistic Regression\nmodel1 = LogisticRegression()\nmodel1.fit(X_train, y_train)\ny_preds = model1.predict(X_test)\nprint('Logistic Regression')\nprint('MAE', mean_absolute_error(y_test, y_preds))\nprint('RMSLE', mean_squared_log_error(y_test, y_preds))\nprint('r-squared', r2_score (y_test, y_preds))\n\n# linear_regression\nmodel2 = LinearRegression()\nmodel2 = LogisticRegression()\nmodel2.fit(X_train, y_train)\ny_preds = model2.predict(X_test)\nprint('linear_regression')\nprint('MAE', mean_absolute_error(y_test, y_preds))\nprint('RMSLE', mean_squared_log_error(y_test, y_preds))\nprint('r-squared', r2_score (y_test, y_preds))\n\n# Random Forest\nmodel3 = RandomForestRegressor()\nmodel3.fit(X_train, y_train)\ny_preds = model3.predict(X_test)\nprint('Random Forest')\nprint('MAE', mean_absolute_error(y_test, y_preds))\nprint('RMSLE', mean_squared_log_error(y_test, y_preds))\nprint('r-squared', r2_score (y_test, y_preds))\n\n# linear_model\nmodel4 = linear_model.Lasso(alpha=0.1)\nmodel4.fit(X_train, y_train)\ny_preds = model4.predict(X_test)\nprint('linear_model')\nprint('MAE', mean_absolute_error(y_test, y_preds))\nprint('RMSLE', mean_squared_log_error(y_test, y_preds))\nprint('r-squared', r2_score (y_test, y_preds))\n\n# XGBoost\nmodel5 = XGBRegressor()\nmodel5.fit(X_train, y_train)\ny_preds = model5.predict(X_test)\nprint('XGBoost')\nprint('MAE', mean_absolute_error(y_test, y_preds))\nprint('RMSLE', mean_squared_log_error(y_test, y_preds))\nprint('r-squared', r2_score (y_test, y_preds))\n\n\n\n\n","a966b905":"%%time\n\n# Instantiate model\nmodel = XGBRegressor()\n\n# Fit the model\nmodel.fit(X_train, y_train)\ny_preds = model.predict(X_test)\n\n","79fbb294":"from sklearn.model_selection import GridSearchCV\ngrid = {'n_estimators' : [10, 1000],\n        'learning_rate': [0.01, 0.1],\n        'gamma' : [0.1, 5],\n        'max_depth' : [3, 5, 8], \n        'subsample' : [0.8 ,0.9, 1],\n        'colsample_bytree' : [0.3, 0,8],\n        'gamma' : [0.1, 5]\n        \n    \n}\n\n\nxgb_model = GridSearchCV(XGBRegressor(),\n\n                              param_grid=grid,\n                              \n                              cv=5,\n                              verbose=20)\n\nxgb_model.fit(X_train, y_train)","c41e71b2":"\n\n# Find the best model hyperparameters\nxgb_model.best_params_","a6b5091b":"best_model = XGBRegressor(colsample_bytree=0.3,\n                          gamma=0.1,\n                          learning_rate=0.01,\n                          max_depth=5,\n                          n_estimators=1000,\n                          subsample=1)","5715ec5f":"# Fit the model\nbest_model.fit(X_train, y_train)\ny_preds = best_model.predict(X_test)\n\nbest_model.score(X_test, y_test)","dbaef1ea":"\nprint('MAE', mean_absolute_error(y_test, y_preds))\nprint('MSLE', mean_squared_log_error(y_test, y_preds))\nprint('r-squared', r2_score (y_test, y_preds))\n","876a7e3d":"# Import the test data\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf_test.head()","d4ccf32c":"# Process the test data \ndf_test = preprocess_data(df_test)\ndf_test.head()","46344cbd":"df_train.head()","10e2d0c3":"set(df_test.columns) - set(X_train.columns)","888a8ec9":"# Manually adjust df_train to missing columns\ndf_test = df_test.drop('BsmtFinSF1_is_missing', axis=1)                            \ndf_test = df_test.drop('BsmtFinSF2_is_missing', axis=1)\ndf_test = df_test.drop('BsmtFullBath_is_missing', axis=1) \ndf_test = df_test.drop('BsmtHalfBath_is_missing', axis=1)\ndf_test = df_test.drop('BsmtUnfSF_is_missing', axis=1)\ndf_test = df_test.drop('GarageArea_is_missing', axis=1)\ndf_test = df_test.drop('GarageCars_is_missing', axis=1)\ndf_test = df_test.drop('TotalBsmtSF_is_missing', axis=1)\ndf_test.head()","dc4503cd":"df_test.head()","4d5b4178":"# Make predictions on updated test data\n\nmodel.fit(X_train, y_train)\n\ntest_preds = model.predict(df_test)","5dc751c5":"test_preds","5cd3c243":"df_preds_test = pd.DataFrame()\ndf_preds_test['Id'] = df_test['Id']\ndf_preds_test[\"SalePrice\"] = test_preds\ndf_preds_test","1d6e1fc6":"# Export prediction data\ndf_preds_test.to_csv(\".\/HousePricesAdvanced.csv\", index=False)","d589d600":"# Feature Importance with Extra Trees Classifier\nfrom sklearn.ensemble import ExtraTreesClassifier\n# feature extraction\nmodel = ExtraTreesClassifier()\nmodel.fit(X, y)\nprint(model.feature_importances_)","7dfecdd7":"import seaborn as sns\n\n# Helper function for plotting feature importance\ndef plot_features(columns, importances, n=20):\n    data = (pd.DataFrame({\"features\": columns,\n                        \"feature_importance\": importances})\n          .sort_values(\"feature_importance\", ascending=False)\n          .reset_index(drop=True))\n    \n    sns.barplot(x=\"feature_importance\",\n                y=\"features\",\n                data=data[:n],\n                orient=\"h\")","d32e8456":"plt.figure(figsize=(20, 6))\nplot_features(X_train.columns, best_model.feature_importances_)","282f4cbe":"## 2.2 Evaluate Models","cbab1b01":"# 1. Data","7e5f679b":"## Format predictions asked by Kaggle","992b7b8d":"#  Predict sales prices and practice feature engineering, RFs, and gradient boosting\n\n## 1. Problem defition\n\n\n> Predict the value of the SalePrice variable. \n\n## 2. Data\n\n\nhttps:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data\n\n\n\n    **Data Details:**\n    \n* Here's a brief version of what you'll find in the data description file.\n* SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n* MSSubClass: The building class\n* MSZoning: The general zoning classification\n* LotFrontage: Linear feet of street connected to property\n* LotArea: Lot size in square feet\n* Street: Type of road access\n* Alley: Type of alley access\n* LotShape: General shape of property\n* LandContour: Flatness of the property\n* Utilities: Type of utilities available\n* LotConfig: Lot configuration\n* LandSlope: Slope of property\n* Neighborhood: Physical locations within Ames city limits\n* Condition1: Proximity to main road or railroad\n* Condition2: Proximity to main road or railroad (if a second is present)\n* BldgType: Type of dwelling\n* HouseStyle: Style of dwelling\n* OverallQual: Overall material and finish quality\n* OverallCond: Overall condition rating\n* YearBuilt: Original construction date\n* YearRemodAdd: Remodel date\n* RoofStyle: Type of roof\n* RoofMatl: Roof material\n* Exterior1st: Exterior covering on house\n* Exterior2nd: Exterior covering on house (if more than one material)\n* MasVnrType: Masonry veneer type\n* MasVnrArea: Masonry veneer area in square feet\n* ExterQual: Exterior material quality\n* ExterCond: Present condition of the material on the exterior\n* Foundation: Type of foundation\n* BsmtQual: Height of the basement\n* BsmtCond: General condition of the basement\n* BsmtExposure: Walkout or garden level basement walls\n* BsmtFinType1: Quality of basement finished area\n* BsmtFinSF1: Type 1 finished square feet\n* BsmtFinType2: Quality of second finished area (if present)\n* BsmtFinSF2: Type 2 finished square feet\n* BsmtUnfSF: Unfinished square feet of basement area\n* Total square feet of basement area\n* Heating: Type of heating\n* HeatingQC: Heating quality and condition\n* CentralAir: Central air conditioning\n* Electrical: Electrical system\n* 1stFlrSF: First Floor square feet\n* 2ndFlrSF: Second floor square feet\n* LowQualFinSF: Low quality finished square feet (all floors)\n* GrLivArea: Above grade (ground) living area square feet\n* BsmtFullBath: Basement full bathrooms\n* BsmtHalfBath: Basement half bathrooms\n* FullBath: Full bathrooms above grade\n* HalfBath: Half baths above grade\n* Bedroom: Number of bedrooms above basement level\n* Kitchen: Number of kitchens\n* KitchenQual: Kitchen quality\n* TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n* Functional: Home functionality rating\n* Fireplaces: Number of fireplaces\n* FireplaceQu: Fireplace quality\n* GarageType: Garage location\n* GarageYrBlt: Year garage was built\n* GarageFinish: Interior finish of the garage\n* GarageCars: Size of garage in car capacity\n* GarageArea: Size of garage in square feet\n* GarageQual: Garage quality\n* GarageCond: Garage condition\n* PavedDrive: Paved driveway\n* WoodDeckSF: Wood deck area in square feet\n* OpenPorchSF: Open porch area in square feet\n* EnclosedPorch: Enclosed porch area in square feet\n* 3SsnPorch: Three season porch area in square feet\n* ScreenPorch: Screen porch area in square feet\n* PoolArea: Pool area in square feet\n* PoolQC: Pool quality\n* Fence: Fence quality\n* MiscFeature: Miscellaneous feature not covered in other categories\n* MiscVal: $Value of miscellaneous feature\n* MoSold: Month Sold\n* YrSold: Year Sold\n* SaleType: Type of sale\n* SaleCondition: Condition of sale\n\n\n\n","f0f519b0":"## 1.3 Data preparation","f83e07d7":" ## 2.1 Building an evaluation function","225bcf28":"##  Evaluation\n\n The evaluation metric for this competition is the RMSLE (root mean squared log error) between the actual and predicted auction prices\n \n https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/overview\/evaluation\n\n","d71de1f6":"## Testing our model on a subset (to tune the hyperparameters)","a8b9f334":"### Choising the model XGBOOST\nscore: 0.828759856683738\nRMSLE 0.021842740178599024   ","e383428f":"### Make predictions on test data","075a8bba":"### Modelling","9a8f2f8d":"## 1.2 Data Visualisation","b779bd59":"### Hyerparameter tuning with GridSearchCV\n","ec02b9f2":"## Feature importance","77b80442":"## 1.1 Import Data"}}