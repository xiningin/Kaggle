{"cell_type":{"9074a32c":"code","66a71838":"code","9768cbf5":"code","8ae2b5ae":"code","af4a9c9a":"code","437415ca":"code","f65bae35":"code","7a878d9b":"code","00a2b47b":"code","afbafde1":"code","aeb6a82f":"code","44c02b33":"code","37d97f68":"code","1a557c90":"code","0ec1c6e4":"code","b4ef3102":"code","6e86d705":"code","0f5d8b19":"code","43200c03":"code","69bbbab3":"code","af9d5293":"code","a3a1f89f":"code","f2885e72":"code","3c113553":"code","adfaa20b":"code","9c2aa5bf":"markdown","0412b96b":"markdown","ec196c2f":"markdown","3b173fb0":"markdown","4f6b2831":"markdown","1af8163e":"markdown","f3b9bc78":"markdown","6f6a55c6":"markdown","5a7f6788":"markdown","c042175c":"markdown","a57cde36":"markdown","ba28a7f1":"markdown","4e3f57bc":"markdown"},"source":{"9074a32c":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt","66a71838":"df= pd.read_csv('..\/input\/real-manufacturing-and-trade-inventories-2020\/INVCMRMT.csv', index_col='DATE', parse_dates=True)\ndf['INVCMRMT']=df['INVCMRMT'].astype(int)\ndf.index.freq= 'MS'\ndf['INVCMRMT'].plot(figsize=(12,6)) ","9768cbf5":"len(df) #We will grap the last year for forecasting\n\ntrain= df.iloc[:265]\ntest = df.iloc[265:]","8ae2b5ae":"from sklearn.preprocessing import MinMaxScaler\nscaler= MinMaxScaler()\nscaler.fit(train)\nscaled_train= scaler.transform(train)\nscaled_test= scaler.transform(test)","af4a9c9a":"len(test)","437415ca":"from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\nlength= 12 #batch size should be smaller than test size\ngenerator= TimeseriesGenerator(scaled_train,scaled_train,length=length,batch_size=1)","f65bae35":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,LSTM\nn_features=1 # 1 variable and 1 col 'INVCMRMT'\n\nmodel= Sequential()\nmodel.add(LSTM(400,activation='relu', input_shape=(length,n_features))) #input shape of batch size\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam',loss='mse')\nmodel.summary()","7a878d9b":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stop = EarlyStopping(monitor='val_loss',patience=2) #patience is number of epochs with no improvment\nvalidation_generator= TimeseriesGenerator(scaled_test,scaled_test,length=length,batch_size=1)\nmodel.fit_generator(generator,epochs=10,validation_data=validation_generator,callbacks=early_stop)","00a2b47b":"losses= pd.DataFrame(model.history.history)\nlosses.plot()","afbafde1":"test_predictions = []\n\nfirst_eval_batch = scaled_train[-length:]\ncurrent_batch = first_eval_batch.reshape((1, length, n_features))\n\nfor i in range(len(test)):\n    \n    # get prediction 1 time stamp ahead ([0] is for grabbing just the number instead of [array])\n    current_pred = model.predict(current_batch)[0]\n    \n    # store prediction\n    test_predictions.append(current_pred) \n    \n    # update batch to now include prediction and drop first value\n    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)","aeb6a82f":"# IGNORE WARNINGS\ntrue_predictions = scaler.inverse_transform(test_predictions)\ntest['Predictions'] = true_predictions\ntest.plot(figsize=(12,8))","44c02b33":"test_predictions = []\n\nfirst_eval_batch = scaled_train[-length:]\ncurrent_batch = first_eval_batch.reshape((1, length, n_features))\n\nfor i in range(len(test)):\n    \n    # get prediction 1 time stamp ahead ([0] is for grabbing just the number instead of [array])\n    current_pred = model.predict(current_batch)[0]\n    \n    # store prediction\n    test_predictions.append(current_pred) \n    \n    # update batch to now include prediction and drop first value\n    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)","37d97f68":"true_predictions = scaler.inverse_transform(test_predictions)\ntest['Predictions'] = true_predictions","1a557c90":"test","0ec1c6e4":"test.plot(figsize=(12,8))","b4ef3102":"full_scaler= MinMaxScaler()\nscaled_full_data=full_scaler.fit_transform(df)","6e86d705":"length=12\ngenerator=TimeseriesGenerator(scaled_full_data,scaled_full_data,length=length,batch_size=1)\nmodel= Sequential()\nmodel.add(LSTM(400,activation='relu', input_shape=(length,n_features))) #input shape of batch size\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam',loss='mse')\n","0f5d8b19":"model.fit_generator(generator,epochs=6)","43200c03":"forecast = []\n# Replace periods with whatever forecast length you want\nperiods = 12\n\nfirst_eval_batch = scaled_full_data[-length:]\ncurrent_batch = first_eval_batch.reshape((1, length, n_features))\n\nfor i in range(periods):\n    \n    # get prediction 1 time stamp ahead ([0] is for grabbing just the number instead of [array])\n    current_pred = model.predict(current_batch)[0]\n    \n    # store prediction\n    forecast.append(current_pred) \n    \n    # update batch to now include prediction and drop first value\n    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)","69bbbab3":"forecast = scaler.inverse_transform(forecast)","af9d5293":"df","a3a1f89f":"forecast_index = pd.date_range(start='2020-08-01',periods=periods,freq='MS')\nforecast_df = pd.DataFrame(data=forecast,index=forecast_index,columns=['Forecast'])","f2885e72":"forecast_df","3c113553":"ax = df.plot()\nforecast_df.plot(ax=ax)","adfaa20b":"ax = df.plot()\nforecast_df.plot(ax=ax)\nplt.xlim('2018-08-01','2021-08-01')","9c2aa5bf":"#### 8- Forecasting","0412b96b":" - Creating new time series for forecast","ec196c2f":"#### 3- Train\/ Test split:","3b173fb0":"#### 2- Load data","4f6b2831":"#### 4- Scaling data :","1af8163e":"#### 1- Importing:","f3b9bc78":"- Grap test","6f6a55c6":"### Here in this notebook I will forecast to 2021 using Deep learning RNN and LSTM for time series\n### The special case of RNN Time series is RNN takes batches from the time series and make a train at each one!\n### for example if we have a series from 0 to 9 : [0,1,2,3,4,5,6,7,8,9]\n### frist we will devide it into baches :\n![](https:\/\/scontent.faly3-1.fna.fbcdn.net\/v\/t1.0-9\/121419725_824808158330800_8470839773498296736_n.jpg?_nc_cat=104&_nc_sid=730e14&_nc_ohc=f7BpQHJqLzUAX-JslIO&_nc_ht=scontent.faly3-1.fna&oh=7dfec2af80009b6043a0d429fa299da2&oe=5FA60AB5)\n\n### Second we take bach 1: [ 0,1,2,3] to predict 4\n### and bach 2 we take : [1,2,3,4] to predict 5 ...\n\n### For the forecasting it will be like :\n![](https:\/\/scontent.faly3-1.fna.fbcdn.net\/v\/t1.0-9\/121238276_824808138330802_5818175079055679020_o.jpg?_nc_cat=106&_nc_sid=730e14&_nc_ohc=vbaF1YMJ0YsAX-qt-ZS&_nc_ht=scontent.faly3-1.fna&oh=8ff20dc08795ce0496593038023356ca&oe=5FA68116)","5a7f6788":"#### 7- Loss Evaluation:","c042175c":"#### 6- Create Early stop:","a57cde36":"- plot the forecast","ba28a7f1":"- Grap predections","4e3f57bc":"#### 5- Create Timesereies Generator"}}