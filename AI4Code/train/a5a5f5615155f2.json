{"cell_type":{"50f4a654":"code","7aeafe98":"code","90b51cc4":"code","823b3564":"code","b9b1a99b":"code","7a03693d":"code","d2ec6596":"code","cf53f088":"code","2680a10f":"code","374fa97e":"code","ab82490f":"code","d44720a8":"code","e8c8ff5a":"code","13f0fb6b":"code","ad6b5a69":"code","2c0ea03f":"code","0f9927c3":"code","5edfa1b9":"code","1f17a071":"code","70086320":"code","beb4ac66":"code","cce0e86c":"code","d07d0620":"code","aa5a3132":"code","e480a520":"code","0ac89d42":"code","72107324":"code","d32b4aeb":"code","977f07f1":"code","52e2415f":"code","6f133ac3":"code","8639c7be":"code","ad52bb42":"code","27f23e92":"code","f2b4e9cd":"markdown","2b70bb45":"markdown","21b08b2a":"markdown","51b5bf41":"markdown","d24730b8":"markdown","6ad1bf70":"markdown","69ea074e":"markdown","4d75b960":"markdown","9f824499":"markdown","8a31894f":"markdown","3633f9df":"markdown","64233ffc":"markdown"},"source":{"50f4a654":"!pip install -U transformers\n#!pip install bert-extractive-summarizer","7aeafe98":"import numpy as np\n\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelWithLMHead\nimport torch\nimport pandas as pd\nimport time","90b51cc4":"import transformers\nprint(transformers.__version__)","823b3564":"#from transformers import pipeline; print(pipeline('sentiment-analysis')('I hate you'))","b9b1a99b":"df_1 = pd.read_csv('..\/input\/all-the-news\/articles1.csv')\ndf_2 = pd.read_csv('..\/input\/all-the-news\/articles2.csv')\ndf_3 = pd.read_csv('..\/input\/all-the-news\/articles3.csv')\ndfs = [df_1, df_2, df_3]\nfor frame in dfs:\n    frame.drop(columns=['Unnamed: 0', 'publication', 'author', 'date', 'month', 'url'], axis=1, inplace=True)\ndf = pd.concat(dfs)","7a03693d":"df.head()","d2ec6596":"df.describe","cf53f088":"model = AutoModelForSeq2SeqLM.from_pretrained('t5-base')\ntokenizer = AutoTokenizer.from_pretrained(\"t5-base\")","2680a10f":"start = time.time()\ninputs = tokenizer.encode(\"summarize: \" + df.iloc[0]['content'], return_tensors=\"pt\", max_length=512)\noutputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\ndecoded_T5 = tokenizer.decode(outputs[0])\nend = time.time()\nt5_time = (end - start)\nprint(t5_time)","374fa97e":"model = AutoModelWithLMHead.from_pretrained('bert-base-cased')\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")","ab82490f":"start = time.time()\ninputs = tokenizer.encode(\"summarize: \" + df.iloc[0]['content'], return_tensors=\"pt\", max_length=100)\noutputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\ndecoded_BERT = tokenizer.decode(outputs[0])\nend = time.time()\nBERT_time = (end-start)\nprint(BERT_time)","d44720a8":"model = AutoModelWithLMHead.from_pretrained('openai-gpt')\ntokenizer = AutoTokenizer.from_pretrained(\"openai-gpt\")","e8c8ff5a":"start = time.time()\ninputs = tokenizer.encode(\"summarize: \" + df.iloc[0]['content'], return_tensors=\"pt\", max_length=149)\noutputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\ndecoded_GPT = tokenizer.decode(outputs[0])\nend = time.time()\nGPT_time = (end - start)\nprint(GPT_time)","13f0fb6b":"model = AutoModelWithLMHead.from_pretrained('distilgpt2')\ntokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")","ad6b5a69":"start = time.time()\ninputs = tokenizer.encode(\"summarize: \" + df.iloc[0]['content'], return_tensors=\"pt\", max_length=149)\noutputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\ndecoded_DistilBERT = tokenizer.decode(outputs[0])\nend = time.time()\nDistilBERT_time = (end - start)\nprint(DistilBERT_time)","2c0ea03f":"model = AutoModelWithLMHead.from_pretrained('gpt2')\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")","0f9927c3":"start = time.time()\ninputs = tokenizer.encode(\"summarize: \" + df.iloc[0]['content'], return_tensors=\"pt\", max_length=149)\noutputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\ndecoded_GPT2 = tokenizer.decode(outputs[0])\nend = time.time()\nGPT2_time = (end - start)\nprint(GPT2_time)","5edfa1b9":"model = AutoModelForSeq2SeqLM.from_pretrained(\"deep-learning-analytics\/wikihow-t5-small\")\ntokenizer = AutoTokenizer.from_pretrained(\"deep-learning-analytics\/wikihow-t5-small\")","1f17a071":"start = time.time()\ninputs = tokenizer.encode(\"summarize: \" + df.iloc[0]['content'], return_tensors=\"pt\", max_length=1024)\noutputs = model.generate(inputs, max_length=200, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\ndecoded_T5_wikihow = tokenizer.decode(outputs[0])\nend = time.time()\nt5_wikihow_time = (end - start)\nprint(t5_wikihow_time)","70086320":"model = AutoModelForSeq2SeqLM.from_pretrained(\"google\/pegasus-newsroom\")\ntokenizer = AutoTokenizer.from_pretrained(\"google\/pegasus-newsroom\")","beb4ac66":"start = time.time()\nbatch = tokenizer.prepare_seq2seq_batch(df['content'].tolist()[:1], max_target_length=200, padding='longest')\ntranslated = model.generate(**batch)\ntgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\nend = time.time()\ndecoded_pegasus_newsroom = tgt_text[0]\npegasus_newsroom_time = (end - start)\nprint(pegasus_newsroom_time)","cce0e86c":"tokenizer = AutoTokenizer.from_pretrained(\"google\/pegasus-cnn_dailymail\")\nmodel = AutoModelWithLMHead.from_pretrained(\"google\/pegasus-cnn_dailymail\")","d07d0620":"start = time.time()\nbatch = tokenizer.prepare_seq2seq_batch(df['content'].tolist()[:1], max_target_length=200, padding='longest')\ntranslated = model.generate(**batch)\ntgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\nend = time.time()\ndecoded_pegasus_cnn = tgt_text[0]\npegasus_cnn_time = (end - start)\nprint(pegasus_cnn_time)","aa5a3132":"decoded_BERT","e480a520":"decoded_T5","0ac89d42":"decoded_DistilBERT","72107324":"decoded_GPT","d32b4aeb":"decoded_DistilBERT","977f07f1":"decoded_T5_wikihow","52e2415f":"decoded_GPT2","6f133ac3":"decoded_pegasus_newsroom","8639c7be":"decoded_pegasus_cnn","ad52bb42":"import matplotlib.pyplot as plot\ntime_array = {'Model': [\"BERT\", \"T5\", \"DistilBERT\", \"GPT\", \"GPT2\", \"T5 wikihow\", \"Pegasus News Room\", \"Pegasus CNN DailyMail\"], 'Times': [BERT_time, t5_time, DistilBERT_time, GPT_time, GPT2_time, t5_wikihow_time, pegasus_newsroom_time, pegasus_cnn_time]}\ndf_time = pd.DataFrame(data=time_array)\ndf_time.sort_values('Times').plot.bar(x=\"Model\", y=\"Times\", title=\"Time to Summarize for each model\", ylabel=\"Time in Seconds\", xlabel=\"\", legend=False)\nplot.show()","27f23e92":"length_dict = {\"Model\": [\"Normal\", \"BERT\", \"T5\", \"DistilBERT\", \"GPT\", \"GPT2\", \"T5 Wikihow\", \"Pegasus News Room\", \"Pegasus CNN DailyMail\"], \"Length\": [(len(df.iloc[0]['content'])), (len(decoded_BERT)), (len(decoded_T5)), (len(decoded_DistilBERT)), (len(decoded_GPT)), (len(decoded_GPT2)), (len(decoded_T5_wikihow)), (len(decoded_pegasus_newsroom)), (len(decoded_pegasus_cnn))]}\ndf_length = pd.DataFrame(data=length_dict)\ndf_length.sort_values('Length').plot.bar(x=\"Model\", y=\"Length\", title=\"Length of Model Summary\", ylabel=\"Length\", xlabel=\"\", legend=False)\nplot.show()","f2b4e9cd":"### BERT","2b70bb45":"### Pegasus NewsRoom","21b08b2a":"## Results Analysis","51b5bf41":"### GPT2","d24730b8":"### T5","6ad1bf70":"### Read in Data\nReads in the 3 csv files, drops unneeded columns, then concatenates them together. ","69ea074e":"### Pegasus CNN Daily Mail","4d75b960":"# Summarization Comparison\nImports necessary libraries. For this project I'm going to be using pytorch instead of tensorflow.","9f824499":"### GPT ","8a31894f":"### T5 Trained on Wikihow","3633f9df":"## Summarizing Using Different Models\n\nUsed different models through huggingface's transformers","64233ffc":"### DistilBERT"}}