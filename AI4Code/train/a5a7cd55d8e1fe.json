{"cell_type":{"e063a81a":"code","9712457f":"code","b2694801":"code","840e4e6d":"code","a866e1f9":"code","b4fd453e":"code","60113a1f":"code","accda3d9":"code","8e285181":"code","2922fff1":"code","c6389e18":"code","ebe2d914":"code","90a9c427":"code","7dec0b8a":"code","63339f44":"code","ae539b68":"code","32f8e6f7":"code","0d02300f":"code","75512f31":"code","ca4b3a1e":"markdown","50b42072":"markdown","0d4f72df":"markdown","e67fb7bc":"markdown"},"source":{"e063a81a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.multiclass import OneVsRestClassifier\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder, LabelBinarizer\n\nfrom sklearn.model_selection import GroupKFold, cross_val_score\nfrom sklearn.metrics import log_loss","9712457f":"train = pd.read_csv('..\/input\/train.csv.gz')\ntest = pd.read_csv('..\/input\/test.csv.gz')","b2694801":"train.fillna('', inplace=True)\ntest.fillna('', inplace=True)","840e4e6d":"vectorizer = CountVectorizer()\nlabeler = LabelEncoder()\nlb = LabelBinarizer()","a866e1f9":"train.head(3)","b4fd453e":"test.head(3)","60113a1f":"X = vectorizer.fit_transform(train.name.values)\nX_test = vectorizer.transform(test.name.values)","accda3d9":"y = labeler.fit_transform(train.category)","8e285181":"labels = lb.fit_transform(y) # matrix with 0 and 1 labels","2922fff1":"logist = LogisticRegression()\nmnb = MultinomialNB()","c6389e18":"gkf = list(GroupKFold(n_splits=3).split(X, y, train.check_id.values)) # spliter for train","ebe2d914":"%%time\nscore = cross_val_score(logist, X, y, scoring='neg_log_loss', cv = gkf, n_jobs = -1)\nprint(\"%.3f +- %.4f\" % (-np.mean(score), np.std(score)))","90a9c427":"print('Score for 1st split for LogisticRegression = %.3f' % (-score[0]))","7dec0b8a":"%%time\nscore = cross_val_score(mnb, X, y, scoring='neg_log_loss', cv = gkf, n_jobs = -1)\nprint(\"%.3f +- %.4f\" % (-np.mean(score), np.std(score)))","63339f44":"print('Score for 1st split for MultinomialNB = %.3f' % (-score[0]))","ae539b68":"X_train, X_valid, y_train, y_valid = X[gkf[0][0]], X[gkf[0][1]], labels[gkf[0][0], :], labels[gkf[0][1],:]","32f8e6f7":"X_train.shape, X_valid.shape, y_train.shape, y_valid.shape","0d02300f":"preds = []\nfor idx, category in enumerate(labeler.classes_):\n    #print (' Prediction for %s' % category)\n    mnb.fit(X_train, y_train[:, idx])\n    ratio = mnb.feature_log_prob_[1] - mnb.feature_log_prob_[0]\n    logist.fit(X_train.multiply(ratio), y_train[:, idx])\n    preds.append(logist.predict_proba(X_valid.multiply(ratio))[:,1])","75512f31":"print('Score after applying NB-SVM on data = %.3f' % (log_loss(y_valid, np.array(preds).T)))","ca4b3a1e":"## Split data for train and valid set","50b42072":"\n#### As example use 1st slit for NB - SVM  improvement","0d4f72df":"Inpire by this [kernel](http:\/\/www.kaggle.com\/jhoward\/nb-svm-strong-linear-baseline)\nIf you wanna read more - go to Jeremy Howard kernel\n","e67fb7bc":"## **Read data**"}}