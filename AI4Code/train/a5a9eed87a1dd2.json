{"cell_type":{"9ecc177e":"code","01f0b5f0":"code","4223f552":"code","b976891e":"code","1d8c01e7":"code","dcc6ad24":"code","22b6fbf5":"code","e6c4071f":"code","b703f6b9":"code","d04fa0ae":"code","4d7cc06b":"code","65e91e07":"code","a5acc7c5":"code","cfb6c03d":"code","dce1e9ad":"markdown","e84bd750":"markdown","72eb9ee0":"markdown","26bd75a5":"markdown","1d5b6d29":"markdown","1de140a5":"markdown","157e2e3b":"markdown","b6d45d4e":"markdown","a3eb7030":"markdown","66a17dae":"markdown","49da2c1b":"markdown","d9bce00c":"markdown","f0c822ec":"markdown","87f74314":"markdown","32997a95":"markdown"},"source":{"9ecc177e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport plotly.express as px\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set()","01f0b5f0":"df = pd.read_csv(\"..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\")\ndf.head()","4223f552":"df.describe()","b976891e":"assert not df.isna().any().any(), \"Missing values found\"","1d8c01e7":"df[\"DEATH_EVENT\"].value_counts()","dcc6ad24":"fig = px.scatter_matrix(df, color=\"DEATH_EVENT\")\nfig.update_traces(diagonal_visible=False)\nfig.update(layout_showlegend=False, layout_coloraxis_showscale=False)\nfig.show()","22b6fbf5":"cols_to_plot = [\"DEATH_EVENT\", \"serum_sodium\", \"serum_creatinine\", \"platelets\", \"creatinine_phosphokinase\", \"age\", \"ejection_fraction\", \"time\"]\ndf_to_plot = df[cols_to_plot].melt(id_vars=[\"DEATH_EVENT\"])\nfig = px.box(df_to_plot, color=\"DEATH_EVENT\", y=\"value\", facet_col=\"variable\")    \nfig.update_yaxes(matches=None)\nfig.show()","e6c4071f":"df[\"age_th\"] = df[\"age\"].apply(lambda x: 0 if x < 65 else 1)\ndf[\"serum_sodium_th\"] = df[\"serum_sodium\"].apply(lambda x: 0 if x < 135 else 1)\ndf[\"serum_creatin_th\"] = df[\"serum_creatinine\"].apply(lambda x: 0 if x < 1.2 else 1)\ndf[\"time_th\"] = df[\"time\"].apply(lambda x: 0 if x < 95 else 1)\ndf[\"ejection_fraction_th\"] = df[\"ejection_fraction\"].apply(lambda x: 0 if x < 38 else 1)","b703f6b9":"df.describe()","d04fa0ae":"from sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.metrics import f1_score\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import SelectFromModel\nimport xgboost","4d7cc06b":"%%time\nlogistic_param_grid = {\"logisticregression__solver\": [\"liblinear\"], \"logisticregression__C\": np.arange(0.2, 1.6, 0.1)}\nlogreg_pipeline = make_pipeline(StandardScaler(), LogisticRegression())\nclf = GridSearchCV(logreg_pipeline, n_jobs=-1, cv=StratifiedShuffleSplit(5, random_state=10), param_grid=logistic_param_grid)\nclf.fit(df.drop(\"DEATH_EVENT\", axis=1), df[\"DEATH_EVENT\"])\nprint(\"F1 Score: \", clf.best_score_)\nprint(\"Best Params\", clf.best_params_)\nbest_clf = clf.best_estimator_  # This is the best estimator","65e91e07":"weights = pd.Series(dict(zip(df.drop(\"DEATH_EVENT\", axis=1).columns, best_clf[1].coef_[0])))\nweights.sort_values().plot.bar(figsize=(20, 5))\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Weight\")\nplt.title(\"Feature Importance\")\nplt.show()","a5acc7c5":"%%time\n\n# I used a bigger grid initially, but I reduced it now for the sake of speeding the process. Check previous commits\nxgboost_param_grid = {\n    'booster': ['gbtree'], \n    'colsample_bytree': [0.9], \n    'eta': [0.5], \n    'eval_metric': ['auc'], \n    'gamma': [0.0], \n    'lambda': [0.8], \n    'min_child_weight': [9], \n    'n_estimators': [600], \n    'subsample': [0.9]}\n\nclf = GridSearchCV(xgboost.XGBClassifier(), n_jobs=-1, cv=StratifiedShuffleSplit(5, random_state=10), param_grid=xgboost_param_grid)\nclf.fit(df.drop(\"DEATH_EVENT\", axis=1), df[\"DEATH_EVENT\"])\nprint(\"F1 Score: \", clf.best_score_)\nprint(\"Best Params\", clf.best_params_)\nbest_clf = clf.best_estimator_  # This is the best estimator","cfb6c03d":"xgboost.plot_importance(best_clf)\nplt.show()","dce1e9ad":"# Visualize the scatter matrix\nThe axis labels are a bit messy, but it is possible to get more information by hovering. I am trying to see if there is some important information here, but at first glance there is not much.","e84bd750":" Some of the features we created, such as serum_creatin_th ended up being pretty good, other like age_th, not so much.\n \n Let's se if we can improve our outcome by creating polynomial features for our logistic regression","72eb9ee0":"## Check if the label is balanced\nLuckily the label is not very unbalanced","26bd75a5":"# Testing model testing\nNow we will create a pipeline for testing different models, perform grid search and select the best model\n\n## Metric used\nWe will use the f1_score metric for this task, since we want to access how well our model can identify the positive samples (deaths by hear failure)","1d5b6d29":"# Conclusion","1de140a5":"## Logistic Regression Baseline\nBuilding a linear baseline with Logistic Regression.","157e2e3b":"# Data Loading and Exploration","b6d45d4e":"## Improving with XGBOOST\nNow we will try to improve our baseline with XGBOOST, this is a much more sofisticated and complex model than Logistic Regression.","a3eb7030":"We got 85% F1 score. Let's see if the features we created were any good","66a17dae":"# Heart Rate death prediction\nBinary Classification for predicting the Heart Rate failure. ","49da2c1b":"We got 92% F1 score. Now let's take a look at feature importance","d9bce00c":"## Assert that there are no missing values","f0c822ec":"In this notebook we have built a notebook for predicting the death events by heart failure. We started out with a Logistic Regression Model and then improved upon it using XGBOOST. The results obtained were the following:\n\n- Logistic Regression: 84% F1 Score\n- XGBOOST: 92% F1 Score","87f74314":"# Creating Features\nBased on the graph above, we can create thresholds for the features. ","32997a95":"## Visualize continuous columns\nNot we will try to visualize the continuous variables, and see if we can create more features to make the decision boundary more clear. "}}