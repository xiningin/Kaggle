{"cell_type":{"cc81b8c1":"code","1029707c":"code","158e5313":"code","d13a1ca8":"code","e986eb1d":"code","01e560f5":"code","22c47345":"code","efc6196d":"code","deb31677":"code","53223822":"code","e05ca860":"code","7094c11c":"code","b3e6e421":"code","59318d96":"code","86b5da47":"code","a0801591":"code","38c0c04d":"code","960951b5":"code","7f558643":"code","0219431e":"code","eb07b525":"code","5d3df3d1":"code","a39a963b":"code","676473d4":"code","6a63722b":"code","971b5de4":"code","3c4c4b83":"code","fb232bbe":"code","bf474877":"code","7c906408":"code","08e2a6bc":"code","94787cf4":"code","96aeed2c":"code","68ccddde":"code","5befbe88":"code","a2d336f3":"code","fff211ad":"code","00fe5b2c":"code","4658739a":"code","64687ca7":"code","10bc910d":"code","01508576":"code","257d469f":"code","9aa2a2b7":"code","c6892d29":"code","11a92ba3":"code","df1194a1":"code","456f57b3":"code","c7e2cb64":"code","2f068e8e":"code","5068844c":"code","80eff586":"code","18086910":"code","dbe61969":"code","8454bee7":"code","a08c2918":"code","31a0bc8d":"code","c4dd1ebb":"code","aca7e372":"code","19ede4b0":"code","5e4fabfd":"code","5745aa1e":"code","73ee8100":"code","05f06388":"code","7f929182":"code","05ec80e0":"code","833db6d3":"code","88dcb375":"code","7d609638":"code","4e7ff065":"code","bea94051":"code","4958d77e":"code","9080d529":"code","cd212964":"code","c024ee7d":"code","a767386f":"code","c3c0cb23":"code","9dd214a6":"code","6ae4de57":"code","4b7279ef":"code","c295bbbd":"code","f85013e3":"code","c8af96e9":"code","60d672b3":"code","686e98fd":"code","ce5b3b35":"code","144023aa":"markdown","2c257f58":"markdown","a234538c":"markdown","0787e570":"markdown","f2931a29":"markdown","e068774d":"markdown","fbf11c7c":"markdown","cd92d2f2":"markdown","4a25541a":"markdown","0e53dfcb":"markdown","2ba68cb6":"markdown","58351b35":"markdown","ba7575a3":"markdown","5c3d2c36":"markdown","1d2e19b9":"markdown","ab5797ce":"markdown","49335da5":"markdown","ec5821f6":"markdown","7a38fbc5":"markdown","80998a14":"markdown","cc5c61ca":"markdown","f0863b7c":"markdown","24edd78f":"markdown","2ef6f702":"markdown","9b749df5":"markdown"},"source":{"cc81b8c1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1029707c":"# load the dataset\ntrain_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nsubmission_data = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')","158e5313":"#Observe the head of the training dataset\ntrain_data.head()","d13a1ca8":"test_data.head()","e986eb1d":"# Checking the size of the dataset\ntrain_data.shape","01e560f5":"test_data.shape","22c47345":"submission_data.head()","efc6196d":"# Checking the info of the training dataset\ntrain_data.info()\n","deb31677":"#Another way of checking null values\nmissing_values = train_data.isnull().sum().sort_values(ascending=False)\npercentage = train_data.isnull().sum()\/train_data.isnull().count().sort_values(ascending=False)*100\nmissing_data = pd.concat([missing_values, percentage], axis=1, keys=['missing_values', 'percentage'])\nmissing_data.head(5)","53223822":"#Fill the Nulls in the Age column\ntrain_data['Age'] = train_data['Age'].fillna(train_data['Age'].mean())\n#Fill the Nulls in the cabin column\ntrain_data['Cabin'] = train_data['Cabin'].fillna(train_data['Cabin'].mode()[0])\n#Fill the Nulls in the Embarked column\ntrain_data['Embarked'] = train_data['Embarked'].fillna(train_data['Embarked'].mode()[0])","e05ca860":"#Checking for any Null values\ntrain_data.isnull().sum().max()","7094c11c":"train_data.head()","b3e6e421":"train_data.drop(['Name'], axis=1, inplace=True)","59318d96":"train_data.columns","86b5da47":"#Separate the Categorical columns from the Numerical columns\ntrain_categorical_cols = train_data.select_dtypes('object')\ntrain_categorical_cols\n","a0801591":"# Unique values in the categorical columns\ntrain_data['Ticket'].unique()","38c0c04d":"train_data['Cabin'].unique()\n","960951b5":"train_data['Embarked'].value_counts()","7f558643":"train_data['Cabin'].value_counts()","0219431e":"train_categorical_cols.head()","eb07b525":"from sklearn.preprocessing import LabelEncoder\nenc = LabelEncoder()\nFeatures = train_categorical_cols['Sex']\nenc.fit(Features)\nFeatures = enc.transform(Features)\nprint(Features)\n","5d3df3d1":"from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder()\nohe.fit(Features.reshape(-1,1))\nFeatures = ohe.transform(Features.reshape(-1,1)).toarray()\nFeatures[:10,:]\n","a39a963b":"def encode_string(cat_feature):\n    \n    ## First encode the strings to numeric categories\n    enc = LabelEncoder()\n    enc.fit(cat_feature)\n    enc_cat_feature = enc.transform(cat_feature)\n    \n    ## Now, apply one hot encoding\n    ohe = OneHotEncoder()\n    encoded = ohe.fit(enc_cat_feature.reshape(-1,1))\n    return encoded.transform(enc_cat_feature.reshape(-1,1)).toarray()\n    \n\ncat_columns = ['Embarked']\nfor col in cat_columns:\n    temp = encode_string(train_data[col])\n    Features = np.concatenate([Features, temp], axis = 1)\nprint(Features.shape)\nFeatures[:2,:]","676473d4":"import seaborn as sns\nimport matplotlib.pyplot as plt","6a63722b":"#correlation matrix\ncorrmat = train_data.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","971b5de4":"#saleprice correlation matrix\nk = 6 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'Survived')['Survived'].index\ncm = np.corrcoef(train_data[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","3c4c4b83":"train_data.head()\n","fb232bbe":"train_numerical_cols = train_data.select_dtypes(exclude='object')\ntrain_numerical_cols","bf474877":"Features = np.concatenate([Features, np.array(train_data[[ 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']])], axis = 1)\nprint(Features[:2,:])\nprint(Features.shape)","7c906408":"train_y = np.array(train_data[['Survived']])\ntrain_y","08e2a6bc":"test_data.info()","94787cf4":"test_data.shape","96aeed2c":"test_data.size","68ccddde":"# Fill the Nulls with \ntest_data['Age'] = test_data['Age'].fillna(test_data['Age'].mean())\ntest_data['Fare'] = test_data['Fare'].fillna(test_data['Fare'].mean())\ntest_data['Cabin'] = test_data['Cabin'].fillna(test_data['Cabin'].mode()[0])","5befbe88":"test_data.isnull().sum().max()","a2d336f3":"test_data.drop(['Name',], axis=1, inplace=True)","fff211ad":"test_categorical_values = test_data.select_dtypes('object')\ntest_categorical_values","00fe5b2c":"test_numerical_data = test_data.select_dtypes(exclude=\"object\")\ntest_numerical_data","4658739a":"enc = LabelEncoder()\nTesting = test_categorical_values['Sex']\nenc.fit(Testing)\nTesting = enc.transform(Testing)\nprint(Testing)","64687ca7":"ohe = OneHotEncoder()\nohe.fit(Testing.reshape(-1,1))\nTesting = ohe.transform(Testing.reshape(-1,1)).toarray()\nTesting[:10,:]","10bc910d":"def encode_string(cat_feature):\n    \n    ## First encode the strings to numeric categories\n    enc = LabelEncoder()\n    enc.fit(cat_feature)\n    enc_cat_feature = enc.transform(cat_feature)\n    \n    ## Now, apply one hot encoding\n    ohe = OneHotEncoder()\n    encoded = ohe.fit(enc_cat_feature.reshape(-1,1))\n    return encoded.transform(enc_cat_feature.reshape(-1,1)).toarray()\n    \n\ncat_columns = ['Embarked']\nfor col in cat_columns:\n    temp = encode_string(test_data[col])\n    Testing = np.concatenate([Testing, temp], axis = 1)\nprint(Testing.shape)\nTesting[:2,:]","01508576":"Testing = np.concatenate([Testing, np.array(test_data[[ 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']])], axis = 1)\nprint(Testing[:2,:])\nprint(Testing.shape)","257d469f":"from sklearn.linear_model import LogisticRegression\n","9aa2a2b7":"#split the data\n# from sklearn.model_selection import train_test_split\n# x_train,x_test,y_train,y_test = train_test_split(X_train, Y_train, test_size=0.2, random_state = 42)","c6892d29":"log_model = LogisticRegression()\nlog_model.fit(Features, train_y)","11a92ba3":"import numpy as np\nprediction = log_model.predict(Testing)","df1194a1":"probabilities = log_model.predict_proba(Testing)\nprint(probabilities[:15,:])","456f57b3":"prediction","c7e2cb64":"prediction.size","2f068e8e":"prediction.dtype","5068844c":"pred=pd.DataFrame(prediction)\nsubdf = submission_data\ndatasets=pd.concat([subdf['PassengerId'],pred],axis=1)\ndatasets.columns=['PassengerId','Survived']\ndatasets.to_csv('sample_submission.csv',index=False)","80eff586":"foo = pd.read_csv('sample_submission.csv', index_col=0)\nfoo","18086910":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(random_state=42)\n","dbe61969":"classifier.fit(Features, train_y)\nprediction2 = classifier.predict(Testing)","8454bee7":"prediction2","a08c2918":"pred2=pd.DataFrame(prediction2)\nsubdf2 = submission_data\ndatasets=pd.concat([subdf['PassengerId'],pred2],axis=1)\ndatasets.columns=['PassengerId','Survived']\ndatasets.to_csv('sample_submission2.csv',index=False)","31a0bc8d":"foo2 = pd.read_csv('sample_submission2.csv', index_col=0)\nfoo2","c4dd1ebb":"from pprint import pprint\n# Look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\npprint(classifier.get_params())","aca7e372":"from sklearn.model_selection import RandomizedSearchCV","19ede4b0":"# Number of trees in the randomforest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum depth of a tree\nmax_depth = [int(x) for x in np.linspace (start = 10, stop=150, num = 15)]\nmax_depth.append(None)\n\n# Minimum number of samples to split a node\nmin_samples_split = [2, 4, 6, 8, 10]\n\n# Minimum number of samples at each leaf node\nmin_samples_leaf = [2, 4, 6, 8, 10]\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n\n# Create a random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\npprint(random_grid)","5e4fabfd":"classifier_random = RandomizedSearchCV(estimator=classifier, param_distributions=random_grid, \n                                      n_iter=100, cv=5, verbose=2, random_state=42, n_jobs=-1)\n\n# Fit the training datasets to the randomsearch model\nclassifier_random.fit(Features, train_y)","5745aa1e":"classifier_random.best_params_","73ee8100":"cf_best = RandomForestClassifier(max_depth = 70, n_estimators = 1400, random_state = 42, bootstrap=False,\n                                min_samples_split = 10, min_samples_leaf=4, max_features = 'auto')","05f06388":"cf_best.fit(Features, train_y)","7f929182":"predictions3 = cf_best.predict(Testing)","05ec80e0":"predictions3","833db6d3":"pred3=pd.DataFrame(predictions3)\nsubdf3 = submission_data\ndatasets=pd.concat([subdf3['PassengerId'],pred3],axis=1)\ndatasets.columns=['PassengerId','Survived']\ndatasets.to_csv('sample_submission3.csv',index=False)","88dcb375":"foo3 = pd.read_csv('sample_submission3.csv', index_col=0)\nfoo3","7d609638":"import xgboost as xgb\nxgb_classifier = xgb.XGBClassifier()","4e7ff065":"xgb_classifier.fit(Features, train_y)","bea94051":"predictions4 = xgb_classifier.predict(Testing)\npredictions4","4958d77e":"pred4=pd.DataFrame(predictions4)\nsubdf4 = submission_data\ndatasets=pd.concat([subdf4['PassengerId'],pred4],axis=1)\ndatasets.columns=['PassengerId','Survived']\ndatasets.to_csv('sample_submission4.csv',index=False)","9080d529":"foo4 = pd.read_csv('sample_submission4.csv', index_col=0)\nfoo4","cd212964":"xgb_classifier","c024ee7d":"estimator = xgb.XGBClassifier(\n    objective= 'binary:logistic',\n    nthread=4,\n    seed=42\n)","a767386f":"parameters = {\n    'max_depth': range (2, 10, 1),\n    'n_estimators': range(60, 220, 40),\n    'learning_rate': [0.1, 0.01, 0.05]\n}","c3c0cb23":"from sklearn.model_selection import GridSearchCV","9dd214a6":"grid_search = GridSearchCV(\n    estimator=estimator,\n    param_grid=parameters,\n    scoring = 'roc_auc',\n    n_jobs = 10,\n    cv = 10,\n    verbose=True\n)","6ae4de57":"grid_search.fit(Features, train_y)","4b7279ef":"grid_search.best_estimator_","c295bbbd":"cf_best2 = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.01, max_delta_step=0, max_depth=3,\n              min_child_weight=1, monotone_constraints='()',\n              n_estimators=180, n_jobs=4, nthread=4, num_parallel_tree=1,\n              random_state=42, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n              seed=42, subsample=1, tree_method='exact', validate_parameters=1,\n              verbosity=None)","f85013e3":"cf_best2.fit(Features, train_y)","c8af96e9":"predictions5 = cf_best2.predict(Testing)","60d672b3":"predictions5","686e98fd":"pred5=pd.DataFrame(predictions5)\nsubdf5 = submission_data\ndatasets=pd.concat([subdf5['PassengerId'],pred5],axis=1)\ndatasets.columns=['PassengerId','Survived']\ndatasets.to_csv('sample_submission5.csv',index=False)","ce5b3b35":"foo5 = pd.read_csv('sample_submission5.csv', index_col=0)\nfoo5","144023aa":"**Dealing with the testing dataset**","2c257f58":"**Dealing with the missing values in the train dataset**","a234538c":"**Coefficient Correlation**\n\nI will use the heatmap","0787e570":"**Label Encoding the Categorical Values**\n\n    Since numpy can not work with categorical columns we need to change its values to dummy variables by encoding them","f2931a29":"*Since ticket has a high cardinality we will drop the column*","e068774d":"**RandomSearch Training**","fbf11c7c":"**Hyperparameter optimization of xgboost**","cd92d2f2":"*I got a 0.77033 RMSE by using Logistic regression model*","4a25541a":"* Scale the Numeric values","0e53dfcb":"*Concatnate Categorical and Numeric Features*","2ba68cb6":"**Explanatory Data Analysis of the training data set**\n(EDA)","58351b35":"**Construct a random forest classifier model**","ba7575a3":"*Without hyperparameter optimization of the Random classifier parameter i scored a RMSE of 0.76555*","5c3d2c36":"I will drop the name column because it will not help in predicting\nif a passender may die or survive ","1d2e19b9":"**we have 3 columns with null values**\n* Age column with 177 nulls\n* Carbin column with 615 nulls\n* Embarked column with 2 nulls\n\n*The columns with object values which are categorical values,the nulls will be filled with mode\nwhile the columns with float values its null values will be filled with the mean value*","ab5797ce":"# Hyperparameter optimization of the RandomClassifier Model","49335da5":"0.76794 is the RMSE","ec5821f6":"the train dataset has 891 rows and 12 columns","7a38fbc5":"**CLASSIFICATION**\n\n* *Construct a logistic regression model*","80998a14":"> One Hot Encoding dummy variables\n","cc5c61ca":"*We can decide on dropping the cabin column which has more than 50% null values*\n\nBut as for now i have decided on working with the whole dataset so i will fill the missing values with mode","f0863b7c":"*Fare and parch have the highest correlation with survival *","24edd78f":"the training dataset has no null values","2ef6f702":"# **Dealing with Categorical Columns**","9b749df5":"> Age, Fare and Cabin have null values"}}