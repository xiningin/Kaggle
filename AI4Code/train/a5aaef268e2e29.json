{"cell_type":{"47a78b20":"code","1bda4a23":"code","2eceb3b9":"code","7c00242f":"code","0ba07570":"code","575cb719":"code","ed3a99a8":"code","b92340a1":"code","df8392e4":"code","6b3c486b":"code","4edc5b2c":"code","8f2832f7":"code","0051a7e7":"code","0f4e2824":"code","6876ff54":"code","60606f14":"code","042f23b5":"code","b570558c":"code","51282305":"code","cd35a227":"code","a6b96c3c":"code","5bb754a9":"code","83f79586":"code","52bb4c1d":"code","3547468e":"code","a2608867":"code","bcc5741d":"code","1d633a4d":"markdown","4c69e558":"markdown","f7fe0ef1":"markdown","641223d8":"markdown","f9ebc9da":"markdown","ebe884b0":"markdown","e1dfb6fe":"markdown","53ec9039":"markdown","b845a703":"markdown","44e5788d":"markdown","ddcbc894":"markdown","95408260":"markdown","bac9e7ba":"markdown","158bc1a1":"markdown","86401d51":"markdown","c1dadbd5":"markdown","9c827c6e":"markdown"},"source":{"47a78b20":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","1bda4a23":"#importing dependencies\nimport numpy as np  #python library for scientific computing\nimport pandas as pd #python library for data analysis and dataframes","2eceb3b9":"data = pd.read_csv('..\/input\/ex1data2.txt', header=None)\ndata.head()","7c00242f":"data.columns =(['Size','Bedroom','Price'])\ndata.head()","0ba07570":"data.drop('Bedroom', axis=1, inplace=True)\ndata.head()","575cb719":"#data = data.sample(frac=1)\n#data.head()","ed3a99a8":"# necessary dependencies for plotting\nimport matplotlib.pyplot as plt #python library for plot and graphs\n%matplotlib inline","b92340a1":"plt.plot(data.Size, data.Price, 'r.')\nplt.show()","df8392e4":"# another way to test the correlation\ndata.corr()","6b3c486b":"class LinearModel():\n    \n    def __init__(self, features, target):\n        self.X = features\n        self.y = target\n    \n    def GradDesc(self, parameters, learningRate, cost):\n        self.a = learningRate\n        self.c = cost\n        self.p = parameters\n        return self.a, self.Cost(self.c), self.p\n    \n    def Cost(self,c):\n        if c =='RMSE':\n            return self.y\n        elif c == 'MSE':\n            return self.X\n            \n            \nX=1\ny=0\na = LinearModel(5,4)\nprint(a.GradDesc(2,0.01,'MSE'))\nprint(a.Cost('RMSE'))","4edc5b2c":"# given a matrix A (3x2) and a matrix B (1x2)\nA = np.array([[1,2],\n              [1,3],\n              [1,4]])\nB = np.array([[2],[3]])\n\nprint('A =')\nprint(A,'\\nsize =',A.shape)\nprint('\\nB =')\nprint(B,'\\nsize =',B.shape)","8f2832f7":"# let's try it\nH = A.dot(B)\nprint(H)","0051a7e7":"X = np.array(data.drop('Price',axis=1))\ny = np.array(data.Price)\nm = len(data)\n\nprint(X.shape)\nprint(y.shape)\nprint(m)","0f4e2824":"y = y.reshape((m,1))\nprint(y.shape)","6876ff54":"def normscaler(Z, normal=False, scale='max'): \n    Zn = np.zeros(Z.shape)\n    for col in range(Zn.shape[1]):\n        std = Z[:,col].std()\n        clm = Z[:,col]\n        mn = Z[:,col].mean()\n        mx = Z[:,col].max()\n        nrm = 0\n        sclr = 1\n        if normal:\n            nrm = mn\n        if scale =='max':\n            sclr = mx\n        elif scale == 'std':\n            sclr = std\n        Zn[:,col] = (clm-nrm)\/sclr\n        \n    return Zn\n    \nXn = normscaler(X, normal=True, scale='std')\nyn = normscaler(y, normal=True, scale='std')","60606f14":"plt.plot(Xn, yn, 'r.')\nplt.show()","042f23b5":"# parameter initialization (choose any theta at initial)\ntheta = np.array([0.9,-1])","b570558c":"lineX = np.linspace(Xn.min(), Xn.max(), 100)\nliney = [theta[0] + theta[1]*xx for xx in lineX]\n\nplt.plot(Xn,yn,'r.', label='Training data')\nplt.plot(lineX,liney,'b--', label='Current hypothesis')\nplt.legend()\nplt.show()","51282305":"def cost_function(X, y, theta, deriv=False):\n    z = np.ones((len(X),1))\n    X = np.append(z, X, axis=1)\n    \n    if deriv:\n        loss     = X.dot(theta)-y\n        gradient = X.T.dot(loss)\/len(X)\n        return gradient, loss\n        \n    else:\n        h = X.dot(theta)\n        j = (h-y.flatten())\n        J = j.dot(j)\/2\/(len(X))\n        return J\n    \ncost_function(Xn, yn, theta)","cd35a227":"def GradDescent(features, target, param, learnRate=0.01, multiple=1, batch=len(X), log=False):\n\n    iterations = batch*len(features)\n    epochs     = iterations*multiple\n    y          = target.flatten()\n    t          = param\n    b          = batch\n    a          = learnRate\n    \n    theta_history  = np.zeros((param.shape[0],epochs)).T\n    cost_history   = [0]*epochs\n    \n    for ix in range(epochs):\n        \n        i    = epochs%len(X)\n        cost = cost_function(features[i:i+b], y[i:i+b], t)\n\n        cost_history[ix]   = cost\n        theta_history[ix]  = t\n\n        g, l = cost_function(features[i:i+b], y[i:i+b], t, deriv=True)\n        t    = t-a*g\n        \n        if log:\n            if ix%250==0:\n                print(\"iteration :\", ix+1)\n                #print(\"\\tloss     = \", l)\n                print(\"\\tgradient = \", g)\n                print(\"\\trate     = \", a*g)\n                print(\"\\ttheta    = \", t)\n                print(\"\\tcost     = \", cost)\n            \n    return cost_history, theta_history\n\nalpha = 0.01\nmul = 10\nbat = 8\nch, th = GradDescent(Xn,yn,theta,alpha,mul,bat,log=False)","a6b96c3c":"lineX = np.linspace(Xn.min(), Xn.max(), 100)\nliney = [th[-1,0] + th[-1,1]*xx for xx in lineX]\n\nplt.plot(Xn,yn,'r.', label='Training data')\nplt.plot(lineX,liney,'b--', label='Current hypothesis')\nplt.legend()\nplt.show()","5bb754a9":"plt.plot(ch,'g--')\nplt.show()","83f79586":"plt.plot(th[:,0],'r-.')\nplt.plot(th[:,1],'b-.')\nplt.show()","52bb4c1d":"#Grid over which we will calculate J\ntheta0_vals = np.linspace(-2, 2, 100)\ntheta1_vals = np.linspace(-2, 3, 100)\n\n#initialize J_vals to a matrix of 0's\nJ_vals = np.zeros((theta0_vals.size, theta1_vals.size))\n\n#Fill out J_vals\nfor t1, element in enumerate(theta0_vals):\n    for t2, element2 in enumerate(theta1_vals):\n        thetaT = np.zeros(shape=(2, 1))\n        thetaT[0][0] = element\n        thetaT[1][0] = element2\n        J_vals[t1, t2] = cost_function(Xn, yn, thetaT.flatten())\n\n#Contour plot\nJ_vals = J_vals.T","3547468e":"A, B = np.meshgrid(theta0_vals, theta1_vals)\nC = J_vals\n\ncp = plt.contourf(A, B, C)\nplt.colorbar(cp)\nplt.plot(th.T[0],th.T[1],'r--')\nplt.show()","a2608867":"#Animation\nimport matplotlib.animation as animation\n\n#Set the plot up,\nfig = plt.figure(figsize=(12,5))\n\nplt.subplot(121)\nplt.plot(Xn,yn,'ro', label='Training data')\nplt.title('Housing Price Prediction')\nplt.axis([Xn.min()-Xn.std(),Xn.max()+Xn.std(),yn.min()-yn.std(),yn.max()+yn.std()])\nplt.grid(axis='both')\nplt.xlabel(\"Size of house in ft^2 (X1) \")\nplt.ylabel(\"Price in $1000s (Y)\")\nplt.legend(loc='lower right')\n\nline, = plt.plot([], [], 'b-', label='Current Hypothesis')\nannotation = plt.text(-2, 3,'',fontsize=20,color='green')\nannotation.set_animated(True)\n\nplt.subplot(122)\ncp = plt.contourf(A, B, C)\nplt.colorbar(cp)\nplt.title('Filled Contours Plot')\nplt.xlabel('theta 0')\nplt.ylabel('theta 1')\ntrack, = plt.plot([], [], 'r-')\npoint, = plt.plot([], [], 'ro')\n\nplt.tight_layout()\nplt.close()\n\n#Generate the animation data,\ndef init():\n    line.set_data([], [])\n    track.set_data([], [])\n    point.set_data([], [])\n    annotation.set_text('')\n    return line, track, point, annotation\n\n# animation function.  This is called sequentially\ndef animate(i):\n    fit1_X = np.linspace(Xn.min()-Xn.std(), Xn.max()+Xn.std(), 1000)\n    fit1_y = th[i][0] + th[i][1]*fit1_X\n    \n    fit2_X = th.T[0][:i]\n    fit2_y = th.T[1][:i]\n    \n    track.set_data(fit2_X, fit2_y)\n    line.set_data(fit1_X, fit1_y)\n    point.set_data(th.T[0][i], th.T[1][i])\n    \n    annotation.set_text('Cost = %.4f' %(ch[i]))\n    return line, track, point, annotation\n\nanim = animation.FuncAnimation(fig, animate, init_func=init,\n                               frames=800, interval=0, blit=True)\n\nanim.save('animation.gif', writer='imagemagick', fps = 30)","bcc5741d":"#Display the animation...\nimport io\nimport base64\nfrom IPython.display import HTML\n\nfilename = 'animation.gif'\n\nvideo = io.open(filename, 'r+b').read()\nencoded = base64.b64encode(video)\nHTML(data='''<img src=\"data:image\/gif;base64,{0}\" type=\"gif\" \/>'''.format(encoded.decode('ascii')))","1d633a4d":"# Machine Learning #1: Linear Regression in the beginning\n\nThis tutorial series is for absolute beginners in machine learning algorithms, for those who want to review\/practice the fundamentals of machine learning and how to build them from scratch.","4c69e558":"## Supervised Learning\n\nIn supervised learning, the training data you feed to the algorithm includes the desired solutions,\ncalled <i>labels<\/i>\n\n* A typical supervised learning task is <b>classification<\/b>. The spam filter is a good example of this: \nit is trained with many examples emails along with their <i>class<\/i> (spam or ham), and it must learn how to\nclassify new emails.\n* Another typical task is to predict a <i>target<\/i> numeric value, such as the price of a car, given a set\nof <i>features<\/i> (mileage, age, brand, etc.) called <i>predictors<\/i>. This sort of task is called <b>regression<\/b>.\nTo train the system, you need to give it many examples of cars, including both their predictors and\ntheir labels (i.e., their prices)\n   \n<i>Let's try <b>Regression!<\/b><i>","f7fe0ef1":"## Linear Regression: Univariate\n\nLet's start with a very simple task of <i>linear regression<\/i> using a sample dataset called Portland \nHousing Prices, wherein we are given some <i>features<\/i> of a house (i.e. area, no. of rooms, etc) and\npredict the <i>target<\/i> price.\n\nTo make things much simpler. Let us use only one <i>feature<\/i> or in this case one variable, also known as \n<b>univariate linear regression<\/b>. That is we are only gonna use the 'Area' of a given house to train a\nlinear model\n\n<i>Let's <b>get<\/b> the data and <b>examine<\/b> it!<\/i>","641223d8":"## Type of Machine Learning Systems\n\n<br>reference 1: [Hands-On Machine Learning with Scikit-Learn and Tensorflow](http:\/\/shop.oreilly.com\/product\/0636920052289.do)\n\nreference 2: [Machine Learning, Stanford University by Andrew Ng](https:\/\/www.coursera.org\/learn\/machine-learning)\n\n<br>There are a lot of different types of Machine Learning Systems and usually it is best to classify them in broad categories based on:\n\n    * Whether or not they are trained with human supervision (supervised, unsupervised, \n        semisupervised, and Reinforcement Learning  \n    \n    * Whether or not they can learn incrementally on the fly (online versus batch learning)\n    \n    * Whether they work by simply comparing new data points to know data points or instead \n        detect patterns in the training data and build a predictive model, much like scientists\n        do (instance-based versus model-based learning)\n\n<i> Let's look at the very <b> first criteria <\/b> a bit more closely... <\/i>","f9ebc9da":"Wow! Worked like a charm ;)","ebe884b0":"Let us remove the 'Bedroom' feature since we are doing <b>univariate linear regression<\/b>","e1dfb6fe":"## What is Machine Learning?\n\n\"Machine Learning is the science (and art) of programming computers so they can <i>learn from data <\/i>\"\n- Aurelion Geron, 2017","53ec9039":"The data itself does not contain feature names or labels, let's set that up first.\nAccording to the source the first column is the <b>size<\/b> of the house in sq.ft. followed by \nthe no. of <b>bedrooms<\/b> and lastly the <b>price<\/b>.","b845a703":"## Featuring Scaling & Normalization\n\nLet's go back to our data and store it as <b>X<\/b> (features) and <b>y<\/b> (target) matrices first\nalso <b>m<\/b> (number of sample data also called 'training samples')","44e5788d":"Suppose A is our feature matrix <b><i>X<\/i><\/b> and B as our parameter matrix <b><i>theta<\/i><\/b>, that is,\n$$X = [\\ 1\\ 2\\ ] \\ \\ \\ \\theta = [\\ 2\\ 3\\ ]$$ \n$$[\\ 1\\ 3\\ ] \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $$\n$$[\\ 1\\ 4\\ ] \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $$\nRemember that we have our linear model\n$$h(x) = \\theta_0 x_0 + \\theta_1 x_1$$\nWe know that\n$$X_0 = [\\ 1\\ ] \\ \\ \\ X_1 = [\\ 2\\ ] \\ \\ \\ \\theta^T = [\\ 2\\ ] $$ \n$$\\ \\ \\ \\ \\ \\ \\ \\ \\ [\\ 1\\ ]\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ [\\ 3\\ ]\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ [\\ 3\\ ]$$\n$$[\\ 1\\ ] \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ [\\ 4\\ ]\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $$\nthen we can actually use matrix dot product to do the multiplication and addition at the same time\n(and faster)\n$$H=[\\ \\theta_0 X_0^0+\\theta_1 X_1^0\\ ]=[\\ \\theta_0+\\theta_1X_1^0\\ ]=[\\ 2+3(2)\\ ]=[\\ \\ 8\\ \\ ]\\ \\ \\ \\ \\ \\ \\ \\ $$\n$$[\\ \\theta_0 X_0^1+\\theta_1 X_1^1\\ ]\\ \\ \\ \\ \\ [\\ \\theta_0+\\theta_1 X_1^1\\ ]\\ \\ \\ \\ \\ [\\ 2+3(3)\\ ]\\ \\ \\ \\ \\ [\\ 11\\ ]$$\n$$[\\ \\theta_0 X_0^2+\\theta_1 X_1^2\\ ]\\ \\ \\ \\ \\ [\\ \\theta_0+\\theta_1 X_1^2\\ ]\\ \\ \\ \\ \\ [\\ 2+3(4)\\ ]\\ \\ \\ \\ \\ [\\ 14\\ ]$$\n\n$$ can\\ be\\ as\\ simple\\ as $$\n\n$$ H = X \\ dot \\ \\theta $$\n<i>Yes, that is the power of <b>Matrices<\/b>!<\/i>","ddcbc894":"Looking at the <b>y<\/b> variable, it is shaped as a flattened array <i>(47, )<\/i>\nLet's reshape it to a matrix of form <i>(47, 1)<\/i>","95408260":"From the plot results we could see that there is a <b>high correlation<\/b> between Housing <b>Area<\/b>\nand Housing <b>Price<\/b> (obviously) and therefore we could use a <b>line<\/b> (linear model) to fit this data.","bac9e7ba":"## Linear Model\n\nThe idea of linear regression is to fit a line to a set of points.\nSo let's use the line function given by:\n$$f(x) = y = mx + b$$\nwhere <b>m<\/b> is the slope and <b>b<\/b> is our <b>y<\/b> intercept, or for a more general form (multiple variables)\n$$h(x) = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n$$\nsuch that for a single variable where <b>n = 1<\/b>, \n$$h(x) = \\theta_0 + \\theta_1 x_1$$\n$$ when \\space x_0 = 1 $$\nwhere theta is our <b>parameters<\/b> (slope and intercept) and h(x) is our <b>hypothesis<\/b> or predicted value\n","158bc1a1":"Test the correlation of data with data.corr(). corr(X,Y) = cov(X,Y)\/pXpY = E[(X-uX)(Y-uY)]\/pXpY","86401d51":"## Matrix Math\n\nAs it turns, using Matrices and Vectors is actually very convenient in these type of problems\n(talking about the obvious)\nTo demonstrate that let's have an example:","c1dadbd5":"## Supervised\/Unsupervised Learning\n\nMachine Learning systems are usually classified according to the amount and type of supervision they\nget during training. There are four major categories: supervised learning, unsupervised learning,\nsemisupervised learning, and Reinforcement Learning.\n\n<i> Let us tackle <b> Supervised Learning <\/b> for now <i>\n\n","9c827c6e":"Now that looks much <b>simpler<\/b>!\nLet's <b>plot<\/b> our data and draw some <b>insights<\/b> of how a <b>linear model<\/b> could fit."}}