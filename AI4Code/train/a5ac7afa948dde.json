{"cell_type":{"338f4682":"code","f79bc1eb":"code","cee00851":"code","f2d88bf7":"code","c0f60762":"code","51fc48f7":"code","7d56aec8":"code","630d079f":"code","0f3c11ac":"code","81ee610f":"code","7f24b795":"code","fbe0f0b8":"code","1a40ef42":"code","2093616b":"code","f9ef7505":"code","0ccdeb65":"code","ed549df7":"code","07f6f41b":"code","c9e86939":"code","c7a1d413":"code","66a9a2ce":"code","d41d5939":"code","f269fadc":"code","8dae5a52":"code","cbbcd162":"code","e0b89849":"code","c7e2aafa":"code","5e47fc07":"code","c3b2be3d":"code","e9f73dbf":"code","2411eb5e":"code","ab5e8edf":"code","261a21f8":"code","ae208e0f":"code","c6d4d287":"code","30388848":"code","201dab22":"code","cc9e0f33":"code","a6e66ea3":"code","d09468d1":"code","763ddbe8":"code","b522fb85":"code","9c14ac8d":"code","c1fc8e19":"code","4bc831f0":"code","6e0f983f":"code","27b775d4":"code","915562f0":"code","9b412d0c":"code","592196e6":"code","62135d74":"code","f4bbd4d5":"code","623c9fd0":"code","10fa33ed":"code","4fd33162":"code","1796b9a5":"code","0268d007":"code","bcd1f977":"code","2c2c0ff4":"code","e622bae5":"code","d312d459":"code","3a7e0aec":"code","7f41b136":"code","0861f378":"code","667e725c":"code","6c1ddd44":"code","c233cf72":"code","ce0e3c9a":"code","bd685fdd":"code","623719d3":"code","f3f3af0a":"code","f1abea21":"code","7d5c23a7":"code","c19ee81c":"code","40c575d9":"code","e5f200c0":"code","fa3cac05":"code","e2b9c7d3":"code","930f7d67":"code","6095446f":"code","c9f29a57":"code","dfcfd4c5":"code","d7613b95":"code","e54425a1":"code","b6571e1d":"code","4388ced8":"code","0645beb8":"code","03005897":"code","6a23b3fc":"code","9bc7af36":"markdown","c76b0e09":"markdown","50591d9a":"markdown","ddb8761d":"markdown","9575c2fe":"markdown","2d7412b3":"markdown","219be3e2":"markdown","9cf9e6ab":"markdown","09704a58":"markdown","d9f8fb2b":"markdown","fce0e395":"markdown","569123f3":"markdown","12e8b777":"markdown","83f01839":"markdown","123a43c8":"markdown","f8aede53":"markdown","24a1649a":"markdown","0d4f43bb":"markdown","7708f2a2":"markdown","90dad913":"markdown","fc94ab5c":"markdown","0eb975dd":"markdown","09be3a30":"markdown","ace04a8c":"markdown","91f95098":"markdown","5ec1bebf":"markdown","25760875":"markdown","e4cec526":"markdown","463f833c":"markdown","3affdb17":"markdown","b58765ef":"markdown","803f8292":"markdown","7b280329":"markdown","d3efb17f":"markdown"},"source":{"338f4682":"# Use the official tokenization script created by the Google team\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","f79bc1eb":"import tensorflow as tf\nimport timeit\n\ndevice_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","cee00851":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\nimport tokenization\n\n# text processing libraries\nimport re\nimport string\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n\n# sklearn \nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.svm import SVC\n\n## scrapping\nfrom bs4 import BeautifulSoup\nimport requests\nimport os\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n \nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_rows', 5000)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 10000)","f2d88bf7":"!pip install tokenization","c0f60762":"#Training data\ntrain = pd.read_excel('..\/input\/fixed-bdc\/Data BDC - Satria Data 2020\/Data Latih\/Data Latih BDC.xlsx')\nprint('Training data shape: ', train.shape)\ntrain.head()","51fc48f7":"# Testing data \ntest = pd.read_excel('..\/input\/fixed-bdc\/Data BDC - Satria Data 2020\/Data Uji\/Data Uji BDC.xlsx')\nprint('Testing data shape: ', test.shape)\ntest.head()","7d56aec8":"train['text'] = train['judul'] + ' ' + train['narasi']\ntest['text'] = test['judul'] + ' ' + test['narasi']","630d079f":"train_ = train.drop(columns=['ID', 'tanggal', 'text', 'narasi', 'nama file gambar'])\ntest_ = test.drop(columns=['ID', 'tanggal', 'text', 'narasi', 'nama file gambar'])\n\nprint(train_.head())\nprint(test_.head())","0f3c11ac":"#Missing values in training set\ntrain_.isnull().sum()","81ee610f":"#Missing values in test set\ntest_.isnull().sum()","7f24b795":"train_['label'].value_counts()","fbe0f0b8":"# take copies of the data to leave the originals for BERT\ntrain1 = train_.copy()\ntest1 = test_.copy()","1a40ef42":"# url = ['https:\/\/turnbackhoax.id\/?s=%5Bbenar%5D',\n#        'https:\/\/turnbackhoax.id\/page\/2\/?s=%5Bbenar%5D',\n#        'https:\/\/turnbackhoax.id\/page\/3\/?s=%5Bbenar%5D',\n#        'https:\/\/turnbackhoax.id\/page\/4\/?s=%5Bbenar%5D',\n#        'https:\/\/turnbackhoax.id\/page\/5\/?s=%5Bbenar%5D',\n#        'https:\/\/turnbackhoax.id\/page\/6\/?s=%5Bbenar%5D',\n#        'https:\/\/turnbackhoax.id\/page\/7\/?s=%5Bbenar%5D',\n#        'https:\/\/turnbackhoax.id\/page\/8\/?s=%5Bbenar%5D',\n#        'https:\/\/turnbackhoax.id\/page\/9\/?s=%5Bbenar%5D',\n#        'https:\/\/turnbackhoax.id\/page\/10\/?s=%5Bbenar%5D',\n#        'https:\/\/turnbackhoax.id\/page\/11\/?s=%5Bbenar%5D',\n#        'https:\/\/turnbackhoax.id\/page\/12\/?s=%5Bbenar%5D',\n#        'https:\/\/turnbackhoax.id\/page\/13\/?s=%5Bbenar%5D',\n#        'https:\/\/turnbackhoax.id\/page\/14\/?s=%5Bbenar%5D',\n#        'https:\/\/turnbackhoax.id\/page\/15\/?s=%5Bbenar%5D',\n#        'https:\/\/turnbackhoax.id\/page\/16\/?s=%5Bbenar%5D',\n#        'https:\/\/turnbackhoax.id\/page\/17\/?s=%5Bbenar%5D',\n#        'https:\/\/turnbackhoax.id\/page\/18\/?s=%5Bbenar%5D',\n#        'https:\/\/turnbackhoax.id\/page\/19\/?s=%5Bbenar%5D',\n#        'https:\/\/turnbackhoax.id\/page\/20\/?s=%5Bbenar%5D',\n#       ]\n# session = requests.Session()\n# vectorizer = TfidfVectorizer()\n# corpus = []\n# term_doc_mtx = None\n# sub='read'\n# links_url = []\n# for u in url:\n#     res = session.get(u)\n#     soup = BeautifulSoup(res.content, 'html.parser')\n#     for a in soup.find_all('a', {\"rel\": \"bookmark\"}):\n#          links_url.append(a.get_text().strip())","2093616b":"# for x in links_url:\n#     x.replace('[BENAR] ','')\n# new_set = {x.replace('[BENAR] ','') for x in links_url}\n# new_set","f9ef7505":"# corpus_df = pd.DataFrame(new_set, columns=['judul'])\n# corpus_df['label'] = 0\n# corpus_df = corpus_df[[\"label\", \"judul\"]]\n# df","0ccdeb65":"tambah = pd.read_csv('..\/input\/judul-clean\/judul_clean.csv', sep=None)\ndisplay(tambah.head(10))\ntambah.shape","ed549df7":"train1.shape","07f6f41b":"train1 = train1.append(tambah)\ntrain1.shape","c9e86939":"train1['label'].value_counts()","c7a1d413":"# emoji removal\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n# Applying the de=emojifying function to both test and training datasets\nn = 'judul'\ntrain1[n] = train1[n].apply(lambda x: remove_emoji(x))\ntest1[n] = test1[n].apply(lambda x: remove_emoji(x))","66a9a2ce":"# Applying a first round of text cleaning techniques\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower() # make text lower case\n    text = re.sub('\\[.*?\\]', '', text) # remove text in square brackets\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text) # remove URLs\n    text = re.sub('<.*?>+', '', text) # remove html tags\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) # remove punctuation\n    text = re.sub('\\n', '', text) # remove words conatinaing numbers\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = re.sub('[\u2018\u2019\u201c\u201d\u2026]', '', text)\n    text = re.sub(r'(\\w)\\1{2,}', r'\\1', text) # remove repeated char\n\n\n    return text","d41d5939":"train1.head()","f269fadc":"# text preprocessing function\ndef text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer_reg = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    \n    nopunc = clean_text(text)\n    tokenized_text = tokenizer_reg.tokenize(nopunc)\n    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('indonesian')]\n    combined_text = ' '.join(remove_stopwords)\n    return combined_text\n\n# Applying the cleaning function to both test and training datasets\ntrain1[n] = train1[n].apply(lambda x: text_preprocessing(x))\ntest1[n] = test1[n].apply(lambda x: text_preprocessing(x))\n\n# Let's take a look at the updated text\ntrain1[n].head()","8dae5a52":"train1.head()","cbbcd162":"!pip install sastrawi\nfrom Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n\nfactory = StopWordRemoverFactory()\nstopword = factory.get_stop_words()\nstopword.append('yg')\nstopword.append('dengan')\nstopword.append('ia')\nstopword.append('bahwa')\nstopword.append('oleh')\nstopword.append('jadi')\nstopword.append('yth')\nstopword.append('nya')\nstopword.append('dengan')\nstopword.append('dgn')\nstopword.append('dlm')\n#stopword.append('indonesia')\n#stopword.append('orang')\n#stopword.append('tersebut')\n#stopword.append('semua')\nprint(stopword)","e0b89849":"def remove_stopwords(text, stem=False):\n    tokens = []\n    for token in text.split():\n        if token not in stopword:\n            tokens.append(token)\n    return \" \".join(tokens)\n\ntrain1['stop'] = train1[n].apply(lambda x: remove_stopwords(x))\ntest1['stop'] = test1[n].apply(lambda x: remove_stopwords(x))","c7e2aafa":"print('Original text:',train.narasi[1399])\nprint('After Stopwords Removal:',train1.stop[1399])","5e47fc07":"from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\nfactory = StemmerFactory()\nstemmer = factory.create_stemmer()\ntrain1['stop'] = train1['stop'].apply(lambda x: stemmer.stem(x))\ntest1['stop'] = test1['stop'].apply(lambda x: stemmer.stem(x))\n\n'''for index, row in train['translate'].iteritems():\n    stem = stemmer.stem(row) #detecting each row\n    train.loc[index, 'stem'] = stem'''","c3b2be3d":"train1.head()","e9f73dbf":"# gensim modules\nfrom gensim import utils\nfrom gensim.models.doc2vec import LabeledSentence\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom gensim.models import doc2vec\nfrom gensim.test.utils import common_texts\nfrom nltk.tokenize import word_tokenize\nfrom random import shuffle\n","2411eb5e":"%%time\n\n## tagged_data for train set\ntagged_data = [TaggedDocument(words=word_tokenize(_d), tags=[str(i)]) for i, _d in enumerate(train1['stop'])]\n## tagged_data for test set\ntagged_data_test = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(test1['stop'])]\n\n## Training Model (PV-DM)\nmax_epochs = 100\nvec_size = 20\nalpha = 0.025\n\n## MODEL FOR TRAIN SET\nmodel = Doc2Vec(size=vec_size,\n                alpha=alpha, \n                min_alpha=0.00025,\n                min_count=1,\n                dm =1)\n  \nmodel.build_vocab(tagged_data)\n\nfor epoch in range(max_epochs):\n    model.train(tagged_data,\n                total_examples=model.corpus_count,\n                epochs=model.iter)\n    # decrease the learning rate\n    model.alpha -= 0.0002\n    # fix the learning rate, no decay\n    model.min_alpha = model.alpha\n    \n\n## MODEL FOR TEST SET\nmodel_test = Doc2Vec(size=vec_size,\n                alpha=alpha, \n                min_alpha=0.00025,\n                min_count=1,\n                dm =1)\n  \nmodel_test.build_vocab(tagged_data_test)\n\nfor epoch in range(max_epochs):\n    model_test.train(tagged_data_test,\n                total_examples=model_test.corpus_count,\n                epochs=model_test.iter)\n    # decrease the learning rate\n    model_test.alpha -= 0.0002\n    # fix the learning rate, no decay\n    model_test.min_alpha = model_test.alpha\n\n# model.save(\"d2v.model\")\n# model_test.save(\"d2v.model\")\n# print(\"Model Saved test\")\n# model= Doc2Vec.load(\".\/d2v.model\")\n# model_test = Doc2Vec.load(\".\/d2v.model_test\")","ab5e8edf":"duplicat_index = list()\ndouble = list()\ncount = 0\nfor i in range(4231):\n    similar_doc = model.docvecs.most_similar(str(i))\n    if (similar_doc[0][1] > 0.98):\n#         print(tagged_data[i])\n#         print(tagged_data[int(similar_doc[0][0])])\n#         print(similar_doc[0][1])\n        if int(similar_doc[0][0]) not in double: \n            duplicat_index.append(int(similar_doc[0][0]))\n            double.append(int(tagged_data[i][1][0]))\n#         print()\n","261a21f8":"duplicat_index_test = list()\ndouble_test = list()\nfor i in range(len(test1['judul'])):\n    similar_doc = model_test.docvecs.most_similar(str(i))\n    if (similar_doc[0][1] > 0.98):\n#         print(tagged_data[i])\n#         print(tagged_data[int(similar_doc[0][0])])\n#         print(similar_doc[0][1])\n        if int(similar_doc[0][0]) not in double: \n            duplicat_index_test.append(int(similar_doc[0][0]))\n            double_test.append(int(tagged_data[i][1][0]))\n#         print()","ae208e0f":"train_ori = train1.copy()\ntrain1 = train1[~train1.index.isin(duplicat_index)]\n\n# test_ori = test1.copy()\n# test1 = test1[~test1.index.isin(duplicat_index_test)]","c6d4d287":"train1.shape, train_ori.shape","30388848":"dup_index = train_ori.shape[0] - train1.shape[0]\n# dup_index_test = test_ori.shape[0] - test1.shape[0]\n\nduplicat_percentage = round( dup_index \/ train_ori.shape[0] * 100, 2)\nprint(f'train data has {dup_index} rows that duplicates, that means {duplicat_percentage} percent of the total')\n\n# duplicat_percentage_test = round( dup_index_test \/ test_ori.shape[0] * 100, 2)\n# print(f'test data has {dup_index_test} rows that duplicates, that means {duplicat_percentage_test} percent of the total')","201dab22":"column = 'narasi'\n\nhave_emoji_train_idx = []\nhave_emoji_test_idx = []\n\nfor idx,text in enumerate(train[column]):\n    before = text \n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    after = emoji_pattern.sub(r'', text)\n    if before != after:\n        have_emoji_train_idx.append(idx)\n#         print(before)\n#         print(after)\n#         print(idx)\n#         print()\n\n","cc9e0f33":"for idx,text in enumerate(test[column]):\n    before = text \n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    after = emoji_pattern.sub(r'', text)\n    if before != after:\n        have_emoji_test_idx.append(idx)\n#         print(before)\n#         print(after)\n#         print(idx)\n#         print()\n","a6e66ea3":"train_emoji_percentage = round(len(have_emoji_train_idx) \/ train.shape[0] * 100, 2)\nprint(f'train data has {len(have_emoji_train_idx)} rows that used emoji, that means {train_emoji_percentage} percent of the total ({column})')\n\ntest_emoji_percentage = round(len(have_emoji_test_idx) \/ test.shape[0] * 100, 2)\nprint(f'test data has {len(have_emoji_test_idx)} rows that used emoji, that means {test_emoji_percentage} percent of the total ({column})')","d09468d1":"lower_train = list()\nlower_test  = list()\n\nfor idx,text in enumerate(train[column]):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    before = text\n    after = text.lower() \n\n    if before != after:\n        lower_train.append(idx)\n#         print(before)\n#         print(after)\n#         print(idx)\n#         print()","763ddbe8":"for idx,text in enumerate(test[column]):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    before = text\n    after = text.lower() \n\n    if before != after:\n        lower_test.append(idx)\n#         print(before)\n#         print(after)\n#         print(idx)\n#         print()","b522fb85":"train_lower_percentage = round(len(lower_train) \/ train.shape[0] * 100, 2)\nprint(f'train data has {len(lower_train)} rows that contains upper case, that means {train_lower_percentage} percent of the total ({column})')\n\ntest_lower_percentage = round(len(lower_test) \/ test.shape[0] * 100, 2)\nprint(f'test data has {len(lower_test)} rows that contains upper case, that means {test_lower_percentage} percent of the total ({column})')","9c14ac8d":"square_brackets_train = list()\nsquare_brackets_test  = list()\n\nfor idx,text in enumerate(train[column]):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    text = text.lower() \n    before = text\n    after = re.sub('\\[.*?\\]', '', text) # remove text in square brackets\n    if before != after:\n        square_brackets_train.append(idx)\n#         print(before)\n#         print(after)\n#         print(idx)\n#         print()\n\n","c1fc8e19":"for idx,text in enumerate(test[column]):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    text = text.lower() \n    before = text\n    after = re.sub('\\[.*?\\]', '', text) # remove text in square brackets\n    if before != after:\n        square_brackets_test.append(idx)\n#         print(before)\n#         print(after)\n#         print(idx)\n#         print()","4bc831f0":"square_brackets_percentage = round(len(square_brackets_train) \/ train.shape[0] * 100, 2)\nprint(f'train data has {len(square_brackets_train)} rows that used square_brackets, that means {square_brackets_percentage} percent of the total ({column})')\n\nsquare_brackets_percentage = round(len(square_brackets_test) \/ test.shape[0] * 100, 2)\nprint(f'test data has {len(square_brackets_test)} rows that used square_brackets, that means {square_brackets_percentage} percent of the total ({column})')","6e0f983f":"urls_train = list()\nurls_test  = list()\n\nfor idx,text in enumerate(train[column]):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    text = text.lower() \n    text = re.sub('\\[.*?\\]', '', text) # remove text in square brackets\n    before = text\n    after = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text) # remove URLs\n    if before != after:\n        urls_train.append(idx)\n#         print(before)\n#         print(after)\n#         print(idx)\n#         print()\n\n","27b775d4":"for idx,text in enumerate(test[column]):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    text = text.lower() \n    text = re.sub('\\[.*?\\]', '', text) # remove text in square brackets\n    before = text\n    after = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text) # remove URLs\n    if before != after:\n        urls_test.append(idx)\n#         print(before)\n#         print(after)\n#         print(idx)\n#         print()","915562f0":"urls_train_percentage = round(len(urls_train) \/ train.shape[0] * 100, 2)\nprint(f'train data has {len(urls_train)} rows that contains urls, that means {urls_train_percentage} percent of the total ({column})')\n\nurls_test_percentage = round(len(urls_test) \/ test.shape[0] * 100, 2)\nprint(f'test data has {len(urls_test)} rows that contains urls, that means {urls_test_percentage} percent of the total ({column})')","9b412d0c":"html_train = list()\nhtml_test = list()\n\nfor idx,text in enumerate(train[column]):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    text = text.lower() \n    text = re.sub('\\[.*?\\]', '', text) # remove text in square brackets\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text) # remove URLs\n    before = text\n    after = re.sub('<.*?>+', '', text) # remove html tags\n    if before != after:\n        html_train.append(idx)\n#         print(before)\n#         print(after)\n#         print(idx)\n#         print()","592196e6":"for idx,text in enumerate(test[column]):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    text = text.lower() \n    text = re.sub('\\[.*?\\]', '', text) # remove text in square brackets\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text) # remove URLs\n    before = text\n    after = re.sub('<.*?>+', '', text) # remove html tags\n    if before != after:\n        html_test.append(idx)\n#         print(before)\n#         print(after)\n#         print(idx)\n#         print()","62135d74":"html_train_percentage = round(len(html_train) \/ train.shape[0] * 100, 2)\nprint(f'train data has {len(html_train)} rows that contains html tags, that means {html_train_percentage} percent of the total ({column})')\n\nhtml_test_percentage = round(len(html_test) \/ test.shape[0] * 100, 2)\nprint(f'test data has {len(html_test)} rows that contains html tags, that means {html_test_percentage} percent of the total ({column})')","f4bbd4d5":"punc_train = list()\npunc_test = list()\n\nfor idx,text in enumerate(train[column]):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    text = text.lower() \n    text = re.sub('\\[.*?\\]', '', text) # remove text in square brackets\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text) # remove URLs\n    text = re.sub('<.*?>+', '', text) # remove html tags\n    before = text\n    after = re.sub('[%s]' % re.escape(string.punctuation), '', text) # remove punctuation\n    if before != after:\n        punc_train.append(idx)\n#         print(before)\n#         print(after)\n#         print(idx)\n#         print()","623c9fd0":"for idx,text in enumerate(test[column]):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    text = text.lower() \n    text = re.sub('\\[.*?\\]', '', text) # remove text in square brackets\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text) # remove URLs\n    text = re.sub('<.*?>+', '', text) # remove html tags\n    before = text\n    after = re.sub('[%s]' % re.escape(string.punctuation), '', text) # remove punctuation\n    if before != after:\n        punc_test.append(idx)\n#         print(before)\n#         print(after)\n#         print(idx)\n#         print()","10fa33ed":"punc_train_percentage = round(len(punc_train) \/ train.shape[0] * 100, 2)\nprint(f'train data has {len(punc_train)} rows that contains punctuation, that means {punc_train_percentage} percent of the total ({column})')\n\npunc_test_percentage = round(len(punc_test) \/ test.shape[0] * 100, 2)\nprint(f'test data has {len(punc_test)} rows that contains punctuation, that means {punc_test_percentage} percent of the total ({column})')","4fd33162":"num_train = list()\nnum_test = list()\nfor idx,text in enumerate(train[column]):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    text = text.lower() \n    text = re.sub('\\[.*?\\]', '', text) # remove text in square brackets\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text) # remove URLs\n    text = re.sub('<.*?>+', '', text) # remove html tags\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) # remove punctuation\n    before = text\n    text = re.sub('\\n', '', text) # remove words conatinaing numbers\n    text = re.sub('\\w*\\d\\w*', '', text)\n    after = re.sub('[\u2018\u2019\u201c\u201d\u2026]', '', text)\n    if before != after:\n        num_train.append(idx)\n#         print(before)\n#         print(after)\n#         print(idx)\n#         print()","1796b9a5":"for idx,text in enumerate(test[column]):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    text = text.lower() \n    text = re.sub('\\[.*?\\]', '', text) # remove text in square brackets\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text) # remove URLs\n    text = re.sub('<.*?>+', '', text) # remove html tags\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) # remove punctuation\n    before = text\n    text = re.sub('\\n', '', text) # remove words conatinaing numbers\n    text = re.sub('\\w*\\d\\w*', '', text)\n    after = re.sub('[\u2018\u2019\u201c\u201d\u2026]', '', text)\n    if before != after:\n        num_test.append(idx)\n#         print(before)\n#         print(after)\n#         print(idx)\n#         print()","0268d007":"num_train_percentage = round(len(num_train) \/ train.shape[0] * 100, 2)\nprint(f'train data has {len(num_train)} rows that contains numbers, that means {num_train_percentage} percent of the total ({column})')\n\nnum_test_percentage = round(len(num_test) \/ test.shape[0] * 100, 2)\nprint(f'test data has {len(num_test)} rows that contains numbers, that means {num_test_percentage} percent of the total ({column})')","bcd1f977":"repeat_train = list()\nrepeat_test = list()\n\nfor idx,text in enumerate(train[column]):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    text = text.lower() \n    text = re.sub('\\[.*?\\]', '', text) # remove text in square brackets\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text) # remove URLs\n    text = re.sub('<.*?>+', '', text) # remove html tags\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) # remove punctuation\n    text = re.sub('\\n', '', text) # remove words conatinaing numbers\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = re.sub('[\u2018\u2019\u201c\u201d\u2026]', '', text)\n    before = text\n    after = re.sub(r'(\\w)\\1{2,}', r'\\1', text)\n    if before != after:\n        repeat_train.append(idx)\n#         print(before)\n#         print(after)\n#         print(idx)\n#         print()","2c2c0ff4":"for idx,text in enumerate(test[column]):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    text = text.lower() \n    text = re.sub('\\[.*?\\]', '', text) # remove text in square brackets\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text) # remove URLs\n    text = re.sub('<.*?>+', '', text) # remove html tags\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) # remove punctuation\n    text = re.sub('\\n', '', text) # remove words conatinaing numbers\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = re.sub('[\u2018\u2019\u201c\u201d\u2026]', '', text)\n    before = text\n    after = re.sub(r'(\\w)\\1{2,}', r'\\1', text)\n    if before != after:\n        repeat_test.append(idx)\n#         print(before)\n#         print(after)\n#         print(idx)\n#         print()","e622bae5":"repeat_train_percentage = round(len(repeat_train) \/ train.shape[0] * 100, 2)\nprint(f'train data has {len(repeat_train)} rows that contains repeated char, that means {repeat_train_percentage} percent of the total ({column})')\n\nrepeat_test_percentage = round(len(repeat_test) \/ test.shape[0] * 100, 2)\nprint(f'test data has {len(repeat_test)} rows that contains repeated char, that means {repeat_test_percentage} percent of the total ({column})')","d312d459":"'''from wordcloud import WordCloud\nwc = WordCloud(\n    height=600,repeat=False,width=1400,max_words=1000,stopwords=stopword,\n    colormap='terrain',background_color='white',mode='RGBA'\n).generate(\n    ' '.join(train1['stop'].dropna().astype(str)))\nplt.figure(figsize = (16,16))\nplt.imshow(wc)\nplt.title('Judul Wordcloud')\nplt.axis('off')\nplt.show()'''","3a7e0aec":"count_vectorizer = CountVectorizer(ngram_range = (1,2), min_df = 1)\ntrain_vectors = count_vectorizer.fit_transform(train1['stop'])\ntest_vectors = count_vectorizer.transform(test1[\"stop\"])\n\n## Keeping only non-zero elements to preserve space \ntrain_vectors.shape","7f41b136":"tfidf = TfidfVectorizer(ngram_range=(1, 2), min_df = 1)\ntrain_tfidf = tfidf.fit_transform(train1['stop'])\ntest_tfidf = tfidf.transform(test1[\"stop\"])\n\ntrain_tfidf.shape","0861f378":"from sklearn.model_selection import StratifiedKFold\n\n# Create a StratifiedKFold object\nskf = StratifiedKFold(n_splits=5, shuffle=False, random_state=42)","667e725c":"%%time\nlogreg_bow = LogisticRegression()\n\nlogreg_bow.fit(train_vectors, train1[\"label\"])\nscores = model_selection.cross_val_score(logreg_bow, train_vectors, train1[\"label\"], cv=skf, scoring=\"f1\")\nscores.mean()","6c1ddd44":"%%time\nlogreg_tfidf = LogisticRegression()\nlogreg_tfidf.fit(train_tfidf, train1[\"label\"])\nscores = model_selection.cross_val_score(logreg_tfidf, train_tfidf, train1[\"label\"], cv=skf, scoring=\"f1\")\nscores.mean()","c233cf72":"'''from scipy.stats import randint\nforest_clf = RandomForestClassifier()\nparam_distribs = {\n        'n_estimators': randint(low=100, high=300),\n        'max_features': randint(low=8, high=17),\n    }\n\nrnd_forest_clf = RandomizedSearchCV(forest_clf, param_distributions=param_distribs,\n                                n_iter=10, cv=skfold, scoring='f1', random_state=42)\nrnd_forest_clf.fit(train_vectors, train1[\"label\"])\n\n{'max_features': 15, 'n_estimators': 288}''' #malah turun","ce0e3c9a":"%%time\n# Fitting a simple Random Forest on BoW\nRF_bow = RandomForestClassifier(n_estimators=288)\nRF_bow.fit(train_vectors, train1[\"label\"])\nscores = model_selection.cross_val_score(RF_bow, train_vectors, train1[\"label\"], cv=skf, scoring=\"f1\")\nscores.mean()","bd685fdd":"%%time\n# Fitting a simple Random Forest on TFIDF\nRF_tfidf = RandomForestClassifier(n_estimators=288)\nRF_tfidf.fit(train_tfidf, train1[\"label\"])\nscores = model_selection.cross_val_score(RF_tfidf, train_tfidf, train1[\"label\"], cv=skf, scoring=\"f1\")\nscores.mean()","623719d3":"%%time\nfrom sklearn.svm import SVC\n# Fitting a simple SVC on BoW\nSVC_bow = SVC(probability=True)\nSVC_bow.fit(train_vectors, train1[\"label\"])\nscores = model_selection.cross_val_score(SVC_bow, train_vectors, train1[\"label\"], cv=skf, scoring=\"f1\")\nscores.mean()","f3f3af0a":"%%time\n# Fitting a simple SVC on TFIDF\nSVC_tfidf = SVC(probability=True)\nSVC_tfidf.fit(train_tfidf, train1[\"label\"])\nscores = model_selection.cross_val_score(SVC_tfidf, train_tfidf, train1[\"label\"], cv=skf, scoring=\"f1\")\nscores.mean()","f1abea21":"%%time\nXGB_bow = XGBClassifier()\n\nXGB_bow.fit(train_vectors, train1[\"label\"])\nscores = model_selection.cross_val_score(XGB_bow, train_vectors, train1[\"label\"], cv=skf, scoring=\"f1\")\nscores.mean()","7d5c23a7":"%%time\n# Fitting a simple XGB on TFIDF\nXGB_tfidf = XGBClassifier()\nXGB_tfidf.fit(train_tfidf, train1[\"label\"])\nscores = model_selection.cross_val_score(XGB_tfidf, train_tfidf, train1[\"label\"], cv=skf, scoring=\"f1\")\nscores.mean()","c19ee81c":"%%time\nfrom sklearn.tree import DecisionTreeClassifier\n# Fitting a simple AdaBoost on BoW\nada_bow = AdaBoostClassifier(\n                        base_estimator=DecisionTreeClassifier(\n                                                             max_depth=7,\n                                                             min_samples_leaf=5,\n                                                             min_samples_split=10,\n                                                             ),\n                   learning_rate=0.3, n_estimators=2, random_state=42)\nada_bow.fit(train_vectors, train1[\"label\"])\nscores = model_selection.cross_val_score(ada_bow, train_vectors, train1[\"label\"], cv=skf, scoring=\"f1\")\nscores.mean()","40c575d9":"%%time\n# Fitting a simple AdaBoost on TFIDF\nada_tfidf = AdaBoostClassifier(\n                        base_estimator=DecisionTreeClassifier(\n                                                             max_depth=5,\n                                                             min_samples_leaf=2,\n                                                             min_samples_split=3,\n                                                             ),\n                            learning_rate=0.3, n_estimators=2, random_state=42)\nada_tfidf.fit(train_tfidf, train1[\"label\"])\nscores = model_selection.cross_val_score(ada_tfidf, train_tfidf, train1[\"label\"], cv=skf, scoring=\"f1\")\nscores.mean()","e5f200c0":"'''ext_clf = ExtraTreesClassifier()\nparam_grid = {\"max_depth\": [None],\n              \"max_features\": [10, 17],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False, True],\n              \"n_estimators\" :[50,100,200],\n              \"criterion\": [\"gini\"]}\n\n# Cross validate model with Kfold stratified cross val\nskfold = StratifiedKFold(n_splits=5)\ngrid_ext_clf = GridSearchCV(ext_clf,param_grid, cv=skfold, scoring=\"f1\", n_jobs= 4, verbose = 1)\ngrid_ext_clf.fit(train_vectors, train1[\"label\"])\n\ngrid_ext_clf.best_params_\n\n{'bootstrap': False,\n 'criterion': 'gini',\n 'max_depth': None,\n 'max_features': 17,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'n_estimators': 100}\n '''","fa3cac05":"%%time\n# Fitting a simple Extra on BoW\next_bow = ExtraTreesClassifier(criterion='gini',\n                               max_features=17,\n                               n_estimators=100,\n                              )\next_bow.fit(train_vectors, train1[\"label\"])\nscores = model_selection.cross_val_score(ext_bow, train_vectors, train1[\"label\"], cv=skf, scoring=\"f1\")\nscores.mean()","e2b9c7d3":"%%time\n# Fitting a simple Extra on TFIDF \next_tfidf = ExtraTreesClassifier()\next_tfidf.fit(train_tfidf, train1[\"label\"])\nscores = model_selection.cross_val_score(ext_tfidf, train_tfidf, train1[\"label\"], cv=skf, scoring=\"f1\")\nscores.mean()","930f7d67":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import StratifiedKFold","6095446f":"def cm_analysis():\n    # splitter\n    SEED = 2020\n    f1 = []\n    tra_fold_df = []\n    val_fold_df = []\n    cm = []\n    skf = StratifiedKFold(n_splits=5, shuffle=False, random_state=SEED)\n    # models\n    models = [LogisticRegression(), RandomForestClassifier(n_estimators=288), SVC(probability=True),\n            XGBClassifier(), \n\n            AdaBoostClassifier(\n                base_estimator=DecisionTreeClassifier(max_depth=7, min_samples_leaf=5,min_samples_split=10,),\n                learning_rate=0.3, n_estimators=2, random_state=42),\n\n            ExtraTreesClassifier(criterion='gini',max_features=17,n_estimators=100,)]\n\n\n    for model in models:\n        label = {'mislabeled_1':0, 'true_1':0,'mislabeled_0':0,'true_0':0}\n        score = []\n        for tra_idx, val_idx in skf.split(train1, train1[['label']]):\n\n            X_tra = train1.iloc[tra_idx]\n            X_val = train1.iloc[val_idx]\n            tra_fold_df.append(X_tra)\n            val_fold_df.append(X_val)\n            y_tra = train1.iloc[tra_idx]['label']\n            y_val = train1.iloc[val_idx]['label']\n\n            #### IF YOU WANT YOU TO USE BOW UNCOMMENT THE CODE BELOW, AND THEN COMMENT THE TF-IDF CODE SECTION\n\n            ## BOW ##\n    #         count_vectorizer = CountVectorizer(ngram_range = (1,2), min_df = 1)\n    #         train_vectors = count_vectorizer.fit_transform(X_tra['stop'])\n    #         validasi_vectors = count_vectorizer.transform(X_val['stop']) \n\n            ## TF-IDF ##\n            tfidf = TfidfVectorizer(ngram_range=(1, 2), min_df = 1)\n            train_vectors = tfidf.fit_transform(X_tra['stop'])\n            validasi_vectors = tfidf.transform(X_val[\"stop\"])\n\n            ## TRAINING AND PREDICT\n            model.fit(train_vectors,y_tra)\n            y_pred = model.predict(validasi_vectors)\n    #         print('For zero :',confusion_matrix(y_val, y_pred)[0])\n    #         print('For one :',confusion_matrix(y_val,y_pred)[1])\n\n\n            ## SAVE CM RESULT TO A DICT\n            label['true_0'] += confusion_matrix(y_val,y_pred)[0][0]\n            label['mislabeled_0'] += confusion_matrix(y_val,y_pred)[0][1]\n            label['true_1'] += confusion_matrix(y_val,y_pred)[1][1]\n            label['mislabeled_1'] += confusion_matrix(y_val,y_pred)[1][0]\n\n            ## SAVE THE F1_SCORE FOR EACH FOLD\n            score.append(f1_score(y_val,y_pred,average=None)[1])\n        cm_each = list()\n    #     print(f'f1_score_avg_none:',sum(score)\/5)\n        f1.append(sum(score)\/5)\n    #     print(label)\n        for key,value in label.items():\n    #         print(model,key,round(value\/5))\n            cm_each.append(round(value\/5))\n        cm.append(cm_each)\n\n    cm = pd.DataFrame(cm, columns = ['Mislabeled_1','True_1','Mislabeled_0','True_0'])\n    cm['Model'] = ['logreg','rf','svc','xgb','ada','ext']\n    cm.set_index('Model',inplace=True)\n    cm = cm.astype('int')\n    cm['f1_score'] = f1\n    display(cm.style.background_gradient(cmap='Blues'))\n    return\n\ncm_analysis()","c9f29a57":"ext_test_pred = ext_tfidf.predict_proba(test_vectors)[:,1]\nforest_test_pred = RF_tfidf.predict_proba(test_vectors)[:,1]\nxgb_test_pred = XGB_tfidf.predict_proba(test_vectors)[:,1]\n#dt_test_pred = dt_tfidf.predict_proba(test_vectors)[:,1]\nada_test_pred = ada_tfidf.predict_proba(test_vectors)[:,1]\nsvc_test_pred = SVC_tfidf.predict_proba(test_vectors)[:,1]\n#nb_test_pred = NB_tfidf.predict_proba(test_vectors)[:,1]\nlg_test_pred = logreg_tfidf.predict_proba(test_vectors)[:,1]\n\next_train_pred = ext_tfidf.predict_proba(train_vectors)[:,1]\nforest_train_pred = RF_tfidf.predict_proba(train_vectors)[:,1]\nxgb_train_pred = XGB_tfidf.predict_proba(train_vectors)[:,1]\n#dt_train_pred = dt_tfidf.predict_proba(train_vectors)[:,1]\nada_train_pred = ada_tfidf.predict_proba(train_vectors)[:,1]\nsvc_train_pred = SVC_tfidf.predict_proba(train_vectors)[:,1]\n#nb_train_pred = NB_tfidf.predict_proba(train_vectors)[:,1]\nlg_train_pred = logreg_tfidf.predict_proba(train_vectors)[:,1]","dfcfd4c5":"ext_test_pred = ext_bow.predict_proba(test_vectors)[:,1]\nforest_test_pred = RF_bow.predict_proba(test_vectors)[:,1]\nxgb_test_pred = XGB_bow.predict_proba(test_vectors)[:,1]\n#dt_test_pred = dt_bow.predict_proba(test_vectors)[:,1]\nada_test_pred = ada_bow.predict_proba(test_vectors)[:,1]\nsvc_test_pred = SVC_bow.predict_proba(test_vectors)[:,1]\n#nb_test_pred = NB_bow.predict_proba(test_vectors)[:,1]\nlg_test_pred = logreg_bow.predict_proba(test_vectors)[:,1]\n\next_train_pred = ext_bow.predict_proba(train_vectors)[:,1]\nforest_train_pred = RF_bow.predict_proba(train_vectors)[:,1]\nxgb_train_pred = XGB_bow.predict_proba(train_vectors)[:,1]\n#dt_train_pred = dt_bow.predict_proba(train_vectors)[:,1]\nada_train_pred = ada_bow.predict_proba(train_vectors)[:,1]\nsvc_train_pred = SVC_bow.predict_proba(train_vectors)[:,1]\n#nb_train_pred = NB_bow.predict_proba(train_vectors)[:,1]\nlg_train_pred = logreg_bow.predict_proba(train_vectors)[:,1]","d7613b95":"import pickle\n# save the model to disk\nrf_bow_model = 'rf_model.sav'\next_bow_model = 'ext_model.sav'\nXGB_bow_model = 'xgb_model.sav'\nada_bow_model = 'ada_model.sav'\nsvc_bow_model = 'svc_model.sav'\nlg_bow_model = 'lg_model.sav'\n\npickle.dump(RF_bow, open(rf_bow_model, 'wb'))\npickle.dump(ext_bow, open(ext_bow_model, 'wb'))\npickle.dump(XGB_bow, open(XGB_bow_model, 'wb'))\npickle.dump(ada_bow, open(ada_bow_model, 'wb'))\npickle.dump(SVC_bow, open(svc_bow_model, 'wb'))\npickle.dump(logreg_bow, open(lg_bow_model, 'wb'))","e54425a1":"base_pred = pd.DataFrame({\n    'ext':ext_train_pred.ravel(),\n    'forest':forest_train_pred.ravel(), \n    'xgb':xgb_train_pred.ravel(), \n    'svc':svc_train_pred.ravel(),\n    'ada':ada_train_pred.ravel(),\n    #'lg':lg_train_pred.ravel(),\n    #'nb':nb_train_pred.ravel(),\n    #'dt':dt_train_pred.ravel()\n    \n})\n\ntest_pred = pd.DataFrame({\n    'ext':ext_test_pred.ravel(),\n    'forest':forest_test_pred.ravel(), \n    'xgb':xgb_test_pred.ravel(), \n    'svc':svc_test_pred.ravel(),\n    'ada':ada_test_pred.ravel(),\n    #'lg':lg_test_pred.ravel(),\n    #'nb':nb_test_pred.ravel(),\n    #'dt':dt_test_pred.ravel()\n})","b6571e1d":"# Display numerical correlations between features on heatmap\nsns.set(font_scale=2.5)\ncorrelation_train = base_pred.corr()\nmask = np.triu(correlation_train.corr())\nplt.figure(figsize=(10, 10))\nsns_plot = sns.heatmap(correlation_train,\n            annot=True,\n            fmt='.1f',\n            cmap='coolwarm',\n            square=True,\n            mask=mask,\n            linewidths=1, alpha=1)\n\nplt.show()\nsns_plot.get_figure().savefig('tfidf.png',transparent=True)","4388ced8":"#grid={\"C\":np.logspace(-4,4,20), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\nfinal_model = LogisticRegression(C=0.012742749857031334, penalty='l2')\n#final_model=GridSearchCV(final_model,grid,cv=10)\nfinal_model.fit(base_pred, train1[\"label\"])\n#print(final_model.best_params_)\n\nscores = model_selection.cross_val_score(final_model, base_pred, train1[\"label\"], cv=5, scoring=\"f1\")\nprint('Cross Validation :', scores.mean())\n\n# make prediction using our test data and model\ny_pred = final_model.predict(base_pred)\nprint('')\nprint('###### Logreg Classifier ######')\n\n# evaluating the model\nprint(\"Testing Accuracy :\", accuracy_score(train1[\"label\"], y_pred))\n#print(\"Best Score :\", final_model.best_score_)\nprint('F1-Score :', metrics.f1_score(train1[\"label\"], y_pred))\n#print('Best Params :', final_model.best_params_)\n#print('Best Estimator', final_model.best_estimator_)","0645beb8":"test_read = pd.read_excel('..\/input\/fixed-bdc\/Data BDC - Satria Data 2020\/Data Uji\/Data Uji BDC.xlsx')\ntemplate = pd.read_csv('..\/input\/submission-bdc\/template jawaban BDC.csv')\nfinal_pred = final_model.predict(test_pred)\nsubmission = pd.DataFrame()\nsubmission[\"ID\"] = test_read[\"ID\"]\nsubmission[\"prediksi\"] = final_pred\nsubmission = pd.merge(template['ID'], submission)","03005897":"submission.to_csv(\"submission.csv\", index=False)\nsub = pd.read_csv('.\/submission.csv')\nsub.prediksi.value_counts()","6a23b3fc":"rf_pred = RF_bow.predict(test_vectors)\nrf_submit = pd.DataFrame()\nrf_submit[\"ID\"] = test_read[\"ID\"]\nrf_submit[\"prediksi\"] = rf_pred\nrf_submit = pd.merge(template['ID'], submission)\nrf_submit.to_csv(\"rf_submit.csv\", index=False)\nrf_sub = pd.read_csv('.\/rf_submit.csv')\nrf_sub.prediksi.value_counts()","9bc7af36":"**Note : We use train and test set which have not been preprocessed to calculate the proportions**","c76b0e09":"## Square Brackets","50591d9a":"# EDA","ddb8761d":"## Prediction","9575c2fe":"# Libraries","2d7412b3":"# Data Preprocessing","219be3e2":"### Logreg","9cf9e6ab":"## Remove Duplicates Using Doc2Vec","09704a58":"## Ensemble: Stacking","d9f8fb2b":"## Cek Emoji","fce0e395":"## Numbers","569123f3":"### RF","12e8b777":"## URLs","83f01839":"### Confusion Matrix","123a43c8":"### Checking Duplicates using Doc2Vec Similarity for train set","f8aede53":"# Models\n\nWe try two different approach : bow and tf-idf\n\nModels that we used are :\n1. Logistic Regression\n2. Random Forest\n3. Support Vector\n4. Xgboost\n5. Adaboost\n6. Extratree","24a1649a":"### Extra","0d4f43bb":"# Counting\n\n\nWe are going to see the diversity of data. How much emoji did train and test set contain? How many symbols & punctuation did train and test set contains? if they have the same proportion that means the train and test set has the same diversity. Also, we use wordcloud to check if the most frequent words are similar between train and test set.","7708f2a2":"### XGB","90dad913":"### Scrapping's Result","fc94ab5c":"## Html tags","0eb975dd":"### Checking Duplicates using Doc2Vec Similarity for test set","09be3a30":"### Adaboost","ace04a8c":"## Cross Validations Analysis","91f95098":"## Lower Case","5ec1bebf":"Size for each model :\n- ext_model = 36.666 kb\n- xgb_model = 503 kb\n- lg_model = 220 kb\n- ada_model = 11 kb\n- rf_model = 37.345 kb","25760875":"## Sparse Matrix","e4cec526":"### SVC","463f833c":"## SAVING EACH MODEL","3affdb17":"# Load and Prepare Data","b58765ef":"## Repeated Char","803f8292":"## Scraping","7b280329":"## Punctuation","d3efb17f":"**Data cleaning:** In summary, we want to tokenize our text then send it through a round of cleaning where we turn all characters to lower case, remove brackets, URLs, html tags, punctuation, numbers, etc. We'll also remove emojis from the text and remove common stopwords. This is a vital step in the Bag-of-words + linear model"}}