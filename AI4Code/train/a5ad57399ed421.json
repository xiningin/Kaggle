{"cell_type":{"5e47141b":"code","95d8327e":"code","0f88cd55":"code","79007db2":"code","bc3f63fe":"code","1009e4d6":"code","390e5797":"code","042ebc48":"code","ce239d7e":"code","b04cfac4":"code","5e20496f":"code","e9a454b6":"code","97a2e434":"code","ed34e5e5":"code","4dbc197a":"code","c35755bc":"code","40944340":"code","f1387d9d":"code","8e1076c2":"code","282be684":"code","cfa780f9":"code","88d6b512":"code","21a83d8b":"code","c5ed9ed0":"code","5b2d14d7":"code","52005ee3":"code","649cc6b7":"code","3b9ce5a2":"code","cca684cd":"markdown","6651a6dc":"markdown","7841e95a":"markdown","aa857f91":"markdown","d3d3a5c5":"markdown","b2c295ea":"markdown","1a922b24":"markdown","ad093c62":"markdown","b6fa0d06":"markdown","2bb34dc3":"markdown","ecb03b34":"markdown","424df7c1":"markdown","c1cfcf25":"markdown","0f24b37d":"markdown","04ae2276":"markdown","3d907572":"markdown","cc4dae73":"markdown","7b7a8fda":"markdown","829637eb":"markdown","762df02a":"markdown","c7b33913":"markdown"},"source":{"5e47141b":"import numpy as np\nimport matplotlib.pyplot as plt","95d8327e":"# Generate input values (gaussian distribtion, 100x1 vector)\nX = 2 * np.random.rand(100,1)\n\n# Generate output values and add some random noise.\n# Output y = 3x + 4 and noise generated from gaussian distribution with mean 0 and standard deviation 0.5\ny = 4 + 3*X + np.random.randn(100,1)*0.5","0f88cd55":"# Plot input and output values using scatter function from pyplot.\n# Here X and y are axis of the scatter plot and will be passed to the function\nplt.scatter(X,y);\n# Add naming to each axis\nplt.xlabel(\"$X$\", fontsize=18);\nplt.ylabel(\"$y$\", rotation=0, fontsize=18);","79007db2":"X_b = np.c_[np.ones((100,1)),X]\n# theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\ntheta_best = y.T.dot(X_b).dot(np.linalg.inv(X_b.T.dot(X_b)))\n\n# Print calculated [b w]\nprint(theta_best)","bc3f63fe":"X_new = np.array([[0],[2]])\nX_new_b = np.c_[np.ones((2,1)),X_new]\ny_predict = theta_best.dot(X_new_b.T).T\nprint(y_predict)","1009e4d6":"# plot red linear regression line ('r-')\nplt.plot(X_new,y_predict,'r-', label='regression line');\n# plot data points as blue dots ('b.')\nplt.plot(X,y,'b.', label='data points');\n# set name for each axis\nplt.xlabel(\"$x_1$\", fontsize=18);\nplt.ylabel(\"$y$\", rotation=0, fontsize=18);\n# create legend on top left\nplt.legend()\n# show plot\nplt.show()","390e5797":"def  cal_cost(theta,X,y):\n    '''\n    \n    Calculates the cost for given X and Y. The following shows and example of a single dimensional X\n    theta = Vector of thetas \n    X     = Row of X's np.zeros((2,j))\n    y     = Actual y's np.zeros((2,1))\n    \n    where:\n        j is the no of features\n    '''\n    \n    # m as number of samples\n    m = len(y)\n    \n    # calculate predictions\n    predictions = X.dot(theta)\n    \n    # calculate cost using the equation above\n    cost = (1\/(2*m)) * np.sum(np.square(predictions-y))\n    \n    return cost","042ebc48":"def gradient_descent(X,y,theta,learning_rate=0.01,iterations=100):\n    '''\n    X    = Matrix of X with added bias units\n    y    = Vector of Y\n    theta=Vector of thetas np.random.randn(j,1)\n    learning_rate \n    iterations = no of iterations\n    \n    Returns the final theta vector and array of cost history over no of iterations\n    '''\n    # m as number of samples\n    m = len(y)\n    \n    # create placeholder variables for historical data (useful for plotting)\n    cost_history = np.zeros(iterations)\n    theta_history = np.zeros((iterations,2))\n    \n    # iterate through each step\n    for it in range(iterations): \n        # calculate current output of each step\n        prediction = np.dot(X,theta)\n        # update theta using the equations above\n        theta = theta -(1\/m)*learning_rate*( X.T.dot((prediction - y)))\n        # update theta and cost history\n        theta_history[it,:] =theta.T\n        cost_history[it]  = cal_cost(theta,X,y)\n        \n    return theta, cost_history, theta_history","ce239d7e":"# learning rate\nlr = 0.01\n# number of iterations\nn_iter = 2000\n\n# initialize random thetas\ntheta = np.random.randn(2,1)\n\n# create input matrix\nX_b = np.c_[np.ones((len(X),1)),X]\n\n# use gradient descent function above to optimize\ntheta,cost_history,theta_history = gradient_descent(X_b,y,theta,lr,n_iter)\n\n\nprint('Theta0:          {:0.3f},\\nTheta1:          {:0.3f}'.format(theta[0][0],theta[1][0]))\nprint('Final cost\/MSE:  {:0.3f}'.format(cost_history[-1]))","b04cfac4":"# plot cost values from history as blue dots ('b.')\nplt.plot(range(n_iter),cost_history,'b.')\n# set name for each axis\nplt.ylabel('Cost')\nplt.xlabel('Iterations')\n# show plot\nplt.show()","5e20496f":"# plot first 150 cost values (range(150)) from history as blue dots ('b.')\nplt.plot(range(150),cost_history[:150],'b.')\n# set name for each axis\nplt.ylabel('Cost')\nplt.xlabel('Iterations')\n# show plot\nplt.show()","e9a454b6":"def plot_GD(n_iter,lr,ax,ax1=None):\n    \"\"\"\n    n_iter = no of iterations\n    lr = Learning Rate\n    ax = Axis to plot the Gradient Descent\n    ax1 = Axis to plot cost_history vs Iterations plot\n\n    \"\"\" \n    # plot sample points as blue dots\n    _ = ax.plot(X,y,'b.')\n    \n    # initialize random thetas\n    theta = np.random.randn(2,1)\n    \n    # set transparency for each step, start with 0.1\n    tr =0.1\n    # place holder for cost history\n    cost_history = np.zeros(n_iter)\n    # iterate through each step\n    for i in range(n_iter):\n        # calculate prediction before optimize step\n        pred_prev = X_b.dot(theta)\n        # optimize with GD\n        theta,h,_ = gradient_descent(X_b,y,theta,lr,1)\n        # new prediction\n        pred = X_b.dot(theta)\n        \n        # update cost history for plotting\n        cost_history[i] = h[0]\n        \n        # plot once every 25 steps\n        if ((i % 25 == 0) ):\n            # plot red linear regression line\n            _ = ax.plot(X,pred,'r-')\n            \n            # increase transparency (for illustration)\n            if tr < 0.8:\n                tr = tr+0.2\n    # plot cost history\n    if not ax1== None:\n        _ = ax1.plot(range(n_iter),cost_history,'b.')  ","97a2e434":"# create big figure size 30x25, resolution 200 dpi\nfig = plt.figure(figsize=(30,25),dpi=200)\n# adjust spacing for each subplot\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\n# create n_iter and lr pairs to try\nit_lr =[(2000,0.001),(500,0.01),(200,0.1),(100,1)]\n\n# count to subplot placement\ncount = 0\n\n# iterate through pairs\nfor n_iter, lr in it_lr:\n    count += 1\n    \n    ax = fig.add_subplot(4, 2, count)\n    count += 1\n   \n    ax1 = fig.add_subplot(4,2,count)\n    \n    ax.set_title(\"lr:{}\".format(lr))\n    ax1.set_title(\"Iterations:{}\".format(n_iter))\n    plot_GD(n_iter,lr,ax,ax1)","ed34e5e5":"_,ax = plt.subplots(figsize=(14,10))\nplot_GD(100,0.1,ax)","4dbc197a":"def stocashtic_gradient_descent(X,y,theta,learning_rate=0.01,iterations=10):\n    '''\n    X    = Matrix of X with added bias units\n    y    = Vector of Y\n    theta=Vector of thetas np.random.randn(j,1)\n    learning_rate \n    iterations = no of iterations\n    \n    Returns the final theta vector and array of cost history over no of iterations\n    '''\n    m = len(y)\n    cost_history = np.zeros(iterations)\n    \n    \n    for it in range(iterations):\n        cost =0.0\n        for i in range(m):\n            rand_ind = np.random.randint(0,m)\n            X_i = X[rand_ind,:].reshape(1,X.shape[1])\n            y_i = y[rand_ind].reshape(1,1)\n            prediction = np.dot(X_i,theta)\n\n            theta = theta -(1\/m)*learning_rate*( X_i.T.dot((prediction - y_i)))\n            cost += cal_cost(theta,X_i,y_i)\n        cost_history[it]  = cost\n        \n    return theta, cost_history","c35755bc":"lr =0.5\nn_iter = 50\n\ntheta = np.random.randn(2,1)\n\nX_b = np.c_[np.ones((len(X),1)),X]\ntheta,cost_history = stocashtic_gradient_descent(X_b,y,theta,lr,n_iter)\n\n\nprint('Theta0:          {:0.3f},\\nTheta1:          {:0.3f}'.format(theta[0][0],theta[1][0]))\nprint('Final cost\/MSE:  {:0.3f}'.format(cost_history[-1]))","40944340":"fig,ax = plt.subplots(figsize=(10,8))\n\nax.set_ylabel('Cost',rotation=0)\nax.set_xlabel('Iterations')\ntheta = np.random.randn(2,1)\n\n_=ax.plot(range(n_iter),cost_history,'b.')","f1387d9d":"def minibatch_gradient_descent(X,y,theta,learning_rate=0.01,iterations=10,batch_size =20):\n    '''\n    X    = Matrix of X without added bias units\n    y    = Vector of Y\n    theta=Vector of thetas np.random.randn(j,1)\n    learning_rate \n    iterations = no of iterations\n    \n    Returns the final theta vector and array of cost history over no of iterations\n    '''\n    m = len(y)\n    cost_history = np.zeros(iterations)\n    n_batches = int(m\/batch_size)\n    \n    for it in range(iterations):\n        cost =0.0\n        indices = np.random.permutation(m)\n        X = X[indices]\n        y = y[indices]\n        for i in range(0,m,batch_size):\n            X_i = X[i:i+batch_size]\n            y_i = y[i:i+batch_size]\n            \n            X_i = np.c_[np.ones(len(X_i)),X_i]\n           \n            prediction = np.dot(X_i,theta)\n\n            theta = theta -(1\/m)*learning_rate*( X_i.T.dot((prediction - y_i)))\n            cost += cal_cost(theta,X_i,y_i)\n        cost_history[it]  = cost\n        \n    return theta, cost_history","8e1076c2":"lr =0.1\nn_iter = 200\n\ntheta = np.random.randn(2,1)\n\n\ntheta,cost_history = minibatch_gradient_descent(X,y,theta,lr,n_iter)\n\n\nprint('Theta0:          {:0.3f},\\nTheta1:          {:0.3f}'.format(theta[0][0],theta[1][0]))\nprint('Final cost\/MSE:  {:0.3f}'.format(cost_history[-1]))","282be684":"fig,ax = plt.subplots(figsize=(10,8))\n\nax.set_ylabel('Cost',rotation=0)\nax.set_xlabel('Iterations')\ntheta = np.random.randn(2,1)\n\n_=ax.plot(range(n_iter),cost_history,'b.')","cfa780f9":"def momentum_gradient_descent(X,y,theta,learning_rate=0.01, momentum=0.5,iterations=100):\n    '''\n    X    = Matrix of X with added bias units\n    y    = Vector of Y\n    theta=Vector of thetas np.random.randn(j,1)\n    learning_rate \n    iterations = no of iterations\n    \n    Returns the final theta vector and array of cost history over no of iterations\n    '''\n    # m as number of samples\n    m = len(y)\n    \n    # create placeholder variables for historical data (useful for plotting)\n    cost_history = np.zeros(iterations)\n    theta_history = np.zeros((iterations,2))\n    v_old = 0\n    # iterate through each step\n    for it in range(iterations): \n        # calculate current output of each step\n        prediction = np.dot(X,theta)\n        v_new = momentum * v_old + (1\/m)*learning_rate*( X.T.dot((prediction - y)))\n        # update theta using the equations above\n        theta = theta - v_new\n        # update theta and cost history\n        theta_history[it,:] =theta.T\n        cost_history[it]  = cal_cost(theta,X,y)\n        v_old = v_new\n        \n    return theta, cost_history, theta_history","88d6b512":"# learning rate\nlr = 0.01\n# number of iterations\nn_iter = 2000\n# momentum\nmomentum = 0.9\n\n# initialize random thetas\ntheta = np.random.randn(2,1)\n\n# create input matrix\nX_b = np.c_[np.ones((len(X),1)),X]\n\n# use gradient descent function above to optimize\ntheta,cost_history,theta_history = momentum_gradient_descent(X_b,y,theta,lr,momentum,n_iter)\n\n\nprint('Theta0:          {:0.3f},\\nTheta1:          {:0.3f}'.format(theta[0][0],theta[1][0]))\nprint('Final cost\/MSE:  {:0.3f}'.format(cost_history[-1]))","21a83d8b":"# plot cost values from history as blue dots ('b.')\nplt.plot(range(n_iter),cost_history,'b.')\n# set name for each axis\nplt.ylabel('Cost')\nplt.xlabel('Iterations')\n# show plot\nplt.show()","c5ed9ed0":"# plot first 150 cost values (range(150)) from history as blue dots ('b.')\nplt.plot(range(150),cost_history[:150],'b.')\n# set name for each axis\nplt.ylabel('Cost')\nplt.xlabel('Iterations')\n# show plot\nplt.show()","5b2d14d7":"def nesterov_gradient_descent(X,y,theta,learning_rate=0.01, momentum=0.5,iterations=100):\n    '''\n    X    = Matrix of X with added bias units\n    y    = Vector of Y\n    theta=Vector of thetas np.random.randn(j,1)\n    learning_rate \n    iterations = no of iterations\n    \n    Returns the final theta vector and array of cost history over no of iterations\n    '''\n    # m as number of samples\n    m = len(y)\n    \n    # create placeholder variables for historical data (useful for plotting)\n    cost_history = np.zeros(iterations)\n    theta_history = np.zeros((iterations,2))\n    v_old = 0\n    # iterate through each step\n    for it in range(iterations): \n        # calculate current output of each step\n        prediction = np.dot(X,theta - momentum * v_old)\n        v_new = momentum * v_old + (1\/m)*learning_rate*( X.T.dot((prediction - y)))\n        # update theta using the equations above\n        theta = theta - v_new\n        # update theta and cost history\n        theta_history[it,:] =theta.T\n        cost_history[it]  = cal_cost(theta,X,y)\n        v_old = v_new\n        \n    return theta, cost_history, theta_history","52005ee3":"# learning rate\nlr = 0.01\n# number of iterations\nn_iter = 2000\n# momentum\nmomentum = 0.9\n\n# initialize random thetas\ntheta = np.random.randn(2,1)\n\n# create input matrix\nX_b = np.c_[np.ones((len(X),1)),X]\n\n# use gradient descent function above to optimize\ntheta,cost_history,theta_history = nesterov_gradient_descent(X_b,y,theta,lr,momentum,n_iter)\n\n\nprint('Theta0:          {:0.3f},\\nTheta1:          {:0.3f}'.format(theta[0][0],theta[1][0]))\nprint('Final cost\/MSE:  {:0.3f}'.format(cost_history[-1]))","649cc6b7":"# plot cost values from history as blue dots ('b.')\nplt.plot(range(n_iter),cost_history,'b.')\n# set name for each axis\nplt.ylabel('Cost')\nplt.xlabel('Iterations')\n# show plot\nplt.show()","3b9ce5a2":"# plot first 150 cost values (range(150)) from history as blue dots ('b.')\nplt.plot(range(150),cost_history[:150],'b.')\n# set name for each axis\nplt.ylabel('Cost')\nplt.xlabel('Iterations')\n# show plot\nplt.show()","cca684cd":"<h3> Let's start with 1000 iterations and a learning rate of 0.01. Start with theta from a Gaussian distribution","6651a6dc":"<h3> Let's plot the cost history over iterations","7841e95a":"<h3> After around 150 iterations the cost is flat so the remaining iterations  are not needed or will not result in any further optimization. Let us zoom in till iteration 150 and see the curve","aa857f91":"#### So you can see GDM converge faster than vanilla GD. Yay!","d3d3a5c5":"# Stochastic Gradient Descent","b2c295ea":"# Create Data\n<h5> Generate some data with:\n\\begin{equation} \\theta_0= 4 \\end{equation} \n\\begin{equation} \\theta_1= 3 \\end{equation} \n\nAdd some Gaussian noise to the data","1a922b24":"We have:\n\\begin{equation}\ny^T = \\begin{bmatrix} b & w \\end{bmatrix} \\cdotp \\begin{bmatrix} 1 & X \\end{bmatrix}^T\n\\end{equation}\nSo:\n\\begin{equation}\ny^T \\cdotp \\begin{bmatrix} 1 & X \\end{bmatrix} = \\begin{bmatrix} b & w \\end{bmatrix} \\cdotp \\begin{bmatrix} 1 & X \\end{bmatrix}^T \\cdotp \\begin{bmatrix} 1 & X \\end{bmatrix}\n\\end{equation}\nSo:\n\\begin{equation}\ny^T \\cdotp \\begin{bmatrix} 1 & X \\end{bmatrix} \\cdotp (\\begin{bmatrix} 1 & X \\end{bmatrix}^T \\cdotp \\begin{bmatrix} 1 & X \\end{bmatrix})^I = \\begin{bmatrix} b & w \\end{bmatrix}\n\\end{equation}","ad093c62":"# Gradient Descent\n## Cost Function & Gradients\n\n<h4> The equation for calculating cost function and gradients are as shown below. Please note the cost function is for Linear regression. For other algorithms the cost function will be different and the gradients would have  to be derived from the cost functions\n\n<b>Cost<\/b>\n\\begin{equation}\nJ(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h(\\theta)^{(i)} - y^{(i)})^2 \n\\end{equation}\n\n<b>Gradient<\/b>\n\n\\begin{equation}\n\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m}\\sum_{i=1}^{m}(h(\\theta^{(i)} - y^{(i)}).X_j^{(i)}\n\\end{equation}\n\n<b>Gradients<\/b>\n\\begin{equation}\n\\theta_0: = \\theta_0 -\\alpha . (\\frac{1}{m} .\\sum_{i=1}^{m}(h(\\theta^{(i)} - y^{(i)}).X_0^{(i)})\n\\end{equation}\n\\begin{equation}\n\\theta_1: = \\theta_1 -\\alpha . (\\frac{1}{m} .\\sum_{i=1}^{m}(h(\\theta^{(i)} - y^{(i)}).X_1^{(i)})\n\\end{equation}\n\\begin{equation}\n\\theta_2: = \\theta_2 -\\alpha . (\\frac{1}{m} .\\sum_{i=1}^{m}(h(\\theta^{(i)} - y^{(i)}).X_2^{(i)})\n\\end{equation}\n\n\\begin{equation}\n\\theta_j: = \\theta_j -\\alpha . (\\frac{1}{m} .\\sum_{i=1}^{m}(h(\\theta^{(i)} - y^{(i)}).X_0^{(i)})\n\\end{equation}","b6fa0d06":"# Nesterov Accelerated Gradient Descent","2bb34dc3":"<h5>Let's plot prediction line with calculated thetas","ecb03b34":"<b>It is worth while to note that the cost drops faster initially and then the gain in cost reduction is not as much\n### It would be great to see the effect of different learning rates and iterations together\n### Let us  build a function which can show the effects together and also show how gradient decent actually is working","424df7c1":"Apply our formular:\n\\begin{equation}\ny^T = \\begin{bmatrix} b & w \\end{bmatrix} \\cdotp \\begin{bmatrix} 1 & X \\end{bmatrix}^T\n\\end{equation}","c1cfcf25":"### Plot the graphs for different iterations and learning rates combination","0f24b37d":"<h5>This is quite close to our real thetas 4 and 3. It cannot be accurate due to the noise I have introduced in data.","04ae2276":"Let's plot our data to check the relation between X and Y","3d907572":"<b> See how useful it is to visualize the effect of learning rates and iterations on gradient descent. The red lines show how the gradient descent starts and then slowly gets closer to the final value","cc4dae73":"# Mini Batch Gradient Descent","7b7a8fda":"# Momentum Gradient Descent","829637eb":"As usual, we start with importing some libraries we are going to use.","762df02a":"#  Analytical way of Linear Regression","c7b33913":"#### Okay now plot 150 first iterations."}}