{"cell_type":{"52b6c609":"code","0fca5158":"code","4187ee1f":"code","4b60dff9":"code","c6d04a61":"code","7e353361":"code","252bfcd1":"code","7d0cf5fc":"code","08e9bce2":"code","bd092c4f":"code","3cefe401":"code","c8e9733a":"code","12cb030f":"code","25a6814e":"code","5aa69802":"code","10846514":"code","1ec9c804":"code","cf3ceb69":"code","7b832456":"code","1df96ad2":"markdown","7ce8cf7f":"markdown","fd4de4c3":"markdown","d599d218":"markdown","de84048d":"markdown","18716ee8":"markdown","ec34be1c":"markdown","c281a85f":"markdown","3f1791f8":"markdown","d1e93a5e":"markdown","47f89566":"markdown","46020ff7":"markdown","1015dc70":"markdown"},"source":{"52b6c609":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","0fca5158":"data = pd.read_csv(r'\/kaggle\/input\/twitter-sentiment-dataset\/Twitter_Data.csv')\ndata.sample(5)\n\ndata.dropna(inplace = True)\n# data_df = data = data[:20000] \ndata_df = data ","4187ee1f":" # length of the longest string in df\ndata.clean_text.str.len().max()","4b60dff9":"data.category.value_counts().plot(kind = 'bar')","c6d04a61":"from nltk.probability import FreqDist\nentire_text = \" \".join([str(text) for text in data.clean_text.values.tolist()])\nfdist = FreqDist(entire_text.split())\ntop_ten = fdist.most_common(10)","7e353361":"top_ten","252bfcd1":"# Text cleaning\nimport nltk\nfrom bs4 import BeautifulSoup\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem import WordNetLemmatizer \nimport re\nfrom tqdm import tqdm\n\ndef text_cleaning(text):\n    # change the text into lower case.(Note: in case of social media text, it is good to leave them as it is!)\n    text=text.lower()\n    # removing xml tags from tweets\n    text=BeautifulSoup(text, 'lxml').get_text()\n    # removing URLS \n    text=re.sub('https?:\/\/[A-Za-z0-9.\/]+','',text)\n    # removing words with \"@\"\n    text=re.sub(r'@[A-Za-z0-9]+','',text) \n    # removing special characters\n    text= re.sub(r\"\\W+|_\", ' ', text)\n    # tokenization of sentences\n    text= word_tokenize(text)\n    # lemmatize the text using WordNet\n    lm=WordNetLemmatizer()\n    words = [lm.lemmatize(word) for word in text if word not in set(stopwords.words('english'))]   \n    \n    return \" \".join(words)","7d0cf5fc":"# data.clean_text = data['clean_text'].apply(text_cleaning)","08e9bce2":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data.clean_text, data.category, test_size=0.25, random_state=42)\nprint(X_train.shape, X_test.shape)\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf=TfidfVectorizer(ngram_range=(1, 1),\n    max_df=1.0,\n    min_df=1,)\nX_train_tfidf=tfidf.fit_transform(X_train)\nX_test_tfidf=tfidf.transform(X_test)","bd092c4f":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import plot_confusion_matrix\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score\nmodel=LogisticRegression(solver='liblinear')\nmodel.fit(X_train_tfidf, y_train)\n\ny_pred=model.predict(X_test_tfidf)\nprint(accuracy_score(y_test, y_pred))\n\nplot_confusion_matrix(model, X_test_tfidf, y_test) ","3cefe401":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\nmodel.fit(X_train_tfidf, y_train)\n\ny_pred=model.predict(X_test_tfidf)\nprint(accuracy_score(y_test, y_pred))\nplot_confusion_matrix(model, X_test_tfidf, y_test) ","c8e9733a":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import losses\nfrom tensorflow.keras import preprocessing\nfrom tensorflow.keras import utils\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional, Dropout","12cb030f":"max_features = 20000\ntokenizer = Tokenizer(num_words = max_features, )\ntokenizer.fit_on_texts(data_df['clean_text'].values)\nX = tokenizer.texts_to_sequences(data_df['clean_text'].values)\nX = pad_sequences(X, padding = 'post' ,maxlen=300)\nY = pd.get_dummies(data_df['category']).values\n\nvocab_size = len(tokenizer.word_index)+1","25a6814e":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.25, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","5aa69802":"embid_dim = 300   # embid_dim will give the word vector value in 300 dimensions\nlstm_out = 128\n\n# will have total 2 layers\nmodel = keras.Sequential()\nmodel.add(Embedding(max_features, embid_dim, input_length = X.shape[1]))\nmodel.add(Bidirectional(LSTM(lstm_out, dropout=0.2)))\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation = 'relu'))\nmodel.add(Dense(3, activation = 'softmax'))     # softmax for final layer\nmodel.summary()","10846514":"batch_size = 128\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nhistory = model.fit(X_train, Y_train, epochs = 5, batch_size=batch_size, verbose = 1, validation_data =(X_test, Y_test))","1ec9c804":"y_prob = model.predict(X_test)\ny_classes = y_prob.argmax(axis=-1)","cf3ceb69":"from sklearn.metrics import confusion_matrix\nfor i in range(len(list(y_classes))):\n    if y_classes[i] == 0 :\n        y_classes[i] = -1\n    elif  y_classes[i] == 1 :\n        y_classes[i] = 0\n    else:\n        y_classes[i] = 1\n        \nprint(accuracy_score(y_test, y_classes)) ","7b832456":"sns.heatmap(confusion_matrix(y_test, y_classes), annot=True, fmt='d' , )","1df96ad2":"# Data Cleaning","7ce8cf7f":"So, we can not words directly sent normal text to any ML\/DL model, so for that we need some techniques where we convert normal text in some form which is understable by models, to name a few, basics ones are BOW, TDIDF and advance such as word2vec, GloVe, fasttext.\n\n#### TFIDF is quite simple understand, we can understand, it gives imprtance to \n1. Rare words in corpus\n2. Common words in document\n\n#### word2vec is another famous method where is trained with deep learning model.\nTwo types of word2vec based on core training idea:\n1. CBOW: Given context word predict focus word\n2. Skipgram: Given focus word predict context words","fd4de4c3":"# Predication with Test data","d599d218":"### Spliting the entire data into train and test, it is being divided in 75: 25 ratio.","de84048d":"# Deep Learning Model: LSTM\n\n\nLSTM is a Deep learning model, it is succesor of RNN, as RNN suffers with Gradiant Explosion and does not work with long sequence of text.","18716ee8":"Observation and Comments: \n* Most Frequent word is \"Modi\", and others are normal stopwords.\n* We will have to remove the stopwords.\n* We can train model with\/without keyword \"Modi\".","ec34be1c":"# Data Transformation","c281a85f":"## What are most frequent words","3f1791f8":"Observation: We are seeing data is not balanced but not also very implance.","d1e93a5e":"### Transforming the data for LSTM model","47f89566":"# Reading the provided data","46020ff7":"Defining our model","1015dc70":"# Lets deep dive into the data"}}