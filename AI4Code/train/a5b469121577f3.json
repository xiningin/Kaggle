{"cell_type":{"df8e9f9b":"code","671570b2":"code","a0c1c74a":"code","ce6e86f2":"code","fe4012f0":"code","2d886a4f":"code","1720c1ed":"code","d60f327e":"code","83805904":"code","8046f8d1":"code","7f8f8d81":"code","dcca1d13":"code","4ac2a927":"code","f68210ce":"code","3c8e8b08":"code","269f4dcc":"code","ae784048":"code","c20cacbc":"code","db51d902":"code","2a021dd6":"code","165b3bf6":"code","89d2b8ec":"code","4b0ac85c":"code","6b50f83a":"code","759e0478":"code","0b051887":"code","b4216523":"code","d58db4b6":"code","50f3a376":"code","e4b59556":"code","b04aefae":"code","42e5e5fe":"code","dee7a9fe":"code","1f907c44":"markdown"},"source":{"df8e9f9b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","671570b2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport eli5\n\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import GridSearchCV, KFold, train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix\nimport lightgbm as lgb\nimport xgboost as xgb\n\nimport gc\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","a0c1c74a":"tourney_result = pd.read_csv('..\/input\/ncaaw-march-mania-2021\/WNCAATourneyCompactResults.csv')\ntourney_result = tourney_result[tourney_result['Season']<2016]\ntourney_seed = pd.read_csv('..\/input\/ncaaw-march-mania-2021\/WNCAATourneySeeds.csv')\ntourney_seed = tourney_seed[tourney_seed['Season'] <2016]\ntourney_result = tourney_result.drop(['DayNum', 'WScore', 'LScore', 'WLoc', 'NumOT'], axis=1)\ntourney_result = pd.merge(tourney_result, tourney_seed, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left')\ntourney_result.rename(columns={'Seed':'WSeed'}, inplace=True)\ntourney_result = tourney_result.drop('TeamID', axis=1)\ntourney_result = pd.merge(tourney_result, tourney_seed, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left')\ntourney_result.rename(columns={'Seed':'LSeed'}, inplace=True)\ntourney_result = tourney_result.drop('TeamID', axis=1)","ce6e86f2":"tourney_seed","fe4012f0":"def get_seed(x):\n    return int(x[1:3])\n\ntourney_result['WSeed'] = tourney_result['WSeed'].map(lambda x: get_seed(x))\ntourney_result['LSeed'] = tourney_result['LSeed'].map(lambda x: get_seed(x))","2d886a4f":"season_result = pd.read_csv('..\/input\/ncaaw-march-mania-2021\/WNCAATourneyCompactResults.csv')\nseason_result = season_result[season_result['Season'] <2016]\nseason_win_result = season_result[['Season', 'WTeamID', 'WScore']]\nseason_lose_result = season_result[['Season', 'LTeamID', 'LScore']]\nseason_win_result.rename(columns={'WTeamID':'TeamID', 'WScore':'Score'}, inplace=True)\nseason_lose_result.rename(columns={'LTeamID':'TeamID', 'LScore':'Score'}, inplace=True)\nseason_result = pd.concat((season_win_result, season_lose_result)).reset_index(drop=True)\nseason_score = season_result.groupby(['Season', 'TeamID'])['Score'].sum().reset_index()","1720c1ed":"tourney_result = pd.merge(tourney_result, season_score, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left')\ntourney_result.rename(columns={'Score':'WScoreT'}, inplace=True)\ntourney_result = tourney_result.drop('TeamID', axis=1)\ntourney_result = pd.merge(tourney_result, season_score, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left')\ntourney_result.rename(columns={'Score':'LScoreT'}, inplace=True)\ntourney_result = tourney_result.drop('TeamID', axis=1)","d60f327e":"tourney_win_result = tourney_result.drop(['Season', 'WTeamID', 'LTeamID'], axis=1)\ntourney_win_result.rename(columns={'WSeed':'Seed1', 'LSeed':'Seed2', 'WScoreT':'ScoreT1', 'LScoreT':'ScoreT2'}, inplace=True)","83805904":"tourney_lose_result = tourney_win_result.copy()\ntourney_lose_result['Seed1'] = tourney_win_result['Seed2']\ntourney_lose_result['Seed2'] = tourney_win_result['Seed1']\ntourney_lose_result['ScoreT1'] = tourney_win_result['ScoreT2']\ntourney_lose_result['ScoreT2'] = tourney_win_result['ScoreT1']","8046f8d1":"tourney_win_result['Seed_diff'] = tourney_win_result['Seed1'] - tourney_win_result['Seed2']\ntourney_win_result['ScoreT_diff'] = tourney_win_result['ScoreT1'] - tourney_win_result['ScoreT2']\ntourney_lose_result['Seed_diff'] = tourney_lose_result['Seed1'] - tourney_lose_result['Seed2']\ntourney_lose_result['ScoreT_diff'] = tourney_lose_result['ScoreT1'] - tourney_lose_result['ScoreT2']","7f8f8d81":"tourney_win_result['result'] = 1\ntourney_lose_result['result'] = 0\ntrain_df = pd.concat((tourney_win_result, tourney_lose_result)).reset_index(drop=True)\ntrain_df","dcca1d13":"season_result","4ac2a927":"test_df = pd.read_csv('..\/input\/ncaaw-march-mania-2021\/WSampleSubmissionStage1.csv')\nsub = test_df.copy()","f68210ce":"test_df","3c8e8b08":"test_df['Season'] = test_df['ID'].map(lambda x: int(x[:4]))\ntest_df['WTeamID'] = test_df['ID'].map(lambda x: int(x[5:9]))\ntest_df['LTeamID'] = test_df['ID'].map(lambda x: int(x[10:14]))","269f4dcc":"tourney_seed = pd.read_csv('..\/input\/ncaaw-march-mania-2021\/WNCAATourneySeeds.csv')\ntourney_seed = tourney_seed[tourney_seed['Season']> 2015 ]","ae784048":"season_result = pd.read_csv('..\/input\/ncaaw-march-mania-2021\/WRegularSeasonCompactResults.csv')\nseason_result = season_result[season_result['Season'] > 2015]\nseason_win_result = season_result[['Season', 'WTeamID', 'WScore']]\nseason_lose_result = season_result[['Season', 'LTeamID', 'LScore']]\nseason_win_result.rename(columns={'WTeamID':'TeamID', 'WScore':'Score'}, inplace=True)\nseason_lose_result.rename(columns={'LTeamID':'TeamID', 'LScore':'Score'}, inplace=True)\nseason_result = pd.concat((season_win_result, season_lose_result)).reset_index(drop=True)\nseason_score = season_result.groupby(['Season', 'TeamID'])['Score'].sum().reset_index()","c20cacbc":"test_df = pd.merge(test_df, tourney_seed, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left')\ntest_df.rename(columns={'Seed':'Seed1'}, inplace=True)\ntest_df = test_df.drop('TeamID', axis=1)\ntest_df = pd.merge(test_df, tourney_seed, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left')\ntest_df.rename(columns={'Seed':'Seed2'}, inplace=True)\ntest_df = test_df.drop('TeamID', axis=1)\ntest_df = pd.merge(test_df, season_score, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left')\ntest_df.rename(columns={'Score':'ScoreT1'}, inplace=True)\ntest_df = test_df.drop('TeamID', axis=1)\ntest_df = pd.merge(test_df, season_score, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left')\ntest_df.rename(columns={'Score':'ScoreT2'}, inplace=True)\ntest_df = test_df.drop('TeamID', axis=1)","db51d902":"test_df['Seed1'] = test_df['Seed1'].map(lambda x: get_seed(x))\ntest_df['Seed2'] = test_df['Seed2'].map(lambda x: get_seed(x))\ntest_df['Seed_diff'] = test_df['Seed1'] - test_df['Seed2']\ntest_df['ScoreT_diff'] = test_df['ScoreT1'] - test_df['ScoreT2']\ntest_df = test_df.drop(['ID', 'Pred', 'Season', 'WTeamID', 'LTeamID'], axis=1)\ntest_df","2a021dd6":"X = train_df.drop('result', axis=1)\ny = train_df.result","165b3bf6":"params = {'num_leaves': 127,\n          'min_data_in_leaf': 10,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.01,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 7,\n          \"metric\": 'logloss',\n          \"verbosity\": -1,\n          'random_state': 42,\n         }\n","89d2b8ec":"params_xgb = {'max_depth':63,\n              'objective':'binary:logistic',\n              'min_child_weight': 10,\n              'learning_rate': 0.0001,\n              'eta'      :0.3,\n              'subsample':0.8,\n              'lambda '  :4,\n              'eval_metric':'logloss',\n              'n_estimators':2000,\n              'colsample_bytree ':0.9,\n              'colsample_bylevel':1\n              }\n\n","4b0ac85c":"# LGB\nlgb_num_leaves_max = 255\nlgb_in_leaf = 50\nlgb_lr = 0.0001\nlgb_bagging = 7\n\n# XGB\nxgb_max_depth = 20\nxgb_min_child_weight = 75\nxgb_lr = 0.0005\nxgb_num_boost_round_max = 4000\n# without early_stopping_rounds\n\n# Set weight of models\nw_lgb = 0.6\nw_xgb = 0.3\nw_logreg = 1 - w_lgb - w_xgb\nw_logreg","6b50f83a":"params_lgb = {'num_leaves': lgb_num_leaves_max,\n              'min_data_in_leaf': lgb_in_leaf,\n              'objective': 'binary',\n              'max_depth': -1,\n              'learning_rate': lgb_lr,\n              \"boosting_type\": \"gbdt\",\n              \"bagging_seed\": lgb_bagging,\n              \"metric\": 'logloss',\n              \"verbosity\": -1,\n              'random_state': 42,\n             }","759e0478":"NFOLDS = 10\nfolds = KFold(n_splits=NFOLDS)\n\ncolumns = X.columns\nsplits = folds.split(X, y)\ny_preds_lgb = np.zeros(test_df.shape[0])\ny_train_lgb = np.zeros(X.shape[0])\ny_oof = np.zeros(X.shape[0])\n\nfeature_importances = pd.DataFrame()\nfeature_importances['feature'] = columns\n  \nfor fold_n, (train_index, valid_index) in enumerate(splits):\n    print('Fold:',fold_n+1)\n    X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    dtrain = lgb.Dataset(X_train, label=y_train)\n    dvalid = lgb.Dataset(X_valid, label=y_valid)\n\n    clf = lgb.train(params_lgb, dtrain, 10000, valid_sets = [dtrain, dvalid], verbose_eval=200)\n    \n    feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importance()\n    \n    y_pred_valid = clf.predict(X_valid)\n    y_oof[valid_index] = y_pred_valid\n    \n    y_train_lgb += clf.predict(X) \/ NFOLDS\n    y_preds_lgb += clf.predict(test_df) \/ NFOLDS\n    \n    del X_train, X_valid, y_train, y_valid\n    gc.collect()","0b051887":"params_xgb = {'max_depth': xgb_max_depth,\n              'objective': 'binary:logistic',\n              'min_child_weight': xgb_min_child_weight,\n              'learning_rate': xgb_lr,\n              'eta'      : 0.3,\n              'subsample': 0.8,\n              'lambda '  : 4,\n              'eval_metric': 'logloss',\n              'colsample_bytree ': 0.9,\n              'colsample_bylevel': 1\n              }","b4216523":"NFOLDS = 10\nfolds = KFold(n_splits=NFOLDS)\n\ncolumns = X.columns\nsplits = folds.split(X, y)\n\ny_preds_xgb = np.zeros(test_df.shape[0])\ny_train_xgb = np.zeros(X.shape[0])\ny_oof_xgb = np.zeros(X.shape[0])\n\ntrain_df_set = xgb.DMatrix(X)\ntest_set = xgb.DMatrix(test_df)\n  \nfor fold_n, (train_index, valid_index) in enumerate(splits):\n    print('Fold:',fold_n+1)\n    X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    train_set = xgb.DMatrix(X_train, y_train)\n    val_set = xgb.DMatrix(X_valid, y_valid)\n    \n    clf = xgb.train(params_xgb, train_set, num_boost_round=xgb_num_boost_round_max, evals=[(train_set, 'train'), (val_set, 'val')], verbose_eval=100)\n    \n    y_train_xgb += clf.predict(train_df_set) \/ NFOLDS\n    y_preds_xgb += clf.predict(test_set) \/ NFOLDS\n    \n    del X_train, X_valid, y_train, y_valid\n    gc.collect()","d58db4b6":"def plot_cm(y_true, y_pred, title, figsize=(7,6)):\n    y_pred = y_pred.round().astype(int)\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)","50f3a376":"# Showing Confusion Matrix for XGB model\nplot_cm(y, y_train_xgb, 'Confusion matrix for XGB model')","e4b59556":"y_preds = 0.5*y_preds_lgb + 0.5*y_preds_xgb ","b04aefae":"sub['Pred'] = y_preds\nsub.head()","42e5e5fe":"sub['Pred'].hist()","dee7a9fe":"sub.to_csv('submission.csv', index=False)","1f907c44":"**Using LGBM**"}}