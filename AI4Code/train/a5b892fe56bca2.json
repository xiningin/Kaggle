{"cell_type":{"77825653":"code","6851d278":"code","8f4feb7d":"code","b83a7814":"code","741da106":"code","6035d0d9":"code","4b6866ba":"code","114758af":"code","566df063":"code","372a5347":"code","6ddb0ca8":"code","b23e7ddd":"code","b4639455":"code","8ab3aa31":"code","53089028":"code","f48c64c3":"code","ae2bb673":"code","a42e5011":"code","3d8d4b9d":"code","b675cbf2":"code","15a5bfcf":"code","7001c4c6":"code","91d7ab7b":"code","aa13bf03":"code","c46bf98f":"code","3dd5dcb6":"code","0e6dd589":"markdown","d03e49ca":"markdown","2af45fd7":"markdown","a9fdf9a8":"markdown","58f5d873":"markdown","2c1dabe2":"markdown","59a1d05a":"markdown","ac13b49c":"markdown","559b1cf3":"markdown","134947ff":"markdown","861cc7c2":"markdown","f24bb1fb":"markdown","bf2ded3a":"markdown","55590d5d":"markdown","63b27b5e":"markdown","9ce64075":"markdown","66c2280e":"markdown","ec3c8080":"markdown","8a13fe95":"markdown","f49ec5a5":"markdown","a3d4d7a9":"markdown"},"source":{"77825653":"import numpy as np\nimport pandas as pd\npd.options.mode.chained_assignment = None","6851d278":"train=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntrain.head()","8f4feb7d":"test=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\npid=test.PassengerId","b83a7814":"train.dtypes","741da106":"train.info()","6035d0d9":"#Null value count\nprint('Train Set')\nprint(train[train.isnull()])\n#print(train.isnull())\nprint('\\nTest Set')\nprint(test.isnull().sum())","4b6866ba":"#Train dataset fillna\ntrain['Age']=train['Age'].fillna(train['Age'].median())\ntrain['Embarked']=train['Embarked'].fillna(train['Embarked'].mode()[0])\n\n#Test datset fillna\ntest['Age']=test['Age'].fillna(test['Age'].median())\ntest['Fare']=test['Fare'].fillna(test['Fare'].mean())","114758af":"#Null values count\nprint('Train Set')\nprint(train.isnull().sum())\nprint('\\nTest Set')\nprint(test.isnull().sum())","566df063":"#Creating new columns my merging or using previous columns\nfor data in [train,test]:\n    data['FamilySize']=data['SibSp']+data['Parch']+1\n    data['IsAlone']=1\n    data['IsAlone'].loc[data['FamilySize']>1]=0\n    \n    #Gets the title from the name (Mr, Mrs, etc)\n    data['Title'] = data['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]","372a5347":"train.head()","6ddb0ca8":"train['Title'].value_counts()","b23e7ddd":"#Too many unknown titles are there\n#Cleaning the 'Title column to just include [Mr, Mrs, Miss, Master, Misc]'\n#Misc contains all the unknown and gender neutral titles\n\n#Replacing known titles\ntrain['Title']=train['Title'].replace('Ms','Miss')\ntrain['Title']=train['Title'].replace('Mlle','Miss')\ntrain['Title']=train['Title'].replace('the Countess','Mrs')\ntrain['Title']=train['Title'].replace('Mme','Mrs')\n\ntest['Title']=test['Title'].replace('Dona','Mrs')\ntest['Title']=test['Title'].replace('Ms','Miss')\n\n#Replacing by misc\nnames=(train['Title'].value_counts() < 10)\ntrain['Title']=train['Title'].apply(lambda x: 'Misc' if names.loc[x] == True else x)\nnames=(test['Title'].value_counts() < 10)\ntest['Title']=test['Title'].apply(lambda x: 'Misc' if names.loc[x] == True else x)\n\nprint('Train Set\\n',train['Title'].value_counts())\nprint('\\nTest Set\\n',test['Title'].value_counts())","b4639455":"#Drop columns\ncolumns=['PassengerId','Cabin','Ticket','Name']\ntrain=train.drop(columns,axis=1)\ntest=test.drop(columns,axis=1)","8ab3aa31":"#Converting categorical columns into numerical columns using LabelEncoder\n#LabelEncoder gives each unique str\/char a numerical value starting from 0 \nfrom sklearn.preprocessing import LabelEncoder\n\nlabel=LabelEncoder()\nfor data in [train,test]:\n    data['Sex']=label.fit_transform(data['Sex'])            # 0:Female, 1:Male\n    data['Embarked']=label.fit_transform(data['Embarked'])  # 0:C , 1:Q 2:S\n    data['Title']=label.fit_transform(data['Title'])        # 0:Master 1:Misc 2:Miss, 3:Mr, 4:Mrs\n    \n    data['Age']=data['Age'].astype('int64')\n      \ntrain.head()    ","53089028":"#Percentage Survived for each category\ntarget=['Survived']\nselected=['Sex','Pclass','Embarked','Title','SibSp','Parch','FamilySize','IsAlone']\nfor x in selected:\n    print('Survival Percentage By',x)\n    print(train[[x, target[0]]].groupby(x,as_index=False).mean(),'\\n')\n        \n# Sex - 0: Female, 1: Male\n# Embarked - 0: C, 1: Q, 2: S\n# Title - 0: Master, 1: Misc, 2. Miss, 3: Mr, 4: Mrs\n# IsAlone - 0:No, 1: Yes","f48c64c3":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Shows the ratio of survived:dead passengers according to age\nplt.figure(figsize=(10,5))\nplt.hist(x=[train[train['Survived']==1]['Age'], train[train['Survived']==0]['Age']], stacked=True, color = ['b','r'],label = ['Survived','Dead'])\nplt.title('Survival by Age')\nplt.xlabel('Age')\nplt.ylabel('# of Passengers')\nplt.legend()","ae2bb673":"#Shows the percent Survival according to each Embarkment, Class and whether the passenger is alone or not\nfig,ax=plt.subplots(1, 3,figsize=(15,5))\nsns.barplot(x='Embarked',y='Survived',data=train,ax=ax[0])\nsns.barplot(x='Pclass',y='Survived',order=[1,2,3],data=train,ax=ax[1])\nsns.barplot(x='IsAlone',y='Survived',order=[1,0],data=train,ax=ax[2])","a42e5011":"#Shows the survival of Passengers according to thier ticket price\ndf=train.copy()\ndf['Fare'] = pd.cut(df['Fare'], bins=[0, 50, 100, 150, 200, 250, 300,600])\nplt.figure(figsize=(10,5))\nsns.pointplot(x='Fare',y='Survived', data=df)","3d8d4b9d":"#Shows the survival of Passengers according to thier age\ndf['Age'] = pd.cut(df['Age'], bins=[0,10,20,30,40,50,60,70,80])\nplt.figure(figsize=(10,5))\nsns.pointplot(x='Age',y='Survived', data=df)","b675cbf2":"#Survivals according to family size\nplt.figure(figsize=(10,5))\nsns.pointplot(x='FamilySize', y='Survived',data=train)","15a5bfcf":"#Survival of each sex on the basis of embarkment, class and whether the passenger is alone or not\nfig,ax=plt.subplots(1,3,figsize=(20,7))\nsns.barplot(x='Sex',y='Survived',hue='Embarked',data=train,ax=ax[0])\nsns.barplot(x='Sex',y='Survived',hue ='Pclass',data=train,ax=ax[1])\nsns.barplot(x='Sex',y='Survived',hue='IsAlone',data=train,ax=ax[2])","7001c4c6":"#Selecting the independent and dependent variables\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\ntarget=train['Survived']\ntrain.drop(['Survived'],axis=1, inplace=True)","91d7ab7b":"#Training on different models\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom catboost import CatBoostRegressor\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(train, target, test_size=0.2, random_state=1, stratify=target)\n\nprint('Mean Absolute Errors:')\n\n#RandomForestClassifier\nmodel = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=4, max_features='auto',\n                       max_leaf_nodes=5, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=15,\n                       min_weight_fraction_leaf=0.0, n_estimators=350,\n                       n_jobs=None, oob_score=True, random_state=1, verbose=0,\n                       warm_start=False)\nmodel.fit(X_train, y_train)\npredict = model.predict(X_val)\nprint('Random Forrest: ' + str(mean_absolute_error(predict, y_val)))\n\n#XGBoost\nmodel = XGBRegressor(learning_rate=0.01, n_estimators=3460, max_depth=3, min_child_weight=0,\n                        gamma=0, subsample=0.7,colsample_bytree=0.7,objective='reg:squarederror',\n                        nthread=-1,scale_pos_weight=1, seed=27, reg_alpha=0.00006)\nmodel.fit(X_train, y_train)\npredict = model.predict(X_val)\nprint('XGBoost: ' + str(mean_absolute_error(predict, y_val)))\n\n#LassoCV\nmodel = LassoCV(max_iter=1e7,  random_state=14, cv=10)\nmodel.fit(X_train, y_train)\npredict = model.predict(X_val)\nprint('Lasso: ' + str(mean_absolute_error(predict, y_val)))\n\n# GradientBoosting   \nmodel = GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, max_depth=4, random_state=5)\nmodel.fit(X_train, y_train)\npredict = model.predict(X_val)\nprint('GradientBoosting: ' + str(mean_absolute_error(predict, y_val)))","aa13bf03":"#Predicting on the best model\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, roc_curve,auc, confusion_matrix, classification_report\nimport sklearn.metrics as metrics\nimport matplotlib.pyplot as plt\n\n#RandomForestClassifier\nmodel=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                           criterion='gini', max_depth=4, max_features='auto',\n                           max_leaf_nodes=5, max_samples=None,\n                           min_impurity_decrease=0.0, min_impurity_split=None,\n                           min_samples_leaf=1, min_samples_split=15,\n                           min_weight_fraction_leaf=0.0, n_estimators=350,\n                           n_jobs=None, oob_score=True, random_state=1, verbose=0,\n                           warm_start=False)\nmodel.fit(X_train, y_train)\npredict = model.predict(X_val)\nprint('Random Forest MAE: ' + str(mean_absolute_error(predict, y_val)))\nprint(\"Out of Bag Score: %.4f\" % model.oob_score_)\n\ny_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_val)\n\n# Building the ROC Curve and Confusion Matrix\nprint(\"Training accuracy: \", accuracy_score(y_train, y_pred_train))\nprint(\"Testing accuracy: \", accuracy_score(y_val, y_pred_test))\nprint(\"\\nConfusion Matrix\\n\")\nprint('[[True Positive    False Positive]\\n[False Negative    True Negative]]\\n')\nprint(confusion_matrix(y_val, y_pred_test))\n\nfpr, tpr, _ = roc_curve(y_val, y_pred_test)\nroc_auc = auc(fpr, tpr)\nprint(\"\\nROC AUC on evaluation set\",roc_auc )\n\nplt.figure()\nplt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","c46bf98f":"#Prediction\nprediction=model.predict(test)\nprediction","3dd5dcb6":"output = pd.DataFrame({'PassengerId': pid, 'Survived': prediction})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Submission successfully saved!\")","0e6dd589":"# 6. Submission","d03e49ca":"* The younger passengers had a higher survival rate","2af45fd7":"* Passengers with the most expensive ticket survived","a9fdf9a8":"# 1. Reading Data and Generating Statistics","58f5d873":"# Conclusion\n* Females had better chance of survival in comparison to males.\n* Younger passengers had better chance of survival than the older ones.\n* Loners had less chance of survival.\n* More Class 1 passengers survived comparitively.","2c1dabe2":"# 4. Model Data","59a1d05a":"* Female survival rate is significantly higher than the male survival rate in each scenario","ac13b49c":"### Feature Engineering","559b1cf3":"### Statistics","134947ff":"# RMS Titanic\n\nNobody saw the huge iceberg and the RMS Titanic sank to the bottom of the Atlantic Ocean on 15 April, 1912.\nIt boarded 2224 passengers on it's maiden journey and out of those 1502 died.\n\nIn this notebook I perform EDA on the passengers details and try to build a model to identify whether a specific group of people had a higher chance of survival or everybody was equally doomed.\n\nNot gonna talk about \"Rose, Jack and the Plank\" \ud83d\ude05","861cc7c2":"* We get the best MAE value with Random Forrest Classifier\n* This shows that Regressor Models are not suitable for these kind of problems(Classification) as all the regressor models give very high value of MAE.","f24bb1fb":"* More passengers embarking from Cherbourg survived than Queenstown and Southampton.\n* More passengers from Class 1 survived.\n* More passengers with thier families aboard were saved.","bf2ded3a":"* More passengers with 4 family members were rescued.\n* Survival of passengers with more than 4 family members was significantly lower.","55590d5d":"### Categorical Encoding","63b27b5e":"# 2. Data Cleaning","9ce64075":"### This gives an idea as to which group had better chances of survival in the tragedy.\n\n#### Completely unrelated thought from the notebook: \"Jack could have been saved if Rose scooted just a little bit on that plank!\" \ud83d\ude02","66c2280e":"### Visualization","ec3c8080":"### Filling Null Values","8a13fe95":"# Thank You \ud83d\ude4f","f49ec5a5":"# 5. Prediction","a3d4d7a9":"# 3. EDA and Visualization"}}