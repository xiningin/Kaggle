{"cell_type":{"00ae561f":"code","74b1d941":"code","2d074d91":"code","12ec99a3":"code","0e0a33fd":"code","7d4c16ff":"code","af9335fe":"code","283237b6":"code","4711d845":"code","184dc972":"code","60f48947":"code","84425167":"code","9485182c":"code","17691cec":"code","4168df03":"code","9b90bfad":"code","347ef8c1":"code","101cedda":"code","716b91a1":"code","79997a6e":"code","15270a98":"markdown","41339686":"markdown","132ca65d":"markdown","6b0bd5fc":"markdown","b8b952f1":"markdown","147f6b5b":"markdown","3a0bc8f6":"markdown","7b62430f":"markdown","8120d649":"markdown","41b1e8fb":"markdown","014e0735":"markdown","e41d2df0":"markdown","cc576f8f":"markdown","4c60ab04":"markdown","0bb58cbe":"markdown","18971bf2":"markdown"},"source":{"00ae561f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","74b1d941":"df = pd.read_csv(\"\/kaggle\/input\/contradictory-my-dear-watson\/train.csv\")","2d074d91":"df.head(20)","12ec99a3":"df.tail()","0e0a33fd":"df.shape","7d4c16ff":"df.isnull().sum()","af9335fe":"print(\n    f\"number of languages: {len(list(df.language.unique()))}\\n\\n\"\n    f\"{list(df.language.unique())}\")\n","283237b6":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ny = df[\"language\"].value_counts()\n\nplt.figure(figsize=(15,5))\nsns.barplot(x=y.index, y=y)\nplt.show()","4711d845":"label, frequencies = np.unique(df.language, return_counts=True)\nplt.figure(figsize=(10,10))\nplt.pie(frequencies, labels=label, autopct='%.1f%%')\nplt.show()","184dc972":"pd.options.plotting.backend = \"matplotlib\"\ndf.label.hist()\nplt.show()","60f48947":"import seaborn as sns\nsns.catplot(data=df, x=\"label\", col=\"lang_abv\", col_wrap=5, kind=\"count\")\nplt.show()","84425167":"from sklearn.model_selection import train_test_split\n\ndf = df[[\"premise\", \"hypothesis\", \"lang_abv\", \"label\"]]\nX = df.iloc[:, :-1]\ny = df.label\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)  ","9485182c":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Normalizer\n\ndef preprossecing1(train, test, train_label, test_label, lan):\n    \n    vec_p = TfidfVectorizer()\n    vec_h = TfidfVectorizer()\n    pca = PCA(0.9)\n    normalizer = Normalizer(copy=False) # para aplicar m\u00e9todo PCA normaliza\u00e7\u00e3o dos dados \u00e9 reconmend\u00e1vel\n    pipe = make_pipeline(normalizer, pca)\n\n    # filtro de linguagem\n    index_train = train[train.lang_abv==lan].index\n    train_label_final = train_label.loc[index_train]\n    \n    index_test = test[test.lang_abv==lan].index\n    test_label_final = test_label.loc[index_test]\n    \n    # premissas\n    train_p = vec_p.fit_transform(train[train.lang_abv==lan].premise)\n    df_train_p = pd.DataFrame.sparse.from_spmatrix(train_p, columns=[\"p_\"+k for k in vec_p.get_feature_names()])\n    \n    test_p = vec_p.transform(test[test.lang_abv==lan].premise)\n    df_test_p = pd.DataFrame.sparse.from_spmatrix(test_p, columns=[\"p_\"+k for k in vec_p.get_feature_names()])\n\n    #hipotesis\n    train_h = vec_h.fit_transform(train[train.lang_abv==lan].hypothesis)\n    df_train_h = pd.DataFrame.sparse.from_spmatrix(train_h, columns=[\"h_\"+k for k in vec_h.get_feature_names()])\n    \n    test_h = vec_h.transform(test[test.lang_abv==lan].hypothesis)\n    df_test_h = pd.DataFrame.sparse.from_spmatrix(test_h, columns=[\"h_\"+k for k in vec_h.get_feature_names()])\n    \n    #concatenando..\n    train_final = pd.concat([df_train_p, df_train_h], axis=1)\n    test_final = pd.concat([df_test_p, df_test_h], axis=1)\n    \n    train_final = pipe.fit_transform(train_final.to_numpy())\n    test_final = pipe.transform(test_final.to_numpy())\n\n    return train_final, test_final, train_label_final, test_label_final","17691cec":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Normalizer\n\ndef preprossecing2(train, test, train_label, test_label, lan):\n    \n    vec = TfidfVectorizer()\n    pca = PCA(0.9)\n    normalizer = Normalizer(copy=False) # para aplicar m\u00e9todo PCA normaliza\u00e7\u00e3o dos dados \u00e9 reconmend\u00e1vel\n    pipe = make_pipeline(normalizer, pca)\n\n    # filtro de linguagem\n    index_train = train[train.lang_abv==lan].index\n    train_label_final = train_label.loc[index_train]\n    \n    index_test = test[test.lang_abv==lan].index\n    test_label_final = test_label.loc[index_test]\n    \n    #juntando premissas e hipotesis\n    train_text = [p+\" \"+h for p, h in zip(train[train.lang_abv==lan].premise, train[train.lang_abv==lan].hypothesis)]\n    \n    test_text = [p+\" \"+h for p, h in zip(test[test.lang_abv==lan].premise, test[test.lang_abv==lan].hypothesis)]\n    \n\n    #tfidvec\n    train_h = vec.fit_transform(train_text)\n    train_final = pd.DataFrame.sparse.from_spmatrix(train_h, columns=[k for k in vec.get_feature_names()])\n    \n    test_h = vec.transform(test_text)\n    test_final = pd.DataFrame.sparse.from_spmatrix(test_h, columns=[k for k in vec.get_feature_names()])\n    \n    \n    train_final = pipe.fit_transform(train_final.to_numpy())\n    test_final = pipe.transform(test_final.to_numpy())\n\n    return train_final, test_final, train_label_final, test_label_final","4168df03":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Separando premissas e hipoteses por idioma\ndef predictRFC(X_train, X_test, y_train, y_test, inteiro):\n    for lan in X_train.lang_abv.unique():\n            \n        rfc = RandomForestClassifier(n_estimators=1000, max_depth=2)\n\n        if inteiro==0:\n            train, test, train_label, test_label = preprossecing1(X_train, y_train, X_test, y_test, lan)\n        else:\n            train, test, train_label, test_label = preprossecing2(X_train, y_train, X_test, y_test, lan)\n            \n        fig, axs = plt.subplots(1,3, figsize=(15,6))\n        fig.suptitle(f\"Language: {lan}\")\n        for n in [0,1,2]:\n            rfc.fit(train, train_label==n)\n            pred = rfc.predict(test)\n\n            accuracy = metrics.accuracy_score(pred, test_label==n)\n            v = metrics.v_measure_score(pred, test_label==n)\n            \n            cm = confusion_matrix(test_label==n, pred, normalize=\"true\")\n            axs[n].imshow(cm)\n            axs[n].set_xticks(np.arange(2))\n            axs[n].set_yticks(np.arange(2))\n            axs[n].set_xticklabels([\"False\", \"True\"])\n            axs[n].set_yticklabels([\"False\", \"True\"])\n            axs[n].set_title(f\"Label {n}\\naccuracy: {accuracy:.3f}\\nv_score: {v:.3f}\")\n            for i in range(2):\n                for j in range(2):\n                    text = axs[n].text(j, i, cm[i, j],\n                                   ha=\"center\", va=\"center\", color=\"k\")\n            \n        plt.show() ","9b90bfad":"# separado\n# predictRFC(X_train, y_train, X_test, y_test, 0)","347ef8c1":"# agrupado\npredictRFC(X_train, y_train, X_test, y_test, 1)","101cedda":"import xgboost as xgb\n\ndef predictXGB(X_train, X_test, y_train, y_test, inteiro):\n    for lan in X_train.lang_abv.unique():\n            \n        xgboost = xgb.XGBClassifier(n_estimators=500, max_detph=2, scale_pos_weight=10)\n\n        if inteiro==0:\n            train, test, train_label, test_label = preprossecing1(X_train, y_train, X_test, y_test, lan)\n        else:\n            train, test, train_label, test_label = preprossecing2(X_train, y_train, X_test, y_test, lan)\n    \n        fig, axs = plt.subplots(1,3, figsize=(15,8))\n        fig.suptitle(f\"Language: {lan}\")\n        for n in [0,1,2]:\n            xgboost.fit(train, train_label==n)\n            pred = xgboost.predict(test)\n\n            accuracy = metrics.accuracy_score(pred, test_label==n)\n            v = metrics.v_measure_score(pred, test_label==n)\n            \n            cm = confusion_matrix(test_label==n, pred, normalize=\"true\")\n            axs[n].imshow(cm)\n            axs[n].set_xticks(np.arange(2))\n            axs[n].set_yticks(np.arange(2))\n            axs[n].set_xticklabels([\"False\", \"True\"])\n            axs[n].set_yticklabels([\"False\", \"True\"])\n            axs[n].set_title(f\"Label {n}\\naccuracy: {accuracy:.3f}\\nv_score: {v:.3f}\")\n            for i in range(2):\n                for j in range(2):\n                    text = axs[n].text(j, i, cm[i, j],\n                                   ha=\"center\", va=\"center\", color=\"k\")\n            \n        plt.show() ","716b91a1":"# separado\n# predictXGB(X_train, y_train, X_test, y_test, 0)","79997a6e":"# agrupado\npredictXGB(X_train, y_train, X_test, y_test, 1)","15270a98":"## Random Forest Classifier:","41339686":"## Importando o DataSet","132ca65d":"* ### Agrupando premissa e hip\u00f3tese por idioma","6b0bd5fc":"# Pre-Processamento\n---","b8b952f1":"## Verificando se h\u00e1 dados nulos ou vazios","147f6b5b":"## XGBoost:","3a0bc8f6":"# Data Exploration\n---","7b62430f":"* ### Separando premissas e hipoteses por idioma:","8120d649":"### OBS:\nAs labels no total s\u00e3o levemente desbalanceadas.Esse desbala\u00e7o est\u00e1 presente em todos os idiomas. ","41b1e8fb":"### OBS:\nAssim pode-se observar que o dataset \u00e9 majoritariamente composto por premissas e hip\u00f3teses na l\u00edngua inglesas enquanto que o restante das frases s\u00e3o igualmente distribu\u00eddas entre as outras 14 l\u00ednguas.","014e0735":"# Conclus\u00e3o\n---\n\n## Modelos\nApesar dos bons resultados de acur\u00e1cia do RandomForest, o resultado n\u00e3o \u00e9 confi\u00e1vel uma vez que o modelo gera a mesma resposta independente da label ou do idioma. J\u00e1 com o m\u00e9todo XGBosst, obteve-se resultado mais consistente com varia\u00e7\u00e3o das predi\u00e7\u00f5es em fun\u00e7\u00e3o tanto da label quanto do idioma.\n\n## Abordagem\nA aboradagem utilizada foi o 'bag of words', no qual considera-se apenas a frequ\u00eancia das palavras presentes, transformando textos em vetores. Entretanto, abordagens com 'word embeddings' aparentemente seriam melhores uma vez que ela considera o valor do textos, permintindo estabelecer compara\u00e7\u00f5es entre elas, o que, neste caso, seria uma \u00f3tima alternativa.","e41d2df0":"## Verificar quantas e quais as l\u00ednguas presentes DF","cc576f8f":"## Bag of Words:","4c60ab04":"## Relacionando a quantidade de labels em cada l\u00edngua","0bb58cbe":"# Modelo de Machine Learning e Valida\u00e7\u00e3o:\n---","18971bf2":"## Relacionando a quantidade de cada l\u00edngua dentro do DF"}}