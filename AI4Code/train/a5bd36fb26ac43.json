{"cell_type":{"e7e82928":"code","7d20c7a7":"code","9da29f6f":"code","ad568e28":"code","9597f02b":"code","c5387ce7":"code","802255bc":"code","175ed984":"code","805b6a3b":"code","063d7608":"code","9e38e40b":"markdown"},"source":{"e7e82928":"import numpy as np\nimport pandas as pd\nimport os\nimport spacy\nfrom spacy import displacy\nimport json","7d20c7a7":"#Upload all articles\narticles = []\nfor dirpath, subdirs, files in os.walk('\/kaggle\/input'):\n    for x in files:\n        if x.endswith(\".json\"):\n            with open(os.path.join(dirpath, x)) as json_file:\n                data = json.load(json_file)\n                articles.append(data)\nprint (len(articles))\n#29315","9da29f6f":"#Extract relevant paragraphs matching keyword\nkeyword = \"risk factor\" #lower case !!!\n\nparagraphs = []\n\nfor a in articles:\n    for p in a[\"body_text\"]:\n        if keyword in p[\"text\"]:\n            paragraphs.append(p[\"text\"])\n            \nprint(len(paragraphs))  \n#9463","ad568e28":"#Perform NLP processing\nnlp = spacy.load(\"en_core_web_sm\")\ndocs = list(nlp.pipe(paragraphs))","9597f02b":"#Define function to extract relations.\n#Check REF-01 for an analysis on the most dep for the token matching the keyword \n\ndef extract_relations(doc,keyword):\n\n    #Consolidate spans\n    spans = list(doc.ents) + list(doc.noun_chunks)\n    spans = spacy.util.filter_spans(spans)\n    with doc.retokenize() as retokenizer:\n        for span in spans:\n            retokenizer.merge(span)  \n    \n    relations = []\n    \n    #For each sentence in paragraph\n    for sent in doc.sents:\n        #For each token in the sentence containing the \"keyword\" i.e. hits       \n        for t in sent:\n            if keyword in t.text.lower():\n                if t.dep_ == \"attr\":\n                    n1 = [w for w in t.head.children if (w.dep_ == \"nsubj\")]\n                    n2 = t\n                    if len(n1) > 0:\n                        n1 = n1[0]\n                        n1_subtree_span = doc[n1.left_edge.i : n1.right_edge.i + 1]\n                        n2_subtree_span = doc[n2.left_edge.i : n2.right_edge.i + 1]\n                        relations.append([n1_subtree_span,n2_subtree_span,n1,n2,sent,\"attr\"])   \n                if t.dep_ == \"pobj\" and t.head.dep_ == \"prep\":\n                    n1 = [w for w in t.head.head.children if (w.dep_ == \"nsubjpass\")]\n                    n2 = t\n                    if len(n1) > 0:\n                        n1 = n1[0]\n                        n1_subtree_span = doc[n1.left_edge.i : n1.right_edge.i + 1]\n                        n2_subtree_span = doc[n2.left_edge.i : n2.right_edge.i + 1]\n                        relations.append([n1_subtree_span,n2_subtree_span,n1,n2,sent,\"pobj\"])  \n    return relations","c5387ce7":"#Extract all risk factors -> cleaning needed\nrisk_factors_raw=[]\n\nfor doc in docs:\n    hit = extract_relations(doc,keyword)\n    if len(hit) > 0:\n        for h in hit:\n            risk_factors_raw.append(h)\n\nprint(len(risk_factors_raw))\n#2654","802255bc":"#Build clean risk_factors list\n\nrisk_factors = []\nexclusion_list = [\"WDT\",\"PRP\",\"CD\",\"DT\",\"WP\"] #Check REF-02 and REF-03 to understand exclusion list\n\nfor r in risk_factors_raw:\n    if r[2].tag_ not in exclusion_list:\n        risk_factors.append([r[0],r[4],r[5]])\n        \nprint(len(risk_factors))\n#count 2473","175ed984":"#Export to csv\noutcome=pd.DataFrame(risk_factors,columns=[\"Risk_factor\",\"Sentence\",\"type\"])\noutcome.to_csv(\"Risk_factors.csv\")","805b6a3b":"#Visualise selected type of deps -> att\nplt=outcome[outcome[\"type\"]==\"attr\"][\"Sentence\"].to_list()[1]\n#displacy.serve(plt, style=\"dep\")","063d7608":"#Visualise selected type of deps -> pobj\nplt=outcome[outcome[\"type\"]==\"pobj\"][\"Sentence\"].to_list()[1]\n#displacy.serve(plt, style=\"dep\")","9e38e40b":"### Appendix\n\n**REF-01**\nCheck most common dep for the tokens matching the keyword\n```python\ns=[]\nfor doc in docs:\n    for sent in doc.sents:\n        for t in sent:\n            if keyword in t.text:\n                s.append({\"t_dep\":t.dep_,\"t_head_dep\":t.head.dep_,\"t_head_head_dep\":t.head.head.tag_,\"sent\":sent})\n\ndf = pd.DataFrame(s)  \nCheck most common dep for the token matching the keyword\n\ndf.groupby([\"t_dep\"]).size().to_frame()\n```\nMost common dep are \"attr\" and \"pobj\". In the relation extraction function we will address these types as 1st priority\n\n**REF-02**\nAnalyse tag associated to the n1 and exclude unwanted values. Case 1 -> \"attr\n```python\nfor r in risk_factors_raw:\n    if r[5]== \"attr\" and r[2].tag_ not in [\"NN\",\"NNS\",\"NNP\",\"NNPS\",\"VBG\",\"VB\"]:\n        print(r[2].tag_ + \"--\" + r[2].text)\n```        \nTO EXCLUDE --> and r[2].tag_ not in [\"WDT\",\"PRP\",\"CD\",\"DT\",\"WP\"] \n\n**REF-03**\nAnalyse tag associated to the n1 and exclude unwanted values. Case 1 -> \"pobj\n```python\nfor r in risk_factors_raw:\n    if r[5]== \"pobj\" and r[2].tag_ not in [\"NN\",\"NNS\",\"NNP\",\"NNPS\",\"VBG\",\"VB\"] :\n        print(r[2].tag_ + \"--\" + r[2].text)\n```        \nTO EXCLUDE --> and r[2].tag_ not in [\"WDT\",\"PRP\",\"CD\",\"DT\",\"WP\"] "}}