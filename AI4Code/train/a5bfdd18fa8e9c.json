{"cell_type":{"df8e5456":"code","4a78aae3":"code","79256cd8":"code","50792eda":"code","ec871349":"code","09068c92":"code","f958a7eb":"code","051f049e":"code","0a4f9286":"code","653fd0f8":"code","dfb4c79f":"code","806e708c":"code","cb42ea51":"code","f48fef62":"code","c53376c3":"code","d42dd693":"code","955082df":"code","f1e8ede5":"code","67e47d96":"code","58fea1dd":"code","1ac6ec1d":"code","514f13a8":"code","38d7b89b":"code","fdb399e3":"code","87c26d00":"code","3952e31b":"code","517a0872":"code","5e3d7860":"code","2b153c23":"code","eceedd52":"code","ef206e0c":"code","13559b28":"code","f7aa58b5":"code","98bb3ab2":"code","7955c810":"code","6edc6774":"code","ae7f29cc":"code","c5a8703a":"code","fc7790a1":"markdown","808ce353":"markdown","7d9bf81b":"markdown"},"source":{"df8e5456":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4a78aae3":"import matplotlib.pyplot as plt #To plot the relation between dependent and independent variables.\nimport seaborn as sns","79256cd8":"df=pd.read_csv('\/kaggle\/input\/vehicle-dataset-from-cardekho\/car data.csv')\ndf.head()","50792eda":"df.describe()","ec871349":"df.info()","09068c92":"cat_features=[features for features in df.columns if len(df[features].unique())<=16 ]\ncat_features","f958a7eb":"num_features=[features for features in df.columns if len(df[features].unique())>16]\nnum_features","051f049e":"for features in num_features:\n    if features not in ['Car_Name']:\n        sns.boxplot(df[features], orient='v')\n        plt.show()","0a4f9286":"for features in cat_features:\n    df[features].value_counts(normalize=True).sort_values(ascending=False).plot.bar()\n    plt.xlabel(features)\n    plt.ylabel('Numbers in each category')\n    plt.show()","653fd0f8":"for features in cat_features:\n    df.groupby(features)['Selling_Price'].median().sort_values(ascending=False).plot.bar()\n    plt.ylabel('Selling Price')\n    plt.xlabel(features)\n    plt.show()","dfb4c79f":"corrmatrix = df.corr()\ntop_corr_features = corrmatrix.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","806e708c":"for features in num_features:\n    if features != 'Car_Name':\n        sns.distplot(df[features])\n        plt.show()","cb42ea51":"for features in cat_features:\n    for features2 in cat_features:\n        if features != 'Year' and features2!='Year':\n            sns.barplot(x=df[features], y=df['Selling_Price'], data=df, hue=df[features2])\n            plt.title('Relation between '+ features + '(at x-axis) and Selling Price at y-axis with hue= '+ features2)\n            plt.legend(loc='upper right')\n            plt.show()","f48fef62":"for features in num_features:\n    if features not in ['Car_Name','Selling_Price']:\n        plt.scatter(df[features],df['Selling_Price'])\n        plt.title('Relation between ' + features + ' and Selling_Price')\n        plt.ylabel('Selling_Price')\n        plt.xlabel(features)\n        plt.show()","c53376c3":"dfc=df.copy()","d42dd693":"dfc['current_year']=2020\ndfc.loc[:,'Year']=dfc['current_year']-dfc.loc[:,'Year']\ndfc","955082df":"dfc.drop(['Car_Name','Year','current_year'], axis=1, inplace=True)\ndfc","f1e8ede5":"dfc=pd.get_dummies(dfc, drop_first=True )\ndfc","67e47d96":"X=dfc.iloc[:,1:]\nY=dfc.iloc[:,0]","58fea1dd":"print('X shape:{} , Y shape:{}'.format(X.shape,Y.shape))","1ac6ec1d":"from sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR, LinearSVR\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_squared_log_error","514f13a8":"X_Train, X_Test, Y_Train, Y_Test= train_test_split(X, Y, test_size=0.33)\ndata_list=[[X_Train, Y_Train, X_Test, Y_Test]]\n\nprint('Shape of X_train: ' + str(X_Train.shape))\nprint('Shape of Y_train: ' + str(Y_Train.shape))\nprint('Shape of X_test:' + str(X_Test.shape))\nprint('Shape of Y_test:' + str(Y_Test.shape))","38d7b89b":"LR=LinearRegression()\nRFR=RandomForestRegressor()\nDTR=DecisionTreeRegressor()\nSVRE=SVR()\nMLPR=MLPRegressor()\nLO=Lasso()\nGBR=GradientBoostingRegressor()\nmodel_list=[LR,RFR,DTR,SVRE,MLPR,LO,GBR]","fdb399e3":"i=-1\npred=[]\nscore_train=[]\nscore_test=[]\ncv_score_train=[]\ncv_score_test=[]\nMSE=[]\nmodel_name=['LR','RFR','DTR','SVRE','MLPR','Lasso','GBR']\nscore_name=['noise','train score','test score','cv train score','cv test score','MSE']\nprint('WITHOUT HYPERPARAMETER TUNING')\nfor model in model_list:\n    i+=1\n    for data in data_list:\n        model.fit(data[0],data[1])\n        pred.append((model.predict(data[2])))\n        score_train.append(model.score(data[0],data[1]))\n        score_test.append(model.score(data[2],data[3]))\n        cv_score_train.append(np.mean(cross_val_score(model,data[0],data[1], cv=5)))\n        cv_score_test.append(np.mean(cross_val_score(model,data[2],data[3], cv=5)))\n        MSE.append(mean_squared_error(pred[i], data[3], squared=False))\n        \n#         print(model_name[i] +  ' has train score :' + str(np.round(score_train[i],3)) + ' , test score : ' + str(np.round(score_test[i],3))+ ',cv train score: '+ str(np.round(cv_score_train[i],3))+', cv test score: '+ str(np.round(cv_score_test[i],3)) + ', MSE:'+str(np.round(MSE[i],3)))\n\npd.DataFrame(data=[pred,score_train,score_test,cv_score_train,cv_score_test,MSE], columns=model_name , index=score_name).drop('noise')","87c26d00":"Rfr=RandomForestRegressor()\nparam={'n_estimators':[3,5,50,200,400,800],'max_depth':[2,4,8,16,32,64,None]}\ncv=GridSearchCV(Rfr, param, cv=5, n_jobs=-1)","3952e31b":"cv.fit(X_Train,Y_Train)","517a0872":"print('Best params are {}'.format(cv.best_params_))\nmeanz=cv.cv_results_['mean_test_score']\nstdz=cv.cv_results_['std_test_score']\npaz=cv.cv_results_['params']\nfor mean,std,par in zip(meanz,stdz,paz):\n    print( str(mean)+ '        '+ str(std)+ '        ' +str(par) +'\\n' )","5e3d7860":"model_rfr=cv.best_estimator_\n","2b153c23":"print('Train Score:{} and Test score:{}'.format(model_rfr.score(X_Train,Y_Train),model_rfr.score(X_Test,Y_Test)))","eceedd52":"pred_rfr=model_rfr.predict(X_Test)","ef206e0c":"plt.scatter(Y_Test,pred_rfr)","13559b28":"sns.distplot(Y_Test-pred_rfr)","f7aa58b5":"parms={'n_estimators':[50,100,200,500,1000],'max_depth':[5,10,15],'learning_rate':[0.05,0.1,0.5]}\ncv=GridSearchCV(GBR,parms,cv=5)\ncv.fit(X_Train, Y_Train)","98bb3ab2":"print('Best params are {}'.format(cv.best_params_))\nmeanz=cv.cv_results_['mean_test_score']\nstdz=cv.cv_results_['std_test_score']\npaz=cv.cv_results_['params']\nfor mean,std,par in zip(meanz,stdz,paz):\n    print( str(mean)+ '        '+ str(std)+ '        ' +str(par) +'\\n' )","7955c810":"model_gbr=cv.best_estimator_\nprint('Train Score:{} and Test score:{}'.format(model_gbr.score(X_Train,Y_Train),model_gbr.score(X_Test,Y_Test)))","6edc6774":"pred_gbr=model_rfr.predict(X_Test)","ae7f29cc":"plt.scatter(Y_Test,pred_gbr)","c5a8703a":"sns.distplot(Y_Test-pred_gbr)","fc7790a1":"HyperParameter tuning","808ce353":"There are no Null values present in the dataset.","7d9bf81b":"Random forest regressor shows better results than Gradient boosting.\n"}}