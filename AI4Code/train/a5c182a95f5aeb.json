{"cell_type":{"1a397337":"code","5ed1d44b":"code","664a361e":"code","9d3728da":"code","00c1c8d3":"code","a70f34b9":"code","5f419c4d":"code","2fe6e277":"code","4bec960b":"code","ffb0936a":"code","9b13b968":"code","3bef0a57":"code","f2e2f552":"code","d102f3a4":"code","b13c3938":"code","4c3b6bf3":"code","966e2e71":"code","03fadabd":"code","391a64d9":"code","3e6a2e4d":"code","2677e587":"code","7394a431":"code","63f5112f":"code","87b8aa23":"code","26635e7d":"code","ed7c77e8":"code","3fdcfeed":"code","f7d36375":"code","f8567ec4":"code","fcba15ef":"code","54ed86bc":"code","ac905dbb":"code","3c488a6d":"code","bc3850cf":"code","b823b4f6":"code","18905806":"code","f8effe79":"code","b37a1a1e":"code","e804c349":"code","cb96285b":"code","29f5a602":"code","04f66c0a":"code","0da11661":"code","8777847f":"code","d3cd35c2":"code","9513f55f":"code","928138bb":"code","7cfc8a07":"code","290b9079":"code","a22d28c3":"code","b28f882a":"code","8ed3b819":"code","7c81563e":"code","9aed6dab":"code","1ae98c4a":"code","ae760ab6":"code","dd6b954b":"code","cb1f5873":"code","1312f5da":"code","49477501":"code","38cdadcb":"code","4f78fc5e":"code","98e4261f":"code","e7574d0e":"code","565fa672":"code","34fd3cf3":"code","4e17e0ee":"code","db2a62fc":"code","d13e3e9f":"markdown","7e95664d":"markdown","1dd5b545":"markdown","0de6c56c":"markdown","3126b938":"markdown","c914f8f6":"markdown","db2ff5e5":"markdown","53ec3162":"markdown","dbe43b11":"markdown","04f49245":"markdown","49671358":"markdown","7aafc6f1":"markdown","f0fb60d4":"markdown","310b2884":"markdown","a063e084":"markdown","94fc67e8":"markdown","a2747975":"markdown","0da82cf4":"markdown","b41e9b4f":"markdown","b8e4ab54":"markdown","8e77f5e6":"markdown","aed8eee7":"markdown","4c552a0e":"markdown","a84f46a4":"markdown","db4bc110":"markdown","ad8e4d71":"markdown","5b0b8a0e":"markdown","8ea79db9":"markdown","637dfa91":"markdown","a19a1dd9":"markdown","95080baf":"markdown","bf8d21de":"markdown","f5995357":"markdown","9ca3e0a7":"markdown","702475d3":"markdown","c377858a":"markdown","d26886e5":"markdown","9f2c5d2e":"markdown","7e9a5e6b":"markdown","e1bb725b":"markdown","31f355d1":"markdown","12f079ff":"markdown","193befe7":"markdown","19fb9e0f":"markdown","7de73fb1":"markdown","d7914fad":"markdown","f7b348ee":"markdown"},"source":{"1a397337":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","5ed1d44b":"df = pd.read_csv('\/kaggle\/input\/nyc-property-sales\/nyc-rolling-sales.csv', index_col = 0)","664a361e":"df.info()","9d3728da":"df.isna().sum()","00c1c8d3":"categorical = df.select_dtypes(include=['object'])\ncategorical.head().transpose() # Transposing make visualization easier for big datasets","a70f34b9":"categorical.describe().transpose()","5f419c4d":"df['SALE PRICE'] = df['SALE PRICE'].apply(lambda s: int(s) if not \"-\" in s else 0)\ndf['LAND SQUARE FEET'] = df['LAND SQUARE FEET'].apply(lambda s: int(s) if not '-' in s else 0)\ndf['GROSS SQUARE FEET'] = df['GROSS SQUARE FEET'].apply(lambda s: int(s) if not '-' in s else 0)\ndf['SALE PRICE'] = df['SALE PRICE'].apply(lambda s: s if type(s) == int else 0)","2fe6e277":"df['SALE DATE'] = pd.to_datetime(df['SALE DATE'], format= '%Y-%m-%d %H:%M:%S')","4bec960b":"df.drop([\"EASE-MENT\", \"ADDRESS\", \"APARTMENT NUMBER\"], axis = 1, inplace = True)","ffb0936a":"numerical = df.select_dtypes(exclude=['object'])\nnumerical.info()","9b13b968":"numerical.describe().transpose()","3bef0a57":"weird_zeros_cols = [\n    \"ZIP CODE\",\n    \"GROSS SQUARE FEET\",\n    \"YEAR BUILT\",\n    \"SALE PRICE\"\n   ]\n\nl = len(df)\nfor col in weird_zeros_cols:\n    print(f\"{col:.10}\\t{len(df[df[col] == 0])\/l:0.2f}% missing\")","f2e2f552":"for col in weird_zeros_cols:\n    df = df[df[col] != 0]","d102f3a4":"categorical = df.select_dtypes(include=['object'])\ncategorical.describe().transpose()","b13c3938":"sns.countplot(\n    x=\"TAX CLASS AT PRESENT\",\n    data = df,\n    order = df[\"TAX CLASS AT PRESENT\"].value_counts().index,\n)\nplt.show()","4c3b6bf3":"pivot = df.pivot_table(index='TAX CLASS AT PRESENT',\n                       values='SALE PRICE',\n                       aggfunc=np.sum,).sort_values(\"SALE PRICE\")\n\npivot.plot(\n    kind='bar',\n    color='orange',\n    title=\"Total Sale Price per Tax Class\"\n)","966e2e71":"pivot = df.pivot_table(index='TAX CLASS AT PRESENT',\n                       values='SALE PRICE',\n                       aggfunc=np.mean).sort_values(\"SALE PRICE\")\npivot.plot(kind='bar',\n           color='black',\n           title=\"Average Price per Tax Class\"\n          )","03fadabd":"g = sns.countplot(\n    x='BUILDING CLASS CATEGORY',\n    data = df,\n    order = df[\"BUILDING CLASS CATEGORY\"].value_counts().index,\n)\ng.set_yscale('log')\ng.set_xticklabels(g.get_xticklabels(), rotation = 90)\nplt.show()","391a64d9":"pivot = df.pivot_table(index='BUILDING CLASS CATEGORY',\n                       values='SALE PRICE',\n                       aggfunc=np.sum).sort_values(\"SALE PRICE\")\npivot.plot(kind='bar', color = 'green')","3e6a2e4d":"df['BUILDING CLASS CATEGORY'].value_counts().head(6)","2677e587":"top_vals = df['BUILDING CLASS CATEGORY'].value_counts().index[:5]\ndf = df[df[\"BUILDING CLASS CATEGORY\"].isin(top_vals)]","7394a431":"numerical = df.select_dtypes(exclude=['object', 'datetime'])\nnumerical.describe().transpose()","63f5112f":"sns.heatmap(numerical.corr()) #, annot= True)","87b8aa23":"df.drop([\"RESIDENTIAL UNITS\", \"LAND SQUARE FEET\"], axis = 1, inplace = True)","26635e7d":"plt.hist(\n    df[\"SALE PRICE\"],\n    bins = 20,\n    log = True,\n    rwidth = 0.8\n)\nplt.show()","ed7c77e8":"df = df[(df['SALE PRICE'] < 5e8) & (df['SALE PRICE'] > 1e5)]","3fdcfeed":"# Plot histogram.\nn, bins, patches = plt.hist(\n    df[\"SALE PRICE\"],\n    bins = 20,\n    log = True,\n    rwidth = 0.8\n) \nbin_centers = 0.5 * (bins[:-1] + bins[1:])\n\n# scale values to interval [0,1]\ncol = bin_centers - min(bin_centers)\ncol \/= max(col)\n\ncm = plt.cm.get_cmap('plasma')\nfor c, p in zip(col, patches):\n    plt.setp(p, 'facecolor', cm(c))\n","f7d36375":"n, bins, patches = plt.hist(\n    df[\"SALE PRICE\"],\n    bins = 20,\n    log = True,\n    rwidth = 0.8\n) \n\nbin_centers = 0.5 * (bins[:-1] + bins[1:])\n\nsorted_patches = [p for _,p in sorted(zip(n,patches), key=lambda pair: pair[0])] #sorts patches in respect to n\nsorted_centers = [c for _,c in sorted(zip(n, bin_centers), key=lambda pair: pair[0])] #sorts bin_centers in respect to n\n\n# scale values to interval [0,1]\ncol = sorted_centers - min(sorted_centers)\ncol \/= max(col)\ncol = sorted(col)\ncm = plt.cm.get_cmap('plasma')\nfor c, p in zip(col, sorted_patches):\n    plt.setp(p, 'facecolor', cm(c))\n\nplt.show()","f8567ec4":"df[\"SALE PRICE\"].skew()","fcba15ef":"price = np.log(df[\"SALE PRICE\"])\nprint(price.skew())","54ed86bc":"plt.hist(price, bins =20)\nplt.show()","ac905dbb":"df.groupby(\"BOROUGH\").mean()['SALE PRICE'].plot(kind='bar') # An alternative to pivot tables","3c488a6d":"df['TOTAL UNITS'].value_counts()","bc3850cf":"sns.countplot(x='TOTAL UNITS', data = df, log=True)\nplt.show()","b823b4f6":"df = df[df['TOTAL UNITS'] < 50]","18905806":"df['TAX CLASS AT TIME OF SALE'].value_counts()","f8effe79":"df['BOROUGH'].value_counts()","b37a1a1e":"plt.hist(df['SALE DATE'], bins=20)\nplt.show()","e804c349":"sns.countplot(df[\"SALE DATE\"].dt.dayofweek)","cb96285b":"df['day'] = df[\"SALE DATE\"].dt.dayofweek\ndf = df[df[\"day\"] < 5 ]\ndf.drop([\"day\"], axis =1, inplace = True)","29f5a602":"sns.countplot(df[\"SALE DATE\"].dt.day)","04f66c0a":"month = np.empty(5 * 7)\nfor day, count in df[\"SALE DATE\"].dt.day.value_counts().iteritems():\n    month[int(day) -1] = count\nmonth = month.reshape((5,7))\nsns.heatmap(month)","0da11661":"import joypy\nmonth_df = pd.DataFrame(month)\njoypy.joyplot(month_df, overlap=2, colormap=plt.cm.OrRd_r, linecolor='w', linewidth=.5)\nplt.show()","8777847f":"axes = df[\"SALE PRICE\"].plot(\n    marker='.',\n    alpha=0.5,\n    linestyle='',\n    figsize=(11, 9),\n    subplots=True\n)","d3cd35c2":"df.info()","9513f55f":"df.drop([\n    \"BUILDING CLASS AT PRESENT\",\n    \"BUILDING CLASS AT TIME OF SALE\",\n    \"NEIGHBORHOOD\",\n    'TAX CLASS AT PRESENT'\n], axis = 1, inplace = True)","928138bb":"df['TAX CLASS AT TIME OF SALE'].value_counts()","7cfc8a07":"df[\"SALE DATE\"] = pd.to_numeric(df[\"SALE DATE\"])","290b9079":"numeric_cols = df.select_dtypes(exclude=['object']).columns.tolist()\nnumeric_cols.remove(\"SALE DATE\")\n\n# Removing exceding skewness from features\nfor col in numeric_cols:\n    if df[col].skew() > 1:\n        df[col] = np.log1p(df[col])","a22d28c3":"one_hot = ['BUILDING CLASS CATEGORY','TAX CLASS AT TIME OF SALE']\ndummies = pd.get_dummies(df[one_hot])\ndummies = pd.concat([dummies, pd.get_dummies(df[\"BOROUGH\"])], axis=1) #BOROUGH are integers, so we need to do it seperately\ndummies.info(verbose=True, memory_usage=True) #Its nice to check how much memory the dummies will require","b28f882a":"df.drop(['BUILDING CLASS CATEGORY', 'TAX CLASS AT TIME OF SALE', 'BOROUGH'], axis = 1, inplace = True)\ndf = pd.concat([df, dummies], axis =1)\ndf.info()","8ed3b819":"from sklearn.model_selection import train_test_split\n\nfeatures = df.drop([\"SALE PRICE\"], axis = 1)\ntarget = df[\"SALE PRICE\"]\n\nx_train, x_test, y_train, y_test = train_test_split(features, target, test_size = 0.3)","7c81563e":"# Outlier detection\nfrom sklearn.ensemble import IsolationForest\nclf = IsolationForest(max_samples = 1000)\nclf.fit(x_train)\n\noutliers = clf.predict(x_train)\nnp.unique(outliers, return_counts = True)","9aed6dab":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nlr = LinearRegression(normalize = True)\nlr.fit(x_train, y_train)\ny_pred = lr.predict(x_test)\nprint(lr.score(x_test, y_test))\nprint(mean_squared_error(y_test, y_pred) ** 0.5)","1ae98c4a":"sns.residplot(y_test, y_pred, color=\"orange\", scatter_kws={\"s\": 3})","ae760ab6":"from sklearn.linear_model import Lars\nlars = Lars()\nlars.fit(x_train, y_train)\ny_pred = lars.predict(x_test)\nprint(lars.score(x_test, y_test))\nprint(mean_squared_error(y_test, y_pred))","dd6b954b":"sns.residplot(y_test, y_pred, color=\"g\", scatter_kws={\"s\": 3})","cb1f5873":"from sklearn.tree import DecisionTreeRegressor\ndtr = DecisionTreeRegressor(max_depth = 100)\ndtr.fit(x_train, y_train)\ny_pred = dtr.predict(x_test)\nprint(dtr.score(x_test,y_test))\nprint(mean_squared_error(y_test, y_pred))","1312f5da":"sns.residplot(y_test, y_pred, color=\"g\", scatter_kws={\"s\": 3})","49477501":"from sklearn.ensemble import AdaBoostRegressor\nadar = AdaBoostRegressor(DecisionTreeRegressor(max_depth=10), n_estimators = 600)\nadar.fit(x_train, y_train)\ny_pred = adar.predict(x_test)\nadar.score(x_train, y_train)","38cdadcb":"sns.residplot(y_test, y_pred, color=\"purple\", scatter_kws={\"s\": 3}) #The best result is plotted in royal purple","4f78fc5e":"fig, ax = plt.subplots(1)\nax.scatter(y_test, y_pred, s=2)\nax.plot([min(y_test.to_list()), max(y_test.to_list())], [min(y_test.to_list()), max(y_test.to_list())], ls='--', c='black', lw=2)\nplt.show()","98e4261f":"from sklearn.ensemble import GradientBoostingRegressor\nclf = GradientBoostingRegressor(n_estimators = 100, learning_rate = 0.05, max_depth = 1, loss='ls')\nclf.fit(x_train, y_train)\ny_pred = clf.predict(x_test)\nprint(clf.score(x_test, y_test))\nprint(mean_squared_error(y_test, y_pred))","e7574d0e":"sns.residplot(y_test, y_pred, color=\"blue\", scatter_kws={\"s\": 3})","565fa672":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\nlreg = LinearRegression(normalize = True)\ny_pred = cross_val_predict(lreg, x_test, y_test, cv=50)\nsns.residplot(y_test, y_pred, color=\"pink\", scatter_kws={\"s\": 3})\nprint(r2_score(y_test, y_pred))\nprint(mean_squared_error(y_test, y_pred))","34fd3cf3":"from sklearn.decomposition import PCA\npca = PCA(n_components = 2)\npca.fit(features)\nfeat_pca = pca.transform(features)","4e17e0ee":"fig, ax = plt.subplots(1, figsize=(15,7))\nax.scatter(feat_pca[:,0], feat_pca[:,1], cmap='plasma', c= target, alpha = 0.5, s=2)\n#ax.set_ylim(-1e1, 3e1)\nplt.show()","db2a62fc":"pca = PCA(n_components = 3)\npca.fit(features)\nfeat_pca = pca.transform(features)\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(feat_pca[:,0], feat_pca[:,1],feat_pca[:,2],cmap='plasma', c = target, alpha = 0.5)\nax.view_init(30,30)","d13e3e9f":"<h1>Models<\/h1>\nThere are just too many models one could use to make a regression. I tried a lot of then and I picked the best performing.","7e95664d":"<h2> Libraries and data importation <\/h2>","1dd5b545":"<h1>Exploratory Data Analysis<\/h1>\nThere are simply too many things to explore in each data set. I like to see wich categories hold most observations, plot the relationship of the features to the target and look for outliers.","0de6c56c":"The color is used to test if prices are regular.","3126b938":"The Tax Class ad Borough could be used as categorical data, as a borough of 5 is not 'larger' that a borough of 1, they are just categories.","c914f8f6":"<h3>Categorical features <\/h3>\nDealing with categorical data is usualy the hardest part of treating any data set, so lets start by it.","db2ff5e5":"<h4> Gradient Boosting <\/h4>\nAnother boosting method","53ec3162":"<h1> Further analysis <\/h1>\nUsually the dataset must be segmented the quality of each regression. This could be done with WOE binning or using a very simple decision tree. With this segmentation we could explore some of the features that we left out such as APARTMENT NUMBER or make a separate analysis for luxury properties, commercial and residential,etc..\n \n# Thanks for reading!! ","dbe43b11":"Lets keep just the main values for TOTAL UNITS to simplify our model","04f49245":"This features is very skewed, in numbers:","49671358":"I want to make a fancy data visualization out of this (just for the sake of doing it).\n\nSeaborn has a nice FacedGrid method that allows for ridge plots, but I find it quite non intuitive, and prefer to use the joyplot library because I find it more intuitive and has a syntax similar to matplotlib. A nice challege would be to implement this with pure matplotlib functions.\n","7aafc6f1":"It's clear that we have some outliers. Lets remove then.","f0fb60d4":"The EASE-MENT column has no data in it so we will just drop it from the dataframe.\n\nThe APPARTMENT NUMBER column is 50% empty. One could use it to split the data in two groups, since the appartment number might hold a very important information: the building floor. For now we will drop it to avoid over complicating things.\n\nThe ADDRESS columns could be used for geographical data analysis, but doing such analysis is not in the scope of this project. It also seems retundant, as the ZIP code should hold the same information.","310b2884":"Some things stand out already. 'EASE-MENT' and 'APARTMENT NUMBER' seem to have empty entries. 'SALE PRICE', our target feature, also has problems as some observations have a '-'. \n\nA quick summary of the statistics of the dataset can be obtained as follows","a063e084":"Its always in good practice to remove skwness from the numerical data and scale it (its not mandatory, but since some of our models will be simetric, removing the skewness from all features should help quite a lot)","94fc67e8":"<h1> Cleaning and regularizing the data set<\/h1>\n    <p> Firstly we will see the overall aspects of the data. Identify (and solve) possible issues. The target feature in this case is quite easy to indentify (SALE VALUE), but in many real case scenarios this might pose a challenge.<\/p>","a2747975":" The colors in the last histogram scale with the x-axis, but we could scale the color by the y axis. This can be achived by odering the patches by desceding (or ascending) order of counts in each bin (given by the 'n' list) ","0da82cf4":"<h3>Numerical features<\/h3>","b41e9b4f":"Some of this data just don't make any sense. 0 values are unaceptable values as ZIP CODE, TOTAL UNITS, GROSS SQUARE FEET and YEAR BUILT.\n\nOur target feature SALE PRICE also have some 0 values. We will just eliminate these observations, but there are some other options. It's possible to use these as the test set (might not be a good idea as this subset can be different from the rest i.e. comparing sold to non sold properties). One could also try to make a classification analisys to decide if a new property will be sold or not.\n\nDepending on the ammount of missing data we might want to delete the observations from the dataframe (as deleting few observation won't affect the statistics) or the entire feature (if most of the data is missing and deleting the observations it would cause severe information loss). Be aware that a feature can be VERY important, and sometimes its worth loosing half your data to keep it. I think that this is the case for the GROSS SQUARE feature, as this is a very important factor on the price of a property. This will drastically reduce the amount of availabe observations.\n\nYet another possibility is to use data imputation techniques (e.g. filling missing observations with the overall average), but I personaly don't like it as it feels wrong filling the target feature \"by hand\".","b8e4ab54":"As a last visualization, the time series itself.","8e77f5e6":"<hr>","aed8eee7":"<h4>LARS<\/h4>","4c552a0e":"<h4>Decision Tree Regression<\/h4>","a84f46a4":"We see a huge correlation between LAND SQUARE FEET and GROSS SQUARE FEET and between RESIDENTIAL UNITS and TOTAL UNITS. Correlated features in general don't improve models so we will keep only one of these features.","db4bc110":"The final step is to separate the train and test data samples","ad8e4d71":"<h4> Linear Regression <\/h4>","5b0b8a0e":"If you want to make a pretier plot (for a executive report for example), it would be interesting to add colors to the histogram. It is good to remember that this is just \"perfumery\", as adding colors won't add any new information to the plot and might just confuse the reader in most casses (as people will try to undertand the meaning of the colors, wich they don't have)","8ea79db9":"<h3>Outlier detection with Isolation Forests<\/h3>","637dfa91":"<h4>Cross validation<\/h4>\nCross validation is a very powerful and simple technique that can be used to improve the overall quality of a regression.","a19a1dd9":"It is important to look at the residuals (or reduced residuals) to check if there are any patterns","95080baf":"To reduce the complexity of our model, lets grab only the most representative BUIDING CLASS CATEGORY","bf8d21de":"It is interesting to look at the weekly and monthly sales","f5995357":"Transforming temporal data to float","9ca3e0a7":"<h4>AdaBoost Regression<\/h4>\nThis was the best performing method, and its also the slowest to train.","702475d3":"Now we encode all categorical features. This can be done in two ways (mainly): One-hot encoding and integer encoding. One-hot is best used when there are only a few unique values of a given feature, as it creates a new column per unique value (i.e. True or False for each value). If we have too many unique values (and a big data set) integer encoding becomes a possibility.","c377858a":"Its always nice to check for NaNs immidiatelly. In this case, as we will see, they are not propper empty observations but '-' or whitespaces, making cleaning a bit harder","d26886e5":"As we will try to fit a simetrical model to the data, it is interesting to remove this skewness. After we explore our numerical features we will remove the skewness from all of then","9f2c5d2e":"<center><h1> NYC Property Sales<\/h1><\/center>\nThis is a data set about property sales in New York City. It's a nice dataset to train our regression skills. In this analysis we will explore some important topics like\n<ul>\n    <li> Linear Regression <\/li>\n    <li> Lasso, Ridge and elasticNet regressions<\/li>\n    <li> Least Angle Regression (LARS) <\/li>\n    <li> Decision Tree Regression <\/li>\n    <li> AdaBoost Regressor <\/li>\n    <li> Gradient Boosting <\/li>\n    <li> Cros Validation <\/li>\n    <li> Principle Component Analysis (PCA)<\/li>\n    <li> Data encoding (integer encoding and one-hot encoding)<\/li>\n    <li> Outlier detection with Isolation forests<\/li>\n<\/ul>\nand the usual exploratory data analysis and data visualization.\n<hr>\n<hr>","7e9a5e6b":"Some of these features should be numeric, so lets convert then. Some have '-' and blackspaces that we need to fix","e1bb725b":"We can plot the monthly sales in the shape of an actual calendar","31f355d1":"The SALE DATE column should be converted to a propper datetime format.","12f079ff":"<h3>Temporal data<\/h3>\nThe only temporal feature is SALE DATE. Lets check structure of the time series","193befe7":"<h3>PCA and Scaling<\/h3>\nAs a closing note we will discuss briefly PCA as it is a very interesting approach to dimensionality reduction.","19fb9e0f":"This is a very interesting result. It sugests that there are internal structures that could be used to make a segmentation of the data set, obtaining better regressions","7de73fb1":"To reduce he dimensionality, we're going to delete some features that feel redundant.\n\nBUIDING CLASS AT PRESENT and BUILDING CLASS AT TIME OF SALE feel redundant with CUILDING CLASS CATEGORY, so we will keep only the latter.\n\nAll \"geological\" information should be in the ZIP CODE, so we will drop all other related features.\n\nWe will chosse only one of TAX CLASS AT PRESENT and TAX CLASS AT TIME OF SALE","d7914fad":"<h1>Data Preparation<\/h1>\nPreperaing the data for regression","f7b348ee":"<h3>Numerical data<\/h3>"}}