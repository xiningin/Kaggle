{"cell_type":{"4c8c8689":"code","9c65cc66":"code","0b5541ef":"code","f4f531f5":"code","801a34d3":"code","8b232f14":"code","f14004f9":"code","90372cda":"code","99d160f2":"code","d89b2c0e":"markdown","ef8fb6cc":"markdown","f92860fa":"markdown","27d9ea29":"markdown","e6b7f935":"markdown","6b5ff72f":"markdown","e14829e6":"markdown","604ccf2b":"markdown","f1fe0000":"markdown","b5657cbf":"markdown","75c01e65":"markdown"},"source":{"4c8c8689":"import numpy as np \nimport pandas as pd\n\nfrom sklearn.pipeline      import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute        import SimpleImputer\nfrom sklearn.linear_model  import LogisticRegression","9c65cc66":"df_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_test  = pd.read_csv('..\/input\/titanic\/test.csv')\ndf_sample= pd.read_csv('..\/input\/titanic\/gender_submission.csv')","0b5541ef":"df_train.info()","f4f531f5":"df_train.head()","801a34d3":"df_train.drop(['Name','Ticket','Cabin'],axis=1,inplace=True)\ndf_test.drop( ['Name','Ticket','Cabin'],axis=1,inplace=True)\n\n# We could create new features from these 3 but I aim to keep it simple and minimal","8b232f14":"sex    = pd.get_dummies(df_train['Sex'],drop_first=True)\nembark = pd.get_dummies(df_train['Embarked'],drop_first=True)\n\ndf_train = pd.concat([df_train,sex,embark],axis=1)\ndf_test  = pd.concat([df_test ,sex,embark],axis=1)\n\ndf_train.drop(['Sex','Embarked'],axis=1,inplace=True)\ndf_test.drop(['Sex','Embarked'],axis=1,inplace=True)","f14004f9":"imputer  = SimpleImputer()\nscaler   = StandardScaler()\nclf      = LogisticRegression()\npipe     = make_pipeline(imputer,scaler,clf)","90372cda":"features = df_train.drop('Survived',axis=1).columns\n\nX,y   = df_train[features], df_train['Survived']\ndf_test.fillna(df_test.mean(),inplace=True)","99d160f2":"pipe.fit(X,y)\ny_pred = pd.DataFrame(pipe.predict(df_test))\n\ny_pred['Survived'] = y_pred[0]\ny_pred.drop(0,axis=1,inplace=True)\ny_pred['PassengerId'] = df_test['PassengerId']\n\ny_pred.to_csv('titanic_pred_logistic.csv',index=False)","d89b2c0e":"# loading the files","ef8fb6cc":"# thank you very much for checking\n# I try to update it frequently!\n\n![](https:\/\/natgeo.imgix.net\/factsheets\/thumbnails\/RMSTitanic_TimelineofDisaster_Titanic.jpg?auto=compress,format&w=1600&h=900&fit=crop)","f92860fa":"# information about the dataset","27d9ea29":"# Dropping unnecessary columns","e6b7f935":"# importing the libraries","6b5ff72f":"This notebook doesn't aim to achieve high score, it serves the very and only purpose\n\nIf you're looking to a more advanced one:\n\nhttps:\/\/www.kaggle.com\/frtgnn\/titanic-survival-classifier\n\nIf you're looking only to some visuals:\n\nhttps:\/\/www.kaggle.com\/frtgnn\/focusing-on-couple-of-visuals-using-titanic-data","e14829e6":"# We both have numerical and categorical features","604ccf2b":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcTr0_0EtE1pjZl9xezJREPfPqCR0xKgHxvUGPDWPnMNh8wne_Rc)\n\n# This notebook only aims to introduce the \"pipeline\" concept. \n\n## So what is \"pipeline\" ? \n\n- Well, the pipeline is an abstract concept aiming to perform several different transformations before the final estimatior. The below is taken from [SKLEARN PIPELINE DOCUMENTATION](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html)\n\n- The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a \u2018__\u2019, as in the example below. A step\u2019s estimator may be replaced entirely by setting the parameter with its name to another estimator, or a transformer removed by setting it to \u2018passthrough\u2019 or None.\n\n\n**Parameters**:\n\n- **steps**: (list) List of (name, transform) tuples (implementing fit\/transform) that are chained, in the order in which they are chained, with the last object an estimator.\n\n- **memory**: (None, str or object with the joblib.Memory interface, optional) Used to cache the fitted transformers of the pipeline. By default, no caching is performed. If a string is given, it is the path to the caching directory. Enabling caching triggers a clone of the transformers before fitting. Therefore, the transformer instance given to the pipeline cannot be inspected directly. Use the attribute named_steps or steps to inspect estimators within the pipeline. Caching the transformers is advantageous when fitting is time consuming.\n\n- **verbose**: (bool, default=False) If True, the time elapsed while fitting each step will be printed as it is completed.\n\n**Attributes**:\n\n- **named_steps**: (bunch object, a dictionary with attribute access) Read-only attribute to access any step parameter by user given name. Keys are step names and values are steps parameters.\n\n\n\n\n\n","f1fe0000":"# Fitting the pipeline","b5657cbf":"## For this notebook, we will be using the Titanic Data\n\n![](https:\/\/res.cloudinary.com\/dk-find-out\/image\/upload\/q_80,w_1920,f_auto\/MA_00079563_yvu84f.jpg)\n\n\n","75c01e65":"# Only numerical features left to feed our pipeline\n\n- Imputation\n- Standard Scaling\n- Predictive Model"}}