{"cell_type":{"c9ea317a":"code","34be731a":"code","32d7aef0":"code","3dae5b30":"code","d3690e45":"code","50169bfd":"code","82bda8b4":"code","57c1b874":"code","21d96ead":"code","d9c0505a":"code","e35d5026":"code","d5e4a463":"code","b06be1a7":"code","fab3a152":"markdown","b2f7e3d8":"markdown","5e0f80f3":"markdown"},"source":{"c9ea317a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport spacy\nspacy.load('en')\nimport nltk\n#nltk.download(\"wordnet\") #(To be done once only)\nfrom spacy.lang.en import English\nparser = English()\nimport nltk\nfrom nltk.corpus import wordnet as wn\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom gensim import corpora\nimport gensim","34be731a":"df = pd.read_csv(\"\/kaggle\/input\/financial-times-brexit-articles-database\/financial_times_brexit_database.csv\")\ndf.reference_time = pd.to_datetime(df.reference_time)","32d7aef0":"df.url = df.url.apply(lambda x: \"https:\/\/www.ft.com\/content\/\"+x)","3dae5b30":"with open(\"urls.txt\",\"w\") as f:\n    f.write(\"\\n\".join(df.url.values))","d3690e45":"documents = df.title.dropna().values","50169bfd":"print(f\"{len(documents)} kept from a total of {len(df.title)}\")","82bda8b4":"stop_words_en=gensim.parsing.preprocessing.STOPWORDS\nstop_words_en = stop_words_en.union([\"brexit\"])\n\n\n\n\n\ndef tokenize(text):\n\t\"\"\"\n    Tokenizing texts\n\t\"\"\"\n\tlda_tokens = []\n\ttokens = parser(text)\n\tfor token in tokens:\n\t\tif token.orth_.isspace():\n\t\t\tcontinue\n\t\telse:\n\t\t\tlda_tokens.append(token.lower_)\n\treturn lda_tokens\n\ndef get_lemma(word):\n\t\"\"\"\n\tlemmatization\n\t\"\"\"\n\tlemma = wn.morphy(word)\n\tif lemma is None:\n\t\treturn word\n\telse:\n\t\treturn lemma\n\n\ndef prepare_text_for_lda(text):\n\t\"\"\"\n\tcomplete preparation of the text\n\t\"\"\"\n\ttokens = tokenize(text.replace(\"-\",\" \"))\n\ttokens = [token for token in tokens if len(token) > 4]\n\ttokens = [token for token in tokens if token not in stop_words_en]\n\ttokens = [get_lemma(token) for token in tokens]\n\treturn tokens\n\ndef texttokens(texts):\n\t\"\"\"\n    Prepare all texts for nlp\n\t\"\"\"\n\ttext_data = []\n\tfor t in texts:\n\t\ttokens = prepare_text_for_lda(t)\n\t\ttext_data.append(tokens)\n\treturn text_data\n\n\n\n\n\n\n\n\ndef LSI_topicExtraction(texts, n_topics):\n\t\"\"\"\n\ttopic extraction with LSI\n\t\"\"\"\n\n\n\tprint(\"Tokenization...\")\n\ttext_data=texttokens(texts)\n\tprint(\"Dictionarisation...\")\n\tdictionary = corpora.Dictionary(text_data)\n\tprint(\"Corpusisation...\")\n\tcorpus = [dictionary.doc2bow(text) for text in text_data]\n    \n\n\t#print(corpus)\n\tprint(\"modelization...\")\n\tlsimodel = gensim.models.LsiModel(corpus, id2word=dictionary,num_topics=n_topics)\n\n\treturn lsimodel, corpus\n\ndef LDA_topicExtraction(texts, n_topics):\n\t\"\"\"\n\ttopic extraction with LDA\n\t\"\"\"\n\n\n\tprint(\"Tokenization...\")\n\ttext_data=texttokens(texts)\n\tprint(\"Dictionarisation...\")\n\tdictionary = corpora.Dictionary(text_data)\n\tprint(\"Corpusisation...\")\n\tcorpus = [dictionary.doc2bow(text) for text in text_data]\n    \n\n\t#print(corpus)\n\tprint(\"modelization...\")\n\tldamodel = gensim.models.LdaModel(corpus, id2word=dictionary,num_topics=n_topics)\n\n\treturn ldamodel, corpus\n\n\ndef format_topic(topic):\n\t\"\"\"\n\tFormatage des topics renvoy\u00e9s par la librairie gensim pour les afficher dans l'interface web\n\t\"\"\"\n\tt = {}\n\tt[\"id\"] = topic[0]\n\ta = topic[1].split(\" + \")\n\tt[\"words\"] = {}\n\tfor i,m in enumerate(a):\n\t\tk = m.split(\"*\")\n\t\tif i == 0:\n\t\t\tmax_weight = float(k[0])\n\t\tt[\"words\"][k[1].replace('\"','')]=float(k[0])\/max_weight\n\n\treturn t\n","57c1b874":"lda_model,corpus = LDA_topicExtraction(documents,10)","21d96ead":"topics = [format_topic(t) for t in lda_model.print_topics()]","d9c0505a":"for t in topics:\n    plt.figure(figsize=(15,7))\n    sns.barplot(list(t[\"words\"].keys()),list(t[\"words\"].values()))\n    plt.title(f\"Most important words of topic n\u00b0{t['id']}\")","e35d5026":"def get_name(u):\n    return \"-\".join([str(u.reference_time.year),str(u.reference_time.month),str(u.reference_time.day)]) + \"  \" + str(u.title)\n","d5e4a463":"df[\"name\"] = df.apply(lambda x: get_name(x),axis=1)","b06be1a7":"df.sort_values(\"comment_count\",ascending=False)[[\"name\",\"comment_count\"]]","fab3a152":"We keep only the documents that have a valid title","b2f7e3d8":"# Topic extraction brexit databse","5e0f80f3":"We also add the word brexit to the stop words because it will obviously be in almost all the title. We want a finer discovery of the topics"}}