{"cell_type":{"0a5ed032":"code","cd2971bc":"code","0e160712":"code","cc6f8d7e":"code","918cce24":"code","15d27c79":"code","22b6ed9a":"code","f24c792f":"code","d0afa483":"code","537ff16b":"code","dcf233f2":"code","e5b7a33a":"code","1c43de28":"code","36473d6f":"code","88444639":"code","1d521182":"code","54b1adf7":"code","f5331044":"code","de6ada55":"code","57addee7":"code","5e948d1d":"code","34278933":"code","7337d0a2":"code","d56fa0f0":"code","5220b5f5":"code","b4b16962":"markdown"},"source":{"0a5ed032":"# loading libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nimport seaborn as sns","cd2971bc":"# loading dataset and checking the shape and data head\nbreast_cancer = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\nprint(\"shape of the data: \", breast_cancer.shape)\nbreast_cancer.head()","0e160712":"# checking the data info \nbreast_cancer.info()","cc6f8d7e":"# dropping the Unnamed: 32 column since all the values are empty\nbreast_cancer = breast_cancer.drop(['Unnamed: 32'], axis=1)\n\n# checking sum of null values \nbreast_cancer.isnull().sum()","918cce24":"# encoding categorical variables on diagnosis column\nprint(\"unique values\", breast_cancer['diagnosis'].unique())\nprint(\"before encoding -->\")\nprint(breast_cancer['diagnosis'].tail())\n\nenc = LabelEncoder()\nbreast_cancer['diagnosis'] = enc.fit_transform(breast_cancer['diagnosis'])\n\nprint(\"after encoding -->\")\nprint(breast_cancer['diagnosis'].tail())","15d27c79":"# checking feature correlation\nb_corr = breast_cancer.corr()\nb_corr\n\nplt.figure(figsize=(25,10))\nsns.heatmap(b_corr, cmap = sns.color_palette(\"vlag\", as_cmap=True), annot=True)","22b6ed9a":"# dropping features based on correlation\nbreast_cancer = breast_cancer.drop(['id', 'symmetry_mean', 'concavity_se', 'compactness_se' ], axis=1)","f24c792f":"# Splitting the dataset into features and labels\n# diagnosis column indicates the labels\nfeatures = breast_cancer.drop(['diagnosis'], axis=1)\nlabels = breast_cancer[['diagnosis']]\nprint(\"features shape\", features.shape)\nprint(\"label shape\", labels.shape)\n\n# splitting test and train data\nX_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\nprint(\"training data shape\", X_train.shape)\nprint(\"testing data shape\", X_test.shape)","d0afa483":"# scaling all the values between 0-1\nscaler = MinMaxScaler(copy=False)\nscaler.fit(X_train)\nscaler.transform(X_train)\nscaler.transform(X_test)\n\n# data head after scaling\nprint(\"minimum value\", X_train.min(axis=0).values)\nprint(\"maximum value\", X_train.max(axis=0).values)\nX_train.head()","537ff16b":"# logistic regression model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train.values.ravel())\ny_pred = model.predict(X_test)\nlg_accuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy using logistic regression: \", lg_accuracy)","dcf233f2":"# Decision tree model\nclf = DecisionTreeClassifier(criterion='entropy', random_state=42)\nclf.fit(X_train,y_train)\ny_pred = clf.predict(X_test)\ndt_accuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy using decision tree: \", dt_accuracy)","e5b7a33a":"# plotting bar chart\nsns.set_style(\"whitegrid\")\nsns.set(rc={'figure.figsize':(6,6)})\n\nax = sns.barplot(x=['Logistic regression', 'Decision tree'], y=[lg_accuracy, dt_accuracy])\nax.set(xlabel='Models', ylabel='Accuracy', title='Classification Model Comparison')\n\na = int(lg_accuracy * 100)\nb = int(dt_accuracy * 100)\nax.text(0, lg_accuracy, str(a)+'%')\nax.text(1, dt_accuracy, str(b)+'%')\nplt.show()","1c43de28":"from sklearn import svm\nsvm_clf = svm.SVC()\n\nsvm_clf.fit(X_train,y_train.values.ravel())\n\nprint(\"Training accuracy of the model is\", (svm_clf.score(X_train, y_train)))\nsvm_accuracy = svm_clf.score(X_test, y_test)\nprint(\"Testing accuracy of the model is\", svm_accuracy)","36473d6f":"from sklearn.neural_network import MLPClassifier\nmlp_clf = MLPClassifier(hidden_layer_sizes=(7), activation=\"relu\", random_state=1, max_iter=10000)\n\nmlp_clf.fit(X_train, y_train.values.ravel())\n\nprint(\"Training accuracy of the model is\", (mlp_clf.score(X_train, y_train)))\nnn_accuracy = mlp_clf.score(X_test, y_test)\nprint(\"Testing accuracy of the model is\", nn_accuracy)","88444639":"from sklearn.ensemble import RandomForestClassifier\nrf_clf = RandomForestClassifier(n_estimators=50)\n\nrf_clf.fit(X_train, y_train.values.ravel())\n                            \nprint(\"Training accuracy of the model is\", (rf_clf.score(X_train, y_train)))\nrf_accuracy = rf_clf.score(X_test, y_test)\nprint(\"Testing accuracy of the model is\", rf_accuracy)                             ","1d521182":"# plotting bar chart\nsns.set_style(\"whitegrid\")\nsns.set(rc={'figure.figsize':(6,6)})\n\nax = sns.barplot(x=['svm', 'nn', 'rf'], y=[svm_accuracy, nn_accuracy, rf_accuracy])\nax.set(xlabel='Models', ylabel='Accuracy', title='Classification Model Comparison')\n\na = int(svm_accuracy * 100)\nb = int(nn_accuracy * 100)\nc = int(rf_accuracy * 100)\nax.text(0, svm_accuracy, str(a)+'%')\nax.text(1, nn_accuracy, str(b)+'%')\nax.text(2, rf_accuracy, str(c)+'%')\nplt.show()","54b1adf7":"# Performing dimensionality reduction using PCA\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=15)\npca.fit(X_train)\nX_train_pca = pca.fit_transform(X_train)\nX_test_pca = pca.fit_transform(X_test)\n\nprint(X_train_pca)\nsum(pca.explained_variance_ratio_)","f5331044":"from sklearn import svm\nsvm_model = svm.SVC()\n\nsvm_model.fit(X_train_pca, y_train.values.ravel())\n\nprint(\"Training accuracy of the model is\", (svm_model.score(X_train_pca, y_train)))\npca_svm_accuracy = svm_model.score(X_test_pca, y_test)\nprint(\"Testing accuracy of the model is\", pca_svm_accuracy)","de6ada55":"from sklearn.neural_network import MLPClassifier\nmlpc_model = MLPClassifier(hidden_layer_sizes=(7), activation=\"relu\", random_state=1, max_iter=10000)\n\nmlpc_model.fit(X_train_pca, y_train.values.ravel())\n\nprint(\"Training accuracy of the model is\", (mlpc_model.score(X_train_pca, y_train)))\npca_nn_accuracy = mlpc_model.score(X_test_pca, y_test)\nprint(\"Testing accuracy of the model is\", pca_nn_accuracy)","57addee7":"from sklearn.ensemble import RandomForestClassifier\nrf_model = RandomForestClassifier(n_estimators=50)\n\nrf_model.fit(X_train_pca, y_train.values.ravel())\n                            \nprint(\"Training accuracy of the model is\", (rf_model.score(X_train_pca, y_train)))\npca_rf_accuracy = rf_model.score(X_test_pca, y_test)\nprint(\"Testing accuracy of the model is\", pca_rf_accuracy)                             ","5e948d1d":"# plotting bar chart for models with pca\nsns.set_style(\"whitegrid\")\nsns.set(rc={'figure.figsize':(6,6)})\n\nax = sns.barplot(x=['svm_pca', 'nn_pca', 'rf_pca'], y=[pca_svm_accuracy, pca_nn_accuracy, pca_rf_accuracy])\nax.set(xlabel='Models', ylabel='Accuracy', title='Classification Model Comparison')\n\na = int(pca_svm_accuracy * 100)\nb = int(pca_nn_accuracy * 100)\nc = int(pca_rf_accuracy * 100)\nax.text(0, pca_svm_accuracy, str(a)+'%')\nax.text(1, pca_nn_accuracy, str(b)+'%')\nax.text(2, pca_rf_accuracy, str(c)+'%')\nplt.show()","34278933":"# plotting bar chart\nsns.set_style(\"whitegrid\")\nsns.set(rc={'figure.figsize':(10,10)})\n\nax = sns.barplot(x=['SVM', 'NN', 'RF', 'SVM(with pca)', 'NN(with pca)', 'RF(with pca)'], y=[svm_accuracy, nn_accuracy, rf_accuracy, pca_svm_accuracy, pca_nn_accuracy, pca_rf_accuracy])\nax.set(xlabel='Models', ylabel='Accuracy', title='Classification Model Comparison')\n\na = int(svm_accuracy * 100)\nb = int(nn_accuracy * 100)\nc = int(rf_accuracy * 100)\nx = int(pca_svm_accuracy * 100)\ny = int(pca_nn_accuracy * 100)\nz = int(pca_rf_accuracy * 100)\nax.text(0, svm_accuracy, str(a)+'%')\nax.text(1, nn_accuracy, str(b)+'%')\nax.text(2, rf_accuracy, str(c)+'%')\nax.text(3, pca_svm_accuracy, str(x)+'%')\nax.text(4, pca_nn_accuracy, str(y)+'%')\nax.text(5, pca_rf_accuracy, str(z)+'%')\n\nplt.show()","7337d0a2":"#Univariate feature selection\n#Selected 20 features according to the k highest scores and using knn\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.neighbors import KNeighborsClassifier\n\nselect_feature = SelectKBest(chi2, k=20).fit(X_train, y_train)\n\nx_train_2 = select_feature.transform(X_train)\nx_test_2 = select_feature.transform(X_test)\nprint('Score list:', select_feature.scores_)\nprint('Feature list:', X_train.columns)\n\nknn=KNeighborsClassifier()\nprint(\"for Selected features\")\nknn.fit(x_train_2, y_train.values.ravel())\nprint(\"Test set accuracy: {:.2f}\".format(knn.score(x_test_2, y_test.values.ravel())))\nprint(\"for all features\")\nknn.fit(X_train, y_train.values.ravel())\nprint(\"Test set accuracy: {:.2f}\".format(knn.score(X_test, y_test.values.ravel())))","d56fa0f0":"#Recursive feature elimination (RFE)\nfrom sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestClassifier\n# Create the RFE object and rank each pixel\nclf_rf_3 = RandomForestClassifier()      \nrfe = RFE(estimator=clf_rf_3, n_features_to_select=10\n          , step=1)\nrfe = rfe.fit(X_train, y_train.values.ravel())\nprint('Chosen best features by rfe:',X_train.columns[rfe.support_])","5220b5f5":"#Recursive feature elimination with cross validation\n#finding optimal number of features needed for best accuracy\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.ensemble import RandomForestClassifier\nclf_rf_4 = RandomForestClassifier() \nrfecv = RFECV(estimator=clf_rf_4, step=1, cv=5,scoring='accuracy')   #5-fold cross-validation\nrfecv = rfecv.fit(X_train, y_train.values.ravel())\n\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', X_train.columns[rfecv.support_])","b4b16962":"ways to find the most representative features:"}}