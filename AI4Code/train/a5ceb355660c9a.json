{"cell_type":{"74447761":"code","562a306a":"code","8ec70f49":"code","e73bcff0":"code","de1f764f":"code","48af70c4":"code","b7f0e776":"code","1be1e0d2":"code","7e846041":"code","89ed7596":"code","a5c1cd1d":"code","80d58606":"code","265e324c":"code","d9da9fd0":"code","79d17577":"code","db2b5fce":"code","a1d51f74":"code","1fa3a227":"code","bc3d165c":"code","804fc58b":"code","6dac8458":"code","a913b1ed":"code","5702c67b":"code","461e0948":"code","8c6f270a":"code","a1cee75b":"code","75caa678":"code","4a7c0edb":"code","bb1a7ea7":"code","091e6c60":"markdown","9e3db00e":"markdown","aad25afa":"markdown","cdaa2145":"markdown","e7071f90":"markdown","95b74375":"markdown","147374fc":"markdown","5aac518e":"markdown","8e7b6e9d":"markdown","056c4f55":"markdown","daac2989":"markdown","79815de2":"markdown","f8ff491e":"markdown","b4985d7c":"markdown","e33caaa2":"markdown","9c43cc23":"markdown","fbe6f5e0":"markdown","a361a8a8":"markdown"},"source":{"74447761":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","562a306a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n!pip install xlrd\n!pip install autoviz\nfrom autoviz.AutoViz_Class import AutoViz_Class\n#Instantiate the AutoViz class\nAV = AutoViz_Class()\n","8ec70f49":"data = pd.read_csv(\"\/kaggle\/input\/mobile-price-classification\/train.csv\")\ndata.head()\n","e73bcff0":"df_viz = AV.AutoViz(\"\/kaggle\/input\/mobile-price-classification\/train.csv\",  depVar = 'price_range')","de1f764f":"data.isnull().any().sum()","48af70c4":"data.describe()","b7f0e776":"plt.figure(figsize=(10,7))\ncorr = data.corr()\nsns.heatmap(corr, cmap=\"YlGnBu\", linewidth = 0.7)","1be1e0d2":"sns.pairplot(data, vars = ['battery_power','px_height', 'px_width', 'ram','pc','fc'], hue = 'price_range', palette = 'coolwarm')","7e846041":"g = sns.FacetGrid(data, col=\"price_range\", hue=\"wifi\", palette = 'mako')\ng.map(sns.scatterplot, \"mobile_wt\", \"int_memory\", alpha=.7)\nplt.figure(figsize=(15,7))\ng.add_legend()","89ed7596":"g = sns.FacetGrid(data, col=\"price_range\", hue=\"wifi\", palette = 'deep')\ng.map(sns.scatterplot, \"ram\", \"int_memory\", alpha=.7)\nplt.figure(figsize=(15,7))\ng.add_legend()","a5c1cd1d":"g = sns.FacetGrid(data, col=\"price_range\", hue=\"four_g\", palette = 'rocket')\ng.map(sns.scatterplot, \"ram\", \"battery_power\", alpha=.7)\nplt.figure(figsize=(15,7))\ng.add_legend()","80d58606":"g = sns.FacetGrid(data, col=\"price_range\", hue=\"n_cores\")\ng.map(sns.scatterplot, \"ram\", \"px_height\", alpha=.7)\nplt.figure(figsize=(15,7))\ng.add_legend()","265e324c":"test = pd.read_csv(\"\/kaggle\/input\/mobile-price-classification\/test.csv\")\ntest = test.drop(['id'], axis = 1)\ntest.head()","d9da9fd0":"test.info()","79d17577":"from sklearn.model_selection import train_test_split","db2b5fce":"X = data.drop(['price_range'], axis = 1)\ny = data.price_range","a1d51f74":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)","1fa3a227":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nscaler = StandardScaler()\nscaler.fit(X_train, y_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","bc3d165c":"classifier=RandomForestClassifier(n_estimators=100)\nmodel = classifier.fit(X_train, y_train)\ny_pred = model.predict(X_test)","804fc58b":"df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndf","6dac8458":"from sklearn import metrics\nfrom sklearn.metrics import r2_score","a913b1ed":"print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred).round(3))  \nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred).round(3))  \nprint('Root Mean Squared:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)).round(3))\nprint('r2_score:', r2_score(y_test, y_pred).round(3))\nprint('Accuracy:', metrics.accuracy_score(y_test.astype('int'), y_pred).round(3))","5702c67b":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","461e0948":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X, y)","8c6f270a":"rf_random.best_params_","a1cee75b":"rf1 = RandomForestClassifier(n_estimators= 1200,\n min_samples_split= 8,\n min_samples_leaf= 4,\n max_features= 'auto',\n max_depth= None,\n bootstrap= False)\nmodel_rf1 = rf1.fit(X_train, y_train)\ny_pred_rf1 = model_rf1.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\naccu_score_rf1 = accuracy_score(y_test, y_pred_rf1)\n\nprint('accu_score_rf1:', accu_score_rf1)","75caa678":"test = scaler.transform(test)","4a7c0edb":"y_pred_test = model_rf1.predict(test)","bb1a7ea7":"df_pred_test = pd.DataFrame({'Predicted': y_pred_test})\ndf_pred_test.head(10)\n","091e6c60":"# Accuracy & R2 Score","9e3db00e":"# reading the test data & removing id column","aad25afa":"# Converting data into standard scaler","cdaa2145":"# Fitting the best parameter on Random Forest","e7071f90":"# Model RandomForest Classifier","95b74375":"# Plotting Heatmap","147374fc":"# Finding affect of mobile wt & int_memory with price range","5aac518e":"# Reading the training set data","8e7b6e9d":"# Plotting pair plots with most corelated parameters only","056c4f55":"# Hyperparameter Tuning on Random Forest","daac2989":"# Train Test & Split","79815de2":"# Dont forget to like \/vote","f8ff491e":"# Predicted Price Range on Test Data Set","b4985d7c":"> *** Corelation exists, Price depends upon ram & somewhat with battery power***","e33caaa2":"# Finding affect of ram & battery power with price range","9c43cc23":"# Checking for null values\nNo Null values found","fbe6f5e0":"# Converting Test data in Standard Scaler","a361a8a8":"# Finding affect of ram & int_memory with price range"}}