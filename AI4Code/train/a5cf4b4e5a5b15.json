{"cell_type":{"b1fa645c":"code","aa998de7":"code","81a7d855":"code","7940f97f":"code","ddf33ca1":"code","46698ba6":"code","2f199347":"code","3d4422d7":"code","c9374dd1":"code","dcfb05d6":"code","0b40474d":"code","80fa352a":"code","d4221867":"code","70a70029":"code","990638bb":"code","d6a6114c":"code","1b0b789d":"code","4457213a":"code","9cf9d67d":"code","59085a20":"code","e9f84695":"code","5ff669f8":"code","6507d1bc":"code","22cb46ca":"code","2e630e2e":"code","6c618b6b":"code","3a6117e5":"code","1bd1ca4a":"code","4b97f6ef":"code","5bd48925":"code","2ce71764":"code","82993c00":"code","8ff333b5":"code","d156c86e":"code","d2bb39b8":"code","d9e3415c":"code","09837eb8":"code","c27cfc97":"code","84308466":"code","5771f8c8":"markdown","385fe18c":"markdown","1fb566e5":"markdown","a1999b94":"markdown","da3c316f":"markdown","bfbd4285":"markdown","93a62f1b":"markdown","41b42368":"markdown","31d211e2":"markdown","2d94d3c7":"markdown","e27cc870":"markdown","9eab3319":"markdown","510243ff":"markdown","0b6e6ac6":"markdown","d42942a5":"markdown","e782d510":"markdown","5f232c88":"markdown","4f02c99d":"markdown","12358b2a":"markdown"},"source":{"b1fa645c":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","aa998de7":"# To begin, I will import the \"gold-price-last-ten-years.csv,\" and \"SP500 2007-2020.csv\" \n# in order to merge them together along the date column.\n\nimport pandas as pd # for importing, reading, cleaning csv files\nimport numpy as np # for maths, of course\nimport matplotlib.pyplot as plt # so we can plot\nfrom matplotlib.pylab import rcParams #\nimport seaborn as sns # so we can beautify the plots\nfrom statsmodels.tsa.stattools import adfuller # for our Dicky Fuller tests.\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.arima_model import ARIMAResults\nimport statsmodels.api as sm # in case I think of anything else I need and didn't bring in here\nfrom sklearn.model_selection import train_test_split # For splitting data effectively\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error\nimport warnings # For handling error messages.\n\n# There is a lot of text prior to the actual columns, so the header has\n# been set appropriatley.\ngold = pd.read_csv('\/kaggle\/input\/gold-sp-500-index-arimax-time-series\/gold-price-last-ten-years.csv', sep=',', header=8)\n\ngold.head()","81a7d855":"# First things first, change 'date' column to actual date \n# format with 'pd.to_datetime()'\ngold['date']=pd.to_datetime(gold['date'])\n\n# Establish which dates we want to focus on and remove the dates we don't care about from the data.\n# 18 weeks is the minimum time period to be considered a \"trend\" in stock prices. I chose August 21, 2017, as it will leave\n# a healthy set of trend data as the test split. Click the link above if you want to no more!\nfocus = (gold['date'] > '2017-08-21') & (gold['date'] <= '2020-08-21')\n\n# Assign into a main data frame to preserve gold in case we want it later\nmaindf = gold.loc[focus]\n\nmaindf.head()","7940f97f":"# Renaming the columns to something more 'formal' helps me visualize \n# the final plot and table I have in mind.\nmaindf.columns = ['Date', 'GLD(USD)']\nmaindf.head()","ddf33ca1":"# We are going to be checking for NaN values in the data quite \n# frequently in order to monitor changes and loss of data as we clean. \n# Best to write a function for checking NaN now and speed things up! Let's\n# have a function that gives us the percentage that NaN values make up in \n# the data.\ndef show_na(df):\n    na_df = pd.concat([df.isnull().sum(), 100 * df.isnull().mean()], axis=1)\n    na_df.columns=['count', '%']\n    na_df.sort_values(by='count', ascending = False)\n    return na_df\n# Use the function\nshow_na(maindf)\n\n#Sweet, no Nan's thus far.","46698ba6":"# Import the next csv and repeat\nSP500=pd.read_csv('\/kaggle\/input\/gold-sp-500-index-arimax-time-series\/SP500 2007-2020.csv', sep=',', header=7)\nSP500['date'] = pd.to_datetime(SP500['date'])\n# Rename columns for appropriate representation.\nSP500.columns = ['Date', 'SPX(USD)']\n# Since GLD and S&P500 are both stock indexes, they should have almost identical Nan's.\nmaindf=maindf.merge(SP500, how='left', on='Date')\nshow_na(maindf)","2f199347":"# Again, repeat for the remaining CSV's\nbarrick = pd.read_csv('\/kaggle\/input\/gold-sp-500-index-arimax-time-series\/Barrick Gold Corp 1985-2020.csv', sep=',', header=9)\nbarrick['date'] = pd.to_datetime(barrick['date'])\nbarrick = barrick[['date', 'close']]\nbarrick.columns = ['Date','BARR(USD)']\nmaindf = maindf.merge(barrick, how='left', on='Date')\nmaindf.head()","3d4422d7":"silver = pd.read_csv('\/kaggle\/input\/gold-sp-500-index-arimax-time-series\/silver history.csv', sep=',', header=7)\nsilver['date']=pd.to_datetime(silver['date'])\nsilver.columns = ['Date', 'SLV(USD)']\nmaindf = maindf.merge(silver, how='left', on='Date')\nmaindf.head()","c9374dd1":"# Check for NaN's\nshow_na(maindf)","dcfb05d6":"# Check for duplicate dates before setting index on \"Date\"\nmaindf['Date'].value_counts().head()","0b40474d":"#Set index to 'Date'  for graphing and visualization\nmaindf= maindf.set_index('Date')\nmaindf.head()","80fa352a":"# Now, to deal with the Nan's. The best option I propose is to forward fill the missing values since \n# the values do not change much over the weekends. Lets be careful, though, and observe the mean, std deviation, \n# etc of each column before we use ffill.\nmaindf.describe()","d4221867":"# Now use ffill to remove the Nan's\nmaindf = maindf.fillna(axis=0, method='ffill')\n# Lets look at the overall summary of each column again.\nmaindf.describe()","70a70029":"# Double check, but there should't be any nan values now.\nmaindf.isnull().sum()","990638bb":"# Take a look at how they compare:\nplt.figure(figsize=(20,15))\nplt.subplot(411)\nplt.plot(maindf['GLD(USD)'], label='GLD', color='y')\nplt.legend(loc='best', fontsize='large')\nplt.ylabel('Value(USD)')\nplt.subplot(412)\nplt.plot(maindf['SPX(USD)'], label='S&P', color='b')\nplt.legend(loc='best', fontsize='large')\nplt.ylabel('Value(USD)')\nplt.subplot(413)\nplt.plot(maindf['BARR(USD)'], label='BARR', color='c')\nplt.legend(loc='best', fontsize='large')\nplt.ylabel('Value(USD)')\nplt.subplot(414)\nplt.plot(maindf['SLV(USD)'], label='SLV', color='g')\nplt.legend(loc='best', fontsize='large')\nplt.xlabel('Date')\nplt.ylabel('Value(USD)')","d6a6114c":"corr = maindf.corr()\nplt.figure(figsize = (8,6))\nsns.heatmap(corr,\n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values,\n            annot=True, fmt='.2f', linewidths=.30)\nplt.title('Correlation of MainDF Features', y =1.05,  size=15)\npos, textvals = plt.yticks()\nplt.yticks(pos,('SLV(USD)','BARR(USD)','SPX(USD)','GLD(USD)'), \n    rotation=0, fontsize=\"10\", va=\"center\")","1b0b789d":"# Correlation Scores:\nprint(corr['GLD(USD)'].sort_values(ascending =False), '\\n')","4457213a":"# Using jointplot to visualize relation of GLD with other variables\nsns.jointplot(x=maindf['BARR(USD)'], y=maindf['GLD(USD)'], color='blue')","9cf9d67d":"# How about S&P500 and GLD?\nsns.jointplot(x= maindf['SPX(USD)'], y=maindf['GLD(USD)'], color='gold')","59085a20":"model = sm.OLS(maindf['GLD(USD)'].iloc[:90], maindf['SPX(USD)'].iloc[:90])\nmodel = model.fit() \nprint(model.params[0])\n#Spread\nmaindf['spread'] = maindf['GLD(USD)'] - model.params[0] *  maindf['SPX(USD)']\n# Plot the spread\nmaindf['spread'].plot(figsize=(8,4))\nplt.ylabel(\"Spread\")\nplt.show()","e9f84695":"# Compute ADF test statistics\nadf = adfuller(maindf.spread, maxlag = 1)\nadf[0]","5ff669f8":"adf[4]","6507d1bc":"'''\nfunction result = johansen(x,p,k)\n% PURPOSE: perform Johansen cointegration tests\n% -------------------------------------------------------\n% USAGE: result = johansen(x,p,k)\n% where:      x = input matrix of time-series in levels, (nobs x m)\n%             p = order of time polynomial in the null-hypothesis\n%                 p = -1, no deterministic part\n%                 p =  0, for constant term\n%                 p =  1, for constant plus time-trend\n%                 p >  1, for higher order polynomial\n%             k = number of lagged difference terms used when\n%                 computing the estimator\n% -------------------------------------------------------\n% RETURNS: a results structure:\n%          result.eig  = eigenvalues  (m x 1)\n%          result.evec = eigenvectors (m x m), where first\n%                        r columns are normalized coint vectors\n%          result.lr1  = likelihood ratio trace statistic for r=0 to m-1\n%                        (m x 1) vector\n%          result.lr2  = maximum eigenvalue statistic for r=0 to m-1\n%                        (m x 1) vector\n%          result.cvt  = critical values for trace statistic\n%                        (m x 3) vector [90% 95% 99%]\n%          result.cvm  = critical values for max eigen value statistic\n%                        (m x 3) vector [90% 95% 99%]\n%          result.ind  = index of co-integrating variables ordered by\n%                        size of the eigenvalues from large to small\n% -------------------------------------------------------\n% NOTE: c_sja(), c_sjt() provide critical values generated using\n%       a method of MacKinnon (1994, 1996).\n%       critical values are available for n<=12 and -1 <= p <= 1,\n%       zeros are returned for other cases.\n% -------------------------------------------------------\n% SEE ALSO: prt_coint, a function that prints results\n% -------------------------------------------------------\n% References: Johansen (1988), 'Statistical Analysis of Co-integration\n% vectors', Journal of Economic Dynamics and Control, 12, pp. 231-254.\n% MacKinnon, Haug, Michelis (1996) 'Numerical distribution\n% functions of likelihood ratio tests for cointegration',\n% Queen's University Institute for Economic Research Discussion paper.\n% (see also: MacKinnon's JBES 1994 article\n% -------------------------------------------------------\n\n% written by:\n% James P. LeSage, Dept of Economics\n% University of Toledo\n% 2801 W. Bancroft St,\n% Toledo, OH 43606\n% jlesage@spatial-econometrics.com\n\n% ****************************************************************\n% NOTE: Adina Enache provided some bug fixes and corrections that\n%       she notes below in comments. 4\/10\/2000\n% ****************************************************************\n'''\n\nimport numpy as np\nfrom numpy import zeros, ones, flipud, log\nfrom numpy.linalg import inv, eig, cholesky as chol\nfrom statsmodels.regression.linear_model import OLS\n\n\ntdiff = np.diff\n\nclass Holder(object):\n    pass\n\ndef rows(x):\n    return x.shape[0]\n\ndef trimr(x, front, end):\n    if end > 0:\n        return x[front:-end]\n    else:\n        return x[front:]\n\nimport statsmodels.tsa.tsatools as tsat\nmlag = tsat.lagmat\n\ndef mlag_(x, maxlag):\n    '''return all lags up to maxlag\n    '''\n    return x[:-lag]\n\ndef lag(x, lag):\n    return x[:-lag]\n\ndef detrend(y, order):\n    if order == -1:\n        return y\n    return OLS(y, np.vander(np.linspace(-1, 1, len(y)), order + 1)).fit().resid\n\ndef resid(y, x):\n    r = y - np.dot(x, np.dot(np.linalg.pinv(x), y))\n    return r\n\n\n\n\ndef coint_johansen(x, p, k, print_on_console=True):\n\n    #    % error checking on inputs\n    #    if (nargin ~= 3)\n    #     error('Wrong # of inputs to johansen')\n    #    end\n    nobs, m = x.shape\n\n    # why this?  f is detrend transformed series, p is detrend data\n    if (p > -1):\n        f = 0\n    else:\n        f = p\n\n    x = detrend(x, p)\n    dx = tdiff(x, 1, axis=0)\n    # dx    = trimr(dx,1,0)\n    z = mlag(dx, k)  # [k-1:]\n#    print z.shape\n    z = trimr(z, k, 0)\n    z = detrend(z, f)\n#    print dx.shape\n    dx = trimr(dx, k, 0)\n\n    dx = detrend(dx, f)\n    # r0t   = dx - z*(z\\dx)\n    r0t = resid(dx, z)  # diff on lagged diffs\n    # lx = trimr(lag(x,k),k,0)\n    lx = lag(x, k)\n    lx = trimr(lx, 1, 0)\n    dx = detrend(lx, f)\n#    print 'rkt', dx.shape, z.shape\n    # rkt   = dx - z*(z\\dx)\n    rkt = resid(dx, z)  # level on lagged diffs\n    skk = np.dot(rkt.T, rkt) \/ rows(rkt)\n    sk0 = np.dot(rkt.T, r0t) \/ rows(rkt)\n    s00 = np.dot(r0t.T, r0t) \/ rows(r0t)\n    sig = np.dot(sk0, np.dot(inv(s00), (sk0.T)))\n    tmp = inv(skk)\n    # du, au = eig(np.dot(tmp, sig))\n    au, du = eig(np.dot(tmp, sig))  # au is eval, du is evec\n    # orig = np.dot(tmp, sig)\n\n    # % Normalize the eigen vectors such that (du'skk*du) = I\n    temp = inv(chol(np.dot(du.T, np.dot(skk, du))))\n    dt = np.dot(du, temp)\n\n\n    # JP: the next part can be done much  easier\n\n    # %      NOTE: At this point, the eigenvectors are aligned by column. To\n    # %            physically move the column elements using the MATLAB sort,\n    # %            take the transpose to put the eigenvectors across the row\n\n    # dt = transpose(dt)\n\n    # % sort eigenvalues and vectors\n\n    # au, auind = np.sort(diag(au))\n    auind = np.argsort(au)\n    # a = flipud(au)\n    aind = flipud(auind)\n    a = au[aind]\n    # d = dt[aind,:]\n    d = dt[:, aind]\n\n    # %NOTE: The eigenvectors have been sorted by row based on auind and moved to array \"d\".\n    # %      Put the eigenvectors back in column format after the sort by taking the\n    # %      transpose of \"d\". Since the eigenvectors have been physically moved, there is\n    # %      no need for aind at all. To preserve existing programming, aind is reset back to\n    # %      1, 2, 3, ....\n\n    # d  =  transpose(d)\n    # test = np.dot(transpose(d), np.dot(skk, d))\n\n    # %EXPLANATION:  The MATLAB sort function sorts from low to high. The flip realigns\n    # %auind to go from the largest to the smallest eigenvalue (now aind). The original procedure\n    # %physically moved the rows of dt (to d) based on the alignment in aind and then used\n    # %aind as a column index to address the eigenvectors from high to low. This is a double\n    # %sort. If you wanted to extract the eigenvector corresponding to the largest eigenvalue by,\n    # %using aind as a reference, you would get the correct eigenvector, but with sorted\n    # %coefficients and, therefore, any follow-on calculation would seem to be in error.\n    # %If alternative programming methods are used to evaluate the eigenvalues, e.g. Frame method\n    # %followed by a root extraction on the characteristic equation, then the roots can be\n    # %quickly sorted. One by one, the corresponding eigenvectors can be generated. The resultant\n    # %array can be operated on using the Cholesky transformation, which enables a unit\n    # %diagonalization of skk. But nowhere along the way are the coefficients within the\n    # %eigenvector array ever changed. The final value of the \"beta\" array using either method\n    # %should be the same.\n\n\n    # % Compute the trace and max eigenvalue statistics *\/\n    lr1 = zeros(m)\n    lr2 = zeros(m)\n    cvm = zeros((m, 3))\n    cvt = zeros((m, 3))\n    iota = ones(m)\n    t, junk = rkt.shape\n    for i in range(0, m):\n        tmp = trimr(log(iota - a), i , 0)\n        lr1[i] = -t * np.sum(tmp, 0)  # columnsum ?\n        # tmp = np.log(1-a)\n        # lr1[i] = -t * np.sum(tmp[i:])\n        lr2[i] = -t * log(1 - a[i])\n        cvm[i, :] = c_sja(m - i, p)\n        cvt[i, :] = c_sjt(m - i, p)\n        aind[i] = i\n    # end\n\n    result = Holder()\n    # % set up results structure\n    # estimation results, residuals\n    result.rkt = rkt\n    result.r0t = r0t\n    result.eig = a\n    result.evec = d  # transposed compared to matlab ?\n    result.lr1 = lr1\n    result.lr2 = lr2\n    result.cvt = cvt\n    result.cvm = cvm\n    result.ind = aind\n    result.meth = 'johansen'\n\n    if print_on_console == True:\n        print ('--------------------------------------------------')\n        print ('--> Trace Statistics')\n        print ('variable statistic Crit-90% Crit-95%  Crit-99%')\n        for i in range(len(result.lr1)):\n            print ('r =', i, '\\t', round(result.lr1[i], 4), result.cvt[i, 0], result.cvt[i, 1], result.cvt[i, 2])\n        print ('--------------------------------------------------')\n        print ('--> Eigen Statistics')\n        print ('variable statistic Crit-90% Crit-95%  Crit-99%')\n        for i in range(len(result.lr2)):\n            print ('r =', i, '\\t', round(result.lr2[i], 4), result.cvm[i, 0], result.cvm[i, 1], result.cvm[i, 2])\n        print ('--------------------------------------------------')\n        print ('eigenvectors:\\n', result.evec)\n        print ('--------------------------------------------------')\n        print ('eigenvalues:\\n', result.eig)\n        print ('--------------------------------------------------')\n\n\n    return result\n\ndef c_sjt(n, p):\n\n# PURPOSE: find critical values for Johansen trace statistic\n# ------------------------------------------------------------\n# USAGE:  jc = c_sjt(n,p)\n# where:    n = dimension of the VAR system\n#               NOTE: routine doesn't work for n > 12\n#           p = order of time polynomial in the null-hypothesis\n#                 p = -1, no deterministic part\n#                 p =  0, for constant term\n#                 p =  1, for constant plus time-trend\n#                 p >  1  returns no critical values\n# ------------------------------------------------------------\n# RETURNS: a (3x1) vector of percentiles for the trace\n#          statistic for [90# 95# 99#]\n# ------------------------------------------------------------\n# NOTES: for n > 12, the function returns a (3x1) vector of zeros.\n#        The values returned by the function were generated using\n#        a method described in MacKinnon (1996), using his FORTRAN\n#        program johdist.f\n# ------------------------------------------------------------\n# SEE ALSO: johansen()\n# ------------------------------------------------------------\n# # References: MacKinnon, Haug, Michelis (1996) 'Numerical distribution\n# functions of likelihood ratio tests for cointegration',\n# Queen's University Institute for Economic Research Discussion paper.\n# -------------------------------------------------------\n\n# written by:\n# James P. LeSage, Dept of Economics\n# University of Toledo\n# 2801 W. Bancroft St,\n# Toledo, OH 43606\n# jlesage@spatial-econometrics.com\n#\n# Ported to Python by Javier Garcia\n# javier.macro.trader@gmail.com\n\n# these are the values from Johansen's 1995 book\n# for comparison to the MacKinnon values\n# jcp0 = [ 2.98   4.14   7.02\n#        10.35  12.21  16.16\n#        21.58  24.08  29.19\n#        36.58  39.71  46.00\n#        55.54  59.24  66.71\n#        78.30  86.36  91.12\n#       104.93 109.93 119.58\n#       135.16 140.74 151.70\n#       169.30 175.47 187.82\n#       207.21 214.07 226.95\n#       248.77 256.23 270.47\n#       293.83 301.95 318.14];\n\n\n\n\n    jcp0 = ((2.9762, 4.1296, 6.9406),\n            (10.4741, 12.3212, 16.3640),\n            (21.7781, 24.2761, 29.5147),\n            (37.0339, 40.1749, 46.5716),\n            (56.2839, 60.0627, 67.6367),\n            (79.5329, 83.9383, 92.7136),\n            (106.7351, 111.7797, 121.7375),\n            (137.9954, 143.6691, 154.7977),\n            (173.2292, 179.5199, 191.8122),\n            (212.4721, 219.4051, 232.8291),\n            (255.6732, 263.2603, 277.9962),\n            (302.9054, 311.1288, 326.9716))\n\n\n    jcp1 = ((2.7055, 3.8415, 6.6349),\n            (13.4294, 15.4943, 19.9349),\n            (27.0669, 29.7961, 35.4628),\n            (44.4929, 47.8545, 54.6815),\n            (65.8202, 69.8189, 77.8202),\n            (91.1090, 95.7542, 104.9637),\n            (120.3673, 125.6185, 135.9825),\n            (153.6341, 159.5290, 171.0905),\n            (190.8714, 197.3772, 210.0366),\n            (232.1030, 239.2468, 253.2526),\n            (277.3740, 285.1402, 300.2821),\n            (326.5354, 334.9795, 351.2150))\n\n    jcp2 = ((2.7055, 3.8415, 6.6349),\n            (16.1619, 18.3985, 23.1485),\n            (32.0645, 35.0116, 41.0815),\n            (51.6492, 55.2459, 62.5202),\n            (75.1027, 79.3422, 87.7748),\n            (102.4674, 107.3429, 116.9829),\n            (133.7852, 139.2780, 150.0778),\n            (169.0618, 175.1584, 187.1891),\n            (208.3582, 215.1268, 228.2226),\n            (251.6293, 259.0267, 273.3838),\n            (298.8836, 306.8988, 322.4264),\n            (350.1125, 358.7190, 375.3203))\n\n\n\n    if (p > 1) or (p < -1):\n        jc = (0, 0, 0)\n    elif (n > 12) or (n < 1):\n        jc = (0, 0, 0)\n    elif p == -1:\n        jc = jcp0[n - 1]\n    elif p == 0:\n        jc = jcp1[n - 1]\n    elif p == 1:\n        jc = jcp2[n - 1]\n\n\n\n    return jc\n\ndef c_sja(n, p):\n\n# PURPOSE: find critical values for Johansen maximum eigenvalue statistic\n# ------------------------------------------------------------\n# USAGE:  jc = c_sja(n,p)\n# where:    n = dimension of the VAR system\n#           p = order of time polynomial in the null-hypothesis\n#                 p = -1, no deterministic part\n#                 p =  0, for constant term\n#                 p =  1, for constant plus time-trend\n#                 p >  1  returns no critical values\n# ------------------------------------------------------------\n# RETURNS: a (3x1) vector of percentiles for the maximum eigenvalue\n#          statistic for: [90# 95# 99#]\n# ------------------------------------------------------------\n# NOTES: for n > 12, the function returns a (3x1) vector of zeros.\n#        The values returned by the function were generated using\n#        a method described in MacKinnon (1996), using his FORTRAN\n#        program johdist.f\n# ------------------------------------------------------------\n# SEE ALSO: johansen()\n# ------------------------------------------------------------\n# References: MacKinnon, Haug, Michelis (1996) 'Numerical distribution\n# functions of likelihood ratio tests for cointegration',\n# Queen's University Institute for Economic Research Discussion paper.\n# -------------------------------------------------------\n\n# written by:\n# James P. LeSage, Dept of Economics\n# University of Toledo\n# 2801 W. Bancroft St,\n# Toledo, OH 43606\n# jlesage@spatial-econometrics.com\n# Ported to Python by Javier Garcia\n# javier.macro.trader@gmail.com\n\n\n    jcp0 = ((2.9762, 4.1296, 6.9406),\n            (9.4748, 11.2246, 15.0923),\n            (15.7175, 17.7961, 22.2519),\n            (21.8370, 24.1592, 29.0609),\n            (27.9160, 30.4428, 35.7359),\n            (33.9271, 36.6301, 42.2333),\n            (39.9085, 42.7679, 48.6606),\n            (45.8930, 48.8795, 55.0335),\n            (51.8528, 54.9629, 61.3449),\n            (57.7954, 61.0404, 67.6415),\n            (63.7248, 67.0756, 73.8856),\n            (69.6513, 73.0946, 80.0937))\n\n    jcp1 = ((2.7055, 3.8415, 6.6349),\n            (12.2971, 14.2639, 18.5200),\n            (18.8928, 21.1314, 25.8650),\n            (25.1236, 27.5858, 32.7172),\n            (31.2379, 33.8777, 39.3693),\n            (37.2786, 40.0763, 45.8662),\n            (43.2947, 46.2299, 52.3069),\n            (49.2855, 52.3622, 58.6634),\n            (55.2412, 58.4332, 64.9960),\n            (61.2041, 64.5040, 71.2525),\n            (67.1307, 70.5392, 77.4877),\n            (73.0563, 76.5734, 83.7105))\n\n    jcp2 = ((2.7055, 3.8415, 6.6349),\n            (15.0006, 17.1481, 21.7465),\n            (21.8731, 24.2522, 29.2631),\n            (28.2398, 30.8151, 36.1930),\n            (34.4202, 37.1646, 42.8612),\n            (40.5244, 43.4183, 49.4095),\n            (46.5583, 49.5875, 55.8171),\n            (52.5858, 55.7302, 62.1741),\n            (58.5316, 61.8051, 68.5030),\n            (64.5292, 67.9040, 74.7434),\n            (70.4630, 73.9355, 81.0678),\n            (76.4081, 79.9878, 87.2395))\n\n\n    if (p > 1) or (p < -1):\n        jc = (0, 0, 0)\n    elif (n > 12) or (n < 1):\n        jc = (0, 0, 0)\n    elif p == -1:\n        jc = jcp0[n - 1]\n    elif p == 0:\n        jc = jcp1[n - 1]\n    elif p == 1:\n        jc = jcp2[n - 1]\n\n\n    return jc\n","22cb46ca":"SPX = maindf['SPX(USD)']\nGLD = maindf['GLD(USD)']\nBARR= maindf['BARR(USD)']\nSLV = maindf['SLV(USD)']\n\ndf= pd.DataFrame({'x':GLD, 'y':SPX})\nprint(coint_johansen(df,0,1))","2e630e2e":"# We have 759 rows, so 20% would be a test size of ~152\nn_obs=152\nX_train, X_test = maindf[0:-n_obs], maindf[-n_obs:]\nprint(X_train.shape, X_test.shape)","6c618b6b":"X_train_transformed = X_train.diff().dropna()\nX_train_transformed.head()","3a6117e5":"X_train_transformed.describe()","1bd1ca4a":"# let's see if the ADF says it's stationary now.\ndef augmented_dickey_fuller_statistics(time_series):\n    result = adfuller(time_series.values)\n    print('ADF Statistic: %f' % result[0])\n    print('p-value: %f' % result[1])\n    print('Critical Values:')\n    for key, value in result[4].items():\n        print('\\t%s: %.3f' % (key, value))\n        \nprint('Augmented Dickey-Fuller Test: Gold Price Time Series')\naugmented_dickey_fuller_statistics(X_train_transformed['GLD(USD)'])\nprint('Augmented Dickey-Fuller Test: SPX Price Time Series')\naugmented_dickey_fuller_statistics(X_train_transformed['SPX(USD)'])\n","4b97f6ef":"# lets look at each of them displayed out to see that they are stationary\nfig, axes = plt.subplots(nrows=5, ncols=1, dpi=120, figsize=(20,20))\nfor i, ax in enumerate(axes.flatten()):\n    d = X_train_transformed[X_train_transformed.columns[i]]\n    ax.plot(d, color='red', linewidth=1)\n    ax.set_title(X_train_transformed.columns[i])\n    ax.xaxis.set_ticks_position('none')\n    ax.yaxis.set_ticks_position('none')\n    ax.spines['top'].set_alpha(0)\n    ax.tick_params(labelsize=8)\nplt.show();","5bd48925":"# now that they are stationary we can test for Granger Causality!\n\nfrom statsmodels.tsa.stattools import grangercausalitytests\nmaxlag=12\n\ntest = 'ssr-chi2test'\n\ndef grangers_causality_matrix(data, variables, test = 'ssr_chi2test', verbose=False):\n\n    dataset = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n\n    for c in dataset.columns:\n        for r in dataset.index:\n            test_result = grangercausalitytests(data[[r,c]], maxlag=maxlag, verbose=False)\n            p_values = [round(test_result[i+1][0][test][1],4) for i in range(maxlag)]\n            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n\n            min_p_value = np.min(p_values)\n            dataset.loc[r,c] = min_p_value\n\n    dataset.columns = [var + '_x' for var in variables]\n\n    dataset.index = [var + '_y' for var in variables]\n\n    return dataset\n\ngrangers_causality_matrix(X_train_transformed, variables = X_train_transformed.columns)  ","2ce71764":"# We will need to calculate the Root of the Mean Squarred Error in order to determine the best set of parameters. \n# Function for calculating the RMSE:\n\ndef RMSEfromResid(X):\n    summ = 0\n    for i in X:\n        summ+=i**2\n    return((summ\/len(X))**0.5)","82993c00":"#ARIMA Models selection\ndate = maindf.index\nX = maindf['GLD(USD)']\nsize = int(len(X)*0.8)\ntrain, test = X[0:size], X[size:len(X)]\ndate_test = date[size:]\ndef evaluate_arima_model(X, model_order):\n    model_arima = ARIMA(X, order=model_order).fit(disp=0)\n    AIC = model_arima.aic\n    BIC = model_arima.bic\n    LLF = model_arima.llf\n    RMSE = RMSEfromResid(model_arima.resid)\n    return([AIC, BIC, LLF, RMSE])\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# evaluate combinations of p, d and q values for an ARIMA model\np_values = [0,1,2,3]\nd_values = [1]\nq_values = [0,1,2]\ndata = list()\nfor p in p_values:\n    for d in d_values:\n        for q in q_values:\n            order = (p,d,q)\n            try:\n                [AIC, BIC, LLF, RMSE] = evaluate_arima_model(train, order)\n                data.append([order,AIC, BIC, LLF, RMSE])\n            except:\n                continue\n\nARIMA_Models = pd.DataFrame(data,columns=['ARIMA', 'AIC', 'BIC', 'Maximum Log-Likelihood', 'RMSE'],dtype=float)\nevaluate_arima_model(X, order)\nARIMA_Models.sort_values(by=['RMSE'])","8ff333b5":"#ARIMA Prediction\nhistory = [x for x in train]\npredictions = list()\ndata=list()\n#len_test = len(test)\nlen_test= len(test)\nfor t in range(len_test):\n    model_arima = ARIMA(endog = history, order=(2, 1, 1)).fit(disp=0)\n    output = model_arima.forecast()\n    yhat = output[0]\n    predictions.append(yhat)\n    obs = test[t]\n    history.append(obs)\n    data.append([date_test[t], obs, yhat])\n    \nRMSE = (mean_squared_error(test, predictions))**0.5\narima_results = pd.DataFrame(data,columns=['Period','Actual Price', 'Predicted Price'],dtype=float)\nprint('Test RMSE: %.3f' % RMSE)","d156c86e":"# plot\nplt.rcParams['figure.figsize'] = (20,10)\nplt.plot(date_test, test, color='Blue', label='ACTUAL', marker='x')\nplt.plot(date_test, predictions, color='green', label='PREDICTED', marker='x')\nplt.legend(loc='upper right')\nplt.show()\narimax_pred = predictions\narimax_RMSE = RMSE","d2bb39b8":"def mape(y_true, y_pred):\n    return np.mean(np.abs((y_pred - y_true) \/ y_true)) * 100\n\nprint('The Mean Absolute Percentage Error is: %.3f' % mape(np.array(test), predictions),'%.')","d9e3415c":"# Train-Test-Split:\nmaindf['diffGLD'] = maindf['GLD(USD)'].diff()\nmaindf['diffSPX'] = maindf['SPX(USD)'].diff()\ndate = maindf.index\nmaindf['SPX_lag']=maindf['diffSPX'].shift()\nmaindf.dropna(inplace=True)\nGLD_end = maindf['GLD(USD)']\nSPX_ex = maindf['SPX_lag']\nm = len(GLD_end)\nsize = int(len(GLD_end)*0.8)\ntrain, test = GLD_end[0:size], GLD_end[size:m]\nex_train, ex_test = SPX_ex[0:size], SPX_ex[size:m]\ndate_test = date[size:]\n\ndef evaluate_arimax_model(y, X, model_order):\n    model_arimax = ARIMA(endog = y, exog=X, order=model_order).fit()\n    AIC = model_arimax.aic\n    BIC = model_arimax.bic\n    LLF = model_arimax.llf\n    RMSE = RMSEfromResid(model_arimax.resid)\n    return([AIC, BIC, LLF, RMSE])\n\nwarnings.filterwarnings(\"ignore\")\np_values = [0,1,2,3]\nd_values = [1]\nq_values = [0,1,2]\ndata = list()\nfor p in p_values:\n    for d in d_values:\n        for q in q_values:\n            order = (p,d,q)\n            try:\n                [AIC, BIC, LLF, RMSE] = evaluate_arimax_model(train, ex_train, order)\n                data.append([order,AIC, BIC, LLF, RMSE])\n            except:\n                continue\n\nARIMAX_Models = pd.DataFrame(data,columns=['ARIMAX', 'AIC', 'BIC', 'Maximum Log-Likelihood', 'RMSE'],dtype=float)\n\nevaluate_arimax_model(train, ex_train, order)\nARIMAX_Models.sort_values(by=['RMSE'])","09837eb8":"# For predicting from grid search cv\nhistory = [x for x in train]\nhis_u = ex_train\npredictions = list()\ndata=list()\ntest_index = list()\nfor t in range(len(ex_test)):\n    model_arimax = ARIMA(endog = history,exog=his_u, order=(2, 1, 1)).fit(disp=0)\n    output = model_arimax.forecast(steps=1, exog=ex_test.iloc[[t]])\n    yhat = output[0]\n    predictions.append(yhat)\n    history.append(test[t])\n    test_index.append(t)\n    his_u = ex_train.append(ex_test.iloc[test_index])\n    data.append([date_test[t], test[t], yhat])\n\n\nRMSE = (mean_squared_error(test, predictions))**0.5\narima_results = pd.DataFrame(data,columns=['Period','Actual Price', 'Predicted Price'],dtype=float)\nprint('Test RMSE: %.3f' % RMSE)","c27cfc97":"# Let's visualize the accuracy of the predictions to the test data with MAPE\ndef mape(y_true, y_pred):\n    return np.mean(np.abs((y_pred - y_true) \/ y_true)) * 100\n\nprint('The Mean Absolute Percentage Error is: %.3f' % mape(np.array(test), predictions),'%.')","84308466":"# plot our calculations above to compare prediction to actual trend\nplt.rcParams['figure.figsize'] = (20,10)\nplt.plot(date_test[2:], test, color='Blue', label='ACTUAL', marker='x')\nplt.plot(date_test[2:], predictions, color='green', label='PREDICTED', marker='x')\nplt.legend(loc='upper right')\nplt.show()\narimax_pred = predictions\narimax_RMSE = RMSE","5771f8c8":"Well, they certainly don't have a Johansen cointegration score of anything signifcant. However, we could try one more method: [Granger Causality.](https:\/\/towardsdatascience.com\/granger-causality-and-vector-auto-regressive-model-for-time-series-forecasting-3226a64889a6) Granger Causality test, in short, simply test time series to see if they are useful in forecasting other time series. However, using them for anything outside of economics, as warned by Clive Granger himself, is \"ridiculous.\"","385fe18c":"Now that I have imported and sorted Gold(GLD), it's time to bring in a second dataframe and begin merging them together. \nEach of the dataframes I have for this project have different start dates, but they all have the same end date.","1fb566e5":"For a full report of the findings here and a slide deck presentation, please take a look at the project repository on my GitHub!\n\n[Report and Slide Deck](https:\/\/github.com\/Shane-McCallum\/ARIMAX-Gold-and-S-P500-Time-Series)","a1999b94":"## Part 3: Training and Modeling ARIMA vs ARIMA(X)","da3c316f":"# ARIMAX Gold and S&P500 Time Series: \n## Part 1 - Data Wrangling","bfbd4285":"## Part 4: Conclusion","93a62f1b":"For this capstone I have collected a total of five different csv files. They are:\n1. \"gold-price-last-ten-years.csv\"\n2. \"SP500 2007-2020.csv\"\n3. \"silver history.csv\"\n4. \"Barrick Gold Corp 1985-2020.csv\"\n\nThe purpose of this capstone is to conduct a small scale test on the belief that the value of GLD increases when the value of stock market indexes decreases (represented by the S&P500 index). To add some additional insight and food for thought, I have included several other sets of data: the value of silver and the stock value of Barrick Gold Corp (a gold mining company). The reasoning for including silver is to see if this trend is unique to gold, or if other precious metals increase in value as the stock market declines. The stock value of Barrick Gold Corp. is included in order to see if the stock value of gold mining companies also increases in value in correlation to the value of gold and the S&P500 index.\n\nHypothesis: When the S&P500 index goes down in value (represented by the S&P500 index), the value of gold increases.\n\nNull Hypothesis: S&P500 index value dropping will not have a statistically significant, positive effect on the stock value of Gold.\n\nAfter discovering any significant relationships between the indexes\/stocks, I will write a time series in order to see how well we can predict one based off of another.\n\nTo begin, I will clean the data in order to properly merge and compare it in a single dataframe.","41b42368":"Conclusion: Since -1.44 > -2.86, the CADF test tells us that there is not a significant cointegration between the GLD and SPX.\n\nLets see if the Johansen Test supports this.","31d211e2":"Sweet, so everything is clean now. No duplicate dates and no NaN  values. Now lets finally take a look to see if there  is any correlation among these datasets. Lets first look at a timeline graph of all the columns to see if anything stands out.\n\nI am going to save the data as a separate .csv so that we have it saved in case anything happens or we need it on a separate drive.","2d94d3c7":"A quick way to test how two time-series may be  is to check for cointergration. First with an CADF test and then a Johansen Test. [Here is the reference for a pre-written Johansen test.](https:\/\/blog.quantinsti.com\/johansen-test-cointegration-building-stationary-portfolio\/)\n\nThe ADF test is in dependent on us making the time series stationary, which we will do by differencing. It is also sensitive to which columns we assign to X and y. The Johansen Cointegration test, however, is not sensitive to which features are assigned to X or Y, and can test for cointegration among 12 time series.","e27cc870":"[Stock trending time period reference](https:\/\/www.investors.com\/how-to-invest\/investors-corner\/sell-rules-growth-stocks-break-uptrend-line\/#:~:text=A%20properly%20drawn%20trend%20line,of%20at%20least%2018%20weeks.)","9eab3319":"Well, just from looking at them, it seems like there might be some truth to this. Around the time of the first lockdowns in 2020 we can clearly see that GLD increased as SPX decreased. Along with this, it seems as Barrick Gold Corp (GOLD) and silver also trended with gold. ","510243ff":"Sweet, so we have an RMSE that, in comparison to our the magnitude of our data, is quite low. However, can we simplify this information into a form for presenting to stakeholders that is clearer and easier to understand? Yes! We can use Mean Absolute Percentage Error, or MAPE for short! More about the pros and cons of MAPE can be found [here](https:\/\/towardsdatascience.com\/choosing-the-correct-error-metric-mape-vs-smape-5328dec53fac). Bottom line is that the lower the percentage, the better our forecast model is at predicting the actual test results.","0b6e6ac6":"That's a lot easier to read and make note of correlations. It looks like the value of gold (GLD) is significantly correlated with the value of the SPX. In addition, silvers correlation to Barrick Gold Corp makes sense as they probably trade in silver as well.","d42942a5":"## Part 2: EDA","e782d510":"To understand what we have here:\n\nRow 1, column 2 refers to the p-value of the Granger's Causality test for SPX(USD)_x causing GLD(USD)_y. What we see is a p-value of 0.0388. That is a significant p-value, as it is under the level of significance (0.05)! So, according to the Granger Causality, we could reject the null hypothesis and say with 95% confidence that SPX causes GLD.\n\nAll right, so, what now? Well, the CADF and Johansen test both told us that there was no significance. However, Granger Causality did make a case for significance. perhaps the best way to conclude this is to conduct the ARIMAX test.  \n\nCredit for the cross-validation model below: [Maytsup Github](https:\/\/github.com\/maytusp\/stock)","5f232c88":"Well, overall the difference between the ARIMA and ARIMAX model's is 0.005. So, while the ARIMAX model technically outperformed the ARIMA model, and the use of S&P 500 index for predicting the value of Gold could be argued, it is not necessarily any better that predicting the value of Gold without it.","4f02c99d":"So using ffill had no significant impact on the std dev or mean for any of the columns. We can see that the strongest change was probably to silver, with its std dev going from 1.825 to 1.787.  Still, not significant. ","12358b2a":"### The issue:\nBelow is the cross validation method I am using to find the ideal parameters for testing. However, after running said cross validation, I cannot get the test to work and I repeatedly get: \n\"LinAlgError: SVD did not converge\"\nUnfortunately, this seems to be due to a recent update from statsmodels. I will return periodically to see test out solutions. However, since the model is not overly complex and we are not tracking several time series, we shouldn't have to use any p,d,q values beyond 2 or 3."}}