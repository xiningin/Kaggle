{"cell_type":{"47c58c6a":"code","046187f9":"code","a9e9f40d":"code","2bce5e08":"code","54b0d488":"code","c8f2270e":"code","8bb09ade":"code","301a02c1":"code","ee6e178f":"code","e9b895b9":"code","5bd34394":"code","0d612dc2":"code","48a35d6d":"code","c6e3012b":"code","a3f2381c":"code","d7aec9f0":"code","fc4dedcc":"code","91580f7d":"code","018cd0a8":"code","ba73149f":"code","a49f3835":"code","b6c51769":"code","5c2ce08b":"code","afd62399":"code","b965fc4e":"code","cf57bf8a":"code","6e3e12d7":"code","db1be88d":"code","49bf44ee":"code","4e4ba6b3":"code","bada8ff8":"code","a7b208d6":"code","b9f53b88":"code","25baedff":"code","e1458a44":"code","51a9c43e":"code","d54b1591":"code","5713f8de":"code","33d694bf":"code","0967b5bd":"code","d349b786":"code","6f2ab0bb":"code","61edf512":"code","c30bebaa":"code","e55312f6":"code","e17c6224":"code","24af2dd6":"markdown","6433f050":"markdown","6508094a":"markdown","dd2187a2":"markdown","fd9e4051":"markdown","0ff83cfb":"markdown","cb8e90b0":"markdown","95c73238":"markdown","279213ce":"markdown","56f2d0e5":"markdown","1b5be35b":"markdown","6b695afc":"markdown"},"source":{"47c58c6a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected=True)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","046187f9":"df1 = pd.read_csv('..\/input\/the-movies-dataset\/ratings.csv', low_memory=False)\n#df2 = pd.read_csv('..\/input\/the-movies-dataset\/links_small.csv', low_memory=False)\ndf3 = pd.read_csv('..\/input\/the-movies-dataset\/credits.csv', low_memory=False)\ndf4 = pd.read_csv('..\/input\/the-movies-dataset\/keywords.csv', low_memory=False)\ndf5 = pd.read_csv('..\/input\/the-movies-dataset\/movies_metadata.csv', low_memory=False)\n#df6 = pd.read_csv('..\/input\/the-movies-dataset\/ratings_small.csv', low_memory=False)\n#df7 = pd.read_csv('..\/input\/the-movies-dataset\/links.csv', low_memory=False)","a9e9f40d":"df5.info()","2bce5e08":"df5.dropna(subset=['title'], inplace=True)\ndf5['popularity'] = pd.to_numeric(df5['popularity'])\ndf5['budget'] = pd.to_numeric(df5['budget'])\ndf5['id'] = pd.to_numeric(df5['id'])\n\n# update genres column from json to list\ndf5['genres'] = df5['genres'].apply(lambda x: ' '.join([i['name'] for i in eval(x)]))","54b0d488":"df3.info()","c8f2270e":"# updates cast and crew columns from json to list\ndf3['cast'] = df3['cast'].apply(lambda x: ' '.join([i['name'].replace(' ', '') for i in eval(x)]))\ndf3['crew'] = df3['crew'].apply(lambda x: ' '.join([i['name'].replace(' ', '') for i in eval(x)]))","8bb09ade":"df4.info()","301a02c1":"# update keywords from json to list\ndf4['keywords'] = df4['keywords'].apply(lambda x: ' '.join([i['name'] for i in eval(x)]))","ee6e178f":"data = df5.sort_values('popularity', ascending=False)\nfig = px.bar(data.head(10), x='popularity', y='title', orientation='h', hover_name='overview')\nfig.update_yaxes(autorange='reversed')\nfig.show()","e9b895b9":"data = df5.copy()\nm = data['vote_count'].quantile(0.99)\ndata = data[data['vote_count']>m]\nprint('m =', m)\nprint(data.shape)\nR = data['vote_average']\nv = data['vote_count']\nC = data['vote_average'].mean()\ndata['weighted rating'] = (v\/(v+m))*R + (m\/(v+m))*C","5bd34394":"data = data.sort_values('weighted rating', ascending=False)\nfig = px.bar(data.head(10), x='weighted rating', y='title', orientation='h', hover_data=['vote_count', 'vote_average'])\nfig.update_yaxes(autorange='reversed')\nfig.show()","0d612dc2":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\nimport nltk\nfrom nltk.tag import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer, WordNetLemmatizer\nfrom nltk.corpus import wordnet as wn\nimport re\nfrom functools import lru_cache\nimport string\nfrom tqdm import tqdm\ntqdm.pandas()","48a35d6d":"wnl     = WordNetLemmatizer()\nporter  = PorterStemmer()\n\ndef tree_2_word(treebank_tag):\n    if treebank_tag.startswith('J'):\n        return wn.ADJ\n    elif treebank_tag.startswith('V'):\n        return wn.VERB\n    elif treebank_tag.startswith('R'):\n        return wn.ADV\n    else:\n        return wn.NOUN\n\n# memonization - to increase speed n efficiency\nlemmatize_mem = lru_cache(maxsize=16384)(wnl.lemmatize)\n\ndef lemmatize(word, pos=wn.NOUN):\n    return(lemmatize_mem(word, pos))\n\ndef text_process(t, pos=True, lemmetize=True, stem=True):\n    # Defining stop words\n    stops = set(stopwords.words(\"english\"))\n    # Remove linebreak, tab, return  \n    t = re.sub('[\\n\\t\\r]+', ' ', t)      \n    # Remove Punctuations  \n    t = re.sub('['+string.punctuation+']+', '', t)\n    t = re.sub('\\s+\\s+', ' ', t)                             # Remove double whitespace\n    t= re.sub(r'\\\\', ' ', t)                                # Remove \\ slash\n    t= re.sub(r'\\\/', ' ', t)                                # Remove \/ slash\n    \n    # Convert to lower case\n    t = t.lower()             \n    # Remove Non-letters                            \n    t = re.sub('[^a-zA-Z]',' ',t)  \n    # Sentence wise tokenize: required otherwise pos tagging will not work properly                       \n    sentence = nltk.sent_tokenize(t)                   \n    modified_sentence = \"\"\n    for s in sentence:\n        # Word Tokenization \n        words = nltk.word_tokenize(s)     \n        # checking if pos is required                \n        if pos:               \n            # Part of speech Tagging                            \n            tag_words = pos_tag(words)                        \n            lemmatized = \" \"\n            for tw in tag_words:\n                # checking stop word\n                if tw[0] not in stops:\n                    # lemmatization\n                    if lemmetize:                         \n                        lemma = lemmatize(tw[0], tree_2_word(tw[1]))  \n                        # check if lemmetizer word is present in the wordnet library if not present then stemming is required\n                        # example running not in synsets: stem to run  \n                        # stemming check if word is in library then perform otherwise skip\n                        if (stem) & (wn.synsets(lemma)==[]) & (wn.synsets(porter.stem(lemma))!=[]):\n                            # Stemming\n                            lemma = porter.stem(tw[0])    \n                    else:\n                        if stem:\n                            lemma = porter.stem(tw[0])\n                            if not wn.synsets(lemma):\n                                lemma = tw[0]\n                        else:\n                            lemma = tw[0]\n                    lemmatized = lemmatized+\" \"+lemma\n        else:\n            lemmatized = \" \"\n            for w in words:\n                if w not in stops:\n                    if lemmetize:                         # lemmatization\n                        lemma = lemmatize(w)\n                        if (stem) & (wn.synsets(lemma)==[]) & (wn.synsets(porter.stem(lemma))!=[]):\n                            # Stemming\n                            lemma = porter.stem(w)    \n                    else:\n                        if stem:\n                            lemma = porter.stem(w)\n                            if not wn.synsets(lemma):\n                                lemma = w\n                        else:\n                            lemma = w\n                    lemmatized = lemmatized+\" \"+lemma               \n        modified_sentence = modified_sentence+\" \"+lemmatized\n    return modified_sentence","c6e3012b":"# get top 15000 movies based on popularity\ndata = df5.sort_values('popularity', ascending=False).reset_index(drop=True)\ndata = data.iloc[:15000].copy()\n\n# drop duplicates based on title\ndata.drop_duplicates(subset=['title'], inplace=True)\n\n# replace NaN values with empty string\ndata['overview'] = data['overview'].fillna('')\n\n# cleaning the text\ndata['overview'] = data['overview'].progress_apply(text_process)","a3f2381c":"# convert overview column data to matrix to be able to compute similarity after removing the stop words\ntfidf = TfidfVectorizer(ngram_range=(1,2),min_df=0.0001)\ntfidf_matrix = tfidf.fit_transform(data['overview'])\n\nprint(tfidf_matrix.shape)","d7aec9f0":"# compute similarity matrix\n# options available - euclidean distance, the Pearson Coeffiecient, cosine similarity score\n\n# compute cosine similarity matrix\ncosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n\n# create reverse title indices series\nindices = pd.Series(data.index, index=data['title'])","fc4dedcc":"def get_recommendation(title):\n    # get index of given title\n    idx = indices[title]\n    \n    # get similarity scores along with indices\n    sim_scores = list(enumerate(cosine_sim[idx]))\n    \n    # sort the similarity scores\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    \n    # get top 10 similarity scores\n    sim_scores = sim_scores[1:11]\n    \n    # get movie indices\n    movie_indices = [i[0] for i in sim_scores]\n    movie_scores = [i[1] for i in sim_scores]\n    \n    # get movie titles\n    movies = data.iloc[movie_indices]['title']\n    \n    return pd.DataFrame({'Recommended Movie':movies, 'Similarity Score':movie_scores})","91580f7d":"get_recommendation('Minions')","018cd0a8":"get_recommendation('Iron Man')","ba73149f":"# merge df3, df4 and df5\ndata = pd.merge(df3, df4, on='id')\ndata = pd.merge(data, df5, on='id')\ndata = data.sort_values('vote_count', ascending=False).drop_duplicates(subset='id').reset_index()\ndata[['title', 'genres', 'cast', 'crew', 'keywords']].head()","a49f3835":"data['text'] = data['genres']+\" \"+data['cast']+\" \"+data['crew']+\" \"+data['keywords']","b6c51769":"data = data.iloc[:20000].copy()\n\n# drop duplicates based on title\ndata.drop_duplicates(subset=['title'], inplace=True)\n\n# replace NaN values with empty string\ndata['text'] = data['text'].fillna('')\n\n# cleaning the text\ndata['text'] = data['text'].progress_apply(text_process)","5c2ce08b":"# convert overview column data to matrix to be able to compute similarity after removing the stop words\ntfidf = TfidfVectorizer(ngram_range=(1,2), min_df=0.0001)\ntfidf_matrix = tfidf.fit_transform(data['text'])\n\nprint(tfidf_matrix.shape)","afd62399":"# compute similarity matrix\n# options available - euclidean distance, the Pearson Coeffiecient, cosine similarity score\n\n# compute cosine similarity matrix\ncosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n\n# create reverse title indices series\nindices = pd.Series(data.index, index=data['title'])","b965fc4e":"get_recommendation(\"Tangled\")","cf57bf8a":"get_recommendation('Spectre')","6e3e12d7":"from sklearn.metrics.pairwise import cosine_similarity","db1be88d":"# merge df1 and df5 to get movie titles and drop rows for which title is not available\ndata = pd.merge(df1, df5[['id', 'title']], left_on='movieId', right_on='id')\n\n# get total counts of no. of occurence of movie\ndata['count'] = data.groupby('movieId').transform('count')['userId']\n\n# fetch top 100 movies based on count\nmovieId = data.drop_duplicates('movieId').sort_values(\n    'count', ascending=False).iloc[:100]['movieId']\n\n# filter out data as per the movieId\ndata = data[data['movieId'].isin(movieId)].reset_index(drop=True)\n\n# get total counts of movies each user has seen\ndata['count'] = data.groupby('userId').transform('count')['movieId']\n\n# fetch top 20000 users based on no. of movies watched\nuserId = data.drop_duplicates('userId').sort_values(\n    'count', ascending=False).iloc[:20001]['userId']\n\n# filter out data as per the userId\ndata = data[data['userId'].isin(userId)].reset_index(drop=True)","49bf44ee":"# create a user movie rating matrix\ndf = data.pivot(index='userId', columns='movieId', values='rating')\ndf.head()","4e4ba6b3":"# replace NaN with user based average rating\ndf_imputed = df.fillna(df.mean(axis=0))\n\n# get similarity between all users\nsimilarity_matrix = cosine_similarity(df_imputed.values)","bada8ff8":"def get_recommendation(user_index):\n    idx = user_index\n    sim_scores = list(enumerate(similarity_matrix[idx]))\n\n    # get movies that are unrated by the given user\n    unrated_movies = df.iloc[idx][df.iloc[idx].isna()].index\n\n    # get weighted ratings of unrated movies by all other users\n    movie_ratings = (df_imputed.iloc[similarity_matrix[idx]][unrated_movies].T * [\n        x[1] for x in sim_scores]).T\n\n    # get top 100 similar users by skipping the current user\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:101]\n\n    # get mean of movie rating by top 100 most similar users for the unrated movies\n    movie_ratings = movie_ratings.iloc[[x[0] for x in sim_scores]].mean()\n\n    # get recommended movie titles in sorted order\n    recommended_movies = df5[df5['id'].isin(movie_ratings.reset_index().sort_values(\n        0, ascending=False)['movieId'])][['title', 'id']]\n    assumed_ratings = sorted(movie_ratings, reverse=True)\n\n    return pd.DataFrame({'movieId':recommended_movies[:10]['id'], \n                         'Recommended Movie':recommended_movies[:10]['title'], \n                         'Assumed Rating':assumed_ratings[:10]})","a7b208d6":"user_id = 101\nrecommended_movies = get_recommendation(user_id).reset_index(drop=True)\n# get other high rated movies by user\ntemp = data[data['userId']==df.index[user_id]].sort_values(\n    'rating', ascending=False)[['rating', 'title', 'userId']].iloc[:10].reset_index(drop=True)\nrecommended_movies['userId'] = temp['userId']\nrecommended_movies['Movie Watched'] = temp['title']\nrecommended_movies['Rated']= temp['rating']\nrecommended_movies","b9f53b88":"user_id = 15554\nrecommended_movies = get_recommendation(user_id).reset_index(drop=True)\n# get other high rated movies by user\ntemp = data[data['userId']==df.index[user_id]].sort_values(\n    'rating', ascending=False)[['rating', 'title', 'userId']].iloc[:10].reset_index(drop=True)\nrecommended_movies['userId'] = temp['userId']\nrecommended_movies['Movie Watched'] = temp['title']\nrecommended_movies['Rated']= temp['rating']\nrecommended_movies","25baedff":"from surprise import Reader, Dataset, SVD, SVDpp, SlopeOne, NMF, NormalPredictor, KNNBaseline, \\\nKNNBasic, KNNWithMeans, KNNWithZScore, BaselineOnly, CoClustering\nfrom surprise.model_selection import train_test_split, cross_validate","e1458a44":"reader = Reader(rating_scale=(0, 10))\nsurprise_data = Dataset.load_from_df(data[['userId', 'movieId', 'rating']], reader)\ntrainset, testset = train_test_split(surprise_data, test_size=0.25)","51a9c43e":"benchmark = []\n# Iterate over all algorithms\nfor algorithm in [SVD()]:\n#KNNBaseline(), CoClustering(), SVDpp(), SlopeOne(), NMF(), NormalPredictor(), KNNBasic(), KNNWithMeans(), KNNWithZScore(), \n#BaselineOnly()\n    # Perform cross validation\n    results = cross_validate(algorithm, surprise_data, measures=['RMSE'], cv=3, verbose=False)\n    \n    # Get results & append algorithm name\n    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n    tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n    benchmark.append(tmp) \n    print(tmp)\n    \npd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse') ","d54b1591":"svd = SVD() \nsvd.fit(trainset)","5713f8de":"index_val = 101\nuserId = df.index[index_val]\nmovies = []\nratings = []\ntitles = []\n\nfor movieId in df.iloc[index_val][df.iloc[index_val].isna()].index:\n    movies.append(movieId)\n    title = df5[df5['id']==movieId]['title'].values[0]\n    titles.append(title)\n    ratings.append(svd.predict(userId, movieId).est)\n\nprediction = pd.DataFrame({'movieId':movies, 'title':titles, 'rating':ratings, 'userId':userId}) \nprediction = prediction.sort_values('rating', ascending=False).reset_index(drop=True).iloc[:10]\n\n# get other high rated movies by user\ntemp = data[data['userId']==userId].sort_values(\n    'rating', ascending=False)[['rating', 'title', 'userId']].iloc[:10].reset_index(drop=True)\nprediction['Movie Watched'] = temp['title']\nprediction['Rated']= temp['rating']\nprediction","33d694bf":"index_val = 15554\nuserId = df.index[index_val]\ndf.iloc[index_val][df.iloc[index_val].isna()].index\nmovies = []\nratings = []\ntitles = []\n\nfor movieId in df.iloc[index_val][df.iloc[index_val].isna()].index:\n    movies.append(movieId)\n    title = df5[df5['id']==movieId]['title'].values[0]\n    titles.append(title)\n    ratings.append(svd.predict(userId, movieId).est)\n\nprediction = pd.DataFrame({'movieId':movies, 'title':titles, 'rating':ratings, 'userId':userId})  \nprediction = prediction.sort_values('rating', ascending=False).reset_index(drop=True).iloc[:10]\n\n# get other high rated movies by user\ntemp = data[data['userId']==userId].sort_values(\n    'rating', ascending=False)[['rating', 'title', 'userId']].iloc[:10].reset_index(drop=True)\nprediction['Movie Watched'] = temp['title']\nprediction['Rated']= temp['rating']\nprediction","0967b5bd":"from scipy.sparse import csr_matrix\nfrom lightfm import LightFM","d349b786":"# prepare item features\nmoviesId = df.columns\n\n# get overview of the movies\nmovies_metadata = df5[df5['id'].isin(moviesId)]\n\n# replace NaN values with empty string\nmovies_metadata['overview'] = movies_metadata['overview'].fillna('')\n\n# cleaning the text\nmovies_metadata['overview'] = movies_metadata['overview'].progress_apply(text_process)\n\n# convert overview column data to matrix\ntfidf = TfidfVectorizer()#ngram_range=(1,1), min_df=0.0001)\ntfidf_matrix = tfidf.fit_transform(movies_metadata['overview'])\n\nprint(tfidf_matrix.shape)","6f2ab0bb":"csr_data = csr_matrix(df_imputed.values) \ncsr_data","61edf512":"# Instantiate and train the model\nmodel = LightFM(loss='warp')\nmodel.fit(csr_data, item_features=tfidf_matrix, epochs=15, num_threads=2, verbose=True)","c30bebaa":"def get_recommendation(user_index):\n    idx = user_index\n    # get scores for all the movies\n    n_users, n_items = csr_data.shape\n    scores = pd.Series(model.predict(0, np.arange(n_items), item_features=tfidf_matrix))\n    scores.index = df.columns\n    scores = scores.reset_index()\n\n    # get movies that are unrated by the given user\n    unrated_movies = df.iloc[idx][df.iloc[idx].isna()].index\n\n    scores = scores[scores['movieId'].isin(unrated_movies)].reset_index(drop=True)\n    scores = scores.sort_values(0, ascending=False).iloc[:10]\n\n    # get recommended movie titles in sorted order\n    recommended_movies = df5[df5['id'].isin(scores.movieId)][['title', 'id']]\n\n    return pd.DataFrame({'movieId':recommended_movies['id'], \n                         'Recommended Movie':recommended_movies['title']})","e55312f6":"user_id = 1156\ntemp = df.reset_index()\nuser_idx = int(temp[temp['userId']==user_id].index[0])\nprint('index val:', user_idx)\nrecommended_movies = get_recommendation(user_idx).reset_index(drop=True)\n# get other high rated movies by user\ntemp = data[data['userId']==user_id].sort_values(\n    'rating', ascending=False)[['rating', 'title', 'userId']].iloc[:10].reset_index(drop=True)\nrecommended_movies['userId'] = temp['userId']\nrecommended_movies['Movie Watched'] = temp['title']\nrecommended_movies['Rated']= temp['rating']\nrecommended_movies","e17c6224":"user_id = 210430\ntemp = df.reset_index()\nuser_idx = int(temp[temp['userId']==user_id].index[0])\nprint('index val:', user_idx)\nrecommended_movies = get_recommendation(user_idx).reset_index(drop=True)\n# get other high rated movies by user\ntemp = data[data['userId']==user_id].sort_values(\n    'rating', ascending=False)[['rating', 'title', 'userId']].iloc[:10].reset_index(drop=True)\nrecommended_movies['userId'] = temp['userId']\nrecommended_movies['Movie Watched'] = temp['title']\nrecommended_movies['Rated']= temp['rating']\nrecommended_movies","24af2dd6":"## 1. Based on Overview","6433f050":"# 3. Using LightFM\nUsing the user ratings along with the movie overview","6508094a":"## 1. Based on Popularity (Trending movies)","dd2187a2":"# Recommender 3: Collaborative Filtering\nBased on records from various users provide recommendations based on user similarities","fd9e4051":"## 2. Using Surprise library","0ff83cfb":"## 2. Based on average rating & number of votes (IMDB Rating)\ngenerating a new metric score as per the imdb weighted rating<br>\n**weighted rating (WR)** = (v \/ (v+m)) \u00d7 R + (m \/ (v+m)) \u00d7 C<br>\nwhere:<br> \nR = average for the movie (mean) = (Rating) <br>\nv = number of votes for the movie = (votes) <br>\nm = minimum votes required to be listed in the Top 250 (more votes than 99% of movies)<br>\nC = mean vote across all movies","cb8e90b0":"## 2. Based on Credits, Genres and Keywords","95c73238":"* Dataset 1 - ratings(26024289)\n    * userId\n    * movieId\n    * rating\n    * timestamp\n* Dataset 2 - links_small(9125)\n    * movieId\n    * imdbId\n    * tmdbId\n* Dataset 3 - credits(45476)\\\n    * cast\n    * crew\n    * id - movieId\n\n* Dataset 4 - keywords(46419)\n    * id - movieId\n    * keywords\n* Dataset 5 - movies_metadata(45466)\n    * adult - is movie adult rated ?\n    * belongs_to_collection - \n    * budget\n    * genres\n    * homepage - homepage link\n    * id - movieId\n    * imdb_id\n    * original_language\n    * original_title\n    * overview\n    * popularity\n    * poster_path\n    * production_companies\n    * production_countries\n    * release_date\n    * revenue\n    * runtime\n    * spoken_languages\n    * status\n    * tagline\n    * title\n    * video\n    * vote_average - rating out of 10\n    * vote_count - total number of votes\n* Dataset 6 - ratings_small(100004)\n    * userId\n    * movieId\n    * rating\n    * timestamp\n* Dataset 7 - links(45843)\n    * movieId\n    * imdbId\n    * tmdbId","279213ce":"## 1. Using mean of other user' weighted ratings based on similarity matrix","56f2d0e5":"**Updates-**\n* drop values without a title\n* popularity, budget, id - convert to numeric","1b5be35b":"# Recommendor 1 : Demographic Filtering\nGeneralized recommendation to all users","6b695afc":"# Recommender 2 : Content Based Filtering\nRecommending based on the content of the movie (overview, cast, crew, keyword, tagline, genre etc)"}}