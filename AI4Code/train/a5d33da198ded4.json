{"cell_type":{"580fe77b":"code","08a7e2ad":"code","0a76c988":"code","489b4a8c":"code","93001111":"code","5892b705":"code","ac05c141":"code","5b737400":"code","b10c5d77":"code","753ff0eb":"code","26894d80":"code","0dece5d6":"code","2eec66b9":"code","68e07912":"code","cd1154d2":"code","dcc63d3f":"code","4631e69b":"code","35c61909":"code","389f7431":"code","30f926b4":"code","391d6c65":"code","4de561a9":"code","cfc0ea98":"code","4c2a49e1":"code","11b0c211":"code","1eef6e9e":"code","d8ac3e32":"code","936dc828":"code","1065237b":"code","af1689b8":"code","f8e573ff":"code","4afd335b":"code","0240b58d":"code","4e18d94c":"code","9baacdb3":"markdown","08551bd1":"markdown","20e8f69b":"markdown","de8d2b5f":"markdown"},"source":{"580fe77b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","08a7e2ad":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport spacy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression, Lasso, Huber, RidgeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import *\nfrom nltk.stem.snowball import SnowballStemmer\nimport tensorflow as tf\nimport json","0a76c988":"nltk.download('stopwords')\nnltk.download('wordnet')\n! python -m spacy download en_core_web_md","489b4a8c":"def parse_data(file):\n    for l in open(file,'r'):\n        yield json.loads(l)\n\ndata_v1 = list(parse_data('\/kaggle\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset.json'))\ndata_v2 = list(parse_data('\/kaggle\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json'))","93001111":"def create_dataframe(list_of_dicts):\n  all_headlines = [i['headline'] for i in list_of_dicts]\n  all_labels = [i['is_sarcastic'] for i in list_of_dicts]\n  return pd.DataFrame({\"Text\":all_headlines, \"Label\":all_labels})","5892b705":"train_df = create_dataframe(data_v1)\ntest_df = create_dataframe(data_v2)","ac05c141":"print(train_df.head())\nprint(test_df.head())","5b737400":"print(train_df.shape, test_df.shape)","b10c5d77":"all_data = pd.concat([train_df, test_df], axis=0)\nprint(all_data.shape)","753ff0eb":"sns.countplot(x='Label', data=all_data);\nplt.title('Target Distribution')","26894d80":"def clean_text(x):\n  \n  #x = re.sub(r\"\\W\", \" \", x)\n  x = re.sub(r\"[0-9]\", \" \", x)\n  x = re.sub(r\"\\s{2,}\", \" \", x)\n  return x.strip()\n\ndef clean_text2(x):\n  \n  #x = re.sub(r\"\\W\", \" \", x)\n  #x = re.sub(r\"[0-9]\", \" \", x)\n  x = re.sub(r\"\\s{2,}\", \" \", x)\n  x = re.sub(r\" s \", \" \", x)\n  \n  return x.strip()","0dece5d6":"#all_clean_text = [clean_text2(clean_text(i)) for i in all_data['Text'].tolist()]\nall_clean_text = all_data['Text'].tolist()","2eec66b9":"tfidf = TfidfVectorizer(ngram_range=(2, 3), max_df=1000, min_df=10)\ntfidf.fit(all_clean_text)","68e07912":"tfidf_feats = tfidf.transform(all_clean_text)","cd1154d2":"trainx, testvalx, trainy, testvaly = train_test_split(tfidf_feats, all_data['Label'].tolist(), test_size=0.4)\nvalx, testx, valy, testy = train_test_split(testvalx, testvaly, test_size=0.5)","dcc63d3f":"lr = LogisticRegression(C=10)\nlr.fit(trainx, trainy)","4631e69b":"print(f\"Validation Set Score: {lr.score(valx, valy)}\")\nprint(f\"Test Set Score: {lr.score(testx, testy)}\")","35c61909":"rf = RandomForestClassifier(n_estimators=200)\nrf.fit(trainx, trainy)\nprint(f\"Validation Set Score: {rf.score(valx, valy)}\")\nprint(f\"Test Set Score: {rf.score(testx, testy)}\")","389f7431":"!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n!unzip -q glove.6B.zip","30f926b4":"#Load pretrained GloVe embeddings\ndict_w2v = {}\nwith open('glove.6B.100d.txt', \"r\") as file:\n    for line in file:\n        tokens = line.split()\n        word = tokens[0]\n        vector = np.array(tokens[1:], dtype=np.float32)\n        if vector.shape[0] == 100:\n            dict_w2v[word] = vector\n        else:\n            print(\"There was an issue with \" + word)\n# let's check the vocabulary size\nprint(\"Dictionary Size: \", len(dict_w2v))","391d6c65":"NUM_WORDS = 10000\nNUM_CLS = 2","4de561a9":"tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=\"\")\ntokenizer.fit_on_texts(all_clean_text)","cfc0ea98":"NUM_WORDS = len(tokenizer.word_index) + 1\nprint(NUM_WORDS)","4c2a49e1":"embedding_dim = 100\nembedding_matrix = np.zeros((NUM_WORDS, embedding_dim))\n\nunk_cnt = 0\nunk_set = set()\nfor word in tokenizer.word_index.keys():\n    embedding_vector = dict_w2v.get(word)\n    if embedding_vector is not None:\n        tkn_id = tokenizer.word_index[word]\n        embedding_matrix[tkn_id] = embedding_vector\n    else:\n        unk_cnt += 1\n        unk_set.add(word)\n# Print how many weren't found\nprint(\"Total unknown words: \", unk_cnt)","11b0c211":"all_sequences = tokenizer.texts_to_sequences(all_clean_text)\nall_padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(all_sequences)","1eef6e9e":"train_x, valtest_x, train_y, valtest_y = train_test_split(all_padded_sequences, np.asarray(all_data['Label'].tolist(), dtype=np.int32), test_size=0.4)\nval_x, test_x, val_y, test_y = train_test_split(valtest_x, valtest_y, test_size=0.5)","d8ac3e32":"print(train_x.shape, val_x.shape, test_x.shape, val_y.shape, train_y.shape, test_y.shape)\ntrain_x, train_y = train_x[:(train_x.shape[0]\/\/32)*32, :], train_y[:(train_y.shape[0]\/\/32)*32]\nval_x, val_y = val_x[:(val_x.shape[0]\/\/32)*32, :], val_y[:(val_y.shape[0]\/\/32)*32]\ntest_x, test_y = test_x[:(test_x.shape[0]\/\/32)*32, :], test_y[:(test_y.shape[0]\/\/32)*32]","936dc828":"class MergeHiddenStates(tf.keras.layers.Layer):\n  def __init__(self):\n    super(MergeHiddenStates, self).__init__()\n  \n  def call(self, inputs):\n    #print(inputs)\n    states = inputs#[0]\n    return tf.reduce_mean(states, axis=1)","1065237b":"def create_model(input_shape=(train_x.shape[1], ), vocabsize=NUM_WORDS, emb_dim=50, rnn_units=128, batch_size=32):\n  model = tf.keras.Sequential([\n                               tf.keras.layers.Embedding(\n                                   vocabsize,\n                                   emb_dim,\n                                   mask_zero=False,\n                                   batch_input_shape=(batch_size, input_shape[0]),\n                                   weights=[embedding_matrix], trainable=True\n                               ),\n                               tf.keras.layers.Bidirectional(tf.keras.layers.GRU(rnn_units, dropout=0.2, return_sequences=True)),\n                               MergeHiddenStates(),\n                               tf.keras.layers.Dropout(0.3),\n                               tf.keras.layers.Dense(128, activation='relu'),\n                               tf.keras.layers.Dropout(0.2),\n                               tf.keras.layers.Dense(NUM_CLS, activation='softmax')\n  ])\n  return model\n\ndef create_model_v2(input_shape=(train_x.shape[1], ), vocabsize=NUM_WORDS, emb_dim=100, rnn_units=128, batch_size=32):\n  \n  inp = tf.keras.layers.Input(shape=input_shape, batch_size=batch_size, dtype=tf.int32)\n  emb_fixed = tf.keras.layers.Embedding(\n                                    vocabsize,\n                                    emb_dim,\n                                    mask_zero=False,\n                                    batch_input_shape=(batch_size, input_shape[0]),\n                                    weights=[embedding_matrix], trainable=False)\n  \n  emb_train = tf.keras.layers.Embedding(\n                                    vocabsize,\n                                    emb_dim,\n                                    mask_zero=False,\n                                    batch_input_shape=(batch_size, input_shape[0]),\n                                    weights=[embedding_matrix], trainable=True)\n  \n  rnn_unit = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(rnn_units, dropout=0.2, return_sequences=True))\n  \n  x1 = emb_fixed(inp)\n  x2 = emb_train(inp)\n  x = tf.keras.layers.Concatenate()([x1, x2])\n  whole_sequence_output = rnn_unit(x)\n  x = MergeHiddenStates()(whole_sequence_output)\n  x = tf.keras.layers.Dropout(0.3)(x)\n  x = tf.keras.layers.Dense(128, activation='relu')(x)\n  x = tf.keras.layers.Dropout(0.2)(x)\n  preds = tf.keras.layers.Dense(NUM_CLS, activation='softmax')(x)\n  model = tf.keras.Model(inputs=inp, outputs=preds)\n  return model","af1689b8":"#model = create_model()\nmodel = create_model_v2()\nprint(model.summary())","f8e573ff":"loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\nmodel.compile(loss=loss_obj, optimizer='adam', metrics=['accuracy'])","4afd335b":"history = model.fit(train_x, train_y, batch_size=32, epochs=5, validation_data=(val_x, val_y), shuffle=True)","0240b58d":"test_preds = np.argmax(model.predict(test_x, batch_size=32), axis=1)","4e18d94c":"print(classification_report(test_y, test_preds))","9baacdb3":"Constructive criticism and suggestions are welcome!","08551bd1":"# Preprocessing","20e8f69b":"# Sequential Model","de8d2b5f":"# Baseline - TFIDF with Logistic Regression"}}