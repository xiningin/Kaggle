{"cell_type":{"3852dc41":"code","c0022e1d":"code","89e51da6":"code","d5887fc7":"code","cabbaf09":"code","bae1f131":"code","b66c489b":"code","134f8707":"code","182cca89":"code","c4a3dd89":"code","fcb32746":"code","10fcf21a":"code","226e33ac":"code","cb89c6bb":"code","3af3856b":"code","04a41c3a":"code","edcc941a":"code","f784261c":"code","5a1f95b4":"code","e2d9727f":"code","73ae98a1":"code","515ce5ec":"code","d3752d84":"code","870d1abf":"code","f9968ac2":"code","6fc3963e":"code","062d5a55":"code","ad7e6895":"code","7b4ccdbf":"code","ec3285ef":"code","dc6b8613":"markdown","30deec2f":"markdown","67adbf8b":"markdown","75eef0fe":"markdown","d462991f":"markdown","452ab0c4":"markdown","285896b7":"markdown","696b511d":"markdown"},"source":{"3852dc41":"# Check the GPU is available or not\n# IMPORT\nimport tensorflow as tf \n\n# FETCH GPU NAME\ndevice_name = tf.test.gpu_device_name()\n\n# IF DEVICE IS NOT AVAILABLE FETCH ERROR\nif device_name != '\/device:GPU:0':\n    raise SystemError('GPU device not found')\nprint('Found GPU at: {}'.format(device_name))","c0022e1d":"# IMPORT DEPENDENCIES\n\nimport requests, zipfile, io\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom scipy import signal\nfrom scipy.io import wavfile\nimport numpy as np\nfrom tqdm import tqdm\nimport cv2\nimport pandas as pd\nseed = 7\nimport pandas as pd\nnp.random.seed(seed)\nimport os","89e51da6":"zip_file_url = 'https:\/\/github.com\/karoldvl\/ESC-50\/archive\/master.zip' # link: ESC-50 Datset","d5887fc7":"# CREATE FOLDER WITH NAME SOUND IF NOT EXIST\nif not os.path.exists('sound'):\n    os.makedirs('sound')","cabbaf09":"# UNZIP THE DATA AND EXTRACT ALL FILES IN THE SOUND FOLDER\nr = requests.get(zip_file_url)\nz = zipfile.ZipFile(io.BytesIO(r.content))\nz.extractall('sound\/')\nz.close()","bae1f131":"# glob('sound\/ESC-50-master\/audio\/*')","b66c489b":"# !pip install librosa\nimport librosa","134f8707":"# GENERATOR FUNCTION\ndef windows(data, window_size):\n    start = 0\n    while start < len(data):\n        yield int(start), int(start + window_size)\n        start += (window_size \/ 2)\n        print(start)\n\n# EXTRACT THE FEATURE OF SOUND USING MELSPECTROGRAM TECHNIQUE\ndef extract_features(bands = 60, frames = 41):\n    window_size = 512 * (frames - 1)\n    log_specgrams = []\n    labels = []\n    for fn in tqdm(glob('sound\/ESC-50-master\/audio\/*')):\n        sound_clip,s = librosa.load(fn) # 5sec\n        sound_clip   = np.concatenate((sound_clip,sound_clip),axis=None) # make it 10s\n        label = fn.split(\"\/\")[-1].split(\"-\")[-1].split(\".\")[0]\n        for (start,end) in windows(sound_clip,window_size):\n            if(len(sound_clip[start:end]) == window_size):\n                signal = sound_clip[start:end]\n                melspec = librosa.feature.melspectrogram(signal, n_mels = bands)\n                logspec = librosa.core.amplitude_to_db(melspec)\n                logspec = logspec.T.flatten()[:, np.newaxis].T\n                log_specgrams.append(logspec)\n                labels.append(label)\n            \n    log_specgrams = np.asarray(log_specgrams).reshape(len(log_specgrams),bands,frames,1)\n    features = np.concatenate((log_specgrams, np.zeros(np.shape(log_specgrams))), axis = 3)\n    for i in range(len(features)):\n        features[i, :, :, 1] = librosa.feature.delta(features[i, :, :, 0])\n    \n    return np.array(features), np.array(labels,dtype = np.int)","182cca89":"features,labels = extract_features()","c4a3dd89":"# label category names\ndf = pd.read_csv(glob('sound\/ESC-50-master\/meta\/esc50.csv')[0])\ndf = df[['target','category']]\ndf = df.drop_duplicates().reset_index(drop=True)\ndf = df.sort_values(by=['target']).reset_index(drop=True)\ndf.head()","fcb32746":"my_dict = {}\nfor i in range(len(df)):\n    my_dict[df['target'][i]] = df['category'][i]\nmy_dict","10fcf21a":"seed = 4\nrng = np.random.RandomState(seed)\nfrom keras.utils import to_categorical","226e33ac":"onehot_labels = to_categorical(labels,num_classes=50)","cb89c6bb":"# Create train test Dataset\n\nrnd_indices = np.random.rand(len(labels)) < 0.70\n\nX_train = features[rnd_indices]\ny_train = onehot_labels[rnd_indices]\nX_test  = features[~rnd_indices]\ny_test  = onehot_labels[~rnd_indices]","3af3856b":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","04a41c3a":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten,InputLayer\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.utils import np_utils\nfrom keras.optimizers import SGD, Adam, RMSprop\nfrom keras.constraints import maxnorm\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.initializers import normal, VarianceScaling\nfrom keras import initializers\nfrom keras.layers import BatchNormalization\n### initializer of weights\n# initializer = normal(mean=0, stddev=0.01, seed=13)\n# initializer = VarianceScaling(scale=1.0, mode='fan_in', distribution='normal', seed=None)","edcc941a":"def basemodel():\n    model = Sequential()\n    model.add(Conv2D(32, (2, 2), input_shape=(60,41,2), activation='relu', padding='same', kernel_initializer=initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None), bias_initializer='ones'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(32, (2, 2), activation='relu', padding='same'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(64, (2, 2), activation='relu', padding='same'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, (2, 2), activation='relu', padding='same'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(128, (2, 2), activation='relu', padding='same'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(128, (2, 2), activation='relu', padding='same'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n    model.add(Dropout(0.25))\n\n    model.add(Flatten())\n    model.add(BatchNormalization())\n    model.add(Dropout(0.25))\n    model.add(Dense(1024, activation='relu', kernel_constraint=maxnorm(3)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.25))\n    model.add(Dense(512, activation='relu', kernel_constraint=maxnorm(3)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.25))\n    model.add(Dense(50, activation='softmax'))\n    \n    # Compile model\n    epochs, lrate = 25, 0.01\n    decay = lrate\/epochs\n    \n#     sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=decay, amsgrad=False)\n#     rms = RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=decay)\n    model.compile(loss='categorical_crossentropy', optimizer = adam, metrics=['accuracy'])\n    return model","f784261c":"if not os.path.exists('model'):\n    os.makedirs('model')\n    \nfilepath=\"model\/weights_0.best.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\ncallbacks_list = [checkpoint]","5a1f95b4":"model = basemodel()\n# model.load_weights('.\/model\/weights_0.best.hdf5')\nprint(model.summary())","e2d9727f":"from keras.preprocessing.image import ImageDataGenerator","73ae98a1":"datagen = ImageDataGenerator(\n              width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n              height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n              horizontal_flip=True,  # randomly flip images\n              vertical_flip=False  # randomly flip images\n          )","515ce5ec":"# init the batch size and epochs\n\n'''\nNote: Due to Memory Error like Buffered data was truncated after reaching the output size limit. What i did is that Save the model in for example 60th epoch and close current program and run new program and restore saved model and train model from 61 epoch to 120 epoch and \nsave that and close program and repeat this work for your interested epoch For this [100,50] three times repeat \n\n'''\nbatch_size = 50\nepochs = 100","d3752d84":"# fit the model\nhistory = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n                              steps_per_epoch=int(np.ceil(X_train.shape[0] \/ float(batch_size))),\n                              epochs=epochs,\n                              validation_data=(X_test, y_test),\n                              verbose=1,callbacks=callbacks_list)\n ","870d1abf":"import matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\n# Plot training & validation accuracy values\nplt.figure(figsize=(15,6))\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.figure(figsize=(15,6))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","f9968ac2":"# evaluate model\nmodel.evaluate(X_test, y_test)","6fc3963e":"from sklearn.metrics import classification_report, confusion_matrix","062d5a55":"y_pred = model.predict_classes(X_test)","ad7e6895":"target_name = np.array(df['category'])","7b4ccdbf":"print(classification_report(np.argmax(y_test,axis=1),y_pred,target_names=target_name))","ec3285ef":"import seaborn as sns\ncn_matrix = confusion_matrix(np.argmax(y_test,axis=1),y_pred)\nplt.figure(figsize = (40,40))\nsns.heatmap(cn_matrix[:10,:10], annot=True,annot_kws={\"size\": 25})","dc6b8613":"Regarding fixed size input, we will divide each sound clip into segments of 60x41 (60 rows and 41 columns). The mel-spec and their deltas will become two channels, which we will be fed into CNN","30deec2f":"# Training with Data Augmentation","67adbf8b":"Note: Due to Memory Error like Buffered data was truncated after reaching the output size limit.\nWhat i did is that Save the model in for example 60th epoch and close current program and run new program and restore saved model and train model from 61 epoch to 120 epoch and save that and close program and repeat this work for your interested epoch \nFor this [100,50] three times repeat \n","75eef0fe":"# Define a function to covert the image based on calculate log scaled mel-spectrograms and their corresponding deltas from a sound clip.","d462991f":"# CNN Model","452ab0c4":"# Load the zip file and unzip and before check the GPU","285896b7":"One of the major reasons for overfitting is that we don\u2019t have enough data to train our network. Apart from regularization, another very effective way to counter Overfitting is Data Augmentation. It is the process of artificially creating more images from the images you already have by changing the size, orientation etc of the image. It can be a tedious task but fortunately, this can be done in Keras using the ImageDataGenerator instance.","696b511d":"# Classification Report and Confusion Matrix"}}