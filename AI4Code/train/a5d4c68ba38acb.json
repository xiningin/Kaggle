{"cell_type":{"08d2ada6":"code","66cc1291":"code","f14c6ea8":"code","e520d3ff":"code","707dac52":"code","5f510857":"code","c825d789":"code","18aff28e":"code","502b49f4":"code","9e5caab6":"code","f4429c5f":"markdown","1b1c9a8b":"markdown","9f3acd75":"markdown","91e939fb":"markdown","dba4f37c":"markdown","3c732862":"markdown"},"source":{"08d2ada6":"import os\nimport random\nimport time\nimport collections\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nfrom PIL import Image, ImageEnhance, ImageFilter\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torchvision\nfrom torchvision.transforms import ToPILImage\nfrom torchvision.transforms import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n\nfrom skimage import exposure\nimport torchvision.transforms as T","66cc1291":"TEST = False\n\ndata_directory = '..\/input\/sartorius-cell-instance-segmentation'\nDEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nBATCH_SIZE = 2\nNUM_EPOCHS = 20\n\nTRAIN_CSV = f\"{data_directory}\/train.csv\"\nTRAIN_PATH = f\"{data_directory}\/train\"\nTEST_PATH = f\"{data_directory}\/test\"\n\nWIDTH = 704\nHEIGHT = 520\n\nRESNET_MEAN = (0.485, 0.456, 0.406)\nRESNET_STD = (0.229, 0.224, 0.225)","f14c6ea8":"# ref: https:\/\/www.kaggle.com\/inversion\/run-length-decoding-quick-start\ndef rle_decode(mask_rle, shape, color=1):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0] * shape[1], dtype=np.float32)\n    for lo, hi in zip(starts, ends):\n        img[lo : hi] = color\n    return img.reshape(shape)\n\n\ndef visualize(**images):\n    n = len(images)\n    plt.figure(figsize=(16, 12))\n    for i, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n, i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(' '.join(name.split('_')).title())\n        plt.imshow(image)\n    plt.show()","e520d3ff":"class Compose:\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, image, target):\n        for t in self.transforms:\n            image, target = t(image, target)\n        return image, target\n\n\nclass VerticalFlip:\n    def __init__(self, prob):\n        self.prob = prob\n\n    def __call__(self, image, target):\n        if random.random() < self.prob:\n            height, width = image.shape[-2:]\n            image = image.flip(-2)\n            bbox = target[\"boxes\"]\n            bbox[:, [1, 3]] = height - bbox[:, [3, 1]]\n            target[\"boxes\"] = bbox\n            target[\"masks\"] = target[\"masks\"].flip(-2)\n        return image, target\n\n\nclass HorizontalFlip:\n    def __init__(self, prob):\n        self.prob = prob\n\n    def __call__(self, image, target):\n        if random.random() < self.prob:\n            height, width = image.shape[-2:]\n            image = image.flip(-1)\n            bbox = target[\"boxes\"]\n            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n            target[\"boxes\"] = bbox\n            target[\"masks\"] = target[\"masks\"].flip(-1)\n        return image, target\n\n\nclass RandomRatation:\n    def __init__(self, prob):\n        self.prob = prob\n\n    def __call__(self, image, masks):\n        if random.random() < self.prob:\n\n            transformT = T.Compose([\n                T.RandomHorizontalFlip(),\n                T.RandomVerticalFlip(),\n                T.RandomChoice([\n                    T.RandomAffine(degrees=45, scale=(.9, 2.), translate=(0.1, 0.5), fill=0), # random transformations\n                    T.RandomPerspective(distortion_scale=0.32, fill=0),                        # random perspective\n                    T.RandomResizedCrop(size = (HEIGHT, WIDTH), scale = (.9, 1.4))             # random resized cropp\n                    # ... ( feel free to add yout own augmentation :) )\n                ]),\n            ])\n\n            transformImg = T.Compose([\n                T.RandomAdjustSharpness(sharpness_factor=0.1),\n            ])\n\n            # Set seed for same augmentations in one run\n            seed = np.random.randint(2147483647)  # make a seed with numpy generator\n            random.seed(seed)  # apply this seed to img transforms\n            torch.manual_seed(seed)\n\n\n            # Transform Image\n            image = transformT(image)\n\n            # Transform Masks\n            masks = np.array(masks)\n            new_masks = np.zeros((masks.shape[0], \n                                  HEIGHT, WIDTH\n                                  ), dtype=np.uint8)\n            for i, mask in enumerate(masks):\n                mask = Image.fromarray((mask).astype(np.uint8))\n                random.seed(seed)  # apply this seed to img tranfsorms\n                torch.manual_seed(seed)\n                mask = transformT(mask)\n                new_masks[i, :, :] = mask\n                \n            return image, new_masks\n        return image, masks\n\n\nclass Normalize:\n    def __call__(self, image, target):\n        image = F.normalize(image, RESNET_MEAN, RESNET_STD)\n        return image, target\n\n\nclass ToTensor:\n    def __call__(self, image, target):\n        image = F.to_tensor(image)\n        return image, target\n\n\ndef get_transform(train):\n    transforms = [ToTensor(), Normalize()]\n    if train:\n        transforms.append(RandomRatation(1.0))\n\n    return Compose(transforms)\n","707dac52":"class CellDataset(Dataset):\n    def __init__(self, image_dir, df, transforms=None, resize=False):\n        self.transforms = transforms\n        self.image_dir = image_dir\n        self.df = df\n        self.height = HEIGHT\n        self.width = WIDTH\n\n        self.image_info = collections.defaultdict(dict)\n        temp_df = self.df.groupby('id')['annotation'].agg(lambda x: list(x)).reset_index()\n        for index, row in temp_df.iterrows():\n            self.image_info[index] = {\n                'image_id': row['id'],\n                'image_path': os.path.join(self.image_dir, row['id'] + '.png'),\n                'annotations': row[\"annotation\"]\n            }\n\n    def get_box(self, a_mask):\n        ''' Get the bounding box of a given mask '''\n        pos = np.where(a_mask)\n        xmin = np.min(pos[1])\n        xmax = np.max(pos[1])\n        ymin = np.min(pos[0])\n        ymax = np.max(pos[0])\n        return [xmin, ymin, xmax, ymax]\n\n    def __getitem__(self, idx):\n        ''' Get the image and the target'''\n        img_path = self.image_info[idx][\"image_path\"]\n        img = Image.open(img_path).convert(\"RGB\")\n\n        info = self.image_info[idx]\n\n        n_objects = len(info['annotations'])\n        masks = np.zeros((len(info['annotations']), self.height, self.width), dtype=np.uint8)\n        boxes = []\n\n        for i, annotation in enumerate(info['annotations']):\n            a_mask = rle_decode(annotation, (HEIGHT, WIDTH))\n            a_mask = Image.fromarray(a_mask)\n            a_mask = np.array(a_mask) > 0\n            masks[i, :, :] = a_mask\n\n        ################################ AUGMENTATION PART ################################\n\n        if self.transforms is not None:\n            img, masks = self.transforms(img, masks)\n\n        new_masks = []\n        for i, mask in enumerate(masks):\n            if np.any((mask != 0)):\n                box = self.get_box(mask)\n                if box[0] != box[2] and box[1] != box[3]:\n                    new_masks.append(mask)\n                    boxes.append(box)\n        masks = np.array(new_masks)\n\n        ###################################################################################\n\n        labels = [1 for _ in range(n_objects)]\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n        masks = torch.as_tensor(masks, dtype=torch.uint8)\n\n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        iscrowd = torch.zeros((n_objects,), dtype=torch.int64)\n\n        target = {\n            'boxes': boxes,\n            'labels': labels,\n            'masks': masks,\n            'image_id': image_id,\n            'area': area,\n            'iscrowd': iscrowd\n        }\n\n        return img, target\n    \n    def __len__(self):\n        return len(self.image_info)","5f510857":"ds_train = CellDataset(TRAIN_PATH, pd.read_csv(TRAIN_CSV), transforms=get_transform(train=True))","c825d789":"a, b = ds_train[0]\nmasks = np.array(b[\"masks\"])\nmask = masks[0, :, :]\nfor i in range(len(masks)):\n    mask += masks[i, :, :]\nimage = a[0, :, :]\n\nprint(a.max(), a.min())\nvisualize(\n    image=image,\n    mask=mask,\n    mask2=masks[2, :, :],\n)\n\nprint(mask.max())\nprint(mask.shape)","18aff28e":"a, b = ds_train[20]\nmasks = np.array(b[\"masks\"])\nmask = masks[0, :, :]\nfor i in range(len(masks)):\n    mask += masks[i, :, :]\nimage = a[0, :, :]\n\nprint(a.max(), a.min())\nvisualize(\n    image=image,\n    mask=mask,\n    mask2=masks[2, :, :],\n)\n\nprint(mask.max())\nprint(mask.shape)","502b49f4":"a, b = ds_train[20]\nmasks = np.array(b[\"masks\"])\nmask = masks[0, :, :]\nfor i in range(len(masks)):\n    mask += masks[i, :, :]\nimage = a[0, :, :]\n\nprint(a.max(), a.min())\nvisualize(\n    image=image,\n    mask=mask,\n    mask2=masks[2, :, :],\n)\n\nprint(mask.max())\nprint(mask.shape)","9e5caab6":"a, b = ds_train[20]\nmasks = np.array(b[\"masks\"])\nmask = masks[0, :, :]\nfor i in range(len(masks)):\n    mask += masks[i, :, :]\nimage = a[0, :, :]\n\nprint(a.max(), a.min())\nvisualize(\n    image=image,\n    mask=mask,\n    mask2=masks[2, :, :],\n)\n\nprint(mask.max())\nprint(mask.shape)","f4429c5f":"# **Enjoy! :)**","1b1c9a8b":"# **Utils**","9f3acd75":"# **Results**","91e939fb":"# **Robust Augmentation Utils**","dba4f37c":"# **Dataset + Augmentation**","3c732862":"# **Imports**"}}