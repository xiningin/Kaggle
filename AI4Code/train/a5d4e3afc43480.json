{"cell_type":{"d598dafa":"code","b16f9e46":"code","c804a7db":"code","01740657":"code","9fe13d06":"code","206c4dd6":"code","213e5a4d":"code","e81417f8":"code","0f249068":"code","3089dd42":"code","93adc04a":"code","5ad9212c":"code","a3f5905a":"code","25321caa":"code","983d33f7":"code","422fe6e0":"code","ce5b5f36":"code","729fd859":"code","3d65d480":"code","d5d17ec4":"code","e36eb728":"markdown","038ae236":"markdown","c154c44f":"markdown","aa91b1fa":"markdown","fcbd0e1c":"markdown","20505f3b":"markdown","041beb80":"markdown","67b1eb54":"markdown","9c638934":"markdown","186cbc3a":"markdown","acd1d20a":"markdown","489777ac":"markdown","e80458e7":"markdown","562c245e":"markdown","f5e06964":"markdown","a12ca028":"markdown","fc8a9f83":"markdown","5cc920ca":"markdown","d3786ca2":"markdown","36d70ac5":"markdown","0b182da5":"markdown","f28bfd14":"markdown","9a21afe7":"markdown","6eb552dd":"markdown","e5fc1901":"markdown","ff1152bb":"markdown"},"source":{"d598dafa":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import pairwise_distances, accuracy_score, classification_report, confusion_matrix, plot_confusion_matrix, recall_score\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import normalize\nfrom sklearn import preprocessing\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndef display_all(df):\n    with pd.option_context(\"display.max_rows\", 20, \"display.max_columns\", 300,'display.max_colwidth', -1):\n        display(df)","b16f9e46":"df = pd.read_csv('..\/input\/drug-classification\/drug200.csv')\ndisplay_all(df)","c804a7db":"df.info()","01740657":"categorical_data = ['Sex','BP','Cholesterol']\nfor column in categorical_data:\n    print(column,df[column].unique())","9fe13d06":"df.replace(\"drugX\", \"DrugX\", inplace = True)\ndf.replace(\"drugA\", \"DrugA\", inplace = True)\ndf.replace(\"drugA\", \"DrugA\", inplace = True)\ndf.replace(\"drugC\", \"DrugC\", inplace = True)\ndf.replace(\"drugB\", \"DrugB\", inplace = True)\ndf = df.replace({'Sex': {'F': 0, 'M': 1}, 'BP': {'HIGH': 1, 'NORMAL': 0, 'LOW': -1}, 'Cholesterol': {'HIGH': 1, 'NORMAL': 0}})\nfor column in categorical_data:\n    print(column,df[column].unique())\ndf_original=df","206c4dd6":"plt.figure(figsize=(10,8))\nplt.title('Matriz de Correla\u00e7\u00e3o')\nsns.heatmap(df.corr(),annot=True,linewidths=3)\nplt.show()\nplt.savefig('Matriz de Correla\u00e7\u00e3o')","213e5a4d":"fig, [ax1, ax2] = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\nax1.boxplot(df[\"Age\"])\nax1.set_title(\"Age\")\nax2.boxplot(df[\"Na_to_K\"])\nax2.set_title(\"Na_to_K\")","e81417f8":"escala = MinMaxScaler()\nescala.fit(df.iloc[:,:-1])\n\ndf_t = escala.transform(df.iloc[:,:-1])\n\ndistances = euclidean_distances(df_t,df_t)\nm = [np.sum(distances[i])\/len(distances[0]) for i in range(len(distances[0]))]\nm_ordenado = sorted(m)\nplt.scatter(np.arange(0,len(m_ordenado)),m_ordenado)","0f249068":"#df_back = df\nescala = MinMaxScaler()\nescala.fit(df.iloc[m<m_ordenado[190],:-1])\n\ndf_t = escala.transform(df.iloc[m<m_ordenado[190],:-1])\n\ndistances = euclidean_distances(df_t,df_t)\nm2 = [np.sum(distances[i])\/len(distances[0]) for i in range(len(distances[0]))]\nm_ordenado2 = sorted(m2)\nplt.scatter(np.arange(0,len(m_ordenado2)),m_ordenado2)\n\ndf = df.iloc[m<m_ordenado[190],:]\n\ndf = pd.DataFrame(data=df.to_numpy(),columns=list(df.columns.values))\n\nfor k in list(df):\n    df[k]=pd.to_numeric(df[k], errors='ignore')\n#df=df_back","3089dd42":"ax = sns.set(style=\"ticks\", color_codes='pallete')\nax = sns.pairplot(df.drop(columns=['Sex','Cholesterol','BP']), hue=\"Drug\")","93adc04a":"Y=df['Drug']\nname=['Age', 'Sex', 'BP', 'Cholesterol', 'Na_to_K']\nY_original = df_original['Drug']\nX_original = df_original[name]\nd = preprocessing.normalize(df[name], axis=0)\ndf = pd.DataFrame(d, columns=df[name].columns.values)\nX=df[name]\n\n#normalMatrix = normalize(X.to_numpy(), norm='l2')\n#X = pd.DataFrame(data=normalMatrix,columns=list(X.columns.values))\n\ndisplay_all(df.describe())","5ad9212c":"oversample = RandomOverSampler(sampling_strategy='all')\nX_oversampled, Y_oversampled = oversample.fit_resample(X, Y)\n\nfig, ax = plt.subplots(1,2, figsize=(10,5))\nsns.countplot(x=Y,ax=ax[0]).set_title('Vari\u00e1vel alvo')\nsns.countplot(x=Y_oversampled,ax=ax[1]).set_title('Oversampling')\n\nX=X_oversampled\nY=Y_oversampled","a3f5905a":"def makeTable(headerRow,columnizedData,columnSpacing=2):\n    \"\"\"Creates a technical paper style, left justified table\n\n    Author: Christopher Collett\n    Date: 6\/1\/2019\"\"\"\n    from numpy import array,max,vectorize\n\n    cols = array(columnizedData,dtype=str)\n    colSizes = [max(vectorize(len)(col)) for col in cols]\n\n    header = ''\n    rows = ['' for i in cols[0]]\n\n    for i in range(0,len(headerRow)):\n        if len(headerRow[i]) > colSizes[i]: colSizes[i]=len(headerRow[i])\n        headerRow[i]+=' '*(colSizes[i]-len(headerRow[i]))\n        header+=headerRow[i]\n        if not i == len(headerRow)-1: header+=' '*columnSpacing\n\n        for j in range(0,len(cols[i])):\n            if len(cols[i][j]) < colSizes[i]:\n                cols[i][j]+=' '*(colSizes[i]-len(cols[i][j])+columnSpacing)\n            rows[j]+=cols[i][j]\n            if not i == len(headerRow)-1: rows[j]+=' '*columnSpacing\n\n    line = '-'*len(header)\n    print(line)\n    print(header)\n    print(line)\n    for row in rows: print(row)\n    print(line)\n    \ndef presentResults(scoresDictionary):\n    fitTimeArray = scoresDictionary['fit_time']\n    fitTimeMean = mean(fitTimeArray)\n    fitTimeStd = std(fitTimeArray)\n    \n    scoreTimeArray = scoresDictionary['score_time']\n    scoreTimeMean = mean(scoreTimeArray)\n    scoreTimeStd = std(scoreTimeArray)\n    \n    precisionArray = scoresDictionary['test_precision_macro']\n    precisionMean = mean(precisionArray)\n    precisionStd = std(precisionArray)   \n    \n    recallArray = scoresDictionary['test_recall_macro']\n    recallMean = mean(recallArray)\n    recallStd = std(recallArray)\n        \n    f1Array = scoresDictionary['test_f1_macro']\n    f1Mean = mean(f1Array)\n    f1Std = std(f1Array)\n    \n    accuracyArray = scoresDictionary['test_accuracy']\n    accuracyMean = mean(accuracyArray)\n    accuracyStd = std(accuracyArray)\n    \n    tableHeader = ['Metric', 'Mean', 'Standard Deviation']\n    metricRow = ['Precision', 'Accuracy', 'Recall', 'F1-score']\n    meanRow = [str(round(precisionMean,2)),str(round(accuracyMean,2)),str(round(recallMean,2)),str(round(f1Mean,2))]\n    stdRow = [str(round(precisionStd,2)),str(round(accuracyStd,2)),str(round(recallStd,2)),str(round(f1Std,2))]\n    makeTable(tableHeader, [metricRow,meanRow,stdRow])\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=5)\nX_train_original, X_test_original, Y_train_original, Y_test_original = train_test_split(X_original, Y_original, test_size=0.33, random_state=5)\ncv = KFold(n_splits=10, random_state=1, shuffle=True)\n\nfig, ax = plt.subplots(1,2, figsize=(10,5))\nsns.countplot(x=Y_train,ax=ax[0]).set_title('Treino')\nsns.countplot(x=Y_test,ax=ax[1]).set_title('Teste')","25321caa":"# X_train_original, X_test_original, Y_train_original, Y_test_original\n\nsvm = SVC(kernel='rbf')\nscoring = ['precision_macro', 'recall_macro', 'f1_macro', 'accuracy']\nscores = cross_validate(svm, X_original, Y_original, scoring=scoring, cv=cv, n_jobs=-1)\npresentResults(scores)\n\nprint(\"\\n\\nMatriz de confus\u00e3o:\\n\")\nsvm.fit(X_train_original, Y_train_original)\nsvm_pred = svm.predict(X_test_original)\nmatrix = confusion_matrix(Y_test_original, svm_pred)\nplot_confusion_matrix(svm, X_test_original, Y_test_original, labels=['DrugY','DrugC','DrugX','DrugA','DrugB'],cmap='Blues')\nprint(classification_report(Y_test_original, svm_pred, zero_division=1))","983d33f7":"svm = SVC(kernel='rbf')\nscoring = ['precision_macro', 'recall_macro', 'f1_macro', 'accuracy']\nscores = cross_validate(svm, X, Y, scoring=scoring, cv=cv, n_jobs=-1)\npresentResults(scores)\n\nprint(\"\\n\\nMatriz de confus\u00e3o:\\n\")\nsvm.fit(X_train, Y_train)\nsvm_pred = svm.predict(X_test)\nmatrix = confusion_matrix(Y_test, svm_pred)\nplot_confusion_matrix(svm, X_test, Y_test, labels=['DrugY','DrugC','DrugX','DrugA','DrugB'],cmap='Blues')\nprint(classification_report(Y_test, svm_pred, zero_division=1))","422fe6e0":"svm = SVC(kernel='poly', degree=3)\nscores = cross_validate(svm, X, Y, scoring=scoring, cv=cv, n_jobs=-1)\npresentResults(scores)\n\nprint(\"\\n\\nMatriz de confus\u00e3o:\\n\")\nsvm.fit(X_train, Y_train)\nsvm_pred = svm.predict(X_test)\nmatrix = confusion_matrix(Y_test, svm_pred)\nplot_confusion_matrix(svm, X_test, Y_test, labels=['DrugY','DrugC','DrugX','DrugA','DrugB'],cmap='Blues')\nprint(classification_report(Y_test, svm_pred, zero_division=1))","ce5b5f36":"svm = SVC(kernel='linear')\nscores = cross_validate(svm, X, Y, scoring=scoring, cv=cv, n_jobs=-1)\npresentResults(scores)\n\nprint(\"\\n\\nMatriz de confus\u00e3o:\\n\")\nsvm.fit(X_train, Y_train)\nsvm_pred = svm.predict(X_test)\nmatrix = confusion_matrix(Y_test, svm_pred)\nplot_confusion_matrix(svm, X_test, Y_test, labels=['DrugY','DrugC','DrugX','DrugA','DrugB'],cmap='Blues')\nprint(classification_report(Y_test, svm_pred, zero_division=1))","729fd859":"svm = SVC(kernel='sigmoid')\nscores = cross_validate(svm, X, Y, scoring=scoring, cv=cv, n_jobs=-1)\npresentResults(scores)\n\nprint(\"\\n\\nMatriz de confus\u00e3o:\\n\")\nsvm.fit(X_train, Y_train)\nsvm_pred = svm.predict(X_test)\nmatrix = confusion_matrix(Y_test, svm_pred)\nplot_confusion_matrix(svm, X_test, Y_test, labels=['DrugY','DrugC','DrugX','DrugA','DrugB'],cmap='Blues')\nprint(classification_report(Y_test, svm_pred, zero_division=1))","3d65d480":"currentMaximum=0\nbestDegree=0\n\nfor degree in range(1,11):\n    svm = SVC(kernel='poly',degree=degree, C=1)\n    scores = cross_validate(svm, X, Y, scoring=scoring, cv=cv, n_jobs=-1)\n    #print(scores)\n    f1Array = scores['test_f1_macro']\n    f1Mean = mean(f1Array)\n    f1Std = std(f1Array)\n    accuracyArray = scores['test_accuracy']\n    accuracyMean = mean(accuracyArray)\n    accuracyStd = std(accuracyArray)\n    if (accuracyMean > currentMaximum):\n        currentMaximum = accuracyMean\n        bestDegree = degree\n    print(\"Degree: \", degree, \n          #\" | F1-Score: \", str(round(f1Mean,4)), \" +- \", str(round(f1Std,4)),\n          \" | Accuracy: \", str(round(accuracyMean,4)), \" +- \", str(round(accuracyStd,4)))\nprint(\"\\nBest degree: \", str(bestDegree))","d5d17ec4":"currentMaximum=0\nbestC=0\n\nprint(\"Polynomial degree: 8\\n\")\ndegree=8\n\nfor c in range(1,20,1):\n    svm = SVC(kernel='poly',degree=degree,C=c\/10)\n    scores = cross_validate(svm, X, Y, scoring=scoring, cv=cv, n_jobs=-1)\n    #print(scores)\n    f1Array = scores['test_f1_macro']\n    f1Mean = mean(f1Array)\n    f1Std = std(f1Array)\n    accuracyArray = scores['test_accuracy']\n    accuracyMean = mean(accuracyArray)\n    accuracyStd = std(accuracyArray)\n    if (accuracyMean > currentMaximum):\n        currentMaximum = accuracyMean\n        bestC = c\/10\n    print(\"c: \", c\/10, \n          #\" | F1-Score: \", str(round(f1Mean,4)), \" +- \", str(round(f1Std,4)),\n          \" | Accuracy: \", str(round(accuracyMean,4)), \" +- \", str(round(accuracyStd,4)))\nprint(\"\\nBest C: \", str(bestC),\"\\n\")","e36eb728":"# Normaliza\u00e7\u00e3o\nOs dados s\u00e3o colocados na faixa de 0 a 1","038ae236":"Uma transforma\u00e7\u00e3o \u00e9 aplicada para converter todos os dados em num\u00e9ricos. Os dados da vari\u00e1vel alvo s\u00e3o tratados para que n\u00e3o haja ambiguidade na representa\u00e7\u00e3o.","c154c44f":"![image.png](attachment:5b3617d3-7cf1-43fe-90c6-8f2a97522a26.png)","aa91b1fa":"# Visualiza\u00e7\u00e3o de valores faltantes\nO dataset n\u00e3o tem valores faltantes","fcbd0e1c":"Utilizamos alguns kernels para experimenta\u00e7\u00e3o de hiperpar\u00e2metros.","20505f3b":"## Kernel Linear","041beb80":"# Matriz de correla\u00e7\u00e3o\nNo caso do sexo, uma correla\u00e7\u00e3o positiva indica que o sexo masculino apresenta aquela caracter\u00edstica com mais frequ\u00eancia. ","67b1eb54":"# Classifica\u00e7\u00e3o com dados originais","9c638934":"# Balanceamento dos dados","186cbc3a":"<img src=\"attachment:62659950-5a34-4955-9afe-a4290c172f40.png\" width=\"250px\">","acd1d20a":"# Classifica\u00e7\u00e3o com dados processados","489777ac":"## Kernel Sigmoid","e80458e7":"# Conclus\u00e3o\n\nUtilizando o conjunto de dados original os resultados foram ruins, portantos optamos por um pr\u00e9 processamento dos dados, sendo a normaliza\u00e7\u00e3o das vari\u00e1veis de idade e Na_to_K a a\u00e7\u00e3o de maior impacto.\n\nCom isso realizamos testes em quatro diferentes kernels do SVM, e \u00e9 not\u00e1vel a diferen\u00e7a de performance entre o RBf e polinomial em rela\u00e7\u00e3o \u00e0s demais, sendo este \u00faltimo o maior destaque. De forma geral, independente do grau utilizado o desempenho foi bom, sendo o melhor a partir do grau tr\u00eas, sem diferen\u00e7as not\u00e1veis.\n\n","562c245e":"## Kernel Radial Base Function (RBF)","f5e06964":"# Leitura e apresenta\u00e7\u00e3o da base de dados\n* 5 caracter\u00edsticas\n* 1 vari\u00e1vel alvo\n* 200 registros","a12ca028":"## Kernel Polynomial Function","fc8a9f83":"# Exerc\u00edcio 6 - SVM\nGrupo 14: Gabriel Mendes. Guilherme Araujo","5cc920ca":"# Classificador SVM\n\nA M\u00e1quina de Vetores de Suporte (*Support Vector Machine* - SVM) calcula a superf\u00edcie de decis\u00e3o que melhor separa as classes no conjunto de treinamento. A margem de separa\u00e7\u00e3o \u00e9 a menor dist\u00e2ncia entre os registros do treinamento e a superf\u00edcie de decis\u00e3o. A melhor superf\u00edcie de decis\u00e3o \u00e9 aquela que maximiza a margem de separa\u00e7\u00e3o sem que haja erros no treinamento. Os vetores mais pr\u00f3ximos \u00e0 superf\u00edcie de decis\u00e3o s\u00e3o denominados vetores de suporte. \n\nPara um problema linearmente separ\u00e1vel, o hiperplano discriminante \u00e9 definido pela Equa\u00e7\u00e3o 1 e assume valores maiores ou iguais que +1 para classes positivas e menores ou iguais que -1 para classes negativas. O hiperplano assume os valores +1 ou -1 para os vetores de suporte e o valor 0 na superf\u00edcie de separa\u00e7\u00e3o. O hiperplano \u00f3timo \u00e9 obtido pela minimiza\u00e7\u00e3o da dire\u00e7\u00e3o normal ao hiperplano **w**, sujeita \u00e0s restri\u00e7\u00f5es impostas. A Figura 1 ilustra a interpreta\u00e7\u00e3o geom\u00e9trica do classificador SVM bin\u00e1rio para um problema de duas vari\u00e1veis. \n\n\\begin{equation}\n    f(x)=<\\mathbf{w},\\mathbf{x}>+b\n\\end{equation}\n","d3786ca2":"# Valida\u00e7\u00e3o cruzada","36d70ac5":"# Gr\u00e1fico de proje\u00e7\u00e3o\nAs vari\u00e1veis n\u00e3o catego\u00f3ricas apresentam distribui\u00e7\u00f5es normais. \u00c9 poss\u00edvel observar que algumas calsses podem ser facilmente separ\u00e1veis com essas vari\u00e1veis.","0b182da5":"## Par\u00e2metro de regulariza\u00e7\u00e3o","f28bfd14":"# Box plot","9a21afe7":"# Elimina\u00e7\u00e3o de outliers\nA dist\u00e2ncia euclideana m\u00e9dia foi calculada entre os registros para valiar poss\u00edveis outliers.","6eb552dd":"# Codifica\u00e7\u00e3o dos dados\nAlguns dados s\u00e3o categ\u00f3ricos.","e5fc1901":"## Grau do polin\u00f4mio","ff1152bb":"Valores superiores a 190 foram removidos"}}