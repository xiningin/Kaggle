{"cell_type":{"571d323e":"code","9ecbf8c5":"code","cf3ac6fe":"code","bdfcbc00":"code","7603683f":"code","8a543823":"code","627355a5":"code","1d064047":"code","c5515104":"code","ef766598":"code","7d091e97":"code","e7167914":"code","3997fe57":"code","1a6a82d7":"markdown","5b1e7d1d":"markdown","b7dca04d":"markdown","01d3db89":"markdown","08713201":"markdown","2e02a76e":"markdown","f0785ab8":"markdown","b3d08028":"markdown","d8af95d7":"markdown","a38dcb7d":"markdown","5692e43c":"markdown","c5281d8b":"markdown","1d86986d":"markdown"},"source":{"571d323e":"import nltk                             \nfrom nltk.corpus import twitter_samples   \nimport matplotlib.pyplot as plt           \nimport random                              ","9ecbf8c5":"nltk.download('twitter_samples')","cf3ac6fe":"all_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\nprint('Number of positive tweets: ', len(all_positive_tweets))\nprint('Number of negative tweets: ', len(all_negative_tweets))\n\nprint('\\nThe type of all_positive_tweets is: ', type(all_positive_tweets))\nprint('The type of a tweet entry is: ', type(all_negative_tweets[0]))","bdfcbc00":"all_positive_tweets[:20]","7603683f":"all_negative_tweets[:20]","8a543823":"total_positive_words = []\nfor sentence in all_positive_tweets:\n    total_positive_words.append(sentence.count(' '))\n    \ntotal_negative_words = []\nfor sentence in all_negative_tweets:\n    total_negative_words.append(sentence.count(' '))\n    \nimport plotly.graph_objects as go\nimport numpy as np\n\nx0 = np.array(total_positive_words)\nx1 = np.array(total_negative_words)\n\nfig = go.Figure()\nfig.add_trace(go.Histogram(x=x1, name = 'Negative'))\nfig.add_trace(go.Histogram(x=x0, name = 'Positive'))\n\n# Overlay both histograms\nfig.update_layout(barmode='overlay')\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.75)\nfig.show()","627355a5":"tweet = all_positive_tweets[1455]\nprint(tweet)","1d064047":"nltk.download('stopwords')\n\nimport re                                  \nimport string                             \nfrom nltk.corpus import stopwords \nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer  ","c5515104":"print('Original Tweet: ')\nprint(tweet)\n\n# it will remove the old style retweet text \"RT\"\ntweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n\n# it will remove hyperlinks\ntweet2 = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet2)\n\n# it will remove hashtags. We have to be careful here not to remove \n# the whole hashtag because text of hashtags contains huge information. \n# only removing the hash # sign from the word\ntweet2 = re.sub(r'#', '', tweet2)\n\n# it will remove single numeric terms in the tweet. \ntweet2 = re.sub(r'[0-9]', '', tweet2)\nprint('\\nAfter removing old style tweet, hyperlinks and # sign')\nprint(tweet2)","ef766598":"print('Before Tokenizing: ')\nprint(tweet2)\n\n# instantiate the tokenizer class\ntokenizer = TweetTokenizer(preserve_case=False, \n                           strip_handles=True,\n                           reduce_len=True)\n\n# tokenize the tweets\ntweet_tokens = tokenizer.tokenize(tweet2)\n\nprint('\\nTokenized string:')\nprint(tweet_tokens)","7d091e97":"#Import the english stop words list from NLTK\nstopwords_english = stopwords.words('english') \n\nprint('Stop words\\n')\nprint(stopwords_english)\n\nprint('\\nPunctuation\\n')\nprint(string.punctuation)","e7167914":"print('Before tokenization')\nprint(tweet_tokens)\n\n\ntweets_clean = []\n\nfor word in tweet_tokens: # Go through every word in your tokens list\n    if (word not in stopwords_english and  # remove stopwords\n        word not in string.punctuation):  # remove punctuation\n        tweets_clean.append(word)\n\nprint('\\n\\nAfter removing stop words and punctuation:')\nprint(tweets_clean)","3997fe57":"# Instantiate stemming class\nstemmer = PorterStemmer() \n\n# Create an empty list to store the stems\ntweets_stem = [] \n\nfor word in tweets_clean:\n    stem_word = stemmer.stem(word)  # stemming word\n    tweets_stem.append(stem_word)  # append to the list\n\nprint('Words after stemming: ')\nprint(tweets_stem)","1a6a82d7":"## The notebook is not finished yet. I am learning gradually and I will keep updating this notebook. Please upvote if you like it. It keep me motivated to do good works.. :) ","5b1e7d1d":"![](https:\/\/miro.medium.com\/max\/2560\/1*sDa7Oqnh-zRXPPewKZid4g.png)\n# Preprocessing\n\nIn this notebook, I will be exploring how to preprocess tweets for sentiment analysis.  By the end of this notebook hopefully we will have a good understanding of common preprocessing steps ncessary for tweet sentiment analysis. I will also explore some of the useful features of [NLTK](http:\/\/www.nltk.org) package to perform a preprocessing pipeline for Twitter Sentiment classification datasets.\n\nIn this notebook we will mainly learn some of the basic concepts in NLP preprocessing. For example we will see the following preprocessings: \n* Removing twitter handles\n* Removing URLs\n* Removing punctuations\n* Removing handles\n* Removing stopwords\n* Stemming\n\nTo help with that, we will be using the [Natural Language Toolkit (NLTK)](http:\/\/www.nltk.org\/howto\/twitter.html) package, an open-source Python library for natural language processing. It has modules for collecting, handling, and processing Twitter data. For this notebook, we will use a Twitter dataset that comes with NLTK. This dataset has been manually annotated and serves to establish baselines for models quickly. Let us import them now as well as a few other libraries we will be using.","b7dca04d":"Data preprocessing is one of the critical steps in any machine learning project. It includes cleaning and formatting the data before feeding into a machine learning algorithm. For NLP, the preprocessing steps are comprised of the following tasks:\n* Tokenizing the string\n* Lowercasing\n* Removing stop words and punctuation\n* Stemming\n\nLet us select any sample from the data given and we will successively work our way up to the higher level of processing. ","01d3db89":"### Remove stop words and punctuations\n\nThe next step is to remove stop words and punctuation. Stop words are words that don't add significant meaning to the text. You'll see the list provided by NLTK when you run the cells below.","08713201":"Now let us load the positive and negative examples separately and then we will have a detalied look at them. ","2e02a76e":"### Stemming\n\nStemming is the process of converting a word to its most general form, or stem. This helps in reducing the size of our vocabulary.\n\nConsider the words: \n * **learn**\n * **learn**ing\n * **learn**ed\n * **learn**t\n \nAll these words are stemmed from its common root **learn**. However, in some cases, the stemming process produces words that are not correct spellings of the root word. For example, **happi** and **sunni**. That's because it chooses the most common stem for related words. For example, we can look at the set of words that comprises the different forms of happy:\n\n * **happ**y\n * **happi**ness\n * **happi**er\n \nWe can see that the prefix **happi** is more commonly used. We cannot choose **happ** because it is the stem of unrelated words like **happen**.\n \nNLTK has different modules for stemming and we will be using the [PorterStemmer](https:\/\/www.nltk.org\/api\/nltk.stem.html#module-nltk.stem.porter) module which uses the [Porter Stemming Algorithm](https:\/\/tartarus.org\/martin\/PorterStemmer\/). Let's see how we can use it in the cell below.","f0785ab8":"### Tokenize the string\nTo tokenize means to split the strings into individual words without blanks or tabs. In this same step, we will also convert each word in the string to lower case. The [tokenize](https:\/\/www.nltk.org\/api\/nltk.tokenize.html#module-nltk.tokenize.casual) module from NLTK allows us to do these easily:","b3d08028":"## NLTK Twitter Dataset\nThe sample dataset from NLTK is separated into positive and negative tweets. It contains **5000 positive tweets** and **5000 negative** tweets exactly. The exact match between these classes is not a coincidence. The intention is to have a balanced dataset. That does not reflect the real distributions of positive and negative classes in live Twitter streams. It is just because balanced datasets simplify the design of most computational methods that are required for sentiment analysis. However, it is better to be aware that this balance of classes is artificial. \n\nNow let's download the dataset. ","d8af95d7":"We observe that the tweet dataset are two list of strings. The individual s trings contains tweeter handles, punctuations, emoticons, urls etc. We need to do some good preprocessing to work with them. Just before that, let's check their comparative lengths. ","a38dcb7d":"### Remove hyperlinks, Twitter marks and styles\nSince we have a Twitter dataset, we'd like to remove some substrings commonly used on the platform like the hashtag, retweet marks, and hyperlinks. We'll use the [re](https:\/\/docs.python.org\/3\/library\/re.html) library to perform regular expression operations on our tweet. We'll define our search pattern and use the `sub()` method to remove matches by substituting with an empty character (i.e. `''`)","5692e43c":"### Observation regarding Stop words and Punctuations\n\n* We can see that the stop words list above contains some words that could be important in some contexts.  These could be words like _i, not, between, because, won, against_. You might need to customize the stop words list for some applications. For our exercise, we will use the entire list.\n\n* For the punctuation, we saw earlier that certain groupings like ':)' and '...'  should be retained when dealing with tweets because they are used to express emotions. In other contexts, like medical analysis, these should also be removed.","c5281d8b":"Let us download the stopwords for the NLTK library. Here the stopwords are different for different languages. So be sure to keep that in your mind. We also need to import regular expression library because we will search for different patterns in the sentences i.e. urls, tweeter marks and styles to remove them.  ","1d86986d":"## Looking at raw texts\n\nBefore anything else, we can print a couple of tweets from the dataset to see how they look. Understanding the data is responsible for 80% of the success or failure in data science projects. We can use this time to observe aspects we'd like to consider when preprocessing our data."}}