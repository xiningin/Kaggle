{"cell_type":{"29aedf48":"code","64e283f1":"code","1d889703":"code","43dc238b":"code","e4e12ded":"code","3e45ea54":"code","b2e13d16":"code","d12742e0":"code","4a80279b":"code","b5f9e169":"code","20e1ae8a":"code","dcf02ce3":"code","c370837a":"code","0bfe4b47":"code","90d002b8":"code","d515c6c6":"code","bfb69098":"code","9b05113f":"code","3ce45bca":"code","0c5f9a26":"code","57b3806f":"code","94ae8d98":"code","d1d1e978":"code","ee1aab05":"code","114cde7e":"code","de3574a3":"code","5f6c5a18":"code","f90cc445":"code","0f84e5cf":"code","a8f77e89":"code","3b112ae5":"code","2a5796ed":"code","b6cca278":"markdown","c0786375":"markdown","7e896a21":"markdown","d7839403":"markdown","58218fee":"markdown","c57c86c2":"markdown","4a4a2348":"markdown","d44fc8de":"markdown","e113b1ae":"markdown","864a79c4":"markdown","54eb06ba":"markdown"},"source":{"29aedf48":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","64e283f1":"df=pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf.head()","1d889703":"import pandas as pd\nimport pylab as pl\nimport numpy as np\nimport seaborn as sns\nimport scipy.optimize as opt\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline \nimport matplotlib.pyplot as plt","43dc238b":"df.describe()","e4e12ded":"df.drop(['id', 'Unnamed: 32'], axis=1, inplace=True)\ndf.head()","3e45ea54":"sns.pairplot(df, hue='diagnosis', vars = ['radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean'])","b2e13d16":"sns.scatterplot(x='radius_mean',y='area_mean',hue='diagnosis',data=df)\nplt.ioff()","d12742e0":"count = df.diagnosis.value_counts()\ncount","4a80279b":"df['target'] = df.diagnosis.map({'B':0, 'M':1})","b5f9e169":"df.dtypes","20e1ae8a":"plt.figure(figsize=(20,20))\nsns.heatmap(df.corr(),annot=True)","dcf02ce3":"X =df.iloc[:, 1:-1]\ny = df['target']\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)\nX.head()","c370837a":"from sklearn import svm\nclf = svm.SVC(kernel='rbf')\nclf.fit(X_train, y_train) ","0bfe4b47":"yhat = clf.predict(X_test)\nyhat [0:5]","90d002b8":"from sklearn.metrics import classification_report, confusion_matrix\nimport itertools","d515c6c6":"cnf_matrix = confusion_matrix(y_test, yhat, labels=[0,1])\nnp.set_printoptions(precision=2)\nprint (classification_report(y_test, yhat))","bfb69098":"cm = confusion_matrix(y_test, yhat)\nsns.heatmap(cm,annot=True,fmt=\"d\")","9b05113f":"from sklearn.metrics import f1_score\nf1_score(y_test, yhat, average='weighted') ","3ce45bca":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nX_train_sc=sc.fit_transform(X_train)\nX_test_sc=sc.fit_transform(X_test)","0c5f9a26":"from sklearn import svm\nclf1 = svm.SVC(kernel='rbf')\nclf1.fit(X_train_sc, y_train) ","57b3806f":"yhat_sc = clf1.predict(X_test_sc)\nyhat_sc [0:5]","94ae8d98":"cnf_matrix = confusion_matrix(y_test, yhat_sc, labels=[0,1])\nnp.set_printoptions(precision=2)\n\nprint (classification_report(y_test, yhat_sc))","d1d1e978":"cm = confusion_matrix(y_test, yhat_sc)\nsns.heatmap(cm,annot=True,fmt=\"d\")","ee1aab05":"from sklearn.metrics import f1_score\nf1_score(y_test, yhat_sc, average='weighted')","114cde7e":"from sklearn import svm\nclf2 = svm.SVC(kernel='linear')\nclf2.fit(X_train_sc, y_train) ","de3574a3":"yhat_sc_ln = clf2.predict(X_test_sc)\nyhat_sc_ln [0:5]","5f6c5a18":"cnf_matrix = confusion_matrix(y_test, yhat_sc_ln, labels=[0,1])\nnp.set_printoptions(precision=2)\n\nprint (classification_report(y_test, yhat_sc_ln))","f90cc445":"cm = confusion_matrix(y_test, yhat_sc_ln)\nsns.heatmap(cm,annot=True,fmt=\"d\")","0f84e5cf":"from sklearn.metrics import f1_score\nf1_score(y_test, yhat_sc_ln, average='weighted')","a8f77e89":"from sklearn.model_selection import GridSearchCV \nparam_grid = {'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf']}  \ngrid = GridSearchCV(svm.SVC(), param_grid, refit = True, verbose = 3)  \ngrid.fit(X_train_sc, y_train)","3b112ae5":"print(grid.best_params_) \nprint(grid.best_estimator_)","2a5796ed":"grid_predictions = grid.predict(X_test_sc) \nprint(classification_report(y_test, grid_predictions))","b6cca278":"# THE ACCURACY INCRAESED TO 97 % ","c0786375":"![image.png](attachment:image.png)","7e896a21":"# Splitting data into test and train set","d7839403":" # The Model only gives 86 % accuracy which is not good.","58218fee":"# \"Importing The data\"","c57c86c2":" CHECKING BETWEEN LINEAR FUNCTION OR RBF FUNCTION AND USING FEATURE SCALING ","4a4a2348":" # The acurracy is better with rbf & feature scaling rather than linear function &  feature scaling","d44fc8de":" # Feature Scaling is done to increase the accuracy of the model","e113b1ae":"One of the great things about GridSearchCV is that it is a meta-estimator. It takes an estimator like SVC, and creates a new estimator, that behaves exactly the same \u2013 in this case, like a classifier. You should add refit=True and choose verbose to whatever number you want, higher the number, the more verbose (verbose just means the text output describing the process)","864a79c4":"#  Using Grid Search for parameters tuning ","54eb06ba":" # THE MODEL USED IS SUPPORT VECTOR MACHINE (RBF )"}}