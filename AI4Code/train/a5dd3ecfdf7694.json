{"cell_type":{"2e77fc83":"code","8e99c46c":"code","3703715e":"code","d9ca1773":"code","dd6dd5a4":"code","dec42b44":"code","8e1eae59":"code","11f465c5":"code","d647893d":"code","f08c5d49":"code","92e583fe":"code","3ebd5fa6":"code","6ddaf743":"code","4bc38024":"code","c35885a9":"code","b00722ad":"code","a1f41040":"code","e586457c":"code","660361e8":"code","340dd7c7":"code","f28ef72d":"code","552769cc":"code","e46c5c8d":"code","1dea2c61":"code","759480f4":"code","6cc38c96":"code","8ed5f570":"code","1d5a5cdf":"code","1c12c9fb":"code","d73a47b4":"code","a78cbfbe":"code","1811707e":"code","517d18aa":"code","d8f9c263":"code","4c9146fc":"code","c550a481":"markdown","7915dac8":"markdown","c3369902":"markdown","49753fb4":"markdown","a23860e9":"markdown","6752e521":"markdown","38e8ec3b":"markdown","2a2b804d":"markdown","b23636e1":"markdown","211e8697":"markdown","e6ffd126":"markdown"},"source":{"2e77fc83":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n","8e99c46c":"df = pd.read_csv('..\/input\/text-classification\/Amazon_text.csv')\ndf.head()","3703715e":"def sent(x):\n    if(x == '__label__1'):\n        return 0\n    else :\n        return 1","d9ca1773":"df['Sentiment'] = df['Label'].apply(sent)","dd6dd5a4":"df.head(10)","dec42b44":"len('__label__2')","8e1eae59":"len((df.loc[0].Label))","11f465c5":"df['Sentiment']= np.where(df['Label'] == '__label__1 ', 0, 1)","d647893d":"df.head(10)","f08c5d49":"import re\ndef clean_reviews(text):\n    text=re.sub(\"[^a-zA-Z]\",\" \",str(text))\n    return re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", text)","92e583fe":"df['Summary'] = df.Review.apply(clean_reviews)\ndf.head(10)","3ebd5fa6":"len(df.Summary[8])","6ddaf743":"from tensorflow.keras.preprocessing.text import Tokenizer","4bc38024":"X = df.Summary\ny = df.Sentiment\n\ntokenizer = Tokenizer(num_words=10000, oov_token='xxxxxxx')","c35885a9":"tokenizer.fit_on_texts(X)\nX_dict = tokenizer.word_index\nprint(len(X_dict))\n#X_dict.items()","b00722ad":"X_seq = tokenizer.texts_to_sequences(X)","a1f41040":"from tensorflow.keras.preprocessing.sequence import pad_sequences","e586457c":"X_padded_seq = pad_sequences(X_seq, padding='post', maxlen=500)","660361e8":"X_padded_seq.shape","340dd7c7":"type(y)","f28ef72d":"y = np.array(y)\ny = y.flatten()\ny.shape","552769cc":"type(y)","e46c5c8d":"import tensorflow as tf","1dea2c61":"num_epochs = 10\ntext_model = tf.keras.Sequential([tf.keras.layers.Embedding(input_length=500, input_dim=10000, output_dim=50),\n                                 tf.keras.layers.Flatten(),\n                                 tf.keras.layers.Dense(6, activation='relu'),\n                                 tf.keras.layers.Dense(1, activation='sigmoid')])","759480f4":"text_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\ntext_model.summary()","6cc38c96":"text_model.fit(X_padded_seq,y, epochs=num_epochs)","8ed5f570":"embeddings = text_model.layers[0]\nembeddings.weights","1d5a5cdf":"weights = embeddings.get_weights()[0]\nprint(weights.shape)","1c12c9fb":"index_based_embedding = dict([(value, key) for (key, value) in X_dict.items()])","d73a47b4":"def decode_review(text):\n    return ' ',join([index_based_embedding.get(i, '?') for i in text])","a78cbfbe":"index_based_embedding[1]","1811707e":"index_based_embedding[2]","517d18aa":"weights[1]","d8f9c263":"import io","4c9146fc":"vec = io.open('embedding_vectors_new.tsv', 'w', encoding='utf-8')\nmeta = io.open('metadata_new.tsv', 'w', encoding='utf-8')\n\nfor i in range(1, len(X_dict)): # x_dict == vocabsize\n    word = index_based_embedding[i]\n    embedding_vec_values = weights[i]\n    meta.write(word + \"\\n\")\n    vec.write('\\t'.join([str(x) for x in embedding_vec_values]) + '\\n')\n    \nmeta.close()\nvec.close()","c550a481":"inorder to visualizethe embeddings in 2D space, we must reverse the key value for embeddings and respective words so as to represent every word via its embedding. to do this, we create a helper function.","7915dac8":"The reason I was not able to apply 'apply' was because the dataset itself is labelled with ne extra space.","c3369902":"Lets convert target variable in Panda series to a numpy array","49753fb4":"## Creating a Deep Learning Model","a23860e9":"You can look at the embedding at\"https:\/\/projector.tensorflow.org\/\"","6752e521":"### Splitting into input and output data\n\nbefore tokenization\n","38e8ec3b":"### Converting to numeric labels\n\nlets convert the labels to 0 and 1","2a2b804d":"In the final part of this exercise, we extract the embeddings value and \nput it into a .tsv file, along with another .tsv file that captures the words \nof the embedding.","b23636e1":"Applying tokenizer to entire sequences","211e8697":"Lets use padding to make vectors of equal length 500","e6ffd126":"### Text preprocessing\n\nWe use a regular expression to remove unwanted symbols , characters and numbers."}}