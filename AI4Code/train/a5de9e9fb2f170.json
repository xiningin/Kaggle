{"cell_type":{"2f40e19f":"code","325b5135":"code","b05d415e":"code","c70d5fdf":"code","199e10bc":"code","3afeb9e2":"code","492c0ced":"code","280a485d":"code","30575911":"code","118a0e86":"code","aa844ca2":"code","1f5ecc49":"code","8e2f351a":"code","1f1f9c0d":"code","9420d011":"code","46f1a02e":"code","edb4b763":"code","6c10415d":"code","1e0c14ac":"code","1c0fc17d":"code","4d4b3e9b":"code","fd54cf13":"code","fce4baa4":"code","f04238dc":"code","12da71b2":"code","7b1bcdfe":"code","de15c942":"code","2542aa80":"code","9705c418":"code","1b9c8249":"code","a5dedc02":"code","5bb13d5d":"code","df8a7e20":"code","3e2ba6b5":"code","37d37abe":"code","ce07ea1f":"markdown","e08aef46":"markdown","278436c3":"markdown","ad33e50c":"markdown","2ebbd3f9":"markdown","12e5547c":"markdown","0113e054":"markdown","cad901c1":"markdown","5dbf5479":"markdown","def8185f":"markdown","104b0804":"markdown","880d316f":"markdown","4153c6b9":"markdown","c49ecdc0":"markdown","94b1d3b1":"markdown","18dcad2e":"markdown","8e7c8c9b":"markdown","64505c9a":"markdown"},"source":{"2f40e19f":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\nimport matplotlib.pyplot as plt\nimport nltk\nfrom sklearn.preprocessing import LabelBinarizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\nfrom keras.preprocessing import text, sequence\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,LSTM,Dropout\nfrom keras.callbacks import ReduceLROnPlateau\nimport tensorflow as tf","325b5135":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b05d415e":"true = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/True.csv\")\ntrue.head()","c70d5fdf":"fake = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/Fake.csv\")\nfake.head()","199e10bc":"true.shape,fake.shape","3afeb9e2":"print(\"Null Values in Real News Data = \",true.isna().any().sum())\nprint(\"Null Values in Fake News Data = \",fake.isna().any().sum())","492c0ced":"true['category'] = 1\nfake['category'] = 0\n\ndf = pd.concat([true,fake])\ndf.head()","280a485d":"df.shape","30575911":"df.describe(include=\"object\")","118a0e86":"print(df['category'].value_counts())\nsns.countplot(df['category'])\nplt.show()","aa844ca2":"sns.countplot(x='subject',hue='category',data=df,)\nplt.xticks(rotation=90)\n\nplt.show()","1f5ecc49":"df[\"text\"] =df[\"title\"]+df[\"text\"]+df['subject']\ndf=df[[\"text\",\"category\"]]","8e2f351a":"import spacy\nnlp = spacy.load('en_core_web_sm')\nlist1 = nlp.Defaults.stop_words","1f1f9c0d":"list2 = stopwords.words('english')\npunctuation = list(string.punctuation)\nStopwords = set((set(list1)|set(list2)|set(punctuation)))\nlen(Stopwords)","9420d011":"#creating instance\nlemma=WordNetLemmatizer()\n\n#text cleaning function\ndef clean_text(text):\n    \n    \"\"\"\n    It takes text as an input and clean it by applying several methods\n    \n    \"\"\"\n    \n    string = \"\"\n    \n    #lower casing\n    text=text.lower()\n    \n    #simplifying text\n    text=re.sub(r\"i'm\",\"i am\",text)\n    text=re.sub(r\"he's\",\"he is\",text)\n    text=re.sub(r\"she's\",\"she is\",text)\n    text=re.sub(r\"that's\",\"that is\",text)\n    text=re.sub(r\"what's\",\"what is\",text)\n    text=re.sub(r\"where's\",\"where is\",text)\n    text=re.sub(r\"\\'ll\",\" will\",text)\n    text=re.sub(r\"\\'ve\",\" have\",text)\n    text=re.sub(r\"\\'re\",\" are\",text)\n    text=re.sub(r\"\\'d\",\" would\",text)\n    text=re.sub(r\"won't\",\"will not\",text)\n    text=re.sub(r\"can't\",\"cannot\",text)\n    \n    #removing any special character\n    text=re.sub(r\"[-()\\\"#!@$%^&*{}?.,:]\",\" \",text)\n    text=re.sub(r\"\\s+\",\" \",text)\n    text=re.sub('[^A-Za-z0-9]+',' ', text)\n    \n    for word in text.split():\n        if word not in Stopwords:\n            string+=lemma.lemmatize(word)+\" \"\n    \n    return string","46f1a02e":"#cleaning the whole data\ndf[\"text\"]=df[\"text\"].apply(clean_text)","edb4b763":"from wordcloud import WordCloud","6c10415d":"plt.figure(figsize = (20,20))\nwc = WordCloud(max_words = 2000 , width = 1000 , height = 500 , stopwords = Stopwords).generate(\" \".join(df[df.category == 1].text))\nplt.axis(\"off\")\nplt.imshow(wc , interpolation = 'bilinear')\nplt.show()\n","1e0c14ac":"plt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1000 , height = 500 , stopwords = Stopwords).generate(\" \".join(df[df.category == 0].text))\nplt.axis(\"off\")\nplt.imshow(wc , interpolation = 'bilinear')\nplt.show()\n","1c0fc17d":"plt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1000 , height = 500 , stopwords = Stopwords,background_color='white').generate(\" \".join(df.text))\nplt.axis(\"off\")\nplt.imshow(wc , interpolation = 'bilinear')\nplt.show()","4d4b3e9b":"from sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics","fd54cf13":"#Split the dataset into Train And Test Dataset.\nX=df[\"text\"] #feature \ny=df[\"category\"] # traget\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","fce4baa4":"#pipe line will take the text and vectorise it , and then TF-IDF, then fitting the model\n\nclf_text=Pipeline([(\"tfidf\",TfidfVectorizer()),(\"clf\",LogisticRegression())])\nclf_text.fit(X_train,y_train)","f04238dc":"#making prediction using the model\npredictions=clf_text.predict(X_test)\n\nprint(metrics.classification_report(y_test,predictions))","12da71b2":"#overall acuracy\nprint(metrics.accuracy_score(y_test,predictions))\nprint(metrics.f1_score(y_test,predictions))\n\n#confusion matrix\nprint(metrics.confusion_matrix(y_test,predictions))","7b1bcdfe":"#pipe line will take the text and vectorise it , and then TF-IDF, then fitting the model\n\nclf_text=Pipeline([(\"tfidf\",TfidfVectorizer()),(\"clf\",MultinomialNB(alpha=0.5))])\nclf_text.fit(X_train,y_train)","de15c942":"#making prediction using the model\npredictions=clf_text.predict(X_test)\n\nprint(metrics.classification_report(y_test,predictions))","2542aa80":"#overall acuracy\nprint(metrics.accuracy_score(y_test,predictions))\nprint(metrics.f1_score(y_test,predictions))\n\n#confusion matrix\nprint(metrics.confusion_matrix(y_test,predictions))","9705c418":"#pipe line will take the text and vectorise it , and then TF-IDF, then fitting the model\n\nclf_text=Pipeline([(\"tfidf\",TfidfVectorizer()),(\"clf\",LinearSVC())])\nclf_text.fit(X_train,y_train)","1b9c8249":"#making prediction using the model\npredictions=clf_text.predict(X_test)\n\nprint(metrics.classification_report(y_test,predictions))","a5dedc02":"#overall acuracy\nprint(metrics.accuracy_score(y_test,predictions))\nprint(metrics.f1_score(y_test,predictions))\n\n#confusion matrix\nprint(metrics.confusion_matrix(y_test,predictions))","5bb13d5d":"#pipe line will take the text and vectorise it , and then TF-IDF, then fitting the model\n\nclf_rf=Pipeline([(\"tfidf\",TfidfVectorizer()),(\"clf\",RandomForestClassifier(random_state=0))])\nclf_rf.fit(X_train,y_train)\n\n#making prediction using the model\npredictions=clf_rf.predict(X_test)\n\n\nprint(metrics.classification_report(y_test,predictions))\n","df8a7e20":"#overall acuracy\nprint(metrics.accuracy_score(y_test,predictions))\nprint(metrics.f1_score(y_test,predictions))\n\n#confusion matrix\nprint(metrics.confusion_matrix(y_test,predictions))","3e2ba6b5":"#pipe line will take the text and vectorise it , and then TF-IDF, then fitting the model\n\nclf_dt=Pipeline([(\"tfidf\",TfidfVectorizer()),(\"clf\",DecisionTreeClassifier(random_state=2))])\nclf_dt.fit(X_train,y_train)\n\n#making prediction using the model\npredictions=clf_dt.predict(X_test)\n\n\nprint(metrics.classification_report(y_test,predictions))\n","37d37abe":"#overall acuracy\nprint(metrics.accuracy_score(y_test,predictions))\nprint(metrics.f1_score(y_test,predictions))\n\n#confusion matrix\nprint(metrics.confusion_matrix(y_test,predictions))","ce07ea1f":"Final Shape of the data","e08aef46":"## Naive Bayes","278436c3":"### Data Loading","ad33e50c":"#### Word Cloud for Whole data","2ebbd3f9":"#### Word Cloud for Real News","12e5547c":"### STOPWORDS\n\nStopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For example, the words like the, he, have etc. Such words are already captured this in corpus named corpus. We first download it to our python environment.","0113e054":"### SVC","cad901c1":"### Classification Model","5dbf5479":"### Import Libraries","def8185f":"### Data Cleaning","104b0804":"### Word Cloud","880d316f":"### Pipeline","4153c6b9":"Updating......","c49ecdc0":"## Logistic Regression","94b1d3b1":"Please upvote the notebook if you find it useful. Your comments are also requested for improvement of the notebook.","18dcad2e":"### Random Forest Cassifier","8e7c8c9b":"#### Word Cloud for Fake News","64505c9a":"### Decision Tree Classifier"}}