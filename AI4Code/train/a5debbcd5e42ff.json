{"cell_type":{"75a4d5de":"code","ed03fe1d":"code","b201d820":"code","bc4fdb70":"code","0a24fd5d":"code","a42747b7":"code","f597d18d":"code","b13dda76":"code","b638be4e":"code","377f087b":"code","1d1164c1":"code","84a4a1e5":"code","7cdd838d":"code","b5f52d8c":"code","a6a9d3f6":"code","0121669d":"code","daf4b8fc":"code","56ba0402":"code","ab9ba2d0":"code","75ca1fd1":"code","d3612c66":"code","c4aa3486":"code","fb28c9ad":"code","ce6b8fc3":"code","106461e0":"code","c608d331":"code","941e7668":"code","103c0d74":"code","eec99b0a":"code","12f4920a":"code","006462b5":"code","e69d8623":"code","51f6e601":"code","373da8e6":"code","6ecaceb4":"code","daa6c962":"code","e1016bb7":"code","c331607f":"code","3367a2bc":"code","62bdf2d0":"code","ad5ea57a":"code","4e11ae56":"code","a5eda12c":"code","5c7a9514":"code","a1416f6f":"code","1544cb14":"code","c98ca187":"code","191f0f4f":"code","6cdd3343":"code","baff3e4d":"code","d50558f7":"code","7f19b05b":"code","3d331c1b":"code","9b41e27c":"code","bab3da49":"code","2b151dab":"code","666d6d2f":"code","c73545de":"code","1c029c3e":"code","f84b68c7":"code","7692edfd":"code","304af24d":"code","66488e14":"code","8f5041e1":"code","bdc4e960":"code","8eb8ef24":"code","47c10e33":"code","3c041373":"code","30f9047c":"code","523f487e":"code","4bd0c604":"code","1e92dfd5":"code","0c537ae2":"code","93314565":"code","f79980f7":"code","5eceeb71":"code","0700facc":"code","3482ccaa":"code","a962665a":"code","70efad8e":"code","0512d32c":"code","99322e8a":"code","becae14c":"code","5ff18e0c":"code","222f8c39":"code","bdbe7639":"code","aeb0f84a":"code","aee4d8bf":"code","f996b34b":"code","0d9967d0":"code","9f4d8207":"code","6fe638ab":"code","5c527450":"code","26cc2ae0":"code","bbccfbdf":"code","c9a4b79b":"code","6a027b9b":"code","0d93a676":"code","b985250c":"code","61553765":"code","79adbcc5":"markdown","c88c44a5":"markdown","15dfb7e1":"markdown","f8f93b50":"markdown","4a0fe98b":"markdown","f863328a":"markdown","5b838df8":"markdown","c2ac30ea":"markdown","aa93a033":"markdown","60b48269":"markdown","d57a779f":"markdown","f0ecca6a":"markdown","5b5f3117":"markdown","0a5e3c72":"markdown","c2bd3386":"markdown","e7c6eec2":"markdown","59b5e6f9":"markdown","8f486860":"markdown","5740e471":"markdown","1f16a7b5":"markdown","62a60b3e":"markdown","fb074294":"markdown","d2bd9fbc":"markdown","3e44a955":"markdown","11c1eb4c":"markdown","42ce3fec":"markdown","f3c0a8bb":"markdown","fc1f718a":"markdown","bb4960e8":"markdown","077b1b08":"markdown","05a1ec75":"markdown","8bd9f647":"markdown","d5628d80":"markdown","e5df8e95":"markdown","0e656c2a":"markdown","2ad63f9b":"markdown","9ac20a31":"markdown","15fe890c":"markdown"},"source":{"75a4d5de":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#plt.style.use('fivethirtyeight')\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn import svm\nimport xgboost\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport shap\n\nimport warnings\nwarnings.filterwarnings('ignore')","ed03fe1d":"def perf_measure(y_actual, y_hat):\n    TP = 0\n    FP = 0\n    TN = 0\n    FN = 0\n\n    for i in range(len(y_hat)): \n        if y_actual[i]==y_hat[i]==1:\n           TP += 1\n        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n           FP += 1\n        if y_actual[i]==y_hat[i]==0:\n           TN += 1\n        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n           FN += 1\n\n    return(TP, FP, TN, FN)","b201d820":"!pip install xai\n!pip install sage-importance","bc4fdb70":"#https:\/\/github.com\/EthicalML\/xai\nimport xai\n#https:\/\/github.com\/iancovert\/sage\nimport sage","0a24fd5d":"df = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")","a42747b7":"df.head()","f597d18d":"np.unique(df[\"sex\"]==1,return_counts=True)","b13dda76":"figure, axes = plt.subplots(7, 2)\nfigure.set_figheight(70)\nfigure.set_figwidth(30)\n\na = sns.countplot(x=\"sex\", data=df,ax=axes[0,0])\na.set_xticklabels([\"female\",\"male\"])\na.set_title(\"Gender\")\n\nb = sns.swarmplot(data=df, x=\"sex\", y=\"age\",ax=axes[0,1],color=\".2\")\nsns.violinplot(data=df, x=\"sex\", y=\"age\",ax=axes[0,1], inner=None)\nb.set_xticklabels([\"female\",\"male\"])\n\nc = sns.countplot(x=\"cp\", hue=\"sex\", data=df,ax=axes[1,0])\nc.set_xticklabels([\"typical angina\", \"atypical angina\",\"non-anginal pain\",\"asymptomatic\"], rotation = 30)\nc.set_title(\"Experienced chest pain\")\n\nd= sns.swarmplot(data=df, x=\"sex\", y=\"trestbps\",ax=axes[1,1],color=\".2\")\nsns.violinplot(data=df, x=\"sex\", y=\"trestbps\",ax=axes[1,1], inner=None)\nd.set_xticklabels([\"female\",\"male\"])\nd.set_ylabel(\"Blood pressure at rest (mmHG)\")\n\ne=sns.swarmplot(data=df, x=\"sex\", y=\"chol\",ax=axes[2,0],color=\".2\", edgecolor=\"gray\")\nsns.violinplot(data=df, x=\"sex\", y=\"chol\",ax=axes[2,0], inner=None)\ne.set_xticklabels([\"female\",\"male\"])\ne.set_ylabel(\"cholesterol (mg\/dl)\")\n\nf = sns.countplot(x=\"fbs\", hue=\"sex\", data=df,ax=axes[2,1])\nf.set_xticklabels([\"< 120\",\"> 120\"])\nf.set_title(\"fasting blood sugar (mg\/dl)\")\n\ng = sns.countplot(x=\"restecg\", hue=\"sex\", data=df,ax=axes[3,0])\ng.set_title(\"ECG measurements (rest)\")\ng.set_xticklabels([\"normal\",\"ST-T wave abnormality\",\"ventricular hypertrophy\"], rotation = 10)\ng.set_xlabel(\" \")\n\nh = sns.swarmplot(data=df, x=\"sex\", y=\"thalach\",ax=axes[3,1],color=\".2\", edgecolor=\"gray\")\nsns.violinplot(data=df, x=\"sex\", y=\"thalach\",ax=axes[3,1], inner=None)\nh.set_xticklabels([\"female\",\"male\"])\nh.set_ylabel(\"maximum heart rate achieved (bpm)\")\n\ni = sns.countplot(x=\"exang\", hue=\"sex\", data=df,ax=axes[4,0])\ni.set_title(\"Exercise induced angina\")\ni.set_xticklabels([\"no\",\"yes\"])\n\nj = sns.swarmplot(data=df, x=\"sex\", y=\"oldpeak\",ax=axes[4,1],color=\".2\", edgecolor=\"gray\")\nj = sns.violinplot(data=df, x=\"sex\", y=\"oldpeak\",ax=axes[4,1], inner=None)\nj.set_ylabel(\"ST segment depression induced by exercise relative to rest\")\nj.set_xticklabels([\"female\",\"male\"])\n\nk = sns.countplot(x=\"slope\", hue=\"sex\", data=df,ax=axes[5,0])\nk.set_title(\"slope of the peak exercise ST segment\")\nk.set_xticklabels([\"upsloping\",\"flat\", \"downsloping\"])\n\nl = sns.countplot(x=\"ca\", hue=\"sex\", data=df,ax=axes[5,1])\nl.set_title(\"The number of major vessels\")\n\nm = sns.countplot(x=\"thal\", hue=\"sex\", data=df,ax=axes[6,0])\nm.set_title(\"Thalassemia (blood disorder)\")\nm.set_xticklabels([\"unknown\",\"normal\", \"fixed defect\",\"reversable defect\"],rotation = 10)\n\nn = sns.countplot(x=\"target\", hue=\"sex\", data=df,ax=axes[6,1])\nn.set_title(\"Heart disease\")\nn.set_xticklabels([\"no\", \"yes\"])\n\nplt.show()","b638be4e":"sns.pairplot(df, hue=\"sex\",corner=True)\nplt.show()","377f087b":"_ = xai.correlations(df, include_categorical=True)","1d1164c1":"_ = xai.correlations(df, include_categorical=True, plot_type=\"matrix\")","84a4a1e5":"X = df.iloc[:,:13]\ny = df.iloc[:,13]","7cdd838d":"x_dev, y_dev, x_test, y_test, train_idx, test_idx = \\\n    xai.balanced_train_test_split(\n            X, y, \"sex\", \n            min_per_group=15,\n            max_per_group=15,\n            categorical_cols=['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n       'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal'])","b5f52d8c":"df_dev = x_dev.copy()","a6a9d3f6":"df_dev[\"target\"]= y_dev","0121669d":"df_exp = df_dev.copy() ","daf4b8fc":"df_exp['sex'][df_exp['sex'] == 0] = 'female'\ndf_exp['sex'][df_exp['sex'] == 1] = 'male'\ndf_exp['target'][df_exp['target'] == 0] = 'no CVD'\ndf_exp['target'][df_exp['target'] == 1] = 'CVD'","56ba0402":"ims = xai.imbalance_plot(df_exp, \"sex\")","ab9ba2d0":"ims = xai.imbalance_plot(df_exp, \"sex\", \"target\")","75ca1fd1":"bal_df = xai.balance(df_exp, \"sex\", \"target\", upsample=1.0)","d3612c66":"figure, axes = plt.subplots(7, 2)\nfigure.set_figheight(70)\nfigure.set_figwidth(30)\n\na = sns.countplot(x=\"sex\", data=bal_df,ax=axes[0,0])\na.set_xticklabels([\"female\",\"male\"])\na.set_title(\"Gender\")\n\nb = sns.swarmplot(data=bal_df, x=\"sex\", y=\"age\",ax=axes[0,1],color=\".2\")\nsns.violinplot(data=bal_df, x=\"sex\", y=\"age\",ax=axes[0,1], inner=None)\nb.set_xticklabels([\"female\",\"male\"])\n\nc = sns.countplot(x=\"cp\", hue=\"sex\", data=bal_df,ax=axes[1,0])\nc.set_xticklabels([\"typical angina\", \"atypical angina\",\"non-anginal pain\",\"asymptomatic\"], rotation = 30)\nc.set_title(\"Experienced chest pain\")\n\nd= sns.swarmplot(data=bal_df, x=\"sex\", y=\"trestbps\",ax=axes[1,1],color=\".2\")\nsns.violinplot(data=bal_df, x=\"sex\", y=\"trestbps\",ax=axes[1,1], inner=None)\nd.set_xticklabels([\"female\",\"male\"])\nd.set_ylabel(\"Blood pressure at rest (mmHG)\")\n\ne=sns.swarmplot(data=bal_df, x=\"sex\", y=\"chol\",ax=axes[2,0],color=\".2\", edgecolor=\"gray\")\nsns.violinplot(data=bal_df, x=\"sex\", y=\"chol\",ax=axes[2,0], inner=None)\ne.set_xticklabels([\"female\",\"male\"])\ne.set_ylabel(\"cholesterol (mg\/dl)\")\n\nf = sns.countplot(x=\"fbs\", hue=\"sex\", data=bal_df,ax=axes[2,1])\nf.set_xticklabels([\"< 120\",\"> 120\"])\nf.set_title(\"fasting blood sugar (mg\/dl)\")\n\ng = sns.countplot(x=\"restecg\", hue=\"sex\", data=bal_df,ax=axes[3,0])\ng.set_title(\"ECG measurements (rest)\")\ng.set_xticklabels([\"normal\",\"ST-T wave abnormality\",\"ventricular hypertrophy\"], rotation = 10)\ng.set_xlabel(\" \")\n\nh = sns.swarmplot(data=bal_df, x=\"sex\", y=\"thalach\",ax=axes[3,1],color=\".2\", edgecolor=\"gray\")\nsns.violinplot(data=bal_df, x=\"sex\", y=\"thalach\",ax=axes[3,1], inner=None)\nh.set_xticklabels([\"female\",\"male\"])\nh.set_ylabel(\"maximum heart rate achieved (bpm)\")\n\ni = sns.countplot(x=\"exang\", hue=\"sex\", data=bal_df,ax=axes[4,0])\ni.set_title(\"Exercise induced angina\")\ni.set_xticklabels([\"no\",\"yes\"])\n\nj = sns.swarmplot(data=bal_df, x=\"sex\", y=\"oldpeak\",ax=axes[4,1],color=\".2\", edgecolor=\"gray\")\nj = sns.violinplot(data=bal_df, x=\"sex\", y=\"oldpeak\",ax=axes[4,1], inner=None)\nj.set_ylabel(\"ST segment depression induced by exercise relative to rest\")\nj.set_xticklabels([\"female\",\"male\"])\n\nk = sns.countplot(x=\"slope\", hue=\"sex\", data=bal_df,ax=axes[5,0])\nk.set_title(\"slope of the peak exercise ST segment\")\nk.set_xticklabels([\"upsloping\",\"flat\", \"downsloping\"])\n\nl = sns.countplot(x=\"ca\", hue=\"sex\", data=bal_df,ax=axes[5,1])\nl.set_title(\"The number of major vessels\")\n\nm = sns.countplot(x=\"thal\", hue=\"sex\", data=bal_df,ax=axes[6,0])\nm.set_title(\"Thalassemia (blood disorder)\")\nm.set_xticklabels([\"unknown\",\"normal\", \"fixed defect\",\"reversable defect\"],rotation = 10)\n\nn = sns.countplot(x=\"target\", hue=\"sex\", data=bal_df,ax=axes[6,1])\nn.set_title(\"Heart disease\")\nn.set_xticklabels([\"no\", \"yes\"])\n\nplt.show()","c4aa3486":"sns.pairplot(bal_df, hue=\"sex\",corner=True)\nplt.show()","fb28c9ad":"_ = xai.correlations(bal_df, include_categorical=True, plot_type=\"matrix\")","ce6b8fc3":"x_dev = x_dev.reset_index()\ny_dev = y_dev","106461e0":"x_dev = x_dev.iloc[:,1:]","c608d331":"corr = df_dev.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\n\nplt.figure(figsize=(10,8))\nsns.heatmap(corr, annot=True,mask=mask)","941e7668":"skf =  StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(x_dev,y_dev):\n    model =GaussianNB()\n    X_train, X_val = x_dev.loc[train_index], x_dev.loc[val_index]\n    y_train, y_val = y_dev[train_index], y_dev[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") \n\n    ","103c0d74":"skf =  StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(x_dev,y_dev):\n    model =LogisticRegression(solver=\"liblinear\", random_state=0)\n    X_train, X_val = x_dev.loc[train_index], x_dev.loc[val_index]\n    y_train, y_val = y_dev[train_index], y_dev[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_test, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") ","eec99b0a":"skf =  StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(x_dev,y_dev):\n    model =svm.SVC()\n    X_train, X_val = x_dev.loc[train_index], x_dev.loc[val_index]\n    y_train, y_val = y_dev[train_index], y_dev[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_test, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") ","12f4920a":"skf =  StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(x_dev,y_dev):\n    model =xgboost.XGBClassifier(use_label_encoder=False, verbosity=0)\n    X_train, X_val = x_dev.loc[train_index], x_dev.loc[val_index]\n    y_train, y_val = y_dev[train_index], y_dev[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_test, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") ","006462b5":"skf =  StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(x_dev,y_dev):\n    model = MLPClassifier(random_state=1, max_iter=500, learning_rate=\"adaptive\", solver=\"adam\", activation=\"relu\", tol=0.0001, momentum=0.99)\n    X_train, X_val = x_dev.loc[train_index], x_dev.loc[val_index]\n    y_train, y_val = y_dev[train_index], y_dev[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_test, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") ","e69d8623":"skf =  StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(x_dev,y_dev):\n    model = RandomForestClassifier()\n    X_train, X_val = x_dev.loc[train_index], x_dev.loc[val_index]\n    y_train, y_val = y_dev[train_index], y_dev[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_test, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") ","51f6e601":"skf =  StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(x_dev,y_dev):\n    model =xgboost.XGBClassifier(use_label_encoder=False, verbosity=0)\n    X_train, X_val = x_dev.loc[train_index], x_dev.loc[val_index]\n    y_train, y_val = y_dev[train_index], y_dev[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_val, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") ","373da8e6":"model =xgboost.XGBClassifier(use_label_encoder=False, verbosity=0)\nmodel.fit(x_dev,y_dev)\ny_hat = model.predict(x_test)\nprint(f\"ROC = {roc_auc_score(y_test, y_hat)}\")\nprint(f\"Accuracy = {accuracy_score(y_test, y_hat)}\")\nprint(f\"F1-score(macro) = {f1_score(y_test, y_hat, average='macro')}\")\nprint(f\"F1-score(micro) = {f1_score(y_test, y_hat, average='micro')}\")\nprint(f\"Recall= {recall_score(y_test, y_hat)}\") \nprint(f\"Precision= {precision_score(y_test, y_hat)}\") ","6ecaceb4":"shap.initjs()","daa6c962":"explainer = shap.KernelExplainer(model.predict_proba, x_dev, link=\"logit\")","e1016bb7":"shap_values = explainer.shap_values(x_test)","c331607f":"shap.force_plot(explainer.expected_value[0], shap_values[0][0,:], x_test.iloc[0,:], link=\"logit\")","3367a2bc":"shap.force_plot(explainer.expected_value[0], shap_values[1][0,:], x_test.iloc[0,:], link=\"logit\")","62bdf2d0":"shap.force_plot(explainer.expected_value[0], shap_values[0], x_test, link=\"logit\")","ad5ea57a":"shap.summary_plot(shap_values, x_test)","4e11ae56":"explainer = shap.Explainer(model, x_dev)\nshap_values = explainer(x_test)\nshap.plots.beeswarm(shap_values,max_display=14)","a5eda12c":"# Setup and calculate\nimputer = sage.MarginalImputer(model, np.asarray(x_dev))\nestimator = sage.PermutationEstimator(imputer, 'cross entropy')\nsage_values = estimator(np.asarray(x_test), np.asarray(y_test))","5c7a9514":"sage_values.plot(x_test.columns, title='Feature Importance (Marginal Sampling)')","a1416f6f":"sensitivity = estimator(np.asarray(x_test))\nsensitivity.plot(x_test.columns, title='Model Sensitivity')","1544cb14":"y_hat_prob = model.predict_proba(x_test)[:,1]","c98ca187":"_= xai.metrics_plot(\n        y_test, \n        y_hat_prob)","191f0f4f":"_ = xai.metrics_plot(\n    y_test, \n    y_hat_prob, \n    df=x_test, \n    cross_cols=[\"sex\"],\n    categorical_cols=['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n       'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal'])","6cdd3343":"xai.confusion_matrix_plot(y_test, y_hat)","baff3e4d":"_ = xai.roc_plot(y_test, y_hat_prob)","d50558f7":"perf_measure(y_test[np.where(x_test[\"sex\"]==0)[0]], y_hat[np.where(x_test[\"sex\"]==0)[0]])","7f19b05b":"perf_measure(y_test[np.where(x_test[\"sex\"]==1)[0]], y_hat[np.where(x_test[\"sex\"]==1)[0]])","3d331c1b":"protected = [\"sex\"]\n_ = [xai.roc_plot(\n    y_test, \n    y_hat_prob, \n    df=x_test, \n    cross_cols=[p],\n    categorical_cols=['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n       'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']) for p in protected]","9b41e27c":"d = xai.smile_imbalance(\n    y_test, \n    np.asarray(y_hat_prob),\n    display_breakdown=True)","bab3da49":"bal_df['target'][bal_df['target'] == 'no CVD'] = 0\nbal_df['target'][bal_df['target'] == 'CVD'] = 1\nbal_df['sex'][bal_df['sex'] == 'female'] = 0\nbal_df['sex'][bal_df['sex'] == 'male'] = 1\n\nbal_df['sex'] = bal_df['sex'].astype(int)\nbal_df['target'] = bal_df['target'].astype(int)","2b151dab":"bal_df","666d6d2f":"corr = bal_df.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\n\nplt.figure(figsize=(10,8))\nsns.heatmap(corr, annot=True,mask=mask)\nplt.show()","c73545de":"X_bal = bal_df.iloc[:,:13]\ny_bal = bal_df.iloc[:,13]","1c029c3e":"skf =  StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(X_bal,y_bal):\n    model =GaussianNB()\n    X_train, X_val = X_bal.loc[train_index], X_bal.loc[val_index]\n    y_train, y_val = y_bal[train_index], y_bal[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_val, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") \n\n    ","f84b68c7":"skf =  StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(X_bal,y_bal):\n    model =LogisticRegression(solver=\"liblinear\", random_state=0)\n    X_train, X_val = X_bal.loc[train_index], X_bal.loc[val_index]\n    y_train, y_val = y_bal[train_index], y_bal[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_test, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") ","7692edfd":"skf =  StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(X_bal,y_bal):\n    model =svm.SVC()\n    X_train, X_val = X_bal.loc[train_index], X_bal.loc[val_index]\n    y_train, y_val = y_bal[train_index], y_bal[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_test, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") ","304af24d":"skf =  StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(X_bal,y_bal):\n    model = MLPClassifier(random_state=1, max_iter=500, learning_rate=\"adaptive\", solver=\"adam\", activation=\"relu\", tol=0.0001, momentum=0.99)\n    X_train, X_val = X_bal.loc[train_index], X_bal.loc[val_index]\n    y_train, y_val = y_bal[train_index], y_bal[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_test, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") ","66488e14":"skf =  StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(X_bal,y_bal):\n    model =xgboost.XGBClassifier(use_label_encoder=False,verbosity=0)\n    X_train, X_val = X_bal.loc[train_index], X_bal.loc[val_index]\n    y_train, y_val = y_bal[train_index], y_bal[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_test, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") ","8f5041e1":"skf =  StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(X_bal,y_bal):\n    model = RandomForestClassifier()\n    X_train, X_val = X_bal.loc[train_index], X_bal.loc[val_index]\n    y_train, y_val = y_bal[train_index], y_bal[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_test, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") ","bdc4e960":"skf =  StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(X_bal,y_bal):\n    model =xgboost.XGBClassifier(use_label_encoder=False,verbosity=0 )\n    X_train, X_val = X_bal.loc[train_index], X_bal.loc[val_index]\n    y_train, y_val = y_bal[train_index], y_bal[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_test, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") ","8eb8ef24":"model =xgboost.XGBClassifier(use_label_encoder=False, verbosity=0)\nmodel.fit(X_bal,y_bal)\ny_hat = model.predict(x_test)\nprint(f\"ROC = {roc_auc_score(y_test, y_hat)}\")\nprint(f\"Accuracy = {accuracy_score(y_test, y_hat)}\")\nprint(f\"F1-score(macro) = {f1_score(y_test, y_hat, average='macro')}\")\nprint(f\"F1-score(micro) = {f1_score(y_test, y_hat, average='micro')}\")\nprint(f\"Recall= {recall_score(y_test, y_hat)}\") \nprint(f\"Precision= {precision_score(y_test, y_hat)}\") ","47c10e33":"explainer = shap.KernelExplainer(model.predict_proba, X_bal, link=\"logit\")","3c041373":"shap_values = explainer.shap_values(x_test)","30f9047c":"shap.force_plot(explainer.expected_value[0], shap_values[0][0,:], x_test.iloc[0,:], link=\"logit\")","523f487e":"shap.force_plot(explainer.expected_value[0], shap_values[1][0,:], x_test.iloc[0,:], link=\"logit\")","4bd0c604":"shap.force_plot(explainer.expected_value[0], shap_values[0], x_test, link=\"logit\")","1e92dfd5":"shap.summary_plot(shap_values, x_test)","0c537ae2":"explainer = shap.Explainer(model, x_dev)\nshap_values = explainer(x_test)\nshap.plots.beeswarm(shap_values,max_display=14)","93314565":"# Setup and calculate\nimputer = sage.MarginalImputer(model, np.asarray(X_bal))\nestimator = sage.PermutationEstimator(imputer, 'cross entropy')\nsage_values = estimator(np.asarray(x_test), np.asarray(y_test))\n\nsage_values.plot(x_test.columns, title='Feature Importance (Marginal Sampling)')","f79980f7":"sensitivity = estimator(np.asarray(x_test))\nsensitivity.plot(x_test.columns, title='Model Sensitivity')","5eceeb71":"y_hat_prob = model.predict_proba(x_test)[:,1]","0700facc":"_= xai.metrics_plot(\n        y_test, \n        y_hat_prob)","3482ccaa":"_ = xai.metrics_plot(\n    y_test, \n    y_hat_prob, \n    df=x_test, \n    cross_cols=[\"sex\"],\n    categorical_cols=['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n       'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal'])","a962665a":"xai.confusion_matrix_plot(y_test, y_hat)","70efad8e":"_ = xai.roc_plot(y_test, y_hat_prob)","0512d32c":"perf_measure(y_test[np.where(x_test[\"sex\"]==0)[0]], y_hat[np.where(x_test[\"sex\"]==0)[0]])","99322e8a":"perf_measure(y_test[np.where(x_test[\"sex\"]==1)[0]], y_hat[np.where(x_test[\"sex\"]==1)[0]])","becae14c":"protected = [\"sex\"]\n_ = [xai.roc_plot(\n    y_test, \n    y_hat_prob, \n    df=x_test, \n    cross_cols=[p],\n    categorical_cols=['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n       'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']) for p in protected]","5ff18e0c":"df_m = df.loc[df['sex'] == 1]\ndf_m = df_m.reset_index()","222f8c39":"X = df_m.iloc[:,1:-1]\ny = df_m.iloc[:,-1]","bdbe7639":"skf =  StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(X,y):\n    model =xgboost.XGBClassifier(use_label_encoder=False, verbosity=0)\n    X_train, X_val = X.loc[train_index], X.loc[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_test, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") ","aeb0f84a":"explainer = shap.KernelExplainer(model.predict_proba, X_train, link=\"logit\")","aee4d8bf":"shap_values = explainer.shap_values(X_val)","f996b34b":"shap.summary_plot(shap_values, X_val)","0d9967d0":"explainer = shap.Explainer(model, X_train)\nshap_values = explainer(X_val)\nshap.plots.beeswarm(shap_values, max_display=14)","9f4d8207":"# Setup and calculate\nimputer = sage.MarginalImputer(model, np.asarray(X_train))\nestimator = sage.PermutationEstimator(imputer, 'cross entropy')\nsage_values = estimator(np.asarray(X_val), np.asarray(y_val))\n\nsage_values.plot(X_val.columns, title='Feature Importance (Marginal Sampling)')","6fe638ab":"sensitivity = estimator(np.asarray(X_val))\nsensitivity.plot(X_val.columns, title='Model Sensitivity')","5c527450":"df_f = df.loc[df['sex'] == 0]\ndf_f = df_f.reset_index()","26cc2ae0":"X = df_f.iloc[:,1:-1]\ny = df_f.iloc[:,-1]","bbccfbdf":"skf =  StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(X,y):\n    model =xgboost.XGBClassifier(use_label_encoder=False, verbosity=0)\n    X_train, X_val = X.loc[train_index], X.loc[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_test, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\nprint(f\"Mean ROC = {roc.mean()}\")\nprint(f\"Mean Accuracy = {acc.mean()}\")\nprint(f\"Mean F1-score(macro) = {f1ma.mean()}\")\nprint(f\"Mean F1-score(micro) = {f1mi.mean()}\") \nprint(f\"Mean Recall= {rec.mean()}\") \nprint(f\"Mean Precision= {pre.mean()}\") ","c9a4b79b":"shap_values = explainer.shap_values(X_val)","6a027b9b":"shap.summary_plot(shap_values, X_val)","0d93a676":"explainer = shap.Explainer(model, X_train)\nshap_values = explainer(X_val)\nshap.plots.beeswarm(shap_values,max_display=14)","b985250c":"# Setup and calculate\nimputer = sage.MarginalImputer(model, np.asarray(X_train))\nestimator = sage.PermutationEstimator(imputer, 'cross entropy')\nsage_values = estimator(np.asarray(X_val), np.asarray(y_val))\n\nsage_values.plot(X_val.columns, title='Feature Importance (Marginal Sampling)')","61553765":"sensitivity = estimator(np.asarray(X_val))\nsensitivity.plot(X_val.columns, title='Model Sensitivity')","79adbcc5":"## Global explanation using SAGE","c88c44a5":"### Lets now look at the same models using the balanced dataset as input","15dfb7e1":"# Setup and calculate\nimputer = sage.MarginalImputer(model, np.asarray(X_train))\nestimator = sage.PermutationEstimator(imputer, 'cross entropy')\nsage_values = estimator(np.asarray(X_test), np.asarray(y_test))\n\nsage_values.plot(X_test.columns, title='Feature Importance (Marginal Sampling)')","f8f93b50":"## Global explanation using SAGE","4a0fe98b":"### Random Forest Classifier","f863328a":"# <center> Gender differences in risk factors for heart disease<\/center>\n### In this notebook I use the well known Heart Disease UCI dataset to explore gender differences related to risk factors for developing heart disease\nA study done in Great Britain estimates that more than 8,200 women in England and Wales died over a ten-year period because they did not receive equal treatment to men. The risk of heart disease in women is often underestimated due to the misperception that females are \u2018protected\u2019 against cardiovascular disease. The under-recognition of heart disease and differences in clinical presentation in women lead to less aggressive treatment strategies and a lower representation of women in clinical trials. Furthermore, self-awareness in women and identification of their cardiovascular risk factors needs more attention, which should result in a better prevention of cardiovascular events.  AI-based algorithms are known to be prone to acquire bias and thus dedicated framwork should be used to reveal those biases in the data","5b838df8":"shap.summary_plot(shap_values, X_test)","c2ac30ea":"### Support Vector Machine Classifier","aa93a033":"### Logistic Regression Classifier","60b48269":"## First we train and validate a couple of models based on the original dataset (almost twice as many males as females). Furthermore, we will explain the predictions using SHAP","d57a779f":"## Only women","f0ecca6a":"### Random Forest Classifier","5b5f3117":"<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https:\/\/c.tenor.com\/lJHChcg2eMgAAAAC\/beating-heart.gif\" alt=\"Heart beating\" style=\"height:500px;margin-top:3rem;\"> <\/div>","0a5e3c72":"# Conclusion\nWe can clearly see that in the balanced data set, the number of major vessels (ca) has a higher importance than in the imbalanced dataset dominated by males. Furthermore, we see that cholesterol (chol) and age (age) have a much higher impact on the total risk of developing heart disease in the balanced dataset than in the unbalanced dataset. This findings can be backed by previous research which shows that both cholesterol and age are a major risk factor in both male and females [1].\n\n[1] Lloyd-Jones DM, Wilson PWF, Larson MG, et al. Lifetime Risk of Coronary Heart Disease by Cholesterol Levels at Selected Ages. Arch Intern Med. 2003;163(16):1966\u20131972. doi:10.1001\/archinte.163.16.1966\n","c2bd3386":"## Check correlation","e7c6eec2":"## Global explanation using SAGE","59b5e6f9":"## Make a gender balanced dataset:","8f486860":"X = df.iloc[:,:13]\ny = df.iloc[:,13]\n\nskf =  StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nroc = np.zeros((10,1))\nacc = np.zeros((10,1))\nf1ma = np.zeros((10,1))\nf1mi = np.zeros((10,1))\nrec = np.zeros((10,1))\npre = np.zeros((10,1))\ncnt = 0\nfor train_index, val_index in skf.split(X,y):\n    model =xgboost.XGBClassifier(use_label_encoder=False, verbosity=0)\n    X_train, X_val = X.loc[train_index], X.loc[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n    model.fit(X_train,y_train)\n    y_hat = model.predict(X_val)\n    #print(classification_report(y_test, y_hat))\n    roc[cnt] = roc_auc_score(y_val, y_hat)\n    acc[cnt] = accuracy_score(y_val, y_hat)\n    f1ma[cnt] = f1_score(y_val, y_hat, average='macro')\n    f1mi[cnt] = f1_score(y_val, y_hat, average='micro')\n    rec[cnt] = recall_score(y_val,y_hat)\n    pre[cnt] = precision_score(y_val,y_hat)\n    \n    cnt += 1\n#print(f\"Mean ROC = {roc.mean()}\")\n#print(f\"Mean Accuracy = {acc.mean()}\")\n#print(f\"Mean F1-score(macro) = {f1ma.mean()}\")\n#print(f\"Mean F1-score(micro) = {f1mi.mean()}\") \n#print(f\"Mean Recall= {rec.mean()}\") \n#print(f\"Mean Precision= {pre.mean()}\") \n\nexplainer = shap.KernelExplainer(model.predict_proba, X_train, link=\"logit\")\nshap_values = explainer.shap_values(X_val)","5740e471":"## Global explanation using SAGE","1f16a7b5":"## Only men","62a60b3e":"### Support Vector Machine Classifier","fb074294":"### XGBoost Classifier","d2bd9fbc":"## Make an equal dev\/test split","3e44a955":"### Gaussian Naive Bayes Classifier","11c1eb4c":"## Ethical AI","42ce3fec":"## Feature explanation:\n* age: The person's age in years\n* sex: The person's sex (1 = male, 0 = female)\n* cp: The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n* trestbps: The person's resting blood pressure (mm Hg on admission to the hospital)\n* chol: The person's cholesterol measurement in mg\/dl\n* fbs: The person's fasting blood sugar (> 120 mg\/dl, 1 = true; 0 = false)\n* restecg: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n* thalach: The person's maximum heart rate achieved\n* exang: Exercise induced angina (1 = yes; 0 = no)\n* oldpeak: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more here)\n* slope: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n* ca: The number of major vessels (0-3)\n* thal: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n* target: Heart disease (0 = no, 1 = yes)","f3c0a8bb":"sensitivity = estimator(np.asarray(X_test))\nsensitivity.plot(X_test.columns, title='Model Sensitivity')","fc1f718a":"### Feed Forward Neural Network","bb4960e8":"## Here we make a copy of the dataset to explore gender differences","077b1b08":"### Gaussian Naive Bayes Classifier","05a1ec75":"## Global explanation using SAGE","8bd9f647":"### Lets retrain the best model and look at the explanation using SHAP","d5628d80":"### Now, lets take look back at the shapley values and feature importance for the model trained on the unbalanced dataset again","e5df8e95":"### Logistic Regression Classifier","0e656c2a":"### Lets retrain the best model and look at the explanation using SHAP","2ad63f9b":"## Ethical AI","9ac20a31":"### XG Boost Classifier","15fe890c":"### Feed Forward Neural Network"}}