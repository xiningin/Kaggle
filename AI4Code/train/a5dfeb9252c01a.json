{"cell_type":{"9c0376b2":"code","f78bd9e6":"code","41399e87":"code","d412bb85":"code","07681715":"code","3418a9e3":"code","e74bf628":"code","440efdc9":"code","6256ee9a":"code","8e444bfd":"code","0da28472":"code","a9024728":"code","a5bae873":"code","b9f0b134":"code","e808fe39":"code","751b1f5a":"code","2cdde5c3":"code","c9b72bf8":"code","1ec439d6":"code","f80d9329":"code","85e39d0c":"code","4dc3d84e":"code","07a0d837":"code","1074e64d":"code","2bed3c86":"code","622f69a8":"code","73db28f7":"code","4d18d770":"code","9b36264e":"code","4ff5c00d":"code","1835c457":"code","a6f9465c":"code","a89d7779":"code","cf4631fb":"code","fd8800f2":"code","051150dd":"code","1e2964fc":"code","5e2b323f":"code","824101c8":"code","5f925636":"code","fe7845a7":"markdown","a214e4ad":"markdown","abffb05c":"markdown","5efe5298":"markdown","4c3ef37b":"markdown","2f5bf8c6":"markdown","066fd4b3":"markdown","7764b832":"markdown","3d3da5ae":"markdown","ea103f10":"markdown","9e3ccb72":"markdown","109866f1":"markdown","e0fda5df":"markdown","5c87259a":"markdown","f86e9df9":"markdown","15cfc3c3":"markdown","64361b98":"markdown","a71b5fdc":"markdown","c487206a":"markdown","51046381":"markdown","b7ee36bc":"markdown","99d6dbf8":"markdown","43668f31":"markdown","b02f869f":"markdown","5ccd4333":"markdown","0272c3af":"markdown","fbc3a4b4":"markdown","97390786":"markdown","f9992097":"markdown","173d8d57":"markdown","36a3daba":"markdown","fb6ee731":"markdown","69890529":"markdown","8fcc399d":"markdown","e900e543":"markdown","132c2531":"markdown","76f558ee":"markdown","9bf25ecc":"markdown","12dcc345":"markdown","d24cb44a":"markdown"},"source":{"9c0376b2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.linear_model import LogisticRegression  \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score, GridSearchCV,train_test_split,cross_val_score\nimport itertools\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neighbors import LocalOutlierFactor # \u00e7ok de\u011fi\u015fkenli ayk\u0131r\u0131 g\u00f6zlem incelemesi\nfrom sklearn.preprocessing import scale,StandardScaler, MinMaxScaler,Normalizer,RobustScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import  accuracy_score, f1_score, precision_score,confusion_matrix, recall_score, roc_auc_score\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nwarnings.filterwarnings(\"ignore\", category=FutureWarning) \nwarnings.filterwarnings(\"ignore\", category=UserWarning) \n\n%config InlineBackend.figure_format = 'retina'\n\n# to display all columns and rows:\npd.set_option('display.max_columns', None); pd.set_option('display.max_rows', None);  # to display all columns and rows\npd.set_option('display.float_format', lambda x: '%.2f' % x) # The number of numbers that will be shown after the comma.\n\n\n","f78bd9e6":"df = pd.read_csv(\"..\/input\/churn-for-bank-customers\/churn.csv\", index_col=0)\ndf.head()","41399e87":"dependent_variable_name = \"Exited\"","d412bb85":"def data_prepare():\n    df_prep = df.copy()\n    \n    \n    missing_value_len = df.isnull().any().sum()\n    if missing_value_len == 0:\n        print(\"No Missing Value\")\n    else:\n        print(\"Investigate Missing Value, Missing Value : \" + str(missing_value_len))\n    print(\"\\n\")\n    \n    show_unique_count_variables(df = df_prep)\n    \n    \n    df_prep['Tenure'] =  df_prep.Tenure.astype(np.float)\n    df_prep['NumOfProducts'] =  df_prep.NumOfProducts.astype(np.float)\n    return df_prep\n","07681715":"def show_unique_count_variables(df):\n    for index, value in df.nunique().items():\n        print(str(index) + \"\\n\\t\\t\\t:\" + str(value))","3418a9e3":"\ndef show_outliers(df):\n    \n    #t\u00fcm s\u00fct\u00fcnlar\u0131 yukardan bakarak  outlier feature'lar\u0131 g\u00f6zlemleme\n    \n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    df_num_cols = df.select_dtypes(include=numerics)\n    sns.set(font_scale = 0.7) \n    fig, axes = plt.subplots(nrows = 2, ncols = 5, gridspec_kw =  dict(hspace=0.3), figsize = (12,9))\n    fig.tight_layout()\n    for ax,col in zip(axes.flatten(), df_num_cols.columns):\n        sns.boxplot(x = df_num_cols[col], color='green', ax = ax)\n    fig.suptitle('Observing Outliers', color = 'r', fontsize = 14)","e74bf628":"\ndef lof_observation(df):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    df_num_cols = df.select_dtypes(include=numerics)\n    df_outlier = df_num_cols.astype(\"float64\")\n    clf = LocalOutlierFactor(n_neighbors = 20, contamination = 0.1)\n    clf.fit_predict(df_outlier)\n    df_scores = clf.negative_outlier_factor_\n    scores_df = pd.DataFrame(np.sort(df_scores))\n    \n    scores_df.plot(stacked=True, xlim = [0,20], color='r', title='Visualization of outliers according to the LOF method', style = '.-');                # first 20 observe\n    th_val = np.sort(df_scores)[2]\n    outliers = df_scores > th_val\n    df = df.drop(df_outlier[~outliers].index)\n    df.shape\n    return df","440efdc9":"\ndef clear_outliers(df):\n    \n    #yas ve kredi_skoru bask\u0131lama\n    \n    Q1 = df[\"Age\"].quantile(0.25)\n    Q3 = df[\"Age\"].quantile(0.75)\n    IQR = Q3 - Q1\n    lower = Q1 - 1.5 * IQR\n    upper = Q3 + 1.5 * IQR\n    print(\"When age and credit score is printed below lower score: \", lower, \"and upper score: \", upper)\n    df_outlier = df[\"Age\"][(df[\"Age\"] > upper)]\n    df[\"Age\"][df_outlier.index] = upper\n    \n    \n    #kredi_skoru\n    \n    Q1 = df[\"CreditScore\"].quantile(0.25)\n    Q3 = df[\"CreditScore\"].quantile(0.75)\n    IQR = Q3 - Q1\n    lower = Q1 - 1.5 * IQR\n    upper = Q3 + 1.5 * IQR\n    print(\"When age and credit score is printed above lower score: \", lower, \"and upper score: \", upper)\n    df_outlier = df[\"CreditScore\"][(df[\"CreditScore\"] < lower)]\n    df[\"CreditScore\"][df_outlier.index] = lower\n    \n    return df","6256ee9a":"\ndef outlier_process(df):\n    #show_outliers(df = df)\n    df_outlier = lof_observation(df = df)\n    df_outlier = clear_outliers(df = df_outlier)\n    return df_outlier","8e444bfd":"\ndef show_dependent_variable(df):\n#     sns.countplot(data = df, x = dependent_variable_name, label = 'Count') \\\n#     .set_title(dependent_variable_name + ' dependent variable situation', fontsize = 18, color = 'r')\n    fig, axarr = plt.subplots(2, 3, figsize=(18, 6))\n    sns.countplot(x = 'Geography', hue = 'Exited',data = df, ax = axarr[0][0])\n    sns.countplot(x = 'Gender', hue = 'Exited',data = df, ax = axarr[0][1])\n    sns.countplot(x = 'HasCrCard', hue = 'Exited',data = df, ax = axarr[0][2])\n    sns.countplot(x = 'IsActiveMember', hue = 'Exited',data = df, ax = axarr[1][0])\n    sns.countplot(x = 'NumOfProducts', hue = 'Exited',data = df, ax = axarr[1][1])\n    sns.countplot(x = 'Tenure', hue = 'Exited',data = df, ax = axarr[1][2])\n    zero, one = df[dependent_variable_name].value_counts()\n    print(\"Dependent variable distribution;\")\n    print(dependent_variable_name + \" 0 count:\", zero)\n    print(dependent_variable_name + \" 1 count:\", one)","0da28472":"\ndef show_numeric_columns_distributions(df):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    df_num_cols = df.select_dtypes(include=numerics)\n    columns = df_num_cols.columns[: len(df_num_cols.columns)]\n    fig = plt.figure()\n    fig.set_size_inches(18, 15)\n    #plt.subplots(figsize=(22,22))\n    length = len(columns)\n    for i,j in itertools.zip_longest(columns, range(length)):\n        plt.subplot((length \/ 2), 3, j+1)\n        plt.subplots_adjust(wspace = 0.2, hspace = 0.5)\n        df_num_cols[i].hist(bins = 20, edgecolor = 'black')\n        plt.title(i)\n    fig = fig.suptitle('Structures of numeric variables', color = 'r' ,fontsize = 18)\n    plt.show()","a9024728":"\ndef show_dependent_variable_cross_others_distributions(df):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    df_dependent_var = df[df[dependent_variable_name] == 1]\n    df_num_cols = df_dependent_var.select_dtypes(include = numerics)\n    columns = df_num_cols.columns[: len(df_num_cols.columns)]\n    \n    fig = plt.figure()\n    fig.set_size_inches(18, 15)\n    length = len(columns)\n    for i,j in itertools.zip_longest(columns, range(length)):\n        plt.subplot((length \/ 2), 3, j+1)\n        plt.subplots_adjust(wspace = 0.2, hspace = 0.5)\n        df_num_cols[i].hist(bins = 20, edgecolor = 'black')\n        plt.title(i)\n    fig = fig.suptitle(dependent_variable_name + ' Status of other variables according to 1 dependent variable', color = 'r', fontsize = 18)\n    plt.show()","a5bae873":"\ndef show_dependent_variable_cross_categorical_distributions(df, categorical_columns):\n    sns.set(font_scale = 0.7) \n    fig, axes = plt.subplots(nrows = int( len(categorical_columns) \/ 2 ) , ncols = 2, figsize = (7,9))\n    fig.tight_layout()\n    for ax,col in zip(axes.flatten(), categorical_columns):\n        sns.countplot(x = df[col], hue = dependent_variable_name, data = df, ax = ax)\n    fig.suptitle('Categorical variables are monitored according to the dependent variable', color = 'r', fontsize = 1)\n  ","b9f0b134":"def show_data_analysis(df):\n    show_dependent_variable(df)\n    show_numeric_columns_distributions(df)\n    show_dependent_variable_cross_others_distributions(df)\n    show_dependent_variable_cross_categorical_distributions(df = df_outlier, categorical_columns = [\"Gender\",\"Geography\",\"HasCrCard\",\"IsActiveMember\"])\n","e808fe39":"\ndef credit_score_table(row):\n    \n    credit_score = row.CreditScore\n    if credit_score >= 300 and credit_score < 500:\n        return \"Very_Poor\"\n    elif credit_score >= 500 and credit_score < 601:\n        return \"Poor\"\n    elif credit_score >= 601 and credit_score < 661:\n        return \"Fair\"\n    elif credit_score >= 661 and credit_score < 781:\n        return \"Good\"\n    elif credit_score >= 851:\n        return \"Top\"\n    elif credit_score >= 781 and credit_score < 851:\n        return \"Excellent\"\n    elif credit_score < 300:\n        return \"Deep\"","751b1f5a":"\ndef product_utilization_rate_by_year(row):\n    number_of_products = row.NumOfProducts\n    tenure = row.Tenure\n    \n    if number_of_products == 0:\n        return 0\n    \n    if tenure == 0:\n        return number_of_products\n    \n    rate = number_of_products \/ tenure\n    return rate","2cdde5c3":"\ndef product_utilization_rate_by_estimated_salary(row):\n    number_of_products = row.number_of_products\n    estimated_salary = row.EstimatedSalary\n    \n    if number_of_products == 0:\n        return 0\n\n    \n    rate = number_of_products \/ estimated_salary\n    return rate","c9b72bf8":"\ndef countries_monthly_average_salaries(row):\n    #brutto datas from  https:\/\/tr.wikipedia.org\/wiki\/Ayl\u0131k_ortalama_\u00fccretlerine_g\u00f6re_Avrupa_\u00fclkeleri_listesi\n    fr = 3696    \n    de = 4740\n    sp = 2257\n    salary = row.EstimatedSalary \/ 12\n    country = row.Geography              # Germany, France and Spain\n    \n    if country == 'Germany':\n        return salary \/ de\n    elif country == \"France\":\n        return salary \/ fr\n    elif country == \"Spain\": \n        return salary \/ sp","1ec439d6":"def feature_engineering(df, is_show_graph = False):\n    df_fe = df.copy()\n    \n    #bakiye_maas_orani\n    balance_salary_rate = 'balance_salary_rate'\n    df_fe[balance_salary_rate] = df_fe.Balance \/ df_fe.EstimatedSalary\n    \n    #yila_gore_urun_kullanim_orani\n    df_fe = df_fe.assign(product_utilization_rate_by_year=df_fe.apply(lambda x: product_utilization_rate_by_year(x), axis=1)) \n    \n    #tahmini_maasa_gore_urun_kullanim_orani\n    #df_fe = df_fe.assign(product_utilization_rate_by_estimated_salary = df_fe.apply(lambda x: product_utilization_rate_by_estimated_salary(x), axis=1)) \n    \n    \n    #musteri_yilina g\u00f6re ya\u015fa g\u00f6re standardize edilmesi, oranlanmasi - ergenlik donemini cikariyoruz!\n    tenure_rate_by_age = 'tenure_rate_by_age'\n    df_fe[tenure_rate_by_age] = df_fe.Tenure \/ (df_fe.Age-17)\n    \n    #ya\u015fa g\u00f6re kredi_skoru standardize edilmesi, oranlanmasi - ergenlik donemini cikariyoruz!\n    credit_score_rate_by_age = 'credit_score_rate_by_age'\n    df_fe[credit_score_rate_by_age] = df_fe.CreditScore \/ (df_fe.Age-17)\n    \n    #maa\u015fa gore kullanilan urun oran, oranlanmasi\n    product_utilization_rate_by_salary = 'product_utilization_rate_by_salary'\n    #df_fe[product_utilization_rate_by_salary] = df_fe.Tenure \/ (df_fe.EstimatedSalary)\n   \n    #maa\u015fa g\u00f6re kredi_skoru urun oran, oranlanmasi\n    credit_score_rate_by_salary = 'credit_score_rate_by_salary'\n    df_fe[credit_score_rate_by_salary] = df_fe.CreditScore \/ (df_fe.EstimatedSalary)\n    \n    #Feature Eng. olu\u015fturulan de\u011fi\u015fkenlerin ba\u011f\u0131ml\u0131 de\u011fi\u015fkene g\u00f6re grafikleri g\u00f6sterilsin mi?\n    if is_show_graph:\n        fig, axes = plt.subplots(nrows = 2, ncols = 2, figsize = (20,12))\n        fig.tight_layout()\n        sns.boxplot(y = balance_salary_rate, x = dependent_variable_name, hue = dependent_variable_name, data = df_fe, ax = axes[0][0])\n        sns.boxplot(y = product_utilization_rate_by_year, x = dependent_variable_name, hue = dependent_variable_name, data = df_fe, ax = axes[0][1])\n        #sns.countplot(x = credit_score_rate_by_age, hue = dependent_variable_name, data = df_fe, ax = axes[1][0])\n        #sns.countplot(x = credit_score_rate_by_age, hue = dependent_variable_name, data = df_fe, ax = axes[1][1])\n        plt.ylim(-1, 5)\n    \n    \n    #feature engineering add- kredi_skor_tablosu\n    df_fe = df_fe.assign(credit_score_table=df_fe.apply(lambda x: credit_score_table(x), axis=1))\n    \n    #feature engineering add- ulkelere ortalama  maas durum\n    df_fe = df_fe.assign(countries_monthly_average_salaries = df_fe.apply(lambda x: countries_monthly_average_salaries(x), axis=1)) \n    \n    return df_fe","f80d9329":"# vucut_yag_orani, vucut_yag_orani_kategori,beden_kitle_kategori,insulin_kategori,kucuk_tansiyon_kategori\n\ndef data_encoding(df):\n    df_model = df.copy()\n    '''\n    # It was attempted to reduce the number of 0 observations.\n    churn_zero = df_model[df_model.apply(lambda x: True if x['Exited'] == 0 else False , axis=1)]\n    df_train = churn_zero.sample(frac=0.1,random_state=100)\n    df_model = df_model.drop(df_train.index)\n    '''\n    \n    \n    # >>>> Categorical columns <<<<<\n    \n    non_encoding_columns = [\"Geography\",\"HasCrCard\",\"IsActiveMember\",\"Gender\",\"NumOfProducts\",\"Tenure\",\"credit_score_table\"]\n    \n    df_non_encoding = df_model[non_encoding_columns]\n    df_model = df_model.drop(non_encoding_columns,axis=1)\n    \n    \n    df_encoding = df_non_encoding.copy()\n    \n    from sklearn.preprocessing import LabelEncoder\n    encoder = LabelEncoder()\n    df_encoding[\"gender_category\"] = encoder.fit_transform(df_non_encoding.Gender)\n    df_encoding[\"country_category\"] = encoder.fit_transform(df_non_encoding.Geography)\n    df_encoding[\"credit_score_category\"] = encoder.fit_transform(df_non_encoding.credit_score_table)\n\n    \n\n    df_encoding.reset_index(drop=True, inplace=True)\n    df_model.reset_index(drop=True, inplace=True)\n    df_model = pd.concat([df_model,df_encoding],axis=1)\n\n    df_model = df_model.drop([\"Geography\",\"Gender\",\"CustomerId\",\"Surname\",\"credit_score_table\",\"CreditScore\",\"EstimatedSalary\"],axis=1)\n    df_model = df_model.reset_index()\n    df_model = df_model.drop('index',axis=1)\n    \n    df_model.loc[df_model.HasCrCard == 0, 'credit_card_situation'] = -1\n    df_model.loc[df_model.IsActiveMember == 0, 'is_active_member'] = -1\n    return df_model\n","85e39d0c":"def model_prepare(df_model):\n    y = df_model[dependent_variable_name]\n    X = df_model.loc[:, df_model.columns != dependent_variable_name]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 12345)\n    from sklearn.preprocessing import StandardScaler\n    sc = StandardScaler()\n    X_train = sc.fit_transform(X_train)\n    X_test = sc.transform (X_test)\n    return X_train, X_test, y_train, y_test","4dc3d84e":"def data_training(X_train, X_test, y_train, y_test):\n\n    models = []\n    models.append(('LOGR',LogisticRegression()))\n    models.append(('KNN',KNeighborsClassifier()))\n    models.append(('CART',DecisionTreeClassifier()))\n    models.append(('RF',RandomForestClassifier()))\n    #models.append(('SVC',SVC()))\n    models.append(('GBM',GradientBoostingClassifier()))\n    models.append(('XGBoost',XGBClassifier()))\n    models.append(('LightGBM',LGBMClassifier()))\n    models.append(('CatBoost',CatBoostClassifier()))\n\n    df_result = pd.DataFrame(columns=[\"model\",\"accuracy_score\",\"scale_method\",\"0_precision\",\"0_recall\",\"1_precision\",\"1_recall\"])\n    index = 0\n    for name,model in models:\n        model.fit(X_train,y_train)\n        y_pred = model.predict(X_test)\n        score = accuracy_score(y_test,y_pred)\n        class_report = classification_report(y_test,y_pred,digits=2,output_dict=True)\n        zero_report = class_report['0']\n        one_report = class_report['1']\n        df_result.at[index,['model','accuracy_score','scale_method',\"0_precision\",\"0_recall\",\"1_precision\",\"1_recall\"]] = [name,score,\"NA\",zero_report['precision'],zero_report['recall'],one_report['precision'],one_report['recall']]\n        index += 1\n    return df_result.sort_values(\"accuracy_score\",ascending=False)","07a0d837":"# Function to give best model score and parameters\n\ndef best_model(model):\n    print(model.best_score_)    \n    print(model.best_params_)\n    print(model.best_estimator_)\n    \ndef get_auc_scores(y_actual, method,method2):\n    auc_score = roc_auc_score(y_actual, method); \n    fpr_df, tpr_df, _ = roc_curve(y_actual, method2); \n    return (auc_score, fpr_df, tpr_df)\n\n\nfrom matplotlib import rc,rcParams\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n    plt.rcParams.update({'font.size': 16})\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45, color=\"blue\")\n    plt.yticks(tick_marks, classes, color=\"blue\")\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"red\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n''' inf yakalama\nindices_to_keep = df_encoded.isin([np.nan, np.inf, -np.inf]).any(1)\ndf_encoded[indices_to_keep].astype(np.float64)\n'''","1074e64d":"df_prep = data_prepare()\n\ndf_outlier = outlier_process(df = df_prep)\n\n\nshow_data_analysis(df_prep)\nshow_outliers(df = df_outlier)","2bed3c86":"df_fe = feature_engineering(df = df_outlier)\ndf_fe.head(3)","622f69a8":"df_encoded = data_encoding(df_fe)\ndf_encoded.drop(['credit_card_situation', 'is_active_member'], axis=1, inplace=True)\ndf_encoded.head(3)","73db28f7":"correlation = df_encoded.corr().abs()\nplt.figure(figsize=(10,10))\nsns.heatmap(correlation, annot=True)\nplt.show()","4d18d770":"corrs_results = df_encoded.corrwith(df_encoded[\"Exited\"]).abs().nlargest(24)\ncorrs_results","9b36264e":"# model_prepare test, train split 0.2\nX_train, X_test, y_train, y_test = model_prepare(df_model = df_encoded)","4ff5c00d":"logr_model = LogisticRegression().fit(X_train,y_train)\ny_pred = logr_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(classification_report(y_test, y_pred, digits=4))\nprint(\"Accuracy score of Logistic Regression: \", accuracy)","1835c457":"training_result = data_training(X_train, X_test, y_train, y_test)\ntraining_result","a6f9465c":"training_result","a89d7779":"xgb_model=XGBClassifier(silent=0, learning_rate=0.23, max_delta_step=5,\n                            objective='reg:logistic',n_estimators=92, \n                            max_depth=5, eval_metric=\"logloss\", gamma=3,base_score=0.5)\nxgb_model.fit(X_train, y_train)\ny_pred = xgb_model.predict(X_test)\nprint(classification_report(y_test,y_pred,digits=2))\nprint(\"Accuracy score of Tuned XGBoost Regression: \", accuracy_score(y_test, y_pred))","cf4631fb":"param_grid = {'max_depth': [3, 5, 6, 7, 8], 'max_features': [2,4,6,7,8,9],'n_estimators' : [50,100], 'min_samples_split': [3, 5, 6, 7]}\nrandFor_grid = GridSearchCV(RandomForestClassifier(), param_grid, cv = 5, refit = True, verbose = 0)\nrandFor_grid.fit(X_train,y_train)\nbest_model(randFor_grid)","fd8800f2":"rnd_model = RandomForestClassifier(max_depth=8, max_features=6, min_samples_split=6,n_estimators=50)\nrnd_model.fit(X_train, y_train)\ny_pred = rnd_model.predict(X_test)\nprint(classification_report(y_test,y_pred,digits=2))\nprint(\"Accuracy score of tuned Random Forest model: \", accuracy_score(y_test, y_pred))","051150dd":"lgbm_model = LGBMClassifier(silent = 0, learning_rate = 0.09, max_delta_step = 2, n_estimators = 100, boosting_type = 'gbdt',\n                            max_depth = 10, eval_metric = \"logloss\", gamma = 3, base_score = 0.5)\nlgbm_model.fit(X_train, y_train)\ny_pred = lgbm_model.predict(X_test)\nprint(classification_report(y_test, y_pred, digits=2))\nprint(\"Accuracy score of tuned LightGBM model: \", accuracy_score(y_test, y_pred))","1e2964fc":"cfm = confusion_matrix(y_test, y_pred=y_pred)\nplot_confusion_matrix(cfm, classes=['Non Churn','Churn'],\n                      title='Churn Confusion matrix')\n\n\ntn, fp, fn, tp = cfm.ravel()\nprint(\"True Negatives: \",tn)\nprint(\"False Positives: \",fp)\nprint(\"False Negatives: \",fn)\nprint(\"True Positives: \",tp)","5e2b323f":"#!pip install scikit-plot","824101c8":"import scikitplot as skplt\ny_pred_proba = lgbm_model.predict_proba(X_test)\nskplt.metrics.plot_roc_curve(y_test, y_pred_proba, figsize=(8,8))\n\nplt.show()","5f925636":"feature_index = df_encoded.loc[:, df_encoded.columns != dependent_variable_name]\n\nfeature_importance = pd.Series(lgbm_model.feature_importances_, \n                               index=feature_index.columns).sort_values(ascending=False)\nsns.barplot(x = feature_importance, y = feature_importance.index, color='r', saturation=1)\nplt.xlabel('Variable Severity Scores')\nplt.ylabel('Variables')\nplt.title('Variable Severity Levels')\nplt.show()","fe7845a7":"And see after data encoding:","a214e4ad":"## 1. Installing","abffb05c":"## 5. Model Preparation","5efe5298":"# Churn Problem for Bank Company\n\n- The aim is to estimate whether a bank's customers leave the bank or not.\n\n- The event that defines the customer abandonment is the closing of the customer's bank account.\n\n### Data Set Story:\n\n- It consists of 10000 observations and 12 variables.\n- Independent variables contain information about customers.\n- Dependent variable refers to customer abandonment.\n\n### Features:\n\n- Surname: Surname\n- CreditScore: Credit score\n- Geography: Country (Germany \/ France \/ Spain)\n- Gender: Gender (Female \/ Male)\n- Age: Age\n- Tenure: How many years of customer\n- Balance: Balance\n- NumOfProducts: Bank product used\n- HasCrCard: Credit card status (0 = No, 1 = Yes)\n- IsActiveMember: Active membership status (0 = No, 1 = Yes)\n- EstimatedSalary: Estimated salary\n- Exited: Abandoned or not? (0 = No, 1 = Yes)","4c3ef37b":"### 3.4. According to countries monthly average salaries","2f5bf8c6":"## 3. Feature Engineering\n### 3.1. Credit Score grouping ( min= 358 and max= 800)","066fd4b3":"## 8. Apply Model","7764b832":"### 8.1. Logistic Regression Model\n\nHave a look with Logistic Regression.","3d3da5ae":"## 4. Data Encoding","ea103f10":"## 9. Model Tunning","9e3ccb72":"#### 2.2.4. Outiler Process","109866f1":"Let's see after feature engineering:","e0fda5df":"Now, let's have a look which model is best for us:","5c87259a":"Using the parameters and get final version accuracy score.","f86e9df9":"### 5.1 Part of Data TRAIN","15cfc3c3":"## 7. Data Cleaning","64361b98":"#### 2.3.2. Numeric columns distribution observation","a71b5fdc":"### 3.5. The main method that started all Feature Engineering","c487206a":"# 13. Conclusion\n\n\n- Our aim in this project was to develop a churn prediction model using machine learning algorithms.\n- There were 10000 rows in the data set and there were no missing values. and the dataset consisted of 13 variables.\n\n\n- The following conclusions came from the analysis on the features:\n\n    * Most customers who using products 3 and 4 stopped working with the bank. In fact, all customers using product number 4 were gone.\n    * Customers between the ages of 40 and 65 were more likely to quit the bank.\n    * Those who had a credit score below 450 had high abandonment rates.\n    * Predictions were made with a total of 8 classification models. The highest head was taken with LightGBM method.\n    * Accuracy and cross validation scores were calculated for each model and results were displayed.\n\n\n\n#### Note:\n\n    * After this notebook, my aim is to prepare 'kernel' which is 'not clear' data set.\n\n    * If you have any suggestions, please could you write for me? I wil be happy for comment and critics!\n\n    * Thank you for your suggestion and votes ;)\n","51046381":"### 9.1. XGBoost Tuning","b7ee36bc":"### 9.2. Random Forest Tuning","99d6dbf8":"Unique value data representation on all variables:","43668f31":"### 3.3. Product utilization rate by estimated SALARY","b02f869f":"## 11. ROC Curve","5ccd4333":"### 2.2. Outliers Observe (LOF method and Supress)\n#### 2.2.1. Outlier Editing","0272c3af":"#### 2.3.5. The main method that started all data analysis","fbc3a4b4":"List of correlation scores:","97390786":"### 2.3. Data Analysis\n#### 2.3.1. Dependent variable distribution","f9992097":"## 10. Confussion Matrix","173d8d57":"#### 2.3.4. Categorical variables are observed according to the dependent variable","36a3daba":"Now, let's see correlation graph:","fb6ee731":"### 3.2. Product utilization RATE by YEAR","69890529":"### 8.2. Model Training\n\nThis is for all LogisticRegression, RandomForestClassifier, GradientBoostingClassifier, XGBClassifier, and LGBMClassifier model objects:","8fcc399d":"#### 2.2.2. Visualization of outliers according to the LOF method","e900e543":"# 12. Feature Importance","132c2531":"#### 2.3.3. Status of other variables according to dependent variable","76f558ee":"#### 2.2.3. Outiler suppression","9bf25ecc":"## 2. EDA (Exploratory of Data Analysis)\n### 2.1. Data Preperation","12dcc345":"### 9.3. LightGBM Tuning","d24cb44a":"## 6. HELPing Functions"}}