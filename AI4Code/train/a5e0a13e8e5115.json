{"cell_type":{"7a7fff2c":"code","61e409b5":"code","6be3c3b9":"code","afa50a28":"code","caade3e4":"code","a15d60f6":"code","5753bf8a":"code","5a188ae2":"markdown","3f1a5964":"markdown","040edda9":"markdown","d43d9f09":"markdown","c486e8ba":"markdown","b8ecdb67":"markdown","c339bfea":"markdown","18c29144":"markdown","6a24e753":"markdown","f4194bc2":"markdown"},"source":{"7a7fff2c":"## importing main libaries requied:)\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf","61e409b5":"def get_data_cleaned(df: pd.DataFrame):\n    X = df.iloc[:, 3:-1].values\n    y = df.iloc[:, -1].values\n    return X, y\n","6be3c3b9":"def get_procsed_dependent_variable(depen_var):\n    le = LabelEncoder()\n    depen_var[:, 2] = le.fit_transform(depen_var[:, 2])\n    return depen_var","afa50a28":"def get_procssed_independent_variable(depen_var):\n    ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough')\n    depen_var = np.array(ct.fit_transform(depen_var))\n    return depen_var\n","caade3e4":"def get_splitted_data_and_feature_scaling(depen_var, indepen_var):\n    X_train, X_test, y_train, y_test = train_test_split(depen_var, indepen_var, test_size=0.25, random_state=32)\n    sc = StandardScaler()\n    X_train = sc.fit_transform(X_train)\n    X_test = sc.transform(X_test)\n    return X_train, X_test, y_train, y_test\n","a15d60f6":"def final_making_of_model(X_train, X_test, y_train, y_test):\n    ann = tf.keras.models.Sequential()\n    ann.add(tf.keras.layers.Dense(units=7, activation='relu'))\n    ann.add(tf.keras.layers.Dense(units=7, activation='relu'))\n    ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n    ann.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    ann.fit(X_train, y_train, batch_size=32, epochs=20)\n    y_pred = ann.predict(X_test)\n    y_pred = (y_pred > 0.5)\n    print(\"Results\")\n    print(np.concatenate((y_pred.reshape(len(y_pred), 1),y_test.reshape(len(y_test), 1)), 1))\n    print(\"Accuracy\")\n    print(accuracy_score(y_test, y_pred))\n    cm = confusion_matrix(y_test, y_pred)\n    print(\"confusion_matrix\")\n    print(cm)","5753bf8a":"def main():\n    df = pd.read_csv(\"\/kaggle\/input\/churn-modelling\/Churn_Modelling.csv\")\n    depen_var, indepen_var = get_data_cleaned(df)\n    depen_var = get_procsed_dependent_variable(depen_var)\n    depen_var = get_procssed_independent_variable(depen_var)\n    X_train, X_test, y_train, y_test = get_splitted_data_and_feature_scaling(depen_var, indepen_var)\n    final_making_of_model(X_train, X_test, y_train, y_test)\n\n\nif __name__ == '__main__':\n    main()\n","5a188ae2":"Now since we have categorical data we need to change that to numeric so here i have used two encoder:\n**1.Label encoder\n2.one hot encoder****","3f1a5964":"![https:\/\/www.google.com\/url?sa=i&url=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2Fwhy-not-deploy-a-machine-learning-model-in-excel-fc95fe4e0629&psig=AOvVaw3DgAFzBKP4bn3txibHPXXb&ust=1594202293350000&source=images&cd=vfe&ved=0CAIQjRxqFwoTCOjShsLwuuoCFQAAAAAdAAAAABAD](http:\/\/)","040edda9":"Since we know that the first three coloums of the dataset are not at all requried.\n1.**The First one Row Number(ID)** is just like the row no which will not add any value to our model\n2.**The customer unique id** which is just to distinguish the customer will not help our model to track any pattern or information.\n3.**The Surname** which is again will be unique and have no impact on the model\n\n**GOlDEN RULE:\n> > > > Always remmember if the data is not neccesary please remove that because that will sometimes affect our model accuracy !!****","d43d9f09":"Hlo Everyone and welcome to another exciting notebook in which we are going to do the churn modelling in which based on the data provided we are going to detect wheather the customer will stay or not with the bank in the upcomming months\/years.\n\nThis approach can apllied through various techniques like Decision Tree, Logistic Regression,etc.\nBut i m here to do it with little suprising way which is obsivously not new but i think it the most applied method when comes to such problems:)\n\nOK let's get started!!!","c486e8ba":"**The results shows us that 1st coloumn is for the predicted values and the other coloumn is for the actual values**","b8ecdb67":"Many of you would have doubt that why **fit_transform()** for train and only **transform()** for the test ??\n\nWell the answer is right here We use fit_transform() on the train data so that we learn the parameters of scaling on the train data and in the same time we scale the train data. We only use transform() on the test data because we use the scaling paramaters learned on the train data to scale the test data.","c339bfea":"So in X by using the slicing from the dataframe I have left leaving 3 coloums from the start\nand in Y we take only last coloumn..:)","18c29144":"Ok guys a final tip from my side before we end up always try to write in methods because those will help you very much in the upcomming life as if you wanna change anything just go to that method andd change no need for full code correction and also thats add a beauty to your work.\n\nIf you see my main code its just matters of some method calling which help the user to read very easily and also the end user do not need ur codes and all that its just output that matters!!!!:):):)\n\nWith this hope lets meet in next notebook!!\n\n**Happy learning:)**","6a24e753":"If you like my work please do **UPVOTE**** for me that will add more potential to work \nThank You:)","f4194bc2":"Then comes the most important thing of ay deep learning model in which we need to scale our data!!\n\n**GOLDEN RULE:**\nAlways remmember to scale the data when dealing with deep learning models as without scaling u model nevers perfor well tasks.\n\nScalability matters in machine learning because:\n\nTraining a model can take a long time.\n\nA model can be so big that it can't fit into the working memory of the training device.\nEven if we decide to buy a big machine with lots of memory and processing power, it is going to be somehow more expensive than using a lot of smaller machines. In other words, vertical scaling is expensive."}}