{"cell_type":{"798f5c0d":"code","d7db337c":"code","ac410ebf":"code","41241aec":"code","92ce5809":"code","42ebd8b3":"code","d570246e":"code","123ca589":"code","763915d5":"code","845937cc":"code","9014c59d":"code","6e66673a":"code","dae1e462":"code","360e5942":"code","f3bfb6b3":"code","8186a8b5":"code","4e1246d3":"code","df3177b1":"code","18290c1f":"code","1789e120":"code","b4f9096d":"code","9df540cc":"code","fa803eed":"code","ebae24c7":"code","6d174d34":"code","b8796329":"code","d00a1547":"code","f884f6cd":"code","84b8c996":"code","9cd96fe3":"code","cce53980":"code","4bd81670":"code","9d48741e":"code","005a4a23":"code","620c8cc7":"code","f446f357":"code","7e7a4a0e":"code","e8774639":"code","ba77f518":"code","35459899":"code","3906eac9":"code","c39de6ba":"code","e386c41b":"code","0ebdb483":"code","35235f4c":"code","2a932166":"code","d69cea43":"code","325e3d9e":"code","81b60d8d":"code","d08dc023":"code","d45929c0":"code","3e338b87":"code","98cd4e97":"code","b88d3d1e":"code","2c10a7ef":"code","e227818a":"code","818d5981":"code","a067797c":"code","92b38024":"code","be80956a":"code","235f9354":"code","d9fe9278":"code","abd38f5e":"code","e9d6323e":"code","62a0f90d":"code","3dd30a33":"code","7cb91e5a":"code","f297ebce":"code","af4f9d6d":"code","7c268284":"code","7bbef5ec":"code","340d873a":"code","d7d1f6a4":"code","ff4c5ab9":"code","5fe2e399":"code","ac0314df":"code","ddd99d6e":"code","b7b95761":"code","b88c16d8":"code","38cbd7f0":"code","ac594ce0":"code","64ff9b09":"code","d0b8e3ed":"code","119dbb89":"code","6723bcfe":"code","fd6bf44d":"code","820fde4f":"code","cdea9601":"code","5587edeb":"markdown","2dedc95e":"markdown","d642b9ed":"markdown","a5e77eeb":"markdown","01449ba3":"markdown","ea6ed25d":"markdown","bfe681a5":"markdown","83086af2":"markdown","f34352d1":"markdown","f31de32e":"markdown","de6bf765":"markdown","2e845acf":"markdown","6c5f908a":"markdown","e749eccd":"markdown","5a794910":"markdown","9529da83":"markdown","ef2a6e20":"markdown","a42ac350":"markdown","8683ef81":"markdown","db6cccdc":"markdown","b51ddfbe":"markdown","9e1a4707":"markdown","dcdaafe8":"markdown","bd999356":"markdown","c9d402df":"markdown","9e38de18":"markdown","42fa8396":"markdown","a4ff1a3f":"markdown","5c885a69":"markdown","e1ac0fc8":"markdown","07521531":"markdown","3c75df7c":"markdown","62d77eed":"markdown","b4ebf83b":"markdown","9f019e2a":"markdown","243ec2e1":"markdown","687d52b9":"markdown","0f3df428":"markdown","549fe53e":"markdown","f089d9cf":"markdown","a3d607ce":"markdown","daeca236":"markdown","a0d5ccab":"markdown","cd93a722":"markdown","afa6e055":"markdown","f7b650e2":"markdown","0a1edcd9":"markdown"},"source":{"798f5c0d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom  sklearn.preprocessing import OrdinalEncoder","d7db337c":"data=pd.read_csv(\"..\/input\/prices-for-food-crops-commodities-2012-to-2015\/Prices for Food Crops_Commodities_2012_to_2015.csv\")","ac410ebf":"data","41241aec":"#getting a summary of the first 5 rows of our dataset.\ndata.head()","92ce5809":"#getting a summary of the last  5 rows of our dataset\n#You can specify if you want more rows by specifying a number inside the .head() or .tail()\ndata.tail()","42ebd8b3":"#remove OBJECTID column we don't need it in our dataset as it's not a feature basis for price prediction\ndata.drop('OBJECTID',inplace=True,axis=1)\n","d570246e":"#find missing values in columns cumulativey\ndata.isna().sum()\n#filter rows with null data values\nnull_data=data.isna()\nrow_null_data=null_data.any(axis=1)\ndata[row_null_data]\n","123ca589":"#Our data has no Duplicates as it returns false \ndata = data.drop_duplicates() \ndata.duplicated().values.any()","763915d5":"#Checking the shape of our data interms of rows and column numbers\n#The Datas is spread across 1145 rows and 6 columns that signify the various attributes\ndata.shape","845937cc":"data.dtypes","9014c59d":"#Which it the most common type weight in Kgs that the commodities are packaged in and how it varies across our data\ndata['Volume_in_Kgs'].value_counts()","6e66673a":"#Finding the packaging  with the highest Unit in Kgs and the variation accross our data\n\ndata['Unit'].value_counts()","dae1e462":"#Finding the total count of values in Produce Variety Column\ndata['Produce_Variety'].value_counts()\n","360e5942":"#using ordinal encoder to encode categorical values into numeric\n#We use this categorical data encoding technique when the categorical feature is ordinal. \n#In this case, retaining the order is important. Hence encoding should reflect the sequence.\nord_enc = OrdinalEncoder()\ndata['produce_variety']=ord_enc.fit_transform(data[['Produce_Variety']]).astype(int)\ndata['Type_of_Commodity']=ord_enc.fit_transform(data[['Commodity_Type']]).astype(int)\ndata['Package_Type']=ord_enc.fit_transform(data[['Unit']]).astype(int).astype(int)\n#create package_weight column\ndata['package_weight(Kg)']=data['Volume_in_Kgs']","f3bfb6b3":"#create day ,month and year columns\ndata['Date']=data['Date'].apply(lambda x: x.split(' ')[0])\ndata['Day']=data['Date'].apply(lambda x: x.split('\/')[1]).astype(int)\ndata['Month']=data['Date'].apply(lambda x: x.split('\/')[0]).astype(int)\ndata['Year']=data['Date'].apply(lambda x: x.split('\/')[2]).astype(int)\n","8186a8b5":"#remove Kes in the Value_in_Ksh column values which is part of data cleaning to better utilised the values\n#This is  our target value bets practice placedm as the last column in our dataset. \ndata['Price']=data['Values_in_Ksh'].str.replace('KES',\" \").replace(\" \",\" \").astype(float)\ndata.isna().sum()","4e1246d3":"#the NAN values in Price column \ndata.isna().sum()\n#replace them with mean of that column so that we don't have NAN values in our dataset\ndata['Price']=data['Price'].fillna((data['Price'].mean()))\n","df3177b1":"#getting a statistical  overview of the data\ndata.describe()","18290c1f":"#This shows the information concerning our dataset\ndata.info()","1789e120":"#Getting to see all the column names  in our dataset\ndata.columns\n","b4f9096d":"plt.figure(figsize=(10,8))\nplt.scatter(data['Year'],data['Commodity_Type'],c=data['Price'], cmap='cool', alpha = 0.8)\nplt.colorbar().set_label(\"Price Range in Ksh (approx 1usd -ksh109)\")\nplt.title('Commodity Prices')\nplt.xlabel('Year')\nplt.ylabel('Produce_Variety')\nplt.show()","9df540cc":"#The towers or bars of a histogram are called bins. \n#The height of each bin shows how many values from that data fall into that range\nplt.figure(figsize=(10, 7))\nplt.hist(data.Price, bins = 50, ec = 'black' ,color = '#f88f01',alpha=0.7)\nplt.xlabel('Median Price of Commodity Sold in the Market in Ksh', fontsize=16)\nplt.ylabel('Count ', fontsize=16)\nplt.title('Average Commodity Price in Ksh ', fontsize=16)\nplt.axvline(data['Price'].mean(), color='red', linestyle='dashed', linewidth=3, label='Average Price of Commodity Sold in the Market in Ksh')\nplt.style.use('dark_background')\nplt.show()\n","fa803eed":"#generating a distribution plot\nplt.figure(figsize=(10, 7))\nsns.distplot(data.Price, bins = 50 ,color = '#f88f01',hist=True)\nplt.style.use('dark_background')\nplt.xlabel('Median Price of Commodity Sold in the Market in Ksh', fontsize=16)\nplt.ylabel('Count ', fontsize=16)\nplt.title('Average Commodity Price in Ksh ', fontsize=16)\nplt.show()","ebae24c7":"mean_price=data['Price'].mean()\nmedian_price=data['Price'].median()\nprint('The mean commodity Price is Ksh',round(mean_price))\nprint('The median commodity Price is Ksh',median_price)","6d174d34":"plt.figure(figsize=(10, 6))\nplt.hist(data['package_weight(Kg)'], bins=20, ec = 'black', color = 'green')\nplt.xlabel('Total Package Weight(KG) per commodity in the market', fontsize=16)\nplt.ylabel('Count', fontsize=16)\nplt.title('Average Package Weight(KG) distribution per commodity sold in the market', fontsize=16)\nplt.axvline(data['package_weight(Kg)'].mean(), color='#21209c', linestyle='dashed', linewidth=3, label='Average Number of Households in a block')\nplt.show()\nplt.style.use('dark_background')","b8796329":"mean_pack_weight=data['package_weight(Kg)'].mean()\nprint('The mean package_weight in KG is:',mean_pack_weight)\nmean_pack_median=data['package_weight(Kg)'].median()\nprint('The median package_weight in KG is:',mean_pack_median)","d00a1547":"plt.figure(figsize=(10, 7))\nplt.hist(data['Month'], bins=20, ec = 'black', color = 'green')\nplt.xlabel('The Month in which commodities are sold in  the market', fontsize=16)\nplt.ylabel('Count', fontsize=16)\nplt.title('Average Monthly distribution commodities sold in the market', fontsize=16)\nplt.axvline(data['Month'].mean(), color='#21209c', linestyle='dashed', linewidth=3, label='Average Monthly distribution commodities sold in the market')\nplt.show()\nplt.style.use('dark_background')","f884f6cd":"#Here 2013 was a good year in that most commodities were sold during this period second 2015 and 2012\ndata.Year.hist()","84b8c996":"#January and December are the months were alot of commodities were sold \ndata.Month.hist()","9cd96fe3":"#Are of products are solds between the first 3-4 days of the month then few are sold during 23-25 days of the month\ndata.Day.hist()","cce53980":"#The total of diffrent produce variety sold for the said period\ndata['Produce_Variety'].value_counts()","4bd81670":"#From our histogram horticulture was the produced variety that was sold most\ndata.Produce_Variety.hist()","9d48741e":"#Finding the correlation between A produce variety with the comodity type with the month and year to price\ndata[['produce_variety','Type_of_Commodity','Package_Type','package_weight(Kg)','Day','Month','Year','Price']].corr()","005a4a23":"plt.figure(figsize=(10, 5))\n#createing a color map\ncmap = sns.diverging_palette(200,10, as_cmap=True)\n#ploting a heatmap\n#The lighter a particular box the greater the corrilation as per the key index\nsns.heatmap(data[['produce_variety','Type_of_Commodity','Package_Type','package_weight(Kg)','Day','Month','Year','Price']].corr(),annot=True, annot_kws={\"size\": 10},linewidths=.7)","620c8cc7":"mask = np.zeros_like(data[['produce_variety','Type_of_Commodity','Package_Type','package_weight(Kg)','Day','Month','Year','Price']].corr())\ntriangle_indicies = np.triu_indices_from(mask)\nmask[triangle_indicies] = True\nmask","f446f357":"plt.figure(figsize=(10, 5))\nsns.heatmap(data[['produce_variety','Type_of_Commodity','Package_Type','package_weight(Kg)','Day','Month','Year','Price']].corr(), mask=mask, annot=True, annot_kws={\"size\": 14})\nplt.style.use('dark_background')\nplt.xticks(fontsize=16)\nplt.yticks(fontsize=16)\nplt.show()","7e7a4a0e":"data.head()","e8774639":"#Plotting bar graphs from the dataset categories\ndata_cat = data[['Produce_Variety', 'Commodity_Type', 'Unit','Volume_in_Kgs', 'Day', 'Month', 'Year']]","ba77f518":"for i in data_cat.columns:\n    cat_num = data_cat[i].value_counts()\n    print(\"graph for %s: total = %d\" % (i, len(cat_num)))\n    chart = sns.barplot(x=cat_num.index, y=cat_num)\n    chart.set_xticklabels(chart.get_xticklabels(), rotation=90)\n    plt.show()","35459899":"#Creating a Word Map","3906eac9":"from wordcloud import WordCloud, ImageColorGenerator\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize","c39de6ba":"text=\" \".join(data['Commodity_Type'])\nword_cloud = WordCloud(collocations = False, background_color = 'white',scale=20,max_words = 1000,width =1000, height = 500).generate(text)\n\nplt.figure(figsize=[10,10])\nplt.imshow(word_cloud,interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()\n#saving the word cloud and storing it as png\n\n# store to file\nword_cloud.to_file(\"commodity_types.png\")\n","e386c41b":"#Save our dataset having cleaned it and analysed as EDA\ndata.to_csv('Food_Crops(EDA_cleaned).csv')","0ebdb483":"from sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split","35235f4c":"#load our cleaned data \ndata=pd.read_csv('..\/input\/prices-for-food-crops-commodities-eda-cleaned\/Prices for Food Crops_Commodities_2012_to_2015 (EDA_cleaned).csv')\n","2a932166":"#choose relevant  columns\ndata_model=data[['produce_variety','Type_of_Commodity','Package_Type','package_weight(Kg)','Day','Month','Year','Price']]\n","d69cea43":"#get dummy data\ndata_dum=pd.get_dummies(data_model)","325e3d9e":"#categorising our data into X feaures and Y target values\n\nX=data_dum.drop('Price',axis=1)\ny=data_dum['Price']","81b60d8d":"#spliting our data into traning and testing\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)","d08dc023":"#Linear Regression\nmodel=LinearRegression()\n","d45929c0":"model.fit(X_train, y_train)","3e338b87":"print(model.intercept_)","98cd4e97":"test_pred=model.predict(X_test)\ntest_train=model.predict(X_train)\n","b88d3d1e":"#This will show how far off the values are. This is showing the predicted value minus the actual test value for all the data points.\n\nresiduals=test_pred-y_test\nresiduals\n#Then, we should plot with a histogram to see how \u201coff\u201d each value is. This can be done with the following command.\n#This graph above shows the distribution of error. Most of the time, it appears that the values are close-ish to 0.\nplt.hist(residuals)","2c10a7ef":"#calculating the mean absolute error on training data\nmae= metrics.mean_absolute_error(y_train,test_train)\n#calculating the mean squared error on training data\nmse= metrics.mean_squared_error(y_train, test_train)\n#calculating the root mean squared error on training data\nrmse = np.sqrt(metrics.mean_squared_error(y_train,test_train))\nprint(\"Trainig Data\")\nprint(\"The mean absolute error using Linear Regression is :\", mae)\nprint(\"The mean squared error using Linear Regression is  :\", mse)\nprint(\"The root mean squared error using Linear Regression is :\", rmse)","e227818a":"#calculating the mean absolute error on testing data\nmae= metrics.mean_absolute_error(y_test,test_pred)\n#calculating the mean squared error on testing data\nmse= metrics.mean_squared_error(y_test, test_pred)\n#calculating the root  mean squared error on testing data\nrmse = np.sqrt(metrics.mean_squared_error(y_test,test_pred))\n\nprint(\"Testing Data\")\nprint(\"The mean absolute error using Linear Regression is :\", mae)\nprint(\"The mean squared error using Linear Regression is :\", mse)\nprint(\"The root mean squared error using Linear Regression is :\", rmse)","818d5981":"#Co-efficient of determination (R2 Score) on training data\nmetrics.r2_score(y_train,test_train)","a067797c":"from sklearn.metrics import r2_score\n#Co-efficient of determination (R2 Score) on testing data\nmetrics.r2_score(y_test,test_pred)","92b38024":"# Checking Normality of errors\nsns.distplot(y_train-test_train)\nplt.title(\"Histogram of Residuals\")\nplt.xlabel(\"Residuals\")\nplt.ylabel(\"Frequency\")\nplt.show()\n","be80956a":"# Visualizing the differences between actual prices and predicted values\nplt.scatter(y_train, test_train,color='red')\nplt.xlabel(\"Prices\")\nplt.ylabel(\"Predicted prices\")\nplt.title(\"Prices vs Predicted prices\")\nplt.show()\n","235f9354":"print('Intercept', model.intercept_)\npd.DataFrame(data = model.coef_, index=X_train.columns, columns = ['Coef'])","d9fe9278":"X=data_dum.drop('Price',axis=1)\ny=np.log(data_dum['Price'])\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)","abd38f5e":"model=LinearRegression(normalize=True)\nmodel.fit(X_train, y_train)","e9d6323e":"pred_score=model.predict(X_test)\ntrain_score=model.predict(X_train)","62a0f90d":"metrics.r2_score(y_train,train_score)","3dd30a33":"metrics.r2_score(y_test,pred_score)","7cb91e5a":"from sklearn.tree import DecisionTreeRegressor\n\n\nX=data_dum.drop('Price',axis=1)\ny=data_dum['Price']\ntrain_x,val_x,train_y,val_y = train_test_split(X,y,random_state=0)\n","f297ebce":"ml_model = DecisionTreeRegressor(max_depth=2)","af4f9d6d":"ml_model.fit(X,y)","7c268284":"dt_pred=ml_model.predict(val_x)","7bbef5ec":"metrics.mean_absolute_error(val_y,dt_pred)","340d873a":"ml_model.score(train_x,train_y)","d7d1f6a4":"ml_model.score(val_x,val_y)","ff4c5ab9":"#calculating the mean absolute error across our testing data\ndef get_mae(leaf_nodes,train_x,val_x,train_y,val_y):\n    ml_model = DecisionTreeRegressor(max_leaf_nodes=leaf_nodes,random_state=0)\n    ml_model.fit(X,y)\n    a = ml_model.predict(val_x)\n    mae = metrics.mean_absolute_error(val_y,a)\n    return(mae)","5fe2e399":"#sample of mean absolute errors obtain with our decision tree model\nfor leaf_nodes in np.arange(10,300,50):\n    mae = get_mae(leaf_nodes,train_x,val_x,train_y,val_y)\n    print(leaf_nodes,\"  >>>>>>>>>>  \",mae)","ac0314df":"\nfrom graphviz import Source\nfrom sklearn import tree\nfrom IPython.display import SVG\nlabels = X.columns\ngraph = Source(tree.export_graphviz(ml_model ,feature_names = labels, max_depth = 2, filled = True))\ndisplay(SVG(graph.pipe(format='svg')))","ddd99d6e":"from sklearn.ensemble import RandomForestRegressor \nfrom sklearn.preprocessing import StandardScaler","b7b95761":"#defining our training data\nX_data=data_dum.drop('Price',axis=1)\ny_data=data_dum.Price\n#Creating our training and test dataset\nX_train,X_test,y_train,y_test=train_test_split(X_data,y_data,test_size=0.3,random_state=42)","b88c16d8":"regressor=RandomForestRegressor(n_estimators=1000)","38cbd7f0":"regressor.fit(X_train,y_train)","ac594ce0":"pred=model.predict(X_test)\ntrain=model.predict(X_train)","64ff9b09":"print('\\nMetrics on Training Data Random Forest Regression\\n ')\nprint('MAE:', metrics.mean_absolute_error(y_train, train))\nprint('MSE:', metrics.mean_squared_error(y_train, train))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_train,train)))\n\n\n\nprint('\\nMetrics on Testing Data Random Forest Regression\\n')\nprint('MAE:', metrics.mean_absolute_error(y_test, pred))\nprint('MSE:', metrics.mean_squared_error(y_test, pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_test,pred)))\n\n\n","d0b8e3ed":"#model score on training data\nregressor.score(X_train,y_train)\n","119dbb89":"#model score on test data\nregressor.score(X_test,y_test)","6723bcfe":"estimators = np.arange(20, 1000, 10)\nscores = []\nfor n in estimators:\n    regressor.set_params(n_estimators=n)\n    regressor.fit(X_train, y_train)\n    scores.append(regressor.score(X_test, y_test))\nplt.title(\"Effect of n_estimators\")\nplt.xlabel(\"n_estimator\")\nplt.ylabel(\"score\")\nplt.plot(estimators, scores)","fd6bf44d":"# Visualizing the differences between actual prices and predicted values\nplt.scatter(y_train, train,color='red')\nplt.xlabel(\"Prices\")\nplt.ylabel(\"Predicted prices\")\nplt.title(\"Prices vs Predicted prices\")\nplt.show()\n\n","820fde4f":"# Checking residuals\nplt.scatter(train, y_train-train,color='red')\nplt.title(\"Predicted vs residuals\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Residuals\")\nplt.show()","cdea9601":"# Checking Normality of errors\nsns.distplot(y_train-train)\nplt.title(\"Histogram of Residuals\")\nplt.xlabel(\"Residuals\")\nplt.ylabel(\"Frequency\")\nplt.show()\n#There is no pattern visible in this plot and values are distributed equally around zero. So Linearity assumption is satisfied","5587edeb":"Trying our algorithm accross different estimated values and we see the score maintained  ranges around 97% ","2dedc95e":"### Regression using Log Prices","d642b9ed":"It involves manipulating the data fields in ways that can help fit our model better and improve it\u2019s accuracy.","a5e77eeb":"    We find that our Linear regression model has an accuracy of around 25%\n\n    Linear regression can be prone to underfitting the data.\n    Hence if you build a model using linear regression and you find that\n    both the test accuracy and the training accuracy are low then this would likely be due to underfitting.\n    \n    To address this som eof the other models we can use include:\n    A)Decision tree regression\n    B)Random forest regression","01449ba3":"Most of the commodity types sold in the market are packaged in Bags","ea6ed25d":"### Define our Model","bfe681a5":"Horticultural crop products are the ones that are sold most","83086af2":"### Model Evaluation","f34352d1":"    When it comes to  regression analysis, all data fields should be continuous. \n    If there are incompatible data types, one needs to either convert them or drop them\n    especially when they don\u2019t contribute much to the desired output.\n    \n    For the days of the week, assign them respective dummy values.\n    \n    If the data is ordinal (first, second, third\u2026), convert it to continuous","f31de32e":"### Predict our Model","de6bf765":"#### Check the data types in our dataset","2e845acf":"### Define our Model","6c5f908a":"### Fitting our model\n","e749eccd":"### Building Our Model\n    The steps to follow are \n    1)Define the ML model to use\n    2)Fit the model\n    3)Use the model to Predict values\n    4)Evaluate the model to see it's performance","5a794910":"#### Model Training","9529da83":"    Noteable Corrilation findings\n    \n    a)There is some corrilation between produce_variety and price.\n    \n    b)Another higher correlation is found between the package_weight and the price\n    \n    c)The day in which a commodity is sold has a corrilation to the month\n    \n    d)The year and the day have a correlation to the price of the commodity\n    (factoring may be the rainfall distribution of that year)\n    \n\n\n","ef2a6e20":"### Import and load dataset","a42ac350":"       #Notes Source https:\/\/machinelearningmastery.com\/regression-metrics-for-machine-learning\/\n     Regression predictive modeling are those problems that involve predicting a numeric value.\n     Metrics for regression involve calculating an error score to summarize the predictive skill of a model.\n    \n    Accuracy (e.g. classification accuracy) is a measure for classification, not regression.\n    We cannot calculate accuracy for a regression model.\n\n    The skill or performance of a regression model must be reported as an error in those predictions.\n    If you are predicting a numeric value like a height or a dollar amount, you don\u2019t want to know if the model predicted           the value exactly (this might be intractably difficult in practice); instead, we want to know how close the predictions         were to the expected values.","8683ef81":"### Summary of our data after loading","db6cccdc":"    There are three error metrics that are commonly used for evaluating and reporting the performance of a regression model;       they are:\n    1)Mean Absolute Error (MAE)\n    2)Mean Squared Error (MSE)\n    3)Root Mean Squared Error (RMSE)        ","b51ddfbe":"### Decision Tree Regression\n    \u2022\tWhen a decision tree classifies things into categories it\u2019s called a classification tree. (Outcome of the data is discrete or its categorical) e.g., presence or absence of students, person died or survived. \n    \u2022\tWhen a decision tree predicts numerical values it\u2019s a regression tree. (Outcome of the data is continuous in nature) e.g., house prices, age of a person, length of stay in hotel.\n    \n       Decision trees learn by splitting the training examples in a way such that the sum of squared residuals is minimized. \n       It then predicts the output value by taking the average of all of the examples that fall into a certain leaf on the              decision tree and using that as the output prediction\n       \n       Worth noting is that decision trees are prone to overting hence to resolve this Random Forest Regression helps to reduce \n       overfitting by  aggregating the results of multiple decision trees on the same dataset.\n","9e1a4707":"### Training Test Split","dcdaafe8":"#### Replacing missing  values","bd999356":"### Fit our Model  ","c9d402df":"### Feature Engineering","9e38de18":"#### Model Definition","42fa8396":"### Evaluate our Model","a4ff1a3f":"### Random Forest Evaluation","5c885a69":"    We find that our model has an improved accuracy of around 97% with use of Random Forest Regression.\n    \n    Random forest works by  builds multiple decision trees and merges them together.\n    Often it's more accurate and stable in prediction \n    Random decision forests correct the decision tree\u2019s habit of overfitting to their training set.\n","e1ac0fc8":"### Preditiction","07521531":"Most of the commodities sold in the Market are packaged in weight of 90KG,50KG, 10KG respectively ","3c75df7c":"Our data has two rows that have missing values all of which fall under the Values_in_Ksh column \n    we shall replace the null values with the mean of the values in that column.","62d77eed":"#### Check for Duplicates","b4ebf83b":"The dataset is on Kenya's average Monthly Price of Food Crops Commodities \nScheduled Horticultural Crops between 2012 to 2015.The task is to be able to predict crop prices using the data features available for market insight and aslso planning purposes. \n\nThe particular outcome that we want to be able to predict is of continous nature (crop prices) hence the the algorithms used include Linear Regression,Decion Tree Regression,Random Forest Regression.\n\n# Share your feedback and also UPVOTE","9f019e2a":"### Data Visualization\nIn data visualization it helps us gain insight into the overall impact of one feature on another.","243ec2e1":"Currently the data has only one integer data type to be able to work with the values_in_ksh,Date and Unit columns we shall need to \nconvert them to float,data_time,and integer data types repectively","687d52b9":"Most of the crop commodities sold are packaged in weight of 90Kg","0f3df428":"### Generating our decision tree image","549fe53e":"#### Correlation\n    Correlation among attributes is an important parameter to know, \n    since variables that are highly correlated to each other or the target variable often add bias to linear models,\n    providing false predictions.","f089d9cf":"On average most of the commodities in the market are sold between the months of January and July followed later on by the months of August to December\n","a3d607ce":"#### Linear Regression Model\nLinear regression tries to establish a linear relationship between an independent variable x and a dependent variable y.\nIt assumes a linear relationship between input variables (x) and output variables (y).\n\nLinear regression tends to establish a relationship between them by formulating an equation that describes the outcome (y) as a linear combination of the input variables (multiplied with the corresponding variables learned by the model from training).\n\n\"If the predictor variable is represented by x, and the outcome variable is represented by y in our case crop prices, then the\nrelationship can be expressed by the equation \n                        \n                        y=\u03b20 + \u03b21x\n\nwhere \u03b21 represents the slope of the x, and \u03b20 is the intercept or error term for the equation.\nWhat linear regression does is estimate the values of \u03b20 and \u03b21 from a set of observed data\npoints, where the values of x, and associated values of y, are provided. So, when a new or\npreviously unobserved data point comes where the value of y is unknown, it can fit the\nvalues of x, \u03b20, and \u03b21 into the above equation to predict the value of y.\"- Shah C (Hands on introduction to Data Science)","daeca236":"From the generated  histogram we see the variation of our prices and  looks like most of commodity prices range between 2500-3200ksh","a0d5ccab":"### Buidling A Random Forest Regression","cd93a722":"    From our scatter plot Most produce variety were sold in 2013 and 2015 compare to 2012. \n    Groundnuts,green grams,Dolichos(Njahi) and Beans of type(mwetemania ) among the crops       that fetched a high price of above ksh10,000($90)in 2013. \n    \n    Groundnuts maintained as steady market price also in 2015 followed by green grams and       Dolichos(Njahi).\n","afa6e055":"We find that our model has an accuracy of around 43% with use of log prices in our regression","f7b650e2":"### Prediction","0a1edcd9":"#### Check for missing values"}}