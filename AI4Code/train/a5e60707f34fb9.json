{"cell_type":{"b25191eb":"code","74281eac":"code","2b0f9672":"code","eb97cfc4":"code","0ac67e65":"code","228ac6f8":"code","cdddca37":"code","8f1a6e2e":"code","0bf9df33":"code","cc41ac5d":"code","f0b7d16a":"code","f60cfac5":"code","b149051b":"code","6ef83355":"code","ba6275ae":"code","a50546f5":"code","a2b45f56":"code","17b1c0cc":"code","42a13823":"code","b4d2c77e":"code","5e28564a":"code","d7615085":"code","21909824":"code","cd6ec1a5":"code","c8dbec6f":"code","b06d4cd9":"code","0dfdd4ef":"code","cab6f369":"code","c4761573":"code","18ed4ec2":"code","f285124b":"code","27cbc9d7":"code","18a3d9b3":"code","0451624a":"code","644c37df":"code","83b5ca64":"code","75537f4a":"code","7e09e0cd":"code","0b3b4a21":"code","49e93a15":"code","cae2e3ae":"code","9490fb33":"code","50a2ec21":"code","40384680":"code","17355779":"code","553ef6d7":"code","b61d148d":"code","df7a1913":"markdown","3034640c":"markdown","5c788294":"markdown","596cf6a0":"markdown","39fdd6c6":"markdown","260e4d97":"markdown","c732ec01":"markdown","44a06813":"markdown"},"source":{"b25191eb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport matplotlib.pyplot as plt #Plotting\nimport seaborn as sns\n# Scaling preprocessing library\nfrom sklearn.preprocessing import StandardScaler\nimport sklearn.preprocessing\nfrom sklearn.preprocessing import Imputer\n# Math Library\nfrom math import ceil\nfrom functools import reduce\n# Boosting Libraries\nfrom xgboost import XGBRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor\nfrom keras.models import Sequential\nfrom keras.layers import Dense , Dropout , Lambda, Flatten\nfrom keras.optimizers import Adam ,RMSprop\nfrom sklearn.model_selection import train_test_split\nfrom keras import  backend as K\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","74281eac":"# Importing the dataset\ntrain = pd.read_csv('..\/input\/train.csv',low_memory=False)\ntrain.head()","2b0f9672":"# Importing the dataset\ntest = pd.read_csv('..\/input\/test.csv',low_memory=False)\ntest.head()","eb97cfc4":"#Filling for NaN values\ntrain = train.fillna(0)\ntest = test.fillna(0)\ntrain.head()","0ac67e65":"#Removal of first row\ntrain= train[1:]","228ac6f8":"#Removal of first row\ntest= test[1:]","cdddca37":"#Feature Selection\nx_train = train.loc[:, train.columns != 'label'].values.astype(int)\ny_train = train.iloc[:, -1].values.astype(int)","8f1a6e2e":"x_train","0bf9df33":"y_train.astype(float)","cc41ac5d":"x_test =test.iloc[:, test.columns != 'label'] .values.astype(int)","f0b7d16a":"x_test","f60cfac5":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(x_train)\nX_test = sc.transform(x_test)","b149051b":"mean_vec = np.mean(X_train, axis=0)\ncov_mat = (X_train - mean_vec).T.dot((X_train - mean_vec)) \/ (X_train.shape[0]-1)\nprint('Covariance matrix \\n%s' %cov_mat)","6ef83355":"mean_vec = np.mean(X_test, axis=0)\ncov_mat1 = (X_test - mean_vec).T.dot((X_test - mean_vec)) \/ (X_test.shape[0]-1)\nprint('Covariance matrix \\n%s' %cov_mat1)","ba6275ae":"print('NumPy covariance matrix: \\n%s' %np.cov(X_train.T))","a50546f5":"print('NumPy covariance matrix: \\n%s' %np.cov(X_test.T))","a2b45f56":"#Plotting of covariance matrix\nplt.figure(figsize=(20,20))\nsns.heatmap(cov_mat, vmax=1, square=True,annot=True,cmap='cubehelix')\n\nplt.title('Correlation between different features')","17b1c0cc":"#Generation of Eigenvectors and Eigenvalues from train covariance matrix\neig_vals, eig_vecs = np.linalg.eig(cov_mat)\n\nprint('Eigenvectors \\n%s' %eig_vecs)\nprint('\\nEigenvalues \\n%s' %eig_vals)","42a13823":"#Generation of Eigenvalues and Eigenvectors from test covariance matrix\neig_vals1, eig_vecs1 = np.linalg.eig(cov_mat1)\n\nprint('Eigenvectors \\n%s' %eig_vecs1)\nprint('\\nEigenvalues \\n%s' %eig_vals1)","b4d2c77e":"# Make a list of (eigenvalue, eigenvector) tuples\neig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n\n# Sort the (eigenvalue, eigenvector) tuples from high to low\neig_pairs.sort(key=lambda x: x[0], reverse=True)\n\n# Visually confirm that the list is correctly sorted by decreasing eigenvalues\nprint('Eigenvalues in descending order:')\nfor i in eig_pairs:\n    print(i[0])","5e28564a":"# Make a list of (eigenvalue, eigenvector) tuples\neig_pairs1 = [(np.abs(eig_vals1[i]), eig_vecs1[:,i]) for i in range(len(eig_vals1))]\n\n# Sort the (eigenvalue, eigenvector) tuples from high to low\neig_pairs1.sort(key=lambda x: x[0], reverse=True)\n\n# Visually confirm that the list is correctly sorted by decreasing eigenvalues\nprint('Eigenvalues in descending order:')\nfor i in eig_pairs1:\n    print(i[0])","d7615085":"#Reshaping of Eigenpairs Matrix\nmatrix_w = np.hstack((eig_pairs[0][1].reshape(55,1), \n                      eig_pairs[1][1].reshape(55,1)\n                    ))\nprint('Matrix W:\\n', matrix_w)","21909824":"#Reshaping Test eigenpairs Matrix\nmatrix_w1 = np.hstack((eig_pairs1[0][1].reshape(55,1), \n                      eig_pairs1[1][1].reshape(55,1)\n                    ))\nprint('Matrix W:\\n', matrix_w1)","cd6ec1a5":"Y = X_train.dot(matrix_w)\nY","c8dbec6f":"Y1 = X_test.dot(matrix_w1)\nY1","b06d4cd9":"#Principal component analysis for feature column dropping\nfrom sklearn.decomposition import PCA\npca = PCA().fit(X_train)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlim(0,55,1)\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')","0dfdd4ef":"from sklearn.decomposition import PCA\npca = PCA().fit(X_test)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlim(0,55,1)\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')","cab6f369":"#Dropping of columns from where covariance is almost 1.0\nfrom sklearn.decomposition import PCA \nsklearn_pca = PCA(n_components=50)\nX_pca_train = sklearn_pca.fit_transform(X_train)","c4761573":"from sklearn.decomposition import PCA \nsklearn_pca = PCA(n_components=50)\nX_pca_test = sklearn_pca.fit_transform(X_test)","18ed4ec2":"print(X_pca_train)","f285124b":"print(X_pca_test)","27cbc9d7":"X_pca_train.shape","18a3d9b3":"#Splitting the train set into training data and validation data\ntrainX, valX, trainY, valY = train_test_split(X_pca_train, y_train, test_size=0.2, random_state=42)","0451624a":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n\nclf = RandomForestClassifier(random_state=1)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_train)\naccuracy_score(y_train,y_pred)\n","644c37df":"score =clf.predict_proba(X_test)","83b5ca64":"def getModel(arr):\n    model=Sequential()\n    for i in range(len(arr)):\n        if i!=0 and i!=len(arr)-1:\n            if i==1:\n                model.add(Dense(arr[i],input_dim=arr[0],kernel_initializer='normal', activation='relu'))\n            else:\n                model.add(Dense(arr[i],activation='relu'))\n    model.add(Dense(arr[-1],kernel_initializer='normal',activation=\"sigmoid\"))\n    model.compile(loss=\"binary_crossentropy\",optimizer='rmsprop',metrics=['accuracy'])\n    return model","75537f4a":"#Define a model of 5 dense layers\nModel=getModel([50,50,70,40,1])","7e09e0cd":"import keras\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nclass PlotLosses(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.i = 0\n        self.x = []\n        self.losses = []\n        self.val_losses = []\n        \n        self.fig = plt.figure()\n        \n        self.logs = []\n\n    def on_epoch_end(self, epoch, logs={}):\n        \n        self.logs.append(logs)\n        self.x.append(self.i)\n        self.losses.append(logs.get('loss'))\n        self.val_losses.append(logs.get('val_loss'))\n        self.i += 1\n        \n        clear_output(wait=True)\n        plt.plot(self.x, self.losses, label=\"loss\")\n        plt.legend()\n        plt.show();\n        \nplot_losses = PlotLosses()","0b3b4a21":"#Fitting the Model\nModel.fit(np.array(trainX),np.array(trainY),epochs=6,callbacks=[plot_losses])\n","49e93a15":"#Accuracy Score\nscores=Model.evaluate(np.array(valX),np.array(valY))","cae2e3ae":"print(scores)","9490fb33":"#Probability Prediction\npredY=Model.predict_proba(np.array(X_pca_test))\n","50a2ec21":"# Uncomment the whole section and run it \n#param_grid = { \n #   'n_estimators': [200, 500],\n  #  'max_features': ['auto', 'sqrt', 'log2'],\n   # 'max_depth' : [4,5,6,7,8],\n    #'criterion' :['gini', 'entropy']\n#} ","40384680":"#CV_rfc = GridSearchCV(estimator=clf, param_grid=param_grid, cv= 5)\n#CV_rfc.fit(x_pca_train, y_train)","17355779":"#CV_rfc.best_params_\n#rfc1=RandomForestClassifier(random_state=42, max_features='auto', n_estimators= 200, max_depth=8, criterion='gini')","553ef6d7":"#rfc1.fit(x_pca_train, y_train)","b61d148d":"#pred=rfc1.predict_proba(x_pca_test)","df7a1913":"> **Best Result came from GridSearchCV**\n\n\n(Hypertuning of Random Forrest ) With best parameters selection in the random forrest GridSearchCV the best accuracy i've got was 0.96623","3034640c":"> **Importing dataset and needed libraries**","5c788294":"* **Handling the Missing Values**\n\n\nI've used 0 as mean or median handling would be a hectic work to do, moreover there are no missing values.","596cf6a0":"1. *Importing of Train data as i've already added the headers inside the csv files.*","39fdd6c6":"> **Neural Network Approach **\n\nI have used a simple neural network to find the accuracy and the prediction score, this model gives pretty low compare to random forrest but hypertuning of parameters will give the better results.","260e4d97":"> **Generation of Covariance Matrix through mean vector**","c732ec01":"> **American Express Artificial Intelligence Hackerearth Problem 2 Soution**\n\nRecently i participated in Hackerearth challenge which lasted 16 days based on Big Data and classification modelling. The accuracy of problem was evaluated by LUAC and AUC as prediction values are probabilities.","44a06813":"> **Modelling of training data and prediciton of scores**\n\nThrough Random Forrest classifier"}}