{"cell_type":{"c1f92a83":"code","808a0488":"code","9a5e282d":"code","f946d1eb":"code","1b43775c":"code","3d4f9929":"code","ecd74810":"code","309c3264":"code","ab5cbd38":"code","d9385c02":"code","2b96e13c":"markdown","f61e50cd":"markdown","cb236cd4":"markdown","f5553f3d":"markdown","2181bded":"markdown"},"source":{"c1f92a83":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","808a0488":"pd.options.mode.chained_assignment = None\n","9a5e282d":"#Load the matches that have already occurred \ndf = pd.read_csv(\"\/kaggle\/input\/ultimate-ufc-dataset\/ufc-master.csv\")\n\n# Load the upcoming matches\ndf_upcoming = pd.read_csv(\"\/kaggle\/input\/ultimate-ufc-dataset\/upcoming-event.csv\")\n\n# Most recent: \n# df_upcoming = pd.read_csv(\"\/kaggle\/input\/ultimate-ufc-dataset\/most-recent-event.csv\")\n\n\nprint(df_upcoming)\n\n\n#Get the number of upcoming fights\nnum_upcoming_fights = len(df_upcoming)\nprint(f\"We are going to predict the winner of {num_upcoming_fights} fights.\")\n\n#Combine the upcoming fights to the previous fights so we can clean it all at the same time.\ndf_combined = df_upcoming.append(df)\n\n#Let's put all the labels into a dataframe\ndf_combined['label'] = ''\n\n#We need to convert 'Red' and 'Blue' to 0 and 1\nmask = df_combined['Winner'] == 'Red'\ndf_combined['label'][mask] = 0\nmask = df_combined['Winner'] == 'Blue'\ndf_combined['label'][mask] = 1\n\n#Make sure label is numeric\ndf_combined['label'] = pd.to_numeric(df_combined['label'], errors='coerce')\n\n#Make sure the date column is datetime\ndf_combined['date'] = pd.to_datetime(df['date'])\n\n#Copy the labels to their own dataframe\nlabel_df = df_combined['label']\n\n#Split the train set from the test set\n\ndf_train = df_combined[num_upcoming_fights:]\nlabel_train = label_df[num_upcoming_fights:]\n\ndf_test = df_combined[:num_upcoming_fights]\nlabel_test = label_df[:num_upcoming_fights]\n\n\n#Make sure the sizes are the same\nprint(len(df_test))\nprint(len(label_test))\n\nprint(len(df_train))\nprint(len(label_train))\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n#Pick a model\nmy_model = DecisionTreeClassifier(max_depth=9)\n\n#Pick some features\n#I would not recommend placing bets based off of these features...\n# my_features = ['B_wins', 'R_wins', 'B_Height_cms', 'R_Height_cms', 'B_Weight_lbs', 'R_Weight_lbs', 'sig_str_dif']\nmy_features = ['B_wins', 'R_wins', 'height_dif', 'R_age', 'B_age', 'B_Height_cms', 'R_Height_cms', 'age_dif', 'sig_str_dif']\n\n#Let's grab the names of the fighters for the upcoming event\n#This will be useful to print predictions at the end.\nfighters_test = df_test[['R_fighter', 'B_fighter']]\nodds_test = df_test[['R_odds', 'B_odds']]\n\n\n#Make dataframes that only contain the relevant features\ndf_train_prepped = df_train[my_features].copy()\ndf_test_prepped = df_test[my_features].copy()\n\n#If we need to dummify the datasets do it now.  We need to be careful that the test set has all of the features\n#that the training set does\n\ndf_train_prepped = pd.get_dummies(df_train_prepped)\ndf_test_prepped = pd.get_dummies(df_test_prepped)\n\n#Ensure both sets are dummified the same\ndf_train_prepped, df_test_prepped = df_train_prepped.align(df_test_prepped, join='left', axis=1)    \n\n#The new test set may have new new features after the above join.  Fill them with zeroes\ndf_test_prepped = df_test_prepped.fillna(0)\n\n#Since we may have dropped some rows we need to drop the matching rows in the labels\nlabel_train_prepped = label_train[label_train.index.isin(df_train_prepped.index)]\nlabel_test_prepped = label_test[label_test.index.isin(df_test_prepped.index)]\nfighters_test_prepped = fighters_test[fighters_test.index.isin(df_test_prepped.index)]\nodds_test_prepped = odds_test[odds_test.index.isin(df_test_prepped.index)]\n\n\n#Quick test that lengths match.\nprint(len(label_train_prepped))\nprint(len(df_train_prepped))\nprint(len(label_test_prepped))\nprint(len(df_test_prepped))\nprint(len(fighters_test_prepped))\nprint(len(odds_test_prepped))\n\nmy_model.fit(df_train_prepped, label_train_prepped)","f946d1eb":"# Now we deviate.  Instead of grabbing the predictions we will grab the probabilities\n\nprobs = my_model.predict_proba(df_test_prepped)","1b43775c":"#What we have here is an array with the probability the RED will win or BLUE will win.  We can use this to calculate bets\n\nprobs","3d4f9929":"#Let's put the fighter names, odds, and probabilities together so we can cycle through them easily\nfighters_array = fighters_test_prepped.to_numpy()\nodds_array = odds_test_prepped.to_numpy()","ecd74810":"probs_list = np.array(list(zip(fighters_array, odds_array, probs)))","309c3264":"probs_list","ab5cbd38":"def get_bet_ev(odds, prob):\n    if odds>0:\n        return ((odds * prob) - (100 * (1-prob)) )\n    else:\n        return ((100 \/ abs(odds))*100*prob - (100 * (1-prob)))\n","d9385c02":"for p in probs_list:\n    red_ev = get_bet_ev(p[1][0], p[2][0])\n    blue_ev = get_bet_ev(p[1][1], p[2][1])\n    \n    print (p[0][0], \"(RED) vs \", p[0][1], \"(BLUE)\")\n    print(p[0][0], \"has a\", \"%.2f\" % (p[2][0]*100), \"percent chance of winning.  His odds are\", p[1][0], \"This give him a single bet EV of\", \"%.2f\" %red_ev)\n    print(p[0][1], \"has a\", \"%.2f\" % (p[2][1]*100), \"percent chance of winning.  His odds are\", p[1][1], \"This give him a single bet EV of\", \"%.2f\" %blue_ev)\n    if red_ev > 0:\n        print(\"RED is a good bet\")\n    elif (blue_ev > 0):\n        print(\"BLUE is a good bet\")\n    else:\n        print(\"There is NO good bet\")\n    \n    print()","2b96e13c":"# Calculate Profitable Bets\n\nThis fucntion will use the American odds and the probability of a fighter winning to return the Expected Value (EV) of a 100 unit bet on a given fighter.\n\nIt works like this:\n\nIf a fighter has positive odds:\n\nEV = (`odds` * `probability a fighter wins`) - (`100` * (`1` - `probability a fighter wins`))\n\nIf a fighter has negative odds:\n\nEV = (`100` \/ abs(`odds`)) * `100` * `probability a fighter wins` - (`100` * (`1` - `probability a fighter wins`))","f61e50cd":"predict_proba will work for LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GradientBoostingClassifier, and GaussianNB among others","cb236cd4":"# Conclusion\n\nThere will never bet 2 good bets for a fight.  There will be times when there is no good bet.  It probably won't occur as much as in the model above.  The model used in the kernel is very basic relying mostly on the fighter odds so it returns a lot of predictions in this \"no man's land\"\n\nI am still new to Machine Learning projects and only taught myself Python in November. Any suggestions would be welcome! I hope this was helpful.\n\nIf you're interested in playing around with this notebook you just need to change these two lines. See what kind of difference it makes in the final predictions:\n\n`my_model = DecisionTreeClassifier(max_depth=5)`\n\n`my_features = ['R_odds', 'B_Stance']`","f5553f3d":"# Method of Acquiring and Implementing Probabilites\n\nLuckily for us many machine learning models have probabilities built in.  We can easily use these features in models including LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GradientBoostingClassifier, and GaussianNB.\n\nTo get the probabilities we will follow my [previous notebook](https:\/\/www.kaggle.com\/mdabbert\/tutorial-train-a-model-to-predict-a-future-event) until we get to where we grabbed the predictions.  In place of this we will grab the probabilities.  If you have any questions about what is going on in this code, please look at the previous notebook.\n\nThe one difference in the following cell of code is that we are creating a df called `odds_test`.  It is a collection of odds for the upcoming fights.  It will make cycling through the fights easier.","2181bded":"# Purpose\n\nThis kernal builds off of my previous kernal, [Tutorial: Train a Model to Predict a Future Event](https:\/\/www.kaggle.com\/mdabbert\/tutorial-train-a-model-to-predict-a-future-event)\n\nThat kernal showed you how to train a model using events that have already occurred to predict the winners of a future event.  This kernal will take the next step and use probabilities to predict profitable bets for future events.\n\n### What is the Difference?\n\nTaking a look at the upcoming fights we have a match between Amanda Ribas and Paige VanZant.  Amanda Rebas has American betting odds of -835.  This means you would need to bet 835 units to receive a return of 100 units on a winning bet.  To determine what percentage of time Amanda Rebas must win for this to be a profitable bet we use the following formula:\n\n`(Payout on 100 Unit Bet) * (Break Even win probability) - 100 * (1 - Break Even win probability) = 0`\n\n`((100 \/ 835) * 100) * x - 100 * (1 - x) = 0`\n\n`11.97 * x - 100 + 100*x = 0`\n\n`111.97x = 100`\n\n`x = .893`\n\nAmanda Rebas must win at least 89.3% of the time for this bet to be profitable.\n(89.3% of the time you win 11.97 units.  10.7% of the time you lose 100 units. .893 * 11.97 - .107 * 100 ~= 0)\n\nAmanda Rebas simply being predicted to win the fight isn't enough.  We need to know that she has around a 90% chance to win to want to bet her.  I'd go as far to say that any model that doesn't predict Amanda Rebas to be the winner based off of a 50% threshold is probably a flawed model.\n\n"}}