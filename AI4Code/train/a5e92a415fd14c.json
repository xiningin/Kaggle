{"cell_type":{"4df951d1":"code","09a92c4d":"code","dedbfad7":"code","b1113495":"code","ed3ebb02":"code","bcd9eb55":"code","35563143":"code","27885090":"code","d4794514":"code","9ba83b6f":"code","cd0dd17f":"code","c243dcdc":"code","94d8f14e":"code","f9cf0953":"code","782b9df6":"code","1267b1fe":"code","ee3f454d":"code","b775cf78":"code","07b523e7":"code","57b92520":"code","8624dd46":"code","90a9058d":"code","4810caed":"code","a8f8af13":"code","01417a95":"code","55465a7d":"markdown","b79f21e8":"markdown","09439aa5":"markdown","5584171c":"markdown","bde3a37d":"markdown","ac83649a":"markdown","1ec226c2":"markdown","b51aecab":"markdown","4af39e7a":"markdown"},"source":{"4df951d1":"import pandas as pd \nimport os, time  \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom typing import Tuple, List, Union, Dict \n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.manifold import TSNE\nimport umap \nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import RidgeClassifier, SGDClassifier, LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, VotingClassifier, RandomTreesEmbedding\nfrom sklearn.metrics import log_loss, accuracy_score\n\nfrom tensorflow.keras.layers import Dense, Dropout \nfrom tensorflow.keras.models import Sequential\nimport warnings \n\nwarnings.simplefilter(\"ignore\")","09a92c4d":"### ---> production dataset \n# train = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\")\n# test = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")\n\n### ----> debug dataset \ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\", nrows=1000)\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\", nrows=1000)\ntrain.drop(\"id\", axis=1, inplace=True)\ntest.drop(\"id\", axis=1, inplace=True)\ntrain.head()","dedbfad7":"test.head()","b1113495":"plt.pie(x = train.claim.value_counts().values, labels=[0, 1],\n        counterclock=90, startangle=False, autopct=\"%1.1f%%\")\nplt.show()","ed3ebb02":"train.isnull().sum()","bcd9eb55":"is_nan = train.isnull().sum(axis=1)\ndf_is_nan = pd.DataFrame({\"nan_cnt\": is_nan.values, \"claim\": train.claim}, index=is_nan.index)\ndf_is_nan.groupby(\"claim\").mean()","35563143":"train[\"is_nan\"] = train.isnull().sum(axis=1)\ntest[\"is_nan\"] = test.isnull().sum(axis=1)","27885090":"cols = train.drop([\"claim\"], axis=1).columns \nfor col in cols:\n    train[col] = train[col].fillna(train[col].mean())\n    test[col] = test[col].fillna(train[col].mean())","d4794514":"# s = MinMaxScaler()\n# train_s = s.fit_transform(train.drop(\"claim\", axis=1))\n# test_s = s.transform(test)\n\n# pca = PCA(n_components=65, random_state=42) # explained ratio over 80.0%\n# pca_tr = pca.fit_transform(train_s)\n# pca_te = pca.transform(test_s)\n\n# plt.figure(figsize=(12, 6))\n# plt.plot(pca.explained_variance_ratio_.cumsum())\n# plt.xticks(np.arange(65).tolist())\n# plt.xlabel(\"n_components\")\n# plt.ylabel(\"explained_variance_ratio_\")\n# plt.grid()\n# plt.show()","9ba83b6f":"# y = train.claim.to_list()\n# train = pd.DataFrame(pca_tr, index=train.index, columns=[f\"c{c}\" for c in range(65)])\n# train[\"claim\"] = y \n# test = pd.DataFrame(pca_te, index=test.index, columns=[f\"c{c}\" for c in range(65)])\n\n# del pca_tr, pca_te \n# train.head()","cd0dd17f":"def k_split(train: pd.DataFrame, k=4) -> pd.DataFrame:\n    kf = StratifiedKFold(n_splits=k, random_state=42, shuffle=True)\n    for i, (tr, va) in enumerate(kf.split(train, train.claim)):\n        train.loc[va, \"fold\"] = int(i)\n    train[\"fold\"] = train.fold.astype(np.uint8)\n    return train \n\ntrain = k_split(train)\ntrain.head()","c243dcdc":"for i in range(4):\n    value = train[train.fold == i][\"claim\"].value_counts().values\n    index = train[train.fold == i][\"claim\"].value_counts().index\n    plt.subplot(2, 2, i+1)\n    plt.pie(x=value, labels=index, startangle=90, counterclock=False, autopct=\"%1.1f%%\")\n    plt.title(f\"Fold{i}\")\nplt.tight_layout()","94d8f14e":"def Net():\n    input_size = 119\n    model = Sequential()\n    model.add(Dense(256, activation=\"relu\", input_shape=(input_size, )))\n    model.add(Dropout(0.2))\n    model.add(Dense(128, activation=\"relu\"))\n    model.add(Dropout(0.2))\n    model.add(Dense(1, activation=\"sigmoid\"))\n    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n    return model \n\ndef model_layers(seed):\n    clfs = []\n    clfs.append((\"XGBClassifier\", Pipeline([\n        (\"XGB\", XGBClassifier(n_jobs=-1, random_state=seed))\n    ])))\n    clfs.append((\"SVC\", Pipeline([\n        (\"SVC\", SVC(random_state=seed))\n    ]))) \n    clfs.append((\"DecisionTreeClassifier\", Pipeline([\n        (\"DecisionTreeClassifier\", DecisionTreeClassifier(random_state=seed))\n    ]))) \n    clfs.append((\"RandomForestClassifier\", Pipeline([\n        (\"RandomForestClassifier\", RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=seed))\n    ]))) \n    clfs.append((\"RidgeClassifier\", Pipeline([\n        (\"RidgeClassifier\", RidgeClassifier(random_state=seed))\n    ]))) \n    clfs.append((\"NN\", Pipeline([\n        (\"NN\", Net())\n    ]))) \n    clfs.append((\"ExtraTreesClassifier\", Pipeline([\n        (\"ExtraTreesClassifier\", ExtraTreesClassifier(n_jobs=-1, random_state=seed))\n    ])))\n    clfs.append((\"BaggingRidgeClassifier\",Pipeline([\n        (\"BaggingClassifier\", BaggingClassifier(n_jobs=-1, random_state=42))\n    ])))\n    return clfs \n\n\n'''\nLayer1:\n\npredict validation -> Next train dataset \npredict test -> mean test \n'''\n\nclass Layer1():\n    def __init__(self, seed=42):\n        self.models = model_layers(seed)\n        \n    def train(self, train, test) -> Tuple[pd.DataFrame, pd.DataFrame]:\n        df_train, df_test = pd.DataFrame(), pd.DataFrame()\n        fold_list = train[\"fold\"].to_list()\n        for name, model in self.models:\n            train_ = train.copy()\n            test_ = test.copy()\n            predict_val, predict_test, val_index = [], [], []\n            for fold in range(4):\n                x_tr, x_va = train_[train_.fold != fold], train_[train_.fold == fold]\n                x_train, y_train = x_tr.drop([\"claim\", \"fold\"], axis=1), x_tr[[\"claim\"]]\n                x_val, y_val = x_va.drop([\"claim\", \"fold\"], axis=1), x_va[[\"claim\"]]\n                x_train, x_val, x_test = self._transform(x_train, x_val, test_)\n                \n                pred_val, va_idx, pred_test = self._predict_cv(model, \n                                                               x_train,\n                                                               y_train,\n                                                               x_val, \n                                                               y_val,\n                                                               x_test,\n                                                               name)\n                self._logs(pred_val, y_val, fold, name)\n                predict_val.append(pred_val)\n                predict_test.append(pred_test)\n                val_index.append(va_idx)\n            # concat predict valid \n            # mean predict test-data \n            va_idxs = np.concatenate(val_index)\n            preds = np.concatenate(predict_val, axis=0)\n            order = np.argsort(va_idxs)\n            pred_train = preds[order] # Next train-dataset \n            pred_test = np.mean(predict_test, axis=0) # Next test-dataset \n            \n            df_train[f\"{name}_feature\"] = pred_train \n            df_test[f\"{name}_feature\"] = pred_test \n        df_train[\"fold\"] = fold_list\n        df_train[\"claim\"] = train[\"claim\"].values.ravel()\n        del pred_train, pred_test, x_train, x_val, y_train, y_val \n        return df_train, df_test \n    \n    def _logs(self, pred_val, y_val, fold, name):\n        loss = log_loss(pred_val, y_val.values.ravel())\n        accuracy = accuracy_score(pred_val.ravel(), y_val.values.ravel())\n        print(\n            f\"Model: {name} | Fold: {fold} | Loss: {loss:.5f} | Accuracy: {accuracy:.5f}\"\n        )\n            \n    def _predict_cv(self, model, x_train, y_train, x_val, y_val, test, name):\n        model.fit(x_train, y_train)\n        pred_val = model.predict(x_val)\n        if name == \"NN\":\n            pred_val = np.where(pred_val >= 0.5, 1, 0)\n        pred_test = model.predict(test)\n        if name == \"NN\":\n            pred_test = np.where(pred_test >= 0.5, 1, 0)\n        va_idx = x_val.index \n        return pred_val, va_idx, pred_test\n        \n    def _transform(self, train, val, test) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n        '''StandardScaler train\/val\/test '''\n        col = train.columns\n        tr_idx, va_idx, te_idx = train.index, val.index, test.index \n        s = StandardScaler()\n        train = s.fit_transform(train)\n        val = s.transform(val)\n        test = s.transform(test)\n        train = pd.DataFrame(train, columns=col, index=tr_idx)\n        val = pd.DataFrame(val, columns=col, index=va_idx)\n        test = pd.DataFrame(test, columns=col, index=te_idx)\n        return train, val, test \n    \n'''\nLayer2:\nparamter chunning \n'''    \n\nclass Layer2():\n    def __init__(self):\n        self.param = {\n            \"max_depth\": [1, 3, 5, 8, 10],\n            \"colsample_bytree\": [1, 3, 5, 8, 10],\n            \"eta\": [0.001, 0.01, 0.1, 1.0, 10.0]\n        }\n    \n    def fit(self, train):\n        train_ = train.copy()\n        predict_proba = []\n        for fold in range(4):\n            start = time.time()\n            tr, va = train_[train_.fold != fold], train_[train_.fold == fold]\n            x_train, x_val = tr.drop([\"fold\", \"claim\"], axis=1), va.drop([\"fold\", \"claim\"], axis=1)\n            y_train, y_val = tr[[\"claim\"]], va[[\"claim\"]]\n            \n            grid = GridSearchCV(XGBClassifier(random_state=42, silent=1),\n                                param_grid=self.param,\n                                cv=2).fit(x_train, y_train)\n            model = XGBClassifier(random_state=42,\n                                  silent=1, \n                                  **grid.best_params_,\n                                  eval_set=[(x_train, y_train), (x_val, y_val)], \n                                  early_stopping_rounds=30).fit(x_train, y_train)\n            pred_val = model.predict(x_val)\n            proba = model.predict_proba(x_val)[:, 1]\n            predict_proba.append(proba)\n            self._logs(pred_val, y_val, fold, start)\n            self._save(model, fold)\n        predict_proba = np.concatenate(predict_proba, axis=0)\n        del model, x_train, x_val, y_train, y_val, proba \n        return predict_proba \n            \n    def _logs(self, pred_val, y_val, fold, start):\n        loss = log_loss(pred_val, y_val.values.ravel())\n        accuracy = accuracy_score(pred_val, y_val.values.ravel())\n        print(\n            f\"Fold: {fold} | Loss: {loss:.5f} | Accuracy: {accuracy:.5f}|\"\n        )\n        \n    def _save(self, model, fold):\n        os.makedirs(\"models\", exist_ok=True)\n        model.save_model(f\"models\/xgb{str(fold)}.model\")\n        print(\"successfully checkpoint model\")\n            \n        \n    ","f9cf0953":"layer_first = Layer1()\nlayer_second = Layer2()","782b9df6":"df_train, df_test = layer_first.train(train, test)","1267b1fe":"tr_col = df_train.drop([\"fold\", \"claim\"], axis=1).columns \ndf_test = df_test[tr_col]\ndf_train.head()","ee3f454d":"pca = TSNE(n_components=2, random_state=42)\npca_tr, pca_te = pca.fit_transform(df_train.drop([\"claim\", \"fold\"], axis=1)), pca.fit_transform(df_test)\n\ny = df_train[\"claim\"]\nplt.scatter(x=pca_tr[:, 0][y==0], y=pca_tr[:, 1][y==0], c=\"b\")\nplt.scatter(x=pca_tr[:, 0][y==1], y=pca_tr[:, 1][y==1], c=\"r\")\nplt.title(\"TSNE vs claim\")\nplt.grid()\nplt.legend([\"0\", \"1\"])\nplt.show()","b775cf78":"km = KMeans(n_clusters=2, random_state=42)\ntr_cluster = km.fit_predict(pca_tr)\nte_cluster = km.predict(pca_te)\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\nax = axes.ravel()\nfor i in range(2):\n    ax[i].scatter(pca_tr[:, 0][tr_cluster == i], pca_tr[:, 1][tr_cluster == i], c=\"b\" if i == 0 else \"r\", alpha=0.8)\n    ax[i].scatter(pca_tr[:, 0], pca_tr[:, 1], c=(0, 0, 0), alpha=0.1)\n    ax[i].set_title(f\"clsuter{i+1}\")","07b523e7":"df_train[\"cluster\"] = tr_cluster \ndf_test[\"cluster\"] = te_cluster ","57b92520":"df_train[[\"cluster\", \"claim\"]].corr()","8624dd46":"df_train.head()","90a9058d":"predict = layer_second.fit(df_train) # return predict proba ","4810caed":"sns.histplot(predict)\nplt.title(\"Train\")\nplt.show()","a8f8af13":"def submittion(test):\n    model = XGBClassifier(random_state=42)\n    root_path = \"models\"\n    filename = os.listdir(root_path)\n    predict = []\n    for fold in range(4):\n        model.load_model(os.path.join(root_path, filename[fold]))\n        pred = model.predict_proba(test)[:, 1]\n        predict.append(pred.ravel())\n    predict = np.mean(np.array(predict), axis=0)\n    \n    try:\n        sub = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")\n        sub[\"claim\"] = predict \n        sub.to_csv(\"submit.csv\", index=False)\n    finally:\n        del model \n        return predict \n    ","01417a95":"predict = submittion(df_test)\n\nsns.histplot(predict)\nplt.title(\"Test\")\nplt.show()","55465a7d":"# Stacking_Models","b79f21e8":"# Submit ","09439aa5":"# Cross-validation split ","5584171c":"# Clustering","bde3a37d":"# Training Layer2","ac83649a":"# PCA ","1ec226c2":"# Fill values ","b51aecab":"# Procedure method  \n---  \n  \n\n1. KFold \n2. Stacking Layer1 \n3. TSNE \n4. Clustering \n5. Stacking Layer2  \n6. submit ","4af39e7a":"# Training Layer1"}}