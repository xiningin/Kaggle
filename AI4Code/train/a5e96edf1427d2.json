{"cell_type":{"12135804":"code","e6f85a03":"code","b4a68e6b":"code","151fe7d5":"code","601074ff":"code","020586c1":"code","8ed86022":"code","5909ebb6":"code","2653fbf5":"code","ce1c72f4":"code","75272c3c":"code","034360f7":"code","dbac571c":"code","aecc3cce":"code","9a6b66c8":"code","27a45e8c":"code","540c01cd":"code","e1e76550":"code","53dfba7d":"code","faea9252":"code","e2cd6e6f":"code","ff2d358c":"code","aa623abc":"code","a693c4ba":"code","3cd9e903":"code","a99bce2f":"code","4b3831b0":"code","4249b048":"code","2343cf1e":"code","0fa2a30a":"code","9300619b":"code","7f6954d7":"code","19f94f75":"code","cd96d2c6":"code","c8b47998":"code","b6e5584b":"code","8226eb0f":"code","10c59ba7":"code","45f5a150":"code","bfef79b3":"code","952fa6b7":"code","d75564ea":"code","0c631d71":"code","a185fe70":"code","1d2ee984":"code","6948838f":"code","aa721f77":"code","a895b98f":"code","75ed0311":"markdown","956acd2b":"markdown","a43ef9e7":"markdown","cd1f8f14":"markdown","d6a137b0":"markdown","a23bef9a":"markdown","3378bf17":"markdown","f0b64937":"markdown","4dcb93d6":"markdown","c812e453":"markdown","e8bf4632":"markdown","3c3a7c85":"markdown","1574cc80":"markdown","0dfc6664":"markdown","2fccbe9e":"markdown","ca5126b3":"markdown","f631fd61":"markdown","ca7f36b2":"markdown","f8e80848":"markdown","8c85ad57":"markdown","c7b48c2f":"markdown","d4aa4ef1":"markdown","1dea9c88":"markdown","3f25dfdb":"markdown","7f48fde9":"markdown","e16c6c8b":"markdown","3b64e82f":"markdown","425ac6c9":"markdown","274c15ab":"markdown"},"source":{"12135804":"import gc\nimport os\nimport time\nimport logging\nimport datetime\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.signal import hann\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\nfrom scipy.signal import hilbert\nfrom scipy.signal import convolve\nfrom sklearn.svm import NuSVR, SVR\nfrom catboost import CatBoostRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.model_selection import KFold,StratifiedKFold, RepeatedKFold\nfrom sklearn.model_selection import cross_val_score, train_test_split, cross_val_predict\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n\n\nwarnings.filterwarnings(\"ignore\")","e6f85a03":"IS_LOCAL = False\nif(IS_LOCAL):\n    PATH=\"..\/input\/LANL\/\"\nelse:\n    PATH=\"..\/input\/\"\nos.listdir(PATH)","b4a68e6b":"print(\"There are {} files in test folder\".format(len(os.listdir(os.path.join(PATH, 'test' )))))","151fe7d5":"%%time\ntrain_df = pd.read_csv(os.path.join(PATH,'train.csv'), \n                       dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})\n","601074ff":"train_df_save = train_df.copy","020586c1":"mean_acoustic = np.mean(train_df.acoustic_data)\ntrain_df.acoustic_data = train_df.acoustic_data - mean_acoustic\nprint(mean_acoustic)\nprint (round(np.mean(train_df.acoustic_data),2))","8ed86022":"print(\"Train: rows:{} cols:{}\".format(train_df.shape[0], train_df.shape[1]))","5909ebb6":"pd.options.display.precision = 15\ntrain_df.head(10)","2653fbf5":"train_ad_sample_df = train_df['acoustic_data'].values[::50]\ntrain_ttf_sample_df = train_df['time_to_failure'].values[::50]\n\ndef plot_acc_ttf_data(train_ad_sample_df, train_ttf_sample_df, title=\"Acoustic data and time to failure: 1% sampled data\"):\n    fig, ax1 = plt.subplots(figsize=(12, 8))\n    plt.title(title)\n    plt.plot(train_ad_sample_df, color='r')\n    ax1.set_ylabel('acoustic data', color='r')\n    plt.legend(['acoustic data'], loc=(0.01, 0.95))\n    ax2 = ax1.twinx()\n    plt.plot(train_ttf_sample_df, color='b')\n    ax2.set_ylabel('time to failure', color='b')\n    plt.legend(['time to failure'], loc=(0.01, 0.9))\n    plt.grid(True)\n\nplot_acc_ttf_data(train_ad_sample_df, train_ttf_sample_df)\ndel train_ad_sample_df\ndel train_ttf_sample_df","ce1c72f4":"gc.collect()\n#train_ad_sample_df = train_df['acoustic_data'].values[:6291455]\n#train_ttf_sample_df = (train_df['time_to_failure'].values[:6291455])\ntrain_ad_sample_df = train_df['acoustic_data'].values[:50580000]\ntrain_ttf_sample_df = (train_df['time_to_failure'].values[:50580000])\n\nplot_acc_ttf_data(train_ad_sample_df, train_ttf_sample_df, title=\"Acoustic data and time to failure: 1st 2 quakes\")\ndel train_ad_sample_df\ndel train_ttf_sample_df","75272c3c":"rows = 150000\nsegments = int(np.floor(train_df.shape[0] \/ rows))\nprint(\"Number of segments: \", segments)","034360f7":"def add_trend_feature(arr, abs_values=False):\n    idx = np.array(range(len(arr)))\n    if abs_values:\n        arr = np.abs(arr)\n    lr = LinearRegression()\n    lr.fit(idx.reshape(-1, 1), arr)\n    return lr.coef_[0]\n\ndef classic_sta_lta(x, length_sta, length_lta):\n    sta = np.cumsum(x ** 2)\n    # Convert to float\n    sta = np.require(sta, dtype=np.float)\n    # Copy for LTA\n    lta = sta.copy()\n    # Compute the STA and the LTA\n    sta[length_sta:] = sta[length_sta:] - sta[:-length_sta]\n    sta \/= length_sta\n    lta[length_lta:] = lta[length_lta:] - lta[:-length_lta]\n    lta \/= length_lta\n    # Pad zeros\n    sta[:length_lta - 1] = 0\n    # Avoid division by zero by setting zero values to tiny float\n    dtiny = np.finfo(0.0).tiny\n    idx = lta < dtiny\n    lta[idx] = dtiny\n    return sta \/ lta","dbac571c":"gc.collect()","aecc3cce":"train_X = pd.DataFrame(index=range(segments), dtype=np.float64)\n\ntrain_y = pd.DataFrame(index=range(segments), dtype=np.float64, columns=['time_to_failure'])\n# These may be needed later\nneed_aggregated_features = False\nif need_aggregated_features:\n    total_mean = train_df['acoustic_data'].mean()\n    total_std = train_df['acoustic_data'].std()\n    total_max = train_df['acoustic_data'].max()\n    total_min = train_df['acoustic_data'].min()\n    total_sum = train_df['acoustic_data'].sum()\n    total_abs_sum = np.abs(train_df['acoustic_data']).sum()","9a6b66c8":"train_X.shape, train_y.shape","27a45e8c":"def create_features(seg_id, seg, X):\n    xc = pd.Series(seg['acoustic_data'].values)\n    zc = np.fft.fft(xc)\n    X.loc[seg_id, 'mean'] = xc.mean()\n    X.loc[seg_id, 'std'] = xc.std()\n    X.loc[seg_id, 'max'] = xc.max()\n    \n    #FFT transform values\n    realFFT = np.real(zc)\n    imagFFT = np.imag(zc)\n    \n    X.loc[seg_id, 'Rmean'] = realFFT.mean()\n    X.loc[seg_id, 'Rstd'] = realFFT.std()\n    X.loc[seg_id, 'Rmax'] = realFFT.max()\n    X.loc[seg_id, 'Rmin'] = realFFT.min()\n    X.loc[seg_id, 'Imean'] = imagFFT.mean()\n    X.loc[seg_id, 'Istd'] = imagFFT.std()\n    X.loc[seg_id, 'Imax'] = imagFFT.max()\n    X.loc[seg_id, 'Imin'] = imagFFT.min()\n    X.loc[seg_id, 'Rmean_last_5000'] = realFFT[-5000:].mean()\n    X.loc[seg_id, 'Rstd__last_5000'] = realFFT[-5000:].std()\n    X.loc[seg_id, 'Rmax_last_5000'] = realFFT[-5000:].max()\n    X.loc[seg_id, 'Rmin_last_5000'] = realFFT[-5000:].min()\n    X.loc[seg_id, 'Rmean_last_15000'] = realFFT[-15000:].mean()\n    X.loc[seg_id, 'Rstd_last_15000'] = realFFT[-15000:].std()\n    X.loc[seg_id, 'Rmax_last_15000'] = realFFT[-15000:].max()\n    X.loc[seg_id, 'Rmin_last_15000'] = realFFT[-15000:].min()\n    X.loc[seg_id, 'mean_change_abs'] = np.mean(np.diff(xc))\n    X.loc[seg_id, 'mean_change_rate'] = np.mean(np.nonzero((np.diff(xc) \/ xc[:-1]))[0])\n    \n    X.loc[seg_id, 'abs_max'] = np.abs(xc).max()\n    X.loc[seg_id, 'abs_min'] = np.abs(xc).min()\n    X.loc[seg_id, 'std_first_50000'] = xc[:50000].std()\n    X.loc[seg_id, 'std_last_50000'] = xc[-50000:].std()\n    X.loc[seg_id, 'std_first_10000'] = xc[:10000].std()\n    X.loc[seg_id, 'std_last_10000'] = xc[-10000:].std()\n\n    X.loc[seg_id, 'avg_first_50000'] = xc[:50000].mean()\n    X.loc[seg_id, 'avg_last_50000'] = xc[-50000:].mean()\n    X.loc[seg_id, 'avg_first_10000'] = xc[:10000].mean()\n    X.loc[seg_id, 'avg_last_10000'] = xc[-10000:].mean()\n   \n    X.loc[seg_id, 'min_first_50000'] = xc[:50000].min()\n    X.loc[seg_id, 'min_last_50000'] = xc[-50000:].min()\n    X.loc[seg_id, 'min_first_10000'] = xc[:10000].min()\n    X.loc[seg_id, 'min_last_10000'] = xc[-10000:].min()\n    \n    X.loc[seg_id, 'max_first_50000'] = xc[:50000].max()\n    X.loc[seg_id, 'max_last_50000'] = xc[-50000:].max()\n    X.loc[seg_id, 'max_first_10000'] = xc[:10000].max()\n    X.loc[seg_id, 'max_last_10000'] = xc[-10000:].max()\n\n    #X.loc[seg_id, 'max_to_min'] = xc.max() \/ np.abs(xc.min())\n    X.loc[seg_id, 'max_to_min_diff'] = xc.max() - np.abs(xc.min())\n    \n    X.loc[seg_id, 'count_big'] = len(xc[np.abs(xc) > 500])\n    X.loc[seg_id, 'sum'] = xc.sum()\n    \n    X.loc[seg_id, 'mean_change_rate_first_50000'] = np.mean(np.nonzero((np.diff(xc[:50000]) \/ xc[:50000][:-1]))[0])\n    X.loc[seg_id, 'mean_change_rate_last_50000'] = np.mean(np.nonzero((np.diff(xc[-50000:]) \/ xc[-50000:][:-1]))[0])\n    X.loc[seg_id, 'mean_change_rate_first_10000'] = np.mean(np.nonzero((np.diff(xc[:10000]) \/ xc[:10000][:-1]))[0])\n    X.loc[seg_id, 'mean_change_rate_last_10000'] = np.mean(np.nonzero((np.diff(xc[-10000:]) \/ xc[-10000:][:-1]))[0])\n    \n    X.loc[seg_id, 'q95'] = np.quantile(xc, 0.95)\n    X.loc[seg_id, 'q99'] = np.quantile(xc, 0.99)\n    X.loc[seg_id, 'q05'] = np.quantile(xc, 0.05)\n    X.loc[seg_id, 'q01'] = np.quantile(xc, 0.01)\n\n    X.loc[seg_id, 'abs_q95'] = np.quantile(np.abs(xc), 0.95)\n    X.loc[seg_id, 'abs_q99'] = np.quantile(np.abs(xc), 0.99)\n    X.loc[seg_id, 'abs_q05'] = np.quantile(np.abs(xc), 0.05)\n    X.loc[seg_id, 'abs_q01'] = np.quantile(np.abs(xc), 0.01)\n    \n    X.loc[seg_id, 'trend'] = add_trend_feature(xc)\n    X.loc[seg_id, 'abs_trend'] = add_trend_feature(xc, abs_values=True)\n    X.loc[seg_id, 'abs_mean'] = np.abs(xc).mean()\n    X.loc[seg_id, 'abs_std'] = np.abs(xc).std()\n    \n    X.loc[seg_id, 'mad'] = xc.mad()\n    \n    X.loc[seg_id, 'Moving_average_700_mean'] = xc.rolling(window=700).mean().mean(skipna=True)\n    X.loc[seg_id, 'Moving_average_1500_mean'] = xc.rolling(window=1500).mean().mean(skipna=True)\n    X.loc[seg_id, 'Moving_average_3000_mean'] = xc.rolling(window=3000).mean().mean(skipna=True)\n    X.loc[seg_id, 'Moving_average_6000_mean'] = xc.rolling(window=6000).mean().mean(skipna=True)\n    ewma = pd.Series.ewm\n    X.loc[seg_id, 'exp_Moving_average_300_mean'] = (ewma(xc, span=300).mean()).mean(skipna=True)\n    X.loc[seg_id, 'exp_Moving_average_3000_mean'] = ewma(xc, span=3000).mean().mean(skipna=True)\n    X.loc[seg_id, 'exp_Moving_average_30000_mean'] = ewma(xc, span=6000).mean().mean(skipna=True)\n    no_of_std = 2\n    X.loc[seg_id, 'MA_700MA_std_mean'] = xc.rolling(window=700).std().mean()\n    X.loc[seg_id,'MA_700MA_BB_high_mean'] = (X.loc[seg_id, 'Moving_average_700_mean'] + no_of_std * X.loc[seg_id, 'MA_700MA_std_mean']).mean()\n    X.loc[seg_id,'MA_700MA_BB_low_mean'] = (X.loc[seg_id, 'Moving_average_700_mean'] - no_of_std * X.loc[seg_id, 'MA_700MA_std_mean']).mean()\n    X.loc[seg_id, 'MA_400MA_std_mean'] = xc.rolling(window=400).std().mean()\n    X.loc[seg_id,'MA_400MA_BB_high_mean'] = (X.loc[seg_id, 'Moving_average_700_mean'] + no_of_std * X.loc[seg_id, 'MA_400MA_std_mean']).mean()\n    X.loc[seg_id,'MA_400MA_BB_low_mean'] = (X.loc[seg_id, 'Moving_average_700_mean'] - no_of_std * X.loc[seg_id, 'MA_400MA_std_mean']).mean()\n    X.loc[seg_id, 'MA_1000MA_std_mean'] = xc.rolling(window=1000).std().mean()\n    \n    X.loc[seg_id, 'iqr'] = np.subtract(*np.percentile(xc, [75, 25]))\n    X.loc[seg_id, 'q999'] = np.quantile(xc,0.999)\n    X.loc[seg_id, 'q001'] = np.quantile(xc,0.001)\n    X.loc[seg_id, 'ave10'] = stats.trim_mean(xc, 0.1)\n\n    for windows in [5, 10, 50, 100, 500, 1000, 5000, 10000]:\n    \n        x_roll_std = xc.rolling(windows).std().dropna().values\n        x_roll_mean = xc.rolling(windows).mean().dropna().values\n        X.loc[seg_id, 'ave_roll_std_' + str(windows)] = x_roll_std.mean()\n        X.loc[seg_id, 'std_roll_std_' + str(windows)] = x_roll_std.std()\n        X.loc[seg_id, 'max_roll_std_' + str(windows)] = x_roll_std.max()\n        X.loc[seg_id, 'min_roll_std_' + str(windows)] = x_roll_std.min()\n        X.loc[seg_id, 'q01_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.01)\n        X.loc[seg_id, 'q05_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.05)\n        X.loc[seg_id, 'q95_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.95)\n        X.loc[seg_id, 'q99_roll_std_' + str(windows)] = np.quantile(x_roll_std, 0.99)\n        X.loc[seg_id, 'av_change_abs_roll_std_' + str(windows)] = np.mean(np.diff(x_roll_std))\n        X.loc[seg_id, 'av_change_rate_roll_std_' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_std) \/ x_roll_std[:-1]))[0])\n        X.loc[seg_id, 'abs_max_roll_std_' + str(windows)] = np.abs(x_roll_std).max()\n        \n        X.loc[seg_id, 'ave_roll_mean_' + str(windows)] = x_roll_mean.mean()\n        X.loc[seg_id, 'std_roll_mean_' + str(windows)] = x_roll_mean.std()\n        X.loc[seg_id, 'max_roll_mean_' + str(windows)] = x_roll_mean.max()\n        X.loc[seg_id, 'min_roll_mean_' + str(windows)] = x_roll_mean.min()\n        X.loc[seg_id, 'q01_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.01)\n        X.loc[seg_id, 'q05_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.05)\n        X.loc[seg_id, 'q95_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.95)\n        X.loc[seg_id, 'q99_roll_mean_' + str(windows)] = np.quantile(x_roll_mean, 0.99)\n        X.loc[seg_id, 'av_change_abs_roll_mean_' + str(windows)] = np.mean(np.diff(x_roll_mean))\n        X.loc[seg_id, 'av_change_rate_roll_mean_' + str(windows)] = np.mean(np.nonzero((np.diff(x_roll_mean) \/ x_roll_mean[:-1]))[0])\n        X.loc[seg_id, 'abs_max_roll_mean_' + str(windows)] = np.abs(x_roll_mean).max()  \n    ","540c01cd":"# iterate over all segments\nfor seg_id in tqdm_notebook(range(segments)):\n    seg = train_df.iloc[seg_id*rows:seg_id*rows+rows]\n    create_features(seg_id, seg, train_X)\n    # the y value is the last entry in the time to failure in the segment\n    train_y.loc[seg_id, 'time_to_failure'] = seg['time_to_failure'].values[-1]","e1e76550":"train_X_save = train_X.copy\ntrain_y_save = train_y.copy\ntrain_y.head(5)\n","53dfba7d":"# We will not train on the segments with a quake, because there are likely outliers\ntrain_y_quake = np.nonzero(np.diff(train_y.time_to_failure) > 0)[0] + 1\nprint(len(train_y_quake))\nprint (len(train_y))\n\nfor idx in train_y_quake: \n    train_y.drop([idx],inplace=True)\n    train_X.drop([idx],inplace = True)\n#np.abs(train_X.corrwith(train_y)).sort_values(ascending=False).head(12)\ntrain_X.to_csv('train_features.csv', index=False)\ntrain_y.to_csv('train_y.csv', index=False)\n","faea9252":"train_X.shape, train_y.shape\n","e2cd6e6f":"train_X.head(), train_y.head()","ff2d358c":"scaler = StandardScaler()\nscaler.fit(train_X)\nscaled_train_X = pd.DataFrame(scaler.transform(train_X), columns=train_X.columns)\n#scaled_train_X = train_X","aa623abc":"scaled_train_X.head(10)","a693c4ba":"submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='seg_id')\ntest_X = pd.DataFrame(columns=train_X.columns, dtype=np.float64, index=submission.index)","3cd9e903":"submission.shape, test_X.shape","a99bce2f":"for seg_id in tqdm_notebook(test_X.index):\n    seg = pd.read_csv('..\/input\/test\/' + seg_id + '.csv')\n    # convert to mean 0 of the training dataset \n    # seg_mean = np.mean(seg.acoustic_data)\n    seg.acoustic_data = seg.acoustic_data - mean_acoustic\n    create_features(seg_id, seg, test_X)","4b3831b0":"# save before scaling\ntest_X.to_csv('test_features.csv', index=False)","4249b048":"scaled_test_X = pd.DataFrame(scaler.transform(test_X), columns=test_X.columns)\n#scaled_test_X = test_X\nscaled_test_X.values[1117]","2343cf1e":"scaled_test_X.shape","0fa2a30a":"scaled_test_X.tail(10)","9300619b":"n_fold = 5\ndef mae_cv (model):\n    folds = KFold(n_splits=n_fold, shuffle=True, random_state=42).get_n_splits(scaled_train_X.values)\n    mae = -cross_val_score (model, scaled_train_X.values, train_y, scoring=\"neg_mean_absolute_error\",\n                           verbose=0,\n                           cv=folds)\n    return mae\n","7f6954d7":"%%time\n\nlgb_params = {'num_leaves': 51,\n         'min_data_in_leaf': 10, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.001,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.91,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.91,\n         \"bagging_seed\": 42,\n         \"metric\": 'mae',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1,\n         \"nthread\": -1,\n         \"random_state\": 42}\n\n\nlgb_model = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.01, n_estimators=720,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11, n_jobs = -1)\n\nscore = mae_cv(lgb_model)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\nlgb_model","19f94f75":"lgb_gamma_model = lgb.LGBMRegressor(objective='gamma',num_leaves=5,\n                              learning_rate=0.01, n_estimators=720,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11, n_jobs = -1)\n\nscore = mae_cv(lgb_gamma_model)\nprint(\"LGBM - gamma score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\nlgb_gamma_model","cd96d2c6":"%%time \nxgb_params = {'eta': 0.03,\n              'max_depth': 9,\n              'subsample': 0.85,\n              'objective': 'reg:linear',\n              'eval_metric': 'mae',\n              'silent': True,\n              'nthread': 4}\n    \nxgb_model = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1, eval_metric = 'mae',)\n\nscore = mae_cv(xgb_model)\nprint(\"XGB score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\nxgb_model\n\n#    xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, \n#                          verbose_eval=500, params=xgb_params)","c8b47998":"%%time\nrf_model = RandomForestRegressor(n_estimators=120, n_jobs=-1, min_samples_leaf=1, \n                           max_features = \"auto\",max_depth=15, )\nscore = mae_cv(rf_model)\nprint(\"Random Forest score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\nrf_model","b6e5584b":"%%time\nparams = {'loss_function':'MAE',}\ncat_model = CatBoostRegressor(iterations=1000,  eval_metric='MAE', verbose=False, **params)\n\nscore = mae_cv(cat_model)\nprint(\"Cat Boost score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\ncat_model","8226eb0f":"%%time\nKRR_model = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nscore = mae_cv(KRR_model)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\nprint (score)\nKRR_model","10c59ba7":"%%time\n#ENet_model = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=0.9, random_state=3,max_iter=5000))\nENet_model = ElasticNet(alpha=0.0005, l1_ratio=0.9, random_state=3,max_iter=5000)\nscore = mae_cv(ENet_model)\nprint(\"Elastic Net score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\nENet_model","45f5a150":"%%time\nlasso_model = Lasso(alpha =0.0005, random_state=1)\nscore = mae_cv(lasso_model)\nprint(\"Lasso score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\nlasso_model","bfef79b3":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        \n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","952fa6b7":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        print (type(X))\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        print (KFold)\n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X.iloc[train_index], y.iloc[train_index])\n                y_pred = instance.predict(X.iloc[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","d75564ea":"class StackingCVRegressorRetrained(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, regressors, meta_regressor, n_folds=5, use_features_in_secondary=False):\n        self.regressors = regressors\n        self.meta_regressor = meta_regressor\n        self.n_folds = n_folds\n        self.use_features_in_secondary = use_features_in_secondary\n\n    def fit(self, X, y):\n        self.regr_ = [clone(x) for x in self.regressors]\n        self.meta_regr_ = clone(self.meta_regressor)\n\n        kfold = KFold(n_splits=self.n_folds, shuffle=True)\n\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.regressors)))\n\n        # Create out-of-fold predictions for training meta-model\n        for i, regr in enumerate(self.regr_):\n            for train_idx, holdout_idx in kfold.split(X, y):\n                instance = clone(regr)\n                instance.fit(X[train_idx], y[train_idx])\n                out_of_fold_predictions[holdout_idx, i] = instance.predict(X[holdout_idx])\n\n        # Train meta-model\n        if self.use_features_in_secondary:\n            self.meta_regr_.fit(np.hstack((X, out_of_fold_predictions)), y)\n        else:\n            self.meta_regr_.fit(out_of_fold_predictions, y)\n        \n        # Retrain base models on all data\n        for regr in self.regr_:\n            regr.fit(X, y)\n\n        return self\n\n    def predict(self, X):\n        meta_features = np.column_stack([\n            regr.predict(X) for regr in self.regr_\n        ])\n\n        if self.use_features_in_secondary:\n            return self.meta_regr_.predict(np.hstack((X, meta_features)))\n        else:\n            return self.meta_regr_.predict(meta_features)\n","0c631d71":"%%time\n#averaged_models = AveragingModels(models = (rf_model, xgb_model, KRR_model, lgb_model, ENet_model, cat_model, lasso_model))\n#averaged_models = AveragingModels(models = (rf_model, lgb_model,  cat_model, lasso_model))\naveraged_models = AveragingModels(models = (rf_model,cat_model))\n\nscore =mae_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n#averaged_models.fit (scaled_train_X.values, train_y)","a185fe70":"averaged_models.fit (scaled_train_X.values, train_y)\naveraged2_train_predict = averaged_models.predict(scaled_train_X.values)\nprint(mean_absolute_error(train_y, averaged2_train_predict))\n","1d2ee984":"\n\naveraged_prediction = np.zeros(len(scaled_test_X))\naveraged_prediction += averaged_models.predict(scaled_test_X.values)\naveraged_prediction","6948838f":"%%time\nstacked_predict = StackingAveragedModels(base_models =(rf_model, xgb_model, lgb_model, cat_model,ENet_model), \n                                          meta_model =lasso_model) \nstacked_predict.fit(scaled_train_X, train_y)\n","aa721f77":"stacked_train_pred = stacked_predict.predict(scaled_train_X)\n\nprint(mean_absolute_error(train_y, stacked_train_pred))\n\nstacked_prediction = np.zeros(len(scaled_test_X))\nstacked_prediction += stacked_predict.predict(scaled_test_X)**1.0\nstacked_prediction[0:4]\n","a895b98f":"submission.time_to_failure = averaged_prediction\nsubmission.to_csv('submissionV30_averaged_cat_rf.csv',index=True)\nsubmission.time_to_failure = stacked_prediction\nsubmission.to_csv('submissionV30_stacked.csv',index=True)","75ed0311":"Let's check the result. We plot the shape and the head of train_X and train_y.","956acd2b":"### Random Forest","a43ef9e7":"Let's check the obtained dataframe.","cd1f8f14":"### Cat Boost\n\n","d6a137b0":"# <a id='1'>Introduction<\/a>  \n\nMy goal with this kernel is to have an end-to-end model which encorporates:\n\n* Extensible feature engineering\n* Ability to try and measure many regression models\n* Fexlibility of both a blended and stacked model production to produce testable ensemble models\n\nThis kernel is based on a fork from Gabriel Preda's excellent kernel, 'LANLEarthquake EDA and Prediction.\" I have left most of his EDA and feature engineering in place, while adding a few features of my own. The modeling portion is based partially on his models. In addition, I use ensembling approaches from Serigne's excellent model in the House Price Advanced Regression Techniques competition, https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard.\n\nThis model produces 1.488 leader board entry with the averaged model, using RF, lgb, cat, and lasso.\n\n\n## Simulated earthquake experiment\n\nThis introduction is from Gabrial Prada's kernel:\n\n\"The data are from an experiment conducted on rock in a double direct shear geometry subjected to bi-axial loading, a classic laboratory earthquake model.\n\nTwo fault gouge layers are sheared simultaneously while subjected to a constant normal load and a prescribed shear velocity. The laboratory faults fail in repetitive cycles of stick and slip that is meant to mimic the cycle of loading and failure on tectonic faults. While the experiment is considerably simpler than a fault in Earth, it shares many physical characteristics. \n\nLos Alamos' initial work showed that the prediction of laboratory earthquakes from continuous seismic data is possible in the case of quasi-periodic laboratory seismic cycles.\"   \n\n## Competition \n\nIn this competition, the team has provided a much more challenging dataset with considerably more aperiodic earthquake failures.  \nObjective of the competition is to predict the failures for each test set.  \n\n## Kernel\n\nThis solution is based on a fork from Gabriel Preda's excellent kernel, 'LANLEarthquake EDA and Prediction.\" His solution uses  Andrew's Data Munging plus some inspiration from Scirpus's [Kernel](https:\/\/www.kaggle.com\/scirpus\/andrews-script-plus-a-genetic-program-model\/).\n\nIt also includes the elegant stacking model from Serigne's regression model (see reference above.)\n\nThis kernel extends the existing model by\n\n* normalizing the values to a mean of zero\n* using log values to remove outliers\n* removal of segments in training which contain the quake\n* additional prediction models\n* combining results from multiple models through various ensemble techniques\n\n","a23bef9a":"### Lasso","3378bf17":"* Let's check the data imported. We normalize the data to a 0 mean value.","f0b64937":"The plot shows only 2% of the full data. \nThe acoustic data shows complex oscilations with variable amplitude. Just before each failure there is an increase in the amplitude of the acoustic data. Because the data is all positive and log, the peaks are not as severe.\n\nWe see that large amplitudes are also obtained at different moments in time (for example about the mid-time between two succesive failures).  \n\nLet's plot as well the first two earthquakes of the data.","4dcb93d6":"We scale the test data.","c812e453":"# <a id='3'>Data exploration<\/a>  \n\nThe dimmension of the data is large, in excess of 600 millions rows of data.  \nThe two columns in the train dataset have the following meaning:   \n*  accoustic_data: is the accoustic signal measured in the laboratory experiment;  \n* time to failure: this gives the time until a failure will occurs.\n\nLet's plot 2% of the data. For this we will sample every 50 points of data.  ","e8bf4632":"<h1><center><font size=\"6\">LANL Earthquake EDA and Ensemble Prediction<\/font><\/center><\/h1>\n\n<h2><center><font size=\"4\">Dataset used: LANL Earthquake Prediction<\/font><\/center><\/h2>\n\n<img src=\"https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/LANL\/nik-shuliahin-585307-unsplash.jpg\" width=\"600\"><\/img>\n\n<br>\n\n# <a id='0'>Content<\/a>\n\n- <a href='#1'>Introduction<\/a>  \n- <a href='#2'>Prepare the data analysis<\/a>  \n- <a href='#3'>Data exploration<\/a>   \n- <a href='#4'>Feature engineering<\/a>\n- <a href='#5'>Model<\/a>\n- <a href='#6'>Submission<\/a>  \n- <a href='#7'>References<\/a>","3c3a7c85":"# <a id='6'>Submission<\/a>  \n\nWe set the predicted time to failure in the submission file.","1574cc80":"# <a id='7'>References<\/a>  \n\n[1] Fast Fourier Transform, https:\/\/en.wikipedia.org\/wiki\/Fast_Fourier_transform   \n[2] Shifting aperture, in Neural network for inverse mapping in eddy current testing, https:\/\/www.researchgate.net\/publication\/3839126_Neural_network_for_inverse_mapping_in_eddy_current_testing   \n[3] Andrews Script plus a Genetic Program Model, https:\/\/www.kaggle.com\/scirpus\/andrews-script-plus-a-genetic-program-model\/","0dfc6664":"### LGB Model\nWe define the model parameters and the definition for the first model, an LGB regression.","2fccbe9e":"## Load the data\n\nLet's see first what files we have in input directory.","ca5126b3":"### XGB Model","f631fd61":"# <a id='4'>Features engineering<\/a>  \n\nThe test segments are 150,000 each.   \nWe split the train data in segments of the same dimmension with the test sets.\n\nWe will create additional aggregation features, calculated on the segments. \n","ca7f36b2":"Let's define some computation helper functions.","f8e80848":"\n# <a id='5'>Model<\/a>  \n\nLet's prepare the model. First we define a validation function to evaluate model performance.\n\nWe next define a set of models which can later be blended and stacked.\n\n\n## Define Validation\n\nFirst define a cross validation measurement routine.","8c85ad57":"On this zoomed-in-time plot we can see that actually the large oscilation before the failure is not quite in the last moment. There are also trains of intense oscilations preceeding the large one and also some oscilations with smaller peaks after the large one. Then, after some minor oscilations, the failure occurs.","c7b48c2f":"## Process train file\n\nNow let's calculate the aggregated functions for train set.","d4aa4ef1":"\n\nLet's load the train file.","1dea9c88":"Let's check the shape of the submission and test_X datasets.","3f25dfdb":"We have two files in the **input** directory and another directory, with the **test** data.  \n\nLet's see how many files are in **test** folder.","7f48fde9":"We scale the data.","e16c6c8b":"# <a id='1'>Prepare the data analysis<\/a>\n\n## Load packages\n\nHere we define the packages for data manipulation, feature engineering and model training.","3b64e82f":"## Process test data\n\nWe apply the same processing done for the training data to the test data.\n\nWe read the submission file and prepare the test file.","425ac6c9":"### Kernel Ridge","274c15ab":"### Elastic Net\n"}}