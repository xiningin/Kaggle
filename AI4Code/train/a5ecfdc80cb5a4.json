{"cell_type":{"000b637b":"code","c2fb3066":"code","b18b1458":"code","83c1416f":"code","6da070c6":"markdown","e0ff6c9f":"markdown"},"source":{"000b637b":"import pandas as pd\ndf = pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-severity-rating\/sample_submission.csv\")\ndf","c2fb3066":"df['score'] = df.reset_index()['index'].sample(frac=1, random_state=2023).values\n","b18b1458":"df","83c1416f":"df.to_csv(\"submission.csv\", index=False)","6da070c6":"During evaluation, we take two comments and take their relative score to calculate average agreement. Thus, the average agreement (AA score) for random ranking should be close to 0.5. This is because if we take two texts A and B from the list, the probability of score of A < score of B will be 0.5 (score for both elements are random).\n\nIf the test data is skewed towards few records, then the AA score will deviate from the 0.5 for random ranks. For example: for data like\n```\nmore_toxic, less_toxic\nA, B\nA, B\nA, C\n```\n\nIf scores are given like this: A:1, B:2, C:3; then AA metric will be 0.  ","e0ff6c9f":"# \u2623\ufe0f Jigsaw - Random rank"}}