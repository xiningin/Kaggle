{"cell_type":{"46d7f37c":"code","aa891dc6":"code","698f99cb":"code","77e09716":"code","ddab0f3d":"code","fbb2a54b":"code","2698212d":"code","7902e518":"code","2c15628a":"code","79f559a3":"code","374c7218":"code","b5dc0381":"code","aac1c8a9":"code","81bd5b47":"code","1386845a":"code","4bdb9bf7":"code","13ecdaa2":"code","78e94e9a":"code","1586df16":"code","7e0c371b":"code","e91cddb0":"code","7dbfd0de":"code","7307ac13":"code","d9b2443b":"code","7cccf840":"code","76713993":"code","891b6e5d":"code","4c8c6bbe":"code","e8ceecfa":"code","15cfa966":"code","2d32e3b3":"markdown","cdab5acc":"markdown","945e5d43":"markdown","fd39a9b8":"markdown","2af559a3":"markdown","5e1ab661":"markdown","e4d5e1c3":"markdown","5ff0ddfd":"markdown","f97e77c0":"markdown","6929cea2":"markdown","0a96e6ce":"markdown","2f144078":"markdown","44659505":"markdown","10f9b266":"markdown","e9c94d35":"markdown","14e707b7":"markdown","4fbfeac1":"markdown","55835ddf":"markdown"},"source":{"46d7f37c":"pip install bigartm","aa891dc6":"import artm\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set_style(\"whitegrid\", {'axes.grid' : False})\nimport numpy as np\nimport pandas as pd\nfrom IPython.core.display import display, HTML","698f99cb":"batch_vectorizer = artm.BatchVectorizer(data_path=\"..\/input\/lectures\/lectures.txt\", data_format=\"vowpal_wabbit\", target_folder=\"lectures\", \n                                       batch_size=100)","77e09716":"T = 30   # number of topics\ntopic_names=[\"sbj\"+str(i) for i in range(T-1)]+[\"bcg\"]\nmodel = artm.ARTM(num_topics=T, topic_names=topic_names, num_processors=2, class_ids={'text':1, 'author':1},\n                  reuse_theta=True, cache_theta=True)","ddab0f3d":"np.random.seed(1)\ndictionary = artm.Dictionary('dict')\ndictionary.gather(batch_vectorizer.data_path)\nmodel.initialize(dictionary=dictionary)","fbb2a54b":"#Adding score\nmodel.scores.add(artm.TopTokensScore(name='top_tokens_score1', class_id='text', num_tokens=15))\nmodel.scores.add(artm.TopTokensScore(name='top_tokens_score2', class_id='author', num_tokens=15))","2698212d":"model.regularizers.add(artm.SmoothSparsePhiRegularizer(tau=1e5, class_ids='text', dictionary='dict', topic_names='bcg'))","7902e518":"model.num_document_passes = 1\nmodel.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=30)","2c15628a":"topic_names_cleaned = topic_names[:-1]","79f559a3":"model.regularizers.add(artm.SmoothSparsePhiRegularizer(tau=-1e5, class_ids='text', dictionary='dict',\n                                                       topic_names=topic_names_cleaned))","374c7218":"# more 15 passes\nmodel.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=15)\n","b5dc0381":"#top words by each topic\ntokens = model.score_tracker['top_tokens_score1'].last_tokens\nfor topic_name in model.topic_names:\n    print(topic_name + ': '),\n    for word in tokens[topic_name]:    \n        print(word),\n    print()\n","aac1c8a9":"#authors by topic\nauthors = model.score_tracker['top_tokens_score2'].last_tokens\nfor topic_name in model.topic_names:\n    print(topic_name + ': ')\n    for author in authors[topic_name]:    \n        print(author)\n    print()","81bd5b47":"sbj_topic_labels = []   # list of topic themes\nfor topic_name in model.topic_names[:29]:\n    sbj_topic_labels.append(tokens[topic_name][0])\n\ntopic_labels = sbj_topic_labels + [u\"\u0424\u043e\u043d\u043e\u0432\u0430\u044f \u0442\u0435\u043c\u0430\"]","1386845a":"topic_labels","4bdb9bf7":"model.theta_columns_naming = \"title\" \n\ntheta = model.get_theta()\nprint('Theta shape: %s' % str(theta.shape))\nphi_a = model.get_phi(class_ids='author')\nprint('Phi_a shape: %s' % str(phi_a.shape))","13ecdaa2":"theta.iloc[:,:100]","78e94e9a":"plt.figure(figsize=(20,10))\nplt.title('Theta matrix for the first 100 documents')\nsns.heatmap(theta.iloc[:,:100], cmap='YlGnBu', xticklabels=False)\nplt.show();","1586df16":"prob_theme_data = [np.sum(theta.iloc[i]) for i in range(theta.shape[0])]\nprob_theme_data_normed = prob_theme_data \/ np.sum(prob_theme_data)\nprob_theme = pd.DataFrame(data=prob_theme_data_normed, index=topic_labels, columns=['prob'])\nprob_theme","7e0c371b":"prob_theme_max = prob_theme\nprob_theme_min = prob_theme\n\nprint('Max 5 probabilities:')\nfor i in range(5):\n    max_value = prob_theme_max.max()[0]\n    print(prob_theme_max[prob_theme_max.values == max_value].index[0])\n    prob_theme_max = prob_theme_max[prob_theme_max.values != max_value]\n\nprint('\\nMin 3 probabilities:')\nfor i in range(3):\n    min_value = prob_theme_min.min()[0]\n    print(prob_theme_min[prob_theme_min.values == min_value].index[0])\n    prob_theme_min = prob_theme_min[prob_theme_min.values != min_value]","e91cddb0":"plt.figure(figsize=(20,10))\nplt.title('Theta matrix for the first 100 documents')\nsns.heatmap(phi_a.iloc[:100], cmap='YlGnBu', yticklabels=False)\nplt.show();","7dbfd0de":"phi_a","7307ac13":"for i in range(phi_a.shape[0]):\n    num_valuble_topics = 0\n    for val in phi_a.iloc[i]:\n        if val > 0.01:\n            num_valuble_topics += 1\n    if num_valuble_topics >= 3:\n        print(i, phi_a.index[i])","d9b2443b":"from sklearn.manifold import MDS\nfrom sklearn.metrics import pairwise_distances","7cccf840":"prob_theme_author = np.empty(phi_a.shape)\nfor i in range(prob_theme_author.shape[0]):\n    for j in range(prob_theme_author.shape[1]):\n        prob_theme_author[i,j] = phi_a.iloc[i,j] * prob_theme.iloc[j,:] \/ np.sum(phi_a.iloc[i,:] * prob_theme.prob.values)","76713993":"similarities = pairwise_distances(prob_theme_author, metric='cosine')\nmds = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\npos = mds.fit_transform(similarities)","891b6e5d":"plt.figure(figsize=(10,5))\nplt.scatter(pos[:,0], pos[:,1])\nplt.show();","4c8c6bbe":"import matplotlib.cm as cm\ncolors = cm.rainbow(np.linspace(0, 1, T)) # colors for themes\n\nmax_theme_prob_for_colors = [np.argmax(author) for author in prob_theme_author]\nplt.figure(figsize=(15,10))\nplt.axis('off')\nplt.scatter(pos[:,0], pos[:,1], s=100, c=colors[max_theme_prob_for_colors])\nfor i, author in enumerate(phi_a.index):\n        plt.annotate(author, pos[i])\nplt.savefig('authors_map.pdf', dpi=200, format='pdf')\nplt.show();","e8ceecfa":"prob_doc_theme = theta.values \/ np.array([np.sum(theme) for theme in theta.values])[:, np.newaxis]\nprob_doc_theme_sorted_indices = prob_doc_theme.argsort(axis=1)[:,::-1]\nprob_doc_theme_sorted_indices","15cfa966":"for i, theme in enumerate(topic_labels):\n    print(\"<h3>%s<\/h3>\" % theme)\n    for j in range(10):\n        print(tokens[model.topic_names[i]][j]),\n    print('')\n    for k in range(10):\n        print(theta.columns[prob_doc_theme_sorted_indices[i,k]])","2d32e3b3":"**Model building**\n\nHere the model is built in two stages: first, the smoothing regularizer of the background theme is added and the parameters of the model are adjusted, then the thinning regularizer of subject themes is added and several more iterations are performed. So we will be able to get the most pure subject topics from background words. Smoothing and sparse regularizers are specified by the same class artm.SmoothSparsePhiRegularizer: if the tau coefficient is positive, then the regularizer will be smoothing, if negative, it will be sparse.","cdab5acc":"\nAuthors significant in at least 3 topics: \n\nMost of the authors are significant in one topic, which is logical.","945e5d43":"Not a very large number of authors correspond to each topic - the matrix is rather sparse. In addition, some topics have a dominant author \ud835\udc4e with a high probability \ud835\udc5d (\ud835\udc4e | \ud835\udc61) - this author has recorded the most lectures on the topic. We will assume that the author \ud835\udc4e is significant in the topic if \ud835\udc5d (\ud835\udc4e | \ud835\udc61)> 0.01. For each author, let's count how many topics he is significant. We will find record-breaking authors who are significant (and therefore lectured) in> = 3 topics.","fd39a9b8":"**Creation of a simple thematic navigator for Post Science**\n\nThe topic navigator will show a list of words for each topic, as well as a list of relevant documents. Need distributions \ud835\udc5d(\ud835\udc51|\ud835\udc61). By Bayes' formula \ud835\udc5d(\ud835\udc51|\ud835\udc61) = \ud835\udc5d(\ud835\udc61|\ud835\udc51)\ud835\udc5d(\ud835\udc51)\u2211\ud835\udc51\u2032\ud835\udc5d(\ud835\udc61|\ud835\udc51\u2032)\ud835\udc5d(\ud835\udc51\u2032), but since considering documents to be equiprobable, it suffices to split each row \u0398 by its amount to estimate the distribution. Sorting the matrix \ud835\udc5d(\ud835\udc51|\ud835\udc61) descending \ud835\udc5d(\ud835\udc51|\ud835\udc61) in each topic (that is, line by line).","2af559a3":"In a loop, for each topic, displaying its title, in the next line - the top 10 words of the topic, then in the form of a list of links to the 10 most relevant documents (by \ud835\udc5d (\ud835\udc51 | \ud835\udc61)) to the topic.","5e1ab661":"**Reading the data**\n\nHere an object of the artm.BatchVectorizer class is created, which will refer to the directory with data packages (batches). The batch size for small collections is not important, you can specify any.","e4d5e1c3":"This notebook shows how to carry out thematic modeling to a collection of text recordings of video lectures downloaded from the Postnauka website. The model will be rendered and a prototype of the thematic collection navigator will be created. The collection contains 1728 documents, the size of the dictionary is 38467 words. The words are lemmatized, that is, brought to their initial form, using the mystem program, the collection is saved in the vowpal wabbit format. In each line, up to the first line, information about the document is written (link to the page with the lecture), after the first line, a description of the document follows. Two modalities are used - text (\"text\") and author (\"author\"); each document has one author. To complete the task, you will need the BigARTM library.","5ff0ddfd":"**Building a thematic map of authors**\n\nIn fact, the thematic clusters of authors are written in the matrix \u03a6 corresponding to the modality of the authors. For any author, we can compose his thematic circle - authors who understand the same topic as this one.\nDrawing up a map of the authors' proximity according to the topic of their research. To do this, we will apply the MDS dimension reduction method to the thematic profiles of the authors.\nTo get the author's thematic profile, the distribution \ud835\udc5d (\ud835\udc61 | \ud835\udc4e), you need to use the Bayes formula: \ud835\udc5d (\ud835\udc61 | \ud835\udc4e) = \ud835\udc5d (\ud835\udc4e | \ud835\udc61) \ud835\udc5d (\ud835\udc61) \u2211\ud835\udc61\u2032\ud835\udc5d (\ud835\udc4e | \ud835\udc61 \u2032) \ud835\udc5d (\ud835\udc61 \u2032 ). You have all the necessary values \u200b\u200bfor this and are recorded in the phi and pt variables.\nUsing the cosine metric (it is well suited for finding the distances between vectors with a fixed sum of components).","f97e77c0":"We find the 5 most common and 3 least covered topics in the collection (the largest and smallest \ud835\udc5d (\ud835\udc61), respectively), not counting the background and general scientific ones.","6929cea2":"The last topic \"bcg\" should contain common words.\nAn important step in working with a topic model when it comes to visualizing or creating a topic navigator is topic naming. You can understand what each topic is about by looking at the list of its top words. For example, the topic\n> particle interaction physics quark symmetry elementary neutrino standard matter proton boson charge mass accelerator weak\n\ncan be called \"Particle Physics\".\n\nNext naming 29 subject topics. Name it the first noun that appears in it, although this approach will make the navigator less informative. Making a list of 29 lines of topic names and write it to the sbj_topic_labels variable. The topic_labels variable will store the names of all topics, including the background one.","0a96e6ce":"**Interpreting Topics**\n\nUsing the created scores, it's displayed tops of words and tops of authors in topics. It is most convenient to display the top words of each topic on a new line, indicating the topic name at the beginning of the line, and similarly with the authors.","2f144078":"**Conclusion**\n\nWe can experiment to apply the Serendip developed at the University of Wisconsin-Madison to the constructed model. This library allows to characterize themes as fully as possible and is written for the python language.","44659505":"It should turn out that some groups of authors form clumps that can be considered thematic groups of authors. Coloring the dots as follows: for each author, choosing the most likely topic for him (max \ud835\udc61\ud835\udc5d(\ud835\udc61|\ud835\udc4e), and assign a color to each topic. In addition, adding the names of the authors to the map, this can be done in a loop over all points using the function plt.annotate, specifying the point label as the first argument and its coordinates in the xy argument.","10f9b266":"**Topic analysis**\n\nNext, we will work with the distributions of topics in documents (matrix \u0398) and authors in topics (one of two matrices \u03a6, corresponding to the modality of authors). Using the get_phi and get_theta methods of the model, we create variables containing these two matrices. Displaying the forms of both matrices in order to understand on which axes the themes stand.","e9c94d35":"**Create the navigator directly in the jupiter notebook**","14e707b7":"Visualizing a fragment of the matrix \u0398 - the first 100 documents, this is the easiest way to visually assess how topics are distributed in documents.","4fbfeac1":"**Initializing the model** \n\nAn object of class artm.Model is created with 30 themes, the names of the themes given below, and unit weights of both modalities. The number of themes is not very large, so that it is more convenient to work with themes. More themes can be built on this collection, then they will be more narrowly specialized.","55835ddf":"You should see that the background theme has a high likelihood in almost every document, which makes sense. In addition, there is another topic that appears more frequently in documents. Apparently, this topic contains a lot of words on science in general, and every document (video) in our collection is related to science. You can (optionally) name this topic \"Science\".\n\nApart from these two topics, background and general scientific, each document is characterized by a small number of other topics.\n\nWe estimate $p(t)$ - the share of each topic in the entire collection. Using the formula of total probability, these values should be calculated as $p(t) = \\sum_d p(t|d) p(d)$. According to the probabilistic model, $p(d)$ is proportional to the length of the document d. Let's do it easier: we will assume that all documents are equally probable. Then you can estimate $p(t)$ by summing $p(t|d)$ over all documents, and then dividing the resulting vector by its sum."}}