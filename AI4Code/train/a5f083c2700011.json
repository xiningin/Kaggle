{"cell_type":{"b623995d":"code","480e4482":"code","0abb081d":"code","d8c77bf5":"code","cdaa422c":"code","75f23242":"code","761716d2":"code","37879f48":"code","60ccea29":"code","d454f124":"code","43a70c50":"code","71bb437f":"code","9bf56e0f":"code","c095aacc":"code","fa40f6e1":"code","9a08ff2e":"code","7453ef2e":"code","50fa1f15":"code","992582c9":"code","275b1a8b":"code","deaee16f":"code","5523f705":"code","cba1c13a":"code","1eebd9d7":"code","79f4968a":"code","6cbecb7d":"code","dc32af56":"code","a3785b36":"markdown","288c6380":"markdown","65bee4ae":"markdown"},"source":{"b623995d":"import torch\nimport pandas as pd\nimport numpy as np\n#torch\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.optim import Adam, lr_scheduler\n\nimport xgboost as xgb\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\n\nimport transformers\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer,AutoModel\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.backends.cudnn.benchmark = True\n\nimport warnings\nwarnings.simplefilter('ignore')","480e4482":"class testDataset:\n    def __init__(self, excerpt, tokenizer, max_len):\n        self.excerpt = excerpt\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.excerpt)\n\n    def __getitem__(self, item):\n        text = str(self.excerpt[item])\n        inputs = self.tokenizer(\n            text, \n            max_length=self.max_len, \n            padding=\"max_length\", \n            truncation=True\n        )\n\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n\n        return {\n            \"input_ids\": torch.tensor(ids, dtype=torch.long),\n            \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n        }","0abb081d":"class AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim, num_targets):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n        self._init_params()    \n    def _init_params(self):\n        nn.init.xavier_normal_(self.W.weight)\n        nn.init.constant_(self.W.bias, 0)\n        nn.init.xavier_normal_(self.V.weight)\n        nn.init.constant_(self.V.bias, 0)\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n\n        score = self.V(att)\n\n        attention_weights = torch.softmax(score, dim=1)\n\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n\nclass dModel(nn.Module):\n    def __init__(self,modelpath, leng):\n        super(dModel, self).__init__()\n        self.roberta = transformers.AutoModel.from_pretrained(modelpath)#,num_labels=1)\n        self.head = AttentionHead(leng,leng,1)\n        self.fc = nn.Linear(leng, 1)\n        self.dropout = nn.Dropout(p=0.1)\n        self.dropouts = nn.ModuleList([\n                nn.Dropout(p=0.1) for _ in range(5)\n            ])\n        self._init_params()    \n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n    def forward(self, **x):\n        x = self.roberta(**x)[0]\n        x = self.head(x)\n        for i, dropout in enumerate(self.dropouts):\n            if i == 0:\n                logits = self.fc(dropout(x))\n            else:\n                logits += self.fc(dropout(x))\n        \n        logits \/= len(self.dropouts)\n        return logits","d8c77bf5":"def generate_embeddings(model_path,weight_path, max_len,leng):\n    model = dModel(model_path,leng)\n    model.load_state_dict(torch.load(weight_path,map_location=device))\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    model = model.to(device)\n    model.eval()\n    df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\n    dataset = testDataset(excerpt=df.excerpt.values, tokenizer=tokenizer, max_len=max_len)\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=16, num_workers=2, pin_memory=True, shuffle=False, drop_last=False\n    )\n    final_output = np.empty((0,leng))\n\n    for b_idx, (data) in enumerate(data_loader):\n        with torch.no_grad():\n            for key, value in data.items():\n                data[key] = value.to(device)\n            x = model.roberta(**data)[0]\n            output = model.head(x)\n            final_output = np.append(final_output,output.clone().detach().cpu().numpy(),axis=0)\n    \n    torch.cuda.empty_cache()\n    return final_output\n\ndef generate_predictions_md(model_path,weight_path, max_len,leng):\n    #model = AutoModelForSequenceClassification.from_pretrained(model_path,num_labels=1)\n    model = dModel(model_path,leng)\n    model.load_state_dict(torch.load(weight_path,map_location=device))\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    model = model.to(device)\n    model.eval()\n    df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\n    dataset = testDataset(excerpt=df.excerpt.values, tokenizer=tokenizer, max_len=max_len)\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=8, num_workers=2, pin_memory=True, shuffle=False, drop_last=False\n    )\n    final_output = []\n\n    for b_idx, data in enumerate(data_loader):\n        with torch.no_grad():\n            for key, value in data.items():\n                data[key] = value.to(device)\n            output = model(**data)\n            #output = output.logits.squeeze(-1).detach().cpu().numpy()\n            output = output.squeeze(-1).detach().cpu().numpy()\n            final_output = np.append(final_output,output)\n    \n    torch.cuda.empty_cache()\n    return final_output","cdaa422c":"#0:LB 0.483 to 0.487 to 0.486\n\n#1:LB 0.487 to 0.489 to 0.488\n\n#2:LB 0.489 to 0.489\u3000to 0.489\n\n#3:LB 0.498 to 0.492 to 0.485\n\n#4:LB 0.498 to 0.492 to 0.491\n\n#5:LB 0.497 to 0.504 \n\n#6:LB 0.500 to 0.509 \n\nmodel_paths = [\"..\/input\/roberta-transformers-pytorch\/roberta-large\/\",\n                   \"..\/input\/roberta-transformers-pytorch\/roberta-large\/\",\n                   #\"..\/input\/roberta-transformers-pytorch\/roberta-large\/\",\n                   #\"..\/input\/roberta-transformers-pytorch\/distilroberta-base\/\",\n                   #\"..\/input\/roberta-transformers-pytorch\/roberta-base\/\",\n                   #\"..\/input\/roberta-transformers-pytorch\/roberta-large\/\",\n                   #\"..\/input\/roberta-transformers-pytorch\/roberta-base\/\",\n                   #\"..\/input\/roberta-transformers-pytorch\/roberta-base\/\"\n              ]\nweight_paths = [\"..\/input\/commonlitreadability-weight\/simplelarge_0.469_fold0_mlen250.pt\",\n                    \"..\/input\/commonlitreadability-weight\/simplelarge_0.455_fold1_mlen250.pt\",\n                    #\"..\/input\/commonlitreadability-weight\/simplelarge_0.464_fold3_mlen250.pt\",\n                    #\"..\/input\/commonlitreadability-weight\/bestmodel_0.469_fold1_mlen250.pt\",\n                    #\"..\/input\/commonlitreadability-weight\/bestbase_0.443_fold4_mlen250.pt\",\n                    #\"..\/input\/commonlitreadability-weight\/multipledropoutrobertalarge_LB0.500_mlen250.pt\",\n                    #\"..\/input\/commonlitreadability-weight\/basebestmodel_0.428_fold4_mlen250.pt\",\n                    #\"..\/input\/commonlitreadability-weight\/multipledropoutrobertabase_LB0.504_mlen250.pt\"\n               ]\nlengs = [1024,1024]#,1024]#,768,768,1024,768]#,768]\n\n\nmodel_paths_emb = [#\"..\/input\/roberta-transformers-pytorch\/roberta-large\/\",\n                   #\"..\/input\/roberta-transformers-pytorch\/roberta-large\/\",\n                   \"..\/input\/roberta-transformers-pytorch\/roberta-large\/\",\n                   \"..\/input\/roberta-transformers-pytorch\/distilroberta-base\/\",\n                   \"..\/input\/roberta-transformers-pytorch\/roberta-base\/\",\n                   #\"..\/input\/roberta-transformers-pytorch\/roberta-large\/\",]\n                   #\"..\/input\/roberta-transformers-pytorch\/roberta-base\/\",]\n                   #\"..\/input\/roberta-transformers-pytorch\/roberta-base\/\"]\n                  ]\nweight_paths_emb = [#\"..\/input\/commonlitreadability-weight\/simplelarge_0.469_fold0_mlen250.pt\",\n                    #\"..\/input\/commonlitreadability-weight\/simplelarge_0.455_fold1_mlen250.pt\",\n                    \"..\/input\/commonlitreadability-weight\/simplelarge_0.464_fold3_mlen250.pt\",\n                    \"..\/input\/commonlitreadability-weight\/bestmodel_0.469_fold1_mlen250.pt\",\n                    \"..\/input\/commonlitreadability-weight\/bestbase_0.443_fold4_mlen250.pt\",\n                    #\"..\/input\/commonlitreadability-weight\/multipledropoutrobertalarge_LB0.500_mlen250.pt\",\n                    #\"..\/input\/commonlitreadability-weight\/basebestmodel_0.428_fold4_mlen250.pt\",\n                    #\"..\/input\/commonlitreadability-weight\/multipledropoutrobertabase_LB0.504_mlen250.pt\"\n                   ]\nlengs = [1024,768,768]#[1024,1024,1024,768,768,1024,768]#,768]","75f23242":"#single LB 0.483\npreds0 = generate_predictions_md(model_path=\"..\/input\/roberta-transformers-pytorch\/roberta-large\/\",\n                              weight_path=\"..\/input\/commonlitreadability-weight\/simplelarge_0.469_fold0_mlen250.pt\",\n                              max_len=250,leng=1024)\n\n#single LB 0.487\npreds1 = generate_predictions_md(model_path=\"..\/input\/roberta-transformers-pytorch\/roberta-large\/\",\n                              weight_path=\"..\/input\/commonlitreadability-weight\/simplelarge_0.455_fold1_mlen250.pt\",\n                              max_len=250,leng=1024)\n\n#single LB 0.489\npreds2 = generate_predictions_md(model_path=\"..\/input\/roberta-transformers-pytorch\/roberta-large\/\",\n                              weight_path=\"..\/input\/commonlitreadability-weight\/simplelarge_0.464_fold3_mlen250.pt\",\n                              max_len=250,leng=1024)","761716d2":"test_embeddings = []\n\nfor mpath,wpath, leng in zip(model_paths_emb,weight_paths_emb, lengs):\n    emb = generate_embeddings(mpath,wpath, max_len=250,leng=leng)\n    test_embeddings.append(emb)","37879f48":"test_embeddings[0].shape","60ccea29":"params = {\n    'objective': 'reg:squarederror',\n    'eval_metric': 'rmse',\n    \n    'eta': 0.05,\n    'max_depth': 2,\n    \n    'gamma': 1,\n    'subsample': 0.9,\n    \n    'nthread': 2,\n    'tree_method' : 'gpu_hist'\n}\n\nnfolds = 5\nkf = KFold(n_splits=nfolds, shuffle=True, random_state=2048)","d454f124":"def get_pred(train_embeddings,test_emb, targets, train_df):\n    best_iterations = []\n    oof_rmses = []\n    preds = np.zeros(test.shape[0])\n    pred_list = []\n\n    for k, (train_idx, valid_idx) in enumerate(kf.split(train_df)):    \n\n        dtrain = xgb.DMatrix(train_embeddings[train_idx], targets[train_idx])\n        dvalid = xgb.DMatrix(train_embeddings[valid_idx], targets[valid_idx])\n        evals_result = dict()\n        booster = xgb.train(params,\n                            dtrain,\n                            evals=[(dtrain, 'train'), (dvalid, 'valid')],\n                            num_boost_round=300,\n                            early_stopping_rounds=20,\n                            evals_result=evals_result,\n                            verbose_eval=False)\n\n        best_iteration = np.argmin(evals_result['valid']['rmse'])\n        best_iterations.append(best_iteration)\n        oof_rmse = evals_result['valid']['rmse'][best_iteration]\n        oof_rmses.append(oof_rmse)\n        \n        preds += booster.predict(xgb.DMatrix(test_emb), ntree_limit=int(best_iteration+1)) \/ nfolds\n        pred_list.append(booster.predict(xgb.DMatrix(test_emb), ntree_limit=int(best_iteration+1)))\n    pred_list = np.array(pred_list)[np.argpartition(oof_rmses, 3)[:2]]\n    preds = pred_list.mean(axis=0)\n    evals_df = pd.DataFrame()\n    evals_df['fold'] = range(1, nfolds+1)\n    evals_df['best_iteration'] = best_iterations\n    evals_df['oof_rmse'] = oof_rmses\n\n    display(evals_df)\n    print('mean oof rmse = {}'.format(np.mean(oof_rmses)))\n    \n    torch.cuda.empty_cache()\n    return preds","43a70c50":"def rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))\n\ndef get_pred_svm(X,y,X_test,bins,nfolds=5,C=10,kernel='rbf'):\n    scores = list()\n    preds = np.zeros((X_test.shape[0]))\n    \n    kfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=2048)\n    for k, (train_idx,valid_idx) in enumerate(kfold.split(X,bins)):\n        model = SVR(C=C,kernel=kernel,gamma='auto')\n        X_train,y_train = X[train_idx], y[train_idx]\n        X_valid,y_valid = X[valid_idx], y[valid_idx]\n        \n        model.fit(X_train,y_train)\n        prediction = model.predict(X_valid)\n        score = rmse_score(prediction,y_valid)\n        print(f'Fold {k} , rmse score: {score}')\n        scores.append(score)\n        preds += model.predict(X_test)\n        \n    print(\"mean rmse\",np.mean(scores))\n    return np.array(preds)\/nfolds","71bb437f":"test = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\ntarget = np.load(\"..\/input\/embedding-featurevector\/target.npy\")\ntrain_df = pd.read_csv(\"..\/input\/step1-exclude-anomaly\/train.csv\")","9bf56e0f":"num_bins = int(np.floor(1 + np.log2(len(train_df))))\ntrain_df.loc[:,'bins'] = pd.cut(train_df['target'],bins=num_bins,labels=False)\nbins = train_df.bins.to_numpy()","c095aacc":"pred_comb_svm = np.empty((0,len(test)))\npred_comb = np.empty((0,len(test)))\nfor i in range(len(test_embeddings)):\n    embeddings = np.load(\"..\/input\/embedding-featurevector\/embeddings\" + str(i+2) + \".npy\")\n    print(embeddings.shape)\n    preds = get_pred(embeddings,test_embeddings[i], target, train_df)\n    pred_comb = np.append(pred_comb,preds[None,:],axis=0)\n    preds_svm = get_pred_svm(embeddings, target, test_embeddings[i], bins = bins)\n    pred_comb_svm = np.append(pred_comb_svm,preds_svm[None,:],axis=0)","fa40f6e1":"pred_ori = (preds0 + preds1 + preds2)\/3\n#pred_ori = (preds0 + preds1)\/2\npred_svm = pred_comb_svm.sum(axis=0)\/len(pred_comb_svm)\npred_xgb = pred_comb.sum(axis=0)\/len(pred_comb)\npredf = (pred_ori*3 + pred_xgb*3)\/6 # +pred_comb_svm[1])\/6","9a08ff2e":"BATCH_SIZE = 32\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\nROBERTA_PATH = \"..\/input\/roberta-transformers-pytorch\/roberta-base\"\nTOKENIZER_PATH = \"..\/input\/roberta-transformers-pytorch\/roberta-base\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ntest_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/test.csv\")\nsubmission_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/sample_submission.csv\")\ntokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)","7453ef2e":"class LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n            \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n\n        # The number of cells is MAX_LEN.\n        # The size of the hidden state of each cell is 768 (for roberta-base).\n        # In order to condense hidden states of all cells to a context vector,\n        # we compute a weighted average of the hidden states of all cells.\n        # We compute the weight of each cell, using the attention neural network.\n        weights = self.attention(last_layer_hidden_states)\n                \n        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n        # Now we compute context_vector as the weighted average.\n        # context_vector.shape is BATCH_SIZE x 768\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)","50fa1f15":"class LitDataset(Dataset):\n    def __init__(self, df, inference_only=False):\n        super().__init__()\n\n        self.df = df        \n        self.inference_only = inference_only\n        self.text = df.excerpt.tolist()\n        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n        \n        if not self.inference_only:\n            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n    \n        self.encoded = tokenizer.batch_encode_plus(\n            self.text,\n            padding = 'max_length',            \n            max_length = MAX_LEN,\n            truncation = True,\n            return_attention_mask=True\n        )        \n \n\n    def __len__(self):\n        return len(self.df)\n\n    \n    def __getitem__(self, index):        \n        input_ids = torch.tensor(self.encoded['input_ids'][index])\n        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n        \n        if self.inference_only:\n            return (input_ids, attention_mask)            \n        else:\n            target = self.target[index]\n            return (input_ids, attention_mask, target)","992582c9":"def predict(model, data_loader):\n    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n    model.eval()\n\n    result = np.zeros(len(data_loader.dataset))    \n    index = 0\n    \n    with torch.no_grad():\n        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)\n                        \n            pred = model(input_ids, attention_mask)                        \n\n            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n            index += pred.shape[0]\n\n    return result","275b1a8b":"from transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom transformers import AutoConfig\nimport gc\ngc.enable()\n\ntest_dataset = LitDataset(test_df, inference_only=True)\n\nNUM_MODELS = 5\n\nall_predictions = np.zeros((NUM_MODELS, len(test_df)))\n\n\n\ntest_dataset = LitDataset(test_df, inference_only=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                         drop_last=False, shuffle=False, num_workers=2)\n\nfor model_index in range(NUM_MODELS):            \n    model_path = f\"..\/input\/commonlit-roberta-0467\/model_{model_index + 1}.pth\"\n    print(f\"\\nUsing {model_path}\")\n                        \n    model = LitModel()\n    model.load_state_dict(torch.load(model_path, map_location=DEVICE))    \n    model.to(DEVICE)\n        \n    all_predictions[model_index] = predict(model, test_loader)\n            \n    del model\n    gc.collect()","deaee16f":"model_predictions = (all_predictions[0] + all_predictions[2] + all_predictions[3])\/3","5523f705":"del all_predictions","cba1c13a":"test = test_df\n\nfrom glob import glob\nimport os\nimport matplotlib.pyplot as plt\nimport json\nfrom collections import defaultdict\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.optimizer import Optimizer\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom torch.utils.data import (\n    Dataset, DataLoader, \n    SequentialSampler, RandomSampler\n)\nfrom transformers import RobertaConfig\nfrom transformers import (\n    get_cosine_schedule_with_warmup, \n    get_cosine_with_hard_restarts_schedule_with_warmup\n)\nfrom transformers import RobertaTokenizer\nfrom transformers import RobertaModel\nfrom IPython.display import clear_output\nfrom tqdm import tqdm, trange\n\ndef convert_examples_to_features(data, tokenizer, max_len, is_test=False):\n    data = data.replace('\\n', '')\n    tok = tokenizer.encode_plus(\n        data, \n        max_length=max_len, \n        truncation=True,\n        return_attention_mask=True,\n        return_token_type_ids=True\n    )\n    curr_sent = {}\n    padding_length = max_len - len(tok['input_ids'])\n    curr_sent['input_ids'] = tok['input_ids'] + ([0] * padding_length)\n    curr_sent['token_type_ids'] = tok['token_type_ids'] + \\\n        ([0] * padding_length)\n    curr_sent['attention_mask'] = tok['attention_mask'] + \\\n        ([0] * padding_length)\n    return curr_sent\n\nclass DatasetRetriever(Dataset):\n    def __init__(self, data, tokenizer, max_len, is_test=False):\n        self.data = data\n        self.excerpts = self.data.excerpt.values.tolist()\n        self.tokenizer = tokenizer\n        self.is_test = is_test\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, item):\n        if not self.is_test:\n            excerpt, label = self.excerpts[item], self.targets[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, \n                self.max_len, self.is_test\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n                'label':torch.tensor(label, dtype=torch.double),\n            }\n        else:\n            excerpt = self.excerpts[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, \n                self.max_len, self.is_test\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n            }\n\nclass CommonLitModel(nn.Module):\n    def __init__(\n        self, \n        model_name, \n        config,  \n        multisample_dropout=False,\n        output_hidden_states=False\n    ):\n        super(CommonLitModel, self).__init__()\n        self.config = config\n        self.roberta = RobertaModel.from_pretrained(\n            model_name, \n            output_hidden_states=output_hidden_states\n        )\n        self.layer_norm = nn.LayerNorm(config.hidden_size)\n        if multisample_dropout:\n            self.dropouts = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        self.regressor = nn.Linear(config.hidden_size, 1)\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.regressor)\n \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n \n    def forward(\n        self, \n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        labels=None\n    ):\n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n        )\n        sequence_output = outputs[1]\n        sequence_output = self.layer_norm(sequence_output)\n \n        # multi-sample dropout\n        for i, dropout in enumerate(self.dropouts):\n            if i == 0:\n                logits = self.regressor(dropout(sequence_output))\n            else:\n                logits += self.regressor(dropout(sequence_output))\n        \n        logits \/= len(self.dropouts)\n \n        # calculate loss\n        loss = None\n        if labels is not None:\n            loss_fn = torch.nn.MSELoss()\n            logits = logits.view(-1).to(labels.dtype)\n            loss = torch.sqrt(loss_fn(logits, labels.view(-1)))\n        \n        output = (logits,) + outputs[1:]\n        return ((loss,) + output) if loss is not None else output\n\ndef make_model(model_name, num_labels=1):\n    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n    config = RobertaConfig.from_pretrained(model_name)\n    config.update({'num_labels':num_labels})\n    model = CommonLitModel(model_name, config=config)\n    return model, tokenizer\n\ndef make_loader(\n    data, \n    tokenizer, \n    max_len,\n    batch_size,\n):\n    \n    test_dataset = DatasetRetriever(data, tokenizer, max_len, is_test=True)\n    test_sampler = SequentialSampler(test_dataset)\n    test_loader = DataLoader(\n        test_dataset, \n        batch_size=batch_size \/\/ 2, \n        sampler=test_sampler, \n        pin_memory=False, \n        drop_last=False, \n        num_workers=0\n    )\n\n    return test_loader\n\nclass Evaluator:\n    def __init__(self, model, scalar=None):\n        self.model = model\n        self.scalar = scalar\n\n    def evaluate(self, data_loader, tokenizer):\n        preds = []\n        self.model.eval()\n        total_loss = 0\n        with torch.no_grad():\n            for batch_idx, batch_data in enumerate(data_loader):\n                input_ids, attention_mask, token_type_ids = batch_data['input_ids'], \\\n                    batch_data['attention_mask'], batch_data['token_type_ids']\n                input_ids, attention_mask, token_type_ids = input_ids.cuda(), \\\n                    attention_mask.cuda(), token_type_ids.cuda()\n                \n                if self.scalar is not None:\n                    with torch.cuda.amp.autocast():\n                        outputs = self.model(\n                            input_ids=input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids\n                        )\n                else:\n                    outputs = self.model(\n                        input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        token_type_ids=token_type_ids\n                    )\n                \n                logits = outputs[0].detach().cpu().numpy().squeeze().tolist()\n                preds += logits\n        return preds\n\ndef config(fold, model_name, load_model_path):\n    torch.manual_seed(2021)\n    torch.cuda.manual_seed(2021)\n    torch.cuda.manual_seed_all(2021)\n    \n    max_len = 250\n    batch_size = 8\n\n    model, tokenizer = make_model(\n        model_name=model_name, \n        num_labels=1\n    )\n    model.load_state_dict(\n        torch.load(f'{load_model_path}\/model{fold}.bin')\n    )\n    test_loader = make_loader(\n        test, tokenizer, max_len=max_len,\n        batch_size=batch_size\n    )\n\n    if torch.cuda.device_count() >= 1:\n        print('Model pushed to {} GPU(s), type {}.'.format(\n            torch.cuda.device_count(), \n            torch.cuda.get_device_name(0))\n        )\n        model = model.cuda() \n    else:\n        raise ValueError('CPU training is not supported')\n\n    # scaler = torch.cuda.amp.GradScaler()\n    scaler = None\n    return (\n        model, tokenizer, \n        test_loader, scaler\n    )\n\ndef run(fold=0, model_name=None, load_model_path=None):\n    model, tokenizer, \\\n        test_loader, scaler = config(fold, model_name, load_model_path)\n    \n    import time\n\n    evaluator = Evaluator(model, scaler)\n\n    test_time_list = []\n\n    torch.cuda.synchronize()\n    tic1 = time.time()\n\n    preds = evaluator.evaluate(test_loader, tokenizer)\n\n    torch.cuda.synchronize()\n    tic2 = time.time() \n    test_time_list.append(tic2 - tic1)\n    \n    del model, tokenizer, test_loader, scaler\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return preds","1eebd9d7":"pred_df1 = pd.DataFrame()\npred_df2 = pd.DataFrame()\npred_df3 = pd.DataFrame()\nfor fold in tqdm(range(5)):\n    pred_df1[f'fold{fold}'] = run(fold, '..\/input\/roberta-transformers-pytorch\/roberta-base\/', '..\/input\/commonlit-roberta-base-i\/')\n    pred_df2[f'fold{fold+5}'] = run(fold, '..\/input\/roberta-transformers-pytorch\/roberta-large\/', '..\/input\/roberta-large-itptfit\/')\n    pred_df3[f'fold{fold+10}'] = run(fold, '..\/input\/roberta-transformers-pytorch\/roberta-large\/', '..\/input\/commonlit-roberta-large-ii\/')","79f4968a":"pred_df1 = np.array(pred_df1)\npred_df2 = np.array(pred_df2)\npred_df3 = np.array(pred_df3)\n\nbest3model = (pred_df2[:,0] + pred_df2[:,1] + pred_df3[:,4])\/3\n# + pred_df3[:,2] + pred_df3[:,3])\/5\n#next5model = (pred_df3[:,2] + pred_df3[:,3] + pred_df2[:,3] + pred_df1[:,0] + pred_df1[:,3] + pred_df1[:,4])\/6\n#model2preds = (best3model + next5model)\/2\n#model2_predictions = (pred_df2.mean(axis=1)*0.5) + (pred_df1.mean(axis=1)*0.3) + (pred_df3.mean(axis=1) * 0.2)","6cbecb7d":"#predfinal = (pred_ori*3 + pred_xgb*2 + model_predictions*3 + model2preds*3)\/11\npredfinal = (pred_ori*3 + model_predictions*3 + best3model*3)\/9","dc32af56":"test['prediction'] = predfinal\nsubmission = pd.DataFrame()\nsubmission['id'] = test['id'].copy()\nsubmission['target'] = test['prediction'].copy()\nsubmission.to_csv('submission.csv', index=False)\nsubmission","a3785b36":"# SVM prediction","288c6380":"# Pytorch inference ensemble notebook\n\nthanks to https:\/\/www.kaggle.com\/andretugan\/pre-trained-roberta-solution-in-pytorch\n\nthanks to https:\/\/www.kaggle.com\/rhtsingh\/commonlit-readability-prize-roberta-torch-infer-3\n\nThis comptition summary.\n\n\n### preprocess\n\n[make NLP](https:\/\/www.kaggle.com\/kunihikofurugori\/make-nlpdataset)\n\n\u2192 not effective (score is not improved.)\n\n[exclude anomaly data](https:\/\/www.kaggle.com\/kunihikofurugori\/step1-exclude-anomaly)\n\n\u2192 score slightly is improved....?\n\n### transformer training\n\n[transformer tpu8fold](https:\/\/www.kaggle.com\/kunihikofurugori\/tpu8fold-transformer)\n\n\u2192 The score is improved because I tried to many seed pattern.\n\n(Please see Section 4.2 in [Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping](https:\/\/arxiv.org\/pdf\/2002.06305.pdf))\n\n\n[Add attention and early stopping](https:\/\/www.kaggle.com\/kunihikofurugori\/tpu8fold-transformer-attention)\n\u2192 attention and early stopping is powerful method. these method is effective.\n\n(Please see Section 5 in [Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping](https:\/\/arxiv.org\/pdf\/2002.06305.pdf))\n\n[Add multi dropout](https:\/\/www.kaggle.com\/kunihikofurugori\/step2-1-tpu8foldtraining-rbase)\n\n\u2192 score slightly is improved....?\n\n### inference\n\nxgb,lgb inference model is slightly imporved. \n\nsvm inference model is not imporved.\n\nLB score in my model ensembleing is 0.461, andretugan model is model 0.467  and torch model is 0.465 and then 3 pair ensemble is then LB 0.457.","65bee4ae":"# model2"}}