{"cell_type":{"918a23b5":"code","13985482":"code","ca7a05b5":"code","edccd67a":"code","ae9eda37":"code","1488cfd0":"code","6d81617f":"code","e4396330":"code","39050e62":"code","9850a369":"code","6ed53b7d":"code","ac1dd837":"code","7b8c34a5":"code","9dc280e4":"code","cc1fa689":"code","31ecd74e":"code","58b5cd86":"code","73450799":"code","3aef8b47":"markdown","f3ecec1e":"markdown","e79c9100":"markdown","aca85281":"markdown","51ef6c7b":"markdown","b81aac59":"markdown","4050011d":"markdown","04f205b6":"markdown","1cc36922":"markdown","502d0362":"markdown","6e7705f3":"markdown","8bcd7956":"markdown","71b6503c":"markdown","60d715a4":"markdown","a48f6528":"markdown","e69a6a20":"markdown","8070cb66":"markdown","8f6a77c7":"markdown","8393d4c8":"markdown"},"source":{"918a23b5":"#Installation\n!pip install interpret -q","13985482":"## Importing the necessary libraries and data\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\n\n#interpretml \nfrom interpret import show\nfrom interpret.data import Marginal\nfrom interpret.glassbox import ExplainableBoostingRegressor, LinearRegression, RegressionTree\n\nseed = 1\ntrain = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/test.csv')\n","ca7a05b5":"train.head()","edccd67a":"columns = ['deg_C','relative_humidity','absolute_humidity','sensor_1','sensor_2','sensor_3','sensor_4','sensor_5']","ae9eda37":"target = 'target_carbon_monoxide'\nX_train, X_test, y_train, y_test = train_test_split(train[columns],train[target], test_size=0.20)","1488cfd0":"marginal = Marginal().explain_data(X_train, y_train, name = 'Train Data')\nshow(marginal)","6d81617f":"ebm = ExplainableBoostingRegressor(random_state=seed, n_jobs=-1)\nebm.fit(X_train, y_train) ","e4396330":"ebm_global = ebm.explain_global(name='EBM')\nshow(ebm_global)","39050e62":"ebm_local = ebm.explain_local(X_test[:5], y_test[:5], name='EBM')\nshow(ebm_local)","9850a369":"from interpret.perf import RegressionPerf\n\nebm_perf = RegressionPerf(ebm.predict).explain_perf(X_test, y_test, name='EBM')\nshow(ebm_perf)","6ed53b7d":"lr = LinearRegression(random_state=seed)\nlr.fit(X_train, y_train)\n\nrt = RegressionTree(random_state=seed)\nrt.fit(X_train, y_train)\n\nrf = RandomForestRegressor(n_estimators=100, n_jobs=-1)\nrf.fit(X_train, y_train)","ac1dd837":"lr_perf = RegressionPerf(lr.predict).explain_perf(X_test, y_test, name='Linear Regression')\nrt_perf = RegressionPerf(rt.predict).explain_perf(X_test, y_test, name='Regression Tree')\nrf_perf = RegressionPerf(rf.predict).explain_perf(X_test, y_test, name='Blackbox')\n","7b8c34a5":"lr_global = lr.explain_global(name='Linear Regression')\nrt_global = rt.explain_global(name='Regression Tree')\n\n","9dc280e4":"show(lr_perf)\nshow(rt_perf)\nshow(ebm_perf)\nshow(rf_perf)","cc1fa689":"def predictions(target_column):\n    \"\"\"\n    Function to calculate EBM predictions based on the target column specified\n    \n    \"\"\"\n    training_set = train[columns]\n    target = train[target_column]\n    ebm = ExplainableBoostingRegressor(n_jobs=-1)\n    ebm.fit(training_set,target)\n    preds = ebm.predict(test[columns])\n    return preds","31ecd74e":"preds_benzene = predictions('target_benzene')\npreds_carbon_monoxide = predictions('target_carbon_monoxide')\npreds_nitrogen_oxides = predictions('target_nitrogen_oxides')","58b5cd86":"submission = pd.DataFrame({\n    'date_time': test.date_time,\n    'target_carbon_monoxide': preds_carbon_monoxide,\n    'target_benzene': preds_benzene,\n    'target_nitrogen_oxides': preds_nitrogen_oxides\n})\n\nsubmission.head()","73450799":"submission.to_csv('submission.csv',index=False)","3aef8b47":"## Global Explanations - explaining the entire model behavior.","f3ecec1e":"# TPS Competition\n## Creating multiple predictions and merging them in a common submission file","e79c9100":"Feel free to interact with the above visualisation and gain understanding of your data","aca85281":"\n\n<p style=\"text-align:center;\"><img src=\"https:\/\/imgur.com\/vYz7PYw.png\" alt=\"Dashboard\" width=\"800\" height=\"400\"><\/p>\n\n<center>A dashboard showing the performance comparison of various regressors in InterpretMl<\/center>\n","51ef6c7b":"Now that we have an idea about the working of EBMs, let's calculate the predictions for different target columns i.e `target_benzene`, `target_carbon_monoxide` and `target_nitrogen_oxides`.","b81aac59":"## Exploring the dataset\n\nInterpret exposes a top-level method `show`, of which acts as the surface for rendering explanation visualizations. ","4050011d":"Columns to be used for training:","04f205b6":"## Interpretability Approaches\n\n![](https:\/\/miro.medium.com\/max\/642\/1*8ov3dWV39WHkx8SG6pMXWA.png)","1cc36922":"## Comparing EBM performance with other Regressors - Linear Regression, RegressionTree and Random Forest\n\nInterpret gives us the ability to compare the performance of multiple models in a single dashboard","502d0362":"## Training the Explainable Boosting Machine (EBM)","6e7705f3":"<center>\n    <h1>Explainable Boosting machines for Tabular data<\/h1>\n    <h2>Interpretable or Accurate? Why Not Both?<\/h2>\n    \n<\/center>","8bcd7956":"As summed up by [Miller](https:\/\/arxiv.org\/abs\/1706.07269), interpretability refers to the degree to which a human can understand the cause of a decision. Lately, there has been a lot of emphasis on creating inherently interpretable models and doing away from their black box counterparts. EBMs or [Explainable Boosting Machine](https:\/\/www.youtube.com\/watch?v=MREiHgHgl0k),are models designed to have accuracy comparable to state-of-the-art machine learning methods like Random Forest and Boosted Trees while being highly intelligible and explainable. In this notebook, we will look at the idea behind EBMs and implement them for the given problem via [InterpretML](https:\/\/arxiv.org\/pdf\/1909.09223.pdf), a Unified Framework for Machine Learning Interpretability.\n\nThis notebook is an extension of an article I wrote on the same topic, which you would find useful : [Interpretable or Accurate? Why Not Both?](https:\/\/towardsdatascience.com\/interpretable-or-accurate-why-not-both-4d9c73512192?sk=2f44377541a2f49939c921e54eb3cde7)\n\n---","71b6503c":"To showcase EBM's properties, we'll first train a model using only the `target_carbon_monoxide` as the target column. However, later we will use all the other target columns to create our final submission.","60d715a4":"## Comparing Performances of different models","a48f6528":"## Local Explanations: explaining individual predictions","e69a6a20":"## Evaluating EBM performance on the hold out dataset","8070cb66":"We'll first split our train datasets so that we can see how Interpretml can help us in the following:\n* Exploring the dataset\n* Train the Explainable Boosting Machine (EBM)\n* Understanding what the model learnt overall - Global Explanations\n* Understanding how an individual prediction was made - Local Explanations","8f6a77c7":"## IntepretML: A Unified Framework for Machine Learning Interpretability\n\nEBMs come packaged within a Machine Learning Interpretability toolkit called [InterpretML](https:\/\/arxiv.org\/pdf\/1909.09223.pdf). It is an open-source package for training interpretable models as well as explaining black-box systems. Within InterpretML, the explainability algorithms are organized into two major sections, i.e., **Glassbox models** and **Blackbox explanations**. This means that this tool can not only explain the decisions of inherently interpretable models but also provide possible reasoning for black-box models. The following code architecture from the [official paper](https:\/\/arxiv.org\/pdf\/1909.09223.pdf) sums it nicely.\n\n![code architecture from the official paper | Source: [InterpretML: A Unified Framework for Machine Learning Interpretability](https:\/\/arxiv.org\/pdf\/1909.09223.pdf)](https:\/\/cdn-images-1.medium.com\/max\/2030\/1*MxM1QHK31w16F9U0d5t7CQ.png)","8393d4c8":"## What are EMBs?\n\nEBM is a type of [generalized additive mode](https:\/\/projecteuclid.org\/journals\/statistical-science\/volume-1\/issue-3\/Generalized-Additive-Models\/10.1214\/ss\/1177013604.full)l or GAM for short. Linear models assume a linear relationship between the response and predictors. Thus, they are unable to capture the non-linearities in the data.\n\nLinear Model: y = \u03b20 + \u03b21x1 + \u03b22x2 + \u2026 + \u03b2n xn\n\nTo overcome this shortcoming, in the late 80\u2019s statisticians [Hastie & Tibshirani developed generalized additive models](https:\/\/projecteuclid.org\/journals\/statistical-science\/volume-1\/issue-3\/Generalized-Additive-Models\/10.1214\/ss\/1177013604.full)(GAMs), which keep the additive structure, and therefore the interpretability of the linear models. Thus, the linear relationship between the response and predictor variable gets [replaced by several non-linear smooth functions](https:\/\/datascienceplus.com\/generalized-additive-models\/)(f1, f2, etc.) to model and capture the non-linearities in the data. GAMs are more accurate than simple linear models, and since they do not contain any interactions between features, users can also easily interpret them.\n\nAdditive Model: y = f1(x1) + f2(x2) + \u2026 + fn(xn)\n\nEBMs are an improvement on the GAMs utilizing techniques like gradient boosting and bagging. EBMs include pairwise interaction terms, which increases their accuracy even further.\n\nEBMs: y = \u01a9i fi (xi) + \u01a9ij fij(xi , xj) + \u01a9ijk fijk (xi , xj , xk )\n\n---\n"}}