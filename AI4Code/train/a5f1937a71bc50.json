{"cell_type":{"157ef89a":"code","209b5167":"code","4dd4fce7":"code","f6cf8e9d":"code","b1ac0c1f":"code","4334b6fc":"code","943121ad":"code","744bb900":"code","f68ff676":"code","53726d2b":"code","4db1fd6c":"code","dc5eb32f":"code","c941ba35":"markdown","dfa18439":"markdown","68418ef8":"markdown","934678fd":"markdown","729092a8":"markdown","aa0a4ef4":"markdown","40ea5f88":"markdown"},"source":{"157ef89a":"!pip install ..\/input\/iterative-stratification\/iterative-stratification-master","209b5167":"import os\nimport random\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.neighbors import NearestNeighbors\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.models import load_model\n\nfrom tensorflow.keras.callbacks import (\n    ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'","4dd4fce7":"def seed_everything(seed=2020):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nSEED = 42\nseed_everything(SEED)","f6cf8e9d":"x_develop = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ny_develop = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\nx_test = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsub = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\n\ntarget_cols = y_develop.columns[1:]\nN_TARGETS = len(target_cols)","b1ac0c1f":"def preprocess_df(df):\n    if 'cp_type' in df.columns:\n        df = df.drop('cp_type', axis=1)\n    df['cp_dose'] = (df['cp_dose'] == 'D2').astype(int)\n    df['cp_time'] = df['cp_time'].map({24:1, 48: 2, 72: 3})\n    return df","4334b6fc":"x_develop = preprocess_df(x_develop)\nx_test = preprocess_df(x_test)\n\nx_develop = x_develop.drop('sig_id', axis=1)\ny_develop = y_develop.drop('sig_id', axis=1)\nx_test = x_test.set_index('sig_id')","943121ad":"def create_folds(df, fold_no, fold_type='mls_kfold', save=False, seed=42):\n    \"\"\"\n    df: target dataframe\n    \"\"\"\n    if fold_type == 'kfold':\n        kf = KFold(n_splits=fold_no, shuffle=True, random_state=seed)\n    elif fold_type == 'mls_kfold':\n        kf = MultilabelStratifiedKFold(n_splits=fold_no, shuffle=False, random_state=seed)\n        \n    df['Fold'] = -1\n    df.reset_index(inplace=True, drop=True)\n    for fold, (t, v) in enumerate(kf.split(df, df)):\n        df.loc[v, 'Fold'] = fold\n    if save:\n        df.to_csv('Folds.csv')\n    return df","744bb900":"N_FOLDS = 5\nfold_type = 'mls_kfold'\ny_develop = create_folds(y_develop, fold_no=N_FOLDS, fold_type=fold_type, seed=SEED)","f68ff676":"def create_model(input_shape, output_bias=None):\n    if output_bias is not None:\n        output_bias = tf.keras.initializers.Constant(output_bias)\n\n    inputs = tf.keras.Input(shape=input_shape)\n    x = L.BatchNormalization()(inputs)\n    x = tfa.layers.WeightNormalization(L.Dense(1000, activation='swish'))(x)\n    x = L.BatchNormalization()(x)\n    x = L.Dropout(0.4)(x)\n    x = tfa.layers.WeightNormalization(L.Dense(500, activation='swish'))(x)\n    x = L.BatchNormalization()(x)\n    x = L.Dropout(0.4)(x)\n    outputs = tfa.layers.WeightNormalization(L.Dense(N_TARGETS,\n                                                     activation='sigmoid',\n                                                     bias_initializer=output_bias\n                                                    )\n                                            )(x)\n    return tf.keras.Model(inputs=inputs, outputs=outputs)","53726d2b":"# Taken from https:\/\/github.com\/niteshsukhwani\/MLSMOTE, modified by Tolga Dincer.\ndef get_tail_label(df: pd.DataFrame, ql=[0.05, 1.]) -> list:\n    \"\"\"\n    Find the underrepresented targets.\n    Underrepresented targets are those which are observed less than the median occurance.\n    Targets beyond a quantile limit are filtered.\n    \"\"\"\n    irlbl = df.sum(axis=0)\n    irlbl = irlbl[(irlbl > irlbl.quantile(ql[0])) & ((irlbl < irlbl.quantile(ql[1])))]  # Filtering\n    irlbl = irlbl.max() \/ irlbl\n    threshold_irlbl = irlbl.median()\n    tail_label = irlbl[irlbl > threshold_irlbl].index.tolist()\n    return tail_label\n\ndef get_minority_samples(X: pd.DataFrame, y: pd.DataFrame, ql=[0.05, 1.]):\n    \"\"\"\n    return\n    X_sub: pandas.DataFrame, the feature vector minority dataframe\n    y_sub: pandas.DataFrame, the target vector minority dataframe\n    \"\"\"\n    tail_labels = get_tail_label(y, ql=ql)\n    index = y[y[tail_labels].apply(lambda x: (x == 1).any(), axis=1)].index.tolist()\n    \n    X_sub = X[X.index.isin(index)].reset_index(drop = True)\n    y_sub = y[y.index.isin(index)].reset_index(drop = True)\n    return X_sub, y_sub\n\ndef nearest_neighbour(X: pd.DataFrame, neigh) -> list:\n    \"\"\"\n    Give index of 10 nearest neighbor of all the instance\n    \n    args\n    X: np.array, array whose nearest neighbor has to find\n    \n    return\n    indices: list of list, index of 5 NN of each element in X\n    \"\"\"\n    nbs = NearestNeighbors(n_neighbors=neigh, metric='euclidean', algorithm='kd_tree').fit(X)\n    euclidean, indices = nbs.kneighbors(X)\n    return indices\n\ndef MLSMOTE(X, y, n_sample, neigh=5):\n    \"\"\"\n    Give the augmented data using MLSMOTE algorithm\n    \n    args\n    X: pandas.DataFrame, input vector DataFrame\n    y: pandas.DataFrame, feature vector dataframe\n    n_sample: int, number of newly generated sample\n    \n    return\n    new_X: pandas.DataFrame, augmented feature vector data\n    target: pandas.DataFrame, augmented target vector data\n    \"\"\"\n    indices2 = nearest_neighbour(X, neigh=5)\n    n = len(indices2)\n    new_X = np.zeros((n_sample, X.shape[1]))\n    target = np.zeros((n_sample, y.shape[1]))\n    for i in range(n_sample):\n        reference = random.randint(0, n-1)\n        neighbor = random.choice(indices2[reference, 1:])\n        all_point = indices2[reference]\n        nn_df = y[y.index.isin(all_point)]\n        ser = nn_df.sum(axis = 0, skipna = True)\n        target[i] = np.array([1 if val > 0 else 0 for val in ser])\n        ratio = random.random()\n        gap = X.loc[reference,:] - X.loc[neighbor,:]\n        new_X[i] = np.array(X.loc[reference,:] + ratio * gap)\n    new_X = pd.DataFrame(new_X, columns=X.columns)\n    target = pd.DataFrame(target, columns=y.columns)\n    return new_X, target\n\ndef SMOLTE_cat_wrapper(x_df, y_df, cat_col, nsamples):\n    x_df_up = pd.DataFrame(columns=x_df.columns)\n    y_df_up = pd.DataFrame(columns=y_df.columns)\n\n    unique_cat_combs = x_df.groupby(cat_col).size().reset_index().rename(columns={0:'count'})[cat_col]\n    num_cols = x_df.columns.drop(cat_col).tolist()\n    for index, row in unique_cat_combs.iterrows():\n        condition = (x_df[cat_col] == row).all(axis=1)\n\n        subx = x_df[condition][num_cols].reset_index(drop=True)\n        suby = y_df[condition].reset_index(drop=True)\n\n        x_df_sub, y_df_sub = get_minority_samples(subx, suby)\n        a, b = MLSMOTE(x_df_sub, y_df_sub, nsamples, 5)\n        cats = pd.concat([row.to_frame().T]*len(a), ignore_index=True)\n        a = pd.merge(cats, a, how='left', left_index=True, right_index=True)\n        x_df_up = x_df_up.append(a, ignore_index=True)\n        y_df_up = y_df_up.append(b, ignore_index=True)\n    #y_df_up = y_df_up.astype(int)\n    \n    print('Number of new samples created: %d' %(len(y_df_up)))\n    \n    x_df_up = pd.concat([x_df, x_df_up], ignore_index=True)\n    y_df_up = pd.concat([y_df, y_df_up], ignore_index=True)\n    \n    x_df_up = x_df_up.sample(len(x_df_up), random_state=1881).reset_index(drop=True)\n    y_df_up = y_df_up.sample(len(y_df_up), random_state=1881).reset_index(drop=True)\n    \n    x_df_up[cat_col] = x_df_up[cat_col].astype(int)\n    return x_df_up, y_df_up","4db1fd6c":"def oof_score(oof: dict):\n    return np.mean(list(oof.values())), np.std(list(oof.values()))\n\n\ndef run_cv(summary=True, debug=False, nsamples=100):\n    histories = {x: '' for x in range(N_FOLDS)} \n    models = {x: '' for x in range(N_FOLDS)}\n    results = {x: '' for x in range(N_FOLDS)}\n    oof_bp = {x: [] for x in range(N_FOLDS)}\n    oof_ap = {x: [] for x in range(N_FOLDS)}\n    \n    for foldno in np.sort(y_develop.Fold.unique()):\n        x_train_fold = x_develop[y_develop.Fold != foldno]\n        y_train_fold = y_develop[y_develop.Fold != foldno].drop('Fold', axis=1)\n        x_val_fold = x_develop[y_develop.Fold == foldno]\n        y_val_fold = y_develop[y_develop.Fold == foldno].drop('Fold', axis=1)\n        \n        train_sample_size = len(y_train_fold)\n        val_sample_size = len(y_val_fold)\n        print(\" \")\n        print(f\"Fold-%d\" % (foldno))\n        print(\"Original Train sample size:\", train_sample_size, \", Original validation sample size:\", val_sample_size)\n\n        FEATURE_SIZE = x_train_fold.shape[-1]\n        \n        cat_col = ['cp_time', 'cp_dose']\n        x_train_fold, y_train_fold = SMOLTE_cat_wrapper(x_train_fold, y_train_fold, cat_col, nsamples=nsamples)\n        print(\"Upsampled Train sample size: %d\" % (len(x_train_fold)))\n        \n        # Train Data Pipeline\n        train_ds = tf.data.Dataset.from_tensor_slices((x_train_fold, y_train_fold))\n        train_ds = train_ds.shuffle(1024).batch(56)\n\n        # Validation Data Pipeline\n        val_ds = tf.data.Dataset.from_tensor_slices((x_val_fold, y_val_fold))\n        val_ds = val_ds.batch(val_sample_size)\n\n        # Callbacks\n        cb_es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\n\n        # Model & Fit\n        models[foldno] = create_model(FEATURE_SIZE, output_bias=8.32)\n        OPTIMIZER = tfa.optimizers.Lookahead(\n            tfa.optimizers.AdamW(weight_decay=1e-5),\n            sync_period=5)\n        models[foldno].compile(optimizer=OPTIMIZER, loss=tf.keras.losses.BinaryCrossentropy()) #label_smoothing=0.001\n        histories[foldno] = models[foldno].fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=[cb_es, reduce_lr_loss], verbose=1)\n        \n        # OOF (Validation) Results\n        oof_y_val = models[foldno].predict(x_val_fold)\n        \n        oof_bp[foldno] = tf.keras.losses.binary_crossentropy(y_val_fold, oof_y_val).numpy().mean()\n        print('Out-of-Fold Score: ', oof_bp[foldno])\n        \n        #oof_y_val[cp_type_val_fold == 0] = 0\n        oof_ap[foldno] = tf.keras.losses.binary_crossentropy(y_val_fold, oof_y_val).numpy().mean()\n        print('Out-of-Fold Score with post processing: ', oof_ap[foldno])\n        \n            \n        # Test Predictions\n        results[foldno] = pd.DataFrame(index=x_test.index,\n                                       columns=target_cols,\n                                       data=models[foldno].predict(x_test))\n        #results[foldno].loc[cp_type_te.index[cp_type_te == 'ctl_vehicle']] = 0\n\n        \n        if foldno == 0:\n            res = results[foldno]\n        else:\n            res += results[foldno]\n\n        # Save Model\n        if SAVE_MODEL:\n            models[foldno].save(f'weights-fold{foldno}.h5')\n    \n    res = res \/ N_FOLDS\n    print('\\n')\n    if summary:\n        print('Summary')\n        # Mean out of score before postprocessing\n        print('Mean OOF score: %f +\/- %f' % (oof_score(oof_bp)))\n\n        # Mean out of score after postprocessing\n        print('Mean OOF score after postprocessing: %f +\/- %f' % (oof_score(oof_ap)))\n    \n    return res, histories, oof_ap","dc5eb32f":"EPOCHS = 45\nSAVE_MODEL = False\n\nif sub.shape[0] != 3982:\n    res, histories, oof_ap = run_cv(summary=True, debug=False, nsamples=50)\n    sub = res.reset_index()\n    sub.to_csv('submission.csv', index=False)\nelse:\n    sub.to_csv('submission.csv', index=False)","c941ba35":"### Define Model Architecture","dfa18439":"### Cross Validation","68418ef8":"### Encode Categorical Variables","934678fd":"### Import Libraries","729092a8":"### Group Data Into Folds","aa0a4ef4":"### Read Data","40ea5f88":"### Upsampling"}}