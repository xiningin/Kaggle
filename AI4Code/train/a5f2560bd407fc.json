{"cell_type":{"52bb9ff3":"code","560bdef3":"code","a94048ee":"code","ce8874c4":"code","efa86768":"code","d8b7a143":"code","6a380e12":"code","868f9991":"code","c2c74b6f":"code","933f32c1":"code","6497f518":"code","ce3b43a9":"code","022195a0":"code","01ef3eb9":"code","c3fb14c4":"code","3b97c7e6":"code","1751b202":"code","1b1cb89e":"code","dfb800f3":"code","703a9918":"code","eec8d6fb":"code","4b5bb556":"code","cf64e610":"code","f59ea68b":"code","7ebbd61e":"code","d4f4a91c":"code","d9adcac9":"code","cf0480f6":"code","0223ed4b":"code","4a69eda1":"code","0323e7c1":"code","41ea69da":"code","326d487d":"markdown","2c1c0d91":"markdown","75c776a8":"markdown","f948a5bf":"markdown","9f32cdf5":"markdown","45708e76":"markdown","a97b0877":"markdown","14498ee2":"markdown","bee6d145":"markdown","41dc1342":"markdown","1f63119e":"markdown","e47d68b4":"markdown","3637f274":"markdown","65e0f3d8":"markdown","5f18389c":"markdown","d1bdc571":"markdown","d74ed738":"markdown","16cbaec8":"markdown","9af0c693":"markdown"},"source":{"52bb9ff3":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# from matplotlib import cm as cm\nimport seaborn as sns","560bdef3":"# Read Data from File\ndata = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')","a94048ee":"data.head()","ce8874c4":"data.tail()","efa86768":"data.info()","d8b7a143":"data.describe()","6a380e12":"data.shape","868f9991":"data.columns","c2c74b6f":"# Checking if there is any attributes that has None or numpy.NaN values\ndata.isna().any()","933f32c1":"# How many rows of each attribute that has NaN value\ndata.apply(lambda x: x.isna().values.ravel().sum())","6497f518":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler","ce3b43a9":"data.drop('id',axis = 1, inplace=True)\ndata.drop('Unnamed: 32',axis = 1, inplace=True)\ndata.shape","022195a0":"print(\"Number of Malignant Records: {0} accounts for: {1:.2f}% of the diagnosis class\\n\"\n      \"Number of Benign Records: {2} accounts for: {3:.2f}% of the diagnosis class\".format(\n    data.loc[data.diagnosis == 'M'].shape[0],\n    100 * data.loc[data.diagnosis == 'M'].shape[0] \/ data.shape[0],\n    data.loc[data.diagnosis == 'B'].shape[0],\n    100 * data.loc[data.diagnosis == 'B'].shape[0] \/ data.shape[0]))","01ef3eb9":"# Check data set imbalance\nunique, counts = np.unique(data.diagnosis, return_counts=True)\nplt.bar(unique, counts, 1, color=['lightgreen', 'lightgray'])\nplt.title('Class Frequency')\nplt.xlabel('Class')\nplt.ylabel('Frequency')\nplt.show()","c3fb14c4":"# Plot distribution\ndata.plot.density(subplots=True, layout=(5,10), sharex=False, legend=False, fontsize=1, figsize=(12,12))\nplt.show()","3b97c7e6":"plt.figure(figsize=(12, 12))\n\ncorr = data.corr()\n\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n)\n","1751b202":"data.drop(['perimeter_mean','perimeter_se', 'perimeter_worst', 'area_mean', 'area_se', 'area_worst'],axis = 1, inplace=True)\ndata.shape","1b1cb89e":"# Encode target column since it is categorical and most learning algorithms require \n# numeric inputs (M)alignant = 1, (B)enign = 0\nlabelencoder_diagnosis = LabelEncoder()\ndata.diagnosis = labelencoder_diagnosis.fit_transform(data.diagnosis)","dfb800f3":"data.diagnosis","703a9918":"sc = StandardScaler()\nfor name in data.columns:\n    if name != 'diagnosis':\n        data[[name]] = sc.fit_transform(data[[name]])\n","eec8d6fb":"data.head","4b5bb556":"X = data.iloc[:, 1:]\ny = data.iloc[:, 0]","cf64e610":"X","f59ea68b":"y","7ebbd61e":"from sklearn.model_selection import StratifiedShuffleSplit \nfrom sklearn.model_selection import learning_curve\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier","d4f4a91c":"def plot_learning_curve(estimator, title, X, y, ylim=None, \n                        cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Number of training examples\")\n    plt.ylabel(\"Accuracy Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n#     print(f\"\\nshape of train_score is : {train_scores.shape}\\n\")\n    train_scores_mean = np.mean(train_scores, axis=1)\n    print(\"Training accuracy scores for different training sizes are:\\n{0}\".format(train_scores_mean))\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    print(\"Testing accuracy scores for different training sizes are:\\n{0}\".format(test_scores_mean))\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Testing score\")\n    plt.legend(loc=\"best\")\n    plt.show()\n    return plt","d9adcac9":"# 1 - Decision Tree\ntitle = \"Learning curves of Decision Tree\\n\" \\\n        \"Cross Validation of 10 splits\\n\" \\\n        \"Training-Test: 80-20%\"\ncv = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\nestimator = DecisionTreeClassifier()\nplot_learning_curve(estimator, title, X, y, ylim=(0.7, 1.01), cv=cv, n_jobs=-1)","cf0480f6":"# 2 - Multilayer Preceptron Network\ntitle = \"Learning Curve for Multi Layer Preceptron (MLP)\\n\" \\\n        \"Cross Validation of 10 splits\\n\" \\\n        \"Training-Test: 80-20%\"\ncv = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\nestimator = MLPClassifier(random_state=42)\nplot_learning_curve(estimator, title, X, y, (0.7, 1.01), cv=cv, n_jobs=4)","0223ed4b":"# 3 - Adaboost\ntitle = \"Learning Curve for Adaboost\\n\" \\\n        \"Cross Validation of 10 splits,\\n\" \\\n        \"Training-Test: 80-20%, n_estimators: 50, learning_rate:1)\"\ncv = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\nestimator = AdaBoostClassifier(n_estimators=50, learning_rate=1, random_state=0)\nplot_learning_curve(estimator, title, X, y, (0.7, 1.01), cv=cv, n_jobs=4)","4a69eda1":"# 4 - K-Nearest Neighbor\ntitle = \"Learning Curves for K-Nearest Neighbor (KNN)\\n\" \\\n        \"Cross Validation of 10 splits\\n\" \\\n        \"Training-Test:80-20%, n_neighbors:5\"\ncv = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\nestimator = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\nplot_learning_curve(estimator, title, X, y, (0.7, 1.01), cv=cv, n_jobs=4)","0323e7c1":"# 5 - rbf Kernel - Support Vector Machines\ntitle = \"Learning Curves for Support Vector Machines (SVM)\\n\" \\\n        \"Cross Validation on 10 splits\\n\" \\\n        \"Training-Test:80-20%, kernel: rbf, gamma:0.001\"\ncv = StratifiedShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\nestimator = SVC(kernel='rbf', gamma=0.001, random_state=0)\nplot_learning_curve(estimator, title, X, y, (0.7, 1.01), cv=cv, n_jobs=4)","41ea69da":"# 6 - linear Kernel - Support Vector Machines\ntitle = \"Learning Curves for Support Vector Machines (SVM)\\n\" \\\n        \"Cross Validation of 10 splits\\n\" \\\n        \"Training-Test:80-20%, kernel: linear, gamma:0.001\"\ncv = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\nestimator = SVC(kernel='linear', gamma=0.001, random_state=0)\nplot_learning_curve(estimator, title, X, y, (0.7, 1.01), cv=cv, n_jobs=4)","326d487d":"The final preprocessing step is to Standardizing Features. I am using `sklearn.preprocessing.StandardScaler` to standardize features, which is to update their values to have a mean of zero and a standard deviation of 1.\nThis is important for machine learning algorithms that use Euclidian distance between two points in their computations like K-Nearest Neighbor, also useful for algorithms that use gradient descent such as Neural Networks.","2c1c0d91":"From this we see strong corelation between perimeter, area, and radius, we should be able to remove two of them, I will keep radius.","75c776a8":"### Yaser Marey\n##### October 15th, 2020\n\n****\n#### Hello, in this Notebook, I am applying different Machine Learning Algorithms to Breast Cancer Wisconsin Diagnostic data set to classify tumor to either malignant or benign based on data attributes of the patient record.\n\n#### The objective is to demonstrate how to use Scikit-Learn tools to compare different algorithms quickly, I am using different tools from `sklearn.model_selection` package such as `StratifiedShuffleSplit` and `learning_curve` and I am applying and also analyzing the behavior of: `AdaBoostClassifier`, `KNeighborsClassifier`,`MLPClassifier`,`SVC` and `DecisionTreeClassifier`\n\n#### My treatment for this task will follow three steps:\n\nStep 0 Basic Exploratory Analysis\n\nStep 1 Preprocess Data\n\nStep 2 Apply ML Algorithms and Plot Learning Curves\n    \n\n_one comment about terminology: I am using Validation and Testing here interchangeably from code to narrative description, I hope you don't find that confusing, I will fix this in a following update_","f948a5bf":"In addition to being balanced, most of the learning algorithms assume normal distribution of the data set attributes, let's look into that next:","9f32cdf5":"Dataset is slightly imbalanced. Some researchers debate that this shouldn't be a problem and the model should generalize will still generalize adequately however I usually find that imbalanced data leads to accuracy degradation where learners performance reflects the underlying class distribution, this problem is especially clear in the case of learners prone to overfitting such as Decision Tree and Neural Network, therefore while applying Cross-Validation I will make use of a special object from Scikit-Learn that performs Stratified resampling: `sklearn.model_selection.StratifiedShuffleSplit` and I will explain this further below.","45708e76":"The `sklearn.model_selection learning_curve` is a convenient method if you want to compare several models quickly.\n\nBut first, I am using another tool from Scikit-learn to shuffle and split the training data for Cross-Validation, that is: StratifiedShuffleSplit [[source](http:\/\/https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedShuffleSplit.html)]\n\n    cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n\nStratifiedShuffleSplit shuffles each time before splitting, and it splits n_splits time while preserving the percentage of samples of each class (label) as the original data in each of the training and test parts.    ","a97b0877":"First let's divide the data dataframe to two, X set of training samples, and y the corresponding labels.","14498ee2":"Good performance, more interestingly the gab between training and testing scores hints that we may be able to obtain better accuracy by regualrizing the model.\nNext is K-Nearest Neighbor, I select K = 5","bee6d145":"The graph shows a perfect accuracy of 1.0 on training while much lower best performance of  ~0.938 on testing, this implies high variance and overfitting problem. This problem is expected because Decision Tree learning algorithm is known to be expressive, and therefore can overfit quickly. It is clear also that adding more data is likely to enhance testing accuracy. Also simplifying the model by applying pruning technique such as limiting maximum depth or minimum number of samples per leaf is expected to improve accuracy on testing. ","41dc1342":"We see that all distributions are Gaussians, how convenient!\nNow we look at correlation among attributes strong correlation between attributes may suggest that we can remove one of them without affecting learning.","1f63119e":"After setting the Cross-Validation splits I am using learning_curve from sklearn.model_selection. [[source](http:\/\/https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.learning_curve.html)]\n\nlearning_curve provides a simple interface to the entire process of training, cross_validation, and visualization that we need to go through for each of the learning algorithms we want to assess.\n\nAn example of how to use this method is as the following:\n\n    train_sizes, train_scores, test_scores = learning_curve(\n                                                            # the estimator, which is an object that object type that implements the \u201cfit\u201d and \u201cpredict\u201d methods \n                                                            RandomForestClassifier(), \n                                                            # training set\n                                                            X, \n                                                            # training labels\n                                                            y,\n                                                            # ShuffleSplit instance\n                                                            cv=cv,\n                                                            # Evaluation metric\n                                                            scoring='accuracy',\n                                                            # 5 different sizes of the training set\n                                                            train_sizes=np.linspace(0.01, 1.0, 5) \n                                                            )\n\nThis method will call the clone the estimator object each time passing in a specific size of data according to train_sizes list, training is then split into training and test parts after being shuffled and according to the percentage specified in StratifiedShuffleSplit method. \n\nSince we have a n_splits = K Folds, we notice that the `learning_curve` method will return K readings for training_scores and for test_scores, so `test_scores.shape` will yield `(len(train_sizes), K)` therefore we take the mean of each row to calculate the mean of different scores from different splits for the same training sample size and plot that.\n\nAlso, I use `fill_between` to plot the area between train_scores_mean and the same +\/- training scores standard deviation:\n\n`plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")` ","e47d68b4":"The graph shows good accuracy on both training and testing compared to the results obtained from the best learning algorithm we have so far which is the Multi-layer Perceptron Neural Network. The gap between the two curves is narrow. All this implies a good fit to data, low variance and low bias.\n\nNow, we will look into SVM with two kernels `rbf` which assumes that data is not linearly separable, and `linear` which assumes linear separability of the data:","3637f274":"Nice performance indeed!, no clear symptmbs for over fitting or high variance. Accuracy on Testing is better compared to Decision Tree algorithm.\nNext I am trying AdaBoostClassifier which is an ensemble classifier that is configured out of a group of 50 weak learners.","65e0f3d8":"Now I use `sclearn.preprocessing.LabelEncoder` to encode categorical labels data ('M','B') to numeric values so that ML methods can handle them:","5f18389c":"# Step 1: Preprocess Data\n\nWe start by removing 'id', 'Unamed: 32' attributes, then checking the distribution of the data, following that we will standardize the attributes.","d1bdc571":"#### From those commands output and by cross-checking with the dataset description from the data set provider we can confirm the following:\n\n1. We have a total of 569 records or samples.\n2. We have 33 attributes, one of them is particularly, not useful and has 0 values which is \"Unnamed : 32\" \n3. We have two other special features those are:\n    * id which is the id of the patient\n    * Diagnosis is whether (M for malignant, or B for benign )\n\n#### Then, we have ten different measurements applied to the cells from each patient:\n\n    1. radius\n    2. texture               \n    3. perimeter             \n    4. area                  \n    5. smoothness            \n    6. compactness           \n    7. concavity             \n    8. concave points        \n    9. symmetry              \n    10. fractal_dimension     \n\n4. The mean (of the three largest values), worst measurement, and standard deviation are computed for each of these measurements resulting in 30 computed attributes for each patient all are real numbers.\n\nNow we are ready to preprocess the data.","d74ed738":"For performance comparison, I produced the learning curves for each of the algorithms. Learning curves show performance on both training and testing data.  Performance is measured using prediction Accuracy. Accuracy is calculated as a function of a progressing number of training samples up to 100% of the training batch size. \n\nTraining batch is 80% of the full number of data set samples while testing is 20%. Also, the measurement is averaged over several iterations to obtain a smooth curve. This number of iterations varies from one algorithm taking into consideration the time complexity of each algorithm.\n\nTo achieve this task as with the least amount of code, I am using a set of scikit-learn tools:\n\n    from sklearn.model_selection import ShuffleSplit\n    from sklearn.model_selection import learning_curve","16cbaec8":"# Step 0: Basic Exploratory Data Analysis\nWe perform basic exploratory data analysis or EDA. \nBasic pandas commands such as head, tail, describe and info are really all that we need here:","9af0c693":"Clearly the dataset is linearly separable and therefore liner kernel achieves better results. The small gab and the close high accuracy on both testing and training indicate a good fit to the data, low variance, and low bias. The accuracy is on par with the best results we have so far from the Multi-Layer Perceptron Neural Network Algorithm."}}