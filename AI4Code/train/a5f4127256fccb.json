{"cell_type":{"289d30c4":"code","4a8e19b1":"code","638131b3":"code","26266594":"code","79105779":"code","0afa6982":"code","2e1b6178":"code","2f219946":"code","c8ee6ec5":"code","7ae93208":"code","9db0404a":"code","ae6b0caf":"code","d4d85acd":"code","80369590":"code","0ac2c34b":"markdown","71992b06":"markdown","acac8e75":"markdown","afe08036":"markdown","a745f17f":"markdown","1ab94776":"markdown","22a9c2a0":"markdown","ad6649d6":"markdown","dda3c90e":"markdown","639c06a6":"markdown"},"source":{"289d30c4":"#Import libraries.\n\nimport pandas as pd\nimport numpy as np\nfrom numpy import radians, cos, sin, arcsin, sqrt","4a8e19b1":"#Import raw bike and weather data.\n\ndf = pd.read_csv('..\/input\/cyclistic-data\/raw_data.csv',low_memory=False)\ndf1 = pd.read_csv('..\/input\/cyclistic-data\/weather_data.csv')","638131b3":"#Add columns to weather data.\n\ndf1['conditions'] = df1.Conditions.apply(lambda x: str(x.split(',')[0]))\ndf1['Date time'] = pd.to_datetime(df1['Date time'])\ndf1['month'] = df1['Date time'].dt.month","26266594":"#Rename weather columns for clarity.\n\ndf1.rename(columns = {'Date time':'date','Temperature':'temp','Relative Humidity':'humidity','Wind Chill':'wind_chill'},inplace=True)\n\n#Keep specific columns that may influence users.\n\ndf1 = df1[['date','month','temp','wind_chill','conditions','humidity']]\n\ndf1.isnull().sum()","79105779":"#Create dataframe where we replace missing windchill values with temperature values.\n\ndf1temp = df1[df1.wind_chill.isnull()]\ndf1temp.wind_chill = df1temp.temp\n\n#Drop missing windchill values from original dataset.\n\ndf1.dropna(subset=['wind_chill'],how='any',inplace=True)\n\n#Combine the two weather datasets to form one dataset without missing values\n\ndf1 = pd.concat([df1,df1temp])\n\n#Check\n\ndf1.isnull().sum()","0afa6982":"#Drop and rename selected columns.\n\ndf.drop(['Unnamed: 0'],axis=1,inplace=True)\ndf.rename(columns = {'rideable_type':'bike_type','member_casual':'user_type'},inplace=True)\n\n#Convert datatypes of two columns.\n\ndf.started_at = pd.to_datetime(df.started_at)\ndf.ended_at = pd.to_datetime(df.ended_at)\n\n#Create new columns.\n\ndf['ride_time'] = df.ended_at - df.started_at\ndf['month'] = df.started_at.dt.month\ndf['start_hour'] = df.started_at.dt.hour\ndf['day_of_week'] = df.started_at.dt.day_name()","2e1b6178":"#Haversine formula. This formula calculates distance between two points on a oblong sphere (the category that our planet falls under).\n\ndef haversine(lon1, lat1, lon2, lat2):\n    \"\"\"\n    Calculate the great circle distance between two points \n    on the earth (specified in decimal degrees)\n    \"\"\"\n\n    #Convert decimal degrees to Radians:\n    lon1 = np.radians(lon1.values)\n    lat1 = np.radians(lat1.values)\n    lon2 = np.radians(lon2.values)\n    lat2 = np.radians(lat2.values)\n\n    #Implementing Haversine Formula: \n    dlon = np.subtract(lon2, lon1)\n    dlat = np.subtract(lat2, lat1)\n\n    a = np.add(np.power(np.sin(np.divide(dlat, 2)), 2),  \n                          np.multiply(np.cos(lat1), \n                                      np.multiply(np.cos(lat2), \n                                                  np.power(np.sin(np.divide(dlon, 2)), 2))))\n    c = np.multiply(2, np.arcsin(np.sqrt(a)))\n    r = 6371\n\n    return c*r","2f219946":"#Apply function and convert results from kilometers into miles.\n\nx = haversine(df.start_lng,df.start_lat,df.end_lng,df.end_lat)\n\n#Create additional columns.\n\ndf['distance_estimate'] = np.round(x*0.6213712,2)\ndf['mph_estimate'] = df.distance_estimate \/ (df.ride_time.dt.seconds \/ 3600)\ndf['returned_location'] = df.distance_estimate.apply(lambda x: 'same location' if x == 0  else 'new location')\ndf['date'] = df.started_at.dt.date","c8ee6ec5":"#Check for null values.\n\ndf.isnull().sum()","7ae93208":"#Check timeframe of dataset.\n\nprint(df.started_at.min())\nprint(df.started_at.max())","9db0404a":"#Reorder columns.\n\ndf = df[['date','ride_id', 'bike_type', 'started_at', 'ended_at', 'start_station_name',\n       'start_station_id', 'end_station_name', 'end_station_id', 'start_lat',\n       'start_lng', 'end_lat', 'end_lng', 'user_type', 'ride_time', 'month',\n       'start_hour', 'day_of_week', 'distance_estimate', 'mph_estimate',\n       'returned_location']]\n\n#Convert data type.\n\ndf.date = pd.to_datetime(df.date)\n\n#Check total amount of datapoints before data cleaning.\n\ndf.shape","ae6b0caf":"#Duplicate data check.\n\nprint('Duplicates?\\n')\nprint(df.drop_duplicates(keep=False).shape)\n\n#Remove data without ending location coordinates.\n\nprint('Ending Coordinates?\\n')\ndf.dropna(subset=['end_lat','end_lng'],how='any',inplace=True)\nprint(df.shape)\n\n#Check the min, max, and average of the dataset.\n\nprint('Ride min,max, and mean?\\n')\nprint(df.ride_time.agg(['min','max','mean']))\n\n#Check data points that contain negative time duration and datapoints longer than 1 day. \n\nprint('Less than 0 days or more than 1 day?\\n')\nts1 = pd.Timedelta('0 days')\nts2 = pd.Timedelta('1 days')\nprint(df[df.ride_time <= ts1].shape)\nprint(df[df.ride_time >= ts2].shape)","d4d85acd":"#Remove data with negative time length durations and data with more than one day of the bicycle rented.\n\ndf = df[df.ride_time >= ts1]\ndf = df[df.ride_time <= ts2]\nprint(df.shape)\n\n#Convert ride time into minutes rounded to nearest whole minute.\n\ndf.ride_time = df.ride_time.dt.total_seconds()\/60\ndf.ride_time = df.ride_time.round()\n\n#Remove rides that are less than 1 minute in duration.\ndf = df[df.ride_time >= 1]\nprint(df.shape)\n\n#Remove rides where estimated mph is over 85 miles per hour.\ndf = df[df.mph_estimate <= 85]\nprint(df.shape)","80369590":"#View timeframe of dataset.\n\ndf.started_at.agg(['min','max'])","0ac2c34b":"## Cleaning Bike Dataset. Part 2: Subraction\n\nLet's finish by removing data that appears unreliable. Here are items we should consider removing:\n- Duplicate data.\n    - Duplicate data points can potentially bias an analysis depending on how prevelant the duplicates are.\n- Data with start coordinates but no ending coordinates.\n    - Data with start coordinates but without ending coordinates could mean that the ride has not completed or that something about that bicycle's tracking device is not working correctly.\n- Unrealistic ride durations.\n    - Rides lasting over 1 day are infrequent and could be a mistake.\n    - Ride times cannot be negative because time machines haven't been invented yet.\n- Unrealistic ride speeds.\n    - According to the [I love bicycling website,](https:\/\/ilovebicycling.com\/average-bike-speed\/) a pro cyclist travels downhill from 69 - 81 MPH. \n\nThere are data points without start station names, end station names, or their corresponding identification to a station. This is okay because not all bikes are returned to station. Many of Cyclistic's bikes are waiting in a convenient location, just not the docked bike variety.","71992b06":"What's windchill and why is it relevant? \n\nWind chill is important because this is how the temperature feels to our body, which is relevant considering our data is based on users going outside to use a bicycle. The wind chill calculations are accurate for temperatures at or below 50 degrees farenheight and for windspeeds above 3 mph. Any readings with null values are readings that are higher than that temperature, in which case, the windchill is the same as the standard temperature reading. For days with temperatures above 50 degress, we can substitute the traditional temperature reading.","acac8e75":"This dataset has been cleaned to my requirements. If you wish to access it, [click here](https:\/\/www.kaggle.com\/ericramoscastillo\/clean-cyclistic-data?select=clean_bike_data.csv).","afe08036":"## Introduction\n\nIn this Jupyter Notebook, I will use Python to clean two datasets that will be used to perform [exploratory analysis](https:\/\/www.kaggle.com\/ericramoscastillo\/cyclistic-exploratory-analysis) and present [key findings](https:\/\/www.kaggle.com\/ericramoscastillo\/cyclistic-key-findings-and-recommendations) to stakeholders.\n\nThe raw datasets can be accessed [here](https:\/\/www.kaggle.com\/ericramoscastillo\/cyclistic-data?select=weather_data.csv).\nThe cleaned datasets can be accessed [here](https:\/\/www.kaggle.com\/ericramoscastillo\/clean-cyclistic-data?select=clean_bike_data.csv).\nThe oringal Cyclistic historical trip data was collected and aggregated from [here](https:\/\/divvy-tripdata.s3.amazonaws.com\/index.html). \nThe original weather data was collected and filtered for Chicago details from [Visual Crossing Weather Service](https:\/\/www.visualcrossing.com\/weather\/weather-data-services#\/login).","a745f17f":"## Cleaning Bike Dataset. Part 1: Addition\n\nFor this dataset, lets start by creating additional data based on what user data we have. Here are the columns we will be constructing:\n- Ride Duration\n- Month\n- Hour Ride Started\n- Day of Week\n- Distance Estimate\n- Speed Estimate\n- Return Location\n\nMost of these are self explanatory but I'll share my rationale about the last 3 columns. \n\n- Distance Estimate: The dataset provides starting and ending coordinates for each ride. This data does not included the route the user took, and thus not a comprehensive distance metric. However, starting and ending coordinates is a decent approximation. We will not put all our faith in this reading but with additional data this could be useful.\n- Speed Estimate: Using the distance estimate and the ride duration, we can calculate speed.\n- Return Lcoation: This is a boolean to see how frequently a bike is returned to the starting location. This could be useful in seeing if users among different bike types or station locations are more likely to return where they started the ride.","1ab94776":"We are finished cleaning the weather dataset. Let's now focus our attention on the raw bike dataset.","22a9c2a0":"## Cleaning Weather Data\n\nFor this dataset, I'm interested in temperature readings and weather conditions. This is because I believe those are the two largest influences over users choosing to ride on a given day. \n\nI will, however, take additional variables from this column to potentially explore in the [exploratory analysis](https:\/\/www.kaggle.com\/ericramoscastillo\/cyclistic-exploratory-analysis) phase. ","ad6649d6":"## Other  Considerations\n\nI was considering removing entries where the ride duration was less than 3 minutes or where the ride times exceeded 3 hours, but I have no proof that those are faulty data points. Moving into the [exploratory analysis](https:\/\/www.kaggle.com\/ericramoscastillo\/cyclistic-exploratory-analysis), let's remember this as some data may have outliers that effect the calculation of averages.","dda3c90e":"# Data Integrity\n\nI chose to use the Visual Crossing Weather dataset because they have a rigourous data collection process, detail how they document this data, and keep daily data formats uniform. To read more on their process, visit their [data documentaion page](https:\/\/www.visualcrossing.com\/resources\/documentation\/weather-data\/weather-data-documentation\/). \n\nThe Cyclistic bike data is more prone to error. This is due to a variety of factors, for example, all bicycles have data regarding their GPS coordinates. This means if a malfunction occurs in the GPS device, that portion of the data will be unreliable. Most of the cleaning in this document will pertain to this dataset.\n\nTo make my methodology more clear, I will include written text to provide more context.","639c06a6":"## Summary of Removed Data:\n\nI started with 4,073,561 data points and removed the following:\n- Removed duplicate data. 0 data points (0%)\n- Removed data without ending GPS coordinates. 5,037 data points removed (<1%)\n- Removed data with unreaslistic ride durations. 12,987 data points (<1%)\n- Removed data with ride durations less than 1 minute. 41,092 data points (~1%)\n- Removed data with estimated mph is greater than 85mph. 15 data points removed (<1%)\n\nI ended with 4,014,850, removing 58,711 data points. We removed ~1.5% of all original data."}}