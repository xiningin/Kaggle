{"cell_type":{"ce53ce56":"code","92654488":"code","751407b4":"code","498d5491":"code","af425e5d":"code","e159dc9c":"code","ebdf86f0":"code","ba1085a3":"code","da8c5525":"code","03355d4a":"code","c83769a1":"code","562c1b22":"code","b822bd1c":"code","135a4420":"code","58c83b61":"code","e5510bb4":"code","776e3b30":"code","d3aa9470":"code","1b0b2c2b":"code","5f710201":"code","7d9774b1":"code","9ef72b78":"code","8b23499c":"code","b3095882":"markdown","9df3e870":"markdown","307d42bc":"markdown","d5016270":"markdown","236fd0a6":"markdown","f4a8bdc7":"markdown","a12ac07a":"markdown","f77b392b":"markdown","1b4931b2":"markdown","94366c0e":"markdown","d6d0aaef":"markdown","a27cd46e":"markdown","75a7ce40":"markdown","134ede48":"markdown","cb2f98ec":"markdown"},"source":{"ce53ce56":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","92654488":"with open('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv') as f:\n    orig_sell_prices_df = pd.read_csv(f)\nprint(\"%0.2f GB\" % (orig_sell_prices_df.memory_usage().sum() \/ 2**30))","751407b4":"with open('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv') as f:\n    orig_calendar_df = pd.read_csv(f)\nprint(\"%0.4f GB\" % (orig_calendar_df.memory_usage().sum() \/ 2**30))","498d5491":"with open('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv') as f:\n    orig_sales_df = pd.read_csv(f)\nprint(\"%0.2f GB\" % (orig_sales_df.memory_usage().sum() \/ 2**30))","af425e5d":"with open('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv') as f:\n    orig_sample_sub_df = pd.read_csv(f, index_col='id')\nprint(\"%0.2f GB\" % (orig_sample_sub_df.memory_usage().sum() \/ 2**30))","e159dc9c":"# Convert Weekly sell price data to daily price data, matching the column format of orig_sales_df\ndef get_daily_sell_price():\n    sell_price_df = orig_sell_prices_df[['wm_yr_wk', 'sell_price']].copy()\n    sell_price_df.set_index(pd.Index(orig_sell_prices_df['item_id'] + '_' + orig_sell_prices_df['store_id'] + '_validation', name='id'), inplace=True)\n    daily_price_df = sell_price_df.join(orig_calendar_df.set_index('wm_yr_wk').d, on='wm_yr_wk')\n    # pivot_table() takes up to 50 seconds and lots of memory\n    res = daily_price_df.pivot_table(values='sell_price', index='id', columns='d')\n    sorted_columns = sorted(list(res.columns),key=lambda x: len(x))\n    return res[sorted_columns]\n\ndef get_daily_sales_vol():\n    sales_vol_df = orig_sales_df.set_index('id')\n    extra_cols = ['store_id', 'item_id','dept_id','cat_id','state_id']\n    saved_descriptive_columns = sales_vol_df[extra_cols].copy()\n    daily_sell_price = get_daily_sell_price()\n    sales_vol_df *= daily_sell_price\n    sales_vol_df[extra_cols] = saved_descriptive_columns\n    return sales_vol_df.reset_index()\n\n# NaN's represent days for which no id price was provided\norig_vol_df = get_daily_sales_vol()\norig_vol_df.head(10)\n","ebdf86f0":"# Change this to anything < d_1913 to generate validation set\nLAST_DAY_TRAINING_SET = 'd_1885'\nGENERATE_SUBMISSION_CSV = False\nMAX_VALIDATION_DAYS = 28\n\n\"\"\"\nWhen GENERATE_SUBMISSION_CSV == True and LAST_DAY_TRAINING_SET == d_1913:\n    submission.csv for Public LB is created\n    Private LB submissions are currently not handled.\n\"\"\"\n\ndef get_df_up_to_column(df, col):\n    col_idx = df.columns.tolist().index(col)\n    return df.iloc[:,slice(col_idx+1)].copy()\n\ndef get_validation_df(last_train_day):\n    valid_start_idx = orig_sales_df.columns.tolist().index(last_train_day) + 1\n    valid_df = None\n    num_valid_days = min([len(orig_sales_df.columns) - valid_start_idx, MAX_VALIDATION_DAYS])\n    if num_valid_days > 0:\n        valid_df =  orig_sales_df.iloc[:,slice(valid_start_idx, valid_start_idx+num_valid_days)]\n        valid_df.columns = ['F' + str(i) for i in range(1,num_valid_days+1)]\n        valid_df.index = orig_sales_df.id.values\n    return valid_df\n\ndef get_predictions_df(validation_df):\n    if validation_df is None or MAX_VALIDATION_DAYS == 0:\n        return orig_sample_sub_df.copy()\n    num_pred_cols = min([len(validation_df.columns), MAX_VALIDATION_DAYS])\n    res = orig_sample_sub_df.copy()\n    col_max = len(res.columns)\n    if num_pred_cols > col_max:\n        for col_idx in range(col_max, num_pred_cols+1):\n            res['F' + str(col_idx)] = 0\n    return res.iloc[:,slice(num_pred_cols)]\n\ndef set_last_training_day(last_train_day):\n    sales_df = get_df_up_to_column(orig_sales_df, last_train_day)\n    vol_df = get_df_up_to_column(orig_vol_df, last_train_day)\n    valid_df = get_validation_df(last_train_day) # None if last_train_day = d_1913\n    predictions_df = get_predictions_df(valid_df)\n\n    return sales_df, vol_df, valid_df, predictions_df\n\nprice_df = orig_sell_prices_df.copy()\ncal_df = orig_calendar_df.copy()\nsales_df, vol_df, valid_df, predictions_df = set_last_training_day(LAST_DAY_TRAINING_SET)\nsales_df.head(10)","ba1085a3":"d_cols = [col for col in sales_df.columns if 'd_' in col]\ntotal_sales_df = sales_df.set_index('id')[d_cols].sum(axis=1)\nzero_sales_idx = total_sales_df[total_sales_df == 0].index\nif len(zero_sales_idx) > 0:\n    sales_df = sales_df.set_index('id').drop(zero_sales_idx).reset_index()\n    vol_df = vol_df.set_index('id').drop(zero_sales_idx).reset_index()\n    valid_df = valid_df.drop(zero_sales_idx)\n    predictions_df = predictions_df.drop(zero_sales_idx)\n\nprint('Removed ' + str(len(zero_sales_idx)) + ' rows with zero sales.')","da8c5525":"from IPython.display import display\n\nif valid_df is not None:\n    display(valid_df.head())\nelse:\n    print('No validation set has been created. Make sure that (LAST_DAY_TRAINING_SET < d_1913) to generate.')","03355d4a":"LAST_N_DAYS_USED_FOR_WEIGHTING_RMSSE = 28 # from M5 Competitors Guide\nNUM_AGGREGATION_LEVELS = 12 # from M5 Competitors Guide\ndef get_vol_based_weights(vol_df, gb):\n    # Aggregate at level gb\n    res = vol_df.groupby(gb).sum()\n    # Only use last N days according to guide\n    res = res[res.columns[-LAST_N_DAYS_USED_FOR_WEIGHTING_RMSSE:]].sum(axis=1)\n    # Turn sales volume into % of aggregation level\n    res \/= res.sum()\n    # Each aggregation level counts evenly (1\/NUM_AGGREGATION_LEVELS) towards final score \n    res \/= NUM_AGGREGATION_LEVELS\n    res.name = 'Weight'\n    return res.reset_index()\n\ndef get_rmsse_denominators(unit_sales_df, gb):\n    d_cols = [col for col in unit_sales_df.columns if 'd_' in col]\n    # Aggregate at level gb\n    agg_sales = unit_sales_df.groupby(gb).sum(min_count=1)[d_cols]\n    # (Y(t) - Y(t-1))\n    res = agg_sales - agg_sales.shift(1,axis=1)\n    # (Y(t) - Y(t-1)) ** 2\n    res = res ** 2\n    # (1 \/ (N-1)) * SUM((Y(t) - Y(t-1)) ** 2)\n    res = (1 \/ (agg_sales.notna().sum(axis=1) -1)) * res.sum(axis=1)\n    res.name = 'rmsse_denom'\n    return res.reset_index()\n\ndef get_rmsse_numerators(error_df, gb):\n    # Get h, the # of day columns \n    # Should be 28 unless validation set contains fewer days.\n    num_pred_days = sum(['F' == col[0] for col in error_df.columns])\n    # Aggregate at level gb\n    error_agg = error_df.groupby(gb).sum()\n    # (1 \/ h) * Sum(Y(t) - Y^(t))\n    res = (1 \/ num_pred_days) * (error_agg ** 2).sum(axis=1)\n    res.name = 'rmsse_numerator'\n    return res.reset_index()\n\n# Applies func to each level of gb_cols\n# Returns DataFrame of combined results\ndef apply_heirarchy_ts(df, func):\n    all_rows = pd.Index([True]*len(df), name='all_rows')\n    gb_cols = [all_rows,   # Level 1\n               'state_id', # Level 2\n               'store_id', # Level 3\n               'cat_id',   # Level 4\n               'dept_id',  # Level 5\n               ['state_id', 'cat_id'],  # Level 6\n               ['state_id', 'dept_id'], # Level 7\n               ['store_id', 'cat_id'],  # Level 8\n               ['store_id', 'dept_id'], # Level 9\n               'item_id',               # Level 10\n               ['item_id', 'state_id'], # Level 11\n               ['item_id', 'store_id']] # Level 12\n\n    idx_cols = ['all_rows','state_id','store_id','cat_id','dept_id','item_id']\n    return pd.concat([func(df, gb) for gb in gb_cols], sort=True).set_index(idx_cols)\n\n","c83769a1":"wrmsse_weights = apply_heirarchy_ts(vol_df, get_vol_based_weights)\n","562c1b22":"\"\"\"\" Note that the denominator of RMSSE is computed only for the time-periods\n     for which the examined product(s) are actively sold, i.e., the periods \n     following the first non-zero demand observed for the series under evaluation.\n\"\"\"\nrmsse_contribution = sales_df.notna()\nrmsse_contribution[d_cols] = sales_df[d_cols].replace(0, np.nan).ffill(axis=1).notna()\n\nrmsse_denom = apply_heirarchy_ts(sales_df[rmsse_contribution], get_rmsse_denominators)\n","b822bd1c":"import datetime as dt\n\ndef get_rmsse_df(actual, predictions):\n    pred_error = sales_df[['state_id','store_id','cat_id','dept_id','item_id', 'id']].join(actual - predictions, on='id')\n    rmsse_numerator = apply_heirarchy_ts(pred_error, get_rmsse_numerators)\n    rmsse_df = wrmsse_weights.join(rmsse_denom)\n    return rmsse_df.join(rmsse_numerator)\n\ndef score_wrmsse(actual, predictions):\n    rmsse_final = get_rmsse_df(actual, predictions)\n    # w(i) * sqrt((RMSSE_NUM \/ RMSSE_DENOM))\n    rmsse_final['wrmsse_contribution'] = rmsse_final['Weight'] * ((rmsse_final['rmsse_numerator'] \/ rmsse_final['rmsse_denom']) ** .5)\n    return rmsse_final, rmsse_final['wrmsse_contribution'].sum()\n\ndef predict_wrmsse(preds):\n    if GENERATE_SUBMISSION_CSV:\n        print('Generating submission.csv.')\n        with open('submission_' + dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '.csv', 'w') as f:\n            preds.to_csv(f)\n\n    if valid_df is None:\n        return None, None\n    return score_wrmsse(valid_df, preds)\n","135a4420":"# Time Series df\nts_df = sales_df.set_index('id').copy()\n# Mask for time series data points that are actually used in RMSSE computation\nts_days_counted_msk = ts_df[d_cols].replace(0, np.nan).ffill(axis=1).notna()","58c83b61":"def get_most_recent_ts_predictions():\n    preds_df = predictions_df.copy()\n    preds_df.loc[ts_df.index,:] = ts_df.iloc[:,[-1]].values\n    return preds_df","e5510bb4":"def get_pred_last_week():\n    preds_df = predictions_df.copy()\n    last_week_sales = ts_df.iloc[:,slice(-7,len(ts_df.columns))]\n    preds_df.loc[ts_df.index] = [list(w) * (len(preds_df.columns) \/\/ 7) + list(w[:(len(preds_df.columns) % 7)]) for w in last_week_sales.values]\n    return preds_df\n","776e3b30":"# Exponential Smoothing Helper Functions\ndef smooth(x, alpha, res):\n    if x.name == 0:\n        return x\n    res[x.name] = x*alpha + res[x.name - 1].fillna(x)*(1-alpha)\n\ndef get_exp_smoothed_df(df, alpha, smooth_func):\n    res = df.copy()\n    res.columns = list(range(len(df.columns)))\n    func = lambda x: smooth_func(x, alpha, res)\n    res.apply(func)\n    res.columns = df.columns\n    return res\n","d3aa9470":"def get_simple_exp_smoothing(alpha):\n    preds_df = predictions_df.copy()\n    smoothed_df = get_exp_smoothed_df(ts_df[ts_days_counted_msk].loc[:,d_cols], alpha, smooth)\n    preds_df.loc[ts_df.index,:] = [list(i)*len(preds_df.columns) for i in smoothed_df.iloc[:,[-1]].values]\n    return preds_df\n","1b0b2c2b":"# Helper function\ndef get_min_mse_rolling_window(df, rolling_window_options):\n    min_mse = None\n    window_res = None\n    for option in rolling_window_options:\n        sq_error = (df.rolling(option, axis=1).mean().shift(axis=1) - df) ** 2\n        total_mse = sq_error.mean(axis=1).sum()\n        if min_mse is None or total_mse < min_mse:\n            min_mse = total_mse\n            window_res = option\n    return window_res","5f710201":"def get_simple_moving_avg_predictions(rolling_window=None):\n    preds_df = predictions_df.copy()\n    min_mse = None\n    if rolling_window is None:\n        rolling_window = get_min_mse_rolling_window(ts_df[ts_days_counted_msk].loc[:,d_cols], [2,3,4,5])\n    preds_df.loc[ts_df.index,:] = [[i]*len(preds_df.columns) for i in ts_df.iloc[:,slice(-rolling_window,len(ts_df.columns))].mean(axis=1).tolist()]\n    return preds_df\n","7d9774b1":"# Helper Functions\ndef smooth_croston(x, alpha, res):\n    if x.name == 0:\n        return x\n    df = res[x.name - 1].copy()\n    df[res[x.name] > 0] = x*alpha + res[x.name - 1].fillna(x)*(1-alpha)\n    res[x.name] = df\n\ndef applyNumDays(x, df):\n    if x.name == 0:\n        return x\n    df[x.name] = (df[x.name] * (df[x.name-1] + 1)).fillna(df[x.name])\n    return df[x.name]\n\ndef get_num_days_since_last_sale(df):\n    msk_df = df.copy()\n    msk_df += 1\n    msk_df[msk_df > 1] = 0\n    msk_df.columns = list(range(len(msk_df.columns)))\n    num_days_func = lambda x: applyNumDays(x, msk_df)\n    msk_df = msk_df.apply(num_days_func).shift(axis=1)\n    msk_df.columns = df.columns\n    return msk_df","9ef72b78":"def get_crost_method(alpha):\n    preds_df = predictions_df.copy()\n    msk_df = ts_df[ts_days_counted_msk].loc[:,d_cols]\n    num_days = get_num_days_since_last_sale(msk_df) + 1\n    num_days = num_days.fillna(1)[ts_days_counted_msk]\n    num_days[msk_df == 0] = 0\n    num_days = get_exp_smoothed_df(num_days, alpha, smooth_croston)\n    smoothed_df = get_exp_smoothed_df(msk_df, alpha, smooth_croston)\n    smoothed_df \/= num_days\n    preds_df.loc[ts_df.index,:] = [list(i)*len(preds_df.columns) for i in smoothed_df.iloc[:,[-1]].values]\n    return preds_df\n","8b23499c":"preds = get_pred_last_week()\nwrmsse_df, wrmsse = predict_wrmsse(preds)\nif wrmsse is not None:\n    print('WRMSSE:', wrmsse)\n    display(wrmsse_df.head(10))\n","b3095882":"## 2) sNaive\n\n| WRMSSE | Training Set | Validation Set |\n|------|------|----|\n| 0.86967 | d_1 - d_1913 | **(Public LB)** d_1914 - d_1941 |\n| 0.92262 | d_1 - d_1885 | d_1886 - d_1913 |\n| 1.24013 | d_1 - d_1400 | d_1401 - d_1500 |\n\nUse last week of sales as prediction for future weeks.","9df3e870":"## 4) Moving Average\n\n| WRMSSE | Training Set | Validation Set |\n|------|------|----|\n| 1.10694 | d_1 - d_1913 | **(Public LB)** d_1914 - d_1941 |\n| 1.17943 | d_1 - d_1885 | d_1886 - d_1913 |\n| 1.39181 | d_1 - d_1400 | d_1401 - d_1500 |","307d42bc":"## Remove ids that have 0 sales in training period\nIf LAST_DAY_TRAINING_SET <= d_1845, at least 1 id will have zero sales in training dataset.\n\nThese rows are removed from all df's to simplify the scoring process.","d5016270":"# Precompute RMSSE Denominators and WRMSSE Volume Weights \n\n### RMSSE Calculation:\n![image.png](attachment:image.png)\nThe Denominator of this equation is only dependent on the values of the training set, so it can be precomputed for faster iteration.\n\n### Volume-based weights (from Competitors Guide):\n\n> \"The forecasting errors computed for each participating method (both RMSSE and SPL) will be weighted across the M5 series based on their **cumulative actual dollar sales**, which is a good and objective proxy of their actual value for the company in monetary terms. The cumulative dollar sales will be computed using the **last 28 observations** of the training sample (sum of units sold multiplied by their respective price), i.e., a period equal to the forecasting horizon. Note that since both the number of units being sold and their respective price change through time, this estimation is based on the sum of the corresponding daily dollar sales.\"\n\n![Screen%20Shot%202020-05-10%20at%202.34.06%20PM.png](attachment:Screen%20Shot%202020-05-10%20at%202.34.06%20PM.png)\n\nSide Note for items with zero sales at beginning of time series:\n\n> \"Note that the denominator of RMSSE is computed only for the time-periods for which the examined product(s) are actively sold, i.e., the periods following the first non-zero demand observed for the series under evaluation.\"","236fd0a6":"## 5) Croston (alpha = .1)\n\n| WRMSSE | Training Set | Validation Set |\n|------|------|----|\n| 1.05648 | d_1 - d_1913 | **(Public LB)** d_1914 - d_1941 |\n| 1.13238 | d_1 - d_1885 | d_1886 - d_1913 |\n| 1.26183 | d_1 - d_1400 | d_1401 - d_1500 |","f4a8bdc7":"# Execute Chosen Prediction Method","a12ac07a":"## 1) Naive\n\n| WRMSSE | Training Set | Validation Set |\n|------|------|----|\n| 1.46378 | d_1 - d_1913 | **(Public LB)** d_1914 - d_1941 |\n| 1.4862651683366184 | d_1 - d_1885 | d_1886 - d_1913 |\n| 1.6850237352357273 | d_1 - d_1400 | d_1401 - d_1500 |","f77b392b":"# Create Daily Volume dataframe\n\nvolume = sell_price_daily * orig_sales_df\n\n*sell_prices.csv* provides **weekly prices**, but we need **daily prices**","1b4931b2":"# Load inputs into orig_* dataframes\norig_* dataframes should remain unchanged.\n\n- ~60 seconds initial setup time\n- Involves loading csvs into dataframes (10 seconds) and creating Daily Sales Volume dataframe (50 seconds)\n- This only needs to run once.","94366c0e":"# Create Training and Validation set\n| LAST_DAY_TRAINING_SET | Validation Set Days | Validation Set Columns |\n|------|------|----|\n| d_1913 | None | None |\n| d_1912 | {d_1913} | {F1} |\n| d_1911 | {d_1912, d_1913} | {F1, F2} |\n| d_1910 | {d_1911, d_1912, d_1913} | {F1, F2, F3} |\n| ... | ... | ... | ... |\n| d_1885 | {d_1886, d_1887, ..., d_1913} | {F1, F2, ..., F28} |\n| d_1884 | {d_1885, d_1886, ..., d_1912} | {F1, F2, ..., F28} |\n| d_1883 | {d_1884, d_1885, ..., d_1911} | {F1, F2, ..., F28} |\n\nwhen MAX_VALIDATION_DAYS = 28","d6d0aaef":"## 3) Simple Exponential Smoothing (alpha=.1)\n\n| WRMSSE | Training Set | Validation Set |\n|------|------|----|\n| 1.06912 | d_1 - d_1913 | **(Public LB)** d_1914 - d_1941 |\n| 1.11068 | d_1 - d_1885 | d_1886 - d_1913 |\n| 1.19542 | d_1 - d_1400 | d_1401 - d_1500 |","a27cd46e":"# M5 Walmart Sales Forcasting competition\n### Validation Set WRMSSE for Statistical Benchmarks\n\n[M5 Participants Guide](https:\/\/mofc.unic.ac.cy\/m5-competition\/)\n\nThis notebook enables the following:\n1. Selecting validation sets with arbitrary start date and number of days.\n2. Scoring these validation sets with \"Weight Root Mean Squared Scaled Error\" WRMSSE (as described in M5 Participants Guide).\n3. Implementing and testing Statistical Benchmarks #1 - #5.","75a7ce40":"## From M5 Competitors Guide\n| Level id | Aggregation Level   | Number of Series  |\n|------|------|----|\n| 1 | Unit sales of all products, aggregated for all stores\/states | 1 |\n| 2 | Unit sales of\u00a0all products, aggregated for each State | 3 |\n| 3 | Unit sales of all products, aggregated for each store\u00a0 | 10 |\n| 4 | Unit sales of\u00a0all products, aggregated for each category | 3 |\n| 5 | Unit sales of\u00a0all products, aggregated for each department | 7 |\n| 6 | Unit sales of\u00a0all products, aggregated for each State and category | 9 |\n| 7 | Unit sales of\u00a0all products, aggregated for each State and department | 21 |\n| 8 | Unit sales of\u00a0all products, aggregated for each store and category | 30 |\n| 9 | Unit sales of\u00a0all products, aggregated for each store and department | 70 |\n| 10 | Unit sales of product x, aggregated for all stores\/states | 3,049 |\n| 11 | Unit sales of product x, aggregated for each State | 9,147 |\n| 12 | Unit sales of product x, aggregated for each store | 30,490 |\n| Total | - | 42,840 |","134ede48":"# Validation Set values (valid_df)","cb2f98ec":"# Statistical Benchmarks"}}