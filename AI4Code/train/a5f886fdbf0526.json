{"cell_type":{"b41d7994":"code","bca9fa70":"code","263f197a":"code","c912f459":"code","4fe30636":"code","47ad39b8":"code","3164a7b2":"code","be909fb2":"code","06b6ebe4":"code","e512a211":"code","77b381e4":"code","a027faf6":"code","41d0bfa2":"code","5f02c393":"code","b5505d06":"code","fe67578e":"code","0b953249":"code","2da27e48":"code","3b9d8baa":"code","2b64d097":"code","e4b6f9be":"code","ae688a70":"code","d36ce5b3":"code","cf92ff98":"markdown","e3b43620":"markdown","1040f53c":"markdown","d3e0f755":"markdown","d7a03dc7":"markdown","ed7b5db0":"markdown","68987e6b":"markdown","87eb763d":"markdown","7201ee46":"markdown","ef54bd11":"markdown","d9b8e091":"markdown","de639744":"markdown","69d4dbff":"markdown","40115074":"markdown","20de9b48":"markdown","48f960ba":"markdown","0be4dfa7":"markdown"},"source":{"b41d7994":"! conda install -c conda-forge gdcm -y","bca9fa70":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport pydicom\nimport glob\nfrom tqdm.notebook import tqdm\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport matplotlib.pyplot as plt\nfrom skimage import exposure\nimport cv2\nimport warnings\nfrom fastai.vision.all import *\nfrom fastai.medical.imaging import *\nwarnings.filterwarnings('ignore')","263f197a":"dataset_path = Path('..\/input\/siim-covid19-detection')","c912f459":"dataset_path.ls()","4fe30636":"train_study_df = pd.read_csv(dataset_path\/'train_study_level.csv')","47ad39b8":"train_study_df.head()","3164a7b2":"study_classes = ['Negative for Pneumonia', 'Typical Appearance', 'Indeterminate Appearance', 'Atypical Appearance']\nnp.unique(train_study_df[study_classes].values, axis=0)","be909fb2":"plt.figure(figsize = (10,5))\nplt.bar([1,2,3,4], train_study_df[study_classes].values.sum(axis=0))\nplt.xticks([1,2,3,4],study_classes)\nplt.ylabel('Frequency')\nplt.show()","06b6ebe4":"train_image_df = pd.read_csv(dataset_path\/'train_image_level.csv')","e512a211":"train_image_df.head()","77b381e4":"train_image_df['split_label'] = train_image_df.label.apply(lambda x: [x.split()[offs:offs+6] for offs in range(0, len(x.split()), 6)])","a027faf6":"classes_freq = []\nfor i in range(len(train_image_df)):\n    for j in train_image_df.iloc[i].split_label: classes_freq.append(j[0])\nplt.hist(classes_freq)\nplt.ylabel('Frequency')","41d0bfa2":"bbox_areas = []\nfor i in range(len(train_image_df)):\n    for j in train_image_df.iloc[i].split_label:\n        bbox_areas.append((float(j[4])-float(j[2]))*(float(j[5])*float(j[3])))\nplt.hist(bbox_areas)\nplt.ylabel('Frequency')","5f02c393":"def dicom2array(path, voi_lut=True, fix_monochrome=True):\n    dicom = pydicom.read_file(path)\n    # VOI LUT (if available by DICOM device) is used to\n    # transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n    return data\n        \n    \ndef plot_img(img, size=(7, 7), is_rgb=True, title=\"\", cmap='gray'):\n    plt.figure(figsize=size)\n    plt.imshow(img, cmap=cmap)\n    plt.suptitle(title)\n    plt.show()\n\n\ndef plot_imgs(imgs, cols=4, size=7, is_rgb=True, title=\"\", cmap='gray', img_size=(500,500)):\n    rows = len(imgs)\/\/cols + 1\n    fig = plt.figure(figsize=(cols*size, rows*size))\n    for i, img in enumerate(imgs):\n        if img_size is not None:\n            img = cv2.resize(img, img_size)\n        fig.add_subplot(rows, cols, i+1)\n        plt.imshow(img, cmap=cmap)\n    plt.suptitle(title)\n    plt.show()","b5505d06":"dicom_paths = get_dicom_files(dataset_path\/'train')\nimgs = [dicom2array(path) for path in dicom_paths[:4]]\nplot_imgs(imgs)","fe67578e":"num_images_per_study = []\nfor i in (dataset_path\/'train').ls():\n    num_images_per_study.append(len(get_dicom_files(i)))\n    if len(get_dicom_files(i)) > 5:\n        print(f'Study {i} had {len(get_dicom_files(i))} images')\n    ","0b953249":"plt.hist(num_images_per_study)","2da27e48":"def image_path(row):\n    study_path = dataset_path\/'train'\/row.StudyInstanceUID\n    for i in get_dicom_files(study_path):\n        if row.id.split('_')[0] == i.stem: return i \n        \ntrain_image_df['image_path'] = train_image_df.apply(image_path, axis=1)","3b9d8baa":"imgs = []\nimage_paths = train_image_df['image_path'].values\n\n# map label_id to specify color\nthickness = 10\nscale = 5\n\n\nfor i in range(8):\n    image_path = random.choice(image_paths)\n    print(image_path)\n    img = dicom2array(path=image_path)\n    img = cv2.resize(img, None, fx=1\/scale, fy=1\/scale)\n    img = np.stack([img, img, img], axis=-1)\n    for i in train_image_df.loc[train_image_df['image_path'] == image_path].split_label.values[0]:\n        if i[0] == 'opacity':\n            img = cv2.rectangle(img,\n                                (int(float(i[2])\/scale), int(float(i[3])\/scale)),\n                                (int(float(i[4])\/scale), int(float(i[5])\/scale)),\n                                [255,0,0], thickness)\n    \n    img = cv2.resize(img, (500,500))\n    imgs.append(img)\n    \nplot_imgs(imgs, cmap=None)","2b64d097":"submission_df = pd.read_csv(dataset_path\/'sample_submission.csv')","e4b6f9be":"submission_df.head()","ae688a70":"submission_df.iloc[2000:2010]","d36ce5b3":"submission_df.to_csv('submission.csv', index=False)","cf92ff98":"We can see we have to provide the study-level class label. These will be of the format `[class]` `1 0 0 1 1`","e3b43620":"# How to submit\n\nLet's now go over the `sample_submission.csv` file so we know how to submit our predictions.\n\nBefore we do so, it's worth reminding ourselves that this is a code-only competition, meaning that your submission file has to be generated in a script\/notebook. The `sample_submission.csv` file demonstrated what kind of file needs to be produced:","1040f53c":"# SIIM-FISABIO-RSNA COVID-19 Detection: A simple EDA \n\nIn this competition, we are provided with DICOM images of chest X-ray radiographs, and we are asked to identify and localize COVID-19 abnormalities. This is important because typical diagnosis of COVID-19 requires molecular testing (polymerase chain reaction) requires several hours, while chest radiographs can be obtained in minutes, but it is hard to distinguish between COVID-19 pneumonia and other other viral and bacterial pneumonias. Therefore, in this competition, be hope to develop AI that that eventually help radiologists diagnose the millions of COVID-19 patients more confidently and quickly.\n\nI'll provide a quick and simple EDA to help you get started with this very interesting competition!","d3e0f755":"We have our bounding box labels provided in the `label` column. The format is as follows:\n\n`[class ID] [confidence score] [bounding box]`\n\n* class ID - either `opacity` or `none`\n* confidence score - confidence from your neural network model. If none, the confidence is `1`.\n* bounding box - typical `xmin ymin xmax ymax` format. If class ID is none, the bounding box is `1 0 0 1 1`.\n\nThe bounding boxes are also provided in easily readable dictionary format in column `boxes`, and the study that each image is a part of is provided in`StudyInstanceUID`.\n\nLet's quick look at the distribution of opacity vs none:","d7a03dc7":"# Imports\nLet's start out by setting up our environment by importing the required modules:","ed7b5db0":"Let's actually look at how many images are available per study:","68987e6b":"# A look at the images\n\nOkay, let's now look at some example images:","87eb763d":"Let's check what data is available to us:","7201ee46":"As you can see, at the study-level, we are predicting the following classes:\n* Negative for Pneumonia\n* Typical Appearance\n* Indeterminate Appearance\n* Atypical Appearance\n\nThis here is a standard multi-label classification problem. In the training set, interestingly they are not multi-label, but it is mentioned that:\n> Studies in the test set may contain more than one label.\n\nLet's look at the distribution:\n","ef54bd11":"Let's look at the unique labels:","d9b8e091":"Let's also look at the distribution of the bounding box areas:","de639744":"# A look at the provided data","69d4dbff":"We can see that we have:\n\n* `train_study_level.csv` - the train study-level metadata, with one row for each study, including correct labels.\n* `train_image_level.csv` - the train image-level metadata, with one row for each image, including both correct labels and any bounding boxes in a dictionary format. Some images in both test and train have multiple bounding boxes.\n* `sample_submission.csv` - a sample submission file containing all image- and study-level IDs.\n* `train` folder - comprises 6,334 chest scans in DICOM format, stored in paths with the form `study`\/`series`\/`image`\n* `test` folder - The hidden test dataset is of roughly the same scale as the training dataset.\n","40115074":"We also have to provide the image-level bounding box. These will be of the format `[class ID] [confidence score] [bounding box]` as described earlier.\n\nOf course, in both cases, you can have multi-label scenarios.","20de9b48":"# A look at the CSVs\n\nLet's check the `train_study_level.csv` file:","48f960ba":"That's it for now!\n\n**Please upvote if you found this helpful!**","0be4dfa7":"Let's now look at `train_image_level.csv`:"}}