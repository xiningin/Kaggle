{"cell_type":{"931db54c":"code","4fea5dce":"code","28ddd07d":"code","c46bd929":"code","0926bf84":"code","73284b48":"code","75c40526":"code","7bf1eeaa":"code","b3dda47e":"code","8a971373":"code","6d746d72":"code","0139ff52":"code","12bed73b":"code","09334865":"code","d7e86c79":"code","1edf1afa":"code","eaf4d341":"code","32605ac6":"code","16a7dce4":"code","a195368a":"code","b217d63b":"code","6e7670ac":"code","a4d01a76":"code","2c5175f8":"code","a5f5592a":"code","59d6d28c":"markdown","490807a1":"markdown","1a9ccdc5":"markdown","f5123db3":"markdown","650afbe6":"markdown","bcec7428":"markdown","72203c49":"markdown","78687e89":"markdown","f8c6d503":"markdown","05b69598":"markdown","62246cb2":"markdown","403d17ce":"markdown","4437f2f4":"markdown","ac04cb37":"markdown","b43995c2":"markdown","31af885f":"markdown","fe483669":"markdown","a515d40a":"markdown","e3e9c8c1":"markdown","b8fc6f2e":"markdown","ae09ae31":"markdown"},"source":{"931db54c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4fea5dce":"# text processing libraries\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\n# sklearn \nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# File system manangement\nimport os\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')","28ddd07d":"df_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv', dtype={'id': np.int16, 'target': np.int8})\ndf_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv', dtype={'id': np.int16})","c46bd929":"percent_missing = df_train.isnull().sum() * 100 \/ len(df_train)\npercent_missing","0926bf84":"percent_missing2 = df_test.isnull().sum() * 100 \/ len(df_test)\npercent_missing2","73284b48":"df_train['target'].value_counts(normalize=True)","75c40526":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=df_train[df_train['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='blue')\nax1.set_title('disaster tweets')\ntweet_len=df_train[df_train['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Words in a tweet')\nplt.show()","7bf1eeaa":"def create_corpus(target):\n    corpus=[]\n    \n    for x in df_train[df_train['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","b3dda47e":"from collections import defaultdict\nstop=set(stopwords.words('english'))\n\ncorpus=create_corpus(1)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n    \n\n\nx,y=zip(*top)\nplt.bar(x,y)","8a971373":"import re\nimport string\n\ndef clean_text_round1(text):\n    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\nround1 = lambda x: clean_text_round1(x)\n# Let's take a look at the updated text\ndf_train['text'] = pd.DataFrame(df_train['text'].apply(round1))\ndf_test['text'] = pd.DataFrame(df_test['text'].apply(round1))\ndf_train.head()","6d746d72":"# Apply a second round of cleaning\ndef clean_text_round2(text):\n    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n    text = re.sub('[\u2018\u2019\u201c\u201d\u2026]', '', text)\n    text = re.sub('\\n', '', text)\n    return text\n\nround2 = lambda x: clean_text_round2(x)\n# Let's take a look at the updated text\ndf_train['text'] = pd.DataFrame(df_train['text'].apply(round2))\ndf_test['text'] = pd.DataFrame(df_test['text'].apply(round2))\ndf_train.head()","0139ff52":"def clean_text_round3(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    return text\nround3 = lambda x: clean_text_round3(x)\n\n# Applying the cleaning function to both test and training datasets\ndf_train['text'] = pd.DataFrame(df_train['text'].apply(round3))\ndf_test['text'] = pd.DataFrame(df_test['text'].apply(round3))\ndf_train.head()","12bed73b":"# Tokenizing the training and the test set\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\ndf_train['text'] = df_train['text'].apply(lambda x: tokenizer.tokenize(x))\ndf_test['text'] = df_test['text'].apply(lambda x: tokenizer.tokenize(x))\ndf_train.head()","09334865":"def remove_stopwords(text):\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words\n\ndf_train['text'] = df_train['text'].apply(lambda x : remove_stopwords(x))\ndf_test['text'] = df_test['text'].apply(lambda x : remove_stopwords(x))\ndf_train.head()","d7e86c79":"from wordcloud import WordCloud, STOPWORDS\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(df_train[df_train[\"target\"]==1], title=\"Word Cloud of disaster tweets\")","1edf1afa":"plot_wordcloud(df_train[df_train[\"target\"]==0], title=\"Word Cloud of not real disaster tweets\")","eaf4d341":"def combine_text(list_of_text):\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ndf_train['text'] = df_train['text'].apply(lambda x : combine_text(x))\ndf_test['text'] = df_test['text'].apply(lambda x : combine_text(x))\ndf_train.head()","32605ac6":"def get_top_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\nplt.figure(figsize=(10,5))\ntop_bigrams=get_top_bigrams(df_train['text'])[:10]\nx,y=map(list,zip(*top_bigrams))\nsns.barplot(x=y,y=x)","16a7dce4":"count_vectorizer = CountVectorizer()\ntrain_vectors = count_vectorizer.fit_transform(df_train['text'])\ntest_vectors = count_vectorizer.transform(df_test['text'])\n\n## Keeping only non-zero elements to preserve space \nprint(train_vectors[0].todense())","a195368a":"tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\ntrain_tfidf = tfidf.fit_transform(df_train['text'])\ntest_tfidf = tfidf.transform(df_test[\"text\"])","b217d63b":"clf_NB = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB, train_vectors, df_train[\"target\"], cv=5, scoring=\"f1\")\nscores","6e7670ac":"clf_NB.fit(train_vectors, df_train[\"target\"])","a4d01a76":"# Fitting a simple Naive Bayes on TFIDF\nclf_NB_TFIDF = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB_TFIDF, train_tfidf, df_train[\"target\"], cv=5, scoring=\"f1\")\nscores","2c5175f8":"clf_NB_TFIDF.fit(train_tfidf, df_train[\"target\"])","a5f5592a":"def submission(submission_file_path,model,test_vectors):\n    sample_submission = pd.read_csv(submission_file_path)\n    sample_submission[\"target\"] = model.predict(test_vectors)\n    sample_submission.to_csv(\"submission.csv\", index=False)\nsubmission_file_path = \"..\/input\/nlp-getting-started\/sample_submission.csv\"\ntest_vectors=test_tfidf\nsubmission(submission_file_path,clf_NB_TFIDF,test_vectors)","59d6d28c":"Top Ngrams, I am analysing Bigrams only (n=2)","490807a1":"Word Clouds\nWord clouds are on the most population visulations method when it comes to text data. Its looks good and I quite like word clouds. They tells the stroy hidden in the text in a beautiful manner. \n\nSo, Lets create two word clouds. One for disaster tweets and another of Non Disaster tweets.","1a9ccdc5":"# TFIDF Features\n\nTerm Frequency: is a scoring of the frequency of the word in the current document.","f5123db3":"# Data Visualisation\nHow many words are in a tweet ?","650afbe6":"# Tokenization\nTokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords.[](http:\/\/)","bcec7428":"# Data Cleaning\nLets start with the cleaning.\n\nI would do the cleaning in couple of rounds.\n\n1st round - We will remove punctuations, and numbers, will convert into lowercase,remove any brackets, etc.\n\n2nd round - We will get rid of some additional punctatuions, and non-sensical text that we might have missed in the first round.\n\n3rd round - I will get rid of https, #, www. , etc.\n\nThis is just to explain how can we approach towards cleaning our text. All of these three rounds of cleaning can be done in just one round as well by writing one function only.","72203c49":"# Stopwords Removal\n\nWhen we deal with text problem in Natural Language Processing, stop words removal process is a one of the important step to have a better input for any models. Stop words means that it is a very common words in a language (e.g. a, an, the in English). It does not help on most of NLP problem such as semantic analysis, classification etc.\n\nWord tokenization and lemmatization are the essential part for removing stop words.","78687e89":"# Bag of Words - Countvectorizer\n\nIt converts a collection of text documents to a matrix of token counts.","f8c6d503":"I would say its a fair distribution- 57% and 43%. There are many instnaces where we might have to work with a data where target variable in hughly inbalanced but here Its fine. SO we don't need to worry much about the distribution of target variable","05b69598":"# NLP\nNLP, is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language. The objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable. So lets start.","62246cb2":"# Missing Values\nFirst thing first. Lets find out the quality of our data. We will check the missing values","403d17ce":"Most common stopwords","4437f2f4":"Here we are building a machine learning model that predicts which Tweets are about real disasters and which one\u2019s aren\u2019t. We have a dataset of  around 10000 tweets. So basically its a classification problem.\n\nFor this purpose we can use various algorithms like - \n\n# Logistic Regression\n\n# XGBoost\n\n# Naive Bayes.\n\nI will Naive Bayes algorithm here specifically; Thats the first algorithm i always use when i deal with something like this. Its a pretty good algorithm.\n\n# So What is Naive Bayes algorithm and how it works.--\n\nNa\u00efve Bayes algorithm is a supervised learning algorithm, which is based on Bayes theorem.\n\nIt is mainly used in text classification that includes a high-dimensional training dataset.\n\nIt is a probabilistic classifier, which means it predicts on the basis of the probability of an object.\n\n# Why is it called Na\u00efve Bayes?\n\nThe Na\u00efve Bayes algorithm is comprised of two words Na\u00efve and Bayes, Which can be described as:\n\nNa\u00efve: It is called Na\u00efve because it assumes that the occurrence of a certain feature is independent of the occurrence of other features. Such as if the fruit is identified on the bases of color, shape, and taste, then red, spherical, and sweet fruit is recognized as an apple. Hence each feature individually contributes to identify that it is an apple without depending on each other.\n\nBayes: It is called Bayes because it depends on the principle of Bayes' Theorem.\n\n# Bayes' Theorem:\nBayes' theorem is also known as Bayes' Rule or Bayes' law, which is used to determine the probability of a hypothesis with prior knowledge. It depends on the conditional probability.It states, for two events A & B, if we know the conditional probability of B given A and the probability of B, then it\u2019s possible to calculate the probability of B given A\n\nFormula for Bayes Theorem - ![image.png](attachment:image.png)\n","ac04cb37":"# Distribution of target variable.\nNow, Lets check the distribution of our target variable","b43995c2":"Here we are just taking a list of text and combines them into one large chunk of text.","31af885f":"# Modelling\n\nFitting a simple Naive Bayes on Count of words","fe483669":"Lets get all the important libraries we need in this task.","a515d40a":"# Import the data","e3e9c8c1":"........To be Continued\n\nPlease support by likes and upovotes. It will motivate me.","b8fc6f2e":"Now we have cleaned our text. We have remove almost all the noise from the data. Now question comes how to transform this data into something meaningful from which our machine can make some sense out it. There are various techniques to transform text into a meaningful vector of numbers. \n\nWe can use - \n\n1) Bag of word\n\n2) TFIDF Features\n","ae09ae31":"So, we have around 33% of the values missing in location column in both the datasets."}}