{"cell_type":{"fb443727":"code","3a62bc8f":"code","b030c6cb":"code","03d5e2df":"code","c1e218be":"code","45f6b48c":"code","2d352242":"code","ddb8bb2a":"code","f079ae76":"code","a1ab8ad0":"code","6b44ece3":"code","ccecc4cd":"code","99c705d8":"code","cd0277e8":"code","a92d65f4":"code","3baa815e":"code","fddbe118":"code","7a02c528":"code","0bdae45e":"code","ee309cd3":"code","ae850b6a":"code","b508491c":"code","5b4e8c21":"code","2d08d99e":"code","83e178f8":"code","c0da84e5":"code","04ffb517":"code","a9cc8a35":"code","cfcd4a25":"code","0e7f7534":"code","d505e3d1":"code","5f338799":"code","8bf00ab3":"code","7343c3ae":"code","0683313e":"code","9bcc2fd6":"code","1897d466":"code","10d0b1a1":"code","5769b6e4":"code","b9b5d660":"code","a965f82d":"code","b42838c7":"code","e71fb401":"code","70c81c22":"code","6e3e1f8f":"code","df86bdd7":"code","65661d09":"code","004fb79d":"markdown","f4d74fd7":"markdown","a7c211a8":"markdown","7cb29492":"markdown","be40c871":"markdown","2fc47e64":"markdown","805df549":"markdown","444dab28":"markdown","c6336df2":"markdown","8a92dc09":"markdown","aede7347":"markdown","84fc784d":"markdown","5ddd5ffb":"markdown","b08bd6b0":"markdown","215aaf06":"markdown","9b6b26ae":"markdown","e2faf1ae":"markdown","2c405255":"markdown","b1062c9d":"markdown","7b7f6c3f":"markdown","689c0de4":"markdown","0bc3a5f0":"markdown","8e6634fd":"markdown","51ef5b2b":"markdown","fc39adbb":"markdown","26014d00":"markdown","613abc35":"markdown","f0d7017f":"markdown","ac462eed":"markdown","56ce3675":"markdown"},"source":{"fb443727":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3a62bc8f":"pip install bs4\n","b030c6cb":"pip install distance","03d5e2df":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom subprocess import check_output\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport os\nimport gc \n\nimport re\nfrom nltk.corpus import stopwords\nimport distance\nfrom nltk.stem import PorterStemmer\nfrom bs4 import BeautifulSoup","c1e218be":"df =pd.read_csv('\/kaggle\/input\/quora-question-pairs\/train.csv.zip')\nprint(\"Number of data points :\",df.shape)","45f6b48c":"df.head()","2d352242":"df.info()","ddb8bb2a":"sns.countplot(data=df,x='is_duplicate')","f079ae76":"print('~> Total number of question pairs for training:\\n {}'.format(len(df)))","a1ab8ad0":"#df['is_duplicate'] contains values of 0 and 1 only,\n#so if we take a mean of those values that will give the percentage of 1s present.\nprint(\"Question pair are not similiar (is_duplicates=0):\\n {}%\".format(100-round(df['is_duplicate'].mean()*100,2)))\nprint(\"Question pair are similiar (is_duplicates=1):\\n {}%\".format(round(df['is_duplicate'].mean()*100,2)))","6b44ece3":"# Number of unique Solution\nqids=pd.Series(df['qid1'].tolist() + df['qid2'].tolist())\nunique_qs=len(np.unique(qids))\nqs_morethan_onetime=np.sum(qids.value_counts()>1)\nprint ('Total number of  Unique Questions are: {}\\n'.format(unique_qs))\n#print len(np.unique(qids))\nprint ('Number of unique questions that appear more than one time: {} ({}%)\\n'.format(qs_morethan_onetime,qs_morethan_onetime\/unique_qs*100))\n\nprint('Max number of times a single question is repeated: {}\\n'.format(max(qids.value_counts()))) \nq_vals=qids.value_counts()\nq_vals=q_vals.values","ccecc4cd":"x = [\"unique_questions\" , \"Repeated Questions\"]\ny =  [unique_qs , qs_morethan_onetime]\nplt.figure(figsize=(10, 6))\nplt.title (\"Plot representing unique and repeated questions  \")\nsns.barplot(x,y)\nplt.show()","99c705d8":"#Checking whether there are any repeated pair of question.\npair_duplicates=df[['qid1','qid2','is_duplicate']].groupby(['qid1','qid2']).count().reset_index()\nprint('Number of duplicats ',pair_duplicates.shape[0]-df.shape[0])","cd0277e8":"pair_duplicates=df[['qid1','qid2','is_duplicate']].groupby(['qid1','qid2']).count().reset_index()","a92d65f4":"plt.figure(figsize=(20,10))\nplt.hist(qids.value_counts(),bins=160)\nplt.yscale('log',nonposy='clip')\nplt.title('Log-Histogram of question appearance counts')\nplt.xlabel('Number of occurences of question')\nplt.ylabel('Number of questions')\nprint('Maximum number of times a single question is repeated : {} \\n'.format(max(qids.value_counts())))","3baa815e":"# Checking whether there are any rows with null value\nnan_rows=df[df.isnull().any(1)]\nprint(nan_rows)","fddbe118":"# Filling the null valus with ' '(space)\ndf=df.fillna('')\nnan_rows=df[df.isnull().any(1)]\nprint(nan_rows)\n","7a02c528":"if os.path.isfile('df_fe_without_preprocessing_train.csv'):\n    df=pd.read_csv('df_fe_without_preprocessing_train.csv',encoding='latin-1')\nelse:\n    #how many times a question is repeated we can cross check ex: len(df[df['qid1']==3])=4\n    df['freq_qid1']=df.groupby('qid1')['qid1'].transform('count') \n    df['freq_qid2']=df.groupby('qid2')['qid2'].transform('count')\n    \n    df['q1len']=df['question1'].str.len() #it will calculate string of each sentence\n    df['q2len']=df['question2'].str.len()\n    \n    df['q1_n_words']=df['question1'].apply(lambda row : len(row.split(\" \")))\n    #it will calculate words in each sentence\n    df['q2_n_words']=df['question2'].apply(lambda row : len(row.split(\" \")))\n    #it will calculate words in each sentence\n    \n    def normalized_word_common(row):\n        w1=set(map(lambda word: word.lower().strip(),row['question1'].split(\" \")))\n        w2=set(map(lambda word: word.lower().strip(),row['question2'].split(\" \")))\n        return 1.0 * len(w1&w2)\n    df['word_common']=df.apply(normalized_word_common,axis=1)\n    \n    def normalized_word_total(row):\n        w1=set(map(lambda word: word.lower().strip(),row['question1'].split(\" \")))\n        w2=set(map(lambda word: word.lower().strip(),row['question2'].split(\" \")))\n        return 1.0 * (len(w1)+len(w2))\n    df['word_total']=df.apply(normalized_word_total,axis=1)\n\n    def normalized_word_share(row):\n        w1=set(map(lambda word: word.lower().strip(),row['question1'].split(\" \")))\n        w2=set(map(lambda word: word.lower().strip(),row['question2'].split(\" \")))\n        return 1.0 * len(w1&w2)\/(len(w1)+len(w2))\n    df['word_share']=df.apply(normalized_word_share,axis=1)   \n    \n    df['freq_q1+q2']=df['freq_qid1']+df['freq_qid2']\n    df['freq_q1-q2']=abs(df['freq_qid1']-df['freq_qid2'])\n    \n    df.to_csv(\"df_fe_without_preprocessing_train.csv\",index=False)\ndf.head()","0bdae45e":"print('Minimum length of the question in question1 :',min(df['q1_n_words']))\nprint('Minimum length of the question in question2 :',min(df['q2_n_words']))\n\nprint('Number of Question with minimum length [question1]:',df[df['q1_n_words']==1].shape[0])\nprint('Number of Question with minimum length [question12]:',df[df['q2_n_words']==1].shape[0])","ee309cd3":"plt.figure(figsize=(12,8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x='is_duplicate',y='word_share',data=df[0:])\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate']==1]['word_share'][0:],label='1',color='orange')\nsns.distplot(df[df['is_duplicate']==0]['word_share'][0:],label='0',color='blue')\nplt.show()\n","ae850b6a":"plt.figure(figsize=(12,8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x='is_duplicate',y='word_common',data=df[0:])\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate']==1]['word_common'][0:],label='1',color='orange')\nsns.distplot(df[df['is_duplicate']==0]['word_common'][0:],label='0',color='blue')\nplt.show()\n","b508491c":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom subprocess import check_output\n%matplotlib inline\nimport plotly.offline as py\nimport plotly.tools as tls\nimport os\nimport gc\n\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom bs4 import BeautifulSoup\nfrom fuzzywuzzy import fuzz\nfrom sklearn.manifold import TSNE\n# Import the Required lib packages for WORD-Cloud generation\n# https:\/\/stackoverflow.com\/questions\/45625434\/how-to-install-wordcloud-in-python3-6\nfrom wordcloud import WordCloud ,STOPWORDS\nfrom os import path\nfrom PIL import Image\nimport distance\n\n","5b4e8c21":"#https:\/\/stackoverflow.com\/questions\/12468179\/unicodedecodeerror-utf8-codec-cant-decode-byte-0x9c\nif os.path.isfile(\"df_fe_without_preprocessing_train.csv\"):\n    df=pd.read_csv('df_fe_without_preprocessing_train.csv',encoding='latin-1')\n    df = df.fillna(\"\")\n    df.head()\nelse:\n    print('get df_fe_without_preprocessing_train.csv from drive or run the previous notebooks')\n    ","2d08d99e":"df.head(2)","83e178f8":"#for getting result in 4 decimal points\nSafe_div=0.0001\nStop_words=stopwords.words('english')\ndef preprocess(x):\n    x=str(x).lower()\n    x=x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"\u2032\", \"'\").replace(\"\u2019\", \"'\")\\\n                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n                           .replace(\"%\", \" percent \").replace(\"\u20b9\", \" rupee \").replace(\"$\", \" dollar \")\\\n                           .replace(\"\u20ac\", \" euro \").replace(\"'ll\", \" will\")\n    \n    x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n    x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n    \n    stemming =PorterStemmer()\n    pattern=re.compile('\\W')\n    \n    if type(x)==type(''):\n        x=re.sub(pattern,' ',x)\n    if type(x)==type(''):\n        x=stemming.stem(x)\n        exam=BeautifulSoup(x)\n        x=exam.get_text()\n    \n    return x\n","c0da84e5":"def get_token_features(q1, q2):\n    token_features = [0.0]*10\n    \n    # Converting the Sentence into Tokens: \n    q1_tokens = q1.split()\n    q2_tokens = q2.split()\n\n    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n        return token_features\n    # Get the non-stopwords in Questions\n    q1_words = set([word for word in q1_tokens if word not in Stop_words])\n    q2_words = set([word for word in q2_tokens if word not in Stop_words])\n    \n    #Get the stopwords in Questions\n    q1_stops = set([word for word in q1_tokens if word in Stop_words])\n    q2_stops = set([word for word in q2_tokens if word in Stop_words])\n    \n    # Get the common non-stopwords from Question pair\n    common_word_count = len(q1_words.intersection(q2_words))\n    \n    # Get the common stopwords from Question pair\n    common_stop_count = len(q1_stops.intersection(q2_stops))\n    \n    # Get the common Tokens from Question pair\n    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n    \n    \n    token_features[0] = common_word_count \/ (min(len(q1_words), len(q2_words)) + Safe_div)\n    token_features[1] = common_word_count \/ (max(len(q1_words), len(q2_words)) + Safe_div)\n    token_features[2] = common_stop_count \/ (min(len(q1_stops), len(q2_stops)) + Safe_div)\n    token_features[3] = common_stop_count \/ (max(len(q1_stops), len(q2_stops)) + Safe_div)\n    token_features[4] = common_token_count \/ (min(len(q1_tokens), len(q2_tokens)) + Safe_div)\n    token_features[5] = common_token_count \/ (max(len(q1_tokens), len(q2_tokens)) + Safe_div)\n    \n    # Last word of both question is same or not\n    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n    \n    # First word of both question is same or not\n    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n    \n    token_features[8] = abs(len(q1_tokens) - len(q2_tokens))\n    \n    #Average Token Length of both Questions\n    token_features[9] = (len(q1_tokens) + len(q2_tokens))\/2\n    return token_features\n\n# get the Longest Common sub string\n\ndef get_longest_substr_ratio(a, b):\n    strs = list(distance.lcsubstrings(a, b))\n    if len(strs) == 0:\n        return 0\n    else:\n        return len(strs[0]) \/ (min(len(a), len(b)) + 1)\n\ndef extract_features(df):\n    # preprocessing each question\n    df[\"question1\"] = df[\"question1\"].fillna(\"\").apply(preprocess)\n    df[\"question2\"] = df[\"question2\"].fillna(\"\").apply(preprocess)\n\n    print(\"token features...\")\n    \n    # Merging Features with dataset\n    \n    token_features = df.apply(lambda x: get_token_features(x[\"question1\"], x[\"question2\"]), axis=1)\n    \n    df[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\n    df[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\n    df[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\n    df[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\n    df[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\n    df[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\n    df[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\n    df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))\n    df[\"abs_len_diff\"]  = list(map(lambda x: x[8], token_features))\n    df[\"mean_len\"]      = list(map(lambda x: x[9], token_features))\n    (\"\"\"#1) list is used to typecast the map object. \n    #Map function, after applying the given function on the iterable returns a map object,\n    #which is then typecasted to list.\n    #2) token_features is a list of lists. Each of the sub list contains 5 elements. \n    And if you do token_features[0], you will get the first sub list of the whole list.\n    Which means that you will get the first list which contains 5 elements.\n    But here we want the first element of all the sublists as one column that is \n    why we used map function. I hope you got the point\"\"\")\n   \n    #Computing Fuzzy Features and Merging with Dataset\n    \n    # do read this blog: http:\/\/chairnerd.seatgeek.com\/fuzzywuzzy-fuzzy-string-matching-in-python\/\n    # https:\/\/stackoverflow.com\/questions\/31806695\/when-to-use-which-fuzz-function-to-compare-2-strings\n    # https:\/\/github.com\/seatgeek\/fuzzywuzzy\n    print(\"fuzzy features..\")\n\n    df[\"token_set_ratio\"]       = df.apply(lambda x: fuzz.token_set_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    # The token sort approach involves tokenizing the string in question, sorting the tokens alphabetically, and \n    # then joining them back into a string We then compare the transformed strings with a simple ratio().\n    df[\"token_sort_ratio\"]      = df.apply(lambda x: fuzz.token_sort_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"fuzz_ratio\"]            = df.apply(lambda x: fuzz.QRatio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"fuzz_partial_ratio\"]    = df.apply(lambda x: fuzz.partial_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    df[\"longest_substr_ratio\"]  = df.apply(lambda x: get_longest_substr_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n    return df","04ffb517":"if os.path.isfile('nlp_features_train.csv'):\n    df = pd.read_csv(\"nlp_features_train.csv\",encoding='latin-1')\n    df.fillna('')\nelse:\n    print(\"Extracting features for train:\")\n    df = pd.read_csv(\"..\/input\/quora-question-pairs\/train.csv.zip\")\n    df = extract_features(df)\n    df.to_csv(\"nlp_features_train.csv\", index=False)\ndf.head(2)","a9cc8a35":"df_dupicates = df[df['is_duplicate']==1]\ndf_nondupicates = df[df['is_duplicate']==0]\n\n#Converting 2d array of q1 and q2 and Flatten the array : like {{1,2},{3,4}} to {1,2,3,4}\np=np.dstack([df_dupicates['question1'],df_dupicates['question2']]).flatten()\nn=np.dstack([df_nondupicates['question1'],df_nondupicates['question2']]).flatten()\n\nprint (\"Number of data points in class 1 (duplicate pairs) :\",len(p))\nprint (\"Number of data points in class 0 (non duplicate pairs) :\",len(n))\n\n#Saving the np array into a text file \nnp.savetxt('train_p.txt',p,delimiter=' ',fmt='%s')\nnp.savetxt('train_n.txt',n,delimiter=' ',fmt='%s')","cfcd4a25":"d=path.dirname('.')\n\ntextp_w=open(path.join(d,'train_p.txt')).read()\ntextn_w=open(path.join(d,'train_n.txt')).read()\nstopwords=set(Stop_words)\nstopwords.add(\"said\")\nstopwords.add(\"br\")\nstopwords.add(\" \")\nstopwords.remove(\"not\")\nstopwords.remove(\"no\")\n#stopwords.remove(\"good\")\n#stopwords.remove(\"love\")\n#stopwords.remove(\"like\")\n#stopwords.remove(\"best\")\n#stopwords.remove(\"!\")\n\nprint('total number of words in duplicates pair question:',len(textp_w))\nprint('total number of words in non duplicates pair question:',len(textn_w))","0e7f7534":"wc=WordCloud(background_color='white',max_words=len(textp_w),stopwords=Stop_words)\nwc.generate(textp_w)\nprint(\"Word cloud for Duplicates Question pair\")\nplt.imshow(wc,interpolation='bilinear')\nplt.axis('off')\nplt.show()","d505e3d1":"wc=WordCloud(background_color='white',max_words=len(textn_w),stopwords=Stop_words)\nwc.generate(textn_w)\nprint(\"Word cloud for non Duplicates Question pair\")\nplt.imshow(wc,interpolation='bilinear')\nplt.axis('off')\nplt.show()","5f338799":"n=df.shape[0]\nsns.pairplot(df[['ctc_min','cwc_min','csc_min','token_sort_ratio','is_duplicate']][0:n],hue='is_duplicate',vars=['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio'])\nplt.show()","8bf00ab3":"# Distribution of the token_sort_ratio\nplt.figure(figsize=(10,8))\nplt.subplot(1,2,1)\nsns.violinplot(x='is_duplicate',y='token_sort_ratio',data=df[0:])\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate']==1.0]['token_sort_ratio'][0:],label='1',color='red')\nsns.distplot(df[df['is_duplicate']==0]['token_sort_ratio'][0:],label='0',color='blue')\nplt.show()","7343c3ae":"# Distribution of the token_sort_ratio\nplt.figure(figsize=(10,8))\nplt.subplot(1,2,1)\nsns.violinplot(x='is_duplicate',y='fuzz_ratio',data=df[0:])\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate']==1.0]['fuzz_ratio'][0:],label='1',color='red')\nsns.distplot(df[df['is_duplicate']==0]['fuzz_ratio'][0:],label='0',color='blue')\nplt.show()","0683313e":"# Using Tsne for Dimensionality reduction for 15 features (Generated after cleaning the data) to 3 dimension\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import MinMaxScaler\ndfp_subsampled=df[0:5000] # picking 5000 data points\nX=MinMaxScaler().fit_transform(dfp_subsampled[['cwc_min', 'cwc_max', 'csc_min', 'csc_max' , 'ctc_min' , 'ctc_max' , 'last_word_eq', 'first_word_eq' , 'abs_len_diff' , 'mean_len' , 'token_set_ratio' , 'token_sort_ratio' ,  'fuzz_ratio' , 'fuzz_partial_ratio' , 'longest_substr_ratio']])\ny=dfp_subsampled['is_duplicate'].values","9bcc2fd6":"tsne2d=TSNE(n_components=2,\n            init='random',\n            random_state=101,\n            method='barnes_hut',\n            n_iter=1000,\n            verbose=2,\n            angle=0.5).fit_transform(X)","1897d466":"df=pd.DataFrame({'x':tsne2d[:,0],'y':tsne2d[:,1],'label':y})\n#draw the plot in appropriate place in the grid\nsns.lmplot(data=df,x='x',y='y',hue='label',fit_reg=False,size=8,palette='Set1',markers=['s','o'])\nplt.title(\"perplexity : {} and max_iter : {}\".format(30, 1000))\nplt.show()","10d0b1a1":"tsne3d=TSNE(n_components=3,\n            init='random',\n            random_state=101,\n            method='barnes_hut',\n            n_iter=1000,\n            verbose=2,\n            angle=0.5).fit_transform(X)","5769b6e4":"tracel =go.Scatter3d(x=tsne3d[:,0]\n                     ,y=tsne3d[:,1],\n                     z=tsne3d[0:,2],\n                     mode='markers',\n                     marker=dict(sizemode='diameter',\n                                 color = y,\n                                 colorscale = 'Portland',\n                                 colorbar = dict(title = 'duplicate'),\n                                 line=dict(color='rgb(255, 255, 255)'),\n                                 opacity=0.75)\n                    )\ndata=[tracel]\nlayout=dict(height=800,width=800,title='3d embedding with engineered features')\nfig=dict(data=data,layout=layout)\npy.iplot(fig,filename='3DBubble')","b9b5d660":"import pandas as pd\nimport matplotlib.pyplot  as plt\nimport seaborn as sns\nimport numpy as np\nimport time \nfrom nltk.corpus import stopwords\nfrom sklearn.preprocessing import normalize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nimport re\nimport sys\nfrom tqdm import tqdm\nimport spacy\n# exctract word2vec vectors\n# https:\/\/github.com\/explosion\/spaCy\/issues\/1721","a965f82d":"#avoiding decoding problems\ndf=pd.read_csv('..\/input\/quora-question-pairs\/train.csv.zip')\n# encode question to unicode\n# https:\/\/stackoverflow.com\/a\/6812069\ndf['question1']=df['question1'].apply(lambda x:str(x))\ndf['question2']=df['question2'].apply(lambda x:str(x))","b42838c7":"df.head()","e71fb401":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# merging text\nquestions = list(df['question1'])+list(df['question2'])\ntf_idf=TfidfVectorizer(lowercase=False)\ntf_idf.fit_transform(questions)\n\n# dict key:word and value:tf-idf score\nword2_tf_idf=dict(zip(tf_idf.get_feature_names(),tf_idf.idf_))","70c81c22":"pip install spacy ","6e3e1f8f":"# \"en_vectors_web_sm\", which includes over 1 million unique vectors.\nnlp = spacy.load('en_core_web_sm')\nvecs1 = []\nfor q1 in tqdm(list(df['question1'])): # tqdm is used to print the progress bar\n    doc1 = nlp(q1) \n    # \n    mean_vec1 = np.zeros([len(doc1), len(doc1[0].vector)])\n    for word1 in doc1:\n        # word2vec\n        vec1 = word1.vector\n        # fetch df score\n        try:\n            idf = wor2_tf_idf[str(word1)]\n        except:\n            idf = 0\n        # compute final vec\n        mean_vec1 += vec1 * idf\n    mean_vec1 = mean_vec1.mean(axis=0)\n    vecs1.append(mean_vec1)\ndf['q1_feats_m'] = list(vecs1)","df86bdd7":"vecs2 = []\nfor q2 in tqdm(list(df['question2'])): # tqdm is used to print the progress bar\n    doc2 = nlp(q2) \n    # \n    mean_vec2 = np.zeros([len(doc2), len(doc2[0].vector)])\n    for word2 in doc2:\n        # word2vec\n        vec2 = word2.vector\n        # fetch df score\n        try:\n            idf = wor2_tf_idf[str(word2)]\n        except:\n            idf = 0\n        # compute final vec\n        mean_vec2 += vec2 * idf\n    mean_vec2 = mean_vec2.mean(axis=0)\n    vecs2.append(mean_vec2)\ndf['q1_feats_m'] = list(vecs2)\n","65661d09":"##","004fb79d":"# Checking for Duplicates","f4d74fd7":"# Machine Learning Problem\n## Data \n### *Data overview:*\n*  Data will be in a file Train.csv\n* Train.csv Contains 5 columns : qid1, qid2, question1, question2, is_duplicate \n* size of train.csv: 60MB\n* Number of rows in Train.csv = 404,290\n\n### *Example Data point*\n\n\"id\",\"qid1\",\"qid2\",\"question1\",\"question2\",\"is_duplicate\"\n\n\"0\",\"1\",\"2\",\"What is the step by step guide to invest in share market in india?\",\"What is the step by step guide to invest in share market?\",\"0\"\n\n\"1\",\"3\",\"4\",\"What is the story of Kohinoor (Koh-i-Noor) Diamond?\",\"What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?\",\"0\"\n\n\"7\",\"15\",\"16\",\"How can I be a good geologist?\",\"What should I do to be a great geologist?\",\"1\"\n\n\"11\",\"23\",\"24\",\"How do I read and find my YouTube comments?\",\"How can I see all my Youtube comments?\",\"1\"\n","a7c211a8":"# <p> The distributions of the word_Common feature in similar and non-similar questions are highly overlapping <\/p>","7cb29492":"# train and test Construction \nWe build train and test by randomly splitting into 70:30 or 80:20 whatever we choose as we have sufficient points to worl out.","be40c871":"# Quora Question Pairs","2fc47e64":"# Number of occurrences of each question","805df549":"# Featurizing text data with tfidf weighted word-vectors","444dab28":"# Advance Feature Extraction (NLP and Fuzzy Features)\n\n* Defination\n    * Token: You get a token by splitting sentence a space\n    * Stop_words: Stop words as per NLTK.\n    * Word: A Token that is not a stop_word \n\n* Feature :\n    * cwc_min: Ratio of common_words_count to min length of word count of Q1 and Q2\n           cwc_min =common_word_count\/(min(len(q1_words),len(q2_words))\n    * cwc_max: Ratio of common_words_count to max length of word count of Q1 and Q2\n           cwc_max =common_word_count\/(max(len(q1_words),len(q2_words)) \n    \n    * csc_min: Ratio of common_stop_count to min length of stops count of Q1 and Q2\n           csc_min =common_stop_count\/(min(len(q1_stops),len(q2_stops))\n    * csc_max: Ratio of common_stop_count to max length of stops count of Q1 and Q2\n           csc_max =common_stop_count\/(max(len(q1_stops),len(q2_stops))\n    \n    * ctc_min: Ratio of common_token_count to min length of token count of Q1 and Q2\n           ctc_min =common_token_count\/(min(len(q1_tokens),len(q2_tokens))\n    * ctc_max: Ratio of common_token_count to max length of token count of Q1 and Q2\n           ctc_max =common_token_count\/(max(len(q1_tokens),len(q2_tokens))      \n           \n    * last_words_eq: Check if Last word of both question is equal or not \n           last_words_eq=int(q1_token[-1]==q2_token[-1])\n    * first_words_eq: Check if First word of both question is equal or not \n           first_words_eq=int(q1_token[-1]==q2_token[-1])\n           \n    * abs_len_diff: Abs length differnce \n           abs_len_diff=abs(len(q1_tokens)-len(q2_token))\n           \n    * mean_len: Average Token Lenght of both questions\n           mean_len=((len(q1_tokens)+len(q2_tokens))\/2 )\n           \n    * fuzz_ratio: https:\/\/github.com\/seatgeek\/fuzzywuzzy#usage http:\/\/chairnerd.seatgeek.com\/fuzzywuzzy-fuzzy-string-matching-in-python\/\n\n\n    * fuzz_partial_ratio:\n             https:\/\/github.com\/seatgeek\/fuzzywuzzy#usage http:\/\/chairnerd.seatgeek.com\/fuzzywuzzy-fuzzy-string-matching-in-python\/\n\n\n    *  token_sort_ratio: https:\/\/github.com\/seatgeek\/fuzzywuzzy#usagehttp:\/\/chairnerd.seatgeek.com\/fuzzywuzzy-fuzzy-string-matching-in-python\/\n\n    * token_set_ratio: https:\/\/github.com\/seatgeek\/fuzzywuzzy#usage http:\/\/chairnerd.seatgeek.com\/fuzzywuzzy-fuzzy-string-matching-in-python\/\n\n    * longest_substr_ratio: Ratio of length longest common substring to min length of token count Q1 and Q2. \n           longest_substr_ratio= len(longest common string)\/(min(len(q1_tokens),len(q2_tokens))","c6336df2":"# Analysing of extracted features\n\n## Plotting Word Clouds\n* Creating Word Cloud of Duplicates and Non-Duplicates Question Pairs\n* We can observe the most frequent occuring words.","8a92dc09":"* Function to compute and get the feature : With 2 parameter of Question1 and Question2","aede7347":"## We have got minimal number of data fields here , consisting of:\n* id: Looks like a simple rodID\n* qid{1,2}: The unique Id of each question in the pair\n* question{1,2}: The actual textual content of question\n* is_duplicate: The label that we are trying to predict -wether the two question are duplicates of each other.","84fc784d":"# Pair Plot of feature ['ctc_min','cwc_min','csc_min','token_sort_ratio']","5ddd5ffb":"# Reading data and basics stats","b08bd6b0":"# Distribution of data points among output classes\n* Number of duplicates (similiar) and non-duplicates(non-similiar)question.","215aaf06":"# Feature: word Share","9b6b26ae":"# Analysis of Some of the feature extracted \n\n* here are some question have only one single words ","e2faf1ae":"# Exploratoty Data Analysis\n\n","2c405255":"# Business Problem \n\n1. Description : \nQuora is a place to gain and share knowledge\u2014about anything. It\u2019s a platform to ask questions and connect with people who contribute unique insights and quality answers. This empowers people to learn from each other and to better understand the world.\n\nOver 100 million people visit Quora every month, so it's no surprise that many people ask similarly worded questions. Multiple questions with the same intent can cause seekers to spend more time finding the best answer to their question, and make writers feel they need to answer multiple versions of the same question. Quora values canonical questions because they provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term.\n\n2. Problem Statement :\n     * Identify Which question are asked on Quora are duplicates of questions\n     * This could be useful to instantly provide answer to question that have already been asked.\n     * We are tasked with predciting whether a pair of quesiton are duplicates or not.\n","b1062c9d":"# Prepricessing of Text.\n* Preprocessing\n    * Removing the HTML TAGS\n    * Removing Punctuation\n    * Performing Stemming\n    * Removing Stopwords \n    * Expanding Contradiction etc .","7b7f6c3f":"# BAsic Feature Extraction(before cleaning)\n\nConstucting few features\n* freq_qid1= Frequency of qid1's\n* freq_qid2= Frequency of qid2's\n* q1_len =Length of Question 1\n* q2_len =Length of Question 2\n* q1_n_words=Number of words in question 1\n* q2_n_words=Number of words in question 2\n* word_common=(Number of common unique words in Question 1 and Question 2)\n* Word_Total=(Total number of words in Question1 + Total number of words in Question 2)\n* Word_share=(word_common)\/(word_tota)\n* freq_q1+freq_q2=sum of frequency of qid1 and  qid2\n* fred_q1-freq_q2=absolute diffrence of frequency of qid1 and qid2","689c0de4":"# Word Cloud generated from pair question's Text","0bc3a5f0":"# Mapping the real world problem to an ML problem\n ## Type of Machine Learning Problem :\n * It is a binary Classification problem ,for a given pair of question we need to predict if they are duplicates or not\n## Performance metrics: \n    * Source: https:\/\/www.kaggle.com\/c\/quora-question-pairs#evaluation\n\n    * Metric(s):\n        * log-loss : https:\/\/www.kaggle.com\/wiki\/LogarithmicLoss\n        * Binary Confusion Matrix\n\n\n ","8e6634fd":"# Feature : Word_common","51ef5b2b":"* The distibution for normalized word_share have some overlap on the far right-hand side,i.e there are quite of question with high word similarity\n* The average word share and Common no. of words of qid1 and qid2 is more when they are duplicate(Similar)","fc39adbb":"# Checking NULL values\n","26014d00":"# EDA: Advanced Feature Extraction.","613abc35":"# Real World \/Business Objective and Constraints\n1. The cost of misclassification can be very high.\n2. We would want a probability of a pair of question to be duplicates so that we can choose any threshold of choice.\n3. No strict latency concerns.\n4. Interpretabilty is partial important.","f0d7017f":"# Useful link.\n* Discussion :https:\/\/www.kaggle.com\/anokas\/data-analysis-xgboost-starter-0-35460-lb\/notebook\n* Kaggle Winning Solution : https:\/\/www.dropbox.com\/sh\/93968nfnrzh8bp5\/AACZdtsApc1QSTQc7X0H3QZ5a?dl=0\n* Blog 1 : https:\/\/engineering.quora.com\/Semantic-Question-Matching-with-Deep-Learning\n* Blog 2 : https:\/\/towardsdatascience.com\/identifying-duplicate-questions-on-quora-top-12-on-kaggle-4c1cf93f1c30","ac462eed":"* After we find Tf-idf scores ,we convert each question to a weighted average of word2vec vectors by these scores \n* We are using a pre-trained **GLOVE** model which come free with \"*Spacy*\"(https:\/\/spacy.io\/usage\/vectors-similarity)\n* It is trained on Wikipedia and therefore, it is stronger in terms of word semantics.","56ce3675":"* There are two rows with null values in question2 "}}