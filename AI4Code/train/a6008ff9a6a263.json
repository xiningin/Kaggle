{"cell_type":{"8a85ea37":"code","93fb6d3b":"code","7e0f2ad0":"code","abc66418":"code","4256ed4b":"code","e735038f":"code","126cad5d":"code","3a9401b6":"code","cb5a5b90":"code","a685a6d2":"code","58aa7121":"code","00c1dd84":"code","e5689e5e":"code","751cbdc3":"markdown"},"source":{"8a85ea37":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nimport lightgbm as lgbm\n\nimport optuna\nfrom optuna import Trial, visualization","93fb6d3b":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/sample_submission.csv')","7e0f2ad0":"!pip install pytorch-tabnet","abc66418":"from pytorch_tabnet.tab_model import TabNetRegressor\nfrom sklearn.model_selection import KFold\nimport torch","4256ed4b":"X = train.drop(['target', 'id'], axis=1).values\ny = train['target'].values\ny = y.reshape(-1, 1)","e735038f":"def Objective(trial):\n    mask_type = trial.suggest_categorical(\"mask_type\", [\"entmax\", \"sparsemax\"])\n    n_da = trial.suggest_int(\"n_da\", 56, 64, step=4)\n    n_steps = trial.suggest_int(\"n_steps\", 1, 3, step=1)\n    gamma = trial.suggest_float(\"gamma\", 1., 1.4, step=0.2)\n    n_shared = trial.suggest_int(\"n_shared\", 1, 3)\n    lambda_sparse = trial.suggest_float(\"lambda_sparse\", 1e-6, 1e-3, log=True)\n    tabnet_params = dict(n_d=n_da, n_a=n_da, n_steps=n_steps, gamma=gamma,\n                     lambda_sparse=lambda_sparse, optimizer_fn=torch.optim.Adam,\n                     optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n                     mask_type=mask_type, n_shared=n_shared,\n                     scheduler_params=dict(mode=\"min\",\n                                           patience=trial.suggest_int(\"patienceScheduler\",low=3,high=10), # changing sheduler patience to be lower than early stopping patience \n                                           min_lr=1e-5,\n                                           factor=0.5,),\n                     scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n                     verbose=0,\n                     ) #early stopping\n    kf = KFold(n_splits=5, random_state=42, shuffle=True)\n    CV_score_array    =[]\n    for train_index, test_index in kf.split(X):\n        X_train, X_valid = X[train_index], X[test_index]\n        y_train, y_valid = y[train_index], y[test_index]\n        regressor = TabNetRegressor(**tabnet_params)\n        regressor.fit(X_train=X_train, y_train=y_train,\n                  eval_set=[(X_valid, y_valid)],\n                  patience=trial.suggest_int(\"patience\",low=15,high=30), max_epochs=trial.suggest_int('epochs', 1, 100),\n                  eval_metric=['rmse'])\n        CV_score_array.append(regressor.best_cost)\n    avg = np.mean(CV_score_array)\n    return avg","126cad5d":"study = optuna.create_study(direction=\"minimize\", study_name='TabNet optimization')\nstudy.optimize(Objective, timeout=6*60) #5 hours","3a9401b6":"#train a TabNet with the best params to make submission\nTabNet_params = study.best_params","cb5a5b90":"#TabNet_params = {'epochs':6}","a685a6d2":"print(TabNet_params)","58aa7121":"final_params = dict(n_d=TabNet_params['n_da'], n_a=TabNet_params['n_da'], n_steps=TabNet_params['n_steps'], gamma=TabNet_params['gamma'],\n                     lambda_sparse=TabNet_params['lambda_sparse'], optimizer_fn=torch.optim.Adam,\n                     optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n                     mask_type=TabNet_params['mask_type'], n_shared=TabNet_params['n_shared'],\n                     scheduler_params=dict(mode=\"min\",\n                                           patience=TabNet_params['patienceScheduler'],\n                                           min_lr=1e-5,\n                                           factor=0.5,),\n                     scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n                     verbose=0,\n                     )\nepochs = TabNet_params['epochs']","00c1dd84":"regressor = TabNetRegressor(**final_params)\nregressor.fit(X_train=X, y_train=y,\n          patience=TabNet_params['patience'], max_epochs=epochs,\n          eval_metric=['rmse'])","e5689e5e":"X_test = test.drop('id', axis=1).values\nsub['target']=regressor.predict(X_test)\nsub.to_csv('submission.csv', index=False)","751cbdc3":"# Making Submission"}}