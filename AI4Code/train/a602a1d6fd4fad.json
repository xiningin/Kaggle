{"cell_type":{"20340d1a":"code","f6a173ba":"code","995a689a":"code","69b5120d":"code","e28418cc":"code","9a7fdd3f":"code","d0589e7e":"code","5c618e6a":"code","063c3bee":"code","40f034dd":"code","f0f33d29":"code","b34c2017":"code","3f3f73e4":"code","310d3375":"code","0c405840":"code","bdf38e09":"code","f1f848b9":"code","2c110187":"code","fbd21654":"code","4d68ed0b":"code","68e73aaf":"code","e77ea5f8":"code","958312cf":"code","839d8bc4":"code","effe9b9c":"code","5b313714":"code","76934c09":"code","cc13ee24":"code","615d063d":"code","4fcd4369":"code","904743d8":"code","1142505e":"code","7906f0f1":"code","d9e89417":"code","99dcf4a7":"code","8f4349dc":"code","fb39228f":"code","38f2e0d2":"code","33a897b9":"code","9a7fc7b8":"code","15d49119":"code","c642017b":"code","5f9a8591":"code","af84cb7a":"code","cc15ce67":"code","9448eab4":"code","0b33fe18":"code","427b0918":"code","a2c64c2a":"code","7a832b81":"code","5dc219c4":"code","d90d42c8":"code","ae4189df":"code","5d832287":"markdown","e614510a":"markdown","3e339993":"markdown","41282358":"markdown","7c0665dc":"markdown","bf651be1":"markdown","4e09efa7":"markdown","ab489d18":"markdown"},"source":{"20340d1a":"#Import all necessary packages \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import SGDRegressor\nimport math","f6a173ba":"#import dataset\nds = pd.read_csv(\"..\/input\/insurance-data\/Insurance data.csv\")","995a689a":"#checking the first 5 elements of dataset\nds.head()","69b5120d":"#to know the whole size of dataset with column size\nds.shape","e28418cc":"#knowing the column names\nds.columns","9a7fdd3f":"#understanding number of unique values in each column in the dataset\nds.nunique(axis=0)","d0589e7e":"#summarize the features for numeric variable to understand better\nds.describe().apply(lambda s: s.apply(lambda x: format(x, 'g')))","5c618e6a":"#correlation between feature in dataset before sepearating into dependent and independent \ncorrelation = ds.corr()\nprint(correlation)","063c3bee":"d=ds.describe\nprint(d)","40f034dd":"import seaborn as sns","f0f33d29":"corr = ds.corr()# plot the heatmap\nsns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True))","b34c2017":"ds.plot.scatter(x='age',y='charges',color='red')\nplt.title(\"Scatter plot for age vs charges in Insurance\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Charges\")\nplt.show()","3f3f73e4":"ds.plot.scatter(x='age',y='bmi', color='blue')\nplt.title(\"Scatter plot for age vs bmi in Insurance\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"BMI\")\nplt.show()","310d3375":"#Histogram of a single vairable 'Charges' this shows the fluctuation of rates and it's frequency\nds['charges'].plot(kind='hist', bins=50, figsize=(12,6), facecolor='grey',edgecolor='black')","0c405840":"#Histogram of a single vairable 'BMI' this shows the fluctuation of values and it's frequency\nds['bmi'].plot(kind='hist', bins=50, figsize=(12,6), facecolor='grey',edgecolor='black')","bdf38e09":"#boxplot\nds.boxplot('charges')","f1f848b9":"sns.pairplot(ds)","2c110187":"\n#Seperating the dataset into Independent and Dependent variable using numpy and converting it into numpyarray\n#droping the last column 'region'\n\nds['sex'].replace('female', 0,inplace=True)\nds['sex'].replace('male', 1,inplace=True)\nds['smoker'].replace('no', 0,inplace=True)\nds['smoker'].replace('yes', 1,inplace=True)\nds['region'].replace('northeast', 0,inplace=True)\nds['region'].replace('northwest', 1,inplace=True)\nds['region'].replace('southeast', 2,inplace=True)\nds['region'].replace('southwest', 3,inplace=True)\n\n\n","fbd21654":"ds.head()","4d68ed0b":"X = ds[['age','sex','bmi','children','smoker']]\ny = ds[['charges']]","68e73aaf":"print(X)","e77ea5f8":"print(y)","958312cf":"#splitting the dataset into Training and Testing set\n\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.3, random_state=0)","839d8bc4":"LR = LinearRegression()\nLR.fit(X_train,y_train)","effe9b9c":"print(\"Coefficient of Linear Regression:\",LR.coef_)","5b313714":"print(\"Intercept of Linear Regression :\",LR.intercept_)","76934c09":"#predicting linearRegression\ny_predict = LR.predict(X_train)","cc13ee24":"#Cross checking using the proper formula\n#y = mx+cy_\nY =252.33666642*28+-265.73101397*1+306.88025581*33+344.3662645*3+24050.20408698*0+-11138.939302606845\nprint(Y)","615d063d":"#total size of X data\n\nnum_data = X.shape[0]\nprint(num_data)","4fcd4369":"#Linear Regression mse,rmse,r squared and rse\n\nfrom sklearn.metrics import mean_squared_error\n\nmse = mean_squared_error(LR.predict(X_test),y_test)\n\nlr_rmse = math.sqrt(mse)\nlr_rse = (lr_rmse**2)*num_data\nlr_rse \/= num_data -2\nr2_score = LR.score(X_test,y_test)\n\nprint(\"MSE: %.2f\" % mse)\nprint(\"RMSE: %.2f\" % lr_rmse)\nprint(\"RSE: %.2f\" % lr_rse)\nprint(\"R^2: %.5f\" % r2_score)","904743d8":"from sklearn import preprocessing\n\nsx = preprocessing.MinMaxScaler()\nsy = preprocessing.MinMaxScaler()\n\nscaled_X = sx.fit_transform(ds.drop('region',axis='columns'))\nprint(scaled_X)","1142505e":"scaled_Y = sy.fit_transform(ds['charges'].values.reshape(-1,1))\nprint(scaled_Y)","7906f0f1":"def gradient_descent(X, y_true, epochs, learning_rate = 0.01):\n    \n    number_of_features = X.shape[1]\n    w = np.ones(shape=(number_of_features))\n    bias = 0\n    total_samples = X.shape[0]\n    \n    cost_list = []\n    epoch_list = []\n    \n    for i in range(epochs):\n        y_predict = np.dot(w, scaled_X.T) + bias\n        \n        w_grad = -(2\/total_samples)*(X.T.dot(y_true-y_predict))\n        bias_grad = -(2\/total_samples)*np.sum(y_true-y_predict)\n        \n        w = w - learning_rate * w_grad\n        bias = bias - learning_rate * bias_grad\n        \n        C = np.square(y_true - y_predict)\n        cost = np.mean(C)\n        \n        if i%10 == 0:\n            \n            cost_list.append(cost)\n            epoch_list.append(i)\n            \n    return w,bias,cost,cost_list, epoch_list\n            \nw,bias,cost,cost_list,epoch_list = gradient_descent(scaled_X,scaled_Y.reshape(scaled_Y.shape[0],),1000)\n","d9e89417":"print(w,bias,cost)","99dcf4a7":"plt.xlabel(\"epoch\")\nplt.ylabel(\"cost\")\nplt.plot(epoch_list,cost_list)","8f4349dc":"import random\ndef strochastic_gradient_descent(X,y_true,epochs,learning_rate = 0.01):\n    number_of_features = X.shape[1]\n    \n    w = np.ones(shape=(number_of_features))\n    b = 0\n    total_samples = X.shape[0]\n    R = random.randint(0,X.shape[0])\n    \n    cost_list = []\n    epoch_list = []\n    \n    for i in range(epochs):    \n        random_index = random.randint(0,total_samples-1) # random index from total samples\n        sample_x = X[random_index]\n        sample_y = y_true[random_index]\n        \n        y_predicted = np.dot(w, sample_x.T) + b\n    \n        w_grad = -(2\/total_samples)*(sample_x.T.dot(sample_y-y_predicted))\n        b_grad = -(2\/total_samples)*(sample_y-y_predicted)\n        \n        w = w - learning_rate * w_grad\n        b = b - learning_rate * b_grad\n        \n        cost = np.square(sample_y-y_predicted)\n        \n        if i%100==0: # at every 100th iteration record the cost and epoch value\n            cost_list.append(cost)\n            epoch_list.append(i)\n        \n    return w, b, cost, cost_list, epoch_list\n\nw_sgd, b_sgd, cost_sgd, cost_list_sgd, epoch_list_sgd = strochastic_gradient_descent(scaled_X,scaled_Y.reshape(scaled_Y.shape[0],),10000)\nw_sgd, b_sgd, cost_sgd","fb39228f":"#print(w_sgd,bias_sgd,cost_sgd)","38f2e0d2":"plt.xlabel(\"epoch\")\nplt.ylabel(\"cost\")\nplt.plot(epoch_list_sgd,cost_list_sgd)","33a897b9":"def mini_batch_gradient_descent(X, y_true, epochs = 100, batch_size = 5, learning_rate = 0.01):\n    \n    number_of_features = X.shape[1]\n    # numpy array with 1 row and columns equal to number of features. In \n    # our case number_of_features = 3 (area, bedroom and age)\n    w = np.ones(shape=(number_of_features)) \n    b = 0\n    total_samples = X.shape[0] # number of rows in X\n    \n    if batch_size > total_samples: # In this case mini batch becomes same as batch gradient descent\n        batch_size = total_samples\n        \n    cost_list = []\n    epoch_list = []\n    \n    num_batches = int(total_samples\/batch_size)\n    \n    for i in range(epochs):    \n        random_indices = np.random.permutation(total_samples)\n        X_tmp = X[random_indices]\n        y_tmp = y_true[random_indices]\n        \n        for j in range(0,total_samples,batch_size):\n            Xj = X_tmp[j:j+batch_size]\n            yj = y_tmp[j:j+batch_size]\n            y_predicted = np.dot(w, Xj.T) + b\n            \n            w_grad = -(2\/len(Xj))*(Xj.T.dot(yj-y_predicted))\n            b_grad = -(2\/len(Xj))*np.sum(yj-y_predicted)\n            \n            w = w - learning_rate * w_grad\n            b = b - learning_rate * b_grad\n                \n            cost = np.mean(np.square(yj-y_predicted)) # MSE (Mean Squared Error)\n        \n        if i%10==0:\n            cost_list.append(cost)\n            epoch_list.append(i)\n        \n    return w, b, cost, cost_list, epoch_list\n\n\n","9a7fc7b8":"plt.xlabel(\"epoch\")\nplt.ylabel(\"cost\")\nplt.plot(epoch_list,cost_list)","15d49119":"w, b, cost, cost_list, epoch_list = mini_batch_gradient_descent(\n    scaled_X,\n    scaled_Y.reshape(scaled_Y.shape[0],),\n    epochs = 120,\n    batch_size = 5\n)\nw, b, cost","c642017b":"x= ds.iloc[:, :-1].values\nY=ds.iloc[:, -1].values","5f9a8591":"print(x.shape)\nprint(y.shape)","af84cb7a":"print(x)","cc15ce67":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test= train_test_split(x,Y, test_size =0.2, random_state=0)","9448eab4":"lr = LinearRegression()\nlr.fit(X_train,y_train)","0b33fe18":"print(\"Coefficient :\", lr.coef_)\nprint(\"Intercept :\", lr.intercept_)","427b0918":"num_data = x.shape[0]\nprint(num_data)","a2c64c2a":"lr_rmse = math.sqrt(mean_squared_error(lr.predict(X_train),y_train))\nlr_rse = (lr_rmse**2)*num_data\nlr_rse \/= num_data-2\nr2_score = lr.score(X_train,y_train)\n\nprint(\"MSE :\",mean_squared_error(lr.predict(X_train),y_train))\nprint(\"RMSE = %.2f\" % lr_rmse)\nprint(\"RSE = %.2f \" % lr_rse)\nprint(\"R^2 - Variance = %.2f\" % r2_score)","7a832b81":"from sklearn.linear_model import SGDRegressor\n\nsgd_LR = SGDRegressor(max_iter =10_00 , tol=0.001, eta0=1e-3)\nsgd_LR.fit(X_train,y_train)","5dc219c4":"print(\"SGD Coefficient :\", sgd_LR.coef_)","d90d42c8":"print(\"SGD Intercept :\", sgd_LR.intercept_)","ae4189df":"sgd_rmse = math.sqrt(mean_squared_error(sgd_LR.predict(X_test),y_test))\nsgd_rse = (sgd_rmse**2)*num_data\nsgd_rse \/= num_data-2\nr2_score = sgd_LR.score(X_test,y_test)\n\nprint(\"MSE :\",mean_squared_error(sgd_LR.predict(X_test),y_test))\nprint(\"RMSE = %.2f\" % sgd_rmse)\nprint(\"RSE = %.2f \" % sgd_rse)\nprint(\"R^2 - Variance = \",r2_score)","5d832287":"## EDA [Exploratory Data Analysis]","e614510a":"## Visualization","3e339993":"### Gradient Descent Algorithm","41282358":"### Linear Regression [Normal Linear Regression]","7c0665dc":"## Model building ","bf651be1":"### Strochastic Gradient Decent","4e09efa7":"###  MSE, RMSE, R2 value","ab489d18":"### Mini Batch Gradient Decent"}}