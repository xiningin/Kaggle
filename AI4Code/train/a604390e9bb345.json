{"cell_type":{"b07ae1fa":"code","32b02e56":"code","951f8a38":"code","a498b139":"code","168f0769":"code","3eb34ec6":"code","5465b872":"code","275c56d4":"code","96046145":"code","e63d3a89":"code","49b88fd7":"code","2b51d4a1":"code","7865c111":"code","9e04d6e3":"code","a2a5ae62":"code","abec0b53":"code","3c688afa":"code","d19602e4":"code","3bce21bf":"code","b49f2755":"code","71bda9f5":"code","505fbab6":"code","0c7c4db8":"code","7aee4d54":"code","1c4b16ff":"code","f4dd4556":"code","9701147e":"code","35bef00b":"code","874db9e6":"code","ac2c61fa":"code","34340543":"code","f1f570c1":"code","60a3d913":"code","9d850ac7":"code","7bbd9e1d":"code","30cde3b8":"code","cba5027e":"code","9dd15846":"code","a0e12193":"code","3bf63970":"code","588ad5bb":"code","e16fef2c":"code","48aacdbf":"code","6c3dd66a":"code","ab4a7822":"code","4f8ad890":"code","b403fec1":"code","f2ff81f6":"markdown","ff236d81":"markdown","1e256460":"markdown","fcceb94e":"markdown"},"source":{"b07ae1fa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","32b02e56":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n%matplotlib inline\n","951f8a38":"# read data\ntrain_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')","a498b139":"# print first five rows of train data\ntrain_data.head(n=3)","168f0769":"test_data.head()","3eb34ec6":"print(\"Total number of rows in training data \", train_data.shape[0])\nprint(\"Total number of columns in training data \", train_data.shape[1])\nprint(\"Total number of rows in test data \", test_data.shape[0])\nprint(\"Total number of columns in test data \", test_data.shape[1])\n\n","5465b872":"# Data visualization and Analysis\n# Visualizing the number of null values in both data\n\nplt.figure(figsize = (13,5))\nplt.bar(train_data.columns, train_data.isna().sum())\nplt.xlabel(\"Columns name\")\nplt.ylabel(\"Number of missing values in training data\")\nplt.show()\n# from the bar plot of missing value we can conclude that Cabin, Embarked and Cabin column has null value so, we \n# can either drop the entire row or can fill the nan value with some values like mean, meadian. \n\n","275c56d4":"plt.figure(figsize=(13, 5))\nplt.bar(test_data.columns, test_data.isnull().sum().values, color='red')\nplt.xlabel(\"Columns name\")\nplt.ylabel(\"Number of missing values in test data\")\nplt.show()","96046145":"#Visualizing the Number of survived passenger\n\nsns.countplot('Survived', data=train_data)\nplt.show()\n# here we plot only for train_data as we donot have Survived column for test data,\n# This plot show that around 600 people died while around 300 survived","e63d3a89":"# visualizing the number of passenger from different embarked column in train_data\nsns.countplot('Embarked', data = train_data)\nplt.show()\n","49b88fd7":"#visualizing whether gender affect the survival rate or not\nsns.countplot('Survived', hue = 'Sex', data = train_data)\nplt.plot()   # show()","2b51d4a1":"# visualizing whether embarked place affects the survival rate or not\nsns.countplot('Survived', hue = 'Embarked', data = train_data)\nplt.show()","7865c111":"# visualizing whether pclass affect the survial rate or not\nsns.countplot('Survived', hue = 'Pclass', data = train_data)\nplt.show()","9e04d6e3":"# Box and whisker plot\nsns.boxplot('Fare', data = train_data)\nplt.show()","a2a5ae62":"sns.boxplot('Age', data = train_data)\nplt.show()","abec0b53":"# plot histogram\ninterval = 10\nvalue_for_bin = np.ceil((train_data.Age.max() - train_data.Age.min()) \/ interval).astype(int)\nplt.hist(train_data.Age, bins = value_for_bin)\nplt.xlabel('Age')\nplt.ylabel('Number')\nplt.show()","3c688afa":"plt.figure(figsize = (10, 4))\nplt.hist(train_data.Fare, bins = 10, color = 'red')\nplt.xlabel('Fare')\nplt.ylabel('Number')\nplt.show()","d19602e4":"grid = sns.FacetGrid(train_data, col = 'Survived', row = 'Pclass', size = 2.2, aspect = 1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()\nplt.show()","3bce21bf":"grid = sns.FacetGrid(train_data, row='Embarked', col='Survived', size=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()\nplt.show()","b49f2755":"# Simple Data Analysis\u00b6\n\ncorr_train = train_data.corr()\nsns.heatmap(corr_train)\nplt.show()\n# this shows that SibSp and Parch columns are releted , so we can combine this two column to reduce the dimension\n# of our data.. this plot only works for columns with numercal data \n","71bda9f5":"(train_data.groupby(['Sex', 'Survived']).Survived.count() * 100) \/ train_data.groupby('Sex').Survived.count()\n# this shows that female have around 74% chance of survival while male have around 81% chance of death","505fbab6":"(train_data.groupby(['Pclass','Survived']).Survived.count() * 100) \/ train_data.groupby('Pclass').Survived.count()\n# this shows that people belonging to third class are likely to die while people in class one are likely to survive","0c7c4db8":"#Survival rate on the basis of Embarked place\n(train_data.groupby(['Embarked','Survived']).Survived.count() * 100) \/ train_data.groupby('Embarked').Survived.count()\n# this shows that people who embarked from Southampton are likely to die\n","7aee4d54":"train_data.groupby(by=['Survived']).mean()[\"Age\"]\n# this show that average age of people who survived was around 28 years old","1c4b16ff":" # Dealing with the Missing values\u00b6\n\n# before filling the missing values, let's drop Cabin column from both data.\ntrain_data.drop('Cabin', axis = 1, inplace = True)\ntest_data.drop('Cabin', axis = 1, inplace = True)\n","f4dd4556":"combined_data = [train_data, test_data]\n\nfor data in combined_data:\n    print(data.isnull().sum())\n    print (\"*\" * 20)","9701147e":"train_data.head()","35bef00b":"#filling the nan values fo Age and fare column with the mean while Embarked column with most_frequent value\n\nfor data in combined_data:\n    data.Age.fillna(data.Age.mean(), inplace = True)\n    data.Fare.fillna(data.Fare.mean(), inplace = True)\n\n    # from visualization we know that Southamptom is most frequent Embarked place so, filling the missing value \n# with 'S'\ntrain_data.Embarked.fillna('S', inplace = True)\n\n# we simply can use SimpleImputer class form the sklearn to deal with the missing value\n# from sklearn.impute import SimpleImputer\n# impute = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n# train_data[['Age']] = impute.fit_transform(train_data[['Age']])\n    ","874db9e6":"# Converting a categorical feature\u00b6  label encoding\n# Let's start by converting Sex feature to categorical female=1 and male=0\n\ndef change_gender(x):\n    if x == 'male':\n        return 0\n    elif x == 'female':\n        return 1\ntrain_data.Sex = train_data.Sex.apply(change_gender)\ntest_data.Sex = test_data.Sex.apply(change_gender)\n\n# we simply can use mapfunction to change the gender\n# train_data.Sex = train_data.Sex.map({'female':1, 'male':0})\n","ac2c61fa":"# using map funcion to change the Embarked column S = 1, C = 2, Q = 0\nchange = {'S':1, 'C':2, 'Q':0}\ntrain_data.Embarked = train_data.Embarked.map(change)\ntest_data.Embarked = test_data.Embarked.map(change)\n\ntrain_data.head()","34340543":"#while visualizing the correlation heatmap we came to know that Sibsp and Parch columns were closely related so lets created new column called Alone using this two columns -------> 1 = Alone , 0 = not Alone\n\ntrain_data['Alone'] = train_data.SibSp + train_data.Parch\ntest_data['Alone'] = test_data.SibSp + test_data.Parch\n\ntrain_data.Alone = train_data.Alone.apply(lambda x: 1 if x == 0 else 0)\ntest_data.Alone = test_data.Alone.apply(lambda x: 1 if x == 0 else 0)","f1f570c1":"# drop SibSp and Parch\ntrain_data.drop(['SibSp', 'Parch'], axis = 1, inplace = True)\ntest_data.drop(['SibSp', 'Parch'], axis = 1, inplace = True)\n","60a3d913":"train_data.Name.str.extract('([A-Za-z]+)\\.', expand = False).unique().size\n\n# create the Title feature which contain the title of the passenger and drop Name column\nfor data in combined_data:\n    data['Title'] = data.Name.str.extract('([A-Za-z]+)\\.', expand = False)\n    data.drop('Name', axis = 1, inplace = True)\n    ","9d850ac7":"train_data.Title.value_counts()","7bbd9e1d":"train_data.head()","30cde3b8":"train_data.Title.unique()","cba5027e":"# replace least occuring title in the data with rare\nleast_occuring = [ 'Don', 'Rev', 'Dr', 'Mme', 'Ms',\n       'Major', 'Lady', 'Sir', 'Mlle', 'Col', 'Capt', 'Countess','Dona',\n       'Jonkheer']\nfor data in combined_data:\n    data.Title = data.Title.replace(least_occuring, 'Rare')\n    ","9dd15846":"#perform title mapping in order to change to ordinal\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor data in combined_data:\n    data['Title'] = data['Title'].map(title_mapping)\n\ntrain_data.head()\n","a0e12193":"# columns_to_drop = ['PassengerId','Ticket']\ncolumns_to_drop = ['PassengerId', 'Ticket']\ntrain_data.drop(columns_to_drop, axis = 1, inplace = True)\ntest_data.drop(columns_to_drop[1], axis = 1, inplace = True)","3bf63970":"train_data.head()\n","588ad5bb":"for dataset in combined_data:\n    dataset.loc[dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4\n","e16fef2c":"for data in combined_data:\n    data.loc[data['Fare'] < 30, 'Fare'] = 1\n    data.loc[(data['Fare'] >= 30) & (data['Fare'] < 50),'Fare'] = 2\n    data.loc[(data['Fare'] >= 50) & (data['Fare'] < 100),'Fare'] = 3\n    data.loc[(data['Fare'] >= 100),'Fare'] = 4\ntrain_data.head()","48aacdbf":"corr_train = train_data.corr()\nsns.heatmap(corr_train)\nplt.show()","6c3dd66a":"# https:\/\/www.kaggle.com\/dansbecker\/running-kaggle-kernels-with-a-gpu\nUSE_GPU = True\n\nif USE_GPU and torch.cuda.is_available():\n    print('using device: cuda')\nelse:\n    print('using device: cpu')","ab4a7822":"# Create ANN Model\nclass ANNModel(nn.Module):\n    \n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(ANNModel, self).__init__()\n        \n        # Linear function 1: 784 --> 150\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Non-linearity 1\n        self.relu1 = nn.ReLU()\n        \n        # Linear function 2: 150 --> 150\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        # Non-linearity 2\n        self.tanh2 = nn.Tanh()\n        \n        # Linear function 3: 150 --> 150\n        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n        # Non-linearity 3\n        self.elu3 = nn.ELU()\n        \n        # Linear function 4 (readout): 150 --> 10\n        self.fc4 = nn.Linear(hidden_dim, output_dim)  \n    \n    def forward(self, x):\n        # Linear function 1\n        out = self.fc1(x)\n        # Non-linearity 1\n        out = self.relu1(out)\n        \n        # Linear function 2\n        out = self.fc2(out)\n        # Non-linearity 2\n        out = self.tanh2(out)\n        \n        # Linear function 2\n        out = self.fc3(out)\n        # Non-linearity 2\n        out = self.elu3(out)\n        \n        # Linear function 4 (readout)\n        out = self.fc4(out)\n        return out\n\n# instantiate ANN\ninput_dim = 28*28\nhidden_dim = 150 #hidden layer dim is one of the hyper parameter and it should be chosen and tuned. For now I only say 150 there is no reason.\noutput_dim = 10\n\n# Create ANN\nmodel = ANNModel(input_dim, hidden_dim, output_dim)\n\n# Cross Entropy Loss \nerror = nn.CrossEntropyLoss()\n\n# SGD Optimizer\nlearning_rate = 0.02\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","4f8ad890":"# ANN model training\ncount = 0\nloss_list = []\niteration_list = []\naccuracy_list = []\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n\n        train = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        \n        # Clear gradients\n        optimizer.zero_grad()\n        \n        # Forward propagation\n        outputs = model(train)\n        \n        # Calculate softmax and ross entropy loss\n        loss = error(outputs, labels)\n        \n        # Calculating gradients\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n        \n        count += 1\n        \n        if count % 50 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Predict test dataset\n            for images, labels in test_loader:\n\n                test = Variable(images.view(-1, 28*28))\n                \n                # Forward propagation\n                outputs = model(test)\n                \n                # Get predictions from the maximum value\n                predicted = torch.max(outputs.data, 1)[1]\n                \n                # Total number of labels\n                total += len(labels)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct \/ float(total)\n            \n            # store loss and iteration\n            loss_list.append(loss.data)\n            iteration_list.append(count)\n            accuracy_list.append(accuracy)\n        if count % 500 == 0:\n            # Print Loss\n            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))","b403fec1":"# visualization loss \nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"ANN: Loss vs Number of iteration\")\nplt.show()\n\n# visualization accuracy \nplt.plot(iteration_list,accuracy_list,color = \"red\")\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"ANN: Accuracy vs Number of iteration\")\nplt.show()","f2ff81f6":"\n**preparing training and testing data **","ff236d81":"\n**Binning Age and Fare columns**","1e256460":"\nCreating new feature Title extracting from existing feature Name\n","fcceb94e":"**Feature Extraction**"}}