{"cell_type":{"60ac8d05":"code","5d90c8b5":"code","7fd215d8":"code","c5ce599a":"code","13b22d15":"code","9bad4656":"code","640bb91c":"code","19064261":"code","2fb46722":"code","bd7138af":"code","085f7683":"code","fd6bf021":"code","dd7eca76":"code","65835292":"code","74fa308e":"code","bbd0d496":"code","4207c573":"code","c8df940c":"code","84cc993b":"code","967201bb":"code","b4745763":"code","4f1ae532":"code","f9a147b8":"code","19c45daa":"code","cae779de":"code","40695cae":"code","85da442a":"code","5547b6e7":"code","2310e71e":"code","87ab2dfe":"code","a1e9763f":"code","a6eab274":"code","fe72f5d2":"code","3e903b71":"code","f8c0754b":"code","14073474":"code","fb2882ab":"code","ba5bac9c":"code","2c54fe35":"code","7189cf2b":"code","bf46329a":"code","655476b3":"code","e00f3aec":"code","9358fc88":"code","d2636231":"code","567243d7":"code","26079879":"code","20315213":"code","54c31a32":"code","65fe2537":"code","1ce90af6":"code","160c0bb8":"code","b5de7825":"code","d45194d8":"code","7185ea61":"code","4e8d3fd0":"code","b5fd4c6f":"code","9bae7876":"code","63ef8d6c":"code","bc812c21":"code","15a455e1":"code","c3f0cefb":"code","e3f00aa5":"code","594cdca0":"code","af876b01":"code","d489bd2d":"code","5bb19de4":"code","4305eb71":"code","4d3897b9":"markdown","96d21a6b":"markdown","0a899b99":"markdown","d464fa94":"markdown","ba07b464":"markdown","c1c0c199":"markdown","d1b5eef6":"markdown","8eaf9f31":"markdown","4132c9dc":"markdown","d711d9dd":"markdown","a9fe7b4f":"markdown","6d42c5cc":"markdown","49e04f00":"markdown","6e3a12a7":"markdown","b33d127f":"markdown","a770fe2c":"markdown","9e980baa":"markdown","cb6f833d":"markdown","60881836":"markdown","9d8f0733":"markdown","b7d760e2":"markdown","5f83a641":"markdown","a54d35c8":"markdown","b2d46d5e":"markdown","31a23cf2":"markdown","1f1a7caa":"markdown","2d24b994":"markdown","ff3b9bf1":"markdown","28c74e2b":"markdown","023824d3":"markdown","0e696333":"markdown","48a9c144":"markdown","e1137871":"markdown","326f171d":"markdown","9ea46b5e":"markdown","4706990c":"markdown","fb04c02b":"markdown","48fbb072":"markdown","8e74a0f3":"markdown","89cb2ea6":"markdown","5469e6a5":"markdown","18d3e677":"markdown","ec4304f8":"markdown","d98dd371":"markdown","24b1fff4":"markdown","4b878223":"markdown","3d021687":"markdown","e5a968a4":"markdown"},"source":{"60ac8d05":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5d90c8b5":"# importing required libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# to ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","7fd215d8":"data = pd.read_csv('\/kaggle\/input\/creditcarddefault\/credit-card-default.csv')\ndata.shape\n","c5ce599a":"data.head()","13b22d15":"# use od set_option\npd.set_option(\"display.max_columns\", None)","9bad4656":"# now one can see all columns\ndata.head()","640bb91c":"# null counts\ndata.isnull().sum()","19064261":"# understanding type of columns\ndata.info()","2fb46722":"# importing test_train_split from sklearn library\nfrom sklearn.model_selection import train_test_split","bd7138af":"# putting feature variable to X\nX = data.drop(['defaulted'],axis=1)\n\n# putting response\/target variable to y\ny = data.defaulted","085f7683":"# splitting data into train and test with test size as 30% and random state as 108\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=108)","fd6bf021":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","dd7eca76":"# importing StandardScaler, DecisionTreeClassifier and make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import make_pipeline","65835292":"scaler = StandardScaler()\ndt = DecisionTreeClassifier(random_state=108)","74fa308e":"# pipeline Estimator \npipeline = make_pipeline(scaler,dt) \n\n# fitting model on training data\npipeline.fit(X_train,y_train)","bbd0d496":"train_score = pipeline.score(X_train,y_train)\ntest_score = pipeline.score(X_test,y_test)\nprint('Model test Score:  %.3f\\nModel train Score: %.3f'%(test_score,train_score))","4207c573":"# model scores on test and training data\nfrom sklearn import metrics","c8df940c":"y_test_pred = pipeline.predict(X_test)\ntest_score = metrics.accuracy_score(y_test,y_test_pred)\n\ny_train_pred = pipeline.predict(X_train)\ntrain_score = metrics.accuracy_score(y_train,y_train_pred)\n\nprint('Model test Score:  %.3f \\nModel train Score: %.3f'%(test_score,train_score))","84cc993b":"# importing BaggingClassifier\nfrom sklearn.ensemble import BaggingClassifier","967201bb":"# all parameters of DecisionTreeClassifier\nDecisionTreeClassifier.get_params(DecisionTreeClassifier).keys()","b4745763":"# all parameters of BaggingClassifier\nBaggingClassifier.get_params(BaggingClassifier).keys()","4f1ae532":"# instantiating bagging classifier\nbgclassifier = BaggingClassifier(random_state=108)\n\npiplin = make_pipeline(bgclassifier)\n\n# fitting above model on training data\npiplin.fit(X_train,y_train)\n\n# model scores on test and training data\ntrain_score = piplin.score(X_train,y_train)\ntest_score = piplin.score(X_test,y_test)\nprint(' Model test Score:  %.3f \\n Model train Score: %.3f'%(test_score,train_score))","f9a147b8":"# all parameters of BaggingClassifier\nBaggingClassifier.get_params(BaggingClassifier).keys()","19c45daa":"# importing GridSearchCV\nfrom sklearn.model_selection import GridSearchCV","cae779de":"param_grid = {\n    'base_estimator__max_depth':[1,5,10,15,20],\n    'max_features':[1,5,10,15,20],\n    'max_samples':[0.05,0.1,0.2,0.5]}\n\n# instantiating gridsearch with BaggingClassifier with base estimator DecisionTreeClassifier\nGsCV_bagclf_baseEst_DT = GridSearchCV(estimator=BaggingClassifier(base_estimator=DecisionTreeClassifier(random_state=108)),param_grid=param_grid,scoring='accuracy')\n\n# fitting grid search on training data\nmodel_gs = GsCV_bagclf_baseEst_DT.fit(X_train,y_train)\nmodel_gs","40695cae":"GsCV_bagclf_baseEst_DT","85da442a":"# best paramaters selected by gridsearch\nprint('Optimal hyperparameter combination: ',model_gs.best_params_)","5547b6e7":"# accuracy score on final tuned  BaggingClassifier\nprint('Mean cross-validated training accuracy score: ',model_gs.score(X_train,y_train))","2310e71e":"# instantiating BaggingClassifier with optimized parameter by GridSearchCV\nbgclassifier = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=6,random_state=108),max_features=20,max_samples=0.05,random_state=108)\n\n\n#fitting above model on training data\nmodel_bgcl = bgclassifier.fit(X_train,y_train)\n\n## Model scores on test and training data\ntrain_score = model_bgcl.score(X_train,y_train)\ntest_score = model_bgcl.score(X_test,y_test)\n\nprint(' Model test Score: %.3f\\n Model training Score: %.3f'%(test_score,train_score))","87ab2dfe":"# importing RandomForestClassifier from sklearn library\nfrom sklearn.ensemble import RandomForestClassifier","a1e9763f":"# running RandomForestClassifier with default parameters\nrfc = RandomForestClassifier(random_state=108)\n\n# fiting model on training data\nmodel = rfc.fit(X_train,y_train)\n\n# making predictions\ny_test_pred = model.predict(X_test)","a6eab274":"# importing classification report and confusion matrix from sklearn metrics\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n\n# checking report of default model\nprint(classification_report(y_test,y_test_pred))","fe72f5d2":"# printing confusion_matrix\nprint(f'Confusion Matrix: \\n{confusion_matrix(y_test,y_test_pred)}','\\n','***'*5)\n\n# printing accuracy\nprint(f'Accuracy Score: \\n{accuracy_score(y_test,y_test_pred)}')","3e903b71":"# all parameters of RandomForestClassifier\nRandomForestClassifier.get_params(RandomForestClassifier).keys()","f8c0754b":"# you can read what above hyperparameter does\nprint(help(RandomForestClassifier))","14073474":"# all parameters of RandomForestClassifier\nRandomForestClassifier.get_params(RandomForestClassifier).keys()","fb2882ab":"# all parameters of DecisionTreeClassifier\nDecisionTreeClassifier.get_params(DecisionTreeClassifier).keys()","ba5bac9c":"# importing Kfold \nfrom sklearn.model_selection import KFold","2c54fe35":"# specifying number of folds for k-fold CV which is 10\nn_folds = 10\n\n# parameters to build model on here max_depth with range(2, 20, 5)\nparam = {\n    'max_depth':range(2,20,5)}\n\n# instantiating model\nrf_clf = RandomForestClassifier(random_state=108)\n\n\n# instantiating GridSearchCV with rf_clf, parameter, cv and scoring as accuracy and return_train_score=True\nrf = GridSearchCV(cv=n_folds,estimator=rf_clf,param_grid=param,return_train_score=True,scoring='accuracy')\n\n# fiting tree on training data\nmodel_rf_clf = rf.fit(X_train,y_train)\nmodel_rf_clf","7189cf2b":"# scores of GridSearch CV\nscores = model_rf_clf.cv_results_\npd.DataFrame(scores)","bf46329a":"# plotting accuracies with max_depth\nplt.figure(figsize=(15,5))\nplt.plot(scores['param_max_depth'],scores['mean_train_score'],label='train accuracy')\nplt.plot(scores['param_max_depth'],scores['mean_test_score'],label='test accuracy')\n\nplt.legend()\nplt.xlabel('max_depth',)\nplt.ylabel('accuracy')\nplt.show()","655476b3":"# specifying number of folds for k-fold CV which is 10\nn_folds = 10\n\n# parameters to build model on (here n_estimators with range(100, 1500, 400))\nparam = {\n    'n_estimators':range(100,1500,400)\n}\n\n# instantiating model\nrf_clf = RandomForestClassifier(max_depth=4,random_state=108)\n\n\n# instantiating GridSearchCVwith rf_clf, parameter, cv and scoring as accuracy and return_train_score as True\nrf = GridSearchCV(cv=n_folds,estimator=rf_clf,param_grid=param,return_train_score=True,scoring='accuracy')\n\n# fitting tree on training data\nrf.fit(X_train,y_train)","e00f3aec":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores)","9358fc88":"# plotting accuracies with n_estimators\nplt.figure(figsize=(15,5))\nplt.plot(scores['param_n_estimators'],scores['mean_train_score'],label='train accuracy')\nplt.plot(scores['param_n_estimators'],scores['mean_test_score'],label='test accuracy')\n\nplt.legend()\nplt.xlabel('n_estimators')\nplt.ylabel('accuracy')\nplt.show()","d2636231":"# specifying number of folds for k-fold CV which is 10\nn_folds = 10\n\n# parameters to build the model on max_features with [4, 8, 14, 20, 24]\nparameters = {\n    'max_features':[4,8,14,20,24]\n} \n\n# instantiating model \nrf_clf = RandomForestClassifier(max_depth=4)\n\n# instantiating GridSearchCVwith rf_clf, parameter, cv and scoring as accuracy and return_train_score as True\nrf = GridSearchCV(cv=n_folds,estimator=rf_clf,param_grid=parameters,return_train_score=True,scoring='accuracy')\n\n# fitting tree on training data\nrf.fit(X_train,y_train)","567243d7":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores)","26079879":"# plotting accuracies with max_features\nplt.figure(figsize=(15,5))\nplt.plot(scores['param_max_features'],scores['mean_train_score'],label='train accuracy')\nplt.plot(scores['param_max_features'],scores['mean_test_score'],label='test accuracy')\n\nplt.legend()\nplt.xlabel('max_features')\nplt.ylabel('accuracy')\nplt.show()","20315213":"# specifying number of folds for k-fold CV which is 10\nn_folds = 10\n\n# parameters to build model on min_samples_leaf of range(100, 400, 50)\nparameters ={\n    'min_samples_leaf':range(100, 400, 50)} \n\n# instantiating model \nrf_clf = RandomForestClassifier(random_state=108)\n\n\n# instantiating GridSearchCVwith rf_clf, parameter, cv and scoring as accuracy and return_train_score as True\nrf = GridSearchCV(cv=5, estimator=rf_clf,param_grid=parameters,return_train_score=True,scoring='accuracy')\n\n# fittig tree on training data\nrf.fit(X_train,y_train)","54c31a32":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores)","65fe2537":"# plotting accuracies with min_samples_leaf\nplt.figure(figsize=(15,5))\nplt.plot(scores['param_min_samples_leaf'],scores['mean_train_score'],label='train accuracy')\nplt.plot(scores['param_min_samples_leaf'],scores['mean_test_score'],label='test accuracy')\n\nplt.legend()\nplt.xlabel('min_samples_leaf')\nplt.ylabel('accuracy')\nplt.show()","1ce90af6":"# specifying number of folds for k-fold CV which is 10\nn_folds = 10\n\n# parameters to build the model on (min_samples_split with range(200, 500, 50))\nparameters = {\n    'min_samples_split':range(200, 500, 50)\n} \n\n# instantiating model (not specifying any max_depth)\nrf_clf = RandomForestClassifier(random_state=1)\n\n\n# instantiating GridSearchCVwith rf, parameter, cv and scoring as accuracy and return_train_score as True\nrf = GridSearchCV(cv=n_folds, estimator=rf_clf,param_grid=parameters,return_train_score=True,scoring='accuracy')\n\n# fit tree on training data\nrf.fit(X_train,y_train)","160c0bb8":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores)","b5de7825":"# plotting accuracies with min_samples_split\nplt.figure(figsize=(15,5))\nplt.plot(scores['param_min_samples_split'],scores['mean_train_score'],label='train accuracy')\nplt.plot(scores['param_min_samples_split'],scores['mean_test_score'],label='test accuracy')\n\nplt.legend()\nplt.xlabel('min_samples_split')\nplt.ylabel('accuracy')\nplt.show()","d45194d8":"# importing RandomizedSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV","7185ea61":"# creating parameter grid based on results of random search \nparam_grid = {\n    'max_depth': [4,8,10],\n    'min_samples_leaf': range(100, 400, 200),\n    'min_samples_split': range(200, 500, 200),\n    'n_estimators': [100,200, 300], \n    'max_features': [5,10]}\n\n\n# creating a based model \nrf_clf = RandomForestClassifier(random_state=108)\n\n# instantiating RadomSearch model with estimator, param_grid, and random_state\nrs = RandomizedSearchCV(estimator=rf_clf,param_distributions=param_grid,random_state=108)\n# fitting grid search to data\nrs.fit(X_train,y_train)","4e8d3fd0":"# printing optimal accuracy score and hyperparameters\nscore = accuracy_score(y_test,rs.predict(X_test))\nbest_hyper = rs.best_estimator_\nprint('We can get accuracy of {} using {}'.format(score,best_hyper))","b5fd4c6f":"# instantiating rf_clf model with best hyperparameters\nrf_clf = RandomForestClassifier(n_estimators=300,min_samples_split=400,min_samples_leaf=300,max_features=10,max_depth=10,random_state=108)\n\n# fitting rf_clf on training data\nrf_clf.fit(X_train,y_train)","9bae7876":"# predicting\npredictions = rf_clf.predict(X_test)\n\n# evaluation metrics using classification_report\nprint(classification_report(y_test,predictions))","63ef8d6c":"# printing confusion_matrix and \nprint(confusion_matrix(y_test,predictions),'\\n','***'*7)\n\n# printing accuracy\nprint('%.3f'%(accuracy_score(y_test,predictions)))","bc812c21":"# importing ExtraTreesClassifier \nfrom sklearn.ensemble import ExtraTreesClassifier","15a455e1":"# instantiating ExtraTreesClassifier with default parameters\net_clf = ExtraTreesClassifier(random_state=108)\n\n#fitting model on training data\net_clf.fit(X_train,y_train)\n\n# predictions\npredictions = et_clf.predict(X_test)","c3f0cefb":"# default model report\nprint(classification_report(y_test,predictions))","e3f00aa5":"# printing confusion matrix\nprint(confusion_matrix(y_test,predictions),'\\n','***'*7)\n\n#printing accuracy_score\naccuracy_score(y_test,predictions)","594cdca0":"# default model\net_clf","af876b01":"# creating parameter grid similar to random forest\nparam_grid = {\n    'max_depth': [4,8,10],\n    'min_samples_leaf': range(100, 400, 200),\n    'min_samples_split': range(200, 500, 200),\n    'n_estimators': [100,200, 300], \n    'max_features': [5,10]}\n\n# instantiating RandomizedSearchCV with et_clf , param_grid and random state=1\nrs = RandomizedSearchCV(param_distributions=param_grid,estimator=et_clf,random_state=108)\n# fitting RandomSearch to training data\nrs.fit(X_train,y_train)","d489bd2d":"# printing optimal accuracy score and hyperparameters\nscore = accuracy_score(y_test,rs.predict(X_test))\nbest_hyper = rs.best_estimator_\nprint('We can get accuracy of {} using {}'.format(score,best_hyper))","5bb19de4":"# instantiating ExtraTreesClassifier with default parameters\net_clf = ExtraTreesClassifier(max_depth=10,max_features=10,min_samples_leaf=100,min_samples_split=400,n_estimators=200,random_state=108)\n\n# fitting model on training data\net_clf.fit(X_train,y_train)","4305eb71":"# making predictions\npredictions = et_clf.predict(X_test)\n\n# printing confusion matrix\nprint(confusion_matrix(y_test,predictions),'\\n','***'*7)\n\n#printing accuracy_score\naccuracy_score(y_test,predictions)","4d3897b9":"# Tuning `n_estimators`\nTrying to find optimum values for `n_estimators` and meanwhile understanding how value of `n_estimators` impacts overall accuracy","96d21a6b":"![image.png](attachment:b0f3bde5-a19d-48d9-9135-6f51f74b60b9.png)","0a899b99":"# Tuning `max_features`\n\nTrying to see how model performance varies with `max_features`, which is maximum number of features considered for splitting at a node","d464fa94":"# `Why Ensemble`\n\nSome of the factors that cause errors in learning are `noise, bias, and variance` Ensemble  method is applied to reduce these factors resulting in stability and accuracy of result","ba07b464":"# Tuning `min_samples_leaf`\n\nHyperparameter `min_samples_leaf` is minimum number of samples required to be at a leaf node:\n- If int, then consider min_samples_leaf as the minimum number\n- If float, then min_samples_leaf is a percentage and ceil(min_samples_leaf * n_samples) are minimum number of samples for each node\n\nTrying to check optimum value for `min_samples_leaf` in this case","c1c0c199":"---\n---\n<h1><center>Bootstrap Aggregation or Bagging<\/h1>\n\n---\n---","d1b5eef6":"# Reducing Overfitting issue is required. I am trying to use various Bagging Methods\nI am trying to fit a `Bagging Classifier` using \n* default Hyperparameters and base estimator as pipeline built using Decision Tree Classifier\n\nOne can further perform a Grid SearchCV or Randomized SearchCV to get most appropriate estimator","8eaf9f31":"# `Jargon`\n1. `Ensemble`: When you take a decision alone see it as one decision tree(weak learner), now if you include more people into it then that is Ensembling\n    * This Ensemble can be made up of maybee single algorith(which is being used multiple times) or different algoriths(which are being used one after other), workign on a single fixed outcome\n    * Ensemble helps in improving our result and overcome overfitting problem which happens mostly with Decision Trees\n2. `Bagging`(parallel): homogeneous(same type) weak learners\u2019 model that `learns from each other independently in parallel` and combines them for determining the model average\n    * it generates additional data for training from main dataset\n        * this is achieved by random sampling with replacement(Bootstrap Aggregation) from original dataset\n        * sampling with replacement may repeat some observations in each new training data set, every element in Bagging is equally probable for appearing in a new dataset\n        * `Random Forest model uses Bagging, where decision tree models with higher variance are present`\n3. `Boosting`(sequential): homogeneous(same type) weak learners\u2019 model that `learns from each other sequentially and adaptively` to improve model prediction\n    *  convert weak learner to a stronger one\n        * it is a sequential ensemble method\n        * it iteratively adjusts weight of observation as per last classification\n            *  if an observation is incorrectly classified(predicted), it increases weight of that observation\n            * ","4132c9dc":"# 3.2 RandomForest Classifier\nRandom forest is an extension of Bagging that also randomly selects subsets of features used in each data sample\n* It consists of a `large number of individual decision trees` that operate as an ensemble\n    * `Each individual tree` in Random Forest `spits out a class prediction`\n    * `class` with `most votes becomes model\u2019s prediction`\n\n[RandomForest-Classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html)","d711d9dd":"Observe train and test scores both seem to increase as `max_features` is increasing\n* model doesn't seem to overfit more with increasing max_features","a9fe7b4f":"# Tuning `max_depth`\nTryign to find optimum values of `max_depth` and understand how value of max_depth impacts overall accuracy of ensemble","6d42c5cc":">**`Why To use RandomSearchCv not GrigSearchCv?????`** <br>\nUsing GridSearchcv system might crash, whereas RandomizedSearchCV is fater than grid search","49e04f00":"`Conclusion:` <br>\nI have used Ensemble bootstrap aggregation using 3 different methods\n>1. Bagging Classifier \n2. Random Forest Classifier \n3. Extra Tree Classifier","6e3a12a7":"This how we can boost accuracy.","b33d127f":"`Compare Hyperparameter of Decision-Tree and Random-Forest almost they common Hyperparameter`","a770fe2c":"# 3.3. Extra Trees Classifier\n[Extra Trees Classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.ExtraTreesClassifier.html) <br>\n`Extra Trees` and `Random Forest` are two very similar ensemble methods\n* Unlike `Bagging` and `Random Forest` that develop each `Decision Tree` from a bootstrap sample of training dataset,\n    * `Extra Trees algorithm fits each decision tree on whole training dataset`\n* Like Random Forest, Extra Trees algorithm will randomly sample features at each split point of a Decision Tree \n    * Unlike `Random Forest` which `uses Greedy Algorithm` to select an optimal split point, `Extra Trees` selects a split point at random","9e980baa":"[BaggingClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.BaggingClassifier.html)","cb6f833d":"Observe model tends to overfit data as test score is 0.81 and training score is 0.98\n\n* `but but but model will give better generalized performance than model fit with Descision Tree alone`","60881836":"# 2. Data Preparation and Model Building","9d8f0733":"# 3. Applying Bagging","b7d760e2":"![image.png](attachment:329a305e-53b6-47df-8b50-61e22b1a65ff.png)","5f83a641":"Observe as value of `max_depth` increases, both train and test scores increase till a point, but after that test score starts to decrease\n* Ensemble `tries to overfit` as `max_depth` increase\n* `controlling depth of constituent trees will help reduce overfitting in forest`\n\nI will try to specify an appropriately low value of max_depth so that trees do not overfit","a54d35c8":"`Decision Tree` is a simple and powerful predictive modeling technique, but it suffer from `high-variance`. Can say trees can get very different results given different training data <br>\nA technique to make Decision Trees more robust and to achieve better performance is called `Bootstrap Aggregation` or `Bagging for short`","b2d46d5e":"# 1. Understanding and Cleaning Data","31a23cf2":"# Tuning `min_samples_split`\n\nTrying to look at performance of ensemble as min_samples_split vary\n\n\n","1f1a7caa":"![image.png](attachment:d9863fc5-01bc-4437-9aef-8d7d0e2b67d0.png)","2d24b994":"# `Random_SearchCV` to Find Optimal Hyperparameters\nParameter:\n* `max_depth`: []\n* `n_estimators`: []\n* `max_features`: []\n* `min_samples_leaf`: range()\n* `min_samples_split`: range()","ff3b9bf1":"![image.png](attachment:7423ce08-d931-4775-94e2-e8b262532dc3.png)","28c74e2b":"# Bagging-Boosting: Similarities\nBagging | Boosting\n------|------\nmerging same type of predictions | merging different types of predictions\ndecreases variance, not bias, and solves over-fitting issues in a model | decreases bias, not variance\neach model receives an equal weight | models are weighed based on their performance\nmodels are built independently | new models are affected by a previously built model\u2019s performance\ntraining data subsets are drawn randomly with a replacement for training dataset | every new subset comprises elements that were misclassified by previous models\nusually applied where classifier is unstable and has a high variance | usually applied where classifier is stable and simple and has high bias","023824d3":"**`Making a `Pipeline Estimator` to standardize X and use DecisionTreeClassifier with random_state=108 as model`**","0e696333":"**`Checking BaggingClassifier again on optimized parameters`**","48a9c144":"# Bagging-Classifier Hyperparameter Tuning \nI am trying to perform a `Grid Search` or `Randomized Search` to get most appropriate estimator\n\nI am using `GridSearchCV` to find best parameters for both BaggingClassifier and DecisionTreeClassifier \n* `max_depth` for `DecisionTreeClassifier `\n* `max_samples` & `max_features` from `BaggingClassifier`","e1137871":"# PROBLEM STATEMENT: Credit Default Prediction\n\nBuild a model to `predict whether a given customer defaults or not`. Credit default is one of the most important problems in the banking and risk analytics industry. There are various attributes which can be used to predict default, such as demographic data (age, income, employment status, etc.), (credit) behavioural data (past loans, payment, number of times a credit payment has been delayed by the customer etc.).\n","326f171d":"# Bagging-Boosting: Differences\n|Bagging|Boosting|Stacking|\n|--------|--------|--------|\nreduces Variance, Increases Accuracy on prediction model | reduces Variance, Increases Accuracy | used to ensemble diverse group of strong lerners(model)\ncan handle outliers or noisy data | can't handle outliers or noisy data | involves training a second-level ml alo. i.e. Metalearner, to learn optimal combination of base learners\noften used with decision tree(i.e Random Forest) | flexible--can be used with any loss function |","9ea46b5e":">**Trying to fit final model with best parameters obtained from GridSearch**","4706990c":"Observe `model` tends to `overfit data` as test score is 0.72 and training score is 1.00","fb04c02b":"# `RandomForest-Classifier Hyperparameter Tuning`\nParameter:\n* `max_depth`: []\n* `n_estimators`: []\n* `max_features`: []\n* `min_samples_leaf`: range()\n* `min_samples_split`: range()","48fbb072":"I am trying to use above chosen best parameters into extra trees and tune it","8e74a0f3":"# 3.1. `Bagging Classifier`: (an ensemble meta-estimator)\nIt can be called as an ensemble meta-estimator which is `created by fitting multiple versions of base estimator`, trained with modified training data set created using bagging sampling technique (data sampled using replacement) or otherwise\n* Nagging sampling technique can result in training set consisting of duplicate dataset or unique data set\n    * This sampling technique is also called as `Bootstrap Aggregation`\n* **`Final predictor(also called as bagging classifier)` combines predictions made by each estimator\/classifier by `Voting(Classification)` or by `Averaging(Regression)`**\n\nBagging Classifier `helps in reducing variance of individual estimators by introducing randomisation into training stage of each of estimators and making an ensemble out of all estimators` \n\n#### First Checking this using simple Decision Tree to build this model","89cb2ea6":"Observe after HyperParameter tuning Random forest accuracy have improved","5469e6a5":"Observe model starts to overfit as value of `min_samples_leaf` decreases","18d3e677":"This Ensembling of models can be done in three ways:\n>1. `Bagging`\n    * Random Forest: `several random trees make a Random Forest`\n2. `Boosting`\n    * AdaBoost\n    * Gradient Boost\n    * Extreme Boost(XGBoost)\n3. `Stacking`","ec4304f8":"No major data quality issues, so I'll go ahead and build the model","d98dd371":"# 3 different types of bagging methods:\nI will use:\n>1. `Bagging Classifier`: an ensemble meta-estimator\n2. `RandomForest Classifier`\n3. `ExtraTree Classifier`","24b1fff4":"Observe 81% accuracy here\n\nI can even try hyperparameter tuning here in order to improve this result. So lets Tune the mode","4b878223":"Observe how overfitting is being eliminated using bagging method and hyperparameter tuning it","3d021687":"I have used GridSearchCV for Hyperparameter Tuning now i an trying to use RandomSearchCV to Find Optimal Hyperparameters\n","e5a968a4":"# Random Search to Find Optimal Hyperparameters\n[RandomizedSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RandomizedSearchCV.html)"}}