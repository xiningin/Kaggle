{"cell_type":{"19a9f1b8":"code","4cc99654":"code","fd2929a4":"code","f3a79881":"code","22aea09a":"code","36dadec1":"code","81d9d5e0":"code","9174f788":"code","fd5d4784":"code","02569842":"code","54ddfe8c":"code","d8557288":"code","409710c7":"code","b29cdf8b":"code","83f7b5b3":"markdown","92426793":"markdown","4a8f2cbb":"markdown","fa346920":"markdown","05cbd74a":"markdown","5f43f805":"markdown","259c3cd0":"markdown","7c96f8f5":"markdown","28db7cfd":"markdown","94284e52":"markdown","9fc021c6":"markdown","46b077d9":"markdown","833b6e11":"markdown"},"source":{"19a9f1b8":"#download data\n!mkdir data\n!wget -c https:\/\/github.com\/agungsantoso\/deep-learning-v2-pytorch\/raw\/master\/sentiment-rnn\/data\/labels.txt\n!wget -c https:\/\/github.com\/agungsantoso\/deep-learning-v2-pytorch\/raw\/master\/sentiment-rnn\/data\/reviews.txt\n!mv *.txt data\/\nimport numpy as np\n# read data from text files\nwith open('data\/reviews.txt', 'r') as f:\n     reviews = f.readlines()\nwith open('data\/labels.txt', 'r') as f:\n     labels = f.readlines()","4cc99654":"# get punctuation from string library\nfrom string import punctuation\n\n# remove Punctuation and get all the words from review dataset\nall_reviews=list()\nfor text in reviews:\n    text = text.lower()\n    text = \"\".join([ch for ch in text if ch not in punctuation])\n    all_reviews.append(text)\nall_text = \" \".join(all_reviews)\nall_words = all_text.split()","fd2929a4":"from collections import Counter \n# Count all the words using Counter Method\n\ncount_words = Counter(all_words)\ntotal_words=len(all_words)\nsorted_words=count_words.most_common(total_words)\nprint(\"Top ten occuring words : \")\nsorted_words[:10]","f3a79881":"'''\nwe will start creating dictionary with index 1 because 0 is reserved for padding\n'''\n\nvocab_to_int={w:i+1 for i,(w,c) in enumerate(sorted_words)}","22aea09a":"encoded_reviews=list()\nfor review in all_reviews:\n    encoded_review=list()\n    for word in review.split():\n        if word not in vocab_to_int.keys():\n            #if word is not available in vocab_to_int put 0 in that place\n            encoded_review.append(0)\n        else:\n            encoded_review.append(vocab_to_int[word])\n    encoded_reviews.append(encoded_review)","36dadec1":"'''\nthis step will Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n'''\n#sequence_length=500\nsequence_length=250\nfeatures=np.zeros((len(encoded_reviews), sequence_length), dtype=int)\nfor i, review in enumerate(encoded_reviews):\n    review_len=len(review)\n    if (review_len<=sequence_length):\n        zeros=list(np.zeros(sequence_length-review_len))\n        new=zeros+review\n    else:\n        new=review[:sequence_length]\n    features[i,:]=np.array(new)","81d9d5e0":"labels=[1 if label.strip()=='positive' else 0 for label in labels]","9174f788":"#split_dataset into 80% training , 10% test and 10% Validation Dataset\ntrain_x=features[:int(0.8*len(features))]\ntrain_y=labels[:int(0.8*len(features))]\nvalid_x=features[int(0.8*len(features)):int(0.9*len(features))]\nvalid_y=labels[int(0.8*len(features)):int(0.9*len(features))]\ntest_x=features[int(0.9*len(features)):]\ntest_y=labels[int(0.9*len(features)):]\nprint(len(train_y), len(valid_y), len(test_y))","fd5d4784":"import torch\nfrom torch.utils.data import DataLoader, TensorDataset\n\n#create Tensor Dataset\ntrain_data=TensorDataset(torch.LongTensor(train_x), torch.FloatTensor(train_y))\nvalid_data=TensorDataset(torch.LongTensor(valid_x), torch.FloatTensor(valid_y))\ntest_data=TensorDataset(torch.LongTensor(test_x), torch.FloatTensor(test_y))\n\n#dataloader\nbatch_size=50\ntrain_loader=DataLoader(train_data, batch_size=batch_size, shuffle=True)\nvalid_loader=DataLoader(valid_data, batch_size=batch_size, shuffle=True)\ntest_loader=DataLoader(test_data, batch_size=batch_size, shuffle=True)","02569842":"# obtain one batch of training data\ndataiter = iter(train_loader)\nsample_x, sample_y = dataiter.next()\nprint('Sample input size: ', sample_x.size()) # batch_size, seq_length\nprint('Sample input: \\n', sample_x)\nprint('Sample label size: ', sample_y.size()) # batch_size\nprint('Sample label: \\n', sample_y)","54ddfe8c":"import torch.nn as nn\n \nclass SentimentalLSTM(nn.Module):\n    \"\"\"\n    The RNN model that will be used to perform Sentiment analysis.\n    \"\"\"\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):    \n        \"\"\"\n        Initialize the model by setting up the layers\n        \"\"\"\n        super().__init__()\n        self.output_size=output_size\n        self.n_layers=n_layers\n        self.hidden_dim=hidden_dim\n        \n        #Embedding and LSTM layers\n        self.embedding=nn.Embedding(vocab_size, embedding_dim)\n        self.lstm=nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n        \n        #dropout layer\n        self.dropout=nn.Dropout(0.3)\n        \n        #Linear and sigmoid layer\n        self.fc1=nn.Linear(hidden_dim, 64)\n        self.fc2=nn.Linear(64, 16)\n        self.fc3=nn.Linear(16,output_size)\n        self.sigmoid=nn.Sigmoid()\n        \n    def forward(self, x, hidden):\n        \"\"\"\n        Perform a forward pass of our model on some input and hidden state.\n        \"\"\"\n        batch_size=x.size()\n        \n        #Embadding and LSTM output\n        embedd=self.embedding(x)\n        lstm_out, hidden=self.lstm(embedd, hidden)\n        \n        #stack up the lstm output\n        lstm_out=lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        #dropout and fully connected layers\n        out=self.dropout(lstm_out)\n        out=self.fc1(out)\n        out=self.dropout(out)\n        out=self.fc2(out)\n        out=self.dropout(out)\n        out=self.fc3(out)\n        sig_out=self.sigmoid(out)\n        \n        sig_out=sig_out.view(batch_size, -1)\n        sig_out=sig_out[:, -1]\n        \n        return sig_out, hidden\n    \n    def init_hidden(self, batch_size):\n        \"\"\"Initialize Hidden STATE\"\"\"\n        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n        # initialized to zero, for hidden state and cell state of LSTM\n        weight = next(self.parameters()).data\n        \n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n        \n        return hidden","d8557288":"# Instantiate the model w\/ hyperparams\nvocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\noutput_size = 1\nembedding_dim = 400\nhidden_dim = 256\nn_layers = 2\n\nnet = SentimentalLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\nprint(net)","409710c7":"# loss and optimization functions\nlr=0.001\n\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\n\n# check if CUDA is available\ntrain_on_gpu = torch.cuda.is_available()\n\n# training params\n\nepochs = 3 # 3-4 is approx where I noticed the validation loss stop decreasing\n\ncounter = 0\nprint_every = 100\nclip=5 # gradient clipping\n\n# move model to GPU, if available\nif(train_on_gpu):\n    net.cuda()\n\nnet.train()\n# train for some number of epochs\nfor e in range(epochs):\n    # initialize hidden state\n    h = net.init_hidden(batch_size)\n\n    # batch loop\n    for inputs, labels in train_loader:\n        counter += 1\n\n        if(train_on_gpu):\n            inputs=inputs.cuda()\n            labels=labels.cuda()\n        # Creating new variables for the hidden state, otherwise\n        # we'd backprop through the entire training history\n        h = tuple([each.data for each in h])\n\n        # zero accumulated gradients\n        net.zero_grad()\n\n        # get the output from the model\n        output, h = net(inputs, h)\n\n        # calculate the loss and perform backprop\n        loss = criterion(output.squeeze(), labels.float())\n        loss.backward()\n        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs \/ LSTMs.\n        nn.utils.clip_grad_norm_(net.parameters(), clip)\n        optimizer.step()\n\n        # loss stats\n        if counter % print_every == 0:\n            # Get validation loss\n            val_h = net.init_hidden(batch_size)\n            val_losses = []\n            net.eval()\n            for inputs, labels in valid_loader:\n\n                # Creating new variables for the hidden state, otherwise\n                # we'd backprop through the entire training history\n                val_h = tuple([each.data for each in val_h])\n\n                inputs, labels = inputs.cuda(), labels.cuda()  \n                output, val_h = net(inputs, val_h)\n                val_loss = criterion(output.squeeze(), labels.float())\n\n                val_losses.append(val_loss.item())\n\n            net.train()\n            print(\"Epoch: {}\/{}...\".format(e+1, epochs),\n                  \"Step: {}...\".format(counter),\n                  \"Loss: {:.6f}...\".format(loss.item()),\n                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))","b29cdf8b":"test_losses = [] # track loss\nnum_correct = 0\n\n# init hidden state\nh = net.init_hidden(batch_size)\n\nnet.eval()\n# iterate over test data\nfor inputs, labels in test_loader:\n\n    # Creating new variables for the hidden state, otherwise\n    # we'd backprop through the entire training history\n    h = tuple([each.data for each in h])\n\n    if(train_on_gpu):\n        inputs, labels = inputs.cuda(), labels.cuda()\n\n\n    output, h = net(inputs, h)\n\n    # calculate loss\n    test_loss = criterion(output.squeeze(), labels.float())\n    test_losses.append(test_loss.item())\n\n    # convert output probabilities to predicted class (0 or 1)\n    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n\n    # compare predictions to true label\n    correct_tensor = pred.eq(labels.float().view_as(pred))\n    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n    num_correct += np.sum(correct)\n\n\n# -- stats! -- ##\n# avg test loss\nprint(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n\n# accuracy over all test data\ntest_acc = num_correct\/len(test_loader.dataset)\nprint(\"Test accuracy: {:.3f}\".format(test_acc))","83f7b5b3":"## Step8: Our dataset has \u2018Positive\u2019 and \u2018Negative\u2019 as a label, it will be easy if we have 1 and 0, instead of \u2018Positive\u2019 and \u2018Negative\u2019","92426793":"## Step2: We need to remove all the punctuation like \u2018 !\u201d#$%&\\'()*+,-.\/:;<=>?@[\\\\]^_`{|}~ \u2019 because it is not very important to us while interpreting text using a deep learning model","4a8f2cbb":"## Step1: Get the dataset and make the list of reviews and labels,","fa346920":"## Step13: Initialize the model","05cbd74a":"## Step9: Split this feature data into Training and Validation set","5f43f805":"## Step10: Create DataLoader objects for Pytorch model","259c3cd0":"## Step3: Count all the words and sort it based on counts\n","7c96f8f5":"## Step14: Train the model","28db7cfd":"## Step15: Test the model accuracy\n","94284e52":"## Step7: make all the encoded_review of the same length","9fc021c6":"## Step12: Create an LSTM, RNN or any other model Architecture and test it to get better accuracy","46b077d9":"## Step5: Create a dictionary to convert words to Integers based on the number of occurrence of the word","833b6e11":"## Step6: Encode review in to list of Integer by using above dictionary"}}