{"cell_type":{"40edbe21":"code","d5a1561c":"code","4b52818c":"code","33f5d8c0":"code","0f4897e8":"code","73372971":"code","df2f8999":"code","8ca2e5b9":"code","bca59d63":"code","bd4b81b3":"code","ab52b303":"code","ce093666":"code","3412b94c":"code","943ac796":"code","cfb13a22":"code","451bbfb3":"code","a8b37194":"code","fce36220":"code","c7ed9d51":"code","63292731":"code","3d470287":"code","3b0cfdf9":"code","0871f211":"code","0ca37536":"code","4c3c154b":"code","8d17c7ed":"code","f0b89368":"code","f6c29e87":"code","0fb44109":"code","7e5988af":"code","715ba37c":"code","7453785b":"code","1a26136c":"code","e6b31b6d":"code","49a97514":"code","e7f3367e":"code","23dffd56":"code","db32a12f":"code","fc05ff7b":"code","fa5f88e2":"code","52250569":"code","decbe5b1":"code","203cb689":"code","51de6cd1":"code","4d0fe877":"code","16524537":"code","c27bd881":"code","a40d0c74":"code","46eb0807":"code","028ae6db":"code","c0e5b835":"code","407dcb5e":"code","002f2285":"code","83863dbf":"code","3da50452":"code","86e569f1":"code","7815bc88":"code","d721f99f":"code","4f9fc3c4":"code","6cda7c40":"code","195b794f":"code","9a5a1f51":"code","ae5e97db":"code","6514a3bb":"code","6573e40b":"code","93cad752":"code","143b2d1a":"code","183ce236":"code","25839c43":"code","3717f436":"code","8424a141":"code","754917c2":"code","8f12614d":"code","fa5eb540":"code","cf650495":"code","ee3ddf31":"code","b99dc09d":"code","3ba949fd":"code","e578c6a4":"code","32621428":"code","70fa14df":"code","935833f5":"code","a1779073":"code","3df819da":"code","a6675cc1":"code","de0672ca":"code","166995f9":"code","64ee3e6e":"code","52d543dc":"code","fec33984":"code","665cd9b6":"code","b334c380":"markdown","461d6cd2":"markdown","b535658e":"markdown","c128e641":"markdown","7377d9a6":"markdown","c6e3dd9e":"markdown","5ae7c41a":"markdown","2c31f610":"markdown","5179726d":"markdown","c720670a":"markdown","647d4584":"markdown","7c455852":"markdown","e111b0d6":"markdown","ad1a3b6b":"markdown","f7e47b2a":"markdown","3f41e2ca":"markdown","9d7389e8":"markdown","caeb102d":"markdown","3d638c74":"markdown","66c8425c":"markdown","afd424b8":"markdown","938a7120":"markdown","d8eae48b":"markdown","1a5fe699":"markdown","b8003f2e":"markdown","876b6311":"markdown","4c96eb04":"markdown","94e580e5":"markdown","ee6f7d25":"markdown","d844d159":"markdown","3daab6bd":"markdown","3ef17a13":"markdown","b6a91b78":"markdown","0b889d0a":"markdown","ce40005c":"markdown","4bbc0c17":"markdown","bb4835d0":"markdown"},"source":{"40edbe21":"import numpy as np\nimport pandas as pd\nfrom os.path import join as opj\nfrom tqdm.notebook import tqdm","d5a1561c":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nsns.set_context('notebook')","4b52818c":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.svm import SVR","33f5d8c0":"path = '\/kaggle\/input\/trends-feature-exploration-engineering\/datasets'","0f4897e8":"# Load target values and corresponding scaler\nimport joblib\nscaler_targets = joblib.load(opj(path, 'targets_scaler.pkl'))\n\ntargets = pd.read_hdf(opj(path, 'targets.h5'))\ntargets.head()","73372971":"def load_dataset_and_scale(target, dataset_id='merge', scale_values=[1, 1, 1, 1]):\n\n    # Load dataset\n    X_tr = pd.read_hdf(opj(path, '%s_train.h5' % dataset_id))\n    X_te = pd.read_hdf(opj(path, '%s_test.h5' % dataset_id))\n\n    #\u00a0Specify target\n    y = pd.read_hdf(opj(path, 'targets.h5'))\n    y_tr = y.loc[X_tr.index, target]\n\n    # Remove missing values\n    missval = y_tr.isnull().values\n    idx = ~missval\n    X_tr = X_tr[idx]\n    X_te = X_te\n    y_tr = y_tr[idx]\n    print('Removing %s missing values from target dataset.' % missval.sum())\n    \n    # Centralize corr_coef features\n    median_offset = X_tr.iloc[:, X_tr.columns.str.contains('corr_coef')].median().mean()\n    X_tr.iloc[:, X_tr.columns.str.contains('corr_coef')] -= median_offset\n    X_te.iloc[:, X_te.columns.str.contains('corr_coef')] -= median_offset\n\n    # Establish masks for different kinds of data\n    mask_ids = []\n    for m in ['IC_', '_vs_', 'corr_coef', '^c[0-9]+_c[0-9]+']:\n        mask_ids.append(X_tr.columns.str.contains(m))\n    mask_ids = np.array(mask_ids)\n\n    # Data scaling\n    for i, m in enumerate(mask_ids):\n\n        if m.sum()==0:\n            continue\n        \n        # Apply Scale\n        scale_value = scale_values[i]\n        unify_mask_scale = np.percentile(X_tr.iloc[:, m].abs(), 90)\n\n        X_te.iloc[:, m] \/= unify_mask_scale\n        X_tr.iloc[:, m] \/= unify_mask_scale\n    \n        X_te.iloc[:, m] *= scale_value\n        X_tr.iloc[:, m] *= scale_value\n\n    # Drop irrelevant measurements\n    X_tr.dropna(axis=1, inplace=True)\n    X_te.dropna(axis=1, inplace=True)\n    \n    # Drop duplicate rows\n    X_tr = X_tr.T.drop_duplicates().T\n    X_te = X_te.T.drop_duplicates().T\n    \n    print('Size of dataset (train\/test): ', X_tr.shape, X_te.shape)\n    \n    X_tr = X_tr.values\n    X_te = X_te.values\n    y_tr = y_tr.values\n    \n    return X_tr, X_te, y_tr","df2f8999":"# Create scorer function\nfrom sklearn.metrics import make_scorer\ndef model_metric(y_true, y_pred, scaler=None, tidx=0):\n    \n    # List of power transformations\n    pow_age = 1.0\n    pow_d1v1 = 1.5\n    pow_d1v2 = 1.5\n    pow_d2v1 = 1.5\n    pow_d2v2 = 1.5\n    pow_d21 = 1.5\n    pow_d22 = 1.0\n\n    powers = [pow_age, pow_d1v1, pow_d1v2, pow_d2v1, pow_d2v2, pow_d21, pow_d22]\n    \n    # Invert scaler\n    t_true = scaler.inverse_transform(np.transpose([y_true] * 7))[:, tidx]\n    t_pred = scaler.inverse_transform(np.transpose([y_pred] * 7))[:, tidx]\n    \n    #\u00a0Assign closest value from training set\n    unique_values = np.unique(t_true)\n    for i, a in enumerate(t_pred):\n        t_pred[i] = unique_values[np.argmin(np.abs(a-unique_values))]\n\n    # Invert power transformation\n    t_true = np.power(t_true, 1.\/powers[tidx])\n    t_pred = np.power(t_pred, 1.\/powers[tidx])\n    \n    # Compute the score\n    score = np.mean(np.sum(np.abs(t_true - t_pred), axis=0) \/ np.sum(t_true, axis=0))\n    return score","8ca2e5b9":"def create_grid(model_metric, alphas=[0.1, 1, 10], estimator=None,\n                cv=5, scaler_targets=None, tidx=0):\n\n    # Create Pipeline\n    pipeline = Pipeline([\n        ('scaler', None),\n        ('estimator', estimator),\n    ])\n\n    # Define parameter grid\n    param_grid = [{'scaler': [None, RobustScaler()],\n                   'estimator__alpha': alphas,\n                  }]\n\n    # Create grid search object\n    f_scorer = make_scorer(model_metric, greater_is_better=False,\n                           scaler=scaler_targets, tidx=tidx)\n    grid = GridSearchCV(pipeline,\n                        cv=cv,\n                        param_grid=param_grid,\n                        scoring=f_scorer,\n                        return_train_score=True,\n                        verbose=5,\n                        n_jobs=-1)\n\n    return grid","bca59d63":"# Select the target (name and index)\ntarget = 'age'\ntidx = 0\n\n# Let's select which features to use and how to scale them\nscale_values = [1,        # IC features\n                1,        # FNC features\n                np.nan,   #\u00a0intra features\n                np.nan]   #\u00a0inter features\n\nX_tr, X_te, y_tr =  load_dataset_and_scale(target, scale_values=scale_values)","bd4b81b3":"# Define estimator\nestimator = Ridge(tol=1e-3)\n\n# Create grid search object\nalphas = np.logspace(0, 4, 21)\ngrid = create_grid(model_metric, alphas=alphas, estimator=estimator,\n                   cv=5, scaler_targets=scaler_targets, tidx=tidx)\n\n# Run grid search\n_ = grid.fit(X_tr, y_tr)\n\n# Provide some insights into the models top performance\nprint('Dataset scales used: ', scale_values)\nprint(\"Best score at: %f using %s\" % (grid.best_score_, grid.best_params_))","ab52b303":"def extract_predictions(X_tr, X_te, grid, y_tr):\n\n    # Store predictions in dictionary\n    res = {}\n    res['tr'] = grid.predict(X_tr)\n    res['te'] = grid.predict(X_te)\n    \n    #\u00a0Assign closest value from training set \n    unique_values = np.unique(y_tr)\n    for t in ['tr', 'te']:\n        for i, a in enumerate(res[t]):\n            res[t][i] = unique_values[np.argmin(np.abs(a-unique_values))]\n\n    return res['tr'], res['te']","ce093666":"# Extract the predictions for the training and the test set\npred_tr, pred_te = extract_predictions(X_tr, X_te, grid, y_tr)","3412b94c":"def plot_predictions(pred_tr, pred_te, y_tr):\n\n    # Plot prediction descrepancy on training and test set\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n\n    ax[0].set_title('Prediction X_te: %s' % target)\n    ax[0].plot(pred_te, '.', alpha=0.5, markersize=5)\n    ax[0].plot(pred_tr, '.', alpha=0.3, markersize=5)\n    ax[0].legend(['Train', 'Test'])\n    ax[0].set_ylabel('target value')\n    ax[0].set_xlabel('Sample')\n\n    ax[1].set_title('Prediction X_tr: %s' % target)\n    sns.regplot(x=pred_tr, y=y_tr, marker='.', ax=ax[1], scatter_kws={'s':10})\n    ax[1].set_xlim([-3.5, 3.5])\n    ax[1].set_ylim([-3.5, 3.5])\n    ax[1].set_ylabel('True value')\n    ax[1].set_xlabel('Predicted value')\n    \n    plt.show()","943ac796":"# Plot the predictions with respect to each other and to the target\nplot_predictions(pred_tr, pred_te, y_tr)","cfb13a22":"def create_df_pred(grid):\n\n    # Store grid search parameters and outcomes in dataframe\n    df_pred = pd.DataFrame(grid.cv_results_)\n    columns = [c for c in df_pred.columns if 'time' not in c\n               and 'split' not in c\n               and 'rank' not in c\n               and c!='params']\n    df_pred = df_pred[columns].sort_values('mean_test_score', ascending=False)\n    df_pred['param_estimator__alpha'] = df_pred['param_estimator__alpha'].astype('float')\n    df_pred['param_scaler'] = df_pred['param_scaler'].astype('str')\n    \n    return df_pred","451bbfb3":"# Creates dataframe about grid point's performance\ndf_pred = create_df_pred(grid).sort_values('mean_test_score')\ndf_pred.head()","a8b37194":"def plot_hyperparam_fitting(df_pred):\n\n    # Plot the model fit information\n    for s in df_pred['param_scaler'].unique():\n\n        df_plot = df_pred[np.prod([df_pred['param_scaler']==s],\n                                  axis=0).astype('bool')]\n\n        df_plot = df_plot.sort_values('param_estimator__alpha')\n\n        # Extract relevant modelling metrics\n        train_scores = df_plot['mean_train_score']\n        valid_scores = df_plot['mean_test_score']\n        std_tr = df_plot['std_train_score']\n        std_va = df_plot['std_test_score']\n\n        plt.figure(figsize=(12, 4))\n        alphas = df_plot['param_estimator__alpha']\n        plt.semilogx(alphas, train_scores, label='Training Set')\n        plt.semilogx(alphas, valid_scores, label='Validation Set')\n\n        # Add marker and text for best score\n        max_id = np.argmax(valid_scores)\n        x_pos = alphas.iloc[max_id]\n        y_pos = valid_scores.iloc[max_id]\n        txt = '{:0.4f}'.format(y_pos)\n        plt.scatter(x_pos, y_pos, marker='x', c='red', zorder=10)\n        plt.text(x_pos, y_pos, txt, fontdict={'size': 18})\n\n        # Quantify variance with \u00b1std curves\n        plt.fill_between(alphas, train_scores-std_tr, train_scores+std_tr, alpha=0.3)\n        plt.fill_between(alphas, valid_scores-std_va, valid_scores+std_va, alpha=0.3)\n        plt.ylabel('Performance metric')\n        plt.xlabel('Model parameter')\n\n        # Adjust x-lim, y-lim, add legend and adjust layout\n        plt.legend()\n        plt.title('Scaler: %s' % s)\n        plt.show()","fce36220":"# Plot prediction behaviour\nplot_hyperparam_fitting(df_pred)","c7ed9d51":"# Select the target (name and index)\ntarget = 'age'\ntidx = 0\n\n# Let's select which features to use and how to scale them\nscale_values = [1,        # IC features\n                1,        # FNC features\n                1,        #\u00a0intra features\n                1]        #\u00a0inter features\n\nX_tr, X_te, y_tr =  load_dataset_and_scale(target, scale_values=scale_values)","63292731":"# Define estimator\nestimator = Ridge(tol=1e-3)\n\n# Create grid search object\nalphas = np.logspace(1, 5, 21)\ngrid = create_grid(model_metric, alphas=alphas, estimator=estimator,\n                   cv=5, scaler_targets=scaler_targets, tidx=tidx)\n\n# Run grid search\n_ = grid.fit(X_tr, y_tr)\n\n# Provide some insights into the models top performance\nprint('Dataset scales used: ', scale_values)\nprint(\"Best score at: %f using %s\" % (grid.best_score_, grid.best_params_))","3d470287":"# Extract the predictions for the training and the test set\npred_tr, pred_te = extract_predictions(X_tr, X_te, grid, y_tr)","3b0cfdf9":"# Plot the predictions with respect to each other and to the target\nplot_predictions(pred_tr, pred_te, y_tr)","0871f211":"# Creates dataframe about grid point's performance\ndf_pred = create_df_pred(grid).sort_values('mean_test_score', ascending=False)\ndf_pred.head()","0ca37536":"# Plot prediction behaviour\nplot_hyperparam_fitting(df_pred)","4c3c154b":"def run_prediction(model_metric, estimator=None, alphas=[0.1, 1, 10], cv=5,\n                   scaler_targets=None, target='age', tidx=0, scale_values=None):\n    \n    # Extract dataset\n    X_tr, X_te, y_tr = load_dataset_and_scale(target, scale_values=scale_values)\n    \n    # Create grid search object\n    grid = create_grid(model_metric, alphas=alphas, estimator=estimator,\n                       cv=cv, scaler_targets=scaler_targets, tidx=tidx)\n    \n    # Run grid search\n    _ = grid.fit(X_tr, y_tr)\n    \n    # Provide some insights into the models top performance\n    print('Dataset scales used: ', scale_values)\n    print(\"Best score at: %f using %s\" % (grid.best_score_, grid.best_params_))\n\n    # Extract the predictions for the training and the test set\n    pred_tr, pred_te = extract_predictions(X_tr, X_te, grid, y_tr)\n\n    # Plot the predictions with respect to each other and to the target\n    plot_predictions(pred_tr, pred_te, y_tr)\n\n    # Creates dataframe about grid point's performance\n    df_pred = create_df_pred(grid)\n    display(df_pred.sort_values('mean_test_score', ascending=False).head())\n\n    # Plot prediction behaviour\n    plot_hyperparam_fitting(df_pred)\n    \n    return df_pred, pred_tr, pred_te, grid, y_tr","8d17c7ed":"# Select the target (name and index)\ntarget = 'age'\ntidx = 0","f0b89368":"# Let's select which features to use and how to scale them\nscale_values = [0.25,     # Feature: IC\n                0.04,     # Feature: FNC\n                np.nan,   # Feature: Intra Corr\n                np.nan,   # Feature: Inter Corr\n               ]","f6c29e87":"# Define model parameters\nestimator = Ridge(tol=1e-3)\nalphas = np.logspace(-2, 4, 31)\ncv = 5","0fb44109":"df_pred, pred_tr, pred_te, grid, y_tr = run_prediction(\n    model_metric, estimator=estimator, alphas=alphas, cv=cv,\n    scaler_targets=scaler_targets, target=target, tidx=tidx,\n    scale_values=scale_values)","7e5988af":"# Select the target (name and index)\ntarget = 'age'\ntidx = 0","715ba37c":"# Let's select which features to use and how to scale them\nscale_values = [0.25,     # Feature: IC\n                0.04,     # Feature: FNC\n                0.087,    # Feature: Intra Corr\n                np.nan,   # Feature: Inter Corr\n               ]","7453785b":"# Define model parameters\nestimator = Ridge(tol=1e-3)\nalphas = np.logspace(-2, 4, 31)\ncv = 5","1a26136c":"df_pred, pred_tr, pred_te, grid, y_tr = run_prediction(\n    model_metric, estimator=estimator, alphas=alphas, cv=cv,\n    scaler_targets=scaler_targets, target=target, tidx=tidx,\n    scale_values=scale_values)","e6b31b6d":"# Select the target (name and index)\ntarget = 'age'\ntidx = 0","49a97514":"# Let's select which features to use and how to scale them\nscale_values = [0.25,     # Feature: IC\n                0.04,     # Feature: FNC\n                0.087,    # Feature: Intra Corr\n                0.025,    # Feature: Inter Corr\n               ]","e7f3367e":"# Define model parameters\nestimator = Ridge(tol=1e-3)\nalphas = np.logspace(-2, 4, 31)\ncv = 5","23dffd56":"df_pred, pred_tr, pred_te, grid, y_tr = run_prediction(\n    model_metric, estimator=estimator, alphas=alphas, cv=cv,\n    scaler_targets=scaler_targets, target=target, tidx=tidx,\n    scale_values=scale_values)","db32a12f":"# Collect test predictions\npredictions_ridge = {}\n\n# Save predictions for age in output variable\npredictions_ridge[target] = pred_te","fc05ff7b":"# Select the target (name and index)\ntarget = 'domain1_var1'\ntidx = 1","fa5f88e2":"# Let's select which features to use and how to scale them\nscale_values = [0.25,    # Feature: IC\n                0.01,    # Feature: FNC\n                0.032,   # Feature: Intra Corr\n                0.019,   # Feature: Inter Corr\n               ]","52250569":"# Define model parameters\nestimator = Ridge(tol=1e-3)\nalphas = np.logspace(-1, 5, 31)\ncv = 5","decbe5b1":"df_pred, pred_tr, pred_te, grid, y_tr = run_prediction(\n    model_metric, estimator=estimator, alphas=alphas, cv=cv,\n    scaler_targets=scaler_targets, target=target, tidx=tidx,\n    scale_values=scale_values)","203cb689":"# Save predictions for domain1_var1 in output variable\npredictions_ridge[target] = pred_te","51de6cd1":"# Select the target (name and index)\ntarget = 'domain1_var2'\ntidx = 2","4d0fe877":"# Let's select which features to use and how to scale them\nscale_values = [0.121,   # Feature: IC\n                0.019,   # Feature: FNC\n                0.032,   # Feature: Intra Corr\n                0.025,   # Feature: Inter Corr\n               ]","16524537":"# Define model parameters\nestimator = Ridge(tol=1e-3)\nalphas = np.logspace(-1, 5, 31)\ncv = 5","c27bd881":"df_pred, pred_tr, pred_te, grid, y_tr = run_prediction(\n    model_metric, estimator=estimator, alphas=alphas, cv=cv,\n    scaler_targets=scaler_targets, target=target, tidx=tidx,\n    scale_values=scale_values)","a40d0c74":"# Save predictions for domain1_var2 in output variable\npredictions_ridge[target] = pred_te","46eb0807":"# Select the target (name and index)\ntarget = 'domain2_var1'\ntidx = 3","028ae6db":"# Let's select which features to use and how to scale them\nscale_values = [0.25,    # Feature: IC\n                0.008,   # Feature: FNC\n                0.012,   # Feature: Intra Corr\n                0.012,   # Feature: Inter Corr\n               ]","c0e5b835":"# Define model parameters\nestimator = Ridge(tol=1e-3)\nalphas = np.logspace(-1, 5, 31)\ncv = 5","407dcb5e":"df_pred, pred_tr, pred_te, grid, y_tr = run_prediction(\n    model_metric, estimator=estimator, alphas=alphas, cv=cv,\n    scaler_targets=scaler_targets, target=target, tidx=tidx,\n    scale_values=scale_values)","002f2285":"# Save predictions for domain2_var1 in output variable\npredictions_ridge[target] = pred_te","83863dbf":"# Select the target (name and index)\ntarget = 'domain2_var2'\ntidx = 4","3da50452":"# Let's select which features to use and how to scale them\nscale_values = [0.261,   # Feature: IC\n                0.025,   # Feature: FNC\n                0.052,   # Feature: Intra Corr\n                0.022,   # Feature: Inter Corr\n               ]","86e569f1":"# Define model parameters\nestimator = Ridge(tol=1e-3)\nalphas = np.logspace(-1, 5, 31)\ncv = 5","7815bc88":"df_pred, pred_tr, pred_te, grid, y_tr = run_prediction(\n    model_metric, estimator=estimator, alphas=alphas, cv=cv,\n    scaler_targets=scaler_targets, target=target, tidx=tidx,\n    scale_values=scale_values)","d721f99f":"# Save predictions for domain2_var2 in output variable\npredictions_ridge[target] = pred_te","4f9fc3c4":"def create_grid(model_metric, alphas=[0.1, 1, 10], estimator=None,\n                cv=5, scaler_targets=None, tidx=0):\n\n    # Create Pipeline\n    pipeline = Pipeline([\n        ('estimator', estimator),\n    ])\n\n    # Define parameter grid\n    param_grid = [{'estimator__C': alphas}]\n\n    # Create grid search object\n    f_scorer = make_scorer(model_metric, greater_is_better=False,\n                           scaler=scaler_targets, tidx=tidx)\n    grid = GridSearchCV(pipeline,\n                        cv=cv,\n                        param_grid=param_grid,\n                        scoring=f_scorer,\n                        return_train_score=True,\n                        verbose=5,\n                        n_jobs=-1)\n\n    return grid","6cda7c40":"def create_df_pred(grid):\n\n    # Store grid search parameters and outcomes in dataframe\n    df_pred = pd.DataFrame(grid.cv_results_)\n    columns = [c for c in df_pred.columns if 'time' not in c\n               and 'split' not in c\n               and 'rank' not in c\n               and c!='params']\n    df_pred = df_pred[columns].sort_values('mean_test_score', ascending=False)\n    df_pred['param_estimator__C'] = df_pred['param_estimator__C'].astype('float')\n\n    return df_pred","195b794f":"def plot_hyperparam_fitting(df_pred):\n\n    # Plot the model fit information\n    df_plot = df_pred.copy()\n\n    df_plot = df_plot.sort_values('param_estimator__C')\n\n    # Extract relevant modelling metrics\n    train_scores = df_plot['mean_train_score']\n    valid_scores = df_plot['mean_test_score']\n    std_tr = df_plot['std_train_score']\n    std_va = df_plot['std_test_score']\n\n    plt.figure(figsize=(12, 4))\n    Cs = df_plot['param_estimator__C']\n    plt.semilogx(Cs, train_scores, label='Training Set')\n    plt.semilogx(Cs, valid_scores, label='Validation Set')\n\n    # Add marker and text for best score\n    max_id = np.argmax(valid_scores)\n    x_pos = Cs.iloc[max_id]\n    y_pos = valid_scores.iloc[max_id]\n    txt = '{:0.4f}'.format(y_pos)\n    plt.scatter(x_pos, y_pos, marker='x', c='red', zorder=10)\n    plt.text(x_pos, y_pos, txt, fontdict={'size': 18})\n\n    # Quantify variance with \u00b1std curves\n    plt.fill_between(Cs, train_scores-std_tr, train_scores+std_tr, alpha=0.3)\n    plt.fill_between(Cs, valid_scores-std_va, valid_scores+std_va, alpha=0.3)\n    plt.ylabel('Performance metric')\n    plt.xlabel('Model parameter')\n\n    # Adjust x-lim, y-lim, add legend and adjust layout\n    plt.legend()\n    plt.show()","9a5a1f51":"# Select the target (name and index)\ntarget = 'age'\ntidx = 0","ae5e97db":"# Let's select which features to use and how to scale them\nscale_values = [0.25,      # Feature: IC\n                0.015,     # Feature: FNC\n                0.042,     # Feature: Intra Corr\n                0.014,     # Feature: Inter Corr\n               ]","6514a3bb":"# Define model parameters\nestimator = SVR(gamma='scale', epsilon=0.2, tol=1e-3, max_iter=5000)\nCs = np.logspace(-1, 1, 11)\ncv = 5","6573e40b":"df_pred, pred_tr, pred_te, grid, y_tr = run_prediction(\n    model_metric, estimator=estimator, alphas=Cs, cv=cv,\n    scaler_targets=scaler_targets, target=target, tidx=tidx,\n    scale_values=scale_values)","93cad752":"# Collect test predictions\npredictions_svr = {}\n\n# Save predictions for age in output variable\npredictions_svr[target] = pred_te","143b2d1a":"# Select the target (name and index)\ntarget = 'domain1_var1'\ntidx = 1","183ce236":"# Let's select which features to use and how to scale them\nscale_values = [0.25,      # Feature: IC\n                0.012,     # Feature: FNC\n                0.032,     # Feature: Intra Corr\n                0.018,     # Feature: Inter Corr\n               ]","25839c43":"# Define model parameters\nestimator = SVR(gamma='scale', epsilon=0.2, tol=1e-3, max_iter=5000)\nCs = np.logspace(-1, 1, 11)\ncv = 5","3717f436":"df_pred, pred_tr, pred_te, grid, y_tr = run_prediction(\n    model_metric, estimator=estimator, alphas=Cs, cv=cv,\n    scaler_targets=scaler_targets, target=target, tidx=tidx,\n    scale_values=scale_values)","8424a141":"# Save predictions for age in output variable\npredictions_svr[target] = pred_te","754917c2":"# Select the target (name and index)\ntarget = 'domain1_var2'\ntidx = 2","8f12614d":"# Let's select which features to use and how to scale them\nscale_values = [0.18,      # Feature: IC\n                0.01,      # Feature: FNC\n                np.nan,    # Feature: Intra Corr (no improvement)\n                0.025,     # Feature: Inter Corr\n               ]","fa5eb540":"# Define model parameters\nestimator = SVR(gamma='scale', epsilon=0.2, tol=1e-3, max_iter=5000)\nCs = np.logspace(-1.5, 0.5, 11)\ncv = 5","cf650495":"df_pred, pred_tr, pred_te, grid, y_tr = run_prediction(\n    model_metric, estimator=estimator, alphas=Cs, cv=cv,\n    scaler_targets=scaler_targets, target=target, tidx=tidx,\n    scale_values=scale_values)","ee3ddf31":"# Save predictions for age in output variable\npredictions_svr[target] = pred_te","b99dc09d":"# Select the target (name and index)\ntarget = 'domain2_var1'\ntidx = 3","3ba949fd":"# Let's select which features to use and how to scale them\nscale_values = [0.25,      # Feature: IC\n                0.025,     # Feature: FNC\n                0.036,     # Feature: Intra Corr (no improvement)\n                0.023,     # Feature: Inter Corr\n               ]","e578c6a4":"# Define model parameters\nestimator = SVR(gamma='scale', epsilon=0.2, tol=1e-3, max_iter=5000)\nCs = np.logspace(-1, 1, 11)\ncv = 5","32621428":"df_pred, pred_tr, pred_te, grid, y_tr = run_prediction(\n    model_metric, estimator=estimator, alphas=Cs, cv=cv,\n    scaler_targets=scaler_targets, target=target, tidx=tidx,\n    scale_values=scale_values)","70fa14df":"# Save predictions for age in output variable\npredictions_svr[target] = pred_te","935833f5":"# Select the target (name and index)\ntarget = 'domain2_var2'\ntidx = 4","a1779073":"# Let's select which features to use and how to scale them\nscale_values = [0.189,      # Feature: IC\n                0.025,      # Feature: FNC\n                0.050,    # Feature: Intra Corr (no improvement)\n                0.022,     # Feature: Inter Corr\n               ]","3df819da":"# Define model parameters\nestimator = SVR(gamma='scale', epsilon=0.2, tol=1e-3, max_iter=5000)\nCs = np.logspace(-1, 1, 11)\ncv = 5","a6675cc1":"df_pred, pred_tr, pred_te, grid, y_tr = run_prediction(\n    model_metric, estimator=estimator, alphas=Cs, cv=cv,\n    scaler_targets=scaler_targets, target=target, tidx=tidx,\n    scale_values=scale_values)","de0672ca":"# Save predictions for age in output variable\npredictions_svr[target] = pred_te","166995f9":"# Load sample submission file\nsubmission = pd.read_csv(opj('\/kaggle', 'input', 'trends-assessment-prediction', 'sample_submission.csv')).set_index('Id')\nsubmission.head()","64ee3e6e":"def back_transform(y_test, unique_values, scaler=None, tidx=0):\n    \n    # List of power transformations\n    pow_age = 1.0\n    pow_d1v1 = 1.5\n    pow_d1v2 = 1.5\n    pow_d2v1 = 1.5\n    pow_d2v2 = 1.5\n    pow_d21 = 1.5\n    pow_d22 = 1.0\n\n    powers = [pow_age, pow_d1v1, pow_d1v2, pow_d2v1, pow_d2v2, pow_d21, pow_d22]\n    \n    #\u00a0Assign closest value from training set\n    for i, a in enumerate(y_test):\n        y_test[i] = unique_values[np.argmin(np.abs(a-unique_values))]\n    \n    # Invert scaler\n    y_test = scaler.inverse_transform(np.transpose([y_test] * 7))[:, tidx]\n    \n    # Invert power transformation\n    y_test = np.power(y_test, 1.\/powers[tidx])\n\n    return y_test","52d543dc":"# Fill up the submission file with the ridge predictions\nprediction_dict = predictions_ridge\nfor i, t in enumerate(['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']):\n    unique_values = sorted(targets[t].dropna().unique())\n    pred_values = back_transform(prediction_dict[t], unique_values, scaler=scaler_targets, tidx=i)\n    submission.iloc[submission.index.str.contains(t), 0] = pred_values\n\n# Let's visualize a few points from the submission file\ndisplay(submission.head(10))\n\n# Store predictions in CSF file\nsubmission.to_csv('submission_ridge.csv')","fec33984":"# Fill up the submission file with the ridge predictions\nprediction_dict = predictions_svr\nfor i, t in enumerate(['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']):\n    unique_values = sorted(targets[t].dropna().unique())\n    pred_values = back_transform(prediction_dict[t], unique_values, scaler=scaler_targets, tidx=i)\n    submission.iloc[submission.index.str.contains(t), 0] = pred_values\n\n# Let's visualize a few points from the submission file\ndisplay(submission.head(10))\n\n# Store predictions in CSF file\nsubmission.to_csv('submission_svr.csv')","665cd9b6":"for c in predictions_ridge.keys():\n    plt.figure(figsize=(6, 6))\n    plt.scatter(predictions_ridge[c], predictions_svr[c], s=2, alpha=0.5)\n    plt.title(c)\n    plt.xlabel('Ridge Prediction')\n    plt.ylabel('SVR Prediction')\n    plt.show()","b334c380":"# 5. Compute predictions for all 5 targets using Ridge","461d6cd2":"# 4. Manual data scaling\n\nGoing against my initial intuition, I've tried the scaling of the FNC features. I was curious why some people scaled with a factor of 500, others with 400, 600 or 300. My solution to this conundrum was, let's triangulate the optimal scaling.\n\nThe task was straight forward and very iterative. Let's try different values within the variable...\n\n```python\nscale_values = [1,        # IC features\n                1,        # FNC features\n                1,        #\u00a0intra features\n                1]        #\u00a0inter features\n\n```\n\nand see if this improves the modeling. As much as I can observe, using different scales means also moving the optimal \"alpha peak\" around. It's kind of like a summazion of different waves where you try to stack the peaks. Changing the scales, changes the peaks location (not necessarily the hight) on the x axis. Problem here is clearly, everytime you add another feature set, the regularization term might shift heavily.\n\nNonetheless, this path proved to be fruitful. So let's see the effect. Again, finding the right values, was a triangulation\/iterative approach.\n\nBut before we start, let's package this all into a function.","b535658e":"# 2. Define scoring function and model parameters\n\nNow that the data is ready, let's write a scoring function which **(1)** reverts the target adaptations (scaling and power transformation) and **(2)** makes sure that the predicted values are within the scope of the target values. The actual scoring function is the mean absolute error, as defined by the competition.","c128e641":"## 3.2. Data modeling with 4 feature sets and RobustScaler","7377d9a6":"### Extract predictions and plot them","c6e3dd9e":"## 5.4. Predicting `domain2_var2` using Ridge","5ae7c41a":"### Score for pure SVR model\n\nOnly using the predictions from the SVR model, we would have reached a score of 0.15769, which in the final rating would have lead to the 19th place in the public leaderboard.\n\n\n**Note**: Out of interest, I also ran the predcition with the \"feature offset correction\" described by the 1st place team (see [here](https:\/\/www.kaggle.com\/c\/trends-assessment-prediction\/discussion\/163017), which moved the score of the SVR model to 0.15738, corresponding to the 13th position on the public leaderboard.","2c31f610":"## Conclusion 1\n\nUsing the manual scaling approach, together with the four feature datastes is very powerful, but mostly only helps with the prediction of `age`.\n\nIt is possible to increase the scores slightly, by focusing in on the grid space and optimize the alpha value (as well as the manual scaling factor) more precisely, but the risk is clearly also there for overfitting.\n\nIncreasing the number of folds (i.e. `cv`) helps with some targets and worsens other. But the generalization probably improves nonetheless.","5179726d":"## 6.3. Predicting `domain1_var2` using SVR(rbf)","c720670a":"Now to better understand what exactly happend, lets write some helper functions.","647d4584":"## 6.2. Predicting `domain1_var1` using SVR(rbf)","7c455852":"# Data Scaling and Modeling\n\nAfter taking a closer look at the targets (see my [first notebook](https:\/\/www.kaggle.com\/miykael\/trends-exploration-of-the-targets)) and performing feature exploration and engineering (see my [second notebook](https:\/\/www.kaggle.com\/miykael\/trends-feature-exploration-engineering)), we are ready to train some models and perform some predictions.\n\nWith this notebook I want to show how I went from an average performing model to one in the top 25th, just by **(1)** using adapted targets, **(2)** use engineered features from the MRI maps and **(3)** fine tune the scaling of the different datasets. Especially the last point is something that went against my initial intuition.","e111b0d6":"# 1. Load targets and features\n\nFirst things first, let's load the adapted targets and prepared feature datasets from my [second notebook](https:\/\/www.kaggle.com\/miykael\/trends-feature-exploration-engineering).","ad1a3b6b":"## Observation 3\n\nAs we saw in other scaling approaches (e.g. scaling with a factor of 500), we can see that the results improved and are now better than ones with the `RobustScaler()`. Let's see what the other two feature sets bring to the table.","f7e47b2a":"## Observation 2\n\nHmm... things seem to have gotten worse. It's at that time that I've explored multiple other models. SVR (linear\/rbf), lasso, elastinet, SGD, KNN, RandomForests, Neural networks, etc. Nothing seems to have help.","3f41e2ca":"## 6.5. Predicting `domain2_var2` using SVR(rbf)","9d7389e8":"## 4.3. Data modeling with 4 manually scaled feature sets","caeb102d":"## 6.4. Predicting `domain2_var1` using SVR(rbf)","3d638c74":"## Observartion 4\n\nAs we can see, using these 4 feature datasets with manually scaled properties, we can reduce the mean absolute error for age quiet a bit. However, using a standard `RobustScaler` approach, this advantage gets lost and we are worse off than before.","66c8425c":"## 5.3. Predicting `domain2_var1` using Ridge","afd424b8":"Let's write a function that inverts the initial target feature adapataions (i.e. scaling and power transformation).","938a7120":"### Score for pure Ridge model\n\nOnly using the predictions from the Ridge model, we would have reached a score of 0.15808, which in the final rating would have lead to the 24th place in the public leaderboard.\n\n**Note**: Out of interest, I also ran the predcition with the \"feature offset correction\" described by the 1st place team (see [here](https:\/\/www.kaggle.com\/c\/trends-assessment-prediction\/discussion\/163017), which moved the score of the Ridge model to 0.15787, corresponding to the 21st position on the public leaderboard.","d8eae48b":"## 6.1. Predicting `age` using SVR(rbf)","1a5fe699":"# 8. Visualize differences in prediction\n\nAs a final last step, let's also look at the prediction differences between the two models.","b8003f2e":"## 4.2. Data modeling with 3 manually scaled feature sets","876b6311":"### Collect information about grid point performance and plot them","4c96eb04":"## 4.1. Data modeling with 2 manually scaled feature sets","94e580e5":"## 5.1. Predicting `domain1_var1` using Ridge","ee6f7d25":"# 6. Compute prediction for all 5 targets using SVR (rbf)\n\nAs other's have mentioned in the discussions, Ridge and SVR seem to be very suited to predict the target values. For this reason, let's do the same thing as we did above, but this time with an SVR estimator, using an rbf kernel.\n\nThe only thing we need to adapt to make this work is the following three functions.\n\n**Note**: To reduce computation time, we will remove the `RobustScaler` from the grid search.","d844d159":"## 5.2. Predicting `domain1_var2` using Ridge","3daab6bd":"To load the feature matrix, let's write a function that performs the following steps:\n\n1. Load the train and test set.\n2. Select the target feature from the 5 (resp. 7) targets.\n3. Centralize the corr_coef features to the median, which is about 0.689\n4. Scale the four feature datasets (IC, FNC, intra and inter correlations) according to a scaler called `scale_values`. Before this scaler is applied, all feature within a dataset are scaled to the 90% value (i.e. top 10%). No centralization was applied.\n5. Missing values are dropped.","3ef17a13":"## Observation 1\n\nAs we can see, the unviersally scaled (i.e. using `RobustScaler()`) data leads to better predictions. Let's see what happens if we include all 4 feature sets.","b6a91b78":"## 3.1. Data modeling with 2 feature sets and RobustScaler","0b889d0a":"The **left** figure shows the individual prediction values (in y direction) in a sequential fashion next to each other (i.e. feature 0 to 5835 = `X_tr.shape[1]`). In blue are the values from the training set, in orange the one from the test set.\n\nIn the **right** figure, we can see the relationship between the predicted and the true target values.","ce40005c":"# 7. Saving predictions","4bbc0c17":"Let's also write a function that generates the modeling pipeline, parameter grid and runs the `GridSearchCV` object.","bb4835d0":"#\u00a03. Initial data modeling approach (standard)\n\nMy first data modeling approach was rather standard. Take the data, apply a scaler, run a Ridge regression. So let's try this out, first only on the IC and FNC features and than once also with the feature engineered features."}}