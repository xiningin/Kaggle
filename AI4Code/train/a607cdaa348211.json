{"cell_type":{"23ef197c":"code","9e66e822":"code","7201a65e":"code","30996fd9":"code","884ad2ab":"code","33fdf431":"code","cc4c9bbc":"code","0283b386":"code","6d860b07":"markdown"},"source":{"23ef197c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nprint(os.listdir(\"..\/input\/siim-acr-pneumothorax-segmentation\/sample images\"))\nprint(os.listdir(\"..\/input\/example-test-images\"))\n\n# Any results you write to the current directory are saved as output.","9e66e822":"#----------------------------------------------------#\n#  Created by Shunxing Bao.                          #\n#  https:\/\/www.kaggle.com\/onealbao                   #\n#----------------------------------------------------#\n\n# define DICE coefficient metric calculation \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef dice_loss(input, target):\n    \"\"\"\n    input is a torch variable of size BatchxnclassesxHxW representing log probabilities for each class\n    target is a 1-hot representation of the groundtruth, shoud have same size as the input\n    \"\"\"\n    return 0","7201a65e":"#----------------------------------------------------#\n#  Created by Shunxing Bao.                          #\n#  https:\/\/www.kaggle.com\/onealbao                   #\n#----------------------------------------------------#\n\n# customized Data loader\n# We recommend users firstly convert DICOM to JPEG, then load JPEG images in dataloader\nfrom torch.utils.data.dataset import Dataset\nimport torch\nimport numpy as np\nimport os\nimport nibabel as nib\n\nclass customDataLoader(Dataset):\n    def __init__(self, img_list, seg_list, state, num_classes): #READ DATA\n        self.img_list = img_list\n        self.seg_list = seg_list\n    \n    def __getitem__(self, index): # RETURN ONE ITEM ON THE INDEX\n        return 0\n    \n    def __len__(self): # RETURN THE DATA LENGTH\n        return len(self.img_list)\n        \n        ","30996fd9":"#----------------------------------------------------#\n#  Created by Shunxing Bao.                          #\n#  contact: shunxing.bao@vanderbilt.edu              #\n#----------------------------------------------------#\n\n# a dummy Unet\nimport torch\nimport torch.nn as nn\n\nclass dummyUNet(nn.Module):\n    def __init__(self, in_channel, n_classes):\n        self.in_channel = in_channel\n        self.n_classes = n_classes\n        super(dummyUNet, self).__init__()\n        \n    def encoder(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1,\n                bias=True, batchnorm=False):\n        return None\n\n    def decoder(self, in_channels, out_channels, kernel_size, stride=1, padding=0,\n                output_padding=0, bias=True):\n        return None\n\n    def forward(self, x):\n        return x","884ad2ab":"#----------------------------------------------------#\n#  Created by Shunxing Bao.                          #\n#  https:\/\/www.kaggle.com\/onealbao                   #\n#----------------------------------------------------#\n\nimport os\nimport time\nimport datetime\nimport numpy as np\nimport random\nimport torch\nimport torch.optim\nimport torch.utils.data\nimport torch.backends.cudnn\nimport nibabel as nib\n\n\ndef train(epoch, num_epochs, net, train_loader):\n    mean_val_loss = []\n    return mean_val_loss\n    # End of train function\n\ndef main():\n    print('::......STARTING SIIM SEGMENTATION ON MMWHS DATASET......::\\n\\n')\n\n    # parameters setting\n    num_epochs = 350\n    last_epoch = 0\n    in_channels = 1 # data per pixel\n    n_classes = 1 # need to further set correctly ...\n\n########################################################################################################################\n    # DATA LOADING\n########################################################################################################################\n\n    print('Building Dataset....')\n\n    # Define input training data and labels directories\n\n    # TRAINING IMAGE PATHS\n    train_data_path = os.path.normpath('..\/input\/siim-acr-pneumothorax-segmentation\/sample images\/')\n    # Label CSV Data Path\n    train_labels_path = os.path.normpath('..\/input\/siim-acr-pneumothorax-segmentation\/sample images\/train-rle-sample.csv')\n\n    # Prepare training dataset for dataloading\n    train_dataloader_input = customDataLoader(train_data_path, train_labels_path, 1, n_classes)\n\n    # Training Data loader (Using built in Torch class Dataset, refer to customDataLoader)\n    train_loader = torch.utils.data.DataLoader(dataset=train_dataloader_input,\n                                               batch_size=1,\n                                               shuffle=True,\n                                               pin_memory=True,\n                                               num_workers=4) #\n\n    print('....Dataset built!')\n\n##############################################################s##########################################################\n    # SETUP NEURAL NETWORK, LOSS FUNCTION\n########################################################################################################################\n    print('Initializing model...')\n\n    # Automatically choose GPU when available, otherwise CPU\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    # Load model to device..\n    net = dummyUNet(in_channels, n_classes)\n    net.to(device)\n\n    # Define loss criterion..\n    # loss_criterion = DiceLoss()\n\n########################################################################################################################\n    # PERFORM NETWORK TRAINING AND SAVE LOSS DATA\n########################################################################################################################\n\n    print('\\n===============================TRAINING BEGINS===============================')\n    \n    # Parse through epochs\n    for epoch in range(last_epoch, num_epochs):\n        train(epoch, num_epochs, net, train_loader)\n    \n    print('\\n===============================TRAINING COMPLETE=============================')\n\nif __name__ == '__main__':\n    main()","33fdf431":"#----------------------------------------------------#\n#  Created by Shunxing Bao.                          #\n#  https:\/\/www.kaggle.com\/onealbao                   #\n#----------------------------------------------------#\n\nimport os\nimport numpy as np\nimport torch\nimport torch.optim\nimport torch.utils.data\nimport torch.backends.cudnn\nfrom shutil import copyfile\n\ndef test():\n\n    print('====================TESTING SIIM SEGMENTATION====================')\n    print('Initializing Neural Network........................')\n    # Automatically choose GPU when available, otherwise CPU\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    #device = torch.device(\"cpu\")\n    print('Device in use: ', device)\n\n    in_channels = 1  # data per pixel\n    n_classes = 1  # need to further set correctly\n\n    # Define network\n    net = dummyUNet(in_channels, n_classes)\n\n    # change processing device to cpu\n    net.eval()\n\n    # Load old weights\n    weights_path = '\/PATH_TO_FINAL_WEIGHT.pth'\n    #net.load_state_dict(torch.load(weights_path, map_location=device))\n    print('....dummyUnet Network loaded with weights')\n\n    # Build dataset for loading into network!!\n    print('Building testing dataset......')\n    #test_data_path = os.path.normpath('.\/sanbdox\/Testing\/img\/')\n    test_data_path = '..\/input\/example-test-images'\n    # Prepare validation dataset for dataloading\n    test_dataloader_input = customDataLoader(test_data_path, \"\", 0, n_classes)\n\n    # Testing Data loader (Using built in Torch class Dataset, refer to dataset.py)\n    test_loader = torch.utils.data.DataLoader(dataset=test_dataloader_input,\n                                              batch_size=1,\n                                              shuffle=True,\n                                              pin_memory=True,\n                                              num_workers=4)  \n    print('...............Dataset Built!')\n    print('===============================TESTING BEGINS===============================')\n    \n    mkdir('result')\n    copyfile('..\/input\/siim-acr-pneumothorax-segmentation\/sample_submission.csv', '.\/result\/sample_result.csv')\n    \n    print('===============================TESTING COMPLETE===============================')\n    print('segmentation result is saved into .\/result\/sample_result.csv ')\n    \n    \ndef mkdir(path):\n\tif not os.path.exists(path):\n\t\tos.makedirs(path)\n\nif __name__ == '__main__':\n    test()","cc4c9bbc":"ls","0283b386":"ls -l result\/","6d860b07":"### Important Note\nThis is just a reference model for the SIIM Pneumothorax Segmentation competition. The directories referenced in the code below are pointing to a **very** small sample of the training and test sets. Therefore, this kernel, as is, will not yield a good result. This is intended to be a framework, so you should expect to have to replace those directories with the actual train & test sets, once you've retrieved them [using the tutorial specified](https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/siim\/SIIM%20Cloud%20Healthcare%20API%20Documentation.pdf)."}}