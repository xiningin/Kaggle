{"cell_type":{"8f155471":"code","f65285e0":"code","98ff8c58":"code","b1900075":"code","156f92d2":"code","75ac7a76":"code","361eb0be":"code","20e56183":"code","dde24abd":"code","d1afc9bb":"code","b3dce967":"code","a3597470":"code","d96a27a5":"code","6d2bffc9":"code","b9ea3939":"code","3cb300e2":"code","2a22899a":"code","cfd5c59e":"markdown","1a49ad3c":"markdown","cb1b6df8":"markdown","f672398b":"markdown","f97e50d1":"markdown","f9582c33":"markdown","7a855c1b":"markdown","9e346fa3":"markdown","4da62567":"markdown","03fd5565":"markdown"},"source":{"8f155471":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f65285e0":"data_raw = pd.read_csv('..\/input\/titanic\/train.csv')\ndata_val = pd.read_csv('..\/input\/titanic\/test.csv')\ndata1 = data_raw.copy(deep=True)\ndata_cleaner = [data1, data_val]\n\nTarget = ['Survived']","98ff8c58":"for dataset in data_cleaner:\n    print(dataset.info())\n    print(dataset.describe(include='all'))","b1900075":"for dataset in data_cleaner:\n    dataset.Age.fillna(dataset.Age.median(), inplace=True)\n    dataset.Embarked.fillna('S', inplace=True)\n    dataset.Fare.fillna(dataset.Fare.median(), inplace=True)\n    # dataset['Title'] = dataset.Name.str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n    dataset['Family_members'] = dataset.Parch + dataset.SibSp","156f92d2":"data1.sample(5)","75ac7a76":"data1.drop(['Name', 'PassengerId', 'Ticket', 'SibSp', 'Parch'], axis=1, inplace=True)","361eb0be":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfor i in data1.columns:\n    sns.countplot(data1[i])\n    plt.show()","20e56183":"# data1.groupby(data1['Cabin'].isnull()).mean()\ndata1.groupby(data1['Cabin'].isnull())['Survived'].mean()","dde24abd":"for dataset in data_cleaner:\n    dataset['Cabin_Allotted'] = np.where(dataset.Cabin.isnull(), 0, 1)\n    dataset.drop('Cabin', axis=1, inplace=True)","d1afc9bb":"data1.sample(5)\n# data1['Title'].value_counts()","b3dce967":"from sklearn.preprocessing import LabelEncoder\n\nlb = LabelEncoder()\n\nfor dataset in data_cleaner:\n    dataset['Sex_labeled'] = lb.fit_transform(dataset.Sex)\n    \n    dataset['AgeBin'] = pd.qcut(dataset.Age, 3)\n    dataset['Age_labeled'] = lb.fit_transform(dataset['AgeBin'])\n\n    dataset['FareBin'] = pd.qcut(dataset.Fare, 4)\n    dataset['Fare_labeled'] = lb.fit_transform(dataset['FareBin'])\n\n    dataset['Embarked_labeled'] = lb.fit_transform(dataset.Embarked)","a3597470":"data1.sample(5)","d96a27a5":"print(data1['Age_labeled'].value_counts())\nprint(data1['Fare_labeled'].value_counts())","6d2bffc9":"data1_X = [\n    'Pclass', \n    'Family_members', \n    'Cabin_Allotted', \n    'Sex_labeled', \n    'Age_labeled', \n    'Fare_labeled', \n    'Embarked_labeled'\n]\n\nfor i in data1[data1_X].columns:\n    sns.lineplot(i, 'Survived', data=data1)\n    plt.show()","b9ea3939":"data1[data1_X].info()","3cb300e2":"from sklearn import ensemble, tree, neighbors\nfrom xgboost import XGBClassifier\nfrom sklearn import model_selection\n\nMLA = [\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    neighbors.KNeighborsClassifier(), \n\n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(), \n\n    XGBClassifier(objective='binary:logistic', eval_metric='logloss')    \n]\n\ncv_split = model_selection.ShuffleSplit(n_splits=10, test_size=.2, train_size=.8, random_state=1)\n\nMLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD', 'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\nrow_index = 0\nfor alg in MLA:\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n\n    cv_results = model_selection.cross_validate(alg, data1[data1_X], data1[Target].values.reshape(-1,), cv=cv_split, return_train_score=True)\n\n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean() \n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3\n\n    row_index += 1\n\n# MLA_compare.sort_values(by=['MLA Test Accuracy Mean'], ascending=False, inplace=True)\nMLA_compare","2a22899a":"# after trying several times submissions, I chose RandomForestClassifier as my highest score\nmodel = ensemble.RandomForestClassifier(**{'criterion': 'entropy', 'max_depth': 5, 'n_estimators': 50, 'random_state': 1})\nmodel.fit(data1[data1_X], data1[Target].values.reshape(-1, ))\npredictions = model.predict(data_val[data1_X])\n\noutput = pd.DataFrame({'PassengerId': data_val.PassengerId, 'Survived': predictions})\noutput.to_csv('.\/my_submission_RandomForestClassifier_tunned_F4.csv', index=False)\nprint(\"Your submission was successfully saved!\")","cfd5c59e":"# Data Preprocessing- Encoder","1a49ad3c":"# Data preprocessing- Cabin","cb1b6df8":"# References\n* [A Data Science Framework: To Achieve 99% Accuracy](https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy)\n* [MY FIRST KAGGLE WORK TITANIC](https:\/\/www.kaggle.com\/saptarshisit\/my-first-kaggle-work-titanic)","f672398b":"# Simple Data Visualization (for setting bins)","f97e50d1":"# Submit","f9582c33":"# Next: Hyperparameters\n\nWhat is the importance of hyperparameter tuning? <br>\nHyperparameters are crucial as they control the overall behaviour of a machine learning model. The ultimate goal is to find an optimal combination of hyperparameters that minimizes a predefined loss function to give better results. To improve your ML skills, see [Titanic Top 11%| Starter II: Hyperparameter Tuning](https:\/\/www.kaggle.com\/chienhsianghung\/titanic-top-11-starter-ii-hyperparameter-tuning).","7a855c1b":"# Data Preprocessing- Fill Na","9e346fa3":"# Data Preprocessing","4da62567":"# Define the Problem\n\nFor this project, the problem statement is given to us on a golden plater, develop an algorithm to predict the survival outcome of passengers on the Titanic.\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy. <br>\n- Tools\n    - It's a classic **Binary classification**. \n- Data\n    - The dataset is given to us on a golden plater with test and train data at Kaggle's Titanic: Machine Learning from Disaster","03fd5565":"# Models Selection\n\nAn easy basic way to compare different models within the same dataset."}}