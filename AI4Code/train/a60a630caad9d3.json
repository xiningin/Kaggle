{"cell_type":{"d0ae5ef9":"code","b5d38d18":"code","d5444a60":"code","48069e3e":"code","e68e030f":"code","cd6064c4":"code","445aa57d":"code","762c846a":"code","bb3c1f29":"code","3a4761a1":"code","5e7b47d5":"code","8dc7418a":"code","ce42ba38":"code","9f598d2f":"markdown","3754d1b3":"markdown","f8ace4f4":"markdown","83cc713b":"markdown","43994872":"markdown"},"source":{"d0ae5ef9":"import mxnet as mx\nimport pandas as pd\nimport numpy as np\nimport logging\nfrom sklearn.model_selection import train_test_split\n\nlogging.getLogger().setLevel(logging.DEBUG)  # logging to stdout\npath = '..\/input\/'","b5d38d18":"# Fix the seed\nmx.random.seed(7)\n\n# Set the compute context, GPU is available otherwise CPU\nctx = mx.gpu() if mx.test_utils.list_gpus() else mx.cpu()","d5444a60":"print(\"model extract train init\")\ndf_train = pd.read_csv(path + 'train.csv')\ny = (np.array(df_train['label'].values.tolist()).astype(np.int)).copy()\ndf_train = df_train.drop(columns=['label'])\nX = (np.array(df_train.values.tolist()).astype(np.float)).copy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nprint(\"model extract train end\")","48069e3e":"print(\"model extract test init\")\ndf_test_final = pd.read_csv(path + 'test.csv')\nX_test_final = np.array(df_test_final.values.tolist()).astype(np.float)\nprint(\"model extract train end\")","e68e030f":"def reshare_array(array, dim):\n    return np.reshape(array, (-1, 1, dim, dim))","cd6064c4":"X_train = reshare_array(X_train, 28)\nX_test = reshare_array(X_test, 28)\nX_test_final = reshare_array(X_test_final, 28)\ny_train = np.array(y_train)\ny_test = np.array(y_test)","445aa57d":"batch_size = 100\ntrain_iter = mx.io.NDArrayIter(X_train, y_train, batch_size)\nval_iter = mx.io.NDArrayIter(X_test, y_test, batch_size)","762c846a":"data = mx.sym.var('data')\n# first conv layer\nconv1 = mx.sym.Convolution(data=data, kernel=(5,5), num_filter=20)\ntanh1 = mx.sym.Activation(data=conv1, act_type=\"tanh\")\npool1 = mx.sym.Pooling(data=tanh1, pool_type=\"max\", kernel=(2,2), stride=(2,2))\n# second conv layer\nconv2 = mx.sym.Convolution(data=pool1, kernel=(5,5), num_filter=50)\ntanh2 = mx.sym.Activation(data=conv2, act_type=\"tanh\")\npool2 = mx.sym.Pooling(data=tanh2, pool_type=\"max\", kernel=(2,2), stride=(2,2))\n# first fullc layer\nflatten = mx.sym.flatten(data=pool2)\nfc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=500)\ntanh3 = mx.sym.Activation(data=fc1, act_type=\"tanh\")\n# second fullc\nfc2 = mx.sym.FullyConnected(data=tanh3, num_hidden=10)\n# softmax loss\nlenet = mx.sym.SoftmaxOutput(data=fc2, name='softmax')","bb3c1f29":"lenet_model = mx.mod.Module(symbol=lenet, context=ctx)","3a4761a1":"# train with the same\nlenet_model.fit(train_iter,\n                eval_data=val_iter,\n                optimizer='sgd',\n                optimizer_params={'learning_rate':0.005},\n                eval_metric='acc',\n                batch_end_callback = mx.callback.Speedometer(batch_size, 100),\n                num_epoch=150)","5e7b47d5":"test_iter = mx.io.NDArrayIter(X_test_final, None, batch_size)\nprob = lenet_model.predict(test_iter)\n\ny = []\nfor p in list(prob):\n   y.append(list(p).index(np.max(p)))","8dc7418a":"test_iter = mx.io.NDArrayIter(X_test, y_test, batch_size)\n# predict accuracy of mlp\nacc = mx.metric.Accuracy()\nlenet_model.score(test_iter, acc)\nprint(acc)\nassert acc.get()[1] > 0.96, \"Achieved accuracy (%f) is lower than expected (0.96)\" % acc.get()[1]","ce42ba38":"df = pd.DataFrame({'ImageId': [x for x in range(1, len(y) + 1)], 'Label': y})\ndf.to_csv('submission.csv', index=False)","9f598d2f":"If all went well, we should see a higher accuracy metric for predictions made using LeNet. With CNN we should be able to correctly predict around 98% of all test images.\n\n## Summary\n\nIn this tutorial, we have learned how to use MXNet, based on [MXNet Examples](https:\/\/mxnet.incubator.apache.org\/versions\/master\/tutorials\/python\/mnist.html).","3754d1b3":"# Handwritten Digit Recognition\n\nIn this tutorial, we'll give you a step by step walk-through of how to build a hand-written digit classifier using the [MNIST](https:\/\/en.wikipedia.org\/wiki\/MNIST_database) dataset. For someone new to deep learning, this exercise is arguably the \"Hello World\" equivalent.\n\nMNIST is a widely used dataset for the hand-written digit classification task. It consists of 70,000 labeled 28x28 pixel grayscale images of hand-written digits. The dataset is split into 60,000 training images and 10,000 test images. There are 10 classes (one for each of the 10 digits). The task at hand is to train a model using the 60,000 training images and subsequently test its classification accuracy on the 10,000 test images.\n\n![png](https:\/\/raw.githubusercontent.com\/dmlc\/web-data\/master\/mxnet\/example\/mnist.png)\n\n**Figure 1:** Sample images from the MNIST dataset.","f8ace4f4":"### Prediction\n\nFinally, we'll use the trained LeNet model to generate predictions for the test data.","83cc713b":"![png](https:\/\/raw.githubusercontent.com\/dmlc\/web-data\/master\/mxnet\/image\/conv_mnist.png)\n\n**Figure 3:** First conv + pooling layer in LeNet.\n\nNow we train LeNet with the same hyper-parameters as before. Note that, if a GPU is available, we recommend using it. This greatly speeds up computation given that LeNet is more complex and compute-intensive than the previous multilayer perceptron. To do so, we only need to change `mx.cpu()` to `mx.gpu()` and MXNet takes care of the rest. Just like before, we'll stop training after 10 epochs.","43994872":"### Convolutional Neural Network\n\nEarlier, we briefly touched on a drawback of MLP when we said we need to discard the input image's original shape and flatten it as a vector before we can feed it as input to the MLP's first fully connected layer. Turns out this is an important issue because we don't take advantage of the fact that pixels in the image have natural spatial correlation along the horizontal and vertical axes. A convolutional neural network (CNN) aims to address this problem by using a more structured weight representation. Instead of flattening the image and doing a simple matrix-matrix multiplication, it employs one or more convolutional layers that each performs a 2-D convolution on the input image.\n\nA single convolution layer consists of one or more filters that each play the role of a feature detector. During training, a CNN learns appropriate representations (parameters) for these filters. Similar to MLP, the output from the convolutional layer is transformed by applying a non-linearity. Besides the convolutional layer, another key aspect of a CNN is the pooling layer. A pooling layer serves to make the CNN translation invariant: a digit remains the same even when it is shifted left\/right\/up\/down by a few pixels. A pooling layer reduces a *n x m* patch into a single value to make the network less sensitive to the spatial location. Pooling layer is always included after each conv (+ activation) layer in the CNN.\n\nThe following source code defines a convolutional neural network architecture called LeNet. LeNet is a popular network known to work well on digit classification tasks. We will use a slightly different version from the original LeNet implementation, replacing the sigmoid activations with tanh activations for the neurons"}}