{"cell_type":{"ba81249a":"code","e65982a7":"code","faaf53c3":"code","3bd84b16":"code","8d1d52d0":"code","f8f78b1c":"code","083dc1c9":"code","ce9a6b90":"code","968e8d79":"code","656ffb61":"code","437d22f2":"code","c4500d90":"code","7e197fbe":"code","7ace3f39":"code","46cd34c6":"code","284aee44":"code","845c5cd3":"code","0718b2df":"code","aadfaefa":"code","b834d53e":"markdown","63bf9e32":"markdown","c54420d4":"markdown","4611462c":"markdown","851e939f":"markdown","119d35b9":"markdown","9bfbdb00":"markdown","c5a31fa1":"markdown"},"source":{"ba81249a":"import warnings\nwarnings.filterwarnings('ignore')\nimport logging\nimport os\nimport re\nimport collections\nimport copy\nimport datetime\nimport random\nimport traceback\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport scipy\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\n\nimport tqdm\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch import nn\nfrom torch.nn import functional as F\n\nnltk.download('stopwords')\nlemmatizer = WordNetLemmatizer()\nstop_words_en = set(stopwords.words('english'))\nstemmer_en = SnowballStemmer('english')","e65982a7":"dir_data = '..\/input\/nlp-getting-started\/'\nfile_test = 'test.csv'\nfile_name = 'train.csv'\nsubmission_name = 'submission.csv'\nvalid_size = .3\nrandom_seed = 123\n\nrandom.seed(random_seed)\nnp.random.seed(random_seed)\ntorch.manual_seed(random_seed)\ntorch.cuda.manual_seed(random_seed)\ntorch.backends.cudnn.deterministic = True","faaf53c3":"df = pd.read_csv(os.path.join(dir_data, file_name))\ndf_test = pd.read_csv(os.path.join(dir_data, file_test))\ndf.shape, df_test.shape","3bd84b16":"df.target.value_counts()","8d1d52d0":"x_train, x_valid, y_train, y_valid = train_test_split(\n    df, \n    df['target'],\n    test_size=valid_size,\n    random_state=random_seed\n)\nx_train.shape, x_valid.shape, y_train.shape, y_valid.shape","f8f78b1c":"config = {\n    'TextPreprocessor': {\n        'del_orig_col': False,\n        'mode_stemming': True,\n        'mode_norm': True,\n        'mode_remove_stops': True,\n        'mode_drop_long_words': True,\n        'mode_drop_short_words': True,\n        'min_len_word': 3,\n        'max_len_word': 15,\n        'max_size_vocab': 2000,\n        'max_doc_freq': 0.5,\n        'min_count': 5,\n        'pad_word': None,\n\n        \n    },\n    'VectorizeTexts': {\n        'mode_bin': False,\n        'mode_idf': False,\n        'mode_idf': False,\n        'mode_tfidf': True,\n        'mode_scale': True,\n    },\n    'TrainModel': {\n        'lr': .01,\n        'step_size_scheduler': 10,\n        'gamma_scheduler': 0.9,\n        'early_stopping_patience': 40,\n        'batch_size': 64,\n        'epoch_n': 500,\n        'criterion': F.cross_entropy\n    },\n    'Predict': {\n        'batch_size': 1,\n    }\n}","083dc1c9":"class TextPreprocessor(object):\n    def __init__(self, config):\n        \"\"\"Preparing text features.\"\"\"\n\n        self._del_orig_col = config.get('del_orig_col', True)\n        self._mode_stemming = config.get('mode_stemming', True)\n        self._mode_norm = config.get('mode_norm', True)\n        self._mode_remove_stops = config.get('mode_remove_stops', True)\n        self._mode_drop_long_words = config.get('mode_drop_long_words', True)\n        self._mode_drop_short_words = config.get('mode_drop_short_words', True)\n        self._min_len_word = config.get('min_len_word', 3)\n        self._max_len_word = config.get('max_len_word', 17)\n        self._max_size_vocab = config.get('max_size_vocab', 100000)\n        self._max_doc_freq = config.get('max_doc_freq', 0.8) \n        self._min_count = config.get('min_count', 5)\n        self._pad_word = config.get('pad_word', None)\n\n    def _clean_text(self, input_text):\n        \"\"\"Delete special symbols.\"\"\"\n\n        input_text = input_text.str.lower()\n        input_text = input_text.str.replace(r'[^a-z ]+', ' ')\n        input_text = input_text.str.replace(r' +', ' ')\n        input_text = input_text.str.replace(r'^ ', '')\n        input_text = input_text.str.replace(r' $', '')\n\n        return input_text\n\n\n    def _text_normalization_en(self, input_text):\n        '''Normalization of english text'''\n\n        return ' '.join([lemmatizer.lemmatize(item) for item in input_text.split(' ')])\n\n\n    def _remove_stops_en(self, input_text):\n        '''Delete english stop-words'''\n\n        return ' '.join([w for w in input_text.split() if not w in stop_words_en])\n\n\n    def _stemming_en(self, input_text):\n        '''Stemming of english text'''\n\n        return ' '.join([stemmer_en.stem(item) for item in input_text.split(' ')])\n\n\n    def _drop_long_words(self, input_text):\n        \"\"\"Delete long words\"\"\"\n        return ' '.join([item for item in input_text.split(' ') if len(item) < self._max_len_word])\n\n\n    def _drop_short_words(self, input_text):\n        \"\"\"Delete short words\"\"\"\n\n        return ' '.join([item for item in input_text.split(' ') if len(item) > self._min_len_word])\n    \n    \n    def _build_vocabulary(self, tokenized_texts):\n        \"\"\"Build vocabulary\"\"\"\n        \n        word_counts = collections.defaultdict(int)\n        doc_n = 0\n\n        for txt in tokenized_texts:\n            doc_n += 1\n            unique_text_tokens = set(txt)\n            for token in unique_text_tokens:\n                word_counts[token] += 1\n                \n        word_counts = {word: cnt for word, cnt in word_counts.items()\n                       if cnt >= self._min_count and cnt \/ doc_n <= self._max_doc_freq}\n        \n        sorted_word_counts = sorted(word_counts.items(),\n                                    reverse=True,\n                                    key=lambda pair: pair[1])\n        \n        if self._pad_word is not None:\n            sorted_word_counts = [(pad_word, 0)] + sorted_word_counts\n            \n        if len(word_counts) > self._max_size_vocab:\n            sorted_word_counts = sorted_word_counts[:self._max_size_vocab]\n            \n        word2id = {word: i for i, (word, _) in enumerate(sorted_word_counts)}\n        word2freq = np.array([cnt \/ doc_n for _, cnt in sorted_word_counts], dtype='float32')\n\n        return word2id, word2freq\n\n    def transform(self, df):        \n        \n        columns_names = df.select_dtypes(include='object').columns\n        df[columns_names] = df[columns_names].astype('str')\n        \n        for i in df.index:\n            df.loc[i, 'union_text'] = ' '.join(df.loc[i, columns_names])\n            \n        if self._del_orig_col:\n            df = df.drop(columns_names, 1)\n            \n        df['union_text'] = self._clean_text(df['union_text'])\n        \n        if self._mode_norm:\n            df['union_text'] = df['union_text'].apply(self._text_normalization_en, 1)\n            \n        if self._mode_remove_stops:\n            df['union_text'] = df['union_text'].apply(self._remove_stops_en, 1)\n            \n        if self._mode_stemming:\n            df['union_text'] = df['union_text'].apply(self._stemming_en)\n            \n        if self._mode_drop_long_words:\n            df['union_text'] = df['union_text'].apply(self._drop_long_words, 1)\n            \n        if self._mode_drop_short_words:\n            df['union_text'] = df['union_text'].apply(self._drop_short_words, 1)\n            \n        df.loc[(df.union_text == ''), ('union_text')] = 'EMPT'\n        \n        tokenized_texts = [[word for word in text.split(' ')] for text in df.union_text]\n        word2id, word2freq = self._build_vocabulary(tokenized_texts)\n\n        return tokenized_texts, word2id, word2freq","ce9a6b90":"class VectorizeTexts(object):\n    def __init__(self, config):\n        \"\"\"Preparing text features.\"\"\"\n        \n        self._mode_bin = config.get('mode_bin', True)\n        self._mode_idf = config.get('mode_idf', True)\n        self._mode_tf = config.get('mode_tf', True)\n        self._mode_tfidf = config.get('mode_tfidf', True)\n        self._mode_scale = config.get('mode_scale', True)\n        \n    def _get_bin(self, result):\n        \"\"\"Get binary vectors\"\"\"\n        \n        result = (result > 0).astype('float32')\n        \n        return result\n    \n    def _get_tf(self, result):\n        \"\"\"Get term frequency.\"\"\"\n        \n        result = result.tocsr()\n        result = result.multiply(1 \/ result.sum(1))\n        \n        return result\n    \n    def _get_idf(self, result):\n        \"\"\"Get inverse document frequency.\"\"\"\n        \n        result = (result > 0).astype('float32').multiply(1 \/ word2freq)\n        \n        return result\n    \n    def _get_tfidf(self, result):\n        \"\"\"Get term frequency and inverse document frequency.\"\"\"\n        \n        result = result.tocsr()\n        result = result.multiply(1 \/ result.sum(1)) \n        result = result.multiply(1 \/ word2freq) \n        \n        return result \n    \n    def _get_scale(self, result):\n        \"\"\"Standardize Tfidf dataset.\"\"\"\n        \n        result = result.tocsc()\n        result -= result.min()\n        result \/= (result.max() + 1e-6)\n        \n        return result\n    \n    def transform(self, tokenized_texts, word2id, word2freq):\n        \n        result = scipy.sparse.dok_matrix((len(tokenized_texts), len(word2id)), dtype='float32')\n        for text_i, text in enumerate(tokenized_texts):\n            for token in text:\n                if token in word2id:\n                    result[text_i, word2id[token]] += 1\n        \n        if self._mode_bin:\n            result = self._get_bin(result)\n        \n        if self._mode_idf:\n            result = self._get_idf(result)\n            \n        if self._mode_tf:\n            result = self._get_tf(result)\n            \n        if self._mode_tfidf:\n            result = self._get_tfidf(result)\n            \n        if self._mode_scale:\n            result = self._get_scale(result)\n            \n        return result.tocsr()","968e8d79":"class SparseFitDataset(Dataset):\n    def __init__(self, features, targets):\n        '''Dataset for train. Return features and labels.'''\n        \n        self.features = features\n        self.targets = targets\n\n    def __len__(self):\n        return self.features.shape[0]\n\n    def __getitem__(self, idx):\n        cur_features = torch.from_numpy(self.features[idx].toarray()[0]).float()\n        cur_label = torch.from_numpy(np.asarray(self.targets[idx])).long()\n        return cur_features, cur_label\n    \nclass SparsePredDataset(Dataset):\n    def __init__(self, features):\n        '''Dataset for predict. Return features only.'''\n        \n        self.features = features\n\n    def __len__(self):\n        return self.features.shape[0]\n\n    def __getitem__(self, idx):\n        cur_features = torch.from_numpy(self.features[idx].toarray()[0]).float()\n        return cur_features","656ffb61":"class TrainModel(object):\n    def __init__(self, config):\n        \"\"\"Train a model.\"\"\"\n        \n        self._lr = config.get('lr', 1.0e-3)\n        self._step_size_scheduler = config.get('step_size_scheduler', 10)\n        self._gamma_scheduler = config.get('gamma_scheduler', 0.1)\n        self._early_stopping_patience = config.get('early_stopping_patience', 10)\n        self._batch_size = config.get('batch_size', 32)\n        self._epoch_n = config.get('epoch_n', 100)\n        self._criterion = config.get('criterion', F.cross_entropy)\n\n    def train(self, model, train_dataset, val_dataset):\n\n        optimizer = torch.optim.Adam(model.parameters(), lr=self._lr)\n        lr_scheduler = StepLR(optimizer, step_size=self._step_size_scheduler, gamma=self._gamma_scheduler)\n        train_dataloader = DataLoader(train_dataset, batch_size=self._batch_size, shuffle=True)\n        val_dataloader = DataLoader(val_dataset, batch_size=self._batch_size, shuffle=False)\n        \n        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        model.to(device)\n\n        best_val_loss = float('inf')\n        best_epoch_i = 0\n        best_model = copy.deepcopy(model)\n\n        for epoch_i in range(self._epoch_n):\n            try:\n                epoch_start = datetime.datetime.now()\n                print('Epoch {}\/{}'.format(epoch_i, self._epoch_n))\n                print('lr_value: {}'.format(lr_scheduler.get_lr()[0]), flush=True)\n                lr_scheduler.step()\n                model.train()\n                mean_train_loss = 0\n                mean_train_acc = 0\n                train_batches_n = 0\n                \n                for batch_x, batch_y in train_dataloader:\n                    \n                    batch_x = batch_x.to(device)\n                    batch_y = batch_y.to(device)\n                    \n                    optimizer.zero_grad()\n                    \n                    pred = model(batch_x)\n                    \n                    loss = self._criterion(pred, batch_y)\n                    acc = accuracy_score(pred.detach().cpu().numpy().argmax(-1), \n                                         batch_y.cpu().numpy())\n                    loss.backward()\n                    optimizer.step()\n                    \n                    mean_train_loss += float(loss)\n                    mean_train_acc += float(acc)\n                    train_batches_n += 1\n\n                mean_train_loss \/= train_batches_n\n                mean_train_acc \/= train_batches_n\n                print('{:0.2f} s'.format((datetime.datetime.now() - epoch_start).total_seconds()))\n                print('Train Loss', mean_train_loss)\n                print('Train Acc', mean_train_acc)\n\n                model.eval()\n                mean_val_loss = 0\n                mean_val_acc = 0\n                val_batches_n = 0\n\n                with torch.no_grad():\n                    for batch_x, batch_y in val_dataloader:\n\n                        batch_x = batch_x.to(device)\n                        batch_y = batch_y.to(device)\n\n                        pred = model(batch_x)\n                        loss = self._criterion(pred, batch_y)\n                        acc = accuracy_score(pred.detach().cpu().numpy().argmax(-1), \n                                             batch_y.cpu().numpy())\n\n                        mean_val_loss += float(loss)\n                        mean_val_acc += float(acc)\n                        val_batches_n += 1\n\n                mean_val_loss \/= val_batches_n\n                mean_val_acc \/= val_batches_n\n                print('Val Loss', mean_val_loss)\n                print('Val Acc', mean_val_acc)\n\n                if mean_val_loss < best_val_loss:\n                    best_epoch_i = epoch_i\n                    best_val_loss = mean_val_loss\n                    best_model = copy.deepcopy(model)\n                    print('New best model!')\n                elif epoch_i - best_epoch_i > self._early_stopping_patience:\n                    print('The model has not improved over the last {} epochs, stop learning.'.format(\n                        self._early_stopping_patience))\n                    break\n                print()\n            except KeyboardInterrupt:\n                print('Stopped by user.')\n                break\n            except Exception as ex:\n                print('Error: {}\\n{}'.format(ex, traceback.format_exc()))\n                break\n\n        return best_val_loss, best_model","437d22f2":"class Predict(object):\n    def __init__(self, config):\n        \"\"\"Predict with trained model\"\"\"\n        \n        self._batch_size = config.get('batch_size', 32)\n\n    def predict(self, model, dataset):\n        \n        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        results_by_batch = []\n        model.to(device)\n        model.eval()\n\n        dataloader = DataLoader(\n            dataset, \n            batch_size=self._batch_size, \n            shuffle=False\n        )\n        labels = []\n        with torch.no_grad():\n            for batch_x in tqdm.tqdm(dataloader, total=len(dataset)\/self._batch_size):\n                batch_x = batch_x.to(device)\n                batch_pred = model(batch_x)\n                results_by_batch.append(batch_pred.detach().cpu().numpy())\n\n        return np.concatenate(results_by_batch, 0)","c4500d90":"%%time\ntrain_tokens, word2id, word2freq = TextPreprocessor(config['TextPreprocessor']).transform(x_train)\nvalid_tokens, _, _ = TextPreprocessor(config['TextPreprocessor']).transform(x_valid)","7e197fbe":"%%time\ntrain_vectors = VectorizeTexts(config['VectorizeTexts']).transform(train_tokens, word2id, word2freq)\nvalid_vectors = VectorizeTexts(config['VectorizeTexts']).transform(valid_tokens, word2id, word2freq)","7ace3f39":"train_dataset = SparseFitDataset(train_vectors, y_train.tolist())\nval_dataset = SparseFitDataset(valid_vectors, y_valid.tolist())\nbest_val_loss, best_model = TrainModel(config['TrainModel']).train(\n    model=nn.Linear(len(word2id), len(set(y_train))),\n    train_dataset=train_dataset,\n    val_dataset=val_dataset \n)","46cd34c6":"%%time\ntest_tokens, _, _ = TextPreprocessor(config['TextPreprocessor']).transform(df_test)\ntest_vectors = VectorizeTexts(config['VectorizeTexts']).transform(test_tokens, word2id, word2freq)","284aee44":"test_pred = Predict(config['Predict']).predict(best_model, SparsePredDataset(test_vectors))","845c5cd3":"submission = pd.DataFrame({'id': df_test.id, 'target': test_pred.argmax(-1)})","0718b2df":"submission.target.value_counts()","aadfaefa":"submission.to_csv(submission_name, index=False)","b834d53e":"<a id='0'><\/a>\n\n+ [Classes](#Classes) \n+ [Pipline](#Pipline)\n  + [Train](#Train)\n  + [Predict](#Predict)","63bf9e32":"<a id='Train'><\/a>\n### Train","c54420d4":"# Custom Tfidf and LogisticRegression with PyTorch","4611462c":"<a id='Pipline'><\/a>\n## Pipline","851e939f":"<a id='Classes'><\/a>\n## Classes","119d35b9":"[Home](#0)","9bfbdb00":"[Home](#0)","c5a31fa1":"<a id='Predict'><\/a>\n### Predict"}}