{"cell_type":{"df9cf7db":"code","126db0f0":"code","b44ea14d":"code","2957d75b":"code","66a9a476":"code","ef04ddfa":"code","e15f0ea7":"code","48639df9":"code","ae0a5fe0":"code","157686d6":"code","3951a7a4":"code","2a5baad8":"code","48051a22":"code","6b231db7":"code","acaea0d5":"code","eab2b5a9":"code","d131bdda":"code","3e00df78":"code","cbe36f85":"code","cfd9ba5a":"code","fae2af0f":"code","3309121f":"code","43502168":"code","5705ea8b":"code","b66bb675":"code","8492718b":"code","e31a11be":"code","c7b3307a":"code","7be18aac":"code","f3bc86f9":"code","0930f695":"code","9ae4bf4e":"code","4e95a46a":"code","5d9155c9":"code","dece740f":"code","0f27771d":"code","8e0c1abc":"code","91cf4b4c":"code","7f6f25ca":"code","7e4c435e":"code","ff84e400":"code","8475433d":"code","8b949234":"code","635d6e2d":"code","688c0d44":"code","3c1b8dfa":"code","bdda8958":"code","e8d63580":"code","bfd98aef":"code","51057274":"code","ded01f88":"code","05c02905":"code","05f61476":"code","0b951cc3":"code","329211fa":"code","8fd782d2":"code","fed39141":"code","e98bae4d":"code","ddb3caf4":"code","6dcc2751":"code","3de0e38f":"code","c77e9cfa":"code","755a3e25":"code","4c594c9d":"code","0feeac95":"code","806884a6":"code","b7c76a3b":"code","8d244a3c":"code","f8f143ac":"code","2667c90f":"code","110ac351":"code","fdc0e48d":"code","d397e66f":"code","2cc55375":"code","38b996e6":"code","a1bf14d3":"code","fc09b1cf":"code","766e3f57":"code","47e0ee73":"code","40e1fc88":"code","c64084c1":"code","32d44026":"code","b04bbb19":"code","cda09ba2":"code","46025b87":"code","0ed4d667":"code","71f2462f":"code","d30dfe4d":"code","82edfcbd":"code","ab6eae37":"code","66e61333":"code","a49c920f":"code","eb6aa54e":"code","63e2e689":"code","8a105e9c":"code","ed20a91c":"code","26fb34eb":"code","5acd0520":"code","500992e9":"code","257a336d":"code","219ba535":"code","d4f2d7c4":"code","0787dc45":"code","f58542ac":"code","76874041":"code","c6af6f8e":"code","b64ac4e1":"code","b1c0b4a8":"markdown","5b7b41b4":"markdown","1a39d020":"markdown","f27fbd68":"markdown","eeb59199":"markdown","a23457ee":"markdown","e1a14f31":"markdown","fa67f1b1":"markdown","7a0ea364":"markdown","44a2ac9e":"markdown","9862d87d":"markdown"},"source":{"df9cf7db":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","126db0f0":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest =  pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","b44ea14d":"train.head(5)","2957d75b":"train.info()","66a9a476":"train.describe().T","ef04ddfa":"test.head(5)","e15f0ea7":"test.info()","48639df9":"test.describe().T","ae0a5fe0":"# Missing Value in test data set\n\ntrain.isnull().sum()","157686d6":"## As we can see Age and Cabin have missing values.","3951a7a4":"test.isnull().sum()","2a5baad8":"# Combining the train and test data\n\ncombinedata = [train,test]","48051a22":"combinedata","6b231db7":"train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","acaea0d5":"train[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","eab2b5a9":"train[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","d131bdda":"train[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","3e00df78":"# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","cbe36f85":"# Checking the corelation\n\ncorrelation = train.corr()\nplt.figure(figsize=(10, 6))\nsns.heatmap(correlation, annot=True, linewidths=0, vmin=-1, cmap=\"RdBu_r\")","cfd9ba5a":"correlation['Survived'].sort_values(ascending=False)","fae2af0f":"# Checking Outliers value\nfrom collections import Counter\ndef detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n# detect outliers from Age, SibSp , Parch and Fare\nOutliers_to_drop = detect_outliers(train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])","3309121f":"train.loc[Outliers_to_drop] # Show the outliers rows","43502168":"train = train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","5705ea8b":"train_len = len(train)\ncombineddataset =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)","b66bb675":"combineddataset.isnull().sum()","8492718b":"# Explore SibSp feature vs Survived\ng = sns.factorplot(x=\"SibSp\",y=\"Survived\",data=train,kind=\"bar\", size = 6 , \npalette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","e31a11be":"# Explore Parch feature vs Survived\ng  = sns.factorplot(x=\"Parch\",y=\"Survived\",data=train,kind=\"bar\", size = 6 , \npalette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","c7b3307a":"# Explore Age vs Survived\ng = sns.FacetGrid(train, col='Survived')\ng = g.map(sns.distplot, \"Age\")","7be18aac":"\n# Explore Age distibution \ng = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 0) & (train[\"Age\"].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 1) & (train[\"Age\"].notnull())], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel(\"Age\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","f3bc86f9":"# Explore skewness of Fare distribution \ncombineddataset[\"Fare\"] = combineddataset[\"Fare\"].fillna(combineddataset[\"Fare\"].median())\ng = sns.distplot(combineddataset[\"Fare\"], color=\"m\", label=\"Skewness : %.2f\"%(combineddataset[\"Fare\"].skew()))\ng = g.legend(loc=\"best\")","0930f695":"combineddataset[\"Fare\"] = combineddataset[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)","9ae4bf4e":"g = sns.distplot(combineddataset[\"Fare\"], color=\"m\", label=\"Skewness : %.2f\"%(combineddataset[\"Fare\"].skew()))\ng = g.legend(loc=\"best\")","4e95a46a":"g = sns.barplot(x=\"Sex\",y=\"Survived\",data=train)\ng = g.set_ylabel(\"Survival Probability\")","5d9155c9":"# Explore Pclass vs Survived\ng = sns.factorplot(x=\"Pclass\",y=\"Survived\",data=train,kind=\"bar\", size = 6 , \npalette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","dece740f":"# Explore Pclass vs Survived by Sex\ng = sns.factorplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=train,\n                   size=6, kind=\"bar\", palette=\"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","0f27771d":"combineddataset[\"Embarked\"].isnull().sum()","8e0c1abc":"combineddataset[\"Embarked\"] = combineddataset[\"Embarked\"].fillna(\"S\")","91cf4b4c":"g = sns.factorplot(x=\"Embarked\", y=\"Survived\",  data=train,\n                   size=6, kind=\"bar\", palette=\"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","7f6f25ca":"# Explore Pclass vs Embarked \ng = sns.factorplot(\"Pclass\", col=\"Embarked\",  data=train,\n                   size=6, kind=\"count\", palette=\"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"Count\")","7e4c435e":"# Explore Age vs Sex, Parch , Pclass and SibSP\ng = sns.factorplot(y=\"Age\",x=\"Sex\",data=combineddataset,kind=\"box\")\ng = sns.factorplot(y=\"Age\",x=\"Sex\",hue=\"Pclass\", data=combineddataset,kind=\"box\")\ng = sns.factorplot(y=\"Age\",x=\"Parch\", data=combineddataset,kind=\"box\")\ng = sns.factorplot(y=\"Age\",x=\"SibSp\", data=combineddataset,kind=\"box\")","ff84e400":"# convert Sex into categorical value 0 for male and 1 for female\ncombineddataset[\"Sex\"] = combineddataset[\"Sex\"].map({\"male\": 0, \"female\":1})","8475433d":"g = sns.heatmap(combineddataset[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(),cmap=\"BrBG\",annot=True)","8b949234":"# Filling missing value of Age \n\n## Fill Age with the median age of similar rows according to Pclass, Parch and SibSp\n# Index of NaN age rows\nindex_NaN_age = list(combineddataset[\"Age\"][combineddataset[\"Age\"].isnull()].index)\n\nfor i in index_NaN_age :\n    age_med = combineddataset[\"Age\"].median()\n    age_pred = combineddataset[\"Age\"][((combineddataset['SibSp'] == combineddataset.iloc[i][\"SibSp\"]) & (combineddataset['Parch'] == combineddataset.iloc[i][\"Parch\"]) & (combineddataset['Pclass'] == combineddataset.iloc[i][\"Pclass\"]))].median()\n    if not np.isnan(age_pred) :\n        combineddataset['Age'].iloc[i] = age_pred\n    else :\n        combineddataset['Age'].iloc[i] = age_med","635d6e2d":"combineddataset.isnull().sum()","688c0d44":"# Feature Enginnering\ncombineddataset[\"Name\"].head()","3c1b8dfa":"\n# Get Title from Name\ncombineddataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in combineddataset[\"Name\"]]\ncombineddataset[\"Title\"] = pd.Series(combineddataset_title)\ncombineddataset[\"Title\"].head()","bdda8958":"\ng = sns.countplot(x=\"Title\",data=combineddataset)\ng = plt.setp(g.get_xticklabels(), rotation=45)","e8d63580":"# Convert to categorical values Title \ncombineddataset[\"Title\"] = combineddataset[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ncombineddataset[\"Title\"] = combineddataset[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ncombineddataset[\"Title\"] = combineddataset[\"Title\"].astype(int)","bfd98aef":"g = sns.countplot(combineddataset[\"Title\"])\ng = g.set_xticklabels([\"Master\",\"Miss\/Ms\/Mme\/Mlle\/Mrs\",\"Mr\",\"Rare\"])","51057274":"g = sns.factorplot(x=\"Title\",y=\"Survived\",data=combineddataset,kind=\"bar\")\ng = g.set_xticklabels([\"Master\",\"Miss-Mrs\",\"Mr\",\"Rare\"])\ng = g.set_ylabels(\"survival probability\")","ded01f88":"# Drop Name variable\ncombineddataset.drop(labels = [\"Name\"], axis = 1, inplace = True)","05c02905":"# Create a family size descriptor from SibSp and Parch\ncombineddataset[\"Fsize\"] = combineddataset[\"SibSp\"] + combineddataset[\"Parch\"] + 1","05f61476":"g = sns.factorplot(x=\"Fsize\",y=\"Survived\",data = combineddataset)\ng = g.set_ylabels(\"Survival Probability\")","0b951cc3":"# Create new feature of family size\ncombineddataset['Single'] = combineddataset['Fsize'].map(lambda s: 1 if s == 1 else 0)\ncombineddataset['SmallF'] = combineddataset['Fsize'].map(lambda s: 1 if  s == 2  else 0)\ncombineddataset['MedF'] = combineddataset['Fsize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ncombineddataset['LargeF'] = combineddataset['Fsize'].map(lambda s: 1 if s >= 5 else 0)","329211fa":"g = sns.factorplot(x=\"Single\",y=\"Survived\",data=combineddataset,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\ng = sns.factorplot(x=\"SmallF\",y=\"Survived\",data=combineddataset,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\ng = sns.factorplot(x=\"MedF\",y=\"Survived\",data=combineddataset,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\ng = sns.factorplot(x=\"LargeF\",y=\"Survived\",data=combineddataset,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")","8fd782d2":"# convert to indicator values Title and Embarked \ncombineddataset = pd.get_dummies(combineddataset, columns = [\"Title\"])\ncombineddataset = pd.get_dummies(combineddataset, columns = [\"Embarked\"], prefix=\"Em\")","fed39141":"combineddataset.head()","e98bae4d":"combineddataset[\"Cabin\"].head()","ddb3caf4":"# Create categorical values for Pclass\ncombineddataset[\"Pclass\"] = combineddataset[\"Pclass\"].astype(\"category\")\ncombineddataset = pd.get_dummies(combineddataset, columns = [\"Pclass\"],prefix=\"Pc\")","6dcc2751":"# Drop useless variables \ncombineddataset.drop(labels = [\"PassengerId\",\"Cabin\"], axis = 1, inplace = True)","3de0e38f":"combineddataset.head()","c77e9cfa":"combineddataset.dtypes","755a3e25":"train.dtypes","4c594c9d":"train = pd.get_dummies(train, columns = [\"Embarked\"], prefix=\"Em\")","0feeac95":"\ntrain = train.drop(\"Ticket\", axis=1)","806884a6":"train.head()","b7c76a3b":"test = pd.get_dummies(test, columns = [\"Embarked\"], prefix=\"Em\")","8d244a3c":"test.head()","f8f143ac":"test = test.drop(\"Ticket\", axis=1)","2667c90f":"combineddataset = combineddataset.drop(\"Ticket\", axis=1)","110ac351":"combineddataset.head()","fdc0e48d":"## Separate train dataset and test dataset\n\ntrain = combineddataset[:train_len]\ntest1 = combineddataset[train_len:]\ntest1.drop(labels=[\"Survived\"],axis = 1,inplace=True)","d397e66f":"## Separate train features and label \n\ntrain[\"Survived\"] = train[\"Survived\"].astype(int)\n\nY_train = train[\"Survived\"]\n\nX_train = train.drop(labels = [\"Survived\"],axis = 1)","2cc55375":"X_train.head()","38b996e6":"test1.head()","a1bf14d3":"X_test1 = test1","fc09b1cf":"X_test1.head()","766e3f57":"logreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred_logreg = logreg.predict(X_test1)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","47e0ee73":"# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred_svc = svc.predict(X_test1)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc\n\n","40e1fc88":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred_knn = knn.predict(X_test1)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","c64084c1":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred_gaussian = gaussian.predict(X_test1)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","32d44026":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred_dt = decision_tree.predict(X_test1)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","b04bbb19":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nrandom_forest.score(X_train, Y_train)\nY_pred_rfc = random_forest.predict(X_test1)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","cda09ba2":"# Gradient boosting tunning\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nkfold = StratifiedKFold(n_splits=10)\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsGBC.fit(X_train,Y_train)\n\nGBC_best = gsGBC.best_estimator_\n\n# Best score\ngsGBC.best_score_","46025b87":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', \n              'Gradient Boosting', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, \n              gsGBC.best_score_, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","0ed4d667":"submission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": Y_pred_rfc\n    })\nsubmission.to_csv('..\/working\/submission.csv', index=False)","71f2462f":"submission1 = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": Y_pred_knn\n    })\nsubmission.to_csv('..\/working\/submission1.csv', index=False)","d30dfe4d":"submission.head()","82edfcbd":"submission1.head()","ab6eae37":"submission2 = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": Y_pred_svc\n    })\nsubmission.to_csv('..\/working\/submission2.csv', index=False)","66e61333":"submission3 = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": Y_pred_dt\n    })\nsubmission.to_csv('..\/working\/submission3.csv', index=False)","a49c920f":"from sklearn import tree, ensemble, linear_model\nensemble = Y_pred_rfc*0.70 + Y_pred_svc*0.15 + Y_pred_logreg*0.15","eb6aa54e":"submission5 = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": ensemble\n    })\nsubmission.to_csv('..\/working\/submission5.csv', index=False)","63e2e689":"# Cross validate model with Kfold stratified cross val\nkfold = StratifiedKFold(n_splits=10)","8a105e9c":"# Modeling step Test differents algorithms \nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nrandom_state = 2\nclassifiers = []\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(LinearDiscriminantAnalysis())\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, X_train, y = Y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighboors\",\"LogisticRegression\",\"LinearDiscriminantAnalysis\"]})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","ed20a91c":"### META MODELING  WITH ADABOOST, RF, EXTRATREES and GRADIENTBOOSTING\n\n# Adaboost\nDTC = DecisionTreeClassifier()\n\nadaDTC = AdaBoostClassifier(DTC, random_state=7)\n\nada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,2],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n\ngsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsadaDTC.fit(X_train,Y_train)\n\nada_best = gsadaDTC.best_estimator_","26fb34eb":"gsadaDTC.best_score_","5acd0520":"#ExtraTrees \nExtC = ExtraTreesClassifier()\n\n\n## Search grid for optimal parameters\nex_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsExtC.fit(X_train,Y_train)\n\nExtC_best = gsExtC.best_estimator_\n\n# Best score\ngsExtC.best_score_","500992e9":"# RFC Parameters tunning \nRFC = RandomForestClassifier()\n\n\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsRFC.fit(X_train,Y_train)\n\nRFC_best = gsRFC.best_estimator_\n\n# Best score\ngsRFC.best_score_","257a336d":"# Gradient boosting tunning\n\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsGBC.fit(X_train,Y_train)\n\nGBC_best = gsGBC.best_estimator_\n\n# Best score\ngsGBC.best_score_","219ba535":"### SVC classifier\nSVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\n\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsSVMC.fit(X_train,Y_train)\n\nSVMC_best = gsSVMC.best_estimator_\n\n# Best score\ngsSVMC.best_score_","d4f2d7c4":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ng = plot_learning_curve(gsRFC.best_estimator_,\"RF mearning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsExtC.best_estimator_,\"ExtraTrees learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsSVMC.best_estimator_,\"SVC learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsadaDTC.best_estimator_,\"AdaBoost learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsGBC.best_estimator_,\"GradientBoosting learning curves\",X_train,Y_train,cv=kfold)","0787dc45":"# Feature Importance\n\nnrows = ncols = 2\nfig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(15,15))\n\nnames_classifiers = [(\"AdaBoosting\", ada_best),(\"ExtraTrees\",ExtC_best),(\"RandomForest\",RFC_best),(\"GradientBoosting\",GBC_best)]\n\nnclassifier = 0\nfor row in range(nrows):\n    for col in range(ncols):\n        name = names_classifiers[nclassifier][0]\n        classifier = names_classifiers[nclassifier][1]\n        indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n        g = sns.barplot(y=X_train.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , orient='h',ax=axes[row][col])\n        g.set_xlabel(\"Relative importance\",fontsize=12)\n        g.set_ylabel(\"Features\",fontsize=12)\n        g.tick_params(labelsize=9)\n        g.set_title(name + \" feature importance\")\n        nclassifier += 1","f58542ac":"test_Survived_RFC = pd.Series(RFC_best.predict(test1), name=\"RFC\")\ntest_Survived_ExtC = pd.Series(ExtC_best.predict(test1), name=\"ExtC\")\ntest_Survived_SVMC = pd.Series(SVMC_best.predict(test1), name=\"SVC\")\ntest_Survived_AdaC = pd.Series(ada_best.predict(test1), name=\"Ada\")\ntest_Survived_GBC = pd.Series(GBC_best.predict(test1), name=\"GBC\")\n\n\n# Concatenate all classifier results\nensemble_results = pd.concat([test_Survived_RFC,test_Survived_ExtC,test_Survived_AdaC,test_Survived_GBC, test_Survived_SVMC],axis=1)\n\n\ng= sns.heatmap(ensemble_results.corr(),annot=True)","76874041":"# Voting Classifier\n\nvotingC = VotingClassifier(estimators=[('rfc', RFC_best), ('extc', ExtC_best),\n('svc', SVMC_best), ('adac',ada_best),('gbc',GBC_best)], voting='soft', n_jobs=4)\n\nvotingC = votingC.fit(X_train, Y_train)\n","c6af6f8e":"submissionNew = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": votingC.predict(test1)\n    })\nsubmission.to_csv('..\/working\/submissionNew.csv', index=False)","b64ac4e1":"submissionNew.head()","b1c0b4a8":"I decided to choose the SVC, AdaBoost, RandomForest , ExtraTrees and the GradientBoosting classifiers for the ensemble modeling.","5b7b41b4":"Age and Cabin features have an important part of missing values.\nSurvived missing values correspond to the join testing dataset (Survived column doesn't exist in test set and has been replace by NaN values when concatenating the train and test set)","1a39d020":"Hyperparameter tunning for best models","f27fbd68":"As we can see, Fare distribution is very skewed. This can lead to overweigth very high values in the model, even if it is scaled.\nIn this case, it is better to transform it with the log function to reduce this skew.","eeb59199":"Modelling","a23457ee":"Again Age and Cabin having missing value in test data also.","e1a14f31":"According to the feature importance of this 4 classifiers, the prediction of the survival seems to be more associated with the Age, the Sex, the family size and the social standing of the passengers more than the location in the boat.","fa67f1b1":"We detect 10 outliers. The 28, 89 and 342 passenger have an high Ticket Fare\nThe 7 others have very high values of SibSP.","7a0ea364":"Boosting Technique","44a2ac9e":"****As we can see Age,Cabin and Embarked have missing values.","9862d87d":"GradientBoosting and Adaboost classifiers tend to overfit the training set. According to the growing cross-validation curves GradientBoosting and Adaboost could perform better with more training examples.\n\nSVC and ExtraTrees classifiers seem to better generalize the prediction since the training and cross-validation curves are close together."}}