{"cell_type":{"2a55d939":"code","4f26b8fe":"code","bde313c9":"code","35a663e1":"code","8e928ecf":"code","95ec4163":"code","5d321789":"code","eeef1c88":"code","1c81cc74":"code","4b9da7e8":"code","2879506e":"code","903ebf9c":"code","e2fefb43":"code","19c08fa8":"code","7a945fd9":"code","a722574a":"code","cfba9aaf":"code","d690e991":"markdown","7e5ea56f":"markdown","1a5cd70f":"markdown","c087285d":"markdown","0a4959bb":"markdown"},"source":{"2a55d939":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4f26b8fe":"pip install tweepy","bde313c9":"import re\nimport string\n\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom imblearn.under_sampling import InstanceHardnessThreshold\nfrom sklearn.svm import LinearSVC\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.naive_bayes import MultinomialNB, ComplementNB\nfrom sklearn.svm import LinearSVC\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils.multiclass import unique_labels\nfrom sklearn.feature_selection import SelectFromModel\n\nimport tweepy\nfrom tweepy import OAuthHandler\nfrom textblob import TextBlob\nimport joblib\nfrom joblib import load\nfrom sklearn.base import TransformerMixin, BaseEstimator\n\nfrom imblearn.pipeline import Pipeline\nimport pickle\n\n#pd.set_option('display.max_colwidth', 1000)\n\nimport spacy\nnlp = spacy.load(\"en_core_web_lg\")\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","35a663e1":"# Text Proccessing\n\nclass TextPreprocessor(BaseEstimator, TransformerMixin):\n    def __init__(self, text_attribute):\n        self.text_attribute = text_attribute\n    \n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X, *_):\n        X_copy = X.copy()\n        return X_copy[self.text_attribute].apply(self._preprocess_text)\n    \n    def _preprocess_text(self, text):\n        return self._lemmatize(self._leave_letters_only(self._clean(text)))\n    \n    def _clean(self, text):\n        bad_symbols = '!\"#%&\\'*+,-<=>?[\\\\]^_`{|}~'\n        text_without_symbols = text.translate(str.maketrans('', '', bad_symbols))\n\n        text_without_bad_words = ''\n        for line in text_without_symbols.split('\\n'):\n            if not line.lower().startswith('from:') and not line.lower().endswith('writes:'):\n                text_without_bad_words += line + '\\n'\n\n        clean_text = text_without_bad_words\n        email_regex = r'([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)'\n        regexes_to_remove = [email_regex, r'Subject:', r'Re:']\n        for r in regexes_to_remove:\n            clean_text = re.sub(r, '', clean_text)\n\n        return clean_text\n    \n    def _leave_letters_only(self, text):\n        text_without_punctuation = text.translate(str.maketrans('', '', string.punctuation))\n        return ' '.join(re.findall(\"[a-zA-Z]+\", text_without_punctuation))\n    \n    def _lemmatize(self, text):\n        doc = nlp(text)\n        words = [x.lemma_ for x in [y for y in doc if not y.is_stop and y.pos_ != 'PUNCT' \n                                    and y.pos_ != 'PART' and y.pos_ != 'X']]\n        return ' '.join(words)\n    \n\nclass DenseTransformer(TransformerMixin):\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def transform(self, X, y=None, **fit_params):\n        return X.todense()","8e928ecf":"# load the saved pipleine model\npipeline = load(\"..\/input\/suicide-text-pipeline2\/Suicide_text_classification (1).joblib\")","95ec4163":"#Twitter client for connecting to Twitter, fetch tweets and get sentiment\n\nclass TwitterClient(object):\n    '''\n    Generic Twitter Class for sentiment analysis.\n    '''\n    def __init__(self):\n        '''\n        Class constructor or initialization method.\n        '''\n        # keys and tokens from the Twitter Dev Console\n        consumer_key = 'xeBnocCW5BUdWkf8nxV4yCdmq'\n        consumer_secret = 'r0IwBH4VV0F8AH7GGXfJ6FueWt5Cc1VstEzcKyVH2Rhn4mBJZj'\n        access_token = '336239701-ELhAtnZciTmNwDrkYAlv3EmS58xfjpWY6JAKD7ik'\n        access_token_secret = 'iaVZc0rZaDSBQonLXpiwnGD3KLPXk1OVf3Q50YHYJlV8x'\n        \n        # attempt authentication\n        try:\n            # create OAuthHandler object\n            self.auth = OAuthHandler(consumer_key, consumer_secret)\n            # set access token and secret\n            self.auth.set_access_token(access_token, access_token_secret)\n            # create tweepy API object to fetch tweets\n            self.api = tweepy.API(self.auth)\n            self.api.verify_credentials()\n            print(\"Authentication OK!\")\n        except:\n            print(\"Error: Authentication Failed\")\n\n    def clean_tweet(self, tweet):\n        '''\n        Utility function to clean tweet text by removing links, special characters\n        using simple regex statements.\n        '''\n        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)\", \" \", tweet).split())\n\n    def get_tweet_sentiment(self, tweet):\n        '''\n        Utility function to classify sentiment of passed tweet\n        using textblob's sentiment method\n        '''\n        # create TextBlob object of passed tweet text\n        analysis = TextBlob(self.clean_tweet(tweet))\n        # set sentiment\n        if analysis.sentiment.polarity > 0:\n            return 'positive'\n        elif analysis.sentiment.polarity == 0:\n            return 'neutral'\n        else:\n            return 'negative'\n\n    def get_tweets(self, query, count, lang, locale):\n        '''\n        Main function to fetch tweets and parse them.\n        '''\n        # empty list to store parsed tweets\n        tweets = []\n\n        try:\n            # call twitter api to fetch tweets\n            fetched_tweets = self.api.search_tweets(q = query, count = count, lang = lang, locale = locale)\n\n            # parsing tweets one by one\n            for tweet in fetched_tweets:\n                # empty dictionary to store required params of a tweet\n                parsed_tweet = {}\n\n                # saving text of tweet\n                parsed_tweet['text'] = tweet.text\n                # saving sentiment of tweet\n                parsed_tweet['sentiment'] = self.get_tweet_sentiment(tweet.text)\n\n                # appending parsed tweet to tweets list\n                if tweet.retweet_count > 0:\n                    # if tweet has retweets, ensure that it is appended only once\n                    if parsed_tweet not in tweets:\n                        tweets.append(parsed_tweet)\n                else:\n                    tweets.append(parsed_tweet)\n\n            # return parsed tweets\n            return tweets\n\n        except tweepy.TweepError as e:\n            # print error (if any)\n            print(\"Error : \" + str(e))\n","5d321789":"import json\nimport plotly\nimport plotly.express as px\n\ndef main(q):\n    # creating object of TwitterClient Class\n    api = TwitterClient()\n    # calling function to get tweets\n    global tweets, tweets_df, graphJSON, data\n    #q = input(\"Enter text to predict :\")\n    tweets = api.get_tweets(query = q, count = 200, lang = \"en\", locale = \"ja\")\n    \n    tweets_df = pd.DataFrame(tweets)\n    \n    if \"sentiment\" in tweets_df:\n        tweets_text = tweets_df.drop(columns = [\"sentiment\"])\n        pred = pipeline.predict(tweets_text)\n        tweets_df[\"prediction\"] = pred\n        \n    else:\n        print(\"THIS ACCOUNT DOESN'T HAVE ENOUGH TWEETS\")\n    \n    #pie chart\n    if \"prediction\" in tweets_df:\n        data = tweets_df.prediction.value_counts()\n        data1 = [0.1, data]\n        labels = [\"non-suicide\", \"suicide\"]\n        colors = sns.color_palette('pastel')[0:5]\n    \n        if len(data) == 1:\n            fig = px.pie(data1, values = data1, names= labels)\n            #fig.show()\n            graphJSON = json.dumps(fig, cls=plotly.utils.PlotlyJSONEncoder)\n\n        else:\n            fig = px.pie(data, values = data, names= labels)\n            #fig.show()\n            graphJSON = json.dumps(fig, cls=plotly.utils.PlotlyJSONEncoder)\n    \n    else:\n        dic = {'text': [\"THIS ACCOUNT DESONT HAVE ENOUGH TWEETS\", \"NO VISUALIZZATION TO DISPLAY\"], 'sentiment': ['NONE', \"NONE\"],\n               'prediction': ['NONE', \"NONE\"]\n              }\n        tweets_df = pd.DataFrame.from_dict(dic)\n        print(\"NO VISUALIZATION TO DISPLAY\")\n        \n        data = tweets_df.prediction.value_counts()\n        data2 = [0.1, 100]\n        labels = [\"suicide\", \"non-suicide\"]\n        colors = sns.color_palette('pastel')[0:5]\n        \n        fig = px.pie(data2, values = data2, names= labels)\n        #fig.show()\n        graphJSON = json.dumps(fig, cls=plotly.utils.PlotlyJSONEncoder)\n        \n    return tweets_df\n\n    \n\n#if __name__ == \"__main__\":\n    # calling main function\n    #main()\n","eeef1c88":"main(\"suicide\")","1c81cc74":"#pip freeze > requirements.txt","4b9da7e8":"!pip install flask","2879506e":"!pip install flask-ngrok","903ebf9c":"! pip install pyngrok","e2fefb43":"#importing the required libraries\nfrom flask import Flask, render_template, request, redirect, url_for\nfrom joblib import load\nfrom flask_ngrok import run_with_ngrok\n#from get_tweets import get_related_tweets\nfrom logging import FileHandler,WARNING","19c08fa8":"\n! ngrok authtoken 20x5v7xjlqWfhYPLBEtGvaen1co_6DTY5UgHZszoT42gJrCxT","7a945fd9":"import os\nimport threading\n\nfrom flask import Flask\nfrom pyngrok import ngrok\n\n# Open a HTTP tunnel on the default port 80\npublic_url = ngrok.connect(port = '4040')\n\nos.environ[\"FLASK_ENV\"] = \"development\"\n\napp = Flask(__name__, template_folder = \"..\/input\/templates\")\nport = 5000\n\n# Open a ngrok tunnel to the HTTP server\npublic_url = ngrok.connect(port).public_url\nprint(\" * ngrok tunnel \\\"{}\\\" -> \\\"http:\/\/127.0.0.1:{}\\\"\".format(public_url, port))\n\n# Update any base URLs to use the public ngrok URL\napp.config[\"BASE_URL\"] = public_url\n\n# ... Update inbound traffic via APIs to use the public-facing ngrok URL\n\n\n# Define Flask routes\n# render default webpage\n@app.route('\/')\ndef home():\n    return render_template('index.html')\n\n# when the post method detect, then redirect to success function\n@app.route('\/', methods=['POST', 'GET'])\ndef get_data():\n    if request.method == 'POST':\n        user = request.form['text']\n        return redirect(url_for('success', name=user))\n\n# get the data for the requested query\n@app.route('\/success\/<name>')\ndef success(name):\n    results = main(name)\n    \n    return render_template('simple.html', tables=[results.to_html(classes='data')], titles= results.columns.values, graphJSON=graphJSON)\n\n","a722574a":"# Start the Flask server in a new thread\nthreading.Thread(target=app.run, kwargs={\"use_reloader\": False}).start()","cfba9aaf":"ngrok.kill()","d690e991":"## Get Tweet & Predict","7e5ea56f":"## Deploy Using Flask","1a5cd70f":"## Import Libraries","c087285d":"## Load Pipeline","0a4959bb":"# Tweet Classification Deployment"}}