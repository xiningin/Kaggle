{"cell_type":{"a1bb2418":"code","7b7cb048":"code","2ca3eae0":"code","5a667b37":"code","6e2f4f82":"code","72019441":"code","fa7fe320":"code","3bca9697":"code","1c89df93":"code","c96f68df":"code","03bb893d":"code","1ef4b516":"code","3f0561b5":"code","a74a695c":"code","5cd98641":"code","ad396b77":"code","9c26aa06":"code","6358d1e4":"code","66406093":"code","6fad0316":"code","4dacd8b8":"code","009a545f":"code","f233e556":"code","b88cab8a":"code","78f45cfa":"code","05aedac4":"code","4e29a23a":"code","610f129b":"code","2e04aa19":"code","b7a78406":"code","576519c4":"code","449fe237":"code","39f40290":"code","9df17114":"code","876dcfc5":"code","54da3bca":"code","b43ef43c":"code","60476d3b":"code","f1fafdc4":"code","8e9d1aa9":"code","ecfc9bed":"code","f4263a6c":"code","1635bb82":"code","4c2e0bf4":"code","3b5686dd":"code","fefdd0e8":"code","52e96d47":"code","6f6d2ea0":"code","da83f78f":"code","1de31ce5":"code","0f2e77b0":"code","0a947d1c":"code","057624c6":"code","a6875646":"code","70449989":"markdown","2fe94da0":"markdown","b805dc7e":"markdown","4eefeb13":"markdown","3e8885dd":"markdown","288aafd5":"markdown","b7eaa350":"markdown","c6f8ceb1":"markdown","6cb5734c":"markdown","f7f86cbb":"markdown","a9cc6ee5":"markdown","aeb9fe6b":"markdown","31ac69c5":"markdown","7bfba1c4":"markdown","38879bbd":"markdown","416d81f8":"markdown"},"source":{"a1bb2418":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7b7cb048":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport spacy\nimport time\nfrom spacy.matcher import Matcher\nfrom spacy.matcher import PhraseMatcher\nfrom wordcloud import WordCloud\npd.set_option('display.max_rows',1000)\npd.set_option('display.max_colwidth',1000)\n\n\ntrain_set_orig= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_set_orig = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","2ca3eae0":"print(f'Training Data has {train_set_orig.shape[0]} rows and {train_set_orig.shape[1]} columns')\nprint(f'Testing Data has {test_set_orig.shape[0]} rows and {train_set_orig.shape[1]} columns')","5a667b37":"test_set_orig.describe()","6e2f4f82":"train_set_orig.columns","72019441":"#number of positive and Negative tweets\nPN=train_set_orig['target'].value_counts()\nPN.plot(kind='bar',color=['red','green'])\nplt.xticks([0,1],['Negative','Real tweets'])\nprint(f'Training set has {PN[1]} Negative tweets and {PN[0]} Positive tweets')","fa7fe320":"#Comparing length of real tweet and non real tweetss\nplt.style.use('fivethirtyeight')\ntrain_group=train_set_orig.groupby('target')\nreal_text_length= [len(text) for text in train_group.get_group(0)['text']]\nneg_text_length= [len(text) for text in train_group.get_group(1)['text']]\nbins=150\nplt.figure(figsize=(20,5))\nplt.hist(real_text_length,bins= bins,label='Positive',color='green',alpha= 0.7)\nplt.hist(neg_text_length,bins=bins,label='Negative',color='red',alpha=0.6)\nplt.xlabel('length of tweets')\nplt.ylabel('Number of tweets')\nplt.legend()\n","3bca9697":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nplt.title('Number of characters in real and Not Real tweets')\nax1.hist(real_text_length,color= 'green')\nax1.set_xlabel('Number of real tweets',color= 'green')\nax1.set_ylim([0,1000])\nax2.hist(neg_text_length,color='red')\nax2.set_xlabel('Number of not real tweets',color='red')\nax2.set_ylim([0,1000])","1c89df93":"def avg(x):\n    return np.sum([len(word) for word in x])\n\nneg_tweet_avg= [np.sum(list(map(avg,text)))\/len(text.split()) for text in train_group.get_group(1)['text']]\npost_tweet_avg= [np.sum(list(map(avg,text)))\/len(text.split()) for text in train_group.get_group(0)['text']]","c96f68df":"fig,(ax1,ax2)= plt.subplots(1,2,figsize=(15,5))\nsns.distplot(neg_tweet_avg,color='red',ax=ax1)\nax1.set_title('negative tweets')\nax1.set_xlabel('average tweet length')\nsns.distplot(post_tweet_avg,color='green',ax=ax2)\nax2.set_title('positive tweets')\nax2.set_xlabel('average tweet length')\n","03bb893d":"nlp= spacy.load('en_core_web_lg')\nstopwords= nlp.Defaults.stop_words","1ef4b516":"# import time\n# tic=time.time()\n# corpus=[nlp(text) for text in train_group.get_group(1)['text']]\n# toc= time.time()\n# print(f'time taken by using list comprehension:{(toc-tic)*1000}ms')\n\n## time taken by using list comprehension:41624.28307533264ms\n","3f0561b5":"tic= time.time()\npos_text= train_group.get_group(0)['text'].to_string().replace('\\n',' ')\nneg_text= train_group.get_group(1)['text'].to_string().replace('\\n',' ')\n\npos_corpus=nlp(pos_text)\nneg_corpus=nlp(neg_text)\n\ntoc=time.time()\n\nprint(f'string conversion takes:{(toc-tic)*100}ms')\n##string conversion takes:1640.8559560775757ms\n\nmatcher= PhraseMatcher(nlp.vocab)\npattern= [nlp(word) for word in stopwords]\nmatcher.add('stopwords',None,*pattern)\n\npos_matches= matcher(pos_corpus)\nneg_matches= matcher(neg_corpus)\n\npos_words_found= [str(pos_corpus[pos_matches[i][1]:pos_matches[i][2]]) for i in range( len(pos_matches))]\nneg_words_found= [str(neg_corpus[neg_matches[i][1]:neg_matches[i][2]]) for i in range( len(neg_matches))]\n","a74a695c":"plt.figure(figsize=(15,5))\npd.Series(pos_words_found).value_counts()[:40].plot(kind='bar',color='green',label='Repetitive words in postitive tweet')\npd.Series(neg_words_found).value_counts()[:40].plot(kind='bar',color='red',label='Repetitive words in negative tweet')\n\nplt.legend()\n","5cd98641":"pos_special_char= train_group.get_group(1)['text'].str.findall('[^A-Za-z0-9\\s]')\nneg_special_char= train_group.get_group(0)['text'].str.findall('[^A-Za-z0-9\\s]')","ad396b77":"fig= plt.figure(figsize=(15,5))\n\nall_neg_char=[ch for char in neg_special_char for ch in char]\npd.Series(all_neg_char).value_counts()[:30].plot(kind='bar',label='characters used in negative tweets',color='red')\n\nall_pos_char=[ch for char in pos_special_char for ch in char]\npd.Series(all_pos_char).value_counts()[:30].plot(kind='bar',label='characters used in positive tweets',color='green',alpha=0.5)\n\nplt.legend()\n","9c26aa06":"#Plotting more clearly using a DataFrame\nn=pd.Series(all_neg_char).value_counts()\np= pd.Series(all_pos_char).value_counts()\ndf=pd.DataFrame({'positive':p,'negative':n}).sort_values('positive',ascending=False)[:20]\ndf.plot(kind='bar',figsize=(15,5),color=['green','red'])","6358d1e4":"a=pd.Series(pos_words_found).value_counts()\nb=pd.Series(neg_words_found).value_counts()\ndf= pd.DataFrame({'common_positive_words':a,'common_negative_words':b}).sort_values('common_positive_words',ascending=False)\ndf[:40].plot(kind='bar',figsize=(15,5),color=['green','red'])\n","66406093":"import re\nmatcher= Matcher(nlp.vocab)\npattern= [{'TEXT':{'REGEX':'#'}},{'TEXT':{'REGEX':'\\w+'}}]\nmatcher.add('p1',None,pattern)\n\ndoc= pos_corpus\nmatches= matcher(doc)\nhash_tags=[]\naa=[hash_tags.append(re.sub(r'_|\\\\n','',str(doc[matches[i][1]:matches[i][2]]))) for i in range(len(matches))]\nplt.figure(figsize=(12,8))\nword_cloud = WordCloud(\n                          background_color='black',\n                          max_font_size = 80\n                         ).generate(' '.join(hash_tags))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","6fad0316":"doc= neg_corpus\nmatches= matcher(doc)\nhash_tags=[]\naa=[hash_tags.append(re.sub(r'_|\\\\n','',str(doc[matches[i][1]:matches[i][2]]))) for i in range(len(matches))]\nplt.figure(figsize=(12,8))\nword_cloud = WordCloud(\n                          background_color='red',\n                          max_font_size = 80\n                         ).generate(' '.join(hash_tags))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","4dacd8b8":"from collections import defaultdict\n\ntrain= train_set_orig.copy()\ntest= test_set_orig.copy()","009a545f":"pd.DataFrame({'train':train.isna().sum()\/len(train)*100,'test':test.isna().sum()\/len(test)*100}).plot(kind='bar')","f233e556":"train['keyword'].value_counts()[:50]","b88cab8a":"train.head()","78f45cfa":"d= defaultdict(int)\n\nunique_keywords= train['keyword'].dropna().unique()\n\nkey_group= train[['keyword','target']].groupby('keyword')\nfor word in unique_keywords:\n    d[word]= [v for v in key_group.get_group(word)['target'].value_counts().values]\n","05aedac4":"for word in unique_keywords:\n    z= key_group.get_group(word)['target'].value_counts()\n    print(f'{word} keyword has {z[0]} postive tweets and {z[1]} negative tweets')\n    break","4e29a23a":"df=pd.DataFrame(d.items())\ndf2= pd.DataFrame(df[1].to_list(),columns=['positive','negative'])\ndf2.index= df[0]\n\ndf2=df2.sort_values(by='positive',ascending=False)","610f129b":"import matplotlib.pyplot as plt\n\ndf2=df2.sort_values(by='positive',ascending=False)\nfig,ax= plt.subplots()\nax.scatter(x='positive',y='negative',data= df2,alpha=0.4)\n\n# ax.scatter(x='positive',y='negative',data= pos,color='green')\n# ax.scatter(x='positive',y='negative',data= neg,color='red')\n\npos_keyword=pd.DataFrame([df2.iloc[i] for i in range(len(df2)) if df2['negative'][i]<5.0 and df2['positive'][i]>25])\nax.scatter(x='positive',y='negative',data= pos_keyword,color='green')\n\n\nneg_keyword= pd.DataFrame([df2.iloc[i] for i in range(len(df2)) if df2['positive'][i]<25.0 and df2['negative'][i]>15.0])\nax.scatter(x='positive',y='negative',data= neg_keyword,color='red')\n\n# # ax.annotate('ablaze',(23,13))\n\nplt.xlabel('positive')\nplt.ylabel('negative')","2e04aa19":"import re\n\nexample=\"New competition launched :https:\/\/www.kaggle.com\/c\/nlp-getting-started\"\ndef clean_url(text):\n    return re.sub(r':?http[s]?:\\S+','',text)\nclean_url(example)","b7a78406":"train['text']= train['text'].apply(lambda x: clean_url(x))\ntest['text']= test['text'].apply(lambda x:clean_url(x))\n","576519c4":"example = \"\"\"<div>\n<h1>Real or Fake<\/h1>\n<p>Kaggle <\/p>\n<a href=\"https:\/\/www.kaggle.com\/c\/nlp-getting-started\">getting started<\/a>\n<\/div>\"\"\"\n\ndef clean_tag(text):\n    return re.sub(r'<.*?>','',text)\nprint(clean_tag(example))","449fe237":"train['text']= train['text'].apply(lambda x:clean_tag(x))\ntest['text']= test['text'].apply(lambda x:clean_tag(x))\n","39f40290":"train.head(200)","9df17114":"def non_alpha(text):\n    return re.sub('[^A-Za-z0-9\\'@#]+',' ',text).lower()\nprint(non_alpha(\"\u00db\u00cf@LeoBlakeCarter: This #dog thinks he's an am.\"))","876dcfc5":"train['text']= train['text'].apply(lambda x: non_alpha(x))\ntest['text']= test['text'].apply(lambda x:non_alpha(x))\n","54da3bca":"train.head(400)","b43ef43c":"example='@Mohit1 @ Mohit2 @m @moo mohit5t01c @mohit5t01c hello Mohit @phdsquares'\ndef remove_user(text):\n    text= re.sub(r'(@[A-Za-z0-9]?[A-Za-z0-9]*)| \\s[A-Za-z0-9]+\\s','',text).split()\n    print(text)\n    text=[word for word in text if word.isalpha() == True and len(word)>2]\n    print(text)\n    text= ' '.join(text)\n    return text\nremove_user(example)","60476d3b":"train['text']= train['text'].apply(lambda x:remove_user(x))\ntest['text']= test['text'].apply(lambda x:remove_user(x))\n","f1fafdc4":"def create_corpus(df,target):\n    group= df.groupby('target').get_group(target)\n    corpus=[]\n    nn=[corpus.append(word) for text in group['text'] for word in text.split()]\n    return corpus\n\n","8e9d1aa9":"train_corpus= create_corpus(train,1)\nplt.figure(figsize=(12,8))\nwc= WordCloud().generate(' '.join(train_corpus))\nplt.imshow(wc)\nplt.title(\"Common words in Disaster's tweet\")\nplt.show()","ecfc9bed":"train_corpus= create_corpus(train,0)\nplt.figure(figsize=(12,8))\nwc= WordCloud().generate(' '.join(train_corpus))\nplt.imshow(wc)\nplt.title(\"Common words in Fake Disaster's tweet\")\nplt.show()","f4263a6c":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split","1635bb82":"train=train.drop(['keyword','id','location'],axis=1)\ntest= test.drop(['keyword','location'],axis=1)","4c2e0bf4":"import nltk\nfrom nltk.corpus import stopwords\nfrom spacy.lang.en.stop_words import STOP_WORDS #-- set\ns1= stopwords.words('english') #-- list\nprint(len(STOP_WORDS))\n\naa=[STOP_WORDS.add(word) for word in s1 if word not in STOP_WORDS]\nprint(len(STOP_WORDS))\n","3b5686dd":"cv= CountVectorizer(stop_words=STOP_WORDS)\ntrain= pd.DataFrame(cv.fit_transform(train['text']).todense())\ntest= pd.DataFrame(cv.transform(test['text']).todense())","fefdd0e8":"X= train\ny= train_set_orig['target']","52e96d47":"print(train.shape)\nprint(test.shape)","6f6d2ea0":"print(X.shape)\nprint(y.shape)","da83f78f":"from sklearn.model_selection import train_test_split\ntrain_x,val_x,test_y,val_y = train_test_split(X,y)\nprint(train_x.shape)\nprint(val_x.shape)\nprint(test_y.shape)\nprint(val_y.shape)","1de31ce5":"print(len(val_x))\nlen(val_y)","0f2e77b0":"import numpy as np\n\ndef initialize_parameters(X):\n    w= np.zeros(X.shape[1])\n    b=0\n    return w,b\ndef sigmoid(z):\n    return 1\/(1+np.exp(-z))\n\n    \ndef propogate(w,b,X,y):\n    m= len(X)\n    z= X.dot(w.T)\n    A= sigmoid(z)\n    cost=- 1\/m * np.sum(y * np.log(A) + (1-y)* np.log(1-A))\n    dz= A-y\n    dw= np.dot(dz.T , X) * (1\/m)\n    db= sum(A-y) * (1\/m)\n    \n    grads={'dw':dw,'db':db}\n    return cost,grads\n\ndef gradient_descent(w,b,X,y,grads,alpha,num_iteration):\n    \n    for i in range(num_iteration):     \n        \n        dw= grads['dw']\n        db= grads['db']\n        \n        w= w- alpha * dw\n        b= b- alpha * db\n        \n        cost,grads= propogate(w,b,X,y)\n        if i%100 ==0:\n            print(f'iterations:{i} --> cost {cost *100}')\n            \n    return cost,grads\n\nX1=train_x\nY1=test_y\n\nw,b=initialize_parameters(X1)\ncost,grads= propogate(w,b,X1,Y1)\nbest_cost,best_grads= gradient_descent(w,b,X1,Y1,grads,0.9,1000)\n\n# print('w',w.shape)\n# print('X',X1.shape)\n# print('y',Y1.shape)","0a947d1c":"dw1= best_grads['dw']\ndef predict(X,w):\n    y_pred= sigmoid(X.dot(w.T))<0.5\n    return y_pred.map({True: 1,False: 0})\ny_pred= predict(val_x,dw1)\nmean_absolute_error(y_pred,val_y)","057624c6":"#1 real disaster == less than 0.5\n#0 unreal disaster \nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(y_pred,val_y)","a6875646":"answer= predict(test,dw1)\noutput= pd.DataFrame({'id':test_set_orig['id'],'target':answer})\noutput.to_csv('submission_correct.csv',index=False)","70449989":"## most common #HashTags in negative tweets","2fe94da0":"## Data Cleaning\n","b805dc7e":"## most common #HashTags in negative tweets","4eefeb13":"## Data Cleaning","3e8885dd":"## Clean Text if their is any Html tags out in our text","288aafd5":"##  ![image.png](attachment:image.png)","b7eaa350":"## Location column are user generated so being that much dirty and lots of empty values, it can't be used as a feature","c6f8ceb1":"## As the name suggest we are removing URL's","6cb5734c":"## I made this Notebook just for learning Purpose and i have taken Idea from multiple sources, take what you want and if have any suggestion kindly put it down in comments. \nThis is my beginner code and I am happy to say that i have come a long way and looking forward to keep going. \nThis was something which gave me the boost to keep working.\n\nI hope it helps you the same!\nThanks :)","f7f86cbb":"## it will be interesting to see these figures after removing stop words","a9cc6ee5":"It can be clearly scene that tweets size between 120 to 400 length are same in both type of tweets.\nBut are the words length same too ??","aeb9fe6b":"## Some values in keyword section can only be written in one context","31ac69c5":"In Both positive and Negative tweet **['The' , 'a' , 'to' , 'of' , 'you']** dominates","7bfba1c4":"## Remove Symbols, and make every text in lower format but keep @ and # because they signify the @User and #Topic","38879bbd":" ## List of keyword which are highly positive\n> pos_keyword.index\n1. > ['body%20bags', 'outbreak', 'typhoon', 'oil%20spill', 'harm', 'ruin',\n       'wrecked', 'explode', 'panic', 'screaming', 'traumatised', 'blazing',\n       'blizzard', 'crush', 'evacuated', 'rescuers', 'suicide%20bomb',\n       'bloody', 'body%20bag', 'suicide%20bombing', 'panicking', 'smoke',\n       'nuclear%20disaster', 'collide', 'razed', 'electrocute', 'blew%20up',\n       'blight', 'suicide%20bomber', 'stretcher', 'screamed', 'drown',\n       'wildfire', 'wild%20fires', 'obliterate', 'crushed', 'mayhem',\n       'bombing', 'obliterated', 'avalanche'],\n      \n ## List of keyword which are highly Negative\n> neg_keyword.index\n 2. >['windstorm', 'collided', 'weapons', 'damage', 'burning%20buildings',\n       'police', 'hurricane', 'ambulance', 'explosion', 'bombed', 'hijacker',\n       'tornado', 'engulfed', 'hail', 'derail', 'rainstorm',\n       'natural%20disaster', 'rescued', 'storm', 'hijack', 'lightning']","416d81f8":"## Removing @User name"}}