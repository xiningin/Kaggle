{"cell_type":{"5ea391d7":"code","317407ab":"code","4aa9b008":"code","14835029":"code","992b32e9":"code","930a3b4e":"markdown"},"source":{"5ea391d7":"!pip install kaggle-environments --upgrade","317407ab":"%%writefile submission.py\n\nimport json\nimport numpy as np\nimport pandas as pd\n\nbandit_state = None\ntotal_reward = 0\nlast_step = None\n    \ndef multi_armed_bandit_agent (observation, configuration):\n    global history, history_bandit\n\n    no_reward_step = 0.3\n    decay_rate = 0.97 # how much do we decay the win count after each call\n    \n    global bandit_state,total_reward,last_step\n        \n    if observation.step == 0:\n        # initial bandit state\n        bandit_state = [[1,1] for i in range(configuration[\"banditCount\"])]\n    else:       \n        # updating bandit_state using the result of the previous step\n        last_reward = observation[\"reward\"] - total_reward\n        total_reward = observation[\"reward\"]\n        \n        # we need to understand who we are Player 1 or 2\n        player = int(last_step == observation.lastActions[1])\n        \n        if last_reward > 0:\n            bandit_state[observation.lastActions[player]][0] += last_reward\n        else:\n            bandit_state[observation.lastActions[player]][1] += no_reward_step\n        \n        bandit_state[observation.lastActions[0]][0] = (bandit_state[observation.lastActions[0]][0] - 1) * decay_rate + 1\n        bandit_state[observation.lastActions[1]][0] = (bandit_state[observation.lastActions[1]][0] - 1) * decay_rate + 1\n\n#     generate random number from Beta distribution for each agent and select the most lucky one\n    best_proba = -1\n    best_agent = None\n    for k in range(configuration[\"banditCount\"]):\n        proba = np.random.beta(bandit_state[k][0],bandit_state[k][1])\n        if proba > best_proba:\n            best_proba = proba\n            best_agent = k\n        \n    last_step = best_agent\n    return best_agent","4aa9b008":"%%writefile random_agent.py\n\nimport random\n\ndef random_agent(observation, configuration):\n    return random.randrange(configuration.banditCount)","14835029":"from kaggle_environments import make\n\nenv = make(\"mab\", debug=True)\n\nenv.reset()\nenv.run([\"random_agent.py\", \"submission.py\"])\nenv.render(mode=\"ipython\", width=800, height=700)","992b32e9":"env.reset()\nenv.run([\"submission.py\", \"submission.py\"])\nenv.render(mode=\"ipython\", width=800, height=700)","930a3b4e":"This notebook shows how to use multi-armed bandit.\n\nMulti-armed bandit is a widely used RL-algorithm because it is very balanced in terms of exploitation\/exploration.\n\nAlgorithm logic:\n\n- At each step for each bandit generate a random number from B(a+1, b+1). B - beta-distribution, a - decay adjusted total reward from this bandit, b - number of this bandits's historical losses.\n- Select the bandit with the largest generated number and use it to generate the next step.\n- Repeat"}}