{"cell_type":{"4cca092d":"code","e3038056":"code","988e69e4":"code","9fec7037":"code","f313ab39":"code","8fbe3ff6":"code","0488bf51":"code","5f431a97":"code","c32fa0ba":"code","339216cb":"code","f9ea8bb0":"code","9a1f6d41":"code","552f2173":"code","e75eab94":"code","19b0fd88":"code","64c88e4b":"code","37bbd4a2":"code","79d40da5":"code","652ac968":"code","78dc6e8a":"code","74e2c8b1":"code","572079eb":"code","f66fd0c7":"code","1cccb0e3":"code","9d9ab3ef":"code","450879a8":"code","9f138851":"code","000beb8c":"code","98c87a54":"code","bcb2042c":"code","d6549e01":"code","f903420f":"code","78e056d1":"code","0b37a929":"code","daaed4de":"code","f3014d10":"code","b8d65d9b":"code","85ad9124":"code","341745d2":"code","1dd764ac":"code","0e07d7d9":"code","4cf5adf5":"code","e989c7db":"code","243e032f":"code","4a485023":"code","30fa335a":"code","e92e725e":"code","45fed828":"code","af9b8282":"markdown","0da96a48":"markdown","8b3488bc":"markdown","dd2c2a2d":"markdown","5b0dd5e1":"markdown","5cb0b3e1":"markdown","198b57a7":"markdown","9c929d46":"markdown","8cee1029":"markdown","28cf142c":"markdown","8d594e84":"markdown","66287888":"markdown","5b87a0fb":"markdown","c1dbf085":"markdown","d78f2fed":"markdown","575351b4":"markdown","0c4d4f18":"markdown","2ab621d2":"markdown","4dcb9212":"markdown"},"source":{"4cca092d":"## Loading necessary libraries\nimport nltk\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer","e3038056":"## Reading the data\ndf=pd.read_csv('..\/input\/fake-news-classifier\/train.csv')","988e69e4":"df","9fec7037":"df.shape\n## There are 20800 rows and 5 columns as seen above","f313ab39":"pd.set_option('display.max_colwidth', None)\n## Increasing the width of the the columns","8fbe3ff6":"df['title']\n## Title contains the headline of the news","0488bf51":"df['text']\n## text contains the information regarding the headline.","5f431a97":"df['label'].value_counts()\n## There are 10413 'ones' and 10387 'zeroes' in the dataframe","c32fa0ba":"df.isnull().sum()\n## There are few null values present in the dataframe","339216cb":"df=df.dropna()\n## The null values are removed using the dropna function","f9ea8bb0":"df.isnull().sum()\n## As seen below there are no null values present in the dataframe now.","9a1f6d41":"df.reset_index(inplace=True)\n## As we can see in the output, the Series. reset_index() function has reset the index of the given Series.","552f2173":"df","e75eab94":"#df=df.head(10)","19b0fd88":"import re\nimport string","64c88e4b":"# remove all numbers with letters attached to them\nalphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n\n# .lower() - convert all strings to lowercase \npunc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n\n# Remove all '\\n' in the string and replace it with a space\nremove_n = lambda x: re.sub(\"\\n\", \" \", x)\n\n# Remove all non-ascii characters \nremove_non_ascii = lambda x: re.sub(r'[^\\x00-\\x7f]',r' ', x)\n\n# Apply all the lambda functions wrote previously through .map on the comments column\ndf['text'] = df['text'].map(alphanumeric).map(punc_lower).map(remove_n).map(remove_non_ascii)","37bbd4a2":"df['text']","79d40da5":"import nltk\nnltk.download('stopwords')","652ac968":"import tqdm","78dc6e8a":"from tqdm import tqdm\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport re\nps = PorterStemmer()\ncorpus = []\nfor i in tqdm(range(0, len(df))):\n    review = re.sub('[^a-zA-Z]', ' ', df['text'][i])\n    review = review.lower()\n    review = review.split()\n    \n    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n    review = ' '.join(review)\n    corpus.append(review)","74e2c8b1":"Y=df['label']\n## We select the label column as Y","572079eb":"Y.head()","f66fd0c7":"df.to_csv('cleaned_data.csv', index=False)","1cccb0e3":"X_train, X_test, Y_train, Y_test = train_test_split(df['text'], Y, test_size=0.30, random_state=40)\n## We have split the data into 70 percent train and 30 percent test","9d9ab3ef":"#Applying tfidf to the data set\ntfidf_vect = TfidfVectorizer(stop_words = 'english',max_df=0.7)\ntfidf_train = tfidf_vect.fit_transform(X_train)\ntfidf_test = tfidf_vect.transform(X_test)","450879a8":"print(tfidf_test)","9f138851":"# Get the feature names of `tfidf_vectorizer` \nprint(tfidf_vect.get_feature_names()[-10:])","000beb8c":"count_vect = CountVectorizer(stop_words = 'english')\ncount_train = count_vect.fit_transform(X_train.values)\ncount_test = count_vect.transform(X_test.values)","98c87a54":"print(count_test)","bcb2042c":"# Get the feature names of `count_vectorizer` \nprint(count_vect.get_feature_names()[-10:])","d6549e01":"from sklearn.naive_bayes import MultinomialNB","f903420f":"from sklearn import metrics\nfrom sklearn.metrics import accuracy_score","78e056d1":"#Applying Naive Bayes\nclf = MultinomialNB() \nclf.fit(tfidf_train, Y_train)                       \npred = clf.predict(tfidf_test)                     \nscore = metrics.accuracy_score(Y_test, pred)\nprint(\"accuracy:   %0.3f\" % score)\ncm = metrics.confusion_matrix(Y_test, pred)\nprint(cm)","0b37a929":"print('Wrong predictions out of total')\nprint((Y_test !=pred).sum(),'\/',((Y_test == pred).sum()+(Y_test != pred).sum()))\nprint('Percentage accuracy: ',100*accuracy_score(Y_test,pred))","daaed4de":"## Plotting confusion matrix for TF-Idf vectorizer","f3014d10":"sns.heatmap(cm, cmap=\"plasma\", annot=True)","b8d65d9b":"#Applying Naive Bayes\nclf = MultinomialNB() \nclf.fit(count_train, Y_train)                       \npred1 = clf.predict(count_test)                    \nscore = metrics.accuracy_score(Y_test, pred1)\nprint(\"accuracy:   %0.3f\" % score)\ncm2 = metrics.confusion_matrix(Y_test, pred1)\nprint(cm2)","85ad9124":"print('Wrong predictions out of total')\nprint((Y_test !=pred1).sum(),'\/',((Y_test == pred1).sum()+(Y_test != pred1).sum()))\nprint('Percentage accuracy: ',100*accuracy_score(Y_test,pred1))","341745d2":"## Plotting confusion matrix for Count vectorizer.","1dd764ac":"sns.heatmap(cm2, cmap=\"plasma\", annot=True)","0e07d7d9":"from sklearn.ensemble import RandomForestClassifier","4cf5adf5":"RF=RandomForestClassifier().fit(tfidf_train,Y_train)\n#predict on train \ntrain_preds2 = RF.predict(tfidf_train)\n#accuracy on train\nprint(\"Model accuracy on train is: \", accuracy_score(Y_train, train_preds2))\n\n#predict on test\ntest_preds2 = RF.predict(tfidf_test)\n#accuracy on test\nprint(\"Model accuracy on test is: \", accuracy_score(Y_test, test_preds2))\nprint('-'*50)\n\n\n\n#Confusion matrix\nprint(\"confusion_matrix train is: \", metrics.confusion_matrix(Y_train, train_preds2))\nprint(\"confusion_matrix test is: \", metrics.confusion_matrix(Y_test, test_preds2))\nprint('Wrong predictions out of total')\nprint('-'*50)\n\n# Wrong Predictions made.\nprint((Y_test !=test_preds2).sum(),'\/',((Y_test == test_preds2).sum()+(Y_test != test_preds2).sum()))\nprint('-'*50)","e989c7db":"RF=RandomForestClassifier().fit(count_train,Y_train)\n#predict on train \ntrain_preds3 = RF.predict(count_train)\n#accuracy on train\nprint(\"Model accuracy on train is: \", accuracy_score(Y_train, train_preds3))\n\n#predict on test\ntest_preds3 = RF.predict(count_test)\n#accuracy on test\nprint(\"Model accuracy on test is: \", accuracy_score(Y_test, test_preds3))\nprint('-'*50)\n\n\n\n#Confusion matrix\nprint(\"confusion_matrix train is: \", metrics.confusion_matrix(Y_train, train_preds3))\nprint(\"confusion_matrix test is: \", metrics.confusion_matrix(Y_test, test_preds3))\nprint('Wrong predictions out of total')\nprint('-'*50)\n\n# Wrong Predictions made.\nprint((Y_test !=test_preds3).sum(),'\/',((Y_test == test_preds3).sum()+(Y_test != test_preds3).sum()))\nprint('-'*50)","243e032f":"from sklearn.neighbors import KNeighborsClassifier","4a485023":"#fit the model on train data \nKNN = KNeighborsClassifier().fit(tfidf_train,Y_train)\n#predict on train \ntrain_preds4 = KNN.predict(tfidf_train)\n#accuracy on train\nprint(\"Model accuracy on train is: \", accuracy_score(Y_train, train_preds4))\n\n#predict on test\ntest_preds4 = KNN.predict(tfidf_test)\n#accuracy on test\nprint(\"Model accuracy on test is: \", accuracy_score(Y_test, test_preds4))\nprint('-'*50)","30fa335a":"#Confusion matrix\nprint(\"confusion_matrix train is: \", metrics.confusion_matrix(Y_train, train_preds4))\nprint(\"confusion_matrix test is: \", metrics.confusion_matrix(Y_test, test_preds4))\nprint('Wrong predictions out of total')\nprint('-'*50)\n\n# Wrong Predictions made.\nprint((Y_test !=test_preds4).sum(),'\/',((Y_test == test_preds4).sum()+(Y_test != test_preds4).sum()))\n\nprint('-'*50)","e92e725e":"#fit the model on train data \nKNN = KNeighborsClassifier().fit(count_train,Y_train)\n#predict on train \ntrain_preds5 = KNN.predict(count_train)\n#accuracy on train\nprint(\"Model accuracy on train is: \", accuracy_score(Y_train, train_preds5))\n\n#predict on test\ntest_preds5 = KNN.predict(count_test)\n#accuracy on test\nprint(\"Model accuracy on test is: \", accuracy_score(Y_test, test_preds5))\nprint('-'*50)","45fed828":"#Confusion matrix\nprint(\"confusion_matrix train is: \", metrics.confusion_matrix(Y_train, train_preds5))\nprint(\"confusion_matrix test is: \", metrics.confusion_matrix(Y_test, test_preds5))\nprint('Wrong predictions out of total')\nprint('-'*50)\n\n# Wrong Predictions made.\nprint((Y_test !=test_preds5).sum(),'\/',((Y_test == test_preds5).sum()+(Y_test != test_preds5).sum()))\n\nprint('-'*50)","af9b8282":"### Count Vectorized","0da96a48":"## Text cleaning","8b3488bc":"### Count vectorizer","dd2c2a2d":"## Making train and test data","5b0dd5e1":"# The aim of the project is to build a fake news classifier using Natural Language Processing.","5cb0b3e1":"#### The Porter stemming algorithm (or 'Porter stemmer') is a process for removing the commoner morphological and inflexional endings from words in English. Its main use is as part of a term normalisation process that is usually done when setting up Information Retrieval systems.","198b57a7":"## Removing stop words and stemming the text","9c929d46":"Understanding TfidfVectorizer Using a Simple Example\nThe TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents. Alternately, if you already have a learned CountVectorizer, you can use it with a TfidfTransformer to just calculate the inverse document frequencies and start encoding documents.","8cee1029":"### Tfidf vectorizer","28cf142c":"#### In natural language processing, useless words (data), are referred to as stop words. ... Stop Words: A stop word is a commonly used word (such as \u201cthe\u201d, \u201ca\u201d, \u201can\u201d, \u201cin\u201d) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.","8d594e84":"### Count Vectorized","66287888":"## Machine learning","5b87a0fb":"# 1. Naive Bayes model","c1dbf085":"# K-Nearest Neighbour","d78f2fed":"# 2. Random Forest Model","575351b4":"Understanding CountVectorizer\nThe CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary.\n\nYou can use it as follows:\n\nCreate an instance of the CountVectorizer class.\nCall the fit() function in order to learn a vocabulary from one or more documents.\nCall the transform() function on one or more documents as needed to encode each as a vector.\nAn encoded vector is returned with a length of the entire vocabulary and an integer count for the number of times each word appeared in the document.","0c4d4f18":"## Splitting the dataframe","2ab621d2":"### TF-Idf Vectorized","4dcb9212":"### TF-Idf vectorized "}}