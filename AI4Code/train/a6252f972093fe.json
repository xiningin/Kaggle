{"cell_type":{"77574525":"code","63cfe715":"code","2ceb9ee9":"code","b0027556":"code","9ca674fb":"code","100e3583":"code","97de2dee":"code","7a64c935":"code","2c9ab467":"code","d7a80635":"code","8f420448":"code","eecb1cfa":"code","ce6e5bc2":"code","2077013e":"code","22c3dccd":"code","36ce86ed":"code","373736b9":"code","7f943fa5":"code","2c370ba6":"code","9cb27f1e":"code","fc50c996":"code","4d71890b":"code","77ba62c7":"code","dfac0fd6":"code","7c194973":"code","b9aed181":"code","769d55cd":"code","626e4a0a":"code","9dc123b9":"code","4b0ce600":"code","8d4bea7a":"code","7b6735e9":"code","b04f151c":"code","d2d6ee3b":"code","f0c4909f":"code","b67354f7":"code","93cbbd2c":"code","765fa2d2":"code","0fb450ca":"code","fd388816":"code","2c9c68ac":"code","bf5b99f1":"code","7a0fdb72":"code","13524933":"code","31bf1af9":"code","a0e57770":"code","3c92878a":"code","715739dd":"code","65808023":"code","8530c1f9":"code","c3456c63":"code","64a0682a":"code","1e734c67":"code","eceaf136":"markdown","0bd3b6b3":"markdown","022e3d52":"markdown","1ba7a6ca":"markdown","c89220d0":"markdown","b273afd3":"markdown","bb17bfdb":"markdown"},"source":{"77574525":"!pip install lifelines","63cfe715":"%config Completer.use_jedi = False\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom collections import Counter\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression, Lasso\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier, AdaBoostClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, plot_confusion_matrix, recall_score, precision_score, make_scorer\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler, LabelEncoder\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.feature_selection import chi2, SelectFromModel, SelectKBest, RFE\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, StratifiedKFold\nfrom sklearn.svm import SVC\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom imblearn.pipeline import Pipeline as imbpipeline\nfrom imblearn.combine import SMOTETomek\nimport xgboost as xgb\nimport lightgbm as gbm\nimport statsmodels.api as sm\nfrom lifelines import CoxPHFitter\nfrom lifelines import KaplanMeierFitter\nfrom lifelines.utils import concordance_index as c_idx","2ceb9ee9":"data = pd.read_csv('..\/input\/lung-cancer-life-expectancy\/expectancy.csv')\ndata.head()","b0027556":"col = ['id', 'Diagnosis', 'Forced_Capacity', 'Forced_Expiration', 'Zubrod_scale', 'Pain', 'Haemoptysis',\n       'Dyspnoea', 'Cough','Weakness', 'Size_of_tumor', 'Diabetes', 'MI_6months', 'PAD', 'Smoker', 'Asthmatic', \n       'Age', 'Dead\/Alive']\ndata.columns = col","9ca674fb":"data['Dead\/Alive'] = data['Dead\/Alive'].map({'F':'Alive', 'T':'Dead'})","100e3583":"data.head()","97de2dee":"data.describe()","7a64c935":"data.isnull().sum()","2c9ab467":"for feature in data.columns:\n    print(feature, ':', len(data[feature].unique()))","d7a80635":"discrete_features, continuous_features = [], []\nfor feature in data.columns:\n    if feature == 'Dead\/Alive' or feature == 'id':\n        continue\n    elif len(data[feature].unique()) > 10:\n        continuous_features.append(feature)\n    else:\n        discrete_features.append(feature)\n        \nprint('Discrete Features:', discrete_features, '\\n', 'Continuous Features:', continuous_features)","8f420448":"fig = go.Figure(data=[go.Pie(labels=list(Counter(data['Dead\/Alive']).keys()),\n                     values=list(Counter(data['Dead\/Alive']).values()),\n                     title='Output Count Distribution',\n                     name='')])\nfig.update_traces(textfont_size=20)\nfig.update_layout(font=dict(size=18))\n\nfig.show()","eecb1cfa":"fig = make_subplots(rows=5, cols=3, specs=5*[3*[{'type':'domain'}]])\nfor i in range(1, 6):\n    for j in range(1, 4):\n        idx = 4 * (i - 1) + j\n        if idx <= len(discrete_features):\n            curr_feature = discrete_features[idx-1]\n            fig.add_trace(go.Pie(labels=list(Counter(data[curr_feature]).keys()),\n                                 values=list(Counter(data[curr_feature]).values()),\n                                 title=curr_feature, name=''), row=i, col=j)\n\nfig.update_traces(textfont_size=10)\nfig.update_layout(height=1000, width=950, margin=dict(t=0, b=0, l=0, r=0), font=dict(size=10))\nfig.show()","ce6e5bc2":"correlation = data[continuous_features].corr()\nplt.figure(figsize=(10, 10))\nsns.heatmap(correlation, annot=True)\nplt.show()","2077013e":"sns.pairplot(data[continuous_features])","22c3dccd":"fig, ax = plt.subplots(len(continuous_features), 2, figsize=(14,12))\n\nfor i, feature in enumerate(continuous_features):\n    sns.kdeplot(ax=ax[i, 0], x=feature, hue='Dead\/Alive', data=data, fill = True)\n    sns.boxplot(ax=ax[i, 1], x=feature, data=data)\nfig.tight_layout(pad=1)\nplt.show()","36ce86ed":"processed_data = data.copy()","373736b9":"encoder = LabelEncoder()\nfor feature in discrete_features:\n    processed_data[feature] = encoder.fit_transform(processed_data[feature])\n    \nprocessed_data['Dead\/Alive'] = processed_data['Dead\/Alive'].map({'Dead':1, 'Alive':0}).astype(int)","7f943fa5":"processed_data.head()","2c370ba6":"correlation = processed_data[discrete_features].corr()\nplt.figure(figsize=(15, 15))\nsns.heatmap(correlation, annot=True)\nplt.show()","9cb27f1e":"def deal_outliers(col, data, cap_value, is_lower_cap=False):\n    if is_lower_cap:\n        data.loc[data[col] < cap_value, col] = cap_value\n        return\n    data.loc[data[col] > cap_value, col] = cap_value\n    return","fc50c996":"deal_outliers(col='Forced_Expiration', data=processed_data, cap_value=20)","4d71890b":"deal_outliers(col='Age', data=processed_data, cap_value=40, is_lower_cap=True)","77ba62c7":"fig, ax = plt.subplots(len(continuous_features), 2, figsize=(14,12))\n\nfor i, feature in enumerate(continuous_features):\n    sns.kdeplot(ax=ax[i, 0], x=feature, hue='Dead\/Alive', data=processed_data, fill = True)\n    sns.boxplot(ax=ax[i, 1], x=feature, data=processed_data)\nfig.tight_layout(pad=1)\nplt.show()","dfac0fd6":"processed_data.drop(['id'], axis=1, inplace=True)","7c194973":"processed_data.head()","b9aed181":"X = processed_data.drop('Dead\/Alive', axis=1)\ny = processed_data['Dead\/Alive']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=123)\n\nprint(len(X_train), len(X_test), len(y_train), len(y_test))\nprint(X_train.shape)","769d55cd":"oversample = SMOTE()\nX_train, y_train = oversample.fit_resample(X_train, y_train)\nprint(len(X_train), len(X_test), len(y_train), len(y_test))","626e4a0a":"Counter(y_train)","9dc123b9":"scaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","4b0ce600":"chi_selector = SelectKBest(chi2, k=10)\nchi_selector.fit(X_train_scaled, y_train)\n\nchi_support = chi_selector.get_support()\nchi_feature = X_train.loc[:,chi_support].columns.tolist()\nprint(str(len(chi_feature)), 'selected features')\nprint(chi_feature)","8d4bea7a":"chi_selector.scores_","7b6735e9":"best_features = SelectKBest(chi2, k=10)\nfeatures_ranking = best_features.fit(X_train_scaled, y_train)\nranking_dictionary = {}\nfor i in range(len(features_ranking.scores_)):\n    ranking_dictionary[X_train.columns[i]] = round(features_ranking.scores_[i], 3)\n\nasc_sort = sorted(ranking_dictionary.items(), key = lambda kv:(kv[1], kv[0]))\n\nfor i, j in asc_sort:\n    print(i, ':', j)","b04f151c":"rfe_selector = RFE(estimator=RandomForestClassifier())\nrfe_selector.fit(X_train_scaled, y_train)\n\nrfe_support = rfe_selector.get_support()\nrfe_feature = X_train.loc[:,rfe_support].columns.tolist()\n\nrfe_selector.ranking_\nfor i in rfe_selector.ranking_:\n    print(i, X_train.columns[i-1])","d2d6ee3b":"lasso_selector = SelectFromModel(RandomForestClassifier(), max_features=10)\nlasso_selector.fit(X_train_scaled, y_train)\n\nlasso_support = lasso_selector.get_support()\nlasso_feature = X_train.loc[:,lasso_support].columns.tolist()\nprint(str(len(lasso_feature)), 'selected features')\nprint(lasso_feature)","f0c4909f":"feature_selection_df = pd.DataFrame({'Feature':X_train.columns, 'Chi-2':chi_support, 'RFE':rfe_support, 'Lasso':lasso_support})\n# counting the votes by each method for each feature\nfeature_selection_df['Total'] = np.sum(feature_selection_df, axis=1)\n\nfeature_selection_df = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)\nfeature_selection_df.index = range(1, len(feature_selection_df)+1)\nfeature_selection_df","b67354f7":"scaler = StandardScaler()\nX_train_all = scaler.fit_transform(X_train)\nX_test_all = scaler.transform(X_test)","93cbbd2c":"selected_features = feature_selection_df[feature_selection_df['Total'] >= 1]['Feature'].tolist()\nprint(selected_features)\n\nscaler = StandardScaler()\nX_train_14 = scaler.fit_transform(X_train[selected_features])\nX_test_14 = scaler.transform(X_test[selected_features])","765fa2d2":"selected_features = feature_selection_df[feature_selection_df['Total'] >= 2]['Feature'].tolist()\nprint(selected_features)","0fb450ca":"X = processed_data.drop('Dead\/Alive', axis=1)\ny = processed_data['Dead\/Alive']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=123)\n\n# X_train = X_train[selected_features]\n# X_test = X_test[selected_features]\n\nprint(len(X_train), len(X_test), len(y_train), len(y_test))\nprint(X_train.shape)","fd388816":"pipeline = imbpipeline(steps = [['smote', SMOTE(random_state=123)],\n                                ['scaler', MinMaxScaler()],\n                                ['classifier', DecisionTreeClassifier(random_state=123)]])\n\nstratified_kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=123)\n    \nparam_grid = {'classifier__max_depth':[4, 5, 6, 7, 8], \n              'classifier__max_features':np.arange(1, X_train.shape[1], 1)}\ngrid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring=make_scorer(recall_score),\n                           cv=stratified_kfold, n_jobs=-1)\n\ngrid_search.fit(X_train, y_train)\ncv_score = grid_search.best_score_\ntest_score = grid_search.score(X_test, y_test)\nprint(grid_search.best_params_)\nprint(f'Cross-validation score: {cv_score}\\nTest score: {test_score}')","2c9c68ac":"X = processed_data[['Forced_Expiration', 'Forced_Capacity', 'Smoker', 'Age', 'Zubrod_scale', 'Weakness', 'Size_of_tumor']]\ny = processed_data['Dead\/Alive']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=123)\n\npipeline = imbpipeline(steps = [['smote', SMOTE(random_state=123)],\n                                ['scaler', MinMaxScaler()],\n                                ['classifier', RandomForestClassifier(random_state=123)]])\n\nstratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n    \nparam_grid = {'classifier__n_estimators':np.arange(10, 50, 10),\n              'classifier__max_depth':np.arange(1, 10, 2),\n              'classifier__max_features':np.arange(1, X_train.shape[1], 1),\n              'classifier__min_samples_leaf':[1, 2, 4],\n              'classifier__min_samples_split':[2, 5, 10],\n              'classifier__bootstrap':[True, False]}\n\nscoring_params = {'acc': make_scorer(accuracy_score), 'rec': make_scorer(recall_score)}\n\ngrid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring=scoring_params,\n                           cv=stratified_kfold, n_jobs=-1, refit='rec')\n\ngrid_search.fit(X_train, y_train)\ncv_score = grid_search.best_score_\ntest_score = grid_search.score(X_test, y_test)\nprint(f'Cross-validation score: {cv_score}\\nTest score: {test_score}')\nprint(grid_search.best_params_)","bf5b99f1":"X = processed_data[['Forced_Expiration', 'Forced_Capacity', 'Smoker', 'Age', 'Zubrod_scale', 'Weakness', 'Size_of_tumor']]\ny = processed_data['Dead\/Alive']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=123)\n\nprint(len(X_train), len(X_test), len(y_train), len(y_test))\nprint(X_train.shape)\nsmote = SMOTE(random_state=123)\nX_train_sampled, y_train_sampled = smote.fit_resample(X_train, y_train)\n\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train_sampled)\nX_test_scaled = scaler.transform(X_test)\n\nrf_model = RandomForestClassifier(max_depth=1, max_features=6, n_estimators=30, bootstrap=False, min_samples_leaf=1,\n                                  min_samples_split=2, random_state=123)\nrf_model.fit(X_train_scaled, y_train_sampled)\ny_pred = rf_model.predict(X_test_scaled)\nconf = plot_confusion_matrix(rf_model, X_test_scaled, y_test)\nprint (\"The accuracy of Random Forest is : \", accuracy_score(y_test, y_pred)*100, \"%\")\nprint(classification_report(y_test, y_pred))\n\nprint(rf_model.feature_importances_)","7a0fdb72":"rf_model.feature_importances_","13524933":"import pickle\npickle.dump(rf_model, open('.\/lung_cancer_rf_model.pkl', 'wb'))","31bf1af9":"pipeline = imbpipeline(steps = [['smote', SMOTE(random_state=123)],\n                                ['scaler', MinMaxScaler()],\n                                ['classifier', GradientBoostingClassifier(random_state=123)]])\n\nstratified_kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=123)\n    \nparam_grid = {'classifier__learning_rate':[0.001, 0.005, 0.01, 0.05, 0.1, 0.5],\n              'classifier__n_estimators':[100, 300, 500, 700, 900, 1000],\n              'classifier__max_depth':[4, 5, 6, 7, 8, 10, 12], \n              'classifier__max_features':[7, 9, 10, 12, 15]}\n\ngrid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, scoring=make_scorer(recall_score),\n                           cv=stratified_kfold, n_jobs=-1)\n\ngrid_search.fit(X_train, y_train)\ncv_score = grid_search.best_score_\ntest_score = grid_search.score(X_test, y_test)\nprint(f'Cross-validation score: {cv_score}\\nTest score: {test_score}')\nprint(grid_search.best_params_)","a0e57770":"X_train_selected = X_train_10\nX_test_selected = X_test_10","3c92878a":"log_model = LogisticRegression(max_iter=100)\nlog_model.fit(X_train_selected, y_train)\ny_pred = log_model.predict(X_test_selected)\nconf = plot_confusion_matrix(log_model, X_test_selected, y_test)\nprint (\"The accuracy of Logistic Regression is : \", accuracy_score(y_test, y_pred)*100, \"%\")\nprint(classification_report(y_test, y_pred))","715739dd":"model = sm.Logit(endog=y_train, exog=X_train).fit()\nmodel.summary()","65808023":"rf_model = RandomForestClassifier(max_depth=5)\nrf_model.fit(X_train_selected, y_train)\ny_pred = rf_model.predict(X_test_selected)\nconf = plot_confusion_matrix(rf_model, X_test_selected, y_test)\nprint (\"The accuracy of Random Forest is : \", accuracy_score(y_test, y_pred)*100, \"%\")\nprint(classification_report(y_test, y_pred))","8530c1f9":"gb_model = GradientBoostingClassifier(learning_rate=0.1, n_estimators=800)\ngb_model.fit(X_train_selected, y_train)\ny_pred = gb_model.predict(X_test_selected)\nconf = plot_confusion_matrix(gb_model, X_test_selected, y_test)\nprint (\"The accuracy of Gradient Boosting is : \", accuracy_score(y_test, y_pred)*100, \"%\")\nprint(classification_report(y_test, y_pred))","c3456c63":"svm_model = SVC(kernel='poly', degree=2, C=6)\nsvm_model.fit(X_train_selected, y_train)\ny_pred = svm_model.predict(X_test_selected)\nconf = plot_confusion_matrix(svm_model, X_test_selected, y_test)\nprint (\"The accuracy of SVM is : \", accuracy_score(y_test, y_pred)*100, \"%\")\nprint(classification_report(y_test, y_pred))","64a0682a":"xgb_model = xgb.XGBClassifier()\nxgb_model.fit(X_train_selected, y_train)\ny_pred = xgb_model.predict(X_test_selected)\nconf = plot_confusion_matrix(xgb_model, X_test_selected, y_test)\nprint (\"The accuracy of XGB is : \", accuracy_score(y_test, y_pred)*100, \"%\")\nprint(classification_report(y_test, y_pred))","1e734c67":"adb_model = AdaBoostClassifier(n_estimators=500, learning_rate=0.01)\nadb_model.fit(X_train_selected, y_train)\ny_pred = adb_model.predict(X_test_selected)\nconf = plot_confusion_matrix(adb_model, X_test_selected, y_test)\nprint (\"The accuracy of XGB is : \", accuracy_score(y_test, y_pred)*100, \"%\")\nprint(classification_report(y_test, y_pred))","eceaf136":"# Import Dataset","0bd3b6b3":"# Import Packages","022e3d52":"# Model Building","1ba7a6ca":"# EDA","c89220d0":"# Feature Engineering","b273afd3":"## Attribute Details\n\n1. DGN - Diagnosis Type\n\n2. PRE4 - Forced vital capacity\n3. PRE5 - Volume that has been exhaled at the end of the first second of forced expiration\n4. PRE6 - Performance status - Zubrod scale\n5. PRE7 - Pain before surgery\n6. PRE8 - Haemoptysis before surgery\n7. PRE9 - Dyspnoea before surgery\n8. PRE10 - Cough before surgery\n9. PRE11 - Weakness before surgery\n10. PRE14 - size of the original tumour, from OC11 (smallest) to OC14 (largest) (OC11,OC14,OC12,OC13)\n11. PRE17 - Type 2 DM - diabetes mellitus\n12. PRE19 - MI up to 6 months\n13. PRE25 - PAD - peripheral arterial diseases\n14. PRE30 - Smoking\n15. PRE32 - Asthama\n16. AGE - Age of the patient during surgery\n17. Risk1Yr - 1 year survival period - True(T) value if died","bb17bfdb":"# Feature Selection"}}