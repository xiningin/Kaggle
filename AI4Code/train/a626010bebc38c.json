{"cell_type":{"c5db3f1a":"code","1c7df625":"code","e9030ee5":"code","aebe294d":"code","c6cec59f":"code","fabf32e0":"code","731b76df":"code","11efa742":"code","382b6041":"code","82e287ac":"code","e16e4fbf":"code","a3d81c36":"code","3e0a782a":"code","ce250f3d":"code","b5e09a68":"code","156063d9":"code","2f65b6c8":"code","0c4464d3":"code","1b8ffeda":"code","59b8c457":"code","1a1444bb":"code","4848c2c7":"code","9db83240":"code","54c661cc":"code","15692ecf":"code","bb0f34f1":"markdown","5a539a22":"markdown","71051aa7":"markdown","6a438657":"markdown","24ec4a18":"markdown","5c1b8e07":"markdown","9c69098a":"markdown","a1f9c388":"markdown","eaeb0ab1":"markdown","3d62bab5":"markdown","aec6a51c":"markdown","3ead456e":"markdown","e05a80bb":"markdown","e581f60a":"markdown","b1e5fc00":"markdown","432addd5":"markdown","a011e0cd":"markdown","1b9b56a6":"markdown","e3bc9fff":"markdown"},"source":{"c5db3f1a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1c7df625":"train = pd.read_csv(\"\/kaggle\/input\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/test.csv\")\nvariable_description = pd.read_csv(\"\/kaggle\/input\/variable_description.csv\")","e9030ee5":"# print varible list\nvariable_description","aebe294d":"# print first 5 rows of train dataset\ntrain.head()","c6cec59f":"test1 = test.drop('crime_rate',axis=1)\ntest1.head()","fabf32e0":"train.describe()","731b76df":"train.isnull().sum()","11efa742":"train.dtypes","382b6041":"for col in train.columns:\n    plot = plt.boxplot(train[col])\n    print(f'plot of feature {col} is {plot}')\n    plt.show()","82e287ac":"# We are deleting the outliers from quartile 12th percentile and 88th percentile as appropriate for our model.\nQ1 = train.quantile(0.12)\nQ3 = train.quantile(0.88)\nIQR = Q3 - Q1\nprint(IQR)","e16e4fbf":"#Deleting Outliers\ntrain1 = train[~((train < (Q1 - 1.5 * IQR)) |(train > (Q3 + 1.5 * IQR))).any(axis=1)]\ntrain1.shape","a3d81c36":"train.shape","3e0a782a":"plt.figure(figsize=(16,8))\ncorr = train.corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","ce250f3d":"#Fetaures \nX = train1.drop('crime_rate', axis=1).copy()","b5e09a68":"X.head()","156063d9":"#label\ny = train1['crime_rate'].copy()","2f65b6c8":"y.head()","0c4464d3":"from sklearn.model_selection import train_test_split\ntrain_X, val_X, train_y, val_y = train_test_split(X, y,train_size=0.8, test_size=0.2,random_state = 0)","1b8ffeda":"from sklearn.linear_model import LinearRegression\nregr = LinearRegression()\nregr.fit(train_X,train_y)","59b8c457":"from sklearn.metrics import r2_score\npredict = regr.predict(val_X)\nr2_score(val_y,predict)","1a1444bb":"from sklearn.model_selection import GridSearchCV\nimport xgboost\n#for tuning parameters\n# parameters_for_testing = {\n#    'colsample_bytree':[0.4,0.6,0.8],\n# #    'gamma':[0,0.03,0.1,0.3],\n# #    'min_child_weight':[1.5,6,10],\n# #    'learning_rate':[0.1,0.07],\n# #    'max_depth':[3,5],\n# #    'n_estimators':[10000],\n# #    'reg_alpha':[1e-5, 1e-2,  0.75],\n# #    'reg_lambda':[1e-5, 1e-2, 0.45],\n# #    'subsample':[0.6,0.95]  \n# }\n\n                    \n# xgb_model = xgboost.XGBRegressor(learning_rate =0.1, n_estimators=1000, max_depth=5,\n#     min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8, nthread=-1, scale_pos_weight=1, seed=27)\n\n# gsearch1 = GridSearchCV(estimator = xgb_model, param_grid = parameters_for_testing, n_jobs=6,iid=False, verbose=10,scoring='neg_mean_squared_error')\n# gsearch1.fit(train_X,train_y)\n# # print (gsearch1.grid_scores_)\n# print('best params')\n# print (gsearch1.best_params_)\n# print('best score')\n# print (gsearch1.best_score_)\n\n#After tuning\nxgb_model = xgboost.XGBRegressor(learning_rate =0.01, n_estimators=10000, max_depth=5,\n    min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8, nthread=-1, scale_pos_weight=1, seed=27)\n\nxgb_model.fit(train_X,train_y)\npredict = xgb_model.predict(val_X)\nr2_score(val_y,predict)","4848c2c7":"from sklearn.datasets import load_boston\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import VotingRegressor\n\n# Training classifiers\nreg1 = GradientBoostingRegressor(random_state=1, n_estimators=10)\nreg2 = RandomForestRegressor(random_state=1, n_estimators=10)\nreg3 = LinearRegression()\nereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)])\nereg = ereg.fit(train_X,train_y)\npredict = ereg.predict(val_X)\nr2_score(val_y,predict)","9db83240":"ereg_final = ereg.fit(X,y)\npredict = ereg_final.predict(test1)\n","54c661cc":"sub = pd.DataFrame(data=predict\n                   ,columns=[\"crime_rate\"])","15692ecf":"sub.to_csv(\"submission.csv\")","bb0f34f1":"# 8.1 Linear Regression\nLinear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable, and the other is considered to be a dependent variable","5a539a22":"This method is giving us a optimal result. \n\n### Let's now make prediction on final test data with full train data.","71051aa7":"# 3.3 Check Non-numeric values\/Categorical data\n\nWe have to convert the non-numeric data if present. In case of my dataset set, there is no non numeric value as already seen in 2, but if there is present we have to transform it.","6a438657":"# 8.2 XGBOOST Regression\n\nXGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.","24ec4a18":"# 3.1 Basic Stat of each column\ndescribe() is used to view some basic statistical details like percentile, mean, std etc. of a data frame or a series of numeric values.","5c1b8e07":"# 8. Model creation\n\nI am implementing different kinds of regression algorithm here and best model is used for final prediction.","9c69098a":"# 2. Data\n\nWe are considering the data about 99 standard metropolitan areas in the US. This dataset provides information on 11 variables for each area for the period 1976-1977.The areas have been divided into 4 geographic regions: 1=North-East, 2=North-Central, 3=South, 4=West. The variables provided are listed in the table below:\n","a1f9c388":"# 4. Univirate Analysis\n\nUnivariate analysis is perhaps the simplest form of statistical analysis. The key fact is that only one variable is involved. Univariate analysis can help in finding outliers(misleading data) that can cause overfitting.","eaeb0ab1":"# 1. Problem Statement\n\n**Crime rates differ widely across different times and different places, and may depend upon number of factors like income of people living there, their education level, population, etc. According to some studies, it is investigated that the crime rate is reduces in police presence. So police play a vital role in reducing crime rate. But number of police personals are not that much or it is not possible to place all police personals in all places. Administration have to choose the places where they are going to place a large number of police personals throughout their county.**\n\n**Now question arises... How they are gonna choose those places. First solution comes to your mind is check the number of cases of that place. Yes, you think well. But may be there are some places where number of cases are low but crime rate start rising at particular time or high cases place is now not showing crime rate. There are lot of questions and if you try to find the answers manually, it take  a large amount of time. **\n\n**But What if told you that you can predict the crime rate easily from the past collected data within short span of time? Yes, this can be done with the help of Machine Learning.**\n\n\n> \"The new crime-fighting weapon of choice for a growing number of police forces around the world isn't a gun, a taser or pepper spray - it's data.\"**","3d62bab5":"Our dataset have no missing value that is good sign because if there is any, then we have to either drop or transform that row\/column.","aec6a51c":"# 7. Train and Test Split\nThe train_test_split function is for splitting a single dataset for two different purposes: training and testing. The testing subset is for building your model. The testing subset is for using the model on unknown data to evaluate the performance of the model.\n\n**training set**\u2014a subset to train a model.\n\n**test set**\u2014a subset to test the trained model.\n\n*We divide the data here into 80% train set and 20% test set.*","3ead456e":"No need to drop any feature.","e05a80bb":"# 3.2 Check Missing values\n\nCheck the missing values in the data set that may cause overfitting or underfitting in the model","e581f60a":"# 5. Bivariate analysis\n\nThis type of data involves two different variables. The analysis of this type of data deals with causes and relationships and the analysis is done to find out the relationship among the two variables.","b1e5fc00":"# 6. Features and labels\n\n**Feature**\nFeatures are individual independent variables which acts as the input in the system. Prediction models uses these features to make predictions. To make it simple, you can consider one column of your data set to be one feature. Features are also called attributes. And the number of features is dimensions.\n\n**Label**\nLabels are the final output or target Output. It can also be considered as the output classes.  We obtain labels as output when provided with features as input.\n\n*Split the data into features and labels*","432addd5":"### IQR score\n\nAccording to Wikipedia,\nThe interquartile range (IQR), also called the midspread or middle 50%, or technically H-spread, is a measure of statistical dispersion, being equal to the difference between 75th and 25th percentiles, or between upper and lower quartiles, IQR = Q3 \u2212 Q1.\nIn other words, the IQR is the first quartile subtracted from the third quartile; these quartiles can be clearly seen on a box plot on the data.\nIt is a measure of the dispersion similar to standard deviation or variance, but is much more robust against outliers","a011e0cd":"# 8.3 Ensemble method\n\nThe idea behind the VotingRegressor is to combine conceptually different machine learning regressors and return the average predicted values. Such a regressor can be useful for a set of equally well performing models in order to balance out their individual weaknesses.\n\nBelow i'm combining three ml regressor method- GradientBoostingRegressor, RandomForestRegressor and LinearRegression","1b9b56a6":"*Let's get started......*","e3bc9fff":"# 3. Data analysis\n\nBefore making the machine learning model that predict the crime rate, we firstly see the data what it can tell us."}}