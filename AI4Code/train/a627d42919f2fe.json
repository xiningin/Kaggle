{"cell_type":{"80f55642":"code","146b9fe6":"code","a9fd51bf":"code","5fcd4f0c":"code","b0a55435":"code","5b790ed8":"code","b096fb39":"code","2614136a":"code","6269f719":"code","7961325d":"code","ad9c0e75":"code","754a897f":"code","5c313ea6":"code","27c47af7":"code","0e6759a5":"code","aa81b431":"code","9650a8b9":"code","f4a94c92":"code","0103ca13":"code","4fa8b89d":"code","9cbbcb07":"markdown","5ce4f043":"markdown","b7239d3e":"markdown","04d3c23d":"markdown","bf474bcb":"markdown","acefa8c7":"markdown","fb09c669":"markdown","bf6b6238":"markdown","1a129268":"markdown","2e4bf4dc":"markdown","4ef94a16":"markdown","736fbb44":"markdown","c42104cc":"markdown","bb28661d":"markdown","2e1c0015":"markdown","ea784077":"markdown"},"source":{"80f55642":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","146b9fe6":"# Importing Liberaries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pandas import read_csv, set_option\nfrom pandas.plotting import scatter_matrix","a9fd51bf":"# Loading Data\nfraud_data = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')","5fcd4f0c":"# Viewing Raw Data\nset_option('display.width', 100)\nfraud_data.head()","b0a55435":"# Dimension of data\nfraud_data.shape","5b790ed8":"# Data Type\nfraud_data.info()","b096fb39":"# CHecking Null values\nfraud_data.isnull().sum()","2614136a":"# Summarizing data\nset_option('precision', 5)\nfraud_data.describe()\n","6269f719":"# Response Variable Analysis\nclass_names = {0:'Not Fraud', 1:'Fraud'}\nrvs = fraud_data.Class.value_counts().rename(index = class_names)\nprint(rvs)","7961325d":"from sklearn.model_selection import train_test_split\n\ny= fraud_data[\"Class\"]\nX = fraud_data.loc[:, fraud_data.columns != 'Class']\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size=1\/6, random_state=42)","ad9c0e75":"#Import Library for Accuracy Score\nfrom sklearn.metrics import accuracy_score\n\n#Import Library for Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\n#Initialize the Logistic Regression Classifier\nlogisreg = LogisticRegression()\n\n#Train the model using Training Dataset\nlogisreg.fit(X_train, y_train)\n\n# Prediction using test data\ny_pred = logisreg.predict(X_test)\n\n# Calculate Model accuracy by comparing y_test and y_pred\nacc_logisreg = round( accuracy_score(y_test, y_pred) * 100, 2 )\nprint( 'Accuracy of Logistic Regression model : ', acc_logisreg )","754a897f":"#Import Library for Linear Discriminant Analysis\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n#Initialize the Linear Discriminant Analysis Classifier\nmodel = LinearDiscriminantAnalysis()\n\n#Train the model using Training Dataset\nmodel.fit(X_train, y_train)\n\n# Prediction using test data\ny_pred = model.predict(X_test)\n\n# Calculate Model accuracy by comparing y_test and y_pred\nacc_lda = round( accuracy_score(y_test, y_pred) * 100, 2 )\nprint( 'Accuracy of Linear Discriminant Analysis Classifier: ', acc_lda )","5c313ea6":"#Import Library for Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\n\n#Initialize the Gaussian Naive Bayes Classifier\nmodel = GaussianNB()\n\n#Train the model using Training Dataset\nmodel.fit(X_train, y_train)\n\n# Prediction using test data\ny_pred = model.predict(X_test)\n\n# Calculate Model accuracy by comparing y_test and y_pred\nacc_ganb = round( accuracy_score(y_test, y_pred) * 100, 2 )\nprint( 'Accuracy of Gaussian Naive Bayes : ', acc_ganb )","27c47af7":"#Import Library for Decision Tree Classifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n#Initialize the Decision Tree Classifier\nmodel = DecisionTreeClassifier()\n\n#Train the model using Training Dataset\nmodel.fit(X_train, y_train)\n\n# Prediction using test data\ny_pred = model.predict(X_test)\n\n# Calculate Model accuracy by comparing y_test and y_pred\nacc_dtree = round( accuracy_score(y_test, y_pred) * 100, 2 )\nprint( 'Accuracy of  Decision Tree Classifier : ', acc_dtree )","0e6759a5":"#Import Library for Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Initialize the Random Forest\nmodel = RandomForestClassifier()\n\n#Train the model using Training Dataset\nmodel.fit(X_train, y_train)\n\n# Prediction using test data\ny_pred = model.predict(X_test)\n\n# Calculate Model accuracy by comparing y_test and y_pred\nacc_rf = round( accuracy_score(y_test, y_pred) * 100, 2 )\nprint( 'Accuracy of  Random Forest : ', acc_rf )","aa81b431":"#Import Library for Support Vector Machine\nfrom sklearn import svm\n\n#Initialize the Support Vector Classifier\nmodel = svm.SVC()\n\n#Train the model using Training Dataset\nmodel.fit(X_train, y_train)\n\n# Prediction using test data\ny_pred = model.predict(X_test)\n\n# Calculate Model accuracy by comparing y_test and y_pred\nacc_svc = round( accuracy_score(y_test, y_pred) * 100, 2 )\nprint( 'Accuracy of Support Vector Classifier: ', acc_svc )","9650a8b9":"#Import Library for K Nearest Neighbour Model\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#Initialize the K Nearest Neighbour Model with Default Value of K=5\nmodel = KNeighborsClassifier()\n\n#Train the model using Training Dataset\nmodel.fit(X_train, y_train)\n\n# Prediction using test data\ny_pred = model.predict(X_test)\n\n# Calculate Model accuracy by comparing y_test and y_pred\nacc_knn = round( accuracy_score(y_test, y_pred) * 100, 2 )\nprint( 'Accuracy of KNN Classifier: ', acc_knn )","f4a94c92":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Linear Discriminant Analysis','Naive Bayes', 'Decision Tree', 'Random Forest', 'Support Vector Machines', \n              'K - Nearest Neighbors'],\n    'Score': [acc_logisreg, acc_lda, acc_ganb, acc_dtree, acc_rf, acc_svc, acc_knn]})\n\nmodels.sort_values(by='Score', ascending=False)\n","0103ca13":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nsns.heatmap(cm, annot = True);","4fa8b89d":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(model, X_test, y_test);","9cbbcb07":"**Dont Forgot Upvote!!!!!!**","5ce4f043":"# Data Modelling","b7239d3e":"# Confusion Matrix\nThis is a binary classification problem (Fraud or No-Fraud). Some of the commonly used terms are:\n* True positives (TP)\n        Predicted positive and are actually positive.\n* False positives (FP)\n        Predicted positive and are actually negative.\n* True negatives (TN)\n        Predicted negative and are actually negative.\n* False negatives (FN)\n        Predicted negative and are actually positive.","04d3c23d":"> Our observations are as \n\n> NaN values do not present in the data set. Because of the Non-Null Count and number of rows in the dataset match.\n\n> There are 29 Input Variables and 1 Output Variable (Class)\n\n> The data type of all the input variables is float64 whereas the data type of out variable (Class) is int64","bf474bcb":"* In this case, overall accuracy is strong, but the confusion metrics tell a different story. \n* Despite the high accuracy level, 36 out of 164 instances of fraud are missed and incorrectly predicted as nonfraud. \n* The false-negative rate is substantial. \n* The intention of a fraud detection model is to minimize these false negatives.","acefa8c7":"***Logistic Regression***","fb09c669":"***Support Vector Machine***","bf6b6238":"***Decision Tree***","1a129268":"***Linear Discriminent Analysis***","2e4bf4dc":"# Model Selection","4ef94a16":">The Best model for Predicting in **Random Forest Model** with **99.96% Accuracy**","736fbb44":"***KNN***","c42104cc":"***Gaussian Naive Bayes***","bb28661d":"![image.png](attachment:c4985768-6fd7-4dec-a7ac-0f8dd50b3a88.png)","2e1c0015":"***Splitting data into training and testing data***","ea784077":"***Random Forest***"}}