{"cell_type":{"4cd83776":"code","e432b9e2":"code","be3d7fce":"code","a7a7076b":"code","8169071c":"code","228e0221":"code","661e912d":"code","732e6f90":"code","2255f636":"code","808a92fe":"code","49d593c4":"code","4027bc54":"code","14d8b386":"code","dcebf1c7":"code","9942f02c":"code","b8f615d7":"code","c57f4a60":"code","ff175e71":"code","a7a4f22b":"code","5b8cd562":"code","0d4196d3":"code","fce354a4":"code","a9879b60":"code","1fe1c7c2":"code","9edbcb15":"code","a3d9e64c":"code","26f4662e":"code","b2e2c45e":"markdown","9c8d4628":"markdown","9472771e":"markdown","60daecf2":"markdown","4739b643":"markdown","a84afbdc":"markdown","d4181d9f":"markdown","f09be89e":"markdown","0181b8e0":"markdown","efbcb462":"markdown","1513cdde":"markdown","cd8b4e4f":"markdown","4a26cece":"markdown","0a356f92":"markdown","8e3c81da":"markdown","8406744a":"markdown","a3aa080b":"markdown","ca21f5a3":"markdown","d0cc965c":"markdown","e67facff":"markdown","187e9fdc":"markdown","4aad2394":"markdown","51cba82e":"markdown","cb03209d":"markdown","952706eb":"markdown","cdbc9e2d":"markdown","b7b782ec":"markdown","94ca6e91":"markdown","e5874f83":"markdown","b4f0a973":"markdown","1c63bf1b":"markdown","d33ffcc3":"markdown","3ccca443":"markdown","6151a8a8":"markdown"},"source":{"4cd83776":"!pip install ..\/input\/fastai2\/fastprogress-0.2.3-py3-none-any.whl\n!pip install ..\/input\/fastai2\/fastcore-0.1.18-py3-none-any.whl\n!pip install ..\/input\/fastai2\/fastai2-0.0.17-py3-none-any.whl","e432b9e2":"from fastai2.basics import *\nfrom fastai2.callback.all import *\nfrom fastai2.vision.all import *","be3d7fce":"import os\nimport cv2\nimport PIL\nfrom PIL import Image as Img\nfrom PIL import ImageTk\nimport random\nimport openslide\nimport skimage.io\nimport skimage.color\nfrom skimage.color import rgb2hsv\nimport matplotlib\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image, display","a7a7076b":"np.random.seed(2)","8169071c":"train_df1 = pd.read_csv('..\/input\/prostate-train-dataset\/train_df1.csv', index_col=False)\n\ntrain_df2 = pd.read_csv('..\/input\/prostate-train-dataset\/train_df2.csv', index_col=False)\n\ntrain_df3 = pd.read_csv('..\/input\/prostate-train-dataset\/train_df3.csv', index_col=False)\n\ndata_dir = '..\/input\/prostate-cancer-grade-assessment\/train_images\/'","228e0221":"train_df = train_df1.append(train_df2, ignore_index=True)\ntrain_df = train_df.append(train_df3, ignore_index=True)","661e912d":"train_df = train_df.drop(columns=['Unnamed: 0'])","732e6f90":"train_df.head ()","2255f636":"train_df = train_df[0:4000]","808a92fe":"image = list(train_df['image_id'])\nlabels = list(train_df['isup_grade'])","49d593c4":"def enhance_image(slide_path, contrast=1, brightness=15):\n    image = skimage.io.MultiImage(slide_path)[-2]\n    image = np.array(image)\n    img_enhanced = cv2.addWeighted(image, contrast, image, 0, brightness)\n    return img_enhanced","4027bc54":"def compute_statistics(image):\n\n    width, height = image.shape[0], image.shape[1]\n    num_pixels = width * height\n    \n    num_white_pixels = 0\n    \n    summed_matrix = np.sum(image, axis=-1)\n    # Note: A 3-channel white pixel has RGB (255, 255, 255)\n    num_white_pixels = np.count_nonzero(summed_matrix > 620)\n    ratio_white_pixels = num_white_pixels \/ num_pixels\n    \n    green_concentration = np.mean(image[1])\n    blue_concentration = np.mean(image[2])\n    \n    return ratio_white_pixels, green_concentration, blue_concentration\n","14d8b386":"def select_k_best_regions(regions, k=20):\n\n    regions = [x for x in regions if x[3] > 180 and x[4] > 180]\n    k_best_regions = sorted(regions, key=lambda tup: tup[2])[:k]\n    return k_best_regions","dcebf1c7":"def get_k_best_regions(coordinates, image, window_size=512):\n    regions = {}\n    for i, tup in enumerate(coordinates):\n        x, y = tup[0], tup[1]\n        regions[i] = image[x : x+window_size, y : y+window_size, :]\n    \n    return regions","9942f02c":"def generate_patches(image, window_size=200, stride=128, k=20):\n        \n    max_width, max_height = image.shape[0], image.shape[1]\n    regions_container = []\n    i = 0\n    \n    while window_size + stride*i <= max_height:\n        j = 0\n        \n        while window_size + stride*j <= max_width:            \n            x_top_left_pixel = j * stride\n            y_top_left_pixel = i * stride\n            \n            patch = image[\n                x_top_left_pixel : x_top_left_pixel + window_size,\n                y_top_left_pixel : y_top_left_pixel + window_size,\n                :\n            ]\n            \n            ratio_white_pixels, green_concentration, blue_concentration = compute_statistics(patch)\n            \n            region_tuple = (x_top_left_pixel, y_top_left_pixel, ratio_white_pixels, green_concentration, blue_concentration)\n            regions_container.append(region_tuple)\n            \n            j += 1\n        \n        i += 1\n    \n    k_best_region_coordinates = select_k_best_regions(regions_container, k=k)\n    k_best_regions = get_k_best_regions(k_best_region_coordinates, image, window_size)\n    \n    return image, k_best_region_coordinates, k_best_regions","b8f615d7":"def glue_to_one_picture(image_patches, window_size=200, k=16):\n    side = int(np.sqrt(k))\n    image = np.zeros((side*window_size, side*window_size, 3), dtype=np.int16)\n        \n    for i, patch in image_patches.items():\n        x = i \/\/ side\n        y = i % side\n        image[\n            x * window_size : (x+1) * window_size,\n            y * window_size : (y+1) * window_size,\n            :\n        ] = patch\n    \n    return image","c57f4a60":"WINDOW_SIZE = 128\nSTRIDE = 64\nK = 16","ff175e71":"def get_i(image):\n    for i, img in enumerate(image):\n        url = data_dir + img + '.tiff'\n        enhanced_image = enhance_image (url)\n        image, best_coordinates, best_regions = generate_patches(enhanced_image, window_size=WINDOW_SIZE, stride=STRIDE, k=K)\n        glued_image = glue_to_one_picture(best_regions, window_size=WINDOW_SIZE, k=K)\n        glued_image = np.uint8(glued_image)\n        return tensor(glued_image)","a7a4f22b":"blocks = (\n          ImageBlock,\n          CategoryBlock\n          )    \ngetters = [\n           get_i,\n           ColReader('isup_grade')\n          ]\ntrends = DataBlock(blocks=blocks,\n              splitter=RandomSplitter(),\n              getters=getters,\n              item_tfms=Resize(512),\n              )","5b8cd562":"dls = trends.dataloaders(train_df, bs=16)","0d4196d3":"dls.show_batch()","fce354a4":"Path('\/root\/.cache\/torch\/checkpoints\/').mkdir(exist_ok=True, parents=True)\n!cp '..\/input\/resnet34\/resnet34.pth' '\/root\/.cache\/torch\/checkpoints\/resnet34-333f7ec4.pth'","a9879b60":"learn = cnn_learner(dls, resnet34, metrics=error_rate)","1fe1c7c2":"torch.cuda.is_available()","9edbcb15":"learn.unfreeze()","a3d9e64c":"learn.fit_one_cycle(3)","26f4662e":"learn.export ('test.pkl')","b2e2c45e":"# Introduction","9c8d4628":"The functions select_k_best_regions and get_k_best_regions list and select the lowest porportion of white pixels in a particular region (PAB97).","9472771e":"Internet is not allowed in this competition.  The files have to be loaded through the fastai2 dataset.","60daecf2":"The prostate is part of the male reproductive system.  The function of the prostate gland is to secrete substances to the urethra.  These secretions nurish and transport sperm.  Prostate cancer is diagnosed from samples from a prostate biopsy (rmicrobe).  The sample is first assigned as a gleason score.  This score is converted to a ISUP grade of 0-5.  The score of 0 is negative and the score of 5 is the most severe form of cancer (Prostate cANcer GraDe Assessment (PANDA) Challenge).\n\nThe purpose of this notebook is to create the appropriate training model and save it to a .pkl file to use to predict the testing data.  Since using the training model would take longer than six hours to run on the testing data, the .pkl file will be used in the competition notebook.","4739b643":"## Install fast.ai without Internet","a84afbdc":"1. Use the dataset for training from the notebook [Creating Training Dataset](http:\/\/www.kaggle.com\/rmicrobe\/creating-training-dataset) (rmicrobe).\n2. Remove gray area surrounding the biopsy. The first step involves removing the gray area from around the biopsy (Zenify).\n3. Create 4X4 patched image. The second step is to take 16 samples that have the lowest portion of white. This ensures that the sample is most likely going to show the appropriate part of the sample (i.e. glands). (PAB97).\n4. Use fast.ai and ResNet34 to train the data.\n5. Create the .pkl file.","d4181d9f":"Setting a random seed makes sure that all randomly picked sequences are in the same order.  It is important to keep the same number everytime to keep the same sequence everytime.","f09be89e":"## Import fast.ai","0181b8e0":"# Functions","efbcb462":"![image.png](attachment:image.png)","1513cdde":"# Prostate cANcer graDe Assessment (PANDA) Challenge: Creating the Training Dataset","cd8b4e4f":"The function compute_statistics calculates the portion of white pixels in the region (PAB97).","4a26cece":"Get a preview of the dataframe.","0a356f92":"The next step is to create two lists for training. Do not create a list longer than 4000 items long because a list longer than 4000 items may result in an error. In this notebook, a list length of 4000 will be used. This is the normal length.","8e3c81da":"# Set Random Seed","8406744a":"This part of the code prepares the images for training.  The images are properly labeled.  A batch of images are shown to make sure everything is working properly.","a3aa080b":"The enhance_image function removes the gray portion from around the prostate biopsy (Zenify).","ca21f5a3":"# Create .pkl File","d0cc965c":"PAB97. \u201cBetter image tiles - Removing white spaces.\u201d Kaggle, 22 May 2020, www.kaggle.com\/rftexas\/better-image-tiles-removing-white-spaces.\n\n\u201cProstate CANcer GraDe Assessment (PANDA) Challenge.\u201d Kaggle, www.kaggle.com\/c\/prostate-cancer-grade-assessment\/overview\/description.\n\nrmicrobe. \u201cCreating Training Dataset.\u201d Kaggle, 3 March 2021, www.kaggle.com\/rmicrobe\/creating-training-dataset.\n\nrmicrobe. \u201cMicroanatomy of the Prostate.\u201d Kaggle, 11 June 2020, www.kaggle.com\/rmicrobe\/microanatomy-of-the-prostate.\n\nZenify. \u201cLet's Enhance the Images!\u201d Kaggle, 03 May 2020, www.kaggle.com\/debanga\/let-s-enhance-the-images.","e67facff":"# Train the Model using ResNet34 and fast.ai","187e9fdc":"# Import fast.ai and Dependencies","4aad2394":"Import the four .csv file from the prostate train dataset directory. This will be the training dataframe for this part of the process.","51cba82e":"This creates a checkpoint directory for ResNet34 and copies the model to the directory.","cb03209d":"# Checkpoints Directory","952706eb":"The function generate_patches slides over the region to calculate the white pixels then calculates the statistics and then selects the region with the least amount of pixels (PAB97).","cdbc9e2d":"The function glue_to_one_picture glues the 16 patches into one 4X4 image (PAB97).","b7b782ec":"Append the four dataframes and get rid of the 'Unnamed:0' column.","94ca6e91":"## Image Processing, Training, and Creating the .pkl File ","e5874f83":"# Import the Training Dataframe","b4f0a973":"## Load Dependencies","1c63bf1b":"## Prostate Gland and Prostate Cancer","d33ffcc3":"The get_i function takes the original image of the biopsy and prepares the image for training.","3ccca443":"# Preparing for Training","6151a8a8":"# Works Cited"}}