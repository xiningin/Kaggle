{"cell_type":{"d1c32a91":"code","ab9cf8e8":"code","93281d9b":"code","b814fd7c":"code","43398875":"code","d9b89768":"code","d778c730":"code","0cf70fa9":"code","3f4d1cb1":"code","66abb2e4":"code","986d8c1a":"code","a1366218":"code","ff166a50":"code","1978c6aa":"code","8b5d2835":"code","0ad6d1e9":"code","1cecc2ef":"code","4f3b866a":"code","b31fe581":"code","5831d255":"code","cc4363a2":"code","d76a87d3":"code","6c001bc5":"code","e20397e8":"code","4cdd73d9":"code","f24a3293":"code","3a2c4633":"code","7bb5f6a6":"code","d2b77033":"code","f851eabf":"code","289f36b8":"code","2a391bff":"code","6bf425c2":"code","276e91dc":"code","18d0c90c":"markdown","68b03219":"markdown","06f67c0c":"markdown","d16f0198":"markdown","8c7a6131":"markdown","b811832d":"markdown","26de43fa":"markdown","c7debe59":"markdown","702cbfa2":"markdown","af37c1f7":"markdown","b2f5372f":"markdown","abba0a05":"markdown","3917cc0d":"markdown","67ac8466":"markdown","cb93943e":"markdown","d57655b0":"markdown","5a9bc037":"markdown","24a1f7bb":"markdown","66dd7e5d":"markdown","a00bfad0":"markdown","e5ab2891":"markdown","74e5f22c":"markdown","5854cc21":"markdown","61b1c351":"markdown","bc4661b4":"markdown","99126bd7":"markdown","183d6aa4":"markdown","acc22640":"markdown"},"source":{"d1c32a91":"import keras\nimport tensorflow\nfrom skimage import io\nimport os\nimport glob\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\n%matplotlib inline","ab9cf8e8":"# function to plot n images using subplots\ndef plot_image(images, captions=None, cmap=None ):\n    f, axes = plt.subplots(1, len(images), sharey=True)\n    f.set_figwidth(15)\n    for ax,image in zip(axes, images):\n        ax.imshow(image, cmap)","93281d9b":"# path to your dataset\n\nDATASET_PATH = '..\/input\/flowers-recognition\/flowers\/'\nflowers_cls = ['daisy', 'rose']","b814fd7c":"# globbing example\n# help(glob)\nflower_path = os.path.join(DATASET_PATH, flowers_cls[1], '*')\nprint(flower_path)\n\n# glob through the directory (returns a list of all file paths)\nflower_path = glob.glob(flower_path)\nprint(flower_path[3]) # access an individual file","43398875":"# run this block multiple times to look at some randomly chosen images of roses\nrand_index = random.randint(0, len(flower_path))\nimage = io.imread(flower_path[rand_index])\nplt.imshow(image)","d9b89768":"# plot a sample image\nflower_path = os.path.join(DATASET_PATH, flowers_cls[1], '*')\nflower_path = glob.glob(flower_path)\n\n# access some element (a file) from the list\n#image = io.imread(flower_path[729])\nimage = io.imread(flower_path[54])\nplt.imshow(image)","d778c730":"print(image.shape)","0cf70fa9":"# plotting the original image and the RGB channels\nf, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, sharey=True)\nf.set_figwidth(15)\nax1.imshow(image)\n\n# RGB channels\n# CHANNELID : 0 for Red, 1 for Green, 2 for Blue. \nax2.imshow(image[:, : , 0]) #Red\nax3.imshow(image[:, : , 1]) #Green\nax4.imshow(image[:, : , 2]) #Blue\nf.suptitle('Different Channels of Image')","3f4d1cb1":"# bin_image will be a (240, 320) True\/False array\n#The range of pixel varies between 0 to 255\n#The pixel having black is more close to 0 and pixel which is white is more close to 255\n# 125 is Arbitrary heuristic measure halfway between 1 and 255 (the range of image pixel) \nbin_image = image[:, :, 0] > 125\nplot_image([image, bin_image], cmap='gray')","66abb2e4":"from skimage.morphology import binary_closing, binary_dilation, binary_erosion, binary_opening\nfrom skimage.morphology import selem\n\n# use a disk of radius 3\nselem = selem.disk(3)\n\n# oprning and closing\nopen_img = binary_opening(bin_image, selem)\nclose_img = binary_closing(bin_image, selem)\n\n# erosion and dilation\neroded_img = binary_erosion(bin_image, selem)\ndilated_img = binary_dilation(bin_image, selem)\n\nplot_image([bin_image, open_img, close_img, eroded_img, dilated_img], cmap='gray')","986d8c1a":"#way1-this is common technique followed in case of RGB images \nnorm1_image = image\/255\n#way2-in case of medical Images\/non natural images \nnorm2_image = image - np.min(image)\/np.max(image) - np.min(image)\n#way3-in case of medical Images\/non natural images \nnorm3_image = image - np.percentile(image,5)\/ np.percentile(image,95) - np.percentile(image,5)\n\nplot_image([image, norm1_image, norm2_image, norm3_image], cmap='gray')","a1366218":"from skimage import transform as tf\n\n# flip left-right, up-down\nimage_flipr = np.fliplr(image)\nimage_flipud = np.flipud(image)\n\nplot_image([image, image_flipr, image_flipud])","ff166a50":"# specify x and y coordinates to be used for shifting (mid points)\nshift_x, shift_y = image.shape[0]\/2, image.shape[1]\/2\n\n# translation by certain units\nmatrix_to_topleft = tf.SimilarityTransform(translation=[-shift_x, -shift_y])\nmatrix_to_center = tf.SimilarityTransform(translation=[shift_x, shift_y])\n\n# rotation\nrot_transforms =  tf.AffineTransform(rotation=np.deg2rad(45))\nrot_matrix = matrix_to_topleft + rot_transforms + matrix_to_center\nrot_image = tf.warp(image, rot_matrix)\n\n# scaling \nscale_transforms = tf.AffineTransform(scale=(2, 2))\nscale_matrix = matrix_to_topleft + scale_transforms + matrix_to_center\nscale_image_zoom_out = tf.warp(image, scale_matrix)\n\nscale_transforms = tf.AffineTransform(scale=(0.5, 0.5))\nscale_matrix = matrix_to_topleft + scale_transforms + matrix_to_center\nscale_image_zoom_in = tf.warp(image, scale_matrix)\n\n# translation\ntransaltion_transforms = tf.AffineTransform(translation=(50, 50))\ntranslated_image = tf.warp(image, transaltion_transforms)\n\n\nplot_image([image, rot_image, scale_image_zoom_out, scale_image_zoom_in, translated_image])","1978c6aa":"# shear transforms\nshear_transforms = tf.AffineTransform(shear=np.deg2rad(45))\nshear_matrix = matrix_to_topleft + shear_transforms + matrix_to_center\nshear_image = tf.warp(image, shear_matrix)\n\nbright_jitter = image*0.999 + np.zeros_like(image)*0.001\n\nplot_image([image, shear_image, bright_jitter])","8b5d2835":"# import module we'll need to import our custom module\nfrom shutil import copyfile\n\n# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"..\/input\/resnet\/resnet.py\", dst = \"..\/working\/resnet.py\")\n","0ad6d1e9":"# import all our functions\nfrom resnet import *","1cecc2ef":"import resnet\n\n# specify image size and channels\nimg_channels = 3\nimg_rows = 100\nimg_cols = 100\n\n# number of classes\nnb_classes = 2","4f3b866a":"import numpy as np\nimport keras\n\nclass DataGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    \n    def __init__(self, mode='train', ablation=None, flowers_cls=['daisy', 'rose'], \n                 batch_size=32, dim=(100, 100), n_channels=3, shuffle=True):\n        \"\"\"\n        Initialise the data generator\n        \"\"\"\n        self.dim = dim\n        self.batch_size = batch_size\n        self.labels = {}\n        self.list_IDs = []\n        \n        # glob through directory of each class \n        for i, cls in enumerate(flowers_cls):\n            paths = glob.glob(os.path.join(DATASET_PATH, cls, '*'))\n            brk_point = int(len(paths)*0.8)\n            if mode == 'train':\n                paths = paths[:brk_point]\n            else:\n                paths = paths[brk_point:]\n            if ablation is not None:\n                paths = paths[:ablation]\n            self.list_IDs += paths\n            self.labels.update({p:i for p in paths})\n            \n        self.n_channels = n_channels\n        self.n_classes = len(flowers_cls)\n        self.shuffle = shuffle\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) \/ self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n\n        # Generate data\n        X, y = self.__data_generation(list_IDs_temp)\n\n        return X, y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n        y = np.empty((self.batch_size), dtype=int)\n        \n        delete_rows = []\n\n        # Generate data\n        for i, ID in enumerate(list_IDs_temp):\n            # Store sample\n            img = io.imread(ID)\n            img = img\/255\n            if img.shape[0] > 100 and img.shape[1] > 100:\n                h, w, _ = img.shape\n                img = img[int(h\/2)-50:int(h\/2)+50, int(w\/2)-50:int(w\/2)+50, : ]\n            else:\n                delete_rows.append(i)\n                continue\n            \n            X[i,] = img\n          \n            # Store class\n            y[i] = self.labels[ID]\n        \n        X = np.delete(X, delete_rows, axis=0)\n        y = np.delete(y, delete_rows, axis=0)\n        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)","b31fe581":"# using resnet 18\nmodel = resnet.ResnetBuilder.build_resnet_18((img_channels, img_rows, img_cols), nb_classes)\nmodel.compile(loss='categorical_crossentropy', optimizer='SGD',\n              metrics=['accuracy'])\n\n# create data generator objects in train and val mode\n# specify ablation=number of data points to train on\ntraining_generator = DataGenerator('train', ablation=100)\nvalidation_generator = DataGenerator('val', ablation=100)\n\n# fit: this will fit the net on 'ablation' samples, only 1 epoch\nmodel.fit_generator(generator=training_generator,\n                    validation_data=validation_generator,\n                    epochs=1,)","5831d255":"# resnet 18\nmodel = resnet.ResnetBuilder.build_resnet_18((img_channels, img_rows, img_cols), nb_classes)\nmodel.compile(loss='categorical_crossentropy',optimizer='SGD',\n              metrics=['accuracy'])\n\n# generators\ntraining_generator = DataGenerator('train', ablation=100)\nvalidation_generator = DataGenerator('val', ablation=100)\n\n# fit\nmodel.fit_generator(generator=training_generator,\n                    validation_data=validation_generator,\n                    epochs=20)","cc4363a2":"# generic way to create custom callback\nclass LossHistory(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = []\n\n    def on_batch_end(self, batch, logs={}):\n        self.losses.append(logs.get('loss'))","d76a87d3":"from keras import optimizers\nfrom keras.callbacks import *\n\n# range of learning rates to tune\nhyper_parameters_for_lr = [0.1, 0.01, 0.001]\n\n# callback to append loss\nclass LossHistory(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = []\n\n    def on_epoch_end(self, epoch, logs={}):\n        self.losses.append(logs.get('loss'))\n\n# instantiate a LossHistory() object to store histories\nhistory = LossHistory()\nplot_data = {}\n\n# for each hyperparam: train the model and plot loss history\nfor lr in hyper_parameters_for_lr:\n    print ('\\n\\n'+'=='*20 + '   Checking for LR={}  '.format(lr) + '=='*20 )\n    sgd = optimizers.SGD(lr=lr, clipnorm=1.)\n    \n    # model and generators\n    model = resnet.ResnetBuilder.build_resnet_18((img_channels, img_rows, img_cols), nb_classes)\n    model.compile(loss='categorical_crossentropy',optimizer= sgd,\n                  metrics=['accuracy'])\n    training_generator = DataGenerator('train', ablation=100)\n    validation_generator = DataGenerator('val', ablation=100)\n    model.fit_generator(generator=training_generator,\n                        validation_data=validation_generator,\n                        epochs=3, callbacks=[history])\n    \n    # plot loss history\n    plot_data[lr] = history.losses","6c001bc5":"# plot loss history for each value of hyperparameter\nf, axes = plt.subplots(1, 3, sharey=True)\nf.set_figwidth(15)\n\nplt.setp(axes, xticks=np.arange(0, len(plot_data[0.01]), 1)+1)\n\nfor i, lr in enumerate(plot_data.keys()):\n    axes[i].plot(np.arange(len(plot_data[lr]))+1, plot_data[lr])","e20397e8":"# learning rate decay\nclass DecayLR(keras.callbacks.Callback):\n    def __init__(self, base_lr=0.001, decay_epoch=1):\n        super(DecayLR, self).__init__()\n        self.base_lr = base_lr\n        self.decay_epoch = decay_epoch \n        self.lr_history = []\n        \n    # set lr on_train_begin\n    def on_train_begin(self, logs={}):\n        K.set_value(self.model.optimizer.lr, self.base_lr)\n\n    # change learning rate at the end of epoch\n    def on_epoch_end(self, epoch, logs={}):\n        new_lr = self.base_lr * (0.5 ** (epoch \/\/ self.decay_epoch))\n        self.lr_history.append(K.get_value(self.model.optimizer.lr))\n        K.set_value(self.model.optimizer.lr, new_lr)\n\n# to store loss history\nhistory = LossHistory()\nplot_data = {}\n\n# start with lr=0.1\ndecay = DecayLR(base_lr=0.1)\n\n# model\nsgd = optimizers.SGD()\nmodel = resnet.ResnetBuilder.build_resnet_18((img_channels, img_rows, img_cols), nb_classes)\nmodel.compile(loss='categorical_crossentropy',optimizer= sgd,\n              metrics=['accuracy'])\ntraining_generator = DataGenerator('train', ablation=100)\nvalidation_generator = DataGenerator('val', ablation=100)\n\nmodel.fit_generator(generator=training_generator,\n                    validation_data=validation_generator,\n                    epochs=3, callbacks=[history, decay])\n\nplot_data[lr] = decay.lr_history","4cdd73d9":"plt.plot(np.arange(len(decay.lr_history)), decay.lr_history)","f24a3293":"# keras data generator\n# help(ImageDataGenerator)\n\nfrom keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator(\n    featurewise_center=True,\n    featurewise_std_normalization=True,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True)","3a2c4633":"import numpy as np\nimport keras\n\n# data generator with augmentation\nclass AugmentedDataGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, mode='train', ablation=None, flowers_cls=['daisy', 'rose'], \n                 batch_size=32, dim=(100, 100), n_channels=3, shuffle=True):\n        'Initialization'\n        self.dim = dim\n        self.batch_size = batch_size\n        self.labels = {}\n        self.list_IDs = []\n        self.mode = mode\n        \n        for i, cls in enumerate(flowers_cls):\n            paths = glob.glob(os.path.join(DATASET_PATH, cls, '*'))\n            brk_point = int(len(paths)*0.8)\n            if self.mode == 'train':\n                paths = paths[:brk_point]\n            else:\n                paths = paths[brk_point:]\n            if ablation is not None:\n                paths = paths[:ablation]\n            self.list_IDs += paths\n            self.labels.update({p:i for p in paths})\n        \n            \n        self.n_channels = n_channels\n        self.n_classes = len(flowers_cls)\n        self.shuffle = shuffle\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) \/ self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n\n        # Generate data\n        X, y = self.__data_generation(list_IDs_temp)\n\n        return X, y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n        y = np.empty((self.batch_size), dtype=int)\n        \n        delete_rows = []\n\n        # Generate data\n        for i, ID in enumerate(list_IDs_temp):\n            # Store sample\n            img = io.imread(ID)\n            img = img\/255\n            if img.shape[0] > 100 and img.shape[1] > 100:\n                h, w, _ = img.shape\n                img = img[int(h\/2)-50:int(h\/2)+50, int(w\/2)-50:int(w\/2)+50, : ]\n            else:\n                delete_rows.append(i)\n                continue\n            \n            X[i,] = img\n          \n            # Store class\n            y[i] = self.labels[ID]\n        \n        X = np.delete(X, delete_rows, axis=0)\n        y = np.delete(y, delete_rows, axis=0)\n        \n        # data augmentation\n        if self.mode == 'train':\n            aug_x = np.stack([datagen.random_transform(img) for img in X])\n            X = np.concatenate([X, aug_x])\n            y = np.concatenate([y, y])\n        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)\n        ","7bb5f6a6":"from sklearn.metrics import roc_auc_score\n\nclass roc_callback(Callback):\n    \n    def on_train_begin(self, logs={}):\n        logs['val_auc'] = 0\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_p = []\n        y_v = []\n        for i in range(len(validation_generator)):\n            x_val, y_val = validation_generator[i]\n            y_pred = self.model.predict(x_val)\n            y_p.append(y_pred)\n            y_v.append(y_val)\n        y_p = np.concatenate(y_p)\n        y_v = np.concatenate(y_v)\n        roc_auc = roc_auc_score(y_v, y_p)\n        print ('\\nVal AUC for epoch{}: {}'.format(epoch, roc_auc))\n        logs['val_auc'] = roc_auc\n","d2b77033":"# model\nmodel = resnet.ResnetBuilder.build_resnet_18((img_channels, img_rows, img_cols), nb_classes)\nmodel.compile(loss='categorical_crossentropy',optimizer= sgd,\n              metrics=['accuracy'])\ntraining_generator = AugmentedDataGenerator('train', ablation=32)\nvalidation_generator = AugmentedDataGenerator('val', ablation=32)\n\n# checkpoint \nfilepath = 'models\/best_model.hdf5'\ncheckpoint = ModelCheckpoint(filepath, monitor='val_auc', verbose=1, save_best_only=True, mode='max')\nauc_logger = roc_callback()\n\n# fit \nmodel.fit_generator(generator=training_generator,\n                    validation_data=validation_generator,\n                    epochs=3, callbacks=[auc_logger, history, decay, checkpoint])","f851eabf":"plt.imshow(image)","289f36b8":"h, w, _ = image.shape\nimg = image[int(h\/2)-50:int(h\/2)+50, int(w\/2)-50:int(w\/2)+50, : ]\n\nmodel.predict(img[np.newaxis,: ])","2a391bff":"def _compute_grads(model, input_array):\n    grads_fn = K.gradients(model.output, model.input)[0]\n    compute_fn = K.function([model.input, K.learning_phase()], [grads_fn])\n    return compute_fn([np.array([input_array]), 0])[0][0]","6bf425c2":"plt.imshow(img)","276e91dc":"grad = _compute_grads(model, img)\ngrad_normed = (grad - np.min(grad))\/(np.max(grad) - np.min(grad))\ngrad_normed *= 255\nplt.imshow(grad_normed.astype('uint8'))","18d0c90c":"In this notebook, we will go through the end-to-end pipeline of training conv nets, i.e. organising the data into directories, preprocessing, data augmentation, model building etc.  ","68b03219":"We'll now use the `glob` module of python to <a href=\"https:\/\/en.wikipedia.org\/wiki\/Glob_(programming)\">glob<\/a> through the directory where the data is stored, i.e. to walk through the directory, subdirectories and the files. It uses regular expressions to access files having names matching some pattern. In our case, we want to access all the files in the path `flowers\/rose\/` and `flowers\/daisy\/`, so we'll just use the regex `*` (used as a 'wildcard' to catch everything). \n\nAn example of how the glob module works is given below - you first join the base directory path with the subdirectory (e.g. `flowers\/rose\/`) and then `glob` through it to access all the individual files (images here).","06f67c0c":"### Understanding images and channels\n\nAs these images are RGB images they would constitute three channels - one for each of the color channels","d16f0198":"### 1.Thresholding\n\nOne of the simpler operations where we take all the pixels whose intensities are above a certain threshold, and convert them to ones; the pixels having value less than the threshold are converted to zero. This results in a *binary image*.","8c7a6131":"If you want to implement your own customized data generator (with augmentation), you can add the augmentation step easily to the `DataGenerator` class created above. The only change is that we stack the augmented images to the X, y arrays (as done in the last section of the code below).","b811832d":"##  Image Augmentation & Preprocessing\n\nIn the following section, we'll look at some common image preprocessing techniques.","26de43fa":"### Metrics to optimise\n\nAUC is often a better metric than accuracy. So instead of optimising for accuracy, let's monitor AUC and choose the best model based on AUC on validaton data. We'll use the callbacks `on_train_begin` and `on_epoch_end` to initialise  (at the start of each epoch) and store the AUC (at the end of epoch).","c7debe59":"### 2.Erosion, Dilation, Opening & Closing","702cbfa2":"## Normalisation ","af37c1f7":"In the code below, we have created a custom callback to append the loss to a list at the end of every epoch. Note that `logs` is an attribute (a dictionary) of `keras.callbacks.Callback`, and we are using it to get the value of the key 'loss'. Some other keys of this dict are `acc`, `val_loss` etc.\n\nTo tell the model that we want to use a callback, we create an object of `LossHistory` called `history` and pass it to `model.fit_generator` using `callbacks=[history]`. In this case, we only have one callback `history`, though you can pass multiple callback objects through this list (an example of multiple callbacks is in the section below - see the code block of `DecayLR()`).\n\nWe highly recommend you to <a href=\"https:\/\/keras.io\/callbacks\/\">read the documentation of keras callbacks here<\/a>. For a gentler introduction to callbacks, you can read this <a href=\"https:\/\/machinelearningmastery.com\/check-point-deep-learning-models-keras\/\"> nice blog post by Jason Brownlee.<\/a>","b2f5372f":"**Erosion** shrinks bright regions and enlarges dark regions. **Dilation** on the other hand is exact opposite side - it shrinks dark regions and enlarges the bright regions. \n\n**Opening** is erosion followed by dilation. Opening can remove small bright spots (i.e. \u201csalt\u201d) and connect small dark cracks. This tends to \u201copen\u201d up (dark) gaps between (bright) features.\n\n**Closing** is dilation followed by erosion. Closing can remove small dark spots (i.e. \u201cpepper\u201d) and connect small bright cracks. This tends to \u201cclose\u201d up (dark) gaps between (bright) features.\n\nAll these can be done using the `skimage.morphology` module. The basic idea is to have a **circular disk** of a certain size (3 below) move around the image and apply these transformations using it.","abba0a05":"## **Data Preprocessing**\n<font size=\"4\">**Morphological Transformations:**<\/font>  This refers to changing the shape and size of images. The typical transformations are erosion, dilation, opening and closing. \n\n \n\n<font size=\"4\">**Augmentation:**<\/font>  Refers to making changes related to rotation, translation, shearing, etc. Augmentation is often used in image-based deep learning tasks to increase the amount and variance of training data. Augmentation should only be done on the training set, never on the validation set.\n\n \n\n<font size=\"4\">**Normalisation:**<\/font>  Refers to rescaling the pixel values so that they lie within a confined range. One of the reasons to do this is to help with the issue of propagating gradients.","3917cc0d":"## Morphological Transformations","67ac8466":"# Network Building\n\nLet's now build the network. We'll import the resnet architecture from the module `resnet.py`.","cb93943e":"The results above show that a learning rate of 0.1 is the best, though using such a high learning rate for the entire training is usually not a good idea. Thus, we should use **learning rate decay** - starting from a high learning rate and decaying it with every epoch.\n\nWe use another **custom callback** (`DecayLR`) to decay the learning rate at the end of every epoch. The decay rate is specified as 0.5 ^ epoch. Also, note that this time we are telling the model to **use two callbacks** (passed as a list `callbacks=[history, decay]` to `model.fit_generator`).\n\n\nAlthough we have used out own custom decay implementation here, you can use the ones built into <a href=\"https:\/\/keras.io\/optimizers\/\">keras optimisers<\/a> (using the `decay` argument).","d57655b0":"### Data Generator ###\n\nLet's now set up the **data generator**. The code below sets up a custom data generator which is slightly different than <a href=\"https:\/\/keras.io\/preprocessing\/image\/\">the one that comes with the keras API<\/a>. The reason to use a custom generator is to be able to modify it according to the problem at hand (customizability). \n\n\nWe won't be going through the entire code step-by-step in the lectures, though the code is explained below.\n\nTo start with, we have the training data stored in $n$ directories (if there are $n$ classes). For a given batch size, we want to generate batches of data points and feed them to the model.\n\n\nThe first `for` loop 'globs' through each of the classes (directories). For each class, it stores the path of each image in the list `paths`. In training mode, it subsets `paths` to contain the first 80% images; in validation mode it subsets the last 20%. In the special case of an ablation experiment, it simply subsets the first `ablation` images of each class.\n\nWe store the paths of all the images (of all classes) in a combined list `self.list_IDs`. The dictionary `self.labels` contains the labels (as key:value pairs of `path: class_number (0\/1)`).\n\nAfter the loop, we call the method `on_epoch_end()`, which creates an array `self.indexes` of length `self.list_IDs` and shuffles them (to shuffle all the data points at the end of each epoch).\n\nThe `_getitem_` method uses the (shuffled) array `self.indexes` to select a `batch_size` number of entries (paths) from the path list `self.list_IDs`. \n\nFinally, the method `__data_generation` returns the batch of images as the pair X, y where X is of shape `(batch_size, height, width, channels)` and y is of shape `(batch size, )`. Note that `__data_generation` also does some preprocessing - it normalises the images (divides by 255) and crops the center 100 x 100 portion of the image. Thus, each image has the shape `(100, 100, num_channels)`. If any dimension (height or width) of an image less than 100 pixels, that image is deleted.","5a9bc037":"### Running Ablation Experiments \n\nBefore training the net on the entire dataset, you should always try to first run some experiments to check whether the net is fitting on a small dataset or not. \n\n#### Checking that the network is 'working'\n\nThe first part of building a network is to get it to run on your dataset. Let's try fitting the net on only a few images and just one epoch. Note that since `ablation=100` is specified, 100 images of each class are used, so total number of batches is `np.floor(200\/32)` = 6. \n\nNote that the `DataGenerator`  class 'inherits' from the `keras.utils.Sequence` class, so it has all the functionalities of the base `keras.utils.Sequence` class (such as the `model.fit_generator` method).","24a1f7bb":"## Final Run\n\nLet's now train the final model. Note that we will keep saving the best model's weights at `models\/best_models.hdf5`, so you will need to create a directory `models`. Note that model weights are usually saved in hdf5 files.\n\n**Saving the best model** is done using the callback functionality that comes with `ModelCheckpoint`. We basically specify the `filepath` where the model weights are to be saved, ` monitor='val_auc'` specifies that you are choosing the best model based on validation accuracy, `save_best_only=True` saves only the best weights, and `mode='max'` specifies that the validation accuracy is to be maximised.","66dd7e5d":"### Overfitting on the Training Data\n\nLet's now perform another important step which should be done before training the full-fledged model-  trying to **deliberately overfit the model** on a small dataset.\n\nWe'll use ablation=100 (i.e. training on 100 images of each class), so it is still a very small dataset, and we will use 20 epochs. In each epoch, 200\/32=6 batches will be used.","a00bfad0":"Normalisation is the most crucial step in the pre-processing part. There are multiple ways to normalise images which we will be talking about.","e5ab2891":"### Hyperparameter Tuning\n\nFirst let's make a list the hyper-parameters we want to tune:\n\n1. Learning Rate & Variation + Optimisers \n2. Augmentation Techniques\n\nThe basic idea is to track the validation loss with increasing epochs for various values of a hyperparameter. \n\n#### Keras Callbacks ####\n\nBefore you move ahead, let's discuss a bit about **callbacks**. Callbacks are basically actions that you want to perform at specific instances of training. For example, we want to perform the action of storing the loss at the end of every epoch (the instance here is the end of an epoch).\n\n\nFormally, a callback is simply a function (if you want to perform a single action), or a list of functions (if you want to perform multiple actions), which are to be executed at specific events (end of an epoch, start of every batch, when the accuracy plateaus out, etc.). Keras provides some very useful callback functionalities through the class `keras.callbacks.Callback`. \n\nKeras has many builtin callbacks (<a href=\"https:\/\/keras.io\/callbacks\/\">listed here<\/a>). The generic way to **create a custom callback in keras** is:","74e5f22c":"The results show that the training accuracy increases consistently with each epoch. The validation accuracy also increases and then plateaus out - this is a sign of 'good fit', i.e. we know that the model is at least able to learn from a small dataset, so we can hope that it will be able to learn from the entire set as well.","5854cc21":"## Augmentations","61b1c351":"Many times, the quantity of data that we have is not sufficient to perform the task of classification well enough. In such cases, we perform data augmentation.\nThere are multiple types of augmentations possible. The basic ones transform the original image using one of the following types of transformations:\n\n1. Linear transformations\n2. Affine transformations","bc4661b4":"## Visualising decisions","99126bd7":"## **Network Building**\n\n<font size=\"4\">**Choosing the architecture:**<\/font>  For this demo, we used the 'ResNet' architecture. Its biggest upside is that the 'skip connections' mechanism allows very deep networks.\n\n \n\n<font size=\"4\">**Ablation Experiments:**<\/font>  These refer to taking a small chunk of data and running your model on it - this helps in figuring out if the model is running at all.\n\n \n\n<font size=\"4\">**Overfitting on Training Data:**<\/font>  This tells you whether the model is behaving as expected or not.\n\n \n\n<font size=\"4\">**Metrics:**<\/font>  Depending on the situation, we choose the appropriate metrics. For binary classification problems, AUC is usually the best metric.\n\n \n\n<font size=\"4\">**Hyperparameter tuning:**<\/font>  We tune hyperparameters such as the learning rate, augmentation of images, batch size, etc. Also, we only change the architecture of the network if we have already tried tuning all other hyperparameters.","183d6aa4":"### Importing Necesseties","acc22640":"## Augmentation Techniques\n\nLet's now write some code to implement data augmentation. Augmentation is usually done with data generators, i.e. the augmented data is generated batch-wise, on the fly. \n\nYou can either use the built-in keras `ImageDataGenerator` or write your own data generator (for some custom features etc if you want). The two cells below show how to implement these respectively. "}}