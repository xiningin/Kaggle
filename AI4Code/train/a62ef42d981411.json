{"cell_type":{"97d9e83a":"code","18c077bc":"code","7743db86":"code","22b8ea56":"code","933b7914":"code","1ea9b796":"code","0e71eb1c":"code","45e06064":"code","191ec885":"code","b39b118c":"code","407eee65":"code","bd3855c2":"code","68e7eafe":"code","99b08b90":"code","8e036849":"code","32ce71cc":"code","a5f11ad8":"code","0fc9d024":"code","ff1346f9":"code","20a07e0c":"code","aecf11d3":"code","95cdf3ca":"code","1c81b156":"code","4d81ffc2":"code","2d752261":"code","853cf766":"code","710c27dc":"code","01006c36":"code","9b23633f":"code","a99d2d4d":"code","d1e70f8e":"code","35ff1f53":"code","93ab0612":"code","2837ae38":"code","800092f1":"code","d0002a79":"code","3f164294":"code","67d65572":"code","c17a8215":"code","d317029d":"code","b9d62933":"code","e503e935":"code","1bbb90e5":"code","4d7723e3":"code","f44bd530":"code","928d4293":"code","66ad35b6":"code","ee2e9bba":"code","b3286954":"code","1c9e6253":"code","630c19a8":"code","6161f8fa":"code","12d6fa1c":"code","10906c32":"code","ccc8f75a":"code","40e1c265":"code","18796fa5":"code","1b89029f":"code","0be7c801":"code","94172ba9":"code","8f054c0a":"code","7e727eaa":"code","8ddfdace":"code","a7ee8b9b":"code","b9ad8c09":"code","3e211332":"code","94b43b40":"code","63f82bec":"code","2ea6dcb2":"code","ff8f6c6d":"code","4b5fc21b":"markdown","e2a35f24":"markdown","a844abf4":"markdown","8885b116":"markdown","1abcbde0":"markdown","db111c75":"markdown","9d42a1c2":"markdown","c5af24a1":"markdown","32f0ab12":"markdown","53cce4f6":"markdown","f52ca783":"markdown","c867343a":"markdown","e567434b":"markdown","b36f323e":"markdown","af3bb3be":"markdown","3d834c53":"markdown","b7e80401":"markdown","02cc4d0c":"markdown","06f9a232":"markdown","0815425c":"markdown","30b5533f":"markdown","d7f6bfce":"markdown","4a0eaa20":"markdown","1e7549d7":"markdown","fae7aef6":"markdown","46d33ac3":"markdown","fe0b0820":"markdown","bb04f4fc":"markdown","38e8eb4d":"markdown","39587b68":"markdown","5e9ea818":"markdown","f783f41c":"markdown","8e167ca4":"markdown","05371a56":"markdown","5c8b37da":"markdown","bdea3648":"markdown","bce657a7":"markdown","5e4b249c":"markdown","a895e3e2":"markdown","d0364d3d":"markdown","265ffcb0":"markdown","aa435310":"markdown"},"source":{"97d9e83a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cufflinks as cf\nimport plotly \nimport plotly.express as px\nimport seaborn as sns\nfrom math import pi\n\nfrom IPython.core.display import HTML\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom pandas import DataFrame\nfrom colorama import Fore, Back, Style\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nsr_ = Style.RESET_ALL\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","18c077bc":"df = pd.read_csv('\/kaggle\/input\/source-based-news-classification\/news_articles.csv')\ndf.head()","7743db86":"print(\"Number of rows present in the dataset are: \", df.shape[0])\nprint(\"Number of columns present in the dataset are: \", df.shape[1])","22b8ea56":"df['type'].unique()","933b7914":"def msv_1(data, thresh = 20, color = 'black', edgecolor = 'black', height = 3, width = 15):\n    \n    plt.figure(figsize = (width, height))\n    percentage = (data.isnull().mean()) * 100\n    percentage.sort_values(ascending = False).plot.bar(color = color, edgecolor = edgecolor)\n    plt.axhline(y = thresh, color = 'r', linestyle = '-')\n    \n    plt.title('Missing values percentage per column', fontsize=20, weight='bold' )\n    \n    plt.text(len(data.isnull().sum()\/len(data))\/1.7, thresh+0.5, f'Columns with more than {thresh}% missing values', fontsize=12, color='crimson',\n         ha='left' ,va='top')\n    plt.text(len(data.isnull().sum()\/len(data))\/1.7, thresh - 0.5, f'Columns with less than {thresh}% missing values', fontsize=12, color='green',\n         ha='left' ,va='top')\n    plt.xlabel('Columns', size=15, weight='bold')\n    plt.ylabel('Missing values percentage')\n    plt.yticks(weight ='bold')\n    \n    return plt.show()\nmsv_1(df, thresh = 5, color=sns.color_palette('Reds',15))","1ea9b796":"df_orig = df.copy()\ndf.dropna(inplace = True)","0e71eb1c":"msv_1(df, thresh = 2, color=sns.color_palette('Reds',15))","45e06064":"from bokeh.io import output_notebook\nfrom bokeh.io import show\nfrom bokeh.plotting import figure\nfrom bokeh.transform import cumsum\nfrom bokeh.palettes import Spectral6\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.layouts import gridplot\nfrom bokeh.io import curdoc","191ec885":"output_notebook()","b39b118c":"def get_top_n_words(corpus, n = None):\n    \"\"\"\n    A function that returns the top 'n' unigrams used in the corpus\n    \"\"\"\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus) ## Shape: (2045, 46774) -> There are 2045 sentences and 46774 words\n    sum_words = bag_of_words.sum(axis=0) ## Shape: (1, 46774) -> Count of occurance of each word\n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()] ## vec.vocabulary_.items returns the dictionary with (word, index)\n    freq_sorted = sorted(words_freq, key = lambda x: x[1], reverse = True)\n    return freq_sorted[:n]\n\ndef get_top_n_bigram(corpus, n = None):\n    \"\"\"\n    A function that returns the top 'n' bigrams used in the corpus\n    \"\"\"\n    vec = CountVectorizer(ngram_range = (2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis = 0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    freq_sorted = sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return freq_sorted[:n]","407eee65":"top_unigram = get_top_n_words(df['text_without_stopwords'], 20)\nwords = [i[0] for i in top_unigram]\ncount = [i[1] for i in top_unigram]\nsource = ColumnDataSource(data = dict(Word = words, counts = count, color = ['#6baed6'] * 20))\n\np = figure(x_range = words, plot_height = 400, plot_width = 800, title = \"Top Unigrams\", tools = \"hover\", tooltips = \"@Word: @counts\")\np.vbar(x = 'Word', top = 'counts', width = 0.8, source = source, color = 'color')\ncurdoc().theme = 'dark_minimal'\np.xgrid.grid_line_color = None\np.y_range.start = 0\np.title.align = 'center'\np.xaxis.major_label_orientation = \"vertical\"\nshow(p)","bd3855c2":"top_bigram = get_top_n_bigram(df['text_without_stopwords'], 20)\nwords = [i[0] for i in top_bigram]\ncount = [i[1] for i in top_bigram]\nsource = ColumnDataSource(data = dict(Word = words, counts = count, color = ['#a1dab4'] * 20))\n\np1 = figure(x_range = words, plot_height = 400, plot_width = 800, title = \"Top Bigrams\", tools = \"hover\", tooltips = \"@Word: @counts\")\np1.vbar(x = 'Word', top = 'counts', width = 0.8, source = source, color = 'color')\n# curdoc().theme = 'dark_minimal'\np1.xgrid.grid_line_color = None\np1.title.align = 'center'\np1.y_range.start = 0\np1.xaxis.major_label_orientation = \"vertical\"\nshow(p1)","68e7eafe":"wc = WordCloud(background_color=\"black\", max_words=100,\n               max_font_size=256,\n               random_state=42, width=1000, height=1000)\nwc.generate(' '.join(df['text_without_stopwords']))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","99b08b90":"different_types = df['type'].value_counts().keys().to_list()\ncount = df['type'].value_counts().values\ncount1 = count \/ sum(count) * 100\nangle = count \/ sum(count) * 2 * pi\n\nsource = ColumnDataSource(data = dict(types = different_types, count = count, color = ['skyblue', 'salmon', 'turquoise', 'cyan', 'red', 'lightseagreen', 'teal', 'mediumaquamarine'], angle = angle, percentage = count1))\n\np3 = figure(plot_height = 400, plot_width = 400, title=\"Proportion of Article Types\",\n           tools = \"hover\", tooltips = \"@types: @percentage\", x_range=(-1.0, 1.0))  ## Pie chart for different types\n\np3.wedge(x = 0, y = 1, radius = 0.8,\n        start_angle = cumsum('angle', include_zero=True), end_angle = cumsum('angle'),\n        line_color = \"white\", fill_color = 'color', legend_field = 'types', source = source)\np3.title.align = 'center'\np3.legend.location = \"top_right\"\n\np3.legend.label_text_font_size = '5pt'\n\nshow(p3)","8e036849":"def top_bigrams_type(type_name, color_hex):\n    \"\"\"\n    A function that plots the bar graph representing the top unigrams for 'type_name'\n    \"\"\"\n    df_type = df[df['type'] == type_name]\n    top = get_top_n_bigram(df_type['text_without_stopwords'], 20)\n    words = [i[0] for i in top]\n    count = [i[1] for i in top]\n    source = ColumnDataSource(data = dict(Word = words, counts = count, color = [color_hex] * 20))\n\n    p = figure(x_range = words, plot_height = 400, plot_width = 800, title = \"Top Bigrams for \" + type_name, tools = \"hover\", tooltips = \"@Word: @counts\")\n    p.vbar(x = 'Word', top = 'counts', width = 0.8, source = source, color = 'color')\n    curdoc().theme = 'dark_minimal'\n    p.xgrid.grid_line_color = None\n    p.title.align = 'center'\n    p.y_range.start = 0\n    p.xaxis.major_label_orientation = \"vertical\"\n    show(p)","32ce71cc":"top_bigrams_type('bias', '#6baed6')","a5f11ad8":"top_bigrams_type('fake', '#32a852')","0fc9d024":"top_bigrams_type('state', '#a83257')","ff1346f9":"top_bigrams_type('hate', '#9aa832')","20a07e0c":"top_bigrams_type('bs', '#11d9bb')","aecf11d3":"different_languages = df['language'].value_counts().keys().to_list()\ncount = df['language'].value_counts().values\ncount1 = count \/ sum(count) * 100\nangle = count \/ sum(count) * 2 * pi\n\nsource = ColumnDataSource(data = dict(types = different_languages, count = count, color = ['skyblue', 'salmon', 'turquoise', 'red', 'lightseagreen'], angle = angle, percentage = count1))\n\np4 = figure(plot_height = 400, plot_width = 400, title = \"Proportion of Article Languages\",\n           tools = \"hover\", tooltips = \"@types: @percentage\", x_range = (-1.0, 1.0))  ## Pie chart for different article languages\n\np4.wedge(x = 0, y = 1, radius = 0.8,\n        start_angle = cumsum('angle', include_zero = True), end_angle = cumsum('angle'),\n        line_color = \"white\", fill_color = 'color', legend_field = 'types', source = source)\np4.title.align = 'center'\n\np4.legend.location = \"top_right\"\n\np4.legend.label_text_font_size = '5pt'\n\nshow(p4)","95cdf3ca":"different_labels = df['label'].value_counts().keys().to_list()\ncount = df['label'].value_counts().values\ncount1 = count \/ sum(count) * 100\nangle = count \/ sum(count) * 2 * pi\n\nsource = ColumnDataSource(data = dict(types = different_labels, count = count, color = ['skyblue', 'salmon'], angle = angle, percentage = count1))\n\np5 = figure(plot_height = 400, plot_width = 400, title = \"Proportion of Real and Fake News\",\n           tools = \"hover\", tooltips = \"@types: @percentage\", x_range = (-1.0, 1.0))  ## Pie chart for different labels\n\np5.wedge(x = 0, y = 1, radius = 0.8,\n        start_angle = cumsum('angle', include_zero = True), end_angle = cumsum('angle'),\n        line_color = \"white\", fill_color = 'color', legend_field = 'types', source = source)\np5.title.align = 'center'\n\np5.legend.location = \"top_right\"\n\np5.legend.label_text_font_size = '5pt'\n\nshow(p5)","1c81b156":"df['hasImage'].value_counts()","4d81ffc2":"def convert(path):\n    return '<img src=\"'+ path + '\" width=\"80\">'","2d752261":"df_sources = df[['site_url','label','main_img_url', 'title_without_stopwords']]\ndf_r = df_sources.loc[df['label']== 'Real'].iloc[6 : 10,:]\ndf_f = df_sources.loc[df['label']== 'Fake'].head(6)","853cf766":"HTML(df_r.to_html(escape = False, formatters = dict(main_img_url = convert)))","710c27dc":"HTML(df_f.to_html(escape = False, formatters = dict(main_img_url = convert)))","01006c36":"print(f\"Sites printing Fake news are: {r_}{df[df['label'] == 'Fake']['site_url'].unique()}\")","9b23633f":"df[df['label'] == 'Fake']['site_url'].value_counts().head(5)","a99d2d4d":"df[df['label'] == 'Fake']['site_url'].value_counts().tail(5)","d1e70f8e":"sites = df[df['label'] == 'Fake']['site_url'].value_counts().head(10).index.tolist()\ncount = df[df['label'] == 'Fake']['site_url'].value_counts().head(10).values.tolist()\nsource = ColumnDataSource(data = dict(sites = sites, counts = count, color = ['#11d9bb'] * 10))\n\np6 = figure(x_range = sites, plot_height = 400, plot_width = 1200, title = \"Count of Fake Stories Published by Sites\", tools = \"hover\", tooltips = \"@sites: @counts\")\np6.vbar(x = 'sites', top = 'counts', width = 0.8, source = source, color = 'color')\ncurdoc().theme = 'dark_minimal'\np6.xgrid.grid_line_color = None\np6.title.align = 'center'\np6.y_range.start = 0\n\nshow(p6)","35ff1f53":"print(f\"Sites printing Real news are: {g_}{df[df['label'] == 'Real']['site_url'].unique()}\")","93ab0612":"sites = df[df['label'] == 'Real']['site_url'].value_counts().head(10).index.tolist()\ncount = df[df['label'] == 'Real']['site_url'].value_counts().head(10).values.tolist()\nsource = ColumnDataSource(data = dict(sites = sites, counts = count, color = ['#6baed6'] * 10))\n\np7 = figure(x_range = sites, plot_height = 400, plot_width = 1200, title = \"Count of Real Stories Published by Sites \", tools = \"hover\", tooltips = \"@sites: @counts\")\np7.vbar(x = 'sites', top = 'counts', width = 0.8, source = source, color = 'color')\ncurdoc().theme = 'dark_minimal'\np7.xgrid.grid_line_color = None\np7.title.align = 'center'\np7.y_range.start = 0\n\nshow(p7)","2837ae38":"real = set(df[df['label'] == 'Real']['site_url'].unique())\nfake = set(df[df['label'] == 'Fake']['site_url'].unique())\nprint(f\"Websites publishing both real & fake news are {y_}{real & fake}\")","800092f1":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder() ## Converting the type column from object datatype to numerical datatype\ndf['type'] = le.fit_transform(df['type'])\ndf.head()","d0002a79":"le.classes_","3f164294":"le.transform(['bias', 'bs', 'conspiracy', 'fake', 'hate', 'junksci', 'satire' , 'state'])","67d65572":"mapping = {}\nfor i in le.classes_:\n    mapping[i] = le.transform([i])[0]\nprint(mapping)","c17a8215":"fig = px.sunburst(df, path=['label', 'type'])\nfig.show()","d317029d":"def sites_type(df):\n    types = df['type'].unique()\n    for type in types:\n        df_type = df[df['type'] == type]\n        type = le.inverse_transform([type])\n        print(f\"{r_}The unique sites publishing article of type {type[0]} are: {g_}{df_type['site_url'].unique()}\")\n        print()","b9d62933":"sites_type(df)","e503e935":"urls = []\nfor url in df['site_url']:\n    urls.append(url.split('.')[0])\ndf['site_url'] = urls","1bbb90e5":"df = df.sample(frac = 1)","4d7723e3":"features = df[['site_url', 'text_without_stopwords']]\nfeatures.head(5)","f44bd530":"features['url_text'] = features[\"site_url\"].astype(str) + \" \" + features[\"text_without_stopwords\"]\nfeatures.drop(['site_url', 'text_without_stopwords'], axis = 1, inplace = True)","928d4293":"features.head()","66ad35b6":"X = features\ny = df['type']","ee2e9bba":"y = y.tolist()","b3286954":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n\ntfidf_vectorizer = TfidfVectorizer(use_idf = True, stop_words = 'english')\n\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train['url_text'])\nX_test_tfidf = tfidf_vectorizer.transform(X_test['url_text'])\n\n# We build our corpus only based on the train data because certain new words might occur in the test data\n# that might occur in real life too. This ensures that our model can be generalized.","1c9e6253":"X_train_tfidf.shape","630c19a8":"X_test_tfidf.shape","6161f8fa":"tfidf_train = pd.DataFrame(X_train_tfidf.A, columns = tfidf_vectorizer.get_feature_names())","12d6fa1c":"tfidf_train.head()","10906c32":"rfc = RandomForestClassifier(n_estimators=100,random_state=0)\nrfc.fit(tfidf_train, y_train)\ny_pred = rfc.predict(X_test_tfidf)\nRFscore = metrics.accuracy_score(y_test, y_pred)\nprint(\"The accuracy is : \", RFscore)","ccc8f75a":"print(\"The Weighted F1 score is: \", metrics.f1_score(y_test, y_pred, average = 'weighted'))","40e1c265":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","18796fa5":"y_train = np.array(y_train)\ny_test = np.array(y_test)","1b89029f":"vocab_size = 10000\noov_token = \"<OOV>\"\nembedding_dim = 32\nmax_length = 120\npadding = 'post' # \ntrunc_type = 'post'","0be7c801":"tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_token)\ntokenizer.fit_on_texts(X_train['url_text'])\n# tokenizer.word_index # Mapping of words to numbers","94172ba9":"training_sequences = tokenizer.texts_to_sequences(X_train['url_text'])","8f054c0a":"testing_sequences = tokenizer.texts_to_sequences(X_test['url_text']) # Converting the test data to sequences","7e727eaa":"train_padded = pad_sequences(training_sequences, maxlen = max_length, padding = 'post', truncating = trunc_type)","8ddfdace":"train_padded","a7ee8b9b":"train_padded.shape","b9ad8c09":"testing_padded = pad_sequences(testing_sequences, maxlen = max_length, padding = 'post', truncating = trunc_type)","3e211332":"testing_padded.shape","94b43b40":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(6, activation = 'relu'),\n    tf.keras.layers.Dense(8, activation = 'softmax')\n])\n\nmodel.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])","63f82bec":"model.summary()","2ea6dcb2":"num_epochs = 10\nhistory = model.fit(train_padded, y_train, epochs = num_epochs, validation_data = (testing_padded, y_test))","ff8f6c6d":"print(\"The Training Accuracy we get is: \", history.history['accuracy'][9])\nprint(\"The Testing Accuracy we get is: \", history.history['val_accuracy'][9])","4b5fc21b":"# Using TF-IDF","e2a35f24":"## Hate","a844abf4":"# EDA | Text Prepocessing | Modelling | Embedding","8885b116":"**By applying the tf-idf method we used above, we observed that we get a lot of zeros for sentence representation, i.e we got a sparse matrix. Sparse Matrix is not a true representation for the corpus, and it doesn't take into account the similarity of the words. That is where Embeddings come to our rescue.**\n\nA word embedding is a class of approaches for r**epresenting words and documents using a dense vector representation.**\n\nIt is an improvement over more the traditional bag-of-word model encoding schemes where large sparse vectors were used to represent each word or to score each word within a vector to represent an entire vocabulary. These representations were sparse because the vocabularies were vast and a given word or document would be represented by a large vector comprised mostly of zero values.\n\nInstead, in an embedding, **words are represented by dense vectors where a vector represents the projection of the word into a continuous vector space.**\n\nThe position of a word within the vector space is learned from text and is based on the words that surround the word when it is used.\n\nThe position of a word in the learned vector space is referred to as its embedding.","1abcbde0":"**512 sentences and each sentence has length of 120 words.**","db111c75":"# Images","9d42a1c2":"# Embedding","c5af24a1":"# Label","32f0ab12":"## State","53cce4f6":"**There are close to 2.5% values which are missing in the columns 'text_without_stopwords' and 'text' and close to 0.5% missing values in columns like 'title_without_stop_words', 'language' etc. We will be dropping these null values.**","f52ca783":"## bs","c867343a":"**Let's combine both of them together to form a new column, url_text**","e567434b":"**TF-IDF stands for term frequency-inverse document frequency**, and the tf-idf weight is often used in information retrieval and text mining. This weight is used to evaluate how important a word is to a document in a collection or corpus. \n\nThe problem with including just the count of words is that certain words like **\u201cthe,\u201d \u201cand,\u201d \u201cis,\u201d may be used a lot so they might be considered as \u201cimportant\u201d instead of the words that are actually important**, that\u2019s where \u2018idf\u2019 comes into the picture. \n\nThe importance increases proportionally to the number of times a word appears in the document but is balanced by the frequency of the word in the corpus.\n\nTypically, the tf-idf weight is composed of two terms:\n**Term Frequency (TF)**, aka. The number of times a word appears in a document, divided by the total number of words in that document;\n\n**Inverse Document Frequency (IDF)**, computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.","b36f323e":"# Modelling","af3bb3be":"**Above is the mapping of the LabelEncoder, that is LabelEncoder has labeled bias as 0, bs as 1, and conspiracy as 2 etc**","3d834c53":"# Different Types","b7e80401":"## Fake","02cc4d0c":"# Sites that Publish Different Types of Stories","06f9a232":"**Once we get the dictionary of word indexes, we need to convert the whole sentence into numerical representation, for that we use 'texts_to_sequences'**","0815425c":"**Above is the representation of the tf-idf matrix. The first represents the 'first url_text' and corresponding colum values represent the value of that column for 1st document. One point to note here is the presence of a very large number of zeros. We will be dealing with that in the next section**","30b5533f":"**pad_sequences is used to ensure that all sequences in a list have the same length. By default this is done by padding 0 in the beginning of each sequence until each sequence has the same length as the longest sequence.**","d7f6bfce":"**Most of the fake news are being given by the 21stcenturywire.com. Let's check which are the sites that deliver fake news**","4a0eaa20":"**Using HTML to view the images given in form of image url**","1e7549d7":"**All the null values have been removed**","fae7aef6":"![news.jpeg](attachment:4daa6dba-dad4-486f-bf7b-87d23ff57fac.jpeg)","46d33ac3":"**From the above unigrams and bigrams, the top words are \"Hillary Clinton\", \"Donald Trump\", \"United States\" and \"election day\" etc which suggests that this dataset was captured around the election days.**","fe0b0820":"**Getting the websites that publish fake news is fine but there might be sites where only 1 or 2 news were fake, let's take into the count consideration as well.**","bb04f4fc":"## Bias","38e8eb4d":"**For the 'hate' type, the top bigrams are quite obvious beacuse during the elections, all would spread hate against the leader that they don't support. Some might be the supporters of Hillary Clinton, and some mighe be the supporters of Donald Trump.**","39587b68":"**Hope you liked the notebook, any suggestions would be highly appreciated.**\n\n**I will continue experimenting in future versions of the notebook.**\n\n**Please upvote if you liked it.**","5e9ea818":"**Tokenizer updates internal vocabulary based on the given list of texts. This method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 1; word_index[\"cat\"] = 2**","f783f41c":"* We will set the maximum number of words allowed to be 100.\n\n* In the test dataset, there can be words that are Out of Vocabulary(OOV), we will encode those words as OOV\n\n* Embedding Dimension has been set to 32\n\n* A lot of sentences might be very long, we will keep the maximum length to be 120.\n\n* You might be wondering that all the sentences are not neccessarry to be of the same length, in order to tackle that we will use the concept of padding i.e to add zeros before or after the sentence to keep the length uniform.","8e167ca4":"**In the dataset, the values are ordered. For example: all the site urls are ordered alphabetically, therefore we need to reshuffle the values.**","05371a56":"# Unigrams and Bigrams","5c8b37da":"# Languages","bdea3648":"**Our main aim is to predict the article type, and from the above output we can observe that the website url can also play a major role in the final prediction along with the text. But before moving forward, let's remove '.com' from the main urls.**","bce657a7":"**Let's also check if there exists websites that publish both real and fake news**","5e4b249c":"**We come across different type of news, some news can be bullshit, some news can be fake, some news are shown to express hate, and some news are published to make fun of others. Given the news content, we as humans are able to classify that article into different categories but can computers do it? Let's answer that question in this notebook.**\n\n**We first explore the data in hand using Bokeh, and then draw certain conclusions from that. Then we will perform some text preprocessing using TF-IDF, once the data is preprocessed we apply Random Forest for modelling. During this, we will reliase the drawback of TF-IDF, which will take us to using Embeddings using TensorFlow.**\n\n**Don't be overwhelmed by these steps, I am there to explain each step along the way.**\n\n**Some of the methods used in this notebook has been inspired by Ruchi Bhatia's notebook**\n\n**Do upvote the notebook if you liked it!**","a895e3e2":"**1533 sentences and each sentence has length of 120 words.**","d0364d3d":"**From the above plot, we can observe that all of the fake news was published for the type 1, 2, 6, 5, 3 i.e for bs, conspiracy, fake, junksci and satire. Whereas all the real news was publised for the type: 0, 4 and 7 i.e for bias, hate and state.**","265ffcb0":"# Null Values","aa435310":"**These are the 8 different types of articles:** \n* bs (i.e. bullshit)\n\n* junksci(i.e. junk science)\n\n* hate\n\n* fake\n\n* conspiracy\n\n* bias \n\n* satire \n\n* state\n"}}