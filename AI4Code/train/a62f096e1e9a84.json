{"cell_type":{"d6745c76":"code","334cb4b4":"code","101e800a":"code","9b2e0c55":"code","605b06d8":"code","20126e76":"code","a7507c24":"code","cced0797":"code","0ae95c87":"code","b9f17607":"code","30ff5608":"code","f9c0d44c":"code","fb62fcc5":"code","0e28a5c8":"code","a7156133":"code","a39836a8":"code","c91663e5":"code","60e56a1d":"code","c6281386":"code","eddb6a98":"code","305da94f":"markdown","3ce4a3a5":"markdown","31d02a44":"markdown","c48e8a49":"markdown"},"source":{"d6745c76":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport warnings\nwarnings.filterwarnings(action = \"ignore\")\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","334cb4b4":"train_df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/test.csv\")","101e800a":"print(test_df)","9b2e0c55":"print(len(train_df))\nsummary = pd.DataFrame(train_df.dtypes)\nsummary[\"Null\"] = train_df.isnull().sum()\nsummary[\"first\"] = train_df.loc[0]\nsummary[\"second\"] = train_df.loc[1]\nsummary[\"third\"] = train_df.loc[2]\nsummary[\"unique\"] = train_df.nunique()\nsummary.tail(10)","605b06d8":"def get_row_col_idx(idx):\n    \"\"\"Gets the index of row and col to plot\"\"\"\n    row_idx = idx\/\/6\n    col_idx = idx%6\n    return row_idx, col_idx","20126e76":"fig, ax = plt.subplots(13,6,figsize = (30,30))\nfor idx, cols in enumerate(train_df.columns.to_list()[1:-1]):\n    row_idx, col_idx = get_row_col_idx(idx)\n    sns.distplot(train_df[cols], ax = ax[row_idx,col_idx], color = \"Blue\")\n    sns.distplot(test_df[cols], ax = ax[row_idx,col_idx], color = \"yellow\")","a7507c24":"def write_percent(ax, train_df):\n    \"\"\"Writes the percentage on top of bars in barplot.\"\"\"\n    for patch in ax.patches:\n        height = patch.get_height()\n        width = patch.get_width()\n        x_loc = patch.get_x()\n        percent = height\/len(train_df)*100\n        ax.text(x = x_loc + width\/2, y = height, s = '{:.1f}'.format(percent))\n","cced0797":"fig, ax = plt.subplots(figsize=(20,5))\nsns.histplot(train_df[\"target\"], color = \"Red\", ax = ax)\nwrite_percent(ax, train_df)","0ae95c87":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ncorr = train_df.corr()\nf, ax = plt.subplots(figsize=(30,30))\nsns.heatmap(corr, vmax=.8, square=True);\n","b9f17607":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ny = train_df.pop(\"target\")\ny = encoder.fit_transform(y)\nprint(pd.Series(y).value_counts())","30ff5608":"from sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss, precision_score, recall_score\nid_col = train_df.pop(\"id\")\nX = train_df\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1, stratify=y)\n# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=1, stratify = y) # 0.25 x 0.8 = 0.2","f9c0d44c":"# model = RandomForestClassifier(n_estimators=300, max_depth=15, n_jobs=-1)\nmodel = XGBClassifier(learning_rate =0.1,max_depth=5, n_estimators=200)\nmodel.fit(X_train, y_train)","fb62fcc5":"y_pred = model.predict(X_train)\nprint(y_pred)","0e28a5c8":"def log_loss(pred_probs, y_valid):\n    pred_probs = pred_probs + 1e-15 # numerical stability\n    y_valid = y_valid.reshape(-1, 1) \n    \n    # get probs for each class\n    preds_class = np.take_along_axis(pred_probs, y_valid, axis=1)\n    return -1 * np.log(preds_class).mean()","a7156133":"preds = model.predict_proba(X_train)\nlog_loss(preds, y_train)","a39836a8":"preds = model.predict_proba(X_test)\nlog_loss(preds, y_test)","c91663e5":"# model = XGBClassifier(learning_rate =0.1,max_depth=5, n_estimators=200)\n# model.fit(X_train, y_train)\n# y_pred = model.predict_proba(X_test)\n# score = log_loss(y_pred, y_test)","60e56a1d":"# print(score)","c6281386":"test_df = test_df.iloc[:, 1:]","eddb6a98":"\ntest_preds = model.predict_proba(test_df)\nsub = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/sample_submission.csv\")\nsub.iloc[:, 1:] = test_preds\nsub.to_csv(\"submission.csv\", index=False)","305da94f":"From here we could see that Class_6 and Class_8 are frequent class ann on the other hand Class_4 and Class_5 are the classes which have occured quite a few times. There are 9 unique values for target variable.","3ce4a3a5":"We have 74 features, every feature is non null and integer so no imputations required. 200,000 rows are there","31d02a44":"We can see from here that the data is skewed towards right.","c48e8a49":"# Univariate Analysis"}}