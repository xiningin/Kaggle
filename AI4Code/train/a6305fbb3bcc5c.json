{"cell_type":{"27ffba71":"code","5df2fb8e":"code","5ab43478":"code","22573ebb":"code","64ecb0d6":"code","e456ff1b":"code","f7a67210":"code","0eecaa9d":"code","04f15a37":"code","00ee3ff5":"code","a3a6554f":"code","d3d62602":"code","3a21bae8":"code","859eb412":"code","b1f0ba9c":"code","9b09aff9":"code","a7d6899e":"code","55c8e12f":"code","6b95cb3c":"code","bcd4390b":"code","e796f524":"code","9f1a9d60":"code","d1d9ff43":"code","1602521d":"code","00bb2d95":"code","1c440c8b":"code","019fdddb":"code","4ca0e76e":"code","9b696a88":"code","37c1bc67":"code","40529efe":"code","58da0f50":"code","055b2af6":"code","b725c7be":"code","c03c76bd":"code","96d4ffc1":"code","a37893ed":"code","925f1afa":"code","887e3379":"code","3a643dc4":"code","82893ffe":"code","7bdf4381":"code","e73f310b":"code","c76ee302":"code","b60fcd0b":"code","20c2f420":"code","24dd18c8":"code","24a6b95e":"code","624db2f8":"code","9575fb33":"code","05a1c2b0":"code","6f2e58b4":"markdown","1df88dab":"markdown","94186c2d":"markdown","ca575156":"markdown","41b0a0fc":"markdown","9b6d8f0c":"markdown","de3882ee":"markdown","f28b8a7a":"markdown","44a8bd35":"markdown","485b7389":"markdown","dcca430b":"markdown"},"source":{"27ffba71":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5df2fb8e":"import plotly.express as px\nimport seaborn as sns\nimport matplotlib.pyplot as plt","5ab43478":"df = pd.read_csv('\/kaggle\/input\/house-price-prediction-challenge\/train.csv')\ndf.head(10)","22573ebb":"df[['BHK_NO.', 'BHK_OR_RK']]","64ecb0d6":"df['BHK_OR_RK'].value_counts()","e456ff1b":"df.describe()","f7a67210":"df['BHK_NO.'].unique()","0eecaa9d":"df.dtypes","04f15a37":"plt.figure(figsize = (15, 6))\nsns.heatmap(data = df.corr(), annot = True, cmap = 'RdYlGn')","00ee3ff5":"fig = px.bar(x=df[\"BHK_NO.\"].unique(), y=df[\"BHK_NO.\"].value_counts())\nfig.show()","a3a6554f":"plt.figure(figsize = (15, 6))\nsns.barplot(data = df, x = 'RERA', y = 'TARGET(PRICE_IN_LACS)')","d3d62602":"plt.figure(figsize = (15, 6))\nsns.barplot(data = df, x = 'POSTED_BY', y = 'TARGET(PRICE_IN_LACS)')","3a21bae8":"plt.figure(figsize = (15, 6))\nsns.barplot(data = df, x = 'UNDER_CONSTRUCTION', y = 'TARGET(PRICE_IN_LACS)')","859eb412":"plt.figure(figsize = (15, 6))\nsns.barplot(data = df, x = 'BHK_NO.', y = 'TARGET(PRICE_IN_LACS)')","b1f0ba9c":"df = df.drop(['BHK_OR_RK', 'ADDRESS', 'LATITUDE', 'LONGITUDE'], axis = 1)\ndf.head()","9b09aff9":"df = pd.get_dummies(df)","a7d6899e":"df.columns","55c8e12f":"df.head()","6b95cb3c":"df = df.drop(['POSTED_BY_Builder'], axis = 1)\ndf.head()","bcd4390b":"X = df.drop(columns = ['TARGET(PRICE_IN_LACS)'])\ny = df['TARGET(PRICE_IN_LACS)']","e796f524":"X","9f1a9d60":"y","d1d9ff43":"#from sklearn.compose import ColumnTransformer\n#from sklearn.preprocessing import StandardScaler\n#ct = ColumnTransformer(transformers=[('SQUARE_FT', StandardScaler(), [3])], remainder='passthrough')\n#X = pd.DataFrame(ct.fit_transform(X))","1602521d":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX = sc.fit_transform(X)","00bb2d95":"X","1c440c8b":"X.shape","019fdddb":"y.shape","4ca0e76e":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","9b696a88":"X_train","37c1bc67":"from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)","40529efe":"y_pred = regressor.predict(X_test)","58da0f50":"from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n\nR2_Score = r2_score(y_test, y_pred)\nMean_Absolute_Error = mean_absolute_error(y_test, y_pred)\nMean_Square_Error = mean_squared_error(y_test, y_pred)\nRoot_Mean_Square_Error = np.sqrt(mean_squared_error(y_test, y_pred))\n\nresults = pd.DataFrame([['Multiple Linear Regression', R2_Score, Mean_Absolute_Error, Mean_Square_Error, Root_Mean_Square_Error]],\n                      columns = ['Model', 'R2 Score', 'Mean Absolute Error', 'Mean Square Error', 'Root Mean Square Error'])","055b2af6":"results","b725c7be":"from sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(n_estimators = 100, random_state = 1)\nregressor.fit(X_train, y_train)","c03c76bd":"y_pred = regressor.predict(X_test)","96d4ffc1":"R2_Score = r2_score(y_test, y_pred)\nMean_Absolute_Error = mean_absolute_error(y_test, y_pred)\nMean_Square_Error = mean_squared_error(y_test, y_pred)\nRoot_Mean_Square_Error = np.sqrt(mean_squared_error(y_test, y_pred))\n\nmodel_results = pd.DataFrame([['Random Forest', R2_Score, Mean_Absolute_Error, Mean_Square_Error, Root_Mean_Square_Error]],\n                      columns = ['Model', 'R2 Score', 'Mean Absolute Error', 'Mean Square Error', 'Root Mean Square Error'])\nresults = results.append(model_results, ignore_index = True)","a37893ed":"results","925f1afa":"from xgboost import XGBRegressor\nregressor = XGBRegressor(random_state = 2)\nregressor.fit(X_train, y_train)","887e3379":"y_pred = regressor.predict(X_test)","3a643dc4":"R2_Score = r2_score(y_test, y_pred)\nMean_Absolute_Error = mean_absolute_error(y_test, y_pred)\nMean_Square_Error = mean_squared_error(y_test, y_pred)\nRoot_Mean_Square_Error = np.sqrt(mean_squared_error(y_test, y_pred))\n\nmodel_results = pd.DataFrame([['XGB Regressor', R2_Score, Mean_Absolute_Error, Mean_Square_Error, Root_Mean_Square_Error]],\n                      columns = ['Model', 'R2 Score', 'Mean Absolute Error', 'Mean Square Error', 'Root Mean Square Error'])\nresults = results.append(model_results, ignore_index = True)","82893ffe":"results","7bdf4381":"parameters = {\"learning_rate\": [0.05, 0.10, 0.15, 0.20, 0.25, 0.30], \"max_depth\": [3, 4, 5, 6, 8, 10, 12, 15], \n              \"min_child_weight\": [1, 3, 5, 7], \"gamma\": [0.0, 0.1, 0.2, 0.3, 0.4], \"colsample_bytree\": [0.3, 0.4, 0.5, 0.7]}","e73f310b":"from sklearn.model_selection import RandomizedSearchCV\nrandom_search = RandomizedSearchCV(estimator = regressor, param_distributions = parameters, n_iter = 5, scoring = 'r2', n_jobs = -1,\n                                  cv = 10, verbose = 3)","c76ee302":"import time\n\nt0 = time.time()\nrandom_search.fit(X_train, y_train)\nt1 = time.time()\nprint(\"Took %0.2f Seconds\" %(t1-t0))","b60fcd0b":"random_search.best_estimator_","20c2f420":"random_search.best_params_","24dd18c8":"regressor = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.3, gamma=0.0, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.15, max_delta_step=0, max_depth=6,\n             min_child_weight=3, monotone_constraints='()',\n             n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=2,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n             tree_method='exact', validate_parameters=1, verbosity=None)","24a6b95e":"regressor.fit(X_train, y_train)","624db2f8":"y_pred = regressor.predict(X_test)","9575fb33":"R2_Score = r2_score(y_test, y_pred)\nMean_Absolute_Error = mean_absolute_error(y_test, y_pred)\nMean_Square_Error = mean_squared_error(y_test, y_pred)\nRoot_Mean_Square_Error = np.sqrt(mean_squared_error(y_test, y_pred))\n\nmodel_results = pd.DataFrame([['XGB Regressor(Hyper-Parameter Tuned)', R2_Score, Mean_Absolute_Error, Mean_Square_Error, Root_Mean_Square_Error]],\n                      columns = ['Model', 'R2 Score', 'Mean Absolute Error', 'Mean Square Error', 'Root Mean Square Error'])\nresults = results.append(model_results, ignore_index = True)","05a1c2b0":"results","6f2e58b4":"## Multiple Linear Regression","1df88dab":"## Random Forest","94186c2d":"---------------------------------------------------------------------------------------------------------------------","ca575156":"# Feature Scaling","41b0a0fc":"# OneHotEncoding","9b6d8f0c":"# Splitting the dataset into Training and Testing Set**","de3882ee":"# Model Building","f28b8a7a":"## XGBoost","44a8bd35":"# Hyper-Parameter Tuning Using RandomizedSearchCV","485b7389":"# Exploratory Data Analysis","dcca430b":"# Hence, we will go with the normal XGBoost algorithm."}}