{"cell_type":{"165dc6ba":"code","0f699879":"code","2e3f486a":"code","feec9c69":"code","1b126817":"code","f641674c":"code","4ce6e373":"code","7d8bfafc":"code","6e81fb5c":"code","da57b6b6":"code","53e010ec":"markdown","399d6a15":"markdown","726621a2":"markdown","d40186ef":"markdown","6da8815d":"markdown","27ed38e3":"markdown","864e99e1":"markdown","509f28e4":"markdown","6e231816":"markdown","cda2ff66":"markdown","e204a75c":"markdown","dc533cfa":"markdown","f5c4f855":"markdown","3c9b810c":"markdown","63a8c58e":"markdown"},"source":{"165dc6ba":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n%matplotlib inline","0f699879":"from datasets_mnist import MNISTDataset\n\nmnist = tf.keras.datasets.mnist\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n\ndata = MNISTDataset(train_images.reshape([-1, 784]), train_labels, \n                    test_images.reshape([-1, 784]), test_labels,\n                    batch_size=128)","2e3f486a":"w=10\nh=10\nfig=plt.figure(figsize=(8, 8))\ncolumns = 4\nrows = 5\nfor i in range(1, columns*rows +1):\n    img = train_images[i]\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(img)\nplt.show()","feec9c69":"W = tf.Variable(tf.zeros([784,10]), trainable=True)\nb = tf.Variable(tf.zeros([10]), trainable=True)","1b126817":"# Computes output of network to be passed through a softmax activation\n@tf.function\ndef get_logits(W,b,x):\n    return tf.matmul(x,W) + b","f641674c":"def compute_cross_entropy():\n    softmax = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=lbl_batch,logits=logits)\n    return tf.reduce_mean(softmax)","4ce6e373":"train_steps = 1000\nlearning_rate = 0.1\n\n# optimizer\noptimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n\nfor step in range(train_steps):\n    img_batch, lbl_batch = data.next_batch()\n\n    with tf.GradientTape() as tape:\n        logits = get_logits(W,b,img_batch)\n        loss = compute_cross_entropy()\n        \n    # Apply gradients and update weights\n    gradients = tape.gradient(loss, [W,b])\n    optimizer.apply_gradients(zip(gradients, [W,b]))\n\n    # Log\n    if step%100 == 0:\n        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n        accuracy = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch),\n                                 tf.float32))\n        print(\"Loss: {} Accuracy: {}\".format(compute_cross_entropy(), accuracy))","7d8bfafc":"test_preds = tf.argmax(tf.matmul(data.test_data, W) + b, axis=1,\n                       output_type=tf.int32)\naccuracy = tf.reduce_mean(tf.cast(tf.equal(test_preds, data.test_labels),\n                             tf.float32))\nprint(accuracy.numpy())","6e81fb5c":"from sklearn.metrics import confusion_matrix\nimport confusion_matrix_util as cm_util","da57b6b6":"cm = confusion_matrix(test_labels, test_preds)\ntargets = [i for i in range(0,9)]\n\ncm_util.plot_confusion_matrix(cm, target_names=targets, rotation_x_label=None, title=\"Confusion Matrix Softmax regression\")","53e010ec":"### Confusion matrix","399d6a15":"Thanks for reading this notebook. Give it a vote if you find it useful.","726621a2":"# Handwritten digits classification using softmax regression","d40186ef":"# Load data","6da8815d":"We end up with 90% accuracy on the test data. ","27ed38e3":"## A view of the dataset content","864e99e1":"## Predict test data ","509f28e4":"This is the architecture of a softmax regression model:","6e231816":"### Training operations","cda2ff66":"*Image from : https:\/\/www.kdnuggets.com\/2016\/07\/softmax-regression-related-logistic-regression.html*","e204a75c":"![image.png](attachment:image.png)","dc533cfa":"![softmax.png](attachment:softmax.png)","f5c4f855":"## Loss function","3c9b810c":"## Initialization of weights and biases","63a8c58e":"# Training the model"}}