{"cell_type":{"49c86498":"code","e40b1177":"code","7c2a12f3":"code","d59f0ed8":"code","5c1147c8":"code","3d9f8ab5":"code","46742b06":"code","5004cc02":"code","21fba798":"code","247ef6e8":"code","6170588b":"code","683a0c6f":"code","aabd0f2b":"code","3dbcfc9c":"code","6eb53dd4":"code","1f131acf":"code","a5e705fb":"code","b74c6d6a":"code","3cd1d698":"code","3d736f6e":"code","35fe00f7":"code","69b8093e":"code","a7eeac33":"code","883af92b":"code","690904bc":"code","cd402edf":"code","54f44d3f":"markdown","92451011":"markdown","4291afba":"markdown","e19886eb":"markdown","d5e724ca":"markdown","e62786ca":"markdown","5b2bae0d":"markdown","e419c80f":"markdown","bca96f7f":"markdown","8975c8fd":"markdown","a08d7df7":"markdown","dbc4690c":"markdown","ce263656":"markdown","3a89655b":"markdown","4536529e":"markdown","6405fda1":"markdown","22b46508":"markdown","82d537d4":"markdown","e63cd966":"markdown","3f8e85ef":"markdown","fcde2220":"markdown"},"source":{"49c86498":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e40b1177":"from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Concatenate,GRU,Dropout,LeakyReLU\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.io import FixedLenFeature\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder, normalize","7c2a12f3":"import tensorflow as tf\nprint(\"Tensorflow version \" + tf.__version__)","d59f0ed8":"try: \n     tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n     strategy = tf.distribute.TPUStrategy(tpu)\nexcept ValueError: # detect GPUs\n     strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)","5c1147c8":"\nAUTO = tf.data.experimental.AUTOTUNE","3d9f8ab5":"train_df = pd.read_csv('..\/input\/higgs-boson\/training.zip')\ntest_df = pd.read_csv('..\/input\/higgs-boson\/test.zip')\n\nprint(train_df.shape,test_df.shape)","46742b06":"list(set(train_df.columns ) - set(test_df.columns))","5004cc02":"train_df.drop(['Weight'], axis=1,inplace=True)","21fba798":"sns.barplot(x = train_df['Label'].value_counts().index, y = train_df['Label'].value_counts().values)\nplt.title('Label counts')\nplt.show()","247ef6e8":"enc = LabelEncoder()\n\ntrain_df['Label'] = enc.fit_transform(train_df['Label'])\ntrain_df.head()","6170588b":"train_df.set_index(['EventId'],inplace = True)\ntest_df.set_index(['EventId'],inplace = True)","683a0c6f":"X = train_df.drop(['Label'], axis=1)\ny = train_df['Label'].values","aabd0f2b":"X = normalize(X)\ntest_df_nor = normalize(test_df)","3dbcfc9c":"splitter=StratifiedShuffleSplit(n_splits=1,random_state=12)\n\nfor train,test in splitter.split(X,y):     #this will splits the index\n    X_train_SS = X[train]\n    y_train_SS = y[train]\n    X_test_SS = X[test]\n    y_test_SS = y[test]","6eb53dd4":"# reshape for rnn\nX_train_SS = np.reshape(X_train_SS, (X_train_SS.shape[0],X_train_SS.shape[1],1))\nX_test_SS = np.reshape(X_test_SS, (X_test_SS.shape[0],X_test_SS.shape[1],1))","1f131acf":"X_train_SS.shape[2]","a5e705fb":"def build_Lstm_model(train_x,train_y,test_x,test_y):\n    inp = Input(shape=(train_x.shape[1],train_x.shape[2]))\n    rnn_1st_model = LSTM(units=60, return_sequences=True,recurrent_dropout=0.1)(inp)\n    rnn_2nd_model = LSTM(units=60,recurrent_dropout=0.1)(rnn_1st_model)\n    dense_layer = Dense(128)(rnn_2nd_model)\n    drop_out = Dropout(0.2)(dense_layer)\n    output = Dense(1, activation= LeakyReLU(alpha=0.1),name=\"class\")(drop_out)\n    model = Model(inp, output)\n    callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=4, verbose=1, factor=0.6),\n                 EarlyStopping(monitor='val_loss', patience=20),\n                 ModelCheckpoint(filepath='best_model_LSTM.h5', monitor='val_loss', save_best_only=True)]\n    model.summary()\n    model.compile(loss=[tf.keras.losses.MeanSquaredLogarithmicError(),\n                        tf.keras.losses.MeanSquaredLogarithmicError()], optimizer=\"adam\")\n    history = model.fit(train_x, train_y, \n          epochs = 20, \n          batch_size = 128, \n          validation_data=(test_x,  test_y), \n          callbacks=callbacks)\n    return history,model","b74c6d6a":"def build_Gru_model(train_x,train_y,test_x,test_y):\n    inp = Input(shape=(train_x.shape[1],train_x.shape[2]))\n    rnn_1st_model = GRU(units=60, return_sequences=True,recurrent_dropout=0.1)(inp)\n    rnn_2nd_model = GRU(units=60,recurrent_dropout=0.1)(rnn_1st_model)\n    dense_layer = Dense(128)(rnn_2nd_model)\n    drop_out = Dropout(0.25)(dense_layer)\n    output = Dense(1, activation= LeakyReLU(alpha=0.1),name=\"class\")(drop_out)\n    model = Model(inp, output)\n    callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=4, verbose=1, factor=0.6),\n                 EarlyStopping(monitor='val_loss', patience=20),\n                 ModelCheckpoint(filepath='best_model_GRU.h5', monitor='val_loss', save_best_only=True)]\n    model.summary()\n    model.compile(loss=[tf.keras.losses.MeanSquaredLogarithmicError(),\n                        tf.keras.losses.MeanSquaredLogarithmicError()], optimizer=\"adam\")\n    history = model.fit(train_x, train_y, \n          epochs = 20, \n          batch_size = 128, \n          validation_data=(test_x,  test_y), \n          callbacks=callbacks)\n    return history,model","3cd1d698":"def plot_Loss(history):\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Loss over epochs')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='best')\n    plt.show()","3d736f6e":"\ndef Save_Result_To_Csv(model,model_name,csv_file):\n    test_pre = np.reshape(test_df_nor, (test_df_nor.shape[0],test_df_nor.shape[1],1))\n    model.load_weights(model_name)\n    prediction = model.predict(test_pre)\n    prediction =  np.where(prediction > 0.5, 1, 0)\n    prediction = pd.Series(prediction[:,0])\n    sub = pd.read_csv('..\/input\/higgs-boson\/random_submission.zip')\n    test_predict = pd.DataFrame({\"EventId\":sub['EventId'],\"RankOrder\":sub['RankOrder'],\"Class\":prediction})\n    test_predict['Class'] = test_predict['Class'].replace(1,'s')\n    test_predict['Class'] = test_predict['Class'].replace(0,'b')\n    test_predict.to_csv(csv_file,index=False)","35fe00f7":"history_LSTM,Lstm_model = build_Lstm_model(X_train_SS,y_train_SS,X_test_SS,y_test_SS)","69b8093e":"plot_Loss(history_LSTM)","a7eeac33":"Save_Result_To_Csv(Lstm_model,\"best_model_LSTM.h5\",\"submission_LSTM.csv\")","883af92b":"history_GRU,Gru_model = build_Gru_model(X_train_SS,y_train_SS,X_test_SS,y_test_SS)","690904bc":"plot_Loss(history_GRU)","cd402edf":"Save_Result_To_Csv(Gru_model,\"best_model_GRU.h5\",\"submission_GRU.csv\")","54f44d3f":"* Final memory at current time step\n- As the last step, the network needs to calculate h_t \u2014 vector which holds information for the current unit and passes it down to the network.\n- In order to do that the update gate is needed. It determines what to collect from the current memory content \u2014 h\u2019_t and what from the previous steps \u2014 h_(t-1). That is done as follows:\n![image.png](attachment:image.png)","92451011":"* Update gate: The update gate helps the model to determine how much of the past information (from     previous time steps) needs to be passed along to the future.We start with calculating the update  gate z_t for time step t using the formula:\n![image.png](attachment:image.png)","4291afba":"* Convert the test array to  dimensional shape to predict target vector values","e19886eb":"* Reset gate\n- Essentially, this gate is used from the model to decide how much of the past information to forget. To calculate it, we use:\n![image.png](attachment:image.png)","d5e724ca":"# Working of gates in LSTMs\nFirst, LSTM cell takes the previous memory state Ct-1 and does element wise multiplication with forget gate (f) to decide if present memory state Ct. If forget gate value is 0 then previous memory state is completely forgotten else f forget gate value is 1 then previous memory state is completely passed to the cell ( Remember f gate gives values between 0 and 1 ).\n\n**Ct = Ct-1 * ft**\n\nCalculating the new memory state:\n\n**Ct = Ct + (It * C`t)**\n\nNow, we calculate the output:\n\n**Ht = tanh(Ct)**","e62786ca":"* Current memory content\n-  First, we start with the usage of the reset gate. We introduce a new memory content which will use the reset gate to store the relevant information from the past. It is calculated as follows:\n![image.png](attachment:image.png)\n- Multiply the input x_t with a weight W and h_(t-1) with a weight U.\n- Calculate the Hadamard (element-wise) product between the reset gate r_t and Uh_(t-1). That will determine what to remove from the previous time steps\n- Sum up the results of above steps\n- Apply the nonlinear activation function tanh.","5b2bae0d":"![image.png](attachment:image.png)","e419c80f":"# What Are Exploding Gradients?\n1. An error gradient is the direction and magnitude calculated during the training of a neural network that is used to update the network weights in the right direction and by the right amount.\n1. In deep networks or recurrent neural networks, error gradients can accumulate during an update and result in very large gradients. These in turn result in large updates to the network weights, and in turn, an unstable network. At an extreme, the values of weights can become so large as to overflow and result in NaN values.\n1. The explosion occurs through exponential growth by repeatedly multiplying gradients through the network layers that have values larger than 1.0.","bca96f7f":"For the vanishing gradient problem, many approaches have been suggested, below are few of them:\n1. Using ReLU activation function.\n1. Long-Short Term Memory (LSTM) architecture, where the forget gate might help.\n1. Initialize the weight matrix, W, with an orthogonal matrix, and use this through the entire training (multiplications of orthogonal matrices doesn\u2019t explode or vanish).","8975c8fd":"# What is Vanishing Gradient problem?\n* Neural networks are trained using stochastic gradient descent.\n* This involves first calculating the prediction error made by the model and using the error to estimate a gradient used to update each weight in the network so that less error is made next time. This error gradient is propagated backward through the network from the output layer to the input layer.\n* It is desirable to train neural networks with many layers, as the addition of more layers increases the capacity of the network, making it capable of learning a large training dataset and efficiently representing more complex mapping functions from inputs to outputs.\n* A problem with training networks with many layers (e.g. deep neural networks) is that the gradient diminishes dramatically as it is propagated backward through the network. The error may be so small by the time it reaches layers close to the input of the model that it may have very little effect. As such, this problem is referred to as the \u201cvanishing gradients\u201d problem.\n![image.png](attachment:image.png)","a08d7df7":"- The main moto to create this kernel is to practice RNNs. RNNs have truly changed the way sequential data is forecasted. I wanted to create the sample reference for RNNs.\n- Please upvote the kernel if you like it","dbc4690c":"# Gated Recurrent Units\n* GRU (Gated Recurrent Unit) aims to solve the vanishing gradient problem which comes with a standard recurrent neural network. GRU can also be considered as a variation on the LSTM because both are designed similarly\n* In simple words, the GRU unit does not have to use a memory unit to control the flow of information like the LSTM unit. It can directly makes use of the all hidden states without any control. GRUs have fewer parameters and thus may train a bit faster or need less data to generalize. But, with large data, the LSTMs with higher expressiveness may lead to better results.\n\n* They are almost similar to LSTMs except that they have two gates: reset gate and update gate. Reset gate determines how to combine new input to previous memory and update gate determines how much of the previous state to keep. Update gate in GRU is what input gate and forget gate were in LSTM. We don't have the second non linearity in GRU before calculating the outpu, .neither they have the output gate.\n![image.png](attachment:image.png)","ce263656":"# StratifiedShuffleSplit\n- This cross-validation object is a merge of StratifiedKFold and ShuffleSplit, which returns stratified randomized folds. The folds are made by preserving the percentage of samples for each class.\n- As we are having class imbalance over here hence it is better to go with stratified split to preserve the same class ratio.","3a89655b":"# Check for the missing column in test dataset\n- the Label which is target column and weight is missing from the test dataset\n- So let's drop the Weight column from the training datset","4536529e":"# Resources:\n* [Intro to Lstm-Gru](https:\/\/www.kaggle.com\/thebrownviking20\/intro-to-recurrent-neural-networks-lstm-gru)\n* [higgs-boson-classification usinfg rnn](https:\/\/www.kaggle.com\/makhloufsabir\/higgs-boson-classification-physics-rnn)\n","6405fda1":"# Target Variable count\n- We can clearly says the dataset is imbalanced and it has more labels of b\n- Label b in training data are doluble in number of Label S","22b46508":"# Long Short Term Memory(LSTM)\n* Long short-term memory (LSTM) units (or blocks) are a building unit for layers of a recurrent neural network (RNN). A RNN composed of LSTM units is often called an LSTM network. A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell is responsible for \"remembering\" values over arbitrary time intervals; hence the word \"memory\" in LSTM. Each of the three gates can be thought of as a \"conventional\" artificial neuron, as in a multi-layer (or feedforward) neural network: that is, they compute an activation (using an activation function) of a weighted sum. Intuitively, they can be thought as regulators of the flow of values that goes through the connections of the LSTM; hence the denotation \"gate\". There are connections between these gates and the cell.\n\n* The expression long short-term refers to the fact that LSTM is a model for the short-term memory which can last for a long period of time. An LSTM is well-suited to classify, process and predict time series given time lags of unknown size and duration between important events. LSTMs were developed to deal with the exploding and vanishing gradient problem when training traditional RNNs.\n\nSource: Wikipedia\n![image.png](attachment:image.png)","82d537d4":"# Recurrent Neural Networks\n- In a recurrent neural network we store the output activations from one or more of the layers of the network. Often these are hidden later activations. Then, the next time we feed an input example to the network, we include the previously-stored outputs as additional inputs. You can think of the additional inputs as being concatenated to the end of the \u201cnormal\u201d inputs to the previous layer. For example, if a hidden layer had 10 regular input nodes and 128 hidden nodes in the layer, then it would actually have 138 total inputs (assuming you are feeding the layer\u2019s outputs into itself \u00e0 la Elman) rather than into another layer). Of course, the very first time you try to compute the output of the network you\u2019ll need to fill in those extra 128 inputs with 0s or something.\n![image.png](attachment:image.png)\n- RNNs are quite powerful, they suffer from Vanishing gradient problem which hinders them from using long term information, like they are good for storing memory 3-4 instances of past iterations but larger number of instances don't provide good results so we don't just use regular RNNs. Instead, we use a better variation of RNNs: Long Short Term Networks(LSTM).","e63cd966":"# Components of LSTMs\nSo the LSTM cell contains the following components\n\n* Forget Gate \u201cf\u201d ( a neural network with sigmoid)\n* Candidate layer \u201cC\"(a NN with Tanh)\n* Input Gate \u201cI\u201d ( a NN with sigmoid )\n* Output Gate \u201cO\u201d( a NN with sigmoid)\n* Hidden state \u201cH\u201d ( a vector )\n* Memory state \u201cC\u201d ( a vector)\n* Inputs to the LSTM cell at any step are Xt (current input) , Ht-1 (previous hidden state ) and Ct-1 (previous memory state).\n* Outputs from the LSTM cell are Ht (current hidden state ) and Ct (current memory state)","3f8e85ef":"# Conclusion:\n- Here we can cleary see that GRU give bette results and it is more faster than LSTM.<br>\n- Bidirectional LSTM is also a good way to make the model stronger which is data sets dependant. <br>\n- Applying both LSTM and GRU together can also give even better results which would be by next experiment.\n","fcde2220":"# Scaling the datset"}}