{"cell_type":{"ff824f67":"code","d8b9706a":"code","058c74f8":"code","b3694946":"code","5094d852":"code","00563f83":"code","358dff76":"code","dfe50bd4":"code","8bf5a0d6":"code","104fd9f6":"code","ca0385ac":"code","4197ce2f":"code","59f709e7":"code","2a2bf71e":"code","e675bbc7":"code","6803f8e1":"code","4ee8253e":"code","7b4b0e83":"code","e0dfa985":"code","598144c8":"code","47dfc5e1":"code","62a91d03":"code","38160f67":"code","75483d80":"code","2f61b44d":"markdown","7e6fa32d":"markdown","8eea220b":"markdown","8ec9c0eb":"markdown","eba5ffb8":"markdown","d235ac28":"markdown","41b583c0":"markdown"},"source":{"ff824f67":"### Load all the needed libraries \n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","d8b9706a":"from glob import glob\nglob('..\/input\/home-data-for-ml-course\/*')","058c74f8":"train = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv')\ntest = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv')\nprint('Training Data Shape: {}'.format(train.shape))\nprint('Test Data Shape: {}'.format(test.shape))\ntrain.head()","b3694946":"train.info()","5094d852":"train_null = 100*train.isnull().sum()\/len(train)          ## percentage of nan values for each columns in training data\ntrain_null = train_null[train_null>0]                      ## consider columns with null values for further analysis\ntrain_null.sort_values(ascending=False).plot(kind = 'bar', \n                                             rot=90, title = 'Percentage Missing data for each column')\ntrain_null_50 = train_null[train_null>45].index.tolist()      ## get columns with more than 50% null data \n\ntest_null = 100*test.isnull().sum()\/len(test)\ntest_null = test_null[test_null>0]\ntest_null_50 = test_null[test_null>45].index.tolist()\n\nnull_to_drop = list(set(train_null_50 + test_null_50))\n\n## percentage of nan values for each columns in training data\n\nzero_data = train.isin([0]).sum().sort_values(ascending=False).head(20)\/len(train)*100      \nzero_cols_80 = zero_data[zero_data>80].index.tolist()\ndrop_cols = null_to_drop + zero_cols_80\n\nfor col in zero_data[(zero_data<80) & (zero_data>40) ].index.tolist():\n    x = train[col].value_counts()\n    zero_ppercent = zero_data.loc[col]\n    if len(x)>6 and zero_ppercent>50:\n        drop_cols.append(col)\n\ndrop_cols = list(set(drop_cols))\n\ntrain.drop(drop_cols, axis=1, inplace=True)\ntest.drop(drop_cols, axis=1, inplace=True)\nprint(train.shape , '\\n', test.shape)","00563f83":"null_data = train.isnull().sum().sort_values(ascending=False)\/len(train)*100\nnull_columns = null_data[null_data>0].index.tolist()\nprint(null_columns)\n\ntrain_des = train[null_columns].describe(include='all')   ## Null data columns statistic description ","358dff76":"fill_mode = train_des.loc['freq'][train_des.loc['freq']>900].index\ntrain[fill_mode]=train[fill_mode].fillna(train.mode().iloc[0])","dfe50bd4":"remain_cols = list(set(null_columns)-set(fill_mode))\ntrain_des = train[remain_cols].describe(include='all')\nprint(remain_cols)\ntrain_des","8bf5a0d6":"sns.violinplot(x ='LotFrontage', data=train)\nsns.violinplot(x = train[train['LotFrontage']<110]['LotFrontage'], data=train, color='r');\n\n### We can see from overlapping figure that values greater than ~ 130 are all outliers and most of the values are lying near to \n#its mean so lets fill the LotFrontage variable's nan data with its mean values","104fd9f6":"train['LotFrontage'].fillna(train['LotFrontage'].mean(), inplace=True)\ntest['LotFrontage'].fillna(test['LotFrontage'].mean(), inplace=True)\nremain_cols.remove('LotFrontage')","ca0385ac":"num_correlation = train.select_dtypes(exclude='object').corr()\nplt.figure(figsize=(10,10))\nplt.title('High Correlation')\nsns.heatmap(num_correlation>.75, annot=True, square=True)","4197ce2f":"from scipy import stats\nslope, intercept, _ , __, ___ = stats.linregress(train['YearBuilt'],train['GarageYrBlt'].fillna(train['YearBuilt']))\nprint('Slope: {} , Intercept: {}'.format(slope, intercept))\nsns.regplot(x='YearBuilt', y= 'GarageYrBlt', data=train)","59f709e7":"train['GarageYrBlt'] = train['GarageYrBlt'].fillna((train['YearBuilt']*slope+intercept).astype(int))\ntest['GarageYrBlt'] = test['GarageYrBlt'].fillna((test['YearBuilt']*slope+intercept).astype(int))\nremain_cols.remove('GarageYrBlt')\nprint(remain_cols)","2a2bf71e":"train[remain_cols] = train[remain_cols].fillna(train.mode().iloc[0])\ntest[remain_cols] = test[remain_cols].fillna(test.mode().iloc[0])","e675bbc7":"test_null = test.isnull().sum().sort_values(ascending=False)\ntest_null = test_null[test_null>0]\nprint(test_null.head(10))\ntest_data_null_cols = test_null.index.tolist()","6803f8e1":"test[test_data_null_cols] = test[test_data_null_cols].fillna(test.mode().iloc[0])","4ee8253e":"from sklearn.model_selection import train_test_split\nX = train.drop(['SalePrice'], axis=1)\ny = np.log1p(train['SalePrice'])\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=2)","7b4b0e83":"categorical_cols = [cname for cname in X.columns if\n                    X[cname].nunique() <= 30 and\n                    X[cname].dtype == \"object\"] \n                \n\n\nnumerical_cols = [cname for cname in X.columns if\n                 X[cname].dtype in ['int64','float64']]\n\n\nmy_cols = numerical_cols + categorical_cols\n\nX_train = X_train[my_cols].copy()\nX_valid = X_valid[my_cols].copy()\nX_test = test[my_cols].copy()","e0dfa985":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nnum_transformer = Pipeline(steps=[\n    ('num_imputer', SimpleImputer(strategy='constant'))\n    ])\n\ncat_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', num_transformer, numerical_cols),       \n        ('cat',cat_transformer,categorical_cols),\n        ])","598144c8":"def inv_y(transformed_y):\n    return np.exp(transformed_y)","47dfc5e1":"# from xgboost import XGBRegressor\n# from sklearn.metrics import mean_absolute_error\n# from sklearn.model_selection import GridSearchCV\n\n\n# model = XGBRegressor()\n# params = {'learning_rate':[0.01, 0.05, 0.1],\n#           'n_estimators' : [100,500, 3000],\n#           'max_depth' : [3, 6, 8],\n#           'subsample' : [0.9, 0.7, 0.5], \n#           'colsample_bytree':[0.7],\n#           'objective':['reg:squarederror'],\n#           'nthread':[-1, 1, 4],\n#           'min_child_weight':[0,4]\n# }\n\n# grid = GridSearchCV(model, param_grid=params, cv=2, n_jobs = 5, verbose=True)\n\n\n# clf = Pipeline(steps=[('preprocessor', preprocessor),\n#                           ('model', grid)])\n# clf.fit(X_train, y_train)\n# print(clf.best_params_)\n\n# predict = clf.predict(X_valid)\n# print('XGBoost: ' + str(mean_absolute_error(inv_y(predict), inv_y(y_valid))))\n","62a91d03":"from xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.ensemble import GradientBoostingRegressor\n\ndef inv_y(transformed_y):\n    return np.exp(transformed_y)\n\nn_folds = 10\n\n# XGBoost\nmodel = XGBRegressor(learning_rate=0.01, n_estimators=3460, max_depth=3, min_child_weight=0,\n                     gamma=0, subsample=0.7,colsample_bytree=0.7,objective='reg:squarederror', \n                     nthread=-1,scale_pos_weight=1, seed=27, reg_alpha=0.00006)\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                          ('model', model)])\nclf.fit(X_train, y_train)\npredict = clf.predict(X_valid)\nprint('XGBoost: ' + str(mean_absolute_error(inv_y(predict), inv_y(y_valid))))\n\n      \n# Lasso   \nmodel = LassoCV(max_iter=1e7,  random_state=14, cv=n_folds)\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                          ('model', model)])\nclf.fit(X_train, y_train)\npredict = clf.predict(X_valid)\nprint('Lasso: ' + str(mean_absolute_error(inv_y(predict), inv_y(y_valid))))\n\n      \n# GradientBoosting   \nmodel = GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, max_depth=4, random_state=5)\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                          ('model', model)])\nclf.fit(X_train, y_train)\npredict = clf.predict(X_valid)\nprint('Gradient: ' + str(mean_absolute_error(inv_y(predict), inv_y(y_valid))))","38160f67":"model = XGBRegressor(learning_rate=0.01, n_estimators=3460, max_depth=3, min_child_weight=0,gamma=0, \n                     subsample=0.7,colsample_bytree=0.7,objective='reg:squarederror', nthread=-1,scale_pos_weight=1, \n                     seed=27, reg_alpha=0.00006)\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                          ('model', model)])\nclf.fit(X, y)\npredict = clf.predict(X_test)\n# print('XGBoost: ' + str(mean_absolute_error(inv_y(predict), inv_y(y_valid))))\npredict = inv_y(predict)","75483d80":"output = pd.DataFrame({'Id': X_test['Id'],\n                       'SalePrice': predict})\n\noutput.to_csv('submission.csv', index=False)","2f61b44d":"# This Notebook will help you to understand the pros of Data exploration and handling it statistically. ","7e6fa32d":"#### After dropping the unrequired columns , find out the missing values for remaining columns and explore the data. ","8eea220b":"#### Check the path of input files","8ec9c0eb":"#### Lets take care of remaining columns with missing data","eba5ffb8":"#### Let's do some data analysis to find out the relationship\/outliers and density of the different variables ","d235ac28":"#### find out missing data \/ zero values \/ outliers and drop the columns with more than 50% null data values and more than 80% zero values (check for both train and test dataset)","41b583c0":"#### One option to fill the missing data is mode, if the frequency of a perticular value in a variable is more than 70-80% it has only 5-10% nan values than fill the nan values with its mode. "}}