{"cell_type":{"5fed4e31":"code","3608b35e":"code","8f73f823":"code","95a274b5":"code","29ea308d":"code","15261ec4":"code","cd233001":"code","4e98d49f":"code","efa03b35":"code","35092280":"code","f16974e3":"code","91610688":"code","40f17bdf":"code","c18f9cd3":"code","813bdd6f":"code","494d56e8":"code","d88040a8":"code","7b3b26c4":"code","4e560710":"code","07258537":"code","fbec4b1e":"code","68c27c73":"code","99e5a66d":"code","1b24c05e":"code","70429700":"code","a39314af":"code","19b5cfe9":"code","11f475a4":"markdown","b19a1db3":"markdown","169612f1":"markdown","98edbea8":"markdown","2337489d":"markdown","ec389ca0":"markdown","7d9fbee9":"markdown","301438dd":"markdown","1df81a1c":"markdown","dcd95b20":"markdown","a4dba38b":"markdown","9b92cb9f":"markdown","5026ccfc":"markdown","a831b3fd":"markdown","268b932e":"markdown","7c26a7c1":"markdown","e8c8f4b1":"markdown","d334a722":"markdown","332f8e93":"markdown","07f454b7":"markdown"},"source":{"5fed4e31":"%load_ext autoreload","3608b35e":"!nvcc --version","8f73f823":"\n! pip install -r \/kaggle\/input\/matterhorn-mask-rcnn-for-here-we-grow\/matterport_mask_rcnn\/requirements_both.txt\n! pip install tensorflow==1.13.1","95a274b5":"#import os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))","29ea308d":"import os\nimport sys\nimport json\nimport numpy as np\nimport time\nfrom PIL import Image, ImageDraw\nfrom pathlib import Path","15261ec4":"# Set the ROOT_DIR variable to the root directory of the Mask_RCNN git repo\nROOT_DIR = '\/kaggle\/input\/matterhorn-mask-rcnn-for-here-we-grow\/matterport_mask_rcnn\/'\nassert os.path.exists(ROOT_DIR), 'ROOT_DIR does not exist. Did you forget to read the instructions above? ;)'\n\n\n# Import mrcnn libraries\nsys.path.append(ROOT_DIR) \nfrom mrcnn.config import Config\nimport mrcnn.utils as utils\nfrom mrcnn import visualize\nimport mrcnn.model as modellib","cd233001":"# Directory to save logs and trained model\nMODEL_DIR = 'kaggle\/working'\n\n# Local path to trained weights file\nCOCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n\n# Download COCO trained weights from Releases if needed\nif not os.path.exists(COCO_MODEL_PATH):\n    utils.download_trained_weights(COCO_MODEL_PATH)","4e98d49f":"class CocoSynthConfig(Config):\n    \"\"\"Configuration for training on the box_synthetic dataset.\n    Derives from the base Config class and overrides specific values.\n    \"\"\"\n    # Give the configuration a recognizable name\n    NAME = \"cocosynth_dataset\"\n\n    # Train on 1 GPU and 1 image per GPU. Batch size is 1 (GPUs * images\/GPU).\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n\n    # Number of classes (including background)\n    NUM_CLASSES = 1 + 7  # background + 7 box types\n\n    # All of our training images are 512x512\n    IMAGE_MIN_DIM = 512\n    IMAGE_MAX_DIM = 512\n\n    # You can experiment with this number to see if it improves training\n    STEPS_PER_EPOCH = 50 #1000 -KYLE- I edited this to help troubleshoot the kernel : change it back for real training\n\n    # This is how often validation is run. If you are using too much hard drive space\n    # on saved models (in the MODEL_DIR), try making this value larger.\n    VALIDATION_STEPS = 5\n    \n    # Matterport originally used resnet101, but I downsized to fit it on my graphics card\n    BACKBONE = 'resnet50'\n\n    # To be honest, I haven't taken the time to figure out what these do\n    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)\n    TRAIN_ROIS_PER_IMAGE = 32\n    MAX_GT_INSTANCES = 50 \n    POST_NMS_ROIS_INFERENCE = 500 \n    POST_NMS_ROIS_TRAINING = 1000 \n    \nconfig = CocoSynthConfig()\nconfig.display()","efa03b35":"class CocoLikeDataset(utils.Dataset):\n    \"\"\" Generates a COCO-like dataset, i.e. an image dataset annotated in the style of the COCO dataset.\n        See http:\/\/cocodataset.org\/#home for more information.\n    \"\"\"\n    def load_data(self, annotation_json, images_dir):\n        \"\"\" Load the coco-like dataset from json\n        Args:\n            annotation_json: The path to the coco annotations json file\n            images_dir: The directory holding the images referred to by the json file\n        \"\"\"\n        # Load json from file\n        json_file = open(annotation_json)\n        coco_json = json.load(json_file)\n        json_file.close()\n        \n        # Add the class names using the base method from utils.Dataset\n        source_name = \"coco_like\"\n        for category in coco_json['categories']:\n            class_id = category['id']\n            class_name = category['name']\n            if class_id < 1:\n                print('Error: Class id for \"{}\" cannot be less than one. (0 is reserved for the background)'.format(class_name))\n                return\n            \n            self.add_class(source_name, class_id, class_name)\n        \n        # Get all annotations\n        annotations = {}\n        for annotation in coco_json['annotations']:\n            image_id = annotation['image_id']\n            if image_id not in annotations:\n                annotations[image_id] = []\n            annotations[image_id].append(annotation)\n        \n        # Get all images and add them to the dataset\n        seen_images = {}\n        for image in coco_json['images']:\n            image_id = image['id']\n            if image_id in seen_images:\n                print(\"Warning: Skipping duplicate image id: {}\".format(image))\n            else:\n                seen_images[image_id] = image\n                try:\n                    image_file_name = image['file_name']\n                    image_width = image['width']\n                    image_height = image['height']\n                except KeyError as key:\n                    print(\"Warning: Skipping image (id: {}) with missing key: {}\".format(image_id, key))\n                \n                image_path = os.path.abspath(os.path.join(images_dir, image_file_name))\n                image_annotations = annotations[image_id]\n                \n                # Add the image using the base method from utils.Dataset\n                self.add_image(\n                    source=source_name,\n                    image_id=image_id,\n                    path=image_path,\n                    width=image_width,\n                    height=image_height,\n                    annotations=image_annotations\n                )\n                \n    def load_mask(self, image_id):\n        \"\"\" Load instance masks for the given image.\n        MaskRCNN expects masks in the form of a bitmap [height, width, instances].\n        Args:\n            image_id: The id of the image to load masks for\n        Returns:\n            masks: A bool array of shape [height, width, instance count] with\n                one mask per instance.\n            class_ids: a 1D array of class IDs of the instance masks.\n        \"\"\"\n        image_info = self.image_info[image_id]\n        annotations = image_info['annotations']\n        instance_masks = []\n        class_ids = []\n        \n        for annotation in annotations:\n            class_id = annotation['category_id']\n            mask = Image.new('1', (image_info['width'], image_info['height']))\n            mask_draw = ImageDraw.ImageDraw(mask, '1')\n            for segmentation in annotation['segmentation']:\n                mask_draw.polygon(segmentation, fill=1)\n                bool_array = np.array(mask) > 0\n                instance_masks.append(bool_array)\n                class_ids.append(class_id)\n\n        mask = np.dstack(instance_masks)\n        class_ids = np.array(class_ids, dtype=np.int32)\n        \n        return mask, class_ids","35092280":"dataset_train = CocoLikeDataset()\ndataset_train.load_data('\/kaggle\/input\/cocosynth-for-here-we-grow\/cocosynth-master\/cocosynth-master\/datasets\/box_dataset_synthetic_complete\/train\/coco_instances.json',\n                        '\/kaggle\/input\/cocosynth-for-here-we-grow\/cocosynth-master\/cocosynth-master\/datasets\/box_dataset_synthetic_complete\/train\/images')\ndataset_train.prepare()\n\ndataset_val = CocoLikeDataset()\ndataset_val.load_data('\/kaggle\/input\/cocosynth-for-here-we-grow\/cocosynth-master\/cocosynth-master\/datasets\/box_dataset_synthetic_complete\/val\/coco_instances.json',\n                      '\/kaggle\/input\/cocosynth-for-here-we-grow\/cocosynth-master\/cocosynth-master\/datasets\/box_dataset_synthetic_complete\/val\/images')\ndataset_val.prepare()","f16974e3":"for name, dataset in [('training', dataset_train), ('validation', dataset_val)]:\n    print(f'Displaying examples from {name} dataset:')\n    \n    image_ids = np.random.choice(dataset.image_ids, 3)\n    for image_id in image_ids:\n        image = dataset.load_image(image_id)\n        mask, class_ids = dataset.load_mask(image_id)\n        visualize.display_top_masks(image, mask, class_ids, dataset.class_names)","91610688":"# Create model in training mode\nmodel = modellib.MaskRCNN(mode=\"training\", config=config,\n                          model_dir=MODEL_DIR)","40f17bdf":"# Which weights to start with?\ninit_with = \"coco\"  # imagenet, coco, or last\n\nif init_with == \"imagenet\":\n    model.load_weights(model.get_imagenet_weights(), by_name=True)\nelif init_with == \"coco\":\n    # Load weights trained on MS COCO, but skip layers that\n    # are different due to the different number of classes\n    # See README for instructions to download the COCO weights\n    model.load_weights(COCO_MODEL_PATH, by_name=True,\n                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n                                \"mrcnn_bbox\", \"mrcnn_mask\"])\nelif init_with == \"last\":\n    # Load the last model you trained and continue training\n    model.load_weights(model.find_last(), by_name=True)","c18f9cd3":"# Train the head branches\n# Passing layers=\"heads\" freezes all layers except the head\n# layers. You can also pass a regular expression to select\n# which layers to train by name pattern.\nstart_train = time.time()\nmodel.train(dataset_train, dataset_val, \n            learning_rate=config.LEARNING_RATE, \n            epochs=1, #4 -KYLE- I edited this to help troubleshoot the kernel : change it back for real training \n            layers='heads')\nend_train = time.time()\nminutes = round((end_train - start_train) \/ 60, 2)\nprint(f'Training took {minutes} minutes')","813bdd6f":"# Fine tune all layers\n# Passing layers=\"all\" trains all layers. You can also \n# pass a regular expression to select which layers to\n# train by name pattern.\nstart_train = time.time()\nmodel.train(dataset_train, dataset_val, \n            learning_rate=config.LEARNING_RATE \/ 10,\n            epochs=1, #8 # -KYLE- I edited this to help troubleshoot the kernel : change it back for real training\n            layers=\"all\")\nend_train = time.time()\nminutes = round((end_train - start_train) \/ 60, 2)\nprint(f'Training took {minutes} minutes')","494d56e8":"class InferenceConfig(CocoSynthConfig):\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n    IMAGE_MIN_DIM = 512\n    IMAGE_MAX_DIM = 512\n    DETECTION_MIN_CONFIDENCE = 0.85\n    \n\ninference_config = InferenceConfig()","d88040a8":"# Recreate the model in inference mode\nmodel = modellib.MaskRCNN(mode=\"inference\", \n                          config=inference_config,\n                          model_dir=MODEL_DIR)","7b3b26c4":"# Get path to saved weights\n# Either set a specific path or find last trained weights\n# model_path = str(Path(ROOT_DIR) \/ \"logs\" \/ \"box_synthetic20190328T2255\/mask_rcnn_box_synthetic_0016.h5\" )\nmodel_path = model.find_last()\n\n# Load trained weights (fill in path to trained weights here)\nassert model_path != \"\", \"Provide path to trained weights\"\nprint(\"Loading weights from \", model_path)\nmodel.load_weights(model_path, by_name=True)","4e560710":"import skimage\n\nreal_test_dir = '\/kaggle\/input\/cocosynth-for-here-we-grow\/cocosynth-master\/cocosynth-master\/datasets\/box_dataset_synthetic_complete\/test\/images'\nimage_paths = []\nfor filename in os.listdir(real_test_dir):\n    if os.path.splitext(filename)[1].lower() in ['.png', '.jpg', '.jpeg']:\n        image_paths.append(os.path.join(real_test_dir, filename))\n\nfor image_path in image_paths:\n    img = skimage.io.imread(image_path)\n    img_arr = np.array(img)\n    results = model.detect([img_arr], verbose=1)\n    r = results[0]\n    visualize.display_instances(img, r['rois'], r['masks'], r['class_ids'], \n                                dataset_train.class_names, r['scores'], figsize=(8,8))","07258537":"video_file = Path(\"\/kaggle\/input\/cocosynth-for-here-we-grow\/cocosynth-master\/cocosynth-master\/datasets\/box_dataset_synthetic_complete\/videotest\/boxvideo_24fps.mp4\")\nvideo_save_dir = Path(\"kaggle\/working\/\")\nvideo_save_dir.mkdir(exist_ok=True)","fbec4b1e":"class VideoInferenceConfig(CocoSynthConfig):\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n    IMAGE_MIN_DIM = 1088\n    IMAGE_MAX_DIM = 1920\n    IMAGE_SHAPE = [1920, 1080, 3]\n    DETECTION_MIN_CONFIDENCE = 0.80\n    \n\ninference_config = VideoInferenceConfig()","68c27c73":"# Recreate the model in inference mode\nmodel = modellib.MaskRCNN(mode=\"inference\", \n                          config=inference_config,\n                          model_dir=MODEL_DIR)","99e5a66d":"# Get path to saved weights\n# Either set a specific path or find last trained weights\n# model_path = str(Path(ROOT_DIR) \/ \"logs\" \/ \"box_synthetic20190328T2255\/mask_rcnn_box_synthetic_0016.h5\" )\nmodel_path = model.find_last()\n\n# Load trained weights (fill in path to trained weights here)\nassert model_path != \"\", \"Provide path to trained weights\"\nprint(\"Loading weights from \", model_path)\nmodel.load_weights(model_path, by_name=True)","1b24c05e":"import cv2\nimport skimage\nimport random\nimport colorsys\nfrom tqdm import tqdm","70429700":"def random_colors(N, bright=True):\n    \"\"\" Generate random colors. \n        To get visually distinct colors, generate them in HSV space then\n        convert to RGB.\n    Args:\n        N: the number of colors to generate\n        bright: whether or not to use bright colors\n    Returns:\n        a list of RGB colors, e.g [(0.0, 1.0, 0.0), (1.0, 0.0, 0.5), ...]\n    \"\"\"\n    brightness = 1.0 if bright else 0.7\n    hsv = [(i \/ N, 1, brightness) for i in range(N)]\n    colors = list(map(lambda c: colorsys.hsv_to_rgb(*c), hsv))\n    random.shuffle(colors)\n    return colors\n\ndef apply_mask(image, mask, color, alpha=0.5):\n    \"\"\" Apply the given mask to the image.\n    Args:\n        image: a cv2 image\n        mask: a mask of which pixels to color\n        color: the color to use\n        alpha: how visible the mask should be (0 to 1)\n    Returns:\n        a cv2 image with mask applied\n    \"\"\"\n    for c in range(3):\n        image[:, :, c] = np.where(mask == 1,\n                                  image[:, :, c] *\n                                  (1 - alpha) + alpha * color[c] * 255,\n                                  image[:, :, c])\n    return image\n\ndef display_instances(image, boxes, masks, ids, names, scores, colors):\n    \"\"\" Take the image and results and apply the mask, box, and label\n    Args:\n        image: a cv2 image\n        boxes: a list of bounding boxes to display\n        masks: a list of masks to display\n        ids: a list of class ids\n        names: a list of class names corresponding to the ids\n        scores: a list of scores of each instance detected\n        colors: a list of colors to use\n    Returns:\n        a cv2 image with instances displayed   \n    \"\"\"\n    n_instances = boxes.shape[0]\n\n    if not n_instances:\n        return image # no instances\n    else:\n        assert boxes.shape[0] == masks.shape[-1] == ids.shape[0]\n\n    for i, color in enumerate(colors):\n        # Check if any boxes to show\n        if not np.any(boxes[i]):\n            continue\n        \n        # Check if any scores to show\n        if scores is not None:\n            score = scores[i] \n        else:\n            score = None\n\n        # Add the mask\n        image = apply_mask(image, masks[:, :, i], color)\n        \n        # Add the bounding box\n        y1, x1, y2, x2 = boxes[i]\n        image = cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n        \n        # Add the label\n        label = names[ids[i]]\n        if score:\n            label = f'{label} {score:.2f}'\n            \n        label_pos = (x1 + (x2 - x1) \/\/ 2, y1 + (y2 - y1) \/\/ 2) # center of bounding box\n        image = cv2.putText(image, label, label_pos, cv2.FONT_HERSHEY_DUPLEX, 0.7, color, 2)\n\n    return image","a39314af":"video_file = Path(\"\/kaggle\/input\/cocosynth-for-here-we-grow\/cocosynth-master\/cocosynth-master\/datasets\/box_dataset_synthetic_complete\/\/videotest\/boxvideo_24fps.mp4\")\nvideo_save_dir = Path(\"kaggle\/working\/\")\nvideo_save_dir.mkdir(exist_ok=True)\nvid_name = video_save_dir \/ \"output.mp4\"\nv_format=\"FMP4\"\nfourcc = cv2.VideoWriter_fourcc(*v_format)\n\nprint('Writing output video to: ' + str(vid_name))","19b5cfe9":"#colors = random_colors(30)\ncolors = [(1.0, 1.0, 0.0)] * 30\n\n# Change color representation from RGB to BGR before displaying instances\ncolors = [(color[2], color[1], color[0]) for color in colors]","11f475a4":"### Set Up Model and Load Trained Weights","b19a1db3":"# Create the Training and Validation Datasets\nMake sure you link to the correct locations for your training dataset in the cell below.","169612f1":"input_video = cv2.VideoCapture(str(video_file))\nframe_count = int(input_video.get(cv2.CAP_PROP_FRAME_COUNT))\nfps = int(input_video.get(cv2.CAP_PROP_FPS))\noutput_video = None\nvid_size = None\ncurrent_frame = 0\n\nfor i in tqdm(range(frame_count)):\n    # Read the current frame\n    ret, frame = input_video.read()\n    if not ret:\n        break\n        \n    current_frame += 1\n    \n    # Change color representation from BGR to RGB before running model.detect()\n    detect_frame = frame[:, :, ::-1]        \n    \n    # Run inference on the color-adjusted frame\n    results = model.detect([detect_frame], verbose=0)\n    r = results[0]\n    n_instances = r['rois'].shape[0]\n    \n    # Make sure we have enough colors\n    if len(colors) < n_instances:\n        # not enough colors, generate more\n        more_colors = random_colors(n_instances - len(colors))\n        \n        # Change color representation from RGB to BGR before displaying instances\n        more_colors = [(color[2], color[1], color[0]) for color in more_colors]\n        colors += more_colors\n        \n    \n    \n    # Display instances on the original frame\n    display_frame = display_instances(frame, r['rois'], r['masks'], r['class_ids'], \n                                dataset_train.class_names, r['scores'], colors[0:n_instances])\n\n    # Make sure we got displayed instances\n    if display_frame is not None:\n        frame = display_frame\n\n    # Create the output_video if it doesn't yet exist\n    if output_video is None:\n        if vid_size is None:\n            vid_size = frame.shape[1], frame.shape[0]\n        output_video = cv2.VideoWriter(str(vid_name), fourcc, float(fps), vid_size, True)\n        \n    # Resize frame if necessary\n    if vid_size[0] != frame.shape[1] and vid_size[1] != frame.shape[0]:\n        frame = cv2.resize(frame, vid_size)\n    \n    # Write the frame to the output_video\n    output_video.write(frame)\n    \ninput_video.release()\noutput_video.release()","98edbea8":"# Prepare to run Inference\nCreate a new InferenceConfig, then use it to create a new model.","2337489d":"## Display a few images from the train and val datasets\nThis will just make sure everything is set up correctly","ec389ca0":"Random colors can be a bit intense with video, so I've set it to use the same color for each object. You can try random colors if you like.\n\nNote: cv2 uses BGR (Blue Green Red) color representation instead of RGB (Red Green Blue), so we have to do a couple conversions here","7d9fbee9":"### Load Trained Weights\nNote: The code is set to find_last() by default, but you can also point the model to a specific pretrained weights file if you use line 3 instead of line 4.","301438dd":"# Define the dataset\nI've attempted to make this generic to any COCO-like dataset. That means if you have another dataset defined in the COCO format, it should work.","1df81a1c":"# Video Inference\n## Prepare to run Video Inference\nNote: This code is adapted from https:\/\/www.dlology.com\/blog\/how-to-run-object-detection-and-segmentation-on-video-fast-for-free\/ created by Chengwei Zhang.\n\nIn this section, we'll open up a video, run inference on it, and output a new video with segmentations and labels.","dcd95b20":"# Course material from \"complete guide to creating COCO datasets\" by Adam Kelly\n\nCourse can be found here:\nhttps:\/\/www.immersivelimit.com\/courses\n\nImported for the Here We Grow! community learning project for SingularityNet.  Virtually all of the code here is borrowed from both of these repos from github : \n\nhttps:\/\/github.com\/akTwelve\/cocosynth\nhttps:\/\/github.com\/matterport\/Mask_RCNN\n","a4dba38b":"### Prepare for Inference\nAdjust the paths below to point to your video","9b92cb9f":"# Configuration\nDefine configurations for training on the box_dataset_synthetic dataset.\n- Look through the code cell below and update any lines relevant to your custom dataset.\n- You may want to change:\n    - NAME (might want to be more specific)\n    - NUM_CLASSES (always 1 + the number of object categories you have)\n    - IMAGE_MIN_DIM (if you have larger training images)\n    - IMAGE_MAX_DIM (if you have larger training images)\n    - STEPS_PER_EPOCH (if you want to train on more images each epoch)\n\n### Note\nThese are settings that worked on my machine (GTX 970 graphics card). If you are getting OOM (Out of Memory) errors, you may need to tweak the settings or your computer may not be powerful enough. If you have a better graphics card, you will want to tweak it to take advantage of that.","5026ccfc":"## Create the Training Model and Train\nThis code is largely borrowed from the train_shapes.ipynb notebook in the Matterport repo.","a831b3fd":"# Mask R-CNN Training and Inference\nIn this notebook we use Matterport's implementation of Mask R-CNN to train on our synthetic dataset, then use the trained weights to run inference on new images.\n\nIf you've never trained a neural network before, I wouldn't recommend starting here. Find a beginner deep learning tutorial\/course and start there. I'd suggest the free course at https:\/\/course.fast.ai\/. There are also lots of great free tutorials on YouTube, as well as paid courses on Udemy, Udacity, Coursera, etc.\n\n## TensorFlow GPU\nYou definitely want TensorFlow GPU installed to run this notebook. Installation instructions are here:\nhttps:\/\/www.tensorflow.org\/install\/gpu\n\nThis includes installing CUDA, which is no small task. If you want to do serious image recognition, you just have to push through the pain.\n\nWhen I created this notebook, I updated my graphics driver to the latest, then installed the CUDA Toolkit and CUDNN. Tensorflow tends to not work if you have the wrong combo of CUDA\/CUDNN, so I recommend checking out the [tested build configurations](https:\/\/www.tensorflow.org\/install\/source#tested_build_configurations). It took me a few frustrating hours of searching and trial and error to get a combination that worked.\n\n## Keras\nIt seems that the latest versions of Keras aren't compatible with the Mask R-CNN code. Using version 2.2.4 worked for me and other students.\n\n## My Combo that Worked\n- **Python 3.6** (conda create -n tf-gpu python=3.6)\n- **CUDA Toolkit 10.0** [https:\/\/developer.nvidia.com\/cuda-zone](https:\/\/developer.nvidia.com\/cuda-zone)\n- **CUDNN 7.4.1** [https:\/\/developer.nvidia.com\/cudnn](https:\/\/developer.nvidia.com\/cudnn)\n- **tensorflow-gpu 1.13.1** (pip install tensorflow-gpu==1.13.1)\n- **keras 2.2.4** (pip install keras==2.2.4)","268b932e":"### Adjust Config Parameters\nDepending on the resolution of your video, you may want to update the parameters below.\n\nMask R-CNN requires IMAGE_MIN_DIM to be divisible by 2, six times, so that's why I'm using 1088 instead of 1080, which is the actual height of the video.","7c26a7c1":"# Import Matterport's \"mrcnn\" libraries\nI'm using Matterport's Mask_RCNN git repo: https:\/\/github.com\/matterport\/Mask_RCNN\n\n- Clone the Mask_RCNN repo to your computer.\n- Update the code cell below to point to the root directory of the repo.\n- Use pip to install everything from the requirements.txt file in that repo.\n\nRead through their documentation and issues if you have any trouble getting started.","e8c8f4b1":"# Image Inference\nRun model.detect() on real images.\n\nWe get some false positives, and some misses. More training images are likely needed to improve the results.\n\nYou can adjust the \"figsize\" in the cell below to make the image larger or smaller.","d334a722":"## Training\nTrain in two stages:\n\n1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass layers='heads' to the train() function.\n\n2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass layers='all' to train all layers.","332f8e93":"### Perform Inference on Video\nUse the cv2 module to open the video and perform inference on each frame.\n\n-KYLE- changed the code cell below to a markdown cell for troubleshooting purposes.  Once GPU issue is resolved I will change it back.","07f454b7":"# Set up logging and pre-trained model paths\nThis will default to sub-directories in your mask_rcnn_dir, but if you want them somewhere else, update it here.\n\nIt will also download the pre-trained coco model."}}