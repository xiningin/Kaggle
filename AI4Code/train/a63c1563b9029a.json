{"cell_type":{"97dfbd43":"code","629e0666":"code","b714199b":"code","6937a71a":"code","5e041b97":"code","192fe7e1":"code","5ad3f2f3":"code","426f169b":"code","713cc7d3":"code","1d77c5bc":"code","3871d343":"code","fca45fc0":"code","f929f806":"code","a2e1b9ac":"code","bd6514db":"code","73ec9275":"code","915e5408":"code","598a1c12":"code","c9697794":"code","22268405":"code","934d4197":"code","76a662b4":"code","2818ff77":"code","64d50b52":"code","16f758bb":"code","74922a8c":"code","659e1b64":"markdown","13fc7b95":"markdown","904d523a":"markdown","bb3e9aa3":"markdown","2212aed9":"markdown","016ca2cc":"markdown","90ef44d3":"markdown","cb21fd75":"markdown","227d3624":"markdown","ae57fd1c":"markdown"},"source":{"97dfbd43":"import numpy as np \nimport pandas as pd\nimport plotly as py\nfrom statistics import mean\nimport plotly.graph_objs as go\nimport plotly.express as px\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected = True)\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom umap import UMAP\n\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nfrom sklearn.ensemble import VotingClassifier\n\nimport optuna\n\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\npd.set_option('display.max_columns', None)\n#########################################################\ntrain = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\nss = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')","629e0666":"train.head(3)","b714199b":"cols = train.drop(['id', 'claim'], axis = 1).columns.tolist()\ndef info(data):\n    \n    print(f'Length of data: {len(data)}')\n    \n    print('')\n    \n    x = pd.Series([])\n    for i in data.columns.tolist():\n        x = x.append(pd.Series([data[i].dtypes]))\n    \n    print(x.value_counts().to_frame().reset_index().rename(columns={0: 'count', 'index': 'type'}))\n    \n    print('')\n    \n    flag = True\n    for i in cols:\n        if data[i].isna().sum() == 0:\n            flag = False\n            break\n            \n    print(f'All features have missing values: {flag}')\n    \n    list_na = []\n    for i in cols:\n        list_na.append(data[i].isna().sum())\n    print(f'Mean of missing values is {mean(list_na)} ({round((mean(list_na)\/len(data)) * 100,2)}%)')\n    print(f'Max of missing values has {cols[list_na.index(max(list_na))]}: {max(list_na)} ({round((max(list_na)\/len(data)) * 100,2)}%)')\n    print(f'Min of missing values has {cols[list_na.index(min(list_na))]}: {min(list_na)} ({round((min(list_na)\/len(data)) * 100,2)}%)')\n\nprint('TRAINING DATASET INFORMATION')\nprint('')\ninfo(train)\nprint('---------------------------------------------')\nprint('TEST DATASET INFORMATION')\nprint('')\ninfo(test)","6937a71a":"fig = plt.figure(figsize = (15, 71))\ncols = train.columns.tolist()[1:119]\nfor i in cols:\n    plt.subplot(24,5,cols.index(i)+1)\n    sns.set_style(\"white\")\n    plt.title(i, size = 12, fontname = 'monospace')\n    a = sns.kdeplot(train[i], color = '#f9ba32', linewidth = 1.3)\n    sns.kdeplot(test[i], color = '#426e86', linewidth = 1.3)\n    plt.ylabel('')\n    plt.xlabel('')\n    plt.xticks(fontname = 'monospace')\n    plt.yticks([])\n    for j in ['right', 'left', 'top']:\n        a.spines[j].set_visible(False)\n        a.spines['bottom'].set_linewidth(1.2)\n        \nfig.tight_layout(h_pad = 3)\n\nplt.figtext(0.335, 1.02, 'Distribution of features', color = '#2f3131', fontname = 'monospace', size = 25)\nplt.figtext(0.3, 1.01, 'train', color = '#f9ba32', fontname = 'monospace', size = 18)\nplt.figtext(0.66, 1.01, 'test', color = '#426e86', fontname = 'monospace', size = 18)\n\nplt.show()","5e041b97":"plt.figure(figsize = (15, 5))\nsns.set_style(\"white\")\nplt.title('Distribution of target', fontname = 'monospace', fontsize = 35, color = '#32384D', x = 0.5, y = 1.05)\na = sns.countplot(y = train['claim'], palette = (['#E29930', '#217CA3']))\na.set_yticklabels(['No claim', 'Claim'])\nplt.axhline(0.5, 0, 0.951, color = '#211F30')\nplt.xticks([])\nplt.yticks(fontname = 'monospace', fontsize = 18)\nplt.ylabel('')\nplt.xlabel('')\n\na.text(210000, 0.05, '50.2%', fontname = 'monospace', fontsize = 40, color = 'white')\na.text(210000, 1.05, '49.8%', fontname = 'monospace', fontsize = 40, color = 'white')\na.text(215000, 0.3, '(480 404)', fontname = 'monospace', fontsize = 20, color = 'white')\na.text(215000, 1.3, '(477 515)', fontname = 'monospace', fontsize = 20, color = 'white')\n\na.spines['left'].set_linewidth(1.5)\nfor w in ['right', 'top', 'bottom']:\n    a.spines[w].set_visible(False)\n        \nplt.show()","192fe7e1":"matrix = np.triu(train.drop('id', axis = 1).corr())\nplt.figure(figsize = (15, 12))\nsns.heatmap(train.drop('id', axis = 1).corr(), annot = False, cmap = 'Spectral', mask = matrix, vmin = -0.05, vmax = 0.05, linewidths = 0.1, linecolor = 'white', cbar = True)\nplt.xticks(size = 8, fontname = 'monospace')\nplt.yticks(size = 8, fontname = 'monospace')\nplt.figtext(0.77, 0.8, '''All 118 features and the target variable\nhave a very small\ncorrelation''', fontsize = 20, fontname = 'monospace', ha = 'right', color = '#f9ba32')\nplt.show()","5ad3f2f3":"corr = train.drop('id', axis = 1).corr()['claim'].reset_index().drop(index=[118])\nmin_corr = corr.min()[1]\nmax_corr = corr.max()[1]\ncorr.query(\"claim == @min_corr | claim == @max_corr\").rename(columns = {'index': 'feature'}).rename(index = {33: 'max_neg_correlation', 94: 'max_pos_correlation'})","426f169b":"features = train.columns.tolist()[1:119]\n\ntrain['n_missing'] = train[features].isna().sum(axis = 1)\ntest['n_missing'] = test[features].isna().sum(axis = 1)\n\ntrain['std'] = train[features].std(axis = 1)\ntest['std'] = test[features].std(axis = 1)\n\nfeatures += ['n_missing', 'std']\n\nimputer = SimpleImputer(strategy = 'mean')\nfor i in features:\n    train[i] = imputer.fit_transform(np.array(train[i]).reshape(-1,1))\n    test[i] = imputer.transform(np.array(test[i]).reshape(-1,1))\n\nsc = StandardScaler()\ntrain[features] = sc.fit_transform(train[features])\ntest[features] = sc.transform(test[features])\n\nX = train.drop(['id', 'claim'], axis = 1)\ny = train['claim']\ntest.drop('id', axis = 1, inplace = True)","713cc7d3":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","1d77c5bc":"reduce_mem_usage(train)\nreduce_mem_usage(X)\nreduce_mem_usage(test)","3871d343":"#umap = UMAP(n_components = 2, n_neighbors = 10, min_dist = 0.99).fit_transform(train.drop(['id', 'claim'], axis = 1).sample(150000, random_state = 228), train['claim'].sample(150000, random_state = 228))\n\nplt.figure(figsize=(15, 12))\nscu = sns.scatterplot(x = umap[:, 0], y = umap[:, 1], hue = train['claim'].sample(150000, random_state = 228), palette = ['#f9ba32','#426e86'], s = 5, edgecolor = 'none', alpha = 0.4)\nplt.xticks([])\nplt.yticks([])\nfor i in ['right', 'left', 'top', 'bottom']:\n    scu.spines[i].set_visible(False)\nplt.legend(ncol = 2, borderpad = 1, frameon = True, fontsize = 11)\nscu.text(-4.6, 6.4, '''n_components = 2\nn_neighbors = 10\nmin_dist = 0.99''', fontname = 'monospace', fontsize = 12, color = 'black')\nplt.show()","fca45fc0":"def objective(trial, data = X, target = y):\n\n    params = {\n        'max_depth': trial.suggest_int('max_depth', 2, 8),\n        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1),\n        'n_estimators': trial.suggest_int('n_estimators', 10000, 50000),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 500),\n        'gamma': trial.suggest_float('gamma', 0.0001, 1.0, log = True),\n        'alpha': trial.suggest_float('alpha', 0.0001, 10.0, log = True),\n        'lambda': trial.suggest_float('lambda', 0.0001, 10.0, log = True),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.8),\n        'subsample': trial.suggest_float('subsample', 0.1, 0.8),\n        'tree_method': 'gpu_hist',\n        'booster': 'gbtree',\n        'random_state': 228,\n        'use_label_encoder': False,\n        'eval_metric': 'auc'\n    }\n    \n    model = XGBClassifier(**params)\n    scores = []\n    k = StratifiedKFold(n_splits = 2, random_state = 228, shuffle = True)\n    for i, (trn_idx, val_idx) in enumerate(k.split(X, y)):\n        \n        X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n        model.fit(X_train, y_train, eval_set = [(X_val, y_val)], early_stopping_rounds = 300, verbose = False)\n        \n        tr_preds = model.predict_proba(X_train)[:,1]\n        tr_score = roc_auc_score(y_train, tr_preds)\n        \n        val_preds = model.predict_proba(X_val)[:,1]\n        val_score = roc_auc_score(y_val, val_preds)\n\n        scores.append((tr_score, val_score))\n        \n        print(f\"Fold {i+1} | AUC: {val_score}\")\n        \n        \n    scores = pd.DataFrame(scores, columns = ['train score', 'validation score'])\n    \n    return scores['validation score'].mean()\n\nstudy = optuna.create_study(direction = 'maximize')\nstudy.optimize(objective, n_trials = 300)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best value:', study.best_value)","f929f806":"# Mean AUC on 2 folds - 0.81532\n# esr - 300\nparamsXGB = {'max_depth': 3, 'learning_rate': 0.005702659398906191, 'n_estimators': 22404, 'min_child_weight': 25, 'gamma': 0.00010151247994797229, 'alpha': 7.148020356730985, 'lambda': 0.1378423649746119, 'colsample_bytree': 0.7969227570988136, 'subsample': 0.6382893449313995,\n             'tree_method': 'gpu_hist',\n             'booster': 'gbtree',\n             'random_state': 228,\n             'use_label_encoder': False,\n             'eval_metric': 'auc'}\n# Solo result - 0.81771","a2e1b9ac":"folds = StratifiedKFold(n_splits = 5, random_state = 228, shuffle = True)\npredictions = np.zeros(len(test))\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n    \n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n    model = XGBClassifier(**paramsXGB)\n   \n    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], verbose = False, early_stopping_rounds = 300)\n    \n    predictions += model.predict_proba(test)[:,1] \/ folds.n_splits ","bd6514db":"ss['claim'] = predictions\nss.to_csv('xgb1', index = False)","73ec9275":"def objective(trial, data = X, target = y):\n    \n    params = {\n        'depth': trial.suggest_int('depth', 2, 6),\n        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2),\n        'iterations': trial.suggest_int('iterations', 10000, 50000),\n        'max_bin': trial.suggest_int('max_bin', 1, 300),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 300),\n        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.0001, 1.0, log = True),\n        'subsample': trial.suggest_float('subsample', 0.1, 0.8),\n        'grow_policy': trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise', 'Lossguide']),\n        'leaf_estimation_method': trial.suggest_categorical('leaf_estimation_method', ['Newton', 'Gradient']),\n        'bootstrap_type': 'Bernoulli',\n        'random_seed': 228,\n        'loss_function': 'Logloss',\n        'eval_metric': 'AUC',\n        'task_type': 'GPU'\n    }\n    \n    model = CatBoostClassifier(**params)\n    scores = []\n    k = StratifiedKFold(n_splits = 2, random_state = 228, shuffle = True)\n    for i, (trn_idx, val_idx) in enumerate(k.split(X, y)):\n        \n        X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n        model.fit(X_train, y_train, eval_set = [(X_val, y_val)], early_stopping_rounds = 300, verbose = False)\n        \n        tr_preds = model.predict_proba(X_train)[:,1]\n        tr_score = roc_auc_score(y_train, tr_preds)\n        \n        val_preds = model.predict_proba(X_val)[:,1]\n        val_score = roc_auc_score(y_val, val_preds)\n\n        scores.append((tr_score, val_score))\n        \n        print(f\"Fold {i+1} | AUC: {val_score}\")\n        \n        \n    scores = pd.DataFrame(scores, columns = ['train score', 'validation score'])\n    \n    return scores['validation score'].mean()\n\nstudy = optuna.create_study(direction = 'maximize')\nstudy.optimize(objective, n_trials = 300)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best value:', study.best_value)","915e5408":"# Mean AUC on 2 folds - 0.81518\n# esr - 300\nparamsCB = {'depth': 3, 'learning_rate': 0.017585381726501453, 'iterations': 11636, 'max_bin': 461, 'min_data_in_leaf': 162, 'l2_leaf_reg': 0.02724781040038058, 'subsample': 0.6892384815879177, 'grow_policy': 'Depthwise', 'leaf_estimation_method': 'Gradient',\n            'bootstrap_type': 'Bernoulli',\n            'random_seed': 228,\n            'loss_function': 'Logloss',\n            'eval_metric': 'AUC',\n            'task_type': 'GPU' }\n# Solo result - 0.81770","598a1c12":"folds = StratifiedKFold(n_splits = 5, random_state = 228, shuffle = True)\npredictions = np.zeros(len(test))\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n    \n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n    model = CatBoostClassifier(**paramsCB)\n   \n    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], verbose = False, early_stopping_rounds = 300)\n    \n    predictions += model.predict_proba(test)[:,1] \/ folds.n_splits ","c9697794":"ss['claim'] = predictions\nss.to_csv('cb1', index = False)","22268405":"def objective(trial, data = X, target = y):\n\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 1000, 15000),\n        'max_depth': trial.suggest_int('max_depth', 2, 3),\n        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2),\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 10.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 50, 500),\n        'min_data_per_group': trial.suggest_int('min_data_per_group', 50, 200),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 200),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.8),\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n        'random_state': 228,\n        'metric': 'auc',\n        'device_type': 'gpu',\n        'gpu_platform_id': 0,\n        'gpu_device_id': 0\n    }\n    \n    model = LGBMClassifier(**params)\n    scores = []\n    k = StratifiedKFold(n_splits = 2, random_state = 228, shuffle = True)\n    for i, (trn_idx, val_idx) in enumerate(k.split(X, y)):\n        \n        X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n        y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n        model.fit(X_train, y_train, eval_set = [(X_val, y_val)], early_stopping_rounds = 300, verbose = False)\n        \n        tr_preds = model.predict_proba(X_train)[:,1]\n        tr_score = roc_auc_score(y_train, tr_preds)\n        \n        val_preds = model.predict_proba(X_val)[:,1]\n        val_score = roc_auc_score(y_val, val_preds)\n\n        scores.append((tr_score, val_score))\n        \n        print(f\"Fold {i+1} | AUC: {val_score}\")\n        \n        \n    scores = pd.DataFrame(scores, columns = ['train score', 'validation score'])\n    \n    return scores['validation score'].mean()\n\nstudy = optuna.create_study(direction = 'maximize')\nstudy.optimize(objective, n_trials = 100)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best value:', study.best_value)","934d4197":"# Mean AUC on 2 folds - 0.8151\nparamsLGBM = {'n_estimators': 11990, 'max_depth': 3, 'learning_rate': 0.016501612373246877, 'reg_alpha': 7.555087388180319, 'reg_lambda': 0.9534606245427513, 'num_leaves': 155, 'min_data_per_group': 177, 'min_child_samples': 150, 'colsample_bytree': 0.22781593823447946,\n            'boosting_type': 'gbdt',\n            'objective': 'binary',\n            'random_state': 228,\n            'metric': 'auc',\n            'device_type': 'gpu',\n            'gpu_platform_id': 0,\n            'gpu_device_id': 0}\n# Solo result - 0.81795","76a662b4":"folds = StratifiedKFold(n_splits = 5, random_state = 228, shuffle = True)\npredictions = np.zeros(len(test))\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n    \n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n    model = LGBMClassifier(**paramsLGBM)\n   \n    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], verbose = False, early_stopping_rounds = 300)\n    \n    predictions += model.predict_proba(test)[:,1] \/ folds.n_splits ","2818ff77":"ss['claim'] = predictions\nss.to_csv('lgbm1', index = False)","64d50b52":"xgb_model = XGBClassifier(**paramsXGB)\ncb_model = CatBoostClassifier(**paramsCB)\nlgbm_model = LGBMClassifier(**paramsLGBM)\n\n# XGB solo result - 0.81771\n# CB solo result - 0.81770\n# LGBM solo result - 0.81795","16f758bb":"folds = StratifiedKFold(n_splits = 5, random_state = 228, shuffle = True)\n\npredictions = np.zeros(len(test))\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n    \n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n    model = VotingClassifier(\n            estimators = [\n                ('xgb', xgb_model),\n                ('cb', cb_model),\n                ('lgbm', lgbm_model)       \n            ],\n            voting = 'soft',\n            weights = [0.3, 0.3, 0.4],\n            n_jobs = -1\n        )\n   \n    model.fit(X_train, y_train)\n    \n    predictions += model.predict_proba(test)[:,1] \/ folds.n_splits\n    \nss['claim'] = predictions","74922a8c":"ss.to_csv('voting', index = False)\nss","659e1b64":"# XGB","13fc7b95":"Memory optimization. It would be nice to convert the data to float16, but the XGB for some reason does not support this format. The function is taken from [here](https:\/\/www.kaggle.com\/rinnqd\/reduce-memory-usage)","904d523a":"# Preprocessing","bb3e9aa3":"**Results:**\n\n1. XGB solo result - 0.81771\n2. CB solo result - 0.81770\n3. LGBM solo result - 0.81795\n4. Voting result - 0.81791","2212aed9":"# LGBM","016ca2cc":"# Basic information","90ef44d3":"# Voting time","cb21fd75":"# EDA","227d3624":"# CatBoost","ae57fd1c":"# UMAP"}}