{"cell_type":{"c60e88fe":"code","99fcdf7f":"code","d14ce373":"code","05581a11":"code","e4eb9c07":"code","98e92328":"code","d8b6b5e4":"code","df6d3b46":"code","530b4d3a":"code","84442aa7":"code","b9f2bfc8":"code","6e616f10":"code","1748fb93":"code","3292ebfd":"markdown","bc32e075":"markdown","5480807c":"markdown","c74e1ebe":"markdown","3ed114cf":"markdown","01bda181":"markdown","8d8f31c0":"markdown","f2a30ea4":"markdown","c1b91cf7":"markdown","2a5a0217":"markdown","bd958527":"markdown","2c90e795":"markdown","ec4e78b8":"markdown","3de4912a":"markdown","db8bd91c":"markdown","9bffb694":"markdown","bafc5726":"markdown","156e2893":"markdown","ce062528":"markdown"},"source":{"c60e88fe":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\n#for data preprocessing\nfrom sklearn.decomposition import PCA\n\n#for modeling\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.ensemble import IsolationForest\n\n#filter warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","99fcdf7f":"df = pd.read_csv(\"..\/input\/creditcard.csv\")\ndf.head()","d14ce373":"sns.countplot(df.Class)\nplt.show()\nprint(df.Class.value_counts())","05581a11":"timedelta = pd.to_timedelta(df['Time'], unit='s')\ndf['Time_hour'] = (timedelta.dt.components.hours).astype(int)\n\nplt.figure(figsize=(12,5))\nsns.distplot(df[df['Class'] == 0][\"Time_hour\"], color='g')\nsns.distplot(df[df['Class'] == 1][\"Time_hour\"], color='r')\nplt.title('Fraud and Normal Transactions by Hours', fontsize=17)\nplt.xlim([-1,25])\nplt.show()","e4eb9c07":"cols= df[['Time', 'Amount']]\n\npca = PCA()\npca.fit(cols)\nX_PCA = pca.transform(cols)\n\ndf['V29']=X_PCA[:,0]\ndf['V30']=X_PCA[:,1]\n\ndf.drop(['Time','Time_hour', 'Amount'], axis=1, inplace=True)\n\ndf.columns","98e92328":"columns = df.drop('Class', axis=1).columns\ngrid = gridspec.GridSpec(6, 5)\n\nplt.figure(figsize=(20,10*2))\n\nfor n, col in enumerate(df[columns]):\n    ax = plt.subplot(grid[n])\n    sns.distplot(df[df.Class==1][col], bins = 50, color='g')\n    sns.distplot(df[df.Class==0][col], bins = 50, color='r') \n    ax.set_ylabel('Density')\n    ax.set_title(str(col))\n    ax.set_xlabel('')\n    \nplt.show()","d8b6b5e4":"def ztest(feature):\n    \n    mean = normal[feature].mean()\n    std = fraud[feature].std()\n    zScore = (fraud[feature].mean() - mean) \/ (std\/np.sqrt(sample_size))\n    \n    return zScore","df6d3b46":"columns= df.drop('Class', axis=1).columns\nnormal= df[df.Class==0]\nfraud= df[df.Class==1]\nsample_size=len(fraud)\nsignificant_features=[]\ncritical_value=2.58\n\nfor i in columns:\n    \n    z_vavlue=ztest(i)\n    \n    if( abs(z_vavlue) >= critical_value):    \n        print(i,\" is statistically significant\") #Reject Null hypothesis. i.e. H0\n        significant_features.append(i)","530b4d3a":"significant_features.append('Class')\ndf= df[significant_features]\n\ninliers = df[df.Class==0]\nins = inliers.drop(['Class'], axis=1)\n\noutliers = df[df.Class==1]\nouts = outliers.drop(['Class'], axis=1)\n\nins.shape, outs.shape","84442aa7":"def normal_accuracy(values):\n    \n    tp=list(values).count(1)\n    total=values.shape[0]\n    accuracy=np.round(tp\/total,4)\n    \n    return accuracy\n\ndef fraud_accuracy(values):\n    \n    tn=list(values).count(-1)\n    total=values.shape[0]\n    accuracy=np.round(tn\/total,4)\n    \n    return accuracy","b9f2bfc8":"state= 42\n\nISF = IsolationForest(random_state=state)\nISF.fit(ins)\n\nnormal_isf = ISF.predict(ins)\nfraud_isf = ISF.predict(outs)\n\nin_accuracy_isf=normal_accuracy(normal_isf)\nout_accuracy_isf=fraud_accuracy(fraud_isf)\nprint(\"Accuracy in Detecting Normal Cases:\", in_accuracy_isf)\nprint(\"Accuracy in Detecting Fraud Cases:\", out_accuracy_isf)","6e616f10":"LOF = LocalOutlierFactor(novelty=True)\nLOF.fit(ins)\n\nnormal_lof = LOF.predict(ins)\nfraud_lof = LOF.predict(outs)\n\nin_accuracy_lof=normal_accuracy(normal_lof)\nout_accuracy_lof=fraud_accuracy(fraud_lof)\nprint(\"Accuracy in Detecting Normal Cases:\", in_accuracy_lof)\nprint(\"Accuracy in Detecting Fraud Cases:\", out_accuracy_lof)","1748fb93":"fig, (ax1,ax2)= plt.subplots(1,2, figsize=[15,2])\n\nax1.set_title(\"Accuracy of Isolation Forest\",fontsize=20)\nsns.barplot(x=[in_accuracy_isf,out_accuracy_isf], \n            y=['normal', 'fraud'],\n            label=\"classifiers\", \n            color=\"b\", \n            ax=ax1)\nax1.set(xlim=(0,1))\n\nax2.set_title(\"Accuracy of Local Outlier Factor\",fontsize=20)\nsns.barplot(x=[in_accuracy_lof,out_accuracy_lof], \n            y=['normal', 'fraud'], \n            label=\"classifiers\", \n            color=\"r\", \n            ax=ax2)\nax2.set(xlim=(0,1))\nplt.show()","3292ebfd":"#### Split data into Inliers and Outliers\n\n`Inliers` are values that are normal.`Outliers` are values that don't belong to normal data and they are the anomalies.","bc32e075":"# Anomaly Detection using Unsupervised Techniques\n\nAnomaly detection is the identification of data points, items, observations or events that do not conform to the expected pattern of a given group. These anomalies occur very infrequently but may signify a large and significant threat such as cyber intrusions or fraud. Anomaly detection is heavily used in behavioral analysis and other forms of analysis in order to aid in learning about the detection, identification and prediction of the occurrence of these anomalies.\n \n***About Dataset:*** Dataset I have used for this kernel is labelled and reason for using this dataset is that I have to compare the result of algorithms. Due to confidentiality issues, features from V1 to V28 have been transformed using PCA and there is no missing value in the dataset.\n\n### Content of this kernel:\n\n#### Data preprocessing\n* Exploratory Data Analysis\n* Features transformation\n* Features selection\n\n#### Modleing\n* Isolation Forest\n* Local Outlier Factor","5480807c":"# Data Preprocessing","c74e1ebe":"Seems like hour of day have some impact on number or fraud cases. Lets move to transform the remaining features.","3ed114cf":"### Feature Transformation\n\nLets transform the remaining features using PCA.","01bda181":"As we have already seen from distribution plots that distribution of normal and fraud data of V13, V15, V22, V23, V25 and 26 features is almost same, now, its proven through hypothesis testing. We will eliminate these features from our dataset as they don't contribute at all.","8d8f31c0":"As we can see that data distribution of normal and fraud cases of some features like V18, V20, V25 are overlapping and they seem same. Such features are not good at differentiating between normal and fraud transactions. ","f2a30ea4":"###  Local Outlier Factor","c1b91cf7":"Lets visualize how many fraud cases we have in this dataset","2a5a0217":"### Exploratory Data Analysis","bd958527":"Kindly Upvote this kernel if you like it!","2c90e795":"## Feature Selection using Z-test\n\nLets move to do some hypothesis testing to find statistically significant features. We will be performing `Z-test` with valid transactions as our population. \n\nSo the case is we have to find if the values of fraud transactions are significantly different from normal transaction or not for all features. The level of significance is 0.01 and its a two tailed test.\n\n#### Scenario:\n* Valid transactions as our population\n* Fraud transactions as sample\n* Two tailed Z-test\n* Level of significance 0.01\n* Corresponding critical value is 2.58\n\n#### Hypothesis:\n* H0: There is no difference (insignificant)\n* H1: There is a difference  (significant)\n\n#### Formula for z-score:\n\n$$ Zscore = (\\bar{x} - \\mu) \/ S.E$$","ec4e78b8":"Both, Isolation Forest and Local Outlier Factor performed same in predicting Normal cases but Isolation Forest performed far better in detecting Fraud cases. So, Isolation Forest is a clear winner here!","3de4912a":"Only `Time` and `Amount` have not been transformed with PCA. Time contains the seconds elapsed between each transaction and the first transaction in the dataset. Lets transofrm this feature into hours to get a better understanding.","db8bd91c":"Now lets have a view at distribution of features","9bffb694":"###  Isolation Forest","bafc5726":"# Modeling\n\nWe will be using two unsupervised learning algorithms for anomaly detection.\n\n####  1. ISOLATION FOREST\n\nIsolation Forest is an unsupervised anomaly detection algorithm that uses the two properties \u201cFew\u201d and \u201cDifferent\u201d of anomalies to detect their existence. Since anomalies are few and different, they are more susceptible to isolation. This algorithm isolates each point in the data and splits them into outliers or inliers. This split depends on how long it takes to separate the points. If we try to separate a point which is obviously a non-outlier, it\u2019ll have many points in its round, so that it will be really difficult to isolate. On the other hand, if the point is an outlier, it\u2019ll be alone and we\u2019ll find it very easily.\n\n####  2. Local Outlier Factor\n\nThe Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. It is a calculation that looks at the neighbors of a certain point to find out its density and compare this to the density of neighbour points later on. In short we can say that the density around an outlier object is significantly different from the density around its neighbors. LOF considers as outliers the samples that have a substantially lower density than their neighbors.","156e2893":"We have 0.17% fraud cases in the dataset which are anomalies.","ce062528":"### CONCLUSION"}}