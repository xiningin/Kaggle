{"cell_type":{"4c06cbf5":"code","dc7ff77d":"code","49b0e22c":"code","4282f825":"code","61bb48ea":"code","4393cf49":"code","16d6ea4c":"code","1ff93be7":"code","72060622":"code","97e23bee":"code","998650bf":"code","f81e8369":"code","5e667d2c":"code","f579cb99":"code","321ddf58":"code","c1077e17":"code","dee02e85":"code","5306409f":"code","c4b2848f":"code","fdc2bc8d":"code","41188ac3":"code","ba2c54a7":"code","7bd0efd6":"code","9b38c273":"code","56f95c6e":"code","b78bb19c":"code","5ecee081":"code","a0207a6c":"code","5aeada63":"code","7d67db0e":"code","e22bb07c":"markdown","66a9d4f7":"markdown"},"source":{"4c06cbf5":"# Install XGBoost if not already installed\n# !pip install xgboost","dc7ff77d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","49b0e22c":"# Import all libraries \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","4282f825":"# Import the data into pandas dataframe, the data in the csv is seperated with ;, hence we need to mention it in deliminator \n# dataset = pd.read_csv('\/kaggle\/input\/subprime-credit-score\/MulticlassTrainData.csv', delimiter=';')\ndataset = pd.read_csv('\/kaggle\/input\/subprime-credit-score\/MulticlassTrainData.csv', delimiter=';')","61bb48ea":"# Visualize the first 5 records \ndataset.head(5)","4393cf49":"# Information related to the dataset does not contains which is the target variable. After analyzing the columns looks like the last column 'default_class' is the dependent (target) variable\ndataset['default_class'].value_counts()","16d6ea4c":"# There are 6 classes related to the credit score. We want to first analyze the data before we go ahead with prediction part.\n# Describe function will not show the columns that has a non-numerical value, i.e. Strings\ndataset.describe()","1ff93be7":"# Counting the frequency of the values in W column\ndataset['W'].value_counts()","72060622":"# Get the column names with their data types and store them in new variable\n# The goal here is to get the column names along with their data types so that we can get a list of columns which needs to be dropped from dataset\ncoln_names = dataset.dtypes\ncol_name = dataset.columns\ncol_name = list(col_name)\n\nnumber_of_columns = []\nfor i in range(len(coln_names)):\n    if coln_names[i] == 'object':\n        number_of_columns.append(col_name[i])\n        \nnumber_of_columns[:10]\ndataset = dataset.drop(number_of_columns, axis=1)","97e23bee":"dataset.head(5)\n# For this experiment, I have dropped all the columns that has object type i.e a column which has non-numeric value\n# Since it produces 40k total columns as features, kaggle notebook will run out of memory. ","998650bf":"# You can use this line to onehot encode all the columns that has non-numeric value\n# Since I do not have any it will keep the number of features same as before except neglecting the last column which is target variable\nab = pd.get_dummies(dataset.iloc[:, :-1], drop_first=True)","f81e8369":"ab.head(5)","5e667d2c":"# Visualize the number of null values for Default_flag column \nab['Default_flag'].isnull().value_counts()","f579cb99":"# Name of columns that has missing values\nnew_abc_cols = ['C','D','E','TRB','NOB','gc_final_score','VAL_App1_IdentityCheck_numprimar', 'VAL_App1_IdentityCheck_numactive',\n                             'VAL_App1_IdentityCheck_levelofco','VAL_App1_IdentityCheck_levelofc0','CifasDetected', 'Segment', 'Number_of_All_Settled_Accounts',\n                              'VAL_App1_IdentityCheck_numsharer', 'Age_of_Oldest_Account_All_Accoun', 'GC_C_S1_CL_027', 'GC_P_C_S1_CL_106','gc_age_of_applicant', 'GC_P_C_S1_CL_154', 'GC_P_C_S1_CL_112','External_Debt']","321ddf58":"# get the names of columns that has missing values (i.e. Null values)\nabc = ab.isnull().sum() # Check each columns has how many total null values   ","c1077e17":"# From above analysis we can see that column FWB and Default_flag has high number of missing values (more than 50% of the data)\n# The best solution for these two columns will be to remove them from the dataset inorder to avoid their impact on the prediction\nab = ab.drop(['FWB', 'Default_flag'], axis=1)","dee02e85":"# In order to treat missing values we need to check how manu unique values are present in that column along with their frequency to decide what approach to use like mean, median and mode\n# Mean is average, Median is the middle value when all values are sorted, and mode gives us the most often appearing value\n# For this problem I am using mode for all columns, since there are very few rows which has null values\nfor i in range(len(new_abc_cols)):\n    if not new_abc_cols[i] in ['Default_flag', 'FWB']:\n        print(ab[new_abc_cols[i]].value_counts())","5306409f":"# Handle all missing values using mode \nfor i in range(len(new_abc_cols)):\n    # print(new_abc_cols[i][0])\n    if new_abc_cols[i] in ['C','D','E','TRB','NOB','gc_final_score','VAL_App1_IdentityCheck_numprimar', 'VAL_App1_IdentityCheck_numactive',\n                             'VAL_App1_IdentityCheck_levelofco','VAL_App1_IdentityCheck_levelofc0','CifasDetected', 'Segment', 'Number_of_All_Settled_Accounts',\n                              'VAL_App1_IdentityCheck_numsharer', 'Age_of_Oldest_Account_All_Accoun', 'GC_C_S1_CL_027', 'GC_P_C_S1_CL_106','gc_age_of_applicant', 'GC_P_C_S1_CL_154', 'GC_P_C_S1_CL_112','External_Debt']:\n        ab[new_abc_cols[i]].fillna(ab[new_abc_cols[i]].mode()[0], inplace=True)\n","c4b2848f":"# ab is our final dataset\n\n# Split the dependent and independent variables\ny = dataset['default_class'].copy()\ny = y.values","fdc2bc8d":"# Creating numpy array with independent variables\n# X = ab.copy()\nX = ab.iloc[:, 4:].values","41188ac3":"# Use this code if you encounter memory error to free up some space\n\n# import gc\n# del dataset\n# gc.collect()","ba2c54a7":"# Create various classifiers to check which performs better comparatively\n\n# Model-1 KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier_1 = KNeighborsClassifier(n_neighbors=5)\n\n# Model-2 RandomForest\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier_2 = RandomForestClassifier(n_estimators=100, random_state=0)\n\n# Model-3 XgBoost\nfrom xgboost import XGBClassifier\nclassifier_3 = XGBClassifier()\n","7bd0efd6":"# Split data into training and testing dataset\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.4, random_state=0)","9b38c273":"# Fitting all 3 models to our training data\nclassifier_1.fit(X_train, y_train)","56f95c6e":"# Fitting RandomForest Classifier\nclassifier_2.fit(X_train, y_train)","b78bb19c":"# Fitting XGBoost Classifier\nclassifier_3.fit(X_train, y_train)","5ecee081":"# Predict variables for test dataset\ny_pred_1 = classifier_1.predict(X_test)\ny_pred_2 = classifier_2.predict(X_test)\ny_pred_3 = classifier_3.predict(X_test)","a0207a6c":"# Create confusion matrix to evaluate model's performance\nfrom sklearn.metrics import confusion_matrix\ncm1 = confusion_matrix(y_test, y_pred_1)\ncm2 = confusion_matrix(y_test, y_pred_2)\ncm3 = confusion_matrix(y_test, y_pred_3)","5aeada63":"# Print all cm to analyze each model's accuracy\nprint(\"KNN Confusion Matrix\")\nprint(cm1)\nprint(\"\\n RandomForest Classifier Matrix\")\nprint(cm2)\nprint('\\n XGBoost Classifier Matrix')\nprint(cm3)","7d67db0e":"from sklearn.metrics import accuracy_score\nprint(\"Accuracy of Knn algorithm\", accuracy_score(y_test, y_pred_1))\nprint(\"Accuracy of RandomForest algorithm\", accuracy_score(y_test, y_pred_2))\nprint(\"Accuracy of XGBoost algorithm\", accuracy_score(y_test, y_pred_3))","e22bb07c":"**Apply Machine Learning Model**","66a9d4f7":"**From the analysis it looks like XGBoost Classifier performed better compared to KNN and RandomForest Classifier**\nHigher accuracy can be achieved by not dropping non-numeric columns and onehot encode them. Number of features can be reduced by applying dimentionality reduction algorithm like PCA, LDA. \nOnce can also use grid search to find the hyper parameter (i.e. right set of parameters related to a classifier which can get better accuracy)"}}