{"cell_type":{"e9490cca":"code","7af25f6c":"code","d9b987ed":"code","e08638fe":"code","54472492":"code","612ca535":"code","b95eb5f2":"code","d32148f0":"code","12cad0bf":"code","4078e896":"code","07902bd9":"code","be6eea27":"code","48d8c15e":"code","1adeb91e":"code","2adf65d2":"code","8e0baed0":"code","c51ab71d":"code","592c0e00":"code","a8427fd9":"code","0ccf419e":"code","11d623a8":"code","4047baa8":"code","4ccfe9e1":"code","243d909f":"code","4cd48a00":"code","2312c20c":"code","34bdcf7d":"code","d15f42e3":"code","f0cca8e8":"code","f3eda937":"code","5b9de195":"code","33a73e1a":"code","013e6dee":"code","0eb92e4a":"code","15553223":"code","970878d5":"code","8617ba7c":"code","abd7b793":"code","6698089a":"code","9244083b":"code","d4e5eef0":"code","849c8ac3":"code","c8a00f7c":"code","636cba89":"code","871053ed":"code","3fb5f969":"code","80b60cc9":"code","b5eb6ce5":"code","af39b98d":"code","26f812ed":"code","ba6745c0":"code","8a958743":"code","5e6e6e0a":"code","c2309879":"code","b7be3808":"code","82833ce1":"code","72f93720":"code","ff91b01c":"code","f4856748":"code","165f6c36":"markdown","8c043a88":"markdown","f90a6dc8":"markdown","19967d0e":"markdown","71a41ddc":"markdown","f705380d":"markdown","ae2aeaca":"markdown","22a7be1b":"markdown","17bdc210":"markdown","f48957a0":"markdown","e0172f91":"markdown","de787b86":"markdown","3af1d1fb":"markdown","64f4d2d5":"markdown","788d52b4":"markdown","e6acf3cb":"markdown","d896529f":"markdown"},"source":{"e9490cca":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7af25f6c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import RobustScaler\n# Train Test Split\nfrom sklearn.model_selection import train_test_split\n# Cross Validation\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n# Metrics\nfrom sklearn.metrics import accuracy_score, classification_report, roc_curve\n# Models\nimport torch\nimport torch.nn as nn\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport itertools\nfrom matplotlib.ticker import NullFormatter\nimport matplotlib.ticker as ticker\n%matplotlib inline","d9b987ed":"#loading data\nheartdata = pd.read_csv('..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\n#copying data\ndf=heartdata.copy()\n#showing data\ndf.head()","e08638fe":"print('Number of rows:',df.shape[0], 'Number of columns: ',df.shape[1])","54472492":"heartdata.describe().T","612ca535":"heartdata.nunique()","b95eb5f2":"print(heartdata.isnull().values.any(),\":There is no null value\")","d32148f0":"cat_data = ['sex','exng','caa','cp','fbs','restecg','slp','thall']\ncon_data = [\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\ntarget_data = [\"output\"]\nprint(\"The categorial variable: \", cat_data)\nprint(\"The continuous variable: \", con_data)\nprint(\"The target variable:  \", target_data)","12cad0bf":"df[cat_data].describe().T","4078e896":"df[con_data].describe().T","07902bd9":"f,axes=plt.subplots(nrows=4, ncols=2, figsize=(14,18))\nf.tight_layout(pad=8) \nf.suptitle('Plots for Categorical Variables', fontsize=30)\n\ncols=df[cat_data].columns \n\nx_axes=0\ny_axes=0\n\nfor col in cols:\n  sns.countplot(data=df, x=col, ax=axes[x_axes,y_axes], palette='pastel')\n\n  if y_axes == 1:  \n    y_axes=0\n    x_axes+=1\n  else:\n    y_axes+=1\n\n\nplt.show()","be6eea27":"f,axes=plt.subplots(nrows=4, ncols=2, figsize=(14,18))\nf.tight_layout(pad=8) \nf.suptitle('Plots for Categorical Variables According to Heart Attack', fontsize=30)\n\ncols=df[cat_data].columns \n\nx_axes=0\ny_axes=0\n\nfor col in cols:\n  sns.countplot(data=df, x=col, hue = \"output\", ax=axes[x_axes,y_axes], palette='pastel')\n\n  if y_axes == 1:  \n    y_axes=0\n    x_axes+=1\n  else:\n    y_axes+=1\n\n\nplt.show()","48d8c15e":"f=plt.figure(figsize=(14,18))\ngs = f.add_gridspec(1,2)\ngs.update(wspace=0.3, hspace=0.15)\nax0 = f.add_subplot(gs[0,0])\nax1 = f.add_subplot(gs[0,1])\n\nf.suptitle('Plots for Target Variables', fontsize=30, color=\"#c486ad\")\nax0.text(0.5,0.5,\"************** \\n \\n Count of the target: \\n \\n **************\", horizontalalignment = 'center',verticalalignment = 'center',fontsize = 20,fontweight='bold',color=\"#c486ad\")\nax0.set_xticklabels([])\nax0.set_yticklabels([])\nax0.tick_params(left=False, bottom=False)\n\nax0.set_facecolor(\"#ffffff\") \n\nax1.text(0.35,177,\"Output\",fontsize=14, color=\"#c486ad\")\nax1.grid(color='#c486ad', linestyle=':', axis='y', zorder=0,  dashes=(1,5))\nsns.countplot(ax=ax1, data=df, x = 'output',palette = \"pastel\")\nax1.set_xlabel(\"\")\nax1.set_ylabel(\"\")\nax1.set_xticklabels([\"Low chances of attack(0)\",\"High chances of attack(1)\"])\n\nax0.spines[\"top\"].set_visible(False)\nax0.spines[\"left\"].set_visible(False)\nax0.spines[\"bottom\"].set_visible(False)\nax0.spines[\"right\"].set_visible(False)","1adeb91e":"sns.pairplot(df, hue='output',palette = \"pastel\");\nplt.suptitle('Distributions According to Target Variable',fontsize=25);","2adf65d2":"(sns\n .FacetGrid(df,\n                    hue='output',\n                    palette='pastel',\n                    xlim=(0,3),\n                    height=10)\n .map(sns.kdeplot, 'cp') \n .add_legend()\n .set(title='KDE Plot of Heart Attack According to the Cheast Pain Types')\n);","8e0baed0":"sns.set(rc={'figure.figsize':(15,10)})\na=sns.barplot(data=df, x='output',y=df[\"output\"].index,hue='cp',palette=\"Set3\");\na.tick_params(axis='x',rotation=90); #x eksenindeki parametreyi 90 derece \u00e7evirdik\nplt.suptitle('Barplot of Heart Attack According to Chest Pain',fontsize=30)\nplt.xticks(fontsize=40)\nplt.yticks(fontsize=40)","c51ab71d":"sns.set(rc={'figure.figsize':(15,10)})\nsns.jointplot(x = \"oldpeak\", y = \"output\", data = df, kind = \"reg\", palette=\"Pastel2\");\nplt.suptitle('Joint Plot of Relation Between Previous Peak and Heart Attack',fontsize=17);","592c0e00":"corr = df[con_data].corr()\ncorr","a8427fd9":"sns.set(rc={'figure.figsize':(15,10)})\nsns.heatmap(df.corr(), annot=True, linewidths='.5', color='#4747b5')\nplt.title('Correlation Matrix', fontsize=30)","0ccf419e":"#Split the features and the target\nX = df.drop(['output'],axis=1)\ny = df[['output']]","11d623a8":"from sklearn import preprocessing\nX = preprocessing.StandardScaler().fit(X).transform(X.astype(float))\nX[0:5]","4047baa8":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 0)\nprint(\"Shape of X_train: \", X_train.shape)\nprint(\"Shape of X_test: \",X_test.shape)\nprint(\"Shape of y_train: \",y_train.shape)\nprint(\"Shape of y_test: \",y_test.shape)","4ccfe9e1":"from sklearn.neighbors import KNeighborsClassifier","243d909f":"k = 4\n#Train Model and Predict  \nneigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nneigh","4cd48a00":"#prediction\nyhat = neigh.predict(X_test)\nyhat[0:5]","2312c20c":"#accuracy evaluation->ger\u00e7ek sonu\u00e7 ile tahmin kar\u015f\u0131la\u015ft\u0131r\u0131l\u0131r\nfrom sklearn import metrics\nprint(\"Train set Accuracy: \", metrics.accuracy_score(y_train, neigh.predict(X_train)))\nprint(\"Test set Accuracy: \", metrics.accuracy_score(y_test, yhat))","34bdcf7d":"Ks = 10\nmean_acc = np.zeros((Ks-1)) #k=1'den 10'a kadar 9 tane de\u011fer vard\u0131r\nstd_acc = np.zeros((Ks-1))\nConfusionMx = [];\nfor n in range(1,Ks):\n    \n    #Train Model and Predict  \n    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n    yhat=neigh.predict(X_test)\n    yhat=yhat.reshape(61,1)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n\n    \n    std_acc[n-1]=np.std(yhat==y_test)\/np.sqrt(yhat.shape[0])\n\nmean_acc","d15f42e3":"print( \"The best KNN accuracy was with\", mean_acc.max(), \"with k=\", mean_acc.argmax()+1) ","f0cca8e8":"from sklearn.metrics import confusion_matrix\n\nc_m = confusion_matrix(yhat,y_test)\nprint(\"Confusion Matrix:\\n\",c_m)","f3eda937":"#how can i calculate precision, recall and f1 score manually\nTP=c_m[0,0]\nFP=c_m[0,1]\nFN=c_m[1,0]\nTN=c_m[1,1]\n\nprecision = TP\/(TP+FP)\nrecall = TP\/(TP+FN)\nf1_score=2*(precision*recall)\/(precision+recall)\nprint(\"F1 Score:\",f1_score)","5b9de195":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\n\ndef score(y_test,yhat):\n    accuracy = accuracy_score(y_test, yhat)\n    print('Accuracy: %f' % accuracy)\n    # precision tp \/ (tp + fp)\n    precision = precision_score(y_test, yhat)\n    print('Precision: %f' % precision)\n    # recall: tp \/ (tp + fn)\n    recall = recall_score(y_test, yhat)\n    print('Recall: %f' % recall)\n    # f1: 2 tp \/ (2 tp + fp + fn)\n    f1 = f1_score(y_test, yhat)\n    print('F1 score: %f' % f1)\n    matrix = confusion_matrix(y_test,yhat)\n    print(matrix)\n","33a73e1a":"score(y_test,yhat)","013e6dee":"from sklearn.tree import DecisionTreeClassifier\n\ndectree  = DecisionTreeClassifier(criterion = \"entropy\")","0eb92e4a":"dectree.fit(X_train,y_train)\ndectree_pred = dectree.predict(X_test)","15553223":"c_m = confusion_matrix(dectree_pred,y_test)\nprint(\"Confusion Matrix:\\n\",c_m)","970878d5":"print(\"Decision Tree Accuracy: \",dectree.score(X_test,y_test))","8617ba7c":"score(y_test,dectree_pred)","abd7b793":"from sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\nLR","6698089a":"yhat = LR.predict(X_test)\nyhat","9244083b":"from sklearn.metrics import confusion_matrix\n\nc_m = confusion_matrix(yhat,y_test)\nprint(\"Logistic Regression\\n\",c_m)\nprint(\"Logistic Regression Accuracy: \",LR.score(X_test,y_test))","d4e5eef0":"score(y_test,yhat)","849c8ac3":"Kn=10\nmean_ranfor = np.zeros((Kn-1))\nConfustionMx = []","c8a00f7c":"for n in range(1,Kn):\n    \n    #Train Model and Predict  \n    ranfor= RandomForestClassifier(n_estimators = n, criterion = \"entropy\").fit(X_train,y_train)\n    ranfor_pred=ranfor.predict(X_test)\n    mean_ranfor[n-1] = metrics.accuracy_score(y_test, ranfor_pred)","636cba89":"mean_ranfor_list = pd.DataFrame(data=mean_ranfor, index = range(9))\nmean_ranfor_list = mean_ranfor_list.values.tolist() # convert dataframe to list\nmax_ranfor = mean_ranfor_list.index(max(mean_ranfor_list))+1","871053ed":"ranfor1 = RandomForestClassifier(criterion = \"entropy\").fit(X_train,y_train)\nranfor1_pred = ranfor1.predict(X_test)","3fb5f969":"c_m = confusion_matrix(ranfor1_pred,y_test)\nprint(\"Confusion Matrix:\\n\",c_m)","80b60cc9":"print(\"Random Forest Accuracy:\", ranfor1.score(X_test,y_test))","b5eb6ce5":"score(y_test,ranfor1_pred)","af39b98d":"from sklearn import svm\n\nsupvec = svm.SVC(kernel='rbf')\nsupvec.fit(X_train, y_train) ","26f812ed":"yhat = supvec.predict(X_test)\nyhat","ba6745c0":"c_m = confusion_matrix(yhat,y_test )\nprint(\"SVC:\\n\",c_m)","8a958743":"from sklearn.naive_bayes import GaussianNB\n\ngauss_nb = GaussianNB()\ngauss_nb.fit(X_train,y_train)","5e6e6e0a":"yhat = gauss_nb.predict(X_test)","c2309879":"c_m = confusion_matrix(yhat,y_test)\nprint(\"Gaussian Naive Bayes:\\n\",c_m)","b7be3808":"print(\"Gaussian Naive Bayes Accuracy: \",gauss_nb.score(X_test,y_test))","82833ce1":"score(y_test,yhat)","72f93720":"print(\"KNN Accuracy: \", mean_acc.max())\nprint(\"Decision Tree Accuracy: \",dectree.score(X_test,y_test))\nprint(\"Logistic Regression Accuracy: \",LR.score(X_test,y_test))\nprint(\"Random Forest Accuracy:\", ranfor1.score(X_test,y_test))\nprint(\"SVM Accuracy: \",supvec.score(X_test,y_test))\nprint(\"Gaussian Naive Bayes Accuracy: \",gauss_nb.score(X_test,y_test))","ff91b01c":"algos=[\"KNN\",\"Decision Tree\", \"Logistic Regression\", \"Random Forest\", \"SVM\", \"Gaussian Naive Bayes\"]\npredictions=[mean_acc.max(), dectree.score(X_test,y_test), LR.score(X_test,y_test), ranfor1.score(X_test,y_test), supvec.score(X_test,y_test), gauss_nb.score(X_test,y_test)]","f4856748":"plt.figure(figsize = (15,10))\nsns.barplot(x = predictions, y = algos, palette = 'Set3')","165f6c36":"# **Modeling**","8c043a88":"# **Preprocessing the Data for Modeling**","f90a6dc8":"# # **Random Forest**","19967d0e":"# # **Logistic Regression**","71a41ddc":"# **Separating The Variables Categorical and Continuous**","f705380d":"# # **Decision Tree**","ae2aeaca":"# # finding the k value which gives the best accuracy","22a7be1b":"# ***Accuracy Comparison***","17bdc210":"# # **SVM (Support Vector Machine)**","f48957a0":"# # **Naive Bayes**","e0172f91":"# **Splitting the Train and Test Data**","de787b86":"The mathematical function used for the transformation is known as the kernel function. Kernel function has different types, these types being:\n\n1. Linear\n1. Polynomial\n1. RBF\n1. Sigmoid","3af1d1fb":"# **Understanding the Data**","64f4d2d5":"# **EDA**","788d52b4":"age - Age of the patient\n\nsex - Sex of the patient\n\ncp - Chest pain type ~ 0 = Typical Angina, 1 = Atypical Angina, 2 = Non-anginal Pain, 3 = Asymptomatic\n\ntrtbps - Resting blood pressure (in mm Hg)\n\nchol - Cholestoral in mg\/dl fetched via BMI sensor\n\nfbs - (fasting blood sugar > 120 mg\/dl) ~ 1 = True, 0 = False\n\nrestecg - Resting electrocardiographic results ~ 0 = Normal, 1 = ST-T wave normality, 2 = Left ventricular hypertrophy\n\nthalachh - Maximum heart rate achieved\n\noldpeak - Previous peak\n\nslp - Slope\n\ncaa - Number of major vessels\n\nthall - Thalium Stress Test result ~ (0,3)\n\nexng - Exercise induced angina ~ 1 = Yes, 0 = No\n\noutput - Target variable","e6acf3cb":"# **Libraries**","d896529f":"# # **KNN**"}}