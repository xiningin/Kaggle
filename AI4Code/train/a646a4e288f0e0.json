{"cell_type":{"941a5fce":"code","f93bd760":"code","706e26dc":"code","e423bc59":"code","193d9135":"code","d2ecd3d5":"code","ed7d6b63":"code","d4a15c14":"code","f995b184":"markdown","3ca0b7ac":"markdown","16afbf5f":"markdown","533535d3":"markdown","7b313e44":"markdown"},"source":{"941a5fce":"from __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm","f93bd760":"# These are the different epsilons we'll try, larger means more perturbation so more noticable, \n# but we have a higher chance of messing the model up\nepsilons = [0, .05, .1, .15, .2, .25, .3]\npretrained_model = \"..\/input\/fgsm-notebook\/lenet_mnist_model.pth\"\nuse_cuda=False","706e26dc":"# LeNet Model definition\n\"\"\"\nBrief overview: 2 layers of convolution, relu, and max pooling, then 2 fully connected layers with softmax at the end\nso we output our confidence in each of the ten possible digits. The loss that will be used is negative log loss, so\nwe will basically be adding noise that decreases our models confidence in the correct answer.\n\"\"\"\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n# MNIST Test dataset and dataloader declaration\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('..\/input\/fgsm-notebook\/data\/data', train=False, download=True, transform=transforms.Compose([\n            transforms.ToTensor(),\n            ])),\n        batch_size=1, shuffle=True)\n\n# Define what device we are using\nprint(\"CUDA Available: \",torch.cuda.is_available())\ndevice = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n\n# Initialize the network\nmodel = Net().to(device)\n\n# Load the pretrained model\nmodel.load_state_dict(torch.load(pretrained_model, map_location='cpu'))\n\n# Set the model in evaluation mode. In this case this is for the Dropout layers\nmodel.eval()","e423bc59":"# FGSM attack code\ndef fgsm_attack(image, epsilon, data_grad):\n    ### YOUR CODE HERE\n    # Collect the element-wise sign of the data gradient\n    perturbation = np.sign(data_grad) * epsilon\n    \n    # Create the perturbed image by adjusting each pixel of the input image\n    perturbed_image = image + perturbation\n    \n    # Adding clipping to maintain [0,1] range\n    perturbed_image = np.clip(perturbed_image, 0, 1)\n    \n    ### DONE\n    # Return the perturbed image\n    return perturbed_image","193d9135":"def test( model, device, test_loader, epsilon ):\n\n    # Accuracy counter\n    correct = 0\n    adv_examples = []\n\n    # Loop over all examples in test set\n    for data, target in test_loader:\n\n        # Send the data and label to the device\n        data, target = data.to(device), target.to(device)\n\n        # Set requires_grad attribute of tensor. Important for Attack\n        data.requires_grad = True\n\n        # Forward pass the data through the model\n        output = model(data)\n        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n\n        # If the initial prediction is wrong, dont bother attacking, just move on\n        if init_pred.item() != target.item():\n            continue\n\n        # Calculate the loss\n        loss = F.nll_loss(output, target)\n\n        # Zero all existing gradients\n        model.zero_grad()\n\n        # Calculate gradients of model in backward pass\n        loss.backward()\n        \n        ### YOUR CODE HERE\n\n        # Get the gradient of the loss with respect to X (data)\n        data_grad = data.grad\n\n        # Call FGSM Attack\n        perturbed_data = fgsm_attack(data.detach(), epsilon, data_grad)\n\n        # Re-classify the perturbed image\n        output = model(perturbed_data)\n        \n        ### DONE\n\n        # Check for success\n        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n        if final_pred.item() == target.item():\n            correct += 1\n            # Special case for saving 0 epsilon examples\n            if (epsilon == 0) and (len(adv_examples) < 5):\n                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n        else:\n            # Save some adv examples for visualization later\n            if len(adv_examples) < 5:\n                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n\n    # Calculate final accuracy for this epsilon\n    final_acc = correct\/float(len(test_loader))\n    print(\"Epsilon: {}\\tTest Accuracy = {} \/ {} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n\n    # Return the accuracy and an adversarial example\n    return final_acc, adv_examples","d2ecd3d5":"accuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in tqdm(epsilons):\n    acc, ex = test(model, device, test_loader, eps)\n    accuracies.append(acc)\n    examples.append(ex)","ed7d6b63":"plt.figure(figsize=(5,5))\nplt.plot(epsilons, accuracies, \"*-\")\nplt.yticks(np.arange(0, 1.1, step=0.1))\nplt.xticks(np.arange(0, .35, step=0.05))\nplt.title(\"Accuracy vs Epsilon\")\nplt.xlabel(\"Epsilon\")\nplt.ylabel(\"Accuracy\")\nplt.show()","d4a15c14":"# Plot several examples of successful adversarial samples at each epsilon\ncnt = 0\nplt.figure(figsize=(8,10))\nfor i in range(len(epsilons)):\n    for j in range(len(examples[i])):\n        cnt += 1\n        plt.subplot(len(epsilons),len(examples[0]),cnt)\n        plt.xticks([], [])\n        plt.yticks([], [])\n        if j == 0:\n            plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n        orig,adv,ex = examples[i][j]\n        plt.title(\"{} -> {}\".format(orig, adv))\n        plt.imshow(ex, cmap=\"gray\")\nplt.tight_layout()\nplt.show()","f995b184":"### Complete fgsm_attack which returns $X_{adv}$ given $X$ (image), $\\epsilon$ (epsilon), and $\\nabla_{X}(J_{\\theta}(X, Y))$ (data_grad)","3ca0b7ac":"FGSM is a way of turning a regular input into an adversarial one. ($J_{\\theta}$ is loss of model parametarized by $\\theta$)\n\n$X_{adv} = X + \\epsilon * sign(\\nabla_{X}(J_{\\theta}(X, Y)))$\n\nIn english, the adversarial input equals the regular input moved in the direction of the gradient of the loss with respect to the input. So we are modifying the input to increase its loss.\n\nThis is conceptually the opposite of weight updates in gradient descent where we move the weights to decrease the loss\n\n$W' = W - \\alpha * \\nabla_{W}(J_{\\theta}(X, Y))$\n\nThe reason we use the sign operator is because we don't want to change one dimension by a lot, we just want to perturb every dimension a little bit in a given direction. These perturbations will add up to change the input by a lot in a direction that tricks the model. This is also why humans aren't tricked by these examples because we aren't sensitive to the small perturbations in every dimension (every pixel in the case of an image). In a way we are exploiting what is known in ML as the \"curse of dimensionality\" (sparsity of data increases with its dimensionality).\n\nSection three of this paper gives a mathematically formal description of the above: https:\/\/arxiv.org\/pdf\/1412.6572.pdf","16afbf5f":"### Before running the next two cells try and predict what the plot of Accuracy of our model vs. Epsilon will look like (e.g. will it have a postivie or negative slope?)","533535d3":"### Fill in the lines in the test function (no need to understand the whole function)","7b313e44":"# Fast Gradient Sign Method for Adversarial Attacks"}}