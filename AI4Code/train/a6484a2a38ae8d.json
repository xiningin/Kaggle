{"cell_type":{"5df3ccb7":"code","d591737c":"code","242af2f8":"code","3e0b85c4":"code","714e4fb6":"code","7434745e":"code","d8426a5f":"markdown","75a0c737":"markdown","efc3583b":"markdown","2bcde5f9":"markdown","2d8f5b20":"markdown"},"source":{"5df3ccb7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA","d591737c":"df1 = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv', nrows=2500)","242af2f8":"X = df1.loc[:, df1.columns.str.contains('feature')]\ny = (df1['resp'] > 0)*1 \nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = .25, random_state = 69)","3e0b85c4":"imp = SimpleImputer(missing_values=np.nan , strategy='mean')\nss = StandardScaler()\n\nX_train = imp.fit_transform(X_train)\nX_test = imp.transform(X_test)\nX_train_scaled = ss.fit_transform(X_train)\nX_test_scaled = ss.transform(X_test)","714e4fb6":"logreg = LogisticRegression(solver = 'saga', penalty='elasticnet', \n                            l1_ratio = .5, max_iter = 5000)\nlogreg.fit(X_train_scaled,y_train)\nprint('Log Reg Score: {:.3f}'.format(logreg.score(X_test_scaled,y_test)))","7434745e":"import janestreet\nimport tqdm\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set\n\nfor (test_df, sample_prediction_df) in tqdm.tqdm(iter_test):\n    y_preds = []\n    X_test_1 = test_df.loc[:, test_df.columns.str.contains('feature')]\n    X_test_1 = ss.transform(imp.transform(X_test_1))\n    sample_prediction_df.action = logreg.predict(X_test_1.reshape(1,-1))[0]\n    env.predict(sample_prediction_df)","d8426a5f":"Some simple cleaning.  Replace any nan with the mean then scale to mean = 0 and sd = 1.  ","75a0c737":"So a simple model not optimized or trained on the whole data set already performs at a 61% accuracy clip.","efc3583b":"Perform a simple logistic regression.  I wanted to add something that uses l1 penalty to get rid of useless features.  max_iter needs to be adjusted so there is no convergence error.","2bcde5f9":"Only pull the features columns.\n\nInstead of trying to predict the resp variable, let's just try to predict up\/down.","2d8f5b20":"Load in some data.  The train csv is fairly large, so I will only load some for memory purposes."}}