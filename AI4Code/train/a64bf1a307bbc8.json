{"cell_type":{"9f752ccc":"code","a88e7b5b":"code","5d0975ab":"code","4a59c907":"code","c53c9326":"code","0a64ebaf":"code","4b896b59":"code","9c9938df":"code","bc8a7ec6":"code","f1ab7bb1":"code","b1a62b2a":"code","1ca97e2c":"code","6db135ae":"code","782200d2":"code","05ced643":"code","983baa6c":"code","fccd7081":"code","9153a8b7":"code","b1e6e0ae":"code","1ded7112":"code","f83c8e91":"code","39384abd":"code","e72a665b":"code","8b7ad748":"code","61b95608":"code","87da4e98":"code","2555e7a7":"code","52fc84dc":"code","52074b9b":"code","729f3faf":"code","74583fe0":"code","3a295ec6":"code","c3544cc0":"code","ec55180a":"code","914749f5":"code","09c5a99b":"code","9ebf1bf2":"code","d758e34f":"code","5f67f3f4":"code","6c9f37f3":"code","3a967722":"code","27fdfd1f":"code","ed6cfc52":"code","84f017dc":"code","e9330198":"code","624be2dc":"code","301fbb33":"code","b6456cf5":"code","63208efa":"code","5eb29c5b":"code","9cd86bf9":"code","83f6d984":"markdown","3fee0065":"markdown","536e1169":"markdown","e83286f7":"markdown","74a0bf17":"markdown","537d312d":"markdown","fe8756b5":"markdown","74e1d781":"markdown","f1666f9b":"markdown","4a41b57d":"markdown","78ee7e2e":"markdown","addd1370":"markdown","ff3c617e":"markdown","1dee1ac7":"markdown","2309bc9b":"markdown","ea27eba4":"markdown","e9baedce":"markdown","e17ce152":"markdown","00457f08":"markdown","1b7d2b4f":"markdown","94452157":"markdown","6195e934":"markdown","4833159a":"markdown","715d005f":"markdown","1339e2b1":"markdown","438018d5":"markdown","1604b7b2":"markdown","b9e5d755":"markdown","9bd812f1":"markdown","283daae4":"markdown","fbfa3e3c":"markdown","9f6a2eff":"markdown","bbe506d7":"markdown","dce6f299":"markdown","5d156ecb":"markdown","fa63fa38":"markdown","fc2da4fb":"markdown","052b1877":"markdown","7910a829":"markdown","b9e626da":"markdown","8805c573":"markdown","078a06f4":"markdown","27a69e47":"markdown","c11bce11":"markdown","690ab7ee":"markdown","da6d495d":"markdown","0ea9882b":"markdown","17e04721":"markdown","ddce08b6":"markdown","b1682b9f":"markdown","1775ffa5":"markdown","5443c6e6":"markdown","a989c5f2":"markdown","41d3220f":"markdown","f937dcd9":"markdown","f02d2b48":"markdown","cef0c479":"markdown","4747d07d":"markdown"},"source":{"9f752ccc":"from IPython.display import Image ","a88e7b5b":"Image(filename='..\/input\/ganspng\/gans.png')","5d0975ab":"# TensorFlow and tf.keras\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Reshape, Dropout\nfrom tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D\nfrom tensorflow.keras.layers import LeakyReLU, UpSampling2D, Conv2D\nfrom tensorflow.keras.models import Sequential , Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import initializers\n# Helper libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport sys\nimport tqdm\n\n","4a59c907":"# Set the seed for reproducible result\nnp.random.seed(1000)\n","c53c9326":"((X_train, _), (_, _)) = mnist.load_data()\n","0a64ebaf":"# normalize the input values such that each pixel has a value in range [-1, 1]\nX_train = (X_train.astype(np.float32) - 127.50) \/ 127.5","4b896b59":"X_train = X_train.reshape(60000, 784)","9c9938df":"# Optimizer\nadam = Adam(lr=0.0002, beta_1=0.5)\n\n# input dim.\nrandomDim = 10 ","bc8a7ec6":"# Generator\n\ngenerator = Sequential()\ngenerator.add(Dense(256, input_dim=randomDim)) #, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\ngenerator.add(LeakyReLU(0.2))\ngenerator.add(Dense(512))\ngenerator.add(LeakyReLU(0.2))\ngenerator.add(Dense(1024))\ngenerator.add(LeakyReLU(0.2))\ngenerator.add(Dense(784, activation='tanh'))\n#generator.compile(loss='binary_crossentropy', optimizer=adam)\n\n","f1ab7bb1":"discriminator = Sequential()\ndiscriminator.add(Dense(1024, input_dim=784, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\ndiscriminator.add(LeakyReLU(0.2))\ndiscriminator.add(Dropout(0.3))\ndiscriminator.add(Dense(512))\ndiscriminator.add(LeakyReLU(0.2))\ndiscriminator.add(Dropout(0.3))\ndiscriminator.add(Dense(256))\ndiscriminator.add(LeakyReLU(0.2))\ndiscriminator.add(Dropout(0.3))\ndiscriminator.add(Dense(1, activation='sigmoid'))\ndiscriminator.compile(loss='binary_crossentropy', optimizer=adam)","b1a62b2a":"discriminator.trainable = False\nganInput = Input(shape=(randomDim,))\nx = generator(ganInput)\nganOutput = discriminator(x)\ngan = Model(inputs = ganInput, outputs = ganOutput)","1ca97e2c":"gan.compile(loss='binary_crossentropy', optimizer='adam')\n\ndLosses = []\ngLosses = []","6db135ae":"# Plot the loss from each batch\ndef plotLoss(epoch):\n    plt.figure(figsize=(10, 8))\n    plt.plot(dLosses, label='Discriminitive loss')\n    plt.plot(gLosses, label='Generative loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n#     plt.savefig('images\/gan_loss_epoch_%d.png' % epoch)\n\n# Create a wall of generated MNIST images\ndef saveGeneratedImages(epoch, examples=100, dim=(10, 10), figsize=(10, 10)):\n    noise = np.random.normal(0, 1, size=[examples, randomDim])\n    generatedImages = generator.predict(noise)\n    generatedImages = generatedImages.reshape(examples, 28, 28)\n\n    plt.figure(figsize=figsize)\n    for i in range(generatedImages.shape[0]):\n        plt.subplot(dim[0], dim[1], i+1)\n        plt.imshow(generatedImages[i], interpolation='nearest', cmap='gray_r')\n        plt.axis('off')\n    plt.tight_layout()\n#     plt.savefig('images\/gan_generated_image_epoch_%d.png' % epoch)","782200d2":"def train(epochs = 1, batchSize=128):\n    batchCount = int(X_train.shape[0] \/ batchSize)\n    print(\"Epochs:\", epochs)\n    print ('Batch size:', batchSize)\n    print ('Batches per epoch:', batchCount)\n    \n    for e in range(1, epochs+1):\n        print('-'*15, 'Epoch %d' % e, '-'*15)\n        for _ in range(batchCount):\n            # Get a random set of input noise and images\n            noise = np.random.normal(0, 1, size=[batchSize, randomDim])\n            imageBatch = X_train[np.random.randint(0, X_train.shape[0], size=batchSize)]\n            \n            # Generate fake MNIST images\n            generatedImages = generator.predict(noise)\n            # print np.shape(imageBatch), np.shape(generatedImages)\n            x = np.concatenate([imageBatch, generatedImages])\n            # Labels for generated and real data\n            yDis = np.zeros(2*batchSize)\n            # One-sided label smoothing\n            yDis[:batchSize] = 0.9\n            \n            # Train discriminator\n            discriminator.trainable = True\n            dloss = discriminator.train_on_batch(x, yDis)\n            \n            # Train generator\n            noise = np.random.normal(0, 1, size=[batchSize, randomDim])\n            yGen = np.ones(batchSize)\n            discriminator.trainable = False\n            gloss = gan.train_on_batch(noise, yGen)\n            \n        # we are saving the losses for each epoch and generating images after every 20 epochs\n        dLosses.append(dloss)\n        gLosses.append(gloss)\n\n        if e == 1 or e % 20 == 0:\n            saveGeneratedImages(e)\n        \n    # Plot losses from every epoch\n    plotLoss(e)","05ced643":"train(epochs = 200, batchSize=128)","983baa6c":"Image(filename='..\/input\/dcgans\/DCGAN.png')","fccd7081":"Image(filename='..\/input\/dcgans\/DCGAN2.png')","9153a8b7":"class DCGAN():\n    def __init__(self, rows, cols, channels, z = 10):\n        # input shape\n        self.img_rows = rows\n        self.img_cols = cols\n        self.channels = channels\n        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n        self.latent_dim = z\n        \n        optimizer = Adam(0.0002, 0.5)\n        \n        # Build and compile the discriminator\n        self.discriminator = self.build_discriminator()\n        self.discriminator.compile(loss='binary_crossentropy',\n                                  optimizer=optimizer,\n                                  metrics=['accuracy'])\n        \n        # Build the generator\n        self.generator = self.build_generator()\n        \n        # The generator takes noise as input and generates imgs\n        z = Input(shape=(self.latent_dim,))\n        img = self.generator(z)\n        \n        # For the combined model we will only train the generator\n        self.discriminator.trainable = False\n        \n        # The discriminator takes generated images as input and determines validity\n        valid = self.discriminator(img)\n        \n        # The combined model  (stacked generator and discriminator)\n        # Trains the generator to fool the discriminator\n        self.combined = Model(z, valid)\n        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n        \n    def build_generator(self):\n        \n        model = Sequential()\n        \n        model.add(Dense(128* 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n        model.add(Reshape((7, 7, 128)))\n        model.add(UpSampling2D())\n        model.add(Conv2D(128, kernel_size=3, padding='same'))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Activation(\"relu\"))\n        model.add(UpSampling2D())\n        model.add(Conv2D(64, kernel_size=3, padding='same'))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Activation('relu'))\n        model.add(Conv2D(self.channels, kernel_size=3, padding='same'))\n        model.add(Activation('tanh'))\n        \n        model.summary()\n        \n        noise = Input(shape=(self.latent_dim,))\n        img = model(noise)\n        \n        return Model(noise, img)\n    \n    def build_discriminator(self):\n        \n        model = Sequential()\n        \n        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Flatten())\n        model.add(Dense(1, activation='sigmoid'))\n\n        model.summary()\n\n        img = Input(shape=self.img_shape)\n        validity = model(img)\n\n        return Model(img, validity)\n    \n    \n    def train(self, epochs, batch_size=256, save_interval=50):\n        \n        # load the data\n        (X_train, _), (_, _) = mnist.load_data()\n\n        # Rescale -1 to 1\n        X_train = X_train \/ 127.5 - 1.\n        X_train = np.expand_dims(X_train, axis=3)\n\n        # Adversarial ground truths\n        valid = np.ones((batch_size, 1))\n        fake = np.zeros((batch_size, 1))\n        \n        for epoch in range(epochs):\n            # ---------------------\n            #  Train Discriminator\n            # ---------------------\n\n            # Select a random half of images\n            idx = np.random.randint(0, X_train.shape[0], batch_size)\n            imgs = X_train[idx]\n            \n            # Sample noise and generate a batch of new images\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            gen_imgs = self.generator.predict(noise)\n            \n            # Train the discriminator (real classified as ones and generated as zeros)\n            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n            \n            #---------------------\n            #  Train Generator\n            # ---------------------\n\n            # Train the generator (wants discriminator to mistake images as real)\n            g_loss = self.combined.train_on_batch(noise, valid)\n            \n            # Plot the progress\n            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n            \n            # If at save interval => save generated image samples\n            if epoch % save_interval == 0:\n                self.save_imgs(epoch)\n                \n    def save_imgs(self, epoch):\n        r, c = 5, 5 \n        noise = np.random.normal(0, 1, (r*c, self.latent_dim))\n        gen_imgs = self.generator.predict(noise)\n        # Rescale images 0 - 1\n        gen_imgs = 0.5 * gen_imgs + 0.5\n\n        fig, axs = plt.subplots(r, c)\n        cnt = 0\n        for i in range(r):\n            for j in range(c):\n                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n                axs[i,j].axis('off')\n                cnt += 1\n        plt.tight_layout()\n#         fig.savefig(\"images\/dcgan_mnist_%d.png\" % epoch)","b1e6e0ae":"dcgan = DCGAN(28,28,1)\ndcgan.train(epochs=1000, batch_size=64, save_interval=100)","1ded7112":"Image(filename='..\/input\/cyclegans\/CycleGan.png')","f83c8e91":"! pip install tensorflow-datasets==1.2.0","39384abd":"import tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom tensorflow.keras import Model, Sequential\nfrom tensorflow.keras.losses import mean_absolute_error, mean_squared_error\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, LeakyReLU, Dropout,\\\n                                    Conv2DTranspose, ReLU, Concatenate\n\nimport os, time\nimport matplotlib.pyplot as plt\nimport numpy as np\n","e72a665b":"dataset, metadata = tfds.load('cycle_gan\/summer2winter_yosemite',\n                             with_info=True, as_supervised=True)","8b7ad748":"train_A, train_B = dataset['trainA'], dataset['trainB']\ntest_A, test_B = dataset['testA'], dataset['testB']","61b95608":"BUFFER_SIZE = 1000\nBATCH_SIZE = 1\nIMG_WIDTH = 256\nIMG_HEIGHT = 256\nEPOCHS = 50","87da4e98":"def normalize(input_image, label):\n    input_image = tf.cast(input_image, tf.float32)\n    input_image = (input_image \/ 127.5) - 1\n    return input_image","2555e7a7":"# get number of cores in your machine\nAUTOTUNE = tf.data.experimental.AUTOTUNE","52fc84dc":"train_A = train_A.map(normalize,num_parallel_calls=AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\ntrain_B = train_B.map(normalize,num_parallel_calls=AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n\ntest_A = test_A.map(normalize,num_parallel_calls=AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\ntest_B = test_B.map(normalize,num_parallel_calls=AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)","52074b9b":"inpA = next(iter(train_A))\ninpB = next(iter(train_B))\n\nplt.subplot(121)\nplt.title(\"Train Set A\")\nplt.imshow(inpA[0]*0.5 + 0.5)\n\nplt.subplot(122)\nplt.title(\"Train Set B\")\nplt.imshow(inpB[0]*0.5 + 0.5)","729f3faf":"def downsample(filters, size=3, apply_batchnorm=True):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    \n    result  = Sequential()\n    result.add(\n        Conv2D(filters, size, strides=2, padding='same',\n                              kernel_initializer=initializer, use_bias=False)\n    )\n    \n    if apply_batchnorm:\n        result.add(BatchNormalization())\n    result.add(LeakyReLU())\n    \n    return result","74583fe0":"def upsample(filters, size=3, apply_dropout=False):\n    initializers = tf.random_normal_initializer(0., 0.02)\n    \n    result = Sequential()\n    result.add(\n        #Deconvolution\n        Conv2DTranspose(\n            filters, size, strides=2,\n            padding='same',\n            kernel_initializer=initializers,\n            use_bias=False))\n\n    result.add(BatchNormalization())\n    if apply_dropout:\n        result.add(Dropout(0.5))\n        \n    result.add(ReLU())\n    \n    return result\n\nclass ResnetIdentityBlock(Model):\n    def __init__(self, kernel_size, filters):\n        super(ResnetIdentityBlock, self).__init__(name='')\n        filters1, filters2, filters3 = filters\n        \n        self.conv2a = Conv2D(filters1, (1,1))\n        self.bn2a = BatchNormalization()\n        \n        self.conv2b = Conv2D(filters2, kernel_size, padding='same')\n        self.bn2b = BatchNormalization()\n        \n        self.conv2c = Conv2D(filters3, (1, 1))\n        self.bn2c = BatchNormalization()\n        \n    def call(self, input_tensor, training=False):\n        x = self.conv2a(input_tensor)\n        x = self.bn2a(x, training=training)\n        x = tf.nn.relu(x)\n        \n        x = self.conv2c(x)\n        x = self.bn2c(x, training=training)\n\n        x += input_tensor\n        return tf.nn.relu(x) ","3a295ec6":"block1 = ResnetIdentityBlock(3, [512, 512, 512])\nblock2 = ResnetIdentityBlock(3, [512, 512, 512])\nblock3 = ResnetIdentityBlock(3, [512, 512, 512])\n\nresnet = [block1, block2, block3]\nprint(block1(tf.zeros([1, 16, 16, 512])).shape)\nprint([x.name for x in block1.trainable_variables])","c3544cc0":"def Generator():\n    down_stack = [\n        downsample(64, 4, apply_batchnorm=False),\n        downsample(128, 4),\n        downsample(256, 4),\n        downsample(512, 4)\n    ]\n    \n    up_stack = [\n        upsample(256, 4),\n        upsample(128, 4),\n        upsample(64, 4)\n    ]\n    \n    initializer = tf.random_normal_initializer(0, 0.02)\n    last = Conv2DTranspose(3, 4 , strides=2, padding='same',\n                          kernel_initializer=initializer,\n                          activation='tanh')\n    \n    inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n    x = inputs\n    \n    # Downsampling through the model\n    skips = []\n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n        \n    for block in resnet:\n        x = block(x)\n    \n#     print(skips)\n    skips = reversed(skips[:-1])\n   \n    # Upsampling and establishing the skip connections\n    for up, skip in zip(up_stack, skips):\n        concat = Concatenate()\n        x = up(x)\n        x = concat([x, skip])\n        \n    x = last(x)\n    \n    return Model(inputs=inputs, outputs=x)","ec55180a":"generator = Generator()","914749f5":"tf.keras.utils.plot_model(generator, 'generator.png', show_shapes=True)","09c5a99b":"generator.summary()","9ebf1bf2":"gen_output = generator(inpA, training=False)\ngen_output = (gen_output +1) \/ 2\nplt.imshow(gen_output[0])\nprint(gen_output.shape, gen_output[0,...].numpy().max(), gen_output[0,...].numpy().min())","d758e34f":"def Discriminator():\n    inputs = tf.keras.layers.Input(shape=[None, None, 3])\n    x = inputs\n    g_filter = 64\n    \n    down_stack = [\n        downsample(g_filter),\n        downsample(g_filter * 2),\n        downsample(g_filter * 4),\n        downsample(g_filter * 8)\n    ]\n    \n    for down in down_stack:\n        x = down(x)\n        \n    last = Conv2D(1, 4, strides=1, padding='same') # (bs, 30, 30, 1)\n    x = last(x)\n    \n    return Model(inputs, x)","5f67f3f4":"discriminator = Discriminator()\ndis_output = discriminator(inpA, training=False)\nprint(dis_output.shape)","6c9f37f3":"tf.keras.utils.plot_model(discriminator, 'discriminator.png', show_shapes=True)","3a967722":"discriminator_A = Discriminator()\ndiscriminator_B = Discriminator()\n\ngenerator_AB = Generator()\ngenerator_BA = Generator()","27fdfd1f":"loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)","ed6cfc52":"@tf.function # to use tf graph\ndef discriminator_loss(disc_real_output, disc_generated_output):\n    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n    total_disc_loss = real_loss + generated_loss\n    return total_disc_loss","84f017dc":"optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5)","e9330198":"valid = np.ones((BATCH_SIZE, 16, 16, 1)).astype('float32')\nfake = np.zeros((BATCH_SIZE, 16, 16, 1)).astype('float32')","624be2dc":"@tf.function\ndef train_batch(imgs_A, imgs_B):\n    with tf.GradientTape() as g, tf.GradientTape() as d_tape:\n        fake_B = generator_AB(imgs_A, training= True)\n        fake_A = generator_BA(imgs_B, training=True)\n        \n        logits_real_A = discriminator_A(imgs_A , training=True)\n        logits_fake_A = discriminator_A(fake_A, training=True)\n        dA_loss = discriminator_loss(logits_real_A, logits_fake_A)\n        \n        logits_real_B = discriminator_B(imgs_B, training=True)\n        logits_fake_B = discriminator_B(fake_B, training=True)\n        dB_loss = discriminator_loss(logits_real_B, logits_fake_B)\n        \n        d_loss = (dA_loss + dB_loss) \/2\n        \n        # Translate images back to original domain\n        reconstr_A = generator_BA(fake_B, training=True)\n        reconstr_B = generator_BA(fake_A, training=True)\n        \n        id_A = generator_BA(imgs_A, training=True)\n        id_B = generator_AB(imgs_B, training=True)\n        \n        gen_loss = tf.math.reduce_sum([\n            1 * tf.math.reduce_mean(mean_squared_error(logits_fake_A, valid)),\n            1 * tf.math.reduce_mean(mean_squared_error(logits_fake_B, valid)),\n            10 * tf.math.reduce_mean(mean_squared_error(reconstr_A, imgs_A)),\n            10 * tf.math.reduce_mean(mean_squared_error(reconstr_A, imgs_B)),\n            0.1 * tf.math.reduce_mean(mean_squared_error(id_A, imgs_A)),\n            0.1 * tf.math.reduce_mean(mean_squared_error(id_B, imgs_B)),\n        ])\n        \n    gradients_of_d = d_tape.gradient(d_loss, discriminator_A.trainable_variables + \n                                                             discriminator_B.trainable_variables)\n    discriminator_optimizer.apply_gradients(zip(gradients_of_d, discriminator_A.trainable_variables + \n                                                discriminator_B.trainable_variables))\n    \n    gradients_of_generator = g.gradient(gen_loss, generator_AB.trainable_variables +\n                                       generator_AB.trainable_variables)\n    \n    optimizer.apply_gradients(zip(gradients_of_generator, generator_AB.trainable_variables+\n                                 generator_BA.trainable_variables))\n    \n    return dA_loss, dB_loss, gen_loss","301fbb33":"checkpoint_dird_A = '.\/training_checkpointsd_A'\ncheckpoint_prefixd_A = os.path.join(checkpoint_dird_A, \"ckpt_{epoch}\")\n\ncheckpoint_dird_B = '.\/training_checkpointsd_B'\ncheckpoint_prefixd_B = os.path.join(checkpoint_dird_B, \"ckpt_{epoch}\")\n\ncheckpoint_dirg_AB = '.\/training_checkpointsg_AB'\ncheckpoint_prefixg_AB = os.path.join(checkpoint_dirg_AB, \"ckpt_{epoch}\")\n\ncheckpoint_dirg_BA = '.\/training_checkpointsg_BA'\ncheckpoint_prefixg_BA = os.path.join(checkpoint_dirg_BA, \"ckpt_{epoch}\")","b6456cf5":"def train(trainA_, trainB_, epochs):\n    for epoch in range(epochs):\n        start = time.time()\n        \n        for batch_i, (imgs_A, imgs_B) in enumerate(zip(trainA_, trainB_)):\n            dA_loss, dB_loss, g_loss = train_batch(imgs_A, imgs_B)\n            \n            if batch_i % 1000 == 0:\n                test_imgA = next(iter(test_A))\n                test_imgB = next(iter(test_B))\n                print('Time taken for epoch {} batch index {} is {} seconds\\n'.format(epoch, batch_i, time.time()-start))\n                print(\"discriminator A: \", dA_loss.numpy())\n                print(\"discriminator B: \", dB_loss.numpy())\n                print(\"generator: {}\\n\".format(g_loss))\n                \n                fig, axs = plt.subplots(2, 2, figsize=(10, 10), sharey=True, sharex=True)\n                gen_outputA = generator_AB(test_imgA, training=False)\n                gen_outputB = generator_BA(test_imgB, training=False)\n                axs[0,0].imshow(test_imgA[0]*0.5 + 0.5)\n                axs[0,0].set_title(\"Generator A Input\")\n                axs[0,1].imshow(gen_outputA[0]*0.5 + 0.5)\n                axs[0,1].set_title(\"Generator A Output\")\n                axs[1,0].imshow(test_imgB[0]*0.5 + 0.5)\n                axs[1,0].set_title(\"Generator B Input\")\n                axs[1,1].imshow(gen_outputB[0]*0.5 + 0.5)\n                axs[1,1].set_title(\"Generator B Output\")\n                plt.show()\n                \n                discriminator_A.save_weights(checkpoint_prefixd_A.format(epoch=epoch))\n                discriminator_B.save_weights(checkpoint_prefixd_B.format(epoch=epoch))\n                generator_AB.save_weights(checkpoint_prefixg_AB.format(epoch=epoch))\n                generator_BA.save_weights(checkpoint_prefixg_BA.format(epoch=epoch))","63208efa":"train(train_A, train_B, EPOCHS)","5eb29c5b":"Image(filename='..\/input\/ingogan\/InfoGan.png')","9cd86bf9":"Image('..\/input\/ingogan\/equ.png')","83f6d984":"The main idea in the design was using convolutional layers\nwithout the use of pooling layers or the end classifier layers. The convolutional\nstrides and transposed convolutions are employed for the downsampling and\nupsampling of images.\nBefore going into the details of the DCGAN architecture and its capabilities,\nlet us point out the major changes that were introduced in the paper","3fee0065":"The goal of training the discriminator D(Y) is to maximize D(Y) for every image\nfrom the true data distribution, and to minimize D(Y) for every image not from\nthe true data distribution. So, G and D play opposite games: hence the name\nadversarial training. Note that we train G and D in an alternating manner, where\neach one of their objectives is expressed as a loss function optimized via a gradient\ndescent. The generative model continues to improve its forgery capabilities, and\nthe discriminative model continues to improve its forgery recognition capabilities.\nThe discriminator network (usually a standard convolutional neural network)\ntries to classify if an input image is real or generated. The important new idea is\nto backpropagate through both the discriminator and the generator to adjust the\ngenerator's parameters in such a way that the generator can learn how to fool the\ndiscriminator more often. At the end the generator will learn how to produce images\nthat are indistinguishable from the real ones:","536e1169":"The generator in an InfoGAN takes two inputs: the latent space Z and a latent code c,\nthus the output of the generator is G(Z,c). The GAN is trained such that it maximizes\nthe mutual information between the latent code c and the generated image G(Z,c).\nThe following figure shows the architecture of the InfoGAN:","e83286f7":"* The network consisted of all convolutional layers. The pooling layers were replaced \n  by strided convolutions in the discriminator and transposed convolutions in the generator.\n* The fully connected classifying layers after the convolutions are removed.\n* To help with the gradient flow, batch normalization is done after every convolutional layer.","74a0bf17":"For our\nimplementation we tried a value of 10. This input is fed to a Dense layer with\n256 neurons with LeakyReLU activation. We next add another Dense layer with\n512 hidden neurons, followed by the third hidden layer with 1024 neurons and\nfinally the output layer with 784 neurons","537d312d":"The trick to train the two is that we first train the discriminator separately; we\nuse binary cross entropy loss for the discriminator. Later we freeze the weights of\nthe discriminator and train the combined GAN; this results in the training of the\ngenerator. The loss this time is also binary cross entropy","fe8756b5":"Let us now build a DCGAN for generating handwritten digits. We first see the\ncode for the generator. The generator is built by adding the layers sequentially. The\nfirst layer is a dense layer that takes the noise of 100 dimensions as an input. The\n100-dimensional input is expanded to a flat vector of size 128 \u00d7 7 \u00d7 7. This is done so\nthat finally we get an output of size 28 \u00d7 28, the standard size of MNIST handwritten\ndigits. The vector is reshaped to a tensor of size 7 \u00d7 7 \u00d7 128. This vector is then\nupsampled using TensorFlow Keras UpSampling2D layer. Please note that this layer\nsimply scales up the image by doubling rows and columns. The layer has no weights,\nso it is computationally cheap.","74e1d781":"we build a discriminator. Notice now that the discriminator takes in the\nimages, either from the training set or images generated by generator, thus its input\nsize is 784 . The output of the discriminator however is a single bit, with 0 signifying\na fake image (generated by generator) and 1 signifying that the image is from the\ntraining dataset","f1666f9b":"in the same for loop, we will train the generator. We want the images\ngenerated by the generator to be detected as real by the discriminator, so we use\na random vector (noise) as input to the generator; this generates a fake image\nand then trains the GAN such that the discriminator perceives the image as real","4a41b57d":"now combine it all and train the network for 50 epochs. Please remember that\nin the paper, the test network was trained for 200 epochs, so our results will not be\nthat good","78ee7e2e":"The term Vg(D,G) is the loss function of the conventional GAN, and the second term\nis the regularization term, where \u03bb is a constant. Its value was set to 1 in the paper,\nand I(c;G(Z,c)) is the mutual information between the latent code c and the generator-","addd1370":"Set hyperparameters:","ff3c617e":"let us see an example of  images","1dee1ac7":"SRGANs use the perceptual loss function. The difference in the feature map\nactivations in high layers of a VGG network between the network output part and\nthe high-resolution part comprises the perceptual loss function. Besides perceptual\nloss, the authors further added content loss and an adversarial loss so that images\ngenerated look more natural and the finer details more artistic. The perceptual\nloss is defined as the weighted sum of content loss and adversarial loss","2309bc9b":"### 5. GAN architectures","ea27eba4":"normalize images in the range [-1,1] for better performance","e9baedce":"Now, define the discriminator too. We are following the same\narchitecture of discriminator and discriminator as in the paper by Zhu et al.","e17ce152":"### How does a Perceptual Loss Function work?\nIn short, the perceptual loss function works by summing all the squared errors between all the pixels and taking the mean. This is in contrast to a per-pixel loss function which sums all the absolute errors between pixels. Johnson et al. (2016)\n\nargues that perceptual loss functions are not only more accurate in generating high quality images, but also do so as much as three times faster, when optimized. The neural network model is trained on images where the perceptual loss function is optimized based upon high level features extracted from already trained networks. ","00457f08":"## Generative Adversarial Networks","1b7d2b4f":"The Upsampling2D layer will now double the rows and columns of the 7 \u00d7 7 \u00d7 128\n(rows \u00d7 columns \u00d7 channels) image, yielding an output of size 14 \u00d7 14 \u00d7 128. The\nupsampled image is passed to a convolutional layer. This convolutional layer learns\nto fill in the details in the upsampled image","94452157":"#### build a generator and discriminator","6195e934":"using the preceding defined generator and discriminator, we construct the 2 \nCycleGANs:","4833159a":"Let us build a simple GAN capable of generating handwritten digits","715d005f":"### What is a Perceptual Loss Function? \nhttps:\/\/deepai.org\/machine-learning-glossary-and-terms\/perceptual-loss-function\n\nPerceptual loss functions are used when comparing two different images that look similar, like the same photo but shifted by one pixel. The function is used to compare high level differences, like content and style discrepancies, between images. A perceptual loss function is very similar to the per-pixel loss function, as both are used for training feed-forward neural networks for image transformation tasks. The perceptual loss function is a more commonly used component as it often provides more accurate results regarding style transfer. ","1339e2b1":"### b.  CycleGAN:-\nA CycleGAN performs the image translation, that is, transfers an image given in one domain (scenery\nfor example) to another domain  in the absence  of training examples. The CycleGAN's ability to perform \nimage translation in the absence of training pairs is what makes it unique.","438018d5":"#### Build our Generator","1604b7b2":"To construct the generator and discriminator we will require three sub modules: the\nupsampling layer, which will take in an image and perform a transpose convolution\noperation; a downsampling layer, which will perform the convention convolutional\noperation, and a residual layer so that we can have a sufficiently deep model. These\nlayers are defined in the functions downsample() , upsample() , and class based\non the TensorFlow Keras Model API ResnetIdentityBlock","b9e5d755":"To achieve image translation the authors used a very simple and yet effective procedure. They made \nuse of two GANs, the generator of each GAN performing the image translation from one domain to another","9bd812f1":"To train the combined GANs, the authors added, besides the conventional GAN\nadversarial loss, a forward cycle consistency loss (left figure) and a backward cycle\nconsistency loss (right figure). This ensures that if an image X is given as input, then\nafter the two translations F(G(X)) ~ X the obtained image is the same, X (similarly the\nbackward cycle consistency loss ensures that? G(F(Y)) ~ Y).","283daae4":"This notebook is consists of code and some explanation from this awsome book \"Deep Learning with TensorFlow 2 and Keras Second Edition\"\nlink -> https:\/\/www.packtpub.com\/gb\/data\/deep-learning-with-tensorflow-2-0-and-keras-second-edition","fbfa3e3c":"To elaborate, let us say the input is X, then the generator of the first GAN performs\na mapping G: X \u2192 Y; thus its output would be Y = G(X). The generator of the\nsecond GAN performs an inverse mapping F: Y \u2192 X, resulting in X = F(Y). Each\ndiscriminator is trained to distinguish between real images and synthesized images.\nThe idea is shown as follows:","9f6a2eff":"create placeholders for the labels of real and fake images:","bbe506d7":"The output of convolution is passed\nto batch normalization for better gradient flow. The batch normalized output then\nundergoes ReLU activation in all the intermediate layers. We repeat the structure,\nthat is, upsampling | convolution | batch normalization | ReLU","dce6f299":"### 1. What is a GAN ?","5d156ecb":"The discriminator network takes in the images (either generated by the generator\nor from the real dataset), and the images undergo convolution followed by batch\nnormalization. At each convolution step the images get downsampled using strides.\nThe final output of the convolutional layer is flattened and feeds a one-neuron\nclassifier layer. In the following diagram, you can see the discriminator","fa63fa38":"### c. InfoGAN\nInfoGAN provides control over various attributes of the images generated. The InfoGAN uses the\nconcepts from information theory such that the noise term is transformed into latent\ncode that provides predictable and systematic control over the output.","fc2da4fb":"We next define the loss and optimizers:","052b1877":"apply normalize to our train and test datasets and create a data generator that\nwill provide images for training in batches","7910a829":"will use a simple multi-layered perceptron (MLP) and we will feed it an image\nas a flat vector of size 784","b9e626da":"the argument num_parallel_calls allows one to take benefit\nfrom multiple CPU cores in the system, one should set its value to the number\nof CPU cores in your system","8805c573":"perform the training. For each epoch we take a sample of random noise\nfirst, feed it to the generator, and the generator produces a fake image. We combine\nthe generated fake images and the actual training images in a batch with their\nspecific labels and use them to train the discriminator first on the given batch","078a06f4":"The concatenated vector (Z,c) is fed to the generator. Q(c|X) is also a neural network.\nCombined with the generator it works to form a mapping between random noise Z\nand its latent code c_hat. It aims to estimate c given X. This is achieved by adding a\nregularization term to the objective function of the conventional GAN","27a69e47":"Here we will use a paired datasets for CycleGANs from TensorFlow's Dataset API, dataset name is summer2winter_yosemite, which contains images\nof Yosemite (USA) in summer (Dataset A) and winter (Dataset B)","c11bce11":"#### Load dataset","690ab7ee":"define the function that trains the generator and discriminator in a batch,\na pair of images at a time. The two discriminators and the two generators are trained\nvia this function with the help of the tape gradient:","da6d495d":"define checkpoints to save the model weights","0ea9882b":"4. ### CGAN for MNIST digits","17e04721":"The generator and the discriminator are combined together to form the DCGAN. The\ntraining follows in the same manner as before; that is, we first train the discriminator\non a mini-batch, then freeze the discriminator and train the generator. The process is\nrepeated iteratively for a few thousand epochs. The authors found that we get more\nstable results with the Adam optimizer and a learning rate of 0.002","ddce08b6":"GAN is trained in such a way that it can generate a photorealistic high-\nresolution image when given a low-resolution image. The SRGAN architecture\nconsists of three neural networks: a very deep generator network (which uses\nResidual modules a discriminator network, and a pretrained VGG-16 network.","b1682b9f":"#### a. SRGAN (Super Resolution GANs)","1775ffa5":"### 3. Deep convolutional GAN (DCGAN)","5443c6e6":"The basic idea of DCGANs is same as the vanilla GAN: we have a generator that\ntakes in noise of 100 dimensions; the noise is projected and reshaped, and then is\npassed through convolutional layers. The following diagram shows the generator\narchitecture:","a989c5f2":"GANs train two neural nets simultaneously. The generator G(Z) is the one that makes the forgery, and the discriminator D(Y) is the one that can judge how realistic the reproductions are, based on its observations of\nauthentic pieces of art and copies. D(Y) takes an input Y (for instance, an image), and\nexpresses a vote to judge how real the input is. In general, a value close to 1 denotes\n\"real,\" while a value close to 0 denotes \"forgery.\" G(Z) takes an input from random\nnoise Z and it trains itself to fool D into thinking that whatever G(Z) produces is real.","41d3220f":"The purpose of the\ngenerator is to take in a noisy input and generate an image similar to the training\ndataset. The size of the noisy input is decided by the variable randomDim ; you\ncan initialize it to any integral value. Conventionally people set it to 100","f937dcd9":"In the following\ngenerator we have two such structures, the first with 128 filters, and the second\nwith 64 filters in the convolution operation. The final output is obtained from\na pure convolutional layer with 3 filters and tan hyperbolic activation, yielding\nan image of size 28 \u00d7 28 \u00d7 1","f02d2b48":"### 2. MNIST using GAN in TensorFlow","cef0c479":"combine the generator and discriminator together to form a GAN.\nIn the GAN we ensure that the discriminator weights are fixed by setting the trainable\nargument to False","4747d07d":"### CycleGAN in TensorFlow 2.0"}}