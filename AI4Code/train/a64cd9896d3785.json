{"cell_type":{"aa542ce4":"code","1317eb87":"code","a42e25fd":"code","026c135b":"code","79045c9b":"code","43335020":"code","e18d7b70":"code","789f13c7":"code","fc2c8dc0":"code","ff0ce3ef":"code","10e4a675":"markdown","c8dfbf46":"markdown","2148eb81":"markdown","33eb5346":"markdown","5f01d693":"markdown","379c7833":"markdown","65481a53":"markdown","c1cf8fc5":"markdown","7537133a":"markdown","1b4454e2":"markdown","a86a4f65":"markdown"},"source":{"aa542ce4":"import numpy as np\nimport matplotlib.pyplot as plt\nimport keras\nfrom keras.applications import VGG16\nfrom keras.applications.vgg16 import preprocess_input, decode_predictions\nfrom keras.preprocessing import image\nimport tensorflow as tf\nfrom IPython.display import Image\nimport cv2\nimport matplotlib.cm as cm\nimport os","1317eb87":"model = VGG16(weights='imagenet')","a42e25fd":"img_path = '..\/input\/dogs-vs-cats\/dataset\/training_set\/dogs\/dog.1016.jpg'\n\ndisplay(Image(img_path))","026c135b":"img = image.load_img(img_path, target_size=(244,244)) #read image, resize image to 224,224\nimg = image.img_to_array(img) #convert it into a tensor\nimg = np.expand_dims(img, axis=0) #expand in first axis to make it's shape (1,244,244,3)\nimg = preprocess_input(img) #standard preprocessing","79045c9b":"pred = model.predict(img)\nprint(decode_predictions(pred))\nclassIdx = np.argmax(pred)\n(imagenetID, label, prob) = decode_predictions(pred)[0][0]\nlabel = \"{}: {:.2f}%\".format(label, prob * 100)","43335020":"model.summary()","e18d7b70":"last_conv_layer_name = \"block5_conv3\"\nclassifier_layer_names = [\n    \"block5_pool\",\n    \"flatten\",\n    \"fc1\",\n    \"fc2\",\n    \"predictions\"\n]\n","789f13c7":"def make_gradcam_heatmap(\n    img_array, model, last_conv_layer_name, classifier_layer_names\n):\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer\n    last_conv_layer = model.get_layer(last_conv_layer_name)\n    last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)\n\n    # Second, we create a model that maps the activations of the last conv\n    # layer to the final class predictions\n    classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n    x = classifier_input\n    for layer_name in classifier_layer_names:\n        x = model.get_layer(layer_name)(x)\n    classifier_model = keras.Model(classifier_input, x)\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        # Compute activations of the last conv layer and make the tape watch it\n        last_conv_layer_output = last_conv_layer_model(img_array)\n        tape.watch(last_conv_layer_output)\n        # Compute class predictions\n        preds = classifier_model(last_conv_layer_output)\n        top_pred_index = tf.argmax(preds[0])\n        top_class_channel = preds[:, top_pred_index]\n\n    # This is the gradient of the top predicted class with regard to\n    # the output feature map of the last conv layer\n    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n    pooled_grads = pooled_grads.numpy()\n    for i in range(pooled_grads.shape[-1]):\n        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n\n    # The channel-wise mean of the resulting feature map\n    # is our heatmap of class activation\n    heatmap = np.mean(last_conv_layer_output, axis=-1)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = np.maximum(heatmap, 0) \/ np.max(heatmap)\n    return heatmap","fc2c8dc0":"heatmap = make_gradcam_heatmap(\n    img, model, last_conv_layer_name, classifier_layer_names\n)\n\n# Display heatmap\nplt.matshow(heatmap)\nplt.show()","ff0ce3ef":"# We load the original image\nimg = keras.preprocessing.image.load_img(img_path)\nimg = keras.preprocessing.image.img_to_array(img)\n\n# We rescale heatmap to a range 0-255\nheatmap = np.uint8(255 * heatmap)\n\n# We use jet colormap to colorize heatmap\njet = cm.get_cmap(\"jet\")\n\n# We use RGB values of the colormap\njet_colors = jet(np.arange(256))[:, :3]\njet_heatmap = jet_colors[heatmap]\n\n# We create an image with RGB colorized heatmap\njet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\njet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\njet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n\n# Superimpose the heatmap on original image\nsuperimposed_img = jet_heatmap * 0.4 + img\nsuperimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n\n# Save the superimposed image\nsave_path = \"dog_cam.jpg\"\nsuperimposed_img.save(save_path)\n\n# Display Grad CAM\ndisplay(Image(save_path))","10e4a675":"## Class Activation Map:\n\nIn this notebook we will see why a convnet classifies an image into a particular class, which parts of the image helped the convnet in identifying it's class, basically we will learn model explainability for convnets.\n\nThe technique we are going to see is Class Activation Map(CAM). \n> **CAM visualization consists of producing heatmaps of class activation over input images. A class activation map is a 2D grid of scores associated with a specific output class, computed for every location in any input image, indicating how important each location is with respect to the class under consideration.** \n\n**How it works?**\n\nlet's start with basics, Each convolutional layer in convnet is nothing but a stack of convolutional filters\/kernels\/channels, each of them meant to learn a particular feature from it's input feature map. A first convolution layer will learn small local patters such as edges, second layer will learn a bit complex features made of the first layer such as an eye and so on. Last convolution layer channels will learn large features such as face. Different channels will have different features learned and then we flatten them and pass to a classifier(mostly dense layers with softmax\/sigmoid activation). \n\nLet's say we have only one dense layer with softmax activation and 2 target classes in our classifier. The dense layer will output probabilities for classes based on the features from last convolutional layer. Each channel from last conv layer will be wieghted differently by each node(class) in dense layer based on how important the feature learned by the channel is for that class. If we trace back to channels with high weights for a particular class(node of dense layer), our job is done!\n\nLet's see how it's done in keras.\n","c8dfbf46":"References:\n\n[https:\/\/keras.io\/examples\/vision\/grad_cam\/](https:\/\/keras.io\/examples\/vision\/grad_cam\/)","2148eb81":"Let's load an image to work with.","33eb5346":"Let's create a superimposed visualization.","5f01d693":"So the model says it's a Dog(Labrador_retriever) with probability of 0.5. Let's find out the parts in input image which led the model to this classification decision.\n\nFirstly, we need to know the model, what are the conv layers, classifier part. Let's print model summary.","379c7833":"Let's find the heatmap for activations for this class(Labrador_retriever).","65481a53":"Let's predict the class using VGG16 model we loaded previously.","c1cf8fc5":"So last conv layer in our model is \"block5_conv3\" and classifier consists of pooling layer, flatten layer and Dense layers.","7537133a":"Import all required packages.","1b4454e2":"Preprocess the image to transform into something that the model can accept.","a86a4f65":"We will use VGG16 pretrained model for our CAM demo. Let's load the model."}}