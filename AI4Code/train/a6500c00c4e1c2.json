{"cell_type":{"43b708e7":"code","ee1e2d3b":"code","97875e20":"code","bd24ae8b":"code","45fed0d4":"code","06fbff81":"code","5738c09b":"code","02623f71":"code","9410a3f9":"code","b8d33204":"code","c941eebe":"code","44589da6":"code","e443df12":"code","5e289b5e":"code","6b473ad9":"code","f7766943":"code","1943f332":"code","7b4057d9":"code","9e035bd4":"code","cab0dc9f":"code","25bc75ab":"code","b8f01b8c":"code","d12be768":"code","210edc8a":"code","41f7b084":"code","9afdf67d":"markdown","f53cc1da":"markdown","51cafd50":"markdown"},"source":{"43b708e7":"import sys\n!cp -r ..\/input\/openai-clip\/CLIP\/CLIP-main \/tmp\/\n\n# Kaggle likes to unpack .gz files in datasets... so we have to pack it back\n!gzip -c \/tmp\/CLIP-main\/clip\/bpe_simple_vocab_16e6.txt > \/tmp\/CLIP-main\/clip\/bpe_simple_vocab_16e6.txt.gz\nsys.path.append('\/tmp\/CLIP-main')","ee1e2d3b":"%%capture\n!pip install ..\/input\/openai-clip\/ftfy-5.9\/ftfy-5.9 \\\n             ..\/input\/openai-clip\/torch-1.7.1+cu110-cp37-cp37m-linux_x86_64.whl \\\n             ..\/input\/openai-clip\/torchvision-0.8.2+cu110-cp37-cp37m-linux_x86_64.whl \\\n             ..\/input\/faiss-163\/faiss_gpu-1.6.3-cp37-cp37m-manylinux2010_x86_64.whl","97875e20":"import numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport clip\nfrom PIL import Image\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport re\nfrom clip.simple_tokenizer import SimpleTokenizer\nimport faiss\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","bd24ae8b":"df_test = pd.read_csv('..\/input\/shopee-product-matching\/test.csv', index_col='posting_id')","45fed0d4":"# Run train only for commit\nDO_TRAIN = len(df_test) == 3","06fbff81":"_tokenizer = SimpleTokenizer()\n\n# Copied from https:\/\/github.com\/openai\/CLIP\/blob\/beba48f35392a73c6c47ae67ddffced81ad1916d\/clip\/clip.py#L164\n# but with relaxed exception\ndef tokenize(texts, context_length: int = 77) -> torch.LongTensor:\n    if isinstance(texts, str):\n        texts = [texts]\n\n    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n\n    for i, tokens in enumerate(all_tokens):\n        n = min(len(tokens), context_length)\n        result[i, :n] = torch.tensor(tokens)[:n]\n        if len(tokens) > context_length:\n            result[i, -1] = tokens[-1]\n\n    return result","5738c09b":"RE_EMOJI = re.compile(r\"\\\\x[A-Za-z0-9.\/]+\", flags=re.UNICODE)\n\ndef strip_emoji(text):\n    return RE_EMOJI.sub(r'', text)","02623f71":"# Load CLIP\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"..\/input\/openai-clip\/RN50.pt\", device=device)","9410a3f9":"embed_dim = model.text_projection.shape[1]\nembed_dim","b8d33204":"class MyDataset(Dataset):\n    def __init__(self, df, images_path):\n        super().__init__()\n        self.df = df\n        self.images_path = images_path\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        \n        image = preprocess(Image.open(self.images_path \/ row['image']))\n        text = tokenize([strip_emoji(row['title'])])[0]\n        \n        return image, text","c941eebe":"if DO_TRAIN:\n    train_images_path = Path('..\/input\/shopee-product-matching\/train_images')\n    \n    df_train = pd.read_csv('..\/input\/shopee-product-matching\/train.csv', index_col='posting_id')\n\n    dstrain = MyDataset(df_train, train_images_path)\n    dltrain = DataLoader(dstrain, batch_size=32, shuffle=False, num_workers=2)\n\n    train_features = np.empty((len(df_train), 2*embed_dim), dtype=np.float32)\n\n    i = 0\n    for images, texts in tqdm(dltrain):\n        n = len(images)\n        with torch.no_grad():\n            images_features = model.encode_image(images.to(device))\n            texts_features = model.encode_text(texts.to(device))\n\n        train_features[i:i+n, :embed_dim] = images_features.cpu()\n        train_features[i:i+n, embed_dim:] = texts_features.cpu()\n\n        i += n\n    \n    np.save('train_features-no-norm.npy', train_features)\n\n    # l2-normalize\n    train_features \/= np.linalg.norm(train_features, 2, axis=1, keepdims=True)\n\n    # Create index\n    index = faiss.IndexFlatIP(2*embed_dim)\n\n    index.add(train_features)","44589da6":"%%time\nif DO_TRAIN:\n    similatiries, indexes = index.search(train_features, 50)","e443df12":"if DO_TRAIN:\n    np.save('similatiries.npy', similatiries)\n    np.save('indexes.npy', indexes)\n    \n    found_groups = df_train['label_group'].values[indexes]\n\n    is_same_group = (found_groups == found_groups[:, [0]])\n\n    plt.hist([similatiries[is_same_group], similatiries[~is_same_group]], density=True, bins=21,\n             label=['Same group', 'Different group'])\n    plt.legend();","5e289b5e":"GROUP_CUT = 0.84  # Use train code to find this number","6b473ad9":"test_images_path = Path('..\/input\/shopee-product-matching\/test_images')","f7766943":"dstest = MyDataset(df_test, test_images_path)\ndltest = DataLoader(dstest, batch_size=32, shuffle=False, num_workers=2)","1943f332":"test_features = np.empty((len(df_test), 2*embed_dim), dtype=np.float32)\n\ni = 0\nfor images, texts in tqdm(dltest):\n    n = len(images)\n    with torch.no_grad():\n        images_features = model.encode_image(images.to(device))\n        texts_features = model.encode_text(texts.to(device))\n        \n    test_features[i:i+n, :embed_dim] = images_features.cpu()\n    test_features[i:i+n, embed_dim:] = texts_features.cpu()\n    \n    i += n","7b4057d9":"# l2-normalize\ntest_features \/= np.linalg.norm(test_features, 2, axis=1, keepdims=True)","9e035bd4":"# Create index\nindex_test = faiss.IndexFlatIP(2 * embed_dim)\n\n\nindex_test.add(test_features)","cab0dc9f":"%%time\nsimilatiries, indexes = index_test.search(test_features, 50)","25bc75ab":"## TODO: try range_search\n# lims, similatiries, indexes = index_test.range_search(test_features, GROUP_CUT)","b8f01b8c":"test_are_same_groups = (similatiries > GROUP_CUT)","d12be768":"results = []\n\nfor i, (test_is_same_group, index_result) in enumerate(zip(test_are_same_groups, indexes)):\n    row_results = df_test.index[index_result[test_is_same_group]]\n    \n    results.append({\n        'posting_id': df_test.index[i],\n        'matches': ' '.join(row_results)\n    })\n    \ndf_sub = pd.DataFrame(results)","210edc8a":"df_sub.to_csv('submission.csv', index=False)","41f7b084":"!head submission.csv","9afdf67d":"## Run on test","f53cc1da":"## Generate features for train","51cafd50":"## Dirty code to make it work..."}}