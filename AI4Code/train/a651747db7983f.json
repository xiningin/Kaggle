{"cell_type":{"efa364cf":"code","2418097f":"code","c8db5380":"code","45bceb3d":"code","b98e4344":"code","b0d2128c":"code","bd2e0688":"code","ba3cbb51":"code","7da5f7d5":"code","9fd02ca7":"code","0e656f38":"code","4d20cb0e":"code","2dca434a":"code","f4260da1":"code","fbc438c9":"code","ecc399af":"markdown","dc21f974":"markdown","3e98f39e":"markdown","53fe89fd":"markdown"},"source":{"efa364cf":"#importing the libraries\nimport pandas as pd\nimport numpy as np\n\n#visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#import machine learning models\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nimport warnings\nwarnings.filterwarnings('ignore')","2418097f":"train=pd.read_csv('..\/input\/data-science-london-scikit-learn\/train.csv',header=None)\ntest=pd.read_csv('..\/input\/data-science-london-scikit-learn\/test.csv',header=None)\ntrainLabel=pd.read_csv('..\/input\/data-science-london-scikit-learn\/trainLabels.csv',header=None,names=['target'])","c8db5380":"print('train shape:', train.shape)\nprint('test shape:', test.shape)\nprint('trainLabel shape:', trainLabel.shape)\ntrain.head(10)","45bceb3d":"train.describe()","b98e4344":"train.info()","b0d2128c":"X,y=train,np.ravel(trainLabel)","bd2e0688":"X_train,X_val,y_train,y_val=train_test_split(X,y,random_state=100,test_size=0.2)","ba3cbb51":"algorithm=['auto', 'ball_tree', 'kd_tree', 'brute']\nweights=['uniform','distance']\nneig=range(1,20)\ntrain_accuracy=[]\nval_accuracy=[]\nbest_score=0.0\nbest_knn=None\n\nfor k in neig:\n    KNN=KNeighborsClassifier(n_neighbors=k,algorithm='auto',weights='uniform')\n    KNN.fit(X_train,y_train)\n    y_pred=KNN.predict(X_val)\n    train_score=KNN.score(X_train,y_train)\n    val_score=accuracy_score(y_val,y_pred)\n    # we can append accuracy in lists\n    train_accuracy.append(train_score)\n    val_accuracy.append(val_score)\n    \n    #we can save best accurcy in best_score\n    if val_score > best_score :\n        best_score=val_score\n        best_knn=KNN\n\n#we can plot the graph to show number of neighbors with accuracy\nplt.figure(figsize=(10,10))\nplt.plot(neig,train_accuracy,c='blue',label='train accuracy')\nplt.plot(neig,val_accuracy,c='red',label='val accuracy')\nplt.legend()\nplt.title('number of neighbors with accuracy')\nplt.xlabel('n _neighbors')\nplt.ylabel('Accuracy')\n\nprint('train score : ',best_knn.score(X_train,y_train))\nprint('val score : ',best_score)\nprint(best_knn)","7da5f7d5":"RandomForesClassifiertModel=RandomForestClassifier(random_state=100)\nestimator=[20,50,70,100]\nmax_depth=[20,30,40,60]\nsplit=[5,10,15]\nparam=dict(n_estimators=estimator,max_depth=max_depth,min_samples_split=split)\nRandomForestCV=GridSearchCV(estimator=RandomForesClassifiertModel,param_grid=param,cv=6,n_jobs=-1)\nRandomForestCV.fit(X_train,y_train)\ny_pred=RandomForestCV.predict(X_val)\nprint(RandomForestCV.best_params_)\nprint('score train : ',RandomForestCV.score(X_train,y_train))\nprint('score test  : ',accuracy_score(y_pred,y_val))","9fd02ca7":"kernel = ['linear','poly','rbf','sigmoid','precomputed']\nSVCModel=SVC(kernel='rbf',max_iter=1000,C=0.1)\nSVCModel.fit(X_train,y_train)\ny_pred=SVCModel.predict(X_val)\nprint('score train : ',SVCModel.score(X_train,y_train))\nprint('score test  : ',accuracy_score(y_val,y_pred))","0e656f38":"\nprint('X shape :',X.shape)\nprint('\\n')\n\n# USING THE GAUSSIAN MIXTURE MODEL \n\n#The Bayesian information criterion (BIC) can be used to select the number of components in a Gaussian Mixture in an efficient way. \n#In theory, it recovers the true number of components only in the asymptotic regime\n# aic and bic The lower the better.\n\nlowest_bic = np.infty\nbic = []\n\n#The GaussianMixture comes with different options to constrain the covariance of the difference classes estimated: \n# spherical, diagonal, tied or full covariance.\n\ncv_types = ['spherical', 'tied', 'diag', 'full']\nfor cv_type in cv_types:\n    for n_components in range(1,7):\n        gmm = GaussianMixture(n_components=n_components,covariance_type=cv_type)\n        gmm.fit(X)\n        bic.append(gmm.aic(X))\n        if bic[-1] < lowest_bic:\n            lowest_bic=bic[-1]\n            best_gmm=gmm\n                    \nbest_gmm.fit(X)\ngmm_train = best_gmm.predict_proba(X_train)\ngmm_val = best_gmm.predict_proba(X_val)\ngmm_test=best_gmm.predict_proba(test)\nbest_gmm\nprint(gmm.aic(X))","4d20cb0e":"print(\"gmm_test\",gmm_val.shape)\nprint(\"gmm_train\",gmm_train.shape)\nprint(\"X_train\",X_train.shape)\nprint(\"x_val\",X_val.shape)","2dca434a":"kernel = ['linear','poly','rbf','sigmoid','precomputed']\nSVCModel=SVC(kernel='rbf',max_iter=1000,C=0.1)\nSVCModel.fit(gmm_train,y_train)\ny_pred=SVCModel.predict(gmm_val)\nprint('score train : ',SVCModel.score(gmm_train,y_train))\nprint('score test  : ',accuracy_score(y_pred,y_val))\n","f4260da1":"algorithm=['auto', 'ball_tree', 'kd_tree', 'brute']\nweights=['uniform','distance']\nneig=range(1,20)\ntrain_accuracy=[]\nval_accuracy=[]\nbest_score=0.0\nbest_knn=None\n\nfor k in neig:\n    KNN=KNeighborsClassifier(n_neighbors=k,algorithm='auto',weights='uniform')\n    KNN.fit(gmm_train,y_train)\n    y_pred=KNN.predict(gmm_val)\n    train_score=KNN.score(gmm_train,y_train)\n    val_score=accuracy_score(y_val,y_pred)\n    # we can append accuracy in lists\n    train_accuracy.append(train_score)\n    val_accuracy.append(val_score)\n    \n    #we can save best accurcy in best_score\n    if val_score > best_score :\n        best_score=val_score\n        best_knn=KNN\n\n#we can plot the graph to show number of neighbors with accuracy\nplt.figure(figsize=(10,10))\nplt.plot(neig,train_accuracy,c='blue',label='train accuracy')\nplt.plot(neig,val_accuracy,c='red',label='val accuracy')\nplt.legend()\nplt.title('number of neighbors with accuracy')\nplt.xlabel('n _neighbors')\nplt.ylabel('Accuracy')\n\nprint('train score : ',best_knn.score(gmm_train,y_train))\nprint('val score : ',accuracy_score(y_pred,y_val))\n\n\nprint(best_knn)","fbc438c9":"RandomForesClassifiertModel=RandomForestClassifier(random_state=100)\nestimator=[20,50,70,100]\nmax_depth=[20,30,40,60]\nsplit=[5,10,15]\nparam=dict(n_estimators=estimator,max_depth=max_depth,min_samples_split=split)\nRandomForestCV=GridSearchCV(estimator=RandomForesClassifiertModel,param_grid=param,cv=6,n_jobs=-1)\nRandomForestCV.fit(gmm_train,y_train)\ny_pred=RandomForestCV.predict(gmm_val)\nprint(RandomForestCV.best_params_)\nprint('score train : ',RandomForestCV.score(gmm_train,y_train))\nprint('score test  : ',accuracy_score(y_pred,y_val))\n","ecc399af":"# we can apply Random Forest Classifier Model","dc21f974":" we can applay GAUSSIAN MIXTURE MODEL ","3e98f39e":"# let's go to apply KNeighborsClassifier Model","53fe89fd":"# now we can apply Support Vector Classifier Model"}}