{"cell_type":{"a16f605a":"code","a3d50f7e":"code","08964fb1":"code","0584844f":"code","3881c8cf":"code","c466cd3c":"code","c4e3903b":"code","76dcca44":"code","821ed14f":"code","2feb8bea":"code","3f6a4739":"code","b154e041":"code","d67991c2":"code","c5af97ed":"code","ea76955b":"code","aa1dce14":"code","9d01f074":"code","231fb61c":"code","83197d1f":"code","18463cdf":"code","67ff25ee":"code","47daddc2":"code","83a4bcd5":"code","36dfeb7a":"code","3850cb07":"code","5453ff7a":"code","348c643c":"code","0d60210d":"code","b60a186b":"code","c0678f9e":"code","b5b152eb":"code","1895cb68":"code","6da58eb3":"code","26d6c1fb":"code","0394db35":"code","a97fde8b":"code","9ebc4117":"code","156d8528":"code","83ad4138":"code","d06303ed":"code","c8f174ee":"code","a075c199":"code","591e8d43":"code","64bef05a":"code","bb639d39":"code","8582b6c9":"code","7d82e39a":"code","bbca9533":"code","f305fdd8":"code","d1a01b0d":"code","9d1403f4":"code","2ea7a5b2":"code","a42cc2e7":"code","2dac7bdb":"code","9f9a9377":"code","7b33a0b7":"code","613bdea9":"code","ecb0c9af":"code","358c3a9a":"code","6df364ee":"code","e7c72c9a":"code","9d6d970e":"code","3642f68a":"code","158df14e":"code","4af82276":"markdown","d77f1d57":"markdown","bfb0c51a":"markdown","abd19993":"markdown","e52de860":"markdown","83dcc60e":"markdown","0f612d75":"markdown","1a0fad5d":"markdown","2949d6a8":"markdown","8fc4c771":"markdown","ac303efb":"markdown","e59103d5":"markdown","88e8d145":"markdown","f5f9d552":"markdown","7fc97723":"markdown","a1243097":"markdown","7f58f585":"markdown"},"source":{"a16f605a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n#import statsmodels as sm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a3d50f7e":"df_train = pd.read_csv('..\/input\/train.csv', nrows= 10_000_000)","08964fb1":"df_train.pickup_datetime = df_train.pickup_datetime.str.slice(0,16)","0584844f":"df_train['pickup_datetime'] = pd.to_datetime(df_train['pickup_datetime'], utc=True, format = '%Y-%m-%d %H:%M')","3881c8cf":"df_train.info()","c466cd3c":"df_train.head()","c4e3903b":"df_train.describe()","76dcca44":"# Filter out negative fare amount and maximum passenger_count\nprint(\"Old Size Before Filter: %d\" %(len(df_train)))\ndf_train = df_train[(df_train.fare_amount >=0) & (df_train.passenger_count <=10)]\nprint(\"New Size After Filter: %d\" %(len(df_train)))","821ed14f":"# Let's plot histogram of fare_amount to see its distribution across data.\n\ndf_train[df_train.fare_amount < 100].fare_amount.hist(bins=100, figsize=(10,5))\nplt.xlabel(\"Fare in USD $\")\nplt.title(\"Fare Amount Distribution\")","2feb8bea":"print(df_train.isnull().sum())","3f6a4739":"print(\"Old Size : %d\" %(len(df_train)))\ndf_train=df_train.dropna(axis = 0)\nprint(\"New Size: %d\" %(len(df_train)))","b154e041":"# Filtering out off boundary points. Boundary of New York City is (-75, -73, 40, 42)\ndef NYC(df):\n    boundary_filter = (df.pickup_longitude >= -75) & (df.pickup_longitude <= -73) & \\\n                      (df.pickup_latitude >= 40) & (df.pickup_latitude <= 42) & \\\n                      (df.dropoff_longitude >= -75) & (df.dropoff_longitude <= -73) & \\\n                      (df.dropoff_latitude >= 40) & (df.dropoff_latitude <= 42)\n    df = df[boundary_filter]\n    return df","d67991c2":"print('Old size: %d' % len(df_train))\ndf_train = NYC(df_train)\nprint('New size: %d' % len(df_train))","c5af97ed":"def distance_between_pickup_dropoff(pickup_lat, pickup_long, dropoff_lat, dropoff_long):\n    d = np.abs(dropoff_lat - pickup_lat) + np.abs(dropoff_long - pickup_long)\n    return d","ea76955b":"# Extracting Features \n\ndf_train['hour'] = df_train.pickup_datetime.dt.hour\ndf_train['day'] = df_train.pickup_datetime.dt.day\ndf_train['month'] = df_train.pickup_datetime.dt.month\ndf_train['year'] = df_train.pickup_datetime.dt.year\ndf_train.drop('pickup_datetime', axis =1, inplace = True)\n\n# Creating actual_distance column as measure of manhattan distance\n\ndf_train['actual_distance'] = distance_between_pickup_dropoff(df_train.pickup_latitude, df_train.pickup_longitude,\n                                                             df_train.dropoff_latitude, df_train.dropoff_longitude)","aa1dce14":"# Let's check how our new data set looks like.\n\ndf_train.head()","9d01f074":"# Here, i am bounding the longitude and latitude values to get clear and zoomed plot.\ndf_plot = df_train[(df_train.pickup_longitude >= -74.1)&(df_train.pickup_longitude <= -73.8) & (df_train.pickup_latitude >=40.6)\n                  & (df_train.pickup_latitude <=40.9)]","231fb61c":"# In scatter plot arguments c = 'r' is \"color red\", s= 0.01 is \"size of dots\" and alpha = 0.5 is \"opacity of dots\"\nfig, ax = plt.subplots(1, 1, figsize=[10,10])\nax.scatter(df_plot.pickup_longitude[:3_00_000], df_plot.pickup_latitude[:3_00_000],c = 'r', s= 0.01,alpha=0.5)","83197d1f":"# Let's zoom little more\n\nzoomed_data =  df_train[(df_train.pickup_longitude >= -74.02)&(df_train.pickup_longitude <= -73.95) & (df_train.pickup_latitude >=40.7)\n                  & (df_train.pickup_latitude <=40.80)]","18463cdf":"fig, ax = plt.subplots(1, 1, figsize=[10,10])\nax.scatter(zoomed_data.pickup_longitude[:3_00_000], zoomed_data.pickup_latitude[:3_00_000],c = 'b', s= 0.01,alpha=0.5)","67ff25ee":"# Let's create feature vector\n# We do not want trip involving 0 passenger_count\nfilt = (df_train.passenger_count > 0) & (df_train.fare_amount < 250)\nfeatures = ['passenger_count','hour','year','day','month','actual_distance']","47daddc2":"for f in features:\n    related = df_train.fare_amount.corr(df_train[f])\n    print(\"%s: %f\" % (f,related))","83a4bcd5":"final_features =['year','hour','actual_distance','passenger_count']","36dfeb7a":"X = df_train[filt][final_features].values # Feature Vector\nY = df_train[filt]['fare_amount'].values # Target Variable","3850cb07":"X.shape, Y.shape","5453ff7a":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score","348c643c":"# Splitting data set into train and test \n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25)","0d60210d":"regressor = LinearRegression()","b60a186b":"metric = 'neg_mean_squared_error'\nscores = cross_val_score(regressor, X_test, y_test, cv = 10, scoring = metric)","c0678f9e":"scores","b5b152eb":"np.sqrt(np.abs(scores))","1895cb68":"np.sqrt(np.abs(scores.mean()))","6da58eb3":"regressor.fit(X_train, y_train)","26d6c1fb":"y_train_pred = regressor.predict(X_train)","0394db35":"def error(y, y_pred):\n    return np.sqrt(mean_squared_error(y,y_pred))","a97fde8b":"rmse = error(y_train, y_train_pred)","9ebc4117":"rmse","156d8528":"y_test_pred = regressor.predict(X_test)","83ad4138":"rmse = error(y_test, y_test_pred)","d06303ed":"rmse","c8f174ee":"from sklearn.linear_model import Lasso","a075c199":"alphas =[1e-5,1e-3, 1e-2, 0.02, 0.04,0.08,0.1]","591e8d43":"for alpha in alphas:\n    lasso = Lasso(alpha = alpha)\n    lasso.fit(X_train, y_train)\n    y_train_pred = lasso.predict(X_train)\n    rmse = error(y_train, y_train_pred)\n    print(\"alpha : {%.5f} RMSE : {%.9f}\" %(alpha,rmse))","64bef05a":"lasso = Lasso(alpha = 0.01)","bb639d39":"lasso.fit(X_train, y_train)","8582b6c9":"y_test_pred = lasso.predict(X_test)","7d82e39a":"rmse = error(y_test, y_test_pred)","bbca9533":"rmse","f305fdd8":"from sklearn.tree import DecisionTreeRegressor","d1a01b0d":"reg = DecisionTreeRegressor(max_depth = 17)","9d1403f4":"reg.fit(X_train, y_train)","2ea7a5b2":"y_trn_pred = reg.predict(X_train)","a42cc2e7":"rmse = error(y_train, y_trn_pred)","2dac7bdb":"rmse","9f9a9377":"y_tst_pred = reg.predict(X_test)\n","7b33a0b7":"rmse = error(y_test, y_tst_pred)","613bdea9":"rmse","ecb0c9af":"df_test =  pd.read_csv('..\/input\/test.csv')\ndf_test.head()","358c3a9a":"df_test.pickup_datetime = df_test.pickup_datetime.str.slice(0,16)\n\ndf_test['pickup_datetime'] = pd.to_datetime(df_test['pickup_datetime'], utc=True, format = '%Y-%m-%d %H:%M')","6df364ee":"# Extracting Features for test set\n\ndf_test['hour'] = df_test.pickup_datetime.dt.hour\ndf_test['day'] = df_test.pickup_datetime.dt.day\ndf_test['month'] = df_test.pickup_datetime.dt.month\ndf_test['year'] = df_test.pickup_datetime.dt.year\ndf_test.drop('pickup_datetime', axis =1, inplace = True)\n\n# Creating actual_distance column as measure of manhattan distance\n\ndf_test['actual_distance'] = distance_between_pickup_dropoff(df_test.pickup_latitude, df_test.pickup_longitude,\n                                                             df_test.dropoff_latitude, df_test.dropoff_longitude)","e7c72c9a":"X_test = df_test[final_features].values","9d6d970e":"y_pred_test_set = reg.predict(X_test)","3642f68a":"submission =  pd.DataFrame({'key': df_test.key, 'fare_amount': y_pred_test_set},columns = ['key', 'fare_amount'])\nsubmission.to_csv('submission.csv', index = False)","158df14e":"submission","4af82276":"### Further Improvement:\nI also came to know that there are many more algorithms available to improve RMSE score. Ex: Gradient Boosting Algorithms\n\nThere is no perfect solution to Machine Learning problem. One algorithm might works well for one problem while the same algorithm might not work for other but same kind of problem. We have to test many algorithm before reaching to any conclusion on problem. ","d77f1d57":"As we know that, some data oints have minimum and maximum lattitude\/longitude off the boundary of NYC. So, we have to filter those outliers too.","bfb0c51a":"# NYC Taxi Fare Prediction: Multiple Linear Regression\n\nThe dataset is provided by Google. It is very large dataset having around 50M records. The decription of the features are avilable in Data section at Kaggle.\n\nThere are two main files. \n1. **train.csv **(We will use this to perform EDA, build and train our model.)\n2. **test.csv** (We will use this file to validate our model by generating predictions.)\n\nFor an excellent EDA you can refer to <a href=\"https:\/\/www.kaggle.com\/breemen\/nyc-taxi-fare-data-exploration\">NYC Taxi Fare: Data Exploration.<\/a> kernel on Kaggle.","abd19993":"## Building a Model:\n\nIt's time to build a model for training. I will use Linear Regression model of Scikit learn library. I will also measure RMSE(Root Mean Squared Error) to check accuracy of training set and test set.","e52de860":"## Visualization\n\nWe have latitude and longitude values. So, we can use those to plot the data and see what will come up.\n\nHere, I am using pickup_latitude and pickup_longitude. We can generate smae plot for dropoff_latitude and dropoff_longitude too.","83dcc60e":"## Data Cleaning: ","0f612d75":"Let's slicing off unecessary components of the datetime and specify the date format. This will results in a more efficiecnt conversion to a datetime object.","1a0fad5d":"### Computing Distance :\n\nWe know that **\"Manhattan Distance\"** metric will give us better approximation of distance between two points in given plane.So, we will use manhattan distance formula to compute distance between pickup and dropoff points.","2949d6a8":"I am refering <a href=\"https:\/\/www.kaggle.com\/breemen\/nyc-taxi-fare-data-exploration\">this kernel <\/a> for EDA. It is really an inspiring one and have very deep analysis of data. There are many good explorations in that kernel that, I have used here.","8fc4c771":"In the histogram of the fare_amount there are some small spikes between USD 40 and USD 60. This could indicate some fixed fare price (e.g. to\/from airport). That is further explored in <a href=\"https:\/\/www.kaggle.com\/breemen\/nyc-taxi-fare-data-exploration\">NYC Taxi Fare: Data Exploration.<\/a>","ac303efb":"# Kaggle Submission","e59103d5":"## Decision Tree Regressor\n\nThe RMSE value using Lasso and Simple Linear Regression is almost same and little bigger. So, I search on the internet for, \"How to reduce RMSE for regression model?\" The use of cross validation was one of the idea. However, I also came to know that Decision Tree Regressor might come in handy to reduce RMSE or improve accuracy of regression problem. \n\nYou can see how its easy to use Decision Tree Regressor on regression problem. One thing to notice that you have to provide value for **max_depth** argument. If you do not provide value then the algorithm will go in too much depth that you will get RMSE near to 0. It means that decision tree run into problem of **\"Overfitting\"**. No one wants that.","88e8d145":"Let's check for missing or null values present in the dataset. It would not affect model if we remove those null values as the dataset is large.","f5f9d552":"## Multiple Linear Regression: \n\nWe all knows that multiple linear regression is used on problems having more than 1 independent variables. Here,\nindependent variables means features(columns of dataset), which are used to predict target variable (dependent\nvariable).  Here, I am going to use MLR technique.","7fc97723":"### Extracting New Features:\n\nI have used taxi many times (e.g Uber) to travel from school to home and home to school. In my general observation I have found many interesting facts about fare prices. \n\n\n1. The fare amount was high in peak hours. (for ex: Morning and Night time)\n2. The fare amount was high during bad weathers too. (snow or thunder storm)\n3. The fare amount was high on holidays and sometimes in weekends.\n4. The fare amount was low in afternoons and before noon.\n\nFrom this observations, I can say that time is also one of the factors affecting fare_amount. So, we can use features like \"hour\" and \"day\". Also, we can consider \"month\" and \"year\"  too as features and see if those affects fare_amount.\n","a1243097":"There are few important observation from this description of dataset.\n\n* Minimum fare_amount is negative, which is quite unrealistic in the scenario. I will drop those fields.  \n* Notice that some minimum and maximum longitude\/latitude are off the boundary of New York city. I will also remove those. May be, I will define boundary for longitude\/lattitude(lat: 40.7141667, long:  -74.0063889) to remove outliers.\n* Also, maximum passenger_count is 208, which is also an outlier. I will drop those records too.\n","7f58f585":"You can see that RMSE value for training is low than RMSE value for testing data. It is common to have this. The model does not have problem of \"Overfitting\" or \"Underfitting\". \n"}}