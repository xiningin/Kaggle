{"cell_type":{"88c56118":"code","70e8107b":"code","f8354173":"code","13978854":"code","52e8ad71":"code","0b000392":"code","841d04e3":"code","cf2fbd87":"code","41b20e7a":"code","390102bb":"code","8660029f":"code","ef49de27":"code","c67ac468":"code","9e22fbac":"code","68f4cbc9":"code","7d710beb":"code","510c9c52":"code","c084333c":"code","bba1aa4f":"code","6198a8f3":"code","dee95bbb":"code","cd51b660":"code","942a7b77":"code","a475151b":"code","33f08a1b":"code","6513fb0c":"code","3d9ec66d":"code","3e29fa6f":"code","e73271c5":"code","6d58358e":"code","4812d565":"code","ec73780e":"code","0a8da726":"code","94287781":"code","1c930f85":"code","09e53aab":"code","39675cc0":"code","b2d0372f":"code","2298f31f":"code","863821dc":"code","7ad77324":"code","e0cb3964":"code","86c44d46":"code","2b85e1ec":"code","d4e8e640":"code","f16d83c0":"code","fac9659a":"code","02845b0b":"code","6be138cb":"code","b4e005be":"code","0504b356":"code","ac92190d":"code","83dcf5e7":"code","e1be7cf6":"code","3feb743f":"code","166e5e90":"code","6fe121e0":"code","6d4509bb":"code","b4b156e1":"code","2bd8619d":"code","da52a763":"code","14e15c9d":"code","1b015f84":"code","6c386bb3":"code","300220fa":"code","fbf68a37":"code","ffc58d6d":"code","6aaa3cfd":"code","7056a781":"code","9ac3a453":"code","50238581":"code","d380cc6f":"code","35a4a262":"code","1b608ae6":"code","ee8efea9":"code","42adad53":"code","74dd9a1c":"code","11413885":"code","b9166ecc":"code","3b8bc724":"code","b6269d44":"code","ab825976":"code","07c21d1b":"markdown","b4ae86a2":"markdown","12068a5a":"markdown","80611f63":"markdown","d9cb7578":"markdown","ead87169":"markdown","1edd7892":"markdown","cc348352":"markdown"},"source":{"88c56118":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\n\n# to display all columns and rows:\npd.set_option('display.max_columns', None); pd.set_option('display.max_rows', None);\n\n#to arrange the decimals\npd.set_option('display.float_format', lambda x: '%.2f' % x) \n","70e8107b":"df_2010_2011 = pd.read_excel(\"..\/input\/online-retail\/online_retail_II.xlsx\", sheet_name = \"Year 2010-2011\")","f8354173":"df = df_2010_2011.copy()","13978854":"df.head()","52e8ad71":"df.info()","0b000392":"#try to understand the data by using the functions that can be used as a first look at the data.","841d04e3":"#the number of unique products\ndf[\"Description\"].nunique()","cf2fbd87":"#how many products are there?\ndf[\"Description\"].value_counts().head()","41b20e7a":"#sorting the products from the most purhased product to the least along with their prices:\ndf.groupby(\"Description\").agg({\"Quantity\":\"sum\", \"Price\":\"sum\"}).sort_values(\"Quantity\", ascending = False).head(10)","390102bb":"#the most purchased product is 'World War 2 gliders designs","8660029f":"#There are products whose qtys are negative and the invoices starting with C are returned. Let's find the most returned product first:","ef49de27":"df.groupby([\"Invoice\",\"Description\"] ).agg({\"Quantity\":\"sum\"}).sort_values(\"Quantity\", ascending=True).head()","c67ac468":"# as can be seen from above, the most refunded product is 'paper craft, little birdie'","9e22fbac":"df.isnull().sum()","68f4cbc9":"df.dropna(inplace = True)","7d710beb":"#Changing type of Customer ID from float to int\ndf[\"Customer ID\"] = df[\"Customer ID\"].astype(int)\ndf.head()","510c9c52":"#checking the most refunded product by controlling the refund invoices (invoices which starts with 'C'): \n#again, we found that the most refunded product is paper craft little birdie\n\ndf_C = df[df[\"Invoice\"].str.contains(\"C\")==True]\n\ndf_C.groupby([\"Description\"]).agg({\"Quantity\":\"sum\"}).sort_values(\"Quantity\", ascending=True).head()","c084333c":"df_C.sort_values(\"Quantity\", ascending=True).head()  #This is another way to do the same thing as above","bba1aa4f":"#how many invoices are there?\ndf[\"Invoice\"].nunique()","6198a8f3":"#How much money has been earned per invoice? (It is necessary to create a new variable by multiplying two variables)\n\ndf[\"TotalPrice\"] = df[\"Quantity\"]*df[\"Price\"]","dee95bbb":"df.head()","cd51b660":"#the invoice with the largest amount and its product:\ndf.groupby([\"Invoice\",\"Description\"]).agg({\"TotalPrice\":\"sum\"}).sort_values(\"TotalPrice\", ascending=False).head()","942a7b77":"#the most expensive products\ndf.sort_values(\"Price\", ascending = False).head()","a475151b":"#the product which we gained the most from:\ndf.groupby(\"Description\").agg({\"TotalPrice\":\"sum\"}).sort_values(\"TotalPrice\", ascending = False).head(10)","33f08a1b":" # remove rows with invalid invoice that contains with C and assign it to a new data frame:\ndf['Invoice'] = df['Invoice'].astype('str')\ndf2 = df[~df['Invoice'].str.contains('C')]    \ndf2.head()","6513fb0c":"#how many products are there from each type?\ndf2[\"Description\"].value_counts().head()","3d9ec66d":"#How many orders came from which country? we discard returns because they would be counted even if they were returned\ndf2[\"Country\"].value_counts()","3e29fa6f":"#how much money we gained from the countries? we use df again because the negative values have to drop from the total\ndf.groupby(\"Country\").agg({\"TotalPrice\":\"sum\"}).sort_values(\"TotalPrice\", ascending = False).head()","e73271c5":"df2.shape","6d58358e":"df2.head()","4812d565":"df2.describe([0.01,0.05,0.10,0.25,0.50,0.75,0.90,0.95, 0.99]).T","ec73780e":"#we got the outlier values for our information, but we will not use it because we do not build a model\nfor feature in [\"Quantity\",\"Price\",\"TotalPrice\"]:\n\n    Q1 = df[feature].quantile(0.01)\n    Q3 = df[feature].quantile(0.99)\n    IQR = Q3-Q1\n    upper = Q3 + 1.5*IQR\n    lower = Q1 - 1.5*IQR\n\n    if df[(df[feature] > upper) | (df[feature] < lower)].any(axis=None):\n        print(feature,\"yes\")\n        print(df[(df[feature] > upper) | (df[feature] < lower)].shape[0])\n    else:\n        print(feature, \"no\")","0a8da726":"df2.head(10)","94287781":"df2.info()","1c930f85":"df2[\"InvoiceDate\"].min()","09e53aab":"df2[\"InvoiceDate\"].max()","39675cc0":"#assigning today date as the 1 day after the max date\nimport datetime as dt\ntoday_date = dt.datetime(2011,12,10)","b2d0372f":"today_date","2298f31f":"df2.groupby(\"Customer ID\").agg({\"InvoiceDate\":\"max\"}).head()","863821dc":"#what should we do now?\n\n#For each customer, we need to subtract the customers' last purchase date from today's date.\n#and we will keep this as a temp data frame","7ad77324":"(today_date - df2.groupby(\"Customer ID\").agg({\"InvoiceDate\":\"max\"})).head()","e0cb3964":"temp_df = (today_date - df2.groupby(\"Customer ID\").agg({\"InvoiceDate\":\"max\"}))","86c44d46":"(today_date - df2.groupby([\"Customer ID\",\"Description\"]).agg({\"InvoiceDate\":\"max\"})).head(5) \n#I did this to check whether the Customer IDs are valid IDs with valid transactions","2b85e1ec":"temp_df.rename(columns={\"InvoiceDate\": \"Recency\"}, inplace = True)  #changing the name of InvoiceDate to Recency","d4e8e640":"temp_df.head()","f16d83c0":"#her bir de\u011feri yakala, g\u00fcnleri al. time tipinde de\u011fi\u015fken oldu\u011fundan x.days diyince g\u00fcnleri, x.months diyince aylar\u0131, x.years diyince y\u0131llar\u0131 \u00e7eker\nrecency_df = temp_df[\"Recency\"].apply(lambda x: x.days)","fac9659a":"recency_df.head()","02845b0b":"#df.groupby(\"Customer ID\").agg({\"InvoiceDate\": lambda x: (today_date - x.max()).days}).head(): k\u0131sa yolla yap\u0131l\u0131\u015f\u0131","6be138cb":"#Frequency, m\u00fc\u015fterinin toplam sat\u0131n alma say\u0131s\u0131d\u0131r.\n# Her bir m\u00fc\u015fteriye kesilmi\u015f farkl\u0131 fatura say\u0131lar\u0131n\u0131 bulursak, m\u00fc\u015fterinin toplam sat\u0131n alma say\u0131s\u0131n\u0131 bulmu\u015f oluruz. Bu da frequency de\u011ferimizdir.\n#\u0130lk \u00f6nce M\u00fc\u015fteri ID ve Fatura No baz\u0131nda gruplayarak, her bir faturan\u0131n M\u00fc\u015fteri baz\u0131nda ka\u00e7 sefer \u00e7okland\u0131\u011f\u0131n\u0131 g\u00f6zlemliyoruz.\u0130lk 5 g\u00f6zleme bakal\u0131m","b4e005be":"temp_df = df2.groupby([\"Customer ID\",\"Invoice\"]).agg({\"Invoice\":\"count\"})\ntemp_df.head()","0504b356":"#this shows how many different products each customer bought:\ntemp_df.groupby(\"Customer ID\").agg({\"Invoice\":\"sum\"}).head() ","ac92190d":"#this shows how many times the customer did shopping:\ntemp_df.groupby(\"Customer ID\").agg({\"Invoice\":\"count\"}).head() ","83dcf5e7":"#assigning this to a new dataframe called freq_df and changing the Invoice name to frequency\nfreq_df = temp_df.groupby(\"Customer ID\").agg({\"Invoice\":\"count\"})\nfreq_df.rename(columns={\"Invoice\": \"Frequency\"}, inplace = True)\nfreq_df.head()","e1be7cf6":"#sorting the values from the highest to the lowest. the customer with most orders:17841\nfreq_df.sort_values(\"Frequency\", ascending = False).head()","3feb743f":"#Monetary is the total spend of the customer.\n# The sum of the TotalPrice values that we have calculated for each customer before is that customer's monetary value:","166e5e90":"monetary_df = df2.groupby(\"Customer ID\").agg({\"TotalPrice\":\"sum\"})\nmonetary_df.head()","6fe121e0":"#finding the customer which we gained the most money from: 14646 (the customer with most orders is 17841 so they are not the same)\nmonetary_df.sort_values(by=\"TotalPrice\", ascending=False).head(10)","6d4509bb":"monetary_df.rename(columns={\"TotalPrice\": \"Monetary\"}, inplace = True)\nmonetary_df.head()","b4b156e1":"print(recency_df.shape, freq_df.shape, monetary_df.shape)","2bd8619d":"recency_df.shape","da52a763":"#Lets bring all the values together under a new data frame called 'rfm':\nrfm = pd.concat([recency_df, freq_df, monetary_df],  axis=1)\nrfm.head()","14e15c9d":"#Normally the smallest of the recency scoring, which is 1, is the best recency score. \n#However, we will define this in reverse and put the value 5 as the best recency value so that it will be the same as the others, \n#so score 5 will be the most recent and the best recency score:\n\nrfm[\"RecencyScore\"] = pd.qcut(rfm['Recency'], 5, labels = [5, 4, 3, 2, 1])","1b015f84":"rfm[\"Frequency\"].describe([0.01,0.05,0.10,0.25,0.50,0.60,0.75,0.90,0.95, 0.99]).T","6c386bb3":"cut_bins = [0,1,2,3,9,210]\n\nrfm[\"FrequencyScore\"] = pd.cut(rfm[\"Frequency\"], bins = cut_bins, labels = [1, 2, 3, 4, 5])\nrfm[\"FrequencyScore\"].value_counts()\n","300220fa":"rfm[\"MonetaryScore\"] = pd.qcut(rfm['Monetary'], 5, labels = [1, 2, 3, 4, 5])\nrfm.head()","fbf68a37":"(rfm['RecencyScore'].astype(str) + \n rfm['FrequencyScore'].astype(str) + \n rfm['MonetaryScore'].astype(str)).head()","ffc58d6d":"rfm[\"RFM_SCORE\"] = rfm['RecencyScore'].astype(str) + rfm['FrequencyScore'].astype(str) + rfm['MonetaryScore'].astype(str)\nrfm.head()","6aaa3cfd":"#just to check whether 12347 is a test account or a valid customer account. it really is a valid account\ndf[df[\"Customer ID\"]==12347].head()   ","7056a781":"rfm.describe().T","9ac3a453":"#the champions:\nrfm[rfm[\"RFM_SCORE\"] == \"555\"].head()","50238581":"#the worst customers:\nrfm[rfm[\"RFM_SCORE\"] == \"111\"].head()","d380cc6f":"#To segment the customers using \u201cRecency\u201d and \u201cFrequency\u201d values: \n#set up regular expression (regex) structure by using dictionaries to name Customer Segments according to \n#Recency and Frequency Scores:\n\nseg_map = {\n    r'[1-2][1-2]': 'Hibernating',\n    r'[1-2][3-4]': 'At Risk',\n    r'[1-2]5': 'Can\\'t Lose',\n    r'3[1-2]': 'About to Sleep',\n    r'33': 'Need Attention',\n    r'[3-4][4-5]': 'Loyal Customers',\n    r'41': 'Promising',\n    r'51': 'New Customers',\n    r'[4-5][2-3]': 'Potential Loyalists',\n    r'5[4-5]': 'Champions'\n}","35a4a262":"#recency ve frequency points are obtained with the following code \n#(as the monetary is a similar value to frequency, it is not considered here):\nrfm['Segment'] = rfm['RecencyScore'].astype(str) + rfm['FrequencyScore'].astype(str)\nrfm['Segment'].head()","1b608ae6":"#Pull the value corresponding to the ranges of the score in the dictionary defined in 'seg_map', \n#and add it as a new column named \"Segment\":\n\nrfm['Segment'] = rfm['Segment'].replace(seg_map, regex=True)\nrfm.head()","ee8efea9":"#Retrieve statistical values of recency, frequency and monetary values by segment classes:\nrfm[[\"Segment\", \"Recency\",\"Frequency\",\"Monetary\"]].groupby(\"Segment\").agg([\"mean\",\"count\"])","42adad53":"rfm[rfm[\"Segment\"] == \"Need Attention\"].head()","74dd9a1c":"need_attention_df = pd.DataFrame()\nneed_attention_df[\"Need_Attention_CustomerID\"] = rfm[rfm[\"Segment\"] == \"Need Attention\"].index\nneed_attention_df.head()","11413885":"#convert it to excel file. business intelligence job. after they get the customer ids, they sent it to sales and marketing department.\nneed_attention_df.to_csv(\"need_attention.csv\")","b9166ecc":"rfm[rfm[\"Segment\"] == \"At Risk\"].head()","3b8bc724":"at_risk_df = pd.DataFrame()\nat_risk_df[\"At_Risk_CustomerID\"] = rfm[rfm[\"Segment\"] == \"At Risk\"].index\nat_risk_df.to_csv(\"at_risk.csv\")","b6269d44":"rfm[rfm[\"Segment\"] == \"Can't Lose\"].head()\n","ab825976":"cant_lose_df = pd.DataFrame()\ncant_lose_df[\"Can't_Lose_CustomerID\"] = rfm[rfm[\"Segment\"] == \"Can't Lose\"].index\ncant_lose_df.to_csv(\"cant_lose.csv\")","07c21d1b":"# Data Understanding ","b4ae86a2":"# Frequency","12068a5a":"\nrfm_level_ag.columns = ['Recencycount','Frequencycount', 'Monetarycount','RFM_SCOREcount']\n#Create our plot and resize it.\nfig = plt.gcf()\nax = fig.add_subplot()\nfig.set_size_inches(16, 9)\nsquarify.plot(sizes=rfm_level_ag['RFM_SCOREcount'],\n              label=['Can\\'t Loose',\n                     'Champions',\n                     'Loyal Customers',\n                     'Need Attention',\n                     'Potential Loyalists',\n                     'Promising',\n                     'Hibernating',\n                     'About to Sleep',\n                     'Loyal Customers',\n                     'New Customers',\n                     'At Risk',\n                     'About to Sleep',], alpha=.6 )\nplt.title(\"RFM Segments\",fontsize=18,fontweight=\"bold\")\nplt.axis('off')\nplt.show()","80611f63":"# Monetary","d9cb7578":"# Data Preparation","ead87169":"# Customer Segmentation with RFM Scores\n\nRFM is abbreviation for Recency, Frequency and Monetary. It is a technique that helps determine marketing and sales strategies based on customers' buying habits.\nRecency: Time passed since the customer's last purchase. In other words, it is the \"time since the last contact of the customer\".\nIt is found from the formula:\u00a0\nRecency= RFM analysis date\u200a-\u200aLast purchase date\nFor example, if we are doing this analysis today, then the analysis date is today's date.\u00a0\nFrequency: Total number of purchases. It shows how frequently the customer does shopping. It can be found from the number of the invoices that one customer has.\nMonetary (Monetary Value): Total spending by the customer.\nCustomer segmentation is the process of separating these values into groups by scoring between 1 and 5. Depending on these scores, the customers are segmented into different groups.\n","1edd7892":"# Business Problem \n\n\nAn e-commerce company which sells souvenirs wants to segment its customers and determine marketing strategies according to these segments.\nFor this purpose, we will define the behavior of customers and we will put the customers into same groups who exhibit common behaviors and then we will try to develop sales and marketing techniques specific to these groups.\n\n**Data set information**\n\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Online+Retail+II\n\n\nThe data set \"online_retail_II\" includes the sales of this online shop between 01\/12\/2009 \u2013 09\/12\/2011. For this study, the data of the year 2010\u20132011 is chosen.\n\n**Variables:**\n\n\nInvoiceNo: Invoice number. It is a unique value. If this code starts with C, it means refund.\nStockCode: Product code. Unique number for each product\nDescription: Product name\nQuantity: Number of products. It means how many of the products in the invoices are sold. Those who start with C get negative value\nInvoiceDate: Invoice date and time\nUnitPrice: Product price (in pounds)\nCustomerID: Customer number. Unique number for each customer\nCountry: Country name. Refers to the country where the customer lives\n\n","cc348352":"Now if we take today's date as the analyse date, then there will be a very serious difference.\n\nFor this reason, we have set the next day of the maximum day of the data set as the analyse date.\n\nNow, we can do the segmentation according to the day of the last recording."}}