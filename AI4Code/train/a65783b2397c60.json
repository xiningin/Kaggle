{"cell_type":{"f12e1ba5":"code","a215bd0b":"code","25206d40":"code","d3002e3a":"code","a8302557":"code","56fb28c0":"code","85bf7c0b":"code","40d1f3e8":"code","3d41d128":"code","457c6d57":"code","9c1bdbcb":"code","355a9270":"code","eb59e1cc":"markdown","044cf17b":"markdown","127a07ea":"markdown","d444068f":"markdown","96116558":"markdown","297f094b":"markdown","7f935f2e":"markdown"},"source":{"f12e1ba5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a215bd0b":"train_data = pd.read_csv('..\/input\/train_data.csv')\ntrain_data.drop(['Unnamed: 0','PassengerId'], axis=1, inplace=True)\n\ntest_data = pd.read_csv('..\/input\/test_data.csv')\ntest_data.drop(['Unnamed: 0','PassengerId'], axis=1, inplace=True)","25206d40":"train_data = (train_data - np.min(train_data)) \/ (np.max(train_data) - np.min(train_data))\n\ntest_data = (test_data - np.min(test_data)) \/ (np.max(test_data) - np.min(test_data))","d3002e3a":"train_data.head(6)","a8302557":"test_data.head(6)","56fb28c0":"x_train = train_data.drop('Survived', axis=1).T\ny_train = train_data.iloc[:,0].values.reshape(1,-1)\n\nx_test = test_data.drop('Survived', axis=1).T\ny_test = test_data.iloc[:,0].values.reshape(1,-1)\n\nprint(' x_train: {}, y_train: {} \\n x_test: {}, y_test: {}'.format(x_train.shape, y_train.shape, x_test.shape, y_test.shape))","85bf7c0b":"def initialize_weight_and_bias(dimension):\n    weight = np.full((dimension, 1), 0.1)   # (dimension,1) is shape number and values of weight are 0.1\n    bias = 0.0\n    \n    return weight, bias\n\n\ndef sigmoid(z):\n    # sigmoid function formul\u00fcn\u00fc kulland\u0131k. \n    \n    y_head = 1 \/ (1 + np.exp(-z))  # np.exp(1) means \"e on 1\". 'e' is euler number. Approximate 2.718...\n    \n    return y_head","40d1f3e8":"def forward_backward_propagation(weight, bias, x_train,y_train):\n    # forward propagation\n    \n    z = np.dot(weight.T, x_train) + bias\n    \n    y_head = sigmoid(z)\n    \n    loss = -(1 - y_train)*np.log(1 - y_head) - y_train*np.log(y_head)\n    \n    cost = np.sum(loss) \/ x_train.shape[1]\n                            # 455\n    \n    # backward propagation\n    \n    derivative_weight = np.dot(x_train, (y_head - y_train).T) \/ x_train.shape[1]\n    derivative_bias = np.sum(y_head - y_train) \/ x_train.shape[1]\n    \n    gradients = {\"derivative_weight\" : derivative_weight, \n                 \"derivative_bias\" : derivative_bias\n                 }\n    \n    return cost, gradients","3d41d128":"def update(weight, bias, x_train, y_train, learning_rate, number_of_iterarion):\n    cost_list = []\n    index = []\n    \n    for i in range(number_of_iterarion):\n        cost, gradients = forward_backward_propagation(weight, bias, x_train, y_train)\n        cost_list.append(cost)\n        index.append(i)\n        \n        weight = weight - learning_rate * gradients[\"derivative_weight\"]\n        bias = bias - learning_rate * gradients[\"derivative_bias\"]\n        \n        \n    parameters = {\"weight\" : weight, \"bias\" : bias}\n    \n    plt.plot(index, cost_list)\n    plt.xlabel('Number of Iterarion')\n    plt.ylabel('Cost')\n    plt.show()\n    \n    return parameters ","457c6d57":"def predict(weight, bias, x_test):\n    \n    z = sigmoid(np.dot(weight.T, x_test) + bias)\n    \n    Y_prediction = np.zeros((1, x_test.shape[1]))\n    \n    for i in range(z.shape[1]):\n        if z[0][i] <= 0.5:\n            Y_prediction[0][i] = 0\n        else:\n            Y_prediction[0][i] = 1\n    \n    return Y_prediction","9c1bdbcb":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate, num_iterarion):\n    weight, bias = initialize_weight_and_bias(x_train.shape[0])\n    \n    parameters = update(weight, bias, x_train, y_train, learning_rate, num_iterarion)\n    \n    y_prediction_test = predict(parameters['weight'], parameters['bias'], x_test)\n    \n    print('Test accuracy: {} %'.format(100 - np.mean(np.abs(y_prediction_test - y_test)) *100))\n    \n    # np.abs() --> absolute value method.","355a9270":"logistic_regression(x_train, y_train, x_test, y_test, 1, 450)","eb59e1cc":"> \nFinally, run!","044cf17b":"****\n> \n**2. Visualize**","127a07ea":"****\n> \n**4. Functions**","d444068f":"**INTRODUCTION**\n1. Data adjustment\n2. Visualize\n3. Train and Test Data\n4. Functions","96116558":"****\n> \n**3. Train and Test Data**","297f094b":"****\n> \n**1. Data Adjustment**","7f935f2e":"> \nWe normalize the data."}}