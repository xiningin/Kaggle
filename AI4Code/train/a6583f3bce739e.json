{"cell_type":{"15868ca0":"code","7ecba96d":"code","e3eb3659":"code","7b162ab0":"code","6f7cb79a":"code","0f4188d0":"code","e59ef79f":"code","39c35f40":"code","4094908e":"code","ffd8ba78":"code","e73ce54d":"code","d2f8fe0b":"code","27be5f7e":"code","0384de42":"code","17e58fd0":"code","3876e662":"code","b4b0b158":"code","0aae192c":"code","498ae007":"code","a91526de":"markdown","0ec15cbe":"markdown","4b660dd7":"markdown","adbcb47b":"markdown","5c488797":"markdown","883093b3":"markdown","2a61f3cb":"markdown","08457dd5":"markdown","ab985fc4":"markdown","3f8c7f4d":"markdown","f4fc0631":"markdown","18482c30":"markdown","31159d5b":"markdown"},"source":{"15868ca0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('darkgrid')\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7ecba96d":"df = pd.read_csv('\/kaggle\/input\/denver-crime-data\/crime.csv', index_col='INCIDENT_ID')\nprint(df.shape)\ndf.head()","e3eb3659":"df.describe()","7b162ab0":"df.isna().sum()","6f7cb79a":"df.drop(['OFFENSE_ID', 'GEO_X', 'GEO_Y', 'LAST_OCCURRENCE_DATE'], axis=1, inplace=True)","0f4188d0":"# feature engineering\ndf['FIRST_OCCURRENCE_DATE'] = pd.to_datetime(df['FIRST_OCCURRENCE_DATE'])\ndf['YEAR'] = df['FIRST_OCCURRENCE_DATE'].dt.year\ndf['MONTH'] = df['FIRST_OCCURRENCE_DATE'].dt.month\ndf['DAY'] = df['FIRST_OCCURRENCE_DATE'].dt.day\ndf['HOUR'] = df['FIRST_OCCURRENCE_DATE'].dt.hour","e59ef79f":"df['OFFENSE_CATEGORY_ID'].value_counts()[:15].sort_values(ascending=True).plot(kind='barh', \n                                                                               title='OFFENSE_CATEGORY_ID')","39c35f40":"df = df[~df['OFFENSE_CATEGORY_ID'].isin(['traffic-accident', 'all-other-crimes'])]\ndf.shape","4094908e":"# Captures 504,098 out of 508,459 rows of data (99%). The rest was outliers and\/or misclassified.\n\ndf = df[(df['GEO_LON'] < -50) & (df['GEO_LAT'] > 38)]\n\nplt.figure(figsize=(12,10))\nax = sns.scatterplot(x='GEO_LON',y='GEO_LAT', data=df)\ndf.shape","ffd8ba78":"## district separation ##\nplt.figure(figsize=(10,10))\nsns.scatterplot(x='GEO_LON', \n                y='GEO_LAT', \n                alpha=0.5,\n                hue='DISTRICT_ID',\n                palette=plt.get_cmap('jet'),\n                legend='full',\n                data=df\n               )\n\n## if data is numerical (not categorical) use this instead ##\n# df.plot(kind='scatter', \n#         x='GEO_LON', \n#         y='GEO_LAT', \n#         figsize=(10,10),\n#         alpha=0.5,\n#         c='DISTRICT_ID',\n#         cmap=plt.get_cmap('jet'),\n#         colorbar=True,\n#         sharex=False\n#        )","e73ce54d":"offense_cats = df['OFFENSE_CATEGORY_ID'].value_counts()[:10].index\n\nplt.figure(figsize=(12,10))\nsns.scatterplot(x='GEO_LON',\n                y='GEO_LAT', \n                hue='OFFENSE_CATEGORY_ID', \n                data=df[df['OFFENSE_CATEGORY_ID'].isin(offense_cats)])","d2f8fe0b":"plt.figure(figsize=(12,10))\nsns.scatterplot(x='GEO_LON',\n                y='GEO_LAT', \n                hue='DISTRICT_ID', \n                data=df[(df['OFFENSE_CATEGORY_ID'] == 'murder')])","27be5f7e":"corr = df.corr()\ncorr.style.background_gradient(cmap='coolwarm')","0384de42":"fig, axes = plt.subplots(2, 2, figsize=(20, 10))\n\ndf.groupby('HOUR').count()['OFFENSE_CODE'].plot(kind='bar', title='Crimes Per Hour', ax=axes[0,0])\ndf.groupby('DAY').count()['OFFENSE_CODE'].plot(kind='bar', title='Crimes Per Day', ax=axes[0,1])\ndf.groupby('MONTH').count()['OFFENSE_CODE'].plot(kind='bar', title='Crimes Per Month', ax=axes[1,0])\ndf.groupby('YEAR').count()['OFFENSE_CODE'].plot(kind='bar', title='Crimes Per Year', ax=axes[1,1])","17e58fd0":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nfeatures = ['GEO_LAT', 'GEO_LON', 'DISTRICT_ID']\ndf2 = df[df['OFFENSE_CATEGORY_ID'] == 'murder'][features]\n\nX = df2[features[:2]].values\ny = df2[features[-1]].values\ny = np.reshape(y, (df2.shape[0], 1))\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","3876e662":"params = {'n_neighbors':range(1,6)}\nknn = KNeighborsClassifier()\nclf = GridSearchCV(knn, params, cv=5)\nclf.fit(X_train, y_train)\n\nknn_model = clf.best_estimator_","b4b0b158":"y_pred = knn_model.predict(X_test)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","0aae192c":"df_pred = pd.DataFrame(np.column_stack((X_test, y_pred)), columns=['GEO_LAT', 'GEO_LON', 'DISTRICT_ID'])\ndf_test = pd.DataFrame(np.column_stack((X_test, y_test)), columns=['GEO_LAT', 'GEO_LON', 'DISTRICT_ID'])","498ae007":"cmap_pred = sns.cubehelix_palette(dark=.9, light=.1, as_cmap=True)\n\nfig, axes = plt.subplots(2, 2, figsize=(20, 10))\nplt.title('Murder Occurences by District (Prediction vs Test)')\n\naxes[0,1].title.set_text('Murder Occurences by District (Prediction vs Test)')\naxes[1,0].title.set_text('Murder Occurences by District (Test)')\naxes[1,1].title.set_text('Murder Occurences by District (Prediction)')\n\nsns.scatterplot(x='GEO_LON',\n                y='GEO_LAT', \n                hue='DISTRICT_ID', \n                legend='full',\n                palette='Set2',\n                alpha=0.5,\n                ax=axes[0,1],\n                data=df_pred)\n\nsns.scatterplot(x='GEO_LON',\n                y='GEO_LAT', \n                hue='DISTRICT_ID', \n                legend='full',\n                palette='Set1',\n                alpha=0.5,\n                ax=axes[0,1],\n                data=df_test)\n\nsns.scatterplot(x='GEO_LON',\n                y='GEO_LAT', \n                hue='DISTRICT_ID', \n                legend='full',\n                palette='Set2',\n                ax=axes[1,0],\n                data=df_pred)\n\nsns.scatterplot(x='GEO_LON',\n                y='GEO_LAT', \n                hue='DISTRICT_ID', \n                legend='full',\n                palette='Set1',\n                ax=axes[1,1],\n                data=df_test)","a91526de":"Looks like the shape of Denver. Fitlering left us with 279,913 rows out of 284,137 (98%).","0ec15cbe":"Nothing really showing up here. Let's do some exploration on how frequently crimes are happening based on times and dates.","4b660dd7":"## Regression\nNot really much correlation b\/w variables so prediction is hard. Let's do something simple and see if we can correctly cluster the districts where murders occured.","adbcb47b":"Out of the original 508,459 rows, we're left with 284,913 (56%).","5c488797":"## Data Manipulation","883093b3":"Traffic is obviously the biggest thing happening here. Let's exlude that and 'all-other-crimes' going forward to get a clearer picture of what's happening.","2a61f3cb":"District 4 and 5 were very separated from the others, which explains the perfect prediction accuracy for them. \nHowever, the other districts don't have a perfect boundary with each other and so there were some mistakes in classification.","08457dd5":"- Hour: Definitely see a dip in crime in the early morning. \n- Day: See a dip on the 31st, but not all months have a 31st. Strangely see a spike on the 1st. Maybe New Year's day has a disproportionately large number of crimes inflating the number.\n- Month: Summer months have more crime, and winter have the least. Not too surprising.\n- Year: Slight uptick over the years. 2019 isn't done yet, so you see the sharp drop.","ab985fc4":"## Data Input","3f8c7f4d":"Looking at the confusion matrix, correct predictions are on the diagonals. SO we can see there were only 5 incorrect prediction out of the total 90 made for the test set. \n\nLooking at the classification report, district 1 seemed to have the worst performance, while districts 4 and 5 were perfect.\n\nFinally let's plot the predicted and test values to see how we did.","f4fc0631":"Too much overlap to make any classifications. Will have to pair it down to a couple categories to do anything meaningful.","18482c30":"## Data Visualization","31159d5b":"Looks like a good candidate for k-means clustering."}}