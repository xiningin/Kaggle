{"cell_type":{"9e5c3cca":"code","71a8044b":"code","79069bcd":"code","f28a091d":"code","cf0a2360":"code","761cf537":"code","96b1d766":"code","53df9650":"code","2725ce04":"code","411e9982":"code","2d1ffbc0":"code","85210a6e":"code","21984371":"code","56390d2f":"code","4f60bed5":"code","6cd0b182":"code","0ea89572":"code","87194a91":"code","613d9f17":"code","7dc10020":"code","3cb87e09":"code","a0ddfa18":"code","c9b6d988":"markdown","322ca884":"markdown","1909867e":"markdown","f2d999fc":"markdown","3346c9ca":"markdown","d6501d53":"markdown","ac9c6224":"markdown","4a583f03":"markdown","27923987":"markdown"},"source":{"9e5c3cca":"from time import time  # To time our operations\nfrom collections import defaultdict  # For word frequency\nfrom pathlib import Path\n\nimport logging  # Setting up the loggings to monitor gensim\nlogging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)","71a8044b":"data_fn = Path('..\/input\/tokenized_lemmatized_paragraphs.txt')","79069bcd":"tokenized_paras = [para.split(' ') for para in data_fn.read_text().split('\\n')]","f28a091d":"tokenized_paras[0]","cf0a2360":"word_freq = defaultdict(int)\nfor para in tokenized_paras:\n    for i in para:\n        word_freq[i] += 1\nlen(word_freq)","761cf537":"sorted(word_freq, key=word_freq.get, reverse=True)[:10]","96b1d766":"import multiprocessing\n\nfrom gensim.models import Word2Vec","53df9650":"cores = multiprocessing.cpu_count() # Count the number of cores in a computer","2725ce04":"w2v_model = Word2Vec(min_count=20,\n                     window=5,\n                     size=150,\n                     sample=6e-5, \n                     alpha=0.03, \n                     min_alpha=0.0007, \n                     negative=20,\n                     workers=cores-1)","411e9982":"t = time()\n\nw2v_model.build_vocab(tokenized_paras, progress_per=10000)\n\nprint('Time to build vocab: {} mins'.format(round((time() - t) \/ 60, 2)))","2d1ffbc0":"t = time()\n\nw2v_model.train(tokenized_paras, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n\nprint('Time to train the model: {} mins'.format(round((time() - t) \/ 60, 2)))","85210a6e":"w2v_model.init_sims(replace=True)","21984371":"w2v_model.wv.most_similar(positive=[\"\u0f66\u0f9f\u0f7c\u0f56\u0f66\u0f0b\"])","56390d2f":"w2v_model.wv.most_similar(positive=[\"\u0f58\u0f5b\u0f51\u0f0b\u0f54\u0f0b\"])","4f60bed5":"w2v_model.wv.most_similar(positive=[\"\u0f56\u0fb3\u0f0b\u0f58\u0f0b\"])","6cd0b182":"w2v_model.wv.most_similar(positive=[\"\u0f62\u0fa9\u0f0b\u0f56\u0f0b\"])","0ea89572":"w2v_model.wv.most_similar(positive=[\"\u0f49\u0f72\u0f51\u0f0b\"])","87194a91":"w2v_model.wv.save_word2vec_format(\".\/bo_word2vec_lammatized\",\n                              \".\/vocab\",\n                               binary=False)","613d9f17":"!ls","7dc10020":"from gensim.models import KeyedVectors","3cb87e09":"wv_from_text = KeyedVectors.load_word2vec_format('bo_word2vec_lammatized', binary=False)","a0ddfa18":"wv_from_text.wv.most_similar(positive=[\"\u0f49\u0f72\u0f51\u0f0b\"])","c9b6d988":"### Dimension of word embedding\nThe optimal dimensionality of word embeddings is mostly task-dependent: a smaller dimensionality works better for more syntactic tasks such as named entity recognition (Melamud et al., 2016) [3] or part-of-speech (POS) tagging (Plank et al., 2016) [4], while a larger dimensionality is more useful for more semantic tasks such as sentiment analysis (Ruder et al., 2016) [5].\n\n- [3] -> http:\/\/arxiv.org\/abs\/1601.00893\n- [4] -> Plank, B., S\u00f8gaard, A., & Goldberg, Y. (2016). Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. \n- [5] -> http:\/\/arxiv.org\/abs\/1609.02745","322ca884":"# Dataset","1909867e":"# Training the Model","f2d999fc":"# Save the word2vec","3346c9ca":"## The parameters:\n\n* `min_count` <font color='purple'>=<\/font> <font color='green'>int<\/font> - Ignores all words with total absolute frequency lower than this - (2, 100)\n\n\n* `window` <font color='purple'>=<\/font> <font color='green'>int<\/font> - The maximum distance between the current and predicted word within a sentence. E.g. `window` words on the left and `window` words on the left of our target - (2, 10)\n\n\n* `size` <font color='purple'>=<\/font> <font color='green'>int<\/font> - Dimensionality of the feature vectors. - (50, 300)\n\n\n* `sample` <font color='purple'>=<\/font> <font color='green'>float<\/font> - The threshold for configuring which higher-frequency words are randomly downsampled. Highly influencial.  - (0, 1e-5)\n\n\n* `alpha` <font color='purple'>=<\/font> <font color='green'>float<\/font> - The initial learning rate - (0.01, 0.05)\n\n\n* `min_alpha` <font color='purple'>=<\/font> <font color='green'>float<\/font> - Learning rate will linearly drop to `min_alpha` as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00\n\n\n* `negative` <font color='purple'>=<\/font> <font color='green'>int<\/font> - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n\n\n* `workers` <font color='purple'>=<\/font> <font color='green'>int<\/font> - Use these many worker threads to train the model (=faster training with multicore machines)","d6501d53":"## Building the Vocabulary Table:\nWord2Vec requires us to build the vocabulary table (simply digesting all the words and filtering out the unique words, and doing some basic counts on them):","ac9c6224":"# Exploring the model","4a583f03":"## Training of the model:\n_Parameters of the training:_\n* `total_examples` <font color='purple'>=<\/font> <font color='green'>int<\/font> - Count of sentences;\n* `epochs` <font color='purple'>=<\/font> <font color='green'>int<\/font> - Number of iterations (epochs) over the corpus - [10, 20, 30]","27923987":"## Why I seperate the training of the model in 3 steps:\nI prefer to separate the training in 3 distinctive steps for clarity and monitoring.\n1. `Word2Vec()`: \n>In this first step, I set up the parameters of the model one-by-one. <br>I do not supply the parameter `sentences`, and therefore leave the model uninitialized, purposefully.\n2. `.build_vocab()`: \n>Here it builds the vocabulary from a sequence of sentences and thus initialized the model. <br>With the loggings, I can follow the progress and even more important, the effect of `min_count` and `sample` on the word corpus. I noticed that these two parameters, and in particular `sample`, have a great influence over the performance of a model. Displaying both allows for a more accurate and an easier management of their influence.\n3. `.train()`:\n>Finally, trains the model.<br>\nThe loggings here are mainly useful for monitoring, making sure that no threads are executed instantaneously."}}