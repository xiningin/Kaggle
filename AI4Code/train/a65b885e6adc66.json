{"cell_type":{"9b2b4312":"code","4b3b5ddc":"code","fd55c6b2":"code","15cd93db":"code","76b56a4c":"code","184fc5d2":"code","de78d7fb":"code","a779b284":"code","d2a07e9f":"code","222dcab9":"code","ea475de9":"code","b6411e4d":"code","8bea35c2":"code","4a6e3696":"code","3b1dad34":"code","aa8e01c2":"code","5cbfaf17":"code","dea416b6":"code","af45547f":"code","8463a332":"code","c7e2ed5a":"code","9c09dae3":"code","6045d19e":"code","b46de90d":"code","18b98e94":"code","92c38764":"code","7c2d1a61":"code","8be34d60":"code","e4a4bfaf":"code","4ef3ba55":"code","88b7e119":"code","c93f1aaa":"code","11c84d59":"code","c541d7a4":"code","bb474e14":"code","fe74491f":"code","73528e45":"code","4aa4a67b":"code","9a8fdd3e":"code","916bc173":"code","eb87e270":"code","ffaf13a6":"code","e8089401":"code","ef9dfcde":"code","b96317d4":"code","88e7742c":"code","23f78e8c":"code","4560fdff":"markdown","52638232":"markdown","131c85f1":"markdown","90ececf1":"markdown","1a8b43d9":"markdown","3386a9ca":"markdown","08356362":"markdown","0ecda4ff":"markdown","89b05da4":"markdown","5eb1557a":"markdown","4cee078e":"markdown","709c9fa9":"markdown","e2412c3e":"markdown","af300a9f":"markdown","1373420c":"markdown","c811e550":"markdown","ee447993":"markdown","fac72d9c":"markdown","48afdfe1":"markdown"},"source":{"9b2b4312":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4b3b5ddc":"import matplotlib.pyplot as plt","fd55c6b2":"data = pd.read_csv('\/kaggle\/input\/vehicle-dataset-from-cardekho\/car data.csv')\ndata.head()","15cd93db":"data.shape #Checking number of rows and cols","76b56a4c":"data.columns","184fc5d2":"data.describe().T # Gives you overview of the data","de78d7fb":"data.info() ","a779b284":"data.isna().sum()","d2a07e9f":"all_cols = data.columns\nnumerical_cols = data._get_numeric_data().columns.to_list()\ncategorical_cols = list(set(all_cols)- set(numerical_cols))","222dcab9":"numerical_cols, categorical_cols","ea475de9":"categorical_cols","b6411e4d":"[data[x].unique() for x in categorical_cols[1:] ] #excluding car name which is first element","8bea35c2":"# we don't need car name so i am gonna drop it right away\ndata.drop(['Car_Name'], inplace=True, axis=1)","4a6e3696":"data.head()","3b1dad34":"# car's age\ndata['Car_Age'] = 2020 - data.Year\ndata.head()","aa8e01c2":"# Dropping year column, we dont need it now\ndata.drop(['Year'], inplace=True, axis=1)","5cbfaf17":"data = pd.get_dummies(data, drop_first=True)# Get dummies will return OHE columns\ndata.head()","dea416b6":"data.shape","af45547f":"data.corr()","8463a332":"import seaborn as sns\nplt.figure(figsize=(18,8))\nsns.heatmap(data.corr(),annot=True);","c7e2ed5a":"y = data.pop('Selling_Price') #will pop the sel price collumn and drop it in y\nX = data #remaining data","9c09dae3":"X.head()","6045d19e":"y.head()","b46de90d":"# Feature importance\nfrom sklearn.ensemble import ExtraTreesRegressor\nmodel = ExtraTreesRegressor()\nmodel.fit(X, y)","18b98e94":"plt.figure(figsize=(18,8))\nsns.barplot(x=data.columns, y=model.feature_importances_);","92c38764":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","7c2d1a61":"X_train.shape, X_test.shape","8be34d60":"#Create the regressor: reg\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression()","e4a4bfaf":"#Fit the regressor to the training data\nreg.fit(X_train, y_train)","4ef3ba55":"# Predict on the test data: y_pred\ny_pred = reg.predict(X_test)","88b7e119":"# Compute and print RMSE\nfrom sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"Root Mean Squared Error: {}\".format(rmse))","c93f1aaa":"from sklearn.model_selection import cross_val_score\n\n# Define the regression_model_cv function, which takes a fitted model as one parameter. The k = 5 hyperparameter gives the number of folds.\ndef regression_model_cv(model, k=5):\n    scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=k)\n    rmse = np.sqrt(-scores)\n    print('Reg rmse:', rmse)\n    print('Reg mean:', rmse.mean ())","11c84d59":"regression_model_cv(LinearRegression())\n","c541d7a4":"#Use the regression_model_cv function on the LinearRegression() model with 3 folds and then 6 folds, as shown in the following code snippet, for 3 folds:\nregression_model_cv(LinearRegression(), k=3)","bb474e14":"# Now, test the values for 6 folds\nregression_model_cv(LinearRegression(), k=6)","fe74491f":"#We begin by setting Ridge() as a parameter for regression_model_cv\nfrom sklearn.linear_model import Ridge\nregression_model_cv(Ridge())","73528e45":"# Now, set Lasso() as the parameter for regression_model_cv:\nfrom sklearn.linear_model import Lasso\nregression_model_cv(Lasso())","4aa4a67b":"from sklearn.ensemble import RandomForestRegressor\nclf = RandomForestRegressor()","9a8fdd3e":"# Hyperparameters\nn_estimators = [100,200,300,500]\nmax_features = ['auto', 'sqrt']\nmax_depth = [5, 10, 15, 20]","916bc173":"from sklearn.model_selection import RandomizedSearchCV","eb87e270":"grid = {'n_estimators': n_estimators,\n        'max_features': max_features,\n        'max_depth': max_depth\n}","ffaf13a6":"clf_cv = RandomizedSearchCV(estimator=clf, param_distributions=grid, scoring='neg_mean_squared_error', n_iter=10, cv=5, verbose=2, random_state=42)","e8089401":"clf_cv.fit(X_train, y_train)","ef9dfcde":"preds = clf_cv.predict(X_test)\npreds","b96317d4":"from sklearn.metrics import mean_squared_error\nrmse_value = mean_squared_error(y_test, preds, squared=False)\nrmse_value","88e7742c":"sns.distplot(y_test-preds);","23f78e8c":"plt.scatter(y_test, preds);","4560fdff":"Now, for the year column, what we need is the difference between current year (2020) and the year when car was manufactured. This will give us the number of years how old car is.","52638232":"which features are important which are not","131c85f1":"# Separating numerical and categorical collumns","90ececf1":"# Checking Feature importance","1a8b43d9":"# Cross validation with linear regression","3386a9ca":"fuel_type has three unique values: Petrol \/ Diesel \/ CNG  \nSeller type : Dealer \/ Individual  \nTransmission: Manual \/ Automatic","08356362":"# Random Forest Regressor","0ecda4ff":"So with k=5 we get best results","89b05da4":"# Train-Test split","5eb1557a":"# Model Building: Linear Regression","4cee078e":"# Loading Our Data","709c9fa9":"# Regularization: Ridge and Lasso","e2412c3e":"so as you can see, present price is the most important feature then comes diesel based cars so on and so fourth","af300a9f":"# Getting our data into X and y","1373420c":"# Finding Correlation","c811e550":"## Null checking","ee447993":"No null values, so far so good..","fac72d9c":"With ridge, we get slightly better result,great!","48afdfe1":"# One Hot Encoding categorical columns"}}