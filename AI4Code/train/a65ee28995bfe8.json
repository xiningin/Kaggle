{"cell_type":{"04ef5d7b":"code","804797d5":"code","a5bd901b":"code","744221b0":"code","1b3a54be":"code","e3f43c5e":"code","4d56ef5f":"code","c72f91ce":"code","341ab13a":"code","12eab9ac":"markdown","87e0c4b7":"markdown","634e9882":"markdown"},"source":{"04ef5d7b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","804797d5":"import pandas as pd\nimport numpy as np\nimport pickle\nimport itertools\nimport gc\nimport math\nimport matplotlib.pyplot as plt\nimport dateutil.easter as easter\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nfrom datetime import datetime, date\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, HuberRegressor","a5bd901b":"original_train_df = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/train.csv')\noriginal_test_df = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/test.csv')\n\n# The dates are read as strings and must be converted\nfor df in [original_train_df, original_test_df]:\n    df['date'] = pd.to_datetime(df.date)\n    df.set_index('date', inplace=True, drop=False)\noriginal_train_df.head(2)","744221b0":"def smape_loss(y_true, y_pred):\n    \"\"\"SMAPE Loss\"\"\"\n    return np.abs(y_true - y_pred) \/ (y_true + np.abs(y_pred)) * 200\n\n#print(smape_loss(tf.constant([1, 2]), tf.constant([3, 4]))) # should print [100, 66.6667]","1b3a54be":"# Feature engineering\ndef engineer(df):\n    \"\"\"Return a new dataframe with the engineered features\"\"\"\n    new_df = pd.DataFrame({'year': df.date.dt.year, # This feature makes it possible to fit an annual growth rate\n                           'dayofyear': df.date.dt.dayofyear,\n                           'wd4': df.date.dt.weekday == 4, # Friday\n                           'wd56': df.date.dt.weekday >= 5, # Saturday and Sunday\n                           'dec29': (df.date.dt.month == 12) & (df.date.dt.day == 29), # end-of-year peak\n                           'dec30': (df.date.dt.month == 12) & (df.date.dt.day == 30),\n                          })\n\n    # Easter\n    new_df['easter_week'] = False\n    for year in range(2015, 2020):\n        easter_date = easter.easter(year)\n        easter_diff = df.date - np.datetime64(easter_date)\n        new_df['easter_week'] = new_df['easter_week'] | (easter_diff > np.timedelta64(0, \"D\")) & (easter_diff < np.timedelta64(8, \"D\"))\n    \n    # Growth is country-specific\n    #for country in ['Finland', 'Norway', 'Sweden']:\n    #    new_df[f\"{country}_year\"] = (df.country == country) * df.date.dt.year\n        \n    # One-hot encoding (no need to encode the last categories)\n    for country in ['Finland', 'Norway']:\n        new_df[country] = df.country == country\n    new_df['KaggleRama'] = df.store == 'KaggleRama'\n    for product in ['Kaggle Mug', 'Kaggle Sticker']:\n        new_df[product] = df['product'] == product\n        \n    # Seasonal variations (Fourier series)\n    # The three products have different seasonal patterns\n    dayofyear = df.date.dt.dayofyear\n    for k in range(1, 100): # 100\n        new_df[f'sin{k}'] = np.sin(dayofyear \/ 365 * 2 * math.pi * k)\n        new_df[f'cos{k}'] = np.cos(dayofyear \/ 365 * 2 * math.pi * k)\n        new_df[f'mug_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Mug']\n        new_df[f'mug_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Mug']\n        new_df[f'sticker_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Sticker']\n        new_df[f'sticker_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Sticker']\n\n    return new_df\n\ntrain_df = engineer(original_train_df)\ntrain_df['date'] = original_train_df.date\ntrain_df['num_sold'] = original_train_df.num_sold.astype(np.float32)\ntest_df = engineer(original_test_df)\ntest_df.year = 2018 # no growth patch, see https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022\/discussion\/298318\n\nfeatures = test_df.columns\n\nfor df in [train_df, test_df]:\n    df[features] = df[features].astype(np.float32)\nprint(list(features))","e3f43c5e":"#%%time\nRUNS = 1 # should be 1. increase the number of runs only if you want see how the result depends on the random seed\nOUTLIERS = True\nTRAIN_VAL_CUT = datetime(2018, 1, 1)\nLOSS_CORRECTION = 1 # correction factor between Huber loss and SMAPE: 1.035 ( for linear regression with MSE use 1.038)\n\ndef fit_model(X_tr, X_va=None):\n    \"\"\"Scale the data, fit a model, plot the training history and validate the model\"\"\"\n    start_time = datetime.now()\n\n    # Preprocess the data\n    X_tr_f = X_tr[features]\n    preproc = StandardScaler()\n    X_tr_f = preproc.fit_transform(X_tr_f)\n    y_tr = X_tr.num_sold.values.reshape(-1, 1)\n    \n    # Train the model\n    #model = LinearRegression() # 5.80558\n    model = HuberRegressor(epsilon=1.20) # 5.80143 (epsilon=1.20) ******************\n    model.fit(X_tr_f, np.log(y_tr))\n\n    if X_va is not None:\n        # Preprocess the validation data\n        X_va_f = X_va[features]\n        X_va_f = preproc.transform(X_va_f)\n        y_va = X_va.num_sold.values.reshape(-1, 1)\n\n        # Inference for validation\n        y_va_pred = np.exp(model.predict(X_va_f)).reshape(-1, 1)\n        \n        # Evaluation: Execution time and SMAPE\n        smape_before_correction = np.mean(smape_loss(y_va, y_va_pred))\n        y_va_pred *= LOSS_CORRECTION\n        smape = np.mean(smape_loss(y_va, y_va_pred))\n        print(f\"Fold {run}.{fold} | {str(datetime.now() - start_time)[-12:-7]}\"\n              f\" | SMAPE: {smape:.5f}   (before correction: {smape_before_correction:.5f})\")\n        \n        # Plot y_true vs. y_pred\n        plt.figure(figsize=(10, 10))\n        plt.scatter(y_va, y_va_pred, s=1, color='r')\n        #plt.scatter(np.log(y_va), np.log(y_va_pred), s=1, color='g')\n        plt.plot([plt.xlim()[0], plt.xlim()[1]], [plt.xlim()[0], plt.xlim()[1]], '--', color='k')\n        plt.gca().set_aspect('equal')\n        plt.xlabel('y_true')\n        plt.ylabel('y_pred')\n        plt.title('OOF Predictions')\n        plt.show()\n\n        # Show the outliers among the predictions\n        if OUTLIERS:\n            print(\"Outlier predictions - work on these to improve your score!\")\n            outliers = original_train_df.iloc[val_idx].copy()\n            outliers['smape'] = smape_loss(y_va, y_va_pred)\n            with pd.option_context(\"display.max_rows\", 1000, \"display.width\", 160):\n                print(outliers.sort_values('smape', ascending=False).head(120).sort_values('row_id'))\n        \n    return preproc, model\n\n# Make the results reproducible\nnp.random.seed(202100)\n\ntotal_start_time = datetime.now()\nfor run in range(RUNS):\n    fold = 0\n    train_idx = np.arange(len(train_df))[train_df.date < TRAIN_VAL_CUT]\n    val_idx = np.arange(len(train_df))[train_df.date > TRAIN_VAL_CUT]\n    print(f\"Fold {run}.{fold}\")\n    X_tr = train_df.iloc[train_idx]\n    X_va = train_df.iloc[val_idx]\n    \n    preproc, model = fit_model(X_tr, X_va)","4d56ef5f":"def plot_demo(country='Norway', store='KaggleMart', product='Kaggle Hat'):\n    demo_df = pd.DataFrame({'row_id': 0,\n                            'date': pd.date_range('2015-01-01', '2019-12-31', freq='D'),\n                            'country': country,\n                            'store': store,\n                            'product': product})\n    demo_df.set_index('date', inplace=True, drop=False)\n    demo_df = engineer(demo_df)\n    demo_df['num_sold'] = np.exp(model.predict(preproc.transform(demo_df[features]))) * LOSS_CORRECTION\n    plt.figure(figsize=(18, 6))\n    plt.plot(np.arange(len(demo_df)), demo_df.num_sold, label='prediction')\n    train_subset = train_df[(original_train_df.country == country) & (original_train_df.store == store) & (original_train_df['product'] == product)]\n    plt.plot(np.arange(len(train_subset)), train_subset.num_sold, label='true', alpha=0.5)\n    plt.legend()\n    plt.show()\n\nplot_demo()","c72f91ce":"# Fit the model on the complete training data\ntrain_idx = np.arange(len(train_df))\nX_tr = train_df.iloc[train_idx]\npreproc, model = fit_model(X_tr, None)\n\nplot_demo()\n\n# Inference for test\ntest_pred_list = []\ntest_pred_list.append(np.exp(model.predict(preproc.transform(test_df[features]))) * LOSS_CORRECTION)\n\nif len(test_pred_list) > 0:\n    # Create the submission file\n    sub = original_test_df[['row_id']].copy()\n    sub['num_sold'] = sum(test_pred_list) \/ len(test_pred_list)\n    sub.to_csv('submission.csv', index=False)\n\n    # Plot the distribution of the test predictions\n    plt.figure(figsize=(16,3))\n    plt.hist(train_df['num_sold'], bins=np.linspace(0, 3000, 201), density=True, label='Training')\n    plt.hist(sub['num_sold'], bins=np.linspace(0, 3000, 201), density=True, rwidth=0.5, label='Test predictions')\n    plt.xlabel('num_sold')\n    plt.ylabel('Frequency')\n    plt.legend()\n    plt.show()","341ab13a":"sub","12eab9ac":"#### What are you trying to do in this notebook?\nFor this challenge, we will be predicting a full year worth of sales for three items at two stores located in three different countries. This dataset is completely fictional, but contains many effects you see in real-world data, e.g., weekend and holiday effect, seasonality, etc. The dataset is small enough to allow us to try numerous different modeling approaches.\n\n#### Why are you trying it?\nThere are two (fictitious) independent store chains selling Kaggle merchandise that want to become the official outlet for all things Kaggle. we want to figure out which of the store chains(KaggleMart or KaggleRama) would have the best sales going forward.\n\n**Files**\n- train.csv - the training set, which includes the sales data for each date-country-store-item combination.\n- test.csv - the test set; your task is to predict the corresponding item sales for each date-country-store-item combination. Note the Public leaderboard is scored on the first quarter of the test year, and the Private on the remaining.\n- sample_submission.csv - a sample submission file in the correct format.","87e0c4b7":"**I Hope you find this notebook useful , Good Luck!**","634e9882":"#### Did it work?\nThe notebook goes together with the EDA notebook, which visualizes the various seasonal effects and the differences in growth rate.\nScikit-learn doesn't offer SMAPE as a loss function. As a workaround, I'm training for Huber loss with a transformed target, apply a correction factor, and we'll see how far we'll get.\n\nThe transformed target for the regression is the log of the sales numbers.\n\n#### What did you not understand about this process?\nWell, everything provides in the competition data page. I've no problem while working on it. If you guys don't understand the thing that I'll do in this notebook then please comment on this notebook.\n\n#### What else do you think you can try as part of this approach?\nLook at a notebook which presents feature engineering (based on the insights of this EDA) and a linear model which makes use of the features."}}