{"cell_type":{"890afbc6":"code","a066f42d":"code","1d4e58a0":"code","5d482086":"code","23fc6c3f":"code","a2759488":"code","b15482b5":"code","e87fad0f":"code","cfe70cee":"code","6320927d":"code","b941ccbb":"code","85a10870":"code","788a9457":"code","1562e936":"code","be159fd3":"code","d029e002":"code","132d0c70":"code","2a347379":"code","971f5892":"code","520643f0":"markdown","55e8f9ea":"markdown","f031ff6e":"markdown","bef686ae":"markdown","f64c329d":"markdown","e911a115":"markdown","05b945d8":"markdown","4d0e47b6":"markdown","bd98d44e":"markdown","f1c8192c":"markdown"},"source":{"890afbc6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.metrics import accuracy_score\n\nfrom xgboost import XGBClassifier, plot_importance\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a066f42d":"train = pd.read_csv(\"..\/input\/learn-together\/train.csv\", index_col='Id')\ntest = pd.read_csv(\"..\/input\/learn-together\/test.csv\", index_col='Id')\n\ny = train['Cover_Type'] # this is the target\nX = train.drop('Cover_Type', axis = 1)\nX_test = test.copy()\n\nprint('Train set shape : ', X.shape)\nprint('Test set shape : ', X_test.shape)","1d4e58a0":"X.head()","5d482086":"X_test.head()","23fc6c3f":"print('Missing Label? ', y.isnull().any())\nprint('Missing train data? ', X.isnull().any().any())\nprint('Missing test data? ', X_test.isnull().any().any())","a2759488":"print (X.dtypes.value_counts())\nprint (X_test.dtypes.value_counts())","b15482b5":"X.describe()","e87fad0f":"X.nunique()","cfe70cee":"X.drop(['Soil_Type15', 'Soil_Type7'], axis=1, inplace = True)\nX_test.drop(['Soil_Type15', 'Soil_Type7'], axis=1, inplace = True)","6320927d":"X_test.describe()","b941ccbb":"columns = X.columns","85a10870":"X_test_index = X_test.index # the scaler drops table index\/columns and outputs simple arrays..\nscaler = RobustScaler()\nX = scaler.fit_transform(X)\nX_test = scaler.transform(X_test)","788a9457":"X_train,  X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, random_state=1)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_val.shape)\nprint(y_val.shape)","1562e936":"xgb= XGBClassifier( n_estimators=1000,  #todo : search for good parameters\n                    learning_rate= 0.5,  #todo : search for good parameters\n                    objective= 'binary:logistic', #this outputs probability,not one\/zero. should we use binary:hinge? is it better for the learning phase?\n                    random_state= 1,\n                    n_jobs=-1)","be159fd3":"xgb.fit(X=X_train, y=y_train,\n        eval_metric='merror', # merror: Multiclass classification error rate. It is calculated as #(wrong cases)\/#(all cases). \n        eval_set=[(X_val,y_val)],\n        early_stopping_rounds = 100,\n        verbose = False\n       )\nprint(xgb.best_score)","d029e002":"plt.figure(figsize=(25,10))\nsns.barplot(y=xgb.feature_importances_, x=columns)","132d0c70":"xgb.fit(X,y)\npreds_test = xgb.predict(X_test)\npreds_test.shape","2a347379":"preds_test","971f5892":"# Save test predictions to file\noutput = pd.DataFrame({'ID': X_test_index,\n                       'TARGET': preds_test})\noutput.to_csv('submission.csv', index=False)\noutput.head()","520643f0":"No missing data, everything in numeric. \nSoil_type and Wilderness_area are categorial data already put as one hot encoded.","55e8f9ea":"TODO:\n- Check and remove outliers\n-- Should we remove columns with too few data (e.g. in soil types)\n- Scale to normal-ish distributions\n- Check correlations and linearities","f031ff6e":"Check for data types and missing values","bef686ae":"#preparing for submission\n- fit model on the whole training set \n- predict test set\n- format output","f64c329d":"#Model setup\nTry classic XGB","e911a115":"#Import datasets","05b945d8":"Many values have large numbers, std and means. Will need for scaling (ideally, we want normal distributions with (0,1))\nHowever, distributions are not very similar. \n- Should we scale based on all data? test data only? train only?  (my intuition:on train only, need to check)\n- Do we need to scale binary data ?","4d0e47b6":"Note : large difference between train and test size. Will need to check input distributions.","bd98d44e":"Soil_Type15 and Soil_Type7 have only one value. meaning these types of soils didnt appear in the training set.\n-> drop these column.\n","f1c8192c":"#checking some scores. Should to some real search\n- without scaler\nXGBClassifier( n_estimators=1000, learning_rate=1, objective= 'binary:logistic', random_state=1,  n_jobs=-1) # Validation split error : 0.165344  (early stop 20)\nXGBClassifier( n_estimators=1000, learning_rate=0.5, objective= 'binary:logistic', random_state=1,  n_jobs=-1) # Validation split error : 0.166336  (early stop 20)\nXGBClassifier( n_estimators=1000, learning_rate=0.1, objective= 'binary:logistic', random_state=1,  n_jobs=-1) # Validation split error : 0.215278  (early stop 20)\n- with scaler\nXGBClassifier( n_estimators=1000, learning_rate=1, objective= 'binary:logistic', random_state=1,  n_jobs=-1) # Validation split error : 0.165675  (early stop 20)\nXGBClassifier( n_estimators=1000, learning_rate=1, objective= 'binary:logistic', random_state=1,  n_jobs=-1) # Validation split error : 0.153439  (early stop 100)\nXGBClassifier( n_estimators=1000, learning_rate=0.5, objective= 'binary:logistic', random_state=1,  n_jobs=-1) # Validation split error : 0.153108 (early stop 100)\nXGBClassifier( n_estimators=1000, learning_rate=0.1, objective= 'binary:logistic', random_state=1,  n_jobs=-1) # Validation split error : 0.161045 (early stop 100)\nXGBClassifier( n_estimators=1000, learning_rate=0.1, objective= 'binary:logistic', random_state=1,  n_jobs=-1) # Validation split error : 0.21x  (early stop 20)\n\n"}}