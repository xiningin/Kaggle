{"cell_type":{"13ef620b":"code","75c32cae":"code","0d0a06b1":"code","26f49cc0":"code","0b584358":"code","acc5e8e1":"code","b76461e4":"code","242eda76":"code","0372110f":"code","1fa75a2d":"code","75bafbb0":"code","63374db6":"code","51f026d5":"code","d0d1454a":"code","7c0dc5f6":"code","b6f8d849":"code","3403c58a":"code","eaf2edd3":"code","a84bcb9a":"code","fbbad780":"code","5d046e5d":"code","bac3c833":"code","78296691":"code","0dc71d2a":"code","fa672a77":"code","e998a5cd":"code","734b0c63":"code","6a0e4072":"code","80f106a8":"markdown","f36f821b":"markdown","cb7ff63d":"markdown","09342eb9":"markdown","87a5d98d":"markdown","7dcd2bed":"markdown"},"source":{"13ef620b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","75c32cae":"# Gi\u1ea3i n\u00e9n file d\u1eef li\u1ec7u word_embed\u0111ing c\u00f3 s\u1eb5n:\n\nimport zipfile\nzip_ref = zipfile.ZipFile('..\/input\/quora-insincere-questions-classification\/embeddings.zip', 'r')\nzip_ref.printdir()\nzip_ref.extract('wiki-news-300d-1M\/wiki-news-300d-1M.vec')","0d0a06b1":"# \u0110\u1eccc d\u1eef li\u1ec7u v\u00e0 chia th\u00e0nh 2 nh\u00f3m sincere v\u00e0 insicere t\u1eeb t\u1eadp d\u1eef li\u1ec7u ban \u0111\u1ea7u:\n\ndf = pd.read_csv('..\/input\/quora-insincere-questions-classification\/train.csv')\ninsincere_questions = [df['question_text'][idx] for idx in range(len(df)) if df['target'].loc[idx] == 1]\nsincere_questions = [df['question_text'][idx] for idx in range(len(df)) if df['target'].loc[idx] == 0]","26f49cc0":"import nltk\nfrom nltk.corpus import stopwords\nimport string","0b584358":"stop_words = set(stopwords.words('english'))\npuncs = set(string.punctuation)","acc5e8e1":"# Ti\u1ebfn h\u00e0nh chuy\u1ec3n c\u00e1c c\u00e2u h\u1ec9 th\u00e0nh 1 list g\u1ed3m nh\u1eefng t\u1eeb c\u00f2n l\u1ea1i sau khi x\u00f3a c\u00e1c stop_words v\u00e0 c\u00e1c d\u1ea5u c\u00e2u:\n\ndef clean(text_list):\n    for idx, text in enumerate(text_list):\n        text = text.lower().split()\n        word_list = [char for char in text if char not in puncs and char not in stop_words]\n        text_list[idx] = word_list\n    \n    return text_list","b76461e4":"insincere_questions = clean(insincere_questions)\nsincere_questions = clean(sincere_questions)","242eda76":"# \u0110\u1ecdc d\u1eef li\u1ec7u embedding word t\u1eeb file wiki-news-300d-1M.vec\nf = open('.\/wiki-news-300d-1M\/wiki-news-300d-1M.vec')\nlines = [line for line in f.readlines()]\nlines = lines[2:]\ntemp = lines[:80000]","0372110f":"# L\u1ea5y 1 s\u1ed1 nh\u1eefng c\u00e2u h\u1ecfi trong t\u1eadp hu\u1ea5n luy\u1ec7n \u0111\u1ec3 th\u1ef1c hi\u1ec7n cho vi\u1ec7c hu\u1ea5n luy\u1ec7n\n\nsample_insincere_questions = insincere_questions[:10000]\nsample_sincere_questions = sincere_questions[:10000]\ntest_insincere_questions = insincere_questions[10000:11000]\ntest_sincere_questions = sincere_questions[10000:11000]","1fa75a2d":"#T\u1ea1o 1 dictionary embedding_list c\u00f3 d\u1ea1ng 'word': array(300) t\u1eeb file embedding trong \u0111\u00f3 array(300) l\u00e0 vector t\u01b0\u1ee3ng tr\u01b0ng cho c\u00e1c words t\u01b0\u01a1ng \u1ee9ng\n\nembedding_list = {}\n\nfor val in temp:\n    val = val.replace('\\n', '').split()\n    key = val.pop(0)\n    key = key.lower()\n    if key in puncs or key in stop_words:\n        continue\n        \n    embedding_list[key] = np.array([float(x) for x in val])","75bafbb0":"# T\u00ecm \u0111\u1ed9 d\u00e0i l\u1edbn nh\u1ea5t c\u00f2n l\u1ea1i sau khi \u0111\u00e3 th\u1ef1c hi\u1ec7n clean d\u1eef li\u1ec7u \nmax_len = 0\nfor idx in range(len(sincere_questions)):\n    if idx < len(insincere_questions):\n        if max_len < max(len(sincere_questions[idx]), len(insincere_questions[idx])):\n            max_len = max(len(sincere_questions[idx]), len(insincere_questions[idx]))\n    else:\n        if max_len < len(sincere_questions[idx]):\n            max_len = len(sincere_questions[idx])\n\nmax_len","63374db6":"# T\u1ea1o vector 0\ndef generate_mt(key):\n    new_arr = np.zeros((key, 300))\n    return new_arr","51f026d5":"# Chuy\u1ec3n c\u00e1c t\u1eeb trong 1 list sang d\u1ea1ng vector \u0111\u1ed3ng th\u1eddi th\u1ef1c hi\u1ec7n \u0111\u1ed3ng b\u1ecd v\u1ec1 chi\u1ec1u d\u00e0i c\u1ee7a c\u00e1c list t\u1eeb trong c\u00e2u h\u1ecfi\n# 1 list t\u1eeb trong d\u1ea1ng vector s\u1ebd c\u00f3 dang: [max_len, 300]\n#     * max_len: l\u00e0 s\u1ed1 t\u1eebu c\u00f3 trong list (n\u1ebfu trong list ban \u0111\u1ea7u kh\u00f4ng c\u00f3 \u0111\u1ee7 s\u1ed1 t\u1eeb, ta thay b\u1eb1ng c\u00e1c vecto 0)\n#     * 300: l\u00e0 s\u1ed1 chi\u1ec1u vector t\u01b0\u1ee3ng tr\u01b0ng cho t\u1eeb \u0111\u00f3 trong t\u1eadp word_embedding \u1edf tr\u00ean\n\n\ndef convert_to_vt(list_questions, max_len):\n    for idx in range(len(list_questions)):\n        if len(list_questions[idx]) == 0:\n            continue\n        for i, item in enumerate(list_questions[idx]):\n            if item not in embedding_list:\n                list_questions[idx][i] = np.zeros(300)\n            else:\n                list_questions[idx][i] = embedding_list[list_questions[idx][i]]\n        if len(list_questions[idx]) < max_len:\n            temp_arr = generate_mt(max_len - len(list_questions[idx]))\n            list_questions[idx] = np.concatenate((list_questions[idx], temp_arr), axis=0)\n            \n    return list_questions","d0d1454a":"# Chuy\u1ec3n sang vector \u0111\u1ed1i v\u1edbi t\u1eadp hu\u1ea5n luy\u1ec7n\n\nsample_insincere_questions = convert_to_vt(sample_insincere_questions, max_len)\nsample_sincere_questions = convert_to_vt(sample_sincere_questions, max_len)\nsample_insincere_questions = [item for item in sample_insincere_questions if type(item) is not list]\nsample_sincere_questions = [item for item in sample_sincere_questions if type(item) is not list]","7c0dc5f6":"# Chuy\u1ec3n sang vector \u0111\u1ed1i v\u1edbi t\u1eadp test\n\ntest_insincere_questions = convert_to_vt(test_insincere_questions, max_len)\ntest_sincere_questions = convert_to_vt(test_sincere_questions, max_len)\ntest_insincere_questions = [item for item in test_insincere_questions if type(item) is not list]\ntest_sincere_questions = [item for item in test_sincere_questions if type(item) is not list]","b6f8d849":"a = np.array(sample_insincere_questions)\nb = np.array(sample_sincere_questions)","3403c58a":"# Sau khi chuy\u1ec3n d\u1eef li\u1ec7u sang d\u1ea1ng vector, ta n\u1ed1i 2 t\u1eadp sincere v\u00e0 insincere \u0111\u1ec3 t\u1ea1o th\u00e0nh t\u1eadp hu\u1ea5n luy\u1ec7n cho m\u00f4 h\u00ecnh\ntrain_set = np.concatenate((a,b), axis=0)","eaf2edd3":"train_set = train_set.reshape(train_set.shape[0], -1)","a84bcb9a":"train_set.shape","fbbad780":"# T\u1ea1o t\u1eadp nh\u00e3n d\u00e1n d\u1ef1a tr\u00ean s\u1ed1 l\u01b0\u1ee3ng c\u00e1c c\u00e2u h\u1ecfi c\u1ee7a t\u1eadp hu\u1ea5n luy\u1ec7n\n# T\u1ec9 l\u1ec7 c\u00e1c c\u00e2u sincere : insincere l\u00e0 50:50\n\nlabel1 = np.ones((10000,))\nlabel2 = np.zeros((9999,))\nlabel = np.concatenate((label2, label1), axis=0)","5d046e5d":"# V\u1edbi t\u1eadp test, ta l\u00e0m t\u01b0\u01a1ng t\u1ef1 nh\u01b0 c\u00e1c b\u01b0\u1edbc \u0111\u00e3 th\u1ef1c hi\u1ec7n trong t\u1eadp hu\u1ea5n luy\u1ec7n\n\na = np.array(test_insincere_questions)\nb = np.array(test_sincere_questions)\ntest_set = np.concatenate((a, b), axis=0)\ntest_set = test_set.reshape(test_set.shape[0], -1)\n\nlabel1 = np.ones((1000))\nlabel2 = np.zeros((1000,))\ntest_label = np.concatenate((label2, label1), axis=0)","bac3c833":"import sklearn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import datasets\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt","78296691":"# Th\u1ef1c hi\u1ec7n khai b\u00e1o m\u00f4 h\u00ecnh v\u00e0 ti\u1ebfn h\u00e0nh hu\u1ea5n luy\u1ec7n (s\u1ed1 l\u1ea7n train = 300)\n# Sau \u0111\u00f3 th\u1ef1c hi\u1ec7n \u0111\u00e1nh gi\u00e1 \u0111\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a t\u1eadp hu\u1ea5n luy\u1ec7n\n\nclf = LogisticRegression(random_state=0, max_iter=300).fit(train_set, label)\n# clf.fit(train_set, label)\nclf.score(train_set, label)","0dc71d2a":"# Ti\u1ebfn h\u00e0nh d\u1ef1 \u0111o\u00e1n t\u1eadp test b\u1eb1ng m\u00f4 h\u00ecnh \u0111\u00e3 hu\u1ea5n luy\u1ec7n\npred = clf.predict(test_set)","fa672a77":"# T\u00ednh \u0111\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a vi\u1ec7c d\u1ef1 \u0111o\u00e1n tr\u00ean t\u1eadp test \ncr = 0\nfor i in range(len(test_set)):\n    if pred[i] == test_label[i]:\n        cr += 1\n\nprint(cr \/ len(test_set))","e998a5cd":"from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n\n# \u0110i\u1ec3m F1 c\u1ee7a m\u00f4 h\u00ecnh \nf1 = f1_score(test_label, pred)\nf1","734b0c63":"accuracy_score(test_label, pred)","6a0e4072":"# B\u00e1o c\u00e1o k\u1ebft qu\u1ea3 th\u1ef1c hi\u1ec7n:\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(test_label, pred))","80f106a8":"- \u0110\u1ecdc t\u1eadp embedding word t\u1eeb file d\u1eef li\u1ec7u\n(Embedding words l\u00e0 file ch\u1ee9a nh\u1eefng vector g\u1ed3m 300 chi\u1ec1u m\u00e3 h\u00f3a cho 1 t\u1eeb ti\u1ebfng anh vd: 'the': '-0.12 0.1678 0.3467 -0.899 ....')\n- Sau khi chuy\u1ec3n nh\u1eefng t\u1eeb c\u00f2n l\u1ea1i trong c\u00e2u h\u1ecfi th\u00e0nh 1 danh s\u00e1ch (\u0111\u00e3 b\u1ecf nh\u1eefng stop word v\u00e0 c\u00e1c d\u1ea5u c\u00e2u), ta chuy\u1ec3n nh\u1eefng t\u1eeb \u0111\u00f3 th\u00e0nh c\u00e1c vector 300 chi\u1ec1u d\u1ef1a v\u00e0o t\u1eadp embedding word","f36f821b":"* S\u1eed d\u1ee5ng m\u00f4 h\u00ecnh LogisticRegression \u0111\u1ec3 ph\u00e2n l\u1edbp cho b\u1ed9 d\u1eef li\u1ec7u c\u00f3 2 nh\u00e3n (sincere:1 v\u00e0 insincere:0)","cb7ff63d":"- \u0110\u1ecdc d\u1eef li\u1ec7u t\u1eebu t\u1eadp csv sau \u0111\u00f3 chia th\u00e0nh 2 list\n+ 1 list ch\u1ee9a nh\u1eefng c\u00e2u insincere\n+ 1 list ch\u1ee9a nh\u1eefng c\u00e2u sincere\n\n(Trong d\u1eef li\u1ec7u ban \u0111\u1ea7u c\u00f3 t\u1ed5ng s\u1ed1 h\u01a1n 2 tri\u1ec7u c\u00e2u, trong \u0111\u00f3 92% s\u1ed1 c\u00e2u h\u1ecfi \u0111\u01b0\u1ee3c g\u00e1n nh\u00e3n sincere v\u00e0 s\u1ed1 c\u00f2n l\u1ea1i \u0111\u01b0\u1ee3c g\u00e1n nh\u00e3n insicere. Do s\u1ef1 gi\u1edbi h\u1ea1n c\u1ee7a b\u1ed9 nh\u1edb, ta s\u1ebd l\u1ea5y 1 s\u1ed1 c\u00e2u v\u1edbi m\u1ed7i lo\u1ea1i nh\u00e3n d\u00e1n \u0111\u1ec3 th\u1ef1c hi\u1ec7n vi\u1ec7c hu\u1ea5n luy\u1ec7n v\u00e0 test)","09342eb9":"* Vi\u00eac clean d\u1eef li\u1ec7u (x\u00f3a 1 s\u1ed1 t\u1eeb) s\u1ebd d\u1eabn \u0111\u1ebfn vi\u1ec7c c\u00e1c list t\u1eeb c\u00f2n l\u1ea1i c\u00f3 \u0111\u1ed9 d\u00e0i kh\u00f4ng gi\u1ed1ng nhau\n=> Kh\u1eafc ph\u1ee5c b\u1eb1ng c\u00e1ch th\u00eam c\u00e1c vector 0 ([0, 0, 0, 0, ...., 0]) v\u00e0o nh\u1eefng t\u1eeb c\u00f2n thi\u1ebfu, t\u1eeb \u0111\u00f3 c\u00e1c list trong t\u1eadp hu\u1ea5n luy\u1ec7n s\u1ebd c\u00f3 c\u00f9ng c\u00f9ng \u0111\u1ed9 d\u00e0i v\u1edbi nhau m\u00e0 kh\u00f4ng l\u00e0m thay \u0111\u1ed5i \u00fd ngh\u0129a c\u1ee7a c\u00e1c c\u00e2u\n","87a5d98d":"T\u1ea1o t\u1eadp ch\u1ee9a c\u00e1c stop words v\u00e0 c\u00e1c d\u1ea5u c\u00e2u (s\u1eed d\u1ee5ng trong vi\u1ec7c clean d\u1eef li\u1ec7u)","7dcd2bed":"CLean d\u1eef li\u1ec7u:\n- X\u00f3a c\u00e1c stop words v\u00e0 c\u00e1c d\u1ea5u c\u00e2u trong 1 c\u00e2u\n- Chuy\u1ec3n c\u00e1c t\u1eeb sang k\u00ed t\u1ef1 th\u01b0\u1eddng\n\n- Chuy\u1ec3n c\u00e1c t\u1eeb c\u00f2n l\u1ea1i th\u00e0nh 1 list c\u00e1c t\u1eeb "}}