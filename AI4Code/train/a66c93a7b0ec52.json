{"cell_type":{"0ca2dc71":"code","8cff2682":"code","0180bd68":"code","0560af45":"code","874df253":"code","3a121652":"code","c1675036":"code","905fe86a":"code","caed0bc9":"code","45664dc7":"code","1aef9452":"code","9ef5e16f":"code","9be17a22":"code","e5ce7321":"code","52a7f4eb":"code","24479a66":"code","3c72a574":"code","ff362789":"code","04c1aebb":"code","6e7958b7":"code","05d66158":"code","fb1973ab":"code","4d5ec357":"code","65913bda":"code","f160c149":"code","44d8ef1d":"code","35e34567":"code","1b4dc582":"code","4c1882ed":"code","dc09a671":"code","fe7fc971":"code","23f0c1bd":"code","edcdeeec":"code","c91e6378":"code","51ca0cbc":"code","ad906d9c":"code","1eddae04":"code","10496776":"code","2045ef7c":"code","fe8d7657":"code","c28d26d5":"code","c1c4b701":"code","c99e0022":"code","4ddab516":"code","74a39142":"code","8203aea9":"code","0d530af9":"code","38711aa5":"code","b8c01efc":"code","dfe10ae0":"code","2eca3396":"markdown","6229becd":"markdown","9be4877d":"markdown","ee354e50":"markdown","690bd3cc":"markdown","61d9442b":"markdown","feab622c":"markdown","dd5fcc5e":"markdown","10a8c349":"markdown","72caefda":"markdown","2a3954b4":"markdown","25014b0c":"markdown","f5cdaad5":"markdown"},"source":{"0ca2dc71":"import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom  datetime import datetime, timedelta\nimport gc\nimport numpy as np\n#plt.style.use('ggplot')","8cff2682":"%matplotlib inline\nimport sys\nimport re\n\nplt.style.use('seaborn-darkgrid')\nimport seaborn as sns\nimport patsy as pt\nimport pymc3 as pm\n\nplt.rcParams['figure.figsize'] = 14, 6\nnp.random.seed(0)\nprint('Running on PyMC3 v{}'.format(pm.__version__))","0180bd68":"#os.listdir('..\/m5-forecasting-uncertainty\/')","0560af45":"submission = pd.read_csv('..\/input\/m5-forecasting-uncertainty\/sample_submission.csv')","874df253":"#submission.shape","3a121652":"#submission.head()","c1675036":"sale = pd.read_csv('..\/input\/m5-forecasting-uncertainty\/sales_train_validation.csv')","905fe86a":"#sale.head()","caed0bc9":"sale.shape","45664dc7":"total_historical = sale.iloc[:,6:].sum()","1aef9452":"total_historical.shape","9ef5e16f":"calendar = pd.read_csv('..\/input\/m5-forecasting-uncertainty\/calendar.csv')","9be17a22":"calendar['event_true_1'] = calendar.event_name_1.notna()\ncalendar['event_true_2'] = calendar.event_name_2.notna()\n\ncalendar['event_true_all'] = calendar.event_true_1 + calendar.event_true_2\ncalendar['event_true_all'] = calendar.event_true_all.apply(lambda x: x>0)\ncalendar['event_true_all'] = calendar.event_true_all.astype('int')\ncalendar['date'] = pd.to_datetime(calendar.date)","e5ce7321":"#calendar.dtypes","52a7f4eb":"#calendar.columns","24479a66":"calendar['d_parse'] = calendar.d.apply(lambda x: int(x.split('_')[1]))","3c72a574":"#calendar.head()","ff362789":"calendar_feature = calendar[['wm_yr_wk', 'wday', 'month', 'year', \\\n       'snap_CA', 'snap_TX', 'snap_WI', \\\n       'event_true_all', 'd_parse']]","04c1aebb":"calendar_feature.dtypes","6e7958b7":"# specify formula\nfml = 'total ~ wday + month + year + snap_CA + snap_TX + snap_WI + event_true_all + d_parse'","05d66158":"from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer\nscaler = StandardScaler()\n#minmax = MinMaxScaler()","fb1973ab":"calendar_feature = calendar[['wm_yr_wk', 'wday', 'month', 'year', \\\n       'snap_CA', 'snap_TX', 'snap_WI', \\\n       'event_true_all', 'd_parse']]\n\nscaled_feature = pd.DataFrame(scaler.fit_transform(calendar_feature))\nscaled_feature.columns = calendar_feature.columns\nscaled_feature.min()","4d5ec357":"np.where(total_historical < 10000)[0]","65913bda":"total_historical.iloc[[ 330,  696, 1061, 1426, 1791]]=np.quantile(total_historical, 0.025)","f160c149":"np.min(total_historical)","44d8ef1d":"#minmax_feature.iloc[:1913,9]","35e34567":"# create data frame\ndf = scaled_feature.iloc[:1913,:]\ndf.loc[:,'total'] = total_historical.values\ndf.loc[:, 'd_parse'] = calendar_feature.iloc[:1913, 8] - np.min(calendar_feature.d_parse) + 1\ndf.head()","1b4dc582":"(mx_en, mx_ex) = pt.dmatrices(fml, df, return_type='dataframe', NA_action='raise')\npd.concat((mx_ex.head(3),mx_ex.tail(3)))\n","4c1882ed":"with pm.Model() as mdl_first:\n\n    # define priors, weakly informative Normal\n    # here we tried to remove all the time variable and \n    # treat all these as 'attributes' of data rather than the exposure\n    b0 = pm.Normal('b0_intercept', mu=0, sigma=1)\n    b2 = pm.Normal('b2_wday', mu=0, sigma=1)\n    b3 = pm.Normal('b3_month', mu=0, sigma=1)\n    b4 = pm.Normal('b4_year', mu=0, sigma=1)\n    b5 = pm.Normal('b5_snapCA', mu=0, sigma=1)\n    b6 = pm.Normal('b6_snapTX', mu=0, sigma=1)\n    b7 = pm.Normal('b7_snapWI', mu=0, sigma=1)\n    b8 = pm.Normal('b8_event_true_all', mu=-0.01, sigma=1)\n\n    # define linear model and exp link function\n    theta = (b0 +\n            b2 * mx_ex['wday'] +\n            b3 * mx_ex['month'] + \n            b4 * mx_ex['year'] + \n            b5 * mx_ex['snap_CA'] + \n             b6 * mx_ex['snap_TX'] + \n             b7 * mx_ex['snap_WI'] + \n             b8 * mx_ex['event_true_all'] + \n              np.log(mx_ex['d_parse'] ))  ## there is the log(t) as an offset\n\n    ## Define Poisson likelihood\n    y = pm.Poisson('y', mu=np.exp(theta), observed=mx_en['total'].values)","dc09a671":"with mdl_first:\n    trace = pm.sample(1000, tune=2000, init='adapt_diag', target_accept =.8)","fe7fc971":"mdl_first.check_test_point()","23f0c1bd":"## helper function from pymc documentation\ndef strip_derived_rvs(rvs):\n    '''Convenience fn: remove PyMC3-generated RVs from a list'''\n    ret_rvs = []\n    for rv in rvs:\n        if not (re.search('_log',rv.name) or re.search('_interval',rv.name)):\n            ret_rvs.append(rv)\n    return ret_rvs\n\n\ndef plot_traces_pymc(trcs, varnames=None):\n    ''' Convenience fn: plot traces with overlaid means and values '''\n\n    nrows = len(trcs.varnames)\n    if varnames is not None:\n        nrows = len(varnames)\n\n    ax = pm.traceplot(trcs, var_names=varnames, figsize=(12,nrows*1.4),\n                      lines=tuple([(k, {}, v['mean'])\n                                   for k, v in pm.summary(trcs, varnames=varnames).iterrows()]))\n\n    for i, mn in enumerate(pm.summary(trcs, varnames=varnames)['mean']):\n        ax[i,0].annotate('{:.2f}'.format(mn), xy=(mn,0), xycoords='data',\n                         xytext=(5,10), textcoords='offset points', rotation=90,\n                         va='bottom', fontsize='large', color='#AA0022')","edcdeeec":"rvs_fish = [rv.name for rv in strip_derived_rvs(mdl_first.unobserved_RVs)]\npm.summary(trace, varnames=rvs_fish)","c91e6378":"pm.plot_trace(trace)","51ca0cbc":"with mdl_first:\n    pp_trace = pm.sample_posterior_predictive(trace, var_names=rvs_fish, samples=4000)","ad906d9c":"df_2 = scaled_feature.iloc[1913:,:]\ntotal_id = [i for i in submission.id if 'Total' in i]\n# change back d_parse\ndf_2['d_parse']= calendar_feature.iloc[1913:,:].d_parse.values\n","1eddae04":"df_2.d_parse.max()","10496776":"submission_validation = df_2.iloc[:28, :]\nsubmission_evaluation = df_2.iloc[28:, :]\nsubmission_validation.shape,submission_evaluation.shape","2045ef7c":"pp_trace.keys()","fe8d7657":"pp_trace['b0_intercept']","c28d26d5":"def return_y(df):\n    result = 1*pp_trace['b0_intercept']\n    for (i,j) in zip([*pp_trace.keys()][1:], df.index[1:]):\n        #print(i, j)\n        result += pp_trace[i]*df[j]\n        #print(result)\n    return np.exp(result + np.log(df['d_parse']))\n    #return result\nvalidation_y = np.zeros((28, 4000))\nevaluation_y = np.zeros((28, 4000))","c1c4b701":"submission_validation.iloc[0].index","c99e0022":"#submission_evaluation.iloc[0]","4ddab516":"for row in range(len(submission_validation)):\n    validation_y[row, :] = return_y(submission_validation.iloc[row])\n    evaluation_y[row, :] = return_y(submission_evaluation.iloc[row])","74a39142":"np.mean(validation_y)","8203aea9":"np.mean(total_historical)","0d530af9":"## organize the data\ntotal_qt = [float(i.split('_')[2]) for i in total_id]\n\ntotal_only_submission = submission[submission.id.isin(total_id)]\n\ntotal_only_submission['qt']=total_qt\n\ntotal_only_submission.reset_index(inplace=True)\n\ntotal_only_submission.loc[:7]","38711aa5":"for i in range(1,29):\n    col_name = 'F' + str(i)\n    total_only_submission.loc[:8,col_name] =np.quantile(validation_y[i-1], total_qt[:9])\n\nfor i in range(1,29):\n    col_name = 'F' + str(i)\n    total_only_submission.loc[9:,col_name] =np.quantile(evaluation_y[i-1], total_qt[:9])","b8c01efc":"total_only_submission","dfe10ae0":"total_only_submission.to_csv('total_submission.csv', index=False)","2eca3396":"#### Standardize data\nTo help with model convergence, it is better to standardardize your data first","6229becd":"Here since we only modeled the total sales, we will specifically use test set that indicates the total sale","9be4877d":"## Goal  \u26f3\ufe0f\n\n* Use bayesian model to forecast daily sales and estimate the posterior interval\n\n### Why bayesian model?\n\n* In this competition we can utilize a lot of historical data (prior); and by updating the prior belief we can make a forecast (which is the posterior)\n* Bayesian model allows to estimate posterior predictive interval on parameters and response variables\n\n### Limitation\n\n* Here we only modeled the total sale and its uncertainty\n* To make the full prediction we need to scale it up to full hierarchies (aggregated by state, by store, by department etc)\n* There can be more features\/dimensions\n* Needs a lot of experimentation on initializing the paramter, due to the overdispersion\n\n### Referenced Notebook\n\nhttps:\/\/www.kaggle.com\/allunia\/m5-uncertainty\n\n\n## Problem formulation\n\nWe considered the sale of products is a poisson process, i.e, increasing exposing variable (days of operating) in this case, the rate of daily product sale is $\\lambda$\n\n\n$$ y | \\beta, X_i \\sim indep. Poisson(\\lambda_{i})$$\nwhere $$  \\lambda = rt$$\n\n$$ log(\\lambda) \\sim log(t) + log(r) $$\nwhere $$r = \\beta_{i}X $$\n\nhere we call log(t) offset, and ideally X doesn't include information on exposure variable (t)\n","ee354e50":"## For improvement...\n\n* Adding more features\n* Adding different hierachies \n* Better prior: it seems really tricky to update the MCMC chain because the overdispersed prior, I have to tune the prior condition multiple times to get a good convergence\n","690bd3cc":"## Create submission data set","61d9442b":"#### Create model and update it using MCMC","feab622c":"## Build Bayesian model in Pymc3","dd5fcc5e":"#### Correct outliers\n\nIt seems the outliers heavily impacted the way the model converged, so we also corrected those","10a8c349":"#### Results\n\nWe can see the posterior parameters have been estimated with very little variance; r_hat is the [gelman-rubin statistics for convergence ](https:\/\/www.stata.com\/new-in-stata\/gelman-rubin-convergence-diagnostic\/). The r_hat = 1 indicates that the simulated chains have been converged. Although this is not a good estimation (by looking at the ess effective sample size), so far we will temporily use this to estimate the **posterior interval**.\n","72caefda":"## Import Data","2a3954b4":"## Sample posterior predictive parameters","25014b0c":"## Use posterior predictive parameters to estimate the posterior interval of Y (uncertainty)","f5cdaad5":"\u2708\ufe0f*please upvote if you like it* \ud83d\ude80"}}