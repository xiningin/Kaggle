{"cell_type":{"98c45e5c":"code","cef6f2bc":"code","0aa86787":"code","35652bd4":"code","74ff5736":"code","4a12e756":"code","c8fdc10c":"code","ad1bb545":"code","30ea879e":"code","2dd6a7a1":"code","4eccd528":"code","d4456fca":"code","bd853d90":"code","73805aef":"code","e0a52e4f":"code","42973524":"code","165be2b5":"code","e488a4f2":"code","e28df8f0":"code","37de580c":"code","1c98b22a":"code","673037d8":"code","b9d7f3df":"code","7dae40a0":"code","400aaa08":"code","f44b571a":"code","06a88c01":"code","0b9f577b":"code","406b5568":"code","33b0b98c":"code","b107a7c2":"code","d4a799f9":"code","557d6b16":"code","8242b62f":"code","db86460a":"code","47c0061a":"code","a9ff66b2":"code","4a9b6ec9":"code","72de4090":"code","1ba134f3":"code","3aa8313c":"code","d0b2b459":"code","576ef34e":"code","72b57262":"code","01087432":"code","993c6f11":"code","bcf9054d":"markdown","88406d28":"markdown","3e774397":"markdown","ad46c76b":"markdown","edfd83b9":"markdown","8ba4b4da":"markdown","47bd0b69":"markdown","90f0bf0c":"markdown","79ac45ce":"markdown","b7b69efb":"markdown","51404277":"markdown","4d42e8ee":"markdown","e668ea75":"markdown","f54b08e4":"markdown","a549681c":"markdown","c4ee8bed":"markdown","7403367c":"markdown","7f5f03be":"markdown"},"source":{"98c45e5c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cef6f2bc":"# Important Library to handel the data\n\n# For Mathematical caluclation and data handling\nimport numpy as np\nimport pandas as pd    \n\n# For Data visulization\nimport seaborn as sns                      \nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# To understand better use of libraries I have imported library as per need further.","0aa86787":"data = pd.read_csv('\/kaggle\/input\/personal-loan-modeling\/Bank_Personal_Loan_Modelling.csv')\ndata.head()","35652bd4":"print('shape: (rows,columns)'+str(data.shape))","74ff5736":"data.info()","4a12e756":"# 5-point summary help to understand the center tendency value,spread and shape of data\ndata.describe()","c8fdc10c":"#Dropping ID,Experience and ZIP code columns from the data.\ndata.drop('ID', axis=1, inplace=True)\ndata.drop('Experience', axis=1, inplace=True)\ndata.drop('ZIP Code', axis=1, inplace=True)\ndata.head()","ad1bb545":"#Replacing '0' value in mortgage column with its mean value.\n\ndata['Mortgage'] = data['Mortgage'].replace(0,data['Mortgage'].mean())\ndata.head()","30ea879e":"#Visualization with histogram to know the frequency of each columns.\nhistogram = list(data)[0:]\ndata[histogram].hist(stacked=True, bins=10,figsize=(15,30),layout=(12,2));","2dd6a7a1":"#Personal Loan Distribution\nPL_T = len(data.loc[data['Personal Loan']==True])       #PL_T = Personal Loan True value\n\nPL_F = len(data.loc[data['Personal Loan']==False])      #PL_F = Personal Loan False value\n\nprint('Number of Personal Loan taken: '+ str(PL_T))\n\nprint('Percentage: '+ str(PL_T*100\/(PL_F+PL_T))+'%')\n\nprint()\n\nprint('Number of Personal Loan not taken: '+ str(PL_F))\n\nprint('Percentage: '+ str(PL_F*100\/(PL_F+PL_T))+'%')\n","4eccd528":"#Checking correlation of personal column with each column.\ncorr = data.corr()                                        # function for correlation\n\ncorr.style.background_gradient(cmap='RdBu_r')            # We just style our correlation table","d4456fca":"#Visulization of our 1st Hypothesis High salaries are less feasible to buy personal loans.\n\nplt.figure(figsize=(10,8))\n\n#Checking Hypothesis_1: High salaries are less feasible to buy personal loans\ndata.groupby('Personal Loan')['Income'].mean().plot(kind='bar',title='Income');","bd853d90":"#Check for 2nd Hypothesis More the number of earning family members, less probability of buying personal loans.\n\nH2 = pd.crosstab(data['Personal Loan'],data['Family'])\nprint(H2)\n\nprint()\n\nfor i in H2.iloc[:]:\n    \n    print('Chances of taking personal loan having family size as '+ str(i) +' '\n          +str(H2.iloc[1,(i-1)]*100\/(H2.iloc[0,(i-1)]+H2.iloc[1,(i-1)]))+' %')","73805aef":"#Check for 3rd Hypothesis People who are graduated or Advanced\/Professionals are more to buy personal loans.\n\nH3 = pd.crosstab(data['Personal Loan'],data['Education'])\nprint(H3)\n\nprint()\n\nfor i in H3.iloc[:]:\n    print('Chances of taking personal loan as '+ str(i) +' '\n          +str(H3.iloc[1,(i-1)]*100\/(H3.iloc[0,(i-1)]+H3.iloc[1,(i-1)]))+' %')\n","e0a52e4f":"#Since we fail to reject our Hypotesis,let's have a Visulization of hypothesis 3\nH3.div(H3.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True);","42973524":"# Before Checking for our 4th hypothesis we have to divide our age column into some range to have our hypothesis 4.\n# here we are trying to create Age Group column. \n# As 22-30 a young age person,30-50 a mid age person and above 50 as Old age person.\n# Note - Left bin is exclusive and right bin is inclusive.\n\ndata['Age_Group'] = pd.cut(x=data['Age'], bins=[22, 30, 50, 67], labels=['Young', 'Mid', 'Old'])\n\n# For refernce of new function, here .cut() function is used to cut the columnn into a range of bin in category.\ndata.head()","165be2b5":"# Check for 4th Hypothesis Customers with probably the age of 30\u201350 will buy personal loans.\n\nH4 = pd.crosstab(data['Age_Group'],data['Personal Loan'])\nprint(H4)\n\nprint()\n\nH4.div(H4.sum(1).astype(float),axis=0).plot(kind='bar',stacked=True);","e488a4f2":"#dropping Age_Group column \n\ndata.drop('Age_Group', axis=1, inplace=True)\ndata.head()","e28df8f0":"#Library for splitting of data\nfrom sklearn.model_selection import train_test_split\n\n#Libarary for confusion matrix\nfrom sklearn import metrics\n\n#Libarary for Roc curve score\nfrom sklearn.metrics import roc_auc_score","37de580c":"#Spliting of data into train and test set\n\n# 1) Split dependent and independent data\nX = data.drop(['Personal Loan'], axis=1)\ny = data['Personal Loan']\n\n# 2) Spliting randomly in train and test set\nX_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.30, random_state=1)","1c98b22a":"#Library for Logistic regression model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report","673037d8":"# 1) Fit model on the Train-set\nLM_model = LogisticRegression(solver='liblinear')\nLM_model.fit(X_train,y_train)\n\n# 2) Predict on Test-set\nypredict_LM = LM_model.predict(X_test)\n\n# 3) Coefficient and intercept of model\ncoef_LM = pd.DataFrame(LM_model.coef_)\ncoef_LM['intercept'] = LM_model.intercept_\nprint(coef_LM)\nprint()\n\n# 4) Score of model\nLM_model_score = LM_model.score(X_test,y_test)\nprint('Score of the model '+str(LM_model_score))","b9d7f3df":"# 5) Confusion Matrix\n\nLM_matrix = metrics.confusion_matrix(y_test, ypredict_LM)\nprint(LM_matrix)\n\nprint()\n\n# For better understanding of confusion matrix we plot it on heatmap\nLM_CM = pd.DataFrame(LM_matrix, index = [i for i in ['0','1']],\n                    columns = [i for i in ['Predict 0', 'Predict 1']])\n\nplt.figure(figsize=(7,5))\n\nsns.heatmap(LM_CM,annot=True, fmt='g') ","7dae40a0":"# 6) CLassification Report\nprint(classification_report(y_test,ypredict_LM))","400aaa08":"# 7) Area under curve\nAUC_LM = round(roc_auc_score(y_test, ypredict_LM)*100)\nprint(\"AUC: \",  AUC_LM)","f44b571a":"#Library for K-NN Model\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom scipy.stats import zscore","06a88c01":"# 1) Reading Personal Loan distribution by group\ntable=data.groupby(['Personal Loan']).count()\nprint(table)\n\n# 2) Converting all attributes to z-score\nXScaled = X.apply(zscore)","0b9f577b":"# 2) Spliting randomly in train and test set\nX_train1,X_test1,y_train1,y_test1 = train_test_split(XScaled, y, test_size=0.30, random_state=1)","406b5568":"# 3) Creating Model on train set\nScore = []\nfor k in range(1,10):\n    KNN_model= KNeighborsClassifier(n_neighbors=k)\n    KNN_model.fit(X_train1,y_train1)\n    Score.append(KNN_model.score(X_test1,y_test1))\n    \nplt.plot(range(1,10),Score)\n\nprint(Score)","33b0b98c":"# 4) Confusion Matrix\nypredict_KNN = KNN_model.predict(X_test1)\n\nKNN_matrix = metrics.confusion_matrix(y_test1, ypredict_KNN)\nprint(KNN_matrix)\n\nKNN_CM = pd.DataFrame(KNN_matrix, index= [i for i in ['0','1']]\n                     , columns= [i for i in ['Predict 0', 'Predict 1']])\n\nplt.figure(figsize=(7,5))\n\nsns.heatmap(KNN_CM, annot=True, fmt='g')","b107a7c2":"# 5) classification_report\nprint(classification_report(y_test1,ypredict_KNN))","d4a799f9":"# 6) Area under curve\nAUC_KNN = round(roc_auc_score(y_test, ypredict_KNN)*100)\n\nprint(\"AUC: \",  AUC_KNN)","557d6b16":"#Library for Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB","8242b62f":"# 1) Creating model on train set\nNB_model = GaussianNB()\nNB_model.fit(X_train,y_train.ravel())\n\n# 2) Predict on test set\nypredict_NB = NB_model.predict(X_test)\n","db86460a":"# 3) Confusion Matrix\nNB_matrix = metrics.confusion_matrix(y_test,ypredict_NB)\nprint(NB_matrix)\n\nNB_CM = pd.DataFrame(NB_matrix, index= [i for i in ['0','1']]\n                     , columns= [i for i in ['Predict 0', 'Predict 1']])\n\nplt.figure(figsize=(7,5))\n\nsns.heatmap(NB_CM, annot=True, fmt='g')","47c0061a":"# 5) CLassification Report\nprint(classification_report(y_test,ypredict_NB))","a9ff66b2":"# 8) Area under curve\nAUC_NB = round(roc_auc_score(y_test, ypredict_NB)*100)\nprint(\"AUC: \",  AUC_NB)","4a9b6ec9":"from sklearn.pipeline import Pipeline                               #For pipeline different models\nfrom sklearn.preprocessing import StandardScaler                    #for Scaling the data\n#Different types of classifier I will used for modeling and comparing\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier","72de4090":"DT_model = Pipeline([('Scalar1', StandardScaler()),\n           ('model_1',DecisionTreeClassifier())])        #Decision Tree Model\n\nRF_model = Pipeline([('Scalar2', StandardScaler()),\n           ('model_2', RandomForestClassifier())])        #Random Forest Model\n\nAda_model = Pipeline([('Scalar3', StandardScaler()),\n                   ('model_3', AdaBoostClassifier())])     #ADa-Boosting model   \n\n# Extra value like n_estimator and many more are taken as default you can change if needed","1ba134f3":"pipeline = [DT_model,RF_model, Ada_model]    # List of differnt model which are used for looping purpose\n\npipeline_dict = { 0: 'Decision Tree', 1: 'Random Forest', 2: 'Ada-Boosting'}","3aa8313c":"for test in pipeline:\n    test.fit(X_train, y_train)\n    \nfor i, model in enumerate(pipeline):\n    print('{} Test Accuracy : {} '.format(pipeline_dict[i],model.score(X_test,y_test)))","d0b2b459":"# Decision Tree\nypredict_DT = DT_model.predict(X_test)\nDT_matrix = metrics.confusion_matrix(y_test,ypredict_DT)\nprint('Decision Tree Confusion Matrix')\nprint(DT_matrix)\n\nprint()\n\n# Random Forest \nypredict_RF = RF_model.predict(X_test)\nRF_matrix = metrics.confusion_matrix(y_test,ypredict_RF)\nprint('Random Forest Confusion Matrix')\nprint(RF_matrix)\n\nprint()\n\n# Ada-Boosting\n# Logistic Regression\nypredict_Ada = Ada_model.predict(X_test)\nAda_matrix = metrics.confusion_matrix(y_test,ypredict_Ada)\nprint('Ada Boosting Confusion Matrix')\nprint(Ada_matrix)","576ef34e":"#for clear vision of confusion matrix\nDT_CM = pd.DataFrame(DT_matrix, index=[i for i in [0,1]],\n                    columns=[i for i in [\"Predict 0\", \"Predict 1\"]])\nplt.figure(figsize=(8,5))\nsns.heatmap(DT_CM, annot=True, fmt='g')","72b57262":"print(classification_report(y_test1,ypredict_DT))","01087432":"AUC_DT = round(roc_auc_score(y_test1,ypredict_DT)*100)\nprint('AUC of Decision Tree model: ', AUC_DT)","993c6f11":"#All the value are taken from aboved solved model for easy comparison among differnt types of model.\nmodel_index = ['Logistic Model','KNN model', 'Naive Bayes', 'Decision Tree model']\ncolumn_lists = ['False Negative', 'True Postive','Recall', 'Accuracy (%)', 'AUC (%)']\nmodels_info = {'False Negative' : [73,69,64,19],'True Postive' : [76,80,85,130],\n               'Recall' : [0.51,0.54,0.57,0.87], 'Accuracy (%)' : [94,95,87,98],\n               'AUC (%)' : [75,77,74,93]}\ncomparison_table = pd.DataFrame(models_info, model_index, column_lists)\ncomparison_table","bcf9054d":"# 5-Point Summary Report:\n\nNOTE:- Currency is in form of dollars.\n\n1) Age in given data ranges from 23 to 67 with mean and median as 45 approx.\n\n2) Average experience of given data is 20 years.\n\n3) Income in given data ranges from 8000 to 224000 with an average of 73000 with an median of 64000 i.e Income column is positive-skewed.\n\n4) Family size of given customer ranges from 1-4.\n\n5) Avg. spending on credit cards per month ranges from 0 to 10000 with an average of 1937.93 (1940 approx). And column is highly postive-skewed.\n\n6) Mortage column is unevenly distrubuted which means it contain some missing-values (from head table we can say that missing value is presented in form of 0). So we will replace null-value with mean of Mortage column.\n\n### Complete report for data columns from given and above 5-pont summary table.\n\n1) It got 7 numeric columns: ID, Age, Experience, Income, Zip Code, CC Avg, Mortgage\n\n2) It got 2 categorical columns: Family,Education\n\n3) It got 5 Boolean columns: Personal Loan, Securities Account, CD Account, Online, Credit Card\n\n4) ID, Experience, and ZIP Code column are not useful for our model desgin so we can drop that column. As Id and Zip_Code are group of number of paticular person and experience is related with Age.\n\n5) Replacing null-value (i.e 0 in value in column) in Mortgage Column with mean of Mortgage.","88406d28":"### Working over our target column i.e Personal Loan column.","3e774397":"### 3) Naive Bayes","ad46c76b":"### 1st Hypothesis Report:\nY-axis is mean of Income of the customers from given sample, we can clearly see that personal loan are taken by people having high mean of income.\n\nOur 1 st Hypothesis of High salaries are less feasible to buy personal loans for given sample is false, So we reject our Hypostesis 1.","edfd83b9":"As we are building model for prediction who will take personal loan we must predict more true postive rate value i.e Recall beacuse we are considering who will take personal loan but model predicted negative it will increase chance of failure. It can be control by controlling or reducing the False Negative value.\n\nAs per that I have Decision tree model which gave better result as 0.87 recall and 19 False Negative value.\n\nWe can try to have better result if we try in bagging or changing the value of gamma in boosting or can try differnt XG boost technique. But for now I have concluded considering Decision tree model best fit model for the given data type.","8ba4b4da":"### Note:\n1 = Undergrad 2 = Graduate 3 = Advanced\/Professional\n\n### 3rd Hypothesis Report:\nIn above output we can clearly see that probability to have personal loan with different education level of given sample differ as Advance\/Proffesional and Graduate have higher chance to take a Personal loan.\n\nOur 3rd Hypothesis of People who are graduated or Advanced\/Professionals have more chances to take a personal loans from a bank rather than people who are undergrad for given sample is True. So, we fail to reject our 3rd Hypothesis","47bd0b69":"4th Hypothesis Report:\nIn above output we can clearly see that probability to have personal loan with different Age group in given sample is approx same.\n\nOur 4th Hypothesis of Customers with probably the age of 30\u201350 will take personal loans is False. So, we reject our 4th Hypothesis. We can also drop age","90f0bf0c":"### Conclusion:\n\nComparison table\n","79ac45ce":"Decision Tree gives best result among all as it have only 19 FN value. So, we will have extra calculation over it.","b7b69efb":"### Personal Loan best Correlation\n1) Personal Loan is highly correlated with Income, average spending on Credit cards, mortgage and if the customer has a certificate of deposit (CD) account with the bank.","51404277":"### Model Building Based On:\n1) Logistic Model\n\n2) K-NN Model\n\n3) Naive Bayes\n\n4) Decsion Tree\n\n5) Random forest\n\n6) Ada  Boosting\n\n1) We are going to build model over all the 3 technique and choose best among this.\n\n2) As we are building model for prediction who will take personal loan we must predict more true postive rate value i.e Recall beacuse we are considering who will take personal loan but model predicted negative it will increase chance of failure. It can be control by controlling or reducing the False Negative value.\n\nRecall or True postive rate = True postive\/(True Postive+False Negative)\n\n2) I have training and testing set is distrubuted with 70:30 ratio.","4d42e8ee":"## EDA","e668ea75":"### 2nd Hypothesis Report:\n2nd Hypothesis In above output we can clearly see that probability to have personal loan with different size of family for given sample is approx same.\n\nOur 2nd Hypothesis of More the number of earning family members, less probability of buying personal loans is false, So we reject the Hypothesis 2.","f54b08e4":"### Visualization Report of each column of given sample data.\n1) Maximum amount of customer have 1 child in there family.\n\n2) Education graph represent maximum amount of customer are undergraduate.\n\n3) Approx 90% of customer have'nt accepted a personal loan offered in the last campaign.\n\n4) Approx 90% of customer don't have securities account with the bank.\n\n5) In CD_account graph we can see more than 90% of customer have'nt deposited there certificate in the bank.\n\n6) Approx 3000 customer have an online banking system.\n\n7) Approx 90% of customer don't use a credit card issued by Bank","a549681c":"## Info Report:\n\n1) All the columns are in numerical form, but chance are they can be in binary format we will have deep look in it further.\n\n2) No null value is present, but chances of missing value can be in form of 0 (we will check it further)","c4ee8bed":"### 1) Logistic Model","7403367c":"### Note:\nWe got different accuracy but we can't defined best among this so we will check for confusion matrix and pick best among this for extra calculation.","7f5f03be":"### 2) K-NN Model"}}