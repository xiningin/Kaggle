{"cell_type":{"8a06c349":"code","795f422a":"code","2343226c":"code","fdcf4e43":"code","7871270d":"code","cb307aa7":"code","e75811bf":"code","bebcdc46":"code","d86aa798":"code","c2434ba1":"code","ef589209":"code","8b0d2f76":"code","443a23e4":"code","096ddc78":"code","9e9eef15":"code","4609b735":"code","ce9aa049":"code","8e95e0ce":"code","2b8409bc":"code","e0efb1a8":"code","e36a7304":"code","c678dac2":"code","5bbc5355":"code","10f7715b":"code","f40cc805":"code","666a3ef2":"code","4a9de942":"code","789e7077":"code","7d7a2bd6":"code","a79f54e9":"code","cd774c45":"code","2381cb75":"code","69d29b31":"code","9ab1a728":"code","b7010367":"code","e19f5586":"code","df34fa3c":"code","9290c03b":"code","0bafe823":"code","3488702c":"code","cdf41462":"code","b0536e71":"code","6860991a":"code","81d75161":"code","167f23cb":"code","ddf70dd4":"code","d3063888":"code","4e7a47e9":"code","403c705d":"code","88c9eeac":"code","61d032b1":"code","98893d04":"code","742df618":"markdown","8bbad216":"markdown","1cc228e5":"markdown","7409bc5f":"markdown","59835afa":"markdown","ea5a34b3":"markdown","7a4d2a53":"markdown","127b65fa":"markdown","e8116671":"markdown","3fdeb266":"markdown","d47135c3":"markdown","a311991a":"markdown","e648a9ef":"markdown","5e4e5cd0":"markdown","ed291237":"markdown","90844e6d":"markdown","dd3a5a0b":"markdown","1336bbc0":"markdown","034899de":"markdown","028763e8":"markdown","6e84b000":"markdown"},"source":{"8a06c349":"import gc\nimport re\nimport operator \n\nimport numpy as np\nimport pandas as pd\n\nfrom gensim.models import KeyedVectors\n\nfrom sklearn import model_selection\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, Input, Dense, CuDNNGRU,CuDNNLSTM, Concatenate, concatenate, Bidirectional, SpatialDropout1D, Conv1D, GlobalAveragePooling1D, GlobalMaxPooling1D\nfrom keras.optimizers import RMSprop, Adam\nfrom keras.models import Model\nfrom keras.callbacks import EarlyStopping\n\nimport seaborn as sns","795f422a":"train = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv\")\ntest = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv\")\n\nprint(\"Train shape : \",train.shape)\nprint(\"Test shape : \",test.shape)\n\n\ntrain.head()\n","2343226c":"test.head()","fdcf4e43":"# Only 13GB of ram available, we gotta be careful !\n\ndf = pd.concat([train[['id','comment_text']], test], axis=0)\ndel(train, test)\ngc.collect()","7871270d":"ft_common_crawl = '..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec'\nembeddings_index = KeyedVectors.load_word2vec_format(ft_common_crawl)","cb307aa7":"gc.collect()","e75811bf":"def build_vocab(texts):\n    sentences = texts.apply(lambda x: x.split()).values\n    vocab = {}\n    for sentence in sentences:\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab","bebcdc46":"def check_coverage(vocab, embeddings_index):\n    known_words = {}\n    unknown_words = {}\n    nb_known_words = 0\n    nb_unknown_words = 0\n    for word in vocab.keys():\n        try:\n            known_words[word] = embeddings_index[word]\n            nb_known_words += vocab[word]\n        except:\n            unknown_words[word] = vocab[word]\n            nb_unknown_words += vocab[word]\n            pass\n\n    print('Found embeddings for {:.3%} of vocab'.format(len(known_words) \/ len(vocab)))\n    print('Found embeddings for  {:.3%} of all text'.format(nb_known_words \/ (nb_known_words + nb_unknown_words)))\n    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n\n    return unknown_words","d86aa798":"df['comment_text'] = df['comment_text'].apply(lambda x: x.lower())\ngc.collect()","c2434ba1":"vocab = build_vocab(df['comment_text'])\noov = check_coverage(vocab, embeddings_index)\noov[:10]","ef589209":"gc.collect()","8b0d2f76":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }","443a23e4":"del(vocab,oov)\ngc.collect()","096ddc78":"def known_contractions(embed):\n    known = []\n    for contract in contraction_mapping:\n        if contract in embed:\n            known.append(contract)\n    return known","9e9eef15":"print(\"- Known Contractions -\")\nprint(\"   FastText :\")\nprint(known_contractions(embeddings_index))","4609b735":"def clean_contractions(text, mapping):\n    specials = [\"\u2019\", \"\u2018\", \"\u00b4\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text","ce9aa049":"df['comment_text'] = df['comment_text'].apply(lambda x: clean_contractions(x, contraction_mapping))","8e95e0ce":"vocab = build_vocab(df['comment_text'])\noov = check_coverage(vocab, embeddings_index)\noov[:10]","2b8409bc":"del(vocab,oov)\ngc.collect()","e0efb1a8":"punct = \"\/-'?!.,#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~\" + '\"\"\u201c\u201d\u2019' + '\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014\u2013&'","e36a7304":"def unknown_punct(embed, punct):\n    unknown = ''\n    for p in punct:\n        if p not in embed:\n            unknown += p\n            unknown += ' '\n    return unknown","c678dac2":"print(unknown_punct(embeddings_index, punct))","5bbc5355":"punct_mapping = {\"_\":\" \", \"`\":\" \"}","10f7715b":"def clean_special_chars(text, punct, mapping):\n    for p in mapping:\n        text = text.replace(p, mapping[p])    \n    for p in punct:\n        text = text.replace(p, f' {p} ')     \n    return text","f40cc805":"df['comment_text'] = df['comment_text'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))","666a3ef2":"vocab = build_vocab(df['comment_text'])\noov = check_coverage(vocab, embeddings_index)\n","4a9de942":"oov[:100]","789e7077":"del(vocab,oov)\ngc.collect()","7d7a2bd6":"swear_words = [\n    ' 4r5e ',\n    ' 5h1t ',\n    ' 5hit ',\n    ' a55 ',\n    ' anal ',\n    ' anus ',\n    ' ar5e ',\n    ' arrse ',\n    ' arse ',\n    ' ass ',\n    ' ass-fucker ',\n    ' asses ',\n    ' assfucker ',\n    ' assfukka ',\n    ' asshole ',\n    ' assholes ',\n    ' asswhole ',\n    ' a_s_s ',\n    ' b!tch ',\n    ' b00bs ',\n    ' b17ch ',\n    ' b1tch ',\n    ' ballbag ',\n    ' balls ',\n    ' ballsack ',\n    ' bastard ',\n    ' beastial ',\n    ' beastiality ',\n    ' bellend ',\n    ' bestial ',\n    ' bestiality ',\n    ' biatch ',\n    ' bitch ',\n    ' bitcher ',\n    ' bitchers ',\n    ' bitches ',\n    ' bitchin ',\n    ' bitching ',\n    ' bloody ',\n    ' blow job ',\n    ' blowjob ',\n    ' blowjobs ',\n    ' boiolas ',\n    ' bollock ',\n    ' bollok ',\n    ' boner ',\n    ' boob ',\n    ' boobs ',\n    ' booobs ',\n    ' boooobs ',\n    ' booooobs ',\n    ' booooooobs ',\n    ' breasts ',\n    ' buceta ',\n    ' bugger ',\n    ' bum ',\n    ' bunny fucker ',\n    ' butt ',\n    ' butthole ',\n    ' buttmuch ',\n    ' buttplug ',\n    ' c0ck ',\n    ' c0cksucker ',\n    ' carpet muncher ',\n    ' cawk ',\n    ' chink ',\n    ' cipa ',\n    ' cl1t ',\n    ' clit ',\n    ' clitoris ',\n    ' clits ',\n    ' cnut ',\n    ' cock ',\n    ' cock-sucker ',\n    ' cockface ',\n    ' cockhead ',\n    ' cockmunch ',\n    ' cockmuncher ',\n    ' cocks ',\n    ' cocksuck ',\n    ' cocksucked ',\n    ' cocksucker ',\n    ' cocksucking ',\n    ' cocksucks ',\n    ' cocksuka ',\n    ' cocksukka ',\n    ' cok ',\n    ' cokmuncher ',\n    ' coksucka ',\n    ' coon ',\n    ' cox ',\n    ' crap ',\n    ' cum ',\n    ' cummer ',\n    ' cumming ',\n    ' cums ',\n    ' cumshot ',\n    ' cunilingus ',\n    ' cunillingus ',\n    ' cunnilingus ',\n    ' cunt ',\n    ' cuntlick ',\n    ' cuntlicker ',\n    ' cuntlicking ',\n    ' cunts ',\n    ' cyalis ',\n    ' cyberfuc ',\n    ' cyberfuck ',\n    ' cyberfucked ',\n    ' cyberfucker ',\n    ' cyberfuckers ',\n    ' cyberfucking ',\n    ' d1ck ',\n    ' damn ',\n    ' dick ',\n    ' dickhead ',\n    ' dildo ',\n    ' dildos ',\n    ' dink ',\n    ' dinks ',\n    ' dirsa ',\n    ' dlck ',\n    ' dog-fucker ',\n    ' doggin ',\n    ' dogging ',\n    ' donkeyribber ',\n    ' doosh ',\n    ' duche ',\n    ' dyke ',\n    ' ejaculate ',\n    ' ejaculated ',\n    ' ejaculates ',\n    ' ejaculating ',\n    ' ejaculatings ',\n    ' ejaculation ',\n    ' ejakulate ',\n    ' f u c k ',\n    ' f u c k e r ',\n    ' f4nny ',\n    ' fag ',\n    ' fagging ',\n    ' faggitt ',\n    ' faggot ',\n    ' faggs ',\n    ' fagot ',\n    ' fagots ',\n    ' fags ',\n    ' fanny ',\n    ' fannyflaps ',\n    ' fannyfucker ',\n    ' fanyy ',\n    ' fatass ',\n    ' fcuk ',\n    ' fcuker ',\n    ' fcuking ',\n    ' feck ',\n    ' fecker ',\n    ' felching ',\n    ' fellate ',\n    ' fellatio ',\n    ' fingerfuck ',\n    ' fingerfucked ',\n    ' fingerfucker ',\n    ' fingerfuckers ',\n    ' fingerfucking ',\n    ' fingerfucks ',\n    ' fistfuck ',\n    ' fistfucked ',\n    ' fistfucker ',\n    ' fistfuckers ',\n    ' fistfucking ',\n    ' fistfuckings ',\n    ' fistfucks ',\n    ' flange ',\n    ' fook ',\n    ' fooker ',\n    ' fuck ',\n    ' fucka ',\n    ' fucked ',\n    ' fucker ',\n    ' fuckers ',\n    ' fuckhead ',\n    ' fuckheads ',\n    ' fuckin ',\n    ' fucking ',\n    ' fuckings ',\n    ' fuckingshitmotherfucker ',\n    ' fuckme ',\n    ' fucks ',\n    ' fuckwhit ',\n    ' fuckwit ',\n    ' fudge packer ',\n    ' fudgepacker ',\n    ' fuk ',\n    ' fuker ',\n    ' fukker ',\n    ' fukkin ',\n    ' fuks ',\n    ' fukwhit ',\n    ' fukwit ',\n    ' fux ',\n    ' fux0r ',\n    ' f_u_c_k ',\n    ' gangbang ',\n    ' gangbanged ',\n    ' gangbangs ',\n    ' gaylord ',\n    ' gaysex ',\n    ' goatse ',\n    ' God ',\n    ' god-dam ',\n    ' god-damned ',\n    ' goddamn ',\n    ' goddamned ',\n    ' hardcoresex ',\n    ' hell ',\n    ' heshe ',\n    ' hoar ',\n    ' hoare ',\n    ' hoer ',\n    ' homo ',\n    ' hore ',\n    ' horniest ',\n    ' horny ',\n    ' hotsex ',\n    ' jack-off ',\n    ' jackoff ',\n    ' jap ',\n    ' jerk-off ',\n    ' jism ',\n    ' jiz ',\n    ' jizm ',\n    ' jizz ',\n    ' kawk ',\n    ' knob ',\n    ' knobead ',\n    ' knobed ',\n    ' knobend ',\n    ' knobhead ',\n    ' knobjocky ',\n    ' knobjokey ',\n    ' kock ',\n    ' kondum ',\n    ' kondums ',\n    ' kum ',\n    ' kummer ',\n    ' kumming ',\n    ' kums ',\n    ' kunilingus ',\n    ' l3itch ',\n    ' labia ',\n    ' lmfao ',\n    ' lust ',\n    ' lusting ',\n    ' m0f0 ',\n    ' m0fo ',\n    ' m45terbate ',\n    ' ma5terb8 ',\n    ' ma5terbate ',\n    ' masochist ',\n    ' master-bate ',\n    ' masterb8 ',\n    ' masterbat3 ',\n    ' masterbate ',\n    ' masterbation ',\n    ' masterbations ',\n    ' masturbate ',\n    ' mo-fo ',\n    ' mof0 ',\n    ' mofo ',\n    ' mothafuck ',\n    ' mothafucka ',\n    ' mothafuckas ',\n    ' mothafuckaz ',\n    ' mothafucked ',\n    ' mothafucker ',\n    ' mothafuckers ',\n    ' mothafuckin ',\n    ' mothafucking ',\n    ' mothafuckings ',\n    ' mothafucks ',\n    ' mother fucker ',\n    ' motherfuck ',\n    ' motherfucked ',\n    ' motherfucker ',\n    ' motherfuckers ',\n    ' motherfuckin ',\n    ' motherfucking ',\n    ' motherfuckings ',\n    ' motherfuckka ',\n    ' motherfucks ',\n    ' muff ',\n    ' mutha ',\n    ' muthafecker ',\n    ' muthafuckker ',\n    ' muther ',\n    ' mutherfucker ',\n    ' n1gga ',\n    ' n1gger ',\n    ' nazi ',\n    ' nigg3r ',\n    ' nigg4h ',\n    ' nigga ',\n    ' niggah ',\n    ' niggas ',\n    ' niggaz ',\n    ' nigger ',\n    ' niggers ',\n    ' nob ',\n    ' nob jokey ',\n    ' nobhead ',\n    ' nobjocky ',\n    ' nobjokey ',\n    ' numbnuts ',\n    ' nutsack ',\n    ' orgasim ',\n    ' orgasims ',\n    ' orgasm ',\n    ' orgasms ',\n    ' p0rn ',\n    ' pawn ',\n    ' pecker ',\n    ' penis ',\n    ' penisfucker ',\n    ' phonesex ',\n    ' phuck ',\n    ' phuk ',\n    ' phuked ',\n    ' phuking ',\n    ' phukked ',\n    ' phukking ',\n    ' phuks ',\n    ' phuq ',\n    ' pigfucker ',\n    ' pimpis ',\n    ' piss ',\n    ' pissed ',\n    ' pisser ',\n    ' pissers ',\n    ' pisses ',\n    ' pissflaps ',\n    ' pissin ',\n    ' pissing ',\n    ' pissoff ',\n    ' poop ',\n    ' porn ',\n    ' porno ',\n    ' pornography ',\n    ' pornos ',\n    ' prick ',\n    ' pricks ',\n    ' pron ',\n    ' pube ',\n    ' pusse ',\n    ' pussi ',\n    ' pussies ',\n    ' pussy ',\n    ' pussys ',\n    ' rectum ',\n    ' retard ',\n    ' rimjaw ',\n    ' rimming ',\n    ' s hit ',\n    ' s.o.b. ',\n    ' sadist ',\n    ' schlong ',\n    ' screwing ',\n    ' scroat ',\n    ' scrote ',\n    ' scrotum ',\n    ' semen ',\n    ' sex ',\n    ' sh!t ',\n    ' sh1t ',\n    ' shag ',\n    ' shagger ',\n    ' shaggin ',\n    ' shagging ',\n    ' shemale ',\n    ' shit ',\n    ' shitdick ',\n    ' shite ',\n    ' shited ',\n    ' shitey ',\n    ' shitfuck ',\n    ' shitfull ',\n    ' shithead ',\n    ' shiting ',\n    ' shitings ',\n    ' shits ',\n    ' shitted ',\n    ' shitter ',\n    ' shitters ',\n    ' shitting ',\n    ' shittings ',\n    ' shitty ',\n    ' skank ',\n    ' slut ',\n    ' sluts ',\n    ' smegma ',\n    ' smut ',\n    ' snatch ',\n    ' son-of-a-bitch ',\n    ' spac ',\n    ' spunk ',\n    ' s_h_i_t ',\n    ' t1tt1e5 ',\n    ' t1tties ',\n    ' teets ',\n    ' teez ',\n    ' testical ',\n    ' testicle ',\n    ' tit ',\n    ' titfuck ',\n    ' tits ',\n    ' titt ',\n    ' tittie5 ',\n    ' tittiefucker ',\n    ' titties ',\n    ' tittyfuck ',\n    ' tittywank ',\n    ' titwank ',\n    ' tosser ',\n    ' turd ',\n    ' tw4t ',\n    ' twat ',\n    ' twathead ',\n    ' twatty ',\n    ' twunt ',\n    ' twunter ',\n    ' v14gra ',\n    ' v1gra ',\n    ' vagina ',\n    ' viagra ',\n    ' vulva ',\n    ' w00se ',\n    ' wang ',\n    ' wank ',\n    ' wanker ',\n    ' wanky ',\n    ' whoar ',\n    ' whore ',\n    ' willies ',\n    ' willy ',\n    ' xrated ',\n    ' xxx '    \n]","a79f54e9":"replace_with_fuck = []\n\nfor swear in swear_words:\n    if swear[1:(len(swear)-1)] not in embeddings_index:\n        replace_with_fuck.append(swear)\n        \nreplace_with_fuck = '|'.join(replace_with_fuck)\nreplace_with_fuck\n        ","cd774c45":"def handle_swears(text):\n    text = re.sub(replace_with_fuck, ' fuck ', text)\n    return text","2381cb75":"df['comment_text'] = df['comment_text'].apply(lambda x: handle_swears(x))\ngc.collect()","69d29b31":"train = df.iloc[:1804874,:]\ntest = df.iloc[1804874:,:]\n\ntrain.head()","9ab1a728":"del(df)\ngc.collect()","b7010367":"train.head()","e19f5586":"train_orig = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv\")\ntrain_orig.head()","df34fa3c":"train = pd.concat([train,train_orig[['target']]],axis=1)\ntrain.head()","9290c03b":"del(train_orig)\ngc.collect()","0bafe823":"train['target'] = np.where(train['target'] >= 0.5, True, False)","3488702c":"train_df, validate_df = model_selection.train_test_split(train, test_size=0.1)\nprint('%d train comments, %d validate comments' % (len(train_df), len(validate_df)))","cdf41462":"MAX_NUM_WORDS = 100000\nTOXICITY_COLUMN = 'target'\nTEXT_COLUMN = 'comment_text'\n\n# Create a text tokenizer.\ntokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\ntokenizer.fit_on_texts(train_df[TEXT_COLUMN])\n\n# All comments must be truncated or padded to be the same length.\nMAX_SEQUENCE_LENGTH = 256\ndef pad_text(texts, tokenizer):\n    return pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=MAX_SEQUENCE_LENGTH)","b0536e71":"gc.collect()","6860991a":"EMBEDDINGS_DIMENSION = 300\nembedding_matrix = np.zeros((len(tokenizer.word_index) + 1,EMBEDDINGS_DIMENSION))","81d75161":"num_words_in_embedding = 0\n\nfor word, i in tokenizer.word_index.items():\n    if word in embeddings_index.vocab:\n        embedding_vector = embeddings_index[word]\n        embedding_matrix[i] = embedding_vector        \n        num_words_in_embedding += 1","167f23cb":"train_text = pad_text(train_df[TEXT_COLUMN], tokenizer)\ntrain_labels = train_df[TOXICITY_COLUMN]\nvalidate_text = pad_text(validate_df[TEXT_COLUMN], tokenizer)\nvalidate_labels = validate_df[TOXICITY_COLUMN]","ddf70dd4":"gc.collect()","d3063888":"sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedding_layer = Embedding(len(tokenizer.word_index) + 1,\n                            EMBEDDINGS_DIMENSION,\n                            weights=[embedding_matrix],\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=False)\nx = embedding_layer(sequence_input)\nx = SpatialDropout1D(0.2)(x)\nx1 = Bidirectional(CuDNNLSTM(256, return_sequences=True))(x) \nx1_1 = Bidirectional(CuDNNLSTM(256, return_sequences=True))(x1) \nx2 = Bidirectional(CuDNNGRU(128, return_sequences=True))(x1)   \nx2_1 = Bidirectional(CuDNNGRU(128, return_sequences=True))(x2)   \nmax_pool1 = GlobalMaxPooling1D()(x1_1)\nmax_pool2 = GlobalMaxPooling1D()(x2_1) \n\n#x = concatenate([max_pool1, max_pool2])\nx = Concatenate()([max_pool1, max_pool2])\n\npreds = Dense(1, activation='sigmoid')(x)\n\n\nmodel = Model(sequence_input, preds)\nmodel.summary()\n","4e7a47e9":"model.compile(loss='binary_crossentropy',\n              optimizer=Adam(),\n              metrics=['acc'])","403c705d":"BATCH_SIZE = 1024\nNUM_EPOCHS = 100","88c9eeac":"model.fit(\n    train_text,\n    train_labels,\n    batch_size=BATCH_SIZE,\n    epochs=NUM_EPOCHS,\n    validation_data=(validate_text, validate_labels),\n    callbacks = [EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)])\n","61d032b1":"submission = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/sample_submission.csv', index_col='id')\nsubmission['prediction'] = model.predict(pad_text(test[TEXT_COLUMN], tokenizer))\nsubmission.reset_index(drop=False, inplace=True)\nsubmission.head()","98893d04":"\nsubmission.to_csv('submission.csv', index=False)","742df618":"# Import Data\n\nLet's have a little look at the data.","8bbad216":"Let's split the data back into train and test","1cc228e5":"The extra features in the training set must be for analysing the bias. This is going to be an interesting competition with such a metric!","7409bc5f":"# Embeddings\n\nTo start we'll just take the FastText Common Crawl embeddings. Later, we'll hopefully combine multiple embeddings.","59835afa":"# Preprocessing Text\n\nAs with most NLP tasks, we will start by using some pre-trained embeddings for our words. This provides us with a numerical representation of our input that we can use for modelling. Mapping words to embeddings isn't always straight forward, however: the data may not be very tidy.\n\nThe first step, then, is to ensure we get as many words mapped to a suitable embedding as possible. To do this, we'll make use of two excellent kernels:\n\n- https:\/\/www.kaggle.com\/christofhenkel\/how-to-preprocessing-when-using-embeddings \n- https:\/\/www.kaggle.com\/theoviel\/improve-your-score-with-text-preprocessing-v2 ","ea5a34b3":"# Model Training","7a4d2a53":"Split into train\/validation sets","127b65fa":"# Further Preparation","e8116671":"Immediately we see contractions are an issue for FastText (such as \"was not\" -> \"wasn't\"). Let's try and fix this.","3fdeb266":"Let's submit this as our first submission. Once we have a reasonable pipeline setup, we can move on to looking at the competition metric in more detail.","d47135c3":"Tokenize the text","a311991a":"# Model Architecture\n\nAdding dropout \/ 1d conv \/ concatenated poolings based on the architecture presented @ https:\/\/www.kaggle.com\/tunguz\/bi-gru-cnn-poolings-gpu-kernel-version","e648a9ef":"Create our embedding matrix","5e4e5cd0":"Convert target to binary flag","ed291237":"# Predict & Submit","90844e6d":"Looks like punctuation is the next issue here, so let's sort it out.","dd3a5a0b":"## Swears\n\nLet's replace any swear words we don't have an embedding for with something we do ;)","1336bbc0":"# Import Libraries","034899de":"We will lower() all words, then look up those that do not appear in lower case in the embeddings but do in upper case, and add them.","028763e8":"There is a lot of words here that just aren't going to have any embeddings. We could go further and try to correct mispellings, but that is likely a small improvement we can worry about when we're trying to improve the model further.","6e84b000":"# References\n\nIn recognition to the original author of this kernel: https:\/\/www.kaggle.com\/taindow\/simple-cudnngru-python-keras\nI made slight changes on the NN architecture.\n\nI've made use of some great kernels already - check them out and give them an upvote if any of this is useful!\n\n### Preprocessing\n\n- https:\/\/www.kaggle.com\/christofhenkel\/how-to-preprocessing-when-using-embeddings \n- https:\/\/www.kaggle.com\/theoviel\/improve-your-score-with-text-preprocessing-v2 \n\n### Model Architecture\n\n-- https:\/\/www.kaggle.com\/tunguz\/bi-gru-cnn-poolings-gpu-kernel-version\n\n### Other\n\n- https:\/\/www.kaggle.com\/dborkan\/benchmark-kernel \n- https:\/\/www.kaggle.com\/c\/jigsaw-unintended-bias-in-toxicity-classification\/discussion\/87245"}}