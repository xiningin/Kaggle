{"cell_type":{"1a059261":"code","6589114d":"code","d06de28c":"code","68772a57":"code","980d5f94":"code","023de951":"code","efb50cc3":"code","d3702a0d":"code","4b749059":"code","ab3ea319":"code","2d380bf5":"code","2b8d9500":"code","ab84ba99":"markdown","49407e4a":"markdown","1f46a475":"markdown","2cd76b47":"markdown","092c1624":"markdown","95512b01":"markdown","831869ab":"markdown","ec764202":"markdown","1f4460a0":"markdown","c8bb055a":"markdown","99f1ccc9":"markdown","026f0157":"markdown","f9ffc2de":"markdown"},"source":{"1a059261":"#Titanic Tutorial | Kaggle\n#just in case if we require tutorial\n#https:\/\/www.kaggle.com\/alexisbcook\/titanic-tutorial - Kaggle tutorial\n#https:\/\/www.kaggle.com\/sinakhorami\/titanic-best-working-classifier - One that i reffered to for data cleaning \n#https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python\/data - one i reffered","6589114d":"import numpy as np\nimport pandas as pd\nimport re #regular expression \nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt \n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected = True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore') #ignore the warnings.\n\nfrom sklearn.ensemble import (RandomForestClassifier , AdaBoostClassifier , \n                             GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC # from support vector machine import support vector classifier\nfrom sklearn.model_selection import KFold","d06de28c":"# Load in the train and test datasets\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n","68772a57":"full_data = [train, test]\n\n# Some features of my own that I have added in\n# Gives the length of the name\ntrain['Name_length'] = train['Name'].apply(len)\ntest['Name_length'] = test['Name'].apply(len)\n# Feature that tells whether a passenger had a cabin on the Titanic\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\n# Feature engineering steps taken from Sina\n# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n# Create new feature IsAlone from FamilySize\nfor dataset in full_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n# Remove all NULLS in the Embarked column\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\nfor dataset in full_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n# Create a New feature CategoricalAge\nfor dataset in full_data:\n    age_avg = dataset['Age'].mean()\n    age_std = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)\n# Define function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n# Create a new feature Title, containing the titles of passenger names\nfor dataset in full_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n# Group all non-common titles into one single grouping \"Rare\"\nfor dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\nfor dataset in full_data:\n    # Mapping Sex\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n    \n    # Mapping titles\n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \n    # Mapping Embarked\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    \n    # Mapping Fare\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n    # Mapping Age\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4 ;","980d5f94":"# Feature selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], ax is = 1)\ntest  = test.drop(drop_elements, axis = 1)","023de951":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","efb50cc3":"g = sns.pairplot(train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked',\n       u'FamilySize', u'Title']], hue='Survived', palette = 'seismic',size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])","d3702a0d":"ntrain = train.shape[0] # gives total number of training datas\nntest = train.shape[0] #gives total number of testing data\nSEED = 0 # for same random distribution of data for reproducibility\nNFOLDS = 5 # to fold a record using kfold\nkf = KFold(ntrain , n_folds = NFOLDS , random_state = SEED)","4b749059":"class SKlearnHelper(object):\n    def __init__(self,clf,seed = 0 ,params = None):\n        params['random_state'] = seed\n        self.clf = clf(**params)#keyword arguments\n    def train(self, x_train , y_train):\n        self.clf.fit(x_train , y_train)\n    def predict(self , x):\n        return self.clf.predict(x)\n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n    def feature_importances(self,x,y):\n        print(self.clf.fit(x,y).feature_importances_)\n        ","ab3ea319":"def get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","2d380bf5":"np.empty((5))","2b8d9500":"#def get_oof(clf , x_train , y_train , x_test):\n    oof_train = np.zeros((ntrain ,)) #np.zeros converts into array of zeros , ntrain is the total number of records in the table.\n    oof_test = np.zeros((ntest ,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n    print(oof_train)","ab84ba99":"## Ensembling and Stacking module","49407e4a":"## FEATURE ENGINEERING RETHINK AND REBUILD","1f46a475":"##  **Titanic ML Prediction - First Kaggle Notebook**","2cd76b47":"**def init** : Python standard for invoking the default constructor for the class. This means that when you want to create an object (classifier), you have to give it the parameters of clf (what sklearn classifier you want), seed (random seed) and params (parameters for the classifiers).\n\nThe rest of the code are simply methods of the class which simply call the corresponding methods already existing within the sklearn classifiers. Essentially, we have created a wrapper class to extend the various Sklearn classifiers so that this should help us reduce having to write the same code over and over when we implement multiple learners to our stacker.","092c1624":"## ** Libraries Imported and it's real uses.**\n* sklearn -> It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. \n * re -> to import regular expression.\n * xgboost ->  XGBoost stands for eXtreme Gradient Boosting and is based on decision trees.It's main goal is to push the extreme of the computation limits of machines to provide a scalable, portable and accurate for large scale tree boosting.\n * seaborn -> Seaborn is a Python data visualization library with an emphasis on statistical plots. The library is an excellent resource for common regression and distribution plots, but where Seaborn really shines is in its ability to visualize many different features at once\n * matplotlib.pyplot -> Matplotlib is a Python plotting library which helps you to create visualization of the data in 2 -D graph. \n * %matplotlib inline -> It is an IPython-specific directive which causes IPython to display matplotlib plots in a notebook cell rather than in another window. To run the code as a script use.\n * plotly -> The plotly Python library (plotly.py) is an interactive, open-source plotting library that supports over 40 unique chart types covering a wide range of statistical, financial, geographic, scientific, and 3-dimensional use-cases.\n * plotly.offline -> No need to call the plotly server again and again for plotting.\n * plotly.graph_objs -> Import the graph objects and elements .Example - from plotly.graph_objs import Scatter, Layout, Data, Figure\n * sklearn.ensemble - The sklearn.ensemble module includes ensemble-based methods for classification, regression and anomaly detection.\n \n","95512b01":"## Build a predictive model which answers following:\n 1. what sorts of people were more likely to survive? \n *  Data :- (name, age, gender, socio-economic class, etc). \n * Total Passengers = 2224\n * Total Death toll = 1502","831869ab":"## Visualisations","ec764202":"##  Basic Terminologies\n\n * Ensemble - Including other models in a model for better predictions.","1f4460a0":"##  **INFORMATIONS**\n \n * AdaBoostClassifier -> AdaBoost is a type of \"Ensemble Learning\" where multiple learners are employed to build a stronger learning algorithm. AdaBoost works by choosing a base algorithm (e.g. decision trees) and iteratively improving it by accounting for the incorrectly classified examples in the training set.\n \n * Gradient BoostingClassifier -> Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees\n \n * ExtraTreeClassifier -> ExtraTreeClassifier is an extremely randomized version of DecisionTreeClassifier meant to be used internally as part of the ExtraTreesClassifier ensemble.\n \n * loc -> DataFrame.loc[] method is a method that takes only index labels and returns row or dataframe if the index label exists in the caller data frame.\n \n * pd.qcut -> The simplest use of qcut is to define the number of quantiles and let pandas figure out how to divide up the data.\n \n * pandas.crosstab -> Compute a simple cross tabulation of two (or more) factors. By default computes a frequency table of the factors unless an array of values and an aggregation function are passed.","c8bb055a":"## STATISTICAL INFORMATION\n\n **Standard deviation -> Standard deviation is a number used to tell how measurements for a group are spread out from the average (mean), or expected value. A low standard deviation means that most of the numbers are close to the average. A high standard deviation means that the numbers are more spread out.**","99f1ccc9":"**Out-of-Fold Predictions**\n*  stacking uses predictions of base classifiers as input for training to a second-level model.\n*   However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions.\n\n","026f0157":"## Pairplots","f9ffc2de":"## **LEARNINGS**\n * A dataset should be clean for machine learning and data analysis - we can use pandas .fillna to fill null values or missing values should be replaced for analysis."}}