{"cell_type":{"d91718b9":"code","b3e63ac0":"code","22e93a2e":"code","d320faed":"code","664cd6cc":"code","14be209b":"code","5d38533b":"code","dfaf8634":"code","92bc6d25":"code","c12c5c25":"code","38c42a36":"code","323fa638":"code","c9034318":"code","71f948b4":"code","2eb94ed8":"code","7c393d3c":"code","e2743ae0":"code","d116d6fc":"code","b898520a":"code","d0238cb8":"code","4b71c48e":"code","40159e4f":"code","d383c7a0":"code","a3cf7e56":"code","4d9cf012":"code","7bdc91e0":"code","796bc19e":"code","40f55a26":"code","c926ca72":"code","2a56e8a6":"code","d8a762cb":"code","eebd3372":"code","76530272":"code","ded7718d":"code","87d4630c":"code","58dcd550":"code","aeea4b14":"code","75607ce4":"markdown","14e3c6c8":"markdown","4f8b0983":"markdown","c4e270c4":"markdown","f0f1bbd0":"markdown","a9d394d4":"markdown","8cb12948":"markdown","afa50484":"markdown","ae2ff5e0":"markdown","503d3335":"markdown","5e49b531":"markdown","521e61ca":"markdown","2369672f":"markdown","0bcf8e9d":"markdown","1fdb8c3f":"markdown","8377346a":"markdown","cc2ea85d":"markdown","0834f63e":"markdown","2e3afd46":"markdown","1f163ece":"markdown"},"source":{"d91718b9":"!pip install pydotplus\n# !pip install six","b3e63ac0":"#import libs\nimport pandas as pd\npd.set_option(\"display.max_columns\", 101)\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.formula.api as smf\nfrom sklearn.model_selection import train_test_split\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.metrics import confusion_matrix ,classification_report,precision_score, recall_score ,f1_score, accuracy_score \n\n#decision tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.tree import export_graphviz\nfrom six import StringIO\nfrom IPython.display import Image  \nimport pydotplus","22e93a2e":"#loading data\ndf = pd.read_csv('..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')","d320faed":"#check data type\ndf.dtypes","664cd6cc":"#convert data type\ndf['TotalCharges'] = df['TotalCharges'].str.replace(\" \", \"0\", regex=False) #regex=False means input string is not a regex pattern\ndf['TotalCharges'] = df['TotalCharges'].astype('float64')\ndf['SeniorCitizen'] = df['SeniorCitizen'].astype('object')","14be209b":"#check for duplicate values - no duplicates\ndf['customerID'].duplicated().sum()","5d38533b":"#drop irrelevant columns - customerID\ndf.index = df['customerID']\ndf = df.drop(columns='customerID')","dfaf8634":"#check for missing values\ndf.isnull().sum()","92bc6d25":"#check outliers using mean and median\ndf.describe()","c12c5c25":"#check outliers using boxplot\nnumeric_col = ['tenure', 'MonthlyCharges', 'TotalCharges']\nfig, ax = plt.subplots(1,3, figsize=(16, 4)) \n\n#enumerate adds a counter to iterable\nfor n, each_col in enumerate(numeric_col):\n    ax[n].boxplot(df[each_col]) \n    ax[n].set_title(each_col)","38c42a36":"numerical_col = []\ncat_col = []\nfor each_feature in df.columns:\n    if df[each_feature].dtypes == 'int64' or df[each_feature].dtypes == 'float64':\n        numerical_col.append(each_feature)\n    else:\n        cat_col.append(each_feature)\n        \nprint(numerical_col)\nprint(cat_col)","323fa638":"fig = plt.figure (figsize=(16, 18))\nfor y, each_feature in enumerate(cat_col[1:], start=1): #counter starts at 1, required by add_subplot\n    x = fig.add_subplot(5,4,y) #5by4 plot, ax1 is subplot at position y\n    x.pie(df.groupby(each_feature).size(), labels=df[each_feature].unique(), autopct=lambda p: '{:.0f}%'.format(p)) #autopct is the label for piechart, requires function\n    x.set_title(each_feature) #set title for each subplot\n","c9034318":"#numerical variables\nfig, ax = plt.subplots(1,3, figsize=(16, 6)) \nfor n, each_feature in enumerate(numeric_col):\n    box_plot = sns.boxplot(ax=ax[n], y=each_feature, x = 'Churn', data=df)\n    \n    medians = round(df.groupby(df['Churn'])[each_feature].median(),1)\n    vertical_offset = df[each_feature].median() * 0.05 # offset from median for display\n\n    for xtick in box_plot.get_xticks():\n        box_plot.text(xtick,medians[xtick] + vertical_offset,medians[xtick], \n                horizontalalignment='center',size='x-small',color='w',weight='semibold')","71f948b4":"#perform encoding for analysis\nfor each in cat_col:\n    df[each] = df[each].astype('category')\n    df[each] = df[each].cat.codes\n    \ndf.head()","2eb94ed8":"#split data into test and train\nx = df.iloc[:,0:-1]\ny = df.iloc[:,-1]\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=4)","7c393d3c":"from imblearn.over_sampling import SMOTENC\n\nsm = SMOTENC(categorical_features=[0,16], random_state = 2)\nx_train_sm, y_train_sm = sm.fit_resample(x_train, y_train)\n\n# what we had before\nprint(\"what we had before\")\nprint(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n\n# what we have now - an increase in training data, does not affect the test data\nprint(\"\\nwhat we have now after oversampling\")\nprint(x_train_sm.shape, y_train_sm.shape, x_test.shape, y_test.shape)\n\n#class distribution\nprint(\"\\nnew class distribution\")\ny_train_sm.value_counts()","e2743ae0":"sns.regplot(x= 'MonthlyCharges', y= 'Churn', data= df, logistic= True).set_title(\"Monthly Charges Log Odds Linear Plot\")","d116d6fc":"sns.regplot(x= 'TotalCharges', y= 'Churn', data= df, logistic= True).set_title(\"Total Charges Log Odds Linear Plot\")","b898520a":"sm_df = x.corr(method='spearman')\nsm_df.style.applymap(lambda x: \"background-color: red\" if x>0.7 else \"background-color: white\")","d0238cb8":"# VIF dataframe\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = x.columns\n  \n# calculating VIF for each feature\nvif_data[\"VIF\"] = [variance_inflation_factor(x.values, i)\n                          for i in range(len(x.columns))]\n  \nprint(vif_data)","4b71c48e":"#drop columns with high multicollinearity\ndf = df.drop(columns=['MonthlyCharges','TotalCharges', 'PhoneService', 'tenure'])","40159e4f":"output = x.columns[0]\nfor each in x.columns[1:]:\n    output +=  \" + \" + each \nformula = 'Churn~ ' + output","d383c7a0":"#full dateset\nmodel= smf.logit(formula=formula, data= x_train.join(y_train)).fit()\nmodel.summary()","a3cf7e56":"ypred = model.predict(x_test)\nypred\nprediction = list(map(round, ypred))\n\n# confusion matrix\ncm = confusion_matrix(y_test, prediction)\nlabels=[0, 1]\ndf_cm = pd.DataFrame(cm, labels, labels)\nax = sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 10}, square=True, cbar=False, fmt='g')\nax.set_ylim(0, 2) \nplt.xlabel(\"Predicted\") \nplt.ylabel(\"Actual\") \nax.invert_yaxis() #optional\nplt.show()\n \n# accuracy score of the model\nprint('Test accuracy = ', accuracy_score(y_test, prediction))","4d9cf012":"# Evaluating the predictive capability of the classification model\n\nprint(' precision score: ',precision_score(y_test, prediction),'\\n')\nprint(' recall score: ',recall_score(y_test, prediction),'\\n')\nprint(classification_report(y_test, prediction))","7bdc91e0":"#im dateset\nmodel= smf.logit(formula=formula, data= x_train_sm.join(y_train_sm)).fit()\nmodel.summary()","796bc19e":"ypred = model.predict(x_test)\nypred\nprediction = list(map(round, ypred))\n\n# confusion matrix\ncm = confusion_matrix(y_test, prediction)\nlabels=[0, 1]\ndf_cm = pd.DataFrame(cm, labels, labels)\nax = sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 10}, square=True, cbar=False, fmt='g')\nax.set_ylim(0, 2) \nplt.xlabel(\"Predicted\") \nplt.ylabel(\"Actual\") \nax.invert_yaxis() #optional\nplt.show()\n \n# accuracy score of the model\nprint('Test accuracy = ', accuracy_score(y_test, prediction))","40f55a26":"# Evaluating the predictive capability of the classification model\n\nprint(' precision score: ',precision_score(y_test, prediction),'\\n')\nprint(' recall score: ',recall_score(y_test, prediction),'\\n')\nprint(classification_report(y_test, prediction))","c926ca72":"clf = DecisionTreeClassifier(random_state=0)\nclf = clf.fit(x_train_sm,y_train_sm)\ny_pred = clf.predict(x_test)","2a56e8a6":"# confusion matrix\ncm = confusion_matrix(y_test, prediction)\nlabels=[0, 1]\ndf_cm = pd.DataFrame(cm, labels, labels)\nax = sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 10}, square=True, cbar=False, fmt='g')\nax.set_ylim(0, 2) \nplt.xlabel(\"Predicted\") \nplt.ylabel(\"Actual\") \nax.invert_yaxis() #optional\nplt.show()\n \n# accuracy score of the model\nprint('Test accuracy = ', accuracy_score(y_test, y_pred))","d8a762cb":"# Evaluating the predictive capability of the classification model\n\nprint(' precision score: ',precision_score(y_test, y_pred),'\\n')\nprint(' recall score: ',recall_score(y_test, y_pred),'\\n')\nprint(classification_report(y_test, y_pred))","eebd3372":"values = [i for i in range(1, 21)]\ntrain_scores, test_scores = list(), list()\n\n# evaluate a decision tree for each depth\nfor i in values:\n    clf = DecisionTreeClassifier(max_depth=i)\n    clf = clf.fit(x_train, y_train)\n    \n    # evaluate on the train dataset\n    train_ypred = clf.predict(x_train)\n    train_acc = metrics.accuracy_score(y_train, train_ypred)\n    train_scores.append(train_acc)\n    \n    # evaluate on the test dataset\n    test_ypred = clf.predict(x_test)\n    test_acc = metrics.accuracy_score(y_test, test_ypred)\n    test_scores.append(test_acc)\n\n# plot of train and test scores vs tree depth\nfig, ax = plt.subplots(figsize=(12, 12))\nax.plot(values, train_scores, '-o', label='Train')\nax.plot(values, test_scores, '-o', label='Test')\nax.set_xticks([x for x in range(1,22)])\nax.legend()\nplt.show()","76530272":"clf = DecisionTreeClassifier(random_state=0, max_depth = 3)\nclf = clf.fit(x_train_sm,y_train_sm)\ny_pred = clf.predict(x_test)","ded7718d":"# confusion matrix\ncm = confusion_matrix(y_test, prediction)\nlabels=[0, 1]\ndf_cm = pd.DataFrame(cm, labels, labels)\nax = sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 10}, square=True, cbar=False, fmt='g')\nax.set_ylim(0, 2) \nplt.xlabel(\"Predicted\") \nplt.ylabel(\"Actual\") \nax.invert_yaxis() #optional\nplt.show()\n \n# accuracy score of the model\nprint('Test accuracy = ', accuracy_score(y_test, y_pred))","87d4630c":"# Evaluating the predictive capability of the classification model\n\nprint(' precision score: ',precision_score(y_test, y_pred),'\\n')\nprint(' recall score: ',recall_score(y_test, y_pred),'\\n')\nprint(classification_report(y_test, y_pred))","58dcd550":"dot_data = StringIO()\nexport_graphviz(clf, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True,feature_names = x.columns ,class_names=['0','1'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('DecisionTree.png')\nImage(graph.create_png())","aeea4b14":"importances = clf.feature_importances_\nweights = pd.Series(importances,\n                 index=x.columns.values)\nweights.sort_values()[-10:].plot(kind = 'barh')","75607ce4":"**Model 1A - Logisitic Regression Model trained on original dataset**","14e3c6c8":"# **1. Introduction**\n\nCustomer churn (AKA Customer attrition) occurs when a customer stops using a business's products or services. Customer retention is critical to the business as it is often more costly to acquire new customers compared to retaining existing ones. As such, predicting customer churn will help in early detection of customers who are at risks of leaving a service\/product, allowing the business to take proactive measures in prioritising retention efforts with these customers. ","4f8b0983":"### 3A. Understand data distribution for each categorical variable\n- In terms of demographic, this dataset is mainly made up customers who are young and has no dependents. \n- As for the firm's core services, 90% of the customers are subscribed to the firm's phone service. Out of these customers, majority of them own multiple lines. 77% of customers have subscribed to the firm's Internet service and out of these customers, 44% of customers are Fibre Optic users and 34% are DSL users.\n- On average, across all the add-on service , 78% of customers do not subscribe or do not need add-on services such as Online Security, Online Backup, Device Protection, Tech Support, StreamingTV and StreamingMovies \n- Month-to-month contracts are more popular than 1-year and 2-year contracts\n- This dataset has an unequal class distribution. 73% of the data represents existing customers while only 27% of the data represents churned customer. This could create an imbalanced classification issue where it will be more challeneging for the model to learn the characterisitics of the minority class (churned customers).","c4e270c4":"### 4D. Model #2 - Decision Tree\nDecision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. ","f0f1bbd0":"### 4C. Model #1 - Logistic Regression\nLogistic regression is a classification algorithm that predicts a binary outcome (churn vs non-churn). The dependent variable is categorical while the independent variables can be continuous or categorical. ","a9d394d4":"### 2B. Check for outliers\nAs seen from the summary stats table and boxplot below:\n- tenure is positively skewed as mean is greater than median monthly charges\n- MonthlyCharges is negatively skewed as mean is lower than median monthly charges\n- TotalCharges is positively skewed as mean is greater than median total charges\n- Based on the IQR formula, there are no outliers","8cb12948":"**Model Discussion**\n\n- Although the first model (1A) produced a high accuracy score of 0.783, it has a low precision and recall scores at 0.594 and 0.524 respectively. The model performed well in terms of accuracy due to the accracy paradox - given that majority of the test data are of the majority class, a model that predicts the majority class for all the test data will have a high number of correct predictions. This results in a misleading accuracy score. \n- As such, I trained a second model (1B) with a balanced data set to evaluate if this would improve the recall and precision. Although the accuracy score decreased slightly to 0.746, the recall score has improved to 0.715. However, the precision score decreased to 0.508. This is the trade-off for correcting the dataset. As the model is now exposed to more data from the minority class (churn), more of its prediction now leans towards the minority class, resulting in higher false positives and thus, a lower precision. \n- Given both models, I would pick the second model (1B) as it has a higher recall. In this customer churn prediction case, false positives are less costly compared to false negatives. In a false positive case, the firm would proactively engage a non-churn customer as the model has wrongly predicted that the customer would churn. In a false negative case, the firm would neglect a churning customer as the model has wrongly predicted that the customer would not churn. The latter would cause greater financial loss to the firm as it is more expensive to acquire a new customer than to retain one. \n\n**Intepreting model output  [4]**\n- StreamingMovies, PaperlessBilling and gender have no significant effect on the log odds of customers churning. (p-value>0.05)\n- The remaining independent variables have a significant effect on the log odds of customer churning (p-value<0.05)\n- Customers subscribed to phone service has a -1.43 decrease in log odds of churning compared to customers who are not subscribed to phone service\n- Customers who have dependents has a -0.67 decrease in log odds of churning compared to customers who do not have dependents\n- Senior citizen customers have a -0.61 decrease in log odds of churning compared to customers who are younger.\n","afa50484":"# 4. Classification Models","ae2ff5e0":"# 3. Exploratory Data Analysis","503d3335":"**Verifying Assumption #1: The independent variables are linearly related to the log odds**\n\nLogistic Regression assumes that the continuous dependent variables are linearly related to the log odds of the dependent variable. To verify this assumption, I have plot the continuous dependent predictor - monthly charges and total charges to the log odds of the dependent variable. We would expect a flat top and bottom with a increasing\/decreasing middle. Indeed, we have validated the assumption that the 2 continuous predictor variables are linearly related to the log odds of the dependent variable.","5e49b531":"**Model 1B - Logisitic Regression Model trained on dataset corrected for imbalanced class**","521e61ca":"**Fine-tuning Decision Tree [5]**\n\nIn general, the more complex our tree is, the more information it captures. However, this often leads to overfitting whereby the model will perform very well on the training data set but it will not be able to generalize well on the test data set. On the flip side, an overly simplified model will underfit as it is unable to learn meaningful patterns from the data and will perform badly on both train and test data set. \n\nThere are several hyper-parameters that affect tree complexity:\n- stopping criteria: max_depth, min_samples_split, and min_samples_leaf \n- pruning methods: min_weight_fraction_leaf and min_impurity_decrease\n\nHere, I have decided to experiment with max_depth. As seen from the line chart below, a max_depth==3 gives the best score for both test and training data set. There are signs of overfitting for trees with max_depth above 7. The model's accuracy for train data set continue to increase while the model's accuracy for test data set has decreased.","2369672f":"### 4A. Prepare data for analysis","0bcf8e9d":"### 2A. Check data type, missing values and duplicates\nAs TotalCharges is a float and SeniorCitizen is a dummy variable, I have corrected the data type.","1fdb8c3f":"# 5. Comparing model performance\n- Overall, the logistic regression model performs better compared to decision tree model. The logistic regression model gave a better accuracy score of 0.746 and macro-F1 score of 0.79. In comparison, the decision tree model produced a lower accuracy score of 0.689 and macro-F1 score of 0.66. \n\n#### Key recommedations:\n- The InternetService type has a 0.2851 increase in log odds ratio of customer churning. I would recommend the firm to conduct a survey on satisfaction with Internet Service to understand if there might be any underlying issues with the InternetService that may cause customers to churn.\n- As younger customers with no dependents and no partners are more likely to churn, I would recommend the firm to create bundle family packages with their older family members \/ friends. \n- For every one unit increase in monthly charges, the logg odds of customer churning increase by 0.029. Based on the EDA findings, churn customers have a higher median monthly charge compared to non-churn customers. As such, I would recommend the firm to investigate the underlying the possible causes of higher median monthly charge and take steps to reduce the cost to prevent losing these customers to other competitors. ","8377346a":"**Verifying Assumption #2: There is little to no multicollinearity between the independent variables**\n\nLogistic Regression also assumes little to no multicollinearity. I have tested this by looking at the correlation between independent variables with Spearman and computed the variance inflation factor (VIF) score. In general, correlation > 0.7 and  VIF>10 indicates presence of multi-collinearity. As seen below, TotalCharges, Tenure, Phone Service and MonthlyCharges are at high risk of multi-collinearity and are removed from the dataset [1, 2]. ","cc2ea85d":"### 3B. Understand relationship between numerical independent variable with dependent variable\n- Churn customers have a shorter median tenure of 10 months, compared to non-churn customers who have a longer median tenure of 38 months\n- Churn customers have a higher median monthly charge bill of \\\\$79.6 compared to non-churn customers who have a lower median monthly bill of \\\\$64.4.\n- Yet, churn customers have a lower median total charge bill of \\\\$703.6 compared to non-churn customers who have a higher median total bill of \\\\$1079.5. This seems to contradict the findings from the monthly charge bill. A possible reason could be that churn customers have shorter tenure where they churn prematurely before the end of the fiscal period, resulting in lower total charges.","0834f63e":"### 4B. Correcting imbalanced class distribution\nSince it was detected that the dataset has unequal class distribution, I have performed oversampling to duplicate samples from the minority class to create equally distributed classes in the training dataset. ","2e3afd46":"# **2. Data Processing**\nThe overall data quality is quite good. There are no duplicates and outliers. There is only a handful of missing values for TotalCharges which was replaced with 0.","1f163ece":"# References:\n* [1] https:\/\/blog.clairvoyantsoft.com\/correlation-and-collinearity-how-they-can-make-or-break-a-model-9135fbe6936a#:~:text=Multicollinearity%20is%20a%20situation%20where,indicates%20the%20presence%20of%20multicollinearity.\n* [2] https:\/\/quantifyinghealth.com\/vif-threshold\/\n* [3] https:\/\/www.geeksforgeeks.org\/logistic-regression-using-statsmodels\/\n* [4] https:\/\/pythonfordatascienceorg.wordpress.com\/logistic-regression-python\/\n* [5] https:\/\/towardsdatascience.com\/how-to-tune-a-decision-tree-f03721801680"}}