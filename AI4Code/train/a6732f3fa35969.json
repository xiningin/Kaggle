{"cell_type":{"684ea2ff":"code","6bbd97e6":"code","3f2b2f48":"code","cd1d8e00":"code","61bae602":"code","9bc51d78":"code","20e3215e":"code","3e8e4770":"code","c3344a8e":"code","c467261d":"code","beb2e687":"code","72e4eb9b":"code","c12a54ad":"code","f043b02e":"code","c21bd7cc":"code","8719be72":"code","425717cc":"code","00a29c34":"code","14f502c2":"code","7a8f2cde":"code","40734f95":"code","36cdb44a":"code","7c1200bf":"code","89f1dc40":"code","277b7db0":"code","7309ca02":"code","33d8df99":"code","343a29ea":"code","d279a7eb":"code","633eb202":"code","b9cb4f00":"code","9dfd0dd7":"code","1c43fe2f":"code","be164196":"code","045c9436":"code","f8329c17":"code","19b5a3fb":"code","9ac4cdbb":"code","b62dfd21":"code","b7844cde":"markdown","8b3129cc":"markdown","fcd1796c":"markdown","1c11c22a":"markdown","04e6dbc8":"markdown","24767447":"markdown","67ea08c9":"markdown","f6cd9b5e":"markdown","092fa25c":"markdown","455e0423":"markdown","b6b9d2b3":"markdown","344c0497":"markdown","ef53f39d":"markdown","1c5befef":"markdown","72b54eb2":"markdown","5efded23":"markdown","d6c0a198":"markdown","17e888a9":"markdown","0e9b0341":"markdown","69ab3ed7":"markdown","b50873e6":"markdown","ba03d0ab":"markdown","a4705e90":"markdown","846c2642":"markdown","acf402f8":"markdown","c57cf6aa":"markdown","09b48def":"markdown","26436d0d":"markdown","78ad547a":"markdown"},"source":{"684ea2ff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6bbd97e6":"df=pd.read_csv(\"\/kaggle\/input\/iris\/Iris.csv\").drop('Id',axis=1)","3f2b2f48":"df.head()","cd1d8e00":"df.info()","61bae602":"df.Species.value_counts()","9bc51d78":"import matplotlib.pyplot as plt\n\ndf.hist(figsize=(8,8))\nplt.show()","20e3215e":"import seaborn as sns\n\nfig,axs=plt.subplots(3,3,figsize=(12,12))\n\nfor i in [1,2,3]:\n    sns.scatterplot(x=df.iloc[:,0],y=df.iloc[:,i],hue=df.Species,ax=axs[0,i-1])\n    \nfor i in [2,3]:\n        sns.scatterplot(x=df.iloc[:,1],y=df.iloc[:,i],hue=df.Species,ax=axs[1,i-2])\n        \nsns.scatterplot(x=df.iloc[:,2],y=df.iloc[:,3],hue=df.Species,ax=axs[2,0])","3e8e4770":"from sklearn.preprocessing import LabelEncoder\n\ndf.Species=LabelEncoder().fit_transform(df.Species)","c3344a8e":"from pandas.plotting import scatter_matrix\n\nscatter_matrix(df,figsize=(12,12))\nplt.show()","c467261d":"sns.heatmap(df.corr(),annot=True)","beb2e687":"df.skew()","72e4eb9b":"X=df.iloc[:,:4]\n\nfrom sklearn.preprocessing import MinMaxScaler\ndf_mm=MinMaxScaler().fit_transform(X)\n\nfrom sklearn.preprocessing import StandardScaler\ndf_ss=StandardScaler().fit_transform(X)\n\nfrom sklearn.preprocessing import Normalizer\ndf_n=Normalizer().fit_transform(X)\n\n\nfig,ax=plt.subplots(2,2,figsize=(8,8))\nax[0,0].boxplot(X), ax[0,0].set_title(\"Original\")\nax[0,1].boxplot(df_mm), ax[0,1].set_title(\"MinMaxScaler\")\nax[1,0].boxplot(df_ss), ax[1,0].set_title(\"StandardScaler\")\nax[1,1].boxplot(df_n), ax[1,1].set_title(\"Normalier\")\n\nplt.show()","c12a54ad":"from sklearn.model_selection import train_test_split\n\ntrain,val=train_test_split(df)\nX,y=train.drop('Species',axis=1),train.Species\nX_train,X_test,y_train,y_test=train_test_split(X,y)","f043b02e":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC","c21bd7cc":"models = []\nmodels.append(('LR', LogisticRegression(solver='liblinear')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC(gamma='auto')))","8719be72":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=0, shuffle=True)\n    cv_results = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","425717cc":"from sklearn.metrics import confusion_matrix\n\nfor name, model in models:\n    model.fit(X_train,y_train)\n    predicted=model.predict(X_test)\n    matrix=confusion_matrix(predicted,y_test)\n    print(name)\n    print(matrix)","00a29c34":"from sklearn.pipeline import Pipeline\n    \nscalers=[]\nscalers.append(('None',None))\nscalers.append(('MinMaxScaler',MinMaxScaler()))\nscalers.append(('StandardScaler',StandardScaler()))\nscalers.append(('Normalizer',Normalizer()))\n\nresults=pd.DataFrame()\n\nfor name,scaler in scalers:\n    kfold = KFold(n_splits=10, random_state=0, shuffle=True)\n    model=LogisticRegression(solver='liblinear')\n    pipe=Pipeline([('scaler',scaler),('model',model)])\n    cv_results = cross_val_score(pipe, X, y, cv=kfold, scoring='accuracy')\n    results[name]=cv_results\nprint(results.mean())\n\nresults.mean().plot(kind='barh')\nplt.title(\"Logistic Regression\")\nplt.show()","14f502c2":"results=pd.DataFrame()\n\nfor name,scaler in scalers:\n    kfold = KFold(n_splits=10, random_state=0, shuffle=True)\n    model=LinearDiscriminantAnalysis()\n    pipe=Pipeline([('scaler',scaler),('model',model)])\n    cv_results = cross_val_score(pipe, X, y, cv=kfold, scoring='accuracy')\n    results[name]=cv_results\nprint(results.mean())\n\nresults.mean().plot(kind='barh')\nplt.title(\"Linear Discriminant Analysis\")\nplt.show()","7a8f2cde":"results=pd.DataFrame()\n\nfor name,scaler in scalers:\n    kfold = KFold(n_splits=10, random_state=0, shuffle=True)\n    model=KNeighborsClassifier()\n    pipe=Pipeline([('scaler',scaler),('model',model)])\n    cv_results = cross_val_score(pipe, X, y, cv=kfold, scoring='accuracy')\n    results[name]=cv_results\nprint(results.mean())\n\nresults.mean().plot(kind='barh')\nplt.title(\"KNeighbors Classifier\")\nplt.show()","40734f95":"results=pd.DataFrame()\n\nfor name,scaler in scalers:\n    kfold = KFold(n_splits=10, random_state=0, shuffle=True)\n    model=DecisionTreeClassifier()\n    pipe=Pipeline([('scaler',scaler),('model',model)])\n    cv_results = cross_val_score(pipe, X, y, cv=kfold, scoring='accuracy')\n    results[name]=cv_results\nprint(results.mean())\n\nresults.mean().plot(kind='barh')\nplt.title(\"DecisionTreeClassifier\")\nplt.show()","36cdb44a":"results=pd.DataFrame()\n\nfor name,scaler in scalers:\n    kfold = KFold(n_splits=10, random_state=0, shuffle=True)\n    model=GaussianNB()\n    pipe=Pipeline([('scaler',scaler),('model',model)])\n    cv_results = cross_val_score(pipe, X, y, cv=kfold, scoring='accuracy')\n    results[name]=cv_results\nprint(results.mean())\n\nresults.mean().plot(kind='barh')\nplt.title(\"GaussianNB\")\nplt.show()","7c1200bf":"results=pd.DataFrame()\n\nfor name,scaler in scalers:\n    kfold = KFold(n_splits=10, random_state=0, shuffle=True)\n    model=SVC(gamma='auto')\n    pipe=Pipeline([('scaler',scaler),('model',model)])\n    cv_results = cross_val_score(pipe, X, y, cv=kfold, scoring='accuracy')\n    results[name]=cv_results\nprint(results.mean())\n\nresults.mean().plot(kind='barh')\nplt.title(\"SVC\")\nplt.show()","89f1dc40":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif","277b7db0":"skb = SelectKBest(score_func=f_classif, k=4)\nfit=skb.fit(X,y)","7309ca02":"fit.scores_","33d8df99":"pd.DataFrame(data=fit.scores_,index=X.columns,columns=['Scores']).sort_values(by='Scores')","343a29ea":"pd.DataFrame(data=fit.scores_,index=X.columns,columns=['Scores']).sort_values(by='Scores').plot(kind='barh')\nplt.show()","d279a7eb":"X_train2=X_train.iloc[:,2:4]\nX_test2=X_test.iloc[:,2:4]","633eb202":"from sklearn.metrics import confusion_matrix\n\nfor name, model in models:\n    model.fit(X_train2,y_train)\n    predicted=model.predict(X_test2)\n    matrix=confusion_matrix(predicted,y_test)\n    print(name)\n    print(matrix)","b9cb4f00":"from sklearn.feature_selection import RFE\nrfe=RFE(estimator=DecisionTreeClassifier(), n_features_to_select=2)\n\nfor name, model in models:\n    pipe=Pipeline([('rfe',rfe),('model',model)])\n    pipe.fit(X_train,y_train)\n    predicted=pipe.predict(X_test)\n    matrix=confusion_matrix(predicted,y_test)\n    print(name)\n    print(matrix)","9dfd0dd7":"pipe.named_steps['rfe'].ranking_","1c43fe2f":"rfe=RFE(estimator=DecisionTreeClassifier(), n_features_to_select=3)\n\nfor name, model in models:\n    pipe=Pipeline([('rfe',rfe),('model',model)])\n    pipe.fit(X_train,y_train)\n    predicted=pipe.predict(X_test)\n    matrix=confusion_matrix(predicted,y_test)\n    print(name)\n    print(matrix)","be164196":"pipe.named_steps['rfe'].ranking_","045c9436":"from sklearn.decomposition import PCA\n\npca=PCA(n_components=2)\n\nfor name, model in models:\n    pipe=Pipeline([('pca',pca),('model',model)])\n    pipe.fit(X_train,y_train)\n    predicted=pipe.predict(X_test)\n    matrix=confusion_matrix(predicted,y_test)\n    print(name)\n    print(matrix)","f8329c17":"pca=PCA(n_components=3)\n\nfor name, model in models:\n    pipe=Pipeline([('pca',pca),('model',model)])\n    pipe.fit(X_train,y_train)\n    predicted=pipe.predict(X_test)\n    matrix=confusion_matrix(predicted,y_test)\n    print(name)\n    print(matrix)","19b5a3fb":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier","9ac4cdbb":"ensembles = []\nensembles.append(('AB', AdaBoostClassifier()))\nensembles.append(('GBM', GradientBoostingClassifier()))\nensembles.append(('RF', RandomForestClassifier(n_estimators=10)))\nensembles.append(('ET', ExtraTreesClassifier(n_estimators=10)))","b62dfd21":"for name, model in ensembles:\n    model.fit(X_train,y_train)\n    predicted=model.predict(X_test)\n    matrix=confusion_matrix(predicted,y_test)\n    print(name)\n    print(matrix)","b7844cde":"We see that Logistic Regression performed worse while others remained the same ","8b3129cc":"# Logistic Regression ","fcd1796c":"# Recursive feature elimination (2 features)","1c11c22a":"# Linear Discriminant Analysis","04e6dbc8":"# KNeighborsClassifier","24767447":"# Modeling","67ea08c9":"# Feature Selection","f6cd9b5e":"# Ensemble","092fa25c":"# Scatter matrix","455e0423":"# Correlation","b6b9d2b3":"# Confusion Matrix for two most important features","344c0497":"Data is perfectly balanced ","ef53f39d":"# Confusion Matrix","1c5befef":"# Scatterplot for each pair of features","72b54eb2":"# SVC","5efded23":"# Skew","d6c0a198":"# Survey of scaling techniques ","17e888a9":"# Create training, testing, and validation sets","0e9b0341":"# GaussianNB","69ab3ed7":"# Class distribution ","b50873e6":"# Survey of scaling techniques","ba03d0ab":"# DecisionTreeClassifier","a4705e90":"# Accuracy","846c2642":"# Histplot","acf402f8":"# SelectKBest","c57cf6aa":"# Principal Component Analysis (2 components)","09b48def":"# Principal Component Analysis (3 components)","26436d0d":"# LabelEncoder","78ad547a":"# Recursive feature elimination (3 features)"}}