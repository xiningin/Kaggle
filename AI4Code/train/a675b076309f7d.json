{"cell_type":{"5c2fe1ab":"code","e6891d25":"code","4f36e088":"code","c928c363":"code","f54fc819":"code","7cda5148":"code","08c0561d":"code","b92e9497":"code","5dad32af":"code","2d46dd12":"code","1c4df0a1":"code","0fe7479e":"code","8e066403":"code","9afb5397":"code","f6ca2d9e":"code","85b1df29":"markdown","0c21e562":"markdown","9cb5d5d9":"markdown","1798d3f8":"markdown","5a9b27ec":"markdown","2258be26":"markdown","027afc68":"markdown","9c033c20":"markdown","c26582f3":"markdown","27dde19f":"markdown","21e8302f":"markdown","213e8142":"markdown","cf4164f5":"markdown","2a3db9ec":"markdown","f5461f51":"markdown","5502d30e":"markdown","3676059b":"markdown","317ac2f1":"markdown","83977fae":"markdown"},"source":{"5c2fe1ab":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e6891d25":"df =pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")\ndf.head()","4f36e088":"data_count = df['Class'].value_counts()\nprint(data_count)","c928c363":"inputs = df.drop(\"Class\", axis=\"columns\")\ntargets = df.Class\nx_train, x_test, y_train, y_test = train_test_split(\n    inputs, targets, test_size=0.2, random_state=10)","f54fc819":"clf = LogisticRegression(max_iter = 10000).fit(x_train,y_train)\ny_predicted = clf.predict(x_test)\naccuracy = clf.score(x_test, y_test)\nprint(accuracy)","7cda5148":"cm = metrics.confusion_matrix(y_test, y_predicted)\nprint(cm)","08c0561d":"sns.heatmap(cm, annot=True, cmap='Blues_r' ,fmt='d')\nplt.ylabel(\"Actual Values\")\nplt.xlabel(\"Predicted Values\")\nplt.show()","b92e9497":"train_data = pd.concat([x_train, y_train], axis=1)\ntrain_data.head()","5dad32af":"legit_train = train_data[train_data.Class == 0]\nfraud_train = train_data[train_data.Class == 1]","2d46dd12":"print(legit_train.shape, fraud_train.shape)","1c4df0a1":"legit_train = legit_train.sample(n=398)\nlegit_train.shape","0fe7479e":"train_data = pd.concat([legit_train, fraud_train], axis=0)\ntrain_data.shape","8e066403":"x_train_new = train_data.drop('Class', axis='columns')\ny_train_new = train_data.Class","9afb5397":"clf = LogisticRegression(max_iter=10000).fit(x_train_new, y_train_new)\ny_predicted = clf.predict(x_test)\naccuracy = clf.score(x_test, y_test)\nprint(accuracy)","f6ca2d9e":"cm = metrics.confusion_matrix(y_test, y_predicted)\nsns.heatmap(cm, annot=True, cmap='Blues_r', fmt='d')\nplt.ylabel(\"Actual Values\")\nplt.xlabel(\"Predicted Values\")\nplt.show()","85b1df29":"Now that we have same number of fraud and legit transactions, let's join them back to recreate a complete balanced training dataset. ","0c21e562":"Split the training and test data set. ","9cb5d5d9":"We don't have to bother with the test dataset because it will stay the same. So, we will just fit the model with new x_train_new, y_train_new and old x_test, y_test.","1798d3f8":"From the confusion matrix, we see that 85 fraud transactions (out of 94, which makes it 90.4% ) were accurately classified as fraud. And 54781 legit transactions (out of 56868, which makes it 96.3%) were accurately classified as legit. \nOnly 9 fraud transactions were classified as legit transactions. This result shows that even though we got less accuracy, this model is better at predicting the fraud transactions. \n\n# Conclusion\n\nIn real world, most of the data are always imbalanced. For example, google ads click through rate, faulty products in an assembly line, spam emails etc. If we don't do anything in such unbalanced dataset, the model tends to be biased towards majority class, whatever that is. And that is a problem because we are trying to predict the minority class, not the majority. So, the biasness of the model towards majority class must be addressed. We do this by undersampling the majority class (decreasing the size of majority class) or oversampling the minority class (increasing the size of minority class).      \n\nFrom above results, we see that even though we get high accuracy without balancing, that accuracy shows how great it is at predicting legit transactions and how bad it is at predicting fraud transactions. But we always need to predict the fraud transactions or spams or faulty products which are always a minority in these kinds of dataset. \n\nAnd that is why balancing is necessary. ","5a9b27ec":"Fraud transaction is represented as 1 and actual as 0. We see that the data is highly imbalanced. Out of whole data, only 492 (0.1730%) are fraud transactions. This is what we call a minority class. The legit transactions are the majority class. ","2258be26":"# Objective\n\nThe objective of this notebook is to find out why balancing is necessary in an unbalanced dataset. First we will see the accuracy without balancing the data and later we will do same with the balanced data and compare the accuracy. ","027afc68":"# Balancing with Undersampling\n\nWe will balance the data by undersampling the legit transactions in the training dataset. We will not balance the test data. The imbalance must be preserved in the test set because the real world data is imbalanced. It doesn't make sense to test on the balanced data if we are eventually going to use the model for real world imbalanced data. \n\nFirst, we will recreate the training data from x_train and y_train to separate legit and fraud transactions. ","9c033c20":"We got an accuracy of 96.32%. Not bad. Let's check the confusion matrix. ","c26582f3":"Let's see it better with matplotlib and seaborn. ","27dde19f":"# Majority and Minority Classes\n\nLet's check how imbalanced the data is. ","21e8302f":"# Confusion Matrix\n\nWe got 99.8% accuracy without doing any balancing. Why is it then necessary to balance ? That will be answered by a confusion matrix. ","213e8142":"Check how many frauds we got so that we can undersample legit transactions to that number.","cf4164f5":"# Random Undersampling\n\nSo, we see that there are 227447 legit transactions and 398 fraud transactions in our training data set. We will balance this dataset by randomly selecting 398 legit transactions out of 227447. This is called random undersampling. ","2a3db9ec":"Now, let's separate features and targets from the new training dataset. ","f5461f51":"Loading the data.","5502d30e":"Fit the model and check the accuracy. ","3676059b":"Separate legit and fraud transactions. ","317ac2f1":"# Let's Not Balance","83977fae":"That's better. Here we see that 36 of fraud transactions (out of 94) have been classified as legit (False Negative). But only 29 of legit transactions (out of 56868) have been classified as fraud(False positive).\nOr in other words,only 58 fraud transactions(out of 94, which makes it 61.7%) were accurately classified as fraud. But 56839 legit transactions (out of 56868, which makes it 99.94%) were accurately classified as legit.  \nWhich means that our model is doing well to predict the legit transactions but not so well to predict the fraud transactions. Which means that 99.8% accuracy is pretty much the accuracy to predict legit transactions, not fraud. But that's not what we are trying to achieve.  "}}