{"cell_type":{"7ba62f67":"code","9bb1204e":"code","2dcb79a5":"code","fa493353":"code","5c87ade2":"code","618a9490":"code","89cf1d29":"code","67e521bc":"code","8b398812":"code","262a90fb":"code","1a15e4c6":"code","73896184":"code","2aa3dcfb":"code","e2742ac4":"code","8d07f2aa":"code","b5846a57":"code","85c8011d":"code","82b0bf5d":"code","7cdf373c":"code","93f1a579":"code","f1ac4233":"code","7e77928d":"code","af2c4dd5":"code","d25b32d2":"code","a7bf39f7":"code","689b13b7":"code","2b4da27f":"code","6e5d78d4":"code","a3336149":"code","fa7380b5":"code","814f8461":"code","e4c287a0":"code","a2106b0f":"code","1c3d4f57":"code","6360e05c":"code","6d6e64df":"code","7650fddd":"code","af6407d1":"code","54379029":"code","1b533561":"code","a4a75473":"code","638e00e6":"code","8faecc49":"code","dbdb5944":"code","01ecbde9":"code","d475fcae":"code","4ab4ce4e":"code","ee0a6cd8":"code","733d4c2b":"code","a13ad2c1":"code","7043dace":"code","d8923805":"code","57dac337":"code","b410e47a":"code","80825279":"code","d77f050c":"code","105bf977":"code","28bbe374":"code","6d022d28":"code","e6344bf3":"code","6931c510":"code","71817e67":"code","efd0b8d8":"code","23c23c32":"code","dff084dc":"code","f0406ba3":"code","cba22c05":"code","67045de4":"code","fa4dec22":"code","7dbb10cf":"code","8e3afb00":"code","d31432d7":"code","88ea83c2":"code","0f9e3a10":"code","8b894647":"code","a5d9fed4":"code","0d1098d8":"markdown","52b74459":"markdown","c3578023":"markdown","e005042b":"markdown","81a533bb":"markdown","e677873d":"markdown","dba7b67f":"markdown","8cb11d3a":"markdown","a4f1dcf9":"markdown","6cdcead3":"markdown","a8d07f27":"markdown","590b53ec":"markdown","6172ccfd":"markdown","57b14c49":"markdown","bbb25678":"markdown","d888841f":"markdown"},"source":{"7ba62f67":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom fancyimpute import KNN\nimport seaborn as sns\nfrom scipy.stats import chi2_contingency\nfrom random import randrange, uniform\n\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","9bb1204e":"# setting working directory\n#os.chdir(\"\")\nos.getcwd()","2dcb79a5":"#loading data\nmarketing_train=pd.read_csv(\"..\/input\/marketing_tr.csv\")\nmarketing_train.shape","fa493353":"marketing_train.head()","5c87ade2":"marketing_train.columns","618a9490":"#Exploratory Data Analysis\nmarketing_train['schooling'] = marketing_train['schooling'].replace(\"illiterate\", \"unknown\")\nmarketing_train['schooling'] = marketing_train['schooling'].replace([\"basic.4y\",\"basic.6y\",\"basic.9y\",\"high.school\",\"professional.course\"], \"high.school\")\nmarketing_train['default'] = marketing_train['default'].replace(\"yes\", \"unknown\")\nmarketing_train['marital'] = marketing_train['marital'].replace(\"unknown\", \"married\")\nmarketing_train['month'] = marketing_train['month'].replace([\"sep\",\"oct\",\"mar\",\"dec\"], \"dec\")\nmarketing_train['month'] = marketing_train['month'].replace([\"aug\",\"jul\",\"jun\",\"may\",\"nov\"], \"jun\")\nmarketing_train['loan'] = marketing_train['loan'].replace(\"unknown\", \"no\")\nmarketing_train['profession'] = marketing_train['profession'].replace([\"management\",\"unknown\",\"unemployed\",\"admin.\"], \"admin.\")\nmarketing_train['profession'] = marketing_train['profession'].replace([\"blue-collar\",\"housemaid\",\"services\",\"self-employed\",\"entrepreneur\",\"technician\"], \"blue-collar\")","89cf1d29":"# checking missing values\nmissing_values=pd.DataFrame(marketing_train.isnull().sum())\nmissing_values\n#reseting index\nmissing_values=missing_values.reset_index()\nmissing_values\n#renaming the column names of the dataframes\nmissing_values= missing_values.rename(columns={'index':'Variables',0:'missing percentage'})\nmissing_values","67e521bc":"#calcualting % of missing values\nmissing_values['missing percentage']=(missing_values['missing percentage']\/len(marketing_train))*100\nmissing_values","8b398812":"# sorting data\nmissing_values=missing_values.sort_values('missing percentage',ascending=False).reset_index(drop=False)\nmissing_values.to_csv(\"missing_perc.csv\",index=False)","262a90fb":"# experiment \nmarketing_train['custAge'].loc[70]=np.nan\nmarketing_train['custAge'].loc[70]","1a15e4c6":"# impute with mean\n#marketing_train['custAge']=marketing_train['custAge'].fillna(marketing_train['custAge'].mean())\n#marketing_train['custAge'].loc[70]","73896184":"#impute with median\n#marketing_train['custAge']=marketing_train['custAge'].fillna(marketing_train['custAge'].median())\n#marketing_train['custAge'].loc[70]","2aa3dcfb":"#impute with KNN imputation method\n#assigning levels to each categories:\nlis=[]\nfor i in range(0, marketing_train.shape[1]):\n    if(marketing_train.iloc[:,i].dtype=='object'):\n        marketing_train.iloc[:,i]=pd.Categorical(marketing_train.iloc[:,i])\n        marketing_train.iloc[:,i]=marketing_train.iloc[:,i].cat.codes\n        marketing_train.iloc[:,i]=marketing_train.iloc[:,i].astype('object')\n        lis.append(marketing_train.columns[i])\n        ","e2742ac4":"#replace -1 with NA to impute\nfor i in range(0, marketing_train.shape[1]):\n    marketing_train.iloc[:,i] = marketing_train.iloc[:,i].replace(-1, np.nan) ","8d07f2aa":"# Apply KNN imputation algo\nmarketing_train=pd.DataFrame(KNN(k=3).fit_transform(marketing_train),columns=marketing_train.columns)\nmarketing_train['custAge'].loc[70]","b5846a57":"# Covert the data in appropriate data type\nfor i in lis:\n    marketing_train.loc[:,i]=marketing_train.loc[:,i].round()\n    marketing_train.loc[:,i]=marketing_train.loc[:,i].astype('object')","85c8011d":"marketing_train.to_csv(\"marketing_campaign_cleaned.csv\",index=False)","82b0bf5d":"marketing_train.head()","7cdf373c":"# import required library\n#import ggplot as glt","93f1a579":"# bar plot\n#ggplot(marketing_train,aes(x='profession',y='campaign'))+\\\n#geom_bar(fill=\"blue\")+theme_bw()+xlab(\"Profession\")+ylab(\"Campaign\")+\\\n#ggtitle(\"Marketing Campaign Analysis\")+theme(text=element_text(size=20))","f1ac4233":"#copying the data for expts\n#df=marketing_train.copy()\n#marketing_train.info()","7e77928d":"# visualization using box plots\n#plt.boxplot(marketing_train['custAge'])","af2c4dd5":"# separating continuous variables from the datasets\ncont_names=['custAge','campaign','pdays','previous','emp.var.rate','cons.price.idx','cons.conf.idx','euribor3m','nr.employed'\n            ,'pmonths','pastEmail']\n","d25b32d2":"#detect and delete the outlier from the data\n#for i in cont_names:\n#    q75,q25=np.percentile(marketing_train.loc[:,i],[75,25])\n#    iqr=q75-q25 #inter quartile range\n#    min=q25-(iqr*1.5)\n#    max=q75+(iqr*1.5)\n#    marketing_train=marketing_train.drop(marketing_train[marketing_train.loc[:,i]<min].index)\n#    marketing_train=marketing_train.drop(marketing_train[marketing_train.loc[:,i]>max].index)\n#marketing_train.shape\n    ","a7bf39f7":"#detect and replace outliers with NAs\n#marketing_train=df.copy()\n#extract outliers\n#q75,q25=np.percentile(marketing_train['custAge'],[75,25])\n#calculate IQR\n#iqr=q75-q25\n#calculate inner and outer fence\n#minimum=q25-(iqr*1.5)\n#maximum=q75+(iqr*1.5)\n#replacing the outiers\n#marketing_train.loc[marketing_train['custAge']<minimum]=np.nan\n#marketing_train.loc[marketing_train['custAge']>maximum]=np.nan\n#calculate the missing values\n#print('missing values=',marketing_train['custAge'].isna().sum())\n# impute wiht KNN\n#marketing_train=pd.DataFrame(KNN(3).fit_transform(marketing_train),columns=marketing_train.columns)\n","689b13b7":"#marketing_train.isna().sum()","2b4da27f":"##Correlation analysis\n#Correlation plot\ndf_corr = marketing_train.loc[:,cont_names]","6e5d78d4":"#Set the width and hieght of the plot\nf, ax = plt.subplots(figsize=(7, 5))\n\n#Generate correlation matrix\ncorr = df_corr.corr()\n\n#Plot using seaborn library\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax)","a3336149":"#Chisquare test of independence\n#Save categorical variables\ncat_names = [\"profession\", \"marital\", \"schooling\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"day_of_week\", \"poutcome\"]","fa7380b5":"#loop for getting chi square values\nfor i in cat_names:\n    print(i)\n    chi2, p, dof, ex = chi2_contingency(pd.crosstab(marketing_train['responded'], marketing_train[i]))\n    print(p)","814f8461":"# deleting unwanted data from the dataframe\nmarketing_train = marketing_train.drop(['pdays', 'emp.var.rate', 'day_of_week', 'loan', 'housing'], axis=1)","e4c287a0":"df = marketing_train.copy()\n#marketing_train = df.copy()","a2106b0f":"#Normality check\n%matplotlib inline  \nplt.hist(marketing_train['campaign'], bins='auto')","1c3d4f57":"# remaining continuous variables after removing unwanted variables\ncont_names = [\"custAge\",\"campaign\",\"previous\",\"cons.price.idx\",\"cons.conf.idx\",\"euribor3m\",\"nr.employed\",\n           \"pmonths\",\"pastEmail\"]","6360e05c":"#Nomalisation\nfor i in cont_names:\n    print(i)\n    marketing_train[i] = (marketing_train[i] - min(marketing_train[i]))\/(max(marketing_train[i]) - min(marketing_train[i]))","6d6e64df":"# #Standarisation\n# for i in cnames:\n#     print(i)\n#     marketing_train[i] = (marketing_train[i] - marketing_train[i].mean())\/marketing_train[i].std()","7650fddd":"##Simple random sampling\n#Sim_Sampling = marketing_train.sample(5000)","af6407d1":"# ##Systematic Sampling\n# #Calculate the K value\n# k = len(marketing_train)\/3500\n\n# # Generate a random number using simple random sampling\n# RandNum = randrange(0, 5)\n\n# #select Kth observation starting from RandNum\n# Sys_Sampling = marketing_train.iloc[RandNum::k, :]","54379029":"# #Stratified sampling\n# from sklearn.cross_validation import train_test_split\n\n# #Select categorical variable\n# y = marketing_train['profession']\n\n#select subset using stratified Sampling\n#Rest, Sample = train_test_split(marketing_train, test_size = 0.6, stratify = y)","1b533561":"#marketing_train = pd.read_csv(\"marketing_train_Model.csv\")","a4a75473":"#Import Libraries for decision tree\nfrom sklearn import tree\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split","638e00e6":"#replace target categories with Yes or No\nmarketing_train['responded'] = marketing_train['responded'].replace(0, 'No')\nmarketing_train['responded'] = marketing_train['responded'].replace(1, 'Yes')","8faecc49":"#Divide data into train and test\nX = marketing_train.values[:, 0:16]\nY = marketing_train.values[:,16]\n\nX_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = 0.2)","dbdb5944":"#Decision Tree\nC50_model = tree.DecisionTreeClassifier(criterion='entropy').fit(X_train, y_train)\n\n#predict new test cases\ny_predict = C50_model.predict(X_test)\n\n#Create dot file to visualise tree  #http:\/\/webgraphviz.com\/\n# dotfile = open(\"pt.dot\", 'w')\n#df = tree.export_graphviz(C50_model, out_file=dotfile, feature_names = marketing_train.columns)","01ecbde9":"# Model Evaluation\n#import the required library and module\nfrom sklearn.metrics import confusion_matrix\n","d475fcae":"#Building confusion matrix\nConf_mat=confusion_matrix(y_test,y_predict)","4ab4ce4e":"Conf_mat=pd.crosstab(y_test,y_predict)\nConf_mat","ee0a6cd8":"# defining the proper parameters\nTN=Conf_mat.iloc[0,0]\nFP=Conf_mat.iloc[0,1]\nFN=Conf_mat.iloc[1,0]\nTP=Conf_mat.iloc[1,1]","733d4c2b":"# checking accuracy of the model\naccuracy_score(y_test,y_predict)*100","a13ad2c1":"# accuracy in another way\n# accuracy=(correctly predicted values)*100\/(total observation)\n(TP+TN)*100\/(TP+TN+FP+FN)","7043dace":"# false negative rate \nFNR=(FN)*100\/(FN+TP)\nFNR\n","d8923805":"# Recall\nrecall=(TP*100)\/(TP+FN)\nrecall","57dac337":"#import required library and apply random forest for it\nfrom sklearn.ensemble import RandomForestClassifier\nRF_model=RandomForestClassifier(n_estimators=100).fit(X_train,y_train)","b410e47a":"#get the predictions\nRF_predictions=RF_model.predict(X_test)","80825279":"# build confusion matrix\n#CM=confusion_matrix(y_test,RF_predictions)\nCM=pd.crosstab(y_test,RF_predictions)\nCM\n# Set the parameters TP,TN,FP,FN\nTN=CM.iloc[0,0]\nFP=CM.iloc[0,1]\nFN=CM.iloc[1,0]\nTP=CM.iloc[1,1]\n","d77f050c":"# calculate the accuracy of the RF model\naccuracy=(TN+TP)*100\/(TN+TP+FN+FP)\naccuracy\n","105bf977":"# false negative rate\nFNR=(FN*100)\/(FN+TP)\nFNR","28bbe374":"# Data Preparation\n# replace target variable categories \"Yes\" and \"No\" with 1 and 0\nmarketing_train['responded'] = marketing_train['responded'].replace('No',0)\nmarketing_train['responded'] = marketing_train['responded'].replace('Yes',1)\n\n","6d022d28":"#Prepare logistic data\n# save target variable temperorily\nmarketing_train_logit=pd.DataFrame(marketing_train[\"responded\"])\n#add continuous variables here\nmarketing_train_logit=marketing_train_logit.join(marketing_train[cont_names])\n","e6344bf3":"# Dummification: Creating dummy variables for categorical variables\ncat_names=[\"profession\",\"marital\",\"schooling\",\"default\",\"contact\",\"month\",\"poutcome\"]\nfor i in cat_names:\n    temp=pd.get_dummies(marketing_train[i],prefix=i)\n    marketing_train_logit=marketing_train_logit.join(temp)\n","6931c510":"marketing_train_logit.shape","71817e67":"# separate the dataset into train and test\nsample_index=np.random.rand(len(marketing_train_logit))<0.8 # this will generate random values ie. random sampling for training dataset \ntrain=marketing_train_logit[sample_index]\ntest=marketing_train_logit[~sample_index]","efd0b8d8":"#save column indices for independent variables\ntrain_cols=train.columns[1:30]","23c23c32":"train.head()","dff084dc":"# Build\/ Train the Logistic Regression Algo\nimport statsmodels.api as sm\nlogit_model=sm.Logit(train[\"responded\"],train[train_cols]).fit()","f0406ba3":"# check the summary of the model\nlogit_model.summary()","cba22c05":"# Now make predictions \ntest[\"actual_prob\"]=logit_model.predict(test[train_cols])\ntest[\"actual_val\"]=1\n#test.loc[test[\"actual_prob\"] < 0.5,\"actual_val\"]=0\ntest.loc[test.actual_prob < 0.5,\"actual_val\"]=0","67045de4":"# model evaluation\n# make a confusion matrix\nCM=pd.crosstab(test[\"responded\"],test[\"actual_val\"])\nCM","fa4dec22":"# set the parameters\nTN=CM.iloc[0,0]\nFP=CM.iloc[0,1]\nFN=CM.iloc[1,0]\nTP=CM.iloc[1,1]\n","7dbb10cf":"# accuracy\naccuracy=(TP+TN)*100\/(TP+TN+FP+FN)\n#FNR\nFNR=(FN*100)\/(FN+TP)\naccuracy,FNR","8e3afb00":"# required libraries\nfrom sklearn.naive_bayes import GaussianNB","d31432d7":"# train the model\nNB_model=GaussianNB().fit(X_train,y_train)","88ea83c2":"#make predictions\nNB_predictions=NB_model.predict(X_test)","0f9e3a10":"#model evaluation\nCM=pd.crosstab(y_test,NB_predictions)\nCM","8b894647":"# set the parameters\nTN=CM.iloc[0,0]\nFP=CM.iloc[0,1]\nFN=CM.iloc[1,0]\nTP=CM.iloc[1,1]","a5d9fed4":"#accuracy\naccuracy=(TP+TN)*100\/(TP+TN+FP+FN)\nprint(accuracy)\n#FNR\nFNR=(FN)*100\/(FN+TP)\nprint(FNR)","0d1098d8":"### Understanding the data","52b74459":"# Sampling Techniques","c3578023":"<h1 align = center> Naive Bayes Classification <\/h1>","e005042b":"<h1 align=center>Logistic Regression <\/h1>","81a533bb":"# Outliers Analysis","e677873d":"## Note this step here","dba7b67f":"# Feature Scaling","8cb11d3a":"### Decision Tree","a4f1dcf9":"# Exploratory Data Analysis","6cdcead3":"## Comparison of models\n### 1) Decision tree:\n* accuracy=84.625%\n* FNR=63.125%\n\n### 2) Random Forest model:\n#### n_estimators=100\n* accuracy=90.559%\n* FNR=64.375%\n\n#### n_estimators=500\n* accuracy=90.087%\n* FNR=67.5%\n\n### 3) Logistic Regression\n* accuracy=90.033%\n* FNR=75.31%\n\n### 4) Naive Bayes Classification\n* accuracy=80.98%\n* FNR=53.07%\n\n","a8d07f27":"# Data Visualization","590b53ec":"## Error Metrics","6172ccfd":"## Missing value Analysis","57b14c49":"<h1 align=center> Random Forest<\/h1>","bbb25678":"# Feature Selection","d888841f":"<h1 align=center> Model Developement <\/h1>"}}