{"cell_type":{"28321b2b":"code","ad857bbf":"code","b6aa6e77":"code","ab20606e":"code","45244f37":"code","2d6c07ed":"markdown","75bbd24d":"markdown","25533c41":"markdown","11525dcd":"markdown","f56af5cb":"markdown"},"source":{"28321b2b":"from ast import literal_eval\nfrom collections import defaultdict \nfrom itertools import groupby\nfrom operator import itemgetter \n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n\nfrom scipy.interpolate import UnivariateSpline\n\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import classification_report, log_loss\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.pipeline import FeatureUnion, make_pipeline\nfrom sklearn.svm import LinearSVC\n\nimport collections\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport re\nimport seaborn as sns\nimport statsmodels.api as sm\nimport xgboost as xgb\nsns.set()\n\nseed = 42\n\nfrom sklearn.metrics import (\n    average_precision_score, confusion_matrix, ConfusionMatrixDisplay, plot_confusion_matrix,\n    plot_precision_recall_curve, plot_roc_curve, precision_recall_curve, roc_auc_score, roc_curve\n)\n\ndef plot_confusion_matrix_from_proba(estimator, X, y_true, threshold, labels=None,\n                          sample_weight=None, normalize=None,\n                          display_labels=None, include_values=True,\n                          xticks_rotation='horizontal',\n                          values_format=None,\n                          cmap='viridis', ax=None):\n\n    y_pred = estimator.predict_proba(X)[:, 1] > threshold\n    cm = confusion_matrix(y_true, y_pred, sample_weight=sample_weight,\n                          labels=labels, normalize=normalize)\n\n    if display_labels is None:\n        if labels is None:\n            display_labels = estimator.classes_\n        else:\n            display_labels = labels\n\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                                  display_labels=display_labels)\n    return disp.plot(include_values=include_values,\n                     cmap=cmap, ax=ax, xticks_rotation=xticks_rotation,\n                     values_format=values_format)\n\n\ndef plot_classification_results(clf, X_test, y_test, y_proba=None, threshold=None, nb_disp_thresh=10, harmonic_opt=True):\n    fig, ax = plt.subplots(figsize=(15, 4), ncols=3)\n\n    if threshold is None:\n        titles_options = [\n            (\"Confusion matrix\", None, \".0f\"),  # , without normalization\n            # (\"Normalized confusion matrix\", 'true', \".4f\")\n        ]\n    else:\n        titles_options = [\n            (\"Confusion matrix with threshold {:.4f}\".format(threshold), None, \".0f\"),  # ,\\n without normalization\n            # (\"Normalized confusion matrix\\n with threshold {:.4f}\".format(threshold), 'true', \".4f\")\n        ]\n\n    for ind, (title, normalize, values_format) in enumerate(titles_options):\n        if threshold is None:\n            plot_confusion_matrix(\n                clf, X_test, y_test, display_labels=['0', '1'], cmap=plt.cm.Blues, normalize=normalize,\n                values_format=values_format, ax=ax[ind]\n            )\n        else:\n            plot_confusion_matrix_from_proba(\n                clf, X_test, y_test, threshold=threshold, display_labels=['0', '1'], cmap=plt.cm.Blues,\n                normalize=normalize, values_format=values_format, ax=ax[ind]\n            )\n        ax[ind].set_title(title)\n        ax[ind].grid(False)\n\n    if y_proba is None:\n        y_proba = clf.predict_proba(X_test)[:, 1]\n\n    plot_roc_curve(clf, X_test, y_test, ax=ax[1], label=\"ROC curve\")\n    \n    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n    step = len(fpr) \/\/ (nb_disp_thresh - 1) if len(fpr) \/\/ (nb_disp_thresh - 1) >= 2 else 1\n\n    ax[1].plot(fpr[1:-1:step], tpr[1:-1:step], 'o', label=\"Thresholds\")\n    for x, y, txt in zip(fpr[1:-1:step], tpr[1:-1:step], thresholds[1:-1:step]):\n        ax[1].annotate(np.round(txt, 2), (x - 0.08, y + 0.02))\n    \n    if harmonic_opt:\n        argmax = np.argmax(2 * tpr * (1 - fpr) \/ (tpr + (1 - fpr) + 1e-6))\n    else:\n        argmax = np.argmax(tpr + (1 - fpr))\n    ax[1].plot(fpr[argmax], tpr[argmax], 'o', label=\"Best threshold\", color=\"green\")\n    ax[1].annotate(np.round(thresholds[argmax], 4), (fpr[argmax] + 0.01, tpr[argmax] - 0.06), color=\"green\")\n\n    ax[1].plot([0, 1], [0, 1], linestyle='--', color='r', label='Chance')\n    ax[1].set_title(\"ROC curve (AUC = {:.4f})\".format(roc_auc_score(y_test, y_proba)))\n    ax[1].legend()\n\n    plot_precision_recall_curve(clf, X_test, y_test, ax=ax[2], label=\"PR curve\")\n    pr, rc, thresholds = precision_recall_curve(y_test, clf.predict_proba(X_test)[:, 1])\n    step = len(rc) \/\/ (nb_disp_thresh - 1) if len(rc) \/\/ (nb_disp_thresh - 1) >= 2 else 1\n    \n    ax[2].plot(\n        rc[::step], pr[::step], 'o', label=\"Thresholds\"\n    )\n    for x, y, txt in zip(rc[1:-1:step], pr[1:-1:step], thresholds[1:-1:step]):\n        ax[2].annotate(np.round(txt, 2), (x, y + 0.02))\n\n    if harmonic_opt:\n        argmax = np.argmax(2 * pr * rc \/ (pr + rc + 1e-6))\n    else:\n        argmax = np.argmax(pr + rc)\n    ax[2].plot(rc[argmax], pr[argmax], 'o', label=\"Best threshold\", color=\"green\")\n    ax[2].annotate(np.round(thresholds[argmax], 4), (rc[argmax] - 0.18, pr[argmax] - 0.03), color=\"green\")\n\n    ax[2].set_title(\"PR curve (AP = {:.4f})\".format(average_precision_score(y_test, y_proba)))\n    ax[2].legend()\n    ax[2].set_xlim(-0.05, 1.05)\n    ax[2].set_ylim(-0.05, 1.05)\n\n    plt.show()","ad857bbf":"processing = False\n\nif processing:\n    tweets = pd.read_csv('\/kaggle\/input\/ncaa-tweets-20152019\/ncaa-tweets-2015-2019.csv', parse_dates=[\"date\"])\n    tweets[\"date\"] = tweets[\"date\"] - pd.Timedelta(hours=4)  # UTC to ET\n    tweets = tweets[tweets[\"date\"].apply(\n        lambda x: str(x)[:10] not in [\"2015-03-16\", \"2016-03-14\", \"2017-03-13\", \"2018-03-12\", \"2019-03-18\"]\n    )]\nelse:\n    tweets = pd.read_csv(\n        '\/kaggle\/input\/ncaa-tweets-processed\/tweets-processed.csv', parse_dates=[\"date\"],\n        converters={\"TeamIDMatch\": literal_eval, \"GameandTeamMatch\": literal_eval}\n    )\n\ndisplay(tweets.sample(5))\n\ngbcount = tweets.groupby([tweets['date'].dt.date]).count()\ngbsumday = tweets.groupby([tweets['date'].dt.date])[\"replies\", \"retweets\", \"favorites\"].sum()\n\ncolors = ['r', 'g', 'b', 'purple', 'orange']\n\nbegin_year, end_year = 2015, 2019\nbegin_day, end_day = 1, 21\n# begin_day, end_day = 134, 154\nnb_days = end_day - begin_day + 1\n\nfig, ax = plt.subplots(figsize=(15, 8), nrows=2, ncols=2)\nfor axis, engagement in enumerate([\"tweets\", \"replies\", \"retweets\", \"favorites\"]):\n    for ind, year in enumerate(range(begin_year, end_year + 1)):\n        if engagement == \"tweets\":\n            ax[axis \/\/ 2, axis % 2].plot(\n                range(1, nb_days + 1), gbcount[\"date\"][nb_days * ind:nb_days * (ind + 1)],c=colors[ind], label=year\n            )\n        else:\n            ax[axis \/\/ 2, axis % 2].plot(\n                range(1, nb_days + 1), gbsumday[engagement][nb_days * ind:nb_days * (ind + 1)], c=colors[ind], label=year\n            )\n    ax[axis \/\/ 2, axis % 2].set_xticks(range(1, nb_days + 1))\n    ax[axis \/\/ 2, axis % 2].set_xlabel(\"Day of the competition\")\n    ax[axis \/\/ 2, axis % 2].set_ylabel(\"Number of \" + engagement)\n    # ax[axis \/\/ 2, axis % 2].set_title(\"Number of \" + engagement)\n    ax[axis \/\/ 2, axis % 2].legend(loc=\"upper center\")\n\nplt.show()","b6aa6e77":"# stop_words = stopwords.words(\"english\")\n# stemmer = SnowballStemmer('english')\n\ndef clean_tweet(tweet):\n    tweet = tweet.lower()\n    tweet = re.sub(r\"http\\S+\", \"\", tweet)  # remove hyperlinks\n    tweet = re.sub(r\"@\\S+\", \"\", tweet)  # remove user names after @\n    tweet = re.sub(r\"&\\w+;\", \"\", tweet)  # remove first words after & (HTML chars)\n    tweet = re.sub(r\"\\n\", \" \", tweet)  # remove newlines\n    #\u00a0tweet = re.sub(r\"[^\\w\\s]\", \"\", tweet)  # remove special characters\n    tweet = re.sub(r\"[^A-Za-z0-9?,.;\/:!\\)\\(\\-\\s]+\", \" \", tweet)  # keep letters, numbers, spaces and some punctuation\n    tweet = re.sub(r\"([\\w\\s-]+|[^\\w\\s-]+)\\s*\", r\"\\1 \", tweet)  # separate words from punctuation except for -\n    tweet = re.sub(r\" +\", \" \", tweet)  # remove multiple whitespaces\n    tweet = re.sub(r\"^\\s+|\\s+$\", \"\", tweet)  # remove leading and trailing spaces\n    # tweet = \" \".join([word for word in tweet.split() if word not in stop_words])  # remove stop words\n    # tweet = \" \".join([stemmer.stem(word) for word in tweet.split()])  # stemming\n    return tweet\n\n\nif processing:\n    tweets[\"processed_text\"] = tweets[\"text\"].apply(clean_tweet)\n    tweets = tweets[tweets[\"processed_text\"] != \"\"].drop_duplicates([\"processed_text\"]).reset_index(drop=True)\n    tweets[\"Season\"] = pd.to_datetime(tweets[\"date\"]).dt.year\n\n\nPATH_TO_MEN_DATA = \"..\/input\/march-madness-analytics-2020\/2020DataFiles\/2020DataFiles\/2020-Mens-Data\"\nPATH_TO_STAGE2_DATA = \"..\/input\/march-madness-analytics-2020\/MDataFiles_Stage2\"\n\nteam_spellings = pd.read_csv(PATH_TO_STAGE2_DATA + \"\/MTeamSpellings.csv\", encoding='latin1')\nteam_players = pd.read_csv(PATH_TO_MEN_DATA + \"\/MPlayers.csv\")\nteam_coaches = pd.read_csv(PATH_TO_STAGE2_DATA + \"\/MTeamCoaches.csv\")\n\nteam_id_to_spellings = team_spellings.groupby('TeamID')['TeamNameSpelling'].apply(lambda x: list(set(x))).to_dict()\nteam_id_to_players = team_players.groupby('TeamID')['LastName'].apply(lambda x: list(set(x))).to_dict()\nteam_id_to_coaches = team_coaches.groupby('TeamID')['CoachName'].apply(lambda x: list(set(x))).to_dict()\n\nteam_id_to_tweet_mentions = {\n    key: sorted(list(\n        possible_name.lower() for possible_name in set(\n            value + team_id_to_players.get(key, []) + team_id_to_coaches.get(key, [])\n        ) if len(possible_name) >= 4\n    )) for key, value in team_id_to_spellings.items()\n}\n\nMEvents = pd.concat([\n    pd.read_csv(PATH_TO_MEN_DATA + \"\/MEvents{}.csv\".format(year)) for year in range(begin_year, end_year + 1)\n])\nMSeasons = pd.read_csv(PATH_TO_STAGE2_DATA + \"\/MSeasons.csv\")\nMNCAATourneySeeds = pd.read_csv(PATH_TO_STAGE2_DATA + \"\/MNCAATourneySeeds.csv\")\nMNCAATourneyCompactResults = pd.read_csv(PATH_TO_STAGE2_DATA + \"\/MNCAATourneyCompactResults.csv\")\n\n# Dictionary with all teams from 2015 to 2019  \nyear_to_team_ids = {\n    year: MNCAATourneySeeds[MNCAATourneySeeds[\"Season\"] == year][\"TeamID\"].tolist()\n    for year in range(begin_year, end_year + 1)\n}  # NCAATeamsDict\n\n# Dictionary with the starting date of a given season\nyear_to_begin_day = {\n    key: pd.to_datetime(value) for key, value in {\n        2015: \"2015-03-17\", 2016: \"2016-03-15\", 2017: \"2017-03-14\", 2018: \"2018-03-13\", 2019: \"2019-03-19\"\n    }.items()\n}\n\n# Dictionary with the individual Tourney games with day number, winning team ID and losing team ID\nyear_to_day_to_game_ids = {\n    year : [\n        (DayNum,WTeamID,LTeamID) \n        for DayNum,WTeamID,LTeamID in zip(\n            MNCAATourneyCompactResults[MNCAATourneyCompactResults[\"Season\"] == year][\"DayNum\"] - 133,\n            MNCAATourneyCompactResults[MNCAATourneyCompactResults[\"Season\"] == year][\"WTeamID\"],\n            MNCAATourneyCompactResults[MNCAATourneyCompactResults[\"Season\"] == year][\"LTeamID\"]\n        )\n    ] for year in range(begin_year, end_year + 1)\n}\n\n# Create embedded dictionnary for each tournement day\nfor year in range(begin_year, end_year + 1):\n    year_to_day_to_game_ids[year] = dict(\n        (DayNum, [team for Teams in itr for team in Teams[1:]])\n        for DayNum, itr in groupby(year_to_day_to_game_ids[year], itemgetter(0))\n    ) \n\n\nif processing:\n    # Setting up DayNum column  \n    tweets[\"DayNum\"] = tweets[\"date\"].apply(lambda x: x - year_to_begin_day[int(x.year)]).dt.days + 1\n\n    for year in range(begin_year, end_year + 1):\n        tweets.loc[tweets[\"Season\"] == year, \"TeamIDMatch\"] = tweets.loc[tweets[\"Season\"] == year, \"text\"].apply(\n            lambda tweet: [\n                key for key in year_to_team_ids[year]\n                if any(keyword in tweet for keyword in team_id_to_tweet_mentions[key])\n            ]\n        )\n        tweets.loc[tweets[\"Season\"] == year, \"GameandTeamMatch\"] = tweets.loc[\n            tweets[\"Season\"] == year, [\"TeamIDMatch\", \"Season\", \"DayNum\"]\n        ].apply(\n            lambda values: list(set(values[0]) & set(year_to_day_to_game_ids[values[1]].get(values[2], []))), axis=1\n        )\n\n    tweets.to_csv(\"tweets-processed.csv\", index=False)\n\n\ndef plot_team_day_heatmap(year_to_matrix, title):\n    fig, ax = plt.subplots(figsize=(14, 15))\n    sns.heatmap(\n        year_to_matrix[end_year], cmap=plt.cm.Blues, ax=ax, xticklabels=range(1, nb_days + 1),\n        yticklabels=[team_id_to_spellings[x][0] for x in year_to_team_ids[end_year]],  # cbar=False\n    )\n    ax.set_title(\"{} in Year {}\".format(title, end_year))\n    ax.set_xlabel(\"Day of competition (1st to 21st)\")\n    ax.set_ylabel(\"Name of the team\")\n    plt.show()\n\n    nb_rows, nb_cols = 2, 3\n    fig, ax = plt.subplots(figsize=(15, 10), nrows=nb_rows, ncols=nb_cols)\n    for ind, year in enumerate(range(begin_year, end_year + 1)):\n        sns.heatmap(\n            year_to_matrix[year], cmap=plt.cm.Blues, ax=ax[ind \/\/ nb_cols, ind % nb_cols],\n            xticklabels=False, yticklabels=False,  # cbar=False\n        )\n        ax[ind \/\/ nb_cols, ind % nb_cols].set_title(\"{} in Year {}\".format(title, year))\n        ax[ind \/\/ nb_cols, ind % nb_cols].set_xlabel(\"Day of competition (1st to 21st)\")\n        ax[ind \/\/ nb_cols, ind % nb_cols].set_ylabel(\"ID of the team\")\n    plt.show()\n\n\nyear_to_team_mentions_count = {\n    year: np.array([[\n        tweets[\n            (tweets[\"Season\"] == year) & (tweets[\"DayNum\"] == day)\n        ][\"TeamIDMatch\"].apply(lambda x: team_id in x).sum()\n        for day in range(begin_day, end_day + 1)\n    ] for team_id in year_to_team_ids[year]\n    ]) for year in range(begin_year, end_year + 1)\n}\n\nplot_team_day_heatmap(year_to_team_mentions_count, \"Number of mentions\")","ab20606e":"if processing:\n    sentiment140 = pd.read_csv(\n        '\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', header=None, parse_dates=[2],\n        names=[\"sentiment\", \"id\", \"date\", \"flag\", \"username\", \"text\"], encoding=\"ISO-8859-1\"\n    )\n    sentiment140[\"processed_text\"] = sentiment140[\"text\"].apply(clean_tweet)\n    sentiment140 = sentiment140[sentiment140[\"processed_text\"] != \"\"].drop_duplicates([\"processed_text\"]).reset_index(drop=True)\n    sentiment140[\"sentiment\"] = sentiment140[\"sentiment\"].apply(lambda x: x \/\/ 4)\n    sentiment140.to_csv(\"sentiment140-processed.csv\", encoding=\"utf-8\", index=False)\nelse:\n    sentiment140 = pd.read_csv('\/kaggle\/input\/ncaa-tweets-processed\/sentiment140-processed.csv')\n\ndisplay(sentiment140.sample(5))\n\nX_train, X_val, y_train, y_val = train_test_split(\n    sentiment140[\"processed_text\"].apply(str), sentiment140[\"sentiment\"], random_state=seed, test_size=0.2\n)\nX_test = tweets[\"processed_text\"].apply(str)\n\npipe = make_pipeline(\n    FeatureUnion([\n        ('words', TfidfVectorizer(ngram_range=(1, 3), analyzer='word')),\n        ('chars', TfidfVectorizer(ngram_range=(1, 3), analyzer='char')),\n    ]),\n    CalibratedClassifierCV(LinearSVC(C=1.0, random_state=seed), cv=5),\n)\npipe.fit(X_train, y_train)\n\ny_val_proba = pipe.predict_proba(X_val)[:, 1]\nthreshold = 0.4874\ny_val_pred = y_val_proba > threshold\n\nplot_classification_results(pipe, X_val, y_val, y_proba=y_val_proba, threshold=threshold)\nplt.show()\n\nplt.figure(figsize=(15, 4))\nplt.hist(y_val_proba, bins=100, density=True, label=\"Sentiment140 Tweets\")\n\ny_proba_1 = pipe.predict_proba(X_test[:len(X_test) \/\/ 2])[:, 1]\ny_proba_2 = pipe.predict_proba(X_test[len(X_test) \/\/ 2:])[:, 1]\ny_proba = np.concatenate([y_proba_1, y_proba_2])\ny_pred = (y_proba > threshold) * 1\n\nplt.hist(y_proba, bins=100, density=True, label=\"NCAA Tweets\", alpha=0.5)\nplt.title(\"Probability density of positive sentiments\")\nplt.xlabel(\"Probability\")\nplt.ylabel(\"Density\")\nplt.legend()\nplt.show()\n\ntweets[\"proba\"] = y_proba\ntweets[\"prediction\"] = y_pred\ntweets.to_csv(\"tweets-sentiments.csv\", index=False)","45244f37":"year_to_average_sentiment = {\n    year: np.array([[\n        tweets[\n            (tweets[\"Season\"] == year) & (tweets[\"DayNum\"] == day) &\n            (tweets[\"TeamIDMatch\"].apply(lambda x: team_id in x))\n        ].proba.mean()\n        for day in range(begin_day, end_day + 1)\n    ] for team_id in year_to_team_ids[year]\n    ]) for year in range(begin_year, end_year + 1)\n}\n\nplot_team_day_heatmap(year_to_average_sentiment, \"Average sentiment\")","2d6c07ed":"Although we do not observe clear distinctions between teams for a positive or a negative sentiment, we see that the average sentiment for a given team on a given day is over 0.5. meaning that overall positive sentiments are more prevalent than negative ones. This follows our previous observation that NCAA March Madness creates positive reactions in its fans.\n\n\n# 4. Conclusion\n\nOur exploration of the NCAA data in Part 1 revealed that there does indeed exist a link between watching higher seeded teams compete and the objective entertainment of those games. Filled with more point-effective shots, a greater degree of player collaboration, and higher scores, we are able to conclude that the madness of a game is correlated to the ranks of the involved teams.\n\nSimilarly, we were able to see trends related to the madness of a match and the closeness of the score throughout. While we feel that the madness cannot truly be predicted from these metrics alone, it in many ways confirms our initial suspicions that much of the delightful madness of this tournament is derived from its inability to be predicted.\n\nThe sentiment analysis we have completed here in Part 2 is telling of the enjoyment of fans. In some ways it may seem that our analysis of Twitter data was unable to uncover any specific and telling trends related to the public\u2019s response to March Madness throughout the season. We see that their social media activity is centered on the main parts of the competition and that the most buzz is generated right at the start when the most teams are involved, and the highest number of games occur. While all of this is relatively intuitive, what is somewhat surprising and worthy of further evaluation is that the overall reaction to these games is positive.\n\nTwitter has in many ways built up a reputation for being a place that people can express short, inflammatory remarks which may be expected when upsets inevitably occur throughout the tournament. However, this is not the trend we saw. As indicated by the trends in positive sentiments expressed on Twitter, even when upsets occur and favorites lose, by and large the some 40 million people following the competition still love the game and we are sure that, after the cancellation of this season, there will be even more people looking forward to watching the madness play out in the 2021 games.","75bbd24d":"If we consider the number of favorites as a key indicator of the madness generated by the competition, we can remark that the major spikes in favorites reflect the March Madness schedule. Spikes at the beginning of the competition correspond to *First Round* and *Second Round*, spikes in the middle to *Sweet Sixteen* and *Elite Eight*, and in the end to the *Final Four*.\n\nThe lack of spikes when there are no games shows that madness is generated by the games themselves. Moreover, the much higher number of favorites compared to the number of tweets, replies, and retweets indicates that NCAA March Madness creates positive reactions. If there were more negative comments in reaction to games, it is likely people would feel the need to reply to either also express their frustration or to argue back against someone rather than simply favorite the initial tweet.\n\n\n## 3.2. Team Mentions Count in Tweets\n\nIn this section, we aim to refine our study by focusing on the number of team mentions per team and per day. Using tables of team spellings, player names, and coach names, we tried to find tweets mentioning specific teams or people. We present these results in the form of heatmaps, the rows corresponding to teams and the columns to the day of the competition.","25533c41":"# Madness at Home and on the Court - Part 2\n\n*Authors: [Emilien Etchevers](https:\/\/www.kaggle.com\/emimis), [Kieran Janin](https:\/\/www.kaggle.com\/kieranjanin), [Michael Karpe](https:\/\/www.kaggle.com\/mika30), [Remi Le Thai](https:\/\/www.kaggle.com\/remilethai), [Haley Wohlever](https:\/\/www.kaggle.com\/haleywohlever)*\n\n# Contents\n\n*In the [first notebook](https:\/\/www.kaggle.com\/emimis\/madness-at-home-and-on-the-court-part-1)*\n\n- [Introduction](#Introduction) <br>\n- [NCAA March Madness Data Analysis](#NCAAMarchMadnessData)<br>\n  * [Seeding and Entertainment](#SeedingandEntertainment)<br>\n  * [Madness through Unpredictability](#MadnessthroughUnpredictability)<br>\n  * [Entertainment due to Closeness of Games](#EntertainmentduetoClosenessofGames)\n    \n*In this notebook*\n\n- [Tweets on NCAA Data Analysis](#TweetsonNCAAData)<br>\n  * [Temporal Evolution of Engagement](#TemporalEvolutionofEngagement)<br>\n  * [Team Mentions Count in Tweets](#TeamMentionsCountinTweets)<br>\n  * [Sentiment Analysis for Tweets on NCAA](#SentimentAnalysisforTweetsonNCAA)\n- [Conclusion](#Conclusion)\n\n\n# 3. Tweets on NCAA Data\n\nAfter thoroughly exploring the NCAA data, we decided to extend our analysis by studying relevant tweets published during the 2015 to 2019 NCAA March Madness tournaments. We built a database of 1.6 million tweets containing the word *NCAA*, using the [GetOldTweets3](https:\/\/github.com\/Mottl\/GetOldTweets3) library. The dataset we built with this library is available [here](https:\/\/www.kaggle.com\/mika30\/ncaa-tweets-20152019) (as well as a processed version [here](https:\/\/www.kaggle.com\/mika30\/ncaa-tweets-processed) to avoid memory issues).\n\nThe purpose of this second part of our analysis is to identify trends in tweets published during and related to the competition. As discussed in Part 1, we want to distinguish an *objective* madness, explainable by match and player data, from a *subjective* madness that can be observed in the public sentiment surrounding the games.\n\n\n## 3.1. Temporal Evolution of Engagement\n\nWe first study *engagement* on Twitter. *Engagement* on Twitter is defined by three types of actions: retweets, replies, and favorites (or likes). We plot the evolution of the number of each of these actions over the 21 days of the competition for each season, as well as the evolution of the number of tweets.\n\nWhile we can see that the number of tweets including the word *NCAA* tends to slightly decrease over the years (or at least remains constant), we can observe an increase in the number of retweets and replies over the years. We also note an even higher increase in the number of favorites, this latest observation showing that people tend to express themselves more and more using the favorites feature on Twitter.","11525dcd":"Again, we note for all years that the darkest columns, i.e. the ones with the highest number of mentions in a given day, correspond to the main stages of the tournament. When looking at the rows, we can observe that some teams are mentioned more than others in our set of tweets.\n\nFor example, on the heatmap corresponding to the year 2019, we observe that the team that generated the highest number of mentions on a given day is *Northern Kentucky* on Day 4. When looking at the 2019 schedule as well as press releases, we can confirm that *Northern Kentucky* was playing on 03\/22\/19. They lost a game against *Texas Tech*, who is known for their solid defense:\n\n|                         |                         |\n|-------------------------|-------------------------|\n| ![north-kentucky-1.png](attachment:north-kentucky-1.png) | ![north-kentucky-press.png](attachment:north-kentucky-press.png) |\n\n<br>\n\nThis new indicator of team mentions on a given day shows that some of the teams likely have more supporters active on Twitter! The number of active supporters on Twitter may then be an indicator for the madness generated by a team.\n\n\n## 3.3. Sentiment Analysis for Tweets on NCAA\n\nIn this final section of our madness exploration, we applied sentiment analysis to the NCAA Tweets, training a classifier on the [Sentiment140](https:\/\/www.kaggle.com\/kazanova\/sentiment140) dataset (which also contains around 1.6 million tweets) and then predicting sentiments on NCAA tweets.\n\nWe display a sample of lines of Sentiment140 dataset, as well as the performance of our classifier on a validation set (with an accuracy around 82.5% and an area under the ROC curve around 0.906) taken from the Sentiment140 dataset. We also display the probability distribution of positive and negative sentiments for the validation set and NCAA tweets.","f56af5cb":"While the classifier is able to make a clear distinction between positive and negative tweets on the Sentiment140 dataset, predictions on our NCAA tweets show a more uniform distribution that is skewed left. The fact that the probability distribution on NCAA tweets tends to be more uniform is due to the neutrality of a sizeable proportion of NCAA tweets. However, the skewed-left-characteristic shows that NCAA generates more positive sentiments than negative sentiments overall. This shows that what we call *madness* in the NCAA competition is more related to positive than negative sentiments.\n\nWith the predictions from our classifier, we display below heatmaps corresponding the average sentiment (between 0 and 1) for a given team on a given day. Here, 0 is taken as a negative sentiment, 1 is taken as a positive sentiment, and 0.5 is taken as the threshold between a positive and negative sentiment."}}