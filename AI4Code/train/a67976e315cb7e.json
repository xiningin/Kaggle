{"cell_type":{"6e2395a9":"code","abfec419":"code","ca61477c":"code","6799ad7a":"code","35545598":"code","4e5ad96f":"code","2b97cb41":"code","2753af67":"code","f7373e9a":"code","d860dbe1":"code","d3dca9ed":"code","28b8df28":"code","6d81b04a":"code","446e43e0":"code","b594c54c":"code","e15aac8a":"code","16630574":"code","5d895644":"code","fbc91a98":"code","94889d61":"code","0b22246e":"code","044bc650":"code","15237d12":"code","14957bc0":"code","acb80d0d":"code","3e8c1169":"code","f85b0e2a":"code","24f645ce":"code","c327cb05":"markdown","5d31df7d":"markdown","d6af9566":"markdown","4712f7e5":"markdown","2af5f3b5":"markdown","9f3c11c6":"markdown","578a5e5f":"markdown","cfd37f12":"markdown","7199d6c0":"markdown","2c6956d6":"markdown","d5ec40e3":"markdown","1639f88b":"markdown","b1536753":"markdown","ac74ed54":"markdown","57dbff91":"markdown","56de429b":"markdown","61eb75cd":"markdown","26179afb":"markdown","beca42b5":"markdown","9c71f77d":"markdown","b429ee01":"markdown","fe878228":"markdown","3c0df37c":"markdown","50ff6e6c":"markdown","7858a100":"markdown","682d0687":"markdown","bf259790":"markdown"},"source":{"6e2395a9":"# Libraries for numerical analysis\nimport numpy as np # linear algebra and matrix operations\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Libraries for data visualisation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Libraries for data mungling\nfrom sklearn import preprocessing # preprocessing.scale() performs mean normalization and feature scaling across the dataset\n\n# Common Machine Learning Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.impute import SimpleImputer # SimpleImputer fills empty cells with the mean\nfrom sklearn.model_selection import cross_validate # To train with cross validation\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n# Others\nimport os\nimport warnings\nwarnings.filterwarnings('ignore') # Had a lot of future warning messages\n\n# Import datasets\ndf_train = pd.read_csv('..\/input\/train.csv') # Training set (The data we are given)\ndf_test = pd.read_csv('..\/input\/test.csv') # Test set (The data we need to test for results)","abfec419":"# Visualise numerical features\ndisplay(df_train.describe())\n# Visualise categorical features\ndisplay(df_train.describe(include=['O']))","ca61477c":"sns.heatmap(df_train.corr(method='pearson'),annot=True,cmap=\"YlGnBu\")","6799ad7a":"# Graph individual features by survival\nsns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=df_train)","35545598":"fig, ax = plt.subplots(1,2)\nsns.barplot(x = 'SibSp', y = 'Survived', order=[1,2,3,4,5,6,7], data=df_train, ax=ax[0])\nsns.barplot(x = 'Parch', y = 'Survived', order=[1,2,3,4,5,6], data=df_train, ax=ax[1])","4e5ad96f":"y = df_train['Survived']\ntest_index = df_test['PassengerId']\n# For simultaneous data cleaning\ncombine = [df_train, df_test]","2b97cb41":"# For feature: Name\n# Convert: Name -> Title\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False) # regex\n\n# Convert: rare titles -> Rare, Mlle -> Miss, Ms -> Miss, Mme -> Mrs\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n                    'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\n# Display the available types in Title feature.\nsns.countplot(x = 'Title', order=df_train['Title'].unique(), data=df_train)","2753af67":"# For feature: Sex\n# Convert: female -> 1, male -> 0\nfor dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\n# For feature: Embarked\n# Fill in missing values with highest frequency port\nfreq_port = df_train.Embarked.dropna().mode()[0]\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \n# Convert: S -> 0, C -> 1, Q -> 2\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\n# For feature: Title\n# Convert: Mr -> 1, Miss -> 2, Mrs -> 3, Master -> 4, Rare -> 5\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\n# Display the values in each feature.\nprint(\"Values in Sex:\", df_train['Sex'].unique())\nprint(\"Values in Embarked:\", df_train['Embarked'].unique())\nprint(\"Values in Title:\", sorted(df_train['Title'].unique()))","f7373e9a":"sns.barplot(x=df_train.corr(method='pearson')[['Survived']].index, y=df_train.corr(method='pearson')['Survived'])","d860dbe1":"df_train[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)\nsns.barplot(x = 'Sex', y = 'Survived', order=[1,0], data=df_train)","d3dca9ed":"# Fill in missing values with median\ndf_test['Fare'].fillna(df_test['Fare'].dropna().median(), inplace=True)\n\n# Split Fare into 4 bands in FareBand to find out where the bins are\ndf_train['FareBand'] = pd.qcut(df_train['Fare'], 4)\ndf_train[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","28b8df28":"# Convert Ranges of fare prices into bins\nfor dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\nsns.barplot(x = 'Fare', y = 'Survived',order=[0,1,2,3],data=df_train)","6d81b04a":"# We converted Fare into 4 bins, so FareBand is no longer useful to us, hence we drop it\ndf_train = df_train.drop(['FareBand'], axis=1)\ncombine = [df_train, df_test]","446e43e0":"sns.barplot(x=df_train.corr(method='pearson')[['Survived']].index, y=df_train.corr(method='pearson')['Survived'])","b594c54c":"for dataset in combine:\n    dataset.loc[(dataset.Age.isnull()), 'Age'] = dataset.Age.median()","e15aac8a":"df_train['AgeBand'] = pd.cut(df_train['Age'], 5)\ndf_train[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","16630574":"for dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4\n    \nsns.barplot(x = 'Age', y = 'Survived',order=[0,1,2,3,4],data=df_train)","5d895644":"df_train = df_train.drop(['AgeBand'], axis=1)\ncombine = [df_train, df_test]\ndf_train.head()","fbc91a98":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\nsns.barplot(x = 'FamilySize', y = 'Survived', order=[1,2,3,4,5,6,7], data=df_train)","94889d61":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\nsns.barplot(x = 'IsAlone', y = 'Survived',order=[0,1],data=df_train)","0b22246e":"display(df_train.corr(method='pearson')[['Survived']])\nsns.barplot(x=df_train.corr(method='pearson')[['Survived']].index, y=df_train.corr(method='pearson')['Survived'])\n\ndf_train = df_train.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ndf_test = df_test.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ncombine = [df_train, df_test]\n\ndisplay(df_train.head())","044bc650":"for dataset in combine:\n    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n\ndf_train.loc[:, ['Age*Class', 'Age', 'Pclass']].head(5)","15237d12":"df_train = df_train.drop(['Name', 'Ticket', 'Cabin'], axis=1)\ndf_test = df_test.drop(['Name', 'Ticket', 'Cabin'], axis=1)\ncombine = [df_train, df_test]\ndisplay(combine[0].head())\ndisplay(combine[1].head())","14957bc0":"# df_train_X is a list of features used for model training.\ndf_train_X = combine[0].drop([\"Survived\", \"PassengerId\"], axis=1)\n# train_y is the training output.\ntrain_y = combine[0][\"Survived\"]\ntest_test_X = combine[1].drop(\"PassengerId\", axis=1).copy()","acb80d0d":"#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n    XGBClassifier()    \n    ]\n\n#split dataset in cross-validation with this splitter class: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n#note: this is an alternative to train_test_split\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60\/30 split intentionally leaving out 10%\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n#create table to compare MLA predictions\nMLA_predict = {}\n\n#index through MLA and save performance to table\nrow_index = 0\nfor alg in MLA:\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    #score model with cross validation: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n    cv_results = model_selection.cross_validate(alg, df_train_X, train_y, cv  = cv_split, return_train_score=True)\n    \n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n    #if this is a non-bias random sample, then +\/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n    \n\n    # Fit, predict test input and evaluate using F1 score.\n    alg.fit(df_train_X, train_y)\n    MLA_compare.loc[row_index, 'F1 Score'] = metrics.f1_score(train_y, alg.predict(df_train_X))\n    MLA_predict[MLA_name] = alg.predict(test_test_X)\n    \n    row_index+=1\n    \n#print and sort table: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.sort_values.html\nMLA_compare.sort_values(by = ['F1 Score'], ascending = False, inplace = True)","3e8c1169":"#barplot using https:\/\/seaborn.pydata.org\/generated\/seaborn.barplot.html\nsns.barplot(x='F1 Score', y = 'MLA Name', data = MLA_compare, color = 'm')\n\n#prettify using pyplot: https:\/\/matplotlib.org\/api\/pyplot_api.html\nplt.title('Machine Learning Algorithm F1 Score \\n')\nplt.xlabel('F1 Score (%)')\nplt.ylabel('Algorithm')","f85b0e2a":"best_model = MLA_compare.loc[MLA_compare['F1 Score'].idxmax()]['MLA Name']\nbest_model_score = round(MLA_compare.loc[MLA_compare['F1 Score'].idxmax()]['F1 Score'],3)\nprint(\"Best model:\",best_model)\nprint(\"F1 Score:\",best_model_score)","24f645ce":"# predict against test set\npredictions = MLA_predict[best_model]\npredictions = predictions.ravel() # To reduce ND-array to 1D-array\ndata_to_submit = pd.DataFrame({\n    'PassengerId': test_index,\n    'Survived': predictions\n})\n# output results to results.csv\ndata_to_submit.to_csv(\"results.csv\", index=False)","c327cb05":"## Accuracy based on Kaggle's leaderboard: 0.78","5d31df7d":"**Correlation of Categorical Features with Survived**\n\nNow, we can identify the categorical features Title, Sex and Embarked and their correlation coefficient against Survived.\n\n**Correlation Coefficient against Survived**\n* Title: 0.408\n* Sex: 0.543\n* Embarked: 0.107","d6af9566":"We leave out Name, Ticket and Cabin because the syntax brings about no direct insight to the survival rate.","4712f7e5":"## (3\/6) Data Cleaning: Conversion of Categorical Data to Numerical Data\n\n### Features:\n\nSex: female -> 1, male -> 0\n\nEmbarked: S -> 0, C -> 1, Q -> 2\n\nTitle: Mr -> 1, Miss -> 2, Mrs -> 3, Master -> 4, Rare -> 5\n\nWe need to convert categorical features to numerical values because our model (MLP Classifier) requires numerical data.","2af5f3b5":"## Visualising Datasets at a Glimpse\n\nIt is important to take a look at the dataset to understand how you can clean the data.\n\nNote the characteristics of our data: Categorical\/Numerical data, Uniqueness\/Correlation to Survivability, Range of values across dataset, etc.","9f3c11c6":"We drop parch, sibsp and family size as correlation coefficient is lower than IsAlone and will create noise.","578a5e5f":"## (5\/6) Test Against Cross-Validation Set and Test Set and Analyse Performance Metrics (F1 Score)\nThe performance metric we are currently using is accuracy. Accuracy is defined as (no. of correct predictions)\/(no. of predictions).\nHowever, this is not a robust indicator of the model's performance. For a binary classification problem which has 2 classes, 0 or 1, a model that predicts all cases as 1 will easily achieve 50% accuracy.\n\nIn other problems such as identification of cancer cells, there may only be 1% of cases with cancer. A model that assumes all cases as non-cancer will easily achieve 99% accuracy. Hence, we need a more robust metric, such as the F1 score.\n\nWe want to measure the precision and recall, and the corresponding F1 score.\n* Precision = Of all that we predict are true, how many are actually true? (no. of true positives)\/(no. of true positives + no. of false positives)\n* Recall = Of all that are really true, how many did we predict are true? (no. of true positives)\/(no. true positives + no. of false negatives)\n\nLet's now see how well our model fits the cross validation set.","cfd37f12":"**Feature: Sex**\n\nCorrelation coefficient against Survived: 0.543\n\nSex = female has very high survival rate at 74%.","7199d6c0":"**Feature: Fare**\n\nCorrelation coefficient against Survived: 0.257\n\nFare has a weak but significant positive correlation with Survived. We can choose to include this feature in our model.\n\nWe observe that although Fare has a sufficiently significant linear correlation against Survived (>0.20), there is too much noise in the data (too many unique values). Hence, we decide to bin Fare into 4 subcategories.","2c6956d6":"## (1\/6) Import Preprocessing Libraries and Datasets","d5ec40e3":"## (3\/6) Data Cleaning\n\nData cleaning is greatly beneficial and necessary. The 4 C's of data cleaning are: Correcting, Completing, Creating and Converting.\n\n1. **Correcting:** Non-acceptable data inputs and outliers create unnecessary noise.\n2. **Completing**: Null values or missing data will make some algorithms fail. Either delete the record or impute missing values using mode (qualitative data) or mean, median, mean + randomized standard deviation (quantitative data).\n3. **Creating**: Use feature engineering to create new features to improve the prediction.\n4. **Converting**: Categorical data are imported as objects which are difficult for mathematical calculations. Converting to categorical dummy variables will help.","1639f88b":"We create a new feature IsAlone for passengers with FamilySize of 1.","b1536753":"## (4\/6) Choosing a Machine Learning Algorithm\n\nWe take a list of ML algorithms and train them and compare the cross-validation test accuracies.\n\nReferred from: https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\/notebook","ac74ed54":"We create a new feature Age * Class to increase the number of features.","57dbff91":"### Note: Importance of a Cross-Validation Set\n\nA cross-validation set is important to ensure that our model does not train to overfit the training and test data. Because we experiment with different hyperparameters of the model (learning rate, iterations, etc), there is a possibility of the model overfitting to the test data as well. Therefore, we use the training set to train the model, evaluate the model using the cross-validation set, then experiment with hyperparameters before evaluating the model on the test set.\n\nIt is important to distinguish between the cross validation set derived from the training data (20% of train.csv) and the actual test data (test.csv) we are supposed to predict against.\n\nWe split our training data into 2 portions: 80% training set and 20% cross-validation set.","56de429b":"We bin Age into AgeBand so as to reduce noise.","61eb75cd":"## (3\/6) Data Cleaning: Extraction of Meaningful Data from Meaningless Data\n**Feature: Name -> Title**\n\nFor the feature Name, we want to extract the passengers' title (Mr, Miss, etc) to use as a feature. We also replace less occuring titles with the label Rare.","26179afb":"### Feature: Pclass\n\nCorrelation coefficient against Survived: -0.338\n\nPclass is negatively correlated to Survived. We decide to include this feature in our model.\n\nObservation: We notice a downward trend of the survival rate as Pclass increases from 1 to 3, from 0.63 to 0.24.","beca42b5":"### Features: SibSp and Parch\n\nThese features have 0 survival rates for certain values. It may be best to derive a feature or a set of features from these individual features.","9c71f77d":"## (6\/6) Wrap Up and Submit Results\nLet's now see how well our model fits the test set.\n\nWe predict the results of the test set and prepare it for submission","b429ee01":"We create a new feature FamilySize that combines SibSp and Parch.","fe878228":"**Imputation (filling of empty cells) of the Dataset**\n\nImputation of the dataset can be done by filling empty cells with each feature data's:\n* Mean value\n* Median value\n* Mode value\n* Median - Standard Deviation\n\nWe can also drop rows of data that have missing values but because there are many missing cells in Age, we cannot afford to drop 200+ rows out of 800+ in the entire dataset. We decide to use median to fill missing Age cells.","3c0df37c":"## (2\/6) Data Visualisation and Understanding of Dataset\n\nLet us make some sense of the correlation between features of the dataset. Take note of how each feature correlates with Survived.\n\nWe note that the features Pclass and Fare have relatively high correlation coefficients (>0.20) as compared to other features. Whilst they are considered having weak linear correlation (<0.50), there is sufficiently significant correlation.\n\nHowever, we also note that categorical features, Name, Sex, Ticket, Cabin and Embarked and their correlation to Survived cannot be determined. Hence, there is a need to convert them to numerical values.","50ff6e6c":"We have improved Fare's correlation with Survived from 0.257 to 0.300.","7858a100":"## (3\/6) Data Cleaning: Creating New Features from Current Data\n\nWe also note that features with particularly low correlation coefficients against Survived (Age, SibSp, Parch) needs to be further cleaned to make sense of the data. We decide to do this by creating new features.\n\n**Correlation Coefficients Against Survived**\n* Age: -0.077\n* SibSp: -0.035\n* Parch: 0.082","682d0687":"## To the Reader (You!) [Update: 3\/9\/2018]\nIf you're a beginner in data science\/Kaggle\/machine learning, I recommend viewing everyone's Titanic notebooks to learn and practise unfamiliar libraries and techniques. If you're an intermediate or an expert, I hope that you can share your valuable experience with me by commenting and giving feedback on areas of improvement to this notebook. All feedback is welcome! :)\n\n## Objective\nMy objective of doing this competition is to apply everything I've learnt from ML on Coursera in practice. I want to learn Python libraries for numerical analysis like numpy and pandas, libraries for data visualisation like seaborn and matplotlib, and libraries for machine learning like tensorflow, keras and scikit-learn.\n\n## My Background\nPrior to this competition, I have completed Machine Learning by Stanford University on Coursera. I only know Matlab implementations of linear\/logistic regression, neural networks, support vector machines (SVMs), K-means, principal component analysis (PCA), anomaly detection and collaborative filtering. I also know some machine learning concepts like bias\/variance trade-offs, precision\/recall and mean normalization\/feature scaling.\nSee what I learnt here: https:\/\/github.com\/jetnew\/My-Work-in-Machine-Learning\/blob\/master\/ML%20by%20Andrew%20Ng\/README.md\n\n## Plan (What I Learnt)\nOur plan for the Titanic problem will be as follows:\n\n1. Import preprocessing libraries and datasets\n2. Data visualisation to find correlations and insights\n3. Data cleaning: Correcting, Completing, Creating, Converting (4C's)\n4. Choose machine learning model\n5. Test against cross-validation set and analyse performance metrics (F1 Score)\n6. Wrap up and submit results","bf259790":"### Note: Mean Normalization and Feature Scaling of Data\n\nOur dataset's values now do not have vastly different ranges of values as they range only from 1 to 6. These small difference in ranges will not affect much our model's loss convergence to the minimum.\n\nIf our dataset has big differences in ranges of values, we will need to use mean normalization and feature scaling on the training data.\nMean normalization and feature scaling: X = (X - mean(X)) \/ std(X).\nMean normalization adjusts the mean of the values to 0 while feature scaling scales the range of values to -1 to 1."}}