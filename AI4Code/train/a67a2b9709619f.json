{"cell_type":{"a7161281":"code","f5cdd07c":"code","26f7e759":"code","fdaaa5d1":"code","9ba1c10b":"code","2810a3da":"code","f4119690":"code","40c89eff":"code","0f4c6e1c":"code","54b7cd92":"code","6384e9d2":"code","21f4a547":"code","055d3cec":"code","da3261eb":"code","984249d1":"code","29c11316":"code","af72ff12":"code","7f54b06e":"code","55c9edbc":"code","1bc6920f":"code","4d0f347f":"code","24064da2":"code","7c8c91fb":"code","f43b4ce5":"code","9315cc7e":"code","33312160":"code","bf68b5cc":"code","1961fa26":"code","39ffac5a":"code","11e96bf0":"code","6c8bc252":"code","320d0438":"code","c26348f0":"code","9a5e15a6":"code","1f625e18":"code","852d6f19":"code","5ee14092":"code","3a521c0f":"code","d56de5a4":"code","a669c851":"code","38db4c55":"markdown"},"source":{"a7161281":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f5cdd07c":"import pandas as pd\nimport numpy as np","26f7e759":"data=pd.read_csv(r'..\/input\/spam-or-not-spam-dataset\/spam_or_not_spam.csv')\ndata.head()\nfrom sklearn.utils import shuffle\ndata = shuffle(data)","fdaaa5d1":"data['label'].value_counts()\n","9ba1c10b":"text =[] \n  \n# Iterate over each row \nfor index, rows in data.iterrows(): \n    # Create list for the current row \n    my_list =str(rows.email)\n      \n    # append the list to the final list \n    text.append(my_list) \n  \n# Print the list \nlen(text)","2810a3da":"label=list(data['label'])","f4119690":"len(label)","40c89eff":"from keras.preprocessing.text import Tokenizer\ntokenizer = Tokenizer(num_words=1000)\ntokenizer.fit_on_texts(text)\nsequences = tokenizer.texts_to_sequences(text)","0f4c6e1c":"x_train=sequences[:2000]\ny_train=label[:2000]\nx_test=sequences[2000:]\ny_test=label[2000:]","54b7cd92":"maxlen = 20\nfrom keras import preprocessing\nx_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)","6384e9d2":"x_train[0]","21f4a547":"import numpy as np\ndef vectorize_sequences(sequences, dimension=1000):\n    results = np.zeros((len(sequences), dimension))\n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1.\n    return results\n\nx_train = vectorize_sequences(x_train)\nx_test = vectorize_sequences(x_test)","055d3cec":"x_train[0]","da3261eb":"y_train[0]","984249d1":"y_train = np.asarray(y_train).astype('float32')\ny_test = np.asarray(y_test).astype('float32')","29c11316":"y_train[0]","af72ff12":"from keras import models\nfrom keras import layers\nmodel = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(1000,)))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))","7f54b06e":"model.compile(optimizer='rmsprop',\nloss='binary_crossentropy',\nmetrics=['accuracy'])","55c9edbc":"history = model.fit(x_train,\ny_train,\nepochs=20,\nbatch_size=32,\nvalidation_split=0.3)","1bc6920f":"history_dict = history.history\nhistory_dict.keys()","4d0f347f":"import matplotlib.pyplot as plt\nhistory_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","24064da2":"acc_values = history_dict['accuracy']\nval_acc_values = history_dict['val_accuracy']\nplt.plot(epochs, acc_values, 'bo', label='Training acc')\nplt.plot(epochs, val_acc_values, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()","7c8c91fb":"results = model.evaluate(x_test, y_test)","f43b4ce5":"results","9315cc7e":"from keras import models\nfrom keras import layers\nfrom keras import regularizers\nmodel = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(1000,),kernel_regularizer=regularizers.l2(0.001)))\nmodel.add(layers.Dense(16, activation='relu',kernel_regularizer=regularizers.l2(0.001)))\nmodel.add(layers.Dense(1, activation='sigmoid'))","33312160":"model.compile(optimizer='rmsprop',\nloss='binary_crossentropy',\nmetrics=['accuracy'])","bf68b5cc":"history_reg1 = model.fit(x_train,\ny_train,\nepochs=20,\nbatch_size=32,\nvalidation_split=0.3)","1961fa26":"import matplotlib.pyplot as plt\nhistory_dict = history_reg1.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","39ffac5a":"acc_values = history_dict['accuracy']\nval_acc_values = history_dict['val_accuracy']\nplt.plot(epochs, acc_values, 'bo', label='Training acc')\nplt.plot(epochs, val_acc_values, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()","11e96bf0":"results1 = model.evaluate(x_test, y_test)","6c8bc252":"results1","320d0438":"from keras import models\nfrom keras import layers\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(1000,)))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(1, activation='sigmoid'))","c26348f0":"model.compile(optimizer='rmsprop',\nloss='binary_crossentropy',\nmetrics=['accuracy'])","9a5e15a6":"history_d = model.fit(x_train,\ny_train,\nepochs=20,\nbatch_size=32,\nvalidation_split=0.3)","1f625e18":"import matplotlib.pyplot as plt\nhistory_dict = history_d.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","852d6f19":"acc_values = history_dict['accuracy']\nval_acc_values = history_dict['val_accuracy']\nplt.plot(epochs, acc_values, 'bo', label='Training acc')\nplt.plot(epochs, val_acc_values, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()","5ee14092":"results2 = model.evaluate(x_test, y_test)","3a521c0f":"results","d56de5a4":"results1","a669c851":"results2","38db4c55":"* We\u2019re creating a layer that will only accept as input 2D tensors where the first dimen-sion is 1000 (axis 0, the batch dimension, is unspecified, and thus any value would beaccepted). This layer will return a tensor where the first dimension has been trans-formed to be 16.\n* Without an activation function like relu (also called a non-linearity), the Dense layerwould consist of two linear operations\u2014a dot product and an addition:output = dot(W, input) + b. So the layer could only learn linear transformations (affine transformations) of theinput data: the hypothesis space of the layer would be the set of all possible lineartransformations of the input data into a 16-dimensional space. Such a hypothesisspace is too restricted and wouldn\u2019t benefit from multiple layers of representations,because a deep stack of linear layers would still implement a linear operation: addingmore layers wouldn\u2019t extend the hypothesis space.In order to get access to a much richer hypothesis space that would benefit fromdeep representations, you need a non-linearity, or activation function. relu is themost popular activation function in deep learning, but there are many other candi-dates, which all come with similarly strange names: prelu, elu, and so on."}}