{"cell_type":{"973b919b":"code","5d632625":"code","6fb8c2f0":"code","bd0a24bf":"code","3a880ffe":"code","e280d615":"code","d587564c":"code","d63e8f92":"code","b2248fea":"code","3b67746e":"code","6f0ad414":"code","0644a73d":"code","3d15afc4":"code","784999e5":"code","c9a6e8e3":"code","7297b5d0":"code","b6f435f6":"code","3b61c440":"markdown","55fb7183":"markdown","f48e0b86":"markdown","113479b0":"markdown","be48fc98":"markdown","86cde35c":"markdown","56c5aa8e":"markdown","67046c2c":"markdown","8c71c902":"markdown","b08097ff":"markdown","7bb75c3a":"markdown","1bc3eec9":"markdown","8914991b":"markdown","e3521090":"markdown"},"source":{"973b919b":"#Importing packages\nimport numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5d632625":"#Load dataset\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest  = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","6fb8c2f0":"#Feature to predict\ntarget = list(set(train.columns) - set(test.columns))\ntarget = target[0] ","bd0a24bf":"X_train = train.loc[:, train.columns != target]\nY_train = train[target]\n\nX_test = test\n\n#Drop id\nX_test_id = X_test[\"Id\"]\n\nX_train.drop(columns='Id',inplace=True)\nX_test.drop(columns='Id',inplace=True)\n\nX_train.reset_index(drop=True, inplace=True)\nX_test.reset_index(drop=True, inplace=True)","3a880ffe":"def fillNa_df(df):\n    \n    #select object columns\n    obj_col = df.columns[df.dtypes == 'object'].values\n\n    #select non object columns\n    num_col = df.columns[df.dtypes != 'object'].values\n\n    #replace null value in obj columns with None\n    df[obj_col] = df[obj_col].fillna('None')\n\n    #replace null value in numeric columns with 0\n    df[num_col] = df[num_col].fillna(0)\n    \n    return df\n\nX_train_001 = fillNa_df(X_train)\nX_test_001 = fillNa_df(X_test)","e280d615":"from sklearn.preprocessing import OneHotEncoder\n\ndef oneHotEncoding(df_train, df_test):\n    \n    #select object columns\n    obj_col = df_train.columns[df_train.dtypes == 'object'].values\n\n    # creating instance of one-hot-encoder\n    enc = OneHotEncoder(handle_unknown='ignore')\n\n    # Ordinal features\n    ordinal_features = [x for x in obj_col]\n\n    # passing cat column (label encoded values)\n    df_train_encoded = pd.DataFrame(enc.fit_transform(df_train[ordinal_features]).toarray())\n    df_test_encoded  = pd.DataFrame(enc.transform(df_test[ordinal_features]).toarray())\n    \n    df_train_encoded.reset_index(drop=True, inplace=True)\n    df_test_encoded.reset_index(drop=True, inplace=True)\n\n    # merge with main df\n    df_train_encoded = pd.concat([df_train, df_train_encoded], axis=1)\n    df_test_encoded  = pd.concat([df_test,  df_test_encoded], axis=1)\n\n    # drop ordinal features\n    df_train_encoded.drop(columns=ordinal_features, inplace=True)\n    df_test_encoded.drop(columns=ordinal_features, inplace=True)\n    \n    return df_train_encoded, df_test_encoded\n\nX_train_002, X_test_002 = oneHotEncoding(X_train_001, X_test_001)","d587564c":"def featureEng(df):\n\n    #TotalBath\n    df['TotalBath'] = (df['FullBath'] + df['HalfBath'] + df['BsmtFullBath'] + df['BsmtHalfBath'])\n\n    #TotalPorch\n    df['TotalPorch'] = (df['OpenPorchSF'] + df['3SsnPorch'] + df['EnclosedPorch'] + df['ScreenPorch'] + df['WoodDeckSF'])\n\n    #Modeling happen during the sale year\n    df[\"RecentRemodel\"] = (df[\"YearRemodAdd\"] == df[\"YrSold\"]) * 1\n\n    #House sold in the year it was built\n    df[\"NewHouse\"] = (df[\"YearBuilt\"] == df[\"YrSold\"]) * 1\n\n    #YrBltAndRemod\n    df[\"YrBltAndRemod\"] = df[\"YearBuilt\"] + df[\"YearRemodAdd\"]\n\n    #Total_sqr_footage\n    df[\"Total_sqr_footage\"] = df[\"BsmtFinSF1\"] + df[\"BsmtFinSF2\"] + df[\"1stFlrSF\"] + df[\"2ndFlrSF\"]\n\n    df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n\n    df['Area_Qual'] = df['TotalSF'] * df['OverallQual']\n\n    #HasPool\n    df['HasPool'] = df['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n\n    #HasFireplaces\n    df['HasFirePlace'] = df['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\n    #Has2ndFloor\n    df['Has2ndFloor'] = df['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n\n    #HasGarage\n    df['HasGarage'] = df['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n\n    #HasBsmnt\n    df['HasBsmnt'] = df['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n    \n    return df\n\n#Feature Engineering\nX_train_003 = featureEng(X_train_002)\nX_test_003 = featureEng(X_test_002)","d63e8f92":"from sklearn.ensemble import IsolationForest\nfrom sklearn.covariance import EllipticEnvelope\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\n\n\ndef dropoutlier(df_X, df_Y, method='IsolationForest'):\n\n    if(method=='IsolationForest'):\n        #Isolation Forest\n\n        # identify outliers in the training dataset\n        iso = IsolationForest(contamination=0.012)\n        yhat = iso.fit_predict(df_X)\n\n    if(method=='MinimumCovarianceDeterminant'):\n        #Minimum Covariance Determinant\n\n        # identify outliers in the training dataset\n        ee = EllipticEnvelope(contamination=0.01, n_jobs=-1, random_state=42)\n        yhat = ee.fit_predict(df_X)\n\n    if(method=='LocalOutlierFactor'):\n        #Local Outlier Factor\n\n        # identify outliers in the training dataset\n        lof = LocalOutlierFactor()\n        yhat = lof.fit_predict(df_X)\n\n    if(method=='OneClassSVM'):\n        #One-Class SVM\n\n        # identify outliers in the training dataset\n        ee = OneClassSVM(nu=0.001)\n        yhat = ee.fit_predict(df_X)\n\n    # select all rows that are not outliers\n    mask = yhat != -1\n    df_X_drop, df_Y_drop = df_X[mask], df_Y[mask]\n\n    # select all rows that are outliers\n    masko = yhat == -1\n    df_X_o, df_Y_o = df_X[masko], df_Y[masko]\n    \n    return df_X, df_Y, [df_X_o, df_Y_o]\n\n#Drop outliers\nX_train_004, Y_train_004, df_o = dropoutlier(X_train_003, Y_train)\n\n# summarize the shape of the updated training dataset\nprint('Total: ', X_train_004.shape)\nprint('Not Outliers: ', X_train_003.shape)\nprint('Outliers: ', df_o[0].shape)","b2248fea":"#Plot GrLivArea vs SalePrice\nplt.scatter(X_train_004['GrLivArea'], Y_train_004, color='blue', alpha=0.5)\nplt.scatter(df_o[0]['GrLivArea'],   df_o[1],   color='red',  alpha=0.5, label='Outlier')\nplt.legend(loc=\"upper left\")\nplt.title(\"GrLivArea vs SalePrice\")\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\nplt.show()","3b67746e":"#Importing packages\nfrom sklearn.model_selection import train_test_split\n\n#Particiona o data set originalmente Train em Train(Treino) e Val(valida\u00e7\u00e3o)\nX_train_005, X_val_005, Y_train_005, Y_val_005 = train_test_split(X_train_004, \n                                                                  Y_train_004, \n                                                                  test_size=0.2, \n                                                                  random_state=42)\n\nX_train_005.shape, X_val_005.shape","6f0ad414":"#Importing Packages\nimport matplotlib.pyplot as plt\n\nfrom xgboost import XGBRegressor\nfrom xgboost import XGBRFRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\nfrom sklearn.preprocessing import Imputer","0644a73d":"#XGBoost hyper-parameter tuning\ndef hyperParameterTuning(X_train, y_train):\n    param_tuning = {\n        'learning_rate': [0.01, 0.1],\n        'max_depth': [3, 5, 10],\n        'min_child_weight': [1, 3, 5],\n        'subsample': [0.5, 0.7],\n        'colsample_bytree': [0.5, 0.7],\n        'n_estimators' : [200, 500],\n        'objective': ['reg:squarederror']\n    }\n\n    xgb_model = XGBRegressor(tree_method='gpu_hist')\n\n    gsearch = GridSearchCV(estimator = xgb_model,\n                           param_grid = param_tuning,\n                           cv = 5,\n                           n_jobs = -1,\n                           verbose = 1)\n\n    gsearch.fit(X_train,y_train)\n\n    return gsearch.best_params_","3d15afc4":"#Run only in the first run of the kernel.\n#hyperParameterTuning(X_train_005, Y_train_005)","784999e5":"#Model fit\nxgb_model = XGBRegressor(\n        objective = 'reg:squarederror',\n        colsample_bytree = 0.7,\n        learning_rate = 0.1,\n        max_depth = 3,\n        min_child_weight = 1,\n        n_estimators = 500,\n        subsample = 0.7)\n\neval_set = [(X_val_005, Y_val_005)]\n\nxgb_model.fit(X_train_005, Y_train_005, early_stopping_rounds=10, eval_metric=\"mae\", eval_set = eval_set, verbose=False)\n\nprint('MAE: ', xgb_model.evals_result()['validation_0']['mae'][-1])","c9a6e8e3":"Y_val_pred = xgb_model.predict(X_val_005)","7297b5d0":"#Plot Real vs Predict\nplt.scatter(X_val_005['GrLivArea'], Y_val_005,   color='blue', label='Real',    alpha=0.5)\nplt.scatter(X_val_005['GrLivArea'], Y_val_pred,  color='red' , label='Predict', alpha=0.5)\nplt.title(\"Real vs Predict\")\nplt.legend(loc='best')\nplt.show()","b6f435f6":"# Use the model to make predictions\nY_pred_test = xgb_model.predict(X_test_003)\n\nsubmission = pd.DataFrame({'Id':X_test_id,'SalePrice':Y_pred_test})\n\n# Save results\nsubmission.to_csv(\"submission.csv\",index=False)","3b61c440":"## Encoding ordinal\/categorical features","55fb7183":"## X\/Y datasets","f48e0b86":"# 1. Loading and Inspecting Data","113479b0":"# Feature Engineering","be48fc98":"# 7. Plot Results","86cde35c":"### Best Fit","56c5aa8e":"## 5.2 XGBoost","67046c2c":"# Outlier Detection\n\nPerhaps the most important hyperparameter in the model is the \u201ccontamination\u201d argument, which is used to help estimate the number of outliers in the dataset. This is a value between 0.0 and 0.5 and by default is set to 0.1.","8c71c902":"# Split dataframe - Train Validation","b08097ff":"### Best Params\n<br>{'colsample_bytree': 0.7,\n 'learning_rate': 0.1,\n 'max_depth': 3,\n 'min_child_weight': 1,\n 'n_estimators': 500,\n 'objective': 'reg:squarederror',\n 'subsample': 0.7}","7bb75c3a":"# 5. Fit Models","1bc3eec9":"## Fill NaN values","8914991b":"# 8. Predic Test & Submission","e3521090":"**GridSearchCV params:**\n* **estimator:** estimator object\n* **param_grid :** dict or list of dictionaries\n* **scoring:** A single string or a callable to evaluate the predictions on the test set. If None, the estimator\u2019s score method is used.\n    * https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n* **n_jobs:** Number of jobs to run in parallel. None means. -1 means using all processors.\n* **cv:** cross-validation, None, to use the default 3-fold cross validation. Integer, to specify the number of folds in a (Stratified)KFold."}}