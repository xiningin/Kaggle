{"cell_type":{"08c0b91b":"code","9d9e391e":"code","f9d89b0b":"code","c55f5b31":"code","35c69d25":"code","d87f0b3b":"code","9e58270f":"code","a132b842":"code","08b4fcf1":"code","cd4540af":"code","a1167b94":"code","eb17e4bb":"code","6f54ea18":"code","99a8140b":"code","6214efcc":"code","243120c8":"code","4c23cf61":"code","b8fe3327":"code","05c36805":"code","65d52e89":"code","1eeed2f8":"code","0a94bf43":"code","7742ff51":"code","d1de42d7":"code","39b03a25":"code","b221c744":"code","554b7ffa":"code","0976dbed":"code","86d72132":"code","ee267619":"code","8d3d7155":"code","6f62c820":"code","16711ccb":"markdown","662a8dcd":"markdown","be39a897":"markdown","461db8e4":"markdown","a390645c":"markdown","f54d272f":"markdown","8ce1fe8d":"markdown","cea53002":"markdown","dff05b44":"markdown"},"source":{"08c0b91b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ncustomers = pd.read_csv(\"\/kaggle\/input\/brazilian-ecommerce\/olist_customers_dataset.csv\")\nsellers = pd.read_csv(\"\/kaggle\/input\/brazilian-ecommerce\/olist_sellers_dataset.csv\")\norder_reviews = pd.read_csv(\"\/kaggle\/input\/brazilian-ecommerce\/olist_order_reviews_dataset.csv\")\norder_items = pd.read_csv(\"\/kaggle\/input\/brazilian-ecommerce\/olist_order_items_dataset.csv\")\nproducts = pd.read_csv(\"\/kaggle\/input\/brazilian-ecommerce\/olist_products_dataset.csv\")\ngeolocation = pd.read_csv(\"\/kaggle\/input\/brazilian-ecommerce\/olist_geolocation_dataset.csv\")\nproduct_category_name_translation = pd.read_csv(\"\/kaggle\/input\/brazilian-ecommerce\/product_category_name_translation.csv\")\norders = pd.read_csv(\"\/kaggle\/input\/brazilian-ecommerce\/olist_orders_dataset.csv\")\norder_payments = pd.read_csv(\"\/kaggle\/input\/brazilian-ecommerce\/olist_order_payments_dataset.csv\")\n\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9d9e391e":"print('Total number of duplicated order_id : {}'.format(orders.shape[0] - len(orders['order_id'].unique())))\nprint('Total number of duplicated customer_id : {}'.format(customers.shape[0] - len(customers['customer_id'].unique())))\norders = orders.drop_duplicates()\ndata_order_customer = pd.merge(orders, customers, on = 'customer_id', how = 'inner')\n\norder_items.drop_duplicates()\nprint('Total number of duplicated seller_id : {}'.format(sellers.shape[0] - len(sellers['seller_id'].unique())))\nprint('Number of sellers : {}, Number of orders_item:{}'.format(sellers.shape[0],order_items.shape[0]))\ndata_order__item_seller = pd.merge(order_items, sellers, on = 'seller_id', how = 'inner')\n\ndata = pd.merge(data_order_customer, data_order__item_seller, on = 'order_id', how = 'inner')\ndata.dropna()\ndata = pd.merge(products, data, on = 'product_id', how = 'inner')\ndata = pd.merge(product_category_name_translation, data, on = 'product_category_name', how = 'inner')\nmissing_data = data.isnull().sum()\nfor i in range(len(missing_data)):\n    print('Missing rows in {}:{}'.format(missing_data.index[i], missing_data.values[i]))","f9d89b0b":"\n# print('Total number of duplicated zip_code_prefix : {}'.format(geolocation.shape[0] - len(geolocation['geolocation_zip_code_prefix'].unique())))\n# print(geolocation['geolocation_zip_code_prefix'].value_counts())\n\ntmp = geolocation[['geolocation_zip_code_prefix']].apply(pd.value_counts).sum(1)\ngeolocation = geolocation.assign(zip_code_count = geolocation['geolocation_zip_code_prefix'].map(tmp))\ngeolocation = geolocation.drop_duplicates(['geolocation_zip_code_prefix'])\ngeolocation.sort_values(by = 'zip_code_count', ascending = False,inplace = True)\n\ngeolocation['geolocation_lat'] = geolocation['geolocation_lat'].astype(float)\ngeolocation['geolocation_lng'] = geolocation['geolocation_lng'].astype(float)\n\n# geomap = gpd.GeoDataFrame(geolocation, geometry = gpd.points_from_xy(geolocation['geolocation_lng'],geolocation['geolocation_lat']))\n# geomap.plot(k = 6, figsize = (9,9),color = '#2BB7B3')\n# plt.show()\n\ngeolocation = geolocation[(geolocation.geolocation_lat <= (float)(5.27438888))]\ngeolocation = geolocation[((float)(-33.75116944) <= geolocation.geolocation_lat)]\ngeolocation = geolocation[((float)(-73.98283055) <= geolocation.geolocation_lng)]\ngeolocation = geolocation[(geolocation.geolocation_lng <= (float)(-34.79314722))]\n\n\nimport unicodedata\n\ndef trans(mystr):\n    k = unicodedata.normalize('NFKD',mystr).encode('ascii','ignore')\n    if(mystr != k.decode('ascii')):\n        print(mystr,'  ',k.decode('ascii'))\n        \n    return k.decode('ascii')\n\ngeolocation['geolocation_city'] = geolocation['geolocation_city'].apply(trans)\n","c55f5b31":"geomap = gpd.GeoDataFrame(geolocation, geometry = gpd.points_from_xy(geolocation['geolocation_lng'],geolocation['geolocation_lat']))\ngeomap.plot(k = 6, figsize = (9,9),color = '#2BB7B3')\nplt.show()","35c69d25":"data_seller_customer = data[['seller_zip_code_prefix','customer_zip_code_prefix']]\nprint(data_seller_customer.shape, geolocation.shape)\ngeolocation.rename(columns = {'geolocation_zip_code_prefix':'seller_zip_code_prefix'},inplace = True)\ndata_seller_customer = pd.merge(data_seller_customer, geolocation, on = 'seller_zip_code_prefix', how = 'inner')\ndata_seller_customer.rename(columns={\n    'geolocation_lat':'seller_lat',\n    'geolocation_lng': 'seller_lng',\n    'geolocation_city':'seller_city',\n    'geolocation_state':'seller_state'\n    },inplace=True)\n\ngeolocation.rename(columns = {'seller_zip_code_prefix':'customer_zip_code_prefix'},inplace = True)\nprint(data_seller_customer.shape)\n\ndata_seller_customer = pd.merge(data_seller_customer, geolocation, on = 'customer_zip_code_prefix', how = 'inner')\ndata_seller_customer.rename(columns={\n    'geolocation_lat':'customer_lat',\n    'geolocation_lng': 'customer_lng',\n    'geolocation_city':'customer_city',\n    'geolocation_state':'customer_state'\n    },inplace=True)\nprint(data_seller_customer.shape)\n\ngeolocation.rename(columns = {'customer_zip_code_prefix':'geolocation_zip_code_prefix'},inplace = True)\ndata_seller_customer.head()","d87f0b3b":"from math import radians, cos, sin, asin, sqrt\n\ndef geodistance(lng1,lat1,lng2,lat2):\n    lng1, lat1, lng2, lat2 = map(radians, [float(lng1), float(lat1), float(lng2), float(lat2)])\n    dlon=lng2-lng1\n    dlat=lat2-lat1\n    a=sin(dlat\/2)**2 + cos(lat1) * cos(lat2) * sin(dlon\/2)**2\n    distance=2*asin(sqrt(a))*6371*1000 \n    distance=round(distance\/1000,3)\n    return distance\n\n# data_seller_customer['distance'] = data_seller_customer[['seller_lng','seller_lat','customer_lng','customer_lat']].apply(geodistance, axis = 1)\nA = data_seller_customer['seller_lng'].tolist()\nB = data_seller_customer['seller_lat'].tolist()\nC = data_seller_customer['customer_lng'].tolist()\nD = data_seller_customer['customer_lat'].tolist()\ndata_seller_customer['distance'] = data_seller_customer['seller_lat']\nfor i in range(len(A)):\n    data_seller_customer.distance.loc[i] = geodistance(A[i],B[i],C[i],D[i])\n\n# data_seller_customer['distance'] = geodistance(data_seller_customer['seller_lng'].tolist(),data_seller_customer['seller_lat'].tolist(), data_seller_customer['customer_lng'].tolist(),data_seller_customer['customer_lat'].tolist())\n\ndata_seller_customer.head()\n\ndata_seller_customer.shape[0]","9e58270f":"data_seller_customer = data_seller_customer.sort_values(by = ['distance'])\ndistance = data_seller_customer['distance'].tolist()\nbins = np.linspace(min(distance), max(distance), 5000)\nprint(bins)\nplt.hist(distance,bins)\nplt.xlabel('distance between sellers and customers')\nplt.ylabel('the number of orders')\nplt.title('Relation of number of orders and distance between sellers and customers')\n","a132b842":"df1 = data_seller_customer[['seller_city','customer_city']]\ndf1['seller_customer_pair'] = df1.apply(lambda x: (x['seller_city'], x['customer_city']), axis = 1)\n\nseller_single = df1[['seller_city']].apply(pd.value_counts).sum(1)\ncustomer_single = df1[['customer_city']].apply(pd.value_counts).sum(1)\ndf1 = df1.assign(seller_count = df1['seller_city'].map(seller_single))\ndf1 = df1.assign(customer_count = df1['seller_city'].map(customer_single))\ndf1['single_count'] = df1['seller_count'] + df1['customer_count']\n\npair = df1[['seller_customer_pair']].apply(pd.value_counts).sum(1)\ndf1 = df1.assign(pair_count = df1['seller_customer_pair'].map(pair))\ndf1.drop_duplicates('seller_customer_pair', inplace = True)\ndf1 = df1.reset_index(drop = True)\n\ncounts = 0\nfor i in range(df1.shape[0]):\n    counts = counts + df1.loc[i,'pair_count']\n\ndf2 = df1.drop_duplicates('seller_city', inplace = False)\ndf2.sort_values(by = 'single_count', ascending= False, inplace = True)\ndf2 = df2.reset_index(drop = True)\n\nprint('the total number of orders ', counts)\nprint('the total number of distinct seller & customer_cities ', df2.shape[0])\n\nx_axis = df2.loc[:9,'seller_city'].to_numpy()\ny_axis = x_axis\n\nxy = [[0 for i in range(len(x_axis))] for i in range(len(x_axis))]\n              \nfor i in range(df1.shape[0]):\n    for ix in range(len(x_axis)):\n        if(x_axis[ix] == df1.loc[i,'seller_city']):\n            for iy in range(len(y_axis)):\n                if(y_axis[iy] == df1.loc[i,'customer_city']):\n                    xy[ix][iy] = df1.loc[i,'pair_count']\n\nprint(xy)\n","08b4fcf1":"from matplotlib.colors import LogNorm\nfig = plt.figure(figsize = (12,12))\nax = sns.heatmap(xy, cmap = 'YlGnBu', annot = True, norm = LogNorm())\nax.set_xticklabels(x_axis, rotation = '45')\nax.set_yticklabels(y_axis, rotation = 'horizontal')\n\n# df2['seller_city_name'] = df2['seller_city'].apply(trans)\n# df2['customer_city_name'] = df2['customer_city'].apply(trans)\n","cd4540af":"data\ndata_product_geolocation = data[['seller_city','customer_city','product_category_name','product_category_name_english']]\ndata_product_geolocation_ibitinga = data_product_geolocation[data_product_geolocation['seller_city'] == 'ibitinga']\n# data_product_geolocation_ibitinga = data_product_geolocation_ibitinga[data_product_geolocation_ibitinga['customer_city'] == 'sao paulo']\n\ndata_product_geolocation_ibitinga","a1167b94":"import wordcloud as wc\n\nwords = ' '.join(data_product_geolocation_ibitinga.product_category_name_english)\n\nw = wc.WordCloud(width = 1000, height = 1000, min_font_size = 30, max_words = 300, relative_scaling = 0, background_color='white')\nw.generate(words)\nw.to_file('wordcloud.png')\nplt.axis(\"off\")\nplt.imshow(w)","eb17e4bb":"geoproduct = data[['price','seller_city','customer_city']]\ncity_sales = pd.DataFrame()\ncity_sales = geoproduct.groupby(by = 'seller_city',as_index = False).agg({'price':sum})\ncity_sales.sort_values(by = 'price',ascending= False,inplace = True)\n\ncity_sales\n","6f54ea18":"city_sales = city_sales.reset_index(drop = True)\ncity_sales\ntmp = city_sales.loc[:9]\n# tmp['price'] = tmp['price'].apply(np.log10)\nax = sns.barplot(x = 'seller_city', y = 'price',data = tmp)\nax.set(title = 'the distribution of sales on seller_city')\nax.set_xticklabels(labels = tmp['seller_city'],rotation = 45)\n","99a8140b":"geoproduct = data[['price','seller_city','customer_city']]\n\ncity_sales = geoproduct.groupby(by = 'customer_city',as_index = False).agg({'price':sum})\ncity_sales.sort_values(by = 'price',ascending= False,inplace = True)\n\ncity_sales","6214efcc":"city_sales = city_sales.reset_index(drop = True)\ncity_sales\ntmp = city_sales.loc[:9]\n# tmp['price'] = tmp['price'].apply(np.log10)\nax = sns.barplot(x = 'customer_city', y = 'price',data = tmp)\nax.set(title = 'the distribution of sales on customer_city')\nax.set_xticklabels(labels = tmp['customer_city'],rotation = 45)\n","243120c8":"city_seller_sales = geoproduct.copy()\ncity_seller_sales = city_seller_sales.groupby(by = ['seller_city']).sum()\ncity_seller_sales = city_seller_sales.sort_values(by = 'price',ascending = False)\ncity_seller_sales = city_seller_sales.reset_index(drop = False)\ndft1 = city_seller_sales\ncity_seller_sales = city_seller_sales.loc[:19,]\ncity_seller_sales","4c23cf61":"# city_sales = city_sales.reset_index(drop = True)\n# city_sales\n# tmp = city_sales.loc[:9]\n# # tmp['price'] = tmp['price'].apply(np.log10)\n# ax = sns.barplot(x = 'seller_city', y = 'price',data = tmp)\n# ax.set(title = 'the distribution of sales on seller_city')\n# ax.set_xticklabels(labels = tmp['seller_city'],rotation = 45)\n","b8fe3327":"geoproduct = data[['price','seller_city','customer_city']]\n\ncity_sales = geoproduct.groupby(by = 'customer_city',as_index = False).agg({'price':sum})\ncity_sales.sort_values(by = 'price',ascending= False,inplace = True)\n\ncity_sales","05c36805":"df = geoproduct.copy()\ndf1 = df.groupby(by = ['seller_city']).sum()\ndf = df1.sort_values(by = 'price', ascending = False)\ndf = df.reset_index(drop = False)\nx_axis = df.loc[:9,'seller_city'].to_numpy()\nprint(x_axis)\nprint(geoproduct)\n\ndf = geoproduct.copy()\ndf2 = df.groupby(by = ['customer_city']).sum()\ndf = df2.sort_values(by = 'price', ascending = False)\ndf = df.reset_index(drop = False)\ny_axis = df.loc[:9,'customer_city'].to_numpy()\nprint(y_axis)\n","65d52e89":"xy = [[0 for i in range(len(x_axis))] for i in range(len(x_axis))]\n              \nfor i in range(geoproduct.shape[0]):\n    for ix in range(len(x_axis)):\n        if(x_axis[ix] == geoproduct.loc[i,'seller_city']):\n            for iy in range(len(y_axis)):\n                if(y_axis[iy] == geoproduct.loc[i,'customer_city']):\n                    xy[ix][iy] = xy[ix][iy] + geoproduct.loc[i,'price']\n\nprint(xy)\n","1eeed2f8":"from matplotlib.colors import LogNorm\nfig = plt.figure(figsize = (12,12))\nax = sns.heatmap(xy, cmap = 'YlGnBu', annot = True, norm = LogNorm())\nax.set_xticklabels(x_axis, rotation = '45')\nax.set_yticklabels(y_axis, rotation = 'horizontal')\n\n# df2['seller_city_name'] = df2['seller_city'].apply(trans)\n# df2['customer_city_name'] = df2['customer_city'].apply(trans)\n","0a94bf43":"city_seller_sales = geoproduct.copy()\ncity_seller_sales = city_seller_sales.groupby(by = ['seller_city']).sum()\ncity_seller_sales = city_seller_sales.sort_values(by = 'price',ascending = False)\ncity_seller_sales = city_seller_sales.reset_index(drop = False)\ndft1 = city_seller_sales\ncity_seller_sales = city_seller_sales.loc[:19,]\ncity_seller_sales","7742ff51":"plt.figure(figsize=(10,5),dpi=80)\n\nax = sns.barplot(x = 'seller_city', y = 'price',data = city_seller_sales,label = 'sales', color = 'blue')\nplt.tight_layout()\nax.set_xticklabels(labels = city_seller_sales['seller_city'],rotation = 60)\nplt.legend(loc=\"upper left\")\nplt.twinx()\np=city_seller_sales['price'].cumsum()\/city_seller_sales['price'].sum()\nkey = p[p>0.8].index[0]\nkey_num=city_seller_sales.index.tolist().index(key)\nprint('cities sales excessed 80%',key)\nkey_city = city_seller_sales[:key]\nprint('key city:',key_city)\n \np.plot(color = 'orange',style = '-o',linewidth=1)\nplt.axvline(key_num,color=\"red\",linestyle=\"--\")\nplt.text(key_num+1,p[key]-0.05,'cumulation:%.3f%%'%(p[key]*100),color=\"red\")\nplt.ylabel(\"sales\")\nplt.title(\"sales of each city to total sales(%)\",fontsize=15)","d1de42d7":"city_customer_sales = geoproduct.copy()\ncity_customer_sales = city_customer_sales.groupby(by = ['customer_city']).sum()\ncity_customer_sales = city_customer_sales.sort_values(by = 'price',ascending = False)\ncity_customer_sales = city_customer_sales.reset_index(drop = False)\ncity_customer_sales = city_customer_sales.loc[:19,]\ncity_customer_sales","39b03a25":"plt.figure(figsize=(10,5),dpi=80)\n\nax = sns.barplot(x = 'customer_city', y = 'price',data = city_customer_sales,label = 'sales', color = 'blue')\nplt.tight_layout()\nax.set_xticklabels(labels = city_customer_sales['customer_city'],rotation = 60)\n\nplt.legend(loc=\"upper left\")\nplt.twinx()\np=city_customer_sales['price'].cumsum()\/city_customer_sales['price'].sum()\nkey = p[p>0.8].index[0]\nkey_num=city_customer_sales.index.tolist().index(key)\nprint('cities sales excessed 80%',key)\nkey_city = city_customer_sales[:key]\nprint('key city:',key_city)\n \np.plot(color = 'orange',style = '-o',linewidth=1)\nplt.axvline(key_num,color=\"red\",linestyle=\"--\")\nplt.text(key_num+1,p[key]-0.05,'cumulation:%.3f%%'%(p[key]*100),color=\"red\")\nplt.ylabel(\"sales\")\nplt.title(\"sales of each city to total sales(%)\",fontsize=15)","b221c744":"city_seller_number = data[['seller_city']]\ncity_seller_number['count'] = 1\ncity_seller_number = city_seller_number.groupby('seller_city').sum()\ncity_seller_number = city_seller_number.sort_values(by = 'count', ascending = False)\ncity_seller_number = city_seller_number.reset_index(drop = False)\ndft2 = city_seller_number\ncity_seller_number = city_seller_number.loc[:19,]\ncity_seller_number","554b7ffa":"plt.figure(figsize=(10,5),dpi=80)\n\nax = sns.barplot(x = 'seller_city', y = 'count',data = city_seller_number,label = 'number of stores', color = 'blue')\nplt.tight_layout()\nax.set_xticklabels(labels = city_seller_number['seller_city'],rotation = 60)\n\nplt.legend(loc=\"upper left\")\nplt.twinx()\np=city_seller_number['count'].cumsum()\/city_seller_number['count'].sum()\nkey = p[p>0.8].index[0]\nkey_num=city_seller_number.index.tolist().index(key)\nprint('cities sales excessed 80%',key)\nkey_city = city_seller_number[:key]\nprint('key city:',key_city)\n \np.plot(color = 'orange',style = '-o',linewidth=1)\nplt.axvline(key_num,color=\"red\",linestyle=\"--\")\nplt.text(key_num+1,p[key]-0.05,'cumulation:%.3f%%'%(p[key]*100),color=\"red\")\nplt.ylabel(\"sales\")\nplt.title(\"sales of each city to total sales(%)\",fontsize=15)","0976dbed":"city_customer_number = data[['customer_city']]\ncity_customer_number['count'] = 1\ncity_customer_number = city_customer_number.groupby('customer_city').sum()\ncity_customer_number = city_customer_number.sort_values(by = 'count', ascending = False)\ncity_customer_number = city_customer_number.reset_index(drop = False)\ncity_customer_number = city_customer_number.loc[:19,]\ncity_customer_number","86d72132":"plt.figure(figsize=(10,5),dpi=80)\n\nax = sns.barplot(x = 'customer_city', y = 'count',data = city_customer_number,label = 'number of stores', color = 'blue')\nplt.tight_layout()\nax.set_xticklabels(labels = city_customer_number['customer_city'],rotation = 60)\n\nplt.legend(loc=\"upper left\")\nplt.twinx()\np=city_customer_number['count'].cumsum()\/city_customer_number['count'].sum()\nkey = p[p>0.8].index[0]\nkey_num=city_customer_number.index.tolist().index(key)\nprint('cities sales excessed 80%',key)\nkey_city = city_customer_number[:key]\nprint('key city:',key_city)\n \np.plot(color = 'orange',style = '-o',linewidth=1)\nplt.axvline(key_num,color=\"red\",linestyle=\"--\")\nplt.text(key_num+1,p[key]-0.05,'cumulation:%.3f%%'%(p[key]*100),color=\"red\")\nplt.ylabel(\"sales\")\nplt.title(\"sales of each city to total sales(%)\",fontsize=15)","ee267619":"seller = pd.merge(dft1, dft2, how = 'inner')\nseller['avg_sales'] = seller['price'] \/ seller['count']\nseller\n","8d3d7155":"seller = seller.sort_values(by = 'avg_sales', ascending = False)\nseller = seller.reset_index(drop = False)\nseller = seller.loc[:19,]\nseller","6f62c820":"plt.figure(figsize=(10,5),dpi=80)\n\nax = sns.barplot(x = 'seller_city', y = 'avg_sales',data = seller,label = 'number of stores', color = 'blue')\nplt.tight_layout()\nax.set_xticklabels(labels = seller['seller_city'],rotation = 60)\n\nplt.ylabel(\"sales\")\nplt.title(\"avg_sales per store in cities\",fontsize=15)","16711ccb":"**From the geopraph drawn using zip-code, it came out with some outliers which lies out of Brazil territory.\nSo, the maximum and minimum of latitude and longtitude are utilized to remove the outliers.**","662a8dcd":"**Further, to explore the top sellers and customers by number of products sold\/bought, df1 and df2 are conducted.**","be39a897":"**A histogram is conducted to visualize it. From which, a city stands out - ibitinga sells so much (top 3), but only buys 24 products. So more exploration are conducted in that city.**","461db8e4":"**Some basic data cleaning here, 1) the datasets are merged together 2) duplicates are removed 3) the missing rows are explored**\n\n**The major data is well-organised, with a few missing data. 1) order_delivered_carrier_date:1155  2) Missing rows in order_delivered_customer_date:2386**","a390645c":"**In order to calulate the distance between two places using latitude and longtitude. However, this can be conducted by using some libraries.**","f54d272f":"Two maps are build.","8ce1fe8d":"**Here, a dataset containing geolocation of sellers and cutomers are made in order to explore the relation btween the number of orders and order distance.**","cea53002":"**Below is the map of relationships between number of orders and the distance between sellers and customers.**","dff05b44":"**A wordmap is formed, with category English names on it.**"}}