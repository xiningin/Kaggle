{"cell_type":{"5be44fb6":"code","a9552d64":"code","cee5d2d0":"code","4b442995":"code","ce6087e0":"code","134034e4":"code","5d63ccaa":"code","13adb7cf":"code","e7b6a980":"code","24cf5a09":"code","4a52ec48":"code","44b5469b":"code","3e63bd17":"code","184c07e4":"code","326f8a4d":"code","68b3a573":"code","265eb67d":"code","2fd831ef":"code","a3c2674d":"code","4516f763":"code","48a1d118":"code","912384f5":"code","ebf03f7a":"code","50a66d9a":"code","a6f65657":"code","69b48fbd":"code","35051037":"code","72a814f5":"code","f2147b39":"code","c06248a0":"code","432ed368":"code","4e155c88":"code","4e70cafd":"code","6f42729a":"code","52fd7ff0":"code","251b6cdc":"code","680cd5e3":"code","ff458cd9":"code","e7ae4d3d":"code","62011566":"code","5e561bd5":"code","bc191b5b":"code","c421dce6":"code","84f90fd5":"code","6417de72":"code","012d3fb6":"code","7c6722ec":"code","a0e770fc":"code","9374f987":"code","758179a0":"code","9bcc5962":"code","0c4791c8":"code","22050d79":"code","33e7d581":"code","e807b91b":"code","b1d43324":"code","5b7793fd":"code","79a2926a":"code","5a497e6e":"code","e2b47da8":"code","a8f38ea7":"code","b09d055d":"code","c0d0d89c":"code","75049462":"code","db294313":"code","0f0540ca":"code","fc51f7b3":"code","01451007":"code","a75caf5a":"code","a0e24968":"code","3bccc2c0":"code","8b591a33":"code","e0f1195a":"code","fa86f32e":"markdown","8ce170bb":"markdown","66c7d90e":"markdown","39eac400":"markdown","8f460839":"markdown","6b8ebf87":"markdown","fd9c06b3":"markdown","1d92deda":"markdown","5751215a":"markdown","ea303912":"markdown","805df211":"markdown","e09745cf":"markdown","0439fbd7":"markdown","c6dc51ea":"markdown","b5fa56aa":"markdown","f4447bd7":"markdown","36f9e2b6":"markdown"},"source":{"5be44fb6":"#imports \nimport numpy as np\nimport pandas as pd\nimport sklearn as sk\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom sklearn.metrics import accuracy_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA","a9552d64":"from numpy import argmax\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_curve\nfrom matplotlib import pyplot","cee5d2d0":"#entrada para os dados de treino dos tweets\ntrain_tweets = pd.read_csv('..\/input\/d\/guilhermeguimaraes12\/dataset\/train_tweets.csv')\ntrain_tweets_vectorized_media = pd.read_csv('..\/input\/d\/guilhermeguimaraes12\/dataset\/train_tweets_vectorized_media.csv')\ntrain_tweets_vectorized_text = pd.read_csv('..\/input\/d\/guilhermeguimaraes12\/dataset\/train_tweets_vectorized_text.csv')","4b442995":"#entrada para os dados de teste dos tweets\ntest_tweets = pd.read_csv('..\/input\/d\/guilhermeguimaraes12\/dataset\/test_tweets.csv')\ntest_tweets_vectorized_media = pd.read_csv('..\/input\/d\/guilhermeguimaraes12\/dataset\/test_tweets_vectorized_media.csv')\ntest_tweets_vectorized_text = pd.read_csv('..\/input\/d\/guilhermeguimaraes12\/dataset\/test_tweets_vectorized_text.csv')","ce6087e0":"#entrada para os dados de user\nuser_vectorized_descriptions = pd.read_csv('..\/input\/d\/guilhermeguimaraes12\/dataset\/user_vectorized_descriptions.csv')\nuser_vectorized_profile_images = pd.read_csv('..\/input\/d\/guilhermeguimaraes12\/dataset\/user_vectorized_profile_images.csv')\nusers = pd.read_csv('..\/input\/d\/guilhermeguimaraes12\/dataset\/users.csv')","134034e4":"train_tweets.virality.value_counts()","5d63ccaa":"sns.set_style(\"dark\")\nsns.countplot(data = train_tweets, x = 'virality')","13adb7cf":"train_tweets.head()","e7b6a980":"#passando as informa\u00e7\u00f5es da coluna tweet_attachment_class para novas colunas\none_hot = pd.get_dummies(train_tweets['tweet_attachment_class'])\ntrain_tweets = train_tweets.drop('tweet_attachment_class',axis = 1)\ntrain_tweets = train_tweets.join(one_hot)\ntrain_tweets.head()","24cf5a09":"#passando as informa\u00e7\u00f5es da coluna tweet_has_attachment para novas colunas\none_hot = pd.get_dummies(train_tweets['tweet_has_attachment'])\ntrain_tweets = train_tweets.drop('tweet_has_attachment',axis = 1)\ntrain_tweets = train_tweets.join(one_hot)\ntrain_tweets.head()","4a52ec48":"train_tweets = train_tweets.drop('tweet_topic_ids',axis = 1)","44b5469b":"c= train_tweets[['tweet_id', 'tweet_user_id', 'virality']]","3e63bd17":"train_tweets = train_tweets.drop(['tweet_id', 'tweet_user_id', 'virality'],axis = 1)","184c07e4":"train_tweets.head()","326f8a4d":"#normalizando as features\nx = StandardScaler().fit_transform(train_tweets) ","68b3a573":"col = train_tweets.columns","265eb67d":"normalised_train = pd.DataFrame(x,columns=col)","2fd831ef":"train_tweets_ = pd.concat([c , normalised_train], axis=1)","a3c2674d":"train_tweets_","4516f763":"#passando as informa\u00e7\u00f5es da coluna tweet_has_attachment para novas colunas\none_hot = pd.get_dummies(test_tweets['tweet_has_attachment'])\ntest_tweets = test_tweets.drop('tweet_has_attachment',axis = 1)\ntest_tweets = test_tweets.join(one_hot)\ntest_tweets.head()","48a1d118":"##passando as informa\u00e7\u00f5es da coluna tweet_attachment_class para novas colunas\none_hot = pd.get_dummies(test_tweets['tweet_attachment_class'])\ntest_tweets = test_tweets.drop('tweet_attachment_class',axis = 1)\ntest_tweets = test_tweets.join(one_hot)\ntest_tweets.head()","912384f5":"test_tweets = test_tweets.drop('tweet_topic_ids',axis = 1)","ebf03f7a":"c= test_tweets[['tweet_id', 'tweet_user_id']]","50a66d9a":"test_tweets = test_tweets.drop(['tweet_id', 'tweet_user_id'],axis = 1)","a6f65657":"test_tweets.head()","69b48fbd":"#normalizando as features\nx = StandardScaler().fit_transform(test_tweets)","35051037":"col = test_tweets.columns","72a814f5":"normalised_test = pd.DataFrame(x,columns=col)","f2147b39":"test_tweets_ = pd.concat([c , normalised_test], axis=1)","c06248a0":"test_tweets_","432ed368":"users.head()","4e155c88":"##passando as informa\u00e7\u00f5es da coluna user_has_url para novas colunas\none_hot = pd.get_dummies(users['user_has_url'])\nusers = users.drop('user_has_url',axis = 1)\nusers = users.join(one_hot)\nusers.head()","4e70cafd":"users = users.drop('user_has_location',axis = 1)","6f42729a":"users.head()","52fd7ff0":"c= users[['user_id']]","251b6cdc":"users = users.drop(['user_id'],axis = 1)","680cd5e3":"users.head()","ff458cd9":"#normalizando as features\nx = StandardScaler().fit_transform(users) ","e7ae4d3d":"col = users.columns","62011566":"normalised_test = pd.DataFrame(x,columns=col)","5e561bd5":"users_ = pd.concat([c , normalised_test], axis=1)","bc191b5b":"users_.head()","c421dce6":"train_model = pd.merge(train_tweets_, train_tweets_vectorized_media, on = 'tweet_id',how = 'left' )","84f90fd5":"train_model = pd.merge(train_model, train_tweets_vectorized_text, on = 'tweet_id', how = 'left' )","6417de72":"train_model.shape ","012d3fb6":"test_model = pd.merge(test_tweets_, test_tweets_vectorized_media, on = 'tweet_id', how = 'left' )","7c6722ec":"test_model = pd.merge(test_model, test_tweets_vectorized_text, on = 'tweet_id', how = 'left' )","a0e770fc":"test_model.shape ","9374f987":"user_model = pd.merge(users_, user_vectorized_descriptions, on = 'user_id', how = 'left' )","758179a0":"user_model = pd.merge(user_model, user_vectorized_profile_images, on = 'user_id', how = 'left' )","9bcc5962":"user_model.shape","0c4791c8":"train_model = train_model.rename(columns={'tweet_user_id': 'user_id'})\n\ntrain_model = pd.merge(train_model, users_, on = 'user_id', how = 'left' )\ntrain_model.shape ","22050d79":"test_model = test_model.rename(columns={'tweet_user_id': 'user_id'})\n\ntest_model = pd.merge(test_model, users_, on = 'user_id', how = 'left' )\ntest_model.shape ","33e7d581":"train_model.head()","e807b91b":"train_model = train_model.fillna(0)","b1d43324":"test_model = test_model.fillna(0)\ntest_model.shape","5b7793fd":"X = train_model.drop(['tweet_id', 'user_id','media_id', 'virality'],axis = 1)\ny = train_model.virality\nX.shape, y.shape","79a2926a":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=12,stratify=y)","5a497e6e":"#fazendo uma inst\u00e2ncia do modelo\npca = PCA(.95)","e2b47da8":"pca.fit(X_train)","a8f38ea7":"X_train = pca.transform(X_train)\nX_test = pca.transform(X_test)","b09d055d":"pca.n_components_","c0d0d89c":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier","75049462":"DecisionTree = DecisionTreeClassifier(random_state = 9847522)","db294313":"DecisionTree.fit(X_train, y_train)","0f0540ca":"y_pred = DecisionTree.predict(X_test)","fc51f7b3":"DecisionTree.score(X_test, y_test)","01451007":"#Tirando duplicatas\ntest_ = test_model.drop_duplicates(subset=[\"tweet_id\"])\ntest_.shape","a75caf5a":"id = test_.tweet_id\ntest_ = test_.drop(['tweet_id', 'user_id','media_id'],axis = 1)","a0e24968":"test_.shape","3bccc2c0":"test_ = pca.transform(test_)","8b591a33":"y_pred_teste= DecisionTree.predict(test_)","e0f1195a":"#Gerando o .CSV da resposta final\nresposta = pd.DataFrame(data = {\"tweet_id\": id, \"virality\": y_pred_teste})\nresposta.to_csv(\"content\\resposta_Task05.csv\", index = False)\nresposta.head()","fa86f32e":"## Unificando o dataset\n\nEstamos juntando os bancos de dados com a fun\u00e7\u00e3o merge a fim de termos uma an\u00e1lise mais direta e pr\u00e1tica.","8ce170bb":"# Modelo\n\nPor fim utilizamos a \u00e1rvore de decis\u00e3o para fazer a predi\u00e7\u00e3o no nosso modelo. A \u00e1vore de decis\u00e3o \u00e9 um m\u00e9todo n\u00e3o-param\u00e9trico de aprendizado supervisionado usado para regress\u00e3o e classifica\u00e7\u00e3o. \nO objetivo \u00e9 a predi\u00e7\u00e3o do valor de uma vari\u00e1vel target. Neste caso, nosso target \u00e9 a vari\u00e1vel 'virality'. Ela possui 5 categorias que v\u00e3o de 1 a 5 medindo o n\u00edvel de viralidade que o tweet possui.\n","66c7d90e":"### Train","39eac400":"Preenchendo NA's com 0\n","8f460839":"## Tweets","6b8ebf87":"## Transformando as features ","fd9c06b3":"### User","1d92deda":"### Train","5751215a":"A coluna Virality \u00e9 destribuida entre as 5 categorias:\n- 1 com 13632 tweets\n- 2 com  8889 tweets\n- 3 com  4646 tweets\n- 4 com  1135 tweets\n- 5 com  1323 tweets","ea303912":"# An\u00e1lise de Componentes Principais (PCA)\n\nA An\u00e1lise de Componentes Principais (PCA) \u00e9 uma t\u00e9cnica que transforma linearmente p vari\u00e1veis correlacionadas em k vari\u00e1veis n\u00e3o-correlacionadas (onde k<p) que explicam parte substancial das informa\u00e7\u00f5es originais.\n\nOs objetivos principais s\u00e3o:\n\n* 1) Redu\u00e7\u00e3o da dimensionalidade dos dados sem alterar a varia\u00e7\u00e3o;\n* 2) Obter combina\u00e7\u00f5es interpret\u00e1veis das vari\u00e1veis originais;\n* 3) Descrever e compreender a estrutura de correla\u00e7\u00e3o das vari\u00e1veis originais.\n\nDecidimos utilizar esse m\u00e9todo devido ao grande volume de dados. ","805df211":"### Test","e09745cf":"### Train\/Test + Users","0439fbd7":"## Users","c6dc51ea":"# Leitura dos dados","b5fa56aa":"# An\u00e1lise do Target","f4447bd7":"### Users","36f9e2b6":"### Test"}}