{"cell_type":{"3d83cf6e":"code","8ff979b2":"code","b66afbc3":"code","7f004dd4":"code","3d1032d3":"code","bc7c90db":"code","4f352b22":"code","b6178f8c":"code","4d29f352":"code","541b5cd0":"code","6c23f3ee":"code","bd572003":"code","e73506b8":"code","d21bd93a":"code","8601f981":"code","59d2b794":"code","a292b9be":"markdown","8277533a":"markdown","715c22da":"markdown","71ed7de4":"markdown","1ef70e9b":"markdown","6e7a6623":"markdown","caa71c95":"markdown"},"source":{"3d83cf6e":"!pip install git+https:\/\/github.com\/glmcdona\/LuxPythonEnvGym.git@main\n#!pip install git+https:\/\/github.com\/glmcdona\/LuxPythonEnvGym.git\n!pip install kaggle-environments -U","8ff979b2":"import torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice\nprint(device)","b66afbc3":"#","7f004dd4":"%%writefile agent_policy.py\nfrom luxai2021.game.match_controller import ActionSequence\nimport sys\nimport time\nfrom functools import partial  # pip install functools\nimport copy\n\nimport numpy as np\nimport gym\nfrom gym import spaces\nimport numpy as np\nfrom collections import OrderedDict\n\nfrom luxai2021.env.agent import Agent\nfrom luxai2021.game.actions import *\nfrom luxai2021.game.game_constants import GAME_CONSTANTS\nfrom luxai2021.game.position import Position\nimport torch as th\nfrom torch import nn\nimport skimage.measure\n\nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n\nimport random\n\n# Custom policy network for the shared feature extraction forward\n# https:\/\/stable-baselines3.readthedocs.io\/en\/master\/guide\/custom_policy.html\nclass CustomCombinedExtractor(BaseFeaturesExtractor):\n    def __init__(self, observation_space: gym.spaces.Dict):\n        # We do not know features-dim here before going over all the items,\n        # so put something dummy for now. PyTorch requires calling\n        # nn.Module.__init__ before adding modules\n        super(CustomCombinedExtractor, self).__init__(observation_space, features_dim=1)\n\n        extractors = {}\n\n        total_concat_size = 0\n        # We need to know size of the output of this extractor,\n        # so go over all the spaces and compute output feature sizes\n        for key, subspace in observation_space.spaces.items():\n            if key.startswith(\"compressed_map\"):\n                input_channels = subspace.shape[0]\n                input_width = subspace.shape[1]\n                input_height = subspace.shape[2]\n                filter_count = 32\n\n                extractors[key] = nn.Sequential(\n                    # 3x3x[num of map layers]. Largest map goes from [32x32xNCHANNELS] down to [32x32xNFILTERS]\n                    # Num parameters for layer = 3x3x11x32 = 3,168\n                    #nn.Conv3d(subspace.shape[0], filter_count, kernel_size=(3,3,input_channels), padding=(1,1,0)), # (1,7,7,20) -> (7,7,32)\n                    nn.Conv2d(input_channels, filter_count, kernel_size=(3,3), padding=(1,1)), # (13,32,32) -> (32,32,32)\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)), # (32,32,32) -> (32,32,32)\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)), # (32,32,32) -> (32,32,32)\n                    nn.ReLU(),\n                    nn.MaxPool2d (kernel_size=(2,2)), # largest map ((32,32,32,1) -> (32,16,16,1)) or smallest map (12x12) -> (6x6)\n\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)),\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)),\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)),\n                    nn.ReLU(),\n                    nn.MaxPool2d (kernel_size=(2,2)), # largest map ((32,16,16,1) -> (32,8,8,1)) or smallest map (12x12xNFILTERS) -> (3x3xNFILTERS)\n\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)),\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)),\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)),\n                    nn.ReLU(),\n                    nn.Flatten(),\n                )\n                #print(extractors[key])\n                #print(\"Flattened size from CNN: %i\" % ((input_width \/\/ 2 \/\/ 2) * (input_height \/\/ 2 \/\/ 2) * filter_count))\n                #total_concat_size += (input_width \/\/ 2 \/\/ 2) * (input_height \/\/ 2 \/\/ 2) * filter_count\n                \n                total_concat_size += np.prod(extractors[key]( th.autograd.Variable(th.rand(1, *subspace.shape)) ).shape)\n                \n                # Test output size\n                #print(\"Actual output size from test:\")\n                #print(extractors[key]( th.autograd.Variable(th.rand(1, *subspace.shape)) ).shape)\n            elif key.startswith(\"map\") and isinstance(subspace, spaces.Box):\n                # Downsample the map. Assumes it is 32x32 for the largest map support. The smallest mapsize of 12x12 is commended for information sake.\n                #NOTE: Number of layers are in channels subspace.shape[0]\n                input_channels = subspace.shape[0]\n                input_width = subspace.shape[1]\n                input_height = subspace.shape[2]\n                \n                filter_count = 32\n                #print(subspace.shape)\n\n                extractors[key] = nn.Sequential(\n                    # 3x3x[num of map layers]. Largest map goes from [32x32xNCHANNELS] down to [32x32xNFILTERS]\n                    # Num parameters for layer = 3x3x11x32 = 3,168\n                    #nn.Conv3d(subspace.shape[0], filter_count, kernel_size=(3,3,input_channels), padding=(1,1,0)), # (1,32,32,13) -> (32,32,32,1)\n                    nn.Conv2d(input_channels, filter_count, kernel_size=(3,3), padding=(1,1)), # (13,32,32) -> (32,32,32)\n                    nn.ReLU(),\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)), # (32,32,32) -> (32,32,32)\n                    nn.ReLU(),\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)), # (32,32,32) -> (32,32,32)\n                    nn.ReLU(),\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)), # (13,32,32) -> (32,32,32)\n                    nn.ReLU(),\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)), # (32,32,32) -> (32,32,32)\n                    nn.ReLU(),\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)), # (32,32,32) -> (32,32,32)\n                    nn.ReLU(),\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)), # (13,32,32) -> (32,32,32)\n                    nn.ReLU(),\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)), # (32,32,32) -> (32,32,32)\n                    nn.ReLU(),\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)), # (32,32,32) -> (32,32,32)\n                    nn.ReLU(),\n                    nn.MaxPool2d (kernel_size=(2,2)), # largest map ((32,32,32,1) -> (32,16,16,1)) or smallest map (12x12) -> (6x6)\n\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)),\n                    nn.ReLU(),\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)),\n                    nn.ReLU(),\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)),\n                    nn.ReLU(),\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)),\n                    nn.ReLU(),\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)),\n                    nn.ReLU(),\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)),\n                    nn.ReLU(),\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)),\n                    nn.ReLU(),\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)),\n                    nn.ReLU(),\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)),\n                    nn.ReLU(),\n                    nn.MaxPool2d (kernel_size=(2,2)), # largest map ((32,16,16,1) -> (32,8,8,1)) or smallest map (12x12xNFILTERS) -> (3x3xNFILTERS)\n\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)),\n                    nn.ReLU(),\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)),\n                    nn.ReLU(),\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)),\n                    nn.ReLU(),\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)),\n                    nn.ReLU(),\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)),\n                    nn.ReLU(),\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)),\n                    nn.ReLU(),\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)),\n                    nn.ReLU(),\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)),\n                    nn.ReLU(),\n                    nn.Conv2d(filter_count, filter_count, kernel_size=(3,3), padding=(1,1)),\n                    nn.ReLU(),\n                    nn.Flatten(),\n                )\n                #print(extractors[key])\n                #print(\"Flattened size from CNN: %i\" % ((input_width \/\/ 2 \/\/ 2) * (input_height \/\/ 2 \/\/ 2) * filter_count))\n                total_concat_size += (input_width \/\/ 2 \/\/ 2) * (input_height \/\/ 2 \/\/ 2) * filter_count\n                \n                # Test output size\n                #print(\"Actual output size from test:\")\n                #print(extractors[key]( th.autograd.Variable(th.rand(1, *subspace.shape)) ).shape)\n                \n            elif isinstance(subspace, spaces.Box):\n                # Run through a simple MLP\n                extractors[key] = nn.Flatten()\n                total_concat_size += np.prod(subspace.shape) # Multiply all the axes together to get total size?\n            elif isinstance(subspace, spaces.Discrete):\n                # Run through a simple MLP\n                extractors[key] = nn.Flatten()\n                total_concat_size += subspace.n\n\n        self.extractors = nn.ModuleDict(extractors)\n\n        # Update the features dim manually\n        self._features_dim = total_concat_size\n\n    def forward(self, observations) -> th.Tensor:\n        encoded_tensor_list = []\n        # self.extractors contain nn.Modules that do all the processing.\n        for key, extractor in self.extractors.items():\n            encoded_tensor_list.append(extractor(observations[key]))\n        # Return a (B, self._features_dim) PyTorch tensor, where B is batch dimension.\n        return th.cat(encoded_tensor_list, dim=1)\n\n\n\n# https:\/\/codereview.stackexchange.com\/questions\/28207\/finding-the-closest-point-to-a-list-of-points\ndef closest_node(node, nodes):\n    dist_2 = np.sum((nodes - node) ** 2, axis=1)\n    return np.argmin(dist_2)\ndef furthest_node(node, nodes):\n    dist_2 = np.sum((nodes - node) ** 2, axis=1)\n    return np.argmax(dist_2)\n\n\ndef compressed_map_observation(map, x, y, map_local_width=21, pool_size=3):\n    # Creates a compressed pair of map observations oriented around the unit\n    # Global: (27x27) -> sum_pool(3)-> (9x9) grid summing each region oriented around the unit\n    # Local: (27x27) -> crop() -> (9x9) map around unit\n    \n    # Input: (32x32) (1024 values)\n    # Output: (9x9x2) (162 values)\n    \n    map = map.copy()\n    \n    # Create the locally-centered map\n    map_local = np.zeros_like(map, shape=(map_local_width, map_local_width))\n    \n    # Brutal map copy only valid in-bound ranges. Sorry for this horrible code.\n    map_start_x_desired = int(x-(map_local_width-1)\/2)\n    map_start_x_clipped = max(map_start_x_desired,0)\n    map_end_x_desired = int(x+(map_local_width-1)\/2)\n    map_end_x_clipped = min(map_end_x_desired,map.shape[0]-1)\n\n    map_start_y_desired = int(y-(map_local_width-1)\/2)\n    map_start_y_clipped = max(map_start_y_desired,0)\n    map_end_y_desired = int(y+(map_local_width-1)\/2)\n    map_end_y_clipped = min(map_end_y_desired,map.shape[1]-1)\n\n    local_map_x_start = map_start_x_clipped - map_start_x_desired\n    local_map_x_end = local_map_x_start + map_end_x_clipped - map_start_x_clipped\n\n    local_map_y_start = map_start_y_clipped - map_start_y_desired\n    local_map_y_end = local_map_y_start + map_end_y_clipped - map_start_y_clipped\n\n    # The outer-most layer has all the resources in that direction regardless of how far it is. Use mean repeated folding.\n    for i in range(0,map_start_x_clipped):\n        # Fold this column\n        map[i+1,:] = map[i] + map[i+1]\n        map[i,:] = 0\n    for i in range(map.shape[0]-1,map_end_x_clipped-1,-1):\n        # Fold this column\n        map[i-1,:] = map[i] + map[i-1]\n        map[i,:] = 0\n    \n    for i in range(0,map_start_y_clipped+1):\n        # Fold this column\n        map[:,i+1] = map[:,i] + map[:,i+1]\n        map[:,i] = 0\n    for i in range(map.shape[1]-1,map_end_y_clipped-1,-1):\n        # Fold this column\n        map[:,i-1] = map[:,i] + map[:,i-1]\n        map[:,i] = 0\n    \n    map_local[\n        local_map_x_start : local_map_x_end + 1,\n        local_map_y_start : local_map_y_end + 1\n    ] = map[\n        map_start_x_clipped : map_end_x_clipped + 1,\n        map_start_y_clipped : map_end_y_clipped + 1\n    ]\n    \n    # Sum-pool the local map into the (output_width x output_width) grid\n    final_width = int(map_local_width \/ pool_size)\n    result = np.zeros_like(map, shape=(2, final_width, final_width))\n    result[0,:,:] = skimage.measure.block_reduce(map_local, (pool_size,pool_size), np.sum) \/ 15.0 # 28x28 -> 7x7\n    \n    # Copy the local 7x7 non-pooled map as well\n    result[1,:,:] = map_local[\n        int((map_local_width - 1) \/ 2) - int((final_width - 1) \/ 2) : int((map_local_width - 1) \/ 2) + int((final_width - 1) \/ 2) + 1,\n        int((map_local_width - 1) \/ 2) - int((final_width - 1) \/ 2) : int((map_local_width - 1) \/ 2) + int((final_width - 1) \/ 2) + 1,\n    ]\n    \n    return result\n    \n\n\n# Constants\nOFFSET_OPPONENT = 10 # Offset added to oppontent types\nCITIES = 2 # Special unit type code for cities\n\n\n\ndef smart_transfer_to_nearby(game, team, unit_id, unit, target_type_restriction=None, **kwarg):\n    \"\"\"\n    Smart-transfers from the specified unit to a nearby neighbor. Prioritizes any\n    nearby carts first, then any worker. Transfers the resource type which the unit\n    has most of. Picks which cart\/worker based on choosing a target that is most-full\n    but able to take the most amount of resources.\n\n    Args:\n        team ([type]): [description]\n        unit_id ([type]): [description]\n\n    Returns:\n        Action: Returns a TransferAction object, even if the request is an invalid\n                transfer. Use TransferAction.is_valid() to check validity.\n    \"\"\"\n\n    # Calculate how much resources could at-most be transferred\n    resource_type = None\n    resource_amount = 0\n    target_unit = None\n\n    if unit != None:\n        for type, amount in unit.cargo.items():\n            if amount > resource_amount:\n                resource_type = type\n                resource_amount = amount\n\n        # Find the best nearby unit to transfer to\n        unit_cell = game.map.get_cell_by_pos(unit.pos)\n        adjacent_cells = game.map.get_adjacent_cells(unit_cell)\n\n        \n        for c in adjacent_cells:\n            for id, u in c.units.items():\n                # Apply the unit type target restriction\n                if target_type_restriction == None or u.type == target_type_restriction:\n                    if u.team == team:\n                        # This unit belongs to our team, set it as the winning transfer target\n                        # if it's the best match.\n                        if target_unit is None:\n                            target_unit = u\n                        else:\n                            # Compare this unit to the existing target\n                            if target_unit.type == u.type:\n                                # Transfer to the target with the least capacity, but can accept\n                                # all of our resources\n                                if( u.get_cargo_space_left() >= resource_amount and \n                                    target_unit.get_cargo_space_left() >= resource_amount ):\n                                    # Both units can accept all our resources. Prioritize one that is most-full.\n                                    if u.get_cargo_space_left() < target_unit.get_cargo_space_left():\n                                        # This new target it better, it has less space left and can take all our\n                                        # resources\n                                        target_unit = u\n                                    \n                                elif( target_unit.get_cargo_space_left() >= resource_amount ):\n                                    # Don't change targets. Current one is best since it can take all\n                                    # the resources, but new target can't.\n                                    pass\n                                    \n                                elif( u.get_cargo_space_left() > target_unit.get_cargo_space_left() ):\n                                    # Change targets, because neither target can accept all our resources and \n                                    # this target can take more resources.\n                                    target_unit = u\n                            elif u.type == Constants.UNIT_TYPES.CART:\n                                # Transfer to this cart instead of the current worker target\n                                target_unit = u\n    \n    # Build the transfer action request\n    target_unit_id = None\n    if target_unit is not None:\n        target_unit_id = target_unit.id\n\n        # Update the transfer amount based on the room of the target\n        if target_unit.get_cargo_space_left() < resource_amount:\n            resource_amount = target_unit.get_cargo_space_left()\n    \n    return TransferAction(team, unit_id, target_unit_id, resource_type, resource_amount)\n\n########################################################################################################################\n# This is the Agent that you need to design for the competition\n########################################################################################################################\nclass AgentPolicy(Agent):\n    def __init__(self, mode=\"train\", model=None) -> None:\n        \"\"\"\n        Arguments:\n            mode: \"train\" or \"inference\", which controls if this agent is for training or not.\n            model: The pretrained model, or if None it will operate in training mode.\n        \"\"\"\n        super().__init__()\n        self.model = model\n        self.mode = mode\n        self.stats = None\n        self.stats_last_game = None\n\n        # Define action and observation space\n        # They must be gym.spaces objects\n        # Example when using discrete actions:\n        '''\n        self.actionSpaceMapUnits = [\n            partial(MoveAction, direction=Constants.DIRECTIONS.CENTER),  # This is the do-nothing action\n            partial(MoveAction, direction=Constants.DIRECTIONS.NORTH),\n            partial(MoveAction, direction=Constants.DIRECTIONS.WEST),\n            partial(MoveAction, direction=Constants.DIRECTIONS.SOUTH),\n            partial(MoveAction, direction=Constants.DIRECTIONS.EAST),\n            partial(ActionSequence, actions=[\n                partial(MoveAction, direction=Constants.DIRECTIONS.NORTH),\n                partial(MoveAction, direction=Constants.DIRECTIONS.NORTH),\n                partial(MoveAction, direction=Constants.DIRECTIONS.NORTH),\n                partial(MoveAction, direction=Constants.DIRECTIONS.NORTH),\n                partial(MoveAction, direction=Constants.DIRECTIONS.NORTH),\n            ]),\n            partial(ActionSequence, actions=[\n                partial(MoveAction, direction=Constants.DIRECTIONS.WEST),\n                partial(MoveAction, direction=Constants.DIRECTIONS.WEST),\n                partial(MoveAction, direction=Constants.DIRECTIONS.WEST),\n                partial(MoveAction, direction=Constants.DIRECTIONS.WEST),\n                partial(MoveAction, direction=Constants.DIRECTIONS.WEST),\n            ]),\n            partial(ActionSequence, actions=[\n                partial(MoveAction, direction=Constants.DIRECTIONS.SOUTH),\n                partial(MoveAction, direction=Constants.DIRECTIONS.SOUTH),\n                partial(MoveAction, direction=Constants.DIRECTIONS.SOUTH),\n                partial(MoveAction, direction=Constants.DIRECTIONS.SOUTH),\n                partial(MoveAction, direction=Constants.DIRECTIONS.SOUTH),\n            ]),\n            partial(ActionSequence, actions=[\n                partial(MoveAction, direction=Constants.DIRECTIONS.EAST),\n                partial(MoveAction, direction=Constants.DIRECTIONS.EAST),\n                partial(MoveAction, direction=Constants.DIRECTIONS.EAST),\n                partial(MoveAction, direction=Constants.DIRECTIONS.EAST),\n                partial(MoveAction, direction=Constants.DIRECTIONS.EAST),\n            ]),\n            smart_transfer_to_nearby, # Transfer to nearby\n            SpawnCityAction,\n            #PillageAction,\n        ]\n        '''\n        self.actionSpaceMapUnits = [\n            partial(MoveAction, direction=Constants.DIRECTIONS.CENTER),  # This is the do-nothing action\n            partial(MoveAction, direction=Constants.DIRECTIONS.NORTH),\n            partial(MoveAction, direction=Constants.DIRECTIONS.WEST),\n            partial(MoveAction, direction=Constants.DIRECTIONS.SOUTH),\n            partial(MoveAction, direction=Constants.DIRECTIONS.EAST),\n            smart_transfer_to_nearby, # Transfer to nearby\n            SpawnCityAction,\n            #PillageAction,\n        ]\n        self.actionSpaceMapCities = [\n            SpawnWorkerAction,\n            SpawnCartAction,\n            ResearchAction,\n        ]\n\n        self.action_space = spaces.Discrete(max(len(self.actionSpaceMapUnits), len(self.actionSpaceMapCities)))\n        \n        self.map_layers = {\n            Constants.UNIT_TYPES.CART + OFFSET_OPPONENT: 0,\n            Constants.UNIT_TYPES.WORKER + OFFSET_OPPONENT: 1,\n            CITIES + OFFSET_OPPONENT: 2,\n            Constants.UNIT_TYPES.CART: 3,\n            Constants.UNIT_TYPES.WORKER: 4,\n            CITIES: 5,\n            \"UnitCargoWood\": 6,\n            \"UnitCargoCoal\": 7,\n            \"UnitCargoUranium\": 8,\n            \"UnitOrCityFuelValue\": 9,\n            \"Cooldown\": 10,\n            \"RoadLevel\": 11,\n            Constants.RESOURCE_TYPES.WOOD: 12,\n            Constants.RESOURCE_TYPES.COAL: 13,\n            Constants.RESOURCE_TYPES.URANIUM: 14,\n            \"ActiveObjectPos\": 15,\n            \"ActiveObjectGradX\": 16,\n            \"ActiveObjectGradY\": 17,\n            \"ActiveObjectDist\": 18,\n            \"ActiveObjectIsWorker\": 19,\n            \"ActiveObjectIsCart\": 20,\n            \"ActiveObjectIsCityTile\": 21,\n            \"GameTimeToNight\" : 22,\n            \"GameTimeToDay\" : 23,\n            \"GamePercentDone\" : 24,\n            \"GameResearch\" : 25,\n            \"GameResearchCoal\" : 26,\n            \"GameResearchUranium\" : 27,\n        }\n\n        self.observation_space_dict = {\n            # Map of the game, organize as maximum map size (num_layersx32x32)\n            \"map\": spaces.Box(low=-1, high=1, shape=(len(self.map_layers), 32, 32), dtype=np.float16) # 32x32x13 = 13,312\n        }\n\n        self.observation_space = spaces.Dict(self.observation_space_dict)\n\n        self.object_nodes = {}\n\n    def get_agent_type(self):\n        \"\"\"\n        Returns the type of agent. Use AGENT for inference, and LEARNING for training a model.\n        \"\"\"\n        if self.mode == \"train\":\n            return Constants.AGENT_TYPE.LEARNING\n        else:\n            return Constants.AGENT_TYPE.AGENT\n    \n    def get_observation(self, game, unit, city_tile, team, is_new_turn):\n        \"\"\"\n        Implements getting a observation from the current game for this unit or city\n        \"\"\"\n        # Map layer definition\n        unit_type_string_map = {\n            Constants.UNIT_TYPES.WORKER: \"WORKER\",\n            Constants.UNIT_TYPES.CART: \"CART\",\n        }\n        \n        if is_new_turn:\n            # It's a new turn this event. This flag is set True for only the first observation from each turn.\n            # Update any per-turn fixed observation space that doesn't change per unit\/city controlled.\n            self.obs = OrderedDict(\n                [\n                    (\"map\", np.zeros((len(self.map_layers),32,32))),\n                ]\n            )\n            \n            # Update night and day states\n            day_length = game.configs[\"parameters\"][\"DAY_LENGTH\"]\n            night_length = game.configs[\"parameters\"][\"NIGHT_LENGTH\"]\n            cycle_length = day_length + night_length\n            self.obs[\"map\"][self.map_layers[\"GamePercentDone\"],:,:] = np.ones((32,32)) * (game.state[\"turn\"] \/ GAME_CONSTANTS[\"PARAMETERS\"][\"MAX_DAYS\"])\n            if game.is_night():\n                self.obs[\"map\"][self.map_layers[\"GameTimeToNight\"],:,:] = np.zeros((32,32))\n                self.obs[\"map\"][self.map_layers[\"GameTimeToDay\"],:,:] = np.ones((32,32)) * (((game.state[\"turn\"] % cycle_length) - day_length) \/ night_length)\n            else:\n                self.obs[\"map\"][self.map_layers[\"GameTimeToNight\"],:,:] = np.ones((32,32)) * (game.state[\"turn\"] % cycle_length) \/ day_length\n                self.obs[\"map\"][self.map_layers[\"GameTimeToDay\"],:,:] = np.zeros((32,32))\n\n            # Update research state\n            self.obs[\"map\"][self.map_layers[\"GameResearch\"],:,:] = np.ones((32,32)) * game.state[\"teamStates\"][team][\"researchPoints\"] \/ 200.0\n            self.obs[\"map\"][self.map_layers[\"GameResearchCoal\"],:,:] = np.ones((32,32)) * float(game.state[\"teamStates\"][team][\"researched\"][\"coal\"])\n            self.obs[\"map\"][self.map_layers[\"GameResearchUranium\"],:,:] = np.ones((32,32)) * float(game.state[\"teamStates\"][team][\"researched\"][\"uranium\"])\n            \n            # Populate unit layer\n            for t in [team, (team + 1) % 2]:\n                for u in game.state[\"teamStates\"][t][\"units\"].values():\n                    layer = u.type\n                    if u.team != team:\n                        layer += OFFSET_OPPONENT\n                    \n                    if layer in self.map_layers:\n                        # Add 0.25 for each unit on each spot. This allows for up to 4 overlapping units of the same time on the same tile (on citytiles)\n                        value = min(\n                            self.obs[\"map\"][self.map_layers[layer]][u.pos.x][u.pos.y] + 0.25,\n                            1.0\n                        )\n                        self.obs[\"map\"][self.map_layers[layer]][u.pos.x][u.pos.y] = value\n\n                    # Unit storage of each resource as % of total\n                    self.obs[\"map\"][self.map_layers[\"UnitCargoWood\"]][u.pos.x][u.pos.y] = (\n                        u.cargo[\"wood\"] \/ GAME_CONSTANTS[\"PARAMETERS\"][\"RESOURCE_CAPACITY\"][unit_type_string_map[u.type]]\n                    )\n                    self.obs[\"map\"][self.map_layers[\"UnitCargoCoal\"]][u.pos.x][u.pos.y] = (\n                        u.cargo[\"coal\"] \/ GAME_CONSTANTS[\"PARAMETERS\"][\"RESOURCE_CAPACITY\"][unit_type_string_map[u.type]]\n                    )\n                    self.obs[\"map\"][self.map_layers[\"UnitCargoUranium\"]][u.pos.x][u.pos.y] = (\n                        u.cargo[\"uranium\"] \/ GAME_CONSTANTS[\"PARAMETERS\"][\"RESOURCE_CAPACITY\"][unit_type_string_map[u.type]]\n                    )\n\n                    # Unit fuel value, overriden by city tiles later\n                    self.obs[\"map\"][self.map_layers[\"UnitOrCityFuelValue\"]][u.pos.x][u.pos.y] = min(\n                        u.get_cargo_fuel_value() \/ (GAME_CONSTANTS[\"PARAMETERS\"][\"LIGHT_UPKEEP\"][\"CITY\"] * 100 ),\n                        1.0\n                    )\n\n                    # Set cooldown, will be overriden by city tile cooldowns\n                    self.obs[\"map\"][self.map_layers[\"Cooldown\"]][u.pos.x][u.pos.y] = (\n                        u.cooldown \/ GAME_CONSTANTS[\"PARAMETERS\"][\"UNIT_ACTION_COOLDOWN\"][unit_type_string_map[u.type]]\n                    )\n\n\n            # Populate city layer\n            for c in game.cities.values():\n                # Load the layer for this city\n                layer = CITIES\n                if c.team != t:\n                    layer += OFFSET_OPPONENT\n\n                # Iterate the citytiles\n                for cc in c.city_cells:\n                    ct = cc.city_tile\n\n                    # Mark this city tile location\n                    if layer in self.map_layers:\n                        self.obs[\"map\"][self.map_layers[layer]][cc.pos.x][cc.pos.y] = 1 # Mark a city tile here\n\n                    # City fuel value, overides unit fuel\n                    # City fuel as % of upkeep for 100 turns\n                    if c.team == t:\n                        self.obs[\"map\"][self.map_layers[\"UnitOrCityFuelValue\"]][cc.pos.x][cc.pos.y] = min(\n                            c.fuel \/ (c.get_light_upkeep() * 100.0),\n                            1.0\n                        )\n                        \n                        # Set citytile cooldown, overrides unit cooldowns on same tile\n                        self.obs[\"map\"][self.map_layers[\"Cooldown\"]][cc.pos.x][cc.pos.y] = ct.cooldown \/ GAME_CONSTANTS[\"PARAMETERS\"][\"CITY_ACTION_COOLDOWN\"]\n\n            # Populate resources layer\n            scaling_by_resource = {\n                Constants.RESOURCE_TYPES.WOOD: 800,\n                Constants.RESOURCE_TYPES.COAL: 300,\n                Constants.RESOURCE_TYPES.URANIUM: 200,\n            }\n            for c in game.map.resources:\n                if c.resource.type in self.map_layers:\n                    self.obs[\"map\"][self.map_layers[c.resource.type]][c.pos.x][c.pos.y] = min(\n                        c.resource.amount \/ scaling_by_resource[c.resource.type],\n                        1.0\n                    )\n            \n            # Populate the road layer\n            for c in game.cells_with_roads:\n                self.obs[\"map\"][self.map_layers[\"RoadLevel\"]][c.pos.x][c.pos.y] = min(\n                        c.road \/ game.configs[\"parameters\"][\"MAX_ROAD\"],\n                        1.0\n                    )\n        \n        # Helper variables\n        active = None\n        city = None\n        if city_tile != None:\n            active = city_tile\n            city = game.cities[city_tile.city_id]\n        unit_type_string = None\n        \n        if unit != None:\n            active = unit\n            unit_type_string = unit_type_string_map[unit.type]\n        \n        \n        # Fill in the current actioning unit type\n        self.obs[\"map\"][self.map_layers[\"ActiveObjectIsWorker\"],:,:] = np.zeros((32,32))\n        self.obs[\"map\"][self.map_layers[\"ActiveObjectIsCart\"],:,:] = np.zeros((32,32))\n        self.obs[\"map\"][self.map_layers[\"ActiveObjectIsCityTile\"],:,:] = np.zeros((32,32))\n\n        type = None\n        if unit is not None:\n            if unit.type == Constants.UNIT_TYPES.WORKER:\n                # Worker\n                self.obs[\"map\"][self.map_layers[\"ActiveObjectIsWorker\"],:,:] = np.ones((32,32))\n            else:\n                self.obs[\"map\"][self.map_layers[\"ActiveObjectIsCart\"],:,:] = np.ones((32,32))\n        if city_tile is not None:\n            self.obs[\"map\"][self.map_layers[\"ActiveObjectIsCityTile\"],:,:] = np.ones((32,32))\n        \n        \n        \n        # Active object position and gradients\n        self.obs[\"map\"][self.map_layers[\"ActiveObjectPos\"],active.pos.x,active.pos.y] = 1.0\n        x, y = np.ogrid[0:32, 0:32]\n        self.obs[\"map\"][self.map_layers[\"ActiveObjectGradX\"],:,:] = (x - active.pos.x) \/ 32\n        self.obs[\"map\"][self.map_layers[\"ActiveObjectGradY\"],:,:] = (y - active.pos.y) \/ 32\n        self.obs[\"map\"][self.map_layers[\"ActiveObjectDist\"],:,:] = (np.abs(y - active.pos.y) + np.abs(x - active.pos.x)) \/ 32\n\n        \n        return self.obs\n\n    def action_code_to_action(self, action_code, game, unit=None, city_tile=None, team=None):\n        \"\"\"\n        Takes an action in the environment according to actionCode:\n            actionCode: Index of action to take into the action array.\n        Returns: An action.\n        \"\"\"\n        # Map actionCode index into to a constructed Action object\n        try:\n            x = None\n            y = None\n            if city_tile is not None:\n                x = city_tile.pos.x\n                y = city_tile.pos.y\n            elif unit is not None:\n                x = unit.pos.x\n                y = unit.pos.y\n            \n            if city_tile != None:\n                action =  self.actionSpaceMapCities[action_code%len(self.actionSpaceMapCities)](\n                    game=game,\n                    unit_id=unit.id if unit else None,\n                    unit=unit,\n                    city_id=city_tile.city_id if city_tile else None,\n                    citytile=city_tile,\n                    team=team,\n                    x=x,\n                    y=y\n                )\n\n                # If the city action is invalid, default to research action automatically\n                if not action.is_valid(game, actions_validated=[]):\n                    action = ResearchAction(\n                        game=game,\n                        unit_id=unit.id if unit else None,\n                        unit=unit,\n                        city_id=city_tile.city_id if city_tile else None,\n                        citytile=city_tile,\n                        team=team,\n                        x=x,\n                        y=y\n                    )\n            else:\n                action =  self.actionSpaceMapUnits[action_code%len(self.actionSpaceMapUnits)](\n                    game=game,\n                    unit_id=unit.id if unit else None,\n                    unit=unit,\n                    city_id=city_tile.city_id if city_tile else None,\n                    citytile=city_tile,\n                    team=team,\n                    x=x,\n                    y=y\n                )\n            \n            return action\n        except Exception as e:\n            # Not a valid action\n            print(e)\n            return None\n\n    def take_action(self, action_code, game, unit=None, city_tile=None, team=None):\n        \"\"\"\n        Takes an action in the environment according to actionCode:\n            actionCode: Index of action to take into the action array.\n        \"\"\"\n        action = self.action_code_to_action(action_code, game, unit, city_tile, team)\n        self.match_controller.take_action(action)\n\n    def game_start(self, game):\n        \"\"\"\n        This funciton is called at the start of each game. Use this to\n        reset and initialize per game. Note that self.team may have\n        been changed since last game. The game map has been created\n        and starting units placed.\n\n        Args:\n            game ([type]): Game.\n        \"\"\"\n        self.last_generated_fuel = game.stats[\"teamStats\"][self.team][\"fuelGenerated\"]\n        self.last_resources_collected = copy.deepcopy(game.stats[\"teamStats\"][self.team][\"resourcesCollected\"])\n        if self.stats != None:\n            self.stats_last_game =  self.stats\n        self.stats = {\n            \"rew\/r_total\": 0,\n            \"rew\/r_wood\": 0,\n            \"rew\/r_coal\": 0,\n            \"rew\/r_uranium\": 0,\n            \"rew\/r_research\": 0,\n            \"rew\/r_city_tiles_end\": 0,\n            \"rew\/r_fuel_collected\":0,\n            \"rew\/r_units\":0,\n            \"rew\/r_city_tiles\":0,\n            \"game\/turns\": 0,\n            \"game\/research\": 0,\n            \"game\/unit_count\": 0,\n            \"game\/cart_count\": 0,\n            \"game\/city_count\": 0,\n\t\t\t\"game\/city_tiles\": 0,\n            \"game\/wood_rate_mined\": 0,\n            \"game\/coal_rate_mined\": 0,\n            \"game\/uranium_rate_mined\": 0,\n        }\n        self.is_last_turn = False\n\n        # Calculate starting map resources\n        type_map = {\n            Constants.RESOURCE_TYPES.WOOD: \"WOOD\",\n            Constants.RESOURCE_TYPES.COAL: \"COAL\",\n            Constants.RESOURCE_TYPES.URANIUM: \"URANIUM\",\n        }\n\n        self.fuel_collected_last = 0\n        self.fuel_start = {}\n        self.fuel_last = {}\n        for type, type_upper in type_map.items():\n            self.fuel_start[type] = 0\n            self.fuel_last[type] = 0\n            for c in game.map.resources_by_type[type]:\n                self.fuel_start[type] += c.resource.amount * game.configs[\"parameters\"][\"RESOURCE_TO_FUEL_RATE\"][type_upper]\n\n        self.research_last = 0\n        self.units_last = 0\n        self.city_tiles_last = 0\n\n    \n    def get_reward(self, game, is_game_finished, is_new_turn, is_game_error):\n        \"\"\"\n        Returns the reward function for this step of the game.\n        \"\"\"\n        if is_game_error:\n            # Game environment step failed, assign a game lost reward to not incentivise this\n            print(\"Game failed due to error\")\n            return -1.0\n\n        if not is_new_turn and not is_game_finished:\n            # Only apply rewards at the start of each turn\n            return 0\n\n        # Get some basic stats\n        unit_count = len(game.state[\"teamStates\"][self.team % 2][\"units\"])\n        cart_count = 0\n        for id, u in game.state[\"teamStates\"][self.team % 2][\"units\"].items():\n            if u.type == Constants.UNIT_TYPES.CART:\n                cart_count += 1\n\n        unit_count_opponent = len(game.state[\"teamStates\"][(self.team + 1) % 2][\"units\"])\n        research = min(game.state[\"teamStates\"][self.team][\"researchPoints\"], 200.0) # Cap research points at 200\n        city_count = 0\n        city_count_opponent = 0\n        city_tile_count = 0\n        city_tile_count_opponent = 0\n        for city in game.cities.values():\n            if city.team == self.team:\n                city_count += 1\n            else:\n                city_count_opponent += 1\n\n            for cell in city.city_cells:\n                if city.team == self.team:\n                    city_tile_count += 1\n                else:\n                    city_tile_count_opponent += 1\n        \n        # Basic stats\n        self.stats[\"game\/research\"] = research\n        self.stats[\"game\/city_tiles\"] = city_tile_count\n        self.stats[\"game\/city_count\"] = city_count\n        self.stats[\"game\/unit_count\"] = unit_count\n        self.stats[\"game\/cart_count\"] = cart_count\n        self.stats[\"game\/turns\"] = game.state[\"turn\"]\n\n        rewards = {}\n\n        # Give up to 1.0 reward for each resource based on % of total mined.\n        type_map = {\n            Constants.RESOURCE_TYPES.WOOD: \"WOOD\",\n            Constants.RESOURCE_TYPES.COAL: \"COAL\",\n            Constants.RESOURCE_TYPES.URANIUM: \"URANIUM\",\n        }\n        fuel_now = {}\n        for type, type_upper in type_map.items():\n            fuel_now = game.stats[\"teamStats\"][self.team][\"resourcesCollected\"][type] * game.configs[\"parameters\"][\"RESOURCE_TO_FUEL_RATE\"][type_upper]\n            rewards[\"rew\/r_%s\" % type] = (fuel_now - self.fuel_last[type]) \/ self.fuel_start[type]\n            self.stats[\"game\/%s_rate_mined\" % type] = fuel_now \/ self.fuel_start[type]\n            self.fuel_last[type] = fuel_now\n        \n        # Give more incentive for coal and uranium\n        rewards[\"rew\/r_%s\" % Constants.RESOURCE_TYPES.COAL] *= 2\n        rewards[\"rew\/r_%s\" % Constants.RESOURCE_TYPES.URANIUM] *= 4\n        \n        # Give a reward based on amount of fuel collected. 1.0 reward for each 20K fuel gathered.\n        fuel_collected = game.stats[\"teamStats\"][self.team][\"fuelGenerated\"]\n        rewards[\"rew\/r_fuel_collected\"] = ( (fuel_collected - self.fuel_collected_last) \/ 20000 )\n        self.fuel_collected_last = fuel_collected\n\n        # Give a reward for unit creation\/death. 0.05 reward per unit.\n        rewards[\"rew\/r_units\"] = (unit_count - self.units_last) * 0.05\n        self.units_last = unit_count\n\n        # Give a reward for unit creation\/death. 0.1 reward per city.\n        rewards[\"rew\/r_city_tiles\"] = (city_tile_count - self.city_tiles_last) * 0.1\n        self.city_tiles_last = city_tile_count\n\n        # Tiny reward for research to help. Up to 0.5 reward for this.\n        rewards[\"rew\/r_research\"] = (research - self.research_last) \/ (200 * 2)\n        self.research_last = research\n        \n        # Give a reward up to around 50.0 based on number of city tiles at the end of the game\n        rewards[\"rew\/r_city_tiles_end\"] = 0\n        if is_game_finished:\n            self.is_last_turn = True\n            rewards[\"rew\/r_city_tiles_end\"] = city_tile_count\n        \n        \n        # Update the stats and total reward\n        reward = 0\n        for name, value in rewards.items():\n            self.stats[name] += value\n            reward += value\n        self.stats[\"rew\/r_total\"] += reward\n\n        # Print the final game stats sometimes\n        if is_game_finished and random.random() <= 0.15:\n            stats_string = []\n            for key, value in self.stats.items():\n                stats_string.append(\"%s=%.2f\" % (key, value))\n            print(\",\".join(stats_string))\n\n\n        return reward\n    \n    def turn_heurstics(self, game, is_first_turn):\n        \"\"\"\n        This is called pre-observation actions to allow for hardcoded heuristics\n        to control a subset of units. Any unit or city that gets an action from this\n        callback, will not create an observation+action.\n\n        Args:\n            game ([type]): Game in progress\n            is_first_turn (bool): True if it's the first turn of a game.\n        \"\"\"\n\n        # Heuristic override for city actions\n        # Build a worker if possible, otherwise research\n        '''\n        cities = game.cities.values()\n        spawned_workers = 0\n        for city in cities:\n            if city.team == self.team:\n                for cell in city.city_cells:\n                    city_tile = cell.city_tile\n                    if city_tile.can_act():\n                        if city_tile.can_build_unit() and game.worker_unit_cap_reached(self.team, spawned_workers):\n                            self.match_controller.take_action(\n                                SpawnWorkerAction(self.team, None, city_tile.pos.x, city_tile.pos.y)\n                            )\n                            spawned_workers += 1\n                        else:\n                            self.match_controller.take_action(\n                                ResearchAction(self.team, city_tile.pos.x, city_tile.pos.y, None )\n                            )\n        '''\n\n    def process_turn(self, game, team):\n        \"\"\"\n        Decides on a set of actions for the current turn. Not used in training, only inference.\n        Returns: Array of actions to perform.\n        \"\"\"\n        start_time = time.time()\n        actions = []\n        new_turn = True\n\n        # Inference the model per-unit\n        units = game.state[\"teamStates\"][team][\"units\"].values()\n        for unit in units:\n            if unit.can_act():\n                obs = self.get_observation(game, unit, None, unit.team, new_turn)\n                action_code, _states = self.model.predict(obs, deterministic=False)\n                if action_code is not None:\n                    actions.append(\n                        self.action_code_to_action(action_code, game=game, unit=unit, city_tile=None, team=unit.team))\n                new_turn = False\n\n        # Inference the model per-city\n        cities = game.cities.values()\n        for city in cities:\n            if city.team == team:\n                for cell in city.city_cells:\n                    city_tile = cell.city_tile\n                    if city_tile.can_act():\n                        obs = self.get_observation(game, None, city_tile, city.team, new_turn)\n                        action_code, _states = self.model.predict(obs, deterministic=False)\n                        if action_code is not None:\n                            actions.append(\n                                self.action_code_to_action(action_code, game=game, unit=None, city_tile=city_tile,\n                                                           team=city.team))\n                        new_turn = False\n\n        time_taken = time.time() - start_time\n        if time_taken > 0.5:  # Warn if larger than 0.5 seconds.\n            print(\"WARNING: Inference took %.3f seconds for computing actions. Limit is 1 second.\" % time_taken,\n                  file=sys.stderr)\n\n        return actions\n\n","3d1032d3":"import argparse\nimport glob\nimport os\nimport random\nfrom typing import Callable\n\nimport numpy as np\n\nfrom stable_baselines3 import PPO  # pip install stable-baselines3\nfrom stable_baselines3.common.callbacks import CheckpointCallback\nfrom stable_baselines3.common.utils import set_random_seed\nfrom stable_baselines3.common.vec_env import SubprocVecEnv\n\nfrom importlib import reload\nimport agent_policy\nreload(agent_policy) # Reload the file from disk incase the above agent-writing cell block was edited\nfrom agent_policy import AgentPolicy, CustomCombinedExtractor, compressed_map_observation\n\nfrom luxai2021.env.agent import Agent\nfrom luxai2021.env.lux_env import LuxEnvironment\nfrom luxai2021.game.constants import LuxMatchConfigs_Default\nfrom luxai2021.game.constants import Constants\n\nimport matplotlib\nfrom matplotlib import pyplot as plt\n\n# Default Lux configs\nconfigs = LuxMatchConfigs_Default\n\n# Create a default opponent agent\nopponent = Agent()\n\n# Create a RL agent in training mode\nplayer = AgentPolicy(mode=\"train\")\n\n# Create the Lux environment\nenv = LuxEnvironment(configs=configs,\n                     learning_agent=player,\n                     opponent_agent=opponent)\n\n# Test the observation space\nobs = env.reset()\nfor _ in range(1):\n    #env.render()\n    (obs, reward, is_game_over, state) = env.step(env.action_space.sample()) # take a random action\n    \n    # Show each of the initial map layers\n    fig_count = len(player.map_layers)\n    fig, axes = plt.subplots((fig_count\/\/4) + 1, 4, figsize=(30, 30))\n    norm = matplotlib.colors.Normalize(vmin=0, vmax=1.0, clip=False)\n    i = 0\n    for name, index in player.map_layers.items():\n        #sub = fig.add_subplot(fig_count, 1, i + 1)\n        data = obs[\"map\"][index,:,:]\n        axes[i\/\/4,i%4].imshow(data.T, interpolation='nearest', norm=norm)\n        axes[i\/\/4,i%4].set_title(str(name))\n        print(\"%s %s. Min=%f, Max=%f, Mean=%f\" % (name, \"Broad\", np.min(data), np.max(data), np.mean(data)) )\n        i += 1\n    \n    \n    print(env.game.map.get_map_string())\n    \n    if is_game_over:\n        obs = env.reset()\n\nenv.close()","bc7c90db":"import argparse\nimport glob\nimport os\nimport random\nfrom typing import Callable\nimport torch\n\nfrom stable_baselines3 import PPO  # pip install stable-baselines3\n\nfrom stable_baselines3.common.callbacks import CheckpointCallback\nfrom stable_baselines3.common.utils import set_random_seed\nfrom stable_baselines3.common.vec_env import SubprocVecEnv\n\nfrom agent_policy import AgentPolicy, CustomCombinedExtractor\nfrom luxai2021.env.agent import Agent\nfrom luxai2021.env.lux_env import LuxEnvironment\nfrom luxai2021.game.constants import LuxMatchConfigs_Default\nfrom stable_baselines3.common.callbacks import BaseCallback, EvalCallback\nfrom stable_baselines3.common.utils import get_schedule_fn\n\nclass TensorboardCallback(BaseCallback):\n    \"\"\"\n    Custom callback for plotting additional values in tensorboard.\n    \"\"\"\n    def __init__(self, verbose=0):\n        super(TensorboardCallback, self).__init__(verbose)\n\n    def _on_step(self) -> bool:\n        for e in self.training_env.envs:\n            if e.learning_agent.stats_last_game != None:\n                for name, value in e.learning_agent.stats_last_game.items():        \n                    self.logger.record(name, value)\n                e.learning_agent.stats_last_game = None\n        \n        return True\n\n\n\n# Train the model\nenv = LuxEnvironment(configs=configs,\n                     learning_agent=player,\n                     opponent_agent=opponent)\n    \npolicy_kwargs = dict(\n    features_extractor_class=CustomCombinedExtractor\n)\n\nmodel = PPO(\n            \"MlpPolicy\",\n            env,\n            verbose=1,\n            tensorboard_log=\".\/lux_tensorboard\/\",\n            learning_rate=0.001,\n            gamma=0.999,\n            gae_lambda=0.95,\n            batch_size=2048*2,\n            n_steps=2048*2,\n            policy_kwargs=policy_kwargs\n        )\n\n\ncallbacks = []\n\n# Save a checkpoint every 100K steps\ncallbacks.append(\n    CheckpointCallback(save_freq=100000,\n                        save_path='.\/models\/',\n                        name_prefix=f'rl_model')\n)\n\n# Tensorboard logger of game values at end-of-game. This comes from Agent.stats. Only\n# works for single-enviornment setups\ncallbacks.append(TensorboardCallback())\n\n\nprint(\"Training model...\")\nmodel.learn(total_timesteps=10000000000,\n            callback=callbacks)\n\nmodel.save(path=f'model.zip')\nprint(\"Done training model.\")","4f352b22":"# Save final model\nmodel.save(path=f'model.zip')\n\nprint(\"Done training model.\")","b6178f8c":"import agent_policy\nreload(agent_policy) # Reload the file from disk incase the above agent-writing cell block was edited\nfrom agent_policy import AgentPolicy, CustomCombinedExtractor, compressed_map_observation\n\n# Create a kaggle-remote opponent agent\nopponent = Agent()\n\n# Create a RL agent in inference mode\nplayer = AgentPolicy(mode=\"inference\", model=model)\n\n# Run the environment\nenv = LuxEnvironment(configs, player, opponent)\ntry:\n    env.reset()  # This will automatically run the game since there is\nexcept StopIteration as e:\n    pass\n\n\nenv.render()","4d29f352":"\"\"\"\nThis downloads two required python package dependencies that are not pre-installed\nby Kaggle yet.\n\nThis places the following two packages in the current working directory:\n    luxai2021\n    stable_baselines3\n\"\"\"\n\nimport os\nimport shutil\nimport subprocess\nimport tempfile\n\ndef localize_package(git, branch, folder):\n    if os.path.exists(folder):\n        print(\"Already localized %s\" % folder)\n    else:\n        # https:\/\/stackoverflow.com\/questions\/51239168\/how-to-download-single-file-from-a-git-repository-using-python\n        # Create temporary dir\n        t = tempfile.mkdtemp()\n\n        args = ['git', 'clone', '--depth=1', git, t, '-b',  branch]\n        res = subprocess.Popen(args, stdout=subprocess.PIPE)\n        output, _error = res.communicate()\n\n        if not _error:\n            print(output)\n        else:\n            print(_error)\n        \n        # Copy desired file from temporary dir\n        shutil.move(os.path.join(t, folder), '.')\n        # Remove temporary dir\n        shutil.rmtree(t, ignore_errors=True)\n\nlocalize_package('https:\/\/github.com\/glmcdona\/LuxPythonEnvGym.git', 'main', 'luxai2021')\nlocalize_package('https:\/\/github.com\/glmcdona\/LuxPythonEnvGym.git', 'main', 'kaggle_submissions')\nlocalize_package('https:\/\/github.com\/DLR-RM\/stable-baselines3.git', 'master', 'stable_baselines3')","541b5cd0":"# Move the dependent packages into kaggle submissions\n!mv luxai2021 kaggle_submissions\n!mv stable_baselines3 kaggle_submissions\n!rm .\/kaggle_submissions\/agent_policy.py\n!rm .\/kaggle_submissions\/model.zip\n!cp agent_policy.py kaggle_submissions\n\n# Copy the agent and model to the submission \n!cp .\/agent_policy.py kaggle_submissions\n!cp .\/models\/model.zip kaggle_submissions\n\n!ls kaggle_submissions","6c23f3ee":"import kaggle_environments\nfrom kaggle_environments import make\nreload(kaggle_environments)\nimport json\n# run another match but with our empty agent\nenv = kaggle_environments.make(\"lux_ai_2021\", configuration={\"seed\": 3, \"loglevel\": 2, \"annotations\": True}, debug=True)\n\n# Play the environment where the RL agent plays against itself\nsteps = env.run([\".\/kaggle_submissions\/main.py\", \".\/kaggle_submissions\/main.py\"])\n\n# Render the match\nenv.render(mode=\"ipython\", width=1200, height=800)","bd572003":"# run another match but with our empty agent\nenv = make(\"lux_ai_2021\", configuration={\"seed\": 56212424, \"loglevel\": 2, \"annotations\": True}, debug=True)\n\n# Play the environment where the RL agent plays against itself\nsteps = env.run([\".\/kaggle_submissions\/main.py\", \".\/kaggle_submissions\/main.py\"])\n\n# Render the match\nenv.render(mode=\"ipython\", width=1200, height=800)","e73506b8":"# run another match but with our empty agent\nenv = make(\"lux_ai_2021\", configuration={\"seed\": 562124, \"loglevel\": 2, \"annotations\": True}, debug=True)\n\n# Play the environment where the RL agent plays against itself\nsteps = env.run([\".\/kaggle_submissions\/main.py\", \".\/kaggle_submissions\/main.py\"])\n\n# Render the match\nenv.render(mode=\"ipython\", width=1200, height=800)","d21bd93a":"# run another match but with our empty agent\nenv = make(\"lux_ai_2021\", configuration={\"seed\": 564, \"loglevel\": 2, \"annotations\": True}, debug=True)\n\n# Play the environment where the RL agent plays against itself\nsteps = env.run([\".\/kaggle_submissions\/main.py\", \".\/kaggle_submissions\/main.py\"])\n\n# Render the match\nenv.render(mode=\"ipython\", width=1200, height=800)","8601f981":"# run another match but with our empty agent\nenv = make(\"lux_ai_2021\", configuration={\"seed\": 54, \"loglevel\": 2, \"annotations\": True}, debug=True)\n\n# Play the environment where the RL agent plays against itself\nsteps = env.run([\".\/kaggle_submissions\/main.py\", \".\/kaggle_submissions\/main.py\"])\n\n# Render the match\nenv.render(mode=\"ipython\", width=1200, height=800)","59d2b794":"!tar -czf submission.tar.gz -C kaggle_submissions .\n!ls","a292b9be":"# Build the environment for training\n\nNotes on metrics:\n* An Episode is a single game between your RL agent and it's opponent. This is generally 360 turns, spanning more than 360 unit + city decision steps.\n* Mean episode length (ep_len_mean) is the number of decision made per game. The larger this gets, means that it is making more unit + city decision per game, meaning that more units and cities were alive for longer during the game.\n* Episode reward mean (ep_rew_mean), is set up as micro-reward funciton for faster learning. Per turn it gets a small reward based on the number of cities and units alive. It gets a really big reward based on the number of cities and units alive at the end of the game.","8277533a":"# Prepare and submit the kaggle submission","715c22da":"# Train the agent against a dummy opponent","71ed7de4":"## Use GPU if available\nNote: GPU provides very little speedup. I recommend using a CPU-only notebook usually.","1ef70e9b":"# Set up a Kaggle Submission and lux replay environment for the agent","6e7a6623":"# Define the RL agent logic\nEdit this agent logic to implement your own observations, action space, and reward shaping.","caa71c95":"# Lux AI Deep Reinforcement Learning Environment Example\nSee https:\/\/github.com\/glmcdona\/LuxPythonEnvGym for environment project and updates.\n\nThis is a python replica of the Lux game engine to speed up training. It reformats the agent problem into making a action decision per-unit for the team."}}