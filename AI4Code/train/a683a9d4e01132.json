{"cell_type":{"ab9752fc":"code","75fe2677":"code","c66448e6":"code","33f30060":"code","2d3b3645":"code","bd035459":"code","1552c175":"code","f145c071":"code","b57eada6":"code","587ef8ea":"code","371311c7":"code","c2c465a2":"code","47b21886":"code","c9b94a71":"code","5442ebe0":"code","bc9bfac1":"code","f1cd05ac":"code","76e21afa":"code","98b97f1f":"code","9feb062d":"code","edd414a8":"code","d74914a7":"code","346360b6":"code","216ed237":"code","8d8609b4":"code","3cfebc51":"markdown","feec4b08":"markdown","ec5334e4":"markdown","0035c15d":"markdown","693fc9e2":"markdown","2752c0ba":"markdown","277e6394":"markdown","1f02f043":"markdown","19f2dde8":"markdown","28cf1dfa":"markdown","fd7f2c9c":"markdown","14bb3b41":"markdown","37838e84":"markdown","dab98d68":"markdown","a492b65c":"markdown","5eecd8d2":"markdown","4e5dfe61":"markdown"},"source":{"ab9752fc":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","75fe2677":"#title input file is in tsv format\ndata=pd.read_csv('..\/input\/reviews\/Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3) \n#delimeter or sep='\\t' \n#quoting =3 no quote or ignore quotes while processing","c66448e6":"data","33f30060":"import seaborn as sns\nsns.countplot(x='Liked', data=data)\nplt.show()","2d3b3645":"good_reviews_count = len(data.loc[data['Liked'] == 1])\nbad_reviews_count=len(data.loc[data['Liked']==0])\n(good_reviews_count, bad_reviews_count)","bd035459":"#Data Cleaning\n# Cleaning the Text\nimport nltk\nimport re\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()","1552c175":"corpus = []\nfor i in range(len(data)):\n    review = re.sub('[^a-zA-Z]', ' ', data['Review'][i])\n    review = review.lower()\n    review = review.split()\n    all_stopwords = stopwords.words('english')\n    all_stopwords.remove('not') \n    #remove negative word 'not' as it is closest word to help determine whether the review is good or not \n    review = [lemmatizer.lemmatize(word) for word in review if not word in set(all_stopwords)]\n    review = ' '.join(review)\n    corpus.append(review)\nprint(corpus)","f145c071":"# Creating the TF-IDF model\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ncv = TfidfVectorizer()\nX = cv.fit_transform(corpus).toarray()\ny = data.iloc[:, -1].values","b57eada6":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)","587ef8ea":"from sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.ensemble import VotingClassifier\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nkfold = StratifiedKFold(n_splits=12)\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","371311c7":"#Logistic Regression\nlr = LogisticRegression(C=0.7286427728546842, max_iter=2000, solver='lbfgs', random_state=0)\ncv = cross_val_score(lr,X_train,y_train,cv=kfold)\nprint(cv)\nprint(cv.mean()*100)\nlr.fit(X_train,y_train)\ny_pred_lr=lr.predict(X_test)\nprint('The accuracy of the Logistic Regression is',metrics.accuracy_score(y_pred_lr,y_test)*100)\ncm=confusion_matrix(y_test, y_pred_lr)\nprint(cm)\nclassification_report(y_test, y_pred_lr)","c2c465a2":"#GaussianNB\ngnb = GaussianNB(var_smoothing=7)\ncv = cross_val_score(gnb,X_train,y_train,cv=kfold)\nprint(cv)\nprint(cv.mean()*100)\ngnb.fit(X_train,y_train)\ny_pred_gnb=gnb.predict(X_test)\nprint('The accuracy of the Naive Bayes is', metrics.accuracy_score(y_pred_gnb,y_test)*100)\ncm=confusion_matrix(y_test, y_pred_gnb)\nprint(cm)\nclassification_report(y_test, y_pred_gnb)","47b21886":"#MultinomialNB\nmnb = MultinomialNB(alpha=3)\ncv = cross_val_score(mnb,X_train,y_train,cv=kfold)\nprint(cv)\nprint(cv.mean()*100)\nmnb.fit(X_train,y_train)\ny_pred_mnb=mnb.predict(X_test)\nprint('The accuracy of the Naive Bayes is', metrics.accuracy_score(y_pred_mnb,y_test)*100)\ncm=confusion_matrix(y_test, y_pred_mnb)\nprint(cm)\nclassification_report(y_test, y_pred_mnb)","c9b94a71":"#Bernoulli NB\nbnb = BernoulliNB(alpha =6)\ncv = cross_val_score(bnb,X_train,y_train,cv=kfold)\nprint(cv)\nprint(cv.mean()*100)\nmnb.fit(X_train,y_train)\ny_pred_bnb=mnb.predict(X_test)\nprint('The accuracy of the Naive Bayes is', metrics.accuracy_score(y_pred_bnb,y_test)*100)\ncm=confusion_matrix(y_test, y_pred_bnb)\nprint(cm)\nclassification_report(y_test, y_pred_bnb)","5442ebe0":"#Random Forest Classifier\nrf = RandomForestClassifier(bootstrap=False, criterion='entropy', max_depth=45,\n                       max_features='log2', min_samples_leaf=1,\n                       n_estimators=1000, random_state=0)\nrf.fit(X_train, y_train)\ncv = cross_val_score(rf,X_train,y_train,cv=kfold)\nprint(cv)\nprint(cv.mean()*100)\ny_pred_rf = rf.predict(X_test)\nprint('The accuracy of the RandomForestClassifier is',metrics.accuracy_score(y_pred_rf,y_test)*100)\ncm=confusion_matrix(y_test, y_pred_rf)\nprint(cm)\nclassification_report(y_test, y_pred_rf)","bc9bfac1":"#Linear SVC\nsvcl = SVC(kernel = 'linear', random_state = 0, probability=True)\nsvcl.fit(X_train, y_train)\ncv = cross_val_score(svcl,X_train,y_train,cv=kfold)\nprint(cv)\nprint(cv.mean()*100)\ny_pred_svcl = svcl.predict(X_test)\nprint('The accuracy of the Linear SVC is',metrics.accuracy_score(y_pred_svcl,y_test)*100)\ncm=confusion_matrix(y_test, y_pred_svcl)\nprint(cm)\nclassification_report(y_test, y_pred_svcl)","f1cd05ac":"#rbf SVC\nfrom sklearn.svm import SVC\nsvck = SVC(kernel = 'rbf', random_state = 0, probability=True, C=0.62)\nsvck.fit(X_train, y_train)\ncv = cross_val_score(svck,X_train,y_train,cv=kfold)\nprint(cv)\nprint(cv.mean()*100)\ny_pred_svck = svck.predict(X_test)\nprint('The accuracy of the Kernel SVC is',metrics.accuracy_score(y_pred_svck,y_test)*100)\ncm=confusion_matrix(y_test, y_pred_svck)\nprint(cm)\nclassification_report(y_test, y_pred_svck)","76e21afa":"#Decision Tree Classifier\ndt = DecisionTreeClassifier(random_state=0, max_depth=30, min_samples_split=2, min_samples_leaf=1)\ndt.fit(X_train, y_train)\ncv = cross_val_score(dt,X_train,y_train,cv=kfold)\nprint(cv)\nprint(cv.mean()*100)\ny_pred_dt = dt.predict(X_test)\nprint('The accuracy of the Decision Tree Classifier is',metrics.accuracy_score(y_pred_dt,y_test)*100)\ncm=confusion_matrix(y_test, y_pred_dt)\nprint(cm)\nclassification_report(y_test, y_pred_dt)","98b97f1f":"#KNN\nknn = KNeighborsClassifier(n_neighbors = 10, metric = 'minkowski', p = 2, leaf_size = 10)\nknn.fit(X_train, y_train)\ncv = cross_val_score(knn,X_train,y_train,cv=kfold)\nprint(cv)\nprint(cv.mean()*100)\ny_pred_knn = knn.predict(X_test)\nprint('The accuracy of the K-Neighbors Classifier is',metrics.accuracy_score(y_pred_knn,y_test)*100)\ncm=confusion_matrix(y_test, y_pred_knn)\nprint(cm)\nclassification_report(y_test, y_pred_knn)","9feb062d":"#VCLF 1\nvoting_clf = VotingClassifier(estimators = [('lr', lr),('gnb',gnb),('bnb',bnb),('mnb',mnb),\n                                            ('knn',knn),('dt',dt),\n                                            ('rf',rf),('svck',svck),('svcl',svcl)], voting = 'soft') \nvoting_clf.fit(X_train, y_train)\ncv = cross_val_score(voting_clf,X_train,y_train,cv=kfold)\nprint(cv)\nprint(cv.mean()*100)\ny_pred_vclf = voting_clf.predict(X_test)\nprint('The accuracy of the Voting Classifier is',metrics.accuracy_score(y_pred_vclf,y_test)*100)\ncm=confusion_matrix(y_test, y_pred_vclf)\nprint(cm)\nclassification_report(y_test, y_pred_vclf)","edd414a8":"#VCLF 2\nvoting_clf = VotingClassifier(estimators = [('lr', lr),('bnb',bnb),('mnb',mnb),('gnb', gnb),\n                                            ('rf',rf),('svck',svck),('svcl',svcl)], voting = 'soft') \nvoting_clf.fit(X_train, y_train)\ncv = cross_val_score(voting_clf,X_train,y_train,cv=kfold)\nprint(cv)\nprint(cv.mean()*100)\ny_pred_vclf = voting_clf.predict(X_test)\nprint('The accuracy of the Voting Classifier is',metrics.accuracy_score(y_pred_vclf,y_test)*100)\ncm=confusion_matrix(y_test, y_pred_vclf)\nprint(cm)\nclassification_report(y_test, y_pred_vclf)","d74914a7":"#VCLF 3\nvoting_clf = VotingClassifier(estimators = [('bnb',bnb),('mnb',mnb),('gnb', gnb),\n                                            ('svck',svck),('svcl',svcl)], voting = 'soft') \nvoting_clf.fit(X_train, y_train)\ncv = cross_val_score(voting_clf,X_train,y_train,cv=kfold)\nprint(cv)\nprint(cv.mean()*100)\ny_pred_vclf = voting_clf.predict(X_test)\nprint('The accuracy of the Voting Classifier is',metrics.accuracy_score(y_pred_vclf,y_test)*100)\ncm=confusion_matrix(y_test, y_pred_vclf)\nprint(cm)\nclassification_report(y_test, y_pred_vclf)","346360b6":"#VCLF 4\nvoting_clf = VotingClassifier(estimators = [('lr', lr),('gnb',gnb),('bnb',bnb),('mnb',mnb),\n                                            ('knn',knn),('dt',dt),\n                                            ('rf',rf),('svck',svck),('svcl',svcl)], voting = 'hard') \nvoting_clf.fit(X_train, y_train)\ncv = cross_val_score(voting_clf,X_train,y_train,cv=kfold)\nprint(cv)\nprint(cv.mean()*100)\ny_pred_vclf = voting_clf.predict(X_test)\nprint('The accuracy of the Voting Classifier is',metrics.accuracy_score(y_pred_vclf,y_test)*100)\ncm=confusion_matrix(y_test, y_pred_vclf)\nprint(cm)\nclassification_report(y_test, y_pred_vclf)","216ed237":"# VCLF 5\nvoting_clf = VotingClassifier(estimators = [('lr', lr),('bnb',bnb),('mnb',mnb),('gnb', gnb),\n                                            ('rf',rf),('svck',svck),('svcl',svcl)], voting = 'hard') \nvoting_clf.fit(X_train, y_train)\ncv = cross_val_score(voting_clf,X_train,y_train,cv=kfold)\nprint(cv)\nprint(cv.mean()*100)\ny_pred_vclf = voting_clf.predict(X_test)\nprint('The accuracy of the Voting Classifier is',metrics.accuracy_score(y_pred_vclf,y_test)*100)\ncm=confusion_matrix(y_test, y_pred_vclf)\nprint(cm)\nclassification_report(y_test, y_pred_vclf)","8d8609b4":"# VCLF 6\nvoting_clf = VotingClassifier(estimators = [('bnb',bnb),('mnb',mnb),('gnb', gnb),\n                                            ('svck',svck),('svcl',svcl)], voting = 'hard') \nvoting_clf.fit(X_train, y_train)\ncv = cross_val_score(voting_clf,X_train,y_train,cv=kfold)\nprint(cv)\nprint(cv.mean()*100)\ny_pred_vclf = voting_clf.predict(X_test)\nprint('The accuracy of the Voting Classifier is',metrics.accuracy_score(y_pred_vclf,y_test)*100)\ncm=confusion_matrix(y_test, y_pred_vclf)\nprint(cm)\nclassification_report(y_test, y_pred_vclf)","3cfebc51":"**4. SVC**","feec4b08":"**6. KNN**","ec5334e4":"**1. Logistic Regression**","0035c15d":"**5. Decision Tree Classifier**","693fc9e2":"# In this notebook, I have made an attempt to get a simple text classification model up and running. In this, restaurant review data from Kaggle (also available in UCI ML Library) was used to perform Sentiment Analysis using Lemmatizing and TFIDF model to classify reviews into two sentiments \u2014 Liked(1) and Disliked(0).","2752c0ba":"**3. Random Forest Classifier**","277e6394":"# Data Cleaning","1f02f043":"**Restaurant Reviews Sentiment Analysis using Lemmatizing and TFIDF**","19f2dde8":"# Importing the Libraries and reading the tsv data file","28cf1dfa":"# Create a TFIDF Model","fd7f2c9c":"**Voting Classifier**\n\nVoting is one of the simplest way of combining the predictions from multiple machine learning algorithms. Voting classifier isn\u2019t an actual classifier but a wrapper for set of different ones that are trained and valuated in parallel in order to exploit the different peculiarities of each algorithm.","14bb3b41":"# Since, the data is fairly balanced, we are only concerned with accuracy_score. From above distinct classifiers and various Voting Classifier combinations, highest accuracy achieved was 80% with cross_val_score of 79.99% with Voting Classifier 6 (# VCLF6).","37838e84":"# Classifier Model Training","dab98d68":"**2. Naive Bayes**","a492b65c":"So, the dataset is a balanced dataset with number of '1's and '0's being equal. ","5eecd8d2":"# Train, Test Split","4e5dfe61":"**Since, the data is fairly balanced, we are only concerned with accuracy_score. From various Classifiers, highest accuracy achieved was 80% with cross_val_score of 79.99% with Voting Classifier 6 (# VCLF6).**"}}