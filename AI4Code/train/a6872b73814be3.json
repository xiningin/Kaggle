{"cell_type":{"8deb5198":"code","25cd8311":"code","344fc835":"code","0f024a9f":"code","97dd3b9f":"code","c55f7e6a":"code","71c5a522":"code","722f0ab6":"code","583cda10":"code","e0da9c86":"code","25da1cd3":"code","2039b2e1":"code","7119f30d":"code","28babd60":"code","5e173368":"code","ea934be0":"code","2bb6b3f1":"code","dd1839f3":"code","2bde9966":"code","63ddc59e":"code","7de5f9a9":"code","aa1ee3f7":"code","3fd73f0b":"code","a47376ce":"code","723b5488":"code","54e51210":"code","754528d6":"code","0cb91a2e":"code","36724f8e":"code","38271eb1":"code","22260b09":"markdown","f2d1add6":"markdown","8d4a8fde":"markdown","42b428b0":"markdown","1c853125":"markdown","2c632f7a":"markdown","438f8005":"markdown","1503d9b7":"markdown","2aa5e8b7":"markdown"},"source":{"8deb5198":"import datetime\nimport spacy \nimport string\nimport seaborn as sb \nimport matplotlib.pyplot as plt \nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\nfrom sklearn.feature_extraction.text import TfidfVectorizer,TfidfTransformer,CountVectorizer\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","25cd8311":"alexa=pd.read_csv('\/kaggle\/input\/amazon-alexa-reviews\/amazon_alexa.tsv',sep='\\t')","344fc835":"# show infos about the df \nalexa.head()","0f024a9f":"#get the shape \nalexa.info()","97dd3b9f":"alexa.shape","c55f7e6a":"#check if there is null values \nalexa.isnull().sum()\n#No empty values ","71c5a522":"# how many feedbacks per class \n#1==pos\n#0==neg\nalexa.feedback.value_counts()","722f0ab6":"sb.countplot(data=alexa,x='feedback')","583cda10":"# class per variation \nalexa.rating.value_counts()","e0da9c86":"sb.countplot(data=alexa,x='rating',)","25da1cd3":"# define punctiations \npunctuations = string.punctuation\n# load the spacy model\nnlp = spacy.load('en_core_web_sm')\n# defien stop words \nstop_words = spacy.lang.en.stop_words.STOP_WORDS\n\n# Load English tokenizer, tagger, parser, NER and word vectors\nparser = English()\n\n# Creating our tokenizer function\ndef spacy_tokenizer(text):\n    # Creating our token object, which is used to create documents with linguistic annotations.\n    tokens = parser(text)\n\n    # Lemmatizing each token and converting each token into lowercase\n    lemmas = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in tokens ]\n\n    # Removing stop words\n    clean_lemmas = [ word for word in lemmas if word not in stop_words and word not in punctuations ]\n\n    # return preprocessed list of tokens\n    return to_string(clean_lemmas)\n\n# Define a function that turn list of tokens to a String object \ndef to_string(lemmatized_text):\n    return ' '.join([str(elem) for elem in lemmatized_text]) \n     ","2039b2e1":"# Now Let's clean our reviews \nalexa['clean_text']= alexa['verified_reviews'].apply(spacy_tokenizer)","7119f30d":"alexa.sample(5)","28babd60":"# create a bag of words by a Countvectorizer \nbow = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1)).fit_transform(alexa['clean_text'])\n\n","5e173368":"#print the spars matrix \nbow.shape","ea934be0":"# Now let's pass the bow to TfIDf Transformer  \ntfidf_transformer= TfidfTransformer()\ntfidf = tfidf_transformer.fit_transform(bow)\n","2bb6b3f1":"print(tfidf.shape)","dd1839f3":"from sklearn.model_selection import train_test_split\n\nX = alexa['verified_reviews'] \ny = alexa['feedback'] \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","2bde9966":"print(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)\n","63ddc59e":"pipeline = Pipeline([\n    ('bow', CountVectorizer(analyzer=spacy_tokenizer)),  \n    ('tfidf', TfidfTransformer()),  \n    ('classifier', RandomForestClassifier(n_estimators=600))\n])\n","7de5f9a9":"#Fit the model \npipeline.fit(X_train,y_train)","aa1ee3f7":"#make predictions \npreds=pipeline.predict(X_test)","3fd73f0b":"print(preds[:10])","a47376ce":"from sklearn import metrics\n# Model Accuracy\nprint(\" Classification report: \\n \",metrics.classification_report(y_test, preds))\nprint('\\n')\nprint(\"Confusion Matrix  :\",metrics.confusion_matrix(y_test, preds))\nprint(\" Accuracy Score :\" ,metrics.accuracy_score(y_test, preds,))","723b5488":"from sklearn.linear_model import LogisticRegression\n\npipeline = Pipeline([\n    ('bow', CountVectorizer(analyzer=spacy_tokenizer)),  \n    ('tfidf', TfidfTransformer()),  \n    ('classifier', LogisticRegression())\n])\n","54e51210":"pipeline.fit(X_train,y_train)","754528d6":"predictions=pipeline.predict(X_test)","0cb91a2e":"# Model Accuracy\nprint(\" Classification report :\",metrics.classification_report(y_test, predictions))\nprint(\"Confusion Matrix  :\",metrics.confusion_matrix(y_test, predictions))\nprint(\" Accuracy Score :\" ,metrics.accuracy_score(y_test, predictions,))","36724f8e":"#submit our predictions \npred_test=pd.Series(preds)\nsubmission = pd.DataFrame({'Id':pred_test.index, 'feedback ': pred_test.values})\nsubmission.to_csv('submission.csv', index=False)\nprint(\" Submission  successfully saved!\")","38271eb1":"submission.sample(10)","22260b09":"### Train test Split ","f2d1add6":"### Evaulate our RFC Classier ","8d4a8fde":"### Logistic Regression Classifier ","42b428b0":"### Import the suspects ","1c853125":"### Load the dataset ","2c632f7a":"### Preprocessing the reviews ","438f8005":"### Build the pipeline ","1503d9b7":"### Logistic Regression Evaulation ","2aa5e8b7":"### Submit the Result "}}