{"cell_type":{"d08da678":"code","90d49acd":"code","adde67fb":"code","7e9ee79b":"code","12cdbc7d":"code","589e7eed":"code","e3d8206d":"code","b1e2f5b2":"code","0905c2e1":"code","96e07691":"code","74c7d258":"code","73c63198":"code","7a261782":"code","b297f28b":"code","59314018":"code","7d97b3c6":"code","c6352acb":"code","9f346558":"code","b860f79a":"code","3de371ec":"code","ca647f9f":"code","f5605986":"code","ba30a65b":"code","5378e330":"code","9bc3e3aa":"code","79b877b4":"code","fff7a1f1":"code","0c4e57f8":"code","7aba64b6":"code","3c6142b4":"code","715a0550":"code","ae878e5b":"code","26710ab9":"code","4598a104":"code","07dfa133":"code","7fc12d02":"code","63da6bfb":"code","21a05576":"code","85eb42ff":"code","a841374e":"code","5d1e4b7f":"code","9eb1f21e":"markdown","a9fc6804":"markdown","95e6b4bc":"markdown","62f01f1c":"markdown","c136c4bf":"markdown","a08d5dee":"markdown","862ad76f":"markdown","cff6e234":"markdown","87c1e422":"markdown","f2b2e397":"markdown","73d5b2c7":"markdown","ea1a4e67":"markdown","aa2a6d4d":"markdown"},"source":{"d08da678":"!pip install catboost -q","90d49acd":"import numpy as np  \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# LOAD DATA\ntrain = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\ntest = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')\nitems = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')\nshops = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv')\nitem_categories = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')","adde67fb":"# Add date_block_num to test\n# Identify the next date_block_num for the test set\ntest_dates = train.groupby(['shop_id'])['date_block_num'].max().reset_index()\ntest_dates['date_block_num'] += 1\n#test_dates['date_block_num'] = 34\ntest = test.merge(test_dates,how='left',on='shop_id')\n\n\n# Similar shop names but different shop_ids - > rewrite shopids\ntrain.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\ntrain.loc[train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11","7e9ee79b":"# Number of items available in test, but not in training\ntest_shops = test.shop_id.unique().tolist()\n\nmissing_items = list(np.setdiff1d(test.item_id.unique().tolist(),train.item_id.unique().tolist()))\nmissing_rows = test[test.item_id.isin(missing_items)].shape[0]\nprint('Number of items not available in training: ',len(missing_items))\nprint('Number of rows not available in training: ', missing_rows)\nprint('Number of rows for testing: ', test.shape[0])\nprint('Percentage missing: ', missing_rows * 100 \/ test.shape[0])","12cdbc7d":"# identify shop item combination across train and test\ntrain_shop_item=train[['shop_id','item_id']].drop_duplicates()\ntest_shop_item=test[['shop_id','item_id']].drop_duplicates()\n\n#combine shop item combinations\nshop_item = pd.concat([train_shop_item,test_shop_item]).drop_duplicates()\n\n# extract dateblocks from train\ntrain_shop_dates = train[['shop_id','date_block_num']].drop_duplicates()\n\nfull=train_shop_dates.merge(shop_item,\n                      how='left',\n                      on='shop_id')\n\nmerged=full.merge(train,\n          how='left',\n          on=['shop_id','item_id','date_block_num'])\n\ntrain = merged.loc[:,['shop_id','item_id','date','date_block_num','item_price','item_cnt_day']]\ntrain.fillna(0,inplace=True)","589e7eed":"items_full=items.merge(item_categories,how='left',on=['item_category_id'])\nmissing=test[test.item_id.isin(missing_items)].merge(items_full,how='left',on='item_id')\n\n# Generate a replacement item_id for items found in test, but missing in training\n# Replacement strategy : most popular item within that category for that particular shop\n\nitem_cat_map = items[items.item_id.isin(missing_items)][['item_id','item_category_id']]\nitem_cat_map_dict = dict(zip(item_cat_map.item_id.values,item_cat_map.item_category_id.values))\n\n\ntrain_monthly=train.groupby(['shop_id','item_id','date_block_num'])\\\n                   .agg(item_sum_mth=pd.NamedAgg(column='item_cnt_day',aggfunc='sum')).reset_index()\\\n                   .merge(items_full[['item_id','item_category_id']],\n                          how='left',\n                          on='item_id')\n\n\nmost_pop_item_per_shop_and_cat=train_monthly.groupby(['shop_id','item_category_id'])['item_id']\\\n                                            .apply(lambda x : x.value_counts().head(1))\\\n                                            .reset_index(name='occurences')\\\n                                            .rename(columns={'level_2' : 'item_id'})\n\nmost_pop_item_per_shop_and_cat['item_id'] = most_pop_item_per_shop_and_cat['item_id'].astype('int64')\n\n\n# Assumption new items will sell similar to the most popular item within the same category sold at the same shop\nreplacement=missing.merge(most_pop_item_per_shop_and_cat,\n                          how='left',\n                          on=['shop_id','item_category_id'], \n                          suffixes=('_original','_replacement'))\n\n# Replacement kind of work, but it seems that some shops are selling items in a category they havent done before\nprint(replacement[replacement.item_id_replacement.isna()].shape,replacement.shape)\nprint('\\n')\n\ntrain_shop_cat_map = train_monthly.groupby(['shop_id'])['item_category_id'] \\\n                                     .apply(lambda x: x.unique().astype('int16').tolist())\\\n                                     .to_dict()\ntest_shop_cat_map = test.merge(items[['item_id','item_category_id']],how='left',on='item_id')\\\n                        .groupby(['shop_id'])['item_category_id']\\\n                        .apply(lambda x: x.unique().astype('int16').tolist())\\\n                        .to_dict()\n\n# Check whats not matching up in train and test\nresult = {}\nfor shop, item_cat in test_shop_cat_map.items():\n    #get item categories that are not in training but in test\n    result[shop] = list(np.setdiff1d(item_cat,train_shop_cat_map[shop]))\n\n#import pprint \n#pprint.pprint(result)\n\n# Lets use historical data from other shops on scenarios where a shop has started selling a new item in a new category\nreplacement_no_nan=missing.merge(most_pop_item_per_shop_and_cat,\n                                 how='left',\n                                 on=['item_category_id'], \n                                 suffixes=('_original','_replacement'))\nreplacement.update(replacement_no_nan,overwrite=False)\nreplacement['item_id_replacement'] = replacement['item_id_replacement'] .astype('int16')\nreplacement['occurences'] = replacement['occurences'] .astype('int16')\n\n# 1 add inn replacement item_ids\ncols = ['ID','shop_id','item_id_original','item_id_replacement']\n\ntest = test.merge(replacement[cols],\n                        how='left',\n                        left_on=['ID','shop_id','item_id'],\n                        right_on=['ID','shop_id','item_id_original'],\n                        suffixes=('','_repl'))\n# \ntest['item_id_new'] = np.where(test.item_id_replacement.isna(),\n                               test.item_id,\n                               test.item_id_replacement)\n\ntest['item_id_new'] = test['item_id_new'].astype('int16')\n\ntest.rename(columns={'item_id' : 'item_id_org'},inplace=True)\ntest.rename(columns={'item_id_new' : 'item_id'},inplace=True)\ntest.drop(columns=['item_id_original','item_id_org','item_id_replacement'],inplace=True)","e3d8206d":"# Preliminary data cleaning\ntrain = train[(train.item_price <100000) & (train.item_price > 0)]","b1e2f5b2":"# Generate monthly aggregates\nagg_train=train.groupby(['shop_id','item_id','date_block_num'])\\\n               .agg(\n                            max_price=pd.NamedAgg(column='item_price', aggfunc='max'), \n                            min_price=pd.NamedAgg(column='item_price', aggfunc='min'),\n                            mean_price=pd.NamedAgg(column='item_price', aggfunc='mean'),\n                            item_sum_mth=pd.NamedAgg(column='item_cnt_day',aggfunc='sum'),\n                            item_mean_mth=pd.NamedAgg(column='item_cnt_day',aggfunc='mean'),\n                            item_std_mth=pd.NamedAgg(column='item_cnt_day', aggfunc=np.nanstd),\n                            num_days_sales=pd.NamedAgg(column='date',aggfunc='nunique')\n                   )\n\nagg_train = agg_train.reset_index()","0905c2e1":"# Add dmy columns (so we may concat dataframes to ease preprocessing)\nagg_train['CATEGORY'] = 'TRAIN'\nagg_train['ID'] = -1\ntest['max_price'] = 0\ntest['min_price'] = 0\ntest['mean_price'] = 0\ntest['item_sum_mth'] = 0\ntest['item_mean_mth'] = 0\ntest['item_std_mth'] = 0\ntest['CATEGORY'] = 'TEST'\ntest['num_days_sales'] = 0\n\n# Combine dataframes to ease feature engineering \nagg_combined = pd.concat([agg_train,test])","96e07691":"# Downcast\nagg_combined['shop_id'] = agg_combined['shop_id'].astype('int32')\nagg_combined['item_id'] = agg_combined['item_id'].astype('int32')\nagg_combined['date_block_num'] = agg_combined['date_block_num'].astype('int32')\n\nagg_combined['max_price'] = agg_combined['max_price'].astype('float32')\nagg_combined['min_price'] = agg_combined['min_price'].astype('float32')\nagg_combined['mean_price'] = agg_combined['mean_price'].astype('float32')\n\nagg_combined['item_sum_mth'] = agg_combined['item_sum_mth'].astype('int32')\nagg_combined['item_mean_mth'] = agg_combined['item_mean_mth'].astype('float32')\nagg_combined['item_std_mth'] = agg_combined['item_std_mth'].astype('float32')","74c7d258":"agg_combined['date_block_num_prev']=agg_combined.sort_values(['shop_id','item_id','date_block_num']).groupby(['shop_id','item_id'])['date_block_num'].shift(-1).values","73c63198":"agg_combined['date_block_num_prev'].fillna(0,inplace=True)\nagg_combined['mth_since_l_sale'] = agg_combined['date_block_num'] - agg_combined['date_block_num_prev']","7a261782":"agg_combined.sort_values(['shop_id','item_id','date_block_num'],inplace=True)\nagg_combined['item_cum_sum']=agg_combined.groupby(['shop_id','item_id']).cumsum()['item_sum_mth']\n\nagg_combined['total_item_trend'] = agg_combined.groupby(['item_id','date_block_num'])['item_sum_mth'].transform(sum)\nagg_combined['totel_item_price_trend'] = agg_combined.groupby(['item_id','date_block_num'])['mean_price'].transform(np.mean)\nagg_combined['totel_item_price_trend_std'] = agg_combined.groupby(['item_id','date_block_num'])['mean_price'].transform(np.std)\n\n\nagg_combined['date_block_num']+=1\n\nagg_combined['first_month_sale'] = agg_combined.groupby(['shop_id','item_id'])['date_block_num'].transform(min)\nagg_combined['month_since_first_sale'] = agg_combined.date_block_num - agg_combined.first_month_sale\n\n# add some dateblock normalized features\nagg_combined['total_item_trend_N'] = agg_combined['total_item_trend']  \/ agg_combined['date_block_num'] \nagg_combined['item_sum_mth_N'] = agg_combined['item_sum_mth'] \/ agg_combined['date_block_num'] \nagg_combined['item_cum_sum_N'] = agg_combined['item_cum_sum'] \/ agg_combined['date_block_num']\nagg_combined['cum_sum_price_N'] = agg_combined.groupby(['shop_id','item_id']).cumsum()['mean_price'] \/ agg_combined['date_block_num']\nagg_combined['item_mean_price_N'] = (agg_combined['mean_price'] - agg_combined['totel_item_price_trend']) \/ agg_combined['totel_item_price_trend']\nagg_combined['item_mean_price_N'].fillna(0,inplace=True)\n\n\n#agg_combined['mean_item_target']=agg_combined.groupby(['shop_id','item_id'])['item_sum_mth'].mean()\n\n#agg_train['item_sum_returns'] = train[train.item_cnt_day < 0].groupby(['shop_id','item_id','date_block_num'])['item_cnt_day'].sum()\n#agg_train['item_sum_returns'].fillna(0,inplace=True)\nagg_combined['item_revenue_mth'] = agg_combined['mean_price'] * agg_combined['item_sum_mth']\n                                                                        \n\n# Lagged features\nagg_combined_lagged = agg_combined\n\ndef create_lag(df,column, lags):\n    \n    for lag in lags:\n        df[column + '_lagged_'+str(lag)] = df.sort_values(['shop_id','item_id','date_block_num'])\\\n                                  .groupby(['shop_id','item_id',])[column].shift(-lag)\n        df[column + '_lagged_' +str(lag)].fillna(0,inplace=True)\n    return df\n\nagg_combined_lagged = create_lag(agg_combined_lagged,'item_sum_mth',[1,2,3,12])\nagg_combined_lagged = create_lag(agg_combined_lagged,'item_cum_sum',[1,2,3,12])\nagg_combined_lagged = create_lag(agg_combined_lagged,'item_std_mth',[1,12])\nagg_combined_lagged = create_lag(agg_combined_lagged,'item_mean_mth',[1,12])\nagg_combined_lagged = create_lag(agg_combined_lagged,'item_std_mth',[1,12])\nagg_combined_lagged = create_lag(agg_combined_lagged,'num_days_sales',[1,12])\nagg_combined_lagged = create_lag(agg_combined_lagged,'mean_price',[1])\nagg_combined_lagged = create_lag(agg_combined_lagged,'item_revenue_mth',[1])\nagg_combined_lagged = create_lag(agg_combined_lagged,'item_cum_sum',[1])\nagg_combined_lagged = create_lag(agg_combined_lagged,'cum_sum_price_N',[1])\nagg_combined_lagged = create_lag(agg_combined_lagged,'item_sum_mth_N',[1])\nagg_combined_lagged = create_lag(agg_combined_lagged,'cum_sum_price_N',[1])\nagg_combined_lagged = create_lag(agg_combined_lagged,'total_item_trend_N',[1])\nagg_combined_lagged = create_lag(agg_combined_lagged,'month_since_first_sale',[1])\nagg_combined_lagged = create_lag(agg_combined_lagged,'mth_since_l_sale',[1])\nagg_combined_lagged = create_lag(agg_combined_lagged,'item_mean_price_N',[1])\n\nagg_combined_lagged['sum_cum_sum_ratio_lagged_1'] = agg_combined_lagged['item_sum_mth_N_lagged_1'] \/ agg_combined_lagged['cum_sum_price_N_lagged_1']\nagg_combined_lagged['sum_cum_sum_ratio_lagged_1'].fillna(0,inplace=True)\n\n\nagg_combined_lagged = agg_combined_lagged.reset_index()\n\nagg_combined_lagged.sort_values(['shop_id','item_id','date_block_num'])\n\n\ndata = agg_combined_lagged.merge(items[['item_id','item_category_id']],\n                                 how='left',\n                                 on='item_id')\n\n# Add month + year\ndata['month'] = (data['date_block_num'] % 12) +1\ndata['year'] = round(data['date_block_num'] \/ 12)\n\n\n# Select feature columns\nall_columns=data.columns.tolist()\nlagged_columns = [x for x in all_columns if 'lagged' in x ]\ncategorical_columns = ['shop_id','item_id','item_category_id','month','year']\nnumeric_columns = ['date_block_num']\n\ntarget = ['item_sum_mth']\nfeatures = categorical_columns + lagged_columns  + numeric_columns\n\ndata = data[features+target + ['CATEGORY', 'ID']]\n\n# clip target\ndata[target] = data[target].clip(0,20)","b297f28b":"# split into the appropiate datasets\nsub_cols = categorical_columns + lagged_columns + numeric_columns + ['ID']\nsubmission = data.loc[data.CATEGORY=='TEST',sub_cols]\n\ndata_cols = categorical_columns + lagged_columns + numeric_columns + target\ntrain_new = data.loc[data.CATEGORY=='TRAIN',data_cols]","59314018":"def train_test_val_split_date(df,features,target, val_block, test_block):\n    train_x = df.loc[df.date_block_num < val_block,features]\n    train_y = df.loc[df.date_block_num < val_block,target]\n    \n    val_x = df.loc[(df.date_block_num >= val_block ) &(df.date_block_num < test_block),features]\n    val_y = df.loc[(df.date_block_num >= val_block ) &(df.date_block_num < test_block),target]\n    \n    \n    test_x = df.loc[df.date_block_num >= test_block,features]\n    test_y = df.loc[df.date_block_num >= test_block,target]\n    \n    return train_x, test_x,val_x, train_y, test_y, val_y\n\n\ntrain_x,test_x,val_x, train_y,test_y, val_y = train_test_val_split_date(train_new, features, target, 32,33)\n\n\ntrain_x.shape, test_x.shape,val_x.shape","7d97b3c6":"train_x[categorical_columns] = train_x[categorical_columns].astype('str')\ntest_x[categorical_columns] = test_x[categorical_columns].astype('str')\nval_x[categorical_columns] = val_x[categorical_columns].astype('str')","c6352acb":"from sklearn.metrics import mean_squared_error\nfrom catboost import Pool, CatBoostRegressor\n\ncatboost_params = {'objective':'poisson',\n                    'iterations' : 1000,\n                   'depth' : 10,\n                   'learning_rate' : 0.2,\n                   'bagging_temperature':0.2,\n                   'l2_leaf_reg' : 9,\n                   'loss_function' : 'RMSE',\n                  'task_type':'GPU',\n                   'early_stopping_rounds':20,\n                  'max_ctr_complexity':1}\n\n\ntrain_pool = Pool(train_x, train_y, cat_features=categorical_columns)\nval_pool = Pool(val_x, val_y, cat_features=categorical_columns)\ntest_pool = Pool(test_x, test_y,cat_features=categorical_columns) \n\ncat_model = CatBoostRegressor(**catboost_params)\ncat_model.fit(train_pool,\n             eval_set=val_pool)\ncat_preds = np.clip(cat_model.predict(test_pool),0,20)\n\nprint('RMSE on test: ', np.sqrt(mean_squared_error(test_y,cat_preds)))","9f346558":"# From https:\/\/www.kaggle.com\/ashishpatel26\/feature-importance-of-lightgbm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport matplotlib as mpl\n\nmpl.rcParams['text.color'] = 'w'\nmpl.rcParams['xtick.color'] = 'w'\nmpl.rcParams['ytick.color'] = 'w'\nmpl.rcParams['axes.labelcolor'] = 'w'\n\nfeat_imp=pd.DataFrame(sorted(zip(cat_model.feature_importances_,train_x.columns)), columns=['Value','Feature'])\n\nplt.figure(figsize=(10, 5))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feat_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('Catboost Features')\nplt.tight_layout()\nplt.show()","b860f79a":"corr=data[features+target].corr()\ncorr.style.background_gradient(cmap='coolwarm')","3de371ec":"from sklearn.metrics import mean_squared_error\nfrom catboost import Pool, CatBoostRegressor\n\ncatboost_params = {'iterations' : 1500,\n                   'depth' : 10,\n                   'learning_rate' : 0.3,\n                   'bagging_temperature':0.2,\n                   'l2_leaf_reg' : 9,\n                   'loss_function' : 'RMSE',\n                  'task_type':'GPU',\n                   'early_stopping_rounds':20,\n                  'max_ctr_complexity':1}\ncat_model_full = CatBoostRegressor(**catboost_params)\n\nX = data[features]\nY = data[target]\nX[categorical_columns] = X[categorical_columns].astype('str')\nfull = Pool(X,Y,cat_features=categorical_columns)\ncat_model_full.fit(full)","ca647f9f":"import lightgbm as lgb\n\nlgbm_params={'learning_rate': 0.2,\n        'objective':'poisson',\n        'metric':'rmse',\n        'verbose': 1,\n        'random_state':42       }\n\n\ntrain_x[categorical_columns] = train_x[categorical_columns].astype('category')\nval_x[categorical_columns] = val_x[categorical_columns].astype('category')\ntest_x[categorical_columns] = test_x[categorical_columns].astype('category')\n\nlgbm_model = lgb.LGBMRegressor(**lgbm_params, n_estimators=1000, categorical_feature='auto')\nlgbm_model.fit(train_x, train_y, eval_set=[(val_x, val_y)], early_stopping_rounds=50, verbose=10)\n\nlgbm_preds = lgbm_model.predict(test_x)\nprint('RMSE on test: ', np.sqrt(mean_squared_error(test_y,np.clip(lgbm_preds,0,20))))","f5605986":"# From https:\/\/www.kaggle.com\/ashishpatel26\/feature-importance-of-lightgbm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nfeat_imp=pd.DataFrame(sorted(zip(lgbm_model.feature_importances_,train_x.columns)), columns=['Value','Feature'])\n\nplt.figure(figsize=(10, 5))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feat_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()","ba30a65b":"# Train full\nX = data[features]\nX[categorical_columns] = X[categorical_columns].astype('category')\nY=data[target]","5378e330":"import lightgbm as lgb\n\nlgbm_params={'learning_rate': 0.2,\n        'objective':'poisson',\n        'metric':'rmse',\n        'random_state':42,\n        'device_type' :'cpu',\n             'nthread':-1\n       }\n\nlgbm_model_full = lgb.LGBMRegressor(**lgbm_params, n_estimators=1500)\nlgbm_model_full.fit(X, Y,categorical_feature='auto')","9bc3e3aa":"train_x[categorical_columns]=train_x[categorical_columns].astype('float32')\nval_x[categorical_columns]=val_x[categorical_columns].astype('float32')\ntest_x[categorical_columns]=test_x[categorical_columns].astype('float32')","79b877b4":"from xgboost import XGBRegressor\n\n\nxgb_params = {\n    'learning_rate':0.2,\n    'max_depth':5,\n    'min_child_weight':300, \n   'colsample_bytree':0.8, \n    'subsample':0.8, \n    'eta':0.3,    \n    'seed':42,\n'tree_method':'gpu_hist'}\n\nxgb_model = XGBRegressor(**xgb_params,n_estimators=3000)\nxgb_model.fit(train_x, train_y, eval_set=[(val_x, val_y)], early_stopping_rounds=50, verbose=10)\n\nxgb_preds = np.clip(xgb_model.predict(test_x),0,20)\nprint('RMSE on test: ', np.sqrt(mean_squared_error(test_y,xgb_preds)))","fff7a1f1":"X = data[features]\nX[categorical_columns] = X[categorical_columns].astype('int32')\nY=data[target]\n\n\nfrom xgboost import XGBRegressor\n\n\nxgb_params = {\n    'learning_rate':0.2,\n    'max_depth':5,\n    'min_child_weight':300, \n   'colsample_bytree':0.8, \n    'subsample':0.8, \n    'eta':0.3,    \n    'seed':42,\n'tree_method':'gpu_hist'}\n\nxgb_model_full = XGBRegressor(**xgb_params,n_estimators=3000)\nxgb_model_full.fit(X,Y,verbose=500)","0c4e57f8":"!pip install fastai2\n!pip install fast_tabnet","7aba64b6":"from fastai2.basics import *\nfrom fastai2.tabular.all import *\nfrom fast_tabnet.core import *","3c6142b4":"train['date'] = pd.to_datetime(train['date'])\nmin_date = train.date.min()\nmax_date = train.date.max()\ndates=pd.date_range(min_date,max_date,freq='MS').tolist()\ndates = [pd.to_datetime(x) for x in dates]\nblocks = range(0,len(dates))\ndmy=pd.DataFrame({'date_block_num' : blocks,'ds' : dates})","715a0550":"fbdata=data[['date_block_num','item_sum_mth']].merge(dmy,\n                                                     how='left',\n                                                     on='date_block_num').rename(columns={'item_sum_mth' : 'y'})","ae878e5b":"from fbprophet import Prophet\nprophet_model = Prophet()\nprophet_model.fit(fbdata)","26710ab9":"future = prophet_model.make_future_dataframe(periods=10, freq='MS')\nfcast = prophet_model.predict(future)\nfig = prophet_model.plot()","4598a104":"features","07dfa133":"#submission[categorical_columns] = submission[categorical_columns].astype('str')\nsubmission[categorical_columns] = submission[categorical_columns].astype('category')","7fc12d02":"#predictions\n#cat_preds = np.clip(cat_model.predict(submission[features]),0,20)\n#cat_model_full = cat_model_full.predict(submission[features])\n#lgbm_preds = lgbm_model.predict(submission[features])\n\n\nmodel = lgbm_model_full\npreds = np.clip(model.predict(submission[features]),0,20)\nsubmission['preds'] = preds\nresults = pd.DataFrame({'Id': submission.ID, 'item_cnt_month': submission.preds})\nresults.to_csv('submission.csv',index=False)","63da6bfb":"# Number of items available in test, but not in training\n\ntest_shops = test.shop_id.unique().tolist()\n\nmissing_items = list(np.setdiff1d(test.item_id.unique().tolist(),train.item_id.unique().tolist()))\nmissing_rows = test[test.item_id.isin(missing_items)].shape[0]\nprint('Number of items not available in training: ',len(missing_items))\nprint('Number of rows not available in training: ', missing_rows)\nprint('Number of rows for testing: ', test.shape[0])\nprint('Percentage missing: ', missing_rows * 100 \/ test.shape[0])\n\nitems_full=items.merge(item_categories,how='left',on=['item_category_id'])\nmissing=test[test.item_id.isin(missing_items)].merge(items_full,how='left',on='item_id')","21a05576":"agg_train = agg_train.merge(items[['item_id','item_category_id']],how='left',on='item_id')","85eb42ff":"# Generate a replacement item_id for items found in test, but missing in training\n# Replacement strategy : most popular item within that category for that particular shop\n\nitem_cat_map = items[items.item_id.isin(missing_items)][['item_id','item_category_id']]\nitem_cat_map_dict = dict(zip(item_cat_map.item_id.values,item_cat_map.item_category_id.values))\n\n\n\nmost_pop_item_per_shop_and_cat=agg_train.groupby(['shop_id','item_category_id'])['item_id']\\\n                                               .apply(lambda x : x.value_counts().head(1))\\\n                                               .reset_index(name='occurences')\\\n                                               .rename(columns={'level_2' : 'item_id'})\n\nmost_pop_item_per_shop_and_cat['item_id'] = most_pop_item_per_shop_and_cat['item_id'].astype('int64')\n\n\n# Assumption new items will sell similar to the most popular item within the same category sold at the same shop\nreplacement=missing.merge(most_pop_item_per_shop_and_cat,\n                          how='left',\n                          on=['shop_id','item_category_id'], \n                          suffixes=('_original','_replacement'))\n\n# Replacement kind of work, but it seems that some shops are selling items in a category they havent done before\nprint(replacement[replacement.item_id_replacement.isna()].shape,replacement.shape)\nprint('\\n')\n\ntrain_shop_cat_map = agg_train.groupby(['shop_id'])['item_category_id'] \\\n                                     .apply(lambda x: x.unique().astype('int16').tolist())\\\n                                     .to_dict()\ntest_shop_cat_map = test.merge(items[['item_id','item_category_id']],how='left',on='item_id')\\\n                        .groupby(['shop_id'])['item_category_id']\\\n                        .apply(lambda x: x.unique().astype('int16').tolist())\\\n                        .to_dict()\n\n# Check whats not matching up in train and test\nresult = {}\nfor shop, item_cat in test_shop_cat_map.items():\n    #get item categories that are not in training but in test\n    result[shop] = list(np.setdiff1d(item_cat,train_shop_cat_map[shop]))\n\nimport pprint \npprint.pprint(result)","a841374e":"# Lets use historical data from other shops on scenarios where a shop has started selling a new item in a new category\nreplacement_no_nan=missing.merge(most_pop_item_per_shop_and_cat,\n                                 how='left',\n                                 on=['item_category_id'], \n                                 suffixes=('_original','_replacement'))\nreplacement.update(replacement_no_nan,overwrite=False)\nreplacement['item_id_replacement'] = replacement['item_id_replacement'] .astype('int16')\nreplacement['occurences'] = replacement['occurences'] .astype('int16')","5d1e4b7f":"# Generate the prediction dataframe for submission\n\ncols = ['ID','shop_id','item_id_original','item_id_replacement']\n\n\n# 1 add inn replacement item_ids\nsubmission2 = test.merge(replacement[cols],\n                        how='left',\n                        left_on=['ID','shop_id','item_id'],\n                        right_on=['ID','shop_id','item_id_original'],\n                        suffixes=('','_repl'))\n# \nsubmission2['item_id_new'] = np.where(submission2.item_id_replacement.isna(),\n                                     submission2.item_id,\n                                     submission2.item_id_replacement)\n\nsubmission2['item_id_new'] = submission2['item_id_new'].astype('int16')\n\n# 2 add in  year and month columns\nlast_month_w_sale=train[['shop_id','item_id','month_date']].drop_duplicates()\\\n                                                           .reset_index(drop=True)\\\n                                                           .sort_values(['shop_id','item_id','month_date'])\\\n                                                           .groupby(['shop_id','item_id'])\\\n                                                           .tail(1)\nlast_month_w_sale['add_1mth'] = last_month_w_sale.month_date + pd.DateOffset(months=1)\nlast_month_w_sale['last_year'] = pd.to_datetime(last_month_w_sale.month_date).dt.year\nlast_month_w_sale['last_month'] = pd.to_datetime(last_month_w_sale.month_date).dt.month\nlast_month_w_sale['year'] = pd.to_datetime(last_month_w_sale.add_1mth).dt.year\nlast_month_w_sale['month'] = pd.to_datetime(last_month_w_sale.add_1mth).dt.month\n \n\nsubmission2 = submission2.merge(last_month_w_sale,how='left',\n                              left_on=['shop_id','item_id_new'],\n                              right_on=['shop_id','item_id'])\n\n# Some items are completely new for the store, the last sold dates will then be drawn from\n# the latest date _any_ item has been sold\n\n# create a map to lookup the last sold date for any store\nlast_month_fillna=train[['shop_id','month_date']].drop_duplicates()\\\n                                                .reset_index(drop=True)\\\n                                                .sort_values(['shop_id','month_date'])\\\n                                                .groupby(['shop_id'])\\\n                                                .tail(1)\n\nlast_month_fillna['add_1mth'] = last_month_fillna.month_date + pd.DateOffset(months=1)\nlast_month_fillna['last_year'] = pd.to_datetime(last_month_fillna.month_date).dt.year\nlast_month_fillna['last_month'] = pd.to_datetime(last_month_fillna.month_date).dt.month\nlast_month_fillna['year'] = pd.to_datetime(last_month_fillna.add_1mth).dt.year\nlast_month_fillna['month'] = pd.to_datetime(last_month_fillna.add_1mth).dt.month\n\nlast_month_fillna_map=last_month_fillna[['shop_id','last_year','last_month','year','month']].groupby(['shop_id'])\\\n                                                                      .apply(lambda x : x.to_dict('records'))\\\n                                                                      .to_dict()\n\n# Fill inn nans\nsubmission2.last_year.fillna(submission2.shop_id.apply(lambda x : last_month_fillna_map[x][0]['last_year']),inplace=True)\nsubmission2.last_month.fillna(submission2.shop_id.apply(lambda x : last_month_fillna_map[x][0]['last_month']),inplace=True)\nsubmission2.year.fillna(submission2.shop_id.apply(lambda x : last_month_fillna_map[x][0]['year']),inplace=True)\nsubmission2.month.fillna(submission2.shop_id.apply(lambda x : last_month_fillna_map[x][0]['month']),inplace=True)\n\n# Fixed nans, downcast to int\ncast_cols = ['last_year','last_month','year','month']\nsubmission2[cast_cols] = submission2[cast_cols].astype('int16')\n\n\"\"\"\n# 3 add inn lagged features\nsubmission2=submission2.merge(agg_train,\n                how='left',\n                left_on=['shop_id','item_id_new','last_year','last_month'],\n                right_on=['shop_id','item_id','year','month'],\n                suffixes=('','_lagged'))\n\n# rename columns into lagged\nrename_map = {'item_mean_mth':'item_mean_mth_lagged',\n             'item_std_mth':'item_std_mth_lagged',\n             'item_sum_mth':'item_sum_mth_lagged',\n             'max_price':'max_price_lagged',\n             'mean_price':'mean_price_lagged',\n             'min_price':'min_price_lagged',\n              'item_id' : 'item_id_old_2',\n             'item_id_new' : 'item_id'}\nsubmission2.rename(columns=rename_map,inplace=True)\n\n# fix nans in item_category_id\nitem_cat_map = items[['item_id','item_category_id']].set_index(['item_id']).to_dict()['item_category_id']\nsubmission.item_category_id.fillna(submission.item_id.apply(lambda x: item_cat_map[x]),inplace=True)\nsubmission['item_category_id'] = submission['item_category_id'].astype('int16') \n\n# 4 select features\nfeatures = ['ID',\n            'shop_id',\n            'item_id',\n            'item_category_id',\n            'year',\n             'month',\n             'item_mean_mth_lagged',\n             'item_std_mth_lagged',\n             'item_sum_mth_lagged',\n             'max_price_lagged',\n             'mean_price_lagged',\n             'min_price_lagged']\n\nsubmission = submission[features]\n\n# fill the rest of nans with 0, this should be okay as the lagged features entail information of last time this item\n# was sold, however the remaining corner case is that the store has not sold this item before. Hence fillna with 0 \n# should be ok\nsubmission.fillna(0,inplace=True)\n\"\"\"","9eb1f21e":"# Catboost","a9fc6804":"> # Archived","95e6b4bc":"## Strategy 2 - Generate a replacement item_id no new item_id in test that exists in training\n\nIncluding new items in training does not provide very much useful information for a model to learn (zero sales). But generating a replacement id, we may introduce some item sale similarities under the assumption that new items will sell similar to the most popular item within the same category sold at the same shop.","62f01f1c":"# Submission","c136c4bf":"# Description\n\nExperimental notebook aimed to get some experience with different frameworks for multicategorical time series forecasting GBDT methods (LightGBM, Catboost, XGBoost) vs tabnet vs Prophet. ","a08d5dee":"## LightGBM","862ad76f":"> # Test Train Split","cff6e234":"# Separate training and submission data","87c1e422":"# Pre-processing","f2b2e397":"# FBProphet\n\nNot really a good use case here, but fun to test!","73d5b2c7":"## Strategy 1 - Add new items from test into training with zero sales\n\nNot every item has been sold every month, this can create an issue whenever we do training or when we check against the test set. Solution is to map out all items for all shops and add zero for items sold. Hence every item may be available every month, but may not be sold. This way we dont introduce new item_id that the shop has never sold before","ea1a4e67":"# TabNet","aa2a6d4d":"# XGBoost"}}