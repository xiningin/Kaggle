{"cell_type":{"0bbf95fd":"code","4930d69e":"code","4a5b2c3c":"code","3e7fc997":"code","a0260833":"code","335b0342":"code","9a078947":"code","becb608f":"code","07734f78":"code","671ae750":"code","19340fc0":"markdown","098aea44":"markdown","22999633":"markdown","a2cb9247":"markdown","d86d3b47":"markdown","f075b1e2":"markdown","532c9288":"markdown","22578bb2":"markdown","8c06f9df":"markdown","ed754c78":"markdown","c3551ed2":"markdown","cf9871e3":"markdown","911723d8":"markdown","aa18d619":"markdown","a20673f1":"markdown","41907924":"markdown","55c7e7d5":"markdown"},"source":{"0bbf95fd":"#@title Load Python libraries\n\n! pip install alpha_vantage -q\n\n# pip install numpy\nimport numpy as np\n\n# pip install torch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\n# pip install matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\n\n# pip install alpha_vantage\nfrom alpha_vantage.timeseries import TimeSeries \n\nprint(\"All libraries loaded\")","4930d69e":"config = {\n    \"alpha_vantage\": {\n        \"key\": \"JS1SVUP23PNY92G7\", # Claim your free API key here: https:\/\/www.alphavantage.co\/support\/#api-key\n        \"symbol\": \"IBM\",\n        \"outputsize\": \"full\",\n        \"key_adjusted_close\": \"5. adjusted close\",\n    },\n    \"data\": {\n        \"window_size\": 20,\n        \"train_split_size\": 0.80,\n    }, \n    \"plots\": {\n        \"show_plots\": True,\n        \"xticks_interval\": 90,\n        \"color_actual\": \"#001f3f\",\n        \"color_train\": \"#3D9970\",\n        \"color_val\": \"#0074D9\",\n        \"color_pred_train\": \"#3D9970\",\n        \"color_pred_val\": \"#0074D9\",\n        \"color_pred_test\": \"#FF4136\",\n    },\n    \"model\": {\n        \"input_size\": 1, # since we are only using 1 feature, close price\n        \"num_lstm_layers\": 2,\n        \"lstm_size\": 32,\n        \"dropout\": 0.2,\n    },\n    \"training\": {\n        \"device\": \"cpu\", # \"cuda\" or \"cpu\"\n        \"batch_size\": 64,\n        \"num_epoch\": 100,\n        \"learning_rate\": 0.01,\n        \"scheduler_step_size\": 40,\n    }\n}\n","4a5b2c3c":"def download_data(config, plot=False):\n    # get the data from alpha vantage\n\n    ts = TimeSeries(key=config[\"alpha_vantage\"][\"key\"])\n    data, meta_data = ts.get_daily_adjusted(config[\"alpha_vantage\"][\"symbol\"], outputsize=config[\"alpha_vantage\"][\"outputsize\"])\n\n    data_date = [date for date in data.keys()]\n    data_date.reverse()\n\n    data_close_price = [float(data[date][config[\"alpha_vantage\"][\"key_adjusted_close\"]]) for date in data.keys()]\n    data_close_price.reverse()\n    data_close_price = np.array(data_close_price)\n\n    num_data_points = len(data_date)\n    display_date_range = \"from \" + data_date[0] + \" to \" + data_date[num_data_points-1]\n    print(\"Number data points:\", num_data_points, display_date_range)\n\n    if plot:\n        fig = figure(figsize=(25, 5), dpi=80)\n        fig.patch.set_facecolor((1.0, 1.0, 1.0))\n        plt.plot(data_date, data_close_price, color=config[\"plots\"][\"color_actual\"])\n        xticks = [data_date[i] if ((i%config[\"plots\"][\"xticks_interval\"]==0 and (num_data_points-i) > config[\"plots\"][\"xticks_interval\"]) or i==num_data_points-1) else None for i in range(num_data_points)] # make x ticks nice\n        x = np.arange(0,len(xticks))\n        plt.xticks(x, xticks, rotation='vertical')\n        plt.title(\"Daily close price for \" + config[\"alpha_vantage\"][\"symbol\"] + \", \" + display_date_range)\n        plt.grid(b=None, which='major', axis='y', linestyle='--')\n        plt.show()\n\n    return data_date, data_close_price, num_data_points, display_date_range\n\ndata_date, data_close_price, num_data_points, display_date_range = download_data(config, plot=config[\"plots\"][\"show_plots\"])","3e7fc997":"class Normalizer():\n    def __init__(self):\n        self.mu = None\n        self.sd = None\n\n    def fit_transform(self, x):\n        self.mu = np.mean(x, axis=(0), keepdims=True)\n        self.sd = np.std(x, axis=(0), keepdims=True)\n        normalized_x = (x - self.mu)\/self.sd\n        return normalized_x\n\n    def inverse_transform(self, x):\n        return (x*self.sd) + self.mu\n\n# normalize\nscaler = Normalizer()\nnormalized_data_close_price = scaler.fit_transform(data_close_price)","a0260833":"def prepare_data_x(x, window_size):\n    # perform windowing\n    n_row = x.shape[0] - window_size + 1\n    output = np.lib.stride_tricks.as_strided(x, shape=(n_row,window_size), strides=(x.strides[0],x.strides[0]))\n    return output[:-1], output[-1]\n\ndef prepare_data_y(x, window_size):\n    # # perform simple moving average\n    # output = np.convolve(x, np.ones(window_size), 'valid') \/ window_size\n\n    # use the next day as label\n    output = x[window_size:]\n    return output\n\ndef prepare_data(normalized_data_close_price, config, plot=False):\n    data_x, data_x_unseen = prepare_data_x(normalized_data_close_price, window_size=config[\"data\"][\"window_size\"])\n    data_y = prepare_data_y(normalized_data_close_price, window_size=config[\"data\"][\"window_size\"])\n\n    # split dataset\n\n    split_index = int(data_y.shape[0]*config[\"data\"][\"train_split_size\"])\n    data_x_train = data_x[:split_index]\n    data_x_val = data_x[split_index:]\n    data_y_train = data_y[:split_index]\n    data_y_val = data_y[split_index:]\n\n    if plot:\n        # prepare data for plotting\n\n        to_plot_data_y_train = np.zeros(num_data_points)\n        to_plot_data_y_val = np.zeros(num_data_points)\n\n        to_plot_data_y_train[config[\"data\"][\"window_size\"]:split_index+config[\"data\"][\"window_size\"]] = scaler.inverse_transform(data_y_train)\n        to_plot_data_y_val[split_index+config[\"data\"][\"window_size\"]:] = scaler.inverse_transform(data_y_val)\n\n        to_plot_data_y_train = np.where(to_plot_data_y_train == 0, None, to_plot_data_y_train)\n        to_plot_data_y_val = np.where(to_plot_data_y_val == 0, None, to_plot_data_y_val)\n\n        ## plots\n\n        fig = figure(figsize=(25, 5), dpi=80)\n        fig.patch.set_facecolor((1.0, 1.0, 1.0))\n        plt.plot(data_date, to_plot_data_y_train, label=\"Prices (train)\", color=config[\"plots\"][\"color_train\"])\n        plt.plot(data_date, to_plot_data_y_val, label=\"Prices (validation)\", color=config[\"plots\"][\"color_val\"])\n        xticks = [data_date[i] if ((i%config[\"plots\"][\"xticks_interval\"]==0 and (num_data_points-i) > config[\"plots\"][\"xticks_interval\"]) or i==num_data_points-1) else None for i in range(num_data_points)] # make x ticks nice\n        x = np.arange(0,len(xticks))\n        plt.xticks(x, xticks, rotation='vertical')\n        plt.title(\"Daily close prices for \" + config[\"alpha_vantage\"][\"symbol\"] + \" - showing training and validation data\")\n        plt.grid(b=None, which='major', axis='y', linestyle='--')\n        plt.legend()\n        plt.show()\n\n    return split_index, data_x_train, data_y_train, data_x_val, data_y_val, data_x_unseen\n\nsplit_index, data_x_train, data_y_train, data_x_val, data_y_val, data_x_unseen = prepare_data(normalized_data_close_price, config, plot=config[\"plots\"][\"show_plots\"])","335b0342":"class TimeSeriesDataset(Dataset):\n    def __init__(self, x, y):\n        x = np.expand_dims(x, 2) # in our case, we have only 1 feature, so we need to convert `x` into [batch, sequence, features] for LSTM\n        self.x = x.astype(np.float32)\n        self.y = y.astype(np.float32)\n        \n    def __len__(self):\n        return len(self.x)\n\n    def __getitem__(self, idx):\n        return (self.x[idx], self.y[idx])\n\ndataset_train = TimeSeriesDataset(data_x_train, data_y_train)\ndataset_val = TimeSeriesDataset(data_x_val, data_y_val)\n\nprint(\"Train data shape\", dataset_train.x.shape, dataset_train.y.shape)\nprint(\"Validation data shape\", dataset_val.x.shape, dataset_val.y.shape)","9a078947":"class LSTMModel(nn.Module):\n    def __init__(self, input_size=1, hidden_layer_size=32, num_layers=2, output_size=1, dropout=0.2):\n        super().__init__()\n        self.hidden_layer_size = hidden_layer_size\n\n        self.linear_1 = nn.Linear(input_size, hidden_layer_size)\n        self.relu = nn.ReLU()\n        self.lstm = nn.LSTM(hidden_layer_size, hidden_size=self.hidden_layer_size, num_layers=num_layers, batch_first=True)\n        self.dropout = nn.Dropout(dropout)\n        self.linear_2 = nn.Linear(num_layers*hidden_layer_size, output_size)\n        \n        self.init_weights()\n\n    def init_weights(self):\n        for name, param in self.lstm.named_parameters():\n            if 'bias' in name:\n                 nn.init.constant_(param, 0.0)\n            elif 'weight_ih' in name:\n                 nn.init.kaiming_normal_(param)\n            elif 'weight_hh' in name:\n                 nn.init.orthogonal_(param)\n\n    def forward(self, x):\n        batchsize = x.shape[0]\n\n        # layer 1\n        x = self.linear_1(x)\n        x = self.relu(x)\n        \n        # LSTM layer\n        lstm_out, (h_n, c_n) = self.lstm(x)\n\n        # reshape output from hidden cell into [batch, features] for `linear_2`\n        x = h_n.permute(1, 0, 2).reshape(batchsize, -1) \n        \n        # layer 2\n        x = self.dropout(x)\n        predictions = self.linear_2(x)\n        return predictions[:,-1]\n\nmodel = LSTMModel(input_size=config[\"model\"][\"input_size\"], hidden_layer_size=config[\"model\"][\"lstm_size\"], num_layers=config[\"model\"][\"num_lstm_layers\"], output_size=1, dropout=config[\"model\"][\"dropout\"])\nmodel = model.to(config[\"training\"][\"device\"])","becb608f":"def run_epoch(dataloader, is_training=False):\n    epoch_loss = 0\n\n    if is_training:\n        model.train()\n    else:\n        model.eval()\n\n    for idx, (x, y) in enumerate(dataloader):\n        if is_training:\n            optimizer.zero_grad()\n\n        batchsize = x.shape[0]\n\n        x = x.to(config[\"training\"][\"device\"])\n        y = y.to(config[\"training\"][\"device\"])\n\n        out = model(x)\n        loss = criterion(out.contiguous(), y.contiguous())\n\n        if is_training:\n            loss.backward()\n            optimizer.step()\n\n        epoch_loss += (loss.detach().item() \/ batchsize)\n\n    lr = scheduler.get_last_lr()[0]\n\n    return epoch_loss, lr\n\n# create `DataLoader`\ntrain_dataloader = DataLoader(dataset_train, batch_size=config[\"training\"][\"batch_size\"], shuffle=True)\nval_dataloader = DataLoader(dataset_val, batch_size=config[\"training\"][\"batch_size\"], shuffle=True)\n\n# define optimizer, scheduler and loss function\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=config[\"training\"][\"learning_rate\"], betas=(0.9, 0.98), eps=1e-9)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=config[\"training\"][\"scheduler_step_size\"], gamma=0.1)\n\n# begin training\nfor epoch in range(config[\"training\"][\"num_epoch\"]):\n    loss_train, lr_train = run_epoch(train_dataloader, is_training=True)\n    loss_val, lr_val = run_epoch(val_dataloader)\n    scheduler.step()\n    \n    print('Epoch[{}\/{}] | loss train:{:.6f}, test:{:.6f} | lr:{:.6f}'\n              .format(epoch+1, config[\"training\"][\"num_epoch\"], loss_train, loss_val, lr_train))","07734f78":"# here we re-initialize dataloader so the data doesn't shuffled, so we can plot the values by date\n\ntrain_dataloader = DataLoader(dataset_train, batch_size=config[\"training\"][\"batch_size\"], shuffle=False)\nval_dataloader = DataLoader(dataset_val, batch_size=config[\"training\"][\"batch_size\"], shuffle=False)\n\nmodel.eval()\n\n# predict on the training data, to see how well the model managed to learn and memorize\n\npredicted_train = np.array([])\n\nfor idx, (x, y) in enumerate(train_dataloader):\n    x = x.to(config[\"training\"][\"device\"])\n    out = model(x)\n    out = out.cpu().detach().numpy()\n    predicted_train = np.concatenate((predicted_train, out))\n\n# predict on the validation data, to see how the model does\n\npredicted_val = np.array([])\n\nfor idx, (x, y) in enumerate(val_dataloader):\n    x = x.to(config[\"training\"][\"device\"])\n    out = model(x)\n    out = out.cpu().detach().numpy()\n    predicted_val = np.concatenate((predicted_val, out))\n\nif config[\"plots\"][\"show_plots\"]:\n\n    # prepare data for plotting, show predicted prices\n\n    to_plot_data_y_train_pred = np.zeros(num_data_points)\n    to_plot_data_y_val_pred = np.zeros(num_data_points)\n\n    to_plot_data_y_train_pred[config[\"data\"][\"window_size\"]:split_index+config[\"data\"][\"window_size\"]] = scaler.inverse_transform(predicted_train)\n    to_plot_data_y_val_pred[split_index+config[\"data\"][\"window_size\"]:] = scaler.inverse_transform(predicted_val)\n\n    to_plot_data_y_train_pred = np.where(to_plot_data_y_train_pred == 0, None, to_plot_data_y_train_pred)\n    to_plot_data_y_val_pred = np.where(to_plot_data_y_val_pred == 0, None, to_plot_data_y_val_pred)\n\n    # plots\n\n    fig = figure(figsize=(25, 5), dpi=80)\n    fig.patch.set_facecolor((1.0, 1.0, 1.0))\n    plt.plot(data_date, data_close_price, label=\"Actual prices\", color=config[\"plots\"][\"color_actual\"])\n    plt.plot(data_date, to_plot_data_y_train_pred, label=\"Predicted prices (train)\", color=config[\"plots\"][\"color_pred_train\"])\n    plt.plot(data_date, to_plot_data_y_val_pred, label=\"Predicted prices (validation)\", color=config[\"plots\"][\"color_pred_val\"])\n    plt.title(\"Compare predicted prices to actual prices\")\n    xticks = [data_date[i] if ((i%config[\"plots\"][\"xticks_interval\"]==0 and (num_data_points-i) > config[\"plots\"][\"xticks_interval\"]) or i==num_data_points-1) else None for i in range(num_data_points)] # make x ticks nice\n    x = np.arange(0,len(xticks))\n    plt.xticks(x, xticks, rotation='vertical')\n    plt.grid(b=None, which='major', axis='y', linestyle='--')\n    plt.legend()\n    plt.show()\n\n    # prepare data for plotting, zoom in validation\n\n    to_plot_data_y_val_subset = scaler.inverse_transform(data_y_val)\n    to_plot_predicted_val = scaler.inverse_transform(predicted_val)\n    to_plot_data_date = data_date[split_index+config[\"data\"][\"window_size\"]:]\n\n    # plots\n\n    fig = figure(figsize=(25, 5), dpi=80)\n    fig.patch.set_facecolor((1.0, 1.0, 1.0))\n    plt.plot(to_plot_data_date, to_plot_data_y_val_subset, label=\"Actual prices\", color=config[\"plots\"][\"color_actual\"])\n    plt.plot(to_plot_data_date, to_plot_predicted_val, label=\"Predicted prices (validation)\", color=config[\"plots\"][\"color_pred_val\"])\n    plt.title(\"Zoom in to examine predicted price on validation data portion\")\n    xticks = [to_plot_data_date[i] if ((i%int(config[\"plots\"][\"xticks_interval\"]\/5)==0 and (len(to_plot_data_date)-i) > config[\"plots\"][\"xticks_interval\"]\/6) or i==len(to_plot_data_date)-1) else None for i in range(len(to_plot_data_date))] # make x ticks nice\n    xs = np.arange(0,len(xticks))\n    plt.xticks(xs, xticks, rotation='vertical')\n    plt.grid(b=None, which='major', axis='y', linestyle='--')\n    plt.legend()\n    plt.show()","671ae750":"# predict on the unseen data, tomorrow's price \n\nmodel.eval()\n\nx = torch.tensor(data_x_unseen).float().to(config[\"training\"][\"device\"]).unsqueeze(0).unsqueeze(2) # this is the data type and shape required, [batch, sequence, feature]\nprediction = model(x)\nprediction = prediction.cpu().detach().numpy()\nprediction = scaler.inverse_transform(prediction)[0]\n\nif config[\"plots\"][\"show_plots\"]:\n        \n    # prepare plots\n\n    plot_range = 10\n    to_plot_data_y_val = np.zeros(plot_range)\n    to_plot_data_y_val_pred = np.zeros(plot_range)\n    to_plot_data_y_test_pred = np.zeros(plot_range)\n\n    to_plot_data_y_val[:plot_range-1] = scaler.inverse_transform(data_y_val)[-plot_range+1:]\n    to_plot_data_y_val_pred[:plot_range-1] = scaler.inverse_transform(predicted_val)[-plot_range+1:]\n\n    to_plot_data_y_test_pred[plot_range-1] = prediction\n\n    to_plot_data_y_val = np.where(to_plot_data_y_val == 0, None, to_plot_data_y_val)\n    to_plot_data_y_val_pred = np.where(to_plot_data_y_val_pred == 0, None, to_plot_data_y_val_pred)\n    to_plot_data_y_test_pred = np.where(to_plot_data_y_test_pred == 0, None, to_plot_data_y_test_pred)\n\n    # plot\n\n    plot_date_test = data_date[-plot_range+1:]\n    plot_date_test.append(\"next trading day\")\n\n    fig = figure(figsize=(25, 5), dpi=80)\n    fig.patch.set_facecolor((1.0, 1.0, 1.0))\n    plt.plot(plot_date_test, to_plot_data_y_val, label=\"Actual prices\", marker=\".\", markersize=10, color=config[\"plots\"][\"color_actual\"])\n    plt.plot(plot_date_test, to_plot_data_y_val_pred, label=\"Past predicted prices\", marker=\".\", markersize=10, color=config[\"plots\"][\"color_pred_val\"])\n    plt.plot(plot_date_test, to_plot_data_y_test_pred, label=\"Predicted price for next day\", marker=\".\", markersize=20, color=config[\"plots\"][\"color_pred_test\"])\n    plt.title(\"Predicted close price of the next trading day\")\n    plt.grid(b=None, which='major', axis='y', linestyle='--')\n    plt.legend()\n    plt.show()\n\nprint(\"Predicted close price of the next trading day:\", round(prediction, 2))","19340fc0":"The LSTM model learns by iteratively making predictions given the training data X. We use mean squared error as the cost function, which measures the difference between the predicted values and the actual values. When the model is making bad predictions, the error value returned by the cost function will be relatively high. The model will fine-tune its weights through backpropagation, improving its ability to make better predictions. Learning stops when the algorithm achieves an acceptable level of performance, where the cost function on the validation dataset is no longer showing incremental improvements.\n\nWe use the [Adam optimizer](https:\/\/pytorch.org\/docs\/master\/generated\/torch.optim.Adam.html) that updates the model's parameters based on the learning rate through its step() method. This is how the model learns and fine-tunes its predictions. The learning rate controls how quickly the model converges. A learning rate that is too large can cause the model to converge too quickly to a suboptimal solution, whereas smaller learning rates require more training iterations and may result in prolonged duration for the model to find the optimal solution. We also use the [StepLR scheduler](https:\/\/pytorch.org\/docs\/master\/generated\/torch.optim.lr_scheduler.StepLR.html) to reduce the learning rate during the training process. You may also try the [ReduceLROnPlateau scheduler](https:\/\/pytorch.org\/docs\/master\/generated\/torch.optim.lr_scheduler.ReduceLROnPlateau.html), which reduces the learning rate when a cost function has stopped improving for a \"patience\" number of epochs. Choosing the proper learning rate for your project is both art and science, and is a heavily researched topic in the machine learning community.\n\nUsing mean squared error as the loss function to optimize our model, we calculate the loss on training and validation based on how well the model is doing in these two sets. After every epoch, a smaller loss value indicates that the model is learning, and 0.0 means that no mistakes were made. From the console's logs, loss train gives an idea of how well the model is learning, while loss test shows how well the model generalizes the validation dataset. A well-trained model is identified by a training and validation loss that decreases to the point of stability with relatively small differences between the two final loss values (at this stage, we say the model has \"converged\"). Generally, the loss of the model will be lower on the training than on the validation dataset.","098aea44":"Supervised machine learning methods such as LSTM learns the mapping function from input variables (X) to the output variable (Y). Learning from the training dataset can be thought of as a teacher supervising the learning process, where the teacher knows all the right answers.\n\nIn this project, we will train the model to predict the 21st day price based on the past 20 days' close prices. The number of days, 20, was selected based on a few reasons:\n\n* When LSTM models are used in natural language processing, the number of words in a sentence typically ranges from 15 to 20 words\n* Gradient descent considerations: attempting to back-propagate across very long input sequences may result in vanishing gradients\n* Longer sequences tend to have much longer training times\n\nAfter transforming the dataset into input features and output labels, the shape of our X is (5388, 20), 5388 for the number of rows, each row containing a sequence of past 20 days' prices. The corresponding Y data shape is (5388,), which matches the number of rows in X.\n\nWe also split the dataset into two parts, for training and validation. We split the data into 80:20 - 80% of the data is used for training, with the remaining 20% to verify our model's performance in predicting future prices. (Alternatively, another common practice is to split the initial data into train, validation, and test set (70\/20\/10), where the test dataset is not used at all during the training process.) This graph shows the portion of data for training and validation, approximately data before 2017 are used for training and after for verifying our model's performance.","22999633":"# Data preparation: acquiring financial market data from Alpha Vantage","a2cb9247":"# Data preparation: normalizing raw financial data","d86d3b47":"The project is grouped into the following sections, which are representative of a typical machine learning workflow:\n\n* Installing Python dependencies\n* Data preparation: acquiring financial market data from Alpha Vantage\n* Data preparation: normalizing raw data\n* Data preparation: generating training and validation datasets\n* Defining the LSTM model\n* Model training\n* Model evaluation\n* Predicting future stock prices\n\nBy the end of this project, you will have a fully functional LSTM model that predicts future stock prices based on historical price movements, all in a single Python file. This tutorial has been written in a way such that all the essential code snippets have been embedded inline. You should be able to develop, train, and test your machine learning model without referring to other external pages or documents.\n\n# Before we will start, don't forget upvote and share your opinion at the comments below!\n\nLet's get started!","f075b1e2":"With the training and evaluation data now fully normalized and prepared, we are ready to build our LSTM model!\n\nAs mentioned before, LSTM is a specialized artificial neural network architecture that can \"memorize\" patterns from historical sequences of data and extrapolate such patterns for future events. Specifically, it belongs to a group of artificial neural networks called Recurring Neural Networks (RNNs).\n\nLSTM is a popular artificial neural network because it manages to overcome many technical limitations of RNNs. For example, RNNs fail to learn when the data sequence is greater than 5 to 10 due to the vanishing gradients problem, where the gradients are vanishingly small, effectively preventing the model from learning. LSTMs can learn long sequences of data by enforcing constant error flow through self-connected hidden layers, which contain memory cells and corresponding gate units. \n\nOur artificial neural network will have three main layers, with each layer designed with a specific logical purpose:\n\n* linear layer 1 (linear_1): to map input values into a high dimensional feature space, transforming the features for the LSTM layer\n* LSTM (lstm): to learn the data in sequence\n* linear layer 2 (linear_2): to produce the predicted value based on LSTM's output\n\nWe added Dropout, where randomly selected neurons are ignored during training; therefore regularizing the network to prevent overfitting and improving overall model performance. As an optional step, we also initialize the LSTM's model weights, as some researchers have observed that it could help the model learn better.","532c9288":"# Predicting future stock prices","22578bb2":"From our results, we can see that the model has managed to learn and predict on both training (green) and validation (blue) datasets very well, as the Predicted prices lines significantly overlap with the Actual prices values.\n\nLet's zoom into the chart and look closely at the blue Predicted price (validation) segment by comparing it against its actual prices values.","8c06f9df":"# **Predicting Stock Prices with Deep Neural Networks**\n\nThis project walks you through the end-to-end data science lifecycle of developing a predictive model for stock price movements with Alpha Vantage APIs and a powerful machine learning algorithm called Long Short-Term Memory (LSTM). By completing this project, you will learn the key concepts of machine learning \/ deep learning and build a fully functional predictive model for the stock market, all in a single Python file.\n\n![](https:\/\/www.bounteous.com\/sites\/default\/files\/insights\/2020-09\/previews\/20200902_blog_-forecasting-with-time-series-models-using-python_pt2_website.png)","ed754c78":"# Data preparation: generating training and validation datasets","c3551ed2":"To visually inspect our model's performance, we will use the newly trained model to make predictions on the training and validation datasets we've created earlier in this project. If we see that the model can predict values that closely mirror the training dataset, it shows that the model managed to memorize the data. And if the model can predict values that resemble the validation dataset, it has managed to learn the patterns in our sequential data and generalize the patterns to unseen data points.\n\n","cf9871e3":"# Model training","911723d8":"# Defining the LSTM model","aa18d619":"By now, we have trained an LSTM model that can (fairly accurately) predict the next day's price based on the past 20 days' close prices. This means we now have a crystal ball in hand! Let's supply the past 20 days' close prices to the model and see what it predicts for the next trading day (i.e., the future!).\n\n","a20673f1":"# Model evaluation","41907924":"# Thanks for reading this tutorial! I'd be glad to see your upvotes and comments!","55c7e7d5":"Machine learning algorithms (such as our LSTM algorithm) that use gradient descent as the optimization technique require data to be scaled. This is due to the fact that the feature values in the model will affect the step size of the gradient descent, potentially skewing the LSTM model in unexpected ways.\n\nThis is where data normalization comes in. Normalization can increase the accuracy of your model and help the gradient descent algorithm converge more quickly towards the target minima. By bringing the input data on the same scale and reducing its variance, none of the weights in the artificial neural network will be wasted on normalizing tasks, which means the LSTM model can more efficiently learn from the data and store patterns in the network. Furthermore, LSTMs are intrinsically sensitive to the scale of the input data. For the above reasons, it is crucial to normalize the data.\n\nSince stock prices can range from tens to hundreds and thousands - $40 to $160 in the case of IBM - we will perform normalization on the stock prices to standardize the range of these values before feeding the data to the LSTM model. The following code snippet rescales the data to have a mean of 0 and the standard deviation is 1."}}