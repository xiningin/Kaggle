{"cell_type":{"40d7d1d3":"code","7976cbcc":"code","0979090c":"code","5bff1c98":"markdown"},"source":{"40d7d1d3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense, Input, GlobalAveragePooling1D, Dropout, SpatialDropout1D\nfrom tensorflow.keras.layers import LSTM, Embedding\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n\n\ndef plot_loss_evaluation(r):\n    plt.figure(figsize=(12, 8))\n\n    plt.subplot(2, 2, 1)\n    plt.plot(r.history['loss'], label='loss')\n    plt.plot(r.history['val_loss'], label='val_loss')\n    plt.legend()\n\n    plt.subplot(2, 2, 2)\n    plt.plot(r.history['accuracy'], label='accuracy')\n    plt.plot(r.history['val_accuracy'], label='val_acc')\n    plt.legend()\n    \n    plt.title('Training and Loss fuction evolution')\n    \ndef evaluate(model, X_train, X_test, y_train, y_test):\n    y_pred_train = np.round(model.predict(X_train))\n    y_pred_test = np.round(model.predict(X_test))\n    \n    print(\"=============Training Data===============\")\n    print(confusion_matrix(y_train, y_pred_train))\n    print(classification_report(y_train, y_pred_train))\n    print(f\"Accuracy score: {accuracy_score(y_train, y_pred_train) * 100:.2f}%\")\n    \n    print(\"=============Testing Data===============\")\n    print(confusion_matrix(y_test, y_pred_test))\n    print(classification_report(y_test, y_pred_test))\n    print(f\"Accuracy score: {accuracy_score(y_test, y_pred_test) * 100:.2f}%\")\n    \ndata = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\n\nprint('=============Splitting the data=============')\nX = data.text\ny = data.target\nprint(f'Data shape: {data.shape}')\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nprint(f'X_Train shape: {X_train.shape}, y_train shape: {y_train.shape}')\nprint(f'X_Test shape: {X_test.shape}, y_test shape: {y_test.shape}')\n\nprint('==============Convert Sentences to Sequences================')\nMAX_VOCAB_SIZE = 20000\ntokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, char_level=False)\ntokenizer.fit_on_texts(X_train)\nsequences_train = tokenizer.texts_to_sequences(X_train)\nsequences_test = tokenizer.texts_to_sequences(X_test)\n\n# pad sequence do that we get a NxT matrix\ndata_train = pad_sequences(sequences_train)\ndata_test = pad_sequences(sequences_test, maxlen=data_train.shape[1])\nprint(f\"Found {len(tokenizer.word_index)} unique tokens.\")\nprint(f\"Training Data shape: {data_train.shape}\")\nprint(f\"Testing Data shape: {data_test.shape}\")\n\nprint('===============Create The Model==========================')\n# We get to choose embedding dimensionality\nD = 100\n# Hidden state dimentionality\nM = 64\nV = len(tokenizer.word_index)\nT = data_train.shape[1]\n\n# model.add(embedding)\n# model.add(SpatialDropout1D(0.2))\n# model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n# model.add(Dense(1, activation='sigmoid'))\n\ni = Input(shape=(T,))\nx = Embedding(V + 1, D)(i)\nx = SpatialDropout1D(0.2)(x)\nx = LSTM(M, return_sequences=True, activation='relu')(x)\nx = GlobalAveragePooling1D()(x)\n# x = Dropout(0.2)(x)\nx = Dense(1, activation='sigmoid')(x)\n\nmodel = Model(i, x)\noptimizer = Adam(learning_rate=1e-5)\n# Compile and fit\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\nprint('Training model...........')\nr = model.fit(data_train, y_train, epochs=15, \n              validation_data=(data_test, y_test), \n              batch_size=1)\n\nprint('================Model Evaluation=====================')\nevaluate(model, data_train, data_test, y_train, y_test)\nplot_loss_evaluation(r)","7976cbcc":"# \/kaggle\/input\/nlp-getting-started\/test.csv\n# \/kaggle\/input\/nlp-getting-started\/sample_submission.csv\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n\nprint('==============Convert Sentences to Sequences================')\nsequences_test = tokenizer.texts_to_sequences(test.text)\n\n# pad sequence do that we get a NxT matrix\ndata_test = pad_sequences(sequences_test, maxlen=data_train.shape[1])\nprint(f\"Found {len(tokenizer.word_index)} unique tokens.\")\nprint(f\"Testing Data shape: {data_test.shape}\")","0979090c":"sample_sub=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\ny_pre = model.predict(data_test)\ny_pre = np.round(y_pre).astype(int).reshape(3263)\nsub = pd.DataFrame({'id':sample_sub['id'].values.tolist(), 'target':y_pre})\nsub.to_csv('submission.csv', index=False)","5bff1c98":"# 4. Making submission"}}