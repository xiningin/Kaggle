{"cell_type":{"0eb04ddb":"code","7cec7cf8":"code","2c6e043d":"code","f1c43fbf":"code","6d79cb7b":"code","d291a1b2":"code","60b02775":"code","21c1f87b":"code","ce8e5bb3":"code","ae6edc03":"code","a24718b1":"code","21be7cae":"code","f39975a0":"code","901c32e0":"code","866c6832":"code","a3818e4f":"code","6be3ebc4":"code","f8506a22":"code","6f5214ca":"code","82538fb7":"code","a0848455":"code","6d1425c2":"code","b4c2240b":"code","ce26771a":"markdown","ffd18b44":"markdown","9d25e84f":"markdown","8266a7b9":"markdown","6b5b8113":"markdown"},"source":{"0eb04ddb":"from IPython.display import Image\nImage(filename='..\/input\/bert-sentiment-architecture\/sentiment_arch_generalised.png')","7cec7cf8":"import pandas as pd\nimport numpy as np\nimport re\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score","2c6e043d":"data = pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', encoding='latin-1', header=None)\ndata.columns = ('target','uid', 'time', 'query', 'user', 'text')","f1c43fbf":"data.head(3)","6d79cb7b":"def preprocess_text(tweet):\n    tweet = tweet.lower()\n    # replace links with 'url'\n    tweet = re.sub(r'((https?:\\\/\\\/)|(www\\.))[A-Za-z0-9.\\\/]+', 'url',  tweet)\n    tweet = re.sub(r'[A-Za-z0-9]+.com', 'url',tweet)\n    # remove , @users, if any\n    tweet = re.sub(r'[@][A-Za-z0-9]+', '',tweet)\n    # remove non-ascii characters\n    tweet = ''.join([w for w in tweet if ord(w)<128])\n    #get hastags\n    \"\"\"\n    # bert tokenizer takes care of such tokens, which have some punctuation attached to it\n    # hastags will be taken care of \n    tags = ' '.join([w.strip(\"#\") for w in tweet.split() if w.startswith(\"#\")])\n    tweet = re.sub(r'[#][A-Za-z0-9]+', '',tweet)\n    \"\"\"\n    tweet = tweet.strip()\n    # return tweet, tags\n    return tweet","d291a1b2":"# Creating processed dataframe\nsent_df = pd.DataFrame(None, columns=('target', 'text'))\nsent_df['target'] = data['target']\nsent_df['text'] = data['text'].apply(preprocess_text)\nsent_df['tweet_size'] = data['text'].apply(lambda x:len(x.split()))","60b02775":"sent_df.head(3)","21c1f87b":"sent_df['tweet_size'].hist(bins = 4)","ce8e5bb3":"# max tweet size\nnp.max(sent_df['tweet_size'])","ae6edc03":"import transformers\nfrom transformers import BertModel, BertTokenizer, AdamW\nfrom transformers import get_linear_schedule_with_warmup\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader","a24718b1":"class SentConfig():\n    \"\"\"\n    Constants, config used during the project.\n    \"\"\"\n    BERT_PRETRAINED = 'bert-base-uncased'\n    TOKENIZER = BertTokenizer.from_pretrained(BERT_PRETRAINED)\n    BERT_HIDDEN_SIZE = 768\n    MAX_TOKENS_LEN = 128\n    TRAIN_BATCH_SIZE = 64\n    VALID_BATCH_SIZE = 32\n    EPOCHS = 3\n    SEED = 10","21be7cae":"# Select random subset of 3L samples, 1.50L from each category. Also tweets must have more than 10 words in it. \n\nsent_df_sample = sent_df[(sent_df['tweet_size']>10) & (sent_df['target']==0)].sample(n=250000, random_state=SentConfig.SEED)\nsent_df_sample = sent_df_sample.append(sent_df[(sent_df['tweet_size']>10) & (sent_df['target']==4)].sample(n=250000, random_state=SentConfig.SEED))\n","f39975a0":"class SentimentDL():\n    \"\"\"\n    DataLoader class, it employs mechanishm of tokenisation using bert, truncation and padding .\n    :param modified_df: any dataframe train, test, validataion\n    :return: DataLoader type object for the provided dataframe\n    \"\"\"\n    def __init__(self, modified_df):\n        self.mdf = modified_df\n    \n    def __len__(self):\n        return self.mdf.shape[0]\n    \n    def __getitem__(self, index_num):\n        \"\"\"\n        :return: dictionary of the BERT Tokenizer representaion, and the corresponding sentiment represented in long tensor (target varible)\n        \"\"\"\n        row = self.mdf.iloc[index_num]\n        tweet = row['text']\n        target = 0 if int(row['target']) is 0 else 1\n        # {0:'positive class', 1:'negative class'}\n    \n        tw_bert_tok = SentConfig.TOKENIZER(tweet)\n\n        tw_input_ids = tw_bert_tok['input_ids']\n        tw_mask = tw_bert_tok['attention_mask']\n        tw_tt_ids = tw_bert_tok['token_type_ids']\n    \n        len_ = len(tw_input_ids)\n        if len_ > SentConfig.MAX_TOKENS_LEN:\n          tw_input_ids = tw_input_ids[:SentConfig.MAX_TOKENS_LEN-1]+[102]\n          tw_mask = tw_mask[:SentConfig.MAX_TOKENS_LEN]\n          tw_tt_ids = tw_tt_ids[:SentConfig.MAX_TOKENS_LEN]\n        elif len_ < SentConfig.MAX_TOKENS_LEN:\n          pad_len = SentConfig.MAX_TOKENS_LEN - len_\n          tw_input_ids = tw_input_ids + ([0] * pad_len)\n          tw_mask = tw_mask + ([0] * pad_len)\n          tw_tt_ids = tw_tt_ids + ([0] * pad_len)\n        return {\n            'input_ids':torch.tensor(tw_input_ids, dtype=torch.long),\n            'attention_mask':torch.tensor(tw_mask, dtype=torch.long),\n            'token_type_ids':torch.tensor(tw_tt_ids, dtype=torch.long),\n            'target':torch.tensor(target, dtype=torch.long)\n        }","901c32e0":"# get train, test, validation set from our preprocessed sample of 3L.\n\ntrain, test = train_test_split(sent_df_sample, test_size=0.1)\ntrain, val = train_test_split(train, test_size=0.05)","866c6832":"# create necessary data loader objects\n\ntrain_dl = SentimentDL(train)\nval_dl = SentimentDL(val)\ntest_dl = SentimentDL(test)\n\ntrain_loader = DataLoader(train_dl, batch_size=SentConfig.TRAIN_BATCH_SIZE, shuffle=True)\nvalidation_loader = DataLoader(val_dl, batch_size=SentConfig.VALID_BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dl, batch_size=SentConfig.VALID_BATCH_SIZE, shuffle=True)","a3818e4f":"class SentimentModel(nn.Module):\n    def __init__(self):\n        super(SentimentModel, self).__init__()\n        self.bert = BertModel.from_pretrained(SentConfig.BERT_PRETRAINED)\n        self.drop_out = nn.Dropout(0.30)  # more dropout value for regularisation\n        self.linear1 = nn.Linear(SentConfig.BERT_HIDDEN_SIZE, 2)\n    \n    def forward(self, input_ids, attention_mask, tt_ids):\n        \"\"\"\n        This is forward step of our model. It takes, BERT Tokenizer generated representation of sentence, \n        and passes it through the bert -> dropout -> linear layers repectively.\n    \n        Note: out_ = token wise output (ignored) from the bert layer, as in our task (sentiment analysis) we need aggregated output \n        \"\"\"\n        out_, pooled_out = self.bert(input_ids, attention_mask, tt_ids)\n        out = self.drop_out(pooled_out)\n        out = self.linear1(out)\n        return out","6be3ebc4":"# get device type and accordingly move generated, model object to device.\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel = SentimentModel()\nmodel.to(device)","f8506a22":"\nloss_function = nn.CrossEntropyLoss()\n\n\n# Adam optimizer with weight decay AdamW from transformers\n# do not apply weight decay in AdamW  to, bias layer and normalization terms\nno_decay = ['bias', 'LayerNorm.weight', 'LayerNorm.bias']  # taken from https:\/\/huggingface.co\/transformers\/training.html \n# more named parameteres in model.named_parameters()\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n]\n\n# optimizer object\noptim = AdamW(optimizer_grouped_parameters, lr=4e-5)\n\n# learning rate scheduling\nnum_train_steps = int((train_dl.__len__()\/SentConfig.TRAIN_BATCH_SIZE)*SentConfig.EPOCHS)\nnum_warmup_steps = int(0.05*num_train_steps)\nscheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps, num_train_steps)","6f5214ca":"def train_function(data_loader, model, optimizer, scheduler, device):\n    \"\"\"\n    Function to train single epoch on the data accessible with data_loader\n    \"\"\"\n    epoch_loss = 0\n    model.train()\n    for batch in data_loader:\n        optimizer.zero_grad()\n\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        token_type_ids = batch['token_type_ids'].to(device)\n        target = batch['target'].to(device)\n\n        outputs = model(input_ids, attention_mask, token_type_ids)\n\n        batch_loss = loss_function(outputs, target)\n        batch_loss.backward()\n        optimizer.step()\n        scheduler.step()\n        epoch_loss += batch_loss.item()\n    epoch_loss = epoch_loss \/ len(data_loader)\n    return epoch_loss\n\n\ndef evaluation_function(data_loader, model, device, inference=False):\n    \"\"\"\n    Function to evaluate current model performance.\n    \"\"\"\n    epoch_loss = 0\n    model.eval()\n    results = []\n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            target = batch['target'].to(device)\n\n            outputs = model(input_ids, attention_mask, token_type_ids)\n#             if not inference:\n            batch_loss = loss_function(outputs, target)\n            epoch_loss += batch_loss.item()\n#             else:\n            outputs = torch.argmax(outputs, dim=1).to('cpu').numpy()\n            target = target.to('cpu').numpy()\n            results.extend(list(zip(outputs, target)))\n    epoch_loss = epoch_loss \/ len(data_loader)\n    return epoch_loss, np.array(results)","82538fb7":"%%time\n\n# Training : done on the basis of validation loss vs training loss\nscores = []\nmin_f1 = 0\n\nfor epoch in range(SentConfig.EPOCHS):\n  _ = train_function(train_loader, model, optim, scheduler, device)\n  _, results = evaluation_function(validation_loader, model, device)\n  validation_f1 = round(f1_score(results[:,1], results[:,0]),4)\n  accuracy = round(accuracy_score(results[:,1], results[:,0]), 4)\n  \n  print('epoch num: ', epoch, 'f1 score: ',validation_f1 , 'accuracy: ', accuracy)\n  scores.append((validation_f1, accuracy))\n\n  if validation_f1 > min_f1:\n        # save  model if validation f1 score is \n        torch.save(model.state_dict(), \"SentimentModel5L.bin\")\n        # update max loss\n        min_f1 =  validation_f1","a0848455":"# plotting losses\n\nscores = np.array(scores)\nfig, ax = plt.subplots(1, 2, figsize=(14,6))\nax[0].plot(range(SentConfig.EPOCHS), scores[:,0], 'r')\nax[1].plot(range(SentConfig.EPOCHS), scores[:,1])\nax[0].set(xlabel='Epoch num', ylabel='F1 Score')\nax[1].set(xlabel='Epoch num', ylabel='Accuracy')\nax[0].set_title('validation set f1 score at each epoch')\nax[1].set_title('validation set accuracy at each apoch')","6d1425c2":"# Load model from the trained saved model\nstate_dict_ = torch.load('SentimentModel5L.bin')\nmodel = SentimentModel()\nmodel.load_state_dict(state_dict_)\nmodel.to(device)\n\n# Calculate F1 score report\n_, results = evaluation_function(test_loader, model, device, inference=True)\nprint(classification_report(results[:,1], results[:,0]))","b4c2240b":"print(round(accuracy_score(results[:,1], results[:,0]),4))","ce26771a":"Thanks to Transformers !! :) ","ffd18b44":"\nBERT: It is a tranformer architecture model proposed in: <a href=\"https:\/\/arxiv.org\/abs\/1810.04805\">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding<\/a>. By this method, we can pre-train general purpose language model and then fine tune it for a given task. BERT takes unlabled text \nas input, mask out 15% words, and tries to predict these words. Masked sentences propogates through transformer architecture. BERT Model also takes care of the relationship between sentences, by training a simple task of predicting, whether the given sentence (in BERT input) is next to previous sentence or not. \n","9d25e84f":"### BERT Fine tuning for sentiment analysis","8266a7b9":"#### Little intro to Loss, optimizer, scheduler\n- Loss : This project uses CrossEntropyLoss, It is multiclass loss calculation function. It basically comes from logarithmic loss defination:  <span class=\"MathJax\" id=\"MathJax-Element-4-Frame\" tabindex=\"0\" style=\"text-align: center; position: relative;\" data-mathml=\"<math xmlns=&quot;http:\/\/www.w3.org\/1998\/Math\/MathML&quot; display=&quot;block&quot;><mo>&amp;#x2212;<\/mo><munderover><mo>&amp;#x2211;<\/mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>c<\/mi><mo>=<\/mo><mn>1<\/mn><\/mrow><mi>M<\/mi><\/munderover><msub><mi>y<\/mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>o<\/mi><mo>,<\/mo><mi>c<\/mi><\/mrow><\/msub><mi>log<\/mi><mo>&amp;#x2061;<\/mo><mo stretchy=&quot;false&quot;>(<\/mo><msub><mi>p<\/mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>o<\/mi><mo>,<\/mo><mi>c<\/mi><\/mrow><\/msub><mo stretchy=&quot;false&quot;>)<\/mo><\/math>\" role=\"presentation\"><nobr aria-hidden=\"true\"><span class=\"math\" id=\"MathJax-Span-35\" style=\"width: 8.87em; display: inline-block;\"><span style=\"display: inline-block; position: relative; width: 7.588em; height: 0px; font-size: 117%;\"><span style=\"position: absolute; clip: rect(0.377em, 1007.48em, 3.582em, -999.997em); top: -2.241em; left: 0em;\"><span class=\"mrow\" id=\"MathJax-Span-36\"><span class=\"mo\" id=\"MathJax-Span-37\" style=\"font-family: MathJax_Main;\">\u2212<\/span><span class=\"munderover\" id=\"MathJax-Span-38\" style=\"padding-left: 0.163em;\"><span style=\"display: inline-block; position: relative; width: 1.445em; height: 0px;\"><span style=\"position: absolute; clip: rect(2.887em, 1001.39em, 4.597em, -999.997em); top: -4.004em; left: 0em;\"><span class=\"mo\" id=\"MathJax-Span-39\" style=\"font-family: MathJax_Size2; vertical-align: 0em;\">\u2211<\/span><span style=\"display: inline-block; width: 0px; height: 4.009em;\"><\/span><\/span><span style=\"position: absolute; clip: rect(3.368em, 1001.18em, 4.276em, -999.997em); top: -2.935em; left: 0.11em;\"><span class=\"texatom\" id=\"MathJax-Span-40\"><span class=\"mrow\" id=\"MathJax-Span-41\"><span class=\"mi\" id=\"MathJax-Span-42\" style=\"font-size: 70.7%; font-family: MathJax_Math; font-style: italic;\">c<\/span><span class=\"mo\" id=\"MathJax-Span-43\" style=\"font-size: 70.7%; font-family: MathJax_Main;\">=<\/span><span class=\"mn\" id=\"MathJax-Span-44\" style=\"font-size: 70.7%; font-family: MathJax_Main;\">1<\/span><\/span><\/span><span style=\"display: inline-block; width: 0px; height: 4.009em;\"><\/span><\/span><span style=\"position: absolute; clip: rect(3.261em, 1000.75em, 4.169em, -999.997em); top: -5.179em; left: 0.377em;\"><span class=\"mi\" id=\"MathJax-Span-45\" style=\"font-size: 70.7%; font-family: MathJax_Math; font-style: italic;\">M<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.056em;\"><\/span><\/span><span style=\"display: inline-block; width: 0px; height: 4.009em;\"><\/span><\/span><\/span><\/span><span class=\"msubsup\" id=\"MathJax-Span-46\" style=\"padding-left: 0.163em;\"><span style=\"display: inline-block; position: relative; width: 1.392em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.421em, 1000.48em, 4.383em, -999.997em); top: -4.004em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-47\" style=\"font-family: MathJax_Math; font-style: italic;\">y<span style=\"display: inline-block; overflow: hidden; height: 1px; width: 0.003em;\"><\/span><\/span><span style=\"display: inline-block; width: 0px; height: 4.009em;\"><\/span><\/span><span style=\"position: absolute; top: -3.843em; left: 0.483em;\"><span class=\"texatom\" id=\"MathJax-Span-48\"><span class=\"mrow\" id=\"MathJax-Span-49\"><span class=\"mi\" id=\"MathJax-Span-50\" style=\"font-size: 70.7%; font-family: MathJax_Math; font-style: italic;\">o<\/span><span class=\"mo\" id=\"MathJax-Span-51\" style=\"font-size: 70.7%; font-family: MathJax_Main;\">,<\/span><span class=\"mi\" id=\"MathJax-Span-52\" style=\"font-size: 70.7%; font-family: MathJax_Math; font-style: italic;\">c<\/span><\/span><\/span><span style=\"display: inline-block; width: 0px; height: 4.009em;\"><\/span><\/span><\/span><\/span><span class=\"mi\" id=\"MathJax-Span-53\" style=\"font-family: MathJax_Main; padding-left: 0.163em;\">log<\/span><span class=\"mo\" id=\"MathJax-Span-54\"><\/span><span class=\"mo\" id=\"MathJax-Span-55\" style=\"font-family: MathJax_Main;\">(<\/span><span class=\"msubsup\" id=\"MathJax-Span-56\"><span style=\"display: inline-block; position: relative; width: 1.445em; height: 0px;\"><span style=\"position: absolute; clip: rect(3.421em, 1000.48em, 4.383em, -999.997em); top: -4.004em; left: 0em;\"><span class=\"mi\" id=\"MathJax-Span-57\" style=\"font-family: MathJax_Math; font-style: italic;\">p<\/span><span style=\"display: inline-block; width: 0px; height: 4.009em;\"><\/span><\/span><span style=\"position: absolute; top: -3.843em; left: 0.483em;\"><span class=\"texatom\" id=\"MathJax-Span-58\"><span class=\"mrow\" id=\"MathJax-Span-59\"><span class=\"mi\" id=\"MathJax-Span-60\" style=\"font-size: 70.7%; font-family: MathJax_Math; font-style: italic;\">o<\/span><span class=\"mo\" id=\"MathJax-Span-61\" style=\"font-size: 70.7%; font-family: MathJax_Main;\">,<\/span><span class=\"mi\" id=\"MathJax-Span-62\" style=\"font-size: 70.7%; font-family: MathJax_Math; font-style: italic;\">c<\/span><\/span><\/span><span style=\"display: inline-block; width: 0px; height: 4.009em;\"><\/span><\/span><\/span><\/span><span class=\"mo\" id=\"MathJax-Span-63\" style=\"font-family: MathJax_Main;\">)<\/span><\/span><span style=\"display: inline-block; width: 0px; height: 2.246em;\"><\/span><\/span><\/span><span style=\"display: inline-block; overflow: hidden; vertical-align: -1.434em; border-left: 0px solid; width: 0px; height: 3.566em;\"><\/span><\/span><\/nobr><span class=\"MJX_Assistive_MathML MJX_Assistive_MathML_Block\" role=\"presentation\"><math xmlns=\"http:\/\/www.w3.org\/1998\/Math\/MathML\" display=\"block\"><mo>\u2212<\/mo><munderover><mo>\u2211<\/mo><mrow class=\"MJX-TeXAtom-ORD\"><mi>c<\/mi><mo>=<\/mo><mn>1<\/mn><\/mrow><mi>M<\/mi><\/munderover><msub><mi>y<\/mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>o<\/mi><mo>,<\/mo><mi>c<\/mi><\/mrow><\/msub><mi>log<\/mi><mo>\u2061<\/mo><mo stretchy=\"false\">(<\/mo><msub><mi>p<\/mi><mrow class=\"MJX-TeXAtom-ORD\"><mi>o<\/mi><mo>,<\/mo><mi>c<\/mi><\/mrow><\/msub><mo stretchy=\"false\">)<\/mo><\/math><\/span><\/span>\n- More on Loss: <a href=\"https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.CrossEntropyLoss.html\">Cross Entropy Loss<\/a>\n\n- Optimizer: AdamW: Adam optimizer with weight decay. Algo used for optimizing weights of the network. More on Optimizer : <a href=\"https:\/\/huggingface.co\/transformers\/main_classes\/optimizer_schedules.html\">AdamW<\/a>\n\n- Scheduler: cosine_scheduler_with_warmup: It will constantly change the learning rate on train steps, according to the cosine function after warm up (some specified) steps. More on scheduler: <a href=\"https:\/\/huggingface.co\/transformers\/_modules\/transformers\/optimization.html#get_cosine_schedule_with_warmup\">Cosine scheduler<\/a>","6b5b8113":"In this excercise:\n- I fine tuned BERT *bert-base-uncased* by following the below steps\n- Took the data set, cleaned and preprocessed it for sentiment task.\n- Converted sentences into BERT Model input by using the BERT Tokenizer.\n- I took 128 as the max length of tokens for BERT Layer input.\n- If the sentence falls to less size, it has been right padded, if it is more then truncated.\n- After all this whole model follows the below architecture."}}