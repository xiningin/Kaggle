{"cell_type":{"1ecc15f0":"code","43ebe2c5":"code","18a16df0":"code","6c116cd3":"code","ec345ce8":"code","46ca402e":"code","cdfef728":"code","22668f94":"code","391b626a":"code","2f66524f":"code","72fcfdbd":"code","7ca34baa":"code","0cbdd27a":"code","97e9287e":"code","60cf341d":"code","258aa1a1":"code","b3cffd66":"code","b0eee634":"code","57bb3ad1":"code","d8f62737":"code","51f3225a":"code","d6677654":"code","104cd492":"code","2abe165f":"code","b7658177":"code","eaf8800d":"code","64276262":"code","45a8c3c2":"code","13e5bab4":"code","65a255ca":"code","17a606b7":"code","7e1b59e7":"code","a72f487e":"code","f0a0c87a":"code","effcc23f":"code","e4bbc3bb":"code","9bf2a58a":"code","20d971a1":"code","6453f7c1":"code","fca11add":"code","39d7c940":"markdown","8175c603":"markdown","d70e968a":"markdown","c2217af6":"markdown","0ca1f32f":"markdown"},"source":{"1ecc15f0":"import os\nfrom glob import glob\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport random","43ebe2c5":"data_parent = '..\/input\/adience-benchmark-gender-and-age-classification\/AdienceBenchmarkGenderAndAgeClassification\/'\nprint(os.listdir(data_parent))","18a16df0":"\nfold_0 = pd.read_csv(os.path.join(data_parent, 'fold_0_data.txt'), sep='\\t')\nfold_1 = pd.read_csv(os.path.join(data_parent, 'fold_1_data.txt'),sep='\\t')\nfold_2 = pd.read_csv(os.path.join(data_parent, 'fold_2_data.txt'),sep='\\t')\nfold_3 = pd.read_csv(os.path.join(data_parent, 'fold_3_data.txt'),sep='\\t')\nfold_4 = pd.read_csv(os.path.join(data_parent, 'fold_4_data.txt'),sep='\\t')\ntotal_data = pd.concat([fold_0, fold_1, fold_2, fold_3, fold_4], ignore_index=True)\ntotal_data.head()","6c116cd3":"print('[+] length of the file:', len(total_data))\nprint('[+] unique values of Age:')\nprint(total_data.age.unique())\nprint('===================================================')\nprint('[+] Number of None Values in Age:')\nprint((total_data.age == 'None').sum())\nprint('[+] unique values of Gender:')\nprint(total_data.gender.unique())\nprint('===================================================')\nprint('[+] Number of nan values in Gender:')\nprint(total_data.gender.isna().sum())","ec345ce8":"total_data.groupby('gender')['gender'].count().plot.pie(figsize=(10, 10))","46ca402e":"total_data.groupby('age')['age'].count().plot.pie(figsize=(10, 10))","cdfef728":"sample_num = 200\nim_name = total_data.iloc[sample_num].original_image\nim_path = os.path.join(data_parent, 'faces',str(total_data.iloc[sample_num].user_id), 'coarse_tilt_aligned_face.' + str(total_data.iloc[sample_num].face_id) + '.' + im_name)\nprint('[+] Image path:', im_path)\nimage = cv2.imread(im_path)\nprint('[+] Image shape:', image.shape)\nprint('[!] Age:', total_data.iloc[sample_num].age, 'Gender:', total_data.iloc[sample_num].gender)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))","22668f94":"images = []\nfor _ in range(16):\n    sample_num = random.randint(0, len(total_data))\n    im_name = total_data.iloc[sample_num].original_image\n    im_path = os.path.join(data_parent, 'faces',str(total_data.iloc[sample_num].user_id), 'coarse_tilt_aligned_face.' + str(total_data.iloc[sample_num].face_id) + '.' + im_name)\n    image = cv2.imread(im_path)\n    age = total_data.iloc[sample_num].age\n    gender = total_data.iloc[sample_num].gender\n    n_col = 4\n    n_rows = 4\n    images.append((image, age, gender))\n    \nfig, axs = plt.subplots(ncols=n_col, nrows=n_rows, figsize=(30,30))\ncount = 0\nfor i in range(n_rows):\n      for j in range(n_col):\n        axs[i][j].imshow(cv2.cvtColor(images[count][0], cv2.COLOR_BGR2RGB))\n        axs[i][j].set_title(f'Age: {images[count][1]}, Gender: {images[count][2]}')\n        count+=1\nplt.show()","391b626a":"age_mapping = [('(0, 2)', '0-2'), ('2', '0-2'), ('3', '0-2'), ('(4, 6)', '4-6'), ('(8, 12)', '8-13'), ('13', '8-13'), ('22', '15-20'), ('(8, 23)','15-20'), ('23', '25-32'), ('(15, 20)', '15-20'), ('(25, 32)', '25-32'), ('(27, 32)', '25-32'), ('32', '25-32'), ('34', '25-32'), ('29', '25-32'), ('(38, 42)', '38-43'), ('35', '38-43'), ('36', '38-43'), ('42', '48-53'), ('45', '38-43'), ('(38, 43)', '38-43'), ('(38, 42)', '38-43'), ('(38, 48)', '48-53'), ('46', '48-53'), ('(48, 53)', '48-53'), ('55', '48-53'), ('56', '48-53'), ('(60, 100)', '60+'), ('57', '60+'), ('58', '60+')]\nage_mapping_dict = {each[0]: each[1] for each in age_mapping}\n\ndrop_labels = []\nfor idx, each in enumerate(total_data.age):\n    if each == 'None':\n        drop_labels.append(idx)\n    else:\n        total_data.age.loc[idx] = age_mapping_dict[each]\ntotal_data = total_data.drop(labels=drop_labels, axis=0) #droped None values\ntotal_data.age.value_counts(dropna=False)","2f66524f":"total_data = total_data.dropna()\ntotal_data['full_path'] = total_data.apply(lambda x: os.path.join(data_parent, 'faces', str(x.user_id), 'coarse_tilt_aligned_face.' + str(x.face_id) + '.' + x.original_image), axis=1)\ntotal_data.age.unique(), len(total_data.age.unique()), total_data.gender.unique()","72fcfdbd":"\ngender_map = {'f':0, \n             'm':1,\n             'u':2}\nage_map = {\n    '0-2'  :0,\n    '4-6'  :1,\n    '8-13' :2,\n    '15-20':3,\n    '25-32':4,\n    '38-43':5,\n    '48-53':6,\n    '60+'  :7\n}\ntotal_data.gender = total_data.gender.replace(gender_map)\ntotal_data.age=total_data.age.replace(age_map)","7ca34baa":"gender_labels = total_data.gender.values.tolist()\nage_labels= total_data.age.values.tolist()\ntrain_paths = total_data.full_path.values.tolist()\nlen(gender_labels), gender_labels[0],len(age_labels),age_labels[0], train_paths[0]","0cbdd27a":"from sklearn.preprocessing import OneHotEncoder","97e9287e":"shuffle_list = list(zip(train_paths, gender_labels,age_labels))\nshuffle_list = random.sample(shuffle_list, len(train_paths))\ntrain_paths, gender_labels,age_labels = zip(*shuffle_list)\nage_labels = np.array(list(age_labels)).reshape((-1, 1))\nenc= OneHotEncoder()\nage_labels = enc.fit_transform(age_labels).toarray() ","60cf341d":"train_split = 0.75\ntrain_sample = int(train_split * len(total_data))\n\ntrain_data = train_paths[:train_sample]\nvalidation_data = train_paths[train_sample:]\n\ntrain_labels_gender = gender_labels[:train_sample]\nvalidation_labels_gender = gender_labels[train_sample:]\n\ntrain_labels_age=age_labels[:train_sample]\nvalidation_labels_age=age_labels[train_sample:]\nprint(\"train data count:\")\nlen(train_data), len(train_labels_gender), len(train_labels_age)\n# print(\"validation data count:\")\n# len(validation_data), len(validation_labels)","258aa1a1":"import tensorflow as tf\nimport random\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OneHotEncoder\nimport multiprocessing","b3cffd66":"# train_dataset = tf.data.Dataset.from_tensor_slices((list(train_data), list(train_labels_gender),list(train_labels_age)))\n# validation_dataset = tf.data.Dataset.from_tensor_slices((list(validation_data), list(validation_labels_gender),list(validation_labels_age)))\ntrain_dataset = tf.data.Dataset.from_tensor_slices((list(train_data), list(train_labels_age)))\nvalidation_dataset = tf.data.Dataset.from_tensor_slices((list(validation_data),list(validation_labels_age)))","b0eee634":"# for path, target1,target2 in train_dataset.take(1):\n#     print(path, target1,target2)\nfor path, target1 in train_dataset.take(1):\n    print(path, target1)","57bb3ad1":"# def preprocess_func(path, label_gender,label_age):\n#     image = tf.io.read_file(path)\n#     image = tf.image.decode_jpeg(image, channels=3)\n#     image = tf.image.resize(image, [128, 128]) \/ 255.0\n        \n#     return image, label_gender, label_age\n\ndef preprocess_func(path, label_age):\n    image = tf.io.read_file(path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, [128, 128]) \/ 255.0\n        \n    return image, label_age","d8f62737":"import multiprocessing","51f3225a":"train_batches = train_dataset.shuffle(1000).map(preprocess_func, num_parallel_calls=multiprocessing.cpu_count()).cache().batch(512).prefetch(tf.data.experimental.AUTOTUNE)\nvalidation_batches = validation_dataset.shuffle(1000).map(preprocess_func, num_parallel_calls=multiprocessing.cpu_count()).cache().batch(512).prefetch(tf.data.experimental.AUTOTUNE)","d6677654":"# for image, target1,target2 in train_batches.take(1):\n#     print(image.shape, target1.shape,target2.shape)\n#     image = tf.squeeze(image[0])\n#     print(target1[0])\n#     print(target2[0])\n#     plt.imshow(image)\n#     plt.show()\n#     break\n\nfor image, target1 in train_batches.take(1):\n    print(image.shape, target1.shape)\n    image = tf.squeeze(image[0])\n    print(target1[0])\n\n    plt.imshow(image)\n    plt.show()\n    break","104cd492":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(8, 3, padding='same', strides=2, activation='relu', input_shape=(128, 128, 3)),\n#     tf.keras.layers.MaxPooling2D(),\n#     tf.keras.layers.Dropout(0.3),\n    \n    tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),\n    tf.keras.layers.MaxPooling2D(),\n#     tf.keras.layers.Dropout(0.35),\n    \n    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n#     tf.keras.layers.MaxPooling2D(),\n#     tf.keras.layers.Dropout(0.45),\n    \n    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n    tf.keras.layers.MaxPooling2D(),\n#     tf.keras.layers.Dropout(0.5),\n    \n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(8, activation = 'softmax')\n])\n\nmodel.compile(optimizer='adam', loss= tf.losses.CategoricalCrossentropy(), metrics=['accuracy'])\nmodel.summary()","2abe165f":"tf.keras.utils.plot_model(model, show_shapes=True)","b7658177":" history = model.fit(train_batches, epochs=25, validation_data = validation_batches)","eaf8800d":"plt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0.5, 1])\nplt.legend(loc='lower right')","64276262":"image_path = validation_data[7]\nimage = cv2.imread(image_path)\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nimage = cv2.resize(image, (128, 128)) \/ 255.0\nplt.imshow(image)\nplt.show()","45a8c3c2":"image = np.expand_dims(image, 0)\nprediction = model.predict(image)","13e5bab4":"index = np.argmax(prediction)\ndecoding = {0:'0-2', 1:'4-6', 2:'8-13',3:'15-20',4:'25-32',5:'38-43',6:'48-53',7:'60+'}\n\nprint('[+] prediction is :', decoding[index]) \n# print(validation_data[7].age_labels)","65a255ca":"train_dataset2 = tf.data.Dataset.from_tensor_slices((list(train_data), list(train_labels_gender)))\nvalidation_dataset2 = tf.data.Dataset.from_tensor_slices((list(validation_data),list(validation_labels_gender)))","17a606b7":"for path, target1 in train_dataset2.take(1):\n    print(path, target1)","7e1b59e7":"def preprocess_func(path, label_gender):\n    image = tf.io.read_file(path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, [128, 128]) \/ 255.0\n        \n    return image, label_gender","a72f487e":"train_batches2 = train_dataset2.shuffle(1000).map(preprocess_func, num_parallel_calls=multiprocessing.cpu_count()).cache().batch(512).prefetch(tf.data.experimental.AUTOTUNE)\nvalidation_batches2 = validation_dataset2.shuffle(1000).map(preprocess_func, num_parallel_calls=multiprocessing.cpu_count()).cache().batch(512).prefetch(tf.data.experimental.AUTOTUNE)","f0a0c87a":"for image, target1 in train_batches.take(1):\n    print(image.shape, target1.shape)\n    image = tf.squeeze(image[0])\n    print(target1[0])\n\n    plt.imshow(image)\n    plt.show()\n    break","effcc23f":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(8, 3, padding='same', strides=2, activation='relu', input_shape=(128, 128, 3)),\n#     tf.keras.layers.MaxPooling2D(),\n#      tf.keras.layers.Dropout(0.3),\n    \n    tf.keras.layers.Conv2D(16, 3, padding='same', activation='relu'),\n    tf.keras.layers.MaxPooling2D(),\n     tf.keras.layers.Dropout(0.3),\n    \n    tf.keras.layers.Conv2D(32, 3, padding='same', activation='relu'),\n#     tf.keras.layers.MaxPooling2D(),\n#      tf.keras.layers.Dropout(0.45),\n    \n    tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu'),\n    tf.keras.layers.MaxPooling2D(),\n     tf.keras.layers.Dropout(0.4),\n    \n     tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu'),\n      tf.keras.layers.Conv2D(258, 3, padding='same', activation='relu'),\n     tf.keras.layers.MaxPooling2D(),\n     tf.keras.layers.Dropout(0.5), \n    \n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(1, activation = 'softmax')\n])\n\nmodel.compile(optimizer='adam', loss= tf.losses.CategoricalCrossentropy(), metrics=['accuracy'])\nmodel.summary()","e4bbc3bb":" history = model.fit(train_batches2, epochs=30, validation_data = validation_batches2)","9bf2a58a":"plt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0.2, 1])\nplt.legend(loc='lower right')","20d971a1":"image_path = validation_data[7]\nimage = cv2.imread(image_path)\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nimage = cv2.resize(image, (128, 128)) \/ 255.0\nplt.imshow(image)\nplt.show()","6453f7c1":"image = np.expand_dims(image, 0)\nprediction = model.predict(image)","fca11add":"index = np.argmax(prediction)\ndecoding = {0:'f', 1:'m', 2:'u'}\n\nprint('[+] prediction is :', decoding[index]) \n","39d7c940":"# Age chart","8175c603":"# gender classification with results","d70e968a":"# Age classification (training with results)","c2217af6":"# Gender chart","0ca1f32f":"# change age and gender mapping "}}