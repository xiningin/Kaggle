{"cell_type":{"9120de89":"code","73b4310e":"code","cad8048d":"code","bf4bd1cd":"code","0566b064":"code","bb770512":"code","fc12f001":"code","2215adc9":"code","1c4d63fa":"code","00d1ffbd":"code","c3f9c26d":"code","53e6cb65":"code","efc6d635":"code","9c97bb52":"code","006a5191":"code","e573b1b3":"code","cfb8d804":"code","ebdd4f63":"code","ae5e1b42":"code","b2a76bfb":"code","447187fc":"code","f30356a1":"code","96674fb8":"code","e74912a5":"markdown","dff207a4":"markdown","c7ed26a3":"markdown","9fc2dfa5":"markdown","251ee39a":"markdown","7d0ab6b3":"markdown","17b7473b":"markdown","1dc1dd9c":"markdown","92cd6141":"markdown","81dadffb":"markdown","a13b8e15":"markdown","f4d30365":"markdown"},"source":{"9120de89":"!pip install pytorch-tabnet","73b4310e":"# Preliminaries\nimport numpy as np\nimport pandas as pd \nimport os\nimport random\n\n#Visuals\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Torch and Tabnet\nimport torch\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n#Sklearn only for splitting\nfrom sklearn.model_selection import KFold","cad8048d":"NUM_FOLDS = 7  # you can specify your folds here\nseed = 2020   # seed for reproducible results","bf4bd1cd":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","0566b064":"seed_everything(seed)","bb770512":"def metric(y_true, y_pred):\n    \n    overall_score = 0\n    \n    weights = [.3, .175, .175, .175, .175]\n    \n    for i,w in zip(range(y_true.shape[1]),weights):\n        ind_score = np.mean(np.sum(np.abs(y_true[:,i] - y_pred[:,i]), axis=0)\/np.sum(y_true[:,i], axis=0))\n        overall_score += w*ind_score\n    \n    return overall_score","fc12f001":"fnc_df = pd.read_csv(\"..\/input\/trends-assessment-prediction\/fnc.csv\")\nloading_df = pd.read_csv(\"..\/input\/trends-assessment-prediction\/loading.csv\")\n\nfnc_features, loading_features = list(fnc_df.columns[1:]), list(loading_df.columns[1:])\ndf = fnc_df.merge(loading_df, on=\"Id\")\nfeatures = fnc_features + loading_features\n\n\nlabels_df = pd.read_csv(\"..\/input\/trends-assessment-prediction\/train_scores.csv\")\ntarget_features = list(labels_df.columns[1:])\nlabels_df[\"is_train\"] = True\n\n\ndf = df.merge(labels_df, on=\"Id\", how=\"left\")\n\ntest_df = df[df[\"is_train\"] != True].copy()\ndf = df[df[\"is_train\"] == True].copy()\n\ndf.shape, test_df.shape","2215adc9":"# Creating FOLDS\n\ndf = df.dropna().reset_index(drop=True)\ndf[\"kfold\"] = -1\n\ndf = df.sample(frac=1,random_state=2020).reset_index(drop=True)\n\nkf = KFold(n_splits=NUM_FOLDS)\n\nfor fold, (trn_, val_) in enumerate(kf.split(X=df, y=df)):\n    df.loc[val_, 'kfold'] = fold","1c4d63fa":"# Giving less importance to FNC features since they are easier to overfit due to high dimensionality.\nFNC_SCALE = 1\/500\n\ndf[fnc_features] *= FNC_SCALE\ntest_df[fnc_features] *= FNC_SCALE","00d1ffbd":"model = TabNetRegressor(n_d=16,\n                       n_a=16,\n                       n_steps=4,\n                       gamma=1.9,\n                       n_independent=4,\n                       n_shared=5,\n                       seed=seed,\n                       optimizer_fn = torch.optim.Adam,\n                       scheduler_params = {\"milestones\": [150,250,300,350,400,450],'gamma':0.2},\n                       scheduler_fn=torch.optim.lr_scheduler.MultiStepLR)","c3f9c26d":"y_test = np.zeros((test_df.shape[0],len(target_features), NUM_FOLDS))  #A 3D TENSOR FOR STORING RESULTS OF ALL FOLDS","53e6cb65":"def run(fold):\n    df_train = df[df.kfold != fold]\n    df_valid = df[df.kfold == fold]\n    \n    X_train = df_train[features].values\n    Y_train = df_train[target_features].values\n    \n    X_valid = df_valid[features].values\n    Y_valid = df_valid[target_features].values\n    \n    y_oof = np.zeros((df_valid.shape[0],len(target_features)))   # Out of folds validation\n    \n    print(\"--------Training Begining for fold {}-------------\".format(fold+1))\n     \n    model.fit(X_train = X_train,\n             y_train = Y_train,\n             X_valid = X_valid,\n             y_valid = Y_valid,\n             max_epochs = 1000,\n             patience =70)\n              \n    \n    print(\"--------Validating For fold {}------------\".format(fold+1))\n    \n    y_oof = model.predict(X_valid)\n    y_test[:,:,fold] = model.predict(test_df[features].values)\n    \n    val_score = metric(Y_valid,y_oof)\n    \n    print(\"Validation score: {:<8.5f}\".format(val_score))\n    \n    # VISUALIZTION\n    plt.figure(figsize=(12,6))\n    plt.plot(model.history['train']['loss'])\n    plt.plot(model.history['valid']['loss'])\n    \n    #Plotting Metric\n    #plt.plot([-x for x in model.history['train']['metric']])\n    #plt.plot([-x for x in model.history['valid']['metric']])","efc6d635":"run(fold=0)","9c97bb52":"run(fold=1)","006a5191":"run(fold=2)","e573b1b3":"run(fold=3)","cfb8d804":"run(fold=4)","ebdd4f63":"run(fold=5)","ae5e1b42":"run(fold=6)","b2a76bfb":"y_test = y_test.mean(axis=-1) # Taking mean of all the fold predictions\ntest_df[target_features] = y_test","447187fc":"test_df = test_df[[\"Id\", \"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]]","f30356a1":"sub_df = pd.melt(test_df, id_vars=[\"Id\"], value_name=\"Predicted\")\nsub_df[\"Id\"] = sub_df[\"Id\"].astype(\"str\") + \"_\" +  sub_df[\"variable\"].astype(\"str\")\n\nsub_df = sub_df.drop(\"variable\", axis=1).sort_values(\"Id\")\nassert sub_df.shape[0] == test_df.shape[0]*5\nsub_df.head(10)","96674fb8":"sub_df.to_csv('submission.csv',index=False)","e74912a5":"# Metric\n\nSince Tabnet allows us to create a MULTIREGRESSOR , we don't have to create multiple models and loop through them . I have modified the metric to account for that","dff207a4":"# Seed Everything\n\nSeeding Everything for Reproducible Results","c7ed26a3":"# Configuration","9fc2dfa5":"# Creating Submission","251ee39a":"# Data Preparation\n\nMostly Taken from Ahmet's kernel","7d0ab6b3":"# Model","17b7473b":"# Advantages of Tabnet\n\nTabnet gives us the following advantages :-\n* The best thing which I found is Tabnet allows us to train a MULTIREGRESSOR and we don't to create separate models for every class\n\n* It uses attention for selecting out the set of features to focus on for a given particular data point and we can even visualize that to see which parts get attention for a particular decision . We can also play with the number of features we want the Tabnet to focus to.\n* It uses backprop for improving decisions and weights thus providing a greater control to us\n* We can use the fine-tuning techniques that have worked for us and all the deep-learning concepts like LR annealing , Custom loss,etc\n* The headache of feature selection is vanished as Tabnet does that on its own.\n* It achieves SOTA results wothout any feature engg, finetuning with just  the defaults , wonder what it can do with sufficient feature engineering and finetuning\n\nThere are a lot of more advantages and ideas that I have for Tabnet which I plan to release in the future\n\nIf you want to learn more about Tabnet and it's inner workings please refer to this [video](https:\/\/www.youtube.com\/watch?v=ysBaZO8YmX8)","1dc1dd9c":"### I am hiding the output of training please unhide the output to look at the results and Loss plots for any fold","92cd6141":"# End Notes:\n* Tabnet allows us to have a greater control over training and predictions\n* With Tabnet we can integrate Image and Tabular data with some ideas\n* I have dropped the missing values in the targets and used raw data without any pre-processing\/feature engineering ,etc\n* I would be glad to see interesting results if someone fine tunes  it further","81dadffb":"# Engine","a13b8e15":"# Token of Gratitude\n\n* For the part other than modelling I have used most of the code from this wonderful [kernel](https:\/\/www.kaggle.com\/aerdem4\/rapids-svm-on-trends-neuroimaging) by Ahmet , Thank you for writing it \n* A big thanks to team of Pytorch-Tabnet for writing such a beautiful implementations with so much functionalities . The repo can be found [here](https:\/\/github.com\/dreamquark-ai\/tabnet)\nThe documentation is very nicely written and Sebastien has also provided with example notebooks to help understand the model and usage better. Everything can be found at above mentioned repo","f4d30365":"# About this Notebook\nIf anyone of you have read my previous kernels , you might know how much I love the EDA part , but it struck me that writing on one particular thing would not help me grow , so I have decided to explore untreaded territories to explore new things. For this competition people are mostly using Rapids and the tabular data . I have hardly seen any kernels using only images and both images and tabular data .\n\nFew days ago I saw Abhishek's post on LinkedIn about Tabnet and I was really curious about it , I wanted to apply the idea here on Trends data but it had already been done and didn't give good results so I dropped it.\n\nAfter watching Sebastian on Abhishek talks , I realized that Tabnet's potential isn't being fully utilized .\n\n**This notebook presents a fully structured working pipeline for training n-folds Tabnet Regressor for this competition . This Notebook achieves 0.1620 without a lot of efforts and this notbook could beat Rapids SVM's and achieve the benchmark 0.1595 with some tweaks . I also explain the pros and cons of using Tabnets (although I don't find a lot cons \ud83d\ude1c )**\n\nHere is the [link](https:\/\/arxiv.org\/pdf\/1908.07442.pdf) to Tabnet Paper\n\n<font color='red'>If you like my efforts please leava an upvote .As I am not planning on doing this competition for now , if you all like my efforts I plan to release more public kernels on Tabnet with higher scores<\/font>"}}