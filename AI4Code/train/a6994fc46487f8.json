{"cell_type":{"e578aef9":"code","2f8ba9f7":"code","2ea549e2":"code","d18ac5fb":"code","4fa13a3f":"code","4ca4471a":"code","78f61e02":"code","283ea888":"code","d70b4551":"code","9a709276":"code","abbc6238":"code","e08bef9f":"code","c53b048d":"code","97dc7219":"code","1fd9a78b":"code","b9973b4c":"code","e918a746":"code","70471bde":"code","18f09c2f":"code","5de4f4d1":"code","8a1b3ef6":"code","f338fee4":"code","a4b91ffd":"code","0a99fb05":"code","044af51c":"code","edb87fb4":"code","3b5e1360":"code","9f92a58b":"code","7251b618":"code","04f30839":"code","e7807cba":"code","629cef16":"code","907ff039":"code","c393a9a3":"code","73f0dc97":"code","68b2db18":"code","18d768df":"code","f7aec838":"code","ff491b74":"code","de0b151e":"code","965db98c":"code","1e8cd093":"code","625087c4":"code","afd8cd87":"code","5f6f8351":"code","761e7d3b":"code","ed5ea2dc":"code","0f98cc7e":"code","d281f169":"code","a9926362":"code","a9f6ba21":"code","e7bf903d":"code","3a7677c2":"code","2897c42a":"code","fd9e6f52":"code","4ac83cfd":"code","8471e4f8":"code","783ab5d7":"code","5926dea4":"code","f5b1e2f5":"code","bc748e1f":"code","dc0b31d4":"code","de539b81":"code","72fd051d":"code","402e499a":"code","bc90e03d":"code","4a42940b":"code","0b5d152c":"code","98b60083":"code","d2e66e5f":"code","1c791cae":"code","b685e669":"code","483e3247":"code","44f22947":"code","63902fce":"code","4eca731e":"code","813b9588":"code","d3a1382a":"markdown","aedd04eb":"markdown","5f6d8512":"markdown","70ff0c2e":"markdown","2645ca94":"markdown","26a7ae6c":"markdown","93ee86aa":"markdown","8b1f8074":"markdown","e4baa6c0":"markdown","954f7f03":"markdown","38f9fce5":"markdown","a37e423c":"markdown","a852e71c":"markdown","5ecf38eb":"markdown","201082db":"markdown","2578a26f":"markdown","fcae9429":"markdown","6a28d0d5":"markdown","6f776695":"markdown","3d470fed":"markdown","5c335393":"markdown","10b83c12":"markdown","d6a02dba":"markdown","742354be":"markdown","43e37acf":"markdown","9fa2b2b7":"markdown","c3b118cd":"markdown","cdfccfb6":"markdown","4a329186":"markdown","db42f089":"markdown","1b82177f":"markdown","ac90ffd4":"markdown","6d85edef":"markdown","4115f6db":"markdown","c8c860e5":"markdown","399c53ab":"markdown","28de160f":"markdown","a9d797b4":"markdown","7673ab35":"markdown","9edbdb48":"markdown","7fe46ccd":"markdown","06d16663":"markdown","3a13dad6":"markdown","7566eebd":"markdown","457950e2":"markdown","d4d62d26":"markdown","5308574e":"markdown","090c781b":"markdown","a1e77d74":"markdown","d4c40327":"markdown","494d262c":"markdown","938e1310":"markdown","aac7dae9":"markdown","23bbbb2e":"markdown","8e6576bb":"markdown","df0b3969":"markdown","43475b05":"markdown","f1aafd97":"markdown"},"source":{"e578aef9":"%%capture\n!pip install transformers;","2f8ba9f7":"%%capture\n!pip install wordcloud;","2ea549e2":"%%capture\n!pip install tqdm;","d18ac5fb":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom sklearn.utils import shuffle\n\n\nimport torch\nimport transformers\nfrom wordcloud import WordCloud\n\n\nimport warnings\nimport seaborn as sns\nfrom tqdm import notebook\nfrom tqdm import tqdm\n\nsns.set_style('darkgrid')\nnltk.download('punkt')\nnltk.download('wordnet')\nwarnings.filterwarnings('ignore')\nnltk.download('stopwords')\nstopwords = set(stopwords.words('english'))\n\nnp.random.seed(42)","4fa13a3f":"train = pd.read_csv('..\/input\/cleaned-toxic-comments\/train_preprocessed.csv')\ntrain.drop(['set', 'id', 'toxicity'], axis=1, inplace=True)\ndisplay(train.head())\ndisplay(train.columns)","4ca4471a":"train.isna().mean()","78f61e02":"train.info()","283ea888":"train.duplicated().sum()","d70b4551":"train['comment_text'] = train['comment_text'].str.lower()","9a709276":"train.head()","abbc6238":"cols = ['identity_hate', 'insult', 'obscene', 'severe_toxic', 'threat', 'toxic']\nfor col in cols:\n  display(train[col].value_counts(normalize=True))","e08bef9f":"def text_preprocessing(text):\n    tokenized = nltk.word_tokenize(text)\n    joined = ' '.join(tokenized)\n    text_only = re.sub(r\"[^a-z0-9!@#\\$%\\^\\&\\*_\\-,\\.' ]\", ' ', joined)\n    final = ' '.join(text_only.split())\n    return final","c53b048d":"tqdm.pandas() \ntrain['token_text'] = train['comment_text'].progress_apply(text_preprocessing)","97dc7219":"corpus_lemm = train['token_text']","1fd9a78b":"corpus_lemm[0]","b9973b4c":"x, y = np.ogrid[:300, :300]\n\nmask = (x - 150) ** 2 + (y - 150) ** 2 > 150 ** 2\nmask = 255 * mask.astype(int)\n\nwc = WordCloud(background_color=\"white\", \n               random_state=42, mask=mask, repeat=True,\n               stopwords=stopwords).generate(corpus_lemm[0])\n\nplt.figure(figsize=(15, 10), dpi=42)\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","e918a746":"train.head()","70471bde":"features = train['token_text']\ntarget = train['toxic']","18f09c2f":"features_train, features_test, target_train, target_test = train_test_split(\n    features, target, test_size=0.25, random_state=42)","5de4f4d1":"def downsample(features, target, fraction):\n    features_zeros = features[target == 0]\n    features_ones = features[target == 1]\n    target_zeros = target[target == 0]\n    target_ones = target[target == 1]\n\n    features_sample = features_zeros.sample(frac=0.1, random_state=42)\n    target_sample = target_zeros.sample(frac=0.1, random_state=42)\n    \n    features_downsampled = pd.concat([features_sample] + [features_ones])\n    target_downsampled = pd.concat([target_sample] + [target_ones])\n    \n    features_downsampled = shuffle(features_downsampled, random_state=42)\n    target_downsampled = shuffle(target_downsampled, random_state=42)\n    \n    return features_downsampled, target_downsampled","8a1b3ef6":"features_downsampled, target_downsampled = downsample(features_train, target_train, 0.1)\n\nprint(features_downsampled.shape)\nprint(target_downsampled.shape)","f338fee4":"target_downsampled.value_counts(normalize=True)","a4b91ffd":"count_tf_idf = TfidfVectorizer(stop_words=stopwords)\ntf_idf = count_tf_idf.fit_transform(features_downsampled)\n\nprint(\"Learning Matrix Size:\", tf_idf.shape)","0a99fb05":"model_name = []\nfscore = []","044af51c":"X_train_ans = tf_idf\ny_train_ans = target_downsampled","edb87fb4":"%%time\n\nrnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)\n\n\n\nX_test_ans = count_tf_idf.transform(features_test)\n\nrnd_clf.fit(X_train_ans, y_train_ans)\npredict = rnd_clf.predict(X_test_ans)\nf_score = f1_score(predict, target_test)\n\nprint('{}'.format(f_score))","3b5e1360":"model_name.append(str(rnd_clf.__class__.__name__)+str(' ')+str('downsampling)'))\nfscore.append(round(f_score, 2))","9f92a58b":"count_tf = TfidfVectorizer(stop_words=stopwords)\ntf_idf_new = count_tf.fit_transform(features_train)","7251b618":"%%time\nX_train = tf_idf_new\ny_train = target_train\n\nX_test = count_tf.transform(features_test)\n\n\nrnd_clf = RandomForestClassifier(n_estimators=10, random_state=42, \n                            class_weight='balanced')\n\n\n\nrnd_clf.fit(X_train, y_train)\npredict_new = rnd_clf.predict(X_test)","04f30839":"f_score = f1_score(predict_new, target_test)\nprint(f_score)","e7807cba":"model_name.append(str(rnd_clf.__class__.__name__)+str(' ')+str('class_weight balanced'))\nfscore.append(round(f_score, 2))","629cef16":"count_tf_idf = TfidfVectorizer(stop_words=stopwords)\ntf_idf = count_tf_idf.fit_transform(features_downsampled)\n\nprint(\"Matrix size:\", tf_idf.shape)","907ff039":"X_train = tf_idf\ny_train = target_downsampled","c393a9a3":"sgb_clf = SGDClassifier(l1_ratio=0.1, random_state=42,\n                            class_weight='balanced')","73f0dc97":"%%time\nsgb_clf.fit(X_train, y_train)\n\nX_test = count_tf_idf.transform(features_test)\n\npredict = sgb_clf.predict(X_test)\nf_score = f1_score(predict, target_test)\nprint(f_score)","68b2db18":"model_name.append(str(sgb_clf.__class__.__name__)+str(' ')+str('class_weight balanced'))\nfscore.append(round(f_score, 2))","18d768df":"X_train, X_val, y_train, y_val = train_test_split(\n    features_train, target_train, test_size=0.2, random_state=42)","f7aec838":"count_tf = TfidfVectorizer(stop_words=stopwords)\nX_train_idf = count_tf.fit_transform(X_train)\nX_val_idf = count_tf.transform(X_val)\nX_test = count_tf.transform(features_test)","ff491b74":"random_forest_clf = RandomForestClassifier(n_estimators=10, random_state=42, \n                                           class_weight='balanced')\nsgd_clf = SGDClassifier(l1_ratio=0.1, random_state=42,\n                            class_weight='balanced')\nmlp_clf = MLPClassifier(random_state=42, early_stopping=True)","de0b151e":"estimators = [random_forest_clf, sgd_clf, mlp_clf]\nfor estimator in estimators:\n    print('Training', estimator)\n    estimator.fit(X_train_idf, y_train)","965db98c":"X_val_predictions = np.empty((X_val_idf.shape[0], len(estimators)), dtype=np.float32)\n\nfor index, estimator in enumerate(estimators):\n    X_val_predictions[:, index] = estimator.predict(X_val_idf)\nX_val_predictions","1e8cd093":"rnd_forest_blender = RandomForestClassifier(n_estimators=50, oob_score=True, random_state=42)\nrnd_forest_blender.fit(X_val_predictions, y_val)","625087c4":"rnd_forest_blender.oob_score_","afd8cd87":"X_test_predictions = np.empty((X_test.shape[0], len(estimators)), dtype=np.float32)\n\nfor index, estimator in enumerate(estimators):\n    X_test_predictions[:, index] = estimator.predict(X_test)","5f6f8351":"%%time\n\ny_pred = rnd_forest_blender.predict(X_test_predictions)\nf_score = f1_score(y_pred, target_test)\nprint(f_score)","761e7d3b":"model_name.append(str(rnd_forest_blender.__class__.__name__)+str(' ')+str('Stacking Ensemble'))\nfscore.append(round(f_score, 2))","ed5ea2dc":"features = train['comment_text']\ntarget = train['toxic']\n\nfeatures_train, features_test, target_train, target_test = train_test_split(\n    features, target, test_size=0.25, random_state=42)\n\nfeatures_downsampled, target_downsampled = downsample(features_train, target_train, 0.1)\n\ntarget_downsampled.value_counts(normalize=True).to_frame()","0f98cc7e":"df_bert = features_downsampled.to_frame().join(\n    target_downsampled.to_frame())\ndf_bert.head()","d281f169":"df_bert.duplicated().sum()","a9926362":"df_bert.drop_duplicates(inplace=True)\ndf_bert.duplicated().sum()","a9f6ba21":"df_bert[df_bert.index == 115222]","e7bf903d":"df_comm = df_bert.sample(1000).reset_index(\n    drop=True)\ndf_comm.head()","3a7677c2":"df_comm['toxic'].value_counts(normalize=True).to_frame()","2897c42a":"configuration = transformers.DistilBertConfig()\nmodel = transformers.DistilBertModel(configuration)\nconfiguration = model.config\n\npretrained_weights = 'distilbert-base-uncased'\n\ntokenizer_class = transformers.DistilBertTokenizer","fd9e6f52":"tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n\ntokenized = df_comm['comment_text'].apply(\n    lambda x: tokenizer.encode(x[:512], add_special_tokens=True))\n\npadded = np.array([i + [0]*(512 - len(i)) for i in tokenized.values])\n\nattention_mask = np.where(padded != 0, 1, 0)","4ac83cfd":"len(padded[0])","8471e4f8":"padded.shape, attention_mask.shape","783ab5d7":"batch_size = 100\nembeddings = []\nfor i in notebook.tqdm(range(padded.shape[0] \/\/ batch_size)):\n        batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]) \n        attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n        \n        with torch.no_grad():\n            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n        \n        \n        embeddings.append(batch_embeddings[0][:,0,:].numpy())","5926dea4":"X_train = np.concatenate(embeddings)\ny_train = df_comm['toxic'][:padded.shape[0]]","f5b1e2f5":"y_train.values[:50]","bc748e1f":"df_comm['toxic'].values[:50]","dc0b31d4":"X_train.shape, y_train.shape","de539b81":"y_train.value_counts(normalize=True).to_frame()","72fd051d":"test = features_test.to_frame().join(\n    target_test.to_frame()).sample(200).reset_index(\n    drop=True)\ntest.head()","402e499a":"tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n\ntokenized = test['comment_text'].apply(\n    lambda x: tokenizer.encode(x[:512], add_special_tokens=True))\n\npadded = np.array([i + [0]*(512 - len(i)) for i in tokenized.values])\n\nattention_mask = np.where(padded != 0, 1, 0)","bc90e03d":"batch_size = 100\nembeddings = []\nfor i in notebook.tqdm(range(padded.shape[0] \/\/ batch_size)):\n        batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]) \n        attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n        \n        with torch.no_grad():\n            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n        \n        \n        embeddings.append(batch_embeddings[0][:,0,:].numpy())","4a42940b":"X_test = np.concatenate(embeddings)\ny_test = test['toxic'][:X_test.shape[0]]","0b5d152c":"y_test.value_counts(normalize=True).to_frame()","98b60083":"%%time\nlog_clf = LogisticRegression(solver=\"liblinear\", random_state=42,\n                             class_weight='balanced')\n\nlog_clf.fit(X_train, y_train)","d2e66e5f":"predict = log_clf.predict(X_test)","1c791cae":"f_score = f1_score(predict, y_test)\nprint(f_score)","b685e669":"model_name.append(str(log_clf.__class__.__name__)+str(' ')+str('BERT'))\nfscore.append(round(f_score, 2))","483e3247":"dummy = DummyClassifier(random_state=42, strategy='constant', constant=1)","44f22947":"dummy.fit(features_train, target_train)\ndummy_pred = dummy.predict(features_test)","63902fce":"f1_const = f1_score(target_test, dummy_pred)\n\nprint(\"Const:\", f1_const)","4eca731e":"model_name.append(str(dummy.__class__.__name__)+str(' ')+str('const 1'))\nfscore.append(round(f1_const, 2))","813b9588":"summary = pd.DataFrame(\n    { 'model' : model_name , 'F1' : fscore }\n    ).sort_values(by='F1', ascending=False).reset_index( drop = True )\n\nsummary.style.highlight_max( 'F1' , color = 'green' , axis = 0 )\n","d3a1382a":"the column `comment_text` contains the text of the comment, and `identity_hate`, `insult`, `obscene`, `severe_toxic`, `threat`, `toxic` \u2014 target features\n\nCheck the gaps","aedd04eb":"Divide our set to test and train","5f6d8512":"## Preprocessing","70ff0c2e":"**Conclusion:** Primary transformations were made, checked for gaps and duplicates. we observe an imbalance in the target class","2645ca94":"We got a fairly balanced sample.\n\nWe transform our signs in order to obtain embeddings","26a7ae6c":"### Preparing characteristics","93ee86aa":"There is an imbalance of classes in the test sample. Let's train a logistic regression model, with a balance","8b1f8074":"Thus, we got a new set, from which we will take slices, while deleting all duplicates, checking one index in order to make sure that the set was assembled adequately","e4baa6c0":"Now use TD-IDF for all three sets","954f7f03":"## Model training","38f9fce5":"![](https:\/\/i.ibb.co\/pjcBRMR\/bbc87fcc-3bb9-422a-a925-60ae8f17b019.jpg)","a37e423c":"### Train DistillBert","a852e71c":"Let's prepare a test sample. Let's take 200 random values \u200b\u200band get embeddings for the test","5ecf38eb":"\n---\n<font size=\"1\">\nArtyKraftyy\n<\/font>     \n","201082db":"We will use three base models - RandomForestClassifier, SGDClassifier and MLP, then we will blend our predictions with RandomForestClassifier","2578a26f":"For convenience, we will convert the text to lower case","fcae9429":"Let's create a set from the training balanced sample, remove duplicates from it and take a sample of 1000 values","6a28d0d5":"F1 measure is not good, the model converges poorly on the test - let's try learning without downsampling with class balance and SGD\n","6f776695":"### Imports","3d470fed":"\nWith this approach, we have decreased F1-measure","5c335393":"### Downloading data and review","10b83c12":"Unfortunately, we got a rather low result. But this fact is due to the fact that in order to reduce the training time, we had to transfer not the entire set for training, and we had to cut off the sentences that the model could work with them, which could affect the context","d6a02dba":"# Cleaned Toxic Comments with stacking","742354be":"Let's build a constant model. It will predict 1 - toxic comment everywhere, since our goal is to identify them.","43e37acf":"Create training and target datasets for our model","9fa2b2b7":"Let's train a model based on the same ensemble, but instead of a balanced set, we use the basic lemmatized one and set the class weight as balanced and set mode estimators","c3b118cd":"we have an improvement here with stacking","cdfccfb6":"Let's write a function that allows you to achieve a balance of the class, through downsampling","4a329186":"### Stacking with Random forest","db42f089":"Now we can predict our test and see the F1-measure","1b82177f":"\nLet's select from the set date the target feature and the training feature - the text","ac90ffd4":"Let's create samples for DistillBERT and remove the class imbalance in the training sample. ","6d85edef":"From the predictions let us make new trainig set for our meta model - blender","4115f6db":"Acceptable results were obtained on a model based on the SGB algorithm and we have **better score on stacking**\n\nLogistic regression based on DistillBERT to classify long texts such as comments for these purposes is not worth it - you have to truncate the text, which can affect the context, you have to limit the amount of data for training and prediction.\n\nThank you for reading","c8c860e5":"The result is better then forest (but in forest we use only 10 estimators)","399c53ab":"The dataset contains 159571 lines, the data types correspond to the desired ones","28de160f":"## Summary","a9d797b4":"**Conclusion:** Transformed the dataset and got word lemmas. In the word cloud, the most common words are explanation, dolls, edits - let's try to train the models to predict the toxicity of the text.\n\nWe going to use an Random Forest and SGD classifier, and we will also use the Distilbert to obtain and predict embeddings - perhaps we will be able to improve the results of the basic models and also we will try stacking\n\nTo speed up the DistillBERT learning process without GPU, only a part of the dataset will be transmitted, which should have a definite effect on the result. Also we will use only toxic target","7673ab35":"Received a body for further processing","9edbdb48":"It is worth noting that the model is trained to work with sentences up to 512 characters. It is necessary to cut our offers if they exceed this limit. It can also affect the results","7fe46ccd":"### Training a random forest with downsampling","06d16663":"We will receive new sets","3a13dad6":"We need to transform the text, get tokens, and also clear lines of characters. We will make the transformations through the function and library `nltk` and` re`","7566eebd":"Let's try the stochastic gradient descent model with downsampling","457950e2":"Check the sets for the form","d4d62d26":"### Training a random forest without downsampling","5308574e":"Discussing things you care about can be difficult. The threat of abuse and harassment online means that many people stop expressing themselves and give up on seeking different opinions. Platforms struggle to effectively facilitate conversations, leading many communities to limit or completely shut down user comments.\n\nWe have several target features, but let us work only with toxic in this data because of kernel limits.\n\n\n**It is just a baseline for beginners, thank you for reading and also you can see the stacking technique for classification and downsampling**\n\nNote: Dataset contains toxic vocabulary","090c781b":"We will check visually whether the target classes were selected correctly","a1e77d74":"<footer id=\"footer\"><\/footer>","d4c40327":"We got tokens of words, we can continue working with the set","494d262c":"### Sanity check","938e1310":"\nTrain an ensemble of models using the downsampling technique","aac7dae9":"### Train SGD model with downsampling","23bbbb2e":"there is a strong class imbalance. Let's try to go in two ways:\n\n- train the model on the network using **downsampling**\n- train the model with the parameter **class_weight = 'balanced'**","8e6576bb":"Let us try stacking via Sklearn models - RandomForestClassifier, SGDClassifier and MLP  \nWe need validation set","df0b3969":"The imbalance is insignificant, with such a set, you can try to train the model. First, let's get the TD-IDF measure for the new set.","43475b05":"To train the model with pretraining using DistillBERT, we will build a new set, balanced, since we will have to transfer an order of magnitude fewer rows for training, which is not an entirely adequate performance estimate","f1aafd97":"The target feature is balanced on the training sample"}}