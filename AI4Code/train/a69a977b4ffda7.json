{"cell_type":{"1f3e9d6b":"code","3dd4b1e4":"code","33e31a92":"code","f52a0588":"code","95b6d446":"code","1c9cdf7c":"code","0dabb99a":"code","92fd7edb":"code","a74bad67":"code","de239009":"code","35cf0141":"code","36bfd993":"code","c7f6c759":"code","a8bb40b1":"code","0a7b23c0":"code","b264cdfe":"code","5f5e5a18":"code","933e9a52":"code","fa79fb7e":"code","2d2ca452":"code","2537569c":"code","309b6a7c":"code","d37c0e74":"code","d13b8605":"code","c91a3c59":"code","97150fb6":"code","9711e249":"code","fe9d78ee":"code","63a67d87":"code","93e133fd":"code","a11e6257":"code","730f1235":"code","f19c53b7":"code","b2b2b7c2":"code","f41b6c50":"code","d3c34af4":"code","af293b6e":"code","c44ccea9":"code","2d363981":"code","28aefe84":"code","ab21c130":"code","b2178a35":"code","1bdd1128":"code","285a3838":"code","b43109eb":"markdown","19b35ff9":"markdown","70a0b78b":"markdown","4d7d79d5":"markdown","6db7c258":"markdown","9c37717c":"markdown","6b6251f1":"markdown","9c97bc56":"markdown","57d17c86":"markdown","5aaa8b9d":"markdown","dba330de":"markdown","3995ae76":"markdown","7dd5ed50":"markdown","c1a958e6":"markdown","0c0aba4b":"markdown","983270f9":"markdown","eca46f17":"markdown","85e60e87":"markdown","f6b31727":"markdown","bde92ab1":"markdown","691c4e3b":"markdown","ceac8961":"markdown","43dfdbec":"markdown","01ff92c3":"markdown","67ca0923":"markdown","d976a77e":"markdown","10ecf9cb":"markdown","dc86da56":"markdown","b3c5a1dc":"markdown","38d63de8":"markdown","3c7ac60b":"markdown","079baf2e":"markdown","0e53104f":"markdown"},"source":{"1f3e9d6b":"from IPython.display import HTML\n# Youtube\nHTML('<iframe width=\"560\" height=\"315\" src=\"https:\/\/pbskids.org\/apps\/media\/video\/Seesaw_v6_subtitled_ccmix.ogv\" width=\"700\" height=\"240\" frameborder=\"0\" allowfullscreen><\/iframe>')","3dd4b1e4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom IPython.core.display import display, HTML\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport plotly.express as px\nfrom plotly import tools, subplots\nfrom tqdm import tqdm_notebook as tqdm\nfrom functools import reduce\nfrom sklearn.preprocessing import LabelEncoder,StandardScaler, MinMaxScaler, OneHotEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom pandas.api.types import is_datetime64_any_dtype as is_datetime\nfrom pandas.api.types import is_categorical_dtype\nfrom sklearn.metrics import confusion_matrix,cohen_kappa_score, mean_squared_error\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold, StratifiedKFold,GroupKFold\nimport shap,random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\nfrom collections import Counter\nfrom catboost import CatBoostRegressor\nimport xgboost as xgb \nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport copy\nimport tensorflow as tf\n\npy.init_notebook_mode()\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport warnings\n\n# Any results you write to the current directory are saved as output.","33e31a92":"path='..\/input\/data-science-bowl-2019\/'","f52a0588":"sample_submission = pd.read_csv(path+'sample_submission.csv')\nspecs = pd.read_csv(path+'specs.csv')\ntest = pd.read_csv(path+'test.csv', parse_dates=[\"timestamp\"])\ntrain = pd.read_csv(path+'train.csv', parse_dates=[\"timestamp\"])\ntrain_labels = pd.read_csv(path+'train_labels.csv')","95b6d446":"train.tail()","1c9cdf7c":"def reduce_mem_usage(df, use_float16=False):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n            # skip datetime type or categorical type\n            continue\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    \n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","0dabb99a":"train=reduce_mem_usage(train)\ntest=reduce_mem_usage(test)","92fd7edb":"train.head(2)","a74bad67":"nrows = train.shape[0]\ninstallids = train[\"installation_id\"].nunique()\ngameids = train[\"game_session\"].nunique()\nevent_codes = train[\"event_code\"].nunique()\n\nmin_date = train[\"timestamp\"].min()\nmax_date = train[\"timestamp\"].max()\n\ndisplay(HTML(f\"\"\"<br>Number of rows in the dataset: {nrows:,}<\/br>\n             <br>Number of unique installation ids in the dataset: {installids:,}<\/br>\n             <br>Number of unique game sessions in the dataset: {gameids:,}<\/br>\n             <br>Number of unique event codes in the dataset: {event_codes:,}<\/br>\n             <br>Min date value in train data is {min_date}<\/br>\n             <br>Max date value in train data is {max_date}<\/br>\n             \"\"\"))\n\n","de239009":"print(f\"Unique installation ids in training data which has assessment data is: {train[train['type']=='Assessment']['installation_id'].nunique()}\")\n\ntrainv1=train[train['installation_id'].isin(train[train['type']=='Assessment']['installation_id'].unique())]\n\nprint(f\"No of unique rows after filtering:{len(trainv1)}\")\n\ndel train","35cf0141":"cnt_srs = trainv1[\"type\"].value_counts().sort_index()\n\ntrace = go.Bar(\n    x=cnt_srs.index,\n    y=cnt_srs.values,\n    marker=dict(\n        color=\"#1E90FF\",\n    ),\n)\n\nlayout = go.Layout(\n    title=go.layout.Title(\n        text=\"Type of activities\",\n        x=0.5\n    ),\n    font=dict(size=14),\n    width=500,\n    height=500,\n)\n\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"Activity type\")","36bfd993":"def _generate_bar_plot_hor(df, col, title, color, w=None, h=None, lm=0):\n    cnt_srs = df[col].value_counts().sort_values(ascending=False)\n    \n    trace = go.Bar(y=cnt_srs.index[::-1], x=cnt_srs.values[::-1], orientation = 'h',\n        marker=dict(color=color))\n\n    layout = dict(title=title, margin=dict(l=lm), width=w, height=h)\n    data = [trace]\n    fig = go.Figure(data=data, layout=layout)\n    py.iplot(fig, filename=\"World\")\n    \n_generate_bar_plot_hor(trainv1,'world', \"Count of world(section of appplication)\", '#1E90FF', 600, 400)","c7f6c759":"sample=trainv1[trainv1['installation_id']==train_labels['installation_id'].unique()[1]]\ntrain_labels[train_labels['installation_id']==sample['installation_id'].values[0]]","a8bb40b1":"noofrows=len(sample[sample['game_session']=='197a373a77101924'])\nnoofuniquetype=sample[sample['game_session']=='197a373a77101924']['type'].nunique()\nnoofuniqueworld=sample[sample['game_session']=='197a373a77101924']['world'].nunique()\nnoofuniquetitle=sample[sample['game_session']=='197a373a77101924']['title'].nunique()\n\ndisplay(HTML(f\"\"\"<br>Number of rows for game session e6a6a262a8243ff7: {noofrows:,}<\/br>\n             <br>Number of unique type for the session: {noofuniquetype:,}<\/br>\n             <br>Number of unique world category for the session: {noofuniqueworld:,}<\/br>\n             <br>Number of unique title for the session: {noofuniquetitle:,}<\/br>\"\"\"))","0a7b23c0":"def time_feature(df):\n    df['hour'] = df['timestamp'].dt.hour\n    df['day'] = df['timestamp'].dt.day\n    df['weekday'] =df['timestamp'].dt.weekday\n    df['month'] = df['timestamp'].dt.month\n    df['year'] = df['timestamp'].dt.year \n    df['date'] = df['timestamp'].dt.date \n    \n    return df\n\ntime_train=time_feature(trainv1)","b264cdfe":"def scatter_plot(cnt_srs, color):\n    trace = go.Scatter(\n        x = cnt_srs.index,\n        y = cnt_srs.values,\n        showlegend = False,\n        marker = dict(\n            color = color,\n        )\n    )\n    return trace\n\ndef get_time_plots(df):\n    print('The dataset start on {} and ends on {}'.format(df['date'].min(), df['date'].max()))\n    cnt_srs = df['date'].value_counts().sort_index()\n    trace1 = scatter_plot(cnt_srs, 'red')\n    cnt_srs = df['month'].value_counts().sort_index()\n    trace2 = scatter_plot(cnt_srs, 'blue')\n    cnt_srs = df['hour'].value_counts().sort_index()\n    trace3 = scatter_plot(cnt_srs, 'green')\n    cnt_srs = df['weekday'].value_counts().sort_index()\n    trace4 = scatter_plot(cnt_srs, 'orange')\n    \n    subtitles = ['Date Frequency', 'Month Frequency', 'Hour Frequency', 'Day of Week Frequency']\n    \n    fig = subplots.make_subplots(rows = 4, cols = 1, vertical_spacing = 0.08, subplot_titles = subtitles)\n    fig.append_trace(trace1, 1, 1)\n    fig.append_trace(trace2, 2, 1)\n    fig.append_trace(trace3, 3, 1)\n    fig.append_trace(trace4, 4, 1)\n    fig['layout'].update(height = 1200, width = 1000, paper_bgcolor = 'rgb(233, 233, 233)')\n    py.iplot(fig, filename = 'time_plots')","5f5e5a18":"get_time_plots(time_train)\ndel time_train","933e9a52":"def plot_count(feature, title, df, size=5):\n    f, ax = plt.subplots(1,1, figsize=(2*size,10))\n    total = float(len(df))\n    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:10], palette='Set2')\n    g.set_title(\"Number and percentage of {}\".format(title))\n    \n    if(size > 2):\n        plt.xticks(rotation=90, size=8)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height\/total),\n                ha=\"center\") \n    plt.show() ","fa79fb7e":"plot_count('title','Title(Activity) -distribution',trainv1[trainv1['type']=='Activity'])","2d2ca452":"plot_count('title','Title(Assessment) -distribution',trainv1[trainv1['type']=='Assessment'])","2537569c":"plot_count('title','Title(Clip) -distribution',trainv1[trainv1['type']=='Clip'])","309b6a7c":"trainv1 = trainv1[trainv1.installation_id.isin(train_labels.installation_id.unique())]\ntrainv1.shape","d37c0e74":"trainv1.drop(['hour','day','weekday','month','year'],axis=1,inplace=True)","d13b8605":"def encode_title(train, test, train_labels):\n    # encode title\n    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n    all_title_event_code = list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique()))\n    \n    # make a list with all the unique 'titles' from the train and test set\n    list_of_user_activities = list(set(train['title'].unique()).union(set(test['title'].unique())))\n    \n    # make a list with all the unique 'event_code' from the train and test set\n    list_of_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\n    list_of_event_id = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n    \n    # make a list with all the unique worlds from the train and test set\n    list_of_worlds = list(set(train['world'].unique()).union(set(test['world'].unique())))\n    \n    # create a dictionary numerating the titles\n    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n    assess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n    \n    # replace the text titles with the number titles from the dict\n    train['title'] = train['title'].map(activities_map)\n    test['title'] = test['title'].map(activities_map)\n    train['world'] = train['world'].map(activities_world)\n    test['world'] = test['world'].map(activities_world)\n    train_labels['title'] = train_labels['title'].map(activities_map)\n    win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n    \n    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n    \n    # convert text into datetime\n    train['timestamp'] = pd.to_datetime(train['timestamp'])\n    test['timestamp'] = pd.to_datetime(test['timestamp'])\n    \n    return train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code","c91a3c59":"# get usefull dict with maping encode\ntrainv1, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code = encode_title(trainv1, test, train_labels)","97150fb6":"def get_data(user_sample, test_set=False):\n    '''\n    The user_sample is a DataFrame from train or test where the only one \n    installation_id is filtered\n    And the test_set parameter is related with the labels processing, that is only requered\n    if test_set=False\n    '''\n    # Constants and parameters declaration\n    last_activity = 0\n    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n    \n    # news features: time spent in each activity\n    time_spent_each_act = {actv: 0 for actv in list_of_user_activities}\n    event_code_count = {eve: 0 for eve in list_of_event_code}\n    last_session_time_sec = 0\n    \n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy=0\n    accumulated_correct_attempts = 0 \n    accumulated_uncorrect_attempts = 0 \n    accumulated_actions = 0\n    counter = 0\n    time_first_activity = float(user_sample['timestamp'].values[0])\n    durations = []\n    \n    # itarates through each session of one instalation_id\n    for i, session in user_sample.groupby('game_session', sort=False):\n        # i = game_session_id\n        # session is a DataFrame that contain only one game_session\n        \n        # get some sessions information\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        session_title_text = activities_labels[session_title] #from Andrew\n        \n        # get current session time in seconds\n        if session_type != 'Assessment':\n            time_spent = int(session['game_time'].iloc[-1] \/ 1000)\n            time_spent_each_act[activities_labels[session_title]] += time_spent\n            \n        # for each assessment, and only this kind off session, the features below are processed\n        # and a register are generated\n        if (session_type == 'Assessment') & (test_set or len(session)>1):\n            # search for event_code 4100, that represents the assessments trial\n            all_attempts = session.query(f'event_code == {win_code[session_title]}')\n            # then, check the numbers of wins and the number of losses\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            # copy a dict to use as feature template, it's initialized with some itens: \n            # {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n            features = user_activities_count.copy()\n            features.update(time_spent_each_act.copy())\n            features.update(event_code_count.copy())\n            \n            # get installation_id for aggregated features\n            features['installation_id'] = session['installation_id'].iloc[-1] #from Andrew\n            \n            # add title as feature, remembering that title represents the name of the game\n            features['session_title'] = session['title'].iloc[0] \n            # the 4 lines below add the feature of the history of the trials of this player\n            # this is based on the all time attempts so far, at the moment of this assessment\n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            features['accumulated_uncorrect_attempts'] = accumulated_uncorrect_attempts\n            accumulated_correct_attempts += true_attempts \n            accumulated_uncorrect_attempts += false_attempts\n            # the time spent in the app so far\n            if durations == []:\n                features['duration_mean'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds)\n            # the accurace is the all time wins divided by the all time attempts\n            features['accumulated_accuracy'] = accumulated_accuracy\/counter if counter > 0 else 0\n            accuracy = true_attempts\/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n            accumulated_accuracy += accuracy\n            # a feature of the current accuracy categorized\n            # it is a counter of how many times this player was in each accuracy group\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n            features.update(accuracy_groups)\n            accuracy_groups[features['accuracy_group']] += 1\n            \n            # mean of the all accuracy groups of this player\n            features['accumulated_accuracy_group'] = accumulated_accuracy_group\/counter if counter > 0 else 0\n            accumulated_accuracy_group += features['accuracy_group']\n            \n            # how many actions the player has done so far, it is initialized as 0 and updated some lines below\n            features['accumulated_actions'] = accumulated_actions\n            \n            # there are some conditions to allow this features to be inserted in the datasets\n            # if it's a test set, all sessions belong to the final dataset\n            # it it's a train, needs to be passed throught this clausule: session.query(f'event_code == {win_code[session_title]}')\n            # that means, must exist an event_code 4100 or 4110\n            if test_set:\n                all_assessments.append(features)\n            elif true_attempts+false_attempts > 0:\n                all_assessments.append(features)\n                \n            counter += 1\n        \n        # this piece counts how many actions was made in each event_code so far\n        n_of_event_codes = Counter(session['event_code'])\n        \n        for key in n_of_event_codes.keys():\n            event_code_count[key] += n_of_event_codes[key]\n\n        # counts how many actions the player has done so far, used in the feature of the same name\n        accumulated_actions += len(session)\n        if last_activity != session_type:\n            user_activities_count[session_type] += 1\n            last_activitiy = session_type\n    # if test_set=True, only the last assessment must be predicted, the previous are scraped\n    if test_set:\n        return all_assessments[-1]\n    # in train_set, all assessments are kept\n    return all_assessments","9711e249":"compiled_data = []\n\nfor i, (ins_id, user_sample) in tqdm(enumerate(trainv1.groupby('installation_id', sort=False)), total=trainv1['installation_id'].nunique()):\n    compiled_data += get_data(user_sample)\n    \nnew_train = pd.DataFrame(compiled_data)","fe9d78ee":"del compiled_data,trainv1\nnew_train.shape","63a67d87":"new_train.head()","93e133fd":"compiled_data = []\nfor ins_id, user_sample in tqdm(test.groupby('installation_id', sort=False), total=1000):\n    a = get_data(user_sample, test_set=True)\n    compiled_data.append(a)\n    \nnew_test = pd.DataFrame(compiled_data)\ndel test,compiled_data","a11e6257":"#cat_features = ['session_title']\nnew_test.head()","730f1235":"def preprocess(reduce_train, reduce_test):\n    for df in [reduce_train, reduce_test]:\n        df['installation_session_count'] = df.groupby(['installation_id'])['Clip'].transform('count')\n        df['installation_duration_mean'] = df.groupby(['installation_id'])['duration_mean'].transform('mean')\n        #df['installation_duration_std'] = df.groupby(['installation_id'])['duration_mean'].transform('std')\n        df['installation_title_nunique'] = df.groupby(['installation_id'])['session_title'].transform('nunique')\n        \n        df['sum_event_code_count'] = df[[2050, 4100, 4230, 5000, 4235, 2060, 4110, 5010, 2070, 2075, 2080, 2081, 2083, 3110, 4010, 3120, 3121, 4020, 4021, \n                                        4022, 4025, 4030, 4031, 3010, 4035, 4040, 3020, 3021, 4045, 2000, 4050, 2010, 2020, 4070, 2025, 2030, 4080, 2035, \n                                        2040, 4090, 4220, 4095]].sum(axis = 1)\n        \n        df['installation_event_code_count_mean'] = df.groupby(['installation_id'])['sum_event_code_count'].transform('mean')\n     \n    features = reduce_train.loc[(reduce_train.sum(axis=1) != 0), (reduce_train.sum(axis=0) != 0)].columns # delete useless columns\n    \n    return reduce_train, reduce_test, features\n\n# call feature engineering function\nreduce_train, reduce_test, features = preprocess(new_train, new_test)","f19c53b7":"cols_to_drop = ['installation_id']\n\nfeatures=[feat for feat in features if feat not in cols_to_drop ]\n\nmytrain=reduce_train[features]\nmytest=reduce_test[features]","b2b2b7c2":"def qwk(act,pred,n=4,hist_range=(0,3)):\n    \n    O = confusion_matrix(act,pred)\n    O = np.divide(O,np.sum(O))\n    \n    W = np.zeros((n,n))\n    for i in range(n):\n        for j in range(n):\n            W[i][j] = ((i-j)**2)\/((n-1)**2)\n            \n    act_hist = np.histogram(act,bins=n,range=hist_range)[0]\n    prd_hist = np.histogram(pred,bins=n,range=hist_range)[0]\n    \n    E = np.outer(act_hist,prd_hist)\n    E = np.divide(E,np.sum(E))\n    \n    num = np.sum(np.multiply(W,O))\n    den = np.sum(np.multiply(W,E))\n        \n    return 1-np.divide(num,den)","f41b6c50":"def run_lgb(reduce_train, reduce_test):\n    \n    kf = StratifiedKFold(n_splits=10)\n    features = [i for i in reduce_train.columns if i not in ['accuracy_group']]\n    target = 'accuracy_group'\n    oof_pred = np.zeros((len(reduce_train)))\n    y_pred = np.zeros((len(reduce_test), 4))\n\n    for fold, (tr_ind, val_ind) in enumerate(kf.split(reduce_train,reduce_train['accuracy_group'])):\n        print('Fold {}'.format(fold + 1))\n        x_train, x_val = reduce_train[features].iloc[tr_ind], reduce_train[features].iloc[val_ind]\n        y_train, y_val = reduce_train[target][tr_ind], reduce_train[target][val_ind]\n        train_set = lgb.Dataset(x_train, y_train)#, categorical_feature=cat_features)\n        val_set = lgb.Dataset(x_val, y_val)#, categorical_feature=cat_features)\n\n        params = {\n            'learning_rate': 0.01,\n            'metric': 'multiclass',\n            'objective': 'multiclass',\n            'num_classes': 4,\n            'feature_fraction': 0.75,\n            'subsample': 0.75,\n            'n_estimators': 2000\n        }\n       \n        model = lgb.train(params, train_set, num_boost_round = 10000, early_stopping_rounds = 100, \n                          valid_sets=[train_set, val_set], verbose_eval = 100)\n        oof_pred[val_ind] = [np.argmax(i) for i in model.predict(x_val)]\n        y_pred += model.predict(reduce_test[features]) \/ 10\n\n        print('OOF QWK:', qwk(reduce_train[target], oof_pred))\n\n    return y_pred,model\n","d3c34af4":"y_pred,modelobj = run_lgb(mytrain,mytest)","af293b6e":"def run_xgb(reduce_train, reduce_test):\n    \n    kf = StratifiedKFold(n_splits=10)\n    features = [i for i in reduce_train.columns if i not in ['accuracy_group']]\n    target = 'accuracy_group'\n    oof_pred = np.zeros((len(reduce_train)))\n    y_pred = np.zeros((len(reduce_test),4))\n\n    for fold, (tr_ind, val_ind) in enumerate(kf.split(reduce_train,reduce_train['accuracy_group'])):\n        print('Fold {}'.format(fold + 1))\n        x_train, x_val = reduce_train[features].iloc[tr_ind], reduce_train[features].iloc[val_ind]\n        y_train, y_val = reduce_train[target][tr_ind], reduce_train[target][val_ind]\n        \n        train_set = xgb.DMatrix(x_train, y_train)\n        val_set = xgb.DMatrix(x_val, y_val)\n        test_set=xgb.DMatrix(reduce_test[features])\n        val_ip = xgb.DMatrix(x_val)\n\n        params = {\n            'learning_rate': 0.5,\n            'metric': 'mlogloss',\n            'objective': 'multi:softprob',\n            'feature_fraction': 0.75,\n            'subsample': 1,\n            'n_estimators': 2000,\n            'num_class': 4\n        }\n       \n        model = xgb.train(params, train_set, num_boost_round = 10000, early_stopping_rounds = 10, \n                          evals=[(train_set, 'train'), (val_set, 'val')], verbose_eval = True)\n        oof_pred[val_ind] = [np.argmax(i) for i in model.predict(val_ip)]\n        y_pred += model.predict(test_set) \/ 10\n\n        print('OOF QWK:', qwk(reduce_train[target], oof_pred))\n\n    return y_pred,model\n","c44ccea9":"y_pred_2,modelobj_2 = run_xgb(mytrain,mytest)","2d363981":"finalpred=y_pred+y_pred_2","28aefe84":"all_features = [x for x in mytrain.columns if x not in ['accuracy_group']]\n\n#select a random row\n# row_to_show = random.randint(0,mytrain.shape[0])\n# data_for_prediction = mytrain[all_features].iloc[row_to_show]  # use 1 row of data here. Could use multiple rows if desired\n# data_for_prediction_array = data_for_prediction.values.reshape(1, -1)","ab21c130":"explainer = shap.TreeExplainer(modelobj)\nshap_values = explainer.shap_values(mytrain[all_features])\nshap.initjs()\nshap.force_plot(explainer.expected_value[0], shap_values[0][0].reshape(1,-1), mytrain[all_features].loc[7].values)","b2178a35":"sns.set_style(\"whitegrid\")\nplt.title(\"Variable Importance Plot\")\nshap.summary_plot(shap_values, mytrain[all_features], plot_type=\"bar\")","1bdd1128":"mytest['accuracy_group'] = finalpred.argmax(axis = 1)\nsample_submission.drop('accuracy_group', inplace = True, axis = 1)\nsample_submission = pd.concat([sample_submission,pd.DataFrame(mytest['accuracy_group'])],axis=1,ignore_index=True)\nsample_submission.columns=['installation_id','accuracy_group']\n\nsample_submission.to_csv('submission.csv', index = False)","285a3838":"plt.bar(sample_submission['accuracy_group'].value_counts().index,sample_submission['accuracy_group'].value_counts().values)","b43109eb":"> Larger type belongs to Game and second largest is Activity, we have lesser assessment data comparitively","19b35ff9":"<font color='green' size=4>Train data preparation<\/font> ","70a0b78b":"**Create a dataframe which will have all the activities of installation id before assessment**","4d7d79d5":"As the training labels has accuracy group calculated for those who have taken their assessment. we will take ids from the training labels alone","6db7c258":"**Create time features to understand the pattern of gameplay**","9c37717c":"<font color='blue' size=5>Data Science Bowl<\/font> ","6b6251f1":"<font color='green' size=4>Model pipeline<\/font> ","9c97bc56":"<font color='green' size=4>Additional features<\/font> ","57d17c86":"* Let's analyze train labels data with respect to title and accuracy group","5aaa8b9d":"<font color='green' size=3>LGBM<\/font> ","dba330de":"**Each game session has unique title,type,world with multiple event codes lets take a random game session and display the same**","3995ae76":"*Let's aggregate the following dataframes and crunch it down and apply the same to the larger set*","7dd5ed50":"Reduce the memory of the dataset  by changing the data types which consume few bytes of memory","c1a958e6":"<font color='green' size=4>Sample data walk through<\/font> ","0c0aba4b":"The following function is replicated from this brilliant [kernel](https:\/\/www.kaggle.com\/mhviraf\/a-new-baseline-for-dsb-2019-catboost-model). It basically extracts all the type of variables for an installation ids before the assessment. Let's add more variables moving forward","983270f9":"****Stay tuned folks!!!****","eca46f17":"<font color='blue' size=3>If you think this kernel is helpful,please don't forget to click on the upvote button.<\/font> ","85e60e87":"<font color='blue' size=\"5\">Problem statement:<\/font>","f6b31727":"<font color='green' size=4>Data glimpse<\/font> ","bde92ab1":"> Most of the section of application is magmapeak followed by crystal caves","691c4e3b":"**Now before going into train labels lets check the sample user who has assessment type and analyze them**","ceac8961":"* The plot seems to be rising mid of the week\n* Also activity is gradually increasing from afternoon","43dfdbec":"<font color='green' size=4>Data Preparation<\/font> ","01ff92c3":"![PBS](https:\/\/upload.wikimedia.org\/wikipedia\/en\/7\/76\/PBS_Kids_Logo.svg)","67ca0923":"<font color='green' size=3>Model interpretability using SHAP- Shapley values<\/font> ","d976a77e":"> Few key things to note here\n* We have 17000 installation ids in training data\n* Training data is of 3 months\n* We need to predict the future assesssment category for the installation id in the test set\n* There are 1000 installation ids in test data\n* It looks like not all the installation ids have assesments in the training data","10ecf9cb":"<font color='green' size=4>Test data preparation<\/font> ","dc86da56":"<font color='green' size=4>Feature engineering<\/font> ","b3c5a1dc":"*The sample user has undergone 3 types of assessment *","38d63de8":"The intent of the competition is to use the gameplay data to forecast how many attempts a child will take to pass a given assessment (an incorrect answer is counted as an attempt. \n\nThe outcomes in this competition are grouped into 4 groups (labeled accuracy_group in the data):\n\n    3: the assessment was solved on the first attempt\n    2: the assessment was solved on the second attempt\n    1: the assessment was solved after 3 or more attempts\n    0: the assessment was never solved\n\n\n\nThis competition has various features which needs an decent exploration to move forward. Let's crack it one by one","3c7ac60b":"<font color='green' size=3>XGB<\/font> ","079baf2e":"We will drop installation ids which doesn't have assessment type as it might not add any information for training ","0e53104f":"*The following time plot is taken from this simplified [kernel](https:\/\/www.kaggle.com\/ragnar123\/simple-exploratory-data-analysis-and-model) for reference* "}}