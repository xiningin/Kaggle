{"cell_type":{"58f0d2e6":"code","28a0b95d":"code","2d9acccb":"code","6da91d6a":"code","7b69476c":"code","ca61b6ef":"code","65502a60":"code","760c507f":"code","b57f4d51":"code","89dcd72e":"code","067deadf":"code","b5121882":"code","01104274":"code","35daea73":"code","b44b51b9":"code","08931051":"markdown"},"source":{"58f0d2e6":"import cv2\nimport gc\nimport os\n\nimport numpy             as np\nfrom PIL import Image, ImageStat\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics         import confusion_matrix, accuracy_score, classification_report\n#from sklearn.model_selection import train_test_split\nfrom keras.models            import Sequential, Model\nfrom keras.layers            import Conv2D, Dropout,MaxPooling2D,Dense, Flatten, Conv2DTranspose,\\\nConcatenate, BatchNormalization, Activation, GlobalAveragePooling2D\nfrom keras import Input\n#from tensorflow.keras.layers import Input\n#from keras.callbacks import Callback\nfrom tensorflow import keras\nfrom tensorflow.keras import callbacks, optimizers\n\nfrom mlxtend.plotting import plot_confusion_matrix\n\nfrom tensorflow.keras.applications import ResNet50\n#from tensorflow.python.keras.applications import ResNet50\n#from keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.vgg19 import VGG19\nfrom tensorflow.keras.applications.mobilenet import MobileNet","28a0b95d":"base_dir= \"..\/input\/pediatric-pneumonia-chest-xray\/Pediatric Chest X-ray Pneumonia\/\"","2d9acccb":"# manual cleaning for black and white images. Not very time efficient... but it does not crash\ndef Targetization(folder_path, target_path, IMG_SIZE=250, color=False): \n    channels = 3 if color else 1\n    images = []\n    label = []\n    for path, tag in target_path:\n        for filename in os.listdir(folder_path + path):\n            img = cv2.imread(os.path.join(folder_path, path, filename), int(color)) # second param: 0 is b&w picture\n            if img is not None:\n                img = cv2.resize(img\/255,(IMG_SIZE,IMG_SIZE)).astype('float16') # key to reduce memory\n                images.append(img)\n                label.append(tag)\n    return np.array(images).reshape(len(images), IMG_SIZE, IMG_SIZE, channels), np.array(label)","6da91d6a":"# b&w clean data of 250x250\n#x_train, y_train = Targetization(base_dir, target_path=[(\"train\/NORMAL\/\", 0,), (\"train\/PNEUMONIA\/\", 1,)])\n#x_test, y_test = Targetization(base_dir,  target_path=[(\"test\/NORMAL\/\", 0,), (\"test\/PNEUMONIA\/\", 1,)])","7b69476c":"# color clean data of 224x224. ONly need those for this part. Check the b&w on the first part notebook.\nx_train_c, y_train = Targetization(base_dir, \n                                 target_path=[(\"train\/NORMAL\/\", 0,), (\"train\/PNEUMONIA\/\", 1,)],\n                                 IMG_SIZE=224, color=True)\nx_test_c, y_test = Targetization(base_dir, \n                                 target_path=[(\"test\/NORMAL\/\", 0,), (\"test\/PNEUMONIA\/\", 1,)],\n                                 IMG_SIZE=224, color=True)","ca61b6ef":"# the default float 64 consumes too much memory\nx_train_c.dtype","65502a60":"print(y_train.shape)\nprint(y_test.shape)\nprint(x_train_c.shape)\nprint(x_test_c.shape)","760c507f":"print(y_test[0])","b57f4d51":"def inception_model(input_shape= (300, 300,3,)):  \n    # inceptionV3 model, with include_top= False we are not using fully connected layer of the inceptionV3 model, instead we\n    #  will create our own Fully Connected and Output Layer according to our training data\n    inception_model= InceptionV3(input_shape=input_shape, include_top= False, weights=\"imagenet\")\n\n    # Since we are creating our own fully connected layer we need output of the last inception model layer and flatten them \n    last_output= inception_model.layers[-1].output\n\n    # Flattening the last output\n    last_output= Flatten()(last_output)\n\n    # Our pretrained model\n    pretrained_model= Model(inception_model.input, last_output)    \n    \n        # layer 1\n    x= Dense(units=512, activation=\"relu\")(last_output)\n    x=Dropout(0.2)(x)\n\n    # layer 2\n    x= Dense(units=128, activation=\"relu\")(x)\n    x=Dropout(0.2)(x)\n\n    # output layer\n    x= Dense(units=1, activation=\"sigmoid\")(x)\n\n    # final model\n    model= Model(pretrained_model.input, x)\n    \n    # compile model\n    model.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.RMSprop(learning_rate=1e-4), metrics=[\"accuracy\"])\n\n    # Since the layers of InceptionV3 model are already trained, we don't want them to be trained again. \n    # So we will freeze these layers\n    for layer in pretrained_model.layers:\n        layer.trainable= False\n\n    #model.summary()\n    return model","89dcd72e":"# add dataset of resnet \n#https:\/\/www.kaggle.com\/dimitreoliveira\/aptos-blindness-detection-eda-and-keras-resnet50\/notebook\ndef create_resnet50(input_shape, n_out):\n    input_tensor = Input(shape=input_shape)\n    base_model = ResNet50(weights='imagenet', \n                                       include_top=False,\n                                       input_tensor=input_tensor)\n    #base_model.load_weights('..\/input\/resnet50\/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5')\n\n    x = GlobalAveragePooling2D()(base_model.output)\n    x = Dropout(0.5)(x)\n    x = Dense(2048, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    final_output = Dense(n_out, activation='softmax', name='final_output')(x)\n    model = Model(input_tensor, final_output)\n\n    for layer in model.layers:\n        layer.trainable = False\n\n    for i in range(-5, 0):\n        model.layers[i].trainable = True\n\n    #optimizer = optimizers.Adam(lr=WARMUP_LEARNING_RATE)\n    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",  metrics=[\"accuracy\"])\n\n    return model","067deadf":"#https:\/\/www.kaggle.com\/sevvalbicer\/dog-classification-vgg-16-vgg-19\/notebook\nfrom keras.applications.vgg16 import VGG16, preprocess_input\ndef create_vgg16(input_shape, n_out):\n    \n    IMG_SIZE=(224,224)\n    #vgg16_weight_path = '..\/input\/vgg16\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n    base_model = VGG16(\n        weights=\"imagenet\",\n        include_top=False, \n        input_shape=input_shape\n    )\n\n    model = Sequential()\n    model.add(base_model)\n    model.add(Flatten())\n    model.add(Dropout(0.5))\n    model.add(Dense(n_out, activation='sigmoid'))\n\n    model.layers[0].trainable = False\n\n    model.compile(\n        loss='binary_crossentropy',\n        optimizer=keras.optimizers.RMSprop(learning_rate=1e-4),\n        metrics=['accuracy']\n    )\n    return model","b5121882":"#tf.keras.applications.MobileNetV2\ndef create_mobilenet(input_shape, n_out):\n\n    base_model = MobileNet(input_shape = input_shape, include_top = False,\n                                               weights = \"imagenet\")\n    base_model.trainable = False\n    model = Sequential([base_model,\n                                 GlobalAveragePooling2D(),\n                                 Dense(20, activation = \"relu\"),\n                                 Dropout(0.4),\n                                 Dense(10, activation = \"relu\"),\n                                 Dropout(0.3),\n                                 Dense(n_out, activation = \"sigmoid\")                                     \n                                ])\n    model.compile(optimizer = keras.optimizers.Adam(lr = 1e-2), loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n    return model","01104274":"# train the rest of models:\nmodels_dict = {\"resnet50\": create_resnet50(input_shape=(224, 224,3), n_out=1),\n               \"vgg16\": create_vgg16(input_shape=(224, 224,3), n_out=1),\n               #\"mobilenet\": create_mobilenet(input_shape=(224, 224,3), n_out=1), # kernel dies with this addittion\n              }","35daea73":"models_history = {}\nmodels_metrics = {}\n\nearly_stopping = callbacks.EarlyStopping(\n    min_delta=0.001,  # minimium amount of change to count as an improvement\n    patience=3,  # how many epochs to wait before stopping\n    verbose=1\n)\n\nfor model_name, model in models_dict.items():\n    print(\"\\n\\n  Training model {}\".format(model_name))\n\n    # automatic data\n    history = model.fit(x=x_train_c, y=y_train,\n                        validation_data=(x_test_c, y_test,),\n                        epochs=15,  \n                        callbacks=[early_stopping])\n    \n    models_history[model_name] = history\n    models_metrics[model_name] = model.evaluate(x_test_c, np.array(y_test))\n    \n    print(\"\\nPredicting...\")\n    prediction= (model.predict(x_test_c, verbose=2) > 0.5)\n    print(plot_confusion_matrix(confusion_matrix(y_test, prediction), figsize=(5,5)))\n    print(accuracy_score(y_test, prediction))\n    print(classification_report(y_test, prediction))\n    plt.show()\n    \n    print(\"\\nDone\\n\")","b44b51b9":"for key in models_history.keys():\n    print(key)\n    print(models_metrics[key])\n    \n    metric_name = \"accuracy\"\n    plt.figure(figsize=(18, 7))\n    plt.subplot(2, 2, 1)\n    plt.plot(models_history[key].history[metric_name], label='Training {}'.format(metric_name))\n    plt.plot(models_history[key].history['val_'+metric_name], label='Validation {}'.format(metric_name))\n    plt.xlabel('epoch')\n    plt.ylabel(metric_name)\n    plt.legend(loc='lower right')\n    plt.title('{} : Training vs. Validation'.format(metric_name))\n\n    plt.subplot(2, 2, 2)\n    plt.plot(models_history[key].history['loss'], label='Training Loss')\n    plt.plot(models_history[key].history['val_loss'], label='Validation Loss')\n    plt.title('Loss : Training vs. Validation ')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(loc='upper left')\n    plt.show()","08931051":"This is the second notebook for the modeling. Here it goes vgg models.\nThe fist part is here"}}