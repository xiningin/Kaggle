{"cell_type":{"28d0e7ab":"code","56a249c8":"code","2325e9be":"code","806d644b":"code","74b97ed6":"code","a689e5ae":"code","4325cd6b":"code","45b7ad66":"code","ed917ca2":"code","9c797529":"code","4b6ab84d":"code","561781f9":"code","50f63078":"code","e83fd50f":"code","6bcc1e99":"code","06dfb344":"code","b9433ac6":"code","cfbe1fcf":"code","074854f5":"code","4c1ce773":"code","5cae346b":"code","08ab647f":"code","635ca742":"markdown","200b940b":"markdown","3a591779":"markdown","3dd4cb98":"markdown","516df1a9":"markdown","bb81419e":"markdown","9f61a4dc":"markdown","51d7e8e8":"markdown","0e9205e7":"markdown","e870ba88":"markdown","ef920695":"markdown","0d451943":"markdown","eab6754f":"markdown","9242ad9b":"markdown","d074c4ee":"markdown"},"source":{"28d0e7ab":"%%capture\nHAVE_GPU = True # change according to environment\nif HAVE_GPU:\n    !pip install --user tensorflow-gpu==1.14 -q\nelse:\n    !pip install --user tensorflow==1.14 -q\n# never mind the `ERROR: tensorflow 2.1...` message below","56a249c8":"# make sure we the required packages\n!pip install --user Cython -q\n!pip install --user contextlib2 -q\n!pip install --user pillow -q\n!pip install --user lxml -q\n!pip install --user matplotlib -q","2325e9be":"!wget -O protobuf.zip https:\/\/github.com\/google\/protobuf\/releases\/download\/v3.0.0\/protoc-3.0.0-linux-x86_64.zip -q\n!unzip -o protobuf.zip\n!rm protobuf.zip","806d644b":"%cd \/kaggle\n!rm -fr models\n!git clone https:\/\/github.com\/tensorflow\/models.git\n!rm -fr models\/.git","74b97ed6":"# compile ProtoBuffers\n%cd models\/research\n!\/kaggle\/working\/bin\/protoc object_detection\/protos\/*.proto --python_out=.","a689e5ae":"import os\n\nos.environ['AUTOGRAPH_VERBOSITY'] = '0'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nos.environ['PYTHONPATH']=os.environ['PYTHONPATH']+':\/kaggle\/models\/research\/slim:\/kaggle\/models\/research'\nos.environ['PYTHONPATH']","4325cd6b":"!pwd\n!python object_detection\/builders\/model_builder_test.py","45b7ad66":"def disable_coco(file):\n    with open(file,'r') as f:\n        file_str = f.read()\n    file_str=file_str.replace('from object_detection.metrics import coco_evaluation',\n                    '#from object_detection.metrics import coco_evaluation')\n    file_str=file_str.replace('object_detection.metrics import coco_tools',\n                    '#object_detection.metrics import coco_tools')\n    file_str=file_str.replace('\\'coco_detection_metrics\\':', '#\\'coco_detection_metrics\\':')\n    file_str=file_str.replace('coco_evaluation.CocoDetectionEvaluator,', '#coco_evaluation.CocoDetectionEvaluator,')\n    file_str=file_str.replace('\\'coco_mask_metrics\\':','#\\'coco_mask_metrics\\':')\n    file_str=file_str.replace('coco_evaluation.CocoMaskEvaluator,','#coco_evaluation.CocoMaskEvaluator,')\n    with open(file,'w') as f:\n        f.write(file_str)\n\ndisable_coco('.\/object_detection\/eval_util.py')","ed917ca2":"!ls \/kaggle\/input\/short-videos\/insects","9c797529":"%%capture\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom io import BytesIO\nfrom PIL import Image, ImageFont, ImageDraw\n%matplotlib inline","4b6ab84d":"# this function creates a TFRecord of one frame\ndef create_tf_example(img, fcount):  \n    height = img.shape[0] # Image height\n    width = img.shape[1] # Image width\n    im_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) \n    encoded_image_data = cv2.imencode('.jpg', frame)[1].tostring()\n    image_format = b'jpeg'\n\n    tf_record = tf.train.Example(features=tf.train.Features(feature={\n        'image\/height': tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n        'image\/width': tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n        'image\/encoded': tf.train.Feature(bytes_list=tf.train.BytesList(value=[encoded_image_data])),\n        'image\/format': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_format])),\n        'video\/frame': tf.train.Feature(int64_list=tf.train.Int64List(value=[fcount]))\n    }))\n    return tf_record","561781f9":"%%time\nvid='\/kaggle\/input\/short-videos\/insects\/butterflies_960p.mp4'\ntrec='\/kaggle\/working\/butterflies_960p.tfrecord'\ncap = cv2.VideoCapture(vid)\nvideo_length, fcount = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)), 0\nwith tf.io.TFRecordWriter(trec) as writer:\n    for i in range(video_length):\n        ret , frame = cap.read()\n        if ret:\n            tf_record = create_tf_example(frame, fcount)\n            writer.write(tf_record.SerializeToString())\n            fcount+=1\ncap.release()","50f63078":"!ls -al \/kaggle\/working\/*.tfrecord","e83fd50f":"!ls \/kaggle\/input\/tensorflow-object-detection-on-custom-data\/trained","6bcc1e99":"%%capture cap_out --no-stderr\n!python object_detection\/inference\/infer_detections.py \\\n  --input_tfrecord_paths=\/kaggle\/working\/butterflies_960p.tfrecord \\\n  --output_tfrecord_path=\/kaggle\/working\/detections.tfrecord \\\n  --inference_graph=\/kaggle\/input\/tensorflow-object-detection-on-custom-data\/trained\/frozen_inference_graph.pb \\\n  --discard_image_pixels","06dfb344":"!ls -al \/kaggle\/working\/*.tfrecord","b9433ac6":"tf.enable_eager_execution() # only for TF1.x\nlabels = pd.read_pickle('\/kaggle\/input\/starter-arthropod-taxonomy-orders-data-exploring\/ArTaxOr_labels.pkl')\ndetections = tf.data.TFRecordDataset('\/kaggle\/working\/detections.tfrecord')\npdf = pd.DataFrame(columns=['score', 'label_idx', 'left', 'top', 'right', 'bottom', 'frame'])\nfor record in detections:\n    det = tf.train.Example.FromString(record.numpy())\n    height = det.features.feature['image\/height'].int64_list.value[0]\n    width = det.features.feature['image\/width'].int64_list.value[0]\n    score = det.features.feature['image\/detection\/score'].float_list.value\n    score = [x for x in score if x >= 0.60]\n    for i in range(len(score)):\n        pdf=pdf.append({'score': score[i],\n                        'label_idx': det.features.feature['image\/detection\/label'].int64_list.value[i],\n                        'left': det.features.feature['image\/detection\/bbox\/xmin'].float_list.value[i],\n                        'top': det.features.feature['image\/detection\/bbox\/ymin'].float_list.value[i],\n                        'right': det.features.feature['image\/detection\/bbox\/xmax'].float_list.value[i],\n                        'bottom': det.features.feature['image\/detection\/bbox\/ymax'].float_list.value[i],\n                        'frame': det.features.feature['video\/frame'].int64_list.value[0]}, ignore_index=True)","cfbe1fcf":"pdf.head()","074854f5":"fontname = '\/usr\/share\/fonts\/truetype\/dejavu\/DejaVuSans.ttf'\nfont = ImageFont.truetype(fontname, 15) if os.path.isfile(fontname) else ImageFont.load_default()\n\ndef frame_idx(img, fcnt):\n    draw = ImageDraw.Draw(img)\n    xres, yres = img.size[0], img.size[1]\n    txt='Frame:{0:04d}'.format(fcnt)\n    draw.text((5, yres-25), txt, font=font, fill='white')\n    \ndef bbox(img, xmin, ymin, xmax, ymax, color, label, score):\n    draw = ImageDraw.Draw(img)\n    xres, yres = img.size[0], img.size[1]\n    box = np.multiply([xmin, ymin, xmax, ymax], [xres, yres, xres, yres]).astype(int).tolist()\n    txt = \" {}: {}%\" if score >= 0. else \" {}\"\n    txt = txt.format(label, round(score, 1))\n    ts = draw.textsize(txt, font=font)\n    draw.rectangle(box, outline=color, width=3)\n    if len(label) > 0:\n        if box[1] >= ts[1]+3:\n            xsmin, ysmin = box[0], box[1]-ts[1]-3\n            xsmax, ysmax = box[0]+ts[0]+2, box[1]\n        else:\n            xsmin, ysmin = box[0], box[3]\n            xsmax, ysmax = box[0]+ts[0]+2, box[3]+ts[1]+1\n        draw.rectangle([xsmin, ysmin, xsmax, ysmax], fill=color)\n        draw.text((xsmin, ysmin), txt, font=font, fill='white')","4c1ce773":"%%time\nvout = cv2.VideoWriter('\/kaggle\/working\/butterflies.mp4', 0x7634706d, 30, (width,height))\ndataset = tf.data.TFRecordDataset(trec)\nfor img_example in dataset:\n    img_parsed = tf.train.Example.FromString(img_example.numpy())\n    fcnt=img_parsed.features.feature['video\/frame'].int64_list.value[0]\n    img_encoded=img_parsed.features.feature['image\/encoded'].bytes_list.value[0]\n    img = Image.open(BytesIO(img_encoded))\n    fdet = pdf[pdf.frame == fcnt]\n    for i in range(len(fdet)):\n        bbox(img, fdet.left.iloc[i], fdet.top.iloc[i], fdet.right.iloc[i], fdet.bottom.iloc[i], \n             labels.color.iloc[int(fdet.label_idx.iloc[i])-1], \n             labels.name.iloc[int(fdet.label_idx.iloc[i])-1], \n             int(fdet.score.iloc[i]*100.))\n    frame_idx(img, fcnt) # add frame counter\n    buff=np.frombuffer(img.tobytes(), dtype=np.uint8)\n    buff=buff.reshape(height, width, 3)\n    buff = cv2.cvtColor(buff, cv2.COLOR_BGR2RGB)\n    vout.write(buff)\nvout.release()","5cae346b":"%cd \/kaggle\/working\n!rm -f *.proto\n!ls -l *.mp4","08ab647f":"from IPython.display import YouTubeVideo\nYouTubeVideo('ZjDRcgXQsR0', width=830, height=467)","635ca742":"We will use OpenCV for video processing. Note that OpenCV uses BGR representation, while Pillow uses RGB representation, so there are a few conversions back and forth between these two representations.","200b940b":"Then compile the protocol buffer messages needed by the API.","3a591779":"# Object Detection Video\nIn this notebook we will run object detection on a video, frame by frame, using a model trained in another notebook. Then we will render a new video with the detections.  \n\nThis notebook builds on the work from the following sources:  \n* [The ArTaxOr dataset](https:\/\/www.kaggle.com\/mistag\/arthropod-taxonomy-orders-object-detection-dataset) - the hardest part, creating a dataset\n* [ArTaxOr starter kernel](https:\/\/www.kaggle.com\/mistag\/starter-arthropod-taxonomy-orders-data-exploring)\n* [Create TFRecords of the dataset](https:\/\/www.kaggle.com\/mistag\/tensorflow-tfrecords-demystified)\n* [Train a object detection model](https:\/\/www.kaggle.com\/mistag\/tensorflow-object-detection-on-custom-data)","3dd4cb98":"We need to install the protoc compiler.","516df1a9":"## Convert input video to TFRecord\nThe easiest way to make predictions (or detections) with the trained model is to use the API supplied script `infer_detections.py`, which expects images in a TFRecord file. We will run detections on a butterfly video from the [Short Videos dataset](https:\/\/www.kaggle.com\/mistag\/short-videos).","bb81419e":"That's it! We can now test our setup by running model_builder_test.py.","9f61a4dc":"Let's check we have our TFRecord file:","51d7e8e8":"## Run detections\nWe are all set to run detections. The detections will be output to a separate TFRecord file. We will use the model we trained in [this kernel](https:\/\/www.kaggle.com\/mistag\/tensorflow-object-detection-on-custom-data):","0e9205e7":"## Render video\nFinally we can render a new video with detections.","e870ba88":"Time to fetch the Object Detection API.","ef920695":"## Convert detections into DataFrame\nTo make it easier to process the detections, we convert `detections.tfrecord` into a Pandas frame.","0d451943":"## Install TensorFlow & Object Detection API\nThe TensorFlow Object Detection API is still TF1.x only, se we need to install TF1.14 plus the API and related tools.","eab6754f":"I have not figured out how to display the video directly from the notebook, so I had to put it on YouTube first. And here is the result:","9242ad9b":"## Summary\nIn this notebook we have seen how to run object detection on a video, and then render a new video with detections. The trained model we used needs some improvement it looks. Although objects are detected pretty well, the object class is wrong quite often.","d074c4ee":"Now, we did not install the Coco API, since we will be using the Pascal VOC evaluation metric. Unfortunately, there are some hardcoded references to the Coco API that needs to be commented out. Alternatively, just install the Coco API."}}