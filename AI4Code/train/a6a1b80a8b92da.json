{"cell_type":{"34edd601":"code","9538c4b2":"code","fcce73d9":"code","9f289b1c":"code","a76f6c6b":"code","8ae17822":"code","e5fa026f":"code","4ee2d59f":"markdown"},"source":{"34edd601":"# Author: Dattaraj Rao (dattarajrao@yahoo.com)\n# Demonstration of generating new hollywood movie names by learning from move names datset\n# More examples covered in book - Keras to Kubernetes: The Journey of a Machine Learning Model to Production\n# https:\/\/www.amazon.com\/Keras-Kubernetes-Journey-Learning-Production\/dp\/1119564832\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","9538c4b2":"# load the movie names and only isolate movie names\nmovies = pd.read_csv('..\/input\/imdb-5000-movie-dataset\/movie_metadata.csv')\nprint(movies.columns)\nmovie_titles = movies['movie_title']\nmovie_titles.head()","fcce73d9":"# tokenize the corpus at word-level and build encodings\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# prepare tokenizer\ntok = Tokenizer()\ntok.fit_on_texts(movie_titles)\nvocab_size = len(tok.word_index) + 1\nprint(\"Vocabulary size = \", vocab_size)\nprint(\"-------------------------------\")\n# integer encode the documents\nencoded_docs = tok.texts_to_sequences(movie_titles)\nprint(\"Encoded = \", encoded_docs[:10])\nprint(\"-------------------------------\")\n# pad documents to a max length of 10 words for movie title\nmax_length = 8\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='pre')\nprint(\"Padded = \", padded_docs[:10])\nprint(\"-------------------------------\")","9f289b1c":"# predict the next word as the result for training\nfrom numpy import array\nfrom keras.utils import to_categorical\n\n# input is sequence and output next word\nsequences = array(padded_docs)\nprint(sequences.shape)\nX, y = sequences[:,:-1], sequences[:,-1]\ny = to_categorical(y, num_classes=vocab_size)\nseq_length = X.shape[1]","a76f6c6b":"# build recurrent network for training\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Embedding\nfrom keras.utils.vis_utils import plot_model\n\n# define model\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 100, input_length=seq_length))\nmodel.add(LSTM(100, return_sequences=True))\nmodel.add(LSTM(100))\n#model.add(Dense(100, activation='relu'))\nmodel.add(Dense(vocab_size, activation='softmax'))\nprint(model.summary())\n# compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\n\nSVG(model_to_dot(model).create(prog='dot', format='svg'))","8ae17822":"# fit the model and show learning curves\nhistory = model.fit(X, y, batch_size=128, epochs=100, validation_split=0.25)\n\nimport matplotlib.pyplot as plt\n\n# Plot training & validation accuracy values\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","e5fa026f":"# generate new data\nfrom random import randint\n\n# generate a sequence from a language model\ndef generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n    result = list()\n    in_text = seed_text\n    # generate a fixed number of words\n    for _ in range(n_words):\n        # encode the text as integer\n        encoded = tokenizer.texts_to_sequences([in_text])[0]\n        # truncate sequences to a fixed length\n        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n        # predict probabilities for each word\n        yhat = model.predict_classes(encoded, verbose=0)\n        # map predicted word index to word\n        out_word = ''\n        for word, index in tokenizer.word_index.items():\n            if index == yhat:\n                out_word = word\n                break\n        # append to input\n        in_text += ' ' + out_word\n        result.append(out_word)\n    return ' '.join(result)\n\n# select a seed text\nfor count in range(100):\n    seed_text = movie_titles[randint(0,len(movie_titles))]\n    print(\"Seed text = \", seed_text)\n\n    # generate new text\n    generated = generate_seq(model, tok, seq_length, seed_text, 5)\n    print(\"Generated text = \", generated)","4ee2d59f":"# Generative Networks for predicting Hollywood movie names\n#### By Dattaraj J Rao ([dattarajrao@yahoo.com](mailto:dattarajrao@yahoo.com))\n![](http:\/\/)### Author of \"Keras to Kubernetes: The Journey of a Machine Learning Model to Production\"\n![image.png](attachment:image.png)\n"}}