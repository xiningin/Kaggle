{"cell_type":{"8f990282":"code","72c68094":"code","15e3657f":"code","2c29581c":"code","58e6a55b":"code","c559f230":"code","674653fd":"code","0766dea0":"code","c72c2786":"code","6d14bb90":"code","8ea55620":"code","269be73e":"code","fd544a03":"code","efb12713":"code","475e27a5":"code","f59efd83":"code","67fabf85":"code","f148b006":"code","10a60821":"code","bcbca4fb":"markdown","b74f9b25":"markdown","06e145f9":"markdown","54016e12":"markdown","6c6ab5b2":"markdown","7231070b":"markdown","bf299061":"markdown","54c0a3fc":"markdown","365243c4":"markdown","f0f930d5":"markdown","e2d35986":"markdown","85552270":"markdown","555f65f3":"markdown","77fd3421":"markdown","4779e19d":"markdown","23a54b4a":"markdown"},"source":{"8f990282":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\n#train_data.head(10)","72c68094":"#Looking at the numeric variables\ntrain_data.describe()","15e3657f":"train_data[\"Name\"] = train_data[\"Name\"].str.split(',').str[1]\ntrain_data[\"Name\"] = train_data[\"Name\"].str.split('.').str[0]\ntrain_data[\"Name\"] = train_data[\"Name\"].str.strip()","2c29581c":"x = train_data.groupby('Name').agg(['count']).index.get_level_values('Name')\nx","58e6a55b":"train_data[\"Age\"] = train_data.groupby(\"Name\").transform(lambda x: x.fillna(x.mean()))['Age']\n#changing sex to be 0 or 1 for female & male\ntrain_data['Sex'].replace({'female':0,'male':1},inplace=True)\ntrain_data.head()","c559f230":"train_data_log = train_data.iloc[:,[False,False,True, False,True,True,True,True,False,True,False,False]]\nnormalized_data_train=(train_data_log-train_data_log.min())\/(train_data_log.max()-train_data_log.min())\ntrain_labels_log = train_data.iloc[:,1]\nnormalized_data_train.head()","674653fd":"def initial_coefs_intercept(data):\n    \"\"\"Function takes a pandas df as input and returns initialized coefficients and intercept\"\"\"\n    coefficients = []\n    intercept = 0\n    for i in range(len(data.columns)):\n        coefficients.append(0)\n    return [coefficients, intercept]\n\ninitial_coefs = initial_coefs_intercept(normalized_data_train)[0]\ninitial_intercept = initial_coefs_intercept(normalized_data_train)[1]\nprint(initial_coefs)\nprint(initial_intercept)","0766dea0":"#log_odds\ndef log_odds(data,coefficients,intercept):\n    \"\"\"Takes pandas dataframe, list of coefficients and an intercept value and returns\n    an array of the log odds of each feature\"\"\"\n    return np.dot(data,coefficients) + intercept\n\nl_o= log_odds(normalized_data_train,initial_coefs, initial_intercept)\n","c72c2786":"def sigmoid(log_odds_vars):\n    \"\"\"Takes log odds calculated with the log odds functions and returns the sigmoid transformed values\n    restricting the values from 0 to 1\"\"\"\n    sigmoid_values = 1\/(1+np.exp(-log_odds_vars))\n    \n    return sigmoid_values\n\nsigmoid_vals = sigmoid(l_o)\n","6d14bb90":"def log_loss(probabilities, labels):\n    \"\"\"Determines the log loss given a set of sigmoid values (probabilities) and a set of training data labels\"\"\"\n    #start_time = time.time()\n    data_length = len(labels)\n    labels = np.array(labels)\n    \n    left_half = np.dot(labels,np.log(probabilities+.0001)) #including small epsilon so no division by 0\n    right_half = np.dot(1-labels,np.log(1-probabilities+.0001))\n    loss = (-1\/data_length) * (left_half + right_half)\n\n    #print(\"--- %s seconds ---\" % (time.time() - start_time)) \n    return loss\n    \n#print(log_loss(sigmoid_vals,train_labels_log))","8ea55620":"def find_coefficients(data, coefficients, intercept,labels,learning_rate, iterations):\n    coefs = coefficients\n    for i in range(iterations):\n        l_odds = log_odds(data,coefs,intercept)\n        sig_vals = sigmoid(l_odds)\n        data_transpose = np.transpose(learning_rate * data)\n        coefs = np.dot(data_transpose,(labels-sig_vals) * sig_vals*(1-sig_vals)) + coefs\n        intercept = intercept + learning_rate * np.dot((labels-sig_vals), (sig_vals*(1-sig_vals)))\n    print(coefs, intercept)\n    return coefs, intercept\nbest_coefs= find_coefficients(normalized_data_train,initial_coefs, initial_intercept,train_labels_log,.0005, 50000)","269be73e":"best_coef = best_coefs[0]\nbest_int = best_coefs[1]\n\nv = sigmoid(log_odds(normalized_data_train,best_coef,best_int))\n\n#print(v)","fd544a03":"def find_threshold(sigmoid_vals):\n    \"\"\"Takes sigmoid vals from best coefficients and best intercept and returns the best classifier threshold\"\"\"\n    predictions = []\n    vals = []\n    accuracies = []\n    \n    for num in range(1000):\n        vals.append(num\/1000)\n        accuracy = 0\n        for i in v:\n            if i > num\/1000:\n                predictions.append(1)\n            else:\n                predictions.append(0)\n        \n        for j in range(len(predictions)):\n            if predictions[j] == train_labels_log[j]:\n                accuracy += 1\n        accuracies.append(accuracy\/len(predictions))\n        accuracy = 0\n        predictions = []\n    indx = accuracies.index(max(accuracies))\n    print(\"Best accuracy on training set:\")\n    print(max(accuracies))\n    best_threshold = vals[indx]\n    return best_threshold\n    \nbest_thresh = find_threshold(v)\nprint(best_thresh)","efb12713":"def calculate_precision(sigmoid_vals, threshold, labels):\n    \"Precision is  True Positives \/ (True Positives + False Positives)\"\n    predictions = []\n    true_positives = 0\n    false_positives = 0\n    for i in sigmoid_vals:\n        if i > threshold:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    \n    \n    for i in range(len(labels)):\n        if labels[i] == 1 and labels[i] == predictions[i]:\n            true_positives += 1\n        elif labels[i] == 0 and labels[i] != predictions[i]:\n            false_positives += 1\n    \n    return true_positives\/(true_positives + false_positives)\n    \nprint(\"Precision:\")\nprint(calculate_precision(v, best_thresh, train_labels_log))\n\n    ","475e27a5":"def calculate_recall(sigmoid_vals, threshold, labels):\n    \"Precision is  True Positives \/ (True Positives + False Negatives)\"\n    predictions = []\n    true_positives = 0\n    false_negatives = 0\n    for i in sigmoid_vals:\n        if i > threshold:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    \n    \n    for i in range(len(labels)):\n        if labels[i] == 1 and labels[i] == predictions[i]:\n            true_positives += 1\n        elif labels[i] == 1 and labels[i] != predictions[i]:\n            false_negatives += 1\n    \n    return true_positives\/(true_positives + false_negatives)\n    \n    \n    \nprint(\"Recall\")          \nprint(calculate_recall(v, best_thresh, train_labels_log))\n    ","f59efd83":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\n\ntest_data[\"Name\"] = test_data[\"Name\"].str.split(',').str[1]\ntest_data[\"Name\"] = test_data[\"Name\"].str.split('.').str[0]\ntest_data[\"Name\"] = test_data[\"Name\"].str.strip()\ntest_data['Sex'].replace({'female':0,'male':1},inplace=True)\n\n\nx = test_data.groupby('Name').agg(['count']).index.get_level_values('Name')\ntest_data[\"Age\"] = test_data.groupby(\"Name\").transform(lambda x: x.fillna(x.mean()))['Age']\n\n\ntest_data_log = test_data.iloc[:,[False,True,False,True,True,True,True,False,True,False,False]]\nnormalized_data_test=(test_data_log-test_data_log.min())\/(test_data_log.max()-test_data_log.min())\n","67fabf85":"pred_test = sigmoid(log_odds(normalized_data_test,best_coef,best_int))","f148b006":"#looping through sigmoid values and comparing to threshold value. If greater than threshold, predict class 1.\n#otherwise predict class 0\nclassifier = []\nfor i in range(len(pred_test)):\n    if pred_test[i] > best_thresh:\n        classifier.append(1)\n    else:\n        classifier.append(0)","10a60821":"data = {'PassengerId': test_data[\"PassengerId\"].values, 'Survived':classifier} \ndf_submission = pd.DataFrame(data)\n\ndf_submission.to_csv(\"submission_log_regression2.csv\",index=False)\n\n#Accuracy was 0.758 on testing set","bcbca4fb":"# 4.Log-Odds","b74f9b25":"# 5.Sigmoid","06e145f9":"# 8.Threshold","54016e12":"With the best coefficients and intercept, insert these into the sigmoid function to get the sigmoid values of the optimized coefficients.","6c6ab5b2":"# 3.Initializing","7231070b":"# 9.Performance","bf299061":"The names are transformed into title. Taking the average age of each group to fill missing data.","54c0a3fc":"The age variable has missing data but all of the other numeric columns are fine. An approach is to group data by another variable and find the average age for each group and impute the average into the missing values. I will take group by the title of each person since people with similar title may have similar ages.\n","365243c4":"# 7.Gradient_Descent\n> ### Using the functions created earlier and the gradients of the coefficients and the intercept I used a learning rate of .005 and number of iterations equal to 10,000. I tried several and this had best performance.","f0f930d5":"For logistic regression, we don't need to standardize the data. The major focus of this notebook is on the algorithm, so I will be using what I think are the most potentially useful variables (6 of the features)","e2d35986":"# 2.Preprocessing","85552270":"Accuracy on the testing set was .758. This could probably be improved by trying different learning rates and number of epochs.","555f65f3":"# 1.Introduction\nThis notebook will be applying logistic regression on the Titanic dataset. I did not include data exploration visuals in this notebook but did some minor data cleaning prior to applying the models. As an overview of the algorithm, I initialized coefficients and an intercept to zero and used the log-odds function to find the logarithm of the odds then applied a sigmoid function to restrict the values from 0 to 1. I then used gradient descent to find the optimal coefficients and intercept. These coefficients and intercept are then put into the sigmoid function with these sigmoid values determining the best threshold for classification. I included accuracy, recall and precision functions. Submission results are also included","77fd3421":"# 6.Log-Loss\nLog-Loss is the loss function. Gradient descent uses the derivative of the loss function for optimization","4779e19d":"# 10.Submission","23a54b4a":"# Logistic Regression: A Vectorized Approach Without Machine Learning Libraries\n\n[1. Introduction](#1.Introduction)\n\n[2. Preprocessing](#2.Preprocessing)\n\n[3. Initialize Coefficients](#3.Initializing)\n\n[4. Log-Odds Function](#4.Log-Odds)\n\n[5. Sigmoid Function](#5.Sigmoid)\n\n[6. Log-Loss Function](#6.Log-Loss)\n\n[7. Gradient Descent](#7.Gradient_Descent)\n\n[8. Find Best Threshold](#8.Threshold)\n\n[9. Performance Metrics](#9.Performance)\n\n[10. Submission](#10.Submission)\n\n\n\n"}}