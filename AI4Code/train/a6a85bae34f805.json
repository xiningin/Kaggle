{"cell_type":{"0c50fb21":"code","840e7f47":"code","76865001":"code","2bd0a8a0":"code","87b20440":"code","7e9cfa6a":"markdown","30af53f7":"markdown"},"source":{"0c50fb21":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","840e7f47":"import numpy as np\nimport torch\nfrom torch import nn\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# real price come from \n# Spot Prices for Crude Oil and Petroleum Product. (May.28.2020). U.S. Energy Information Administration. Retrieved from https:\/\/www.eia.gov\/dnav\/pet\/pet_pri_spt_s1_d.htm\ndf1 = pd.read_csv(r'..\/input\/real-price\/RealPrice.csv')\nprint(df1)","76865001":"# real price come from \ndf1 = pd.read_csv(r'..\/input\/real-price\/RealPrice.csv', usecols=[1])\n\ntable_val = df1.values\n#print(table_val)\nseq_number = table_val[0:165, :]\nprint(seq_number[0])\nprint(seq_number.shape)\n\nseq_number = seq_number.flatten()\nseq_number = seq_number[:, np.newaxis]\nprint(seq_number.shape)\n\n# # print(repr(seq))\n# # 1949~1960, 12 years, 12*12==144 month\nseq_week = np.arange(33)\nseq_day = np.arange(5)\nseq_week_day = np.transpose(\n    [np.repeat(seq_week, len(seq_day)),\n     np.tile(seq_day, len(seq_week))],\n)  # Cartesian Product\n\nseq = np.concatenate((seq_number, seq_week_day), axis=1)\nprint(seq.shape)\nprint(seq)\n\n# normalization\nprint(\"seq.mean(axis=0)\")\ntrain_mean = seq[:125].mean(axis=0)\ntrain_std = seq[:125].std(axis=0)\nprint(train_mean)\nprint(train_std)\nseq = (seq - train_mean) \/ train_std","2bd0a8a0":"class RegLSTM(nn.Module):\n    def __init__(self, inp_dim, out_dim, mid_dim, mid_layers):\n        super(RegLSTM, self).__init__()\n\n        self.rnn = nn.LSTM(inp_dim, mid_dim, mid_layers)  # rnn\n        self.reg = nn.Sequential(\n            nn.Linear(mid_dim, mid_dim),\n            nn.Tanh(),\n            nn.Linear(mid_dim, out_dim),\n        )  # regression\n\n    def forward(self, x):\n        y = self.rnn(x)[0]  # y, (h, c) = self.rnn(x)\n\n        seq_len, batch_size, hid_dim = y.shape\n        y = y.view(-1, hid_dim)\n        y = self.reg(y)\n        y = y.view(seq_len, batch_size, -1)\n        return y\n\n    \"\"\"\n    PyCharm Crtl+click nn.LSTM() jump to code of PyTorch:\n    Examples::\n        >>> rnn = nn.LSTM(10, 20, 2)\n        >>> input = torch.randn(5, 3, 10)\n        >>> h0 = torch.randn(2, 3, 20)\n        >>> c0 = torch.randn(2, 3, 20)\n        >>> output, (hn, cn) = rnn(input, (h0, c0))\n    \"\"\"\n\n    def output_y_hc(self, x, hc):\n        y, hc = self.rnn(x, hc)  # y, (h, c) = self.rnn(x)\n\n        seq_len, batch_size, hid_dim = y.size()\n        y = y.view(-1, hid_dim)\n        y = self.reg(y)\n        y = y.view(seq_len, batch_size, -1)\n        return y, hc","87b20440":"inp_dim = 3\nout_dim = 1\nmid_dim = 8\nmid_layers = 1\nbatch_size = 12 * 4\nmod_dir = '.'\n\n'''load data'''\n# data, train_mean, train_std = load_data()\ndata = seq\n\ndata_x = data[:-1, :]\ndata_y = data[+1:, 0]\nassert data_x.shape[1] == inp_dim\n\ntrain_size = int(len(data_x) * 0.78)\nprint(train_size)\n\ntrain_x = data_x[:train_size]\ntrain_y = data_y[:train_size]\ntrain_x = train_x.reshape((train_size, inp_dim))\ntrain_y = train_y.reshape((train_size, out_dim))\n\n'''build model'''\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnet = RegLSTM(inp_dim, out_dim, mid_dim, mid_layers).to(device)\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=1e-2)\n\n'''train'''\nvar_x = torch.tensor(train_x, dtype=torch.float32, device=device)\nvar_y = torch.tensor(train_y, dtype=torch.float32, device=device)\n\nbatch_var_x = list()\nbatch_var_y = list()\n\nfor i in range(batch_size):\n    j = train_size - i\n    batch_var_x.append(var_x[j:])\n    batch_var_y.append(var_y[j:])\n\nfrom torch.nn.utils.rnn import pad_sequence\nbatch_var_x = pad_sequence(batch_var_x)\nbatch_var_y = pad_sequence(batch_var_y)\n\nwith torch.no_grad():\n    weights = np.tanh(np.arange(len(train_y)) * (np.e \/ len(train_y)))\n    weights = torch.tensor(weights, dtype=torch.float32, device=device)\n\nprint(\"Training Start\")\nfor e in range(384):\n    out = net(batch_var_x)\n\n    # loss = criterion(out, batch_var_y)\n    loss = (out - batch_var_y) ** 2 * weights\n    loss = loss.mean()\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    if e % 64 == 0:\n        print('Epoch: {:4}, Loss: {:.5f}'.format(e, loss.item()))\n# torch.save(net.state_dict(), '{}\/net.pth'.format(mod_dir))\n# print(\"Save in:\", '{}\/net.pth'.format(mod_dir))\n\n'''eval'''\n# net.load_state_dict(torch.load('{}\/net.pth'.format(mod_dir), map_location=lambda storage, loc: storage))\nnet = net.eval()\n\ntest_x = data_x.copy()\n\n# why ? zhangkuan\n# test_x[train_size:, 0] = 0\n\ntest_x = test_x[:, np.newaxis, :]\nprint(test_x.shape)\n# print(test_x)\n\ntest_x = torch.tensor(test_x, dtype=torch.float32, device=device)\n\n'''simple way but no elegant'''\n# for i in range(train_size, len(data) - 2):\n#     test_y = net(test_x[:i])\n#     test_x[i, 0, 0] = test_y[-1]\n\n'''elegant way but slightly complicated'''\neval_size = 1\nzero_ten = torch.zeros((mid_layers, eval_size, mid_dim), dtype=torch.float32, device=device)\ntest_y, hc = net.output_y_hc(test_x[:train_size], (zero_ten, zero_ten))\ntest_x[train_size + 1, 0, 0] = test_y[-1]\nfor i in range(train_size + 1, len(data) - 2):\n    test_y, hc = net.output_y_hc(test_x[i:i + 1], hc)\n    test_x[i + 1, 0, 0] = test_y[-1]\npred_y = test_x[1:, 0, 0]\npred_y = pred_y.cpu().data.numpy()\n\nprint(\"pred_y\")\n# print(pred_y)\n\ndiff_y = pred_y[train_size:] - data_y[train_size:-1]\nl1_loss = np.mean(np.abs(diff_y))\nl2_loss = np.mean(diff_y ** 2)\nprint(\"L1: {:.3f}    L2: {:.3f}\".format(l1_loss, l2_loss))\n\nplt.plot(pred_y, 'r', label='pred')\nplt.plot(data_y[0:127], 'b', label='real', alpha=0.3)\nplt.plot([train_size, train_size], [-1, 2], color='k', label='train | pred')\nplt.legend(loc='best')\n#plt.savefig('lstm_reg.png')\n\n#print(pred_y*train_std[0] + train_mean[0])\nplt.pause(0.5)","7e9cfa6a":"then get the 75day moving average price\n\n![image.png](attachment:image.png)","30af53f7":"![image.png](attachment:image.png)"}}