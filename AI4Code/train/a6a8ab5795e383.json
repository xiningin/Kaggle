{"cell_type":{"9c4f78e7":"code","433b9b1f":"code","003f4ed5":"code","60c66036":"code","b9c3dd30":"code","e8117d9b":"code","ef8861c2":"code","4ae356b0":"code","3432ab25":"code","d6b7b467":"code","5c4dcee1":"code","058d9c28":"code","3202b0a9":"code","2bc3a824":"code","9e87fab2":"code","70a155ed":"code","ffb5becf":"code","c1f50d3c":"code","99991db1":"code","ff4f5f92":"code","e95c6263":"code","2ff9bbb7":"code","b4904a96":"code","895d4b47":"code","b847b067":"code","48277add":"code","f7876513":"code","b69327c6":"markdown","98739929":"markdown","5b1cf5a7":"markdown","8c0d8799":"markdown","e8b5d20b":"markdown","0b7b29dd":"markdown","0c728bd6":"markdown","8d9a3eec":"markdown","e111ce27":"markdown","e85d934f":"markdown","c1f085c7":"markdown","6cb451b8":"markdown","b8129806":"markdown","f2f4b831":"markdown","317a9f79":"markdown","8b91ced0":"markdown","1dbf64af":"markdown","ccd83805":"markdown","4d35bb5e":"markdown","b5dc1c06":"markdown","6ee6653d":"markdown","23bced24":"markdown","ae2cf104":"markdown","2c312dd9":"markdown","758924f7":"markdown","3b2d8829":"markdown","f36b63cb":"markdown","7b1f7142":"markdown"},"source":{"9c4f78e7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","433b9b1f":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LogisticRegression\n \nfrom sklearn.model_selection import train_test_split\n \nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error","003f4ed5":"df = pd.read_csv('\/kaggle\/input\/diamonds\/diamonds.csv')\nprint(df.head())","60c66036":"print(df.columns)","b9c3dd30":"df = df.reindex( columns=['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'x', 'y', 'z', 'price'] )  \nprint(df)","e8117d9b":"sns.pairplot(df, hue='cut', plot_kws={'s':1})","ef8861c2":"#1. Im not really sure the purpose of x, y, or z but also the depth seems extraneous and the cut variable.\n#2. The price and carat seem most useful for predicting price.\n#3. When the price is lower, all atributes seem to be more random and dispersed. When the cut is ideal or premium.","4ae356b0":"print(df.describe())","3432ab25":"df['cut'].loc[df['cut'] == 'Fair'] = 1\ndf['cut'].loc[df['cut'] == 'Good'] = 2\ndf['cut'].loc[df['cut'] == 'Very Good'] = 3\ndf['cut'].loc[df['cut'] == 'Premium'] = 4\ndf['cut'].loc[df['cut'] == 'Ideal'] = 5\ndf['color'].loc[df['color'] == 'E'] = 1\ndf['color'].loc[df['color'] == 'I'] = 2\ndf['color'].loc[df['color'] == 'J'] = 3\ndf['color'].loc[df['color'] == 'H'] = 4\ndf['color'].loc[df['color'] == 'F'] = 5\ndf['color'].loc[df['color'] == 'G'] = 6\ndf['color'].loc[df['color'] == 'D'] = 7\ndf['clarity'].loc[df['clarity'] == 'I1'] = 1\ndf['clarity'].loc[df['clarity'] == 'SI2'] = 2\ndf['clarity'].loc[df['clarity'] == 'SI1'] = 3\ndf['clarity'].loc[df['clarity'] == 'VS2'] = 4\ndf['clarity'].loc[df['clarity'] == 'VS1'] = 5\ndf['clarity'].loc[df['clarity'] == 'VVS2'] = 6\ndf['clarity'].loc[df['clarity'] == 'VVS1'] = 7\ndf['clarity'].loc[df['clarity'] == 'IF'] = 8\ndf['clarity'].loc[df['clarity'] == 'FL'] = 9\ndf['cut'] = df['cut'].astype('int64')\ndf['color'] = df['color'].astype('int64')\ndf['clarity'] = df['clarity'].astype('int64')\nprint(df.dtypes)","d6b7b467":"df.loc[ (df.x == 0) | (df.y == 0) | (df.z == 0) ] = None\ndf = df.dropna()\nprint(df.reset_index(inplace=True))","5c4dcee1":"print(df.corr(method = 'pearson').sort_values('price'))","058d9c28":"#1. Clarity\n#2. Carat (i had to look up what this one is, but it's the weight if i'm not mistaken)\n#3. Color\n#4. Cut","3202b0a9":"sns.heatmap(df.corr(method = 'pearson'), annot = True)","2bc3a824":"linReg = LinearRegression()\nX = df[['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'x', 'y', 'z']]\ny = df.price\ntrain_X, test_X, train_y, test_y = train_test_split(X, y)\nlinReg.fit(train_X, train_y)\nlinReg_pred = linReg.predict( test_X )\n\nprint(mean_absolute_error(test_y, linReg_pred))","9e87fab2":"linReg_coefs = linReg.coef_\ncoef_df = pd.DataFrame( columns = ['Feature', 'Coefficient'] )\ncoef_df['Feature'] = ['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'x', 'y', 'z']\ncoef_df['Coefficient'] = linReg_coefs\nprint(coef_df)","70a155ed":"coef_df['diamond_1'] = [0.8, 3, 5, 6, 61.8, 57.5, 5.7, 5.7, 3.5]\nprint( sum( coef_df['Coefficient'] * [0.8, 3, 5, 6, 61.8, 57.5, 5.7, 5.7, 3.5] ) )","ffb5becf":"linReg16 = LinearRegression()\nX = df[['carat', 'cut', 'color', 'clarity']]\ny = df.price\ntrain_X, test_X, train_y, test_y = train_test_split(X, y)\nlinReg16.fit(train_X, train_y)\nlinReg_pred16 = linReg16.predict( test_X )\n\nprint(mean_absolute_error(test_y, linReg_pred16))","c1f50d3c":"linReg17 = LinearRegression()\nX = df[['carat']]\ny = df.price\ntrain_X, test_X, train_y, test_y = train_test_split(X, y)\nlinReg17.fit(train_X, train_y)\nlinReg_pred17 = linReg17.predict( test_X )\n\nprint(mean_absolute_error(test_y, linReg_pred17))","99991db1":"linReg18 = LinearRegression()\nX = df[['carat', 'x', 'y', 'z']]\ny = df.price\ntrain_X, test_X, train_y, test_y = train_test_split(X, y)\nlinReg18.fit(train_X, train_y)\nlinReg_pred18 = linReg18.predict( test_X )\n\nprint(mean_absolute_error(test_y, linReg_pred18))","ff4f5f92":"df2 = df.copy()\ndf2 = df2[['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'x', 'y', 'z']]\ndf2['predictions'] = linReg.predict(df2)\ndf2['price'] = df['price'].copy()\ndf2['abs_error'] = abs(df2['price']) - df2['predictions']\nprint(df2)","e95c6263":"#Certain carats yield wild results, even negative numbers so I feel the Linear Regression may be calculated wrong","2ff9bbb7":"df2.plot.scatter(x = 'price', y = 'predictions', s=0.01, title = 'Linear_Regression')","b4904a96":"decTree = DecisionTreeRegressor(criterion = 'mse', max_depth = 5)\nX = df[['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'x', 'y', 'z']]\ny = df.price\ntrain_X, test_X, train_y, test_y = train_test_split(X, y)\ndecTree.fit(train_X, train_y)\ndecTree_pred = decTree.predict( test_X )\n\nprint(mean_absolute_error(test_y, decTree_pred))","895d4b47":"df2 = df.copy()\ndf2 = df2[['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'x', 'y', 'z']]\ndf2['predictions'] = decTree.predict(df2)\ndf2['price'] = df['price'].copy()\ndf2['abs_error'] = abs(df2['price']) - df2['predictions']\nprint(df2)\ndf2.plot.scatter(x = 'price', y = 'predictions', s = 0.01, title = 'Deciduos Tree')","b847b067":"randForrest = RandomForestRegressor(criterion = 'mse', max_depth = 5)\nX = df[['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'x', 'y', 'z']]\ny = df.price\ntrain_X, test_X, train_y, test_y = train_test_split(X, y)\nrandForrest.fit(train_X, train_y)\nrandForrest_pred = randForrest.predict( test_X )\n\nprint(mean_absolute_error(test_y, randForrest_pred))","48277add":"df2 = df.copy()\ndf2 = df2[['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'x', 'y', 'z']]\ndf2['predictions'] = randForrest.predict(df2)\ndf2['price'] = df['price'].copy()\ndf2['abs_error'] = abs(df2['price']) - df2['predictions']\nprint(df2)\ndf2.plot.scatter(x = 'price', y = 'predictions', s = 0.01, title = 'Random Forrest')","f7876513":"linReg = LinearRegression()\nX = df[['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'x', 'y', 'z']]\ny = df.price\ntrain_X, test_X, train_y, test_y = train_test_split(X, y)\nlinReg.fit(train_X, train_y)\nlinReg_pred = linReg.predict( test_X )\ndf2 = df.copy()\ndf2 = df2[['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'x', 'y', 'z']]\ndf2['predictions'] = linReg.predict(df2)\ndf2['price'] = df['price'].copy()\ndf2['abs_error'] = abs(df2['price']) - df2['predictions']\ndf2.plot.scatter(x = 'price', y = 'predictions', s=0.01, title = 'Linear_Regression')","b69327c6":"#### (25)\nNow run the model as a random forest.","98739929":"#### (2)\nDisplay the column names only.","5b1cf5a7":"#### (9)\nDisplay the correlations of your DF, sorted in increasing order. You can either display all of them or limit the display only to the price column, which is the only correlation that will ultimately matter for our predictions.","8c0d8799":"#### (3)\n\nData clean up time.\n\nFirst, let's re-index all of the columns so that price becomes the final column. While doing so, we will remove the excess column by not placing in the list to replace.\n\nDisplay the df.","e8b5d20b":"#### (11)\nNow we will look a bit more at those correlations as a whole.  \n\nA heatmap can be used to display the different correlations in a graphical way. A correlation can be anywhere from -1 to 1. A correlation of 0 means there is almost nothing connecting one variable to another. Closer to 1 means that it is almost guaranteed that as one variable goes up, another goes up almost linearly the same. Closer to -1 means as one goes up, another goes down almost linearly.\n\nTo display a heatmap, use: sns.heatmap( your dataframe's correlation ) or sns.heatmap( your dataframe's correlations, annot=True). Try out both to see the difference.","0b7b29dd":"#### (14)\nDisplay the coefficients your linear model computed in a nice chart showing the variable name and the computed coefficient.\n\n","0c728bd6":"#### (7)\n\nYou should notice that some of the columns are missing in the statistics and were also missing in the PairGrid. This is because these are categorical data instead of numerical. Most machine learning algorithms will not be able to work with categorical data without converting the data first into numerical values. There are libraries which can convert categories to numbers automatically for you, but they will just assign a number to each category with no regards to the 'value' of one category over another if the categories could be sorted. For instance, Bristow may get a 0, Gainesville may get a 1, Haymarket may get a 2. The order here could be arbitrary or alphabetic and it would not matter.  \n\nSome categorical values could be ordered. In diamonds, there is an order to the cut, color, and clarity values. One is 'better' than another.  \n\nConvert your categorical data into numerical data.  \nFair < Good < Very Good < Premium < Ideal  \nJ < I < H < ... < D  \nI1 < SI2 < SI1 < VS2 < VS1 < VVS2 < VVS1 < IF < FL\n\nDisplay your DF to check the categories converted.","8d9a3eec":"#### (18)\nOne more time with carat, x, y, and z (the four highest correlating features compared to price).","e111ce27":"#### (23)\nNow run the model as a decision tree using all variables. Again show the mean absolute error.","e85d934f":"#### (1)\nLoad your DF. Display the first 5 rows.","c1f085c7":"\n#### (5)\n\nSome early project questions:\n\n1) Which two potential features (variables) may not be useful for predicting price?\n\n2) Which potential features look like they may be most useful for predicing price?\n\n3) Can you identify some potential incorrect data that needs to be removed? Name a few features and some ranges we may want to remove. (example: remove data with age > 100 )\n","6cb451b8":"#### (17)\nTry again, this time only with carat.","b8129806":"#### (8)\n\nLet's deal with some of the outliers you may have noticed now. These may be explainable, but we are going to assume they are data that will interfere with our predictions. You should notice a few points in the y and z features that are definitely not consistent. There are also diamonds showing x\/y\/z values as 0, which present incomplete data we cannot use. Figure out a way to remove a good portion of these pieces of data that will interfere with our predictions.\n\nRe-display all the main statistics of your DF.\n\n","f2f4b831":"#### (20)\nCreate a copy of your dataframe, then add a new column 'predictions' and fill it with the results from predicting your model on the entire original X with all of the columns. (If you re-used the same variable for the last few regression models again, make sure you reset the model back to using all of the columns).  \n\nCreate another new column called abs_error. Set this column to equal the absolute value of the price column minus the predictions column to show how far off the prediction was.\n\nDisplay your DF.","317a9f79":"#### (15)\n\nHard-code a line of code using each of your found coefficients to calculate\/display the predicted price of the following diamond:\n\nCarat: 0.8  \nCut: Very Good  \nColor: F  \nClarity: VVS2   \nDepth: 61.8  \nTable: 57.5  \nx: 5.7  \ny: 5.7  \nz: 3.5  ","8b91ced0":"#### (12)\nThese are similar questions to the earlier ones.\n\n1) Which features seem to be the best to be used to predict the price of a diamond?  \n\nCarat\n\n2) Which features may need to be thrown out during predictions?  \n\nColor, Depth,or Clarity\n\n3) There are several variables that are correlated so highly that perhaps it would be okay to remove some of them to cut down on computing times. Which variable(s) may you be able to cut and which variable(s) would you keep because it covers the same overall trends?\n\nThe X, Y, and Z columns all havea  similar coorelation to one another. Clarity is an attribute we ought to keep. \n","1dbf64af":"#### (26)\nDisplay a scatter plot comparing your price column to your predictions column. Ideally this would be an almost perfect line. You may want to make the size of the individual plotted points to be smaller. Title your plot with the type of regression used.\n","ccd83805":"#### (21)\nDisplay the full statistics on your DF again. Look particularly at the mean on the abs_error column. Does it match the number calculated above or is it slightly off? If it is off, can you explain why that may be?","4d35bb5e":"#### (4)\n\nLet's take a look at the current data graphically. This will be useful later when we want to clean up some potential misleading data.  \n\nSeaborn is a library designed for some amazing data visualizations.  \nImport: seaborn as sns  \nThen use: sns.pairplot( your data frame, hue='cut', plot_kws={'s':1} )  \nThis will take a bit of time to run. You can try a different column for the hue, color or clarity should work well. The plot_kws is to make the size of the individual plots be smaller.  \nAfter running it and studying it you may want to open the image in a new tab then comment this line out while you work on the rest of the project so you do not run it again. Uncomment it before submitting.\n","b5dc1c06":"#### (22)\nDisplay a scatter plot comparing your price column to your predictions column. Ideally this would be an almost perfect line. You may want to make the size of the individual plotted points to be smaller. Title your plot with the type of regression used (Linear Regression in this case).","6ee6653d":"#### (16)\nRun another LinearRegression model, but this time use only the \"Four C's\" as your features. Display your mean absolute error.","23bced24":"#### (10)\nDiamonds are often talked about with their \"Four C's\". Cut, Color, Clarity, and Carat. According to these correlations, rank the \"Four C's\" from most important to least important in price. (Think for a moment on this one!)  \n  \nMost Important  \n1)  \n2)  \n3)  \n4)  \nLeast Important  ","ae2cf104":"#### (24)\nDisplay a scatter plot comparing your price column to your predictions column. Ideally this would be an almost perfect line. You may want to make the size of the individual plotted points to be smaller. Title your plot with the type of regression used.","2c312dd9":"#### (27)\nChoose the model with the least mean absolute error. Update the prediction column and the abs_error column to use this model's predictions.\nDisplay your final df.","758924f7":"#### (6)\n\nDisplay all of the description statistics of the df.","3b2d8829":"#### (19)\nCompare the last three models used which utilized less features than the first.\n\n1) Is there anything that seems non-intuitive about which of these three sets is best?\n\nI dont understand why the numbers change every I rerun the code. \n\n2) Can you attempt to explain why it is the best of these three?\n\nI feel the set using carat, x, y, and z yielded the most accurate prediction because they are most closely coorelated. Additionally, there is just the right amount of data for a smooth curve without the extra noise that utilizing all the columns would add.\n\n3) Using all possible features gave the best result (hopefully). Assuming you had no concern over how long your model took to execute, can you think of a reason why you may want to leave a feature out of your model?\n\nCertain columns are inversely related to the price of a diamond and would make the answer inaccurate.","f36b63cb":"### Import other libraries as you need them here.\n#### You will need to re-run all or just this one cell if you add more imports later. You may also ignore this and import as you need later.","7b1f7142":"#### (13)\nBuild a linear regression model.\nChoose all of the features (aside from price) for your X values.\nChoose price for your y values.\n\nSplit your data into two equal halves for training and validating.\nRun your model on your training set.\nPredict your y values based on your validation X values.\n\nWhat is your mean absolute error?  \n"}}