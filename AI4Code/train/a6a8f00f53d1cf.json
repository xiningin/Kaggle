{"cell_type":{"e9e82a3c":"code","c796048c":"code","21b81f32":"code","2e801557":"code","8cbb9200":"code","e069067b":"code","8b513009":"code","ff8edbaa":"code","a3af9ce7":"code","e389a88b":"code","61937c46":"code","9f39e455":"code","2616f312":"code","5fd0d149":"code","6a21d1b3":"code","83d8b54b":"code","dc7d5fb3":"code","54b139a8":"code","386497fa":"code","833583f9":"code","ed887551":"code","97ae40cc":"code","e5b070b6":"code","8be94b5d":"code","86ec8ff8":"code","6cfd2757":"code","e649db68":"code","2955834d":"code","1f56163e":"code","42103b60":"code","e48f581e":"code","eccd1735":"code","16a65dd1":"code","bfe4b754":"code","89aa7bcb":"code","b165857b":"code","aaec99ff":"code","33d6372b":"code","820e80d8":"code","ad5722fb":"code","477f3eea":"code","893590e8":"code","74f24a70":"code","2341d4f5":"code","788bc513":"code","dd3ee3d3":"code","63033b77":"code","3ef8d83b":"code","af2b0711":"code","16893be1":"code","87bdc9eb":"code","99a0c5a5":"code","35516429":"code","4111f81a":"code","1b6ceca8":"code","516e3fb9":"code","6abee789":"code","a30bdd46":"code","579fa841":"code","1255f41b":"code","7909f026":"code","e6dea174":"code","3c166865":"code","b40a66b0":"code","5a7d0f98":"code","06c048af":"code","6c1164c4":"code","041f74e8":"code","8a78d4df":"code","e0a27abd":"code","498d44b0":"code","b579c754":"code","670581d4":"code","a994c84a":"code","d0177f92":"code","644ceb4f":"code","ad2382e3":"code","7281fccf":"code","2077bb58":"code","f473919c":"code","f39ae0be":"code","bb3a2681":"code","447d88f0":"code","91f391ca":"code","50c2b30c":"code","8a6e2232":"code","5aec9855":"code","691545db":"code","8d67c7e1":"code","227d40e1":"code","599ca219":"code","77cc1f49":"code","b1c59a07":"code","6973e387":"code","b804a00b":"code","a3e4cce0":"code","d97530e8":"code","421c07a5":"code","bd86f6c7":"code","c6ae6a38":"code","f8830a37":"code","163f1811":"code","c1c88f9c":"code","575a3511":"code","c2a4bdeb":"code","82240e12":"code","77589dbb":"code","b34c5dc3":"code","e92fb38d":"markdown","5541c128":"markdown","fbac6f35":"markdown","09ee3f71":"markdown","682546c0":"markdown","c142bc35":"markdown","f1901a8b":"markdown","71d70acc":"markdown","7ceb6654":"markdown","21f9c2d0":"markdown","b3f3a1ff":"markdown","30557229":"markdown","67e6d2c0":"markdown","7ec8ecb7":"markdown","f47a607a":"markdown","ea97dcd1":"markdown","0df39a6d":"markdown","6a684cca":"markdown","dace9183":"markdown","8f8b1738":"markdown","3b8887fc":"markdown","31d67f0f":"markdown","525457c8":"markdown","d1a61eb5":"markdown","02ae69dc":"markdown","33e662d1":"markdown","9daadc02":"markdown","f2dabaef":"markdown","45d23fa2":"markdown","d917d3ff":"markdown","da229b33":"markdown","6d0370e5":"markdown","328a0f98":"markdown","453e50d3":"markdown","ad83a620":"markdown","c7772466":"markdown","037a6c72":"markdown","4668ea7b":"markdown","9d5b5f08":"markdown","04658de0":"markdown","93f25b95":"markdown","9bc08974":"markdown","c95efd0e":"markdown","c919bf2b":"markdown","7a78fc7b":"markdown"},"source":{"e9e82a3c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.options.display.max_columns = 30\npd.set_option('float_format', '{:f}'.format)\npd.options.display.max_rows = 1000","c796048c":"df_train = pd.read_csv('..\/input\/credit-card-dataset\/fraudTrain.csv')","21b81f32":"df_test = pd.read_csv('..\/input\/credit-card-dataset\/fraudTest.csv')","2e801557":"df_train = df_train.drop(df_train.columns[0], axis=1)\ndf_test = df_test.drop(df_test.columns[0], axis=1)","8cbb9200":"df = pd.concat([df_train,df_test])","e069067b":"df.info()","8b513009":"df.shape","ff8edbaa":"df.isnull().sum()","a3af9ce7":"df.nunique()","e389a88b":"df.describe()","61937c46":"# share of non-fraudulent vs fraudulent transaction\ndf['is_fraud'].value_counts()","9f39e455":"df['is_fraud'].value_counts(normalize=True)*100","2616f312":"# visualizing the data imbalance using a pie chart\nsizes = [df.is_fraud.value_counts()[0], df.is_fraud.value_counts()[1]]\nexplode = (0,0.2) #explode the second slice i.e. fraudulent transaction\nplt.pie(sizes, explode=explode, labels=['Non-Fraudulent', 'Fraudulent'], colors=['gold', 'lightskyblue'], autopct='%1.1f%%', shadow=True, startangle=140)\nplt.show()","5fd0d149":"# following columns can be dropped as they don't influence nature of transaction\n# merchant, first, last, street, zip, trans_num, unix_time\ndrop_col_list = ['merchant', 'first', 'last', 'street', 'zip', 'unix_time','merch_lat','merch_long']\ndf.drop(labels=drop_col_list, inplace=True, axis=1)","6a21d1b3":"df.info()","83d8b54b":"df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])","dc7d5fb3":"df['hour'] = df['trans_date_trans_time'].dt.hour\ndf['hour']","54b139a8":"df['day'] = df['trans_date_trans_time'].dt.day_name()\ndf['day']","386497fa":"df['month'] = df['trans_date_trans_time'].dt.month\ndf['month']","833583f9":"# converting dob feature using to_datetime()\ndf['dob'] = pd.to_datetime(df['dob'])","ed887551":"# Calculate age = Transaction - DOB\ndf['age'] = np.round((df['trans_date_trans_time'] - df['dob'])\/np.timedelta64(1,'Y'))\ndf['age']","97ae40cc":"# now we can drop the 'dob' column\ndf = df.drop(labels='dob', axis=1)","e5b070b6":"# binning based on these rules\n# Upto 25% population - low density city\n# between 25-50% population - medium density city\n# above 50% population - high density city\n# This will help analyse if population is a factor for higher occurence of fraudulent transactions\ndf['density'] = pd.qcut(df['city_pop'], q=[0, .25, .75, 1], labels=['low_density', 'med_density', 'high_density'])","8be94b5d":"df['age_group'] = pd.cut(df['age'],bins=[10,20,30,40,50,60,100],labels=['10-20', '20-30', '30-40', '40-50', '50-60', '60 - Above'])","86ec8ff8":"df.nunique()","6cfd2757":"# we will include features with low unique value count so that a count plot can be easily readable \ncateg_cols = df[['category', 'gender', 'day', 'month', 'density']]","e649db68":"for i in categ_cols:\n    sns.countplot(x=df[i])\n    plt.xticks(rotation = 90)\n    plt.show()","2955834d":"# let's check proportion of transactions based on job profile of the customer\ndf['job'].value_counts(normalize=True)*100","1f56163e":"# let's check proportion of transactions based on state of residence of the customer\ndf['state'].value_counts(normalize=True)*100","42103b60":"# let's check proportion of transactions based on city of the customer\ndf['city'].value_counts(normalize=True)*100","e48f581e":"df_fraud = df.loc[df['is_fraud']==1]","eccd1735":"# we will include features with low unique value count so that a count plot can be easily readable \ncateg_cols_fraud = df_fraud[['category', 'gender', 'day', 'month', 'density']]","16a65dd1":"for i in categ_cols_fraud:\n    sns.countplot(x=df_fraud[i])\n    plt.xticks(rotation = 90)\n    plt.show()","bfe4b754":"df_fraud['state'].value_counts(normalize=True)*100","89aa7bcb":"df_fraud['city'].value_counts(normalize=True)*100","b165857b":"df_fraud['job'].value_counts(normalize=True)*100","aaec99ff":"#let's create a series of hours of the day corresponding to number of fraudulent transactions\ndf_hour_pattern = df.groupby(df['hour'])['is_fraud'].sum()","33d6372b":"df_hour_pattern","820e80d8":"x = np.arange(0,len(df_hour_pattern),1)\nfig, ax = plt.subplots(1,1,figsize=(20,5))\nax.plot(df_hour_pattern)\nax.set_xticks(x)\nplt.show()","ad5722fb":"temp_dummy = pd.get_dummies(df[['category','gender','age_group','day','density']], drop_first=True)\ndf = pd.concat([df, temp_dummy], axis=1)\ndf = df.drop(['category', 'gender', 'age_group', 'day', 'density'],axis=1)","477f3eea":"df.info()","893590e8":"#let's drop more unrequired columns before modelling\ndf = df.drop(['city', 'state', 'job', 'lat', 'long', 'city_pop'], axis=1)","74f24a70":"#df2 contains dataset without 'trans_num' since henceforth we need to perform scaling and modelling\n#However, we retain, 'df' dataset with 'trans_num' for Cost Benefit Analysis (if needed)\ndf2 = df.drop(['trans_num'], axis=1)","2341d4f5":"from sklearn.preprocessing import MinMaxScaler","788bc513":"from sklearn.model_selection import train_test_split","dd3ee3d3":"df2 = df2.drop('trans_date_trans_time',axis=1)","63033b77":"df = df.drop('trans_date_trans_time',axis=1)","3ef8d83b":"# we must split the dataset into train and test before treating for data imbalance\nX = df2.drop(['cc_num','is_fraud'],axis=1)\ny = df2['is_fraud']","af2b0711":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify = y, random_state=42)","16893be1":"scaler = MinMaxScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","87bdc9eb":"from imblearn.over_sampling import SMOTE","99a0c5a5":"# now we use SMOTE (oversampling) to treat data imbalance\nsm = SMOTE()\nX_train_bal, y_train_bal = sm.fit_resample(X_train, y_train.ravel())","35516429":"X_train_bal.shape","4111f81a":"y_train_bal.shape","1b6ceca8":"X_train_bal","516e3fb9":"#for drawing AUC ROC Curve\nimport sklearn.metrics as metrics","6abee789":"#user defined function to generate AUC plot\ndef auc_plot(model, title): \n  probs = model.predict_proba(X_test)\n  preds = probs[:,1]\n  fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n  roc_auc = metrics.auc(fpr, tpr)\n\n  import matplotlib.pyplot as plt\n  plt.title(title)\n  plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n  plt.legend(loc = 'lower right')\n  plt.plot([0, 1], [0, 1],'r--')\n  plt.xlim([0, 1])\n  plt.ylim([0, 1])\n  plt.ylabel('True Positive Rate')\n  plt.xlabel('False Positive Rate')\n  plt.show()","a30bdd46":"from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score","579fa841":"from sklearn.linear_model import LogisticRegression","1255f41b":"logreg = LogisticRegression(random_state=42, solver='liblinear')","7909f026":"logreg.fit(X_train_bal , y_train_bal)","e6dea174":"y_train_pred = logreg.predict(X_train_bal)\ny_test_pred = logreg.predict(X_test)","3c166865":"print(confusion_matrix(y_train_bal, y_train_pred))\nprint(classification_report(y_train_bal, y_train_pred))","b40a66b0":"print(confusion_matrix(y_test, y_test_pred))\nprint(classification_report(y_test, y_test_pred))","5a7d0f98":"# AUC plot for logistic regression model\nauc_plot(logreg, \"AUC-ROC Curve for Logistic Regression Model\")","06c048af":"from sklearn.tree import DecisionTreeClassifier","6c1164c4":"dt_default = DecisionTreeClassifier(random_state=42, criterion = 'gini', max_depth = 20)","041f74e8":"dt_default.fit(X_train_bal, y_train_bal)","8a78d4df":"y_train_pred_dt = dt_default.predict(X_train_bal)","e0a27abd":"y_test_pred_dt = dt_default.predict(X_test)","498d44b0":"print('\\n clasification report:\\n', classification_report(y_train_bal, y_train_pred_dt))\nprint('\\n confussion matrix:\\n',confusion_matrix(y_train_bal, y_train_pred_dt))","b579c754":"print('\\n clasification report:\\n', classification_report(y_test,y_test_pred_dt))\nprint('\\n confussion matrix:\\n',confusion_matrix(y_test, y_test_pred_dt))","670581d4":"#AUC-ROC Curve for for Decision Tree without Tuning\nauc_plot(dt_default, \"AUC-ROC Curve for for Decision Tree without Tuning\")","a994c84a":"from sklearn.model_selection import RandomizedSearchCV","d0177f92":"dt_tuned = DecisionTreeClassifier(random_state=42)","644ceb4f":"#@title\n# Create the parameter grid based on the results of random search \ndt_params = {\n    'max_depth': [2, 3, 5, 10, 20, 30],\n    'min_samples_leaf': [5, 10, 20, 50],\n    'criterion': [\"gini\", \"entropy\"]\n}","ad2382e3":"#@title  { form-width: \"20px\" }\n# instantiate the RandomizedSearchCV model\ndt_rs = RandomizedSearchCV(estimator=dt_tuned, param_distributions = dt_params, n_jobs=-1, verbose=1, return_train_score=True, n_iter = 10, scoring = 'recall')","7281fccf":"dt_rs.fit(X_train_bal, y_train_bal)","2077bb58":"#let's check he parameters of the best Decision Tree built in the forest\ndt_rs.best_estimator_","f473919c":"# accuracy of the best Decision Tree\ndt_rs.best_score_","f39ae0be":"# save the best Decision Tree of the Forest in a variable for further prediction and evaluation metrics\ndt_best = dt_rs.best_estimator_","bb3a2681":"print(classification_report(y_train_bal, dt_best.predict(X_train_bal)))\nprint(confusion_matrix(y_train_bal, dt_best.predict(X_train_bal)))","447d88f0":"print(classification_report(y_test, dt_best.predict(X_test)))\nprint(confusion_matrix(y_test, dt_best.predict(X_test)))","91f391ca":"#AUC-ROC Curve for for Decision Tree with Tuning\nauc_plot(dt_best, \"AUC-ROC Curve for for Decision Tree with Tuning\")","50c2b30c":"from sklearn.ensemble import RandomForestClassifier","8a6e2232":"rf = RandomForestClassifier(max_depth = 20, random_state=45, n_estimators=50, verbose = 1)","5aec9855":"rf.fit(X_train_bal, y_train_bal)","691545db":"y_train_pred_rf = rf.predict(X_train_bal)","8d67c7e1":"print('\\n clasification report:\\n', classification_report(y_train_bal, y_train_pred_rf))\nprint('\\n confussion matrix:\\n',confusion_matrix(y_train_bal, y_train_pred_rf))","227d40e1":"y_test_pred_rf = rf.predict(X_test)","599ca219":"print('\\n clasification report:\\n', classification_report(y_test,y_test_pred_rf))\nprint('\\n confussion matrix:\\n',confusion_matrix(y_test, y_test_pred_rf))","77cc1f49":"#AUC-ROC Curve for for Random Forest without Tuning\nauc_plot(rf, \"AUC-ROC Curve for for Random Forest without Tuning\")","b1c59a07":"rf_tuned = RandomForestClassifier(random_state=42, n_jobs=-1)","6973e387":"# Create the parameter grid based on the results of random search \nrf_params = {\n    'max_depth': [1, 2, 5, 10, 20],\n    'min_samples_leaf': [5, 10, 20, 50, 100],\n    'max_features': [2,3,4],\n    'n_estimators': [10, 30, 50, 100, 200]\n}","b804a00b":"rf_rs = RandomizedSearchCV(estimator=rf_tuned, param_distributions = rf_params, n_jobs=-1, verbose=1, return_train_score=True, n_iter = 10, scoring = 'recall')","a3e4cce0":"rf_rs.fit(X_train_bal, y_train_bal)","d97530e8":"rf_rs.best_estimator_","421c07a5":"rf_rs.best_score_","bd86f6c7":"rf_best = rf_rs.best_estimator_","c6ae6a38":"print(classification_report(y_train_bal, rf_best.predict(X_train_bal)))\nprint(confusion_matrix(y_train_bal, rf_best.predict(X_train_bal)))","f8830a37":"print(classification_report(y_test, rf_best.predict(X_test)))\nprint(confusion_matrix(y_test, rf_best.predict(X_test)))","163f1811":"#AUC-ROC Curve for for Random Forest with Tuning\nauc_plot(rf_best, \"AUC-ROC Curve for for Random Forest with Tuning\")","c1c88f9c":"#avg no. of transactions per month\nx = df.groupby(df['month'])[['trans_num', 'cc_num']].nunique().reset_index()","575a3511":"x['trans_num'].mean()\/2","c2a4bdeb":"df['is_fraud'].value_counts()","82240e12":"#since there are 24 months, divide 9651\/24, to get average no. of fraud trans\ndf['is_fraud'].value_counts()[1]\/24","77589dbb":"#avg. amount per fraudulent transaction = total amount in fraudulent transaction \/ total number of fraudulent transaction\nfraud_df = df[df['is_fraud']==1]","b34c5dc3":"fraud_df.amt.sum()\/df['is_fraud'].value_counts()[1]","e92fb38d":"##### Note: Although 'cc_num' and 'trans_num' seems irrelevant, I will not be removing it. It will be used for Cost-Benefit Analysis later on.","5541c128":"## 2.1 Read the dataset and create dataframe","fbac6f35":"## 3.1 Creating Dummy Variables","09ee3f71":"### 2.7.2 Deriving Day of the Week feature","682546c0":"One can see that X_train and y_train is balanced now","c142bc35":"### Average amount per fraudulent transaction ","f1901a8b":"## 2.3 Check for null values, unique value count for each feature and shape of combined dataset","71d70acc":"### Average number of fraudulent transactions per month","7ceb6654":"## 5.2 Decision Tree","21f9c2d0":"## 2.2 Combine the two dataframes","b3f3a1ff":"## 2.7 Deriving additional features","30557229":"# Random Forest with Hyper Parameter Tuning","67e6d2c0":"## 5.3 Random Forest","7ec8ecb7":"## 2.9 Analysing Fraudulent Transactions","f47a607a":"# 3 Feature Engineering","ea97dcd1":"# 2 Data Understanding, Preparation and EDA","0df39a6d":"### Average number of transactions per month ","6a684cca":"### 5.2.1 Decision Tree without hyper parameter tuning","dace9183":"### Train Scores","8f8b1738":"##### Observation: One can see that nearly 20% of fraudulent transactions are also from the same three states New York, Pennsylvania and Texas (which accounted for 20% of total transactions)","3b8887fc":"## 5.1 Logistic Regression","31d67f0f":"Skewness present in 'amt' feature will be corrected by using MinMaxScaler()","525457c8":"# 1 Install packages","d1a61eb5":"# 4 Train-Test Split and Treating Data Imbalance","02ae69dc":"##### One can see that class 1 i.e. fraudulent transactions represent 0.52 % of the dataset, making it a highly imbalanced dataset","33e662d1":"## 5.2.2 Decision Tree with hyper-parameter tuning","9daadc02":"### 2.7.4 Deriving Age of the customer at the time of transaction","f2dabaef":"## 2.4 Observing Data Imbalance","45d23fa2":"## 2.8 EDA on Categorical Features","d917d3ff":"## 2.6 Convert trans_date_tans_time into DateTime feature","da229b33":"## 2.5 Removing unnecessary columns","6d0370e5":"### 2.7.3 Deriving Month of the Year feature","328a0f98":"## Part I: Analyse the dataset and find the following figures:","453e50d3":"So we have obtained a baseline model where Accuracy of test improves over train set. Also, Recall metric which is important to detect False Negative is also nearly same for train and test set. \n\nSo, now we have a baseline model (without any hyper-parameter tuning ) to use as reference and see if using more advanced models, if the accuracy and recall scores improve or not.","ad83a620":"##### Observations: Similar to total transactions, even proportion of fraudulent transactions are spread out across all job profile and across all cities of USA. ","c7772466":"##### Observation: \n1. Category: Highest number of transactions occur in gas_transport\n2. Gender: Women have recorded higher number of transactions\n3. Day: Most number of transactions take place on Monday followed by Sunday\n4. Month: Highest number of transaction occur in the month of December\n5. Density: Cities with medium density report highest number of transactions.","037a6c72":"### **NOTE:** 'df2' dataset is used for scaling, resampling and modelling. \n### 'df' dataset is to be used for Cost Benefit Analysis in the end (after the modelling section)","4668ea7b":"# Cost Benefit Analysis","9d5b5f08":"# 5 Modelling","04658de0":"##### One can see there are no null values present in the dataset","93f25b95":"##### Observation: No significant proportion of tranasaction performed by one particular \"City\" or \"Job\" profile. However, based on \"State\" of residence, top 20% of transactions are performed by just 3 out of 51 states listed in the dataset which is a large share. They are New York, Texas and Pennsylvania.","9bc08974":"### 2.7.1 Deriving Hour of the Day feature","c95efd0e":"### 2.7.5 Deriving Population Density based on population of a city - Binning","c919bf2b":"### Test Scores","7a78fc7b":"##### Observations: One can see that number of incidents of fraudulent transactions increases drastically after 9 PM !!!"}}