{"cell_type":{"768d2a14":"code","a641620c":"code","a905bf88":"code","89362f54":"code","59852a03":"code","d740e8f6":"code","23af0044":"code","45a31379":"code","bd6b33f7":"code","54811864":"code","c2ffa504":"code","ff9ae94f":"code","0eb614fb":"code","669f437a":"code","5d14685d":"code","f9ed9dd2":"code","7240b377":"code","47bed4ab":"code","d863aac3":"code","08bef96a":"code","19abf281":"code","efc1426d":"code","5e656d8b":"code","b0369592":"code","1f185d08":"markdown","509ae8dc":"markdown","cdad1838":"markdown","3d95f3de":"markdown","b83c55f8":"markdown","7e2157b5":"markdown","7fd807fc":"markdown","745d58ea":"markdown","edf3a802":"markdown","b554ba13":"markdown"},"source":{"768d2a14":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a641620c":"#Load dataset\ndata = pd.read_csv('\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')","a905bf88":"input_binary_variable = ['anaemia','diabetes','high_blood_pressure','sex','smoking']\ninput_continous_variable = ['age','creatinine_phosphokinase','ejection_fraction','platelets','serum_creatinine','serum_sodium','time']\noutput_variable = 'DEATH_EVENT'","89362f54":"from sklearn import preprocessing\n\nx = data.values #returns a numpy array\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\nnorm_data = pd.DataFrame(x_scaled, columns=data.columns)","59852a03":"ax = sns.violinplot(x=\"variable\", y=\"value\", hue=\"DEATH_EVENT\",\n                   data=pd.melt(norm_data,id_vars='DEATH_EVENT'), split=True, linewidth=1,inner=\"quart\",\n                    palette={1: \"b\", 0: \".85\"})\nax.set_ylim([-0.5,1.5])\nax.set_xticklabels(ax.get_xticklabels(), rotation=90)","d740e8f6":"features_no_time = input_binary_variable+input_continous_variable\nfeatures_no_time.remove('time')\nfeatures_no_time","23af0044":"features_with_time = input_binary_variable+input_continous_variable\nfeatures_with_time","45a31379":"import tensorflow as tf\ntfkl = tf.keras.layers","bd6b33f7":"#FUNCTION TO PLOT THE TRAINING\ndef plot_training(fit, evaluation):\n    best_epoch = fit.epoch[fit.history['val_loss'].index(min(fit.history['val_loss']))]\n    fig, ax = plt.subplots(2,1,figsize=(3,5))\n    \n    ax[0].plot(fit.epoch,fit.history['val_loss'],'.-',color='red', label='validation')\n    ax[0].plot(fit.epoch,fit.history['loss'],'.-',color='orange', label='train')\n    ax[0].set(ylabel='Loss',ylim=[0,1])\n    ax[0].axvspan(best_epoch-0.5,best_epoch+0.5, alpha=0.5, color='red')\n    #ax[0].autoscale(False)\n    ax[0].scatter(best_epoch, evaluation[0],s=2, zorder=1,color='green')\n    ax[0].legend()\n    \n    ax[1].plot(fit.epoch,fit.history['val_accuracy'],'.-',color='red', label='validation')\n    ax[1].plot(fit.epoch,fit.history['accuracy'],'.-',color='orange', label='train')\n    ax[1].set(ylabel='Accuracy',ylim=[0,1])\n    ax[1].axvspan(best_epoch-0.5,best_epoch+0.5, alpha=0.5, color='red')\n    #ax[1].autoscale(False)\n    ax[1].scatter(best_epoch, evaluation[1],s=2, zorder=1,color='green')\n    ax[1].legend()\n    plt.show()\n    print(\"[Best epoch]:\", best_epoch)\n    print(\"[Loss]:\", min(fit.history['val_loss']), \" test:\", evaluation[0])\n    print(\"[Accuracy]:\", max(fit.history['val_accuracy']), \" test:\", evaluation[1])\n    \n","54811864":"input_array = norm_data[features_no_time].to_numpy()[:,:,np.newaxis]\noutput_array = norm_data[output_variable].to_numpy()[:,np.newaxis]\nprint(input_array.shape)\nprint(output_array.shape)","c2ffa504":"BATCH_SIZE = 1\nDATASET_SIZE = input_array.shape[0]\nbase_depth = 128\nconv_filters = 512\ndropout_prob = 0.4\nactivation_func = tf.nn.leaky_relu","ff9ae94f":"train_size = int(0.6 * DATASET_SIZE)\/\/BATCH_SIZE\nval_size = int(0.2 * DATASET_SIZE)\/\/BATCH_SIZE\ntest_size = int(0.2 * DATASET_SIZE)\/\/BATCH_SIZE\n\ndataset = tf.data.Dataset.from_tensor_slices( (input_array,output_array) ).shuffle(1000).batch(BATCH_SIZE)\ntrain_data = dataset.take(train_size)\ntest_data = dataset.skip(train_size)\nvalid_data = test_data.skip(test_size)\ntest_data = test_data.take(test_size)\n\nprint(\"\\n[Train size]:\",len(list(train_data)),\"\\n[Valid size]:\", len(list(valid_data)),\"\\n[Test size]:\", len(list(test_data)))","0eb614fb":"HFmodel_no_time = tf.keras.Sequential([\n    tf.keras.Input(shape=(len(features_no_time),1,)),\n    tfkl.Conv1D(filters=conv_filters,kernel_size=11, strides=2),\n    tfkl.Dropout(dropout_prob),\n    tfkl.Dense(base_depth,activation=activation_func),\n    tfkl.Dense(base_depth,activation=activation_func),\n    tfkl.Dense(base_depth,activation=activation_func),\n    tfkl.Dropout(dropout_prob),\n    tfkl.Dense(1,activation=tf.nn.sigmoid)\n], name=\"heart_failure_model_notime\")\n\nHFmodel_no_time.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy',metrics=['accuracy'])","669f437a":"fit = HFmodel_no_time.fit(train_data, epochs=400, validation_data=valid_data,\n                    batch_size=BATCH_SIZE, verbose=False,\n                    callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.000001),\n                               tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.0, patience=100, verbose=1, mode='auto', restore_best_weights=True)])\n\nevaluation = HFmodel_no_time.evaluate(test_data)\nplot_training(fit, evaluation)","5d14685d":"evaluations_no_time = []\nfor i in range(30):\n    \n    dataset = tf.data.Dataset.from_tensor_slices( (input_array,output_array) ).shuffle(300).batch(BATCH_SIZE)\n    train_data = dataset.take(train_size)\n    test_data = dataset.skip(train_size)\n    valid_data = test_data.skip(test_size)\n    test_data = test_data.take(test_size)\n    \n    tf.keras.backend.clear_session()\n    \n    HFmodel_no_time = tf.keras.Sequential([\n        tf.keras.Input(shape=(len(features_no_time),1,)),\n        tfkl.Conv1D(filters=conv_filters,kernel_size=11, strides=2),\n        tfkl.Dropout(dropout_prob),\n        tfkl.Dense(base_depth,activation=activation_func),\n        tfkl.Dense(base_depth,activation=activation_func),\n        tfkl.Dense(base_depth,activation=activation_func),\n        tfkl.Dropout(dropout_prob),\n        tfkl.Dense(1,activation=tf.nn.sigmoid)\n    ], name=\"heart_failure_model_notime\")\n    \n    HFmodel_no_time.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy',metrics=['accuracy'])\n    \n    fit = HFmodel_no_time.fit(train_data, epochs=400, validation_data=valid_data,\n                        batch_size=BATCH_SIZE, verbose=False,\n                        callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.000001),\n                                   tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.0, patience=100, verbose=1, mode='auto', restore_best_weights=True)])\n\n    accuracy = HFmodel_no_time.evaluate(test_data)[1]\n    evaluations_no_time.append(accuracy)\n","f9ed9dd2":"ax = sns.histplot(evaluations_no_time)\nax.set(xlim=(0,1))\nax.set(xlabel='Accuracy')\nnp.mean(evaluations_no_time)","7240b377":"input_array = norm_data[features_with_time].to_numpy()[:,:,np.newaxis]\noutput_array = norm_data[output_variable].to_numpy()[:,np.newaxis]\nprint(input_array.shape)\nprint(output_array.shape)","47bed4ab":"train_size = int(0.6 * DATASET_SIZE)\/\/BATCH_SIZE\nval_size = int(0.2 * DATASET_SIZE)\/\/BATCH_SIZE\ntest_size = int(0.2 * DATASET_SIZE)\/\/BATCH_SIZE\n\ndataset = tf.data.Dataset.from_tensor_slices( (input_array,output_array) ).shuffle(1000).batch(BATCH_SIZE)\ntrain_data = dataset.take(train_size)\ntest_data = dataset.skip(train_size)\nvalid_data = test_data.skip(test_size)\ntest_data = test_data.take(test_size)\n\nprint(\"\\n[Train size]:\",len(list(train_data)),\"\\n[Valid size]:\", len(list(valid_data)),\"\\n[Test size]:\", len(list(test_data)))","d863aac3":"HFmodel_with_time = tf.keras.Sequential([\n    tf.keras.Input(shape=(len(features_with_time),1,)),\n    tfkl.Conv1D(filters=conv_filters,kernel_size=12, strides=2),\n    tfkl.Dropout(dropout_prob),\n    tfkl.Dense(base_depth,activation=activation_func),\n    tfkl.Dense(base_depth,activation=activation_func),\n    tfkl.Dense(base_depth,activation=activation_func),\n    tfkl.Dropout(dropout_prob),\n    tfkl.Dense(1,activation=tf.nn.sigmoid)\n], name=\"heart_failure_model_time\")\n\nHFmodel_with_time.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy',metrics=['accuracy'])","08bef96a":"fit = HFmodel_with_time.fit(train_data, epochs=400, validation_data=valid_data,\n                    batch_size=BATCH_SIZE, verbose=False,\n                    callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.000001),\n                               tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.0, patience=100, verbose=1, mode='auto', restore_best_weights=True)])\n\nevaluation = HFmodel_with_time.evaluate(test_data)\nplot_training(fit, evaluation)","19abf281":"evaluations_with_time = []\nfor i in range(30):\n    \n    dataset = tf.data.Dataset.from_tensor_slices( (input_array,output_array) ).shuffle(300).batch(BATCH_SIZE)\n    train_data = dataset.take(train_size)\n    test_data = dataset.skip(train_size)\n    valid_data = test_data.skip(test_size)\n    test_data = test_data.take(test_size)\n    \n    tf.keras.backend.clear_session()\n    \n    HFmodel_with_time = tf.keras.Sequential([\n        tf.keras.Input(shape=(len(features_with_time),1,)),\n        tfkl.Conv1D(filters=conv_filters,kernel_size=12, strides=2),\n        tfkl.Dropout(dropout_prob),\n        tfkl.Dense(base_depth,activation=activation_func),\n        tfkl.Dense(base_depth,activation=activation_func),\n        tfkl.Dense(base_depth,activation=activation_func),\n        tfkl.Dropout(dropout_prob),\n        tfkl.Dense(1,activation=tf.nn.sigmoid)\n    ], name=\"heart_failure_model_time\")\n    \n    HFmodel_with_time.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy',metrics=['accuracy'])\n    \n    fit = HFmodel_with_time.fit(train_data, epochs=400, validation_data=valid_data,\n                        batch_size=BATCH_SIZE, verbose=False,\n                        callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.000001),\n                                   tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.0, patience=100, verbose=1, mode='auto', restore_best_weights=True)])\n\n    accuracy = HFmodel_with_time.evaluate(test_data)[1]\n    evaluations_with_time.append(accuracy)\n","efc1426d":"ax = sns.histplot(evaluations_with_time)\nax.set(xlim=(0,1))\nax.set(xlabel='Accuracy')\nnp.mean(evaluations_with_time)","5e656d8b":"df = pd.DataFrame({'no follow-up time':evaluations_with_time,'with follow-up time':evaluations_with_time})","b0369592":"ax = sns.boxplot(x='variable',y='value', data=df.melt())\nsns.stripplot(x='variable',y='value', data=df.melt(), ax=ax,color='black')\n#0.83 with time\n#0.74 without time\nax.set(ylim=(0,1))","1f185d08":"## Randomization ","509ae8dc":"## Normalization","cdad1838":"## Randomization ","3d95f3de":"## Utilities","b83c55f8":"# Conclusion","7e2157b5":"... discuss the follow up time ...","7fd807fc":"## Features distributions","745d58ea":"# With follow-up time","edf3a802":"# Without follow-up time","b554ba13":"The models outperfom the results in the original paper."}}