{"cell_type":{"1a23d850":"code","a82c8ccf":"code","d1451872":"code","ff5e45a5":"code","b66f6a7f":"code","d5f2c709":"code","683f0fe9":"code","52cc3e63":"code","8075524a":"code","aa7899a2":"code","f52a8e43":"code","59fb1698":"code","02f829d0":"code","ec5c8c8b":"code","0a7d694b":"code","9e473092":"code","353358e6":"code","f91946f5":"code","16202c06":"code","7b5561ba":"code","89e8cc6a":"code","09877c5c":"code","6f0bf1cd":"code","f996c328":"code","fd35e753":"code","df152c84":"code","75269dc2":"code","5bf830f1":"code","b95e9409":"code","439201ba":"code","90ec3bd6":"code","1a4791c4":"code","e2b8c63a":"code","5d5bf37c":"code","0c90fc8b":"code","974b7dd6":"code","ce851be2":"markdown","4f7340c7":"markdown","8f9d8bc6":"markdown","d874c993":"markdown","05fb889c":"markdown","f29a8c10":"markdown","84708f1e":"markdown","d3bf82b5":"markdown","2951c952":"markdown","2636c036":"markdown","c84b36a8":"markdown","0247180d":"markdown","569bb93a":"markdown","edf00990":"markdown","ca3f6df7":"markdown","5b74aad4":"markdown","1e8eed7a":"markdown","333b39ef":"markdown","f78b3cac":"markdown","6eae4976":"markdown","77f7b752":"markdown","8528b76e":"markdown","6b85cb8d":"markdown","41ab8d58":"markdown","e54fad3e":"markdown","f0382596":"markdown","3ecf1d45":"markdown","6ee4a0c9":"markdown","fd4f2270":"markdown","01701c61":"markdown","48deacf4":"markdown","27ad3672":"markdown"},"source":{"1a23d850":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split \nfrom imblearn import under_sampling, over_sampling\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\n","a82c8ccf":"data = pd.read_csv('..\/input\/startup-success-prediction\/startup data.csv')\ndata.head()","d1451872":"data.info()","ff5e45a5":"data.describe()","b66f6a7f":"data.isnull().sum()","d5f2c709":"data_missing_value = data.isnull().sum().reset_index()\ndata_missing_value.columns = ['feature','missing_value']\ndata_missing_value['percentage'] = round((data_missing_value['missing_value']\/len(data))*100,2)\ndata_missing_value = data_missing_value.sort_values('percentage', ascending=False).reset_index(drop=True)\ndata_missing_value = data_missing_value[data_missing_value['percentage']>0]\ndata_missing_value","683f0fe9":"data = data.drop(['Unnamed: 6'], axis=1)\ndata = data.drop(['state_code.1'], axis=1)","52cc3e63":"data.duplicated().sum()","8075524a":"data.duplicated(subset=['name']).sum()","aa7899a2":"data=data.drop_duplicates(subset=['name'])","f52a8e43":"data['age_first_milestone_year'] = data['age_first_milestone_year'].fillna(0)\ndata['age_last_milestone_year'] = data['age_last_milestone_year'].fillna(0)","59fb1698":"data['closed_at'] = pd.to_datetime(data['closed_at'])\ndata['founded_at'] = pd.to_datetime(data['founded_at'])\n#convert to datetime data\n\ndata['last_date']=data['closed_at'] #copy data\ndata['last_date']=data['last_date'].fillna('2013-12-31')\ndata['last_date']=pd.to_datetime(data['last_date'])","02f829d0":"data[\"founded_at\"] = pd.to_datetime(data[\"founded_at\"])\n\ndata[\"age\"] = (data[\"last_date\"]-data[\"founded_at\"])\ndata[\"age\"]=round(data.age\/np.timedelta64(1,'Y'))","ec5c8c8b":"data[[ 'age', 'age_first_funding_year','age_last_funding_year', 'age_first_milestone_year',\n       'age_last_milestone_year']].sort_values('age').head()","0a7d694b":"data=data.drop(data[data.age<0].index)\ndata=data.drop(data[data.age_first_funding_year<0].index)\ndata=data.drop(data[data.age_last_funding_year<0].index)\ndata=data.drop(data[data.age_first_milestone_year<0].index)\ndata=data.drop(data[data.age_last_milestone_year<0].index)","9e473092":"features = ['age_first_funding_year', 'relationships','funding_total_usd',\n            'age_last_funding_year','age_first_milestone_year', \n            'age_last_milestone_year', 'funding_rounds', \n            'milestones','avg_participants', 'age']\ndata[features].skew(axis=0, skipna=True)>2","353358e6":"norm = ['age_first_funding_year', 'relationships', 'funding_total_usd']\ndata = data\nfor var in norm:\n    data['norm_'+var]=MinMaxScaler().fit_transform(data[var].values.reshape(len(data),1))","f91946f5":"data_grp_3=data[data['labels']==1].groupby(['age']).agg({'labels':'count'}).reset_index()\ndata_grp_3.columns=['age','total_succes']\n\ndata_grp_4=data.groupby(['age']).agg({'labels':'count'}).reset_index()\ndata_grp_4.columns=['age','total']\n\ndata_grp_3=data_grp_3.merge(data_grp_4,\n                           on='age')\ndata_grp_3['succes_rate']=round((data_grp_3['total_succes']\/data_grp_3['total'])*100,2)\n\ndata_grp_3\n","16202c06":"fig, ax = plt.subplots(figsize=(15,7))\n\ng = sns.barplot(x = 'age',y='succes_rate',data=data_grp_3,ax=ax, \n               palette=sns.color_palette(\"Blues_d\", n_colors=13, desat=1))\n\nx = np.arange(len(data_grp_3))\ny = data_grp_3['succes_rate']\n\nfor i, v in enumerate(y):\n    ax.text(x[i]- 0.1, v+3, str(v)+'%', fontsize = 12, color='gray', fontweight='bold')\n    \ntitle = '''\n\n'''\nax.text(2.80,30,title,horizontalalignment='left',color='black',fontsize=12,fontweight='bold')\n    \n\ntext = '''\n\n'''\nax.text(0.5,50,text,horizontalalignment='left',color='black',fontsize=16,fontweight='normal')\n    \nax.set_ylim(0,100)\n\nax.set_xticklabels(ax.get_xticklabels(),rotation=0);\nplt.tight_layout","7b5561ba":"data_grp_5=data[data['labels']==1].groupby(['milestones']).agg({'labels':'count'}).reset_index()\ndata_grp_5.columns=['milestones','total_succes']\n\ndata_grp_6=data.groupby(['milestones']).agg({'labels':'count'}).reset_index()\ndata_grp_6.columns=['milestones','total']\n\ndata_grp_5=data_grp_5.merge(data_grp_6,\n                           on='milestones')\ndata_grp_5['succes_rate']=round((data_grp_5['total_succes']\/data_grp_5['total'])*100,2)\n\ndata_grp_5","89e8cc6a":"fig, ax = plt.subplots(figsize=(15,7))\n\ng = sns.barplot(x = 'milestones',y='succes_rate',data=data_grp_5,ax=ax, \n               palette=sns.color_palette(\"Blues_d\", n_colors=13, desat=1))\n\nx = np.arange(len(data_grp_5))\ny = data_grp_5['succes_rate']\n\nfor i, v in enumerate(y):\n    ax.text(x[i]- 0.1, v+3, str(v)+'%', fontsize = 12, color='gray', fontweight='bold')\n    \ntitle = '''\n\n'''\nax.text(2.80,30,title,horizontalalignment='left',color='black',fontsize=12,fontweight='bold')\n    \n\ntext = '''\n\n'''\nax.text(0.5,50,text,horizontalalignment='left',color='black',fontsize=16,fontweight='normal')\n    \nax.set_ylim(0,100)\n\nax.set_xticklabels(ax.get_xticklabels(),rotation=0)\nplt.tight_layout","09877c5c":"data['Relationships Range'] = data['relationships'].apply(lambda x : 'relationship 0' if x==0 else 'relationships >10' if x>10 else 'relationships 1-10' )\ndata20 = data.groupby(['Relationships Range', 'labels']).agg({'id' : 'count'}).reset_index()\ndata20_pv = pd.pivot_table(data20,\n                          index=['Relationships Range'],\n                          columns=['labels'],\n                          values=['id']).reset_index()\ndata20_pv.columns = ['Relationships Range', 'Closed', 'Acquired']\ndata20_pv['Total Company'] = data20_pv['Closed']+data20_pv['Acquired']\ndata20_pv['Success Rate'] = round(data20_pv['Acquired']\/data20_pv['Total Company']*100,2)\ndata20_pv\n","6f0bf1cd":"fig, ax = plt.subplots(figsize=(8,6))\n\ng = sns.barplot(x = 'Relationships Range',y='Success Rate',data=data20_pv,ax=ax, \n               palette=sns.color_palette(\"Blues_d\", n_colors=13, desat=1))\n\nx = np.arange(len(data20_pv['Relationships Range']))\ny = data20_pv['Success Rate']\n\nfor i, v in enumerate(y):\n    ax.text(x[i]- 0.1, v+3, str(v)+'%', fontsize = 20, color='gray', fontweight='bold')\n  \nax.set_xticklabels(ax.get_xticklabels(),rotation=0);","f996c328":"# Split Feature Vector and Label\nX = data[['norm_relationships', 'norm_age_first_funding_year','norm_funding_total_usd',\n          \n          'age_last_funding_year',\n          'age_first_milestone_year', 'age_last_milestone_year', \n          'funding_rounds', 'milestones','age',\n\n          'is_CA', 'is_NY', 'is_MA', 'is_TX', 'is_otherstate', \n          'is_software', 'is_web', 'is_mobile', 'is_enterprise', 'is_advertising', 'is_gamesvideo', \n          'is_ecommerce', 'is_biotech', 'is_consulting','is_othercategory', \n          'has_VC', 'has_angel', 'has_roundA','has_roundB', 'has_roundC', 'has_roundD', \n          'avg_participants','is_top500'\n          ]]\ny = data['labels'] # target \/ label\n\n#Splitting the data into Train and Test\nX_train, X_test,y_train,y_test = train_test_split(X,\n                                                y,\n                                                test_size = 0.3,\n                                                random_state = 42)\n# Oversampling\nX_train, y_train = over_sampling.RandomOverSampler(random_state=42).fit_resample(X_train, y_train)\n","fd35e753":"ab = AdaBoostClassifier(random_state=42)\nab.fit(X_train, y_train)\ny_predicted = ab.predict(X_test)\ny_predicted_train = ab.predict(X_train)\n\nprint('\\nconfusion matrix') # generate the confusion matrix\nprint(confusion_matrix(y_test, y_predicted))\nprint('\\naccuracy')\nprint(accuracy_score(y_test, y_predicted))\nprint('\\nclassification report')\nprint(classification_report(y_test, y_predicted)) # generate the precision, recall, f-1 score, num\nroc_auc_score(y_test, y_predicted)\n\nregression = AdaBoostClassifier(random_state=42)\nregression.fit(X_train, y_train)\nprint(\"Train Accuracy:\",regression.score(X_train, y_train))\nprint(\"Test Accuracy:\",regression.score(X_test, y_test))\n\nroc_auc_score(y_test, y_predicted)\nprint('AUC Score:',roc_auc_score(y_test, y_predicted))\n","df152c84":"feat_importances = pd.Series(ab.feature_importances_, index=X.columns)\nax = feat_importances.nlargest(10).plot(kind='barh')\nax.invert_yaxis()\nplt.xlabel('score')\nplt.ylabel('feature')\nplt.title('feature importance score')","75269dc2":"y_predicted=pd.DataFrame(y_predicted)\ny_test=pd.DataFrame(y_test)\n\ny_test=y_test.reset_index()\ny_test=y_test.drop(['index'],axis=1)\n\nX_test['funding_total_usd']=data['funding_total_usd']\nX_test=X_test.reset_index()\nX_test=X_test.drop(['index'],axis=1)\n\nX_test['y_predicted']=y_predicted\nX_test['y_test']=y_test\nX_test.head()","5bf830f1":"y_test[y_test['labels']==0].count()","b95e9409":"X_test[(X_test['y_test']==0)&(X_test['y_predicted']==0)]['y_predicted'].count()","439201ba":"y_test[y_test['labels']==1].count()","90ec3bd6":"X_test[(X_test['y_test']==1)&(X_test['y_predicted']==1)]['y_predicted'].count()","1a4791c4":"X_test['funding_total_usd'].sum()","e2b8c63a":"X_test[(X_test['y_predicted']==1)]['funding_total_usd'].sum()","5d5bf37c":"X_test[(X_test['y_test']==0)]['funding_total_usd'].sum()","0c90fc8b":"X_test[(X_test['y_test']==0)&(X_test['y_predicted']==1)]['funding_total_usd'].sum()","974b7dd6":"X_test[(X_test['y_test']==0)&(X_test['y_predicted']==0)]['funding_total_usd'].sum()","ce851be2":"3. Business insigt from relationship: startups with relationships more than 1 has potential to be successful startup (more than 61%)","4f7340c7":"**Add new feature 'Age'.**","8f9d8bc6":"If we check it, there is minus number in here. Drop minus number","d874c993":"**Total Invest with ML:**","05fb889c":"### Modeling with AdaBoost","f29a8c10":"# **Training and Test Data**","84708f1e":"Now we fill column 'age_first_milestone_year' and 'age_last_milestone_year' with 0. Zero is the the smallest value assumption for a company that has not passed its first milestone","d3bf82b5":"1. Business insight from age : startups that have lifespan of more than 4 years have a tendency to be successful startup (more than 52%)","2951c952":"# **Data Visualization**","2636c036":"**Check for missing values and duplicated data**","c84b36a8":"**Saving Fund Investment with predict ML:**","0247180d":"With ML AdaBoost, we are saving 1.5 Billion USD","569bb93a":"Predict True Fail Startup (True Negatif):","edf00990":"If we invest with AdaBoost, it will cost 3.3 Billion USD. It's 34% effieciency invesment","ca3f6df7":"# **Check Dataset**","5b74aad4":"**Potential Loss without ML:**","1e8eed7a":"Now we check for distribution, there is some features with skewness disribution more than 2. Handling it with normalization.","333b39ef":"# **Data Preprocessing**","f78b3cac":"There are 5 missing value in this dataset, namely 'closed_at', 'Unnamed:6', 'age_last_milestone_year', 'age_first_milestone_year', and 'state_code.1'\n\nFirst, drop column Unnamed:6 and state_code.1 because this is useless features","6eae4976":"If we Invest without ML AdaBoost it will cost 5 Billion USD","77f7b752":"**Potential Loss With ML:**","8528b76e":"We fill column last_date with 2013-12-31 with assumption that is the last number of dataset ","6b85cb8d":"# **Features Importances**","41ab8d58":"Without AdaBoost, potential loss investment is 1.8 Billion USD","e54fad3e":"**Total Invest without ML:**","f0382596":"# **Business Simulation**","3ecf1d45":"there is no duplicated data. But if we check the duplicated data with subset name, that will appear 1 duplicated data. Drop it","6ee4a0c9":"Total Fail in start up:","fd4f2270":"Predict True Success Startup(True Positif):","01701c61":"With AdaBoost, potential loss is reducting until 82%. It just cost 300 Million USD","48deacf4":"2. Business insight from milestone : Startups that have min 1 milestone has potential to be successful startup (more than 60%)","27ad3672":"Total Success in startup:"}}