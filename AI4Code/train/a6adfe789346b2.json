{"cell_type":{"9e9b8987":"code","2428b22b":"code","f6d661f1":"code","8cd9ba15":"code","5e1f7ef2":"code","446dff95":"code","c867a12b":"code","377ecf37":"code","caa44d06":"code","696dda55":"code","589caee8":"code","779a0d3b":"code","6b001a08":"code","ede02b5a":"code","a0212888":"code","c349af9d":"markdown"},"source":{"9e9b8987":"#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n#Common Model Helpers\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\npd.options.mode.chained_assignment = None\n\nimport time\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\n%matplotlib inline\nplt.style.use('ggplot')\nsns.set_style('darkgrid')\nplt.rcParams['figure.figsize'] = 12,8","2428b22b":"data_raw = pd.read_csv('..\/input\/titanic\/train.csv')\ndata_val  = pd.read_csv('..\/input\/titanic\/test.csv')\n\ndata1 = data_raw.copy(deep = True)\ndata_cleaner = [data1, data_val]\n\ndata_raw.sample(5) ","f6d661f1":"def missing_percentage(df):\n    total = df.isnull().sum().sort_values(ascending = False)\n    percent = round(df.isnull().sum().sort_values(ascending = False)\/len(df)*100,2)\n    return pd.concat([total, percent], axis=1, keys=['Total','Percent'])","8cd9ba15":"print('Train columns with null values:\\n',missing_percentage(data1))\nprint(\"-\"*10)\nprint('Test\/Validation columns with null values:\\n',missing_percentage(data_val))\nprint(\"-\"*10)","5e1f7ef2":"label = LabelEncoder()\n\nfor dataset in data_cleaner:    \n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n    dataset['IsAlone'] = 1 #initialize to yes\/1 is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no\/0 if family size is greater than 1\n    \n    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n    \n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n   \nstat_min = 10 #while small is arbitrary, we'll use the common minimum in statistics: http:\/\/nicholasjjackson.com\/2012\/03\/08\/sample-size-is-10-a-magic-number\/\ntitle_names = (data1['Title'].value_counts() < stat_min) #this will create a true false series with title name as index\n\n#apply and lambda functions are quick and dirty code to find and replace with fewer lines of code: https:\/\/community.modeanalytics.com\/python\/tutorial\/pandas-groupby-and-python-lambda-functions\/\ndata1['Title'] = data1['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)","446dff95":"#correlation heatmap of dataset\ndef correlation_heatmap(df,title):\n    _ , ax = plt.subplots(figsize =(18, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title(f\"Pearson Correlation of {title}\", y=1.05, size=15)\n\ncorrelation_heatmap(data1,\"Features\")","c867a12b":"#define y variable aka target\/outcome\nTarget = ['Survived']\n\n#define x variables for original features aka feature selection\ndata1_x = ['Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone'] #pretty name\/values for charts\ndata1_x_calc = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'Age', 'Fare','IsAlone','FamilySize']\n\n#define x variables for original with bin features to remove continuous variables\ndata1_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'AgeBin_Code', 'FareBin_Code','IsAlone']\ndata1_xy_bin = Target + data1_x_bin\n\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\n\nprint(\"Data1 Shape: {}\".format(data1.shape))\nprint(\"Train1 Shape: {}\".format(train1_x.shape))\nprint(\"Test1 Shape: {}\".format(test1_x.shape))\ntrain1_x.head()","377ecf37":"#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n    XGBClassifier()    \n    ]\n\n\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60\/30 split intentionally leaving out 10%\n\n#create table to compare MLA metrics\nMLA_columns = ['Alg Name', 'Parameters','Train Acc Mean', 'Test Acc Mean' ,'Time Taken']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n#create table to compare MLA predictions\nMLA_predict = data1[Target]\n\n#index through MLA and save performance to table\nrow_index = 0\nfor alg in MLA:\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'Alg Name'] = MLA_name\n    MLA_compare.loc[row_index, 'Parameters'] = str(alg.get_params())\n\n    cv_results = model_selection.cross_validate(alg, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True)\n\n    MLA_compare.loc[row_index, 'Time Taken'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'Train Acc Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'Test Acc Mean'] = cv_results['test_score'].mean()   \n\n    Y = np.reshape(data1[Target].values, (-1, ))\n    #save MLA predictions - see section 6 for usage\n    alg.fit(data1[data1_x_bin], Y)\n    MLA_predict[MLA_name] = alg.predict(data1[data1_x_bin])\n    row_index+=1\n    \n#print and sort table: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.sort_values.html\nMLA_compare.sort_values(by = ['Test Acc Mean'], ascending = False, inplace = True)\nMLA_compare","caa44d06":"#barplot using https:\/\/seaborn.pydata.org\/generated\/seaborn.barplot.html\nsns.barplot(x='Test Acc Mean', y = 'Alg Name', data = MLA_compare,palette=\"rocket\")\n\n#prettify using pyplot: https:\/\/matplotlib.org\/api\/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')","696dda55":"#base model\ndtree = tree.DecisionTreeClassifier(random_state = 0)\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split, return_train_score=True)\ndtree.fit(data1[data1_x_bin], data1[Target])\n\nprint('BEFORE DT Parameters: ', dtree.get_params())\nprint(\"BEFORE DT Training w\/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT Test w\/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint('-'*10)\n\nparam_grid = {'criterion': ['gini', 'entropy'],\n              'max_depth': [2,4,6,8,10,None],\n              'random_state': [0] \n             }\n\ntune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'accuracy', cv = cv_split,return_train_score=True)\ntune_model.fit(data1[data1_x_bin], data1[Target])\n\nprint('AFTER DT Parameters: ', tune_model.best_params_)\nprint(\"AFTER DT Training w\/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \nprint(\"AFTER DT Test w\/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\nprint('-'*10)","589caee8":"correlation_heatmap(MLA_predict,\"Algorithms\")","779a0d3b":"vote_est = [\n    ('ada', ensemble.AdaBoostClassifier()),\n    ('bc', ensemble.BaggingClassifier()),\n    ('etc',ensemble.ExtraTreesClassifier()),\n    ('gbc', ensemble.GradientBoostingClassifier()),\n    ('rfc', ensemble.RandomForestClassifier()),\n    ('gpc', gaussian_process.GaussianProcessClassifier()),\n    ('lr', linear_model.LogisticRegressionCV()),\n    ('bnb', naive_bayes.BernoulliNB()),\n    ('gnb', naive_bayes.GaussianNB()),\n    ('knn', neighbors.KNeighborsClassifier()),\n    ('svc', svm.SVC(probability=True)),\n    ('xgb', XGBClassifier())\n]\n\n#Hard Vote or majority rules\nvote_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\nvote_hard_cv = model_selection.cross_validate(vote_hard, data1[data1_x_bin], data1[Target], cv  = cv_split,return_train_score=True)\nvote_hard.fit(data1[data1_x_bin], data1[Target])\n\nprint(f\"Hard Voting Training w\/bin score mean: {vote_hard_cv['train_score'].mean()*100}\") \nprint(f\"Hard Voting Test w\/bin score mean: {vote_hard_cv['test_score'].mean()*100}\")\nprint('-'*10)\n\n\n#Soft Vote or weighted probabilities\nvote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\nvote_soft_cv = model_selection.cross_validate(vote_soft, data1[data1_x_bin], data1[Target], cv  = cv_split,return_train_score=True)\nvote_soft.fit(data1[data1_x_bin], data1[Target])\n\nprint(\"Soft Voting Training w\/bin score mean: {:.2f}\". format(vote_soft_cv['train_score'].mean()*100)) \nprint(\"Soft Voting Test w\/bin score mean: {:.2f}\". format(vote_soft_cv['test_score'].mean()*100))\nprint('-'*10)","6b001a08":"grid_n_estimator = range(50,350,50)\ngrid_ratio = [.1, .25, .5, .75, 1.0]\ngrid_learn = [.01, .03, .05, .1, .25]\ngrid_max_depth = [2, 4, 6, 8, 10, None]\ngrid_min_samples = [5, 10, .03, .05, .10]\ngrid_criterion = ['gini', 'entropy']\ngrid_bool = [True, False]\ngrid_seed = [0]\n\n\ngrid_param = [\n            [{\n            #AdaBoostClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostClassifier.html\n            'n_estimators': grid_n_estimator, #default=50\n            'learning_rate': grid_learn, #default=1\n            'algorithm': ['SAMME', 'SAMME.R'], #default=\u2019SAMME.R\n            'random_state': grid_seed\n            }],\n       \n    \n            [{\n            #BaggingClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n            'n_estimators': grid_n_estimator, #default=10\n            'max_samples': grid_ratio, #default=1.0\n            'random_state': grid_seed\n             }],\n\n    \n            [{\n            #ExtraTreesClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n            'n_estimators': grid_n_estimator, #default=10\n            'criterion': grid_criterion, #default=\u201dgini\u201d\n            'max_depth': grid_max_depth, #default=None\n            'random_state': grid_seed\n             }],\n\n\n            [{\n            #GradientBoostingClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n            'learning_rate': [.05], #default=0.1 -- 12\/31\/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n            'n_estimators': [300], #default=100 -- 12\/31\/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n            'max_depth': grid_max_depth, #default=3   \n            'random_state': grid_seed\n             }],\n\n    \n            [{\n            #RandomForestClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n            'n_estimators': grid_n_estimator, #default=10\n            'criterion': grid_criterion, #default=\u201dgini\u201d\n            'max_depth': grid_max_depth, #default=None\n            'oob_score': [True],\n            'random_state': grid_seed\n             }],\n    \n            [{    \n            #GaussianProcessClassifier\n            'max_iter_predict': grid_n_estimator, #default: 100\n            'random_state': grid_seed\n            }],\n        \n    \n            [{\n            #LogisticRegressionCV - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n            'fit_intercept': grid_bool, #default: True\n            'penalty': ['l1','l2'],\n            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], #default: lbfgs\n            'random_state': grid_seed\n             }],\n            \n    \n            [{\n            #BernoulliNB - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB\n            'alpha': grid_ratio, #default: 1.0\n             }],\n    \n    \n            #GaussianNB - \n            [{}],\n    \n            [{\n            #KNeighborsClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n            'n_neighbors': [1,2,3,4,5,6,7], #default: 5\n            'weights': ['uniform', 'distance'], #default = \u2018uniform\u2019\n            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n            }],\n            \n    \n            [{\n            #SVC - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html#sklearn.svm.SVC\n#             'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n            'C': [1,2,3,4,5], #default=1.0\n            'gamma': grid_ratio, #edfault: auto\n            'decision_function_shape': ['ovo', 'ovr'], #default:ovr\n            'probability': [True],\n            'random_state': grid_seed\n             }],\n\n    \n            [{\n            #XGBClassifier - http:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\n            'learning_rate': grid_learn, #default: .3\n            'max_depth': [1,2,4,6,8,10], #default 2\n            'n_estimators': grid_n_estimator, \n            'seed': grid_seed  \n             }]   \n        ]\n\n\nstart_total = time.perf_counter() \nfor clf, param in zip (vote_est, grid_param): \n    start = time.perf_counter()        \n    best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = 'roc_auc')\n    best_search.fit(data1[data1_x_bin], data1[Target])\n    run = time.perf_counter() - start\n\n    best_param = best_search.best_params_\n    print('The best parameter for {} is {} with a runtime of {:.2f} seconds.'.format(clf[1].__class__.__name__, best_param, run))\n    clf[1].set_params(**best_param) \n    print('-'*10)\n\nrun_total = time.perf_counter() - start_total\nprint('Total optimization time was {:.2f} minutes.'.format(run_total\/60))","ede02b5a":"grid_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\ngrid_hard_cv = model_selection.cross_validate(grid_hard, data1[data1_x_bin], data1[Target], cv  = cv_split,return_train_score=True)\ngrid_hard.fit(data1[data1_x_bin], data1[Target])\n\nprint(\"Hard Voting w\/Tuned Hyperparameters Training w\/bin score mean: {:.2f}\". format(grid_hard_cv['train_score'].mean()*100)) \nprint(\"Hard Voting w\/Tuned Hyperparameters Test w\/bin score mean: {:.2f}\". format(grid_hard_cv['test_score'].mean()*100))\nprint('-'*10)\n\n#Soft Vote or weighted probabilities w\/Tuned Hyperparameters\ngrid_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\ngrid_soft_cv = model_selection.cross_validate(grid_soft, data1[data1_x_bin], data1[Target], cv  = cv_split,return_train_score=True)\ngrid_soft.fit(data1[data1_x_bin], data1[Target])\n\nprint(\"Soft Voting w\/Tuned Hyperparameters Training w\/bin score mean: {:.2f}\". format(grid_soft_cv['train_score'].mean()*100)) \nprint(\"Soft Voting w\/Tuned Hyperparameters Test w\/bin score mean: {:.2f}\". format(grid_soft_cv['test_score'].mean()*100))\nprint('-'*10)","a0212888":"data_val['Survived'] = vote_soft.predict(data_val[data1_x_bin])\nsubmit = data_val[['PassengerId','Survived']]\nsubmit.to_csv(\"submit.csv\", index=False)","c349af9d":"> Credits: **LD Freeman**'s<a href=\"https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\">Notebook<\/a> greatly helped me to get started with my kaggle journey! "}}