{"cell_type":{"61179a7d":"code","cad4e307":"code","77b2e392":"code","9b2a3153":"code","d6e4a3d3":"code","78216eec":"code","b21dc526":"code","b7be7580":"code","d064df49":"code","92e29be0":"code","4b6c61f4":"code","8de5ff0c":"code","e852cc7d":"code","5db0129d":"code","453bfec4":"code","c8d6b320":"code","123878ef":"code","a52c5d0e":"code","d30939a9":"code","091776c7":"markdown","bd3c29cb":"markdown","d2245cf7":"markdown","fb307c07":"markdown","3ae1e9f1":"markdown","b4ea2bb4":"markdown","edd0e225":"markdown","68254ee3":"markdown","76ede5b7":"markdown","9d5e25b5":"markdown","3690a922":"markdown","99e97d8c":"markdown","3d936c3b":"markdown","21f1317c":"markdown","bc215f9f":"markdown","ee72381b":"markdown","bee5cbd3":"markdown","c6a6d76a":"markdown","c4750151":"markdown","8dad2f89":"markdown","d021ed49":"markdown","2917b603":"markdown","2448e55c":"markdown","ea8c510f":"markdown","818aec95":"markdown","6cf52eb3":"markdown","558e4b64":"markdown"},"source":{"61179a7d":"import time\nimport numpy as np\nimport h5py\nimport matplotlib.pyplot as plt\nimport scipy\nfrom PIL import Image\nfrom scipy import ndimage\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (5.0, 4.0) # tama\u00f1o de las graficas por defecto\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\n%load_ext autoreload\n%autoreload 2\n\nnp.random.seed(1) # se utiliza para replicar las funciones aleatorias ","cad4e307":"from shutil import copyfile\n\n# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"..\/input\/scriptstaller5\/dnn_app_utils_v3.py\", dst = \"..\/working\/dnn_app_utils_v3.py\")\n\n# import all our functions\n\nfrom dnn_app_utils_v3 import *","77b2e392":"def load_data():\n    train_dataset = h5py.File('..\/input\/neural\/train_catvnoncat.h5', \"r\")\n    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:])\n    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) \n\n    test_dataset = h5py.File('..\/input\/neural\/test_catvnoncat.h5', \"r\")\n    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) \n    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) \n\n    classes = np.array(test_dataset[\"list_classes\"][:]) \n    \n    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n    \n    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n","9b2a3153":"train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\nclases=[\"no-gato\", \"gato\"]","d6e4a3d3":"# Ejemplo de una imagen\nindice = 200\nplt.imshow(train_x_orig[indice])\nprint (\"La imagen #\" + str(indice) + \", es un '\" + str(clases[np.squeeze(train_y[:, indice])]) + \"'\" )","78216eec":"# Explore los datos\nm_train = train_x_orig.shape[0]\nm_test = test_x_orig.shape[0]\nnum_px = train_x_orig.shape[1]\n\nprint (\"N\u00famero de ejemplos de entrenamiento: m_train = \" + str(m_train))\nprint (\"N\u00famero de ejemplos de prueba: m_test = \" + str(m_test))\nprint (\"Cada imagen es de tama\u00f1o: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n\nprint (\"Dimensi\u00f3n del train_x_orig: \" + str(train_x_orig.shape))\nprint (\"Dimensi\u00f3n del train_y: \" + str(train_y.shape))\nprint (\"Dimensi\u00f3n del test_x_orig: \" + str(test_x_orig.shape))\nprint (\"Dimensi\u00f3n del test_y: \" + str(test_y.shape))","b21dc526":"# Cambiamos la dimensi\u00f3n de los ejemplos de entrenamiento y prueba\ntrain_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # El \"-1\" hace que la funci\u00f3n reshape() aplane (flatten) las restantes dimensiones\ntest_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n\n# Normalizamos los datos para tener valores entre 0 y 1.\ntrain_x = train_x_flatten\/255.\ntest_x = test_x_flatten\/255.\n\nprint (\"Dimensiones de train_x: \" + str(train_x.shape))\nprint (\"Dimensiones de test_x: \" + str(test_x.shape))\n","b7be7580":"### DEFINICI\u00d3N DE CONSTANTES (HIPER-PAR\u00c1METROS) DEL MODELO ####\nn_x = 12288     # num_px * num_px * 3\nn_h = 7\nn_y = 1\nlayers_dims = (n_x, n_h, n_y)","d064df49":"# FUNCI\u00d3N A CALIFICAR: two_layer_model\n\ndef two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n    \"\"\"\n    Implemente una red neuronal de dos capas: LINEAL->RELU->LINEAL->SIGMOIDE\n    Input:\n    X: datos de entrada de tama\u00f1o (n_x, n\u00famero de ejemplos)\n    Y: vector de etiquetas observadas de tama\u00f1o (1, n\u00famero de ejemplos)\n    layers_dims: dimensiones de las capas (n_x, n_h, n_y)\n    num_iterations: n\u00famero de iteraciones del bucle de optimizaci\u00f3n\n    learning_rate: tasa de aprendizaje de la regla de actualizaci\u00f3n por G.D. \n    print_cost: si es verdadero \"True\", muestra el coste cada 100 iteraciones \n    Output:\n    parameters: diccionario con W1, W2, b1, y b2\n    \"\"\"\n    \n    np.random.seed(1)\n    grads = {}\n    costs = []                              \n    m = X.shape[1]                           \n    (n_x, n_h, n_y) = layers_dims\n    \n    # Inicialice el diccionario de parametros, llamando una de las funciones que ya implement\u00f3\n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ###  (\u2248 1 l\u00ednea de c\u00f3digo)\n    parameters = \n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    # Obtenga W1, b1, W2 y b2 del diccionario de par\u00e1metros.\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    \n    # Bucle (G.D.)\n    for i in range(0, num_iterations):\n\n        # Propagaci\u00f3n hacia delante: LINEAL -> RELU -> LINEAL -> SIGMOIDE. Inputs: \"X, W1, b1, W2, b2\". Outputs: \"A1, cache1, A2, cache2\".\n        ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ###  (\u2248 2 l\u00edneas de c\u00f3digo)\n        A1, cache1 = \n        A2, cache2 = \n        ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n        \n        # Calcule el coste\n        ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ###  (\u2248 1 l\u00ednea de c\u00f3digo)\n        cost = \n        ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n        \n        # Inicialice la retro-propagaci\u00f3n\n        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n        \n        # Retro-propagaci\u00f3n. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; , dW1, db1; dA0 no es utilizado\".\n        ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ###  (\u2248 2 l\u00edneas de c\u00f3digo)\n        dA1, dW2, db2 = \n        dA0, dW1, db1 = \n        ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n        \n        # Iguale grads['dWl'] a dW1, grads['db1'] a db1, grads['dW2'] a dW2, grads['db2'] a db2\n        grads['dW1'] = dW1\n        grads['db1'] = db1\n        grads['dW2'] = dW2\n        grads['db2'] = db2\n        \n        # Actualice los par\u00e1metros.\n        ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ###  (1 l\u00ednea de c\u00f3digo)\n        parameters = \n        ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n\n        # Recupere W1, b1, W2, b2 de parameters\n        W1 = parameters[\"W1\"]\n        b1 = parameters[\"b1\"]\n        W2 = parameters[\"W2\"]\n        b2 = parameters[\"b2\"]\n        \n        # Imprima el coste cada 100 iteraciones\n        if print_cost and i % 100 == 0:\n            print(\"Coste tras la iteraci\u00f3n {}: {}\".format(i, np.squeeze(cost)))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)\n       \n    # grafique el coste\n\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('iteraciones')\n    plt.title(\"Tasa de aprendizaje =\" + str(learning_rate))\n    plt.show()\n    \n    return parameters","92e29be0":"parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True)","4b6c61f4":"predictions_train = predict(train_x, train_y, parameters)","8de5ff0c":"predictions_test = predict(test_x, test_y, parameters)","e852cc7d":"### CONSTANTES ###\nlayers_dims = [12288, 20, 7, 5, 1] #  Modelo de 4 capas","5db0129d":"# FUNCI\u00d3N A CALIFICAR: L_layer_model\n\ndef L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n    \"\"\"\n    Implemente una red neuronal de L capas: [LINEAL->RELU]*(L-1)->LINEAL->SIGMOIDE\n    Input:\n    X: datos de entrada de tama\u00f1o (n_x, n\u00famero de ejemplos)\n    Y: vector de etiquetas observadas de tama\u00f1o (1, n\u00famero de ejemplos)\n    layers_dims: dimensiones de las capas (n_x, n_h, n_y)\n    num_iterations: n\u00famero de iteraciones del bucle de optimizaci\u00f3n\n    learning_rate: tasa de aprendizaje de la regla de actualizaci\u00f3n por G.D. \n    print_cost: si es verdadero \"True\", muestra el coste cada 100 iteraciones \n    Output:\n    parameters: diccionario con W1, W2, b1, y b2\n    \"\"\"\n\n    np.random.seed(1)\n    costs = []                         \n    \n    # Inicializaci\u00f3n de par\u00e1metros. (\u2248 1 linea de c\u00f3digo)\n    ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### \n    parameters = \n    ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n    # Bucle (G.D.)\n    for i in range(0, num_iterations):\n\n        # Propagaci\u00f3n hacia delante: [LINEAL -> RELU]*(L-1) -> LINEAL -> SIGMOIDE.\n        ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ###  (\u2248 1 l\u00ednea de c\u00f3digo)\n        AL, caches = \n        ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n        \n        # Calcule el coste.\n        ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ###  (\u2248 1 l\u00ednea de c\u00f3digo)\n        cost = \n        ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n    \n        # Retro-propagaci\u00f3n.\n        ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### (\u2248 1 l\u00ednea de c\u00f3digo)\n        grads = \n        ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n \n        # Actualize los par\u00e1metros\n        ### EMPIEZE EL C\u00d3DIGO AQU\u00cd ###  (\u2248 1 l\u00ednea de c\u00f3digo)\n        parameters = \n        ### TERMINE EL C\u00d3DIGO AQU\u00cd ###\n                \n        # Imprime el coste cada 100 iteraciones\n        if print_cost and i % 100 == 0:\n            print (\"Coste tras la iteraci\u00f3n %i: %f\" %(i, cost))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)\n            \n    # grafique el coste\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('Coste')\n    plt.xlabel('Iteraciones')\n    plt.title(\"Tasa de aprendizaje =\" + str(learning_rate))\n    plt.show()\n    \n    return parameters","453bfec4":"parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)","c8d6b320":"pred_train = predict(train_x, train_y, parameters)","123878ef":"pred_test = predict(test_x, test_y, parameters)","a52c5d0e":"print_mislabeled_images(clases, test_x, test_y, pred_test)","d30939a9":"### EMPIEZE EL C\u00d3DIGO AQU\u00cd ### \nmi_imagen =   # el nombre debe coincidir con el de su imagen\nmi_etiqueta =  # la clase verdadera para su imagen (1 -> gato, 0 -> no-gato)\n## TERMINE EL C\u00d3DIGO AQU\u00cd ##\n\nfname = \"images\/\" + mi_imagen\nimage = np.array(ndimage.imread(fname, flatten=False))\nmi_imagen = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((num_px*num_px*3,1))\nmi_imagen = mi_imagen\/255.\nmi_imagen_pred = predict(mi_imagen, mi_etiqueta, parameters)\n\nplt.imshow(image)\nprint(\"y = \" + str(int(np.squeeze(mi_imagen_pred))) + \", el modelo predice que es una imagen de un \\\"\" + clases[int(np.squeeze(mi_imagen_pred))]+  \"\\\".\")\n","091776c7":"# Redes neuronales profundas para clasificaci\u00f3n de imagenes: aplicaci\u00f3n \n\nEn este taller va a utilizar las funciones que implement\u00f3 en el taller anterior: \"Taller4_RedesNeuronalesProfundas\", y va a construir una red neuronal profunda, aplicandola a la clasificaci\u00f3n de imagenes. \n\nEl modelo se va a aplicar sobre las imagenes de \u00b4gato, no-gato', y esperamos ver una mejor\u00eda en la precisi\u00f3n relativa (accuracy) con respecto a la regresi\u00f3n log\u00edstica implementada anteriormente.  \n\n**Tras este taller va a ser capaz de:**\n- Construir y aplicar una red neuronal profunda para el aprendizaje supervisado. \n","bd3c29cb":"**Nota**: Puede ver que si se ejecuta el modelo con menos iteraciones (e.g. 1500) se obtendr\u00eda una mayor precisi\u00f3n de prueba. Esto se conoce como \"early stopping\" y es un criterio de parada, \u00fatil para prevenir el sobreajuste, que veremos m\u00e1s adelante. \n\nMuy bien! Parece que su modelo de red neuronal con 2 capas tiene un mejor desempe\u00f1o (72%) que el de la regresi\u00f3n log\u00edstica del Taller 2 (70%). Ahora vamos a ver si podemos mejorar los resultados con un modelo de $L$ capas.","d2245cf7":"Como de costumbre, cambiamos las dimensiones (reshape) y estandarizamos las imagenes antes de alimentarlas a la red. ","fb307c07":"Primero se deben importar todos los paquetes que se van a necesitar durante este taller.\n- [numpy](www.numpy.org) paquete b\u00e1sico para ciencias computacionales con Python.\n- [matplotlib](http:\/\/matplotlib.org) librer\u00eda para graficar en Python.\n- [h5py](http:\/\/www.h5py.org) paquete para interactuar con datos guardados como archivo H5.\n- [PIL](http:\/\/www.pythonware.com\/products\/pil\/) y [scipy](https:\/\/www.scipy.org\/) usados para probar el modelo con imagenes propias al final del taller.\n- dnn_app_utils provee las funciones implementadas en el taller anterior.","3ae1e9f1":"**Salida esperada**\n<table> \n    <tr>\n        <td> **Coste tras la iteraci\u00f3n 0**<\/td>\n        <td> 0.6930497356599888 <\/td>\n    <\/tr>\n    <tr>\n        <td> **Coste tras la iteraci\u00f3n 100**<\/td>\n        <td> 0.6464320953428849 <\/td>\n    <\/tr>\n    <tr>\n        <td> **...**<\/td>\n        <td> ... <\/td>\n    <\/tr>\n    <tr>\n        <td> **Coste tras la iteraci\u00f3n 2400**<\/td>\n        <td> 0.048554785628770206 <\/td>\n    <\/tr>\n<\/table>","b4ea2bb4":"Muy bien! Parece que la red neuronal de 4 capas tiene mejor desempe\u00f1o (80%) que la red de 2 capas (72%) sobre el mismo conjunto de datos. \n\nEste es un buen desempe\u00f1o! \n\nEn las pr\u00f3ximas clases veremos c\u00f3mo podemos obtener una precisi\u00f3n aun mayor, buscando de manera sistem\u00e1tica mejores estimaciones para los hiper-par\u00e1metros (tasa de parendizaje, tama\u00f1o de las capas, n\u00famero de iteraciones, entre otros). ","edd0e225":"Espero te haya gustado este Notebook. Por favor compartelo para que entre todos aprendamos juntos.\n\nPuedes Seguirme a mi cuenta en Twitter **[@andres_jejen](https:\/\/twitter.com\/andres_jejen)** Constantemente comparto noticias y contenido educativo sibre Machine Learning, Big Data y Data Science.","68254ee3":"## 7) Prueba con su propia imagen ##\n\nAhora puede utilizar su propia imagen para probar el modelo. Para hacerlo agregue su(s) imagen(es) al directorio de este cuaderno en la carpeta \"images\", cambie el nombre de la (s) imagen(es) en el c\u00f3digo siguiente, y compruebe si el algoritmo acierta (1=gato, 0=no-gato). ","76ede5b7":"**Las distintas imagenes sobre las que el modelo tiende a equivocarse incluyen:** \n- El cuerpo del gato est\u00e1 en una posici\u00f3n inusual\n- El gato aparece contra el fondo de un color similar\n- Color y especie de gato inusual \n- Angulo de la c\u00e1mara inusual\n- Imagen muy\/poco brillante\n- Escala de la variaci\u00f3n (el gato aparece muy grande o muy peque\u00f1o en la imagen) ","9d5e25b5":"**Salida esperada**\n\n<table> \n    <tr>\n        <td> **Precisi\u00f3n**<\/td>\n        <td> 0.72 <\/td>\n    <\/tr>\n<\/table>","3690a922":"## 4 - Red neuronal de 2 capas\n\n**Ejercicio**:  Utilice las funciones auxiliares que ha implementado en el taller anterior para construir una red de dos capas con la siguiente estructura: *LINEAL -> RELU -> LINEAL -> SIGMOIDE*. Las funciones que va a necesitar y sus entradas son:\n```python\ndef initialize_parameters(n_x, n_h, n_y):\n    ...\n    return parameters \ndef linear_activation_forward(A_prev, W, b, activation):\n    ...\n    return A, cache\ndef compute_cost(AL, Y):\n    ...\n    return cost\ndef linear_activation_backward(dA, cache, activation):\n    ...\n    return dA_prev, dW, db\ndef update_parameters(parameters, grads, learning_rate):\n    ...\n    return parameters\n```","99e97d8c":"Ahora va a entrenar el modelo como una red neuronal de 4 capas. \n\nEjecute la siguente celda para aprender los par\u00e1metros. Aseg\u00farese que su modelo corre bien: el coste deber\u00eda decrecer. Puede tomar algunos minutos el ejecutar 2500 iteraciones. Revise que el \"Coste tras la iteraci\u00f3n 0\" coincida con la salida esperada abajo, si no, haga click en el cuadrado de STOP (\u2b1b) arriba en la barra del cuaderno para detener el proceso e intente encontrar su error.","3d936c3b":"**Salida esperada**\n\n<table> \n    <tr>\n        <td> **Precisi\u00f3n de prueba**<\/td>\n        <td> 0.8 <\/td>\n    <\/tr>\n<\/table>","21f1317c":"## 5 - Red neuronal de L capas\n\n**Ejercicio**:  Utilice las funciones auxiliares que ha implementado en el anterior taller para construir una red de L capas con la siguiente estructura: *[LINEAL -> RELU]$\\times$(L-1) -> LINEAL -> SIGMOIDE*. Las funciones que va a necesitar y sus entradas son:\n\n```python\ndef initialize_parameters_deep(layers_dims):\n    ...\n    return parameters \ndef L_model_forward(X, parameters):\n    ...\n    return AL, caches\ndef compute_cost(AL, Y):\n    ...\n    return cost\ndef L_model_backward(AL, Y, caches):\n    ...\n    return grads\ndef update_parameters(parameters, grads, learning_rate):\n    ...\n    return parameters\n```\n","bc215f9f":"## 1 - Paquetes","ee72381b":"N\u00f3tese que gracias a la implementaci\u00f3n vectorizada, el c\u00f3digo no tarda demasiado en ejecutarse (de otro modo, podr\u00eda haber llevado hasta 10 veces m\u00e1s tiempo).\n\nAhora puede usar los parametros entrenados para clasificar las imagenes del conjunto de datos. Para ver las predicciones de entrenamiento y prueba (test), ejecute la celda abajo.","bee5cbd3":"El siguiente c\u00f3digo muestra una imagen del conjunto de datos. Puede cambiar el \u00edndice y volver a ejecutar para cambiar de imagen.  ","c6a6d76a":"**Salida esperada**\n<table> \n    <tr>\n        <td> **Precisi\u00f3n**<\/td>\n        <td> 1.0 <\/td>\n    <\/tr>\n<\/table>","c4750151":"**Salida esperada**\n\n<table>\n    <tr>\n    <td>\n    **Precisi\u00f3n de entrenamiento**\n    <\/td>\n    <td>\n    0.985645933014\n    <\/td>\n    <\/tr>\n<\/table>","8dad2f89":"##  6) An\u00e1lisis de resultados\n\nPrimero analicemos el error de manera manual. Revisemos algunas de las imagenes que el modelo de L capas etiquet\u00f3 incorrectamente. Ejecute la celda abajo para visualizar algunos de estos errores. ","d021ed49":"**Salida esperada**\n<table> \n    <tr>\n        <td> **Coste tras la iteraci\u00f3n 0**<\/td>\n        <td> 0.771749 <\/td>\n    <\/tr>\n    <tr>\n        <td> **Coste tras la iteraci\u00f3n 100**<\/td>\n        <td> 0.672053 <\/td>\n    <\/tr>\n    <tr>\n        <td> **...**<\/td>\n        <td> ... <\/td>\n    <\/tr>\n    <tr>\n        <td> **Coste tras la iteraci\u00f3n 2400**<\/td>\n        <td> 0.092878 <\/td>\n    <\/tr>\n<\/table>","2917b603":"Ejecute la siguente celda para aprender los par\u00e1metros. Aseg\u00farese que su modelo corre bien: el coste deber\u00eda decrecer. Puede tomar algunos minutos el ejecutar 2500 iteraciones. Revise que el \"Coste tras la iteraci\u00f3n 0\" coincida con la salida esperada abajo, si no, haga click en el cuadrado de STOP (\u2b1b) arriba en la barra del cuaderno para detener el proceso e intente encontrar su error.","2448e55c":"## 3 - Arquitectura del modelo","ea8c510f":"## 2 - Conjunto de datos\n\nVamos a usar el mismo conjunto de datos del Taller 2 \"Regresi\u00f3nLog\u00edstica\". El modelo que construy\u00f3 ten\u00eda un 70% de precisi\u00f3n de prueba (test) al clasificar imagenes de gatos y todo aquello que no era gato (no-gato). Ahora vamos a implementar un modelo de redes profundas, y esperamos que su desempe\u00f1o sea mejor!\n\n**Problema**: Dado el conjunto de datos (\"data.h5\") con:\n    - un conjunto de datos de entrenamiento (CE) con imagenes etiquetadas como gato (1) y no-gato(0)\n    - un conjunto de prueba (test) con imagenes etiquetadas como gato y no-gato\n    - cada imagen es de tama\u00f1o (num_px, num_px, 3) donde 3 es para los canales (RGB).\n\nA continuaci\u00f3n exploramos el conjunto de datos. Cargue los datos ejecutando la celda abajo.","818aec95":"Ahora es el momento de construir una red neuronal profunda para la clasificaci\u00f3n de imagenes.\n\nVa a construir dos modelos distintos:\n- Una red neuronal de dos capas\n- Una red neuronal profunda de L capas\n\nLuego va a comparar el desempe\u00f1o de los modelos, y va a intentar con distintos valores para $L$. \n\nVeamos las dos arquitecturas.\n\n### 3.1 - Red neuronal de 2 capas\n\n- El input es una imagen de dimensiones (64,64,3) que es aplanada a un vector de tama\u00f1o $(12288,1)$. \n- El vector correspondiente: $[x_0,x_1,...,x_{12287}]^T$ es multiplicado por una matriz de pesos $W^{[1]}$ de tama\u00f1o $(n^{[1]}, 12288)$.\n- Le a\u00f1ade el intercepto o t\u00e9rmino de sesgo, y le aplica ReLU para conseguir el vector: $[a_0^{[1]}, a_1^{[1]},..., a_{n^{[1]}-1}^{[1]}]^T$.\n- Repite el mismo proceso.\n- Multiplica el vector resultante por $W^{[2]}$ y le a\u00f1ade el sesgo. \n- Finalmente, toma el sigmoide del resultado. Si es mayor que 0.5, clasifica como de clase 1.\n\n### 3.2 - Red neuronal profunda de L capas\n\n- El input es una imagen de dimensiones (64,64,3) que es aplanada a un vector de tama\u00f1o $(12288,1)$. \n- El vector correspondiente: $[x_0,x_1,...,x_{12287}]^T$ es multiplicado por una matriz de pesos $W^{[1]}$, y le a\u00f1ade el sesgo $b^{[1]}$. Al resultado se le llama \"unidad lineal\".\n- Luego, toma la ReLU para esta unidad. Este proceso se repite tantas veces como sea necesario para cada $(W^{[l]}, b^{[l]})$, dependiendo de la arquitectura del modelo.\n- Finalmente, toma el sigmoide de la \u00faltima unidad lineal. Si es mayor que 0.5, clasifica como de clase 1.\n\n### 3.3 - Metodolog\u00eda general\n\nComo es usual, seguir\u00e1 una metodolog\u00eda de aprendizaje profundo para consturir el modelo:\n    1. Inicializaci\u00f3n de par\u00e1metros y definici\u00f3n de hiper-par\u00e1metros\n    2. Bucle para el total de iteraciones (num_iterations):\n        a. Propagaci\u00f3n hacia delante\n        b. C\u00e1lculo de la funci\u00f3n de coste\n        c. Retro-propagaci\u00f3n\n        d. Actualizaci\u00f3n de par\u00e1metros \n    4. En base a los par\u00e1metros aprendidos, predecir las etiquetas de clases\n","6cf52eb3":"**Salida esperada para m_train, m_test y tama\u00f1o de la imagen**: \n<table style=\"width:25%\">\n  <tr>\n    <td>**m_train**<\/td>\n    <td> 209 <\/td> \n  <\/tr>\n  \n  <tr>\n    <td>**m_test**<\/td>\n    <td> 50 <\/td> \n  <\/tr>\n  \n  <tr>\n    <td>**num_px**<\/td>\n    <td> (64, 64, 3) <\/td> \n  <\/tr>\n  \n<\/table>\n","558e4b64":"$12,288$ equivale a $64 \\times 64 \\times 3$, el cual es el tama\u00f1o de un vector de imagen transformado."}}