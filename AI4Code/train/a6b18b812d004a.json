{"cell_type":{"e9217692":"code","03bdf800":"code","d1f14171":"code","5152194a":"code","ca80dabc":"code","54e27afa":"code","da5532b4":"code","f1f7d5cc":"code","a7652bbb":"code","908241d6":"code","0b8a1c03":"code","47ad254d":"code","7f1a112c":"code","27a38fb9":"code","5b1cc9e9":"code","1ebe5920":"code","b8cdcf8b":"code","efe50105":"code","32f943e0":"code","25434068":"code","ef832106":"markdown","ecd63c72":"markdown","04db27cf":"markdown","1706bc15":"markdown","3e4432a5":"markdown","60fd045a":"markdown","bc368429":"markdown","9ecb9bae":"markdown","795e2099":"markdown"},"source":{"e9217692":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.metrics import mean_squared_error\n\nimport xgboost as xgb\n\nimport os\nprint(os.listdir(\"..\/input\"))","03bdf800":"df = pd.read_csv('..\/input\/train.csv', parse_dates=True, encoding='UTF-8')\n# Change the nominal variables' dtype to categorical\ndf.vendor_id = df.vendor_id.astype('category')\ndf.store_and_fwd_flag = df.store_and_fwd_flag.astype('category')","d1f14171":"X_submit = pd.read_csv('..\/input\/test.csv', parse_dates=True, encoding='UTF-8')\nX_submit.vendor_id = X_submit.vendor_id.astype('category')\nX_submit.store_and_fwd_flag = X_submit.store_and_fwd_flag.astype('category')","5152194a":"vals = df.sample(10000)['trip_duration']  # sample to speed up the processing a bit\n\nfig, ax = plt.subplots()\nsns.distplot(vals, ax=ax)\nax.set(xlabel=\"Trip Duration\",  title='Distribution of Trip Duration')\nax.legend()\nplt.show()","ca80dabc":"df['log_trip_duration'] = np.log(df['trip_duration'].values + 1)","54e27afa":"vals = df.sample(10000)['log_trip_duration']  # sample to speed up the processing a bit\n\nfig, ax = plt.subplots()\nsns.distplot(vals, ax=ax)\nax.set(xlabel=\"Log Trip Duration\", xlim=[0,15], title='Distribution of Log Trip Duration')\nax.axvline(x=np.median(vals), color='m', label='Median', linestyle='--', linewidth=2)\nax.axvline(x=np.mean(vals), color='b', label='Mean', linestyle='--', linewidth=2)\nax.legend()\nplt.show()","da5532b4":"features_to_keep = ['passenger_count','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','vendor_id', 'store_and_fwd_flag']\nX, y = df[features_to_keep], df.log_trip_duration","f1f7d5cc":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=38)","a7652bbb":"# Class to select Dataframe columns based on dtype\nclass TypeSelector(BaseEstimator, TransformerMixin):\n    '''\n    Returns a dataframe while keeping only the columns of the specified dtype\n    '''\n    def __init__(self, dtype):\n        self.dtype = dtype\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        assert isinstance(X, pd.DataFrame)\n        return X.select_dtypes(include=[self.dtype])","908241d6":"# Class to convert a categorical column into numeric values\nclass StringIndexer(BaseEstimator, TransformerMixin):\n    '''\n    Returns a dataframe with the categorical column values replaced with the codes\n    Replaces missing value code -1 with a positive integer which is required by OneHotEncoder\n    '''\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        assert isinstance(X, pd.DataFrame)\n        return X.apply(lambda s: s.cat.codes.replace(\n            {-1: len(s.cat.categories)}\n        ))","0b8a1c03":"'''\npipeline = Pipeline([\n    ('features', FeatureUnion(n_jobs=1, transformer_list=[\n        # Part 1\n        ('boolean', Pipeline([\n            ('selector', TypeSelector('bool')),\n        ])),  # booleans close\n        \n        ('numericals', Pipeline([\n            ('selector', TypeSelector(np.number)),\n            ('scaler', StandardScaler()),\n        ])),  # numericals close\n        \n        # Part 2\n        ('categoricals', Pipeline([\n            ('selector', TypeSelector('category')),\n            ('labeler', StringIndexer()),\n            ('encoder', OneHotEncoder(handle_unknown='ignore')),\n        ]))  # categoricals close\n    ])),  # features close\n    (\"clf\", xgb.XGBRegressor(objective=\"reg:linear\", booster=\"gbtree\", nthread=4))\n])  # pipeline close\n'''","47ad254d":"'''\n# 'clf__learning_rate': np.arange(0.05, 1.0, 0.05),\n# 'clf__n_estimators': np.arange(50, 200, 50)\nparam_grid = {\n    'clf__max_depth': np.arange(3, 10, 1)\n}\n'''","7f1a112c":"'''\nrandomized_mse = RandomizedSearchCV(param_distributions=param_grid, estimator=pipeline, n_iter=2, scoring=\"neg_mean_squared_error\", verbose=1, cv=3)\n\n# Fit the estimator\nrandomized_mse.fit(X_train, y_train)\nprint(randomized_mse.best_score_)\nprint(randomized_mse.best_estimator_)\n'''","27a38fb9":"'''\npreds_test = randomized_mse.best_estimator_.predict(X_test)\nmean_squared_error(y_test.values, preds_test)\n'''","5b1cc9e9":"'''\npreds_submit = randomized_mse.best_estimator_.predict(X_submit)\nX_submit['trip_duration'] = np.exp(preds_submit) - 1\nX_submit[['id', 'trip_duration']].to_csv('bc_xgb_submission.csv', index=False)\n'''","1ebe5920":"from catboost import Pool, CatBoostRegressor","b8cdcf8b":"cat_features = [5,6]\n# initialize Pool\ntrain_pool = Pool(X_train, y_train, cat_features=cat_features)\ntest_pool = Pool(X_test, cat_features=cat_features) ","efe50105":"# specify the training parameters \nmodel = CatBoostRegressor(iterations=1000, loss_function='RMSE', random_seed=38, logging_level='Silent', learning_rate=0.1)\n#train the model\nmodel.fit(X_train, y_train, cat_features=cat_features)","32f943e0":"# make the prediction using the resulting model\npreds_test = model.predict(test_pool)\nmean_squared_error(y_test.values, preds_test)","25434068":"preds_submit = model.predict(X_submit[features_to_keep])\nX_submit['trip_duration'] = np.exp(preds_submit) - 1\nX_submit[['id', 'trip_duration']].to_csv('bc_catb_submission.csv', index=False)","ef832106":"### Grid Search","ecd63c72":"## XGBoost","04db27cf":"### Applying the best model on the test set","1706bc15":"### Preparing for submission","3e4432a5":"### Constructing the Pipeline\nThe Pipeline will apply different transformations depending on the dtype of the columns. Finally it will fit an XGBoost Regressor. The advantage now is that we can use this in a Grid Search.[](http:\/\/)","60fd045a":"## CatBoost","bc368429":"### Transforming the target variable\nAs trip_duration is not normally distributed, we apply a log transformation on it. ","9ecb9bae":"### Integrating Pandas and Sklearn\nBased on this [great article on Medium](https:\/\/medium.com\/bigdatarepublic\/integrating-pandas-and-scikit-learn-with-pipelines-f70eb6183696) I could use a Pandas DataFrame in a Pipeline without loosing track of the column names.","795e2099":"## Data Preparation\nImporting the data and converting nominal variables to the type *category*. This is needed later in the StringIndexer class."}}