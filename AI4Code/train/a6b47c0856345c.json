{"cell_type":{"7a345606":"code","99c33f2d":"code","b57ffe76":"code","6095fc76":"code","f7f7fb38":"code","f54dd377":"code","8e9f67c9":"code","e674aa7c":"code","673dc4e5":"code","c3b222f6":"code","d278d145":"code","ba596604":"code","1fcb17ce":"markdown","f5b26101":"markdown","5bfbe8b9":"markdown","dac210a7":"markdown","66a50906":"markdown","bc6af924":"markdown","3c530976":"markdown","1f47316c":"markdown","7133e2d9":"markdown","c751cc40":"markdown"},"source":{"7a345606":"import numpy as np\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import mean_squared_log_error\nfrom lightgbm import LGBMRegressor\nfrom math import sqrt\n\nimport warnings\nwarnings.filterwarnings('ignore')","99c33f2d":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b57ffe76":"train=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\", index_col=[0])\ntest=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\", index_col=[0])\nsample=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\", index_col=[0])","6095fc76":"#Dropping features with very high missing values\ntrain_clean=train.drop(columns=['MiscFeature','Fence','PoolQC','FireplaceQu','Alley'])\n\n#Seperating features and Label\nX=train_clean.drop(columns=['SalePrice'])\ny=train_clean[['SalePrice']]","f7f7fb38":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test=train_test_split(X,y, test_size=0.2, random_state=42)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","f54dd377":"#Seperate numerical and categorical features\nnum_feat=X_train.select_dtypes(include='number').columns.to_list()\ncat_feat=X_train.select_dtypes(exclude='number').columns.to_list()\n\n#Pipeline to handle numerical features\nnum_pipe=Pipeline([\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\n#Pipeline to handle categorical features\ncat_pipe=Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n])\n\n#Transforming numerical and categorical data using pipeline\nct=ColumnTransformer(remainder='drop',\n                    transformers=[\n                        ('numerical', num_pipe, num_feat),\n                        ('categorical', cat_pipe, cat_feat)\n                    ])\n#Building the model\nmodel=Pipeline([\n    ('transformer', ct),   \n    ('predictor', LGBMRegressor())\n])","8e9f67c9":"model.fit(X_train, y_train);","e674aa7c":"y_pred_train=model.predict(X_train)\ny_pred_test=model.predict(X_test)","673dc4e5":"print('In sample error: ', sqrt(mean_squared_log_error(y_pred_train, y_train)))\nprint('Out sample error: ', sqrt(mean_squared_log_error(y_pred_test, y_test)))","c3b222f6":"#model.fit(X,y);","d278d145":"def submission(test, model):\n    y_pred=model.predict(test)\n    result=pd.DataFrame(y_pred, index=test.index, columns=['SalePrice'])\n    result.to_csv('\/kaggle\/working\/result.csv')","ba596604":"#submission(test, model)","1fcb17ce":"Buiding Pipeline","f5b26101":"**Pipelines are very effective for building predictive models because you need not to worry about all the steps inside and do not need to change all the inside codes, you just change the inputs & pipe will do the rest. It makes the mocel cleaner in terms of code and also do not need to do all the processing steps over and over again for train, validation and test data. Have Fun!.**","5bfbe8b9":"Split the training dataset to understand in and out sample performance.","dac210a7":"Loading Input files from Kaggle","66a50906":"Standard Libraries","bc6af924":"Custom Build Submission Function","3c530976":"Reading Inputs","1f47316c":"**Please upvote if you like or find this notebook useful, thanks.**","7133e2d9":"# Let's Build a Standard Pipeline for Regression","c751cc40":"Feature Engineering"}}