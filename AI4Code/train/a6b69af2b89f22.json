{"cell_type":{"52f9157a":"code","6fd3c495":"code","bd9dc384":"code","c1e26a39":"code","8306d675":"code","dd0dfc7f":"code","182f96d3":"code","ec3b5d3c":"code","32cc4b83":"code","1ab53fa8":"code","52f83750":"code","47dd3645":"code","8777dbc7":"code","84c34ef4":"code","f50dfd89":"code","58342d87":"code","db5ef18e":"code","00a7186a":"code","f4f5583b":"code","497ca835":"code","fa03a747":"code","b36ff22f":"code","e765c9ce":"code","558e9cbc":"code","6130ba23":"code","562fba21":"code","20d3535e":"code","7fbb0b08":"code","f7ef0753":"code","0ee81b64":"code","9359d805":"code","99ef5b3d":"code","d721a7b0":"code","5fe98060":"code","966f16e2":"code","08b6e3ff":"code","cb10e03b":"code","8a49ed6a":"code","a2367bb8":"code","18204ebb":"code","cb11bd90":"markdown","4fdd69dc":"markdown","e96eb65d":"markdown","4289bc36":"markdown","42eddb94":"markdown","dbddd3e3":"markdown","14112ef9":"markdown","8f404c99":"markdown","a9f033e2":"markdown","52383183":"markdown","0739899c":"markdown","c6c24c0c":"markdown","28d3298d":"markdown","1e96527b":"markdown","7fb2c44c":"markdown","f18da772":"markdown"},"source":{"52f9157a":"import warnings\nwarnings.filterwarnings('ignore')","6fd3c495":"import pandas as pd\ndf=pd.read_csv('..\/input\/diabetes\/diabetes.csv')\ndf.head()","bd9dc384":"import numpy as np\ndf['Glucose']=np.where(df['Glucose']==0,df['Glucose'].median(),df['Glucose'])\ndf.head()","c1e26a39":"#### Independent And Dependent features\nX=df.drop('Outcome',axis=1)\ny=df['Outcome']","8306d675":"pd.DataFrame(X,columns=df.columns[:-1])","dd0dfc7f":"#### Train Test Split\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=0)","182f96d3":"from sklearn.ensemble import RandomForestClassifier\nrf_classifier=RandomForestClassifier(n_estimators=10).fit(X_train,y_train)\nprediction=rf_classifier.predict(X_test)","ec3b5d3c":"y.value_counts()","32cc4b83":"from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\nprint(confusion_matrix(y_test,prediction))\nprint(accuracy_score(y_test,prediction))\nprint(classification_report(y_test,prediction))","1ab53fa8":"### Manual Hyperparameter Tuning\nmodel=RandomForestClassifier(n_estimators=300,criterion='entropy',\n                             max_features='sqrt',min_samples_leaf=10,random_state=100).fit(X_train,y_train)\npredictions=model.predict(X_test)\nprint(confusion_matrix(y_test,predictions))\nprint(accuracy_score(y_test,predictions))\nprint(classification_report(y_test,predictions))","52f83750":"import numpy as np\nfrom sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt','log2']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 1000,10)]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10,14]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4,6,8]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n              'criterion':['entropy','gini']}\nprint(random_grid)","47dd3645":"rf=RandomForestClassifier()\nrf_randomcv=RandomizedSearchCV(estimator=rf,param_distributions=random_grid,n_iter=100,cv=3,verbose=2,\n                               random_state=100,n_jobs=-1)\n### fit the randomized model\nrf_randomcv.fit(X_train,y_train)","8777dbc7":"rf_randomcv.best_params_","84c34ef4":"rf_randomcv","f50dfd89":"best_random_grid=rf_randomcv.best_estimator_","58342d87":"from sklearn.metrics import accuracy_score\ny_pred=best_random_grid.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Accuracy Score {}\".format(accuracy_score(y_test,y_pred)))\nprint(\"Classification report: {}\".format(classification_report(y_test,y_pred)))","db5ef18e":"rf_randomcv.best_params_","00a7186a":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'criterion': [rf_randomcv.best_params_['criterion']],\n    'max_depth': [rf_randomcv.best_params_['max_depth']],\n    'max_features': [rf_randomcv.best_params_['max_features']],\n    'min_samples_leaf': [rf_randomcv.best_params_['min_samples_leaf'], \n                         rf_randomcv.best_params_['min_samples_leaf']+2, \n                         rf_randomcv.best_params_['min_samples_leaf'] + 4],\n    'min_samples_split': [rf_randomcv.best_params_['min_samples_split'] - 2,\n                          rf_randomcv.best_params_['min_samples_split'] - 1,\n                          rf_randomcv.best_params_['min_samples_split'], \n                          rf_randomcv.best_params_['min_samples_split'] +1,\n                          rf_randomcv.best_params_['min_samples_split'] + 2],\n    'n_estimators': [rf_randomcv.best_params_['n_estimators'] - 200, rf_randomcv.best_params_['n_estimators'] - 100, \n                     rf_randomcv.best_params_['n_estimators'], \n                     rf_randomcv.best_params_['n_estimators'] + 100, rf_randomcv.best_params_['n_estimators'] + 200]\n}\n\nprint(param_grid)","f4f5583b":"#### Fit the grid_search to the data\nrf=RandomForestClassifier()\ngrid_search=GridSearchCV(estimator=rf,param_grid=param_grid,cv=10,n_jobs=-1,verbose=2)\ngrid_search.fit(X_train,y_train)\n","497ca835":"grid_search.best_estimator_","fa03a747":"best_grid=grid_search.best_estimator_","b36ff22f":"best_grid","e765c9ce":"y_pred=best_grid.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Accuracy Score {}\".format(accuracy_score(y_test,y_pred)))\nprint(\"Classification report: {}\".format(classification_report(y_test,y_pred)))","558e9cbc":"from hyperopt import hp,fmin,tpe,STATUS_OK,Trials","6130ba23":"space = {'criterion': hp.choice('criterion', ['entropy', 'gini']),\n        'max_depth': hp.quniform('max_depth', 10, 1200, 10),\n        'max_features': hp.choice('max_features', ['auto', 'sqrt','log2', None]),\n        'min_samples_leaf': hp.uniform('min_samples_leaf', 0, 0.5),\n        'min_samples_split' : hp.uniform ('min_samples_split', 0, 1),\n        'n_estimators' : hp.choice('n_estimators', [10, 50, 300, 750, 1200,1300,1500])\n    }","562fba21":"space","20d3535e":"\ndef objective(space):\n    model = RandomForestClassifier(criterion = space['criterion'], max_depth = space['max_depth'],\n                                 max_features = space['max_features'],\n                                 min_samples_leaf = space['min_samples_leaf'],\n                                 min_samples_split = space['min_samples_split'],\n                                 n_estimators = space['n_estimators'], \n                                 )\n    \n    accuracy = cross_val_score(model, X_train, y_train, cv = 5).mean()\n\n    # We aim to maximize accuracy, therefore we return it as a negative value\n    return {'loss': -accuracy, 'status': STATUS_OK }","7fbb0b08":"from sklearn.model_selection import cross_val_score\ntrials = Trials()\nbest = fmin(fn= objective,\n            space= space,\n            algo= tpe.suggest,\n            max_evals = 80,\n            trials= trials)\nbest","f7ef0753":"crit = {0: 'entropy', 1: 'gini'}\nfeat = {0: 'auto', 1: 'sqrt', 2: 'log2', 3: None}\nest = {0: 10, 1: 50, 2: 300, 3: 750, 4: 1200,5:1300,6:1500}\n\n\nprint(crit[best['criterion']])\nprint(feat[best['max_features']])\nprint(est[best['n_estimators']])","0ee81b64":"best['min_samples_leaf']","9359d805":"trainedforest = RandomForestClassifier(criterion = crit[best['criterion']], max_depth = best['max_depth'], \n                                       max_features = feat[best['max_features']], \n                                       min_samples_leaf = best['min_samples_leaf'], \n                                       min_samples_split = best['min_samples_split'], \n                                       n_estimators = est[best['n_estimators']]).fit(X_train,y_train)\npredictionforest = trainedforest.predict(X_test)\nprint(confusion_matrix(y_test,predictionforest))\nprint(accuracy_score(y_test,predictionforest))\nprint(classification_report(y_test,predictionforest))\nacc5 = accuracy_score(y_test,predictionforest)","99ef5b3d":"import numpy as np\nfrom sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt','log2']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 1000,10)]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10,14]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4,6,8]\n# Create the random grid\nparam = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n              'criterion':['entropy','gini']}\nprint(param)","d721a7b0":"\nfrom tpot import TPOTClassifier\n\n\ntpot_classifier = TPOTClassifier(generations= 5, population_size= 24, offspring_size= 12,\n                                 verbosity= 2, early_stop= 12,\n                                 config_dict={'sklearn.ensemble.RandomForestClassifier': param}, \n                                 cv = 4, scoring = 'accuracy')\ntpot_classifier.fit(X_train,y_train)","5fe98060":"\naccuracy = tpot_classifier.score(X_test, y_test)\nprint(accuracy)","966f16e2":"import optuna\nimport sklearn.svm\ndef objective(trial):\n\n    classifier = trial.suggest_categorical('classifier', ['RandomForest', 'SVC'])\n    \n    if classifier == 'RandomForest':\n        n_estimators = trial.suggest_int('n_estimators', 200, 2000,10)\n        max_depth = int(trial.suggest_float('max_depth', 10, 100, log=True))\n\n        clf = sklearn.ensemble.RandomForestClassifier(\n            n_estimators=n_estimators, max_depth=max_depth)\n    else:\n        c = trial.suggest_float('svc_c', 1e-10, 1e10, log=True)\n        \n        clf = sklearn.svm.SVC(C=c, gamma='auto')\n\n    return sklearn.model_selection.cross_val_score(\n        clf,X_train,y_train, n_jobs=-1, cv=3).mean()\n","08b6e3ff":"study = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)\n\ntrial = study.best_trial\n\nprint('Accuracy: {}'.format(trial.value))\nprint(\"Best hyperparameters: {}\".format(trial.params))","cb10e03b":"trial","8a49ed6a":"study.best_params","a2367bb8":"rf=RandomForestClassifier(n_estimators=330,max_depth=30)\nrf.fit(X_train,y_train)","18204ebb":"y_pred=rf.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(accuracy_score(y_test,y_pred))\nprint(classification_report(y_test,y_pred))","cb11bd90":"## So now we can compare the results of all the tuning methods and choose the one you need\n","4fdd69dc":"<center style=\"font-family:cursive; font-size:30px; color:#159364;\">Do UpVote the kernel if it was helpful.<\/center>\n","e96eb65d":"\n<h1 id=\"basics\" style=\"font-family:verdana;\"> \n    <center>3. Automated Hyperparameter Tuning \ud83c\udf96<\/center>\n    <\/h1>\n\n### -> Automated Hyperparameter Tuning can be done by using techniques such as \n###    1.Bayesian Optimization\n###    2.Gradient Descent\n###    3.Evolutionary Algorithms\n       \n","4289bc36":"### Do check out- [@KRISHNAIK](https:\/\/www.youtube.com\/user\/krishnaik06) Youtube channel -https:\/\/www.youtube.com\/user\/krishnaik06","42eddb94":"## Bayesian Optimization\nBayesian optimization uses probability to find the minimum of a function. The final aim is to find the input value to a function which can gives us the lowest possible output value.It usually performs better than random,grid and manual search providing better performance in the testing phase and reduced optimization time.\nIn Hyperopt, Bayesian Optimization can be implemented giving 3 three main parameters to the function fmin.\n\n- Objective Function = defines the loss function to minimize.\n- Domain Space = defines the range of input values to test (in Bayesian Optimization this space creates a probability distribution for each of the used Hyperparameters).\n- Optimization Algorithm = defines the search algorithm to use to select the best input values to use in each new iteration.","dbddd3e3":"## All Techniques Of Hyper Parameter Optimization\n\n### 1. GridSearchCV\n### 2. RandomizedSearchCV\n### 3. Bayesian Optimization -Automate Hyperparameter Tuning (Hyperopt)\n### 4. Sequential Model Based Optimization(Tuning a scikit-learn estimator with skopt)\n### 4. Optuna- Automate Hyperparameter Tuning\n### 5. Genetic Algorithms (TPOT Classifier)\n\n## References\n- https:\/\/github.com\/fmfn\/BayesianOptimization\n- https:\/\/github.com\/hyperopt\/hyperopt\n- https:\/\/www.jeremyjordan.me\/hyperparameter-tuning\/\n- https:\/\/optuna.org\/\n- https:\/\/towardsdatascience.com\/hyperparameters-optimization-526348bb8e2d(By Pier Paolo Ippolito )\n- https:\/\/scikit-optimize.github.io\/stable\/auto_examples\/hyperparameter-optimization.html\n","14112ef9":"\n<h1 id=\"basics\" style=\"font-family:verdana;\"> \n    <center>4. Genetic Algorithms \ud83c\udf96<\/center>\n    <\/h1>\n\n## Genetic Algorithms tries to apply natural selection mechanisms to Machine Learning contexts.\n\n- Let's immagine we create a population of N Machine Learning models with some predifined Hyperparameters. We can then calculate the accuracy of each model   and decide to keep just half of the models (the ones that performs best). We can now generate some offsprings having similar Hyperparameters to the ones of the best models so that go get again a population of N models. At this point we can again caltulate the accuracy of each model and repeate the cycle for a        defined number of generations. In this way, just the best models will survive at the end of the process.","8f404c99":"![](https:\/\/analyticsindiamag.com\/wp-content\/uploads\/2020\/08\/2020-08-11-1.png)","a9f033e2":"## \u2753 Why are Hyperparameters essential?\n<h1 style=\"font-family:verdana; font-size:20px; \">Hyperparameters are important because they directly control the behaviour of the training algorithm and have a significant impact on the performance of the model is being trained.\n    <\/h>","52383183":"<h1 id=\"basics\" style=\"font-family:verdana;\"> \n    <center>2. GridSearch CV \ud83c\udf96\n","0739899c":"<h1 style=\"font-family:verdana; font-size:40px; \"> <center>\ud83d\udcda All-Hyperparamter-Optimization \ud83d\udcda<\/center> <\/h1>\n<p><center style=\"color:#159364; font-size:30px; font-family:cursive;\">\u201cA good choice of hyperparameters can really make an algorithm shine\u201d.<\/center><\/p>\n\n***","c6c24c0c":"\n\n<h1 id=\"basics\" style=\"font-family:verdana;\"> \n    <center>5. Optimize hyperparameters of the model using Optuna \ud83c\udf96<\/center>\n    <\/h1>\n\n\n## The hyperparameters of the above algorithm are `n_estimators` and `max_depth` for which we can try different values to see if the model accuracy can be improved. The `objective` function is modified to accept a trial object. This trial has several methods for sampling hyperparameters. We create a study to run the hyperparameter optimization and finally read the best hyperparameters.","28d3298d":"## -> The main parameters used by a Random Forest Classifier are:\n\n### criterion = the function used to evaluate the quality of a split.\n### max_depth = maximum number of levels allowed in each tree.\n### max_features = maximum number of features considered when splitting a node.\n### min_samples_leaf = minimum number of samples which can be stored in a tree leaf.\n### min_samples_split = minimum number of samples necessary in a node to cause node splitting.\n### n_estimators = number of trees in the ensamble.","1e96527b":"\n<h1 id=\"basics\" style=\"font-family:verdana;\"> \n    <center>1. Randomized Search Cv \ud83c\udf96\n","7fb2c44c":"<h1 id=\"markup\" style=\"font-family:verdana; font-size:30px;  \"> \n    <center> THANK YOU SO MUCH\n        \n<\/h1>","f18da772":"<h1 style=\"font-family:verdana; font-size:30px; \"> A model hyperparameter is a configuration that is external to the model and whose value cannot be estimated from data.\nThey are often used in processes to help estimate model parameters.\nThey are often specified by the practitioner.\nThey can often be set using heuristics. <\/h1>\n"}}