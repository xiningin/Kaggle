{"cell_type":{"4376a6ec":"code","d822426a":"code","c1f31d90":"code","79ccd223":"code","c47ee7f3":"code","0f46ed6d":"code","20a95b66":"code","5faa154a":"code","10f86ec1":"code","0571afb4":"code","7c7659e9":"code","b55537c7":"code","cbd2c8c8":"code","de1b68f2":"code","b6cdf39f":"code","b8892291":"code","8dc8f9e8":"code","57d7d39e":"code","0647069c":"code","bb0fd2a5":"code","86111660":"code","475fd1bb":"code","67efdc6c":"code","57e22f20":"code","068138fe":"code","cff31c87":"code","2a9f70da":"code","6f2e53c2":"code","6b3b14ea":"code","5c83ccfe":"code","9a289553":"code","44ee211d":"code","05076a82":"code","d298806a":"code","cb418b05":"code","30957b6f":"code","dbdaf2a6":"code","7a1a2030":"code","8e968ca4":"code","602952a1":"code","0db17a20":"code","479f409f":"code","919ecaf4":"code","ef38768e":"code","b6ff5051":"code","adcf08a0":"code","9b22fa5f":"code","5b2cf586":"code","e479d920":"code","d6f70d83":"code","3c9ce6e4":"code","5798dab4":"code","de33b673":"markdown","35a7d6cd":"markdown","3d59ed4f":"markdown","87baf7da":"markdown","b5041313":"markdown","3779bbe3":"markdown","6ee0db94":"markdown","bfe375c2":"markdown","e00dc49d":"markdown","bcaabff1":"markdown","d1ea0ab7":"markdown","f8afd79f":"markdown","3cba6abe":"markdown","168aaa95":"markdown","92964f19":"markdown","de852b2b":"markdown","d84b25c6":"markdown","ece541db":"markdown","9072c8f0":"markdown","99de136e":"markdown","403abf4e":"markdown","06bc2b78":"markdown","86921b9e":"markdown","98d8f7ac":"markdown","49640e86":"markdown","6962c18d":"markdown","56f1a60b":"markdown","41946550":"markdown","1140f1ac":"markdown","3e82b0c0":"markdown","e2da995c":"markdown","23d16ed8":"markdown","e3bdc05a":"markdown","d763ca99":"markdown","21ba411e":"markdown","62a5df08":"markdown","6307f33e":"markdown","6df536f5":"markdown","25e09a48":"markdown","a39292f4":"markdown","cb2a74b0":"markdown","8c820646":"markdown","f68e9afa":"markdown","11af658d":"markdown","0c549b44":"markdown","b0b481a5":"markdown","5c953287":"markdown","12264ab9":"markdown","043e04c3":"markdown","baca34ca":"markdown","5f6e60ab":"markdown","49650715":"markdown","f22fa978":"markdown"},"source":{"4376a6ec":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.utils.data import DataLoader, sampler, random_split\n\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport torchvision.models as models\n\nfrom sklearn import decomposition\nfrom sklearn import manifold\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport copy\nfrom collections import namedtuple\nimport os\nimport random\nimport shutil\nimport time","d822426a":"SEED = 1234\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","c1f31d90":"!tar -xvzf \/kaggle\/input\/200-bird-species-with-11788-images\/CUB_200_2011.tgz","79ccd223":"def get_data_loaders(data_dir, batch_size):\n  transform = transforms.Compose([transforms.Resize(255),\n                                transforms.CenterCrop(224),\n                                transforms.ToTensor()])\n  all_data = datasets.ImageFolder(data_dir, transform=transform)\n  train_data_len = int(len(all_data)*0.75)\n  valid_data_len = int((len(all_data) - train_data_len)\/2)\n  test_data_len = int(len(all_data) - train_data_len - valid_data_len)\n  train_data, val_data, test_data = random_split(all_data, [train_data_len, valid_data_len, test_data_len])\n  train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n  val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n  test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n  return ((train_loader, val_loader, test_loader),train_data, val_data, test_data, all_data.classes)","c47ee7f3":"(train_loader, val_loader, test_loader),train_data, val_data, test_data, classes = get_data_loaders(\"CUB_200_2011\/images\/\", 64)","0f46ed6d":"print(len(train_loader))\nprint(len(val_loader))\nprint(len(test_loader))","20a95b66":"def normalize_image(image):\n    image_min = image.min()\n    image_max = image.max()\n    image.clamp_(min = image_min, max = image_max)\n    image.add_(-image_min).div_(image_max - image_min + 1e-5)\n    return image","5faa154a":"def plot_images(images, labels, normalize = True):\n\n    n_images = len(images)\n\n    rows = int(np.sqrt(n_images))\n    cols = int(np.sqrt(n_images))\n\n    fig = plt.figure(figsize = (15, 15))\n\n    for i in range(rows*cols):\n\n        ax = fig.add_subplot(rows, cols, i+1)\n        \n        image = images[i]\n\n        if normalize:\n            image = normalize_image(image)\n\n        ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n        label = labels[i]\n        ax.set_title(label)\n        ax.axis('off')","10f86ec1":"N_IMAGES = 25\n\nimages, labels = zip(*[(image, label) for image, label in \n                           [train_data[i] for i in range(N_IMAGES)]])\nplot_images(images, labels)","0571afb4":"class ResNet(nn.Module):\n    def __init__(self, config, output_dim):\n        super().__init__()\n                \n        block, n_blocks, channels = config\n        self.in_channels = channels[0]\n            \n        assert len(n_blocks) == len(channels) == 4\n        \n        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size = 7, stride = 2, padding = 3, bias = False)\n        self.bn1 = nn.BatchNorm2d(self.in_channels)\n        self.relu = nn.ReLU(inplace = True)\n        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n        \n        self.layer1 = self.get_resnet_layer(block, n_blocks[0], channels[0])\n        self.layer2 = self.get_resnet_layer(block, n_blocks[1], channels[1], stride = 2)\n        self.layer3 = self.get_resnet_layer(block, n_blocks[2], channels[2], stride = 2)\n        self.layer4 = self.get_resnet_layer(block, n_blocks[3], channels[3], stride = 2)\n        \n        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n        self.fc = nn.Linear(self.in_channels, output_dim)\n        \n    def get_resnet_layer(self, block, n_blocks, channels, stride = 1):\n    \n        layers = []\n        \n        if self.in_channels != block.expansion * channels:\n            downsample = True\n        else:\n            downsample = False\n        \n        layers.append(block(self.in_channels, channels, stride, downsample))\n        \n        for i in range(1, n_blocks):\n            layers.append(block(block.expansion * channels, channels))\n\n        self.in_channels = block.expansion * channels\n            \n        return nn.Sequential(*layers)\n        \n    def forward(self, x):\n        \n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        \n        x = self.avgpool(x)\n        h = x.view(x.shape[0], -1)\n        x = self.fc(h)\n        \n        return x, h","7c7659e9":"class BasicBlock(nn.Module):\n    \n    expansion = 1\n    \n    def __init__(self, in_channels, out_channels, stride = 1, downsample = False):\n        super().__init__()\n                \n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 3, \n                               stride = stride, padding = 1, bias = False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        \n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, \n                               stride = 1, padding = 1, bias = False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        \n        self.relu = nn.ReLU(inplace = True)\n        \n        if downsample:\n            conv = nn.Conv2d(in_channels, out_channels, kernel_size = 1, \n                             stride = stride, bias = False)\n            bn = nn.BatchNorm2d(out_channels)\n            downsample = nn.Sequential(conv, bn)\n        else:\n            downsample = None\n        \n        self.downsample = downsample\n        \n    def forward(self, x):\n        \n        i = x\n        \n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        \n        x = self.conv2(x)\n        x = self.bn2(x)\n        \n        if self.downsample is not None:\n            i = self.downsample(i)\n                        \n        x += i\n        x = self.relu(x)\n        \n        return x","b55537c7":"ResNetConfig = namedtuple('ResNetConfig', ['block', 'n_blocks', 'channels'])","cbd2c8c8":"resnet18_config = ResNetConfig(block = BasicBlock,\n                               n_blocks = [2,2,2,2],\n                               channels = [64, 128, 256, 512])\n\nresnet34_config = ResNetConfig(block = BasicBlock,\n                               n_blocks = [3,4,6,3],\n                               channels = [64, 128, 256, 512])","de1b68f2":"class Bottleneck(nn.Module):\n    \n    expansion = 4\n    \n    def __init__(self, in_channels, out_channels, stride = 1, downsample = False):\n        super().__init__()\n    \n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 1, \n                               stride = 1, bias = False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        \n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, \n                               stride = stride, padding = 1, bias = False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        \n        self.conv3 = nn.Conv2d(out_channels, self.expansion * out_channels, kernel_size = 1,\n                               stride = 1, bias = False)\n        self.bn3 = nn.BatchNorm2d(self.expansion * out_channels)\n        \n        self.relu = nn.ReLU(inplace = True)\n        \n        if downsample:\n            conv = nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size = 1, \n                             stride = stride, bias = False)\n            bn = nn.BatchNorm2d(self.expansion * out_channels)\n            downsample = nn.Sequential(conv, bn)\n        else:\n            downsample = None\n            \n        self.downsample = downsample\n        \n    def forward(self, x):\n        \n        i = x\n        \n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        \n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        \n        x = self.conv3(x)\n        x = self.bn3(x)\n                \n        if self.downsample is not None:\n            i = self.downsample(i)\n            \n        x += i\n        x = self.relu(x)\n    \n        return x","b6cdf39f":"\nresnet50_config = ResNetConfig(block = Bottleneck,\n                               n_blocks = [3, 4, 6, 3],\n                               channels = [64, 128, 256, 512])\n\nresnet101_config = ResNetConfig(block = Bottleneck,\n                                n_blocks = [3, 4, 23, 3],\n                                channels = [64, 128, 256, 512])\n\nresnet152_config = ResNetConfig(block = Bottleneck,\n                                n_blocks = [3, 8, 36, 3],\n                                channels = [64, 128, 256, 512])","b8892291":"\nclass CIFARResNet(nn.Module):\n    def __init__(self, config, output_dim):\n        super().__init__()\n                \n        block, layers, channels = config\n        self.in_channels = channels[0]\n            \n        assert len(layers) == len(channels) == 3\n        assert all([i == j*2 for i, j in zip(channels[1:], channels[:-1])])\n        \n        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size = 3, stride = 1, padding = 1, bias = False)\n        self.bn1 = nn.BatchNorm2d(self.in_channels)\n        self.relu = nn.ReLU(inplace = True)\n        \n        self.layer1 = self.get_resnet_layer(block, layers[0], channels[0])\n        self.layer2 = self.get_resnet_layer(block, layers[1], channels[1], stride = 2)\n        self.layer3 = self.get_resnet_layer(block, layers[2], channels[2], stride = 2)\n        \n        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n        self.fc = nn.Linear(self.in_channels, output_dim)\n        \n    def get_resnet_layer(self, block, n_blocks, channels, stride = 1):\n    \n        layers = []\n        \n        if self.in_channels != channels:\n            downsample = True\n        else:\n            downsample = False\n        \n        layers.append(block(self.in_channels, channels, stride, downsample))\n        \n        for i in range(1, n_blocks):\n            layers.append(block(channels, channels))\n\n        self.in_channels = channels\n            \n        return nn.Sequential(*layers)\n        \n    def forward(self, x):\n        \n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        \n        x = self.avgpool(x)\n        h = x.view(x.shape[0], -1)\n        x = self.fc(h)\n        \n        return x, h","8dc8f9e8":"class Identity(nn.Module):\n    def __init__(self, f):\n        super().__init__()\n        self.f = f\n        \n    def forward(self, x):\n        return self.f(x)\n        \n\nclass CIFARBasicBlock(nn.Module):\n        \n    def __init__(self, in_channels, out_channels, stride = 1, downsample = False):\n        super().__init__()\n                \n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 3, \n                               stride = stride, padding = 1, bias = False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        \n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, \n                               stride = 1, padding = 1, bias = False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        \n        self.relu = nn.ReLU(inplace = True)\n        \n        if downsample:\n            identity_fn = lambda x : F.pad(x[:, :, ::2, ::2], \n                                           [0, 0, 0, 0, in_channels \/\/ 2, in_channels \/\/ 2])\n            downsample = Identity(identity_fn)\n        else:\n            downsample = None\n        \n        self.downsample = downsample\n        \n    def forward(self, x):\n        \n        i = x\n        \n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        \n        x = self.conv2(x)\n        x = self.bn2(x)\n        \n        if self.downsample is not None:\n            i = self.downsample(i)\n                                \n        x += i\n        x = self.relu(x)\n        \n        return x","57d7d39e":"cifar_resnet20_config = ResNetConfig(block = CIFARBasicBlock,\n                                     n_blocks = [3, 3, 3],\n                                     channels = [16, 32, 64])\n\ncifar_resnet32_config = ResNetConfig(block = CIFARBasicBlock,\n                                     n_blocks = [5, 5, 5],\n                                     channels = [16, 32, 64])\n\ncifar_resnet44_config = ResNetConfig(block = CIFARBasicBlock,\n                                     n_blocks = [7, 7, 7],\n                                     channels = [16, 32, 64])\n\ncifar_resnet56_config = ResNetConfig(block = CIFARBasicBlock,\n                                     n_blocks = [9, 9, 9],\n                                     channels = [16, 32, 64])\n\ncifar_resnet110_config = ResNetConfig(block = CIFARBasicBlock,\n                                      n_blocks = [18, 18, 18],\n                                      channels = [16, 32, 64])\n\ncifar_resnet1202_config = ResNetConfig(block = CIFARBasicBlock,\n                                       n_blocks = [20, 20, 20],\n                                       channels = [16, 32, 64])","0647069c":"pretrained_model = models.resnet50(pretrained = True)","bb0fd2a5":"print(pretrained_model)","86111660":"IN_FEATURES = pretrained_model.fc.in_features \nOUTPUT_DIM = 200\n\nfc = nn.Linear(IN_FEATURES, OUTPUT_DIM)","475fd1bb":"pretrained_model.fc = fc","67efdc6c":"model = ResNet(resnet50_config, OUTPUT_DIM)","57e22f20":"model.load_state_dict(pretrained_model.state_dict())","068138fe":"\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')\n","cff31c87":"START_LR = 1e-7\n\noptimizer = optim.Adam(model.parameters(), lr=START_LR)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ncriterion = nn.CrossEntropyLoss()\n\nmodel = model.to(device)\ncriterion = criterion.to(device)","2a9f70da":"class LRFinder:\n    def __init__(self, model, optimizer, criterion, device):\n        \n        self.optimizer = optimizer\n        self.model = model\n        self.criterion = criterion\n        self.device = device\n        \n        torch.save(model.state_dict(), 'init_params.pt')\n\n    def range_test(self, iterator, end_lr = 10, num_iter = 100, \n                   smooth_f = 0.05, diverge_th = 5):\n        \n        lrs = []\n        losses = []\n        best_loss = float('inf')\n\n        lr_scheduler = ExponentialLR(self.optimizer, end_lr, num_iter)\n        \n        iterator = IteratorWrapper(iterator)\n        \n        for iteration in range(num_iter):\n\n            loss = self._train_batch(iterator)\n\n            #update lr\n            lr_scheduler.step()\n            \n            lrs.append(lr_scheduler.get_lr()[0])\n\n            if iteration > 0:\n                loss = smooth_f * loss + (1 - smooth_f) * losses[-1]\n                \n            if loss < best_loss:\n                best_loss = loss\n\n            losses.append(loss)\n            \n            if loss > diverge_th * best_loss:\n                print(\"Stopping early, the loss has diverged\")\n                break\n                       \n        #reset model to initial parameters\n        model.load_state_dict(torch.load('init_params.pt'))\n                    \n        return lrs, losses\n\n    def _train_batch(self, iterator):\n        \n        self.model.train()\n        \n        self.optimizer.zero_grad()\n        \n        x, y = iterator.get_batch()\n        \n        x = x.to(self.device)\n        y = y.to(self.device)\n        \n        y_pred, _ = self.model(x)\n                \n        loss = self.criterion(y_pred, y)\n        \n        loss.backward()\n        \n        self.optimizer.step()\n        \n        return loss.item()\n\nclass ExponentialLR(_LRScheduler):\n    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n        self.end_lr = end_lr\n        self.num_iter = num_iter\n        super(ExponentialLR, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        curr_iter = self.last_epoch + 1\n        r = curr_iter \/ self.num_iter\n        return [base_lr * (self.end_lr \/ base_lr) ** r for base_lr in self.base_lrs]\n\nclass IteratorWrapper:\n    def __init__(self, iterator):\n        self.iterator = iterator\n        self._iterator = iter(iterator)\n\n    def __next__(self):\n        try:\n            inputs, labels = next(self._iterator)\n        except StopIteration:\n            self._iterator = iter(self.iterator)\n            inputs, labels, *_ = next(self._iterator)\n\n        return inputs, labels\n\n    def get_batch(self):\n        return next(self)","6f2e53c2":"END_LR = 10\nNUM_ITER = 100\n\nlr_finder = LRFinder(model, optimizer, criterion, device)\nlrs, losses = lr_finder.range_test(train_loader, END_LR, NUM_ITER)","6b3b14ea":"def plot_lr_finder(lrs, losses, skip_start = 5, skip_end = 5):\n    \n    if skip_end == 0:\n        lrs = lrs[skip_start:]\n        losses = losses[skip_start:]\n    else:\n        lrs = lrs[skip_start:-skip_end]\n        losses = losses[skip_start:-skip_end]\n    \n    fig = plt.figure(figsize = (16,8))\n    ax = fig.add_subplot(1,1,1)\n    ax.plot(lrs, losses)\n    ax.set_xscale('log')\n    ax.set_xlabel('Learning rate')\n    ax.set_ylabel('Loss')\n    ax.grid(True, 'both', 'x')\n    plt.show()","5c83ccfe":"plot_lr_finder(lrs, losses, skip_start = 30, skip_end = 30)","9a289553":"FOUND_LR = 1e-3\n\nparams = [\n          {'params': model.conv1.parameters(), 'lr': FOUND_LR \/ 10},\n          {'params': model.bn1.parameters(), 'lr': FOUND_LR \/ 10},\n          {'params': model.layer1.parameters(), 'lr': FOUND_LR \/ 8},\n          {'params': model.layer2.parameters(), 'lr': FOUND_LR \/ 6},\n          {'params': model.layer3.parameters(), 'lr': FOUND_LR \/ 4},\n          {'params': model.layer4.parameters(), 'lr': FOUND_LR \/ 2},\n          {'params': model.fc.parameters()}\n         ]\n\n\noptimizer = optim.Adam(params, lr = FOUND_LR)","44ee211d":"EPOCHS = 10\nSTEPS_PER_EPOCH = len(train_loader)\nTOTAL_STEPS = EPOCHS * STEPS_PER_EPOCH\n\nMAX_LRS = [p['lr'] for p in optimizer.param_groups]\n\nscheduler = lr_scheduler.OneCycleLR(optimizer,\n                                    max_lr = MAX_LRS,\n                                    total_steps = TOTAL_STEPS)","05076a82":"def calculate_topk_accuracy(y_pred, y, k = 5):\n    with torch.no_grad():\n        batch_size = y.shape[0]\n        _, top_pred = y_pred.topk(k, 1)\n        top_pred = top_pred.t()\n        correct = top_pred.eq(y.view(1, -1).expand_as(top_pred))\n        correct_1 = correct[:1].view(-1).float().sum(0, keepdim = True)\n        correct_k = correct[:k].view(-1).float().sum(0, keepdim = True)\n        acc_1 = correct_1 \/ batch_size\n        acc_k = correct_k \/ batch_size\n    return acc_1, acc_k","d298806a":"def train(model, iterator, optimizer, criterion, scheduler, device):\n    \n    epoch_loss = 0\n    epoch_acc_1 = 0\n    epoch_acc_5 = 0\n    \n    model.train()\n    \n    for (x, y) in iterator:\n        \n        x = x.to(device)\n        y = y.to(device)\n        \n        optimizer.zero_grad()\n                \n        y_pred, _ = model(x)\n        \n        loss = criterion(y_pred, y)\n        \n        acc_1, acc_5 = calculate_topk_accuracy(y_pred, y)\n        \n        loss.backward()\n        \n        optimizer.step()\n        \n        scheduler.step()\n        \n        epoch_loss += loss.item()\n        epoch_acc_1 += acc_1.item()\n        epoch_acc_5 += acc_5.item()\n        \n    epoch_loss \/= len(iterator)\n    epoch_acc_1 \/= len(iterator)\n    epoch_acc_5 \/= len(iterator)\n        \n    return epoch_loss, epoch_acc_1, epoch_acc_5","cb418b05":"def evaluate(model, iterator, criterion, device):\n    \n    epoch_loss = 0\n    epoch_acc_1 = 0\n    epoch_acc_5 = 0\n    \n    model.eval()\n    \n    with torch.no_grad():\n        \n        for (x, y) in iterator:\n\n            x = x.to(device)\n            y = y.to(device)\n\n            y_pred, _ = model(x)\n\n            loss = criterion(y_pred, y)\n\n            acc_1, acc_5 = calculate_topk_accuracy(y_pred, y)\n\n            epoch_loss += loss.item()\n            epoch_acc_1 += acc_1.item()\n            epoch_acc_5 += acc_5.item()\n        \n    epoch_loss \/= len(iterator)\n    epoch_acc_1 \/= len(iterator)\n    epoch_acc_5 \/= len(iterator)\n        \n    return epoch_loss, epoch_acc_1, epoch_acc_5","30957b6f":"def epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time \/ 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","dbdaf2a6":"best_valid_loss = float('inf')\n\nfor epoch in range(EPOCHS):\n    \n    start_time = time.time()\n    \n    train_loss, train_acc_1, train_acc_5 = train(model, train_loader, optimizer, criterion, scheduler, device)\n    valid_loss, valid_acc_1, valid_acc_5 = evaluate(model, val_loader, criterion, device)\n        \n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'tut5-model.pt')\n\n    end_time = time.time()\n\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc @1: {train_acc_1*100:6.2f}% | ' \\\n          f'Train Acc @5: {train_acc_5*100:6.2f}%')\n    print(f'\\tValid Loss: {valid_loss:.3f} | Valid Acc @1: {valid_acc_1*100:6.2f}% | ' \\\n          f'Valid Acc @5: {valid_acc_5*100:6.2f}%')","7a1a2030":"model.load_state_dict(torch.load('tut5-model.pt'))\n\ntest_loss, test_acc_1, test_acc_5 = evaluate(model, test_loader, criterion, device)\n\nprint(f'Test Loss: {test_loss:.3f} | Test Acc @1: {test_acc_1*100:6.2f}% | ' \\\n      f'Test Acc @5: {test_acc_5*100:6.2f}%')","8e968ca4":"def get_predictions(model, iterator):\n\n    model.eval()\n\n    images = []\n    labels = []\n    probs = []\n\n    with torch.no_grad():\n\n        for (x, y) in iterator:\n\n            x = x.to(device)\n\n            y_pred, _ = model(x)\n\n            y_prob = F.softmax(y_pred, dim = -1)\n            top_pred = y_prob.argmax(1, keepdim = True)\n\n            images.append(x.cpu())\n            labels.append(y.cpu())\n            probs.append(y_prob.cpu())\n\n    images = torch.cat(images, dim = 0)\n    labels = torch.cat(labels, dim = 0)\n    probs = torch.cat(probs, dim = 0)\n\n    return images, labels, probs","602952a1":"images, labels, probs = get_predictions(model, test_loader)","0db17a20":"pred_labels = torch.argmax(probs, 1)","479f409f":"def get_representations(model, iterator):\n\n    model.eval()\n\n    outputs = []\n    intermediates = []\n    labels = []\n\n    with torch.no_grad():\n        \n        for (x, y) in iterator:\n\n            x = x.to(device)\n\n            y_pred, _ = model(x)\n\n            outputs.append(y_pred.cpu())\n            labels.append(y)\n        \n    outputs = torch.cat(outputs, dim = 0)\n    labels = torch.cat(labels, dim = 0)\n\n    return outputs, labels","919ecaf4":"outputs, labels = get_representations(model, train_loader)","ef38768e":"# We can then perform PCA on these representations to plot them in two dimensions.\n\ndef get_pca(data, n_components = 2):\n    pca = decomposition.PCA()\n    pca.n_components = n_components\n    pca_data = pca.fit_transform(data)\n    return pca_data","b6ff5051":"def plot_representations(data, labels, n_images = None):\n            \n    if n_images is not None:\n        data = data[:n_images]\n        labels = labels[:n_images]\n                \n    fig = plt.figure(figsize = (15, 15))\n    ax = fig.add_subplot(111)\n    scatter = ax.scatter(data[:, 0], data[:, 1], c = labels, cmap = 'hsv')","adcf08a0":"output_pca_data = get_pca(outputs)\nplot_representations(output_pca_data, labels)","9b22fa5f":"def get_tsne(data, n_components = 2, n_images = None):\n    \n    if n_images is not None:\n        data = data[:n_images]\n        \n    tsne = manifold.TSNE(n_components = n_components, random_state = 0)\n    tsne_data = tsne.fit_transform(data)\n    return tsne_data","5b2cf586":"output_tsne_data = get_tsne(outputs)\nplot_representations(output_tsne_data, labels)","e479d920":"def plot_filtered_images(images, filters, n_filters = None, normalize = True):\n\n    images = torch.cat([i.unsqueeze(0) for i in images], dim = 0).cpu()\n    filters = filters.cpu()\n\n    if n_filters is not None:\n        filters = filters[:n_filters]\n\n    n_images = images.shape[0]\n    n_filters = filters.shape[0]\n\n    filtered_images = F.conv2d(images, filters)\n\n    fig = plt.figure(figsize = (30, 30))\n\n    for i in range(n_images):\n\n        image = images[i]\n\n        if normalize:\n            image = normalize_image(image)\n\n        ax = fig.add_subplot(n_images, n_filters+1, i+1+(i*n_filters))\n        ax.imshow(image.permute(1,2,0).numpy())\n        ax.set_title('Original')\n        ax.axis('off')\n\n        for j in range(n_filters):\n            image = filtered_images[i][j]\n\n            if normalize:\n                image = normalize_image(image)\n\n            ax = fig.add_subplot(n_images, n_filters+1, i+1+(i*n_filters)+j+1)\n            ax.imshow(image.numpy(), cmap = 'bone')\n            ax.set_title(f'Filter {j+1}')\n            ax.axis('off');\n\n    fig.subplots_adjust(hspace = -0.7)","d6f70d83":"N_IMAGES = 5\nN_FILTERS = 7\n\nimages = [image for image, label in [train_data[i] for i in range(N_IMAGES)]]\nfilters = model.conv1.weight.data\n\nplot_filtered_images(images, filters, N_FILTERS)\n","3c9ce6e4":"# Finally, we can plot the values of the filters themselves.\n\ndef plot_filters(filters, normalize = True):\n\n    filters = filters.cpu()\n\n    n_filters = filters.shape[0]\n\n    rows = int(np.sqrt(n_filters))\n    cols = int(np.sqrt(n_filters))\n\n    fig = plt.figure(figsize = (30, 15))\n\n    for i in range(rows*cols):\n\n        image = filters[i]\n\n        if normalize:\n            image = normalize_image(image)\n\n        ax = fig.add_subplot(rows, cols, i+1)\n        ax.imshow(image.permute(1, 2, 0))\n        ax.axis('off')\n        \n    fig.subplots_adjust(wspace = -0.9)","5798dab4":"plot_filters(filters)","de33b673":"From the table above, we can see that for ResNet18 and ResNet34 that the first block contains two 3x3 convolutional layers with 64 filters, and that ResNet18 has two of these blocks in the first layer, whilst Resnet34 has three. ResNet50, ResNet101 and ResNet152 blocks have a different structure than those in ResNet18 and ResNet34, and these blocks are called bottleneck blocks. Bottleneck blocks reduce the number of number of channels within the input before expanding them back out again. Below shows a standard BasicBlock (left) - used by ResNet18 and ResNet34 - and the Bottleneck block used by ResNet50, ResNet101 and ResNet152.","35a7d6cd":"We'll be using our own dataset instead of using one provided by torchvision.datasets.\n\nFirst, we'll need to download and extract our data. This can be done outside of the notebook that runs our model on the data as it only ever needs to be run once - but we leave it here so this notebook can be run on cloud computing platforms, such as Google Colab.\n\ndatasets.utils contains some functionality for downloading and extract data which means we don't have to write it ourselves.\n\nThe url of the CUB200-2011 dataset can be found on its website. The md5 checksum is used to determine if the data was downloaded correctly - it is optional, but recommended as it can save some headaches later on in case the data becomes corrupted whilst downloading.\n\nWe use the download_and_extract_archive function, which unsurprisingly, downloads and extracts a file from a given url to the provided root folder. If an md5 hash is provided it will check the downloaded (compressed) file against it. If the compressed file already exists then it will not re-download it.","3d59ed4f":"The initial convolutional layer has smaller filter size, lower stride and less padding, and is not followed by a pooling layer. It also only has 3 layers, instead of 4, and has its own type of block.\n\nOne requirement is that the number of channels for each layer has to be exactly double that of the previous layer - this is checked by the second assert statement.","87baf7da":"We then use this to plot a confusion matrix.\n\nA 200x200 confusion matrix is pretty large, so we've increased the figure size and removed the colorbar. The text is still too small to read, but all we care about is that the diagonals are generally darker than the rest of the matrix - which they are.","b5041313":"The pre-trained ResNet model provided by torchvision does not provide an intermediate output, which we'd like to potentially use for analysis. We solve this by initializing our own ResNet50 model and then copying the pre-trained parameters into our model.\n\nWe then initialize our ResNet50 model from the configuration...","3779bbe3":"\nThe test accuracies are a little lower than the validation accuracies, but not by so much that we should be concerned.","6ee0db94":"We can also get the representations from the model in order to perform some dimensionality reduction techniques.\n\nIn this notebook we'll only get the output representations, and not the intermediate ones","bfe375c2":"First up is the BasicBlock.\n\nThe BasicBlock is made of two 3x3 convolutional layers. The first, conv1, has stride which varies depending on the layer (one in the first layer and two in the other layers), whilst the second, conv2, always has a stride of one. Each of the layers has a padding of one - this means before the filters are applied to the input image we add a single pixel, that is zero in every channel, around the entire image. Each convolutional layer is followed by a ReLU activation function and batch normalization.\n\nAs mentioned in the previous notebook, it makes more sense to use batch normalization after the activation function, rather than before. However, the original ResNet models used batch normalization before the activation, so we do here as well.\n\nWhen downsampling, we add a convolutional layer with a 1x1 filter, and no padding, to the residual path. This also has a variable stride and is followed by batch normalization. With a stride of one, a 1x1 filter does not change the height and width of an image - it simply has out_channels number of filters, each with a depth of in_channels, i.e. it is increasing the number of channels in an image via a linear projection and not actually downsampling at all. With a stride of two, it reduces the height and width of the image by two as the 1x1 filter only passes over every other pixel - this time it is actually downsampling the image as well as doing the linear projection of the channels.\n\nThe BasicBlock has an expansion of one as the number of filters used by each of the convolutional layers within a block is the same.","e00dc49d":"Finally, we can train our model!\n\nWe get around 80% top-1 and 95% top-5 validation accuracy.","bcaabff1":"As we can see, it starts at slightly less than $1x10^{-4}$ before gradually increasing to the maximum value of $1x10^{-3}$ at around a third of the way through training, then it begins decreasing to almost zero.\n\nThe different parameter groups defined by the optimizer for the discriminative fine-tuning will all have their own learning rate curves, each with different starting and maximum values.\n\nThe hypothesis is that the initial stage where the learning rate increases is a \"warm-up\" phase is used to get the model into a generally good area of the loss landscape. The middle of the curve, where the learning rate is at maximum is supposedly good for acting as a regularization method and prevents the model from overfitting or becoming stuck in saddle points. Finally, the \"cool-down\" phase, where the learning rate decreases, is used to reach small crevices in the loss surface which have a lower loss value.\n\nThe one cycle learning rate also cycles the momentum of the optimizer. The momentum is cycled from a maximum value, down to a minimum and then back up to the maximum where it is held constant for the last few steps. The default maximum and minimum values of momentum used by PyTorch's one cycle learning rate scheduler should be sufficient and we will not change them.\n\nTo set-up the one cycle learning rate scheduler we need the total number of steps that will occur during training. We simply get this by multiplying the number of epochs with the number of batches in the training iterator, i.e. number of parameter updates. We get the maximum learning rate for each parameter group and pass this to max_lr. \n__Note__: if you only pass a single learning rate and not a list of learning rates then the scheduler will assume this learning rate should be used for all parameters and will not do discriminative fine-tuning.","d1ea0ab7":"We can see the images look fine, however the names of the classes provided by the folders containing the images are a little long and sometimes overlap with neighbouring images.","f8afd79f":"# Introduction:\n\nIn this notebook I have shown:\n<br>\n- downloading and extracting custom datasets\n- loading custom datasets\n- calculating the mean and std for normalization on custom datasets\n- loading transforms to augment and normalize our data\n- defining a ResNet model\n- defining the ResNet blocks\n- defining a CIFAR ResNet model\n- loading a pre-trained model\n- loading pre-trained model parameters into a defined model\n- how to use the learning rate finder\n- how to use discriminative fine-tuning\n- how to use the one cycle learning rate scheduler\n- fine-tuning a pre-trained model to achieve ~80% top-1 accuracy and ~95% top-5 accuracy on a dataset with 200 classes and only 60 examples per class\n- viewing our model's mistakes\n- visualizing our data in lower dimensions with PCA and t-SNE\n- viewing the learned weights of our model","3cba6abe":"The Bottleneck block, used for ResNet50, ResNet101 and ResNet152.\n\nInstead of two 3x3 convolutional layers it has a 1x1, 3x3 and then another 1x1 convolutional layer. Only the 3x3 convolutional layer has a variable stride and padding, whilst the 1x1 filters have a stride of one and no padding.\n\nThe first 1x1 filter, conv1, is used to reduce the number of channels in all layers except the first, where it keeps the number of channels the same, e.g. in first block in of second layer it goes from 256 channels to 128. In the case where a 1x1 filter reduces the number of channels it can be thought of as a pooling layer across the channel dimension, but instead of doing a simple maximum or average operation it learns - via its weights - how to most efficiently reduce dimensionality. Reducing the dimensionality is also useful for simply reducing the number of parameters within the model and making it feasible to train.\n\nThe second 1x1 filter, conv3, is used to increase the number of channels - similar to the convolutional layer in the downsampling path.\n\nThe Bottleneck block has an expansion of four, which means that the number of channels in the image output a block isn't out_channels, but expansion * out_channels.\n\nThe downsampling convolutional layer is similar to that used in the BasicBlock, with the expansion factor taken into account.","168aaa95":"We can see that the filters perform many different types of image processing - from edge detection to color inversion.","92964f19":"We can then plot what a few of the images look like after having gone through the first convolutional layer.","de852b2b":"The evaluation function is also similar to previous notebooks, with the addition of the top-k accuracy.\n\nAs the one cycle scheduler should only be called after each parameter update, it is not called here as we do not update parameters whilst evaluating.","d84b25c6":"![image.png](attachment:image.png)","ece541db":"In this notebook we'll also be showing how to use torchvision to handle datasets that are not part of torchvision.datasets. Specificially we'll be using the 2011 version of the CUB200 dataset. This is a dataset with 200 different species of birds. Each species has around 60 images, which are around 500x500 pixels each. Our goal is to correctly determine which species an image belongs to - a 200-dimensional image classification problem.\n\nAs this is a relatively small dataset - ~12,000 images compared to CIFAR10's 60,000 images - we'll be using a pre-trained model and then performing transfer learning using discriminative fine-tuning.\n\n__Note__: on the CUB200 dataset website there is a warning about some of the images in the dataset also appearing in ImageNet, which our pre-trained model was trained on. If any of those images are in our test set then this would be a form of \"information leakage\" as we are evaluating our model on images it has been trained on. However, the GitHub gist linked at the end of this article states that only 43 of the images appear in ImageNet. Even if they all ended up in the test set this would only be ~1% of all images in there so would have a negligible impact on performance.\n\nWe'll also be using a learning rate scheduler, a PyTorch wrapper around an optimizer which allows us to dynamically alter its learning rate during training. Specifically, we'll use the one cycle learning learning rate scheduler, also known as superconvergnence, from this paper and is commonly used in the fast.ai course.\n\n# Data Processing\nAs always, we'll start by importing all the necessary modules. We have a few new imports here:\n\nlr_scheduler for using the one cycle learning rate scheduler\nnamedtuple for handling ResNet configurations\nos and shutil for handling custom datasets","9072c8f0":"Before we detail the Bottleneck block, we'll first show how to create the configurations for the ResNet models.\n\nInstead of using a simple list, like we did the for the VGG model in the previous notebook, we'll use a namedtuple. This will store: the block class, the number of blocks in each layer, and the number of channels in each layer.","99de136e":"One other thing we are going to implement is top-k accuracy. Our task is to classify an image into one of 200 classes of bird, however some of these classes look very similar and it is even difficult for a human to correctly label them. So, maybe we should be more lenient when calculating accuracy?\n\nOne method of solving this is using top-k accuracy, where the prediction is labelled correct if the correct label is in the top-k predictions, instead of just being the first. Our calculate_topk_accuracy function calculates the top-1 accuracy as well as the top-k accuracy, with $k=5$ by default.\n\n**Note**: our value of k should be chosen sensibly. If we had a dataset with 10 classes then a k of 5 isn't really that informative.","403abf4e":"Next, we define a function to plot the results of the range test.","06bc2b78":"Remember that the key concept in the ResNet models is the residual (aka skip\/identity) connection. However, if the number of channels within the image is changed in the main connection of the block then it won't have the same number of channels as the image from the residual connection and thus we cannot sum them together. Consider the first block in second layer of ResNet18, the image tensor passed to it will have 64 channels and the output will have 128 channels. Thus, we need to make a residual connection between a 64 channel tensor and a 128 channel tensor. ResNet models solve this using a downsampling connection - technically, it doesn't always downsample the image as sometimes the image height and width stay the same - which increases the number of channels in the image through the residual connection by passing them through a convolutional layer.\n\nThus, to check if we need to downsample within a block or not, we simply check if the number of channels into the block - in_channels - is the number of channels out of the block - defined by the channels argument multipled by the expansion factor of the block. Only the first block in each layer is checked if it needs to downsample or not. After each layer is created, we update in_channels to be the number of channels of the image when it is output by the layer.\n\nWe then follow the four layers with a 1x1 adaptive average pool. This will take the average over the entire height and width of the image separately for each channel. Thus, if the input to the average pool is [512, 7, 7] (512 channels and a height and width of seven) then the output of the average pool will be [512, 1, 1]. We then pass this average pooled output to a linear layer to make a prediction. We always know how many channels will be in the image after the fourth layer as we continuously update in_channels to be equal to the number of the channels in the image output by each layer.\n\nOne thing to note is that the initial convolutional layer has bias = False, which means there is no bias term used by the filters. In fact, every convolutional layer used within every ResNet model always has bias = False. The authors of the ResNet paper argue that the bias terms are unnecessary as every convolutional layer in a ResNet is followed by a batch normalization layer which has a $\\beta$ (beta) term that does the same thing as the bias term in the convolutional layer, a simple addition. See the previous notebook for more details on how batch normalization works.","86921b9e":"As there are 200 classes, it is difficult for every class to have a unique color.\n\nAlso a legend with 200 elements is quite large, so we do not plot it - however we leave the code (commented out) to do so if required.","98d8f7ac":"We can then set the learning rates of our model using discriminative fine-tuning - a technique used in transfer learning where later layers in a model have higher learning rates than earlier ones.\n\nWe use the learning rate found by the learning rate finder as the maximum learning rate - used in the final layer - whilst the remaining layers have a lower learning rate, gradually decreasing towards the input.","49640e86":"![image.png](attachment:image.png)","6962c18d":"Below are the configurations for the CIFAR ResNet models.\n\nSimilar to previous ResNet models, the channels do not change across configurations, just the number of blocks in each layer.\n\nWe can use the configurations to get the actual models as:\n\nciafar_resnet_20 = CIFARResNet(cifar_resnet20_config, output_dim)\nciafar_resnet_32 = CIFARResNet(cifar_resnet32_config, output_dim)\nciafar_resnet_44 = CIFARResNet(cifar_resnet44_config, output_dim)","56f1a60b":"Next up, we plot the t-SNE data. As we have a much smaller dataset than the CIFAR datasets used previously we can perform t-SNE on the entire dataset in a reasonable amount of time.","41946550":"The classes are definitely well separated here - which is usually a good sign.","1140f1ac":"\nBelow are the configurations for the ResNet50, ResNet101 and ResNet152 models.\n\nSimilar to the ResNet18 and ResNet34 models, the channels do not change between configurations, just the number of blocks in each layer.\n\nWe can use the configurations to get the actual models as:\n\nresnet50 = ResNet(resnet50_config, output_dim)\nresnet101 = ResNet(resnet101_config, output_dim)\nresnet152 = ResNet(resnet152_config, output_dim)","3e82b0c0":"We can then get all of the correct predictions, filter them out, and then sort all of the incorrect predictions based on how confident they were on their incorrect prediction.","e2da995c":"\nNext up, we set the learning rate scheduler. A learning rate scheduler dynamically alters the learning rate whilst the model is training. We'll be using the one cycle learning rate scheduler, however many schedulers are available in PyTorch.\n\nThe one cycle learning rate scheduler starts with a small initial learning rate which is gradually increased to a maximum value - the value found by our learning rate finder - it then slowly decreases the learning rate to a final value smaller than the initial learning rate. This learning rate is updated after every parameter update step, i.e. after every training batch. For our model, the learning rate for the final fc layer throughout training will look like:","23d16ed8":"As we can see, there is a common 7x7 convolutional layer and max pooling layer at the start of all ResNet models - these layers also have padding, which is not shown in the table. These are followed by four \"layers\", each containing a different number of blocks. There are two different blocks used, one for the ResNet18 and ResNet34 - called the BasicBlock - , and one for the ResNet50, ResNet101 and ResNet152 - called the Bottleneck block.\n\nOur ResNet class defines the initial 7x7 convolutional layer along with batch normalization, a ReLU activation function and a downsampling max pooling layer. We then build the four layers from the provided configuration, config, which specifies: the block to use, the number of blocks in the layer, and the number of channels in that layer. For the BasicBlock the number of channels in a layer is simply the number of filters for both of the convolutional layers within the block. For the Bottleneck block, the number of channels refers to the number of filters used by the first two convolutional layers - the number of the filters in the final layer is the number of channels multiplied by an expansion factor, which is 4 for the Bottleneck block (and 1 for the BasicBlock). Also note that the stride of the first layer is one, whilst the stride of the last three layers is two. This stride is only used to change the stride of the first convolutional layer within a block and also in the \"downsampling\" residual path - we'll explain what downsampling in ResNets means shortly.\n\nget_resnet_layer is used to define the layers from the configuration by creating a nn.Sequential from a list of blocks. The first thing it checks is if the first block in a layer needs to have a downsampling residual path - only the first block within a layer ever needs to have a downsampling residual path. So, what is a downsampling residual path?","e3bdc05a":"# Training the Model\nNext we'll move on to training our model. As in previous notebooks, we'll use the learning rate finder to set a suitable learning rate for our model.\n\nWe start by initializing an optimizer with a very low learning rate, defining a loss function (criterion) and device, and then placing the model and the loss function on to the device.","d763ca99":"# Thankyou For Reading And Do Upvote If you like it!!!","21ba411e":"The CIFARBasicBlock is similar to the standard BasicBlock, the only difference is in the downsampling residual connection.\n\nThe paper states that the ResNet models for CIFAR use a downsampling connection that uses \"zero padding\" and \"all shortcuts are parameter free\". We achieve this by using an Identity module, which is initialized with a function and applies that function when called.\n\nFor the function, we first slice the input with x[:, :, ::2, ::2]. This removes every other row and column in the image - downsampling it by simply throwing away pixels - whilst keeping the number of channels (depth) the same.","62a5df08":"Next up is the training function. This is similar to all the previous notebooks, but with the addition of the scheduler and calculating\/returning top-k accuracy.\n\nThe scheduler is updated by calling scheduler.step(). This should always be called after optimizer.step() or else the first learning rate of the scheduler will be skipped.\n\nNot all schedulers need to be called after each training batch, some are only called after each epoch. In that case, the scheduler does not need to be passed to the train function and can be called in the main training loop.","6307f33e":"then we load the parameters (called state_dict in PyTorch) of the pre-trained model into our model.\n\nThis is also a good sanity check to ensure our ResNet model matches those used by torchvision.","6df536f5":"The different ResNet configurations are known by the total number of layers within them - ResNet18, ResNet34, ResNet50, ResNet101 and ResNet152.","25e09a48":"Then, we replace the pre-trained model's linear layer with our own, randomly initialized linear layer.\n\nNote: even if our dataset had 1000 classes, the same as ImageNet, we would still remove the linear layer and replace it with a randomly initialized one as our classes are not equal to those of ImageNet.","a39292f4":"The images in our dataset are around 500x500 pixels in size, much larger than the 32x32 images used in the CIFAR dataset. This means it's more appropriate for us to use one of the standard ResNet models, instead of the CIFAR versions.\n\nWe'll choose ResNet50 as it seems to be the most commonly used ResNet variant.\n\nAs we have a relatively small dataset - 12,000 images - with a very small amount of examples per class - 60 images - we'll be using a pre-trained model.\n\nTorchvision provides pre-trained models for all of the standard ResNet variants, but unfortunately not for any of the CIFAR ResNet models.\n\nFirst, we load the pre-trained ResNet model.","cb2a74b0":"## Defining the Model\nNext up, we'll be defining our model. As mentioned previously, we'll be using one of the residual network (ResNet) models. Let's look at the ResNet configuration table again:","8c820646":"![image.png](attachment:image.png)\nWe then double the number of channels with zeros using pad, which adds half the padding on to the front of the depth dimension, and half to the back.","f68e9afa":"![image.png](attachment:image.png)","11af658d":"We can see that the loss reaches a minimum at around $3x10^{-3}$.\n\nA good learning rate to choose here would be the middle of the steepest downward curve - which is around $1x10^{-3}$.","0c549b44":"![image.png](attachment:image.png)","b0b481a5":"There are some interesting patterns contained in these filters, however all of these were already present in the pre-trained ResNet model. The learning rate used on this initial convolutional layer was most probably too small to change these significantly.","5c953287":"Why do ResNets work? The key is in the residual connections. Training incredibly deep neural networks is difficult due to the gradient signal either exploding (becoming very large) or vanishing (becoming very small) as it gets backpropagated through many layers. Residual connections allow the model to learn how to \"skip\" layers - by setting all their weights to zero and only rely on the residual connection. Thus, in theory, if your ResNet152 model can actually learn the desired function between input and output by only using the first 52 layers the remaining 100 layers should set their weights to zero and the output of the 52nd layer will simply pass through the residual connections unhindered. This also allows for the gradient signal to also backpropagate through those 100 layers unhindered too. This outcome could also also be achieved in a network without residual connections, the \"skipped\" layers would learn to set their weights to one, however adding the residual connection is more explicit and is easier for the model to learn to use these residual connections.\n\nThe image below shows a comparison between VGG-19, a convolutional neural network architecture without residual connections, and one with residual connections - ResNet34.","12264ab9":"From the ResNet configuration table, we can create the following configurations for ResNet18 and ResNet34. Notice how the specified channels does not change across configurations, simply the number of blocks used in each layer.\n\nWe could then define the ResNet18 and ResNet34 models with:\n\nresnet18 = ResNet(resnet18_config, output_dim)\nresnet34 = ResNet(resnet34_config, output_dim)","043e04c3":"![image.png](attachment:image.png)","baca34ca":"# Examining the Model\nWe'll be doing the same examinations of the model as done in previous notebooks.\n\nFirst, we get the predictions for each image in the test set...","5f6e60ab":"ResNet, like VGG, also has multiple configurations which specify the number of layers and the sizes of those layers. Each layer is made out of blocks, which are made up of convolutional layers, batch normalization layers and residual connections (also called skip connections or shortcut connections). Confusingly, ResNets use the term \"layer\" to refer to both a set of blocks, e.g. \"layer 1 has two blocks\", and also the total number of layers within the entire ResNet, e.g. \"ResNet18 has 18 layers\".\n\nA residual connection is simply a direct connection between the input of a block and the output of a block. Sometimes the residual connection has layers in it, but most of the time it does not. Below is an example block with an identity residual connection, i.e. no layers in the residual path.","49650715":"Our dataset, however, only has 200 classes, so we first create a new linear layer with the required dimensions.","f22fa978":"\nWe can also see the number of parameters in our model - noticing that ResNet50 only has ~24M parameters compared to VGG11's ~129M. This is mostly due to the lack of high dimensional linear layers which have been replaced by more parameter efficient convolutional layers."}}