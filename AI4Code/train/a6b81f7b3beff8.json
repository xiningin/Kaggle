{"cell_type":{"c72bad89":"code","22ee92f4":"code","76da079f":"code","53cfd1ba":"code","9d02310c":"code","70176a67":"code","5da4cab3":"code","20a2ee97":"code","75c439f3":"code","fc27f620":"code","071e788e":"code","d22d3b88":"code","a5e9e94d":"code","65c10e08":"code","fe72f7d4":"code","0699479a":"code","d8327a0b":"code","28d5beba":"code","ad6920f2":"code","89e84147":"code","fa74ec60":"code","d97a645f":"code","37c02e0f":"code","b4af954a":"code","5ed0388e":"code","823c6085":"code","0c64ef7d":"code","808a9b82":"code","00378ab2":"code","ca5f9249":"code","fe41df63":"code","d27b2c07":"code","c032e169":"code","c2e86121":"code","7feceb34":"code","0939a392":"code","d0ce46c7":"code","87bc17f3":"code","9208ef7c":"code","f1babb1f":"code","74f7a8b7":"code","78cb86e0":"markdown","88c30b6c":"markdown","03d2ee4f":"markdown","9e91a8fc":"markdown","f479b48c":"markdown","1420f091":"markdown","092b6b87":"markdown","9fc488a2":"markdown","daa4cffa":"markdown","251f29b8":"markdown","c7c51164":"markdown"},"source":{"c72bad89":"# Load packages\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import FunctionTransformer, LabelEncoder\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom wordcloud import WordCloud,STOPWORDS\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom sklearn.decomposition import LatentDirichletAllocation\nimport seaborn as sns\nimport spacy \nfrom collections import defaultdict,Counter\nimport warnings\nwarnings.filterwarnings('ignore')\nnltk.download('vader_lexicon')\nsns.set()\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","22ee92f4":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","76da079f":"# Load data from the csv file\ndf = pd.read_csv('\/kaggle\/input\/internet-articles-data-with-users-engagement\/articles_data.csv', index_col=0)\nprint(f\"Number of rows\/records: {df.shape[0]}\")\nprint(f\"Number of columns\/variables: {df.shape[1]}\")\ndf.head() \n","53cfd1ba":"df.describe(include='all')","9d02310c":"info = df.describe()\ninfo.loc['Skewness'] = df.skew()\ninfo.loc['Kurtosis'] = df.kurt()\ninfo.loc['Median'] = df.median()\ninfo","70176a67":"# Heatmap of missing values\nplt.figure(figsize=(8,12))\nsns.heatmap(df.isnull(),cbar=False)","5da4cab3":"# histogram most of the value centered around 0, not really useful for modelling\nplt.hist(df['engagement_reaction_count'].fillna(0),bins=[0,10,100,200,300,400,500,600,700,1000])\nplt.xlim(0,1000)\nplt.show()","20a2ee97":"# show distribution of numeric columns by removing outliers\nfig, ax = plt.subplots(2,2,figsize = (12,8))\ncol = ['engagement_reaction_count', 'engagement_comment_count',\n       'engagement_share_count', 'engagement_comment_plugin_count']\ndf2 = df[col].fillna(0)\ndf2 = df2[df2.engagement_reaction_count<np.percentile(df2.engagement_reaction_count,95)]\nfor i,ax in enumerate(ax.ravel()):\n    sns.distplot(df2[col[i]],ax =ax )\n        ","75c439f3":"# convert to time type\ndf['published_at'] = pd.to_datetime(df.published_at)\ndf['date'] = df.published_at.dt.date\ndf['hour'] = df.published_at.dt.hour\ndf['Day_Of_Week'] = df.published_at.apply(lambda x: x.dayofweek)\ndf['Month'] = df.published_at.apply(lambda x: x.month)\ndf['Year'] = df.published_at.apply(lambda x: x.year)\n\n# drop na for title and desc\ndf = df[~df.title.isna()]\ndf = df[~df.description.isna()]","fc27f620":"# Column where postive or negative sentiment regarding the news\nvader = SentimentIntensityAnalyzer()\ndf['scores']=df['title'].fillna(\"\").apply(lambda x : vader.polarity_scores(x))\ndf['title_sentiment'] = df['scores'].apply(lambda x: 'pos' if x['compound']>=0 else 'neg')\ndf['pos']=df.scores.apply(lambda x : x['pos'])\nprint(df['title_sentiment'].value_counts())\ndff = df.copy()","071e788e":"# show distribution of numeric columns by removing outliers\nfig, ax = plt.subplots(2,2,figsize = (12,8))\ncol = ['engagement_reaction_count', 'engagement_comment_count',\n       'engagement_share_count', 'engagement_comment_plugin_count']\nfor i,ax in enumerate(ax.ravel()):\n    sns.kdeplot(np.log(df[col[i]]+0.00001),ax =ax )\n    df[col[i]]=np.log(df[col[i]]+0.00001)","d22d3b88":"## which day get the most views\nplt.figure(figsize=(18,6))\nplt.subplot(1,3,1)\nplt.title('Engagement Reaction Of Each Month At A Particular Day')\nsns.heatmap(df.pivot_table(columns='Day_Of_Week',index='Month',values='engagement_reaction_count'),cbar=False,cmap='coolwarm')\nplt.subplot(1,3,2)\nplt.title('Engagement Comment Of Each Month At A Particular Day')\nsns.heatmap(df.pivot_table(columns='Day_Of_Week',index='Month',values='engagement_comment_count'),cbar=False,cmap='coolwarm')\nplt.subplot(1,3,3)\nplt.title('Engagement Share Of Each Month At A Particular Day')\nsns.heatmap(df.pivot_table(columns='Day_Of_Week',index='Month',values='engagement_share_count'),cmap='coolwarm')\nplt.show()","a5e9e94d":"sns.countplot(y='source_name',data=df,order=df.source_name.value_counts().index)\nplt.ylabel('Source Name')\nplt.show()","65c10e08":"plt.title(\"Top 10 Authors In Our Dataset\",fontsize=20)\nsns.barplot(x=df.author.value_counts()[:10].values,y=df.author.value_counts()[:10].index)\nplt.xlabel('Number Of Articles')\nplt.show()","fe72f7d4":"wc = WordCloud(stopwords=STOPWORDS).generate(' '.join(df.title))\nprint(\"Most seen words in title\")\nwc.to_image()","0699479a":"sns.catplot(x='date',data=df,y='pos',kind='bar',hue='source_name',height=8,aspect=1.6,ci=None)\nplt.title('Sentime across date at different source',fontsize=15,fontweight='bold')\nplt.show()","d8327a0b":"nlp = spacy.load('en_core_web_lg')\n\ndf.description = df.description.apply(lambda x : nlp(x))","28d5beba":"def nametag(name):\n    ppl_name = [ent.text for ent in name.ents if ent.label_ =='PERSON']\n    return Counter(ppl_name)","ad6920f2":"df = df.reset_index(drop=True)\n# Create collection of person name\nname_count = defaultdict(Counter)\nfor x in np.arange(len(df)):\n    if df.source_name[x] not in name_count.keys():\n        name_count[df.source_name[x]] = nametag(df.description[x])\n    else:\n        name_count[df.source_name[x]].update(nametag(df.description[x]))\n    ","89e84147":"# Create a new data frame contain souurce person and count\ndf_2 = pd.DataFrame(columns=['source_name','person','count'])\npos = 0\nfor source in name_count.keys():\n    for name , count in name_count[source].most_common(10):\n        df_2.loc[pos] = [source,name,count]\n        pos +=1\n    ","fa74ec60":"# Select the top 15 person name and visualize on the next plot\ndf_3 = df_2.groupby(['person'])['count'].sum()\ntop_15 = df_3.sort_values(ascending=False).index.tolist()[:15]\ndf_2 = df_2[df_2.person.isin(top_15)]\ndf_2.person = df_2.person.replace('Trump','Donald Trump')\ndf_2.person = df_2.person.replace('Johnson','Boris Johnson')\ndf_2 = df_2.groupby(['source_name','person'])['count'].sum().reset_index()","d97a645f":"sns.catplot(x='person',data=df_2,y='count',hue='source_name',kind='bar',height=12,aspect=1.5, palette=sns.color_palette())\nplt.xticks(rotation = 45)\nax = plt.gca()\n# change xtick size\nax.set_xticklabels(ax.get_xticklabels(),size = 18)\nplt.show()","37c02e0f":"nlp = spacy.load('en_core_web_lg')\n\ndef preprocess(text):\n    # Create Doc object\n    #doc = nlp(text, disable=['ner', 'parser'])\n    # Generate lemmas\n    lemmas = [token.lemma_ for token in text]\n    # Remove stopwords and non-alphabetic characters\n    a_lemmas = [lemma for lemma in lemmas if lemma.isalpha() and lemma not in STOPWORDS]\n    \n    return ' '.join(a_lemmas)\ndf.description = df.description.apply(preprocess)","b4af954a":"vectorizer = CountVectorizer(stop_words='english',lowercase=True,max_features=5000,token_pattern='[a-zA-Z0-9]{4,}')\n\ntf = vectorizer.fit_transform(df.description)","5ed0388e":"lda_model = LatentDirichletAllocation()\nlda_output = lda_model.fit_transform(tf)","823c6085":"topicnames = [\"Topic\" + str(i) for i in range(lda_model.n_components)]\ndocnames =[\"Doc\" + str(i) for i in range(len(df))]\ndf_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=df.title)\ndf_document_topic = df_document_topic.assign(main_topic =df_document_topic.idxmax(axis=1) )\ndf_document_topics=df_document_topic.iloc[:10]\ndf_document_topics.style.background_gradient()","0c64ef7d":"# get topic from models function\ndef get_model_topics(model, vectorizer, topics, n_top_words=10):\n    word_dict = {}\n    feature_names = vectorizer.get_feature_names()\n    for topic_idx, topic in enumerate(model.components_):\n        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n        top_features = [feature_names[i] for i in top_features_ind]\n        word_dict[topics[topic_idx]] = top_features\n\n    return pd.DataFrame(word_dict)","808a9b82":"word_df = get_model_topics(lda_model,vectorizer,topicnames,10)\nword_df.index = [f'Word {x}'  for x in range(10)]\nprint('               Most common words in each topic')\nword_df\n","00378ab2":"lad_components = lda_model.components_ \/ lda_model.components_.sum(axis=1)[:, np.newaxis]\ndef find_tags(text,threshold = 0.000001,vect=vectorizer,lda_model=lda_model):\n    words = text.lower().split(' ')\n    tag=[]\n    vect = vectorizer.transform([text])\n    scores = lda_model.transform(vect)\n    chosen_topic = topicnames[np.argmax(scores)]\n    for topic in range(lda_model.n_components):\n    # get the score in each topic\n        topic_score = scores[0][topic]\n    # get each word from tested topic and check its score in each topic\n        for word in words:\n            try: # the word in different weight differently\n                word_score =lad_components[topic][vectorizer.get_feature_names().index(word)]\n            except:\n                word_score = 0\n            score = topic_score*word_score\n            if score >= threshold:\n                tag.append(word)\n    \n    return list(set(tag)),chosen_topic","ca5f9249":"find_tags('I love seeing trumps Donald in election',0.0009)","fe41df63":"sns.factorplot(x='top_article', y='hour', hue ='title_sentiment',kind='violin',data = df,ci=None)\nplt.title('Most people post around 3 pm',fontsize=\"20\")","d27b2c07":"sns.catplot(x='top_article', y='engagement_comment_count', hue ='title_sentiment',data = dff,kind='strip', jitter=True)\nplt.title('Engagement count vs top article',fontsize='20')","c032e169":"# label the cat group\nlabel_encoder = LabelEncoder()\ndf['source_name_code'] = label_encoder.fit_transform(df.source_name)\ndf['sentiment_code']= label_encoder.fit_transform(df.title_sentiment)","c2e86121":"# shuffle the data and assign X and y\ndf3 = df[['source_name_code','engagement_reaction_count', 'engagement_comment_count',\n       'engagement_share_count', 'engagement_comment_plugin_count','sentiment_code','hour','top_article']]\ndf3 = df3.dropna()\n\nshuffled_indices = np.arange(df3.shape[0])\nnp.random.shuffle(shuffled_indices)\ndf3 = df3.iloc[shuffled_indices]\ndf3 = df3.reset_index()\nX = df3.iloc[:,:-1]\ny = df3.iloc[:,-1]","7feceb34":"#Balance the dataset\nnum_one_targets =int(np.sum(y))\nzero_targets_counter = 0\nindices_to_remove = []\n\nfor i in range(y.shape[0]):\n    if y[i] == 0:\n        zero_targets_counter += 1\n        if zero_targets_counter > num_one_targets-1:\n            indices_to_remove.append(i)\n\n# y being top article            \nX = X.drop(indices_to_remove)           \ny = y.drop(indices_to_remove)","0939a392":"# split the data set.\nX.drop('index',axis = 1,inplace=True)\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=1,stratify=y)\n\n","d0ce46c7":"# Logistic Regression\nlog = LogisticRegression()\nlog.fit(X_train,y_train)\nprint('The score is %.2f  for logistic regression' %log.score(X_test,y_test))\ny_pred = log.predict(X_test)\ncf_matrix = confusion_matrix(y_test,y_pred)\n\nsns.heatmap(cf_matrix\/np.sum(cf_matrix),annot = True, fmt='.2%',cmap= 'Blues')","87bc17f3":"# SVC score\nsvc =SVC()\nsvc.fit(X_train,y_train)\nprint('The score is %.2f  for SVC' %svc.score(X_test,y_test))\ny_pred = log.predict(X_test)\ncf_matrix = confusion_matrix(y_test,y_pred)\n\nsns.heatmap(cf_matrix\/np.sum(cf_matrix),annot = True, fmt='.2%',cmap= 'Blues')","9208ef7c":"## pipeline test data\nX = df\ny = df['engagement_reaction_count'].fillna(np.mean(df['engagement_reaction_count']))\nX_train,X_test, y_train,y_test = train_test_split(X,y,train_size = 0.2, random_state =1)\n\ndef combine_preproces_text(df):\n    text_data = df['description'].str.replace('[^a-zA-Z]',' ')\n    text_data = text_data.str.lower().fillna(\"\")\n    return text_data\n\nreturn_text = FunctionTransformer(combine_preproces_text)\n\npl = Pipeline([\n        ('text', return_text),\n        ('tfid',TfidfVectorizer(stop_words='english',max_features=20000)),\n        ('rfr',RandomForestRegressor())\n])\npl.fit(X_train,y_train)\n\n# Results\ny_pred = pl.predict(X_test)\n\nmse_score =MSE(y_pred,y_test)\nrmse_score = mse_score**(1\/2)\nprint(rmse_score)\n\nprint(pl.score(X_test,y_test))\n\n","f1babb1f":"# top 20 words with importances\ntop_20 = sorted(zip(pl[1].get_feature_names(),pl[2].feature_importances_),key=lambda x : x[1],reverse=True)[:100]\nprint([x[0]for x in top_20])","74f7a8b7":"df.sort_values('engagement_reaction_count',ascending=False)[['title','engagement_reaction_count']].iloc[:20]","78cb86e0":"## Context\nThis dataset ([source](https:\/\/www.kaggle.com\/szymonjanowski\/internet-articles-data-with-users-engagement)) consists of data about news articles collected from Sept. 3, 2019 until Nov. 4, 2019. Afterwards, it is enriched by Facebook engagement data, such as number of shares, comments and reactions. It was first created to predict the popularity of an article before it was published. However, there is a lot more you can analyze; take a look at some suggestions at the end of this template.","88c30b6c":"## LatentDirichletAllocation for topic modelling","03d2ee4f":"## Further Exploring","9e91a8fc":"## Explore Data","f479b48c":"## Using Spacy collect the person name mention in different sources","1420f091":"## Pipeline & ML","092b6b87":"## Understand your variables","9fc488a2":"## Create Features","daa4cffa":"\n# Internet News and Consumer Engagement\n\n","251f29b8":"## Load your data","c7c51164":"### Choose Topic Modeling based on text inference"}}