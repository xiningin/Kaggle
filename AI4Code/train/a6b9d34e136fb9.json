{"cell_type":{"6fc9abb3":"code","81a74565":"code","678edfc3":"code","847318c2":"code","f8c962a9":"code","f0005770":"code","58fc7eb4":"code","ea1e79cc":"code","c26a6f9e":"code","c5cea0f1":"code","b3f8d631":"code","9e98d7b3":"code","114cdbba":"code","39e239a7":"code","b34307e6":"code","1bad9373":"code","6ebcc288":"code","bec50296":"code","387e4b2d":"code","12bfaab8":"code","29b740e5":"code","de34c3fe":"code","b87120da":"code","e396ba71":"markdown"},"source":{"6fc9abb3":"# Importing Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom xgboost import XGBClassifier\nimport seaborn as sns\nnp.random.seed(10)","81a74565":"# Reading the data\ntrain = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv')\nfeatures = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/features.csv')\nsample_submission = pd.read_csv(\"\/kaggle\/input\/jane-street-market-prediction\/example_sample_submission.csv\")","678edfc3":"features = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/features.csv')\n","847318c2":"features.feature.unique()","f8c962a9":"# Quick view of text data\nprint(train.head())\nprint(train.shape)","f0005770":"sample_submission.head()","58fc7eb4":"# Plotting missing value historgram to remove columns with high missing values\nplt.hist(train.isnull().mean())","ea1e79cc":"# Only selecting the columns where missing values is less than7 percent based on above graph\nfinal_cols = train.isnull().mean()[train.isnull().mean() < 0.07]","c26a6f9e":"# Selecting only the required columns\ntrain = train[final_cols.index]","c5cea0f1":"# Filling NA values with median\ntrain = train.fillna(train.median())","b3f8d631":"# Plotting correlation\n\nf, ax = plt.subplots(figsize=(10, 8))\n\nsns.heatmap(train.drop(['date', 'weight', 'resp_1', 'resp_2', 'resp_3', 'resp_4','resp', 'ts_id'], axis = 1).corr(), \n            mask=np.zeros_like(train.drop(['date', 'weight', 'resp_1', 'resp_2', 'resp_3', 'resp_4','resp', 'ts_id'], axis = 1).corr(), dtype=np.bool), \n            cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax)","9e98d7b3":"# Creating Y Variable based on the condition that is resp is greater than 0 for all five columns then the action will be 1\ntrain['action'] = np.where((train.resp_1 > 0) & (train.resp_2 > 0) & (train.resp_3 > 0) & (train.resp_4 > 0) & (train.resp > 0),1,0)","114cdbba":"# Calculating the number responders\ntrain.action.sum()","39e239a7":"# A parameter grid for XGBoost\nparams = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5],\n        'learning_rate' : [0.02, 0.5], \n        'n_estimators' : [400,600]        \n        }","b34307e6":"# Decalring XGB\nxgb = XGBClassifier(nthread = -1)","1bad9373":"# Fitting the train data using default XGB model to remove columns with zero feature importance\nxgb.fit(train.drop(['date', 'weight', 'resp_1', 'resp_2', 'resp_3', 'resp_4','resp', 'ts_id', 'action'], axis = 1),train.action, verbose = 3)","6ebcc288":"feat_imp = pd.DataFrame({\"features\":train.drop(['date', 'weight', 'resp_1', 'resp_2', 'resp_3', 'resp_4','resp', 'ts_id', 'action'], axis = 1).columns, \"importances\" : xgb.feature_importances_})","bec50296":"# Selecting only those columns where feature importance is not equal to zero and selecting only top 20 columns\nfeat_imp = feat_imp[feat_imp['importances']!=0]\nfeat_imp = feat_imp.sort_values(by='importances', ascending=False)\nfeat_imp = feat_imp.head(10)","387e4b2d":"feat_imp.shape","12bfaab8":"# Perform 3 fold random search CV\nfolds = 2\nparam_comb = 2\n\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n\nrandom_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, \n                                   cv=skf.split(train[feat_imp.features],train.action), verbose=3, random_state=1001 )\n\n# Here we go\nrandom_search.fit(train[feat_imp.features], train.action)","29b740e5":"# ## Prediction on test data. Please refer to https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/overview\/evaluation\n# import janestreet\n# env = janestreet.make_env() # initialize the environment\n# iter_test = env.iter_test() # an iterator which loops over the test set\n# for (test_df, sample_prediction_df) in iter_test:\n#     test_df = train.fillna(test_df.median())\n#     test_df['action'] = np.where(test_df['weight'] > 0, xgb.predict(test_df[feat_imp.features]), 0) #make your 0\/1 prediction here\n#     env.predict(test_df[['action']])","de34c3fe":"# ## Prediction on test data. Please refer to https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/overview\/evaluation\n# feat_imp = pd.DataFrame({\"features\":train.drop(['date', 'weight', 'resp_1', 'resp_2', 'resp_3', 'resp_4','resp', 'ts_id', 'action'], axis = 1).columns, \"importances\" : 1})\n# import janestreet\n# env = janestreet.make_env() # initialize the environment\n# iter_test = env.iter_test(np.where(test_df['weight'] > 0, xgb.predict(test_df[feat_imp.features]), 0)) # an iterator which loops over the test set\n# for (test_df, sample_prediction_df) in iter_test:\n#     test_df = train.fillna(test_df.median())\n#     predictions = test_df[feat_imp.features]\n#     sample_prediction_df.action = 0 #make your 0\/1 prediction here\n# #     sample_prediction_df.action\n#     env.predict(sample_prediction_df)","b87120da":"## Prediction on test data. Please refer to https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/overview\/evaluation\nimport janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set\nfor (test_df, pred_df) in iter_test:\n        if test_df['weight'].item() > 0:\n            X_test = test_df.loc[:, feat_imp.features.values]\n            for k in feat_imp.features.values:\n                if k not in X_test:\n                    X_test[k] = np.nan\n            X_test = test_df.fillna(train[feat_imp.features.values].median())\n            print(X_test.columns)\n            pred = xgb.predict(X_test)\n            pred_df.action = np.where(pred >= 0.5, 1, 0).astype(int)\n        else:\n            pred_df.action = 0\n        env.predict(pred_df)","e396ba71":"This is a simple XGB Model which uses only train data with no feature engineering to check the baseline performance"}}