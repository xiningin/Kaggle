{"cell_type":{"946ddc31":"code","c85c62ec":"code","06dcacde":"code","006e53b2":"code","28499db7":"code","ddee0faf":"code","66c5799f":"code","b9e263d4":"code","9215bfc2":"code","71b600af":"code","17814966":"code","3418274f":"code","1800103d":"code","18d1e2a1":"code","a1478fe6":"code","56a01fc6":"code","6a28dedd":"code","8459d649":"code","4ad69097":"code","85ab4a2f":"code","79e7c14d":"code","999ce8d1":"code","eb510807":"code","76eecb6e":"code","1cada6e4":"code","67e6eb14":"code","c0a3ddbb":"code","516125f0":"code","a7fa8394":"code","282716fa":"code","3b8d6aa9":"markdown","fbfb1ad1":"markdown","dd2afdcd":"markdown","9dead5a7":"markdown","50b28a46":"markdown","0ba47324":"markdown","e8d677bd":"markdown","66e27173":"markdown","11defffb":"markdown","53f2511e":"markdown"},"source":{"946ddc31":"import os\nimport numpy as np\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport folium\nfrom folium import plugins\nimport seaborn as sns\nimport plotly.express as px\n\nfrom contextlib import contextmanager\nfrom time import time\nfrom tqdm import tqdm\nimport lightgbm as lgbm\nimport category_encoders as ce\n\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.metrics import classification_report, log_loss, accuracy_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold","c85c62ec":"data0 = pd.read_csv(\"..\/input\/miami-housing-dataset\/miami-housing.csv\")\ndata0","06dcacde":"print(data0.columns.to_list())","006e53b2":"fig, ax = plt.subplots(figsize=(16,6)) \nsns.histplot(data0['SALE_PRC'], label='SALE_PRC', ax=ax, color='C1',bins=80) \nax.legend() \nax.grid()","28499db7":"data0[data0['SALE_PRC']>2.5*10**6]","ddee0faf":"# SALE_PRC > 2.5 million dollars\ndata2a=data0[['LATITUDE','LONGITUDE']][data0['SALE_PRC']>2.5*10**6]\ndata2a","66c5799f":"eq_map = folium.Map(location=[25.8,-80.3],tiles='Stamen Terrain',zoom_start=11,min_zoom=2.0)\neq_map.add_child(plugins.HeatMap(data2a))\neq_map","b9e263d4":"target=['SALE_PRC']\ndataY=data0['SALE_PRC']\ndataX=data0.drop('SALE_PRC',axis=1)","9215bfc2":"n=len(dataX)\nN=[]\nfor i in range(n):\n    N+=[i]\nrandom.seed(2021)\nrandom.shuffle(N)","71b600af":"trainX=dataX.loc[N[0:(n\/\/4)*3]]\ntrainY=dataY.loc[N[0:(n\/\/4)*3]]\ntestX=dataX.loc[N[(n\/\/4)*3:]]\ntestY=dataY.loc[N[(n\/\/4)*3:]]","17814966":"df_columns = list(dataX.columns)\nprint(df_columns)","3418274f":"def create_numeric_feature(input_df):\n    use_columns = df_columns \n    return input_df[use_columns].copy()","1800103d":"from contextlib import contextmanager\nfrom time import time\n\nclass Timer:\n    def __init__(self, logger=None, format_str='{:.3f}[s]', prefix=None, suffix=None, sep=' '):\n\n        if prefix: format_str = str(prefix) + sep + format_str\n        if suffix: format_str = format_str + sep + str(suffix)\n        self.format_str = format_str\n        self.logger = logger\n        self.start = None\n        self.end = None\n\n    @property\n    def duration(self):\n        if self.end is None:\n            return 0\n        return self.end - self.start\n\n    def __enter__(self):\n        self.start = time()\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.end = time()\n        out_str = self.format_str.format(self.duration)\n        if self.logger:\n            self.logger.info(out_str)\n        else:\n            print(out_str)","18d1e2a1":"from tqdm import tqdm\n\ndef to_feature(input_df):\n\n    processors = [\n        create_numeric_feature,\n    ]\n    \n    out_df = pd.DataFrame()\n    \n    for func in tqdm(processors, total=len(processors)):\n        with Timer(prefix='create' + func.__name__ + ' '):\n            _df = func(input_df)\n\n        assert len(_df) == len(input_df), func.__name__\n        out_df = pd.concat([out_df, _df], axis=1)\n        \n    return out_df","a1478fe6":"train_feat_df = to_feature(trainX)\ntest_feat_df = to_feature(testX)","56a01fc6":"import lightgbm as lgbm\nfrom sklearn.metrics import mean_squared_error\n\ndef fit_lgbm(X, y, cv, \n             params: dict=None, \n             verbose: int=50):\n\n    if params is None:\n        params = {}\n\n    models = []\n    oof_pred = np.zeros_like(y, dtype=np.float)\n\n    for i, (idx_train, idx_valid) in enumerate(cv): \n        x_train, y_train = X[idx_train], y[idx_train]\n        x_valid, y_valid = X[idx_valid], y[idx_valid]\n\n        clf = lgbm.LGBMRegressor(**params)\n        \n        with Timer(prefix='fit fold={} '.format(i)):\n            clf.fit(x_train, y_train, \n                    eval_set=[(x_valid, y_valid)],  \n                    early_stopping_rounds=100,\n                    verbose=verbose)\n\n        pred_i = clf.predict(x_valid)\n        oof_pred[idx_valid] = pred_i\n        models.append(clf)\n        print(f'Fold {i} RMSLE: {mean_squared_error(y_valid, pred_i) ** .5:.4f}')\n        print()\n\n    score = mean_squared_error(y, oof_pred) ** .5\n    print('-' * 50)\n    print('FINISHED | Whole RMSLE: {:.4f}'.format(score))\n    return oof_pred, models","6a28dedd":"params = {\n    'objective': 'rmse', \n    'learning_rate': .1,\n    'reg_lambda': 1.,\n    'reg_alpha': .1,\n    'max_depth': 5, \n    'n_estimators': 10000, \n    'colsample_bytree': .5, \n    'min_child_samples': 10,\n    'subsample_freq': 3,\n    'subsample': .9,\n    'importance_type': 'gain', \n    'random_state': 71,\n    'num_leaves': 62\n}","8459d649":"y = trainY","4ad69097":"ydf=pd.DataFrame(y)\nydf","85ab4a2f":"from sklearn.model_selection import KFold\n\nfor i in range(1):\n    fold = KFold(n_splits=5, shuffle=True, random_state=71)\n    ydfi=ydf.iloc[:,i]\n    y=np.array(ydfi)\n    cv = list(fold.split(train_feat_df, y))\n    oof, models = fit_lgbm(train_feat_df.values, y, cv, params=params, verbose=500)\n    \n    fig,ax = plt.subplots(figsize=(6,6))\n    ax.set_title(target[i],fontsize=20)\n    ax.set_xlabel('Train Predicted '+target[i],fontsize=12)\n    ax.set_ylabel('Train Actual '+target[i],fontsize=12)\n    ax.scatter(oof,y)\n","79e7c14d":"def visualize_importance(models, feat_train_df):\n\n    feature_importance_df = pd.DataFrame()\n    for i, model in enumerate(models):\n        _df = pd.DataFrame()\n        _df['feature_importance'] = model.feature_importances_\n        _df['column'] = feat_train_df.columns\n        _df['fold'] = i + 1\n        feature_importance_df = pd.concat([feature_importance_df, _df], \n                                          axis=0, ignore_index=True)\n\n    order = feature_importance_df.groupby('column')\\\n        .sum()[['feature_importance']]\\\n        .sort_values('feature_importance', ascending=False).index[:50]\n    \n    order0=order[0:5]\n    print(order0.tolist())\n\n    fig, ax = plt.subplots(figsize=(8, max(6, len(order) * .25)))\n    sns.boxenplot(data=feature_importance_df, \n                  x='feature_importance', \n                  y='column', \n                  order=order, \n                  ax=ax, \n                  palette='viridis', \n                  orient='h')\n    \n    ax.tick_params(axis='x', rotation=0)\n    #ax.set_title('Importance')\n    ax.grid()\n    fig.tight_layout()\n    \n    return fig,ax\n\n#fig, ax = visualize_importance(models, train_feat_df)","999ce8d1":"for i in range(1):\n    fold = KFold(n_splits=5, shuffle=True, random_state=71)\n    ydfi=ydf.iloc[:,i]\n    y=np.array(ydfi)\n    cv = list(fold.split(train_feat_df, y))\n    oof, models = fit_lgbm(train_feat_df.values, y, cv, params=params, verbose=500)\n    fig, ax = visualize_importance(models, train_feat_df)\n    ax.set_title(target[i]+' Imortance',fontsize=20)\n","eb510807":"pred0 = np.array([model.predict(test_feat_df.values) for model in models])\nPRED = pred0[4]\nPRED[0:5]","76eecb6e":"print(type(testY))","1cada6e4":"testY.iloc[0]","67e6eb14":"ans=[]\nfor i in range(len(testY)):\n    ans+=[testY.iloc[i]]\nANS=np.array(ans)\nANS[0:5]","c0a3ddbb":"fig, ax = plt.subplots(figsize=(16,6))\nsns.histplot(PRED, label='Test Predict '+target[0], ax=ax, color='black',bins=80)\nsns.histplot(oof, label='Train Predict '+target[0], ax=ax, color='C1',bins=80)\nax.legend()\nax.grid()","516125f0":"fig,ax = plt.subplots(figsize=(6,6))\nax.set_title(target[0],fontsize=20)\nax.set_xlabel('Test Actual '+target[0],fontsize=12)\nax.set_ylabel('Test Predicted '+target[0],fontsize=12)\nax.scatter(ANS,PRED)","a7fa8394":"data2c=data0[['LATITUDE','LONGITUDE','SALE_PRC']]\ndata2c['SALE_RANK']=data2c['SALE_PRC'].apply(lambda x: int(np.log(x\/10**4)))\nprint(data2c['SALE_RANK'].value_counts())                                             ","282716fa":"sns.jointplot(data=data2c, x='LONGITUDE', y='LATITUDE', hue=\"SALE_RANK\", kind=\"kde\", height=10)","3b8d6aa9":"<img src=\"https:\/\/alamode-blog.com\/wp-content\/uploads\/2019\/12\/%E3%83%9E%E3%82%A4%E3%82%A2%E3%83%9F-700x376.jpg\" width=100%>","fbfb1ad1":"# Data preparation","dd2afdcd":"# Model","9dead5a7":"### The dataset contains the following columns:\n\n- PARCELNO: unique identifier for each property. About 1% appear multiple times.\n- SALE_PRC: sale price (dollar)(TARGET)\n- LND_SQFOOT: land area (square feet)\n- TOTLVGAREA: floor area (square feet)\n- SPECFEATVAL: value of special features (e.g., swimming pools) (dollar)\n- RAIL_DIST: distance to the nearest rail line (an indicator of noise) (feet)\n- OCEAN_DIST: distance to the ocean (feet)\n- WATER_DIST: distance to the nearest body of water (feet)\n- CNTR_DIST: distance to the Miami central business district (feet)\n- SUBCNTR_DI: distance to the nearest subcenter (feet)\n- HWY_DIST: distance to the nearest highway (an indicator of noise) (feet)\n- age: age of the structure\n- avno60plus: dummy variable for airplane noise exceeding an acceptable level\n- structure_quality: quality of the structure\n- month_sold: sale month in 2016 (1 = jan)\n- LATITUDE:\n- LONGITUDE:\n","50b28a46":"# Visualize Importance","0ba47324":"### Price Distribution","e8d677bd":"- TOT_LVG_AREA: floor area (square feet)\n- structure_quality: quality of the structure\n- OCEAN_DIST: distance to the ocean (feet)\n- SPEC_FEAT_VAL: value of special features (e.g., swimming pools) (dollar)\n- LONGITIUDE:","66e27173":"# Target setting","11defffb":"# Importance Factor Best 5 for sale price\n### 'TOT_LVG_AREA', 'structure_quality', 'OCEAN_DIST', 'SPEC_FEAT_VAL', 'LONGITUDE'","53f2511e":"### Location and Sale Price Ranking"}}