{"cell_type":{"cb7ca82e":"code","9c4b4d5e":"code","1afd8a2d":"code","38afe976":"code","4aa296f3":"code","9a918338":"code","bd3a489b":"code","ae3151c8":"code","a608e295":"code","5211537f":"code","01e69481":"code","53a64b1e":"code","ec4892b3":"code","6f0e5808":"code","c28adbfe":"code","020a136e":"code","1c7ffc68":"code","bf62884b":"code","0200dff1":"code","d081ea7c":"code","4a3a4e02":"code","87e5effe":"code","bdcf052f":"code","f473b9a3":"code","04a4ce7a":"code","332b3f18":"code","778cfde6":"code","26b7aa75":"code","74d48483":"code","6ee78bcc":"code","a7652ea6":"code","dbf760a1":"code","4e3dbf08":"code","0f3a38b8":"code","89648f6e":"code","0a4bacca":"code","aa34036b":"code","1ff4d150":"code","c07ff6d8":"code","3ff28304":"code","bd42035b":"code","90c10352":"code","3bf81a04":"code","0fdb2db2":"code","771af307":"markdown","3657d7e4":"markdown","e0e03556":"markdown","ac8a5f40":"markdown","c0a520d8":"markdown","c4f709d1":"markdown","b6df361d":"markdown","45b2d763":"markdown","c65263a1":"markdown","7fafa433":"markdown"},"source":{"cb7ca82e":"import math\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd","9c4b4d5e":"DATA_DIR = Path('\/kaggle\/input')\n\n# list all file paths in DATA_DIR and its subdirectories\nfor filepath in DATA_DIR.rglob('*'):\n     print(filepath)","1afd8a2d":"data_filepath = DATA_DIR \/ 'DontGetKicked'\n\ntrain_data = pd.read_csv(data_filepath \/ 'training.csv', index_col='RefId')","38afe976":"train_data.head()","4aa296f3":"# constants\nRANDOM_STATE = 24\nTARGET = 'IsBadBuy'","9a918338":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['figure.figsize'] = (14,8)\nsns.set_theme(style='whitegrid')","bd3a489b":"train_data.info()","ae3151c8":"# utility function\ndef missing_value_stats(dataframe):\n    count = dataframe.isna().sum().sort_values(ascending=False)\n    proportion = count \/ dataframe.shape[0]\n\n    missing_value_df = pd.concat([count, proportion], axis=1)\n    missing_value_df.columns = ['missing values', 'proportion']\n    return missing_value_df","a608e295":"train_data_missing = missing_value_stats(train_data)\ntrain_data_missing.loc[train_data_missing.proportion > 0]","5211537f":"fig, ax =  plt.subplots()\n\nsns.heatmap(train_data.isnull(), cbar=False, ax=ax)\nax.set_title('Heatmap of missing values')\nplt.show()","01e69481":"train_data['PurchDate'] = pd.to_datetime(train_data.PurchDate, format='%m\/%d\/%Y')","53a64b1e":"# select feature types\ndef classify_features(dataframe, cardinality_threshold=5):\n    cardinality = dataframe.nunique()\n    dtypes = dataframe.dtypes\n    \n    low_cardinality = cardinality[cardinality < cardinality_threshold]\n    low_cardinality_cols = list(low_cardinality.index)\n    object_cols = list(dtypes[dtypes == 'object'].index)\n\n    discrete_cols = list(set(low_cardinality_cols + object_cols))\n    discrete_cols = sorted(discrete_cols)\n\n    continuous_cols = list(set(dataframe.columns) - set(discrete_cols))\n    continuous_cols = sorted(continuous_cols)\n    return cardinality, continuous_cols, discrete_cols","ec4892b3":"cardinality, continuous_cols, discrete_cols = classify_features(train_data)\nassert len(train_data.columns) == len(continuous_cols + discrete_cols)","6f0e5808":"continuous_cols = [\n    'MMRAcquisitionAuctionAveragePrice', 'MMRAcquisitionAuctionCleanPrice',\n    'MMRAcquisitionRetailAveragePrice', 'MMRAcquisitonRetailCleanPrice',\n    'MMRCurrentAuctionAveragePrice', 'MMRCurrentAuctionCleanPrice',\n    'MMRCurrentRetailAveragePrice', 'MMRCurrentRetailCleanPrice',\n    'VehBCost', 'VehOdo', 'WarrantyCost', 'PurchDate'\n]\ncardinality[continuous_cols]","c28adbfe":"discrete_cols = list(set(train_data.columns) - set(continuous_cols))\ncardinality[discrete_cols]","020a136e":"continuous_features = continuous_cols\ndiscrete_features = discrete_cols.copy()\ndiscrete_features.remove(TARGET)","1c7ffc68":"# columns with missing values beyond a threshold\nthreshold = 0.2\ncondition = train_data_missing.proportion > threshold\nmissing_above = list(train_data_missing[condition].index)\nmissing_above","bf62884b":"# flag unknowns for columns with missing\n# values beyond a threshold\ntrain_data[missing_above] = train_data[missing_above].fillna('unknown')","0200dff1":"# use median for continuous features\ncontinuous_data = train_data[continuous_features].copy()\ncontinuous_fillna = continuous_data.median()\ndisplay(continuous_fillna)\nclean_continuous_data = continuous_data.fillna(continuous_fillna)","d081ea7c":"# sample some columns\ncol_idx = 2\npre = continuous_data.iloc[:, col_idx]\npost = clean_continuous_data.iloc[:, col_idx]\npd.concat([pre[pre.isna()], post[pre.isna()]], axis=1).head()","4a3a4e02":"# use mode for continuous features\ndiscrete_data = train_data[discrete_features].copy()\n\n# convert the mode df to a series\ndiscrete_fillna = np.squeeze(discrete_data.mode())\ndisplay(discrete_fillna)\nclean_discrete_data = discrete_data.fillna(discrete_fillna)","87e5effe":"# sample some columns\ncol_idx = 4\npre = discrete_data.iloc[:, col_idx]\npost = clean_discrete_data.iloc[:, col_idx]\npd.concat([pre[pre.isna()], post[pre.isna()]], axis=1).head()","bdcf052f":"# swap variables\ndata = train_data.copy()\ntrain_data = pd.concat([\n    data[TARGET], clean_continuous_data, clean_discrete_data\n], axis=1)\n\ntrain_data_missing = missing_value_stats(train_data)\ntrain_data_missing.loc[train_data_missing.proportion > 0]","f473b9a3":"slide = 4\n\nfor i in range(math.ceil(len(continuous_features)\/slide)):\n    cols = continuous_features[slide * i: slide * (i+1)]\n    display(train_data[cols].describe())","04a4ce7a":"cols = 2\nrows = math.ceil(len(continuous_features)\/cols)\nfig, axes = plt.subplots(rows, cols, figsize=(14, 8\/\/cols*rows))\nplt.tight_layout()\n\nfor i, col in enumerate(continuous_features):\n    ax = axes[i\/\/cols, i%cols]\n    sns.histplot(data=train_data, x=col, hue=TARGET, element='step', ax=ax)\n    ax.set_title(f'Histogram of {col}', y=0.88)\n\nplt.show()","332b3f18":"from scipy import stats","778cfde6":"anova_results = []\nfor col in continuous_features:\n    groupby = train_data.groupby(TARGET)[col]\n    categories = train_data[TARGET].dropna().unique()\n    anova_data = [\n        groupby.get_group(category) for category in categories\n    ]\n\n    F, p = stats.f_oneway(*anova_data)\n    anova_results.append([col, F, p])\n\ncolumns = ['feature', 'F-statistic', 'p-value']\nanova_df = pd.DataFrame(anova_results, columns=columns)\nanova_df = anova_df.sort_values('p-value').set_index('feature')\nanova_df","26b7aa75":"# get the columns whose p-value is statistically significant\nthreshold = 0.01\nsignificant_anova = anova_df[anova_df['p-value'] < threshold]\n\nprint(\"Features with significant ANOVA p-value: {}\".format(\n    significant_anova.shape[0]\n))\nprint(\"Features with insignificant ANOVA p-value: {}\".format(\n    anova_df.shape[0] - significant_anova.shape[0]\n))","74d48483":"slide = 8\n\nfor i in range(math.ceil(len(discrete_cols)\/slide)):\n    cols = discrete_cols[slide * i: slide * (i+1)]\n    display(train_data[cols].describe())","6ee78bcc":"# helper function\ndef cramers_corrected_stat(confusion_matrix):\n    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n        uses correction from Bergsma and Wicher, \n        Journal of the Korean Statistical Society 42 (2013): 323-328\n    \"\"\"\n    chi2 = stats.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2\/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2 - ((k-1)*(r-1))\/(n-1))    \n    rcorr = r - ((r-1)**2)\/(n-1)\n    kcorr = k - ((k-1)**2)\/(n-1)\n    return np.sqrt(phi2corr \/ min( (kcorr-1), (rcorr-1)))","a7652ea6":"# Chi-square and Cramer's V\nchi2_results = []\n\nfor col in discrete_features:\n    # contigency table\n    chi2_data = pd.crosstab(train_data[col], train_data[TARGET])\n    #display(chi2_data)\n\n    # Chi-square test\n    chi2, p, dof, expected = stats.chi2_contingency(chi2_data)\n    \n    # Cramer's V\n    V = cramers_corrected_stat(chi2_data)\n    \n    chi2_results.append([col, chi2, p, V])\n\ncolumns = ['feature', 'Chi-square statistic', 'p-value',\"Cramer's V\"]\nchi2_df = pd.DataFrame(chi2_results, columns=columns)\nchi2_df = chi2_df.sort_values(\"Cramer's V\", ascending=False)\nchi2_df.set_index('feature', inplace=True)\nchi2_df","dbf760a1":"# get the columns whose Cramer's V correlation exceeds\n# a set threshold\nthreshold = 0.05\nsignificant_chi2 = chi2_df[chi2_df[\"Cramer's V\"] > threshold]\n\nprint(\"Features with high Cramer's V correlation: {}\".format(\n    significant_chi2.shape[0]\n))\nprint(\"Features with low Cramer's V correlation: {}\".format(\n   chi2_df.shape[0] - significant_chi2.shape[0]\n))","4e3dbf08":"# choose discrete features with cardinality lower than threshold\nthreshold = 20\nlow_cardinality = cardinality[cardinality < threshold]\nlow_cardinality =  low_cardinality[\n    low_cardinality.index.isin(discrete_features)\n]\nlow_cardinality","0f3a38b8":"chi2_low_cardinality =  list(set(significant_chi2.index) & set(low_cardinality.index))\nchi2_low_cardinality","89648f6e":"cols = 2\nrows = math.ceil(len(chi2_low_cardinality)\/cols)\nfig, axes = plt.subplots(rows, cols, figsize=(14, 8\/\/cols*rows))\nplt.tight_layout()\n\nfor i, col in enumerate(chi2_low_cardinality):\n    ax = axes[i\/\/cols, i%cols]\n    sns.countplot(data=train_data, x=col, ax=ax)\n    ax.set_title(f'Count plot of {col}', y=0.88)\n    # aesthetics\n    xticklabels = ax.get_xticklabels()\n    if len(xticklabels) > 5:\n        ax.set_xticklabels(xticklabels, rotation=15)\n\nplt.show()","0a4bacca":"cols = 2\nrows = math.ceil(len(chi2_low_cardinality)\/cols)\nfig, axes = plt.subplots(rows, cols, figsize=(14, 8\/\/cols*rows))\nplt.tight_layout()\n\nfor i, col in enumerate(chi2_low_cardinality):\n    ax = axes[i\/\/cols, i%cols]\n    sns.countplot(data=train_data, x=col, hue=TARGET, ax=ax)\n    ax.set_title(f'Count plot of {col} by {TARGET}',\n                 y=0.88)\n    # aesthetics\n    xticklabels = ax.get_xticklabels()\n    if len(xticklabels) > 5:\n        ax.set_xticklabels(xticklabels, rotation=15)\n\nplt.show()","aa34036b":"X = train_data.copy()\ny = X.pop(TARGET)\n\n# prepare data for MI scoring function\nX.drop('PurchDate', axis=1, inplace=True)\nis_discrete = X.columns.isin(discrete_features)","1ff4d150":"dtypes = X.dtypes\ncategorical_features = list(dtypes[dtypes == 'object'].index)\n# set(categorical_features) - set(discrete_features)\nprint(\"Numeric discrete features: \\n{}\\n\".format(\n    list(set(discrete_features) - set(categorical_features) )\n))\nprint(\"Categorical features: \\n{}\\n\".format(categorical_features))","c07ff6d8":"from sklearn.preprocessing import OrdinalEncoder\n\n# label encode the categories in discrete data\nencoder = OrdinalEncoder(handle_unknown='use_encoded_value',\n                         unknown_value=-10)\ncategorical = X[categorical_features]\ncategorical = encoder.fit_transform(categorical, y)\nX[categorical_features] = categorical\nX[categorical_features].head()","3ff28304":"from sklearn.feature_selection import mutual_info_classif","bd42035b":"mi_scores = mutual_info_classif(\n    X, y, discrete_features=is_discrete,\n    random_state=RANDOM_STATE\n)\nmutual_info = pd.Series(mi_scores, index=X.columns,\n                        name='mutual_info')\nmutual_info = mutual_info.sort_values(ascending=False)\nnonzero_mutual_info = mutual_info[mutual_info != 0]\n\nprint('Number of features with non-zero MI score: {}'.format(\n    nonzero_mutual_info.shape[0]\n))\nprint('Number of features with zero MI score: {}'.format(\n    mutual_info.shape[0] - nonzero_mutual_info.shape[0]\n))","90c10352":"print(f'Top 5: \\n{nonzero_mutual_info.head()}\\n')\nprint(f'Bottom 5: \\n{nonzero_mutual_info.tail()}\\n')","3bf81a04":"fig, ax = plt.subplots()\ndata = nonzero_mutual_info.head(10)\n\nsns.barplot(x=data.values, y=data.index, ax=ax)\nax.set_title('Mutual information scores')\nplt.show()","0fdb2db2":"list(nonzero_mutual_info.head(10).index)","771af307":"19 out of 33 columns have missing values","3657d7e4":"The missing values seem to be uniformly distributed within the data.","e0e03556":"# Handle missing values","ac8a5f40":"# Convert data to appropriate types","c0a520d8":"# Check for missing values","c4f709d1":"# Continuous features","b6df361d":"# Loading the data","45b2d763":"# Discrete features","c65263a1":"# Don't Get Kicked! EDA","7fafa433":"# Mutual information"}}