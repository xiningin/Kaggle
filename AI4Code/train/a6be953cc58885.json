{"cell_type":{"4d9fcb45":"code","d22f36a4":"code","1968eab6":"code","31d16504":"code","11ecef37":"code","17195819":"code","3a85a983":"code","60136c01":"code","904cdc05":"code","568ed31d":"code","105be503":"code","bbf129c6":"code","1b164388":"code","aa81992c":"code","62216d83":"code","e1791605":"code","8183fd21":"code","475cc603":"code","c7d5136c":"code","7fa6632e":"code","b14e9871":"code","1107654c":"markdown","87b7b8e5":"markdown","208741e7":"markdown","c62de9c0":"markdown","4538a35b":"markdown","fb4c3baf":"markdown","f5eba456":"markdown","49eb1448":"markdown","6d2742c0":"markdown","12a37198":"markdown"},"source":{"4d9fcb45":"import pandas as pd\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport nltk\nimport spacy\nimport seaborn as sns\nimport pickle\nfrom nltk.stem import PorterStemmer\nimport string","d22f36a4":"complaint_data = pd.read_csv(\"..\/input\/consumer-complaints-financial-products\/Consumer_Complaints.csv\",low_memory = False)","1968eab6":"### Convert the columns names so that they don't have space and are more readable\ncomplaint_data.columns = [i.lower().replace(\" \",\"_\").replace(\"-\",\"_\") for i in complaint_data.columns]\ncomplaint_data.columns","31d16504":"print (\"The shape of data is \",complaint_data.shape)\nprint (\"The data types for our data are as follows \")\n","11ecef37":"### All the varables are text - which may correspond to categories and other variables\nprint (\" The number of unique values in each column is as follows\")\n### Lets do a describe with including objects\ncomplaint_data.describe(include = 'object').T.reset_index()\n","17195819":"#### Keep only the consumer complaints is not null\ncomplaint_data = complaint_data[~complaint_data['consumer_complaint_narrative'].isna()]","3a85a983":"#### Create a distirbution of length of customers complaints. We have very left skew in length of complaints\n### Which is expected as most compalints can be written in less than 500 words\ncomplaint_data['consumer_complaint_narrative'].apply(len).plot(kind = 'hist')","60136c01":"### Keep the length columns as a new column\ncomplaint_data['comp_length'] = complaint_data['consumer_complaint_narrative'].apply(len)\ncomplaint_data.reset_index(inplace = True)\ncomplaint_data['consumer_complaint_narrative'] = complaint_data['consumer_complaint_narrative'].str.replace(r\"XX+\\s\",\"\")\ncomplaint_data['consumer_complaint_narrative'] = complaint_data['consumer_complaint_narrative'].str.replace(\"XXXX\",\"\")","904cdc05":"complaint_data = complaint_data.sample(frac = 0.5,random_state = 5)\ncomplaint_data.reset_index(inplace = True)","568ed31d":"nlp = spacy.load('en_core_web_sm')\n### Remove the stop words using spacy predefined list \nstop_words = nlp.Defaults.stop_words\n#### Create a list of puntuation to be removed\nsymbols = \" \".join(string.punctuation).split(\" \") \n### As we are doing topic modelling itsa good idea to do lemmatisation - as it uses morphologial analysis\nps = PorterStemmer()\nimport re\n#### Lets define the cleaning function and see how it works\ndef cleanup_text(docs,logging = False):\n    texts = []\n    counter = 1\n    for doc in docs:\n        \n        if counter % 5000 == 0 :\n            print (\"Processed %d of out of %d documents\"% (counter,len(docs)))\n        counter += 1\n        \n        doc = nlp(doc) ### We are disabling parser as will nt be using it\n        \n        \n        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != \"-PRON-\"]\n        \n        tokens =[tok for tok in tokens if tok not in symbols]\n        tokens = [tok for tok in tokens if tok not in stop_words]\n        tokens = [re.sub('[0-9]', '', i) for i in tokens]\n        tokens = ' '.join(tokens)\n        texts.append(tokens)\n    return (pd.Series(texts))","105be503":"complaint_data['comp_preprocessed'] = cleanup_text(complaint_data['consumer_complaint_narrative'])","bbf129c6":"print (\"Shape of data before removing NA's ,\",complaint_data.shape)\ncomplaint_data =complaint_data[~complaint_data['comp_preprocessed'].isna()]\nprint (\"Shape of data before removing NA's ,\",complaint_data.shape)","1b164388":"complaint_data[['comp_preprocessed','consumer_complaint_narrative']].head(5)","aa81992c":"### Lets Create the piprle line for NMF models \nfrom time import time\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF \n#####  Let extract from act the features from the dataset\n\nprint (\"Extracting the tf-idf features form NMF\")\ntfidf_vectorizer = TfidfVectorizer(max_df = 0.5, min_df = 5, max_features = 500, ngram_range = (1,4))\n\nt0 = time()\ntfidf = tfidf_vectorizer.fit_transform(complaint_data['comp_preprocessed'])\nprint (\"done in %0.3fs.\" % (time() - t0))","62216d83":"#### Now we will fit model for 10 diff values of clusters\nn_comp = [10,20,30,40,50,60,70,80,90,100,110]\nloss = []\nfor comps in n_comp:\n  \n    t0 = time()\n    nmf = NMF(n_components = comps, random_state = 1, beta_loss = 'kullback-leibler',solver = 'mu',max_iter = 200,\n             alpha = 0.1, l1_ratio = 0.5).fit(tfidf)\n    loss.append(nmf.reconstruction_err_)\n    print (\"done in %0.3f \" % (time() -t0))","e1791605":"### Let try to create a elbow and find out the best model clusters\nplt.plot(loss)\nplt.xlabel('Number of Topics')\nplt.ylabel('Reconstruction Error - Frobenius Norm')","8183fd21":"def print_top_words(model, feature_names, n_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        message = \"Topic #%d: \" % topic_idx\n        message += \" \".join([feature_names[i]\n                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n        print(message)\n    print()","475cc603":"print_top_words(nmf, tfidf_vectorizer.get_feature_names(), 10)","c7d5136c":"from sklearn.decomposition import LatentDirichletAllocation","7fa6632e":"# Fit the NMF model\n#### Now we will fit model for 10 diff values of clusters\nn_comp = [10,20,30,40,50]\n\nfor comps in n_comp:\n    loss1 = []\n    t0 = time()\n    lda = LatentDirichletAllocation(n_components=comps, max_iter=2,\n                                learning_method='online',\n                                learning_offset=50.,\n                                random_state=0)\n    lda.fit(tfidf)\n    print(\"done in %0.3fs.\" % (time() - t0))","b14e9871":"print_top_words(lda, tfidf_vectorizer.get_feature_names(), 10)","1107654c":"#### Objective : This code make use of complaint data against financial companies to perform following tasks on the data\n    1. Create Topic models to classify comlplaints in various categories.\n    2. We will do test preprcessingw here which remove all token which belong to org and other preprocessing\n    3. We will try out two Topic Modelling Algorithms\n        1. Latent Dirichtlet ALogorithm - In my exp does not work well on short text data\n        2. Non Negative Matrix factorisation","87b7b8e5":"### Read the complaints data csv","208741e7":"### Define a function which goes through - topic * word matrix and extract the top keywords","c62de9c0":"######  From above description we see that only 114704 rows have complaint text and as we are interested in only those row which have complaint text. We wll drop all rows where complaint narrative is na","4538a35b":"### Lets tranform the data using TFIDF vectoriser\n1. For learning purpose i am using only tf-idf , but inpractical it as advised to trial out count vectoriser also\n2. Also, all paarmeters in TF IDF can be treated as Hyperparameters","fb4c3baf":"### Next Steps\n1. Data needs a lot of cleaning for example \"XX\",\"XXX\",organisation names\n2. After looking at Topics we realise that a lot of keywords are important but do not give anyinformation about complaints. We should create a custom stop word list & clean the required data\n3. We may want to try clustering topics into K means for naming and better understanding and grouping\n4. Use LDA visualisation to understand topic in much more details","f5eba456":"### We will first craete the data processing pipeline to do cleaning on the data","49eb1448":"#### Import the required package","6d2742c0":"### Lets understand the shape an dtypes of the data","12a37198":"#### About the Dataset\nThe dataset comprises of Consumer Complaints on Financial products and we\u2019ll see how to classify consumer complaints text into these categories: Debt collection, Consumer Loan, Mortgage, Credit card, Credit reporting, Student loan, Bank account or service, Payday loan, Money transfers, Other financial service, Prepaid card.\nAlso we will try to identify the companies from the dataset"}}