{"cell_type":{"d9eaa4f7":"code","7abcc637":"code","6c882135":"code","319bc19b":"code","c3b87154":"code","a49f208c":"code","2405e03b":"code","b0b5200a":"code","71ec94d4":"code","ae819859":"code","18c15c27":"code","89fa009a":"code","659dcf23":"code","39a79460":"code","120bfc2d":"markdown"},"source":{"d9eaa4f7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport cv2\nimport os\nimport random\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout, Activation, Conv2D, MaxPooling2D\nfrom tensorflow.keras import layers\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7abcc637":"cats_train_path = \"\/kaggle\/input\/dogs-cats-images\/dataset\/training_set\/cats\"\ndogs_train_path = \"\/kaggle\/input\/dogs-cats-images\/dataset\/training_set\/dogs\"\ncats_test_path = \"\/kaggle\/input\/dogs-cats-images\/dataset\/test_set\/cats\"\ndogs_test_path = \"\/kaggle\/input\/dogs-cats-images\/dataset\/test_set\/dogs\"","6c882135":"for dirname, _, filenames in os.walk(cats_train_path):\n    for i, filename in enumerate(filenames):\n        image = os.path.join(dirname, filename)\n        img_array = cv2.imread(image,cv2.IMREAD_GRAYSCALE)\n        new_img_array = cv2.resize(img_array, dsize=(80, 80))\n        plt.imshow(new_img_array,cmap=\"gray\")\n        if(i == 4):\n            break","319bc19b":"for dirname, _, filenames in os.walk(dogs_train_path):\n    for i, filename in enumerate(filenames):\n        image = os.path.join(dirname, filename)\n        img_array = cv2.imread(image,cv2.IMREAD_GRAYSCALE)\n        new_img_array = cv2.resize(img_array, dsize=(80, 80))\n        plt.imshow(new_img_array,cmap=\"gray\")\n        if(i == 4):\n            break","c3b87154":"x_train = []\ny_train = []\nx_test = []\ny_test = []\ndef create_train_dataset(path, flag):\n    #flag == 1 for dogs, flag = 0 for cats\n    for dirname, _, filenames in os.walk(path):\n        for i, filename in enumerate(filenames):\n            image = os.path.join(dirname, filename)\n            img_array = cv2.imread(image,cv2.IMREAD_GRAYSCALE)\n            new_img_array = cv2.resize(img_array, dsize=(80, 80))\n            x_train.append(new_img_array)\n            y_train.append(flag)\n\ndef create_test_dataset(path, flag):\n    #flag == 1 for dogs, flag = 0 for cats\n    for dirname, _, filenames in os.walk(path):\n        for i, filename in enumerate(filenames):\n            image = os.path.join(dirname, filename)\n            img_array = cv2.imread(image,cv2.IMREAD_GRAYSCALE)\n            new_img_array = cv2.resize(img_array, dsize=(80, 80))\n            x_test.append(new_img_array)\n            y_test.append(flag)\n        \ncreate_train_dataset(cats_train_path, 0)\ncreate_train_dataset(dogs_train_path, 1)\ncreate_test_dataset(cats_test_path, 0)\ncreate_test_dataset(dogs_test_path, 1)","a49f208c":"len(x_train)","2405e03b":"#Ignore this part. This is for checking purposes for correctly reading the required data.\ncount = 0\nfor _ in y_test:\n    if _ == 0:\n        count+= 1\nprint(count)","b0b5200a":"X = np.array(x_train).reshape(-1, 80,80,1)\ny = np.array(y_train)","71ec94d4":"X = X\/255.0 # for normalization of values","ae819859":"data_augmentation = tf.keras.Sequential([\n    layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n#     layers.experimental.preprocessing.RandomRotation(0.2),\n#     layers.experimental.preprocessing.RandomRotation(0.4),\n])\nfrom keras import layers\nfrom keras import models\nfrom keras import optimizers\ndef create_cnn_model():\n#     model = Sequential([\n#         data_augmentation\n#     ])\n# #     model.add(data_augmentation)\n#     # Adds a densely-connected layer with 64 units to the model:\n#     model.add(Conv2D(64,(3,3), activation = 'relu', input_shape = X.shape[1:]))\n#     model.add(MaxPooling2D(pool_size = (2,2)))\n#     # Add another:\n#     model.add(Conv2D(64,(3,3), activation = 'relu'))\n#     model.add(MaxPooling2D(pool_size = (2,2)))\n\n#     model.add(Flatten())\n#     model.add(Dense(64, activation='relu'))\n#     # Add a softmax layer with 10 output units:\n#     model.add(Dense(1, activation='sigmoid'))\n\n#     model.compile(optimizer=\"adam\",\n#                   loss='binary_crossentropy',\n#                   metrics=['accuracy'])\n    model = models.Sequential()\n    model.add(layers.Conv2D(32, (3, 3), activation = \"relu\", input_shape = X.shape[1:]))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Conv2D(64, (3, 3), activation = \"relu\"))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Conv2D(128, (3, 3), activation = \"relu\"))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Conv2D(128, (3, 3), activation = \"relu\"))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Flatten())\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(512, activation = \"relu\"))\n    model.add(layers.Dense(1, activation = \"sigmoid\"))\n\n    model.compile(loss = \"binary_crossentropy\",\n                 optimizer = optimizers.RMSprop(lr = 1e-4),\n                 metrics = [\"acc\"])\n\n    return model\n\nmodel = create_cnn_model()","18c15c27":"model.fit(X, y, epochs=400, batch_size=64, validation_split=0.3)","89fa009a":"X_test = np.array(x_test).reshape(-1, 80,80,1)\nX_test = X_test\/255.0\ny_test = np.array(y_test)","659dcf23":"predictions = model.predict(X_test)\npredicted_val = [int(round(p[0])) for p in predictions]","39a79460":"TP = 0\nFP = 0\nTN = 0\nFN = 0\ntotal_positives = 0\ntotal_negetives = 0\n\nfor i, prediction in enumerate(predicted_val):\n    if prediction == 0:\n#         print(prediction)\n        total_positives += 1\n    else:\n        total_negetives += 1\n        \n    if y_test[i] == 0 and prediction == 0:\n        TN += 1\n    elif y_test[i] == 1 and prediction == 1:\n        TP += 1\n    elif y_test[i] == 1 and prediction == 0:\n        FN += 1\n    else:\n        FP += 1\nprint(\"TP : {} FP : {} TN : {} FN : {} \".format(TP, FP, TN, FN))\n\nprecision = TP\/(TP+FP)\nrecall = TP\/(TP+FN)\nf1 = 2*precision*recall\/(precision + recall)\nprint(\"Precision : \",precision)\nprint(\"Recall    : \", recall)\nprint(\"f1        : \", f1)\nprint(\"Mis-classification : \", (FP + FN)\/len(y_test))","120bfc2d":"`def create_cnn_model():`\nIt returns the CNN model which is later used to train and test using respective data."}}