{"cell_type":{"41345e44":"code","7090cc53":"code","647323b3":"code","e08b2095":"code","11afaadd":"code","f5d603bd":"code","ced9a01f":"code","8b70d368":"code","a0d5cea0":"code","8b96fb44":"code","c8080daa":"code","9260dd9f":"code","0ba66dd9":"code","0c650fdb":"code","2f66edf7":"code","04813c03":"code","ace47ba0":"code","a45ab54a":"code","701a3269":"code","51c13c3b":"code","926d151b":"code","f60df28f":"code","a7051942":"code","cb7d8d40":"code","1515860b":"code","de1a06e8":"code","81627ade":"code","6a3078e6":"code","dbfbb931":"code","8b05786b":"markdown","b73b232f":"markdown","9d70776e":"markdown","7484f715":"markdown","2f6404b7":"markdown","d9050e42":"markdown","49cdb506":"markdown","56c69f5b":"markdown","961eb0b4":"markdown","bd77d280":"markdown","489a882f":"markdown","d1f4e55b":"markdown","cfece5b8":"markdown","bf7a119b":"markdown","273e6b15":"markdown","8ca292db":"markdown","4ac41f7d":"markdown","153817f8":"markdown","0d6f687e":"markdown","059b68a8":"markdown","31834706":"markdown","02fd2b43":"markdown","0537407b":"markdown","e8f01945":"markdown","96bc8b59":"markdown","72e47c57":"markdown","e6de8daa":"markdown","315302fb":"markdown","7eee53aa":"markdown","e4e1e083":"markdown","059fa82c":"markdown","1dbff91a":"markdown","198e6a89":"markdown","dc8abbcf":"markdown","ef135035":"markdown","374af8d1":"markdown","fd8ff420":"markdown","ff89e1a9":"markdown","bfffb60b":"markdown","ebe5818b":"markdown","4a103bef":"markdown","0bb52b0f":"markdown","d60d5dc8":"markdown","7502ccff":"markdown","a317150d":"markdown","b05184a9":"markdown","c0ab47dc":"markdown","f5de22ed":"markdown"},"source":{"41345e44":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\ndf = pd.read_csv('..\/input\/heart.csv')","7090cc53":"df.head()","647323b3":"df.shape","e08b2095":"df.isna().sum()","11afaadd":"fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(18, 10))\nfig.suptitle(\"Categorical Variable Distributions\", fontsize=16)\n\nx = ['Female','Male']\ny = df.sex.value_counts(sort = False).values\naxes[0][0].bar(x,y)\naxes[0][0].set_title('Sex')\n\nx = ['Typical','Atypical','Non-Anginal','Aysyptomatic']\ny = df.cp.value_counts(sort = False).values\naxes[0][1].bar(x,y)\naxes[0][1].set_title('Chest Pain')\n\nx = ['Healthy','Unhealthy']\ny = df.fbs.value_counts(sort = False).values\naxes[0][2].bar(x,y)\naxes[0][2].set_title('Fasting Blood Sugar')\n\nx = ['Regular','Abnormality','Severe']\ny = df.restecg.value_counts(sort = False).values\naxes[0][3].bar(x,y)\naxes[0][3].set_title('Electrocardiographic')\n\n\nx = ['No','Yes']\ny = df.exang.value_counts(sort = False).values\naxes[1][0].bar(x,y)\naxes[1][0].set_title('Exercise induced Angina')\n\nx = ['Downward','Flat','Upward']\ny = df.slope.value_counts(sort = False).values\naxes[1][1].bar(x,y)\naxes[1][1].set_title('ST excercise peak')\n\nx = ['None','Normal','Fixed Defect','Reversable Defect']\ny = df.thal.value_counts(sort = False).values\naxes[1][2].bar(x,y)\naxes[1][2].set_title('Thalium Stress Test')\n\nx = ['No','Yes']\ny = df.target.value_counts(sort = False).values\naxes[1][3].bar(x,y)\naxes[1][3].set_title('Heart Disease')\n\nplt.show()","f5d603bd":"df['max_heart_rate'] = 220 - df['age']\ndf['peak_to_max_ratio'] = df['thalach']\/df['max_heart_rate']","ced9a01f":"continuous_df = df[['age','trestbps','chol','thalach','oldpeak','ca','max_heart_rate','peak_to_max_ratio','target']]","8b70d368":"sns.pairplot(continuous_df)","a0d5cea0":"fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 12))\nsns.violinplot(x=\"target\", y=\"age\", data=df,color = 'white',edgecolor = 'black',ax=axes[0][0]).set_title('Age')\nsns.swarmplot(x=\"target\", y=\"age\", data=df,ax = axes[0][0])\n\nsns.violinplot(x=\"target\", y=\"trestbps\", data=df,color = 'white',edgecolor = 'black',ax = axes[0][1]).set_title('Resting Blood Pressure')\nsns.swarmplot(x=\"target\", y=\"trestbps\", data=df,ax = axes[0][1])\n\nsns.violinplot(x=\"target\", y=\"chol\", data=df,color = 'white',edgecolor = 'black',ax = axes[0][2]).set_title('Cholesterol')\nsns.swarmplot(x=\"target\", y=\"chol\", data=df,ax = axes[0][2])\n\nsns.violinplot(x=\"target\", y=\"thalach\", data=df,color = 'white',edgecolor = 'black',ax = axes[1][0]).set_title('Max Heart Rate Achieved')\nsns.swarmplot(x=\"target\", y=\"thalach\", data=df,ax = axes[1][0])\n\nsns.violinplot(x=\"target\", y=\"oldpeak\", data=df,color = 'white',edgecolor = 'black',ax = axes[1][1]).set_title('ST Depression Peak')\nsns.swarmplot(x=\"target\", y=\"oldpeak\", data=df,ax = axes[1][1])\n\nsns.violinplot(x=\"target\", y=\"peak_to_max_ratio\", data=df,color = 'white',edgecolor = 'black',ax = axes[1][2]).set_title('Peak Heart Rate to Max Heart Rate Ratio')\nsns.swarmplot(x=\"target\", y=\"peak_to_max_ratio\", data=df,ax = axes[1][2])","8b96fb44":"fig,ax = plt.subplots(figsize=(16, 10))\nsns.heatmap(df.corr(), ax=ax, annot=True, linewidths=0.05, fmt= '.2f',cmap=\"magma\")\nplt.show()","c8080daa":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn import metrics","9260dd9f":"X = df.drop('target',axis = 1)\ny = df.target\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = .2,random_state = 123)","0ba66dd9":"log = LogisticRegression()\nlog.fit(X_train,y_train)\nprint('Accuracy for Logistic Regression: %0.4f' %log.score(X_test,y_test))","0c650fdb":"rf = RandomForestClassifier(n_estimators = 1000,random_state = 123)\nrf.fit(X_train,y_train)\nrf.score(X_test,y_test)\nprint('Accuracy for Random Forest: %0.4f' %rf.score(X_test,y_test))","2f66edf7":"feature_importance = pd.DataFrame(sorted(zip(rf.feature_importances_, X.columns)), columns=['Value','Feature'])\nplt.figure(figsize=(10, 6))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_importance.sort_values(by=\"Value\", ascending=False))\nplt.title('Random Forest Feature Importance')\nplt.tight_layout()","04813c03":"X = df.drop(['target','age','thalach'],axis = 1)\ny = df.target\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = .2,random_state = 123)","ace47ba0":"rf = RandomForestClassifier(n_estimators = 1000,random_state = 123)\nrf.fit(X_train,y_train)\nrf.score(X_test,y_test)\nprint('Accuracy for Random Forest: %0.4f' %rf.score(X_test,y_test))","a45ab54a":"feature_importance = pd.DataFrame(sorted(zip(rf.feature_importances_, X.columns)), columns=['Value','Feature'])\nplt.figure(figsize=(10, 6))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_importance.sort_values(by=\"Value\", ascending=False))\nplt.title('Random Forest Feature Importance')\nplt.tight_layout()","701a3269":"test_accuracy = []\nrf = RandomForestClassifier(random_state = 123,n_jobs = -1)\nforest_sizes = range(100,1001,100)\nfor forest_size in forest_sizes:\n    rf.set_params(n_estimators = forest_size)\n    rf.fit(X_train,y_train)\n    test_accuracy.append(rf.score(X_test,y_test))","51c13c3b":"plt.plot(forest_sizes, test_accuracy)\nplt.title('N_Estimators Accuracy')\nplt.xlabel('Number of Trees (By Hundred)')\nplt.ylabel('Accuracy')\nplt.show()","926d151b":"test_accuracy = []\nrf = RandomForestClassifier(n_estimators = 600,random_state = 123,n_jobs = -1)\ndepths = range(5,15)\nfor depth in depths:\n    rf.set_params(max_depth = depth)\n    rf.fit(X_train,y_train)\n    test_accuracy.append(rf.score(X_test,y_test))","f60df28f":"plt.plot(depths,test_accuracy)\nplt.title('Max_Depth Accuracy')\nplt.xlabel('Maximum Depth')\nplt.ylabel('Accuracy')\nplt.show()","a7051942":"test_accuracy = []\nrf = RandomForestClassifier(n_estimators = 600,max_depth = 10,random_state = 123,n_jobs = -1)\nsplits = range(2,15)\nfor split in splits:\n    rf.set_params(min_samples_split = split)\n    rf.fit(X_train,y_train)\n    test_accuracy.append(rf.score(X_test,y_test))","cb7d8d40":"plt.plot(splits,test_accuracy)\nplt.title('Min_Samples_Split Accuracy')\nplt.xlabel('Minimum Sample Split')\nplt.ylabel('Accuracy')\nplt.show()","1515860b":"test_accuracy = []\nrf = RandomForestClassifier(n_estimators = 600,max_depth = 10,min_samples_split = 10,random_state = 123,n_jobs = -1)\nfeatures = range(2,14)\nfor feature in features:\n    rf.set_params(max_features = feature)\n    rf.fit(X_train,y_train)\n    test_accuracy.append(rf.score(X_test,y_test))","de1a06e8":"plt.plot(features, test_accuracy)\nplt.title('Max_Features Accuracy')\nplt.xlabel('Maximum Features')\nplt.ylabel('Accuracy')\nplt.show()","81627ade":"rf = RandomForestClassifier(n_estimators = 600,max_depth = 10,min_samples_split = 10,max_features = 3,random_state = 123,n_jobs = -1)\nprint(rf)","6a3078e6":"rf.fit(X_train,y_train)\nrf.score(X_test,y_test)","dbfbb931":"feature_importance = pd.DataFrame(sorted(zip(rf.feature_importances_, X.columns)), columns=['Value','Feature'])\nplt.figure(figsize=(10, 6))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_importance.sort_values(by=\"Value\", ascending=False))\nplt.title('Random Forest Feature Importance')\nplt.tight_layout()","8b05786b":"#### Variable Differences by Target Value","b73b232f":"<a id=\"6\"><\/a>\n### Correlation matrix","9d70776e":"[Back to Top](#top)","7484f715":"<a id=\"5\"><\/a>\n### Continuous Variables\nBefore we look at these I'm going to create another feature using the max_heart_rate formula mentioned earlier (max heart rate = 220 - age). I will also create a feature of max heart rate achieved (thalach) over the new feature to scale it.","2f6404b7":"500 Trees is the most accurate forest so we'll set it as that","d9050e42":"#### Min Sample Split\nmin_samples_split tells the trees they can only split if both splits have this minimum amount of observations. Like max_depth this attempts to fix overfitting.","49cdb506":"<hr>","56c69f5b":"Finally, we see which variables are the most important. It seems the top 5 before it starts dropping off significantly are:\n1. Type of chest pain (cp)\n2. number of major vessels visable by xray (ca)\n3. thalium stress test results (thal)\n4. heartwave change relative to excercising (oldpeak) \n5. max heart rate achieved in comparison to theoretical max heart rate (peak_to_max_ratio)","961eb0b4":"<a id=\"10\"><\/a>\n### Model Fine Tuning\nDue to the dataset being so small in observations we will be doing simple train-test validation instead of cross-validation to fine tune the hyper-parameters","bd77d280":"Pretty even mix between discrete and continuous variables","489a882f":"We see the model has all the features we tuned as well as others we didnt","d1f4e55b":"We see that thalach and peak_to_max_ratio as well as age and max_heart_rate are very similar in importance due to their close relationship. To get a more refined more I will take age and thalach out of the dataset and see how it improves","cfece5b8":"<a id=\"9\"><\/a>\n### Simple Random Forest Classifier","bf7a119b":"<a id=top ><\/a>\n**Table of Contents:**\n1. [Variable Overview](#1)\n2. [Loading in Data (SKIP TO CODE)](#2)\n3. [Variable Insight](#3)\n    1. [Categorical Variables](#4)\n    1. [Continuous Variables](#5)\n    1. [Correlations](#6)\n4. [Modeling](#7)\n     1. [Logistic Regression](#8)\n     1. [Simple Random Forest Classifier](#9)\n     1. [Model Fine Tuning](#10)\n     1. [Final Model](#11)\n5. [Conclusion](#12)\n6. [Future Works](#13)","273e6b15":"<hr>","8ca292db":"<b> What is this? <\/b>  \nA correlation matrix showing the correlations between each variable (ranges from -1 to 1)\n\n<b> Why did we do this? <\/b>  \nTo see how the variables interact with each other. Similar to the pairplot before but easier to understand the numeric correlations.\n\n<b> Interesting Insights <\/b>  \nThe engineered features have high correlations to the features they're derived from (that's expected).  \nIt's about split with what features seem to have either a decent negative or positive correlation to the target.  \n","4ac41f7d":"Interesting pieces are peak_to_max_ratio and oldpeak moved up while thal shifted down a little bit. Everything else stayed the same","153817f8":"#### Max Depth\nmax_depth tells the trees how deep they're allowed to grow, each split equals a new level so max depth restricts the amount of splits","0d6f687e":"NOTES: \n1. Stumbled upon this rule of thumb while researching: Maximum Heart Rate = 220 - Age  \n    - Possibly use it as a baseline for the max heart rate achieved?","059b68a8":"Gained about an additional 1.5% accuracy compared to the model after we stripped away age and thalach.","31834706":"<a id=\"11\"><\/a>\n### Final Model","02fd2b43":"Interesting to see that some of the signs (blood sugar, excercise induced angina, and the electrocardiogram) have healty signs for most people but the target variable is a majority with heart disease. Seperately these tests don't seem to be able to pinpoint it but maybe together it'll work","0537407b":"#### N Estimators\nn_estimators is the number of trees within the random forest, more trees typically leads to more accruate results but takes longer to train","e8f01945":"<b> What is this? <\/b>  \nAn overlay of a violinplot (outside lining) with a swarmplot (inside dots).\n\n<b> Why did we do this? <\/b>  \nTo see how the distribution of each continuous variable is different in reference to the target results.  \n\n<b> Interesting Insights <\/b>  \nOlder people seem to be in the non heart disease category. This may be because people who have heart disease don't tend to live so long.  \nMax heart rate achieved has quite a different between target subsets.\nMax heart rate achieved\/theoretical max heart rate seems to group around 1 for people with heart disease. This shows that people with heart disease in the hospital typical hit around or above their maximum heart rate theoretically possible.","96bc8b59":"#### Problem with Multicolinearity\nThalach and age are tightly related to peak_max_ratio and max_heart_rate to age so we'll remove thalach and age and see what happens","72e47c57":"We see here that we only have 300 observations which is quite low to build robust models so EDA will be the concentration here.","e6de8daa":"Accuracy actually dropped, lets see how importances moved around","315302fb":"# Heart Disease EDA\n## Marco Gancitano\n### 1 Feb 2019","7eee53aa":"Random Forest beats the typical classification baseline of Logistic Regression so it proves to have some extra explanatroy power for this dataset","e4e1e083":"Good to see we don't have any NA values","059fa82c":"#### Feature Importance","1dbff91a":"<a id=\"4\"><\/a>\n### Categorical Variables","198e6a89":"#### Distributions and Relations","dc8abbcf":"<b>Why did we use this? <\/b>   \nWe use this to see the distributions of the independent varabiles as well as how they interact just incase we have problems with multicolinearlity.  \n\n<b>Interesting Insights<\/b>  \nRelationship between age and maximum heart rate.   As age increases, maximum heart rate decrease which relates back to the maximum heart rate = 220 - age.","ef135035":"<hr>\n<hr>","374af8d1":"<a id=\"7\"><\/a>\n## Modeling","fd8ff420":"1. age: Age in years\n2. sex: Sex (1 = Male, 0 = Female)\n3. cp: Chest Pain Type (0-4)\n    - 0, Typical angina\n        - Chest pain related decrease blood supply to the heart\n    - 1, Atypical angina\n        - Chest pain not related to heart\n    - 2, Non-anginal pain\n        - Typically esophageal spasms (non heart related)\n    - 3, Asymptomatic\n        - Chest pain not showing signs of disease\n4. trestbps: Resting blood pressure upon admission (mm Hg)\n    - Anything above 130-140 is typically cause for concern\n5. chol: Serum cholestrol level (mg\/dL)\n    - serum = LDL + HDL + .2 * triglycerides\n    - Above 200 is cause for concern\n6. fbs: Fasting blood sugar > 120 mg\/dL (1 = True, 0 = False) \n    - fbs > 126 mg\/dL signals diabetes\n7. restecg: Resting electrocardiographic results (0 - 2)\n    - 0, Nothing to note\n    - 1, ST-T Wave abnormality\n        - Can range from mild symptoms to severe problems\n        - Signals non-normal heart beat\n    - 2, Possible or definite Left ventricular hypertrophy\n        - Enlarged heart's main pumping chamber\n8. thalach: Maximum heart rate achieved\n    - Rate above 100 is cause for concern\n9. exang: exercise induced angina (1 = yes; 0 = no)\n10. oldpeak: ST depression induced by exercise relative to rest\n    - Looks at stress of heart during excercise\n    - Unhealthy heart will stress more\n11. slope: the slope of the peak exercise ST segment\n    - 0, upsloping\n       - Better heart rate with excercise (uncommon)\n    - 1, flatsloping\n        - Minimal change (typical healthy heart)\n    - 2, downsloping\n        - Signs of unhealthy heart\n12. ca: number of major vessels (0-3) colored by flourosopy\n    - Colored vessel means the doctor can see the blood passing through\n        - Move blood movement the better (no clots)\n13. thal: Thalium stress test result\n    - Sees how blood moves through your heart while excercising\n    - 1, Normal\n    - 6, fixed defect\n        - Used to be defect but now okay\n    - 7, reversable defect\n        - Not proper blood movement when excercising\n14. target: Heart Disease (1 = True, 0 = False)\n\n\n","ff89e1a9":"<hr>","bfffb60b":"<a id=\"3\"><\/a>\n## Variable Insight\nWe'll look through the categorical variables first and then move on to continuous variables and check out their correlations","ebe5818b":"<a id=\"13\"><\/a>\n### Future Works\n1. Try other machine learning models\n2. More feature engineering from someone with more domain knowledge\n3. Further tuning of hyper-parameters","4a103bef":"#### Feature Importance","0bb52b0f":"<a id=\"1\"><\/a>\n## Variable Overview\nI start with looking at each variable and understanding what it means and the values it can entail.  \nI do this to help with domain knowledge and hopefully feature engineering throughout the process. ","d60d5dc8":"#### Max Features\nmax_features selects how many features are subsetted to be passed into each tree of the forest. There should be a sweet spot between creating a robust model but not under or over fitting.","7502ccff":"<hr><hr>","a317150d":"<a id=\"8\"><\/a>\n### Logistic Regression","b05184a9":"<a id=\"12\"><\/a>\n### Conclusion\n\nWe've seen here that the main indicators of heart disease stem from the healthiness of your heart (seems quite obvious). Although other health indicators like resting bps and blood sugar are good for overall health. Heart disease stems from having an unhealthy heart","c0ab47dc":"For this fine tuning we will not be creating a validation set and will just be using the test set to test the parameters. Because we aren't using this model against unseen data and this is purely explanatory we don't need one.","f5de22ed":"<a id=\"2\"><\/a>\n## Import Packages and Data"}}