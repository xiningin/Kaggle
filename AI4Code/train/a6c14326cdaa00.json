{"cell_type":{"f67a456c":"code","832d3321":"code","8144a473":"code","91228b02":"code","bde57275":"code","c10e7e15":"code","b924fc67":"code","8d281e24":"code","a4b07887":"code","dc2e3061":"code","b1b842eb":"code","1b2ad02d":"code","4982ec8c":"code","73694091":"code","639b70ca":"code","c6262c02":"code","ef02e29b":"code","9f405e3c":"code","f7290768":"code","e13e9934":"code","a8fb8585":"code","60b73e34":"code","4464e09a":"markdown","5c94084a":"markdown","fdb293f9":"markdown","b99b1213":"markdown","79b202b5":"markdown","be17f0e6":"markdown","070c2a24":"markdown","09591e29":"markdown","e01acffc":"markdown","35c752bd":"markdown","f2fc3341":"markdown","705a1b04":"markdown","5caf13b1":"markdown","25ae9435":"markdown","b14c0949":"markdown","df46feae":"markdown","011fa4c0":"markdown","1cd8a968":"markdown","18be9a98":"markdown","1a3a1766":"markdown","38d9ead5":"markdown","824d9fe3":"markdown","67bfdb0c":"markdown","9359ac72":"markdown"},"source":{"f67a456c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","832d3321":"df=pd.read_csv('..\/input\/tsf-datasets\/Iris.csv')","8144a473":"df.head()","91228b02":"df = df.drop('Id', axis = 1)","bde57275":"#looking for imbalance in the dataset\ndf.info()","c10e7e15":"#chceking for shape\ndf.shape","b924fc67":"#chceking null values\ndf.isnull().sum()","8d281e24":"df.describe()","a4b07887":"sns.pairplot(df, hue = 'Species')\nplt.show()","dc2e3061":"#correaltion matrix\nsns.heatmap(df.corr(), annot = True)\nplt.plot()","b1b842eb":"x = df.iloc[:,[0, 1, 2, 3]].values   #attributes\ny = df['Species'].values   #labels","1b2ad02d":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)","4982ec8c":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier(max_depth = 2)\ndtree.fit(x_train, y_train)\nprint(\"Decision Tree Classifier Created!\")","73694091":"#predicting the values of test data\ny_pred = dtree.predict(x_test)","639b70ca":"score = dtree.score(x_test, y_test)\nprint(score)","c6262c02":"from sklearn.metrics import classification_report, confusion_matrix\nprint(\"Classification Report - \\n\", classification_report(y_test, y_pred))","ef02e29b":"cm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize = (8, 8))\n\nsns.heatmap(cm, annot = True, fmt = \".0f\", linewidths = 0.5, square = True, cmap = 'Greens')\n\nplt.ylabel('Actual Label', fontsize = 15)\nplt.xlabel('Predicted Label', fontsize = 15)\ntitle = 'Accuracy Score: {0}'.format(score)\nplt.title(title, size = 15)\nplt.tick_params(labelsize = 15)","9f405e3c":"#list of values to try for max_depth\nmax_depth_range = list(range(1, 6))\n\n#list to store the average RMSE for each value of max_depth\naccuracy = []\n\nfor depth in max_depth_range:\n    dtree = DecisionTreeClassifier(max_depth =  depth, random_state = 0)\n    dtree.fit(x_train, y_train)\n    \n    score = dtree.score(x_test, y_test)\n    accuracy.append(score)","f7290768":"fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (10, 7))\n\nax.plot(max_depth_range, accuracy, lw = 2, color = 'blue')\n\nax.set_xlim([1, 5])\nax.set_ylim([0.5, 1.0])\nax.grid(True, axis = 'both', zorder = 0, linestyle = ':', color = 'k')\n\nax.tick_params(labelsize = 18)\nax.set_xticks([1, 2, 3, 4, 5])\nax.set_xlabel('max-depth', fontsize = 24)\nax.set_ylabel('Accuracy', fontsize = 24)\nfig.tight_layout()","e13e9934":"print(\"Accuracy after setting max_depth=3: {0}\".format(score))","a8fb8585":"# Putting the feature names and class names into variables\nfn = ['Sepal length (cm)','Sepal width (cm)','Petal length (cm)','Petal width (cm)']\ncn = ['setosa', 'versicolor', 'virginica']","60b73e34":"fig, axes = plt.subplots(nrows = 1, ncols = 1, figsize = (7,4), dpi = 300)\nfrom sklearn.tree import plot_tree\nplot_tree(dtree, feature_names = fn, class_names=cn, filled = True, rounded= True);","4464e09a":"After performing above steps we can say that this dataset perfectly clean and we can process further with this dataset.","5c94084a":"### Splitting Data into Training and Testing Sets","fdb293f9":"Now, as you can see the optimal max_depth is <b>3<\/b>","b99b1213":"<ul>\n    <li>After Importing, Fit our dataset in our model, accuracy is 89.47%.<\/li>\n    <li>We can clearly see model performance by confusion matrix and classification report.<\/li>\n    <li>By ploting accuracy score depth wise graph, optimal depth for model is 3.<\/li>\n    <li>After implementing the optimal max_depth, the accuracy rises to 97.36%.<\/li>","79b202b5":"# Author : Akash Kothare","be17f0e6":"###  Visualizing the Tree","070c2a24":"### Training the Model","09591e29":"### Plotting accuracy score depth wise","e01acffc":"### Loading the dataset","35c752bd":"We can easily observe that \"iris-setosa\" makes a distinctive cluster in every parameter, while the other two species are overlapping a bit on each other.","f2fc3341":"### Classification Report","705a1b04":"Now, let's plot pair plot to visualise the attributes all at once","5caf13b1":"No Null Values found!","25ae9435":"### Conclusion:","b14c0949":"### Common Dataset exploration checking for shape, null, missing values","df46feae":"In this task:\n<ul>\n    <li>We have to create a Decision Tree classifier and visualize it graphically.<\/li>\n    <li>The purpose is if we feed any new data to this classifier, it would be able to predict the right class accordingly.<\/li>","011fa4c0":"## Task 6: Prediction using Decision Tree Algorithm","1cd8a968":"Data Science & Business Analytics Intern (Batch - Dec'20)","18be9a98":"Also, lets see the correaltion among the features","1a3a1766":"### Importing Libraries","38d9ead5":"### Finding the Optimal max_depth","824d9fe3":"Observations made -\n<ol>\n    <li>Petal length is highly related to petal width.<\/li>\n    <li>Sepal length is not related to sepal width.<\/li>\n<\/ol>","67bfdb0c":"### Confusion Matrix","9359ac72":"### Measuring Model Perfomance"}}