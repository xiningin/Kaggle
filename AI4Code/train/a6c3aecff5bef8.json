{"cell_type":{"af5fb6bd":"code","6d6b448c":"code","0d147ad9":"code","c8992901":"code","aa957d7c":"code","65ad9c0a":"code","c606ef5c":"code","cca40b49":"code","ced8e9b9":"code","0f62252f":"code","6e400f7b":"code","98d239da":"code","fba87091":"code","f008ad6b":"code","755d96e8":"code","ac199d4b":"code","cd0798cd":"code","45849f8b":"code","e288b709":"markdown","b06eb217":"markdown","2aa57c30":"markdown","6a83811c":"markdown","d34fa6aa":"markdown","50edab0a":"markdown","a5683fad":"markdown","edb6b5c6":"markdown","db924363":"markdown","a673f9ae":"markdown","b23f977d":"markdown","a844ad69":"markdown","79cd5967":"markdown","25b0449d":"markdown","4a98e5df":"markdown","5747d475":"markdown","6aa428c7":"markdown","7c89114b":"markdown","cf4fadae":"markdown","e033c084":"markdown","30798c56":"markdown","64e6cdd6":"markdown","53aa9e47":"markdown","34811f85":"markdown","0b16d618":"markdown","6fff2674":"markdown","bd32c7d1":"markdown","37252922":"markdown","7370098b":"markdown","558a203f":"markdown","22b27798":"markdown","aa80982f":"markdown","eba8b8fa":"markdown","4709d3be":"markdown","35f0a430":"markdown","613e81ff":"markdown"},"source":{"af5fb6bd":"#Basic necessary Libraries\nimport numpy as np\nimport pandas as pd\n\n#Visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport altair as alt\nimport plotly.express as px\nfrom wordcloud import WordCloud,STOPWORDS\nimport holoviews as hv\nfrom holoviews import opts\nhv.extension('bokeh')\n\n#Apriori libraries \nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules","6d6b448c":"groceries=pd.read_csv('..\/input\/groceries-dataset\/Groceries_dataset.csv')\nprint(f'Groceries_dataset.csv : {groceries.shape}')\ngroceries.head()","0d147ad9":"groceries.info()","c8992901":"#Renaming the columns to simple words\ngroceries.rename(columns = {'Member_number':'id','itemDescription':'item'}, inplace = True) ","aa957d7c":"#Convert the 'Date' column to datetime format\ngroceries['Date']= pd.to_datetime(groceries['Date'])\n \n#Extracting year,month and day\ngroceries['year'] = groceries['Date'].apply(lambda x : x.year)\ngroceries['month'] = groceries['Date'].apply(lambda x : x.month)\ngroceries['day'] = groceries['Date'].apply(lambda x : x.day)\ngroceries['weekday'] = groceries['Date'].apply(lambda x : x.weekday())\n\n#Rearranging the columns\ngroceries=groceries[['id', 'Date','year', 'month', 'day','weekday','item']]\ngroceries.head()","65ad9c0a":"#Filtering data by year 2014 and 2015  \ndf1=groceries.groupby(['year']).filter(lambda x: (x['year'] == 2014).any())\ndf2=groceries.groupby(['year']).filter(lambda x: (x['year'] == 2015).any())\n\n#Plotting monthly data of number of quantity purchased in 2014 and 2015 \nsales_2014=hv.Bars(df1.groupby(['month'])['item'].count()).opts(ylabel=\"# of items\", title='# of items sold in 2014')\nsales_2015=hv.Bars(df2.groupby(['month'])['item'].count()).opts(ylabel=\"# of items\", title='# of items sold in 2015')\n\n#Merging both plots\n(sales_2014 + sales_2015).opts(opts.Bars(width=380, height=300,tools=['hover'],show_grid=True))","c606ef5c":"#Plotting day transaction across a typical month in 2014 and 2015\nsales_day=hv.Curve(groceries.groupby(['day'])['item'].count()).opts(ylabel=\"# of items\", title='Cummulative day transactions-2014 & 2015')\n\n#Line chart\nsales_day.opts(opts.Curve(width=800, height=300,tools=['hover'],show_grid=True))","cca40b49":"#Creating temporary data which has quantity purchased column\ntemp=groceries.copy()\ntemp['qty_purchased']=groceries['id'].map(groceries['id'].value_counts())\n\n#Slicing first 5000 rows as altair library can't plot any data which has record beyond that\ntemp1=temp[:5000]\ntemp1.columns\n\n#Plotting\nbrush = alt.selection(type='interval', encodings=['x'])\n\n#Plotting the bar chart\nbars = alt.Chart().mark_bar(color=\"green\").encode(\n    x=alt.X('month(Date):O',title=\"Month\"),\n    y=alt.Y('mean(qty_purchased):Q',title=\"Last Price\"),\n    opacity=alt.condition(brush, alt.OpacityValue(1), alt.OpacityValue(0.7)),\n    tooltip=['month(Date)','mean(qty_purchased)']\n).add_selection(\n    brush\n).properties(height=400,width=600,title=\"Monthly quantity purchased from grocery store-Drag over bars and find average\")\n\n#Plotting avrage line\nline = alt.Chart().mark_rule(color='firebrick').encode(\n    y='mean(qty_purchased):Q',\n    size=alt.SizeValue(3),\n    tooltip=['mean(qty_purchased)']\n).transform_filter(\n    brush\n)\n\n#Display plot using sliced data\nalt.layer(bars, line, data=temp1)","ced8e9b9":"#Converting weekday variable to category\ntemp1.weekday = temp1.weekday.astype('category') \n\n#Creating a new dataframe which has the frequency of weekdays\nweekday_bin=temp1['weekday'].value_counts().to_frame().reset_index().rename(columns={'index':'weekday','weekday':'count'})\n\n#Plotting bar chart\nbars = alt.Chart(weekday_bin).mark_bar(color=\"darkorange\").encode(\n    x='weekday',\n    y=alt.Y(\"count\",title='Number of purchases')\n)\n\n#Adding data labels\ntext = bars.mark_text(\n    align='center',\n    baseline='middle',\n    dy=-7 ,\n    size=15,\n).encode(\n    text='count',\n    tooltip=[alt.Tooltip('weekday'),\n            alt.Tooltip('count')]\n)\n\n#Combining both\n(bars + text).properties(\n    width=800,\n    height=400,\n    title=\"Number of quanityt purchases across weekdays\"\n)","0f62252f":"#Setting plot style\nplt.figure(figsize = (15, 8))\nplt.style.use('seaborn-white')\n\n#Top 10 fast moving products\nplt.subplot(1,2,1)\nax=sns.countplot(y=\"item\", hue=\"year\", data=groceries, palette=\"pastel\",\n              order=groceries.item.value_counts().iloc[:10].index)\n\nax.set_xticklabels(ax.get_xticklabels(),fontsize=11,rotation=40, ha=\"right\")\nax.set_title('Top 10 Fast moving products',fontsize= 22)\nax.set_xlabel('Total # of items purchased',fontsize = 20) \nax.set_ylabel('Top 10 items', fontsize = 20)\nplt.tight_layout()\n\n#Bottom 10 fast moving products\nplt.subplot(1,2,2)\nax=sns.countplot(y=\"item\", hue=\"year\", data=groceries, palette=\"pastel\",\n              order=groceries.item.value_counts().iloc[-10:].index)\nax.set_xticklabels(ax.get_xticklabels(),fontsize=11,rotation=40, ha=\"right\")\nax.set_title('Bottom 10 Fast moving products',fontsize= 22)\nax.set_xlabel('Total # of items purchased',fontsize = 20) \nax.set_ylabel('Bottom 10 items', fontsize = 20)\nplt.tight_layout()","6e400f7b":"#Getting the top customers based on quantity purchased\ntop_customers=temp[['id', 'qty_purchased','year']].sort_values(by = 'qty_purchased',ascending = False).head(500)\n\n#Converting the datatype of id and year\ntop_customers.id = top_customers.id.astype('category') \ntop_customers.year = top_customers.year.astype('category') \n\n#Plotting\nalt.Chart(top_customers).mark_bar(color=\"darkgreen\").encode(\n    x='qty_purchased',\n    y=alt.Y('id', sort='-x'),\n    color='year',\n    tooltip=['id','qty_purchased']\n).properties(height=400,width=600,title=\"Top Customers\")","98d239da":"#Wordcloud\nwordcloud = WordCloud(\n    width = 3000,\n    height = 2000,\n    background_color = 'white').generate(\"\".join(groceries['item']))\nfig = plt.figure(\n    figsize = (50, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\n\n#Display plot\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","fba87091":"#Creating sparse matrix \nbasket = (temp.groupby(['id', 'item'])['qty_purchased']\n          .sum().unstack().reset_index().fillna(0)\n          .set_index('id'))\n\n#Eoding the quantity urchased\ndef encode(x):\n    '''Encoding the quantity of products with 0s and 1s\n    0:when qty is less than or equal to 0\n    1:when qty is greater than or equal to 1'''\n    if x <= 0:\n        return 0\n    if x >= 1:\n        return 1\n    \n#Appying on our data\nbasket_sets = basket.applymap(encode)\nbasket_sets","f008ad6b":"#Apriori- Support70%\nfrequent_itemsets = apriori(basket_sets, min_support=0.07, use_colnames=True)\n\n#Associaton rules-using lift\nrules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\nrules.tail()","755d96e8":"#Customizable function to change the lift and confidence\ndef rules_mod(lift,confidence):\n    '''rules_mod is a function to control the rules \n    based on lift and confidence threshold'''\n    return rules[ (rules['lift'] >= lift) &\n      (rules['confidence'] >= confidence) ]\n\n#Calling function\nrules_mod(0.7,0.2)","ac199d4b":"#Setting up the style\nplt.figure(figsize = (15, 15))\nplt.style.use('seaborn-white')\n#Plotting the relationship between the metrics\nplt.subplot(221)\nsns.scatterplot(x=\"support\", y=\"confidence\",data=rules)\nplt.subplot(222)\nsns.scatterplot(x=\"support\", y=\"lift\",data=rules)\nplt.subplot(223)\nsns.scatterplot(x=\"confidence\", y=\"lift\",data=rules)\nplt.subplot(224)\nsns.scatterplot(x=\"antecedent support\", y=\"consequent support\",data=rules)","cd0798cd":"'''a function to build a network diagram connecting antecedents and consequents'''\ndef draw_graph(rules, rules_to_show):\n  import networkx as nx  \n  G1 = nx.DiGraph()\n   \n  color_map=[]\n  N = 50\n  colors = np.random.rand(N)    \n  strs=['R0', 'R1', 'R2', 'R3', 'R4', 'R5', 'R6', 'R7', 'R8', 'R9', 'R10', 'R11']   \n   \n   \n  for i in range (rules_to_show):      \n    G1.add_nodes_from([\"R\"+str(i)])\n    \n     \n    for a in rules.iloc[i]['antecedents']:\n                \n        G1.add_nodes_from([a])\n        \n        G1.add_edge(a, \"R\"+str(i), color=colors[i] , weight = 2)\n       \n    for c in rules.iloc[i]['consequents']:\n             \n            G1.add_nodes_from([a])\n            \n            G1.add_edge(\"R\"+str(i), c, color=colors[i],  weight=2)\n \n  for node in G1:\n       found_a_string = False\n       for item in strs: \n           if node==item:\n                found_a_string = True\n       if found_a_string:\n            color_map.append('yellow')\n       else:\n            color_map.append('green')       \n \n \n   \n  edges = G1.edges()\n  colors = [G1[u][v]['color'] for u,v in edges]\n  weights = [G1[u][v]['weight'] for u,v in edges]\n \n  pos = nx.spring_layout(G1, k=16, scale=1)\n  nx.draw(G1, pos, edges=edges, node_color = color_map, edge_color=colors, width=weights, font_size=16, with_labels=False)            \n   \n  for p in pos:  # raise text positions\n           pos[p][1] += 0.07\n  nx.draw_networkx_labels(G1, pos)\n  plt.show()\n\n#Calling function with 10 rules\ndraw_graph(rules, 10)","45849f8b":"rules['lhs items'] = rules['antecedents'].apply(lambda x:len(x) )\nrules[rules['lhs items']>1].sort_values('lift', ascending=False).head()\n\n# Replace frozen sets with strings\nrules['antecedents_'] = rules['antecedents'].apply(lambda a: ','.join(list(a)))\nrules['consequents_'] = rules['consequents'].apply(lambda a: ','.join(list(a)))\n\n# Transform the DataFrame of rules into a matrix using the lift metric\npivot = rules[rules['lhs items']>1].pivot(index = 'antecedents_', \n                    columns = 'consequents_', values= 'lift')\n\n\n# Replace frozen sets with strings\nrules['antecedents_'] = rules['antecedents'].apply(lambda a: ','.join(list(a)))\nrules['consequents_'] = rules['consequents'].apply(lambda a: ','.join(list(a)))\n\n# Transform the DataFrame of rules into a matrix using the lift metric\npivot = rules[rules['lhs items']>1].pivot(index = 'antecedents_', \n                    columns = 'consequents_', values= 'lift')\n\n# Generate a heatmap with annotations on and the colorbar off\nsns.heatmap(pivot, annot = True)\nplt.yticks(rotation=0)\nplt.xticks(rotation=90)\nplt.show()","e288b709":"## Date information ","b06eb217":"><div class=\"alert alert-info\" role=\"alert\">\n><ul>\n><b>Insights<\/b> <br>\n    <li>We have a strong relationship between yogurt,milk and veggies<\/li> \n    <li>Roll buns are highly correlated with whole milk.<\/li> \n><\/ul>\n><\/div>","2aa57c30":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>\n\n# 4. Pre-processing\n## Renaming column","6a83811c":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>\n# 7. Visualizing the results\n> The results in tabular form will not convey much insights into our algorithm so let's visualize the rules\n>\n## Relationship between the metrics","d34fa6aa":">According to dataset information, it has the following features : \n>* **Member_number:** This is like a customer id given to the customer post purchase transaction\n>* **Date:** This is the date at which purchase\/ transaction was made\n>* **itemDescription:** Name of the item which was purchased","50edab0a":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>\n\n# 2. Import libraries","a5683fad":"## Best Product Wordcloud","edb6b5c6":"><div class=\"alert alert-info\" role=\"alert\">\n><ul>\n><b>Insights<\/b> <br>\n    <li>Milk,Bakery items,vegetables and fruits are the top items from the store<\/li> \n    <li>There are many varities of furit products like fruit citrus,fruitwhole etc which makes me believe that this store emphasizes in selling fruit and fruit derivative products<\/li> \n><\/ul>\n><\/div>","db924363":"><div class=\"alert alert-info\" role=\"alert\">\n><ul>\n><b>Insights<\/b> <br>\n    <li>The quantity purchased were highest during the weekend(6-Friday) followed by recording high on Tuesday<\/li> \n    <li>Sunday has the lowest quantity purchase. Is the store open during Sunday?<\/li> \n><\/ul>\n><\/div>","a673f9ae":"<div class=\"alert alert-block alert-info\"> \ud83d\udccc It is important to look at the number of non-null records and their data types.Often the date doesn't have the datetime format.<\/div>","b23f977d":"## Number of quantity purchased across weekdays","a844ad69":"## Top and bottom 10 Fast moving products","79cd5967":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>\n\n# 5. EDA\n## No. of items sold in 2014 and 2015","25b0449d":"><div class=\"alert alert-info\" role=\"alert\">\n><ul>\n><b>Insights<\/b> <br>\n    <li>It is simpler to visualize with the help of network diagram than seeing in a tabular format<\/li> \n    <li>The arrow coming to the rules(yellow circle) is from anticedents and the arrows going from rules circle are towards conequents.<\/li> \n><\/ul>\n><\/div>","4a98e5df":"### Building dynamic function to customize rules","5747d475":"><div class=\"alert alert-info\" role=\"alert\">\n><ul>\n><b>Insights<\/b> <br>\n    <li>3180 id customer has topped the list and has been a loyal customer in both the year<\/li> \n    <li>There can be few customers who are seen to be inconsitent where they have purchased a lot in 2014 and not in 2015 when it comes to customer life expectancy these consistency are considered. Since we have only two year data we can't comment on each customer about their customer life expectancy much<\/li> \n \n><\/ul>\n><\/div>","6aa428c7":"<h2 style=\"text-align:center;font-size:200%;;\">Market Basket Analysis-Association Rule Mining<\/h2>\n<h3  style=\"text-align:center;\">Keywords : <span class=\"label label-success\">Apriori<\/span> <span class=\"label label-success\">Association rules<\/span> <span class=\"label label-success\">Holoviews<\/span> <span class=\"label label-success\">Support<\/span><\/h3>","7c89114b":"# 1. Overview\n## Introduction\n> The era has come where the computer knows better about us than we do. Our device is so powerful that it knows what we are doing right now and what are we going to do in the future. The following application of AI is calleda as Market Basket Analysis which is widely used in the Retail stores where the application predicts the closely associated items we are likely to buy along with the product we purhcased.\n## Project Detail\n>In this project, we use [Groceries dataset](https:\/\/www.kaggle.com\/heeraldedhia\/groceries-dataset),  which has the dataset with 38765 rows of the purchase orders of people from the grocery stores. The dataset has only one csv <br>\n> \n> **CSV name**: Groceries_dataset.csv\n> \n>\n>In these dataset above, I have analysed the dataset with visualizations and perform A rule mining with the help of Apriori algorithm. I have never realized or questioned myself why these items are kept closely in the supermarket, thought that it was for customer's convenience but little did I know that it had a business impact.\n>\n>You can also get this code on my GitHub wall [github page](https:\/\/github.com\/BenRoshan100\/Market-Basket-Analysis).\n\n## Goal of this notebook\n>* Getting and cleaning the data\n>* Understanding the data using EDA techniques\n>* Perform A-rule mining \n>    * using Apriori algorithm\n>* Visualizing the results of association between items","cf4fadae":"## Top Customers in 2014 and 2015","e033c084":"## Cummulative day transactions in 2014 & 2015","30798c56":"><div class=\"alert alert-info\" role=\"alert\">\n><ul>\n><b>Insights<\/b> <br>\n    <li>It is highly noticeable that the prucahse quanity dips low at the end <\/li> \n    <li>The sales quanity purchased is stationary across time, but there is no seasonality nor trend <\/li> \n><\/ul>\n><\/div>","64e6cdd6":"><div class=\"alert alert-info\" role=\"alert\">\n><ul>\n><b>Insights<\/b> <br>\n    <li>Support and confidence has a bleak linear relationship, which means that the most frequent items have some other items associated to it<\/li> \n    <li>When it come to lift the relationship is squashed in support when it goes beyond 0.10 and in confidence there is no relationship<\/li> \n    <li>In antecedent and consequent support relationship there is no linear relationship but it's rather inverse ,when conequent support increases the antecedent support fades out -can we consider this phenomenon as when butter quantity of purchase increases the quantity of bread fades?<\/li>\n><\/ul>\n><\/div>","53aa9e47":"## Strength of association using heatmap\n> We have discovered the nassociation of items, what's  good if we don't know the strength of their relationship","34811f85":"## Monthly quantity purchased from grocery store","0b16d618":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>\n\n# 3. Getting the data","6fff2674":"## Network diagram of rules\n> Here we make network diagram of specified number of rules where we can see the antecedents and consequents connected to the rules","bd32c7d1":"## Applying Apriori\n> Here we apply apriori algorithm and get all the frequent itemsets(with 70% as support threshold) and apply association rules function to derive rules where we use lift metric ","37252922":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>\n# 8. Conclusion\n\n><div class=\"alert alert-success\" role=\"alert\">\n><ul>\n><li>The model built with <b>Apriori algorithms<\/b> <\/li>\n><li>It is considered that a apriori algorithm is effective in association rules application and gives constant results everytime<\/li>\n><\/ul>\n>In this analysis, the model wasn't evaluated with any test data, the following viewpoints should be added to the evaluation.\n><ul>\n><li>Detection of signs of anomalies(rare item)-can help in pushing that item with the associated item<\/li>\n><li>Balance of importance of support and confidence with lift<\/li>\n><li>Model interpretability with visualiations<\/li>\n><\/ul>\n><\/div>","7370098b":"><div class=\"alert alert-info\" role=\"alert\">\n><ul>\n><b>Insights<\/b> <br>\n    <li>The average sales in 2015 is higher than 2014 so we can say that the store revenue is progressing up<\/li> \n    <li>September and February had poor performing sales in 2014 and 2015 respectively as per the data<\/li> \n    <li>The record sale can be observed on October 2015 where the sale quantity has reached nearly 2000 items transacted that month<\/li>\n><\/ul>\n><\/div>","558a203f":"><div class=\"alert alert-info\" role=\"alert\">\n><ul>\n><b>Insights<\/b> <br>\n    <li>There isn't much change in the average across the months the top difference was experienced when the band shift after Jan-Apr months(window-4 months)<\/li> \n    <li>The highest average is obviously between May-Aug month where highest was from June which is mostly the beginning of an academic year for school\/colleges. Is a correlation or causality?<\/li> \n><\/ul>\n><\/div>","22b27798":"><div class=\"alert alert-info\" role=\"alert\">\n><ul>\n><b>Insights<\/b> <br>\n    <li>Milk is the top product purchased in both 2014 and 2015 whereas lowest is preservation product which no one purchased in 2015<\/li> \n    <li>Almost all the top products has seen a rise in 2015 except soda and bottled water<\/li> \n    <li>Most of the bottom products nevr saw a rise in 2015 except whiskey,chicken,bag and baby cosmetics<\/li> \n><\/ul>\n><\/div>","aa80982f":"<a href=\"#top\" class=\"btn btn-success btn-sm active\" role=\"button\" aria-pressed=\"true\" style=\"color:white;\">Table of Contents<\/a>\n# 6. Associate Rule Mining with Apriori Algorithm\n\n## What is market basket analysis?\n![1.PNG](attachment:1.PNG)\n> Have you ever wandered around super market and wondered all the sections and racks are designed in a way that the products are related ? like you can get bread and butter in nearby racks; brush and toothpaste in same racks. These products are associated. If you buy a brush the likelihood of you buying the paste is high. These are marketing tactics to make you fill up the basket with products with their associated items thereby increasing sales revenue. Few business introduce discount in the associated item or combine both the products and sell at a lower rate inorder to make you buy the item+item associated to it \n\n## What is association rule mining?\n> Association rule mining is the technique used to unveil the association between items, where the items we pruchase are denoted as X->Y <br>\n>\n>Here X is the item we buy and Y is the item we most likely to buy (More like if->then) it is also called as \n>* X- Anticedent \n>* Y- Consequent\n>\n> Asociation rule mining helps in designing the rules for the assocation of items. These rules are formed with the help of three terminologies\n\n![](https:\/\/s3.ap-south-1.amazonaws.com\/techleer\/243.jpg)\n>1.<b>Support:<\/b> It signifies the popularity of the item, if an item is less frequently bought then it will be ignored in the association. <br>\n>\n>2.<b>Confidence:<\/b> It tells the likelihood of purchasing Y when X is bought.Sounds more like a conditional probability.Infact it is ! But it fails to check the popularity(frequency) of Y to overcome that we got lift.<br>\n>\n>3.<b>Lift:<\/b> It combines both confidence and support.A lift greater than 1 suggests that the presence of the antecedent increases the chances that the consequent will occur in a given transaction.\nLift below 1 indicates that purchasing the antecedent reduces the chances of purchasing the consequent in the same transaction.\n>\n>\n>For example\n> Assume there are 100 customers where 10 of them bought milk, 8 bought butter and 6 bought both of them. We need to check the association of bought milk => bought butter\n* support = P(Milk & Butter) = 6\/100 = 0.06\n* confidence = support\/P(Butter) = 0.06\/0.08 = 0.75\n* lift = confidence\/P(Milk) = 0.75\/0.10 = 7.5\n\n## What is Apriori?\n>Apriori algorithm uses frequent itemsets to get association rules,but on the assumptions that\n* All subsets of frequent itemsets must be frequent \n* Similarly incase of infrequent subset their parent set is infrequent too\n> The algorithm works in such a way that a minimum support value is set and iterations happen with frequent itemsets. Itemsets and subsets are ignored if their support is below the threshold till there can't be any removal.\n>\n![](https:\/\/imgs.developpaper.com\/imgs\/158566895-ae3e33380c542b03_articlex.png)\n> Later lift of these selected itemsets(rules) are calculated and if the value is below the threshold the rules are eliminated since algorithm may take time to compile if we take all rules","eba8b8fa":"## Preparing the data\n> Before proceeding with apriori we have to prepare the data in a sparse matrix format where products are in column and id as index . Initially we group by based on the quantity purchased and later we encode it with 0s and 1s","4709d3be":"### My other notebooks can be accessed [here](https:\/\/www.kaggle.com\/benroshan\/notebooks)","35f0a430":"# Table of Contents<a id='top'><\/a>\n>1. [Overview](#1.-Overview)  \n>    * [Introduction](#Introduction)\n>    * [Project Detail](#Project-Detail)\n>    * [Goal of this notebook](#Goal-of-this-notebook)\n>1. [Import libraries](#2.-Import-libraries)\n>1. [Getting the data](#3.-Getting-the-data)\n>1. [Pre-processing](#4.-Pre-processing)\n>    * [Renaming column](#Renaming-column)\n>    * [Date information ](#Date-information )\n>1. [EDA](#5.-EDA)  \n>    * [No. of items sold in 2014 and 2015](#No.-of-items-sold-in-2014-and-2015)\n>    * [Cummulative day transactions in 2014 & 2015](#Cummulative-day-transactions-in-2014-&-2015)\n>    * [Monthly quantity purchased from grocery store](#Monthly-quantity-purchased-from-grocery-store)\n>    * [Number of quantity purchased across weekdays](#Number-of-quantity-purchased-across-weekdays)\n>    * [Top and bottom 10 Fast moving products](#Top-and-bottom-10-Fast-moving-products)\n>    * [Top Customers in 2014 and 2015](#Top-Customers-in-2014-and-2015)\n>    * [Best Product Wordcloud](#Best-Product-Wordcloud)\n>    * [Year month and items Suburst](#Year-month-and-items-Suburst)\n>1. [Associate Rule Mining with Apriori Algorithm](#6.-Associate-Rule-Mining-with-Apriori-Algorithm)\n>    * [What is market basket analysis?](#What-is-market-basket-analysis?)\n>    * [What is association rule mining?](#What-is-association-rule-mining?)\n>    * [What is Apriori?](#What-is-Apriori?)\n>    * [Preparing the data](#Preparing-the-data)\n>    * [Applying Apriori](#Applying-Apriori)\n>1. [Visualizing the results](#7.-Visualizing-the-results)\n>    * [Relationship between the metrics](#Relationship-between-the-metrics)\n>    * [Network diagram of rules](#Network-diagram-of-rules)\n>    * [Strength of association using heatmap](#Strength-of-association-using-heatmap)\n>1. [Conclusion](#8.-Conclusion)","613e81ff":">From the information we can identify that\n>* We don't have any null records in the dataset. BAM !\n>* Date column is an object data type. small bam!"}}