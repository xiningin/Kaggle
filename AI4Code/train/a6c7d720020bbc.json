{"cell_type":{"bdafad8c":"code","d3d5e3bd":"code","e430f9cd":"code","6d7bb6a0":"code","951f13e3":"code","c4309180":"code","45626c88":"code","d2ce3a38":"code","9746ab65":"code","0c39e27e":"code","3ad31f76":"code","ec3b6b40":"code","19cf23eb":"markdown","d7ef1d17":"markdown","012649fe":"markdown"},"source":{"bdafad8c":"import requests\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision import transforms as pth_transforms\nimport numpy as np\nfrom PIL import Image\nimport os\nimport numpy as np\nimport pandas as pd\nimport cv2","d3d5e3bd":"patch_size = 8  #8\nmodel = torch.hub.load('facebookresearch\/dino:main', 'dino_vits16')\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")","e430f9cd":"for p in model.parameters():\n    p.requires_grad = False\n        \nmodel.eval()\nmodel.to(device)","6d7bb6a0":"transform = pth_transforms.Compose([\n    pth_transforms.ToTensor(),\n    pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n])","951f13e3":"!ls","c4309180":"data_dir='..\/input\/dandelionimages\/Images\/dandelion'\nfiles = os.listdir(data_dir)\nAttentions_mean=[]\nImg_npy=[]\n\nfor file in files[0:12]:\n    path=os.path.join(data_dir,file)\n    #img_npy = Image.open(path)\n    img_npy = cv2.imread(path)\n    #img_npy = img_npy.convert('RGB')  \n    img_npy=cv2.resize(img_npy,dsize=(600,600),interpolation=cv2.INTER_CUBIC)\n\n    img = transform(img_npy)\n    w, h = img.shape[1] - img.shape[1] % patch_size, img.shape[2] - img.shape[2] % patch_size\n    img = img[:, :w, :h].unsqueeze(0)\n    w_featmap = img.shape[-2] \/\/ patch_size\n    h_featmap = img.shape[-1] \/\/ patch_size\n    attentions = model.get_last_selfattention(img)\n    nh = attentions.shape[1]\n    attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n    val, idx = torch.sort(attentions)\n    val \/= torch.sum(val, dim=1, keepdim=True)\n    cumval = torch.cumsum(val, dim=1)\n\n    threshold = 0.6\n    th_attn = cumval > (1 - threshold)\n    idx2 = torch.argsort(idx)\n\n    for head in range(nh):\n        th_attn[head] = th_attn[head][idx2[head]]\n    th_attn = th_attn.reshape(nh, w_featmap\/\/2, h_featmap\/\/2).float()\n    th_attn = nn.functional.interpolate(th_attn.unsqueeze(0), scale_factor=patch_size, mode=\"nearest\")[0].cpu().numpy()\n    attentions = attentions.reshape(nh, w_featmap\/\/2, h_featmap\/\/2)\n\n    attentions = nn.functional.interpolate(attentions.unsqueeze(0), scale_factor=patch_size, mode=\"nearest\")[0].cpu().numpy()\n    attentions_mean = np.mean(attentions, axis=0)\n\n    Attentions_mean+=[attentions_mean] \n    Img_npy+=[img_npy]","45626c88":"print(len(Attentions_mean))","d2ce3a38":"IMG=np.array(Img_npy)\nprint(IMG.shape)\nATT=np.array(Attentions_mean)\nprint(ATT.shape)\n","9746ab65":"!ls","0c39e27e":"np.save('IMG.npy', IMG)\nnp.save('ATT.npy', ATT)\n","3ad31f76":"IMG0= np.load('IMG.npy')\nprint(IMG0.shape)\nATT0 = np.load('ATT.npy')\nprint(ATT0.shape)\n","ec3b6b40":"for img, att in zip(IMG0,ATT0):\n    plt.figure(figsize=(8,4))\n    plt.subplot(1,2,1)\n    plt.title(\"Input\")\n    plt.imshow(img)\n    plt.axis(\"off\")\n\n    plt.subplot(1,2,2)\n    plt.title(\"Head Mean\")\n    plt.imshow(att)\n    plt.axis(\"off\")\n    plt.tight_layout()","19cf23eb":"# Dandelion Images DINO Vision Transformers ","d7ef1d17":"# img_npy, img","012649fe":"## Visualize"}}