{"cell_type":{"cfc9f3d9":"code","eeb83f83":"code","f3c0bf24":"code","fec3576f":"code","7fc13e6c":"code","24ef4fb2":"code","ef8738c0":"code","6eb2977a":"code","a800a28d":"code","b70edebd":"code","094cafb4":"code","a4c85937":"code","981a94a5":"code","489bb088":"code","f9bda0c4":"code","5620f483":"code","9232bdf7":"code","cf3cb03a":"code","d8f08651":"code","3098dc5e":"code","571996bb":"code","ec5965cd":"markdown","84d2a7b5":"markdown","6926791f":"markdown","727452a6":"markdown","c18e3b26":"markdown","c5b4459d":"markdown","a63bc160":"markdown","8227d4da":"markdown","fc056050":"markdown","3c56ab9e":"markdown","95fb64c3":"markdown","1f48cbfe":"markdown","4f5ac886":"markdown"},"source":{"cfc9f3d9":"denoise = True\n\n\n## copy pretrain model to working dir\nimport shutil\nimport glob\n\n    \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\nimport os\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nprint(tf.__version__)\n#%matplotlib inline\n\nstrategy = tf.distribute.get_strategy()\nprint('Number of devices: {}'.format(strategy.num_replicas_in_sync))","eeb83f83":"root_dir = '..\/input\/stanford-covid-vaccine\/'","f3c0bf24":"import json\nimport glob\nfrom tqdm import tqdm\n\ntrain = pd.read_json(root_dir + \"train.json\",lines=True)\nif denoise:\n    train = train[train.signal_to_noise > 1].reset_index(drop = True)\ntest  = pd.read_json(root_dir + \"test.json\",lines=True)\nsub = pd.read_csv(root_dir + \"sample_submission.csv\")\naug_df = pd.read_csv('..\/input\/covid-aug\/'+ 'aug_data.csv')\n\n\n\n\ndef aug_data(df):\n    target_df = df.copy()\n    new_df = aug_df[aug_df['id'].isin(target_df['id'])]                     \n    del target_df['structure']\n    del target_df['predicted_loop_type']\n    new_df = new_df.merge(target_df, on=['id','sequence'], how='left')\n\n    df['cnt'] = df['id'].map(new_df[['id','cnt']].set_index('id').to_dict()['cnt'])\n    df['log_gamma'] = 100\n    df['score'] = 1.0\n    df = df.append(new_df[df.columns])\n    return df\nprint(train.shape, test.shape)\n\norg_train = train.copy()\ntrain = aug_data(train)\ntest = aug_data(test)\nprint('aug_reulst:',train.shape, test.shape)\n\n\n\ntest_pub = test[test[\"seq_length\"] == 107]\ntest_pri = test[test[\"seq_length\"] == 130]\n\n\nAs = []\nfor id in tqdm(train[\"id\"]):\n    a = np.load(root_dir + f\"bpps\/{id}.npy\")\n    As.append(a)\nAs = np.array(As)\n\nAs_org=[]\nfor id in tqdm(org_train[\"id\"]):\n    a = np.load(root_dir + f\"bpps\/{id}.npy\")\n    As_org.append(a)\nAs_org = np.array(As_org)\n\nAs_pub = []\nfor id in tqdm(test_pub[\"id\"]):\n    a = np.load(root_dir + f\"bpps\/{id}.npy\")\n    As_pub.append(a)\nAs_pub = np.array(As_pub)\nAs_pri = []\nfor id in tqdm(test_pri[\"id\"]):\n    a = np.load(root_dir + f\"bpps\/{id}.npy\")\n    As_pri.append(a)\nAs_pri = np.array(As_pri)","fec3576f":"def read_bpps_sum(df):\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps_arr.append(np.load(root_dir + f\"bpps\/{mol_id}.npy\").sum(axis=1))\n    bpps_arr = np.expand_dims(np.array(bpps_arr), -1)\n    return bpps_arr\n\ndef read_bpps_max(df):\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps_arr.append(np.load(root_dir + f\"\/bpps\/{mol_id}.npy\").max(axis=1))\n    \n    bpps_arr = np.expand_dims(np.array(bpps_arr), -1)\n    return bpps_arr\n\ndef read_bpps_nb(df):\n    # normalized non-zero number\n    # from https:\/\/www.kaggle.com\/symyksr\/openvaccine-deepergcn \n    bpps_nb_mean = 0.077522 # mean of bpps_nb across all training data\n    bpps_nb_std = 0.08914   # std of bpps_nb across all training data\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps = np.load(root_dir + f\"bpps\/{mol_id}.npy\")\n        bpps_nb = (bpps > 0).sum(axis=0) \/ bpps.shape[0]\n        bpps_nb = (bpps_nb - bpps_nb_mean) \/ bpps_nb_std\n        bpps_arr.append(bpps_nb)\n    bpps_arr = np.expand_dims(np.array(bpps_arr), -1)\n    return bpps_arr \n\ntrain_sum = read_bpps_sum(train)\ntest_sum_pub = read_bpps_sum(test_pub)\ntest_sum_pri = read_bpps_sum(test_pri)\ntrain_max = read_bpps_max(train)\ntest_max_pub = read_bpps_max(test_pub)\ntest_max_pri = read_bpps_max(test_pri)\ntrain_nb = read_bpps_nb(train)\ntest_nb_pub = read_bpps_nb(test_pub)\ntest_nb_pri = read_bpps_nb(test_pri)\n\n\n\norg_train_sum = read_bpps_sum(org_train)\norg_train_max = read_bpps_max(org_train)\norg_train_nb = read_bpps_nb(org_train)","7fc13e6c":"targets = list(sub.columns[1:])\nprint(targets)\n\ny_train = []\nseq_len = train[\"seq_length\"].iloc[0]\nseq_len_target = train[\"seq_scored\"].iloc[0]\nignore = -10000\nignore_length = seq_len - seq_len_target\nfor target in targets:\n    y = np.vstack(train[target])\n    dummy = np.zeros([y.shape[0], ignore_length]) + ignore\n    y = np.hstack([y, dummy])\n    y_train.append(y)\ny = np.stack(y_train, axis = 2)\ny.shape","24ef4fb2":"def get_structure_adj(train):\n    Ss = []\n    for i in tqdm(range(len(train))):\n        seq_length = train[\"seq_length\"].iloc[i]\n        structure = train[\"structure\"].iloc[i]\n        sequence = train[\"sequence\"].iloc[i]\n\n        cue = []\n        a_structures = {\n            (\"A\", \"U\") : np.zeros([seq_length, seq_length]),\n            (\"C\", \"G\") : np.zeros([seq_length, seq_length]),\n            (\"U\", \"G\") : np.zeros([seq_length, seq_length]),\n            (\"U\", \"A\") : np.zeros([seq_length, seq_length]),\n            (\"G\", \"C\") : np.zeros([seq_length, seq_length]),\n            (\"G\", \"U\") : np.zeros([seq_length, seq_length]),\n        }\n        a_structure = np.zeros([seq_length, seq_length])\n        for i in range(seq_length):\n            if structure[i] == \"(\":\n                cue.append(i)\n            elif structure[i] == \")\":\n                start = cue.pop()\n#                 a_structure[start, i] = 1\n#                 a_structure[i, start] = 1\n                a_structures[(sequence[start], sequence[i])][start, i] = 1\n                a_structures[(sequence[i], sequence[start])][i, start] = 1\n        \n        a_strc = np.stack([a for a in a_structures.values()], axis = 2)\n        a_strc = np.sum(a_strc, axis = 2, keepdims = True)\n        Ss.append(a_strc)\n    \n    Ss = np.array(Ss)\n    print(Ss.shape)\n    return Ss\nSs = get_structure_adj(train)\nSs_pub = get_structure_adj(test_pub)\nSs_pri = get_structure_adj(test_pri)\n\nSs_org = get_structure_adj(org_train)","ef8738c0":"def get_distance_matrix(As):\n    idx = np.arange(As.shape[1])\n    Ds = []\n    for i in range(len(idx)):\n        d = np.abs(idx[i] - idx)\n        Ds.append(d)\n\n    Ds = np.array(Ds) + 1\n    Ds = 1\/Ds\n    Ds = Ds[None, :,:]\n    Ds = np.repeat(Ds, len(As), axis = 0)\n    \n    Dss = []\n    for i in [1, 2, 4]:\n        Dss.append(Ds ** i)\n    Ds = np.stack(Dss, axis = 3)\n    print(Ds.shape)\n    return Ds\n\nDs = get_distance_matrix(As)\nDs_pub = get_distance_matrix(As_pub)\nDs_pri = get_distance_matrix(As_pri)\n\nDs_org = get_distance_matrix(As_org)","6eb2977a":"## concat adjecent\nAs = np.concatenate([As[:,:,:,None], Ss, Ds], axis = 3).astype(np.float32)\nAs_pub = np.concatenate([As_pub[:,:,:,None], Ss_pub, Ds_pub], axis = 3).astype(np.float32)\nAs_pri = np.concatenate([As_pri[:,:,:,None], Ss_pri, Ds_pri], axis = 3).astype(np.float32)\nAs_org = np.concatenate([As_org[:,:,:,None], Ss_org, Ds_org], axis = 3).astype(np.float32)\n\naslist=[]\nfor i in range(len(As)):\n    aslist.append(As[i])\ntrain['As']=aslist\n\naslist=[]\nfor i in range(len(As_pub)):\n    aslist.append(As_pub[i])\ntest_pub['As']=aslist\n\naslist=[]\nfor i in range(len(As_pri)):\n    aslist.append(As_pri[i])\ntest_pri['As']=aslist\n\naslist=[]\nfor i in range(len(As_org)):\n    aslist.append(As_org[i])\norg_train['As']=aslist\n\n\ndel Ss, Ds, Ss_pub, Ds_pub, Ss_pri, Ds_pri , Ds_org\nAs.shape, As_pub.shape, As_pri.shape\n\nimport gc\ngc.collect()","a800a28d":"## sequence\ndef return_ohe(n, i):\n    tmp = [0] * n\n    tmp[i] = 1\n    return tmp\n\ndef get_input(train , ts, tm, tn):\n    mapping = {}\n    vocab = [\"A\", \"G\", \"C\", \"U\"]\n    for i, s in enumerate(vocab):\n        mapping[s] = return_ohe(len(vocab), i)\n    X_node = np.stack(train[\"sequence\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n\n    mapping = {}\n    vocab = [\"S\", \"M\", \"I\", \"B\", \"H\", \"E\", \"X\"]\n    for i, s in enumerate(vocab):\n        mapping[s] = return_ohe(len(vocab), i)\n    X_loop = np.stack(train[\"predicted_loop_type\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n    \n    mapping = {}\n    vocab = [\".\", \"(\", \")\"]\n    for i, s in enumerate(vocab):\n        mapping[s] = return_ohe(len(vocab), i)\n    X_structure = np.stack(train[\"structure\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n    \n    \n    X_node = np.concatenate([X_node, X_loop], axis = 2)\n    \n    ## interaction\n    a = np.sum(X_node * (2 ** np.arange(X_node.shape[2])[None, None, :]), axis = 2)\n    vocab = sorted(set(a.flatten()))\n    print(vocab)\n    ohes = []\n    for v in vocab:\n        ohes.append(a == v)\n    ohes = np.stack(ohes, axis = 2)\n    X_node = np.concatenate([X_node, ohes, ts, tm, tn], axis = 2).astype(np.float32)\n    \n    xnodelist =[]\n    for i in range(len(X_node)):\n        xnodelist.append(X_node[i])\n    train['X_node']= xnodelist\n    return train, X_node\n\ntrain, X_node = get_input(train, train_sum, train_max, train_nb)\ntest_pub, _ = get_input(test_pub, test_sum_pub, test_max_pub, test_nb_pub)\ntest_pri, _ = get_input(test_pri, test_sum_pri, test_max_pri, test_nb_pri)\n\norg_train, X_node_org = get_input(org_train, org_train_sum, org_train_max, org_train_nb)\n","b70edebd":"import tensorflow as tf\nfrom tensorflow.keras import layers as L\nimport tensorflow_addons as tfa\nfrom tensorflow.keras import backend as K\n\ndef mcrmse_5loss(t, y, seq_len_target = seq_len_target ):\n    t = t[:, : seq_len_target]\n    y = y[:, :seq_len_target]\n    loss = tf.reduce_mean(tf.sqrt(tf.reduce_mean((t - y) ** 2, axis = 2)))\n    return loss\n\ndef mcrmse_3loss(true, pred):\n    t0 = true[:, :68, 0]\n    y0 = pred[:, :68, 0]\n    loss1 = tf.reduce_mean(tf.sqrt((t0 - y0) ** 2))\n    t1 = true[:, :68, 1]\n    y1 = pred[:, :68, 1]\n    loss2 = tf.reduce_mean(tf.sqrt((t1 - y1) ** 2))\n    t3 = true[:, :68, 3]\n    y3 = pred[:, :68, 3]\n    loss3 = tf.reduce_mean(tf.sqrt((t3 - y3) ** 2))\n    return (loss1+loss2+loss3)\/3\n\ndef attention(x_inner, x_outer, n_factor, dropout):\n    x_Q =  L.Conv1D(n_factor, 1, activation='linear', \n                  kernel_initializer='glorot_uniform',\n                  bias_initializer='glorot_uniform',\n                 )(x_inner)\n    x_K =  L.Conv1D(n_factor, 1, activation='linear', \n                  kernel_initializer='glorot_uniform',\n                  bias_initializer='glorot_uniform',\n                 )(x_outer)\n    x_V =  L.Conv1D(n_factor, 1, activation='linear', \n                  kernel_initializer='glorot_uniform',\n                  bias_initializer='glorot_uniform',\n                 )(x_outer)\n    x_KT = L.Permute((2, 1))(x_K)\n    res = L.Lambda(lambda c: K.batch_dot(c[0], c[1]) \/ np.sqrt(n_factor))([x_Q, x_KT])\n#     res = tf.expand_dims(res, axis = 3)\n#     res = L.Conv2D(16, 3, 1, padding = \"same\", activation = \"relu\")(res)\n#     res = L.Conv2D(1, 3, 1, padding = \"same\", activation = \"relu\")(res)\n#     res = tf.squeeze(res, axis = 3)\n    att = L.Lambda(lambda c: K.softmax(c, axis=-1))(res)\n    att = L.Lambda(lambda c: K.batch_dot(c[0], c[1]))([att, x_V])\n    return att\n\ndef multi_head_attention(x, y, n_factor, n_head, dropout):\n    if n_head == 1:\n        att = attention(x, y, n_factor, dropout)\n    else:\n        n_factor_head = n_factor \/\/ n_head\n        heads = [attention(x, y, n_factor_head, dropout) for i in range(n_head)]\n        att = L.Concatenate()(heads)\n        att = L.Dense(n_factor, \n                      kernel_initializer='glorot_uniform',\n                      bias_initializer='glorot_uniform',\n                     )(att)\n    x = L.Add()([x, att])\n    x = L.LayerNormalization()(x)\n    if dropout > 0:\n        x = L.Dropout(dropout)(x)\n    return x\n\ndef res(x, unit, kernel = 3, rate = 0.1, use_seper=False):\n    if use_seper==False:\n        h = L.Conv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)\n    else:\n        h = L.SeparableConv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)        \n    h = L.LayerNormalization()(h)\n#     h = L.BatchNormalization()(h)\n    h = L.LeakyReLU()(h)\n    h = L.Dropout(rate)(h)\n    return L.Add()([x, h])\n\ndef forward(x, unit, kernel = 3, rate = 0.1, use_seper=False):\n#     h = L.Dense(unit, None)(x)\n    if use_seper==False:\n        h = L.Conv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)\n    else:\n        h = L.SeparableConv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)        \n    h = L.LayerNormalization()(h)\n#     h = L.BatchNormalization()(h)\n    h = L.Dropout(rate)(h)\n#         h = tf.keras.activations.swish(h)\n    h = L.LeakyReLU()(h)\n    h = res(h, unit, kernel, rate, use_seper=use_seper)\n    return h\n\ndef adj_attn(x, adj, unit, n = 2, rate = 0.1):\n    x_a = x\n    x_as = []\n    for i in range(n):\n        x_a = forward(x_a, unit)\n        x_a = tf.matmul(adj, x_a)\n        x_as.append(x_a)\n    if n == 1:\n        x_a = x_as[0]\n    else:\n        x_a = L.Concatenate()(x_as)\n    x_a = forward(x_a, unit)\n    return x_a\n\ndef adj_conv_learn(adj, kernel_size = 15):\n    adj_id = L.Conv2D(4, kernel_size, 1, padding = \"same\", activation = 'relu')(adj) \n    adj_learned = L.Conv2D(4, kernel_size, 1, padding = \"same\", activation = 'relu')(adj_id)     \n    adj_learned = L.Add()([adj_id, adj_learned])\n    adj_learned = L.BatchNormalization()(adj_learned)\n    return adj_learned\n    \n    \ndef get_base(config):\n    node = tf.keras.Input(shape = (None, X_node.shape[2]), name = \"node\")\n    adj = tf.keras.Input(shape = (None, None, As.shape[3]), name = \"adj\")\n#     adj_learned = adj_conv_learn(adj)\n    \n    adj_learned = L.Dense(1, \"relu\")(adj)\n    adj_all = L.Concatenate(axis = 3)([adj, adj_learned])\n        \n    xs = []\n    xs.append(node)\n    \n    x1 = forward(node, 128, kernel = 3, rate = config['inner_dropout'],use_seper = config['use_seper'])\n    x2 = forward(x1, 64, kernel = 6, rate = config['inner_dropout'],use_seper = config['use_seper'])\n    x3 = forward(x2, 32, kernel = 15, rate = config['inner_dropout'],use_seper = config['use_seper'])\n    x4 = forward(x3, 16, kernel = 30, rate = config['inner_dropout'],use_seper = config['use_seper'])\n\n    x = L.Concatenate()([x1, x2, x3, x4])\n\n    \n    for unit in [64, 32]:\n        x_as = []\n        for i in range(adj_all.shape[3]):\n            x_a = adj_attn(x, adj_all[:, :, :, i], unit, rate = 0.0)\n            x_as.append(x_a)\n        x_c = forward(x, unit, kernel = 30)\n        \n        x = L.Concatenate()(x_as + [x_c])\n        x = forward(x, unit)\n#         x = multi_head_attention(x, x, unit, 4, 0.0)\n        x = multi_head_attention(x, x, unit, 4, 0.0)\n        xs.append(x)\n        \n    x = L.Concatenate()(xs)\n\n    model = tf.keras.Model(inputs = [node, adj], outputs = [x])\n    return model\n\n\n\ndef get_ae_model(base, config):\n    node = tf.keras.Input(shape = (None, X_node.shape[2]), name = \"node\")\n    adj = tf.keras.Input(shape = (None, None, As.shape[3]), name = \"adj\")\n\n    x = base([L.SpatialDropout1D(0.3)(node), adj])\n    x = forward(x, 64, rate = 0.3)\n    p = L.Dense(X_node.shape[2], \"linear\")(x)\n    \n    print(node.shape, p.shape)\n    loss = tf.reduce_mean( (node - p)**2)\n    \n    model = tf.keras.Model(inputs = [node, adj], outputs = [loss])\n\n    opt = tf.optimizers.Adam()\n\n\n    model.compile(optimizer = opt, loss = lambda t, y : y)\n    return model\n\n\ndef get_model(base, config):\n    node = tf.keras.Input(shape = (None, X_node.shape[2]), name = \"node\")\n    adj = tf.keras.Input(shape = (None, None, As.shape[3]), name = \"adj\")\n    x = base([node, adj])\n#     x = L.GlobalAvgPool1D()(x)\n    for i in range(config['last_layer_n']):\n        x = forward(x, config['last_forward_n'], rate = config['last_dropout'])\n    x = L.Dense(5, None)(x)\n\n    model = tf.keras.Model(inputs = [node, adj], outputs = [x])\n    \n\n    return model","094cafb4":"config = dict(use_seper=True)\nconfigs=[]\nconfigs.append(config)\ntest_profile='gnn_attn_cnn_debug_lastremove'\nimport os\nsave_dir = f'.\/weights\/{test_profile}'\ntry:\n    os.mkdir(save_dir)\nexcept:\n    print('already exist dir')\n    \ndebug =False\nif debug ==True:\n    ae_epochs = 1\n    ae_epochs_each = 1\n    ae_batch_size = 32\n\n    epochs_list = [1]\n    batch_size_list = [32]\nelse:\n    ae_epochs = 20\n    ae_epochs_each = 5\n    ae_batch_size = 32\n\n    batch_size_list=[8, 16, 32, 64, 128, 256]\n    epochs_list = [10, 10, 10, 20, 30, 30]","a4c85937":"def train_base_ae(save_dir, config):\n    print('train_base_ae', save_dir, config)\n    base = get_base(config)\n    with strategy.scope():\n        ae_model = get_ae_model(base, config)\n    ## TODO : simultaneous train\n    for i in range(ae_epochs\/\/ae_epochs_each):\n        print(f\"------ {i} ------\")\n        print(\"--- train ---\")\n        ae_model.fit([np.array(train['X_node'].tolist()), np.array(train['As'].tolist())], [np.array(train['X_node'].tolist())[:,0]],\n                  epochs = ae_epochs_each,\n                  batch_size = ae_batch_size)\n        \n        print(\"--- public ---\")\n        ae_model.fit([np.array(test_pub['X_node'].tolist()), np.array(test_pub['As'].tolist())], [np.array(test_pub['X_node'].tolist())[:,0]],\n                  epochs = ae_epochs_each,\n                  batch_size = ae_batch_size)\n        print(\"--- private ---\")\n        ae_model.fit([np.array(test_pri['X_node'].tolist()), np.array(test_pri['As'].tolist())], [np.array(test_pri['X_node'].tolist())[:,0]],\n                  epochs = ae_epochs_each,\n                  batch_size = ae_batch_size)\n        gc.collect()\n    print(\"****** save ae model ******\")\n    base.save_weights(f\"{save_dir}\/base_ae.h5\")\n    \n# debug=True\nif debug==True:\n    config['inner_dropout']=0.2\n    config['last_dropout']=0.0\n    train_base_ae(save_dir, config)","981a94a5":"from sklearn.model_selection import KFold,GroupKFold\nfrom sklearn.cluster import KMeans\n# kfold = KFold(5, shuffle = True, random_state = 42)\nn_folds=5\nseed=42\n# clustering for  GroupKFold\nkmeans_model = KMeans(n_clusters=200, random_state=111).fit((X_node_org)[:,:,0])\norg_train['cluster_id'] = kmeans_model.labels_\ngkf = GroupKFold(n_splits=n_folds)\nif 'cluster_id' not in train.columns:\n    train = train.merge(org_train[['id', 'cluster_id']], how='left', on='id')","489bb088":"from tensorflow.keras.utils import Sequence\nclass CovidGenerator(Sequence):\n    def __init__(self, Node, Adj, y, sn_tr, batch_size, shuffle=True, crop_length=107, min_crop=68\n                 , crop_ratio=0.5, left_padd=0):\n        self.Node = Node\n        self.Adj = Adj\n        self.y = y\n        self.w_trn = np.abs(np.array(np.log(train.signal_to_noise+1.104)\/np.log(train.signal_to_noise+1.104).mean()))\n        self._batch_size = batch_size\n        self._list_idx = [i for i in range(len(Node))]\n        self._shuffle = shuffle\n        self.crop_length = crop_length\n        self.min_crop = min_crop\n        self.crop_ratio = crop_ratio\n        self.left_padd=left_padd\n        self.on_epoch_end()  \n        \n    def __len__(self):\n        return int(np.ceil(len(self.Node)\/self._batch_size))\n    \n    def __getitem__(self, index):\n        batch_idx = self._indices[index*self._batch_size:(index+1)*self._batch_size]\n        X_nodes = []\n        Adjs =[]\n        ys =[]\n        ws = []\n\n        cur_crop_len = self.crop_length\n        \n                \n        for i in batch_idx:\n            X_nodes.append(np.pad(self.Node[i],((self.left_padd,0),(0,0)))[:cur_crop_len])\n            Adjs.append(np.pad(self.Adj[i],((self.left_padd,0),(self.left_padd,0),(0,0)))[:cur_crop_len,:cur_crop_len])\n            ys.append(np.pad(self.y[i],((self.left_padd,0),(0,0)))[:cur_crop_len])\n            ws.append(self.w_trn[i])            \n        return ([np.array(X_nodes), np.array(Adjs)], np.array(ys), np.array(ws))\n    \n    \n    def on_epoch_end(self):\n        self._indices = np.concatenate([np.arange(len(self._list_idx)),np.arange(len(self._list_idx))])\n        if self._shuffle:\n            np.random.shuffle(self._indices)","f9bda0c4":"# train_gen = CovidGenerator(X_node, As, y,train.signal_to_noise, 64, shuffle=True, min_crop=68, crop_length=91, crop_ratio=1.0, left_padd=0)\n# kk = train_gen.__getitem__(0)\n# kk[0][0].shape","5620f483":"def mcrmse(t, y, seq_len_target = seq_len_target ):\n    t = t[:, :seq_len_target]\n    y = y[:, :seq_len_target]\n    loss = np.mean(np.sqrt(np.mean((t - y) ** 2, axis = 2)))\n    print(loss)\n    return np.mean(loss)","9232bdf7":"cv_scores=[]\n\ndef train_main_cv_model(save_dir, config, foldid=None, trainskip=False):\n#for i, (tr_idx, va_idx) in enumerate(kfold.split(X_node, As)):\n    print('train_main_model', save_dir, config)\n    scores = []\n    preds = np.zeros([len(X_node), X_node.shape[1], 5])\n    for i, (tr_idx, va_idx) in enumerate(gkf.split(train, train['reactivity'], train['cluster_id'])):\n        print(f\"------ fold {i} start -----\")\n#         if i<= config['skip_fold']:\n#             print('config[skip_fold]',config['skip_fold'])\n#             continue\n        \n        print(i, foldid)\n        if foldid is not None:\n            if i!= foldid:\n                continue\n        \n        X_node_tr = np.array(train.X_node.tolist())[tr_idx]#X_node[tr_idx]\n        X_node_va = np.array(train.X_node.tolist())[va_idx]#X_node[va_idx]\n        As_tr = np.array(train.As.tolist())[tr_idx]##As[tr_idx]\n        As_va = np.array(train.As.tolist())[va_idx]#As[va_idx]\n        sn_tr = train.loc[tr_idx].signal_to_noise\n        y_tr = y[tr_idx]\n        y_va = y[va_idx]\n        \n\n        base = get_base(config)\n        if ae_epochs > 0:\n            print(\"****** load ae model ******\")\n            base.load_weights(f\"{save_dir}\/base_ae.h5\")\n        with strategy.scope():\n            model = get_model(base, config)\n        w_path = f'{save_dir}\/model{i}.h5'\n        if trainskip==True:\n            model.load_weights(w_path)\n            val_pred = model.predict([X_node_va, As_va], verbose=1) \n            scores.append(mcrmse(y_va, val_pred))\n            continue\n        \n\n        checkpoint = tf.keras.callbacks.ModelCheckpoint(w_path,monitor='val_mcrmse_3loss', verbose=1, save_best_only=True, mode='min')\n\n\n        val=train.loc[va_idx]\n    #     print('val=train[va_idx]', val.shape)        \n        val=val[val.SN_filter == 1]\n        X_node_va = X_node[val.index]\n        As_va =As[val.index]\n        y_va = y[val.index]\n\n        for bidx, (epochs, batch_size) in enumerate(zip(epochs_list, batch_size_list)):\n            print(f\"epochs : {epochs}, batch_size : {batch_size}\")\n            if bidx != 0:\n                model.load_weights(w_path)\n\n            lr = 0.001\n            lrs = np.arange(lr, lr*0.01, -(lr)\/epochs)\n    \n           \n            for ep in range(epochs):\n                cur_lr = lrs[ep]\n                left_padd = np.random.randint(24)\n                cur_crop_len = 107\n                if np.random.rand() < config['crop_ratio']:\n                    cur_crop_len = np.random.randint(config['min_crop'] + left_padd, 108)\n                print('left_padd', left_padd, 'cur_crop_len',cur_crop_len)\n                train_gen = CovidGenerator( X_node_tr, As_tr, y_tr, sn_tr, batch_size\n                                           , shuffle=True,  crop_length=cur_crop_len\n#                                            , crop_ratio=config['crop_ratio']min_crop=config['min_crop'],\n                                          ,left_padd=left_padd)\n                \n                print('current lr', cur_lr)\n                opt = tf.optimizers.Adam(learning_rate=  cur_lr)         \n                def mcrmse_loss(t, y, seq_len_target = seq_len_target , left_padd=left_padd):\n                    t = t[:, left_padd:left_padd+ seq_len_target]\n                    y = y[:, left_padd:left_padd+ seq_len_target]\n                    loss = tf.reduce_mean(tf.sqrt(tf.reduce_mean((t - y) ** 2, axis = 2)))\n                    return loss\n                model.compile(optimizer = opt, loss = mcrmse_loss, metrics=[mcrmse_5loss, mcrmse_3loss] )\n                \n                history = model.fit( train_gen, # x = [X_node_tr, As_tr], y= [y_tr], batch_size = batch_size,\n                          validation_data=([X_node_va, As_va], [y_va]),\n                          epochs = ep+1,callbacks=[checkpoint],\n                          verbose=1,max_queue_size=1, initial_epoch=ep)\n\n\n        scores.append(min(history.history['val_mcrmse_5loss']))\n\n    print(scores)\n    return model, np.mean(scores)\n\n\nif debug==True:\n    config={'use_seper': True, 'inner_dropout':0.2, 'last_dropout':0.0}\n    model, cv_score = train_main_cv_model(save_dir, config)\n    print(np.mean(cv_score))","cf3cb03a":"def predict(save_dir, model):\n    p_pub = 0\n    p_pri = 0\n    \n    X_node_pub = np.array(test_pub['X_node'].tolist())\n    X_node_pri = np.array(test_pri['X_node'].tolist())\n    As_pub = np.array(test_pub['As'].tolist())\n    As_pri = np.array(test_pri['As'].tolist())\n    for i in range(n_folds):\n        w_path = f'{save_dir}\/model{i}.h5'        \n        model.load_weights(w_path) \n        p_pub += model.predict([X_node_pub, As_pub], verbose=1) \/ n_folds\n        p_pri += model.predict([X_node_pri, As_pri], verbose=1) \/ n_folds\n\n    for i, target in enumerate(targets):\n        test_pub[target] = [list(p_pub[k, :, i]) for k in range(p_pub.shape[0])]\n        test_pri[target] = [list(p_pri[k, :, i]) for k in range(p_pri.shape[0])]\n\n    preds_ls = []\n    for df, preds in [(test_pub, p_pub), (test_pri, p_pri)]:\n        for i, uid in enumerate(df.id):\n            single_pred = preds[i]\n            single_df = pd.DataFrame(single_pred, columns=targets)\n            single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n            preds_ls.append(single_df)\n            \n    preds_df = pd.concat(preds_ls).groupby('id_seqpos').mean().reset_index()\n    preds_df.to_csv(f\"{save_dir}\/submission.csv\", index = False)\n    return preds_df\n\n\nif debug==True:            \n    preds_df = predict(save_dir,model)","d8f08651":"def train_and_predict(save_dir, config, skip_ae=False, foldid=None, trainskip=False):\n    print('train_and_predict', save_dir, config)\n    try:\n        os.mkdir(save_dir)\n    except:\n        print('already exist dir')\n    if skip_ae==False:\n        train_base_ae(save_dir, config)\n        \n    model, cv_score = train_main_cv_model(save_dir, config, foldid=foldid, trainskip=trainskip)\n    \n    if foldid is not None:\n        print('individual fold train end')\n        return None\n    \n    sub_df = predict(save_dir, model)\n    with open(f'{save_dir}\/scores.txt', mode='at') as score_memo:\n        logstr = f'{config},{cv_score}'\n        score_memo.write(logstr+'\\r')\n    return sub_df\n\n# if debug == True:\n#     train_and_predict(save_dir, config)","3098dc5e":"def dict_to_path(tt):\n    path_str =''\n    for key, value in tt.items():\n        path_str+= str(key)+str(value)\n    return path_str","571996bb":"configs=[]\nprofiles=[]\nconfigs.append(dict(use_seper=False, inner_dropout = 0.2, last_dropout=0.0\n                    , crop_ratio=0.3, last_forward_n=512, min_crop=68, last_layer_n =1))\nprofiles.append('gnncnnpaddcrop_'+ dict_to_path(configs[-1]))\n# configs.append(dict(use_seper=True, inner_dropout = 0.2, last_dropout=0.0\n#                     , crop_ratio=0.3, last_forward_n=512, min_crop=68, last_layer_n =1))\n# profiles.append('gnncnnpaddcrop_'+ dict_to_path(configs[-1]))\n\nprint(len(profiles))\nfor profile, config in zip(profiles, configs):\n    save_dir = f'.\/weights\/{profile}'\n    sub_df = train_and_predict(save_dir, config)\n#     sub_df = train_and_predict(save_dir, config, skip_ae=True, foldid=None, trainskip=True)\n    ","ec5965cd":"## Generator","84d2a7b5":"## train","6926791f":"## predict","727452a6":"## target","c18e3b26":"#### 16th place solution. Single Model. Private Score 0.40760, Public Score 0.24019.\n\nIn the Kaggle notebook environment, you will run out of memory. \n\nUpload it to the notebook environment for code sharing.\n\nIf you want to use it, please use it after downloading.\n","c5b4459d":"# score update~!\n\nno - crop and padd : 0.35215\n\ncrop : 0.35274\n\ncrop and padd : 0.35175","a63bc160":"## structure adj","8227d4da":"## pretrain","fc056050":"## load","3c56ab9e":"## distance adj","95fb64c3":"## model","1f48cbfe":"## node","4f5ac886":"## fork. [covid] AE pretrain + GNN + Attn + CNN  @mrkmakr  \n\nI started with this notebook. Thanks.  Please upvote original notebook."}}