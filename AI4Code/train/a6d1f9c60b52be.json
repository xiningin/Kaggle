{"cell_type":{"2190d2a3":"code","8bf1c7b8":"code","53e30257":"code","9152598e":"code","db2e286f":"code","21a83070":"code","e0532725":"code","57273cee":"code","b257b09b":"code","6e6ec459":"code","c1547623":"code","925f9c09":"code","528b898a":"code","fdbd33fd":"code","61edffd8":"code","208c82bc":"markdown","e2ffd184":"markdown","6fdb0f35":"markdown","eb0a89b9":"markdown","939f472b":"markdown","605be55d":"markdown","6a00768c":"markdown","d48c99b2":"markdown","d3232194":"markdown","ff5f5ce5":"markdown","b380c8fe":"markdown","21ee96ce":"markdown","f21ebb3b":"markdown","edb51ee1":"markdown","9ddf6400":"markdown","38996814":"markdown","ce02ebd1":"markdown","e81169bc":"markdown","4e05cb9f":"markdown","99fa86ba":"markdown","066bb158":"markdown","e36b8f3d":"markdown","8cd5d7f2":"markdown","2c42e5df":"markdown","a31cb842":"markdown","42f26310":"markdown"},"source":{"2190d2a3":"from keras.datasets import mnist\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense","8bf1c7b8":"\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()","53e30257":"print('Training data: ', train_images.shape, train_labels.shape)\n\nprint('Testing data: ', test_images.shape, test_labels.shape)","9152598e":"print(train_labels[0:20])","db2e286f":"#Unique numbers from the train labels\ntotal_classes = np.unique(train_labels)\nnumber_of_total_classes = len(total_classes)\nprint('Total number of output classes : ', number_of_total_classes)\nprint('All Output classes : ', total_classes)","21a83070":"# Plotting some sample data\nplt.figure(figsize=[10,5])\n# Displaying the first image in training data\nplt.subplot(121)\nplt.imshow(train_images[0,:,:], cmap='gray')\nplt.title(\"Label : {}\".format(train_labels[0]))\n","e0532725":"# Displaying the first image in testing data\nplt.figure(figsize=[10,5])\nplt.subplot(122)\nplt.imshow(test_images[0,:,:], cmap='gray')\nplt.title(\"Label : {}\".format(test_labels[0]))","57273cee":"# Change from matrix to array of dimension 28x28 to array of dimention 784\nnewdimnsionData = np.prod(train_images.shape[1:])\ntrain_data = train_images.reshape(train_images.shape[0], newdimnsionData)\ntest_data = test_images.reshape(test_images.shape[0], newdimnsionData)\nprint(train_data.shape)\nprint(test_data.shape)","b257b09b":"# Change to float datatype\ntrain_data = train_data.astype('float32')\ntest_data = test_data.astype('float32')\n\n# Scale the data to lie between 0 to 1\ntrain_data \/= 255\ntest_data \/= 255","6e6ec459":"# Change the labels from integer to categorical data\ntrain_labels_one_hot = to_categorical(train_labels)\ntest_labels_one_hot = to_categorical(test_labels)\n\n# Display the change for category label using one-hot encoding\nprint('Original label 6 : ', train_labels[10])\nprint('After conversion to categorical ( one-hot ) : ', train_labels_one_hot[10])","c1547623":"total_classes","925f9c09":"model = Sequential()\nmodel.add(Dense(512, activation='relu', input_shape=(newdimnsionData,)))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(number_of_total_classes, activation='softmax'))","528b898a":"model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])","fdbd33fd":"history = model.fit(train_data, train_labels_one_hot, batch_size=256, epochs=10, verbose=1, \n                   validation_data=(test_data, test_labels_one_hot))","61edffd8":"[test_loss, test_acc] = model.evaluate(test_data, test_labels_one_hot)\nprint(\"Evaluation result: Loss = {}, accuracy = {}\".format(test_loss, test_acc))","208c82bc":"# How an ANN learns from data: <br>\n***Weight:*** <br><br>\nEach neuron is given a numeric weight. A node takes the input data and multiply it by an assigned weight value, then adds a bias before passing the data to the next layer. Neural networks are trained by fine-tuning weights. And the goal is to discover the optimal set of weights that generates the most accurate prediction.<br><br>\n***Error Function:*** <br><br>\nError function tells how far the actual output of the current model is from the correct output. When we train a model the objective is to minimize the error function and bring output as close as possible to the correct value. <br><br>\n***Forward Pass:***<br><br>\nThe forward pass takes the inputs. Then passes those inputs through the network and allows each node to react to a fraction of the input. Each node generates its outputs and passes them on to the next layer until the network generates an output. <br><br>\n***Backpropagation:*** <br><br>\nBackpropagation is the method of fine-tuning the weights of a neural network based on the error rate obtained in the previous iteration. Backpropagation tracks the derivatives of the activation functions in each successive neuron, to find weights that brings the loss function to a minimum. This method helps to calculate the gradient of a loss function with respect to all the weights in the network. <br>\nNow, we have sufficient knowledge to build a neural network model.","e2ffd184":"***Artificial Neural Networks (ANN):*** <br>\nArtificial Neural Networks (ANN) is a supervised learning system. It is built of a large number of connected units or nodes, called neurons or perceptrons. <br><br>\n***Neuron:*** <br>\nA neuron is a computational unit. Each neuron can make simple decisions and feeds those decisions to other neurons, organized in interconnected layers.\n<img src =\"https:\/\/miro.medium.com\/max\/600\/1*HPu_0vrTsPIui0sgeJLV-A.png\">\nsource: https:\/\/www.researchgate.net\/figure\/Basic-artificial-neural-network-cell-artificial-neuron_fig13_286020106","6fdb0f35":"First, we will convert image matrix(28 * 28) to an array(28*28 = 784 dimensional). This will be fed to the network as a single feature.","eb0a89b9":"# Let\u2019s Start:","939f472b":"***Activation Functions:*** <br><br>\nActivation functions help generate output values within an acceptable range, and their non-linear form is crucial for training the network. Each neuron accepts part of the input and passes it through the activation function. Common activation functions are sigmoid, TanH, ReLu and Softmax. <br>\n***i) Sigmoid:*** Sigmoid activation function is form of **f(x) = 1 \/ 1 + exp(-x)** . Its range is between 0 and 1. <br>\n***ii) TanH:*** TanH activation function is form of **f(x) = 1 \u2014 exp(-2x) \/ 1 + exp(-2x)**. Its range is between -1and 1.<br>\n***iii) ReLu:*** TanH activation function is form of **f(x) = 1 \u2014 exp(-2x) \/ 1 + exp(-2x)**. Its range is between -1and 1. <br>\n<img src=\"https:\/\/miro.medium.com\/max\/700\/1*ERbJqGSdcgZRA1Z__oRFtw.png\" ><br>\nsource: https:\/\/www.researchgate.net\/figure\/Activation-function-plots-for-Sigmoid-Tanh-and-ReLU_fig4_337173375 <br>\n***iv) Softmax:*** Softmax function we generally use in output layers. It is a function that takes as input a vector of K real numbers and normalizes it into a probability distribution.","605be55d":"Thank you so much, if you like this notebook please upvote it.","6a00768c":"# Build an Image Classifier using Deep Neural Network and Keras","d48c99b2":"Now, we will make out data to float and will scale the values between 0 to 1.","d3232194":"***Model Configuration:***","ff5f5ce5":"Here we will use two hidden layers and an output layer with 10 units. The number of nodes in each hidden layer is 512. The input to the network is the 784-dimensional array converted from the 28\u00d728 image.","b380c8fe":"We will convert the labels from integer to categorical ( one-hot encoding). to_categorical function from Keras library will help to do that.","21ee96ce":"***Model Evaluation:***","f21ebb3b":"***Model Initialization:***","edb51ee1":"Now we will plot some sample images from training and test sets also we will see the labels defined for those images in our dataset.plt.figure(figsize=[10,5])","9ddf6400":"***Model Training:***","38996814":"***Layers:*** <br><br>\nA \u201cshallow\u201d neural network has only three layers of neurons.<br>\ni) Input layer <br>\nii) Hidden Layer and<br>\niii) Output Layer<br>\nA Deep Neural Network has a similar structure. But it has two or more hidden layers of neurons that process inputs. <br>\n<img src=\"https:\/\/miro.medium.com\/max\/511\/1*BMdnuQUqGEb_FSEmYIxw5g.jpeg\"> <br>\nsource: https:\/\/cs231n.github.io\/neural-networks-1\/","ce02ebd1":"***Load Data:***","e81169bc":"Now we will solve a digit classification problem based on given different digit images. <br><br>\n***Data Description:*** <br><br>\nMNIST is a commonly used handwritten digit dataset consisting of 60,000 images in the training set and 10,000 images in the test set. The digits are size-normalized and centered in a fixed-size ( 28\u00d728 ) image. Now we will build a neural network model to recognize a new sample from the test set correctly.<br><br>\n***Importing all the required libraries:***\n","4e05cb9f":"<img src=\"https:\/\/miro.medium.com\/max\/700\/1*9ETNMLoiJL3zhXDsFQMqLw.png\">","99fa86ba":"# ***Objective:*** <br>\nIn this notebook, we will learn about the architecture of an Artificial Neural Network. Then we will build a deep neural network model that can be able to classify digit images using Keras. So this is a very good start for the beginner. And for this application, we will use the open-source Keras MNIST dataset.","066bb158":"# Let\u2019s Start Coding: <br>","e36b8f3d":"We can improve system accuracy by using a Convolutional Neural Network.","8cd5d7f2":"So, in our data, we have 60000 training samples and 10000 test samples. Each sample is a 28\u00d728 grayscale image. And in the label, we have handwritten numbers ranging from 0 to 9.","2c42e5df":"***Data Preprocessing:***","a31cb842":"Displaying the first image in test data:","42f26310":"Displaying the first image in training data:"}}