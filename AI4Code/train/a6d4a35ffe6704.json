{"cell_type":{"ab2dabda":"code","871587a8":"code","15cc1780":"code","67586c82":"code","e4c07a39":"code","97b55109":"code","19a32c18":"code","4e107c18":"code","6e73564b":"code","bc69bd67":"code","cffa3ebc":"code","1993eddf":"code","884d4bfc":"code","d3b14b92":"code","fc405f8c":"code","24de9ed8":"code","e6c6405b":"code","db3397a0":"code","45e71c8a":"code","8a1b3137":"markdown","9315ed7a":"markdown","ac795456":"markdown","d3a8e136":"markdown","2c6a19ca":"markdown","e3cde553":"markdown","343facec":"markdown","88eca73e":"markdown","bb54792f":"markdown","bf4858a0":"markdown","74d70ed7":"markdown","35746edd":"markdown"},"source":{"ab2dabda":"import pandas as pd\nimport numpy as np\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.models import Sequential\nfrom keras.layers import Convolution2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import SGDClassifier\n\nimport seaborn as sns\n\nfrom tqdm import tqdm\nimport os","871587a8":"# Initialize global variables\nSAMPLE_SIZE = 10000\nBATCH_SIZE = 32\nTEST_PERC = 0.2","15cc1780":"segmentations = pd.read_csv(\"..\/input\/train_ship_segmentations.csv\")","67586c82":"segmentations['path'] = '..\/input\/train\/' + segmentations['ImageId']\nsegmentations.shape","e4c07a39":"segmentations = segmentations.sample(n=SAMPLE_SIZE)","97b55109":"def has_ship(encoded_pixels):\n    hs = [0 if pd.isna(n) else 1 for n in tqdm(encoded_pixels)]\n    return hs","19a32c18":"segmentations['HasShip'] = has_ship(segmentations['EncodedPixels'].values)\nsegmentations['HasShip'].head()","4e107c18":"segmentations.head()","6e73564b":"sns.countplot(segmentations['HasShip'])","bc69bd67":"np.shape(load_img(segmentations['path'].values[0]))","cffa3ebc":"train,test = train_test_split(segmentations, test_size=TEST_PERC)","1993eddf":"idg_train = ImageDataGenerator(rescale=1. \/ 255,\n                               shear_range=0.2,\n                               zoom_range=0.2,\n                               horizontal_flip=True)\n\nidg_test = ImageDataGenerator(rescale=1. \/ 255)","884d4bfc":"def flow_from_dataframe(img_data_gen, in_df, path_col, y_col, **dflow_args):\n    base_dir = os.path.dirname(in_df[path_col].values[0])\n    print('## Ignore next message from keras, values are replaced anyways')\n    df_gen = img_data_gen.flow_from_directory(base_dir, \n                                     class_mode = 'sparse',\n                                    **dflow_args)\n    df_gen.filenames = in_df[path_col].values\n    df_gen.classes = np.stack(in_df[y_col].values)\n    df_gen.samples = in_df.shape[0]\n    df_gen.n = in_df.shape[0]\n    df_gen._set_index_array()\n    df_gen.directory = '' # since we have the full path\n    print('Reinserting dataframe: {} images'.format(in_df.shape[0]))\n    return df_gen","d3b14b92":"train_images = flow_from_dataframe(idg_train, train, 'path', 'HasShip', batch_size=BATCH_SIZE, target_size=(256, 256))\ntest_images = flow_from_dataframe(idg_train, test, 'path', 'HasShip', batch_size=BATCH_SIZE, target_size=(256, 256))","fc405f8c":"train_images.target_size","24de9ed8":"model = Sequential()\nmodel.add(Convolution2D(32, (3, 3),\n                       input_shape=(256, 256, 3),\n                       activation='relu'))\n\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(32, (3, 3),\n                       input_shape=(256, 256, 3),\n                       activation='relu'))\n\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(32, (3, 3),\n                       input_shape=(256, 256, 3),\n                       activation='relu'))\n\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(units=128, activation='relu', kernel_initializer='normal'))\nmodel.add(Dense(units=1, activation='sigmoid', kernel_initializer='normal'))\n\nmodel.compile(optimizer='adam',\n             loss='binary_crossentropy',\n             metrics=['accuracy'])\nmodel.summary()","e6c6405b":"fitted_model = model.fit_generator(train_images,\n                   steps_per_epoch=SAMPLE_SIZE*(1-TEST_PERC)\/BATCH_SIZE,\n                   epochs=20,\n                   validation_data=test_images,\n                   validation_steps=SAMPLE_SIZE*(TEST_PERC)\/BATCH_SIZE)","db3397a0":"import matplotlib.pyplot as plt\nimport pylab\n\n\npath = 'results'\nname = 'adam'\n\nplt.plot(fitted_model.history['acc'])\nplt.plot(fitted_model.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n\nplt.show()","45e71c8a":"plt.figure()\nplt.gcf().clear()\nplt.plot(fitted_model.history['loss'])\nplt.plot(fitted_model.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n\nplt.show()\n","8a1b3137":"# 1. Data preperation and preprocessing","9315ed7a":"Now Let's plot the Accuracy and Loss history of our model for both train and validation sets","ac795456":"# 2. Creating the NN Model","d3a8e136":"# 0. Importing the libraries","2c6a19ca":"Let's add a new column to our dataframe which will give us the paths to each image in the dataframe","e3cde553":"We will create the following model\n\nInput -> 3 Convolutional and Max Pooling Layers -> Fully connected ANN with 2 hidden layers","343facec":"## 1.1 Feature engineering","88eca73e":"Since the data is too big, we will not need to use all the data for training. Here we take a sample from our data.","bb54792f":"Now let's see what we got","bf4858a0":"In this kernel we will try to detect if theres is a ship in the image. For that we will build a Convoluional Neural Network and solve the problem as a binary classification problem.","74d70ed7":"Now let's add a column which will indicate whether there is ship in the image or no. ","35746edd":"## 1.2 Image preprocessing"}}