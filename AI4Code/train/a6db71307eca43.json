{"cell_type":{"8be2f201":"code","03a9a251":"code","9fc3b764":"code","75d0bd50":"code","81f3572a":"code","d1d2768e":"code","fe8779f1":"code","d7ff2bbd":"code","50c08def":"code","7c7a6226":"code","efcb2a6b":"code","83d01ac0":"code","881acb35":"code","22b8ecd5":"code","2aaa6f74":"code","f1d96e8f":"code","413b7ff1":"code","33126cce":"code","761e1b85":"code","ebd220cc":"code","3cfb6404":"code","3d48162e":"code","b13aee2b":"code","2f726881":"code","4d5bac37":"code","e10d01f3":"code","8ccab6a6":"code","5e3be850":"code","f9d26a19":"code","1300fb4f":"code","7c00ef0f":"code","015e6d05":"code","6eba17bb":"code","cf205eaa":"code","d3ce9f8c":"code","0c0fdfd1":"code","bc6a876f":"code","077803b3":"code","d40d68f8":"code","c6e77417":"code","63828596":"code","95e7dd49":"code","390f75d1":"code","34c2e0df":"code","e8c6af0b":"code","8b33e9b7":"code","7739b89f":"code","a5a026c3":"code","6f3e27af":"code","117e249d":"code","3fb66bab":"code","2470e2c9":"code","0ec510de":"code","b0a49f61":"markdown","0acf6292":"markdown","1f55ffe7":"markdown"},"source":{"8be2f201":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","03a9a251":"from sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import  Dense, Activation,Dropout, Flatten\nfrom sklearn.linear_model import Ridge, Lasso, SGDRegressor\nfrom sklearn.svm import NuSVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import accuracy_score,mean_squared_error\nfrom tensorflow.keras.callbacks import ModelCheckpoint","9fc3b764":"train=pd.read_csv('\/kaggle\/input\/ventilator-pressure-prediction\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/ventilator-pressure-prediction\/test.csv')","75d0bd50":"sample=pd.read_csv('\/kaggle\/input\/ventilator-pressure-prediction\/sample_submission.csv')\nsample.head(5)","81f3572a":"train.head(3)","d1d2768e":"train=train.iloc[0:, 1:]","fe8779f1":"train.head()","d7ff2bbd":"test=test.iloc[0:, 1:]","50c08def":"train.info()","7c7a6226":"train.describe()","efcb2a6b":"train.isna().sum()","83d01ac0":"from sklearn.feature_selection import VarianceThreshold","881acb35":"var_thres=VarianceThreshold(threshold=0)","22b8ecd5":"var_thres.fit(train)","2aaa6f74":"constant_colums=[i for i in train.columns if i not in train.columns[var_thres.get_support()]]","f1d96e8f":"constant_colums","413b7ff1":"Xtrain=train[['breath_id','R','C','time_step','u_in','u_out']]","33126cce":"Ytrain=train[['pressure']]","761e1b85":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(12,10))\ncorr=Xtrain.corr()\nsns.heatmap(corr, annot=True)\nplt.show()","ebd220cc":"def correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr","3cfb6404":"\ncorr_features=correlation(Xtrain, threshold=0.7)","3d48162e":"Xtrain.drop(corr_features,axis=1)\ntest.drop(corr_features,axis=1)","b13aee2b":"for i in train.columns:\n    print(train[i].unique)","2f726881":"from sklearn.preprocessing import StandardScaler","4d5bac37":"sc=StandardScaler()","e10d01f3":"train1 =pd.DataFrame(sc.fit_transform(Xtrain))","8ccab6a6":"test1=pd.DataFrame(sc.fit_transform(test))","5e3be850":"train1","f9d26a19":"NN_model = Sequential()\nNN_model.add(Dense(256, kernel_initializer='normal',input_dim = train1.shape[1], activation='relu'))\nNN_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\nNN_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\nNN_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\nNN_model.add(Dense(512, kernel_initializer='normal',activation='linear'))\nNN_model.add(Dense(1, kernel_initializer='normal'))\nNN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\nNN_model.summary()","1300fb4f":"#checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n#checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n#callbacks_list = [checkpoint]","7c00ef0f":"NN_model.fit(train1, Ytrain, epochs=5, batch_size=32, validation_split = 0.2)\n             #, callbacks=callbacks_list)\n","015e6d05":"#wights_file = 'Weights-478--18738.19831.hdf5' # choose the best checkpoint \n#NN_model.load_weights(wights_file) # load it\n#NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])","6eba17bb":"predictions = NN_model.predict(test1)","cf205eaa":"predictions","d3ce9f8c":"train1.shape","0c0fdfd1":"train_x=train1.values.reshape(train1.shape[0],1,train1.shape[1])","bc6a876f":"\"\"\"model = Sequential()\nmodel.add(LSTM(units=400, activation='relu',return_sequences=True,input_shape=(train_x.shape[1],train_x.shape[2])))\nmodel.add(Dense(150))\nmodel.add(LSTM(units=200, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\"\"\"","077803b3":"#history=model.fit(train_x,Ytrain,epochs=2,batch_size=128, validation_split=0.3)","d40d68f8":"\"\"\"plt.figure(figsize=(20,6))\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Val Loss')\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epochs')\nplt.legend(loc='upper right')\nplt.show()\"\"\"","c6e77417":"#test_x=test1.values.reshape(test1.shape[0],1,test1.shape[1])","63828596":"#y_output=model.predict(test_x)","95e7dd49":"#y_output","390f75d1":"test=pd.read_csv('\/kaggle\/input\/ventilator-pressure-prediction\/test.csv')","34c2e0df":"sub=pd.DataFrame()\nsub['id']=test['id']\nsub['pressure']=pd.Series(predictions.flatten())","e8c6af0b":"sub.to_csv('submission.csv', index=False)\n","8b33e9b7":"#from sklearn.ensemble import RandomForestRegressor","7739b89f":"#regr = RandomForestRegressor(max_depth=2, random_state=0)\n#regr.fit(train1, Ytrain)","a5a026c3":"#ot=regr.predict(test1)","6f3e27af":"#sub=pd.DataFrame()\n#sub['id']=test['id']\n#sub['pressure']=pd.Series(ot)","117e249d":"#sub.to_csv('sub1.csv', index=False)","3fb66bab":"#ot","2470e2c9":"train['pressure'].plot()","0ec510de":"sub['pressure'].plot()","b0a49f61":"# building model for training and testing of the dataset","0acf6292":"# dropping feature using correlation","1f55ffe7":"here we will use the deep neural network for model building and data prediction and final data analyze the model."}}