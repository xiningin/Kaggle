{"cell_type":{"8d79a257":"code","db4fffa5":"code","aef1eefe":"code","7b2e8993":"code","9042e85c":"code","e7cc4fb3":"code","5b280c3f":"code","2c2d46d4":"code","6d0455c0":"code","a5de60c0":"code","1e2086f9":"code","ff052726":"code","7d6306ce":"code","5ae89430":"code","885a3b94":"code","fbc23d99":"code","213cf340":"code","edcb1b33":"code","715dff4f":"code","422ae396":"code","737eff03":"code","ac81257a":"code","39930dad":"code","13184a0b":"code","9b7265f4":"code","43e7ddf8":"code","a3f2ebfa":"code","0900af6b":"code","28cdb773":"code","4ff383d6":"markdown","af65a8db":"markdown","c00579ce":"markdown","ccad4bbc":"markdown","1679d6da":"markdown"},"source":{"8d79a257":"import pandas as pd\nimport numpy as np\nfrom math import *\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.preprocessing import OneHotEncoder\nfrom matplotlib.pyplot import boxplot \nfrom sklearn.model_selection import KFold\nfrom vecstack import stacking\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","db4fffa5":"X_train_full = pd.read_csv('..\/input\/train.csv', index_col='Id')\nX_test = pd.read_csv('..\/input\/test.csv', index_col='Id')","aef1eefe":"## Detecting of NA\nmissing_val_count_by_column = (X_train_full.isnull().sum())\nmissing_val_count_by_column[missing_val_count_by_column > 0].plot.bar(figsize=(12,8))\nplt.xlabel('Columns with missing values')\nplt.ylabel('Count')","7b2e8993":"## So, a lot of missing values in column 'Alley' - it is better to delete it\n# Also I decided to drop 'Utilities' because it's close to constant variable\nX_train_full = X_train_full.drop(['Alley', 'Utilities'], axis = 1)\nX_test = X_test.drop(['Alley', 'Utilities'], axis = 1)","9042e85c":"# Finding the most expencive houses by SalePrice\npd.DataFrame(X_train_full['SalePrice']).sort_values(by = 'SalePrice', ascending = False).head(4)","e7cc4fb3":"# Density plot for Sale Prices\nsns.distplot(X_train_full['SalePrice'])","5b280c3f":"## Preprocessing data\n### Simplifying features\nX_train_full['Street'], X_test['Street'] = X_train_full['Street'].map({'Pave': 1, 'Grvl': 0}), X_test['Street'].map({'Pave': 1, 'Grvl': 0})\n\nX_train_full['Condition2'] = X_train_full['Condition2'].apply(lambda z: 1 if z == 'Norm' else 0)\nX_test['Condition2'] = X_test['Condition2'].apply(lambda z: 1 if z == 'Norm' else 0)\n\nX_train_full['RoofMatl'] = X_train_full['RoofMatl'].apply(lambda x: 'CompShg' if x == 'CompShg' else 'Tar&Grv' if x == 'Tar&Grv' else 'Others')\nX_test['RoofMatl'] = X_test['RoofMatl'].apply(lambda x: 'CompShg' if x == 'CompShg' else 'Tar&Grv' if x == 'Tar&Grv' else 'Others')\n\nX_train_full['Heating'] = X_train_full['Heating'].apply(lambda x: 'GasA' if x == 'GasA' else 'GasW' if x == 'GasW' else 'Others')\nX_test['Heating'] = X_test['Heating'].apply(lambda x: 'GasA' if x == 'GasA' else 'GasW' if x == 'GasW' else 'Others')\n\nX_train_full['CentralAir'] = X_train_full['CentralAir'].map({'Y': 1, 'N': 0})\nX_test['CentralAir'] = X_test['CentralAir'].map({'Y': 1, 'N': 0})\n\n### Filling NA with variables\nX_train_full['LotFrontage'] = X_train_full.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.mean()))\nX_test['LotFrontage'] = X_test.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.mean()))\n\nX_test.update(X_test[['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath',\n                     'GarageYrBlt', 'GarageCars', 'GarageArea', 'MasVnrArea']].fillna(0))\nX_train_full.update(X_train_full[['GarageYrBlt', 'MasVnrArea']].fillna(0))\n\nX_train_full.update(X_train_full[['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']].fillna('NoBsmt'))\nX_test.update(X_test[['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']].fillna('NoBsmt'))\n\nX_train_full['FireplaceQu'], X_test['FireplaceQu'] = X_train_full['FireplaceQu'].fillna('NoFireplace'), X_test['FireplaceQu'].fillna('NoFireplace')\n\nX_train_full.update(X_train_full[['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']].fillna('NoGarage'))\nX_test.update(X_test[['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']].fillna('NoGarage'))\n\nX_train_full['PoolQC'], X_test['PoolQC'] = X_train_full['PoolQC'].fillna('NoPool'), X_test['PoolQC'].fillna('NoPool')\nX_train_full['Fence'], X_test['Fence'] = X_train_full['Fence'].fillna('NoFence'), X_test['Fence'].fillna('NoFence')\n\nX_train_full['MiscFeature'], X_test['MiscFeature'] = X_train_full['MiscFeature'].fillna('NoMiscFeature'), X_test['MiscFeature'].fillna('NoMiscFeature')\n\n\n### Creating additional syntetic variables\nX_train_full['Total_amt_bathrooms'] = X_train_full.BsmtFullBath + X_train_full.FullBath + 0.5*X_train_full.BsmtHalfBath + 0.5*X_train_full.HalfBath\nX_test['Total_amt_bathrooms'] = X_test.BsmtFullBath + X_test.FullBath + 0.5*X_test.BsmtHalfBath + 0.5*X_test.HalfBath\n\nX_train_full['Total_porch_sf'] = X_train_full.OpenPorchSF + X_train_full.EnclosedPorch + X_train_full['3SsnPorch'] + X_train_full.ScreenPorch\nX_test['Total_porch_sf'] = X_test.OpenPorchSF + X_test.EnclosedPorch + X_test['3SsnPorch'] + X_test.ScreenPorch\n\nX_train_full['Remodel_aft_building_years'] = X_train_full.YearRemodAdd - X_train_full.YearBuilt\nX_test['Remodel_aft_building_years'] = X_test.YearRemodAdd - X_test.YearBuilt\n\nX_train_full['Total_SF'] = X_train_full.TotalBsmtSF + X_train_full['1stFlrSF'] + X_train_full['2ndFlrSF']\nX_test['Total_SF'] = X_test.TotalBsmtSF + X_test['1stFlrSF'] + X_test['2ndFlrSF']\n\nX_train_full['Has2ndFloor'] = X_train_full['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nX_test['Has2ndFloor'] = X_test['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n\nX_train_full['HasFireplace'] = X_train_full['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\nX_test['HasFireplace'] = X_test['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\nX_train_full['HasBsmt'] = X_train_full['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nX_test['HasBsmt'] = X_test['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n\nX_train_full['HasGarage'] = X_train_full['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nX_test['HasGarage'] = X_test['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n\nX_train_full['HasPorch'] = X_train_full['Total_porch_sf'].apply(lambda x: 1 if x > 0 else 0)\nX_test['HasPorch'] = X_test['Total_porch_sf'].apply(lambda x: 1 if x > 0 else 0)\n\nX_train_full['HasPool'] = X_train_full['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nX_test['HasPool'] = X_test['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n\nX_train_full['HasFence'] = X_train_full['Fence'].apply(lambda x: 1 if x != 'NoFence' else 0)\nX_test['HasFence'] = X_test['Fence'].apply(lambda x: 1 if x != 'NoFence' else 0)\n\n\n### Changing dtypes of some columns\nX_train_full['GarageYrBlt'], X_test['GarageYrBlt'] = X_train_full['GarageYrBlt'].astype('int64'), X_test['GarageYrBlt'].astype('int64')\n\nX_train_full = X_train_full.drop('YearRemodAdd', axis=1)\nX_test = X_test.drop('YearRemodAdd', axis=1)","2c2d46d4":"## Correlation between variables\ncorr_mat = X_train_full[[\"SalePrice\",\"MSSubClass\",\"MSZoning\",\"LotFrontage\",\"LotArea\", \"BldgType\",\n                       \"OverallQual\", \"OverallCond\",\"YearBuilt\", \"BedroomAbvGr\", \"PoolArea\", \"GarageArea\",\n                       \"SaleType\", \"MoSold\"]].corr()\n\nf, ax = plt.subplots(figsize=(16, 8))\nsns.heatmap(corr_mat, vmax=1 , square=True)","6d0455c0":"## We can see that SalePrice is the most correlated with OveralQual, YearBuilt and GarageArea, so let's check it's \n## dependecies between each other via plotting.\n\nf, ax = plt.subplots(figsize=(16, 8))\nsns.lineplot(x='YearBuilt', y='SalePrice', data=X_train_full)","a5de60c0":"# We can see uptrend in general, but here we can see possible 2 outliers: 2 expencive houses, which was built approx\n## in 1890-s and 4 in 1935-s\n\n# POSIIBLE OUTLIES:\n    # 186, 584\n    # 191, 219, 609, 1269\n    \nf, ax = plt.subplots(figsize=(16, 8))\nsns.lineplot(x='OverallQual', y='SalePrice', data=X_train_full)","1e2086f9":"## We can see that houses with best Overall Quality Rate costs higher, s it's pretty expectable.\n\nX_train_full.query('YearBuilt < 1900 and OverallQual > 9')","ff052726":"## Separating target value and features\ny = X_train_full.SalePrice              \nX_train_full.drop(['SalePrice'], axis=1, inplace=True)","7d6306ce":"## I decided to log variables to make it with normal distribution: 'LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea'\n## And the same for target variable\n\nloged_features = ['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea']\nX_train_full.update(np.log(X_train_full[loged_features]))\nX_test.update(X_test[loged_features])\ny = np.log(y)","5ae89430":"## Imputing missing variables with simple imputer for least vars\n\nX_train_full = pd.get_dummies(X_train_full)\nX_test = pd.get_dummies(X_test)\n\nsi = SimpleImputer(strategy='most_frequent')\n\nX_train_a = pd.DataFrame(si.fit_transform(X_train_full))\nX_train_a.columns = X_train_full.columns\n\nX_test_a = pd.DataFrame(si.fit_transform(X_test))\nX_test_a.columns = X_test.columns\nX_test_a.index = X_test.index\n\nX_train_a, X_test_a = X_train_a.align(X_test_a, join='left', axis=1)","885a3b94":"# train_cols = X_train_a.columns\n# test_cols = X_test_a.columns\n# common_cols = train_cols.intersection(test_cols)\n# train_not_test = train_cols.difference(test_cols)\n# train_not_test\n# X_train_a = X_train_a.drop(train_not_test, axis=1)","fbc23d99":"## Deleting possible outliers\n\n# The most expensive SalePrice is obsevations # 692, 1183, 1170, 899 (> 600000)\n\n# Huge 'LotArea' in observations # 314, 336, 250, 707 (> 100000 )\n# Huge 'LotFrontage' in observations # 1299, 935, 1128 ( > 180 )\n# Huge 'GrLivArea'(\u043e\u0431\u0449\u0430\u044f \u0436\u0438\u043b\u0430\u044f \u043f\u043b\u043e\u0449\u0430\u0434\u044c) # 1299, 524, 1183, 692 ( > 4000 )\n\n# 186, 584\ny = y.drop([1183, 692], axis = 0)\nX_train_a = X_train_a.drop([1183, 692], axis = 0)","213cf340":"## Splitting variables on train\/\/test sizes\nX_train, X_valid, y_train, y_valid = train_test_split(X_train_a, y, train_size = 0.8, test_size = 0.2, random_state=15)","edcb1b33":"xgb = XGBRegressor(n_estimators = 500, \n                   learning_rate=0.05, \n                   subsample= 0.75, \n                   colsample_bytree = 0.63\n                       )\nkf = KFold(n_splits=5)\ny_pred = cross_val_score(xgb, X_train, y_train, cv=kf, n_jobs=-1)\ny_pred.mean()","715dff4f":"xgb.fit(X_train, y_train)","422ae396":"lightgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=6,\n                                       learning_rate=0.03, \n                                       n_estimators=900,\n                                       verbose=-1,\n                                       bagging_fraction=0.80,\n                                       bagging_freq=4, \n                                       bagging_seed=6,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7\n                         )\n\n# Perform cross-validation to see how well our model does\nkf = KFold(n_splits=5)\ny_pred = cross_val_score(lightgbm, X_train, y_train, cv=kf)\ny_pred.mean()","737eff03":"lightgbm.fit(X_train, y_train)","ac81257a":"g_boost = GradientBoostingRegressor(alpha = 0.9,\n                                    learning_rate=0.023,\n                                    loss = 'ls',\n                                    max_features='sqrt',\n                                    n_estimators=900,\n                                    subsample=1\n                                   )\nkf = KFold(n_splits=5)\ny_pred = cross_val_score(g_boost, X_train, y_train, cv=kf)\ny_pred.mean()","39930dad":"g_boost.fit(X_train, y_train)","13184a0b":"models = [xgb, lightgbm, g_boost]\nS_train, S_test = stacking(models,\n                           X_train, y_train, X_valid,\n                           regression=True,\n                           mode='oof_pred_bag',\n                           n_folds=5,\n                           random_state=25,\n                           verbose=2\n                          )","9b7265f4":"xgb_lev2 = XGBRegressor(learning_rate=0.04, \n                        n_estimators=900,\n                        max_depth=3,\n                        n_jobs=-1,\n                        random_state=17\n                       )\n\n# Fit the 2nd level model on the output of level 1\nxgb_lev2.fit(S_train, y_train)","43e7ddf8":"stacked_pred = xgb_lev2.predict(S_test)\nxgb_pred = xgb.predict(X_valid)\nlightgbm_pred = lightgbm.predict(X_valid)\ngboost_pred = g_boost.predict(X_valid)\nprint(\"RMSE of Stacked Model: {}\".format(np.sqrt(mean_squared_error(y_valid, stacked_pred))))\nprint(\"RMSE of XGBoost Model: {}\".format(np.sqrt(mean_squared_error(y_valid, xgb_pred))))\nprint(\"RMSE of LGBMRegressor: {}\".format(np.sqrt(mean_squared_error(y_valid, lightgbm_pred))))\nprint(\"RMSE of GradientBoostingRegressor: {}\".format(np.sqrt(mean_squared_error(y_valid, gboost_pred))))","a3f2ebfa":"### CHECKING MAE, RMSE OF NECESSARY MODEL\n\npredictions = xgb.predict(X_valid)\n# predictions = np.exp(predictions)\n\nmae = mean_absolute_error(y_valid, predictions)\nprint(\"Mean Absolute Error:\" , mae)\n\nrmse = np.sqrt(mean_squared_error(np.log(predictions), np.log(y_valid)))\nprint(\"Root Mean Squared Error:\" , rmse)\n\nrmse2 = np.sqrt(mean_squared_error(predictions, y_valid))\nprint(\"Root Mean Squared Error:\" , rmse2)","0900af6b":"### SAVING RESULT FOR 1 CHOSEN MODEL\n\n\n# preds_test = xgb.predict(X_test_a)\n# # preds_test = np.exp(preds_test)\n# output = pd.DataFrame({'Id': X_test_a.index,\n#                        'SalePrice': preds_test})\n# output.to_csv('submissionModel.csv', index=False)\n# preds_test","28cdb773":"### PREDICTING TEST VALUE BY STACKED MODEL\n\n# y1_pred_L1 = models[0].predict(X_test_a)\n# y2_pred_L1 = models[1].predict(X_test_a)\n# y3_pred_L1 = models[2].predict(X_test_a)\n# S_test_L1 = np.c_[y1_pred_L1, y2_pred_L1, y3_pred_L1]\n# preds_test = xgb_lev2.predict(S_test_L1)\n# preds_test = np.exp(preds_test)\n# output = pd.DataFrame({'Id': X_test_a.index,\n#                        'SalePrice': preds_test})\n# output.to_csv('submissionStacked.csv', index=False)\n# preds_test","4ff383d6":"#### RMSE Results","af65a8db":"### LGBMRegressor","c00579ce":"### XGBoost Model","ccad4bbc":"### GradientBoostingRegressor","1679d6da":"### Stacked Models"}}