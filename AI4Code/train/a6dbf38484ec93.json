{"cell_type":{"a9baf4e3":"code","11487ce4":"code","ee90171d":"code","96fbf3c6":"code","2c006cd0":"code","7e364f5d":"code","cb819eb0":"code","5a88dcc2":"code","c90d3f40":"code","6901a118":"code","cb1bb77a":"code","73b2ee53":"code","d244ba0c":"code","64b96356":"code","2b059b72":"code","b73c4472":"code","8a61c405":"code","90c84a5a":"code","c1845664":"code","ca72ef3c":"code","7b251ef1":"code","e67897b2":"code","067bb8ae":"code","99bd91cd":"code","3d6df1e5":"code","113fa67a":"code","f50bf171":"markdown","03229f7e":"markdown","a45b0557":"markdown","1e22a4d5":"markdown","858e9d2d":"markdown","c5b5f1bf":"markdown","78cc7dd2":"markdown","c21c4034":"markdown","4cfbe5e3":"markdown","352c2ecb":"markdown","614c8972":"markdown","9eae7684":"markdown","eacb5001":"markdown","73585d1a":"markdown","06745619":"markdown","1c81e98f":"markdown"},"source":{"a9baf4e3":"import os\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import kurtosis, skew\nfrom matplotlib.offsetbox import AnchoredText\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","11487ce4":"train_ = pd.read_csv(r'\/kaggle\/input\/tabular-playground-series-mar-2021\/train.csv', index_col='id')\ntest = pd.read_csv(r'\/kaggle\/input\/tabular-playground-series-mar-2021\/test.csv', index_col='id')\n\nsubmission= pd.read_csv(r'\/kaggle\/input\/tabular-playground-series-mar-2021\/sample_submission.csv', index_col='id')","ee90171d":"train = train_.copy()","96fbf3c6":"print('Train data of shape {}'.format(train.shape))\ndisplay(train.head())\nprint('Test data of shape {}'.format(test.shape))\ndisplay(test.head())","2c006cd0":"display(train.describe().T)","7e364f5d":"target = train.pop('target')","cb819eb0":"cat_features =[]\nnum_features =[]\n\nfor col in train.columns:\n    if train[col].dtype=='object':\n        cat_features.append(col)\n    else:\n        num_features.append(col)\nprint('Catagoric features: ', cat_features)\nprint('Numerical features: ', num_features)","5a88dcc2":"print('Number of NA values in train data is {}'.format(train.isna().sum().sum()))\nprint('Number of NA values in test data is {}'.format(test.isna().sum().sum()))","c90d3f40":"for col in cat_features:\n    print('{} unique values in {}'.format(train[col].nunique(), col))","6901a118":"for col in cat_features:\n    print('{} unique values in {}'.format(test[col].nunique(), col))","cb1bb77a":"train_cat10 = list(pd.DataFrame(train_['cat10'].value_counts()).index)\ntest_cat10 = list(pd.DataFrame(test['cat10'].value_counts()).index)\nprint('Elements of cat10 which are present in train_data but NOT in test_data.')\nprint('-----------------------------------------------------------------------')\nfor item in train_cat10:\n    if item not in test_cat10:\n        print(item)\nprint('')\nprint('Elements of cat10 which are present in test_data but NOT in train_data.')\nprint('-----------------------------------------------------------------------')\nfor item in test_cat10:\n    if item not in train_cat10:\n        print(item)","73b2ee53":"# group columns according to cardinality\/only for plotting\nlow_cardinal_cols = []\nhigh_cardinal_cols = []\n\nfor col in cat_features:\n    if train[col].nunique() <= 20:\n        low_cardinal_cols.append(col)\n    else:\n        high_cardinal_cols.append(col)\n\n# display the values\nprint(\"low_cardinal_cols\")\nprint(low_cardinal_cols)\nprint(\"high_cardinal_cols\")\nprint(high_cardinal_cols)","d244ba0c":"def count_plot_testTrain(data1, data2, features, titleText):\n    L = len(features)\n    nrow= int(np.ceil(L\/2))\n    ncol= 2\n\n    remove_last= (nrow * ncol) - L\n\n    fig, ax = plt.subplots(nrow, ncol,figsize=(18, 26))#, facecolor='#D6E8D8')\n    ax.flat[-remove_last].set_visible(False)\n    fig.subplots_adjust(top=0.95)\n    i = 1\n    for feature in features:\n        plt.subplot(nrow, ncol, i)\n        ax = sns.countplot(x=feature, color='#1eb069', data=data1, label='train')\n        ax = sns.countplot(x=feature, color='#056d87', data=data2, label='test')\n        plt.legend()\n        i += 1\n    plt.suptitle(titleText ,fontsize = 20)\n    plt.show()","64b96356":"count_plot_testTrain(train, test, low_cardinal_cols, titleText='Train & test data categorical features (low cardinality)')","2b059b72":"def count_plot(data, features, titleText, hue=None):\n    L = len(features)\n    nrow= int(np.ceil(L\/2))\n    ncol= 2\n\n    remove_last= (nrow * ncol) - L\n\n    fig, ax = plt.subplots(nrow, ncol,figsize=(18, 26))\n    ax.flat[-remove_last].set_visible(False)\n    fig.subplots_adjust(top=0.95)\n    i = 1\n    for feature in features:\n        total = float(len(data)) \n        plt.subplot(nrow, ncol, i)\n        ax = sns.countplot(x=feature, palette='viridis', data=data, hue=hue)        \n        i += 1\n    plt.suptitle(titleText ,fontsize = 20)\n    plt.show()    \n    ","b73c4472":"count_plot(train, low_cardinal_cols, 'Train data cat_feats (low cardinal): target dist', hue=target)","8a61c405":"# got a hint from notebook (https:\/\/www.kaggle.com\/dwin183287\/tps-mar-2021-eda\/) for this code snippet\nfor cat in high_cardinal_cols:\n    new_cat_train = f'train_{cat}' \n    new_cat_train= list(pd.DataFrame(train_[cat].value_counts()\/len(train_[cat]))[:19].index)\n    new_cat_test = f'test{cat}' \n    new_cat_test = list(pd.DataFrame(test[cat].value_counts()\/len(test[cat]))[:19].index)\n    train_[cat] = np.where(~train_[cat].isin(new_cat_train), 'etc', train_[cat])\n    test[cat] = np.where(~test[cat].isin(new_cat_test), 'etc', test[cat])","90c84a5a":"L = len(high_cardinal_cols)\ni =1\nnrow= int(np.ceil(L\/2))\nncol= 2\n\nremove_last= (nrow * ncol) - L\n\nfig, ax = plt.subplots(nrow, ncol,figsize=(18, 8))\nax.flat[-remove_last].set_visible(False)\nfig.subplots_adjust(top=0.95)\n\nfor cat in train_[high_cardinal_cols]:\n    plt.subplot(nrow, ncol, i)\n    ax = sns.countplot(x=cat, color='#1eb069', data=train_, label='train') \n    ax = sns.countplot(x=cat, color='#056d87', data=test, label='test')\n    plt.suptitle('Train & test data categorical features (high cardinality)' ,fontsize = 20, y=1.002)\n    plt.legend()\n    i+=1\nplt.show() \n\nL = len(high_cardinal_cols)\ni =1\nnrow= int(np.ceil(L\/2))\nncol= 2\n\nremove_last= (nrow * ncol) - L\n\nfig, ax = plt.subplots(nrow, ncol,figsize=(18, 8))\nax.flat[-remove_last].set_visible(False)\nfig.subplots_adjust(top=0.95)\nfor cat in train_[high_cardinal_cols]:\n    plt.subplot(nrow, ncol, i)\n    ax = sns.countplot(x=cat, palette='viridis', data=train_, hue=target) \n    plt.suptitle('High cardinality categorical features: target dist' ,fontsize = 20, y=1.002)\n    plt.legend()\n    i+=1\nplt.show()","c1845664":"L = len(num_features)\nnrow= int(np.ceil(L\/4))\nncol= 4\n\nremove_last= (nrow * ncol) - L\n\nfig, ax = plt.subplots(nrow, ncol,figsize=(18, 12))\nax.flat[-remove_last].set_visible(False)\nfig.subplots_adjust(top=0.95)\ni = 1\nfor feature in num_features:\n    plt.subplot(nrow, ncol, i)\n    ax = sns.kdeplot(train_[feature], shade=True, color='#1eb069',  alpha=0.5, label='train')\n    ax = sns.kdeplot(test[feature], shade=True, color='#056d87',  alpha=0.5, label='test')\n    plt.xlabel(feature, fontsize=9)\n    plt.legend()\n    i += 1\nplt.suptitle('DistPlot: numerical features of train & test data', fontsize=20)\nplt.show()","ca72ef3c":"L = len(num_features)\nnrow= int(np.ceil(L\/4))\nncol= 4\n\nremove_last= (nrow * ncol) - L\n\nfig, ax = plt.subplots(nrow, ncol,figsize=(18, 12))\nax.flat[-remove_last].set_visible(False)\nfig.subplots_adjust(top=0.95)\ni = 1\nfor feature in num_features:\n    plt.subplot(nrow, ncol, i)\n    ax = sns.kdeplot(train_[feature], shade=True, palette='viridis',  alpha=0.5, hue= target, multiple=\"stack\")\n    plt.xlabel(feature, fontsize=9)\n    plt.legend(['1', '0'])\n    i += 1\nplt.suptitle('DistPlot: numerical features of train data', fontsize=20)\nplt.show()","7b251ef1":"L = len(num_features)\nnrow= int(np.ceil(L\/4))\nncol= 4\n\nremove_last= (nrow * ncol) - L\n\nfig, ax = plt.subplots(nrow, ncol,figsize=(18, 12))\nax.flat[-remove_last].set_visible(False)\nfig.subplots_adjust(top=0.92)\ni = 1\nfor feature in num_features:\n    plt.subplot(nrow, ncol, i)\n    ax = sns.kdeplot(train_[feature], shade=True, palette='coolwarm',  alpha=0.75, hue= target, multiple=\"fill\")\n    plt.xlabel(feature, fontsize=9)\n    plt.legend(['1', '0'])\n    i += 1\nplt.suptitle('DistPlot: numerical features of train data', fontsize=20)\nplt.show()","e67897b2":"plt.figure(figsize=(8, 6))\nax = sns.countplot(x=target, palette='viridis')\nax.set_title('Target variable distribution', fontsize=20, y=1.05)\n\nsns.despine(right=True)\nsns.despine(offset=10, trim=True)\n","067bb8ae":"correlation_table = []\nfor cols in num_features:\n    y = target\n    x = train[cols]\n    corr = np.corrcoef(x, y)[1][0]\n    dict ={\n        'Features': cols,\n        'Correlation coefficient' : corr,\n        'Feat_type': 'numerical'\n    }\n    correlation_table.append(dict)\ndF1 = pd.DataFrame(correlation_table)\nfig = plt.figure(figsize=(10,6), facecolor='#EAECEE')\nax = sns.barplot(x=\"Correlation coefficient\", y=\"Features\", \n                     data=dF1.sort_values(\"Correlation coefficient\", ascending=False),\n                     palette='viridis', alpha=0.75)\nax.grid()\n#ax.set_title(\"Correlation of numerical features with Target\", fontsize=20, y=1.05)\n\ntitle =  'Correlation of numerical features with target'\nsub_title = 'In comparison with categorical features \\\n\\nnumericals are less correlated with target.'\n\nplt.gcf().text(0.05, 1.02, title, fontsize=24)\n#plt.gcf().text(0.05, 0.9, sub_title, fontsize=14)\n\nat1 = AnchoredText(sub_title,\n                   loc='lower left', frameon=True,\n                   bbox_to_anchor=(-0.1, 1.01),\n                   bbox_transform=ax.transAxes,\n                   #prop=dict(size=8),\n                   )\nat1.patch.set_boxstyle(\"round,pad=0.,rounding_size=0.2\")\nax.add_artist(at1);\n","99bd91cd":"correlation_table= []\nfor cols in cat_features:\n    y = train_['target']\n    X = train[cols]\n    corr = pd.concat((X, y), axis=1).apply(lambda x : pd.factorize(x)[0]).corr()\n    dict ={\n        'Features': cols,\n        'Correlation coefficient' : corr['target'][:].values[0],\n        'Feat_type': 'categorical'\n    }\n    correlation_table.append(dict)\ndF2 = pd.DataFrame(correlation_table)\nfig = plt.figure(figsize=(12,8), facecolor='#EAECEE')\nax = sns.barplot(x=\"Correlation coefficient\", y=\"Features\", \n                     data=dF2.sort_values(\"Correlation coefficient\", ascending=False),\n                     palette='viridis', alpha=0.75)\nax.grid()\n#ax.set_title(\"Correlation of categorical features with target\", fontsize=20, y=1.05)\n\ntitle =  'Correlation of categorical features with target'\nsub_title = 'Categorical features are better\\ncorrelated with target \\\nthan\\nnumerical features.\\n\\ncat16 and cat18 stand-out'\n\nplt.gcf().text(0.05, 1.04, title, fontsize=24)\n#plt.gcf().text(0.05, 0.9, sub_title, fontsize=14)\n\nat1 = AnchoredText(sub_title,\n                   loc='lower left', frameon=True,\n                   bbox_to_anchor=(-0.1, 1.01),\n                   bbox_transform=ax.transAxes,\n                   #prop=dict(size=8),\n                   )\nat1.patch.set_boxstyle(\"round,pad=0.,rounding_size=0.2\")\nax.add_artist(at1);","3d6df1e5":"sns.set_style(\"darkgrid\")\n\ncorr = train.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nf, ax = plt.subplots(figsize=(16, 10), facecolor='#EAECEE')\ncmap = sns.color_palette(\"vlag\", as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=1.0, vmin=-1.0, center=0, annot=True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": 0.75})\n\n#ax.set_title('Correlation heatmap: numerical features', fontsize=24, y= 1.05)\ncolorbar = ax.collections[0].colorbar\ncolorbar.set_ticks([-0.75, 0, 0.75])\ncolorbar.set_ticklabels(['negative_corr','Little_to_no_corr','positive_corr'])\n\ntitle = 'Highly correlated ones are:\\ncont1_cont2\\ncont0_cont10'\ntitle_ =  'Correlation heatmap: numerical features (train data)'\nplt.gcf().text(0.23, 0.98, title_, fontsize=24)\n#plt.gcf().text(0.2, 0.9, title, fontsize=12)\n\n#textstr = 'Features with highest correlation\\ncon1$con2, cont0&cont10, cont'\nat1 = AnchoredText(title,\n                   loc='lower left', frameon=True,\n                   bbox_to_anchor=(-0.1, 1.01),\n                   bbox_transform=ax.transAxes,\n                   #prop=dict(size=8),\n                   )\nat1.patch.set_boxstyle(\"round,pad=0.,rounding_size=0.2\")\nax.add_artist(at1);","113fa67a":"corr = (train_[cat_features]).apply(lambda x : pd.factorize(x)[0]).corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nf, ax = plt.subplots(figsize=(20, 12), facecolor=\"#EAECEE\")\ncmap = sns.color_palette(\"vlag\", as_cmap=True)\nsns.heatmap(np.round(corr, 2), mask=mask, cmap=cmap, vmax=1.0, vmin=-1.0, center=0, annot=True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": 0.75})\n\ncolorbar = ax.collections[0].colorbar\ncolorbar.set_ticks([-0.75, 0, 0.75])\ncolorbar.set_ticklabels(['negative_corr','Little_to_no_corr','positive_corr'])\n\nsub_title = 'Not as many highly correlated as num_features.\\nMost notable is:\\ncat11_cat2'\ntitle =  'Correlation heatmap: catagorical features (train data)'\nplt.gcf().text(0.26, 0.98, title, fontsize=24)\n\nat1 = AnchoredText(sub_title,\n                   loc='lower left', frameon=True,\n                   bbox_to_anchor=(-0.098, 1.02),\n                   bbox_transform=ax.transAxes, #prop=dict(size=8),\n                   )\nat1.patch.set_boxstyle(\"round,pad=0.,rounding_size=0.4\")\nax.add_artist(at1);","f50bf171":"# Feature-target correlation \n- More numerical features than categoricals seem to correlate with target\n- cat16 has highest correlation with target\n- cont0 is the least correlated with target \n","03229f7e":"## Categorical features (group by cardinality)\n\n- There are four features with high cardinality (>20, one very high with 299)\n- The rest is less that or equal to 20\n<div class=\"alert alert-block alert-danger\">  \nWatch for cat10 !!!\n<\/div>\n\n- In **cat10** there are 299 unique values in train data whereas the test data has 295\n- Some elements of cat10 are present in test data but **NOT** in train data and the vice-versa (details below)","a45b0557":"### The same story, different look (looks cool, for me) of the above kde plots\n(Normalized distribution at each value)","1e22a4d5":"# Data Visualization\n## Low cardinality features","858e9d2d":"# Tabular Playground Series: March 2021\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/25225\/logos\/header.png?t=2021-01-27-17-34-26)","c5b5f1bf":"# Explore the data","78cc7dd2":"## End of notebook!","c21c4034":"# Load the data","4cfbe5e3":"# Target Variable\n(Target variable is imbalanced: more 0's than 1's)","352c2ecb":"# Thank you very much for reading this notebook!","614c8972":"## Numerical features\n- Kde plots are made to compare train and test data\n- No major differences in distribution \n- Density is consistent with the embalance of the target variable\n","9eae7684":"## Introduction:\n\nStarting from January this year, the kaggle competition team is offering a month-long tabulary playground competitions. This series aims to bridge between inclass competition and featured competitions with a friendly and approachable datasets.\n\nFor the month of March, kaggle is offering a dataset which is synthetic but based on a real dataset and generated using a CTGAN. The original dataset, this synthetic dataset is derived from, deals with predicting the amount of an insurance claim. Although the features are anonymized, they have properties relating to real-world features.\n\nThe data has: \n\n* 19 categorical variables: **cat0** to **cat18**\n* 11 continuous variables: **cont0** to **cont10**\n* 1 binary **target** column\n\nFiles provides:\n\n- train.csv - the training data with the target column\n- test.csv - the test set; you will be predicting the target for each row in this file\n- sample_submission.csv - a sample submission file in the correct format\n\nThe goal of the competition is to predict a binary **target** based on the given categorical and continuous features. However, the goal of **this notebook** is to explore (EDA) and visualize the given data. And when possible try to discover (engineer) *potentially usefull* features for further data modelling and prediction.\n\n\n[1. Set-up](#Set-up)","eacb5001":"## Null-values in the data\n(No null values in the data.)","73585d1a":"## High cardinality categorical features\n- As described above there are FOUR catagories which has more than 20 elements (cat5, 7, 8 and 10)\n- For plotting convenience, these catagories are condensed to 20 elements (19 most frequent elements + the rest merged to 'etc' catagory). Here 20 is an arbitrary number but equal to the highest cardinality in the 'low cardinality' categories. ","06745619":"# Set-up","1c81e98f":"# Feature-to-feature correlation\n- Correlation between numerical features dominates correlation between categoricals"}}