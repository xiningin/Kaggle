{"cell_type":{"1d11b007":"code","086e49ba":"code","42ba13e2":"code","6fa4a609":"code","344b1dfa":"code","5fd9b5ad":"code","b53d44b0":"code","a5ad11f8":"code","a88fab3b":"code","864bc354":"code","10a29901":"code","ad9b4640":"code","7a7c2c49":"code","63a873bb":"code","e253e9cc":"code","0ea435f3":"code","ddc6d719":"code","d354aafd":"code","0ef8ec4b":"code","0801b7bc":"code","5a3df1e3":"markdown","4186c24e":"markdown","d03e5273":"markdown","1b0eb17b":"markdown","407b8eaa":"markdown","7eccc489":"markdown","027ced3b":"markdown","94ff7b78":"markdown","c459f17a":"markdown","f50d0fd1":"markdown","77d81573":"markdown","9f713089":"markdown","dcaabce6":"markdown","f52adb09":"markdown","220f3983":"markdown","82ab43fe":"markdown","65725270":"markdown","a703415c":"markdown","55ce1702":"markdown","3cfd7d7d":"markdown","49d3c28a":"markdown","f6d0a8bf":"markdown","715f2ca3":"markdown","c3874fc3":"markdown","a9624d5c":"markdown","3b58852f":"markdown"},"source":{"1d11b007":"import pandas as pd\nimport numpy as np\nimport re\nfrom re import sub\nimport multiprocessing\nfrom unidecode import unidecode\nimport os\nimport glob\nfrom gensim.models.phrases import Phrases, Phraser\nfrom gensim.models import Word2Vec\nfrom gensim.test.utils import get_tmpfile\nfrom gensim.models import KeyedVectors\nfrom gensim.models import Word2Vec\nfrom time import time \nfrom collections import defaultdict","086e49ba":"#turn on internet option in kernel\n!pip install bangla-stemmer","42ba13e2":"from bangla_stemmer.stemmer import stemmer","6fa4a609":"\ndef text_to_word_list(text):\n    text = text.split()\n    return text\n\ndef replace_strings(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           u\"\\u00C0-\\u017F\"          #latin\n                           u\"\\u2000-\\u206F\"          #generalPunctuations\n                               \n                           \"]+\", flags=re.UNICODE)\n    english_pattern=re.compile('[a-zA-Z0-9]+', flags=re.I)\n    #latin_pattern=re.compile('[A-Za-z\\u00C0-\\u00D6\\u00D8-\\u00f6\\u00f8-\\u00ff\\s]*',)\n    \n    text=emoji_pattern.sub(r'', text)\n    text=english_pattern.sub(r'', text)\n\n    return text\n\ndef stopwordRemoval(text):    \n    x=str(text)\n    l=x.split()\n\n    stm=[elem for elem in l if elem not in stop]\n    \n    out=' '.join(stm)\n    \n    return str(out)\n\ndef remove_punctuations(my_str):\n    # define punctuation\n    punctuations = '''```\u0012\u0010\u0002\b`\u0007\b\u00a3|\u00a2|\u0007\u00d1+-*\/=EROero\u09f3\u09e6\u09e7\u09e8\u09e9\u09ea\u09eb\u09ec\u09ed\u09ee\u09ef012\u201334567\u202289\u0964!()-[]{};:'\"\u201c\\\u2019,<>.\/?@#$%^&*_~\u2018\u2014\u0965\u201d\u2030\u26bd\ufe0f\u270c\ufffd\ufff0\u09f7\ufff0'''\n    \n    no_punct = \"\"\n    for char in my_str:\n        if char not in punctuations:\n            no_punct = no_punct + char\n\n    # display the unpunctuated string\n    return no_punct\n\n\n\ndef joining(text):\n    out=' '.join(text)\n    return out\n\ndef preprocessing(text):\n    out=remove_punctuations(replace_strings(text))\n    return out\n\n\ndef Stemming(text):\n    \n    x=str(text)\n    l=x.split()\n\n    stmr = stemmer.BanglaStemmer()\n    stm = stmr.stem(l)\n\n    out=' '.join(stm)\n    \n    return str(out)\n","344b1dfa":"df =pd.read_csv('\/kaggle\/input\/corpus\/main_dataset_v3.csv')\ndata1 =pd.read_excel('\/kaggle\/input\/bangla-stopwords\/stopwords_bangla.xlsx')\nstop = data1['words'].tolist()","5fd9b5ad":"\ndf = df[(df['Sentence'].str.len()<100)&(df['Sentence'].str.len()>1)]\ndf['Sentence'].apply(lambda x: len(str(x))).plot(kind='hist');","b53d44b0":"df['Sentence'] = df.Sentence.apply(lambda x: preprocessing(str(x)))","a5ad11f8":"#df['Sentence'] = df.Sentence.apply(lambda x: Stemming(str(x)))","a88fab3b":"df['Sentence'] = df.Sentence.apply(lambda x: stopwordRemoval(str(x)))","864bc354":"df['Sentence'].apply(lambda x: len(str(x))).plot(kind='hist');\ndf.reset_index(drop=True, inplace=True)","10a29901":"df['Sentence'] = df.Sentence.apply(lambda x: text_to_word_list(str(x)))\nword2vecinput = [row for row in df.Sentence]","ad9b4640":"\nmodel = Word2Vec(word2vecinput, size=400, window=20, min_count=5,sg=0,negative=3,workers=multiprocessing.cpu_count()-1)\n","7a7c2c49":"print(model.wv.most_similar(\"\u09ae\u09be\", topn=5))","63a873bb":"print(model.wv.most_similar(\"\u0996\u09c1\u09b2\u09a8\u09be\", topn=5))","e253e9cc":"print(model.wv.most_similar(\"\u09b0\u09cb\u099c\u09be\", topn=5))","0ea435f3":"print(model.wv.most_similar(\"\u0985\u09aa\u09b0\u09be\u09a7\", topn=5))","ddc6d719":"print(model.wv.similarity('\u098b\u09a3', '\u09ac\u09cd\u09af\u09be\u0982\u0995'))","d354aafd":"print(model.wv.doesnt_match(\"\u09ac\u09be\u09b0\u09cd\u09b8\u09c7\u09b2\u09cb\u09a8\u09be \u09ab\u09c1\u099f\u09ac\u09b2 \u0997\u09cb\u09b2\u0995\u09bf\u09aa\u09be\u09b0 \u09b0\u09be\u099c\u09a8\u09c0\u09a4\u09bf\".split()))\n","0ef8ec4b":"model.wv.save_word2vec_format('corpus')\n#python -m gensim.scripts.word2vec2tensor --input model_name --output model_name\n#python -m gensim.scripts.word2vec2tensor --input model_name --output model_name","0801b7bc":"!python -m gensim.scripts.word2vec2tensor --input corpus --output corpus","5a3df1e3":"# Training Model","4186c24e":"The embeddings of word2vec can also detect that which word is not relatable and odd respective to other words in a sentence.","d03e5273":"here we can see that our trained Word2vec embeddings is well trained.As we can see from examples,it can predict the similar words pretty good.","1b0eb17b":"Stopwords removal function is applied here.","407b8eaa":"### Datasets are loaded here:\nThe datasets are the corpus and the stopwordslist.\n","7eccc489":"![Screenshot%20%288%29.png](attachment:Screenshot%20%288%29.png)","027ced3b":"![Screenshot%20%284%29.png](attachment:Screenshot%20%284%29.png)","94ff7b78":"Here i have applied Stemming.","c459f17a":"# Training Word2Vec in Bangla Corpus:\n\nIn this notebook, i will be using corpus from [this](https:\/\/scdnlab.com\/corpus\/) website.It is a fully categorised full corpus set.This dataset is created from prominent and popular newspapers from Bangladesh.It is created on varied domains like Accident,Art,Crime,Economics,Education,Entertainment,Environment,International,Opinion,Politics,Science and Tech  Sports.I used this whole corpus and created a dataset which is a collection of about 2 million sentences.This dataset will be used for Word2Vec.","f50d0fd1":" # Stemming \nTo stem the bangla words i have used a Bangla Stemmer.I have used [this](https:\/\/pypi.org\/project\/bangla-stemmer\/) package.\n\nTo activate this stemmer please turn on your internet option in kaggle notebook and run the command below.\n\nIt is a rule based stemmer.","77d81573":"A histogram is plotted here to visualize the dataset characteristics.","9f713089":"You can play with words and see their embeddings by searching on the right side.","dcaabce6":"After loading the data files,The embeddings will look something like this.It is the embedding in projected in 3D space.","f52adb09":"We can also saved the embeddings for further use.Now We will visualize it.","220f3983":"# Visualization","82ab43fe":"# Predefined Functions:\n1.text_to_word_list()-->This function is used to convert sentence into wordlist.It is necessary to prepare the input for word2vec.\n\n\n2.replace_strings()--> This function is used to remove emoji,Engish Words etc from the sentences.\n\n\n3.stopwordRemoval()--> I have a dataset for Bangla stopwords.This function removes stopwords from sentence.\n\n\n4.remove_punctuations()--> This function is used for removing many UNICODE punctuations that are not in general list but the are quite often  used in Bangla texts.\n\n\n5.joining()-->This joins the word list back into a sentence.\n\n\n6.preprocessing()-->This function calls both replace_string() and remove_punctuations() function.\n\n\n7.Stemming()-->This function stem all the words in a sentence.","65725270":"Google came up with their new tool for creating visualization for high dimensional data such as word embeddings. It\u2019s called \nembedding projector.We will use the saved model to create this visualize it.It\u2019s called embedding projector.[Here](https:\/\/projector.tensorflow.org\/) it is.They have pre-loaded visualization for MNIST dataset, Iris dataset, etc that you can play with and get used to the tool.It also provides an option to load your own word embedding data for visualization.We are going to do that thing now.After running this command,this command will generate two files: TSV file and Metadatafile.The TSV will contain all the vectors and Metadata will contain labels associated with the vectors.","a703415c":"I have only taken the sentences which are only in limited range if 100 to 1.","55ce1702":"After downloading the data from kaggle kernel,You can use the load data option in the tensorflow projector where you can select your \n\nTSV and metadatafiles.Before loading the data it will look like this.","3cfd7d7d":"Now we will run word2vec.The description about the parameter i used.\n\n1.sentences (iterable of iterables, optional) \u2013 The sentences iterable can be simply a list of lists of tokens, but for larger corpora, consider an iterable that streams the sentences directly from disk\/network.\n\n\n2.size (int, optional) \u2013 Dimensionality of the word vectors.Here i have used 400.It is the dimension of our embeddings.\n\n\n3.window (int, optional) \u2013 Maximum distance between the current and predicted word within a sentence.\n\n\n4.sg ({0, 1}, optional) \u2013 Training algorithm: 1 for skip-gram; otherwise CBOW.CBOW sometimes get better result,so i used it.\n\n\n5.negative (int, optional) \u2013 If > 0, negative sampling will be used, the int for negative specifies how many \u201cnoise words\u201d should be drawn (usually between 5-20). If set to 0, no negative sampling is used.\n\n\n6.workers (int, optional) \u2013 Use these many worker threads to train the model (=faster training with multicore machines).\n","49d3c28a":"As the first step of preprocessing,I have cleaned the datase from unnecessary things with my predefined function Preprocessing().","f6d0a8bf":"# Result","715f2ca3":"# Preprocessing","c3874fc3":"# Importing Packages and Libraries:\nFor the training of Word2Vec,i will be using Gensim.It is a widely used library for NLP works.\n\nGensim is an open-source library for unsupervised topic modeling and natural language processing.","a9624d5c":"Here the function compute cosine similarity between two words.We can see that words are very similar as the value.\n\n","3b58852f":"Preprocessed sentences are converted into wordlist as the word2vec requiers word list for each sentence."}}