{"cell_type":{"a0f5bb33":"code","5e31903c":"code","9fd444d6":"code","6a18e11b":"code","115af86a":"code","14817a3f":"code","2ddf41a7":"code","72e3c144":"code","d473ffa7":"code","275e1b31":"code","f52cb8a6":"code","6bf1b5a0":"code","db1484cb":"code","4f60b266":"code","c14dd767":"code","0a912ec3":"code","981f8260":"code","426b14b9":"code","a8312fbe":"code","66b502bc":"code","c7dbabfd":"code","232ec072":"code","4f421d2b":"code","c3de41bf":"markdown","7d4aad38":"markdown","de76db82":"markdown","6f2a98b7":"markdown","7c75b0f5":"markdown","80d1a101":"markdown","51c960e9":"markdown","832d8244":"markdown","a00742b5":"markdown","b3920817":"markdown","79f818f2":"markdown","fbd98fce":"markdown","c667ab02":"markdown","cf1f3945":"markdown","009e5f82":"markdown","ba8eaf6f":"markdown","268d64eb":"markdown","3aff19bc":"markdown","e82e5dd9":"markdown","cf759861":"markdown","71ab28fd":"markdown","36ef9efa":"markdown","5ce42a69":"markdown","b00c17ea":"markdown","238fd3ad":"markdown","abba1ae3":"markdown","f49db0d8":"markdown","4a0b3bf6":"markdown","128b3f30":"markdown","afecde13":"markdown"},"source":{"a0f5bb33":"!pip install dexplot","5e31903c":"import pandas as pd\nimport numpy as np\nimport dexplot as dxp\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom PIL import Image\n\n#plotly\n!pip install chart_studio\nimport plotly.express as px\nimport chart_studio.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot\nimport cufflinks\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True, theme='pearl')\n\n\nimport re                                  # library for regular expression operations\nimport string                              # for string operations\nimport nltk \nnltk.download('stopwords')\nfrom nltk.corpus import stopwords          # module for stop words that come with NLTK\nfrom nltk.stem import PorterStemmer        # module for stemming\nfrom nltk.tokenize import TweetTokenizer   # module for tokenizing strings\n\n#datetime\nfrom datetime import datetime\nimport os\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","9fd444d6":"ctds_episodes = pd.read_csv('..\/input\/chai-time-data-science\/Episodes.csv',parse_dates=['recording_date','release_date'])\nyoutube_thumbnails = pd.read_csv('..\/input\/chai-time-data-science\/YouTube Thumbnail Types.csv')\nanchor_thumbnails = pd.read_csv('..\/input\/chai-time-data-science\/Anchor Thumbnail Types.csv')\ndescription = pd.read_csv('..\/input\/chai-time-data-science\/Description.csv')\n\n","6a18e11b":"print(\"youtube_thumbnails dataset\")\nyoutube_thumbnails.head()\n","115af86a":"print(\"anchor_thumbnails dataset\")\nanchor_thumbnails.head()","14817a3f":"print(\"youtube_thumbnails dataset\")\nyoutube_thumbnails.head()\n","2ddf41a7":"print(\"CTDS Episodes Dataset\")\nctds_episodes.head()\n","72e3c144":"ctds_episodes.info()","d473ffa7":"labels = ctds_episodes['heroes_gender'].value_counts()[:10].index\nvalues = ctds_episodes['heroes_gender'].value_counts()[:10].values\ncolors=['#2678bf',\n '#98adbf']\n\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, textinfo='label+percent',\n                             insidetextorientation='radial',marker=dict(colors=colors))])\nfig.show()","275e1b31":"labels = ctds_episodes['heroes_nationality'].value_counts()[:10].index\nvalues = ctds_episodes['heroes_nationality'].value_counts()[:10].values\ncolors=['#bfbfbf',\n '#98adbf',\n '#1d4466',\n '#2678bf',\n '#2c6699',\n '#3780bf',\n '#3a88cc',\n '#4c89bf',\n '#729bbf',\n '#98adbf',\n '#bfbfbf']\n\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, textinfo='label+percent',\n                             insidetextorientation='radial',marker=dict(colors=colors))])\nfig.show()","f52cb8a6":"dxp.count(val='category', data=ctds_episodes,cmap='tab10',figsize=(4,3),normalize=True)\n","6bf1b5a0":"dxp.count(val='category', data=ctds_episodes,normalize=True,split='heroes_gender',figsize=(4,3))","db1484cb":"dxp.count(val='heroes_nationality', data=ctds_episodes, split='category',normalize=True,figsize=(10,6),size=0.9,stacked=True)","4f60b266":"dxp.count(val='flavour_of_tea', data=ctds_episodes,normalize=True,figsize=(6,3))","c14dd767":"ctds_episodes['episode_id'].count()","0a912ec3":"labels = ctds_episodes['recording_time'].value_counts()[:10].index\nvalues = ctds_episodes['recording_time'].value_counts()[:10].values\ncolors=['#bfbfbf','#3795bf','#2678bf','#98adbf']\n\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, textinfo='label+percent',\n                             insidetextorientation='radial',marker=dict(colors=colors))])\nfig.show()","981f8260":"dxp.count(val='flavour_of_tea', data=ctds_episodes, split='recording_time', \n          orientation='v', stacked=True)","426b14b9":"ctds_episodes['episode_duration'].iplot(kind='area',fill=True,opacity=1,xTitle='Episode',yTitle='Duration(sec)')","a8312fbe":"df = ctds_episodes[['release_date','episode_duration']]\ndf.set_index('release_date').iplot(kind='scatter',mode='markers',symbol='cross',xTitle='Release Date',yTitle='Duration(sec)')","66b502bc":"description.head()","c7dbabfd":"# text preprocessing helper functions\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n\ndef text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    #remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(tokenized_text)\n    return combined_text","232ec072":"# Applying the cleaning function to both test and training datasets\ndescription['description'] = description['description'].apply(str).apply(lambda x: text_preprocessing(x))\ndescription.head()","4f421d2b":"from wordcloud import WordCloud\n\nfont = '..\/input\/quicksandboldttf\/Quicksand-Bold.ttf'\nword_cloud = WordCloud(font_path=font,\n                       width=1600,\n                       height=800,\n                       colormap='PuRd', \n                       margin=0,\n                       max_words=500, # Maximum numbers of words we want to see \n                       min_word_length=3, # Minimum numbers of letters of each word to be part of the cloud\n                       max_font_size=150, min_font_size=20,  # Font size range\n                       background_color=\"white\").generate(\" \".join(description['description']))\n\nplt.figure(figsize=(10, 16))\nplt.imshow(word_cloud, interpolation=\"gaussian\")\nplt.axis(\"off\")\nplt.show()","c3de41bf":"Wordclouds would be a great way to explore which kinds of words dominate the description\n","7d4aad38":"\n# Conclusion\n\n> These are some of the ways in which you can explore this data and create a story around it.Remember that the motive of analysis is not to output pretty charts but to elicit important information from the data. It is just what Ben Shneiderman said, \"The purpose of visualization is insight, not pictures\".","de76db82":"### Genderwise Category distribution","6f2a98b7":"The disparity is Gender Ratio that affects the DTEM seeps sown here too. CTDS Show, you can do better.","7c75b0f5":"## Recording Time vs Tea Flavour","80d1a101":"## Tea","51c960e9":"Majority of the people who were interviewd are from U.S.A. Does it mean most of the Machine learning Practioners, Kagglers and  Researchers are in US or CTDS show?","832d8244":"Let's now view all the imported datasets.","a00742b5":"CTDS show hasn't had any female Kaggler in the past. Is this the issue with CTDS show or the proportion of female Kagglers is actually low? ","b3920817":"# Heroes\n\n## Gender Distribution","79f818f2":"## Nationality","fbd98fce":"What could be the reason that most of the shows have been recorded at night? Well, its obvious. Most of the people who were interviewed are from US and hence they fall in a different time zone. Similarly, here are some other relationships that we can decipher from the data.","c667ab02":"`clean_text()` function applies a first round of text cleaning techniques.the function text_preprocessing then takes in the processed text from the `clean_text()` function and applies techniques like tokenization and stop word removal.","cf1f3945":"As understoof above, out of the all the people interviewed, USA leads in terms of people working in Industry. On the others, Canadian Researchers outnumber researchers from other countries.","009e5f82":"# Loading the Dataset\n\nWe shall begin by importing the various datasets and the necessary libraries for the analysis.Let's first import the non-subtitles datasets.","ba8eaf6f":"There are a lot of dataset and each dataset contains some specific amount of information. Let's first begin by explorint the episodes dataset and print a concise summary of the same.","268d64eb":"# How to explore the data : A starter notebook\n\nEDA or Exploratory Data Analysis is an important aspect of any Data Analysis task. EDA essentially refers to employing a variety of techniques to uncover some underlying patterns in data. This in turn helps to get insights into the data which might otherwise may not be apparent. Apart from this EDA can also help detect outliers, missing values and at times corrupted data. There is no single technique to perform EDA. This means there are no set of rules to analyse and explore data but there are some ways by which you can get started when you encounter a new dataset. \n\nIn a way, EDA helps to explore and gather hidden insights which are backed by data and not by mere assumptions.\n\n\n## Ways to proceed in this competition\n\nBefore proceeding, it is advisable to go through https:\/\/www.kaggle.com\/rohanrao\/chai-time-data-science\/discussion\/156137 to understand the context of the competition.\n\n**Step 1**: You can either fork this notebook and continue working on this as a base notebook by clicking on the \"**Copy and Edit**\" button in the top right corner of this window.\n\nAlternatively, you can create a fresh notebook by clicking on the \"**New Notebook**\" button on the Dataset's Homepage.\n\n**Step 2**: Click on the \"**Save Version**\" button to publish your notebook.\n\n**Step 3**: Adjust the \"**Sharing\"\/\"Access**\" setting to change your notebook from private to public (and add citations if necessary).\n\n**Step 4**: Explore additional analyses at https:\/\/www.kaggle.com\/rohanrao\/chai-time-data-science\/version\/5\/kernels\n\n**Step 5**: Tell a story using the data for the chance to win a prize!\n\n**Step 6**: Make a Submission on or before 14th July, 2020. To be accepted as a valid submission, you need to submit your public notebook to the Task here : https:\/\/www.kaggle.com\/rohanrao\/chai-time-data-science\/tasks?taskId=1183. Here is a.gif which explains the process in more detail.\n\n![](https:\/\/imgur.com\/rYglABC.gif)\n\nRemember that only the notebooks submitted to the Task will be eligible for a prize.\n\nLet's start by exploring the dataset and see if we can uncover some interesting facts which could benefit the [CTDS show](https:\/\/chaitimedatascience.com\/).","3aff19bc":"\ud83d\udccc Points to note :\n\n* There are 35 columns in the dataset which are a mix of string, int, float and datetime\n* Some of the columns have missing values which we shall address later.\n* We can also quickly scan over the names of various columns.SOme columns refer to demography like country and location, other point to the views of episodes on youtube. We also have apple, spotify and anchor details.Overall it is a rich dataset and can tell about the show.\n\n\n> ### Now, there are multiple ways in which you can proceed. Tell a data story about the entire data or a subset of the data through a combination of both narrative text and data exploration. For instance :\n* What makes an interview bring more audience?\n* Does time of the day decide, whether an interview will be popular or not\n* What pattern is seen in the likes and views of a particular episode.\n\nIn this notebook, I shall explore few of the columns and then you all can take it forward with your imagination and ideas.\n","e82e5dd9":"## Episode Duration in sec","cf759861":"# Episodes dataset","71ab28fd":"# Datasets\n\nThe dataset consists of metadata of all episodes like title, category, flavour of tea, recording\/release dates along with statistics like duration, views, watch hours from YouTube, Spotify, Apple now updated until 00:00 21st June, 2020. Following data files are available :\n\n* `Description.csv` : This file consists of the descriptions texts from YouTube and Audio\n\n* `Episodes.csv` : This file contains the statistics of all the Episodes of the Chai Time Data Science show.\n\n* `YouTube Thumbnail Types.csv` : This file consists of the description of Anchor\/Audio thumbnail of the episodes\n\n* `Anchor Thumbnail Types.csv` : his file contains the statistics of the Anchor\/Audio thumbnail\n\n* `Raw Subtitles` : It is a zip file containing 74 text files having raw subtitles of all the episodes\n\n* `Cleaned Subtitles` : Zip file containing cleaned subtitles\n\nOne could either choose to create a story around all the datafiles or explore a subset.\n","36ef9efa":"Most of the interviewed people belong to Industry followed by Kaggle. Well, let's drill down and see their breaup genderwise.","5ce42a69":"# Description dataset\n\nNow, let's look the description dataset to see if we can get some information from the descriptions of the episodes.","b00c17ea":"# Episodes\n## Total no of Episodes recorded","238fd3ad":"### Nationality wise Category distribution","abba1ae3":"## Category distribution","f49db0d8":"## Preprocessing raw text\n\nBefore we start with any NLP project we need to pre-process the data to get it all in a consistent format.We need to clean, tokenize and convert our data into a matrix. Let's create a function which will perform the following tasks on the text columns:\n\nMake text lowercase,\nremoves hyperlinks,\nremove punctuation\nremoves numbers\ntokenizes\nremoves stopwords","4a0b3bf6":"## Episode Duration vs Release Date","128b3f30":"Interestingly, Masala and Ginger Tea, has been CTDS show's favourite beverage. Does that affect the episode's popularity in any way? Well, go on and find out.","afecde13":"\n## Recording Time"}}