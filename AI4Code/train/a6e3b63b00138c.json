{"cell_type":{"33ccacc1":"code","ff7ea47b":"code","535acba8":"code","8c7cf47f":"code","a9448afd":"code","64fe9877":"code","a79d87e9":"code","fab176b1":"code","20478503":"code","6af69ef5":"code","e5bd39b8":"code","ce743c54":"code","c4084522":"code","b9e86a9c":"code","457ad648":"code","770d6158":"code","5be757d1":"code","7ae4e0c5":"code","09b373aa":"code","40c0177a":"code","314947bd":"code","d7f10a29":"code","3ea52c53":"code","bff24c0d":"code","8e706ce4":"code","8f781045":"code","5b70d9ee":"code","589fb1e8":"code","0ab7c7e3":"code","90ed9818":"code","248d81d3":"code","322e5c3f":"code","4dbb5ff8":"code","2915c901":"code","e86d0328":"code","efabb755":"code","695a648e":"code","9498a8d9":"code","8512ed1c":"code","7b3f9077":"code","b210683c":"code","42c54da3":"code","04a7cda8":"code","5e930180":"code","e567e73a":"code","8249502f":"code","2f69d2c9":"code","59d2d6a5":"code","ad787ae9":"code","55f8e238":"code","f2d12656":"code","4c7d101e":"code","facaca1c":"code","0bb57a76":"code","b22ac654":"code","b185184e":"code","78998ba3":"code","6a2ed7e2":"code","8a2d463d":"code","b938c141":"code","9f282081":"code","7647c1ff":"code","0485cf17":"markdown","ae43997c":"markdown","4799586f":"markdown","01cdace2":"markdown"},"source":{"33ccacc1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ff7ea47b":"from sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score","535acba8":"pip install hiplot","8c7cf47f":"train = pd.read_csv('\/kaggle\/input\/jobathon-analytics-vidhya\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/jobathon-analytics-vidhya\/test.csv')","a9448afd":"import hiplot as hip\ndata = train.drop(['Region_Code', 'ID'], axis = 1).to_dict(orient = 'records')","64fe9877":"hip.Experiment.from_iterable(data).display()","a79d87e9":"test['Response'] = -1","fab176b1":"test.columns","20478503":"from lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score","6af69ef5":"from functools import wraps\nimport datetime as dt\n\ndef log_step(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        tic = dt.datetime.now()\n        result = func(*args, **kwargs)\n        time_taken = str(dt.datetime.now() - tic)\n        print(f\"just ran step {func.__name__} shape={result.shape} took {time_taken}s\")\n        return result\n    return wrapper","e5bd39b8":"@log_step\ndef LGBM(train, test): \n\n    NUM_OF_BOOST_ROUND = 10000\n    EARLY_STOPPING = 300\n    SEED = 2021  \n\n    params = {\n        'cat_features': cat_features_index,\n        'metric': 'auc',\n        'seed': SEED,\n        'n_estimators': NUM_OF_BOOST_ROUND\n    }\n\n    clf = LGBMClassifier(**params)\n    clf.fit(X_train, y_train, eval_set = (X_valid, y_valid), early_stopping_rounds = 100, verbose = -1)\n    \n    ypred_lgb = clf.predict_proba(X_valid)[:,1]\n    print(metrics.roc_auc_score(y_valid, ypred_lgb))\n    \n    return metrics.roc_auc_score(y_valid, ypred_lgb)","ce743c54":"@log_step\ndef categorical(data):\n    \n    \"\"\"\n    Find categorical columns and Label Encode them \n    \"\"\"\n\n    cat_columns = []\n\n    for col in data.select_dtypes('object').columns:\n        print(col)\n        cat_columns.append(col)\n        le = LabelEncoder()\n        data[col] = le.fit_transform(data[col])\n        \n        cat_features_index = [i for i, col in enumerate(train.columns) if col in cat_columns]\n    \n    return data, cat_features_index","c4084522":"from sklearn.preprocessing import LabelEncoder","b9e86a9c":"@log_step\ndef encoding(data):\n\n    \"\"\"\n    One Hot Encoding and Label Encoding \n    \"\"\"\n    le = LabelEncoder()\n    data['Holding_Policy_Duration'] = le.fit_transform(data['Holding_Policy_Duration'])\n\n    var_mod = ['Accomodation_Type', 'Reco_Insurance_Type', 'Is_Spouse','Health Indicator', 'Holding_Policy_Duration']\n\n    for i in var_mod:\n        data[i] = le.fit_transform(data[i])\n\n    # One Hot Encoding : \n    data = pd.get_dummies(data, columns = ['Accomodation_Type', 'Reco_Insurance_Type', 'Is_Spouse','Health Indicator', 'Holding_Policy_Duration'])\n    \n    return data\n    ","457ad648":"@log_step\ndef preprocess(data):\n\n    data['Holding_Policy_Type'] = data['Holding_Policy_Type'].astype(str)\n\n    data['Reco_Policy_Cat'] = data['Reco_Policy_Cat'].astype(str)\n\n    data['Region_Code'] = data['Region_Code'].astype(str)\n    \n    return data\n","770d6158":"@log_step\ndef impute(data):\n    \n    data['Holding_Policy_Duration'] = data['Holding_Policy_Duration'].fillna(str(0.0))\n    data['Holding_Policy_Type'] = data['Holding_Policy_Type'].fillna('no_policies')\n    data['Health Indicator'] = data['Health Indicator'].fillna(data['Health Indicator'].mode()[0])\n    \n    return data\n","5be757d1":"@log_step\ndef feature_engineering(data):\n    \n    for i in range(len(data.drop('Response', axis = 1).columns)): \n    \n        for j in range(i):\n\n            a = data.columns[i]\n            b = data.columns[j]\n            data[a+'_'+b] = data[a].astype(str)+'_'+data[b].astype(str)\n\n    return data","7ae4e0c5":"@log_step\ndef start_pipeline(dataf):\n    return dataf.copy() ","09b373aa":"train_df = (train\n      .pipe(start_pipeline)\n      .pipe(impute)\n      .pipe(feature_engineering)\n      .pipe(encoding))","40c0177a":"test.columns","314947bd":"test_df = (test\n      .pipe(start_pipeline)\n      .pipe(impute)\n      .pipe(feature_engineering)\n      .pipe(encoding))","d7f10a29":"import matplotlib.pyplot as plt","3ea52c53":"from sklearn.feature_selection import VarianceThreshold\nsel = VarianceThreshold(threshold = (.4 * (1- .4)))\ny = train_df.select_dtypes(include = ('boolean'))\nsel.fit_transform(train_df[y])","bff24c0d":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2","8e706ce4":"train = train_df.select_dtypes(exclude='object')\ntest = test_df.select_dtypes(exclude='object')\n\n# Seperate Features and Target\nX= train_df.drop(columns = ['Response'], axis=1)\ny= train_df['Response']\n\nX_train,X_valid,y_train,y_valid = train_test_split(X,y,test_size=0.2)\n","8f781045":"X1 = train_df.select_dtypes(include = 'int')","5b70d9ee":"X_new = SelectKBest(chi2, k = 2).fit_transform(X1, y)","589fb1e8":"X_new.shape","0ab7c7e3":"X.head()","90ed9818":"X2 = train_df.select_dtypes(include = ['int'])","248d81d3":"X2","322e5c3f":"from sklearn.ensemble import ExtraTreesClassifier","4dbb5ff8":"X = X.select_dtypes(exclude = 'object')","2915c901":"extra_tree_forest = ExtraTreesClassifier(n_estimators = 5,\n                                        criterion = 'entropy', max_features = 2)\n\nextra_tree_forest.fit(X, y)","e86d0328":"feature_importance = extra_tree_forest.feature_importances_\n\nfeature_importance_normalized = np.std([tree.feature_importances_ for tree in extra_tree_forest.estimators_],\n                                      axis = 0)","efabb755":"import matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nfigure(figsize = (20, 18))\n\nplt.bar(X.columns, feature_importance_normalized)\nplt.xlabel('Feature_Labels')\nplt.ylabel('Feature Importances')\nplt.title('Comparison of Different Feature Importances')\nplt.show()","695a648e":"feature_importance_normalized > feature_importance_normalized.mean()","9498a8d9":"a = []\n\nfor i in range(len(feature_importance_normalized)):\n    \n    if feature_importance_normalized[i] > feature_importance_normalized.mean():\n        \n        a.append(i)","8512ed1c":"a","7b3f9077":"len(feature_importance_normalized)","b210683c":"X.columns","42c54da3":"len(X.columns)","04a7cda8":"len(a)","5e930180":"# Columns with feature importances extracted from Extra tree classifier\nmat = X.iloc[:,a]","e567e73a":"mat.shape","8249502f":"# 20 % data as validation set\nX_train,X_valid,y_train,y_valid = train_test_split(train_df,y,test_size=0.2,random_state=22)","2f69d2c9":"from lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score","59d2d6a5":"lgb = LGBMClassifier(boosting_type='gbdt',n_estimators=500,max_depth=7,learning_rate=0.04,objective='binary',metric='auc',is_unbalance=True,\n                 colsample_bytree=0.5,reg_lambda=2,reg_alpha=2,random_state=101,n_jobs=-1)\n\n\n\nlgb.fit(X_train,y_train)","ad787ae9":"\nprint(roc_auc_score(y_valid,lgb.predict_proba(X_valid)[:,1]))","55f8e238":"from catboost import CatBoostClassifier","f2d12656":"X_train = X_train.astype(int)\nX_valid = X_valid.astype(int)","4c7d101e":"train_df = train_df.select_dtypes(exclude = 'int').astype(str)\ntest_df = test_df.select_dtypes(exclude = 'int').astype(str)\n","facaca1c":"cat_columns = []\n\nfor col in train_df.select_dtypes('object').columns:\n    print('Train:',col)\n    cat_columns.append(col)\n    le = LabelEncoder()\n    train_df[col] = le.fit_transform(train_df[col])\n    \n\nfor col in test_df.select_dtypes('object').columns:\n    print('Test:',col)\n\n    le = LabelEncoder()\n    test_df[col] = le.fit_transform(test_df[col])","0bb57a76":"cat_columns","b22ac654":"cat_features_index = [i for i, col in enumerate(train.columns) if col in cat_columns]\ncat_features_index","b185184e":"NUM_OF_BOOST_ROUND = 10000\nEARLY_STOPPING = 300\nparams = {\n    'cat_features': cat_features_index,\n    'eval_metric': 'AUC',\n    'random_seed': SEED,\n    'n_estimators': NUM_OF_BOOST_ROUND,\n}\n\nSEED = 2021","78998ba3":"params","6a2ed7e2":"X_train","8a2d463d":"bst = CatBoostClassifier(**params, early_stopping_rounds=EARLY_STOPPING)\n_ = bst.fit(X_train, y_train, eval_set=(X_valid,y_valid), plot=True, verbose=False)\n","b938c141":"pip install hiplot","9f282081":"import hiplot as hip","7647c1ff":"pip install -U streamlit hiplot","0485cf17":"# THIS IS THE BOMBS !!!","ae43997c":"# EXTRA TREE CLASSIFIER FEATURE SELECTION","4799586f":"# I tried Feature Selection using Extra Tree Classifier but it gave a much less score 0.62\n# But when I took in all columns it is giving me a much higher score. Why ? \n# What can be done ??","01cdace2":"# TRAIN TEST SPLIT"}}