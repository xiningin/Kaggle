{"cell_type":{"b0823a67":"code","7f57fe17":"code","473a8fd5":"code","f36c754d":"code","80c2b4f3":"code","28985b3e":"code","2de48d4d":"code","b9d0ef3d":"code","6d0043d3":"code","784e25ed":"code","f40e6d77":"code","82cd5bd3":"code","39760847":"code","06f38bdc":"code","acf0f86e":"code","8ef7b934":"code","6eeceb8c":"code","2f73274a":"code","16f58613":"code","aeb9e666":"code","b8c67e8a":"code","2fca7908":"code","d729642a":"code","fb9ad412":"code","dd561bcc":"code","7f58b221":"code","9a210a32":"code","f522ead0":"code","2a63ce75":"code","eb935503":"code","a3b3a6e2":"code","b8d93ffd":"code","dc71b619":"code","6a751511":"code","d0d206b3":"code","cf097ea3":"code","bdbd7843":"code","ce7c6f81":"code","aca5234c":"code","ea502f68":"markdown","4fc14382":"markdown","55b0a32f":"markdown","92793743":"markdown","8f8ae577":"markdown","a7e3486c":"markdown","65816e85":"markdown","f9ff2b96":"markdown","714a5e0b":"markdown","9236fce2":"markdown","db246f79":"markdown","cd885706":"markdown","f0ee4865":"markdown","12ce9446":"markdown","ba4a6cb8":"markdown","858fa41f":"markdown","81e2a947":"markdown","2469f9df":"markdown","0a7a8bda":"markdown","32c20628":"markdown","848891a9":"markdown","f84479e4":"markdown","cd967520":"markdown","66a2091e":"markdown","4ed40b51":"markdown","31ae30e6":"markdown","0354c8c1":"markdown","e045a8d8":"markdown","60fb990a":"markdown","44716cec":"markdown","166a5de2":"markdown","a7a2fc24":"markdown","20f0adb2":"markdown","f6ef861d":"markdown"},"source":{"b0823a67":"import os\nimport json\nfrom pprint import pprint\nfrom copy import deepcopy\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm","7f57fe17":"def format_name(author):\n    middle_name = \" \".join(author['middle'])\n    \n    if author['middle']:\n        return \" \".join([author['first'], middle_name, author['last']])\n    else:\n        return \" \".join([author['first'], author['last']])\n\n\ndef format_affiliation(affiliation):\n    text = []\n    location = affiliation.get('location')\n    if location:\n        text.extend(list(affiliation['location'].values()))\n    \n    institution = affiliation.get('institution')\n    if institution:\n        text = [institution] + text\n    return \", \".join(text)\n\ndef format_authors(authors, with_affiliation=False):\n    name_ls = []\n    \n    for author in authors:\n        name = format_name(author)\n        if with_affiliation:\n            affiliation = format_affiliation(author['affiliation'])\n            if affiliation:\n                name_ls.append(f\"{name} ({affiliation})\")\n            else:\n                name_ls.append(name)\n        else:\n            name_ls.append(name)\n    \n    return \", \".join(name_ls)\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    \n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    \n    return body\n\ndef format_bib(bibs):\n    if type(bibs) == dict:\n        bibs = list(bibs.values())\n    bibs = deepcopy(bibs)\n    formatted = []\n    \n    for bib in bibs:\n        bib['authors'] = format_authors(\n            bib['authors'], \n            with_affiliation=False\n        )\n        formatted_ls = [str(bib[k]) for k in ['title', 'authors', 'venue', 'year']]\n        formatted.append(\", \".join(formatted_ls))\n    return \"; \".join(formatted)","473a8fd5":"def load_files(dirname):\n    filenames = os.listdir(dirname)\n    raw_files = []\n\n    for filename in tqdm(filenames):\n        filename = dirname + filename\n        file = json.load(open(filename, 'rb'))\n        raw_files.append(file)\n    \n    return raw_files\n\ndef generate_clean_df(all_files):\n    cleaned_files = []\n    \n    for file in tqdm(all_files):\n        features = [\n            file['paper_id'],\n            file['metadata']['title'],\n            format_authors(file['metadata']['authors']),\n            format_authors(file['metadata']['authors'], \n                           with_affiliation=True),\n            format_body(file['abstract']),\n            format_body(file['body_text']),\n            format_bib(file['bib_entries']),\n            file['metadata']['authors'],\n            file['bib_entries']\n        ]\n\n        cleaned_files.append(features)\n\n    col_names = ['paper_id', 'title', 'authors',\n                 'affiliations', 'abstract', 'text', \n                 'bibliography','raw_authors','raw_bibliography']\n    clean_df = pd.DataFrame(cleaned_files, columns=col_names)\n    clean_df.head()\n    \n    return clean_df","f36c754d":"biorxiv_dir = '\/kaggle\/input\/cord1933k\/biorxiv_medrxiv\/biorxiv_medrxiv\/'\nfilenames = os.listdir(biorxiv_dir)\nprint(\"Number of articles retrieved from biorxiv:\", len(filenames))","80c2b4f3":"all_files = []\n\nfor filename in filenames:\n    filename = biorxiv_dir + filename\n    file = json.load(open(filename, 'rb'))\n    all_files.append(file)","28985b3e":"file = all_files[0]\nprint(\"Dictionary keys:\", file.keys())","2de48d4d":"pprint(file['abstract'])","b9d0ef3d":"#dictionary \nprint(\"body_text type:\", type(file['body_text']))\nprint(\"body_text length:\", len(file['body_text']))\nprint(\"body_text keys:\", file['body_text'][0].keys())","6d0043d3":"print(\"body_text content:\")\npprint(file['body_text'][:2], depth=3)","784e25ed":"#let's see what the grouped section titles are for the example above:\n\ntexts = [(di['section'], di['text']) for di in file['body_text']]\ntexts_di = {di['section']: \"\" for di in file['body_text']}\nfor section, text in texts:\n    texts_di[section] += text\n\npprint(list(texts_di.keys()))","f40e6d77":"#dictionary\nprint(all_files[0]['metadata'].keys())\n","82cd5bd3":"print(all_files[0]['metadata']['title'])","39760847":"authors = all_files[0]['metadata']['authors']\npprint(authors[:3])","06f38bdc":"#use of format name and affiliation\nfor author in authors:\n    print(\"Name:\", format_name(author))\n    print(\"Affiliation:\", format_affiliation(author['affiliation']))\n    print()","acf0f86e":"#another example\n#pprint(all_files[4]['metadata'], depth=4)","8ef7b934":"bibs = list(file['bib_entries'].values())\npprint(bibs[:2], depth=4)","6eeceb8c":"#format_authors(bibs[1]['authors'], with_affiliation=False)","2f73274a":"bib_formatted = format_bib(bibs[:5])\nprint(bib_formatted)","16f58613":"cleaned_files = []\n\nfor file in tqdm(all_files):\n    features = [\n        file['paper_id'],\n        file['metadata']['title'],\n        format_authors(file['metadata']['authors']),\n        format_authors(file['metadata']['authors'], \n                       with_affiliation=True),\n        format_body(file['abstract']),\n        format_body(file['body_text']),\n        format_bib(file['bib_entries']),\n        file['metadata']['authors'],\n        file['bib_entries']\n    ]\n    \n    cleaned_files.append(features)","aeb9e666":"\ncol_names = [\n    'paper_id', \n    'title', \n    'authors',\n    'affiliations', \n    'abstract', \n    'text', \n    'bibliography',\n    'raw_authors',\n    'raw_bibliography'\n]\n\nclean_df = pd.DataFrame(cleaned_files, columns=col_names)\nclean_df.head()","b8c67e8a":"clean_df.to_csv('biorxiv_clean.csv', index=False)\n","2fca7908":"#pmc_dir = '\/kaggle\/input\/CORD-19-research-challenge\/custom_license\/custom_license\/pdf_json\/'\n#pmc_files = load_files(pmc_dir)\n#pmc_df = generate_clean_df(pmc_files)\n#pmc_df.to_csv('clean_pmc.csv', index=False)\n#pmc_df.head()","d729642a":"biorxiv_clean = pd.read_csv('\/kaggle\/input\/cord1933k\/biorxiv_clean.csv')","fb9ad412":"#verification everything is allright\n#biorxiv_clean.head(2)\n#first paper example\n#biorxiv_clean.text[0]","dd561bcc":"df_covid['abstract_word_count'] = biorxiv_clean['abstract'].apply(lambda x: len(x.strip().split()))  # word count in abstract\ndf_covid['body_word_count'] = biorxiv_clean['body_text'].apply(lambda x: len(x.strip().split()))  # word count in body\ndf_covid['body_unique_words']=biorxiv_clean['body_text'].apply(lambda x:len(set(str(x).split())))  # number of unique words in body\ndf_covid.head()","7f58b221":"df_covid.info()","9a210a32":"df_covid['abstract'].describe(include='all')","f522ead0":"df_covid.drop_duplicates(['abstract', 'body_text'], inplace=True)\ndf_covid['abstract'].describe(include='all')","2a63ce75":"df_covid['body_text'].describe(include='all')","eb935503":"from tqdm import tqdm\nfrom langdetect import detect\nfrom langdetect import DetectorFactory\n\n# set seed\nDetectorFactory.seed = 0\n\n# hold label - language\nlanguages = []\n\n# go through each text  #CAMBIAR ESTO. \nfor ii in tqdm(range(0,len(df))):\n    # split by space into list, take the first x intex, join with space\n    text = df.iloc[ii]['body_text'].split(\" \")\n    \n    lang = \"en\"\n    try:\n        if len(text) > 50: \n            lang = detect(\" \".join(text[:50]))\n        elif len(text) > 0:\n            lang = detect(\" \".join(text[:len(text)]))\n    # ught... beginning of the document was not in a good format\n    except Exception as e:\n        all_words = set(text)\n        try:\n            lang = detect(\" \".join(all_words))\n        # what!! :( let's see if we can find any text in abstract...\n        except Exception as e:\n            \n            try:\n                # let's try to label it through the abstract then\n                lang = detect(df.iloc[ii]['abstract_summary'])\n            except Exception as e:\n                lang = \"unknown\"\n                pass\n    \n    # get the language    \n    languages.append(lang)","a3b3a6e2":"from pprint import pprint\n\nlanguages_dict = {}\nfor lang in set(languages):\n    languages_dict[lang] = languages.count(lang)\n    \nprint(\"Total: {}\\n\".format(len(languages)))\npprint(languages_dict)","b8d93ffd":"df['language'] = languages\nplt.bar(range(len(languages_dict)), list(languages_dict.values()), align='center')\nplt.xticks(range(len(languages_dict)), list(languages_dict.keys()))\nplt.title(\"Distribution of Languages in Dataset\")\nplt.show()","dc71b619":"df = df[df['language'] == 'en'] \ndf.info()","6a751511":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\n\nstop_words = set(STOPWORDS)\n#https:\/\/www.kaggle.com\/gpreda\/cord-19-solution-toolbox\n","d0d206b3":"print(stop_words)","cf097ea3":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n\nstop_words.extend(['background', 'methods', 'introduction', 'conclusions', 'results', \n                   'purpose', 'materials', 'discussions','methodology','result analysis',\n                   'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n                   'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', \n                   'al.', 'Elsevier', 'PMC', 'CZI', 'www'])","bdbd7843":"print (stop_words)","ce7c6f81":"def show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stop_words,\n        max_words=200,\n        max_font_size=30, \n        scale=5,\n        random_state=1\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(10,10))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=14)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","aca5234c":"show_wordcloud(biorxiv_clean['abstract'], title = 'biorxiv_clean - papers Abstract - frequent words (400 sample)')","ea502f68":"IN THIS CODE WE ARE GOING TO PREPROCESS OUR ORIGINAL DATABASE, THE CORD-19 CHALLENGE ONE, SO IT CAN BE EASIER PROCESSED LATER FOR OUT OBJECTIVE. \n\nWe are goiing to develop this pre-process in 3: \n1. Cleanser\n2. Eliminate duplicates\n3. Select English languae\n\nAnd as an extra , we are going to write down our personalized STOPWORDS for later. ","4fc14382":"**STEP 2: ELIMINATE DUPLICATES**","55b0a32f":"**STEP 1: CLEANSER**","92793743":"**********************************","8f8ae577":"**STEP 3: SELECT ENGLISH**","a7e3486c":"***************\n","65816e85":"Stopwords can be extracted from various libraries. Dos not matter which one we choose to use, as far as it is the english ones. \n\nWe also need to customize it a bit so they are the most accurate they can be. \n\nThis part was inspired in various notebooks such as: \n\nhttps:\/\/www.kaggle.com\/mobassir\/mining-covid-19-scientific-papers \nhttps:\/\/www.kaggle.com\/imdevskp\/covid-19-analysis-visualization-comparisons\nhttps:\/\/www.kaggle.com\/imdevskp\/covid-19-analysis-visualization-comparisons\n","f9ff2b96":"**EXTRA: OUR PERSONALIZED STOPWORDS  (TO BE USED  IN THE PROCESSING PART)**","714a5e0b":"-> we cab do tge same for Generate Custom (PMC), Commercial, Non-commercial licenses, it will be the blow code repeated for each of them (x3)","9236fce2":"*****************\n","db246f79":"Now we have the CSV datafram, we are going to analize the content.For that , we use a counter.  This part is highly inspired on this previous notebook,  https:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clustering from which we extracted what interested us. \n\n-> Adding word count columns for both abstract and body_text can be useful parameters later:\n","cd885706":"**-> OPTION 2:** from NLTK","f0ee4865":"**-> OPTION 1:** from wordcloud. ","12ce9446":"-> or the metadata","ba4a6cb8":"Biorxiv: Generate CSV\n\nIn this section, I show you how to manually generate the CSV files.\nAs you can see, it's now super simple because of the format_ helper functions.\nIn the next sections, I show you have to generate them in 3 lines using the load_files and generate_clean_dr helper functions.","858fa41f":"Before developping our code we are going to clean it. Therefore, we are going to use this previous notebook: https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv\/data?\nWe are almost using the same things that where developed, but with sigle differences. As we want to focus on the BIORXIV part and on the abstracts.\n","81e2a947":"but we cannot customize them!!! Therefore:","2469f9df":"*********************\n\n","0a7a8bda":"We will be dropping any language that is not English.","32c20628":"-> Lets take a look at the language distribution in the dataset","848891a9":"-> When we look at the unique values above, we can see that tehre are duplicates. It may have caused because of author submiting the article to multiple journals. Let's remove the duplicats from our dataset:\n\n(Thank you Desmond Yeoh for recommending the below approach on Kaggle)","f84479e4":"The following function let you format the bibliography all at once. It only extracts the title, authors, venue, year, and separate each entry of the bibliography with a ;.","cd967520":"-> last element, the bibliography","66a2091e":"The content: the body text is separated into a list of small subsections, each containing a section and a text key. Since multiple subsection can have the same section, we need to first group each subsection before concatenating everything.","4ed40b51":"**Biorxiv: Exploration**\n\n> -> load all of the json files into a list of nested dictionaries (each dict is an article).","31ae30e6":"1. IMPORT LIBRARIES","0354c8c1":"First visualization of words of the BIORXIV papers so we can have a first idea of what we may find -> WordCloud","e045a8d8":"****\n","60fb990a":"->* Abstract*\nThe abstract dictionary is fairly simple:","44716cec":"BUT WE ARE NOT INTERESTED","166a5de2":"-> Handling multiple languages\nNext we are going to determine the language of each paper in the dataframe. Not all of the sources are English and the language needs to be identified so that we know how handle these instances\n","a7a2fc24":"2. CREATE FUNCTIONS\n","20f0adb2":"Now the data is clean , we have decided we are going to focus on the ABSTRACT from the BIORXIV part, and that we'll do. ","f6ef861d":"-> eventually, we can look at other elements, as for example the body text\n"}}