{"cell_type":{"c56f6f99":"code","b6f7bdb8":"code","fdef8e75":"code","5ed049d9":"code","e5ad92e0":"code","f6ded71f":"code","d5fc2b02":"code","3e342fa1":"code","a2a38151":"code","91149140":"code","1984779d":"code","be21bff4":"code","1a99df9c":"code","daf6fc9d":"code","bce16656":"code","d7f2246e":"code","74af9247":"code","9b75fa45":"code","cc2c70d4":"code","1001016e":"code","8616e993":"code","ed7baf3f":"code","f50c6580":"code","978ac847":"code","093e6cbd":"code","b66c14fe":"markdown","af752a72":"markdown","dad0fd87":"markdown","6beb6bec":"markdown","fbf12867":"markdown","f725f048":"markdown","b4fcaf4a":"markdown","4598b19f":"markdown","b9e86e78":"markdown","01288b42":"markdown","76f21f3f":"markdown","b720c5a8":"markdown","f9917bf4":"markdown","d2dcccc0":"markdown","64d41f8d":"markdown","2db6654c":"markdown","b6093609":"markdown","2a336751":"markdown","244340e8":"markdown","5701f149":"markdown","1f99b018":"markdown","7ffba2af":"markdown","26c5fcc4":"markdown","2dc51c94":"markdown","c221756f":"markdown","2baa3b6b":"markdown","a4483549":"markdown","f5d91c16":"markdown","fc92f609":"markdown","a57829b5":"markdown","73ecc284":"markdown","0c0de679":"markdown","c6ec5ebc":"markdown","e3732bb0":"markdown","6ed0cf19":"markdown","eb9df6b1":"markdown","9bd0a5fb":"markdown","62791211":"markdown","0947893a":"markdown"},"source":{"c56f6f99":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport warnings\n","b6f7bdb8":"train=pd.read_csv(\"..\/input\/train.csv\")\ntest=pd.read_csv(\"..\/input\/test.csv\")\ntitanic=pd.concat([train, test], sort=False)\nlen_train=train.shape[0]\n\n","fdef8e75":"train.head(20)","5ed049d9":"train.isnull().sum()[train.isnull().sum()>0]","e5ad92e0":"test.isnull().sum()[test.isnull().sum()>0]","f6ded71f":"test.Fare=test.Fare.fillna(titanic.Fare.mean())","d5fc2b02":"train.Cabin=train.Cabin.fillna(\"unknow\")\ntest.Cabin=test.Cabin.fillna(\"unknow\")","3e342fa1":"train.Embarked.mode()[0]","a2a38151":"train.Embarked=train.Embarked.fillna(train.Embarked.mode()[0])\ntest.Embarked=test.Embarked.fillna(train.Embarked.mode()[0])","91149140":"warnings.filterwarnings(action=\"ignore\")\nplt.figure(figsize=[20,15])\nplt.subplot(3,3,1)\nsns.barplot('Pclass','Survived',data=train).set_title(\"Survivors by passenger class\")\nplt.subplot(3,3,2)\nsns.barplot('SibSp','Survived',data=train).set_title(\"Survivors that had siblings\")\nplt.subplot(3,3,3)\nsns.barplot('Parch','Survived',data=train).set_title(\"Survivors with Partner\")\nplt.subplot(3,3,4)\nsns.barplot('Sex','Survived',data=train).set_title(\"Survivors by gender\")\nplt.subplot(3,3,5)\nsns.barplot('Ticket','Survived',data=train).set_title(\"Correlation of Ticket\")\nplt.subplot(3,3,6)\nsns.barplot('Cabin','Survived',data=train).set_title(\"Correlation of Cabin\")\nplt.subplot(3,3,7)\nsns.barplot('Embarked','Survived',data=train).set_title(\"Correlation of Embarkment location\")\nplt.subplot(3,3,8)\nsns.distplot(train[train.Survived==1].Age.dropna(), color='green', kde=False).set_title(\"Distribution of Survivors by Age\")\nsns.distplot(train[train.Survived==0].Age.dropna(), color='orange', kde=False)\nplt.subplot(3,3,9)\nsns.distplot(train[train.Survived==1].Fare, color='green', kde=False).set_title(\"Distribution of Survivors by Fare\")\nsns.distplot(train[train.Survived==0].Fare, color='orange', kde=False)","1984779d":"train['Title']=train.Name.apply(lambda x: x.split('.')[0].split(',')[1].strip())\ntest['Title']=test.Name.apply(lambda x: x.split('.')[0].split(',')[1].strip())\n\nnewtitles={\n    \"Capt\":       \"Officer\",\n    \"Col\":        \"Officer\",\n    \"Major\":      \"Officer\",\n    \"Jonkheer\":   \"Royalty\",\n    \"Don\":        \"Royalty\",\n    \"Sir\" :       \"Royalty\",\n    \"Dr\":         \"Officer\",\n    \"Rev\":        \"Officer\",\n    \"the Countess\":\"Royalty\",\n    \"Dona\":       \"Royalty\",\n    \"Mme\":        \"Mrs\",\n    \"Mlle\":       \"Miss\",\n    \"Ms\":         \"Mrs\",\n    \"Mr\" :        \"Mr\",\n    \"Mrs\" :       \"Mrs\",\n    \"Miss\" :      \"Miss\",\n    \"Master\" :    \"Master\",\n    \"Lady\" :      \"Royalty\"}\ntrain['Title']=train.Title.map(newtitles)\ntest['Title']=test.Title.map(newtitles)\n\n","be21bff4":"train.groupby(['Title','Sex']).Age.mean()","1a99df9c":"title_mapping = {'Master': 1, 'Miss':2,\"Mr\": 3, \"Mrs\": 4, \"Officer\":5, \"Royalty\":6}\ntrain['Title'] = train.Title.map(title_mapping)\ntest['Title'] = test.Title.map(title_mapping)\nembarked_mapping = {'S': 1, 'C':2, 'Q': 3, '': 0}\ntrain['Embarked'] = train.Embarked.map(embarked_mapping)\ntest['Embarked'] = test.Embarked.map(embarked_mapping)\ngender_mapping = {'male': 0, 'female': 1}\ntrain['Sex'] = train.Sex.map(gender_mapping)\ntest['Sex'] = test.Sex.map(gender_mapping)\ntrain = train.astype({'Title': 'int32', 'Embarked' : 'int32', 'Sex' : 'int32'})\ntest = test.astype({'Title': 'int32', 'Embarked' : 'int32', 'Sex' : 'int32'})\ntrain.head()","daf6fc9d":"train_age = pd.concat([train, test], sort=False).drop(['PassengerId','Survived','SibSp','Parch','Ticket','Fare','Cabin','Name'],axis=1).dropna()\ntrain_age = train_age.astype({'Age': 'int32'})\ntrain_age.head(10)","bce16656":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=150,criterion='gini',random_state=1)\ntrain_age_X = train_age.drop(['Age'],axis=1)\ntrain_age_y = train_age['Age']\nrf.fit(train_age_X, train_age_y)\n\n","d7f2246e":"train_predict = train.drop(['PassengerId','Survived','SibSp','Parch','Ticket','Fare','Cabin','Name','Age'],axis=1)\ntrain['Age_predicted'] = rf.predict(train_predict)\ntest_predict = test.drop(['PassengerId','SibSp','Parch','Ticket','Fare','Cabin','Name','Age'],axis=1)\ntest['Age_predicted'] = rf.predict(test_predict)","74af9247":"train[np.isnan(train.Age)].head(20)","9b75fa45":"test[np.isnan(test.Age)].head(20)","cc2c70d4":"train['FamilySize'] = train['SibSp'] + train['Parch'] + 1\ntest['FamilySize'] = test['SibSp'] + test['Parch'] + 1\n    \npd.crosstab(train['FamilySize'], train['Survived'], normalize='index').plot(kind='bar', stacked=True, title=\"Survived by family size (%)\")","1001016e":"train.drop(['PassengerId','Name','Ticket','SibSp','Parch','Fare','Ticket','Cabin','Age'],axis=1,inplace=True)\ntest.drop(['PassengerId','Name','Ticket','SibSp','Parch','Fare','Ticket','Cabin','Age'],axis=1,inplace=True)\ntrain.head(10)","8616e993":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\n\ntrain_survival_X = train.drop(['Survived'],axis=1)\ntrain_survival_y = train['Survived']\n","ed7baf3f":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(random_state=1)\n\nrf_params=[{'n_estimators':[100,150],'criterion':['gini','entropy']}]\ngs_rf=GridSearchCV(estimator=rf, param_grid=rf_params, scoring='accuracy')\ngs_rf.fit(train_survival_X,train_survival_y)\n\nprint (gs_rf.best_score_)\nprint (gs_rf.best_estimator_)\n","f50c6580":"from sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\nsvc=make_pipeline(StandardScaler(),SVC(random_state=1))\nr=[0.0001,0.001,0.1,1,10,50,100]\nsvm_params=[{'svc__C':r, 'svc__kernel':['linear']},\n      {'svc__C':r, 'svc__gamma':r, 'svc__kernel':['rbf']}]\ngs_svm=GridSearchCV(estimator=svc, param_grid=svm_params, scoring='accuracy', cv=10)\ngs_svm.fit(train_survival_X.astype(float), train_survival_y)\nprint (gs_svm.best_score_)\nprint (gs_svm.best_estimator_)","978ac847":"from sklearn.neighbors import KNeighborsClassifier\n\nknn_params = {'algorithm': ['auto'], 'weights': ['uniform', 'distance'], 'leaf_size': list(range(1,50,5)), \n               'n_neighbors': [6,7,8,9,10,11,12,14,16,18,20,22]}\ngs_knn=GridSearchCV(estimator = KNeighborsClassifier(), param_grid = knn_params, scoring = \"accuracy\", cv=15)\ngs_knn.fit(train_survival_X, train_survival_y)\nprint(gs_knn.best_score_)\nprint(gs_knn.best_estimator_)\n","093e6cbd":"prediction = gs_svm.best_estimator_.predict(test)\n\n# load test set again to ensure we have the correct PassengerIDs as we dropped them before\ntest=pd.read_csv(\"..\/input\/test.csv\")\noutput=pd.DataFrame({'PassengerId':test['PassengerId'],'Survived':prediction})\n\noutput.to_csv('submission_rf.csv', index=False)","b66c14fe":"Within this chapter we will now apply threee classification models to our prepared training set. Why those three? Because I selected them for demonstration purpose. You can of course try out even more approaches and see if you can exceed the results.","af752a72":"KNN is a rather simple algorithm, but perfect for classifying based on numerical values. It calculates the distances of data points and selects the _k_ smallest distances in order to find values that are as similar as possible.Finally it returns the mode of the labels (in our case 'Survived') for the list of _k_ smallest distances. More details here: \nhttps:\/\/towardsdatascience.com\/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761\n\nWe define also here different parameters you will find in the dict *knn\\_params*","dad0fd87":"This one is known from the age prediction. Random Forest produces a 'forest' of decision trees to finally rate them and select best matching decision tree for a prediction. The details are explained very comprehensive in this article: https:\/\/towardsdatascience.com\/understanding-random-forest-58381e0602d2\n\nWe will actually try out several random forest classifier settings:\n- estimators: between 100 and 150\n- criterion: gini or entropy\n\nand test different parameter settings by using GridSearchCV for hyper parameter tuning.\nFinally, we print the best score we reached and the parameter setting that was used to produce this score.","6beb6bec":"## 1.5 - Conclude Feature Engineering","fbf12867":"When exploring the data we recognized, that SibSp and Parch do not have a strong correlation to Survived.\nNevertheless, families on board had a higher chance to survive than a single person, because of the rule \"women and children first\".\nSee the statistical evidence here on the new feature FamilySize:","f725f048":"## 2 - Train and Test three different models: Random Forest, SVM and k-Nearest Neighbors","b4fcaf4a":"This would be the replacements for Age if we would use mean per group of Title and gender.","4598b19f":"### Random Forest","b9e86e78":"## 1.4 Derive additional Feature FamilySize","01288b42":"## 1.1 - Load the data sets","76f21f3f":"Embarked has category values and also here only two values are missing in the train set. \nWe will replace them with the mode, which is 'S'.","b720c5a8":"Import data from CSV files _train.csv_ and _test.csv_ into separate pandas DataFrames. Combine both into one frame (for exploration).","f9917bf4":"Spoiler:  Cabin might have no correlation with survival, thus we simply replace unknown cabins with value 'unknown'.\nLater on we can still decide to drop 'Cabin' or to keep it.","d2dcccc0":"# 3 - Submission","64d41f8d":"#### Now we train the Random Forest Classifier with this dataset.","2db6654c":"There is only one value missing in 'Fare'. We can replace that with the mean.","b6093609":"The 'Title' in the name indicates a certain age group and a certain social level as well as gender, so we introduce this as a derived feature from 'Name'.\nWe will just classify the titles a bit before, as there is many titles indicating same age, gender and social group.\nDefined classes are:  Officer (=male, adult), Royalty, Mrs (=female, adult), Miss (=female, junior adult), Mr (=male, adult), Master (child, infant)","2a336751":"Another classification model approach we will try is the Support Vector Machine (SVM). This is making use of support vectors, defining the hyperplane with the smallest margin. Didn't understand a word? Then read this please: https:\/\/towardsdatascience.com\/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47 \n\nPlease be patient, running SVM can really take some time.","244340e8":"### Findings from Data exploration \n\n1. Most passengers in 1st and 2nd class survived, while most in 3rd class died\n2. Most female passengers (70%) survived, while most men died\n3. SibSp and Parch don't seem to have a clear relationship with the target, but putting them together can be a good idea (see later on).\n4. For Ticket and Cabin we don't see any relation, so let's forget about them.\n5. Most survivors embarked in Cherbourg ('C')\n6. Age matters! Passengers between 15 and 35 had the best chances to survive. However, there is a lot of Ages missing (250 across both sets). See next chapter how to handle this\n","5701f149":"Check some of the predictions","1f99b018":"## 1.3 - How to handle missing values in 'Age' ?\n\nFrom looking on the values and possible correlations, Age definitively will have a correlation to 'Survived', so we need\nto find a way to handle the missing values in 'Age'.\nInspired by: https:\/\/www.kaggle.com\/sedrak\/titanic-survivals-with-age-prediction we predict the missing 'Ages' with a RandomForestClassifier instead of simply impute them with mean or median.","7ffba2af":"Find all missing values in training set","26c5fcc4":"# Titanic: Who survived?\n\nThis notebook is one of my attempts to tackle the Titanic challenge. However, by far not being best-in-class, it demonstrates you how implementing a machine learning can be done. I tried to explain as much as necessary for the different steps, but I am sure you will come across some questions. If so, please do not hesitate to ask me.\n\n### Steps:\n__1 - Preprocessing and exploring__\n<br\/>    1.1 - Load the data sets\n<br\/>    1.2 - Explore the training data\n<br\/>    1.3 - How to handle missing values in Age?\n<br\/>    1.4 - Derive additional feature FamilySize\n<br\/>    1.5 - Conclude feature engineering\n<br\/><br\/>\n__2 - Train and Test three different models: Random Forest, SVM and k-Nearest Neighbors__\n\n__3 - Submission__\n    \n   ","2dc51c94":"### k-Nearest Neighbors or KNN Classifier","c221756f":"We see that especially families with 2-4 members have survived and will use FamilySize as an additional features for prediction.","2baa3b6b":"And predict the ages that is missing for the train dataset and the test dataset","a4483549":"The best out of our three models was the SVM. So we will use this to predict our results we want to submit based on the provided test set.\nThe results are matched with the passenger IDs and stored in a file we can submit to Kaggle.","f5d91c16":"### Cabin","fc92f609":"### Fare","a57829b5":"After exploring the data, we have already seen that the following columns can help us a features for a model:\n- Pclass\n- Sex (gender)\n- Embarked\n- Age\nFor Age we solved the issue of missing values by predicting them based on a model that was trained with the available data.\nFurthermore, we derived a new feature 'FamilySize' based on SibSp and Parch.\n\nHence, now we drop all other features that will not help us. PassengerID we will also drop for training a model, but will add it again later for submission.\n","73ecc284":"### SVM - Support Vector Machine","0c0de679":"Let's have a short look, how the data is structured","c6ec5ebc":"## 1- Preprocessing and exploring","e3732bb0":"Now we plot what we have so far and set it into relation to 'Survived', e.g. What percentage of passengers of which 'Pclass' survived?\nI will use bar chart, but pie chart or any other visualization that helps you to see a distribution is fine as well.","6ed0cf19":"Find all missing values in test set","eb9df6b1":"As said before, we try a more sophisticated approach by predicting each missing Age value individually with a RandomForestClassifier.\nTo train the classifier, we combine train and test dataset, we drop all columns not necessary and clean all NaN.","9bd0a5fb":"### Embarked","62791211":"## 1.2 - Explore the training data","0947893a":"When classifying, unlabeled Strings have to be converted to numerical in order to build classified, labeled data, so we map our newly created titles, as well as 'Embarked' and 'Sex' to categorical numeric data"}}