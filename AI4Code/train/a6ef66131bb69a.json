{"cell_type":{"8696b32c":"code","002e4ac2":"code","bf49246d":"code","6c5621b6":"code","3c584719":"code","3f3aa975":"code","a5cb6655":"code","90ad8bb8":"code","0f083a67":"code","a8595e62":"code","c9bfa90b":"code","bc191069":"code","5ea685cf":"code","620007f5":"code","0bbc8897":"code","c8f437c5":"code","0feeafe6":"code","e182242f":"code","537c66b7":"code","a24fb8e1":"code","0a3adc7c":"code","e538d1a6":"code","d7ee37c3":"code","4e18311a":"code","1458caf3":"code","fdd02035":"markdown","7e6d43f9":"markdown","5ec842cb":"markdown","88a6e2ff":"markdown","1ff10413":"markdown","a9f5a514":"markdown","97d84cce":"markdown","038796cd":"markdown","ef475411":"markdown","22caa77e":"markdown","a7c4716b":"markdown","811e72b2":"markdown","d4904937":"markdown","db340fe9":"markdown","130ae793":"markdown","ba467d15":"markdown","b80c4921":"markdown","d8ffe0d8":"markdown","bd184117":"markdown","55ee9ff5":"markdown","007ab16d":"markdown","ed4fe649":"markdown","5c34b877":"markdown"},"source":{"8696b32c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","002e4ac2":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelBinarizer\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndata = pd.read_csv('\/kaggle\/input\/star-type-classification\/Stars.csv')\ndata.head()","bf49246d":"data.shape","6c5621b6":"data.info()","3c584719":"data.isnull().sum()","3f3aa975":"data[data.duplicated()]","a5cb6655":"fig, axs = plt.subplots(2, 2,figsize=(15,15))\naxs[0, 0].boxplot(data['Temperature'])\naxs[0, 0].set_title('Temperature')\naxs[0, 1].boxplot(data['R'], 'tab:orange')\naxs[0, 1].set_title('Radius')\naxs[1, 0].boxplot(data['A_M'], 'tab:green')\naxs[1, 0].set_title('Absolute Magnitutde')\naxs[1, 1].boxplot(data['L'], 'tab:red')\naxs[1, 1].set_title('Luminosity')\n\nfor ax in axs.flat:\n    ax.set(xlabel='x-label', ylabel='y-label')\n\n# Hide x labels and tick labels for top plots and y ticks for right plots.\nfor ax in axs.flat:\n    ax.label_outer()","90ad8bb8":"a= pd.DataFrame(data['Color'].value_counts())\nplt.figure(figsize=(8,6))\nsns.barplot(a['Color'], a.index, palette= 'Spectral')\nplt.title(\"Star Color Analysis\")","0f083a67":"a= pd.DataFrame(data['Spectral_Class'].value_counts())\nplt.figure(figsize=(8,6))\nsns.barplot(a['Spectral_Class'], a.index, palette= 'rainbow')\nplt.title(\"Star Spectral Class Analysis\")","a8595e62":"a =pd.DataFrame(data['Type'].value_counts())\nplt.figure(figsize=(10,8))\nplt.pie(a['Type'],labels=a.index,autopct='%1.1f%%')\nplt.title(\"Percentage Distribution of Star Type\")","c9bfa90b":"matrix= data.corr()\nmask = np.zeros_like(matrix, dtype=np.bool)\nmask[np.triu_indices_from(mask)]= True\n\n\nplt.figure(figsize=(11,6))\nsns.heatmap(matrix,annot=True,cmap='viridis',annot_kws = {'size': 10},mask=mask)\nplt.title(\"Correlation Analysis\")\nplt.show()","bc191069":"from sklearn.preprocessing import LabelEncoder\nx1=LabelEncoder()  \ndata['Spectral_Class']= x1.fit_transform(data['Spectral_Class'])\ndata['Color']= x1.fit_transform(data['Color'])\n\nY= data[['Type']]\nX= data.drop(['Type'], axis=1)\n\nx_train, x_test, y_train, y_test= train_test_split(X, Y, test_size=0.2, random_state=0, stratify=Y)","5ea685cf":"LogReg= LogisticRegression()\nLogReg= LogReg.fit(x_train,y_train)\ny_pred= LogReg.predict(x_test)\nprint(\"Accuracy Score: \",metrics.accuracy_score(y_pred,y_test))\ncm= confusion_matrix(y_test,y_pred)\nprint(\"Confusion Matrix: \",cm,sep='\\n')","620007f5":"def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n    lb = LabelBinarizer()\n    lb.fit(y_test)\n    y_test = lb.transform(y_test)\n    y_pred = lb.transform(y_pred)\n    return roc_auc_score(y_test, y_pred, average=average)\n\n\nLogReg= multiclass_roc_auc_score(y_test,y_pred)\nLogReg","0bbc8897":"knn= KNeighborsClassifier(n_neighbors=5)\nknn.fit(x_train,y_train)\n\ny_pred= knn.predict(x_test)\nprint(\"Accuracy Score: \",metrics.accuracy_score(y_pred,y_test))\n\ncm= confusion_matrix(y_test,y_pred)\nprint(\"Confusion Matrix: \",cm,sep='\\n')","c8f437c5":"KNN=multiclass_roc_auc_score(y_test,y_pred)\nKNN","0feeafe6":"dtc= DecisionTreeClassifier(criterion=\"entropy\")\ndtc.fit(x_train,y_train)\n\ny_pred= dtc.predict(x_test)\nprint(\"Accuracy Score: \",metrics.accuracy_score(y_pred,y_test))\n\ncm= confusion_matrix(y_test,y_pred)\nprint(\"Confusion Matrix: \",cm,sep='\\n')","e182242f":"DT=multiclass_roc_auc_score(y_test,y_pred)\nDT","537c66b7":"rf= RandomForestClassifier(n_estimators=100,random_state=0)\nrf.fit(x_train,y_train)\n\ny_pred= rf.predict(x_test)\nprint(\"Accuracy Score RandomForest: \",metrics.accuracy_score(y_test,y_pred))\n\ncm= confusion_matrix(y_test,y_pred)\nprint(\"Confusion Matrix: \",cm,sep='\\n')","a24fb8e1":"RF= multiclass_roc_auc_score(y_test,y_pred)\nRF","0a3adc7c":"ada= AdaBoostClassifier(n_estimators=200,random_state=0)\nada.fit(x_train,y_train)\n\ny_pred= ada.predict(x_test)\nprint(\"Accuracy Score of AdaBoost Classifier: \",metrics.accuracy_score(y_test,y_pred))\n\ncm= confusion_matrix(y_test,y_pred)\nprint(\"Confusion Matrix: \",cm,sep='\\n')","e538d1a6":"AB= multiclass_roc_auc_score(y_test,y_pred)\nAB","d7ee37c3":"gradient= GradientBoostingClassifier(n_estimators=200,random_state=0,max_depth=2)\ngradient.fit(x_train,y_train)\n\ny_pred= gradient.predict(x_test)\nprint(\"Accuracy Score of GradientBoost Classifier: \",metrics.accuracy_score(y_test,y_pred))\n\ncm= confusion_matrix(y_test,y_pred)\nprint(\"Confusion Matrix: \",cm,sep='\\n')","4e18311a":"GB= multiclass_roc_auc_score(y_test,y_pred)\nGB","1458caf3":"mc= pd.DataFrame([LogReg,KNN,DT,RF,AB,GB],['Logistic Regression','KNN Classifier','Decision Tree','Random Forest','Ada-Boost','Gradient Boost'])\nmc.columns=['ROC_AUC']\nmc\n\nplt.figure(figsize=(11,6))\nsns.barplot(mc.index,mc.ROC_AUC,palette='rainbow')\nplt.title('ML Model Comparison')","fdd02035":"### Gradient Boost Classifiier","7e6d43f9":"### Random Forest","5ec842cb":"### Logistic Regression","88a6e2ff":"## Duplicates Analysis","1ff10413":"### It is seen that Random Forest, Decision Tree & Gradient Boost Classifiers give the best Classification Performance","a9f5a514":"#### No Missing Values in the Data","97d84cce":"## Correlation Analysis","038796cd":"### Ada Boost Classifier","ef475411":"#### 111 Stars belong to \"M\" Spectral Class, making it the most dominant class in the sample","22caa77e":"## Feature Analysis","a7c4716b":"## Missing Value Analysis","811e72b2":"## Importing the Required Libraries","d4904937":"#### 112 stars have Red color & 56 stars have Blue color","db340fe9":"## Star Classification Analysis","130ae793":"#### It is seen that the comprises of equal distribution of Star Types","ba467d15":"#### No Duplicate Values present","b80c4921":"- There is a Moderate Positive Correlation between Luminosity-Temperature and Star Type-Temperature\n- Moderately High Positive Correlation is seen between Lumionsity-Radius, Luminosity-Star Type & Radius- Star Type\n- Moderately High Negatively Correlation is seen between Lumionsity-Magnitude & Radius-Magnitude\n- Strong Negative Correlation is seen between Magnitude & Star Type","d8ffe0d8":"### KNN ","bd184117":"## Star Spectral Class Analysis","55ee9ff5":"### Model Comparison","007ab16d":"## Star Color Analysis","ed4fe649":"## Star Type Analysis","5c34b877":"### Decision Tree"}}