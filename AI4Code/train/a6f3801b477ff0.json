{"cell_type":{"51de016a":"code","6e328915":"code","41d2545a":"code","5d7673b2":"code","9a2381b7":"code","00d0403a":"code","a12936e3":"code","0fb46bc8":"code","ee8cd358":"code","ff4341a1":"code","32a84ecd":"code","7641e5c8":"code","41d0552d":"code","5b7a4971":"code","4afb8954":"code","39b1ff80":"code","6dd7d04b":"markdown","c48acc7c":"markdown","35c9f35c":"markdown","1c2e9739":"markdown","10181926":"markdown","35f8bfc9":"markdown","ff2cb975":"markdown","d21657d7":"markdown","54fda975":"markdown","aa9376bf":"markdown","3269e208":"markdown","03ce22cb":"markdown","131afcc6":"markdown","f18f81fe":"markdown","fa4fa082":"markdown"},"source":{"51de016a":"%matplotlib inline\n\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\n","6e328915":"# Number of GPUs available. Use 0 for CPU mode.\nngpu = 1\n# Decide which device we want to run on\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\nprint(device)\n\n# number of subprocesses to use for data loading\nnum_workers = 2\n# how many samples per batch to load\nbatch_size = 64\n\n# convert data to torch.FloatTensor\ntransform = transforms.ToTensor()\n\n# get the training MNIST datasets\ntrain_data = datasets.MNIST(root='data', train=True,\n                                   download=True, transform=transform)\n\n# prepare data loader\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n                                           num_workers=num_workers)\n","41d2545a":"# obtain one batch of training images\ndataiter = iter(train_loader)\nimages, labels = dataiter.next()\nimages = images.numpy()\n\n\n\n# Tr\u1ef1c quan ho\u00e1 d\u1eef li\u1ec7u\ndef draw_samples(imgs,labels):\n  plt.subplots(4, 4, figsize=(8,8))\n  # s\u1ed1 l\u01b0\u1ee3ng sample tr\u00ean h\u00e0ng v\u00e0 c\u1ed9t\n  columns = 4\n  rows = 4\n  for i in range(1, columns*rows+1):\n      img = np.squeeze(imgs[i-1])\n      plt.subplot(rows, columns, i)\n      plt.title((f' {labels[i-1]}'))\n\n      plt.imshow(img,  cmap='gray')\n  plt.show()  \n\n\ndraw_samples(images,labels)\n\n\n","5d7673b2":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Discriminator(nn.Module):\n\n    def __init__(self, input_size, hidden_dim, output_size, ngpu):\n        super(Discriminator, self).__init__()\n        self.ngpu = ngpu\n        # define hidden linear layers\n        self.fc1 = nn.Linear(input_size, hidden_dim*4)\n        self.fc2 = nn.Linear(hidden_dim*4, hidden_dim*2)\n        self.fc3 = nn.Linear(hidden_dim*2, hidden_dim)\n        \n        # final fully-connected layer\n        self.fc4 = nn.Linear(hidden_dim, output_size)\n        \n        # dropout layer \n        self.dropout = nn.Dropout(0.3)\n        \n        \n    def forward(self, x):\n        # flatten image\n        x = x.view(-1, 28*28)\n        # all hidden layers\n        x = F.leaky_relu(self.fc1(x), 0.2) # (input, negative_slope=0.2)\n        x = self.dropout(x)\n        x = F.leaky_relu(self.fc2(x), 0.2)\n        x = self.dropout(x)\n        x = F.leaky_relu(self.fc3(x), 0.2)\n        x = self.dropout(x)\n        # final layer\n        out = self.fc4(x)\n\n        return out\n","9a2381b7":"class Generator(nn.Module):\n\n    def __init__(self, input_size, hidden_dim, output_size, ngpu):\n        super(Generator, self).__init__()\n        self.ngpu = ngpu\n        # define hidden linear layers\n        self.fc1 = nn.Linear(input_size, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim*2)\n        self.fc3 = nn.Linear(hidden_dim*2, hidden_dim*4)\n        \n        # final fully-connected layer\n        self.fc4 = nn.Linear(hidden_dim*4, output_size)\n        \n        # dropout layer \n        self.dropout = nn.Dropout(0.3)\n\n    def forward(self, x):\n        # all hidden layers\n        x = F.leaky_relu(self.fc1(x), 0.2) # (input, negative_slope=0.2)\n        x = self.dropout(x)\n        x = F.leaky_relu(self.fc2(x), 0.2)\n        x = self.dropout(x)\n        x = F.leaky_relu(self.fc3(x), 0.2)\n        x = self.dropout(x)\n        # final layer with tanh applied\n        out = F.tanh(self.fc4(x))\n\n        return out","00d0403a":"# Discriminator hyperparams\n\n# Size of input image to discriminator (28*28)\ninput_size = 784\n# Size of discriminator output (real or fake)\nd_output_size = 1\n# Size of last hidden layer in the discriminator\nd_hidden_size = 32\n\n\n# Generator hyperparams\n\n# Size of latent vector to give to generator\nz_size = 100\n# Size of discriminator output (generated image)\ng_output_size = 784\n# Size of first hidden layer in the generator\ng_hidden_size = 32","a12936e3":"# instantiate discriminator and generator\nD = Discriminator(input_size, d_hidden_size, d_output_size,ngpu).to(device)\nG = Generator(z_size, g_hidden_size, g_output_size, ngpu).to(device)\nif (device.type == 'cuda') and (ngpu > 1):\n    D = nn.DataParallel(netG, list(range(ngpu)))\nif (device.type == 'cuda') and (ngpu > 1):\n    G = nn.DataParallel(netG, list(range(ngpu)))\n# check that they are as you expect\nprint(D)\nprint()\nprint(G)","0fb46bc8":"# Calculate losses\ndef real_loss(D_out, smooth=False):\n    batch_size = D_out.size(0)\n    # label smoothing\n    if smooth:\n        # smooth, real labels = 0.9\n        labels = torch.ones(batch_size,device=device)*0.9\n    else:\n        labels = torch.ones(batch_size,device=device) # real labels = 1\n        \n    # numerically stable loss\n    criterion = nn.BCEWithLogitsLoss()\n    # calculate loss\n    loss = criterion(D_out.squeeze(), labels)\n    return loss\n\ndef fake_loss(D_out):\n    batch_size = D_out.size(0)\n    labels = torch.zeros(batch_size,device=device) # fake labels = 0\n    criterion = nn.BCEWithLogitsLoss()\n    # calculate loss\n    loss = criterion(D_out.squeeze(), labels)\n    return loss","ee8cd358":"import torch.optim as optim\n\n# Optimizers\nlr = 0.002\n\n# Create optimizers for the discriminator and generator\nd_optimizer = optim.Adam(D.parameters(), lr)\ng_optimizer = optim.Adam(G.parameters(), lr)","ff4341a1":"import pickle as pkl\n# training hyperparams\nnum_epochs = 100\n\n# keep track of loss and generated, \"fake\" samples\nsamples = []\nlosses = []\n\nprint_every = 800\n\n# Get some fixed data for sampling. These are images that are held\n# constant throughout training, and allow us to inspect the model's performance\nsample_size=16\nfixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size))\nfixed_z = torch.from_numpy(fixed_z).float()\n\n# train the network\nD.train()\nG.train()\nfor epoch in range(num_epochs):\n\n  for batch_i, (real_images, _) in enumerate(train_loader):\n\n      batch_size = real_images.size(0)\n\n      ## Important rescaling step ## \n      real_images = real_images*2 - 1  # rescale input images from [0,1) to [-1, 1)\n\n      # ============================================\n      #            TRAIN THE DISCRIMINATOR\n      # ============================================\n\n      d_optimizer.zero_grad()\n      real_images = real_images.to(device)\n      # 1. Train with real images\n\n      # Compute the discriminator losses on real images \n      # smooth the real labels\n      D_real = D(real_images).to(device)\n      d_real_loss = real_loss(D_real, smooth=True).to(device)\n\n      # 2. Train with fake images\n\n      # Generate fake images\n      # gradients don't have to flow during this step\n      with torch.no_grad():\n          z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n          z = torch.from_numpy(z).float().to(device)\n          fake_images = G(z)\n\n      # Compute the discriminator losses on fake images\n             \n      D_fake = D(fake_images)\n      D_fake = D_fake.to(device)\n      d_fake_loss = fake_loss(D_fake).to(device)\n\n      # add up loss and perform backprop\n      d_loss = d_real_loss + d_fake_loss\n      d_loss.backward()\n      d_optimizer.step()\n\n\n      # =========================================\n      #            TRAIN THE GENERATOR\n      # =========================================\n      g_optimizer.zero_grad()\n\n      # 1. Train with fake images and flipped labels\n\n      # Generate fake images\n      z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n      z = torch.from_numpy(z).float().to(device)\n      fake_images = G(z).to(device)\n\n      # Compute the discriminator losses on fake images \n      # using flipped labels!\n      D_fake = D(fake_images).to(device)\n      g_loss = real_loss(D_fake).to(device) # use real loss to flip labels\n\n      # perform backprop\n      g_loss.backward()\n      g_optimizer.step()\n\n      # Print some loss stats\n      if batch_i % print_every == 0:\n        if epoch % 4 ==0:\n          # print discriminator and generator loss\n          print('Epoch [{:5d}\/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(\n                  epoch+1, num_epochs, d_loss.item(), g_loss.item()))\n\n\n  ## AFTER EACH EPOCH##\n  # append discriminator loss and generator loss\n  losses.append((d_loss.item(), g_loss.item()))\n\n  # generate and save sample, fake images\n  G.eval() # eval mode for generating samples\n  samples_z = G(fixed_z.to(device))\n  samples.append(samples_z)\n  G.train() # back to train mode\n\n\n# Save training generator samples\nwith open('train_samples.pkl', 'wb') as f:\n  pkl.dump(samples, f)","32a84ecd":"fig, ax = plt.subplots()\nlosses = np.array(losses)\nplt.plot(losses.T[0], label='Discriminator')\nplt.plot(losses.T[1], label='Generator')\nplt.title(\"Training Losses\")\nplt.legend()","7641e5c8":"# helper function for viewing a list of passed in sample images\ndef view_samples(epoch, samples):\n    fig, axes = plt.subplots(figsize=(7,7), nrows=4, ncols=4, sharey=True, sharex=True)\n    for ax, img in zip(axes.flatten(), samples[epoch]):\n        img = img.detach().cpu()\n        ax.xaxis.set_visible(False)\n        ax.yaxis.set_visible(False)\n        im = ax.imshow(img.reshape((28,28)), cmap='Greys_r')","41d0552d":"# Load samples from generator, taken while training\nwith open('train_samples.pkl', 'rb') as f:\n    samples = pkl.load(f)","5b7a4971":"# -1 indicates final epoch's samples (the last in the list)\n# value, index = torch.max(samples,1)\n\nview_samples(-1, samples)","4afb8954":"rows = 10 # split epochs into 10, so 100\/10 = every 10 epochs\ncols = 6\nfig, axes = plt.subplots(figsize=(7,12), nrows=rows, ncols=cols, sharex=True, sharey=True)\n\nfor sample, ax_row in zip(samples[::int(len(samples)\/rows)], axes):\n    for img, ax in zip(sample[::int(len(sample)\/cols)], ax_row):\n        img = img.detach().cpu()\n        ax.imshow(img.reshape((28,28)), cmap='Greys_r')\n        ax.xaxis.set_visible(False)\n        ax.yaxis.set_visible(False)","39b1ff80":"# randomly generated, new latent vectors\nsample_size=16\nrand_z = np.random.uniform(-1, 1, size=(sample_size, z_size))\nrand_z = torch.from_numpy(rand_z).float()\n\nG.eval() # eval mode\n# generated samples\nrand_images = G(rand_z.to(device))\n\n# 0 indicates the first set of samples in the passed in list\n# and we only have one batch of samples, here\nview_samples(0, [rand_images])","6dd7d04b":"## Training loss\nBi\u1ec3u \u0111\u1ed3 training losses cho G v\u00e0 D, l\u01b0u l\u1ea1i sau m\u1ed7i epoch train.","c48acc7c":"# Generative Adversarial Network\n\nTrong b\u1ea3n notebook n\u00e0y em s\u1ebd x\u00e2y d\u1ef1ng l\u1ea1i m\u1ed9t generative adversarial network (GAN) tr\u00ean b\u1ed9 d\u1eef li\u1ec7u MNIST v\u00e0 d\u1eef li\u1ec7u khu\u00f4n m\u1eb7t Toronto (TFD) v\u00e0 CIFAR-10.\nT\u1eeb \u0111\u00f3 ch\u00fang ta c\u00f3 th\u1ec3 sinh d\u1eef li\u1ec7u m\u1edbi . Notebook n\u00e0y \u0111\u01b0\u1ee3c tham kh\u1ea3o b\u00e0i MNIST_GAN\n\n\n\u00dd t\u01b0\u1edfng \u0111\u1eb1ng sau GAN l\u00e0 2 m\u1ea1ng l\u01b0\u1edbi \u0111\u1ed1i ngh\u1ecbch, 1 m\u1ea1ng sinh d\u1eef li\u1ec7u gi\u1ea3 generator $G$ v\u00e0 1 m\u1ea1ng ph\u00e2n l\u1edbp discriminator $D$. The generator c\u1ea7n ph\u1ea3i l\u00e0m gi\u1ea3 d\u1eef li\u1ec7u \u0111\u1ec3 qua m\u1eb7t \u0111\u01b0\u1ee3c b\u1ed9 ph\u00e2n l\u1edbp D. The discriminator xem \u0111\u01b0\u1ee3c d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n th\u1ef1c v\u00e0 ph\u1ea7n bi\u1ec7t n\u00f3 l\u00e0 th\u1eadt hay gi\u1ea3.\n> * B\u1ed9 sinh (G) \u0111\u01b0\u1ee3c hu\u1ea5n luy\u1ec7n \u0111\u1ec3 \u0111\u00e1nh l\u1eeba b\u1ed9 ph\u00e2n bi\u1ec7t, n\u00f3 mu\u1ed1n sinh ra d\u1eef li\u1ec7u gi\u1ed1ng v\u1edbi d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n th\u1eadt nh\u1ea5t c\u00f3 th\u1ec3. \n> * B\u1ed9 ph\u00e2n bi\u1ec7t (D) l\u00e0 m\u1ed9t b\u1ed9 ph\u00e2n lo\u1ea1i \u0111\u01b0\u1ee3c \u0111\u00e0o t\u1ea1o \u0111\u1ec3 t\u00ecm ra d\u1eef li\u1ec7u n\u00e0o l\u00e0 th\u1eadt v\u00e0 d\u1eef li\u1ec7u n\u00e0o l\u00e0 gi\u1ea3 m\u1ea1o. \n\nB\u1ed9 sinh (G) h\u1ecdc c\u00e1ch t\u1ea1o d\u1eef li\u1ec7u kh\u00f4ng th\u1ec3 ph\u00e2n bi\u1ec7t \u0111\u01b0\u1ee3c v\u1edbi d\u1eef li\u1ec7u th\u1ef1c cho b\u1ed9 ph\u00e2n bi\u1ec7t (D).\n\n<img src='https:\/\/github.com\/udacity\/deep-learning-v2-pytorch\/raw\/661c38e1c6a1f6734c26ecd899b958533c41cf1f\/gan-mnist\/assets\/gan_pipeline.png' width=70% \/>\n","35c9f35c":"### Visualize d\u1eef li\u1ec7u","1c2e9739":"---\n## Discriminator and Generator Losses ( X\u00e2y d\u1ef1ng h\u00e0m m\u1ea5t m\u00e1t cho D v\u00e0 G )\n\nT\u00ednh to\u00e1n h\u00e0m m\u1ea5t m\u00e1t (losses)\n\n### Discriminator Losses\n\n> * \u0110\u1ed1i v\u1edbi b\u1ed9 ph\u00e2n bi\u1ec7t, loss b\u1eb1ng t\u1ed5ng loss tr\u00ean d\u1eef li\u1ec7u th\u1eadt $x$ v\u00e0 d\u1eef li\u1ec7u sinh ra t\u1eeb $G(z)$, `d_loss = d_real_loss + d_fake_loss `\n\n\nS\u1eed d\u1ee5ng h\u00e0m k\u00edch ho\u1ea1t sigmoid v\u00e0 h\u00e0m m\u1ea5t m\u00e1t cross entropy loss \u0111\u1ec3 ph\u00e2n l\u1edbp MLP.\n\n\u0110\u1ed1i v\u1edbi h\u00ecnh \u1ea3nh th\u1ef1c, ch\u00fang ta mu\u1ed1n D `D(real_images) = 1`. \u0110\u1ec3 gi\u00fap b\u1ed9 ph\u00e2n bi\u1ec7t t\u1ed5ng qu\u00e1t t\u1ed1t h\u01a1n, c\u00e1c nh\u00e3n \u0111\u01b0\u1ee3c gi\u1ea3m m\u1ed9t ch\u00fat t\u1eeb 1,0 xu\u1ed1ng 0,9. S\u1eed d\u1ee5ng tham s\u1ed1 `smooth`; n\u1ebfu True, th\u00ec ch\u00fang ta n\u00ean l\u00e0m m\u1ecbn nh\u00e3n. Trong PyTorch, n\u00f3 gi\u1ed1ng nh\u01b0 `labels = torch.ones(size) * 0.9`\n\nLoss c\u1ee7a d\u1eef li\u1ec7u gi\u1ea3 c\u0169ng t\u01b0\u01a1ng t\u1ef1, ch\u00fang ta mu\u1ed1n `D(fake_images) = 0` trong \u0111\u00f3 `fake_images = G(z)`. \n\n### Generator Loss\n\nM\u1ee5c ti\u00eau c\u1ee7a b\u1ed9 sinh l\u00e0 `D(fake_images) = 1`. ","10181926":"---\n# X\u00e2y d\u1ef1ng m\u00f4 h\u00ecnh\n\nGAN g\u1ed3m 2 m\u1ea1ng \u0111\u1ed1i ngh\u1ecbch , b\u1ed9 ph\u00e2n bi\u1ec7t(D) v\u00e0 b\u1ed9 sinh(G)\n","35f8bfc9":"## Discriminator\nX\u00e2y d\u1ef1ng b\u1ed9 ph\u00e2n bi\u1ec7t (D) l\u00e0 m\u1ed9t m\u1ea1ng MLP \u0111i\u1ec3n h\u00ecnh. M\u1ea1ng \u0111\u1ea7u ra v\u1edbi 2 l\u1edbp l\u00e0 0 (fake data) v\u00e0 1 (realdata).\nM\u1ea1ng s\u1eed d\u1ee5ng h\u00e0m k\u00edch ho\u1ea1t activation leaky_relu. S\u1eed d\u1ee5ng dropout \u0111\u1ec3 ch\u1ed1ng over-fitting. H\u00e0m sigmoid \u0111\u1ec3 ph\u00e2n bi\u1ec7t v\u1ec1 c\u00e1c l\u1edbp \u0111\u1ea7u ra l\u00e0 0,1.\n\n","ff2cb975":"# Input d\u1eef li\u1ec7u","d21657d7":"## Generator\nM\u1ea1ng sinh(G) s\u1ebd gi\u1ed1ng v\u1edbi m\u1ea1ng ph\u00e2n bi\u1ec7t nh\u01b0ng s\u1ebd \u0111i ng\u01b0\u1ee3c l\u1ea1i chi\u1ec1u c\u1ee7a m\u1ea1ng ph\u00e2n bi\u1ec7t. \u1ede m\u1ea1ng ph\u00e2n bi\u1ec7t(D) \u0111\u1ea7u ra l\u00e0 2 l\u1edbp nh\u01b0ng v\u1edbi m\u1ea1ng sinh (G) \u0111\u1ea7u v\u00e0o c\u1ee7a n\u00f3 l\u1ea1i l\u00e0 c\u00e1c noize (z) v\u1edbi d chi\u1ec1u (tu\u1ef3 ch\u1ec9nh).\n\n#### tanh Output\nB\u1ed9 sinh \u0111\u00e3 \u0111\u01b0\u1ee3c ph\u00e1t hi\u1ec7n l\u00e0 ho\u1ea1t \u0111\u1ed9ng t\u1ed1t nh\u1ea5t h\u00e0m k\u00edch ho\u1ea1t $ tanh $ cho \u0111\u1ea7u ra c\u1ee7a b\u1ed9 sinh (-1,1)\n\n","54fda975":"---\n## Training ( Hu\u1ea5n luy\u00ean)\n\nHu\u1ea5n luy\u1ec7n s\u1ebd xen k\u1ebd gi\u1eefa hu\u1ea5n luy\u1ec7n b\u1ed9 ph\u00e2n bi\u1ec7t v\u00e0 b\u1ed9 sinh.\n\n### Discriminator training ( Hu\u1ea5n luy\u1ec7n b\u1ed9 ph\u00e2n bi\u1ec7t )\n1. T\u00ednh to\u00e1n discriminator loss tr\u00ean d\u1eef li\u1ec7u th\u1eadt th\u1eadt, d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n.     \n2. Sinh d\u1eef li\u1ec7u gi\u1ea3\n3. T\u00ednh to\u00e1n discriminator loss tr\u00ean d\u1eef li\u1ec7u gi\u1ea3 \u0111\u01b0\u1ee3c G sinh ra \n4. C\u1eadp nh\u1eadt realloss v\u00e0 fakeloss\n5. Lan truy\u1ec1n ng\u01b0\u1ee3c sai s\u1ed1 \u0111\u1ec3 c\u1eadp nh\u1eadt tr\u1ecdng s\u1ed1 cho b\u1ed9 ph\u00e2n bi\u1ec7t D\n\n### Generator training ( Hu\u1ea5n luy\u1ec7n b\u1ed9 sinh )\n1. Sinh d\u1eef li\u1ec7u gi\u1ea3\n2. T\u00ednh to\u00e1n discriminator loss tr\u00ean d\u1eef li\u1ec7u gi\u1ea3, d\u00f9ng nh\u00e3n **flipped** !\n3. Lan truy\u1ec1n ng\u01b0\u1ee3c sai s\u1ed1 \u0111\u1ec3 c\u1eadp nh\u1eadt tr\u1ecdng s\u1ed1 cho b\u1ed9 sinh G\n\n#### Saving Samples (L\u01b0u samples)\n\nTrong qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n, in ra loss statistics v\u00e0 l\u01b0u m\u1ed9t s\u1ed1 d\u1eef li\u1ec7u gi\u1ea3 \u0111\u01b0\u1ee3c sinh ra b\u1edfi G.","aa9376bf":"## Optimizers \n\nS\u1eed d\u1ee5ng h\u00e0m t\u1ed1i \u01b0u ho\u00e1 Adam cho c\u1ea3 2 m\u1ea1ng","3269e208":"## Si\u00eau tham s\u1ed1 c\u1ee7a m\u00f4 h\u00ecnh","03ce22cb":"## Sampling from the generator\n\n","131afcc6":"## Generator samples from training (T\u1ea1o samples t\u1eeb qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n)\n\n\nXem 1 s\u1ed1 h\u00ecnh \u1ea3nh t\u1eeb b\u1ed9 sinh generator trong qu\u00e1 tr\u00ecnh \u0111\u00e0o t\u1ea1o","f18f81fe":"## X\u00e2y d\u1ef1ng m\u1ea1ng ho\u00e0n ch\u1ec9nh\n","fa4fa082":"# Import th\u01b0 vi\u1ec7n"}}