{"cell_type":{"f6fda812":"code","f1739e2d":"code","827ecd2d":"code","aceef37b":"code","cffe33df":"code","be39ab7f":"code","f2ddb47a":"code","9dc0ca24":"code","547203ae":"code","79021304":"code","6974e4df":"code","1e5b4c5a":"code","09015fcb":"code","de81a58f":"code","391f39a4":"code","d94d41ec":"code","35f670c5":"code","39919725":"code","1e17f904":"code","b39a9bd5":"code","f181bb32":"code","1cb76144":"code","ab949714":"code","6b6a55ed":"code","fa1ece93":"code","4f17001b":"code","72cca3e7":"code","e1df2844":"code","4be91e90":"code","0e30f334":"code","992c2921":"code","652ed346":"code","23b74af0":"code","3b7150fe":"code","d077e4e1":"code","9ba8214f":"code","e6951626":"code","c0a67738":"code","f175583b":"code","8f3da13d":"code","38bfb6af":"code","7cf45ed6":"code","e1daaee2":"code","2bc8b3b0":"code","2d0b85a6":"code","5b03743c":"code","65c558e1":"code","9c841fe3":"code","23443f4f":"code","9081a1f3":"code","50f0e9f3":"code","21900b16":"code","45a1c784":"code","af646f38":"code","2f9c023d":"code","a08054c5":"markdown","23d29b04":"markdown","f583b4ec":"markdown","cfbbdb49":"markdown","9e64ae24":"markdown","0ddd4f45":"markdown","2039cd41":"markdown","7c5d0d28":"markdown","314973c0":"markdown","162d3e0f":"markdown","9195de57":"markdown","9f512663":"markdown","d91846e8":"markdown","a7c372f3":"markdown","c1e92357":"markdown","49e06860":"markdown","5068a09f":"markdown","62186bcd":"markdown","8ce911c3":"markdown","253845e0":"markdown","61e25238":"markdown","5d16396f":"markdown","dea021d9":"markdown","a4e947b9":"markdown","584b57d1":"markdown","e0d50d51":"markdown","dc63f901":"markdown","95ca2a87":"markdown","d8d97531":"markdown","bf2e465b":"markdown","308928e9":"markdown","25fe6adc":"markdown","3410a32c":"markdown"},"source":{"f6fda812":"# Import Libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport datetime as dt\nimport matplotlib.pyplot as plt\nimport calendar\n\nfrom datetime import datetime\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n## Setting max displayed rows to 500, in order to display the full output of any command \npd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 1000)\npd.set_option('display.width', 1000)\n","f1739e2d":"pip install openpyxl","827ecd2d":"#Importing The transactions dataset\nTransactions = pd.read_excel(\"\/kaggle\/input\/data-files\/Transactions.xlsx\")","aceef37b":"# checking the data first rows\nTransactions.head(1)","cffe33df":"#creating the function\ndef titlehead(x):\n    x.rename(columns=x.iloc[0], inplace = True)\n    x.drop([0], inplace = True)","be39ab7f":"# applying the function on the data sets\ntitlehead(Transactions)","f2ddb47a":"# checking for results\nTransactions.head(1)","9dc0ca24":"#Dropping (nan) header columns from dataset as it doesnt exist in the riginal dataset\nTransactions.columns = Transactions.columns.fillna('to_drop')\nTransactions.drop('to_drop', axis = 1, inplace = True)","547203ae":"# Checking dataset info\nTransactions.info()","79021304":"# How many missing points in each variable\ncount_missing_Transactions = Transactions.isnull().sum()\npercent_missing_Transactions = round(Transactions.isnull().sum()\/len(Transactions) * 100, 1)\nmissing_Transactions = pd.concat([count_missing_Transactions, percent_missing_Transactions], axis = 1)\nmissing_Transactions.columns = [\"Missing (count)\", \"Missing (%)\"]\nmissing_Transactions","6974e4df":"# filling the online order column with the mode value \nTransactions['online_order'] = Transactions['online_order'].fillna(Transactions['online_order'].mode()[0])","1e5b4c5a":"# Removing other data rows with nul values\nTransactions.dropna(axis=0,inplace=True)","09015fcb":"# checking results after removing null values\n\nTransactions.isnull().sum()","de81a58f":"# ammending column data types\nTransactions['list_price']=pd.to_numeric(Transactions['list_price'])\nTransactions['standard_cost']=pd.to_numeric(Transactions['standard_cost'])\nTransactions['transaction_date']=pd.to_datetime(Transactions['transaction_date'])","391f39a4":"# Checking for the changes info\nTransactions.info()","d94d41ec":"# checking for duplicated rows\n\nTransactions[Transactions.duplicated()]","35f670c5":"# Collecting the categorical columns into  list\ncat_col=[]\nfor x in Transactions.dtypes.index:\n    if Transactions.dtypes[x]=='object':\n        cat_col.append(x)\ncat_col","39919725":"#checking for duplicated values in the categorical columns nd the accuracy of the values\nfor col in cat_col:\n    print(col)\n    print(Transactions[col].unique())\n    print()\n    print('*******')\n    print()","1e17f904":"#converting dates to datetime in pandas\ndef convert_to_datetime(num):\n    dt = datetime.fromordinal(datetime(1900, 1, 1).toordinal() + num - 2)\n    return dt","b39a9bd5":"#applying the function on the date transactions\nTransactions['product_first_sold_date']=Transactions['product_first_sold_date'].apply(convert_to_datetime)\nTransactions.head()","f181bb32":"# Changing transactions date column into transaction year,month,day,day_name columns\nTransactions['Transaction_year']=Transactions['transaction_date'].dt.year\nTransactions['Transaction_month']=Transactions['transaction_date'].dt.month_name()\nTransactions['Transaction_day']=Transactions['transaction_date'].dt.day\nTransactions['day_of_the_week']=Transactions['transaction_date'].dt.day_name()\n\n# converting the transaction year , day columnes into string columns\nTransactions['Transaction_year']=Transactions['Transaction_year'].astype(str)\nTransactions['Transaction_day']=Transactions['Transaction_day'].astype(str)","1cb76144":"#Importing customer demographics dataset\nCustomer_demographic=pd.read_excel('..\/input\/data-files\/Customer demographic.xlsx')","ab949714":"#checking dataset head\nCustomer_demographic.head(2)","6b6a55ed":"titlehead(Customer_demographic)","fa1ece93":"#Dropping (nan) header columns from dataset as it doesnt exist in the original dataset\nCustomer_demographic.columns = Customer_demographic.columns.fillna('to_drop')\nCustomer_demographic.drop('to_drop', axis = 1, inplace = True)","4f17001b":"#checking results\nCustomer_demographic.head(2)","72cca3e7":"Customer_demographic.info()","e1df2844":"# How many missing points in each variable\ncount_missing_Customer_demographic = Customer_demographic.isnull().sum()\npercent_missing_Customer_demographic = round(Customer_demographic.isnull().sum()\/len(Customer_demographic) * 100, 1)\nmissing_train = pd.concat([count_missing_Customer_demographic, percent_missing_Customer_demographic], axis = 1)\nmissing_train.columns = [\"Missing (count)\", \"Missing (%)\"]\nmissing_train","4be91e90":"#Filling in columns with mode \nCustomer_demographic['last_name'] = Customer_demographic['last_name'].fillna(Customer_demographic['last_name'].mode()[0])\nCustomer_demographic['DOB'] = Customer_demographic['DOB'].fillna(Customer_demographic['DOB'].mode()[0])\nCustomer_demographic['job_title'] = Customer_demographic['job_title'].fillna(Customer_demographic['job_title'].mode()[0])\nCustomer_demographic['job_industry_category'] = Customer_demographic['job_industry_category'].fillna(Customer_demographic['job_industry_category'].mode()[0])\nCustomer_demographic['default'] = Customer_demographic['default'].fillna(Customer_demographic['default'].mode()[0])\nCustomer_demographic['tenure'] = Customer_demographic['tenure'].fillna(Customer_demographic['tenure'].mean())","0e30f334":"# Changing column values types\nCustomer_demographic['past_3_years_bike_related_purchases']=pd.to_numeric(Customer_demographic['past_3_years_bike_related_purchases'])\nCustomer_demographic['tenure']=pd.to_numeric(Customer_demographic['tenure'])\nCustomer_demographic['DOB']=pd.to_datetime(Customer_demographic['DOB'])","992c2921":"# checking for duplicated rows \nCustomer_demographic[Customer_demographic.duplicated()]","652ed346":"# Collecting the categorical columns into  list\n\ncat_col=[]\nfor x in Customer_demographic.dtypes.index:\n    if Customer_demographic.dtypes[x]=='object':\n        cat_col.append(x)\ncat_col","23b74af0":"#checking for consistency of the values in each categorical column in the data\n\nfor col in cat_col:\n    print(col)\n    print(Customer_demographic[col].unique())\n    print()\n    print('*******')\n    print()","3b7150fe":"# Ammending values in gender column\nCustomer_demographic.replace({'gender':{'F':'Female','Femal':'Female','M':'Male'}},inplace=True)","d077e4e1":"# removing U value from gender column\nCustomer_demographic=Customer_demographic[Customer_demographic.gender!='U']","9ba8214f":"# drop default columns\nCustomer_demographic.drop('default',axis=1,inplace=True)","e6951626":"# This function converts given date to age\ndef from_dob_to_age(born):\n    today = dt.date.today()\n    return today.year - born.year - ((today.month, today.day) < (born.month, born.day))","c0a67738":"#applying the function on the DOB column\nCustomer_demographic['Age']=Customer_demographic['DOB'].apply(lambda x: from_dob_to_age(x))","f175583b":"# dropping DOB column\nCustomer_demographic.drop('DOB',axis=1,inplace=True)","8f3da13d":"Customer_demographic.info()","38bfb6af":"#checking results\nCustomer_demographic.head(2)","7cf45ed6":"# Importing dataset\nCustomer_address=pd.read_excel('..\/input\/data-files\/Customer address.xlsx')","e1daaee2":"#checking first rows \nCustomer_address.head(2)","2bc8b3b0":"#using the function to remove unnamed row and adjust first row to header\ntitlehead(Customer_address)","2d0b85a6":"#Dropping (nan) header columns from dataset as it doesnt exist in the original dataset\nCustomer_address.columns = Customer_address.columns.fillna('to_drop')\nCustomer_address.drop('to_drop', axis = 1, inplace = True)","5b03743c":"#checking for results\nCustomer_address.head(2)","65c558e1":"#checking dataset info\nCustomer_address.info()","9c841fe3":"#checking for duplicated rows\nCustomer_address[Customer_address.duplicated()]","23443f4f":"# appending the categorical columns into a list\ncat_col=[]\nfor x in Customer_address.dtypes.index:\n    if Customer_address.dtypes[x]=='object':\n        cat_col.append(x)\ncat_col","9081a1f3":"#checking for values inconsistencies in the categorical columns\n\nfor col in cat_col:\n    print(col)\n    print(Customer_address[col].unique())\n    print()\n    print('*******')\n    print()","50f0e9f3":"# joining Transactions with customer demographics datasets\nTransactions_demographics=Transactions.merge(Customer_demographic,on='customer_id',how='inner')\nTransactions_demographics.head(2)","21900b16":"# merging customersdemographics & transactions with addresses dataset into a CTA datasets (Customers,Transcations,Addresses)\nCTA=Transactions_demographics.merge(Customer_address,on='customer_id',how='inner')","45a1c784":"#checking the final general information of the three datasets together\nCTA.info()","af646f38":"#checking final data first rows\nCTA.head(2)","2f9c023d":"#Exporting data to csv file\nCTA.to_csv('CTA_wrangling.csv')","a08054c5":"listing all the unique values inside each categorical column of the recently created list","23d29b04":"**How to deal with Missing Data?**\n\nThere are many strategies to fill missing data:\n\n1. Fill with the mean (better used in case of continous variables without outliers)\n2. Fill with the median (better used in case of continous variables with outliers)\n3. Fill with the mode (better used in case of categorical variables)\n4. Drop the entire variable if the number of missing points is too large\n5. Drop the rows containing null values if it's not going to affect our analysis\n\n**Based on the above strategies:**\n\n- **online order:** has just 1.8% missing values, se we will fill them with the mode (the most frequent data point)\n- **brand, product line,product class,product size,standard cost and product_first_sold_date** have 1% missing values , they seem to have the same missing data pattern, we will remove the rows contatining their null values since it is just 1% and will not affect our analysis","f583b4ec":"**Dealing with Missing Data**\n\nall of the columns are categorical columns so either i will fill in the null values with the mode of these columns or i will drop the column entirley if the missing percentage is high\n\n1. **last_name:** has just 3.1% missing values, so we fill nulls with mode\n2. **DOB:** has 2.2% so we fill nulls with mode\n3. **job_title:** has 12.6% we will fill these values with mode\n4. **job_industry_category:** has 16.4% so we fill nulls with mode\n5. **default:** has 7.6% so we fill nulls with mode\n6. **Tenure:** has 2.2% missing from the column so we will fill it with mean","cfbbdb49":"We need to drop the first unnamed row that is created in the dataframe and adjust the first row as header,we will create a function that will remove the header and exchange it with the first row of the data so we can use it in the other dataframes.","9e64ae24":"### Transactions Dataset","0ddd4f45":"The general information about the dataframe points out to several problems:\n\n- **list price,standard cost** columns are stored as object, where in fact it should be numeric.\n- The **Transactions date** column should be converted into datetime columns not object\n- **Transaction date** column should have year,month name,day,day name columns\n- **online_order,brand, product line,product class,product size,standard cost and product_first_sold_date** have missing values.","2039cd41":"now we will Export data to csv file to use it in our next juyter notebook for phase 2 of the project which is Exploratory data analysis","7c5d0d28":"There are no duplicated rows in the dataset","314973c0":"General data information states that data types are consistent and there are no null values","162d3e0f":"### About the Dataset","9195de57":"we will create features engineering for some columns\n\n1. Changing DOB column into Age column\n2. Dropping DOB after conversion","9f512663":"There are no duplicated rows in the data","d91846e8":"IT seems that there are repeated values in **Gender** column and  inconsistent values in **default** column","a7c372f3":"### Customer demographics dataset","c1e92357":"Making the first row as header using the (titlehead) function created","49e06860":"### Objective of the report\n\nOur objective is to review the data quality for the three datasets to ensure that they are ready for our analysis , we will start wrangling and fixing each one of them seperately and then join them together into one dataset","5068a09f":"We will create new variables as follows:\n\n1. Transaction year\n2. Transaction month name\n3. Transaction day\n4. Transaction day name","62186bcd":"Since the three dataset are related to each other in which the first dataset is the transactions made by customers and the second dataset is the demographics of customers who made the transactions and third dataset is the addresses of these customers , we can conduct an inner join between them throught the customer id primary key column in the first dataset and customer id foregin keys columns in demographics and addresses datasets.","8ce911c3":"### Features Engineering","253845e0":"### 1. Data Wrangling & cleaning","61e25238":"we will now check the consistency of the values in each categorical column in the dataset,we will create a list that has all the categorical columns from the data set","5d16396f":"**product_first_sold_date** column seem to have a wrong date format which we will ammend","dea021d9":"**Country:** Country column represents one value only which is Australia so i will remove it since it will not be helpful in my analysis","a4e947b9":"**Customer demographic dataset is clean and ready for analysis**","584b57d1":"In this Phase #1 we will improve the quality of Sprocket Central Pty Ltd\u2019s data through:\n\n- Data wrangling and Cleaning\n\n- Features engineering\n\n- Joinning the datasets together","e0d50d51":"**Customer address dataset is clean and ready for analysis**","dc63f901":"The general information about the dataframe points out to several problems:\n\n1. **past_3_years_bike_related_purchases,tenure** is stored as object, where in fact it has no numeric meaning.\n2. **DOB** is stored as object where it should be converted to datetime\n3. **last_name,DOB,job_title,job_industry_category,default,tenure** has null values","95ca2a87":"There are no duplicate rows","d8d97531":"**Transactions dataset is clean and ready for analysis**","bf2e465b":"### Features Engineering","308928e9":"**Sprocket Central Pty Ltd**, a medium size bikes & cycling accessories organisation which has a large dataset relating to its customers, but their team is unsure how to effectively analyse it to help optimise its marketing strategy. \n\nThe client provided us with 3 datasets:\n\n- Customer Demographic\n\n- Customer Addresses\n\n- Transactions data in the past 3 months","25fe6adc":"## Sprocket Central Pty Ltd Company Customers Recommendation Project Module #1 Data wrangling","3410a32c":"### Customer address dataset"}}