{"cell_type":{"99da55cc":"code","006d09d0":"code","dbf8a1d1":"code","9e0c4ba9":"code","60ff24d0":"code","2de8d795":"code","1a39fcc9":"code","b1978512":"code","1a9243a7":"code","274fe74f":"code","34355d3b":"code","001c0bf1":"code","adf43da1":"code","00f12c4c":"code","919ed73d":"code","f5557bb9":"code","047663ae":"code","56973b7b":"code","98d1a976":"code","c46086d2":"code","2adb9513":"code","e98b72db":"code","6f8423e9":"code","dc6e125a":"code","3382b1ae":"code","838b3130":"code","5d49a2fb":"code","683b3014":"code","eaa54f48":"code","35d9fe11":"code","43ebd63e":"code","db17df1c":"code","fc15ffda":"code","00b47d23":"code","65335095":"code","38b3a262":"code","d36ba964":"code","06dd1829":"code","30b37f2f":"code","05a95682":"code","b8042768":"code","b56af4d8":"code","c3592c22":"code","5c20c853":"code","700800df":"code","3aa8fc6a":"code","ddd522ea":"code","ba6aa0bd":"code","3cad1028":"code","8284986b":"code","ecf3bab4":"code","540a63f3":"code","51c16511":"code","1047be84":"code","4ab17c07":"code","bbcd5893":"code","2314d7bc":"code","0dc5e0d5":"code","08dea80a":"code","1126cc3c":"code","9443358c":"code","f810f8d9":"code","75563de0":"code","4e6dc115":"markdown","738fcae7":"markdown","f1c990ea":"markdown","74ce9545":"markdown","6568edbd":"markdown","bc391949":"markdown","0ee16d9c":"markdown","841bc915":"markdown","bb5f7b59":"markdown","9e7722ab":"markdown","fb171292":"markdown","2b5aef4e":"markdown","892ce708":"markdown","724a55e7":"markdown","2356408e":"markdown","be899750":"markdown","5d65d9dc":"markdown","a8274bea":"markdown","f24af476":"markdown","06a550da":"markdown","68997996":"markdown","3572a015":"markdown","d503aec0":"markdown"},"source":{"99da55cc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","006d09d0":"df_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf_train.head()","dbf8a1d1":"df_train.info()","9e0c4ba9":"df_train.describe()","60ff24d0":"df_train.isnull().sum()","2de8d795":"df_train['Survived'].value_counts()","1a39fcc9":"df_train['Survived'].value_counts()\/len(df_train)*100","b1978512":"df_train = df_train.drop(['PassengerId','Ticket','Name','Cabin'],axis=1)","1a9243a7":"ax = sns.countplot(df_train['Survived'])","274fe74f":"ax = sns.catplot(x=\"Sex\", y=\"Age\", hue=\"Survived\", kind='bar', data=df_train)","34355d3b":"ax = sns.catplot(x='Pclass',y=\"Fare\", hue=\"Survived\", kind='bar', data=df_train)","001c0bf1":"ax = sns.countplot(df_train['Survived'], hue=df_train['Pclass'])","adf43da1":"_ = sns.distplot(df_train[df_train['Survived']==1]['Age'],  kde=True, label='Survived')\n_ = sns.distplot(df_train[df_train['Survived']==0]['Age'],  kde=True, label='Deceased')\nplt.legend(prop={'size': 12})\nplt.show()","00f12c4c":"_ = sns.distplot(df_train[df_train['Survived']==1]['Fare'],  kde=True, label='Survived')\n_ = sns.distplot(df_train[df_train['Survived']==0]['Fare'],  kde=True, label='Deceased')\nplt.legend(prop={'size': 12})\nplt.show()","919ed73d":"df_train['Sex'].value_counts()\/len(df_train)*100","f5557bb9":"df_train[df_train['Survived']==0]['Sex'].value_counts()\/len(df_train[df_train['Survived']==0])*100","047663ae":"df_train[df_train['Survived']==1]['Sex'].value_counts()\/len(df_train[df_train['Survived']==1])*100","56973b7b":"_ = sns.distplot(df_train[df_train['Survived']==1]['Pclass'],  kde=True, label='Survived')\n_ = sns.distplot(df_train[df_train['Survived']==0]['Pclass'],  kde=True, label='Deceased')\nplt.legend(prop={'size': 12})\nplt.show()","98d1a976":"df_train[df_train['Survived']==1]['Pclass'].value_counts()\/len(df_train[df_train['Survived']==1])*100","c46086d2":"df_train[df_train['Survived']==0]['Pclass'].value_counts()\/len(df_train[df_train['Survived']==0])*100","2adb9513":"ax = sns.countplot(df_train['Parch'],hue=df_train['Survived'])","e98b72db":"ax = sns.boxplot(x='Pclass',y='Fare',hue='Survived',data=df_train)","6f8423e9":"ax = sns.countplot(df_train['Parch'],hue=df_train['Pclass'])","dc6e125a":"ax = sns.countplot(x='Parch',hue='Survived',data=df_train)\nplt.legend(loc='upper right')\nplt.show()","3382b1ae":"ax = sns.countplot(df_train['SibSp'],hue=df_train['Survived'])","838b3130":"ax = sns.catplot(x='SibSp',y='Pclass',hue='Survived',kind='bar',data=df_train)","5d49a2fb":"ax = sns.catplot(x='Parch',y='Pclass',hue='Survived',kind='bar',data=df_train)","683b3014":"df_train['Family']=df_train['SibSp']+df_train['Parch']","eaa54f48":"_ = sns.distplot(df_train[df_train['Survived']==1]['Family'],  kde=True, label='Survived')\n_ = sns.distplot(df_train[df_train['Survived']==0]['Family'],  kde=True, label='Deceased')\nplt.legend(prop={'size': 12})\nplt.show()","35d9fe11":"ax = sns.countplot(df_train['Embarked'],hue=df_train['Survived'])","43ebd63e":"df_train['Embarked'].value_counts()\/len(df_train)","db17df1c":"df_train[df_train['Survived']==1]['Embarked'].value_counts()\/len(df_train[df_train['Survived']==1])*100","fc15ffda":"df_train[df_train['Survived']==0]['Embarked'].value_counts()\/len(df_train[df_train['Survived']==0])*100","00b47d23":"df_train['Sex'] = pd.get_dummies(df_train['Sex'])\ndf_train['Age'] = df_train['Age'].fillna(df_train['Age'].median())\ndf_train['Embarked'] = df_train['Embarked'].map({'C':0,'Q':1,'S':2})","65335095":"df_train.isnull().sum()","38b3a262":"df_train = df_train.dropna()","d36ba964":"df_train.head()","06dd1829":"correlation = df_train.corr()","30b37f2f":"ax = sns.heatmap(correlation,vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True)","05a95682":"ax = sns.pairplot(df_train)","b8042768":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, precision_recall_curve\nimport time\nfrom sklearn import __version__ as sklearn_version","b56af4d8":"# define your X and y\nX = df_train.drop(['Survived'],axis=1)\ny = df_train['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2,stratify=y, random_state=42)","c3592c22":"columns = ['Model Name', 'accuracy','precision','recall','ROC AUC score','run time']\nresults = pd.DataFrame(columns=columns)","5c20c853":"def metrics(model_name,y_test,y_pred):\n    accuracy = accuracy_score(y_test,y_pred)\n    roc_auc =roc_auc_score(y_test, y_pred)\n    precision = precision_score(y_pred=y_pred, y_true=y_test,zero_division=1)\n    recall = recall_score(y_pred=y_pred, y_true=y_test,zero_division=1)\n    \n    print(classification_report(y_test, y_pred,zero_division=1))\n\n    cf_matrix = confusion_matrix(y_test, y_pred)\n    sns.heatmap(cf_matrix, annot=True,fmt='3', cmap='Blues')\n    plt.xlabel('Predicted user status',fontsize=12)\n    plt.ylabel('True user status',fontsize=12)\n    plt.title('%s Confusion Matrix' % model_name,fontsize=20)\n    plt.show()\n\n    fpr, tpr, threshold = roc_curve(y_test, y_pred)\n    plt.plot([0,1], [0,1], 'k--')\n    plt.plot(fpr, tpr, label=model_name)\n    plt.xlabel('False Positive Rate',fontsize=12)\n    plt.ylabel('True Positive Rate',fontsize=12)\n    plt.title('%s ROC Curve'% model_name,fontsize=20)\n    plt.legend(fontsize=12)\n    plt.show()\n    #pipes = pipes.append(pipe)\n    return pd.DataFrame([[model_name,accuracy, precision, recall,roc_auc,t1]],columns=columns)","700800df":"model_name = 'Dummy'\nmodel = DummyClassifier(strategy='most_frequent')\n\npipe_dummy = make_pipeline(\n       SimpleImputer(strategy='median'), \n       StandardScaler(), \n       model)\n      \n##training the model\nt0 = time.time()\npipe_dummy.fit(X_train,y_train)\nt1 = time.time() - t0\n    \n#prediction with model\ny_pred = pipe_dummy.predict(X_test)\nprint('time to run in seconds: ',format(t1)) \nresults = results.append(metrics(model_name,y_test,y_pred))","3aa8fc6a":"model_name = 'Naive Bayes'\nNB = GaussianNB()\nparams_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\ngrid = GridSearchCV(estimator=NB, param_grid=params_NB, cv=5)\ngrid = grid.fit(X_train,y_train)\nmodel = grid.best_estimator_\n\npipe_NB = make_pipeline(\n       SimpleImputer(strategy='median'), \n       StandardScaler(), \n       model)\n      \n##training the model\nt0 = time.time()\npipe_NB.fit(X_train,y_train)\nt1 = time.time() - t0\n    \n#prediction with model\ny_pred = pipe_NB.predict(X_test)\nprint('time to run in seconds: ',format(t1))  \nresults = results.append(metrics(model_name,y_test,y_pred))","ddd522ea":"model_name = 'Logistic Regression'\nparam_grid = [{'penalty' : ['l1', 'l2'],\n               'C' : np.logspace(0, 4, 10),\n               'solver' : ['liblinear']}]\nLR = LogisticRegression()\ngrid = GridSearchCV(estimator=LR, param_grid=param_grid, cv=5)\ngrid = grid.fit(X_train,y_train)\nmodel = grid.best_estimator_        \n        \npipe_LR = make_pipeline(\n       SimpleImputer(strategy='median'), \n       StandardScaler(), \n       model)\n      \n##training the model\nt0 = time.time()\npipe_LR.fit(X_train,y_train)\nt1 = time.time() - t0\n    \n#prediction with model\ny_pred = pipe_LR.predict(X_test)\nprint('time to run in seconds: ',format(t1))  \nresults = results.append(metrics(model_name,y_test,y_pred))","ba6aa0bd":"model_name = 'kNN'\nknn = KNeighborsClassifier()\nparam_grid = {'n_neighbors': [3, 5, 7, 9, 11],\n              'weights': ['uniform', 'distance']\n             }  \ngrid = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5)\ngrid = grid.fit(X_train,y_train)\nmodel = grid.best_estimator_\n\npipe_kNN = make_pipeline(\n       SimpleImputer(strategy='median'), \n       StandardScaler(), \n       model)\n      \n##training the model\nt0 = time.time()\npipe_kNN.fit(X_train,y_train)\nt1 = time.time() - t0\n    \n#prediction with model\ny_pred = pipe_kNN.predict(X_test)\nprint('time to run in seconds: ',format(t1))  \nresults = results.append(metrics(model_name,y_test,y_pred))","3cad1028":"model_name = 'Random Forest'\nrfc=RandomForestClassifier(random_state=1234)\nparam_grid = {'n_estimators': [100,200],\n              'max_features': ['auto', 'sqrt', 'log2'],\n              'max_depth' : [5,10,15,20,25,50],\n              'criterion' :['gini', 'entropy']}\ngrid = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5) \ngrid = grid.fit(X_train,y_train)\nmodel = grid.best_estimator_\n        \npipe_rfc = make_pipeline(\n       SimpleImputer(strategy='mean'), \n       StandardScaler(), \n       model)\n      \n##training the model\nt0 = time.time()\npipe_rfc.fit(X_train,y_train)\nt1 = time.time() - t0\n    \n#prediction with model\ny_pred = pipe_rfc.predict(X_test)\nprint('time to run in seconds: ',format(t1))  \nresults = results.append(metrics(model_name,y_test,y_pred))\n\n","8284986b":"results","ecf3bab4":"df_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndf_test.head()","540a63f3":"df_test.isnull().sum()","51c16511":"passengerId = df_test['PassengerId']\ndf_test['Family'] = df_test['SibSp'] + df_test['Parch']\ndf_test = df_test.drop(['Cabin','Name','Ticket','PassengerId'],axis=1)","1047be84":"df_test['Age'] = df_test['Age'].fillna(df_test['Age'].median())\ndf_test['Fare'] = df_test['Fare'].fillna(df_test['Fare'].median())","4ab17c07":"df_test.info()","bbcd5893":"df_test['Embarked'] = df_test['Embarked'].map({'C':0,'Q':1,'S':2})\ndf_test['Sex'] = pd.get_dummies(df_test['Sex'])","2314d7bc":"df_test.head()","0dc5e0d5":"prediction = []\nfor i in range(len(df_test)):\n    test = pipe_rfc.predict([df_test.loc[i]])\n    prediction.append(test)","08dea80a":"prediction = pd.DataFrame(prediction, columns = ['Survived'])","1126cc3c":"test_data_predictions = pd.concat([passengerId,prediction],axis=1)","9443358c":"test_data_predictions.head()","f810f8d9":"test_data_predictions.to_csv('Titanic_survivor_prediction.csv',index=False)","75563de0":"pd.read_csv('Titanic_survivor_prediction.csv')","4e6dc115":"Since more people were travelling from Southampton, more deceased embarked from that place. ","738fcae7":"It seems people travelling with smaller family had better survival chances. We can generate a new feature family that comprises of parents, children, siblings and spouse.","f1c990ea":"More people were travelling to Southampton. The second highest was Cherbourg and last Queenstown","74ce9545":"Approximately 70% of the people who survived were female.","6568edbd":"Clearly we can see that as the size of family increased the survival reduced.","bc391949":"People who survived were mostly around 25 to 30 years. This is also the mean age of the passengers. It looks like more female passengers survived compared to male passengers.","0ee16d9c":"Let's impute the missing Age with median values and convert Sex as Male:0 and Female: 1\nWe will also map the Embarked columns as C:0, Q:1 nad S:2","841bc915":"The large spike of deceased people suggests more people with a cheaper ticket had less chance of survival.","bb5f7b59":"Most of the deceased were travelling with either no child or no parents.","9e7722ab":"There were more people in passenger class 1 and mostly survived. Passenger Class 2 has small difference in the survival. In class 3 there seems to be an equal number of people who survive and who did not.","fb171292":"Out of the passengers who died 85% were male.","2b5aef4e":"Out of all the models random forest seemed to perform the best. It has the highest ROC-AUC score meaning less misclassification overall. Let's plot the features that were used in the model in order of importance.","892ce708":"We will drop Name, PassengerId and Cabin from our analysis","724a55e7":"We can see some nan values in column cabin ","2356408e":"There is a correlation between Survived and Pclass, Sex, and Fare.","be899750":"Again, class 3 passengers were travlling without child\/parents. Class 1 had mostly 0 parent child followed by 1 and 2 child. ","5d65d9dc":"Passengers in Pclass 3 were mostly not able to survive. However, among the passengers who survived there is not a clear distinction among the 3 class. ","a8274bea":"Column age in training dataset is missing a few values and cabin has a lot of missing data as well","f24af476":"The histograms of survival by Age does not show any distinct difference. However, we can see a spike around 0 years suggesting babies are seen in the group of people who survived. Also, the deceased group is skewed towards younger people.","06a550da":"Most people from Pclass 3 did not survive. On the other hand most of the survivors were from Pclass 1. \nCould this be due to age? Perhaps the passengers in Pclass 1 had better access to safety equipments and were also located on a better part of the ship. Lets see the age distribution first.","68997996":"More than 60% of the passengers died in the disaster.","3572a015":"From the above statistics we can conclude that mean age of passengers were 28. With 75% of the passengers were around 40 years. Although there were some old passengers. The average fare was 32 dollars which was more to the 75%.","d503aec0":"Passengers in Pclass 1 who survived paid a higher Fare in comparison to passengers in the same class who were not able to survive. "}}