{"cell_type":{"8412087b":"code","2764d5b3":"code","9e500d12":"code","7332bc8b":"code","2d08fe35":"code","c3104bd8":"code","30d7f349":"code","cdb0197c":"code","44976a69":"code","fae85938":"code","daf490f5":"code","fa0dcdd9":"code","455b9184":"code","c7d71427":"markdown","b98c4421":"markdown","510cfb42":"markdown","6df87b00":"markdown","26727b46":"markdown"},"source":{"8412087b":"#Imports\nimport pandas as pd\nimport numpy as np","2764d5b3":"# Load the data\ndata_clf=pd.read_csv('..\/input\/iris\/Iris.csv') # for classification problem\ndata_reg=pd.read_csv('..\/input\/50-startups\/50_Startups.csv') # for regression problem","9e500d12":"# Check first five datapoints by using head() method\nprint(data_clf.head(2))\nprint(data_reg.head(2))","7332bc8b":"# Check numerical statistics using info() method\ndata_clf.info(), data_reg.info()","2d08fe35":"# Create feature and target variable for Classification problem\nX_clf=data_clf.iloc[:,1:5] # features: SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm'\ny_clf=data_clf.iloc[:,5] # Target variable: Iris species","c3104bd8":"# Create feature and target variable for Regression problem\nX_reg=data_reg.iloc[:,0:3] # features: R&D Spend, Administration, Marketing Spend\n# I have not considered 'State' in feature set. You can use it after label encoding.\ny_reg=data_reg.iloc[:,4] # Target variable: Profit","30d7f349":"# Import SelectKBest, chi2(score function for classification), f_regression (score function for regression)\nfrom sklearn.feature_selection import SelectKBest, chi2, f_regression","cdb0197c":"# Create the object for SelectKBest and fit and transform the classification data\n# k is the number of features you want to select [here it's 2]\nX_clf_new=SelectKBest(score_func=chi2,k=2).fit_transform(X_clf,y_clf)","44976a69":"# Check the newly created variable for top two best features\nprint(X_clf_new[:5])","fae85938":"# Compare the newly created values with feature set values to know the selected features\nprint(X_clf.head())","daf490f5":"# Create the object for SelectKBest and fit and transform the regression data\nX_reg_new=SelectKBest(score_func=f_regression, k=2).fit_transform(X_reg,y_reg)","fa0dcdd9":"# Check the newly created variable for top two best features\nprint(X_reg_new[:5])","455b9184":"# Compare the newly created values with feature set values to know the selected features\nprint(X_reg.head())","c7d71427":"**From the above we see that the best two predictors for start up profit are: **\n\n1. R&D Spend\n2. Marketing Spend","b98c4421":"Feature selection is a technique where we choose those features in our data that contribute most to the target variable.\nIn other words we choose the best predictors for the target variable.\n\nThe classes in the **sklearn.feature_selection** module can be used for feature selection\/dimensionality reduction on sample sets, either to improve estimators\u2019 accuracy scores or to boost their performance on very high-dimensional datasets.\n\n**[Advantage:](https:\/\/machinelearningmastery.com\/feature-selection-machine-learning-python\/) ** \n_Refer this link for a nice article on this by Jason Brownlee_\n\n1. Reduces Overfitting: Less redundant data means less possibility of making decisions based on redundant data\/noise.\n2. Improves Accuracy: Less misleading data means modeling accuracy improves.\n3. Reduces Training Time: Less data means that algorithms train faster.","510cfb42":"**Feature selection method:  <font color=red>SelectKBest<\/font>**\n\nScore function: <br>\n**For regression:** f_regression, mutual_info_regression<br>\n**For classification:** chi2, f_classif, mutual_info_classif","6df87b00":"**From the above we see that the best two predictors for Iris species are: **\n\n1. PetalLengthCm\n2. PetalWidthCm","26727b46":"# Feature selection for supervised models using <font color=green>SelectKBest<\/font>\n[scikit-learn link](http:\/\/scikit-learn.org\/stable\/modules\/feature_selection.html)"}}