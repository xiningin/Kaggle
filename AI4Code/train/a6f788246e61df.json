{"cell_type":{"7ddcf5a2":"code","139cfb01":"code","b819b3b5":"code","3f376b73":"code","d7046e6b":"code","ecf8f80b":"code","f53f5e27":"code","f96aa433":"code","7d8b5e82":"code","6c1e7562":"code","bf97e61c":"code","7ca10efa":"code","c3ea8224":"code","1a8cddaf":"code","67c1ca8c":"code","a21e619f":"code","177e169e":"code","e19c5817":"code","7e247c88":"code","65070e9a":"code","9eb8d524":"code","0800523c":"code","77fd9cd1":"code","b1db41ee":"code","8298a340":"code","100a3f8e":"code","ec4941f5":"code","7bbd34a4":"code","6da79961":"code","56463eee":"code","04ac021c":"code","3f0b2f0d":"code","79244119":"code","e1c85688":"code","2e196039":"code","25c5545a":"code","e5ad2c21":"code","6c5a0aca":"code","e4c16c90":"code","4f0fd8d2":"markdown","1469fe67":"markdown","d4e50322":"markdown","63e762b2":"markdown","5a7df4d8":"markdown","cea07328":"markdown","ae529d5a":"markdown","0fe375bd":"markdown","3744cade":"markdown","0f06dd31":"markdown","882edbb6":"markdown","00d1b63b":"markdown","fedb66fa":"markdown","43827fc9":"markdown","e5217934":"markdown","9d3d7066":"markdown","7ef07621":"markdown","d871b66b":"markdown","899cdc72":"markdown","a60d78c7":"markdown","2d9b6bcf":"markdown","3c3379c3":"markdown","a454f219":"markdown","e71bc3a5":"markdown","c2fd625d":"markdown","c662718d":"markdown","fa26d1bf":"markdown","50be318b":"markdown","1dfd1659":"markdown","f3abae52":"markdown","47c23084":"markdown","5281d78f":"markdown","f959fa06":"markdown"},"source":{"7ddcf5a2":"import numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","139cfb01":"import warnings\nwarnings.simplefilter('ignore')","b819b3b5":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","3f376b73":"def get_data():\n    train = pd.read_csv(\"..\/input\/steam-game-reviews\/train.csv\")\n    test = pd.read_csv(\"..\/input\/steam-game-reviews\/test.csv\")\n    game = pd.read_csv(\"..\/input\/steam-game-reviews\/game_overview.csv\")\n    sub = pd.read_csv(\"..\/input\/steam-game-reviews\/sample_submission.csv\")\n    \n    print(\"Train Shape : \\t{}\\nTest Shape : \\t{}\\nOverview Shape :{}\\n\".format(train.shape, test.shape, game.shape))\n\n    return train, test, game, sub","d7046e6b":"train, test, game, sub = get_data()","ecf8f80b":"train.head(2)","f53f5e27":"train_unique_titles = set(train['title'].unique())\ntest_unique_titles = set(test['title'].unique())\n\ncommon_titles = set.intersection(train_unique_titles, test_unique_titles)\n\nprint(\"Number of Common Titles between Train and Test : {}\".format(len(common_titles)))","f96aa433":"train.drop(['title'], axis=1, inplace=True)\ntest.drop(['title'], axis=1, inplace=True)","7d8b5e82":"plt.figure(figsize=(15, 5))\nsns.countplot(train['year'], hue=train['user_suggestion'])\nplt.show()","6c1e7562":"train.drop(['year'], axis=1, inplace=True)\ntest.drop(['year'], axis=1, inplace=True)","bf97e61c":"# Sample Review : \n\ntrain['user_review'].iloc[0]","7ca10efa":"# user_review distribution\nlens = train['user_review'].str.len()\nprint(\"Mean : \\t{}\\nSTD : \\t{}\\nMAX: \\t{}\".format(lens.mean(), lens.std(), lens.max()))\n\nlens.hist()\nplt.title(\"User Review Distribution\")\nplt.show()","c3ea8224":"train['user_suggestion'].value_counts()","1a8cddaf":"train[train['user_suggestion'] == 0]['user_review'].iloc[1]","67c1ca8c":"train[train['user_suggestion'] == 1]['user_review'].iloc[1]","a21e619f":"# Sample non-english reviews\ntrain['user_review'].iloc[331]","177e169e":"!pip install langdetect\nfrom langdetect import detect\nfrom tqdm import tqdm_notebook","e19c5817":"# Sample usage : \ntrain['user_review'].iloc[1][: 20], detect(train['user_review'].iloc[1])","7e247c88":"def get_language(X):\n    try:\n        return detect(X)\n    except:\n        return \"Error\"","65070e9a":"%%time\n\ntrain['user_review_language'] = train['user_review'].apply(lambda x: get_language(x))","9eb8d524":"print(\"Original Shape : {}\".format(train.shape))\n\nnew_lang_df = train[train['user_review_language'] != 'en'].copy()\ntrain = train[train['user_review_language'] == 'en'].copy()\n\nprint(\"New Shape : {}\".format(train.shape))","0800523c":"import re\nfrom re import finditer\n\ndef remove_EAR(X):\n    \"\"\"\n    Removing 'Early Access Review'\n    \"\"\"\n    X = X.replace(\"Early Access Review\", \"\")\n    \n    return X\n\ndef split_number_and_text(X):\n    x = re.split('(\\d+)', X)\n    x = \" \".join(x)\n    x = x.strip()\n    \n    return x\n\ndef handle_camelcase(X):\n    matches = finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', X)\n    return \" \".join([m.group(0) for m in matches])\n\ndef handling_whitespaces(X):\n    X = \" \".join(X.split())\n    X = X.strip()\n    \n    return X\n\nwaste_symbols = \"\u4eba\u0333\u28df\u28e6\u032a\u2813\u2592\u034e\u00b8\u281f\u28c5>\u287e \u283b\u28c0\u28db\u201e\u036d\u28ee\u287b\u2826\u2840\u0350\u2018\u0328\u28c6\u0324\u28ff<\uff0f\u4e36\u28de\u0347\u28f5\u035e\u2839\u0369\u2892\u032f\u28b8\u28e4\u036f\u0317\u0346\u0314\u0360\u032b\u281b\u28bb\u280f-\u0301\u2610\u035b\u033a\u030b\u2838\u28e5\u2804\u0337\uff3c\u035f\u00b7\u2312\u0357\u2801\u0341\uff40\u28b9\\\\\u2884\u030c\u0348\u0368\u28a4\u5f61~\u00af\/\u2836\u2832\u02c6\u2865\u032e\u033b\u0354\u2609\u28fb\u0323\u309d\u285e\u033f\u0359\u0312\u030a\u0327\u0311\u30ce\u282d\u0364_\u2810\u28c7\u0489\u031a\u2013\u2844\u00b4\u0343\u2588\u2584\u2611\u28e7\u0334\u030d\u0356\uff5c\u28f7\u0358\u032d\u035d\uff61\u2834\u0304\u031c\u0296\u00a8\u030f\u0362\u0335\u2882\u034b;\u0352:\u2889\u3064\u033e\uff3f\u0308\u28f4\u28cc\u036b\u289b\u2879\u28c8\u3078\u28af,\u0305\u28ed\u0315\u0329\u032c\u2848\u30e0\u0361\u28fc\u0366)\u031b\u035c\u30fd\u031d\u0325\u28e0\u289f\u0336\u2824\u0349\u0321\u2818\u0344\u0339\u2874\u0320\u2880\uff09\u2807\u28fe\u034a\u28b0\u031e\u036e\u0307`\u2811\u287f\\u3000\u2803\u28f8\u283e\u0345\u0306\u034d\uffe3\u289a\u0313\u2802\u2875\u2500\u28ac\u30fc\u283f(\u2806\u2809\u0326*\u0355\uff89\u28f9\u285f\u28ec\u2819\u2593\u28507\u034f\u031f\u0332\u28bf\u28a6\uff08\u0330\u2665\u0322\u0338\u28d9\u0302\u0353\u2580\u304f\uff8c\u2800.\u2830\u2852\u00b0\u030e\u0316\uff64\u28d2\u28f0\u033c\u2885\u28c1\u2812\u0351\u28be\u2842\u034c\u0340\u0367\u2026\u0303\u2590\uff9a\u3001\u4e3f\u288c|\u0331\u28b4\u2860\u28e9\u258c\u28c9\u036a\u035a'\u2886\u28a0\u2847\u285b\u28cf\u2876\u28dc\u28c4\u2878\u2808\u0318\u0363\u28fd\u0309\u033d\u0310\u0365\u284f\u036c\u28d7\u28f6\u2591\u280b\u2814\u0342\u0319^\"\n\ndef remove_waste_symbols(X):\n    for item in waste_symbols:\n        X = X.replace(item, \" \")\n        \n    return X","77fd9cd1":"def clean_review(X):\n    X = remove_EAR(X)\n    X = remove_waste_symbols(X)\n    X = handle_camelcase(X)\n    X = split_number_and_text(X)\n    X = handling_whitespaces(X)\n    \n    return X","b1db41ee":"%%time\n\ntrain['user_review_clean'] = train['user_review'].apply(lambda x: clean_review(x))\ntest['user_review_clean'] = test['user_review'].apply(lambda x: clean_review(x))","8298a340":"train.reset_index(drop=True, inplace=True)","100a3f8e":"from sklearn.feature_extraction.text import CountVectorizer\n\nsample_texts = ['Hello this is review number 1, Bye Bye', 'I am not a review']\n\nvect = CountVectorizer()\nvect.fit(sample_texts)\n\nvect.vocabulary_","ec4941f5":"for item in sample_texts:\n    \n    print(\"Text : {}\\nEncoded Format : {}\".format(item, vect.transform([item]).toarray()))","7bbd34a4":"X = pd.DataFrame(sample_texts, columns=['text'])\nenc_texts = vect.transform(X['text'].values)\nenc_texts = pd.DataFrame(enc_texts.toarray(), columns=vect.get_feature_names())\n\nX = pd.concat([X, enc_texts], axis=1)\nX.head(2)","6da79961":"%%time\n\ntotal_reviews = pd.concat([train['user_review_clean'], test['user_review_clean']], axis=0)\ntotal_reviews.reset_index(drop=True, inplace=True)\n\nvect = CountVectorizer()\n\nvect.fit(total_reviews.values)\n\ntrain_count_vect = vect.transform(train['user_review_clean'].values)\ntest_count_vect = vect.transform(test['user_review_clean'].values)\n\nprint(\"Number of features \/ words in vocab : {}\".format(len(vect.get_feature_names())))","56463eee":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score","04ac021c":"X_train, X_valid, y_train, y_valid = train_test_split(train_count_vect, train['user_suggestion'], test_size=0.15, random_state=13)","3f0b2f0d":"model = LogisticRegression()\n\nmodel.fit(X_train, y_train)\n\ntrain_score = accuracy_score(y_train, model.predict(X_train))\nvalid_score = accuracy_score(y_valid, model.predict(X_valid))\n\nprint(\"Train Score : {}\\nValid Score : {}\".format(train_score, valid_score))","79244119":"results = cross_val_score(model, train_count_vect, train['user_suggestion'].values, cv=3, scoring='accuracy')\n\nprint(\"Accuracy Mean : \\t{}\\n3-Fold Scores : \\t{}\".format(results.mean(), results))","e1c85688":"from sklearn.feature_extraction.text import TfidfVectorizer","2e196039":"%%time\n\nvect = TfidfVectorizer()\n\nvect.fit(total_reviews.values)\n\ntrain_tfidf_vect = vect.transform(train['user_review_clean'].values)\ntest_tfidf_vect = vect.transform(test['user_review_clean'].values)\n\nprint(\"Number of features \/ words in vocab : {}\".format(len(vect.get_feature_names())))","25c5545a":"from nltk.corpus import stopwords\nenglish_stopwords = stopwords.words('english')\n\nprint(english_stopwords[: 5])\n\nvect = TfidfVectorizer(stop_words=english_stopwords)","e5ad2c21":"X_train, X_valid, y_train, y_valid = train_test_split(train_tfidf_vect, train['user_suggestion'], test_size=0.15, random_state=13)","6c5a0aca":"model = LogisticRegression()\n\nmodel.fit(X_train, y_train)\n\ntrain_score = accuracy_score(y_train, model.predict(X_train))\nvalid_score = accuracy_score(y_valid, model.predict(X_valid))\n\nprint(\"Train Score : {}\\nValid Score : {}\".format(train_score, valid_score))","e4c16c90":"results = cross_val_score(model, train_tfidf_vect, train['user_suggestion'].values, cv=3, scoring='accuracy')\n\nprint(\"Accuracy Mean : \\t{}\\n3-Fold Scores : \\t{}\".format(results.mean(), results))","4f0fd8d2":"### Import Packages","1469fe67":"It seems there are no common titles so it doesn't make sense to add 'title' to our feature set.","d4e50322":"#### Implementing\n\nSometimes we can have unique words in our test set so its better to fit the vectorizer on [train + test] sets to avoid any errors.","63e762b2":"### Preprocessing\n\n1. Non-English Reviews?\n2. Cleaning reviews","5a7df4d8":"This looks almost a balanced dataset.","cea07328":"### CountVectorizer\n\n#### Understanding the concept\n\nThe idea here is very simple. It is similar to One-Hot Encoding of categorical veriables. \n\nIn this method we create N features where N = (number of words in vocab). Here vocab is the set of all unique words in reviews. And the value in the feature created is the count of the word in the particular review.\n\n![](https:\/\/miro.medium.com\/proxy\/1*YEJf9BQQh0ma1ECs6x_7yQ.png)","ae529d5a":"#### Cleaning Text\n\n1. Removing \"Early Access Review\" : This phrase is added in the starting for reviews which are given by the users who get an early access pass.\n1. Splitting combined numbers and text : \"12AB\" --> \"12 AB\"\n1. Handling CamelCase : \"youOn\" --> \"you On\"\n1. Removing extra whitespaces in between words, sentences, left & right trailing.\n1. Removing waste symbols.","0fe375bd":"I can't see any pattern between year and user_suggestion. Thinking, If a game is great\/trash it doesn't depend on the year. (I guess)","3744cade":"Here we can see the vocabulary with the index assigned to each word.","0f06dd31":"#### Non-English Reviews?\n\nAre there any reviews which are not in english. Let's check.","882edbb6":"### Conclusion","00d1b63b":"### How does user_suggestion == 0 look?\n\nThat is how does a negative user_review look like?","fedb66fa":"#### Year","43827fc9":"sklearn's TfidfVectorizer has an inbuilt feature to remove stopwords.","e5217934":"#### User_Review","9d3d7066":"#### User Suggestion","7ef07621":"#### Evaluating our data","d871b66b":"#### We can see that our accuracy has increased from 78% (CountVect) to 79.9% (TF-IDF Vect).\n\nUsually TF-IDF is a better and intelligent method, but it has its own limitations.\n\nSome of the limitations are : \n\n1. As TF-IDF and CountVect are methods built on frequencies of words, they do not capture \n    - position in text (whether the word is in starting, end, between any words, and most importantly the *CONTEXT*)\n    - semantics (*CONTEXT*)\n    - co-occurrences in different documents \n    \n2. Cannot capture semantics (e.g. as compared to topic models, word embeddings, etc)\n\n\nThe limitations will become more inherent when we use other methods which can tackle these limitations like : \n\n1. Word Embeddings\n2. Temporal Networks which can take in the structure of sentence and understand the context in the review (LSTM, RNN, etc)\n3. Complex Context Based Networks (BERT, GPT, XLNET)\n\nI'll try to cover these in the upcoming kernels in this series!\n\nPlease upvote if you liked the kernel!\n\nStay Safe!","899cdc72":"Let's get all the reviews list which are not english and remove them.","a60d78c7":"'Bye' has a count of 2 in first review so hence the value 2 in the feature 'bye'.","2d9b6bcf":"#### Title\n\nChecking the common titles.","3c3379c3":"'en' is the language code for english. Any language codes different than 'en' would represent that is non-english.","a454f219":"### How does user_suggestion == 1 look?\n\nThat is how does a positive user_review look like?","e71bc3a5":"Almost removed 214 records.","c2fd625d":"### Little EDA\n\nThe various columns are : \n\n1. Title : Game title\n2. Year : Year the review was written\n3. User_Review : The user review on steam platform\n4. User_Suggestion : A binary variable which represents whether a user suggests whehter you should buy the game or not.","c662718d":"Cross Validation Score : ","fa26d1bf":"Cross-Validating our data : ","50be318b":"As the feature set is huge (56484) it is better if we fit smaller and faster models to evaluate our data.\n\nTo limit the number of features we can use *max_features* to set the maximum number of features we want.\n\n    vect = CountVectorizer(max_features=5000)","1dfd1659":"Let's make a dataframe to better understand this","f3abae52":"### TFIDF-Vectorizer\n\nSo a common question might be that *count* can't be a significance of how important a feature is. \n\n - In a review word \"a\", \"are\", etc can occur many times, but doesn't add to semantic meaning of the review? \n - But words like \"good\", \"bad\" can occur less frequently but has a great influence of the sentiment of the review.\n\n**Certain words are used to formulate sentences but do not add any semantic meaning to the text.**\n\nThese are known as stop-words in NLP.\n\nSo we can score the relative importance of words using the strategy *TF-IDF (Term Frequecy | Inverse Document Frequency).*\n\nLets understand these terms seperatly for now, and then we will combine these two to complete the strategy.\n\n1. Term-Frequency : \n    \n    The number of times a word appears in a document divded by the total number of words in the document. Every document has its own term frequency.\n    \n    This score  measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization:\n    \n        TF(t) = (Number of times term t appears in a document) \/ (Total number of terms in the document).\n\n    ![term-frequency-formula](https:\/\/miro.medium.com\/proxy\/1*HM0Vcdrx2RApOyjp_ZeW_Q.png)    \n    \n    \n    \n2. Inverse-Document-Frequency :\n\n    The log of the number of documents divided by the number of documents that contain the word w. Inverse data frequency determines the weight of rare words across all documents in the corpus. \n    \n    This measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:\n    \n        IDF(t) = log_e(Total number of documents \/ Number of documents with term t in it).\n        \n    ![tfidf-formula](https:\/\/miro.medium.com\/proxy\/1*A5YGwFpcTd0YTCdgoiHFUw.png)\n    \n\n    \n - Simple Example : \n\n    Consider a document containing 100 words wherein the word cat appears 3 times. \n    \n    The term frequency (i.e., tf) for cat is then (3 \/ 100) = 0.03. \n    \n    Now, assume we have 10 million documents and the word cat appears in one thousand of these. \n    \n    Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 \/ 1,000) = 4. \n    \n    Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12.","47c23084":"### Vectorizing the reviews \n\nNow the problem of making sense of text to our machine learning comes. Normally when we have categorical labels we use LabelEncoding, One-Hot Encoding to encode the strings to numbers, but when we have full-blown text with no common occurences, what can we do?\n\nThere are multiple methods to handing text. In this kernel we are going to learn about two such methods namely : \n\n1. CountVectorizer\n2. TFIDFVectorizer","5281d78f":"# Getting Started with NLP\n\nHey. I've started this Getting Started series to explain some concepts which can be used to tackle NLP problems.\n\nIn this kernel I'll be explaining the basic concepts of how to *Vectorizing Text* so that our Machine Learning Model can make some sense out of it.","f959fa06":"We can see that some seperate words are appended as one : \n\nExample : youOverall --> \"you Overall\""}}