{"cell_type":{"5aad7707":"code","c9f015c4":"code","7d53b575":"code","e6cd63b1":"code","d71a6dc4":"code","9b28b7f0":"code","729abbf1":"code","207ef86e":"code","5f02d567":"code","59e54395":"code","55b731e2":"code","86e3b8ac":"code","cecc5427":"code","2dd5cca2":"code","6a3f80a0":"code","f07e4f1a":"code","87b34945":"code","68ae5c95":"code","42544d3f":"code","b054d216":"code","60924179":"code","7a1a43cb":"code","0ed241ad":"code","2db78960":"code","79f83cc1":"code","ad2365a5":"code","9bea26ff":"code","8b491675":"code","46c9048b":"code","0f7fc010":"code","c1221bc7":"code","38c2c0b9":"code","f7add9ba":"code","ba8247e9":"code","32859c8c":"code","95dba48f":"markdown","f4a8bf06":"markdown","6d38b5d6":"markdown","f3dd30b7":"markdown","e01ebf81":"markdown","8cf9a1a4":"markdown","8a92587f":"markdown","04dfbc4d":"markdown","b9d5071e":"markdown","9cf221eb":"markdown","3e7daf62":"markdown","55058834":"markdown","2d506fac":"markdown","88df71e0":"markdown","eea49035":"markdown","bad3696e":"markdown","74ead06a":"markdown","72acd8f6":"markdown","066a4bc4":"markdown","194d2b00":"markdown","0f9889f6":"markdown","ba3095fb":"markdown","fecd6738":"markdown","089a0388":"markdown"},"source":{"5aad7707":"import pandas as pd\nimport numpy as np\n\ndata = pd.read_csv('..\/input\/parkinson-disease-detection\/Parkinsson disease.csv')\ndata.head()","c9f015c4":"num_rows = len(data)\ndata = data.sample(num_rows, random_state=489)\ndata = data.reset_index(drop=True)","7d53b575":"cut = int(num_rows*0.8)\n\nX_train = data[:cut]\nX_test = data[cut:]\nprint(X_train.shape), print(X_test.shape)","e6cd63b1":"X_test['status'].mean()","d71a6dc4":"# seems that we need this only in decision tree\n# from sklearn.model_selection import train_test_split\n\n# X_train, X_val = train_test_split(X, train_size=0.7, random_state=13)","9b28b7f0":"y_train = X_train['status'].to_numpy()\n#y_val = X_val['status'].to_numpy()\ny_test = X_test['status'].to_numpy()\n#data.drop(['status'], axis = 1, inplace = True)","729abbf1":" y_train.mean(), y_test.mean()","207ef86e":"print('The training data length is:', len(y_train))\n#print('The validation data length is:', len(y_val))\nprint('The test data length is:', len(y_test))","5f02d567":"X_summary = X_train.describe()\nX_summary","59e54395":"max_outlier = ((X_summary.iloc[7,:] - X_summary.iloc[6,:]) \/ X_summary.iloc[2,:]).to_numpy()\nmin_outlier = ((X_summary.iloc[5,:] - X_summary.iloc[4,:]) \/ X_summary.iloc[2,:]).to_numpy()\noutliers = np.logical_or((max_outlier > 2), (min_outlier > 2))\n\npossible_outliers_indx = [i for i, x in enumerate(outliers) if x]\npossible_outliers = X_train.columns[possible_outliers_indx]\npossible_outliers","55b731e2":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# draw data\ndef plot_data(data, plot_type, grid_size, fig_size, y = None):\n    fig = plt.figure(figsize = fig_size)\n    column_names = data.select_dtypes(exclude='object').columns\n    for i, column_name in enumerate(column_names):\n        fig.add_subplot(grid_size[0], grid_size[1], i + 1)\n        if plot_type == 'hist':\n            plot = sns.histplot(data[column_name], kde = True, color = 'red')\n        elif plot_type == 'boxplot':\n             plot = sns.boxplot(y=data[column_name], x=y, color = 'red')\n        else:\n            raise ValueError(\"Input value for the parameter 'plot_type' should be 'hist' or 'boxplot'.\")\n        plot.set_xlabel(column_name, fontsize = 16)\n    plt.tight_layout()","86e3b8ac":"plot_data(X_train, plot_type = 'hist', grid_size = (8,3), fig_size = (12, 20))","cecc5427":"plot_data(X_train, y=y_train, plot_type = 'boxplot', grid_size = (8,3), fig_size = (12, 20))","2dd5cca2":"# previously found possible outliers\npossible_outliers","6a3f80a0":"correlation = X_train.corr()\nplt.figure(figsize = (20,10))\nsns.heatmap(correlation.loc[::-1,::-1])","f07e4f1a":"import seaborn as sns\n\nhigh_corr_shimmer = ['MDVP:Shimmer', 'MDVP:Shimmer(dB)', 'Shimmer:APQ3', \n                     'Shimmer:APQ5', 'MDVP:APQ', 'Shimmer:DDA']\n\nsns.pairplot(X_test[high_corr_shimmer])","87b34945":"high_corr_jitter = ['MDVP:Jitter(%)', 'MDVP:Jitter(Abs)', 'MDVP:RAP',\n                    'MDVP:PPQ', 'Jitter:DDP']\n\nsns.pairplot(X_test[high_corr_jitter])","68ae5c95":"status_corr = correlation.sort_values(by='status', ascending = False).iloc[:,-7]","42544d3f":"status_corr.to_frame().style.background_gradient(cmap='Reds')","b054d216":"def is_missing(col):\n    missing = sum(col.isna())\n    return missing\n\nX_train.apply(is_missing)","60924179":"# X_train = X_train.drop(['name', 'status'], axis = 1)\n\n# X_val = X_val.drop(['name', 'status'], axis = 1)\n\n# X_test = X_test.drop(['name', 'status'], axis = 1)","7a1a43cb":"X_train_lr = X_train.copy()\n#X_val_lr = X_val.copy()\nX_test_lr = X_test.copy()\n\nX_train_lr = X_train_lr.drop(['name', 'status'], axis = 1)\nX_test_lr = X_test_lr.drop(['name', 'status'], axis = 1)","0ed241ad":"# log-transform the data\n\nskewed_var = ['MDVP:Fo(Hz)', 'MDVP:Fhi(Hz)', 'MDVP:Flo(Hz)', \n              'MDVP:Jitter(%)', 'MDVP:Jitter(Abs)', 'MDVP:RAP', \n              'MDVP:PPQ', 'Jitter:DDP', 'MDVP:Shimmer', 'MDVP:Shimmer(dB)', \n              'Shimmer:APQ3', 'Shimmer:APQ5', 'MDVP:APQ', 'Shimmer:DDA', \n              'NHR', 'HNR', 'RPDE', 'DFA', 'PPE']\n\nX_train_lr[skewed_var] = np.log(X_train_lr[skewed_var])\n#X_val_lr[skewed_var] = np.log(X_val_lr[skewed_var])\nX_test_lr[skewed_var] = np.log(X_test_lr[skewed_var])","2db78960":"# drop the columns with high correlation\n# we will keep only 'MDVP:Jitter(Abs)', 'Shimmer:APQ5' since\n# they are the most correlated with the response out of others\n\ndrop_corr_columns = ['MDVP:Shimmer', 'MDVP:Shimmer(dB)', 'Shimmer:APQ3', \n                     'MDVP:APQ', 'Shimmer:DDA', 'MDVP:Jitter(%)', \n                     'MDVP:RAP', 'MDVP:PPQ', 'Jitter:DDP']\n\nX_train_lr.drop(drop_corr_columns, axis=1, inplace=True)\n#X_val_lr.drop(drop_corr_columns, axis=1, inplace=True)\nX_test_lr.drop(drop_corr_columns, axis=1, inplace=True)","79f83cc1":"# normalize the data\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_lr = scaler.fit_transform(X_train_lr)\nX_test_lr = scaler.transform(X_test_lr)","ad2365a5":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nclf = LogisticRegression()\nclf.fit(X_train_lr, y_train)\n#accuracy_score(clf.predict(X_val_lr), y_val)","9bea26ff":"accuracy_score(clf.predict(X_test_lr), y_test)","8b491675":"X_train_pls = X_train.copy()\n#X_val_pls = X_val.copy()\nX_test_pls = X_test.copy()\n\nX_train_pls = X_train_pls.drop(['name', 'status'], axis = 1)\nX_test_pls = X_test_pls.drop(['name', 'status'], axis = 1)","46c9048b":"from sklearn.model_selection import KFold\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.metrics import accuracy_score, log_loss\nimport math\n\ndef sigmoid(x):\n    return 1 \/ (1 + math.exp(-x))\nsigmoid = np.vectorize(sigmoid)\n\nkf = KFold(n_splits=10)\n\ncv_score = []\ncv_loss = []\nfor i in range(1, 16):\n    scores = []\n    losses = []\n    for train_index, test_index in kf.split(X_train_pls):\n        lm = PLSRegression(n_components=i)\n        lm.fit(X_train_pls.iloc[train_index,:], y_train[train_index])\n        y_pred_pls = lm.predict(X_train_pls.iloc[test_index,:])\n        y_pred = np.array([1 if y > 0.5 else 0 for y in y_pred_pls])\n        scores.append(accuracy_score(y_pred, y_train[test_index]))\n        losses.append(log_loss(y_train[test_index], sigmoid(y_pred_pls)))\n    cv_score.append(np.mean(scores))\n    cv_loss.append(np.mean(losses))","0f7fc010":"fig, ax = plt.subplots(1,2, figsize = (12,5))\nax[0].plot(cv_score)\nax[1].plot(cv_loss)\nax[0].set_xlabel('Number of components')\nax[0].set_ylabel('Cross-Validation accuracy')\nax[1].set_xlabel('Number of components')\nax[1].set_ylabel('Cross-Validation loss');","c1221bc7":"lm = PLSRegression(n_components=12)\nlm.fit(X_train_pls, y_train)\ny_pred_pls = lm.predict(X_test_pls)\ny_pred = np.array([1 if y > 0.5 else 0 for y in y_pred_pls])\naccuracy_score(y_pred, y_test)","38c2c0b9":"# seems that we need this only in decision tree\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val = train_test_split(X_train, train_size=0.7, random_state=13)","f7add9ba":"X_train_t = X_train.copy()\nX_val_t = X_val.copy()\nX_test_t = X_test.copy()\n\ny_train = X_train_t['status'].to_numpy()\ny_val = X_val_t['status'].to_numpy()\ny_test = X_test_t['status'].to_numpy()","ba8247e9":"# drop unnecessary columns\nX_train_t = X_train_t.drop(['name', 'status'], axis = 1)\nX_val_t = X_val_t.drop(['name', 'status'], axis = 1)\nX_test_t = X_test_t.drop(['name', 'status'], axis = 1)","32859c8c":"from xgboost import XGBClassifier\n\nmodel=XGBClassifier()\nmodel.fit(X_train_t, y_train)\ny_pred=model.predict(X_val_t)\n#print(accuracy_score(y_pred, y_train))\nprint(accuracy_score(y_pred, y_val))","95dba48f":"We will shortly check if it is the case by boxplots for each feature. However, at first let's normalize the features as suggested.","f4a8bf06":"So, let's deal with the outilers and construct PLS model.","6d38b5d6":"# Reading data","f3dd30b7":"## Logistic Regression","e01ebf81":"# Data exploration","8cf9a1a4":"In this kind of settings it is a good idea to use trees for classification since trees are indifferent towords outliers. Moreover, since the trees don't make any assumptions about the underlying data distribution they are not sensitive to skewed distributions. An additional bonus is that decision trees will chose only one of the correlated features at a time when deciding the split. Hence, the variables correlation should not have a big impact on the decision.","8a92587f":"-------------------------\nParkinson\u2019s Disease is a neurological disorder marked by decreased dopamine levels in the brain. It manifests itself through a deterioration of movement, including the presence of tremors and stiffness.\nThere is no definitive laboratory test to diagnose Parkinson\u2019s Disease. Hence, in the early stages of the disease is often complicated to detect it.","04dfbc4d":"## Decision Trees","b9d5071e":"## Partial Least Squares ","9cf221eb":"![](https:\/\/www.vanraam.com\/getmedia\/3078b293-12dd-4a5b-8bdf-028ee835bdcc\/Cycling-with-Parkinson-s-disease-with-Van-Raam-special-needs-bicycles.png)","3e7daf62":"# Model evaluation","55058834":"It seems to be the case that the classes are well seperated by the corresponding values distributions of the variables. So, in this cases the outliers do not contribute much on the determination of the class. Hence, we can either ignore them all change the encoding by grouping them.","2d506fac":"Given the peculiarities of the data:  \n- **Highly correlated variables**\n\n- **Presence of the outliers**\n\n- **The skewness of the variables**\n\n- **Number of the variables**\n\n- **Number of observations**\n\nThere are different strategies that can be used to deal with this data.  \n1. We can delete the correlated variables, delete the outliers, apply log-transformation on the variables to deal with the skewness and use logistic regression or a penalized version of logistic regression.\n\n2. We can delete or change the encoding of the outliers and apply Partial Least Squares regression.\n\n3. We can leave the data unchanged and apply a robust decision trees method.\n\nCertainly, it is not an exhaustive list of the strategies\/models that can be applied to the data however, we will constraint ourselves with these ones.","88df71e0":"Now we will explore the interrealtionship of the variables. Let's create a correlation matrix.","eea49035":"Notes for preprocessing:  \nSeveral variables are skewed and can be log transformed:  \n'MDVP:Fo(Hz)', 'MDVP:Fhi(Hz)', 'MDVP:Flo(Hz)', 'MDVP:Jitter(%)',\n       'MDVP:Jitter(Abs)', 'MDVP:RAP', 'MDVP:PPQ', 'Jitter:DDP',\n       'MDVP:Shimmer', 'MDVP:Shimmer(dB)', 'Shimmer:APQ3', 'Shimmer:APQ5',\n       'MDVP:APQ', 'Shimmer:DDA', 'NHR', 'HNR', 'RPDE', 'DFA', 'PPE'\n       \nRemarks: Though several variables appeared to be highly skewed it seems to be the case that the skewness can be avoided if we delete the corresponding variables outliers.","bad3696e":"The data contains $23$ different measures of different patients that are aimed to characterize the *'status'* of the Parkinson disease for each patient. The *'status'* variable is our binary response variable where $1$ stands for the the presence of the Parkinson disease and $0$ for the healthy patient. There are in total $195$ observations in our dataset.\n\n\n\nCitation Request:\n\n'Exploiting Nonlinear Recurrence and Fractal Scaling Properties for Voice Disorder Detection', Little MA, McSharry PE, Roberts SJ, Costello DAE, Moroz IM. BioMedical Engineering OnLine 2007, 6:23 (26 June 2007)\n","74ead06a":"What we see from the above summary is that there is an apparent scale differences between different features. Hence, it would be a good idea to normalize the variables.\n\nGiven the standard deviation, the $25\/75$ percent quartiles and min\/max values for each numerical variable  it is likely that the following features have outliers:","72acd8f6":"Partial Least Squares (PLS) does not make any assumptions about the underlying distribution of the features. Hence, the skewness should not be a big problem.  \nIt constructs derived variables by computing the weighted combination of the orignal inputs where the weights are given by the strength of their univariate effect of the response $y$. Then the variables are orthogonalized with respect to the derived input. Hence, the correlations between variables should not be a big problem.  \nThe process is continued untill $M$ derived features are constucted. Here $M$ is less than or equal than the original number of features. Hence, the number of variables is reduced too.","066a4bc4":"At this stage we are going to do some exploratory analysis. In particular, we will look at:\n- The distribution of each variable.\n- The presence of outliers in the data.\n- The distribution of each variable given the response.\n- Connection of the features with each other.\n- Connection of the features with the response.","194d2b00":"We see that they are variables that are moderately correlated with the response.","0f9889f6":"-------------------------\n**Dataset**: In this project we deal with Parkinson disease dataset.  \n**The main question of the project is**: Given all the health measermant does the patient have a Parkinson data or no?  \n**Methods and Findings**: We explored the data and found out a bunch of highly correlated variables, numerious variables with outliers and the ones that are skewed. In order to deal with all this peculiarities of the dataset we decided to use three approaches:\n1. Modify the data and apply logistic regression.\n2. Apply Partial Least Squares regression to reduce data dimension and deal with correlations.\n3. Apply decision trees algorithm.","ba3095fb":"Notes for preprocessing:  \nThere are outliers in:\n\n'MDVP:Fhi(Hz)', 'MDVP:Jitter(%)',\n       'MDVP:Jitter(Abs)', 'MDVP:RAP', 'MDVP:PPQ', 'Jitter:DDP',\n       'MDVP:Shimmer', 'MDVP:Shimmer(dB)', 'Shimmer:APQ3', 'Shimmer:APQ5',\n       'MDVP:APQ', 'Shimmer:DDA', 'NHR', 'HNR', 'spread1', 'D2', 'PPE'","fecd6738":"Check the for the missing values.","089a0388":"The plot shows that the varibales \n       'MDVP:Shimmer', 'MDVP:Shimmer(dB)', 'Shimmer:APQ3', 'Shimmer:APQ5',\n       'MDVP:APQ', 'Shimmer:DDA' are higly correlated.\n       \nAs well as: 'MDVP:Jitter(%)',\n       'MDVP:Jitter(Abs)', 'MDVP:RAP', 'MDVP:PPQ', 'Jitter:DDP'.\n       \nIt was expected since, as it is mentioned in the data description, they are different measures of the same feature."}}