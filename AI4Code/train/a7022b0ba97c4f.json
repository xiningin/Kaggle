{"cell_type":{"02d405ff":"code","d778e6fa":"code","2d57933e":"code","c63186ea":"code","5c01a296":"code","e73f6dba":"code","4584dde1":"code","e7d1e97e":"code","1947e566":"code","e08b5fe5":"code","ee5510e8":"markdown","bcb8e76c":"markdown"},"source":{"02d405ff":"# Project for the exam of Signal Processing and Optimization for Big Data\n# Master degree in computer engineering, curriculum Data Science\n# Matteo Rinalduzzi - Universit\u00e0 degli Studi di Perugia\n\n\n# This project aims to investigate 3 aspects:\n# - PART 1: implementation of SVM centralized version to evaluate the effect of L1 norm as lambda changes\n# - PART 2: comparison between centralized version and distributed version with ADMM, splitting\n#           across data (framework of CONSENSUS OPTIMIZATION)\n# - PART 3: distributed algorithm applied to a simple dataset of quasi linearly separable data in two dimensions\n#           and plot of the straight lines in the plane tha separate the data (multiclass classification)\n\n\n# ------ PART 1: Norm L1 effect -------\nimport numpy as np\nnp.random.seed(1)\n\n# Data generation\nn = 20 #n. of features\nm = 1000 #n. of examples\nbeta_true = np.random.randn(n,1) # hyperplane coefficients (beta)\noffset = np.random.randn(1) # intercept (beta_0)\n\n# I generate a set of linearly separable data from the hyperplane identified by (beta_true, offset)\nX = np.random.normal(0, 5, size=(m,n))\nY = np.sign(X.dot(beta_true) + offset)\n","d778e6fa":"# Setting of the centralized problem\nimport cvxpy as cp\nbeta = cp.Variable((n,1))\nv = cp.Variable()\nloss = cp.sum(cp.pos(1 - cp.multiply(Y, X @ beta + v)))\nreg = cp.norm(beta, 1)\nlambd = cp.Parameter(nonneg=True)\nprob = cp.Problem(cp.Minimize(loss\/m + lambd*reg))","2d57933e":"# Problem solved for different values of lambda\nTRIALS = 100\nlambda_vals = np.logspace(-2, 0, TRIALS) # lambda values from 0.01 to 1\n#lambda_vals = np.linspace(0.01, 1, TRIALS) # from 0.01 to 1\nbeta_vals = []\nfor i in range(TRIALS):\n    lambd.value = lambda_vals[i]\n    prob.solve()\n    beta_vals.append(beta.value)","c63186ea":"# Plot beta trend as a function of lambda, normalized by the smallest coefficient\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\nbeta_vals_norm = np.copy(beta_vals)\nfor i in range(len(beta_vals)):\n    if np.max(beta_vals[i]) > abs(np.min(beta_vals[i])):\n        beta_vals_norm[i] = beta_vals[i]\/np.max(beta_vals[i])\n    else:\n        beta_vals_norm[i] = beta_vals[i]\/abs(np.min(beta_vals[i]))\n\nfor i in range(n):\n    plt.plot(lambda_vals, [wi[i,0] for wi in beta_vals_norm])\nplt.ylabel(r\"$\\beta$\", fontsize=16).set_rotation(0)\nplt.xlabel(r\"$\\lambda$\", fontsize=16)\nplt.xscale(\"log\")\n\n","5c01a296":"# -------- PART 2: Comparison between centralized code and distributed code ------\nimport numpy as np\nimport math\nnp.random.seed(1)\n\nn = 20\nm = 1000\nbeta_true = np.random.randn(n,1)\noffset = np.random.randn(1)\nX = np.random.normal(0, 5, size=(m,n))\nY = np.sign(X.dot(beta_true) + offset)\n\nbeta = np.append(beta_true, offset).reshape(21)\n#print('True', np.around(beta,6))\n\n# Centralized algorithm\nimport cvxpy as cp\nA = np.hstack((X*Y,Y))\nn_features = n + 1\n# Parameters\nrho = 1\nlamda = 0.5\nC = np.identity(n_features)\nC[n_features-1,n_features-1] = 0\n\nbeta = cp.Variable((n_features,1))\nloss = cp.sum(cp.pos(1 - A @ beta ))\nreg = cp.norm(C@beta, 1)\nprob = cp.Problem(cp.Minimize(loss\/m + lamda*reg))\n\n# Solving\nprob.solve()\nvar = beta.value\nvar = var.reshape((var.shape[0],))\nprint('Centralized solution')\nprint(np.around(var,6))\n#print(np.around(var\/np.linalg.norm(var),6))\n","e73f6dba":"# Distributed algorithm\nN = 20 # n. of agentss, every agents has a small piece of the dataset\nn_iter = 500 # n. of iterations\nn_samples = math.floor(A.shape[0] \/ N) # dataset division betweeen the N agents\n\nX = np.zeros((n_iter, N, n_features))  # X[k,i,:] is the vector x_i at the iteration k\nZ = np.zeros((n_iter, n_features))     # Z[k,:] is the vector z at the iteration k\nU = np.zeros((n_iter, N, n_features))  # U[k,i,:] is the vector u_i at the iteration k\nLOSS_1 = np.zeros((n_iter, N))         # vector that holds the trend of the loss function of the step 1 for the N agents\n\nfor k in range(0,n_iter-1,1): #start from k+1=1 and not from 0    \n    #Step 1\n    count = 0\n    for i in range(N):        \n        x_cp = cp.Variable(n_features)\n        loss = cp.sum(cp.pos(np.ones(n_samples) - A[count:count+n_samples,:] @ x_cp))\n        reg = cp.sum_squares(x_cp - Z[k,:] + U[k,i,:])\n        aug_lagr = loss\/m + (rho\/2)*reg\n        prob = cp.Problem(cp.Minimize(aug_lagr))\n        prob.solve(solver=cp.ECOS) #verbose=True, adaptive_rho = False, \n        X[k+1,i,:] = x_cp.value\n        # LOSS computation\n        for j in range(n_samples):\n            cost = 1 - np.inner(A[count+j,:], X[k+1,i,:])\n            if cost >0:\n                LOSS_1[k+1,i] += cost\n        LOSS_1[k+1,i] += rho\/2 * np.linalg.norm(X[k+1,i,:] - Z[k,:] + U[k,i,:])**2\n        \n        count += n_samples\n    \n    \n    #Step 2\n    mean_X = np.zeros(n_features)\n    mean_U = np.zeros(n_features)\n    for i in range(N):\n        mean_X += X[k+1,i,:]\n        mean_U += U[k,i,:]\n    mean_X = 1\/N * mean_X\n    mean_U = 1\/N * mean_U\n    \n    for i in range(n_features-1):\n        if mean_X[i] + mean_U[i] > lamda\/(N*rho):\n            Z[k+1,i] = mean_X[i] + mean_U[i] - lamda\/(N*rho)\n        elif mean_X[i] + mean_U[i] < - lamda\/(N*rho):\n            Z[k+1,i] = mean_X[i] + mean_U[i] + lamda\/(N*rho)\n        else:\n            Z[k+1,i] = 0\n    Z[k+1,n_features-1] = mean_X[n_features-1] + mean_U[n_features-1] #l'ultima \u00e8 un caso particolare\n    \n    \n    #Step 3\n    for i in range(N):\n        U[k+1,i,:] = U[k,i,:] + X[k+1,i,:] - Z[k+1,:] \n\n        \nprint('Distributed solution')\nprint(Z[n_iter-1,:])","4584dde1":"# Plot of the LOSS of step 1 for one of the N agents \nimport matplotlib.pyplot as plt\nplt.plot(np.linspace(0,n_iter,n_iter), LOSS_1[:,0])\nplt.ylabel(\"LOSS\", fontsize=16)\nplt.xlabel(\"n\u00b0 iterations\", fontsize=16)\nplt.title(\"Loss trend\")\nplt.show()\n\nplt.plot(np.linspace(50,n_iter,450), LOSS_1[50:500,0])\nplt.ylabel(\"LOSS\", fontsize=16)\nplt.xlabel(\"n\u00b0 iterations\", fontsize=16)\nplt.title(\"Zoom loss trend\")\nplt.show()","e7d1e97e":"# ------- PART 3: Real dataset -------- \nimport numpy as np\nimport pandas as pd\nimport math\nimport cvxpy as cp\n\ndf2 = pd.read_csv(\"..\/input\/wall-following-robot\/sensor_readings_2.csv\")\n\ndf2.columns = ['SD_front', 'SD_left', 'Label']\nclass_names = ['Move-Forward', 'Slight-Right-Turn', 'Sharp-Right-Turn', 'Slight-Left-Turn']\noutput_dictionary = {'Move-Forward': 1, 'Slight-Right-Turn': 2, 'Sharp-Right-Turn': 3, 'Slight-Left-Turn': 4}\n\ndf = df2\n\nx1 = df['SD_front'].to_numpy() # first feature\nx2 = df['SD_left'].to_numpy() # second feature\ny  = df['Label'].replace(output_dictionary).to_numpy()  #class\n\n# Train-test split (80% - 20%)\ntrain_samples = np.int(np.around(x1.shape[0]*0.8))\nx1_train = x1[0:train_samples]\nx2_train = x2[0:train_samples]\ny_train = y[0:train_samples]\n\nx1_test = x1[train_samples:y.size]\nx2_test = x2[train_samples:y.size]\ny_test = y[train_samples:y.size]\n\nn_iter = 500\n\n\n#Split across data with L1 regularization\ndef svm(classe1,classe2):\n    A = []\n    n_rows_tot = y_train.size\n    n_features = 2 + 1\n    for i in range(n_rows_tot):\n        if y_train[i] == classe1:\n            ai_t = 1 * np.array([x1_train[i], x2_train[i], 1])\n            A.append(ai_t)\n        elif y_train[i] == classe2:\n            ai_t = -1 * np.array([x1_train[i], x2_train[i], 1])\n            A.append(ai_t)\n    A = np.array(A)\n        \n    N = 5 # (<-> n. of agents)\n    m = A[:,0].size\n    n_samples = math.floor(m \/ N)\n    rho = 1\n    lamda = 0.1\n\n    X = np.zeros((n_iter, N, n_features))\n    Z = np.zeros((n_iter, n_features))\n    U = np.zeros((n_iter, N, n_features))\n    LOSS_1 = np.zeros(n_iter)\n\n    mean_AX = np.zeros(n_samples)\n\n    for k in range(0,n_iter-1,1):   \n        #Step 1\n        count = 0\n        for i in range(N):        \n            x_cp = cp.Variable(n_features)\n            loss = cp.sum(cp.pos(np.ones(n_samples) - A[count:count+n_samples,:] @ x_cp))\n            reg = cp.sum_squares(x_cp - Z[k,:] + U[k,i,:])\n            aug_lagr = loss + (rho\/2)*reg\n            prob = cp.Problem(cp.Minimize(aug_lagr))\n            prob.solve(solver=cp.ECOS)#verbose=True, adaptive_rho = False, \n            X[k+1,i,:] = x_cp.value\n\n            #LOSS\n            for j in range(n_samples):\n                cost = 1 - np.inner(A[count+j,:], X[k+1,i,:])\n                if cost >0:\n                    LOSS_1[k+1] += cost\n                LOSS_1[k+1] += rho\/2 * np.linalg.norm(X[k+1,i,:] - Z[k,:] + U[k,i,:])**2\n        \n            count += n_samples\n    \n    \n        #Step 2\n        mean_X = np.zeros(n_features)\n        mean_U = np.zeros(n_features)\n        for i in range(N):\n            mean_X += X[k+1,i,:]\n            mean_U += U[k,i,:]\n        mean_X = 1\/N * mean_X\n        mean_U = 1\/N * mean_U\n    \n        for i in range(n_features-1):\n            if mean_X[i] + mean_U[i] > lamda\/(N*rho):\n                Z[k+1,i] = mean_X[i] + mean_U[i] - lamda\/(N*rho)\n            elif mean_X[i] + mean_U[i] < - lamda\/(N*rho):\n                Z[k+1,i] = mean_X[i] + mean_U[i] + lamda\/(N*rho)\n            else:\n                Z[k+1,i] = 0\n        Z[k+1,n_features-1] = mean_X[n_features-1] + mean_U[n_features-1]\n    \n    \n        #Step 3\n        for i in range(N):\n            U[k+1,i,:] = U[k,i,:] + X[k+1,i,:] - Z[k+1,:] \n\n        \n    print(Z[n_iter-1,:])\n\n    return Z[n_iter-1,:];\n\ndef plot_train(beta_tilde,classe1,classe2,color1,color2):\n    a = beta_tilde[0]\n    b = beta_tilde[1]\n    c = beta_tilde[2]\n    print('-a\/b',-a\/b, '   -c\/b',-c\/b)\n\n    ics = np.linspace(0,5,100)\n    ipsilon = -a\/b*ics - c\/b\n    e = []\n    r = []\n    t = []\n    p = []\n    for i in range(x1_train.size):\n        if y_train[i] == classe1:\n            e.append(x1_train[i])\n            r.append(x2_train[i])\n        elif y_train[i] == classe2:\n            t.append(x1_train[i])\n            p.append(x2_train[i])\n    \n    plt.plot(ics, ipsilon, '-k')\n    plt.plot(e, r, color1, marker='o', linestyle=\"\")\n    plt.plot(t, p, color2, marker='o', linestyle=\"\")\n","1947e566":"# ----------   TRAIN   --------------\nbeta_tilde_1 = svm(1,2)\nbeta_tilde_2 = svm(1,3)\nbeta_tilde_3 = svm(1,4)\nbeta_tilde_4 = svm(2,3)\nbeta_tilde_5 = svm(2,4)\nbeta_tilde_6 = svm(3,4)\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (12, 6))\nplt.title('Plot of the lines associated with classifiers', fontsize=30)\nplt.xlabel('x', fontsize=30)\nplt.ylabel('y', fontsize=30).set_rotation(0)\nplt.xlim(0.5, 3.5) \nplt.ylim(0, 1.5)\n\nplot_train(beta_tilde_1,1,2,'-r','-g')\nplot_train(beta_tilde_2,1,3,'-r','-b')\nplot_train(beta_tilde_3,1,4,'-r','-m')\nplot_train(beta_tilde_4,2,3,'-g','-b')\nplot_train(beta_tilde_5,2,4,'-g','-m')\nplot_train(beta_tilde_6,3,4,'-b','-m')\n\n\n#plt.legend(loc='upper right', fontsize=20)\nplt.grid()\nplt.show()","e08b5fe5":"# ----------   TEST   ---------------\nfrom sklearn.metrics import confusion_matrix\n# Plot confusion matrix for multiclass classification\ndef plot_confusion_matrix(y_test, y_pred, title, normalize=False):\n    cm = confusion_matrix(y_test, y_pred)\n    print(cm) # Confusion Matrix NOT normalized\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    fig, ax = plt.subplots(figsize=(10, 10))\n    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    ax.figure.colorbar(im, ax=ax)\n\n    classes = ['M-F', 'Sl-R-T', 'Sh-R-T', 'Sl-L-T']\n    #classes = ['1', '2', '3', '4']\n    plt.xticks(np.arange(cm.shape[1]), classes)\n    plt.yticks(np.arange(cm.shape[0]), classes)\n    ax.set(\n        xticklabels=classes, yticklabels=classes,\n        title=title,\n        ylabel='True label',\n        xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n    plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    \n    \ny_pred = np.zeros(y_test.size,dtype='int')    \nfor i in range(y_test.size):\n    pred_count = np.zeros(7) # the ith cell i identifies the ith classifier\n    #Classifier 1\n    a = beta_tilde_1[0]\n    b = beta_tilde_1[1]\n    c = beta_tilde_1[2]\n    if x1_test[i]*a + x2_test[i]*b + c > 0: # scalar product\n        pred_count[1] += 1\n    else:\n        pred_count[2] += 1\n        \n    #Classifier 2\n    a = beta_tilde_2[0]\n    b = beta_tilde_2[1]\n    c = beta_tilde_2[2]\n    if x1_test[i]*a + x2_test[i]*b + c > 0:\n        pred_count[1] += 1\n    else:\n        pred_count[3] += 1\n        \n    #Classifier 3\n    a = beta_tilde_3[0]\n    b = beta_tilde_3[1]\n    c = beta_tilde_3[2]\n    if x1_test[i]*a + x2_test[i]*b + c > 0:\n        pred_count[1] += 1\n    else:\n        pred_count[4] += 1\n        \n    #Classifier 4\n    a = beta_tilde_4[0]\n    b = beta_tilde_4[1]\n    c = beta_tilde_4[2]\n    if x1_test[i]*a + x2_test[i]*b + c > 0:\n        pred_count[2] += 1\n    else:\n        pred_count[3] += 1\n        \n    #Classifier 5\n    a = beta_tilde_5[0]\n    b = beta_tilde_5[1]\n    c = beta_tilde_5[2]\n    if x1_test[i]*a + x2_test[i]*b + c > 0:\n        pred_count[2] += 1\n    else:\n        pred_count[4] += 1\n        \n    #Classifier 6\n    a = beta_tilde_6[0]\n    b = beta_tilde_6[1]\n    c = beta_tilde_6[2]\n    if x1_test[i]*a + x2_test[i]*b + c > 0:\n        pred_count[3] += 1\n    else:\n        pred_count[4] += 1\n    \n    #print(pred_count)\n    #print(np.argmax(pred_count))\n    y_pred[i] = np.argmax(pred_count) # majority (the index with more count is the class most likely)\n    \nplot_confusion_matrix(y_test, y_pred, 'Confusion matrix', normalize=False)","ee5510e8":"**Comparison between centralized code and distributed code**","bcb8e76c":"**Test of distributed algorithm with a real dataset**"}}