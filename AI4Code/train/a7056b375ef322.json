{"cell_type":{"56078d8f":"code","7820bfd1":"code","49517f98":"code","518b3ed6":"code","abff4907":"code","421e88e9":"code","d55fe1c2":"code","586e6c0d":"code","a09c0dd9":"code","aa99f404":"code","53e94027":"code","73301f04":"markdown","4e8ce1ce":"markdown"},"source":{"56078d8f":"# To be able to use fastai with GPU\n# see here: https:\/\/www.kaggle.com\/product-feedback\/279990\n!pip install -q --user torch==1.9.0 torchvision==0.10.0 torchaudio==0.9.0 torchtext==0.10.0","7820bfd1":"!pip install -q --user unpackai\n!pip install -q --user transformers==4.11.3","49517f98":"\nfrom pathlib import Path\n\nimport torch\nfrom torch import nn\n\nfrom transformers import EncoderDecoderModel\n\nfrom unpackai.utils import ls, download","518b3ed6":"# SPECIFIC TO GPU ACCELERATION ... maybe include in a try\/except block\ntry:\n    torch.cuda.set_device(\"cuda:0\")\nexcept Exception as e:\n    print(f\"Error while trying to enable GPU accelaration: {e}\")\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Running on device: {}\".format(device))","abff4907":"class Caption(nn.Module):\n    \"\"\"\n    The caption model is created by\n        combining decoder and encoder\n    \"\"\"\n    def __init__(self, encoder: nn.Module, decoder: nn.Module):\n        \"\"\"\n        \n        - encoder: The encoder model that can extract image features\n        - decoder: The decoder model that can generate text sequence\n            - you have to set add_cross_attention\n                to True when instantiate the model\n        \"\"\"\n        super().__init__()\n        self.encoder_decoder = EncoderDecoderModel(\n            encoder=encoder,\n            decoder=decoder,)\n        \n        # update generate documentation\n        self.generate.__func__.__doc__ = f\"\"\"\n        Generate text with image:\n        - batch_img: a batch of image tensor\n        - other generate kwargs please see following\n        {self.encoder_decoder.decoder.generate.__doc__}\"\"\"\n        \n    def forward(self, inputs):\n        x, input_ids = inputs\n        # extract image feature with encoder\n        # the extracted feature we call them: encoder_outputs\n        encoder_outputs = self.encoder_decoder.encoder(x)\n        \n        # predict text sequence logits\n        # with the encoder_outputs\n        seq_out = self.encoder_decoder(\n            encoder_outputs=encoder_outputs,\n            # decoder_inputs is to help decoders learn better, \n            # decoder has mask that allow model to see\n            # only the previous text tokens and encoder feature\n            decoder_input_ids=input_ids,\n            labels=input_ids,\n            )\n        return seq_out\n    \n    def generate(self, batch_img, **generate_kwargs):\n        with torch.no_grad():\n            # extract image features first\n            encoder_outputs = self.encoder_decoder.encoder(pixel_values=batch_img)\n            return self.encoder_decoder.decoder.tokenizer.batch_decode(\n                # encoder_decoder has a 'generate' function we can use\n                self.encoder_decoder.generate(encoder_outputs=encoder_outputs, **generate_kwargs)\n            )","421e88e9":"import pickle\n\ndef load_pickle(path):\n    with open(path, 'rb') as f:\n        return pickle.load(f)","d55fe1c2":"# encoder_decoder = EncoderDecoderModel.from_pretrained(\"model\")\nencoder_decoder = load_pickle(\"..\/input\/image-captioning\/encoder_decoder.pkl\")","586e6c0d":"loaded_caption = Caption(encoder=encoder_decoder.encoder, decoder=encoder_decoder.decoder)","a09c0dd9":"from torchvision import transforms as tfm\nfrom PIL import Image\n\nINPUT_MEAN = [.5,.5,.5]\nINPUT_STD = [.5,.5,.5]\n\ndef image_2_caption(img_path, caption:Caption) -> str:\n    \"\"\"Get caption from an image (need \"model\" and \"tokenizer\")\"\"\"\n    img = Image.open(img_path)\n    image_to_batch = tfm.Compose([\n        tfm.Resize((224,224)),\n        tfm.ToTensor(),\n        tfm.Normalize(INPUT_MEAN, INPUT_STD)\n    ])\n\n    batch = image_to_batch(img)[None, :].to(device)\n    vocab = caption.encoder_decoder.decoder.tokenizer.vocab\n   \n    return caption.generate(\n        batch,\n        top_p=0.6,\n        do_sample=True,  # text generation startegy\n        bos_token_id=vocab[\":\"],\n    )[0].replace(\"<|endoftext|>\", \"\")","aa99f404":"import matplotlib.pyplot as plt\n\nurl = \"https:\/\/i.pinimg.com\/originals\/df\/9e\/b5\/df9eb52fadfccef7846c84e3356581a4.jpg\"\nimg_path = download(url, \"some_picture.jpg\")\ncaption = image_2_caption(img_path, loaded_caption)\n\nprint()\nplt.imshow(Image.open(img_path))\nplt.title(caption)\nplt.show()","53e94027":"loaded_caption.encoder_decoder.save_pretrained(\"en-de-coder\")","73301f04":"# Inference from pickle","4e8ce1ce":"# From pre-trained model"}}