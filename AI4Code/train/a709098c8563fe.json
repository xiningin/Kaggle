{"cell_type":{"5e2b637c":"code","9c694bc0":"code","53ed1512":"code","15765ae0":"code","f26aa8de":"code","ce0122de":"code","3d591487":"code","f31e1957":"code","a238dc16":"code","19601ee7":"code","1291fe92":"code","479fa5b4":"code","731cd17b":"code","47c88a22":"code","572136e4":"code","754b95a2":"code","8ce66296":"code","ce9d137e":"code","c29db653":"code","29d3bf9a":"code","eabe3857":"code","8796a640":"code","14bd36ab":"code","62a7a9e8":"code","a8ed5250":"code","65847210":"code","73846f67":"code","bb4238af":"code","0946962c":"code","719fb43d":"code","03a9b909":"code","4164c311":"code","aed5af74":"code","832dad77":"code","0f5f63d8":"code","28816fd6":"code","e8268aa7":"code","7502ca05":"code","fd43ea44":"code","47aae7ee":"code","09c2c658":"code","92575e74":"code","0d6c1e11":"code","1be1b828":"code","f7f93c69":"code","7371bea2":"code","6f5641b4":"code","166547dd":"code","dcca352f":"code","33515fc0":"code","9dba7a2a":"code","cc1b462a":"code","dc1f23a5":"code","35a2f68a":"code","9d9715c3":"code","8e19b1d4":"code","f51e7427":"code","61abfa9b":"code","490db04b":"code","3c83b027":"markdown","e9f16fe9":"markdown","0a7bdb33":"markdown","6f60e574":"markdown","9cc3cc75":"markdown","922e545a":"markdown","f86d5306":"markdown","d79ff9f8":"markdown","a9bfd4f5":"markdown","3413d8d8":"markdown","90f54b24":"markdown"},"source":{"5e2b637c":"# Importing Libraries\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Data Preprocessing\nfrom sklearn.preprocessing import StandardScaler as ss\n\n# Dimensionality Reduction\nfrom sklearn.decomposition import PCA\n\n# Data Splitting and model parameter search\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# For modelling\nfrom xgboost.sklearn import XGBClassifier\n\n# For model pipelining\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\n\n# For model evaluation\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import auc, roc_curve\nfrom sklearn.metrics import confusion_matrix\n\n# For plotting\nimport matplotlib.pyplot as plt\nfrom xgboost import plot_importance\nimport seaborn as sns\n\n# For Bayes Optimization\nfrom sklearn.model_selection import cross_val_score\n\nfrom bayes_opt import BayesianOptimization\n\n# For finding feature importance\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\n# Miscellaneous\nimport time\nimport os\nimport gc\nimport random\nfrom scipy.stats import uniform","9c694bc0":"# Set option to dislay many rows\npd.set_option('display.max_columns', 100)","53ed1512":"# set file directory\nos.chdir('..\/input\/')","15765ae0":"# Data has 6497 rows \ntr_f = \"winequalityN.csv\"\n\n# Total number of lines and lines to read:\ntotal_lines = 6497\nnum_lines = 6487\n\n# Read randomly 'p' fraction of files\np = num_lines\/total_lines  # fraction of lines to read (99% approximately)\np","f26aa8de":"# Pick up random rows from hard-disk\ndata = pd.read_csv(\n         tr_f,\n         header=0,   \n         skiprows=lambda i: (i>0) and (random.random() > p)\n         )","ce0122de":"# Explore data\ndata.shape","3d591487":"data.info()","f31e1957":"data.type.value_counts()","a238dc16":"# Check for null values\ndata.isnull().sum()","19601ee7":"# Deleting the rows with null values\ndata.dropna(axis=0, inplace=True)","1291fe92":"data.shape","479fa5b4":"data.head(3)","731cd17b":"data.describe()","47c88a22":"data.corr()","572136e4":"sns.countplot(x = data.quality, data=data, hue='type', palette=\"rocket\")","754b95a2":"sns.set(style=\"ticks\")\ndef hide_current_axis(*args, **kwds):\n    plt.gca().set_visible(False)\n\np = sns.pairplot(data, vars = ['fixed acidity','free sulfur dioxide', 'total sulfur dioxide', 'volatile acidity', 'residual sugar','chlorides','density','citric acid'], diag_kind = 'kde', \n             hue='type',\n             height = 4,\n             palette=\"rocket\")\np.map_upper(hide_current_axis)","8ce66296":"plt.figure(figsize=(14,14))\nsns.heatmap(data.iloc[:,0:13].corr(), cbar = True,  square = True, annot=True, cmap= 'BuGn_r')\n\n#Free sulfur dioxide and total sulfar dioxide, and Density and alcohol are the most correlated features","ce9d137e":"fig = plt.figure(figsize=(22,10))\nfeatures = [\"total sulfur dioxide\", \"residual sugar\", \"volatile acidity\", \"total sulfur dioxide\", \"chlorides\", \"fixed acidity\", \"density\",\"sulphates\"]\n\nfor i in range(8):\n    ax1 = fig.add_subplot(2,4,i+1)\n    sns.boxplot(x=\"type\", y=features[i],data=data, palette=\"rocket\");\n    \n# Fixed Acidity: acid that contributes to the conservation of wine.\n# Volatile Acidity: Amount of acetic acid in wine at high levels can lead to an unpleasant taste of vinegar.\n# Citric Acid: found in small amounts, can add \u201cfreshness\u201d and flavor to wines.\n# Residual sugar: amount of sugar remaining after the end of the fermentation.\n# Chlorides: amount of salt in wine.\n# Free Sulfur Dioxide: it prevents the increase of microbes and the oxidation of the wine.\n# Total Sulfur Dioxide: it shows the aroma and taste of the wine.\n# Density: density of water, depends on the percentage of alcohol and amount of sugar.\n# pH: describes how acid or basic a wine is on a scale of 0 to 14.\n# Sulfates: additive that acts as antimocrobian and antioxidant.\n# Alcohol: percentage of alcohol present in the wine.","c29db653":"fig = plt.figure(figsize=(24,10))\nfeatures = [\"total sulfur dioxide\", \"residual sugar\", \"volatile acidity\", \"total sulfur dioxide\", \"chlorides\", \"fixed acidity\", \"citric acid\",\"sulphates\"]\n\nfor i in range(8):\n    ax1 = fig.add_subplot(2,4,i+1)\n    sns.barplot(x='quality', y=features[i],data=data, hue='type', palette='rocket')\n    ","29d3bf9a":"# Divide data into predictors and target\nX = data.iloc[ :, 1:13]\nX.head(2)","eabe3857":"# 1st index or 1st column is target\ny = data.iloc[ : , 0]\ny.head()","8796a640":"#  Transform type data to '1' and '0'\ny = y.map({'white':1, 'red' : 0})\ny.dtype           # int64","14bd36ab":"# Store column names somewhere for use in feature importance\ncolnames = X.columns.tolist()\n\ncolnames","62a7a9e8":"# Split dataset into train and validation parts\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.30,\n                                                    shuffle = True\n                                                    )\n","a8ed5250":"X_train.shape  ","65847210":"#### Create pipeline ####\n#### Pipe using XGBoost\n\nsteps_xg = [('sts', ss() ),\n            ('pca', PCA()),\n            ('xg',  XGBClassifier(silent = False,\n                                  n_jobs=2)        # Specify other parameters here\n            )\n            ]\n","73846f67":"# Instantiate Pipeline object\npipe_xg = Pipeline(steps_xg)","bb4238af":"##################### Grid Search #################\n\n#   Specify xgboost parameter-range\n#   Dictionary of parameters (16 combinations)\n#     Syntax: {\n#              'transformerName_parameterName' : [ <listOfValues> ]\n#              }\n\n\nparameters = {'xg__learning_rate':  [0.3, 0.05],\n              'xg__n_estimators':   [50,  100],\n              'xg__max_depth':      [3,5],\n              'pca__n_components' : [5,7]\n              }                               # Total: 2 * 2 * 2 * 2","0946962c":"#    Grid Search (16 * 2) iterations\n#    Create Grid Search object first with all necessary\n#    specifications. Note that data, X, as yet is not specified\nclf = GridSearchCV(pipe_xg,            # pipeline object\n                   parameters,         # possible parameters\n                   n_jobs = 2,         # USe parallel cpu threads\n                   cv =5 ,             # No of folds\n                   verbose =2,         # Higher the value, more the verbosity\n                   scoring = ['accuracy', 'roc_auc'],  # Metrics for performance\n                   refit = 'roc_auc'   # Refitting final model on what parameters?\n                                       # Those which maximise auc\n                   )","719fb43d":"# Start fitting data to pipeline\nstart = time.time()\nclf.fit(X_train, y_train)\nend = time.time()\n(end - start)\/60","03a9b909":"f\"Best score: {clf.best_score_} \"","4164c311":"f\"Best parameter set {clf.best_params_}\"","aed5af74":"y_pred_gs = clf.predict(X_test)\n# Accuracy\naccuracy_gs = accuracy_score(y_test, y_pred_gs)\nf\"Accuracy: {accuracy_gs * 100.0}\"","832dad77":"#  Find feature importance of any BLACK Box model\n\n# Instantiate the importance object\nperm = PermutationImportance(\n                            clf,\n                            random_state=1\n                            )\n\n# fit data & learn\nstart = time.time()\nperm.fit(X_test, y_test)\nend = time.time()\n(end - start)\/60","0f5f63d8":"#  Conclude: Get feature weights\n\neli5.show_weights(\n                  perm,\n                  feature_names = colnames      # X_test.columns.tolist()\n                  )\n","28816fd6":"fw = eli5.explain_weights_df(\n                  perm,\n                  feature_names = colnames      # X_test.columns.tolist()\n                  )\n\n# Print importance\nfw","e8268aa7":"#####################  Randomized Search #################\n\n# Tune parameters using randomized search\n# Hyperparameters to tune and their ranges\nparameters = {'xg__learning_rate':  uniform(0, 1),\n              'xg__n_estimators':   range(50,100),\n              'xg__max_depth':      range(3,5),\n              'pca__n_components' : range(5,7)}","7502ca05":"#     Tune parameters using random search\n#     Create the object first\nrs = RandomizedSearchCV(pipe_xg,\n                        param_distributions=parameters,\n                        scoring= ['roc_auc', 'accuracy'],\n                        n_iter=15,          # Max combination of\n                                            # parameter to try. Default = 10\n                        verbose = 3,\n                        refit = 'roc_auc',\n                        n_jobs = 2,          # Use parallel cpu threads\n                        cv = 2               # No of folds.\n                                             # So n_iter * cv combinations\n                        )","fd43ea44":"# Run random search for 25 iterations. 21 minutes\nstart = time.time()\nrs.fit(X_train, y_train)\nend = time.time()\n(end - start)\/60","47aae7ee":"# Evaluate\nf\"Best score: {rs.best_score_} \"\n","09c2c658":"f\"Best parameter set: {rs.best_params_} \"","92575e74":"# Make predictions\ny_pred_rs = rs.predict(X_test)","0d6c1e11":"# Accuracy\naccuracy_rs = accuracy_score(y_test, y_pred_rs)\nf\"Accuracy: {accuracy_rs * 100.0}\"","1be1b828":"###############  Tuning using Bayes Optimization ############\n# Which parameters to consider and what is each one's range\npara_set = {\n           'learning_rate':  (0, 1),                 \n           'n_estimators':   (50,100),               \n           'max_depth':      (3,5),                 \n           'n_components' :  (5,7)          \n            }","f7f93c69":"#    Create a function that when passed some parameters\n#    evaluates results using cross-validation\n#    This function is used by BayesianOptimization() object\n\ndef xg_eval(learning_rate,n_estimators, max_depth,n_components):\n    #  Make pipeline. Pass parameters directly here\n    pipe_xg1 = make_pipeline (ss(),                        # Why repeat this here for each evaluation?\n                              PCA(n_components=int(round(n_components))),\n                              XGBClassifier(\n                                           silent = False,\n                                           n_jobs=2,\n                                           learning_rate=learning_rate,\n                                           max_depth=int(round(max_depth)),\n                                           n_estimators=int(round(n_estimators))\n                                           )\n                             )\n\n    # Now fit the pipeline and evaluate\n    cv_result = cross_val_score(estimator = pipe_xg1,\n                                X= X_train,\n                                y = y_train,\n                                cv = 2,\n                                n_jobs = 2,\n                                scoring = 'f1'\n                                ).mean()             # take the average of all results\n\n\n    #  Finally return maximum\/average value of result\n    return cv_result","7371bea2":"#      Instantiate BayesianOptimization() object\n#      This object  can be considered as performing an internal-loop\n#      i)  Given parameters, xg_eval() evaluates performance\n#      ii) Based on the performance, set of parameters are selected\n#          from para_set and fed back to xg_eval()\n#      (i) and (ii) are repeated for given number of iterations\n#\nxgBO = BayesianOptimization(\n                             xg_eval,     # Function to evaluate performance.\n                             para_set     # Parameter set from where parameters will be selected\n                             )","6f5641b4":"#     Gaussian process parameters\n#     Modulate intelligence of Bayesian Optimization process\n#     This parameters controls how much noise the GP can handle,\n#     so increase it whenever you think that extra flexibility is needed.\ngp_params = {\"alpha\": 1e-5}      # Initialization parameter for gaussian\n                                 # process.","166547dd":"#  Fit\/train (so-to-say) the BayesianOptimization() object\n#     Start optimization. 25minutes\n#     Our objective is to maximize performance (results)\nstart = time.time()\nxgBO.maximize(init_points=5,    # Number of randomly chosen points to\n                                 # sample the target function before\n                                 #  fitting the gaussian Process (gp)\n                                 #  or gaussian graph\n               n_iter=25,        # Total number of times the\n               #acq=\"ucb\",       # ucb: upper confidence bound\n                                 #   process is to be repeated\n                                 # ei: Expected improvement\n               # kappa = 1.0     # kappa=1 : prefer exploitation; kappa=10, prefer exploration\n              **gp_params\n               )\nend = time.time()\n(end-start)\/60","dcca352f":"#  Get values of parameters that maximise the objective\nxgBO.res\nxgBO.max","33515fc0":"# Model with parameters of grid search\nmodel_gs = XGBClassifier(\n                    learning_rate = clf.best_params_['xg__learning_rate'],\n                    max_depth = clf.best_params_['xg__max_depth'],\n                    n_estimators=clf.best_params_['xg__n_estimators']\n                    )\n\n#  Model with parameters of random search\nmodel_rs = XGBClassifier(\n                    learning_rate = rs.best_params_['xg__learning_rate'],\n                    max_depth = rs.best_params_['xg__max_depth'],\n                    n_estimators=rs.best_params_['xg__n_estimators']\n                    )\n\n#  Model with parameters of Bayesian Optimization\nmodel_bo = XGBClassifier(\n                    learning_rate = xgBO.max['params']['learning_rate'],\n                    max_depth = int(xgBO.max['params']['max_depth']),\n                    n_estimators= int(xgBO.max['params']['n_estimators'])\n                    )","9dba7a2a":"# Modeling with all the parameters\nstart = time.time()\nmodel_gs.fit(X_train, y_train)\nmodel_rs.fit(X_train, y_train)\nmodel_bo.fit(X_train, y_train)\nend = time.time()\n(end - start)\/60","cc1b462a":"# Predictions with all the models\ny_pred_gs = model_gs.predict(X_test)\ny_pred_rs = model_rs.predict(X_test)\ny_pred_bo = model_bo.predict(X_test)","dc1f23a5":"# 9.4 Accuracy from all the models\naccuracy_gs = accuracy_score(y_test, y_pred_gs)\naccuracy_rs = accuracy_score(y_test, y_pred_rs)\naccuracy_bo = accuracy_score(y_test, y_pred_bo)\nprint(\"Grid Search\",accuracy_gs)\nprint(\"Random Search\",accuracy_rs)\nprint(\"Bayesian Optimization\",accuracy_bo)","35a2f68a":"#  Get feature importances from all the models\nmodel_gs.feature_importances_\nmodel_rs.feature_importances_\nmodel_bo.feature_importances_\nplot_importance(model_gs)\nplot_importance(model_rs)\nplot_importance(model_bo)\n","9d9715c3":"# Confusion matrix for all the models\n\ncm_gs = confusion_matrix(y_test,y_pred_gs)\ncm_rs = confusion_matrix(y_test,y_pred_rs)\ncm_bo = confusion_matrix(y_test,y_pred_bo)\n\ncms = [cm_gs, cm_rs, cm_bo]\nclassifiers = [\"Grid Search\",\"Random Search\",\"Bayesian Optimization\"]\n\ndef plot_confusion_matrix(cms):\n   \n    fig = plt.figure(figsize=(20,12))\n    plt.subplots_adjust( hspace=0.5, wspace=0.4)\n    for i in range(3):\n        j = i+1\n        ax = fig.add_subplot(1,3,j)\n        plt.imshow(cms[i], interpolation='nearest', cmap=plt.cm.Pastel1)\n\n        classNames = ['Red','White']\n\n        plt.ylabel('Actual', size='large')\n\n        plt.xlabel('Predicted', size='large')\n\n        tick_marks = np.arange(len(classNames))\n        plt.xticks(tick_marks, classNames, size='x-large')\n\n        plt.yticks(tick_marks, classNames, size='x-large')\n\n        s = [['TN','FP'], ['FN', 'TP']]\n    \n    \n        plt.text(-0.23,0.05, str(s[0][0])+\" = \"+str(cms[i][0][0]), size='x-large')\n        plt.text(0.8,0.05, str(s[0][1])+\" = \"+str(cms[i][0][1]), size='x-large')\n        plt.text(-0.23,1.05, str(s[1][0])+\" = \"+str(cms[i][1][0]), size='x-large')\n        plt.text(0.8,1.05, str(s[1][1])+\" = \"+str(cms[i][1][1]), size='x-large')\n        plt.title(classifiers[i], fontsize=15)\n\nplot_confusion_matrix(cms)","8e19b1d4":"# Get probability of occurrence of each class\ny_pred_prob_gs = model_gs.predict_proba(X_test)\ny_pred_prob_rs = model_rs.predict_proba(X_test)\ny_pred_prob_bo = model_bo.predict_proba(X_test)\n\n# Draw ROC curve\nfpr_gs, tpr_gs, thresholds = roc_curve(y_test,\n                                 y_pred_prob_gs[: , 0],\n                                 pos_label= 0\n                                 )\n\nfpr_rs, tpr_rs, thresholds = roc_curve(y_test,\n                                 y_pred_prob_rs[: , 0],\n                                 pos_label= 0\n                                 )\n\nfpr_bo, tpr_bo, thresholds = roc_curve(y_test,\n                                 y_pred_prob_bo[: , 0],\n                                 pos_label= 0\n                                 )\n# AUC\nauc_gs = auc(fpr_gs,tpr_gs)\nauc_rs = auc(fpr_rs,tpr_rs)\nauc_bo = auc(fpr_bo,tpr_bo)","f51e7427":"performance = pd.DataFrame({ \"Classifiers\":[\"Grid Search\",\"Random Search\",'Bayesian Optimization'],\n                             \"Accuracy\": [accuracy_score(y_test,y_pred_gs),accuracy_score(y_test,y_pred_rs),accuracy_score(y_test,y_pred_bo)],\n                             \"Precision\": [precision_score(y_test,y_pred_gs),precision_score(y_test,y_pred_rs),precision_score(y_test,y_pred_bo)],\n                             \"AUC\":[auc_gs,auc_rs,auc_bo],\n                             \"Recall\":[recall_score(y_test,y_pred_gs),recall_score(y_test,y_pred_rs),recall_score(y_test,y_pred_bo)],\n                             \"f1_score\":[f1_score(y_test,y_pred_gs),f1_score(y_test,y_pred_rs),f1_score(y_test,y_pred_bo)]})","61abfa9b":"performance","490db04b":"fig = plt.figure(figsize=(12,10))   # Create window frame\nax = fig.add_subplot(111)   # Create axes\n\n#8.1 Connect diagonals\nax.plot([0, 1], [0, 1], ls=\"--\")  # Dashed diagonal line\n\n#8.2 Labels \nax.set_xlabel('False Positive Rate')  # Final plot decorations\nax.set_ylabel('True Positive Rate')\nax.set_title('ROC curve for models')\n\n#8.3 Set graph limits\nax.set_xlim([0.0, 1.0])\nax.set_ylim([0.0, 1.0])\n\n#8.4 Plot each graph now\nax.plot(fpr_gs, tpr_gs, label = \"gs\")\nax.plot(fpr_rs, tpr_rs, label = \"rs\")\nax.plot(fpr_bo, tpr_bo, label = \"bo\")\n\n\n#8.5 Set legend and show plot\nax.legend(loc=\"lower right\")\nplt.show()\n    ","3c83b027":"## <font color=\"brown\">Bayesian Optimization<\/font>","e9f16fe9":"## <font color=\"brown\">Random Search<\/font>","0a7bdb33":"# Fitting parameters in model","6f60e574":"# Reading Data","9cc3cc75":"# Importing Libraries","922e545a":"* <font color=blue>The volatile acidity of red wine decreases with better quality.<\/font>\n* <font color=blue>Better quality red and white wines have shown decreased level of chlorides in them, meaning less amount of salt.<\/font>\n* <font color=blue>There is an increase in the levels of Citric acid and sulphates in higher quality of red wine, which could mean that good quality red wines have more freshness\/flavor and antioxidants in them.<\/font>","f86d5306":"# Splitting data into predictors and target, then to train and test data","d79ff9f8":"# Parameter tuning using Grid Search, Random Search and Bayesian Optimization\n\n## <font color=\"brown\">Grid Search<\/font>","a9bfd4f5":"# Data Exploration","3413d8d8":"# Creating Pipeline","90f54b24":"### <font color='orange'>Finding feature importance - Grid Search<\/font>"}}