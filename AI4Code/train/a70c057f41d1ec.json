{"cell_type":{"53be2f93":"code","c1cfa7f3":"code","23d56769":"code","99f3cb41":"code","adb9a01c":"code","a158d5e5":"code","1e878a9d":"code","ca8996b7":"code","7cddbd92":"code","a0e91ac9":"code","b310daaa":"code","cd0e62ae":"code","ccfa30e7":"code","32f63b74":"code","1865cd82":"code","abd8d179":"code","e2c19c68":"code","1048f0f2":"code","160c2952":"code","b673169c":"code","e700d8b8":"code","0872e7ce":"code","afda015c":"code","883f5a09":"code","bee4a917":"code","a4aaf8fe":"code","67a7a9b2":"code","e1aae514":"code","7b911ca2":"markdown","c5172227":"markdown","a3fc7258":"markdown","25d28326":"markdown","783b7606":"markdown","d85de178":"markdown","20933b12":"markdown","40a267c4":"markdown","7d580304":"markdown","8957f8cf":"markdown","72717e33":"markdown","d2ed340f":"markdown","04f2817e":"markdown","636ef0e6":"markdown","9ad0368d":"markdown","cdd96675":"markdown","9e6b07c5":"markdown","6484168b":"markdown","b9759dd5":"markdown","6e79a68d":"markdown","ff6eeed4":"markdown","cfb1f9b0":"markdown","e3c8e556":"markdown","38e15bd4":"markdown","e887c770":"markdown","bc820cb7":"markdown","74dc3385":"markdown","16e8ae13":"markdown","f455a1a9":"markdown","44f91aa9":"markdown","a646833f":"markdown","01c4ade3":"markdown","78eaf2f8":"markdown","908638cb":"markdown","172c843c":"markdown","2476f16e":"markdown","242e75fe":"markdown","046343b9":"markdown","7cc308b9":"markdown","daec2348":"markdown","8dd26743":"markdown"},"source":{"53be2f93":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.offline as pyo\nfrom sklearn.tree import export_graphviz\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\nimport keras\nimport warnings\nfrom tensorflow.keras import layers\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras.optimizers import Adam\nfrom keras import regularizers\nfrom keras.utils.np_utils import to_categorical\n\nimport os\npyo.init_notebook_mode()\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c1cfa7f3":"data_filepath = '..\/input\/heart-disease-uci\/heart.csv'\ndata = pd.read_csv(data_filepath)\ndata.head()","23d56769":"print(\"Column Names -\\n\", data.columns.unique())\nprint(\"\\nShape of the Dataset -\\n\", data.shape)\nprint(\"\\n\\nUnique Values -\")\ncolumns = data.columns\nfor i in columns:\n    print(data[i].unique())","99f3cb41":"data = data.apply(pd.to_numeric)\ndata.dtypes","adb9a01c":"age_data = data.copy()\n\nage_grp = ['29-33', '34-38', '39-43', '44-48', '49-53', '54-58', '59-63', '64-68', '69-73', '74-78']\nage_data['age_cat'] = pd.cut(age_data['age'], 10, labels=age_grp)\n\nage_plt = pd.crosstab(age_data.age_cat, age_data.target)\n\nfig, ax= plt.subplots()\nage_plt.plot(kind=\"bar\", figsize=(15,6), color=['#F4B41A', '#143D59'], ax=ax)\nplt.title('Heart Disease Frequency')\nplt.xlabel('Age Categories')\nax.set_xticklabels(age_grp)\nplt.xticks(rotation = 45)\nplt.legend([\"Absence\", \"Presence\"], title=\"Heart Disease\", title_fontsize='large', fancybox=True, borderpad=0.9)\nax.spines['top'].set_visible(False) \nax.spines['right'].set_visible(False)\nplt.ylabel('Disease Count')\nplt.show()","a158d5e5":"bps_data = data.copy()\n\nbps_grp = ['94-104', '105-115', '116-126', '127-137', '138-148', '149-159', '160-170', '171-181', '182-191', '192-200']\nbps_data['bps_cat'] = pd.cut(bps_data['trestbps'], 10, labels=bps_grp)\n\nbps_plt = pd.crosstab(bps_data.bps_cat, bps_data.target)\n\nfig, ax= plt.subplots()\nbps_plt.plot(kind=\"bar\", figsize=(15,6), color=['#9BD3CB','#129A7D'], ax=ax)\nplt.title('Heart Disease Frequency')\nplt.xlabel('Resting Blood Pressure Categories')\nax.set_xticklabels(bps_grp)\nplt.xticks(rotation = 45)\nplt.legend([\"Absence\", \"Presence\"], title=\"Heart Disease\", title_fontsize='large', fancybox=True, borderpad=0.9)\nax.spines['top'].set_visible(False) \nax.spines['right'].set_visible(False)\nplt.ylabel('Disease Count')\nplt.show()","1e878a9d":"chol_data = data.copy()\n\nchol_grp = ['125-213', '214-301', '302-388', '389-476', '477-564']\nchol_data['chol_cat'] = pd.cut(chol_data['chol'], 5, labels=chol_grp)\n\nchol_plt = pd.crosstab(chol_data.chol_cat, chol_data.target)\n\nfig, ax= plt.subplots()\nchol_plt.plot(kind=\"bar\", figsize=(15,6), color=['#C9A4A0','#3B3638'], ax=ax)\nplt.title('Heart Disease Frequency')\nplt.xlabel('Cholestrol Level Categories')\nax.set_xticklabels(chol_grp)\nplt.xticks(rotation = 45)\nplt.legend([\"Absence\", \"Presence\"], title=\"Heart Disease\", title_fontsize='large', fancybox=True, borderpad=0.9)\nax.spines['top'].set_visible(False) \nax.spines['right'].set_visible(False)\nplt.ylabel('Disease Count')\nplt.show()","ca8996b7":"thal_data = data.copy()\n\nthal_grp = ['70-84', '84-96', '97-110', '111-122', '123-136', '137-149', '150-162', '163-175', '176-189', '190-202']\nthal_data['thal_cat'] = pd.cut(thal_data['thalach'], 10, labels=thal_grp)\n\nthal_plt = pd.crosstab(thal_data.thal_cat, thal_data.target)\n\nfig, ax= plt.subplots()\nthal_plt.plot(kind=\"bar\", figsize=(15,6), color=['#BEC7D0','#536166'], ax=ax)\nplt.title('Heart Disease Frequency')\nplt.xlabel('Highest Heart Rate Categories')\nax.set_xticklabels(thal_grp)\nplt.xticks(rotation = 45)\nplt.legend([\"Absence\", \"Presence\"], title=\"Heart Disease\", title_fontsize='large', fancybox=True, borderpad=0.9)\nax.spines['top'].set_visible(False) \nax.spines['right'].set_visible(False)\nplt.ylabel('Disease Count')\nplt.show()","7cddbd92":"old_data = data.copy()\n\nold_grp = ['0.00-1.24', '1.25-2.48', '2.49-3.72', '3.73-4.96', '4.96-6.20']\nold_data['old_cat'] = pd.cut(old_data['oldpeak'], 5, labels=old_grp)\n\nold_plt = pd.crosstab(old_data.old_cat, old_data.target)\n\nfig, ax= plt.subplots()\nold_plt.plot(kind=\"bar\", figsize=(15,6), color=['#E5DB9C','#E6A57E'], ax=ax)\nplt.title('Heart Disease Frequency')\nplt.xlabel('Plot Depression Categories')\nax.set_xticklabels(old_grp)\nplt.xticks(rotation = 45)\nplt.legend([\"Absence\", \"Presence\"], title=\"Heart Disease\", title_fontsize='large', fancybox=True, borderpad=0.9)\nax.spines['top'].set_visible(False) \nax.spines['right'].set_visible(False)\nplt.ylabel('Disease Count')\nplt.show()","a0e91ac9":"sex_data = data.loc[data['target'] == 1, ['sex', 'target']]\nsex_data.loc[sex_data['sex'] == 0, 'sex'] = 'Female'\nsex_data.loc[sex_data['sex'] == 1, 'sex'] = 'Male'\n\ncp_data = data.loc[data['target'] == 1, ['cp', 'target']]\ncp_data.loc[cp_data['cp'] == 0, 'cp'] = 'Asymptomatic'\ncp_data.loc[cp_data['cp'] == 1, 'cp'] = 'Typical Angina'\ncp_data.loc[cp_data['cp'] == 2, 'cp'] = 'Atypical Angina'\ncp_data.loc[cp_data['cp'] == 3, 'cp'] = 'Non-Anginal Pain'\ncp_data.loc[cp_data['cp'] == 4, 'cp'] = 'Asymptomatic'\n\nfbs_data = data.loc[data['target'] == 1, ['fbs', 'target']]\nfbs_data.loc[fbs_data['fbs'] == 0, 'fbs'] = 'Lower Than 120mg\/ml'\nfbs_data.loc[fbs_data['fbs'] == 1, 'fbs'] = 'Higher Than 120mg\/ml'\n\nrest_data = data.loc[data['target'] == 1, ['restecg', 'target']]\nrest_data.loc[rest_data['restecg'] == 0, 'restecg'] = 'Normal'\nrest_data.loc[rest_data['restecg'] == 1, 'restecg'] = 'ST-T Wave Abnormality'\nrest_data.loc[rest_data['restecg'] == 2, 'restecg'] = 'Left Ventricular Hypertrophy'\n\nexa_data = data.loc[data['target'] == 1, ['exang', 'target']]\nexa_data.loc[exa_data['exang'] == 0, 'exang'] = 'No'\nexa_data.loc[exa_data['exang'] == 1, 'exang'] = 'Yes'\n\nslo_data = data.loc[data['target'] == 1, ['slope', 'target']]\nslo_data.loc[slo_data['slope'] == 0, 'slope'] = 'Unmeasured'\nslo_data.loc[slo_data['slope'] == 1, 'slope'] = 'Upsloping'\nslo_data.loc[slo_data['slope'] == 2, 'slope'] = 'Flat'\nslo_data.loc[slo_data['slope'] == 3, 'slope'] = 'Downsloping'\n\nca_data = data.loc[data['target'] == 1, ['ca', 'target']]\nca_data.loc[ca_data['ca'] == 0, 'ca'] = 'No Vessels'\nca_data.loc[ca_data['ca'] == 1, 'ca'] = '1 Vessel'\nca_data.loc[ca_data['ca'] == 2, 'ca'] = '2 Vessels'\nca_data.loc[ca_data['ca'] == 3, 'ca'] = '3 Vessels'\nca_data.loc[ca_data['ca'] == 4, 'ca'] = '4 Vessels'\n\nthal_data = data.loc[data['target'] == 1, ['thal', 'target']]\nthal_data.loc[thal_data['thal'] == 0, 'thal'] = 'Undetected'\nthal_data.loc[thal_data['thal'] == 1, 'thal'] = 'Normal'\nthal_data.loc[thal_data['thal'] == 2, 'thal'] = 'Fixed Defect'\nthal_data.loc[thal_data['thal'] == 3, 'thal'] = 'Reversible Defect'\n\nsex_colors = ['#143D59','#F4B41A']\ncp_colors = ['#213970', '#FFE042','#210070', '#E71989']\nfbs_colors = ['#5B0E2D','#FFA781']\nrest_colors = ['#5E001F','#00E1D9', '#143D59']\nexa_colors = ['#E71989','#F49F1C']\nslo_colors = ['#1F8AC0','#F93800', '#283350', '#FFB500']\nca_colors = ['#293250','#FFD55A', '#6DD47E', '#EFC9AF', '#104C91']\nthal_colors = ['#F9858B','#ED335F', '#761137', '#F2BC94']\n\nspecs = [[{'type':'domain'}, {'type':'domain'}, {'type':'domain'}, {'type':'domain'}], [{'type':'domain'}, {'type':'domain'} ,{'type':'domain'}, {'type':'domain'}]]\nfig = make_subplots(rows=2, cols=4, specs=specs)\n\nfig.add_trace(go.Pie(labels=sex_data.sex, values=sex_data.target, title='Gender', scalegroup='one', \n                     hovertemplate = \"%{label}: <br>Risk of Heart Disease: %{percent} <\/br><extra><\/extra>\",\n                     marker_colors = sex_colors, marker = dict(line = dict(color='#000000', width=0.5)), pull=[0.1, 0.1]), 1, 1)\nfig.add_trace(go.Pie(labels=cp_data.cp, values=cp_data.target, title='Chest Pain Level', scalegroup='one', \n                     hovertemplate = \"%{label}: <br>Risk of Heart Disease: %{percent} <\/br><extra><\/extra>\",\n                     marker_colors = cp_colors, marker = dict(line = dict(color='#000000', width=0.5)), pull=[0.1, 0.1, 0.1, 0.1]), 1, 2)\nfig.add_trace(go.Pie(labels=fbs_data.fbs, values=fbs_data.target, title='Fasting Blood Sugar Level', scalegroup='one', \n                     hovertemplate = \"%{label}: <br>Risk of Heart Disease: %{percent} <\/br><extra><\/extra>\",\n                     marker_colors = fbs_colors, marker = dict(line = dict(color='#000000', width=0.5)), pull=[0.1, 0.1]), 1, 3)\nfig.add_trace(go.Pie(labels=rest_data.restecg, values=rest_data.target, title='Resting ECG Results', scalegroup='one', \n                     hovertemplate = \"%{label}: <br>Risk of Heart Disease: %{percent} <\/br><extra><\/extra>\",\n                     marker_colors = rest_colors, marker = dict(line = dict(color='#000000', width=0.5)), pull=[0.1, 0.1, 0.1]), 1, 4)\nfig.add_trace(go.Pie(labels=exa_data.exang, values=exa_data.target, title='Exercise Induced Angina', scalegroup='one', \n                     hovertemplate = \"%{label}: <br>Risk of Heart Disease: %{percent} <\/br><extra><\/extra>\",\n                     marker_colors = exa_colors, marker = dict(line = dict(color='#000000', width=0.5)), pull=[0.1, 0.1]), 2, 1)\nfig.add_trace(go.Pie(labels=slo_data.slope, values=slo_data.target, title='ST Segment Slope', scalegroup='one', \n                     hovertemplate = \"%{label}: <br>Risk of Heart Disease: %{percent} <\/br><extra><\/extra>\",\n                     marker_colors = slo_colors, marker = dict(line = dict(color='#000000', width=0.5)), pull=[0.1, 0.1, 0.1]), 2, 2)\nfig.add_trace(go.Pie(labels=ca_data.ca, values=ca_data.target, title='Coloured Blood Vessels Fluoroscopy', scalegroup='one', \n                     hovertemplate = \"%{label}: <br>Risk of Heart Disease: %{percent} <\/br><extra><\/extra>\",\n                     marker_colors = ca_colors, marker = dict(line = dict(color='#000000', width=0.5)), pull=[0.1, 0.1, 0.1, 0.1, 0.1]), 2, 3)\nfig.add_trace(go.Pie(labels=thal_data.thal, values=thal_data.target, title='Heart Rate Defect', scalegroup='one', \n                     hovertemplate = \"%{label}: <br>Risk of Heart Disease: %{percent} <\/br><extra><\/extra>\",\n                     marker_colors = thal_colors, marker = dict(line = dict(color='#000000', width=0.5)), pull=[0.1, 0.1, 0.1, 0.1]), 2, 4)\n\nfig.update_traces(hoverinfo='label+percent', textinfo='none')\nfig.update(layout_title_text='Various Factors Responsible for Heart Disease', layout_showlegend=False)\nfig.update_layout(hoverlabel=dict(bgcolor=\"white\", font_size=16, font_family=\"Rockwell\"))\nfig.update_layout(margin=dict(t=100, b=0, l=0, r=0))\nfig.update_layout(width=1400, height=1000, hovermode='x')\nfig = go.Figure(fig)\nfig.show()","b310daaa":"age_data = sorted(data['age'].unique())\nage_thal_data = data.groupby('age')['thalach'].count().values\nmthal=[]\nfor i, age in enumerate(age_data):\n    mthal.append(sum(data[data['age'] == age].thalach) \/ age_thal_data[i])\nage_rest_data = data.groupby('age')['trestbps'].count().values\nmrest=[]\nfor i, age in enumerate(age_data):\n    mrest.append(sum(data[data['age'] == age].trestbps) \/ age_rest_data[i])\n    \nfig = go.Figure()\nfig.add_trace(go.Scatter(x=age_data, y=mthal, mode='lines+markers', name='Max Heart Rate', line=dict(color='firebrick', width=4),\n                         hovertemplate = \"Max Heart Rate: <br>Age: %{x} <\/br>Value: %{y} <\/br><extra><\/extra>\"))\nfig.add_trace(go.Scatter(x=age_data, y=mrest, mode='lines+markers', name='Resting Blood Pressure', line=dict(color='royalblue', width=4),\n                         hovertemplate = \"Resting Blood Pressure: <br>Age: %{x} <\/br>Value: %{y} <\/br><extra><\/extra>\"))\n\nfig.update(layout_title_text='Age Comparisons against Heart Rate & Blood Pressure')\nfig.update_layout(hoverlabel=dict(bgcolor=\"white\", font_size=16, font_family=\"Rockwell\"), plot_bgcolor='white')\nfig.update_layout(xaxis=dict(showline=True, showgrid=False, showticklabels=True, linecolor='rgb(2, 2, 2)',\n                             linewidth=2, ticks='outside', tickfont=dict(family='Arial', size=12, color='rgb(82, 82, 82)')),\n                  yaxis=dict(showgrid=True, zeroline=True, showline=True, showticklabels=True, linecolor='rgb(2, 2, 2)',\n                             linewidth=2, ticks='outside', tickfont=dict(family='Arial', size=12, color='rgb(82, 82, 82)')))\nfig.show()","cd0e62ae":"age_chol_data = data.groupby('age')['chol'].count().values\nmchol=[]\nfor i, age in enumerate(age_data):\n    mchol.append(sum(data[data['age'] == age].chol) \/ age_chol_data[i])\n    \nfig = go.Figure()\nfig.add_trace(go.Scatter(x=age_data, y=mchol, mode='lines+markers', name='Cholestrol Levels', line=dict(color='firebrick', width=4),\n                         hovertemplate = \"Cholestrol Levels: <br>Age: %{x} <\/br>Value: %{y} <\/br><extra><\/extra>\"))\nfig.add_trace(go.Scatter(x=age_data, y=mrest, mode='lines+markers', name='Resting Blood Pressure', line=dict(color='royalblue', width=4),\n                         hovertemplate = \"Resting Blood Pressure: <br>Age: %{x} <\/br>Value: %{y} <\/br><extra><\/extra>\"))\n\nfig.update(layout_title_text='Age Comparisons against Cholestrol Levels & Blood Pressure')\nfig.update_layout(hoverlabel=dict(bgcolor=\"white\", font_size=16, font_family=\"Rockwell\"), plot_bgcolor='white')\nfig.update_layout(xaxis=dict(showline=True, showgrid=False, showticklabels=True, linecolor='rgb(2, 2, 2)',\n                             linewidth=2, ticks='outside', tickfont=dict(family='Arial', size=12, color='rgb(82, 82, 82)')),\n                  yaxis=dict(showgrid=True, zeroline=True, showline=True, showticklabels=True, linecolor='rgb(2, 2, 2)',\n                             linewidth=2, ticks='outside', tickfont=dict(family='Arial', size=12, color='rgb(82, 82, 82)')))\nfig.show()","ccfa30e7":"pearsoncorrelation = data.corr(method='pearson')\nplt.figure(figsize = (12,12))\nsns.heatmap(pearsoncorrelation, xticklabels=pearsoncorrelation.columns, yticklabels=pearsoncorrelation.columns, cmap='RdBu_r', annot=True, linewidth=0.5)","32f63b74":"X = data[['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n         'exang', 'oldpeak', 'slope', 'ca', 'thal']]\ny = data['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=30)","1865cd82":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\nY_train = to_categorical(y_train, num_classes=None)\nY_test = to_categorical(y_test, num_classes=None)","abd8d179":"lr = LogisticRegression(max_iter = 10000)\nlr.fit(X_train, y_train)\n\ny_pred = lr.predict(X_test)\nlra = accuracy_score(y_test,y_pred)\nlr_acs = lra*100\nprint('Accuracy Score = ', lr_acs)\n\npipe_lr = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000, random_state=1))\npipe_lr.fit(X_train, y_train)\nlr_pacs = pipe_lr.score(X_test, y_test)*100\nprint('\\nPipeline Accuracy Score = ', lr_pacs)\n\nprint(\"\\nClassification Report\\n\\n\",classification_report(y_test,y_pred))","e2c19c68":"knn = KNeighborsClassifier(n_neighbors = 5, metric = 'manhattan', p = 2)\nknn.fit(X_train,y_train)\n\ny_pred = knn.predict(X_test)\nknna = accuracy_score(y_test,y_pred)\nknn_acs = knna*100\nprint('Accuracy Score = ', knn_acs)\n\npipe_knn = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors = 5, metric = 'euclidean', p = 2))\npipe_knn.fit(X_train, y_train)\nknn_pacs = pipe_knn.score(X_test, y_test)*100\nprint('\\nPipeline Accuracy Score = ', knn_pacs)\n\nprint(\"\\nClassification Report\\n\\n\", classification_report(y_test,y_pred))","1048f0f2":"svc = SVC(kernel = 'linear')\nsvc.fit(X_train,y_train)\n\ny_pred = svc.predict(X_test)\nsvca = accuracy_score(y_test,y_pred)\nsvc_acs = svca*100\nprint('Accuracy Score = ', svc_acs)\n\npipe_svc = make_pipeline(StandardScaler(), SVC(max_iter=1000, gamma='auto', kernel='poly'))\npipe_svc.fit(X_train, y_train)\nsvc_pacs = pipe_svc.score(X_test, y_test)*100\nprint('\\nPipeline Accuracy Score = ', svc_pacs)\n\nprint(\"\\nClassification Report\\n\\n\", classification_report(y_test,y_pred))","160c2952":"nb = GaussianNB()\nnb.fit(X_train,y_train)\n\ny_pred = nb.predict(X_test)\nnba = accuracy_score(y_test,y_pred)\nnb_acs = nba*100\nprint('Accuracy Score = ', nb_acs)\n\npipe_gnb = make_pipeline(StandardScaler(), GaussianNB())\npipe_gnb.fit(X_train, y_train)\nnb_pacs = pipe_gnb.score(X_test, y_test)*100\nprint('\\nPipeline Accuracy Score = ', nb_pacs)\n\nprint(\"\\nClassification Report\\n\\n\", classification_report(y_test,y_pred))","b673169c":"dt = DecisionTreeClassifier(criterion = 'entropy')\ndt.fit(X_train,y_train)\n\ny_pred = dt.predict(X_test)\ndta = accuracy_score(y_test,y_pred)\ndt_acs = dta*100\nprint('Accuracy Score = ', dt_acs)\n\npipe_dtc = make_pipeline(StandardScaler(), DecisionTreeClassifier(criterion='entropy', max_depth=3))\npipe_dtc.fit(X_train, y_train)\ndt_pacs = pipe_dtc.score(X_test, y_test)*100\nprint('\\nPipeline Accuracy Score = ', dt_pacs)\n\nprint(\"\\nClassification Report\\n\\n\", classification_report(y_test,y_pred))","e700d8b8":"rf = RandomForestClassifier(n_estimators = 60, criterion = 'entropy',random_state = 0)\nrf.fit(X_train,y_train)\n\ny_pred = rf.predict(X_test)\nrfa = accuracy_score(y_test,y_pred)\nrf_acs = rfa*100\nprint('Accuracy Score = ', rf_acs)\n\npipe_rft = make_pipeline(StandardScaler(), RandomForestClassifier(criterion='entropy', random_state=0))\npipe_rft.fit(X_train, y_train)\nrf_pacs = pipe_rft.score(X_test, y_test)*100\nprint('\\nPipeline Accuracy Score = ', rf_pacs)\nprint(\"\\nClassification Report\\n\\n\", classification_report(y_test,y_pred))","0872e7ce":"model = Sequential()\nmodel.add(Dense(16, input_dim=13, kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.001), activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(8, kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.001), activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(2, activation='softmax'))\n    \nadam = Adam(lr=0.001)\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n\nmodel.fit(X_train, Y_train, validation_data=(X_test, Y_test),epochs=100, batch_size=10)","afda015c":"Ytr_binary = y_train.copy()\nYt_binary = y_test.copy()\n\nYtr_binary[Ytr_binary > 0] = 1\nYt_binary[Yt_binary > 0] = 1\n\nYtr2_binary = Y_train.copy()\nYt2_binary = Y_test.copy()\n\nYtr2_binary[Ytr2_binary > 0] = 1\nYt2_binary[Yt2_binary > 0] = 1","883f5a09":"pipe_lr_bin = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000, random_state=1))\npipe_lr_bin.fit(X_train, Ytr_binary)\nlr_bpacs = pipe_lr_bin.score(X_test, Yt_binary)*100\nprint('Logistic Regression Accuracy Score = ', lr_bpacs)\n\npipe_knn_bin = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=5, p=2, metric='euclidean'))\npipe_knn_bin.fit(X_train, Ytr_binary)\nknn_bpacs = pipe_knn_bin.score(X_test, Yt_binary)*100\nprint('\\nK Neighbours Classifier Accuracy Score = ', knn_bpacs)\n\npipe_svc_bin = make_pipeline(StandardScaler(), SVC(max_iter=1000, gamma='auto', kernel='poly'))\npipe_svc_bin.fit(X_train, Ytr_binary)\nsvc_bpacs = pipe_svc_bin.score(X_test, Yt_binary)*100\nprint('\\nSVC Accuracy Score = ', svc_bpacs)\n\npipe_gnb_bin = make_pipeline(StandardScaler(), GaussianNB())\npipe_gnb_bin.fit(X_train, Ytr_binary)\nnb_bpacs = pipe_gnb_bin.score(X_test, Yt_binary)*100\nprint('\\nGaussian NB Accuracy Score = ', nb_bpacs)\n\npipe_dtc_bin = make_pipeline(StandardScaler(), DecisionTreeClassifier(criterion='entropy', max_depth=3))\npipe_dtc_bin.fit(X_train, Ytr_binary)\ndt_bpacs = pipe_dtc_bin.score(X_test, Yt_binary)*100\nprint('\\nDecision Tree Classifier Accuracy Score = ', dt_bpacs)\n\npipe_rfc_bin = make_pipeline(StandardScaler(), RandomForestClassifier(criterion='entropy', random_state=0))\npipe_rfc_bin.fit(X_train, Ytr_binary)\nrf_bpacs = pipe_rfc_bin.score(X_test, Yt_binary)*100\nprint('\\nRandom Forest Classifier Accuracy Score = ', rf_bpacs)","bee4a917":"model_bin = Sequential()\nmodel_bin.add(Dense(16, input_dim=13, kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.001), activation='relu'))\nmodel_bin.add(Dropout(0.1))\nmodel_bin.add(Dense(8, kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.001), activation='relu'))\nmodel_bin.add(Dropout(0.1))\nmodel_bin.add(Dense(2, activation='softmax'))\n    \nadam_bin = Adam(lr=0.001)\nmodel_bin.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n\nmodel_bin.fit(X_train, Ytr2_binary, validation_data=(X_test, Yt2_binary),epochs=100, batch_size=10)","a4aaf8fe":"ml_class = [\"Logistic Regression\", \"K Nearest Neighbours\", \"Support Vector\", \"Gaussian Naive Bayes\", \"Decision Trees\", \"Random Forest\"]\nml_score = [lr_acs, knn_acs, svc_acs, nb_acs, dt_acs, rf_acs]\nml_pscore = [lr_pacs, knn_pacs, svc_pacs, nb_pacs, dt_pacs, rf_pacs]\nml_bpscore = [lr_bpacs, knn_bpacs, svc_bpacs, nb_bpacs, dt_bpacs, rf_bpacs]\n\nscores = pd.DataFrame(data={\"Classifier\": ml_class, \"Avg Score\": ml_score, \"Pipe Avg Score\": ml_pscore})\nbin_scores = pd.DataFrame(data={\"Classifier\": ml_class, \"Pipe Avg Score\": ml_pscore, \"Bin Pipe Score\": ml_pscore})","67a7a9b2":"fig = go.Figure(data=[\n    go.Bar(name='Average Scores', x = ml_class, y = ml_score, hovertemplate = \"Classification Levels: <br>Classification Technique: %{x} <\/br>Percentage Score: %{y} <\/br><extra><\/extra>\"), \n    go.Bar(name='Pipeline Average Scores', x = ml_class, y = ml_pscore, hovertemplate = \"Classification Levels: <br>Classification Technique: %{x} <\/br>Percentage Score: %{y} <\/br><extra><\/extra>\"),\n    go.Bar(name='Binary Pipeline Scores', x = ml_class, y = ml_bpscore, hovertemplate = \"Classification Levels: <br>Classification Technique: %{x} <\/br>Percentage Score: %{y} <\/br><extra><\/extra>\")\n])\n\nfig.update(layout_title_text='Classification Models and their Scores')\nfig.update_layout(hoverlabel=dict(bgcolor=\"white\", font_size=16, font_family=\"Rockwell\"), plot_bgcolor='white')\nfig.update_layout(xaxis=dict(showline=True, showgrid=False, showticklabels=True, linecolor='rgb(2, 2, 2)',\n                             linewidth=2, ticks='outside', tickfont=dict(family='Arial', size=12, color='rgb(82, 82, 82)')),\n                  yaxis=dict(showgrid=True, zeroline=True, showline=True, showticklabels=True, linecolor='rgb(2, 2, 2)',\n                             linewidth=2, ticks='outside', tickfont=dict(family='Arial', size=12, color='rgb(82, 82, 82)')))\nfig.update_layout(barmode='group')\nfig.show()","e1aae514":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=ml_class, y=ml_score, mode='lines+markers', name='Average Scores', line=dict(color='purple', width=4),\n                         hovertemplate = \"Classification Levels: <br>Classification Technique: %{x} <\/br>Percentage Score: %{y} <\/br><extra><\/extra>\"))\nfig.add_trace(go.Scatter(x=ml_class, y=ml_pscore, mode='lines+markers', name='Pipeline Average Scores', line=dict(color='yellowgreen', width=8),\n                         hovertemplate = \"Classification Levels: <br>Classification Technique: %{x} <\/br>Percentage Score: %{y} <\/br><extra><\/extra>\"))\nfig.add_trace(go.Scatter(x=ml_class, y=ml_bpscore, mode='lines+markers', name='Binary Pipeline Scores', line=dict(color='darksalmon', width=4),\n                         hovertemplate = \"Classification Levels: <br>Classification Technique: %{x} <\/br>Percentage Score: %{y} <\/br><extra><\/extra>\"))\n\nfig.update(layout_title_text='Classification Models and their Scores')\nfig.update_layout(hoverlabel=dict(bgcolor=\"white\", font_size=16, font_family=\"Rockwell\"), plot_bgcolor='white')\nfig.update_layout(xaxis=dict(showline=True, showgrid=False, showticklabels=True, linecolor='rgb(2, 2, 2)',\n                             linewidth=2, ticks='outside', tickfont=dict(family='Arial', size=12, color='rgb(82, 82, 82)')),\n                  yaxis=dict(showgrid=True, zeroline=True, showline=True, showticklabels=True, linecolor='rgb(2, 2, 2)',\n                             linewidth=2, ticks='outside', tickfont=dict(family='Arial', size=12, color='rgb(82, 82, 82)')))\nfig.show()","7b911ca2":"## Prediction using Classification Models","c5172227":"### Deeper Analysis using Multiple Factors","a3fc7258":"##### Binary Calculations refer to using a two-valued approach instead of multiple values. \nWe use this method to see if there are any improvements to our predictions","25d28326":"## Thank you!\n\n\n\n### Please upvote","783b7606":"## Prediction using Neural Networks","d85de178":"## Basic Feature Engineering ","20933b12":"### Value Based Comparison and Analysis\n\nThis particular method will help us understand, on a surface level what limiting factors are responsible for the presence of Heart Disease.","40a267c4":"### Differences in Predictions - A Visualisation","7d580304":"## Basic Data Analysis","8957f8cf":"#### Resting Blood Pressure vs Target","72717e33":"#### Cholestrol Levels vs Target","d2ed340f":"This analysis focuses on the factors that could potentially result in heart disease. It will help us understand not just the responsible factors, but also the ways one could prevent or reduce the chances of a heart diease.\n\nIn the end, we will use rudimentary machine learning algorithms to determine the truth behind our analysis.","04f2817e":"## Optimising Dataset for Better Predictions","636ef0e6":"#### Using Neural Networks - Again","9ad0368d":"##### Now that our rudimentary analysis is complete, we can look into more complex forms of analysis. \n##### For that, we will use age as our constant factor and compare multiple limiting factors against each other, to get a more accurate representation.","cdd96675":"#### K-Nearest Neighbours Classification","9e6b07c5":"### Binary Calculations - Do They Make a Difference?","6484168b":"##### Now we move on to our next part - Prediction. \nPredictions help us foresee who is potentially at risk of heart disease. This can be calulated with the following 2 processes - \n\n* **Feature Engineering** - To enhance our dataset\n* **Machine Learning** - To help create a prediction model that acts as our base for all onset patients","b9759dd5":"#### Decision Tree Classification","6e79a68d":"Value based factors look at individual numbers. They could be 1s and 0s, that signify a particular value. We will use these values and compare them against chances of contracting a heart disease to see which factors are relevant to our analysis.","ff6eeed4":"#### Maximum Heart Rate vs Target","cfb1f9b0":"### Importing Dataset","e3c8e556":"#### Maximum Heart Rate and Resting Blood Pressure","38e15bd4":"#### Exercise Induced ST Plot Depressions vs Target","e887c770":"#### Age vs Target","bc820cb7":"#### Gaussian Naive Bayes Classification","74dc3385":"* **Age** - Patients within the ages of 39 and 68 are at a higher risk than others. But at the same time, they have a lower percentage of cases with respect to the total admissions. \n* **Sex** - Male patients are at a significantly higher risk than female patients when it comes to the contraction of heart disease\n* **Chest Pain Type** - It seems that patients with Atypical Angina are at a greater risk of heart disease than others\n* **Resting Blood Pressure** - Patients with a resting BP of 116 to 148 are at a significantly higher risk than others\n* **Cholestrol Levels** - Cholestrol levels of 214 to 301 are prime targets for heart disease\n* **Fasting Blood Sugar Levels** - Sugar Levels lower than 120 mg\/ml have an 86% chance of contracting heart disease\n* **Resting ECG Results** - Patients recording ST-T Wave Abnormality are at the greater risk than others\n* **Maximum Heart Rate** - Accelerated Heart Rates of 150 to 175 signal the onset of heart disease\n* **Presence of Exercise Induced Angina** - Contrarily, its the absence of Exercise Induced Angina that resonates more with heart disease\n* **ST Depression Levels** - The lowest recorded values indicate the strongest chances at heart disease\n* **Slope of ST Depression Segments** - Flat slopes record a staggering 2\/3rd chances of heart disease\n* **Number of Fluoroscopy Colored Vessels** - No colored vessels among patients indicate higher chances of heart disease\n* **Heart Rate Defects** - Presence of a fixed defect seems more likely to indicate heart disease","16e8ae13":"### Limit Based Comparison and Analysis.\n\nThis particular method will help us understand, on a surface level what limiting factors are responsible for the presence of Heart Disease.","f455a1a9":"#### Using Classification Models - Again.\n\n##### With Pipeline only","44f91aa9":"##### Now that we have compared limit based factors responsible for heart disease, we can look to value based factors. ","a646833f":"# Finding out the chances of a Heart Disease","01c4ade3":"#### Cholestrol Levels and Resting Blood Pressure","78eaf2f8":"#### Figuring the importance of each feature to our prediction","908638cb":"##### Now that we have our rudimentary analysis completed, we can complete a surface level report. ","172c843c":"#### Support Vector Classification","2476f16e":"### Unique Dataset Values","242e75fe":"#### Random Forest Classification","046343b9":"#### Logistic Regression\n","7cc308b9":"### Data contains - \n\n* **age** - Age in Years\n* **sex** - *(1 = male; 0 = female)*\n* **cp** - Chest Pain Type\n* **trestbps** - Resting Blood Pressure *(in mm Hg on admission to the hospital)*\n* **chol** - Serum Cholestoral *(in mg\/dl)*\n* **fbs** - Fasting Blood Sugar > 120 mg\/dl *(1 = true; 0 = false)*\n* **restecg** - Resting ElectroCardioGraphic *(ECG)* Results\n* **thalach** - Maximum Heart Rate Achieved\n* **exang** - Exercise Induced Angina *(1 = yes; 0 = no)*\n* **oldpeak** - ST Depression Induced by Exercise Relative to Rest\n* **slope** - The Slope of the Peak Exercise ST Segment\n* **ca** - Number of Major Vessels (0-4) Colored by Flourosopy\n* **thal** - *(3 = normal; 6 = fixed defect; 7 = reversible defect)*\n* **target** - Have a Heart Disease or Not *(1 = yes, 0 = no)*","daec2348":"#### Getting dataset ready for prediction","8dd26743":"##### We first compare limit-based factors against target."}}