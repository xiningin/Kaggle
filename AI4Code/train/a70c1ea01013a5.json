{"cell_type":{"01bfc33e":"code","4932451b":"code","7df2cbf3":"code","4bf0e052":"code","d12f52a3":"code","cded92d2":"code","3f5a3e27":"code","1c6a7100":"code","e181956d":"code","228d37dc":"code","ee77f168":"code","ef84595c":"code","217c3b6a":"code","3dc0e5fa":"code","1e2c8d60":"code","9ea78929":"code","610a1e9d":"code","8f219e68":"code","2b6dd0a3":"code","0340579b":"code","27ee639f":"code","ed7769de":"code","273320e5":"code","dc982350":"code","81edd0bb":"code","39bd2479":"markdown","8c34dee3":"markdown","11dec565":"markdown","e31ca0ff":"markdown","6d9b9c25":"markdown","99131df7":"markdown","8e976a9a":"markdown","3f961639":"markdown","a4e48e7f":"markdown","3493fd09":"markdown","1afd0696":"markdown","e2242cee":"markdown","694f114d":"markdown"},"source":{"01bfc33e":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm_notebook\nfrom collections import Counter\nimport warnings \nwarnings.filterwarnings(\"ignore\")\nimport optuna\nimport multiprocessing\nfrom joblib import Parallel, delayed\nfrom typing import Any\nimport gc\nimport re\nimport random\npd.set_option('display.max_columns', None)\n# pd.set_option('display.max_rows', None)\ntarget = 'accuracy_group_target'","4932451b":"def seed_everything(seed=1234): \n    random.seed(seed) \n    os.environ['PYTHONHASHSEED'] = str(seed) \n    np.random.seed(seed)","7df2cbf3":"seed_everything(2020)","4bf0e052":"DATA_PATH = '\/kaggle\/input\/data-science-bowl-2019\/'\ndef read_data():\n    print('Reading train.csv file....')\n    train = pd.read_csv(DATA_PATH + 'train.csv')\n    print('Training.csv file have {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n\n    print('Reading test.csv file....')\n    test = pd.read_csv(DATA_PATH + 'test.csv')\n    print('Test.csv file have {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n\n    print('Reading train_labels.csv file....')\n    train_labels = pd.read_csv(DATA_PATH + 'train_labels.csv')\n    print('Train_labels.csv file have {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n\n    print('Reading specs.csv file....')\n    specs = pd.read_csv(DATA_PATH + 'specs.csv')\n    print('Specs.csv file have {} rows and {} columns'.format(specs.shape[0], specs.shape[1]))\n\n    print('Reading sample_submission.csv file....')\n    sample_submission = pd.read_csv(DATA_PATH + 'sample_submission.csv')\n    print('Sample_submission.csv file have {} rows and {} columns'.format(sample_submission.shape[0], sample_submission.shape[1]))\n    return train, test, train_labels, specs, sample_submission\n\ntrain, test, train_labels, specs, sample_submission = read_data()\nkeep_id = train[(train.type == \"Assessment\")  & ((train.event_code==4100) | (train.event_code==4110))][['installation_id']].drop_duplicates()\ntrain_target = pd.merge(train, keep_id, on=\"installation_id\", how=\"inner\")\ntrain_target['flag'] = 1\ntest['flag'] = 0\ndf = pd.concat([train_target, test])","d12f52a3":"print(df.shape)\n\ndel train, test, train_labels, train_target\ngc.collect()","cded92d2":"def get_unique(df):\n#     title_event_codes = df.title_event_code.unique()\n    titles = df.title.unique()\n    event_codes = df.event_code.unique()\n    event_ids = df.event_id.unique()\n    worlds = df.world.unique()\n    types = df.type.unique()\n    asses_titles = ['Mushroom Sorter (Assessment)', 'Bird Measurer (Assessment)',\n       'Chest Sorter (Assessment)', 'Cauldron Filler (Assessment)',\n       'Cart Balancer (Assessment)']\n    event_data = ['description', 'round_number', 'shell_size', 'location', 'nest', 'rocket', 'exit_type', 'target_water_level', 'crystal_id', 'height', 'buckets_placed', 'dinosaur_count', 'round_target', 'tutorial_step', 'dinosaurs', 'tape_length', 'game_time', 'media_type', 'buglength', 'weight', 'holding_shell', 'castles_placed', 'sand', 'has_water', 'crystals', 'end_position', 'movie_id', 'event_count', 'cloud', 'previous_jars', 'bottle', 'duration', 'starting_weights', 'event_code', 'table_weights', 'level', 'growth', 'stumps', 'scale_weights', 'resources', 'hat', 'time_played', 'bottles', 'containers', 'cauldron', 'dinosaur', 'target_containers', 'hats_placed', 'bowls', 'options', 'pillars', 'caterpillar', 'current_containers', 'holes', 'jar', 'scale_contents', 'container_type', 'bucket', 'bug', 'target_bucket', 'bowl_id', 'hats', 'group', 'toy', 'stage_number', 'bird_height', 'misses', 'house', 'target_weight', 'dinosaurs_placed', 'molds', 'scale_weight', 'bug_length', 'houses', 'buckets', 'prompt', 'right', 'object_type', 'position', 'session_duration', 'target_size', 'jar_filled', 'total_duration', 'item_type', 'caterpillars', 'layout', 'dinosaur_weight', 'animal', 'hole_position', 'correct', 'target_distances', 'gate', 'dwell_time', 'object', 'weights', 'flowers', 'round_prompt', 'source', 'max_position', 'chests', 'version', 'side', 'round', 'cloud_size', 'shells', 'flower', 'mode', 'distance', 'total_bowls', 'size', 'identifier', 'launched', 'toy_earned', 'diet', 'has_toy', 'animals', 'water_level', 'left', 'filled', 'coordinates', 'destination', 'total_containers']\n    win_codes = {t:4100 for t in titles}\n    win_codes['Bird Measurer (Assessment)'] = 4110\n    df.timestamp = pd.to_datetime(df.timestamp)\n    return titles, event_codes, event_ids, worlds, types, asses_titles, event_data, win_codes\ntitles, event_codes, event_ids, worlds, types, asses_titles, event_data, win_codes = get_unique(df)\nasses_titles = ['Mushroom Sorter (Assessment)', 'Bird Measurer (Assessment)',\n'Chest Sorter (Assessment)', 'Cauldron Filler (Assessment)',\n'Cart Balancer (Assessment)']\nwin_codes = {t:4100 for t in titles}\nwin_codes['Bird Measurer (Assessment)'] = 4110\ndf.timestamp = pd.to_datetime(df.timestamp)\ndf = df.merge(specs, how='left', on='event_id', suffixes=('','_y'))","3f5a3e27":"compiled_session = []\ndef update_counters(counter: dict, col: str, session):\n    num_of_session_count = Counter(session[col])\n    for k in num_of_session_count.keys():\n        x = k\n        counter[x] += num_of_session_count[k]\n    return counter\nfor i, session in tqdm_notebook(df.groupby('game_session', sort=False), total=203912):\n    features = {c: 0 for c in list(worlds) + list(types) + list(titles) + list(event_codes) + list(event_ids)}\n    session_type = session['type'].iloc[0]\n    session_title = session['title'].iloc[0]\n    features = update_counters(features, \"event_id\", session)\n    features = update_counters(features, \"world\", session)\n    features = update_counters(features, \"type\", session)\n    features = update_counters(features, \"title\", session)\n    features = update_counters(features, \"event_code\", session)\n    features['installation_id'] = session['installation_id'].iloc[0]\n    features['game_session'] = session['game_session'].iloc[0]\n    features['session_title'] = session_title\n    features['flag'] = session['flag'].iloc[0]\n    features['session_type'] = session_type\n    features['world'] = session['world'].iloc[0]\n    features['event_count'] = session['event_count'].iloc[-1]\n    features['session_count'] = 1\n    features['var_event_id'] =  session.event_id.nunique()\n    features['var_title'] = session.title.nunique()\n    features[session_type] = 1\n    features['start_time'] = session['timestamp'].iloc[0]\n    features['end_time'] = session['timestamp'].iloc[-1]\n    features['duration'] =  (session['timestamp'].iloc[-1] - session['timestamp'].iloc[0]).seconds\n    features['game_time'] = session['game_time'].iloc[-1]\n    features['0'] = 0\n    features['1'] = 0\n    features['2'] = 0\n    features['3'] = 0\n    features['num_click'] = session['info'].str.contains('click').sum()\n    if (session_type == 'Assessment') & (len(session) > 1):\n        all_attempts = session.query(f'event_code == {win_codes[session_title]}')\n        true_attempts = all_attempts['event_data'].str.contains('true').sum()\n        false_attempts = all_attempts['event_data'].str.contains('false').sum()\n        features['num_incorrect'] = false_attempts\n        features['num_correct'] = true_attempts\n        if (true_attempts+false_attempts)>0:\n            accuracy = true_attempts\/(true_attempts+false_attempts)\n            features['accuracy'] = accuracy\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n            features[str(features['accuracy_group'])] = 1\n    if (session_type == 'Game') & (len(session) > 1):\n        true_attempts = session['info'].str.contains('Correct').sum()\n        false_attempts = session['info'].str.contains('Incorrect').sum()\n        play_times = session['info'].str.contains('again').sum()\n        features['game_num_incorrect'] = false_attempts\n        features['game_num_correct'] = true_attempts\n        features['game_play_again'] = play_times\n    compiled_session.append(features)","1c6a7100":"del df\ngc.collect()","e181956d":"compiled_df = pd.DataFrame(compiled_session)\ndel compiled_session, df\ngc.collect()","228d37dc":"clip_lengh = {\n    'Welcome to Lost Lagoon!':19,\n    'Tree Top City - Level 1':17,\n    'Ordering Spheres':61,\n    'Costume Box':61,\n    '12 Monkeys':109,\n    'Tree Top City - Level 2':25,\n    \"Pirate's Tale\":80,\n    'Treasure Map':156,\n    'Tree Top City - Level 3':26,\n    'Rulers':126,\n    'Magma Peak - Level 1':20,\n    'Slop Problem':60,\n    'Magma Peak - Level 2':22,\n    'Crystal Caves - Level 1':18,\n    'Balancing Act':72,\n    'Lifting Heavy Things':118,\n    'Crystal Caves - Level 2':24,\n    'Honey Cake':142,\n    'Crystal Caves - Level 3':19,\n    'Heavy, Heavier, Heaviest':61\n}\ncompiled_df['clip_time'] = compiled_df['session_title'].map(clip_lengh)\ncompiled_df['clip_time'].fillna(0, inplace=True)\ncompiled_df['game_time'] = compiled_df['game_time']\/1000\ncompiled_df['game_time'] = compiled_df['game_time'] + compiled_df['clip_time']\ncompiled_df.drop(['clip_time'], axis=1, inplace=True)","ee77f168":"def block2feature(sample_id):\n    installation_id = sample_id['installation_id'].values[0]\n    sample_id.drop(columns=['installation_id','game_session','world'], inplace=True, axis=1)\n    ## find the user has previous assessment or not\n    idx = list(sample_id[sample_id.accuracy.notnull()].index)\n    b = sample_id.index[0]\n    ## if train and have more than two assessment : len(idx) > 1\n    ## if train and only have one assessment :len(idx) = 1\n    ## if test and have previous assessment: len(idx) > 0\n    ## if test and no previous assessment: len(idx) = 0 \n    if sample_id['flag'].values[0] == 0:\n        idx.append(sample_id.index[-1])\n    for e in idx:\n        one_block = sample_id.loc[b:e-1]\n        features = {}\n        drop_cols = ['session_title','session_type','start_time', 'end_time', 'flag']\n        for col in one_block.columns.drop(drop_cols):\n            features[str(col)+'_sum'] = one_block[col].sum()\n#             features[str(col)+'_mean'] = one_block[col].mean()\n#             features[str(col)+'_max'] = one_block[col].max()\n#             features[str(col)+'_min'] = one_block[col].min()\n#             features[str(col)+'_std'] = one_block[col].std()\n#             if len(one_block[col].mode()) != 0:\n#                 features[str(col)+'_mode'] = one_block[col].mode()[0]\n#             features[str(col)+'_skew'] = one_block[col].skew()\n        features['accuracy_target'] = sample_id['accuracy'].loc[e]\n        features['accuracy_group_target'] = sample_id['accuracy_group'].loc[e]\n        features['installation_id'] = installation_id\n        features['start_time'] = sample_id['start_time'].loc[e]\n        features['session_title'] = sample_id['session_title'].loc[e]\n        features['flag'] = sample_id['flag'].values[0]\n        feature_df.append(features)\n    return feature_df","ef84595c":"myList = []\nfor i, sample_id in compiled_df.groupby('installation_id', sort=False):\n    myList.append(sample_id)\ninputs = tqdm_notebook(myList)\nfeature_df = []\nfeature_df = Parallel(n_jobs=8)(delayed(block2feature)(sample_id) for sample_id in inputs)\nfeature_df = [l for f in feature_df for l in f]\nfeature_df = pd.DataFrame(feature_df)\nfeature_df.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in feature_df.columns]","217c3b6a":"feature_df['hour'] = feature_df['start_time'].dt.hour\nfeature_df['weekday'] = feature_df['start_time'].dt.weekday\nfeature_df['is_weekday'] = feature_df['weekday'] < 5\nfeature_df.drop(['start_time', 'accuracy_target'], axis=1, inplace=True)\n\n# category\ntitle2no = {t:n for n, t in enumerate(titles)}\ncat_features = ['session_title']\nfor c in cat_features:\n    feature_df[c] = feature_df[c].map(title2no)","3dc0e5fa":"del compiled_df\ngc.collect()","1e2c8d60":"features = feature_df.columns.drop(['accuracy_group_target','installation_id'])\nreduce_train_org = feature_df[feature_df.accuracy_group_target.notnull()]\nreduce_test = feature_df[feature_df.accuracy_group_target.isnull()]\nreduce_train_org.fillna(-1, inplace=True)\nreduce_test.fillna(-1, inplace=True)","9ea78929":"import time\nimport lightgbm as lgb\nfrom math import sqrt\nfrom sklearn.metrics import mean_squared_error\n\ndef get_feature_importances(data,features, cat_features, target, shuffle, seed=None):\n    # Shuffle target if required\n    y = data[target].copy()\n    if shuffle:\n        # Here you could as well use a binomial distribution\n        y = data[target].copy().sample(frac=1.0)\n    \n    # Fit LightGBM in RF mode, yes it's quicker than sklearn RandomForest\n    dtrain = lgb.Dataset(data[features], y, free_raw_data=False, silent=False, categorical_feature=cat_features)\n\n    params = {\n            'n_estimators':1000,\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': 'rmse',\n            'subsample': 0.75,\n            'subsample_freq': 12,\n            'learning_rate': 0.03341868192252964,\n            'feature_fraction': 0.9219472462181388,\n            'max_depth': 13,\n            'lambda_l1': 0.8355562498835661,  \n            'lambda_l2': 0.09460962025087172,\n            'bagging_seed':seed\n            }\n    \n    \n    # Fit the model\n    clf = lgb.train(params=params, train_set=dtrain,verbose_eval=500)\n\n    # Get feature importances\n    imp_df = pd.DataFrame()\n    imp_df[\"feature\"] = list(features)\n    imp_df[\"importance_gain\"] = clf.feature_importance(importance_type='gain')\n    imp_df[\"importance_split\"] = clf.feature_importance(importance_type='split')\n    imp_df['trn_score'] = sqrt(mean_squared_error(y, clf.predict(data[features])))\n    \n    return imp_df","610a1e9d":"# Get the actual importance, i.e. without shuffling\nactual_imp_df1 = get_feature_importances(data=reduce_train_org,features=features,cat_features=cat_features, target='accuracy_group_target', shuffle=False, seed=42)\nactual_imp_df2 = get_feature_importances(data=reduce_train_org,features=features,cat_features=cat_features, target='accuracy_group_target', shuffle=False, seed=88)\nactual_imp_df3 = get_feature_importances(data=reduce_train_org,features=features,cat_features=cat_features, target='accuracy_group_target', shuffle=False, seed=999)\nactual_imp_df4 = get_feature_importances(data=reduce_train_org,features=features,cat_features=cat_features, target='accuracy_group_target', shuffle=False, seed=2020)\nactual_imp_df5 = get_feature_importances(data=reduce_train_org,features=features,cat_features=cat_features, target='accuracy_group_target', shuffle=False, seed=1000)\nactual_imp_df = actual_imp_df1.copy()\nactual_imp_df['importance_gain'] = (actual_imp_df1['importance_gain'] + actual_imp_df2['importance_gain'] +actual_imp_df3['importance_gain']+actual_imp_df4['importance_gain']+actual_imp_df5['importance_gain'])\/5","8f219e68":"null_imp_df = pd.DataFrame()\nnb_runs = 30\nimport time\nstart = time.time()\ndsp = ''\nfor i in range(nb_runs):\n    # Get current run importances\n    imp_df = get_feature_importances(data=reduce_train_org,features=features,cat_features=cat_features, target='accuracy_group_target', shuffle=True)\n    imp_df['run'] = i + 1 \n    # Concat the latest importances with the old ones\n    null_imp_df = pd.concat([null_imp_df, imp_df], axis=0)\n    # Erase previous message\n    for l in range(len(dsp)):\n        print('\\b', end='', flush=True)\n    # Display current run and time used\n    spent = (time.time() - start) \/ 60\n    dsp = 'Done with %4d of %4d (Spent %5.1f min)' % (i + 1, nb_runs, spent)\n    print(dsp, end='', flush=True) ","2b6dd0a3":"import matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\n%matplotlib inline\n\nfeature_scores = []\nfor _f in actual_imp_df['feature'].unique():\n    f_null_imps_gain = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_gain'].values\n    f_act_imps_gain = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_gain'].mean()\n    # act_importance should be much bigger than null importance\n    gain_score = np.log(1e-10 + f_act_imps_gain \/ (1 + np.percentile(f_null_imps_gain, 75)))  # Avoid didvide by zero\n    f_null_imps_split = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_split'].values\n    f_act_imps_split = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_split'].mean()\n    split_score = np.log(1e-10 + f_act_imps_split \/ (1 + np.percentile(f_null_imps_split, 75)))  # Avoid didvide by zero\n    feature_scores.append((_f, split_score, gain_score))\n\nscores_df = pd.DataFrame(feature_scores, columns=['feature', 'split_score', 'gain_score'])\n\nplt.figure(figsize=(20, 20))\ngs = gridspec.GridSpec(1, 2)\n# Plot Gain importances\nax = plt.subplot(gs[0, 1])\nsns.barplot(x='gain_score', y='feature', data=scores_df.sort_values('gain_score', ascending=False).iloc[0:100], ax=ax)\nax.set_title('Feature scores wrt gain importances', fontweight='bold', fontsize=14)\nplt.tight_layout()\n\npd.set_option('max_rows',2000)\nnew_list = scores_df.sort_values(by=['gain_score'],ascending=False).reset_index(drop=True)\nnew_list.head(2000)\n\nfor item in new_list['feature']:\n    #print (item) \n    print ('\"' + str(item) +  '\",')   ","0340579b":"reduce_valid = pd.DataFrame()\nfor i, row in reduce_train_org.groupby('installation_id', sort=False):\n    reduce_valid = reduce_valid.append(row.sample(1))\nreduce_train = reduce_train_org.drop(reduce_valid.index)","27ee639f":"features = new_list.loc[new_list.gain_score >= 0.05, 'feature'].values","ed7769de":"params = {\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': 'rmse',\n            'subsample': 0.75,\n            'subsample_freq': 12,\n            'learning_rate': 0.03341868192252964,\n            'feature_fraction': 0.9219472462181388,\n            'max_depth': 13,\n            'lambda_l1': 0.8355562498835661,  \n            'lambda_l2': 0.09460962025087172,\n            }","273320e5":"target = 'accuracy_group_target'\n\ntrain_x, train_y = reduce_train[features], reduce_train[target]\nvalid_x, valid_y = reduce_valid[features], reduce_valid[target]\nprint ('train_x shape:',train_x.shape)\nprint ('valid_x shape:',valid_x.shape)\ndtrain = lgb.Dataset(train_x, label=train_y,categorical_feature=cat_features)\ndval = lgb.Dataset(valid_x, label=valid_y, reference=dtrain,categorical_feature=cat_features) \nbst = lgb.train(params, dtrain, num_boost_round=50000,categorical_feature = cat_features,\n    valid_sets=[dval,dtrain], verbose_eval=500,early_stopping_rounds=300)\nvalid_pred = bst.predict(valid_x, num_iteration=bst.best_iteration)","dc982350":"from sklearn.metrics import cohen_kappa_score\ndist = Counter(reduce_train_org['accuracy_group_target'])\nfor k in dist:\n    dist[k] \/= len(reduce_train_org)\nreduce_train_org['accuracy_group_target'].hist()\nacum = 0\nbound = {}\nfor i in range(3):\n    acum += dist[i]\n    bound[i] = np.percentile(valid_pred, acum * 100)\nprint(bound)\n\ndef classify(x):\n    if x <= bound[0]:\n        return 0\n    elif x <= bound[1]:\n        return 1\n    elif x <= bound[2]:\n        return 2\n    else:\n        return 3\nvalid_pred = np.array(list(map(classify, valid_pred))).reshape(valid_y.shape)\nprint(cohen_kappa_score(valid_y, valid_pred, weights='quadratic'))","81edd0bb":"test_pred = bst.predict(reduce_test[features], num_iteration=bst.best_iteration)\ndist = Counter(reduce_valid['accuracy_group_target'])\nfor k in dist:\n    dist[k] \/= len(reduce_valid)\nreduce_valid['accuracy_group_target'].hist()\nacum = 0\nbound = {}\nfor i in range(3):\n    acum += dist[i]\n    bound[i] = np.percentile(test_pred, acum * 100)\nprint(bound)\n\ndef classify(x):\n    if x <= bound[0]:\n        return 0\n    elif x <= bound[1]:\n        return 1\n    elif x <= bound[2]:\n        return 2\n    else:\n        return 3\ntest_pred = np.array(list(map(classify, test_pred)))\nsample_submission['accuracy_group'] = test_pred.astype(int)\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission['accuracy_group'].value_counts(normalize=True)","39bd2479":"This part is to convert agg rows inside each session. (The aggregation is simply sum, you could dvide the feature by event_code_count to get average)","8c34dee3":"### 2.3 Feature Selection","11dec565":"### 2.1 Row to Session","e31ca0ff":"### 3.1 Split Trian and Valid","6d9b9c25":"This part is to agg session into training feature, here I only sum up sessions, but you can do other type of aggregation easily","99131df7":"### 3.2 Train","8e976a9a":"## 2. Feature engineering","3f961639":"### 2.2 Session to Train feature","a4e48e7f":"# What the kernel about\n\n1. Two step feature aggregation\n    1. agg low level information to session level\n    2. agg session level information to training feature\n2. Null importance to select features\n3. Truncted training data to do validation \n4. optimize bound use trunc data distribution","3493fd09":"## 3. Train","1afd0696":"**Hope some one could improve the speed of the kernel, and also share more feature engineering idea. Glad to hear advice and bug report**","e2242cee":"### Submit","694f114d":"## 1. Load Data"}}