{"cell_type":{"7ca735b3":"code","4cea1f4a":"code","ccefd6b7":"code","1806169f":"code","55b1080e":"code","fbdd7665":"code","fde9adbe":"code","773e4708":"code","8c3f2573":"code","d610c798":"code","86facbe6":"code","39831701":"code","0f8387f4":"code","e00fa210":"code","91318f8f":"code","48f2ed54":"code","7790c840":"code","e55103f9":"code","d748771e":"code","c2c24967":"code","f99ad7ec":"code","bcee5fe0":"code","3cfc18f7":"code","37730fca":"code","c8add370":"code","a265efc6":"code","24c28b6b":"code","09484a9f":"code","06fa10d7":"code","9b36e24e":"code","b640b6f0":"code","c19b42d6":"markdown","85702412":"markdown","9c7f9169":"markdown","4c4e9084":"markdown","31fbcbf3":"markdown","44f585d2":"markdown","7397345a":"markdown"},"source":{"7ca735b3":"# data manuipulation\nimport numpy as np\nimport pandas as pd\n\n# modeling utilities\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_predict, train_test_split\n\n\n# plotting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Plotting parameters tuning\nsns.set_style('whitegrid')\nsns.set_context('talk')\nparams = {'legend.fontsize': 'x-large',\n          'figure.figsize': (30, 10),\n          'axes.labelsize': 'x-large',\n          'axes.titlesize':'x-large',\n          'xtick.labelsize':'x-large',\n          'ytick.labelsize':'x-large'}\n\nplt.rcParams.update(params)","4cea1f4a":"hour_df = pd.read_csv(\"..\/input\/bike-sharing-dataset\/hour.csv\")\nhour_df.info()","ccefd6b7":"# Renaming columns names to more readable names\nhour_df.rename(columns={'instant':'rec_id',\n                        'dteday':'datetime',\n                        'holiday':'is_holiday',\n                        'workingday':'is_workingday',\n                        'weathersit':'weather_condition',\n                        'hum':'humidity',\n                        'mnth':'month',\n                        'cnt':'total_count',\n                        'hr':'hour',\n                        'yr':'year'},inplace=True)\n\n###########################\n# Setting proper data types\n###########################\n# date time conversion\nhour_df['datetime'] = pd.to_datetime(hour_df.datetime)\n\n# categorical variables\nhour_df['season'] = hour_df.season.astype('category')\nhour_df['is_holiday'] = hour_df.is_holiday.astype('category')\nhour_df['weekday'] = hour_df.weekday.astype('category')\nhour_df['weather_condition'] = hour_df.weather_condition.astype('category')\nhour_df['is_workingday'] = hour_df.is_workingday.astype('category')\nhour_df['month'] = hour_df.month.astype('category')\nhour_df['year'] = hour_df.year.astype('category')\nhour_df['hour'] = hour_df.hour.astype('category')","1806169f":"# Defining categorical variables encoder method\ndef fit_transform_ohe(df,col_name):\n\n    # label encode the column\n    le = preprocessing.LabelEncoder()\n    le_labels = le.fit_transform(df[col_name])\n    df[col_name+'_label'] = le_labels\n    # one hot encoding\n    ohe = preprocessing.OneHotEncoder()\n    feature_arr = ohe.fit_transform(df[[col_name+'_label']]).toarray()\n    feature_labels = [col_name+'_'+str(cls_label) for cls_label in le.classes_]\n    features_df = pd.DataFrame(feature_arr, columns=feature_labels)\n    return le,ohe,features_df\n\n# given label encoder and one hot encoder objects, \n# encode attribute to ohe\ndef transform_ohe(df,le,ohe,col_name):\n   \n    # label encode\n    col_labels = le.transform(df[col_name])\n    df[col_name+'_label'] = col_labels\n    \n    # ohe \n    feature_arr = ohe.fit_transform(df[[col_name+'_label']]).toarray()\n    feature_labels = [col_name+'_'+str(cls_label) for cls_label in le.classes_]\n    features_df = pd.DataFrame(feature_arr, columns=feature_labels)\n    \n    return features_df","55b1080e":"# Divide the dataset into training and testing sets\nX, X_test, y, y_test = train_test_split(hour_df.iloc[:,0:-3],\n                                        hour_df.iloc[:,-1],\n                                        test_size=0.33,\n                                        random_state=42)\nX.reset_index(inplace=True)\ny = y.reset_index()\n\nX_test.reset_index(inplace=True)\ny_test = y_test.reset_index()\nX","fbdd7665":"hour_df.shape","fde9adbe":"# Encoding all the categorical features\ncat_attr_list = ['season','is_holiday',\n                 'weather_condition','is_workingday',\n                 'hour','weekday','month','year']\n# though we have transformed all categoricals into their one-hot encodings, note that ordinal\n# attributes such as hour, weekday, and so on do not require such encoding.\nnumeric_feature_cols = ['temp','humidity','windspeed',\n                        'hour','weekday','month','year']\nsubset_cat_features =  ['season','is_holiday','weather_condition','is_workingday']\n\n###############\n# Train dataset\n###############\nencoded_attr_list = []\nfor col in cat_attr_list:\n    return_obj = fit_transform_ohe(X,col)\n    encoded_attr_list.append({'label_enc':return_obj[0],\n                              'ohe_enc':return_obj[1],\n                              'feature_df':return_obj[2],\n                              'col_name':col})\n\n\nfeature_df_list  = [X[numeric_feature_cols]]\nfeature_df_list.extend([enc['feature_df'] \\\n                        for enc in encoded_attr_list \\\n                        if enc['col_name'] in subset_cat_features])\n\ntrain_df_new = pd.concat(feature_df_list, axis=1)\nprint(\"Train dataset shape::{}\".format(train_df_new.shape))\nprint(train_df_new.head())\n\n##############\n# Test dataset\n##############\ntest_encoded_attr_list = []\nfor enc in encoded_attr_list:\n    col_name = enc['col_name']\n    le = enc['label_enc']\n    ohe = enc['ohe_enc']\n    test_encoded_attr_list.append({'feature_df':transform_ohe(X_test,\n                                                              le,ohe,\n                                                              col_name),\n                                   'col_name':col_name})\n    \n    \ntest_feature_df_list = [X_test[numeric_feature_cols]]\ntest_feature_df_list.extend([enc['feature_df'] \\\n                             for enc in test_encoded_attr_list \\\n                             if enc['col_name'] in subset_cat_features])\n\ntest_df_new = pd.concat(test_feature_df_list, axis=1) \nprint(\"Test dataset shape::{}\".format(test_df_new.shape))\nprint(test_df_new.head())","773e4708":"# Constructing train dataset\nX = train_df_new\ny= y.total_count.values.reshape(-1,1)\n\n# Constructing test dataset\nX_test = test_df_new\ny_test = y_test.total_count.values.reshape(-1,1)\nprint(X.shape,y.shape)","8c3f2573":"dtm = DecisionTreeRegressor(max_depth=4,\n                           #min_samples_split=5,\n                           #max_leaf_nodes=10\n                           )\n\ndtm.fit(X,y)\nprint(\"R-Squared on train dataset={}\".format(dtm.score(X_test,y_test)))\n\ndtm.fit(X_test,y_test)   \nprint(\"R-Squared on test dataset={}\".format(dtm.score(X_test,y_test)))","d610c798":"!pip install --upgrade scikit-learn==0.21.3\n!pip install pydotplus\n#!pip install scikit-learn joblib\n#!pip3 install --user sklearn","86facbe6":"# Importing required packages for visualization\nfrom IPython.display import Image  \nfrom sklearn.externals.six import StringIO  \nfrom sklearn.tree import export_graphviz\nimport pydotplus, graphviz","39831701":"# Putting features\nfeatures = list(train_df_new.columns[:])\nfeatures\n#hour_df","0f8387f4":"# plotting the tree\ndot_data = StringIO()  \nexport_graphviz(dtm, out_file=dot_data,feature_names=features,filled=True,rounded=True)\n\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())","e00fa210":"# GridSearchCV to find optimal max_depth\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'max_depth': range(1, 40)}\n\n# instantiate the model\ndtree = DecisionTreeRegressor(criterion = \"mse\", \n                               random_state = 100)\n\n# fit tree on training data\ntree = GridSearchCV(dtree, parameters, \n                    cv=n_folds,\n                    return_train_score=True,\n                   scoring=\"r2\")\ntree.fit(X, y)","91318f8f":"# scores of GridSearch CV\nscores = tree.cv_results_\npd.DataFrame(scores).head()","48f2ed54":"# plotting r2 with max_depth\nplt.figure()\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_train_score\"], \n         label=\"training r2\")\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_test_score\"], \n         label=\"test r2\")\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"r2\")\nplt.legend()\nplt.show()\n","7790c840":"# GridSearchCV to find optimal min_samples_leaf\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_leaf': range(5, 200, 20)}\n\n# instantiate the model\ndtree = DecisionTreeRegressor(criterion = \"mse\", \n                               random_state = 100)\n\n# fit tree on training data\ntree = GridSearchCV(dtree, parameters, \n                    cv=n_folds, \n                    return_train_score=True,\n                   scoring=\"r2\")\ntree.fit(X, y)","e55103f9":"# scores of GridSearch CV\nscores = tree.cv_results_\npd.DataFrame(scores).head()","d748771e":"# plotting r2 with min_samples_leaf\nplt.figure()\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_train_score\"], \n         label=\"training r2\")\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_test_score\"], \n         label=\"test r2\")\nplt.xlabel(\"min_samples_leaf\")\nplt.ylabel(\"r2\")\nplt.legend()\nplt.show()","c2c24967":"# GridSearchCV to find optimal min_samples_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_split': range(5, 200, 20)}\n\n# instantiate the model\ndtree = DecisionTreeRegressor(criterion = \"mse\", \n                               random_state = 100)\n\n# fit tree on training data\ntree = GridSearchCV(dtree, parameters, \n                    cv=n_folds, \n                   return_train_score=True,\n                   scoring=\"r2\")\ntree.fit(X, y)","f99ad7ec":"# scores of GridSearch CV\nscores = tree.cv_results_\npd.DataFrame(scores).head()","bcee5fe0":"# plotting accuracies with min_samples_leaf\nplt.figure()\nplt.plot(scores[\"param_min_samples_split\"], \n         scores[\"mean_train_score\"], \n         label=\"training r2\")\nplt.plot(scores[\"param_min_samples_split\"], \n         scores[\"mean_test_score\"], \n         label=\"test r2\")\nplt.xlabel(\"min_samples_split\")\nplt.ylabel(\"r2\")\nplt.legend()\nplt.show()\n","3cfc18f7":"param_grid = {\n    'max_depth': range(5, 15, 5),\n    'min_samples_leaf': range(50, 150, 50),\n    'min_samples_split': range(50, 150, 50),\n    'criterion': [\"mse\", \"mae\"]\n}\n\n\ngrid_cv_dtm = GridSearchCV(dtm, param_grid, cv=5)\n\ngrid_cv_dtm.fit(X,y)\n\n\n","37730fca":"print(\"R-Squared::{}\".format(grid_cv_dtm.best_score_))\nprint(\"Best Hyperparameters::\\n{}\".format(grid_cv_dtm.best_params_))","c8add370":"df = pd.DataFrame(data=grid_cv_dtm.cv_results_)\ndf.head()","a265efc6":"# model with optimal hyperparameters\nclf = DecisionTreeRegressor(criterion = \"mse\", \n                                  random_state = 100,\n                                  max_depth=10, \n                                  min_samples_leaf=50,\n                                  min_samples_split=50)\n\nclf.fit(X, y)\n#clf_gini.fit(X_train, y_train){'criterion': 'mse', 'max_depth': 10, 'min_samples_leaf': 50, 'min_samples_split': 50}","24c28b6b":"# plotting tree with max_depth=10\ndot_data = StringIO()  \nexport_graphviz(clf, out_file=dot_data,feature_names=features,filled=True,rounded=True)\n\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())","09484a9f":"# model with optimal hyperparameters\nclf = DecisionTreeRegressor(criterion = \"mse\", \n                                  random_state = 100,\n                                  max_depth=3, \n                                  min_samples_leaf=50,\n                                  min_samples_split=50)\n\nclf.fit(X, y)","06fa10d7":"# plotting tree with max_depth=3 or 4\ndot_data = StringIO()  \nexport_graphviz(clf, out_file=dot_data,feature_names=features,filled=True,rounded=True)\n\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())","9b36e24e":"# Checking the training model scores\nr2_scores = cross_val_score(grid_cv_dtm.best_estimator_, X, y, cv=10)\nmse_scores = cross_val_score(grid_cv_dtm.best_estimator_, X, y, cv=10,scoring='neg_mean_squared_error')\n\nprint(\"avg R-squared::{:.3f}\".format(np.mean(r2_scores)))\nprint(\"MSE::{:.3f}\".format(np.mean(mse_scores)))","b640b6f0":"best_dtm_model = grid_cv_dtm.best_estimator_\n\ny_pred = best_dtm_model.predict(X_test)\nresiduals = y_test.flatten() - y_pred\n\n\nr2_score = best_dtm_model.score(X_test,y_test)\nprint(\"R-squared:{:.3f}\".format(r2_score))\nprint(\"MSE: %.2f\" % metrics.mean_squared_error(y_test, y_pred))","c19b42d6":"#### Running the model with best parameters obtained from grid search.","85702412":"## Hyperparameter tuning with GridSearchCV","9c7f9169":"# Decision Tree Regressor on Bike Sharing Dataset","4c4e9084":"### # Divide the dataset into training and testing sets","31fbcbf3":"### DecisionTreeRegressor ","44f585d2":"### You can see that this tree is too complex to understand. Let's try reducing the max_depth and see how the tree looks.","7397345a":"### Import libraries"}}