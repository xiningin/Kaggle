{"cell_type":{"92ffc264":"code","6dea41af":"code","d851dd30":"code","b6e01046":"code","682ead62":"code","907ef0ab":"code","030cb3d8":"code","bdd9d1a7":"code","87898ea8":"code","54faf439":"code","df9f6bb5":"code","15f97aa1":"code","f17c0151":"code","dbcc14c1":"code","2b1527fc":"code","d1d9f5ba":"code","a269e431":"code","b8b78319":"code","77212319":"code","54214bc2":"code","85e7087b":"code","fc724c81":"code","49c3d2fd":"code","abe61413":"code","a3fe2c66":"code","e6525e60":"code","831d20a9":"code","229b23bc":"code","810483b9":"code","3351d33a":"code","b8373509":"code","b39227ea":"code","e3318301":"code","66ec26af":"code","aa14336c":"code","eeb09bb1":"code","4438fe7e":"code","e91e76dc":"code","09eb982b":"code","eb734fb4":"code","059d0be6":"code","029c0feb":"code","9e65da19":"code","98e62565":"code","797f5f62":"code","9c0b5bfd":"code","91d0bc7d":"code","9dc691b8":"code","2f787516":"code","e8506335":"code","f0a69649":"code","8c55573a":"code","d566f50f":"code","1bbcdd01":"code","b5966c88":"code","9a04041e":"code","564c2e68":"code","ccb62146":"code","f4ba737e":"code","7b5bb778":"code","0c773143":"markdown","78e57c3b":"markdown","edd7f93f":"markdown","b9110a8c":"markdown","a4ee3c25":"markdown","8d955539":"markdown","29654219":"markdown","5f0f6e18":"markdown","8b5281a0":"markdown","4c9d2f1e":"markdown","1008d5c7":"markdown","3f1eb9f5":"markdown","79d71792":"markdown","897ed450":"markdown","a7c31c71":"markdown","304ede16":"markdown","69e39645":"markdown","f8410a5a":"markdown","37a9a5a0":"markdown","0348070d":"markdown"},"source":{"92ffc264":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","6dea41af":"df = pd.read_csv('..\/input\/spam-email-from-enron-dataset\/labeled_emails.csv')","d851dd30":"df.head()","b6e01046":"df.iloc[:,1].unique()","682ead62":"#Select example to visualize\nn_mail = 18","907ef0ab":"df['email'][n_mail]","030cb3d8":"import nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords","bdd9d1a7":"stop = set(stopwords.words('english')) \nprint(stop)","87898ea8":"from nltk.stem import SnowballStemmer\nimport re\nsnow = nltk.stem.SnowballStemmer('english')","54faf439":"#Funcion de preprocesamiento\ndef preprocesamiento_words(sentence):\n    #Minisculas\n    sentence=sentence.lower() \n    #Remoci\u00f3n de HTML\n    cleanr = re.compile('<.*?>')\n    sentence = re.sub(cleanr, ' ', sentence)\n    #Normalizacion URLs\n    cleanr = re.compile('(http|https):\/\/[^\\s]*')\n    sentence = re.sub(cleanr, 'httpaddr', sentence)\n    #Removing Punctuations\n    sentence = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    sentence = re.sub(r'[.|,|)|(|\\|\/]',r' ',sentence)\n    #Normalizacion de Direcciones de Correo Electronico\n    cleanr = re.compile('[^\\s]+@[^\\s]+.com')\n    sentence = re.sub(cleanr, 'emailaddr', sentence)\n    #Normalizacion de Numeros\n    cleanr = re.compile('[0-9]+')\n    sentence = re.sub(cleanr, 'number', sentence)\n    #Normalizacion de $\n    cleanr = re.compile('[$]+')\n    sentence = re.sub(cleanr, 'dollar', sentence)\n    #Remoci\u00f3n de no-palabras (caracteres no alfanumericos)\n    cleanr = re.compile('[^a-zA-Z0-9]')\n    sentence = re.sub(cleanr, ' ', sentence)\n    #Remoci\u00f3n de 'subject'\n    cleanr = re.compile('subject')\n    sentence = re.sub(cleanr, ' ', sentence)\n    #Remoci\u00f3n de stop-words y lematizaci\u00f3n\n    words = [snow.stem(word) for word in sentence.split() if word not in stopwords.words('english')]   # Stemming and removing stopwords\n    return words","df9f6bb5":"text_list = []\ncounter = 0\n#Applying Preprocessing function\nfor sentence in df['email']:\n    text_list.append(preprocesamiento_words(sentence))\n    counter += 1\n    print('\\r{}\/{}'.format(counter,len(df['email'])),end='')","15f97aa1":"print(text_list[n_mail])","f17c0151":"email_process = []\ncounter = 0\nfor row in text_list:\n    sequ = ''\n    for word in row:\n        sequ = sequ + ' ' + word\n    email_process.append(sequ)\n    counter += 1\n    print('\\r{}\/{}'.format(counter,len(text_list)),end='')","dbcc14c1":"email_process[n_mail]","2b1527fc":"#Print email 20 words per row and count number words\nprint('Count words in email: {}'.format(len(text_list[n_mail])))\nprint('\\nEmail list words:\\n')\n#Print list words in email\ncount = 0\nfor word in text_list[n_mail]:\n  print(word,end=' ')\n  count += 1\n  if count == 20:\n    print('')\n    count = 0","d1d9f5ba":"from sklearn.feature_extraction.text import CountVectorizer","a269e431":"#Funci\u00f3n tokenization\ndef email_tokenization(data,features=500):\n  count_vect = CountVectorizer(max_features=features)\n  count_matrix = count_vect.fit_transform(data)\n  count_array = count_matrix.toarray()\n  tokens = pd.DataFrame(data=count_array,columns = count_vect.get_feature_names())\n  voca = count_vect.vocabulary_\n  return (tokens, voca)","b8b78319":"tokens, voca = email_tokenization(email_process, features=2000)\nprint(voca)","77212319":"print(tokens.shape)","54214bc2":"tokens.head()","85e7087b":"tokens.iloc[[n_mail]]","fc724c81":"for key in text_list[n_mail]:\n  if voca.get(key):\n    print(voca[key], end=\" \")","49c3d2fd":"#Print token email 20 words per row and count number words\nprint('Count keys in email: {}'.format(len(text_list[n_mail])))\nprint('\\nEmail list keys:\\n')\n#Print list words in email\ncount = 0\nfor key in text_list[n_mail]:\n  if voca.get(key):\n    print(voca[key],end=' ')\n    count += 1\n  if count == 20:\n    print('')\n    count = 0","abe61413":"from sklearn.preprocessing import LabelBinarizer","a3fe2c66":"lb = LabelBinarizer()\nlb.fit(df['label'])\nprint(lb.classes_)\ny = lb.transform(df['label'])","e6525e60":"print('Spam\/Total')\nprint('{}\/{} '.format(int(sum(y)),len(y)))\nprint('Spam proportion = {:0.2f}'.format(int(sum(y))\/len(y)))","831d20a9":"export_data = tokens\nexport_data['label'] = y\nexport_data.head()","229b23bc":"export_data.to_csv('token_mails_2000f_labeled.csv', index=False)","810483b9":"df = pd.read_csv('.\/token_mails_2000f_labeled.csv')","3351d33a":"X = df.iloc[:,0:-1]","b8373509":"y = df['label']","b39227ea":"print(X.shape)\nprint(y.shape)","e3318301":"from sklearn.model_selection import train_test_split","66ec26af":"#test size is 20%\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20)","aa14336c":"pd.DataFrame([len(X),len(X_train),len(X_test)],\n             index=['Total','Train','Test'],\n             columns=['Size'])","eeb09bb1":"print('Spam\/Total')\nprint('{}\/{} '.format(int(sum(y_train)),len(y_train)))\nprint('Spam proportion = {:0.2f}'.format(int(sum(y_train))\/len(y_train)))","4438fe7e":"from sklearn.naive_bayes import GaussianNB","e91e76dc":"gnb = GaussianNB()","09eb982b":"from sklearn.model_selection import learning_curve","eb734fb4":"train_sizes_fraction = np.arange(0.1,0.85,0.05)\ntrain_sizes = np.array(train_sizes_fraction*len(y_train)).astype(int)\nprint('Train sizes')\nprint(train_sizes)","059d0be6":"train_sizes, train_scores, valid_scores = learning_curve(gnb,X_train,y_train, train_sizes=train_sizes.astype(int), cv=5,scoring='f1')","029c0feb":"cv_results = pd.DataFrame([np.round(train_sizes),np.mean(train_scores,axis=1),np.mean(valid_scores,axis=1)],\n             index=['Training size','Training F1-score','CV F1-score']).T\ncv_results = cv_results.sort_values(by='CV F1-score', ascending=False)\ncv_results","9e65da19":"plt.figure(figsize=(10,10))\nplt.style.use('bmh');\nplt.plot(train_sizes,np.mean(train_scores,axis=1),'-o',label='Training score',);\nplt.plot(train_sizes,np.mean(valid_scores,axis=1),'-o',label='Cross-validation score');\nplt.ylim((0.5,1))\nplt.legend(loc=4,frameon=True);\nplt.xlabel('Training size');\nplt.ylabel('F1-Score');\nplt.title('Gaussian Naive Bayes');","98e62565":"from sklearn.metrics import f1_score","797f5f62":"gnb.fit(X_train,y_train)\nprint('GNB score: ', f1_score(y_test,gnb.predict(X_test)))","9c0b5bfd":"iterat = np.arange(200,2001,200)\nbest_models = np.zeros((len(iterat),4))\ncounter = 0\n\nplt.figure(figsize=(20,10))\nplt.style.use('bmh');\n\nfor n_features in iterat:\n  #Cross-Validation for n features\n  print('\\r{}\/{}'.format(counter+1,len(iterat)),end='')\n  (train_sizes, train_scores, valid_scores, \n   fit_time, score_time) = learning_curve(gnb,X_train.iloc[:,0:n_features],\n                                          y_train, train_sizes=train_sizes.astype(int),\n                                          cv=5,scoring='f1',return_times=True)\n   \n  cv_results = pd.DataFrame([np.round(train_sizes),np.mean(train_scores,axis=1),\n                             np.mean(valid_scores,axis=1),np.mean(fit_time,axis=1),\n                             np.mean(score_time,axis=1)],\n             index=['Training size','Training scores','CV scores','Fit time','Score time']).T\n  #Learning curves\n  plt.subplot(2,5,counter+1)\n  plt.plot(train_sizes,np.mean(train_scores,axis=1),'-o',label='Training score',);\n  plt.plot(train_sizes,np.mean(valid_scores,axis=1),'-o',label='Cross-validation score');\n  plt.ylim((0.5,1))\n  plt.legend(loc=4,frameon=True);\n  plt.xlabel('Training size');\n  plt.ylabel('F1-Score');\n  plt.title('Features: {}'.format(n_features));\n\n  #Extract best CV Score\n  best_models[counter,0] = n_features\n  best_models[counter,1] = cv_results[cv_results['CV scores']==cv_results['CV scores'].max()]['CV scores']\n  best_models[counter,2] = cv_results[cv_results['CV scores']==cv_results['CV scores'].max()]['Fit time']\n  best_models[counter,3] = cv_results[cv_results['CV scores']==cv_results['CV scores'].max()]['Score time']\n  counter += 1","91d0bc7d":"best_models = pd.DataFrame(best_models,\n             columns=['Features','CV F1-scores','Fit time','Score time'])\nbest_models","9dc691b8":"plt.figure(figsize=(10,5))\nplt.subplot(1,2,1)\nplt.plot(best_models['Features'],best_models['CV F1-scores'],'-o');\nplt.xlabel('Features');\nplt.ylabel('CV F1-Score');\n\nplt.subplot(1,2,2)\nplt.plot(best_models['Features'],best_models['Fit time'],'-o');\nplt.xlabel('Features');\nplt.ylabel('Fit time');","2f787516":"#Training size for best model\nX_subtrain, X_cv, y_subtrain, y_cv = train_test_split(X_train,y_train,train_size=0.8)","e8506335":"pd.DataFrame([len(X_train),len(X_subtrain),len(X_cv)],\n             index=['Total','Train','Test'],\n             columns=['Size'])","f0a69649":"gnb.fit(X_subtrain, y_subtrain)","8c55573a":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay","d566f50f":"print('y-test size: {}'.format(len(y_test)))\nprint('Ham: {}'.format(len(y_test)-sum(y_test)))\nprint('Spam: {}'.format(sum(y_test)))","1bbcdd01":"gnb.fit(X_train,y_train)","b5966c88":"predictions = gnb.predict(X_test)","9a04041e":"cm = confusion_matrix(y_test, predictions, labels=gnb.classes_);\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=gnb.classes_);\ndisp.plot()\nplt.show()","564c2e68":"from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score","ccb62146":"print('Metrics for best model')\nprint('Accuracy: {:.2f}%'.format(accuracy_score(y_test,predictions)*100))\nprint('Precision: {:.2f}%'.format(precision_score(y_test,predictions)*100))\nprint('Recall: {:.2f}%'.format(recall_score(y_test,predictions)*100))\nprint('F1 score: {:.2f}%'.format(f1_score(y_test,predictions)*100))","f4ba737e":"from sklearn.metrics import classification_report","7b5bb778":"print(classification_report(y_test,predictions,target_names=['ham','spam']))","0c773143":"## Model selection","78e57c3b":"## Dataset split","edd7f93f":"### Learning Curve","b9110a8c":"## Tokenization","a4ee3c25":"## Data Labels","8d955539":"## Best model validation","29654219":"## Preprocess text-words message","5f0f6e18":"## Dataset","8b5281a0":"## Print token email","4c9d2f1e":"# Model train","1008d5c7":"## Print processed email","3f1eb9f5":"## Stopwords","79d71792":"# Basic Librarys","897ed450":"# Data preprocessing","a7c31c71":"## Confusion amtrix and F1 score","304ede16":"## Feature vs performance","69e39645":"Import data","f8410a5a":"## Export process messages and labels","37a9a5a0":"## Cross Validation","0348070d":"### Message Normalization"}}