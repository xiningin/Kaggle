{"cell_type":{"9440e315":"code","9192105c":"code","d69cab2e":"code","eda249e8":"code","b7efdd9d":"code","4f92479d":"code","371c7b84":"code","06281170":"code","6eed846d":"code","909ee212":"code","826df01e":"code","77bb85f0":"code","7541104c":"code","a1a7cc0f":"code","269abea2":"code","afd4f296":"code","f2cf17d9":"code","fcd34f64":"code","3d0f3259":"markdown","7d5e850b":"markdown","73a44326":"markdown","6233199c":"markdown","71294f77":"markdown","f0b1308f":"markdown","2b83480e":"markdown"},"source":{"9440e315":"import numpy as np\nimport pandas as pd\n\nimport catboost as cb\nfrom catboost import CatBoostClassifier\n\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom tqdm import tqdm","9192105c":"%%time\ntrain = pd.read_csv('..\/input\/jane-street-market-prediction\/train.csv')\ntrain = train.astype({c: np.float32 for c in train.select_dtypes(include='float64').columns}) #limit memory use","d69cab2e":"#\u041d\u0430\u043c \u043d\u0435 \u043d\u0443\u0436\u043d\u044b \u0441\u0434\u0435\u043b\u043a\u0438 \u0441 \u043d\u0443\u043b\u0435\u0432\u044b\u043c \u0432\u0435\u0441\u043e\u043c, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043c\u044b \u0438\u0445 \u0438\u0433\u043d\u043e\u0440\u0438\u0440\u0443\u0435\u043c\ntrain = train.query('weight > 0').reset_index(drop = True)\ntrain.shape","eda249e8":"#\u0414\u0430\u043d\u043d\u044b\u0435 \u0431\u0443\u0434\u0443\u0442 \u0441 86 \u0434\u043d\u044f\ntrain = train.query('date > 85').reset_index(drop = True)\ntrain.shape","b7efdd9d":"#\u0417\u0430\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u043f\u0440\u043e\u043f\u0443\u0449\u0435\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0441\u0440\u0435\u0434\u043d\u0438\u043c \ntrain.fillna(train.mean(),inplace=True)","4f92479d":"#\u0413\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f 0 \u0438\u043b\u0438 1 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0439 resp \u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u0438\u0445 \u0432 \u0441\u0442\u043e\u043b\u0431\u0446\u0435 'action'\ntrain['action'] = (train['resp'] > 0 ).astype('int')","371c7b84":"resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp']","06281170":"features_train_data  = train.iloc[:,7:137]","6eed846d":"# \u041d\u0430\u0439\u0434\u0435\u043c \u043f\u0430\u0440\u044b \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0441 \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u0435\u0439 > |0.9|\ndef corrFilter(x: pd.DataFrame, bound: float):\n    xCorr = x.corr()\n    xFiltered = xCorr[((xCorr >= bound) | (xCorr <= -bound)) & (xCorr !=1.000)]\n    xFlattened = xFiltered.unstack().sort_values().drop_duplicates()\n    return xFlattened\n\nhigh_correlations=corrFilter(features_train_data, .9).to_frame()","909ee212":"all_drop_cols = set(high_correlations.index.get_level_values(0))","826df01e":"features = features_train_data.columns.tolist()","77bb85f0":"for i in all_drop_cols:\n    features.remove(i)","7541104c":"f_mean = train.loc[:, features].mean()","a1a7cc0f":"# train: 80%, validate: 20% ( (499-85) * 0.8 + 85 = 416 )\n# train\nX_train = train[train['date'] <= 416]\ny_train = np.stack([(X_train[c]>0).astype('int') for c in resp_cols]).T\nX_train = X_train.loc[:, features].values\nprint(X_train.shape, y_train.shape)\n\n# valid\nX_valid = train[train['date'] > 416]\ny_valid = np.stack([(X_valid[c]>0).astype('int') for c in resp_cols]).T\nX_valid = X_valid.loc[:, features].values\nprint(X_valid.shape, y_valid.shape)","269abea2":"cat_models = [] # \u0441\u043f\u0438\u0441\u043e\u043a \u043c\u043e\u0434\u0435\u043b\u0435\u0439, \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u043e\u0431\u0443\u0447\u0430\u0442\u044c\n\n\nc_model = CatBoostClassifier(iterations = 5000,\n                          depth=10,\n                          learning_rate = 0.1,\n                          random_seed = 42,\n                          eval_metric='Accuracy',\n                          custom_metric=['Logloss', 'AUC'],\n                          od_wait=500,\n                          task_type='GPU',\n                                )\n    \nfor i in tqdm(range(y_train.shape[1])):                         \n    c_model.fit(X_train, y_train[:,i],\n         eval_set=(X_valid, y_valid[:,i]),\n         verbose_eval=100,\n         use_best_model=True,\n         #plot=True\n         )\n    \n    nom_fich = \"weights_target_\" + resp_cols[i]\n    \n    c_model.save_model(nom_fich)\n    \n    cat_models.append(c_model)","afd4f296":"# ------------------------------------------- #\n#                   model                     #\n# ------------------------------------------- #\nHIDDEN_LAYER_1 = [256, 256]\nHIDDEN_LAYER_2 = [160, 160, 160]\nHIDDEN_LAYER_3 = [128, 128, 128, 128]\nTARGET_NUM = 5   # \u041e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u044d\u0442\u0438 5 \u0440\u0435\u0441\u043f\n\ninput = tf.keras.layers.Input(shape=(X_train.shape[1], ))\n\n#part_1\nx1 = tf.keras.layers.BatchNormalization()(input)\nx1 = tf.keras.layers.Dropout(0.25)(x1)\nfor units in HIDDEN_LAYER_1:\n    x1 = tf.keras.layers.Dense(units)(x1)\n    x1 = tf.keras.layers.BatchNormalization()(x1)\n    x1 = tf.keras.layers.Activation(tf.keras.activations.swish)(x1)\n    x1 = tf.keras.layers.Dropout(0.25)(x1)\n\n# part_2\nx2 = tf.keras.layers.BatchNormalization()(input)\nx2 = tf.keras.layers.Dropout(0.25)(x2)\nfor units in HIDDEN_LAYER_2:\n    x2 = tf.keras.layers.Dense(units)(x2)\n    x2 = tf.keras.layers.BatchNormalization()(x2)\n    x2 = tf.keras.layers.Activation(tf.keras.activations.swish)(x2)\n    x2 = tf.keras.layers.Dropout(0.25)(x2)\n    \n# part_3\nx3 = tf.keras.layers.BatchNormalization()(input)\nx3 = tf.keras.layers.Dropout(0.25)(x3)\nfor units in HIDDEN_LAYER_3:\n    x3 = tf.keras.layers.Dense(units)(x3)\n    x3 = tf.keras.layers.BatchNormalization()(x3)\n    x3 = tf.keras.layers.Activation(tf.keras.activations.swish)(x3)\n    x3 = tf.keras.layers.Dropout(0.25)(x3)\n\nx = tf.keras.layers.concatenate([x1, x2, x3])\nx = tf.keras.layers.Dense(TARGET_NUM)(x)\n\noutput = tf.keras.layers.Activation(\"sigmoid\")(x)\n\nmodel = tf.keras.models.Model(inputs=input, outputs=output)\nmodel.compile(\n    optimizer = tfa.optimizers.RectifiedAdam(learning_rate=1e-3),\n    metrics   = tf.keras.metrics.AUC(name=\"AUC\"),\n    loss      = tf.keras.losses.BinaryCrossentropy(label_smoothing=1e-2),\n)","f2cf17d9":"history = model.fit(\n    x = X_train, \n    y = y_train, \n    epochs=25, \n    batch_size=4096, \n    validation_data=(X_valid, y_valid),\n)\nmodels = []\nmodels.append(model)","fcd34f64":"THRESHOLD = 0.5\n\nimport janestreet\nfrom tqdm import tqdm\njanestreet.make_env.__called__ = False\nenv = janestreet.make_env()\n\nprint('predicting...')\n\nfor (test_df, pred_df) in tqdm(env.iter_test()):\n    if test_df['weight'].item() > 0:\n        \n        X_test = test_df.loc[:, features].values\n        if np.isnan(X_test.sum()):  # \u0417\u0430\u043f\u043e\u043b\u043d\u0438\u0442\u0435 numpy, \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u044c \u0431\u0443\u0434\u0435\u0442 \u0431\u044b\u0441\u0442\u0440\u0435\u0435\n            X_test = np.nan_to_num(X_test) + np.isnan(X_test) * f_mean.values\n            \n        # cat_pred\n        cat_pred = np.median(np.stack([c_model.predict(X_test) for c_model in cat_models]),axis=0).T\n            \n        # nn_pred\n        nn_pred = np.median(np.mean([model(X_test, training = False).numpy() for model in models],axis=0))\n            \n        # avg\n        pred = cat_pred * 0.5 + nn_pred * 0.5\n        \n        pred_df.action = np.where(pred >= THRESHOLD, 1, 0).astype(int)\n\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","3d0f3259":"# Prediction","7d5e850b":"# NN model","73a44326":"# Preparing Data","6233199c":"# Catboost","71294f77":"# Import Libraries \ud83d\udcc2","f0b1308f":"# Creating Train and Test DataFrame ","2b83480e":"# Importing Data \ud83d\udcda\n\n"}}