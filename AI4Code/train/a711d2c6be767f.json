{"cell_type":{"52c74d10":"code","b46f7e08":"code","25012044":"code","d378f285":"code","69c2db3c":"code","0fbcfe1c":"code","c6129580":"code","ca4ec525":"code","27d98da3":"code","84e0e4fc":"code","9170db95":"code","837a787a":"code","fe501599":"code","9b71d774":"code","eac10a02":"code","0497579c":"code","442d2382":"code","92c61b6d":"code","03970b58":"code","94b229fc":"code","71cae34b":"code","7fdc7395":"code","872db388":"code","3ecd7fa7":"code","3a2c47b6":"code","be9fe95a":"code","f4d469ec":"code","2764fd0c":"code","e1a636fd":"code","8dd58c71":"code","fc8aeda5":"code","828e8103":"code","e1e0afed":"code","782b80e7":"code","1b693450":"code","60c0ac73":"code","bc7a9e32":"code","5f3be227":"code","efa44c43":"code","faf3ef51":"code","0489317e":"code","efe9f782":"code","d20064ab":"code","2d7ea88a":"code","8f3b16e2":"code","ed50ff95":"code","c1f53155":"code","c12bdd79":"code","5ddadff1":"code","67083f9a":"code","abc405d5":"code","89e03d1b":"code","2a368c87":"code","1ccd207c":"code","61a60411":"code","109c74bf":"code","1b32da56":"code","931771b4":"code","a1c71680":"code","6edb142b":"code","b36ef27d":"code","4bd9d2e6":"code","42645f4c":"code","3a5e4320":"code","4b58436d":"code","b9388af1":"code","a75a96c2":"code","3490e140":"code","83accd6c":"code","7f52cf11":"markdown"},"source":{"52c74d10":"\n# import the required libraries\n\nfrom collections import Counter\nfrom scipy import sparse\nimport numpy as np\nimport pandas as pd\nimport pickle","b46f7e08":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","25012044":"\n# reading train and test file\n\ntrain = pd.read_csv(\"..\/input\/av-recommendation-systems\/train_mddNHeX\/train.csv\")\ntest = pd.read_csv(\"..\/input\/av-recommendation-systems\/test_HLxMpl7\/test.csv\")","d378f285":"print(train.shape)\ntrain.head()","69c2db3c":"\n# convert the train in the long format to wide format\n\nwide_train = train.pivot_table(index = \"user_id\", columns=\"challenge_sequence\", values=\"challenge\", aggfunc= lambda x : x).reset_index()","0fbcfe1c":"# dropping the user_id, since we won't be needing those for our co-occurrence matrix\nwide_train.drop('user_id',axis = 1,inplace = True)","c6129580":"\nwide_train.head(20)","ca4ec525":"# convert each row for a user into a string\n\nrows = []\nfor index, row in wide_train.iterrows():\n    r = \" \".join(row.map(str))\n    rows.append(r)","27d98da3":"\n# converting test to wide format\n\nwide_test = test.pivot_table(index = \"user_id\", columns=\"challenge_sequence\", values=\"challenge\", aggfunc= lambda x : x).reset_index()","84e0e4fc":"wide_test.shape","9170db95":"# saving test user_id for future use\n\ntest_ids = wide_test['user_id']","837a787a":"# dropping user_id from wide test\n\nwide_test.drop([\"user_id\"], axis =1, inplace = True)","fe501599":"for index, row in wide_test.iterrows():\n    r = \" \".join(row.map(str))\n    rows.append(r)","9b71d774":"# creating a corpus\nthefile = open(\"corpus.txt\",\"w\")","eac10a02":"for element in rows:\n    thefile.write(\"%s\\n\"%element)","0497579c":"thefile.close()","442d2382":"# reading the corpus\n\ncorpus = open(\"corpus.txt\",\"r\")","92c61b6d":"corpus","03970b58":"# creating a dictionary with key = challenge_name and value = frequency\nvocab = Counter()","94b229fc":"for line in corpus:\n    tokens = line.strip().split()\n    vocab.update(tokens)","71cae34b":"vocab = {word:(i,freq) for i,(word,freq) in enumerate(vocab.items())}\n    ","7fdc7395":"vocab","872db388":"id2word = dict((i, word) for word, (i, _) in enumerate(vocab.items()))\n","3ecd7fa7":"id2word","3a2c47b6":"vocab_size = len(vocab)\nprint(vocab_size)","be9fe95a":"cooccurrences = sparse.lil_matrix((vocab_size, vocab_size),dtype=np.float64)\ncooccurrences","f4d469ec":"\n# context window size\n\nwindow_size = 10","2764fd0c":"corpus = open(\"..\/input\/recommendation-corpus\/corpus.txt\",\"r\")","e1a636fd":"\n# Tuneable parameters : window_size, distance\n\nfor i, line in enumerate(corpus):\n    tokens = line.strip().split()\n    token_ids = [vocab[word][0] for word in tokens]\n    \n    for center_i, center_id in enumerate(token_ids):\n        # Collect all word IDs in left window of center word\n        context_ids = token_ids[max(0, center_i - window_size) : center_i]\n        contexts_len = len(context_ids)\n\n        for left_i, left_id in enumerate(context_ids):\n            # Distance from center word\n            \n            distance = contexts_len - left_i\n\n            # Weight by inverse of distance between words\n            increment = 1.0 \/ float(distance)\n\n            # Build co-occurrence matrix symmetrically (pretend we\n            # are calculating right contexts as well)\n            cooccurrences[center_id, left_id] += increment\n            cooccurrences[left_id, center_id] += increment","8dd58c71":"# If anything other than None will exclude challenges whose frequencies are below this value.\n\nmin_count = None\n#min_count = 20\nprint(min_count)","fc8aeda5":"# filling the values in a matrix form\n\nco_matrix = np.zeros([len(id2word),len(id2word)])\n\nfor i, (row, data) in enumerate(zip(cooccurrences.rows,cooccurrences.data)):\n    if min_count is not None and vocab[id2word[i]][0] < min_count:\n        continue\n        \n    for data_idx, j in enumerate(row):\n        if min_count is not None and vocab[id2word[j]][0] < min_count:\n            continue\n            \n        co_matrix[i,j] = data[data_idx]","828e8103":"co_matrix","e1e0afed":"#saving the mapping to a dictionary\npickle_path = \"..\/input\/vocab-mapping\/vocab_mapping.pkl\"\n#pickle_mapping = open(pickle_path,\"wb\")\n#pickle.dump(id2word, pickle_mapping)\n#pickle_mapping.close()","782b80e7":"# saving the co-occurence matrix as a dataframe\n\nco_occurence_dataframe = pd.DataFrame(co_matrix)","1b693450":"co_occurence_dataframe.head()","60c0ac73":"\nres = {v:k for k,v in id2word.items()}","bc7a9e32":"co_occurence_dataframe =co_occurence_dataframe.rename(columns=res)","5f3be227":"co_occurence_dataframe = co_occurence_dataframe.rename(index=res)","efa44c43":"co_occurence_dataframe.to_csv(\"co_matrix_with_window_size_1.csv\", index = False)","faf3ef51":"co_occurence_dataframe.head()","0489317e":"wide_test.head()","efe9f782":"wide_test.shape","d20064ab":"\nfinal_predictions = []\n\nfor i in range(0,39732):\n    predictions = [wide_test.loc[i,10]]\n    counter = 0\n    for stimulus in predictions:\n        predictions.append(co_occurence_dataframe[stimulus].idxmax())\n        counter+=1\n        if counter == 3:\n            break\n            \n    final_predictions.append(predictions[1:])","2d7ea88a":"# making predictions with the co-occurence_matrix based on 10th challenge only\nfinal_predictions_new = []\n\nfor i in range(0,39732):\n    stimulus = wide_test.loc[i,10]\n    \n    final_predictions_new.append(list(co_occurence_dataframe[stimulus].nlargest(3).index))","8f3b16e2":"largest_3 = pd.DataFrame(final_predictions_new)","ed50ff95":"largest_3['user_id'] = test_ids","c1f53155":"\nlargest_3.head()","c12bdd79":"largest_3_long = pd.melt(largest_3,id_vars=\"user_id\",var_name=\"sequence\", value_name=\"challenge\" )","5ddadff1":"final_predictions","67083f9a":"sub = pd.read_csv('..\/input\/av-recommendation-systems\/sample_submission_J0OjXLi_DDt3uQN.csv')","abc405d5":"seq = []\nfor i in final_predictions:\n    for j in i:\n        seq.append(j)","89e03d1b":"sub['challenge'] = seq","2a368c87":"sub.to_csv('nlp_corr.csv',index = False)","1ccd207c":"df = pd.read_csv(\"..\/input\/av-recommendation-systems\/train_mddNHeX\/challenge_data.csv\")","61a60411":"print(df.shape)\ndf.head()","109c74bf":"print(train.shape)\ntrain.head()","1b32da56":"train.rename(columns = {'challenge' : 'challenge_ID'},inplace = True)\ntest.rename(columns = {'challenge' : 'challenge_ID'},inplace = True)\ntrain.head()","931771b4":"df2 = df.merge(train,on = 'challenge_ID')\ndf2_test = df.merge(test,on = 'challenge_ID')","a1c71680":"print(df2.shape)\ndf2.head()","6edb142b":"print(df2_test.shape)\ndf2_test.head()","b36ef27d":"print(\"No. of challenges: \", df2['challenge_ID'].nunique())\nprint(\"No. of progaming languages: \", df2['programming_language'].nunique())\nprint(\"Minimum submissions: \", df2['total_submissions'].min())\nprint(\"Max submissions: \", df2['total_submissions'].max())\nprint(\"No. of Authors: \", df2['author_ID'].nunique())\nprint(\"No. org of Authors: \", df2['author_org_ID'].nunique())\nprint(\"No. categories: \", df2['category_id'].nunique())","4bd9d2e6":"df2.isnull().sum()","42645f4c":"df[['challenge_ID','programming_language']].groupby('challenge_ID').count()","3a5e4320":"df2[['user_id','programming_language']].groupby('user_id').nunique().sort_values(by = 'programming_language',ascending = False)\n#df2[df2['user_id'] == 77954]","4b58436d":"df2['programming_language'] = df2['programming_language'].astype(str)\ndf2['total_submissions'] = df2['total_submissions'].astype(str)\ndf2['category_id'] = df2['category_id'].astype(str)","b9388af1":"def create_soup(x):\n    return ' '.join(x['programming_language']) + ' ' + ' '.join(x['total_submissions']) + ' ' + x['category_id'] \ndf2['soup'] = df2.apply(create_soup, axis=1)","a75a96c2":"# Import CountVectorizer and create the count matrix\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncount = CountVectorizer(stop_words='english')\ncount_matrix = count.fit_transform(df2['soup'])\n","3490e140":"type(count_matrix)","83accd6c":"# Compute the Cosine Similarity matrix based on the count_matrix\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ncosine_sim2 = cosine_similarity(count_matrix, count_matrix)","7f52cf11":"Second way"}}