{"cell_type":{"0332db6e":"code","6f4d1068":"code","7a8e6665":"code","ee037329":"code","d33d0cc0":"code","c35bb9e1":"code","233ccd1f":"code","993404e9":"code","d3274c28":"code","5ea76c1c":"code","ce44b626":"code","6e2cc700":"code","5cc53e2c":"code","f6caaf37":"code","9fd6abc2":"code","282d75a6":"code","bf2e2c0d":"code","d072109e":"code","aaf0eb8e":"code","4916fb69":"code","09eb9e59":"code","c681ae38":"code","4288c423":"code","fa32f596":"code","c7782842":"code","df362df8":"code","bb0266a4":"code","3fe2d332":"code","9597db6f":"code","1232f0fb":"code","3aa110b9":"code","8bb5b398":"code","1cd59244":"code","076d7ab0":"code","3701a098":"code","57ff2ac8":"code","77950b9f":"code","1f40de6b":"code","a2b60aac":"code","d6e12b63":"markdown","6289f33f":"markdown"},"source":{"0332db6e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6f4d1068":"PATH = '\/kaggle\/input\/tabular-playground-series-oct-2021\/'\ntrain = pd.read_csv(f'{PATH}train.csv')\ntest = pd.read_csv(f'{PATH}test.csv')\nsub = pd.read_csv(f'{PATH}sample_submission.csv')","7a8e6665":"print(f'Number of missing values in training data: {train.isna().sum().sum()}')\nprint(f'Number of missing values in testing data: {test.isna().sum().sum()}')","ee037329":"train.info(memory_usage=\"deep\")","d33d0cc0":"#Garbage Collector\nimport gc\n#This is a function that downcast the integer columns\ndef downcast_df_int_columns(df):\n    list_of_columns = list(df.select_dtypes(include=[\"int32\", \"int64\"]).columns)\n        \n    if len(list_of_columns)>=1:\n        max_string_length = max([len(col) for col in list_of_columns]) # finds max string length for better status printing\n        print(\"downcasting integers for:\", list_of_columns, \"\\n\")\n        \n        for col in list_of_columns:\n            print(\"reduced memory usage for:  \", col.ljust(max_string_length+2)[:max_string_length+2],\n                  \"from\", str(round(df[col].memory_usage(deep=True)*1e-6,2)).rjust(8), \"to\", end=\" \")\n            df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n            print(str(round(df[col].memory_usage(deep=True)*1e-6,2)).rjust(8))\n    else:\n        print(\"no columns to downcast\")\n    \n    gc.collect()\n    \n    print(\"done\")","c35bb9e1":"#This is a function that downcast the float columns, if you have too many columns to adjust and do not want to see to many messages proceesing, you could comment our the print() columns\ndef downcast_df_float_columns(df):\n    list_of_columns = list(df.select_dtypes(include=[\"float64\"]).columns)\n        \n    if len(list_of_columns)>=1:\n        max_string_length = max([len(col) for col in list_of_columns]) # finds max string length for better status printing\n        print(\"downcasting float for:\", list_of_columns, \"\\n\")\n        \n        for col in list_of_columns:\n            print(\"reduced memory usage for:  \", col.ljust(max_string_length+2)[:max_string_length+2],\n                  \"from\", str(round(df[col].memory_usage(deep=True)*1e-6,2)).rjust(8), \"to\", end=\" \")\n            df[col] = pd.to_numeric(df[col], downcast=\"float\")\n            print(str(round(df[col].memory_usage(deep=True)*1e-6,2)).rjust(8))\n    else:\n        print(\"no columns to downcast\")\n    \n    gc.collect()\n    \n    print(\"done\")","233ccd1f":"downcast_df_int_columns(train)\ndowncast_df_float_columns(train)","993404e9":"train.info(memory_usage=\"deep\")","d3274c28":"test.info(memory_usage=\"deep\")","5ea76c1c":"downcast_df_int_columns(train)\ndowncast_df_float_columns(train)","ce44b626":"test.info(memory_usage=\"deep\")","6e2cc700":"features = [col for col in train.columns if 'f' in col]\nfeatures.append('id')\n","5cc53e2c":"len(features)","f6caaf37":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","9fd6abc2":"train_frac = train.sample(frac = 1).reset_index(drop = True)","282d75a6":"test_frac = test.sample(frac = 1).reset_index(drop = True)","bf2e2c0d":"# plot the first 5 features \n# i = 1\n# plt.figure()\n# fig, ax = plt.subplots(285, 5,figsize=(30, 285))\n# for feature in features:\n#     plt.subplot(285, 5,i)\n#     sns.histplot(train_frac[feature],color=\"blue\", kde=True,bins=100, label='train_'+feature)\n#     sns.histplot(test_frac[feature],color=\"olive\", kde=True,bins=100, label='test_'+feature)\n#     plt.xlabel(feature, fontsize=9); plt.legend()\n#     i += 1\n# plt.show()","d072109e":"# import pandas_profiling as pp\n# pp.ProfileReport(train_frac)","aaf0eb8e":"# train['std'] = train[features].std(axis=1)","4916fb69":"#Check scale\ntrain_frac.describe().T.style.bar(subset=['mean'], color='#FF595E')\\\n                           .background_gradient(subset=['50%'], cmap='PiYG')","09eb9e59":"from sklearn.preprocessing import RobustScaler\n# scaler = RobustScaler()\n# X_train[features] = scaler.fit_transform(X_train[features])\n","c681ae38":"# Library\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgbm\n\n# train_X, val_X, train_y, val_y = train_test_split(X_train, y, train_size=0.8, test_size=0.2,\n#                                                                 random_state=0)","4288c423":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import roc_auc_score","fa32f596":"# model = lgbm.LGBMClassifier()\n# model.fit(\n#         train_X, \n#         train_y,\n#         eval_set=[(val_X, val_y)],\n#         eval_metric='auc',\n#         early_stopping_rounds=200,\n#         verbose=1000,\n#     )\n\n# lgb_oof = model.predict_proba(val_X)[:,-1]\n\n# print(roc_auc_score(val_y, lgb_oof))\n\n# tmp = pd.DataFrame()\n# tmp['imp'] = model.feature_importances_\n# tmp['feature'] = model.feature_name_\n# tmp_or = tmp\n# tmp = tmp[tmp.imp > 0]\n# order = list(tmp.groupby('feature').mean().sort_values('imp', ascending=False).index)\n# fig = plt.figure(figsize=(16, 16), tight_layout=True)\n# sns.barplot(x=\"imp\", y=\"feature\", data=tmp.groupby('feature').mean().reset_index(), order=order)\n# plt.title(\"LightGBM feature importances\")\n","c7782842":"train_or = train\ntest_or = test\ntrain = train_frac\ntest = test_frac\ntarget = train.target","df362df8":"train = train[features].set_index('id')\ntest = test[features].set_index('id')\n\nfeatures.remove('id')\n\ntrain['std'] = train[features].std(axis=1)\ntrain['min'] = train[features].min(axis=1)\ntrain['median'] = train[features].median(axis=1)\ntrain['max'] = train[features].max(axis=1)\ntrain['mean'] = train[features].mean(axis=1)\n\ntest['std'] = test[features].std(axis=1)\ntest['min'] = test[features].min(axis=1)\ntest['median'] = test[features].median(axis=1)\ntest['max'] = test[features].max(axis=1)\ntest['mean'] = test[features].mean(axis=1)\n\nfeatures.extend(['std','min','median','max','mean'])","bb0266a4":"scaler = RobustScaler()\ntrain[features] = scaler.fit_transform(train[features])\n\ntest[features] = scaler.transform(test[features])","3fe2d332":"N_SPLITS = 5\nN_ESTIMATORS = 20000\nEARLY_STOPPING_ROUNDS = 200\nVERBOSE = 1000\nSEED = 2021\n\nimport pandas as pd\nimport numpy as np\nimport random\nimport time\nimport os\nimport gc\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nseed_everything(SEED)","9597db6f":"\nfrom optuna.integration import LightGBMPruningCallback\nimport optuna  # pip install optuna\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\ndef objective(trial, train, target):\n    param_grid = {\n        \"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [10000]),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 5e-4, 0.5),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 3000, step=20),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 10000, step=100),\n        \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100, step=5),\n        \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100, step=5),\n        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n        \"bagging_fraction\": trial.suggest_float(\n            \"bagging_fraction\", 0.2, 0.95, step=0.1\n        ),\n        \"bagging_freq\": trial.suggest_categorical(\"bagging_freq\", [1]),\n        \"feature_fraction\": trial.suggest_float(\n            \"feature_fraction\", 0.2, 0.95, step=0.1\n        ),\n        'subsample': trial.suggest_float(\n            \"subsample\", 0.1, 1, step=0.1\n        ),\n        'subsample_freq': trial.suggest_int('subsample_freq',0,1),\n        'colsample_bytree':trial.suggest_float(\n            \"colsample_bytree\", 0, 10),\n        'reg_alpha': trial.suggest_float(\n            \"reg_alpha\", 1.0, 20),\n        'reg_lambda': trial.suggest_float(\n            \"reg_lambda\", 1e-2, 5),\n        'min_child_weight': trial.suggest_int(\"min_child_weight\", 0, 500),\n        'min_child_samples': trial.suggest_int(\"min_child_samples\", 20, 1000)\n    }\n\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1121218)\n\n    cv_scores = np.empty(5)\n    for idx, (train_idx, test_idx) in enumerate(cv.split(train, target)):\n        X_train, X_test = train.iloc[train_idx], train.iloc[test_idx]\n        y_train, y_test = target[train_idx], target[test_idx]\n\n        model = lgbm.LGBMClassifier(objective=\"binary\", **param_grid)\n        model.fit(\n            X_train,\n            y_train,\n            eval_set=[(X_test, y_test)],\n            eval_metric=\"auc\",\n            early_stopping_rounds=100\n        )\n        preds = model.predict_proba(X_test)[:,1]\n        cv_scores[idx] = roc_auc_score(y_test, preds)\n\n    return np.mean(cv_scores)\n    \nstudy = optuna.create_study(direction=\"maximize\", study_name=\"LGBM Classifier\")\nfunc = lambda trial: objective(trial, train, target)\nstudy.optimize(func, n_trials=20)","1232f0fb":"print(f\"\\tBest value (auc): {study.best_value:.5f}\") \n\n\nprint(f\"\\tBest params:\")\n\nbest_params = study.best_params\n","3aa110b9":"best_params","8bb5b398":"from sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)","1cd59244":"# best_params = {'device_type': 'gpu',\n#  'n_estimators': 10000,\n#  'learning_rate': 0.005561317553182468,\n#  'num_leaves': 20,\n#  'max_depth': 5,\n#  'min_data_in_leaf': 9400,\n#  'lambda_l1': 0,\n#  'lambda_l2': 95,\n#  'min_gain_to_split': 4.175935643818434,\n#  'bagging_fraction': 0.9,\n#  'bagging_freq': 1,\n#  'feature_fraction': 0.30000000000000004,\n#  'subsample': 0.6,\n#  'subsample_freq': 1,\n#  'colsample_bytree': 9.959598465442731,\n#  'reg_alpha': 11.705140163646389,\n#  'reg_lambda': 3.897135186715671,\n#  'min_child_weight': 266,\n#  'min_child_samples': 985}","076d7ab0":"lgb_oof = np.zeros(train.shape[0])\nlgb_pred = np.zeros(test.shape[0])\nlgb_importances = pd.DataFrame()\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X=train, y=target)):\n    print(f\"===== fold {fold} =====\")\n    X_train = train[features].iloc[trn_idx]\n    y_train = target.iloc[trn_idx]\n    X_valid = train[features].iloc[val_idx]\n    y_valid = target.iloc[val_idx]\n    X_test = test[features]\n    \n    start = time.time()\n    model = lgbm.LGBMClassifier(**best_params)\n    model.fit(\n        X_train, \n        y_train,\n        eval_set=[(X_valid, y_valid)],\n        eval_metric='auc',\n        early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n        verbose=VERBOSE,\n    )\n    \n    fi_tmp = pd.DataFrame()\n    fi_tmp['feature'] = model.feature_name_\n    fi_tmp['importance'] = model.feature_importances_\n    fi_tmp['fold'] = fold\n    fi_tmp['seed'] = SEED\n    lgb_importances = lgb_importances.append(fi_tmp)\n\n    lgb_oof[val_idx] = model.predict_proba(X_valid)[:, -1]\n    lgb_pred += model.predict_proba(X_test)[:, -1] \/ N_SPLITS\n\n    elapsed = time.time() - start\n    auc = roc_auc_score(y_valid, lgb_oof[val_idx])\n    print(f\"fold {fold} - lgb auc: {auc:.6f}, elapsed time: {elapsed:.2f}sec\\n\")\n\nprint(f\"oof lgb roc = {roc_auc_score(target, lgb_oof)}\")\n\nnp.save(\"lgb_oof.npy\", lgb_oof)\nnp.save(\"lgb_pred.npy\", lgb_pred)","3701a098":"order = list(lgb_importances.groupby('feature').mean().sort_values('importance', ascending=False).index)\n\nfig = plt.figure(figsize=(16, 120), tight_layout=True)\nsns.barplot(x=\"importance\", y=\"feature\", data=lgb_importances.groupby('feature').mean().reset_index(), order=order)\nplt.title(\"LightGBM feature importances\")","57ff2ac8":"# best_params['learning_rate'] = 5e-3\n# Didn't do any good","77950b9f":"# lgb_oof = np.zeros(train.shape[0])\n# lgb_pred = np.zeros(test.shape[0])\n# lgb_importances = pd.DataFrame()\n\n# for fold, (trn_idx, val_idx) in enumerate(skf.split(X=train, y=target)):\n#     print(f\"===== fold {fold} =====\")\n#     X_train = train[features].iloc[trn_idx]\n#     y_train = target.iloc[trn_idx]\n#     X_valid = train[features].iloc[val_idx]\n#     y_valid = target.iloc[val_idx]\n#     X_test = test[features]\n    \n#     start = time.time()\n#     model = lgbm.LGBMClassifier(**best_params)\n#     model.fit(\n#         X_train, \n#         y_train,\n#         eval_set=[(X_valid, y_valid)],\n#         eval_metric='auc',\n#         early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n#         verbose=VERBOSE,\n#     )\n    \n#     fi_tmp = pd.DataFrame()\n#     fi_tmp['feature'] = model.feature_name_\n#     fi_tmp['importance'] = model.feature_importances_\n#     fi_tmp['fold'] = fold\n#     fi_tmp['seed'] = SEED\n#     lgb_importances = lgb_importances.append(fi_tmp)\n\n#     lgb_oof[val_idx] = model.predict_proba(X_valid)[:, -1]\n#     lgb_pred += model.predict_proba(X_test)[:, -1] \/ N_SPLITS\n\n#     elapsed = time.time() - start\n#     auc = roc_auc_score(y_valid, lgb_oof[val_idx])\n#     print(f\"fold {fold} - lgb auc: {auc:.6f}, elapsed time: {elapsed:.2f}sec\\n\")\n\n# print(f\"oof lgb roc = {roc_auc_score(target, lgb_oof)}\")\n\n# np.save(\"lgb_oof.npy\", lgb_oof)\n# np.save(\"lgb_pred.npy\", lgb_pred)","1f40de6b":"# lgb_pred = model.predict_proba(test)[:,-1]","a2b60aac":"# sub['target'] = lgb_pred\n# sub.to_csv(\"submission.csv\", index=False)\n# sub","d6e12b63":"# Training Models","6289f33f":"# Optuna Training"}}