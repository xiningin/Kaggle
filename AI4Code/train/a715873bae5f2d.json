{"cell_type":{"25a55092":"code","4f26dca1":"code","b8eeccad":"code","69724df4":"code","04464795":"code","49aedf48":"code","6d68c665":"code","e76bb2ef":"code","b754c19f":"code","a5de6d9d":"code","37360b35":"code","d5782d16":"code","e00f7e67":"code","592a74ed":"code","7c81e273":"code","c9237f17":"code","8f567c55":"code","5ae44c85":"code","8f72edac":"code","62343dee":"code","c5363500":"code","0d5bee19":"code","af93d417":"code","d4d2acc1":"code","fab1f392":"code","0d11e9a9":"code","122f128a":"code","e62f7fc4":"code","0bbfe40c":"code","9a5ace94":"code","fcdb673c":"code","c72b50fd":"code","2fbd7dd4":"code","b31dbdb4":"markdown","a366b6a6":"markdown","3354c9d8":"markdown","55afa61d":"markdown","079e4e8a":"markdown","ac97de85":"markdown","24e67321":"markdown","867f8856":"markdown","257ba66b":"markdown","78e3f4a6":"markdown","59e6ba35":"markdown","3127a614":"markdown","8c19e53c":"markdown","0693df50":"markdown","075c8834":"markdown","899e50a6":"markdown","037bdfe7":"markdown","81cfb2f9":"markdown","ac18a470":"markdown","edf20346":"markdown","29dbe35d":"markdown","43fa3311":"markdown","bc2ba71c":"markdown","ad57bfa1":"markdown","ac8ae66a":"markdown","b06bc1f9":"markdown","a46aeb2c":"markdown","3a255929":"markdown","db0f749c":"markdown","fe3d8336":"markdown","9828a25f":"markdown","d86d8552":"markdown","41f6092d":"markdown","6b989fd5":"markdown","63a008d0":"markdown","de5f4ad4":"markdown","3caf022b":"markdown","c79b631b":"markdown"},"source":{"25a55092":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm_notebook\n\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nplt.style.use(['seaborn-darkgrid'])\nplt.rcParams['figure.figsize'] = (12, 9)\nplt.rcParams['font.family'] = 'DejaVu Sans'\n\nfrom sklearn import metrics\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\n\nRANDOM_STATE = 17","4f26dca1":"# change this if necessary\nPATH_TO_SAMSUNG_DATA = \"..\/input\"","b8eeccad":"X_train = np.loadtxt(os.path.join(PATH_TO_SAMSUNG_DATA, \"samsung_train.txt\"))\ny_train = np.loadtxt(os.path.join(PATH_TO_SAMSUNG_DATA,\n                                  \"samsung_train_labels.txt\")).astype(int)\n\nX_test = np.loadtxt(os.path.join(PATH_TO_SAMSUNG_DATA, \"samsung_test.txt\"))\ny_test = np.loadtxt(os.path.join(PATH_TO_SAMSUNG_DATA,\n                                  \"samsung_test_labels.txt\")).astype(int)","69724df4":"# Checking dimensions\nassert(X_train.shape == (7352, 561) and y_train.shape == (7352,))\nassert(X_test.shape == (2947, 561) and y_test.shape == (2947,))","04464795":"# Your code here\nX = np.vstack([X_train, X_test])\ny = np.hstack([y_train, y_test])","49aedf48":"np.unique(y)","6d68c665":"n_classes = np.unique(y).size\nn_classes","e76bb2ef":"# Your code here\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)","b754c19f":"# Your code here\npca = PCA(n_components=0.9, random_state=RANDOM_STATE).fit(X_scaled)\nX_pca = pca.transform(X_scaled)","a5de6d9d":"# Your code here\nX_pca.shape","37360b35":"# Your code here\nround(float(pca.explained_variance_ratio_[0] * 100))","d5782d16":"# Your code here\n\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, s=20, cmap='viridis');","e00f7e67":"# Your code here\nkmeans = KMeans(n_clusters=n_classes, n_init=100, random_state=RANDOM_STATE, n_jobs=1)\nkmeans.fit(X_pca)\ncluster_labels = kmeans.labels_","592a74ed":"# Your code here\nplt.scatter(X_pca[:, 0], X_pca[:, 1] , c=cluster_labels, s=20, cmap='viridis');","7c81e273":"tab = pd.crosstab(y, cluster_labels, margins=True)\ntab.index = ['walking', 'going up the stairs',\n            'going down the stairs', 'sitting', 'standing', 'laying', 'all']\ntab.columns = ['cluster' + str(i + 1) for i in range(6)] + ['all']\ntab","c9237f17":"pd.Series(tab.iloc[:-1,:-1].max(axis=1).values \/ \n          tab.iloc[:-1,-1].values, index=tab.index[:-1])\n","8f567c55":"# # Your code here\ninertia = []\nfor k in tqdm_notebook(range(1, n_classes + 1)):\n    kmeans = KMeans(n_clusters=k, random_state=1).fit(X_pca)\n    inertia.append(np.sqrt(kmeans.inertia_))","5ae44c85":"plt.plot(range(1, 7), inertia, marker='s');\nplt.xlabel('$k$')\nplt.ylabel('$J(C_k)$');","8f72edac":"d = {}\nfor k in range(2, 6):\n    i = k - 1\n    d[k] = (inertia[i] - inertia[i + 1]) \/ (inertia[i - 1] - inertia[i])\nd","62343dee":"ag = AgglomerativeClustering(n_clusters=n_classes, \n                             linkage='ward').fit(X_pca)","c5363500":"# Your code here\nprint('KMeans: Adjusted Rand Index =', metrics.adjusted_rand_score(y, cluster_labels))\nprint('Agglomerative CLustering: Adjusted Rand Index =', \n      metrics.adjusted_rand_score(y, ag.labels_))","0d5bee19":"# # Your code here\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n","af93d417":"svc = LinearSVC(random_state=RANDOM_STATE)\nsvc_params = {'C': [0.001, 0.01, 0.1, 1, 10]}","d4d2acc1":"%%time\n# # Your code here\nbest_svc = None\nbest_svc = GridSearchCV(svc, svc_params, n_jobs=1, cv=3, verbose=1)\nbest_svc.fit(X_train_scaled, y_train);\n","fab1f392":"best_svc.best_params_, best_svc.best_score_","0d11e9a9":" y_predicted = best_svc.predict(X_test_scaled)","122f128a":"tab = pd.crosstab(y_test, y_predicted, margins=True)\ntab.index = ['walking', 'climbing up the stairs',\n              'going down the stairs', 'sitting', 'standing', 'laying', 'all']\ntab.columns = ['walking', 'climbing up the stairs',\n             'going down the stairs', 'sitting', 'standing', 'laying', 'all']\ntab","e62f7fc4":"from sklearn.metrics import precision_recall_fscore_support as score\nfrom tabulate import tabulate\n\nprecision, recall, fscore, support = score(y_test, y_predicted)\nprint(\"precision:\")\nprint(tabulate([precision], headers=['walking', 'climbing up the stairs',\n              'going down the stairs', 'sitting', 'standing', 'laying'], tablefmt=\"simple\"))\n\nprint(\"recall:\")\nprint(tabulate([recall], headers=['walking', 'climbing up the stairs',\n             'going down the stairs', 'sitting', 'standing', 'laying'], tablefmt=\"simple\"))\n\n\n","0bbfe40c":"# Your code here\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\npca = PCA(n_components=0.9, random_state=RANDOM_STATE)\nX_train_pca = pca.fit_transform(X_train_scaled)\nX_test_pca = pca.transform(X_test_scaled)","9a5ace94":"svc = LinearSVC(random_state=RANDOM_STATE)\nsvc_params = {'C': [0.001, 0.01, 0.1, 1, 10]}","fcdb673c":"%%time\nbest_svc_pca = GridSearchCV(svc, svc_params, n_jobs=1, cv=3, verbose=1)\nbest_svc_pca.fit(X_train_pca, y_train);","c72b50fd":"best_svc_pca.best_params_, best_svc_pca.best_score_","2fbd7dd4":"round(100 * (best_svc_pca.best_score_ - best_svc.best_score_))","b31dbdb4":"```Python\nmatplotlib.pyplot.scatter(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, verts=None, edgecolors=None, *, plotnonfinite=False, data=None, **kwargs)\n```\nx : X_pca[:, 0] All rows of first component\n\ny : X_pca[:, 1] All rows of second component\n\ns : scalar or array_like, shape (n, ), optional\n\nc : color, sequence, or sequence of color, optional\n\ncmap : Colormap, optional, default: None","a366b6a6":"```Python\nnp.vstack(tup)\n```\nStack arrays in sequence vertically (row wise).\nExample:\n```Python\n>>> a = np.array([[1], [2], [3]])\n>>> b = np.array([[2], [3], [4]])\n>>> np.vstack((a,b))\narray([[1],\n       [2],\n       [3],\n       [2],\n       [3],\n       [4]])\n```       \n\n","3354c9d8":"```Python\nclass sklearn.decomposition.PCA(n_components=None, copy=True, whiten=False, svd_solver=\u2019auto\u2019, tol=0.0, iterated_power=\u2019auto\u2019, random_state=None)\n```\nPrincipal component analysis (PCA)\n\nLinear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.\n\n```Python\ntransform(self, X)\n```\nApply dimensionality reduction to X.\n\n","55afa61d":"[These labels correspond to:](https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/00240\/UCI%20HAR%20Dataset.names)\n- 1 \u2013 walking\n- 2 \u2013 walking upstairs\n- 3 \u2013 walking downstairs\n- 4 \u2013 sitting\n- 5 \u2013 standing\n- 6 \u2013 laying down","079e4e8a":"Perform clustering with the `KMeans` method, training the model on data with reduced dimensionality (by PCA). In this case, we will give a clue to look for exactly 6 clusters, but in general case we will not know how many clusters we should be looking for.\n\nOptions:\n\n- ** n_clusters ** = n_classes (number of unique labels of the target class)\n- ** n_init ** = 100\n- ** random_state ** = RANDOM_STATE (for reproducibility of the result)\n\nOther parameters should have default values.","ac97de85":"**Question 7**<br>\nWhich value of the hyperparameter `C` was chosen the best on the basis of cross-validation? <br>\n\n**Answer options:**\n- 0.001\n- 0.01\n- 0.1 *This*\n- 1\n- 10","24e67321":"Visualize data in projection on the first two principal components. Color the dots according to the clusters obtained.","867f8856":"Let's try another clustering algorithm, described in the article \u2013 agglomerative clustering.","257ba66b":"**Question 3:**<br>\nIf everything worked out correctly, you will see a number of clusters, almost perfectly separated from each other. What types of activity are included in these clusters? <br>\n\n**Answer options:**\n- 1 cluster: all 6 activities\n- 2 clusters: (walking, walking upstairs, walking downstairs ) and (sitting, standing, laying) *This*\n- 3 clusters: (walking), (walking upstairs, walking downstairs) and (sitting, standing, laying)\n- 6 clusters\n\nYellow is the first by c, so yellow and green, and light green are the first 1,2,3 and then blue, purple and light purple are the second 4,5,6 in another cluster.","78e3f4a6":"** Question 6: ** <br>\nSelect all the correct statements. <br>\n\n** Answer options: **\n- According to ARI, KMeans handled clustering worse than Agglomerative Clustering *This*\n- For ARI, it does not matter which tags are assigned to the cluster, only the partitioning of instances into clusters matters *This*\n- In case of random partitioning into clusters, ARI will be close to zero *This*\n\n\n1. The higher the ARI the better\n2. Tags don't matter to caculate ARI.\n2. True","59e6ba35":"# <center>Assignment template\n## <center> Unsupervised learning","3127a614":"** Question 10: ** <br>\nSelect all the correct statements:\n\n** Answer options: **\n- Principal component analysis in this case allowed to reduce the model training time, while the quality (mean cross-validation accuracy) suffered greatly, by more than 10%\n- PCA can be used to visualize data, but there are better methods for this task, for example, tSNE. However, PCA has lower computational complexity *This*\n- PCA builds linear combinations of initial features, and in some applications they might be poorly interpreted by humans *This*\n\n2. tSNE is a more complex algorithm that finds the complex polynomial relationship between features.\n3. Creates correlations and linear combinations that maybe a human can't see.","8c19e53c":"Using:\n$$\\Large D(k) = \\frac{|J(C_k) - J(C_{k+1})|}{|J(C_{k-1}) - J(C_k)|}  \\rightarrow \\min\\limits_k $$","0693df50":"Define the number of unique values of the labels of the target class.","075c8834":"For clustering, we do not need a target vector, so we'll work with the combination of training and test samples. Merge `X_train` with `X_test`, and `y_train` with `y_test`.","899e50a6":"It can be seen that kMeans does not distinguish activities very well. Use the elbow method to select the optimal number of clusters. Parameters of the algorithm and the data we use are the same as before, we change only `n_clusters`.","037bdfe7":"You can notice that the task is not very well solved when we try to detect several clusters (> 2). Now, let's solve the classification problem, given that the data is labeled.\n\nFor classification, use the support vector machine \u2013 class `sklearn.svm.LinearSVC`. In this course, we did study this algorithm separately, but it is well-known and you can read about it, for example [here](http:\/\/cs231n.github.io\/linear-classify\/#svmvssoftmax).\n\nChoose the `C` hyperparameter for` LinearSVC` using `GridSearchCV`.\n\n- Train the new `StandardScaler` on the training set (with all original features), apply scaling to the test set\n- In `GridSearchCV`, specify `cv` = 3.","81cfb2f9":"```Python\nsklearn.metrics.adjusted_rand_score(labels_true, labels_pred)\n```\n\nRand index adjusted for chance.\n\nThe Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.\n\n$\\Large \\text{RI} = \\frac{2(a + b)}{n(n-1)}.$\n\n Let $ n $ be the number of observations in a sample. Let $ a $ to be the number of observation pairs with the same labels and located in the same cluster, and let $ b $ to be the number of observations with different labels and located in different clusters.\n\n$\\Large \\text{ARI} = \\frac{\\text{RI} - E[\\text{RI}]}{\\max(\\text{RI}) - E[\\text{RI}]}.$\n\nThis metric is symmetric and does not depend in the label permutation. Therefore, this index is a measure of distances between different sample splits.  $ARI$  takes on values in the $ [\u22121,1] $ range. Negative values indicate the independence of splits, and positive values indicate that these splits are consistent (they match $ ARI=1 $).","ac18a470":"** Question 5: ** <br>\nHow many clusters can we choose according to the elbow method? <br>\n\n**Answer options:**\n- 1\n- 2 *This*\n- 3 \n- 4","edf20346":"```Python\nclass sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity=\u2019euclidean\u2019, memory=None, connectivity=None, compute_full_tree=\u2019auto\u2019, linkage=\u2019ward\u2019, pooling_func=\u2019deprecated\u2019, distance_threshold=None)\n```\n\nn_clusters : int or None, optional (default=2)\n\nlinkage : {\u201cward\u201d, \u201ccomplete\u201d, \u201caverage\u201d, \u201csingle\u201d}, optional (default=\u201dward\u201d)\nWhich linkage criterion to use. The linkage criterion determines which distance to use between sets of observation. The algorithm will merge the pairs of cluster that minimize this criterion.\n\n- ward minimizes the variance of the clusters being merged.\n- average uses the average of the distances of each observation of the two sets.\n- complete or maximum linkage uses the maximum distances between all observations of the two sets.\n- single uses the minimum of the distances between all observations of the two sets.\n\nCalculate the Adjusted Rand Index (`sklearn.metrics`) for the resulting clustering and for ` KMeans` with the parameters from the 4th question.","29dbe35d":"We see that for each class (i.e., each activity) there are several clusters. Let's look at the maximum percentage of objects in a class that are assigned to a single cluster. This will be a simple metric that characterizes how easily the class is separated from others when clustering.\n\nExample: if for class \"walking downstairs\" (with 1406 instances belonging to it), the distribution of clusters is:\n - cluster 1 - 900\n - cluster 3 - 500\n - cluster 6 - 6,\n \nthen such a share will be 900\/1406 $ \\approx $ 0.64.\n \n\n** Question 4: ** <br>\nWhich activity is separated from the rest better than others based on the simple metric described above? <br>\n\n**Answer:**\n- walking\n- standing\n- walking downstairs\n- all three options are incorrect *This*","43fa3311":"Finally, do the same thing as in Question 7, but add PCA.\n\n- Use `X_train_scaled` and` X_test_scaled`\n- Train the same PCA as before, on the scaled training set, apply scaling to the test set\n- Choose the hyperparameter `C` via cross-validation on the training set with PCA-transformation. You will notice how much faster it works now.\n\n** Question 9: ** <br>\nWhat is the difference between the best quality (accuracy) for cross-validation in the case of all 561 initial characteristics and in the second case, when the principal component method was applied? Round to the nearest percent. <br>\n\n** Options: **\n- quality is the same\n- 2%\n- 4% *This*\n- 10%\n- 20%","bc2ba71c":"------------------------","ad57bfa1":"** Question 8: ** <br>\nWhich activity type is worst detected by SVM in terms of precision? Recall?<br>\n\n**Answer options:**\n- precision \u2013 going up the stairs, recall \u2013 laying\n- precision \u2013 laying, recall \u2013 sitting\n- precision \u2013 walking, recall \u2013 walking\n- precision \u2013 standing, recall \u2013 sitting *This* ","ac8ae66a":"```Python\nclass sklearn.cluster.KMeans(n_clusters=8, init=\u2019k-means++\u2019, n_init=10, max_iter=300, tol=0.0001, precompute_distances=\u2019auto\u2019, verbose=0, random_state=None, copy_x=True, n_jobs=None, algorithm=\u2019auto\u2019)\n```\n\t\nn_clusters : int, optional, default: 8\nThe number of clusters to form as well as the number of centroids to generate.\n\nn_classes in this case:\n- 1 \u2013 walking\n- 2 \u2013 walking upstairs\n- 3 \u2013 walking downstairs\n- 4 \u2013 sitting\n- 5 \u2013 standing\n- 6 \u2013 laying down\n\nn_init : int, default: 10\nNumber of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia.\n\nrandom_state : int, RandomState instance or None (default)\n\nn_jobs : int or None, optional (default=None)\nThe number of jobs to use for the computation. This works by computing each of the n_init runs in parallel.\n\nkmeans.labels_ :\nLabels of each point","b06bc1f9":"Scale the sample using `StandardScaler` with default parameters.","a46aeb2c":"Reduce the number of dimensions using PCA, leaving as many components as necessary to explain at least 90% of the variance of the original (scaled) data. Use the scaled dataset and fix `random_state` (RANDOM_STATE constant).","3a255929":"**Answer options:**\n\n- 56 \n- 65 *This*\n- 66\n- 193\n","db0f749c":"Visualize data in projection on the first two principal components.","fe3d8336":"------------------------------","9828a25f":"```Python\nclass sklearn.preprocessing.StandardScaler(copy=True, with_mean=True, with_std=True)\n```\nStandardize features by removing the mean and scaling to unit variance\n\nThe standard score of a sample x is calculated as:\n\n$\\Large z = \\frac{(x - u)}{s}$\n\nwhere $u$ is the mean of the training samples or zero if with_mean=False, and $s$ is the standard deviation of the training samples or one if with_std=False.","d86d8552":"```Python\nexplained_variance_ratio_ : array, shape (n_components,)\n```\nPercentage of variance explained by each of the selected components.\n\nIf n_components is not set then all components are stored and the sum of the ratios is equal to 1.0.","41f6092d":"**\u0412\u043e\u043f\u0440\u043e\u0441 2:**<br>\nWhat percentage of the variance is covered by the first principal component? Round to the nearest percent.\n\n**Answer options:**\n- 45\n- 51 *This*\n- 56\n- 61","6b989fd5":"Look at the correspondence between the cluster marks and the original class labels and what kinds of activities the `KMeans` algorithm is confused at.","63a008d0":"In this task, we will look at how data dimensionality reduction and clustering methods work. At the same time, we'll practice solving classification task again.\n\nWe will work with the [Samsung Human Activity Recognition](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Human+Activity+Recognition+Using+Smartphones) dataset. Download the data [here](https:\/\/drive.google.com\/file\/d\/14RukQ0ylM2GCdViUHBBjZ2imCaYcjlux\/view?usp=sharing). The data comes from accelerometers and gyros of Samsung Galaxy S3 mobile phones ( you can find more info about the features using the link above), the type of activity of a person with a phone in his\/her pocket is also known \u2013 whether he\/she walked, stood, lay, sat or walked up or down the stairs.\n\nFirst, we pretend that the type of activity is unknown to us, and we will try to cluster people purely on the basis of available features. Then we solve the problem of determining the type of physical activity as a classification problem.","de5f4ad4":"<center>\n<img src=\"https:\/\/habrastorage.org\/files\/fd4\/502\/43d\/fd450243dd604b81b9713213a247aa20.jpg\">\n## Open Machine Learning Course\n<center>Authors: [Olga Daykhovskaya](https:\/\/www.linkedin.com\/in\/odaykhovskaya\/), [Yury Kashnitsky](https:\/\/www.linkedin.com\/in\/festline\/) <br>\n    All content is distributed under the [Creative Commons CC BY-NC-SA 4.0](https:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/) license.","3caf022b":"** Question 1: ** <br>\nWhat is the minimum number of principal components required to cover the 90% of the variance of the original (scaled) data?","c79b631b":"-------------------------------"}}