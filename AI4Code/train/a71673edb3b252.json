{"cell_type":{"eac3fe7e":"code","d3369a5d":"code","370bdddd":"code","0528a112":"code","17f49aaa":"code","d2ee34b0":"code","24ec323d":"code","94ce43da":"code","f43eb870":"code","22a3881a":"code","d5d48d84":"code","079f8996":"code","adcd9f91":"markdown","3e0ab127":"markdown","672a42aa":"markdown","bf586710":"markdown","c48407bc":"markdown","8dfecf5a":"markdown","c3f07c19":"markdown","1cc1a53d":"markdown"},"source":{"eac3fe7e":"# https:\/\/github.com\/divamgupta\/image-segmentation-keras","d3369a5d":"import cv2\nimport numpy as np\n\n# ann_img = np.zeros((30,30,3)).astype('uint8')\n# ann_img[ 3 , 4 ] = 1 # this would set the label of pixel 3,4 as 1\n# ann_img[ 0 , 0 ] = 2 # this would set the label of pixel 0,0 as 2\n","370bdddd":"# from PIL import Image\n# import matplotlib.pyplot as plt\n# %matplotlib inline\n\n# original_image = \"\/kaggle\/input\/semantic-drone-dataset\/semantic_drone_dataset\/original_images\/001.jpg\"\n# label_image_semantic = \"\/kaggle\/input\/semantic-drone-dataset\/semantic_drone_dataset\/label_images_semantic\/001.png\"\n\n# # original_image = \"\/kaggle\/input\/pascal-voc-2012\/VOC2012\/JPEGImages\/2007_000027.jpg\"\n# # label_image_semantic = \"\/kaggle\/input\/pascal-voc-2012\/VOC2012\/SegmentationObject\/2007_000027.png\"\n# fig, axs = plt.subplots(1, 2, figsize=(16, 8), constrained_layout=True)\n\n# axs[0].imshow( Image.open(original_image))\n# axs[0].grid(False)\n\n# label_image_semantic = Image.open(label_image_semantic)\n# label_image_semantic = np.asarray(label_image_semantic)\n# axs[1].imshow(label_image_semantic)\n# axs[1].grid(False)","0528a112":"!cp '..\/input\/vggunetckp\/vgg_unet.19' '\/kaggle\/working\/vgg_unet'\n!cp '..\/input\/vggunetckp\/vgg_segnet.19' '\/kaggle\/working\/vgg_segnet'","17f49aaa":"!pip install keras-segmentation","d2ee34b0":"# kaggle_commit = True\n\n# epochs = 20\n# if kaggle_commit:\n#     epochs = 5","24ec323d":"# from keras_segmentation.models.unet import vgg_unet\n# from keras_segmentation.models.segnet import vgg_segnet\n# n_classes = 23\n# models = []\n# for model_name in ['vgg_segnet', 'vgg_unet']:\n#     model = None\n#     if model_name == 'vgg_unet':\n#         model = vgg_unet(n_classes=n_classes,  input_height=416, input_width=608  )\n#     elif model_name == 'vgg_segnet':\n#         model = vgg_segnet(n_classes=n_classes,  input_height=416, input_width=608  )\n#     print('Start trainning ' + model_name)\n#     model.train( \n#         train_images =  \"\/kaggle\/input\/customziedsemanticdronedataset\/semantic_drone_dataset\/original_images\/train\/\",\n#         train_annotations = \"\/kaggle\/input\/customziedsemanticdronedataset\/semantic_drone_dataset\/label_images_semantic\/train\/\",\n#         val_images=\"\/kaggle\/input\/customziedsemanticdronedataset\/semantic_drone_dataset\/original_images\/test\/\",\n#         val_annotations=\"\/kaggle\/input\/customziedsemanticdronedataset\/semantic_drone_dataset\/label_images_semantic\/test\/\",\n#         checkpoints_path = model_name , epochs=epochs, auto_resume_checkpoint=True, validate=False, verify_dataset=False\n#     )\n#     print('Model evaluate segmentation: ')\n#     print(model.evaluate_segmentation(inp_images_dir='\/kaggle\/input\/customziedsemanticdronedataset\/semantic_drone_dataset\/original_images\/test\/',annotations_dir='\/kaggle\/input\/customziedsemanticdronedataset\/semantic_drone_dataset\/label_images_semantic\/test\/'))\n","94ce43da":"from keras_segmentation.models.unet import vgg_unet\nfrom keras_segmentation.models.segnet import vgg_segnet\nn_classes = 23\nvgg_unet_model = vgg_unet(n_classes=n_classes,  input_height=416, input_width=608  )\nvgg_unet_model.load_weights('.\/vgg_unet')\nvgg_segnet_model = vgg_segnet(n_classes=n_classes,  input_height=416, input_width=608  )\nvgg_segnet_model.load_weights('.\/vgg_segnet')","f43eb870":"import time\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nstart = time.time()\nimage_name = \"532\"\ninput_image = \"..\/input\/customziedsemanticdronedataset\/semantic_drone_dataset\/original_images\/test\/\" + image_name + \".jpg\"\nout = vgg_unet_model.predict_segmentation(\n    inp=input_image,\n    out_fname=\"out.png\"\n)\n\nfig, axs = plt.subplots(1, 3, figsize=(20, 20), constrained_layout=True)\n\nimg_orig = Image.open(input_image)\naxs[0].imshow(img_orig)\naxs[0].set_title('original image-' + image_name + '.jpg')\naxs[0].grid(False)\n\naxs[1].imshow(out)\naxs[1].set_title('prediction image-out.png')\naxs[1].grid(False)\n\nvalidation_image = \"..\/input\/customziedsemanticdronedataset\/semantic_drone_dataset\/label_images_semantic\/test\/\" + image_name + \".png\"\naxs[2].imshow( Image.open(validation_image))\naxs[2].set_title('true label image-' + image_name +'.png')\naxs[2].grid(False)\n\ndone = time.time()\nelapsed = done - start","22a3881a":"print(elapsed)\nprint(out)\nprint(out.shape)","d5d48d84":"# from keras_segmentation.models.segnet import vgg_segnet\n\n# n_classes = 23 # Aerial Semantic Segmentation Drone Dataset tree, gras, other vegetation, dirt, gravel, rocks, water, paved area, pool, person, dog, car, bicycle, roof, wall, fence, fence-pole, window, door, obstacle\n# model = vgg_segnet(n_classes=n_classes)\n\n# model.train( \n#     train_images =  \"\/kaggle\/input\/semantic-drone-dataset\/semantic_drone_dataset\/original_images\/\",\n#     train_annotations = \"\/kaggle\/input\/semantic-drone-dataset\/semantic_drone_dataset\/label_images_semantic\/\",\n#     checkpoints_path = \"vgg_segnet\" , epochs=epochs\n# )\n# # print(keras_segmentation.models.unet.modules.keys)\n# # from keras_segmentation.models.unet import vgg_unet\n","079f8996":"# Epoch 1\/5\n# 512\/512 [==============================] - 733s 1s\/step - loss: 1.5814 - accuracy: 0.5864\n# saved  vgg_unet.0\n# Epoch 2\/5\n# 512\/512 [==============================] - 735s 1s\/step - loss: 1.1894 - accuracy: 0.6453\n# saved  vgg_unet.1\n# Epoch 3\/5\n# 512\/512 [==============================] - 728s 1s\/step - loss: 1.0776 - accuracy: 0.6758\n# saved  vgg_unet.2\n# Epoch 4\/5\n# 512\/512 [==============================] - 731s 1s\/step - loss: 0.9908 - accuracy: 0.7022\n# saved  vgg_unet.3\n# Epoch 5\/5\n# 512\/512 [==============================] - 740s 1s\/step - loss: 0.9220 - accuracy: 0.7229\n# saved  vgg_unet.4\n\n# Start trainning vgg_segnet\n# Loading the weights from latest checkpoint  vgg_segnet.0\n# Epoch 1\/20\n# 512\/512 [==============================] - 714s 1s\/step - loss: 1.5872 - accuracy: 0.5714\n# saved  vgg_segnet.0\n# Epoch 2\/20\n# 512\/512 [==============================] - 698s 1s\/step - loss: 1.4656 - accuracy: 0.6076\n# saved  vgg_segnet.1\n# Epoch 3\/20\n# 512\/512 [==============================] - 696s 1s\/step - loss: 1.3238 - accuracy: 0.6378\n# saved  vgg_segnet.2\n# Epoch 4\/20\n# 512\/512 [==============================] - 696s 1s\/step - loss: 1.1879 - accuracy: 0.6626\n# saved  vgg_segnet.3\n# Epoch 5\/20\n# 512\/512 [==============================] - 695s 1s\/step - loss: 1.0878 - accuracy: 0.6874\n# saved  vgg_segnet.4\n# Epoch 6\/20\n# 512\/512 [==============================] - 699s 1s\/step - loss: 1.0210 - accuracy: 0.7047\n# saved  vgg_segnet.5\n# Epoch 7\/20\n# 512\/512 [==============================] - 704s 1s\/step - loss: 0.9397 - accuracy: 0.7245\n# saved  vgg_segnet.6\n# Epoch 8\/20\n# 512\/512 [==============================] - 711s 1s\/step - loss: 0.8878 - accuracy: 0.7408\n# saved  vgg_segnet.7\n# Epoch 9\/20\n# 512\/512 [==============================] - 719s 1s\/step - loss: 0.8179 - accuracy: 0.7594\n# saved  vgg_segnet.8\n# Epoch 10\/20\n# 512\/512 [==============================] - 698s 1s\/step - loss: 0.7850 - accuracy: 0.7691\n# saved  vgg_segnet.9\n# Epoch 11\/20\n# 512\/512 [==============================] - 701s 1s\/step - loss: 0.7244 - accuracy: 0.7865\n# saved  vgg_segnet.10\n# Epoch 12\/20\n# 512\/512 [==============================] - 698s 1s\/step - loss: 0.6933 - accuracy: 0.7952\n# saved  vgg_segnet.11\n# Epoch 13\/20\n# 512\/512 [==============================] - 707s 1s\/step - loss: 0.6480 - accuracy: 0.8087\n# saved  vgg_segnet.12\n# Epoch 14\/20\n# 512\/512 [==============================] - 723s 1s\/step - loss: 0.6276 - accuracy: 0.8129\n# saved  vgg_segnet.13\n# Epoch 15\/20\n# 512\/512 [==============================] - 720s 1s\/step - loss: 0.5831 - accuracy: 0.8263\n# saved  vgg_segnet.14\n# Epoch 16\/20\n# 512\/512 [==============================] - 710s 1s\/step - loss: 0.5512 - accuracy: 0.8351\n# saved  vgg_segnet.15\n# Epoch 17\/20\n# 512\/512 [==============================] - 710s 1s\/step - loss: 0.5232 - accuracy: 0.8427\n# saved  vgg_segnet.16\n# Epoch 18\/20\n# 512\/512 [==============================] - 706s 1s\/step - loss: 0.4923 - accuracy: 0.8519\n# saved  vgg_segnet.17\n# Epoch 19\/20\n# 512\/512 [==============================] - 705s 1s\/step - loss: 0.4946 - accuracy: 0.8515\n# saved  vgg_segnet.18\n# Epoch 20\/20\n# 512\/512 [==============================] - 724s 1s\/step - loss: 0.4488 - accuracy: 0.8654\n# {'frequency_weighted_IU': 0.7396276385324942, 'mean_IU': 0.4142282055343959, 'class_wise_IU': array([0.6283008 , 0.89201673, 0.3146251 , 0.85808337, 0.65618186,\n#        0.82793787, 0.10199099, 0.70008597, 0.58409139, 0.76913934,\n#        0.29419667, 0.13517449, 0.        , 0.09904147, 0.        ,\n#        0.2648533 , 0.        , 0.5327716 , 0.17824968, 0.49366883,\n#        0.40093133, 0.48764754, 0.3082604 ])}","adcd9f91":"### Prediction","3e0ab127":"# **About This Kernel**\n* What is the purpose of the study?\n\nI am working on Deep Learning and Computer Vision in Flying Automobile Project. The project I am working on are Semantic segmentation (Aerial images) during the flight of the vehicle to find suitable areas where the vehicle can land. To make volumetric control of the vehicle to these areas. \n\nWith this kernel, I have completed working on the **Semantic segmentation**\n    \n <a class=\"anchor\" id=\"0.\"><\/a>\n# **Content**\n\n1. [What is semantic segmentation](#1.)\n1. [Implementation of Segnet, FCN, UNet , PSPNet and other models in Keras](#2.)\n1. [I extracted Github codes](#3.)","672a42aa":"<a class=\"anchor\" id=\"1.\"><\/a> \n# 1.What is semantic segmentation\n\nSource: https:\/\/divamgupta.com\/image-segmentation\/2019\/06\/06\/deep-learning-semantic-segmentation-keras.html\n\nSemantic image segmentation is the task of classifying each pixel in an image from a predefined set of classes. In the following example, different entities are classified.\n\n![Semantic segmentation of a bedroom image](https:\/\/divamgupta.com\/assets\/images\/posts\/imgseg\/image15.png?style=centerme)\n\nIn the above example, the pixels belonging to the bed are classified in the class \u201cbed\u201d, the pixels corresponding to the walls are labeled as \u201cwall\u201d, etc.\n\nIn particular, our goal is to take an image of size W x H x 3 and generate a W x H matrix containing the predicted class ID\u2019s corresponding to all the pixels.\n\n![Image source: jeremyjordan.me](https:\/\/divamgupta.com\/assets\/images\/posts\/imgseg\/image14.png?style=centerme)\n\nUsually, in an image with various entities, we want to know which pixel belongs to which entity, For example in an outdoor image, we can segment the sky, ground, trees, people, etc.\n\nSemantic segmentation is different from object detection as it does not predict any bounding boxes around the objects. We do not distinguish between different instances of the same object. For example, there could be multiple cars in the scene and all of them would have the same label.\n\n![An example where there are multiple instances of the same object class](https:\/\/divamgupta.com\/assets\/images\/posts\/imgseg\/image7.png?style=centerme)\n\nIn order to perform semantic segmentation, a higher level understanding of the image is required. The algorithm should figure out the objects present and also the pixels which correspond to the object. Semantic segmentation is one of the essential tasks for complete scene understanding.\n\n\n## Dataset\n\nThe first step in training our segmentation model is to prepare the dataset. We would need the input RGB images and the corresponding segmentation images. If you want to make your own dataset, a tool like labelme or GIMP can be used to manually generate the ground truth segmentation masks.\n\nAssign each class a unique ID. In the segmentation images, the pixel value should denote the class ID of the corresponding pixel. This is a common format used by most of the datasets and keras_segmentation. For the segmentation maps, do not use the jpg format as jpg is lossy and the pixel values might change. Use bmp or png format instead. And of course, the size of the input image and the segmentation image should be the same.\n\nIn the following example, pixel (0,0) is labeled as class 2, pixel (3,4) is labeled as class 1 and rest of the pixels are labeled as class 0.\n","bf586710":"# Results\n","c48407bc":"## [Aerial Semantic Segmentation Drone Dataset](https:\/\/www.kaggle.com\/bulentsiyah\/semantic-drone-dataset)","8dfecf5a":"<a class=\"anchor\" id=\"2.\"><\/a> \n# 2.Implementation of Segnet, FCN, UNet , PSPNet and other models in Keras\n\nSource Github Link: https:\/\/github.com\/divamgupta\/image-segmentation-keras\n\nModels\nFollowing models are supported:\n\n| model_name       | Base Model     | Segmentation Model     |\n| :------------- | :----------: | -----------: |\n|  fcn_8 | Vanilla CNN  | FCN8   |\n|  fcn_32  | Vanilla CNN | FCN8 |\n|  fcn_8_vgg | VGG 16  | FCN8   |\n|  fcn_32_vgg  | VGG 16 | FCN32 |\n|  fcn_8_resnet50 | Resnet-50  | FCN32  |\n|  fcn_32_resnet50  | Resnet-50 | FCN32 |\n| fcn_8_mobilenet  | MobileNet  | FCN32   |\n| fcn_32_mobilenet   | MobileNet | FCN32 |\n| pspnet  | Vanilla CNN  | PSPNet   |\n| vgg_pspnet   | VGG 16 | PSPNet |\n|  resnet50_pspnet  | Resnet-50  |  PSPNet  |\n| unet_mini   | Vanilla Mini CNN  | U-Net   |\n| unet   | Vanilla CNN  | U-Net   |\n| vgg_unet   | VGG 16  | U-Net   |\n| resnet50_unet   | Resnet-50  |  U-Net  |\n|  mobilenet_unet  | MobileNet  | U-Net   |\n| segnet   | Vanilla CNN  | Segnet   |\n| vgg_segnet   | VGG 16  |  Segnet  |\n|  resnet50_segnet  | Resnet-50  | Segnet   |\n|  mobilenet_segnet  | MobileNet  | Segnet   |\n\t\t\n        \n\n","c3f07c19":"After generating the segmentation images, place them in the training\/testing folder. Make separate folders for input images and the segmentation images. The file name of the input image and the corresponding segmentation image should be the same. For this tutorial we would be using a data-set which is already prepared. You can download it from here ([Aerial Semantic Segmentation Drone Dataset](https:\/\/www.kaggle.com\/bulentsiyah\/semantic-drone-dataset)).","1cc1a53d":"### Train"}}