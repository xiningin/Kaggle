{"cell_type":{"f16b42e7":"code","52c6992f":"code","01f62119":"code","1a98aad8":"code","60baac07":"code","f6496afe":"code","1c61689c":"code","71339430":"code","65bd9745":"code","e94e1164":"code","6527efb1":"code","d8d457cf":"code","9e2344fb":"code","a83f43a7":"code","6f872eb7":"code","30f33834":"code","fc5b8e67":"code","d115e755":"code","9908665a":"code","de81b04e":"code","03d9c098":"code","62be90b2":"code","c5c7d5fe":"code","f4d73641":"markdown","8c953d54":"markdown","4a557754":"markdown","b430c2e3":"markdown","4ee362f1":"markdown","032f1d35":"markdown","eef8188e":"markdown","8bc1541b":"markdown","84022b6f":"markdown","e9d8c4cc":"markdown","ed762ddd":"markdown","105c3c64":"markdown","d74eb6e2":"markdown","4a40765b":"markdown","68a51d27":"markdown","7cdeeafa":"markdown"},"source":{"f16b42e7":"# data manipulation\nimport pandas as pd\nimport numpy as np\n\n# sklearn helper functions\nfrom sklearn.base import clone\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import ShuffleSplit, cross_validate, GridSearchCV, cross_val_predict, cross_val_score\nfrom sklearn.metrics import confusion_matrix, classification_report, plot_precision_recall_curve, plot_roc_curve\n\n# ML algorithms\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier, VotingClassifier, ExtraTreesClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier","52c6992f":"df_train_raw = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndf_test_raw = pd.read_csv(\"..\/input\/titanic\/test.csv\")","01f62119":"df1 = df_train_raw.copy()\n\nprint(df_train_raw.info())\ndf_train_raw.sample(10)","1a98aad8":"# check null values in train and test set\nprint(\"Null value count in train set\\n\", df1.isnull().sum())\nprint(\"-\"*30)\nprint(\"Null value count in test set\\n\", df_test_raw.isnull().sum())\nprint(\"-\"*30)\n\n# summary statistics for train set\ndf1.describe(include=\"all\")","60baac07":"# Remove target variable from training set and keep a copy of it\ntrain_labels = df1['Survived'].copy()\ndf1.drop('Survived', axis=1, inplace=True)","f6496afe":"# convert 'Pclass' from int to string\ndf1['Pclass'] = df1['Pclass'].astype(str)","1c61689c":"df1.sample(10)","71339430":"# lists for different type of preprocessing\npreprocess_features1 = ['PassengerId', 'Name', 'Ticket', 'Cabin']\npreprocess_features2 = ['Pclass', 'Sex']\npreprocess_features3 = ['Age']\npreprocess_features4 = ['SibSp', 'Parch']\npreprocess_features5 = ['Fare']\npreprocess_features6 = ['Embarked']\n\n# creating transformation pipelines for different features\ntransformer1 = 'drop'\n\ntransformer2 = Pipeline(steps=[\n    ('onehot', OneHotEncoder())\n])\n\ntransformer3 = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\ntransformer4 = Pipeline(steps=[\n    ('scaler', StandardScaler())\n])\n\ntransformer5 = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\ntransformer6 = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder())\n])\n\n# final transfomer\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('t1', transformer1, preprocess_features1),\n        ('t2', transformer2, preprocess_features2),\n        ('t3', transformer3, preprocess_features3),\n        ('t4', transformer4, preprocess_features4),\n        ('t5', transformer5, preprocess_features5),\n        ('t6', transformer6, preprocess_features6)])","65bd9745":"# apply full pipeline to train data\ndf1 = preprocessor.fit_transform(df1)","e94e1164":"MLA = [\n    RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, min_samples_leaf=4, n_jobs=-1),\n    \n    ExtraTreesClassifier(n_estimators=500, max_leaf_nodes=16, min_samples_leaf=4, n_jobs=-1),\n    \n    AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), \n                       n_estimators=200, \n                       algorithm='SAMME.R', \n                       learning_rate=0.5),\n    \n    XGBClassifier(learning_rate=0.05, max_depth=4, n_estimators=50, reg_alpha=0.01, reg_lambda=0.3, seed=0),\n    \n    BaggingClassifier(base_estimator=DecisionTreeClassifier(), \n                      n_estimators=500,\n                      max_samples=50,\n                      bootstrap=True,\n                      n_jobs=-1),    # Bagging\n    \n    BaggingClassifier(base_estimator=DecisionTreeClassifier(), \n                      n_estimators=500,\n                      max_samples=50,\n                      bootstrap=False,\n                      n_jobs=-1),    # Pasting\n    \n    BaggingClassifier(base_estimator=DecisionTreeClassifier(), \n                      n_estimators=500,\n                      max_samples=100,\n                      bootstrap=True,\n                      max_features=5,\n                      bootstrap_features=True,\n                      n_jobs=-1),    # Random Patches Method\n    \n    BaggingClassifier(base_estimator=DecisionTreeClassifier(), \n                      n_estimators=500,\n                      bootstrap=False,\n                      max_features=5,\n                      bootstrap_features=True,\n                      n_jobs=-1),    # Random Subspaces Method\n    \n    GaussianNB(),\n    \n    SVC(kernel='linear', C=1),\n    \n    SVC(kernel='poly', degree=3, coef0=0.01, C=5),\n    \n    SVC(kernel='rbf', gamma=0.1, C=2),\n    \n    DecisionTreeClassifier(criterion='entropy', min_samples_leaf=10)\n]","6527efb1":"cv_split = ShuffleSplit(n_splits=10, test_size=0.3, train_size=0.6, random_state=0)\n\nMLA_columns = ['MLA Name', 'MLA Parameters', 'Train F1 Score Mean', 'Test F1 Score Mean', 'Test F1 Score 3*STD' , 'Test Precision Mean', 'Test Recall Mean', 'Training Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\nMLA_predict = train_labels.copy()\n\nrow_index = 0\nfor alg in MLA:\n\n    # set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    # cross validation\n    cv_results = cross_validate(alg, df1, train_labels, cv=cv_split, scoring=['f1','precision', 'recall'], return_train_score=True)\n    \n    MLA_compare.loc[row_index, 'Training Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'Train F1 Score Mean'] = cv_results['train_f1'].mean()\n    MLA_compare.loc[row_index, 'Test F1 Score Mean'] = cv_results['test_f1'].mean()\n    MLA_compare.loc[row_index, 'Test F1 Score 3*STD'] = cv_results['test_f1'].std()*3\n    MLA_compare.loc[row_index, 'Test Precision Mean'] = cv_results['test_precision'].mean()\n    MLA_compare.loc[row_index, 'Test Recall Mean'] = cv_results['test_recall'].mean()\n\n    # save MLA predictions\n    alg.fit(df1, train_labels)\n    MLA_predict[MLA_name] = alg.predict(df1)\n    \n    row_index+=1\n    \n#print and sort table\nMLA_compare.sort_values(by = ['Test F1 Score Mean'], ascending = False, inplace = True)\nMLA_compare","d8d457cf":"cv_split = ShuffleSplit(n_splits=10, test_size=0.3, train_size=0.6, random_state=0)\n\nshortlisted_model = XGBClassifier()\nbase_results = cross_validate(shortlisted_model, df1, train_labels, cv=cv_split, scoring='f1', return_train_score=True)\n\nprint('BEFORE GridSearch Parameters: ', shortlisted_model.get_params())\nprint(\"BEFORE GridSearch Training F1 score mean: {:.2f}\". format(base_results['train_score'].mean()))\nprint(\"BEFORE GridSearch Test F1 score mean: {:.2f}\". format(base_results['test_score'].mean()))\nprint(\"BEFORE GridSearch Test F1 score 3*std: +\/- {:.2f}\". format(base_results['test_score'].std()*3))\nprint('-'*10)\n\nparam_grid = [\n    {\n        'learning_rate': [0.01, 0.03, 0.05, 0.1, 0.25, 0.3],\n         'max_depth': [1,2,4,6,8,10],\n         'n_estimators': [10, 50, 100, 300],\n         'reg_alpha': [0.01, 0.1, 0.3],\n         'reg_lambda': [0.1, 0.2, 0.3, 0.5],\n         'seed': [0]\n    }\n]\n\ngrid_search = GridSearchCV(shortlisted_model, param_grid=param_grid, scoring='f1', cv=cv_split, return_train_score=True)\ngrid_search.fit(df1, train_labels)\n\nprint('AFTER GridSearch Parameters: ', grid_search.best_params_)\nprint(\"AFTER GridSearch Training F1 score mean: {:.2f}\". format(grid_search.cv_results_['mean_train_score'][grid_search.best_index_]))\nprint(\"AFTER GridSearch Test F1 score mean: {:.2f}\". format(grid_search.cv_results_['mean_test_score'][grid_search.best_index_]))\nprint(\"AFTER GridSearch Test F1 score 3*std: +\/- {:.2f}\". format(grid_search.cv_results_['std_test_score'][grid_search.best_index_]*3))\nprint('-'*10)","9e2344fb":"# predictors for voting classifier\npredictor1 = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, min_samples_leaf=4, n_jobs=-1)\npredictor2 = SVC(kernel='rbf', gamma=0.1, C=2, probability=True)\npredictor3 = DecisionTreeClassifier(criterion='entropy', min_samples_leaf=10)\npredictor4 = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n                               n_estimators=500,\n                               bootstrap=False,\n                               max_features=5,\n                               bootstrap_features=True,\n                               n_jobs=-1)\npredictor5 = GaussianNB()\npredictor6 = XGBClassifier(learning_rate=0.05, max_depth=4, n_estimators=50, reg_alpha=0.01, reg_lambda=0.3, seed=0)\npredictor7 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), \n                                n_estimators=200, \n                                algorithm='SAMME.R', \n                                learning_rate=0.5)","a83f43a7":"voting_clf = VotingClassifier(\n    estimators=[('pred1', predictor1), ('pred2', predictor2), ('pred3', predictor3), ('pred4', predictor4), ('pred5', predictor5), ('pred6', predictor6), ('pred7', predictor7)],\n    voting='soft'\n)\n\ncross_val_score(voting_clf, df1, train_labels, cv=5, scoring='f1').mean()","6f872eb7":"rfc = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, min_samples_leaf=4, n_jobs=-1).fit(df1, train_labels)\n\nfeature_names = preprocess_features1 + preprocess_features2 + preprocess_features3 + preprocess_features4 + preprocess_features5 + preprocess_features6\n\nprint(\"Feature Name\\t\", \"Feature Importance\")\nprint(\"-\"*40)\nfor feature_name, score in zip(feature_names, rfc.feature_importances_):\n    print(feature_name,\"\\t\", score)","30f33834":"eval_model = clone(voting_clf)\neval_model.fit(df1, train_labels)\n\neval_predictions = cross_val_predict(eval_model, df1, train_labels, cv=3)","fc5b8e67":"confusion_matrix(train_labels, eval_predictions)","d115e755":"print(classification_report(train_labels, eval_predictions))","9908665a":"plot_precision_recall_curve(eval_model, df1, train_labels, response_method='predict_proba')","de81b04e":"plot_roc_curve(eval_model, df1, train_labels, response_method='predict_proba')","03d9c098":"# copy test set 'Id'\nsubmission_Ids = df_test_raw['PassengerId'].copy()\n\ndf_test_raw['Pclass'] = df_test_raw['Pclass'].astype(str)\n\n# apply preprocessing pipeline to test set\ndf_test_prepared = preprocessor.transform(df_test_raw)","62be90b2":"# FINAL MODEL\nfinal_model = clone(voting_clf)\nfinal_model.fit(df1, train_labels)\n\n# submission predictions\nfinal_pred = final_model.predict(df_test_prepared)","c5c7d5fe":"# create submission file\nmy_submission = pd.DataFrame({'PassengerId': submission_Ids, 'Survived': final_pred})\nmy_submission.to_csv(\"submission.csv\", index=False)","f4d73641":"### 3.3 Data Cleaning (Correcting, Completing, Creating, Converting)","8c953d54":"# 2. Gather The Data\n\nData is provided to us in seperate train and test sets.","4a557754":"### 4.1 Cross-Validation","b430c2e3":"And the ROC curve as well.","4ee362f1":"# 1. Define The Problem\n\nPredict whether a passenger on the Titanic survived or not aka Binary Classification.","032f1d35":"# 3. Prepare Data for Consumption","eef8188e":"Now lets show Precision\/Recall Curve","8bc1541b":"# 5. Validate and Implement","84022b6f":"### 3.2 First Look at the Data","e9d8c4cc":"### 4.2 Tune Model with Hyper-Parameters","ed762ddd":"#### 4.2.2 Feature Importance\n\nNow we will check which features are contributing most towards the final predictions.","105c3c64":"# 4. Model Data","d74eb6e2":"#### 4.2.1 Combine Multiple Predictors (Voting Classifier)","4a40765b":"### 4.3 Evaluate The Model","68a51d27":"We were using F1 score in *cross-validate* but if you want to see exactly how many records were correctly predicted, you can use Confusion Matrix for that.","7cdeeafa":"### 3.1 Import Libraries"}}