{"cell_type":{"dd61bf0f":"code","eaa6b22b":"code","bfb076f2":"code","26aa7455":"code","0789b60f":"code","efc0b19b":"code","9f718ffc":"code","ae31a8ff":"code","3d856746":"code","6bea68f8":"code","36fd6544":"code","a73d9054":"code","248b79d4":"code","aa6e9e5f":"code","ea298a5d":"code","4a848dca":"code","98e355e0":"code","65c82673":"code","f51b991d":"code","06dfd324":"code","8f0f923f":"code","d199910d":"code","de8e6ec8":"code","030aaff6":"code","96770699":"code","cae7577f":"code","9c149592":"code","91618d5d":"code","b2488e5b":"code","d7c2b711":"code","eb8e294c":"code","1e5983b4":"code","5397b1ab":"code","e682ccc3":"code","30ece6e5":"code","01ab9b67":"code","a72e6a9e":"code","1d8eeb32":"code","b3159890":"code","3e888424":"code","e011b53f":"code","cb3d38e4":"code","f85910f1":"code","00cca7a7":"code","d9d9a756":"code","0679bf7f":"code","014685fb":"code","05b8cafb":"code","74508f66":"code","6bb7e0ce":"code","25e5e156":"code","c9676c4d":"code","b3a37e50":"code","9be22a21":"code","e2f0e7cb":"code","43b4699f":"code","9b7b6464":"code","f9b3ce3d":"code","01706093":"code","3003e547":"code","1624b85d":"code","c109ff09":"code","a3277464":"code","3c4e2f10":"code","2ec5d6ad":"code","77191aaa":"code","5f0f8c9e":"code","660be5b3":"code","d6bea42d":"code","3daf1bc7":"code","70ef4980":"code","d4d0f8c6":"code","14dcc761":"code","f6dd0072":"code","f1d88695":"markdown","d8fcf44d":"markdown","656c8c29":"markdown","491db6ef":"markdown","fff6b862":"markdown","bc71d9a1":"markdown","5d292f0b":"markdown","591f554a":"markdown","3dae54fd":"markdown","e644cd3e":"markdown","2f13ab94":"markdown","3d559aec":"markdown","0602b10e":"markdown","6759a4f9":"markdown","15be3fab":"markdown","2bdd837a":"markdown","6bcf4a00":"markdown","2b1aab66":"markdown","8e192dbc":"markdown","f2573703":"markdown","e5a291fe":"markdown","c88e9e0c":"markdown","70ac7fc7":"markdown","68645415":"markdown","61005b1f":"markdown","f610d6c4":"markdown","51c22c3f":"markdown","5069b40f":"markdown","e2562739":"markdown","76ceac2a":"markdown","996a0dfc":"markdown"},"source":{"dd61bf0f":"#load packages\n\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport random as rnd","eaa6b22b":"# loading Common Model Algorithms\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC,LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,BaggingClassifier,AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score,accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.datasets import load_digits\nfrom sklearn import tree\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns","bfb076f2":"train_df= pd.read_csv('..\/input\/lt-vehicle-loan-default-prediction\/train.csv')\ntest_df= pd.read_csv('..\/input\/lt-vehicle-loan-default-prediction\/test.csv')","26aa7455":"# train_df\n# preview the data\n\ntrain_df.head(10)","0789b60f":"# train_df\n#data info\n\ntrain_df.info(max_cols=1000)","efc0b19b":"# train_df\n# data describe\n\ntrain_df.describe()","9f718ffc":"# train_df\n# data describe for object\n\ncategorical_varaibles=train_df.describe(include=['O'])\ncategorical_varaibles","ae31a8ff":"print(test_df.shape,train_df.shape)","3d856746":"#checking missing values(in percentage) in test data\nprint(train_df.isnull().sum()*100\/train_df.shape[0])","6bea68f8":"#checking missing values(in percentage) in test data\nprint(test_df.isnull().sum()*100\/test_df.shape[0])","36fd6544":"train_df = train_df.fillna(train_df.mode().iloc[0])\ntest_df = test_df.fillna(test_df.mode().iloc[0])","a73d9054":"print(train_df['Employment.Type'].isnull().sum(),test_df['Employment.Type'].isnull().sum())","248b79d4":"test_df.shape","aa6e9e5f":"train_df.shape","ea298a5d":"train_df.nunique()","4a848dca":"train_df=train_df.drop(columns ='MobileNo_Avl_Flag')\n","98e355e0":"test_df=test_df.drop(columns ='MobileNo_Avl_Flag')","65c82673":"#lets get all the categorical train data present \nstr_cols = train_df.select_dtypes(include = 'object').columns\ntrain_df[str_cols].head()","f51b991d":"#lets get all the categorical test data \nstr_cols = test_df.select_dtypes(include = 'object').columns\ntest_df[str_cols].head()","06dfd324":"train_df.dtypes","8f0f923f":"#For our analysis we need to change date of birth to age so that its more relevant and acceptable to the model\nnow = pd.Timestamp('now')\ntrain_df['Date.of.Birth'] = pd.to_datetime(train_df['Date.of.Birth'], format='%d-%m-%y')\ntrain_df['Date.of.Birth'] = train_df['Date.of.Birth'].where(train_df['Date.of.Birth'] < now, train_df['Date.of.Birth'] -  np.timedelta64(100, 'Y'))\ntrain_df['Age'] = (now - train_df['Date.of.Birth']).astype('<m8[Y]')\ntrain_df=train_df.drop('Date.of.Birth',axis=1)\n#doing the same for our test data\nnow = pd.Timestamp('now')\ntest_df['Date.of.Birth'] = pd.to_datetime(test_df['Date.of.Birth'], format='%d-%m-%y')\ntest_df['Date.of.Birth'] = test_df['Date.of.Birth'].where(test_df['Date.of.Birth'] < now, test_df['Date.of.Birth'] -  np.timedelta64(100, 'Y'))\ntest_df['Age'] = (now - test_df['Date.of.Birth']).astype('<m8[Y]')\ntest_df=test_df.drop('Date.of.Birth',axis=1)","d199910d":"train_df['Age'].head(5)","de8e6ec8":"test_df['Age'].head(5)","030aaff6":"#For our analysis we need to change date of birth to age so that its more relevant and acceptable to the model\nnow = pd.Timestamp('now')\ntrain_df['DisbursalDate'] = pd.to_datetime(train_df['DisbursalDate'], format='%d-%m-%y')\ntrain_df['DisbursalDate'] = train_df['DisbursalDate'].where(train_df['DisbursalDate'] < now, train_df['DisbursalDate'] -  np.timedelta64(100, 'Y'))\ntrain_df['time_since_loan_dispursed_in_yrs'] = (now - train_df['DisbursalDate']).astype('<m8[Y]')\ntrain_df=train_df.drop('DisbursalDate',axis=1)\n\n#For our analysis we need to change date of birth to age so that its more relevant and acceptable to the model\nnow = pd.Timestamp('now')\ntest_df['DisbursalDate'] = pd.to_datetime(test_df['DisbursalDate'], format='%d-%m-%y')\ntest_df['DisbursalDate'] = test_df['DisbursalDate'].where(test_df['DisbursalDate'] < now, test_df['DisbursalDate'] -  np.timedelta64(100, 'Y'))\ntest_df['time_since_loan_dispursed_in_yrs'] = (now - test_df['DisbursalDate']).astype('<m8[Y]')\ntest_df=test_df.drop('DisbursalDate',axis=1)","96770699":"print(train_df['time_since_loan_dispursed_in_yrs'].head(10),test_df['time_since_loan_dispursed_in_yrs'].head(10))","cae7577f":"#so whats left now.....lets get all the categorical test data \nstr_cols = test_df.select_dtypes(include = 'object').columns\ntest_df[str_cols].head()","9c149592":"#Creating a function for encoding features with only 2 classes (we have only 1 variable like that but still a useful function in case we need in the future)\ndef two_feat_encoding(df_to_transform):\n    le = LabelEncoder()\n\n    for cols in df_to_transform:\n        if df_to_transform[cols].dtype == 'object':\n            if len(list(df_to_transform[cols].unique())) == 2:\n                le.fit(df_to_transform[cols])\n                df_to_transform[cols] = le.transform(df_to_transform[cols])\n    return df_to_transform\ntrain_df=two_feat_encoding(train_df)\ntest_df=two_feat_encoding(test_df)","91618d5d":"#yet again..... whats left now.....lets get all the categorical test data \nstr_cols = test_df.select_dtypes(include = 'object').columns\ntest_df[str_cols].head()","b2488e5b":"# for PERFORM_CNS_SCORE_DESCRIPTION we can reduce the 20 classes by replacing wherever risk information\ntrain_df['PERFORM_CNS.SCORE.DESCRIPTION'].replace(to_replace=['Not Scored: More than 50 active Accounts found','No Bureau History Available','Not Scored: No Activity seen on the customer (Inactive)','Not Scored: No Updates available in last 36 months','Not Enough Info available on the customer','Not Scored: Only a Guarantor','Not Scored: Sufficient History Not Available','Not Scored: Not Enough Info available on the customer'], value= 'No_score', inplace = True)\ntest_df['PERFORM_CNS.SCORE.DESCRIPTION'].replace(to_replace=['Not Scored: More than 50 active Accounts found','No Bureau History Available','Not Scored: No Activity seen on the customer (Inactive)','Not Scored: No Updates available in last 36 months','Not Enough Info available on the customer','Not Scored: Only a Guarantor','Not Scored: Sufficient History Not Available','Not Scored: Not Enough Info available on the customer'], value= 'No_score', inplace = True)\n","d7c2b711":"train_df['PERFORM_CNS.SCORE.DESCRIPTION'].nunique()","eb8e294c":"\nvlow_risk=['A-Very Low Risk','B-Very Low Risk','C-Very Low Risk','D-Very Low Risk']\nlow_risk= ['E-Low Risk','F-Low Risk','G-Low Risk']\nmid_risk= ['H-Medium Risk','I-Medium Risk']\nhigh_risk= ['J-High Risk','K-High Risk']\nvhigh_risk=['L-Very High Risk','M-Very High Risk']\n\ntrain_df['PERFORM_CNS.SCORE.DESCRIPTION'].replace(to_replace='No_score',value = 0,inplace = True)\ntest_df['PERFORM_CNS.SCORE.DESCRIPTION'].replace(to_replace='No_score',value = 0,inplace = True)\ntrain_df['PERFORM_CNS.SCORE.DESCRIPTION'].replace(to_replace=vlow_risk, value= 1, inplace = True)\ntest_df['PERFORM_CNS.SCORE.DESCRIPTION'].replace(to_replace=vlow_risk, value= 1, inplace = True)\ntrain_df['PERFORM_CNS.SCORE.DESCRIPTION'].replace(to_replace=low_risk, value= 2, inplace = True)\ntest_df['PERFORM_CNS.SCORE.DESCRIPTION'].replace(to_replace=low_risk, value= 2, inplace = True)\ntrain_df['PERFORM_CNS.SCORE.DESCRIPTION'].replace(to_replace=mid_risk, value= 3, inplace = True)\ntest_df['PERFORM_CNS.SCORE.DESCRIPTION'].replace(to_replace=mid_risk, value= 3, inplace = True)\ntrain_df['PERFORM_CNS.SCORE.DESCRIPTION'].replace(to_replace=high_risk, value= 4, inplace = True)\ntest_df['PERFORM_CNS.SCORE.DESCRIPTION'].replace(to_replace=high_risk, value= 4, inplace = True)\ntrain_df['PERFORM_CNS.SCORE.DESCRIPTION'].replace(to_replace=vhigh_risk, value= 5, inplace = True)\ntest_df['PERFORM_CNS.SCORE.DESCRIPTION'].replace(to_replace=vhigh_risk, value= 5, inplace = True)\n","1e5983b4":"test_df['PERFORM_CNS.SCORE.DESCRIPTION'].head()","5397b1ab":"#now yet again..... whats left now.....lets get all the categorical train data \nstr_cols = test_df.select_dtypes(include = 'object').columns\ntrain_df[str_cols].head()","e682ccc3":"train_df[['AVERAGE.ACCT.AGE_1','AVERAGE.ACCT.AGE_2']]=train_df['AVERAGE.ACCT.AGE'].str.split(expand=True)\ntest_df[['AVERAGE.ACCT.AGE_1','AVERAGE.ACCT.AGE_2']]=test_df['AVERAGE.ACCT.AGE'].str.split(expand=True)\ntrain_df=train_df.drop(columns ='AVERAGE.ACCT.AGE')\ntest_df=test_df.drop(columns ='AVERAGE.ACCT.AGE')","30ece6e5":"train_df[['CREDIT.HISTORY.LENGTH_1','CREDIT.HISTORY.LENGTH_2']]=train_df['CREDIT.HISTORY.LENGTH'].str.split(expand=True)\ntest_df[['CREDIT.HISTORY.LENGTH_1','CREDIT.HISTORY.LENGTH_2']]=test_df['CREDIT.HISTORY.LENGTH'].str.split(expand=True)\ntrain_df=train_df.drop(columns ='CREDIT.HISTORY.LENGTH')\ntest_df=test_df.drop(columns ='CREDIT.HISTORY.LENGTH')","01ab9b67":"#so whats left now.....lets get all the categorical test data \nstr_cols = train_df.select_dtypes(include = 'object').columns\ntrain_df[str_cols].head()","a72e6a9e":"#stripping months and years\ntrain_df['CREDIT.HISTORY.LENGTH_1']=train_df['CREDIT.HISTORY.LENGTH_1'].str.strip('yrs')\ntest_df['CREDIT.HISTORY.LENGTH_1']=test_df['CREDIT.HISTORY.LENGTH_1'].str.strip('yrs')\ntrain_df['CREDIT.HISTORY.LENGTH_2']=train_df['CREDIT.HISTORY.LENGTH_2'].str.strip('mon')\ntest_df['CREDIT.HISTORY.LENGTH_2']=test_df['CREDIT.HISTORY.LENGTH_2'].str.strip('mon')\ntrain_df['AVERAGE.ACCT.AGE_1']=train_df['AVERAGE.ACCT.AGE_1'].str.strip('yrs')\ntest_df['AVERAGE.ACCT.AGE_1']=test_df['AVERAGE.ACCT.AGE_1'].str.strip('yrs')\ntrain_df['AVERAGE.ACCT.AGE_2']=train_df['AVERAGE.ACCT.AGE_2'].str.strip('mon')\ntest_df['AVERAGE.ACCT.AGE_2']=test_df['AVERAGE.ACCT.AGE_2'].str.strip('mon')\n\n#converting datatype\ntrain_df['CREDIT.HISTORY.LENGTH_1'] = train_df['CREDIT.HISTORY.LENGTH_1'].astype(int)\ntest_df['CREDIT.HISTORY.LENGTH_1'] = test_df['CREDIT.HISTORY.LENGTH_1'].astype(int)\ntrain_df['CREDIT.HISTORY.LENGTH_2'] = train_df['CREDIT.HISTORY.LENGTH_2'].astype(int)\ntest_df['CREDIT.HISTORY.LENGTH_2'] = test_df['CREDIT.HISTORY.LENGTH_2'].astype(int)\ntrain_df['AVERAGE.ACCT.AGE_1'] = train_df['AVERAGE.ACCT.AGE_1'].astype(int)\ntest_df['AVERAGE.ACCT.AGE_1'] = test_df['AVERAGE.ACCT.AGE_1'].astype(int)\ntrain_df['AVERAGE.ACCT.AGE_2'] = train_df['AVERAGE.ACCT.AGE_2'].astype(int)\ntest_df['AVERAGE.ACCT.AGE_2'] = test_df['AVERAGE.ACCT.AGE_2'].astype(int)\n\n# since we need to conctanate month value lets divide by 12 and round them off\ntrain_df['CREDIT.HISTORY.LENGTH_2']=round((train_df['CREDIT.HISTORY.LENGTH_2']\/12),2)\ntest_df['CREDIT.HISTORY.LENGTH_2']=round((test_df['CREDIT.HISTORY.LENGTH_2']\/12),2)\ntrain_df['AVERAGE.ACCT.AGE_2']=round((train_df['AVERAGE.ACCT.AGE_2']\/12),2)\ntest_df['AVERAGE.ACCT.AGE_2']=round((test_df['AVERAGE.ACCT.AGE_2']\/12),2)\n\n","1d8eeb32":"#concatenating and converting\ncolumnss=['AVERAGE.ACCT.AGE_1','AVERAGE.ACCT.AGE_2','CREDIT.HISTORY.LENGTH_1','CREDIT.HISTORY.LENGTH_2']\ntrain_df['AVERAGE.ACCT.AGE']= train_df['AVERAGE.ACCT.AGE_1'].astype(float) + train_df['AVERAGE.ACCT.AGE_2'].astype(float)\ntest_df['AVERAGE.ACCT.AGE']= test_df['AVERAGE.ACCT.AGE_1'].astype(float) + test_df['AVERAGE.ACCT.AGE_2'].astype(float)\ntrain_df['CREDIT.HISTORY.LENGTH']= train_df['CREDIT.HISTORY.LENGTH_1'].astype(float) + train_df['CREDIT.HISTORY.LENGTH_2'].astype(float)\ntest_df['CREDIT.HISTORY.LENGTH']= test_df['CREDIT.HISTORY.LENGTH_1'].astype(float) + test_df['CREDIT.HISTORY.LENGTH_2'].astype(float)\ntrain_df['AVERAGE.ACCT.AGE']=train_df['AVERAGE.ACCT.AGE'].astype(float)\ntest_df['AVERAGE.ACCT.AGE']=test_df['AVERAGE.ACCT.AGE'].astype(float)\ntrain_df['CREDIT.HISTORY.LENGTH']=train_df['CREDIT.HISTORY.LENGTH'].astype(float)\ntest_df['CREDIT.HISTORY.LENGTH']=test_df['CREDIT.HISTORY.LENGTH'].astype(float)\n","b3159890":"train_df['AVERAGE.ACCT.AGE'].head(12)","3e888424":"train_df=train_df.drop(columns=columnss)\ntest_df=test_df.drop(columns=columnss)\n","e011b53f":"train_df.columns","cb3d38e4":"#so whats left now.....lets get all the categorical test data \nstr_cols = test_df.select_dtypes(include = 'object').columns\ntest_df[str_cols].head()","f85910f1":"train_df.nunique()","00cca7a7":"ids_to_drop = ['UniqueID','supplier_id','Current_pincode_ID','branch_id','Employee_code_ID']\ntrain_df=train_df.drop(columns=ids_to_drop)\ntest_df=test_df.drop(columns=ids_to_drop)","d9d9a756":"train_df.nunique()","0679bf7f":"related_to_drop = ['PERFORM_CNS.SCORE','PRI.NO.OF.ACCTS','SEC.NO.OF.ACCTS']\ntrain_df=train_df.drop(columns=related_to_drop)\ntest_df=test_df.drop(columns=related_to_drop)","014685fb":"train_df.head(4)","05b8cafb":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()","74508f66":"scaler.fit(train_df.drop(columns='loan_default'))\nscaler.fit(test_df)","6bb7e0ce":"scaled_values_1=scaler.transform(train_df.drop(columns='loan_default'))\nscaled_values_2=scaler.transform(test_df)\n","25e5e156":"num_data = list(train_df._get_numeric_data().columns)\nnum_data.remove('loan_default')\nscaler = StandardScaler()\nscaler.fit(train_df[num_data])\nnormalized = scaler.transform(train_df[num_data])\nnormalized2 = scaler.transform(test_df[num_data])\nnormalized_train = pd.DataFrame(normalized , columns=num_data)\nnormalized2_test = pd.DataFrame(normalized2 , columns=num_data)\nprint(\"The shape of normalised numerical data : \" , normalized.shape)\nprint(\"The shape of normalised numerical data : \" , normalized2.shape)","c9676c4d":"#adding our predictor variable\nnormalized_train['loan_default']=train_df['loan_default']\n","b3a37e50":"\nX=normalized_train.drop(columns='loan_default')\ny=normalized_train['loan_default']","9be22a21":"normalized_train.head(2)","e2f0e7cb":"from sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(X,y)","43b4699f":"print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers","9b7b6464":"#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(15).plot(kind='barh')\nplt.show()","f9b3ce3d":"feat_importances.nlargest(15)","01706093":"# here we will select the top 10 variables\nnew_train_df=train_df[['ltv','disbursed_amount','asset_cost','Age','State_ID','manufacturer_id','CREDIT.HISTORY.LENGTH','AVERAGE.ACCT.AGE','PRIMARY.INSTAL.AMT','PRI.CURRENT.BALANCE','PRI.DISBURSED.AMOUNT','PRI.SANCTIONED.AMOUNT','NO.OF_INQUIRIES','PRI.ACTIVE.ACCTS','PERFORM_CNS.SCORE.DESCRIPTION','loan_default']]\nnew_test_df=test_df[['ltv','disbursed_amount','asset_cost','Age','State_ID','manufacturer_id','CREDIT.HISTORY.LENGTH','AVERAGE.ACCT.AGE','PRIMARY.INSTAL.AMT','PRI.CURRENT.BALANCE','PRI.DISBURSED.AMOUNT','PRI.SANCTIONED.AMOUNT','NO.OF_INQUIRIES','PRI.ACTIVE.ACCTS','PERFORM_CNS.SCORE.DESCRIPTION']]","3003e547":"new_train_df.columns","1624b85d":"X=new_train_df.drop(columns='loan_default')\ny=new_train_df['loan_default']","c109ff09":"# this is a function for complete evaluation of models\ndef model_performance(model):\n    model.fit(X_train,y_train)\n    RF_training_labels = model.predict(X_train)\n    RF_test_labels = model.predict(X_test)\n    Training_accuracy = model.score(X_train, y_train, sample_weight=None)\n    Test_accuracy = model.score(X_test, y_test, sample_weight=None)\n    F1_score_train = f1_score(y_train, RF_training_labels, average = 'weighted')\n    F1_score_test = f1_score(y_test, RF_test_labels, average = 'weighted')\n    Recall_train = recall_score(y_train, RF_training_labels, average = 'weighted') \n    Recall_test  = recall_score(y_test, RF_test_labels, average = 'weighted') \n    Precision_train = precision_score(y_train, RF_training_labels, average = 'weighted')\n    Precision_test = precision_score(y_test, RF_test_labels, average = 'weighted')\n    accuracy_train = accuracy_score(y_train, RF_training_labels, )\n    accuracy_test = accuracy_score(y_test, RF_test_labels)\n    rf_cm_tr = confusion_matrix(y_train, RF_training_labels)\n    rf_cm_te = confusion_matrix(y_test, RF_test_labels)\n    print(\"Training_accuracy - \", Training_accuracy)\n    print(\"Test_accuracy - \", Test_accuracy)\n    print(\"F1_score_train - \", F1_score_train)\n    print(\"F1_score_test - \", F1_score_test)\n    print(\"Recall_train - \", Recall_train)\n    print(\"Recall_test - \", Recall_test)\n    print(\"Precision_train - \", Precision_train)\n    print(\"Precision_test - \", Precision_test)\n    #Confusion Matrix\n    class_names=[0,1] # name  of classes\n    fig, ax = plt.subplots()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names)\n    plt.yticks(tick_marks, class_names)\n    # create heatmap\n    sns.heatmap(pd.DataFrame(rf_cm_te), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n    ax.xaxis.set_label_position(\"top\")\n    plt.tight_layout()\n    plt.title('Test Confusion matrix', y=1.1)\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n\n    ","a3277464":"\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)\n\nlen(X_train)\nlen(X_test)\n\n\n\nm1 = tree.DecisionTreeClassifier()\nm2 = BaggingClassifier()\nm3 = RandomForestClassifier()\nm4 = AdaBoostClassifier()\nm5 = GradientBoostingClassifier()\nm6= GaussianNB()\n\nmodels=[m1,m2,m3,m4,m5,m6]\n\nfor i in range(0,len(models)):\n   print(model_performance(models[i]))\n    \n","3c4e2f10":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis","2ec5d6ad":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)","77191aaa":"m1 = LogisticRegression()\nm2 = KNeighborsClassifier()\n\nmodels=[m1,m2]\nfor i in range(0,len(models)):\n   print(model_performance(models[i]))","5f0f8c9e":"\nclf = DecisionTreeClassifier(random_state=0)\nclf.fit(X_train,y_train)","660be5b3":"path = clf.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities","d6bea42d":"ccp_alphas","3daf1bc7":"\n#clfs = []\n#for ccp_alpha in ccp_alphas:\n#    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n#    clf.fit(X_train, y_train)\n#    clfs.append(clf)\n#print(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n#      clfs[-1].tree_.node_count, ccp_alphas[-1]))","70ef4980":"predictions = clf.predict(new_test_df)","d4d0f8c6":"y_pred=pd.DataFrame(predictions)","14dcc761":"new_test_df['predicted default on loan']=y_pred","f6dd0072":"new_test_df.to_csv(r'C:\\Users\\Admin\\Desktop\\Data_science_projects\\Submissions.csv', index = False)\n","f1d88695":"# Vehicle loan prediction \n## A Predictive Model by \n## Arshan Khan\n## Connect with me on : https:\/\/www.linkedin.com\/in\/arshan-khan-b54420117\/","d8fcf44d":"## so far so good :)","656c8c29":"## we can now finally move to training our model \n\n## 1. Tree Based Models","491db6ef":"## Problem Defination\nProblem statement  develop an algorithm to predict the default of a customer for vehicle loans By identifying key attritubes.\n\nMotivation : Financial institutions incur significant losses due to the default of vehicle loans. This has led to the tightening up of vehicle loan underwriting and increased vehicle loan rejection rates. The need for a better credit risk scoring model is also raised by these institutions. ","fff6b862":"## the above step has taken a lot of time since the dataset is large its diffcult to do DT prunining hence training the model as it is","bc71d9a1":"### converting date and time related variables to int\n","5d292f0b":"## pruning \n","591f554a":"## feature selection ","3dae54fd":"## Here we can see that KNN performs better however not as good as DT so hence we will go with DT","e644cd3e":"#### from the above information we can straightaway say that the variable MobileNo_Avl_Flag is not going to provide any unique information to our predictive since it has only 1 class hence we can drop that variable","2f13ab94":"### lets look at other variables\n","3d559aec":"### checking missing values","0602b10e":"### Here we will select the model that has high accuracy, high TN and FN and equal ration of FP and FN .. so now lets look at \n\n#1 DT : this model looks good as it has high TP and almost equal ratio of FN and FP but it has low TN\n#2 Bagging : Ratio of FN and FP is high ---need to discard\n#3 RandomForestClassifier : Ratio of FN and FP is high ---need to discard\n#4 AdaBoostClassifier :Ratio of FN and FP is high ---need to discard\n#5 Gradient Boosting : Ratio of FN and FP is high ---need to discard\n#6 GNB : very high amount of FP we will discard this model\n\n### hence here we can only go with DT rest all models will certainly misclassify . we can try pruning for imrpving the model but before lets try distance based models\n\n### note: here hyper parameter optimization for ensemble models was not possible because my pc was overheating and was taking a lot of time during the tuning process\n\n","6759a4f9":"## Great ! we have completed our task with categorical variables now lets move on to continous variables.....but before that lets check out our target variable","15be3fab":"#### here we can see the following insights\n1. We need to change the date and time related variables  (Date.of.Birth, DisbursalDate,AVERAGE.ACCT.AGE and AVERAGE.ACCT.AGE)  to int \n2. Employment.Type has 2 classes we need to label encode it \n3. The beauruea data history is given under the variable PERFORM_CNS.SCORE.DESCRIPTION with 20 distinct classes  we need to figure out a way to encode it such that its acceptable to our model","2bdd837a":"#### still a lot of classes :(...... lets try to reduce further..","6bcf4a00":"###  checking for unique attributes ","2b1aab66":"# Finally Submission !!","8e192dbc":"# VEHICLE LOAN PREDICTION","f2573703":"## Challenges faced during this modelling task\n## 1. Not too much familiar with financial data but eager to learn more\n## 2. No margin to do hyperparameter tuning as pc is not configured for such task in a big dataset","e5a291fe":"##### Note :we could have gone for complete case analysis (Removing all records corresponding to the missing value column) but that wouldnt be favorable because we will be losing all the information vital to our analysis as a result of those removed records. so imputation seems to be the best case scenario\n","c88e9e0c":"## 2. Distance based models\n","70ac7fc7":"####  Load Data Modelling Libraries\nWe will use the popular scikit-learn library to develop our machine learning algorithms and for data visualization, we will use the matplotlib and seaborn library. Below are common classes to load.","68645415":"## Model fitting","61005b1f":"#### we will select these variables for our analysis","f610d6c4":"### Data preparation\nTo begin this step, The data is imported firstly . Next we use the info()function, to get a quick and dirty overview of variable datatypes (i.e. qualitative vs quantitative). Click here for the [Source Data Dictionary](https:\/\/www.kaggle.com\/sneharshinde\/ltfs-av-data).","51c22c3f":"#### we can see that in both trian and test data we have one variable that is Employment.Type which has less than 4 percent of missing data we can hence we can do mode imputation for the same ","5069b40f":"## now lets go ahead and select top 15 features from this using tree selection method","e2562739":"#### we can see that within the classes we have 'A-very Low risk', 'B-Very Low risk'......and so one till 'M-Very High risk'  since we trying to simplify our model as much as possible we will reduce these features further. From a finance standpoint a high risk means high Probability of default so we will assign the following values to risk information\n#### No_score = 0\n#### 'A-Very Low Risk','B-Very Low Risk','C-Very Low Risk','D-Very Low Risk' = 1\n#### 'E-Low Risk','F-Low Risk','G-Low Risk' = 2\n#### 'H-Medium Risk','I-Medium Risk = 3\n#### 'J-High Risk','K-High Risk' = 4 \n#### 'L-Very High Risk','M-Very High Risk' = 5\n","76ceac2a":"## The ID variable dilemma\n### here we need to look out for ID related variables such as manufacturer id employee id etc. how do we need whether we should select these variables? lets look from a financial standpoint\n#1 UniqueID - unique id of loan candidate. this is purely nominal and needs to be dropped\n#2 supplier_id - denotes distinct supplier. needs to be dropped beacuse it is nominal\n#3 Current_pincode_ID - denotes location and has nothing to do with probablity of default or default prediction will be dropped\n#4 Branch.id - there are 82 branch.ids denoting 82 seperate branches this variable denotes the branch from which the vehicle was taken .This is purely nominal and has no order at all so we can drop this variable\n#5 State_ID - Donotes the states registration of the vehicle (like MH for maharashtra , KA for karnataka) this may matter because prices of vehicels vary state to state\n#6 Employee_code_ID - denotes the respective employee number - purely nominal - dropped\n#7 VoterID_flag  - certainly will make an impact... should be there in our analysis\n#8 manufacturer_id - there are 11 such variables and it denotes unique manufacturer for vehicles. this should be considered since prices vary manufacturer to manufacturer\n","996a0dfc":"## related variables :\n#1. clearly PERFORM_CNS.SCORE and PERFORM_CNS.SCORE.DESCRIPTION are related lets drop PERFORM_CNS.SCORE since we dont need to 573 type of scores ...thats makes the model a little too complicated\n#2.PRI.NO.OF.ACCTS contains PRI.ACTIVE.ACCTS, PRI.OVERDUE.ACCTS this should be dropped as it we dont need this extra variable\n#3 clearly SEC.NO.OF.ACCTS = SEC.OVERDUE.ACCTS + SEC.ACTIVE.ACCTS we can drop this also\n"}}