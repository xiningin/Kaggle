{"cell_type":{"a6f430f0":"code","a90cb30c":"code","05269f62":"code","c56639e2":"code","cce2d4dd":"code","3010542d":"code","0bec32c4":"code","9d03856c":"code","d32fa932":"code","effd2cab":"code","38f62872":"code","5f4c2f6e":"code","cd53b538":"code","eca412a9":"code","1f33163f":"code","e1ce5388":"code","07fcee3f":"code","3e348bc3":"code","74090fd6":"code","c8f38ab6":"code","275d2350":"code","276620f2":"code","93c0a322":"code","3a3d6b85":"code","24bf2440":"code","8ea11ef6":"code","435a5e80":"code","30a92557":"code","6bd889ea":"code","09ed384d":"code","7ad9af91":"code","5b6649a3":"code","ee9c7032":"code","61e2ef39":"code","9f4b9390":"code","bdc6cae3":"code","f79ffe5d":"code","99fcefcf":"code","5d5a844b":"code","f2a17d21":"code","42f38508":"code","4081e95f":"code","d055dfdc":"code","a76fbf01":"code","0bb4f74d":"code","ae30cb93":"code","048762b1":"code","a982768b":"code","0021940e":"code","22440d7e":"code","68cdc436":"code","590857a8":"code","8b506419":"code","de0ca982":"code","7a36139e":"code","42e880ee":"code","72dded58":"code","53f1c7a5":"code","f2f2059d":"code","940b461c":"code","1fcd1c0a":"code","354a31c4":"code","51b467f2":"code","6413c02a":"code","85657c0e":"code","0d83c4c3":"code","93f2eddf":"code","15f78965":"code","26cfcc4b":"code","5f6d1166":"code","5ecc2637":"code","aadd0bae":"code","c66fc71a":"code","eb5334d8":"code","b1802c00":"code","7da9c6ff":"code","5264d032":"code","59fe0c55":"code","c47e0e8b":"code","69b03959":"code","0b02b4c5":"code","0f963609":"code","872cb038":"code","0d9d1ec2":"code","cb22808b":"code","bd1e4bdf":"code","ea6f8a82":"code","dcac6f94":"code","3c05b185":"code","b868d155":"code","554a4c55":"code","ca41c256":"code","ee0b7322":"code","5b1512ec":"code","aba6db75":"code","dd2b72d9":"code","ce6d157e":"code","99f0d6d8":"code","9fe770d7":"code","667e2e78":"code","b4eb5e3b":"code","c4c93bb0":"code","b4b3c2f2":"code","dec267a8":"code","1a19d1d5":"code","9bbb3f94":"code","91c045e3":"code","472312ae":"code","8e4e0b62":"code","809c83a0":"code","c5871f3c":"code","4f8ad20d":"code","2ad682e5":"code","60438ca8":"code","e91cc71f":"code","6eb7715e":"code","63da6d3f":"code","66c2cd80":"code","7be090d5":"code","6ace31ec":"code","1c5df470":"code","ab0ba80b":"code","49ee66c3":"code","d9e8a7e5":"code","19b2db94":"code","b55b8aa3":"code","a02375d6":"code","25fe51cb":"code","1aedec1a":"code","5b9a57a5":"code","0bfdf6df":"code","5b70a932":"code","377cb749":"code","fa9a179b":"code","fb75a3cd":"markdown","c351fd34":"markdown","fd798608":"markdown","5f5249ca":"markdown","ce7a1879":"markdown","c9b1e2dc":"markdown","c7600aa5":"markdown","b71e88c8":"markdown","11c0621e":"markdown","351e21b1":"markdown","1d5608ec":"markdown","bf35d3b4":"markdown","cd31726c":"markdown","ebeb4029":"markdown","f6c22255":"markdown","bcf442ff":"markdown","b290fe11":"markdown","92da4b6d":"markdown","67c043dd":"markdown","916ae621":"markdown","b295e720":"markdown","53bdacd0":"markdown","2f712e6e":"markdown","2d8e0d8f":"markdown","4a66a68b":"markdown","b36ee727":"markdown","cf696f08":"markdown","488f8712":"markdown","89b73294":"markdown","bd6d2acb":"markdown","7a6862fc":"markdown","8dda32ed":"markdown","17aeb635":"markdown","77606e03":"markdown","28b831ae":"markdown","43791665":"markdown","ff049d7b":"markdown","8053aac6":"markdown","d754adde":"markdown","48227fca":"markdown","ea5abce0":"markdown","bb171604":"markdown","a61cfc50":"markdown","31c9e2f3":"markdown","8278495b":"markdown","75dbfd9b":"markdown","224d3861":"markdown","c3496aa2":"markdown","3379fa29":"markdown","ad12e27c":"markdown","aad5fc37":"markdown","90ad5503":"markdown","d46cec3a":"markdown","1e960ce2":"markdown","56165665":"markdown","d9f17f33":"markdown","f148b77a":"markdown","5b28c902":"markdown","7518d81b":"markdown","e7cff5a3":"markdown","71678e24":"markdown","923d093d":"markdown","17c02569":"markdown","0c8b70b6":"markdown","82a80221":"markdown","853cc5cc":"markdown","ed316182":"markdown","d363a74d":"markdown","41e100bc":"markdown","093cf339":"markdown","6628447f":"markdown","18c85750":"markdown","07946e7c":"markdown","24cc26e9":"markdown","3f9feaf6":"markdown"},"source":{"a6f430f0":"#Import Required Packages\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport math\nimport datetime\n\nfrom scipy import stats\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler,OrdinalEncoder,LabelEncoder\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.metrics import r2_score,mean_squared_error\n\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# set default plot style\nsns.set_style('darkgrid')","a90cb30c":"# import warnings module and set ignore to hide the warning messages\nimport warnings\nwarnings.filterwarnings('ignore')","05269f62":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c56639e2":"# read the csv dataset\ndf = pd.read_csv('\/kaggle\/input\/housing-prices-dataset\/train.csv')","cce2d4dd":"# Preview the data\ndf.head()","3010542d":"# check the shape of the dataset\ndf.shape","0bec32c4":"# view the dataframe info\ndf.info()","9d03856c":"# dropping column Id from dataframe\ndf.drop('Id',axis=1,inplace=True)","d32fa932":"# defind a method to check null percentage of the features\ndef check_null_percentage(df):\n    missing_info = pd.DataFrame(np.array(df.isnull().sum().sort_values(ascending=False).reset_index())\\\n                                ,columns=['Columns','Missing_Percentage']).query(\"Missing_Percentage > 0\").set_index('Columns')\n    return 100*missing_info\/df.shape[0]","effd2cab":"# view the null percentage of each feature\ncheck_null_percentage(df)","38f62872":"# list all the null columns\nNA_columns = ['Alley','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','FireplaceQu',\\\n          'GarageType','GarageFinish','GarageQual','GarageCond','PoolQC','Fence','MiscFeature']\n\n# fill the null values with NA\ndf[NA_columns] = df[NA_columns].fillna('NA')","5f4c2f6e":"# check in any rows had more than 5 na if it has more tha 5 na features willcan consider to drop them\ndf[df.isnull().sum(axis=1) > 5]","cd53b538":"# check if there are duplicated rows so if there are any we can drop them\ndf.duplicated(keep=False).sum()","eca412a9":"# Create a method that returns a tuple that give information on the top most common value,\n# its percentage and count for each feature\ndef top_unique_count(x):\n    unq_cnt = ( x.value_counts(ascending=False,dropna=False).head(1).index.values[0],\n               100 * x.value_counts(ascending=False,dropna=False).head(1).values[0]\/df.shape[0],\n               x.value_counts(ascending=False,dropna=False).head(1).values[0])\n    return unq_cnt","1f33163f":"#Assign it to a variable and provide column name once the tuble get converted to actual dataframe columns\nunique_df = df.apply(lambda x: top_unique_count(x)).rename(index={0:\"Value\",1:'Percentage',2:'Count'})\\\n    .T.sort_values(by='Count',ascending=False)\nunique_df.head(25)","e1ce5388":"# view the null percentage of each feature\ncheck_null_percentage(df)","07fcee3f":"df['LotFrontage'] = df.groupby(['Neighborhood','LotConfig'])['LotFrontage'].\\\n                        apply(lambda x: np.Nan if x.median() == np.NaN else x.fillna(x.median()))\ndf['LotFrontage'].isnull().sum()","3e348bc3":"df['LotFrontage'] = df.groupby(['LotConfig'])['LotFrontage'].apply(lambda x: x.fillna(x.median()))","74090fd6":"df.loc[df.GarageYrBlt.isnull(),['GarageType','GarageCars','GarageArea','GarageFinish','GarageYrBlt','GarageQual','GarageCond']]","c8f38ab6":"df.YearBuilt.isnull().sum()","275d2350":"# replacing null values of GarageYrBlt with YearBuilt\ndf.loc[df.GarageYrBlt.isnull(),'GarageYrBlt'] = df.loc[df.GarageYrBlt.isnull(),'YearBuilt']","276620f2":"# fill 0 and Not Present for numerical and categorical feature's null values\ndf.MasVnrArea.fillna(0,inplace=True)\ndf.MasVnrType.fillna('Not present',inplace=True)","93c0a322":"# Adding square feet of first floor and second floor\ndf['TotalFlrSFAbvGrd'] = df[['1stFlrSF','2ndFlrSF']].sum(axis=1)\n# Adding all the bathrooms\ndf['TotalBath'] = df[['BsmtFullBath','BsmtHalfBath','FullBath','HalfBath']].sum(axis=1)\n# Adding square feet of all Porcch\ndf['TotalPorchSF'] = df[['OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','WoodDeckSF']].sum(axis=1)","3a3d6b85":"check_null_percentage(df)","24bf2440":"# get the features that holds more than 90% of its data with a same value\ndrop_columns = unique_df.query('Percentage > 90.0').index.values\ndrop_columns","8ea11ef6":"# drop the columns from above analysis\ndf.drop(columns=drop_columns,inplace=True)\ndel drop_columns","435a5e80":"# create a list of numerical features\nnumerical_features = list(df.select_dtypes(include=[np.number]).columns.values)\n\n# create a list of features that or categorical\ncategorical_features = list(df.select_dtypes(include=[np.object]).columns.values)\n\n# Creata feature list for time sereis data\ntimeseries_features = ['YearBuilt', 'YearRemodAdd', 'YrSold', 'MoSold', 'GarageYrBlt']","30a92557":"# removing times series features from numerice to avoid repetition\nfor col in timeseries_features:\n    numerical_features.remove(col) ","6bd889ea":"# adding numerical features to categrical if the unique value count in a feature is less tha are equal to 10\ncat_feature = pd.Series(df[numerical_features].nunique().sort_values(),name='Count').to_frame().query('Count <= 10').index.values\ncategorical_features.extend(cat_feature)","09ed384d":"# removing the numerical features tht belong to time series\nfor col in cat_feature:\n    numerical_features.remove(col)","7ad9af91":"# define subplot columns and rows and the figure size\nfig,ax = plt.subplots(math.ceil(len(numerical_features)\/3),3,figsize=(15,30),sharey=True)\n\n# initialize the row and column variable\n\ni ,j = 0, 0\nfor col in sorted(numerical_features):\n\n    # Plot a regression plot for the numerical feature and SalePrice\n    sns.regplot(col,'SalePrice',data=df,ax=ax[i][j])\n    if j == 2:\n        j=0\n        i +=1\n    else:\n        j +=1\n        \n# hide last two grids as it doesn't have any plots to show\nax[6][1].set_visible(False)\nax[6][2].set_visible(False)","5b6649a3":"# define the number of outliers to be handled for each feature\nfeature_outlier_count = {'1stFlrSF':1,\n                'BsmtFinSF1':1,\n                'BsmtFinSF2':1,\n                'EnclosedPorch':2,\n                'GarageArea':4,\n                'GrLivArea':4,\n                'LotArea':4,\n                'LotFrontage':2,\n                'MasVnrArea':1,\n                'OpenPorchSF':3,\n                'TotalBsmtSF':4,\n                'TotRmsAbvGrd':1,\n                'TotalFlrSFAbvGrd':2,\n                'TotalPorchSF':1,\n                'WoodDeckSF':3}","ee9c7032":"# define a method to print the outlier to have a visual representation of the data with saleprice\ndef print_outliers(feature_list):\n    for k,v in feature_list.items():\n        if v:\n            display(df.loc[df[k].isin(sorted(df[k])[-v:]),[k,'SalePrice']])\n\n# returns the outlier highest value or the value specific to index when specified \ndef get_outliers(feature,index=-1):\n    return df.loc[df[feature] == sorted(df[feature])[index],[feature,'SalePrice']].sort_values(by=feature,ascending=False)","61e2ef39":"# prints the outlier for each feature\nprint_outliers(feature_outlier_count)","9f4b9390":"# Get teh index of the outlier in Feature 1stFlrSF\noutlier_index = get_outliers('1stFlrSF').index.values[0]\noutlier_index","bdc6cae3":"df.iloc[1298]","f79ffe5d":"# Remove the outlier record with its index value and if the same outlier \n# is present in other features as well reduce the count in outlier_features\ndef remove_outlier_features_count_for_index(outlier_idx):\n    for col in feature_outlier_count.keys():\n        if (feature_outlier_count[col] > 0) & (outlier_index in get_outliers(col).index.values):\n            feature_outlier_count[col] = feature_outlier_count[col]-1 \n    df.drop(outlier_index,inplace=True)\n    df.reset_index(drop=True,inplace=True)","99fcefcf":"remove_outlier_features_count_for_index(outlier_index)","5d5a844b":"df.loc[df.index[get_outliers('TotRmsAbvGrd').index.values[0]],'TotRmsAbvGrd'] = df.loc[df['SalePrice'] == get_outliers('TotRmsAbvGrd').SalePrice.values[0],'TotRmsAbvGrd'].mode()[0]\n\nfeature_outlier_count['TotRmsAbvGrd'] = 0","f2a17d21":"def fix_outliers(outlier_features_list):\n    for k,v in outlier_features_list.items():\n        while v > 0:\n            # replacing the outliers by taking mean of four closest feature value of the outlier at the salePrice Range\n            replace_with = df.loc[(df['SalePrice']-get_outliers(k)['SalePrice'].values[0]).abs().argsort()[v:v+4],k].mean()\n            if (df[k].dtypes == np.int64) | (df[k].dtypes == np.int32):\n                df.loc[df.index[get_outliers(k).index.values[0]],k] = int(replace_with)\n            else:\n                df.loc[df.index[get_outliers(k).index.values[0]],k] = round(replace_with,1)        \n            v = v-1\n            feature_outlier_count[k] = v","42f38508":"# pass the dictionary containg all the features with number of outliers to be fixed\nfix_outliers(feature_outlier_count)","4081e95f":"df[['1stFlrSF','SalePrice']].sort_values(by='1stFlrSF',ascending=False)[:3]","d055dfdc":"df.loc[df.index[get_outliers('1stFlrSF',-2).index.values[0]],'1stFlrSF'] = df.loc[(df['SalePrice']-get_outliers('1stFlrSF',-2)['SalePrice'].values[0]).abs().argsort()[1:1+4],'1stFlrSF'].mean()","a76fbf01":"df[['BsmtFinSF1','SalePrice']].sort_values(by='BsmtFinSF1',ascending=False)[:3]","0bb4f74d":"df[['LotArea','SalePrice']].sort_values(by='LotArea',ascending=False)[:7]","ae30cb93":"#Assign the number of outliers to be fixed\nfeature_outlier_count['LotArea']=3\nfeature_outlier_count['BsmtFinSF1']=1\n\n# call the method to fix hte outliers\nfix_outliers(feature_outlier_count)","048762b1":"#Delet the variable as its not needed\ndel feature_outlier_count","a982768b":"# define subplot columns and rows and the figure size\nfig,ax = plt.subplots(math.ceil(len(numerical_features)\/3),3,figsize=(15,30),sharey=True)\n\n# initialize the row and column variable\n\ni ,j = 0, 0\nfor col in sorted(numerical_features):\n\n    # Plot a regression plot for the numerical feature and SalePrice\n    sns.regplot(col,'SalePrice',data=df,ax=ax[i][j])\n    if j == 2:\n        j=0\n        i +=1\n    else:\n        j +=1\n        \n# hide last two grids as it doesn't have any plots to show\nax[6][1].set_visible(False)\nax[6][2].set_visible(False)","0021940e":"# drop the variables that are not in correlation with sale price\ndf.drop(['BsmtFinSF2','BsmtUnfSF','EnclosedPorch'],axis=1,inplace=True)\nfor col in ['BsmtFinSF2','BsmtUnfSF','EnclosedPorch']:\n    numerical_features.remove(col)","22440d7e":"# change the type to string \ndf.MSSubClass = df.MSSubClass.astype(str)\n\n# reducing the number of categories\ndf.MSSubClass.replace({'20':'1story', '30':'1story', '40':'1story', '45':'1story', '50':'1story', \n                           '60':'2story', '70':'2story', '75':'2story', '80':'nstory',\n                           '85':'nstory', '90':'nstory', '120':'1story', '150':'1story',\n                           '160':'2story','180':'nstory','190':'nstory'}, inplace=True)\n\n# adding MSSubClass to catgeorical Feature list\ncategorical_features.append('MSSubClass')\n\n# removing it from numerical feature list\nnumerical_features.remove('MSSubClass')","68cdc436":"# define the subplots with col and row count\nfig,ax = plt.subplots(math.ceil(len(categorical_features)\/3),3,figsize=(20,60),sharey=True)\n\n# initialize the row and column number\ni ,j = 0, 0\n\n# add properties to the boxplot style\nPROPS = {\n    'boxprops':{'facecolor':'none', 'edgecolor':'black','linewidth':0.3},\n} \n\nfor col in sorted(categorical_features):\n    # plot a boxplot for SalePrice with feature\n    sns.boxplot(col,'SalePrice',data=df,ax=ax[i][j],showfliers=False,**PROPS)\n    \n    # plot a stripplot with salePrice for the fature\n    sns.stripplot(col,'SalePrice',data=df,ax=ax[i][j],alpha=0.5)\n    \n    # rotate the x-ticsk if the length is more\n    if df[col].nunique() > 8:\n        ax[i][j].tick_params(axis='x',rotation=45)\n    if j == 2:\n        j=0\n        i +=1\n    else:\n        j +=1","590857a8":"#Combine Categories that are not ordinal as ordinal catgeories \n# need to be factorized laters, hence combining nomial categorical vairables\n\n\n# df.BedroomAbvGr = df.BedroomAbvGr.astype(str)\n# df.BedroomAbvGr.replace({'0':'5','6':'5','8':'5'},inplace=True)\ndf.BldgType.replace({'2fmCon':'Twnhs','Duplex':'Twnhs'},inplace=True)\n# df.BsmtCond.replace({'No Basement':'Fa','Poor':'Fa'},inplace=True)\ndf.BsmtExposure.replace({'Mn':'Av'},inplace=True)\n# df.BsmtFinType1.replace({'ALQ':'Rec', 'BLQ':'Rec','LwQ':'Rec'},inplace=True)\n# df.BsmtFinType2.replace({'BLQ':'LwQ' , 'Rec':'LwQ' },inplace=True)\n# df.BsmtFullBath = df.BsmtFullBath.astype(str)\n# df.BsmtFullBath.replace({'3':'2'},inplace=True)\n# df.BsmtQual.replace({'No Basement' : 'Fa'},inplace=True)\ndf.Condition1.replace({'RRNn' : 'RRAn', 'PosN' : 'PosA' , 'RRNe' : 'RRAe' , 'Feedr' : 'Artery'},inplace=True)\ndf.Exterior2nd.replace({'MetalSd':'Wd Sdng', 'Wd Shng':'Wd Sdng', 'HbBoard':'Wd Sdng','Plywood':'Wd Sdng',\\\n                        'Stucco':'Wd Sdng' , 'CBlock': 'BrkFace','Other': 'BrkFace' , 'Stone': 'BrkFace',\\\n                        'AsphShn': 'BrkFace', 'ImStucc': 'BrkFace', 'Brk Cmn': 'BrkFace'},inplace=True)\n# df.FireplaceQu.replace({'Po':'No Fireplace', 'Fa':'No Fireplace'},inplace=True)\ndf.Foundation.replace({'Wood':'Stone','Slab':'Stone'},inplace=True)\n# df.FullBath = df.FullBath.astype(str)\n# df.FullBath.replace({'0':'1'},inplace=True)\ndf.GarageType.replace({'CarPort':'Detchd', 'No Garage':'Detchd', 'Basment':'Detchd' , '2Types':'Detchd'},inplace=True)\n# df.GarageQual.replace({'Ex':'NA', 'Gd':'NA' , 'Po':'NA' , 'Fa':'NA' },inplace=True)\n# df.HeatingQC.replace({'Po':'Fa'},inplace=True)\n# df.HouseStyle.replace({'2.5Fin':'2Story', '1.5Fin':'SFoyer', 'SLvl':'1Story', '1.5Unf': '2.5Unf'},inplace=True)\ndf.LotShape.replace({'IR3':'IR2'},inplace=True)\ndf.MSZoning.replace({'RH':'RM'},inplace=True)\ndf.MasVnrType.replace({'None':'BrkCmn', 'Not present':'BrkCmn'},inplace=True)\ndf.Neighborhood.replace({'BrDale':'MeadowV' , 'IDOTRR':'MeadowV' ,\\\n                         'NAmes':'Sawyer' , 'NPkVill':'Sawyer' , 'Mitchel':'Sawyer' , 'SWISU':'Sawyer', 'Blueste':'Sawyer' ,\\\n                         'Blmngtn':'Gilbert' , 'SawyerW':'Gilbert', 'NWAmes':'Gilbert',\\\n                         'ClearCr':'Crawfor' , 'CollgCr' :'Crawfor',\\\n                         'Timber':'Veenker', 'Somerst':'Veenker' ,\\\n                         'Edwards':'OldTown', 'BrkSide':'OldTown' ,\\\n                         'StoneBr' : 'NridgHt' , 'NoRidge': 'NridgHt'},inplace=True)\n# df.OverallCond = df.OverallCond.astype(str)\n# df.OverallCond.replace({'2': '3','1':'3', '6': '7', '8':'7'},inplace=True)\n# df.OverallQual = df.OverallQual.astype(str)\n# df.OverallQual.replace({'1':'2'},inplace=True)\ndf.SaleCondition.replace({'AdjLand':'Abnorml', 'Alloca':'Abnorml', 'Family' :'Abnorml'},inplace=True)\ndf.SaleType.replace({'ConLD':'COD', 'ConLI':'COD', 'CwD':'COD', 'ConLw':'COD', 'Con':'COD', 'Oth':'COD'},inplace=True)","8b506419":"# add columns to drop\ndrop_columns = ['ExterCond', 'Fence', 'LotConfig' ,'RoofStyle' ,'Exterior1st']\n\n# drop the selected features\ndf.drop(columns=drop_columns,inplace=True)\n\n# remove the dropped columns from categorical feature list\nfor cat in drop_columns[:]:\n    categorical_features.remove(cat)","de0ca982":"# list down the variables for time series\ntimeseries_features","7a36139e":"df[timeseries_features].info()","42e880ee":"# change the types to integer\ndf.YrSold = df.YrSold.astype(int)\ndf.GarageYrBlt = df.GarageYrBlt.astype(int)","72dded58":"# create a derieved column date sold by combining month sold and year sold\ndf['dateSold'] = df['MoSold'].astype(str)+'-1-'+df['YrSold'].astype(str)\ndf['dateSold'] =pd.to_datetime(df['dateSold'])\n\n# add the new column to timeseries list\ntimeseries_features.append('dateSold')","53f1c7a5":"# preview the new column\ndf['dateSold'].head()","f2f2059d":"df.loc[df.GarageYrBlt < 1900,['GarageYrBlt','YearBuilt']]","940b461c":"# define the subplot iwth number of rows and columns and the figure size accordingly\nfig,ax = plt.subplots(math.ceil(len(timeseries_features)\/2),2,figsize=(15,15),sharey=True)\n\n# initialize the row and column index\ni ,j = 0, 0\nfor col in sorted(timeseries_features):\n    if col == 'GarageYrBlt':\n        # create a line plot for GarageYrBlt with SalePrice for year\n        # greater than 1880 as there are only two data available below this year\n        sns.lineplot(df.loc[df[col] >= 1880,col],df.loc[df[col] != 0,'SalePrice'],ax=ax[i][j])\n    else:\n        # create a line plot for the time data with SalePrice\n        sns.lineplot(col,'SalePrice',data=df,ax=ax[i][j])\n    \n    # if the x-ticks are more rotate the labels\n    if df[col].nunique() > 8:\n        ax[i][j].tick_params(axis='x',rotation=45)\n    if col == \"YrSold\":\n        ax[i][j].xaxis.set_ticks([2006,2007,2008,2009,2010])\n    if j == 1:\n        j=0\n        i +=1\n    else:\n        j +=1","1fcd1c0a":"# reorder the sale price to the end for ease of visualiation in heat map\ndf_dummy = df.pop('SalePrice')\ndf.insert(df.shape[1],'SalePrice',df_dummy)\ndel df_dummy","354a31c4":"# set figure size\nplt.figure(figsize=(15,15))\n\n# plot correlation heatmap\nsns.heatmap(df.corr(),annot=True);","51b467f2":"# drop MoSold and YrSold as they are almost zero correlated\ndf.drop(['MoSold','YrSold'],axis=1,inplace=True)\nfor col in ['MoSold','YrSold']:\n    timeseries_features.remove(col)","6413c02a":"# reset these categorical numerical variables to integer \ndf[['HalfBath','Fireplaces','FullBath','BsmtFullBath','GarageCars','BedroomAbvGr','OverallCond','OverallQual']] = df[['HalfBath','Fireplaces','FullBath','BsmtFullBath','GarageCars','BedroomAbvGr','OverallCond','OverallQual']].astype(int)","85657c0e":"# assign the categorical columns that are non integer to categorical_columns as a list\ncategorical_columns =['ExterQual','BsmtQual','BsmtCond','HeatingQC','KitchenQual','FireplaceQu','GarageQual','HouseStyle','BsmtFinType2','BsmtFinType1','GarageFinish']","0d83c4c3":"# assign the labels in the order of decreasing to increasing as when creating a categorical feature\n# Converting normal Object features to Categorical Data Type features\ndf['ExterQual']=pd.Categorical(df['ExterQual'],ordered=True,categories=['Fa','TA','Gd','Ex'])\ndf['BsmtQual']=pd.Categorical(df['BsmtQual'],ordered=True,categories=['NA','Fa','TA','Gd','Ex'])\ndf['BsmtCond']=pd.Categorical(df['BsmtCond'],ordered=True,categories=['NA','Po','Fa','TA','Gd'])\ndf['HeatingQC']=pd.Categorical(df['HeatingQC'],ordered=True,categories=['Po','Fa','TA','Gd','Ex'])\ndf['KitchenQual']=pd.Categorical(df['KitchenQual'],ordered=True,categories=['Fa','TA','Gd','Ex'])\ndf['FireplaceQu']=pd.Categorical(df['FireplaceQu'],ordered=True,categories=['NA','Po','Fa','TA','Gd','Ex'])\ndf['GarageQual']=pd.Categorical(df['GarageQual'],ordered=True,categories=['NA','Po','Fa','TA','Gd','Ex'])","93f2eddf":"df['GarageFinish'] = pd.Categorical(df['GarageFinish'],ordered=True,categories=['NA','Unf','RFn','Fin'])","15f78965":"df['BsmtFinType1']=pd.Categorical(df['BsmtFinType1'],ordered=True,categories=['NA','Unf','LwQ','Rec','BLQ','ALQ','GLQ'])\ndf['BsmtFinType2']=pd.Categorical(df['BsmtFinType2'],ordered=True,categories=['NA','Unf','LwQ','Rec','BLQ','ALQ','GLQ'])","26cfcc4b":"df['HouseStyle']=pd.Categorical(df['HouseStyle'],ordered=True,categories=[ 'SFoyer','1.5Unf','1Story','1.5Fin','SLvl','2.5Unf','2Story','2.5Fin'])","5f6d1166":"# factorize the categories to Integer representation\nfor col in categorical_columns:\n    code, _ = pd.factorize(df[col],sort=True)\n    df[col] = pd.Series(code)","5ecc2637":"df.info()","aadd0bae":"# reassign the categorical features \ncategorical_features = list(df.select_dtypes(include=[np.object]).columns.values)","c66fc71a":"df.shape","eb5334d8":"# print the shape of categorical columns and the number of created dummy columns\npd.get_dummies(df[categorical_features],drop_first=True).shape,len(categorical_features)","b1802c00":"# created dummy variables for categorical features\nhouse_price = pd.concat([df,pd.get_dummies(df[categorical_features],drop_first=True)],axis=1)","7da9c6ff":"# drop the actual categorical feature from list\nhouse_price.drop(columns=categorical_features,inplace=True)","5264d032":"house_price","59fe0c55":"# reset index for the new dataframe\nhouse_price.reset_index(drop=True,inplace=True)","c47e0e8b":"# preview the dateSold feature\nhouse_price.dateSold.head()","69b03959":"#We need time HH:MM:SS to be added to date to convert it unixtimestamp\n\n# lets create a constant time\ntm = datetime.time(10,10)\n\n# convert the dateSold to unixstimestamp\nhouse_price.dateSold = house_price.dateSold.apply(lambda x: datetime.datetime.combine(x, tm).timestamp())","0b02b4c5":"house_price.dateSold.head()","0f963609":"# reassigning all the numerical features to the numerical_features variable as a list\nnumerical_features = list(df.select_dtypes(include=[np.number]).columns.values)","872cb038":"house_price.shape","0d9d1ec2":"# extract the target fesature out from dataFrame\ny = house_price.pop('SalePrice')\n# assign the independent variables to x\nX = house_price\n\n# remove the target feature from numerical features before we perform scaling\nnumerical_features.remove('SalePrice')","cb22808b":"# split the data to test and train\nX_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.7,test_size=0.3,random_state=0)","bd1e4bdf":"# create a StandardScaler object\nscaler = StandardScaler()\n\n# Fit and transform our train data with Standard Scaler\nX_train[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n\n# transform our test data with the same scaler object\nX_test[numerical_features] = scaler.transform(X_test[numerical_features])","ea6f8a82":"X_train.head()","dcac6f94":"y_train.head()","3c05b185":"# creating linar regressor\nlr = LinearRegression()\n\n# Initializing our model with the predictors and target features \nlr.fit(X_train,y_train)\n\n# Predcting values in our train data set\ny_train_pred = lr.predict(X_train)\n\nprint('\\nIntercept:', lr.intercept_)\nprint('Coefficients:', lr.coef_)\nprint('\\n\\nTrain results \\n')\nprint('Mean squared error (MSE): {:.2f}'.format(mean_squared_error(y_train, y_train_pred)))\nprint('Coefficient of Determination (R2): {:.2f}'.format(r2_score(y_train, y_train_pred)))\nprint('Residual Sum of Squares (RSS): {:.2f}'.format(np.sum(np.square(y_train - y_train_pred))))\n\n# Predicting values for our test data\ny_test_pred = lr.predict(X_test)\nprint('\\n\\nTest results \\n')\nprint('Test Mean squared error (MSE): {:.2f}'.format(mean_squared_error(y_test, y_test_pred)))\nprint('Test Coefficient of Determination (R2): {:.2f}'.format(r2_score(y_test, y_test_pred)))\nprint('Test Residual Sum of Squares (RSS): {:.2f} \\n'.format(np.sum(np.square(y_test - y_test_pred))))","b868d155":"# Lets calculate some metrics such as R2 score, RSS and RMSE\n\n# initialize a list\nmetric = []\n\n# calcuate R2 Score for Training data\nmetric.append(r2_score(y_train, y_train_pred))\n\n# calculate R2 Score for Test data\nmetric.append(r2_score(y_test, y_test_pred))\n\n# calcuate RSS for Train Data\nmetric.append(np.sum(np.square(y_train - y_train_pred)))\n\n# calcuate RSS for Test Data\nmetric.append(np.sum(np.square(y_test - y_test_pred)))\n\n# calcuate MSE for Train Data\nmetric.append(mean_squared_error(y_train, y_train_pred)**0.5)\n\n# calcuate MSE for Test Data\nmetric.append(mean_squared_error(y_test, y_test_pred)**0.5)\n\n# add number of features for the model\nmetric.append(len(X_train.columns))\n\n# add the alpha value if present\nmetric.append(0)","554a4c55":"# perform correlation on the train data set for the predictor\ncorrelation_df = pd.concat([X_train,y_train],axis=1)","ca41c256":"# Creating a method to get the top correlated features\ndef get_top_corr_features():\n    # creating a correlation list for the top features with SalesPrice\n    corr_cols = correlation_df.corr().loc[:,'SalePrice'].sort_values(ascending=False)\n    corr_cols = corr_cols.reset_index()\n    # order the data with positive corr first and negative corr last\n    corr_cols = corr_cols[corr_cols.SalePrice>0].append(corr_cols[corr_cols.SalePrice<0])\n    return corr_cols","ee0b7322":"# get teh top 30 +ve corelation appended with top 25 most +ve corr freature and lastly the reaming features\ncorr_cols = get_top_corr_features().head(30).append(get_top_corr_features().tail(25).sort_values('SalePrice',ascending=True)).append(get_top_corr_features().iloc[30:57])[1:].reset_index(drop=True)\n# rename columns\ncorr_cols.columns = ['Corr Feature','SalePrice Corr']\n# preview the result\ncorr_cols[corr_cols['Corr Feature'].isin(['2ndFlrSF',\n 'Condition1_PosA',\n 'Exterior2nd_BrkFace',\n 'Exterior2nd_CmentBd',\n 'Exterior2nd_VinylSd',\n 'LotShape_IR2',\n 'MSZoning_FV',\n 'SaleCondition_Partial',\n 'SaleType_CWD'])]","5b1512ec":"# Reordering the columns based on correlation so when performing RFE build by adding feature on by one noise features will be added only at the end\nX_train = X_train[corr_cols['Corr Feature']]\nX_test = X_test[corr_cols['Corr Feature']]","aba6db75":"# create a Linear model for RFE\nlm_rfe = LinearRegression()\n\n# fit the train and test data to linaer model\nlm_rfe.fit(X_train,y_train)\n\n# create RFE for our Linear Regressor Model and reduce to 50 features\nrfe = RFE(lm_rfe,50)\n\n# fit the train and test data to RFE\nrfe = rfe.fit(X_train,y_train)","dd2b72d9":"# print the rank of each features\npd.DataFrame(zip(X_train.columns,rfe.support_,rfe.ranking_),columns=['Feature','Selected','Rank']).sort_values('Rank')","ce6d157e":"# lets extract the top 50 selected columns by RFE\nrfe_selected_columns = X_train.columns[rfe.support_]","99f0d6d8":"# Top 50 correlated feature list\ncorr_selected_columns = get_top_corr_features().head(25).append(get_top_corr_features().tail(25).sort_values('SalePrice',ascending=True))[1:].reset_index(drop=True)\n# rename the columns\ncorr_selected_columns.columns = ['Corr_feature','SalePrice_Corr']","9fe770d7":"# Sort the DataFrame based on the column name\ncorr_selected_columns = corr_selected_columns.sort_values(by='Corr_feature').reset_index(drop=True)\n# Sort the Sereis based on the feature name\nrfe_selected_columns = pd.Series(sorted(rfe_selected_columns),name='RFE')\n# make a inner join and merge to get the common features\ncorr_rfe_features = pd.merge(left=rfe_selected_columns,right=corr_selected_columns,how='inner',\\\n                             left_on='RFE',right_on='Corr_feature').sort_values(by='SalePrice_Corr',ascending=False).reset_index(drop=True)","667e2e78":"corr_rfe_features","b4eb5e3b":"# this function can be resued to build ols model for given features\ndef build_model(X_train_rfe):\n    # adding a constant variable for intercept\n    X_train_rfe = sm.add_constant(X_train_rfe)\n\n    # Initialize an OLS model for our dataset and fit the data to model\n    lm = sm.OLS(y_train,X_train_rfe).fit()\n\n    # view the summary of the model for selected features\n    print(lm.summary())\n\n    return lm","c4c93bb0":"def VIF(X_train_rfe):\n    # create a dummy dataframe\n    vif = pd.DataFrame(columns=['Features','VIF'])\n    \n    # extract the column values to vif features column value\n    vif['Features'] = X_train_rfe.columns\n\n    # calculate vif for the train data for the added features\n    vif['VIF'] = [variance_inflation_factor(X_train_rfe.values, i) for i in range(X_train_rfe.shape[1])]\n    \n    # round the value to 2 decimals\n    vif['VIF'] = round(vif['VIF'], 2)\n    \n    # sort values by hightevif value first\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    \n    # print vif table\n    display(vif)\n    \n    # retrun vif object\n    return vif","b4b3c2f2":"def perform_feature_selection(train_data,rfe=False,corr=False):\n    # create a empty data frame for xtrain and vif\n    X_train_rfe = pd.DataFrame()\n    vif = pd.DataFrame()\n\n    # creating this object to ignore vif for a single feature\n    count = 1\n\n    # created this varible to stop the outer loop of adding futher features for model\n    stop = False\n\n    # prev r2score\n    r2score = 0.0\n\n    # fetch the features based on corr\/rfe selection\n    if rfe:\n        cols = rfe_selected_columns\n    elif corr:\n        cols = corr_selected_columns.Corr_feature\n    else: \n        cols = corr_rfe_features.RFE.values\n        \n    for col in cols:\n    #for v in rfe_selected_columns:\n\n        # add the column to the traing data set\n        if col in train_data.columns.values:\n            X_train_rfe[col] = train_data[col]\n\n            # rebuild the model again to ckeck for high vifs and p-values \n            # once a feature is dropped on the above conditions after adding \n            # the new feature from the previous step to the model\n            while True:\n                # build the model\n                lm = build_model(X_train_rfe)\n\n                # Drop the previous column if r2score doesn't increase\n                if round(r2score,3) == round(lm.rsquared,3):\n\n                    print(\"\\n\\n Dropping \"+X_train_rfe.columns.values[-1]+\" and rebuilding the model as it did not add any info to model \\n\\n\")\n\n                    X_train_rfe.drop(X_train_rfe.columns.values[-1],axis=1, inplace=True)\n                    \n                    # build the model again as we have removed a feature \n                    lm = build_model(X_train_rfe)\n\n                # Assign new r2score to check for the next build on adding new feature\n                r2score = lm.rsquared\n\n                # ignore vif and p-value check since there will\n                # be only 1 column on first iteration\n                if count != 1:\n\n                    # calculate VIF\n                    vif = VIF(X_train_rfe)\n\n                    # if the model reaches required r2 score stop the model from executing furher steps\n                    if lm.rsquared >= 0.90:\n                        stop = True\n                        break\n\n                    # Check if the p-value if high\n                    if (lm.pvalues > 0.05).sum() > 0:\n\n                        # extract feature fo high p-value\n                        feature = lm.pvalues[lm.pvalues > 0.05].index\n\n                        # check if this feature is not const\n                        if feature[0] != 'const':\n\n                            # if the VIF value is aslo high drop this columns first\n                            if feature[0] in vif.loc[vif.VIF > 5,'Features']:\n                                X_train_rfe.drop(feature[0],axis=1,inplace=True)                # order 1\n                            else:\n                                # if only the p-value is high drop it\n                                X_train_rfe.drop(feature[0],axis=1,inplace=True)                # order 2\n\n                        # if the p-value column is 2nd in the list extract \n                        # that feature name to drop if from dataset if there is \n                        # a third value with high p-value it will be\n                        # validated in the next loop after rebuild on dropping the current feature\n                        elif (feature[0] == 'const') & (len(feature) > 1):\n                            X_train_rfe.drop(feature[1],axis=1,inplace=True)                    # order 2\n\n                    # if VIF value is high drop it\n                    if ((vif.VIF > 5).sum() > 0) & (col in X_train_rfe.columns.values):\n                        X_train_rfe.drop(col,axis=1,inplace=True)   # order 3\n                    else:\n                        break                                                                   # order 4\n                else:\n                    break\n            # stop the process\n            if stop:\n                break\n\n            # increment count on adding new feature\n            count = count + 1\n            \n    return X_train_rfe","dec267a8":"X_train_rfe = perform_feature_selection(X_train,corr=True)","1a19d1d5":"# filter our X_Test with the rfe selected features\nX_test_rfe = X_test[X_train_rfe.columns.values]\nX_test_rfe.shape,y_test.shape","9bbb3f94":"# Lets calculate some metric11s such as R2 score, RSS and RMSE\n\nlr_rfe = LinearRegression()\n\nlr_rfe.fit(X_train_rfe,y_train)\n\ny_train_pred = lr_rfe.predict(X_train_rfe)\ny_test_pred = lr_rfe.predict(X_test_rfe)\n\n# initialize a list\nmetric1 = []\n\n# calcuate R2 Score for Training data\nmetric1.append(r2_score(y_train, y_train_pred))\n\n# calculate R2 Score for Test data\nmetric1.append(r2_score(y_test, y_test_pred))\n\n# calcuate RSS for Train Data\nmetric1.append(np.sum(np.square(y_train - y_train_pred)))\n\n# calcuate RSS for Test Data\nmetric1.append(np.sum(np.square(y_test - y_test_pred)))\n\n# calcuate MSE for Train Data\nmetric1.append(mean_squared_error(y_train, y_train_pred)**0.5)\n\n# calcuate MSE for Test Data\nmetric1.append(mean_squared_error(y_test, y_test_pred)**0.5)\n\n# add number of features for the model\nmetric1.append(len(X_train_rfe.columns))\n\n# add the alpha value if present\nmetric1.append(0)","91c045e3":"# Residual analysis\ny_test_res = y_test - y_test_pred","472312ae":"# calculate the residual and plot the results\nres = y_test_res\nplt.scatter( y_test_pred , res)\nplt.axhline(y=0, color='r', linestyle=':')\nplt.xlabel(\"Predictions\")\nplt.ylabel(\"Residual\")\nplt.show()","8e4e0b62":"# Distribution of errors\np = sns.distplot(y_test_res,kde=True)\n\np = plt.title('Normality of error terms\/residuals')\nplt.xlabel(\"Residuals\")\nplt.show()","809c83a0":"# list of alphas to tune - if value too high it will lead to underfitting, if it is too low, \n# it will not handle the overfitting\nparams = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, 3.5,\n 4.0, 4.5, 5.0, 6.0, 7.0, 8.0, 9.0, 10, 15, 20, 25, 50, 100, 500, 1000 ]}","c5871f3c":"ridge = Ridge()\n\n# cross validation\nfolds = 5\n#\nridge_cv = GridSearchCV(estimator = ridge, \n                        scoring= 'neg_mean_absolute_error',  \n                        param_grid = params, \n                        return_train_score=True,\n                        cv = folds, \n                        verbose = 1)            \nridge_cv.fit(X_train, y_train)","4f8ad20d":"# fetch the results of our gridsearch\nridge_results = pd.DataFrame(ridge_cv.cv_results_)\nridge_results","2ad682e5":"# plot a graph for comparing the test and train performance on different alpha params\nridge_results['param_alpha'] = ridge_results['param_alpha'].astype('float32')\n\n# plotting\nplt.figure(figsize=(16,5))\n\n# plot train result\nplt.plot(ridge_results['param_alpha'], ridge_results['mean_train_score'])\n\n# plot test results\nplt.plot(ridge_results['param_alpha'], ridge_results['mean_test_score'])\n\n# set labels and title \nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\nplt.xscale('log')\nplt.title(\"Negative Mean Absolute Error and alpha\")\n\n# plot legend\nplt.legend(['train score', 'test score'], loc='upper right')\n\n# show graph\nplt.show()","60438ca8":"print(ridge_cv.best_params_)","e91cc71f":"#Fitting Ridge model for alpha = 15\nalpha = ridge_cv.best_params_['alpha']\n\n# initialise Ridge Model\nridge = Ridge(alpha=alpha)\n\n# Fit the data\nridge.fit(X_train, y_train)\n\n# printing coefficients which have been penalised\nprint(ridge.coef_)","6eb7715e":"# Lets calculate some metric21s such as R2 score, RSS and RMSE\n\ny_train_pred = ridge.predict(X_train)\ny_test_pred = ridge.predict(X_test)\n\n# initialize a list\nmetric2 = []\n\n# calcuate R2 Score for Training data\nmetric2.append(r2_score(y_train, y_train_pred))\n\n# calculate R2 Score for Test data\nmetric2.append(r2_score(y_test, y_test_pred))\n\n# calcuate RSS for Train Data\nmetric2.append(np.sum(np.square(y_train - y_train_pred)))\n\n# calcuate RSS for Test Data\nmetric2.append(np.sum(np.square(y_test - y_test_pred)))\n\n# calcuate MSE for Train Data\nmetric2.append(mean_squared_error(y_train, y_train_pred)**0.5)\n\n# calcuate MSE for Test Data\nmetric2.append(mean_squared_error(y_test, y_test_pred)**0.5)\n\n# add number of features for the model\nmetric2.append(len(X_train.columns))\n\n# add the alpha value if present\nmetric2.append(ridge_cv.best_params_['alpha'])","63da6d3f":"# fit the RFE data to gridcv for ridge\nridge_cv.fit(X_train_rfe, y_train)","66c2cd80":"print(ridge_cv.best_params_)","7be090d5":"#Fitting Ridge model for alpha = 5\nalpha = ridge_cv.best_params_['alpha']\n\n# Initialze Ridhe Model\nridge_rfe = Ridge(alpha=alpha)\n\n# fit rfe data to RIdge model\nridge_rfe.fit(X_train_rfe, y_train)\n\n# printing coefficients which have been penalised\nprint(ridge_rfe.coef_)","6ace31ec":"# Lets calculate some metric31s such as R2 score, RSS and RMSE\n\ny_train_pred = ridge_rfe.predict(X_train_rfe)\ny_test_pred = ridge_rfe.predict(X_test_rfe)\n\n# initialize a list\nmetric3 = []\n\n# calcuate R2 Score for Training data\nmetric3.append(r2_score(y_train, y_train_pred))\n\n# calculate R2 Score for Test data\nmetric3.append(r2_score(y_test, y_test_pred))\n\n# calcuate RSS for Train Data\nmetric3.append(np.sum(np.square(y_train - y_train_pred)))\n\n# calcuate RSS for Test Data\nmetric3.append(np.sum(np.square(y_test - y_test_pred)))\n\n# calcuate MSE for Train Data\nmetric3.append(mean_squared_error(y_train, y_train_pred)**0.5)\n\n# calcuate MSE for Test Data\nmetric3.append(mean_squared_error(y_test, y_test_pred)**0.5)\n\n# add number of features for the model\nmetric3.append(len(X_train_rfe.columns))\n\n# add the alpha value if present\nmetric3.append(ridge_cv.best_params_['alpha'])","1c5df470":"metric3","ab0ba80b":"# create lasso regression instance\nlasso = Lasso()\n\nparams = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, 10.0, 20, 50, 75, 100, 200, 250, 300, 500, 1000 ]}\n\n\n# cross validation\nlasso_cv = GridSearchCV(estimator = lasso, \n                        return_train_score=True,\n                        cv = folds, \n                        param_grid = params, \n                        verbose = 1,\n                        scoring= 'neg_mean_absolute_error')            \n\n# fit the train and test data\nlasso_cv.fit(X_train, y_train) ","49ee66c3":"# featch Lasso results\nlasso_results = pd.DataFrame(lasso_cv.cv_results_)\nlasso_results","d9e8a7e5":"# plot a graph for comparing the test and train performance on different alpha params\nlasso_results['param_alpha'] = lasso_results['param_alpha'].astype('float32')\n\n# plotting\nplt.figure(figsize=(16,5))\n\n# plot train result\nplt.plot(lasso_results['param_alpha'], lasso_results['mean_train_score'])\n\n# plot test results\nplt.plot(lasso_results['param_alpha'], lasso_results['mean_test_score'])\n\n# set labels and title \nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\nplt.xscale('log')\nplt.title(\"Negative Mean Absolute Error and alpha\")\n\n# plot legend\nplt.legend(['train score', 'test score'], loc='upper right')\n\n# show graph\nplt.show()","19b2db94":"# Printing the best hyperparameter alpha\nprint(lasso_cv.best_params_)","b55b8aa3":"#Fitting Ridge model for alpha = 200\nalpha = lasso_cv.best_params_['alpha']\n\n# build Lasso model\nlasso = Lasso(alpha=alpha)\n        \n# fit Train and test data\nlasso.fit(X_train, y_train)","a02375d6":"# Lets calculate some metric41s such as R2 score, RSS and RMSE\n\ny_train_pred = lasso.predict(X_train)\ny_test_pred = lasso.predict(X_test)\n\n# initialize a list\nmetric4 = []\n\n# calcuate R2 Score for Training data\nmetric4.append(r2_score(y_train, y_train_pred))\n\n# calculate R2 Score for Test data\nmetric4.append(r2_score(y_test, y_test_pred))\n\n# calcuate RSS for Train Data\nmetric4.append(np.sum(np.square(y_train - y_train_pred)))\n\n# calcuate RSS for Test Data\nmetric4.append(np.sum(np.square(y_test - y_test_pred)))\n\n# calcuate MSE for Train Data\nmetric4.append(mean_squared_error(y_train, y_train_pred)**0.5)\n\n# calcuate MSE for Test Data\nmetric4.append(mean_squared_error(y_test, y_test_pred)**0.5)\n\n# add number of features for the model\nfeature_cnt = len(lasso.coef_)-pd.Series(lasso.coef_).apply(lambda x: x == 0).sum()\nmetric4.append(feature_cnt)\n\n# add the alpha value if present\nmetric4.append(lasso_cv.best_params_['alpha'])","25fe51cb":"# fit the Training data from RFE selection\nlasso_cv.fit(X_train_rfe, y_train) ","1aedec1a":"# Printing the best hyperparameter alpha\nprint(lasso_cv.best_params_)","5b9a57a5":"#Fitting Ridge model for alpha = 200 and printing coefficients which have been penalised\nalpha = lasso_cv.best_params_['alpha']\n\n# build lasso for 200 alpha\nlasso_rfe = Lasso(alpha=alpha)\n\n# fit the rfe train data and\nlasso_rfe.fit(X_train_rfe,y_train)","0bfdf6df":"# Lets calculate some metrics such as R2 score, RSS and RMSE\n\ny_train_pred = lasso_rfe.predict(X_train_rfe)\ny_test_pred = lasso_rfe.predict(X_test_rfe)\n\n# initialize a list\nmetric5 = []\n\n# calcuate R2 Score for Training data\nmetric5.append(r2_score(y_train, y_train_pred))\n\n# calculate R2 Score for Test data\nmetric5.append(r2_score(y_test, y_test_pred))\n\n# calcuate RSS for Train Data\nmetric5.append(np.sum(np.square(y_train - y_train_pred)))\n\n# calcuate RSS for Test Data\nmetric5.append(np.sum(np.square(y_test - y_test_pred)))\n\n# calcuate MSE for Train Data\nmetric5.append(mean_squared_error(y_train, y_train_pred)**0.5)\n\n# calcuate MSE for Test Data\nmetric5.append(mean_squared_error(y_test, y_test_pred)**0.5)\n\n# add number of features for the model\nfeature_cnt = len(lasso_rfe.coef_)-pd.Series(lasso_rfe.coef_).apply(lambda x: x == 0).sum()\nmetric5.append(feature_cnt)\n\n# add the alpha value if present\nmetric5.append(lasso_cv.best_params_['alpha'])","5b70a932":"betas = pd.DataFrame(index=X_train.columns.values, \n                     columns = ['Linear', 'Linear RFE','Ridge','Ridge RFE', 'Lasso', 'Lasso RFE'])\n\nbetas.loc[X_train.columns,'Linear'] = lr.coef_ # Polynomial Regression\nbetas.loc[X_train_rfe.columns,'Linear RFE'] = lr_rfe.coef_ # Polynomial Regression\nbetas.loc[X_train.columns,'Ridge'] = ridge.coef_ # Ridge Regression\nbetas.loc[X_train_rfe.columns,'Ridge RFE'] = ridge_rfe.coef_ # Ridge Regression\nbetas.loc[X_train.columns,'Lasso'] = lasso.coef_ # Lasso Regression\nbetas.loc[X_train_rfe.columns,'Lasso RFE'] = lasso_rfe.coef_ # Lasso Regression\n\nprint(\"\\n\\n   ======== Coefficients for features under each model built ========\")\n\nbetas['Total Coeff'] = abs(betas).sum(axis=1)\n\nbetas.sort_values(by='Total Coeff',ascending=False)","377cb749":"# Creating a table which contain all the metrics\n\nlr_table = {'Metric': ['R2 Score (Train)','R2 Score (Test)','RSS (Train)','RSS (Test)',\n                       'MSE (Train)','MSE (Test)','No of Features','Alpha'], \n        'Linear Regression': metric\n        }\n\nlr_metric = pd.DataFrame(lr_table ,columns = ['Metric', 'Linear Regression'] )\n\nlr_metric_rfe = pd.Series(metric1, name = 'Linear Regression RFE')\n\nrg_metric = pd.Series(metric2, name = 'Ridge Regression')\nrg_rfe_metric = pd.Series(metric3, name = 'Ridge Regression RFE')\nls_metric = pd.Series(metric4, name = 'Lasso Regression')\nls_rfe_metric = pd.Series(metric5, name = 'Lasso Regression RFE')\n\nfinal_metric = pd.concat([lr_metric,lr_metric_rfe, rg_metric,rg_rfe_metric, ls_metric,ls_rfe_metric], axis = 1)\n\nprint(\"\\n\\n  \\t\\t\\t\\t  ============= Overall view of the models ================\")\n\nround(final_metric,4).astype(str)","fa9a179b":"print(\"\\n\\n \\t   =============== Final models ================\")\n\nround(final_metric[['Metric','Linear Regression RFE','Ridge Regression RFE','Lasso Regression RFE']],4).astype(str)","fb75a3cd":"<center><h1>US House Pricing Prediction<\/h1><\/center>\n\n<h3>Introduction<\/h3>\n\n\nA US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to purchase houses at a price below their actual values and flip them on at a higher price. For the same purpose, the company has collected a data set from the sale of houses in Australia. The data is provided in the CSV file below.\n\n<h3>Business Problem<\/h3>\n\nThe company is looking at prospective properties to buy to enter the market. You are required to build a regression model using regularisation in order to predict the actual value of the prospective properties and decide whether to invest in them or not.\n\n<br>\n\n**The company wants to know:**\n\nWhich variables are significant in predicting the price of a house, and\n\nHow well those variables describe the price of a house.\n\nAlso, determine the optimal value of lambda for ridge and lasso regression.\n\n <br>\n\n**Business Goal**\n\n\nYou are required to model the price of houses with the available independent variables. This model will then be used by the management to understand how exactly the prices vary with the variables. They can accordingly manipulate the strategy of the firm and concentrate on areas that will yield high returns. Further, the model will be a good way for management to understand the pricing dynamics of a new market.\n","c351fd34":"<h2>Creating Derived Features<\/h2>","fd798608":"<h1> Conclusion <\/h1>","5f5249ca":"Top Five features on overall coeff score based on the six models are :\n<b>\n<ul>\n<ol>TotalFlrSFAbvGrd<\/ol>\n<ol>Neighborhood_NridgHt<\/ol>\n<ol>Neighborhood_Sawyer<\/ol>\n<ol>GrLivArea<\/ol>\n<ol>BldgType_Twnhs<\/ol>\n<\/ul>\n<\/b>","ce7a1879":"The dataset contains 81 features and 1460 records.","c9b1e2dc":"<br>\n<h2> Dummy Variable Creation <h2>","c7600aa5":"Lets create dummy variables for the remaing categorical features.\n\nAs the labels in these categorical variable are shrinked to lower number the number of features generated will be less","b71e88c8":"**Combine Categories:**\n\n    BedroomAbvGr : Combine 0 , 5 , 6 and 8\n    \n    BldgType : Combine 2fmCon ,Twnhs and Duplex  \n    \n    BsmtCond : Combine No Basement, Fa and Poor\n    \n    BsmtExposure : Combine Mn and Av\n    \n    BsmtFinType1 : Combine ALQ, Rec, BLQ and LwQ\n    \n    BsmtFinType2 : Combine BLQ , Rec and LwQ\n    \n    BsmtFullBath : Combine 2 and 3\n    \n    BsmtQual : Combine No Basement and Fa\n    \n    Condition1 : Combine RRNn and RRAn, PosN and PosA , RRNe and RRAe and Feedr and Artery\n    \n    Exterior2nd : Combine MetalSd, Wd Shng, HbBoard, Plywood, Wd Sdng , Stucco and combine CBlock, Other , Stone, AsphShn, ImStucc, Brk Cmn, BrkFace\n    \n    FireplaceQu : Combine No Fireplace, Po and Fa\n    \n    Foundation: Combine Wood, Slab and Stone\n    \n    FullBath : Combine 0 and 1\n    \n    GarageType: Combine Detchd, CarPort, No Garage, Basment and 2Types.\n    \n    GarageQual : Combine Ex and Gd , Po , Fa and No Garage\n    \n    HeatingQC : Combine Fa and Po\n    \n    House Style : Combine 2Story and 2.5Fin, SFoyer and 1.5Fin, SLvl and 1Story, 1.5Unf and 2.5Unf\n    \n    LotShape: Combine IR2 and IR3\n    \n    MSZoning : Combine RM and RH to other\n    \n    MasVnrType: Combine None, Not present and BrkCmn\n    \n    Neighborhood : combine MeadowV , BrDale and IDOTRR , Sawyer , NAmes , NPkVill , Mitchel , SWISU and Blueste , Gilbert , Blmngtn , SawyerW and NWAmes, ClearCr , CollgCr and Crawfor, Veenker, Timber and Somerst , OldTown , Edwards and BrkSide , StoneBr , NridgHt and NoRidge.\n    \n    OverallCond : 1, 2 and 3 , 6, 7, and 8\n    \n    OverallQual : 1 and 2\n    \n    SaleCondition: Combine AdjLand, Alloca, Family and Abnorml\n    \n    SaleType: Combine COD, ConLD, ConLI, CwD, ConLw, Con and Oth.\n    ","11c0621e":"<H2> Feature Selection<\/H2>\n<h3> Analysis the Correlated Features <\/h3>","351e21b1":"dropping the categorical features that are less correlated .ie has same mean across all its labels","1d5608ec":"Lets calculate and store the outputs in a metric variable","bf35d3b4":"<h2>Observation<\/h2>","cd31726c":"Reducing the number of features to 50 using RFE","ebeb4029":"<h2> Ridge Regression <\/h2>","f6c22255":"There are no duplicated rows in the given data. Lets proceed with some further analyasis","bcf442ff":"The missing Percentage here means that the house doesn't have that specific feature like Gargae has 5% of all its features missing then it means that house doesn't have garage and hence 'GarageType','GarageFinish','GarageQual','GarageCond' are all have same percenntage of misiing value. These can hence be filled with \"NA\" because using mode will give a different meaning and won't be the right approch to fix this. SO lets proceed by will these kind of missingvalues with NA ","b290fe11":"<h2> Feature Scaling<\/h2>","92da4b6d":"We have one more data to clean up that is our date object. For model to use dat object we need to convert it to integer. Thinking about it we have a way to conver ti to integer... Ofcourse thats unixtimestamp","67c043dd":"Observation:\n    \n    Ridge has done a good coefficient balance with 15 as alpha value for regularizing. But since the number of features is 81 it will make the mode complex and overfit so let try building Ridge model with the RFE train data","916ae621":"**Highly Correlated Features:**\n    \n    Fireplaces, GarageCars, HeatingQC, KitchenQual\n    ","b295e720":"<h1> Data Cleaning<h1>","53bdacd0":"Lets starts datacleaning by droping the ID column first as it has no value to our model","2f712e6e":"Lets build lasso with our rfe data","2d8e0d8f":"We can see that many fetatures are filled with same values in 90% of its data","4a66a68b":"Outlier and Correlation:\n\n    1stFlrSF : It has an outlier with sqft greater than 4000 at lower price whihc is not possible to sell\n    \n    2ndFlrSF : is not well correlated with salePrice\n    \n    BsmtFinSF1 : The Sqft is greater than 5000 with lower price which is not possible\n    \n    BsmtFinSF2 : A outlier with sqft almost 1500 is rated at lower price which doesn't look right and is not correlated with salePrice\n    \n    BsmtUnfSF : The feature doesn't influence the Saleprice much\n    \n    EnclosedPorch : AN outlier with sqft above 500 is rated at lower price.\n    \n    GarageArea : There are few outliers above 1200 sqft in realtion to price\n    \n    GrLivArea : There are few outliers at lower price and extreme sqft\n    \n    LotArea : Few outliers are present above 100000 sqft\n    \n    LotFrontage : Two outliers are at the exterme of above 300 linear feet\n    \n    MasVnrArea : There is one outlier above 1500 sqft\n    \n    MSSubClass : This feature doesn't provide any information about salePrice\n    \n    OpenPorchSF : Few outliers are present above 400 sqft\n    \n    TotalBsmtSF : An outlier is present with sqft of about 6000\n","b36ee727":"<h1> Data Preparation <\/h1>\n    \n<h3> Our data has been cleaned and now tuned with 81 independent variables and 1 Target variable<\/h3>\n\nSplit the independent features and target feature to x and y respectively","cf696f08":"<h2>Lasso Regression<\/h2>","488f8712":"Lets Analyse Garage features to determine how GarageYrBlt can be filled","89b73294":"<h3>Observation:<\/h3>\n\n    Our model output has higher R2 score in train data and lower R2 score in test data with a huge difference. Hence it clearly **overfits**.\n    \n    There are several ways to overcome this problem:\n        - reduce the number of features to has the model simple(less complex) using RFE\n        - Ridge regression can reduce the coeff values a lot and use the hyper parameter(alpha) to tune the model\n        - Use Lasso regression to reduce the improve the model best fit by reducing the coeff to zero to reject the features, this helps in selceting the features for out model with teh use of hyper parameter","bd6d2acb":"<br>\n<h2> Analysis on Categorical Variables<\/h2>","7a6862fc":"Lets check the Year build column before we proceed as nul values in Yearbuild will have no effect on our approach","8dda32ed":"<h2>Observation<\/h2>","17aeb635":"lets take train data from RFE and Perform Ridge Regression for as a less complex model","77606e03":"Lets check the Regression plot on numerical variables again to see if it look better ","28b831ae":"<h3>OLS Model<\/h3>","43791665":"<br>\n<h2>Analysing Numerical Variables<\/h2>","ff049d7b":"**The final models have the train and test R2 Score with a very minimal difference of 0.001 R2 score between the models for train data and 0.002 incease in R2 score for successive models**\n\nLasso looks better than the other models, So we can conclude that Lasso performs well in these regressions","8053aac6":"<h2>Linear Regression<\/h2>","d754adde":"<br>\n<h2> Encoding Category Labels <h2>","48227fca":"Observation:\n\nAfter fixing the outliers we can see the correlation better now. Lets determine which features are less correlated and drop them\n\n    BsmtFinSF2 : It has a low correlation and hence can be dropped\n    \n    BsmtUnfSF : It depics less correlation towards SalePrice and hence can be dropped\n    \n    EnclosedPorch : Has a low and bit of negative correlation and doesn't provide much information\n    \n    MSSubClass : This looks like it belongs to Category variable let reassign the variable to categorical feature list\n    \n    ","ea5abce0":"We have two way to build a model to find the best features selected by RFE that would fit.\n<ul><li>Dropping a feature one by one from the model built with 50 features until it shows good performane without overfitting<\/li><li>Adding a feature one by one to the model until it shows a good performance metrics<\/li><\/ul>\n\nLets add features one by one and build our model\n\nLets create a custom logic based on the below condition\n\n<table>\n<tr><th style=\"text-align:center\">Order<\/th><th style=\"text-align:center\">P-value<\/th><th style=\"text-align:center\">VIF<\/th><th style=\"text-align:center\">Action<\/th><\/tr>\n<tr><td>1<\/td><td>High<\/td><td>High<\/td><td>Drop these columns First<\/td><\/tr>\n<tr><td>2<\/td><td>High<\/td><td>Low<\/td><td>Drop these columns one by one, because this could lower the VIF values of other columns to prevent it from being dropped in next step <\/td><\/tr>\n<tr><td>3<\/td><td>Low<\/td><td>High<\/td><td>Drop the colums with VIF greater than 5<\/td><\/tr>\n<tr><td>4<\/td><td>Low<\/td><td>Low<\/td><td>Keep these features<\/td><\/tr>\n<\/table>\t","bb171604":"RFE gives a rank with number , the number that are greater than one can be dropped. The selected columns shows true to retain and false to reject.","a61cfc50":"From inspecting the Garage data there are few homes that doesn't have agarage and hence the data are null, for categorical variables we can fill it with NA and for numerical variables if the data is based on count\/measurement we can fill it with 0. For Garage Year we can't fill it with 0 but we can check if the house build year and replace it with the same. This is just my assumption as it will have some significatent corr between the columns but less on target column compartively.","31c9e2f3":"Lets split the data to train and test data with sklearn liberary.\n\nLets defien the test size as 30% and train data size as 70%","8278495b":"<br>\n<h2> Time Series Analysis<\/h2>","75dbfd9b":"Since MSSubClass has too many categories less reduce them to grouped label categories","224d3861":"Even after fixing the exterme outliers we still have some outliers that are not in extreme but or incorrect values like for larger squarefeet the salePrice is really ow which is not normal.\n\nLets havea  look at those missleading values","c3496aa2":"**Observation:**\n    \n    Lasso did a great analysis with the advantage oof feature selection which took 53 features outof 81 almost 30 features got to zero coefficient.","3379fa29":"Lets fix the outliers by getting the mean value of the feature at that SalePrice range","ad12e27c":"Lets fill Masonry veneer Area with 0 for all the Na values and Masonry veneer Type with Not Present ","aad5fc37":"<h2>Data Definition<\/h2>\n\n<ul><ol>MSSubClass: Identifies the type of dwelling involved in the sale.\t<\/ol>\n<ol>MSZoning: Identifies the general zoning classification of the sale.<\/ol>\n<ol>LotFrontage: Linear feet of street connected to property<\/ol>\n<ol>LotArea: Lot size in square feet<\/ol>\n<ol>Street: Type of road access to property<\/ol>\n<ol>Alley: Type of alley access to property<\/ol>\n<ol>LotShape: General shape of property<\/ol>\n<ol>LandContour: Flatness of the property<\/ol>\n<ol>Utilities: Type of utilities available<\/ol>\n<ol>LotConfig: Lot configuration<\/ol>\n<ol>LandSlope: Slope of property<\/ol>\n<ol>Neighborhood: Physical locations within Ames city limits<\/ol>\n<ol>Condition1: Proximity to various conditions<\/ol>\n<ol>Condition2: Proximity to various conditions (if more than one is present)<\/ol>\n<ol>BldgType: Type of dwelling<\/ol>\n<ol>HouseStyle: Style of dwelling<\/ol>\n<ol>OverallQual: Rates the overall material and finish of the house<\/ol>\n<ol>OverallCond: Rates the overall condition of the house<\/ol>\n<ol>YearBuilt: Original construction date<\/ol>\n<ol>YearRemodAdd: Remodel date (same as construction date if no remodeling or additions)<\/ol>\n<ol>RoofStyle: Type of roof<\/ol>\n<ol>RoofMatl: Roof material<\/ol>\n<ol>Exterior1st: Exterior covering on house<\/ol>\n<ol>Exterior2nd: Exterior covering on house (if more than one material)<\/ol>\n<ol>MasVnrType: Masonry veneer type<\/ol>\n<ol>MasVnrArea: Masonry veneer area in square feet<\/ol>\n<ol>ExterQual: Evaluates the quality of the material on the exterior <\/ol>\n<ol>ExterCond: Evaluates the present condition of the material on the exterior<\/ol>\n<ol>Foundation: Type of foundation<\/ol>\n<ol>BsmtQual: Evaluates the height of the basement<\/ol>\n<ol>BsmtCond: Evaluates the general condition of the basement<\/ol>\n<ol>BsmtExposure: Refers to walkout or garden level walls<\/ol>\n<ol>BsmtFinType1: Rating of basement finished area<\/ol>\n<ol>BsmtFinSF1: Type 1 finished square feet<\/ol>\n<ol>BsmtFinType2: Rating of basement finished area (if multiple types)<\/ol>\n<ol>BsmtFinSF2: Type 2 finished square feet<\/ol>\n<ol>BsmtUnfSF: Unfinished square feet of basement area<\/ol>\n<ol>TotalBsmtSF: Total square feet of basement area<\/ol>\n<ol>Heating: Type of heating<\/ol>\n<ol>HeatingQC: Heating quality and condition<\/ol>\n<ol>CentralAir: Central air conditioning<\/ol>\n<ol>Electrical: Electrical system<\/ol>\n<ol>1stFlrSF: First Floor square feet<\/ol>\n<ol>2ndFlrSF: Second floor square feet<\/ol>\n<ol>LowQualFinSF: Low quality finished square feet (all floors)<\/ol>\n<ol>GrLivArea: Above grade (ground) living area square feet<\/ol>\n<ol>BsmtFullBath: Basement full bathrooms<\/ol>\n<ol>BsmtHalfBath: Basement half bathrooms<\/ol>\n<ol>FullBath: Full bathrooms above grade<\/ol>\n<ol>HalfBath: Half baths above grade<\/ol>\n<ol>Bedroom: Bedrooms above grade (does NOT include basement bedrooms)<\/ol>\n<ol>Kitchen: Kitchens above grade<\/ol>\n<ol>KitchenQual: Kitchen quality<\/ol>\n<ol>TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)<\/ol>\n<ol>Functional: Home functionality (Assume typical unless deductions are warranted)<\/ol>\n<ol>Fireplaces: Number of fireplaces<\/ol>\n<ol>FireplaceQu: Fireplace quality<\/ol>\n<ol>GarageType: Garage location<\/ol>\n<ol>GarageYrBlt: Year garage was built<\/ol>\n<ol>GarageFinish: Interior finish of the garage<\/ol>\n<ol>GarageCars: Size of garage in car capacity<\/ol>\n<ol>GarageArea: Size of garage in square feet<\/ol>\n<ol>GarageQual: Garage quality<\/ol>\n<ol>GarageCond: Garage condition<\/ol>\n<ol>PavedDrive: Paved driveway<\/ol>\n<ol>WoodDeckSF: Wood deck area in square feet<\/ol>\n<ol>OpenPorchSF: Open porch area in square feet<\/ol>\n<ol>EnclosedPorch: Enclosed porch area in square feet<\/ol>\n<ol>3SsnPorch: Three season porch area in square feet<\/ol>\n<ol>ScreenPorch: Screen porch area in square feet<\/ol>\n<ol>PoolArea: Pool area in square feet<\/ol>\n<ol>PoolQC: Pool quality<\/ol>\n<ol>Fence: Fence quality<\/ol>\n<ol>MiscFeature: Miscellaneous feature not covered in other categories<\/ol>\n<ol>MiscVal: $Value of miscellaneous feature<\/ol>\n<ol>MoSold: Month Sold (MM)<\/ol>\n<ol>YrSold: Year Sold (YYYY)<\/ol>\n<ol>SaleType: Type of sale<\/ol>\n<ol>SaleCondition: Condition of sale<\/ol><\/ul>","90ad5503":"Lets first build a Linear Regression model with the number of features we have to check if it overfits","d46cec3a":"Converting the Object category featurs to Categorical data type for encoding","1e960ce2":"<h3> Handling DATE Object <\/h3>","56165665":"We have brounght down to 53 columns finally but we still have to convert few categories to dummy variable creation","d9f17f33":"Before we use RFE lets analyse the top correlated features","f148b77a":"<h2> Model Summary <\/h2><br>","5b28c902":"<h3> Handling Nominal Categories <\/h2>","7518d81b":"And same here in BsmtFinSF1 it has one more outlier with misleading value","e7cff5a3":"LotFrontage has some null values which should be handled. As from my analysis we can groupby neighbourhod and LotCOnfig and view the median value of the them grouped together as Lotfrontage on each neighbourhood and LotFrontage for LotConfig are similar.","71678e24":"There are still some null values in LotFrontage. Okay now lets take just LotConfig and group it up and fill in the na with  median of LotFrontage as LotConfig is more relevant to LotFrontage.","923d093d":"To make a good prediction we need the categrical features to have a balanced or a fair ratio of labels. If a same value applies for 90% of the data set then that feature can't explain the target variable much and would product incorrect results when used in our model. So lets find the featurs which has most repetitive value.","17c02569":"<h2>Linear Regression With RFE<\/h2>","0c8b70b6":"<h2>RFE<\/h2>","82a80221":"To do:\n    \n    We can see that the least used features where the **Exterior2nd** column which doesn't impact SalePrice much.. we can try rebuilding the model by removing it","853cc5cc":"From the data we can observe that we have TotalBsmtSF and TotRmsAbvGrd which sums up the subs categories or values of the related data. Similarly we can get calculate a TotalSF for the floors and PorchSF And also calculate total number of Bathrooms","ed316182":"Here there are three outliers with too low values","d363a74d":"**Explenation:**\n<pre>Now as we have converted all these ordinal vaiables to Categorical Data Type features with ordered relation. Lets convert the labels to integers.\n\nThe integers are assigned bsaed on the order for example for lowest category it will assign zero and highest catgoery it will assign the nth position form zero.\n\nExample: For **FireplaceQu**\n\n['NA' < 'Po' < 'Fa' < 'TA' < 'Gd' < 'Ex'] : Categorical Data Type\n\n[  0  <   1  <  2   <  3   <  4   <  5  ] : Integer Representation<\/pre>","41e100bc":"<h3> VIF Analysis <\/h3>\n\nThe VIF score should be below 5 for an ideal model.","093cf339":"If we can see here the first two values of 1stFlrSF are incorrect and are at both extreme","6628447f":"Lets drop the data in index 1298, this data doesn't seem to be wrong as this could be an exceptional record in our dataset but it lies far greater than other features and is being an outlier for the existing data so hence we need to handle it.\n\n**This index has a huge impact on all the features deleting this record will be the best option, cause its affecting our good correlated features**","18c85750":"<br>\n<h1> Model Building <\/h1>","07946e7c":"From the above column count 29 new columns will be added to our final dataFrame","24cc26e9":"<h3> Manual Ordinal Data Encoding<\/h3>","3f9feaf6":"**Columns to Drop:**\n    \n    ExterCond : drop this column as mean is same for TA and Gd and other values are too less for prediction\n    \n    Exterior1st : The spread of data is across the price range so the correlation will be less and might not be helpful in prediction\n    \n    Fence : The mean is almost same for all types of fence so lets drop it\n    \n    LotConfig : The mean of all labels are in same range\n    \n    RoofStyle : Two categories has same mean with most of the datapoints\n    "}}