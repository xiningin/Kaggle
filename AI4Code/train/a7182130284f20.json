{"cell_type":{"76541bdc":"code","2d32ca1c":"code","13430f6e":"code","7c17951d":"code","a8c8d98e":"code","7036ea57":"code","b498e1ae":"code","d724e475":"code","e3758f9d":"code","6b1e2479":"code","0ed09560":"code","bb1040c1":"code","b414d012":"code","962d57d8":"code","f99e9991":"code","59ebbf5c":"code","8f0714ed":"markdown"},"source":{"76541bdc":"import numpy as np\nimport cv2\nimport glob\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.models import Model\nfrom keras import layers\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.utils.vis_utils import plot_model\nimport matplotlib.pyplot as plt","2d32ca1c":"data=[]\nlabels=[]\nfor i in glob.glob('..\/input\/cats-versus-dogs\/data\/train\/*\/*'):\n    img=cv2.imread(i)\n    re_img=cv2.resize(img,(28,28))\n    data.append(re_img)\n    target=i.split('\/')[-2]\n    labels.append(target)","13430f6e":"valid_data=[]\nvalid_label=[]\nfor i in glob.glob('..\/input\/cats-versus-dogs\/data\/validation\/*\/*'):\n    imgg=cv2.imread(i)\n    re_imgg=cv2.resize(imgg,(28,28))\n    valid_data.append(re_imgg)\n    target=i.split('\/')[-2]\n    valid_label.append(target)","7c17951d":"valid_data=np.array(valid_data)\/255\nvalid_label=np.array(valid_label)","a8c8d98e":"data=np.array(data)\/255\nlabels=np.array(labels)","7036ea57":"print(valid_data.shape)\nprint(\"\\n\")\nprint(valid_label.shape)","b498e1ae":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\nlabels=le.fit_transform(labels)","d724e475":"labels=to_categorical(labels,2)","e3758f9d":"valid_label=le.fit_transform(valid_label)\nvalid_label=to_categorical(valid_label,2)","6b1e2479":"valid_label.shape","0ed09560":"Input_layer = layers.Input(shape=(28,28,3))\n\nX_copy = layers.Conv2D(32, (3, 3),activation='relu', padding='same')(Input_layer)\n\nConv_layer1 = layers.Conv2D(32, (3, 3),activation='relu', padding='same')(Input_layer)\nBatch_norm1=layers.BatchNormalization()(Conv_layer1)\nConv_layer2 = layers.Conv2D(32, (3, 3),activation='relu', padding='same')(Batch_norm1)\nBatch_norm2=layers.BatchNormalization()(Conv_layer2)\nBlock1_output = layers.Add()([X_copy, Batch_norm2])\n\nX_copy = Block1_output\n\nConv_layer3 = layers.Conv2D(32, (3, 3),activation='relu', padding='same')(Block1_output)\nBatch_norm3=layers.BatchNormalization()(Conv_layer3)\nConv_layer4 = layers.Conv2D(32, (3, 3),activation='relu', padding='same')(Batch_norm2)\nBatch_norm4=layers.BatchNormalization()(Conv_layer4)\nBlock2_output = layers.Add()([X_copy, Batch_norm4])\n\n\nX_copy = Block2_output\n\nConv_layer5 = layers.Conv2D(64, (3, 3),strides=(2, 2), activation='relu', padding='same')(Block2_output)\nBatch_norm5=layers.BatchNormalization()(Conv_layer5)\nConv_layer6 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(Batch_norm5)\nBatch_norm6=layers.BatchNormalization()(Conv_layer6)\n\n\n# Block3_output = layers.Add()([X_copy, Conv_layer6])\n\nX_copy = Batch_norm6 # Block3_output\n\nConv_layer7 = layers.Conv2D(64, (3, 3),activation='relu', padding='same')(Batch_norm6)\nBatch_norm7=layers.BatchNormalization()(Conv_layer7)\nConv_layer8 = layers.Conv2D(64, (3, 3),activation='relu', padding='same')(Batch_norm7)\nBatch_norm8=layers.BatchNormalization()(Conv_layer8)\nBlock3_output = layers.Add()([X_copy, Batch_norm8])\n\nX_copy =(Block3_output)\n\n\nConv_layer9 = layers.Conv2D(64, (3, 3),activation='relu', padding='same')(Block3_output)\nBatch_norm9=layers.BatchNormalization()(Conv_layer9)\nConv_layer10 = layers.Conv2D(64, (3, 3),activation='relu', padding='same')(Batch_norm5)\nBatch_norm10=layers.BatchNormalization()(Conv_layer10)\nBlock3_output = layers.Add()([X_copy, Batch_norm10])\n\nX_copy = Batch_norm10\n\nConv_layer11 = layers.Conv2D(64, (3, 3),activation='relu', padding='same')(Batch_norm10)\nBatch_norm11=layers.BatchNormalization()(Conv_layer11)\nConv_layer12 = layers.Conv2D(64, (3, 3),activation='relu', padding='same')(Batch_norm11)\nBatch_norm12=layers.BatchNormalization()(Conv_layer12)\nBlock3_output = layers.Add()([X_copy, Batch_norm12])\n\nflatten = layers.Flatten()(Block3_output)\nBatch_norm7=layers.BatchNormalization()(flatten)\ndropout=layers.Dropout(0.7)(Batch_norm7)\ndense1=layers.Dense(500,activation='relu')(dropout)\nBatch_norm8=layers.BatchNormalization()(dense1)\ndropout2=layers.Dropout(0.8)(Batch_norm8)\ndense2=layers.Dense(250,activation='relu')(dropout2)\nBatch_norm9=layers.BatchNormalization()(dense2)\ndropoutt=layers.Dropout(0.8)(Batch_norm9)\n\ndense3=layers.Dense(10,activation='relu')(dropoutt)\nBatch_norm10=layers.BatchNormalization()(dense3)\ndropouttt=layers.Dropout(0.9)(Batch_norm10)\n\nCf = layers.Dense(2, activation='softmax')(dropouttt)","bb1040c1":"my_model = Model(inputs = Input_layer, outputs=Cf)","b414d012":"my_model.compile(\n   optimizer='Adam',\n   loss='categorical_crossentropy',\n   metrics=['accuracy']\n)","962d57d8":"history= my_model.fit(data, labels, batch_size=128, epochs=50,validation_data=(valid_data,valid_label))","f99e9991":"fig, axs = plt.subplots(1,2,figsize=(15,5)) \n    # summarize history for accuracy\naxs[0].plot(history.history['accuracy']) \naxs[0].plot(history.history['val_accuracy']) \naxs[0].set_title('Model Accuracy')\naxs[0].set_ylabel('Accuracy') \naxs[0].set_xlabel('Epoch')\naxs[0].legend(['train', 'validate'], loc='upper left')\n    # summarize history for loss\naxs[1].plot(history.history['loss']) \naxs[1].plot(history.history['val_loss']) \naxs[1].set_title('Model Loss')\naxs[1].set_ylabel('Loss') \naxs[1].set_xlabel('Epoch')\naxs[1].legend(['train', 'validate'], loc='upper left')\nplt.show()","59ebbf5c":"#####################################################","8f0714ed":"# Conclusion:\n# \n# Resnet network doesn't work well on this dataset.\n# \n# You can implement Sequential model."}}