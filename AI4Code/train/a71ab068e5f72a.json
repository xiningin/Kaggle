{"cell_type":{"35e48f32":"code","4208ab2d":"code","a6ea9c03":"code","01fe948d":"code","e1fcedc0":"code","a976a333":"code","130526e9":"markdown","9293b243":"markdown","137a084d":"markdown","9b2392b7":"markdown","a40ddee4":"markdown","c2f3c484":"markdown","8506f5d2":"markdown","1b29c57c":"markdown"},"source":{"35e48f32":"import numba as nb\nimport numpy as np\nimport pandas as pd\nfrom sklearn import metrics\n\nfrom contextlib import contextmanager\nfrom time import perf_counter\n\n\n@contextmanager\ndef timer(name):\n    t0 = perf_counter()\n    yield\n    t1 = perf_counter()\n    print('[{}] done in {:.3f} s'.format(name, t1-t0))\n","4208ab2d":"def macro_f1_score(y_true, y_pred, n_labels):\n    total_f1 = 0.\n    for i in range(n_labels):\n        yt = y_true == i\n        yp = y_pred == i\n\n        tp = np.sum(yt & yp)\n\n        tpfp = np.sum(yp)\n        tpfn = np.sum(yt)\n        if tpfp == 0:\n            print('[WARNING] F-score is ill-defined and being set to 0.0 in labels with no predicted samples.')\n            precision = 0.\n        else:\n            precision = tp \/ tpfp\n        if tpfn == 0:\n            print(f'[ERROR] label not found in y_true...')\n            recall = 0.\n        else:\n            recall = tp \/ tpfn\n\n        if precision == 0. or recall == 0.:\n            f1 = 0.\n        else:\n            f1 = 2 * precision * recall \/ (precision + recall)\n        total_f1 += f1\n    return total_f1 \/ n_labels","a6ea9c03":"macro_f1_score_nb = nb.jit(nb.float64(nb.int32[:], nb.int32[:], nb.int64), nopython=True, nogil=True)(macro_f1_score)","01fe948d":"n_class = 10\ndatasize = 5_000_000\ny_true = np.random.randint(0, n_class, datasize).astype(np.int32)\ny_pred = np.random.randint(0, n_class, datasize).astype(np.int32)","e1fcedc0":"with timer('sklearn'):\n    f1_sk = metrics.f1_score(y_true, y_pred, average='macro')\nwith timer('custom'):\n    f1_custom = macro_f1_score(y_true, y_pred, n_class)\nwith timer('custom numba'):\n    f1_custom_nb = macro_f1_score_nb(y_true, y_pred, n_class)","a976a333":"print('f1_sk', f1_sk, 'f1_custom', f1_custom, 'f1_custom_nb', f1_custom_nb)","130526e9":"Also, we can use **[numba](http:\/\/numba.pydata.org\/)**, which does [JIT (Just In Time compilation)](https:\/\/en.wikipedia.org\/wiki\/Just-in-time_compilation) for further acceleration.","9293b243":"# Benchmark\n\nLet's compare the speed for each implementation!\n\nI set `n_class=10` and `datasize=5M`, which match to this competition's training data.","137a084d":"# Fast Macro F1 computation\n\nMacro F1 score is used as [evaluation metric](https:\/\/www.kaggle.com\/c\/liverpool-ion-switching\/overview\/evaluation) for this Liverpool ion switching competition.\n\nHere I will introduce implementation which is **around 10 times faster than [sklearn implementation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.f1_score.html)**.","9b2392b7":"# Why we need fast computation?\n\nWhen you just calculate macro f1 score for training validation, we don't execute computation many times so its profit is not so high.\nIts benefit comes when you want to optimize F1 score, as introduced in the discussion by @artgor: [It is easy to change OptimizedRounder to optimize F1](https:\/\/www.kaggle.com\/c\/liverpool-ion-switching\/discussion\/134138).","a40ddee4":"As we can see custom implementation is about **10 times faster!**, also we can get **2 times boost** by using numba version.","c2f3c484":"# Implementation","8506f5d2":"As written in the [evaluation metric](https:\/\/www.kaggle.com\/c\/liverpool-ion-switching\/overview\/evaluation) page, macro F1 score is defined as:\n\n![image.png](attachment:image.png)\n\n\nLet's simply implement this equation.","1b29c57c":"Check value is same!"}}