{"cell_type":{"7935f00d":"code","557ec306":"code","0783956f":"code","b6e38234":"code","4a2fe2a4":"code","bcfe7e3e":"code","ab5ed288":"code","46b37265":"code","bf4c9ece":"markdown","4d586274":"markdown","c114cbec":"markdown","77f4ebfe":"markdown","491f9830":"markdown"},"source":{"7935f00d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","557ec306":"from sklearn.model_selection import train_test_split\nfrom keras.utils import np_utils\n\n# Read data\ntrain = pd.read_csv('..\/input\/train.csv')\ny = train['label'].values\ny = np_utils.to_categorical(y)\nX = train[train.columns[1:]].values\nX_test = pd.read_csv('..\/input\/test.csv').values\n\n# split data\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=.2, random_state=42)\nprint(\"Shape of X_train: {}\".format(X_train.shape))\nprint(\"Shape of y_train: {}\".format(y_train.shape))\nprint(\"Shape of X_val: {}\".format(X_val.shape))\nprint(\"Shape of y_val: {}\".format(y_val.shape))","0783956f":"X_train = X_train \/ 255\nX_val = X_val \/ 255\nX_test = X_test \/ 255","b6e38234":"from keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization, Activation\nfrom keras.layers.core import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras import callbacks\nfrom keras.optimizers import Adagrad\n\n\nmodel = Sequential()\n\nmodel.add(Conv2D(100, kernel_size=(5,5), activation='elu', padding='same', input_shape=(28, 28, 1)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(100, kernel_size=(5,5), activation='elu', padding='same'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Flatten())\nfor j in range(12):\n    model.add(Dense(100, use_bias=False))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.2))\n    model.add(Activation('elu'))   \n\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer=Adagrad(), metrics=['accuracy'])\n\nmodel.fit(X_train.reshape(-1, 28, 28, 1), y_train, epochs=50, batch_size=32, \n          validation_data=(X_val.reshape(-1, 28, 28, 1), y_val),\n          callbacks=[callbacks.TerminateOnNaN(), callbacks.EarlyStopping(patience=3)])","4a2fe2a4":"scores = model.evaluate(X_val.reshape(-1, 28, 28, 1), y_val)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))","bcfe7e3e":"y_pred = model.predict(X_val.reshape(-1, 28, 28, 1))\nprint(y_pred.shape)\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import balanced_accuracy_score\n\ny_pred_class = (y_pred >= 0.5).astype(float).argmax(axis=1);\ny_val_class = y_val.argmax(axis=1)\n\nprint(confusion_matrix(y_val_class, y_pred_class))\nprint(balanced_accuracy_score(y_val_class, y_pred_class))","ab5ed288":"y_pred_test = model.predict(X_test.reshape(-1, 28, 28, 1))\ny_test_class = y_pred_test.argmax(axis=1)","46b37265":"# output result\ndataframe = pd.DataFrame({\"ImageId\": list(range(1,len(y_test_class)+1)), \"Label\": y_test_class})\ndataframe.to_csv('output_nn.csv', index=False, header=True)","bf4c9ece":"...and output it!","4d586274":"# Multi hidden layers neural network\n\nLet's try to add some hidden layers","c114cbec":"## Befor continue, let's normalize data","77f4ebfe":"So, finally we could get a better result than SVM! Let's predict the tests...","491f9830":"# Loading data\n\nTo be a more real problem, we should not load mnist dataset, but rely on the supplied data. It would be easy to mess with data if we consider mnist complete dataset."}}