{"cell_type":{"84033f70":"code","1ee8db50":"code","6b4cd5c2":"markdown","2e8bcd17":"markdown","bf34df29":"markdown","384a0d94":"markdown","4f087789":"markdown"},"source":{"84033f70":"from yellowbrick.model_selection import LearningCurve\nfrom sklearn.model_selection import KFold\nimport xgboost as xgb\nimport numpy as np\nimport pandas as pd\n\nCV = KFold(4, shuffle=True)\n\ndf_1000 = pd.read_csv(\"\/kaggle\/input\/fem-simulations\/1000randoms.csv\")\nmetric = 'r2'\n\nfeatures = ['ecc', 'N', 'gammaG', 'Esoil', 'Econc', 'Dbot', 'H1', 'H2', 'H3']\nlabel = ['Mr_t', 'Mt_t', 'Mr_c', 'Mt_c']\n\nX = df_1000[features]\ny = df_1000[label]","1ee8db50":"model = xgb.XGBRegressor(max_depth=3, n_estimators=200)\nts = np.linspace(0.1, 1.0, 8)\nviz1 = LearningCurve(model=model, train_sizes=ts, cv=CV, scoring=metric)\nviz1.fit(X, y.iloc[:,0])\nviz1.show();\n","6b4cd5c2":"# Data\nLoad Data and initialize environment","2e8bcd17":"# Intro ","bf34df29":"# Comment\nAs we can se from the Learning Curve, reducing half of the simulations reduces less than 0.1 of the R\u00b2 score. Therefore, few simulations are necessary to fit the data with a reliable surrogate model.","384a0d94":"This notebook is a brief expansion on the commentary that @pascal456 gave in his notebook. Finite Element Simulations are expensive, so we might desire less instances to train our model. But how much? Thats what we are going to try to answer. For this, we are going to use Learning Curve.","4f087789":"# Configuration\nConfiguration of Learning Curve"}}