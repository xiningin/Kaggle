{"cell_type":{"7d5a8ad8":"code","1559eed2":"code","71149dac":"code","431b9e9f":"code","e4e59ba3":"code","07d5e7b8":"code","0cc74e92":"code","5a8b2030":"code","d0bffe3a":"code","3b98887a":"code","e44e0059":"code","d8ecb1e2":"code","5ad8db45":"code","b1001a19":"code","7100d256":"code","03289e10":"code","9f4bc927":"code","42d7b2ed":"code","a9dbefab":"code","83d678f3":"code","00bca4dd":"code","397a3f70":"code","6ca0f333":"markdown","543d34e1":"markdown","239c7a83":"markdown","74cf73ec":"markdown","bb800e5e":"markdown","47859736":"markdown","8e27155f":"markdown","8c9889c5":"markdown","f817ec83":"markdown","9b9a138f":"markdown","c789343a":"markdown","57911462":"markdown","a1db1a15":"markdown","49064c32":"markdown","3d21d669":"markdown","075285ad":"markdown","2e8f545c":"markdown","37c7c89a":"markdown"},"source":{"7d5a8ad8":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\nimport os\n","1559eed2":"\n#we will unzip our data with numpy \nxd = np.load('..\/input\/Sign-language-digits-dataset\/X.npy')\nyd = np.load('..\/input\/Sign-language-digits-dataset\/Y.npy')\n#we will plot our data\nimg_size = 64\nplt.subplot(1, 2, 1)\nplt.imshow(xd[203].reshape(img_size, img_size))\nplt.axis('off')\nplt.subplot(1, 2, 2)\nplt.imshow(xd[821].reshape(img_size, img_size))\nplt.axis('off')\n","71149dac":"# Join a sequence of arrays along an row axis.\nX = np.concatenate((xd[:204], xd[615:821] ), axis=0)  \nz = np.zeros(205)\no = np.ones(205)\nY = np.concatenate((z, o), axis=0).reshape(X.shape[0],1)\nprint(\"X shape: \" , X.shape) # answer will be 410,64,64 and 410 means that there are 205 zeros and 205 ones\nprint(\"Y shape: \" , Y.shape)","431b9e9f":"#we will create our train and test datas\n\nfrom sklearn.model_selection import train_test_split\nXtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.25, random_state=21)\nnumber_of_train = Xtrain.shape[0]\nnumber_of_test = Xtest.shape[0]","e4e59ba3":"#we have 3 dimnetionals input arrary (X), we need to make it 2 dimentional\n\nXtrain_flatten = Xtrain.reshape(number_of_train,Xtrain.shape[1]*Xtrain.shape[2])\nXtest_flatten = Xtest .reshape(number_of_test,Xtest.shape[1]*Xtest.shape[2])\nprint(\"X train flatten\",Xtrain_flatten.shape)\nprint(\"X test flatten\",Xtest_flatten.shape)\n#we will see that we have 328 images and each image 4096 pixels","07d5e7b8":"xtrain = Xtrain_flatten.T\nxtest = Xtest_flatten.T\nytrain = Ytrain.T\nytest = Ytest.T\nprint(\"x train: \",xtrain.shape)\nprint(\"x test: \",xtest.shape)\nprint(\"y train: \",ytrain.shape)\nprint(\"y test: \",ytest.shape)","0cc74e92":"#initializing parameteres w=weight and b=bias\ndimension=xtrain.shape[0]\ndef initialize_weight_and_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w, b\n\n#we will see our results that w=(4096,1) and b=0.0\nw,b=initialize_weight_and_bias(dimension)\nw.shape,b","5a8b2030":"#sigmoid function and z=(w.T)*x+b\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head","d0bffe3a":"#forward propogation\n\ndef forward_propagation(w,b,xtrain,ytrain):\n    z = np.dot(w.T,xtrain) + b\n    y_head = sigmoid(z) # probabilistic 0-1\n    #loss (error) function mathematical expression\n    loss = -ytrain*np.log(y_head)-(1-ytrain)*np.log(1-y_head)\n    #Cost function mathematical expression\n    cost = (np.sum(loss))\/xtrain.shape[1]      # x_train.shape[1]  is for scaling\n    return cost  ","3b98887a":"#we can write forward and backward propogation in the same function\n\ndef forward_backward_propagation(w,b,xtrain,ytrain):\n    #part of the forward propogation \n    z = np.dot(w.T,xtrain) + b\n    y_head = sigmoid(z)\n    #loss (error) function mathematical expression\n    loss = -ytrain*np.log(y_head)-(1-ytrain)*np.log(1-y_head)\n    #Cost function mathematical expression\n    cost = (np.sum(loss))\/xtrain.shape[1]      # x_train.shape[1]  is for scaling\n    \n    #part of the backward propogation\n    derivative_weight = (np.dot(xtrain,((y_head-ytrain).T)))\/xtrain.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-ytrain)\/xtrain.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    return cost,gradients","e44e0059":"#Updating (Learning) data\n\ndef update(w, b, xtrain, ytrain, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    #updating parameters depens on number of iterations\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,xtrain,ytrain)\n        cost_list.append(cost)\n        #part of update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","d8ecb1e2":"#Prediction\n\ndef predict(w,b,xtest):\n    #Xtest is input for forward propogation\n    z = sigmoid(np.dot(w.T,xtest)+b)\n    Y_prediction = np.zeros((1,xtest.shape[1]))\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","5ad8db45":"#Logistic Regression\ndef logistic_regression(Xtrain,Ytrain,Xtest,Ytest,learning_rate,number_of_iterations):\n    dimension=Xtrain.shape[0] #4096\n    w,b=initialize_weight_and_bias(dimension)\n    parameters,gradients,cost_list=update(w,b,Xtrain,Ytrain,learning_rate,number_of_iterations)\n    \n    y_prediction_test=predict(parameters[\"weight\"],parameters[\"bias\"],Xtest)\n    y_prediction_train=predict(parameters[\"weight\"],parameters[\"bias\"],Xtrain)\n    \n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - Ytrain)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - Ytest)) * 100))\n    \nlogistic_regression(xtrain, ytrain, xtest, ytest,learning_rate = 0.01, number_of_iterations = 150)","b1001a19":"# a1,a2,a3 are hidden layers\ndef initialize_parameters_and_layer_sizes_Neural_Network(xtrain, ytrain):\n    parameters = {\"weight1\": np.random.randn(3,xtrain.shape[0]) * 0.1,\n                  \"bias1\": np.zeros((3,1)),\n                  \"weight2\": np.random.randn(ytrain.shape[0],3) * 0.1,\n                  \"bias2\": np.zeros((ytrain.shape[0],1))}\n    return parameters","7100d256":"def forward_propagation_Neural_Network(xtrain, parameters):\n\n    Z1 = np.dot(parameters[\"weight1\"],xtrain) +parameters[\"bias1\"]\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(parameters[\"weight2\"],A1) + parameters[\"bias2\"]\n    A2 = sigmoid(Z2)\n\n    cache = {\"Z1\": Z1,\n             \"A1\": A1,\n             \"Z2\": Z2,\n             \"A2\": A2}\n    \n    return A2, cache","03289e10":"def compute_cost_Neural_Network(A2, Y, parameters):\n    logprobs = np.multiply(np.log(A2),Y)\n    cost = -np.sum(logprobs)\/Y.shape[1]\n    return cost\n#forward prpogation is complited","9f4bc927":"def backward_propagation_Neural_Network(parameters, cache, X, Y):\n\n    dZ2 = cache[\"A2\"]-Y\n    dW2 = np.dot(dZ2,cache[\"A1\"].T)\/X.shape[1]\n    db2 = np.sum(dZ2,axis =1,keepdims=True)\/X.shape[1]\n    dZ1 = np.dot(parameters[\"weight2\"].T,dZ2)*(1 - np.power(cache[\"A1\"], 2))\n    dW1 = np.dot(dZ1,X.T)\/X.shape[1]\n    db1 = np.sum(dZ1,axis =1,keepdims=True)\/X.shape[1]\n    grads = {\"dweight1\": dW1,\n             \"dbias1\": db1,\n             \"dweight2\": dW2,\n             \"dbias2\": db2}\n    return grads","42d7b2ed":"def update_parameters_Neural_Network(parameters, grads, learning_rate = 0.01):\n    parameters = {\"weight1\": parameters[\"weight1\"]-learning_rate*grads[\"dweight1\"],\n                  \"bias1\": parameters[\"bias1\"]-learning_rate*grads[\"dbias1\"],\n                  \"weight2\": parameters[\"weight2\"]-learning_rate*grads[\"dweight2\"],\n                  \"bias2\": parameters[\"bias2\"]-learning_rate*grads[\"dbias2\"]}\n    \n    return parameters","a9dbefab":"def predict_Neural_Network(parameters,xtest):\n    # x_test is a input for forward propagation\n    A2, cache = forward_propagation_Neural_Network(xtest,parameters)\n    Y_prediction = np.zeros((1,xtest.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(A2.shape[1]):\n        if A2[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","83d678f3":"def two_layer_neural_network(xtrain, ytrain,xtest,ytest, num_iterations):\n    cost_list = []\n    index_list = []\n    #initialize parameters and layer sizes\n    parameters = initialize_parameters_and_layer_sizes_Neural_Network(xtrain, ytrain)\n\n    for i in range(0, num_iterations):\n         # forward propagation\n        A2, cache = forward_propagation_Neural_Network(xtrain,parameters)\n        # compute cost\n        cost = compute_cost_Neural_Network(A2, ytrain, parameters)\n         # backward propagation\n        grads = backward_propagation_Neural_Network(parameters, cache, xtrain, ytrain)\n         # update parameters\n        parameters = update_parameters_Neural_Network(parameters, grads)\n        \n        if i % 100 == 0:\n            cost_list.append(cost)\n            index_list.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    plt.plot(index_list,cost_list)\n    plt.xticks(index_list,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    # predict\n    y_prediction_test = predict_Neural_Network(parameters,xtest)\n    y_prediction_train = predict_Neural_Network(parameters,xtrain)\n\n    # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - ytrain)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - ytest)) * 100))\n    return parameters\n\nparameters = two_layer_neural_network(xtrain, ytrain,xtest,ytest, num_iterations=2000)","00bca4dd":"xtrain,xtest,ytrain,ytest=xtrain.T,xtest.T,ytrain.T,ytest.T","397a3f70":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential # initialize neural network library\nfrom keras.layers import Dense # build our layers library\ndef build_classifier():\n    classifier = Sequential() # initialize neural network\n    #uints=output dimensions of node, kernel_initializer=to initialize weight\n    classifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu', input_dim = xtrain.shape[1]))\n    #with Dense , we can add new layer\n    classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    #adam is optimization alghoritm for training Neural Network\n    return classifier\nclassifier = KerasClassifier(build_fn = build_classifier, epochs = 100)# epochs=number of iterations\naccuracies = cross_val_score(estimator = classifier, X = xtrain, y = ytrain, cv = 3)\nmean = accuracies.mean()\nvariance = accuracies.std()\n# predict\ny_prediction_test = predict_Neural_Network(parameters,xtest.T)\ny_prediction_train = predict_Neural_Network(parameters,xtrain.T)\n\n    # Print train\/test Errors\nprint(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - ytrain.T)) * 100))\nprint(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - ytest.T)) * 100))\nprint(\"Accuracy mean: \"+ str(mean))\nprint(\"Accuracy variance: \"+ str(variance))","6ca0f333":"   # c. Loss function and Cost function\n  we wil find Loss and Cost function like at logistic regression. but we will use cross entropy function and this function is also the same with logistic regression\n  <a href=\"https:\/\/imgbb.com\/\"><img src=\"https:\/\/image.ibb.co\/nyR9LU\/as.jpg\" alt=\"as\" border=\"0\"><\/a><br \/>","543d34e1":"# we will look what we have in our data\n\nThere are 2062 sign language digits images in this data set\nwe have 205 image for each sign number that are from zero to 9","239c7a83":"# a. Size of layers and initializing parameters weight and bias\n* For x_train that has 348 sample $x^{(348)}$:\n$$z^{[1] (348)} =  W^{[1]} x^{(348)} + b^{[1] (348)}$$ \n$$a^{[1] (348)} = \\tanh(z^{[1] (348)})$$\n$$z^{[2] (348)} = W^{[2]} a^{[1] (348)} + b^{[2] (348)}$$\n$$\\hat{y}^{(348)} = a^{[2] (348)} = \\sigma(z^{ [2] (348)})$$","74cf73ec":"   # f. prediction with learn parameters weight and bias   ","bb800e5e":"# General Notes:\n1. Deep learning is a technique like machine learning, in deep learning technique learns features from data.\n2. machine learning algorithms are not sufficient for big datas, because of that, deep learning gives better results\n3. Machine learning covers deep learning\n![image.png](attachment:image.png)\n\n4. Learning rate is called as a hyperparameter\n\n\n","47859736":"** WHAT  WILL WE LEARN! **\n\n1. Logistic Regression\n2. Artificial Neural Network\n3. \n","8e27155f":"# Artificial Neural Network (ANN)\n* In this part first, we will learn 2-Layer ANN to understand how it works, Later we will see how we can make more label too\n* first we will learn 2-layer Neural Network and we will learn;\n        a. Size of layers and initializing parameters weight and bias\n        b. forward propogation\n        c. Loss function and Cost function\n        d. backward propogation\n        e. update parameters\n        f. prediction with learn parameters weight and bias\n        g. creat model","8c9889c5":"# Logistic Regression\n1. initialization\n2.  Forward propogation\n    * sigmoid function\n    * loss(error) function\n    * cost function\n    \n    ![image.png](attachment:image.png)","f817ec83":"# Layer Neural Network\nwe will use keras library\n\nloss:\n$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{1}$$","9b9a138f":"# b. forward propogation","c789343a":" # g. Create Model","57911462":"   Note: Derivative of the weight and bias function and in here J=cost , m=Xtrain.shape[1]\n    $$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}x(  y_head - y)^T$$\n$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (y_head-y)$$","a1db1a15":" # Updating parameters\n alpha=learning rate\n <a href=\"http:\/\/imgbb.com\/\"><img src=\"http:\/\/image.ibb.co\/hYTTJH\/8.jpg\" alt=\"8\" border=\"0\"><\/a>","49064c32":"# e. update parameters ","3d21d669":"Thank you for looking my kernel and thank you in advance for your comment and votes\nThanks to DATAI Team","075285ad":"# d. backward propogation\nnow we will update our weight and bias values because of this we need to make a backward propogation\n","2e8f545c":"# Optimization algorithm with Gradient Descent\n    1. Backward propogation\n    2. Updating parameters\n    ","37c7c89a":"<a href=\"http:\/\/ibb.co\/eF315x\"><img src=\"http:\/\/preview.ibb.co\/dajVyH\/9.jpg\" alt=\"9\" border=\"0\"><\/a>"}}