{"cell_type":{"41150d4a":"code","ad92fa7e":"code","031c4602":"code","a494d2d3":"code","a2312d30":"code","cbae7369":"code","7d8e0c5c":"code","ab66a41f":"code","4efda5a3":"code","2d8b961a":"code","e0f68691":"code","db2d3f48":"code","50c0a8b7":"code","2af87ad1":"code","d1ee69a4":"code","d4577e26":"code","52d701b3":"code","ca30d9b0":"code","8677c002":"code","8e5b9d26":"code","3c1a473f":"code","43c673b5":"code","e4cbee5a":"code","3036e9f5":"code","9c6641ec":"code","7977cc69":"code","da0e65ce":"code","84691aa6":"code","8470997c":"code","304adf09":"code","778eb12f":"code","a13bb480":"code","f41639fd":"code","fdd9eb2b":"code","85879662":"code","741f8b50":"code","5f1c8b5d":"code","802fcdff":"code","6eb9e1a6":"code","e6204b0a":"code","a127fbb0":"code","32614165":"code","e7461446":"code","f01273f5":"code","478e7fdb":"code","ad43bc0d":"code","9b3f2d23":"code","69150e3a":"code","885d2ea9":"code","6538d1a2":"code","df14e42b":"code","0b096abc":"code","b8fa8874":"code","899a383a":"code","ca99412d":"code","6535da12":"code","a36f356a":"code","425ee145":"code","a29e1929":"code","bcf3fc2b":"code","e45807a0":"code","d3a9b9b8":"code","8a31876e":"code","bb8ee609":"code","fe67b847":"code","985c3223":"code","a13417c4":"code","9478aa66":"code","5c6bdd06":"code","60362ff1":"code","f3c3a008":"code","873505ba":"code","58af7090":"code","6067d1c6":"code","9d4acd37":"code","e2a124bc":"code","ad7bcd0d":"code","d0ea6c50":"code","5d22df08":"markdown","f6f919f2":"markdown","2e3ef10c":"markdown","48b4d567":"markdown","3d1c0508":"markdown","506a7dc5":"markdown","4cc46737":"markdown","9c63100b":"markdown","efcb59fe":"markdown","997a7def":"markdown","e5c22939":"markdown","c4e5e7cc":"markdown","9f836562":"markdown","5e55c763":"markdown","f1b6cb80":"markdown","c08ba88d":"markdown","5114228b":"markdown","a04cb8aa":"markdown","7835d773":"markdown","2b9e809c":"markdown","501bba06":"markdown","eec70955":"markdown","f1998a7c":"markdown","98a07ea9":"markdown","f5505c82":"markdown","3901973a":"markdown","6bd11fa2":"markdown","56794408":"markdown","03fd9a2f":"markdown","fe2d75ec":"markdown","d698893b":"markdown","79940bbf":"markdown","7463c2d6":"markdown","0cbffc8f":"markdown","451e6444":"markdown","f473211f":"markdown","ba659bc0":"markdown","215582e6":"markdown","cff89b8d":"markdown","846220c0":"markdown","618b1352":"markdown","ce56e9e9":"markdown","35e46a8d":"markdown","be63d939":"markdown","eabd7c8a":"markdown","c0f3415a":"markdown","5ecd70f4":"markdown","210f6b08":"markdown","8f511e44":"markdown","42717a7e":"markdown","32b91188":"markdown","8556fb13":"markdown","c4914ad6":"markdown","e6017274":"markdown","e495ce7a":"markdown","f4e7ba6c":"markdown","b0e71d42":"markdown","423ca70c":"markdown","5cd6edaa":"markdown","8ea41376":"markdown","bb1f14ac":"markdown","48fe8068":"markdown","3688e937":"markdown"},"source":{"41150d4a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pdb # Python debugger\nfrom IPython.display import Image, display\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import export_graphviz\nfrom sklearn.impute import KNNImputer\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, learning_curve, train_test_split, GridSearchCV\nfrom keras import models\nfrom keras import layers\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.regularizers import l2\n\nsns.set()\n\n%config InlineBackend.figure_format = 'retina' # Increase the figures' resolution in jupyter notebook\npd.set_option('display.max_colwidth', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","ad92fa7e":"df_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('..\/input\/titanic\/test.csv')\ndf_data = df_train.append(df_test).reset_index(drop=True)\ndf_submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')","031c4602":"df_data_raw = df_data.copy() # We can use this to restore the mistakes we make in df_data","a494d2d3":"display(df_train.head(), df_test.head())","a2312d30":"print('Training null values\\n')\nprint(df_train.isnull().sum()) # to know if there's missing value in the data\nprint('-'*30)\nprint('Testing null values\\n')\nprint(df_test.isnull().sum())","cbae7369":"df_submission.head(2)","7d8e0c5c":"df_data.describe()","ab66a41f":"df_data.describe(include='all')","4efda5a3":"sns.countplot(x=df_data['Sex'], hue=df_data['Survived'])\ndisplay(df_data[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean())","2d8b961a":"sns.countplot(x=df_data['Pclass'], hue=df_data['Survived'])\ndisplay(df_data[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean())","e0f68691":"# Split two target data from df_data\ndf_parch = df_data[['Parch', 'Survived']].groupby(['Parch'], as_index=False).mean()\ndf_sibsp = df_data[['SibSp', 'Survived']].groupby(['SibSp'], as_index=False).mean()\n\n# Plot\nfig = plt.figure(figsize=(10, 5))\nax = plt.gca()\nax.plot(df_parch['Parch'], df_parch['Survived'], '-og', label='Parch')\nax.plot(df_sibsp['SibSp'], df_sibsp['Survived'], '-ob', label='SibSp')\nax.set_xlabel('Numbers')\nax.set_ylabel('Mean of Survival')\nax.legend()\nplt.show()","db2d3f48":"# Filling missing values\ndf_data['Fare'] = df_data['Fare'].fillna(df_data['Fare'].median())","50c0a8b7":"fg = sns.FacetGrid(df_train, hue=\"Survived\",aspect=3, height=4)\nfg.map(sns.kdeplot, 'Fare', shade=True)\nfg.set(xlim=(-50, df_train['Fare'].max()))\nfg.add_legend()\nplt.show()","2af87ad1":"fg = sns.FacetGrid(df_data, row='Embarked', height=3, aspect=2)\nfg.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep', hue_order=None, order=[1, 2, 3])\nfg.add_legend()\nplt.show()","d1ee69a4":"sns.countplot(x=df_data['Embarked'], hue=df_data['Survived'])\ndisplay(df_data[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean())","d4577e26":"df_data['Sex#'] = df_data['Sex'].map({'male': 0, 'female': 1})","52d701b3":"# Create new feature\ndf_data['Fsize'] = df_data['Parch'] + df_data['SibSp'] + 1 #Fsize stands for faily size = # of parents + # of siblins + self\n\n# Plot\nfg = plt.figure(figsize=(12, 8))\nsns.countplot(x=df_data['Fsize'], hue=df_data['Survived'])\nlegend = plt.legend(title='Survived', loc='upper right')\nplt.show()","ca30d9b0":"sns.pointplot(x='Fsize', y='Survived', data=df_data[['Fsize', 'Survived']].groupby(['Fsize'], as_index=False).mean())\nplt.show()","8677c002":"# Create new feature\ndf_data['IsAlone']  = 0\ndf_data.loc[df_data.Fsize == 1, 'IsAlone'] = 1\n\n# Plot\nsns.countplot(x=df_data['IsAlone'], hue=df_data['Survived'])\nplt.show()","8e5b9d26":"# Making bins\ndf_data['FareBin_4'] = pd.qcut(df_data['Fare'], 4)\ndf_data['FareBin_5'] = pd.qcut(df_data['Fare'], 5)\ndf_data['FareBin_6'] = pd.qcut(df_data['Fare'], 6)\n\n# Mapping the bins\nlabel_encoder = LabelEncoder()\ndf_data['FareBin_4_#'] = label_encoder.fit_transform(df_data['FareBin_4'])\ndf_data['FareBin_5_#'] = label_encoder.fit_transform(df_data['FareBin_5'])\ndf_data['FareBin_6_#'] = label_encoder.fit_transform(df_data['FareBin_6'])\n\n# Plot\nfig, [ax1, ax2, ax3] = plt.subplots(1, 3,sharey=True)\nfig.set_figwidth(18)\nfor axi in [ax1, ax2, ax3]:\n    axi.axhline(0.5, linestyle='dashed', c='black', alpha = .3)\ng1 = sns.barplot(x='FareBin_4_#', y=\"Survived\", data=df_data, ax=ax1)\ng2 = sns.barplot(x='FareBin_5_#', y=\"Survived\", data=df_data, ax=ax2)\ng3 = sns.barplot(x='FareBin_6_#', y=\"Survived\", data=df_data, ax=ax3)","3c1a473f":"# splits again beacuse we just engineered new feature\ndf_train = df_data[:len(df_train)]\ndf_test = df_data[len(df_train):]\n\n# Training set and labels\nx_train = df_train.drop(labels=['Survived','PassengerId'], axis=1)\ny_train = df_train['Survived']\n\n# show columns\nx_train.columns","43c673b5":"compare_features = ['Sex#','Pclass','FareBin_4_#','FareBin_5_#','FareBin_6_#']\nestimator = RandomForestClassifier(n_estimators=250, min_samples_split=20)\nselector = RFECV(estimator, cv=10, n_jobs=-1) # cv=10 means using 10-fold cross validation, n_jobs=-1 means using all processors\nselector.fit(x_train[compare_features], y_train)\n\nprint('The mask of selected features: ', selector.support_) # The mask of selected features.\nprint('Feature rankings: ', selector.ranking_) # The feature ranking, such that ranking[i] corresponds to the ranking position of the i-th feature. \nprint('CV scores: ', selector.grid_scores_*100) # The cross-validation scores such that grid_scores[i] corresponds to the CV score of the i-th subset of features.","e4cbee5a":"score_b4,score_b5, score_b6 = [], [], []\nseeds = 10\n\nfor i in range(seeds):\n    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=i)\n    estimator = RandomForestClassifier(random_state=i, n_estimators=250, min_samples_split=20)\n    selector = RFECV(estimator=estimator, cv=cv, n_jobs=-1)\n    selector.fit(x_train[compare_features], y_train)\n    \n    score_b4.append(selector.grid_scores_[2])\n    score_b5.append(selector.grid_scores_[3])\n    score_b6.append(selector.grid_scores_[4])\n\n\n# class sklearn.model_selection.StratifiedKFold(n_splits=5, *, shuffle=False, random_state=None)\n\n# Parameters:\n\n# n_splitsint: default=5\n# Number of folds, like the k-value in K-fold. Must be at least 2.\n\n# shufflebool: default=False\n# Whether to shuffle each class\u2019s samples before splitting into batches. Note that the samples within each split will not be shuffled.\n\n# random_state: RandomState instance or None, default=None\n# When shuffle is True, random_state affects the ordering of the indices, which controls the randomness of each fold for each class. Otherwise, leave random_state as None.","3036e9f5":"# to np.array\nscore_list = [score_b4, score_b5, score_b6]\nfor item in score_list:\n    item = np.array(item*100)\n\n# plot\nfig = plt.figure(figsize= (18,8))\nax = plt.gca()\nax.plot(range(seeds), score_b4,'-ok',label='bins = 4')\nax.plot(range(seeds), score_b5,'-og',label='bins = 5')\nax.plot(range(seeds), score_b6,'-ob',label='bins = 6')\nax.set_xlabel('Seed #', fontsize=16)\nax.set_ylabel('Accuracy', fontsize=16)\nax.set_ylim(0.785, 0.815)\nax.set_title('Accuracy of different bin sizes', fontsize=20)\nplt.legend(fontsize=14, loc='upper right', title='Bin size')\nplt.show()","9c6641ec":"b4, b5, b6 = ['Sex#', 'Pclass','FareBin_4_#'], ['Sex#','Pclass','FareBin_5_#'], ['Sex#','Pclass','FareBin_6_#']\n\n# Train models and predict\nb4_Model = RandomForestClassifier(\n    random_state=2, \n    n_estimators=250, \n    min_samples_split=20, \n    oob_score=True)\nb4_Model.fit(x_train[b4], y_train)\n\nb5_Model = RandomForestClassifier(\n    random_state=2, \n    n_estimators=250, \n    min_samples_split=20, \n    oob_score=True)\nb5_Model.fit(x_train[b5], y_train)\n\nb6_Model = RandomForestClassifier(\n    random_state=2, \n    n_estimators=250, \n    min_samples_split=20, \n    oob_score=True)\nb6_Model.fit(x_train[b6], y_train)\n\n# Show oob scores\nprint('Bin size = 4, oob score :{}'.format(b4_Model.oob_score_))\nprint('Bin size = 5, oob score :{}'.format(b5_Model.oob_score_))\nprint('Bin size = 6, oob score :{}'.format(b6_Model.oob_score_))","7977cc69":"# Drop features we created in the process exclude FareBin_5_# which is what we need\ndf_data['FareBin'] = df_data['FareBin_5_#']\ndf_data = df_data.drop(['FareBin_4', 'FareBin_5', 'FareBin_6', 'FareBin_4_#', 'FareBin_5_#', 'FareBin_6_#'], axis=1)","da0e65ce":"df_data['Fname'] = df_data['Name'].str.extract('([A-Za-z]+.[A-Za-z]+)\\,', expand=True)","84691aa6":"duplicates = []\n\nfor uniq in df_data['Ticket'].unique():\n    temp = df_data.loc[df_data['Ticket'] == uniq, 'Name']\n    if temp.count() > 1:\n        duplicates.append(df_data.loc[df_data['Ticket'] == uniq, ['Name', 'Ticket', 'Fare', 'FareBin', 'Fsize', 'Survived']])\nduplicates = pd.concat(duplicates)\nduplicates.head(20)","8470997c":"df_friend = duplicates.loc[(duplicates.Fsize == 1) & (duplicates.Survived.notnull())]\ndf_family = duplicates.loc[(duplicates.Fsize > 1) & (duplicates.Survived.notnull())]\ndisplay(df_friend.head(), df_family.head())","304adf09":"print('Passengers taking the same ticket: ', duplicates['Name'].count())\nprint('Family: ', df_family['Name'].count())\nprint('Friend: ', df_friend['Name'].count())\nprint('Other: ', duplicates['Name'].count() - df_family['Name'].count() - df_friend['Name'].count())","778eb12f":"df_data['Connected_Survival'] = 0.5\n\nfor ticket_num, df_grp in df_data.groupby('Ticket'):\n    if len(df_grp) > 1: # We just want the data with duplicated tickets\n            for index, row in df_grp.iterrows():\n                smax = df_grp.drop(index).Survived.max()\n                smin = df_grp.drop(index).Survived.min()\n                pid = row.PassengerId\n                if smax == 1.0:\n                    df_data.loc[df_data['PassengerId'] == pid, 'Connected_Survival'] = 1\n                elif smin == 0.0:\n                    df_data.loc[df_data['PassengerId'] == pid, 'Connected_Survival'] = 0","a13bb480":"# Take a look at the missing values\ndf_data[df_data['Embarked'].isnull()][['Embarked', 'Pclass', 'FareBin']]","f41639fd":"# Check their relation in groups\ndf_data.groupby(['Embarked', 'Pclass'])[['FareBin']].median()","fdd9eb2b":"# Take a look at the missing values. Using Fare instead of FareBin\ndf_data[df_data['Embarked'].isnull()][['Embarked', 'Pclass', 'Fare']]","85879662":"# Check their relation in groups\ndf_data.groupby(['Embarked', 'Pclass'])[['Fare']].median()","741f8b50":"# Filling missing values with the value that has greatest frequency\ndf_data['Embarked'] = df_data['Embarked'].fillna('C')\n\n# Mapping\ndf_data['Embarked#'] = df_data['Embarked'].map({'S': 1, 'C': 2, 'Q': 3})\ndf_data.head()","5f1c8b5d":"df_data['Title'] = df_data['Name'].str.extract('([A-Za-z]+)\\.', expand=False)\ndf_data['Title'] = df_data['Title'].replace(['Capt', 'Col', 'Rev', 'Don', 'Countess', 'Jonkheer', 'Dona', 'Sir', 'Dr', 'Major', 'Dr'], 'Rare')\ndf_data['Title'] = df_data['Title'].replace(['Mlle', 'Mme', 'Ms'], 'Miss')\ndf_data['Title'] = df_data['Title'].replace(['Lady'], 'Mrs')\ndf_data['Title'] = df_data['Title'].map({\"Mr\":0, \"Rare\" : 1, \"Master\" : 2,\"Miss\" : 3, \"Mrs\" : 4 })","802fcdff":"df_data.Age.describe()","6eb9e1a6":"df_data.loc[df_data.Age.notnull(), 'Age'] = df_data.loc[df_data.Age.notnull(), 'Age'].apply(np.ceil).astype('int')","e6204b0a":"df_age = df_data.loc[(df_data.Age.notnull() & df_data.Survived.notnull())]\n\nfg, ax = plt.subplots(figsize=(15, 5))\nax = sns.histplot(df_age, x='Age', hue='Survived', kde=False, bins=10, palette='deep', label='Survived', element='step', stat='density')\nax.legend(['Survived', 'Dead'])\nplt.show()","a127fbb0":"df_sex = df_data.loc[df_data.Age.notnull()]\n\nfg, ax = plt.subplots(figsize=(15, 5))\nax = sns.histplot(df_age, x='Age', hue='Sex', kde=False, bins=30, palette='deep', label='Sex', element='step')\nax.legend(['Female', 'Male'])\nplt.show()","32614165":"grid = sns.FacetGrid(df_train, row='Sex', col='Pclass', height=2.5, aspect=1.6)\ngrid.map(sns.histplot, 'Age',kde=True, alpha=.5, bins=20)\ngrid.set(xlim=(0, 80), ylim=(0, 55))\nplt.show()","e7461446":"grid = sns.FacetGrid(df_train, row='Sex', col='FareBin_4_#', height=2.4, aspect=1.6)\ngrid.map(sns.histplot, 'Age',kde=True, alpha=.5, bins=20)\ngrid.set(xlim=(0, 80), ylim=(0, 30))\nplt.show()","f01273f5":"grid = sns.FacetGrid(df_train, row='Pclass', col='FareBin_4_#', height=2.4, aspect=1.6)\ngrid.map(sns.histplot, 'Age',kde=True, alpha=.5, bins=20)\ngrid.set(xlim=(0, 80), ylim=(0, 30))\nplt.show()","478e7fdb":"grid = sns.FacetGrid(df_train, col='Pclass', height=5, aspect=1)\ngrid.map(sns.histplot, 'Age',kde=True, alpha=.5, bins=20)\ngrid.set(xlim=(0, 80), ylim=(0, 80))\nplt.show()","ad43bc0d":"grid = sns.FacetGrid(df_train, col='FareBin_4_#', height=4, aspect=1)\ngrid.map(sns.histplot, 'Age',kde=True, alpha=.5, bins=20)\ngrid.set(xlim=(0, 80), ylim=(0, 40))\nplt.show()","9b3f2d23":"class_1 = df_train[df_train['Pclass']==1]\nclass_2 = df_train[df_train['Pclass']==2]\nclass_3 = df_train[df_train['Pclass']==3]\n\ncolor = ['g', 'b', 'r']\nalpha = 0.5\nscale = 60.\n\nplt.figure(figsize=(20, 5))\nscatter = plt.scatter(class_1['Fare'], class_1['Age'], label='Pclass = 1', c=color[0], alpha=alpha, s=scale)\nscatter = plt.scatter(class_2['Fare'], class_2['Age'], label='Pclass = 2', c=color[1], alpha=alpha, s=scale)\nscatter = plt.scatter(class_3['Fare'], class_3['Age'], label='Pclass = 3', c=color[2], alpha=alpha, s=scale)\nplt.xlabel('Fare', fontsize=15)\nplt.ylabel('Age', fontsize=15)\nplt.xlim(-5, 300)\nplt.legend()\nplt.show()","69150e3a":"df_data['Age_pred1'] = df_data['Age']\nguess_ages = np.zeros((2, 3))\n\nfor sex in range(0, 2):\n    for pclass in range(0, 3):\n        guess_df = df_data[(df_data['Sex#'] == sex) & (df_data['Pclass'] == pclass + 1)]['Age_pred1'].dropna()\n        age_guess = guess_df.median()\n\n        # Convert random age float to nearest .5 age\n        guess_ages[sex, pclass] = int(age_guess\/0.5 + 0.5) * 0.5\n            \nfor sex in range(0, 2):\n    for pclass in range(0, 3):\n        df_data.loc[(df_data['Age_pred1'].isnull()) & (df_data['Sex#'] == sex) & (df_data.Pclass == pclass + 1), 'Age_pred1'] = guess_ages[sex,pclass]\n\ndf_data['Age_pred1'] = df_data['Age_pred1'].astype(int)\n\n# Check our prediction correctly saved in df\ndf_data.loc[df_data.Age.isnull()].head()","885d2ea9":"trans = df_data.groupby('Title')['Age'].median().values\ndf_data['Age_pred2'] = df_data['Age']\nfor i in range(5):\n    df_data.loc[(df_data['Title'] == i) & (df_data['Age'].isnull()), 'Age_pred2'] = trans[i]","6538d1a2":"x_train = df_data[df_data.Age.notnull()]\ny_train = df_data[df_data.Age.notnull()]['Age']\nx_test = df_data[df_data.Age.isnull()]\n\nselect_feature = ['Sex#', 'Pclass', 'Title', 'Fare']","df14e42b":"reg = LinearRegression()\nreg.fit(x_train[select_feature], y_train)\nreg.score(x_train[select_feature], y_train)","0b096abc":"df_data['Age_pred3'] = df_data['Age']\ndf_data.loc[df_data['Age'].isnull(), 'Age_pred3'] = reg.predict(x_test[select_feature]).astype('int')","b8fa8874":"df_data.columns","899a383a":"df_data['Age_pred4'] = df_data.Age","ca99412d":"selected_features = ['PassengerId','Pclass', 'Sex#', 'SibSp', 'Parch', 'Fare', 'Age_pred4']\n\nimputer = KNNImputer(n_neighbors=10, missing_values=np.nan)\nimputer.fit(df_data[selected_features])","6535da12":"df_data.loc[:, selected_features] = pd.DataFrame(imputer.transform(df_data[selected_features]), index=df_data.index, columns = selected_features)","a36f356a":"df_data.loc[df_data.Age.isnull()].head()","425ee145":"selected_features = ['PassengerId','Pclass', 'Sex#', 'SibSp', 'Parch', 'Fare']\n\nx_train = df_data[df_data.Age.notnull()][selected_features]\ny_train = df_data[df_data.Age.notnull()]['Age']\nx_test = df_data[df_data.Age.isnull()][selected_features]","a29e1929":"model = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(len(selected_features),))) # The number of features we use\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1))\n\nmodel.compile(loss='mse', optimizer='rmsprop', metrics=['acc', 'mae'])\n\nhistory = model.fit(x_train[selected_features], y_train, batch_size=256, epochs=500, verbose=0)\nmodel.evaluate(x_train[selected_features], y_train, batch_size=256, verbose=0)","bcf3fc2b":"loss = history.history['loss']\nacc = history.history['acc']\nmae = history.history['mae']\n\nepochs = range(1, len(loss) + 1)\n\nfig, [ax1, ax2, ax3] = plt.subplots(1, 3, figsize=(15, 5))\nsns.lineplot(x=epochs, y=loss, palette='deep', ax=ax1)\nsns.lineplot(x=epochs, y=acc, palette='deep', ax=ax2)\nsns.lineplot(x=epochs, y=mae, palette='deep', ax=ax3)\nax1.title.set_text('loss')\nax2.title.set_text('acc')\nax3.title.set_text('mae')\nplt.show()","e45807a0":"df_data['Age_pred5'] = df_data['Age']\ndf_data.loc[df_data['Age'].isnull(), 'Age_pred5'] = model.predict(x_test[selected_features]).astype('int32')","d3a9b9b8":"df_data['HasAge'] = df_data['Age'].notnull().map({True: 1, False: 0})\nfig, [ax1, ax2] = plt.subplots(1, 2)\n\nfig.set_figwidth(15)\nsns.countplot(x=df_data['Pclass'], hue=df_data['HasAge'], ax=ax1)\nsns.countplot(x=df_data['Sex'], hue=df_data['HasAge'], ax=ax2)\nplt.show()","8a31876e":"# Masks\nMask_Has_Age_P12 = df_data.loc[((df_data.HasAge == 1) & (df_data.Pclass != 3 ))]\n\nfg, ax = plt.subplots(figsize=(15, 5))\nax = sns.histplot(Mask_Has_Age_P12, x='Age', hue='Survived', kde=False, bins=15, palette='deep', label='Survived', element='step')\nax.legend(['Female', 'Male'])\nplt.show()","bb8ee609":"df_data['Minor_pred1'] = ((df_data['Age_pred1']) < 16)*1\ndf_data['Minor_pred2'] = ((df_data['Age_pred2']) < 16)*1\ndf_data['Minor_pred3'] = ((df_data['Age_pred3']) < 16)*1\ndf_data['Minor_pred4'] = ((df_data['Age_pred4']) < 16)*1\ndf_data['Minor_pred5'] = ((df_data['Age_pred5']) < 16)*1","fe67b847":"df_data['AgeBin_pred1'] = pd.qcut(df_data['Age_pred1'], 5)\ndf_data['AgeBin_pred2'] = pd.qcut(df_data['Age_pred2'], 5)\ndf_data['AgeBin_pred3'] = pd.qcut(df_data['Age_pred3'], 5)\ndf_data['AgeBin_pred4'] = pd.qcut(df_data['Age_pred4'], 5)\ndf_data['AgeBin_pred5'] = pd.qcut(df_data['Age_pred5'], 5)\n\ndf_data['AgeBin_pred1'] = label_encoder.fit_transform(df_data['AgeBin_pred1'])\ndf_data['AgeBin_pred2'] = label_encoder.fit_transform(df_data['AgeBin_pred2'])\ndf_data['AgeBin_pred3'] = label_encoder.fit_transform(df_data['AgeBin_pred3'])\ndf_data['AgeBin_pred4'] = label_encoder.fit_transform(df_data['AgeBin_pred4'])\ndf_data['AgeBin_pred5'] = label_encoder.fit_transform(df_data['AgeBin_pred5'])","985c3223":"df_data[['PassengerId', 'Pclass', 'Sex#']] = df_data[['PassengerId', 'Pclass', 'Sex#']].astype('int32')\ndf_data.head()","a13417c4":"df_train = df_data[:len(df_train)]\ndf_test = df_data[len(df_train):]","9478aa66":"train_features = ['Survived', 'Pclass', 'SibSp', 'Parch', 'Fare', 'Sex#', 'Fsize', 'IsAlone', 'FareBin', 'Connected_Survival', 'Embarked#', 'Age_pred1', 'Age_pred2', 'Age_pred3', 'Age_pred4', 'HasAge', 'Age_pred5', 'Minor_pred1', 'Minor_pred2', 'Minor_pred3', 'Minor_pred4', 'Minor_pred5', 'AgeBin_pred1', 'AgeBin_pred2', 'AgeBin_pred3', 'AgeBin_pred4', 'AgeBin_pred5']","5c6bdd06":"corr_mat = df_train[train_features].astype(float).corr()\ncorr_mat_fil = corr_mat.loc[:, 'Survived'].sort_values(ascending=False)\ncorr_mat_fil = pd.DataFrame(data=corr_mat_fil[1:])","60362ff1":"plt.figure(figsize=(20,12))\nbar = sns.barplot(x=corr_mat_fil.Survived.abs(), y=corr_mat_fil.index, data=corr_mat_fil, palette='deep')","f3c3a008":"train_features = ['Survived', 'Pclass', 'Sex#', 'IsAlone', 'FareBin', 'Connected_Survival', 'Embarked#', 'Age_pred4', 'Minor_pred4']\ncorr_mat = df_train[train_features].astype(float).corr()\n\nplt.figure(figsize=(20,10))\nsns.heatmap(corr_mat.abs(), annot=True)\nplt.show()","873505ba":"selected_features = ['Sex#', 'Pclass', 'FareBin', 'Connected_Survival', 'Minor_pred2']\n\n#=====================================================================================================\n# 0.79665: Sex#, Pclass, FareBin(#6), Connected_Survival, Minor_pred2, n=250, min=20\n# 0.81100: Sex#, Pclass, FareBin(#5), Connected_Survival, Minor_pred2, n=250, min=20      Current Best\n# 0.81100: Sex#, Pclass, FareBin(#5), Connected_Survival, Minor_pred2, n=300, min=20\n# 0.80861: Sex#, Pclass, FareBin(#5), Connected_Survival, Minor_pred4, n=300, min=20\n#=====================================================================================================\n\ndf_train = df_data[:len(df_train)]\ndf_test = df_data[len(df_train):]\n\nx_train = df_train[selected_features]\ny_train = df_train['Survived']\nx_test = df_test[selected_features]","58af7090":"model = RandomForestClassifier(random_state=2)\n\ngrid_parameters = {'n_estimators': [i for i in range(300, 601, 50)], 'min_samples_split' : [10, 20, 30, 40]}\ngrid = GridSearchCV(estimator=model, param_grid=grid_parameters)\ngrid_result = grid.fit(x_train, y_train)\n\n# summarize results\nprint('Best: {} using {}'.format(grid_result.best_score_, grid_result.best_params_))","6067d1c6":"n_estimator = grid_result.best_params_['n_estimators']\nmin_samples_split = grid_result.best_params_['min_samples_split']\n\nRFC = RandomForestClassifier(random_state=2, n_estimators=250, min_samples_split=20)\nRFC.fit(x_train, y_train)\ny_pred = RFC.predict(x_test)\n\noutput = pd.DataFrame({'PassengerId': df_test['PassengerId'], 'Survived': y_pred})\noutput = output.astype('int')\noutput.to_csv('RFC_prediction.csv', index=False)\nprint('Your file was successfully saved!')","9d4acd37":"# split into training and testing data\nselected_features = ['Sex#', 'Pclass', 'FareBin', 'Connected_Survival', 'Minor_pred4', 'Embarked#', 'IsAlone', 'Title']\ndf_train = df_data[:len(df_train)]\ndf_test = df_data[len(df_train):]\nx_train = df_train[selected_features]\ny_train = df_train['Survived']\nx_test = df_test[selected_features]","e2a124bc":"# Function to create model, required for KerasClassifier\n# include adjustable parameters optimizer and init\ndef create_keras_model(optimizer='adam', init='glorot_normal'):\n    \n    # create model\n    model = models.Sequential()\n    model.add(layers.Dense(8, activation='relu', kernel_initializer=init, kernel_regularizer=l2(0.1), input_shape=(x_train.shape[1],)))\n    model.add(layers.Dense(16, activation='relu', kernel_initializer=init, kernel_regularizer=l2(0.1)))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(16, activation='relu', kernel_initializer=init, kernel_regularizer=l2(0.1)))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(1, activation='sigmoid'))\n    \n    # compile model\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    \n    return model","ad7bcd0d":"# fix random seed for reproducibility\nseed = 7\nnp.random.seed(seed)\n\n# Create model\nmodel = KerasClassifier(build_fn=create_keras_model)\n\n# Set grid parameters\noptimizers = ['adam', 'rmsprop']\ninits = ['glorot_normal', 'uniform', 'normal']\nepochs = [50, 100, 150]\nbatch_sizes = [32, 64, 128]\n# grid_parameters = dict(optimizer=optimizers, nb_epoch=epochs, batch_size=batch_sizes, init=inits)\ngrid_parameters = {'optimizer':optimizers, 'init': inits, 'nb_epoch': epochs, 'batch_size': batch_sizes}\ngrid = GridSearchCV(estimator=model, param_grid=grid_parameters, n_jobs=-1, verbose=0)\n\ngrid_result = grid.fit(x_train, y_train)\n\n# summarize results\nprint('Best: {} using {}'.format(grid_result.best_score_, grid_result.best_params_))","d0ea6c50":"optimizer = grid_result.best_params_['optimizer']\nepochs = grid_result.best_params_['nb_epoch']\nbatch_size = grid_result.best_params_['batch_size']\ninit = grid_result.best_params_['init']\n\nkeras_model = create_keras_model(optimizer=optimizer, init=init)\nkeras_model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n\ny_pred = keras_model.predict(x_test)\n\noutput = pd.DataFrame({'PassengerId': df_test['PassengerId'], 'Survived': y_pred.reshape(y_pred.shape[0],)})\noutput = output.astype('int')\noutput.to_csv('keras_prediction.csv', index=False)\nprint('Your file was successfully saved!')","5d22df08":"<a id='information'><\/a>\n## 2.4 Information from the data","f6f919f2":"<a id='sex_coding'><\/a>\n## 4.1 Encoding Sex feature","2e3ef10c":"<a id='materials'><\/a>\n## 1.1 Study materials\n### Chinese\n* [[\u8cc7\u6599\u5206\u6790&\u6a5f\u5668\u5b78\u7fd2] \u7b2c3.5\u8b1b : \u6c7a\u7b56\u6a39(Decision Tree)\u4ee5\u53ca\u96a8\u6a5f\u68ee\u6797(Random Forest)\u4ecb\u7d39]('https:\/\/medium.com\/jameslearningnote\/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC3-5%E8%AC%9B-%E6%B1%BA%E7%AD%96%E6%A8%B9-decision-tree-%E4%BB%A5%E5%8F%8A%E9%9A%A8%E6%A9%9F%E6%A3%AE%E6%9E%97-random-forest-%E4%BB%8B%E7%B4%B9-7079b0ddfbda')\n* [[\u6a5f\u5668\u5b78\u7fd2\u5c08\u6848] Kaggle\u7af6\u8cfd-\u9435\u9054\u5c3c\u865f\u751f\u5b58\u9810\u6e2c(Top 3%)]('https:\/\/yulongtsai.medium.com\/https-medium-com-yulongtsai-titanic-top3-8e64741cc11f')\n* [\u96a8\u6a5f\u68ee\u6797 (Random Forest)]('https:\/\/rstudio-pubs-static.s3.amazonaws.com\/378052_30d987a09ea54b6db5aa1e82f5dce6bf.html')\n\n### English\n* [OOB score vs Validation score]('https:\/\/forums.fast.ai\/t\/oob-score-vs-validation-score\/7859')\n* [Titanic Data Science Solutions]('https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions')\n* [Ultimate EDA + FE + Neural Network Model [TOP 2%]]('https:\/\/www.kaggle.com\/sreevishnudamodaran\/ultimate-eda-fe-neural-network-model-top-2#8.-Model-Development')\n* [RFECV Sklearn document]('https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFECV.html')\n* [StratifiedKFold Sklearn document]('https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedKFold.html')","48b4d567":"It seems that with data bucketing, we can not clearly distinguish which Embared the set (Pclass, FareBin) = (1, 4) belongs to. Therefore, we can try with another aspect - the Fare.","3d1c0508":"<a id='keras_pred'><\/a>\n### 5.3.3 Set best parameters and make prediction","506a7dc5":"<a id='build'><\/a>\n# 5. Build models","4cc46737":"<a id=\"Introduction\"><\/a>\n# 1. Introduction","9c63100b":"<a id='check'><\/a>\n## 2.3 Check the content of data","efcb59fe":"After plotting the data, we can change the aspect to the oob scores of models with different bin sizes.","997a7def":"The dash line in the plot represents the probablity of guessing without any data. The probability will be 0.5 because there are only two consequences: Survived or Dead.","e5c22939":"<a id='parch_sibsp'><\/a>\n## 3.3 Analysis of Parch and SibSp","c4e5e7cc":"#### Method 5: Keras model","9f836562":"<a id='connected'><\/a>\n## 4.5 Connected survival\nWe can assume that passengers with same Ticket are friends or families.","5e55c763":"<span style=\"font-size: 2.5em; font-weight: bold\">Titanic-Machine Learning from Disaster<\/span>","f1b6cb80":"About **bucketing\/binning data**, the followings are the reply from a kaggler(@foxale), which summarize the pros and cons.\n\n##### Pros\n* Bucketing categorical data reduces cardinality and sometimes makes it easier for the model to develop more general rules. For example when you try predicting product sales, instead of product name you can use product category and it should be able to generalize more.\n* It also reduces the amount of columns created during encoding. Less columns = smaller chance of overfitting.\n\n##### Cons\n* You lose information.\n\nIf you want to read the detail, please go to [here]('https:\/\/www.kaggle.com\/questions-and-answers\/254229') and do not be hesitated to share your opinions.","c08ba88d":"<a id='fname'><\/a>\n## 4.4 Family Name","5114228b":"#### Observations\n* Males had higher survival rate than females at port C.\n* Highest survival rate at C and lowest survival rate at S.\n* Most passengers embarked at port S.\n\n#### Decisions\n* The feature Embarked should be completed.\n* We should consider Embarked in our training model.","a04cb8aa":"ALthough we can obtain best parameters from GridSearchCV, the kaggle score seems to decline. In my opinion, we can only get the actual best parameters for the test data through submitting the prediction and checking the kaggle score.","7835d773":"<a id='import'><\/a>\n## 2.1 Import packages and modules","2b9e809c":"For the same reason as Fare, we can use the same method to bucket the Age feature and check if this helps to improve the model.","501bba06":"<a id='def_keras'><\/a>\n### 5.3.1 Define function for wrapping keras model","eec70955":"<a id='rfc'><\/a>\n## 5.2 Random forest classifier","f1998a7c":"<a id='farebin'><\/a>\n## 4.3 Bucketing\/Binning Fare data","98a07ea9":"<a id='keras_gridcv'><\/a>\n### 5.3.2 Conduct feature selection by GridSearchCV","f5505c82":"<a id='acquire'><\/a>\n## 2.2 Acquire data\nHere we use the Python Panda package to get the data as DataFrame objects. We read the .csv files to get the training and testing data, and then combine them into another DataFrame called **df_data**. Besides, we also create the DataFrame of the submission file to know how the submission form looks like.","3901973a":"<a id='process_age'><\/a>\n## 4.8 Age preprocessing","6bd11fa2":"##### What is the distribution of numercial features?\n* There are\n\n##### What is the distribution of categorical features?\n* All names in Name are unique value, that is, there is no passengers having the same name. (count=unique=891)\n* The Ticket has 681 unique values, which also states that there are 210 duplicates.","56794408":"<a id='bin_age'><\/a>\n### 4.8.3 Age data bucketing","03fd9a2f":"From the left graph, we can know that most of the missing values are in Pclass = 3. Therefore, if Age is an important feature, then our observation of Pclass = 3 might be incorrect. To avoid this, we can only observe age values in Pclass = 2 and 3. The right graph shows higher age-missing rate for males than females.","fe2d75ec":"<a id='define'><\/a>\n## 1.3 Question and problem definition\nThis project if from the competition [titanic data science problem]('https:\/\/www.kaggle.com\/c\/titanic'). Our target is to predict the survival status of passengers in the test.csv with given features.","d698893b":"# 0. Content\n* [1. Introduction](#Introduction)\n    * [1.1 Study materials](#materials)\n    * [1.2 Workflow stages](#workflow)\n    * [1.3 Question and problem definition](#define)\n* [2. Prepare environment and data](#prepare)\n    * [2.1 Import packages and modules](#import)\n    * [2.2 Acquire data](#acquire)\n    * [2.3 Check the content of data](#check)\n    * [2.4 Information from the data](#information)\n* [3. Exporatory Data Analysis (EDA)](#eda)\n    * [3.1 Analysis of Sex](#sex)\n    * [3.2 Analysis of Pclass](#pclass)\n    * [3.3 Analysis of Parch and Sibsp](#parch_sibsp)\n    * [3.4 Analysis of Fare](#fare)\n    * [3.5 Analysis of Embarked](#embarked)\n* [4. Feature engineering and data cleaning](#eng)\n    * [4.1 Derive Fsize(family size) from Parch and SibSp](#fsize)\n    * [4.2 Use IsAlone alternatively](#isalone)\n    * [4.3 Bucketing\/Binning Fare data](#farebin)\n    * [4.4 Family Name](#fname)\n    * [4.5 Connected survival](#connected)\n    * [4.6 Filling missing Embarked](#fill_embarked)\n    * [4.7 Extract Title from Name](#title)\n    * [4.8 Age preprocessing](#process_age)\n        * [4.8.1 Filling missing values](#fill_age)\n        * [4.8.2 Divide Age distribution into two with certain value](#divide_age)\n        * [4.8.3 Age data bucketing](#bin_age)\n* [5. Build models](#build)\n    * [5.1 Select features by correlation](#correlation)\n    * [5.2 Random forest classifier](#rfc)\n    * [5.3 Keras deep learning model](#keras)\n        * [5.3.1 Define function for wrapping keras model](#def_keras)\n        * [5.3.2 Conduct feature selection by GridSearchCV](#keras_gridcv)\n        * [5.3.3 Set best parameters and make prediction](#keras_pred)","79940bbf":"There are 177 and 86 missing values of Age in trainin data and testing data. Since we assume that Age is an important factor to Survived, we need to fill in the null values. It seems that several methods can help us to obtain (or guess) the values:\n* Just use the median or mean of Age to fill in all the null values.\n* Find existing features that are highly correlated with Age and proceed prediction to get the value. Here I will try five approches to do this. (maybe there is no need to do some of them?)\n    * Guess Age values using median values for Age across sets of feature combinations (manual way).\n    * Guess Age values using the Title of the passengers.\n    * Predict Age values with linear regression model.\n    * Filling null values with KNN imputer.\n    * Predict Age values with keras model (Deep Learning).","7463c2d6":"#### Observations\n* Fare has some outliers.\n* Higher Fare has higher survival rate.\n\n#### Decisions\n* We should consider Fare, but we should also bucketing it to avoid overfitting.","0cbffc8f":"<a id='eng'><\/a>\n# 4. Feature engineering and data cleaning","451e6444":"After getting the contents, we can conclude the following\n#### How many features and what does these features stand for?\n* PassengerId\n* Survived\n* Pclass (Ticket class)\n* Name\n* Sex\n* Age\n* SibSp (# of siblings \/ spouses aboard the Titanic)\n* Parch (# of parents \/ children aboard the Titanic)\n* Ticket\n* Fare\n* Cabin\n* Embarked (Port of Embarkation)\n\n#### How is our goal related to the data?\n* Survival is the goal of this work, so the testing data will not have this feature.\n\n#### Is there missing values in the features?\n* The two features **Age** and **Cabin** has a great number of missing values in both training and testing data, we will have to search a proper method to deal with this probelm in the process.\n* Other missing values are not that much, we can use the mean or median to fill the blanks.\n\n#### What is the type of the features?\n* Categorical features: Survived, Sex, Embarked, and Pclass(ordinal).\n* Numercial features: Age, Fare. Discrete: SibSp, Parch.","f473211f":"#### Method 2: Guess with age median from new feature - Title","ba659bc0":"<a id='prepare'><\/a>\n# 2. Prepare the environment and data","215582e6":"With (Pclass, Fare) = (1, 80), we can find that Embarked C with (1, 76.73) is relatively closed to this. So we can fill the null values with Embarked = C.","cff89b8d":"<a id='fsize'><\/a>\n## 4.2 Derive Fsize (family size) from Parch and SibSp","846220c0":"#### Observations\n* The number of male is greater than that of female.\n* Female is more easier to survive than male. (0.742 > 0.188)\n\n#### Decisions\n* We should consider Age feature in our training model.\n* We need to map the categorical data into numercial data later.","618b1352":"<a id='pclass'><\/a>\n## 3.2 Analysis of Pclass","ce56e9e9":"<a id='correlation'><\/a>\n## 5.1 Select features by correlation","35e46a8d":"<a id='fill_embarked'><\/a>\n## 4.6 Filling missing Embarked","be63d939":"#### Method 4: KNN Imputer","eabd7c8a":"<a id='sex'><\/a>\n## 3.1 Analysis of Sex","c0f3415a":"#### Method 1: Guess age median with Pclass and Sex","5ecd70f4":"#### Method 3: Linear regression in SKlearn","210f6b08":"With the plot, we can know that the Fare values are across a wide range, so in order to avoid overfitting, we can try to bucket this feature. However, how can we decide how many bins to use? ","8f511e44":"<a id='fill_age'><\/a>\n### 4.8.1 Filling missing values","42717a7e":"High survival rate appears when Age < 16. When Age > 16, the distribution is not that distinct. To conclude, we should find those missing values under 16 years old.","32b91188":"<a id='divide_age'><\/a>\n### 4.8.2 Divide Age distribution into two with certain value","8556fb13":"#### From the plot, we can know:\n* Bin size = 6 gets better accuracy at most of the time.\n* Bin size = 5 works better in Seed 4 and Seed 5.\n* Bin size = 4 only gets the best accuracy in Seed 0 and performs lowest accuracy at most of the time.\n* The graph indicates that bin size = 6 is the best choice.","c4914ad6":"<a id='eda'><\/a>\n# 3. Exporatory Data Analysis (EDA)","e6017274":"#### Assumptions based on data analysis\n**Correlating**\n\nWe want to know how well does these features correlate to Survival.\n\n**Completing**\n* The feature Age has to be completed since we want to assume that age definetly correlated to survival.\n* Although we're not sure how well does Embarked correlated to survival, the feature has only 2 null values, it can be fixed and then we can check the correlation.\n\n**Correcting**\n* We can assume that PassengerId does not contribute to Survival. For the same reason, Name can also be neglected.\n* The Cabin contains a great number of missing values in both training and testing data, so we can drop it instead of spending time filling the values.\n* 23% duplicates means that there are 23% passengers having same ticket number with at least one of other passengers. We may not directly use Ticket to train the model. Instead, we should think of some features related with Ticket. ","e495ce7a":"<a id='embakred'><\/a>\n## 3.5 Analysis of Embarked","f4e7ba6c":"<a id='workflow'><\/a>\n## 1.2 Workflow stages\n1. Question or problem definition.\n2. Acquire training and testing data.\n3. Wrangle, prepare, cleanse the data.\n4. Analyze, identify patterns, and explore the data.\n5. Model, predict and solve the problem.\n6. Visualize, report, and present the problem solving steps and final solution.\n7. Supply or submit the results.\n\nReference: [Titanic data science solution]('https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions')","b0e71d42":"<a id='fare'><\/a>\n## 3.4 Analysis of Fare","423ca70c":"<a id='title'><\/a>\n## 4.7 Extract Title from Name","5cd6edaa":"<a id='isalone'><\/a>\n## 4.3 Use IsAlone alternatively","8ea41376":"##### Observations\n* High dead rates between 20 to 30 years old.\n* High survival rates under 15 years old. It's reasonable to assume that they are the priority to be secured.","bb1f14ac":"This is my first EDA (Exploratory Data Analysis) project. If you like my work or want to encourage me to keep moving on, please **UPVOTE** this notebook and let me know!\n\nBesides, I really recommand any beginner to take a look at this [**notebook**]('https:\/\/www.kaggle.com\/tarunpaparaju\/titanic-competition-how-top-lb-got-their-score') and do not be misled by the 100% scores on leaderboard while enjoying the journey of data science :)","48fe8068":"#### Observations\n* The better Pclass makes it more possible to survive. (0.629 > 0.472 > 0.242)\n\n#### Decisions\n* We should consider this feature in our training model.","3688e937":"<a id='keras'><\/a>\n## 5.3 Keras deep learning model"}}