{"cell_type":{"9b6c24db":"code","5a6997e7":"code","3a224f5c":"code","22ec3901":"code","520a68e0":"code","299530a9":"code","692dd3d5":"code","ff59d679":"code","e5e1a0a5":"markdown","6cfd0d84":"markdown","1c78181a":"markdown","e475564b":"markdown","1b093837":"markdown","d4dfd633":"markdown","5fde6bd1":"markdown","98712e46":"markdown","3a7fe1b9":"markdown","459e3232":"markdown"},"source":{"9b6c24db":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntrain_survivors = train_data.loc[train_data.Survived == 1].drop(columns=[\"Survived\"])\ntrain_non_survivors = train_data.loc[train_data.Survived == 0].drop(columns=[\"Survived\"])\n\n# Handle missing values\ntrain_data[\"Age\"].fillna(train_data[\"Age\"].mean(), inplace = True)\ntest_data[\"Age\"].fillna(test_data[\"Age\"].mean(), inplace = True)\ntest_data[\"Fare\"].fillna(test_data[\"Fare\"].mean(), inplace = True)","5a6997e7":"axes = plt.subplots(1, 4, figsize=(20, 5))[1]\nsns.histplot(data=train_data, x=\"Pclass\", hue=\"Survived\", multiple=\"stack\", stat=\"probability\", ax=axes[0])\nsns.histplot(data=train_data, x=\"Age\", hue=\"Survived\", multiple=\"stack\", stat=\"probability\", ax=axes[1])\nsns.histplot(data=train_data, x=\"SibSp\", hue=\"Survived\", multiple=\"stack\", stat=\"probability\", ax=axes[2])\nsns.histplot(data=train_data, x=\"Parch\", hue=\"Survived\", multiple=\"stack\", stat=\"probability\", ax=axes[3])\n\naxes = plt.subplots(1, 1, figsize=(20, 5))[1]\nsns.histplot(data=train_data, x=\"Fare\", hue=\"Survived\", multiple=\"stack\", stat=\"probability\")\n\ntrain_data.describe()","3a224f5c":"train_survivors.describe()","22ec3901":"train_non_survivors.describe()","520a68e0":"from sklearn.feature_selection import mutual_info_classif\n\n# Heavily inspired by Ryan Holbrook's \"Feature Engineering\" Kaggle course\ndef make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_classif(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nX = train_data.copy()\ny = X.pop(\"Survived\")\n\n# This might mess up some assumptions because of encoding, I have no idea what to do about it right now\nfor colname in X.select_dtypes(\"object\"):\n    X[colname], _ = X[colname].factorize()\n    \ndiscrete_features = X.dtypes == int\n\nmi_scores = make_mi_scores(X, y, discrete_features)\nmi_scores","299530a9":"features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n\nX_train = pd.get_dummies(train_data[features])\ny_train = train_data.Survived\n\nX_test = pd.get_dummies(test_data[features])","692dd3d5":"from sklearn.preprocessing import scale\n\nX_train = scale(X_train)\nX_test = scale(X_test)","ff59d679":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# Definition\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)\n\n# Fitting\nmodel.fit(X_train, y_train)\n\n# Validation\nscore = cross_val_score(\n    model, X_train, y_train, cv=5, scoring=\"accuracy\"\n)\nscore = score.mean()\nprint(f\"Validation Score: {score:.4}\")\n\n# Prediction\npredictions = model.predict(X_test)\n\n# Submission file creation\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('kaggle_submission.csv', index=False)\nprint(\"Submission file was successfully saved!\")","e5e1a0a5":"# Imports and Data","6cfd0d84":"# Mutual Information\nIn this section, we'll take a look at the mututal information between all given features and passenger survival rate.","1c78181a":"# Feature Extraction","e475564b":"# TODO\n* Figure out Mutual Information and draw the right conclusions.\n* Engineer new and helpful features.","1b093837":"# Feature Scaling","d4dfd633":"# Data Visualization\nIn this section, I'll visualize the data, and draw conclusions which may lead to better design.\n## Data Description","5fde6bd1":"## The Non Survivors","98712e46":"# Model Definition, Fitting, Validation and Prediction","3a7fe1b9":"So, in the training data:\n* Most 3rd class passengers didn't survive.\n* Most passengers with no siblings \/ spouses didn't survive.\n* Most passengers without parents \/ children didn't survive.\n* The lower the fare, the less likely a passenger has survived.","459e3232":"## The Survivors"}}