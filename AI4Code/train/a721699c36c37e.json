{"cell_type":{"511903a3":"code","99696223":"code","641bfa19":"code","34724542":"code","aa42b81a":"code","bcbf90cb":"code","1bd134fe":"code","8d3fe5d7":"code","ed68a7c7":"code","bd42b0c0":"code","602f3519":"code","8ae6ff7d":"code","1500eab9":"code","c42fdcc6":"code","8c6945bc":"code","be900e9d":"code","202c53df":"code","0aa1b075":"code","126833b8":"code","3ad21277":"code","79b1a156":"code","3facc7b2":"code","d34ce455":"code","0cb70815":"code","a890b5a7":"code","813b845c":"code","7b1d396a":"code","1141d124":"code","d5d85d0f":"code","80679364":"code","516746b2":"code","edec01c1":"code","89ae894a":"code","1f1335fd":"code","9c7c75ee":"code","bd05ec6f":"code","6a096a31":"code","2c522e35":"code","904cc2d0":"code","80a92cac":"code","96b08f74":"code","8a89a353":"code","d3c11a23":"code","d8be4d63":"code","6eb5818d":"code","268032b8":"code","79323753":"code","80017a9e":"code","2beadf1d":"code","e42325e8":"code","3a25c05d":"code","95636e83":"code","dd0ece6b":"markdown","78971af7":"markdown","0058cc5d":"markdown","51d9701a":"markdown","c37b2ed3":"markdown","c9844c29":"markdown","f35e4f83":"markdown","2601e5d6":"markdown","38538e9f":"markdown","f3397cda":"markdown","682de445":"markdown","04ebcd2f":"markdown","d3db4dfc":"markdown","36440399":"markdown","e987d865":"markdown","a818d02f":"markdown","9f59823b":"markdown","f837b3e6":"markdown","4798dd75":"markdown","6fca8309":"markdown","d54a538d":"markdown","b62d3e75":"markdown","a47e7f28":"markdown","497453f8":"markdown","eb266e81":"markdown","73e11b79":"markdown","c3ade411":"markdown","d9cd232c":"markdown","7e47174d":"markdown","8a936318":"markdown","a3fe8408":"markdown","e2a1e784":"markdown","4a347ca5":"markdown","2d5f6161":"markdown","4f344143":"markdown"},"source":{"511903a3":"# Import all the necessary libraries \n\n# commonly used libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# visualization library\nimport seaborn as sns\n\n# data manipulation utility libraries\nimport distutils\nimport datetime\nimport re\n\n# sklearn libraries\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error\n\n# stats library\nfrom scipy import stats\nfrom scipy.stats import boxcox\n\n\n","99696223":"# Import the data\ndf = pd.read_csv('..\/input\/airbnb-amsterdam\/listings_details.csv')","641bfa19":"#Look at top rows of dataframe\ndf.head()\n","34724542":"#Quick summary of dataframe\ndf.describe()\n","aa42b81a":"#number of rows and columns\ndf.shape\n","bcbf90cb":"#sum of NaN values in price column\nnp.sum(df.price.notnull())","1bd134fe":"# price column\ndf.price","8d3fe5d7":"# check out all columns with numeric values\nnum_vars = df.select_dtypes(include=['float', 'int']).columns\nnum_vars","ed68a7c7":"# check out all columns with categorical values\nnum_cat = df.select_dtypes(include=['object']).columns\n\nnum_cat","bd42b0c0":"#check values in different neighbourhood columns\ndf[['neighbourhood','neighborhood_overview','neighbourhood_cleansed']]","602f3519":"# see ratio of categorical values \ndf.neighbourhood_cleansed.value_counts() \/ df.shape[0]","8ae6ff7d":"# drop columns that are irelevant \ndf_clean = df.drop(['id', 'scrape_id', 'thumbnail_url', 'medium_url', 'xl_picture_url',\n              'host_id', 'host_total_listings_count', 'neighbourhood_group_cleansed',\n              'latitude','longitude', 'calculated_host_listings_count', \n              'listing_url', 'last_scraped', 'name', 'summary', 'space',\n              'description', 'experiences_offered', 'neighborhood_overview', 'notes',\n              'transit', 'access', 'interaction', 'house_rules', 'picture_url',\n              'host_url', 'host_name', 'host_location', 'host_about',\n              'host_thumbnail_url', 'host_picture_url', 'host_neighbourhood','host_verifications',\n              'street', 'neighbourhood', 'city', 'state',\n              'zipcode', 'market', 'smart_location', 'country_code', 'country',\n              'weekly_price', 'monthly_price','security_deposit', 'cleaning_fee',\n              'extra_people', 'calendar_last_scraped', 'requires_license', 'license',\n              'jurisdiction_names','guests_included','host_response_time','host_response_rate',\n              'host_acceptance_rate','square_feet'\n             ], axis=1)\n\n","1500eab9":"# drop all NaN values\ndf_clean = df_clean.dropna()","c42fdcc6":"# use string.replace to get rid of string items in price column\ndf_clean = df_clean.assign(price=df_clean['price'].str.replace(r'$', ''))\ndf_clean = df_clean.assign(price=df_clean['price'].str.replace(r',', ''))\n\n#Set price as float type\ndf_clean['price'] = df_clean['price'].astype(float)","8c6945bc":"# use lambda and distutils to go from string to boolean expression\ndf_clean = df_clean.assign(host_is_superhost=df_clean['host_is_superhost'].apply(lambda x: bool(distutils.util.strtobool(x))))\ndf_clean = df_clean.assign(host_has_profile_pic=df_clean['host_has_profile_pic'].apply(lambda x: bool(distutils.util.strtobool(x))))\ndf_clean = df_clean.assign(host_identity_verified=df_clean['host_identity_verified'].apply(lambda x: bool(distutils.util.strtobool(x))))\ndf_clean = df_clean.assign(is_location_exact=df_clean['is_location_exact'].apply(lambda x: bool(distutils.util.strtobool(x))))\ndf_clean = df_clean.assign(instant_bookable=df_clean['instant_bookable'].apply(lambda x: bool(distutils.util.strtobool(x))))\ndf_clean = df_clean.assign(is_business_travel_ready=df_clean['is_business_travel_ready'].apply(lambda x: bool(distutils.util.strtobool(x))))\ndf_clean = df_clean.assign(require_guest_profile_picture=df_clean['require_guest_profile_picture'].apply(lambda x: bool(distutils.util.strtobool(x))))\ndf_clean = df_clean.assign(require_guest_phone_verification=df_clean['require_guest_phone_verification'].apply(lambda x: bool(distutils.util.strtobool(x))))\n","be900e9d":"# find amenity availability by amenity\ndf_clean = df_clean.assign(has_tv=df_clean['amenities'].apply(lambda x: x.find('Wifi') != -1))\ndf_clean = df_clean.assign(has_fireplace=df_clean['amenities'].apply(lambda x: x.find('Indoor fireplace') != -1))\ndf_clean = df_clean.assign(has_kitchen=df_clean['amenities'].apply(lambda x: x.find('Kitchen') != -1))\ndf_clean = df_clean.assign(has_family_friendly=df_clean['amenities'].apply(lambda x: x.find('Family\/kid friendly') != -1))\ndf_clean = df_clean.assign(has_host_greeting=df_clean['amenities'].apply(lambda x: x.find('Host greets you') != -1))\ndf_clean = df_clean.assign(has_24hrs_checkin=df_clean['amenities'].apply(lambda x: x.find('24-hour check-in') != -1))\ndf_clean = df_clean.assign(has_breakfast=df_clean['amenities'].apply(lambda x: x.find('Breakfast') != -1))\ndf_clean = df_clean.assign(has_pets=df_clean['amenities'].apply(lambda x: x.find('Pets live on this property') != -1))\ndf_clean = df_clean.assign(has_dishwasher=df_clean['amenities'].apply(lambda x: x.find('Dishwasher') != -1))\ndf_clean = df_clean.assign(has_private_entrance=df_clean['amenities'].apply(lambda x: x.find('Private entrance') != -1))\ndf_clean = df_clean.assign(has_patio_balcony=df_clean['amenities'].apply(lambda x: x.find('Patio or balcony') != -1))\ndf_clean = df_clean.assign(has_self_checkin=df_clean['amenities'].apply(lambda x: x.find('Self check-in') != -1))\ndf_clean = df_clean.assign(has_workspace=df_clean['amenities'].apply(lambda x: x.find('Laptop friendly workspace') != -1))\ndf_clean = df_clean.assign(has_bathtub=df_clean['amenities'].apply(lambda x: x.find('Bathtub') != -1))\ndf_clean = df_clean.assign(has_longterm=df_clean['amenities'].apply(lambda x: x.find('Long term stays allowed') != -1))\ndf_clean = df_clean.assign(has_parking=df_clean['amenities'].apply(lambda x: x.find('Free parking on premises') != -1))\ndf_clean = df_clean.assign(has_garden=df_clean['amenities'].apply(lambda x: x.find('Garden or backyard') != -1))\n\n# drop amenities column\ndf_clean = df_clean.drop(['amenities'],axis=1)","202c53df":"# create days delta calculation function\nday_calc = lambda x: (datetime.date.today() - datetime.datetime.strptime(x, \"%Y-%m-%d\").date()).days\n\n# apply on host_since column\ndf_clean = df_clean.assign(host_since=df_clean['host_since'].apply(day_calc))","0aa1b075":"#drop columns that will not add more value than host since column\ndf_clean = df_clean.drop(['first_review','last_review'],axis=1)","126833b8":"# max price\ndf_clean.price.max()","3ad21277":"# ratio of occurrences of value by certain column\ndf_clean.number_of_reviews.value_counts() \/ df_clean.shape[0]","79b1a156":"# ratio of occurrences of value by certain column\ndf_clean.minimum_nights.value_counts() \/ df_clean.shape[0]\n    ","3facc7b2":"# check boxplot for price\nsns.boxplot(x=df_clean['price'])\n","d34ce455":"# create mod_z function (copied from: https:\/\/stackoverflow.com\/questions\/58127935\/how-to-calculate-modified-z-score-and-iqr-on-each-column-of-the-dataframe)\ndef mod_z(col: pd.Series, thresh: float=3.5) -> pd.Series:\n    med_col = col.median()\n    med_abs_dev = (np.abs(col - med_col)).median()\n    mod_z = 0.7413 * ((col - med_col) \/ med_abs_dev)\n    mod_z = mod_z[np.abs(mod_z) < thresh]\n    return np.abs(mod_z)\n\n# run mod_z function on dataframe\ndf_mod_z = df_clean.select_dtypes(include=[np.number]).apply(mod_z)","0cb70815":"#Apply above function to price \ndf_clean_filtered = df_clean[df_mod_z['price'] >= 0]\ndf_clean_filtered = df_clean_filtered[df_clean_filtered['price'] > 0]\n\n#check shape\ndf_clean_filtered.shape","a890b5a7":"# check summary\ndf_clean_filtered.describe()","813b845c":"#Cut outliers\ndf_clean_filtered = df_clean_filtered[df_clean_filtered['host_listings_count'] < 60]\ndf_clean_filtered = df_clean_filtered[df_clean_filtered['bathrooms'] < 20]\ndf_clean_filtered = df_clean_filtered[df_clean_filtered['beds'] < 20]\ndf_clean_filtered = df_clean_filtered[df_clean_filtered['minimum_nights'] < 365]\ndf_clean_filtered = df_clean_filtered[df_clean_filtered['maximum_nights'] < 2000]\n\n#Check shape\ndf_clean_filtered.shape","7b1d396a":"# drop some  because they're too tricky \ndf_clean_filtered_drop = df_clean_filtered.drop(['calendar_updated','has_availability'],axis=1)","1141d124":"# create dummy in dataframe function\ndef create_dummy_df(df, cat_cols, dummy_na):\n    '''\n    INPUT:\n    df - pandas dataframe with categorical variables you want to dummy\n    cat_cols - list of strings that are associated with names of the categorical columns\n    dummy_na - Bool holding whether you want to dummy NA vals of categorical columns or not\n    \n    OUTPUT:\n    df - a new dataframe that has the following characteristics:\n            1. contains all columns that were not specified as categorical\n            2. removes all the original columns in cat_cols\n            3. dummy columns for each of the categorical columns in cat_cols\n            4. if dummy_na is True - it also contains dummy columns for the NaN values\n            5. Use a prefix of the column name with an underscore (_) for separating \n    '''\n    for col in  cat_cols:\n        try:\n            # for each cat add dummy var, drop original column\n            df = pd.concat([df.drop(col, axis=1), pd.get_dummies(df[col], prefix=col, prefix_sep='_', drop_first=True, dummy_na=dummy_na)], axis=1)\n        except:\n            continue\n    return df","d5d85d0f":"#Set categorical columns to be dummied\ncat_cols_lst = df_clean_filtered_drop.select_dtypes(include=['object']).columns\n\n#Apply create dummy function\ndf_model = create_dummy_df(df_clean_filtered_drop, cat_cols_lst, dummy_na=False) #Use your newly created function\n\n#check df\ndf_model","80679364":"#plot distribution of Price\nplt.figure(figsize=(15,10))\nplt.tight_layout()\nsns.distplot(df_model['price'])","516746b2":"#Import Normalizer from sklearn\nfrom sklearn.preprocessing import Normalizer \n\n#Set variables to normalize\nnorm_vars = ['beds','bedrooms','accommodates']\n\n# initiate normalizer and apply to scaled data array\nnormalize = Normalizer().fit(df_model[norm_vars])\nnorm_array = normalize.transform(df_model[norm_vars])\n\n# create a DataFrame from the array\ndf_model_norm_vars = pd.DataFrame(norm_array, columns = norm_vars, index = df_model.index)\n\n# merge new DataFrame with full dataframe model \ndf_model_merged = pd.merge(df_model_norm_vars,df_model.drop(norm_vars,axis=1), right_index=True, left_index=True)\n\n# create combined column for beds, bedrooms and acommodates\ndf_model_merged['combine_beds_bedrooms_acommodates'] = df_model_merged['beds'] + df_model_merged['bedrooms'] + df['accommodates']\n\n# drop already combined variables\ndf_model_merged = df_model_merged.drop(['beds','bedrooms','accommodates'],axis=1)","edec01c1":"# Histograms of all vars with power transformation BoxCox\n\n# set vars to check\ncheck_vars = ['host_since', 'host_listings_count', 'bathrooms',\n       'price', 'minimum_nights', 'maximum_nights',\n       'number_of_reviews', 'review_scores_rating',\n       'review_scores_communication', 'review_scores_location',\n       'review_scores_value','combine_beds_bedrooms_acommodates',\n           'reviews_per_month','availability_30', 'availability_60','availability_90', 'availability_365',\n            'review_scores_accuracy','review_scores_cleanliness', 'review_scores_checkin']\n\n# For loop on showing separate histograms per item\ni = 0\nfor x in check_vars:\n    # set data\n    data = df_model_merged[x]\n    \n    # plot\n    plt.figure(i)\n    plt.title(x)\n    plt.hist(data)\n    print(plt.figure(i))\n    \n    # iterate \n    i = i + 1\n    \n\n","89ae894a":"# list of variables that need to be transformed to fit a normal distribution\n\nto_check_vars = ['price','host_listings_count','bathrooms','minimum_nights','maximum_nights','number_of_reviews','review_scores_rating',\n            'review_scores_communication','review_scores_location','review_scores_value','review_scores_accuracy',\n             'review_scores_cleanliness','review_scores_checkin',\n             'combine_beds_bedrooms_acommodates',\n            'reviews_per_month'\n           ]\n\n\npoly_vars = ['availability_30','availability_60','availability_90','availability_365']","1f1335fd":"# For loop on showing separate histograms per item\ni = 0\nfor x in to_check_vars:\n    plt.figure(i)\n    plt.title(x)\n    \n    # power transform\n    data = df_model_merged[x] + 1\n    data = boxcox(data)\n    \n    #print the boxcox lambda value\n    print(data[1])\n    \n    #plot the graph\n    plt.hist(data)\n    print(plt.figure(i))\n    \n    #increment the counter\n    i = i + 1","9c7c75ee":"# variables to keep and boxcox transform \nboxcox_vars = ['price','number_of_reviews','review_scores_rating',\n            'review_scores_location','review_scores_value'\n              ,'combine_beds_bedrooms_acommodates']\n\n\n# variables to drop\nto_drop = ['host_listings_count','bathrooms','minimum_nights','maximum_nights',\n           'review_scores_communication','review_scores_accuracy','review_scores_cleanliness',\n           'review_scores_checkin','reviews_per_month']","bd05ec6f":"#drop variables\ndf_model_merged = df_model_merged.drop(to_drop,axis=1)\n\n#drop poly variables\ndf_model_merged = df_model_merged.drop(poly_vars,axis=1)\n","6a096a31":"# create boxcox in dataframe function\ndef create_boxcox_df(df, boxcox_vars):\n    '''\n    INPUT:\n    df - pandas dataframe with categorical variables you want to dummy\n    boxcox_vars - list of strings that are associated with selected boxcox columns\n    \n    OUTPUT:\n    df - a new dataframe that has the following characteristics:\n            1. contains all columns that were not specified as boxcox columns\n            2. removes all the original columns in boxcox_vars\n            3. boxcox transforms for each of the boxcox columns in boxcox_vars\n            4. returns df\n            5. returns list of lambda values for maxlog()\n    '''\n    #iniate empty list\n    lambda_list = list()\n    \n    #start for loop\n    for col in boxcox_vars:\n        try:\n            # for each var boxcox transform\n            data = df[col] +1\n            data = boxcox(data)\n            \n            #lambda list append\n            lambda_list.append(data[1])\n            \n            #create dataframe from array\n            df_insert = pd.DataFrame(data[0],columns = [col],index = df_model_merged.index)\n            \n            #concat dataframes\n            df = pd.merge(df_insert,df.drop(col, axis=1), right_index=True, left_index=True)\n\n            \n        except:\n            continue\n    return df, lambda_list","2c522e35":"# apply to dataframe\ndf_model_merged, lambda_list = create_boxcox_df(df_model_merged,boxcox_vars=boxcox_vars)\n","904cc2d0":"# histograms of the variables\ndf_hist = df_model_merged[boxcox_vars]\n\ndf_hist.hist()\nprint(plt.show())","80a92cac":"# performing preprocessing part \nfrom sklearn.preprocessing import StandardScaler\n\n# take all the variables that we want to standardize \nscaler_vars = ['number_of_reviews','review_scores_rating',\n            'review_scores_location','review_scores_value'\n              ,'combine_beds_bedrooms_acommodates']\n\n\n# initiate standardscaler and apply to data\nsc = StandardScaler()\nscaled_array = sc.fit_transform(df_model_merged[scaler_vars])\n\n# create new dataframe with scaled variables\ndf_model_scaled = pd.DataFrame(scaled_array, columns = scaler_vars, index = df_model.index)\n\n# merge them all back together\ndf_model_merged = pd.merge(df_model_scaled,df_model_merged.drop(scaler_vars,axis=1), right_index=True, left_index=True)\n\n#print(df_model_merged.describe())","96b08f74":"# histograms of the variables\ndf_hist = df_model_merged[boxcox_vars]\n\ndf_hist.hist()\nprint(plt.show())","8a89a353":"# histograms of the variables\ndf_hist = df_model_merged[boxcox_vars]\n\ndf_hist.hist()\nplt.show\n\n","d3c11a23":"num_vars = ['host_since','price','number_of_reviews', 'review_scores_rating', 'review_scores_location','review_scores_value']\n\n#df_plot = df_model_merged.select_dtypes(include=[np.number])\ndf_plot = df_model_merged[num_vars]\n\nmatrix = np.triu(df_plot.corr())\n\nplt.figure(figsize=(12, 9))\nsns.heatmap(df_plot.corr(), annot=False, mask=matrix, linewidths=.5, fmt='.1f')","d8be4d63":"# based on above correlation matrix, take out a few columns\ndf_model_merged = df_model_merged.drop(['review_scores_value'],axis=1)\n\n","6eb5818d":"def fit_linear_mod(df, response_col, test_size=.3, rand_state=42):\n    '''\n    INPUT:\n    df - a dataframe holding all the variables of interest\n    response_col - a string holding the name of the column\n    test_size - a float between [0,1] about what proportion of data should be in the test dataset\n    rand_state - an int that is provided as the random state for splitting the data into training and test \n    \n    OUTPUT:\n    test_score - float - r2 score on the test data\n    train_score - float - r2 score on the test data\n    lm_model - model object from sklearn\n    X_train, X_test, y_train, y_test - output from sklearn train test split used for optimal model\n    \n    This function should:\n    1. Split your data into an X matrix and a response vector y\n    2. Create training and test sets of data\n    3. Instantiate a LinearRegression model with normalized data\n    4. Fit your model to the training data\n    5. Predict the response for the training data and the test data\n    6. Obtain an rsquared value for both the training and test data\n    '''\n\n    #Split into explanatory and response variables\n    X = df.drop(response_col, axis=1)\n    y = df[response_col]\n\n    #Split into train and test\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=rand_state)\n\n    lm_model = LinearRegression(normalize=True) # Instantiate\n    lm_model.fit(X_train, y_train) #Fit\n\n    #Predict using your model\n    y_test_preds = lm_model.predict(X_test)\n    y_train_preds = lm_model.predict(X_train)\n\n    #Score using your model\n    test_score = r2_score(y_test, y_test_preds)\n    train_score = r2_score(y_train, y_train_preds)\n\n    return test_score, train_score, lm_model, X_train, X_test, y_train, y_test, y_test_preds, y_train_preds","268032b8":"\n#Test your function with the above dataset\ntest_score, train_score, lm_model, X_train, X_test, y_train, y_test, y_test_preds, y_train_preds = fit_linear_mod(df_model, 'price')\n\n#Print training and testing score\nprint(\"The rsquared on the training data was {}.  The rsquared on the test data was {}.\".format(train_score, test_score))\n\n# The coefficients\n#print('Coefficients: \\n', lm_model.coef_)\n\n# print RMSE\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_test_preds)))","79323753":"# after preprocessing\n\n#Test your function with the above dataset\ntest_score, train_score, lm_model, X_train, X_test, y_train, y_test, y_test_preds, y_train_preds = fit_linear_mod(df_model_merged, 'price')\n\n#Print training and testing score\nprint(\"The rsquared on the training data was {}.  The rsquared on the test data was {}.\".format(train_score, test_score))\n\n# The coefficients\n#print('Coefficients: \\n', lm_model.coef_)\n\n# print RMSE\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_test_preds)))","80017a9e":"# Applying PCA function on training \nfrom sklearn.decomposition import PCA \n  \npca = PCA()\n  \nX_train = pca.fit_transform(X_train) \n  \nexplained_variance = pca.explained_variance_ratio_\nprint(explained_variance)","2beadf1d":"#import statsmodel library\nimport statsmodels.api as sm\n\n#set response column and df to use\nresponse_col = 'price'\ndf_to_use_ols = df_model_merged\n\n# set X matrix and y \nX = df_to_use_ols.drop(response_col, axis=1)\nX = sm.add_constant(X)\ny = df_to_use_ols[response_col]\n\n# fit and predict\nest = sm.OLS(y.astype(float), X.astype(float)).fit()\nypred = est.predict(X)\n\n# evaluate\nrmse = np.sqrt(mean_squared_error(y, ypred))\nprint(rmse)\n\n# show stats summary\nest.summary()\n","e42325e8":"from sklearn.ensemble import RandomForestRegressor\n\nregr = RandomForestRegressor(max_depth=2, random_state=0)\n\nresponse_col = 'price'\ndf_rfr_to_use = df_model_merged\n\nX = df_rfr_to_use.drop(response_col, axis=1)\ny = df_rfr_to_use[response_col]\nregr.fit(X, y)\nmodelPred = regr.predict(X)\n\nprint(\"The R2 score: \",regr.score(X,y))\n\nprint(\"Number of predictions:\",len(modelPred))\n\nmeanSquaredError=mean_squared_error(y, modelPred)\nprint(\"MSE:\", meanSquaredError)\nrootMeanSquaredError = np.sqrt(meanSquaredError)\nprint(\"RMSE:\", rootMeanSquaredError)\n","3a25c05d":"#https:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/automl.html\n# tutorial: https:\/\/github.com\/h2oai\/h2o-tutorials\/blob\/master\/h2o-world-2017\/automl\/Python\/automl_binary_classification_product_backorders.ipynb\n\nimport h2o\nfrom h2o.automl import H2OAutoML\nh2o.init()\n\ndf_h2o_to_use = df_model_merged\n\n# Identify predictors and response\nx = df.columns.tolist()\ny = \"price\"\nx = x.remove(y)\n\ndf_model_h2o = h2o.H2OFrame(df_h2o_to_use)\n","95636e83":"# Run AutoML for 20 base models (limited to 1 hour max runtime by default)\naml = H2OAutoML(max_models=20, seed=1)\naml.train(x=x, y=y, training_frame=df_model_h2o)\n\n# View the AutoML Leaderboard\nlb = aml.leaderboard\nlb.head(rows=lb.nrows)  # Print all rows instead of default (10 rows)","dd0ece6b":"# Notebook on Predicting Amsterdam Airbnb listing price","78971af7":"## Check normal distribution of Response variable","0058cc5d":"## Identify usefull amenities","51d9701a":"Drop a few final columns\n","c37b2ed3":"# Preprocessing","c9844c29":"# PCA analysis\n","f35e4f83":"# Implement Statsmodels OLS","2601e5d6":"# Check for Multicollinearity","38538e9f":"# Use dataframe after preprocessing","f3397cda":"# Standardization with StandardScaler","682de445":"# #Drop all NaN values ","04ebcd2f":"# Boxcox transform and check outputs","d3db4dfc":"## Cut outliers with hardcoded parameter","36440399":"## Get rid of string items in Price column","e987d865":"# Use dataframe before preprocessing","a818d02f":"Opinions are my own.\n\nThis notebook has served as a playing ground for me to explore the Airbnb Amsterdam data and try to predict listing prices through several different methods, including:\n\n* Linear Regression\n* Random Forrest Regression\n* OLS\n* H2O Auto ML\n","9f59823b":"# Mark variables that can fit a normal distribution and get rid a few others","f837b3e6":"# Implement RandomForestRegressor","4798dd75":"# Create dummy variables\n","6fca8309":"## Normalize and combine ","d54a538d":"* Normalize some columns to combine them\n* Transform some data to create normal distribution of input variables\n* Standardization using StandardScaler","b62d3e75":"# Modeling\n\n* Implement Sklearn Linear Regression\n* Refine with some PolyNomial Features\n* Implement Sklearn RandomForestRegressor\n* Implement Statsmodels OLS\n* Implement H2O AutoML\n","a47e7f28":"## Cut outliers for Price using Mod-z ","497453f8":"# Boxcox transform needed variables","eb266e81":"# **Data Preparation**\n\n* Dropping many columns\n* Cleaning some data quality issues\n* Cutting outliers based on mod-z\n* Create dummy variables","73e11b79":"## Getting up and running\nImport all the libaries","c3ade411":"# Data exploration","d9cd232c":"## Use datetime calculation to get days metric for Host Since","7e47174d":"# Look at all adapted distributions","8a936318":"# Implement H2O AutoML solution","a3fe8408":"# Take out variables where needed to avoid multicollinearity","e2a1e784":"## Get boolean expression from 'f'& 't' string","4a347ca5":"## Drop some more columns","2d5f6161":"# # Check normal distribution of all numerical input variables ","4f344143":"## Check outliers"}}