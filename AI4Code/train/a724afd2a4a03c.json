{"cell_type":{"07f558f1":"code","9451d6eb":"code","28f1d52a":"code","43f74ab6":"code","9d76db58":"code","191f0588":"code","66c06be7":"code","54680262":"code","9b73299a":"code","3419f0d2":"code","b9741209":"code","9936cc66":"code","2b13ff00":"code","79f06d63":"code","56cafa9b":"code","318bd813":"code","4d5de714":"code","cbf27b8f":"code","ba73dcc6":"code","74b4de75":"code","f7fb778e":"code","dac5d844":"code","615a7355":"code","d5a7722d":"code","5e2c4731":"code","b00eda40":"code","819f6d0d":"code","aaee9508":"code","e988f0df":"code","fbe9061f":"code","72040250":"code","8c53cd69":"code","4d9e3c93":"code","68f587b8":"code","f0f18e13":"code","899de521":"code","e0b8dd28":"code","9f1f734a":"code","18213e60":"code","af994c85":"code","5c25095c":"code","2f4f25c4":"code","68e18dc9":"code","627874ae":"code","b7847c91":"code","53b26d88":"code","79271a7f":"code","eb0c8846":"code","06a3a522":"code","f5bfc086":"code","5078bf21":"code","e4c26c14":"code","66999a00":"code","5b9bc441":"code","d139001b":"code","578e1e88":"code","37bcdbfb":"code","4bacd35a":"code","6ba6d40a":"markdown","36d2db96":"markdown","1fb1175d":"markdown","bd64c493":"markdown","13297d10":"markdown","408b7f05":"markdown","476a19d0":"markdown","885d8166":"markdown","b05a6474":"markdown","a458fe15":"markdown","38482ed0":"markdown","88c11968":"markdown","a4c9c0ef":"markdown","971d850f":"markdown","99efe0ac":"markdown","3aa6c91c":"markdown","68c03562":"markdown","18ef2b18":"markdown","ff5cc938":"markdown","4bf6eec4":"markdown","46a59f24":"markdown","4fc9fc12":"markdown","dbca576d":"markdown","504168e9":"markdown","5a2fdbaa":"markdown","691f9842":"markdown","134b05cf":"markdown","a468b569":"markdown","68c7635b":"markdown","5b08be05":"markdown","65354386":"markdown","e3acddc2":"markdown","f44e132a":"markdown","c0a69fdf":"markdown","acb3c993":"markdown"},"source":{"07f558f1":"# !kaggle competitions download -c bluebook-for-bulldozers\n# !unzip bluebook-for-bulldozers.zip\n# !rm *.zip *.7z","9451d6eb":"# Data manipulations libraries\nimport pandas as pd\nimport numpy as np\n \n# Data visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n \n# Machine learning imports\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error, r2_score, mean_squared_log_error\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n \n# Misc\nimport joblib","28f1d52a":"INPUT_PATH = \"..\/input\/bluebook-for-bulldozers\/\"\nOUTPUT_PATH = \"\/kaggle\/working\"\n# INPUT_PATH = \".\/\"\n# OUTPUT_PATH = \".\/\"","43f74ab6":"# This is a utility function to display all columns and rows of a dataframe.\ndef display_all(df):\n    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n        display(df)","9d76db58":"# Load TrainAndValid.csv, parsing saledate as datetime\ntrain_df = pd.read_csv(INPUT_PATH + \"TrainAndValid.csv\",\n                       low_memory=False,\n                       parse_dates=[\"saledate\"])\ntrain_df.head().T","191f0588":"# Print information about the dataframe\ntrain_df.info()","66c06be7":"fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\nsns.distplot(train_df.SalePrice, label=\"SalePrice\", bins=50, ax=ax0, kde=True)\nax0.legend()\nsns.boxplot(x=train_df.SalePrice, ax=ax1)\nplt.show();","54680262":"# Describing the numerical data.\ndesc_df = train_df.describe().T\n\n# Add more useful information\ndesc_df[\"% non-null\"] = desc_df[\"count\"] \/ len(train_df)\n\ndesc_df","9b73299a":"# Ploting feature correlation matrix (only numerical features).\nfig, ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(train_df.corr(), annot=True, cmap=\"YlGnBu\", cbar=False, ax=ax)\nplt.show()","3419f0d2":"fig, (ax0, ax1) = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(15, 10))\n\n# Plotting the sell count for each year\nsns.countplot(x=\"YearMade\",\n              data=train_df,\n              palette=sns.color_palette(\"Blues_d\"),\n              ax=ax0)\nax0.set_ylabel(\"Number of Sales\")\n\n# Plotting the mean price for each year\nsns.barplot(x=\"YearMade\",\n            y=\"SalePrice\",\n            data=train_df,\n            palette=sns.color_palette(\"Blues_d\"),\n            ax=ax1)\nax1.set_ylabel(\"Sale Prices Mean\")\n\nplt.xticks(rotation=90)\nplt.show()","b9741209":"# Print the rate of unique values for MachineID\nn_uniques = len(train_df.MachineID.unique())\nuniques_rate = n_uniques \/ len(train_df)\nprint(f\"Number of unique MachineIDs: {n_uniques} -- Rate of uniques: {uniques_rate}\")","9936cc66":"train_df.plot.scatter(x=\"MachineID\", y=\"SalePrice\", c=\"datasource\", figsize=(15, 10));","2b13ff00":"# Describe non-numerical features\ndesc_df = train_df.describe(include=\"O\").T\n\n# Add more useful information.\ndesc_df[\"% non-null\"] = desc_df[\"count\"] \/ len(train_df)\n\ndesc_df","79f06d63":"fig, (ax0, ax1) = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(15, 10))\n\n# We added the missing values as as separate class for ilustration purposes.\nplt_data = train_df.fillna({\"Blade_Extension\": \"missing\"})\n\n# Plot the SalePrice distribution per class. \nsns.boxplot(x=\"Blade_Extension\", y=\"SalePrice\", data=plt_data, ax=ax0)\nax0.set(xlabel=\"\")\n\n# Plot the class counts.\nsns.countplot(x=\"Blade_Extension\", data=plt_data, ax=ax1)\n\nplt.show()","56cafa9b":"# Implementation of RMSLE.\ndef root_mean_squared_log_error(y_true, y_pred):\n    return np.sqrt(mean_squared_log_error(y_true, y_pred))","318bd813":"def add_date_parts(df):\n    saledate = df.saledate\n\n    df[\"sale_day\"] = saledate.dt.day\n    df[\"sale_month\"] = saledate.dt.month\n    df[\"sale_quarter\"] = saledate.dt.quarter\n    df[\"sale_year\"] = saledate.dt.year\n    df[\"sale_dayofweek\"] = saledate.dt.dayofweek\n    df[\"sale_dayofyear\"] = saledate.dt.dayofyear\n    df[\"sale_weekofyear\"] = saledate.dt.weekofyear\n    df[\"sale_is_month_start\"] = saledate.dt.is_month_start\n    df[\"sale_is_month_end\"] = saledate.dt.is_month_end\n    df[\"sale_is_quarter_start\"] = saledate.dt.is_quarter_start\n    df[\"sale_is_quarter_end\"] = saledate.dt.is_quarter_end\n    df[\"sale_is_year_start\"] = saledate.dt.is_year_start\n    df[\"sale_is_year_end\"] = saledate.dt.is_year_end\n\n    # Get rid of \"saledate\" column\n    df.drop(\"saledate\", axis=1, inplace=True)\n    \n    return df","4d5de714":"train_df = add_date_parts(train_df)\ndisplay_all(train_df.head().T)","cbf27b8f":"def downcast(df):\n    \"\"\"\n    Downcasts the columns of a Dataframe in order to save memory\n    \"\"\"\n    df_copy = df.copy()\n    \n    for nm, col in df_copy.items():\n        if pd.api.types.is_integer_dtype(col):\n            col_min, col_max = col.min(), col.max()\n            if (col_min > np.iinfo(np.int8).min\n                    and col_max < np.iinfo(np.int8).max):\n                df_copy[nm] = col.astype(np.int8)\n            elif (col_min > np.iinfo(np.int16).min\n                  and col_max < np.iinfo(np.int16).max):\n                df_copy[nm] = col.astype(np.int16)\n            elif (col_min > np.iinfo(np.int32).min\n                  and col_max < np.iinfo(np.int32).max):\n                df_copy[nm] = col.astype(np.int32)\n            else:\n                df_copy[nm] = cols.astype(np.int64)\n        elif pd.api.types.is_float_dtype(col):\n            col_min, col_max = col.min(), col.max()\n            #-----------------------------------------------------------\n            # In pandas stable, half floats (float16) is not implemented\n            #-----------------------------------------------------------\n            # if (col_min > np.finfo(np.float16).min\n            #         and col_max < np.finfo(np.float16).max):\n            #     df_copy[nm] = col.astype(np.float16)\n            # elif (col_min > np.finfo(np.float32).min\n            #-----------------------------------------------------------\n            if (col_min > np.finfo(np.float32).min\n                  and col_max < np.finfo(np.float32).max):\n                df_copy[nm] = col.astype(np.float32)\n            else:\n                df_copy[nm] = cols.astype(np.float64)\n        elif pd.api.types.is_object_dtype(col):\n            df_copy[nm] = col.astype(\"category\")\n            \n    return df_copy","ba73dcc6":"old_memory_usage = train_df.memory_usage(index=True, deep=True).sum()","74b4de75":"train_df = downcast(train_df)\ntrain_df.info()","f7fb778e":"new_memory_usage = train_df.memory_usage(index=True, deep=True).sum()\n\nmemory_gain_ration = new_memory_usage \/ old_memory_usage\nprint(f\"Memory usage before\/after downcasting: {old_memory_usage} \/ {new_memory_usage} -- Memory gain: {((old_memory_usage - new_memory_usage) \/ old_memory_usage * 100):.2f}%\")","dac5d844":"train_df.state.cat.categories","615a7355":"train_df.state.cat.codes","d5a7722d":"train_df.to_feather(OUTPUT_PATH + \"TrainAndValid_raw.feather\")","5e2c4731":"train_df = pd.read_feather(OUTPUT_PATH + \"TrainAndValid_raw.feather\")","b00eda40":"for nm, col in train_df.items():\n    if pd.api.types.is_categorical_dtype(col):\n        # Replace the categorical values with their codes.\n        # As the missing values are represented with category code \"-1\",\n        # we add 1 to the codes. So all code values are positive\n        train_df[nm] = col.cat.codes + 1","819f6d0d":"for nm, col in train_df.items():\n    # Search column for missing values\n    is_missing = pd.isnull(col)\n    # Check if column type is numerical\n    if pd.api.types.is_numeric_dtype(col) and is_missing.sum():\n        # Create a missing values indicator column\n        train_df[nm + \"_is_missing\"] = is_missing\n        # Fill missing values\n        train_df[nm] = col.fillna(col.median())","aaee9508":"train_df.info()","e988f0df":"display_all(train_df.head().T)","fbe9061f":"display_all(train_df.isna().sum() \/ len(train_df))","72040250":"train_df.to_feather(OUTPUT_PATH + \"TrainAndValid_preprocessed.feather\")","8c53cd69":"train_df = pd.read_feather(OUTPUT_PATH + \"TrainAndValid_preprocessed.feather\")","4d9e3c93":"valid_df = train_df[train_df.sale_year == 2012]\ntrain_df = train_df[train_df.sale_year != 2012]\n\n# Split the sets in independent variables and dependent variables\nX_train, y_train = train_df.drop(\"SalePrice\", axis=1), train_df.SalePrice\nX_valid, y_valid = valid_df.drop(\"SalePrice\", axis=1), valid_df.SalePrice\n\nX_train.shape, y_train.shape, X_valid.shape, y_valid.shape","68f587b8":"def score_model(model):\n    \"\"\"\n    Computes the MAE, R2 and RMSLE scores.\n    \"\"\"\n    train_pred = model.predict(X_train)\n    valid_pred = model.predict(X_valid)\n    return {\n        \"Train MAE\": mean_absolute_error(y_train, train_pred),\n        \"Valid MAE\": mean_absolute_error(y_valid, valid_pred),\n        \"Train R2\": r2_score(y_train, train_pred),\n        \"Valid R2\": r2_score(y_valid, valid_pred),\n        \"Train RMSLE\": root_mean_squared_log_error(y_train, train_pred),\n        \"Valid RMSLE\": root_mean_squared_log_error(y_valid, valid_pred),\n    }","f0f18e13":"model = RandomForestRegressor(n_jobs=-1, random_state=42)\n \nmodel.fit(X_train, y_train)\n \nmodel.score(X_valid, y_valid)","899de521":"# Using our evaluation function\nscore = score_model(model)\nscore","e0b8dd28":"# Grid parameters.\nrs_params = {\n    \"n_estimators\": 2 ** np.arange(1, 7, 2) * 10,\n    \"max_features\": [0.3, 0.5, \"auto\", \"sqrt\", \"log2\"],\n    \"max_depth\": np.arange(5, 36, 10),\n    \"min_samples_leaf\": np.arange(1, 7, 2),\n    \"min_samples_split\": np.arange(10, 17, 2),\n    \"max_samples\": [1000]\n}\n \n# Instantiate  the grid search class\nrs_model = RandomizedSearchCV(RandomForestRegressor(),\n                              param_distributions=rs_params,\n                              n_jobs=-1,\n                              n_iter=200,\n                              verbose=True,\n                              random_state=42)\n \nrs_model.fit(X_train, y_train)","9f1f734a":"rs_model.best_params_","18213e60":"joblib.dump(rs_model, OUTPUT_PATH + \"rs_model.bz2\", compress=True)","af994c85":"rs_model = joblib.load(OUTPUT_PATH + \"rs_model.bz2\")","5c25095c":"%%time\nscore_model(rs_model)","2f4f25c4":"%%time\nmodel = RandomForestRegressor(\n    n_estimators=rs_model.best_params_[\"n_estimators\"],\n    max_depth=rs_model.best_params_[\"max_depth\"],\n    max_features=rs_model.best_params_[\"max_features\"],\n    min_samples_leaf=rs_model.best_params_[\"min_samples_leaf\"],\n    min_samples_split=rs_model.best_params_[\"min_samples_split\"],\n    n_jobs=-1,\n    random_state=42\n    )\n\nmodel.fit(X_train, y_train)","68e18dc9":"%%time\nscore_model(model)","627874ae":"joblib.dump(model, OUTPUT_PATH + \"model.bz2\", compress=True)","b7847c91":"model = joblib.load(OUTPUT_PATH + \"model.bz2\")","53b26d88":"# We'll train a classifier that uses only half o features for the splits.\nmodel = RandomForestRegressor(n_estimators=320,\n                              max_features=0.5,\n                              max_depth=25,\n                              min_samples_leaf=3,\n                              min_samples_split=10,\n                              n_jobs=-1,\n                              random_state=42)","79271a7f":"%%time\nmodel.fit(X_train, y_train)","eb0c8846":"%%time\nscore_model(model)","06a3a522":"joblib.dump(model, OUTPUT_PATH + \"model_max_features.bz2\", compress=True)","f5bfc086":"model = joblib.load(OUTPUT_PATH + \"model_max_features.bz2\")","5078bf21":"df_raw = pd.read_feather(OUTPUT_PATH + \"TrainAndValid_raw.feather\")","e4c26c14":"def adjust_types(df, ref_df):\n    df_copy = df.copy()\n    \n    for nm, col in df_copy.items():\n        if pd.api.types.is_categorical_dtype(col):\n            categories = ref_df[nm].cat.categories\n            df_copy[nm] = pd.Categorical(col, categories=categories, ordered=True)\n        else:\n            df_copy[nm] = col.astype(ref_df[nm].dtype)\n    \n    return df_copy\n\n\ndef preprocess(df, ref_df=None):\n    \n    df_copy = df.copy()\n    \n    df_copy = add_date_parts(df_copy)\n    \n    if ref_df is None:\n        df_copy = downcast(df_copy)\n    else:\n        df_copy = adjust_types(df_copy, ref_df)\n    \n    for nm, col in df_copy.items():\n        is_missing = pd.isnull(col)\n        if pd.api.types.is_numeric_dtype(col):\n            if ref_df is None:\n                if is_missing.sum():\n                    df_copy[nm + \"_is_missing\"] = is_missing\n                    # Fill missing values with col median of df_copy\n                    df_copy[nm] = col.fillna(col.median())\n            else:\n                ref_col = ref_df[nm]\n                ref_have_missing = pd.isnull(ref_col).sum()\n                if ref_have_missing:\n                    df_copy[nm + \"_is_missing\"] = is_missing\n                    # Fill missing values with col median of ref_df\n                    df_copy[nm] = col.fillna(ref_col.median())\n                \n        elif pd.api.types.is_categorical_dtype(col):\n            df_copy[nm] = col.cat.codes + 1\n    \n    return df_copy","66999a00":"test_df = pd.read_csv(INPUT_PATH + \"Test.csv\", low_memory=False, parse_dates=[\"saledate\"])\ntest_df.head().T","5b9bc441":"test_df.info()","d139001b":"test_df = preprocess(test_df, df_raw)\ndisplay_all(test_df.head().T)","578e1e88":"test_df.info()","37bcdbfb":"# Make the predictions\ntest_preds = model.predict(test_df)\n\n# Prepare the submission Dataframe\nsubmission_df = pd.DataFrame()\nsubmission_df[\"SalesID\"] = test_df[\"SalesID\"]\nsubmission_df[\"SalesPrice\"] = test_preds\n\ndisplay_all(submission_df)","4bacd35a":"# Save the predictions file.\nsubmission_df.to_csv(\"test_predictions.csv\")","6ba6d40a":"The high positive skewness presented by the `SalePrice` distribution indicates that in the majority of the sales were made with prices lower than the mean sale price. This is evidenced by the box plot of the distribution.\n\n## Feature correlation analysis\n\nWe'll look into the correlation between features and try to identify which ones have high incluence on the target variable.\n\n### Numerical features","36d2db96":"## Saving processed dataset","1fb1175d":"Sample submission files can be downloaded from the data page. Submission files should be formatted as follows:\n\n* Have a header: \"`SalesID`,`SalePrice`\"\n* Contain two columns\n    * `SalesID`: SalesID for the validation set in sorted order\n    * `SalePrice`: Your predicted price of the sale","bd64c493":"## Target variable analysis\nLet's check the `SalePrice` distribution.","13297d10":"##  Reducing overfitting\n\nThe obtained model seems to be overfitted to the training set. By adjusting some parameters we can enforce variation in the estimators (random trees) pertaining to the model, improving its generalization.\n\nDuring process of RandomSearchCV, some of those parameters were tested, and some of their chosen values are already good:\n* **n_estimators:** In general the more trees the less likely the algorithm is to overfit. So try increasing this. The lower this number, the closer the model is to a decision tree, with a restricted feature set.\n* **max_features:** This determines how many features each tree is randomly assigned. The smaller, the less likely to overfit, but too small will start to introduce under fitting.\n* **max_depth:** This will reduce the complexity of the learned models, lowering over fitting risk. Try starting small, say 5-10, and increasing you get the best result.\n* **min_samples_leaf:** This has a similar effect to the max_depth parameter, it means the branch will stop splitting once the leaves have that number of samples each.\n\nThe discussion about these informations can be found in this stackoverflow thread:\nhttps:\/\/stackoverflow.com\/questions\/20463281\/how-do-i-solve-overfitting-in-random-forest-of-python-sklearn\n\nAs said before, `n_estimators`, `max_depth` and `min_samples_leaf` parameter values seems good, but the `max_features` value seems strange. According to the *Scikit-Learn* documentation about the *RandomForestClassfier*:\n\n> The number of features to consider when looking for the best split:\n- If int, then consider `max_features` features at each split.\n- If float, then `max_features` is a fraction and\n  `int(max_features * n_features)` features are considered at each\n  split.\n- If \"auto\", then `max_features=n_features`.\n- If \"sqrt\", then `max_features=sqrt(n_features)`.\n- If \"log2\", then `max_features=log2(n_features)`.\n- If None, then `max_features=n_features`.\n\nSo, with value our `auto`, the classifier will use all features for the splits. Let's change these values a bit to see if the predictions can be improved.","408b7f05":"## Downcast\n\nWe'll downcast the Dataframe to reduce the amount of memory used and speed up the operations that we'll perform later.\n\n* **Numerical Columns:** Depending on your environment, pandas automatically creates int32, int64, float32 or float64 columns for numeric ones. If you know the min or max value of a column, you can use a subtype which is less memory consuming. You can also use an unsigned subtype if there is no negative value.\nHere are the different subtypes you can use:  \n`int8` \/ `uint8` : consumes 1 byte of memory, range between -128\/127 or 0\/255  \n`bool` : consumes 1 byte, true or false  \n`float16` \/ int16 \/ uint16: consumes 2 bytes of memory, range between -32768 and 32767 or 0\/65535  \n`float32` \/ `int32` \/ `uint32` : consumes 4 bytes of memory, range between -2147483648 and 2147483647  \n`float64` \/ `int64` \/ `uint64`: consumes 8 bytes of memory  \nIf one of your column has values between 1 and 10 for example, you will reduce the size of that column from 8 bytes per row to 1 byte, which is more than 85% memory saving on that column!\n\n\n* **Categorical Columns:** Pandas stores categorical columns as objects. One of the reason this storage is not optimal is that it creates a list of pointers to the memory address of each value of your column. For columns with low cardinality (the amount of unique values is lower than 50% of the count of these values), this can be optimized by forcing pandas to use a virtual mapping table where all unique values are mapped via an integer instead of a pointer. This is done using the category datatype.","476a19d0":"The only features that have significant correlation with `SalePrice` are `YearMade` and `MachineID`, so we'll investigate them more deeply.\n\n#### YearMade\n\n`YearMade` indicates the registered `year of manufactoring` of the auctioned machine. Let's check the distribution of the sales and mean prices of each year.","885d8166":"# Preprocessing\n\nBefore we proceed, we would like to keep the actual state of data, for future rerefences.","b05a6474":"Of all non-numerical features, only 6 of them are complete. Their majority are even not present in 50% of the data, so it may be difficult to find a correlation between any of them and `SalePrice`. But it doesn't mean that we'll justo drop them.\n\nFor example, lets take a look at `Blade_Extension` feature.","a458fe15":"# Modelling\n\n## Train and Validation sets\nWe'll split the DataFrame in train and validation sets. Quoting the Kaggle's description of the dataset.\n\n> * **Train.csv** is the training set, which contains data through the end of 2011.\n* **Valid.csv** is the validation set, which contains data from January 1, 2012 - April 30, 2012 You make predictions on this set throughout the majority of the competition. Your score on this set is used to create the public leaderboard.","38482ed0":"# Feature Engineering?","88c11968":"## Extracting information from \"saledate\"\n\nThe `saledate` feature informs the **datatime** of the sale of each entry onf the dataset. This type of feature is known as *Time-series* data, and all data scientists will agree that this is one of the most important data types for a in the development of machine learning models.\n\nWe'll extract as many as possible of information that this feature provides.","a4c9c0ef":"## Converting categories to numbers","971d850f":"## RandomizedSearchCV","99efe0ac":"At first, we can't draw any assumption from this distribution aside that the majority of seliing prices are located at the lower part of the plotting, indicating that much of the prices are below the mean price as we have seem before. Also, it seems that some data sources are specialized at the reporting of specific machines.\n\n### Non-numerical features","3aa6c91c":"## Fit a Random Forest Regressor\n\nNow, we'll fit a Random Forest Regressor with default parameters.","68c03562":"Also, `SalePrice` is the feature that we want to predict (a.k.a *dependent variable*).","18ef2b18":"Now, let's check how well does the found model is fitted.","ff5cc938":"Not bad for our first model, and we have not touched the hyperparameters.\n\nBut can we do better? Let's check out.","4bf6eec4":"# Preparing for submission\n\n## Pre-processing function","46a59f24":"Year 1000 concentrates a high amount of manufactured machines. It's not clear if it's data collection error or just a placeholder for an unregistered year, so we'll just leave it as it is.\n\nAside from that, from the first plot we can see most of the auctioned machines were made around years 1998 and 2005, with few machines made in years after 2008.\n\nDistribution of prices per year of manufactoring  is as expected. Newer generally machines cost higher than the old ones, with few exceptions.\n\n#### MachineID\n\nThe `MachineID` feature denotes a identifier for a particular machine. Machines may have multiple sales, so we can expect same values in some entries. Let's check for unique values.","4fc9fc12":"## Make predictions on the Testing set.","dbca576d":"We have a mix of **numerical** and **string** features. Also, a lot of the features (machine configuration, mainly) does have missing values.","504168e9":"# What defines success?\n\nThe evaluation metric for this competition is the **RMSLE** (*root mean squared log error*) between the actual and predicted auction prices.\n\n$$\nRMSLE = \\sqrt{\\frac{1}{n} \\sum_{i = n}^n (log(predicted_i + 1) - log(actual_i - 1)^2}\n$$\n\nAlthough Scikit-Learn doesn't have a default implementation of this metric, the implementation is straight forward:","5a2fdbaa":"OK! `MachineID` has *high cardinality*, so it's unlikely that its distribution will make some sense.\n\nLet's plot a scatter plot of the `SalePrice` by `MachineID`. To support our understanding, we'll use the information from `datasource`, a feature that have a significant correlation with `MachineID`, for the plotting.","691f9842":"## Evaluation function\n\nNow, we'll build our first model. But first, let's implement and evaluation function, that will compute three performance metrics:\n\n* *Mean Absolute error* (**MAE**)\n* *R<sup>2<\/sup> regression score*\n* *Root Mean Squared Logarithmic Error* (**RMSLE**)","134b05cf":"As we can see, the class information doesn't give us any valuable information, as the classes distributions doesn't differ from each order signicantly. But the lack of class specification, by itself, seemms to be imbued with some predictive information. Our feature engineering will have to consider this kind of information.","a468b569":"# What data do we have?\n\nFor this competition, you are predicting the sale price of bulldozers sold at auctions.\n\nThe data for this competition is split into three parts:\n\n* **Train.csv** is the training set, which contains data through the end of 2011.\n* **Valid.csv** is the validation set, which contains data from January 1, 2012 - April 30, 2012 You make predictions on this set throughout the majority of the competition. Your score on this set is used to create the public leaderboard.\n* **Test.csv** is the test set, which won't be released until the last week of the competition. It contains data from May 1, 2012 - November 2012. Your score on the test set determines your final rank for the competition.\n\nThe key fields are in train.csv are:\n\n* `SalesID`: the unique identifier of the sale\n* `MachineID`: the unique identifier of a machine.  A machine can be sold multiple times\n* `saleprice`: what the machine sold for at auction (only provided in train.csv)\n* `saledate`: the date of the sale\n\nThere are several fields towards the end of the file on the different options a machine can have.  The descriptions all start with `machine configuration` in the data dictionary.  Some product types do not have a particular option, so all the records for that option variable will be null for that product type.  Also, some sources do not provide good option and\/or hours data.\n\nThe **machine_appendix.csv** file contains the correct year manufactured for a given machine along with the make, model, and product class details. There is one machine id for every machine in all the competition datasets (training, evaluation, etc.).","68c7635b":"# Problem Definition\n\n## \ud83d\ude9c Predicting the Sale Price of Bulldozers using Machine Learning \ud83d\ude9c\n\n![Competition image](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/3316\/media\/bulldozer.jpg)\n\nThe goal of the contest is to predict the sale price of a particular piece of heavy equiment at auction based on it's usage, equipment type, and configuaration.  The data is sourced from auction result postings and includes information on usage and equipment configurations.\n\nFast Iron is creating a \"blue book for bull dozers,\" for customers to value what their heavy equipment fleet is worth at auction.\n\n## About Fast Iron\nThis competition was launched under the [Kaggle Startup Program](https:\/\/medium.com\/kaggle-blog). If you're a startup with a predictive modelling challenge, please apply!","5b08be05":"## Filling missing numerical values with the median values of each column","65354386":"## Loading the Testing set","e3acddc2":"## Pre-process the Testing set","f44e132a":"The processed dataset now has less than 5% of the original size in memory. A thanks from our computers.\n\nAlso, all object type features were transformed into categories","c0a69fdf":"The overall of numerical features is present in all entries, excluding `auctioneerID` and `MachineHoursCurrentMeter` (which is absent in 50% of the dataset).\n\nLet's check the feature correlations.","acb3c993":"## Fit a RandomForest with the best found hyperparameters."}}