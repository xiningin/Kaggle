{"cell_type":{"052771e8":"code","0b073050":"code","0cb4d5ff":"code","41f46af4":"code","92ef2d45":"code","6e77ea0e":"code","9288d2d3":"code","b5f0a869":"code","4c5dace2":"code","725e8130":"code","d636cd01":"code","02117519":"code","9b37dbf0":"code","926e796f":"code","897ef4f8":"code","928b930d":"code","e7305017":"code","3c756966":"code","2df2393d":"code","4067f707":"code","74ba0bda":"code","e9973ad3":"code","6185204c":"code","3ecb5a78":"code","66ebcf9e":"code","7fa62520":"code","e58c5751":"code","9a9e62a5":"code","1a48e21c":"code","d79db593":"code","f47dfc06":"code","556aba12":"code","ef5a8600":"code","ca04fba7":"code","f9940123":"code","e750610b":"code","4523da6c":"code","fb71c3e5":"code","3b4f8787":"code","30fad2cc":"code","458b5c61":"code","d8982014":"code","a01eeb70":"code","eb21875a":"code","ae3c6676":"code","2357c0b5":"code","28537b1a":"code","b0e6c9d8":"code","c7f3bfe2":"code","4e547947":"code","91e61356":"code","233da6ea":"code","43172225":"code","76ed2e04":"markdown","a813bba4":"markdown","064b4482":"markdown","fd384239":"markdown","d8a68940":"markdown","c8e3fdde":"markdown"},"source":{"052771e8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0b073050":"#Reading train and test file\ntrain_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","0cb4d5ff":"train_data.columns","41f46af4":"train_data.shape","92ef2d45":"test_data.shape","6e77ea0e":"train_data.head()","9288d2d3":"#to check if there is any nan value\ntrain_data.isnull().values.any()","b5f0a869":"#using following commands we came to know that three columns contain NaN values, given below\ntrain_data['Embarked'].isnull().values.any()","4c5dace2":"train_data['Cabin'].isnull().values.any()","725e8130":"train_data['Age'].isnull().values.any()","d636cd01":"train_data['Age'].isna().sum()","02117519":"train_data['Cabin'].isna().sum()","9b37dbf0":"train_data['Embarked'].isna().sum()","926e796f":"#How much fraction of rows includes NaN values\ntrain_data['Age'].isna().sum()\/len(train_data)","897ef4f8":"train_data['Embarked'].isna().sum()\/len(train_data)","928b930d":"train_data['Cabin'].isna().sum()\/len(train_data)","e7305017":"#Almost 20 % and 77 % values are missing for \"Cabin\" and \"Age\", \n#Now by replacing with avg value in \"Cabin\" for such a large missing values does not seems realistic\ntrain_data.drop(['Cabin'], axis=1, inplace=True)","3c756966":"#Replacing Nan values in Embarked with mode value\ntrain_data['Embarked'].fillna(train_data['Embarked'].mode()[0], inplace=True)","2df2393d":"train_data.shape","4067f707":"#Now checking NaN values\ntrain_data.isnull().values.any()","74ba0bda":"train_data.head(10)","e9973ad3":"train_data.drop('Name',axis =1, inplace = True)","6185204c":"train_data.shape","3ecb5a78":"#check data types\ntrain_data.dtypes","66ebcf9e":"#After looking correlation btw Age and Other variables, we can see that there exist a correlation btw AGE & Pclass,\n#now we will replcae NaN values in Age with median value of each Pclass \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,5))\nsns.set(style=\"whitegrid\")\nax = sns.boxplot(x=train_data['Pclass'], y=train_data['Age'])","7fa62520":"#Replacing NaN values in Age for each Pclass with median values (38,29,24)\nfor p, v in zip([1,2,3], [38, 29, 24]):\n    train_data.loc[train_data[\"Pclass\"].eq(p), \"Age\"] = train_data.loc[train_data[\"Pclass\"].eq(p), \"Age\"].fillna(v)\nprint(train_data[\"Age\"])","e58c5751":"#Seperating X_feature set\nX = train_data[['PassengerId','Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']].values","9a9e62a5":"X.shape","1a48e21c":"#Now convert non-numeric values type (object) to numeric ones\nfrom sklearn import preprocessing\npe_sex = preprocessing.LabelEncoder()\npe_sex.fit(['male','female'])\nX[:,2] = pe_sex.transform(X[:,2]) \n\n\npe_emb = preprocessing.LabelEncoder()\npe_emb.fit([ 'S', 'C', 'Q'])\nX[:,7] = pe_emb.transform(X[:,7])\n\n\nX[0:5]","d79db593":"Y = train_data[['Survived']].values","f47dfc06":"Y.shape","556aba12":"MinMaxScaler = preprocessing.MinMaxScaler()\nXX = MinMaxScaler.fit_transform(X)","ef5a8600":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(XX, Y, test_size=0.2, random_state=1)","ca04fba7":"X_train.shape","f9940123":"X_test.shape","e750610b":"from sklearn.tree import DecisionTreeClassifier\nDTree = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 3)","4523da6c":"DTree.fit(X_train,y_train)","fb71c3e5":"predValue = DTree.predict(X_test)","3b4f8787":"from sklearn import metrics\nprint(\"DecisionTrees's Accuracy: \", metrics.accuracy_score(y_test, predValue))","30fad2cc":"test_data['Fare'].fillna(test_data['Fare'].mode()[0], inplace=True)","458b5c61":"for p, v in zip([1,2,3], [38, 29, 24]):\n    test_data.loc[train_data[\"Pclass\"].eq(p), \"Age\"] = test_data.loc[train_data[\"Pclass\"].eq(p), \"Age\"].fillna(v)","d8982014":"XTEST = test_data[['PassengerId','Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']].values","a01eeb70":"XTEST.shape","eb21875a":"XTEST[:,2] = pe_sex.transform(XTEST[:,2])\nXTEST[:,7] = pe_emb.transform(XTEST[:,7])","ae3c6676":"XTEST","2357c0b5":"predictions = DTree.predict(XTEST)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","28537b1a":"from sklearn.neighbors import KNeighborsClassifier","b0e6c9d8":"#Selecting different Ks in range [1,10] and choose the one with high accuracy\nfrom sklearn import metrics\nK_range = 11\nacc = [] \nfor i in range(1,K_range):\n    \n    #Train Model and Predict  \n    neigh = KNeighborsClassifier(n_neighbors = i).fit(X_train,y_train)\n    yhat=neigh.predict(X_test)\n    acc.append(metrics.accuracy_score(y_test, yhat))\n    \n\n\nacc","c7f3bfe2":"neighModel = KNeighborsClassifier(n_neighbors = 9).fit(X_train,y_train)\nyhat=neighModel.predict(X_test)\nmetrics.accuracy_score(y_test, yhat)","4e547947":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\n# We have different optimization methods, for instance, \u2018newton-cg\u2019, \u2018lbfgs\u2019, \u2018liblinear\u2019, \u2018sag\u2019, \u2018saga\u2019.\nLRmodel = LogisticRegression(C=1, solver='sag').fit(X_train,y_train)\nLRmodel","91e61356":"ypredd = LRmodel.predict(X_test)","233da6ea":"metrics.accuracy_score(y_test, ypredd)","43172225":"predictions = DTree.predict(XTEST)\n\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': y_pred_svm})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","76ed2e04":"## Setting decision tree","a813bba4":"## KNN classifier","064b4482":"from sklearn import svm\nclf = svm.SVC(kernel='rbf')\nclf.fit(X_train, y_train) ","fd384239":"## SVM","d8a68940":"yhatSVM = clf.predict(X_test)","c8e3fdde":"## Logistic Regression"}}