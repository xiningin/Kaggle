{"cell_type":{"2d38d5a3":"code","591542bf":"code","b8b776c1":"code","a4718d6e":"code","9589cfe2":"code","d850ccfd":"code","16ea3ff1":"code","ee394319":"code","34494888":"code","72e18771":"code","b1b31ce7":"code","7184b1bf":"code","aa24ca59":"code","93f485b8":"code","4f575199":"code","e8e97e09":"code","5b6d8b86":"code","2ae6f35d":"code","bb897ca3":"code","c1b83166":"code","a12ddb55":"code","0c1daabb":"code","7bea0e50":"code","0eda691e":"code","6c725456":"code","ed084fe7":"code","d37989fa":"code","ad587956":"code","6c958853":"code","e4467920":"code","f5c85950":"code","2db2fd97":"code","66f12b7c":"code","7ee58066":"code","1d8ae530":"code","1655f83e":"code","d5cbb542":"code","81b703e1":"code","ff371c81":"code","04784a7f":"code","bdb508cd":"code","c874e0ba":"code","55572569":"code","1ec1f0fe":"code","8d815393":"code","2ecb08ad":"code","df3d1d16":"code","c7384d8a":"code","06001c89":"code","fb997cc2":"code","0f862d40":"code","74023335":"code","d4dc9f16":"code","3d1adaef":"code","e4c8d327":"code","da66c251":"code","09c125fa":"code","9b0cacef":"code","fbfc650a":"code","3707b52c":"code","1acdbdea":"code","ca18b1e3":"code","d506fda2":"code","756b7f7a":"code","2b9bc296":"code","ec9ecc07":"code","0173536e":"code","58427b3d":"code","ccb497fb":"code","409e0524":"code","843aebfd":"code","aa9e7d57":"code","1099e63a":"code","01ac19d7":"code","840ea197":"code","a1e0a42d":"code","26eac86f":"code","10a6a4b0":"code","7471cb6f":"code","e03b96d9":"code","fd891a49":"code","36f00373":"code","17c95268":"code","f63e8bc1":"code","3528ac2e":"code","7aabb14c":"code","421f39da":"markdown","c91d6e84":"markdown","74170605":"markdown","fafadd32":"markdown","a1912347":"markdown","0a5e5130":"markdown","46ecd912":"markdown","ee9f1fe1":"markdown","533adb31":"markdown","1c04f72c":"markdown","7e6b638f":"markdown","1931aedd":"markdown","e0bcbe0f":"markdown","2627deee":"markdown","83b309e4":"markdown","2520ae55":"markdown","bde92a2e":"markdown","030324cd":"markdown"},"source":{"2d38d5a3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Configs\npd.options.display.float_format = '{:,.4f}'.format\nsns.set(style=\"whitegrid\")\nplt.style.use('seaborn')\nseed = 42\nnp.random.seed(seed)","591542bf":"# df = pd.read_csv('dados\/olist_order_reviews_dataset.csv')\n\nfile_path = '\/kaggle\/input\/brazilian-ecommerce\/olist_order_reviews_dataset.csv'\ndf = pd.read_csv(file_path)\nprint(\"DataSet = {:,d} rows and {} columns\".format(df.shape[0], df.shape[1]))\n\nprint(\"\\nAll Columns:\\n=>\", df.columns.tolist())\n\nquantitative = [f for f in df.columns if df.dtypes[f] != 'object']\nqualitative = [f for f in df.columns if df.dtypes[f] == 'object']\n\nprint(\"\\nStrings Variables:\\n=>\", qualitative,\n      \"\\n\\nNumerics Variables:\\n=>\", quantitative)\n\ndf.head()","b8b776c1":"df = df[['review_score','review_comment_message']]\ndf.columns = ['score', 'comment']\ndf.head()","a4718d6e":"import time\n\ndef time_spent(time0):\n    t = time.time() - time0\n    t_int = int(t) \/\/ 60\n    t_min = t % 60\n    if(t_int != 0):\n        return '{}min {:.3f}s'.format(t_int, t_min)\n    else:\n        return '{:.3f}s'.format(t_min)","9589cfe2":"from sklearn.metrics import confusion_matrix, classification_report\n\nthis_labels = ['Negative','Positive']\n\ndef class_report(y_real, y_my_preds, name=\"\", labels=this_labels):\n    if(name != ''):\n        print(name,\"\\n\")\n    print(confusion_matrix(y_real, y_my_preds), '\\n')\n    print(classification_report(y_real, y_my_preds, target_names=labels))","d850ccfd":"def plot_words_distribution(mydf, target_column, title='Words distribution', x_axis='Words in column'):\n    # adaptade of https:\/\/www.kaggle.com\/alexcherniuk\/imdb-review-word2vec-bilstm-99-acc\n    # def statistics\n    len_name = target_column +'_len'\n    mydf[len_name] = np.array(list(map(len, mydf[target_column])))\n    sw = mydf[len_name]\n    median = sw.median()\n    mean   = sw.mean()\n    mode   = sw.mode()[0]\n    # figure\n    fig, ax = plt.subplots()\n    sns.distplot(mydf[len_name], bins=mydf[len_name].max(),\n                hist_kws={\"alpha\": 0.9, \"color\": \"blue\"}, ax=ax,\n                kde_kws={\"color\": \"black\", 'linewidth': 3})\n    ax.set_xlim(left=0, right=np.percentile(mydf[len_name], 95)) # Dont get outiliers\n    ax.set_xlabel(x_axis)\n    ymax = 0.020\n    plt.ylim(0, ymax)\n    # plot vertical lines for statistics\n    ax.plot([mode, mode], [0, ymax], '--', label=f'mode = {mode:.2f}', linewidth=4)\n    ax.plot([mean, mean], [0, ymax], '--', label=f'mean = {mean:.2f}', linewidth=4)\n    ax.plot([median, median], [0, ymax], '--', label=f'median = {median:.2f}', linewidth=4)\n    ax.set_title(title, fontsize=20)\n    plt.legend()\n    plt.show()","16ea3ff1":"def eda_categ_feat_desc_plot(series_categorical, title = \"\", fix_labels=False, bar_format='{}'):\n    series_name = series_categorical.name\n    val_counts = series_categorical.value_counts()\n    val_counts.name = 'quantity'\n    val_percentage = series_categorical.value_counts(normalize=True)\n    val_percentage.name = \"percentage\"\n    val_concat = pd.concat([val_counts, val_percentage], axis = 1)\n    val_concat.reset_index(level=0, inplace=True)\n    val_concat = val_concat.rename( columns = {'index': series_name} )\n    \n    fig, ax = plt.subplots(figsize = (12,4), ncols=2, nrows=1) # figsize = (width, height)\n    if(title != \"\"):\n        fig.suptitle(title, fontsize=18)\n        fig.subplots_adjust(top=0.8)\n\n    s = sns.barplot(x=series_name, y='quantity', data=val_concat, ax=ax[0])\n    if(fix_labels):\n        val_concat = val_concat.sort_values(series_name).reset_index()\n    \n    for index, row in val_concat.iterrows():\n        s.text(row.name, row['quantity'], bar_format.format(row['quantity']), color='black', ha=\"center\")\n\n    s2 = val_concat.plot.pie(y='percentage', autopct=lambda value: '{:.2f}%'.format(value),\n                             labels=val_concat[series_name].tolist(), legend=None, ax=ax[1],\n                             title=\"Percentage Plot\")\n\n    ax[1].set_ylabel('')\n    ax[0].set_title('Quantity Plot')\n\n    plt.show()","ee394319":"def plot_nn_loss_acc(history):\n    fig, (axis1, axis2) = plt.subplots(nrows=1, ncols=2, figsize=(16,6))\n    # summarize history for accuracy\n    axis1.plot(history.history['accuracy'], label='Train', linewidth=3)\n    axis1.plot(history.history['val_accuracy'], label='Validation', linewidth=3)\n    axis1.set_title('Model accuracy', fontsize=16)\n    axis1.set_ylabel('accuracy')\n    axis1.set_xlabel('epoch')\n    axis1.legend(loc='upper left')\n    # summarize history for loss\n    axis2.plot(history.history['loss'], label='Train', linewidth=3)\n    axis2.plot(history.history['val_loss'], label='Validation', linewidth=3)\n    axis2.set_title('Model loss', fontsize=16)\n    axis2.set_ylabel('loss')\n    axis2.set_xlabel('epoch')\n    axis2.legend(loc='upper right')\n    plt.show()","34494888":"def describe_y_by_x_cat_boxplot(dtf, x_feat, y_target, title='', figsize=(15,5), rotatioon_degree=0):\n    the_title = title if title != '' else '{} by {}'.format(y_target, x_feat)\n    fig, ax1 = plt.subplots(figsize = figsize)\n    sns.boxplot(x=x_feat, y=y_target, data=dtf, ax=ax1)\n    ax1.set_title(the_title, fontsize=18)\n    plt.xticks(rotation=rotatioon_degree)\n    plt.show()","72e18771":"from sklearn.feature_extraction.text import CountVectorizer\n\ndef ngrams_corpus_counter(corpus,ngram_range,n=None):\n    \"\"\"\n    List the top n words in a vocabulary according to occurrence in a text corpus.\n    \"\"\"\n    vec = CountVectorizer(stop_words = 'english',ngram_range=ngram_range).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    total_list=words_freq[:n]\n    df=pd.DataFrame(total_list,columns=['text','count'])\n    return df\n\ndef plot_ngrams_words(series_words, title='Top 10 words'):\n    \"\"\"Plot 3 graphs\n    @series_words: a series where each row is a set of words\n    \"\"\"\n    # Generate\n    df_1_grams = ngrams_corpus_counter(series_words, (1,1), 10)\n    df_2_grams = ngrams_corpus_counter(series_words, (2,2), 10)\n    df_3_grams = ngrams_corpus_counter(series_words, (3,3), 10)\n\n    fig, axes = plt.subplots(figsize = (18,4), ncols=3)\n    fig.suptitle(title)\n\n    sns.barplot(y=df_1_grams['text'], x=df_1_grams['count'],ax=axes[0])\n    axes[0].set_title(\"1 grams\")\n\n    sns.barplot(y=df_2_grams['text'], x=df_2_grams['count'],ax=axes[1])\n    axes[1].set_title(\"2 grams\",)\n\n    sns.barplot(y=df_3_grams['text'], x=df_3_grams['count'],ax=axes[2])\n    axes[2].set_title(\"3 grams\")\n\n    plt.show()\n    \n# Example: plot_ngrams_words(df['comment'])","b1b31ce7":"import random \n\ndef compare_text_cleaning(mydf, column1, column2, rows=10):\n    \"\"\"Compare Text after text cleaning\n    \"\"\"\n    max_values = len(mydf)\n    for i in range(rows):\n        anumber = random.randint(0, max_values)\n        print('Before:', mydf[column1][anumber])\n        print('After :',  mydf[column2][anumber], '\\n')","7184b1bf":"before = df.shape[0]\nsns.heatmap(df.isnull(), cbar=False, yticklabels=False)\nplt.show()","aa24ca59":"# df.duplicated().sum() # 4187 rows\n# df = df.drop_duplicates()\n\nafter = df.shape[0]\n\ndf = df.dropna()\ndf = df.reset_index(drop=True)\n\nprint('Before {:,d} | After Clean Data {:,d} | was removed {:,d} rows, that is {:.2%} of data'.format(\n    before, after, before - after, (before - after)\/before))","93f485b8":"eda_categ_feat_desc_plot(df['score'], fix_labels=True, bar_format='{:,.0f}', title='Distribution of score')","4f575199":"plot_words_distribution(df, 'comment', 'Words distribution of all reviews')","e8e97e09":"describe_y_by_x_cat_boxplot(df, 'score', 'comment_len', figsize=(10,5))","5b6d8b86":"plot_words_distribution(df.query('score == 5'), 'comment', 'Distribution of words for the best evaluations')","2ae6f35d":"plot_words_distribution(df.query('score == 1'), 'comment', 'Words distribution for the worst evaluations')","bb897ca3":"plot_ngrams_words(df['comment'], 'Top 10 words to all comments')","c1b83166":"plot_ngrams_words(df.query('score == 1')['comment'], 'Top 10 words for the worst ratings')","a12ddb55":"plot_ngrams_words(df.query('score == 5')['comment'], 'top 10 words for the best reviews')","0c1daabb":"mispell_dict = {'hj': 'hoje', 'nao': 'n\u00e3o', 'MT': 'muito', 'pessima': 'p\u00e9ssima'}\n\ndef correct_spelling(x, dic):\n    for word in dic.keys():\n        x = x.replace(word, dic[word])\n    return x","7bea0e50":"'nao' in stops","0eda691e":"import re\nfrom string import punctuation\n\nfrom nltk.corpus import stopwords\n# Stop Words in Python. Is better search in a 'set' structure\nstops = set(stopwords.words(\"portuguese\"))  \nstops.remove(\"n\u00e3o\")\n\ndef clean_text(words):\n    words = re.sub(\"[^a-zA-Z\u00e1\u00e0\u00e2\u00e3\u00e9\u00e8\u00ea\u00ed\u00ef\u00f3\u00f4\u00f5\u00f6\u00fa\u00e7\u00f1\u00c1\u00c0\u00c2\u00c3\u00c9\u00c8\u00cd\u00cf\u00d3\u00d4\u00d5\u00d6\u00da\u00c7\u00d11234567890]\", \" \", words) \n    words = words.lower().split()\n    words = [w for w in words if not w in stops]   \n    # Join the words back into one string separated by space, \n    words = ' '.join([c for c in words if c not in punctuation]).replace('\\r\\n',' ')\n    return words","6c725456":"df['clean_comment'] = df['comment'].apply(clean_text)\n\ndf['clean_comment'] = df['clean_comment'].apply(lambda x: correct_spelling(x, mispell_dict))","ed084fe7":"compare_text_cleaning(df, 'comment', 'clean_comment')","d37989fa":"df.head()","ad587956":"from sklearn.model_selection import train_test_split\n\nX = df['clean_comment'].values\n\ny = df['score'].replace({1: 0, 2: 0, 3: 0, 4: 1, 5: 1})\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","6c958853":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model, Sequential\nfrom keras.layers import Convolution1D\nfrom keras import initializers, regularizers, constraints, optimizers, layers\n\n# Tokenizer\nmaxlen = 130\nmax_features = 6000\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(np.concatenate((x_train, x_test), axis=0))\n\n# Convert x_train\nlist_tokenized_train = tokenizer.texts_to_sequences(x_train) # convert string to numbers, \nX_t = pad_sequences(list_tokenized_train, maxlen=maxlen) # create a array of 130 spaces and put all words in end\n\n## Convert x_test\nX_tt = tokenizer.texts_to_sequences(x_test)\nX_tt = pad_sequences(X_tt, maxlen=maxlen)","e4467920":"model = Sequential()\nmodel.add(Embedding(max_features, 128))\nmodel.add(Bidirectional(LSTM(32, return_sequences = True)))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dense(20, activation=\"relu\"))\nmodel.add(Dropout(0.05))\nmodel.add(Dense(1, activation=\"sigmoid\"))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nbatch_size = 100\nepochs = 5\nhistory = model.fit(X_t, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2, validation_data=(X_tt, y_test))","f5c85950":"plot_nn_loss_acc(history)","2db2fd97":"y_pred = model.predict_classes(X_tt)\n\nclass_report(y_test, y_pred, \"Strategie 1: Keras NN\")","66f12b7c":"from sklearn.feature_extraction.text import CountVectorizer\n\n# instantiate the vectorizer\nvect = CountVectorizer()\nvect.fit(np.concatenate((x_train, x_test), axis=0))\n\nX_train_dtm = vect.transform(x_train)\nX_test_dtm = vect.transform(x_test)\nx_full_dtm = vect.transform( np.concatenate((x_train, x_test), axis=0) )\n\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf_transformer = TfidfTransformer()\ntfidf_transformer.fit( x_full_dtm )\nX_train_dtm_tfft = tfidf_transformer.transform(X_train_dtm)\nX_test_dtm_tfft  = tfidf_transformer.transform(X_test_dtm)","7ee58066":"from sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC(C=0.5, random_state=42)\nlinear_svc.fit(X_train_dtm, y_train)\ny_pred_class = linear_svc.predict(X_test_dtm)\nclass_report(y_test, y_pred_class, \"Strategie 1: Keras NN CountVectorizer\") # 0.89, 0.88\n\nlinear_svc = LinearSVC(C=0.5, random_state=42)\nlinear_svc.fit(X_train_dtm_tfft, y_train)\ny_pred_class = linear_svc.predict(X_test_dtm_tfft)\nclass_report(y_test, y_pred_class, \"Strategie 1: Keras NN TD-IDF\") # 0.90, 0.89","1d8ae530":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(solver='liblinear')\nlogreg.fit(X_train_dtm, y_train)\ny_pred_class = logreg.predict(X_test_dtm)\nclass_report(y_test, y_pred_class, \"Strategie 2: Keras NN TD-IDF\")\n\nlogreg = LogisticRegression(solver='liblinear')\nlogreg.fit(X_train_dtm_tfft, y_train)\ny_pred_class = logreg.predict(X_test_dtm_tfft)\nclass_report(y_test, y_pred_class, \"Strategie 2: Keras NN TD-IDF\")","1655f83e":"from sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier() \nforest = forest.fit(X_train_dtm, y_train)\ny_pred = forest.predict(X_test_dtm)\n\nclass_report(y_test, y_pred, 'Strategie 2: RandomForestClassifier VetCount:')","d5cbb542":"# plt.figure(figsize = (12,5)) # fom imbd-popcorn my kernel\n# feat_importances = pd.Series(forest.feature_importances_)#, index=X.columns)\n# feat_importances.nlargest(30).reset_index().replace(inv_map).set_index('index').plot(kind='barh', use_index=True)\n# plt.show()\n\nint_top = 30\ninv_map = {v: k for k, v in vect.vocabulary_.items()}\nfeat_importances = pd.Series(forest.feature_importances_)\nlabels_top = [ inv_map[el] for el in feat_importances.nlargest(int_top).index.tolist()]","81b703e1":"plt.figure(figsize = (12,5))\nsns.barplot(y=labels_top, x=feat_importances.nlargest(int_top))\nplt.show()","ff371c81":"pause_here","04784a7f":"from collections import Counter\n\n## Constroi um dict que mapeia palavras para inteiros\ncounts = Counter(df['clean_comment'])\nvocab = sorted(counts, key=counts.get, reverse=True)\nvocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n\n## Usa o dict para ccodificar cada comentario em reviews_split\n## Armazenar os coment\u00e1rios em reviews_ints\nreviews_ints = []\nfor review in reviews_split:\n    reviews_ints.append([vocab_to_int[word] for word in review.split()])","bdb508cd":"print('Palavras unicas: ', len((vocab_to_int)), '\\n')  \n\n#imprime primeiro comentario codificado\nprint('Coment\u00e1rio codificado: \\n', reviews_ints[:2])","c874e0ba":"encoded_labels = [1 if c>3 else 0 for c in df['score']]\nplt.hist(encoded_labels); # Qtd de positivos (1) e negativos (0) depois de mapear {1,2,3} => Zero| Negativo, resto Um|Positivo","55572569":"# Comentarios  \nreview_lens = Counter([len(x) for x in reviews_ints])\nprint(\"Coment\u00e1rios de tamanho zero: {}\".format(review_lens[0]))\nprint(\"Tamanho m\u00e1ximo de um coment\u00e1rio: {}\".format( max(review_lens)))\n# Coment\u00e1rios de tamanho zero: 132\n# Tamanho m\u00e1ximo de um coment\u00e1rio: 45\n## Remove quaisquer coment\u00e1rios \/ etiquetas com comprimento zero da lista reviews_ints.","1ec1f0fe":"\n# Obtem indices de comentarios com comprimento difrente de zero\nnon_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n\n# Remove comntarios de tamanho zero\nreviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\nencoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\nreviews_ints = reviews_ints[:41500]\nencoded_labels = encoded_labels[:41500]","8d815393":"# punctuation \u00e9 uma lista de strings de pontua\u00e7\u00e3o {'.', ',', ';', '!' ...}\nfrom string import punctuation\n\n# Remove pontua\u00e7\u00e3o e outros caracteres\nreviews_split= []\nfor review in reviews:\n    reviews_split.append(''.join([c for c in review if c not in punctuation]).replace('\\r\\n',' '))\n\nall_text = ' '.join(reviews_split)\n\n# Cria uma lista com as palavras\nwords = all_text.split()","2ecb08ad":"words[:30]","df3d1d16":"reviews_split[:5]","c7384d8a":"dfe = pd.read_pickle('vocab_to_int')","06001c89":"dfe['o']","fb997cc2":"from collections import Counter\n\n## Constroi um dict que mapeia palavras para inteiros\ncounts = Counter(words)\nvocab = sorted(counts, key=counts.get, reverse=True)\nvocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n\n## Usa o dict para ccodificar cada comentario em reviews_split\n## Armazenar os coment\u00e1rios em reviews_ints\nreviews_ints = []\nfor review in reviews_split:\n    reviews_ints.append([vocab_to_int[word] for word in review.split()])","0f862d40":"print('Palavras unicas: ', len((vocab_to_int)), '\\n')  \n\n#imprime primeiro comentario codificado\nprint('Coment\u00e1rio codificado: \\n', reviews_ints[:1])","74023335":"# Mapeamento de palavras em n\u00fameros\nvocab_to_string = { ii: word for word, ii  in vocab_to_int.items()}\nprint(vocab_to_string)","d4dc9f16":"encoded_labels = [1 if c>3 else 0 for c in labels]","3d1adaef":"plt.hist(encoded_labels); # Qtd de positivos (1) e negativos (0) depois de mapear {1,2,3} => Zero| Negativo, resto Um|Positivo","e4c8d327":"# Comentarios  \nreview_lens = Counter([len(x) for x in reviews_ints])\nprint(\"Coment\u00e1rios de tamanho zero: {}\".format(review_lens[0]))\nprint(\"Tamanho m\u00e1ximo de um coment\u00e1rio: {}\".format(max(review_lens)))","da66c251":"## Remove quaisquer coment\u00e1rios \/ etiquetas com comprimento zero da lista reviews_ints.\n\n# Obtem indices de comentarios com comprimento difrente de zero\nnon_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n\n# Remove comntarios de tamanho zero\nreviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\nencoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\nreviews_ints = reviews_ints[:41500]\nencoded_labels = encoded_labels[:41500]","09c125fa":"def pad_features(reviews_ints, seq_length):\n\n    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n\n    for i, row in enumerate(reviews_ints):\n        features[i, -len(row):] = np.array(row)[:seq_length]\n    \n    return features","9b0cacef":"seq_length = 20 # cada comentario na rede ser\u00e1 de size = 20\n\nfeatures = pad_features(reviews_ints, seq_length=seq_length)\n\n# imprime os primeiros 10 valores dos primeiros 30 lotes\nprint(features[:30,:10])","fbfc650a":"split_frac = 0.8 # Dividir 80% para treino e 20% para teste\n\n## Separa os dados em treino, valida\u00e7\u00e3o e teste\n\nsplit_idx = int(len(features)*split_frac)\ntrain_x, remaining_x = features[:split_idx], features[split_idx:]\ntrain_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n\ntest_idx = int(len(remaining_x)*0.5)\nval_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\nval_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n\nprint(\"\\t\\t\\tFeature Shapes:\")\nprint(\"Train set: \\t\\t{}\".format(train_x.shape), \n      \"\\nValidation set: \\t{}\".format(val_x.shape),\n      \"\\nTest set: \\t\\t{}\".format(test_x.shape))","3707b52c":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# cria Tensor datasets\ntrain_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\nvalid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\ntest_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n\n# dataloaders\nbatch_size = 25\n\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\nvalid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)","1acdbdea":"#Checa se tem GPU\ntrain_on_gpu = torch.cuda.is_available()\n\nif(train_on_gpu):\n    print('Treina em GPU.')\nelse:\n    print('GPU n\u00e3o disponivel, treina em CPU.')","ca18b1e3":"import torch.nn as nn\n\nclass SentimentRNN(nn.Module):\n    \"\"\"\n    The RNN model that will be used to perform Sentiment analysis.\n    \"\"\"\n\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n        \"\"\"\n        Initialize the model by setting up the layers.\n        \"\"\"\n        super(SentimentRNN, self).__init__()\n\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        # embedding and LSTM layers\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n                            dropout=drop_prob, batch_first=True)\n        \n        # dropout layer\n        self.dropout = nn.Dropout(0.5)\n        \n        # linear and sigmoid layers\n        self.fc = nn.Linear(hidden_dim, output_size)\n        self.sig = nn.Sigmoid()\n        \n\n    def forward(self, x, hidden):\n        \"\"\"\n        Perform a forward pass of our model on some input and hidden state.\n        \"\"\"\n        batch_sizes = x.size(0)\n\n        # embeddings and lstm_out\n        embeds = self.embedding(x)\n        lstm_out, hidden = self.lstm(embeds, hidden)\n    \n        # stack up lstm outputs\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        # dropout and fully-connected layer\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        # sigmoid function\n        sig_out = self.sig(out)\n        \n        # reshape to be batch_size first\n        sig_out = sig_out.view(batch_sizes, -1)\n        sig_out = sig_out[:, -1] # get last batch of labels\n        \n        # return last sigmoid output and hidden state\n        return sig_out, hidden\n    \n    \n    def init_hidden(self, batch_size):\n        ''' Initializes hidden state '''\n        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n        # initialized to zero, for hidden state and cell state of LSTM\n        weight = next(self.parameters()).data\n        \n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n        \n        return hidden\n        ","d506fda2":"#  Instancia o modelo com os par\u00e2metros\nvocab_size = len(vocab_to_int)+1 \noutput_size = 1         # n\u00f3 de saida\nembedding_dim = 200     # cada palavra vai ser convertida em um array de 200\nhidden_dim = 256        # 256 neuronios na hidden layer\nn_layers = 2            # 2 camdas de hidden layer\n\nnet = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n\nprint(net)","756b7f7a":"# Fun\u00e7oes de perda e otimiza\u00e7\u00e3o\nlr=0.001\n\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)","2b9bc296":"# Treina o modelo\nif(not train_on_gpu):\n    raise print('Sem gpu') \nepochs = 1 # 3-4 is approx where I noticed the validation loss stop decreasing\n\ncounter = 0\nprint_every = 100\nclip=5 # gradient clipping\n\n# move model to GPU, if available\nif(train_on_gpu):\n    net.cuda()\n\n# initialize tracker for minimum validation loss\nvalid_loss_min = np.Inf # set initial \"min\" to infinity    \n    \nnet.train()\n# train for some number of epochs\nfor e in range(epochs):\n    # initialize hidden state\n    h = net.init_hidden(batch_size)\n\n    # batch loop\n    for inputs, labels in train_loader:\n        counter += 1\n\n        if(train_on_gpu):\n            inputs, labels = inputs.cuda(), labels.cuda()\n\n        # Creating new variables for the hidden state, otherwise\n        # we'd backprop through the entire training history\n        h = tuple([each.data for each in h])\n\n        # zero accumulated gradients\n        net.zero_grad()\n\n        # get the output from the model\n        output, h = net(inputs, h)\n\n        # calculate the loss and perform backprop\n        loss = criterion(output.squeeze(), labels.float())\n        loss.backward()\n        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs \/ LSTMs.\n        nn.utils.clip_grad_norm_(net.parameters(), clip)\n        optimizer.step()\n\n        # loss stats\n        if counter % print_every == 0:\n            # Get validation loss\n            val_h = net.init_hidden(batch_size)\n            val_losses = []\n            net.eval()\n            for inputs, labels in valid_loader:\n\n                # Creating new variables for the hidden state, otherwise\n                # we'd backprop through the entire training history\n                val_h = tuple([each.data for each in val_h])\n\n                if(train_on_gpu):\n                    inputs, labels = inputs.cuda(), labels.cuda()\n\n                output, val_h = net(inputs, val_h)\n                val_loss = criterion(output.squeeze(), labels.float())\n\n                val_losses.append(val_loss.item())\n                valid_loss = np.mean(val_losses)\n            net.train()\n            print(\"Epoch: {}\/{}...\".format(e+1, epochs),\n                  \"Step: {}...\".format(counter),\n                  \"Loss: {:.6f}...\".format(loss.item()),\n                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n            \n            # save model if validation loss has decreased\n            if valid_loss <= valid_loss_min:\n                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n                valid_loss_min,\n                valid_loss))\n                torch.save(net.state_dict(), 'model.pt')\n                valid_loss_min = valid_loss","ec9ecc07":"#carrega o modelo treinado\nnet.load_state_dict(torch.load('model.pt',map_location='cpu') )","0173536e":"train_on_gpu = False","58427b3d":"# Testa o modelo nos dados de teste\n\ntest_losses = [] # track loss\nnum_correct = 0\n\n# init hidden state\nh = net.init_hidden(batch_size)\n\nnet.eval()\n# iterate over test data\nfor inputs, labels in test_loader:\n\n    # Creating new variables for the hidden state, otherwise\n    # we'd backprop through the entire training history\n    h = tuple([each.data for each in h])\n\n    if(train_on_gpu):\n        inputs, labels = inputs.cuda(), labels.cuda()\n    \n    # get predicted outputs\n    output, h = net(inputs.long(), h)\n    \n    # calculate loss\n    test_loss = criterion(output.squeeze(), labels.float())\n    test_losses.append(test_loss.item())\n    \n    # convert output probabilities to predicted class (0 or 1)\n    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n    \n    # compare predictions to true label\n    correct_tensor = pred.eq(labels.float().view_as(pred))\n    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n    num_correct += np.sum(correct)\n\n# -- stats! -- ##\n# avg test loss\nprint(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n\n# accuracy over all test data\ntest_acc = num_correct\/len(test_loader.dataset)\nprint(\"Test accuracy: {:.3f}\".format(test_acc))","ccb497fb":"# Exemplo de texto que ser\u00e1 testado: negative test review\ntest_review_neg = 'O produto n\u00e3o chegou'","409e0524":"from string import punctuation\n\ndef tokenize_review(test_review):\n    test_review = test_review.lower() # lowercase\n    # get rid of punctuation\n    test_text = ''.join([c for c in test_review if c not in punctuation])\n\n    # splitting by spaces\n    test_words = test_text.split()\n\n    # tokens\n    test_ints = []\n    test_ints.append([vocab_to_int.get(word) for word in test_words if vocab_to_int.get(word) != None ])\n    \n\n    return test_ints","843aebfd":"def predict(net, test_review, sequence_length=200):\n    \n    net.eval()\n    \n    # tokenize review\n    test_ints = tokenize_review(test_review)\n    \n    # pad tokenized sequence\n    seq_length=sequence_length\n    features = pad_features(test_ints, seq_length)\n    \n    # convert to tensor to pass into your model\n    feature_tensor = torch.from_numpy(features)\n    \n    batch_size = feature_tensor.size(0)\n    \n    # initialize hidden state\n    h = net.init_hidden(batch_size)\n    \n    if(train_on_gpu):\n        feature_tensor = feature_tensor.cuda()\n    \n    # get the output from the model\n    output, h = net(feature_tensor.long(), h)\n    \n    \n    # convert output probabilities to predicted class (0 or 1)\n    pred = torch.round(output.squeeze()) \n    # printing output value, before rounding\n    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n    \n    # print custom response\n    if(pred.item()==1):\n        print(\"Positive review detected!\")\n    else:\n        print(\"Negative review detected.\")\n    return feature_tensor","aa9e7d57":"# positive test review\ntest_review_pos = 'bom '\n# negative test review\ntest_review_neg = 'ruim '\n# test otimo, comparar com bom\ntest_review_poscomp = 'otimo'\n# pessimo","1099e63a":"# call function\nseq_length=20 # good to use the length that was trained on\n# positive test review\nbom = predict(net, test_review_pos, seq_length)\n# negative test review\nruim = predict(net, test_review_neg, seq_length)\n# otimo\notimo = predict(net, test_review_poscomp, seq_length)\n# perfeito\nperfeito = predict(net, 'perfeito', seq_length)\npessimo = predict(net, 'pessimo', seq_length)","01ac19d7":"return_bom = net.embedding(bom.long())\nreturn_ruim = net.embedding(ruim.long())\nreturn_otimo = net.embedding(otimo.long())\nreturn_perfeito = net.embedding(perfeito.long())\nreturn_pessimo = net.embedding(pessimo.long())","840ea197":"return_bom[0][19]","a1e0a42d":"return_ruim[0][19]","26eac86f":"return_otimo[0][19]","10a6a4b0":"# return_otimo.detach().numpy()[0][19] ## Converte em Numpy, tem que usar detach primeiro e depois pegar o que voce^ quer\ntensor_dot(return_otimo, return_bom)","7471cb6f":"def tensor_dot(x,y):\n    return np.dot(x.detach().numpy()[0][19], y.detach().numpy()[0][19])\n\n# abs(return_otimo[0][19][0].item() - return_ruim[0][19][0].item())\ndef diff_tensors(x,y):\n    sun = 0\n    for i in range(200):\n        sun += abs(x[0][19][i].item() - y[0][19][i].item())\n    return sun\n\ndef tensor_value(x):\n    sun = 0\n    for i in range(200):\n        sun += x[0][19][i].item()\n    return sun","e03b96d9":"dict_tensors = [('otimo', return_otimo),\n               ('bom', return_bom),\n               ('ruim', return_ruim),\n               ('perfeito', return_perfeito),\n               ('pessimo', return_pessimo),]","fd891a49":"for i in dict_tensors:\n    print(i[0])\n    print(tensor_value(i[1]), '\\n')","36f00373":"tensor_dot(return_otimo, return_bom)","17c95268":"tensor_dot(return_otimo, return_ruim)","f63e8bc1":"tensor_dot(return_otimo, return_perfeito)","3528ac2e":"# Exemplo de chamada de predi\u00e7\u00e3o\nseq_length=20 # good to use the length that was trained on\nprint(test_review_neg)\npredict(net, test_review_neg, seq_length)","7aabb14c":"from sklearn.metrics import confusion_matrix,classification_report","421f39da":"A Etapa anterior treina o modelo, Agora, s\u00f3 vamos carreg\u00e1lo\n\n# CARREGANDO MODELO","c91d6e84":"================================\n\n## Vectorizer PyTorch","74170605":"---\n\n\nPara lidar com avalia\u00e7\u00f5es curtas e muito longas, preenchemos ou truncamos todos os coment\u00e1rios para um tamanho espec\u00edfico. Para coment\u00e1rios mais curtos do que `seq_length`,preenche-se com 0s. Para revis\u00f5es maiores que `seq_length`, s\u00e3o truncadas para as primeiras palavras `seq_length`.\n\nExemplo, se  `seq_length = 10` e um coment\u00e1rio de entrada forem:\n`` \n[117, 18, 128].\n`` \nA sequ\u00eancia resultante  deve ser:\n\n`` \n[0, 0, 0, 0, 0, 0, 0, 117, 18, 128]\n`` \n","fafadd32":"### Codifica as avalia\u00e7\u00f5es\n\nAs avalia\u00e7\u00f5es s\u00e3o numeros de 1 a 5. Para usar esses valores, \u00e9 necessario converte-los para 'positivo' e 'negativo' 0 e 1,respectivamente. Os comentarios com nota 3 ou menor foram classificados como negativos e o restante sendo como positivos.\n","a1912347":"==========================================================","0a5e5130":"## Training, Validation, Test\n\nCom dados formatados,dividimos em conjuntos de treinamento, valida\u00e7\u00e3o e teste.\n\n","46ecd912":"## Clean Data","ee9f1fe1":"<h1 align=\"center\"> Reviewer of product comments in Portuguese <\/h1>\n\n<img src=\"https:\/\/www.univention.com\/wp-content\/uploads\/2018\/11\/check_Marks_AdobeStock_163979731_Zeichenfl%C3%A4che-1-713x273.png\" width=\"50%\" \/>\n\nin progess ....\n\nCreated: 2020-09-15\n\nLast updated: 2020-09-16\n\nKaggle Kernel made by \ud83d\ude80 <a href=\"https:\/\/www.kaggle.com\/rafanthx13\"> Rafael Morais de Assis<\/a>","533adb31":"## Table Of Content (TOC) <a id=\"top\"><\/a>\n\n## Import Libs and DataSet","1c04f72c":"## Split in Train and Test (APPLY STOP WORDS)","7e6b638f":"## Snippets","1931aedd":"## Problem Description\n\nGoal: Given a review of a product in Portuguese, assess whether it is a positive or negative review\n\n[Kaggle Dataset](https:\/\/www.kaggle.com\/olistbr\/brazilian-ecommerce)\n","e0bcbe0f":"## Strategie 2","2627deee":"### Codifica as palavras\n\nCria um dicion\u00e1rio que mapeia as palavras no vocabul\u00e1rio para n\u00fameros inteiros.Em seguida, converte-se cada um dos  comentarios em n\u00fameros inteiros para que possam ser passados para a rede.\n\n","83b309e4":"## EDA","2520ae55":"TESTANDO Embedding","bde92a2e":"## Pr\u00e9-processamento de dados\n\nCodifica cada palavra como um numero inteiro e remove toda a pontua\u00e7\u00e3o.\n","030324cd":"## Text Cleaning"}}