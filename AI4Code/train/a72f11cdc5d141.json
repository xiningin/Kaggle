{"cell_type":{"b5be6e17":"code","81cb4791":"code","5e20ef85":"code","ffb17404":"code","e0dd8d4d":"code","9ed403cf":"code","06f426c9":"code","688ea36a":"code","8a81f46a":"code","3c4ed6d3":"code","80c440c4":"code","05864b22":"code","87e677b7":"code","eea74b05":"code","b41a1cfa":"code","7f04ee9c":"code","48267e05":"code","378f4593":"code","7cc1737b":"code","8f6edade":"code","34e3d333":"code","bbbb23d8":"code","35606d9a":"code","d3a1d875":"code","933f6c79":"code","58355183":"code","8efd93dc":"code","06251d85":"code","7018fec5":"code","3c5e8397":"code","0227df27":"code","72e264ff":"code","47a79676":"code","cb6a18ae":"code","b2389b1e":"code","e849a88b":"code","a2fef695":"code","9b27cfcc":"code","494d7b7b":"code","9f63581e":"code","17b4f002":"code","c17ac0f7":"code","744f2340":"code","3fa87e1b":"code","138648bc":"code","5fabe2c6":"code","38f27e60":"code","2acf39ff":"code","2be3d09c":"code","21082366":"code","6da22bc5":"code","d73a6fbf":"code","4774568e":"code","b024e539":"code","eb6e921f":"code","0e4254e0":"code","13aab96e":"code","dc70fb63":"code","e45e3f8d":"code","f07db327":"code","c9f2405d":"code","2a4b8750":"code","66671bf9":"code","e8296702":"code","c5b521a8":"code","106dd331":"code","01d20b02":"code","e82e22e2":"code","3fe800c3":"code","3f9fc6cc":"code","3c5be471":"code","b9c9f882":"code","ae8cf30c":"code","7eb7903d":"code","af1386c3":"code","89a1b5f2":"code","4ecba4a3":"code","3fafeb0e":"code","0defe440":"code","012c047f":"code","3874eb88":"code","d95a83ff":"code","ec8f84e7":"markdown","5a4f61fd":"markdown","f12d14db":"markdown","dc81a746":"markdown","985cb8c8":"markdown","55168356":"markdown","e2698442":"markdown","3712fdfc":"markdown","bd0c8263":"markdown","ebd26bf0":"markdown","3807ad62":"markdown","00f5b2c1":"markdown","a3ad53c1":"markdown","e486bd8a":"markdown","a2c58dba":"markdown","2bc4f067":"markdown","005bdbe2":"markdown","7a1fee8e":"markdown","bd3ef0ed":"markdown","c3fc8a67":"markdown","32f9249c":"markdown","9d3fe27e":"markdown","5346580c":"markdown","ca2a6e1b":"markdown","84bcbdaf":"markdown","6725fd0e":"markdown","3af4ad4e":"markdown","f98b1373":"markdown","407fabe1":"markdown","89deb238":"markdown","a59ffe5e":"markdown","3c4d46e3":"markdown","77b03581":"markdown","a32c8361":"markdown","ae5ae7b2":"markdown","7c0e65a6":"markdown","8a5b4c7d":"markdown"},"source":{"b5be6e17":"#data proccesing and cleaning \nimport numpy as np\nimport pandas as pd \n#data visulization  \n%matplotlib inline \nimport matplotlib.pyplot as plt \nimport matplotlib as mpl\nimport seaborn as sns \n\n#ml model \nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\n\n\n","81cb4791":"train_df = pd.read_csv('.\/titanic\/train.csv') #just replace dir with (..\/input\/'name of the df')\ntest_df = pd.read_csv('.\/titanic\/test.csv')","5e20ef85":"train_df.info()","ffb17404":"train_df.head(10)","e0dd8d4d":"train_df.describe(include='all').transpose()","9ed403cf":"train_df.isnull().sum()","06f426c9":"train_df.Age","688ea36a":"total = train_df.isnull().sum().sort_values(ascending=False)\nper_1 = train_df.isnull().sum() \/ train_df.isnull().count() *100\nper_2 = (round(per_1,1)).sort_values(ascending=False)\nmissing = pd.concat([total,per_2],axis=1,keys=['total','%'])\nmissing.head()","8a81f46a":"all_count = train_df.isnull().count() #isnull to also conclude Nan values\n","3c4ed6d3":"train_df.columns.values\n","80c440c4":"men = train_df[train_df['Sex']=='male']\nmen","05864b22":"women = train_df[train_df['Sex'] == 'female'].Age.dropna() #Return a new Series with missing values removed\nwomen # use ctrl+i for docs ","87e677b7":"survived = 'Survived' #have value 1\nwasted = 'Wasted' # have value 0\nfig,axes =plt.subplots(nrows=1,ncols=2,figsize=(10,5))\nwomen = train_df[train_df['Sex']=='female']\nmen = train_df[train_df['Sex']=='male']\nax = sns.distplot(women[women['Survived'] == 1].Age.dropna(),bins=15,label=survived,kde=False,ax=axes[0])\nax = sns.distplot(women[women['Survived'] == 0].Age.dropna(),bins=15,label=wasted,kde=False,ax=axes[0])\nax.legend()\nax.set_title('Female')\nax = sns.distplot(men[men['Survived'] == 1].Age.dropna(),bins=18,label=survived,kde=False,ax=axes[1])\nax = sns.distplot(men[men['Survived'] == 0].Age.dropna(),bins=18,label=wasted,kde=False,ax=axes[1])\nax.legend()\nax.set_title('Male')","eea74b05":"sns.barplot(x='Pclass',y='Survived',data=train_df)","b41a1cfa":"grid = sns.FacetGrid(train_df,row='Pclass',col='Survived',height=2.5,aspect=1.5)\ngrid.map(plt.hist,'Age',alpha=.7,bins=20)\ngrid.add_legend()","7f04ee9c":"data = [train_df,test_df]\nfor d in data :\n    d['relatives'] = d['SibSp'] + d['Parch']\n    d.loc[d['relatives']>0, 'not_alone'] = 0\n    d.loc[d['relatives']==0,'not_alone'] = 1\n    d['not_alone'] = d['not_alone'].astype(int)\n\n    ","48267e05":"train_df.info","378f4593":"axes = sns.catplot(x='relatives',y='Survived',data=train_df,aspect=2.5,kind='point')\n","7cc1737b":"#drop passengerId\ntrain_df = train_df.drop(['PassengerId'],axis=1)","8f6edade":"train_df.Cabin","34e3d333":"train_df['Cabin'].fillna('U0')","bbbb23d8":"import re\ndeck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    dataset['Deck'] = dataset['Deck'].map(deck)\n    dataset['Deck'] = dataset['Deck'].fillna(0)\n    dataset['Deck'] = dataset['Deck'].astype(int) ","35606d9a":"data=[train_df,test_df]\nfor dataset in data :\n    mean = dataset['Age'].mean()\n    std = dataset[\"Age\"].std()\n    is_null = dataset['Age'].isnull().sum()\n# compute random numbers between the mean, std and is_null\n    range_num = np.random.randint(mean-std,mean+std,size=is_null)\n# fill NaN values in Age column with random values generated\n    age = dataset[\"Age\"].copy()\n    age[np.isnan(age)]=range_num\n    dataset['Age']=age\n    dataset[\"Age\"] = dataset[\"Age\"].astype(int)\n    ","d3a1d875":"train_df.Age.isnull().sum()","933f6c79":"train_df.Embarked.describe()","58355183":"train_df.Embarked.isnull().sum()","8efd93dc":"#let's replace Nan\ncommon_value = 'S'\ndata=[train_df,test_df]\nfor dataset in data :\n    dataset['Embarked'].fillna(common_value)","06251d85":"train_df.info()","7018fec5":"#converting fare type into int\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Fare'] = dataset['Fare'].fillna(0)\n    dataset['Fare'] = dataset['Fare'].astype(int)","3c5e8397":"train_df[train_df['Fare']==0].count()","0227df27":"train_df.Name","72e264ff":"#Name we need it as aGender \ndata = [train_df, test_df]\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\nfor dataset in data:\n    # extract titles\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    # replace titles with a more common title or as Rare\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    # convert titles into numbers\n    dataset['Title'] = dataset['Title'].map(titles)\n    # filling NaN with 0, to get safe\n    dataset['Title'] = dataset['Title'].fillna(0)","47a79676":"#drop Name\ntrain_df = train_df.drop(['Name'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)","cb6a18ae":"train_df.Title","b2389b1e":"data = [train_df,test_df]\nGenders = {'male':1,'female':0}\nfor dataset in data :\n    dataset['Sex'] = dataset['Sex'].map(Genders)","e849a88b":"train_df.Sex","a2fef695":"train_df.Ticket.describe()","9b27cfcc":"#let's drop ticket columns many unique values\ntrain_df.drop(['Ticket'],axis=1)\n\ntest_df.drop(['Ticket'],axis=1)","494d7b7b":"train_df.drop(['Cabin'],axis=1)","9f63581e":"train_df = train_df.drop(['Cabin'],axis=1)\ntest_df = test_df.drop(['Cabin'],axis=1)","17b4f002":"train_df = train_df.drop(['Ticket'],axis=1)\ntest_df = test_df.drop(['Ticket'], axis=1)","c17ac0f7":"train_df","744f2340":"pla = pd.read_csv('.\/titanic\/train.csv')\npla2 = pd.read_csv('.\/titanic\/test.csv')\ntrain_df['Embarked'] = pla['Embarked']\ntest_df['Embarked'] = pla2['Embarked']\n","3fa87e1b":"train_df","138648bc":"train_df.Embarked=train_df.Embarked.fillna('S')","5fabe2c6":"ports = {\"S\": 0, \"C\": 1, \"Q\": 2}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].map(ports)\n    ","38f27e60":"train_df.isnull().sum()","2acf39ff":"train_df.Embarked","2be3d09c":"train_df.Embarked.isnull().sum()","21082366":"train_df.Embarked[:20]","6da22bc5":"train_df","d73a6fbf":"pla['Age'].describe()","4774568e":"data = [train_df, test_df]\nfor dataset in data:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 66, 'Age'] = 66","b024e539":"# let's see how it's distributed\ntrain_df['Age'].value_counts()","eb6e921f":"train_df","0e4254e0":"train_df['Fare'].describe()","13aab96e":"data = [train_df, test_df]\n\nfor dataset in data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\n    dataset['Fare'] = dataset['Fare'].astype(int)","dc70fb63":"train_df","e45e3f8d":"#how test.csv are doin here\ntest_df","f07db327":"test_df","c9f2405d":"train_df","2a4b8750":"x_train = train_df.drop('Survived',axis=1)\ny_train = train_df['Survived']\nx_test = test_df.drop('PassengerId',axis=1).copy()","66671bf9":"#sgd algorithm \nsgd = linear_model.SGDClassifier(max_iter=5,tol=None)\nsgd.fit(x_train,y_train)\npredictions = sgd.predict(x_test)\nacc_sgd = round(sgd.score(x_train,y_train)*100,2) #the score between x and y ,it's an apgrade of LR\nprint(acc_sgd, \"%\")\n","e8296702":"#randomforest\nrandomForest = RandomForestClassifier(n_estimators=100,oob_score=True)\nrandomForest.fit(x_train,y_train)\nran_predict = randomForest.predict(x_test)\nacc_ran = round(randomForest.score(x_train,y_train)*100,2)\nprint(acc_ran,'%')","c5b521a8":"#LogisticRegression \nlog_reg = LogisticRegression()\nlog_reg.fit(x_train,y_train)\nlog_predict = log_reg.predict(x_test)\nacc_log = round(log_reg.score(x_train,y_train)*100,2)\nprint(acc_log,'%')","106dd331":"# Gaussian Naive Bayes\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\n\nY_pred = gaussian.predict(x_test)\n\nacc_gaussian = round(gaussian.score(x_train, y_train) * 100, 2)\nprint(acc_gaussian, \"%\")","01d20b02":"# Linear SVC\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train, y_train)\n\nY_pred = linear_svc.predict(x_test)\n\nacc_linear_svc = round(linear_svc.score(x_train, y_train) * 100, 2)\nprint(acc_linear_svc, \"%\")","e82e22e2":"# Decision Tree\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(x_train, y_train)\n\nY_pred = decision_tree.predict(x_test)\n\nacc_decision_tree = round(decision_tree.score(x_train, y_train) * 100, 2)\nprint(acc_decision_tree ,\"%\")","3fe800c3":"# KNN\nknn = KNeighborsClassifier(n_neighbors = 2)\nknn.fit(x_train, y_train)\n\nY_pred = knn.predict(x_test)\n\nacc_knn = round(knn.score(x_train, y_train) * 100, 2)\nprint(acc_knn, \"%\")","3f9fc6cc":"Models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes',  \n              'Stochastic Gradient Decent', \n              'Decision Tree'],\n    'Score': [acc_linear_svc, acc_knn, acc_log, \n              acc_ran, acc_gaussian, \n              acc_sgd, acc_decision_tree]})\nmodels_df = Models.sort_values(by='Score',ascending=False)\nmodels_df = models_df.set_index('Score')\nmodels_df","3c5be471":"from sklearn.model_selection import cross_val_score\nforest = RandomForestClassifier(n_estimators=100)\nscore = cross_val_score(forest,x_train,y_train,cv=10,scoring='accuracy')\n","b9c9f882":"score","ae8cf30c":"print(\"Scores:\", score)\nprint(\"Mean:\", score.mean())\nprint(\"Standard Deviation:\", score.std())","7eb7903d":"importances = pd.DataFrame({'feature':x_train.columns,'importance':np.round(randomForest.feature_importances_,3)})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')","af1386c3":"importances","89a1b5f2":"importances.plot.bar()","4ecba4a3":"train_df  = train_df.drop(\"not_alone\", axis=1)\ntest_df  = test_df.drop(\"not_alone\", axis=1)\n\ntrain_df  = train_df.drop(\"Parch\", axis=1)\ntest_df  = test_df.drop(\"Parch\", axis=1)","3fafeb0e":"random_forest = RandomForestClassifier()","0defe440":"print(\"oob score:\", round(randomForest.oob_score_, 4)*100, \"%\")","012c047f":"#random forest \nrandom_forest = RandomForestClassifier(\n    n_estimators=100,\n    criterion='gini',\n    oob_score=True,\n    min_samples_split=10,\n    min_samples_leaf=1,\n    max_features='auto',\n    random_state=1,\n    n_jobs=-1 )\nrandom_forest.fit(x_train,y_train)\ny_predict = random_forest.predict(x_test)\nprint('accuracy: ',round(random_forest.score(x_train,y_train)*100,2),'%')\nprint(\"oob score:\", round(random_forest.oob_score_, 4)*100, \"%\")","3874eb88":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": y_predict\n    })\nsubmission.to_csv('submission.csv', index=False)","d95a83ff":"pd.read_csv('.\/submission.csv')","ec8f84e7":"## sibsp & parch 're all the relatives  've on board let's combine them and see if a family survive together or not\n","5a4f61fd":"# **Submission**","f12d14db":"**abig family burdens you! have 1 kid only bro ^_^**","dc81a746":"## ymm!! we still only have Faire to categorize\n","985cb8c8":"This looks much more realistic than before. Our model has a average accuracy of 82% with a standard deviation of 4 %. The standard deviation shows us, how precise the estimates are .\n\nThis means in our case that the accuracy of our model can differ + - 4%.","55168356":"## convert Embarked feature into numeric\n","e2698442":"## convert Sex into numeric ","3712fdfc":"**ithink passengerId ,name will not be helpful as feature**","bd0c8263":"## let's explore out data ","ebd26bf0":"## Name","3807ad62":"## Feature Importance\n\nAnother great quality of random forest is that  they make it very easy to measure the relative importance of each feature.","00f5b2c1":"## Bam!! looks like the female youth are the luckist here ","a3ad53c1":"**let's stick with RandomForest**","e486bd8a":"**cabin 1st letter coresponds to adick floor number** ","a2c58dba":"# k-fold CrossValidation","2bc4f067":"# convert features into numerical categories \n","005bdbe2":"## not_alone & Parch doesn't play asignificant role let's get rid of it ","7a1fee8e":"# Data processing","bd3ef0ed":"### precent% of missing data per columns ","c3fc8a67":" **imade this notebook at local host #just replace dir with (..\/input\/'name of the df')**","32f9249c":"## we must handle missing value <br>\n","9d3fe27e":"## Tickets","5346580c":"**people in the first class seems to survive more that's anew info for me thx alot sns**","ca2a6e1b":"### Emparked ","84bcbdaf":"**Age can be handeled but cabin needs further understanding**","6725fd0e":"## let's play with ML models ,we are doin' magic here ^_^","3af4ad4e":"## Creating Categories:\n\nWe will now create categories within the following features:\n\n### Age:\nNow we need to convert the 'age' feature. First we will convert it from float into integer. Then we will create the new 'AgeGroup\" variable, by categorizing every age into a group. Note that it is important to place attention on how you form these groups, since you don't want for example that 80% of your data falls into group 1.","f98b1373":"**iam ok with these features**","407fabe1":"\n*> this notebook isn't apure work of me igot some help\n*","89deb238":"# Hyperparameter Tunning ","a59ffe5e":" the more features you have, the more likely your model will suffer from overfitting and vice versa. But I think our data looks fine for now and hasn't too much features.\n\nThere is also another way to evaluate a random-forest classifier, which is probably much more accurate than the score we used before. What I am talking about is the **out-of-bag samples** to estimate the generalization accuracy.","3c4d46e3":"### Age\n**let's replace Nan values with random values across the mean and( + or -) std**","77b03581":"**this assures that a young person of class=1 will survive the most** ","a32c8361":"# Building ML Models","ae5ae7b2":"### #cabin needs to be dropped coz we replace it with deck","7c0e65a6":"## comparing between the best Models","8a5b4c7d":"K-Fold Cross Validation randomly splits the training data into K subsets called folds. Let's image we would split our data into 4 folds (K = 4). Our random forest model would be trained and evaluated 4 times, using a different fold for evaluation everytime, while it would be trained on the remaining 3 folds. \n**K-Fold Cross Validation repeats this process till every fold acted once as an evaluation fold.**\nThe result of our K-Fold Cross Validation example would be an array that contains 4 different scores. We then need to compute the mean and the standard deviation for these scores. "}}