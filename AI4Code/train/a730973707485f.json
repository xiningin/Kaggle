{"cell_type":{"b58cbb39":"code","9c9bd488":"code","27155845":"code","45d58724":"code","3208bcbe":"code","3809721e":"code","97fe3293":"code","a4b69be1":"code","400e1285":"code","2c483795":"code","4b25266e":"code","3b5e3fdf":"code","556f41dd":"code","9738de39":"code","64993593":"code","180190db":"markdown","6a96bdb1":"markdown","3eb62f83":"markdown","18d12cb8":"markdown","2f302530":"markdown","8ff40c2c":"markdown","2c44bc3b":"markdown"},"source":{"b58cbb39":"import numpy as np\nimport pandas as pd\ntrain = pd.read_csv('..\/input\/cat-in-the-dat-ii\/train.csv')\ntest = pd.read_csv('..\/input\/cat-in-the-dat-ii\/test.csv')\ntrain.sort_index(inplace=True)\ntrain_y = train['target']; test_id = test['id']\ntrain.drop(['target', 'id'], axis=1, inplace=True); test.drop('id', axis=1, inplace=True)\nfrom sklearn.metrics import roc_auc_score\ncat_feat_to_encode = train.columns.tolist();  smoothing=0.20\nimport category_encoders as ce\noof = pd.DataFrame([])\nfrom sklearn.model_selection import StratifiedKFold\nfor tr_idx, oof_idx in StratifiedKFold(n_splits=5, random_state=2020, shuffle=True).split(train, train_y):\n    ce_target_encoder = ce.TargetEncoder(cols = cat_feat_to_encode, smoothing=smoothing)\n    ce_target_encoder.fit(train.iloc[tr_idx, :], train_y.iloc[tr_idx])\n    oof = oof.append(ce_target_encoder.transform(train.iloc[oof_idx, :]), ignore_index=False)\nce_target_encoder = ce.TargetEncoder(cols = cat_feat_to_encode, smoothing=smoothing)\nce_target_encoder.fit(train, train_y)\ntrain = oof.sort_index()\ntest = ce_target_encoder.transform(test)","9c9bd488":"from sklearn import linear_model\nglm = linear_model.LogisticRegression( random_state=1, solver='lbfgs', max_iter=2020, fit_intercept=True, penalty='none', verbose=0); glm.fit(train, train_y)","27155845":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC, LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","45d58724":"n_folds = 5\ndef auc_score(model):\n    kf = KFold( n_folds, shuffle= True).get_n_splits(train.values)\n    auc_score = cross_val_score(model, train.values, train_y, scoring = \"roc_auc\", cv = kf)\n    return auc_score","3208bcbe":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","3809721e":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n","97fe3293":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","a4b69be1":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","400e1285":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","2c483795":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","4b25266e":"score = auc_score(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = auc_score(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = auc_score(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\nscore = auc_score(glm)\nprint(\"Logistic Regression: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\n# score = auc_score(KRR)\n# print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n# score = auc_score(GBoost)\n# print(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n# score = auc_score(model_xgb)\n# print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","3b5e3fdf":"class average_stacking(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self,models):\n        self.models = models\n    def fit(self, x,y):\n        self.model_clones = [clone(x) for x in self.models]\n        \n        for model in self.model_clones:\n            model.fit(x,y)\n        return self\n    def predict(self, x):\n        preds = np.column_stack([\n            model.predict(x) for model in self.model_clones\n        ])\n        return np.mean(preds, axis = 1)","556f41dd":"averaged_models = average_stacking(models = (ENet, glm,model_lgb, lasso))\n\nscore = auc_score(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","9738de39":"averaged_models.fit(train.values, train_y)\navg_pred = averaged_models.predict(test)","64993593":"pd.DataFrame({'id': test_id, 'target': avg_pred}).to_csv('submission.csv', index=False)","180190db":"**I am currently working on adding a meta-model stacking pipeline here as well, so stay tuned.\n**\n\nIf you learnt something on reading this, feel free to upvote!","6a96bdb1":"**Function for cross val**","3eb62f83":"# Introduction :\nThis is a kernel to get newcomers an introduction to stacking. Here we use the simplest stacking technique (Averaging base models), which basically averages the output predictions of multiple base models as demonstrated in this [kernel](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard#Modelling).\n\nCredits for the target encoding goes to our very own [caesarlupum](https:\/\/www.kaggle.com\/caesarlupum\/2020-20-lines-target-encoding)\n\nI have all but combined these kernels in order to teach and implement stacking for myself.","18d12cb8":"**Base model scores**","2f302530":"**Base models**","8ff40c2c":"**Stacking models**","2c44bc3b":"# Modelling"}}