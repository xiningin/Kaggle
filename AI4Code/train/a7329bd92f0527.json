{"cell_type":{"12e00cf6":"code","95fc29e8":"code","82efe988":"code","a7f5d06b":"code","683df0e7":"code","0b57dd88":"code","65b0c9ee":"code","5a39b0ce":"code","ead19dd1":"code","6c979abb":"code","3b842124":"code","f5d087a4":"code","50977560":"code","4e4baed1":"code","ad74648b":"code","5c519ad2":"code","97259b02":"code","97d21d94":"code","4a82f742":"code","1242d5a2":"code","0c4c3218":"code","c172de3b":"code","4e17b065":"code","1008ffcf":"code","88a1096a":"code","eb1d5083":"code","f2876f95":"code","b1e15e4a":"code","2ab5cb37":"code","3f33799a":"code","756ece85":"code","40198e26":"code","8df93e05":"code","2d801f64":"code","3378c0c1":"code","7d5434e8":"code","115474a6":"code","dae8d206":"code","6d03934e":"code","b3b9fb5e":"code","29c6a19e":"code","ab249168":"code","cbd8dbb9":"code","69b0a705":"code","b66cee18":"code","bc9130f9":"markdown","90c7aaa1":"markdown","30de6e22":"markdown","a5622aed":"markdown","9b19aac3":"markdown","59568792":"markdown","e6ce7da0":"markdown","ae7f8e9b":"markdown","a517b554":"markdown","2a9e438f":"markdown","28df52e5":"markdown","f62971e2":"markdown","f89c415a":"markdown","2c968725":"markdown","5bb78998":"markdown","bac694ed":"markdown","a69650b1":"markdown","0ceb1cec":"markdown","04be531d":"markdown","95280f52":"markdown","b6125237":"markdown","352fd0e9":"markdown","1f0eb758":"markdown","bbf221a1":"markdown","beb82ee1":"markdown","ca6e7fe9":"markdown","a6d8b571":"markdown","4af7b277":"markdown","961b0b81":"markdown","4d152a68":"markdown","f3cb02b8":"markdown","efb033e3":"markdown","0d796128":"markdown","3bc5ec07":"markdown","73722d0e":"markdown","797a7722":"markdown","25e25de4":"markdown","ad14afb7":"markdown","7632027f":"markdown","8734e3cd":"markdown","0f1a9587":"markdown"},"source":{"12e00cf6":"#Import Libraries\nimport numpy as np \nimport pandas as pd \n\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix\n\n","95fc29e8":"train = pd.read_csv('kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('kaggle\/input\/titanic\/test.csv')\nall_data = [train,test]","82efe988":"train.head()\n\n","a7f5d06b":"train.info()","683df0e7":"train.describe()\n","0b57dd88":"print (train[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean())\n","65b0c9ee":"print (train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean())\n","5a39b0ce":"grid = sns.FacetGrid(train, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();\n","ead19dd1":"print(pd.crosstab(train.Survived, train.Pclass))\n\nplt.figure(figsize=(12,5))\n\nsns.countplot(x=\"Pclass\", data=train, hue=\"Survived\",palette=\"hls\")\nplt.xlabel('PClass',fontsize=17)\nplt.ylabel('Count', fontsize=17)\nplt.title('Class Distribuition by Survived or not', fontsize=20)\n\nplt.show()\n","6c979abb":"for dataset in all_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\nprint (train[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean())","3b842124":"print(pd.crosstab(train.FamilySize, train.Survived))\nsns.factorplot(x=\"FamilySize\",y=\"Survived\", data=train, kind=\"bar\",size=6, aspect=1.6)\nplt.show()\n","f5d087a4":"train = train.drop(['Parch', 'SibSp'], axis=1)\ntest = test.drop(['Parch', 'SibSp'], axis=1)\nall_data = [train, test]\n\ntrain.head()\n","50977560":"g = sns.FacetGrid(train, col='Survived')\ng.map(plt.hist, 'Age', bins=25)\n","4e4baed1":"print(pd.crosstab(train.Survived,train.Sex))\n\nplt.figure(figsize=(12,5))\nsns.countplot(x=\"Sex\", data=train, hue=\"Survived\",palette=\"hls\")\nplt.title('Sex Distribuition by survived or not', fontsize=20)\nplt.xlabel('Sex Distribuition',fontsize=17)\nplt.ylabel('Count', fontsize=17)\n\nplt.show()\n","ad74648b":"print(pd.crosstab(train.Survived, train.Embarked))\n\nplt.figure(figsize=(12,5))\n\nsns.countplot(x=\"Embarked\", data=train, hue=\"Survived\",palette=\"hls\")\nplt.title('Class Distribuition by survived or not',fontsize=20)\nplt.xlabel('Embarked',fontsize=17)\nplt.ylabel('Count', fontsize=17)\n\nplt.show()\n","5c519ad2":"print('Shape Befor drop: ', train.shape)\ntrain = train.drop(['Ticket', 'Cabin'], axis=1)\ntest = test.drop(['Ticket', 'Cabin'], axis=1)\nall_data = [train, test]\nprint('Shape After drop: ',train.shape)\n","97259b02":"for dataset in all_data:\n    # Mapping Sex\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\ntrain.head()","97d21d94":"guess_ages = np.zeros((2,3))\nguess_ages\n","4a82f742":"for dataset in all_data:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_data = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_data.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain.head()\n","1242d5a2":"train['Age_group'] = pd.cut(train['Age'], 5)\ntrain[['Age_group', 'Survived']].groupby(['Age_group'], as_index=False).mean().sort_values(by='Age_group', ascending=True)\n","0c4c3218":"for dataset in all_data:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ntrain.head()\n","c172de3b":"train = train.drop(['Age_group'], axis=1)\nall_data = [train, test]\ntrain.head()\n","4e17b065":"for dataset in all_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\nprint (train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean())\n\n","1008ffcf":"for dataset in all_data:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain.head()\n","88a1096a":"train = train.drop(['Fare'], axis=1)\ntest = test.drop(['Fare'], axis=1)\nall_data = [train,test]\ntrain.head()","eb1d5083":"# retain the new Title feature for model training.\nfor dataset in all_data:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train['Title'], train['Sex'])\n","f2876f95":"for dataset in all_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\n","b1e15e4a":"plt.figure(figsize=(12,5))\n\n#Plotting the result\nsns.countplot(x='Title', data=train, palette=\"hls\")\nplt.xlabel(\"Title\", fontsize=16) #seting the xtitle and size\nplt.ylabel(\"Count\", fontsize=16) # Seting the ytitle and size\nplt.title(\"Title Name Count\", fontsize=20) \nplt.xticks(rotation=45)\nplt.show()\n","2ab5cb37":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in all_data:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain.head()\n","3f33799a":"train = train.drop(['Name', 'PassengerId'], axis=1)\ntest = test.drop(['Name'], axis=1)\nall_data = [train, test]\n","756ece85":"train.head()\n","40198e26":"test.head()","8df93e05":"X_train = train.drop(\"Survived\", axis=1)\nY_train = train[\"Survived\"]\nX_test  = test.drop(\"PassengerId\", axis=1).copy()\n","2d801f64":"#Apply RandomForestClassifier\nrandom_forest= RandomForestClassifier(n_estimators=100,\n                             max_features='auto',\n                             criterion='entropy',\n                             max_depth=10)\nrandom_forest.fit(X_train, Y_train)\n\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(round(acc_random_forest,2,), \"%\")\n\n\n","3378c0c1":"#Apply GradientBoostingClassifier\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nclf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n                                 max_depth=1, random_state=0).fit(X_train, Y_train)\ny_prediction= clf.predict(X_test)\nclf.score(X_train, Y_train)\nacc_clf = round(clf.score(X_train, Y_train) * 100, 2)\nprint(round(acc_clf,2,), \"%\")\n","7d5434e8":"#Apply LGBMClassifier\n\nfrom lightgbm import LGBMClassifier\nmodel = LGBMClassifier().fit(X_train, Y_train)\ny_predict= model.predict(X_test)\nmodel.score(X_train, Y_train)\nacc_model = round(model.score(X_train, Y_train) * 100, 2)\nprint(round(acc_model,2,), \"%\")\n\n","115474a6":"#Apply Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\n\nY_pred = logreg.predict(X_test)\n\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nprint(round(acc_log,2,), \"%\")\n","dae8d206":"# Apply Decision Tree\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\n\nY_pred = decision_tree.predict(X_test)\n\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nprint(round(acc_decision_tree,2,), \"%\")\n","6d03934e":"from xgboost import XGBClassifier\n\nparams_xgb = {'colsample_bylevel': 0.7, 'learning_rate': 0.03, 'max_depth': 3, \n              'n_estimators': 400, 'reg_lambda': 15, 'subsample': 0.5}\nxgb = XGBClassifier(**params_xgb)\ny_preds = xgb.fit(X_train, Y_train).predict(X_test)\nacc_xgb = round(xgb.score(X_train, Y_train) * 100, 2)\nprint(round(acc_xgb,2,), \"%\")\n\n","b3b9fb5e":"results = pd.DataFrame({\n    'Model': ['LGBMClassifier', 'Logistic Regression', \n              'Random Forest', 'Boosting', \n              'Decision Tree','xgb'],\n    'Score': [ acc_model,acc_log,\n              acc_random_forest, acc_clf,\n              acc_decision_tree,acc_xgb]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head(7)\n","29c6a19e":"from sklearn.model_selection import cross_val_score\nrf = RandomForestClassifier(n_estimators=100)\nscores = cross_val_score(rf, X_train, Y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())\n","ab249168":"importances = pd.DataFrame({'feature':X_train.columns,'importance':np.round(random_forest.feature_importances_,3)})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\nimportances.head(15)\n\n","cbd8dbb9":"importances.plot.bar()\n","69b0a705":"params_xgb = {'colsample_bylevel': 0.7, 'learning_rate': 0.03, 'max_depth': 3, \n              'n_estimators': 400, 'reg_lambda': 15, 'subsample': 0.5}\nxgb = XGBClassifier(**params_xgb)\n\ny_preds = xgb.fit(X_train, Y_train).predict(X_test)\nprint(\"Score: \",xgb.score, 4*100, \"%\")\n\n\n\n","b66cee18":"submission = pd.DataFrame({\n        \"PassengerId\": test['PassengerId'],\n        \"Survived\":  y_preds\n    })\n\nsubmission.to_csv('submission.csv', index=False)\n","bc9130f9":"### Test","90c7aaa1":"## 2. Pclass","30de6e22":"### Another graph for representation","a5622aed":"## 3. Cleaning","9b19aac3":"### Above we can see that: \n1. Infants (Age <=4) had high survival rate.\n2. Oldest passengers (Age = 80) survived.\n3. Large number of 15-25 year olds did not survive.\n4. Most passengers are in 15-35 age range.","59568792":"### 4. Age","e6ce7da0":"It's meaning that the females were rescued at the expense of the males\n","ae7f8e9b":"## 5. Sex","a517b554":"### Prop5: categorical feature (Embarked)\n\nWe can now convert the EmbarkedFill","2a9e438f":"It's meaning that dies to mens are much higher than female\n","28df52e5":"### Prop6: Name \n- Most titles band Age groups accurately. For example: Master title has Age mean of 5 years.\n- Survival among Title Age bands varies slightly.\n- Certain titles mostly survived (Mme, Lady, Sir) or did not (Don, Rev, Jonkheer).","f62971e2":"## Feature Importance","f89c415a":"## 4. Applying ML Models:","2c968725":"We note that we have: \n1. mix of numeric and alphanumeric data types in Ticket column\n2. Categorical data in Embarked and Sex columns\n","5bb78998":"## 2. Assessment Data","bac694ed":"The title was the most impact in survive! ","a69650b1":"### Prop4: Missing value in Embarked:\nOur training dataset has two missing values. We simply fill these with the most common occurance.\n\n","0ceb1cec":"## 1. Sex","04be531d":"## Exploration\n\n","95280f52":"We can convert the categorical titles to ordinal.\n\n","b6125237":"### Prop2: categorical feature (Sex)\n\nLet us start by converting Sex feature to a new feature called Gender where female=1 and male=0.\n\n","352fd0e9":"## 6. Embarked","1f0eb758":"### 3. SibSp and Parch","bbf221a1":" ### Prop3: Missing value in Age\n \nWe have plenty of missing values in this feature. \nWe can generate random numbers between (mean - std) and (mean + std). then we part age into 5 range.\n\n","beb82ee1":"### The Best Model?\n","ca6e7fe9":"Looking the graphs, is clear that 3st class and Embarked at Southampton have a high probabilities to not survive\n\n","a6d8b571":"#### Let us replace Age with ordinals based on these groups.\n\n","4af7b277":"## 1. Introduction\n\n> This is my first work of machine learning in kaggle. the notebook is written in python. \nIn this kernel I will go through the whole process of creating a machine learning model on the famous Titanic dataset, which is used by many people all over the world.\n\n> feel free to fork this kernel to play around with the code and test it for yourself. If you plan to use any part of this code, please reference this kernel! I will be glad to answer any questions you may have in the comments. Thank You!\n","961b0b81":"Here we can create a new feature who contain SibSp(siblings\/spouse ) and Parch (children\/parents) family_size","4d152a68":"### Prop6: Fare\nI think Fare column it's not important so i decided to drop this","f3cb02b8":"We have a new probleme here, there is missing value in columns('Age' - 'Cabine' - 'Embarked')","efb033e3":"We can replace many titles with a more common name","0d796128":"## Gathering Data\n\n> We downloaded two files (train.csv) & (test.csv) and We have to read them","3bc5ec07":"Most family suvived who are consisting of Four people","73722d0e":"## Problems:\n - Missing Value in (Age, Cabin, Embarked)\n \n - Categorical data in columns (Sex, Embarked)\n \n - mix of numeric and alphanumeric data types in Ticket column\n \n - Name feature may contain errors or typos as there are several ways used to describe a name including titles, round brackets, and quotes used for alternative or short names.\n \n ","797a7722":"#### remove the Age_group column.","25e25de4":"### Submission\n","ad14afb7":"### Show Titles names in a graph","7632027f":"### Prop1: Cabine and Ticket Data[](http:\/\/)\n* I think that the Cabin and Ticket features not impact in survive so I decided to drob these features","8734e3cd":"Let us drop Parch, SibSpeatures in favor of FamilySize.\n\n","0f1a9587":"- we can drop the Name feature now from our data.\n- We also don't need the PassengerId column in the training dataset."}}