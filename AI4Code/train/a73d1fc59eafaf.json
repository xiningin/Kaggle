{"cell_type":{"3341eddc":"code","0d0287ae":"code","a2b6bae5":"code","e685a902":"markdown"},"source":{"3341eddc":"import numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer","0d0287ae":"class WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_hidden_states):\n        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) \/ self.layer_weights.sum()\n        return weighted_average\n    \nclass LSTMPooling(nn.Module):\n    def __init__(self, num_layers, hidden_size, hiddendim_lstm):\n        super(LSTMPooling, self).__init__()\n        self.num_hidden_layers = num_layers\n        self.hidden_size = hidden_size\n        self.hiddendim_lstm = hiddendim_lstm\n        self.lstm = nn.LSTM(self.hidden_size, self.hiddendim_lstm, batch_first=True)\n        self.dropout = nn.Dropout(0.1)\n    \n    def forward(self, all_hidden_states):\n        ## forward\n        hidden_states = torch.stack([all_hidden_states[layer_i][:, 0].squeeze()\n                                     for layer_i in range(1, self.num_hidden_layers+1)], dim=-1)\n        hidden_states = hidden_states.view(-1, self.num_hidden_layers, self.hidden_size)\n        out, _ = self.lstm(hidden_states, None)\n        out = self.dropout(out[:, -1, :])\n        return out\n\nclass AttentionPooling(nn.Module):\n    def __init__(self, num_layers, hidden_size, hiddendim_fc):\n        super(AttentionPooling, self).__init__()\n        self.num_hidden_layers = num_layers\n        self.hidden_size = hidden_size\n        self.hiddendim_fc = hiddendim_fc\n        self.dropout = nn.Dropout(0.1)\n\n        q_t = np.random.normal(loc=0.0, scale=0.1, size=(1, self.hidden_size))\n        self.q = nn.Parameter(torch.from_numpy(q_t)).float().cuda()\n        w_ht = np.random.normal(loc=0.0, scale=0.1, size=(self.hidden_size, self.hiddendim_fc))\n        self.w_h = nn.Parameter(torch.from_numpy(w_ht)).float().cuda()\n\n    def forward(self, all_hidden_states):\n        hidden_states = torch.stack([all_hidden_states[layer_i][:, 0].squeeze()\n                                     for layer_i in range(1, self.num_hidden_layers+1)], dim=-1)\n        hidden_states = hidden_states.view(-1, self.num_hidden_layers, self.hidden_size)\n        out = self.attention(hidden_states)\n        out = self.dropout(out)\n        return out\n\n    def attention(self, h):\n        v = torch.matmul(self.q, h.transpose(-2, -1)).squeeze(1)\n        v = F.softmax(v, -1)\n        v_temp = torch.matmul(v.unsqueeze(1), h).transpose(-2, -1)\n        v = torch.matmul(self.w_h.transpose(1, 0), v_temp).squeeze(2)\n        return v","a2b6bae5":"class ModelWithPooling(nn.Module):\n    def __init__(self, base_model, pooling='mean'):\n        super().__init__()\n\n        self.pooling = pooling\n        \n        config = AutoConfig.from_pretrained(base_model)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        self.base_model = AutoModel.from_pretrained(base_model, config=config)\n        \n        if pooling == 'mean_max':\n            self.logits = nn.Linear(config.hidden_size*2, 1)\n            \n        elif pooling == 'conv':\n            self.cnn1 = nn.Conv1d(config.hidden_size, 256, kernel_size=2, padding=1)\n            self.cnn2 = nn.Conv1d(256, 1, kernel_size=2, padding=1)\n            \n        elif pooling == 'concat':\n            self.logits = nn.Linear(config.hidden_size*4, 1)\n            \n        elif pooling == 'weighted_layer':\n            layer_start = 9\n            self.pooler = WeightedLayerPooling(\n                config.num_hidden_layers, \n                layer_start=layer_start, \n                layer_weights=None\n            )\n            self.logits = nn.Linear(config.hidden_size, 1)\n            \n        elif pooling == 'lstm':\n            hiddendim_lstm=256\n            self.pooler = LSTMPooling(\n                config.num_hidden_layers, \n                config.hidden_size,\n                hiddendim_lstm\n            )\n            self.logits = nn.Linear(hiddendim_lstm, 1)\n            \n        elif pooling == 'attention':\n            hiddendim_fc=128\n            self.pooler = AttentionPooling(\n                config.num_hidden_layers, \n                config.hidden_size,\n                hiddendim_fc\n            )\n            self.logits = nn.Linear(hiddendim_fc, 1)\n            \n        else:\n            self.logits = nn.Linear(config.hidden_size, 1)\n\n    def forward(self, input_ids, attention_mask):\n        base_model_output = self.base_model(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        if self.pooling == 'mean':\n            last_hidden_state = base_model_output.hidden_states[-1]\n            input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float() \n            sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n            sum_mask = input_mask_expanded.sum(1)\n            sum_mask = torch.clamp(sum_mask, min=1e-9)\n            mean_embeddings = sum_embeddings \/ sum_mask\n            logits = self.logits(mean_embeddings)\n            \n        elif self.pooling == 'max':\n            last_hidden_state = base_model_output.hidden_states[-1]\n            input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n            last_hidden_state[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value\n            max_embeddings = torch.max(last_hidden_state, 1)[0]\n            logits = self.logits(max_embeddings)\n            \n        elif self.pooling == 'mean_max':\n            last_hidden_state = base_model_output.hidden_states[-1]\n            mean_pooling_embeddings = torch.mean(last_hidden_state, 1)\n            max_pooling_embeddings = torch.max(last_hidden_state, 1)[0]\n            mean_max_embeddings = torch.cat((mean_pooling_embeddings, max_pooling_embeddings), 1)\n            logits = self.logits(mean_max_embeddings)\n            \n        elif self.pooling == 'conv':\n            last_hidden_state = base_model_output.hidden_states[-1]\n            last_hidden_state = last_hidden_state.permute(0, 2, 1)   \n            cnn_embeddings = F.relu(self.cnn1(last_hidden_state))\n            cnn_embeddings = self.cnn2(cnn_embeddings)\n            logits = torch.max(cnn_embeddings, 2)[0]\n            \n        elif self.pooling == 'concat':\n            all_hidden_states = torch.stack(base_model_output[2])\n            concatenate_pooling = torch.cat(\n                (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]),-1\n            )\n            concatenate_pooling = concatenate_pooling[:, 0]\n            logits = self.logits(concatenate_pooling)\n        \n        elif self.pooling == 'weighted_layer':\n            all_hidden_states = torch.stack(base_model_output[2])\n            weighted_pooling_embeddings = self.pooler(all_hidden_states)\n            weighted_pooling_embeddings = weighted_pooling_embeddings[:, 0]\n            logits = self.logits(weighted_pooling_embeddings)\n            \n        elif self.pooling == 'lstm':\n            all_hidden_states = torch.stack(base_model_output[2])\n            lstm_pooling_embeddings = self.pooler(all_hidden_states)\n            logits = self.logits(lstm_pooling_embeddings)\n        \n        elif self.pooling == 'attention':\n            all_hidden_states = torch.stack(base_model_output[2])\n            attention_pooling_embeddings  = self.pooler(all_hidden_states)\n            logits = self.logits(attention_pooling_embeddings)\n            \n        else:\n            raise ValueError('Incorrect pooler specified.')\n            \n        return logits","e685a902":"This notebook introduces model with various poolers available.\n\nIt's based on https:\/\/www.kaggle.com\/rhtsingh\/utilizing-transformer-representations-efficiently notebook.\n\n**If you liked this notebook, please upvote it!**"}}