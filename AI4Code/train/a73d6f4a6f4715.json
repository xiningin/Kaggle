{"cell_type":{"434420e6":"code","b00ab96a":"code","48b5e988":"code","002944d8":"code","7bf72a3e":"code","a7298470":"code","77f6554b":"code","e2b47da9":"code","0a13a30a":"code","ae367571":"code","7bd1b201":"markdown","cd0bb45c":"markdown","552337f1":"markdown","5042b3b4":"markdown","4ad995e8":"markdown","1d8865eb":"markdown","6885823a":"markdown","29263a42":"markdown"},"source":{"434420e6":"from datetime import datetime\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\n\nimport tensorflow as tf, re, math\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\n\nTF_CPP_MIN_LOG_LEVEL = 2\n\nimport gc","b00ab96a":"DEVICE = \"TPU\" #or \"GPU\"","48b5e988":"\nif DEVICE == \"TPU\":\n    print(\"connecting to TPU...\")\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        print(\"Could not connect to TPU\")\n        tpu = None\n\n    if tpu:\n        try:\n            print(\"initializing  TPU ...\")\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n            print(\"TPU initialized\")\n        except _:\n            print(\"failed to initialize TPU\")\n    else:\n        DEVICE = \"GPU\"\n\nif DEVICE != \"TPU\":\n    print(\"Using default strategy for CPU and single GPU\")\n    strategy = tf.distribute.get_strategy()\n\nif DEVICE == \"GPU\":\n    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n    \n\nAUTO     = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","002944d8":"datenow = datetime.now().strftime('%d%m%Y_%H%M%S')  # will be appended to the oof and submission\nmodelname = 'nn' # will be appended to the oof and submission\n\nn_folds = 5      # will be appended to the oof and submission\n\nbatch_size = 1024\nepochs = 30\ninitial_learning_rate = 1e-5\ndecay_rate = 1e-8","7bf72a3e":"def visualize_history(history):\n    # Setting Parameters\n    auc = history.history['auc']\n    val_auc = history.history['val_auc']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs = range(len(auc))\n\n    # 1) AUC Plt\n    plt.plot(epochs, auc, 'bo' ,label = 'training auc')\n    plt.plot(epochs, val_auc, 'r' , label= 'validation auc')\n    plt.title('Training and Validation auc')\n    plt.legend()\n\n    plt.figure()\n\n    # 2) Loss Plt\n    plt.plot(epochs, loss, 'bo' ,label = 'training loss')\n    plt.plot(epochs, val_loss, 'r' , label= 'validation loss')\n    plt.title('Training and Validation loss')\n    plt.legend()","a7298470":"train = pd.read_csv(f'..\/input\/stratifiedkfoldsplits-oct2021\/Stratified{n_folds}Fold_OCT2021_TPS.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/test.csv')\n\nnumerical_cols = [col for col in train if col.startswith('f')]\ntarget_col = 'target'\n\nfor c in numerical_cols:\n    prep = StandardScaler()\n    train[c] = prep.fit_transform(train[[c]])\n    test[c] = prep.transform(test[[c]])\n\nX_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\nX_test = test.drop('id', axis=1)\n\ndel train,test\n_ = gc.collect()","77f6554b":"y_preds = []\nmodels = []\noof_train = np.zeros((len(X_train),))\n\nfor fold in range(n_folds):\n    \n    print(f'************************** FOLD {fold + 1} **************************')\n    \n    train_index = X_train['Fold'] != fold\n    valid_index = X_train['Fold'] == fold\n    \n    X_tr = X_train.loc[train_index][numerical_cols]\n    X_val = X_train.loc[valid_index][numerical_cols]\n    y_tr = y_train.loc[train_index]\n    y_val = y_train.loc[valid_index]\n\n    \n    model = keras.Sequential([\n        layers.Dense(256, activation='relu'),\n        layers.Dropout(0.2),\n        layers.Dense(128, activation='relu'),\n        layers.Dropout(0.2),\n        layers.Dense(1, activation='sigmoid'),\n    ])\n    \n    model.compile(\n        optimizer='adam',\n        loss='binary_crossentropy',\n        metrics=['AUC']\n    )\n    \n    early_stopping = keras.callbacks.EarlyStopping(\n        monitor='val_auc',\n        patience=10,\n        mode = 'max',\n        verbose=1,\n        restore_best_weights=True\n    )\n    \n    # from @shivansh002 's excellent notebook https:\/\/www.kaggle.com\/shivansh002\/i-am-groot-tpu-war\n    scheduler = ExponentialDecay(initial_learning_rate, 400*((len(train_index)*0.8)\/batch_size), decay_rate)\n    lr = LearningRateScheduler(scheduler, verbose=1)\n\n    history = model.fit(\n        X_tr, y_tr,\n        validation_data=(X_val, y_val),\n        batch_size=batch_size,\n        epochs=epochs,\n        callbacks=[early_stopping,lr],\n    )\n\n    oof_train[valid_index] = model.predict(X_val).reshape(1, -1)[0]\n    y_pred = model.predict(X_test).reshape(1, -1)[0]\n\n    y_preds.append(y_pred)\n    models.append(model)\n    \n    del X_tr,X_val,y_tr,y_val,train_index,valid_index\n    _ = gc.collect()","e2b47da9":"print(f'CV: {roc_auc_score(y_train, oof_train)}')\npd.DataFrame(oof_train).to_csv(f'oof_{modelname}_{datenow}.csv',index=0)","0a13a30a":"visualize_history(history)","ae367571":"sub = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv')\ny_sub = sum(y_preds) \/ len(y_preds)\nsub['target'] = y_sub\nsub.to_csv(f'submission_{modelname}_{datenow}.csv', index=False)\nsub.head()","7bd1b201":"# Configuration","cd0bb45c":"# Read Input Data","552337f1":"# Submission","5042b3b4":"# Enable TPU\/GPU","4ad995e8":"# Tensorflow pipeline\n\nI think NN will play pivotal role in this months competition. Here is a Tensorflow Baseline model.\n\nWhile your GPU is busy with XGboost, make use of your TPU quota :).\n\nThis notebook is copied from @sishihara 's excellent notebook https:\/\/www.kaggle.com\/sishihara\/neural-network-for-tabular-example\n\nLearning rate scheduler code is from @shivansh002 's excellent notebook https:\/\/www.kaggle.com\/shivansh002\/i-am-groot-tpu-war\n","1d8865eb":"# Import Libraries","6885823a":"# Train","29263a42":"# Helper Method"}}