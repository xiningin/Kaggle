{"cell_type":{"3e2ee81f":"code","5fb9e6e2":"code","8b397cab":"code","3ec8175a":"code","86a92de8":"code","17739552":"code","8892ea32":"code","95655917":"code","06b4a808":"code","34790835":"code","5e4061c3":"code","09797748":"code","c80abf85":"code","503246fa":"code","238e1c2c":"code","9fce8c9f":"code","9f7a73b4":"code","764653e0":"code","3d944237":"code","75099f4f":"code","90f3ef8d":"code","783e32b9":"code","6e6eb8df":"code","b2b85476":"code","f3d708ca":"code","16b6c018":"code","b8462ccd":"code","6f23238a":"code","c4b911ea":"code","da0479ca":"code","34f30afe":"markdown","e2a26dd0":"markdown","b5f98587":"markdown","9c670e57":"markdown","cb5154ba":"markdown","92940d7c":"markdown","234d3c09":"markdown","d5085c34":"markdown","6eb0dfe7":"markdown","881198a0":"markdown","34b6a3b0":"markdown","1ede86a1":"markdown","f5d445d7":"markdown","4d6e58f4":"markdown","f4957ba5":"markdown","97ae3581":"markdown","419b23a6":"markdown","0232bb86":"markdown","25f222a9":"markdown","2e1d812c":"markdown","c209b29e":"markdown","157378b7":"markdown"},"source":{"3e2ee81f":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport matplotlib.patches as mpatches\nimport time\n\n# Classifier Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport collections\nimport xgboost as xgb\n\n\n# Other Libraries\nfrom imblearn.datasets import fetch_datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\nfrom imblearn.pipeline import make_pipeline as make_pipeline_imb # To do our transformation in a unique time\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report, confusion_matrix, fbeta_score, precision_recall_curve\nfrom collections import Counter\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5fb9e6e2":"df = pd.read_csv(\"..\/input\/creditcard.csv\")","8b397cab":"df.head()","3ec8175a":"pd.options.display.max_columns = 300","86a92de8":"print('Shape: ',df.shape)","17739552":"print('\\nColumns: ',df.columns.values)","8892ea32":"print('\\nData types:\\n',df.dtypes.value_counts())","95655917":"df.describe()","06b4a808":"print('No Frauds', round(df['Class'].value_counts()[0]\/len(df) * 100,2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]\/len(df) * 100,2), '% of the dataset')","34790835":"colors = [\"#0101DF\", \"#DF0101\"]\n\nsns.countplot('Class', data=df, palette=colors)\nplt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)","5e4061c3":"total = df.isnull().sum().sort_values(ascending = False)\npercent = (df.isnull().sum()\/df.isnull().count()*100).sort_values(ascending = False)\npd.concat([total, percent], axis=1, keys=['Total', 'Percent']).transpose()","09797748":"df.isnull().sum().sum()","c80abf85":"plt.figure(figsize = (10,10))\nplt.title('Credit Card Transactions features correlation plot (Pearson)')\ncorr = df.corr()\nsns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns,linewidths=.1,cmap=\"Greens\")\nplt.show()","503246fa":"s = sns.lmplot(x='V20', y='Amount',data=df, hue='Class', fit_reg=True,scatter_kws={'s':2})\ns = sns.lmplot(x='V7', y='Amount',data=df, hue='Class', fit_reg=True,scatter_kws={'s':2})\nplt.show()","238e1c2c":"s = sns.lmplot(x='V2', y='Amount',data=df, hue='Class', fit_reg=True,scatter_kws={'s':2})\ns = sns.lmplot(x='V5', y='Amount',data=df, hue='Class', fit_reg=True,scatter_kws={'s':2})\nplt.show()","9fce8c9f":"target = 'Class'\npredictors = ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\\\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19',\\\n       'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28',\\\n       'Amount']","9f7a73b4":"train_df, test_df = train_test_split(df, test_size=0.20, random_state=2018, shuffle=True )\ntrain_df, valid_df = train_test_split(train_df, test_size=0.20, random_state=2018, shuffle=True )","764653e0":"dtrain = xgb.DMatrix(train_df[predictors], train_df[target].values)\ndvalid = xgb.DMatrix(valid_df[predictors], valid_df[target].values)\ndtest = xgb.DMatrix(test_df[predictors], test_df[target].values)","3d944237":"#What to monitor (in this case, **train** and **valid**)\nwatchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n\n# Set xgboost parameters\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eta'] = 0.039\nparams['silent'] = True\nparams['max_depth'] = 2\nparams['subsample'] = 0.8\nparams['colsample_bytree'] = 0.9\nparams['eval_metric'] = 'auc'\nparams['random_state'] = 2018","75099f4f":"model = xgb.train(params, \n                dtrain, \n                1000, \n                watchlist, \n                early_stopping_rounds=50, \n                maximize=True, \n                verbose_eval=50)","90f3ef8d":"fig, (ax) = plt.subplots(ncols=1, figsize=(8,5))\nxgb.plot_importance(model, height=0.8, title=\"Features importance (XGBoost)\", ax=ax, color=\"green\") \nplt.show()","783e32b9":"preds = model.predict(dtest)","6e6eb8df":"roc_auc_score(test_df[target].values, preds)","b2b85476":"classifier = RandomForestClassifier\nX = df.drop([\"Class\"], axis=1).values #Setting the X to do the split\ny = df[\"Class\"].values # transforming the values in array\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2, test_size=0.20)# splitting data into training and test set","f3d708ca":"smote_pipeline = make_pipeline_imb(SMOTE(random_state=4), \\\n                                   classifier(random_state=42))\n\nsmote_model = smote_pipeline.fit(X_train, y_train)\nsmote_prediction = smote_model.predict(X_test)","16b6c018":"print(\"normal data distribution: {}\".format(Counter(y)))\nX_smote, y_smote = SMOTE().fit_sample(X, y)\nprint(\"SMOTE data distribution: {}\".format(Counter(y_smote)))","b8462ccd":"print(\"Confusion Matrix: \")\nprint(confusion_matrix(y_test, smote_prediction))","6f23238a":"print('\\nSMOTE Pipeline Score {}'.format(smote_pipeline.score(X_test, y_test)))","c4b911ea":"# the function that we will use to better evaluate the model\ndef print_results(headline, true_value, pred):\n    print(headline)\n    print(\"accuracy: {}\".format(accuracy_score(true_value, pred)))\n    print(\"precision: {}\".format(precision_score(true_value, pred)))\n    print(\"recall: {}\".format(recall_score(true_value, pred)))\n    print(\"f2: {}\".format(fbeta_score(true_value, pred, beta=2)))","da0479ca":"print_results(\"\\nSMOTE + RandomForest classification\", y_test, smote_prediction)","34f30afe":"Features correlation","e2a26dd0":"Got the best score when we use the SMOTE (OverSampling) + RandomForest, that performed a f2 score of 0.8669","b5f98587":"Calculate ROC-AUC","9c670e57":"Plot variable importance","cb5154ba":"check if there are null values in the dataset","92940d7c":"Build model with SMOTE imblearn","234d3c09":"There is no missing data in the whole dataset","d5085c34":"The AUC score for the prediction of fresh data (test set) is 0.974","6eb0dfe7":"Train the model","881198a0":"Data Exploration","34b6a3b0":"Let's look into more details to the data","1ede86a1":"Split data in train, test and validation set","f5d445d7":"The best validation score (ROC-AUC) was 0.984, for round 241","4d6e58f4":"plot the correlated values: {V20;Amount} and {V7;Amount}","f4957ba5":"Showing the diference before and after the transformation used","97ae3581":"Thus completed experimenting with XGBoost model. In this case, we used the validation set for validation of the training model. The best validation score obtained was 0.984. \nThen we used the model with the best training step, to predict target value from the test data; the AUC score obtained was 0.974","419b23a6":"# XGBoost - Prepare the model ","0232bb86":"Predict test set","25f222a9":"Now start with inverse corelated values v5 and v2 Amounts","2e1d812c":"Evaluating the model SMOTE + Random Forest","c209b29e":"Import all the required libraries","157378b7":"Let's check if there is any missing data"}}