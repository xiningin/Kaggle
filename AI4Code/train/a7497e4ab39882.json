{"cell_type":{"d4638436":"code","bc50d26c":"code","dc78aaf3":"code","7584fa94":"code","93072601":"code","05ddb7b3":"code","caeff34a":"code","3000e1db":"code","6f785ac2":"code","9439e382":"code","bee17f84":"code","e9c0c109":"code","c4fe9aaf":"code","52509b5c":"code","eca5d653":"code","0e0fccfa":"code","e661accc":"code","69926c7f":"code","dd142d43":"code","7c204acb":"code","37bd57a2":"code","c355560e":"code","828b8548":"code","d628dac0":"code","58549a16":"code","21bd6626":"code","1f2073c9":"code","07fc2bf2":"code","51ddcefd":"code","c40abf02":"code","0ba44210":"code","f388e96b":"code","197b75fc":"code","082770e5":"code","0bd8b4b9":"code","18e7a6f5":"code","125e1379":"code","711ec6ef":"code","c7b4c486":"code","9166b4cd":"code","be7ec075":"code","352d7d6f":"code","c776cc92":"markdown","ade93a7a":"markdown","e5698aad":"markdown","aeb8b6b1":"markdown","b12c8499":"markdown","e381de1d":"markdown"},"source":{"d4638436":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","bc50d26c":"emp=pd.read_csv(\"..\/input\/HR-Employee-Attrition.csv\")","dc78aaf3":"emp.shape","7584fa94":"emp.info()","93072601":"emp.describe().T","05ddb7b3":"emp.Attrition.value_counts()","caeff34a":"emp.isna().sum()","3000e1db":"emp.columns","6f785ac2":"emp.duplicated().sum()","9439e382":"emp.head(5)","bee17f84":"emp['Attrition'] = emp['Attrition'].map(lambda x: 1 if x== 'Yes' else 0)","e9c0c109":"emp.head(5)","c4fe9aaf":"cat_col = emp.select_dtypes(exclude=np.number)    ### to select all category types\ncat_col\nnum_col = emp.select_dtypes(include=np.number)\nnum_col","52509b5c":"for i in cat_col:\n    print(emp[i].value_counts())","eca5d653":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics","0e0fccfa":"one_hot = pd.get_dummies(cat_col)\none_hot.head(5)","e661accc":"emp = pd.concat([num_col,one_hot],sort=False,axis=1)\nemp.head()","69926c7f":"x = emp.drop(columns='Attrition')","dd142d43":"y = emp['Attrition']","7c204acb":"x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=0)\nlogreg = LogisticRegression()\nlogreg.fit(x_train,y_train)\ntrain_pred = logreg.predict(x_train)","37bd57a2":"metrics.confusion_matrix(y_train,train_pred)","c355560e":"metrics.accuracy_score(y_train,train_pred)","828b8548":"test_Pred = logreg.predict(x_test)","d628dac0":"metrics.confusion_matrix(y_test,test_Pred)","58549a16":"metrics.accuracy_score(y_test,test_Pred)","21bd6626":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, test_Pred))","1f2073c9":"import matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nlogit_roc_auc = roc_auc_score(y_test, logreg.predict(x_test))\nfpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(x_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","07fc2bf2":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom math import sqrt","51ddcefd":"x_train, x_test, y_train, y_test = train_test_split(\nx, y, test_size = 0.3, random_state = 100)\ny_train=np.ravel(y_train)\ny_test=np.ravel(y_test)\n#y_train = y_train.ravel()\n#y_test = y_test.ravel()","c40abf02":"accuracy_train_dict={}\naccuracy_test_dict={}\ndf_len=round(sqrt(len(emp)))\nfor k in range(3,df_len):\n    K_value = k+1\n    neigh = KNeighborsClassifier(n_neighbors = K_value, weights='uniform', algorithm='auto')\n    neigh.fit(x_train, y_train) \n    y_pred_train = neigh.predict(x_train)\n    y_pred_test = neigh.predict(x_test)    \n    train_accuracy=accuracy_score(y_train,y_pred_train)*100\n    test_accuracy=accuracy_score(y_test,y_pred_test)*100\n    accuracy_train_dict.update(({k:train_accuracy}))\n    accuracy_test_dict.update(({k:test_accuracy}))\n    print (\"Accuracy for train :\",train_accuracy ,\" and test :\",test_accuracy,\"% for K-Value:\",K_value)","0ba44210":"elbow_curve_train = pd.Series(accuracy_train_dict,index=accuracy_train_dict.keys())\nelbow_curve_test = pd.Series(accuracy_test_dict,index=accuracy_test_dict.keys())\nelbow_curve_train.head(10)","f388e96b":"ax=elbow_curve_train.plot(title=\"Accuracy of train VS Value of K \")\nax.set_xlabel(\"K\")\nax.set_ylabel(\"Accuracy of train\")","197b75fc":"ax=elbow_curve_test.plot(title=\"Accuracy of test VS Value of K \")\nax.set_xlabel(\"K\")\nax.set_ylabel(\"Accuracy of test\")","082770e5":"from sklearn.naive_bayes import GaussianNB","0bd8b4b9":"NB=GaussianNB()\nNB.fit(x_train, y_train)","18e7a6f5":"GaussianNB(priors=None,var_smoothing=1e-09)","125e1379":"train_pred=NB.predict(x_train)\naccuracy_score(train_pred,y_train)","711ec6ef":"test_pred=NB.predict(x_test)\naccuracy_score(test_pred,y_test)","c7b4c486":"from sklearn.svm import SVC\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ndef fit_predict(train, test, y_train, y_test, scaler, kernel = 'linear', C = 1.0, degree = 3):\n    train_scaled = scaler.fit_transform(train)\n    test_scaled = scaler.transform(test)        \n    lr = SVC(kernel = kernel, degree = degree, C = C)\n    lr.fit(train_scaled, y_train)\n    y_pred = lr.predict(test_scaled)\n    print(accuracy_score(y_test, y_pred))","9166b4cd":"from sklearn.preprocessing import StandardScaler\ny_train=np.ravel(y_train)\ny_test=np.ravel(y_test)\nfor kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n    print('Accuracy score using {0} kernel:'.format(kernel), end = ' ')\n    fit_predict(x_train, x_test, y_train, y_test, StandardScaler(), kernel)","be7ec075":"for \u0441 in np.logspace(-1,3 ,base = 2, num = 6):\n    print('Accuracy score using penalty = {0} with rbf kernel:'.format(\u0441), end = ' ')\n    fit_predict(x_train, x_test, y_train, y_test, StandardScaler(), 'linear', \u0441)","352d7d6f":"for degree in range(2, 6):\n    print('Accuracy score using degree = {0} with poly kernel:'.format(degree), end = ' ')\n    fit_predict(x_train, x_test, y_train, y_test, StandardScaler(), 'linear', 0.5, degree = degree)","c776cc92":"**Choose degree for poly-kernel**","ade93a7a":"will try to implement KNN for this problem to see accuracy is better than logistic regression","e5698aad":"From the above iteration we see K=12 had better accuracy","aeb8b6b1":"**PENALTY TUNING**","b12c8499":"**KERNEL TUNING**","e381de1d":"**SVM for Accuracy**"}}