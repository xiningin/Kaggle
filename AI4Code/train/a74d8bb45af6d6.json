{"cell_type":{"3a461765":"code","b8d751f5":"code","772b48e9":"code","5cd38ab9":"code","c6a64ad7":"code","7dbf1ce3":"code","ce598dc7":"code","1f41f723":"code","622c55ff":"code","d0aa98ca":"code","77af7829":"code","ee40dc98":"code","3a69f382":"code","b1ffe86d":"code","02ff8a5f":"code","ef8c0ef1":"code","adee9b07":"code","f75bc051":"code","35e454cb":"code","0cf73149":"code","bc7a55ff":"code","b04ddcc2":"code","c3e3bb03":"code","c6001f1b":"code","d777b62a":"code","f814dd09":"code","dc99cf8b":"code","5713efc7":"code","e567a009":"code","563b282c":"code","e6253a37":"code","401e00d8":"code","3aa50f4e":"code","7221c70f":"code","d7c87d0b":"code","7ef9cbb7":"code","659bedd7":"code","1401b6eb":"code","308599e2":"code","519f3e92":"code","febc0396":"code","d8c538b8":"code","9309b22c":"code","be4d60d6":"code","acf68d91":"code","961cde97":"code","5a919df8":"code","11ff926a":"code","d3ceb1f0":"code","c397ff68":"code","4621d15e":"code","33406655":"code","bc566e4b":"code","13c59c01":"code","735c6371":"code","17c339c4":"code","260c4bdf":"code","52367ed3":"code","8c3d8b92":"code","8a073f2d":"code","90b0a02b":"code","c04a0d40":"code","e393e478":"code","fea94a95":"code","b62ac947":"code","c1994160":"code","a0090c8c":"code","3b43fb5e":"code","9de90ae8":"code","26e5dec1":"code","fdd616f1":"code","ef4939c0":"code","47ba6f77":"code","4b9f0afb":"code","50b81e45":"code","4156eb78":"code","f573b7a6":"code","fe9bdade":"code","4425bf36":"code","1e2a9c81":"code","25d73cb8":"code","3eb9fc09":"markdown","d03a458c":"markdown","e53bbad8":"markdown","ddc64a39":"markdown","4c6cac08":"markdown","0a7a4c1d":"markdown","b8e8fe5d":"markdown","29dedb4d":"markdown","2dcfdba2":"markdown","8a5cb5a5":"markdown","96994626":"markdown","75ca6234":"markdown","6a4dc954":"markdown","441e8da3":"markdown","2168c9de":"markdown","233fe0ec":"markdown","e7581b5e":"markdown","ee7474ae":"markdown","564cd96a":"markdown","236c6119":"markdown","3b933d27":"markdown","5f1fe657":"markdown","da7c5cd0":"markdown","24b4a57a":"markdown","14ee6350":"markdown","f52eeb96":"markdown","c850de05":"markdown","39a16c50":"markdown","b01c377a":"markdown","5f489790":"markdown","a5af7a3f":"markdown","50d5c750":"markdown","d730af64":"markdown","4a70446e":"markdown","aa429e7a":"markdown","3ae6c824":"markdown","bbaa13d8":"markdown","73c3533d":"markdown","5aefbda3":"markdown"},"source":{"3a461765":"from sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import KFold\nfrom matplotlib import pyplot as plt\nimport xgboost as xgb\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport shap\n%matplotlib inline","b8d751f5":"import warnings\nwarnings.simplefilter('ignore')","772b48e9":"# read the csv train and test files \ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","5cd38ab9":"# number of rows and columns in train and test set\ntrain.shape, test.shape","c6a64ad7":"train.head()","7dbf1ce3":"test.head()","ce598dc7":"#identify numerical and categorical variables\ntrain.dtypes","1f41f723":"train.drop(['PassengerId', 'Ticket'], axis=1, inplace=True)\ntest.drop(['PassengerId', 'Ticket'], axis=1, inplace=True)","622c55ff":"def get_titles(data):\n    \"\"\" To get the title from each name in the dataset and to add it to a new feature column 'Title.\n    \n        Args:\n\n            data (dataframe) - Dataset from which we need to get and set the titles in the name.\n\n        Returns:\n\n            data (dataframe) - Dataset after the update is returned.\n        \n    \"\"\"\n    # extract the title from each name\n    data['Title'] = data['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())\n    return data","d0aa98ca":"train = get_titles(train)\ntest = get_titles(test)","77af7829":"# remove the varibale 'Name' from both the test and train set\ntrain.drop('Name', axis=1, inplace=True)\ntest.drop('Name', axis=1, inplace=True)","ee40dc98":"# find unique titles in train\ntrain.Title.unique()","3a69f382":"# find unique titles in test\ntest.Title.unique()","b1ffe86d":"title_dict = {\n    \"Capt\": \"Rare\",\n    \"Col\": \"Rare\",\n    \"Major\": \"Rare\",\n    \"Jonkheer\": \"Rare\",\n    \"Don\": \"Rare\",\n    \"Dona\": \"Miss\",\n    \"Sir\" : \"Rare\",\n    \"Dr\": \"Rare\",\n    \"Rev\": \"Rare\",\n    \"the Countess\":\"Rare\",\n    \"Mme\": \"Mrs\",\n    \"Mlle\": \"Miss\",\n    \"Ms\": \"Mrs\",\n    \"Mr\" : \"Mr\",\n    \"Mrs\" : \"Mrs\",\n    \"Miss\" : \"Miss\",\n    \"Master\" : \"Master\",\n    \"Lady\" : \"Rare\"\n}\n","02ff8a5f":"# map each title to its class or category\ntrain.Title = train.Title.map(title_dict)\ntest.Title = test.Title.map(title_dict)","ef8c0ef1":"train.Title.unique()","adee9b07":"test.Title.unique()","f75bc051":"train.isna().sum()","35e454cb":"test.isna().sum()","0cf73149":"train.drop(['Cabin'], axis=1, inplace=True)\ntest.drop(['Cabin'], axis=1, inplace=True)","bc7a55ff":"# groupby and find the median of age for train\ngrouped_median_train = train.groupby(['Sex','Pclass','Title']).median()\ngrouped_median_train = grouped_median_train.reset_index()[['Sex', 'Pclass', 'Title', 'Age']]","b04ddcc2":"grouped_median_train.head()","c3e3bb03":"def fill_age(row):\n    \"\"\" To assign value for the missing values of based on the group.\n\n            If a title is missing then the age will be assigned based on sex and class.\n            \n        Args: \n            \n            row (dataframe) - A row in the dataframe.\n        \n        Returns:\n        \n            The values of the computed age in the dataframe.\n    \"\"\"\n    condition = (\n        (grouped_median_train['Sex'] == row['Sex']) & \n        (grouped_median_train['Title'] == row['Title']) & \n        (grouped_median_train['Pclass'] == row['Pclass'])\n    ) \n    if np.isnan(grouped_median_train[condition]['Age'].values[0]):\n        print('true')\n        condition = (\n            (grouped_median_train['Sex'] == row['Sex']) & \n            (grouped_median_train['Pclass'] == row['Pclass'])\n        )\n\n    return grouped_median_train[condition]['Age'].values[0]\n\n\ndef process_age(data):\n    \"\"\" To fill the missing values of the Age variable\n        \n        Args:\n        \n            data (dataframe) - The dataset into which we need to fill the missing values of the age.\n            \n        Returns:\n        \n            data (dataframe) - Processed dataset\n    \"\"\"\n    data.Age = data.apply(lambda row: fill_age(row) if np.isnan(row['Age']) else row['Age'], axis=1)\n    return data","c6001f1b":"# replace missing values of Embarked in train with its most frequent data\ntrain.Embarked.fillna('S', inplace=True)\n# fill the missing values of age in train with the median age of the group\ntrain = process_age(train)\ntest = process_age(test)\n# assign mean of the variable Fare in test to the missing value of fare \ntest.Fare = test['Fare'].fillna(train['Fare'].median())\n# to convert the variable Age to dataframe which is currently series \ntrain = pd.DataFrame(train)\ntest = pd.DataFrame(test)","d777b62a":"train.isna().sum()","f814dd09":"test.isna().sum()","dc99cf8b":"train.head()","5713efc7":"test.head()","e567a009":"plt.figure(figsize=[10, 8])\nplt.hist(x=train.Age, bins='auto', color='#0504aa',alpha=0.7, rwidth=0.85)\nplt.xlabel('Value',fontsize=15)\nplt.ylabel('Frequency',fontsize=15)\nplt.title('Data Distribution Histogram - Age',fontsize=15)\nplt.show()","563b282c":"plt.figure(figsize=[10, 8])\nplt.hist(x=train.Fare, bins='auto', color='#0504aa',alpha=0.7, rwidth=0.85)\nplt.xlabel('Value',fontsize=15)\nplt.ylabel('Frequency',fontsize=15)\nplt.title('Data Distribution Histogram - Fare',fontsize=15)\nplt.show()","e6253a37":"# find frequency of each label in the variable\nPclass_counts = train.Pclass.value_counts()\nSex_counts = train.Sex.value_counts()\nSibSp_counts = train.SibSp.value_counts()\nParch_counts = train.Parch.value_counts()\nEmbarked_counts = train.Embarked.value_counts()\nTitle_counts = train.Title.value_counts()\nSurvived_counts = train.Survived.value_counts()","401e00d8":"fig, axs = plt.subplots(3, 2, figsize=[15, 20])\n\naxs[0,0].bar(Pclass_counts.index, Pclass_counts, alpha=0.7)\naxs[0,0].set_title('Categorical Distribution - Pclass')\n\naxs[0,1].bar(Sex_counts.index, Sex_counts,alpha=0.7)\naxs[0,1].set_title('Categorical Distribution - Sex')\n\naxs[1,0].bar(SibSp_counts.index, SibSp_counts,alpha=0.7)\naxs[1,0].set_title('Categorical Distribution - SibSp')\n\naxs[1,1].bar(Parch_counts.index, Parch_counts,alpha=0.7)\naxs[1,1].set_title('Categorical Distribution - Parch')\n\naxs[2,0].bar(Embarked_counts.index, Embarked_counts,alpha=0.7)\naxs[2,0].set_title('Categorical Distribution - Embarked')\n\naxs[2,1].bar(Title_counts.index, Title_counts,alpha=0.7)\naxs[2,1].set_title('Categorical Distribution - Title')\n\nfor ax in axs.flat:\n    ax.set(xlabel='Category', ylabel='Frequency')","3aa50f4e":"plt.figure(figsize=[10, 8])\np = plt.bar(Survived_counts.index, Survived_counts, width=.4)\nplt.xlabel('Category',fontsize=15)\nplt.ylabel('Frequency',fontsize=15)\nplt.title('Categorical Distribution - Survival',fontsize=15)\nplt.show()","7221c70f":"data = train[['Survived', 'Sex']]\ndata['Died'] = 1 - data['Survived']","d7c87d0b":"data.groupby('Sex').agg('sum')[['Survived','Died']].plot(kind='bar',stacked=True, figsize=[10, 6])","7ef9cbb7":"plt.figure(figsize=[10, 6])\nsns.violinplot(x='Sex', y='Age', hue='Survived', data=train, split=True)","659bedd7":"plt.figure(figsize=[10, 6])\nsns.violinplot(x='Sex', y='Fare', hue='Survived', data=train, split=True)","1401b6eb":"fig, ax = plt.subplots(figsize=[10, 8])\ntrain.boxplot(column='Fare', by='Survived', ax=ax, grid=False, fontsize=15)","308599e2":"plt.figure(figsize=[10, 6])\nsns.violinplot(x='Sex', y='Pclass', hue='Survived', data=train, split=True)","519f3e92":"data['Title'] = train['Title']","febc0396":"data.groupby('Title').agg('sum')[['Survived','Died']].plot(kind='bar',stacked=True, figsize=[10, 6])","d8c538b8":"train.head()","9309b22c":"test.head()","be4d60d6":"def process_family(data):\n    # the size of families (including the passenger)\n    data['FamilySize'] = data['Parch'] + data['SibSp'] + 1 \n    return data","acf68d91":"train = process_family(train)\ntest = process_family(test)","961cde97":"sex_map = {\n    'male' : 0,\n    'female' : 1\n}\n\nembarked_map = {\n    'Q' : 0,\n    'S' : 1,\n    'C' : 2\n}\n\ntitle_map = {\n             \"Mr\": 1, \n             \"Master\": 2, \n             \"Mrs\": 3, \n             \"Miss\": 4, \n             \"Rare\": 5\n            }","5a919df8":"# label encode the categorical variables\ntrain.Sex = train.Sex.map(sex_map)\ntest.Sex = test.Sex.map(sex_map)\ntrain.Embarked = train.Embarked.map(embarked_map)\ntest.Embarked = test.Embarked.map(embarked_map)\ntrain.Title = train.Title.map(title_map)\ntest.Title = test.Title.map(title_map)","11ff926a":"train.head()","d3ceb1f0":"test.head()","c397ff68":"# drop the variable 'SibSp' as we have already created a similar variable FamilySize\ntrain = train.drop(['SibSp'], axis = 1)\ntest  = test.drop(['SibSp'], axis = 1)","4621d15e":"# seperate the feature set and the target set\nX_train = train.loc[:, train.columns!='Survived']\ny_train = train['Survived']\nX_test = test","33406655":"#split the train data into train and valid set\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.10, random_state=111)","bc566e4b":"X_train.shape, X_valid.shape","13c59c01":"model = RandomForestClassifier(n_estimators=150, min_samples_leaf=3, max_features=0.5, n_jobs=-1)\nmodel.fit(X_train, y_train)\nmodel.score(X_train, y_train)","735c6371":"train_sizes, train_scores, test_scores = learning_curve(model, X_train, y_train, train_sizes=np.linspace(0.01, 1.0, 10), scoring='accuracy')\n\n# Create means of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\n\n# Create means of test set scores\ntest_mean = np.mean(test_scores, axis=1)","17c339c4":"# Create plot\nplt.plot(train_sizes, train_mean, label=\"Training\")\nplt.plot(train_sizes, test_mean, label=\"Validation\")\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\")\nplt.ylabel(\"Accuracy Score\")\nplt.legend()\nplt.show()","260c4bdf":"predict = model.predict(X_valid)\naccuracy_score(y_valid, predict)","52367ed3":"print(classification_report(y_valid, predict))","8c3d8b92":"# find the variable or feature importance\nshap_values = shap.TreeExplainer(model).shap_values(X_train)\nshap.summary_plot(shap_values, X_train)","8a073f2d":"# Desired number of Cross Validation folds\ncv = KFold(n_splits=10)            \naccuracies = list()\nmax_attributes = len(list(test))\ndepth_range = range(1, max_attributes + 1)\n\n# Testing max_depths from 1 to max attributes\n# Uncomment prints for details about each Cross Validation pass\nfor depth in depth_range:\n    fold_accuracy = []\n    tree_model = RandomForestClassifier(n_estimators=150, min_samples_leaf=3, max_features=0.5, n_jobs=-1, max_depth=depth)\n    # print(\"Current max depth: \", depth, \"\\n\")\n    for train_fold, valid_fold in cv.split(train):\n        f_train = train.loc[train_fold] # Extract train data with cv indices\n        f_valid = train.loc[valid_fold] # Extract valid data with cv indices\n\n        model = tree_model.fit(X = f_train.drop(['Survived'], axis=1), \n                               y = f_train[\"Survived\"]) # We fit the model with the fold train data\n        valid_acc = model.score(X = f_valid.drop(['Survived'], axis=1), \n                                y = f_valid[\"Survived\"])# We calculate accuracy with the fold validation data\n        fold_accuracy.append(valid_acc)\n\n    avg = sum(fold_accuracy)\/len(fold_accuracy)\n    accuracies.append(avg)\n    # print(\"Accuracy per fold: \", fold_accuracy, \"\\n\")\n    # print(\"Average accuracy: \", avg)\n    # print(\"\\n\")\n    \n# Just to show results conveniently\ndf = pd.DataFrame({\"Max Depth\": depth_range, \"Average Accuracy\": accuracies})\ndf = df[[\"Max Depth\", \"Average Accuracy\"]]\nprint(df.to_string(index=False))","90b0a02b":"print(np.mean(accuracies))","c04a0d40":"params = {\n\n    'max_depth' : 5,\n    'n_estimators': 5,\n    'gamma': 6,\n    'objective' : 'binary:logistic',\n    'eval_metric' : [\"error\", \"logloss\"],\n    'n_gpus' : 0\n}","e393e478":"model = xgb.XGBClassifier(**params)","fea94a95":"evallist = [(X_train, y_train), (X_valid, y_valid)]","b62ac947":"model.fit(X_train, y_train, eval_set=evallist, eval_metric=[\"error\", \"logloss\"], verbose=False)","c1994160":"predictions = model.predict(X_valid)","a0090c8c":"print(classification_report(y_valid, predictions))","3b43fb5e":"# retrieve performance metrics\nresults = model.evals_result_\nepochs = len(results['validation_0']['error'])\nx_axis = range(0, epochs)\nfig, ax = plt.subplots(figsize=(10,8))\nplt.plot(x_axis, results['validation_0']['error'], label = 'Train')\nplt.plot(x_axis, results['validation_1']['error'], label = 'Test')\nax.legend()\nplt.ylabel('Classification Loss')\nplt.title('XGBoost Classification Loss')\nplt.show()","9de90ae8":"# retrieve performance metrics\nresults = model.evals_result_\nepochs = len(results['validation_0']['logloss'])\nx_axis = range(0, epochs)\nfig, ax = plt.subplots(figsize=(10,8))\nplt.plot(x_axis, results['validation_0']['logloss'], label = 'Train')\nplt.plot(x_axis, results['validation_1']['logloss'], label = 'Test')\nax.legend()\nplt.ylabel('Classification Loss')\nplt.title('XGBoost Classification Loss')\nplt.show()","26e5dec1":"# find the variable or feature importance\nshap_values = shap.TreeExplainer(model).shap_values(X_train)\nshap.summary_plot(shap_values, X_train)","fdd616f1":"params = {\n\n    'n_estimators': 5,\n    'gamma': 6,\n    'objective' : 'binary:logistic',\n    'eval_metric' : [\"error\", \"logloss\"],\n    'n_gpus' : 0\n}","ef4939c0":"# Desired number of Cross Validation folds\ncv = KFold(n_splits=10)            \naccuracies = list()\nmax_attributes = len(list(test))\ndepth_range = range(1, max_attributes + 1)\n\n# Testing max_depths from 1 to max attributes\n# Uncomment prints for details about each Cross Validation pass\nfor depth in depth_range:\n    fold_accuracy = []\n    tree_model = xgb.XGBClassifier(**params, max_depth=depth)\n    # print(\"Current max depth: \", depth, \"\\n\")\n    for train_fold, valid_fold in cv.split(train):\n        f_train = train.loc[train_fold] # Extract train data with cv indices\n        f_valid = train.loc[valid_fold] # Extract valid data with cv indices\n\n        model = tree_model.fit(X = f_train.drop(['Survived'], axis=1), \n                               y = f_train[\"Survived\"]) # We fit the model with the fold train data\n        valid_acc = model.score(X = f_valid.drop(['Survived'], axis=1), \n                                y = f_valid[\"Survived\"])# We calculate accuracy with the fold validation data\n        fold_accuracy.append(valid_acc)\n\n    avg = sum(fold_accuracy)\/len(fold_accuracy)\n    accuracies.append(avg)\n    # print(\"Accuracy per fold: \", fold_accuracy, \"\\n\")\n    # print(\"Average accuracy: \", avg)\n    # print(\"\\n\")\n    \n# Just to show results conveniently\ndf = pd.DataFrame({\"Max Depth\": depth_range, \"Average Accuracy\": accuracies})\ndf = df[[\"Max Depth\", \"Average Accuracy\"]]\nprint(df.to_string(index=False))","47ba6f77":"print(np.mean(accuracies))","4b9f0afb":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler","50b81e45":"numerical_columns = ['Age', 'Fare']","4156eb78":"scaler = StandardScaler()\ntrain[numerical_columns] = scaler.fit_transform(train[numerical_columns])\ntest[numerical_columns] = scaler.fit_transform(test[numerical_columns])","f573b7a6":"X_train = train.loc[:, train.columns!='Survived']\ny_train = train['Survived']\nX_test = test\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.10)","fe9bdade":"# Instantiate our model\nlogreg = LogisticRegression()\n# Fit our model to the training data\nlogreg.fit(X_train, y_train)\n# Predict on the test data\nlogreg_predictions = logreg.predict(X_valid)\nprint(classification_report(y_valid, logreg_predictions))","4425bf36":"model=LogisticRegression()\npredicted = cross_val_predict(model, X_train, y_train, cv=10)\nprint(classification_report(y_train, predicted) )\nprint(accuracy_score(y_train, predicted))","1e2a9c81":"# may use which ever model is the best\noutput = model.predict(X_test)","25d73cb8":"test_set = pd.read_csv('..\/input\/titanic\/test.csv')\noutput_csv = pd.DataFrame()\noutput_csv['PassengerId'] = test_set['PassengerId']\noutput_csv['Survived'] = output\noutput_csv[['PassengerId','Survived']].to_csv('..\/output\/random_forest_predictions.csv', index=False)","3eb9fc09":"Missing data in the training dataset can reduce the power \/ fit of a model or can lead to a biased model as we don't know its behavior and relationship with other variables correctly. It can also lead to wrong prediction or classification.\n\nLet's analyze the missing values in our dataset.","d03a458c":"### 2. Missing Value Treatment","e53bbad8":"#### K-Fold","ddc64a39":"Most of the male passengers in the age 20 to 30 were died but most of the survived male were in the age 25 to 35. In the case of female passengers, most of them have survived but the age of the most people died were in their 20's.","4c6cac08":"### 4. Bivariate Analysis","0a7a4c1d":"## Exploratory Data Analysis","b8e8fe5d":"We can see that Random Forest model has performed well for the train and validation set. Thus, hoping it would also perform well for the test set, we may execute the model again and then run the following code to create the prediction file at your local directory. File is saved in the format suitable for submission in the Kaggle:Titanic challenge.","29dedb4d":"## Define and train the ML algorithm","2dcfdba2":"### 3. Univariate Analysis","8a5cb5a5":"People who have paid the lowest fare has the highest highest death rate. ","96994626":"We have the following features in the dataset:\n\n**PassengerId** (numerical): &emsp;Id given to each traveler on the ship. (**Target**)\n\n**Pclass** (numerical): &emsp; &emsp; &emsp;The passenger class. \n\n**Name** (categorical): &emsp; &emsp; &emsp;The Name of the passenger.\n\n**Sex** (categorical): &emsp; &emsp; &emsp; &ensp; Gender of the passenger.\n\n**Age** (numerical): &emsp; &emsp; &emsp; &emsp; Age of the passenger.\n\n**SibSp** (numerical): &emsp; &emsp; &emsp; &nbsp;Number of siblings and spouses traveling with the passenger.\n\n**Parch** (numerical): &emsp; &emsp; &emsp; &nbsp;Number of parents and children traveling with the passenger.\n\n**Ticket** (categorical): &emsp; &emsp; &emsp;The ticket id.\n\n**Fare** (numerical): &emsp; &emsp; &emsp; &emsp;The ticket Fare.\n\n**Cabin** (categorical): &emsp; &emsp; &ensp; &nbsp;The cabin number.\n\n**Embarked** (categorical): &emsp; &nbsp;The embarkation. This describe three possible areas of the Titanic from which the people embark. Three possible values S,C,Q.\n\nFrom these features, let's identify the useful ones and explore them more. Id of the passenger is not a good feature as it doesn't convey any meaning by itself and with other variables to the model. The case of ticket id is also the same. Thus, let's remove the variables 'PassengerId' and 'Ticket' from the dataset and work with the rest. The variable 'Name' by itself is meaningless to the model but we could extract the salutation or title from it to create a new feature 'Title' which indicates the honorifics to address the person. ","75ca6234":"Most of the people were in the class 3 (almost double of class 1 and 2.). The number of male passengers were almost double of female passengers. From the frequency graph of SibSp and Parch we can understand that most of the passengers were alone. People embarked from 'S' is really high. People who are addressed with the honorific 'Mr' is the highest.  ","6a4dc954":"If you get to takeaway any information from this notebook, please UPVOTE the same. It keeps me motivated! Thank you!","441e8da3":"Both male and female passengers in class 3 had died the most.","2168c9de":"We may use the K-Fold cross-validation method to test the effectiveness of our Linear Regression model. I have set the parameter \u2018K\u2019 to 10 i.e. we may split the dataset into 10 parts or folds. Each time on the loop we may consider a single fold for testing and rest for training, non-repeatedly. We may evaluate the model on accuracy each time on the respective test set and all the scores are averaged to obtain a more comprehensive model validation score.","233fe0ec":"Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.\n\nWe may use the K-Fold cross-validation method to test the effectiveness of our Random Forest model. I have set the parameter \u2018K\u2019 to 10 i.e. we may split the dataset into 10 parts or folds. Each time on the loop we may consider a single fold for testing and rest for training, non-repeatedly. We may evaluate the model on accuracy each time on the respective test set and all the scores are averaged to obtain a more comprehensive model validation score.","e7581b5e":"At this stage, we explore variables one by one. Method to perform uni-variate analysis will depend on whether the variable type is categorical or continuous.","ee7474ae":"In Feature Engineering we are not going to add any new data, but we are going to make the data that we already have useful.\n\nSince we are going to use tree based ensemble methods, we don't need to standardize or normalize the data. ","564cd96a":"We will try the tree based ensemble methods Random Forest, XGBoost and the linear model Logistic Regressor for creating the model and may choose the best model. And, may evaluate each model using the KFold cross-validation method.","236c6119":"### Random Forest","3b933d27":"#### K-Fold","5f1fe657":"Most of the people paid between 5 to 25 units.","da7c5cd0":"We can see that there are 177 missing values in the variable 'Age', 687 in the variable 'Cabin' and 2 in the variable 'Embarked'.\n\nLet's drop the variable 'Cabin' as 77% of the variable's data is missing. Since the variable 'Embarked' has only two missing values, we may impute it with the most frequent label in the variable (i.e. 'S'). And in the case of the variable 'Age', just by replacing the same with the mean\/median age might not be the best solution, since the age may differ by group and categories of passengers. Hence, we can impute values based on the median of age while the variables are grouped by 'Sex', 'Pclass', and 'Title'.","24b4a57a":"From the figure it is clear that the dataset is imbalanced. We are having the data of non-survived people than the survived people. Sometimes it could influence our predictions.","14ee6350":"# Survival Prediction - Titanic\n---\n#### Introduction\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. That's why the name DieTanic. This is a very unforgetable disaster that no one in the world can forget.\n\nIt took about $7.5 million to build the Titanic and it sunk under the ocean due to collision. The Titanic Dataset is a very good dataset for beginners to start a journey in data science and participate in competitions in Kaggle.\n\nThe objective of this notebook is to give an idea how is the workflow of any predictive modeling problem. How do we analyze features, how do we add new features, how do we make the existing features useful for the model and some Machine Learning Concepts. I have tried to keep the notebook as basic as possible so that even newbies can understand every phase of it.\n\nWe will predict whether a passenger in the Titanic would survive or not based on the passenger's data (name, age, price of ticket, etc). \n\nThe process will be broken down into the following steps:\n>1. Exploratory Data Analysis\n2. Feature Engineering\n3. Define and train the ML models\n4. Evaluate the performance of our trained models on the validation set\n5. Select the best model and predict the targets for the test set\n\nBefore we begin, let's import the necessary libraries and the dataset for working.","f52eeb96":"Boys and women are the most survived people relative to their respective class population.","c850de05":"We may use the K-Fold cross-validation method to test the effectiveness of our XGBoost model. I have set the parameter \u2018K\u2019 to 10 i.e. we may split the dataset into 10 parts or folds. Each time on the loop we may consider a single fold for testing and rest for training, non-repeatedly. We may evaluate the model on accuracy each time on the respective test set and all the scores are averaged to obtain a more comprehensive model validation score.","39a16c50":"In this stage, we will find out the relationship between two variables. In other words, we look for association and disassociation between variables at a predefined significance level. We can perform bivariate analysis for any combination of categorical and continuous variables.","b01c377a":"On submission, i got a public score of 77.03% for the model. Good score to start with! Later, i built a model using Decision Tree and it was in the top 2% of the leaderboard. Hopefully, i will submit the notebook to kaggle soon. ","5f489790":"### Logistic Regression","a5af7a3f":"#### K-Fold","50d5c750":"## Feature Engineering","d730af64":"From the chart we can infer that most of the male passengers are died compared to females. Let's explore this chart more to find out whether the died female or male passenger has any correlation with their Status, Title and Age.","4a70446e":"From the figure we can understand that the data distribution of Age is positively skewed or right skewed. So in skewed data, the tail region may act as an outlier for the statistical model and we know that outliers adversely affect the model\u2019s performance. As tree based models are not effected by skewness or outliers we may not normalize the data for the same. But, other linear or non-linear models will be effected by the skewness or outliers and hence, we may need to normalize the data afterwards.\n\nAnd also, we can infer from the figure that most of the passengers were aged between 18 and 35. ","aa429e7a":"### XGBoost","3ae6c824":"We can see that there are many titles and many are confusing, so let's categorize them further and we may update it to the feature column 'Title'. Let's define a dictionary to categorize each titles into its respective class.","bbaa13d8":"#### Saving the Model","73c3533d":"### 1. Variable Identification\n\nIdentify the types and categories of all the data variables or features. ","5aefbda3":"We can observe that there are some outliers in the variable Fare. The passenger record with Fare between 200 and 300 can be considered as outliers as it exist in both the survival and non-survival. Whereas, the outliers in survival with the Fare greater than 500 indicates some meaning to the model that the people who paid high is survived."}}