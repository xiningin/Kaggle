{"cell_type":{"937ae9a5":"code","e56a8b20":"code","36b2d359":"code","1a60efeb":"code","0320f31a":"code","51c2cd88":"code","21c992be":"code","a724d64a":"code","d6491551":"code","fe89be46":"code","2a75961b":"code","f66b424c":"code","78693905":"code","46d56a90":"code","8c6aa1cb":"code","efcdb9b1":"code","6af82c1f":"code","40dc66c4":"code","0eac8e6e":"code","2f290cca":"code","422533cf":"code","ed89303a":"code","36835a5b":"code","3ca41acd":"code","9064e632":"code","5e3a6b1a":"code","1976efd2":"code","fc4e236a":"code","2c15509f":"code","d43d0ebe":"code","7bc27cb8":"code","4e56aa13":"code","f51210f5":"code","5cf5fec6":"code","7efc28ab":"code","6ca3fca8":"code","894d788d":"code","38d8dce3":"code","6f5dcde9":"code","5f9f02ea":"code","5e650a27":"code","49aacc84":"code","c0db6349":"code","11e361b4":"markdown","98285830":"markdown","fdf23446":"markdown","36a44fed":"markdown","758cc913":"markdown","58bbfaeb":"markdown","3c78c698":"markdown","c4ca2fdc":"markdown","77d6c215":"markdown","9c0276de":"markdown","2c444f7b":"markdown","ad374b66":"markdown"},"source":{"937ae9a5":"# Importaing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","e56a8b20":"dataset = pd.read_csv(\"..\/input\/Admission_Predict.csv\")","36b2d359":"dataset.head()","1a60efeb":"# Removing column 'Serial No.' from dataframe\ndataset.drop('Serial No.', axis = 1 , inplace = True)","0320f31a":"dataset.info()","51c2cd88":"dataset.describe()","21c992be":"# This is another method which we can use to check for null values.\n\nround(( dataset.isnull().sum() \/ len(dataset.index) )*100 , 2)","a724d64a":"# Creating a derived column\n\navg_score = round(((dataset.loc[:,['GRE Score', 'TOEFL Score', 'SOP', 'LOR ', 'CGPA']].sum(axis = 1))\/(340+120+5+5+10))*100, 2)\ndataset.insert(7, 'Average_score', avg_score)\n","d6491551":"dataset.head()","fe89be46":"corr_matrix = dataset.corr()\nround( corr_matrix , 3)","2a75961b":"plt.figure(figsize = (15,12))\nsns.heatmap(corr_matrix , annot = True)\nplt.show()","f66b424c":"sns.pairplot(dataset)\nplt.show()","78693905":"#defining a normalisation function \ndef normalize (x): \n    return ( (x-np.mean(x))\/ (max(x) - min(x)))\n\ndef inv_normalize (y , x):\n    return ( (y*( max(x) - min(x) )) + np.mean(x))\n\nscaled_df = dataset.apply(normalize)\n\n# to inverse normalizaion\n#for column in dataset.columns:\n#    scaled_df[column] = inv_normalize(scaled_df[column] , dataset[column])\n#    scaled_df[column] = scaled_df[column].astype(type(dataset[column][0]))","46d56a90":"scaled_df.head()","8c6aa1cb":"# Dividing data in X and Y\nX = scaled_df.iloc[:,[0,1,2,3,4,5,6,7]]\ny = scaled_df.iloc[:,8]\n\n# dividing the data into training data and test data\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, test_size = 0.2, random_state = 100)","efcdb9b1":"# creating regression model\nX_trainsm = sm.add_constant(X_train)\nlm1 = sm.OLS(y_train, X_trainsm).fit()\n\nprint(lm1.summary())","6af82c1f":"# For each X, calculate VIF and save in dataframe\nvif = pd.DataFrame()\nvif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif[\"features\"] = X.columns\nvif.sort_values(by = 'VIF', axis=0, ascending=False, inplace=True)\n\nvif","40dc66c4":"# Removed Average_score\nX = scaled_df.iloc[:,[0,1,2,3,4,5,6]]\ny = scaled_df.iloc[:,8]\n\n# dividing the data into training data and test data\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, test_size = 0.2, random_state = 100)","0eac8e6e":"# creating regression model\nX_trainsm = sm.add_constant(X_train)\nlm2 = sm.OLS(y_train, X_trainsm).fit()\n\nprint(lm2.summary())","2f290cca":"# For each X, calculate VIF and save in dataframe\nvif = pd.DataFrame()\nvif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif[\"features\"] = X.columns\nvif.sort_values(by = 'VIF', axis=0, ascending=False, inplace=True)\n\nvif","422533cf":"# Removed CGPA\nX = scaled_df.iloc[:,[0,1,2,3,4,6]]\ny = scaled_df.iloc[:,8]\n\n# dividing the data into training data and test data\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, test_size = 0.2, random_state = 100)","ed89303a":"# creating regression model\nX_trainsm = sm.add_constant(X_train)\nlm3 = sm.OLS(y_train, X_trainsm).fit()\nprint(lm3.summary())","36835a5b":"# For each X, calculate VIF and save in dataframe\nvif = pd.DataFrame()\nvif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif[\"features\"] = X.columns\nvif.sort_values(by = 'VIF', axis=0, ascending=False, inplace=True)\n\nvif","3ca41acd":"# Removed SOP\nX = scaled_df.iloc[:,[0,1,2,4,6]]\ny = scaled_df.iloc[:,8]\n\n# dividing the data into training data and test data\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, test_size = 0.2, random_state = 100)","9064e632":"# creating regression model\nX_trainsm = sm.add_constant(X_train)\nlm4 = sm.OLS(y_train, X_trainsm).fit()\nprint(lm4.summary())","5e3a6b1a":"# For each X, calculate VIF and save in dataframe\nvif = pd.DataFrame()\nvif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif[\"features\"] = X.columns\nvif.sort_values(by = 'VIF', axis=0, ascending=False, inplace=True)\n\nvif","1976efd2":"# Removed GRE\nX = scaled_df.iloc[:,[1,2,4,6]]\ny = scaled_df.iloc[:,8]\n\n# dividing the data into training data and test data\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, test_size = 0.2, random_state = 100)","fc4e236a":"# creating regression model\nX_trainsm = sm.add_constant(X_train)\nlm5 = sm.OLS(y_train, X_trainsm).fit()\nprint(lm5.summary())","2c15509f":"# For each X, calculate VIF and save in dataframe\nvif = pd.DataFrame()\nvif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif[\"features\"] = X.columns\nvif.sort_values(by = 'VIF', axis=0, ascending=False, inplace=True)\n\nvif","d43d0ebe":"# Removed University Rating\nX = scaled_df.iloc[:,[1,4,6]]\ny = scaled_df.iloc[:,8]\n\n# dividing the data into training data and test data\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, test_size = 0.2, random_state = 100)","7bc27cb8":"# creating regression model\nX_trainsm = sm.add_constant(X_train)\nlm6 = sm.OLS(y_train, X_trainsm).fit()\nprint(lm6.summary())","4e56aa13":"# For each X, calculate VIF and save in dataframe\nvif = pd.DataFrame()\nvif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif[\"features\"] = X.columns\nvif.sort_values(by = 'VIF', axis=0, ascending=False, inplace=True)\n\nvif","f51210f5":"# As TOEFL is highly correlated with GRE, CGPA and Average_value , tried with different combinations and chose CGPA\n\nX = scaled_df.iloc[:,[4,5,6]]\ny = scaled_df.iloc[:,8]\n\n# dividing the data into training data and test data\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, test_size = 0.2, random_state = 100)","5cf5fec6":"# creating regression model\nX_trainsm = sm.add_constant(X_train)\nlm7 = sm.OLS(y_train, X_trainsm).fit()\nprint(lm7.summary())","7efc28ab":"# For each X, calculate VIF and save in dataframe\nvif = pd.DataFrame()\nvif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif[\"features\"] = X.columns\nvif.sort_values(by = 'VIF', axis=0, ascending=False, inplace=True)\n\nvif","6ca3fca8":"lm = LinearRegression()\nlm.fit(X_train, y_train)\ny_pred = lm.predict(X_test)\n\n# Note lm7 directly could have been used for which the below code will be required\n#X_testsm = sm.add_constant(X_test)\n#y_pred = lm7.predict(X_testsm)","894d788d":"# Actual vs Predicted\n\nc = [i for i in range(1,81,1)]\nplt.figure(figsize = (10,8))\nplt.plot(c,y_test, color = 'blue', linewidth = 3 , linestyle = '-')\nplt.plot(c,y_pred, color = 'red', linewidth = 3 , linestyle = '-')\nplt.suptitle(\"Actual vs Predicted\", fontsize = 20)\nplt.xlabel(\"Index\", fontsize = 18)\nplt.ylabel('Probability of getting into a college', fontsize = 16)\nplt.show()","38d8dce3":"# Error terms\nc = [i for i in range(1,81,1)]\nfig = plt.figure(figsize = (10,8))\nplt.plot(c,y_test-y_pred, color=\"blue\", linewidth=3, linestyle=\"-\")\nfig.suptitle('Error Terms', fontsize=20)              # Plot heading \nplt.xlabel('Index', fontsize=18)                      # X-label\nplt.ylabel('y_test-y_pred', fontsize=16)\nplt.show()","6f5dcde9":"# Error terms scatter plot\nc = [i for i in range(1,81,1)]\nplt.figure(figsize = (10,8))\nplt.scatter(c, y_test-y_pred)\nfig.suptitle('Error Terms', fontsize=20)              # Plot heading \nplt.xlabel('Index', fontsize=18)                      # X-label\nplt.ylabel('y_test-y_pred', fontsize=16) \nplt.show()","5f9f02ea":"# Plotting the error terms to understand the distribution.\nplt.figure(figsize = (10,8))\nsns.distplot((y_test-y_pred),bins=50)\nfig.suptitle('Error Terms', fontsize=20)                  # Plot heading \nplt.xlabel('y_test-y_pred', fontsize=18)                  # X-label\nplt.ylabel('Index', fontsize=16)                          # Y-label\nplt.show()","5e650a27":"print(\"r2 value of train data : \" + str(lm.score(X_train,y_train)))\nprint(\"r2 value of test data : \" + str(lm.score(X_test,y_test)))\nprint('RMSE :', np.sqrt(mean_squared_error(y_test, y_pred)))","49aacc84":"lm.coef_\ncoeff_df = pd.DataFrame(lm.coef_, X_test.columns, columns = ['Coefficient'])\ncoeff_df","c0db6349":"# Visualizeing coefficients\nplt.figure(figsize = (10,8))\nsns.barplot(x = 'Coefficient', y = coeff_df.index, data = coeff_df)\nplt.suptitle('Coefficient strength plot', fontsize = 18)\nplt.show()","11e361b4":"# Constructing 5th model","98285830":"# Model Evaluation","fdf23446":"# Independent variables were selected based on p-value and VIF(Variance Inflation Factor).\n# For p-value alpha = 005. p-value greater than 0.05 signifies the independent variable was insignificant.\n# For VIF , if the vif value was <=2 or very close to 2, the independent variable was selected. Higher VIF for a particular independent variable means that the independent variable can be predicted with other independent variables, thus multicollinier .","36a44fed":"# Constructing 1st model","758cc913":"# Constructing 6th model","58bbfaeb":"* From the above correlation matrix , heat map and pair plot, it is clear that 'Chance of Admit' is somewhat correlated with every other independent variable.\n* Strong correlation exists between 'Chance of Admit' and 'GRE Score' , 'TOEFL Score' , 'CGPA' and 'Average_score'","3c78c698":"# Constructing 3rd model","c4ca2fdc":"# Predicting using 7th model","77d6c215":"# Constructing 4th model","9c0276de":"# Here we find that although 3 independent variables are included , CGPA has the highest coefficient. Therefore CGPA playes an important factor in deciding the chances of admission","2c444f7b":"# Constructing 7th model","ad374b66":"# Constructing 2nd model"}}