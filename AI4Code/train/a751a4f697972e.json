{"cell_type":{"b95ef54e":"code","2a6ac98e":"code","754afe85":"code","b2be1f2b":"code","609428f2":"code","25fead90":"code","14e42582":"code","cf68450e":"code","03f647c1":"code","bee67faf":"code","6bea4b4b":"code","b3a37528":"code","7c0d6ddd":"code","7a73622e":"code","e3b250c3":"code","ea55996e":"code","d6183f56":"code","5cc50754":"code","51933c96":"code","7c2cf576":"code","d6b337dc":"code","4b90f50c":"code","79956fed":"code","0d0dbcbc":"code","828314c1":"code","e6bb4ff4":"code","4388bc90":"code","c0170b80":"code","5ab61384":"code","ff431089":"code","d8f3e2dc":"code","f12205d2":"code","895d8da2":"code","9b51e2f7":"code","b38f968c":"code","1de4ec60":"code","3964b101":"code","3f7afdfa":"code","a59c9314":"code","da7ced04":"code","7192f28d":"code","b9a21cb2":"code","44be27e8":"code","60492d6e":"code","14eacca2":"code","3eb5d0cb":"code","e639e87f":"code","288b5b99":"code","ce4aa0e7":"code","630501eb":"code","6384ded5":"code","b8eb90cf":"code","df3d5c26":"code","a0f56466":"code","11e51c3e":"code","ba0a43e9":"code","77975d97":"code","72c19da2":"code","85810c3d":"code","c52957e4":"code","ef463f73":"code","6b53b6da":"code","c2b42936":"code","ca65798d":"code","76d45833":"code","d02f73b8":"code","ac5ac992":"code","998bc133":"code","0568b4b1":"code","cc9e4575":"code","79773cec":"code","b33a67be":"code","c9c4bc77":"markdown","800fcb69":"markdown","f2e37eeb":"markdown","ff327a40":"markdown","adca13a5":"markdown","789c94b5":"markdown","9f7c5b7a":"markdown","30e7ec98":"markdown","a04e0cdd":"markdown","ab51fea5":"markdown","2f95104f":"markdown","6268fbd8":"markdown","9b922e5d":"markdown","7d935830":"markdown","de7e37f1":"markdown","d6056166":"markdown","d792c87a":"markdown","27967f69":"markdown","fab2ceb6":"markdown"},"source":{"b95ef54e":"# import of packages\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nimport nltk\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\nfrom IPython.display import HTML","2a6ac98e":"pip install NRClex","754afe85":"from nrclex import NRCLex\n","b2be1f2b":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nsid = SentimentIntensityAnalyzer()","609428f2":"df = pd.read_csv('..\/input\/trip-advisor-hotel-reviews\/tripadvisor_hotel_reviews.csv')","25fead90":"df.head()","14e42582":"# checking for blanks\n\nblanks =[]\n\n#index, review, review rating.\nfor i,rv,rat in df.itertuples():\n    if rv.isspace():\n        blanks.append(i)\n","cf68450e":"blanks","03f647c1":"# checking for null values\n\ndf.isnull().sum()","bee67faf":"import re\n\ndef  clean_text(text):\n    \"\"\"\n    Fuction to clean the text data\n    * symbols\n    * change to lower_case\n    \"\"\"\n    text = text.str.lower()\n    text = text.apply(lambda T: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)|^rt|http.+?\", \"\", T))  \n        \n    return text","6bea4b4b":"df['Review']= clean_text(df['Review'])","b3a37528":"df['Review'] = df['Review'].str.replace('#','')","7c0d6ddd":"df.head()","7a73622e":"# packages for text preprocesing\n\nimport re\nimport nltk\nimport spacy\nimport string\npd.options.mode.chained_assignment = None # ignoring the warning","e3b250c3":"# text to lower case\n\ndf['Review'] = df['Review'].str.lower()","ea55996e":"# Removing Punctuation\n\npunk_Remove = string.punctuation\n\ndef del_punk(text):\n    \"\"\"\n    function to remove the Punctuation\n    \"\"\"\n    return text.translate(str.maketrans('','',punk_Remove))\n\ndf['text_no_punk'] = df['Review'].apply(lambda T: del_punk(T))\ndf.head(2)","d6183f56":"# removal of stopwords\n\n\nfrom nltk.corpus import stopwords\n\nFre_word = set(stopwords.words('english'))\n\ndef del_stopwords(text):\n    \"\"\"\n    function to remove the stopwords\n    \"\"\"\n    return \" \".join([word for word in str(text).split() if word not in Fre_word])\n\ndf['text_no_stopword'] = df['text_no_punk'].apply(lambda T: del_stopwords(T))\n\ndf.head(2)","5cc50754":"# most frequent words\n\nfrom collections import Counter\n\ncount = Counter()\n\nfor text in df['text_no_stopword'].values:\n    for word in text.split():\n        count[word] = count[word] + 1\n\ncount.most_common(10)","51933c96":"# Removing most Frequent words\n\nFreq_word = set([i for (i, ic) in count.most_common(10)])\n\n\ndef del_freq_word(text):\n    \"\"\"\n    This function will remove the frequent words\n    \n    \"\"\"\n    return \" \".join([word for word in str(text).split() if word not in Freq_word])\n\ndf['text_no_freqword'] = df['text_no_stopword'].apply(lambda text: del_freq_word(text))\n\ndf.head(2)","7c2cf576":"# Removal of Rare words\n\nrare_word = 10\n\nRarewords = set([i for (i, ic) in count.most_common()[:-rare_word:-1]])\n\ndef del_rarewords(text):\n    \"\"\"\n    function to remove the rare words\n    \"\"\"\n    return \" \".join([word for word in str(text).split() if word not in Rarewords])\n\n\n\ndf['text_no_Rareword'] = df['text_no_freqword'].apply(lambda T: del_rarewords(T))","d6b337dc":"#  Lemmatization (turn the words to base form for example: cars, car's should become car)\n\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\nwordnet_m = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n\n\ndef lemmat_pos_word(text):\n    \n    pos_tagger_text = nltk.pos_tag(text.split())\n    \n    return \" \".join([lemmatizer.lemmatize(word, wordnet_m.get(pos[0],wordnet.NOUN)) for word,pos in pos_tagger_text])\n\n\n\ndf['lemma_text'] = df['text_no_Rareword'].apply(lambda T : lemmat_pos_word(T))\n\ndf.head(2)","4b90f50c":"# removal of Urls\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndf['text_no_urls'] = df['lemma_text'].apply(lambda T : remove_html(T))\n\ndf.head(2)","79956fed":"# Removal of Emojis\n\n# Reference :https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\n\n\ndef remove_emoji(string):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', string)\n\n\ndf['text'] = df['text_no_urls'].apply(lambda T : remove_emoji(T))\n\ndf.head(2)","0d0dbcbc":"# drop of all other columns except rating and text review\n\ndf.columns","828314c1":"df.drop(['Review', 'text_no_punk', 'text_no_stopword',\n       'text_no_freqword', 'text_no_Rareword', 'lemma_text', 'text_no_urls'],axis= 1,inplace = True)","e6bb4ff4":"df['Rating'].unique()","4388bc90":"df.head()","c0170b80":"df['scores'] = df['text'].apply(lambda review: sid.polarity_scores(review))\n\ndf['compound']  = df['scores'].apply(lambda score_dict: score_dict['compound'])\n\ndf['comp_score'] = df['compound'].apply(lambda c: 'pos' if c >=0 else 'neg')\n\n","5ab61384":"df['number_of_words'] = df['text'].str.split().apply(lambda x: len(x))","ff431089":"df.head(2)","d8f3e2dc":"\ndist_t1 = df[df['Rating'] == 1]['number_of_words']\ndist_t2 = df[df['Rating'] == 2]['number_of_words']\ndist_t3 = df[df['Rating'] == 3]['number_of_words']\ndist_t4 = df[df['Rating'] == 4]['number_of_words']\ndist_t5 = df[df['Rating'] == 5]['number_of_words']\n\nfig,axes=plt.subplots(1,5,figsize=(18,5))\n\nsns.set_context(context='notebook',font_scale=1)\nsns.set_style('darkgrid')\n\nsns.distplot(dist_t1,color='#ee4035',bins=10,kde=False,ax=axes[0]);\nsns.distplot(dist_t2,color='#f37736',bins=10,kde=False,ax=axes[1]);\nsns.distplot(dist_t3,color='#fdf498',bins=10,kde=False,ax=axes[2]);\nsns.distplot(dist_t4,color='#7bc043',bins=10,kde=False,ax=axes[3]);\nsns.distplot(dist_t5,color='#0392cf',bins=10,kde=False,ax=axes[4]);\n\naxes[0].set_title(\"1 start rating\");\naxes[1].set_title(\"2 start rating\");\naxes[2].set_title(\"3 start rating\");\naxes[3].set_title(\"4 start rating\");\naxes[4].set_title(\"5 start rating\");\n\nplt.tight_layout()","f12205d2":"#let see what we have in the words\nimport spacy\n\nnlp = spacy.load('en_core_web_sm')\n\n\n\nhastag_token = ''\n\nfor words in df[df['Rating']== 1]['text'].value_counts().index[0:50]:\n    hastag_token += words\n\ndoc1 = nlp(hastag_token)","895d8da2":"from spacy import displacy\n\n\ndisplacy.render(doc1, style='ent',jupyter=True)\n\n#  using spacy-displacy analysing the one start rating review words ","9b51e2f7":"plt.figure(figsize=(10,6))\nsns.set_style('darkgrid')\nsns.set_context(context='notebook',font_scale=1.5)\nsns.countplot(x='Rating',data=df,palette='viridis');\nplt.title('Hotel Review Rating count for each start rating');","b38f968c":"plt.figure(figsize=(10,8))\nsns.set_style('darkgrid')\nsns.set_context(context='notebook',font_scale=1.5)\nsns.boxplot(x='Rating',y='number_of_words',data=df,palette='rainbow');\nplt.title('Text lenght for each star category');","1de4ec60":"plt.figure(figsize=(9,6))\nsns.set_style('darkgrid')\nsns.set_context(context='notebook',font_scale=1.5)\nsns.countplot(x='comp_score',data=df,palette='Set2');\nplt.title('overall Positive and Negative sentiments');","3964b101":"review_rating = df.groupby('Rating').mean()\nreview_rating\n\nplt.figure(figsize=(10,6))\nsns.set_style('darkgrid')\nsns.set_context(context='notebook',font_scale=1.5)\nsns.barplot(y='number_of_words',x=review_rating.index,data=review_rating,palette='rainbow');\nplt.title('Average number of words in each type of star rating');","3f7afdfa":"def Review_emo(word):\n    \"\"\"\n    Function to convert the raw data to utf-8 formate\n    * remove stopwords\n    * convert to data Frame\n    \n    \"\"\"\n    word = [word for word in word if word not in stopwords.words('english')]\n    word = str([cell.encode('utf-8') for cell in word])# to convert the text into utf-8 unicode\n    str_text = NRCLex(word) \n    str_text = str_text.raw_emotion_scores\n    str_text = pd.DataFrame(str_text,index=[0])\n    str_text = pd.melt(str_text)\n    str_text.columns = ('Emotions','Count')\n    str_text = str_text.sort_values('Count')\n    \n    return str_text\n\n","a59c9314":"rating_clean = Review_emo(df['text'])","da7ced04":"# emotional affects plot\n\n\nplt.figure(figsize=(9,6))\nsns.set_style('darkgrid')\nsns.set_context(context='notebook',font_scale=1.5)\nsns.barplot(y ='Emotions',x='Count', data = rating_clean[0:8] ,palette='viridis_r');\nplt.title('Emotions in the Review');","7192f28d":"from wordcloud import WordCloud, STOPWORDS\n\ncomment_words = '' \nstopwords = set(STOPWORDS) \n  \n# iterate through the csv file \nfor val in df['text']: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n  \nwordcloud = WordCloud(width = 1900, height = 1500, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = '#40e0d0') \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.title('high frequency words in Hotel Reviews')  \nplt.show() ","b9a21cb2":"from sklearn.feature_extraction.text import CountVectorizer\n\ndef text_ngrams(corpus, n, g):\n    vec = CountVectorizer(ngram_range=(g, g)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","44be27e8":"plt.figure(figsize = (10,6))\nsns.set_style('darkgrid')\nsns.set_context(context='notebook',font_scale=1.5)\nUnigram_word = text_ngrams(df.text,10,1)\nUnigram_word = dict(Unigram_word)\nsns.barplot(x=list(Unigram_word.values()),y=list(Unigram_word.keys()),palette = 'rainbow');\nplt.title('Unigram Analysis');","60492d6e":"plt.figure(figsize = (10,6))\nsns.set_style('darkgrid')\nsns.set_context(context='notebook',font_scale=1.5)\nUnigram_word = text_ngrams(df.text,10,2)\nUnigram_word = dict(Unigram_word)\nsns.barplot(x=list(Unigram_word.values()),y=list(Unigram_word.keys()),palette = 'rainbow');\nplt.title('Bigram Analysis');","14eacca2":"# Trigram analysis for one star rating\n\nplt.figure(figsize = (10,6))\nsns.set_style('darkgrid')\nsns.set_context(context='notebook',font_scale=1.5)\nUnigram_word = text_ngrams(df[df['Rating']==1]['text'],10,3)\nUnigram_word = dict(Unigram_word)\nsns.barplot(x=list(Unigram_word.values()),y=list(Unigram_word.keys()),palette = 'rainbow');\nplt.title('Trigram analysis for one star rating');","3eb5d0cb":"plt.figure(figsize = (10,6))\nsns.set_style('darkgrid')\nsns.set_context(context='notebook',font_scale=1.5)\nUnigram_word = text_ngrams(df[df['Rating']==5]['text'],10,3)\nUnigram_word = dict(Unigram_word)\nsns.barplot(x=list(Unigram_word.values()),y=list(Unigram_word.keys()),palette = 'rainbow');\nplt.title('Trigram analysis for five star rating');","e639e87f":"from sklearn.model_selection import train_test_split","288b5b99":"X = df['text']","ce4aa0e7":"y= df['comp_score']","630501eb":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","6384ded5":"from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC","b8eb90cf":"review_text = Pipeline([('tfidf',TfidfVectorizer()),('clf',LinearSVC())])","df3d5c26":"review_text.fit(X_train,y_train)","a0f56466":"predictions = review_text.predict(X_test)","11e51c3e":"from sklearn.metrics import  classification_report, accuracy_score","ba0a43e9":"print(classification_report(y_test,predictions))","77975d97":"print(accuracy_score(y_test,predictions))","72c19da2":"!pip install ktrain","85810c3d":"import ktrain\nfrom ktrain import text","c52957e4":"label = {1: 0,2: 1,3: 2,4: 3,5: 4}\n        \ncate = df['Rating'].copy()\ncate.replace(label, inplace=True)","ef463f73":"df['Rating']= cate","6b53b6da":"# spliting the data into train and test\n\ndf['split'] = np.random.randn(df.shape[0], 1)\n\nmn = np.random.rand(len(df)) <= 0.7\n\ntrain = df[mn]\ntest = df[~mn]","c2b42936":"# ktrain preprocess\n\n(X_train,y_train),(X_test,y_test),preprocess = text.texts_from_df(train_df = train,\n                   text_column ='text',\n                  label_columns = 'Rating',\n                  val_df = test,\n                  maxlen = 200,\n                  preprocess_mode ='bert')","ca65798d":"# model \n\nmodel = text.text_classifier(name= 'bert',\n                            train_data = (X_train, y_train),\n                            preproc = preprocess)","76d45833":"#  To Get optimal learning Rate\n\nlearner = ktrain.get_learner(model = model,\n                            train_data = (X_train, y_train),\n                            val_data = (X_test,y_test),\n                            batch_size = 6)","d02f73b8":"#learner.lr_find(max_epochs=4)","ac5ac992":"#learner.lr_plot()","998bc133":"learner.fit_onecycle(lr = 4e-5, epochs =2)","0568b4b1":"predictor = ktrain.get_predictor(learner.model, preprocess)","cc9e4575":"#  predicting hotel review actual rating 1\n\nhotel_review1_neg = [\"Hotel location good environment general room window is closed can not be opened the first day check-in air conditioning is broken especially sultry after negotiation to send us a day of breakfast as compensation result hotel no restaurant is just a two-meter long bar breakfast only a cup of coffee and a piece The bread is especially difficult to eat. The next time you stay, you will choose the same price. You will stay away from the city.\"]\n\npredictor.predict(hotel_review1_neg)","79773cec":"#  predicting hotel review rating 1\n\nhotel_review2_neg = ['The rooms are super small. There is barely any room to walk. After a tired day of sightseeing also we did not feel like going back to the room knowing it is so small']\n\npredictor.predict(hotel_review2_neg)\n\n","b33a67be":"#  predicting hotel review actual rating 5\n\nhotel_pos = ['Beautiful hotel, incredible service. The front dealing texting service was amazing - so incredibly helpful. Check in and out was easy. We felt safe with all of the covid procedures in place. Gorgeous view of the CN Tower and we got a really great rate!!! Thank you for taking care of us for a little weekend getaway']\n\npredictor.predict(hotel_pos)","c9c4bc77":"![]()![image.png](attachment:image.png)","800fcb69":"NRCLex will measure emotional affect from a body of text. Affect dictionary contains approximately 27,000 words, and is based on the National Research Council Canada (NRC) affect lexicon.\nhttp:\/\/sentiment.nrc.ca\/lexicons-for-research\/","f2e37eeb":"# <font color='#ffcc5c'>Agenda<\/font>\n\n*  [About this notebook](#About-this-notebook)\n*  [Import of data](#Import-of-data)\n* [Adding basic Features](#Adding-basic-Features)\n* [Data cleaning](#Data-cleaning)\n*  [Text preprocessing](#Text-preprocessing)\n* [Data Visualization](#Data-Visualization)\n*  [Emotions](#NCRLex)\n* [BERT model using k train](#BERT-Ktrain-model-to-predict-the-review-rating)\n*  [Reference](#Reference)\n","ff327a40":"\n\n\n## thanks\n\n\n\n#### [Back to Agenda](#Agenda)","adca13a5":"# <font color='#ffcc5c'>Adding basic Features<\/font>\n\n\n\n#### [Back to Agenda](#Agenda)","789c94b5":"# <font color='#ffcc5c'>Reference<\/font>\n\n* Thanks to SRK for creating very good kernal on NLP Text preprocessing, [link to notebook](https:\/\/www.kaggle.com\/sudalairajkumar\/getting-started-with-text-preprocessing)\n\n* Thanks to  Trigram analysis notebook [Trigram function link](https:\/\/www.kaggle.com\/madz2000\/nlp-using-glove-embeddings-99-87-accuracy)\n\n* [image credit](https:\/\/www.competethemes.com\/blog\/create-site-like-tripadvisor\/)","9f7c5b7a":"# <font color='#ffcc5c'>Data Visualization<\/font>\n\n\n#### [Back to Agenda](#Agenda)","30e7ec98":"# <font color='#ffcc5c'>Import of data<\/font>\n\n\n\n#### [Back to Agenda](#Agenda)","a04e0cdd":"# <font color='#ffcc5c'>About this notebook<\/font>\n\n##### <font color='#1ebbd7'>This notebook aims to predict the Trip Advisor Hotel Review Sentiments and review rating with the given hotel review further to that we have some text data analysis like bigram and trigram with some text length distribution to see some insight and we have some future Engineering with text using Vader from NLTK package and we have NRC lexicon which explores the emotions affects in the review text. for prediction part, we are using TFIT that term frequency and document frequency to predict the review sentiment.<\/font>\n\n* Predict Review Rating\n\n\n\n#### [Back to Agenda](#Agenda)","ab51fea5":"# NCRLex","2f95104f":"#### ktrain is a wrapper for TensorFlow Keras that makes deep learning and AI more accessible and easier to apply\n\nhttps:\/\/pypi.org\/project\/ktrain","6268fbd8":"### Trigram analysis for one star rating\n\n\n#### It shows the hints of the problems\n\n* problem with Air condition in the hotel room\n\n* credit card charges for the hotel room booking\n\n* 20 minute of waiting time\n\n\n\n","9b922e5d":"# Trigram analysis for five star shows\n\n#### what made customer happy in hotel stay\n\n* Flat screen tv in the hotel room\n* Free internet access in the hotel room\n* walk away distance from the public transport\n","7d935830":"# <font color='#88d8b0'>Trip Advisor Hotel Review Sentiments prediction and review rating prediction<\/font>","de7e37f1":"# <font color='#ffcc5c'>BERT-Ktrain model to predict the review rating<\/font>\n\n\n\n#### [Back to Agenda](#Agenda)","d6056166":"# <font color='#ffcc5c'>Machine learning model<\/font>\n\n\n\n#### [Back to Agenda](#Agenda)","d792c87a":"# <font color='#ffcc5c'>Text preprocessing<\/font>\n\n\n\n\nTest preprocessing is most important part of any NLP machine learning proces and it is removing any noise out of the detail need to concentrate\nTokenization(chopping text into each individual word)\n\n* Removing stop words(stop word are extremely common words \u201cthe\u201d, \u201cis\u201d and \u201cand\u201d)\n\n* Normalization of words (USA has more foams like U.S., United States and so we eliminate data redundancy(repetition)\n\n* Lemmatization (turn the words to base form for example: cars, car's should become car)\n\n\n\n\n#### [Back to Agenda](#Agenda)","27967f69":"#  <font color='#ffcc5c'>Data cleaning<\/font>\n\n\n\n#### [Back to Agenda](#Agenda)","fab2ceb6":"# VADER\n\nA SentimentAnalyzer is a tool to implement and facilitate Sentiment Analysis tasks using NLTK features and classifiers, especially for teaching and demonstrative purposes."}}