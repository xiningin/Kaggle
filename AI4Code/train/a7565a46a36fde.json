{"cell_type":{"11ba4acb":"code","6c065cf3":"code","7d26847f":"code","42fd528a":"code","92e3d77c":"code","9ab87552":"code","b88138a4":"code","a97770e1":"code","fe8a399e":"code","9a96a43f":"code","7f4f2294":"code","dde668e0":"code","73684477":"code","dfc1e94b":"code","11f4942a":"markdown","92e6a5af":"markdown","6a85afff":"markdown","704245d3":"markdown","7f3a29b7":"markdown","06b5be22":"markdown"},"source":{"11ba4acb":"! apt-get update && apt-get install -y openjdk-8-jdk\n! pip install --upgrade konlpy \"tweepy<4\" scikit-learn\n! bash <(curl -s https:\/\/raw.githubusercontent.com\/konlpy\/konlpy\/master\/scripts\/mecab.sh)","6c065cf3":"import csv\n\ntrain_dataset = []\nwith open('\/kaggle\/input\/2021-lg-ai-camp-nlp-final-project\/ratings_train.csv', 'r', encoding='utf-8') as fd:\n    reader = csv.reader(fd)\n    for idx, (_, sentence, rating) in enumerate(reader):\n        if idx == 0:\n            continue\n        train_dataset.append((sentence, int(rating)))        # 0 for positive, 1 for negative\nprint(len(train_dataset))","7d26847f":"print(train_dataset[0])\ntrain_sentences, train_ratings = zip(*train_dataset)\nprint(train_sentences[0])","42fd528a":"from konlpy.tag import Mecab\n\nclass MorphTokenizer:\n    def __init__(self):\n        self.tagger = Mecab()\n        \n    def __call__(self, sentence):\n        return self.tagger.morphs(sentence)","92e3d77c":"tokenizer = MorphTokenizer()\nprint(tokenizer('\ud55c\uad6d\uc5b4\ub294 \ud1a0\ud070\ud654\ub294 \uc5b4\ub824\uc6cc\uc694....'))","9ab87552":"from collections import Counter\nfrom itertools import chain\nfrom tqdm import tqdm\n\nimport numpy as np\nimport scipy.sparse as sparse\n\nclass BoWVectorizer:\n    def __init__(self, vocab, tokenizer):\n        self.unk_token = '<unk>'\n        self.id2token = [self.unk_token] + vocab\n        self.token2id = {token: token_id for token_id, token in enumerate(self.id2token)}\n        self.tokenizer = tokenizer\n    \n    @classmethod\n    def build_vocab(cls, sentences, tokenizer, min_freq=2):\n        counter = Counter(chain.from_iterable(\n            tokenizer(sentence) for sentence in tqdm(sentences, desc=\"Counting\")\n        ))\n        \n        return cls([token for token, counts in counter.items() if counts >= min_freq], tokenizer)\n    \n    @property\n    def unk_token_id(self):\n        return self.token2id[self.unk_token]\n    \n    @property\n    def vocab_size(self):\n        return len(self.id2token)\n    \n    def __call__(self, sentences, verbose=False):\n        counters = [\n            Counter(\n                self.token2id[token if token in self.token2id else self.unk_token]\n                for token in self.tokenizer(sentence)\n            ) for sentence in (tqdm(sentences, desc='Counting') if verbose else sentences)\n        ]\n        \n        data = []\n        row = []\n        col = []\n        \n        for idx, counter in enumerate(tqdm(counters, desc=\"Building\") if verbose else counters):\n            if counter:\n                indices, counts = zip(*counter.items())\n                data.extend([count \/ sum(counts) for count in counts])\n                row.extend([idx] * len(indices))\n                col.extend(indices)\n            else:\n                data.append(1.)\n                row.append(idx)\n                col.append(0)\n        return sparse.csr_matrix((data, (row, col)), shape=(len(sentences), self.vocab_size))\n\n                ","b88138a4":"vectorizer = BoWVectorizer.build_vocab(train_sentences, tokenizer)","a97770e1":"print(vectorizer.vocab_size)\nprint(vectorizer.token2id['\ud55c\uad6d'])\nvector = vectorizer(['\ud55c\uad6d\uc5b4 \uc784\ubca0\ub529\uc740 \uc5b4\ub824\uc6cc\uc694....', '\uc790\uc5f0\uc5b4 \ucc98\ub9ac \ud654\uc774\ud305!'])\nprint(vector)","fe8a399e":"from sklearn.linear_model import LogisticRegression\n\nclass LinearClassifier:\n    def __init__(self, dataset, vectorizer, verbose=1):\n        self.vectorizer = vectorizer\n        \n        sentences, ratings = zip(*dataset)\n        vectors = vectorizer(sentences, verbose=bool(verbose))\n        ratings = np.array(ratings)\n        \n        self.model = LogisticRegression(verbose=verbose, max_iter=500)\n        self.model.fit(vectors, ratings)\n        \n    def __call__(self, sentences):\n        return self.model.predict(self.vectorizer(sentences)).tolist()\n","9a96a43f":"classifier = LinearClassifier(train_dataset, vectorizer)","7f4f2294":"classifier(['\ubcf4\uba74\uc11c \uc9dc\uc99d\ub098\ub354\ub77c', '\ubcfc\ub9cc\ud568'])","dde668e0":"import csv\n\ntest_dataset = []\nwith open('\/kaggle\/input\/2021-lg-ai-camp-nlp-final-project\/ratings_test.csv', 'r', encoding='utf-8') as fd:\n    reader = csv.reader(fd)\n    for idx, (sid, sentence) in enumerate(reader):\n        if idx == 0:\n            continue\n        test_dataset.append((sid, sentence))        # 0 for positive, 1 for negative\nprint(len(test_dataset))","73684477":"print(test_dataset[0])\nsids, test_sentences = zip(*test_dataset)\nprint(test_sentences[0])","dfc1e94b":"with open('\/kaggle\/working\/submission.csv', 'w', encoding='utf-8') as fd:\n    writer = csv.writer(fd)\n    writer.writerow(['Id', 'Label'])\n    test_pred = classifier(test_sentences)\n    writer.writerows(zip(sids, test_pred))","11f4942a":"## Output test result","92e6a5af":"## Install prerequisites (KoNLPy & Mecab)","6a85afff":"## Define a vectorizer","704245d3":"## Define a Mecab tokenizer","7f3a29b7":"## Define & train a linear classifer\n","06b5be22":"## Load training data"}}