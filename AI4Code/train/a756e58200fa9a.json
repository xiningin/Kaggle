{"cell_type":{"02985d16":"code","0cc7b96f":"code","436c19bd":"code","ce78065f":"code","09ee19e8":"code","9c37b6e2":"code","79c8ba4c":"markdown","0aa4652c":"markdown","9f60e1fe":"markdown","62939802":"markdown","fbfd3d78":"markdown","c1a0bb64":"markdown"},"source":{"02985d16":"import numpy as np # linear algebra\nimport matplotlib.pylab as plt # Plotting\nimport sklearn # Machine learning models.\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nimport sklearn.metrics # Area under ROC curve calculations. \nfrom sklearn.model_selection import train_test_split as tts # Train test split \nfrom sklearn.model_selection import KFold # KFold cross validation \nimport pandas as pd # Quick dataframe previewing \n\n\n\nfilename = '\/kaggle\/input\/higgs-boson-detection\/train.csv'\n\n\ndata = np.loadtxt(filename, skiprows=1, delimiter=',')\n\nX = data[:,1:]\nY = data[:,0:1]\n\n# Split off validation set, size = 30% validation 70% training\nXtrain, Xvalid, Ytrain, Yvalid = tts(X, Y, test_size=.3)\n\n","0cc7b96f":"# import numpy as np # linear algebra\n# import matplotlib.pylab as plt # Plotting\n# import sklearn # Machine learning models.\n# from sklearn.neighbors import KNeighborsClassifier as KNN\n# import sklearn.metrics # Area under ROC curve calculations. \n# from sklearn.model_selection import train_test_split as tts # Train test split \n# from sklearn.model_selection import KFold # KFold cross validation \n# import pandas as pd # Quick dataframe previewing \n\n\n\n# filename = '\/kaggle\/input\/higgs-boson-detection\/train.csv'\n\n\n# data = np.loadtxt(filename, skiprows=1, delimiter=',')\n\n# X = data[:,1:]\n# Y = data[:,0:1]\n\n# # Split off validation set, size = 30% validation 70% training \n# Xtrain, Xvalid, Ytrain, Yvalid = tts(X, Y, test_size=.3)\n\n\n# Split training set with KFoldValidation in order to perform hyperparameter optimization\nkf = KFold(n_splits = 15)\nkFolds = kf.split(Xtrain)\n\n# Set 'k' in kNN algo, declare dictionary to hold AUROC values {kval : auroc}\nk = 1\naurocs = {}\n\n\n# Use kFold technique on training set to test models with different values of k starting at 3\nfor train_index, test_index in kFolds:\n    k += 2\n    model = KNN(n_neighbors=k)\n    model.fit(Xtrain[train_index], Ytrain[train_index][:,0])\n    predictions = model.predict_proba(Xtrain[test_index])\n    val = sklearn.metrics.roc_auc_score(Ytrain[test_index], predictions[:,1])\n    print(f'Validation AUROC for KNN with k = {k}: {val}')\n    aurocs[k] = val\n\n# Choose optimized k-value and evaulate AUROC value on the untouched validation set (note: not the same validation set used in kFolds technique)\nmodel = KNN(n_neighbors=max(aurocs, key=aurocs.get)) # Set model with optimized k-value\nmodel.fit(Xtrain[train_index], Ytrain[train_index][:,0]) # Fit model on the training set\npredictions = model.predict_proba(Xvalid) # Predict on validation set\nval = sklearn.metrics.roc_auc_score(Yvalid, predictions[:,1]) # Calculate AUROC value\nprint(f'Max AUROC value at k = {max(aurocs, key=aurocs.get)}. AUROC value with optimized kValue on validation set = {val}') # Print AUROC value on untouched validation set\n\n# Plot ROC curve.\nfpr, tpr, thresholds = sklearn.metrics.roc_curve(Yvalid, predictions[:,1])\nplt.plot(fpr, tpr)\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(f'kNN with k = {max(aurocs, key=aurocs.get)}')","436c19bd":"# Using decision tree classifier\nfrom sklearn import tree # Decision tree algo\n\n# Split training set with KFoldValidation\nkf_two = KFold(n_splits = 10)\nkFolds_two = kf_two.split(Xtrain)\n\n# Set 'max_depth' in decision tree algo, declare dictionary to hold AUROC values {max_depth : auroc}\nmdepth = 1\naurocs_two = {}\n\n\n# Use kFold technique on training set to test models with different values of max_depth starting at 3\nfor train_index, test_index in kFolds_two:\n    mdepth += 2\n    model_two = tree.DecisionTreeClassifier(max_depth=mdepth)\n    model_two.fit(Xtrain[train_index], Ytrain[train_index][:,0])\n    predictions_two = model_two.predict_proba(Xtrain[test_index])\n    val_two = sklearn.metrics.roc_auc_score(Ytrain[test_index], predictions_two[:,1])\n    print(f'Validation AUROC for Decision Tree with max_depth = {mdepth}: {val_two}')\n    aurocs_two[mdepth] = val_two\n    \n# Choose optimized max_depth and evaulate AUROC value on the untouched validation set (note: not the same validation set used in kFolds technique)\nmodel_two = tree.DecisionTreeClassifier(max_depth=max(aurocs_two, key=aurocs_two.get)) # Set model with optimized max_depth value\nmodel_two.fit(Xtrain[train_index], Ytrain[train_index][:,0]) # Fit model on the training set\npredictions_two = model_two.predict_proba(Xvalid) # Predict on validation set\nval_two = sklearn.metrics.roc_auc_score(Yvalid, predictions_two[:,1]) # Calculate AUROC value\nprint(f'Max AUROC value at max_depth = {max(aurocs_two, key=aurocs_two.get)}. AUROC value with optimized kValue on validation set = {val_two}') # Print AUROC value on untouched validation set\n\n# Plot ROC curve.\nfpr_two, tpr_two, thresholds_two = sklearn.metrics.roc_curve(Yvalid, predictions_two[:,1])\nplt.plot(fpr_two, tpr_two)\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(f'Decision tree with max_depth = {max(aurocs_two, key=aurocs_two.get)}')","ce78065f":"# Using decision tree classifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Split training set with KFoldValidation\nkf_two = KFold(n_splits = 10)\nkFolds_two = kf_two.split(Xtrain)\n\n# Set 'max_depth' in random forest algo, declare dictionary to hold AUROC values {max_depth : auroc}\nmdepth = 1\naurocs_two = {}\n\n\n# Use kFold technique on training set to test models with different values of max_depth starting at 3\nfor train_index, test_index in kFolds_two:\n    mdepth += 2\n    model_two = RandomForestClassifier(max_depth=mdepth)\n    model_two.fit(Xtrain[train_index], Ytrain[train_index][:,0])\n    predictions_two = model_two.predict_proba(Xtrain[test_index])\n    val_two = sklearn.metrics.roc_auc_score(Ytrain[test_index], predictions_two[:,1])\n    print(f'Validation AUROC for Random Forest with max_depth = {mdepth}: {val_two}')\n    aurocs_two[mdepth] = val_two\n\n# Assign a variable to the max_depth value that gave the best AUROC value\nbestDepth = max(aurocs_two, key=aurocs_two.get)\n\nn_est = 80 # n_estimators, the next hyperparameter that will be tested\naurocs_est = {} # Declare a dictionary to hold {n_estimators : auroc}\n\n# Redo training set split with KFoldValidation to test the hyperparameter 'n_estimators'\nkf_two = KFold(n_splits = 10)\nkFolds_two = kf_two.split(Xtrain)\n\n# Use kFold technique on training set to test models with different values of n_estimators starting at 100\n# In this iteration we will be using the optimized value of max_depth found in the previous iteration\nfor train_index, test_index in kFolds_two:\n    n_est += 20\n    model_two = RandomForestClassifier(max_depth=bestDepth, n_estimators=n_est)\n    model_two.fit(Xtrain[train_index], Ytrain[train_index][:,0])\n    predictions_two = model_two.predict_proba(Xtrain[test_index])\n    val_two = sklearn.metrics.roc_auc_score(Ytrain[test_index], predictions_two[:,1])\n    print(f'Validation AUROC for Random Forest with max_depth = {bestDepth} and n_estimators = {n_est}: {val_two}')\n    aurocs_est[n_est] = val_two # {n_est : auroc}\n\n# Assign a variable to the n_estimators value that gave the best AUROC value\nbestEst = max(aurocs_est, key=aurocs_est.get)\n    \n# Use optimized hyperparameters and evaulate AUROC value on the untouched validation set (note: not the same validation set used in kFolds technique)\nmodel_two = RandomForestClassifier(max_depth=bestDepth, n_estimators=bestEst) # Set model with optimized hyperparameter values\nmodel_two.fit(Xtrain[train_index], Ytrain[train_index][:,0]) # Fit model on the training set\npredictions_two = model_two.predict_proba(Xvalid) # Predict on validation set\nval_two = sklearn.metrics.roc_auc_score(Yvalid, predictions_two[:,1]) # Calculate AUROC value\nprint(f'Max AUROC value at max_depth = {bestDepth} and n_estimators = {bestEst}. AUROC value with optimized kValue on validation set = {val_two}') # Print AUROC value on untouched validation set\n\n# Plot ROC curve.\nfpr_two, tpr_two, thresholds_two = sklearn.metrics.roc_curve(Yvalid, predictions_two[:,1])\nplt.plot(fpr_two, tpr_two)\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(f'Random Forest with max_depth = {bestDepth} and n_estimators = {bestEst}')","09ee19e8":"# Using gradient boosting classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Split training set with KFoldValidation\nkf_two = KFold(n_splits = 10)\nkFolds_two = kf_two.split(Xtrain)\n\n# Set 'max_depth' in random gradient boosting algo, declare dictionary to hold AUROC values {max_depth : auroc}\nmdepth = 1\naurocs_two = {}\n\n\n# Use kFold technique on training set to test models with different values of max_depth starting at 3\nfor train_index, test_index in kFolds_two:\n    mdepth += 2\n    model_two = GradientBoostingClassifier(max_depth=mdepth)\n    model_two.fit(Xtrain[train_index], Ytrain[train_index][:,0])\n    predictions_two = model_two.predict_proba(Xtrain[test_index])\n    val_two = sklearn.metrics.roc_auc_score(Ytrain[test_index], predictions_two[:,1])\n    print(f'Validation AUROC for Gradient Boosting with max_depth = {mdepth}: {val_two}')\n    aurocs_two[mdepth] = val_two\n\n# Assign a variable to the max_depth value that gave the best AUROC value\nbestDepth = max(aurocs_two, key=aurocs_two.get)\n\nn_est = 80 # n_estimators, the next hyperparameter which will be tested\naurocs_est = {} # Declare a dictionary to hold {n_estimators : auroc}\n\n# Redo training set split with KFoldValidation to test the hyperparameter 'n_estimators'\nkf_two = KFold(n_splits = 10)\nkFolds_two = kf_two.split(Xtrain)\n\n# Use kFold technique on training set to test models with different values of n_estimators starting at 100\n# In this iteration we will be using the optimized value of max_depth found in the previous iteration\nfor train_index, test_index in kFolds_two:\n    n_est += 20\n    model_two = GradientBoostingClassifier(max_depth=bestDepth, n_estimators=n_est)\n    model_two.fit(Xtrain[train_index], Ytrain[train_index][:,0])\n    predictions_two = model_two.predict_proba(Xtrain[test_index])\n    val_two = sklearn.metrics.roc_auc_score(Ytrain[test_index], predictions_two[:,1])\n    print(f'Validation AUROC for Gradient Boosting with max_depth = {bestDepth} and n_estimators = {n_est}: {val_two}')\n    aurocs_est[n_est] = val_two # {n_est : auroc}\n\n# Assign a variable to the n_estimators value that gave the best AUROC value\nbestEst = max(aurocs_est, key=aurocs_est.get)\n    \n# Use optimized hyperparameters and evaulate AUROC value on the untouched validation set (note: not the same validation set used in kFolds technique)\nmodel_two = GradientBoostingClassifier(max_depth=bestDepth, n_estimators=bestEst) # Set model with optimized hyperparameter values\nmodel_two.fit(Xtrain[train_index], Ytrain[train_index][:,0]) # Fit model on the training set\npredictions_two = model_two.predict_proba(Xvalid) # Predict on validation set\nval_two = sklearn.metrics.roc_auc_score(Yvalid, predictions_two[:,1]) # Calculate AUROC value\nprint(f'Max AUROC value at max_depth = {bestDepth} and n_estimators = {bestEst}. AUROC value with optimized kValue on validation set = {val_two}') # Print AUROC value on untouched validation set\n\n# Plot ROC curve.\nfpr_two, tpr_two, thresholds_two = sklearn.metrics.roc_curve(Yvalid, predictions_two[:,1])\nplt.plot(fpr_two, tpr_two)\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(f'Gradient Boosting with max_depth = {bestDepth} and n_estimators = {bestEst}')","9c37b6e2":"# Make probabilistic predictions.\nfilename = '\/kaggle\/input\/higgs-boson-detection\/test.csv' # This is the path if running on kaggle.com. Otherwise change this.\nXtest1 = np.loadtxt(filename, skiprows=1, delimiter=',', usecols=range(1,29))\npredictions = model_two.predict_proba(Xtest1) # Choose the best model\npredictions = predictions[:,1:2] # Predictions has two columns. Get probability that label=1.\nN = predictions.shape[0]\nassert N == 50000, \"Predictions should have length 50000.\"\nsubmission = np.hstack((np.arange(N).reshape(-1,1), predictions)) # Add Id column.\nnp.savetxt(fname='submission.csv', X=submission, header='Id,Predicted', delimiter=',', comments='')\n\n# If running on Kaggle.com, submission.csv can be downloaded from this Kaggle Notebook under Sessions->Data->output->\/kaggle\/working.","79c8ba4c":"# Optimize and Validate kNN model","0aa4652c":"# Evaluating Alternate Model (Gradient Boosting)","9f60e1fe":"# Make Predictions and Prepare Submission File","62939802":"# Evaluating Alternate Model (Random Forests)","fbfd3d78":"# Evaluating Alternate Model (Decision Trees)","c1a0bb64":"# Model Selection and Hyperparameter Optimization\n\n## Hyperparameter Optimization\nIn this notebook, I perform hyperparameter optimization on 4 models with the help of the kFold cross validation technique. I do this using kFold splitting and ROC curves. Instead of performing standard kFold validation to validate a model's performance (k-1 sets for training and the 1 held out set for validation) I do the following: \n\n1) Import *training* data only\n\n2) Split training data **further** into training and validation sets. This validation set will remain *untouched* for now\n\n3) Split the training set **further** into k subsets with 1 held-out validation set (kFolds)\n\n4) On each iteration of the kFolds technique: \n    - k-1 sets are used for training the model with hyperparameter(n)\n    - 1 held-out set is used to calculate the AUROC values of the model with hyperparameter(n) \n    - Change the value of n\n    \n5) Choose the hyperparameter value that resulted in the highest AUROC value\n\n6) Evaluate the model with optimized hyperparameters on the *untouched* validation set \n\nAnd so, rather than using kFold cross-validation to validate the accuracy of a single model across k subsets, I am using each subset to test the model with **different** hyperparameters. I then select the hyperparameter value which produced the highest AUROC value and retrain the model using this value. Afterwards, I validated the model's performance on the *untouched* validation set from the first split I performed on the training data (step 2). \n\nWhat this approach does:\n* Prevents model overfitting by *first* performing hyperparameter optimization *then* validating the model on an untouched set \n* Model validation performed on the untouched validation set \n\nWhat this approach **does not** do: \n* Actual kFold cross validation\n\n## Model Validation\nAnd so, although the above steps may be misleading because I use the kFolds technique, I do not use kFold splitting to *validate* my model, but rather to test and select the optimal hyperparameter. I then perform a simple one-step validation using the set I split off in step 2.\n\n## Model Selection and Performance Metrics\nI performed the above hyperparameter optimization and validation strategies on 4 learning algorithms: kNearestNeighbors, Decision Trees, Random Forests, and Gradient Boosting. The metric I used to evaluate each model's performance is their AUROC values as derived in step 6. I compared all values and chose the model with the higher AUROC value.\n\nTo summarize my results, I found k = 23 to be the optimal k-value for kNN, and max_depth = 7 to be optimal for my decision tree. Thinking about this now, if I wanted to optimize the models further I could repeat the same process with other hyperparameters to try to improve performance. However, I think I chose the most impactful hyperparameters to adjust because of the large resulting impact in AUROC values.\n\n## Result of Analysis \nThe winning model turned out to be Gradient Booosted Trees: with an AUROC value of .79 compared to the .63 of my first model kNN. This is clearly visually represented by the graphed ROC curves that show the large difference in area under the curve between the two models. \n\nThis analysis has shown me why it's so important to 1) Pick the most effective model, 2) Optimize it's hyperparameters, and 3) Evaluate its performance on a validation set. These three steps maximize your model's effectiveness, prevent overfitting, and make sure you pick the best model for the situation. I will definitely be using these techniques to compare model performance in the future. \n\n*Notes:*\n- Because of randomized data splits, running the notebook gives slightly different values for the best hyperparameters each time. My code automatically uses the 'best' value to create the final model, and so the models' performance may vary slightly each time the notebook is ran. \n- Originally I was performing between 10 and 15 splits with the kFold technique, and evaluating 10-15 different hyperparameters. After the completion of this notebook, while writing this top portion, I had the idea that since this is such a large dataset, I could do something super large such as 100 splits. Testing 100 different hyperparameters might actually give me more optimal values than testing with 10-15, so I tried this out. Very interestingly, even testing 100 different k-values with kNN resulted in an optimal k very close to the one I found testing only 15. The same exact thing can be said about the decision tree.\n\n*by Chinenye Ndili*"}}