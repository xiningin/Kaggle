{"cell_type":{"e0d2922b":"code","7db44966":"code","0c5dacb0":"code","121ded06":"code","cd81775f":"code","5d0f3988":"code","49b44294":"code","c392ae93":"code","7aeaedde":"code","48664aa2":"code","41d69ff9":"code","1044b1ae":"code","b39b0960":"code","35176285":"code","c969fea3":"code","f0ecd908":"code","c9904219":"code","465c45ca":"code","ba11d500":"code","1fd467b2":"code","39c65107":"code","6b666262":"code","a7798143":"code","5a731b11":"markdown","d2007448":"markdown","d4f04b67":"markdown","61271845":"markdown","e9c3befe":"markdown","9b2dfc04":"markdown","752d36eb":"markdown","ea8a8f19":"markdown","66f5f290":"markdown","d7960579":"markdown","76c38d96":"markdown","cb8d33e3":"markdown","1c3f7925":"markdown","9b9d9758":"markdown","ea6d3723":"markdown","4e0fb4b8":"markdown","1c6d9fa3":"markdown","31e46071":"markdown","0fa5f542":"markdown","37699033":"markdown","813fb851":"markdown"},"source":{"e0d2922b":"import os\nimport numpy as np\nimport random\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport string\nimport wordcloud\nimport spacy\nimport nltk\nfrom collections import Counter\n\nrandom.seed(42)\nsns.set(rc={'figure.figsize':(12,6)})","7db44966":"train_dir = \"..\/input\/feedback-prize-2021\/train\"\ntest_dir = \"..\/input\/feedback-prize-2021\/test\"\ntrain_files = os.listdir(train_dir)\ntest_files = os.listdir(test_dir)\n\nfor file in range(len(train_files)):\n    train_files[file] = str(train_dir) + \"\/\" +  str(train_files[file])\nfor file in range(len(test_files)):\n    test_files[file] = str(test_dir) + \"\/\" +  str(test_files[file])\n    \ndf_train = pd.read_csv(\"..\/input\/feedback-prize-2021\/train.csv\")","0c5dacb0":"df_train.head(2)","121ded06":"pd.set_option('display.max_colwidth', 200)\ndf_train[df_train['id']=='0000D23A521A'][['discourse_start', 'discourse_end', 'discourse_text', 'predictionstring',  'discourse_type' ]]","cd81775f":"df_train['discourse_type'] = df_train['discourse_type'].astype('category')\ndf_train['discourse_type_num'] = df_train['discourse_type_num'].astype('category')\ndf_train['discourse_start'] = df_train['discourse_start'].astype(int)\ndf_train['discourse_end'] = df_train['discourse_end'].astype(int)\ndf_train['discourse_id'] = df_train['discourse_id'].astype('category')\ndf_train['id'] = df_train['id'].astype('category')\ndf_train['discourse_words'] = df_train['discourse_text'].apply(lambda x : len(x.split(' ')))\ndf_train['discourse_len'] = df_train['discourse_end'] - df_train['discourse_start']\ndf_train['discourse_text_lower'] = df_train['discourse_text'].str.lower()","5d0f3988":"print(\"Total number of train files = \" , len(train_files))\nprint(\"Total number of test files = \" , len(test_files))","49b44294":"assert len(df_train.id.unique()) == len(train_files)","c392ae93":"ax = sns.histplot(data=df_train, x=\"discourse_type\")\nticks = plt.xticks(rotation=45)","7aeaedde":"ax = sns.histplot(data=df_train, x=\"discourse_type_num\", hue='discourse_type')\nticks = plt.xticks(rotation=90)\nax = sns.barplot(\n    data=df_train, \n    x=\"discourse_type_num\", hue='discourse_type', y=np.ones(len(df_train))\n)\n","48664aa2":"df_count = df_train.groupby('id').size().reset_index(name='counts')\nax = sns.boxplot(x=df_count['counts'])\nlabels = ax.set(xlabel='Number of discourses by essay')\nprint(df_count['counts'].describe())","41d69ff9":"ax = sns.boxplot(data=df_train, y='discourse_words', x='discourse_type', orient='v', showfliers = False)\nlabels = ax.set(xlabel='Number of words by discourse')\nticks = plt.xticks(rotation=45)","1044b1ae":"from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nstop_words = set(stopwords.words('english'))\n","b39b0960":"with open(random.sample(train_files,1)[0], \"r\") as essay:\n    print(essay.read())","35176285":"with open(random.sample(test_files,1)[0], \"r\") as essay:\n    print(essay.read())","c969fea3":"## code from: https:\/\/www.kaggle.com\/odins0n\/feedback-prize-eda\n\nsample_essay = random.sample(train_files,1)[0]\nsample_id = sample_essay.split('\/')[-1][:-4]\n                                        \nents = []\nfor i, row in df_train[df_train['id'] == sample_id].iterrows():\n    ents.append({\n                    'start': int(row['discourse_start']), \n                     'end': int(row['discourse_end']), \n                     'label': row['discourse_type']\n                })\n\nwith open(sample_essay, 'r') as file: \n    data = file.read()\n\ndoc2 = {\n    \"text\": data,\n    \"ents\": ents,\n}\n\ncolors = {'Lead': '#EE11D0','Position': '#AB4DE1','Claim': '#1EDE71','Evidence': '#33FAFA',\n          'Counterclaim': '#4253C1','Concluding Statement': 'yellow','Rebuttal': 'red'}\noptions = {\"ents\": df_train['discourse_type'].unique().tolist(), \"colors\": colors}\nspacy.displacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True);","f0ecd908":"all_text = df_train['discourse_text'].str.cat(sep=' ').translate(str.maketrans('', '', string.punctuation))\nimg_wordcloud = wordcloud.WordCloud(stopwords=wordcloud.STOPWORDS, background_color='black').generate(all_text)\nax = plt.imshow(img_wordcloud, interpolation='bilinear')\nplt.axis(\"off\")","c9904219":"all_test_text = ''\nfor test_file in test_files:\n    with open(test_file) as file:\n        all_test_text += ' ' + file.read().translate(str.maketrans('', '', string.punctuation)).lower()\nimg_wordcloud = wordcloud.WordCloud(stopwords=wordcloud.STOPWORDS, background_color='black').generate(all_test_text)\nax = plt.imshow(img_wordcloud, interpolation='bilinear')\nax = plt.axis(\"off\")","465c45ca":"df_wordfreq = (df_train.set_index('discourse_type')['discourse_text_lower']\n       .str.split(' ', expand=True)\n       .stack()\n       .rename('discourse_type_lower')\n       .reset_index(name='discourse_word'))\n#df_wordfreq.head()\ndiscourse_text_by_type = {}\nfor discourse_type in df_train['discourse_type'].unique():\n    df_temp = df_wordfreq[df_wordfreq['discourse_type']==discourse_type]\n    discourse_text_by_type[discourse_type] = df_temp['discourse_word'].str.cat(\n        sep=' ').translate(str.maketrans('', '', string.punctuation))   ","ba11d500":"for discourse_type, text in discourse_text_by_type.items():\n    text_without_stop = [w for w in word_tokenize(text) if not w in stop_words]\n    cnt = Counter(text_without_stop)\n    print(' --------  Most common words for discourse type ', discourse_type, '---------- \\n')\n    print(cnt.most_common(10), '\\n')","1fd467b2":"from pandarallel import pandarallel\npandarallel.initialize()","39c65107":"nlp = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'lemmatizer', 'textcat'])\n\ndef show_ents(row):\n    ents = []\n    doc = nlp(row)\n    if doc.ents: \n        ents = [ent.text for ent in doc.ents]\n    return ents\n\ndf_train['spacy_entities'] = df_train['discourse_text'].parallel_apply(show_ents)","6b666262":"df_wordfreq = (df_train.set_index('discourse_type').apply(lambda x: pd.Series(x['spacy_entities']),axis=1).stack()\n       .rename('spacy_entities')\n       .reset_index(name='spacy_entities'))\n\ndiscourse_entities_by_type = {}\nfor discourse_type in df_train['discourse_type'].unique():\n    df_temp = df_wordfreq[df_wordfreq['discourse_type']==discourse_type]\n    discourse_entities_by_type[discourse_type] = df_temp['spacy_entities'].to_list()","a7798143":"for discourse_type, text in discourse_entities_by_type.items():\n    cnt = Counter(text)\n    print(' --------  Most common Entities for discourse type ', discourse_type, '---------- \\n')\n    print(cnt.most_common(10), '\\n')","5a731b11":"Check if the number of unique IDs for essays are the same of train files","d2007448":"<b>Let's visualize discourses type on a sample essay text<\/b>\n","d4f04b67":"Now let's vizualize for test data","61271845":"It's possible to check that punctuation and apostrophe are included withing the word spam. For instance `mars.` or `NASAS's` counts only one position in `predictionstring`, so the correct text split looks like simple empty space.","e9c3befe":"## 3. Entity recognition using Spacy","9b2dfc04":"Let's plot the box plot of number of words by `discourse_type`","752d36eb":"## 1. Some statistics from data","ea8a8f19":"The images are a little differents, but we can see some words in both clouds.\n\nNow let's count word frequency for each type of discourse","66f5f290":"Sample test essay","d7960579":"Word Cloud from train data. I removed stop words, punctuation and converted to lower case","76c38d96":"Since Claim and Evidence are most frequent types, it's natural them have more items two.\n\nNow let's plot bloxplot of number of discourses by essay","cb8d33e3":"Plot `discourse_type_num` grouped by `discourse_type`","1c3f7925":"**Important to notice that several labels consists of multiple sentences and not all text on essay has a label**.","9b9d9758":"# Feedback Prize - Yet Another EDA (work in progress)\n\nThis is a EDA for data available by [Feedback Prize](https:\/\/www.kaggle.com\/c\/feedback-prize-2021) competition on Kaggle.","ea6d3723":"He I convert some datatypes and create some counts","4e0fb4b8":"Load CSV files and list of essays (I'll call a train file as an *essay*)","1c6d9fa3":"Let's print the most commont entities by `discourse_type`","31e46071":"## 2. Text Analysis","0fa5f542":"Plot `discourse_type` counts","37699033":"Sample train essay","813fb851":"Analyzing label structure for first paragraph in `0000D23A521A` essay: <\/br><\/br>\n*Some people belive that the so called \"face\" on mars was created by life on mars. This is not the case. The face on Mars is a naturally occuring land form called a mesa. It was not created by aliens, and there is no consiracy to hide alien lifeforms on mars. There is no evidence that NASA has found that even suggests that this face was created by aliens.*"}}