{"cell_type":{"e7651971":"code","7915afc9":"code","59ba9eae":"code","acfce986":"code","8550fc91":"code","d30a9afb":"code","9e2ac546":"code","193e0d6f":"code","10238a13":"code","16b9ca08":"code","36308cd1":"code","88433734":"code","73657ce2":"code","45522f50":"code","59092ddd":"code","5a824bd1":"markdown","30147629":"markdown","7436893f":"markdown","6e25649e":"markdown","e1ab7ac0":"markdown","d77097d0":"markdown","246a5589":"markdown","8fc949b0":"markdown"},"source":{"e7651971":"!pip install efficientnet\nimport efficientnet","7915afc9":"import json\nimport math\nimport os\n\nimport cv2\nimport tensorflow as tf\nfrom PIL import Image\nimport numpy as np\nimport keras\nfrom keras import layers\nfrom keras.applications import MobileNetV2\nfrom keras.callbacks import Callback, ModelCheckpoint\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Model, load_model\nfrom keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization, concatenate, Input, add\nfrom keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score\nimport scipy\nfrom tqdm import tqdm","59ba9eae":"!mkdir ..\/tmp\n!unzip -q ..\/input\/recursion-2019-load-resize-and-save-images\/train.zip -d ..\/tmp\n!unzip -q ..\/input\/recursion-2019-load-resize-and-save-images\/test.zip -d ..\/tmp ","acfce986":"# load_path = '..\/tmp\/train'\n# save_path = '..\/tmp\/train_resized'\n# if not os.path.exists(save_path):\n#     os.makedirs(save_path)\n\n# for code in tqdm(os.listdir(load_path)):\n#     path = f'{load_path}\/{code}'\n    \n#     img = cv2.imread(path)\n#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n#     img = cv2.resize(img, (300, 300))\n    \n#     cv2.imwrite(f'{save_path}\/{code}.png', img)","8550fc91":"# load_path = '..\/tmp\/test'\n# save_path = '..\/tmp\/test_resized'\n# if not os.path.exists(save_path):\n#     os.makedirs(save_path)\n\n# for code in tqdm(os.listdir(load_path)):\n#     path = f'{load_path}\/{code}'\n    \n#     img = cv2.imread(path)\n#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n#     img = cv2.resize(img, (300, 300))\n    \n#     cv2.imwrite(f'{save_path}\/{code}.png', img)","d30a9afb":"train_df = pd.read_csv('..\/input\/recursion-cellular-image-classification\/train.csv')\ntest_df = pd.read_csv('..\/input\/recursion-cellular-image-classification\/test.csv')\n\ntrain_df['category'] = train_df['experiment'].apply(lambda x: x.split('-')[0])\ntest_df['category'] = test_df['experiment'].apply(lambda x: x.split('-')[0])\n\ntrain_target_df = pd.get_dummies(train_df['sirna'])\n\nprint(train_df.shape)\nprint(test_df.shape)\nprint(train_target_df.shape)\n\ntrain_df.head()","9e2ac546":"train_idx, val_idx = train_test_split(\n    train_df.index, test_size=0.15, random_state=2019\n)\n\nprint(train_idx.shape)\nprint(val_idx.shape)","193e0d6f":"class DataGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, list_IDs, df, target_df=None, mode='fit',\n                 base_path = '..\/tmp\/train',\n                 batch_size=32, dim=(224, 224), n_channels=3, ext='jpeg',\n                 rotation_range=0, fill_mode='nearest', swap=False,\n                 vertical_flip=False, horizontal_flip=False, rescale=1\/255.,\n                 n_classes=5, random_state=2019, shuffle=True):\n        self.dim = dim\n        self.batch_size = batch_size\n        self.df = df\n        self.mode = mode\n        self.base_path = base_path\n        self.rotation_range=rotation_range\n        self.target_df = target_df\n        self.list_IDs = list_IDs\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.shuffle = shuffle\n        self.ext = ext\n        self.rescale = rescale\n        self.vertical_flip = vertical_flip\n        self.horizontal_flip = horizontal_flip\n        self.random_state = random_state\n        self.swap = swap\n        \n        self.fill_mode = self.__compute_fill_mode(fill_mode)\n        \n        np.random.seed(self.random_state)\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) \/ self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_batch = [self.list_IDs[k] for k in indexes]\n        \n        X = self.__generate_X(list_IDs_batch)\n        \n        if self.mode == 'fit':\n            y = self.__generate_y(list_IDs_batch)\n            return X, y\n        \n        elif self.mode == 'predict':\n            return X\n        else:\n            raise AttributeError('The parameter mode should be set to \"fit\" or \"predict\".')\n        \n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n    \n    def __generate_X(self, list_IDs_batch):\n        'Generates data containing batch_size samples'\n        # Initialization\n        X_1 = np.empty((self.batch_size, *self.dim, self.n_channels))\n        X_2 = np.empty((self.batch_size, *self.dim, self.n_channels))\n        \n        # Generate data\n        for i, ID in enumerate(list_IDs_batch):\n            code = self.df['id_code'].iloc[ID]\n            \n            img_path_1 = f\"{self.base_path}\/{code}_s1.{self.ext}\"\n            img_path_2 = f\"{self.base_path}\/{code}_s2.{self.ext}\"\n            \n            img1 = self.__load_image(img_path_1)\n            img2 = self.__load_image(img_path_2)\n            \n            if self.swap and np.random.rand() > 0.5:\n                img1, img2 = img2, img1\n            \n            # Store samples\n            X_1[i,] = img1\n            X_2[i,] = img2\n\n        return [X_1, X_2]\n    \n    def __generate_y(self, list_IDs_batch):\n        y = np.empty((self.batch_size, self.n_classes), dtype=int)\n        \n        for i, ID in enumerate(list_IDs_batch):\n            sirna = self.target_df.iloc[ID]\n            y[i, ] = sirna\n        \n        return y\n    \n    def __load_image(self, img_path):\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n        img = self.rescale * img.astype(np.float32)\n\n        return img\n    \n    def __compute_fill_mode(self, fill_mode):\n        convert_cv2 = {\n            'nearest': cv2.BORDER_REPLICATE,\n            'reflect': cv2.BORDER_REFLECT,\n            'wrap': cv2.BORDER_WRAP,\n            'constant': cv2.BORDER_CONSTANT\n        }\n        \n        return convert_cv2[fill_mode]\n    \n    def __random_transform(img):\n        if np.random.rand() > 0.5 and self.vertical_flip:\n            img = cv2.flip(img, 0)\n        if np.random.rand() > 0.5 and self.horizontal_flip:\n            img = cv2.flip(img, 1)\n        \n        # Random Rotation\n        rotation = self.rotation_range * np.random.rand()\n        \n        rows,cols = img.shape[:2]\n        M = cv2.getRotationMatrix2D((cols\/2,rows\/2),rotation,1)\n        img = cv2.warpAffine(img,M,(cols,rows), borderMode=self.fill_mode)\n        \n        return img","10238a13":"BATCH_SIZE = 8\ntrain_generator = DataGenerator(\n    train_idx, \n    df=train_df,\n    target_df=train_target_df,\n    batch_size=BATCH_SIZE, \n    vertical_flip=True,\n    horizontal_flip=True,\n    swap=True,\n    dim=(400, 400),\n    base_path='..\/tmp\/train',\n    rotation_range=15,\n    n_classes=train_target_df.shape[1]\n)\n\nval_generator = DataGenerator(\n    val_idx, \n    df=train_df,\n    target_df=train_target_df,\n    batch_size=BATCH_SIZE, \n    vertical_flip=True,\n    horizontal_flip=True,\n    swap=True,\n    dim=(400, 400),\n    base_path='..\/tmp\/train',\n    rotation_range=15,\n    n_classes=train_target_df.shape[1]\n)\n\ntest_generator = DataGenerator(\n    test_df.index, \n    df=test_df,\n    batch_size=1, \n    shuffle=False,\n    mode='predict',\n    n_classes=train_target_df.shape[1],\n    dim=(400, 400),\n    base_path='..\/tmp\/test'\n)","16b9ca08":"def build_model(n_classes, input_shape=(224, 224, 3)):\n    # First load mobilenet\n    backbone = efficientnet.EfficientNetB3(\n        weights='imagenet', \n        include_top=False,\n        input_shape=input_shape\n    )\n    \n    im_inp_1 = Input(shape=input_shape)\n    im_inp_2 = Input(shape=input_shape)\n\n    x1 = backbone(im_inp_1)\n    x2 = backbone(im_inp_2)\n\n    x1 = GlobalAveragePooling2D()(x1)\n    x2 = GlobalAveragePooling2D()(x2)\n\n    out = add([x1, x2])\n    out = Dropout(0.5)(out)\n\n    out = Dense(n_classes, activation='softmax')(out)\n\n    model = Model(inputs=[im_inp_1, im_inp_2], outputs=out)\n    \n    model.compile(Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","36308cd1":"model = build_model(\n    input_shape=(400, 400, 3),\n    n_classes=train_target_df.shape[1]\n)\nmodel.summary()","88433734":"checkpoint = ModelCheckpoint(\n    'model.h5', \n    monitor='val_loss', \n    verbose=0, \n    save_best_only=True, \n    save_weights_only=False,\n    mode='auto'\n)\n\nhistory = model.fit_generator(\n    train_generator,\n    validation_data=val_generator,\n    callbacks=[checkpoint],\n    use_multiprocessing=False,\n    workers=1,\n    verbose=1,\n    epochs=12\n)","73657ce2":"with open('history.json', 'w') as f:\n    json.dump(history.history, f)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df[['loss', 'val_loss']].plot()\nhistory_df[['acc', 'val_acc']].plot()","45522f50":"categories = train_df['category'].unique()\noutput_df = []\n\nfor category in categories:\n    # Retrieve desired category\n    category_df = train_df[train_df['category'] == category]\n    cat_test_df = test_df[test_df['category'] == category].copy()\n    \n    print('\\n' + '=' * 40)\n    print(\"CURRENT CATEGORY:\", category)\n    print('-' * 40)\n    \n    train_idx, val_idx = train_test_split(\n        category_df.index, \n        random_state=2019,\n        test_size=0.15\n    )\n    \n    # Create new generators\n    train_generator = DataGenerator(\n        train_idx, \n        df=train_df,\n        target_df=train_target_df,\n        batch_size=BATCH_SIZE, \n        vertical_flip=True,\n        horizontal_flip=True,\n        swap=True,\n        rotation_range=15,\n        dim=(400, 400),\n        base_path='..\/tmp\/train_resized',\n        n_classes=train_target_df.shape[1]\n    )\n\n    val_generator = DataGenerator(\n        val_idx, \n        df=train_df,\n        target_df=train_target_df,\n        batch_size=BATCH_SIZE, \n        vertical_flip=True,\n        horizontal_flip=True,\n        swap=True,\n        rotation_range=15,\n        dim=(400, 400),\n        base_path='..\/tmp\/train_resized',\n        n_classes=train_target_df.shape[1]\n    )\n\n    test_generator = DataGenerator(\n        cat_test_df.index, \n        df=test_df,\n        batch_size=1, \n        shuffle=False,\n        mode='predict',\n        n_classes=train_target_df.shape[1],\n        dim=(400, 400),\n        base_path='..\/tmp\/test_resized'\n    )\n\n    # Restore previously trained model\n    model.load_weights('model.h5')\n    model.compile(\n        Adam(0.001), \n        loss='categorical_crossentropy', \n        metrics=['accuracy']\n    )\n\n    # Train model only on data for specific category\n    checkpoint = ModelCheckpoint(\n        f'model_{category}.h5', \n        monitor='val_loss', \n        verbose=0, \n        save_best_only=True, \n        save_weights_only=False,\n        mode='auto'\n    )\n\n    history_category = model.fit_generator(\n        train_generator,\n        validation_data=val_generator,\n        callbacks=[checkpoint],\n        use_multiprocessing=False,\n        workers=1,\n        verbose=2,\n        epochs=8\n    )\n\n    # Make prediction and add to output dataframe\n    y_pred = model.predict_generator(\n        test_generator,\n        workers=2,\n        use_multiprocessing=True,\n        verbose=1\n    )\n\n    cat_test_df['sirna'] = y_pred.argmax(axis=1)\n    output_df.append(cat_test_df[['id_code', 'sirna']])\n\n    # Save history\n    with open(f'history_{category}.json', 'w') as f:\n        json.dump(history_category.history, f)","59092ddd":"output_df = pd.concat(output_df)\noutput_df.to_csv('submission.csv', index=False)","5a824bd1":"# Submission","30147629":"# About this kernel\n\nThis is a rather quick and dirty kernel I created, with two ideas in mind: Training a \"2-headed\" network that will learn to predict siRNA using images from both sites at the same time, and split the learning process into two stages, namely first training on all data, then training the CNN on data from a single experiment at a time. The second idea comes from [this thread by Phalanx](https:\/\/www.kaggle.com\/c\/recursion-cellular-image-classification\/discussion\/100414#latest-586901). The data comes from my previous kernel on preprocessing.\n\nHere are the relevant sections:\n* **Data Generator**: The `__generate_X` method is pretty different, since it loads two images at the same time. Everything else is standard\n* **Model**: The CNN architecture used here is `EfficientNetB2`. With the right learning rates and enough time, you can probably try B1-B5; they have unfortunately not succeeded in my case. The inputs are two images, i.e. from site 1 and site 2. The two images are passed through the same CNN, then global-average-pooled, and added to form a single 1280-dimensional vector, which is ultimately used to perform predictions. This means that the networks will be updated simultaneously from the gradients of both sites.\n* **Phase 1**: Train the model on all data from 10 epochs, and save results to `model.h5`.\n* **Phase 2**: Load `model.h5` and train the model for 15 epochs on data from a single cell line, i.e. *HEPG2, HUVEC, RPE, U2OS*.\n\n## Changelog\n\n* V20: Added random flipping.","7436893f":"# Unzip Files","6e25649e":"# Phase 2: train on each cell line","e1ab7ac0":"# Preprocessing","d77097d0":"# Data Generator","246a5589":"# Phase 1: Train on all data","8fc949b0":"# Model"}}