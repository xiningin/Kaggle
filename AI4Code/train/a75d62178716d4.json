{"cell_type":{"c9021d7e":"code","140484c1":"code","a3f5f0a9":"code","7d2d1049":"code","c65b300e":"code","5a4e5358":"code","a92f3d44":"code","edeea10d":"code","0f174515":"code","caa497d4":"code","1dbe30f7":"code","5598e35e":"code","544a3785":"code","35f90b1e":"code","4f7a093e":"code","3d918c38":"code","ebe490f1":"code","29263054":"code","7db7aa1f":"code","be7be439":"code","a4b1cd3f":"code","68ddba9e":"code","f8c2bdde":"code","54f81534":"code","87d77c15":"code","6b79913e":"code","fb2fec8b":"code","848c4574":"code","2be3c0ac":"code","540b7d3b":"code","bfee537b":"code","65b10efc":"markdown","82788833":"markdown","42da24ff":"markdown","7a528dbc":"markdown","31e5315c":"markdown","7a059ca8":"markdown","5d735d98":"markdown","93f1cbcf":"markdown","b62e65fe":"markdown"},"source":{"c9021d7e":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\ndata = pd.read_csv(\"..\/input\/drug-classification\/drug200.csv\")","140484c1":"data","a3f5f0a9":"data.isna().sum()","7d2d1049":"data.duplicated().sum()","c65b300e":"data[\"Drug\"].replace({\"DrugY\": \"Y\", \"drugX\": \"X\",\"drugA\":\"A\",\"drugC\":\"C\",\"drugB\":\"B\"}, inplace=True)","5a4e5358":"def count_values(dataframe):\n    categorical=dataframe.columns.values.tolist()\n    for col in categorical:\n        print(f\"Value Counts in {col} is: \\n {dataframe[col].value_counts()}\")\n        print(\"\\n\")\ncount_values(data)","a92f3d44":"import matplotlib.pyplot as plt\ndata['Drug'].value_counts().head(5).plot.pie()\n\ndata_feyn=data\nplt.gca().set_aspect('equal')","edeea10d":"def var_distribution2(dataframe):\n    import matplotlib.pyplot as plt\n    numbers = pd.Series(dataframe.columns)\n    dataframe[numbers].hist(figsize=(14,14))\n    plt.show();\n    return dataframe.var()\nvar_distribution2(data)","0f174515":"sns.catplot(x=\"Cholesterol\", kind=\"count\", palette=\"ch:.25\", data=data)","caa497d4":"sns.catplot(x=\"BP\", kind=\"count\", palette=\"ch:.25\", data=data)","1dbe30f7":"sns.catplot(x=\"Sex\", kind=\"count\", palette=\"ch:.25\", data=data)","5598e35e":"data.groupby(\"Drug\").mean()","544a3785":"sns.catplot(x=\"Sex\", y=\"Age\", hue=\"Drug\", kind=\"point\", data=data)\n","35f90b1e":"sns.catplot(x=\"Sex\", y=\"Na_to_K\", hue=\"Drug\", kind=\"point\", data=data)","4f7a093e":"sns.violinplot(x=data.Drug, y=data.Age)","3d918c38":"sns.violinplot(x=data.Drug, y=data.Na_to_K)","ebe490f1":"sns.FacetGrid(data, hue=\"Drug\", height=6) \\\n   .map(sns.histplot, \"Age\") \\\n   .add_legend();\nplt.suptitle(\"Age\",size=28)\nplt.show();","29263054":"sns.FacetGrid(data, hue=\"Drug\", height=6) \\\n   .map(sns.histplot, \"Na_to_K\") \\\n   .add_legend();\nplt.suptitle(\"Na_to_K\",size=28)\nplt.show();","7db7aa1f":"def corr(dataframe,target_variable):\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    fig, ax = plt.subplots(figsize=(15,15))\n    correlation_matrix = dataframe.corr().round(2)\n    sns.heatmap(data=correlation_matrix, annot=True)\n    \n    correlation = data.corr()[target_variable].abs().sort_values(ascending = False)\n    return correlation\ncorr(data,\"Drug\")","be7be439":"def Outliers(dataframe,cols):\n    import plotly.graph_objects as go\n    from plotly.subplots import make_subplots\n    numeric_col2=[]\n    for x in cols:\n        numeric_col2.append(x)\n\n    fig=make_subplots(rows=1, cols=len(cols))\n\n    for i,col in enumerate(numeric_col2):\n        fig.add_trace(go.Box(y=dataframe[col].values, name=dataframe[col].name), row=1, col=i+1)\n\n    return fig.show()\n\ncols=data.columns.values.tolist()\nOutliers(data,cols)","a4b1cd3f":"from sklearn import preprocessing\ndata2=data\nsubset=[\"Age\",\"Na_to_K\",\"Drug\"]\ndata2=data2.drop(subset,axis=1)\ndata2=data2.apply(preprocessing.LabelEncoder().fit_transform)\ndata=data.drop([\"Sex\",\"BP\",\"Cholesterol\"],axis=1)\ndata=pd.concat([data,data2],axis=1)\n\ndata","68ddba9e":"def feature_selector(dataframe,feature_number,target_variable):\n    from sklearn import datasets\n    from sklearn.feature_selection import RFE\n    from sklearn.linear_model import LogisticRegression\n    \n    dataframe2=dataframe[target_variable]\n    dataframe=dataframe.drop([target_variable],axis=1)\n     \n    n=feature_number\n    lr = LogisticRegression(solver=\"liblinear\")\n    rfe=RFE(lr,n)\n    rfe=rfe.fit(dataframe,dataframe2)\n    cols=[]\n    for x in dataframe.columns.values.tolist():\n        cols.append(x)\n    ranking=[]\n    for x in rfe.ranking_:\n        ranking.append(x)\n    n=0\n    for x in rfe.support_:\n        print(f\"{ranking[n]}: {x}----> {cols[n]}\")\n        n+=1\n    selected=[]\n    n=0\n    z=zip(dataframe.columns.values.tolist(),rfe.support_)\n    z=list(z)\n    for x in range(len(z)+1):\n    \n        try:\n            if str(z[n][1])==\"True\":\n                selected.append(z[n])\n            else:\n                pass\n        except IndexError:\n            pass\n        n+=1\n    cols_selected=[]   \n    \n    for x,y in selected:\n        cols_selected.append(x)\n    \n    if len(cols_selected)==feature_number:\n        return  cols_selected\n    else:\n        print(\"ERROR!. Cols_Selected does not meet feature_number requirements\")\n\n        \nfeature_selector(data,3,\"Drug\")\n","f8c2bdde":"def VIF(dataframe,chosen_cols):\n    from statsmodels.stats.outliers_influence import variance_inflation_factor as VIF\n    from statsmodels.tools.tools import add_constant\n    X=dataframe[chosen_cols]\n    X=add_constant(X)\n    vif_data=pd.DataFrame()\n    vif_data[\"feature\"]=X.columns\n    vif_data[\"VIF\"]=[VIF(X.values, i) for i in range(len(X.columns))]\n    return vif_data","54f81534":"chosen_cols=['Na_to_K', 'BP', 'Cholesterol']\nVIF(data,chosen_cols)","87d77c15":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nimport pandas as pd\nfrom sklearn import metrics\nimport numpy as np\nX=data[chosen_cols]\nY=data[\"Drug\"]\nX_train,X_test,Y_train,Y_test= train_test_split(X,Y,test_size=0.2,random_state=0)\nlm=linear_model.LogisticRegression(max_iter=1000)\nlm.fit(X_train,Y_train)\npredict=lm.predict(X_test)\nprobs=lm.predict_proba(X_test)\nprob=probs[:,1]\nprob_df=pd.DataFrame(prob)\n    \n\ntreshold = 0.25\n\n    \nprint(f\"Prediction Accuracy (Test): {metrics.accuracy_score(Y_test,predict)}\")\nprint()\nprob_df[\"prediction\"]= np.where(prob_df[0]> treshold,1,0)\nprint(prob_df)\nprint(f\"\\nTreshold: {treshold}\")\nprint()\ncon_tab=pd.crosstab(prob_df[\"prediction\"],columns=\"Count\")\nprint(f\"Number of Positive Cases: {con_tab.values[1]\/len(prob_df)*100}%\")\ncon_tab\n    ","6b79913e":"def sklearn_decision_tree_clasiffier(dataframe,chosen_cols,target_variable,max_depth):\n    from sklearn.tree import DecisionTreeClassifier\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    from sklearn import metrics\n    X=dataframe[chosen_cols]\n    Y=dataframe[target_variable]\n    X_train,X_test,Y_train,Y_test= train_test_split(X,Y,test_size=0.2,random_state=0)\n    \n    tree=DecisionTreeClassifier(criterion=\"entropy\",min_samples_split=int((len(dataframe)\/16)),max_depth=max_depth)\n    tree.fit(X_train,Y_train)\n    predict=tree.predict(X_test)\n    predict2=tree.predict(X_train)\n    print(f\"Prediction Accuracy (Train): {metrics.accuracy_score(Y_train,predict2)}\")\n    print(f\"Prediction Accuracy (Test): {metrics.accuracy_score(Y_test,predict)}\")\n    ","fb2fec8b":"sklearn_decision_tree_clasiffier(data,chosen_cols,\"Drug\",5)","848c4574":"def sklearn_Random_Forest_Clasiffier(dataframe,chosen_cols,target_variable):\n    from sklearn.ensemble import RandomForestClassifier\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    from sklearn import metrics\n  \n    X=dataframe[chosen_cols]\n    Y=dataframe[target_variable]\n    X_train,X_test,Y_train,Y_test= train_test_split(X,Y,test_size=0.2,random_state=0)\n    forest=RandomForestClassifier(n_jobs=2,n_estimators=10)\n    forest.fit(X_train,Y_train)\n    predict=forest.predict(X_test)\n    predict2=forest.predict(X_train)\n    print(f\"Prediction Accuracy (Train): {metrics.accuracy_score(Y_train,predict2)}\")\n    print(f\"Prediction Accuracy (Test): {metrics.accuracy_score(Y_test,predict)}\")","2be3c0ac":"sklearn_Random_Forest_Clasiffier(data,chosen_cols,\"Drug\")","540b7d3b":"def Sklearn_KNMC(dataframe,chosen_cols,target_variable):\n    from sklearn.neighbors import KNeighborsClassifier\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    from sklearn import metrics\n  \n    X=dataframe[chosen_cols]\n    Y=dataframe[target_variable]\n    X_train,X_test,Y_train,Y_test= train_test_split(X,Y,test_size=0.2,random_state=0)\n    KNNC=KNeighborsClassifier()\n    KNNC.fit(X_train,Y_train)\n    predict=KNNC.predict(X_test)\n    predict2=KNNC.predict(X_train)\n    print(f\"Prediction Accuracy (Train): {metrics.accuracy_score(Y_train,predict2)}\")\n    print(f\"Prediction Accuracy (Test): {metrics.accuracy_score(Y_test,predict)}\")","bfee537b":"Sklearn_KNMC(data,chosen_cols,\"Drug\")","65b10efc":"# EDA","82788833":"**LOGISTIC REGRESSION**","42da24ff":"**DECISION TREE**","7a528dbc":"# Predicting","31e5315c":"**KNN**","7a059ca8":"# Data Preprocessing","5d735d98":"**RANDOM FOREST**","93f1cbcf":"# Data Cleaning","b62e65fe":"**Our Best Model Is Log Regression and Random Forest With 95% Acc**"}}