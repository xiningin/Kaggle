{"cell_type":{"c07e4fab":"code","cf7ccff2":"code","be2ba892":"code","5e3766db":"code","4c7b758e":"code","7acaeb5a":"code","fa040b3d":"code","d5ab70c4":"code","821d7550":"code","13d80b3d":"code","ef4553a7":"code","950307a6":"code","ce4e6eae":"code","b3af539c":"code","2829f51b":"code","79897448":"code","333357f5":"markdown","2ccf5aa2":"markdown","381a1fbd":"markdown","272cd89c":"markdown","5a971053":"markdown","329531d4":"markdown","199b3b6d":"markdown","100d1cc8":"markdown","3ccb339a":"markdown","3a1d55bf":"markdown","bb7e0866":"markdown","5f3c54c5":"markdown","947120a2":"markdown","99ffbce6":"markdown","b170bba7":"markdown","94952044":"markdown","42136119":"markdown"},"source":{"c07e4fab":"import numpy as np\nimport matplotlib.pyplot as plt\nlbs = [0.706] +  [0.707]*6 + [0.708]*5 + [0.709]*7 + [0.710]*8 + [0.711]*4 + [0.712]*8 + [0.713]*2 + [0.714]*2 + [0.715]*2\npp = plt.hist(lbs, bins=10)\nplt.title(\"LB Scores\")\nplt.show()","cf7ccff2":"jac = [0.708]*5 + [0.707]*9 + [0.706]*6 + [0.705]*12 + [0.704]*8 + [0.703]*2 + [0.702] + [0.701]*2\npp2 = plt.hist(jac, bins=8)\nplt.title(\"Estimated (Rounded) Mean Jaccard Scores\")\nplt.show()","be2ba892":"print('Number of submissions above: ' + str(len(lbs)))","5e3766db":"!pip install -U tensorflow==2.2.0","4c7b758e":"import pandas as pd, numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nimport math\nprint('TF version',tf.__version__)","7acaeb5a":"# Temporarily disabled\n# import tensorflow.compat.v1.logging as tf_logging\n# tf_logging.set_verbosity(tf_logging.INFO)\n\nimport os\n\nK.clear_session()\ntry:\n    TPU_WORKER = os.environ[\"TPU_NAME\"]\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(f\"Running on TPU: {tpu.cluster_spec().as_dict()['worker']}\")\n    print(f\"TPU_WORKER: {TPU_WORKER}\")\nexcept ValueError: \n    tpu = None\n    gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n    \nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelif len(gpus) > 1: # multiple GPUs on the VM\n    strategy = tf.distribute.MirroredStrategy(gpus)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nREPLICAS_OR_WORKERS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS_OR_WORKERS}')\n\nAUTO = tf.data.experimental.AUTOTUNE","fa040b3d":"MAX_LEN = 96\nPATH = '..\/input\/tf-roberta\/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\nEPOCHS = 3 # originally 3\nBATCH_SIZE = 16 * REPLICAS_OR_WORKERS # originally 32\nPAD_ID = 1\nSEED = 88888\nLABEL_SMOOTHING = 0.1\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\ntrain = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv').fillna('')\ntrain.head()","d5ab70c4":"ct = train.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(train.shape[0]):\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n    text2 = \" \".join(train.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[train.loc[k,'sentiment']]\n    input_ids[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n    attention_mask[k,:len(enc.ids)+3] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+2] = 1\n        end_tokens[k,toks[-1]+2] = 1","821d7550":"test = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv').fillna('')\n\nct = test.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test.shape[0]):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n    attention_mask_t[k,:len(enc.ids)+3] = 1","13d80b3d":"import pickle\nfrom functools import lru_cache\n\ndef save_weights(model, dst_fn):\n    weights = model.get_weights()\n    with open(dst_fn, 'wb') as f:\n        pickle.dump(weights, f)\n\n\ndef load_weights(model, weight_fn):\n    with open(weight_fn, 'rb') as f:\n        weights = pickle.load(f)\n    model.set_weights(weights)\n    return model\n\ndef loss_fn(y_true, y_pred):\n    # adjust the targets for sequence bucketing\n    ll = tf.shape(y_pred)[1]\n    y_true = y_true[:, :ll]\n    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n    loss = tf.reduce_mean(loss)\n    return loss\n\n# @lru_cache(maxsize=None)\ndef build_model():\n    print(\"Started to build model...\")\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n\n    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n    max_len = tf.reduce_max(lens)\n    ids_ = ids[:, :max_len]\n    att_ = att[:, :max_len]\n    tok_ = tok[:, :max_len]\n\n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids_,attention_mask=att_,token_type_ids=tok_)\n\n    x1 = tf.keras.layers.Dropout(0.1)(x[0])\n    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n\n    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) \n    model.compile(loss=loss_fn, optimizer=optimizer)\n\n    # this is required as `model.predict` needs a fixed size!\n    x1_padded = tf.pad(x1, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n    x2_padded = tf.pad(x2, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n\n    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded,x2_padded])\n    print(\"Finished building model.\")\n    return model, padded_model","ef4553a7":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","950307a6":"%%time\njac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\noof_start = np.zeros((input_ids.shape[0],MAX_LEN))\noof_end = np.zeros((input_ids.shape[0],MAX_LEN))\npreds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n\nskf = StratifiedKFold(n_splits=5,shuffle=True,random_state=SEED) #originally 5 splits\n\nfor fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n\n    print('#'*25)\n    print('### FOLD %i'%(fold+1))\n    print('#'*25)\n    \n    K.clear_session()\n    with strategy.scope():\n        model, padded_model = build_model()\n        \n    #sv = tf.keras.callbacks.ModelCheckpoint(\n    #    '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n    #    save_weights_only=True, mode='auto', save_freq='epoch')\n    inpT = [input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]]\n    targetT = [start_tokens[idxT,], end_tokens[idxT,]]\n    inpV = [input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]]\n    targetV = [start_tokens[idxV,], end_tokens[idxV,]]\n    # sort the validation data\n    shuffleV = np.int32(sorted(range(len(inpV[0])), key=lambda k: (inpV[0][k] == PAD_ID).sum(), reverse=True))\n    inpV = [arr[shuffleV] for arr in inpV]\n    targetV = [arr[shuffleV] for arr in targetV]\n    weight_fn = '%s-roberta-%i.h5'%(VER,fold)\n    for epoch in range(1, EPOCHS + 1):\n        print('### Epoch %i'%(epoch))\n        # sort and shuffle: We add random numbers to not have the same order in each epoch\n        shuffleT = np.int32(sorted(range(len(inpT[0])), key=lambda k: (inpT[0][k] == PAD_ID).sum() + np.random.randint(-3, 3), reverse=True))\n        # shuffle in batches, otherwise short batches will always come in the beginning of each epoch\n        num_batches = math.ceil(len(shuffleT) \/ BATCH_SIZE)\n        batch_inds = np.random.permutation(num_batches)\n        shuffleT_ = []\n        for batch_ind in batch_inds:\n            shuffleT_.append(shuffleT[batch_ind * BATCH_SIZE: (batch_ind + 1) * BATCH_SIZE])\n        shuffleT = np.concatenate(shuffleT_)\n        # reorder the input data\n        inpT = [arr[shuffleT] for arr in inpT]\n        targetT = [arr[shuffleT] for arr in targetT]\n        model.fit(inpT, targetT, \n            epochs=epoch, initial_epoch=epoch - 1, batch_size=BATCH_SIZE, verbose=DISPLAY, callbacks=[],\n            validation_data=(inpV, targetV), shuffle=False)  # don't shuffle in `fit`\n        save_weights(model, weight_fn)\n\n    print('Loading model...')\n    # model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n    load_weights(model, weight_fn)\n\n    print('Predicting OOF...')\n    oof_start[idxV,],oof_end[idxV,] = padded_model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n    \n    print('Predicting Test...')\n    preds = padded_model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]\/skf.n_splits\n    preds_end += preds[1]\/skf.n_splits\n    \n    # DISPLAY FOLD JACCARD\n    all = []\n    for k in idxV:\n        a = np.argmax(oof_start[k,])\n        b = np.argmax(oof_end[k,])\n        if a>b: \n            st = train.loc[k,'text'] # IMPROVE CV\/LB with better choice here\n        else:\n            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-2:b-1])\n        all.append(jaccard(st,train.loc[k,'selected_text']))\n    jac.append(np.mean(all))\n    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n    print()","ce4e6eae":"print('>>>> OVERALL 5Fold CV Jaccard =',np.mean(jac))","b3af539c":"print(jac) # Jaccard CVs","2829f51b":"all = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = test.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-2:b-1])\n    all.append(st)","79897448":"test['selected_text'] = all\ntest[['textID','selected_text']].to_csv('submission.csv',index=False)\npd.set_option('max_colwidth', 60)\ntest.sample(25)","333357f5":"# Acknowledgements\n\n- Built-upon See--'s notebook at https:\/\/www.kaggle.com\/seesee\/faster-2x-tf-roberta\n- Many thanks to Chris Deotte for his TF roBERTa dataset at https:\/\/www.kaggle.com\/cdeotte\/tf-roberta\n\n### The remaining majority of the notebook is from See--'s notebook above","2ccf5aa2":"# Training Data\nWe will now convert the training data into arrays that roBERTa understands. Here are example inputs and targets: \n![ids.jpg](attachment:ids.jpg)\nThe tokenization logic below is inspired by Abhishek's PyTorch notebook [here][1].\n\n[1]: https:\/\/www.kaggle.com\/abhishek\/roberta-inference-5-folds","381a1fbd":"And their corresponding estimated (rounded) mean 5-fold Jaccard scores are:","272cd89c":"### Make keras models work with TPU (TF < version 2.0)\nhttps:\/\/www.dlology.com\/blog\/how-to-train-keras-model-x20-times-faster-with-tpu-for-free\/\n\n### Make keras models work with TPU (TF >= version 2.0)\nhttps:\/\/stackoverflow.com\/questions\/55541881\/how-to-convert-tf-keras-model-to-tpu-using-tensorflow-2-0-in-google-colab","5a971053":"# TensorFlow roBERTa with CNN Head\nThis notebook is a TensorFlow template for solving Kaggle's Tweet Sentiment Extraction competition as a question and answer roBERTa formulation. In this notebook, we show how to tokenize the data, create question answer targets, and how to build a custom question answer head for roBERTa in TensorFlow. Note that HuggingFace transformers don't have a `TFRobertaForQuestionAnswering` so we must make our own from `TFRobertaModel`. This notebook can achieve LB 0.715 with some modifications. Have fun experimenting!\n\nYou can also run this code offline and it will save the best model weights during each of the 5 folds of training. Upload those weights to a private Kaggle dataset and attach to this notebook. Then you can run this notebook with the line `model.fit()` commented out, and this notebook will instead load your offline models. It will use your offline models to predict oof and predict test. Hence this notebook can easily be converted to an inference notebook. An inference notebook is advantageous because it will only take 10 minutes to commit and submit instead of 2 hours. Better to train 2 hours offline separately.","329531d4":"# Additional Findings & Suggestions\n\n## The \"Magical\" Seed\nI've tried various seeds, around 25-30 of them. There are about 5 seeds that gave me either consistently high LB scores or very wide LB ranges, for example 0.708 to 0.715. The seed used in this note book so far gave ranges between 0.709 and 0.715, and is not the \"best\" seed I've found, which consistently gave LB scores between 0.712 and 0.715 for this notebook. Feel free to try it out but honestly, you may be better off working on your model instead.\n\n## Suggestions?\nAccording to Ambitious at https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/142404#809872, he states that \"Actual LB score should be a little lower because you use early stopping on OOF folds. In order to truly replicate LB scores you should split train data into train and dev sets, train your models with cross-validation on train set and then evaluate on a dev set. Especially interesting would be to estimate influence of using different CV splits.\"\n\nTo-date, I've tried various splits and I found 10 splits to be the configuration that gave the most consistent CV and LB scores. For example, resubmission and rescorings of See-'s notebook (with some modifications) gave LB 0.710 9 out of 10 times. You may want to explore different splits, but if you lack the time to do so, perhaps sticking to 10 splits once you have a model with good Jaccard CVs will be best.","199b3b6d":"# Train roBERTa Model\nWe train with 5 Stratified KFolds (based on sentiment stratification). Each fold, the best model weights are saved and then reloaded before oof prediction and test prediction. Therefore you can run this code offline and upload your 5 fold models to a private Kaggle dataset. Then run this notebook and comment out the line `model.fit()`. Instead your notebook will load your model weights from offline training in the line `model.load_weights()`. Update this to have the correct path. Also make sure you change the KFold seed below to match your offline training. Then this notebook will proceed to use your offline models to predict oof and predict test.","100d1cc8":"## Setting up TPU for Keras model creation\/fitting","3ccb339a":"# Kaggle Submission","3a1d55bf":"https:\/\/colab.research.google.com\/github\/GoogleCloudPlatform\/training-data-analyst\/blob\/master\/courses\/fast-and-lean-data-science\/07_Keras_Flowers_TPU_xception_fine_tuned_best.ipynb#scrollTo=hleIN5-pcr0N","bb7e0866":"# Load Libraries, Data, Tokenizer\nWe will use HuggingFace transformers [here][1]\n\n[1]: https:\/\/huggingface.co\/transformers\/","5f3c54c5":"# Overview\n\nAwhile ago, I chanced upon Chris Deotte's analysis at https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/142404#809872. Found it interesting and since I had GPU to spare while training my models offline, I tried to replicate the process with Think deep.'s work at https:\/\/www.kaggle.com\/seesee\/faster-2x-tf-roberta. After modifying the CNN head a bit (don't ask me why, its part of my experiments), and attempting to replicate Chris' findings, I arrived at the below results in the historgram plots. In summary, the changes are:\n- Modified CNN layers\n- Varied seeds and performed repeated submissions and commits for same seeds","947120a2":"# Findings","99ffbce6":"# Metric","b170bba7":"## That's all for now, all the best and happy Kaggling!","94952044":"# Build roBERTa Model\nWe use a pretrained roBERTa base model and add a custom question answer head. First tokens are input into `bert_model` and we use BERT's first output, i.e. `x[0]` below. These are embeddings of all input tokens and have shape `(batch_size, MAX_LEN, 768)`. Next we apply `tf.keras.layers.Conv1D(filters=1, kernel_size=1)` and transform the embeddings into shape `(batch_size, MAX_LEN, 1)`. We then flatten this and apply `softmax`, so our final output from `x1` has shape `(batch_size, MAX_LEN)`. These are one hot encodings of the start tokens indicies (for `selected_text`). And `x2` are the end tokens indicies.\n\n![bert.jpg](attachment:bert.jpg)","42136119":"# Test Data\nWe must tokenize the test data exactly the same as we tokenize the training data"}}