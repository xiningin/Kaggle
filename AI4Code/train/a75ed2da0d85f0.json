{"cell_type":{"07937b8a":"code","aa17357c":"code","5f6faafb":"code","b38350b8":"code","a78e7077":"code","9679b208":"code","3470c1f8":"code","643f4e60":"code","b8d7b69f":"code","325c9934":"code","999b6184":"code","b5fe01b5":"code","ea67e1b4":"code","dee64f79":"code","ebec185e":"code","3f79e7fe":"code","5f0d8035":"code","bef52e3f":"code","08e7dc2c":"code","66ac0fb7":"code","3405dee9":"code","d7d24c33":"code","fcc22f92":"code","da9ae9e5":"code","25da7a5c":"code","7bbb3f4c":"code","5849afad":"code","90fe37af":"code","4166ff6b":"code","8a4f3608":"code","db59ca9a":"code","8183a9b0":"code","bdf3616a":"code","2e808986":"code","72d7187d":"code","8456eeef":"code","5b4097c7":"code","f67b1d62":"code","b0a40c8b":"code","e98b53c2":"code","d147939c":"code","30f3d3a9":"code","8dc61ff0":"code","7f365670":"code","b6428186":"code","77e85dcc":"code","a193b032":"code","6f1b428e":"code","47d9818d":"code","6ccfe6e0":"code","0667ecad":"code","71e27df1":"code","47711669":"code","8ddcec16":"code","eb10d08c":"code","ca7d0309":"code","8eeff3c1":"code","519f5851":"code","e6233daa":"code","8bb15b9c":"code","66f475c3":"code","e3c95082":"code","c83eab8a":"code","90770a46":"code","eb4a6f88":"code","de3d37ae":"code","01b5e705":"code","aa04f99e":"code","15d06a1d":"code","4f75a435":"code","107e10a5":"code","bfa9d780":"code","5e8428b6":"code","4121109e":"code","30b5c63d":"code","1f487de4":"code","d90da3e1":"code","37afe191":"code","bb8890bf":"code","88f5e32b":"markdown","e7575f3e":"markdown","57587c33":"markdown","0db32268":"markdown","9f543bcf":"markdown","6ba4dc0c":"markdown","ea883bb0":"markdown","aa4d8965":"markdown","d3709d51":"markdown","9ee158d0":"markdown","40f3a300":"markdown","982e3f24":"markdown","fe2b7775":"markdown","8b83b6c7":"markdown","558a721b":"markdown","93b415ab":"markdown","3104dec9":"markdown","fe29140c":"markdown","840adeab":"markdown","9edba346":"markdown","5fcc46d3":"markdown","c534c864":"markdown","18a83b6d":"markdown","4c665da6":"markdown","bad10255":"markdown","8ef45839":"markdown","e0896849":"markdown","1b1d61ab":"markdown","6afb3d15":"markdown","5617cfb0":"markdown","02dbd9d7":"markdown","ce0617a8":"markdown","ac5fb9fb":"markdown","429ee869":"markdown","f561d0f3":"markdown","693962f4":"markdown","eb70ffdc":"markdown","a58c51a3":"markdown","167db135":"markdown","4065d51e":"markdown","2919104a":"markdown","52b38afd":"markdown","14206f35":"markdown","054bcee3":"markdown","f4784000":"markdown","e3e27cc6":"markdown","5a140df7":"markdown","de488945":"markdown","ceb18ca9":"markdown","6438aee0":"markdown","ef236d40":"markdown","a13d974d":"markdown","b4f7aef4":"markdown","d0673921":"markdown","4264b55a":"markdown","87dcb0ed":"markdown","d66705b0":"markdown","be7a6478":"markdown","6252ee8f":"markdown","31b7bd5d":"markdown","1b7b38ae":"markdown","39b8dc26":"markdown"},"source":{"07937b8a":"import numpy as np \nimport pandas as pd \nimport os\nimport itertools\n\n#plots\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.colors import n_colors\nfrom plotly.subplots import make_subplots\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nfrom PIL import Image\nfrom nltk.corpus import stopwords\nstop=set(stopwords.words('english'))\nfrom nltk.util import ngrams\n\n\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nimport re\nfrom collections import Counter\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport nltk\nfrom nltk.corpus import stopwords\n\nimport requests\nimport json","aa17357c":"covid=pd.read_csv('\/kaggle\/input\/covid19-tweets\/covid19_tweets.csv')\n\nsentiment=pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv',encoding = 'latin',header=None,names=['target','id',\n                                                                                                                             'time','query','usr','text'])\n# Useful for code matching with countries - Plotly Chlorepeth MAP\ncountry_code=pd.read_csv('https:\/\/raw.githubusercontent.com\/plotly\/datasets\/master\/2014_world_gdp_with_codes.csv') \n\n# Below files are used for data cleaning - Retrieving countries with help of city codes\/state codes...(optional)\nstate = json.loads(requests.get(\"https:\/\/raw.githubusercontent.com\/praneshsaminathan\/country-state-city\/master\/states.json\").text)\ncountry=json.loads(requests.get(\"https:\/\/raw.githubusercontent.com\/praneshsaminathan\/country-state-city\/master\/countries.json\").text)\ncity=json.loads(requests.get(\"https:\/\/raw.githubusercontent.com\/praneshsaminathan\/country-state-city\/master\/cities.json\").text)\nus_state_code=pd.read_csv('https:\/\/worldpopulationreview.com\/static\/states\/abbr-name.csv',names=['state_code','state'])\n\n\n# All above mentioned data are functioned and retrieved in below file to get valid country name- without_country_name\n#without_country_name=pd.read_csv('..\/input\/country-tweet\/without_country_name.csv',low_memory=False)\nwithout_country_name = pd.DataFrame([state,country,city])\n\ncovid.head(3)","5f6faafb":"print('Total tweets in this data: {}'.format(covid.shape[0]))\nprint('Total Unique Users in this data: {}'.format(covid['user_name'].nunique()) )","b38350b8":"covid['country_name']=covid['user_location'].str.split(',').str[-1]\ncovid['only_date']=pd.to_datetime(covid['date']).dt.date\n\n#Keeping countries with valid country name\n\nwith_country_name=covid[covid['country_name'].isin(list(country_code['COUNTRY']))]\nwith_country_name['filtered_name']=covid['country_name']\n\n#Without valid country name is programmed below\n","a78e7077":"# country=pd.DataFrame(country['countries'])\n# states =pd.DataFrame(state['states'])\n# city=pd.DataFrame(city['cities'])\n\n# all_world=pd.merge(city,states,left_on=\"state_id\",right_on=\"id\",how=\"left\")\n# all_world=pd.merge(all_world,country,left_on=\"country_id\",right_on=\"id\",how=\"left\")\n# all_world=pd.merge(all_world,us_state_code,left_on=\"name_y\",right_on=\"state\",how=\"left\")\n\n# temp_df=all_world[['name_x','name']].rename(columns={'name_x':'place'})\n# temp_df=temp_df.append(all_world[['name_y','name']].rename(columns={'name_y':'place'}))\n# temp_df=temp_df.append(all_world[['sortname','name']].rename(columns={'sortname':'place'}))\n# temp_df=temp_df.append(all_world[['state_code','name']].rename(columns={'state_code':'place'}))\n# temp_df=temp_df.drop_duplicates()\n# temp_df.shape\n\n\n# dict_count=dict({'USA':'United States','UK':'United Kingdom','\u092d\u093e\u0930\u0924':'India','British Columbia':'Canada','Deutschland':'Germany','Jammu And Kashmir':'India',\n#                  'ON':'Canada','DC':'United States','UAE':'United Arab Emirates','hyderabad':'India','New York City':'United States','Everywhere':'United States',\n#                  'Republic of the Philippines':'Philippines','Africa':'South Africa','WORLDWIDE':'United States','Washington DC':'United States','mumbai':'India',\n#                  'INDIA':'India','Worldwide':'United States','North America':'United States','Washington DC & Virginia':'United States','PRC':'China',\n#                  'San Francisco Bay Area':'United States','America':'United States','BC':'United Kingdom','BENGALURU':'India','#AFRICA #MENA':'South Africa',\n#                  'online':'India','Qu\u00e9bec':'Canada','Earth':'United States','Canberra':'Australia','Europe':'Canada','World':'India','Northern Ireland':'United Kingdom',\n#                  'Sun Valley Idaho':'United States','Hong Kong':'China','Sydney':'Australia','NYC':'United States','New South Wales':'Australia','D.C.':'United States','The Netherlands':'Netherlands','Global':'United States','Planet Earth':'India','Bangalore':'India','U.S.':'United States',\n#                  'CANADA':'Canada','Nig':'Nigeria','Western Australia':'Australia','The seventh house':'United States','Ngovhela Mahunguni':'Russia','Kashmir':'India','Etats-Unis':'United States','Mumbai | Kolkata':'India','VadaChennai':'India','SoCal':'United States','Sverige':'Sweden','Victoria BC':'Canada','Kingdom of Saudi Arabia':'Saudi Arabia','worldwide':'United States','Koramangala':'India','East Legon':'Ghana','india':'India','Silicon Valley':'United States','BHARAT':'India','Melbourne but I tour worldwide':'Australia','Remote':'United States','New Delhi.':'India','Cape Town':'South Africa','Nigeria.':'Nigeria','Netherlands':'Netherlands','Kamloops':'Canada','EU Citizen':'Canada','SF Bay Area':'United States','South Florida':'United States','Nova Scotia':'Canada','AB':'Canada','City of London':'United Kingdom','NOIDA':'India','NEW DELHI':'India','Lancashire and Europe':'United Kingdom','Washington D.C.':'United States','Middle East':'Saudi Arabia','Quezon City':'Philippines','@CapricornFMNews':'Russia','South Australia':'Australia','India.':'India','International':'United States','Kashmir & Ladakh':'India','WorldWide':'India','Ca':'united states','Montr\u00c3\u00a9al':'Canada','Asia':'India','CHINA':'China','World Wide':'India','Northern California':'United States','uk':'United Kingdom','Kuala Lumpur':'Singapore','Global Citizen':'United States','Johannesburg South Africa':'South Africa','J&K':'India','Australia \u00f0\u0178\u2021\u00a6\u00f0\u0178\u2021\u00ba':'Australia','Abuja':'Nigeria','Makati City':'Philippines','Detroit-Northville-St. Heights':'United States','South Africa- Gauteng':'South Africa','Southern California':'United States','Espa\u00c3\u00b1a':'Spain',\n#                  'California USA \u00f0\u0178\u2021\u00ba\u00f0\u0178\u2021\u00b8':'United States','United States of America':'United States','West of Minsk':'Belarus',\n#                  'Ontario Canada':'Canada','Greater Vancouver':'Canada','Chicago\/Washington D.C.':'United States','California USA \ud83c\uddfa\ud83c\uddf8':'United States','U.S.A.':'United States','Macau S.A.R.':'China','Montr\u00c3\u00a9al':'Spain','Espa\u00c3\u00b1a':'Spain','Montserrat':'United Kingdom','California USA \u00f0\u0178\u2021\u00ba\u00f0\u0178\u2021\u00b8':'United States','T\u00c3\u00bcrkiye':'Turkey','united states':'United States','Australia \u00f0\u0178\u2021\u00a6\u00f0\u0178\u2021\u00ba':'Australia','Islamabad':'Pakistan',\n#                 'Netherlands The':'Netherlands','Australia \ud83c\udde6\ud83c\uddfa':'Australia' ,'Montr\u00e9al':'Canada','Espa\u00f1a':'Spain','T\u00fcrkiye':'T\u00fcrkey','East of England':'United Kingdom','NY USA':'United States','Waikato New Zealand':'New Zealand','Mexico City':'Mexico','West Yorkshire':'United Kingdom','NIGERIA':'Nigeria','London UK':'United Kingdom','Ngunnawal Country Aka Canberra':'Australia','Blackburn with Darwen':'United Kingdom','JHB':'South Africa','New England':'United Kingdom','UK.':'United Kingdom','Odisha(India)':'India','london':'United Kingdom','B.C.':'United Kingdom','Mysore and BERLIN':'India','Appalachia':'United States','Philly':'United States','criminal australia':'Australia','EU':'Spain','New York Metropolitan Area':'United States','U.K.':'United States','Islamic Republic of Iran':'Iran','Yorkshire and The Humber':'United Kingdom',\n#                  'Northwest Indiana':'Indiana','Kenya.':'Kenya','Nairobi Kenya':'Kenya','Abu Dhabi':'Saudi Arabia'})\n\n# #dict_count.keys()\n# dict_country= pd.DataFrame(dict_count.items(), columns=['mislabel', 'correct_label'])\n# # dict_country['mislabel']=dict_count.keys()\n# # dict_country['correct_label']=dict_count.values()\n\n\n# def manual_fix(cnt):\n#     if cnt in list(dict_country['mislabel']):\n#         return dict_country[dict_country['mislabel']==cnt]['correct_label'].to_string().split(\"  \")[-1]  \n#     else:\n#         return cnt\n\n\n# def get_country(x):\n#     if type(x) is str:\n#         x = x.replace('\\D+', '')\n#         print(x)\n#         if(len(temp_df[temp_df['place']==x.strip()]['name'])>0):\n#             return temp_df[temp_df['place']==x.strip()]['name'][:1].to_string().split('  ')[-1]\n#         else:\n#             return manual_fix(x.strip())\n#     else:\n#         return x\n\n# without_country_name['filtered_name']=covid['country_name'].apply(lambda x:get_country(x))\n# without_country_name.to_csv('without_country_name.csv',index=False)","9679b208":"tweet_df=with_country_name.append(without_country_name)\ntweet_state_count=tweet_df['filtered_name'].value_counts().to_frame().reset_index().rename(columns={'index':'country','filtered_name':'count'})\nall_tweet_location=pd.merge(tweet_state_count,country_code[['COUNTRY','CODE']],left_on=\"country\",right_on=\"COUNTRY\",how=\"left\")\nall_tweet_location=all_tweet_location[all_tweet_location['COUNTRY'].notnull()]\nall_tweet_location[['COUNTRY','count']].head(5)","3470c1f8":"fig = go.Figure(go.Bar(\n    x=all_tweet_location['COUNTRY'][:10],y=all_tweet_location['count'][:10],\n    marker={'color': all_tweet_location['count'][:10], \n    'colorscale': 'blues'},  \n    text=all_tweet_location['count'][:10],\n    textposition = \"outside\",\n))\nfig.update_layout(title_text='Top Countries with most tweets',xaxis_title=\"Countries\",\n                  yaxis_title=\"Number of Tweets \",template=\"plotly_dark\",height=700,title_x=0.5)\n\nfig.show()","643f4e60":"fig = go.Figure(data=go.Choropleth(\n    locations = all_tweet_location['CODE'],\n    z = all_tweet_location['count'],\n    text = all_tweet_location['COUNTRY'],\n    colorscale = 'rainbow', \n    autocolorscale=False,\n    reversescale=False,\n    marker_line_color='darkgray',\n    marker_line_width=0.5,\n    colorbar_title = '# of Tweets',\n))\n\nfig.update_layout(\n    title_text='Tweets over the world - ({} - {}) '.format(covid['only_date'].sort_values()[0].strftime(\"%d\/%m\/%Y\"),\n                                                       covid['only_date'].sort_values().iloc[-1].strftime(\"%d\/%m\/%Y\")),title_x=0.5,\n    geo=dict(\n        showframe=True,\n        showcoastlines=False,\n        \n        projection_type='equirectangular',\n    )\n)\n\n\nfig.show()","b8d7b69f":"covid['tweet_date']=pd.to_datetime(covid['date']).dt.date\ntweet_date=covid['tweet_date'].value_counts().to_frame().reset_index().rename(columns={'index':'date','tweet_date':'count'})\ntweet_date['date']=pd.to_datetime(tweet_date['date'])\ntweet_date=tweet_date.sort_values('date',ascending=False)\ntweet_date.head(5)","325c9934":"fig=go.Figure(go.Scatter(x=tweet_date['date'],\n                                y=tweet_date['count'],\n                               mode='markers+lines',\n                               name=\"Submissions\",\n                               marker_color='dodgerblue'))\n\nfig.update_layout(\n    title_text='Tweets per Day : ({} - {}) '.format(covid['only_date'].sort_values()[0].strftime(\"%d\/%m\/%Y\"),\n                                                       covid['only_date'].sort_values().iloc[-1].strftime(\"%d\/%m\/%Y\")),template=\"plotly_dark\",\n    title_x=0.5)\n\nfig.show()\n","999b6184":"source_df=covid['source'].value_counts().to_frame().reset_index().rename(columns={'index':'source','source':'count'})[:15]\nsource_df.head(5)","b5fe01b5":"fig = go.Figure(go.Bar(\n    x=source_df['source'],y=source_df['count'],\n    marker={'color': source_df['count'], \n    'colorscale': 'blues'},  \n    text=source_df['count'],\n    textposition = \"outside\",\n))\n\nfig.update_layout(title_text='Top Sources ',xaxis_title=\"Sources\",yaxis_title=\"Count \",\n                  template=\"plotly_dark\",title_x=0.5)\nfig.show()\n","ea67e1b4":"def find_hash(text):\n    line=re.findall(r'(?<=#)\\w+',text)\n    return \" \".join(line)\ncovid['hash']=covid['text'].apply(lambda x:find_hash(x))","dee64f79":"hastags=list(covid[(covid['hash'].notnull())&(covid['hash']!=\"\")]['hash'])\nhastags = [each_string.lower() for each_string in hastags]\nhash_df=dict(Counter(hastags))\ntop_hash_df=pd.DataFrame(list(hash_df.items()),columns = ['word','count']).sort_values('count',ascending=False)[:20]\ntop_hash_df.head(4)","ebec185e":"fig = go.Figure(go.Bar(\n    x=top_hash_df['word'],y=top_hash_df['count'],\n    marker={'color': top_hash_df['count'], \n    'colorscale': 'blues'},  \n    text=top_hash_df['count'],\n    textposition = \"outside\",\n))\nfig.update_layout(title_text='Top Trended Hastags',xaxis_title=\"Hashtags \",\n                  yaxis_title=\"Number of Tags \",template=\"plotly_dark\",height=700,title_x=0.5)\nfig.show()\n","3f79e7fe":"def find_at(text):\n    line=re.findall(r'(?<=@)\\w+',text)\n    return \" \".join(line)\ncovid['mention']=covid['text'].apply(lambda x:find_at(x))\n","5f0d8035":"mentions=list(covid[(covid['mention'].notnull())&(covid['mention']!=\"\")]['mention'])\nmentions = [each_string.lower().split() for each_string in mentions]\nmentions=list(itertools.chain.from_iterable(mentions))\nmention_df=dict(Counter(mentions))\ntop_mention_df=pd.DataFrame(list(mention_df.items()),columns = ['word','count']).sort_values('count',ascending=False)[:20]\ntop_mention_df.head(10)","bef52e3f":"fig = go.Figure(go.Bar(\n    x=top_mention_df['word'],y=top_mention_df['count'],\n    marker={'color': top_mention_df['count'], \n    'colorscale': 'blues'},  \n    text=top_mention_df['count'],\n    textposition = \"outside\",\n))\n\nfig.update_layout(title_text='Top Trended Hastags ',xaxis_title=\"Hashtags\",\n                  yaxis_title=\"Number of Tags \",template=\"plotly_dark\",height=700,title_x=0.5)\nfig.show()\n","08e7dc2c":"def remove_tag(string):\n    text=re.sub('<.*?>','',string)\n    return text\ndef remove_mention(text):\n    line=re.sub(r'@\\w+','',text)\n    return line\ndef remove_hash(text):\n    line=re.sub(r'#\\w+','',text)\n    return line\n\ndef remove_newline(string):\n    text=re.sub('\\n','',string)\n    return text\ndef remove_url(string): \n    text = re.sub('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',string)\n    return text\ndef remove_number(text):\n    line=re.sub(r'[0-9]+','',text)\n    return line\ndef remove_punct(text):\n    line = re.sub(r'[!\"\\$%&\\'()*+,\\-.\\\/:;=#@?\\[\\\\\\]^_`{|}~]*','',text)\n    #string=\"\".join(line)\n    return line\ndef text_strip(string):\n    line=re.sub('\\s{2,}', ' ', string.strip())\n    return line\ndef remove_thi_amp_ha_words(string):\n    line=re.sub(r'\\bamp\\b|\\bthi\\b|\\bha\\b',' ',string)\n    return line","66ac0fb7":"covid['refine_text']=covid['text'].str.lower()\ncovid['refine_text']=covid['refine_text'].apply(lambda x:remove_tag(str(x)))\ncovid['refine_text']=covid['refine_text'].apply(lambda x:remove_mention(str(x)))\ncovid['refine_text']=covid['refine_text'].apply(lambda x:remove_hash(str(x)))\ncovid['refine_text']=covid['refine_text'].apply(lambda x:remove_newline(x))\ncovid['refine_text']=covid['refine_text'].apply(lambda x:remove_url(x))\ncovid['refine_text']=covid['refine_text'].apply(lambda x:remove_number(x))\ncovid['refine_text']=covid['refine_text'].apply(lambda x:remove_punct(x))\ncovid['refine_text']=covid['refine_text'].apply(lambda x:remove_thi_amp_ha_words(x))\ncovid['refine_text']=covid['refine_text'].apply(lambda x:text_strip(x))\n\ncovid['text_length']=covid['refine_text'].str.split().map(lambda x: len(x))\n","3405dee9":"fig = go.Figure(data=go.Violin(y=covid['text_length'], box_visible=True, line_color='black',\n                               meanline_visible=True, fillcolor='royalblue ', opacity=0.6,\n                               x0='Tweet Text Length '))\n\nfig.update_layout(yaxis_zeroline=False,title=\"Distribution of Text length \",template='ggplot2')\nfig.show()","d7d24c33":"fig, (ax2) = plt.subplots(1,1,figsize=[17, 10])\nwordcloud2 = WordCloud(background_color='black',colormap=\"Blues\", \n                        width=600,height=400).generate(\" \".join(covid['refine_text']))\n\nax2.imshow(wordcloud2,interpolation='bilinear')\nax2.axis('off')\nax2.set_title('Most Used Words in Comments ',fontsize=35)\n","fcc22f92":"def ngram_df(corpus,nrange,n=None):\n    vec = CountVectorizer(stop_words = 'english',ngram_range=nrange).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    total_list=words_freq[:n]\n    df=pd.DataFrame(total_list,columns=['text','count'])\n    return df\nunigram_df=ngram_df(covid['refine_text'],(1,1),20)\nbigram_df=ngram_df(covid['refine_text'],(2,2),20)\ntrigram_df=ngram_df(covid['refine_text'],(3,3),20)\n","da9ae9e5":"fig = make_subplots(\n    rows=3, cols=1,subplot_titles=(\"Unigram\",\"Bigram\",'Trigram'),\n    specs=[[{\"type\": \"scatter\"}],\n           [{\"type\": \"scatter\"}],\n           [{\"type\": \"scatter\"}]\n          ])\n\nfig.add_trace(go.Bar(\n    y=unigram_df['text'][::-1],\n    x=unigram_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=unigram_df['count'],\n    textposition = \"outside\",\n    orientation=\"h\",\n    name=\"Months\",\n),row=1,col=1)\n\nfig.add_trace(go.Bar(\n    y=bigram_df['text'][::-1],\n    x=bigram_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=bigram_df['count'],\n     name=\"Days\",\n    textposition = \"outside\",\n    orientation=\"h\",\n),row=2,col=1)\n\nfig.add_trace(go.Bar(\n    y=trigram_df['text'][::-1],\n    x=trigram_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=trigram_df['count'],\n     name=\"Days\",\n    orientation=\"h\",\n    textposition = \"outside\",\n),row=3,col=1)\n\nfig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_layout(title_text='Top N Grams',xaxis_title=\" \",yaxis_title=\" \", showlegend=False,title_x=0.5,\n                  height=1200,template=\"plotly_dark\")\nfig.show()","25da7a5c":"sentiment=sentiment.sample(int(sentiment.shape[0]*0.4))\nsentiment=sentiment[['text','target']]\nsentiment['emotion']=np.where(sentiment['target']==0,'negative',np.where(sentiment['target']==2,'neutral',np.where(sentiment['target']==4,'postitive',\"none\")))\nsentiment['target']=np.where(sentiment['target']==4,1,sentiment['target'])","7bbb3f4c":"# Optimizing text\nsentiment['refine_text']=sentiment['text'].str.lower()\nsentiment['refine_text']=sentiment['refine_text'].apply(lambda x:remove_tag(str(x)))\nsentiment['refine_text']=sentiment['refine_text'].apply(lambda x:remove_mention(str(x)))\nsentiment['refine_text']=sentiment['refine_text'].apply(lambda x:remove_hash(str(x)))\nsentiment['refine_text']=sentiment['refine_text'].apply(lambda x:remove_newline(x))\nsentiment['refine_text']=sentiment['refine_text'].apply(lambda x:remove_url(x))\nsentiment['refine_text']=sentiment['refine_text'].apply(lambda x:remove_number(x))\nsentiment['refine_text']=sentiment['refine_text'].apply(lambda x:remove_punct(x))\nsentiment['refine_text']=sentiment['refine_text'].apply(lambda x:remove_thi_amp_ha_words(x))\nsentiment['refine_text']=sentiment['refine_text'].apply(lambda x:text_strip(x))\nsentiment['text_length']=sentiment['refine_text'].str.split().map(lambda x: len(x))\n\n# Removing stopwords\nstop_words = set(stopwords.words('english'))\nsentiment['refine_text'] =  sentiment['refine_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n","5849afad":"# Setting tha randomly\ntrain_data=sentiment[['refine_text','target']]\n\ntrain_sent=np.array(train_data['refine_text'])\ntrain_label=np.array(train_data['target'])\n\nx_train, x_test, y_train, y_test = train_test_split(train_sent, train_label, test_size=0.20, random_state=42)","90fe37af":"# Hyper Parameters\nvocab_size = 1000\nembedding_dim =16 \nmax_length = 50\ntrunc_type='post'\noov_tok = \"<OOV>\"","4166ff6b":"tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(x_train)\nword_index = tokenizer.word_index\n\ntraining_sequences = tokenizer.texts_to_sequences(x_train)\ntraining_padded = pad_sequences(training_sequences,maxlen=max_length, truncating=trunc_type)\n\ntesting_sequences = tokenizer.texts_to_sequences(x_test)\ntesting_padded = pad_sequences(testing_sequences,maxlen=max_length)\n\ntraining_label=y_train\ntesting_label=y_test","8a4f3608":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","db59ca9a":"num_epochs = 1 # Edit here\nhistory=model.fit(training_padded, training_label, epochs=num_epochs, validation_data=(testing_padded, testing_label))","8183a9b0":"tweet_df['refine_text']=tweet_df['text'].str.lower()\ntweet_df['refine_text']=tweet_df['refine_text'].apply(lambda x:remove_tag(str(x)))\ntweet_df['refine_text']=tweet_df['refine_text'].apply(lambda x:remove_mention(str(x)))\ntweet_df['refine_text']=tweet_df['refine_text'].apply(lambda x:remove_hash(str(x)))\ntweet_df['refine_text']=tweet_df['refine_text'].apply(lambda x:remove_newline(x))\ntweet_df['refine_text']=tweet_df['refine_text'].apply(lambda x:remove_url(x))\ntweet_df['refine_text']=tweet_df['refine_text'].apply(lambda x:remove_number(x))\ntweet_df['refine_text']=tweet_df['refine_text'].apply(lambda x:remove_punct(x))\ntweet_df['refine_text']=tweet_df['refine_text'].apply(lambda x:remove_thi_amp_ha_words(x))\ntweet_df['refine_text']=tweet_df['refine_text'].apply(lambda x:text_strip(x))\ntweet_df['text_length']=tweet_df['refine_text'].str.split().map(lambda x: len(x))","bdf3616a":"sen = np.array(tweet_df.refine_text)\nseq = tokenizer.texts_to_sequences(sen)\npadd = pad_sequences(seq, maxlen=max_length, truncating=trunc_type)\nresult=model.predict(padd)\nvalidated_result=np.where(result>0.5,1,0)","2e808986":"pred_df=pd.DataFrame({'text':tweet_df['refine_text'],'pred_sentiment':list(validated_result),'country':tweet_df['filtered_name'],'text_length':tweet_df['text_length']})\npred_df['pred_sentiment']=np.where(pred_df['pred_sentiment']>0.5,1,0)\npred_df[['text','pred_sentiment']].head(4)","72d7187d":"from PIL import Image\nimport requests\nfrom io import BytesIO\n\nresponse = requests.get('https:\/\/banner2.cleanpng.com\/20180723\/vvy\/kisspng-computer-icons-clip-art-twitter-logo-vector-5b5693f7952128.7797517715324006316109.jpg')\nbird = np.array(Image.open(BytesIO(response.content)))\n\n# d = '..\/input\/twitter\/'\n# bird = np.array(Image.open(d + 'twitter_mask.png'))\nfig, (ax2, ax3) = plt.subplots(1, 2, figsize=[30, 15])\nwordcloud2 = WordCloud( background_color='white',mask=bird,colormap=\"Reds\",\n                        width=600,\n                        height=400).generate(\" \".join(pred_df[pred_df['pred_sentiment']==0]['text']))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Negative Sentiment',fontsize=35);\n\nwordcloud3 = WordCloud( background_color='white',mask=bird,colormap=\"Greens\",\n                        width=600,\n                        height=400).generate(\" \".join(pred_df[pred_df['pred_sentiment']==1]['text']))\nax3.imshow(wordcloud3)\nax3.axis('off')\nax3.set_title('Positive Sentiment',fontsize=35);","8456eeef":"print(\"Average length of Positive Sentiment tweets : {} \".format(round(pred_df[pred_df['pred_sentiment']==1]['text_length'].mean(),2)))\nprint(\"Average length of Negative Sentiment tweets : {} \".format(round(pred_df[pred_df['pred_sentiment']==0]['text_length'].mean(),2)))","5b4097c7":"fig = go.Figure()\n\nfig.add_trace(go.Violin(y=pred_df[pred_df['pred_sentiment']==1]['text_length'], box_visible=False, line_color='black',\n                               meanline_visible=True, fillcolor='green', opacity=0.6,name=\"Positive\",\n                               x0='Positive')\n             )\n\nfig.add_trace(go.Violin(y=pred_df[pred_df['pred_sentiment']==0]['text_length'], box_visible=False, line_color='black',\n                               meanline_visible=True, fillcolor='red', opacity=0.6,name=\"Negative\",\n                               x0='Negative')\n             )\n\nfig.update_traces(box_visible=False, meanline_visible=True)\nfig.update_layout(title_text=\"Violin - Tweet Length \",\n                  title_x=0.5)\n\nfig.show()\n","f67b1d62":"all_pos_country=pred_df[pred_df['pred_sentiment']==1]['country'].value_counts().reset_index().rename(columns={'index':'country','country':'count'})\n\nall_pos_country_df=pd.merge(all_pos_country,country_code[['COUNTRY','CODE']],left_on=\"country\",right_on=\"COUNTRY\",how=\"left\")\nall_pos_country_df=all_pos_country_df[all_pos_country_df['COUNTRY'].notnull()]","b0a40c8b":"fig = go.Figure(data=go.Choropleth(\n    locations = all_pos_country_df['CODE'],\n    z = all_pos_country_df['count'],\n    text = all_pos_country_df['COUNTRY'],\n    colorscale = 'greens', \n    autocolorscale=False,\n    reversescale=False,\n    marker_line_color='darkgray',\n    marker_line_width=0.5,\n    colorbar_title = '# of Tweets ',\n))\n\nfig.update_layout(\n    title_text='Tweets over the world - ({} - {})'.format(covid['only_date'].sort_values()[0].strftime(\"%d\/%m\/%Y\"),\n                                                       covid['only_date'].sort_values().iloc[-1].strftime(\"%d\/%m\/%Y\")),title_x=0.5,\n    \n    geo=dict(\n        showframe=True,\n        showcoastlines=False,\n        projection_type='equirectangular',\n    )\n)\n\nfig.show()\n","e98b53c2":"all_pos_country_df[['country','count']][:5]","d147939c":"all_neg_country=pred_df[pred_df['pred_sentiment']==0]['country'].value_counts().reset_index().rename(columns={'index':'country','country':'count'})\n\nall_neg_country_df=pd.merge(all_neg_country,country_code[['COUNTRY','CODE']],left_on=\"country\",right_on=\"COUNTRY\",how=\"left\")\nall_neg_country_df=all_neg_country_df[all_neg_country_df['COUNTRY'].notnull()]","30f3d3a9":"fig = go.Figure(data=go.Choropleth(\n    locations = all_neg_country_df['CODE'],\n    z = all_neg_country_df['count'],\n    text = all_neg_country_df['COUNTRY'],\n    colorscale = 'reds',  \n    autocolorscale=False,\n    reversescale=False,\n    marker_line_color='darkgray',\n    marker_line_width=0.5,\n    colorbar_title = '# of Tweets',\n))\n\nfig.update_layout(\n    title_text='Tweets over the world - ({} - {}) '.format(covid['only_date'].sort_values()[0].strftime(\"%d\/%m\/%Y\"),\n                                covid['only_date'].sort_values().iloc[-1].strftime(\"%d\/%m\/%Y\")),title_x=0.5,\n    geo=dict(\n        showframe=True,\n        showcoastlines=False,\n        projection_type='equirectangular',\n    )\n)\n\nfig.show()\n","8dc61ff0":"all_neg_country_df[['country','count']][:5]","7f365670":"fig, (ax2, ax3) = plt.subplots(1, 2, figsize=[30, 15])\nwordcloud2 = WordCloud( background_color='white',mask=bird,colormap=\"Reds\",\n                        width=600,\n                        height=400).generate(\" \".join(pred_df[(pred_df['pred_sentiment']==0)&(pred_df['country']=='United States')]['text']))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Negative US Bird',fontsize=35);\n\nwordcloud3 = WordCloud( background_color='white',mask=bird,colormap=\"Greens\",\n                        width=600,\n                        height=400).generate(\" \".join(pred_df[(pred_df['pred_sentiment']==1)&(pred_df['country']=='United States')]['text']))\nax3.imshow(wordcloud3)\nax3.axis('off')\nax3.set_title('Positive US Bird',fontsize=35);","b6428186":"fig, (ax2, ax3) = plt.subplots(1, 2, figsize=[30, 15])\nwordcloud2 = WordCloud( background_color='white',mask=bird,colormap=\"Reds\",\n                        width=600,\n                        height=400).generate(\" \".join(pred_df[(pred_df['pred_sentiment']==0)&(pred_df['country']=='India')]['text']))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Negative India Bird ',fontsize=35);\n\nwordcloud3 = WordCloud( background_color='white',mask=bird,colormap=\"Greens\",\n                        width=600,\n                        height=400).generate(\" \".join(pred_df[(pred_df['pred_sentiment']==1)&(pred_df['country']=='India')]['text']))\nax3.imshow(wordcloud3)\nax3.axis('off')\nax3.set_title('Positive India Bird ',fontsize=35);","77e85dcc":"fig, (ax2, ax3) = plt.subplots(1, 2, figsize=[30, 15])\nwordcloud2 = WordCloud( background_color='white',mask=bird,colormap=\"Reds\",\n                        width=600,\n                        height=400).generate(\" \".join(pred_df[(pred_df['pred_sentiment']==0)&(pred_df['country']=='United Kingdom')]['text']))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Negative UK Bird ',fontsize=35);\n\nwordcloud3 = WordCloud( background_color='white',mask=bird,colormap=\"Greens\",\n                        width=600,\n                        height=400).generate(\" \".join(pred_df[(pred_df['pred_sentiment']==1)&(pred_df['country']=='United Kingdom')]['text']))\nax3.imshow(wordcloud3)\nax3.axis('off')\nax3.set_title('Positive UK Bird ',fontsize=35);","a193b032":"vn_sentiment=pd.read_csv('..\/input\/vietnamese-sentiment\/data - data.csv',usecols=[\"comment\",\"label\"])","6f1b428e":"vn_sentiment.head(10)","47d9818d":"category = vn_sentiment['label'].unique()\ncategory_to_id = {cate: idx for idx, cate in enumerate(category)}\nid_to_category = {idx: cate for idx, cate in enumerate(category)}\nprint(category_to_id)\nprint(id_to_category)","6ccfe6e0":"data_label = vn_sentiment['label']\ndata_label = pd.DataFrame(data_label, columns=['label']).groupby('label').size()\ndata_label.plot.pie(figsize=(15, 15), autopct=\"%.2f%%\", fontsize=12)","0667ecad":"!pip install transformers","71e27df1":"vn_sentiment.isnull().values.any()\n\nvn_sentiment.shape","47711669":"reviews = []\nsentences = list(vn_sentiment['comment'])\nfor sen in sentences:\n    reviews.append(sen)","8ddcec16":"reviews[10]","eb10d08c":"vn_sentiment.label.unique()","ca7d0309":"vn_sentiment.label.head(10)","8eeff3c1":"y = vn_sentiment['label']\n\n# NEU : 0 , POS : 1 , NEG : 2\ny = np.array(list(map(lambda x: 1 if x==\"POS\" else (2 if x == \"NEG\" else 0 ), y)))","519f5851":"print(reviews[10],y[10])","e6233daa":"!pip install bert-for-tf2\n!pip install sentencepiece\nimport tensorflow as tf\n\nimport tensorflow_hub as hub\n\nfrom tensorflow.keras import layers\nimport bert","8bb15b9c":"BertTokenizer = bert.bert_tokenization.FullTokenizer\nbert_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\",\n                            trainable=False)\nvocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\nto_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = BertTokenizer(vocabulary_file, to_lower_case)","66f475c3":"def tokenize_reviews(text_reviews):\n    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_reviews))","e3c95082":"tokenized_reviews = [tokenize_reviews(review) for review in reviews]","c83eab8a":"print(tokenized_reviews[10])","90770a46":"import random \n\nreviews_with_len = [[review, y[i], len(review)]\n                 for i, review in enumerate(tokenized_reviews)]\n# Word Vector - Label - length of sequence.\nprint(reviews_with_len[0])","eb4a6f88":"# Shuffle random dataset\nrandom.shuffle(reviews_with_len)","de3d37ae":"reviews_with_len[0]","01b5e705":"#reviews_with_len.sort(key=lambda x: x[2])\nsorted_reviews_labels = [(review_lab[0], review_lab[1]) for review_lab in reviews_with_len]\nsorted_reviews_labels\nprocessed_dataset = tf.data.Dataset.from_generator(lambda: sorted_reviews_labels, output_types=(tf.int32, tf.int32))","aa04f99e":"BATCH_SIZE = 32 # You can edit here i.e 128\nbatched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))","15d06a1d":"next(iter(batched_dataset))","4f75a435":"import math\n\nTOTAL_BATCHES = math.ceil(len(sorted_reviews_labels) \/ BATCH_SIZE)\nTEST_BATCHES = TOTAL_BATCHES \/\/ 10\nbatched_dataset.shuffle(TOTAL_BATCHES)\ntest_data = batched_dataset.take(TEST_BATCHES)\ntrain_data = batched_dataset.skip(TEST_BATCHES)","107e10a5":"class TEXT_MODEL(tf.keras.Model):\n    \n    def __init__(self,\n                 vocabulary_size,\n                 embedding_dimensions=128,\n                 cnn_filters=50,\n                 dnn_units=512,\n                 model_output_classes=2,\n                 dropout_rate=0.1,\n                 training=False,\n                 name=\"text_model\"):\n        super(TEXT_MODEL, self).__init__(name=name)\n        \n        self.embedding = layers.Embedding(vocabulary_size,\n                                          embedding_dimensions)\n        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n                                        kernel_size=2,\n                                        padding=\"valid\",\n                                        activation=\"relu\")\n        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n                                        kernel_size=3,\n                                        padding=\"valid\",\n                                        activation=\"relu\")\n        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n                                        kernel_size=4,\n                                        padding=\"valid\",\n                                        activation=\"relu\")\n        self.pool = layers.GlobalMaxPool1D()\n        \n        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n        self.dropout = layers.Dropout(rate=dropout_rate)\n        if model_output_classes == 2:\n            self.last_dense = layers.Dense(units=1,\n                                           activation=\"sigmoid\")\n        else:\n            self.last_dense = layers.Dense(units=model_output_classes,\n                                           activation=\"softmax\")\n    \n    def call(self, inputs, training):\n        l = self.embedding(inputs)\n        l_1 = self.cnn_layer1(l) \n        l_1 = self.pool(l_1) \n        l_2 = self.cnn_layer2(l) \n        l_2 = self.pool(l_2)\n        l_3 = self.cnn_layer3(l)\n        l_3 = self.pool(l_3) \n        \n        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n        concatenated = self.dense_1(concatenated)\n        concatenated = self.dropout(concatenated, training)\n        model_output = self.last_dense(concatenated)\n        \n        return model_output","bfa9d780":"VOCAB_LENGTH = len(tokenizer.vocab)\nEMB_DIM = 200\nCNN_FILTERS = 100\nDNN_UNITS = 256\nOUTPUT_CLASSES = 3\n\nDROPOUT_RATE = 0.2\n\nNB_EPOCHS = 5","5e8428b6":"text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n                        embedding_dimensions=EMB_DIM,\n                        cnn_filters=CNN_FILTERS,\n                        dnn_units=DNN_UNITS,\n                        model_output_classes=OUTPUT_CLASSES,\n                        dropout_rate=DROPOUT_RATE)\n","4121109e":"# from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\n# es = EarlyStopping(monitor='val_f1_m', mode='max', verbose=1, patience=5)\n# reduce_lr = ReduceLROnPlateau(monitor='val_f1_m', factor=0.2, patience=8, min_lr=1e7)\n# checkpoint = ModelCheckpoint('best_full.h5', monitor='val_f1_m', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)","30b5c63d":"if OUTPUT_CLASSES == 2:\n    text_model.compile(loss=\"binary_crossentropy\",\n                       optimizer=\"adam\",\n                       metrics=[\"accuracy\"])\nelse:\n    text_model.compile(loss=\"sparse_categorical_crossentropy\",\n                       optimizer=\"adam\",\n                       metrics=[\"sparse_categorical_accuracy\"])","1f487de4":"history = text_model.fit(train_data,validation_data=test_data, epochs=2)","d90da3e1":"plt.figure(figsize=(20, 5))\n\nplt.subplot(121)\n\n# Get training and test loss histories\ntraining_loss = history.history['loss']\ntest_loss = history.history['val_loss']\n\n# Create count of the number of epochs\nepoch_count = range(1, len(training_loss) + 1)\n\n# Visualize loss history\nplt.title('Model loss')\n\nplt.plot(epoch_count, training_loss, 'r--')\nplt.plot(epoch_count, test_loss, 'b-')\nplt.legend(['Training Loss', 'Test Loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\n\n# Pylot iou-score \nplt.subplot(122)\n\n# Get training and test loss histories\ntraining_loss = history.history['sparse_categorical_accuracy']\ntest_loss = history.history['val_sparse_categorical_accuracy']\n\n# Create count of the number of epochs\nepoch_count = range(1, len(training_loss) + 1)\n\n# Visualize loss history\nplt.title('Accuracy')\nplt.plot(epoch_count, training_loss, 'r--')\nplt.plot(epoch_count, test_loss, 'b-')\nplt.legend(['Training Accuracy', 'Val Accuracy'])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\n\nplt.show()","37afe191":"def pad_to_size(vec, size):\n  zeros = [0] * (size - len(vec))\n  vec.extend(zeros)\n  return vec\n\ndef sample_predict(sample_pred_text, pad):\n  encoded_sample_pred_text = tokenize_reviews(sample_pred_text)\n\n  if pad:\n    encoded_sample_pred_text = pad_to_size(encoded_sample_pred_text, 64)\n  encoded_sample_pred_text = tf.cast(encoded_sample_pred_text, tf.float32)\n  predictions = text_model.predict(tf.expand_dims(encoded_sample_pred_text, 0))\n\n  return (predictions)","bb8890bf":"sample_pred_text = ('B\u1edbt \u0111\u00f9a \u0111i, d\u1ea1o n\u00e0y tao kh\u00f4ng c\u00f2n vui t\u00ednh nh\u01b0 tr\u01b0\u1edbc \u0111\u00e2u.')\n\npredictions = sample_predict(sample_pred_text, pad=True)\nprint(predictions)","88f5e32b":"Once the data is shuffled, we will sort the data by the length of the reviews. To do so, we will use the sort() function of the list and will tell it that we want to sort the list with respect to the third item in the sublist i.e. the length of the review.","e7575f3e":"**US,India,UK,Canada & Australia** have produced more tweets.","57587c33":"### Tuning Parametes (Change below parameters\/add or remove layers to get more accurate result)","0db32268":"### Build model, you need to re-build end-to end or using pretrained model as Bert, GPT-2","9f543bcf":"<a id=\"10\"><\/a>\n\n<font size=\"+2\" color=\"indigo\"><b>10. N-Gram<\/b><\/font><br>","6ba4dc0c":"What device\/source did people use to tweet?","ea883bb0":"Below we will observe the post prediction visualizations.It may differ slightly for every change in models.","aa4d8965":"<font size=+2 color=\"indigo\"><center><b>COVID19 Tweets - Sentiment & Geographical Analysis<\/b><\/center><\/font>","d3709d51":"<a id=\"6\"><\/a>\n\n<font size=\"+2\" color=\"indigo\"><b>6. Hashtag<\/b><\/font><br>","9ee158d0":"**In covid-19-nlp-text-classification we have 6 classes, but our model only predict two classes. So all you will re-build model for 6 classes and predict this dataset again.**","40f3a300":"Now we will see the most tweeted countries positive and negative emotional words. US,India & UK are the most involved countries in twitter tweeting on CORONA.  ","982e3f24":"**new cases,pandemic,due to** are most used ones","fe2b7775":"Read data","8b83b6c7":"### I used bert pretrained model to tokenize, you need use phobert to have a good performance","558a721b":"### Predict","93b415ab":"<a id=\"14\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>14. Sentiment length<\/b><\/font><br>","3104dec9":"<a id=\"9\"><\/a>\n\n<font size=\"+2\" color=\"indigo\"><b>9. Most Used Words<\/b><\/font><br>","fe29140c":"<a id=\"8\"><\/a> \n<font size=\"+2\" color=\"indigo\"><b>8. Average Length<\/b><\/font><br>","840adeab":"![](https:\/\/pbs.twimg.com\/profile_images\/1308010958862905345\/-SGZioPb_400x400.jpg)","9edba346":"* **new,people,pandemic** are most used single word.\n* **cases death,tested postive and active cases** are most used bigrams.\n* **cases new deaths,help slow spread,slow spread indentity** are most used trigrams\n\nAll observed words are biased towards covid detection and admission of patients.People tend to worry lot about other peoples around.","5fcc46d3":"<a id=\"12\"><\/a>\n\n<font size=\"+2\" color=\"indigo\"><b>12. Prediction<\/b><\/font><br>","c534c864":"<a id=\"18\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>18. Vietnamese Sentiment Analyst<\/b><\/font><br>","18a83b6d":"The main data  for this analysis and sentiment prediction is based on [Gabriel dataset](https:\/\/www.kaggle.com\/gpreda\/covid19-tweets)\n\n[Kazanova dataset](https:\/\/www.kaggle.com\/kazanova\/sentiment140) with 1.6 milllion tweets will be used for modeling.This dataset has target feature - sentiment as 0 & 4 (0 - Negative , 4 - Positive) which will be relabeled as (0 & 1)\n\nWe wil be scaling positive and negative emotions in our covid tweet data through predictions.\n\n**Postive emotions** - Hope,Pride,Interest,Joy,Satisfaction,Happy etc <br>\n**Negative emotions** - Fear,Anger,Discust,Sadness,Rude etc","4c665da6":"<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Table of content<\/h3>\n\n* [1. Library](#1)\n* [2. Data](#2)\n* [3. Location](#3)\n* [4. Tweet Series](#4)\n* [5. Tweet Source](#5)\n* [6. Hashtag](#6)\n* [7. Mentions](#7)\n* [8. Average Length](#8)  \n* [9. Most Used Words](#9)\n* [10. N-Gram](#10)\n* [11. Modeling](#11)\n* [12. Prediction](#12)\n* [13. Angry Bird vs Happy Bird](#13)\n* [14. Sentiment length](#14)\n* [15. World Emotions](#15)\n* [16. Top Countries - Emotional Bird](#16)\n* [17. Expand sentiment classes](#17)\n* [18. Vietnamese sentiment analysis](#18)","bad10255":"Average length of tweets lies around **14** and median lies around **15**.And interquartiles lies between **11 & 18**.There is not much significant difference between mean and median which displays that people are more biased towrds this particular length.","8ef45839":"We will find most frequent used words on Negative tweet & Positive tweet.","e0896849":"Import libaries","1b1d61ab":"<a id=\"15\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>15. World Emotions<\/b><\/font><br>","6afb3d15":"Let us look at heatmap of positive and negative sentiment of each countries in the world.","5617cfb0":"Import necessary libraries - Numpy,Ploltly,Sklearn,NLTK,Tensorflow","02dbd9d7":"This is quite expected as there are more **Web** users,we are observing high count on it.Next comes the **Android & Iphone** users which stands next to web apps","ce0617a8":"<a id=\"7\"><\/a>\n\n<font size=\"+2\" color=\"indigo\"><b>7. Mentions<\/b><\/font><br>","ac5fb9fb":"Let us find most mentioned user or organization in covid tweets","429ee869":"Average words for positive is around 13 whereas for negative is around 15.People tend to type long when they are in negative mode.","f561d0f3":"How many tweets were posted everyday?","693962f4":"<a id=\"4\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>4. Tweet Series<\/b><\/font><br>","eb70ffdc":"<a id=\"5\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>5. Tweet Sources<\/b><\/font><br>","a58c51a3":"We observed that most tweeted countries are **US,India and UK**.As a result of them,here we could see **Trump,Biden,Boris,Modi (Top Leaders)** are tagged.Apart from them,**WHO** has paid attention to whole word.youtube is in the game too.","167db135":"In here we use transformer model to tokenize word","4065d51e":"What hashtags has been viral\/most used in covid tweets?","2919104a":"Listing below the top N-gram sequential words used in covid tweets","52b38afd":"It took a few minutes","14206f35":"### Negative Sentiment counts","054bcee3":"<a id=\"13\"><\/a>\n\n<font size=\"+2\" color=\"indigo\"><b>13. Angry Bird vs Happy Bird<\/b><\/font><br>","f4784000":"Finding the most used words from entire population of tweets","e3e27cc6":"Hello readers, I am going to perform analysis on peoples tweets about COVID throughout the world.I hope i will make you realize about sentiments and emotions spread out by people in terms of tweets in twitter.This kernel will be a mixture of geographical positions and sentiment analysis of tweets.Along with EDA,sentiment analysis, deep learning model is performed to predict people's real tone of voice.After modelling,i will showcase some of post prediction visulaizations which would give more insights and results.\n\n**Kernel Main Agenda:**\n\n*  Data\n*  EDA\n*  Geographical tweets\n*  Modeling & Predictions - Sentiment Analysis\n*  Post Prediction Visualization\n","5a140df7":"We have almost hedged our model accuracy with validation accuracy.The model achieved around **75%** accuracy which is not too good or too bad to measure.Still we can tune hyper parameters and produce more good results.","de488945":"In here, we have two options: bert and gpt pretrained model to tokenize word.","ceb18ca9":"<a id=\"17\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>17. Homework<\/b><\/font><br>","6438aee0":"<a id=\"11\"><\/a>\n\n<font size=\"+2\" color=\"indigo\"><b>11. Modeling<\/b><\/font><br>","ef236d40":"<a id=\"3\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>3. Location<\/b><\/font><br>","a13d974d":"### Positive Sentiment counts","b4f7aef4":"<a id=\"1\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>1. Library<\/b><\/font><br>","d0673921":"I will be using 40% of the data to train.I have optimized the text by removing noisy and unwanted characters present in tweet.","4264b55a":"Comparitive to positive sentiments,negatives are low in numbers.This is good insight which displays people are not hatred always instead handling this pandemic with good positive gesture","87dcb0ed":"# Introduction","d66705b0":"Most tweeted countries on COVID matters. \n\n(Given that the location fields is  a mixture and noisy.I have done some data cleaning and fitted data into 'without_country_name.csv')","be7a6478":"<a id=\"2\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>2. Data<\/b><\/font><br>","6252ee8f":"More tweets were made on july 25 (weekend)","31b7bd5d":"What is the average length for a covid tweet?","1b7b38ae":"<a id=\"16\"><\/a>\n<font size=\"+2\" color=\"indigo\"><b>16. Top Countries - Emotional Bird<\/b><\/font><br>","39b8dc26":"### Visualization \nYou can put a loss visualization in here"}}