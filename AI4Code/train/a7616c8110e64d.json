{"cell_type":{"3188973d":"code","b514ffb3":"code","adde9804":"code","2c620982":"code","b28ddd84":"code","ef6677e6":"code","b19149f7":"code","8233cc60":"code","eb30fff3":"code","6d457dda":"code","480e84f0":"code","07687485":"code","762ec299":"code","7abad3d7":"code","c8cb0249":"code","4877bbdf":"code","1fdbce1e":"code","af855828":"code","fd106f80":"code","2b7ca645":"code","d21f5f1d":"code","b7cab4df":"code","712ebfeb":"code","28980225":"code","b0a38ac8":"code","09ef96e2":"code","98526525":"code","7ec0731c":"code","223a3672":"code","5051074c":"code","425ae574":"code","d2b84f3a":"code","57752aae":"code","d97fea55":"code","ac4e20a8":"code","8d580ab3":"code","eb1852bf":"code","74eeff75":"code","3af5f9a9":"code","e544c2fb":"code","5e5c18a9":"code","958adeed":"code","53866409":"code","f6b69f90":"code","91159125":"code","d946dc26":"code","2819777b":"markdown","39afd0a1":"markdown","da6b2bb6":"markdown","6544a996":"markdown","9724d78e":"markdown","fa138615":"markdown","64768ea2":"markdown","bc4ec1db":"markdown","380476fe":"markdown","c6097ffd":"markdown"},"source":{"3188973d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n                            # Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b514ffb3":"pip install lifetimes==0.10.1","adde9804":"pip install openpyxl","2c620982":"pip install pymysql","b28ddd84":"import lifetimes\nfrom lifetimes import BetaGeoFitter\nfrom lifetimes import GammaGammaFitter","ef6677e6":"import datetime as dt\nimport pandas as pd\nimport pymysql\nfrom sqlalchemy import create_engine\nfrom sklearn.preprocessing import MinMaxScaler\nfrom mlxtend.frequent_patterns import apriori, association_rules","b19149f7":"df_ = pd.read_excel('..\/input\/online-retail-2010-full\/online_retail_II_2nd _page.xlsx')","8233cc60":"df_.shape[0]","eb30fff3":"df_.shape[1]","6d457dda":"df = df_.copy()","480e84f0":"df.head()","07687485":"# First of all, total price (MONETARY) which is one of RFM metrics should be obtained.\n# Due to missing values after processes undefined values can occur, these are should be deleted\n# According to data information, some invoice values have 'C'. This means this transactions were canceled. These are should be deleted\n# Lastly, quantity (FREQUENCY), one of the RFM metric, can't be less than zero.\n\n## Following function will ensure to get done items above.\n\n\n\ndef crm_data_prep(dataframe):\n    dataframe[\"TotalPrice\"] = dataframe[\"Quantity\"] * dataframe[\"Price\"]\n    dataframe.dropna(axis=0, inplace=True)\n    dataframe = dataframe[~dataframe[\"Invoice\"].str.contains(\"C\", na=False)]\n    dataframe = dataframe[dataframe[\"Quantity\"] > 0]\n    dataframe = dataframe[dataframe[\"TotalPrice\"] > 0]\n\n    return dataframe","762ec299":"df1 = crm_data_prep(df)\ndf1.head()","7abad3d7":"df1.head()","c8cb0249":"def create_cltv_p(dataframe):\n    today_date = dt.datetime(2011, 12, 11)\n\n    rfm = dataframe.groupby('Customer ID').agg({'InvoiceDate': [lambda date: (date.max()-date.min()).days,\n                                                                lambda date: (today_date - date.min()).days],\n                                                'Invoice': lambda num: num.nunique(),\n                                                'TotalPrice': lambda TotalPrice: TotalPrice.sum()})\n\n    rfm.columns = rfm.columns.droplevel(0)\n\n    ## recency_cltv_p\n    rfm.columns = ['recency_cltv_p', 'T', 'frequency', 'monetary']\n\n\n\n    ## basitle\u015ftirilmi\u015f monetary_avg\n    rfm[\"monetary\"] = rfm[\"monetary\"] \/ rfm[\"frequency\"]\n\n    rfm.rename(columns={\"monetary\": \"monetary_avg\"}, inplace=True)\n\n\n    ## recency_weekly_cltv_p\n    rfm[\"recency_weekly_cltv_p\"] = rfm[\"recency_cltv_p\"] \/ 7\n    rfm[\"T_weekly\"] = rfm[\"T\"] \/ 7\n\n\n\n    # CONTROL\n    rfm = rfm[rfm[\"monetary_avg\"] > 0]\n\n    rfm = rfm[(rfm['frequency'] > 1)]\n\n    rfm[\"frequency\"] = rfm[\"frequency\"].astype(int)\n\n    # BGNBD\n    bgf = BetaGeoFitter(penalizer_coef=0.01)\n    bgf.fit(rfm['frequency'],\n            rfm['recency_weekly_cltv_p'],\n            rfm['T_weekly'])\n\n    # exp_sales_1_month\n    rfm[\"exp_sales_1_month\"] = bgf.predict(4,\n                                           rfm['frequency'],\n                                           rfm['recency_weekly_cltv_p'],\n                                           rfm['T_weekly'])\n    # exp_sales_3_month\n    rfm[\"exp_sales_3_month\"] = bgf.predict(12,\n                                           rfm['frequency'],\n                                           rfm['recency_weekly_cltv_p'],\n                                           rfm['T_weekly'])\n\n    # expected_average_profit\n    ggf = GammaGammaFitter(penalizer_coef=0.01)\n    ggf.fit(rfm['frequency'], rfm['monetary_avg'])\n    rfm[\"expected_average_profit\"] = ggf.conditional_expected_average_profit(rfm['frequency'],\n                                                                             rfm['monetary_avg'])\n    # 6 ayl\u0131k cltv_p\n    cltv = ggf.customer_lifetime_value(bgf,\n                                       rfm['frequency'],\n                                       rfm['recency_weekly_cltv_p'],\n                                       rfm['T_weekly'],\n                                       rfm['monetary_avg'],\n                                       time=6,\n                                       discount_rate=0.01)\n\n    rfm[\"cltv_p\"] = cltv\n\n    # minmaxscaler\n    scaler = MinMaxScaler(feature_range=(1, 100))\n    scaler.fit(rfm[[\"cltv_p\"]])\n    rfm[\"cltv_p\"] = scaler.transform(rfm[[\"cltv_p\"]])\n\n    # rfm.fillna(0, inplace=True)\n\n    # cltv_p_segment\n    rfm[\"cltv_p_segment\"] = pd.qcut(rfm[\"cltv_p\"].rank(method=\"first\"), 3, labels=[\"C\", \"B\", \"A\"])\n\n    ## recency_cltv_p, recency_weekly_cltv_p\n    rfm = rfm[[\"recency_cltv_p\", \"T\", \"monetary_avg\", \"recency_weekly_cltv_p\", \"T_weekly\",\n               \"exp_sales_1_month\", \"exp_sales_3_month\", \"expected_average_profit\",\n               \"cltv_p\", \"cltv_p_segment\"]]\n\n\n    return rfm\n\n","4877bbdf":"# Calculated CLTV for each customer\n\ncltv_p = create_cltv_p(df1)\ncltv_p.head()","1fdbce1e":"#  CustomerID of those belonging to segments A, B and C.\n\na_segment_ids = cltv_p[cltv_p[\"cltv_p_segment\"] == \"A\"].index\nb_segment_ids = cltv_p[cltv_p[\"cltv_p_segment\"] == \"B\"].index\nc_segment_ids = cltv_p[cltv_p[\"cltv_p_segment\"] == \"C\"].index","af855828":"# To create product matrix for ARL we need invoice, quantity and stockcode or product name values. \n# Due to df1 has these information segment ids have been reducted.\n# Reduction of dataframe based on IDs that belongs segment A, B or C.\n\na_segment_df = df1[df1[\"Customer ID\"].isin(a_segment_ids)]\nb_segment_df = df1[df1[\"Customer ID\"].isin(b_segment_ids)]\nc_segment_df = df1[df1[\"Customer ID\"].isin(c_segment_ids)]\n\na_segment_df.head()","fd106f80":"def create_invoice_product_df(dataframe):\n    return dataframe.groupby(['Invoice', 'StockCode'])['Quantity'].sum().unstack().fillna(0). \\\n        applymap(lambda x: 1 if x > 0 else 0)\n\n# Generation of product matrix","2b7ca645":"\ndef create_rules(dataframe, country=False, head=5):\n    dataframe = create_invoice_product_df(dataframe)\n    frequent_itemsets = apriori(dataframe.iloc[:, 0:500], min_support=0.01, use_colnames=True)\n    rules = association_rules(frequent_itemsets, metric=\"support\", min_threshold=0.01)\n    print(rules.sort_values(\"lift\", ascending=False).head(head))\n\n    return rules\n\n# To eleminate errors data minimized with dataframe.iloc[:, 0:500]","d21f5f1d":"rules_a = create_rules(a_segment_df)","b7cab4df":"rules_a.head()","712ebfeb":"type(rules_a['consequents'][0])\n","28980225":"# Frozenset is very rarely used data type.","b0a38ac8":"rules_b = create_rules(b_segment_df)","09ef96e2":"rules_c = create_rules(c_segment_df)","98526525":"#  Choosing a product from a product cluster to recommend. It is a perfect code for frozenset data types. \n\nproduct_a = int(rules_a[\"consequents\"].apply(lambda x: list(x)[0]).astype(\"unicode\")[0])","7ec0731c":"product_b = int(rules_b[\"consequents\"].apply(lambda x: list(x)[0]).astype(\"unicode\")[0])","223a3672":"product_c = int(rules_c[\"consequents\"].apply(lambda x: list(x)[0]).astype(\"unicode\")[0])","5051074c":"product_a","425ae574":"def check_id(stock_code):\n    product_name = df1[df1[\"StockCode\"] == stock_code][[\"Description\"]].values[0].tolist()\n    return print(product_name)\n\n# To see description of recommended product ","d2b84f3a":"check_id(20713)","57752aae":"cltv_p.head()","d97fea55":"### cltv_p doesn't have country information. \n### To add recommendation column for each customer on cltv_p dataframe df1 and cltv_p have been crossed\n\ngermany_ids = df1[df1[\"Country\"] == \"Germany\"][\"Customer ID\"].drop_duplicates()","ac4e20a8":"germany_ids.head()","8d580ab3":"\ncltv_p[\"recommended_product\"] = \"\"","eb1852bf":"cltv_p.loc[cltv_p.index.isin(germany_ids)].head()","74eeff75":"cltv_p.loc[(cltv_p.index.isin(germany_ids)) & (cltv_p[\"cltv_p_segment\"] == \"A\")].head()","3af5f9a9":"# Recommendation product for customer who lives in Germany and belongs segment A \n\ncltv_p.loc[(cltv_p.index.isin(germany_ids)) & (cltv_p[\"cltv_p_segment\"] == \"A\"), \"recommended_product\"] = product_a","e544c2fb":"cltv_p.loc[(cltv_p.index.isin(germany_ids)) & (cltv_p[\"cltv_p_segment\"] == \"A\")].head()","5e5c18a9":"# Recommendation product for customer who lives in Germany and belongs segment B \n\ncltv_p.loc[(cltv_p.index.isin(germany_ids)) & (cltv_p[\"cltv_p_segment\"] == \"B\"), \"recommended_product\"] = product_b","958adeed":"cltv_p.loc[(cltv_p.index.isin(germany_ids)) & (cltv_p[\"cltv_p_segment\"] == \"B\")]","53866409":"# Recommendation product for customer who lives in Germany and belongs segment C\n\ncltv_p.loc[(cltv_p.index.isin(germany_ids)) & (cltv_p[\"cltv_p_segment\"] == \"C\"), \"recommended_product\"] = product_c","f6b69f90":"cltv_p.loc[(cltv_p.index.isin(germany_ids)) & (cltv_p[\"cltv_p_segment\"] == \"C\")]","91159125":"# Product recommendations to customers who live in Germany and classified according to segments are in a single table.\n\ncltv_p.loc[cltv_p.index.isin(germany_ids)]","d946dc26":"\ncltv_p[cltv_p.index == 12427].head()","2819777b":"### ","39afd0a1":"## Some Inferences\n\n*  For low frequency products LIFT, for high frequency products LEVERAGE could be considered to understand relation of between two products.\n*  CONFIDENCE can't be more than 1. \n*  For independent items CONFIDENCE is 1.\n*  For dependent items CONVICTION is inf.","da6b2bb6":"## Data Preparation","6544a996":"*  Support Value : Possibility Of Products To Be Seen With Each Other\n*  Confidence Value: Possibility Of Products To Be Sold Together\n*  Both LIFT and LEVERAGE measure the relation between the probability of a given rule to occur (support(A\u2192C)) and its expected probability if the items were independent (support(A)*support(C)) of each other. \n\n*  The only difference is that lift computes the ratio of both factors (support(A\u2192C)\/(support(A)*support(C))) and leverage computes the difference (support(A\u2192C)-support(A)*support(C))). The implications are that lift may find very strong associations for less frequent items, while leverage tends to prioritize items with higher frequencies\/support in the dataset.\n*  Conviction = A high conviction value means that the consequent is highly depending on the antecedent. For instance, in the case of a perfect confidence score, the denominator becomes 0 (due to 1 - 1) for which the conviction score is defined as 'inf'. Similar to lift, if items are independent, the conviction is 1.","9724d78e":"## Calculated Customer Lifetime Value\n\n### ***Use following create_cltv_p function to:***\n\n*  Create RFM \n*  Calculate 1 and 3 months sales prediction with BGNBD\n*  Calculate average profit prediction with GAMA GAMA\n*  Calculate customer lifetime value with ggf.customer_lifetime_value\n","fa138615":"## Recommendations to German Customers by Segment\n\n","64768ea2":"## This notebook is combination of Association Rule Learning, RFM with Customer Life Time Value.\n## My detailed studies about Association Rule Learning and RFM with Customer Life Time Value on the link below.\n### **https:\/\/www.kaggle.com\/atilaysamiloglu\/association-rule-learning**\n### **https:\/\/www.kaggle.com\/atilaysamiloglu\/crm-with-rfm-calculated-predicted-cltv**\n\n\n## Steps:\n\n* **Prepare data**\n* **Generate RFM metrics**\n* **Generate sales and profit predictions for future with BGNBD and GAMA GAMA**\n* **Add these predictions as columns to the RFM dataframe and then create cltv_p**\n* **Reduction of cltv_p based on segments of IDs**\n* **Generate association rules for each segments**\n* **Understanding of apriori metrics**\n\n","bc4ec1db":"## Generating Association Rules for Each Segment\n\n*  Since data has been splitted according to 3 segments, 3 different association rules has been created for each segment.\n*  That's why we can call that project **Developing Customized Product Recommendations**.\n","380476fe":"# Association Rule Learning and Developing Customized Product Recommendations","c6097ffd":"![apiori2.png](attachment:9a4f17cf-09d9-4323-8b2a-5858604c229f.png)"}}