{"cell_type":{"915d7bb7":"code","6b174227":"code","51fb72a7":"code","ddb8e637":"code","02755ba3":"code","b8e18db7":"code","4d907cea":"code","0f799b49":"code","03e00fb9":"code","fcecba4d":"code","470b832f":"code","851b5412":"code","21cc4606":"code","f1250f20":"code","7db577a6":"code","97f98bdb":"code","9a6b54f7":"code","27215263":"code","3f6c1c83":"code","6991da06":"code","6e8fb606":"code","86b319d4":"code","85c98330":"code","e1a81df3":"code","f3da991b":"code","9d44716b":"code","0a140e96":"code","f745d2ee":"code","e54a6ff3":"code","c0f4e2c4":"code","18ab8f6a":"code","cfd1eac0":"code","ea839a07":"code","2b665a35":"code","ffdd18ed":"code","6d45a6ae":"code","10e0c414":"code","d4d18288":"code","9c4cdfcb":"code","07b978a2":"code","4a847215":"code","f7a6d1ae":"code","92918291":"code","4709f1fb":"code","e2f9c673":"code","216fbc41":"code","717752fa":"code","75f11714":"code","84f7e508":"code","b6619433":"code","0b2ff259":"code","107a5453":"code","503df177":"code","87822a99":"code","2d7a1e70":"code","36329346":"code","6b30aba3":"code","035aeb5c":"code","456cba70":"code","3ed31279":"code","3e5de00e":"code","d555660f":"code","33391a71":"code","7a8c3484":"code","f4b8d560":"code","f5ab137c":"code","ba3cf85e":"code","6c0f2262":"code","8bbacd87":"code","08aa6f71":"code","39e24f3a":"code","44f12a54":"code","0139bf1b":"code","b7c58ab8":"code","11e23668":"code","dc572e6c":"code","6c900906":"code","e02ea499":"code","f23ebac6":"code","d69b1b26":"code","d260194b":"code","b4c78062":"code","2722d859":"code","06132209":"code","3f6473c4":"code","44a891b0":"code","2a570f75":"code","fc08aff4":"code","882f68bd":"code","47a757bd":"code","d3e9c886":"code","b49cd281":"code","56077c0c":"code","b666bdb8":"code","92433b1f":"code","961d4ce0":"code","ad35724a":"code","d9fc14b6":"code","aca4e954":"code","f882b7c6":"code","9445e73c":"code","2682cd55":"code","8d50f15f":"code","e8699e15":"code","c0b0aaa6":"code","f6ca49f6":"code","f24be960":"code","dfac06b2":"code","36bc17a4":"code","4555f16d":"code","4ef08f5c":"code","74154074":"markdown","9b165ba1":"markdown","c859420c":"markdown","4bfd5204":"markdown","fd577319":"markdown","ef363b0a":"markdown","b787aa34":"markdown","d76b6f79":"markdown","754cba7a":"markdown","54f5e653":"markdown","d98321d2":"markdown","5bf3d3d6":"markdown","a8e67a27":"markdown","9dceb4f9":"markdown","1931bdc2":"markdown","af50a3b4":"markdown","71dc6a0b":"markdown","95f784c7":"markdown","40eedb9b":"markdown","ccee2cfb":"markdown","538caa75":"markdown","0af73174":"markdown","155e2d3d":"markdown","62cdf308":"markdown","d4fe82db":"markdown","f5e0c048":"markdown","3f4d0411":"markdown","05e13395":"markdown","374044d0":"markdown","2531b94a":"markdown","c2806a58":"markdown","401fda20":"markdown","a6934679":"markdown","c68d7c9e":"markdown","c81e87c0":"markdown","ae91950a":"markdown","649030a3":"markdown","82a0c557":"markdown","fd2b6f89":"markdown"},"source":{"915d7bb7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nimport datetime\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.feature_selection import f_regression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error ","6b174227":"data_raw = pd.read_csv(\"\/kaggle\/input\/shanghai-air-pollution-and-wheather-20142021\/shanghai-air-quality.csv\")","51fb72a7":"data_raw.head()","ddb8e637":"data_raw.dtypes","02755ba3":"df = data_raw\ndf.date = pd.to_datetime(df['date'], format=\"%Y-%m-%d\") \ndf_sorted = df.sort_values(by= [\"date\"])\ndf_sorted.set_index(\"date\")","b8e18db7":"df_sorted","4d907cea":"df_sorted.columns","0f799b49":"df_sorted.columns = ['date', 'pm25', 'pm10', 'o3', 'no2', 'so2', 'co']\ndf = df_sorted","03e00fb9":"for col in ['pm25', 'pm10', 'o3', 'no2', 'so2', 'co']:\n    for i in range(len(df[col])):\n        df[col][i] = df[col][i].strip()\n    df[col] = pd.to_numeric(df[col])","fcecba4d":"shanghai = pd.read_csv(\"\/kaggle\/input\/shanghai-air-pollution-and-wheather-20142021\/shanghai.csv\"); shanghai","470b832f":"shanghai.columns = ['date', 'maxtempC', 'mintempC', 'totalSnow_cm', 'sunHour',\n       'uvIndex', 'moon_illumination', 'moonrise', 'moonset', 'sunrise',\n       'sunset', 'DewPointC', 'FeelsLikeC', 'HeatIndexC', 'WindChillC',\n       'WindGustKmph', 'cloudcover', 'humidity', 'precipMM', 'pressure',\n       'tempC', 'visibility', 'winddirDegree', 'windspeedKmph', 'location']\nshanghai.date = pd.to_datetime(shanghai['date'], format=\"%Y-%m-%d\") ; ","851b5412":"shanghai.isnull().sum()","21cc4606":"merged_data = df.merge(shanghai, how = \"left\", on = \"date\")\nmerged_data.head()","f1250f20":"merged_data.isnull().sum()","7db577a6":"data_cleaned = merged_data;\ndata_cleaned.head()","97f98bdb":"data_cleaned.drop(data_cleaned.index[-1], inplace=True)\ndata_cleaned.drop(['moonrise', 'moonset', 'sunrise', 'sunset','location'], axis=1, inplace=True)","9a6b54f7":"df= data_cleaned","27215263":"for i in range(2):\n    for col in ['pm25', 'pm10', 'o3', 'no2', 'so2', 'co']:\n        #print (col * 3)\n        for i in range(len(df[col])):\n            if np.isnan (df[col][i]):\n                #print(\"index = \" + str(i))\n                df[col][i] = (np.average(df[col][i+1:i+5]) + np.average(df[col][i-5:i-1]))\/2","3f6c1c83":"df.isnull().sum()","6991da06":"df1 = df\nimp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\nimp_mean = imp_mean.fit(df1.iloc[:,1:7])\ndf1.iloc[:,1:7] = imp_mean.transform(df1.iloc[:,1:7])\ndf1.isnull().sum()","6e8fb606":"## PM2.5 Sub-Index calculation\ndef get_PM25_subindex(x):\n    if x <= 30:\n        return x * 50 \/ 30\n    elif x <= 60:\n        return 50 + (x - 30) * 50 \/ 30\n    elif x <= 90:\n        return 100 + (x - 60) * 100 \/ 30\n    elif x <= 120:\n        return 200 + (x - 90) * 100 \/ 30\n    elif x <= 250:\n        return 300 + (x - 120) * 100 \/ 130\n    elif x > 250:\n        return 400 + (x - 250) * 100 \/ 130\n    else:\n        return 0\n    \n## PM10 Sub-Index calculation\ndef get_PM10_subindex(x):\n    if x <= 50:\n        return x\n    elif x <= 100:\n        return x\n    elif x <= 250:\n        return 100 + (x - 100) * 100 \/ 150\n    elif x <= 350:\n        return 200 + (x - 250)\n    elif x <= 430:\n        return 300 + (x - 350) * 100 \/ 80\n    elif x > 430:\n        return 400 + (x - 430) * 100 \/ 80\n    else:\n        return 0    \n\n## SO2 Sub-Index calculation\ndef get_SO2_subindex(x):\n    if x <= 40:\n        return x * 50 \/ 40\n    elif x <= 80:\n        return 50 + (x - 40) * 50 \/ 40\n    elif x <= 380:\n        return 100 + (x - 80) * 100 \/ 300\n    elif x <= 800:\n        return 200 + (x - 380) * 100 \/ 420\n    elif x <= 1600:\n        return 300 + (x - 800) * 100 \/ 800\n    elif x > 1600:\n        return 400 + (x - 1600) * 100 \/ 800\n    else:\n        return 0\n    \n\n\n\n## NOx Sub-Index calculation\ndef get_NOx_subindex(x):\n    if x <= 40:\n        return x * 50 \/ 40\n    elif x <= 80:\n        return 50 + (x - 40) * 50 \/ 40\n    elif x <= 180:\n        return 100 + (x - 80) * 100 \/ 100\n    elif x <= 280:\n        return 200 + (x - 180) * 100 \/ 100\n    elif x <= 400:\n        return 300 + (x - 280) * 100 \/ 120\n    elif x > 400:\n        return 400 + (x - 400) * 100 \/ 120\n    else:\n        return 0\n    \n\n\n## CO Sub-Index calculation\ndef get_CO_subindex(x):\n    if x <= 1:\n        return x * 50 \/ 1\n    elif x <= 2:\n        return 50 + (x - 1) * 50 \/ 1\n    elif x <= 10:\n        return 100 + (x - 2) * 100 \/ 8\n    elif x <= 17:\n        return 200 + (x - 10) * 100 \/ 7\n    elif x <= 34:\n        return 300 + (x - 17) * 100 \/ 17\n    elif x > 34:\n        return 400 + (x - 34) * 100 \/ 17\n    else:\n        return 0\n\n    \n## O3 Sub-Index calculation\ndef get_O3_subindex(x):\n    if x <= 50:\n        return x * 50 \/ 50\n    elif x <= 100:\n        return 50 + (x - 50) * 50 \/ 50\n    elif x <= 168:\n        return 100 + (x - 100) * 100 \/ 68\n    elif x <= 208:\n        return 200 + (x - 168) * 100 \/ 40\n    elif x <= 748:\n        return 300 + (x - 208) * 100 \/ 539\n    elif x > 748:\n        return 400 + (x - 400) * 100 \/ 539\n    else:\n        return 0","86b319d4":"df[\"pm25_sub_index\"] = df[\"pm25\"].apply(lambda x: get_PM25_subindex(x))\ndf[\"pm10_sub_index\"] = df[\"pm10\"].apply(lambda x: get_PM10_subindex(x))\ndf[\"o3_sub_index\"] = df[\"o3\"].apply(lambda x: get_O3_subindex(x))\ndf[\"no2_sub_index\"] = df[\"no2\"].apply(lambda x: get_NOx_subindex(x))\ndf[\"so2_sub_index\"] = df[\"so2\"].apply(lambda x: get_SO2_subindex(x))\ndf[\"co_sub_index\"] = df[\"co\"].apply(lambda x: get_CO_subindex(x))\ndf.head()","85c98330":"df[\"AQI\"] = round(df[['pm25_sub_index', 'pm10_sub_index', 'o3_sub_index', 'no2_sub_index', 'so2_sub_index', 'co_sub_index']].max(axis = 1))\ndf.head()","e1a81df3":"color_AQI = [\"maroon\", \"purple\", \"red\", \"orange\", \"yellow\",\"green\"]\n\ndef get_AQI_bucket(x):\n    if x <= 50:\n        return \"Good\"\n    elif x <= 100:\n        return \"Moderate\"\n    elif x <= 150:\n        return \"Unhealthy for sensitive groups\"\n    elif x <= 200:\n        return \"Unhealthy\"\n    elif x <= 300:\n        return \"Very unhealthy\"\n    elif x > 300:\n        return \"Hazardous\"\n    else:\n        return np.NaN","f3da991b":"df[\"AQI_Explained\"] = df[\"AQI\"].apply(lambda x: get_AQI_bucket(x))","9d44716b":"df.head()","0a140e96":"plt.figure(figsize=(16,8))\nsns.set_palette(color_AQI)\ncategory_order = [\"Hazardous\",\n                 \"Very unhealthy\",\n                  \"Unhealthy\",\n                  \"Unhealthy for sensitive groups\",\n                  \"Moderate\",\n                  \"Good\"\n                 ]\n\nsns.catplot(x=\"AQI_Explained\", data = df, kind=\"count\", order=category_order)\n\nplt.xticks(rotation=90)\nplt.show()\n","f745d2ee":"df_time = df.set_index(\"date\")","e54a6ff3":"df_time","c0f4e2c4":"df_time.loc[\"2020\"].AQI.mean()","18ab8f6a":"years = [\"2021\",\"2020\",\"2019\",\"2018\",\"2017\",\"2016\",\"2015\",\"2014\"]\nfor year in years:\n    sns.set_palette(color_AQI)\n    sns.catplot( x= \"AQI_Explained\",data = df_time.loc[year],kind = \"count\", order=category_order)\n    plt.xticks(rotation=90)\n    plt.title(year)\n","cfd1eac0":"for year in years:\n    print (year)\n    print (df_time.loc[year][\"AQI_Explained\"].value_counts())\n    print()\n    print (\"--------------\"*3)\n    print()","ea839a07":"for year in reversed(years) :\n    print()\n    \n    days = df_time.loc[year][\"AQI_Explained\"].count()\n    Hazardous = df_time.loc[year][\"AQI_Explained\"].value_counts()[\"Hazardous\"]\n    Percent = round ((Hazardous\/days)*100)\n    print(\"In \" + str(year) + \" %\"+str(Percent)+ \" of the days were Hazardous\")\n    \n    days = df_time.loc[year][\"AQI_Explained\"].count()\n    Hazardous = df_time.loc[year][\"AQI_Explained\"].value_counts()[\"Very unhealthy\"]\n    Percent = round ((Hazardous\/days)*100)\n    print(\"In \" + str(year) + \" %\"+str(Percent)+ \" of the days were Very unhealthy\")\n    \n    days = df_time.loc[year][\"AQI_Explained\"].count()\n    Hazardous = df_time.loc[year][\"AQI_Explained\"].value_counts()[\"Unhealthy\"]\n    Percent = round ((Hazardous\/days)*100)\n    print(\"In \" + str(year) + \" %\"+str(Percent)+ \" of the days were Unhealthy\")\n    \n    days = df_time.loc[year][\"AQI_Explained\"].count()\n    Hazardous = df_time.loc[year][\"AQI_Explained\"].value_counts()[[\"Unhealthy\",\"Very unhealthy\",\"Hazardous\"]].sum()\n    Percent = round ((Hazardous\/days)*100)\n    print(\"In \" + str(year) + \" %\"+str(Percent)+ \" of the days the air was unhealthy for the general public\")\n    \n    print()\n    print (\"------------------\"*3)\n    print()","2b665a35":"df_time.drop(['pm25', 'pm10', 'o3', 'no2', 'so2', 'co','pm25_sub_index', 'pm10_sub_index',\n       'o3_sub_index', 'no2_sub_index', 'so2_sub_index', 'co_sub_index'],axis=1,inplace=True)\n# We drop the sub-indexes and also the pollution correspondors \n#since we have the AQI calculated, \n#they are no longer necessary on our cleaned data\n\ndf_time.to_csv(\"All Values Cleaned Shanghai1.csv\")\n#df_time.to_excel(\"Shanghai AQI Analysis Data.xlsx\")","ffdd18ed":"ls","6d45a6ae":"df = pd.read_csv(\"All Values Cleaned Shanghai1.csv\")\ndf.date = pd.to_datetime(df['date'], format=\"%Y-%m-%d\") \ndf_sorted = df.sort_values(by= [\"date\"])\ndf_sorted.set_index(\"date\",inplace = True)\ndf = df_sorted\ndf.columns\ndf","10e0c414":"columns_to_be_used = df.columns[1:(len(df.columns)-2)]","d4d18288":"x = df[columns_to_be_used]\ny = df.AQI\nx","9c4cdfcb":"p_values = f_regression (x,y)[1]\np_values.round(5)","07b978a2":"columns_to_be_selected_using_f_statistics =[]\nfor index, i in enumerate(p_values.round(3)):\n    if i > 0 :\n        columns_to_be_selected_using_f_statistics.append(x.columns[index])\n        \ncolumns_to_be_selected_using_f_statistics","4a847215":"x_fstat = df.drop(columns_to_be_selected_using_f_statistics, axis=1)","f7a6d1ae":"x_fstat.drop([\"AQI\",\"AQI_Explained\"],axis=1,inplace=True)","92918291":"names = x.columns\nlasso = Lasso(alpha=0.2)\nlasso_coef = lasso.fit(x, y).coef_\n\nlasso_coef_data = pd.DataFrame({\"Features\":names,\"lasso_coef\": lasso_coef })\nsns.barplot(data=lasso_coef_data, x= \"Features\", y = \"lasso_coef\")\nplt.xticks(rotation = 75)\nplt.show()","4709f1fb":"lasso_coef = np.abs(lasso_coef)\nlasso_Coef = pd.DataFrame({'Columns':names,'Coef':lasso_coef})\nlasso_Coef","e2f9c673":"columns_to_be_delated = lasso_Coef[lasso_Coef.Coef < 1].Columns; columns_to_be_delated","216fbc41":"x.drop(columns_to_be_delated,axis=1,inplace=True); x","717752fa":"x_train = x.loc[\"2014-01\":\"2017-12\"]\ny_train = y.loc[\"2014-01\":\"2017-12\"]\n\nx_test = x.loc[\"2018-01\":\"2019-10\"]\ny_test = y.loc[\"2018-01\":\"2019-10\"]","75f11714":"#Linear Regression\n\nregression = LinearRegression()\nregression.fit(x_train,y_train)\ny_predicted_sk_simple = regression.predict(x_test)\nscore_reg = regression.score(x_test,y_test)\n\n\n#Cross Validation Regression\n\nreg= LinearRegression()\ncv_resutls = cross_val_score(reg,x_train,y_train,cv=4); cv_resutls\n\ncross_y_pred = cross_val_predict(reg,x_test,y_test,cv=4)\nscore_cross = np.mean(cv_resutls)\n\n#Ridge Regression\n\nridge = Ridge(alpha=0.1,normalize=True)\nridge.fit(x_train,y_train)\nridge_pred = ridge.predict(x_test)\nscore_ridge = ridge.score (x_test,y_test)\n\n#Lasso Regression\n\nlasso = Lasso(alpha=0.1, normalize=True)\nlasso.fit(x_train, y_train)\nlasso_pred = lasso.predict(x_test)\nscore_lasso = lasso.score(x_test, y_test)","84f7e508":"\nscore_comparison_daily = pd.DataFrame([score_reg,score_cross,score_ridge,score_lasso],\n                                columns=[\"Scores\"],\n                                index= ['Linear_Reg','Cross_Val','Ridge','Lasso'],dtype=float)","b6619433":"score_comparison_daily","0b2ff259":"x = x_fstat\n\nx_train = x.loc[\"2014-01\":\"2017-12\"]\ny_train = y.loc[\"2014-01\":\"2017-12\"]\n\nx_test = x.loc[\"2018-01\":\"2019-10\"]\ny_test = y.loc[\"2018-01\":\"2019-10\"]","107a5453":"#Linear Regression\n\nregression = LinearRegression()\nregression.fit(x_train,y_train)\ny_predicted_sk_simple = regression.predict(x_test)\nscore_reg = regression.score(x_test,y_test)\n\n\n#Cross Validation Regression\n\nreg= LinearRegression()\ncv_resutls = cross_val_score(reg,x_train,y_train,cv=4); cv_resutls\n\ncross_y_pred = cross_val_predict(reg,x_test,y_test,cv=4)\nscore_cross = np.mean(cv_resutls)\n\n#Ridge Regression\n\nridge = Ridge(alpha=0.1,normalize=True)\nridge.fit(x_train,y_train)\nridge_pred = ridge.predict(x_test)\nscore_ridge = ridge.score (x_test,y_test)\n\n#Lasso Regression\n\nlasso = Lasso(alpha=0.1, normalize=True)\nlasso.fit(x_train, y_train)\nlasso_pred = lasso.predict(x_test)\nscore_lasso = lasso.score(x_test, y_test)","503df177":"comparison_daily_F = pd.DataFrame({\"Linear_Reg\":y_predicted_sk_simple, \n                           \"Cross Val\" :cross_y_pred, \n                           \"Ridge\": ridge_pred,\n                          \"Lasso\": lasso_pred,\n                          \"AQI\":y_test},index=y_test.index)","87822a99":"comparison_daily_F","2d7a1e70":"score_comparison_daily_F = pd.DataFrame([score_reg,score_cross,score_ridge,score_lasso],\n                                columns=[\"Scores\"],\n                                index= ['Linear_Reg','Cross_Val','Ridge','Lasso'],dtype=float)","36329346":"score_comparison_daily_F","6b30aba3":"daily_score_comp = score_comparison_daily.merge(score_comparison_daily_F,\n                                                on = score_comparison_daily.index,\n                                                how= \"left\", suffixes=(\"_Lasso\",\"_F\"))","035aeb5c":"daily_score_comp.columns =[\"Algorithm\",\"Lesso_Feature\",\"F_Stat_Feature\"]\ndaily_score_comp.set_index(\"Algorithm\",inplace=True)","456cba70":"daily_score_comp","3ed31279":"df = pd.read_csv(\"All Values Cleaned Shanghai1.csv\")\ndf.date = pd.to_datetime(df['date'], format=\"%Y-%m-%d\") \n\ndf_sorted = df.sort_values(by= [\"date\"])\nmonthly = df.groupby(df['date'].dt.strftime('%Y-%m'))['AQI'].mean()\ndf_month = pd.DataFrame(monthly)\ny = df_month.sort_values(by= [\"date\"])","3e5de00e":"monthly = df.groupby(df['date'].dt.strftime('%Y-%m'))['mintempC', 'uvIndex', 'DewPointC', \n                                                      'FeelsLikeC', 'HeatIndexC',\n       'WindChillC', 'WindGustKmph', 'cloudcover', 'humidity', 'precipMM',\n       'pressure', 'tempC', 'winddirDegree', 'windspeedKmph'].mean()\ndf_month = pd.DataFrame(monthly)\nx = df_month.sort_values(by= [\"date\"])\nx","d555660f":"p_values = f_regression (x,y)[1]\nprint(p_values.round(3))\n\ncolumns_to_be_selected_using_f_statistics =[]\nfor index, i in enumerate(p_values.round(10)):\n    if i > 0 :\n        columns_to_be_selected_using_f_statistics.append(x.columns[index])\n        \nprint(columns_to_be_selected_using_f_statistics)","33391a71":"x_fstat = x.drop(columns_to_be_selected_using_f_statistics,axis=1); x_fstat.columns","7a8c3484":"names = x.columns\nlasso = Lasso(alpha=0.2)\nlasso_coef = lasso.fit(x, y).coef_\n\nlasso_coef_data = pd.DataFrame({\"Features\":names,\"lasso_coef\": lasso_coef })\nsns.barplot(data=lasso_coef_data, x= \"Features\", y = \"lasso_coef\")\nplt.xticks(rotation = 75)\nplt.show()","f4b8d560":"lasso_coef = np.abs(lasso_coef)\nlasso_Coef = pd.DataFrame({'Columns':names,'Coef':lasso_coef})\nlasso_Coef","f5ab137c":"columns_to_be_delated = lasso_Coef[lasso_Coef.Coef < 1].Columns; print(columns_to_be_delated)\n\nlasso_feature_x = x.drop(columns_to_be_delated,axis=1); lasso_feature_x.shape","ba3cf85e":"x = lasso_feature_x\nx_train = x.loc[\"2014-01\":\"2017-12\"]\ny_train = y.loc[\"2014-01\":\"2017-12\"]\n\nx_test = x.loc[\"2018-01\":\"2019-10\"]\ny_test = y.loc[\"2018-01\":\"2019-10\"]","6c0f2262":"def Ridge_Regression (x_train,y_train,x_test,y_test):\n    from sklearn.linear_model import Ridge\n    \n    Alpha_array = []\n    Score_array = []\n    for alpha in range (1,9):\n\n        ridge = Ridge(alpha=(alpha\/10),normalize=True)\n        ridge.fit(x_train,y_train)\n        ridge_pred = ridge.predict(x_test)\n        score_ridge = ridge.score (x_test,y_test)\n        Score_array.append(score_ridge) \n        Alpha_array.append(alpha\/10)\n        print (\"alpha = \" + str(alpha)+ \"  Score = \" + str(score_ridge))\n\n    score_ridge_df = pd.DataFrame({\"Scores\": Score_array,\"Alpha\":Alpha_array})\n    Score_ridge = score_ridge_df[score_ridge_df[\"Scores\"] == score_ridge_df.Scores.max()]\n    np_Score_ridge = Score_ridge.to_numpy()\n    \n      \n    ridge = Ridge(alpha=np_Score_ridge[0,1],normalize=True)\n    ridge.fit(x_train,y_train)\n    ridge_pred = ridge.predict(x_test)\n    \n    \n    return np_Score_ridge[0,0] , ridge_pred\n","8bbacd87":"def Lasso_Regression (x_train,y_train,x_test,y_test):\n    from sklearn.linear_model import Lasso\n\n    Alpha_array = []\n    Score_array = []\n    for alpha in range (1,10):\n        lasso = Lasso(alpha=(alpha\/10),normalize=True)\n        lasso.fit(x_train,y_train)\n        lasso_pred = lasso.predict(x_test)\n        score_lasso = lasso.score (x_test,y_test)\n        Score_array.append(score_lasso) \n        Alpha_array.append(alpha\/10)\n        \n        print (\"alpha = \" + str(alpha)+ \"  Score = \" + str(score_lasso))\n    score_lasso_df = pd.DataFrame({\"Scores\": Score_array,\"Alpha\":Alpha_array})      \n        \n    Score_lasso = score_lasso_df[score_lasso_df[\"Scores\"] == score_lasso_df.Scores.max()]\n    \n    np_Score_Lasso = Score_lasso.to_numpy()\n\n    lasso = Lasso(alpha=np_Score_Lasso[0,1] , normalize=True)\n    lasso.fit(x_train,y_train)\n    lasso_pred = lasso.predict(x_test)\n    return np_Score_Lasso[0,0] , lasso_pred\n","08aa6f71":"#Linear Regression\n\nregression = LinearRegression()\nregression.fit(x_train,y_train)\ny_predicted_sk_simple = regression.predict(x_test)\nscore_reg = regression.score(x_test,y_test)\n\n\n#Cross Validation Regression\n\nreg= LinearRegression()\ncv_resutls = cross_val_score(reg,x_train,y_train,cv=4); cv_resutls\n\ncross_y_pred = cross_val_predict(reg,x_test,y_test,cv=4)\nscore_cross = np.mean(cv_resutls)\n\n#Ridge Regression\n\n#ridge = Ridge(alpha=0.1,normalize=True)\n#ridge.fit(x_train,y_train)\n#ridge_pred = ridge.predict(x_test)\n#score_ridge = ridge.score (x_test,y_test)\nprint(\"Ridge Regression\")\nscore_ridge , ridge_pred = Ridge_Regression(x_train,y_train,x_test,y_test)\n\n\n#Lasso Regression\n\n#lasso = Lasso(alpha=0.1, normalize=True)\n#lasso.fit(x_train, y_train)\n#lasso_pred = lasso.predict(x_test)\n#score_lasso = lasso.score(x_test, y_test)\nprint()\nprint(\"Lasso Regression\")\n\nscore_lasso , lasso_pred = Lasso_Regression(x_train,y_train,x_test,y_test)","39e24f3a":"data_cross_valid = pd.DataFrame(cross_y_pred,index=y_test.index)\ndata_ridge = pd.DataFrame(ridge_pred,index=y_test.index)\ndata_lineer = pd.DataFrame(y_predicted_sk_simple,index=y_test.index)\ndata_lasso = pd.DataFrame(lasso_pred,index=y_test.index)\n\ndata1 = data_cross_valid.merge(data_ridge, on = \"date\", how=\"left\")\ndata1.columns = [\"Cross_Valid\", \"Ridge\"]\n\ndata2 = data1.merge(data_lasso,on = \"date\", how=\"left\")\ndata2.columns = [\"Cross_Valid\", \"Ridge\", \"Lasso\"]\n\ndata3 = data2.merge (data_lineer, on = \"date\", how = \"left\")\ndata3.columns = [\"Cross_Valid\", \"Ridge\", \"Lasso\", \"Lineer\"]\n\nprediction_compare_monthly = data3.merge (y_test,on =\"date\", how= \"left\")\nprediction_compare_monthly.columns = [\"Cross_Valid\", \"Ridge\", \"Lasso\", \"Lineer\", \"AQI\"]\n\nprediction_compare_monthly","44f12a54":"def convert_to_explained_AQI (data):\n    explained_AQI = data\n    for i in data.columns:\n        explained_AQI[i] = data[i].apply(lambda x: get_AQI_bucket(x))\n    return explained_AQI\n    ","0139bf1b":"my_data_Lasso_Selected = convert_to_explained_AQI(prediction_compare_monthly); my_data_Lasso_Selected","b7c58ab8":"score_comparison_Monthly = pd.DataFrame([score_reg,score_cross,score_ridge,score_lasso],\n                                columns=[\"Scores\"],\n                                index= ['Linear_Reg','Cross_Val','Ridge','Lasso'],dtype=float)\nscore_comparison_Monthly","11e23668":"x = x_fstat\n\nx_train = x.loc[\"2014-01\":\"2017-12\"]\ny_train = y.loc[\"2014-01\":\"2017-12\"]\n\nx_test = x.loc[\"2018-01\":\"2019-10\"]\ny_test = y.loc[\"2018-01\":\"2019-10\"]","dc572e6c":"#Linear Regression\n\nregression = LinearRegression()\nregression.fit(x_train,y_train)\ny_predicted_sk_simple = regression.predict(x_test)\nscore_reg = regression.score(x_test,y_test)\n\n\n#Cross Validation Regression\n\nreg= LinearRegression()\ncv_resutls = cross_val_score(reg,x_train,y_train,cv=4); cv_resutls\n\ncross_y_pred = cross_val_predict(reg,x_test,y_test,cv=4)\n#score_cross = r2_score(cross_y_pred, y_test)\ny_true = y_test\ny_pred1 = cross_y_pred\nscore_cross = r2_score(y_true, y_pred1)\n\n#Ridge Regression\n\n#ridge = Ridge(alpha=0.1,normalize=True)\n#ridge.fit(x_train,y_train)\n#ridge_pred = ridge.predict(x_test)\n#score_ridge = ridge.score (x_test,y_test)\nprint()\nprint(\"Ridge Regression\")\n\nscore_ridge , ridge_pred = Ridge_Regression(x_train,y_train,x_test,y_test)\n\n#Lasso Regression\n\n#lasso = Lasso(alpha=0.1, normalize=True)\n#lasso.fit(x_train, y_train)\n#lasso_pred = lasso.predict(x_test)\n#score_lasso = lasso.score(x_test, y_test)\n\nprint()\nprint(\"Lasso Regression\")\n\nscore_lasso , lasso_pred = Lasso_Regression(x_train,y_train,x_test,y_test)","6c900906":"data_cross_valid = pd.DataFrame(cross_y_pred,index=y_test.index)\ndata_ridge = pd.DataFrame(ridge_pred,index=y_test.index)\ndata_lineer = pd.DataFrame(y_predicted_sk_simple,index=y_test.index)\ndata_lasso = pd.DataFrame(lasso_pred,index=y_test.index)\n\ndata1 = data_cross_valid.merge(data_ridge, on = \"date\", how=\"left\")\ndata1.columns = [\"Cross_Valid\", \"Ridge\"]\n\ndata2 = data1.merge(data_lasso,on = \"date\", how=\"left\")\ndata2.columns = [\"Cross_Valid\", \"Ridge\", \"Lasso\"]\n\ndata3 = data2.merge (data_lineer, on = \"date\", how = \"left\")\ndata3.columns = [\"Cross_Valid\", \"Ridge\", \"Lasso\", \"Lineer\"]\n\npred_comp_monthly_F = data3.merge (y_test,on =\"date\", how= \"left\")\npred_comp_monthly_F.columns = [\"Cross_Valid\", \"Ridge\", \"Lasso\", \"Lineer\", \"AQI\"]\n\n\npred_comp_monthly_F","e02ea499":"my_data_F_Selected = convert_to_explained_AQI(pred_comp_monthly_F); my_data_F_Selected","f23ebac6":"score_comp_Monthly_F = pd.DataFrame([score_reg,score_cross,score_ridge,score_lasso],\n                                columns=[\"Scores\"],\n                                index= ['Linear_Reg','Cross_Val','Ridge','Lasso'],dtype=float)\nscore_comp_Monthly_F","d69b1b26":"import math\nmse =[]\nfor i in [cross_y_pred,lasso_pred,y_predicted_sk_simple,ridge_pred]:\n    \n    mse.append(math.sqrt(mean_squared_error(y_test ,i)) )\n\nmse","d260194b":"two_feature_selection_comp = score_comparison_Monthly.merge(score_comp_Monthly_F, \n                                                            how = \"left\", \n                                                            on=score_comparison_Monthly.index, \n                                                            suffixes=[\"_Lasso\",\"_F\"])\ntwo_feature_selection_comp.columns = [\"Algorithm\",\"Lasso_Feature_Selection\",\"F_Feature_Selection\"]\ntwo_feature_selection_comp.set_index(\"Algorithm\",inplace=True)","b4c78062":"features_used_F      = pd.DataFrame({\"id\":range(len(x_fstat.columns)),\"F_Selected\":x_fstat.columns})\nfeatures_used_Lasso  = pd.DataFrame ({\"id\":range(len(lasso_feature_x.columns)),\"Lasso_Selected\":lasso_feature_x.columns})\n","2722d859":"features_used = pd.merge(features_used_Lasso,features_used_F, left_on=\"id\",right_on=\"id\", how= \"outer\")\nfeatures_used.set_index(\"id\",inplace=True)","06132209":"features_used ","3f6473c4":"two_feature_selection_comp","44a891b0":"def decision_tree_reg (x_train, y_train, x_test, y_test):\n    from sklearn.tree import DecisionTreeRegressor\n    from sklearn.metrics import mean_squared_error as MSE\n   \n    score_tree_array = []\n    max_depth_array = []\n    min_samp_array = []\n    random_st_array =[]\n    for max_depth in range(1,7):\n        for min_samp in range(1,5):\n            for random_st in range (10):\n                dt = DecisionTreeRegressor(max_depth=max_depth,\n                              min_samples_leaf=(min_samp\/10),\n                              random_state=random_st)\n                dt.fit(x_train,y_train)\n                score_tree_array.append(dt.score(x_test,y_test)) \n                max_depth_array.append(max_depth)\n                min_samp_array.append(min_samp\/10)\n                random_st_array.append(random_st)\n\n    score_tree_df = pd.DataFrame({\"max_depth\":max_depth_array,\"min_samples_leaf\":min_samp_array,\"random_state\": random_st_array,\"Scores\":score_tree_array})\n    \n    #if the max score can be achieved by several other settings then we can select the minimum values so that our model runs faster\n    \n    if score_tree_df[score_tree_df.Scores.max() == score_tree_df.Scores].Scores.count() > 1:\n        max_depth = score_tree_df[score_tree_df.Scores.max() == score_tree_df.Scores].max_depth.min()\n        min_leaf = score_tree_df[score_tree_df.Scores.max() == score_tree_df.Scores].min_samples_leaf.min()\n        rnd_state = score_tree_df[score_tree_df.Scores.max() == score_tree_df.Scores].random_state.min()\n        dt = DecisionTreeRegressor(max_depth=max_depth,\n                              min_samples_leaf=min_leaf,\n                              random_state=rnd_state)\n        dt.fit(x_train,y_train)\n        y_pred_tree = dt.predict(x_test)\n        mse_dt = MSE (y_test,y_pred_tree)\n        rmse_dt = mse_dt**(1\/2)\n    else:\n        max_depth = score_tree_df[score_tree_df.Scores.max() == score_tree_df.Scores].max_depth\n        min_leaf = score_tree_df[score_tree_df.Scores.max() == score_tree_df.Scores].min_samples_leaf\n        rnd_state = score_tree_df[score_tree_df.Scores.max() == score_tree_df.Scores].random_state\n        dt = DecisionTreeRegressor(max_depth=max_depth,\n                              min_samples_leaf=min_leaf,\n                              random_state=rnd_state)\n        dt.fit(x_train,y_train)\n        y_pred_tree = dt.predict(x_test)\n        mse_dt = MSE (y_test,y_pred_tree)\n        rmse_dt = mse_dt**(1\/2)\n    return  score_tree_df.Scores.max()  , rmse_dt , y_pred_tree","2a570f75":"x = lasso_feature_x\nx_train = x.loc[\"2014-01\":\"2017-12\"]\ny_train = y.loc[\"2014-01\":\"2017-12\"]\n\nx_test = x.loc[\"2018-01\":\"2019-10\"]\ny_test = y.loc[\"2018-01\":\"2019-10\"]","fc08aff4":" score_tree_lasso, rmse_dt_lasso, y_pre_tree_lasso = decision_tree_reg(x_train,y_train,x_test,y_test)","882f68bd":"x = x_fstat\nx_train = x.loc[\"2014-01\":\"2017-12\"]\ny_train = y.loc[\"2014-01\":\"2017-12\"]\n\nx_test = x.loc[\"2018-01\":\"2019-10\"]\ny_test = y.loc[\"2018-01\":\"2019-10\"]","47a757bd":" score_tree_f, rmse_dt_f, y_pre_tree_f = decision_tree_reg(x_train,y_train,x_test,y_test)","d3e9c886":"df_tree = pd.DataFrame({\"Lasso_Feature_Selection\":score_tree_lasso,\n                        \"F_Feature_Selection\": score_tree_f},index=[\"Decision_reg_tree\"])\ntwo_feature_sel_final = pd.concat([two_feature_selection_comp,df_tree])","b49cd281":"two_feature_sel_final","56077c0c":"import tensorflow as tf","b666bdb8":"df = pd.read_csv(\"All Values Cleaned Shanghai1.csv\")\ndf.date = pd.to_datetime(df['date'], format=\"%Y-%m-%d\") \n\ndf_sorted = df.sort_values(by= [\"date\"])\nmonthly = df.groupby(df['date'].dt.strftime('%Y-%m'))['AQI'].mean()\ndf_month = pd.DataFrame(monthly)\ny = df_month.sort_values(by= [\"date\"])","92433b1f":"monthly = df.groupby(df['date'].dt.strftime('%Y-%m'))['mintempC', 'uvIndex', 'DewPointC', \n                                                      'FeelsLikeC', 'HeatIndexC',\n       'WindChillC', 'WindGustKmph', 'cloudcover', 'humidity', 'precipMM',\n       'pressure', 'tempC', 'winddirDegree', 'windspeedKmph'].mean()\ndf_month = pd.DataFrame(monthly)\nx = df_month.sort_values(by= [\"date\"])\nx","961d4ce0":"# IMPORTANT : x_train, y_train, x_test, y_test should be dataframes.\n\ndef tf_nn (x_train,y_train,x_test,y_test,steps_to_see,epochs): #epochs should be dividable by steps\n    \n    from sklearn.metrics import r2_score #will be used to calculate the score of our model \n    \n    #first we change the data top arrays since tf works with arrays\n    x_train_np = x_train.to_numpy()\n    y_train_np = y_train.to_numpy()\n\n    x_test_np = x_test.to_numpy()\n    y_test_np = y_test.to_numpy()\n    \n    #second we create the model for our neural network \n    input_size = x_train_np.shape[1]\n    output_size = y_train_np.shape[1]\n    model = tf.keras.Sequential([tf.keras.layers.Dense(output_size)])\n    \n    #then we compile with nadam optimizer, there are other optimizers as well but for this example I go with nadam\n    # also I will use the mean squared error for calculating the loss values\n    model.compile (optimizer = \"nadam\", loss = \"mean_squared_error\")\n    \n    #this loop will run number_of_runs times for 500 steps each, \n    #then print out the predicted vaues after each 500 steps\n    #so that we can see how our model comes closer to the actual values\n    for i in range(int(epochs\/steps_to_see)):\n        # fitting the regression\n        model.fit(x_train_np,y_train_np,epochs=(steps_to_see),verbose=0)\n        \n        # predicting the values\n        np_predicted_tf = model.predict_on_batch(x_test_np); \n        \n        # getting y_test data and y_predicted data to the same data frame for comparison\n        tf_data_pred = pd.DataFrame( np_predicted_tf,index=range(y_test.AQI.count()));  \n        tf_data_y = pd.DataFrame(y_test_np, index=range(y_test_np.size)); \n        #tf_data_pred[\"y_test\"] = y_test\n        tf_data_merged = tf_data_pred.merge(tf_data_y,on=tf_data_pred.index,how=\"left\")\n        tf_data_merged[\"key_0\"]=y_test.index\n        tf_data_merged.columns =[\"Date\",\"Predicted_Y\",\"Test_Y\"]\n\n        #you can change the size of the graphs that has been showed below\n        sns.set(rc={'figure.figsize':(8,8)})\n        \n        #plotting the predictions over y_test data\n        sns.lineplot(data= tf_data_merged, x=\"Date\", y=\"Predicted_Y\",label = \"Predicted AQI\")\n        sns.lineplot(data= tf_data_merged, x=\"Date\", y=\"Test_Y\", label = \"AQI\")\n        plt.xticks(rotation=90)\n        plt.legend()\n        plt.show()\n        \n        #score calculation\n        R2 = r2_score(y_test_np, np_predicted_tf, multioutput='variance_weighted')\n        print (\"Score = \" + str(R2))\n        \n        # root of mean squared error calculation\n        def rmse(predictions, targets):\n            return np.sqrt(((predictions - targets) ** 2).mean())\n        \n        print(\"--------------------------------\")\n        print(\"Number of Epochs = \" + str(steps_to_see*(i+1)))\n        print(\"--------------------------------\")\n        print (\"Root of Mean Squared Error:\")\n        rmse_val = rmse(np_predicted_tf,y_test_np)\n        print (rmse(np_predicted_tf,y_test_np))\n        \n    explained_tf_df = pd.DataFrame({\"Score\":R2 ,\"Features used\": x_train.shape[1],\n                                    \"Epochs\": (epochs),\"rmse\":rmse_val}, index=range(1))\n    \n    # tf_data_merged is the compaison dataframe of y_test and y_pred values\n    # R2 is the score of the model \n    # rmse_val is the root of mean_squared_error \n    # explained_df_tf is the data frame of R2, rmse, epoch number, and the total features used in the model \n    return tf_data_merged, R2, rmse_val , explained_tf_df\n\n","ad35724a":"x = lasso_feature_x\nx_train = x.loc[\"2014-01\":\"2017-12\"]\ny_train = y.loc[\"2014-01\":\"2017-12\"]\n\nx_test = x.loc[\"2018-01\":\"2019-10\"]\ny_test = y.loc[\"2018-01\":\"2019-10\"]\ny_test.AQI.count()\n\ny_test_np = y_test.to_numpy()\ny_test_np.size","d9fc14b6":"tf_lasso_df, score_tf_lasso, rmse_val_tf_lasso, explained_tf_df1 = tf_nn (x_train,y_train,x_test,y_test,25000,100000)\n# First number 25000 represents at how many epochs to show the graph\n# The second 100000 represents how many epochs in total the model will run\n# the total_epoch number should be dividable by epoch_steps  ","aca4e954":"x = x_fstat\nx_train = x.loc[\"2014-01\":\"2017-12\"]\ny_train = y.loc[\"2014-01\":\"2017-12\"]\n\nx_test = x.loc[\"2018-01\":\"2019-10\"]\ny_test = y.loc[\"2018-01\":\"2019-10\"]","f882b7c6":"tf_f_feat_df, score_tf_f_feat, rmse_val_tf_f_feat, explained_tf_df2 = tf_nn (x_train,y_train,x_test,y_test,25000,100000)\n# First number 25000 represents at how many epochs to show the graph\n# The second 100000 represents how many epochs in total the model will run\n# the total_epoch number should be dividable by epoch_steps  ","9445e73c":"df_tf_scores = pd.DataFrame({\"Lasso_Feature_Selection\": score_tf_lasso,\n                        \"F_Feature_Selection\": score_tf_f_feat},index=[\"TensorFlow-NN-Reg\"])\ntwo_feature_sel_final = pd.concat([two_feature_sel_final,df_tf_scores])","2682cd55":"two_feature_sel_final","8d50f15f":"df = pd.read_csv(\"All Values Cleaned Shanghai1.csv\")\ndf.date = pd.to_datetime(df['date'], format=\"%Y-%m-%d\") \n\ndf_sorted = df.sort_values(by= [\"date\"])\nmonthly = df.groupby(df['date'].dt.strftime('%Y-%m'))['AQI'].mean()\ndf_month = pd.DataFrame(monthly)\ny = df_month.sort_values(by= [\"date\"])\nmonthly = df.groupby(df['date'].dt.strftime('%Y-%m'))['mintempC', 'uvIndex', 'DewPointC', \n                                                      'FeelsLikeC', 'HeatIndexC',\n       'WindChillC', 'WindGustKmph', 'cloudcover', 'humidity', 'precipMM',\n       'pressure', 'tempC', 'winddirDegree', 'windspeedKmph'].mean()\ndf_month = pd.DataFrame(monthly)\nx = df_month.sort_values(by= [\"date\"])\nx","e8699e15":"x_train = x.loc[\"2014-01\":\"2017-12\"]\ny_train = y.loc[\"2014-01\":\"2017-12\"]\n\nx_test = x.loc[\"2018-01\":\"2019-10\"]\ny_test = y.loc[\"2018-01\":\"2019-10\"]","c0b0aaa6":"tf_all_df, score_tf, rmse_val_tfv, explained_tf_df3 = tf_nn (x_train,y_train,x_test,y_test,25000,75000)\n# First number 25000 represents at how many epochs to show the graph\n# The second 100000 represents how many epochs in total the model will run\n# the total_epoch number should be dividable by epoch_steps  ","f6ca49f6":"df_tf_scores_all = pd.DataFrame({\"Lasso_Feature_Selection\": score_tf,\n                        \"F_Feature_Selection\": score_tf},index=[\"TensorFlw-All-Features\"])\ntwo_feature_sel_final_all = pd.concat([two_feature_sel_final,df_tf_scores_all])","f24be960":"two_feature_sel_final_all","dfac06b2":"explained_tf_df = pd.concat([explained_tf_df2,explained_tf_df1,explained_tf_df3]); \nexplained_tf_df.index = [\"Lasso_Feature_selection\", \"F_Feature_Selection\", \"All_Features\"]","36bc17a4":"explained_tf_df","4555f16d":"x_train = x.loc[\"2014-01\":\"2019-09\"]\ny_train = y.loc[\"2014-01\":\"2019-09\"]\n\nx_test = x.loc[\"2019-10\":\"2020-12\"]\ny_test = y.loc[\"2019-10\":\"2020-12\"]\n\ntf_all_df_cov, score_tf_cov, rmse_val_tfv_cov, explained_tf_df3_cov = tf_nn (x_train,y_train,x_test,y_test,25000,100000)\n\nexplained_tf_df = pd.concat([explained_tf_df2,explained_tf_df1,explained_tf_df3,explained_tf_df3_cov]); \nexplained_tf_df.index = [\"Lasso_Feature_selection\", \"F_Feature_Selection\", \"All_Features\", \"All feats with Covid timeline\"]","4ef08f5c":"explained_tf_df","74154074":"### Below we will define a NN engine which will help us analyze the steps virtually and then compare the results among each other, also will save us same space in our analysis","9b165ba1":"# Explaining the Data, and checking the COVID-19 Effect on AQI","c859420c":"After selecting which features to be used, we have used 4 different regression algorithms for predictions\n\n1) Linear Regression\n\n2) Cross Validation\n\n3) Ridge Regression\n\n4) Lasso Regression","4bfd5204":"### The last row TensorFlw-All_features means we didn't do any feature selection but feed all the features to the model\n### Since the NN algorithm makes the bias of the unusefull features close to zero, we don't have to do the feature selection ourselves. \n#### After 1000 epochs we have similar results as the other machine learning algorithms. ","fd577319":"Yeah we did the right thing, the covid clearly changed the air quality :)","ef363b0a":"## 4.2  F Statistics Features","b787aa34":"# Let's change the data type of \"date\" column to a datetime data type","d76b6f79":"## 2 Predictions","754cba7a":"# TENSORFLOW","54f5e653":"### We have both used daily analysis and monthly analysis. \n#### Monthly mean predictions has worked much better on seeing the effects of the weather conditions on AQI\n#### Therefore we will only show below the monthly results of the model on the conclusion","d98321d2":"# According to both feature selection models the fallowing weather conditions has the highest impact on the AQI \n\n#### 1) Minimum Temp C\n\n#### 2) Feels Like C \n\n#### 3) UV Index : \nThe ultraviolet index, or UV index, is an international standard measurement of the strength of sunburn-producing ultraviolet (UV) radiation at a particular place and time.\n\n#### 4) DewPointC : \nRepresents the temperature to which air would have to be cooled to reach a level of moisture saturation. When it reaches the dew point, droplets of water, or dew, begin to form on solid objects like grass and cars.\n\n#### 5) Heat Index C : \nrepresents how hot the temperature actually feels when humidity is considered. The more humid the air is, the less perspiration is able to evaporate, which cripples the human body\u2019s cooling system and makes it feel hotter when it\u2019s humid outside.\n\n#### 6) Wind Chill C : \nAlso known as the \u201cfeels-like\u201d temperature, wind chill represents how cold the weather feels on human skin when the chilling effect of the wind is taken into consideration.","5bf3d3d6":"## Regression Models","a8e67a27":"To fill the missing values we will not use the full mean value of the column but rather use 10 days interval. Because the values change too much through out the year and the mean of the of whole year would not be a good fit for filling the data","9dceb4f9":"# Daily Predictions","1931bdc2":"### 3.1 F Statistics Feature Selections","af50a3b4":"# Handling the missing values","71dc6a0b":"### 4.1 LASSO FEATURE SELECTED","95f784c7":"## 4. Monthly Predictions","40eedb9b":"We have used 2 different feature selection methods \n\n1) Lasso Feature Selection \n\n2) F Feature Selection\n\nThe Features selected to be used in both regression models are as below:","ccee2cfb":"# Below we will run the NN model with full features, without eliminating any","538caa75":"###  2.1 Lasso Selection Features\n","0af73174":"### 3.2 Lasso Feature Selection","155e2d3d":"As we have seen on mainly from the start of COVID-19, due to lockdowns the AQI index has been effected on 2020, therefore we will not use the data of COVID-19 period which would effect our prediction model.\n","62cdf308":"To calculate the Air Quality Index we should start calculating the sub index values of the responsible pollution values. The highest index value is accepted to be AQI.","d4fe82db":"### 2.2 F Statistics Features","f5e0c048":"### An important Note!\n#### The F_Feature_selection has been run 500 times, which makes a total of 500*500 = 250.000 which is a lot\n#### For the first 3 run which makes 8*500 = 4000 steps, Lasso feature selection method works better for determining the variables.\n#### Lasso_feature_selection had 9 different features\n#### F_Selected_feature had 7 different features","3f4d0411":"### 1.1 Feature Selection Using F Statistics","05e13395":"## Decision Tree Reg","374044d0":"# 3. MONTHLY ANALYSIS","2531b94a":"Gather all the data for cross check to see the values in each different regression models","c2806a58":"There is a big gap for the o3 measurements data therefore we cannot use the 10 day interval anymore for the o3. Instead we will use the SimpleImputer from Sklearn to fit the missing data for only o3 ","401fda20":"We will also write the cleaned data to a new CSV file to be used on further analysis. Such as if we want to add another variable to make our model better, like energy consumption. But I couldn't get the energy consumptions data yet. \n(the main reason is all the energy consumption datas of Shanghai are paid data)","a6934679":"## 1 - Feature Selection","c68d7c9e":"# Finally lets check if we did the right to to not to include covid time period to our model","c81e87c0":"### 1.2 Lasso Feature Selection","ae91950a":"# Calculating the Sub_Index Values for the air pollution and calculating the AQI","649030a3":"For this part of the analysis we will use two seperate methods and try to see the difference between two seperate methods.\n\n- Lasso Feature Selection\n- F Statistics","82a0c557":"## Feature Selections","fd2b6f89":"Daily Predictions has small score levels. Due to the difficulty of the prediction we can try to predictions monthly which can have a much higher prediction score"}}