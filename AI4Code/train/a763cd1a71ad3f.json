{"cell_type":{"19e38051":"code","84704dbd":"code","3ae75558":"code","28f27b6c":"code","d662b85a":"code","9f721df5":"code","60d9c02c":"code","1c971a19":"code","20f3af1d":"code","f345c45a":"code","e3433925":"markdown","84939e99":"markdown","69278cbe":"markdown"},"source":{"19e38051":"import os\nimport glob\nimport random\n\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport tqdm.notebook as tqdm\n\n%matplotlib inline\ntqdm.tqdm.pandas()\n\n# %pip install keras-ocr\n# import keras_ocr\n# detector = keras_ocr.detection.Detector()","84704dbd":"images = sorted(glob.glob('..\/input\/iam-handwritten-forms-dataset\/data\/*\/*.png'))","3ae75558":"def crop_and_split(img):\n    if isinstance(img, str):\n        img = cv2.imread(img)\n        assert img is not None\n    \n    img = img[:, 200:]\n    \n    prt = img[:600]\n    hand = img[650:2500]\n    return hand, prt\n\nimg = random.choice(images)\nhand, prt = crop_and_split(img)\n_, (ax1, ax2) = plt.subplots(1, 2)\nax1.imshow(hand)\nax2.imshow(prt)\nplt.show()\nplt.close()","28f27b6c":"hand = cv2.cvtColor(hand, cv2.COLOR_BGR2GRAY)\n_, th = cv2.threshold(hand, 0, 255 , cv2.THRESH_BINARY + cv2.THRESH_OTSU)\ncontours, _ = cv2.findContours(th, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE) \nchars = cv2.drawContours(np.zeros(hand.shape, dtype='uint8'), contours, -1, (1), -1)\nplt.imshow(chars, cmap='gray')\nplt.show()\nplt.close()","d662b85a":"from scipy.spatial.distance import cdist\n\n\ndef crop_textblocks(images, detector):\n    hands, prts = zip(*[crop_and_split(img) for img in images])\n    hands = [keras_ocr.tools.warpBox(img, box) for (img, boxes) in zip(hands, detector.detect(hands)) for box in boxes]\n    prts = [keras_ocr.tools.warpBox(img, box) for (img, boxes) in zip(prts, detector.detect(prts)) for box in boxes]\n    crops = prts + hands\n    labels = ['prt' for _ in prts] + ['hand' for _ in hands]\n    return crops, labels\n\n\ndef order_points(pts):\n    \"\"\"https:\/\/www.pyimagesearch.com\/2016\/03\/21\/ordering-coordinates-clockwise-with-python-and-opencv\/\"\"\"\n    x = pts[np.argsort(pts[:, 0]), :]\n\n    left = x[:2, :]\n    right = x[2:, :]\n\n    left = left[np.argsort(left[:, 1]), :]\n    (tl, bl) = left\n\n    d = cdist(tl[np.newaxis], right, \"euclidean\")[0]\n    (br, tr) = right[np.argsort(d)[::-1], :]\n    return np.array([tl, tr, br, bl], dtype=\"float32\")\n\n\ndef find_chars(img):\n    if len(img.shape) == 3:\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    \n    _, th = cv2.threshold(img, 0, 255 , cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    contours, _ = cv2.findContours(th, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n    contours = [c[:, 0, :] for c in contours]\n    return contours\n\n\ndef crop_min_area_rect(rect, img):\n    box = cv2.boxPoints(rect)\n    box = np.int0(box)\n\n    # Reorder points to [tl, tr, br, bl]\n    box = order_points(box).astype(\"float32\")\n    w = int(np.linalg.norm(box[0] - box[1]))\n    h = int(np.linalg.norm(box[0] - box[3]))\n    \n    dst_pts = np.array([\n        [0, 0], [w-1, 0], [w-1, h-1], [0, h-1]], dtype=\"float32\")\n\n    M = cv2.getPerspectiveTransform(box, dst_pts)\n\n    # directly warp the rotated rectangle to get the straightened rectangle\n    warped = cv2.warpPerspective(img, M, (w, h))\n    return warped\n\n\ndef extract_lines(img, warp_crop=False, padx=4, pady=4):\n    if len(img.shape) == 3:\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    \n    # Finding text lines\n    blur = cv2.GaussianBlur(img, (3, 3), 0)\n    _, th = cv2.threshold(blur, 0, 255 , cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n    block_kern_shape = (100, 1)\n    block_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, block_kern_shape)\n    dilation = cv2.dilate(th, block_kernel, iterations=1)\n    contours, _ = cv2.findContours(dilation, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n    \n    crops, chars, bboxes = [], [], []\n    for ctr in contours:\n        box = cv2.boundingRect(ctr)\n        if warp_crop:\n            rect = cv2.minAreaRect(ctr)\n            crop = crop_min_area_rect(rect, img)\n            if is_unwanted(crop.shape[0], crop.shape[1]):\n                continue\n        else:\n            x, y, w, h = box\n            if is_unwanted(h, w):\n                continue\n            crop = img[y-pady:y+h+pady, x-padx:x+w+padx]\n\n        char = find_chars(crop)\n        if len(char) < 3:\n            continue\n\n        crops.append(crop)\n        chars.append(char)\n        bboxes.append(box)\n\n    # crops, chars, bboxes = filter_crops(crops, chars, bboxes)\n    return crops, chars, bboxes\n\n\ndef is_unwanted(h, w):\n    return any([h < 10, w < 200, w\/h < 1, w\/h > 100])","9f721df5":"img = prt.copy()\ncrops, contours, bboxes = extract_lines(img)\n\nk = int(np.ceil(len(crops) \/ 4))\n_, axs = plt.subplots(k, 4, figsize=(25, 5))\nfor crop, box, ax in zip(crops, bboxes, axs.flatten()):\n    ax.imshow(crop, cmap='gray', vmin=0, vmax=255)\n    ax.set_title(box)\n    \nplt.show()\n\n_, axs = plt.subplots(k, 4, figsize=(25, 5))\nfor crop, ctrs, ax in zip(crops, contours, axs.flatten()):\n    _img = cv2.drawContours(np.zeros(crop.shape[:2], dtype='uint8'), ctrs, -1, (255), -1)\n    ax.imshow(_img, cmap='gray', vmin=0, vmax=255)\n    \nplt.show()\n\n_, axs = plt.subplots(k, 4, figsize=(25, 5))\nfor box, ax in zip(bboxes, axs.flatten()):\n    x, y, w, h = box\n    ax.imshow(img[y:y+h, x:x+w])\n    \nplt.show()\n\n_img = img.copy()\nfor (x, y, w, h) in bboxes:\n    _img = cv2.rectangle(_img, (x, y), (x+w, y+h), 2)\nplt.imshow(_img)\nplt.show()","60d9c02c":"def compute_ulp(img, contours):\n    if len(img.shape) == 3:\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    h, w = img.shape\n\n    chars = cv2.drawContours(np.zeros(img.shape, dtype='uint8'), contours, -1, (1), -1)\n    mask = chars == 0\n    \n    ulp = []\n    for i in range(w):\n        y = np.zeros(h, dtype='uint8')\n        m = mask[:, i]\n        if m.any():\n            idxs = np.arange(0, h)\n            y[idxs[m].max()] = 1\n            y[idxs[m].min()] = 1\n        ulp.append(y)\n    \n    ulp = np.array(ulp).T\n    return ulp\n\nplt.figure(figsize=(10, 2))\nulp = compute_ulp(crops[0], contours[0])    \nplt.imshow(ulp, cmap='gray', vmin=0, vmax=1)\nplt.show()\n\nplt.figure(figsize=(5, 5))\nhist = ulp.sum(axis=1)\nplt.plot(hist, np.arange(0, len(hist)))\nplt.show()\nplt.close()","1c971a19":"def get_peak_idxs(hist):\n    midpoint_idx = int(round(len(hist) \/ 2))\n    \n    lower_hist = hist[:midpoint_idx]\n    lower_peak_val = lower_hist.max()\n    lower_peak_idx = np.where(lower_hist == lower_peak_val)[0].min()\n    \n    upper_hist = hist[midpoint_idx:]\n    upper_peak_val = upper_hist.max()\n    upper_peak_idx = np.where(upper_hist == upper_peak_val)[0].max() + midpoint_idx\n    \n    assert upper_peak_idx > lower_peak_idx\n    return lower_peak_idx, upper_peak_idx\n\n\ndef extract_features(hist):\n    hist = hist.astype('float32')\n    lower_peak_idx, upper_peak_idx = get_peak_idxs(hist)\n    ascender_w = len(hist) - upper_peak_idx\n    desender_w = lower_peak_idx\n    main_body_w = upper_peak_idx - lower_peak_idx\n    \n    f_i = ascender_w \/ main_body_w\n    f_ii = desender_w \/ main_body_w\n    \n    _max = hist.max()\n    f_iii = hist.sum() \/ _max if _max > 0 else 0\n    return [f_i, f_ii, f_iii]","20f3af1d":"def process_crop(crop, chars):\n    ulp = compute_ulp(crop, chars)\n    hist = ulp.sum(axis=1)\n    features = extract_features(hist)\n    return features\n\n\ndef process_file(path):\n    hand, prt = crop_and_split(path)\n    \n    features, labels = [], []\n    \n    crops, chars, _ = extract_lines(hand)\n    for crop, ctrs in zip(crops, chars):\n        features.append(process_crop(crop, ctrs))\n        labels.append(\"hand\")\n        \n    crops, chars, _ = extract_lines(prt)\n    for crop, ctrs in zip(crops, chars):\n        features.append(process_crop(crop, ctrs))\n        labels.append(\"prt\")\n\n    return features, labels\n\n\nfeatures, labels = [], []\nfor img in tqdm.tqdm(images):\n    f, l = process_file(img)\n    features += f\n    labels += l\n    \nfeatures = np.array(features)\nlabels = np.array(labels)","f345c45a":"from imblearn.pipeline import Pipeline\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegressionCV as LRCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n\nx = features.copy()\ny = labels.copy()\n\nlabel_encoder = LabelEncoder().fit(y)\ny = label_encoder.transform(y)\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, shuffle=True, test_size=0.2, stratify=y)\n\npipe = Pipeline([\n    ('rus', RandomUnderSampler(random_state=0)),\n    ('scaler', StandardScaler()),\n    # ('sampler', SMOTE(random_state=0)),\n    ('clf', LRCV(random_state=0, class_weight='balanced')),\n]).fit(x_train, y_train)\n\nprint(classification_report(\n    y_pred=pipe.predict(x_train), y_true=y_train, labels=label_encoder.transform(label_encoder.classes_), target_names=label_encoder.classes_))","e3433925":"My re-implementation of Discrimination of machine-printed from handwritten text using simple structural characteristicsDiscrimination of machine-printed from handwritten text using simple structural characteristics\nhttps:\/\/ieeexplore.ieee.org\/document\/1334152https:\/\/ieeexplore.ieee.org\/document\/1334152","84939e99":"# Preprocessing","69278cbe":"# Model"}}