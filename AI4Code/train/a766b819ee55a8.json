{"cell_type":{"267d7240":"code","6d6e5089":"code","583d74d2":"code","a72ce318":"code","8df7f7e9":"code","0263b997":"code","0c8fe404":"code","823fb658":"code","f8b61ea4":"code","5e990d4e":"code","ef472697":"code","78a651c1":"code","358daf4d":"code","075d15de":"code","c28ff1f1":"code","321369ae":"code","bb61279a":"code","b61526c6":"code","e7b3c5b9":"code","d59ded6b":"code","81346b13":"code","3eea8e01":"code","69b64317":"code","e088fd5d":"markdown","3010367c":"markdown","1af2bfe8":"markdown","5fbe0c58":"markdown","594d9edb":"markdown","bb9b8865":"markdown","3be9d76a":"markdown","6f377a0d":"markdown","41119699":"markdown","123c03fd":"markdown","056d3e1d":"markdown","79a55db4":"markdown","8a0d36d4":"markdown","2942dbe6":"markdown","582c88a7":"markdown","c19a7d79":"markdown","e30ebfe0":"markdown","11d051cc":"markdown","1f1e77a1":"markdown","d7ad2a85":"markdown","f87d0ca7":"markdown"},"source":{"267d7240":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","6d6e5089":"data = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')","583d74d2":"data.head()","a72ce318":"data.shape","8df7f7e9":"sns.heatmap(data.isnull(),cmap = 'magma' )","0263b997":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndata['diagnosis'] = le.fit_transform(data.diagnosis) ","0c8fe404":"data.shape","823fb658":"data.drop('Unnamed: 32', axis = 1, inplace = True)","f8b61ea4":"cor = data.corr()\nsns.heatmap(cor, annot = False, center= 0)\nplt.show()","5e990d4e":"from scipy.stats import zscore\nz_scores = zscore(data)\n\nabs_z_scores = np.abs(z_scores)\nfiltered_entries = (abs_z_scores < 3).all(axis=1)\nnew_data = data[filtered_entries]\ndf = pd.DataFrame(new_data)\ndf","ef472697":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndef calculate_vif(X):\n\n    # Calculating VIF\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n\n    return(vif)","78a651c1":"X = df.iloc[:,:]\ncalculate_vif(X)","358daf4d":"drop_cols = ['perimeter_mean','area_mean','perimeter_se','area_se','perimeter_worst','area_worst','id']","075d15de":"df.drop(drop_cols, axis = 1, inplace = True)","c28ff1f1":"df.head()","321369ae":"X = df.drop('diagnosis', axis = 1)                # Defining X and y variables\ny = df['diagnosis']","bb61279a":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","b61526c6":"# Splitting the data into training and testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)","e7b3c5b9":"lm = LogisticRegression()\nlm.fit(X_train, y_train)\ny_pred = lm.predict(X_test)\n\nlm_accuracy = round(lm.score(X_test, y_test) * 100, 2)\nprint('Test Accuracy: ', lm_accuracy)","d59ded6b":"knn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n\nknn_accuracy = round(knn.score(X_test, y_test) * 100, 2)\nprint('Test Accuracy: ', knn_accuracy)","81346b13":"dt = DecisionTreeClassifier(max_depth = 5)\ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)\n\ndt_accuracy = round(dt.score(X_test, y_test) * 100, 2)\nprint('Test Accuracy: ', dt_accuracy)","3eea8e01":"rf = RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\n\nrf_accuracy = round(rf.score(X_test, y_test) * 100, 2)\nprint('Test Accuracy: ', rf_accuracy)","69b64317":"xgb = XGBClassifier()\nxgb.fit(X_train, y_train)\ny_pred = xgb.predict(X_test)\n\nxgb_accuracy = round(xgb.score(X_test, y_test) * 100, 2)\nprint('Test Accuracy: ', xgb_accuracy)","e088fd5d":"**Prediction using KNN**","3010367c":"**Prediction using Decision Tree**","1af2bfe8":"**Checking for missing values**","5fbe0c58":"**Variance Inflation Method**\nVIF is a very popular method to detect the multicollinearity which is one of the assumption under regression model. \nThe VIF estimates how much the variance of a regression coefficient is inflated due to multicollinearity in the model.\n","594d9edb":"**Prediction using Random Forest**","bb9b8865":"**Checking for Outliers using Zscore method**","3be9d76a":"![cancer.png](attachment:cancer.png)","6f377a0d":"**Multicollinearity**\nWhen features are correlated then the coefficient of Determination for the defined model increases which affects the prediction accuracy. \nThis problem is treated as a critical problem because it can mislead the prediction i.e a person having cancer will be predicted as negative which is a serious matter. \nSo here we check if some features are  correlated. ","41119699":"Normal Distribution defines the law of nature. Though there are many techniques to detect \"wilds\" in the dataset. Here I am using Zscore method:\n\nWhere Z is the standard normal variate. And Zscore is defined as (X-mean)\/SD. \nThe area under the bell-shaped curve of Standard normal distribution lies within 3*Sigma limit. \nThe values which are beyond the limits are treated as outliers. \n","123c03fd":"![standard%20normal%20distribution.jpg](attachment:standard%20normal%20distribution.jpg)","056d3e1d":"**Prediction using Logistic Regression**","79a55db4":"**Read Data using pandas**","8a0d36d4":"# <span style=\"color:maroon\"> Breast Cancer Prediction<\/span>","2942dbe6":"**Conclusion**\nThere is no perfect algorithm for a dataset. For the cancer prediction problem (a classification problem), XGBoost is found to give better accuracy on test data. \nThe model is built after removing the outliers and correlated data. \n\n**Any Suggestions are most Welcome!**\n","582c88a7":"**Sufficiency**\n\nWe try to keep sufficient features in the model which is one of characteristics of a good estimator. \nSince, area and perimeter can be calculated by using radius we can drop these features from the model. ","c19a7d79":"**Prediction using XGBoost**","e30ebfe0":"**Converting Categorical to Numerical Data**","11d051cc":"**Import libraries**","1f1e77a1":"**Finding correlation between features**","d7ad2a85":"Luckily we don't have missing values in it!!!!","f87d0ca7":"**Import necessary libraries for the prediction**"}}