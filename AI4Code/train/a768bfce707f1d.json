{"cell_type":{"94baf970":"code","8ceff4a2":"code","614c3664":"code","90807e63":"code","11f79d0e":"code","3f66d69d":"code","b19b6d00":"code","f28c9e55":"code","9e2e6e5c":"code","3fb85347":"code","a8632346":"code","b6aed36b":"code","15fc5000":"code","83ae0f7f":"code","b548d4bc":"code","ead848eb":"code","8e8349c9":"code","679051d5":"code","e3f83697":"code","87dc479c":"code","46864052":"code","d16066a8":"code","0e28e411":"code","f677a3cf":"code","18cb2a91":"code","45e1e779":"code","5b5968c3":"code","0ebfc5a3":"code","d6b1e495":"code","4712c71c":"code","54ade88b":"code","a4f6b745":"code","b30602ac":"code","1acf107a":"code","d9b97ec7":"code","4be4f17a":"code","44e68fb0":"code","62082eae":"code","610768b6":"code","42d5888a":"code","43078ec6":"code","e87b8dfd":"code","786db177":"code","845376d8":"code","5a500dce":"code","19846541":"code","98ea36c8":"code","1682572b":"code","24e99d28":"code","6d66998e":"code","53a79197":"code","fdebda91":"code","cc4f2e20":"code","b870b8ef":"code","241e7564":"code","a354251f":"code","cdd7d7fd":"markdown","14078983":"markdown","09247696":"markdown","32041c51":"markdown","75b3ff79":"markdown","d814ceba":"markdown","605622e8":"markdown","69e5b110":"markdown","54cc255d":"markdown","6cde4127":"markdown","8d1274ad":"markdown","083bc464":"markdown","139b4543":"markdown","afe8663d":"markdown"},"source":{"94baf970":"# The libraries that we will need throughout the project\nimport pandas as pd\nimport numpy as np\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom lightgbm import LGBMRegressor\nfrom sklearn.preprocessing import RobustScaler, LabelEncoder\nfrom sklearn.ensemble import RandomForestRegressor, VotingRegressor\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, cross_validate, RandomizedSearchCV\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\n# for making output full : \npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.width', 500)\npd.set_option('display.expand_frame_repr', False)","8ceff4a2":"# Let's read our data\ndf = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")\ndf.head()","614c3664":"df.describe().T","90807e63":"# Is there any missing value in the data ?\ndf.isnull().values.any() \n# Since  output is 'True' that means we have missing value in our data","11f79d0e":"df.isnull().sum() # How many missing value do we have in eavh column ? \n# It seems we have missing values only in target column","3f66d69d":"# In this data we have a few number of observation only 322 and 59 of it is missing, Only 322 - 59 = 263 observation is avaliable for modelling\n# So actually since missing value is only in our target column,  we can take that missing value part and, after building model with 263,we can predict this 59\n# After predicting them, that 59 part will also have target values, so in that case we can concatenate that part to the data and remodel again with whole 322\n# In that way we will not loose our 59 observation becasue as we all  know number of data is important for the ML models\n# So in the end I will build new model with whole 322 observations. I hope it will boost the prediction score, since we will provide more data.\n\n# Let's take that part\ndf_nan = df.loc[ df.isnull().any(axis=1), :]\ndf_nan.shape\n# Done !","b19b6d00":"# Since we have already taken that Nan part, let's drop that part from data \ndf.dropna(inplace=True)\ndf.isnull().values.any() # Now false","f28c9e55":"# Let's see whole picture of data with that function\ndef check_df(dataframe, head=5):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(head))\n\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\ncheck_df(df)","9e2e6e5c":"# In that point I want to make my whole column names upper, just for convenience\ndf.columns = [col.upper() for col in df.columns]\ndf.columns","3fb85347":"# This function will help us to determine numeric, categoric and seems as a catgorical but due to this column has a lot of class we consider it as cardinal\n# columns \ndef grab_col_names(dataframe, cat_th=10, car_th=20):\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n\n    cat_cols = cat_cols + num_but_cat\n\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    return cat_cols, num_cols, cat_but_car\n# Let's use that functon \ncat_cols, num_cols, cat_but_car = grab_col_names(df)","a8632346":"cat_cols","b6aed36b":"cat_but_car # We don't have in that case","15fc5000":"num_cols # We have plenty of numeric columns","83ae0f7f":"# Let's investigate categoric columns\ndef cat_summary(dataframe, col_name, ratio=True, plot=False):\n    if ratio:\n        print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n                            \"Ratio\": dataframe[col_name].value_counts() \/ len(dataframe)}))\n        print(\"##########################################\")\n\n    else:\n        print(pd.DataFrame({col_name: dataframe[col_name].value_counts()}))\n        print(\"##########################################\")\n    if plot:\n        sns.countplot(x=dataframe[col_name], data=dataframe)\n        plt.show()\nfor col in cat_cols:\n    cat_summary(df, col, plot = True)","b548d4bc":"# Let's analyse categoric columns in terms of target column 'SALARY'\ndef target_summary_with_cat(dataframe, target, categorical_col):\n\n    print(pd.DataFrame({\"TARGET_MEAN\": dataframe.groupby(categorical_col)[target].mean(),\n                        'COUNT': dataframe.groupby(categorical_col)[target].count()}),\n                         end=\"\\n\\n\\n\")\nfor col in cat_cols:\n    target_summary_with_cat(df, 'SALARY', col) ","ead848eb":"# Do we have any outliers in our numerical columns ?\ndef outlier_thresholds(dataframe, col_name, q1=0.05, q3=0.95):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\ndef check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False\n\n# Since all columns return 'False' that means I do not have any outlier in my numeric columns\nfor col in num_cols:\n    print(col, check_outlier(df, col))","8e8349c9":"# Since it's regression task we can find the columns that are highly correlated with target column 'SALARY'. It will help us in 'Feature Engineering' part\n# We have to give importance to highly correlated features while generating new features. Because this highly correlated columns already have ability to ex-\n# plain varinace of 'SALARY'. If we generate new features from these columns actually we can pass this ability.\n# So let's find what are they\ndef target_correlation_matrix(dataframe, target, corr_th=0.50, plot=False):\n    corr = dataframe.corr()\n    try:\n        filter = np.abs(corr[target]) > corr_th\n        corr_features = corr.columns[filter].tolist()\n        if plot:\n            sns.clustermap(dataframe[corr_features].corr(), annot=True, fmt=\".2f\")\n            plt.show()\n        corr_features.remove(target)\n        return corr_features\n    except:\n        print(f\"There is no column that have more than {corr_th} correlation value with target {target} column. You may decrease threshold value\")\ntarget_correlation_matrix(df, target=\"SALARY\",  corr_th=0.50, plot = True)\n# This columns ['CATBAT', 'CHITS', 'CHMRUN', 'CRUNS', 'CRBI'] have more than '0.5' correlation valu with 'SALARY' column","679051d5":"sns.distplot(df['SALARY']); # A bit right skew distribution","e3f83697":"# Outliers in SALARY column\nsns.boxplot(x=df[\"SALARY\"])\nplt.show()\n# As we see there are some outliers let's try to treat them, since it's regression problem outliers in target column may not allow us to reach high \n# prediction scores","87dc479c":"# Since we have outliers only in upper part we find only upper bound\nq1 = df['SALARY'].quantile(0.25) # 25 and 75 are default value for q1 and q3 in boxplot method\nq3 = df['SALARY'].quantile(0.75)\niqr = q3-q1\nup_bound = q3 + 1.5 * iqr\ndf.loc[ df['SALARY'] > up_bound, : ]","46864052":"# Let's change all outlier with up_bound\ndf.loc[ df['SALARY'] > up_bound, 'SALARY'] = up_bound\n# Again seeing box plot\nsns.boxplot(x=df[\"SALARY\"])\nplt.show()\n# Now we don't have any outliers in SALARY column","d16066a8":"# As we all know 'feature engineering part' means generating new features crucial getting high scores\n# Let's generate some new features\n\n# Years column represents \"player's playing time in major league (years)\". So since this data is about sport, actually by using that column we can generate\n# new column under the name 'NEW_EXPERIENCE'. Let's perform that\ndf.loc[(df['YEARS'] <= 5), 'NEW_EXPERIENCE'] = 'FRESH'\ndf.loc[(df['YEARS'] > 5) & (df['YEARS'] <= 10), 'NEW_EXPERIENCE'] = 'STARTER'\ndf.loc[(df['YEARS'] > 10) & (df['YEARS'] <= 15), 'NEW_EXPERIENCE'] = 'AVERAGE'\ndf.loc[(df['YEARS'] > 15) & (df['YEARS'] <= 20), 'NEW_EXPERIENCE'] = 'EXPERINECED'\ndf.loc[(df['YEARS'] > 20), 'NEW_EXPERIENCE'] = 'VETERAN'\ndf.head()","0e28e411":"# Since 'ATBAT' column represents number of total hit, and 'HITS' column represents number of successful hits actually we can generate new column that \n# represents the ratio of successful hit :\ndf['NEW_HIT_RATIO'] = df['HITS'] \/ df['ATBAT']","f677a3cf":"# We see the same scenario in ('HMRUN','RUNS'), ('CHITS','CATBAT') and ('CRUNS', 'CRUNS')  columns, let's generate ratop also from these columns\ndf['NEW_RUN_RATIO'] = df['HMRUN'] \/ df['RUNS']\ndf['NEW_CHIT_RATIO'] = df['CHITS'] \/ df['CATBAT']\ndf['NEW_CRUN_RATIO'] = df['CHMRUN'] \/ df['CRUNS']","18cb2a91":"# Since there was '0' in one observation in df['RUNS'] column while generating df['NEW_RUN_RATIO'] since dividing by 0 is undefined it made \n# df['NEW_RUN_RATIO'] ==> NaN in that part, let's replace that value with zero \ndf['NEW_RUN_RATIO'].isnull().sum() # there is one NaN\ndf['NEW_RUN_RATIO'].fillna(0, inplace=True)\ndf['NEW_RUN_RATIO'].isnull().sum() # Now we have fixed it ","45e1e779":"# Since we have said we have to give importance to these ['CATBAT', 'CHITS', 'CHMRUN', 'CRUNS', 'CRBI'] columns during feature engineering part\n# Let's generate some meaningful features from these columns\n\n# New columns represents some average statistics of players per year \ndf['NEW_AVG_CATBAT'] = df['CATBAT'] \/ df['YEARS']\ndf['NEW_AVG_HITS'] = df['CHITS'] \/ df['YEARS']\ndf['NEW_AVG_HMRUN'] = df['CHMRUN'] \/ df['YEARS']\ndf['NEW_AVG_RUNS'] = df['CRUNS'] \/ df['YEARS']\ndf['NEW_AVG_RBI'] = df['CRBI'] \/ df['YEARS']\ndf['NEW_AVG_WALKS'] = df['CWALKS'] \/ df['YEARS']","5b5968c3":"# In that part we will encode categorical features\n# In binary encoding we will find the categorical features with two classes, and make their classes 0 and 1\n# So let's find the categorical features with two classes first\nbinary_cols = [col for col in df.columns if df[col].dtype not in [int, float]\n               and df[col].nunique() == 2]\nbinary_cols ","0ebfc5a3":"# Encoding their clases == making 0 and 1\ndef label_encoder(dataframe, binary_col):\n    labelencoder = LabelEncoder()\n    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n    return dataframe\nfor col in binary_cols:\n    df = label_encoder(df, col)\ndf[binary_cols].head() # Ready","d6b1e495":"# We have to use one-hot encoding for making categoric columns numeric, let's perform that\nohe_cols = [col for col in df.columns if 10 >= df[col].nunique() > 2] # First let's select which columns have to be one-hot encoded \nohe_cols","4712c71c":"# Performing One-Hot \ndef one_hot_encoder(dataframe, categorical_cols, drop_first=False): \n    # Since I will use tree based models they do not suffer 'Dummy variable trap', therefore drop_first=False \n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n    return dataframe\ndf = one_hot_encoder(df, ohe_cols)\ndf.head()","54ade88b":"# I will not do feature scaling because I will use tree-based methods\n# So our data preprocessing part has finished ","a4f6b745":"# Let's select our independent and dependent columns\ny = df[\"SALARY\"]  \nX = df.drop([\"SALARY\"], axis=1) ","b30602ac":"# First let's build see the result of model without hyperparameter tuning\nrf_model = RandomForestRegressor(n_jobs=-1)","1acf107a":"rmse = np.mean(np.sqrt(-cross_val_score(rf_model, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\nprint(f\"Before hyperparameter tuning : RMSE = {round(rmse, 4)} ==> {rf_model} \")","d9b97ec7":"# Since we will use Randomized Search I give wide range for hyperparameters\nrf_random_params = {\"max_depth\": [int(x) for x in np.linspace(start=8, stop=30, num=19)] + [None],\n             \"max_features\": [int(x) for x in np.linspace(start=5, stop=20, num=10)] + ['auto'],\n             \"min_samples_split\": [int(x) for x in np.linspace(start=2, stop=20, num=19)],\n             \"n_estimators\": [int(x) for x in np.linspace(start=100, stop=2000, num=20)],\n             \"max_samples\" : [ random.uniform(0.5,1) for i in range(15)]}","4be4f17a":"rf_random = RandomizedSearchCV(estimator=rf_model,\n                               param_distributions=rf_random_params,\n                               n_iter=100, # Selects randomly 100 combinations from given set of hyperparametrs(means from rf_random_params)\n                               cv=10, \n                               verbose=True,\n                               random_state=42,\n                               n_jobs=-1,\n                               scoring=\"neg_mean_squared_error\").fit(X, y)\n# It may take a bit of time :)","44e68fb0":"# Let's see best parameters\nrf_random.best_params_","62082eae":"# Let's use that best params to build model\nfinal_model_rf = rf_model.set_params(**rf_random.best_params_)\nnp.mean(np.sqrt(-cross_val_score(final_model_rf, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n# Before hyperparameter tuning it was \"228.6013\"","610768b6":"# Let's try also LGBMRegressor() model\nlgbm_model = LGBMRegressor()","42d5888a":"rmse = np.mean(np.sqrt(-cross_val_score(lgbm_model, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\nprint(f\"Before hyperparameter tuning : RMSE = {round(rmse, 4)} ==> {lgbm_model} \")","43078ec6":"# Let's tune the hyperparameters of LGBMRegressor() I will use GridSearchCV in that time\nlgbm_params = {\"learning_rate\": [0.01, 0.1, 0.001],\n               \"n_estimators\": [300, 500, 1500],\n               \"colsample_bytree\": [0.5, 0.7, 1]}\nlgbm_best_grid = GridSearchCV(lgbm_model,\n                            lgbm_params,\n                            cv=10, \n                            n_jobs=-1,\n                            verbose=True).fit(X, y) ","e87b8dfd":"lgbm_best_grid.best_params_","786db177":"final_model_lgbm = lgbm_model.set_params(**lgbm_best_grid.best_params_)\nnp.mean(np.sqrt(-cross_val_score(final_model_lgbm, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n# RandomForestRegressor was better than it so I will continue with RandomForestRegressor","845376d8":"df_nan.head()","5a500dce":"# As we talked above we will predict df_nan with our model\ndf_nan.columns = [col.upper() for col in df_nan.columns]\ndf_nan['SALARY'].head()","19846541":"# Since we are going to predict SALARY column let's remove that column from df_nan\ndf_nan.drop('SALARY', axis=1, inplace=True)","98ea36c8":"# Before predicting df_nan we have to preprocess it for the model(as we did for df)\ndf_nan.loc[(df_nan['YEARS'] <= 5), 'NEW_EXPERIENCE'] = 'FRESH'\ndf_nan.loc[(df_nan['YEARS'] > 5) & (df_nan['YEARS'] <= 10), 'NEW_EXPERIENCE'] = 'STARTER'\ndf_nan.loc[(df_nan['YEARS'] > 10) & (df_nan['YEARS'] <= 15), 'NEW_EXPERIENCE'] = 'AVERAGE'\ndf_nan.loc[(df_nan['YEARS'] > 15) & (df_nan['YEARS'] <= 20), 'NEW_EXPERIENCE'] = 'EXPERINECED'\ndf_nan.loc[(df_nan['YEARS'] > 20), 'NEW_EXPERIENCE'] = 'VETERAN'\ndf_nan['NEW_HIT_RATIO'] = df_nan['HITS'] \/ df_nan['ATBAT']\ndf_nan['NEW_RUN_RATIO'] = df_nan['HMRUN'] \/ df_nan['RUNS']\ndf_nan['NEW_RUN_RATIO'].fillna(0, inplace=True)\ndf_nan['NEW_CHIT_RATIO'] = df_nan['CHITS'] \/ df_nan['CATBAT']\ndf_nan['NEW_CRUN_RATIO'] = df_nan['CHMRUN'] \/ df_nan['CRUNS']\ndf_nan['NEW_AVG_CATBAT'] = df_nan['CATBAT'] \/ df_nan['YEARS']\ndf_nan['NEW_AVG_HITS'] = df_nan['CHITS'] \/ df_nan['YEARS']\ndf_nan['NEW_AVG_HMRUN'] = df_nan['CHMRUN'] \/ df_nan['YEARS']\ndf_nan['NEW_AVG_RUNS'] = df_nan['CRUNS'] \/ df_nan['YEARS']\ndf_nan['NEW_AVG_RBI'] = df_nan['CRBI'] \/ df_nan['YEARS']\ndf_nan['NEW_AVG_WALKS'] = df_nan['CWALKS'] \/ df_nan['YEARS']","1682572b":"binary_cols = [col for col in df_nan.columns if df_nan[col].dtype not in [int, float]\n               and df_nan[col].nunique() == 2]\nfor col in binary_cols:\n    df_nan = label_encoder(df_nan, col)\ndf_nan[binary_cols].head() ","24e99d28":"ohe_cols = [col for col in df_nan.columns if 10 >= df_nan[col].nunique() > 2]\ndf_nan = one_hot_encoder(df_nan, ohe_cols)","6d66998e":"# Now it's ready for prediction\nfinal_model_rf.fit(X, y)\ndf_nan['SALARY'] = final_model_rf.predict(df_nan)\ndf_nan['SALARY'].head() # We have successfully predicted NaN values for 'SALARY' column","53a79197":"# Now let's concatenate that part to the 'df' and perdorm modeling with 322 observations\ndf = pd.concat([df, df_nan], ignore_index=True)\ny = df[\"SALARY\"]  \nX = df.drop([\"SALARY\"], axis=1) ","fdebda91":"rf_random_params = {\"max_depth\": [int(x) for x in np.linspace(start=8, stop=30, num=19)] + [None],\n             \"max_features\": [int(x) for x in np.linspace(start=5, stop=20, num=10)] + ['auto'],\n             \"min_samples_split\": [int(x) for x in np.linspace(start=2, stop=20, num=19)],\n             \"n_estimators\": [int(x) for x in np.linspace(start=100, stop=2000, num=20)],\n             \"max_samples\" : [ random.uniform(0.5,1) for i in range(15)]}","cc4f2e20":"rf_random = RandomizedSearchCV(estimator=rf_model,\n                               param_distributions=rf_random_params,\n                               n_iter=100, \n                               cv=10, \n                               verbose=True,\n                               random_state=42,\n                               n_jobs=-1,\n                               scoring=\"neg_mean_squared_error\").fit(X, y)","b870b8ef":"# best parameters for new 'df'\nrf_random.best_params_","241e7564":"final_model_rf = rf_model.set_params(**rf_random.best_params_)\nnp.mean(np.sqrt(-cross_val_score(final_model_rf, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n# Our final RMSE is : 183.53070069833103","a354251f":"# Let's see important features\ndef plot_importance(model, features, num=len(X), save=False):\n    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})\n    plt.figure(figsize=(10, 10))\n    sns.set(font_scale=1)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n                                                                     ascending=False)[0:num])\n    plt.title('Features')\n    plt.tight_layout()\n    plt.show()\n    if save:\n        plt.savefig('importances.png')\n\n\nplot_importance(rf_model, X, num=20) # It's nice to see that our hand-generated 'NEW_...' features are also included to important features list","cdd7d7fd":"# **One-Hot encoding**","14078983":"# **Hyperparametr Tuning**\n# RandomizedSearchCV \n##### **Since we be able to search best values for important hyperparameters by using wide range in RandomizedSearchCV I will use that approach.** \n![image.png](attachment:5d8b49ad-e738-4e1e-8d6d-0a7aedcadaab.png)","09247696":"# **Binary encoding**","32041c51":"# **Data Preprocessing**","75b3ff79":"# **Feature engineering part**","d814ceba":"# **Can we predict the salaries of the baseball players based on their career infromation?**\n* We are given 322 players' career information and their amount of salary. \n* Our challenge is building a Machine Learning model that predicts the amount of the players for the given career information.\n* Let's start the challenge !\n\n![image.png](attachment:999a4036-dc38-405b-8a4c-699952f78ad1.png)\n\n\n# **Dataset story**\n\nThis dataset was originally taken from the StatLib library at Carnegie Mellon University. The dataset is part of the data used in the 1988 ASA Graphics Section Poster Session. Salary data originally from Sports Illustrated, April 20, 1987. 1986 and career statistics are from the 1987 Baseball Encyclopedia Update, published by Collier Books, Macmillan Publishing Company, New York.\n\n* AtBat: Number of hits with a baseball bat during the 1986-1987 season\n* Hits: the number of successful hits in the 1986-1987 season\n* HmRun: Most valuable hits in the 1986-1987 season\n* Runs: The points he earned for his team in the 1986-1987 season\n* RBI: The number of players a batter had jogged when he hit\n* Walks: Number of mistakes made by the opposing player\n* Years: Player's playing time in major league (years)\n* CAtBat: Number of hits during the player's career\n* CHits: The number of hits the player has made throughout his career\n* CHmRun: The player's most valuable number during his career\n* CRuns: The number of points the player has earned for his team during his career\n* CRBI: The number of players the player has made during his career\n* CWalks: The number of mistakes the player has made to the opposing player during his career\n* League: A factor with A and N levels showing the league in which the player played until the end of the season\n* Division: a factor with levels E and W indicating the position played by the player at the end of 1986\n* PutOuts: Helping your teammate in-game\n* Assists: Number of assists made by the player in the 1986-1987 season\n* Errors: the number of errors of the player in the 1986-1987 season\n* Salary: The salary of the player in the 1986-1987 season (over thousand)\n* NewLeague: a factor with A and N levels indicating the player's league at the start of the 1987 season\n\n","605622e8":"# Light GBM Regressor","69e5b110":"# Thank you :)","54cc255d":"# **Before Hyperparameter Tuning**","6cde4127":"# **Predicting 'df_nan' with our RandomForestRegressor model**","8d1274ad":"# **Analysing target column**","083bc464":"# **Modeling**\n# Random Forest Regressor","139b4543":"# **Exploratory Data Analysis**","afe8663d":"## **We have to do the same thing for new 'df'**"}}