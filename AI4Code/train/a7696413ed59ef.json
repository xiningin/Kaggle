{"cell_type":{"faf10a5a":"code","07118b6b":"code","dafd29c4":"code","5b9c916b":"code","0f7487ff":"code","6e97093b":"code","22d347eb":"code","b6b96214":"code","a3b734ee":"code","f4bc6bf7":"code","637dd61f":"code","5e254f38":"code","fe2e95d8":"code","816c5343":"code","229695b6":"code","1fdd5844":"code","bca21d9d":"code","4bbc1738":"code","1dab5780":"code","9c90f21f":"code","80723c4c":"code","a356230a":"code","244b2f35":"code","087d58e8":"code","0ffae1fa":"code","2697645e":"code","8e60a951":"code","609e02bf":"code","5fc7fbed":"code","7a201d5a":"code","46ad39cc":"code","193c14ff":"code","d911a541":"code","a2e9e283":"code","4d563a26":"code","51a5e304":"code","bcef717c":"code","92997d5f":"code","d8501817":"code","7c8bb034":"code","b9725a1b":"markdown","6e86338c":"markdown","de73e13b":"markdown","911cf254":"markdown","b502321b":"markdown","87311150":"markdown","9a27d595":"markdown","7f2f2560":"markdown","71c054af":"markdown","95274723":"markdown","cf1a82ae":"markdown","ac658304":"markdown","c3986c8e":"markdown","ecfa7d12":"markdown","6b5b4c65":"markdown","f0aed2ae":"markdown","1fc47a8e":"markdown","c365acf7":"markdown","38ccc2b7":"markdown","6c3fec0e":"markdown","8b997e7c":"markdown","ccfa9923":"markdown","010f923a":"markdown","a13d40de":"markdown","bc111b21":"markdown","82cf8e4f":"markdown","12fb5bcf":"markdown","bd2e6398":"markdown","bc164495":"markdown","4bc41e1c":"markdown","a35f9501":"markdown","4c05eec9":"markdown","3e1b6908":"markdown","a57437ec":"markdown","2530363d":"markdown","b6bddd3d":"markdown","71a94aa2":"markdown","8142cb7d":"markdown","da8b12d5":"markdown","03ef1298":"markdown","3b86a040":"markdown","4b4c607b":"markdown","aba05970":"markdown","be19479d":"markdown","912943ab":"markdown","9c984404":"markdown","31d2cc01":"markdown","0d705995":"markdown"},"source":{"faf10a5a":"!pip install segmentation-models-pytorch","07118b6b":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport cv2\nimport collections\nimport segmentation_models_pytorch as smp\nimport albumentations as albu\nimport torch\nimport seaborn as sns\n\nfrom torch.utils.data import DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom glob import glob\nfrom os import path\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom matplotlib import pyplot as plt\n\n%matplotlib inline","dafd29c4":"ROOT_PATH_TRAIN = '\/kaggle\/input\/imaterialist-fashion-2019-FGVC6\/train'\nDF_PATH_TRAIN = '\/kaggle\/input\/imaterialist-fashion-2019-FGVC6\/train.csv'\nPATH_TO_MODEL_WEIGHTS = '\/kaggle\/input\/best-twohead-unet\/best_model.pth'\n\nIMAGE_SIZE = (512, 512)\nIAMGE_PREDICTION_SIZE = (256, 256)","5b9c916b":"def rle_decode(mask_rle, shape):\n    '''\n    mask_rle: run-length as string formated: [start0] [length0] [start1] [length1]... in 1d array\n    shape: (height,width) of array to return\n    Returns numpy array according to the shape, 1 - mask, 0 - background\n    '''\n    shape = (shape[1], shape[0])\n    s = mask_rle.split()\n    # gets starts & lengths 1d arrays\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0::2], s[1::2])]\n    starts -= 1\n    # gets ends 1d array\n    ends = starts + lengths\n    # creates blank mask image 1d array\n    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n    # sets mark pixles\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    # reshape as a 2d mask image\n    return img.reshape(shape).T","0f7487ff":"def rle_to_string(runs):\n    return ' '.join(str(x) for x in runs)\n\ndef rle_encode(mask):\n    pixels = mask.T.flatten()\n    # We need to allow for cases where there is a '1' at either end of the sequence.\n    # We do this by padding with a zero at each end when needed.\n    use_padding = False\n    if pixels[0] or pixels[-1]:\n        use_padding = True\n        pixel_padded = np.zeros([len(pixels) + 2], dtype=pixels.dtype)\n        pixel_padded[1:-1] = pixels\n        pixels = pixel_padded\n    rle = np.where(pixels[1:] != pixels[:-1])[0] + 2\n    if use_padding:\n        rle = rle - 1\n    rle[1::2] = rle[1::2] - rle[:-1:2]\n    return rle_to_string(rle)","6e97093b":"def visualise_masks(image_name, real_df, predicted_df=None, r_p=ROOT_PATH_TRAIN, im_class=None):\n    # get image\n    img_path = os.path.join(r_p, image_name)\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, IMAGE_SIZE)\n    \n    # get original mask\n    if im_class is not None:\n        original_raws = real_df[(real_df['ImageId'] == image_name) & (real_df['ClassId'] == im_class)]\n    else:\n        original_raws = real_df[real_df['ImageId'] == image_name]\n                \n    o_h, o_w = int(original_raws['Height'].mean()), int(original_raws['Width'].mean())\n    \n    o_mask = np.zeros(IMAGE_SIZE)        \n        \n    for annotation in original_raws['EncodedPixels']:\n        o_mask += cv2.resize(rle_decode(annotation, (o_h, o_w)), IMAGE_SIZE)\n        \n    o_mask = (o_mask > 0.5).astype(np.uint8)\n    \n    if predicted_df is not None:\n        # get predicted mask\n        if im_class is not None:\n            predicted_raws = predicted_df[(predicted_df['ImageId'] == image_name) & (predicted_df['ClassId'] == im_class)]\n        else:\n            predicted_raws = predicted_df[predicted_df['ImageId'] == image_name]\n                \n        p_mask = np.zeros(IMAGE_SIZE)\n        \n        for annotation in predicted_raws['EncodedPixels']:\n            p_mask += rle_decode(annotation, IMAGE_SIZE)\n        \n        p_mask = (p_mask > 0.5).astype(np.uint8)\n    \n    fig=plt.figure(figsize=(20, 20))\n    fig.add_subplot(1, 3, 1)\n    plt.title('image')\n    plt.imshow(img)\n    fig.add_subplot(1, 3, 2)\n    plt.title('original_mask')\n    plt.imshow(o_mask)\n    \n    if predicted_df is not None:\n        fig.add_subplot(1, 3, 3)\n        plt.title('predicted_mask')\n        plt.imshow(p_mask)\n        plt.show()","22d347eb":"train_df = pd.read_csv(DF_PATH_TRAIN)\ntrain_df.head()","b6b96214":"train_df['CategoryId'] = train_df.ClassId.apply(lambda x: str(x).split(\"_\")[0])","a3b734ee":"plt.figure(figsize=(20, 7))\nsns.countplot(train_df['CategoryId']);","f4bc6bf7":"train_df[train_df['ImageId'] == '00000663ed1ff0c4e0132b9b9ac53f6e.jpg']","637dd61f":"visualise_masks('00000663ed1ff0c4e0132b9b9ac53f6e.jpg', train_df, im_class='31')","5e254f38":"def get_unique_class_id_df(inital_df):\n    temp_df = inital_df.groupby(['ImageId','ClassId'])['EncodedPixels'].agg(lambda x: ' '.join(list(x))).reset_index()\n    size_df = inital_df.groupby(['ImageId','ClassId'])['Height', 'Width'].mean().reset_index()\n    temp_df = temp_df.merge(size_df, on=['ImageId','ClassId'], how='left')\n    \n    return temp_df","fe2e95d8":"train_df = get_unique_class_id_df(train_df)","816c5343":"train_df[train_df['ImageId'] == '00000663ed1ff0c4e0132b9b9ac53f6e.jpg']","229695b6":"visualise_masks('00000663ed1ff0c4e0132b9b9ac53f6e.jpg', train_df, im_class='31')","1fdd5844":"def create_one_represent_class(df_param):\n    v_c_df = df_param['CategoryId'].value_counts().reset_index()\n    one_represent = v_c_df.loc[v_c_df['CategoryId'] == 1, 'index'].tolist()\n    df_param.loc[df_param['CategoryId'].isin(one_represent), 'CategoryId'] = 'one_represent'\n    return df_param\n\ndef custom_train_test_split(df_param):\n    \n    df_param['CategoryId'] = df_param.ClassId.apply(lambda x: str(x).split(\"_\")[0])\n    \n    img_categ = train_df.groupby('ImageId')['CategoryId'].apply(list).reset_index()\n    img_categ['CategoryId'] = img_categ['CategoryId'].apply(lambda x: ' '.join(sorted(x)))\n    \n    img_categ = create_one_represent_class(img_categ)\n    \n    img_train, img_val  = train_test_split(img_categ, test_size=0.2, random_state=42, stratify=img_categ['CategoryId'])\n    \n    df_param = df_param.drop(columns='CategoryId')\n    \n    df_train = df_param[df_param['ImageId'].isin(img_train['ImageId'])].reset_index(drop=True)\n    df_val = df_param[df_param['ImageId'].isin(img_val['ImageId'])].reset_index(drop=True)\n    \n    return df_train, df_val","bca21d9d":"train_df = pd.read_csv(DF_PATH_TRAIN)\ntrain_df, val_df = custom_train_test_split(train_df)\n\ntrain_df = get_unique_class_id_df(train_df)\nval_df = get_unique_class_id_df(val_df)","4bbc1738":"plt.figure(figsize=(20, 7))\nplt.title('Train')\nsns.countplot(train_df['ClassId'].apply(lambda x: str(x).split(\"_\")[0]));","1dab5780":"plt.figure(figsize=(20, 7))\nplt.title('Validation')\nsns.countplot(val_df['ClassId'].apply(lambda x: str(x).split(\"_\")[0]));","9c90f21f":"class UnetDataset(torch.utils.data.Dataset):\n    def __init__(self, image_dir, df, height, width, augmentation=None, preprocessing=None):\n        \n        self.preprocessing = preprocessing\n        self.augmentation = augmentation\n        \n        self.image_dir = image_dir\n        self.df = df\n        \n        self.height = height\n        self.width = width\n        \n        self.image_info = collections.defaultdict(dict)\n        \n        self.df['CategoryId'] = self.df.ClassId.apply(lambda x: str(x).split(\"_\")[0])\n        self.num_classes = self.df['CategoryId'].nunique()\n        \n        temp_df = self.df.groupby('ImageId')['EncodedPixels', 'CategoryId'].agg(lambda x: list(x)).reset_index()\n        size_df = self.df.groupby('ImageId')['Height', 'Width'].mean().reset_index()\n        temp_df = temp_df.merge(size_df, on='ImageId', how='left')\n        \n        for index, row in tqdm(temp_df.iterrows(), total=len(temp_df)):\n            image_id = row['ImageId']\n            image_path = os.path.join(self.image_dir, image_id)\n            self.image_info[index][\"image_id\"] = image_id\n            self.image_info[index][\"image_path\"] = image_path\n            self.image_info[index][\"width\"] = self.width\n            self.image_info[index][\"height\"] = self.height\n            self.image_info[index][\"labels\"] = row[\"CategoryId\"]\n            self.image_info[index][\"orig_height\"] = row[\"Height\"]\n            self.image_info[index][\"orig_width\"] = row[\"Width\"]\n            self.image_info[index][\"annotations\"] = row[\"EncodedPixels\"]\n\n    def __getitem__(self, idx):\n        \n        img_path = self.image_info[idx][\"image_path\"]\n        \n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, (self.width, self.height))\n        \n        # apply preprocessing\n        if self.preprocessing is not None:\n            img = self.preprocessing(image=img)['image']\n            \n        return img, os.path.basename(img_path)\n\n    def __len__(self):\n        return len(self.image_info)","80723c4c":"def to_tensor(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n\ndef get_preprocessing(preprocessing_fn):\n    \"\"\"Construct preprocessing transform\n    \n    Args:\n        preprocessing_fn (callbale): data normalization function \n            (can be specific for each pretrained neural network)\n    Return:\n        transform: albumentations.Compose\n    \n    \"\"\"\n    \n    _transform = [\n        albu.Lambda(image=preprocessing_fn),\n        albu.Lambda(image=to_tensor, mask=to_tensor),\n    ]\n    return albu.Compose(_transform)","a356230a":"ENCODER = 'mobilenet_v2'\nENCODER_WEIGHTS = 'imagenet'\nDEVICE = 'cuda'\n\nACTIVATION = 'sigmoid'\n\naux_params=dict(\n    pooling='avg',             \n    dropout=0.2,               \n    activation=None,      \n    classes=46,                 \n)","244b2f35":"model = smp.Unet(\n    encoder_name=ENCODER, \n    encoder_weights=ENCODER_WEIGHTS, \n    classes=46, \n    activation=ACTIVATION,\n    aux_params=aux_params\n)\n\npreprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)","087d58e8":"model.load_state_dict(torch.load(PATH_TO_MODEL_WEIGHTS, \n                                 map_location=torch.device(DEVICE)))","0ffae1fa":"model = model.to(DEVICE)\nmodel.eval();","2697645e":"test_dataset = UnetDataset(\n    ROOT_PATH_TRAIN,\n    val_df,\n    IAMGE_PREDICTION_SIZE[0],\n    IAMGE_PREDICTION_SIZE[0], \n    preprocessing=get_preprocessing(preprocessing_fn),\n)\n\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)","8e60a951":"def create_final_df(result_dict):\n    final_df = {'ImageId':[], 'EncodedPixels':[], 'ClassId':[]}\n    for im_name, im_dict in result_dict.items():\n        final_df['ImageId'] += [im_name]*len(im_dict)\n        for cls_id, enc_p in im_dict.items():\n            final_df['EncodedPixels'].append(enc_p)\n            final_df['ClassId'].append(cls_id)\n            \n    return pd.DataFrame(final_df)\n\nclass InfernceModel(object):\n    def __init__(self, nn_model, device, output_size=(512,512), threshold=0.5, size_min_mask=250):\n        self.nn_model = nn_model\n        self.output_size = output_size\n        self.threshold = threshold\n        self.size_min_mask = size_min_mask\n        self.device = device\n        \n    def __call__(self, inf_dataloader):\n        result = {}\n        with torch.no_grad():\n            for batch in tqdm(inf_dataloader, \n                              total=len(inf_dataloader.dataset) \/\/ inf_dataloader.batch_size):\n                names = batch[1]\n                batch_mask, batch_label = model(batch[0].to(self.device))\n                batch_mask, batch_label = batch_mask.detach().cpu().numpy(), batch_label.detach().cpu().numpy()\n                for idx in range(batch_mask.shape[0]):\n                    cur_result = self.post_process(batch_mask[idx], batch_label[idx])\n                    if len(cur_result) > 0:\n                        result[names[idx]] = cur_result\n                        \n        final_df = create_final_df(result) \n        final_df['Height'] = self.output_size[0]\n        final_df['Width'] = self.output_size[1]\n        final_df['ClassId'] = final_df['ClassId'].astype(str)\n                    \n        return final_df\n        \n    def post_process(self, mask, labels):\n        rle_mask = {}\n        for idx in range(mask.shape[0]):\n            if labels[idx] < 0:\n                continue\n                \n            item_mask = mask[idx]\n            \n            if item_mask.max() > 1 or item_mask.min() < 0:\n                raise ValueError('Prob out of range')\n            \n            item_mask = (item_mask > self.threshold).astype(np.uint8)\n            item_mask = cv2.resize(item_mask, self.output_size)\n            \n            if item_mask.sum() < self.size_min_mask:\n                continue\n            else:\n                rle_mask[str(idx)] = rle_encode(item_mask)\n            \n        return rle_mask","609e02bf":"inf_model = InfernceModel(nn_model=model, device=DEVICE)","5fc7fbed":"predicted_df = inf_model(test_loader)","7a201d5a":"predicted_df.head()","46ad39cc":"visualise_masks('000b3a87508b0fa185fbd53ecbe2e4c6.jpg', val_df, predicted_df)","193c14ff":"visualise_masks('fffc631acce2e28e1628de685d40c980.jpg', val_df, predicted_df)","d911a541":"visualise_masks('ffec8295f37df6ea12eecbb60d2c23d4.jpg', val_df, predicted_df)","a2e9e283":"visualise_masks('001039acb67251508b1b32fd37a49f43.jpg', val_df, predicted_df, im_class='31')","4d563a26":"visualise_masks('005ccdb239e2d6cfe62506dd6eb5693e.jpg', val_df, predicted_df, im_class='10')","51a5e304":"visualise_masks('000cd2e13d1bdd28f480304d7bb9e1ca.jpg', val_df, predicted_df, im_class='23')","bcef717c":"visualise_masks('05e6ef19957d43524d972de6d2f41b57.jpg', val_df, predicted_df, im_class='45')","92997d5f":"# mainly from here https:\/\/www.kaggle.com\/kyazuki\/calculate-evaluation-score\n\ndef IoU(A,B):\n    AorB = np.logical_or(A,B).astype('int')\n    AandB = np.logical_and(A,B).astype('int')\n    IoU = AandB.sum() \/ AorB.sum()\n    return IoU\n\ndef IoU_threshold(data):\n    # Note: This rle_to_mask should be called before loop below for speed-up! We currently implement here to reduse memory usage.\n    mask_gt = rle_decode(data['EncodedPixels_true'], (int(data['Height_true']), int(data['Width_true'])))\n    mask_pred = rle_decode(data['EncodedPixels_pred'], (int(data['Height_pred']), int(data['Width_pred'])))\\\n    \n    if (int(data['Height_true']), int(data['Width_true'])) != IMAGE_SIZE:\n        mask_gt = cv2.resize(mask_gt, IMAGE_SIZE)\n    if (int(data['Height_pred']), int(data['Width_pred'])) != IMAGE_SIZE:\n        mask_pred = cv2.resize(mask_pred, IMAGE_SIZE)\n    \n    mask_gt = mask_gt > 0.5\n    mask_pred = mask_pred > 0.5\n    \n    return IoU(mask_gt, mask_pred)\n\ndef best_metric(true_df, pred_df):\n    eval_df = pd.merge(true_df, pred_df, how='outer', on=['ImageId', 'ClassId'], suffixes=['_true', '_pred'])\n\n    # IoU for True Positive\n    idx_ = eval_df['EncodedPixels_true'].notnull() & eval_df['EncodedPixels_pred'].notnull()\n    IoU = eval_df[idx_].apply(IoU_threshold, axis=1)\n\n    # False Positive\n    fp = (eval_df['EncodedPixels_true'].isnull() & eval_df['EncodedPixels_pred'].notnull()).sum()\n\n    # False Negative\n    fn = (eval_df['EncodedPixels_true'].notnull() & eval_df['EncodedPixels_pred'].isnull()).sum()\n\n    threshold_IoU = np.arange(0.5, 1.0, 0.05)\n    scores = []\n    for th in threshold_IoU:\n        # True Positive\n        tp = (IoU > th).sum()\n        iou_fp = (IoU <= th).sum()\n\n        # False Positive (not Ground Truth) + False Positive (under IoU threshold)\n        fp = fp + iou_fp\n\n        # Calculate evaluation score\n        score = tp \/ (tp + fp + fn)\n        scores.append(score)\n\n    mean_score = sum(scores) \/ len(threshold_IoU)\n    return mean_score","d8501817":"images_to_count_metric = val_df['ImageId'].unique()[:100]\n\nbest_metric(val_df[val_df['ImageId'].isin(images_to_count_metric)], val_df[val_df['ImageId'].isin(images_to_count_metric)])","7c8bb034":"best_metric(val_df, predicted_df)","b9725a1b":"But as for smaller masks and less frequent","6e86338c":"![](http:\/\/static.issue.life\/Content\/img\/17-03-2019\/636884100747932656.png)","de73e13b":"We think that such format of test task is one of the worst variants. Why ?\n* This task requires Deep Learning background, which is overkill for contest, where most participants are students \n* Even if participant has Deep Learning background, he\/she will need a lot of computational resources (GPUs), which cost money or use his\/her GPU Quota on Kaggle\n* Metric and Train\/Test were not provided. So all participants will have hardly biased results. It is important, because evaluating DL\/ML\/DS tasks by 'beautiful code' or 'interesting approach' or something like this is not the best practice. Of course, these criteria should be taken into account, but main criteria is target metric !","911cf254":"### Metric","b502321b":"So we get some main cloth parts","87311150":"Not very good results for these masks","9a27d595":"## Model","7f2f2560":"## What can be improved","71c054af":"So lets fix it","95274723":"# DL part","cf1a82ae":"# Final thoughts or kozaki pishut pismo BESTu","ac658304":"# Data exploration","c3986c8e":"Lets take a look on our categoryid distribution","ecfa7d12":"Why we did not do all this stuff\n\nBecause\n\n* 30 hours per week of railway\n* one GPU per account\n* execution - up to 6 hours","6b5b4c65":"Got it. Also these masks are huge enough for our week net","f0aed2ae":"### Visualize","1fc47a8e":"# Inference","c365acf7":"We have trained our Net only for 2 epochs (Kaggle Kernel limitation on time)\n* With Dice Loss\n* With Horizontal and Vertical flip augmentations\n* With Adam optimizer\n* Data was normalized by ImageNet stats","38ccc2b7":"Awesome matching !!!","6c3fec0e":"We will use classical Unet architecture with mobilenet backbone, pretrained on ImageNet. Why ?\n* Unet is classic\n* Backbone is light enough to train it in Kaggle Kernel","8b997e7c":"![Unet](https:\/\/i.stack.imgur.com\/DjXVU.png)","ccfa9923":"We have images with different cloth parts and we have to identify cloth id and attributes","010f923a":"# Dataloader","a13d40de":"* Use MaskRCNN or other appropriate architecture for biag amount of classes\n* Use bigger backbone\n* Train until Net converged\n* Use bigger augmentations\n* Tune net hyperparams\n* Use TwoHead Net\n* Use Test Time Augmentations\n* Use Blending\n\nAnd so on ....","bc111b21":"# pip installs","82cf8e4f":"# Imports","12fb5bcf":"# Starting points","bd2e6398":"![](https:\/\/pbs.twimg.com\/media\/CnN1pRvUAAA0lWq.jpg)","bc164495":"Now lets look at different CategoryID","4bc41e1c":"Of course, it is better to have train\/validation\/test datasets. But now we will stop on train\/test. Why?\n* We do not have Early Stopping and other stuff, that will overfit us\n* Orgs did not clarify this point properly ","a35f9501":"In our Classid we have Category id and it's attributes. Predicting attributes is a really hard stuff and competion best practises showed that in most cases it will only harm the perfomence of your Neural Net. So we will lets get Category id","4c05eec9":"Firstly, on some frequent","3e1b6908":"# Train\/Test split","a57437ec":"## Evaluate results","2530363d":"# Utils fucntions","b6bddd3d":"Moreover in our dataframe we have to have unique pair (ImageId, ClassId) but in our initial dataframe we have each pair for each mask segment.\nIn the following example we have duplicates with ClassId - 31, 32","71a94aa2":"## Dataset","8142cb7d":"Lets test it","da8b12d5":"We will make train\/test split on Classid combinations ","03ef1298":"# Power Rangers Solution","3b86a040":"So now we are ready for DEEEEEEEEEEEEEEEEEEEEEEEEEEEEEep Learning","4b4c607b":"# Constants","aba05970":"And now evaluate our prediction","be19479d":"![](https:\/\/zaxid.net\/resources\/photos\/news\/640x360_DIR\/201511\/1371769.jpg?201805281755)","912943ab":"Firstly, on all classes together","9c984404":"EVERYTHING","31d2cc01":"We will use classical deep learning approach. We will use this [awesome framework](https:\/\/github.com\/qubvel\/segmentation_models.pytorch)","0d705995":"## Preprocessing"}}