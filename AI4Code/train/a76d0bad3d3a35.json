{"cell_type":{"7b63832c":"code","12f7acaa":"code","55dae69c":"code","448c2481":"code","05cd0dc1":"code","10eee189":"code","af06b40d":"code","7b3e64c1":"code","4c8a2339":"code","09b01766":"code","f66c9b0b":"code","86859430":"code","5c0dce88":"code","400db127":"code","b6efef67":"code","595732ec":"code","a5686840":"code","e379d9b8":"code","3426c58b":"markdown","29c5c893":"markdown","f6ebe449":"markdown","636b4db2":"markdown","7c26d124":"markdown","a7e33597":"markdown","63672505":"markdown","9254b587":"markdown","2459a22a":"markdown","e9c2a774":"markdown","33dcaee9":"markdown","040cf4d7":"markdown","a3b9d00a":"markdown","b883a913":"markdown"},"source":{"7b63832c":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\n\nfrom keras.layers import Dense\nfrom keras.models import Sequential\n\nimport matplotlib.pyplot as plt","12f7acaa":"def calculate(x):\n    return x * x\n\ninputs = [-0.5, -0.4, -0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.4, 0.5]\n\noutputs = [calculate(x) for x in inputs]\n\nplt.plot(inputs, outputs)\n\nplt.show()","55dae69c":"print(np.random.rand(5))\nprint(np.random.rand(5) - 0.5)\nprint(np.ones((5,1)))","448c2481":"# generate randoms sample from x^2\ndef generate_samples(n=100):\n    X1 = np.random.rand(n) - 0.5\n    X2 = X1 * X1\n    X1 = X1.reshape(n, 1)\n    X2 = X2.reshape(n, 1)\n    \n    return np.hstack((X1, X2))\n\ndata = generate_samples()\n# plot samples\nplt.scatter(data[:, 0], data[:, 1])\nplt.show()","05cd0dc1":"def generate_real_samples(n):\n    \n    # generate inputs in [-0.5, 0.5]\n    X1 = np.random.rand(n) - 0.5\n    X2 = X1 * X1\n    X1 = X1.reshape(n, 1)\n    X2 = X2.reshape(n, 1)\n    \n    X = np.hstack((X1, X2))\n    y = np.ones((n, 1))\n    \n    return X, y\n\n\ndef generate_fake_samples(n):\n    \n    # generate inputs in [-1, 1]\n    X1 = -1 + np.random.rand(n) * 2\n    \n    # generate outputs in [-1, 1]\n    X2 = -1 + np.random.rand(n) * 2\n    \n    X1 = X1.reshape(n, 1)\n    X2 = X2.reshape(n, 1)\n    \n    X = np.hstack((X1, X2))\n    y = np.ones((n, 1))\n    \n    return X, y","10eee189":"def define_discriminator(n_inputs=2):\n    model = Sequential()\n    \n    model.add(Dense(25, input_dim=n_inputs, \n                    activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model","af06b40d":"def train_discriminator(model, n_epochs=1000, n_batch=128):\n    half_batch = int(n_batch \/ 2)\n    \n    for i in range(n_epochs):\n        X_real, y_real = generate_real_samples(half_batch)\n        model.train_on_batch(X_real, y_real)\n        \n        X_fake, y_fake = generate_fake_samples(half_batch)\n        model.train_on_batch(X_fake, y_fake)\n        \n        _, acc_real = model.evaluate(X_real, y_real, verbose=0)\n        _, acc_fake = model.evaluate(X_fake, y_fake, verbose=0)\n        \n        print(i, acc_real, acc_fake)\n\nmodel = define_discriminator()\ntrain_discriminator(model)","7b3e64c1":"def define_generator(latent_dim, n_outputs=2):\n    model = Sequential()\n    \n    model.add(Dense(15, activation='relu', \n                    kernel_initializer='he_uniform', input_dim=latent_dim))\n    \n    model.add(Dense(n_outputs, activation='linear'))\n    return model","4c8a2339":"def generate_latent_points(latent_dim, n):\n    x_input = np.random.randn(latent_dim * n)\n    # reshape into a batch of inputs for the network\n    \n    x_input = x_input.reshape(n, latent_dim)\n    \n    return x_input\n\n\ndef generate_fake_samples(generator, latent_dim, n):\n    x_input = generate_latent_points(latent_dim, n)\n    X = generator.predict(x_input)\n    \n    y = np.zeros((n, 1))\n    \n    return X, y\n","09b01766":"latent_dim = 5\nmodel = define_generator(latent_dim)\n\nX, y = generate_fake_samples(model, latent_dim, 100)\nplt.scatter(X[:, 0], X[:, 1])\nplt.show()","f66c9b0b":"# define the combined generator and discriminator model, for updating the generator\n\ndef define_gan(generator, discriminator):\n    discriminator.trainable = False\n    model = Sequential()\n    model.add(generator)\n    model.add(discriminator)\n    \n    model.compile(loss='binary_crossentropy', optimizer='adam')\n    \n    return model","86859430":"def summarize_performance(epoch, generator, discriminator, latent_dim, n=100):\n    x_real, y_real = generate_real_samples(n)\n    _, acc_real = discriminator.evaluate(x_real, y_real, verbose=0)\n    \n    x_fake, y_fake = generate_fake_samples(generator, latent_dim, n)\n    _, acc_fake = discriminator.evaluate(x_fake, y_fake, verbose=0)\n\n    print(epoch, acc_real, acc_fake)\n    \n    plt.scatter(x_real[:, 0], x_real[:, 1], color='red') \n    plt.scatter(x_fake[:, 0], x_fake[:, 1], color='blue') \n    # save plot to file\n    filename = 'generated_plot_e%03d.png' % (epoch+1) \n    plt.savefig(filename)\n    plt.close()","5c0dce88":"def train(g_model, d_model, gan_model, latent_dim, n_epochs=10000, n_batch=128, n_eval=1000):\n    half_batch = int(n_batch \/ 2)\n    \n    for i in range(n_epochs):\n        x_real, y_real = generate_real_samples(half_batch)\n        x_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n        \n        # Train Discriminator\n        d_model.train_on_batch(x_real, y_real)\n        d_model.train_on_batch(x_fake, y_fake)\n        \n        # prepare points in latent space as input for the generator\n        x_gan = generate_latent_points(latent_dim, n_batch)\n        y_gan = np.ones((n_batch, 1))\n        \n        # update the generator via the discriminator's error\n        gan_model.train_on_batch(x_gan, y_gan)\n        \n        # evaluate the model every n_eval epochs\n        if (i+1) % n_eval == 0:\n            summarize_performance(i, g_model, d_model, latent_dim)\n        \n        ","400db127":"latent_dim = 5\n\n# create the discriminator\ndiscriminator = define_discriminator()\n\n# create the generator\ngenerator = define_generator(latent_dim)\n\n# create the gan\ngan_model = define_gan(generator, discriminator)\n\ntrain(generator, discriminator, gan_model, latent_dim)","b6efef67":"PATH = '\/kaggle\/working\/'\nimg = plt.imread(PATH + 'generated_plot_e1000.png')\nplt.imshow(img)","595732ec":"img = plt.imread(PATH + 'generated_plot_e2000.png')\nplt.imshow(img)","a5686840":"img = plt.imread(PATH + 'generated_plot_e6000.png')\nplt.imshow(img)","e379d9b8":"img = plt.imread(PATH + 'generated_plot_e10000.png')\nplt.imshow(img)","3426c58b":"The model is not compiled. The reason for this is that the generator model is not fit directly.","29c5c893":"<h3><center>Introduction<\/center><\/h3>\n\n\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\nGANs are comprised of both generator and discriminator models. The generator is responsible for generating new samples from the domain, and the discriminator is responsible for classifying whether samples are real or fake (generated).<br><br> Importantly, the performance of the discriminator model is used to update both the model weights of the discriminator itself and the generator model. This means that the generator never actually sees examples from the domain and is adapted based on how well the discriminator performs.\n<br>\n    <br>\n    <ol><b>NoteBook Topics:<\/b><br><br>\n<li>Select a One-Dimensional Function\n<li>Define a Discriminator Model\n<li>Define a Generator Model\n<li>Training the Generator Model\n<li>Evaluating the Performance of the GAN\n<li>Complete Example of Training the GAN\n    <\/ol>\n\n<\/div>\n    ","f6ebe449":"<h3><center>2. Training the Generator Model<\/center><\/h3>\n\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\n    The weights in the generator model are updated based on the performance of the discriminator model. When the discriminator is good at detecting fake samples, the generator is updated more (via a larger error gradient), and when the discriminator model is relatively poor or confused when detecting fake samples, the generator model is updated less.<br> This defines the zero-sum or adversarial relationship between these two models.<br><br>\nA new GAN model can be defined that stacks the generator and discriminator such that the generator receives as input random points in the latent space, generates samples that are fed into the discriminator model directly, classified, and the output of this larger model can be used to update the model weights of the generator.<br><br>\nThe generator wants the discriminator to think that the samples output by the generator are real, not fake. Therefore, when the generator is trained as part of the GAN model, we will mark the generated samples as real (class = 1). We can imagine that the discriminator will then classify the generated samples as not real (class = 0) or a low probability of being real (0.3 or 0.5). The backpropagation process used to update the model weights will see this as a large error and will update the model weights (i.e. only the weights in the generator) to correct for this error, in turn making the generator better at generating plausible fake samples.<\/div>","636b4db2":"<h3><center>4. Training after 10,000 Epochs<\/center><\/h3>","7c26d124":"<div style=\"font-family:verdana; word-spacing:1.7px;\">\nRunning the example generates 100 random points from the latent space, uses this as input to the generator and generates 100 fake samples from our one-dimensional function domain. <br><br>As the generator has not been trained, the generated points are complete rubbish, as we expect, but we can imagine that as the model is trained, these points will slowly begin to resemble the target function and its u-shape.\n    <\/div>","a7e33597":"<h3><center>1. Training after 1000 Epochs<\/center><\/h3>","63672505":"<div style=\"font-family:verdana; word-spacing:1.7px;\">\nWe can use this function as a starting point for generating real samples for our discriminator function. Specifically, a sample is comprised of a vector with two elements, one for the input and one for the output of our one-dimensional function.<br><br> We can also imagine how a generator model could generate new samples that we can plot and compare to the expected u-shape of the X2 function.<br><br> Specifically, a generator would output a vector with two elements: one for the input and one for the output of our one-dimensional function.\n    <\/div>","9254b587":"<h3><center>3. Training after 6000 Epochs<\/center><\/h3>","2459a22a":"<div style=\"font-family:verdana; word-spacing:1.7px;\">\nFirst, we generate uniformly random values between 0 and 1, then shift them to the range -0.5 and 0.5. We then calculate the output value for each randomly generated input value and combine the arrays into a single NumPy array with n rows (100) and two columns.\n    <\/div>","e9c2a774":"<h3><center>1. Select a One-Dimensional Function<\/center><\/h3>","33dcaee9":"<h4>Generate points in Latent space.<\/h4>\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\nWe have to generate new random points in the latent space. We can achieve this by calling the randn() NumPy function for generating arrays of random numbers drawn from a standard Gaussian.\n    <\/div>","040cf4d7":"<h3><center>2. Define a Generator Model<\/center><\/h3>\n\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\n    The generator model takes as input a point from the latent space and generates a new sample\n    <\/div>","a3b9d00a":"<h3><center>2. Training after 2000 Epochs<\/center><\/h3>","b883a913":"<h3><center>2. Define a Discriminator Model & Train it<\/center><\/h3>\n\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\nThe model will minimize the binary cross-entropy loss function, and the Adam version of stochastic gradient descent will be used because it is very effective.\n    <\/div>"}}