{"cell_type":{"c1374937":"code","ed07468a":"code","89ea97eb":"code","32248409":"code","9e57a8e7":"code","1f0161ee":"code","f16c0ce4":"code","f77f3a80":"code","ce6c52a8":"code","8bd2a864":"code","ab307e8e":"code","c2915fdc":"code","42941157":"code","37a64a7c":"code","7ea8f90a":"code","8bbb314c":"code","23794c04":"code","ddeaaecf":"code","83fe57d7":"code","4b39e2af":"code","454b3c53":"code","26abeb48":"code","052b3d17":"code","16ca1b1d":"code","f90e8f5e":"code","0be26697":"code","7959d82c":"code","a615af5f":"code","50205aaf":"code","c54140f7":"code","eee0ee3b":"code","dbaa7650":"code","be872371":"code","66d90390":"code","7234a0dc":"code","5b0e81b2":"code","89d1a84b":"code","3cce79c7":"code","851ab953":"code","3ad2a931":"code","340ed877":"code","a249f417":"code","edf54578":"code","a23cc55e":"code","a8c87b75":"code","31a48b8c":"code","6630da03":"code","b6e53cce":"code","14058e4d":"code","098a44b1":"code","f8c7bc7a":"code","d40804ff":"code","19da196a":"code","cc3b6c38":"code","c42b71d6":"code","499472bc":"code","cf1b54aa":"markdown","e586644a":"markdown","4346d99b":"markdown","73905df6":"markdown","cd891076":"markdown","fd48e41c":"markdown","a2a98dba":"markdown","174dd907":"markdown","e87c848f":"markdown","b12b9f50":"markdown","1ac7a78d":"markdown","3eded098":"markdown","4161ec3b":"markdown","b538daa1":"markdown","cefffb61":"markdown","79946253":"markdown","3d2c8ff7":"markdown","9e4694ac":"markdown","1e5e9b45":"markdown","b6f4ee65":"markdown","705ba851":"markdown","946a0814":"markdown","c6e56938":"markdown","761b9618":"markdown","fefb14c3":"markdown","94a8a72d":"markdown","45db217a":"markdown"},"source":{"c1374937":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno\n\n%matplotlib inline","ed07468a":"df = pd.read_csv('..\/input\/ad-click-data\/Ad Click Data.csv')\ndf.head()","89ea97eb":"print('Number of examples and features:', df.shape)\nprint('features in the dataset:', df.columns.tolist())","32248409":"df.info()","9e57a8e7":"# Let's look at stats of the non-object features\ndf.describe()","1f0161ee":"missingno.matrix(df)","f16c0ce4":"#plotting pairlot of numeric features\ng = sns.pairplot(df[['Daily Time Spent on Site',\n                     'Age',\n                     'Area Income',\n                     'Daily Internet Usage']], diag_kind=\"kde\", palette='rocket')\ng.map_lower(sns.kdeplot, levels=4, color=\".2\")","f77f3a80":"sns.pairplot(df, hue='Clicked on Ad', vars=['Daily Time Spent on Site', \n                                            'Age',\n                                           'Area Income',\n                                           'Daily Internet Usage'], palette='rocket')","ce6c52a8":"#get the info of the number of ad clicked\nfig = plt.figure(figsize = (20, 5))\nsns.countplot(x ='Age', data = df)","8bd2a864":"g = sns.JointGrid(data=df, x=\"Age\", y='Daily Time Spent on Site')\ng.plot(sns.regplot, sns.boxplot)","ab307e8e":"sns.jointplot(x='Age',y='Daily Time Spent on Site', data=df, hue=\"Clicked on Ad\", palette='rocket')\n","c2915fdc":"sns.scatterplot(x='Age',y='Daily Time Spent on Site', \n                hue='Clicked on Ad', data=df, palette='rocket')","42941157":"g = sns.JointGrid(data=df, x=\"Area Income\", y='Daily Time Spent on Site')\ng.plot(sns.regplot, sns.boxplot)","37a64a7c":"sns.jointplot(x='Area Income',y='Daily Time Spent on Site', data=df, hue=\"Clicked on Ad\", palette='rocket')","7ea8f90a":"sns.scatterplot(x='Area Income',y='Daily Time Spent on Site', \n                hue='Clicked on Ad', data=df, palette='rocket')","8bbb314c":"g = sns.JointGrid(data=df, x=\"Area Income\", y='Daily Internet Usage')\ng.plot(sns.regplot, sns.boxplot)","23794c04":"sns.jointplot(x='Area Income',y='Daily Internet Usage', data=df, hue=\"Clicked on Ad\", palette='rocket')","ddeaaecf":"sns.scatterplot(x='Area Income',y='Daily Internet Usage', \n                hue='Clicked on Ad', data=df, palette='rocket')","83fe57d7":"g = sns.JointGrid(data=df, x=\"Age\", y='Daily Internet Usage')\ng.plot(sns.regplot, sns.boxplot)","4b39e2af":"sns.jointplot(x='Age',y='Daily Internet Usage', data=df, hue=\"Clicked on Ad\", palette='rocket')","454b3c53":"g = sns.JointGrid(data=df, x='Daily Internet Usage',y='Daily Time Spent on Site')\ng.plot(sns.regplot, sns.boxplot)","26abeb48":"sns.scatterplot(x='Daily Internet Usage',y='Daily Time Spent on Site', \n                hue='Clicked on Ad', data=df, palette='rocket')","052b3d17":"sns.jointplot(x='Daily Internet Usage',y='Daily Internet Usage', data=df, hue=\"Clicked on Ad\", palette='rocket')","16ca1b1d":"import warnings\nwarnings.filterwarnings('ignore')\n    \nplots = ['Daily Time Spent on Site',\n         'Area Income','Daily Internet Usage', 'Age']\nfor i in plots:\n    plt.figure(figsize=(12,6))\n    \n    plt.subplot(2,3,1)\n    sns.boxplot(data=df,x = 'Clicked on Ad', y=i)\n    \n    plt.subplot(2,3,2)\n    sns.boxplot(data=df,y=i)\n    \n    plt.subplot(2,3,3)\n    sns.distplot(df[i],bins=20)\n    plt.tight_layout()\n    plt.title(i)\n    plt.show()","f90e8f5e":"fig = plt.figure(figsize=(12,10))\nsns.heatmap(df.corr(), annot=True)","0be26697":"fig = plt.figure(figsize = (6,2))\nsns.countplot(data = df , y = 'Male')\nprint(df['Male'].value_counts())\n","7959d82c":"\n#get the info of the number of ad clicked\nfig = plt.figure(figsize = (6,2))\nsns.countplot(y ='Clicked on Ad', data = df)\nprint(df['Clicked on Ad'].value_counts())","a615af5f":"#get the info of the Ad Topic Line\nprint(df['Ad Topic Line'].value_counts())","50205aaf":"object_features = ['Ad Topic Line', 'City', 'Country', 'Timestamp']\ndf[object_features].describe(include=['O'])  ","c54140f7":"pd.crosstab(index=df['Country'], columns='count').sort_values(\n    ['count'], ascending=False).head(20)","eee0ee3b":"df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n\ndf['Month'] = df['Timestamp'].dt.month\ndf['Day'] = df['Timestamp'].dt.day\ndf['Weekday'] = df['Timestamp'].dt.dayofweek\ndf['Hour'] = df['Timestamp'].dt.hour\ndf = df.drop(['Timestamp'], axis=1)\n\ndf.head()","dbaa7650":"df['Month'].unique()","be872371":"df['Month'][df['Clicked on Ad'] == 1].value_counts().sort_index()","66d90390":"df['Month'][df['Clicked on Ad'] == 1].value_counts().sort_index().plot()","7234a0dc":"df['Day'][df['Clicked on Ad'] == 1].value_counts().sort_index().plot()","5b0e81b2":"df['Weekday'][df['Clicked on Ad'] == 1].value_counts().sort_index()","89d1a84b":"df['Weekday'][df['Clicked on Ad'] == 1].value_counts().sort_index().plot()","3cce79c7":"df['Hour'][df['Clicked on Ad'] == 1].value_counts().sort_index().plot()","851ab953":"missing = df.isnull().sum()\nmissing_precent = 100*missing\/len(df)\nmissing_table = pd.concat([missing, missing_precent], axis=1)\nmissing_table.columns = ['missing_value', '% of missing_value']\nmissing_table = missing_table.loc[missing_table['missing_value'] != 0].sort_values('missing_value')\nprint('The dataset has total {} columns \\nThere are {} columns that have missing values\\n\\n'.format(df.shape[1], missing_table.shape[0]))\nmissing_table.head()","3ad2a931":"ncounts = pd.DataFrame([df.isna().mean()]).T\nncounts = ncounts.rename(columns={0: \"train_missing\", 1: \"test_missing\"})\n\nncounts.query(\"train_missing > 0\").plot(\n    kind=\"barh\", figsize=(8, 5), title=\"% of Values Missing\"\n)\nplt.show()","340ed877":"df.select_dtypes(exclude='object').shape","a249f417":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\n#filling null values with the median values\nimputer = SimpleImputer()\n#using features with only neumeric values \n#since we already decided to drop all the object features in this dataset\ndf_numeric = df.select_dtypes(exclude='object')\nimputed_df = imputer.fit_transform(df_numeric)\n\n# since imputation converts the data frame into a numpy array, \n# let's convert it into a dataframe back\ndf_train = pd.DataFrame(imputed_df)\n\n# since imputation removes column names, let's put them back\ndf_train.columns = df.select_dtypes(exclude='object').columns\ntrain_features = ['Daily Time Spent on Site', 'Age', 'Area Income',\n                   'Daily Internet Usage', 'Male', \n                   'Month', 'Day', 'Weekday', 'Hour']\n\nnumeric_features = ['Daily Time Spent on Site', 'Age', 'Area Income',\n                   'Daily Internet Usage']\n\nscaler = StandardScaler()\ndf_train[numeric_features] = scaler.fit_transform(df_train[numeric_features])\n\nX = df_train[train_features]\ny = df_train['Clicked on Ad']\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.25,\n                                                    random_state=101)\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)","edf54578":"x_train.head()","a23cc55e":"x_train.isnull().sum().sum()","a8c87b75":"LR = LogisticRegression(solver='lbfgs')\nLR.fit(x_train, y_train)\npredictions_LR = LR.predict(x_test)\n\nprint('\\nLogistic regression accuracy:', accuracy_score(predictions_LR, y_test))\n\ncf_matrix = confusion_matrix(predictions_LR, y_test)\nprint('\\nConfusion Matrix:')\nprint(cf_matrix)\n\nprint(classification_report(y_test, predictions_LR))","31a48b8c":"group_names = ['TN','FP','FN','TP']\ngroup_counts = ['{0:0.0f}'.format(value) for value in\n                cf_matrix.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in\n                     cf_matrix.flatten()\/np.sum(cf_matrix)]\nlabels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')","6630da03":"from sklearn.tree import DecisionTreeClassifier\n\nmodel2 = DecisionTreeClassifier()\nmodel2.fit(x_train, y_train)\npredictions_DT = model2.predict(x_test)\n\nprint('\\nDecisionTreeClassifier accuracy:', accuracy_score(predictions_DT, y_test))\ncf_matrix = confusion_matrix(predictions_DT, y_test)\nprint('\\nConfusion Matrix:')\nprint(cf_matrix)","b6e53cce":"print(classification_report(y_test, predictions_DT))\ngroup_names = ['TN','FP','FN','TP']\ngroup_counts = ['{0:0.0f}'.format(value) for value in\n                cf_matrix.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in\n                     cf_matrix.flatten()\/np.sum(cf_matrix)]\nlabels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')","14058e4d":"from xgboost import XGBClassifier\n\nmodel3 = XGBClassifier()\nmodel3.fit(x_train, y_train)\npredictions_XGB = model3.predict(x_test)\n\nprint('\\nXGBClassifier accuracy:', accuracy_score(predictions_XGB, y_test))\ncf_matrix = confusion_matrix(predictions_DT, y_test)\nprint('\\nConfusion Matrix:')\nprint(cf_matrix)","098a44b1":"print(classification_report(y_test, predictions_XGB))\ngroup_names = ['TN','FP','FN','TP']\ngroup_counts = ['{0:0.0f}'.format(value) for value in\n                cf_matrix.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in\n                     cf_matrix.flatten()\/np.sum(cf_matrix)]\nlabels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')","f8c7bc7a":"import lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfeature_importances = np.zeros(X.shape[1])\n\n# Create the model with several hyperparameters\nmodel = lgb.LGBMClassifier(objective='binary', \n                           boosting_type = 'goss', \n                           n_estimators = 10000, \n                           class_weight = 'balanced')\n\n# Fit the model twice to avoid overfitting\nfor i in range(2):\n    \n    # Split into training and validation set\n    train_features, valid_features, train_y, valid_y = train_test_split(X,\n                                                                        y,\n                                                                        test_size = 0.25, \n                                                                        random_state = i)\n    \n    # Train using early stopping\n    model.fit(train_features, train_y, early_stopping_rounds=100, eval_set = [(valid_features, \n                                                                               valid_y)], \n              eval_metric = 'auc', verbose = 200)\n    predictions_LGB = model.predict(valid_features)\n\n    print('\\nLGB accuracy:', accuracy_score(predictions_LGB, valid_y))\n    print('\\nConfusion Matrix:')\n    print(confusion_matrix(predictions_LGB, valid_y))\n    \n    # Record the feature importances\n    feature_importances += model.feature_importances_","d40804ff":"print(classification_report(y_test, predictions_LGB))\ngroup_names = ['TN','FP','FN','TP']\ngroup_counts = ['{0:0.0f}'.format(value) for value in\n                cf_matrix.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in\n                     cf_matrix.flatten()\/np.sum(cf_matrix)]\nlabels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')","19da196a":"# Make sure to average feature importances! \nfeature_importances = feature_importances \/ 2\nfeature_importances = pd.DataFrame({'feature': list(X.columns),\n                                    'importance': feature_importances}\n                                  ).sort_values('importance', ascending = False)\n\nfeature_importances.head(10)","cc3b6c38":"# Find the features with zero importance\nzero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\nprint('There are %d features with 0.0 importance' % len(zero_features))\nfeature_importances.tail()","c42b71d6":"def plot_feature_importances(df, threshold = 0.9):\n    \"\"\"\n    Plots 10 most important features and the cumulative importance of features.\n    Prints the number of features needed to reach threshold cumulative importance.\n    \n    Parameters\n    --------\n    df : dataframe\n        Dataframe of feature importances. Columns must be feature and importance\n    threshold : float, default = 0.9\n        Threshold for prining information about cumulative importances\n        \n    Return\n    --------\n    df : dataframe\n        Dataframe ordered by feature importances with a normalized column (sums to 1)\n        and a cumulative importance column\n    \n    \"\"\"\n    \n    plt.rcParams['font.size'] = 18\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] \/ df['importance'].sum()\n    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    # Cumulative importance plot\n    plt.figure(figsize = (8, 6))\n    plt.plot(list(range(len(df))), df['cumulative_importance'], 'r-')\n    plt.xlabel('Number of Features'); plt.ylabel('Cumulative Importance'); \n    plt.title('Cumulative Feature Importance');\n    plt.show();\n    \n    importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n    print('%d features required for %0.2f of cumulative importance' % (importance_index + 1, threshold))\n    \n    return df","499472bc":"norm_feature_importances = plot_feature_importances(feature_importances,\n                                                   threshold = 0.99)","cf1b54aa":"We Logistic Regression gave litter better preformance than Decision tree algorithm","e586644a":"#### Now let's take a look at the object features: Ad Topic Line, City, Country, Timestamp ","4346d99b":"Let's go over each of the non-object features one by one:\n1. **Daily Time Spent on Site:** We see users spend between 32 min to 91 min on the site with a mean value of 65min, which is quite a large amount of time. This indicates that it is a popular site. We would like to see if there is any corellation with time spend on the site and 'clicked on Ad'.\n\n2. **Age:** The user age ranges from 19years to 61 years with a mean of 36 years, which tells us that the target users are adults.\n\n3. **Area Income:** The minimum users income is around 13k and the maximum user income is 79k, which tells us that the users belongs to different social classes. We would like to further investigate how the income is corelates with the click on the ad.\n\n4. **Daily Internet Usage:** The daily internet use ranges from 104min to 269min. Out of total daily internet use, users spend quite a large amount of time on the site, which ranges from 32 to 91 min. We will check if they both are relates to each other in some way.\n\n5. **Male:** 48% of the users are male. We will check if gender affects the rate of click on the ad. \n\n6. **Clicked on Ad:** From the cell above and the cell below, we see that 50% of the ads were clicked and 50% of the ad weren't clicked by the user. Which tells us that our ad dataset is balanced, which will have a positive affect on training accuracy. ","73905df6":"### Logistic Regression\n","cd891076":"Users who spen less time on the internet tends to click on add regardless the age range. On the other hand, users younger than 45 seems to spend more time on the internet but avoid to click on ad.","fd48e41c":"Data seems very balanced interms of Male feature and the target feature. ","a2a98dba":"### Categorical Features","174dd907":"### Missing Values:\n","e87c848f":"Pairplot represents the relationshi between the target feature and the explanatory features. \n\nWe also see that users with higher area income who spends more time on the site does not click on ad also relatively younger users with higher income do not click on ads. So this group of users could be the target users. \n\nAgain the users with higher area income who more likely to spend longer time on the site do not click on ad.\n","b12b9f50":"We see mjority of the users are in the age range 25 to 45, which could be our target age group for ad recommnedation. We need to check if this age group is actually clicking on the ad or no. ","1ac7a78d":"### Numerical features","3eded098":"Here, we see similar result with the user group with higher area income and Time spent on the site.","4161ec3b":"From above cell, we see there is few null values in the dataset, which is a not bad. We will work on this later in the notebook","b538daa1":"Our confusion matrix tells us that the total number of accurate predictions is 125 + 109 = 234. On the other hand, the number of incorrect predictions is 9 + 7 = 16. We can be satisfied with the prediction accuracy of our model.\n\nIt can be concluded that the Decision Tree model showed better performances in comparison to the Logistic Regression model. The confusion matrix shows us that the 234 predictions have been done correctly and that there are only 16 incorrect predictions. Additionally, Decision Tree accuracy is better by about 3% in comparison to the first regression model.","cefffb61":"### Problem Statement\nWe have a advertising dataset of a marketing agency. Goal is to develop a ML algorithm that predicts if a particular user will click on an advertisement. The dataset has 10 features:\n\n'Daily Time Spent on Site', \n'Age', \n'Area Income',\n'Daily Internet Usage', \n'Ad Topic Line',\n'City',\n'Male',\n'Country',\nTimestamp' \n'Clicked on Ad'.\n\n**'Clicked on Ad'** is the categorical target feature, which has two possible values: 0 (user didn't click) and 1(user clicked). ","79946253":"This plot tells us that all the users who spent less time on the site and has more area income tend to click on ad. On the other hand, user group with higher area income who spend more time on the site does not seem to click on the ad. Which is interesting, it could be the add is not personalized to this group of users. ","3d2c8ff7":"### Import data","9e4694ac":"### EDA: Exploratory Data Analysis","1e5e9b45":"From above cell we see that all ad topic lines are unique, which indicates this features has less chace of carying any useful information for the prediction model. There are 969 diffirent cities out of 237 countries. These indicates that the users are not from a spcecific demograhic but from all over the world. Even though we see France repeates 9 times, meaning highest number of visitors are from France but still it just 9 of them. \n\nLet's see if there is any other countries with same number of users.\n","b6f4ee65":"Here we see Daily Time Spent on Site, Age, Area Income, Daily Intenert Usage are highly correlate with the target variable. Which indicates they are important features and will be useful for ML model.\n\nWe also notice that Daily Time spent on site has strong correlation with other with daily intenet usage, Age, Area Inocme and so does daily internet usage. ","705ba851":"**Confusion Matrix:** The users that are predicted to click on commercials and the actually clicked were 112, the people who were predicted not to click on the commercials and actually did not click on them were 129.\n\nThe people who were predicted to click on commercial and actually did not click on them are 5, and the users who were not predicted to click on the commercials and actually clicked on them are 2.\n\nWe have only a few mislabelled points which is not bad from the given size of the dataset.\n\nClassification Report:\n\nFrom the report obtained, the precision & recall are 0.96 which depicts the predicted values are 98% accurate. Hence the probability that the user can click on the commercial is 0.96 which is a great precision value to get a good model.","946a0814":"### Feature Importances","c6e56938":"We have total 1000 training examples and 10 features.","761b9618":"This plot tells us that the younger users spceially from age 20 to 40, spent most time on the site. So this group of users could be good target group for the ad campaign. We can also say that if a product is targetting a population whose age does not fall into the range 19 to 61, this site is not right platform to advertize the product. \n\nThis plot tells us that all the users who spent less time on the site tend to click on ad. On the other hand, among the 20 to 55 years user group who spent most time on the site apperently don't click on the ad, whereas the same user group who spents less time clicks on ad.","fefb14c3":"### Reference:\n1. https:\/\/www.kaggle.com\/farhanmd29\/predicting-customer-ad-clicks\/notebook\n2. https:\/\/www.kaggle.com\/imprime\/logistic-regression-with-ad-click-dataset\/data","94a8a72d":"Since there are 237 countries in the dataset and no single country is too dominant. It might be better to remove these features from the dataset. ","45db217a":"We see daily internet use and daily time spent on the site is linearly correlated, which make sense. "}}