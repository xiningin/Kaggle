{"cell_type":{"b7808867":"code","a0772c90":"code","9701b4be":"code","8eb371ce":"code","630ecc54":"code","491a5012":"code","d693b6c4":"code","fa10df54":"code","d9f24f16":"code","be67849f":"code","d88f1cfa":"code","09e5cb10":"code","aa034eaa":"code","b8dbb9da":"markdown","c74f9f6b":"markdown","20accd57":"markdown","015e6946":"markdown","e8513853":"markdown","a10cc678":"markdown","149bca00":"markdown","1e372f53":"markdown","db500be4":"markdown","fe083716":"markdown","e59138db":"markdown","f47a964f":"markdown","0ffc057d":"markdown","c8ad413e":"markdown","e53a9639":"markdown"},"source":{"b7808867":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a0772c90":"x_data = np.load('\/kaggle\/input\/sign-language-digits-dataset\/X.npy')\ny_data = np.load('\/kaggle\/input\/sign-language-digits-dataset\/Y.npy')\n\nprint('Shape of x data: ', x_data.shape)\nprint('Shape of y data: ', y_data.shape)","9701b4be":"plt.figure(figsize=(10,10))\nplt.subplot(1,3,1)\nplt.imshow(x_data[0], cmap='gray')\nplt.subplot(1,3,2)\nplt.imshow(x_data[100], cmap='gray')\nplt.title('Example Images')\nplt.subplot(1,3,3)\nplt.imshow(x_data[500], cmap='gray');","8eb371ce":"# numpy reshape function is used to change matrix format:\nx_data = x_data.reshape(-1, 64, 64, 1)\nprint('New shape of x_data: ', x_data.shape)\n\n# y_data is already in proper matrix format:\nprint('Shape of y_data: ', y_data.shape)","630ecc54":"# Using 80% of the data for training and 20% for testing. \n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.20, random_state = 1)\n\nprint('Shape of x_train: ', x_train.shape)\nprint('Shape of y_train: ', y_train.shape)\nprint('....')\nprint('Shape of x_test: ', x_test.shape)\nprint('Shape of y_test: ', y_test.shape)","491a5012":"from keras.preprocessing.image import ImageDataGenerator\n\ntrain_gen = ImageDataGenerator(\n            rotation_range = 5,        # 5 degrees of rotation will be applied\n            zoom_range = 0.1,          # 10% of zoom will be applied\n            width_shift_range = 0.1,   # 10% of shifting will be applied\n            height_shift_range = 0.1)  # 10% of shifting will be applied\n\ntrain_gen.fit(x_train)","d693b6c4":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n\n# Creating model structure\nmodel = Sequential()\n# Adding the first layer of CNN\nmodel.add(Conv2D(filters=20, kernel_size=(4,4), padding='Same', activation='relu', input_shape=(64, 64, 1)))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.15))\n# Adding the second layer of CNN\nmodel.add(Conv2D(filters=30, kernel_size=(3,3), padding='Same', activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.15))\n# Flattening the x_train data\nmodel.add(Flatten()) \n# Creating fully connected NN with 4 hidden layers\nmodel.add(Dense(220, activation='relu'))\nmodel.add(Dropout(0.15))\nmodel.add(Dense(150, activation='relu'))\nmodel.add(Dropout(0.15))\nmodel.add(Dense(80, activation='relu'))\nmodel.add(Dropout(0.15))\nmodel.add(Dense(10, activation='softmax'))","fa10df54":"# Defining the optimizer\n\nfrom keras.optimizers import Adam\n\noptimizer = Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.99)","d9f24f16":"model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics=['accuracy'])","be67849f":"batch_size = 100\nepochs = 25","d88f1cfa":"history = model.fit_generator(train_gen.flow(x_train, y_train, batch_size = batch_size), \n                                                  epochs = epochs, \n                                                  validation_data = (x_test, y_test), \n                                                  steps_per_epoch = x_train.shape[0] \/\/ batch_size)","09e5cb10":"# Visiualize the validation loss and validation accuracy progress:\n\nplt.figure(figsize=(13,5))\nplt.subplot(1,2,1)\nplt.plot(history.history['val_loss'], color = 'r', label = 'validation loss')\nplt.title('Validation Loss Function Progress')\nplt.xlabel('Number Of Epochs')\nplt.ylabel('Loss Function Value')\n\nplt.subplot(1,2,2)\nplt.plot(history.history['val_accuracy'], color = 'g', label = 'validation accuracy')\nplt.title('Validation Accuracy Progress')\nplt.xlabel('Number Of Epochs')\nplt.ylabel('Accuracy Value')\nplt.show()","aa034eaa":"# Confusion Matrix\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# First of all predict labels from x_test data set and trained model\ny_pred = model.predict(x_test)\n\n# Convert prediction classes to one hot vectors\ny_pred_classes = np.argmax(y_pred, axis = 1)\n\n# Convert validation observations to one hot vectors\ny_true_classes = np.argmax(y_test, axis = 1)\n\n# Create the confusion matrix\nconfmx = confusion_matrix(y_true_classes, y_pred_classes)\nf, ax = plt.subplots(figsize = (8,8))\nsns.heatmap(confmx, annot=True, fmt='.1f', ax = ax)\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show();","b8dbb9da":"# 9. Evaluation of the Predictions\n\nNormally I should have dived the data set into train and test. \n\nAnd then divide the train data into train and validation data. \n\nI should use the train and validation data for my model learning and use test data for predicting and evaluating with the true labels.\n\nBut I didn't choose this path. Because:\n\n1. I don't have big number of samples. So I did't want to cut some of my training data for validation.\n2. This is a study for my learning (And hopefully those who read this notebook)","c74f9f6b":"# 5. Creating Model\n\nI will create a CNN and fully connected neural network (NN) model by intuition. Model plan will be like:\n\n    model plan = conv --> max pool --> dropout --> conv --> max pool --> dropout --> flatten --> fully connected\n\nWhere:\n\n    conv            : Filters the image features using kernels\n    max pool        : Extracts the most important feature in a defined matrix (pool_size)\n    dropout         : Helps to eliminate overfitting by randomly choosing and not using the nodes\n    flatten         : Flattens the data into a vector, which will be the input vector for NN\n    fully connected : Means feeding input features to the designed NN ","20accd57":"# 8. Fitting the Model","015e6946":"# 6. Compiling Model\n\nThree parameters are important for compiling the model:\n\na. Choosing the defined optimizer\n\nb. Choosing loss function according to the application (Because this study is categorical classification 'categorical_crossentropy' is chosen\n\nc. Choosing the parameter how the model is going to evaluate its learning parameter (Which will be accuracy for this study)","e8513853":"The diagonal line in the middle shows how many of the predictions and the true labels match together. \n\nOne can see the model mixed 8 times, the image represents the digit 0 and guessed those as digit 5.\n\nThis is the conclusion of my study. \n\nLastly I would like to thank to my teachers:\n* The DataI Team https:\/\/www.kaggle.com\/kanncaa1 , https:\/\/dataiteam.com , https:\/\/www.udemy.com\/user\/datai-team\/\n* Mustafa Vahit Keskin https:\/\/www.veribilimiokulu.com\/author\/mvk\/ , https:\/\/www.udemy.com\/user\/mustafa-vahit-keskin\/\n\nfor helping me progress in data science, machine learning and deep learning subjects. \n\nI hope you enjoyed my notebook, if so please don't forget to upvote.","a10cc678":"Have a look at some of the images:","149bca00":"# 4. Data Augmentation\n\nThis data set doesn't have a great number of samples. But I can increase my training data by apply data augmentation. Some ramdom images from the x_train will be rotated, zoomed and shifted. I didn't use flipping the images because it wouldn't be a proper decision for this data set.   ","1e372f53":"## 1. Loading the Data Sets","db500be4":"Optimizer will be used for compiling the model. Optimizer is used to define our model's learning rate. Learning rate is an important hyperparameters which effects two things:\n\na. Finding the smallest cost function (If it is a big value, model can miss to find the lowest value of cost function while applying gradient descent)\n\nb. Speed of the model (If it is a really small value, model will be slow in learning)","fe083716":"## 3. Train and Test Split","e59138db":"# 2. Reshaping Data\n\nIn order to use the datas with keras library. I should have x_data as (2062, 64, 64, 1) and y_data as (2062, 10) matrix formats.\n\n    Shape of x_data is (2062, 64, 64, 1), where:\n        * 2062 is the number of images\n        * 64, 64 are the pixel sizes for width and height\n        * 1 is the channel value which represents the grayscale\n\n    Shape of y_data is (2062, 10), where:\n        * 2062 is the number of images\n        * 10 is the number of classes","f47a964f":"As one can see that by time the model gets better for predicting labels (The decrease of the loss function and the increase of the accuracy).\n\nNow let's see the comparison of predictions and the real labels by using confusion matrix.","0ffc057d":"# Sign Language Digit Classifier with CNN\n\nIn this notebook I will apply convolutional neural network (CNN) algorithm to a deep learning model in order to classify sign language digits from photographs.\n\nIn the data set, 218 students are used for photographing 10 sign language digits (Between the number 0 and 9) so there should be 2180 images. But the data set consists of 2062 images. Probably some of the images are removed by the data supplier bacause of inproper quality. So there should be around 200 images for each digit. \n\nImages are in grayscale (black and white) and each image has 64 x 64 pixels of width and height size.\n\nImages are already saved as numpy arrays ('X.npy'). So I didn't have to load jpeg images and convert them to numpy arrays. \n\nLabels of the images are also saved as a different numpy array file ('Y.npy') and already encoded. So I dind't have to apply categorical encoding for the labels. \n\n**It is important to set a goal and draw the work plan accordingly. The aim of this study is to achieve a good classification model with a validation accuracy of at least 0.90.**\n\n**WORK PLAN:**\n    1. Loading the data sets\n    2. Reshaping data\n    3. Splitting data sets into train and test\n    4. Applying data augmentation to x_train data set\n    5. Creating a CNN model & defining optimizer\n    6. Compiling the model\n    7. Defining epoch and batch sizes\n    8. Fitting the model\n    9. Evaluating the predictions by loss function, validation accuracy and confusion matrix.","c8ad413e":"The model is trained and the it reached to a validation accuracy of 0.92 which is just right for my initial aim. \n\nNow let's see the progress of the model learning and compare the prediction with the real labels of the test data.","e53a9639":"# 7. Defining Epoch and Batch Size\n\nWe have 1649 training images and we will divide them into batches of 100 images. So it will take around 16 iterations to complete 1 epoch. This will repeat 25 times."}}