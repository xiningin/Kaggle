{"cell_type":{"a04dcd57":"code","585c5bac":"code","04c6d862":"code","86e70a15":"code","40058c5a":"code","2a82ced7":"code","04eb5535":"code","07924719":"code","5fd5fdb9":"code","6e9a1794":"code","b5f764e6":"code","fc191354":"code","bedec66a":"code","3346893a":"code","798b5524":"code","ff0ea582":"code","61ec7022":"code","b86dd5ca":"code","175cf78e":"code","2959e652":"code","deb26b28":"code","b2d0bcee":"code","8af8403b":"code","a77995b0":"code","8450fc79":"code","dfcac839":"code","f4816bdf":"code","be85ac56":"code","ed1bedd4":"code","579c24ad":"code","a487cfce":"code","bcab74f3":"code","eee38d0d":"code","40f57db6":"code","b3da75be":"code","9c9e0e9c":"code","b90a2905":"code","e93883d5":"code","64c9b0fc":"code","af568ddd":"code","e88c1acf":"code","3a0afd81":"code","fb1d28ad":"code","ce76057e":"code","1b60d4fb":"code","e67a7bee":"code","50975932":"markdown","85ad9977":"markdown","1534915f":"markdown","5572fcad":"markdown","e5a0fcc8":"markdown","07c9fc4e":"markdown","4e242f81":"markdown","5a2685cf":"markdown","9c8dc485":"markdown","77b08f97":"markdown","ca3bc118":"markdown","9f41f89b":"markdown","afcffbee":"markdown","1abe7890":"markdown","53dc01cc":"markdown","401a151f":"markdown","dac8ec13":"markdown","83cea37c":"markdown","66e42732":"markdown","e13b02cf":"markdown","d5186271":"markdown","44401c89":"markdown","026bfb95":"markdown","e453fb05":"markdown","434d3d9c":"markdown","3dba3364":"markdown","691bac34":"markdown","b470cee9":"markdown","ac0e6601":"markdown","2008c1ae":"markdown","ad273f6b":"markdown","3bfdba03":"markdown","4f126705":"markdown","32ccf334":"markdown","2813061c":"markdown","98afeb35":"markdown","7152c7e8":"markdown","db1d7964":"markdown","aae2e9a0":"markdown","320620be":"markdown","0be143a8":"markdown","5b87f153":"markdown","a6c235e0":"markdown"},"source":{"a04dcd57":"# Let's load in the dataset then check the head\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\ndf = pd.read_csv('\/kaggle\/input\/churn-modelling\/Churn_Modelling.csv')\n\ndf.head()","585c5bac":"# Let's take a look at the summary statistics\ndf.describe()","04c6d862":"# Drop RowNumber\ndf.drop('RowNumber', axis=1, inplace=True)","86e70a15":"# Check for null values\ndf.isnull().values.any()","40058c5a":"# Check number of unique values for CustomerId\ndf['CustomerId'].nunique()","2a82ced7":"# Drop CustomerId\ndf.drop('CustomerId', axis=1, inplace=True)","04eb5535":"# Let's look at the data once again.\ndf.head()","07924719":"# Drop Surname\ndf.drop('Surname', axis=1, inplace=True)","5fd5fdb9":"import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib.patches import Rectangle\n\nfig, ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(df.corr().round(2), annot=True, ax=ax)\nhighlight_color = 'blue'\nax.add_patch(Rectangle((0, 8), 9, 1, fill=False, edgecolor=highlight_color, lw=3))\nax.add_patch(Rectangle((8, 0), 1, 9, fill=False, edgecolor=highlight_color, lw=3))\nax.set_title('Feature Correlations')\n\nfor axis in [ax.get_xticklabels(), ax.get_yticklabels()]:\n    label = [i for i in axis if i.get_text() == 'Exited']\n    [(l.set_weight('bold'), l.set_size(25), l.set_color(highlight_color)) for l in label]","6e9a1794":"def feature_bar_graph(values, title, xlab, ylab='Proportion of Exited', rotate_x=False):\n    temp_df = pd.DataFrame({'Feature': values, 'Exited': df['Exited']})\n    gb_obj = temp_df.groupby('Feature')['Exited'].mean()\n    plt.bar(gb_obj.index.astype(str), gb_obj.values, width=0.5)\n    plt.title(title, fontsize=15)\n    plt.xlabel(xlab, fontsize=15)\n    plt.ylabel(ylab, fontsize=15)\n    if rotate_x:\n        plt.xticks(rotation=45)\n        \n# Bin Age and plot a bar graph\nbin_age = pd.qcut(df['Age'].values, q=5).astype(str)\nfeature_bar_graph(bin_age, 'Age Analysis', 'Binned Age', rotate_x=True)","b5f764e6":"feature_bar_graph(df['Tenure'].values, 'Tenure Analysis', 'Tenure')","fc191354":"bin_cc = pd.qcut(df['CreditScore'].values, q=5).astype(str)\nfeature_bar_graph(bin_cc, 'Credit Score Analysis', 'Binned Credit Score', rotate_x=True)","bedec66a":"bin_salary = pd.qcut(df['EstimatedSalary'].values, q=5).astype(str)\nfeature_bar_graph(bin_salary, 'Salary Analysis', 'Estimated Salary', rotate_x=True)","3346893a":"feature_bar_graph(df['IsActiveMember'].values, 'Active Member Analysis', 'Is Active Member')","798b5524":"feature_bar_graph(df['NumOfProducts'].values, 'Number of Products Analysis', 'Number of Products')","ff0ea582":"# I will save a copy of our dataframe then one-hot encode it.\ndf_old = df.copy()\ndf = pd.get_dummies(df, drop_first=True)","61ec7022":"# Separate into features and target variable\nfeatures = df.drop('Exited', axis=1)\ntarget = df['Exited']","b86dd5ca":"# Import all libraries that we will need and split the data into training and testing sets\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.pipeline import Pipeline\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import f1_score, make_scorer\nimport os\nrs = {'random_state': 42}\n\nX_train, X_test, y_train, y_test = train_test_split(features, target, train_size=0.6, **rs)\nX_val, X_test, y_val, y_test, = train_test_split(X_test, y_test, train_size=0.5, **rs)","175cf78e":"def train_models(X_train, X_val, X_test, y_train, y_val, y_test):\n    log_reg = LogisticRegression(**rs)\n    nb = BernoulliNB()\n    knn = KNeighborsClassifier()\n    svm = SVC(**rs)\n    mlp = MLPClassifier(max_iter=5000, **rs)\n    dt = DecisionTreeClassifier(**rs)\n    et = ExtraTreesClassifier(**rs)\n    rf = RandomForestClassifier(**rs)\n    xgb = XGBClassifier(**rs, verbosity=0)\n    scorer = make_scorer(f1_score)\n\n    clfs = [('Logistic Regression', log_reg), ('Naive Bayes', nb),\n            ('K-Nearest Neighbors', knn), ('SVM', svm), \n            ('MLP', mlp), ('Decision Tree', dt), ('Extra Trees', et), \n            ('Random Forest', rf), ('XGBoost', xgb)]\n    pipelines = []\n    scores_df = pd.DataFrame(columns=['model', 'val_score', 'test_score'])\n    test_scores = []\n    for clf_name, clf in clfs:\n        pipeline = Pipeline(steps=[\n            ('scaler', StandardScaler()),\n            ('classifier', clf)])\n        pipeline.fit(X_train, y_train)\n        val_score = cross_val_score(pipeline, X_val, y_val, scoring=scorer, cv=3).mean()\n        print(f'{clf_name}\\n{\"-\" * 30}\\nModel Score Validation: {val_score:.4f}')\n        test_score = f1_score(y_test, pipeline.predict(X_test))\n        print(f'Model Score Testing: {test_score:.4f}\\n\\n')\n        pipelines.append(pipeline)\n        scores_df = scores_df.append({'model': clf_name, \n                                      'val_score': val_score, \n                                      'test_score': test_score}, ignore_index=True)\n    return pipelines, scores_df\n\npipelines1, scores1 = train_models(X_train, X_val, X_test, y_train, y_val, y_test)","2959e652":"scores1.sort_values('test_score', ascending=False)","deb26b28":"# Create classification report\nfrom sklearn.metrics import classification_report, confusion_matrix\nmodel = pipelines1[4]\nprint(model['classifier'])\npreds = model.predict(X_test)\nprint(classification_report(y_test, preds))","b2d0bcee":"# Create confusion matrix\ncfm = confusion_matrix(y_test, preds)\nprint(cfm)","8af8403b":"# Create confusion matrix with seaborn\ndef create_confusion_matrix(y_true, y_pred):\n    cfm = confusion_matrix(y_true, y_pred)\n    fig, ax = plt.subplots(figsize=(7,7))\n    sns.heatmap(cfm, annot=True, annot_kws={\"size\": 15}, ax=ax,\n                cbar=False, square=True, cmap='Blues', fmt='d')\n    sns.set(font_scale=1.5)\n    plt.xlabel('Predicted', fontsize=15)\n    plt.ylabel('Actual', fontsize=15)\n    ax.set_xticklabels(np.unique(y_pred))\n    ax.set_yticklabels(np.unique(y_pred))\n    plt.title('Confusion Matrix\\nChurn Data', fontsize=18)\n    \ncreate_confusion_matrix(y_test, preds)","a77995b0":"# Check class distribution\ny_train.value_counts()","8450fc79":"# Generate synthetic examples of minority class\nfrom imblearn.over_sampling import ADASYN\n\nadasyn = ADASYN(**rs)\nX_train, y_train = adasyn.fit_resample(X_train, y_train)","dfcac839":"y_train.value_counts()","f4816bdf":"# Training with synthetic dataset\npipelines2, scores2 = train_models(X_train, X_val, X_test, y_train, y_val, y_test)","be85ac56":"scores2.sort_values('test_score', ascending=False)","ed1bedd4":"# Model classification report\nmodel = pipelines2[-1]\nprint(model['classifier'])\npreds = model.predict(X_test)\nprint(classification_report(y_test, preds))","579c24ad":"create_confusion_matrix(y_test, preds)","a487cfce":"feat_imp = pd.DataFrame({'feature': features.columns, 'importance': model['classifier'].feature_importances_})\nfeat_imp.sort_values('importance', ascending=False, inplace=True)\nfeat_imp","bcab74f3":"# RFE with Logistic Regression\nfrom sklearn.feature_selection import SelectFromModel, RFE\n\nparams = [StandardScaler(), XGBClassifier(**rs, verbosity=0), X_train, X_val, X_test, y_train, y_val, y_test]\n\ndef create_pipeline(feature_selection, scaler, classifier, X_train, X_val, X_test, y_train, y_val, y_test):\n    pipeline = Pipeline(steps=[('feature_selection', feature_selection(LogisticRegression(max_iter=5e3))),\n                        ('scaler', scaler),\n                        ('classifier', classifier)])\n    scorer = make_scorer(f1_score)\n    pipeline.fit(X_train, y_train)\n    chosen_features = X_train.iloc[:, pipeline['feature_selection'].get_support(indices=True)]\n    print(f'Feature Selection Method {feature_selection.__name__} selected {len(chosen_features.columns)} features')\n    print(f'Model Score Validation: {cross_val_score(pipeline, X_val, y_val, scoring=scorer, cv=3).mean():.4f}')\n    print(f'Model Score Testing: {f1_score(y_test, pipeline.predict(X_test)):.4f}')\n    return pipeline, chosen_features\n\nrfe_pipe, rfe_feats = create_pipeline(RFE, *params)\n","eee38d0d":"# SelectFromModel with Logistic Regression\nsfm_pipe, sfm_feats = create_pipeline(SelectFromModel, *params)","40f57db6":"# Check colums\nsfm_feats.columns","b3da75be":"# Perform feature selection using feature importance\nfeat_imp['CumPerc'] = np.cumsum(model['classifier'].feature_importances_)\/sum(model['classifier'].feature_importances_)\ncutoff = 0.9\nnew_feats = feat_imp[feat_imp['CumPerc'] < cutoff]\nnew_feats","9c9e0e9c":"# Create a new pipeline using new features\nX_train, X_val, X_test = X_train[new_feats['feature']], X_val[new_feats['feature']], X_test[new_feats['feature']]\n\nfi_pipe = Pipeline(steps=[('scaler', StandardScaler()),\n                          ('classifier', XGBClassifier(**rs, verbosity=0))])\nscorer = make_scorer(f1_score)\nfi_pipe.fit(X_train, y_train)\nprint(f'Model Score Validation: {cross_val_score(fi_pipe, X_val, y_val, scoring=scorer, cv=3).mean():.4f}')\nprint(f'Model Score Testing: {f1_score(y_test, fi_pipe.predict(X_test)):.4f}')","b90a2905":"print(classification_report(y_test, fi_pipe.predict(X_test)))","e93883d5":"# Create class_weight dict and pass this as an argument when creating the classifier\n\nweight = (y_val == 0).sum() \/ (y_val == 1).sum()\ncw_pipe = Pipeline(steps=[('scaler', StandardScaler()),\n                          ('classifier', XGBClassifier(scale_pos_weight=weight, **rs, verbosity=0))])\ncw_pipe.fit(X_train, y_train)\nprint(f'Model Score Validation: {cross_val_score(cw_pipe, X_val, y_val, scoring=scorer, cv=3).mean():.4f}')\nprint(f'Model Score Testing: {f1_score(y_test, cw_pipe.predict(X_test)):.4f}')","64c9b0fc":"print(classification_report(y_test, cw_pipe.predict(X_test)))","af568ddd":"test_preds = fi_pipe.predict(X_test)\nprint(classification_report(y_test, test_preds))","e88c1acf":"# Create confusion matrix\ncfm = confusion_matrix(y_test, test_preds)\nprint(cfm)","3a0afd81":"# Create confusion matrix with seaborn\ncreate_confusion_matrix(y_test, test_preds)","fb1d28ad":"# Plot roc\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\nlogit_roc_auc = roc_auc_score(y_test, test_preds)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, fi_pipe.predict_proba(X_test)[:,1])\n\nfig = plt.figure()\nplt.tight_layout()\nplt.subplots_adjust(bottom=0.15)\nplt.plot(false_positive_rate, true_positive_rate, label=f'XGBoost (area = {logit_roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], '--', color='grey')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic plot')\nplt.legend(loc=\"lower right\")\nplt.show()","ce76057e":"# Create out-of-sample data\noos = pd.DataFrame({'RowNumber': [10000, 10001, 10002, 10003, 10004], \n                    'CustomerId': [15849068, 15784210, 15690576, 15984739, 15893045],\n                   'Surname': ['Garcia', 'Miller', 'Rodriguez', 'Lee', 'Hill'],\n                   'CreditScore': [145, 566, 392, 669, 478],\n                   'Geography': ['France', 'Germany', 'Spain', 'France', 'France'],\n                    'Gender': ['Male', 'Female', 'Male', 'Male', 'Female'],\n                   'Age': [46, 32, 25, 66, 47],\n                   'Tenure': [1, 5, 4, 4, 8],\n                   'Balance': [14569.43, 0.00, 129804.44, 1589.04, 0.00],\n                   'NumOfProducts': [3, 3, 1, 3, 2],\n                   'HasCrCard': [1, 1, 1, 1, 1],\n                   'IsActiveMember': [0, 1, 1, 0, 1],\n                   'EstimatedSalary': [164032.87, 56890.44, 98349.51, 57098.64, 122548.65]})\n\noos","1b60d4fb":"# Let's write a function to preprocess the dataset and make sure it matches with our original\ndef preprocess_data(df):\n    df = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)\n    df = pd.get_dummies(df, drop_first=True)\n    df = df[new_feats['feature']]\n    return df\n\noos_processed = preprocess_data(oos)\nassert all(oos_processed.columns == X_train.columns)","e67a7bee":"# Predict and append to the dataframe\noos['predictions'] = fi_pipe.predict(oos_processed)\noos","50975932":"## Hyperparameter Tuning","85ad9977":"The XGBoost model performs the best in our second training run. Let's dig deeper and look at the feature importance of the model. ","1534915f":"We will first do a baseline test with all features and no hyperparameter tuning to see which models perform best on our dataset. The only preprocessing we will do is feature scaling particularly to help our non tree-based models.","5572fcad":"We will use a variety of models and see which one performs the best.<br\/>\nThe models include:\n* Logistic Regression\n* Naive Bayes\n* k-nearest neighbors\n* SVM\n* Neural Network\n* Decision Tree\n* Extra Trees\n* Random Forest\n* XGBoost\n\nFor this training run we will use 3-fold cross validation using `cross_val_score`. All tree based models will have a random seed to ensure reproducibility.","e5a0fcc8":"Weighting the positive class seems to decrease the model's overall f1_score. Let's check the classification report.","07c9fc4e":"The model is very good at predicting negative examples as seen from the `f1-score` for class 0. However, it struggles with positive examples. This can be seen from the low recall score that it gets for class 1.","4e242f81":"## Model Training Run 1","5a2685cf":"## EDA","9c8dc485":"SelectFromModel chose 1 feature. Our model's score did not improve. What is interesting to note though is we are able to achieve a respectable score with just `Age`.","77b08f97":"With our final model we are able to address some of the problems regarding sensitivity and arrive at a respectable f1_score. Let's make predictions on out-of-sample data.","ca3bc118":"## Out of Sample Prediction","9f41f89b":"## Model Training Run 2","afcffbee":"## Model Analysis","1abe7890":"Let's now look at the `CustomerId` column in our dataset.","53dc01cc":"`Tenure` doesn't seem to show any trends when it comes to churn. The proportions are roughly the same for each of the values.","401a151f":"A look at the confusion matrix verifies are findings. Model sensitivity has increased although specificity has taken a small hit. This is an acceptable compromise. Let's look at feature importance.","dac8ec13":"When adding the `weight` parameter, we are able to make improvements to sensitivity, but specificity takes a large hit. It is debatable whether we would keep a change like this in our model. Using it would certainly allow us to detect more churn among customers but this would be at the cost of an increase in type 1 errors (false positives). I will opt to leave this out moving forward.","83cea37c":"`RowNumber` is just a number that identifies each row. We can drop it and use the dataframe's index instead.","66e42732":"Looking at `IsActiveMember` we can see that being inactive is correlated to customer churn.","e13b02cf":"When looking at the class distribution of our training set, we see a significant skew where most of our data consists of negative (0) examples. To overcome this problem, we could either undesample and majority class or oversample the minority. Undersampling will leave us with less training data so we will generate synthetic examples of our minority class. There are many methods to do this but we will be using ADASYN (Adaptive Synthetic) sampling for this purpose.","d5186271":"Looking at the confusion matrix reinforces our previous analysis of the model struggling to correctly classify positive examples.","44401c89":"We can assume from `CustomerId` that every entry in the dataset is a unique individual. Since `CustomerId` is unique, it does not give us any information. We can drop it.","026bfb95":"Looking at `EstimatedSalary`, the trend continues and there are no differences in the values.","e453fb05":"## Feature Selection","434d3d9c":"# Analyzing Customer Churn\n\n## Introduction\n\nThe [data](https:\/\/www.kaggle.com\/shubh0799\/churn-modelling) we are using for this analysis consists of customers subscribed to services at a company. Our goal is to explore and solve the problem of predicting **customer churn**. The dataset contains features like Age, Tenure, Salary, and Credit Score; features which could potentially give insight as to why a customer end their subscription or stop buying products from a company.\n<br\/><br\/>The workflow will cover EDA, where we explore the features of the dataset and try to determine which features are correlated with customer churn. We will then do feature engineering and selection with the intention of creating a predictive model that is able to predict whether a given customer will churn.\n\n### References\n * [numpy API reference](https:\/\/numpy.org\/doc\/stable\/reference\/index.html)\n * [pandas API reference](https:\/\/pandas.pydata.org\/docs\/reference\/index.html#api)\n * [scikit-learn documentation](https:\/\/scikit-learn.org\/stable\/)\n * [xgboost parameter documentation](https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html#parameters-for-tree-booster)\n * [original SMOTE paper](https:\/\/arxiv.org\/abs\/1106.1813)","3dba3364":"Sensitivity has increased yet again. Our model is getting better at detecting churn among customers.","691bac34":"Let's first check for null values in our dataset.","b470cee9":"We see from the bar graph that `Age` positively correlates with customer churn. We have binned the data and can observe that the bins with higher `Age` have a larger proportion of customers churning.","ac0e6601":"Though our f1_score remains comparable to our first training run, we notice a large increase in sensitivity when adding synthetic samples. This is beneficial to us since we would like to be able to detect when a customer will churn.","2008c1ae":"RFE chose 5 features. Our model's score did not improve. Let's try using SelectFromModel.","ad273f6b":"Looking at the correlation heatmap, we can see that `Age` has the highest correlation with our target variable `Exited`. This makes sense given that the older you are, the more likely you are to churn as a customer. Let's take a closer look at some of the variables.","3bfdba03":"Finally, let's create an ROC plot.","4f126705":"We see the same thing when we look at `CreditScore`. There is no notable difference between any of the values.","32ccf334":"Let's look at some feature selection methods now. First we will try Recursive Feature Elimination with Logistic Regression. We will test the chosen features with our XGBoost model.","2813061c":"Let's create a correlation heatmap to see linear relationships between variables.","98afeb35":"Looking at the data again, we see the column `Surname`. Thinking intutively, any correlation of this column with our target variable would be completely coincidental. These relationships would be spurious since we can never really predict if a customer will churn based on their name. Thus, we will drop this column as well.","7152c7e8":"The MLP model performs best on the test set in our initial training run. Let's analyze its performance with a classification report and confusion matrix on the test set.","db1d7964":"We will try a different method of feature selection using our model. By getting the cumulative percentage of feature importance, we can set a percentage cutoff after which we will drop all other features. I will set it to 0.9.","aae2e9a0":"We see a notable trend in `NumOfProducts` as well. It is not linear but when a customer has >2 products, churn seems to increase by a large amount.","320620be":"Looking at the generated training data, we see that our class distribution is now pretty much even. Let's try another round of training and see the results.","0be143a8":"In line with what we saw in our earlier analysis, `IsActiveMember` and `NumOfProducts` play a large role in determining whether an employee will churn or not.","5b87f153":"Using feature importance has improved our f1 score by a respectable margin. We will use this feature set moving forward. Let's check the classification report.","a6c235e0":"Let's create a `weight` to try to put more weight on positive classes."}}