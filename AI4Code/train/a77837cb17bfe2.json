{"cell_type":{"74d318e1":"code","02f0dbdd":"code","33974da6":"code","d19dac06":"code","b28d2dd9":"code","64b55c0f":"code","a3512d9a":"code","a190ce1f":"code","f53ada60":"code","c948ffcc":"code","4ceb1b34":"code","ff2b4fe7":"code","c26afd28":"code","491760bf":"code","d2dde9d9":"code","6fecc4c6":"code","086e0294":"code","f5f5e9fa":"code","03b57b5b":"markdown","81826246":"markdown","f1209d7a":"markdown","75026662":"markdown","45a3b8cf":"markdown","b1fd53e8":"markdown","c404d40b":"markdown","a3291071":"markdown","80385bf4":"markdown","cc556474":"markdown","5efec09a":"markdown","64d07748":"markdown"},"source":{"74d318e1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport h2o\nfrom h2o.automl import H2OAutoML\nfrom h2o.estimators.glm import H2OGeneralizedLinearEstimator\nh2o.init()\n\n# Any results you write to the current directory are saved as output.","02f0dbdd":"# Load data into H2O\ndf = h2o.import_file(\"..\/input\/train.csv\")","33974da6":"df.describe()","d19dac06":"y = \"C2\"\nx = df.columns\nx.remove(y)\nx.remove(\"C1\")","b28d2dd9":"aml = H2OAutoML(max_runtime_secs = 30000, seed = 1,balance_classes=True)\naml.train(x = x, y = y, training_frame = df)","64b55c0f":"lb = aml.leaderboard","a3512d9a":"lb.head()","a190ce1f":"lb.head(rows=lb.nrows)","f53ada60":"# Get model ids for all models in the AutoML Leaderboard\nmodel_ids = list(aml.leaderboard['model_id'].as_data_frame().iloc[:,0])\n# Get the \"All Models\" Stacked Ensemble model\nse = h2o.get_model([mid for mid in model_ids if \"StackedEnsemble_AllModels\" in mid][0])\n# Get the Stacked Ensemble metalearner model\nmetalearner = h2o.get_model(se.metalearner()['name'])","c948ffcc":"metalearner.coef_norm()","4ceb1b34":"%matplotlib inline\nmetalearner.std_coef_plot()","ff2b4fe7":"aml.leader.model_id","c26afd28":"h2o.save_model(aml.leader, path = \"..\/working\/\")","491760bf":"model = h2o.load_model(aml.leader.model_id)","d2dde9d9":"df_test = h2o.import_file(\"..\/input\/test.csv\")\ndisplay(df_test.head())\ndf_test = df_test[1:,:]","6fecc4c6":"predict = aml.predict(df_test)","086e0294":"predict.shape","f5f5e9fa":"submission = h2o.import_file(\"..\/input\/sample_submission.csv\")\nsubmission['target1'] = predict\nsubmission = submission.as_data_frame()\nsubmission.columns = ['id', 'target1', 'target']\nsubmission.pop('target1')\nsubmission.to_csv(\"h2o.csv\", index=False)\nsubmission.head()","03b57b5b":"# **8.Model Importance**\n\n---\n[**TOP**](#Outline-of-the-notebook)  ","81826246":"# **Starter H2O for Don't Overfit**\n\n---\n\n## ***Outline of the notebook***\n\n---\n\n* Step [**1.Import h2o**](#1.Import-h2o)\n* Step [**2.Load Data**](#2.Load-Data)\n* Step [**3.Statistics of Data**](#3.Statistics-of-Data)\n* Step [**4.Split Train and target**](#4.Split-Train-and-target)\n* Step [**5.Automl Model Training**](#5.Automl-Model-Training)\n* Step [**6.Leader Board of Models**](#6.Leader-Board-of-Models)\n* Step [**7.Ensemble Exploration**](#7.Ensemble-Exploration)\n* Step [**8.Model Importance**](#8.Model-Importance)\n* Step [**9.Save-Leader-Model**](#9.Save-Leader-Model)\n* Step [**10.Read test data**](#10.Read-test-data)\n* Step [**11.Predict Result**](#11.Predict-Result)\n\n\n---\n\n# **1.Import h2o**\n\n---\n[**TOP**](#Outline-of-the-notebook)  \nImport the h2o Python module and H2OAutoML class and initialize a local H2O cluster.","f1209d7a":"# **5.Automl Model Training**\n\n---\n[**TOP**](#Outline-of-the-notebook)  \n\nRun AutoML, stopping after 10 models. The max_models argument specifies the number of individual (or \"base\") models, and does not include the two ensemble models that are trained at the end.","75026662":"Examine the variable importance of the metalearner (combiner) algorithm in the ensemble. This shows us how much each base learner is contributing to the ensemble. The AutoML Stacked Ensembles use the default metalearner algorithm (GLM with non-negative weights), so the variable importance of the metalearner is actually the standardized coefficient magnitudes of the GLM.","45a3b8cf":"# **3.Statistics of Data**\n\n---\n[**TOP**](#Outline-of-the-notebook)  ","b1fd53e8":"# **9.Save Leader Model**\n\n---\n[**TOP**](#Outline-of-the-notebook)  \nThere are two ways to save the leader model -- binary format and MOJO format. If you're taking your leader model to production, then we'd suggest the MOJO format since it's optimized for production use.","c404d40b":"# **11.Predict Result**\n\n---\n[**TOP**](#Outline-of-the-notebook)  ","a3291071":"# **10.Read test data**\n\n---\n[**TOP**](#Outline-of-the-notebook)  ","80385bf4":"# **4.Split Train and target**\n\n---\n[**TOP**](#Outline-of-the-notebook)  ","cc556474":"# **7.Ensemble Exploration**\n\n---\n[**TOP**](#Outline-of-the-notebook)  \n\n---\n\nTo understand how the ensemble works, let's take a peek inside the Stacked Ensemble \"All Models\" model. The \"All Models\" ensemble is an ensemble of all of the individual models in the AutoML run. This is often the top performing model on the leaderboard.","5efec09a":"# **6.Leader Board of Models**\n\n---\n[**TOP**](#Outline-of-the-notebook)  \n\nNext, we will view the AutoML Leaderboard. Since we did not specify a leaderboard_frame in the H2OAutoML.train() method for scoring and ranking the models, the AutoML leaderboard uses cross-validation metrics to rank the models.\n\nA default performance metric for each machine learning task (binary classification, multiclass classification, regression) is specified internally and the leaderboard will be sorted by that metric. In the case of binary classification, the default ranking metric is Area Under the ROC Curve (AUC). In the future, the user will be able to specify any of the H2O metrics so that different metrics can be used to generate rankings on the leaderboard.\n\nThe leader model is stored at aml.leader and the leaderboard is stored at aml.leaderboard.","64d07748":"# **2.Load Data**\n\n---\n[**TOP**](#Outline-of-the-notebook)  \n\nThe goal here is to predict whether or not status."}}