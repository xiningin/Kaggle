{"cell_type":{"5c0b3aea":"code","562f348e":"code","3fbe5069":"code","e8f7559c":"code","f82433a5":"code","813a9902":"code","fa4cfd76":"code","d32e8dab":"code","0d853e2b":"code","93c6e837":"code","0e1a7513":"code","1da46d6c":"code","a2adbbf8":"code","f071ee35":"code","6284cded":"code","f5a65ebb":"code","879f4791":"code","5e24ea50":"markdown","ac49f934":"markdown","c24e994b":"markdown"},"source":{"5c0b3aea":"import os\n%matplotlib inline\nimport matplotlib.pyplot as plt  \nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport gc\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\n#Importing most common alogorithms \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC#we will not be using SVM due tot he huge training time required on our dataset.\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis,LinearDiscriminantAnalysis\n\nfrom sklearn import model_selection #Cross-validation multiple scoring function\nimport warnings\nwarnings.simplefilter(\"ignore\")\nfrom tqdm import tqdm","562f348e":"tqdm.pandas()","3fbe5069":"%%time\n# Only load those columns \nkeep_cols = ['event_id', 'game_session', 'installation_id', 'event_count', 'event_code', 'title', 'game_time', 'type', 'world']\n\ntrain = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/train.csv', usecols=keep_cols)\ntest = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/test.csv', usecols=keep_cols)\ntrain_labels = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/train_labels.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/sample_submission.csv')","e8f7559c":"test_assess = test[test.type == 'Assessment'].copy()\ntest_labels = submission.copy()\ntest_labels['title'] = test_labels.installation_id.progress_apply(\n    lambda install_id: test_assess[test_assess.installation_id == install_id].iloc[-1].title\n)","f82433a5":"df = train\ngroup1 = df.drop(columns=['event_id', 'event_code']).groupby(\n    ['game_session', 'installation_id', 'title', 'type', 'world']\n).max().reset_index()\n\ngroup2 = pd.get_dummies(\n    df[['game_session', 'installation_id', 'event_code']], \n    columns=['event_code']\n).groupby(['game_session', 'installation_id']).sum().reset_index()","813a9902":"def group_and_reduce(df):\n    \"\"\"\n    Author: https:\/\/www.kaggle.com\/xhlulu\/\n    Source: https:\/\/www.kaggle.com\/xhlulu\/ds-bowl-2019-simple-lgbm-using-aggregated-data\n    \"\"\"\n    # group1 and group2 are intermediary \"game session\" groups,\n    # which are reduced to one record by game session. group1 takes\n    # the max value of game_time (final game time in a session) and \n    # of event_count (total number of events happened in the session).\n    # group2 takes the total number of event_code of each type\n    group1 = df.drop(columns=['event_id', 'event_code']).groupby(\n        ['game_session', 'installation_id', 'title', 'type', 'world']\n    ).max().reset_index()\n\n    group2 = pd.get_dummies(\n        df[['game_session', 'installation_id', 'event_code']], \n        columns=['event_code']\n    ).groupby(['game_session', 'installation_id']).sum().reset_index()\n\n    # group3, group4, group5 are grouped by installation_id \n    # and reduced using summation and other summary stats\n    group3 = pd.get_dummies(\n        group1.drop(columns=['game_session', 'event_count', 'game_time']),\n        columns=['title', 'type', 'world']\n    ).groupby(['installation_id']).sum()\n\n    group4 = group1[\n        ['installation_id', 'event_count', 'game_time']\n    ].groupby(\n        ['installation_id']\n    ).agg([np.sum, np.mean, np.std, np.min, np.max, np.median, sp.stats.skew])\n\n    group5 = group2.drop(\n        columns=['game_session']\n    ).groupby(\n        ['installation_id']\n    ).agg([np.sum, np.mean, np.std, np.min, np.max, np.median])\n\n    return group3.join(group4).join(group5)","fa4cfd76":"%%time\ntrain_small = group_and_reduce(train)\ntest_small = group_and_reduce(test)\n\nprint(train_small.shape)\ntrain_small.head()","d32e8dab":"final_train = pd.get_dummies(train_labels.set_index('installation_id').drop(\n    columns=['num_correct', 'num_incorrect', 'accuracy', 'game_session']\n).join(train_small), columns=['title'])\n\n# Experimental:\nfinal_train = final_train.reset_index().groupby('installation_id').apply(lambda x: x.iloc[-1])\n\nprint(final_train.shape)\nfinal_train.head()","0d853e2b":"final_train = final_train.drop(columns='installation_id')","93c6e837":"final_test = pd.get_dummies(test_labels.set_index('installation_id').join(test_small), columns=['title'])\n\nprint(final_test.shape)\nfinal_test.head()","0e1a7513":"final_train_ = final_train.dropna()\nX = final_train_.drop(columns='accuracy_group').values\ny = final_train_['accuracy_group'].values","1da46d6c":"seed = 42\nnp.random.seed(seed)\n# prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression(multi_class='auto',n_jobs=-1)))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('QDA',QuadraticDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier(7, n_jobs=-1)))\nmodels.append(('CART', DecisionTreeClassifier(max_depth=10)))\nmodels.append(('NB', GaussianNB()))\n#models.append(('Gaussian Process Classifier', GaussianProcessClassifier()))\n#models.append(('SVM_linear', SVC(kernel=\"linear\", C=0.025)))#we will not be using SVM due to the huge training time required for this dataset\n#models.append(('SVM_',SVC(gamma=2, C=1)))\nmodels.append(('RandomForest',RandomForestClassifier( n_estimators=100, n_jobs=-1)))\nmodels.append(('MLP',MLPClassifier(alpha=0.0001)))\nmodels.append(('ADABoost',AdaBoostClassifier()))\nmodels.append(('One-Vs-Rest LR', OneVsRestClassifier(LogisticRegression(multi_class='auto',n_jobs=-1),n_jobs=-1)))\nmodels.append(('One-Vs-Rest LDA', OneVsRestClassifier(OneVsRestClassifier(LinearDiscriminantAnalysis(),n_jobs=-1))))\nmodels.append(('One-Vs-Rest QDA',OneVsRestClassifier(QuadraticDiscriminantAnalysis())))\nmodels.append(('One-Vs-Rest KNN', OneVsRestClassifier(KNeighborsClassifier(7, n_jobs=-1))))\nmodels.append(('One-Vs-Rest CART', OneVsRestClassifier(DecisionTreeClassifier(max_depth=10),n_jobs=-1)))\nmodels.append(('One-Vs-Rest NB', OneVsRestClassifier(GaussianNB(),n_jobs=-1)))\n#models.append(('One-Vs-Rest Gaussian Process Classifier', OneVsRestClassifier(GaussianProcessClassifier(),n_jobs=-1)))\n#models.append(('One-Vs-Rest SVM_linear', SVC(kernel=\"linear\", C=0.025)))#we will not be using SVM due to the huge training time required for this dataset\n#models.append(('One-Vs-Rest SVM_',SVC(gamma=2, C=1)))\nmodels.append(('One-Vs-Rest RandomForest',OneVsRestClassifier(RandomForestClassifier( n_estimators=100, n_jobs=-1),n_jobs=-1)))\nmodels.append(('One-Vs-Rest MLP',OneVsRestClassifier(MLPClassifier(alpha=0.0001),n_jobs=-1)))\nmodels.append(('One-Vs-Rest ADABoost',OneVsRestClassifier(AdaBoostClassifier(),n_jobs=-1)))\n\n# evaluate each model in turn\n\nresults = []\nscoring = ['accuracy',\n          'precision_weighted',\n          'recall_weighted',\n          'f1_weighted']\nnames = []\n\nsk = model_selection.StratifiedKFold(n_splits=10, random_state=42)\nfor name, model in models:\n    cv_results = model_selection.cross_validate(model, X, y, cv=sk, scoring=scoring) \n    results.append(cv_results)\n    names.append(name)\n    msg ='-------------------------------------------------------------------------------------------------------------\\n'\n    msg = \"Model : %s \\n\" % (name)\n    msg = msg +'\\n'\n    msg =  msg + \"Accuracy :  %f (%f)\\n\" % (cv_results['test_accuracy'].mean(),cv_results['test_accuracy'].std())\n    msg =  msg + \"Precision score :  %f (%f)\\n\" % (cv_results['test_precision_weighted'].mean(),cv_results['test_precision_weighted'].std())\n    msg =  msg + \"Recall score :  %f (%f)\\n\" % (cv_results['test_recall_weighted'].mean(),cv_results['test_recall_weighted'].std())\n    msg =  msg + \"F1 score :  %f (%f)\\n\" % (cv_results['test_f1_weighted'].mean(),cv_results['test_f1_weighted'].std())\n    msg = msg + '------------------------------------------------------------------------------------------------------------\\n'\n    print(msg)","a2adbbf8":"Accuracy = []\nPrecision = []\nRecall = []\nF1 = []\nfor idx,scores in enumerate(results):\n    Accuracy.append(scores['test_accuracy'])\n    Precision.append(scores['test_precision_weighted'])\n    Recall.append(scores['test_recall_weighted'])\n    F1.append(scores['test_f1_weighted'])","f071ee35":"fig = plt.figure(figsize=(30,30))\nfig.suptitle('Algorithms Comparison')\nax = fig.add_subplot(221)\nplt.boxplot(Accuracy)\nplt.title('Accuracy score')\nax.set_xticklabels(names)\nax = fig.add_subplot(222)\nplt.boxplot(Precision)\nplt.title('Precision Score')\nax.set_xticklabels(names)\nax = fig.add_subplot(223)\nplt.boxplot(Recall)\nax.set_xticklabels(names)\nplt.title('Recall score')\nax = fig.add_subplot(224)\nplt.title('F1 score')\nplt.boxplot(F1)\nax.set_xticklabels(names)\n\nplt.show()","6284cded":"final_model =  DecisionTreeClassifier(max_depth=10)\n\nfinal_model.fit(X,y)","f5a65ebb":"X_test = final_test.drop(columns=['accuracy_group'])\ntest_pred = final_model.predict(X_test.fillna(0))\n","879f4791":"\nfinal_test['accuracy_group'] = test_pred\nfinal_test[['accuracy_group']].to_csv('submission.csv')","5e24ea50":"## Group and Reduce","ac49f934":"## Training and Comparing models","c24e994b":"## Combine train\/test labels with summary stats"}}