{"cell_type":{"933eb110":"code","0232eb69":"code","7a1e9788":"code","88b27610":"code","d217798b":"code","c1d5175b":"code","cf63054a":"code","0dd217cd":"code","50404580":"code","701fe934":"markdown","4f57885f":"markdown"},"source":{"933eb110":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0232eb69":"# Load the model and see its architecture\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torchvision import models\nfrom torchvision import transforms, utils\n\nimport numpy as np\nimport scipy.misc\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom PIL import Image\n\nmodel = models.vgg16(pretrained=True)\nprint(model.features)","7a1e9788":"# We will load all the module details in a list\n\nmodules = list(model.features.modules())\nmodules = modules[1:]\nprint(modules,\"\\n\\n\")\nprint(\"third module = \", modules[2])","88b27610":"# Load and preprocess an image to pass as input to the network\n\ndef normalize(image):\n    normalize = transforms.Normalize(\n    mean=[0.485, 0.456, 0.406],\n    std=[0.229, 0.224, 0.225]\n    )\n    preprocess = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n    normalize\n    ])\n    image = Variable(preprocess(image).unsqueeze(0))\n    return image\n\nimg_raw = Image.open(\"\/kaggle\/input\/heart-image\/heart.jpg\")\nplt.imshow(img_raw)\nplt.title(\"Image loaded successfully\")\n\nimg = normalize(img_raw)","d217798b":"def visualize_weights(image, layer):\n    weight_used = []\n    \n    ## Gather all Convolution layers and append their corresponding filters in a list\n    for w in model.features.children():\n        if isinstance(w, torch.nn.modules.conv.Conv2d):\n            weight_used.append(w.weight.data)\n\n    print(\"(#filters, i\/p depth, size of filter) === \",weight_used[layer].shape)\n    print(\"No. of filters: \", weight_used[layer].shape[0])\n    filters = []\n    for i in range(weight_used[layer].shape[0]):\n        filters.append(weight_used[layer][i,:,:,:].sum(dim=0))    ##summing across input depth(3 in the first layer)\n        filters[i].div(weight_used[layer].shape[1])\n        \n    fig = plt.figure()\n    plt.rcParams[\"figure.figsize\"] = (10, 10)\n    for i in range(int(np.sqrt(weight_used[layer].shape[0])) * int(np.sqrt(weight_used[layer].shape[0]))):\n        a = fig.add_subplot(np.sqrt(weight_used[layer].shape[0]),np.sqrt(weight_used[layer].shape[0]),i+1)\n        imgplot = plt.imshow(filters[i])\n        plt.axis('off')\n\nvisualize_weights(img, 1)","c1d5175b":"# Visualizing the image as it passes through the network\n\ndef to_grayscale(image):\n    image = torch.sum(image, dim=0)\n    image = torch.div(image, image.shape[0])\n    return image\n\ndef layer_outputs(image):\n    outputs = []\n    names = []\n    \n    ## feed forward the image through the network and store the outputs\n    for layer in modules:\n        image = layer(image) \n        outputs.append(image)\n        names.append(str(layer))\n    \n    ## for visualization purposes, convert the output into a 2D image by averaging across the filters.\n    output_im = []\n    for i in outputs:\n        i = i.squeeze(0)\n        temp = to_grayscale(i)  ## convert say 64x112x112 to 112x112\n        output_im.append(temp.data.numpy())\n        \n    fig = plt.figure()\n    plt.rcParams[\"figure.figsize\"] = (30, 40)\n\n\n    for i in range(len(output_im)):\n        a = fig.add_subplot(8,4,i+1)\n        imgplot = plt.imshow(output_im[i])\n        plt.axis('off')\n        a.set_title(str(i+1)+\". \"+names[i].partition('(')[0], fontsize=15)\n\n#     ##save the resulting visualization\n#     plt.savefig('layer_outputs.jpg', bbox_inches='tight')\n\nlayer_outputs(img)","cf63054a":"# Visualizing output of each filter at a given layer\n\ndef filter_outputs(image, layer_to_visualize, num_filters=64):\n    if layer_to_visualize < 0:\n        layer_to_visualize += 31\n    output = None\n    name = None\n    #image at each layer\n    ## get outputs corresponding to the mentioned layer\n    for count, layer in enumerate(modules):\n        image = layer(image)\n        if count == layer_to_visualize: \n            output = image\n            name = str(layer)\n    \n    filters = []\n    output = output.data.squeeze()\n\n    ## if num_filters==-1, visualize all the filters\n    num_filters = min(num_filters, output.shape[0])\n    if num_filters==-1:\n        num_filters = output.shape[0]\n\n    for i in range(num_filters):\n        filters.append(output[i,:,:])\n        \n    fig = plt.figure()\n    plt.rcParams[\"figure.figsize\"] = (10, 10)\n\n    for i in range(int(np.sqrt(len(filters))) * int(np.sqrt(len(filters)))):\n        fig.add_subplot(np.sqrt(len(filters)), np.sqrt(len(filters)),i+1)\n        imgplot = plt.imshow(filters[i])\n        plt.axis('off')\n\n## if num_filters==-1, visualize all the filters\nfilter_outputs(img,0,16)    #visualize the outputs of first 16 filters of the 1st layer","0dd217cd":"# Understanding Deep Image Representations by Inverting Them [Mahendran, Vedaldi]\n\n# Like Zeiler and Fergus, their method starts from a specific input image. They record the network\u2019s representation of that specific image and then reconstruct an image that produces a similar code. Thus, their method provides insight into what the activation of a whole layer represent, not what an individual neuron represents.\n# They show what each neuron \u201cwants to see\u201d, and thus what each neuron has learned to look for.\n# To visualize the function of a specific unit in a neural network, we  synthesize  inputs that cause that unit to have high activation. To synthesize such a \u201cpreferred input example\u201d, we start with a random image, meaning we randomly choose a color for each pixel. The image will initially look like colored TV static.\n\n\nrandom_noise_img = Variable(1e-1 * torch.randn(1, 3, 224, 224), requires_grad=True)","50404580":"# Now we take an image  X  whose representation  X0  at some layer  \u2018\u2018target_layer\"  we want to learn. Our aim is to reconstruct the noise image to get this representation  X0 . The principle behind this is that the noise image will be so reconstructed such that it will represent what the particular layer for which it is trained against wants to see.\n\n\ndef get_output_at_nth_layer(inp, layer):\n    for i in range(layer):\n        inp = modules[i](inp)\n    return inp[0]\n\n## dont forget that the system is 0 indexed\ntarget_layer = 18    ## which is this layer Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\ninp_img = normalize(Image.open(\"\/kaggle\/input\/heart-image\/heart.jpg\"))\ninp_img_representation = get_output_at_nth_layer(inp_img, target_layer)","701fe934":"#Codes from Abhishek Shrivastana https:\/\/www.kaggle.com\/abhi8923shriv\/cnns-visualization-at-each-layer","4f57885f":"It takes 8 min to run this cells. Thanks https:\/\/www.kaggle.com\/abhi8923shriv\/cnns-visualization-at-each-layer\n\nDas War's Kaggle Notebook Runner: Mar\u00edlia Prata  @mpwolke"}}