{"cell_type":{"889c8b16":"code","7395c530":"code","9eae46bf":"code","8107d1e5":"code","41919163":"code","dc77efb7":"code","28ff3f50":"code","581793d1":"code","1e52bc72":"code","5d8f9c9a":"code","cace0605":"code","b6417d83":"code","011048a3":"code","045cddc2":"code","7b535427":"code","deea08cd":"code","fd2b580d":"code","6bacef0e":"code","f7f0a129":"code","e97b684f":"code","24226c2b":"code","d7dd6198":"code","606dc8ad":"code","ad203296":"code","33ded196":"code","42b6c42a":"code","6dfc2fbc":"code","8e328ee4":"code","0cc7b31b":"code","8bbd2c12":"code","d4d0a8af":"code","4a6b2a2f":"code","a803cacb":"code","ba50c358":"code","ce0e6c67":"code","551d3f62":"code","2127b704":"code","5e4ba8b4":"code","ac736d48":"code","9d31fc38":"code","439d893e":"code","5a2e2088":"code","e20967ec":"code","74917317":"code","c9ab4e2c":"code","fc4149f5":"code","321f29c5":"code","c5e749cf":"code","714e18a3":"code","da6d8796":"code","049b1e21":"code","63a6b614":"code","86a9411e":"code","f0434a3a":"code","e67f5d66":"code","8e95284b":"code","0576ddc0":"code","15187469":"code","56626bef":"code","e2fb1e9b":"code","96d35236":"code","c6ccdcbe":"code","f30d8f05":"code","887dbfe8":"code","2ecc0d8d":"code","6293968e":"code","4623e101":"code","898ec89a":"code","3a814f23":"code","4766d96d":"code","e9fff9c1":"code","92719739":"code","72f8c601":"code","372d99c3":"code","d2e6240c":"code","d9ea564b":"code","184ed854":"code","2145cd52":"code","b04cae8f":"code","44ac8d2a":"code","0e6ba156":"code","95b050dc":"code","6b293cb1":"code","4c3fb6ae":"code","145e2a47":"code","afcb6054":"code","6264dd0d":"code","b25b489f":"code","d760b27f":"code","7910c0bd":"code","2aebb65c":"code","b0bc9bbf":"code","8a2b2e0e":"code","d3fca16a":"code","592a8691":"code","3aecffc0":"code","b2b0dd22":"code","42711031":"code","fe221b9b":"code","a31ff911":"code","e536f07b":"code","97743932":"code","b8d18739":"code","0c9871da":"code","31ca9625":"code","9d6958d0":"code","341a7387":"code","7f3775f6":"code","44d0f522":"code","a797cd43":"code","557c47e9":"code","7b748167":"code","77b44c1d":"code","a1d99d80":"code","c414bb85":"code","171b72fd":"code","9642bd1e":"code","62ee88cd":"code","8e51f289":"code","960ee6f0":"code","16485843":"code","ed2db402":"code","0fd427e5":"code","a502ed17":"code","91951192":"code","63a57952":"code","85038652":"code","a96f4638":"code","e6d430d8":"code","e58627f2":"code","63996eba":"code","549ed7c0":"code","9cfff4cc":"code","2ee42111":"code","c8f8cdef":"code","81d2ea17":"code","6089b40c":"code","f16a6622":"code","e2a14e7d":"code","f1befde7":"code","1e829e2b":"code","05eba292":"code","727e5a47":"code","d7c24c45":"code","c45070f8":"code","8cf3dd4d":"code","fc4617d1":"code","5dabbd3e":"code","9d00802e":"code","301561ff":"code","d3b3ae22":"code","5306c724":"markdown","ca78b2f6":"markdown","ee5fa3d9":"markdown","d770f267":"markdown","f5c3d337":"markdown","d352fabc":"markdown","9466caea":"markdown","4f1ebd6c":"markdown","7c3a8167":"markdown","bb6df8eb":"markdown","5de2acd9":"markdown","122070b5":"markdown","16ee2faa":"markdown","e8e71de5":"markdown","3790f001":"markdown","8bb8f034":"markdown","326a80f4":"markdown","9d34285b":"markdown","e68d2ead":"markdown","5f7eee16":"markdown","3d511233":"markdown","0f4d7ff7":"markdown","6c4884eb":"markdown","e8c9461d":"markdown","5e21e5f3":"markdown","3e1d003f":"markdown","f7ae68e3":"markdown","1071b8d2":"markdown","fcdee99e":"markdown","776dae18":"markdown","091e02ea":"markdown","7c58f3d1":"markdown","fade5b80":"markdown","91a2dd06":"markdown","51e3f0be":"markdown","d1cd90e5":"markdown","3faaf72e":"markdown","8f83c68d":"markdown","c413c84c":"markdown","a3fbe9f9":"markdown","03494530":"markdown","f0f68f77":"markdown","1f658cb4":"markdown","94f25bcc":"markdown","76dda701":"markdown","7211bfbe":"markdown","5959139c":"markdown","061b18e1":"markdown","c17eff42":"markdown","3fae8dc4":"markdown","3788baeb":"markdown","5817c46a":"markdown","21bb6cfe":"markdown","c9c8069b":"markdown","ba042ccb":"markdown","b7cc1531":"markdown","72ccf025":"markdown","3a5e43de":"markdown","3f5e7b50":"markdown","08bd0028":"markdown","3a5d1d59":"markdown","fc0ca18f":"markdown","fa73241e":"markdown","e32007c3":"markdown","8d2370e3":"markdown","71f37018":"markdown","107e7e48":"markdown","1f8b69c9":"markdown","966a6711":"markdown","7f33b4cb":"markdown","0afbab3c":"markdown","5c41b965":"markdown","efd1f5c4":"markdown","9f5ca039":"markdown","d026de9d":"markdown","ddda7d06":"markdown","12fc5064":"markdown","e8c911d7":"markdown","d43c9357":"markdown","fc9d99dc":"markdown","4c7869d2":"markdown"},"source":{"889c8b16":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set()","7395c530":"! ls ..\/input\/competitive-data-science-predict-future-sales\/","9eae46bf":"SEED = 5","8107d1e5":"item_categories = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nitem_categories.head()","41919163":"item_categories['item_category_id'].nunique()","dc77efb7":"item_categories['item_category_name'].values","28ff3f50":"items = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nitems.head()","581793d1":"items.item_id.nunique()","1e52bc72":"items.item_category_id.nunique()","5d8f9c9a":"plt.figure(figsize=(14,14))\nitems.groupby('item_category_id')['item_id'].size().plot.barh(rot=0)\nplt.title('Number of items related to different categories')\nplt.xlabel('Categories')\nplt.ylabel('Number of items');","cace0605":"items.groupby('item_category_id')['item_id'].size().mean(), items.groupby('item_category_id')['item_id'].size().max(),items.groupby('item_category_id')['item_id'].size().min()","b6417d83":"item_categories[item_categories['item_category_id'].isin(items.groupby('item_category_id')['item_id'].size().nlargest(5).index)]","011048a3":"item_categories[item_categories['item_category_id']\\\n                .isin((items.groupby('item_category_id')['item_id'].size()[items.groupby('item_category_id')['item_id'].size()==1])\\\n                      .index)]","045cddc2":"(items.groupby('item_category_id')['item_id'].size()==0).astype(int).sum()","7b535427":"(items.groupby('item_id')['item_category_id'].size()>=2).sum()","deea08cd":"items_categories_merged = items.merge(item_categories,left_on='item_category_id',right_on='item_category_id',how='inner')","fd2b580d":"items_categories_merged.head()","6bacef0e":"from collections import Counter\ncounter = Counter([i for i in np.hstack(items_categories_merged['item_name'].str.split(' ').values) if i])\nsorted(counter.items(),key=lambda x: x[1])[::-1][:30]","f7f0a129":"len(items_categories_merged), len(items)","e97b684f":"shops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')","24226c2b":"shops.head()","d7dd6198":"len(shops)","606dc8ad":"shops.shop_name","ad203296":"train_sales = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')","33ded196":"train_sales.isnull().sum(axis=1).sum()","42b6c42a":"train_sales.describe()","6dfc2fbc":"train_sales.head()","8e328ee4":"to_plot = train_sales['item_cnt_day'].rolling(5).sum()\nplt.figure(figsize=(14,8))\nplt.plot(range(len(to_plot.index)),to_plot.values)","0cc7b31b":"train_sales['day'] = train_sales['date'].apply(lambda x: x.split('.')[0])\ntrain_sales['month'] = train_sales['date'].apply(lambda x: x.split('.')[1])\ntrain_sales['year'] = train_sales['date'].apply(lambda x: x.split('.')[2])","8bbd2c12":"train_sales.head()","d4d0a8af":"train_sales['date'] = pd.to_datetime(train_sales['date'],format='%d.%m.%Y')","4a6b2a2f":"train_sales['date'].min(),train_sales['date'].max()","a803cacb":"train_sales.head(10)","ba50c358":"fig, (ax1, ax2, ax3, ax4) = plt.subplots(4,1,figsize=(16,20))\nfig.tight_layout(pad=3.0)\n\nto_plot = train_sales.groupby('date',as_index=False)['item_cnt_day'].sum().reset_index()\nz = np.polyfit(y=to_plot['item_cnt_day'],x=to_plot['index'], deg=1)\np = np.poly1d(z)\nax1.plot(to_plot['date'],to_plot['item_cnt_day'],'-')\nax1.plot(to_plot['date'],p(to_plot['index'].values),'--r')\nax1.legend(['Sum of sold items','Trend line'])\nax1.title.set_text('Sum of sold items by date')\n\nto_plot = train_sales.groupby('day')['item_cnt_day'].sum()\nax2.plot(to_plot.values,'-o')\nax2.title.set_text('Sum of sold items by day')\nax2.set_xticks(range(len(to_plot)))\n\nto_plot = train_sales.groupby('month')['item_cnt_day'].sum()\nax3.plot(to_plot.values,'-o')\nax3.title.set_text('Sum of sold items by month')\nax3.set_xticks(range(len(to_plot)))\n\nto_plot = train_sales.groupby('year',as_index=False)['item_cnt_day'].sum()\nax4.plot(to_plot['item_cnt_day'].values,'-o')\nax4.title.set_text('Sum of sold items by year')\nax4.set_xticks(range(len(to_plot)))\nax4.set_xticklabels(list(to_plot['year'].values));\n","ce0e6c67":"fig, (ax1, ax2, ax3, ax4) = plt.subplots(4,1,figsize=(16,20))\nfig.tight_layout(pad=3.0)\n\nto_plot = train_sales.groupby('date',as_index=False)['item_price'].mean().reset_index()\nz = np.polyfit(y=to_plot['item_price'],x=to_plot['index'], deg=1)\np = np.poly1d(z)\nax1.plot(to_plot['date'],to_plot['item_price'],'-')\nax1.plot(to_plot['date'],p(to_plot['index'].values),'--r')\nax1.legend(['Mean price of items','Trend line'])\nax1.title.set_text('Mean price of items by date')\n\nto_plot = train_sales.groupby('day')['item_price'].mean()\nax2.plot(to_plot.values,'-o')\nax2.title.set_text('Mean price of items by day')\nax2.set_xticks(range(len(to_plot)))\n\nto_plot = train_sales.groupby('month')['item_price'].mean()\nax3.plot(to_plot.values,'-o')\nax3.title.set_text('Mean price of items by month')\nax3.set_xticks(range(len(to_plot)))\n\nto_plot = train_sales.groupby('year',as_index=False)['item_price'].mean()\nax4.plot(to_plot['item_price'].values,'-o')\nax4.title.set_text('Mean price of items by year')\nax4.set_xticks(range(len(to_plot)))\nax4.set_xticklabels(list(to_plot['year'].values));\n","551d3f62":"dict_monthes = dict(train_sales['month'].value_counts())\nmonthes, frequencies = zip(*sorted(dict_monthes.items(),key=lambda x: int(x[0][1]) if x[0][0]=='0' else int(x[0])))\nplt.figure(figsize=(15,12))\nplt.bar(range(len(monthes)),frequencies)\nplt.title('Distribution of monthes in dataset')\nplt.xticks(range(len(monthes)),monthes);","2127b704":"train_sales['revenue'] = train_sales['item_price']*train_sales['item_cnt_day']\nplt.figure(figsize=(14,8))\ntrain_sales.groupby('month')['revenue'].mean().plot.bar(rot=0)\nplt.title('Averaged revenue per month')\nplt.xlabel('Monthes')\nplt.ylabel('Revenue');","5e4ba8b4":"plt.figure(figsize=(14,8))\ntrain_sales.groupby('month')['revenue'].max().plot.bar(rot=0)\nplt.title('Maximum revenue per month')\nplt.xlabel('Monthes')\nplt.ylabel('Revenue');","ac736d48":"plt.figure(figsize=(14,8))\ntrain_sales[train_sales['revenue']>0].groupby('month')['revenue'].min().plot.bar()\nplt.title('Minimum revenue per month')\nplt.xlabel('Monthes')\nplt.ylabel('Revenue');","9d31fc38":"plt.figure(figsize=(14,8))\ntrain_sales.groupby('date_block_num')['revenue'].mean().plot.bar(rot=0)\nplt.title('Averaged revenue per month (count)')\nplt.xlabel('Relative number of monthes')\nplt.ylabel('Revenue');","439d893e":"plt.figure(figsize=(14,8))\ntrain_sales.groupby('day')['revenue'].mean().plot.bar(rot=0)\nplt.title('Averaged revenue per day')\nplt.xlabel('Days')\nplt.ylabel('Revenue');","5a2e2088":"mean_revenue_day_month = train_sales.groupby(['month','day'])['revenue'].mean()\n\nmean_revenue_day_month[mean_revenue_day_month.isin(mean_revenue_day_month.nlargest(5))]","e20967ec":"train_sales['year'].value_counts()","74917317":"plt.figure(figsize=(14,8))\ntrain_sales.groupby('year')['revenue'].mean().plot.bar(rot=0)\nplt.title('Averaged revenue per year')\nplt.xlabel('Year')\nplt.ylabel('Revenue');","c9ab4e2c":"train_sales['dayname'] = train_sales['date'].dt.day_name()\ntrain_sales.groupby('dayname')['revenue'].mean().plot.bar(rot=90)\nplt.title('Averaged revenue per week day')\nplt.xlabel('Week day')\nplt.ylabel('Revenue');","fc4149f5":"train_sales['dayname'].value_counts().plot.bar()\nplt.title('Distribution of week days in dataframe');","321f29c5":"plt.figure(figsize=(14,8))\nplt.title('Distribution of mean item_price')\nmean_price = train_sales.groupby(['shop_id','item_id','date_block_num'])['item_price'].mean().values\nplt.hist(mean_price,bins=30)\nplt.xlabel('Values')\nplt.ylabel('Frequency');","c5e749cf":"plt.figure(figsize=(14,8))\nplt.title('Distribution of mean item_price on log scale')\nplt.hist(np.log1p(train_sales.groupby(['shop_id','item_id','date_block_num'])['item_price'].mean().values),bins=30)\nplt.xlabel('Values')\nplt.ylabel('Frequency');","714e18a3":"plt.scatter(train_sales['month'],train_sales['item_price']);","da6d8796":"train_sales[train_sales['item_price']==train_sales['item_price'].max()]","049b1e21":"train_sales[train_sales['item_price']==train_sales['item_price'].min()]","63a6b614":"shops_per_item = (train_sales.groupby('item_id')['shop_id'].nunique()>=2).astype(int).sum()\nprint('There are {0} items that relate to more than one shop'.format(shops_per_item))","86a9411e":"(train_sales['item_id'].value_counts()==1).astype(int).sum()","f0434a3a":"test_sales = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')\ntest_sales.info()","e67f5d66":"test_sales.head()","8e95284b":"test_sales['shop_id'].value_counts().unique()","0576ddc0":"test_sales['item_id'].value_counts().unique()","15187469":"diff_test_items = set(train_sales.item_id.unique()).difference(test_sales.item_id.unique())\nprint('Number of items that are in train set, but are not in test one : {0}'.format(len(diff_test_items))) \ndiff_train_items = set(test_sales.item_id.unique()).difference(train_sales.item_id.unique())\nprint('Number of items that are in test set, but are not in train one : {0}'.format(len(diff_train_items))) \ndiff_test_shops = set(train_sales.shop_id.unique()).difference(test_sales.shop_id.unique())\nprint('Number of shops that are in train set, but are not in test one : {0}'.format(len(diff_test_shops))) \ndiff_train_shops = set(test_sales.shop_id.unique()).difference(train_sales.shop_id.unique())\nprint('Number of shops that are in test set, but are not in train one : {0}'.format(len(diff_train_shops))) ","56626bef":"plt.figure(figsize=(15,12))\ndict_returned = dict(train_sales[train_sales['item_cnt_day']<0].month.value_counts())\ndict_returned = dict(sorted(dict_returned.items(), key=lambda x: int(x[0][1]) if x[0][0]=='0' else int(x[0])))\nplt.bar(range(len(dict_returned.values())),dict_returned.values())\nplt.xticks(range(len(dict_returned.values())),dict_returned.keys())\nplt.title('Number of times the goods were returned during different monthes')\nplt.xlabel('Monthes')\nplt.ylabel('Cases of returning the goods');\n","e2fb1e9b":"plt.figure(figsize=(15,12))\ndict_returned = dict(train_sales[train_sales['item_cnt_day']<0].date_block_num.value_counts())\ndict_returned = dict(sorted(dict_returned.items(), key=lambda x: int(x[0])))\nplt.bar(range(len(dict_returned.values())),dict_returned.values())\nplt.xticks(range(len(dict_returned.values())),dict_returned.keys())\nplt.title('Number of times the goods were returned during different date_block_num')\nplt.xlabel('date_block_num')\nplt.ylabel('Cases of returning the goods');\n\n","96d35236":"(train_sales[train_sales['item_cnt_day']<0]['item_id'].value_counts()).nlargest(50)","c6ccdcbe":"sales = train_sales[train_sales['item_cnt_day']<0]['item_id'].value_counts()\nidx = list(sales[sales>=10].index)","f30d8f05":"items_categories_merged[items_categories_merged['item_id'].isin(idx)].item_category_name.unique()","887dbfe8":"submission = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')\nsubmission[\"item_cnt_month\"] = 1\nsubmission.to_csv('lb_probing1.csv',index=False)\nsubmission[\"item_cnt_month\"] = 0\nsubmission.to_csv('lb_probing2.csv',index=False)","2ecc0d8d":"y_hat_mean = (1.41241**2-1.25011**2-1)\/-2\nprint('Mean of target values in public leaderboard is : {0}'.format(y_hat_mean))","6293968e":"items_categories_merged.head()","4623e101":"def exclude_preprositions(x):\n    x = x.split(' ')\n    x = ' '.join(i for i in x if not i in prepositions_to_exclude).strip()\n    return x","898ec89a":"from sklearn.feature_extraction.text import TfidfVectorizer\nitems_categories_merged['type_of_category']=items_categories_merged['item_category_name'].apply(lambda x: x.split(' ')[0].strip())\ndict_types = dict(items_categories_merged['type_of_category'].value_counts())\ncat, _ = zip(*sorted(dict_types.items(),key=lambda x: x[1])[::-1][:5])\nprint('Most frequent types of categories : {0}'.format(cat))\nnum_features = 10\nsymbols_to_exclude = ['[',']','!','.',',','*','(',')','\"',':']\nprepositions_to_exclude = ['\u0432','\u043d\u0430','\u0443','the','a','an','of','\u0434\u043b\u044f']\nfor symbol in symbols_to_exclude:\n    items_categories_merged['item_name'] = items_categories_merged['item_name'].str.replace(symbol,'')\nitems_categories_merged['item_name'] = items_categories_merged['item_name'].str.lower()\nitems_categories_merged['item_name'] = items_categories_merged['item_name'].str.replace('-',' ')\nitems_categories_merged['item_name'] = items_categories_merged['item_name'].str.replace('\/',' ')\nitems_categories_merged['item_name'] = items_categories_merged['item_name'].str.strip()\nitems_categories_merged['item_name'] = items_categories_merged['item_name'].apply(exclude_preprositions)\nvectorizer = TfidfVectorizer(max_features=num_features)\nres = vectorizer.fit_transform(items_categories_merged['item_name'])\nprint('Top {0} features of tfidf : {1}'.format(num_features,vectorizer.get_feature_names()))\ncount_vect_df = pd.DataFrame(res.todense(), columns=vectorizer.get_feature_names())\nitems_categories_merged = pd.concat([items_categories_merged,count_vect_df],axis=1)","3a814f23":"items_categories_merged['type_of_category'].unique()","4766d96d":"items_categories_merged.drop(columns=['item_name','item_category_name'],inplace=True)","e9fff9c1":"import gc\ndel vectorizer, res, count_vect_df\ngc.collect();","92719739":"import re\ndef create_city_name(x):\n    for i in not_city:\n        if i in x:\n            return 'unk_city'\n    return x.split(' ')[0].strip()\ndef create_shop_type(x):\n    to_return = 'unk_type'\n    for i in type_of_shops:\n        regex = re.compile(i)\n        if re.search(regex,x):\n                to_return = i \n    return to_return\nnot_city = ['\u0412\u044b\u0435\u0437\u0434\u043d\u0430\u044f \u0422\u043e\u0440\u0433\u043e\u0432\u043b\u044f','\u0418\u043d\u0442\u0435\u0440\u043d\u0435\u0442-\u043c\u0430\u0433\u0430\u0437\u0438\u043d','\u0426\u0438\u0444\u0440\u043e\u0432\u043e\u0439 \u0441\u043a\u043b\u0430\u0434 1\u0421-\u041e\u043d\u043b\u0430\u0439\u043d']\ntype_of_shops = ['\u0422\u0420\u0426', '\u0422\u0426','\u0422\u0420\u041a','\u0422\u041a','\u041c\u0422\u0420\u0426']+not_city\nshops['city_name'] = shops['shop_name'].apply(create_city_name)\nshops['shop_type'] = shops['shop_name'].apply(create_shop_type)","72f8c601":"shops.head()","372d99c3":"shops.drop(columns='shop_name',inplace=True)","d2e6240c":"mean = train_sales.groupby(['date_block_num','shop_id','item_id'])['item_cnt_day'].sum().mean()\nprint('Mean of target value in train data : {0}'.format(mean))\nif np.abs(mean-y_hat_mean)<0.2:\n    print('The mean of train and test targets is aligned!')\nelse:\n    print('The mean of train and test targets is not aligned!')","d9ea564b":"from itertools import product\nmatrix = []\ncols = ['shop_id','item_id','date_block_num']\nfor i in range(34):\n    sales = train_sales[train_sales.date_block_num==i]\n    matrix.append(np.array(list(product(sales.shop_id.unique(), sales.item_id.unique(),[i])), dtype='int16'))\n\nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix.sort_values(cols,inplace=True)","184ed854":"def fn(x):\n    return list(x)[0]","2145cd52":"train_sales = train_sales.groupby(['shop_id','item_id','date_block_num'],as_index=False).agg({'item_cnt_day': np.sum,'item_price' : np.mean,\n                                               'month' : fn})\ntrain_sales = matrix.merge(train_sales,on=['shop_id','item_id','date_block_num'],how='left')\ntrain_sales['item_cnt_month'] = train_sales['item_cnt_day'].fillna(0).clip(0,20)\ntrain_sales.drop(columns='item_cnt_day',inplace=True)\nprint('Mean of target value in train_sales column : {0}'.format(train_sales['item_cnt_month'].mean()))\nif np.abs(train_sales['item_cnt_month'].mean()-y_hat_mean)<2:\n    print('The mean of train and test targets is aligned!')","b04cae8f":"test_sales['date_block_num'] = 34\ntest_sales.drop(columns='ID',inplace=True)\ndata = pd.concat([train_sales,test_sales],ignore_index=True, sort=False, keys=['shop_id','item_id','date_block_num'])\ndata.head()","44ac8d2a":"month_mapping = data[['month','date_block_num']].dropna().drop_duplicates().sort_values(by=['date_block_num'])\\\n.set_index('date_block_num').to_dict()['month']\nmonth_mapping.update({34:'11'})","0e6ba156":"data = data.sort_values(by=['date_block_num','shop_id','item_id'])\ndata['item_price'] = data['item_price'].fillna(0)\ndata['month'] = data['date_block_num'].map(month_mapping)","95b050dc":"data.head()","6b293cb1":"del test_sales, train_sales, matrix\ngc.collect();","4c3fb6ae":"holiday_monthes = ['01','02','03','05','06','11']\ndata['is_holiday']=data['month'].apply(lambda x: 1 if x in holiday_monthes else 0)","145e2a47":"data['item_price'] = data['item_price'].fillna(0)\nlower, upper = np.percentile(data['item_price'].values,[1,99])\ndata['item_price'] = data['item_price'].clip(lower,upper)","afcb6054":"plt.hist(np.log1p(data['item_price'].values));","6264dd0d":"data['revenue'] = data['item_price']*data['item_cnt_month']","b25b489f":"def lag_feature(df, lags, col):\n    tmp = df[['shop_id','item_id','date_block_num',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['shop_id','item_id','date_block_num',col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","d760b27f":"def aggregated_previous(data,column,target_col,lags=[1],type_='mean'):\n    for i in lags:\n        tmp_data = data.copy()\n        tmp_data.loc[:,'date_block_num'] +=1\n        if isinstance(column,list):\n            to_group = ['date_block_num']+column\n            name = '_'.join(i for i in column)\n        else:\n            to_group = ['date_block_num']+[column]\n            name = column\n        tmp_data = tmp_data.groupby(to_group).agg({target_col:type_})\n        tmp_data.rename(columns={target_col:target_col+'_previous_{0}_by_'.format(type_)+name+'_lag_'+str(i)},inplace=True)\n        data = data.merge(tmp_data,how='left',right_index=True,left_on=to_group)\n    return data","7910c0bd":"data = lag_feature(data, [1,3,6,12], 'item_cnt_month')\ndata = lag_feature(data, [1,3,6,12], 'item_price')\ndata = aggregated_previous(data,'shop_id','item_cnt_month',[1,3])\ndata = aggregated_previous(data,'item_id','item_cnt_month',[1,3])\ndata = aggregated_previous(data,'shop_id','revenue',[1])\ndata = aggregated_previous(data,'item_id','revenue',[1])","2aebb65c":"data.drop(columns='revenue',inplace=True)","b0bc9bbf":"data.head()","8a2b2e0e":"data.fillna(0,inplace=True)","d3fca16a":"data = data.merge(shops,on='shop_id',how='left')\ndata = data.merge(items_categories_merged,on='item_id',how='left')","592a8691":"# data = aggregated_previous(data,['shop_id','item_category_id'],'item_cnt_month',[1])\n# data = aggregated_previous(data,['shop_id','item_category_id'],'item_cnt_month',[1],'sum')\ndata.fillna(0,inplace=True)","3aecffc0":"data.head(5)","b2b0dd22":"data.columns","42711031":"del items, shops, items_categories_merged\ngc.collect();","fe221b9b":"data.drop(columns='item_price',inplace=True)","a31ff911":"to_encode = ['month','city_name','shop_type','type_of_category']\nnunique_cat = {}\nfor i in to_encode:\n    data[i] = data[i].factorize()[0]\n    nunique_cat.update({i:data[i].nunique()})\nnunique_cat.update({'shop_id':data['shop_id'].nunique()})\nnunique_cat.update({'item_id':data['item_id'].nunique()})\nnunique_cat.update({'item_category_id':data['item_category_id'].nunique()})\nprint('Factorized all the columns!')","e536f07b":"data.head()","97743932":"data.columns","b8d18739":"def cast_categorical(data):\n    data['is_holiday'] = data['is_holiday'].astype('uint8')\n    data['shop_id'] = data['shop_id'].astype('uint8')\n    data['month'] = data['month'].astype('uint8')\n    data['shop_type'] = data['shop_type'].astype('uint8')\n    data['city_name'] = data['city_name'].astype('uint8')\n    data['item_category_id'] = data['item_category_id'].astype('uint8')\n    data['date_block_num'] = data['date_block_num'].astype('uint8')\n    data['item_id'] = data['item_id'].astype('uint16')\n    data['type_of_category'] = data['type_of_category'].astype('uint8')","0c9871da":"def cast_numerical(data):\n    for i in data.columns:\n        if 'float' in str(data[i].dtype):\n            data[i] = data[i].astype('float16')","31ca9625":"cast_categorical(data)\ncast_numerical(data)","9d6958d0":"data.info()","341a7387":"np.isfinite(data).sum()","7f3775f6":"train, test = data[data.date_block_num<34],data[data.date_block_num==34]\ndel data\ngc.collect();","44d0f522":"partA = train[train.date_block_num<32]\npartB = train[train.date_block_num == 32]\npartC = train[train.date_block_num == 33]","a797cd43":"part_A_x = partA.drop(columns=['item_cnt_month','date_block_num'])\npart_A_y = partA['item_cnt_month']\npart_B_x = partB.drop(columns=['item_cnt_month','date_block_num'])\npart_B_y = partB['item_cnt_month']\npart_C_x = partC.drop(columns=['item_cnt_month','date_block_num'])\npart_C_y = partC['item_cnt_month']\ntest = test.drop(columns=['item_cnt_month','date_block_num'])","557c47e9":"del train, partA,partB, partC\ngc.collect();","7b748167":"to_rename = {'\u0432\u0435\u0440\u0441\u0438\u044f':'version','\u0440\u0435\u0433\u0438\u043e\u043d':'region','\u0440\u0443\u0441\u0441\u043a\u0430\u044f':'rus','\u0446\u0438\u0444\u0440\u043e\u0432\u0430\u044f':'numeric','\u0444\u0438\u0433\u0443\u0440\u043a\u0430':'figure',\n            '\u0444\u0438\u0440\u043c':'firm','\u043a\u043e\u043b\u043b\u0435\u043a\u0446\u0438\u044f':'collection'}\npart_A_x.rename(columns=to_rename,inplace=True)\npart_B_x.rename(columns=to_rename,inplace=True)\npart_C_x.rename(columns=to_rename,inplace=True)\ntest.rename(columns=to_rename,inplace=True)\n","77b44c1d":"eval_set = [(part_A_x,part_A_y),(part_B_x,part_B_y),(part_C_x,part_C_y)]","a1d99d80":"import lightgbm as lgb\nfrom lightgbm import plot_importance","c414bb85":"lgb_model = lgb.LGBMRegressor(feature_fraction= 0.75,\n               metric = 'rmse',\n               max_depth = 8, \n               min_data_in_leaf = 2**7, \n               bagging_fraction = 0.75, \n               learning_rate = 0.03, \n               objective = 'mse', \n               bagging_seed = 2**7, \n               num_leaves = 100,\n               bagging_freq =1,\n               verbose = 1,\n            random_state=5,\n                             n_estimators=300)\nlgb_model.fit(part_A_x,part_A_y,eval_metric=\"rmse\", \n    eval_set=eval_set, \n    verbose=True, \n    early_stopping_rounds = 10)","171b72fd":"plot_importance(lgb_model,ax=plt.subplots(1,1,figsize=(15,12))[1])","9642bd1e":"lgb_B = lgb_model.predict(part_B_x)\nlgb_C = lgb_model.predict(part_C_x)\nlgb_test = lgb_model.predict(test)","62ee88cd":"lgb_model._Booster.__del__()","8e51f289":"gc.collect();","960ee6f0":"def make_arch(numerical_cols,categorical_cols):\n    tf.keras.backend.clear_session()\n    tf.random.set_seed(5)\n    inputs = []\n    embeddings = []\n    for cat_col in categorical_cols:\n        if not cat_col=='is_holiday':\n            no_of_unique_cat = nunique_cat[cat_col]\n            embedding_size = int(min(np.ceil((no_of_unique_cat)\/2), 50))\n            input = tf.keras.layers.Input(shape = (1,),name='input_for_{0}'.format(cat_col))\n            embs = tf.keras.layers.Embedding(no_of_unique_cat+1, embedding_size, name = 'embeddings_for_{0}'.format(cat_col))(input)\n            drop = tf.keras.layers.SpatialDropout1D(0.4)(embs)\n            reshape = tf.keras.layers.Reshape(target_shape = (embedding_size,),name='reshape_for_{0}'.format(cat_col))(drop)\n            embeddings.append(reshape)\n            inputs.append(input)\n        else:\n            input = tf.keras.layers.Input(shape = (1,),name='input_for_{0}'.format(cat_col))\n            embs = tf.keras.layers.Dense(4,activation='relu')(input)\n            embeddings.append(reshape)\n            inputs.append(input)\n    numeric_input = tf.keras.layers.Input(shape=(len(numerical_cols),), name='input_for_numerical')\n    numeric_embs = tf.keras.layers.Dense(32)(numeric_input)\n    leaky_relu = tf.keras.layers.LeakyReLU(0.1)(numeric_embs)\n    drop_concat = tf.keras.layers.Dropout(0.2)(leaky_relu)\n    inputs.append(numeric_input)\n    embeddings.append(drop_concat)\n    concat = tf.keras.layers.Concatenate()(embeddings)\n    concat_dense = tf.keras.layers.Dense(8)(concat)\n    leaky_relu2 = tf.keras.layers.LeakyReLU(0.1)(concat_dense)\n    last_dense = tf.keras.layers.Dense(1,activation='relu')(leaky_relu2)\n    model = tf.keras.Model(outputs=last_dense,inputs=inputs)\n    return model\n\ndef root_mean_squared_error(y_true, y_pred):\n        return tf.keras.backend.sqrt(tf.keras.backend.mean(tf.keras.backend.square(y_pred - y_true))) ","16485843":"import tensorflow as tf\nfrom sklearn.preprocessing import StandardScaler\ncategorical_cols = [i for i in part_A_x.columns if 'int' in str(part_A_x[i].dtype)]\nnumerical_cols = [i for i in part_A_x.columns if 'float' in str(part_A_x[i].dtype)]\n\ncategorical_input_A = [part_A_x[i].values for i in categorical_cols]\nscaler = StandardScaler()\nnumerical_input_A = scaler.fit_transform(part_A_x[numerical_cols].astype('float32'))\ncategorical_input_A.append(numerical_input_A)\n\ncategorical_input_B = [part_B_x[i].values for i in categorical_cols]\nnumerical_input_B = scaler.transform(part_B_x[numerical_cols].astype('float32'))\ncategorical_input_B.append(numerical_input_B)\n\ncategorical_input_C = [part_C_x[i].values for i in categorical_cols]\nnumerical_input_C = scaler.transform(part_C_x[numerical_cols].astype('float32'))\ncategorical_input_C.append(numerical_input_C)\n\ncategorical_input_test = [test[i].values for i in categorical_cols]\nnumerical_input_test = scaler.transform(test[numerical_cols].astype('float32'))\ncategorical_input_test.append(numerical_input_test)\n\n","ed2db402":"model = make_arch(numerical_cols,categorical_cols)\n\nmodel.compile(loss=root_mean_squared_error,optimizer=tf.keras.optimizers.SGD(momentum=0.1,lr=0.009))\n\nhistory = model.fit(x=categorical_input_A,y=part_A_y.values,validation_data = [categorical_input_B,part_B_y.values],\n     batch_size=512,epochs=4,callbacks=[tf.keras.callbacks.EarlyStopping(patience=1)])","0fd427e5":"loss, val_loss = history.history['loss'],history.history['val_loss']\nplt.figure(figsize=(13,8))\nplt.title('NN training loss versus validation')\nplt.plot(range(len(loss)),loss,'b')\nplt.plot(range(len(val_loss)),val_loss,'r')\nplt.xticks(range(len(val_loss)));\nplt.yticks(np.arange(min(val_loss),max(loss),0.01));","a502ed17":"model.evaluate(categorical_input_C,part_C_y.values)","91951192":"nn_B = model.predict(categorical_input_B)\nnn_C = model.predict(categorical_input_C)\nnn_test = model.predict(categorical_input_test)","63a57952":"from sklearn.linear_model import Lasso\nfrom sklearn.metrics import r2_score, mean_squared_error","85038652":"lasso = Lasso(random_state=SEED,alpha=0.04)\nlasso.fit(numerical_input_A,part_A_y)","a96f4638":"r2_B = r2_score(y_true=part_B_y,y_pred=lasso.predict(numerical_input_B))\nmse_B = np.sqrt(mean_squared_error(y_true=part_B_y,y_pred=lasso.predict(numerical_input_B)))\nprint('RMSE on B part: {0}'.format(mse_B))\nprint('r2_score on B part: {0}'.format(r2_B))","e6d430d8":"r2_C = r2_score(y_true=part_C_y,y_pred=lasso.predict(numerical_input_C))\nmse_C = np.sqrt(mean_squared_error(y_true=part_C_y,y_pred=lasso.predict(numerical_input_C)))\nprint('RMSE on C part: {0}'.format(mse_C))\nprint('r2_score on C part: {0}'.format(r2_C))","e58627f2":"lasso_B = lasso.predict(numerical_input_B)\nlasso_C = lasso.predict(numerical_input_C)\nlasso_test = lasso.predict(numerical_input_test)","63996eba":"from sklearn.linear_model import Ridge\n","549ed7c0":"ridge = Ridge(random_state=SEED,alpha=0.04)\nridge.fit(numerical_input_A,part_A_y)","9cfff4cc":"r2_B = r2_score(y_true=part_B_y,y_pred=ridge.predict(numerical_input_B))\nmse_B = np.sqrt(mean_squared_error(y_true=part_B_y,y_pred=ridge.predict(numerical_input_B)))\nprint('RMSE on B part: {0}'.format(mse_B))\nprint('r2_score on B part: {0}'.format(r2_B))","2ee42111":"r2_C = r2_score(y_true=part_C_y,y_pred=ridge.predict(numerical_input_C))\nmse_C = np.sqrt(mean_squared_error(y_true=part_C_y,y_pred=ridge.predict(numerical_input_C)))\nprint('RMSE on C part: {0}'.format(mse_C))\nprint('r2_score on C part: {0}'.format(r2_C))","c8f8cdef":"ridge_B = ridge.predict(numerical_input_B)\nridge_C = ridge.predict(numerical_input_C)\nridge_test = ridge.predict(numerical_input_test)","81d2ea17":"part_B_2 = pd.DataFrame(index=range(len(nn_B)))\npart_B_2['lasso'] = lasso_B\npart_B_2['ridge'] = ridge_B\npart_B_2['lgb'] = lgb_B\npart_B_2['nn'] = nn_B\ncols = part_B_2.columns\nfor i in cols:\n    for j in cols:\n        if i!=j:\n            part_B_2['{0}_{1}_distance'.format(i,j)] = part_B_2[i]-part_B_2[j]\npart_B_2['target'] = part_B_y.values","6089b40c":"plt.figure(figsize=(12,8))\nsns.heatmap(part_B_2.corr(), \n        xticklabels=part_B_2.corr().columns,\n        yticklabels=part_B_2.corr().columns)","f16a6622":"part_C_2 = pd.DataFrame(index=range(len(nn_C)))\npart_C_2['lasso'] = lasso_C\npart_C_2['ridge'] = ridge_C\npart_C_2['lgb'] = lgb_C\npart_C_2['nn'] = nn_C\ncols = part_C_2.columns\nfor i in cols:\n    for j in cols:\n        if i!=j:\n            part_C_2['{0}_{1}_distance'.format(i,j)] = part_C_2[i]-part_C_2[j]\npart_C_2['target'] = part_C_y.values","e2a14e7d":"plt.figure(figsize=(12,8))\nsns.heatmap(part_C_2.corr(), \n        xticklabels=part_C_2.corr().columns,\n        yticklabels=part_C_2.corr().columns)","f1befde7":"test_2 = pd.DataFrame(index=range(len(nn_test)))\ntest_2['lasso'] =lasso_test\ntest_2['ridge'] =ridge_test\ntest_2['lgb'] = lgb_test\ntest_2['nn'] = nn_test\ncols = test_2.columns\nfor i in cols:\n    for j in cols:\n        if i!=j:\n            test_2['{0}_{1}_distance'.format(i,j)] = test_2[i]-test_2[j]","1e829e2b":"test_2.head()","05eba292":"test_2.corr()","727e5a47":"features = part_B_2.columns.tolist()\ntarget = features.pop(features.index('target'))\nX_B , Y_B = part_B_2[features], part_B_2[target]\nX_C , Y_C = part_C_2[features], part_C_2[target]","d7c24c45":"features","c45070f8":"from sklearn.linear_model import SGDRegressor","8cf3dd4d":"lr = SGDRegressor(alpha=0.001,random_state=SEED)","fc4617d1":"lr.fit(X_B,Y_B)","5dabbd3e":"r2_B = np.sqrt(r2_score(y_true=Y_B,y_pred=lr.predict(X_B)))\nmse_B = np.sqrt(mean_squared_error(y_true=Y_B,y_pred=lr.predict(X_B)))\nprint('RMSE on B part: {0}'.format(mse_B))\nprint('r2_score on B part: {0}'.format(r2_B))","9d00802e":"r2_C = np.sqrt(r2_score(y_true=Y_C,y_pred=lr.predict(X_C)))\nmse_C = np.sqrt(mean_squared_error(y_true=Y_C,y_pred=lr.predict(X_C)))\nprint('RMSE on C part: {0}'.format(mse_C))\nprint('r2_score on C part: {0}'.format(r2_C))","301561ff":"final_preds = np.clip(lr.predict(test_2),0,20)","d3b3ae22":"submission = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')\nto_merge = test\nY_test = final_preds\nto_merge['item_cnt_month'] = Y_test\nsub_to_merge = to_merge[['shop_id','item_id','item_cnt_month']].copy()\nsubmission = submission.merge(sub_to_merge,how='left',on=['shop_id','item_id'])\nsubmission = submission[['ID','item_cnt_month']]\nsubmission.to_csv('submission_stacking.csv',index=False)","5306c724":"Let's add lag features based on price and item_cnt_month (it can take some time).","ca78b2f6":"That's obvious that from the related field we can already make some new.","ee5fa3d9":"Let's understand what is the name of the categories which conist of maximum and minumum number of items.","d770f267":"## Final predictions","f5c3d337":"We see that we basically dealed with outliers using winsorization.","d352fabc":"## (1,3). Additional features based on items_categories_merged df.","9466caea":"From plot above that's obvious that data is shuffled.","4f1ebd6c":"That's pretty obvious that lots of items are related to movies stuff.","7c3a8167":"Let's plot the averaged, maximum and minumum revenue per month during all the time. But first of all let's see how monthes are distributed across the dataset.","bb6df8eb":"## 4. Aggregating data.","5de2acd9":"Before all of this, we should delete outliers in item_price column. For this step we will use a technique called winsorization.","122070b5":"We see that there are items which weren't used in the test set at all! And the same for the train one. We can also see that there some shops which are not included in test set. To deal with items we will than make the empty dataframe which will have all the possible products of item_id,shop_id and date_block_num and merge it with our ones. For now let's work with item_cnt_day column and make some usefull plots.","16ee2faa":"From the plots above we can say that people tend to return items right after the New Year. Maybe it's because their presents weren't so good. Basically, our hypotezis can be wrong, as events related to January seems to appear oftener than others, but I don't think that this is the case, as events related to December (12) appears oftener than the ones related to February (02), but still more items are returned during February. Now let's see what items are returned most often.","e8e71de5":"We see that from the shop_name we can already retrieve two new features : name of the city of the shop and type of the shop.","3790f001":"## 2nd lvl model training","8bb8f034":"## Lasso Regression training","326a80f4":"Now let's move to train_sales data.","9d34285b":"$MSE0 = \\frac{ \\sum{i=1}^{N} (yi - 0)^2 }{N} = \\frac{ \\sum{i=1}^{N} y_i^2 }{N}$\n\n$MSE1 = \\frac{ \\sum{i=1}^{N} (yi - 1)^2 }{N} = \\frac{ \\sum{i=1}^{N} (yi^2 - 2yi + 1) }{N} = \\frac{ \\sum{i=1}^{N} yi^2 - 2 \\sum{i=1}^{N} yi + N }{N} = MSE0 - \\frac{2}{N} \\sum{i=1}^{N} y_i + 1$\n\n$\\frac{\\sum{i=1}^{N} yi}{N} = \\frac{MSE1 - MSE0 - 1}{-2}$","e68d2ead":"Function make_arch - makes neural network architecture. In order to work correctly with categorical columns - embedding layer is used. Also a spatial dropout along with dropout is used to reduce overfitting.","5f7eee16":"If we search for date 2013\/11\/29, we will find out that it was the date of black friday in russia, thus that's adequate to have the maximum revenue on this day.","3d511233":"Let's also concat everything with test data in order to use lagged features.","0f4d7ff7":"We will firstly reduce the memory usage by casting columns to appropriate dtypes and split the data by month.","6c4884eb":"We see that there are few categories that have only one related item. ","e8c9461d":"For now we gonna only use the xgboost as our main model (that was also an idea to use lstm or to make ensemble, but that ideas will be exploited with time), thus we only need to factorize our categorical columns.","5e21e5f3":"For sure there are some items that are out of date. Let's compare our dataframe with test data.","3e1d003f":"# NN training","f7ae68e3":"# Ridge regression training","1071b8d2":"We see that as mean price goes higher, the sum of sold items goes lower meaning that there is a dependency between price and item_cnt. We should probably use this as feature.","fcdee99e":"Hm, we have a maximum value for 11th month, that's interesting, it could be because of \"Black Friday\". As we in revenue column we can have negative values (if the goods are returned) we will visualize only the data rows that have values > 0.","776dae18":"Let's check for NaN's.","091e02ea":"Now let's visualize the revenue for each year and for each week day and make some additional charts.","7c58f3d1":"## LGBM training","fade5b80":"## 6. Features based on time-series, merging everything togather","91a2dd06":"Now let's derive some deeper insights from our data.","51e3f0be":"Let's see how much items related to each category we have.","d1cd90e5":"We see that our item_price field contains -1 value as minimum and 3e05 as maximum. This values could be NaN's or outliers.","3faaf72e":"And obviously we have only one item per category. Now let's merge two dataframes.","8f83c68d":"We see that there are some words that tend to appear much frequently than others. Maybe we should make a feature based on it.","c413c84c":"That's strange that we have the same amount of events related to each shop and each item. By multiplying those values together we will get the exact number of rows as in dataset, and that's very strange, mainly it seems that the test set is just a catalog of items for which we need to predict prices. The other thing is that if we look on shop_id and item_id columns we will notice that the data is ordered a bit. Ordered by the shop_id and item_id columns.","a3fbe9f9":"From the chart above that's obvious that there are some categories that are the most popular.","03494530":"Let's make some analysis of timesires data. First of all we will plot sum of item_cnt_day grouped by different date related columns.","f0f68f77":"As we see monthes occurancies are in general evenly distributed across dataset, only January (01) appears more freuqently than others.","1f658cb4":"We will also visualize distribution of averaged item_price.","94f25bcc":"# Part 5: Machine learning part","76dda701":"As the length of data before and after merge is the same, it seems that we haven't missed any values. Now we will go on with data containing shops.","7211bfbe":"That's important to now in which date range we are working, so let's understand what is the minimum and maximum dates.","5959139c":"That's obvious that with time number of item sold is decreasing, let's now plot the situation for the price.","061b18e1":"We can now submit two predictions and calculate the mean of the leader board target. We then can use it to make our score better and to align cross_validation set with test one. We will use the following calculations (full conversation about LB probing is accessible by the following [link](https:\/\/www.kaggle.com\/c\/competitive-data-science-predict-future-sales\/discussion\/79142)).","c17eff42":"# Part 1 : EDA","3fae8dc4":"Obviously there are no NaN's or they are imputed by other values.","3788baeb":"Hello everyone! The notebook is related to Future sales prediction task. It consists of EDA, feature engineering, leaderboard probing and finally model training. I tried to grasp all the concepts learned in the course (https:\/\/www.coursera.org\/learn\/competitive-data-science) and fullfill them here. Please notice that some of concepts were borrowed from other competitors and from forum, you will find the links to them by following the notebook.","5817c46a":"Let's now merge our data with other dataframes.","21bb6cfe":"It's useful to examine if we have any category that doesn't have any item or if we have any item that belongs to more than one category.","c9c8069b":"The columns are divided into numerical and categorical. Numerical columns are scaled.","ba042ccb":"We should also check for some additional features at item_category_name field.","b7cc1531":"We will also clean unuseful data.","72ccf025":"# First level models : LGBM, NN, Lasso, Ridge.","3a5e43de":"# Part 4: Feature processing","3f5e7b50":"We need our training data to be very similar to the test one. In the test data there are many items that were not sold, as we need to predict number of sales for a catalog. To achieve the similarity of train and test data we will basically, create a product data frame which consists of each pair of shop and item for a unique month, by this we will achieve the same target distribution as in test set (the idea is retrieved from this [notebook](https:\/\/www.kaggle.com\/dlarionov\/feature-engineering-xgboost)).","08bd0028":"## Casting data to related dtypes and basic preparation.","3a5d1d59":" ## 5. Additional features based on monthes","fc0ca18f":"# Part 3: Feature engineering","fa73241e":"That's obviously that we can have multiple shops that sell one item. Let's now understand the difference between training and testing datasets. We already now that data in testing dataset is montly aggregated.","e32007c3":"# Part 2: Leader board probing","8d2370e3":"Let's check if our dataset is shuffled.","71f37018":"That's obvious that the averaged revenue is bigger during the 12th month, as the dates related to it are close to New Year holiday. Let's now visualize min and max values.","107e7e48":"We now will gather all the predictions and train second lvl model.","1f8b69c9":"In the code below we will make a new feature name type_of_category, make some item_name cleaning (exlucde mess) and construct tfidf features based on it.","966a6711":"We see that item_prices are normally distributed with some outliers, thus we will need to clip them or to use log scale.","7f33b4cb":"We also want to make the same for day column.","0afbab3c":"My hypotesis is that the data is grouped by date_block_num, shop_id and item_id as the date is a bit unordered.","5c41b965":"We see that most of all - games are returned. Basically the above analysis didn't help us to derive new features but we got the point that we should probably make features based on categories and types of items. So far we got the following things:\n\n1. We can make additional features from item_categories df such as type of category. \n1. We can make additional features from shop_name field in shops df such as shop_name and shop_type.\n1. We can make additional features based on item name, maybe using tfidf or count vectorizer.\n1. We should probably concat our data with all the other shops, date_block_nums and item_ids, if there is a missing one it means that it just wasn't sold. Also it's benefitial to make our data of the same format as test one.\n1. There is a dependency between the revenue and month number, thus people tend to by more products during monthes that have holidays, thus we can add a new feature indicating if month has a holiday plus number of month.\n1. There is a dependency between price and number of sold items, we can make some time series features based on it, but we should also remember to deal with outliers in item price. Also there is a way to construct new features via mean encoding (as we have lot's of categorical features).\n\n","efd1f5c4":"After the submission we see that the score for MSE0 is $1.25011^2$ and for MSE1 is $1.41241^2$. Let's now calculate the target mean of public leaderboard.","9f5ca039":"Let's make two new features for simplicity, mainly year, month and day.","d026de9d":"Creating nn predictions.","ddda7d06":"First of all, we will read and explore data. We will start with item_categories dataframe.","12fc5064":"Now we can move on to items.csv.","e8c911d7":"We will add a feature based on monthes, that indicates if the month contains holiday or not.","d43c9357":"## 2. Additional features based on shop_name.","fc9d99dc":"We see that all the items belong to at least one category.","4c7869d2":"We will now plot results of the network."}}