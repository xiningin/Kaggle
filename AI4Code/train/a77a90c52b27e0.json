{"cell_type":{"e58f8ad6":"code","6bc5b311":"code","f197a1d0":"code","aa82c5ee":"code","ff48dabf":"code","74f42c09":"code","d8b5c91d":"code","78a8ea84":"code","188f29ed":"code","8d8b126e":"code","e67a0738":"code","7adb1640":"code","19c637df":"code","8adb0ba0":"code","ad421ed8":"code","c34f70f8":"code","9497f134":"code","3aae7dba":"code","ce4d1ce8":"code","03afccd6":"code","0184ea6a":"code","074d7795":"code","64095390":"code","05b7c144":"code","38227909":"code","08b58e6e":"code","42dfe83b":"code","bc40ca00":"code","eb9181f6":"code","813b8c23":"code","96154e94":"code","fe6a6577":"code","39a4d94f":"code","6d751809":"code","65a3cffc":"code","faa2b7d8":"code","4fc9401b":"code","239f0409":"code","c2a78902":"code","7d6a4811":"code","c383d0ee":"code","beaefd0f":"code","31374a5a":"code","875153f6":"code","81d60a16":"code","1735e35d":"markdown","37b677dd":"markdown","af841a09":"markdown","5b96358b":"markdown","5ab6a450":"markdown","b841f218":"markdown","22f20f04":"markdown","d3baceaa":"markdown","c25c039e":"markdown","4e41c982":"markdown","b48996b4":"markdown","32e0470a":"markdown","824452d8":"markdown"},"source":{"e58f8ad6":"#importing the necessary libraries\nimport warnings\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import RobustScaler\nimport numpy as np\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, \\\n    classification_report\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\npd.set_option('display.max_columns',None)\npd.set_option('display.max_rows',None)\nwarnings.simplefilter(action=\"ignore\")","6bc5b311":"#reading the data set\ndf = pd.read_csv(\"..\/input\/diabetes-data-set\/diabetes.csv\")","f197a1d0":"# To display the top 5 rows\ndf.head()","aa82c5ee":"# To display the bottom 5 rows\ndf.tail()","ff48dabf":"# total number of rows and columns\n# Dataset comprises of 768 observations and 9 characteristics.\n# Out of which one is dependent variable and rest 8 are independent variables\ndf.shape","74f42c09":"# Data has only float and integer values\n# No variable column has null\/missing values\ndf.info()","d8b5c91d":"# Getting various summary statistics\n# There is notably a large difference between 99% and max values of predictors \u201cInsulin\u201d,\u201dSkinThickness\u201d,\u201dDiabetesPedigreeFunction\u201d\n# There are extreme values-Outliers in our data set\ndf.describe([0.05,0.25,0.50,0.75,0.90,0.95,0.99]).T","78a8ea84":"# Target variable categorical\ndf.Outcome.unique()","188f29ed":"df.Outcome.value_counts()","8d8b126e":"plt.figure(figsize=(6,4))\nsns.heatmap(df.corr(),cmap='Blues',annot=False)","e67a0738":"#Outcome correlation matrix\nk = 9 #number of variables for heatmap\ncols = df.corr().nlargest(k, 'Outcome')['Outcome'].index\ncm = df[cols].corr()\nplt.figure(figsize=(10,6))\nsns.heatmap(cm, annot=True, cmap = 'viridis')","7adb1640":"# see how the data is distributed.\ndf.hist(figsize = (20,20))","19c637df":"for col in df.columns:\n    if col != \"Outcome\":\n        sns.catplot(\"Outcome\", col, data = df)","8adb0ba0":"#Observation units for variables with a minimum value of zero are NaN, except for the pregnancy variable.\ndf.describe([0.05,0.25,0.50,0.75,0.90,0.95,0.99]).T","ad421ed8":"# NaN values of 0 for Glucose, Blood Pressure, Skin Thickness, Insulin, BMI\n# We can write Nan instead of 0\ncols = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"]\nfor col in cols:\n    df[col].replace(0,np.NaN,inplace=True)","c34f70f8":"# now we can see missing values\ndf.isnull().sum()","9497f134":"# We can fill in NaN values with a median according to the target\nfor col in df.columns:\n    df.loc[(df[\"Outcome\"]==0) & (df[col].isnull()),col] = df[df[\"Outcome\"]==0][col].median()\n    df.loc[(df[\"Outcome\"]==1) & (df[col].isnull()),col] = df[df[\"Outcome\"]==1][col].median()","3aae7dba":"df.isnull().sum()","ce4d1ce8":"def outlier_thresholds(dataframe, variable):\n    quartile1 = dataframe[variable].quantile(0.10)\n    quartile3 = dataframe[variable].quantile(0.90)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit","03afccd6":"def has_outliers(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    if dataframe[(dataframe[variable] < low_limit) | (dataframe[variable] > up_limit)].any(axis=None):\n        print(variable, \"yes\")","0184ea6a":"for col in df.columns:\n    has_outliers(df, col)","074d7795":"def replace_with_thresholds(dataframe, numeric_columns):\n    for variable in numeric_columns:\n        low_limit, up_limit = outlier_thresholds(dataframe, variable)\n        dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n        dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit","64095390":"replace_with_thresholds(df, df.columns)","05b7c144":"for col in df.columns:\n    has_outliers(df, col)","38227909":"df.describe([0.05,0.25,0.50,0.75,0.90,0.95,0.99]).T","08b58e6e":"df['New_Glucose_Class'] = pd.cut(x=df['Glucose'], bins=[0,139,200],labels = [\"Normal\",\"Prediabetes\"])","42dfe83b":"df['New_BMI_Range'] = pd.cut(x=df['BMI'], bins=[0,18.5,24.9,29.9,100],labels = [\"Underweight\",\"Healty\",\"Overweight\",\"Obese\"])","bc40ca00":"df['New_BloodPressure'] = pd.cut(x=df['BloodPressure'], bins=[0,79,89,123],labels = [\"Normal\",\"HS1\",\"HS2\"])","eb9181f6":"df['New_SkinThickness'] = df['SkinThickness'].apply(lambda x: 1 if x <= 18.0 else 0)","813b8c23":"df.head()","96154e94":"def one_hot_encoder(dataframe, categorical_columns, nan_as_category=False):\n    original_columns = list(dataframe.columns)\n    dataframe = pd.get_dummies(dataframe, columns=categorical_columns,\n                               dummy_na=nan_as_category, drop_first=True)\n    new_columns = [col for col in dataframe.columns if col not in original_columns]\n    return dataframe, new_columns","fe6a6577":"categorical_columns = [col for col in df.columns\n                           if len(df[col].unique()) <= 10\n                      and col != \"Outcome\"]\ncategorical_columns","39a4d94f":"df, new_cols_ohe = one_hot_encoder(df,categorical_columns)\nnew_cols_ohe","6d751809":"df.head()","65a3cffc":"def robust_scaler(variable):\n    var_median = variable.median()\n    quartile1 = variable.quantile(0.25)\n    quartile3 = variable.quantile(0.75)\n    interquantile_range = quartile3 - quartile1\n    if int(interquantile_range) == 0:\n        quartile1 = variable.quantile(0.05)\n        quartile3 = variable.quantile(0.95)\n        interquantile_range = quartile3 - quartile1\n        if int(interquantile_range) == 0:\n            quartile1 = variable.quantile(0.01)\n            quartile3 = variable.quantile(0.99)\n            interquantile_range = quartile3 - quartile1\n            z = (variable - var_median) \/ interquantile_range\n            return round(z, 3)\n\n        z = (variable - var_median) \/ interquantile_range\n        return round(z, 3)\n    else:\n        z = (variable - var_median) \/ interquantile_range\n    return round(z, 3)","faa2b7d8":"like_num = [col for col in df.columns if df[col].dtypes != 'O' and len(df[col].value_counts()) < 10]\ncols_need_scale = [col for col in df.columns if col not in new_cols_ohe\n                   and col not in \"Outcome\"\n                   and col not in like_num]\n\nfor col in cols_need_scale:\n    df[col] = robust_scaler(df[col])","4fc9401b":"df.head()","239f0409":"df.info()","c2a78902":"X = df.drop(\"Outcome\",axis=1)\ny = df[\"Outcome\"]","7d6a4811":"models = [('LR', LogisticRegression()),\n          ('KNN', KNeighborsClassifier()),\n          ('CART', DecisionTreeClassifier()),\n          ('RF', RandomForestClassifier()),\n          ('SVR', SVC(gamma='auto')),\n          ('XGBM', XGBClassifier()),\n          ('GB',GradientBoostingClassifier()),\n          (\"LightGBM\", LGBMClassifier())]\n\n# evaluate each model in turn\nresults = []\nnames = []\n\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=123456)\n    cv_results = cross_val_score(model, X, y, cv=kfold, scoring=\"accuracy\")\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","c383d0ee":"#Let's choose the highest 4 models\n# GBM\ngbm_model = GradientBoostingClassifier()\n# Model Tuning\ngbm_params = {\"learning_rate\": [0.01, 0.1, 0.001],\n               \"max_depth\": [3,5, 8, 10],\n               \"n_estimators\": [200, 500, 1000],\n               \"subsample\": [1, 0.5, 0.8]}\ngbm_cv_model = GridSearchCV(gbm_model,\n                            gbm_params,\n                            cv=10,\n                            n_jobs=-1,\n                            verbose=2).fit(X, y)\ngbm_cv_model.best_params_\n# Final Model\ngbm_tuned = GradientBoostingClassifier(**gbm_cv_model.best_params_).fit(X,y)","beaefd0f":"# LightGBM: \nlgb_model = LGBMClassifier()\n# Model Tuning\nlgbm_params = lgbm_params = {\"learning_rate\": [0.01, 0.5, 1],\n                             \"n_estimators\": [200, 500, 1000],\n                             \"max_depth\": [6, 8, 10],\n                             \"colsample_bytree\": [1, 0.5, 0.4 ,0.3 , 0.2]}\nlgbm_cv_model = GridSearchCV(lgb_model,\n                             lgbm_params,\n                             cv=10,\n                             n_jobs=-1,\n                             verbose=2).fit(X, y)\nlgbm_cv_model.best_params_\n# Final Model\nlgbm_tuned = LGBMClassifier(**lgbm_cv_model.best_params_).fit(X, y)","31374a5a":"# Random Forests:\nrf_model = RandomForestClassifier()\n# Model Tuning\nrf_params = {\"n_estimators\" :[100,200,500,1000], \n             \"max_features\": [3,5,7], \n             \"min_samples_split\": [2,5,10,30],\n            \"max_depth\": [3,5,8,None]}\nrf_cv_model = GridSearchCV(rf_model, \n                           rf_params, \n                           cv=10, \n                           n_jobs=-1, \n                           verbose=2).fit(X, y)\nrf_cv_model.best_params_\n# Final Model\nrf_tuned = RandomForestClassifier(**rf_cv_model.best_params_).fit(X, y)","875153f6":"# XGB\nxgb_model = XGBClassifier()\n# Model Tuning\nxgb_params = {\n    \"learning_rate\": [0.01, 0.1, 0.2, 1],\n    \"min_samples_split\": np.linspace(0.1, 0.5, 10),\n    \"max_depth\":[3,5,8],\n    \"subsample\":[0.5, 0.9, 1.0],\n    \"n_estimators\": [100,1000]}\nxgb_cv_model  = GridSearchCV(xgb_model,\n                             xgb_params, \n                             cv = 10, \n                             n_jobs = -1, \n                             verbose = 2).fit(X,y)\nxgb_cv_model.best_params_\nxgb_tuned = XGBClassifier(**xgb_cv_model.best_params_).fit(X,y)","81d60a16":"# evaluate each model in turn\nmodels = [('RF', rf_tuned),\n          ('GBM',gbm_tuned ),\n          (\"LightGBM\", lgbm_tuned),\n          (\"XGB\",xgb_tuned)]\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=123456)\n    cv_results = cross_val_score(model, X, y, cv=10, scoring=\"accuracy\")\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","1735e35d":"## 3.BMI\n\nThe standard weight status categories associated with BMI ranges for adults are shown in the following table.\n\n* Below 18.5 -> **Underweight**\n* 18.5 \u2013 24.9 -> **Normal or Healthy Weight**\n* 25.0 \u2013 29.9 -> **Overweight**\n* 30.0 and Above -> **Obese**","37b677dd":"# General Information on Variables","af841a09":"## 4.Triceps Skinfolds \nFor adults, the standard normal values for triceps skinfolds are: \n* 18.0mm (women)","5b96358b":"## 2.BloodPressure\nThe diastolic reading, or the bottom number, is the pressure in the arteries when the heart rests between beats. This is the time when the heart fills with blood and gets oxygen. A normal diastolic blood pressure is lower than 80. A reading of 90 or higher means you have high blood pressure.\n\n* **Normal**: Systolic below 120 and diastolic below 80\n* **Elevated**: Systolic 120\u2013129 and diastolic under 80\n* **Hypertension stage 1**: Systolic 130\u2013139 and diastolic 80\u201389\n* **Hypertension stage 2**: Systolic 140-plus and diastolic 90 or more\n* **Hypertensive crisis**: Systolic higher than 180 and diastolic above 120.","5ab6a450":"## Model Tuning","b841f218":"# Pima Indians Diabetes Database\n\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, **all patients here are females at least 21 years old of Pima Indian heritage**.\n\nThe datasets consists of several medical predictor variables and one target variable, **Outcome**. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n\nWe build a **machine learning model** to accurately predict whether or not the patients in the dataset have **diabetes or not.**\n\n- **Pregnancies**: Number of times pregnant\n- **Glucose**: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n- **BloodPressure**: Diastolic blood pressure (mm Hg)\n- **SkinThickness**: Triceps skin fold thickness (mm)\n- **Insulin**: 2-Hour serum insulin (mu U\/ml)\n- **BMI**: Body mass index (weight in kg\/(height in m)^2)\n- **DiabetesPedigreeFunction**: Diabetes pedigree function\n- **Age**: Age (years)\n- **Outcome**: Class variable (0 or 1) 268 of 768 are 1, the others are 0","22f20f04":"## Exploratory Data Analysis","d3baceaa":"## Data Visualization","c25c039e":"### Conclusion: \n- Classification models used for diabetes data set; Logistic Regression, KNN, CART, RF, SVC, XGBM, GB, LightGBM. \n- Hyperparameter optimization was applied to the 4 models(RF,GBM,LightGBM,XGB) with the highest score.\n- The best cross validation score belongs to GB.\n","4e41c982":"## FEATURE ENGINEERING","b48996b4":"##\u00a01.Glucose Tolerance Test\nIt is a blood test that involves taking multiple blood samples over time, usually 2 hours.It used to diagnose diabetes. The results can be classified as normal, impaired, or abnormal. \n* **Normal Results for Diabetes ->** Two-hour glucose level less than 140 mg\/dL\n\n* **Impaired Results for Diabetes ->** Two-hour glucose level 140 to 200 mg\/dL \n\n* **Abnormal (Diagnostic) Results for Diabetes ->** Two-hour glucose level greater than 200 mg\/dL\n\n","32e0470a":"## MODELING","824452d8":"## DATA PREPROCESSING"}}