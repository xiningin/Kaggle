{"cell_type":{"6f62ca3b":"code","91bb4537":"code","871781f2":"code","a0bf3623":"code","73238802":"code","55549249":"code","14ca9791":"code","fb5b5d25":"code","1a6c8eec":"code","4f5297f3":"code","dc888b2e":"code","fb883552":"code","a11b4600":"code","9c808023":"code","964efb99":"markdown","2266df4e":"markdown"},"source":{"6f62ca3b":"import os\nimport sys\nimport random\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport cv2\n\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom itertools import chain\nimport skimage\nfrom PIL import Image\nfrom skimage.io import imread, imshow, imread_collection, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.util import crop, pad\nfrom skimage.morphology import label\nfrom skimage.color import rgb2gray, gray2rgb, rgb2lab, lab2rgb\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\nfrom keras.models import Model, load_model,Sequential\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Dense, UpSampling2D, RepeatVector, Reshape\nfrom keras.layers.core import Dropout, Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras import backend as K\n\n\nimport tensorflow as tf\n\nwarnings.filterwarnings('ignore', category=UserWarning, module='skimage')\nseed = 42\nrandom.seed = seed\nnp.random.seed = seed","91bb4537":"# IMG_WIDTH = 256\n# IMG_HEIGHT = 256\nIMG_WIDTH = 224\nIMG_HEIGHT = 224\nIMG_CHANNELS = 3\nINPUT_SHAPE=(IMG_HEIGHT, IMG_WIDTH, 1)\n# TRAIN_PATH = '..\/input\/art-images-drawings-painting-sculpture-engraving\/dataset\/dataset_updated\/training_set\/painting\/'\n# TRAIN_PATH = '..\/input\/nature\/nature\/'\nTRAIN_PATH = '..\/input\/landscapes\/data\/'\n\n\ntrain_ids = next(os.walk(TRAIN_PATH))[2]","871781f2":"X_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\nmissing_count = 0\nprint('Getting train images ... ')\nsys.stdout.flush()\ncnt = 0\nfor n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):\n    path = TRAIN_PATH + id_+''\n    try:\n        img = imread(path)\n        img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n        X_train[n-missing_count] = img\n    except:\n#         print(\" Problem with: \"+path)\n        missing_count += 1\n#     cnt+=1\n#     if (cnt > 2000):\n#         break\n\nX_train = X_train.astype('float32') \/ 255.\nprint(\"Total missing: \"+ str(missing_count))","a0bf3623":"imshow(X_train[5])\nplt.show()","73238802":"X_train, X_test = train_test_split(X_train, test_size=20, random_state=seed)","55549249":"def Colorize():\n    encoder_input = Input(shape=(224, 224, 1,))\n    \n    # Encoder\n    encoder_output = Conv2D(128, (3, 3), activation='relu', padding='same', strides=1)(encoder_input)\n    encoder_output = MaxPooling2D((2, 2), padding='same')(encoder_output)\n    encoder_output = Conv2D(128, (4, 4), activation='relu', padding='same')(encoder_output)\n    encoder_output = Conv2D(128, (3, 3), activation='relu', padding='same', strides=1)(encoder_output)\n    encoder_output = MaxPooling2D((2, 2), padding='same')(encoder_output)\n    encoder_output = Conv2D(256, (4, 4), activation='relu', padding='same')(encoder_output)\n    encoder_output = Conv2D(256, (3, 3), activation='relu', padding='same', strides=1)(encoder_output)\n    encoder_output = MaxPooling2D((2, 2), padding='same')(encoder_output)\n    encoder_output = Conv2D(256, (4, 4), activation='relu', padding='same')(encoder_output)\n    encoder_output = Conv2D(256, (3, 3), activation='relu', padding='same')(encoder_output)\n    encoder_output = Conv2D(256, (3, 3), activation='relu', padding='same')(encoder_output)\n\n    # Decoder\n    decoder_output = Conv2D(128, (3, 3), activation='relu', padding='same')(encoder_output)\n    decoder_output = Conv2D(64, (3, 3), activation='relu', padding='same')(decoder_output)\n    decoder_output = UpSampling2D((2, 2))(decoder_output)\n    decoder_output = Conv2D(128, (3, 3), activation='relu', padding='same')(decoder_output)\n    decoder_output = UpSampling2D((2, 2))(decoder_output)\n    decoder_output = Conv2D(64, (4, 4), activation='relu', padding='same')(decoder_output)\n    decoder_output = Conv2D(64, (3, 3), activation='relu', padding='same')(decoder_output)\n    decoder_output = Conv2D(32, (2, 2), activation='relu', padding='same')(decoder_output)\n    decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n    decoder_output = UpSampling2D((2, 2))(decoder_output)\n    return Model(inputs=encoder_input, outputs=decoder_output)\n\nmodel = Colorize()\nmodel.compile(optimizer='adam', loss='mean_squared_error')\nmodel.summary()","14ca9791":"\n# Image transformer\ndatagen = ImageDataGenerator(\n        shear_range=0,\n        zoom_range=0,\n        rotation_range=0,\n        horizontal_flip=False)\n\n#Generate training data\ndef image_a_b_gen(dataset=X_train, batch_size = 20):\n    for batch in datagen.flow(dataset, batch_size=batch_size):\n        X_batch = rgb2gray(batch)\n        grayscaled_rgb = gray2rgb(X_batch)\n        lab_batch = rgb2lab(batch)\n        X_batch = lab_batch[:,:,:,0]\n        X_batch = X_batch.reshape(X_batch.shape+(1,))\n        Y_batch = lab_batch[:,:,:,1:] \/ 128\n        yield X_batch, Y_batch","fb5b5d25":"# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='loss', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5,\n                                            min_lr=0.00001)\nfilepath = \"Art_Colorization_Model.h5\"\ncheckpoint = ModelCheckpoint(filepath,\n                             save_best_only=True,\n                             monitor='loss',\n                             mode='min')\n\nmodel_callbacks = [learning_rate_reduction,checkpoint]\n# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)","1a6c8eec":"BATCH_SIZE = 20\nhistory = model.fit_generator(image_a_b_gen(X_train,BATCH_SIZE),\n            epochs=15,\n            verbose=1,\n            steps_per_epoch=X_train.shape[0]\/BATCH_SIZE,\n             callbacks=model_callbacks\n                   )","4f5297f3":"model.save(filepath)\nmodel.save_weights(\"Art_Colorization_Weights.h5\")","dc888b2e":"loaded_model = load_model('..\/input\/colormodel\/Art_Colorization_Model.h5')\n# BATCH_SIZE = 20\n# history = loaded_model.fit_generator(image_a_b_gen(X_train,BATCH_SIZE),\n#             epochs=15,\n#             verbose=1,\n#             steps_per_epoch=X_train.shape[0]\/BATCH_SIZE,\n#              callbacks=model_callbacks\n#                    )","fb883552":"loaded_model.save(filepath)\nloaded_model.save_weights(\"Art_Colorization_Weights.h5\")","a11b4600":"sample = X_test\ncolor_me = gray2rgb(rgb2gray(sample))\n# color_me_embed = create_inception_embedding(color_me)\ncolor_me = rgb2lab(color_me)[:,:,:,0]\ncolor_me = color_me.reshape(color_me.shape+(1,))\n\noutput = loaded_model.predict(color_me)\noutput = output * 128\n\ndecoded_imgs = np.zeros((len(output),224, 224, 3))\n\nfor i in range(len(output)):\n    cur = np.zeros((224, 224, 3))\n    cur[:,:,0] = color_me[i][:,:,0]\n    cur[:,:,1:] = output[i]\n    decoded_imgs[i] = lab2rgb(cur)\n    cv2.imwrite(\"img_\"+str(i)+\".jpg\", lab2rgb(cur))","9c808023":"plt.figure(figsize=(20, 6))\nfor i in range(10):\n    # grayscale\n    plt.subplot(3, 10, i + 1)\n    plt.imshow(rgb2gray(X_test)[i].reshape(224, 224))\n    plt.gray()\n    plt.axis('off')\n \n    # recolorization\n    plt.subplot(3, 10, i + 1 +10)\n    plt.imshow(decoded_imgs[i].reshape(224, 224,3))\n    plt.axis('off')\n    \n    # original\n    plt.subplot(3, 10, i + 1 + 20)\n    plt.imshow(X_test[i].reshape(224, 224,3))\n    plt.axis('off')\n \nplt.tight_layout()\nplt.show()","964efb99":"## NOTE:-\n\n* Our Dataset consists of all coloured images and we have to use black and white image for training the model therefore we would be converting the colored image to grayScaled image and then turning it to RGB format to get the complete black and white image conversion.\n* I have also used Opencv to read the image but Opencv reads the image in BGR format, so we need to convert the image to RGB format first before turning it to greyscale.\n* You could also avoid using Opencv.\n* There are many lines of code which you would to be see in traditional colorizing technique but I have commented out those for better results in my case and also I have explained the reason with comments why that part of the code is used in traditional Colorizing technique.","2266df4e":"**There are two techniques to generate colored image from its gray scaled form:-**\n1. **Turn the RGB image into LAB image, then separate the L value and ab value from the image and then train the model to predict the ab value**.\n2. **Turn the RGB image into LUV image, then separate the L value and UV value from the image and then train the model to predict the UV value**.\n\nHere I have used Lab value to colorize the image, where  L stands for lightness, and a and b for the color spectra green\u2013red and blue\u2013yellow.**"}}