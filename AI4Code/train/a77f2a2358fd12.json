{"cell_type":{"6aaac69a":"code","5b16d32e":"code","656b2c60":"code","ce127115":"code","c465f4cd":"code","f83f74cc":"code","3c9ce9c0":"code","6cf8a6e0":"code","d48776e4":"code","3e8018fa":"code","56ba8c74":"code","23ae4566":"code","843adb8c":"code","a5b656fc":"code","f6430ab1":"code","cd6c5b39":"code","bd83110b":"code","bee6cadb":"code","f82276be":"code","e37977fa":"code","cc5dbd75":"code","8f1f9b5d":"code","a0fec9aa":"code","a7a43509":"code","7096d78c":"code","f1855eae":"code","10019562":"code","34fd59e2":"code","30c862aa":"code","5398a57c":"code","766ab8f1":"code","67bd0c44":"code","96c98d2f":"code","7e0deeaf":"code","aeb68d2e":"code","2d0b8ab7":"code","bfcdc0f4":"code","0d9cd854":"code","7a16d283":"code","2cae1cd3":"code","7cd9fc77":"code","234933bb":"code","acf2e21a":"code","67228042":"code","179bc467":"code","1ef85165":"code","68bc96bf":"code","0c41e14b":"code","bed6c099":"code","a9385ec9":"code","b1feff59":"code","9443ee70":"code","6f39602c":"code","59c9c48e":"code","fad63d78":"code","4ab5e9a9":"code","9e0e7095":"code","28552092":"code","e3b25bf7":"code","6cd4526a":"code","6ce26865":"code","64ee54e0":"code","1503887f":"code","800edb3b":"code","877fb966":"code","c7249eb8":"code","f2bc1e67":"code","1aeb3cea":"code","5fd8d91e":"code","990e356e":"code","fec25675":"code","82300ce5":"code","1267f4d4":"markdown","4b0f3d9e":"markdown","789067c1":"markdown","569c8f3b":"markdown","cc1dbc75":"markdown","186572d1":"markdown","3eab5a2e":"markdown","28a2f05e":"markdown","c5fa2046":"markdown","50521da8":"markdown","78a0a62e":"markdown","8a9ab4e8":"markdown","bde67d39":"markdown","0b51096b":"markdown","d2c7e67f":"markdown","ebf51599":"markdown","04f61546":"markdown","12c7edac":"markdown","cd6c9639":"markdown","a603346b":"markdown","696ade1c":"markdown","b0f08e0c":"markdown","e5b56858":"markdown","2a278a97":"markdown","c4cd5bac":"markdown","9456403c":"markdown","301a68c8":"markdown","3732483f":"markdown","e5a71fe7":"markdown","5b2bd9c3":"markdown","bda5fd53":"markdown","15cf63a9":"markdown","69066c83":"markdown","d559f4de":"markdown","3a1fd8b1":"markdown","b4c43272":"markdown","a87bbc57":"markdown","684dc4f1":"markdown","ab08228e":"markdown","bda5b693":"markdown","d1f7039e":"markdown","943f61ff":"markdown","47004ee3":"markdown","1887e93f":"markdown"},"source":{"6aaac69a":"import pandas as pd\nfrom scipy.io import arff\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nimport numpy as np\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score, roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nimport xgboost as xgb\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\n\nFOLDS =10\n%matplotlib inline","5b16d32e":"# Function to graph number of people by age\ndef cont_age(field):\n    plt.figure()\n    g = None\n    if field == \"Age\":\n        df_query_mri = df[df[\"Age\"] > 0]\n        g = sns.countplot(df_query_mri[\"Age\"])\n        g.figure.set_size_inches(18.5, 10.5)\n    else:\n        g = sns.countplot(df[field])\n        g.figure.set_size_inches(18.5, 10.5)\n    \nsns.despine()","656b2c60":"# Function to graph number of people per state [Demented, Nondemented]\ndef cont_Dementes(field):\n    plt.figure()\n    g = None\n    if field == \"Group\":\n        df_query_mri = df[df[\"Group\"] >= 0]\n        g = sns.countplot(df_query_mri[\"Group\"])\n        g.figure.set_size_inches(18.5, 10.5)\n    else:\n        g = sns.countplot(df[field])\n        g.figure.set_size_inches(18.5, 10.5)\n    \nsns.despine()","ce127115":"# 0 = F y 1= M\ndef bar_chart(feature):\n    Demented = df[df['Group']==1][feature].value_counts()\n    Nondemented = df[df['Group']==0][feature].value_counts()\n    df_bar = pd.DataFrame([Demented,Nondemented])\n    df_bar.index = ['Demented','Nondemented']\n    df_bar.plot(kind='bar',stacked=True, figsize=(8,5))","c465f4cd":"def report_performance(model):\n\n    model_test = model.predict(X_test)\n\n    print(\"Confusion Matrix\")\n    print(\"{0}\".format(metrics.confusion_matrix(y_test, model_test)))\n    print(\"\")\n    print(\"Classification Report\")\n    print(metrics.classification_report(y_test, model_test))","f83f74cc":"data = '\/kaggle\/input\/mri-and-alzheimers\/oasis_longitudinal.csv'\ndf = pd.read_csv (data)\ndf.head()","3c9ce9c0":"df.describe()","6cf8a6e0":"nu = pd.DataFrame(df['Group']=='Nondemented')\nnu[\"Group\"].value_counts() ","d48776e4":"f, ax = plt.subplots(figsize=(10, 8)) \ncorr = df.corr(method = 'pearson') \nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True), \n            square=True, ax=ax) ","3e8018fa":"df.corr(method = 'pearson') ","56ba8c74":"pd.scatter_matrix(df, alpha = 0.3, figsize = (14,8), diagonal = 'kde'); ","23ae4566":"g = sns.PairGrid(df, vars=['Visit','MR Delay','M\/F', 'Age', 'EDUC', 'SES', 'MMSE', 'eTIV', 'nWBV', 'ASF'],\n                 hue='Group', palette='RdBu_r')\ng.map(plt.scatter, alpha=0.8)\ng.add_legend();","843adb8c":"import seaborn as sb\nsb.factorplot('M\/F',data=df,hue='Group',kind=\"count\")","a5b656fc":"facet= sns.FacetGrid(df,hue=\"Group\", aspect=3)\nfacet.map(sns.kdeplot,'MMSE',shade= True)\nfacet.set(xlim=(0, df['MMSE'].max()))\nfacet.add_legend()\nplt.xlim(12.5)","f6430ab1":"cont_age(\"Age\")","cd6c5b39":"df['Group'] = df['Group'].replace(['Converted'], ['Demented'])\ndf.head(3)","bd83110b":"df.drop(['Subject ID'], axis = 1, inplace = True, errors = 'ignore')\ndf.drop(['MRI ID'], axis = 1, inplace = True, errors = 'ignore')\ndf.drop(['Visit'], axis = 1, inplace = True, errors = 'ignore')\n#for this study the CDR we eliminated it\ndf.drop(['CDR'], axis = 1, inplace = True, errors = 'ignore')\ndf.head(3)","bee6cadb":"# 1 = Demented, 0 = Nondemented\ndf['Group'] = df['Group'].replace(['Converted'], ['Demented'])\n\ndf['Group'] = df['Group'].replace(['Demented', 'Nondemented'], [1,0])    \ndf.head(3)","f82276be":"# 1= M, 0 = F\n\ndf['M\/F'] = df['M\/F'].replace(['M', 'F'], [1,0])  \ndf.head(3)","e37977fa":"from sklearn.preprocessing import LabelEncoder\nencoder=LabelEncoder()\nencoder.fit(df.Hand.values)\nlist(encoder.classes_)\n#Transoformamos\nencoder.transform(df.Hand.values)\ndf[['Hand']]=encoder.transform(df.Hand.values)\nencoder2=LabelEncoder()\nencoder2.fit(df.Hand.values)\nlist(encoder2.classes_)","cc5dbd75":"data_na = (df.isnull().sum() \/ len(df)) * 100\ndata_na = data_na.drop(data_na[data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Lost proportion (%)' :round(data_na,2)})\nmissing_data.head(20)","8f1f9b5d":"from sklearn.impute  import SimpleImputer\n# We perform it with the most frequent value \nimputer = SimpleImputer ( missing_values = np.nan,strategy='most_frequent')\n\nimputer.fit(df[['SES']])\ndf[['SES']] = imputer.fit_transform(df[['SES']])\n\n# We perform it with the median\nimputer = SimpleImputer ( missing_values = np.nan,strategy='median')\n\nimputer.fit(df[['MMSE']])\ndf[['MMSE']] = imputer.fit_transform(df[['MMSE']])","a0fec9aa":"from sklearn.impute  import SimpleImputer\n# We perform it with the median\nimputer = SimpleImputer ( missing_values = np.nan,strategy='median')\n\nimputer.fit(df[['MMSE']])\ndf[['MMSE']] = imputer.fit_transform(df[['MMSE']])","a7a43509":"from sklearn.preprocessing import StandardScaler\ndf_norm = df\nscaler = StandardScaler()\ndf_norm[['Age','MR Delay','M\/F','Hand','EDUC','SES','MMSE','eTIV','nWBV','ASF']]=scaler.fit_transform(df[['Age','MR Delay','M\/F','Hand','EDUC','SES','MMSE','eTIV','nWBV','ASF']])","7096d78c":"df_norm.head(3)","f1855eae":"df.drop(['Hand'], axis = 1, inplace = True, errors = 'ignore')\ndf.drop(['MR Delay'], axis = 1, inplace = True, errors = 'ignore')","10019562":"df.head()","34fd59e2":"data_test = df","30c862aa":"X = data_test.drop([\"Group\"],axis=1)\ny = data_test[\"Group\"].values\nX.head(3)","5398a57c":"# We divide our data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 0)","766ab8f1":"print(\"{0:0.2f}% Train\".format((len(X_train)\/len(data_test.index)) * 100))\nprint(\"{0:0.2f}% Test\".format((len(X_test)\/len(data_test.index)) * 100))","67bd0c44":"print(\"Original Demented : {0} ({1:0.2f}%)\".format(len(df_norm.loc[df_norm['Group'] == 1]), 100 * (len(df_norm.loc[df_norm['Group'] == 1]) \/ len(df_norm))))\nprint(\"Original Nondemented : {0} ({1:0.2f}%)\".format(len(df_norm.loc[df_norm['Group'] == 0]), 100 * (len(df_norm.loc[df_norm['Group'] == 0]) \/ len(df_norm))))\nprint(\"\")\nprint(\"Training Demented : {0} ({1:0.2f}%)\".format(len(y_train[y_train[:] == 1]), 100 * (len(y_train[y_train[:] == 1]) \/ len(y_train))))\nprint(\"Training Nondemented : {0} ({1:0.2f}%)\".format(len(y_train[y_train[:] == 0]), 100 * (len(y_train[y_train[:] == 0]) \/ len(y_train))))\nprint(\"\")\nprint(\"Test Demented : {0} ({1:0.2f}%)\".format(len(y_test[y_test[:] == 1]), 100 * (len(y_test[y_test[:] == 1]) \/ len(y_test))))\nprint(\"Test Nondemented : {0} ({1:0.2f}%)\".format(len(y_test[y_test[:] == 0]), 100 * (len(y_test[y_test[:] == 0]) \/ len(y_test))))","96c98d2f":"# Number of trees in random forest\nn_estimators = range(10,250)\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = range(1,40)\n# Minimum number of samples required to split a node\nmin_samples_split = range(3,60)","7e0deeaf":"# Create the random grid\nparametro_rf = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split}","aeb68d2e":"model_forest = RandomForestClassifier(n_jobs=-1)\nforest_random = RandomizedSearchCV(estimator = model_forest, param_distributions = parametro_rf, n_iter = 100, cv = FOLDS, \n                               verbose=2, random_state=42, n_jobs = -1, scoring='neg_mean_absolute_error')\nforest_random.fit(X_train, y_train)","2d0b8ab7":"forest_random.best_params_","bfcdc0f4":"model_rf = forest_random.best_estimator_\nmodel_rf =  RandomForestClassifier(n_estimators=60,min_samples_split=8,max_features='sqrt',max_depth= 37)\nmodel_rf.fit(X_train,y_train)","0d9cd854":"test_score = cross_val_score(model_rf, X_train, y_train, cv=FOLDS, scoring='roc_auc').mean()\ntest_score","7a16d283":"test_score = cross_val_score(model_rf, X_train, y_train, cv=FOLDS, scoring='accuracy').mean()\ntest_score","2cae1cd3":"Predicted_rf= model_rf.predict(X_test)\ntest_recall = recall_score(y_test, Predicted_rf, pos_label=1)\nfpr, tpr, thresholds = roc_curve(y_test, Predicted_rf, pos_label=1)\ntest_auc = auc(fpr, tpr)","7cd9fc77":"# Number of trees in random forest\nn_estimators = range(50,280)\n# Maximum number of levels in tree\nmax_depth =  range(1,40)\n# Minimum number of samples required to split a node\nmin_samples_leaf = [3,4,5,6,7,8,9,10,15,20,30,40,50,60]","234933bb":"# Create the random grid\nparametro_Et = {'n_estimators': n_estimators,\n               'max_depth': max_depth,\n               'min_samples_leaf': min_samples_leaf}","acf2e21a":"model_et = ExtraTreesClassifier(n_jobs=-1)\net_random = RandomizedSearchCV(estimator = model_et, param_distributions = parametro_rf, n_iter = 100, cv = FOLDS, \n                               verbose=2, random_state=42, n_jobs = -1, scoring='roc_auc')\net_random.fit(X_train, y_train)","67228042":"et_random.best_params_","179bc467":"n_estimators = range(10,200)\n\nlearning_rate = [0.0001, 0.001, 0.01, 0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.95,1]","1ef85165":"# Create the random grid\nparametros_ada = {'n_estimators': n_estimators,\n               'learning_rate': learning_rate}","68bc96bf":"model_ada = AdaBoostClassifier()\n\nada_random = RandomizedSearchCV(estimator = model_ada, param_distributions = parametros_ada, n_iter = 100, cv = FOLDS, \n                               verbose=2, random_state=42, n_jobs = -1, scoring='roc_auc')\nada_random.fit(X_train, y_train)","0c41e14b":"ada_random.best_params_","bed6c099":"parametros_gb = {\n    \"loss\":[\"deviance\"],\n    \"learning_rate\": [0.01, 0.025, 0.005,0.5, 0.075, 0.1, 0.15, 0.2,0.3,0.8,0.9],\n    \"min_samples_split\": [0.01, 0.025, 0.005,0.4,0.5, 0.075, 0.1, 0.15, 0.2,0.3,0.8,0.9],\n    \"min_samples_leaf\": [1,2,3,5,8,10,15,20,40,50,55,60,65,70,80,85,90,100],\n    \"max_depth\":[3,5,8,10,15,20,25,30,40,50],\n    \"max_features\":[\"log2\",\"sqrt\"],\n    \"criterion\": [\"friedman_mse\",  \"mae\"],\n    \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n    \"n_estimators\":range(1,100)\n    }","a9385ec9":"model_gb= GradientBoostingClassifier()\n\n\ngb_random = RandomizedSearchCV(estimator = model_gb, param_distributions = parametros_gb, n_iter = 100, cv = FOLDS, \n                               verbose=2, random_state=42, n_jobs = -1, scoring='roc_auc')\ngb_random.fit(X_train, y_train)","b1feff59":"gb_random.best_params_","9443ee70":"C = [0.001, 0.10, 0.1, 10, 25, 50,65,70,80,90, 100, 1000,2000,10000,20000,25000,30000,40000]\n\nkernel =  ['rbf']\n    \ngamma =[1e-2, 1e-3, 1e-4, 1e-5,1e-6,1e-7,1e-8,1]","6f39602c":"# Create the random grid\nparametros_svm = {'C': C,\n            'gamma': gamma,\n             'kernel': kernel}","59c9c48e":"model_svm = SVC()\nfrom sklearn.model_selection import GridSearchCV\nsvm_random = GridSearchCV(model_svm, parametros_svm,  cv = 20, \n                               verbose=2, n_jobs = -1, scoring='roc_auc')\nsvm_random.fit(X, y)","fad63d78":"param_xgb = {\n        'silent': [False],\n        'max_depth': [6, 10, 15, 20],\n        'learning_rate': [0.001, 0.01, 0.1, 0.2, 0,3],\n        'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n        'colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n        'colsample_bylevel': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n        'min_child_weight': [0.5, 1.0, 3.0, 5.0, 7.0, 10.0],\n        'gamma': [0, 0.25, 0.5, 1.0],\n        'reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0, 100.0],\n        'n_estimators': [50,100,120]}","4ab5e9a9":"from sklearn.model_selection import GridSearchCV\n\nmodel_xgb = xgb.XGBClassifier()\nxgb_random = RandomizedSearchCV(estimator = model_xgb, param_distributions = param_xgb, n_iter = 100, cv = FOLDS, \n                               verbose=2, random_state=42, n_jobs = -1, scoring='roc_auc')\nxgb_random.fit(X_train.values, y_train)","9e0e7095":"xgb_random.best_params_","28552092":"parametro_rf = forest_random.best_params_\n\nparametro_et = et_random.best_params_\n\nparametro_ada = ada_random.best_params_\n\nparametro_gb = gb_random.best_params_\n\nparametro_svm = svm_random.best_params_\n\nparametro_xgb= xgb_random.best_params_\n","e3b25bf7":"model_rf = forest_random.best_estimator_\n\nmodel_et = et_random.best_estimator_\n\nmodel_ada = ada_random.best_estimator_\n\nmodel_gb = gb_random.best_estimator_\n\nmodel_svc = svm_random.best_estimator_\n\nmodel_xgb= xgb_random.best_estimator_\n","6cd4526a":"kf = KFold(n_splits=FOLDS, random_state = 0, shuffle = True)\nfor i, (train_index, val_index) in enumerate(kf.split(X_train, y_train)):\n    Xtrain, Xval = X_train.values[train_index], X_train.values[val_index]\n    ytrain, yval = y_train[train_index], y_train[val_index]\n    \n    model_rf.fit(Xtrain, ytrain)\n    model_et.fit(Xtrain, ytrain)\n    model_ada.fit(Xtrain, ytrain)\n    model_gb.fit(Xtrain, ytrain)\n    model_svc.fit(Xtrain, ytrain)\n    model_xgb.fit(Xtrain, ytrain)\n    ","6ce26865":"rf_feature = model_rf.feature_importances_\nada_feature = model_ada.feature_importances_\ngb_feature = model_gb.feature_importances_\net_feature = model_et.feature_importances_\nxbg_feature = model_xgb.feature_importances_","64ee54e0":"cols = X.columns.tolist()\n# Create a dataframe with features\nfeature_dataframe = pd.DataFrame( {'features': cols,\n     'Random Forest feature importances': rf_feature,\n      'AdaBoost feature importances': ada_feature,\n    'Gradient Boost feature importances': gb_feature,\n    'Extra Trees  feature importances': et_feature,\n    'Xgboost feature importances': xbg_feature,\n    })","1503887f":"xbg_feature","800edb3b":"# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Random Forest feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Random Forest feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Random Forest Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')\n\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Extra Trees  feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Extra Trees  feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Extra Trees Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')\n\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['AdaBoost feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['AdaBoost feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'AdaBoost Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')\n\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Gradient Boost feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Gradient Boost feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Gradient Boosting Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')\n\ntrace = go.Scatter(\n    y = feature_dataframe['Xgboost feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Xgboost feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'XgboostFeature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')\n","877fb966":"# Create the new column that contains the average of the values.\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\nfeature_dataframe.head(3)","c7249eb8":"y = feature_dataframe['mean'].values\nx = feature_dataframe['features'].values\ndata = [go.Bar(\n            x= x,\n             y= y,\n            width = 0.5,\n            marker=dict(\n               color = feature_dataframe['mean'].values,\n            colorscale='Portland',\n            showscale=True,\n            reversescale = False\n            ),\n            opacity=0.6\n        )]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Barplots of Mean Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='bar-direct-labels')","f2bc1e67":"Predicted_rf= model_rf.predict(X_test)\nPredicted_ada = model_ada.predict(X_test)\nPredicted_gb = model_gb.predict(X_test)\nPredicted_et = model_et.predict(X_test)\nPredicted_svm= model_svc.predict(X_test)\nPredicted_xgb= model_xgb.predict(X_test.values)","1aeb3cea":"base_predictions_train = pd.DataFrame( {'RandomForest': Predicted_rf.ravel(),\n      'AdaBoost': Predicted_ada.ravel(),\n      'GradientBoost': Predicted_gb.ravel(),\n      'ExtraTrees': Predicted_et.ravel(),\n      'SVM': Predicted_svm.ravel(),\n      'XGB': Predicted_xgb.ravel(),\n     'Real value': y_test                                \n                                        \n    })\nbase_predictions_train.head(10)","5fd8d91e":"acc = [] # list to store all performance metric","990e356e":"model='Random Forest'\ntest_score = cross_val_score(model_rf, X_train, y_train, cv=FOLDS, scoring='accuracy').mean() # Get recall for each parameter setting\ntest_recall = recall_score(y_test, Predicted_rf, pos_label=1)\nfpr, tpr, thresholds = roc_curve(y_test, Predicted_rf, pos_label=1)\ntest_auc = auc(fpr, tpr)\nacc.append([model,test_score, test_recall, test_auc, fpr, tpr, thresholds])\n\nmodel='AdaBoost'\ntest_score = cross_val_score(model_ada, X_train, y_train, cv=FOLDS, scoring='accuracy').mean() # Get recall for each parameter setting\ntest_recall = recall_score(y_test, Predicted_ada, pos_label=1)\nfpr, tpr, thresholds = roc_curve(y_test, Predicted_ada, pos_label=1)\ntest_auc = auc(fpr, tpr)\nacc.append([model, test_score,test_recall, test_auc, fpr, tpr, thresholds])\n\nmodel='Gradient Boosting'\ntest_score = cross_val_score(model_gb, X_train, y_train, cv=FOLDS, scoring='accuracy').mean() # Get recall for each parameter setting\ntest_recall = recall_score(y_test, Predicted_gb, pos_label=1)\nfpr, tpr, thresholds = roc_curve(y_test, Predicted_gb, pos_label=1)\ntest_auc = auc(fpr, tpr)\nacc.append([model, test_score,test_recall, test_auc, fpr, tpr, thresholds])\n\nmodel='ExtraTrees'\ntest_score = cross_val_score(model_et, X_train, y_train, cv=FOLDS, scoring='accuracy').mean() # Get recall for each parameter setting\ntest_recall = recall_score(y_test, Predicted_et, pos_label=1)\nfpr, tpr, thresholds = roc_curve(y_test, Predicted_et, pos_label=1)\ntest_auc = auc(fpr, tpr)\nacc.append([model, test_score, test_recall, test_auc, fpr, tpr, thresholds])\n\nmodel='SVM'\ntest_score = cross_val_score(model_svc, X_train, y_train, cv=FOLDS, scoring='accuracy').mean() # Get recall for each parameter setting\ntest_recall = recall_score(y_test, Predicted_svm, pos_label=1)\nfpr, tpr, thresholds = roc_curve(y_test, Predicted_svm, pos_label=1)\ntest_auc = auc(fpr, tpr)\nacc.append([model, test_score, test_recall, test_auc, fpr, tpr, thresholds])\n\nmodel='Xgboost'\ntest_score = cross_val_score(model_xgb, X_train, y_train, cv=FOLDS, scoring='accuracy').mean() # Get recall for each parameter setting\ntest_recall = recall_score(y_test, Predicted_xgb, pos_label=1)\nfpr, tpr, thresholds = roc_curve(y_test, Predicted_xgb, pos_label=1)\ntest_auc = auc(fpr, tpr)\nacc.append([model,test_score, test_recall, test_auc, fpr, tpr, thresholds])\n","fec25675":"report_performance(model_et)","82300ce5":"result = pd.DataFrame(acc, columns=['Model', 'Accuracy', 'Recall', 'AUC', 'FPR', 'TPR', 'TH'])\nresult[['Model', 'Accuracy', 'Recall', 'AUC']]","1267f4d4":"## Graphics:","4b0f3d9e":" <a id=\"ch11\"><\/a>\n## 3.2 LabelEncoder","789067c1":"**Which are sex and our class**","569c8f3b":"<a id=\"ch5\"><\/a>\n## 2.3 Correlation matrix","cc1dbc75":" <a id=\"ch14\"><\/a>\n## 3.5 Export them to then select the features\n\ndf_norm.to_csv('DatasetSelectionAttributes.csv', sep=',',index=False)\n\nFor the selection of attributes we use the R Boruta framework.\n\n**Commands (R) :**\n\nlibrary(readr)\n\nlibrary(Boruta)\n\ncovertype <- read_csv('DatasetSelectionAttributes.csv')\n\nset.seed(111)\n\nboruta.trainer <- Boruta(Group~., data = covertype , doTrace = 2, maxRuns=500)\n\nprint(boruta.trainer)\n\nplot(boruta.trainer, las = 2)\n","186572d1":"**3\u00b0 AdaBoos**","3eab5a2e":"** 4\u00b0 Gradient Boosting**","28a2f05e":"**5\u00b0 Support Vector**","c5fa2046":"## Required libraries","50521da8":"<a id=\"ch21\"><\/a>\n# 7. Performance Metric for each model","78a0a62e":" <a id=\"ch9\"><\/a>\n# 3. Preprocessing","8a9ab4e8":"**6\u00b0 xgboost **","bde67d39":"<a href=\"https:\/\/ibb.co\/QMGP76c\"><img src=\"https:\/\/i.ibb.co\/cQd6KNv\/AzBoruta.png\" alt=\"AzBoruta\" border=\"0\"><\/a>","0b51096b":"<a id=\"ch8\"><\/a>\n## 2.6 Miscellaneous Graphics","d2c7e67f":"## Remove Columns selected by boruta","ebf51599":"Alzheimer's is a type of dementia that affects a person's Memory, Thought and Behavior. It is a disease that begins mildly and affects parts of the brain, which makes the person have difficulty, to remember newly learned information, constant changes in mood, and confusion with events, times and places.\n\u00a0\nAlzheimer's usually starts after age 60. The risk increases as the person ages. The risk of having this disease is greater if there are people in the family who have had this disease.\n\u00a0\nAs for the treatments that have been done for this disease, there is none that can stop the progress of this. So far what these treatments can achieve is to help alleviate some symptoms, reducing their intensity and contributing to a higher quality of life for patients and their families.\n\n<img src=\"https:\/\/gx0ri2vwi9eyht1e3iyzyc17-wpengine.netdna-ssl.com\/wp-content\/uploads\/2017\/01\/dementia2-804x369.jpg\" alt=\"AzBoruta\" border=\"0\">\n\n## objective\n\nImplement classification algorithms for the analysis of the medical dataset, in order to provide a prediction tool for the early diagnosis of the disease.\n\nV2 Predict CDR : https:\/\/www.kaggle.com\/rodrigox93\/multi-class-model-detect-cdr-acc-83-f1-78","04f61546":"**1\u00b0  Random Forest**","12c7edac":"# Selected Parameters\n\nAfter running RandomizedSearchCV several times, we found the most acceptable parameters for each of our models.\nWe will save these parameters to then make the adjustment of our models.","cd6c9639":"**Lost data**","a603346b":"<a id=\"ch19\"><\/a>\n# 5. Importance of characteristics \n\nAccording to the Sklearn documentation, most classifiers are built with an attribute that returns important features by simply typing *. Feature_importances _ *. Therefore, we will invoke this very useful attribute through our graph of the function of the importance of the characteristic as such","696ade1c":"**Replace data Convert a Dement**","b0f08e0c":"****We are going to use Binarized LabelEncoder for our Binary attributes********","e5b56858":" <a id=\"ch10\"><\/a>\n## 3.1 Remove Useless Columns","2a278a97":"**Number of patients of each age**","c4cd5bac":" <a id=\"ch16\"><\/a>\n## 4.1 Tuning Hyperparameters for better models\n\nBefore adjusting our models, we will look for the parameters that give us a high AUC","9456403c":"**** 2\u00b0 Extra Tree****","301a68c8":"**Number of Demented, Nondemented and Converted depending on the sex of the patient**","3732483f":" <a id=\"ch12\"><\/a>\n## 3.3 Imputation of lost values\n\nFor various reasons, many real-world data sets contain missing values, often encoded as blanks, NaNs, or other placeholders. However, these data sets are incompatible with scikit-learn estimators that assume that all values \u200b\u200bin a matrix are numeric, and that they all have and have meaning. A basic strategy for using incomplete datasets is to discard rows and \/ or complete columns that contain missing values. However, this has the price of losing data that can be valuable (though incomplete). A better strategy is to impute the lost values, that is, to deduce them from the known part of the data.\n\nThe Imputer class provides basic strategies for imputation of missing values, using either the mean, the median or the most frequent value of the row or column in which the missing values \u200b\u200bare found. This class also allows different encodings of missing values.","e5a71fe7":"<a id=\"ch22\"><\/a>\n## 7.1 Report \n\nfor the Extra Trees model\n","5b2bd9c3":"<a id=\"ch23\"><\/a>\n## 7.1 Results","bda5fd53":"<a id=\"ch20\"><\/a>\n# 6. Predictions","15cf63a9":"<a id=\"ch3\"><\/a>\n## 2.1 read dataset","69066c83":"<a id=\"ch17\"><\/a>\n## 4. 2 Generating our models\n\nSo now let's prepare five learning models as our classification. All these models can be invoked conveniently through the Sklearn library and are listed below:\n\n1. random forest sorter\n2. AdaBoost classifier.\n3. Gradient Boosting classifer\n4. Support vector machine\n5. Extra Trees\n","d559f4de":"<a id=\"ch1\"><\/a>\n# 1. Declaration of functions\n\n## Graphing functions ","3a1fd8b1":"# Table of Contents\n\n* **1. [ Declaration of functions](#ch1)**\n* ** 2 [ Analysis of data](#ch2)**\n     * 2.1 [Read dataset](#ch3) \n     * 2.2 [Correlation Analysis](#ch4) \n\u00a0\u00a0\u00a0\u00a0 * 2.3 [Correlation matrix](#ch5) \n\u00a0\u00a0\u00a0\u00a0 * 2.4 [Dispersion matrix](#ch6) \n     * 2.5 [Graphs of all these correlations](#ch7) \n     * 2.6 [Miscellaneous Graphics](#ch8) \n* ** 3 [Preprocessing](#ch9)**\n\u00a0\u00a0\u00a0\u00a0 * 3.1 [Remove Useless Columns](#ch10)\n\u00a0\u00a0\u00a0\u00a0 * 3.2 [LabelEncoder](#ch11)\n     * 3.3 [Imputation of lost values](#ch12)\n     * 3.4 [Standardization](#ch13)\n     * 3.5 [Export them to then select the features](#ch14)\n* **  4 [Modeling](#ch15)** \n     * 4.1 [Tuning Hyperparameters for better models](#ch15)\n\u00a0\u00a0\u00a0\u00a0\u00a0* 4.2 [Generating our models](#ch16)\n     * 4.3 [Cross Validation](#ch17)\n* **  5. [Importance of characteristics](#ch18)**\n* **  6. [Predictions](#ch19)**\n* ** 7. [Performance Metric for each model](#ch21)**\n    * 7.1 [Report ](#ch22)\n    * 7.2 [Results ](#ch23)","b4c43272":" <a id=\"ch15\"><\/a>\n# 4 Modeling","a87bbc57":" <a id=\"ch13\"><\/a>\n# 3.4 Standardization","684dc4f1":"<a id=\"ch6\"><\/a>\n## 2.4 Dispersion matrix","ab08228e":"<a id=\"ch7\"><\/a>\n## 2.5 Graphs of all these correlations","bda5b693":"**Variation of the dementia according to the MMSE depending on the scores of each patient**","d1f7039e":"## Result:","943f61ff":"<a id=\"ch2\"><\/a>\n # 2. Analysis of data","47004ee3":"<a id=\"ch18\"><\/a>\n## 4.3 Cross Validation","1887e93f":"<a id=\"ch4\"><\/a>\n## 2.2 Correlation Analysis"}}