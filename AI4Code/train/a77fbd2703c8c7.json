{"cell_type":{"b4e3a196":"code","1b2bc2f2":"code","4f595b75":"code","1699e851":"code","ddfefa13":"code","d3639ad8":"code","d64282d0":"code","5e5bcade":"code","43de295f":"code","fb6ac886":"code","4ffae6c4":"code","20f8b890":"code","c68023ad":"code","85382c9d":"code","e0d2b923":"code","42c723c6":"code","10cbb566":"code","fa34962d":"code","e41d2efc":"code","aab6fd8a":"code","b413863b":"code","e18a07e5":"code","fe5f9ac7":"code","147964e0":"code","229a5c76":"code","d15b49b5":"code","9c87af5c":"code","20f61278":"code","f38e2ae3":"code","8646d5e5":"code","f03909a8":"code","314c4c87":"code","56966c78":"code","d02baf91":"code","83deffe7":"code","0aa9f7df":"code","6b3cf005":"code","8e40225b":"code","fcfadf22":"code","2d8f2599":"code","a2b41355":"code","4dcb326f":"code","22817e62":"code","4bffda4c":"code","dc0bae3e":"code","882c082c":"code","44342cf5":"code","04672f37":"code","539b1de6":"code","49ec5d2f":"code","26780f14":"code","e7a4cd0f":"code","911494a7":"code","64466fca":"code","ffa00621":"code","0d610315":"code","4775d8df":"code","28cd74ee":"code","f37205a5":"code","90fbfbf2":"code","610037ed":"code","4681ece3":"code","8c2161e3":"code","4c00c7dd":"code","df438d91":"code","eb11bf0a":"code","01c7cb88":"code","5eea7ddc":"code","c27a5482":"code","5d208346":"code","2953f566":"code","de0f273d":"code","7a8be061":"code","320f7e51":"code","0e624d13":"code","040ca9d8":"code","4d487359":"code","bd481712":"code","7530f210":"markdown","fad63864":"markdown","3be718b1":"markdown","abafb844":"markdown","78c0a7e0":"markdown","d7bad60b":"markdown","3a853873":"markdown","357c29f5":"markdown","2f753039":"markdown","3b446203":"markdown","901c8fd3":"markdown","c2015287":"markdown","50ca91dc":"markdown","6698def3":"markdown","1252ca4c":"markdown","65550211":"markdown","c9efd2b2":"markdown","11266e86":"markdown","0f331be5":"markdown","9c067fa4":"markdown","04be68db":"markdown","ae3476f7":"markdown","bf2e52ac":"markdown","2b737207":"markdown","85b12289":"markdown","c8b7872f":"markdown","7fa4cef9":"markdown","4fe31471":"markdown","510f648d":"markdown","28107568":"markdown","43aa2e09":"markdown","49210bbf":"markdown","3c8a632c":"markdown","e92e0f7d":"markdown"},"source":{"b4e3a196":"# this may need to be installed separately with\n# !pip install category-encoders\nimport category_encoders as ce\n\n# python general\nimport pandas as pd\nimport numpy as np\nfrom collections import OrderedDict\n\n#scikit learn\n\nimport sklearn\nfrom sklearn.base import clone\nfrom sklearn.datasets import load_iris\n\n#feature selection\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.datasets import make_regression\nfrom sklearn.feature_selection import f_regression\nfrom sklearn.decomposition import PCA\nfrom numpy import loadtxt\nfrom numpy import sort\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_selection import SelectFromModel\n\n\n# model selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\n# ML models\nfrom sklearn import tree\nfrom sklearn.linear_model import LinearRegression \nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\n# error metrics\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, median_absolute_error\n\n# plotting and display\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib as mpl\n\nfrom IPython.display import display\npd.options.display.max_columns = None\n\n# widgets and widgets based libraries\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interactive","1b2bc2f2":"def rmse(y_true, y_pred):\n    res = np.sqrt(((y_true - y_pred) ** 2).mean())\n    return res\n\ndef mape(y_true, y_pred):\n    y_val = np.maximum(np.array(y_true), 1e-8)\n    return (np.abs(y_true -y_pred)\/y_val).mean()","4f595b75":"metrics_dict_res = OrderedDict([\n            ('mean_absolute_error', mean_absolute_error),\n            ('median_absolute_error', median_absolute_error),\n            ('root_mean_squared_error', rmse),\n            ('mean abs perc error', mape)\n            ])","1699e851":"def regression_metrics_yin(y_train, y_train_pred, y_test, y_test_pred,\n                           metrics_dict, format_digits=None):\n    df_results = pd.DataFrame()\n    for metric, v in metrics_dict.items():\n        df_results.at[metric, 'train'] = v(y_train, y_train_pred)\n        df_results.at[metric, 'test'] = v(y_test, y_test_pred)\n\n    if format_digits is not None:\n        df_results = df_results.applymap(('{:,.%df}' % format_digits).format)\n\n    return df_results","ddfefa13":"def describe_col(df, col):\n    display(df[col].describe())\n\ndef val_count(df, col):\n    display(df[col].value_counts())\n\ndef show_values(df, col):\n    print(\"Number of unique values:\", len(df[col].unique()))\n    return display(df[col].value_counts(dropna=False))","d3639ad8":"def plot_distribution(df, col, bins=100, figsize=None, xlim=None, font=None, histtype='step'):\n    if font is not None:\n        mpl.rc('font', **font)\n\n    if figsize is not None:\n        plt.figure(figsize=figsize)\n    else:\n        plt.figure(figsize=(10, 6))\n    dev = df[col]    \n    dev.plot(kind='hist', bins=bins, density=True, histtype=histtype, color='b', lw=2,alpha=0.99)\n    print('mean:', dev.mean())\n    print('median:', dev.median())\n    if xlim is not None:\n        plt.xlim(xlim)\n    return plt.gca()","d64282d0":"def plot_feature_importances(model, feature_names=None, n_features=20):\n    if feature_names is None:\n        feature_names = range(n_features)\n    \n    importances = model.feature_importances_\n    importances_rescaled = 100 * (importances \/ importances.max())\n    xlabel = \"Relative importance\"\n\n    sorted_idx = np.argsort(-importances_rescaled)\n\n    names_sorted = [feature_names[k] for k in sorted_idx]\n    importances_sorted = [importances_rescaled[k] for k in sorted_idx]\n\n    pos = np.arange(n_features) + 0.5\n    plt.barh(pos, importances_sorted[:n_features], align='center')\n\n    plt.yticks(pos, names_sorted[:n_features])\n    plt.xlabel(xlabel)\n\n    plt.title(\"Feature importances\")\n\n    return plt.gca()","5e5bcade":"def plot_act_vs_pred(y_act, y_pred, scale=1, act_label='actual', pred_label='predicted', figsize=None, xlim=None,\n                     ylim=None, font=None):\n    \n    if font is not None:\n        mpl.rc('font', **font)\n\n    if figsize is not None:\n        plt.figure(figsize=figsize)\n    else:\n        plt.figure(figsize=(10, 6))\n    plt.scatter(y_act\/scale, y_pred\/scale)\n    x = np.linspace(0, y_act.max()\/scale, 10)\n    plt.plot(x, x)\n    plt.xlabel(act_label)\n    plt.ylabel(pred_label)\n    if xlim is not None:\n        plt.xlim(xlim)\n    else:\n        plt.xlim([0, 1e2])\n    if ylim is not None:\n        plt.ylim(ylim)\n    else:\n        plt.ylim([0, 1e2])\n    return plt.gca()","43de295f":"def compute_perc_deviation(y_act, y_pred, absolute=False):\n    dev = (y_pred - y_act)\/y_act * 100\n    if absolute:\n        dev = np.abs(dev)\n        dev.name = 'abs % error'\n    else:\n        dev.name = '% error'\n    return dev\n\ndef plot_dev_distribution(y_act, y_pred, absolute=False, bins=100, figsize=None, xlim=None, font=None):\n    if font is not None:\n        mpl.rc('font', **font)\n\n    if figsize is not None:\n        plt.figure(figsize=figsize)\n    else:\n        plt.figure(figsize=(10, 6))\n    dev = compute_perc_deviation(y_act, y_pred, absolute=absolute)\n    dev.plot(kind='hist', bins=bins, density=True)\n    print('mean % dev:', dev.mean())\n    print('median % dev:', dev.median())\n    # plt.vlines(dev.mean(), 0, 0.05)\n    plt.title('Distribution of errors')\n    plt.xlabel('% deviation')\n    if xlim is not None:\n        plt.xlim(xlim)\n    else:\n        plt.xlim([-1e2, 1e2])\n    return plt.gca()","fb6ac886":"categorical_features = [\n    'Body_Type',\n    'Driven_Wheels',\n    'Global_Sales_Sub-Segment',\n    'Brand',\n    'Nameplate',\n    'Transmission',\n    'Turbo',\n    'Fuel_Type',\n    'PropSysDesign',\n    'Plugin',\n    'Registration_Type',\n    'country_name'\n]\n\nnumeric_features = [\n    'Generation_Year',\n    'Length',\n    'Height',\n    'Width',\n    'Engine_KW',\n    'No_of_Gears',\n    'Curb_Weight',\n    'CO2',\n    'Fuel_cons_combined',\n    'year'\n]\n\nall_numeric_features = list(numeric_features)\nall_categorical_features = list(categorical_features)\n\ntarget = [\n    'Price_USD'\n]\n\ntarget_name = 'Price_USD'","4ffae6c4":"#ml_model_type = 'Linear Regression'\n#ml_model_type = 'Decision Tree'\n#ml_model_type = 'Random Forest'\nml_model_type = 'XG Boost'\n\nregression_metric = 'mean abs perc error'\n\ndo_grid_search_cv = True\nscoring_greater_is_better = False  # Because the scoring is being done for loss (error) function \n\ndo_retrain_total = True\nwrite_predictions_file = True\n\n# relative size of test set\ntest_size = 0.20\nrandom_state = 33","20f8b890":"df = pd.read_csv('\/kaggle\/input\/ihsmarkit-hackathon-june2020\/train_data.csv',index_col='vehicle_id')\ndf['date'] = pd.to_datetime(df['date'])\n#df['Age'] = df['year'] - df['Generation_Year']\n#len(df[df['Age'] < 0])","c68023ad":"#df[df['Age'] < 0].shape","85382c9d":"# basic commands on a dataframe\n#df.info()\n#df.head(5)\n#df.shape\n# df.head()\ndf.nunique(axis=0)\n# df.tail()","e0d2b923":"df.tail(5)","42c723c6":"df['country_name'].value_counts()\ndf['Nameplate'].value_counts()\ndf['Body_Type'].value_counts()","10cbb566":"df[df['Brand'] == 'volkswagen'].groupby(['Brand', 'Nameplate'])['date'].count()","fa34962d":"df_oos = pd.read_csv('\/kaggle\/input\/ihsmarkit-hackathon-june2020\/oos_data.csv', index_col='vehicle_id')\ndf_oos['date'] = pd.to_datetime(df_oos['date'])\ndf_oos['year'] = df_oos['date'].map(lambda d: d.year)\n#df_oos['Age'] = df_oos['year'] - df_oos['Generation_Year']","e41d2efc":"# df_oos.shape\ndf_oos.head()","aab6fd8a":"df_oos.nunique(axis=0)","b413863b":"df_oos[df_oos['Brand'] == 'volkswagen'].groupby(['Brand', 'Nameplate'])['date'].count().sort_values(ascending=False)","e18a07e5":"df['country_name'].value_counts()\ndf_oos['Brand'].value_counts()\n#df_oos['Body_Type'].value_counts()","fe5f9ac7":"count = df_oos.groupby(['Brand', 'Nameplate'])['date'].count()\ncount.sort_values(ascending=False)","147964e0":"df_oos.groupby(['year', 'country_name'])['date'].count()\nc=df_oos.groupby(['Body_Type', 'No_of_Gears'])['date'].count()\nc.sort_values(ascending=False)","229a5c76":"df_oos.loc[~df_oos[\"Nameplate\"].isin(df[\"Nameplate\"])]['Nameplate'].unique()\ndf_oos.loc[~df_oos[\"Body_Type\"].isin(df[\"Body_Type\"])]['Body_Type'].unique()","d15b49b5":"# unique values, categorical variables\nfor col in all_categorical_features:\n    print(col, len(df[col].unique()))","9c87af5c":"interactive(lambda col: show_values(df, col), col=all_categorical_features)","20f61278":"# summary statistics\ndf[numeric_features + target].describe()","f38e2ae3":"df.sort_values('Price_USD',ascending=False).head(10)","8646d5e5":"# data an outlier with high price as seen from the summary above where mean and median of Price_USD are far apart \n\ndf[df.Brand == 'koenigsegg']\ndf = df[df.Brand != 'koenigsegg']\ndf.shape\n\n#How can car be generated in 2018 when exact price is being captured in 2016. Negative differences between age and year are unexpected so remove noisy\n\ndf = df[(df.year - df.Generation_Year) >= 0 ]\ndf.shape \n\n#There are duplicates!\ndf = df.drop_duplicates()\ndf.shape","f03909a8":"figsize = (16,12)\nsns.set(style='whitegrid', font_scale=2)\n\nbins = 1000\nbins = 40\n#xlim = [0,100000]\nxlim = None\n#price_mask = df['Price_USD'] < 50000000\n#interactive(lambda col: plot_distribution(df[price_mask], col, bins=bins, xlim=xlim), col=sorted(all_numeric_features + target))\ninteractive(lambda col: plot_distribution(df, col, bins=bins, xlim=xlim), col=sorted(all_numeric_features + target))","314c4c87":"# this is quite slow\nsns.set(style='whitegrid', font_scale=1)\nsns.pairplot(df[numeric_features[0:10] + target].iloc[:10000])\n#sns.pairplot(df[['Engine_KW'] + target].iloc[:10000])\n","56966c78":"price_mask = df['Price_USD'] < 100000\ndf_temp = df[price_mask].copy()\nsns.pairplot(df_temp[['Generation_Year'] + target])","d02baf91":"plt.scatter(df['No_of_Gears'],df[target])\nplt.show()","83deffe7":"plt.scatter(df['year'],df[target])\nplt.show()","0aa9f7df":"price_mask = df['Price_USD'] < 100000\ndf_temp = df[price_mask].copy()\n#sns.pairplot(df_temp[['Generation_Year'] + target])\n#plt.scatter(df_temp['Age'],df_temp[target])\n#plt.show()","6b3cf005":"plt.scatter(df['Body_Type'],df['Global_Sales_Sub-Segment'])\nplt.xticks(rotation=90)\nplt.show()\n","8e40225b":"plt.scatter(df['Body_Type'],df[target])\nplt.xticks(rotation=90)\nplt.show()","fcfadf22":"plt.scatter(df['PropSysDesign'],df[target])\nplt.xticks(rotation=90)\nplt.show()","2d8f2599":"plt.scatter(df['Transmission'],df[target])\nplt.xticks(rotation=90)\nplt.show()","a2b41355":"\n#df_temp = df[price_mask].copy()\n#plt.scatter(df_temp['Generation_Year'],df_temp['Nameplate'])\n#plt.xticks(rotation=90)\n#plt.show()\n#'Nameplate','Body_Type'","4dcb326f":"corrmat = df.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","22817e62":"#additional_numeric_features = ['Age']\nadditional_numeric_features = []","4bffda4c":"fs = SelectKBest(score_func=f_regression, k=6)\n# apply feature selection\nX_selected = fs.fit_transform(df[numeric_features], df[target_name])\nprint(X_selected.shape)","dc0bae3e":"print(X_selected)","882c082c":"print(df[numeric_features].head())","44342cf5":"features_drop = []\n\nif ml_model_type == 'Linear Regression':\n    features_drop = categorical_features + numeric_features\n    #features_to_use = ['Engine_KW']\n    features_to_use = ['country_name', 'Engine_KW']\n    for feature in features_to_use:\n        features_drop.remove(feature)\nelse:\n    features_drop = categorical_features + numeric_features\n    features_to_use = ['Generation_Year','Turbo','Global_Sales_Sub-Segment','Brand', 'country_name', 'Length','Height','Width','Curb_Weight','Engine_KW','Body_Type','Fuel_cons_combined','No_of_Gears','Transmission','PropSysDesign']\n    #features_to_use = ['Generation_Year','Length','Height','Width','Engine_KW','No_of_Gears','Curb_Weight','Fuel_cons_combined','year']\n    for feature in features_to_use:\n        features_drop.remove(feature)\n    # features_drop = ['Nameplate']\n    \n\ncategorical_features = list(filter(lambda f: f not in features_drop, categorical_features))\nnumeric_features = list(filter(lambda f: f not in features_drop, numeric_features))","04672f37":"numeric_features","539b1de6":"categorical_features","49ec5d2f":"features = categorical_features + numeric_features + additional_numeric_features\nmodel_columns = features + [target_name]\nmodel_columns","26780f14":"#dataframe for further processing\ndf_proc = df[model_columns].copy()\ndf_proc.shape","e7a4cd0f":"# One-hot encoding\nencoder = ce.OneHotEncoder(cols=categorical_features, handle_unknown='value', \n                           use_cat_names=True)\nencoder.fit(df_proc)\ndf_comb_ext = encoder.transform(df_proc)\nfeatures_ext = list(df_comb_ext.columns)\nfeatures_ext.remove(target_name)","911494a7":"#del df_proc\ndf_comb_ext.head()","64466fca":"#df_comb_ext.memory_usage(deep=True).sum()\/1e9\n#features_model\ndf_comb_ext.shape","ffa00621":"X_train, X_test, y_train, y_test = train_test_split(df_comb_ext[features_ext], df_comb_ext[target_name], \n                                                    test_size=test_size, random_state=random_state)\n\nprint(X_train.shape)\nprint(X_test.shape)","0d610315":"#xgb cv for model params\n#import xgboost as xgb\n#from sklearn.metrics import mean_absolute_error\n#fixed_params = {'max_depth': 10,\n#                     'min_child_weight': 1, 'learning_rate' : 0.3}\n#dtrain = xgb.DMatrix(X_train, label=y_train)\n#dtest = xgb.DMatrix(X_test, label=y_test)\n\n#params = {'max_depth': 10,'min_child_weight': 1, 'learning_rate' : 0.2, 'colsample_bytree': 0.8, 'subsample': 1}\n#params['eval_metric'] = \"mae\"\n\n#cvresult = xgb.train(params, dtrain, num_boost_round=999,early_stopping_rounds=10,evals=[(dtest, \"Test\")])\n#cvresult\n","4775d8df":"#print(\"Best MAE: {:.2f} with {} rounds\".format(\n#                 cvresult.best_score,\n#                 cvresult.best_iteration+1))","28cd74ee":"#cv_results['test-mae-mean'].min()","f37205a5":"if ml_model_type == 'Linear Regression':\n    model_hyper_parameters_dict = OrderedDict(fit_intercept=True, normalize=False)\n    regressor =  LinearRegression(**model_hyper_parameters_dict)\n\nif ml_model_type == 'Decision Tree':\n    model_hyper_parameters_dict = OrderedDict(max_depth=3, random_state=random_state)\n    regressor =  DecisionTreeRegressor(**model_hyper_parameters_dict)\n      \nif ml_model_type == 'Random Forest':\n\n    model_hyper_parameters_dict = OrderedDict(n_estimators=30, \n                                              max_depth=10, \n                                              min_samples_split=2, \n                                              max_features='sqrt',\n                                              min_samples_leaf=5, \n                                              random_state=random_state, \n                                              n_jobs=2)\n   \n    regressor = RandomForestRegressor(**model_hyper_parameters_dict)\n        \nif ml_model_type == 'XG Boost':\n    \n    model_hyper_parameters_dict = OrderedDict(n_estimators=500,\n                                              max_depth=10,\n                                              learning_rate=0.3, min_child_weight=1)\n    \n    regressor = XGBRegressor(**model_hyper_parameters_dict)\n    \nbase_regressor = clone(regressor)\n     \n     \nif do_grid_search_cv:\n    \n    scoring = make_scorer(metrics_dict_res[regression_metric], greater_is_better=scoring_greater_is_better)\n    \n    if ml_model_type == 'Random Forest':\n        \n\n        grid_parameters = [{'n_estimators': [10,30,100,1000], 'max_depth': [5, 10], \n                             'min_samples_split': [1,2], 'min_samples_leaf': [1,5],\n                              'max_features' : ['sqrt','auto']}]\n\n    if ml_model_type == 'XG Boost':  \n        \n        grid_parameters = [{\"n_estimators\" : [500],'max_depth': [10],\n                             'min_child_weight': [1], 'learning_rate' : [0.3]}]\n   \n        \n    n_splits = 5\n    n_jobs = 4\n    cv_regressor = GridSearchCV(regressor, grid_parameters, cv=n_splits, scoring=scoring, return_train_score=True,\n                                refit=True, n_jobs=n_jobs)    \n        ","90fbfbf2":"if do_grid_search_cv:\n    cv_regressor.fit(X_train, y_train)\n    regressor_best = cv_regressor.best_estimator_\n    model_hyper_parameters_dict = cv_regressor.best_params_\n    train_scores = cv_regressor.cv_results_['mean_train_score']\n    test_scores = cv_regressor.cv_results_['mean_test_score']\n    test_scores_std = cv_regressor.cv_results_['std_test_score']\n    cv_results = cv_regressor.cv_results_\nelse:\n    regressor.fit(X_train, y_train)","610037ed":"if do_grid_search_cv:\n    # print(cv_results)\n    print(model_hyper_parameters_dict)\n    plt.plot(-train_scores, label='train')\n    plt.plot(-test_scores, label='test')\n    plt.xlabel('Parameter set #')\n    plt.legend()\n    regressor = regressor_best","4681ece3":"#params = { \"n_estimators\" : 100, 'max_depth': 10,\n#                             'min_child_weight': 1, 'learning_rate' : 0.3, \n#                             'colsample_bytree' : 0.4,\n#                             'alpha' : 5}\n\n#cv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,\n #                   num_boost_round=50,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True, seed=123)\n\n#print((cv_results[\"test-rmse-mean\"]).tail(1))","8c2161e3":"y_train_pred = regressor.predict(X_train)\ny_test_pred = regressor.predict(X_test)","4c00c7dd":"if ml_model_type == 'Linear Regression':\n    df_reg_coef = (pd.DataFrame(zip(['intercept'] + list(X_train.columns), \n                               [regressor.intercept_] + list(regressor.coef_)))\n                 .rename({0: 'feature', 1: 'coefficient value'}, axis=1))\n    display(df_reg_coef)","df438d91":"if hasattr(regressor, 'feature_importances_'):\n    sns.set(style='whitegrid', font_scale=1.5)\n    plt.figure(figsize=(12,10))\n    plot_feature_importances(regressor, features_ext, n_features=np.minimum(20, X_train.shape[1]))","eb11bf0a":"df_regression_metrics = regression_metrics_yin(y_train, y_train_pred, y_test, y_test_pred,\n                                               metrics_dict_res, format_digits=3)\n\ndf_output = df_regression_metrics.copy()\ndf_output.loc['Counts','train'] = len(y_train)\ndf_output.loc['Counts','test'] = len(y_test)\ndf_output","01c7cb88":"figsize = (16,10)\nxlim = [0, 250]\nfont={'size': 20}\nsns.set(style='whitegrid', font_scale=2.5)\nact_label = 'actual price [k$]'\npred_label='predicted price [k$]'\nplot_act_vs_pred(y_test, y_test_pred, scale=1000, act_label=act_label, pred_label=pred_label, \n                 figsize=figsize, xlim=xlim, ylim=xlim, font=font)\nprint()","5eea7ddc":"figsize = (14,8)\nxlim = [0, 100]\n#xlim = [-100, 100]\n# xlim = [-50, 50]\n#xlim = [-20, 20]\n\nfont={'size': 20}\nsns.set(style='whitegrid', font_scale=1.5)\n\np_error = (y_test_pred - y_test)\/y_test *100\ndf_p_error = pd.DataFrame(p_error.values, columns=['percent_error'])\n#display(df_p_error['percent_error'].describe().to_frame())\n\nbins=1000\nbins=500\n#bins=100\nabsolute = True\n#absolute = False\nplot_dev_distribution(y_test, y_test_pred, absolute=absolute, figsize=figsize, \n                      xlim=xlim, bins=bins, font=font)\nprint()","c27a5482":"if do_retrain_total:\n    cv_opt_model = clone(base_regressor.set_params(**model_hyper_parameters_dict))\n    # train on complete data set\n    X_train_full = df_comb_ext[features_ext].copy()\n    y_train_full = df_comb_ext[target_name].values\n    cv_opt_model.fit(X_train_full, y_train_full) \n    regressor = cv_opt_model","5d208346":"df_oos.head()","2953f566":"df_proc_oos = df_oos[model_columns[:-1]].copy()\ndf_proc_oos[target_name] = 1","de0f273d":"df_comb_ext_oos = encoder.transform(df_proc_oos)","7a8be061":"df_comb_ext_oos.drop(target_name, axis=1, inplace=True)","320f7e51":"y_oos_pred = regressor.predict(df_comb_ext_oos)","0e624d13":"id_col = 'vehicle_id'\ndf_out = (pd.DataFrame(y_oos_pred, columns=[target_name], index=df_comb_ext_oos.index)\n            .reset_index()\n            .rename({'index': id_col}, axis=1))","040ca9d8":"df_out.head()","4d487359":"df_out.shape","bd481712":"if write_predictions_file:\n    df_out.to_csv('submission.csv', index=False)","7530f210":"## Categorical features","fad63864":"## Metrics","3be718b1":"## Define features","abafb844":"# Machine learning model\n\nSupervised learning\n\nhttps:\/\/scikit-learn.org\/stable\/supervised_learning.html\n\nEnsemble methods in scikit learn\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/ensemble.html\n\n\nDecision trees\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/tree.html\n","78c0a7e0":"#  ML data preparation","d7bad60b":"## Metrics","3a853873":"## Categorical feature encoding","357c29f5":"# Load  data\n","2f753039":"## Optionally retrain on the whole data set","3b446203":"## Training data","901c8fd3":"## Drop features (optional)","c2015287":"## Apply categorical encoding","50ca91dc":"##  Model definition","6698def3":"## Out of sample data (to predict)","1252ca4c":"## Display functions","65550211":"##  Model Performance plots","c9efd2b2":"## Apply model and produce output","11266e86":"# Model evaluation","0f331be5":"# Feature exploration","9c067fa4":"## Numerical features","04be68db":"## Regression coefficients\/Feature importance","ae3476f7":"# Feature generation","bf2e52ac":"# Global options","2b737207":"# Feature selection\n\nYou can read about feature selection here\nhttps:\/\/scikit-learn.org\/stable\/modules\/feature_selection.html#","85b12289":"# Locally defined functions","c8b7872f":"## Train, test predictions","7fa4cef9":"## Subset to relevant columns","4fe31471":"* Sup thoughts : Length and Width are quite correlated , can choose to drop width if using regression . Curb weight?\n* CO2 & Fuel_cons_combined coorelated ofcourse former is just city.","510f648d":"# Apply model to OOS data","28107568":"## ML model training","43aa2e09":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-libraries\" data-toc-modified-id=\"Import-libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Import libraries<\/a><\/span><\/li><li><span><a href=\"#Locally-defined-functions\" data-toc-modified-id=\"Locally-defined-functions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Locally defined functions<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Metrics\" data-toc-modified-id=\"Metrics-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;<\/span>Metrics<\/a><\/span><\/li><li><span><a href=\"#Display-functions\" data-toc-modified-id=\"Display-functions-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;<\/span>Display functions<\/a><\/span><\/li><li><span><a href=\"#Define-features\" data-toc-modified-id=\"Define-features-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;<\/span>Define features<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Global-options\" data-toc-modified-id=\"Global-options-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Global options<\/a><\/span><\/li><li><span><a href=\"#Load--data\" data-toc-modified-id=\"Load--data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Load  data<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Training-data\" data-toc-modified-id=\"Training-data-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;<\/span>Training data<\/a><\/span><\/li><li><span><a href=\"#Out-of-sample-data-(to-predict)\" data-toc-modified-id=\"Out-of-sample-data-(to-predict)-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;<\/span>Out of sample data (to predict)<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Feature-exploration\" data-toc-modified-id=\"Feature-exploration-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Feature exploration<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Categorical-features\" data-toc-modified-id=\"Categorical-features-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>Categorical features<\/a><\/span><\/li><li><span><a href=\"#Numerical-features\" data-toc-modified-id=\"Numerical-features-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;<\/span>Numerical features<\/a><\/span><\/li><li><span><a href=\"#Pair-plot\" data-toc-modified-id=\"Pair-plot-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;<\/span>Pair plot<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Feature-generation\" data-toc-modified-id=\"Feature-generation-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Feature generation<\/a><\/span><\/li><li><span><a href=\"#Feature-selection\" data-toc-modified-id=\"Feature-selection-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Feature selection<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Drop-features-(optional)\" data-toc-modified-id=\"Drop-features-(optional)-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;<\/span>Drop features (optional)<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#ML-data-preparation\" data-toc-modified-id=\"ML-data-preparation-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;<\/span>ML data preparation<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Categorical-feature-encoding\" data-toc-modified-id=\"Categorical-feature-encoding-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;<\/span>Categorical feature encoding<\/a><\/span><\/li><li><span><a href=\"#Train-test-split\" data-toc-modified-id=\"Train-test-split-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;<\/span>Train test split<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Machine-learning-model\" data-toc-modified-id=\"Machine-learning-model-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;<\/span>Machine learning model<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Model-definition\" data-toc-modified-id=\"Model-definition-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;<\/span>Model definition<\/a><\/span><\/li><li><span><a href=\"#ML-model-training\" data-toc-modified-id=\"ML-model-training-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;<\/span>ML model training<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Model-evaluation\" data-toc-modified-id=\"Model-evaluation-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;<\/span>Model evaluation<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Train,-test-predictions\" data-toc-modified-id=\"Train,-test-predictions-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;<\/span>Train, test predictions<\/a><\/span><\/li><li><span><a href=\"#Regression-coefficients\/Feature-importance\" data-toc-modified-id=\"Regression-coefficients\/Feature-importance-10.2\"><span class=\"toc-item-num\">10.2&nbsp;&nbsp;<\/span>Regression coefficients\/Feature importance<\/a><\/span><\/li><li><span><a href=\"#Metrics\" data-toc-modified-id=\"Metrics-10.3\"><span class=\"toc-item-num\">10.3&nbsp;&nbsp;<\/span>Metrics<\/a><\/span><\/li><li><span><a href=\"#Model-Performance-plots\" data-toc-modified-id=\"Model-Performance-plots-10.4\"><span class=\"toc-item-num\">10.4&nbsp;&nbsp;<\/span>Model Performance plots<\/a><\/span><\/li><li><span><a href=\"#Optionally-retrain-on-the-whole-data-set\" data-toc-modified-id=\"Optionally-retrain-on-the-whole-data-set-10.5\"><span class=\"toc-item-num\">10.5&nbsp;&nbsp;<\/span>Optionally retrain on the whole data set<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Apply-model-to-OOS-data\" data-toc-modified-id=\"Apply-model-to-OOS-data-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;<\/span>Apply model to OOS data<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Subset-to-relevant-columns\" data-toc-modified-id=\"Subset-to-relevant-columns-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;<\/span>Subset to relevant columns<\/a><\/span><\/li><li><span><a href=\"#Apply-categorical-encoding\" data-toc-modified-id=\"Apply-categorical-encoding-11.2\"><span class=\"toc-item-num\">11.2&nbsp;&nbsp;<\/span>Apply categorical encoding<\/a><\/span><\/li><li><span><a href=\"#Apply-model-and-produce-output\" data-toc-modified-id=\"Apply-model-and-produce-output-11.3\"><span class=\"toc-item-num\">11.3&nbsp;&nbsp;<\/span>Apply model and produce output<\/a><\/span><\/li><\/ul><\/li><\/ul><\/div>","49210bbf":"## Pair plot","3c8a632c":"# Import libraries","e92e0f7d":"## Train test split"}}