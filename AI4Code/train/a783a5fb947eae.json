{"cell_type":{"14308667":"code","46d5cc07":"code","01c7c9c6":"code","226f4e23":"code","016f703f":"code","85c79e1a":"code","15e85696":"code","bff9f493":"code","82128824":"code","da91329e":"code","cba77ac1":"code","336b6ac8":"code","6cae1f2e":"code","f9f2f391":"code","b1b6e981":"code","69931c85":"code","5d9855d6":"code","10910e94":"code","5d4b6d5e":"code","0fe83de6":"code","eb0f41ae":"code","560a89da":"code","1d41f8e1":"code","e9aa3d12":"code","c8787707":"code","1cf2ae02":"code","af7664b9":"code","b2dea985":"code","d6ccfac2":"code","f2f64058":"code","a80c4211":"code","09649c18":"code","1b0fcdb5":"code","7fe7b7b5":"code","2bbab5fc":"code","fb692134":"code","994ce184":"code","6b3fdd2d":"code","1ee8f7b9":"code","2462ad7b":"markdown","4e8b56ed":"markdown","a0341932":"markdown","118e1fd9":"markdown","28643b84":"markdown","a7eadbc2":"markdown","2a1690cf":"markdown","9fa947a7":"markdown","3e838f29":"markdown"},"source":{"14308667":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","46d5cc07":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import SCORERS\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\n\n# For Categorical Data Encoding\nfrom numpy import asarray\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n\nimport sys","01c7c9c6":"# Load train data\ntrain = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntrain.shape","226f4e23":"# Load test data\ntest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntest.shape","016f703f":"# Get target dataframe\ntarget = 'SalePrice'\ntrain_y = train[[target]]\nprint(\"y data shape: \", train_y.shape)\ntrain_y.head()","85c79e1a":"# Check datatypes\ntrain_y.dtypes","15e85696":"# Change y shape\ntrain_y_array = train_y.values.ravel()\nprint(\"y_array data shape: \", train_y_array.shape)","bff9f493":"# Get train_X dataframe\ntrain_X = train.drop([target, 'Id'], axis=1)\ntrain_X.head()","82128824":"# Check datatypes\ntrain_X.dtypes","da91329e":"# Get test_X dataframe\ntest_X = test.drop(['Id'], axis=1)\ntest_X.head()","cba77ac1":"# Check datatypes\ntest_X.dtypes","336b6ac8":"# Convert numerical categories to object type\nnum_cat_cols = ('MSSubClass', 'OverallQual', 'OverallCond')\n\nfor column in num_cat_cols:\n    train_X = train_X.astype({column: 'object'})\n    test_X = test_X.astype({column: 'object'})","6cae1f2e":"# Check datatypes\ntrain_X.dtypes","f9f2f391":"# Check datatypes\ntest_X.dtypes","b1b6e981":"train_X_numeric = train_X.loc[: , train_X.dtypes!=\"object\"]\ntrain_X_numeric.head()","69931c85":"test_X_numeric = test_X.loc[: , test_X.dtypes!=\"object\"]\ntest_X_numeric.head()","5d9855d6":"# Check missing values\nprint(\"Missing values in train_X_numeric:\",train_X_numeric.isnull().sum().sum())\nprint(\"Missing values in test_X_numeric:\",test_X_numeric.isnull().sum().sum())","10910e94":"# Missing values imputation\nfor column in train_X_numeric.columns:\n    train_X_numeric[column].fillna(train_X_numeric[column].median(), inplace = True)\n    \nfor column in test_X_numeric.columns:\n    test_X_numeric[column].fillna(test_X_numeric[column].median(), inplace = True)","5d4b6d5e":"# Check missing values\nprint(\"Missing values in train_X_numeric:\",train_X_numeric.isnull().sum().sum())\nprint(\"Missing values in test_X_numeric:\",test_X_numeric.isnull().sum().sum())","0fe83de6":"train_X_categorical = train_X.loc[: , train_X.dtypes==\"object\"]\ntrain_X_categorical.head()","eb0f41ae":"test_X_categorical = test_X.loc[: , test_X.dtypes==\"object\"]\ntest_X_categorical.head()","560a89da":"# Check missing values\nprint(\"Missing values in train_X_categorical:\",train_X_categorical.isnull().sum().sum())\nprint(\"Missing values in test_X_categorical:\",test_X_categorical.isnull().sum().sum())","1d41f8e1":"# Missing values imputation\nfor column in train_X_categorical.columns:\n    train_X_categorical[column].fillna(\"NONE\", inplace = True)\n    \nfor column in test_X_categorical.columns:\n    test_X_categorical[column].fillna(\"NONE\", inplace = True)","e9aa3d12":"# Check missing values\nprint(\"Missing values in train_X_categorical:\",train_X_categorical.isnull().sum().sum())\nprint(\"Missing values in test_X_categorical:\",test_X_categorical.isnull().sum().sum())","c8787707":"# Define ordinal encoding\nordinal = OrdinalEncoder()\n#\nX_categorical = pd.concat([train_X_categorical,test_X_categorical])\n# Fit encoder\nordinal.fit(X_categorical)\n# Transform data\n#X_cat_ordinal = encoder.fit_transform(X_categorical)\ntrain_X_cat_ordinal = ordinal.transform(train_X_categorical)\ntest_X_cat_ordinal = ordinal.transform(test_X_categorical)\nprint(\"train X_categorical data shape: \", train_X_cat_ordinal.shape)\nprint(\"test X_categorical data shape: \", test_X_cat_ordinal.shape)","1cf2ae02":"print(\"train X_numeric data shape: \",train_X_numeric.shape)\nprint(\"test X_numeric data shape: \",test_X_numeric.shape)","af7664b9":"# combine numeric with categorical data\ntrain_X_array = np.concatenate((train_X_numeric.values, train_X_cat_ordinal), axis=1)\ntest_X_array = np.concatenate((test_X_numeric.values, test_X_cat_ordinal), axis=1)\nprint(\"train X_array data shape: \",train_X_array.shape)\nprint(\"test X_array data shape: \",test_X_array.shape)","b2dea985":"# Define scaler\nscaler = MinMaxScaler()\n# Combine train and test data and fit scaler\ndata = np.concatenate((train_X_array, test_X_array), axis=0)\nscaler.fit(data)\n# Scale train and test data\ntrain_X_array = scaler.transform(train_X_array)\ntest_X_array = scaler.transform(test_X_array)\nprint(\"train X_array data shape: \",train_X_array.shape)\nprint(\"test X_array data shape: \",test_X_array.shape)","d6ccfac2":"# Create a GB Regressor\nGBregressor = GradientBoostingRegressor(random_state=0, verbose=0)\n\n# Calculate CV score\ncv_score = cross_val_score(GBregressor, train_X_array, train_y_array, cv = 5, scoring='neg_mean_squared_log_error', n_jobs=-1, verbose=1)\nnp.sqrt(-cv_score.mean())","f2f64058":"# Create a GB Regressor\nGBregressor = GradientBoostingRegressor(max_depth=None,\n                                        learning_rate=0.05,                                        \n                                        max_features=0.3,\n                                        max_leaf_nodes=8,\n                                        min_samples_leaf=1,\n                                        min_samples_split=2,\n                                        n_estimators=1500,\n                                        random_state=1234,\n                                        subsample=1.0,\n                                        verbose=0\n                                       )\n\n# Calculate CV score\ncv_score = cross_val_score(GBregressor, train_X_array, train_y_array, cv = 5, scoring='neg_mean_squared_log_error', n_jobs=-1, verbose=1)\nnp.sqrt(-cv_score.mean())","a80c4211":"import optuna","09649c18":"def objective(trial):\n   \n    max_d = trial.suggest_int('max_depth', 2, 15)\n    lr = trial.suggest_float('learning_rate', 0.01, 0.5, step=0.01)\n    max_f = trial.suggest_float('max_features', 0.01, 1.0, step=0.01)\n    max_l_n = trial.suggest_int('max_leaf_nodes', 5, 100)\n    min_s_split = trial.suggest_int('min_samples_split', 2, 100)\n    min_s_leaf = trial.suggest_int('min_samples_leaf', 1, (int(min_s_split\/2)+1))\n    n_estim = trial.suggest_int('n_estimators', 1000, 10000)\n    random_st = trial.suggest_int('random_state', 0, 10000)\n    subsam = trial.suggest_float('subsample', 0.1, 1.0, step=0.01)\n    \n    GBregressor = GradientBoostingRegressor(max_depth=max_d,\n                                            learning_rate=lr,                                        \n                                            max_features=max_f,\n                                            max_leaf_nodes=max_l_n,\n                                            min_samples_split=min_s_split,\n                                            min_samples_leaf=min_s_leaf,\n                                            n_estimators=n_estim,\n                                            random_state=random_st,\n                                            subsample=subsam,\n                                            verbose=0\n                                           )\n    \n    try:\n        cv_score = cross_val_score(GBregressor, \n                                   train_X_array, \n                                   train_y_array, \n                                   cv = 5, \n                                   scoring='neg_mean_squared_log_error', \n                                   n_jobs=-1, \n                                   verbose=0)\n    except:\n        print(\"Error during Optuna trial\")\n        cv_score = np.array(-9801)\n\n    return np.sqrt(-cv_score.mean())","1b0fcdb5":"# Create Optuna Trial\n#study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=256))\nstudy = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.RandomSampler(seed=512))\n\n# Run 200 trials\nstudy.optimize(objective , n_trials = 500)","7fe7b7b5":"# Best study\nprint(study.best_trial.params)\nprint(study.best_trial.value)","2bbab5fc":"study.best_trial.params","fb692134":"# Create a GB Regressor\nGBregressor = GradientBoostingRegressor(max_depth=study.best_trial.params['max_depth'],\n                                        learning_rate=study.best_trial.params['learning_rate'],                                        \n                                        max_features=study.best_trial.params['max_features'],\n                                        max_leaf_nodes=study.best_trial.params['max_leaf_nodes'],\n                                        min_samples_leaf=study.best_trial.params['min_samples_leaf'],\n                                        min_samples_split=study.best_trial.params['min_samples_split'],\n                                        n_estimators=study.best_trial.params['n_estimators'],\n                                        random_state=study.best_trial.params['random_state'],\n                                        subsample=study.best_trial.params['subsample'],\n                                        verbose=0\n                                       )\n\n# Calculate CV score\ncv_score = cross_val_score(GBregressor, train_X_array, train_y_array, cv = 5, scoring='neg_mean_squared_log_error', n_jobs=-1, verbose=1)\nnp.sqrt(-cv_score.mean())","994ce184":"GBregressor.fit(train_X_array, train_y_array)\ntest_y_array = GBregressor.predict(test_X_array)","6b3fdd2d":"predictions = pd.DataFrame()\npredictions['Id'] = test['Id']\npredictions[target] = test_y_array.round()\npredictions","1ee8f7b9":"# Create submission file\npredictions.to_csv('submission.csv', index=False)\n#Score: ","2462ad7b":"### Fill missing values with \"NONE\"","4e8b56ed":"### Ordinal Encoding of Categorical Variables","a0341932":"# Gradient Boosting Regressor Building and Scoring\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingRegressor.html","118e1fd9":"## Features Normalization (Min-Max Scaler)","28643b84":"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_val_score.html\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html","a7eadbc2":"## Train model and make predictions","2a1690cf":"# Optuna Optimization for Gradient Boosting Regressor","9fa947a7":"## Handle Categorical Variables","3e838f29":"## Handle Numeric Variables"}}