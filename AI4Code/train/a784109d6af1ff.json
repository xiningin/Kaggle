{"cell_type":{"8ec4abe9":"code","61fe068e":"code","8c3c3e3d":"code","77caaeb8":"code","3824d0f6":"code","eae1139b":"code","745e6d22":"code","42ec6377":"code","e1cc22ae":"code","9df449f2":"code","fa5dc3d2":"code","01db8127":"code","a8b32703":"code","dae204c5":"code","13796b28":"code","87068897":"code","a8edf96d":"code","657fd70f":"code","1d5d4398":"code","5159cdb9":"code","979b65a3":"code","b0feeb5c":"code","e3c93a50":"code","44c1d02c":"code","8c9bd021":"code","bae675d8":"code","6a8a292b":"code","f6e0431f":"markdown","91bbc33c":"markdown"},"source":{"8ec4abe9":"%reload_ext tensorboard","61fe068e":"import os\nimport re\nimport pickle\nimport numpy as np\nimport random as rn\nimport pandas as pd\nfrom tqdm import tqdm\nimport nltk\nfrom nltk import ne_chunk, pos_tag, word_tokenize\nfrom nltk.tree import Tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, auc, roc_curve\nfrom sklearn.preprocessing import LabelEncoder\nimport tensorflow as tf\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Input, Embedding, Conv1D, Concatenate, MaxPool1D, Flatten, Dropout, Dense, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, TerminateOnNaN, Callback\nfrom tensorflow.keras.optimizers import RMSprop, SGD, Adam\nfrom tensorflow.keras.utils import to_categorical","8c3c3e3d":"if not os.path.isfile(\"text_label.csv\"):\n    files = os.listdir(\".\/documents\")\n    label = []\n    text = []\n    num = []\n    \n    for file in tqdm(files):\n        f = open(\".\/documents\"+\"\/\"+file, \"r\")\n        text.append(f.read())\n        f.close()\n        label.append(file.split(\"_\")[0])\n        num.append(file.split(\"_\")[1].split(\".\")[0])\n        \n    d = {\"Text\": text, \"D_Number\": num, \"Label\": label}\n    data = pd.DataFrame(d)    \n    data.to_csv(\"text_label.csv\", index = False, header = True)\n    \nelse:\n    data = pd.read_csv(\"text_label.csv\", index_col = False)","77caaeb8":"data.head()","3824d0f6":"print(\"Number of classes: \", len(data.Label.value_counts()))\nprint(\"Number of documents of each class\\n\", data.Label.value_counts())","eae1139b":"def Preprocess(text_data):\n    \"\"\"\"\n    Function to preprocess text file, generate preprocessed email domain, subjects, and text\n    \"\"\"\n    \n    preprocessed_email = []\n    preprocessed_subject = []\n    preprocessed_text = []\n    \n    for sentence in tqdm(text_data):\n        \n        #preprocessing email\n        domain = re.findall(\"@[\\w.]+\", sentence)\n        email = \"\"\n        for items in domain:\n            items = items.replace(\"@\", \"\")    \n            items = items.split(\".\")\n            for i in set(items):\n                if((len(i) > 2) and i != \"com\" and i != \"COM\"):\n                    email += i + \" \"\n        preprocessed_email.append(email.strip())\n        \n        #preprocessing subject    \n        text_split = sentence.split(\"\\n\")\n        for item in text_split:\n            if(item.startswith(\"Subject:\")):\n                subject = \"\"\n                for word in item.split():\n                    if not word.endswith(\":\"):\n                        subject += word + \" \"\n                subject = re.sub(\"[^0-9a-zA-Z\\s]\", \" \", subject)\n                subject = \" \".join(subject.split()).strip()\n        preprocessed_subject.append(subject.lower())\n        \n        #preprocessing text\n        #https:\/\/towardsdatascience.com\/how-i-preprocessed-text-data-using-regular-expressions-for-my-text-classification-task-cnn-cb206e7274ed\n\n        text = re.sub(r\"(.*)Subject:(.*?)(.*)\\n\", \" \", sentence)   #remove subject line\n        text = re.sub(r\"(.*)From:(.*?)(.*)\\n\", \" \", text)          #remove from line\n        text = re.sub(r\"(.*)Write to:(.*?)(.*)\\n\", \" \", text)      #remove write to line\n        text = re.sub(r\"(.*):(.*?)\", \" \", text)                    #remove words ending with :\n\n        #decontract\n        text = re.sub(r\"won't\", \"will not\", text)\n        text = re.sub(r\"can\\'t\", \"can not\", text)\n        text = re.sub(r\"n\\'t\", \" not\", text)\n        text = re.sub(r\"\\'re\", \" are\", text)\n        text = re.sub(r\"\\'s\", \" is\", text)\n        text = re.sub(r\"\\'d\", \" would\", text)\n        text = re.sub(r\"\\'ll\", \" will\", text)\n        text = re.sub(r\"\\'t\", \" not\", text)\n        text = re.sub(r\"\\'ve\", \" have\", text)\n        text = re.sub(r\"\\'m\", \" am\", text)\n\n        text = re.sub(r\"[\\w\\-\\.]+@[\\w\\.-]+\\b\", \" \", text)          #remove all emails\n        text = re.sub(r\"[\\n\\t]\",\" \", text)                         #remove line feeds and tabs\n        text = re.sub(r\"<.*>\", \" \", text)                          #remove text within angular brackets\n        text = re.sub(r\"\\(.*\\)\", \" \", text)                        #remove text within parantheses\n        text = re.sub(r\"\\[.*\\]\", \" \", text)                        #remove text within square brackets\n        text = re.sub(r\"\\{.*\\}\", \" \", text)                        #remove text within curly braces\n        text = re.sub(\"[0-9]+\", \" \", text)                         #remove all digts\n        text = re.sub(\"[^A-Za-z\\s]\", \" \", text)                    #remove all characters except alphabets and spaces\n\n        #https:\/\/towardsdatascience.com\/how-i-preprocessed-text-data-using-regular-expressions-for-my-text-classification-task-cnn-cb206e7274ed\n        chunks = []\n        chunks = (list(ne_chunk(pos_tag(word_tokenize(text)))))\n\n        for i in chunks:\n            if(type(i) == Tree):\n                if i.label() == \"GPE\":\n                    j = i.leaves()\n                    if len(j)>1:   #if a city name has two or more words we combine it with underscore\n                        gpe = \"_\".join([term for term, pos in j])\n                        text = re.sub(rf\"{j[1][0]}\", gpe, text)\n                        text = re.sub(rf\"{j[0][0]}\", \" \", text)\n                if i.label() == \"PERSON\":\n                    for term, pos in i.leaves():\n                        text = re.sub(re.escape(term), \"\", text)\n                \n        #https:\/\/stackoverflow.com\/questions\/20802056\/python-regular-expression-1\n        text = re.sub(r\"\\b_([a-zA-z]+)_\\b\", r\"\\1\", text) #replace _word_ to word\n        text = re.sub(r\"\\b_([a-zA-z]+)\\b\", r\"\\1\", text) #replace_word to word\n        text = re.sub(r\"\\b([a-zA-z]+)_\\b\", r\"\\1\", text) #replace word_ to word\n\n        text = re.sub(r\"\\b[a-zA-Z]{1}_([a-zA-Z]+)\", r\"\\1\", text) \n        text = re.sub(r\"\\b[a-zA-Z]{2}_([a-zA-Z]+)\", r\"\\1\", text)\n\n        text = text.lower()\n        text_split = text.split()\n        text = \"\"\n        for words in text_split:\n            if((len(words) > 2) and (len(words) < 15)):\n                text += words + \" \"\n        preprocessed_text.append(text.strip())\n    \n    return (preprocessed_email, preprocessed_subject, preprocessed_text)","745e6d22":"if not os.path.isfile(\"preprocessed_final.csv\"):\n    processed_email, processed_subject, processed_text = Preprocess(data.Text.values)\n    data[\"Email\"] = processed_email\n    data[\"Subject\"] = processed_subject\n    data[\"Processed_text\"] = processed_text\n    \n    data.to_csv(\"preprocessed_final.csv\", index = False, header = True)\n    \nelse:\n    data = pd.read_csv(\"preprocessed_final.csv\", index_col = False)","42ec6377":"data.head()","e1cc22ae":"#checking if preprocessing is correct\n#document 49960 is in index 0\n\nprint(\"Label: \", data.Label[0])\nprint(\"-\"*25)\nprint(\"Email: \", data.Email[0])\nprint(\"-\"*25)\nprint(\"Subject: \", data.Subject[0])\nprint(\"-\"*25)\nprint(\"Processed Text\\n\", data.Processed_text[0])\nprint(\"-\"*25)","9df449f2":"data[\"combined_data\"] = data[\"Email\"].astype(str)+\" \"+data[\"Subject\"].astype(str)+\" \"+data[\"Processed_text\"].astype(str)\ndata = data.drop([\"Email\", \"Subject\", \"Processed_text\"], axis = 1)\ndata.head()","fa5dc3d2":"X = data[\"combined_data\"]\ny = data[\"Label\"]\ny = pd.get_dummies(data[\"Label\"].values)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, stratify = y, random_state = 42)","01db8127":"print(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","a8b32703":"class F1_Score(Callback):\n\n    def on_train_begin(self, logs = {}):\n        self.f1 = []       \n\n    def on_epoch_end(self, epoch, logs = {}):\n        y_pred = []\n        y_true = []\n\n        val_predict = np.asarray(self.model.predict(padded_test))\n        for y in val_predict:\n            y_pred.append(np.argmax(y))\n        for y in np.asarray(y_test):\n            y_true.append(np.argmax(y))\n\n        val_f1 = f1_score(np.asarray(y_true), np.asarray(y_pred), average = \"micro\")\n        self.f1.append(val_f1)\n        print(\"\\t F1 Score: \", val_f1)","dae204c5":"!rm -rf .\/logs\/","13796b28":"os.environ[\"PYTHONHASHSEED\"] = \"0\"\ntf.keras.backend.clear_session()\nnp.random.seed(42)\nrn.seed(42)","87068897":"t = Tokenizer(filters = \"_\")\nt.fit_on_texts(X_train)\nvocab_size = len(t.word_index) + 1\nencoded_train = t.texts_to_sequences(X_train)\nencoded_test = t.texts_to_sequences(X_test)\nmax_length = 100                   \npadded_train = pad_sequences(encoded_train, maxlen = max_length, padding = \"post\", truncating = \"post\")\npadded_test = pad_sequences(encoded_test, maxlen = max_length, padding = \"post\", truncating = \"post\")\n\nwith open(\".\/glove_vectors\", \"rb\") as f:\n    glove = pickle.load(f)\n    glove_words = set(glove.keys())\n    \nembedding_matrix = np.zeros((vocab_size, 300))\nfor word, i in t.word_index.items():\n    if word in glove_words:\n        embedding_vector = glove[word]\n        embedding_matrix[i] = embedding_vector","a8edf96d":"input_layer = Input(shape = (max_length, ), name = \"Input_Layer\")\n\nembed_layer = Embedding(input_dim = vocab_size, output_dim = 300, weights = [embedding_matrix], \n                        input_length = max_length, trainable = False, \n                        name = \"Embedding_Layer\")(input_layer)\n\nconv_1 = Conv1D(128, kernel_size = 3, padding = \"valid\", activation = \"relu\",\n                kernel_initializer = tf.keras.initializers.HeUniform(),\n                name = \"Convolution_layer_1\")(embed_layer)\n\nconv_2 = Conv1D(126, kernel_size = 3, padding = \"valid\", activation = \"relu\",\n                kernel_initializer = tf.keras.initializers.HeUniform(),\n                name = \"Convolution_layer_2\")(embed_layer)\n\nconv_3 = Conv1D(124, kernel_size = 3, padding = \"valid\", activation = \"relu\",\n                kernel_initializer = tf.keras.initializers.HeUniform(),\n                name = \"Convolution_layer_3\")(embed_layer)              \n\nconcat_layer_1 = Concatenate()([conv_1, conv_2, conv_3]) \n\n\nmax_pool_layer_1 = MaxPool1D(pool_size = 2, padding = \"valid\",\n                             name = \"MaxPooling_layer_1\")(concat_layer_1)\n\nconv_4 = Conv1D(68, kernel_size = 3, padding = \"valid\", activation = \"relu\",\n                kernel_initializer = tf.keras.initializers.HeUniform(),\n                name = \"Convolution_layer_4\")(max_pool_layer_1)\nconv_5 = Conv1D(64, kernel_size = 3, padding = \"valid\", activation = \"relu\",\n                kernel_initializer = tf.keras.initializers.HeUniform(),\n                name = \"Convolution_layer_5\")(max_pool_layer_1)\nconv_6 = Conv1D(62, kernel_size = 3, padding = \"valid\", activation = \"relu\",\n                kernel_initializer = tf.keras.initializers.HeUniform(),\n                name = \"Convolution_layer_6\")(max_pool_layer_1)\n\nconcat_layer_2 = Concatenate()([conv_4, conv_5, conv_6])\n\nmax_pool_layer_2 = MaxPool1D(pool_size = 2, padding = \"valid\",\n                             name = \"MaxPooling_layer_2\")(concat_layer_2)\n\ndrop_1 = Dropout(0.8, name = \"Dropout_layer_1\")(max_pool_layer_2)\n\nconv_7 = Conv1D(32, kernel_size = 3, padding = \"valid\", activation = \"relu\",\n                kernel_initializer = tf.keras.initializers.HeUniform(),\n                name = \"Convolution_layer_7\")(drop_1)\n\nflatten = Flatten(name = \"Flatten_layer\")(conv_7)\n\ndrop_2 = Dropout(0.1, name = \"Dropout_layer_2\")(flatten)\n\ndense_1 = Dense(100, activation = \"relu\", \n                kernel_initializer = tf.keras.initializers.HeUniform(),\n                name = \"Dense_Layer_1\")(drop_2)\n\ndrop_3 = Dropout(0.05, name = \"Dropout_layer_3\")(dense_1)\n\noutput = Dense(20, activation = \"softmax\", \n               name = \"Output_Layer\")(drop_3)\n\nmodel1 = Model(inputs = input_layer, outputs = output, \n               name = \"Model-1\")\n\nmodel1.summary()","657fd70f":"tf.keras.utils.plot_model(model1, show_shapes = True, to_file = \"model1.png\")","1d5d4398":"optimizer = Adam(learning_rate = 0.001) \n\nmodel1.compile(optimizer = optimizer, loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n\nf1 = F1_Score()\n\ncheckpoint = ModelCheckpoint(filepath = \"best_model_1_{val_accuracy:.4f}.hdf5\",\n                             monitor='val_accuracy',  verbose = 1, save_best_only = True, mode = \"auto\")\n\nearlystop = EarlyStopping(monitor = \"val_accuracy\", patience = 1, verbose = 1, mode = \"auto\")\n\nlog_dir = \"logs\/model1\"\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir, histogram_freq = 1, write_graph = True)\n\ncallback_list = [checkpoint, earlystop, tensorboard_callback, f1]\n\nmodel1.fit(padded_train, y_train, validation_data = (padded_test, y_test), batch_size = 512, epochs = 100, callbacks = callback_list)","5159cdb9":"%tensorboard --logdir logs\/model1","979b65a3":"!rm -rf .\/logs\/","b0feeb5c":"os.environ[\"PYTHONHASHSEED\"] = \"0\"\ntf.keras.backend.clear_session()\nnp.random.seed(42)\nrn.seed(42)","e3c93a50":"t = Tokenizer(filters = \"_\", char_level = True, oov_token = \"UNK\")\nt.fit_on_texts(X_train)\nvocab_size = len(t.word_index) + 1\nencoded_train = t.texts_to_sequences(X_train)\nencoded_test = t.texts_to_sequences(X_test)\nmax_length = 3000              \npadded_train = pad_sequences(encoded_train, maxlen = max_length, padding = \"post\", truncating = \"post\")\npadded_test = pad_sequences(encoded_test, maxlen = max_length, padding = \"post\", truncating = \"post\")\n\nembedding_index = dict()\nwith open(\".\/glove.840B.300d-char.txt\", \"rb\") as f:\n    for line in f:\n        line_split = line.strip().split()\n        char = line_split[0]\n        coefs = np.asarray(line_split[1:])\n        embedding_index[char] = coefs\nf.close()\n\n#https:\/\/towardsdatascience.com\/character-level-cnn-with-keras-50391c3adf33\nembedding_matrix = []\nembedding_matrix.append(np.zeros(vocab_size))\nfor char, i in t.word_index.items():\n    onehot = np.zeros(vocab_size)\n    onehot[i - 1] = 1\n    embedding_matrix.append(onehot)\n\nembedding_matrix = np.asarray(embedding_matrix)","44c1d02c":"input_layer = Input(shape = (max_length, ), name = \"Input_Layer\")\n\nembed_layer = Embedding(input_dim = vocab_size, output_dim = 40, weights = [embedding_matrix], \n                        input_length = max_length, trainable = False, \n                        name = \"Embedding_Layer\")(input_layer)\n\nconv_1 = Conv1D(256, kernel_size = 3, padding = \"valid\", activation = \"relu\",\n                kernel_initializer = tf.keras.initializers.HeNormal(),\n                name = \"Convolution_layer_1\")(embed_layer)\n\nconv_2 = Conv1D(128, kernel_size = 3, padding = \"valid\", activation = \"relu\",\n                kernel_initializer = tf.keras.initializers.HeNormal(),\n                name = \"Convolution_layer_2\")(conv_1)\n\nmax_pool_layer_1 = MaxPool1D(pool_size = 4, padding = \"valid\",\n                             name = \"MaxPooling_layer_1\")(conv_2)\n\ndrop_1 = Dropout(0.4, name = \"Dropout_layer_1\")(max_pool_layer_1)\n\nconv_3 = Conv1D(64, kernel_size = 3, padding = \"valid\", activation = \"relu\",\n                kernel_initializer = tf.keras.initializers.HeNormal(),\n                name = \"Convolution_layer_3\")(drop_1)\n\nconv_4 = Conv1D(32, kernel_size = 3, padding = \"valid\", activation = \"relu\",\n                kernel_initializer = tf.keras.initializers.HeNormal(),\n                name = \"Convolution_layer_4\")(conv_3)\n\nmax_pool_layer_2 = MaxPool1D(pool_size = 4, padding = \"valid\",\n                             name = \"MaxPooling_layer_2\")(conv_4)\n\ndrop_2 = Dropout(0.4, name = \"Dropout_layer_2\")(max_pool_layer_2)\n\nflatten = Flatten(name = \"Flatten_layer\")(drop_2)\n\ndense_1 = Dense(100, activation = \"relu\", \n                kernel_initializer = tf.keras.initializers.HeNormal(),\n                name = \"Dense_Layer_1\")(flatten)\n\ndrop_3 = Dropout(0.1, name = \"Dropout_layer_3\")(dense_1)\n\noutput = Dense(20, activation = \"softmax\", \n               name = \"Output_Layer\")(drop_3)\n\nmodel2 = Model(inputs = input_layer, outputs = output, \n               name = \"Model-2\")\n\nmodel2.summary()             ","8c9bd021":"tf.keras.utils.plot_model(model2, show_shapes = True, to_file = \"model2.png\")","bae675d8":"optimizer = Adam(learning_rate = 0.01) \n\nmodel2.compile(optimizer = optimizer, loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n\nf1 = F1_Score()\n\ncheckpoint = ModelCheckpoint(filepath = \"best_model_2_{val_accuracy:.4f}.hdf5\",\n                             monitor='val_accuracy',  verbose = 1, save_best_only = True, mode = \"auto\")\n\nearlystop = EarlyStopping(monitor = \"val_accuracy\", patience = 2, verbose = 1, mode = \"auto\")\n\nlog_dir = \"logs\/model2\"\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir, histogram_freq = 1, write_graph = True)\n\ncallback_list = [checkpoint, earlystop, tensorboard_callback, f1]\n\nmodel2.fit(padded_train, y_train, validation_data = (padded_test, y_test), batch_size = 512, epochs = 100, callbacks = callback_list)","6a8a292b":"%tensorboard --logdir logs\/model2","f6e0431f":"## Model-2 Using CNN with character embeddings","91bbc33c":"## Model-1 Using CNN with word embeddings"}}