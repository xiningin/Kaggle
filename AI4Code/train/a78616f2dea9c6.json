{"cell_type":{"beb7a5bf":"code","b497a2df":"code","c22cb27f":"code","b5d54dd1":"code","86f535f2":"code","fed13a02":"code","6939c8f7":"code","c3c138d6":"code","2b1dc6f3":"code","bd7cef55":"code","26c94fe5":"code","c4a9598f":"code","cff254b1":"code","b9594245":"code","fb277482":"code","217de3b2":"code","597d82b0":"code","623e9bad":"code","adb52d27":"code","f0f74d6c":"code","2e8e20d2":"code","291a68b2":"code","5b1ed08a":"code","0e86fdd8":"code","452d2aa9":"code","71b18cbc":"code","86f4adf4":"code","1e205b26":"code","fa63136a":"code","6b08297a":"code","875bd83a":"code","dd4f2c28":"code","95eb4bc9":"code","057d1e4d":"code","4cd7565d":"code","c46d0c14":"code","7e7dfab3":"code","e165f3c2":"code","b2d078bc":"code","0a4004e9":"code","9fc1be3f":"code","cae22237":"code","54a22cf4":"code","d509a7d0":"code","682ef19c":"markdown","687453aa":"markdown","45b104c7":"markdown","844ad5cd":"markdown","6300d9b8":"markdown","0bc18d73":"markdown","14295262":"markdown","31a2541c":"markdown","6101c29f":"markdown","eec5699a":"markdown","c5f236b9":"markdown","e47d703b":"markdown","5519c106":"markdown","6f60cb26":"markdown","05d6e22a":"markdown"},"source":{"beb7a5bf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport gc\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b497a2df":"CSV_FILE = \"\/kaggle\/input\/stanford-plato-corpus\/simple_data.csv\"","c22cb27f":"df = pd.read_csv(CSV_FILE)","b5d54dd1":"df.size","86f535f2":"df.shape","fed13a02":"df.head()","6939c8f7":"df.columns","c3c138d6":"def string_to_python_list(x):\n    return eval(x) if isinstance(x,str) else None\n\ndf['related_entries_list'] = df['related_entries_list'].apply(string_to_python_list)","2b1dc6f3":"df['sections'] = df['sections'].apply(string_to_python_list)","bd7cef55":"df.dtypes","26c94fe5":"example_df = pd.DataFrame({'A': [[1, 2, 3], 'foo', [], [3, 4]], 'B': 1})\nexample_df","c4a9598f":"example_df.explode(\"A\")","cff254b1":"# related_topic_per_row_df = df.explode(\"related_entries_list\")\n","b9594245":"# related_topic_per_row_df.head()","fb277482":"# section_per_row_and_related_topic_per_row_df = related_topic_per_row_df.explode(\"sections\")","217de3b2":"# section_per_row_and_related_topic_per_row_df.head()","597d82b0":"df = df.explode(\"sections\")","623e9bad":"df.columns","adb52d27":"# df = df.rename(columns={\"related_entries_list\": \"related_topic\"})\ndf = df.rename(columns={\"sections\": \"section\"})\ndf = df.rename(columns={\"filenames\": \"filename\"})","f0f74d6c":"df.head()","2e8e20d2":"df['section'].values[1]","291a68b2":"df['section'].values[2].keys()","5b1ed08a":"df['section'].values[2]['paragraphs'][0].keys()","0e86fdd8":"def safe_get(getter=lambda x, n: x):\n    def _get(x):\n        if x == None:\n            return None\n        elif isinstance(x, dict):\n            return getter(x)\n        else:\n            raise NotImplementedError()\n    return _get\n\ndef _id_getter(x):\n    return x[\"id\"]\n\ndef _ht_getter(x):\n    return x[\"heading_text\"]\n\ndef _p_getter(x):\n    return x[\"paragraphs\"]\n\ndef _text_getter(x):\n    return x[\"text\"]","452d2aa9":"df['section.id'] = df['section'].apply(safe_get(_id_getter))\ndf['section.heading_text'] = df['section'].apply(safe_get(_ht_getter))\ndf['section.paragraphs'] = df['section'].apply(safe_get(_p_getter))","71b18cbc":"df = df.explode(\"section.paragraphs\")","86f4adf4":"df = df.rename(columns={\"section.paragraphs\": \"section.paragraph\"})","1e205b26":"df['section.paragraph.id'] = df['section.paragraph'].apply(safe_get(_id_getter))\ndf['section.paragraph.text'] = df['section.paragraph'].apply(safe_get(_text_getter))","fa63136a":"df.head()","6b08297a":"df.columns","875bd83a":"mask = [\n    'filename', 'filetype', 'topic', 'title', 'author', 'creator',\n    'preamble_text', \n    #'section', \n    'related_entries_list', \n    # 'plain_text',\n    'section.id', 'section.heading_text', \n    # 'section.paragraph', \n    'section.paragraph.id', 'section.paragraph.text'\n]\ndf = df[mask]","dd4f2c28":"df.size","95eb4bc9":"df.shape","057d1e4d":"len(df)","4cd7565d":"df.head()","c46d0c14":"def chunks(lst, N):\n    \"\"\"Yield N successive `len(lst)\/\/N`-sized chunks from lst.\"\"\"\n    chunk_size = len(lst)\/\/N\n    for i in range(0, len(lst), len(lst)\/\/N):\n        yield lst[i:i + chunk_size]","7e7dfab3":"TOTAL_CHUNKS = 10000","e165f3c2":"df_generator = chunks(df, TOTAL_CHUNKS)","b2d078bc":"CSV_FILENAME = \"data_per_paragraph.csv\"\n\n# save first one\nfirst_with_header = next(df_generator)\n\nfirst_with_header.to_csv(CSV_FILENAME, mode='w', header=True)","0a4004e9":"del first_with_header\ngc.collect()","9fc1be3f":"i = 2\nCOLLECTED = gc.collect()\nprint(COLLECTED)\nfor x in df_generator:\n#     if (i % 1000 == 0):\n#         COLLECTED = gc.collect()\n    print(f\"appending... {i} \/ {TOTAL_CHUNKS}\\r\", end=\"\")\n    x.to_csv(CSV_FILENAME, mode='a', header=False)\n    i+=1","cae22237":"pd.read_csv(CSV_FILENAME).head()","54a22cf4":"df[df[\"section.paragraph.text\"].isna()].groupby(\"filetype\").count()","d509a7d0":"na_par_df = df[df[\"section.paragraph.text\"].isna()]\n(\n    na_par_df.join(\n        na_par_df[\"filename\"].apply(lambda x: os.path.split(x)[1]),\n        rsuffix=\"_____\"\n    ).groupby(\"filename_____\")\n    .count()\n    .sort_values([\"filename\",\"filetype\",\"topic\",\"title\",\"author\"], ascending=False)\n)","682ef19c":"# What's up with the np.nan paragraphs?\n\nIt seems those paragraphs (if any) did not fit the HTML stucture assumptions when the data was extracted from the web archive. \n\nThat's okay.\n\nTODO: consider dealing with these files counted below...","687453aa":"# Explode the section.paragraphs which is also list-of-dict","45b104c7":"# Now save","844ad5cd":"# Now extract all the section.id & section.heading_text & section.paragraphs on the section JSON","6300d9b8":"# Now extract the *section.paragraph.id* and *section.paragraph.text*","0bc18d73":"a better name","14295262":"### [example](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.explode.html)","31a2541c":"# ~~Explode ***related_entries_list***~~\n## bad idea... too many rows","6101c29f":"# Now explode","eec5699a":"# more tricks to conserve memory","c5f236b9":"# Just explode the sections \n## to later extract paragraphs","e47d703b":"## Now safely get the dictionary values","5519c106":"### first notice the keys","6f60cb26":"# Now just choose the useful columns to conserve memory\n\n### consider that the `section` dict is no longer needed since the data was extracted. same for `section.paragraph`\n\n### also consider that `plain_text` duplicates the `section.paragraph.text` without the granualar section\/paragraph structure","05d6e22a":"# read the output to make sure all is well"}}