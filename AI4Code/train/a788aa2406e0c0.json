{"cell_type":{"c6707e4a":"code","359107d8":"code","d33fabf4":"code","2bf50c4e":"code","5defe5bd":"code","30446e25":"code","5a6bf0ee":"code","05e909bc":"code","c7e63950":"code","9ba0410d":"code","b100b0a0":"code","240269c8":"code","ca517d8f":"code","a19a6fea":"code","83fa5b69":"code","6981d9de":"code","df890165":"code","20602de4":"code","eeaa0266":"code","ade77246":"code","21f8b503":"code","e15572da":"code","213b01ca":"code","173ac330":"code","d6ed631a":"code","8c04d58f":"code","620c10d7":"code","170ded40":"code","59b3df55":"code","ae9e871f":"code","6e3ba436":"code","7a7bce80":"code","7fc8d394":"code","2bc95dec":"code","ec2f93ce":"code","a294efed":"code","2d02d072":"code","41334828":"code","cd28a1b4":"code","f3a07148":"code","e72fe96a":"code","91997496":"code","08a2db2e":"code","a55205b1":"code","c908ebc1":"code","390e509c":"code","c50da878":"code","333bc0f1":"code","c9b3259f":"code","0b35d9be":"code","08cb8917":"code","83c8506f":"code","7c6a6149":"code","50cb0f83":"code","4d88fccd":"code","db06ff5b":"code","989ab827":"markdown","9d132861":"markdown","588c5a90":"markdown","d0a5b145":"markdown","ebe9a8ea":"markdown","f9872988":"markdown","77e7b1d4":"markdown","e3c225f2":"markdown","136ce59f":"markdown","d17e5071":"markdown","04192453":"markdown","bb530eba":"markdown","5e9d1fd0":"markdown","a94a79a4":"markdown","56d90ffd":"markdown","98f530fb":"markdown","e39d69b9":"markdown","f68f8926":"markdown","b8d26ff8":"markdown","b297deed":"markdown","8df4afcc":"markdown"},"source":{"c6707e4a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","359107d8":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d33fabf4":"train=pd.read_csv('..\/input\/flight-fare\/Data_Train.csv')\ntest=pd.read_csv('..\/input\/flight-fare\/Test_set.csv')","2bf50c4e":"test.head()\n\n","5defe5bd":"train.isnull().sum()","30446e25":"train[train[\"Route\"].isnull()==True]","5a6bf0ee":"train.dropna(inplace=True)","05e909bc":"train.shape","c7e63950":"plt.figure(figsize=(10,5))\nsns.set_theme(style=\"darkgrid\")\nsns.countplot(y=\"Airline\",data=train,palette=\"coolwarm\")","9ba0410d":"train[\"Airline\"].value_counts()","b100b0a0":"Airline=pd.get_dummies(train[\"Airline\"],drop_first=True)","240269c8":"Airline.head()","ca517d8f":"fig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\nfig.suptitle('Visualization of Source and Destination')\nsns.set_theme(style=\"darkgrid\")\nsns.countplot(x=\"Source\",data=train,ax=axes[0],palette=\"coolwarm\")\nsns.countplot(x=\"Destination\",data=train,ax=axes[1],palette=\"coolwarm\")","a19a6fea":"Source=pd.get_dummies(train[[\"Source\"]],drop_first=True)\nDestination=pd.get_dummies(train[[\"Destination\"]],drop_first=True)","83fa5b69":"Destination.head()","6981d9de":"train.drop([\"Route\"],axis=1,inplace=True)\ntrain.head()","df890165":"info=train[\"Additional_Info\"].value_counts()","20602de4":"sum=0\nfor i in info:\n    sum=sum+i\nprint(\"The percent missing is:\", (info[0]\/sum)*100)    ","eeaa0266":"train.drop([\"Additional_Info\"],axis=1,inplace=True)","ade77246":"train.head()","21f8b503":"train[\"Total_Stops\"].value_counts()","e15572da":"train.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)","213b01ca":"train.head()","173ac330":"plt.figure(figsize=(15,8))\nsns.set_theme(style=\"darkgrid\")\nplt.figure(figsize=(15,8))\nsns.boxplot(x=\"Total_Stops\",y=\"Price\",data=train,palette=\"coolwarm\")\nplt.title(\"Visualization of Price and Total Stops\")","d6ed631a":"train[\"Journey_day\"] = pd.to_datetime(train.Date_of_Journey, format=\"%d\/%m\/%Y\").dt.day\ntrain[\"Journey_month\"] = pd.to_datetime(train[\"Date_of_Journey\"], format = \"%d\/%m\/%Y\").dt.month\ntrain.drop([\"Date_of_Journey\"], axis = 1, inplace = True)\n","8c04d58f":"train.head()","620c10d7":"# Dep_Time\ntrain[\"Dep_hour\"] = pd.to_datetime(train[\"Dep_Time\"]).dt.hour\ntrain[\"Dep_min\"] = pd.to_datetime(train[\"Dep_Time\"]).dt.minute\ntrain.drop([\"Dep_Time\"], axis = 1, inplace = True)\n\n# Arrival_Time\ntrain[\"Arrival_hour\"] = pd.to_datetime(train.Arrival_Time).dt.hour\ntrain[\"Arrival_min\"] = pd.to_datetime(train.Arrival_Time).dt.minute\ntrain.drop([\"Arrival_Time\"], axis = 1, inplace=True)","170ded40":"train.head()","59b3df55":"duration = list(train[\"Duration\"])\n\nfor i in range(len(duration)):\n    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n        if \"h\" in duration[i]:\n            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n        else:\n            duration[i] = \"0h \" + duration[i]           # Adds 0 hour\n\nduration_hours = []\nduration_mins = []\nfor i in range(len(duration)):\n    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))   # Extracts only minutes from duration\n\n# Adding Duration column to test set\ntrain[\"Duration_hours\"] = duration_hours\ntrain[\"Duration_mins\"] = duration_mins\ntrain.drop([\"Duration\"], axis = 1, inplace = True)\n","ae9e871f":"train[\"Duration\"]=(train[\"Duration_hours\"]*60)+train[\"Duration_mins\"]\n","6e3ba436":"train.drop([\"Duration_hours\",\"Duration_mins\"],inplace=True,axis=1)\ntrain.head()","7a7bce80":"final_train = pd.concat([train, Airline, Source, Destination], axis = 1)\n","7fc8d394":"final_train.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)\nfinal_train.head()","2bc95dec":"pd.set_option('display.max_columns', None)\nfinal_train.head()","ec2f93ce":"final_train.iloc[:,2:9].columns","a294efed":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nscaled=scaler.fit_transform(final_train.iloc[:,2:9])\nfinal_train['Journey_day']=scaled[:,0]\nfinal_train[\"Journey_month\"]=scaled[:,1]\nfinal_train[\"Dep_hour\"]=scaled[:,2]\nfinal_train['Dep_min']=scaled[:,3]\nfinal_train[\"Arrival_hour\"]=scaled[:,4]\nfinal_train[\"Arrival_min\"]=scaled[:,5]\nfinal_train[\"Duration\"]=scaled[:,6]","2d02d072":"final_train.head()","41334828":"# Preprocessing\n\nprint(\"Test data Info\")\nprint(\"-\"*75)\nprint(test.info())\n\nprint()\nprint()\n\nprint(\"Null values :\")\nprint(\"-\"*75)\ntest.dropna(inplace = True)\nprint(test.isnull().sum())\n\n# EDA\n\n# Date_of_Journey\ntest[\"Journey_day\"] = pd.to_datetime(test.Date_of_Journey, format=\"%d\/%m\/%Y\").dt.day\ntest[\"Journey_month\"] = pd.to_datetime(test[\"Date_of_Journey\"], format = \"%d\/%m\/%Y\").dt.month\ntest.drop([\"Date_of_Journey\"], axis = 1, inplace = True)\n\n# Dep_Time\ntest[\"Dep_hour\"] = pd.to_datetime(test[\"Dep_Time\"]).dt.hour\ntest[\"Dep_min\"] = pd.to_datetime(test[\"Dep_Time\"]).dt.minute\ntest.drop([\"Dep_Time\"], axis = 1, inplace = True)\n\n# Arrival_Time\ntest[\"Arrival_hour\"] = pd.to_datetime(test.Arrival_Time).dt.hour\ntest[\"Arrival_min\"] = pd.to_datetime(test.Arrival_Time).dt.minute\ntest.drop([\"Arrival_Time\"], axis = 1, inplace = True)\n\n# Duration\nduration = list(test[\"Duration\"])\n\nfor i in range(len(duration)):\n    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n        if \"h\" in duration[i]:\n            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n        else:\n            duration[i] = \"0h \" + duration[i]           # Adds 0 hour\n\nduration_hours = []\nduration_mins = []\nfor i in range(len(duration)):\n    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))   # Extracts only minutes from duration\n\n# Adding Duration column to test set\ntest[\"Duration_hours\"] = duration_hours\ntest[\"Duration_mins\"] = duration_mins\ntest.drop([\"Duration\"], axis = 1, inplace = True)\ntest[\"Duration\"]=(test[\"Duration_hours\"]*60)+test[\"Duration_mins\"]\ntest.drop([\"Duration_hours\",\"Duration_mins\"],inplace=True,axis=1)\n\n\n# Categorical data\n\nprint(\"Airline\")\nprint(\"-\"*75)\nprint(test[\"Airline\"].value_counts())\nAirline = pd.get_dummies(test[\"Airline\"], drop_first= True)\n\nprint()\n\nprint(\"Source\")\nprint(\"-\"*75)\nprint(test[\"Source\"].value_counts())\nSource = pd.get_dummies(test[\"Source\"], drop_first= True)\n\nprint()\n\nprint(\"Destination\")\nprint(\"-\"*75)\nprint(test[\"Destination\"].value_counts())\nDestination = pd.get_dummies(test[\"Destination\"], drop_first = True)\n\n# Additional_Info contains almost 80% no_info\n# Route and Total_Stops are related to each other\ntest.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)\n\n# Replacing Total_Stops\ntest.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)\n\n# Concatenate dataframe --> test + Airline + Source + Destination\nfinal_test = pd.concat([test, Airline, Source, Destination], axis = 1)\n\nfinal_test.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)\n\nprint()\nprint()\n\nprint(\"Shape of test data : \", final_test.shape)","cd28a1b4":"final_test.head()","f3a07148":"scaled=scaler.transform(final_test.iloc[:,1:8])\nfinal_test['Journey_day']=scaled[:,0]\nfinal_test[\"Journey_month\"]=scaled[:,1]\nfinal_test[\"Dep_hour\"]=scaled[:,2]\nfinal_test['Dep_min']=scaled[:,3]\nfinal_test[\"Arrival_hour\"]=scaled[:,4]\nfinal_test[\"Arrival_min\"]=scaled[:,5]\nfinal_test[\"Duration\"]=scaled[:,6]","e72fe96a":"X=final_train.drop([\"Price\"],axis=1)\nX.head()","91997496":"y=final_train.Price\ny.head()","08a2db2e":"plt.figure(figsize = (12,12))\nsns.heatmap(train.corr(), annot = True, cmap = \"viridis\")\nplt.show()","a55205b1":"from sklearn.ensemble import ExtraTreesRegressor\nselection = ExtraTreesRegressor()\nselection.fit(X, y)\nplt.figure(figsize = (10,8))\nfeature = pd.Series(selection.feature_importances_, index=X.columns)\nfeature.nlargest(20).plot(kind='barh')\nplt.show()","c908ebc1":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n","390e509c":"from sklearn.svm import SVR\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn import metrics\n\nparam={'C':[0.001,0.01,0.1,10,100],'kernel':['rbf','poly']}\nrf_svc = RandomizedSearchCV(estimator = SVR(), param_distributions =param,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\nrf_svc.fit(X_train,y_train)\n\npredictions=rf_svc.predict(X_test)\nprint('MAE:', metrics.mean_absolute_error(y_test, predictions))\nprint('MSE:', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\nprint('R-squared: {}'.format(metrics.r2_score(y_test,predictions)))","c50da878":"rf_svc.best_score_","333bc0f1":"rf_svc.score(X_test,y_test)","c9b3259f":"from sklearn.ensemble import RandomForestRegressor\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\n\nreg_rf = RandomForestRegressor()\n\nrf_random = RandomizedSearchCV(estimator = reg_rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42)\n\n\nrf_random.fit(X_train,y_train)","0b35d9be":"rf_random.best_params_","08cb8917":"prediction = rf_random.predict(X_test)\n\nplt.figure(figsize = (8,8))\nsns.distplot(y_test-prediction)\nplt.show()","83c8506f":"plt.figure(figsize = (8,8))\nplt.scatter(y_test, prediction, alpha = 0.5)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\n","7c6a6149":"print('MAE:', metrics.mean_absolute_error(y_test, prediction))\nprint('MSE:', metrics.mean_squared_error(y_test, prediction))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, prediction)))","50cb0f83":"metrics.r2_score(y_test,prediction)","4d88fccd":"import pickle\nwith open(\"flight_prices\",'wb') as f:\n    pickle.dump(rf_random,f)\n\nwith open(\"flight_prices\",'rb') as f:\n    model=pickle.load(f)","db06ff5b":"y_prediction = model.predict(X_test)\nmetrics.r2_score(y_test,prediction)","989ab827":"***Total_Stops***","9d132861":"***Source and Destination***\n- As Source and Destination is a categorical and nominal we will use One-Hot-Encoding using pandas get_dummies()","588c5a90":"# Model Building ","d0a5b145":"### Train Set","ebe9a8ea":"# Future Aspects","f9872988":"As about 80% of data is NO INFO we will drop this column","77e7b1d4":"### Test Set","e3c225f2":"**Best model: Random Forest Regressor**\n- MAE: 1143.70131\n- MSE: 3524552.2059\n- RMSE: 1877.37907\n- R-squared: 0.83653\n\n**Most important features**\n- Total_Stops\n- Journey_Day\n- Jet Airways","136ce59f":"As Route and Total_stops imply the same thing we can drop the Route feature.\n","d17e5071":"# Feature Importance","04192453":"***Additional_Info***","bb530eba":"Use of other models.\nDeployment of Model using Flask.","5e9d1fd0":"# Importing Necessary Data","a94a79a4":"***Date Of Journey***","56d90ffd":"# Save Model","98f530fb":"# Conclusions","e39d69b9":"As from the data we see as stop increases fare decreases so it plays a vital role and so we need to Label encode it.","f68f8926":"***Duration***","b8d26ff8":"***Airline***\n- As Airline is a categorical and nominal we will use One-Hot-Encoding using pandas get_dummies()","b297deed":"# Exploratory Data Analysis","8df4afcc":"***Dep_Time and Arrival_Time***"}}