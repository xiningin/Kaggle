{"cell_type":{"5943e48a":"code","afd442b3":"code","a807c9f3":"code","73fc1817":"code","3e36612b":"code","3e708c98":"code","37bd0707":"code","3b8b0792":"code","540831f4":"code","43475489":"code","bf6c6e2d":"code","668208e4":"code","de98930e":"code","4a9c12a8":"code","58f10678":"code","d3633017":"code","02596707":"code","bf6e9155":"markdown","07733f4f":"markdown","d4c75a88":"markdown","986bab35":"markdown","5b5f1a82":"markdown","a11d87ea":"markdown","da3105e3":"markdown","be556375":"markdown","cc11f9d5":"markdown","2e6664aa":"markdown"},"source":{"5943e48a":"import pandas as pd\nimport numpy as np","afd442b3":"# -*- coding: utf-8 -*-\nfile_txt = open('..\/input\/nlp-pdf\/codeExtractCorpus\/Amavisse\/PDF\/PDF.txt', mode='r', encoding='UTF-8')\ntexto = file_txt.read()\ntexto","a807c9f3":"\nfile_txt_ = open('..\/input\/nlp-pdf\/codeExtractCorpus\/Amavisse\/Keywords\/keywords key terms.txt', mode='r',  encoding='latin-1')\ntexto_keywords = file_txt_.read()\n# texto_keywords","73fc1817":"!pip install -U spacy","3e36612b":"import spacy\n# nlp = spacy.load(\"pt_core_news_sm\")\n!python -m spacy download pt_core_news_sm ","3e708c98":"from sklearn.feature_extraction.text import CountVectorizer\nfrom textblob import TextBlob\nimport scipy.stats as stats\n\nimport spacy \nfrom tqdm import tqdm\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation,  PCA, NMF\nimport random \n\nimport gensim\nfrom gensim import corpora, models, similarities\n\nimport logging\nimport tempfile\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nfrom collections import OrderedDict\n\nimport seaborn as sns\nimport pyLDAvis.gensim\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport pandas as pd\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","37bd0707":"# Define helper functions\ndef get_top_n_words(n_top_words, count_vectorizer, text_data):\n    '''\n    returns a tuple of the top n words in a sample and their \n    accompanying counts, given a CountVectorizer object and text sample\n    '''\n    vectorized_headlines = count_vectorizer.fit_transform(text_data.values)\n    vectorized_total = np.sum(vectorized_headlines, axis=0)\n    word_indices = np.flip(np.argsort(vectorized_total)[0,:], 1)\n    word_values = np.flip(np.sort(vectorized_total)[0,:],1)\n    \n    word_vectors = np.zeros((n_top_words, vectorized_headlines.shape[1]))\n    for i in range(n_top_words):\n        word_vectors[i,word_indices[0,i]] = 1\n\n    words = [word[0].encode('ascii').decode('utf-8') for \n             word in count_vectorizer.inverse_transform(word_vectors)]\n\n    return (words, word_values[0,:n_top_words].tolist()[0])","3b8b0792":"# Criando stopwords em portugu\u00eas utilizando a biblioteca spacy\nimport pt_core_news_sm\nnlp = pt_core_news_sm.load()","540831f4":"\npt_stopwords = sorted([token.text for token in nlp.vocab if token.is_stop])\nlist_exclude = ['obrigado', 'bom', 'mal', 'nenhuma', 'maior',\n             'bem', 'n\u00e3o', 'm\u00e1ximo', 'boa', 'mais',\n               'bastante', 'certamente', 'certeza', 'contra',\n                'quarentena', 'coronav\u00edrus', 'presidente', 'impeachment', 'demitido', 'demitida']\nfor word in list_exclude:\n    nlp.vocab[word].is_stop = False\nlist_include = set(['o', 'a', 't\u00e1', 'ta', 'ser', 'pro', 'to', 't\u00f4', 'vc', 'voc\u00ea', 'voce', 'pra','pro',\n                    'pq', '\u00e9', 'vou', 'que','t\u00e3o', 'gt', 'de', 'da', 'do', 'em', 'uma', 'l\u00e1',\n                    'j\u00e1', 'no', 'para', 'na', 'com', 'um', 'minha', 'se', 'isso', 'por', 'vou',\n                    'os', 'isso', 'como', 'mesmo', 'tenho', 'aqui', 'ele', 'ela', 'quem', 'fazer',\n                    'eu', 's\u00f3', 'ai', 'mais', 's\u00f3', 'querer', 'https', 'ter', 'estar', 'ficar',\n                    'dos', 'das', 'vcs', 'tem', 'as', 'mas','ao', 'para', '\"', 'em', 'do', 'da', 'dos', 'das','e', 'pelo', 'pelas', 'pela','voc\u00ea', 'por',\n                    'tava', 'nao', 'sao', 'ja', 'so', 'nossa',\n                    'nosso', 'estao', 'tco', 'me', 'dia', 'te', 'ver', 'sera', 'porra', 'fez', 'ne',\n                    'kkk','kkkkkk', 'puta', 'kkkkkkkk', 'hj', 'afff', 'gbr', 'meu', 'cara', 'guri', 'cmg',\n                    'ctg', 'agr', 'pqp', 'vdd', 'eh', 'va', 'obg',\n                    'corona','virus','coronavirus','covid','covid19','19'\n                   'nem', 'numa', 'num', 'nuns', 'ces', 'voces', 'oce', 'oces', 'kkkk', 'vao', 'via',\n                    'hj', 'hoje', 'tudo', 'todo', 'toda',\n                    'vir', 'bem','ao','sem','ou','vai', 'dizer', 'entao', 'dizer', 'entao',\n                    'tao', 'tu', 'mim', 'mano', 'oq', 'pos', 'dm', 'dps',\n                    'coronavirusoutbreak', 'coronavirusPandemic', 'dar', 'vairus',\n                    'ainda', 'assim']\n                  )\nfor w in list_include:\n    nlp.vocab[w].is_stop = True\nstop_words = sorted([token.text for token in nlp.vocab if token.is_stop])","43475489":"# Criar dicion\u00e1rio\nid2word = corpora.Dictionary([list(set(texto_keywords))] )\nprint(len(id2word), type(id2word))\nid2word","bf6c6e2d":"# Criar dicion\u00e1rio\nid2word = corpora.Dictionary([texto_keywords.split()])\n# Criar o Corpus: A Frequ\u00eancia das palavras nos documentos\ncorpus = [id2word.doc2bow(text) for text in [texto_keywords.split()]]","668208e4":"tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model\ncorpus_tfidf = tfidf[corpus]  # step 2 -- use the model to transform vectors","de98930e":"# https:\/\/medium.com\/somos-tera\/como-modelar-t%C3%B3picos-atrav%C3%A9s-de-latent-dirichlet-allocation-lda-atrav%C3%A9s-da-biblioteca-gensim-1fa17357ad4b\n#I will try 15 topics\ntotal_topics = 5\n\nlda = models.LdaModel(corpus, id2word=id2word, num_topics=total_topics)\ncorpus_lda = lda[corpus_tfidf] # create a double wrapper over the original corpus: bow->tf","4a9c12a8":"# Show first n important word in the topics\ndata_lda = {i: OrderedDict(lda.show_topic(i,25)) for i in range(total_topics)}\ndata_lda","58f10678":"df_lda = pd.DataFrame(data_lda)\ndf_lda = df_lda.fillna(0).T\nprint(df_lda.shape)","d3633017":"corpus_graph_1 = list(set( texto_keywords ))\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nfirstcloud = WordCloud(\n                          stopwords=STOPWORDS,\n                          background_color='black',\n                          width=4500,\n                          height=2800\n                         ).generate(\" \".join(corpus_graph_1))\nplt.imshow(firstcloud)\nplt.axis('off')\nplt.show()","02596707":"pyLDAvis.enable_notebook()\npanel = pyLDAvis.gensim.prepare(lda, corpus_lda, id2word, mds='tsne')\npanel","bf6e9155":"Criando stopwords em portugu\u00eas utilizando a biblioteca spacy","07733f4f":"Stop words\n\n- Adicionar mais stopwords.","d4c75a88":"Nuvem de palavras\n- \u00e9 preciso fazer um tratamento do txt de entrada removendo mais stopwords.","986bab35":"Criando dicion\u00e1rio","5b5f1a82":"Visualiza\u00e7\u00e3o dos t\u00f3picos","a11d87ea":"instalando o spacy","da3105e3":"Mostrando as primeiras n palavras importantes nos t\u00f3picos.","be556375":"Importando o txt","cc11f9d5":"Algumas fun\u00e7\u00f5es que podem ser usadas na an\u00e1lise","2e6664aa":"Importando as bibliotecas"}}