{"cell_type":{"1595638d":"code","f4f38f53":"code","3caf2bd1":"code","55953363":"code","f91cea9f":"code","c40d8f41":"code","da54429f":"code","6b1b160e":"code","0b1f48fa":"code","b26a7f2f":"code","390ed41c":"code","0088b128":"code","465b91c3":"code","cf315d8a":"code","c3f74eee":"code","16f961a4":"code","929a4b59":"code","80774929":"code","5ad51157":"code","aa4cd0b6":"code","630c3fb6":"code","d2a51339":"code","034c3adc":"code","40c330b0":"code","a3c2d9e1":"code","aada2c62":"code","63a0ddee":"code","18b95384":"code","247d1b75":"code","67ca020f":"code","5a6e339c":"code","3f71cf0d":"code","ead0573f":"code","35e722fb":"markdown","f9b8ccdf":"markdown","5ded7474":"markdown","6365ac79":"markdown","154b4b60":"markdown","b4125bd9":"markdown","d37c789a":"markdown","63dcd767":"markdown","cc83b3a6":"markdown","c0b4afc2":"markdown","b5e53c27":"markdown","69c8a6b1":"markdown","969d2a3d":"markdown","cc8efb0e":"markdown","1fefb48a":"markdown","141b86a5":"markdown","b63c3b89":"markdown","ef83ef4b":"markdown","9183a109":"markdown","4e62ca3d":"markdown","0db20240":"markdown","e3e97007":"markdown","e5d65400":"markdown","be2bdacd":"markdown","f4f48a06":"markdown","ecee314a":"markdown","cad413cb":"markdown","68e89c3a":"markdown","8af2d4e5":"markdown","a352389a":"markdown","30f8ce23":"markdown","7efae55f":"markdown","b53e1458":"markdown"},"source":{"1595638d":"import numpy as np\nimport pandas as pd\nimport re\n\nfrom nltk import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom gensim.summarization import bm25\n\nimport json\n\nimport time\n\nfrom bs4 import BeautifulSoup\nimport requests as r\nimport string","f4f38f53":"metadata = pd.read_csv('..\/input\/CORD-19-research-challenge\/metadata.csv', dtype={'cord_uid': 'str',\n                                                                         'sha': 'str',\n                                                                         'source_x': 'str',\n                                                                         'title': 'str',\n                                                                         'doi': 'str',\n                                                                         'pmcid': 'str',\n                                                                         'pubmed_id': 'str',\n                                                                         'license': 'str',\n                                                                         'abstract': 'str',\n                                                                         'publish_time': 'str',\n                                                                         'authors': 'str',\n                                                                         'journal': 'str',\n                                                                         'mag_id': 'str',\n                                                                         'who_covidence_id': 'str',\n                                                                         'arxiv_id': 'str',\n                                                                         'pdf_json_files': 'str',\n                                                                         'pmc_json_files': 'str',\n                                                                         'url': 'str'})\n\nhas_pdf_json = metadata[pd.notna(metadata['pdf_json_files'])].reset_index(drop=True)\nonly_pmc_json = metadata[pd.isna(metadata['pdf_json_files']) & pd.notna(metadata['pmc_json_files'])].reset_index(drop=True)\n\ndef semi_colon_split(string):\n    return string.split(sep=';')\n\nhas_pdf_json['pdf_json_files'] = has_pdf_json['pdf_json_files'].apply(semi_colon_split)\nonly_pmc_json['pmc_json_files'] = only_pmc_json['pmc_json_files'].apply(semi_colon_split)\n\nclass JsonReader:\n    def __init__(self,file_path):\n        with open(file_path) as file:\n            \n            article = json.load(file)\n            \n            intro_text = []\n            body_text = []\n            results_discussion_text = []\n            for section in article['body_text']:\n                                \n                #identifying introduction sections\n                intro_keys = ['introduction']\n                intro_keys_filter = []\n                for key in intro_keys:\n                    intro_keys_filter.append(key in section['section'].lower())\n                    \n                #identifying disucssion\/results sections\n                discussion_keys = ['discussion', 'conclusion', 'results']\n                discussion_keys_filter = []\n                for key in discussion_keys:\n                    discussion_keys_filter.append(key in section['section'].lower())\n                \n                #compiling introduction sections\n                if any(intro_keys_filter):\n                    intro_text.append(section['text'])\n                                \n                #compiling discussion sections\n                elif (any(discussion_keys_filter) | ('in conclusion' in section['text'].lower())):\n                    results_discussion_text.append(section['text'])\n                \n                #compiling all other sections to body_text\n                else:\n                    body_text.append(section['text'])\n                \n            self.intro_text = ' '.join(intro_text)\n            self.body_text = ' '.join(body_text)\n            self.results_discussion_text = ' '.join(results_discussion_text)\n            \n#PDF JSON file parsing\n            \nroot_path = '..\/input\/CORD-19-research-challenge\/'\n\npdf_body_text = []\npdf_intro_text = []\npdf_results_discussion_text = []\n\nfor paths in has_pdf_json.pdf_json_files:\n    file_reader = JsonReader(root_path + paths[0])\n    pdf_intro_text.append(file_reader.intro_text)\n    pdf_body_text.append(file_reader.body_text)\n    pdf_results_discussion_text.append(file_reader.results_discussion_text)\n    \nhas_pdf_json['intro'] = pdf_intro_text\nhas_pdf_json['body_text'] = pdf_body_text\nhas_pdf_json['results_discussion'] = pdf_results_discussion_text\n\n#PMC JSON file parsing\n\npmc_body_text = []\npmc_intro_text = []\npmc_results_discussion_text = []\n\nfor paths in only_pmc_json.pmc_json_files:\n    file_reader = JsonReader(root_path + paths[0])\n    pmc_intro_text.append(file_reader.intro_text)\n    pmc_body_text.append(file_reader.body_text)\n    pmc_results_discussion_text.append(file_reader.results_discussion_text)\n    \nonly_pmc_json['intro'] = pmc_intro_text\nonly_pmc_json['body_text'] = pmc_body_text\nonly_pmc_json['results_discussion'] = pmc_results_discussion_text\n\n#Combining the PMC and PDF features together\n\nfull_articles = pd.concat([has_pdf_json, only_pmc_json], axis = 0)\nfull_articles = full_articles.drop_duplicates(subset='cord_uid', keep='first').reset_index(drop=True)\nfull_articles.loc[pd.isna(full_articles.title), 'title'] = ''\nfull_articles.loc[pd.isna(full_articles.abstract), 'abstract'] = ''\n\ndef remove_multiple_spaces(string):\n    return re.sub(r'\\s+', ' ', string)\n\nfull_text = full_articles.title + ' ' + full_articles.abstract + ' ' + full_articles.intro + \\\n            ' ' +  full_articles.body_text + ' ' + full_articles.results_discussion\nfull_text = full_text.apply(remove_multiple_spaces)\nfull_text = full_text.str.strip()\n\nfull_articles['full_text'] = full_text\n\n#If no results\/discussion section was found, then the last 6 sentences of the paper were used instead.\n\nfor i in range(len(full_articles)):\n    if full_articles.loc[i, 'results_discussion'] == '':\n        full_text = full_articles.loc[i, 'full_text']\n        sentences = sent_tokenize(full_text)\n        last = sentences[-5:]\n        full_articles.at[i, 'results_discussion'] = ' '.join(last)\n        \n#If no intro section is found, the first 6 sentences of the paper are used.\n        \nfor i in range(len(full_articles)):\n    if full_articles.loc[i, 'intro'] == '':\n        full_text = full_articles.loc[i, 'full_text']\n        sentences = sent_tokenize(full_text)\n        first = sentences[:6]\n        full_articles.at[i, 'intro'] = ' '.join(first)\n        \nfull_articles.to_csv('full_articles_r2.csv', index=False)","3caf2bd1":"class KeywordFilter():\n    \"\"\"\n    ***\n\n    Class that returns a df containing the rows in which the specified columns\n    (which are assumed to contain strings) contain any one of the keywords.\n    Also returns a dataframe containing the amount of articles each keyword is\n    contained in.\n \n    Param: df - A DataFrame containing columns of text\n        keywords - A list of the keywords being looked for.\n        columns - A list of column names where the keyword will be \n                  searched for.\n\n    ***\n    \"\"\"\n    def __init__(self, df, keywords, columns):\n        \n        self.df = df.copy(deep=True)\n        self.keywords = keywords\n        self.columns = columns\n        \n    # Helper functions that indicates if the indicated columns \n    #    of a row contain the keyword parameter.\n    #\n    # Param: keyword - The keyword being looked for.\n    #\n    # Returns: A series of booleans.\n\n    def contains_keyword(self, keyword: str):\n        keyword_tags = ([False] * len(self.df))\n        for c in self.columns:\n            keyword_tags = keyword_tags \\\n                | self.df[c].str.lower().str.contains(keyword, na=False)\n        return keyword_tags\n\n    def transform(self):\n        \"\"\"\n        Function that filters out rows that do not contain any of the\n        keywords in the specified columns.\n      \n        Returns: The original DataFrame with the added column.\n        \"\"\"\n        tag = 'TAG'\n        counts = {}\n        #set all rows false\n        self.df[tag] = False\n        for key in self.keywords:\n            key_filter = self.contains_keyword(key)\n            counts[key] = sum(key_filter)\n            #The rows in key filter marked True will indicate that row in df to True\n            self.df.loc[key_filter, tag] = True\n        self.df = self.df[self.df[tag]]\n        self.df = self.df.drop(columns=['TAG'])\n        return self.df, pd.Series(counts)","55953363":"df = full_articles[['cord_uid','doi', 'url', 'journal',\n                    'publish_time', 'title','abstract',\n                    'intro', 'body_text', 'results_discussion',\n                    'full_text']]\n\ncovid19_keywords =['sars-cov-2', 'covid-19', '2019-ncov', \n                    'novel-coronavirus',\n                    'coronavirus 2019','wuhan pneumonia',\n                    '2019ncov', 'covid19',\n                    'sarscov2', 'coronavirus-2019']\n\ncovid19_filter = KeywordFilter(df=df, columns=['abstract'], keywords=covid19_keywords)\ncovid19_articles, covid19_counts = covid19_filter.transform()","f91cea9f":"covid19 = covid19_articles.dropna(subset=['title', 'abstract'])\ncovid19['title_abstract'] = covid19.title + ' ' + covid19.abstract","c40d8f41":"hypercoag_filter = KeywordFilter(df=covid19, columns=['title_abstract'], \n                                 keywords=['anticoagulant therapy', 'treatment',\n                                           'hypercoagulable state', 'thrombophilia', \n                                           'anticoagulation'])\nhypercoag_articles, _ = hypercoag_filter.transform()","da54429f":"novel_therapy_filter = KeywordFilter(df=covid19, columns=['title_abstract'], \n                                     keywords=['novel therapeutics', 'clinical testing', \n                                               'treatment', 'clinical trial', 'medicine'])\nnovel_therapy_articles, _ = novel_therapy_filter.transform()","6b1b160e":"def ngram(wordlist, n):\n    output = []\n    for i in range(len(wordlist) - (n-1)):\n        output.append(' '.join(wordlist[i:i+n]))\n    return output\n\ndef clean(string):\n    temp = re.sub(r'[^\\w\\s]', '', string)\n    temp = re.sub(r'\\b\\d+\\b', '', temp)\n    temp = re.sub(r'\\s+', ' ', temp)\n    temp = re.sub(r'^\\s', '', temp)\n    temp = re.sub(r'\\s$', '', temp)\n    return temp.lower()\n\ndef bm25_rank(df, text_column, query):\n    data = df.copy(deep=True)\n    \n    # Clean articles to train the BM25 model\n    clean_text = data[text_column].apply(clean)\n    tokens = clean_text.apply(word_tokenize)\n    two_grams = tokens.apply(ngram, args=(2,))\n    tok_articles_list = list(tokens + two_grams)\n\n    # BM25 model\n    ranker = bm25.BM25(tok_articles_list)\n    ranks = ranker.get_scores(query)\n    \n    data['bm25_rank'] = ranks\n    return data","0b1f48fa":"hypercoag_articles = bm25_rank(hypercoag_articles, \n                               'title_abstract', \n                               ['anticoagulant therapy', \n                                'treatment', \n                                'hypercoagulable state', \n                                'thrombophilia', \n                                'anticoagulation'])\n\nhypercoag_articles.hist(column='bm25_rank')\nhypercoag = hypercoag_articles.sort_values(by='bm25_rank', ascending=False).head(125)","b26a7f2f":"hypercoag.head()","390ed41c":"novel_therapy_articles = bm25_rank(novel_therapy_articles, \n                                   'title_abstract', \n                                   ['novel therapeutics', \n                                    'clinical testing', \n                                    'treatment', \n                                    'clinical trial', \n                                    'medicine'])\n\nnovel_therapy_articles.hist(column='bm25_rank')\nnovel_therapy = novel_therapy_articles.sort_values(by='bm25_rank', ascending=False).head(125)","0088b128":"novel_therapy.head()","465b91c3":"alphabet = list(string.ascii_lowercase)\n\nlinks = []\nfor letter in alphabet:\n    links.append(f'https:\/\/druginfo.nlm.nih.gov\/drugportal\/drug\/names\/{letter}')\n    \npages = []\nfor link in links:\n    page = r.get(link)\n    pages.append(page.text)\n    time.sleep(3)\n    \ndrug_tables = []\nfor p in pages:    \n    parser = BeautifulSoup(p, 'html.parser')\n    tables = parser.find_all('table')\n    drug_tables.append(tables[2])\n    \ndrug_names = []\nfor table in drug_tables:\n    a_refs = table.find_all('a')\n    for a in a_refs:\n        drug_names.append(a.string)\n        \ndrug_names.append('oseltamivir') #adding other relevant drugs by hand\ndrug_names.append('lopinavir') #adding other relevant drugs by hand\n\nnih_drug_names = pd.DataFrame({'drug_name':drug_names})\nnih_drug_names.drug_name = nih_drug_names.drug_name.str.lower()\nnih_drug_names.to_csv('nih_drug_names.csv', index=False)","cf315d8a":"nih_drug_names","c3f74eee":"drug_names = nih_drug_names.drug_name\n\ndef find_drugs(df):\n    mentioned_drugs = []\n    for article in df.iterrows():\n        article_drugs = ''\n        for drug in drug_names:\n            if re.search(fr'([\\W\\b])({drug})([\\b\\W])', article[1]['title_abstract'].lower()) != None:\n                article_drugs = article_drugs + drug + ';'\n        if (len(article_drugs) > 0):\n            article_drugs = article_drugs[:-1]\n        mentioned_drugs.append(article_drugs)\n    return mentioned_drugs","16f961a4":"hypercoag_drugs = find_drugs(hypercoag)\nhypercoag['mentioned_drugs'] = hypercoag_drugs\nhypercoag.head()","929a4b59":"novel_drugs = find_drugs(novel_therapy)\nnovel_therapy['mentioned_drugs'] = novel_drugs\nnovel_therapy.head()","80774929":"def remove_stopwords(words):\n    stop_words = stopwords.words('english')\n    clean = [word for word in words if word not in stop_words]\n    return clean\n\nstudy_type_adjs = ['cohort', 'observational', 'clinical', \n              'randomized', 'open-label', 'control',\n              'case', 'meta-analysis', 'systematic',\n              'narrative', 'literature', 'critical',\n              'retrospective', 'controlled', 'non-randomized',\n              'secondary', 'rapid', 'double-blinded',\n              'open-labelled', 'concurrent', 'pilot', \n              'empirical', 'retrospective', 'single-center',\n              'collaborative', 'case-control']\nstudy_type_nouns = ['study', 'trial', 'report', 'review',\n                   'analysis', 'studies']\n\ndef find_study_types(df):\n    study_types = []\n    for article in df.iterrows():\n        title = article[1].title.lower()\n        clean_title = clean(title)\n        title_tokens = word_tokenize(clean_title)\n        title_tokens = remove_stopwords(title_tokens)\n        study_type = ''\n        study_type_name = False\n\n        article_study_type = []\n        for word in title_tokens:\n            if word in study_type_adjs:\n                study_type = study_type + word + ' '\n                study_type_name = True\n\n            elif word in study_type_nouns:\n                study_type = study_type + word + ' '\n                study_type_name = True\n\n            if (study_type_name == False):\n                study_type = ''\n\n        study_types.append(study_type[:-1])\n\n    return study_types","5ad51157":"hypercoag_study_types = find_study_types(hypercoag)\nhypercoag['study_types'] = hypercoag_study_types\nhypercoag.head()","aa4cd0b6":"novel_study_types = find_study_types(novel_therapy)\nnovel_therapy['study_types'] = novel_study_types\nnovel_therapy.head()","630c3fb6":"severity_types = ['severe', 'critical', 'icu', 'mild']\n\ndef find_severities(df):\n    severity_of_disease = []\n    for abstract in df.abstract:\n        text = re.sub('Severe acute respiratory', 'sar', abstract)\n        severities = []\n        tokens = word_tokenize(text)\n        for severity_type in severity_types:\n            if severity_type in tokens:\n                severities.append(severity_type)\n        severity_of_disease.append(';'.join(severities))\n    return severity_of_disease","d2a51339":"hypercoag_severities = find_severities(hypercoag)\nhypercoag['severity_of_disease'] = hypercoag_severities\nhypercoag.head()","034c3adc":"novel_severities = find_severities(novel_therapy)\nnovel_therapy['severity_of_disease'] = novel_severities\nnovel_therapy.head()","40c330b0":"page = r.get('https:\/\/www.ef.edu\/english-resources\/english-grammar\/numbers-english\/')\nparser = BeautifulSoup(page.content, 'html.parser')\ntables = parser.find_all('table')\ncardinal_numbers = pd.read_html(str(tables[0]))[0].loc[0:37, \"Cardinal\"]\n\ndef find_sample_sizes(df):\n    patient_num_list = []\n    for abstract in df.abstract:\n        matches = re.findall(r'(\\s)([0-9,]+)(\\s|\\s[^0-9\\s]+\\s)(patients)', abstract)\n        num_patients = ''\n        for match in matches:\n            num_patients = num_patients + ''.join(match[1:]) + ';'\n        for number in cardinal_numbers:\n            cardinal_regex_search = re.search(fr'({number})(\\s)(patients)', abstract)\n            if cardinal_regex_search != None:\n                num_patients = num_patients + cardinal_regex_search[0] + ';'\n        num_patients = num_patients[:-1]\n        patient_num_list.append(num_patients)\n    return patient_num_list","a3c2d9e1":"hypercoag['sample_size'] = find_sample_sizes(hypercoag)\nhypercoag.head()","aada2c62":"novel_therapy['sample_size'] = find_sample_sizes(novel_therapy)\nnovel_therapy.head()","63a0ddee":"def discussion_excerpts(df):\n    results_table = pd.DataFrame(columns=['date','study', 'study_link', 'journal', \n                        'study_type', 'therapeutic_method', 'sample_size', \n                        'severity_of_disease', 'conclusion_excerpt', 'primary_endpoints', \n                        'clinical_imporovement', 'added_on'])\n    for article in df.iterrows():\n        \n        # Code here looks for the mention of drugs\n        drugs_with_abbrv = []\n        drugs = article[1].mentioned_drugs.split(sep=',')\n        for i in range(len(drugs)):\n            abbrv=''\n            # This searches for the mentioned drug names that are \n            # followed by abbreviations that a contained in parethesis.\n            # Ex. drug (xy-z)\n            if (re.search(fr'({drugs[i]})(\\s)(\\()([a-z-]+)(\\))', article[1].full_text.lower())) != None:\n                source = re.search(fr'({drugs[i]})(\\s)(\\()([a-z-]+)(\\))', article[1].full_text.lower())\n                abbrv = source[4]\n            drugs_with_abbrv.append([drugs[i], abbrv])\n\n        sentences = sent_tokenize(article[1].results_discussion)\n        for drug in drugs_with_abbrv:\n            excerpts = ''\n            for sentence in sentences:\n                # If the drug name has an abbreviation, sentences \n                # in the results\/discussion that contain it are added\n                # to a list of excerpts.\n                if drug[1] != '':\n                    if (re.search(fr'([\\W\\b])({drug[0]})([\\b\\W])', sentence.lower()) != None) | \\\n                        (re.search(fr'([\\W\\b])({drug[1]})([\\b\\W])', sentence.lower()) != None):\n                        excerpts += sentence + '\\n'\n                # If the drug does not have an abbreviation, then\n                # only the drug name is searched for.\n                if drug[1] == '':\n                    if (re.search(fr'([\\W\\b])({drug[0]})([\\b\\W])', sentence.lower()) != None):\n                        excerpts += sentence + '\\n'\n            if excerpts != '':\n                drug_row = {'date':[article[1].publish_time],'study':[article[1].title], 'study_link':[article[1].url], \n                            'journal':[article[1].journal], 'study_type':[article[1].study_types], 'therapeutic_method':[f'{drug[0]}'], \n                            'sample_size':[''], 'severity_of_disease':[article[1].severity_of_disease], 'conclusion_excerpt':[excerpts], \n                            'primary_endpoints':[''], 'clinical_imporovement':[''], 'added_on':['']}\n                results_table = pd.concat([results_table, pd.DataFrame(drug_row)], axis = 0)\n                                \n    return results_table","18b95384":"hypercoag = hypercoag[hypercoag.mentioned_drugs != '']\nresults_table_hypercoag = discussion_excerpts(hypercoag)\nresults_table_hypercoag.reset_index(drop=True).head()","247d1b75":"results_table_hypercoag.to_csv('results_table_hypercoag.csv', index=False)","67ca020f":"novel_therapy = novel_therapy[novel_therapy.mentioned_drugs != '']\nresults_table_novel = discussion_excerpts(novel_therapy)\nresults_table_novel.reset_index(drop=True).head()","5a6e339c":"results_table_novel.to_csv('results_table_novel_therapy.csv', index=False)","3f71cf0d":"from IPython.display import Image\nImage('..\/input\/cv19-r2-dash\/cv19_dash_pt1.png')","ead0573f":"Image('..\/input\/cv19-r2-dash\/cv19_dash_pt2.png')","35e722fb":"## 2.4 Number of Patients\nThe abstracts of papers were searched for phrases that resembled '# patients', '#  icu patients', '# hospitalized patients', etc. These types of phrases were extracted and added to a new column.","f9b8ccdf":"Contributions made to this notebook by Colin Lagator (Data Analysis), Emily Morgan (Biomedical\/Health SME)","5ded7474":"## 1.3 Finding Subsets of Articles for Each Task\nA subset of articles was found for each of the questions related to this task. These subsets were found using the same keyword filter as above.","6365ac79":"## 1.1 Processing Raw Data\n\nThe data was processed into one CSV called full_articles_r2.csv. Only articles that had a PMC or PDF JSON associated with them were kept. The JSON file processing was based on work done in [this notebook](https:\/\/www.kaggle.com\/ivanegapratama\/covid-eda-initial-exploration-tool). For consistency, if an article had a PDF JSON file, the PDF was used. Otherwise, the PMC JSON file was used.\n\nThree features were extracted from the JSON files; the introduction sections, the main body text, and the discussion section. The intro and discussion sections were identified with the section titles, all other text was taken as the body text. If no results section was identified, then the last 6 sentences of the body text were used instead. If no intro section was found, the first 6 sentences of the body text were used.","154b4b60":"## 2.5 Conclusion Excerpts\nFrom the work above, each article is tagged with the drugs that it mentioned along with other details. In this section, excerpts from the conclusion section are found which contain any mentioned drug. For each drug mentioned in a paper, a row is created in the summary table. If no excerpts are found, no summary row is created.","b4125bd9":"## 2.1 Finding Mentioned Treatments (Drugs)\nThe different drugs mentioned in research papers were found and treated as possible treatment methods.","d37c789a":"### 1.3.2 BM25 Ranking\nThe keywords used above were also used with BM25 ranking. The articles in the subsets were ranked according to their relevance to these keywords. It can be seen in the two histograms below that a small number of articles hold the most relevance. Therefore, the top 125 most relevant articles were taken from each task subset. These top 125 articles approximately represent the 90th percentile of the ranked articles.","63dcd767":"*Click CODE to Reveal Code for Data Processing*","cc83b3a6":"*Click CODE to Reveal Regex Feature Extraction Code*","c0b4afc2":"# 2. Finding Summary Table Features Using Regex and Webscraping","b5e53c27":"### 2.1.2 Tagging Mentioned Drugs in Articles\nNow that a list of known drugs has been found, it can be used to search articles for mentions of drugs. A new column is created that contains all drugs mentioned by an article separated by semicolons.","69c8a6b1":"----------------","969d2a3d":"#### BM25 Ranking with Novel Treatments and Drug Trial Keywords","cc8efb0e":"*Click CODE to Reveal Regex Feature Extraction Code*","1fefb48a":"#### Articles with Novel Treatments and Drug Trial Keywords","141b86a5":"## 1.2 Creating a COVID-19 Subset of Articles\nOnly articles that were associated with COVID-19 were used in the analysis. These article were identified using a keyword filter. The keyword filter class is based on [this notebook](https:\/\/www.kaggle.com\/ajrwhite\/covid-19-thematic-tagging-with-regular-expressions\/notebook).","b63c3b89":"# Creating Summary Tables Using Regex and Webscraping\n## CORD-19 Challenge\n#### Contributions made by employees of Booz Allen Hamilton.\n### Task: Therapeutics, Interventions, and Clinical Studies\n\n1. Data Cleaning and Subsetting\n    - Processing Raw Data\n    - Covid-19 Subset\n    - Tasks Subsets\n2. Finding Summary Table Features\n    - Mentioned Treatments\n    - Study Types\n    - Severity of Disease\n    - Number of Patients\n    - Conclusion Excerpts\n3. Completing Table with Dashboard\n    - Dashboard made with Plotly's Dash library\n\nThis task focuses on two main questions; What is the best method to combat the hypercoagulable state seen in COVID-19? What is the efficacy of novel therapeutics being tested currently? The task requests that summary tables be provided for each question which contain certain features. This noteboook details a process of finding the summary tables for the two questions above. This process includes keyword searching, regex, webscraping, and an interactive dashboard. Using all these components, the authors feel that useful summary tables can be found. \n\nThe summary tables are outputted as *results_table_hypercoag.csv* and *results_table_novel_therapy.csv*.","ef83ef4b":"*Click CODE to Reveal Regex Feature Extraction Code*","9183a109":"### What is the efficacy of novel therapeutics being tested currently?","4e62ca3d":"### Necessary Imports","0db20240":"## 2.2 Study Types\nA list of possible study types was found by looking through the titles of papers. The study types were split into adjectives and nouns. For example, the adjective 'observational' and the noun 'study' combine for 'observational study'. Another adjective could be added to make 'retrospective observational sutdy'. Combinations of words like this were searched for in the titles of papers.","e3e97007":"# 1. Data Cleaning and Subsetting","e5d65400":"#### BM25 Ranking with Hypercoagulable State Keywords","be2bdacd":"*Click CODE to Reveal Keyword Filter Code*","f4f48a06":"### 1.3.1 Keyword Filtering","ecee314a":"*Click CODE to Reveal BM25 Ranking Code*","cad413cb":"#### Articles with Hypercoagulable State Keywords","68e89c3a":"*Click CODE to Reveal Regex Feature Extraction Code*","8af2d4e5":"*Click CODE to Reveal Regex Feature Extraction Code*","a352389a":"### 2.1.1 Web Scraping Drug Names\nTo identify drugs mentioned in articles, a list of known drugs was needed. The [NIH website](https:\/\/druginfo.nlm.nih.gov\/drugportal\/drug\/names) was scraped to obtain a list of drugs. The library BeautifulSoup4 was used to do this.","30f8ce23":"## 2.3 Severity of Disease\nFour keywords associated with disease severtiy were found in the abstracts of papers. These keywords were; severe, critical, icu, and mild. Papers are tagged by which keywords they contain. Note: The prescence of severe in Severe Acute Respiratory Disease, was not counted in this search.","7efae55f":"# 3 Interactive Dashboard\n\nMany of the summary table features can be found automatically using regex. However, some of the the features require insight from a researcher or subject matter expert. These being the *Primary Endpoints* of an article and an indication of *Clinical Improvement (Yes or No)*. An interactive dashboard is a potential method to complete the target tables. An example was made using Plotly's Dash library. This example dashboard allows a researcher to fill in the missing features, add to or change existing features, and export their results to a CSV file.\n\nThe code for the dashboard below is included in the input data under **cv19-r2-plotly-dash\/dash_app.py.**","b53e1458":"### What are the best methods to combat the hypercoagulable state associated with COVID-19?"}}