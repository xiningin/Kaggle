{"cell_type":{"fd6a0508":"code","ccdb6521":"code","6ee2bffa":"code","c1eb1d6e":"code","aa93fb4c":"code","70ea0969":"code","8587c6e5":"code","01408043":"code","89fa2e41":"code","633ee58d":"code","80ff62f6":"code","a0de8f99":"code","a1af92ff":"code","5f0d9359":"code","fd54b876":"code","dcc97f7a":"code","fd11120d":"code","9e013cad":"code","52ccc1da":"code","f9370be7":"code","22c0daa9":"code","698f6994":"code","eaf5c6c3":"code","5f91bf56":"code","ebe7afd9":"code","1dc75f94":"code","d21f3f6c":"code","c4811871":"code","a95de627":"code","9a579ea8":"code","d317533b":"code","d9662107":"code","5c44d3cc":"code","fe599534":"code","6612765c":"code","4b845e8d":"code","f270609e":"code","b9366a24":"code","2be4d61d":"code","e2cdacd5":"code","bba26ea6":"code","b311520b":"code","d38a8158":"code","1c876e0e":"code","8ca58db2":"code","1bbf5bb5":"code","7b093121":"code","fd08eec1":"code","ef43d37a":"code","bbc7748c":"code","a4625318":"code","6dff9433":"code","b669ab9e":"code","fe4f4750":"code","bac10578":"markdown","9e08b129":"markdown","8aa8d835":"markdown","59c1aa73":"markdown","e86afb86":"markdown","9253d02c":"markdown","f573f31e":"markdown","2e82ae46":"markdown","777481e6":"markdown","9068b3cb":"markdown","f3987193":"markdown","25e06907":"markdown","4c19423c":"markdown","d253258b":"markdown","594b16df":"markdown","411c7fd4":"markdown","8b6ae9a8":"markdown","cc57ffb9":"markdown","07fbed2a":"markdown","357303f1":"markdown"},"source":{"fd6a0508":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ccdb6521":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.style as style\n# style.use('fivethirtyeight')\nimport seaborn as sns","6ee2bffa":"# load first 10*5 rows of train data\nfolder = '..\/input\/riiid-test-answer-prediction\/'\ntrain = pd.read_csv(folder + 'train.csv', low_memory=False, nrows=10**5,\n                       dtype={'row_id': 'int64', 'timestamp': 'int64', 'user_id': 'int32', 'content_id': 'int16', 'content_type_id': 'int8',\n                              'task_container_id': 'int16', 'user_answer': 'int8', 'answered_correctly': 'int8', 'prior_question_elapsed_time': 'float32',\n                             'prior_question_had_explanation': 'boolean'})\n\n","c1eb1d6e":"train.info()","aa93fb4c":"# drop the lecture and first question bundle (Typically the first several questions a user sees were part of an onboarding diagnostic test where they did not get any feedback.)\ntrain = train.dropna()\n\n# covert prior_question_had_explanation data type False,True to 0,1\n# train['prior_question_had_explanation'] = train.prior_question_had_explanation.astype('int8')","70ea0969":"# user accuracy and # of question answered\nuser_correct = train.groupby('user_id')['answered_correctly'].agg(user_sum = 'sum', \n                user_mean = 'mean').reset_index()\n# total time spent on riiid APP of each user\nuser_time_total = train.groupby('user_id')['timestamp'].agg(user_time_total = 'max')\n\n# merge\nuser_df = user_correct.merge(user_time_total, on = 'user_id')","8587c6e5":"# only select the needed columns\ntrain = train[['user_id', 'content_id', 'answered_correctly','prior_question_elapsed_time', 'prior_question_had_explanation']]","01408043":"# load question data\nquestion = pd.read_csv(folder + 'questions.csv')","89fa2e41":"# merge question to train\ntrain = train.merge(question[['question_id', 'part']],\n            left_on ='content_id', right_on = 'question_id', how = 'left')\\\n            .drop('question_id', axis = 1)","633ee58d":"# get question mean and part mean\nquestion_mean = train.groupby('content_id')['answered_correctly']\\\n                .agg( question_mean = 'mean', question_sum = 'sum').reset_index()\npart_mean = train.groupby('part')['answered_correctly'].agg( part_mean = 'mean').reset_index()","80ff62f6":"# merge question mean, part mean to train\ntrain_df = train.merge(question_mean,on ='content_id', how = 'left')\\\n            .drop('content_id', axis = 1)\ntrain_df = train_df.merge(part_mean, on = 'part', how ='left')\\\n            .drop('part', axis = 1)","a0de8f99":"# merge user_df to train\ntrain_df = train_df.merge(user_df, on = 'user_id', how ='left')\\\n            .drop('user_id', axis = 1)","a1af92ff":"train_df.shape # same rows as original train data","5f0d9359":"train_df.head()","fd54b876":"# feature_names = list(train_df.columns)[1:]\n# for i in feature_names:\n#     print(i)\n#     train_df[i] = train_df[i] \/ train_df[i].std()","dcc97f7a":"# plot histogram to show distribution of features by answered_correctly categories\ndef plot_histogram_answered_correctly(x,y):\n    plt.hist(list(x[y==0]), alpha = 0.5, label='answered_correctly = 0')\n    plt.hist(list(x[y==1]), alpha = 0.5, label='answered_correctly = 1')\n    plt.title(\"Histogram of '{var_name}'\".format(var_name = x.name))\n    plt.xlabel('Value')\n    plt.ylabel('Frequency')\n    plt.legend()\n    plt.show()","fd11120d":"plot_histogram_answered_correctly(train_df['question_mean'], train_df['answered_correctly'])","9e013cad":"# define a func to find outliers\ndef find_outliers(x):\n    q1 =  data.quantile(q=.25)\n    q3 =  data.quantile(q=.75)\n    iqr = q3-q1\n    floor = q1 - 1.5*iqr\n    ceiling = q3 + 1.5*iqr\n    outlier_indices = list(x.index[(x < floor)|(x > ceiling)])\n    outlier_values = list(x[outlier_indices])\n    \n    return outlier_indices, outlier_values","52ccc1da":"# find outliers in each column\n# try different features\noutlier_index = []\n\nfor c in train_df.columns:\n    data = train_df[c]\n    outlier_indices, outlier_values = find_outliers(data) \n    outlier_index.extend(outlier_indices)\n    print('Ther are ', len(outlier_values), ' outliers in', c)","f9370be7":"# interpret the outliers\nprint('Ther are a total of {} outliers in train_df, accounting for {}% of the total train_df data ({}).'\\\n      .format(len(outlier_index),round(100*(len(outlier_index)\/len(train_df)),2), len(train_df)))","22c0daa9":"# remove outliers from feature dataframe\ncleaned_train_df = train_df.drop(train_df.index[outlier_index])","698f6994":"# check the shape \ncleaned_train_df.shape","eaf5c6c3":"train_df.shape","5f91bf56":"# split data into train and test sets\nfrom sklearn.model_selection import train_test_split\n\n# get X, y data\ndata = cleaned_train_df\nX = data.drop('answered_correctly', axis=1)\ny = data.answered_correctly\n\n# Split the dataset\nX_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=.3, random_state = 0)","ebe7afd9":"X.shape","1dc75f94":"# with such a large of features(9 in total) an cause overfitting and also slow computing\n# use feature selection to select the most important features\nimport sklearn.feature_selection","d21f3f6c":"def select_features(X, X_train, y_train, k):\n    if k > len(X):\n        print('K must less than the lence of input X')\n    select = sklearn.feature_selection.SelectKBest(k=k)\n    selected_features = select.fit(X_train, y_train)\n    indices_selected = selected_features.get_support(indices = True)\n    colnames_selected = [X.columns[i] for i in indices_selected]\n    return colnames_selected","c4811871":"colnames_selected = select_features(X, X_train, y_train, 3)","a95de627":"colnames_selected","9a579ea8":"# use the selected feature to fit model\nX_train_selected = X_train[colnames_selected]\nX_test_selected = X_test[colnames_selected]","d317533b":"# Perform Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train_selected, y_train)\n\n# Make prediction using the model\ny_pred = log_reg.predict(X_test_selected) ","d9662107":"# quickly visualize results\nfrom sklearn import metrics\ndef ROC_Curve(log_model, X_test, y_test):\n    #define metrics\n    y_pred_proba = log_model.predict_proba(X_test)[::,1]\n    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n\n    #create ROC curve\n    plt.plot(fpr,tpr,label=\"AUC=\"+str(auc))\n    plt.legend(loc=4)\n    plt.show()","5c44d3cc":"ROC_Curve(log_reg, X_test_selected, y_test)","fe599534":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, roc_auc_score\n\ndef find_model_perf(X_train, X_test, y_train, y_test):\n    # Perform Logistic Regression\n    log_reg = LogisticRegression()\n    log_reg.fit(X_train, y_train)\n    y_pred = [x[1] for x in log_reg.predict_proba(X_test)]\n    auc = roc_auc_score(y_test, y_pred)  ## ?? roc_auc_score vs accuraccy\n    return auc","6612765c":"auc_processed = find_model_perf(X_train_selected, X_test_selected, y_train, y_test)\nprint(auc_processed)","4b845e8d":"from sklearn.metrics import accuracy_score, recall_score, precision_score\n\ndef estimate_logreg_scores(y_test, y_pred):\n\n    print('\\nLogistic Regression Report')\n    print('\\nUsing 0.3 as test size:')\n    print('Accuracy = {:.5f}'.format(accuracy_score(y_test, y_pred)))\n    print('Precision = {:.5f}'.format(precision_score(y_test, y_pred)))\n    print('Recall = {:.5f}'.format(recall_score(y_test, y_pred)))","f270609e":"estimate_logreg_scores(y_test, y_pred)","b9366a24":"from sklearn.metrics import plot_confusion_matrix\n\ndef confusion_matrix_plot(log_model,  X_test, y_test):\n    plot_confusion_matrix(log_model, X_test, y_test,\n                             cmap=plt.cm.Blues);","2be4d61d":"# Display the Confusion Matrix\nconfusion_matrix(y_test, y_pred)","e2cdacd5":"confusion_matrix_plot(log_reg, X_test_selected, y_test)","bba26ea6":"# interpret feature coefficients \ncoefficients = np.hstack((log_reg.intercept_, log_reg.coef_[0]))\ninterpret_result = pd.DataFrame(data={'variable': ['intercept'] + list(X_train_selected.columns), \n                   'coefficient': coefficients}).sort_values('coefficient', ascending = False)\ninterpret_result","b311520b":"# get X, y data\ndata = train_df\nX_outlier = data.drop('answered_correctly', axis=1)\ny_outlier = data.answered_correctly\n\n# Split the dataset\nX_train_outlier, X_test_outlier, y_train_outlier, y_test_outlier = train_test_split(\n       X_outlier, y_outlier, test_size=.3, random_state = 0)","d38a8158":"X_outlier.shape","1c876e0e":"colnames_selected_outlier = select_features(X_outlier, X_train_outlier, y_train_outlier, 2)","8ca58db2":"X.head()","1bbf5bb5":"# colnames_selected_outlier","7b093121":"colnames_selected_outlier = ['user_mean','question_mean','question_sum','part_mean',\n       'prior_question_had_explanation']","fd08eec1":"# use the selected feature to fit model\nX_train_selected_outlier = X_train_outlier[colnames_selected_outlier]\nX_test_selected_outlier = X_test_outlier[colnames_selected_outlier]","ef43d37a":"# Perform Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlog_reg_outlier = LogisticRegression()\nlog_reg_outlier.fit(X_train_selected_outlier, y_train_outlier)\n\n# Make prediction using the model\ny_pred_outlier = log_reg_outlier.predict(X_test_selected_outlier) ","bbc7748c":"\nROC_Curve(log_reg_outlier, X_test_selected_outlier, y_test_outlier)","a4625318":"find_model_perf(X_train_selected_outlier, X_test_selected_outlier, y_train_outlier, y_test_outlier)","6dff9433":"estimate_logreg_scores(y_test_outlier, y_pred_outlier)","b669ab9e":"# Display the Confusion Matrix\nconfusion_matrix(y_test_outlier, y_pred_outlier)","fe4f4750":"print('Three features with outliers')\nconfusion_matrix_plot(log_reg_outlier, X_test_selected_outlier, y_test_outlier)\n","bac10578":"### Build model using cleaned data (without outliers)","9e08b129":"# Feature outliers detection\n## ****Detect outliers\nFirst, let's define a function to find all the possible outliers.","8aa8d835":"#### Feature coefficients","59c1aa73":"# Machine Learning\n## Feature selection and model building\n### Split data to train and test data","e86afb86":"### Evaluate model\n#### Evaluate Model --- ROC Curve\nNow, we can plot the ROC (Receiver Operating Characteristic) Curve which displays the percentage of true positives predicted by the model as the prediction probability cutoff is lowered from 1 to 0.\n\nThe higher the AUC (area under the curve), the more accurately our model is able to predict outcomes:","9253d02c":"#### Evaluate Model --- Confusion Matrix","f573f31e":"### By question\n1. mean accuracy of each question\n2. number of questions answered\n3. mean accuracy of question part(section of the TOEIC test)\n","2e82ae46":"#### Evaluate Model --- ROC & AUC Score","777481e6":"### IQR (inter quartile range)\nEquation: InterQuartile Range = Q3 - Q1\n\nThe first quartile, denoted Q1, is the value in the data set that holds 25% of the values below it.\n\nThe third quartile, denoted Q3, is the value in the data set that holds 25% of the values above it.\n\nOutliers and Tukey Fences: Tukey is one of th emethods fro determing outliers in a sample. It is very popular method is based on the following:\n\noutliers = below (Q1 - 1.5(Q3-Q1)) or above (Q3 + 1.5(Q3-Q1))","9068b3cb":"### Other features from train data\n1. answered_correctly\n2. prior_question_elapsed_time\n3. prior_question_had_explanation","f3987193":"### Remove outliers\nNow, let's remove all the outliers and get a clean data frame ready for next step.","25e06907":"## Select features\n\n1. collect all possible features in a dataframe.\n2. normalize feature values.\n3. try and decide feature importance.\n\nFirst, make a dataframe to hold all the values of possible features.","4c19423c":"## Make logistic regression model with outliers","d253258b":"Note: How to interpret the coefficient? need scale or standardize the numerical values?","594b16df":"### By user\n1. mean (answered_correctly)\n2. number of questions answered\n3. total time spent on APP","411c7fd4":"#### Evaluate Model --- Model Scores","8b6ae9a8":"# Normalize inputs(features) \uff1f","cc57ffb9":"# Feature engineering","07fbed2a":"### Feature distributions","357303f1":"### Feature selection"}}