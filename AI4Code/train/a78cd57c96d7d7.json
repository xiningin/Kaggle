{"cell_type":{"81662bfb":"code","efe7fe90":"code","0e3f2a7d":"code","54a0a39d":"code","ed9a984f":"code","1551026c":"code","905563f9":"code","32450a3d":"code","97708f85":"code","040d0822":"code","384b69a6":"code","838c6121":"code","583cd621":"code","3b8be8a6":"code","d1288f39":"code","575756cb":"code","b05fda02":"code","dd7bbf3e":"code","5a0f9683":"code","34d79492":"markdown","532096ca":"markdown","74287a3b":"markdown","66634d2b":"markdown","5049317c":"markdown","295a8eda":"markdown","4e1d4547":"markdown","26786b47":"markdown","9923440e":"markdown","75fbb4d6":"markdown","10e08723":"markdown","68290f53":"markdown","c7527f3c":"markdown","1b826d13":"markdown"},"source":{"81662bfb":"import sys\nfrom pathlib import Path\nimport os\nfrom datetime import datetime\nimport time\nimport random\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport albumentations as A\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom sklearn.model_selection import StratifiedKFold\nfrom glob import glob\nimport pandas as pd\nimport gc\nimport warnings\nfrom scipy import ndimage\nfrom skimage.measure import find_contours\nfrom matplotlib.patches import Polygon\nimport copy\n\nwarnings.filterwarnings(\"ignore\")","efe7fe90":"# os.chdir('..\/input\/mask-rcnn')\nos.chdir('..\/input\/maskrcnn-tf2-keras')\nDATA_DIR = Path('..\/kaggle\/input\/')\nROOT_DIR = \"..\/..\/input\"\nsys.path.append(ROOT_DIR+'\/maskrcnn-tf2-keras')\nfrom mrcnn.config import Config\n\nfrom mrcnn import utils\nimport mrcnn.model as modellib\nfrom mrcnn import visualize\nfrom mrcnn.model import log","0e3f2a7d":"NUM_CATS=1\nIMAGE_SIZE=512\nclass HelmetConfig(Config):\n    NAME = \"helmet_detection\"\n    NUM_CLASSES = NUM_CATS + 1 # +1 for the background class\n    \n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 2 # a memory error occurs when IMAGES_PER_GPU is too high\n    \n    BACKBONE = 'resnet50'\n    \n    IMAGE_MIN_DIM = IMAGE_SIZE\n    IMAGE_MAX_DIM = IMAGE_SIZE    \n    IMAGE_RESIZE_MODE = 'none'\n    \n    BACKBONE_STRIDES = [4, 8, 16, 32, 64]\n    BACKBONESHAPE = (8, 16, 24, 32, 48)\n    RPN_ANCHOR_SCALES = (8,16,24,32,48)\n    RPN_TRAIN_ANCHORS_PER_IMAGE = 300\n    POST_NMS_ROIS_TRAINING = 800\n    POST_NMS_ROIS_INFERENCE = 700\n    MAX_GROUNDTRUTH_INSTANCES = 50\n    TRAIN_ROI_PER_IMAGE = 300\n    ROI_POSITIVE_RATIO = 0.33\n    DETECTION_MAX_INSTANCES = 300\n    DETECTION_MIN_CONFIDENCE = 0.7\n    DETECTION_NMS_THRESHOLD = 0.5\n    \nconfig = HelmetConfig()\nconfig.display()","54a0a39d":"# Fix overlapping masks\ndef refine_masks(masks, rois):\n    areas = np.sum(masks.reshape(-1, masks.shape[-1]), axis=0)\n    mask_index = np.argsort(areas)\n    union_mask = np.zeros(masks.shape[:-1], dtype=bool)\n    for m in mask_index:\n        masks[:, :, m] = np.logical_and(masks[:, :, m], np.logical_not(union_mask))\n        union_mask = np.logical_or(masks[:, :, m], union_mask)\n    for m in range(masks.shape[-1]):\n        mask_pos = np.where(masks[:, :, m]==True)\n        if np.any(mask_pos):\n            y1, x1 = np.min(mask_pos, axis=1)\n            y2, x2 = np.max(mask_pos, axis=1)\n            rois[m, :] = [y1, x1, y2, x2]\n    return masks, rois\n\ndef resize_image(image_path):\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_AREA)  \n    return img\n\ndef decode_rle(rle, height, width):\n    s = rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(height*width, dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape((height, width)).T\n\ndef annotations_to_mask(annotations, height, width):\n    if isinstance(annotations, list):\n        # The annotation consists in a list of RLE codes\n        mask = np.zeros((height, width, len(annotations)))\n        for i, rle_code in enumerate(annotations):\n            mask[:, :, i] = decode_rle(rle_code, height, width)\n    else:\n        error_message = \"{} is expected to be a list or str but received {}\".format(annotation, type(annotation))\n        raise TypeError(error_message)\n    return mask","ed9a984f":"class InferenceConfig(HelmetConfig):\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n    IMAGE_MIN_DIM = IMAGE_SIZE\n    IMAGE_MAX_DIM = IMAGE_SIZE    \n    IMAGE_RESIZE_MODE = 'none'\n    DETECTION_NMS_THRESHOLD = 0.5\n\ninference_config = InferenceConfig()\n\nmodel = modellib.MaskRCNN(mode='inference', \n                          config=inference_config,\n                          model_dir=\"\")\n\nmodel.load_weights(\"..\/..\/input\/maskrcnn-helmet-detection\/mask_rcnn_helmet_detection.h5\", by_name=True)","1551026c":"def predict_on_image(img, y_origin, x_origin, y_margin, x_margin):\n    img = np.array(img)[y_origin:y_origin+512,x_origin:x_origin+512]\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    result = model.detect([img])\n    r = result[0]\n    \n    x_start_box = 0\n    y_start_box = 0\n    x_end_box = 512\n    y_end_box = 512\n    if x_origin==0 and y_origin==0:\n        x_end_box = x_end_box-x_margin\n        y_end_box = y_end_box-y_margin\n    elif x_origin>0 and y_origin==0:\n        x_end_box = x_end_box-x_margin\n        y_end_box = y_end_box-y_margin\n        if x_origin<768:\n            x_start_box = x_start_box+x_margin\n    elif x_origin>0 and y_origin>0:\n        y_start_box = y_start_box+y_margin \n        x_start_box = x_start_box+x_margin\n        if x_origin<768:\n            x_end_box = x_end_box-x_margin\n    elif x_origin==0 and y_origin>0:\n        y_start_box = y_start_box+y_margin \n        x_end_box = x_end_box-x_margin\n    \n    box_list = []\n    rois_list = copy.deepcopy(r['rois'])\n\n    if r['masks'].size > 0:\n        masks = np.zeros((img.shape[0], img.shape[1], r['masks'].shape[-1]), dtype=np.uint8)\n        for m in range(r['masks'].shape[-1]):\n            masks[:, :, m] = cv2.resize(r['masks'][:, :, m].astype('uint8'), \n                                        (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)\n\n        for idx in range(masks.shape[-1]-1):\n            #maybe x and y centroids are swapped, double check\n            y_centroid, x_centroid = ndimage.measurements.center_of_mass(masks[:,:,idx])\n            if x_centroid>x_start_box and x_centroid<x_end_box and y_centroid>y_start_box and y_centroid<y_end_box:\n                roi = copy.deepcopy(rois_list[idx])\n                absolute_x_start = x_origin+roi[1]\n                absolute_x_end = x_origin+roi[3]\n                absolute_y_start = y_origin+roi[0]\n                absolute_y_end = y_origin+roi[2]\n                box_list.append(np.array([absolute_x_start, absolute_y_start, absolute_x_end, absolute_y_end]))\n\n\n        for m in range(r['masks'].shape[-1]):\n            masks[:, :, m] = cv2.resize(r['masks'][:, :, m].astype('uint8'), \n                                        (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)\n\n        y_scale = img.shape[0]\/IMAGE_SIZE\n        x_scale = img.shape[1]\/IMAGE_SIZE\n        rois = (r['rois'] * [y_scale, x_scale, y_scale, x_scale]).astype(int)\n\n        rois = np.array([list(roi) for roi in rois if roi[3]-roi[1]<40 or roi[2]-roi[0]<40])\n        masks = r['masks']\n    else:\n        masks, rois = r['masks'], r['rois']\n\n    return np.array(box_list)","905563f9":"def get_helmet_boxes(img_path):\n    img = cv2.imread(img_path)\n    y_tile_origins = [0,208]\n    x_tile_origins = [0,384,768]\n    x_margin = 64\n    y_margin = 152\n\n    rois = []\n    for y in y_tile_origins:\n        for x in x_tile_origins:\n            results = predict_on_image(img, y, x, y_margin, x_margin)\n            for result in results:\n                rois.append(result)\n    return np.array(rois)","32450a3d":"def boxes_overlap(box1, box2):\n    pixel_shift_flexibility = 7\n    if abs(box1[0]-box2[0])<pixel_shift_flexibility:\n        if abs(box1[2]-box2[2])<pixel_shift_flexibility:\n            if abs(box1[1]-box2[1])<pixel_shift_flexibility:\n                if abs(box1[3]-box2[3])<pixel_shift_flexibility:\n                    return True\n    return False\n\ndef remove_box(box_to_remove, box_list):\n    clean_box_list = []\n    for box in box_list:\n        if not (box_to_remove==box).all():\n            clean_box_list.append(box)\n    return np.array(clean_box_list)\n\ndef check_overlap(helmet_boxes):\n    clean_helmet_box_list = []\n    \n    helmet_boxes_to_scan = copy.deepcopy(helmet_boxes)\n    item_suppressed = 0\n    for idx, helmet_box in enumerate(helmet_boxes):\n        other_helmet_boxes = remove_box(helmet_box, helmet_boxes_to_scan)\n        duplicated = False\n        for other_box in other_helmet_boxes:\n            if boxes_overlap(helmet_box, other_box):\n                duplicated = True\n                item_suppressed +=1 \n                helmet_boxes_to_scan = remove_box(helmet_box, helmet_boxes_to_scan)\n        if not duplicated:\n            clean_helmet_box_list.append(helmet_box)\n            \n    return clean_helmet_box_list","97708f85":"def bbox_from_mask(img):\n    rows = np.any(img, axis=1)\n    cols = np.any(img, axis=0)\n    rmin, rmax = np.where(rows)[0][[0, -1]]\n    cmin, cmax = np.where(cols)[0][[0, -1]]\n\n    return [cmin, rmin, cmax, rmax]","040d0822":"def detect_helmets(img_path):\n    \n    helmet_boxes = get_helmet_boxes(img_path)\n    helmet_boxes = check_overlap(helmet_boxes)\n            \n    return helmet_boxes\n\ndef display_helmet_detected(helmet_boxes, img_path):\n    masked_image = cv2.imread(img_path)\n    for box in helmet_boxes:\n        masked_image[box[1]:box[3],box[0]:box[2],:] = np.full((box[3]-box[1], box[2]-box[0], 3), (255,255,0))\n    plt.figure(figsize=(15,10))\n    plt.imshow(masked_image)\n    plt.show()","384b69a6":"def mk_images(video_name, video_labels, video_dir, out_dir, only_with_impact=True):\n    video_path=f\"{video_dir}\/{video_name}\"\n    video_name = os.path.basename(video_path)\n    vidcap = cv2.VideoCapture(video_path)\n    if only_with_impact:\n        boxes_all = video_labels.query(\"video == @video_name\")\n        print(video_path, boxes_all[boxes_all.impact == 1.0].shape[0])\n    else:\n        print(video_path)\n    frame = 0\n    while True:\n        it_worked, img = vidcap.read()\n        if not it_worked:\n            break\n        frame += 1\n        if only_with_impact:\n            boxes = video_labels.query(\"video == @video_name and frame == @frame\")\n            boxes_with_impact = boxes[boxes.impact == 1.0]\n            if boxes_with_impact.shape[0] == 0:\n                continue\n        img_name = f\"{video_name}_frame{frame}\"\n        image_path = f'{out_dir}\/{video_name}'.replace('.mp4',f'_{frame}.png')\n        _ = cv2.imwrite(image_path, img)","838c6121":"os.mkdir(\"..\/..\/working\/frames\")\nout_dir = \"..\/..\/working\/frames\"\n\n!mkdir -p $out_dir\nvideo_dir = '..\/..\/input\/nfl-health-and-safety-helmet-assignment\/train\/'\nvideo_folder = [path.split('\/')[-1] for path in glob(f'{video_dir}\/*.mp4')]\nfor video_name in video_folder[:6]:\n    mk_images(video_name, pd.DataFrame(), video_dir, out_dir, only_with_impact=False)","583cd621":"train_baseline_df = pd.read_csv(\"..\/..\/input\/nfl-health-and-safety-helmet-assignment\/train_baseline_helmets.csv\")","3b8be8a6":"def run_comparison(img_path):\n    frame_name = img_path.split(\"\/\")[-1].replace(\".png\",\"\")\n    frame_df = train_baseline_df[train_baseline_df[\"video_frame\"]==frame_name]\n    #show the baseline\n    baseline_boxes = np.array([np.array([row.left, row.top, row.left+row.width, row.top+row.height ])  for idx, row in frame_df.iterrows()])\n    display_helmet_detected(baseline_boxes, img_path)\n    #run the model\n    helmet_boxes = detect_helmets(img_path)\n    display_helmet_detected(helmet_boxes, img_path)","d1288f39":"run_comparison(\"..\/..\/working\/frames\/57784_001741_Endzone_200.png\")","575756cb":"run_comparison(\"..\/..\/working\/frames\/58104_000352_Sideline_30.png\")","b05fda02":"run_comparison(\"..\/..\/working\/frames\/57686_002546_Endzone_120.png\")","dd7bbf3e":"run_comparison(\"..\/..\/working\/frames\/58106_002918_Sideline_70.png\")","5a0f9683":"!rm -rf ..\/..\/working\/frames","34d79492":"The first image represents the baseline helmet boxes given by the competition host, while the 2nd image is the MaskRCNN model.","532096ca":"In the following cell, the first line moves the current directory to the MaskRCNN folder. This is very important for future relative path!","74287a3b":"## Thanks for reading this notebook! If you found this notebook helpful, please give it an upvote. It is always greatly appreciated!","66634d2b":"# Helmet detection using MaskRCNN without downscaling\n\nThis notebook has for goal to share a MaskRCNN model that I had trained for the previous [NFL Impact Detection competition](https:\/\/www.kaggle.com\/c\/nfl-impact-detection). While this model takes images of dimension 512x512 pixels, there is also a set of functions in this notebook that allows to scan over the image in its original resolution. All the boxes detected corresponding to helmets are then merged and filtered when the overlap is too high between 2 boxes. I hope the code is readable enough as I did not initially mean to share it and that it will help people appreciate how good the baseline helmet boxes are before building their own!","5049317c":"# Get the MaskRCNN ready","295a8eda":"The MaskRCNN has many parameters. We have a fairly standard set of parameters with an input size of 512 pixels and a ResNet50 as a backbone.","4e1d4547":"# Run the helmet detection model","26786b47":"The 2 main functions to run the tiles creation and the prediction on each tiles, and then simply display the boxes onto the original image.","9923440e":"# Functions to scan over the image and clean outputs","75fbb4d6":"# Extracting the frames from MP4 files","10e08723":"Some of the hard-coded values in the function below allows to create the 512x512-pixel tiles from the original image and run the MaskRCNN in inference mode on the tiles.","68290f53":"As you can see, the model is not perfect but I would tend to believe it is a good start! In comparison to the baseline, this MaskRCNN model seems to be more conversative. It translates into missed helmets on the edges of the images (possibly due to the tiling process) but may help to have less false positives when it gets messy and players climb on top of each other. That being said, it is only a quick visual assessment. It seems like the baseline helmet boxes are already reliable enough and it may be difficult to do better! Definitely not a low-hanging fruit!","c7527f3c":"When all the helmets have been detected, the set of functions below cleans the outputs. If 2 boxes have less than 7 pixels of difference between their borders, one of the boxes will be removed.","1b826d13":"Let's test the model now on a few images now!"}}