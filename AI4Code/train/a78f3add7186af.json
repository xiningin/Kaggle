{"cell_type":{"b9c33373":"code","700e5739":"code","bfea6cc4":"code","27c78599":"code","dda18f8f":"code","83569529":"code","7ca1a5c7":"code","260e4db9":"code","1660648d":"code","fd37732a":"code","86545739":"code","c16b7bc5":"code","28091b84":"code","677d32f8":"code","93ccb236":"code","e706dfcf":"code","92bf5081":"code","9d197e5b":"code","d5499a3d":"code","0dd5b0b0":"code","c9153fc6":"markdown","fff1d0cf":"markdown","4a66778d":"markdown","290e2754":"markdown","0b7ec83f":"markdown","737fe097":"markdown","495719a0":"markdown","d34afc4e":"markdown","c4bc3813":"markdown","2a0708a5":"markdown","785ad569":"markdown","10fe8d2e":"markdown","91549d92":"markdown","50eb9079":"markdown","3c7d9fa9":"markdown","0706d54e":"markdown","7126117a":"markdown"},"source":{"b9c33373":"import warnings\nwarnings.filterwarnings('ignore')\nimport random\nimport os\nfrom tqdm import tqdm\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 50)\npd.set_option('display.max_columns', 50)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy.stats import probplot, kurtosis, skew, gmean\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Ridge\nfrom sklearn.svm import SVR\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.ensemble import RandomTreesEmbedding\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, Reshape, Concatenate, Dropout, BatchNormalization, Activation, GaussianNoise\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l1, l2\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping","700e5739":"df_train = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/test.csv')\n\ncontinuous_features = [feature for feature in df_train.columns if feature.startswith('cont')]\ntarget = 'target'\n\nprint(f'Training Set Shape = {df_train.shape}')\nprint(f'Training Set Memory Usage = {df_train.memory_usage().sum() \/ 1024 ** 2:.2f} MB')\nprint(f'Test Set Shape = {df_test.shape}')\nprint(f'Test Set Memory Usage = {df_test.memory_usage().sum() \/ 1024 ** 2:.2f} MB')","bfea6cc4":"def plot_target(target):\n    \n    print(f'Target feature {target} Statistical Analysis\\n{\"-\" * 42}')\n        \n    print(f'Mean: {df_train[target].mean():.4}  -  Median: {df_train[target].median():.4}  -  Std: {df_train[target].std():.4}')\n    print(f'Min: {df_train[target].min():.4}  -  25%: {df_train[target].quantile(0.25):.4}  -  50%: {df_train[target].quantile(0.5):.4}  -  75%: {df_train[target].quantile(0.75):.4}  -  Max: {df_train[target].max():.4}')\n    print(f'Skew: {df_train[target].skew():.4}  -  Kurtosis: {df_train[target].kurtosis():.4}')\n    missing_values_count = df_train[df_train[target].isnull()].shape[0]\n    training_samples_count = df_train.shape[0]\n    print(f'Missing Values: {missing_values_count}\/{training_samples_count} ({missing_values_count * 100 \/ training_samples_count:.4}%)')\n\n    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(24, 12), dpi=100)\n\n    sns.distplot(df_train[target], label=target, ax=axes[0][0])\n    axes[0][0].axvline(df_train[target].mean(), label='Target Mean', color='r', linewidth=2, linestyle='--')\n    axes[0][0].axvline(df_train[target].median(), label='Target Median', color='b', linewidth=2, linestyle='--')\n    probplot(df_train[target], plot=axes[0][1])\n    \n    gmm = GaussianMixture(n_components=2, random_state=42)\n    gmm.fit(df_train[target].values.reshape(-1, 1))\n    df_train[f'{target}_class'] = gmm.predict(df_train[target].values.reshape(-1, 1))\n    \n    sns.distplot(df_train[target], label=target, ax=axes[1][0])\n    sns.distplot(df_train[df_train[f'{target}_class'] == 0][target], label='Component 1', ax=axes[1][1])\n    sns.distplot(df_train[df_train[f'{target}_class'] == 1][target], label='Component 2', ax=axes[1][1])\n    \n    axes[0][0].legend(prop={'size': 15})\n    axes[1][1].legend(prop={'size': 15})\n    \n    for i in range(2):\n        for j in range(2):\n            axes[i][j].tick_params(axis='x', labelsize=12)\n            axes[i][j].tick_params(axis='y', labelsize=12)\n            axes[i][j].set_xlabel('')\n            axes[i][j].set_ylabel('')\n    axes[0][0].set_title(f'{target} Distribution in Training Set', fontsize=15, pad=12)\n    axes[0][1].set_title(f'{target} Probability Plot', fontsize=15, pad=12)\n    axes[1][0].set_title(f'{target} Distribution Before GMM', fontsize=15, pad=12)\n    axes[1][1].set_title(f'{target} Distribution After GMM', fontsize=15, pad=12)\n    plt.show()\n    \n\nplot_target(target)","27c78599":"def plot_continuous(continuous_feature):\n            \n    print(f'Continuous feature {continuous_feature} Statistical Analysis\\n{\"-\" * 42}')\n\n    print(f'Training Mean: {float(df_train[continuous_feature].mean()):.4}  - Training Median: {float(df_train[continuous_feature].median()):.4} - Training Std: {float(df_train[continuous_feature].std()):.4}')\n    print(f'Test Mean: {float(df_test[continuous_feature].mean()):.4}  - Test Median: {float(df_test[continuous_feature].median()):.4} - Test Std: {float(df_test[continuous_feature].std()):.4}')\n    print(f'Training Min: {float(df_train[continuous_feature].min()):.4}  - Training Max: {float(df_train[continuous_feature].max()):.4}')\n    print(f'Test Min: {float(df_test[continuous_feature].min()):.4}  - Training Max: {float(df_test[continuous_feature].max()):.4}')\n    print(f'Training Skew: {float(df_train[continuous_feature].skew()):.4}  - Training Kurtosis: {float(df_train[continuous_feature].kurtosis()):.4}')\n    print(f'Test Skew: {float(df_test[continuous_feature].skew()):.4}  - Test Kurtosis: {float(df_test[continuous_feature].kurtosis()):.4}')\n    training_missing_values_count = df_train[df_train[continuous_feature].isnull()].shape[0]\n    test_missing_values_count = df_test[df_test[continuous_feature].isnull()].shape[0]\n    training_samples_count = df_train.shape[0]\n    test_samples_count = df_test.shape[0]\n    print(f'Training Missing Values: {training_missing_values_count}\/{training_samples_count} ({training_missing_values_count * 100 \/ training_samples_count:.4}%)')\n    print(f'Test Missing Values: {test_missing_values_count}\/{test_samples_count} ({test_missing_values_count * 100 \/ test_samples_count:.4}%)')\n\n    fig, axes = plt.subplots(ncols=2, figsize=(24, 6), dpi=100, constrained_layout=True)\n    title_size = 18\n    label_size = 18\n\n    # Continuous Feature Training and Test Set Distribution\n    sns.distplot(df_train[continuous_feature], label='Training', ax=axes[0])\n    sns.distplot(df_test[continuous_feature], label='Test', ax=axes[0])\n    axes[0].set_xlabel('')\n    axes[0].tick_params(axis='x', labelsize=label_size)\n    axes[0].tick_params(axis='y', labelsize=label_size)\n    axes[0].legend()\n    axes[0].set_title(f'{continuous_feature} Distribution in Training and Test Set', size=title_size, pad=title_size)\n    \n    # Continuous Feature vs target\n    sns.scatterplot(df_train[continuous_feature], df_train[target], ax=axes[1])\n    axes[1].set_title(f'{continuous_feature} vs {target}', size=title_size, pad=title_size)\n    axes[1].set_xlabel('')\n    axes[1].set_ylabel('')\n    axes[1].tick_params(axis='x', labelsize=label_size)\n    axes[1].tick_params(axis='y', labelsize=label_size)\n    \n    plt.show()\n    \n    \nfor continuous_feature in sorted(continuous_features, key=lambda x: int(x.split('cont')[-1])):\n    plot_continuous(continuous_feature)","dda18f8f":"class Preprocessor:\n    \n    def __init__(self, train, test, n_splits, shuffle, random_state, scaler, discretize_features, create_features):\n        \n        self.train = train.copy(deep=True)        \n        self.test = test.copy(deep=True)   \n        \n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.random_state = random_state\n        self.scaler = scaler() if scaler else None\n        self.discretize_features = discretize_features\n        self.create_features = create_features\n        \n    def drop_outliers(self):\n        \n        outlier_idx = self.train[self.train[target] < 4].index\n        self.train.drop(outlier_idx, inplace=True)\n        self.train.reset_index(drop=True, inplace=True)\n        \n        print(f'Dropped {len(outlier_idx)} outliers')\n        del outlier_idx\n        \n    def create_folds(self):\n                \n        kf = KFold(n_splits=self.n_splits, shuffle=self.shuffle, random_state=self.random_state)\n        for fold, (_, val_idx) in enumerate(kf.split(self.train), 1):\n            self.train.loc[val_idx, 'fold'] = fold\n\n        self.train['fold'] = self.train['fold'].astype(np.uint8)\n                \n    def scale(self):\n        \n        df_all = pd.concat([self.train[continuous_features], self.test[continuous_features]], ignore_index=True, axis=0)\n        self.scaler.fit(df_all[continuous_features])\n        self.train.loc[:, continuous_features] = self.scaler.transform(self.train.loc[:, continuous_features].values)\n        self.test.loc[:, continuous_features] = self.scaler.transform(self.test.loc[:, continuous_features].values)\n        \n        print(f'Scaled {len(continuous_features)} features with {self.scaler.__class__.__name__}')\n        del df_all\n        \n    def create_peak_features(self):\n        \n        for df in [self.train, self.test]:\n            df['cont2_peak'] = ((df_train['cont2'].round(2) == 0.36) | (df_train['cont2'].round(2) == 0.42) | (df_train['cont2'].round(2) == 0.49) |\\\n                                (df_train['cont2'].round(2) == 0.55) | (df_train['cont2'].round(2) == 0.56) | (df_train['cont2'].round(2) == 0.62) |\\\n                                (df_train['cont2'].round(2) == 0.68) | (df_train['cont2'].round(2) == 0.74)).astype(np.uint8)\n            \n            df['cont5_peak'] = (df_train['cont5'].round(2) == 0.28).astype(np.uint8)\n            \n        peak_features = ['cont2_peak', 'cont5_peak']\n        print(f'Created {len(peak_features)} peak features')\n            \n    def create_idx_features(self):\n        \n        for df in [self.train, self.test]:\n            df['cont_argmin'] = np.argmin(df[continuous_features].values, axis=1)\n            df['cont_argmax'] = np.argmax(df[continuous_features].values, axis=1)\n            \n        idx_features = ['cont_argmin', 'cont_argmax']\n        print(f'Created {len(idx_features)} idx features with argmin and argmax')\n            \n    def create_gmm_features(self):\n        \n        n_component_mapping = {\n            1: 4,\n            2: 10,\n            3: 6,\n            4: 4,\n            5: 3,\n            6: 2,\n            7: 3,\n            8: 4,\n            9: 4,\n            10: 8,\n            11: 5,\n            12: 4,\n            13: 6,\n            14: 6\n        }\n        \n        for i in range(1, 15):\n            gmm = GaussianMixture(n_components=n_component_mapping[i], random_state=self.random_state)            \n            gmm.fit(pd.concat([self.train[f'cont{i}'], self.test[f'cont{i}']], axis=0).values.reshape(-1, 1))\n            \n            self.train[f'cont{i}_class'] = gmm.predict(self.train[f'cont{i}'].values.reshape(-1, 1))\n            self.test[f'cont{i}_class'] = gmm.predict(self.test[f'cont{i}'].values.reshape(-1, 1))\n            \n        gmm_features = [f'cont{i}_class' for i in range(1, 15)]\n        print(f'Created {len(gmm_features)} gmm features with GaussianMixture')\n                \n    def transform(self):\n        \n        self.drop_outliers()\n        self.create_folds()\n        \n        if self.create_features:\n            self.create_peak_features()\n            self.create_idx_features()\n            \n        if self.discretize_features:\n            self.create_gmm_features()\n                    \n        if self.scaler:\n            self.scale()\n        \n        return self.train.copy(deep=True), self.test.copy(deep=True)\n","83569529":"cross_validation_seed = 0\npreprocessor = Preprocessor(train=df_train, test=df_test,\n                            n_splits=5, shuffle=True, random_state=cross_validation_seed,\n                            scaler=None,\n                            create_features=False, discretize_features=False)\ndf_train_processed, df_test_processed = preprocessor.transform()\n\nprint(f'\\nPreprocessed Training Set Shape = {df_train_processed.shape}')\nprint(f'Preprocessed Training Set Memory Usage = {df_train_processed.memory_usage().sum() \/ 1024 ** 2:.2f} MB')\nprint(f'Preprocessed Test Set Shape = {df_test_processed.shape}')\nprint(f'Preprocessed Test Set Memory Usage = {df_test_processed.memory_usage().sum() \/ 1024 ** 2:.2f} MB\\n')","7ca1a5c7":"class TreeModels:\n    \n    def __init__(self, predictors, target, model, model_parameters, boosting_rounds, early_stopping_rounds, seeds):\n        \n        self.predictors = predictors\n        self.target = target\n               \n        self.model = model\n        self.model_parameters = model_parameters\n        self.boosting_rounds = boosting_rounds\n        self.early_stopping_rounds = early_stopping_rounds\n        self.seeds = seeds\n                \n    def _train_and_predict_lgb(self, X_train, y_train, X_test):\n        \n        seed_avg_oof_predictions = np.zeros(X_train.shape[0])\n        seed_avg_test_predictions = np.zeros(X_test.shape[0])        \n        seed_avg_importance = pd.DataFrame(data=np.zeros(len(self.predictors)), index=self.predictors, columns=['Importance'])\n        \n        for seed in self.seeds:\n            print(f'{\"-\" * 30}\\nRunning LightGBM model with seed: {seed}\\n{\"-\" * 30}\\n')\n            self.model_parameters['seed'] = seed\n            self.model_parameters['feature_fraction_seed'] = seed\n            self.model_parameters['bagging_seed'] = seed\n            self.model_parameters['drop_seed'] = seed\n            self.model_parameters['data_random_seed'] = seed\n                \n            for fold in sorted(X_train['fold'].unique()):\n\n                trn_idx, val_idx = X_train.loc[X_train['fold'] != fold].index, X_train.loc[X_train['fold'] == fold].index\n                trn = lgb.Dataset(X_train.loc[trn_idx, self.predictors], label=y_train.loc[trn_idx])\n                val = lgb.Dataset(X_train.loc[val_idx, self.predictors], label=y_train.loc[val_idx])\n\n                model = lgb.train(params=self.model_parameters,\n                                  train_set=trn,\n                                  valid_sets=[trn, val],\n                                  num_boost_round=self.boosting_rounds,\n                                  early_stopping_rounds=self.early_stopping_rounds,\n                                  verbose_eval=500)            \n\n                val_predictions = model.predict(X_train.loc[val_idx, self.predictors])\n                seed_avg_oof_predictions[val_idx] += (val_predictions \/ len(self.seeds))\n                test_predictions = model.predict(X_test[self.predictors])\n                seed_avg_test_predictions += (test_predictions \/ X_train['fold'].nunique() \/ len(self.seeds))\n                seed_avg_importance['Importance'] += (model.feature_importance(importance_type='gain') \/ X_train['fold'].nunique() \/ len(self.seeds))\n\n                fold_score = mean_squared_error(y_train.loc[val_idx], val_predictions, squared=False)\n                print(f'\\nLGB Fold {int(fold)} - X_trn: {X_train.loc[trn_idx, self.predictors].shape} X_val: {X_train.loc[val_idx, self.predictors].shape} - Score: {fold_score:.6} - Seed: {seed}\\n')\n            \n        df_train_processed['LGBPredictions'] = seed_avg_oof_predictions\n        df_test_processed['LGBPredictions'] = seed_avg_test_predictions\n        oof_score = mean_squared_error(y_train, df_train_processed['LGBPredictions'], squared=False)\n        print(f'{\"-\" * 30}\\nLGB OOF RMSE: {oof_score:.6} ({len(self.seeds)} Seed Average)\\n{\"-\" * 30}')\n                \n        self._plot_importance(seed_avg_importance)\n        self._plot_predictions(df_train_processed[target], df_train_processed['LGBPredictions'], df_test_processed['LGBPredictions'])\n        \n    def _train_and_predict_cb(self, X_train, y_train, X_test):\n        \n        seed_avg_oof_predictions = np.zeros(X_train.shape[0])\n        seed_avg_test_predictions = np.zeros(X_test.shape[0])        \n        seed_avg_importance = pd.DataFrame(data=np.zeros(len(self.predictors)), index=self.predictors, columns=['Importance'])\n            \n        for seed in self.seeds:\n            print(f'{\"-\" * 30}\\nRunning CatBoost model with seed: {seed}\\n{\"-\" * 30}\\n')\n            self.model_parameters['random_seed'] = seed\n            \n            for fold in sorted(X_train['fold'].unique()):\n\n                trn_idx, val_idx = X_train.loc[X_train['fold'] != fold].index, X_train.loc[X_train['fold'] == fold].index\n                trn = cb.Pool(X_train.loc[trn_idx, self.predictors], label=y_train.loc[trn_idx])\n                val = cb.Pool(X_train.loc[val_idx, self.predictors], label=y_train.loc[val_idx])\n\n                model = cb.CatBoostRegressor(**self.model_parameters)\n                model.fit(X=trn, eval_set=val)\n\n                val_predictions = model.predict(val)\n                seed_avg_oof_predictions[val_idx] += (val_predictions \/ len(self.seeds))\n                test_predictions = model.predict(cb.Pool(X_test[self.predictors]))\n                seed_avg_test_predictions += (test_predictions \/ X_train['fold'].nunique() \/ len(self.seeds))\n                seed_avg_importance['Importance'] += (model.get_feature_importance() \/ X_train['fold'].nunique() \/ len(self.seeds))\n\n                fold_score = mean_squared_error(df_train_processed.loc[val_idx, self.target], val_predictions, squared=False)\n                print(f'\\nCB Fold {int(fold)} - X_trn: {X_train.loc[trn_idx, self.predictors].shape} X_val: {X_train.loc[val_idx, self.predictors].shape} - Score: {fold_score:.6} - Seed: {seed}\\n')\n            \n        df_train_processed['CBPredictions'] = seed_avg_oof_predictions\n        df_test_processed['CBPredictions'] = seed_avg_test_predictions\n        oof_score = mean_squared_error(y_train, df_train_processed['CBPredictions'], squared=False)\n        print(f'{\"-\" * 30}\\nCB OOF RMSE: {oof_score:.6} ({len(self.seeds)} Seed Average)\\n{\"-\" * 30}')\n                \n        self._plot_importance(seed_avg_importance)\n        self._plot_predictions(df_train_processed[target], df_train_processed['CBPredictions'], df_test_processed['CBPredictions'])\n        \n    def _train_and_predict_xgb(self, X_train, y_train, X_test):\n        \n        seed_avg_oof_predictions = np.zeros(X_train.shape[0])\n        seed_avg_test_predictions = np.zeros(X_test.shape[0])\n        seed_avg_importance = pd.DataFrame(data=np.zeros(len(self.predictors)), index=self.predictors, columns=['Importance'])\n        \n        for seed in self.seeds:\n            print(f'{\"-\" * 30}\\nRunning XGBoost model with seed: {seed}\\n{\"-\" * 30}\\n')\n            self.model_parameters['seed'] = seed\n        \n            for fold in sorted(X_train['fold'].unique()):\n\n                trn_idx, val_idx = X_train.loc[X_train['fold'] != fold].index, X_train.loc[X_train['fold'] == fold].index\n                trn = xgb.DMatrix(X_train.loc[trn_idx, self.predictors], label=y_train.loc[trn_idx])\n                val = xgb.DMatrix(X_train.loc[val_idx, self.predictors], label=y_train.loc[val_idx])\n\n                model = xgb.train(params=self.model_parameters,\n                                  dtrain=trn,\n                                  evals=[(trn, 'train'), (val, 'val')],\n                                  num_boost_round=self.boosting_rounds, \n                                  early_stopping_rounds=self.early_stopping_rounds,\n                                  verbose_eval=500) \n\n                val_predictions = model.predict(xgb.DMatrix(X_train.loc[val_idx, self.predictors]))\n                seed_avg_oof_predictions[val_idx] += (val_predictions \/ len(self.seeds))\n                test_predictions = model.predict(xgb.DMatrix(X_test[self.predictors]))\n                seed_avg_test_predictions += (test_predictions \/ X_train['fold'].nunique() \/ len(self.seeds))\n                seed_avg_importance['Importance'] += (np.array(list(model.get_score(importance_type='gain').values())) \/ X_train['fold'].nunique() \/ len(self.seeds))\n\n                fold_score = mean_squared_error(df_train_processed.loc[val_idx, self.target], val_predictions, squared=False)\n                print(f'\\nXGB Fold {int(fold)} - X_trn: {X_train.loc[trn_idx, self.predictors].shape} X_val: {X_train.loc[val_idx, self.predictors].shape} - Score: {fold_score:.6} - Seed: {seed}\\n')\n            \n        df_train_processed['XGBPredictions'] = seed_avg_oof_predictions\n        df_test_processed['XGBPredictions'] = seed_avg_test_predictions\n        oof_score = mean_squared_error(y_train, df_train_processed['XGBPredictions'], squared=False)\n        print(f'{\"-\" * 30}\\nXGB OOF RMSE: {oof_score:.6} ({len(self.seeds)} Seed Average) \\n{\"-\" * 30}')\n                \n        self._plot_importance(seed_avg_importance)\n        self._plot_predictions(df_train_processed[target], df_train_processed['XGBPredictions'], df_test_processed['XGBPredictions'])\n        \n    def _train_and_predict_rf(self, X_train, y_train, X_test):\n        \n        seed_avg_oof_predictions = np.zeros(X_train.shape[0])\n        seed_avg_test_predictions = np.zeros(X_test.shape[0])\n        \n        for seed in self.seeds:\n            print(f'{\"-\" * 30}\\nRunning RandomForest model with seed: {seed}\\n{\"-\" * 30}\\n')\n            self.model_parameters['random_state'] = seed\n                \n            for fold in sorted(X_train['fold'].unique()):\n\n                trn_idx, val_idx = X_train.loc[X_train['fold'] != fold].index, X_train.loc[X_train['fold'] == fold].index\n                X_trn, y_trn = X_train.loc[trn_idx, self.predictors].astype(np.float32), y_train.loc[trn_idx].astype(np.float32)\n                X_val, y_val = X_train.loc[val_idx, self.predictors].astype(np.float32), y_train.loc[val_idx].astype(np.float32)\n\n                import cuml\n                model = cuml.ensemble.RandomForestRegressor(**self.model_parameters)\n                model.fit(X_trn, y_trn)\n\n                val_predictions = model.predict(X_val)\n                seed_avg_oof_predictions[val_idx] += (val_predictions \/ len(self.seeds))\n                test_predictions = model.predict(X_test[self.predictors])\n                seed_avg_test_predictions += (test_predictions \/ X_train['fold'].nunique() \/ len(self.seeds))\n\n                fold_score = mean_squared_error(df_train_processed.loc[val_idx, self.target], val_predictions, squared=False)\n                print(f'RF Fold {int(fold)} - X_trn: {X_train.loc[trn_idx, self.predictors].shape} X_val: {X_train.loc[val_idx, self.predictors].shape} - Score: {fold_score:.6}')\n\n        df_train_processed['RFPredictions'] = seed_avg_oof_predictions\n        df_test_processed['RFPredictions'] = seed_avg_test_predictions\n        oof_score = mean_squared_error(y_train, df_train_processed['RFPredictions'], squared=False)\n        print(f'{\"-\" * 30}\\nRF OOF RMSE: {oof_score:.6} ({len(self.seeds)} Seed Average) \\n{\"-\" * 30}')\n        \n        self._plot_predictions(df_train_processed[target], df_train_processed['RFPredictions'], df_test_processed['RFPredictions'])\n        \n    def _plot_importance(self, df_importance):\n        \n        df_importance.sort_values(by='Importance', inplace=True, ascending=False)\n        \n        plt.figure(figsize=(25, 6))       \n        sns.barplot(x='Importance', y=df_importance.index, data=df_importance, palette='Blues_d')\n        plt.xlabel('')\n        plt.tick_params(axis='x', labelsize=20)\n        plt.tick_params(axis='y', labelsize=20)\n        plt.title(f'{self.model} Feature Importance (Gain)', size=20, pad=20)\n        plt.show()\n        \n    def _plot_predictions(self, train_labels, train_predictions, test_predictions):\n        \n        fig, axes = plt.subplots(ncols=2, figsize=(25, 6))                                            \n        sns.scatterplot(train_labels, train_predictions, ax=axes[0])\n        sns.distplot(train_predictions, label='Train Predictions', ax=axes[1])\n        sns.distplot(test_predictions, label='Test Predictions', ax=axes[1])\n\n        axes[0].set_xlabel(f'Train Labels', size=18)\n        axes[0].set_ylabel(f'Train Predictions', size=18)\n        axes[1].set_xlabel('')\n        axes[1].legend(prop={'size': 18})\n        for i in range(2):\n            axes[i].tick_params(axis='x', labelsize=15)\n            axes[i].tick_params(axis='y', labelsize=15)\n        axes[0].set_title(f'Train Labels vs Train Predictions', size=20, pad=20)\n        axes[1].set_title(f'Predictions Distributions', size=20, pad=20)\n            \n        plt.show() \n        \n    def run(self, X_train, y_train, X_test):\n        \n        if self.model == 'LGB':\n            self._train_and_predict_lgb(X_train, y_train, X_test)\n        elif self.model == 'CB':\n            self._train_and_predict_cb(X_train, y_train, X_test)\n        elif self.model == 'XGB':\n            self._train_and_predict_xgb(X_train, y_train, X_test)\n        elif self.model == 'RF':\n            self._train_and_predict_rf(X_train, y_train, X_test)\n","260e4db9":"TRAIN_LGB = False\n\nif TRAIN_LGB:\n    model = 'LGB'\n    lgb_preprocessor = Preprocessor(train=df_train, test=df_test,\n                                    n_splits=5, shuffle=True, random_state=cross_validation_seed, scaler=None,\n                                    create_features=False, discretize_features=False)\n    df_train_lgb, df_test_lgb = lgb_preprocessor.transform()\n\n    print(f'\\n{model} Training Set Shape = {df_train_lgb.shape}')\n    print(f'{model} Training Set Memory Usage = {df_train_lgb.memory_usage().sum() \/ 1024 ** 2:.2f} MB')\n    print(f'{model} Test Set Shape = {df_test_lgb.shape}')\n    print(f'{model} Test Set Memory Usage = {df_test_lgb.memory_usage().sum() \/ 1024 ** 2:.2f} MB\\n')\n\n    X_train_lgb = df_train_lgb.copy(deep=True)\n    y_train_lgb = df_train_lgb[target].copy(deep=True)\n    X_test_lgb = df_test_lgb.copy(deep=True)\n\n    lgb_parameters = {\n        'predictors': continuous_features,\n        'target': target,\n        'model': model,\n        'model_parameters': {\n            'num_leaves': 2 ** 8, \n            'learning_rate': 0.001,\n            'bagging_fraction': 0.6,\n            'bagging_freq': 3,\n            'feature_fraction': 0.5,\n            'feature_fraction_bynode': 0.8,\n            'min_data_in_leaf': 100,\n            'min_data_per_group': 1,            \n            'min_gain_to_split': 0.001,\n            'lambda_l1': 6,\n            'lambda_l2': 0.0005,\n            'max_bin': 768,\n            'max_depth': -1,\n            'objective': 'regression',\n            'seed': None,\n            'feature_fraction_seed': None,\n            'bagging_seed': None,\n            'drop_seed': None,\n            'data_random_seed': None,\n            'boosting_type': 'gbdt',\n            'verbose': 1,\n            'metric': 'rmse',\n            'n_jobs': -1,\n        },\n        'boosting_rounds': 20000,\n        'early_stopping_rounds': 200,\n        'seeds': [541992, 721991, 1337]\n    }\n\n    lgb_model = TreeModels(**lgb_parameters)\n    lgb_model.run(X_train_lgb, y_train_lgb, X_test_lgb)\n\n    del df_train_lgb, df_test_lgb, X_train_lgb, y_train_lgb, X_test_lgb\n    del lgb_preprocessor, lgb_parameters, lgb_model\n    \n    print('Saving LightGBM OOF and Test predictions to current working directory.')\n    df_train_processed[['id', 'LGBPredictions']].to_csv('lgb_oof_predictions.csv', index=False)\n    df_test_processed[['id', 'LGBPredictions']].to_csv('lgb_test_predictions.csv', index=False)\n    \nelse:\n    print('Loading LightGBM OOF and Test predictions from current working directory.')\n    df_train_processed['LGBPredictions'] = pd.read_csv('..\/input\/tabular-playground-series\/Tabular Playground Series - Jan 2021\/lgb_oof_predictions.csv')['LGBPredictions']\n    df_test_processed['LGBPredictions'] = pd.read_csv('..\/input\/tabular-playground-series\/Tabular Playground Series - Jan 2021\/lgb_test_predictions.csv')['LGBPredictions']    \n    oof_score = mean_squared_error(df_train_processed['target'], df_train_processed['LGBPredictions'], squared=False)\n    print(f'LGB OOF RMSE: {oof_score:.6}')\n    TreeModels._plot_predictions(None, df_train_processed[target], df_train_processed['LGBPredictions'], df_test_processed['LGBPredictions'])\n    ","1660648d":"TRAIN_CB = False\n\nif TRAIN_CB:\n    model = 'CB'\n    cb_preprocessor = Preprocessor(train=df_train, test=df_test,\n                                   n_splits=5, shuffle=True, random_state=cross_validation_seed, scaler=None,\n                                   create_features=False, discretize_features=False)\n    df_train_cb, df_test_cb = cb_preprocessor.transform()\n\n    print(f'\\n{model} Training Set Shape = {df_train_cb.shape}')\n    print(f'{model} Training Set Memory Usage = {df_train_cb.memory_usage().sum() \/ 1024 ** 2:.2f} MB')\n    print(f'{model} Test Set Shape = {df_test_cb.shape}')\n    print(f'{model} Test Set Memory Usage = {df_test_cb.memory_usage().sum() \/ 1024 ** 2:.2f} MB\\n')\n\n    X_train_cb = df_train_cb[continuous_features + ['fold']].copy(deep=True)\n    y_train_cb = df_train_cb[target].copy(deep=True)\n    X_test_cb = df_test_cb[continuous_features].copy(deep=True)\n\n    cb_parameters = {\n        'predictors': continuous_features,\n        'target': target,\n        'model': model,\n        'model_parameters': {\n            'n_estimators': 20000, \n            'learning_rate': 0.006,\n            'depth': 10,\n            'subsample': 0.8,\n            'colsample_bylevel': 0.5,\n            'l2_leaf_reg': 0.1,\n            'metric_period': 1000,\n            'boost_from_average': True,\n            'use_best_model': True,\n            'eval_metric': 'RMSE',\n            'loss_function': 'RMSE',   \n            'od_type': 'Iter',\n            'od_wait': 200,\n            'random_seed': None,\n            'verbose': 1,\n        },\n        'boosting_rounds': None,\n        'early_stopping_rounds': None,\n        'seeds': [541992, 721991, 1337, 42, 0]\n    }\n\n    cb_model = TreeModels(**cb_parameters)\n    cb_model.run(X_train_cb, y_train_cb, X_test_cb)\n    \n    del df_train_cb, df_test_cb, X_train_cb, y_train_cb, X_test_cb\n    del cb_preprocessor, cb_parameters, cb_model\n    \n    print('Saving CatBoost OOF and Test predictions to current working directory.')\n    df_train_processed[['id', 'CBPredictions']].to_csv('cb_oof_predictions.csv', index=False)\n    df_test_processed[['id', 'CBPredictions']].to_csv('cb_test_predictions.csv', index=False)\n    \nelse:\n    print('Loading CatBoost OOF and Test predictions from current working directory.')\n    df_train_processed['CBPredictions'] = pd.read_csv('..\/input\/tabular-playground-series\/Tabular Playground Series - Jan 2021\/cb_oof_predictions.csv')['CBPredictions']\n    df_test_processed['CBPredictions'] = pd.read_csv('..\/input\/tabular-playground-series\/Tabular Playground Series - Jan 2021\/cb_test_predictions.csv')['CBPredictions']    \n    oof_score = mean_squared_error(df_train_processed['target'], df_train_processed['CBPredictions'], squared=False)\n    print(f'CB OOF RMSE: {oof_score:.6}')\n    TreeModels._plot_predictions(None, df_train_processed[target], df_train_processed['CBPredictions'], df_test_processed['CBPredictions'])\n","fd37732a":"TRAIN_XGB = False\n\nif TRAIN_XGB:\n    model = 'XGB'\n    xgb_preprocessor = Preprocessor(train=df_train, test=df_test,\n                                    n_splits=5, shuffle=True, random_state=cross_validation_seed, scaler=None,\n                                    create_features=False, discretize_features=False)\n    df_train_xgb, df_test_xgb = xgb_preprocessor.transform()\n\n    print(f'\\n{model} Training Set Shape = {df_train_xgb.shape}')\n    print(f'{model} Training Set Memory Usage = {df_train_xgb.memory_usage().sum() \/ 1024 ** 2:.2f} MB')\n    print(f'{model} Test Set Shape = {df_test_xgb.shape}')\n    print(f'{model} Test Set Memory Usage = {df_test_xgb.memory_usage().sum() \/ 1024 ** 2:.2f} MB\\n')\n\n    X_train_xgb = df_train_xgb[continuous_features + ['fold']].copy(deep=True)\n    y_train_xgb = df_train_xgb[target].copy(deep=True)\n    X_test_xgb = df_test_xgb[continuous_features].copy(deep=True)\n\n    xgb_parameters = {\n        'predictors': continuous_features,\n        'target': target,\n        'model': model,\n        'model_parameters': {\n            'learning_rate': 0.002,\n            'colsample_bytree': 0.6, \n            'colsample_bylevel': 0.6,\n            'colsample_bynode': 0.6,\n            'sumbsample': 0.8,\n            'max_depth': 14,\n            'gamma': 0,\n            'min_child_weight': 200,\n            'lambda': 0,\n            'alpha': 0,\n            'objective': 'reg:squarederror',\n            'seed': None,\n            'boosting_type': 'gbtree',\n            'tree_method': 'gpu_hist',\n            'silent': True,\n            'verbose': 1,\n            'n_jobs': -1,\n        },\n        'boosting_rounds': 25000,\n        'early_stopping_rounds': 200,\n        'seeds': [541992, 721991, 1337]\n    }\n\n    xgb_model = TreeModels(**xgb_parameters)\n    xgb_model.run(X_train_xgb, y_train_xgb, X_test_xgb)\n\n    del df_train_xgb, df_test_xgb, X_train_xgb, y_train_xgb, X_test_xgb\n    del xgb_preprocessor, xgb_parameters, xgb_model\n    \n    print('Saving XGBoost OOF and Test predictions to current working directory.')\n    df_train_processed[['id', 'XGBPredictions']].to_csv('xgb_oof_predictions.csv', index=False)\n    df_test_processed[['id', 'XGBPredictions']].to_csv('xgb_test_predictions.csv', index=False)\n    \nelse:\n    print('Loading XGBoost OOF and Test predictions from current working directory.')\n    df_train_processed['XGBPredictions'] = pd.read_csv('..\/input\/tabular-playground-series\/Tabular Playground Series - Jan 2021\/xgb_oof_predictions.csv')['XGBPredictions']\n    df_test_processed['XGBPredictions'] = pd.read_csv('..\/input\/tabular-playground-series\/Tabular Playground Series - Jan 2021\/xgb_test_predictions.csv')['XGBPredictions']    \n    oof_score = mean_squared_error(df_train_processed['target'], df_train_processed['XGBPredictions'], squared=False)\n    print(f'XGB OOF RMSE: {oof_score:.6}')\n    TreeModels._plot_predictions(None, df_train_processed[target], df_train_processed['XGBPredictions'], df_test_processed['XGBPredictions'])\n","86545739":"TRAIN_RF = False\n\nif TRAIN_RF:\n    model = 'RF'\n    rf_preprocessor = Preprocessor(train=df_train, test=df_test,\n                                   n_splits=5, shuffle=True, random_state=cross_validation_seed, scaler=None,\n                                   create_features=False, discretize_features=False)\n    df_train_rf, df_test_rf = rf_preprocessor.transform()\n\n    print(f'\\n{model} Training Set Shape = {df_train_rf.shape}')\n    print(f'{model} Training Set Memory Usage = {df_train_rf.memory_usage().sum() \/ 1024 ** 2:.2f} MB')\n    print(f'{model} Test Set Shape = {df_test_rf.shape}')\n    print(f'{model} Test Set Memory Usage = {df_test_rf.memory_usage().sum() \/ 1024 ** 2:.2f} MB\\n')\n\n    X_train_rf = df_train_rf[continuous_features + ['fold']].copy(deep=True)\n    y_train_rf = df_train_rf[target].copy(deep=True)\n    X_test_rf = df_test_rf[continuous_features].copy(deep=True)\n\n    rf_parameters = {\n        'predictors': continuous_features,\n        'target': target,\n        'model': model,\n        'model_parameters': {\n            'n_estimators': 400,\n            'split_algo': 0,\n            'split_criterion': 2,             \n            'bootstrap': True,\n            'bootstrap_features': False,\n            'max_depth': 13,\n            'max_leaves': -1,\n            'max_features': 0.5,\n            'n_bins': 2 ** 6,\n            'random_state': None,\n            'verbose': True,\n        },\n        'boosting_rounds': None,\n        'early_stopping_rounds': None,\n        'seeds': [541992, 721991, 1337, 42, 0]\n    }\n\n    rf_model = TreeModels(**rf_parameters)\n    rf_model.run(X_train_rf, y_train_rf, X_test_rf)\n\n    del df_train_rf, df_test_rf, X_train_rf, y_train_rf, X_test_rf\n    del rf_preprocessor, rf_parameters, rf_model\n    \n    print('Saving RandomForest OOF and Test predictions to current working directory.')\n    df_train_processed[['id', 'RFPredictions']].to_csv('rf_oof_predictions.csv', index=False)\n    df_test_processed[['id', 'RFPredictions']].to_csv('rf_test_predictions.csv', index=False)\n    \nelse:\n    print('Loading RandomForest OOF and Test predictions from current working directory.')\n    df_train_processed['RFPredictions'] = pd.read_csv('..\/input\/tabular-playground-series\/Tabular Playground Series - Jan 2021\/rf_oof_predictions.csv')['RFPredictions']\n    df_test_processed['RFPredictions'] = pd.read_csv('..\/input\/tabular-playground-series\/Tabular Playground Series - Jan 2021\/rf_test_predictions.csv')['RFPredictions']    \n    oof_score = mean_squared_error(df_train_processed['target'], df_train_processed['RFPredictions'], squared=False)\n    print(f'RF OOF RMSE: {oof_score:.6}')\n    TreeModels._plot_predictions(None, df_train_processed[target], df_train_processed['RFPredictions'], df_test_processed['RFPredictions'])\n","c16b7bc5":"class LinearModels:\n    \n    def __init__(self, predictors, target, model, model_parameters):\n        \n        self.predictors = predictors\n        self.target = target\n               \n        self.model = model\n        self.model_parameters = model_parameters       \n                \n    def _train_and_predict_ridge_regression(self, X_train, y_train, X_test):\n                \n        X = pd.concat([X_train[continuous_features], X_test[continuous_features]], ignore_index=True, axis=0)\n        embedder = RandomTreesEmbedding(n_estimators=800,\n                                        max_depth=8,\n                                        min_samples_split=100,\n                                        n_jobs=-1,\n                                        random_state=0, \n                                        verbose=False)\n        embedder.fit(X)        \n        del X\n        X_test = embedder.transform(X_test.loc[:, continuous_features]).astype(np.uint8)\n                        \n        oof_predictions = np.zeros(X_train.shape[0])\n        test_predictions = np.zeros(X_test.shape[0])\n        \n        for fold in sorted(X_train['fold'].unique()):\n            \n            trn_idx, val_idx = X_train.loc[X_train['fold'] != fold].index, X_train.loc[X_train['fold'] == fold].index\n\n            X_trn = embedder.transform(X_train.loc[trn_idx, continuous_features]).astype(np.uint8)\n            y_trn = y_train.loc[trn_idx]\n            X_val = embedder.transform(X_train.loc[val_idx, continuous_features]).astype(np.uint8)\n            y_val = y_train.loc[val_idx]\n            \n            model = Ridge(**self.model_parameters)\n            model.fit(X_trn, y_trn)\n            \n            val_predictions = model.predict(X_val)\n            oof_predictions[val_idx] += val_predictions\n            fold_test_predictions = model.predict(X_test)\n            test_predictions += (fold_test_predictions \/ X_train['fold'].nunique())\n                        \n            fold_score = mean_squared_error(y_val, val_predictions, squared=False)\n            print(f'RR Fold {int(fold)} - X_trn: {X_trn.shape} X_val: {X_val.shape} - Score: {fold_score:.6}')\n            \n        df_train_processed['RRPredictions'] = oof_predictions\n        df_test_processed['RRPredictions'] = test_predictions\n        oof_score = mean_squared_error(y_train, df_train_processed['RRPredictions'], squared=False)\n        print(f'{\"-\" * 30}\\nRR OOF RMSE: {oof_score:.6}\\n{\"-\" * 30}')\n        \n        self._plot_predictions(df_train_processed[target], df_train_processed['RRPredictions'], df_test_processed['RRPredictions'])\n            \n    def _train_and_predict_svm(self, X_train, y_train, X_test):\n        \n        oof_predictions = np.zeros(X_train.shape[0])\n        test_predictions = np.zeros(X_test.shape[0])\n        \n        for fold in sorted(X_train['fold'].unique()):\n            \n            trn_idx, val_idx = X_train.loc[X_train['fold'] != fold].index, X_train.loc[X_train['fold'] == fold].index\n\n            X_trn = X_train.loc[trn_idx, self.predictors]\n            y_trn = y_train.loc[trn_idx]\n            X_val = X_train.loc[val_idx, self.predictors]\n            y_val = y_train.loc[val_idx]\n\n            import cuml\n            model = cuml.SVR(**self.model_parameters)\n            model.fit(X_trn, y_trn)\n            \n            val_predictions = model.predict(X_val)\n            oof_predictions[val_idx] += val_predictions\n            fold_test_predictions = model.predict(X_test)\n            test_predictions += (fold_test_predictions \/ X_train['fold'].nunique())\n            \n            fold_score = mean_squared_error(y_val, val_predictions, squared=False)\n            print(f'SVM Fold {int(fold)} - X_trn: {X_trn.shape} X_val: {X_val.shape} - Score: {fold_score:.6}')\n            \n        df_train_processed['SVMPredictions'] = oof_predictions\n        df_test_processed['SVMPredictions'] = test_predictions\n        oof_score = mean_squared_error(y_train, df_train_processed['SVMPredictions'], squared=False)\n        print(f'{\"-\" * 30}\\nSVM OOF RMSE: {oof_score:.6}\\n{\"-\" * 30}')\n        \n        self._plot_predictions(df_train_processed[target], df_train_processed['SVMPredictions'], df_test_processed['SVMPredictions'])\n        \n    def _plot_predictions(self, train_labels, train_predictions, test_predictions):\n        \n        fig, axes = plt.subplots(ncols=2, figsize=(25, 6))                                            \n        sns.scatterplot(train_labels, train_predictions, ax=axes[0])\n        sns.distplot(train_predictions, label='Train Predictions', ax=axes[1])\n        sns.distplot(test_predictions, label='Test Predictions', ax=axes[1])\n\n        axes[0].set_xlabel(f'Train Labels', size=18)\n        axes[0].set_ylabel(f'Train Predictions', size=18)\n        axes[1].set_xlabel('')\n        axes[1].legend(prop={'size': 18})\n        for i in range(2):\n            axes[i].tick_params(axis='x', labelsize=15)\n            axes[i].tick_params(axis='y', labelsize=15)\n        axes[0].set_title(f'Train Labels vs Train Predictions', size=20, pad=20)\n        axes[1].set_title(f'Predictions Distributions', size=20, pad=20)\n            \n        plt.show() \n            \n    def run(self, X_train, y_train, X_test):\n        \n        if self.model == 'Ridge':\n            self._train_and_predict_ridge_regression(X_train, y_train, X_test)\n        elif self.model == 'SVM':\n            self._train_and_predict_svm(X_train, y_train, X_test)\n","28091b84":"FIT_RR = False\n\nif FIT_RR:\n    model = 'Ridge'\n    ridge_preprocessor = Preprocessor(train=df_train, test=df_test,\n                                      n_splits=5, shuffle=True, random_state=cross_validation_seed, scaler=None,\n                                      create_features=False, discretize_features=False)\n    df_train_ridge, df_test_ridge = ridge_preprocessor.transform()\n\n    print(f'\\n{model} Training Set Shape = {df_train_ridge.shape}')\n    print(f'{model} Training Set Memory Usage = {df_train_ridge.memory_usage().sum() \/ 1024 ** 2:.2f} MB')\n    print(f'{model} Test Set Shape = {df_test_ridge.shape}')\n    print(f'{model} Test Set Memory Usage = {df_test_ridge.memory_usage().sum() \/ 1024 ** 2:.2f} MB\\n')\n    \n    X_train_rr = df_train_ridge[continuous_features + ['fold']].copy(deep=True)\n    y_train_rr = df_train_ridge[target].copy(deep=True)\n    X_test_rr = df_test_ridge[continuous_features].copy(deep=True)\n    \n    ridge_parameters = {\n        'predictors': continuous_features,\n        'target': target,\n        'model': model,\n        'model_parameters': {\n            'alpha': 7000\n        }\n    }\n\n    ridge_model = LinearModels(**ridge_parameters)\n    ridge_model.run(X_train_rr, y_train_rr, X_test_rr)\n\n    del df_train_ridge, df_test_ridge, X_train_ridge, y_train_ridge, X_test_ridge\n    del ridge_preprocessor, ridge_parameters, ridge_model\n    \n    print('Saving RidgeRegression OOF and Test predictions to current working directory.')\n    df_train_processed[['id', 'RRPredictions']].to_csv('rr_oof_predictions.csv', index=False)\n    df_test_processed[['id', 'RRPredictions']].to_csv('rr_test_predictions.csv', index=False)\n    \nelse:\n    print('Loading RidgeRegression OOF and Test predictions from current working directory.')\n    df_train_processed['RRPredictions'] = pd.read_csv('..\/input\/tabular-playground-series\/Tabular Playground Series - Jan 2021\/rr_oof_predictions.csv')['RRPredictions']\n    df_test_processed['RRPredictions'] = pd.read_csv('..\/input\/tabular-playground-series\/Tabular Playground Series - Jan 2021\/rr_test_predictions.csv')['RRPredictions']    \n    oof_score = mean_squared_error(df_train_processed['target'], df_train_processed['RRPredictions'], squared=False)\n    print(f'RR OOF RMSE: {oof_score:.6}')\n    LinearModels._plot_predictions(None, df_train_processed[target], df_train_processed['RRPredictions'], df_test_processed['RRPredictions'])\n","677d32f8":"FIT_SVM = False\n\nif FIT_SVM:\n    model = 'SVM'\n    svm_preprocessor = Preprocessor(train=df_train, test=df_test,\n                                    n_splits=5, shuffle=True, random_state=cross_validation_seed, scaler=StandardScaler,\n                                    create_features=False, discretize_features=True)\n    df_train_svm, df_test_svm = svm_preprocessor.transform()\n\n    print(f'\\n{model} Training Set Shape = {df_train_svm.shape}')\n    print(f'{model} Training Set Memory Usage = {df_train_svm.memory_usage().sum() \/ 1024 ** 2:.2f} MB')\n    print(f'{model} Test Set Shape = {df_test_svm.shape}')\n    print(f'{model} Test Set Memory Usage = {df_test_svm.memory_usage().sum() \/ 1024 ** 2:.2f} MB\\n')\n\n    X_train_svm = df_train_svm[continuous_features + ['fold'] + [f'{cont}_class' for cont in continuous_features]].copy(deep=True)\n    y_train_svm = df_train_svm[target].copy(deep=True)\n    X_test_svm = df_test_svm[continuous_features + [f'{cont}_class' for cont in continuous_features]].copy(deep=True)\n    \n    svm_parameters = {\n        'predictors': continuous_features + [f'{cont}_class' for cont in continuous_features],\n        'target': target,\n        'model': model,\n        'model_parameters': {\n            'C': 0.5\n        }\n    }\n\n    svm_model = LinearModels(**svm_parameters)\n    svm_model.run(X_train_svm, y_train_svm, X_test_svm)\n\n    del df_train_svm, df_test_svm, X_train_svm, y_train_svm, X_test_svm\n    del svm_preprocessor, svm_parameters, svm_model\n    \n    print('Saving SVM OOF and Test predictions to current working directory.')\n    df_train_processed[['id', 'SVMPredictions']].to_csv('svm_oof_predictions.csv', index=False)\n    df_test_processed[['id', 'SVMPredictions']].to_csv('svm_test_predictions.csv', index=False)\n    \nelse:\n    print('Loading SVM OOF and Test predictions from current working directory.')\n    df_train_processed['SVMPredictions'] = pd.read_csv('..\/input\/tabular-playground-series\/Tabular Playground Series - Jan 2021\/svm_oof_predictions.csv')['SVMPredictions']\n    df_test_processed['SVMPredictions'] = pd.read_csv('..\/input\/tabular-playground-series\/Tabular Playground Series - Jan 2021\/svm_test_predictions.csv')['SVMPredictions']    \n    oof_score = mean_squared_error(df_train_processed['target'], df_train_processed['SVMPredictions'], squared=False)\n    print(f'SVM OOF RMSE: {oof_score:.6}')\n    LinearModels._plot_predictions(None, df_train_processed[target], df_train_processed['SVMPredictions'], df_test_processed['SVMPredictions'])\n","93ccb236":"class NeuralNetworks:\n    \n    def __init__(self, predictors, target, model, model_parameters, seeds):\n        \n        self.predictors = predictors\n        self.target = target\n               \n        self.model = model\n        self.model_parameters = model_parameters\n        self.seeds = seeds\n                \n    def _set_seed(self, seed):\n        \n        random.seed(seed)\n        np.random.seed(seed)\n        tf.random.set_seed(seed)\n        os.environ['PYTHONHASHSEED'] = str(seed)\n                \n    def _rmse_loss(self, y_true, y_pred):\n        return K.sqrt(K.mean(K.square(y_true - y_pred)))\n        \n    def _dense_block(self, x, units, activation, dropout_rate, kernel_regularizer, batch_normalization, weight_normalization, gaussian_noise):\n        \n        if weight_normalization:\n            x = tfa.layers.WeightNormalization(Dense(units=units, activation=activation, kernel_regularizer=kernel_regularizer))(x)\n        else:\n            x = Dense(units=units, activation=activation, kernel_regularizer=kernel_regularizer)(x)\n            \n        if batch_normalization:\n            x = BatchNormalization()(x)\n        if dropout_rate > 0:\n            x = Dropout(rate=dropout_rate)(x)\n        if gaussian_noise > 0:\n            x = GaussianNoise(gaussian_noise)(x)\n            \n        return x\n    \n    def _get_tmlp(self, input_shape, output_shape):        \n        \n        inputs = []\n        \n        cont1_discretized_input = Input(shape=(1, ), name='cont1_discretized_input')\n        cont1_embedding_dim = 2\n        cont1_discretized_embedding = Embedding(input_dim=4, output_dim=cont1_embedding_dim, input_length=1, name='cont1_discretized_embedding')(cont1_discretized_input)\n        cont1_discretized_embedding = Reshape(target_shape=(cont1_embedding_dim, ))(cont1_discretized_embedding)\n        inputs.append(cont1_discretized_embedding)\n        \n        cont2_discretized_input = Input(shape=(1, ), name='cont2_discretized_input')\n        cont2_embedding_dim = 4\n        cont2_discretized_embedding = Embedding(input_dim=10, output_dim=cont2_embedding_dim, input_length=1, name='cont2_discretized_embedding')(cont2_discretized_input)\n        cont2_discretized_embedding = Reshape(target_shape=(cont2_embedding_dim, ))(cont2_discretized_embedding)\n        inputs.append(cont2_discretized_embedding)\n        \n        cont3_discretized_input = Input(shape=(1, ), name='cont3_discretized_input')\n        cont3_embedding_dim = 3\n        cont3_discretized_embedding = Embedding(input_dim=6, output_dim=cont3_embedding_dim, input_length=1, name='cont3_discretized_embedding')(cont3_discretized_input)\n        cont3_discretized_embedding = Reshape(target_shape=(cont3_embedding_dim, ))(cont3_discretized_embedding)\n        inputs.append(cont3_discretized_embedding)\n        \n        cont4_discretized_input = Input(shape=(1, ), name='cont4_discretized_input')\n        cont4_embedding_dim = 2\n        cont4_discretized_embedding = Embedding(input_dim=4, output_dim=cont4_embedding_dim, input_length=1, name='cont4_discretized_embedding')(cont4_discretized_input)\n        cont4_discretized_embedding = Reshape(target_shape=(cont4_embedding_dim, ))(cont4_discretized_embedding)\n        inputs.append(cont4_discretized_embedding)\n        \n        cont5_discretized_input = Input(shape=(1, ), name='cont5_discretized_input')\n        cont5_embedding_dim = 2\n        cont5_discretized_embedding = Embedding(input_dim=3, output_dim=cont5_embedding_dim, input_length=1, name='cont5_discretized_embedding')(cont5_discretized_input)\n        cont5_discretized_embedding = Reshape(target_shape=(cont5_embedding_dim, ))(cont5_discretized_embedding)\n        inputs.append(cont5_discretized_embedding)\n        \n        cont6_discretized_input = Input(shape=(1, ), name='cont6_discretized_input')\n        cont6_embedding_dim = 2\n        cont6_discretized_embedding = Embedding(input_dim=2, output_dim=cont6_embedding_dim, input_length=1, name='cont6_discretized_embedding')(cont6_discretized_input)\n        cont6_discretized_embedding = Reshape(target_shape=(cont6_embedding_dim, ))(cont6_discretized_embedding)\n        inputs.append(cont6_discretized_embedding)\n        \n        cont7_discretized_input = Input(shape=(1, ), name='cont7_discretized_input')\n        cont7_embedding_dim = 2\n        cont7_discretized_embedding = Embedding(input_dim=3, output_dim=cont7_embedding_dim, input_length=1, name='cont7_discretized_embedding')(cont7_discretized_input)\n        cont7_discretized_embedding = Reshape(target_shape=(cont7_embedding_dim, ))(cont7_discretized_embedding)\n        inputs.append(cont7_discretized_embedding)\n        \n        cont8_discretized_input = Input(shape=(1, ), name='cont8_discretized_input')\n        cont8_embedding_dim = 2\n        cont8_discretized_embedding = Embedding(input_dim=4, output_dim=cont8_embedding_dim, input_length=1, name='cont8_discretized_embedding')(cont8_discretized_input)\n        cont8_discretized_embedding = Reshape(target_shape=(cont8_embedding_dim, ))(cont8_discretized_embedding)\n        inputs.append(cont8_discretized_embedding)\n        \n        cont9_discretized_input = Input(shape=(1, ), name='cont9_discretized_input')\n        cont9_embedding_dim = 2\n        cont9_discretized_embedding = Embedding(input_dim=4, output_dim=cont9_embedding_dim, input_length=1, name='cont9_discretized_embedding')(cont9_discretized_input)\n        cont9_discretized_embedding = Reshape(target_shape=(cont9_embedding_dim, ))(cont9_discretized_embedding)\n        inputs.append(cont9_discretized_embedding)\n        \n        cont10_discretized_input = Input(shape=(1, ), name='cont10_discretized_input')\n        cont10_embedding_dim = 3\n        cont10_discretized_embedding = Embedding(input_dim=8, output_dim=cont10_embedding_dim, input_length=1, name='cont10_discretized_embedding')(cont10_discretized_input)\n        cont10_discretized_embedding = Reshape(target_shape=(cont10_embedding_dim, ))(cont10_discretized_embedding)\n        inputs.append(cont10_discretized_embedding)\n        \n        cont11_discretized_input = Input(shape=(1, ), name='cont11_discretized_input')\n        cont11_embedding_dim = 3\n        cont11_discretized_embedding = Embedding(input_dim=5, output_dim=cont11_embedding_dim, input_length=1, name='cont11_discretized_embedding')(cont11_discretized_input)\n        cont11_discretized_embedding = Reshape(target_shape=(cont11_embedding_dim, ))(cont11_discretized_embedding)\n        inputs.append(cont11_discretized_embedding)\n        \n        cont12_discretized_input = Input(shape=(1, ), name='cont12_discretized_input')\n        cont12_embedding_dim = 2\n        cont12_discretized_embedding = Embedding(input_dim=4, output_dim=cont12_embedding_dim, input_length=1, name='cont12_discretized_embedding')(cont12_discretized_input)\n        cont12_discretized_embedding = Reshape(target_shape=(cont12_embedding_dim, ))(cont12_discretized_embedding)\n        inputs.append(cont12_discretized_embedding)\n        \n        cont13_discretized_input = Input(shape=(1, ), name='cont13_discretized_input')\n        cont13_embedding_dim = 3\n        cont13_discretized_embedding = Embedding(input_dim=6, output_dim=cont13_embedding_dim, input_length=1, name='cont13_discretized_embedding')(cont13_discretized_input)\n        cont13_discretized_embedding = Reshape(target_shape=(cont13_embedding_dim, ))(cont13_discretized_embedding)\n        inputs.append(cont13_discretized_embedding)\n        \n        cont14_discretized_input = Input(shape=(1, ), name='cont14_discretized_input')\n        cont14_embedding_dim = 3\n        cont14_discretized_embedding = Embedding(input_dim=6, output_dim=cont14_embedding_dim, input_length=1, name='cont14_discretized_embedding')(cont14_discretized_input)\n        cont14_discretized_embedding = Reshape(target_shape=(cont14_embedding_dim, ))(cont14_discretized_embedding)\n        inputs.append(cont14_discretized_embedding)\n        \n        continuous_inputs = Input(shape=(14, ), name='continuous_inputs')\n        inputs.append(continuous_inputs)\n        \n        sparse_inputs = Input(shape=(input_shape - (14 * 2), ), name='sparse_inputs')\n        inputs.append(sparse_inputs)\n        \n        x = Concatenate()(inputs)\n        x = self._dense_block(x, units=256, activation='elu', dropout_rate=0.50, kernel_regularizer=None, batch_normalization=True, weight_normalization=True, gaussian_noise=0.01)        \n        x = self._dense_block(x, units=256, activation='elu', dropout_rate=0.50, kernel_regularizer=None, batch_normalization=True, weight_normalization=True, gaussian_noise=0.01)        \n        outputs = Dense(output_shape, activation='linear')(x)\n        \n        model = Model(inputs=[continuous_inputs,\n                              cont1_discretized_input,\n                              cont2_discretized_input,\n                              cont3_discretized_input,\n                              cont4_discretized_input,\n                              cont5_discretized_input,\n                              cont6_discretized_input,\n                              cont7_discretized_input,\n                              cont8_discretized_input,\n                              cont9_discretized_input,\n                              cont10_discretized_input,\n                              cont11_discretized_input,\n                              cont12_discretized_input,\n                              cont13_discretized_input,\n                              cont14_discretized_input, \n                              sparse_inputs],\n                      outputs=outputs,\n                      name='TMLP')                \n        \n        optimizer = tfa.optimizers.AdamW(learning_rate=self.model_parameters['learning_rate'], weight_decay=self.model_parameters['weight_decay'])          \n        model.compile(optimizer=optimizer, loss=self._rmse_loss, metrics=None)\n        \n        return model\n    \n    def _get_rmlp(self, input_shape, output_shape):\n        \n        inputs = Input(shape=(input_shape, ), name='inputs')\n        x = self._dense_block(inputs, units=128, activation='elu', dropout_rate=0.30, kernel_regularizer=None, batch_normalization=True, weight_normalization=False, gaussian_noise=0)        \n        x = self._dense_block(x, units=128, activation='elu', dropout_rate=0.30, kernel_regularizer=None, batch_normalization=True, weight_normalization=False, gaussian_noise=0)        \n        outputs = Dense(output_shape, activation='linear')(x)\n        \n        model = Model(inputs=inputs, outputs=outputs, name='RMLP')                \n        \n        optimizer = Adam(learning_rate=self.model_parameters['learning_rate'])          \n        model.compile(optimizer=optimizer, loss=self._rmse_loss, metrics=None)\n        \n        return model\n        \n    def train_and_predict_tmlp(self, X_train, y_train, X_test):\n        \n        all_model_histories = {}\n        seed_avg_oof_predictions = np.zeros(X_train.shape[0])\n        seed_avg_test_predictions = np.zeros(X_test.shape[0])\n                                        \n        oof_predictions = np.zeros(X_train.shape[0])\n        test_predictions = np.zeros(X_test.shape[0])\n        \n        for seed in self.seeds:\n            \n            X = pd.concat([X_train[continuous_features], X_test[continuous_features]], ignore_index=True, axis=0)\n            embedder = RandomTreesEmbedding(n_estimators=300,\n                                            max_depth=3,\n                                            min_samples_split=100,\n                                            n_jobs=-1,\n                                            random_state=seed, \n                                            verbose=True)\n            embedder.fit(X)        \n            del X\n\n            train_sparse_features = embedder.transform(X_train.loc[:, continuous_features]).astype(np.uint8).toarray()\n            X_train = pd.concat([X_train, pd.DataFrame(train_sparse_features)], axis=1)\n            sparse_feature_columns = list(np.arange(train_sparse_features.shape[1]))\n            self.predictors += sparse_feature_columns\n            del train_sparse_features\n\n            test_sparse_features = embedder.transform(X_test.loc[:, continuous_features]).astype(np.uint8).toarray()\n            del embedder\n            X_test = pd.concat([X_test, pd.DataFrame(test_sparse_features)], axis=1)\n            del test_sparse_features\n\n            print(f'{\"-\" * 30}\\nRunning TMLP model with seed: {seed}\\n{\"-\" * 30}\\n')\n            self._set_seed(seed)\n            model_histories = []\n        \n            for fold in sorted(X_train['fold'].unique()):\n\n                trn_idx, val_idx = X_train.loc[X_train['fold'] != fold].index, X_train.loc[X_train['fold'] == fold].index\n                X_trn, y_trn = X_train.loc[trn_idx, self.predictors], y_train.loc[trn_idx]\n                X_val, y_val = X_train.loc[val_idx, self.predictors], y_train.loc[val_idx]\n\n                model = self._get_mlp(input_shape=X_trn.shape[1], output_shape=1)\n                reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n                                              factor=self.model_parameters['reduce_lr_factor'],\n                                              patience=self.model_parameters['reduce_lr_patience'],\n                                              min_lr=self.model_parameters['reduce_lr_min'],\n                                              mode='min',\n                                              verbose=True)\n                early_stopping = EarlyStopping(monitor='val_loss',\n                                               mode='min',\n                                               min_delta=self.model_parameters['early_stopping_min_delta'],\n                                               patience=self.model_parameters['early_stopping_patience'],\n                                               restore_best_weights=True, \n                                               verbose=True)\n                \n                history = model.fit([X_trn[self.predictors[:14]],\n                                     X_trn['cont1_class'],\n                                     X_trn['cont2_class'],\n                                     X_trn['cont3_class'],\n                                     X_trn['cont4_class'],\n                                     X_trn['cont5_class'],\n                                     X_trn['cont6_class'],\n                                     X_trn['cont7_class'],\n                                     X_trn['cont8_class'],\n                                     X_trn['cont9_class'],\n                                     X_trn['cont10_class'],\n                                     X_trn['cont11_class'],\n                                     X_trn['cont12_class'],\n                                     X_trn['cont13_class'],\n                                     X_trn['cont14_class'], \n                                     X_trn[sparse_feature_columns]],\n                                    y_trn,\n                                    validation_data=([X_val[self.predictors[:14]],\n                                                      X_val['cont1_class'],\n                                                      X_val['cont2_class'],\n                                                      X_val['cont3_class'],\n                                                      X_val['cont4_class'],\n                                                      X_val['cont5_class'],\n                                                      X_val['cont6_class'],\n                                                      X_val['cont7_class'],\n                                                      X_val['cont8_class'],\n                                                      X_val['cont9_class'],\n                                                      X_val['cont10_class'],\n                                                      X_val['cont11_class'],\n                                                      X_val['cont12_class'],\n                                                      X_val['cont13_class'],\n                                                      X_val['cont14_class'], \n                                                      X_val[sparse_feature_columns]],\n                                                     y_val),\n                                    epochs=self.model_parameters['epochs'], \n                                    batch_size=self.model_parameters['batch_size'],\n                                    callbacks=[reduce_lr, early_stopping],\n                                    verbose=True)\n\n                val_predictions = model.predict([X_val[self.predictors[:14]],\n                                                 X_val['cont1_class'],\n                                                 X_val['cont2_class'],\n                                                 X_val['cont3_class'],\n                                                 X_val['cont4_class'],\n                                                 X_val['cont5_class'],\n                                                 X_val['cont6_class'],\n                                                 X_val['cont7_class'],\n                                                 X_val['cont8_class'],\n                                                 X_val['cont9_class'],\n                                                 X_val['cont10_class'],\n                                                 X_val['cont11_class'],\n                                                 X_val['cont12_class'],\n                                                 X_val['cont13_class'],\n                                                 X_val['cont14_class'],\n                                                 X_val[sparse_feature_columns]])\n                seed_avg_oof_predictions[val_idx] += (val_predictions.flatten() \/ len(self.seeds))\n                test_predictions = model.predict([X_test[self.predictors[:14]],\n                                                  X_test['cont1_class'],\n                                                  X_test['cont2_class'],\n                                                  X_test['cont3_class'],\n                                                  X_test['cont4_class'],\n                                                  X_test['cont5_class'],\n                                                  X_test['cont6_class'],\n                                                  X_test['cont7_class'],\n                                                  X_test['cont8_class'],\n                                                  X_test['cont9_class'],\n                                                  X_test['cont10_class'],\n                                                  X_test['cont11_class'],\n                                                  X_test['cont12_class'],\n                                                  X_test['cont13_class'],\n                                                  X_test['cont14_class'],\n                                                  X_test[sparse_feature_columns]])\n                seed_avg_test_predictions += (test_predictions.flatten() \/ X_train['fold'].nunique() \/ len(self.seeds))\n                model_histories.append(history)\n\n                fold_score = mean_squared_error(df_train_processed.loc[val_idx, self.target], val_predictions, squared=False)\n                print(f'\\nTMLP Fold {int(fold)} - X_trn: {X_train.loc[trn_idx, self.predictors].shape} X_val: {X_train.loc[val_idx, self.predictors].shape} - Score: {fold_score:.6} - Seed: {seed}\\n')\n            \n            all_model_histories[seed] = model_histories\n            \n        df_train_processed['TMLPPredictions'] = seed_avg_oof_predictions\n        df_test_processed['TMLPPredictions'] = seed_avg_test_predictions\n        oof_score = mean_squared_error(y_train, df_train_processed['TMLPPredictions'], squared=False)\n        print(f'{\"-\" * 30}\\nTMLP OOF RMSE: {oof_score:.6} ({len(self.seeds)} Seed Average) \\n{\"-\" * 30}')\n        \n        self._plot_learning_curve(all_model_histories)\n        self._plot_predictions(df_train_processed[target], df_train_processed['TMLPPredictions'], df_test_processed['TMLPPredictions'])\n        \n    def train_and_predict_rmlp(self, X_train, y_train, X_test):\n        \n        all_model_histories = {}\n        seed_avg_oof_predictions = np.zeros(X_train.shape[0])\n        seed_avg_test_predictions = np.zeros(X_test.shape[0])\n                                        \n        oof_predictions = np.zeros(X_train.shape[0])\n        test_predictions = np.zeros(X_test.shape[0])\n        \n        for seed in self.seeds:\n            \n            print(f'{\"-\" * 30}\\nRunning RMLP model with seed: {seed}\\n{\"-\" * 30}\\n')\n            self._set_seed(seed)\n            model_histories = []\n        \n            for fold in sorted(X_train['fold'].unique()):\n\n                trn_idx, val_idx = X_train.loc[X_train['fold'] != fold].index, X_train.loc[X_train['fold'] == fold].index\n                X_trn, y_trn = X_train.loc[trn_idx, self.predictors], y_train.loc[trn_idx]\n                X_val, y_val = X_train.loc[val_idx, self.predictors], y_train.loc[val_idx]\n\n                model = self._get_rmlp(input_shape=X_trn.shape[1], output_shape=1)\n                reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n                                              factor=self.model_parameters['reduce_lr_factor'],\n                                              patience=self.model_parameters['reduce_lr_patience'],\n                                              min_lr=self.model_parameters['reduce_lr_min'],\n                                              mode='min',\n                                              verbose=True)\n                early_stopping = EarlyStopping(monitor='val_loss',\n                                               mode='min',\n                                               min_delta=self.model_parameters['early_stopping_min_delta'],\n                                               patience=self.model_parameters['early_stopping_patience'],\n                                               restore_best_weights=True, \n                                               verbose=True)\n                \n                history = model.fit(X_trn, y_trn,\n                                    validation_data=(X_val, y_val),\n                                    epochs=self.model_parameters['epochs'], \n                                    batch_size=self.model_parameters['batch_size'],\n                                    callbacks=[reduce_lr, early_stopping],\n                                    verbose=True)\n\n                val_predictions = model.predict(X_val)\n                seed_avg_oof_predictions[val_idx] += (val_predictions.flatten() \/ len(self.seeds))\n                test_predictions = model.predict(X_test)\n                seed_avg_test_predictions += (test_predictions.flatten() \/ X_train['fold'].nunique() \/ len(self.seeds))\n                model_histories.append(history)\n\n                fold_score = mean_squared_error(df_train_processed.loc[val_idx, self.target], val_predictions.flatten(), squared=False)\n                print(f'\\nRMLP Fold {int(fold)} - X_trn: {X_train.loc[trn_idx, self.predictors].shape} X_val: {X_train.loc[val_idx, self.predictors].shape} - Score: {fold_score:.6} - Seed: {seed}\\n')\n            \n            all_model_histories[seed] = model_histories\n            \n        df_train_processed['RMLPPredictions'] = seed_avg_oof_predictions\n        df_test_processed['RMLPPredictions'] = seed_avg_test_predictions\n        oof_score = mean_squared_error(y_train, df_train_processed['RMLPPredictions'], squared=False)\n        print(f'{\"-\" * 30}\\nRMLP OOF RMSE: {oof_score:.6} ({len(self.seeds)} Seed Average) \\n{\"-\" * 30}')\n        \n        self._plot_learning_curve(all_model_histories)\n        self._plot_predictions(df_train_processed[target], df_train_processed['RMLPPredictions'], df_test_processed['RMLPPredictions'])\n        \n    def _plot_learning_curve(self, all_model_histories):\n    \n        n_folds = 5\n        fig, axes = plt.subplots(nrows=n_folds, figsize=(32, 50), dpi=100)\n\n        for i in range(n_folds):\n\n            for seed, histories in all_model_histories.items():\n\n                axes[i].plot(np.arange(1, len(histories[i].history['loss']) + 1), histories[i].history['loss'], label=f'train_loss (Seed: {seed})', alpha=0.5)\n                axes[i].plot(np.arange(1, len(histories[i].history['val_loss']) + 1), histories[i].history['val_loss'], label=f'val_loss (Seed: {seed})', alpha=0.5)    \n\n            axes[i].set_xlabel('Epochs', size=20)\n            axes[i].set_ylabel('RMSE', size=20)\n            axes[i].set_yscale('log')\n            axes[i].tick_params(axis='x', labelsize=20)\n            axes[i].tick_params(axis='y', labelsize=20)\n            axes[i].legend(prop={'size': 20}) \n            axes[i].set_title(f'Fold {i + 1} Learning Curve', fontsize=20, pad=10)\n\n        plt.show()\n        \n    def _plot_predictions(self, train_labels, train_predictions, test_predictions):\n        \n        fig, axes = plt.subplots(ncols=2, figsize=(25, 6))                                            \n        sns.scatterplot(train_labels, train_predictions, ax=axes[0])\n        sns.distplot(train_predictions, label='Train Predictions', ax=axes[1])\n        sns.distplot(test_predictions, label='Test Predictions', ax=axes[1])\n\n        axes[0].set_xlabel(f'Train Labels', size=18)\n        axes[0].set_ylabel(f'Train Predictions', size=18)\n        axes[1].set_xlabel('')\n        axes[1].legend(prop={'size': 18})\n        for i in range(2):\n            axes[i].tick_params(axis='x', labelsize=15)\n            axes[i].tick_params(axis='y', labelsize=15)\n        axes[0].set_title(f'Train Labels vs Train Predictions', size=20, pad=20)\n        axes[1].set_title(f'Predictions Distributions', size=20, pad=20)\n            \n        plt.show()\n","e706dfcf":"TRAIN_TMLP = False\n\nif TRAIN_TMLP:\n    model = 'TMLP'\n    tmlp_preprocessor = Preprocessor(train=df_train, test=df_test,\n                                     n_splits=5, shuffle=True, random_state=cross_validation_seed, scaler=StandardScaler,\n                                     create_features=False, discretize_features=True)\n    df_train_tmlp, df_test_tmlp = tmlp_preprocessor.transform()\n\n    print(f'\\n{model} Training Set Shape = {df_train_tmlp.shape}')\n    print(f'{model} Training Set Memory Usage = {df_train_tmlp.memory_usage().sum() \/ 1024 ** 2:.2f} MB')\n    print(f'{model} Test Set Shape = {df_test_tmlp.shape}')\n    print(f'{model} Test Set Memory Usage = {df_test_tmlp.memory_usage().sum() \/ 1024 ** 2:.2f} MB\\n')\n\n    X_train_tmlp = df_train_tmlp[continuous_features + ['fold'] + [f'{cont}_class' for cont in continuous_features]].copy(deep=True)\n    y_train_tmlp = df_train_tmlp[target].copy(deep=True)\n    X_test_tmlp = df_test_tmlp[continuous_features + [f'{cont}_class' for cont in continuous_features]].copy(deep=True)\n            \n    tmlp_parameters = {        \n        'predictors': continuous_features + [f'{cont}_class' for cont in continuous_features],\n        'target': target,\n        'model': model,\n        'model_parameters': {\n            'learning_rate': 0.0009,\n            'weight_decay': 0.000001,\n            'epochs': 150,\n            'batch_size': 2 ** 10,\n            'reduce_lr_factor': 0.8,\n            'reduce_lr_patience': 5,\n            'reduce_lr_min': 0.000001,\n            'early_stopping_min_delta': 0.0001,\n            'early_stopping_patience': 15\n        },\n        'seeds': [541992]\n    }\n        \n    tmlp_model = NeuralNetworks(**tmlp_parameters)\n    tmlp_model.train_and_predict_tmlp(X_train_tmlp, y_train_tmlp, X_test_tmlp)\n    \n    del df_train_tmlp, df_test_tmlp, X_train_tmlp, y_train_tmlp, X_test_tmlp\n    del tmlp_preprocessor, tmlp_parameters, tmlp_model\n    \n    print('Saving TMLP OOF and Test predictions to current working directory.')\n    df_train_processed[['id', 'TMLPPredictions']].to_csv('tmlp_oof_predictions.csv', index=False)\n    df_test_processed[['id', 'TMLPPredictions']].to_csv('tmlp_test_predictions.csv', index=False)\n    \nelse:\n    print('Loading TMLP OOF and Test predictions from current working directory.')\n    df_train_processed['TMLPPredictions'] = pd.read_csv('..\/input\/tabular-playground-series\/Tabular Playground Series - Jan 2021\/tmlp_oof_predictions.csv')['MLPPredictions']\n    df_test_processed['TMLPPredictions'] = pd.read_csv('..\/input\/tabular-playground-series\/Tabular Playground Series - Jan 2021\/tmlp_test_predictions.csv')['MLPPredictions']    \n    oof_score = mean_squared_error(df_train_processed['target'], df_train_processed['TMLPPredictions'], squared=False)\n    print(f'TMLP OOF RMSE: {oof_score:.6}')\n    NeuralNetworks._plot_predictions(None, df_train_processed[target], df_train_processed['TMLPPredictions'], df_test_processed['TMLPPredictions'])\n","92bf5081":"TRAIN_RMLP = False\n\nif TRAIN_RMLP:\n    model = 'RMLP'\n    rmlp_preprocessor = Preprocessor(train=df_train, test=df_test,\n                                     n_splits=5, shuffle=True, random_state=cross_validation_seed, scaler=StandardScaler,\n                                     create_features=False, discretize_features=False)\n    df_train_rmlp, df_test_rmlp = rmlp_preprocessor.transform()\n    \n    for feature in continuous_features:\n        df_train_rmlp[f'{feature}_square'] = df_train_rmlp[feature] ** 2\n        df_test_rmlp[f'{feature}_square'] = df_test_rmlp[feature] ** 2\n    \n    print(f'\\n{model} Training Set Shape = {df_train_rmlp.shape}')\n    print(f'{model} Training Set Memory Usage = {df_train_rmlp.memory_usage().sum() \/ 1024 ** 2:.2f} MB')\n    print(f'{model} Test Set Shape = {df_test_rmlp.shape}')\n    print(f'{model} Test Set Memory Usage = {df_test_rmlp.memory_usage().sum() \/ 1024 ** 2:.2f} MB\\n')\n\n    X_train_rmlp = df_train_rmlp[continuous_features + ['fold'] + [f'{feat}_square' for feat in continuous_features]].copy(deep=True)\n    y_train_rmlp = df_train_rmlp[target].copy(deep=True)\n    X_test_rmlp = df_test_rmlp[continuous_features + [f'{feat}_square' for feat in continuous_features]].copy(deep=True)\n                \n    rmlp_parameters = {        \n        'predictors': continuous_features + [f'{feat}_square' for feat in continuous_features],\n        'target': target,\n        'model': model,\n        'model_parameters': {\n            'learning_rate': 0.01,\n            'epochs': 150,\n            'batch_size': 2 ** 10,\n            'reduce_lr_factor': 0.8,\n            'reduce_lr_patience': 5,\n            'reduce_lr_min': 0.000001,\n            'early_stopping_min_delta': 0.0001,\n            'early_stopping_patience': 15\n        },\n        'seeds': [541992, 721991, 1337, 42, 0]\n    }\n        \n    rmlp_model = NeuralNetworks(**rmlp_parameters)\n    rmlp_model.train_and_predict_rmlp(X_train_rmlp, y_train_rmlp, X_test_rmlp)\n    \n    del df_train_rmlp, df_test_rmlp, X_train_rmlp, y_train_rmlp, X_test_rmlp\n    del rmlp_preprocessor, rmlp_parameters, rmlp_model\n    \n    print('Saving RMLP OOF and Test predictions to current working directory.')\n    df_train_processed[['id', 'RMLPPredictions']].to_csv('rmlp_oof_predictions.csv', index=False)\n    df_test_processed[['id', 'RMLPPredictions']].to_csv('rmlp_test_predictions.csv', index=False)\n    \nelse:\n    print('Loading RMLP OOF and Test predictions from current working directory.')\n    df_train_processed['RMLPPredictions'] = pd.read_csv('..\/input\/tabular-playground-series\/Tabular Playground Series - Jan 2021\/rmlp_oof_predictions.csv')['RMLPPredictions']\n    df_test_processed['RMLPPredictions'] = pd.read_csv('..\/input\/tabular-playground-series\/Tabular Playground Series - Jan 2021\/rmlp_test_predictions.csv')['RMLPPredictions']    \n    oof_score = mean_squared_error(df_train_processed['target'], df_train_processed['RMLPPredictions'], squared=False)\n    print(f'RMLP OOF RMSE: {oof_score:.6}')\n    NeuralNetworks._plot_predictions(None, df_train_processed[target], df_train_processed['RMLPPredictions'], df_test_processed['RMLPPredictions'])\n","9d197e5b":"prediction_columns = [col for col in df_train_processed.columns if col.endswith('Predictions')]\n\nfig = plt.figure(figsize=(12, 12), dpi=100)\nsns.heatmap(df_train_processed[prediction_columns + [target]].corr(),\n            annot=True,\n            square=True,\n            cmap='coolwarm',\n            annot_kws={'size': 15},\n            fmt='.4f')\n\nplt.tick_params(axis='x', labelsize=18, rotation=90)\nplt.tick_params(axis='y', labelsize=18, rotation=0)\nplt.title('Prediction Correlations', size=20, pad=20)\n\nplt.show()","d5499a3d":"class SubmissionPipeline:\n    \n    def __init__(self, train, test, blend, prediction_columns, add_public_best):\n        \n        self.train = train\n        self.test = test\n        self.blend = blend\n        self.prediction_columns = prediction_columns\n        self.add_public_best = add_public_best\n        \n    def weighted_average(self):\n        \n        self.train['FinalPredictions'] = (0.77 * self.train['LGBPredictions']) +\\\n                                         (0.04 * self.train['CBPredictions']) +\\\n                                         (0.06 * self.train['XGBPredictions']) +\\\n                                         (0.0 * self.train['RFPredictions']) +\\\n                                         (0.0 * self.train['RRPredictions']) +\\\n                                         (0.0 * self.train['SVMPredictions']) +\\\n                                         (0.12 * self.train['TMLPPredictions']) +\\\n                                         (0.01 * self.train['RMLPPredictions'])\n        \n        self.test['FinalPredictions'] = (0.77 * self.test['LGBPredictions']) +\\\n                                        (0.04 * self.test['CBPredictions']) +\\\n                                        (0.06 * self.test['XGBPredictions']) +\\\n                                        (0.0 * self.test['RFPredictions']) +\\\n                                        (0.0 * self.test['RRPredictions']) +\\\n                                        (0.0 * self.test['SVMPredictions']) +\\\n                                        (0.12 * self.test['TMLPPredictions']) +\\\n                                        (0.01 * self.test['RMLPPredictions'])\n        \n    def geometric_average(self):\n        \n        self.train['FinalPredictions'] = gmean(self.train[self.prediction_columns], axis=1)\n        self.test['FinalPredictions'] = gmean(self.test[self.prediction_columns], axis=1)\n        \n    def weighted_average_public(self):\n        \n        public_best = pd.read_csv('..\/input\/h2o-machine-learning\/submission.csv')\n        self.test['FinalPredictions'] = (self.test['FinalPredictions'] * 0.02) + (public_best['target'] * 0.98) \n        \n    def transform(self):\n        \n        if self.blend == 'weighted_average':\n            self.weighted_average()\n        elif self.blend == 'geometric_average':\n            self.geometric_average()\n            \n        for prediction_column in prediction_columns:\n            oof_score = mean_squared_error(self.train[target], df_train_processed[prediction_column], squared=False)\n            print(f'{prediction_column.split(\"Predictions\")[0]} OOF RMSE: {oof_score:.6}')\n        final_oof_score = mean_squared_error(self.train[target], df_train_processed['FinalPredictions'], squared=False)\n        print(f'{\"-\" * 30}\\nFinal OOF RMSE: {final_oof_score:.6}\\n{\"-\" * 30}')\n                \n        if self.add_public_best:\n            self.weighted_average_public()\n                \n        return self.train[['id'] + self.prediction_columns + ['FinalPredictions']].copy(deep=True), self.test[['id'] + self.prediction_columns + ['FinalPredictions']].copy(deep=True)\n            \n\n\nsubmission_pipeline = SubmissionPipeline(train=df_train_processed, test=df_test_processed,\n                                         blend='weighted_average', prediction_columns=prediction_columns, add_public_best=True)   \ndf_train_submission, df_test_submission = submission_pipeline.transform()","0dd5b0b0":"df_test_processed['target'] = df_test_submission['FinalPredictions']\ndf_test_processed[['id', 'target']].to_csv('submission.csv', index=False)\ndf_test_processed[['id', 'target']].describe()","c9153fc6":"### 6.1 Tree MLP","fff1d0cf":"### 4.2 CatBoost","4a66778d":"## 1. Target\n\n`target` is the name of target feature. It follows an extremely left tailed bimodal distribution. Target mean and median are very close to each other because there are very few outliers which can be seen from the probability plot. Those two outliers are 0 and 3.7, and they should be dealt with.\n\nBimodal distribution can be break into two components with gaussian mixture model, but it is not possible to predict components of test set. At best, it can be done with 61% accuracy, which leads models to predict `target` closer to mean of two components in both training and test set. ","290e2754":"## 4. Tree-based Models\n\n4 tree-based models are used in the ensemble and they are LightGBM, CatBoost, XGBoost, and Random Forest. All of them are trained with more than 3 seeds for diversity. Only raw continuous features are used in tree-based models without any transformation. Even though the training logs are not present in this version of the notebook, those are the parameters used for getting the scores displayed below each cell.","0b7ec83f":"### 6.1 Raw MLP","737fe097":"### 4.1 LightGBM","495719a0":"### 5.1 Ridge Regression","d34afc4e":"### 5.2 SVM","c4bc3813":"This is the example usage of preprocessing pipeline. It returns processed training and test sets ready for model training, but those dataframes created in this cell are only used for storing model predictions. Models are trained with their custom datasets that designed specifically for them.","2a0708a5":"## 3. Preprocessing\n\n`Preprocessor` class incorporates the preprocessing steps such as cross-validation folds creation, outlier removal, standardization and feature engineering.\n\n* Two outliers data points in `target` that are less than 4, are dropped\n* Created 5 random split folds for cross-validation\n* Continuous features are standardized for linear and neural network models, but it is not necessary for tree-based models\n* Peak features are created for capturing vertical patterns in feature x target interactions but they didn't improve any of the model performances\n* argmax and argmin features are created for continuous features but they didn't improve any of the model performances\n* Continuous features are discretized with distribution components extracted by a gaussian mixture model ","785ad569":"### 4.4 Random Forest","10fe8d2e":"## 7. Submission","91549d92":"## Tabular Playground Series - Jan 2021","50eb9079":"## 6. Neural Networks","3c7d9fa9":"## 5. Linear Models\n\n2 linear models are used in the ensemble and they are Ridge Regression and SVM. Ridge is fitted to very high dimensional sparse features extracted by RandomTreesEmbedding, but can't be used on SVM because `cuml.SVM` doesn't support `scipy.sparse.csr_matrix` format. Even though the training logs are not present in this version of the notebook, those are the parameters used for getting the scores displayed below each cell.","0706d54e":"### 4.3 XGBoost","7126117a":"## 2. Features\n\nThere are 14 continuous features that are named from `cont1` to `cont14`. All of their distributions are different from each other, but they have one thing in common. All of them have multimodal distributions which means they have multiple peaks. Number of peaks changes from feature to feature. None of the features have any missing values and their distributions are very similar in training and test sets."}}