{"cell_type":{"c264d4d5":"code","0379f359":"code","e432388f":"code","bfd019b5":"code","f09e2264":"code","0421e68a":"code","61c16212":"code","75dae25a":"code","6ab497ec":"code","a72d91df":"code","0c6171af":"code","b2ff121b":"code","b0607a21":"code","2ac91aec":"code","45d370e6":"code","6b5d583c":"code","96ed3c8c":"code","c3b40a0c":"code","b8a1ce59":"code","8f38edd3":"code","e094e5e9":"code","a7594b04":"code","2d2f26dc":"code","a75a3b82":"code","99db60a6":"code","064efbd0":"code","f9464372":"code","2c0c84c4":"code","01c97d98":"code","f6bdd081":"code","d45314e2":"code","3e406964":"code","27c8b5da":"code","52b392c1":"code","70e98a46":"code","a5e2a225":"code","e7311d82":"code","5c210f93":"code","ba7aff83":"code","0b000bbc":"code","3284606c":"code","0ae78f48":"code","2942e2e9":"code","de0cf77b":"code","803ad674":"code","3f657e62":"code","5e0d0e0f":"code","f340c86d":"code","119eeb78":"code","0ac1f6f0":"code","11565521":"code","d881e4fc":"code","909b2545":"code","fdfa5541":"code","bd8e3a19":"code","ed938ecd":"code","37ff1f77":"code","44656984":"code","86745ced":"code","c467d36d":"code","a865c3a5":"code","10f22e66":"code","594390f8":"code","85f4b2e3":"code","090ef82f":"code","33283bd1":"code","fc61d97f":"code","c052103e":"code","77b123fb":"code","a229c83e":"code","471feb69":"code","9efef6bc":"code","9315b056":"code","d35fc174":"code","531fdb01":"code","6b1045ec":"code","1a995413":"code","cf4ef7e6":"code","1cdbdb0d":"code","30ec72a6":"code","da899f97":"code","662d6a21":"code","16d56e5d":"code","5736446c":"code","8ecc2b9b":"code","916bf102":"code","708c95c8":"code","83000246":"code","bcd68ab4":"code","928f10be":"code","71cf2af7":"code","96ee4dfe":"code","3adac31e":"code","6faca444":"code","d484446f":"code","275c8cf9":"code","66d9d694":"code","bd73079a":"code","4ea8b32c":"code","bb938b24":"code","7541a824":"code","401049f1":"markdown","1ffb0d80":"markdown","8bc0a22e":"markdown","af9c7dcd":"markdown","3a7a41d6":"markdown","c27d545a":"markdown","ef833243":"markdown","19254bf2":"markdown","4313192f":"markdown","9117100e":"markdown","18d4b7aa":"markdown","c1fde59f":"markdown","0e960ef8":"markdown","36c0c4ba":"markdown","23e49047":"markdown","61dd3e17":"markdown","3112ac4c":"markdown","396a3c70":"markdown","1c328f8f":"markdown","b0b92ab1":"markdown","fccb39f9":"markdown","a79dd891":"markdown","ceb3818d":"markdown","e23fb7a1":"markdown","424335b0":"markdown","09eccc8e":"markdown","74f19b53":"markdown","7e5d1e61":"markdown"},"source":{"c264d4d5":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","0379f359":"#loading loading weather data\nweatherAndCases = pd.read_csv('..\/input\/covidandpopulationdata\/training_data_with_weather_info_week_4.csv')\nweatherAndCases.head()","e432388f":"#loading testing data \ncovidTestingData = pd.read_csv('..\/input\/covidandpopulationdata\/owid-covid-data.csv')\ncovidTestingData.head()","bfd019b5":"#poplulation data \npopulationdata = pd.read_csv('..\/input\/covidandpopulationdata\/WPP2019_TotalPopulationBySex.csv')\npopulationdata.head()","f09e2264":"# all of our data is in 2020 and some columns are superflous \npopulationdata = populationdata[populationdata['Time']==2020]\npopulationdata = populationdata[['Location','Time','PopMale','PopFemale','PopTotal','PopDensity']]\npopulationdata = populationdata.drop_duplicates()\npopulationdata.head()","0421e68a":"#join the population and testing data\npopulationAndTesting = populationdata.merge(covidTestingData, left_on='Location', right_on='location')\npopulationAndTesting.head()","61c16212":"populationAndTesting.dtypes","75dae25a":"populationAndTesting.head()","6ab497ec":"#clearly some countries dont match up so lets fix that\n\nprint(len(populationdata['Location'].unique()))\nprint(len(covidTestingData['location'].unique()))\nprint(len(populationAndTesting['Location'].unique()))\nprint(set(covidTestingData['location'])-set(populationAndTesting['Location']))","a72d91df":"#most of these we can disregard since they are pretty small countries we wont use in our analyssi\n#but some countries we cant look past\n#United States, Hong Kong, Bolivia, Russia, etc.. \n#we will have to do a map and transform on these data points\n\ndef changecountryname(x):\n    countryMap = {\"United States of America\":\"United States\",\n             \"China, Hong Kong SAR\": \"Hong Kong\",\n             \"Bolivia (Plurinational State of)\":\"Bolivia\",\n             \"Russian Federation\":\"Russia\",\n             \"Viet Nam\": \"Vietnam\",\n             \"China, Taiwan Province of China\":\"Taiwan\",\n             \"Iran (Islamic Republic of)\":\"Iran\",\n             \"Republic of Korea\":\"South Korea\",\n             \"United Republic of Tanzania\": \"Tanzania\",\n             \"Czechia\":\"Czech Republic\",\n             \"Democratic Republic of the Congo\":\"Democratic Republic of Congo\",\n             \"Syrian Arab Republic\":\"Syria\",\n             \"Venezuela (Bolivarian Republic of)\":\"Venezuela\"}\n    if x in countryMap.keys():\n        return countryMap[x]\n    else:\n        return x\npopulationdata['New_Country_Code'] = populationdata.Location.apply(lambda x: changecountryname(x))","0c6171af":"populationdata.head()","b2ff121b":"#join the population and testing data\npopulationAndTesting = populationdata.merge(covidTestingData, left_on='New_Country_Code', right_on='location')\npopulationAndTesting.head()","b0607a21":"#Test new missing set\nprint(set(covidTestingData['location'])-set(populationAndTesting['New_Country_Code']))","2ac91aec":"populationAndTesting.dtypes","45d370e6":"#the weather data we're good with but we will be adding some \n#population information to standardize our points\npopulationandweather = populationdata.merge(weatherAndCases, left_on='New_Country_Code', right_on='Country_Region')\npopulationandweather.head()","6b5d583c":"#clearly some countries dont match up so lets fix that\nprint(len(populationdata['Location'].unique()))\nprint(len(weatherAndCases['Country_Region'].unique()))\nprint(len(populationandweather['Location'].unique()))\nprint(set(weatherAndCases['Country_Region'])-set(populationandweather['New_Country_Code']))","96ed3c8c":"#diamond princess and MS Zaandam are a cruise liners so we obvioulsy dont need that\n#from this list to simplify things we're only going to change the some of the countries\n\ndef changecountryname2(x):\n    countryMap = {\n        \"United States\":\"US\",\n        \"Taiwan\":\"Taiwan*\",\n        \"South Korea\":\"Korea, South\",\n        \"Czech Republic\":\"Czechia\"}\n    if x in countryMap.keys():\n        return countryMap[x]\n    else:\n        return x\npopulationdata['New_Country_Code_Weather'] = populationdata.New_Country_Code.apply(lambda x: changecountryname2(x))","c3b40a0c":"#the weather data we're good with but we will be adding some \n#population information to standardize our points\npopulationandweather = populationdata.merge(weatherAndCases, left_on='New_Country_Code_Weather', right_on='Country_Region')\npopulationandweather.head()","b8a1ce59":"#clearly some countries dont match up so lets fix that\nprint(len(populationdata['Location'].unique()))\nprint(len(weatherAndCases['Country_Region'].unique()))\nprint(len(populationandweather['Location'].unique()))\nprint(set(weatherAndCases['Country_Region'])-set(populationandweather['New_Country_Code_Weather']))","8f38edd3":"from dateutil import parser\npopulationAndTesting['DTDate'] = populationAndTesting.date.apply(lambda x: parser.parse(x))\npopulationAndTesting.head()","e094e5e9":"populationAndTesting.dtypes","a7594b04":"#cleaning up data\npopulationAndTesting = populationAndTesting[['Location','PopTotal','PopDensity',\n                                             'total_cases','total_deaths',\n                                             'total_tests','DTDate']]","2d2f26dc":"ax = sns.lineplot(x=\"DTDate\", y=\"total_deaths\", hue=\"Location\",\n                  data=populationAndTesting)","a75a3b82":"populationAndTesting = populationAndTesting.sort_values(by='PopTotal', ascending=False)\npopulationAndTesting.head()","99db60a6":"populationAndTestingWorld = populationAndTesting[populationAndTesting['Location']=='World']\npopulationAndTesting = populationAndTesting[populationAndTesting['Location']!='World']\npopulationAndTesting.head()","064efbd0":"topCountries = populationAndTesting.Location.unique()[:25]\ntopCountries = np.concatenate((topCountries,['Sweden', 'Spain']))\ntopCountries","f9464372":"populationAndTestingTop = populationAndTesting[populationAndTesting['Location'].isin(topCountries)]\npopulationAndTestingTop.head()","2c0c84c4":"ax = sns.lineplot(x=\"DTDate\", y=\"total_deaths\", hue=\"Location\",\n                  data=populationAndTestingTop)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.setp(ax.get_xticklabels(), fontsize=5)\nplt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n","01c97d98":"ax = sns.lineplot(x=\"DTDate\", y=\"total_tests\", hue=\"Location\",\n                  data=populationAndTestingTop)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.setp(ax.get_xticklabels(), fontsize=5)\nplt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n","f6bdd081":"#lets standardize total deaths and cases by population \n#population is in 1000\npopulationAndTestingTop['total_cases_perpopulation'] = populationAndTestingTop['total_cases'] \/ (1000*populationAndTestingTop['PopTotal'])\npopulationAndTestingTop['total_tests_perpopulation'] = populationAndTestingTop['total_tests'] \/ (1000*populationAndTestingTop['PopTotal'])\npopulationAndTestingTop['total_deaths_perpopulation'] = populationAndTestingTop['total_deaths'] \/ (1000*populationAndTestingTop['PopTotal'])\npopulationAndTestingTop.head()\n","d45314e2":"ax = sns.lineplot(x=\"DTDate\", y=\"total_cases_perpopulation\", hue=\"Location\",\n                  data=populationAndTestingTop.dropna())\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.setp(ax.get_xticklabels(), fontsize=5)\nplt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n","3e406964":"ax = sns.lineplot(x=\"DTDate\", y=\"total_tests_perpopulation\", hue=\"Location\",\n                  data=populationAndTestingTop.dropna())\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.setp(ax.get_xticklabels(), fontsize=5)\nplt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n","27c8b5da":"ax = sns.lineplot(x=\"DTDate\", y=\"total_deaths_perpopulation\", hue=\"Location\",\n                  data=populationAndTestingTop)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.setp(ax.get_xticklabels(), fontsize=5)\nplt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n","52b392c1":"#lets control deaths and cases for tests\npopulationAndTestingTop['deaths_per_test'] = populationAndTestingTop['total_deaths'] \/ populationAndTestingTop['total_tests']\npopulationAndTestingTop.head()\nset(populationAndTestingTop.dropna().Location)\n","70e98a46":"ax = sns.lineplot(x=\"DTDate\", y=\"deaths_per_test\", hue=\"Location\",\n                  data=populationAndTestingTop.dropna())\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.setp(ax.get_xticklabels(), fontsize=5)\nplt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n","a5e2a225":"corr = populationAndTestingTop.corr()\ncorr ","e7311d82":"mask = np.triu(np.ones_like(corr, dtype=np.bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","5c210f93":"corr = populationAndTesting.corr()\ncorr ","ba7aff83":"mask = np.triu(np.ones_like(corr, dtype=np.bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","0b000bbc":"ax = sns.lineplot(x=\"PopDensity\", y=\"deaths_per_test\",\n                  data=populationAndTestingTop.dropna())\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n","3284606c":"#lets use our standarized deaths measure with the full dataset\npopulationAndTesting['total_deaths_perpopulation'] = populationAndTesting['total_deaths'] \/ (1000*populationAndTesting['PopTotal'])\npopulationAndTesting.head()\n","0ae78f48":"#lets create a new measure if a country is sweden or its not\npopulationAndTesting['Is_Sweden'] = populationAndTesting.Location.apply(lambda x: \"Sweden\" in x)\npopulationAndTesting.head()\n","2942e2e9":"set(populationAndTesting.Is_Sweden)","de0cf77b":"# load packages\nimport scipy.stats as stats\n# stats f_oneway functions takes the groups as input and returns F and P-value\nfvalue, pvalue = stats.f_oneway(populationAndTesting[populationAndTesting['Is_Sweden']==True].total_deaths_perpopulation,\n                               populationAndTesting[populationAndTesting['Is_Sweden']==False].total_deaths_perpopulation)\nprint(fvalue, pvalue)","803ad674":"#lets instead take a subset of some of the most populous top 20 developed countries as based on the HDI index.\n#this seems to be a more fair comparison\n\ndeveloped = ['Norway', 'Ireland', 'Germany', \n            'Australia', 'Iceland', 'Sweden',\n            'Singapore', 'Netherlands', 'Denmark',\n            'Finland', 'Canada', 'New Zealand',\n            'United Kingdom', 'United States of America']\npopulationAndTestingDeveloped = populationAndTesting[populationAndTesting['Location'].isin(developed)]\npopulationAndTestingDeveloped.head()","3f657e62":"#test to see if we got them all\nset(populationAndTestingDeveloped.Location)-set(populationAndTestingDeveloped.Location)\n#cool it worked","5e0d0e0f":"fvalue, pvalue = stats.f_oneway(populationAndTestingDeveloped[populationAndTestingDeveloped['Is_Sweden']==True].total_deaths_perpopulation,\n                               populationAndTestingDeveloped[populationAndTestingDeveloped['Is_Sweden']==False].total_deaths_perpopulation)\nprint(fvalue, pvalue)","f340c86d":"ax = sns.lineplot(x=\"DTDate\", y=\"total_deaths_perpopulation\", hue=\"Is_Sweden\",\n                  data=populationAndTestingDeveloped.dropna())\nplt.setp(ax.get_xticklabels(), fontsize=7)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n","119eeb78":"#we have to address one annoying thing\npopulationandweather.dtypes","0ac1f6f0":"populationandweather = populationandweather[['Location','PopTotal','Province_State','Date',\n                                             'ConfirmedCases','Fatalities','Lat','Long',\n                                             'day_from_jan_first','temp','min',\n                                             'max','stp','slp','dewp','rh',\n                                             'ah','wdsp','prcp','fog']]\npopulationandweather.head()","11565521":"set(populationandweather[populationandweather['Province_State'].notnull()].Location)","d881e4fc":"#Some of our data is more modular so we have to use state\/province level data for \n#Australia, Canada, China, United States of America. The rest are just islands so we can honestly drop those\n\n#australia we scraped from\n#https:\/\/www.abs.gov.au\/ausstats\/abs@.nsf\/mediareleasesbyCatalogue\/CA1999BAEAA1A86ACA25765100098A47\n\n#canada \n#https:\/\/www150.statcan.gc.ca\/t1\/tbl1\/en\/tv.action?pid=1710000901\n\n#china\n#http:\/\/data.stats.gov.cn\/english\/easyquery.htm?cn=E0103\n\n#usa\n#https:\/\/www.census.gov\/data\/tables\/time-series\/demo\/popest\/2010s-state-total.html\naustralia = pd.read_csv('..\/input\/covidandpopulationdata\/australia.csv')\nchina = pd.read_csv('..\/input\/covidandpopulationdata\/china.csv')\ncanada = pd.read_csv('..\/input\/covidandpopulationdata\/canada.csv')\nusa = pd.read_csv('..\/input\/covidandpopulationdata\/usa.csv')","909b2545":"populationandweather_usa = populationandweather[populationandweather['Location']=='United States of America']\npopulationandweather_canada = populationandweather[populationandweather['Location']=='Canada']\npopulationandweather_china = populationandweather[populationandweather['Location']=='China']\npopulationandweather_australia = populationandweather[populationandweather['Location']=='Australia']","fdfa5541":"populationandweather = populationandweather[populationandweather['Province_State'].isnull()]","bd8e3a19":"populationandweather_usa.head()","ed938ecd":"usa.head()","37ff1f77":"#join the population and testing data\npopulationandweather_usa = populationandweather_usa.merge(usa, left_on='Province_State', right_on='state')\npopulationandweather_usa.head()","44656984":"populationandweather_usa['PopTotal'] = populationandweather_usa['population']\npopulationandweather_usa = populationandweather_usa[['Location', 'PopTotal', 'Province_State', 'Date', 'ConfirmedCases',\n       'Fatalities', 'Lat', 'Long', 'day_from_jan_first', 'temp', 'min', 'max',\n       'stp', 'slp', 'dewp', 'rh', 'ah', 'wdsp', 'prcp', 'fog']]\npopulationandweather_usa.head()\n","86745ced":"#test to see if we got them \nset(populationandweather_usa.Province_State) - set(usa.state)","c467d36d":"china = china[['State','Population']]\nchina.head()","a865c3a5":"populationandweather_china.head()","10f22e66":"#join the population and testing data\npopulationandweather_china = populationandweather_china.merge(china, left_on='Province_State', right_on='State')\npopulationandweather_china.head()","594390f8":"populationandweather_china['PopTotal'] = populationandweather_china['Population']\npopulationandweather_china = populationandweather_china[['Location', 'PopTotal', 'Province_State', 'Date', 'ConfirmedCases',\n       'Fatalities', 'Lat', 'Long', 'day_from_jan_first', 'temp', 'min', 'max',\n       'stp', 'slp', 'dewp', 'rh', 'ah', 'wdsp', 'prcp', 'fog']]\npopulationandweather_china.head()\n","85f4b2e3":"#test to see if we got them \nset(populationandweather_china.Province_State) - set(china.State)","090ef82f":"australia = australia[['State','population']]\naustralia.head()","33283bd1":"print(set(populationandweather_australia.Province_State))\npopulationandweather_australia.head()","fc61d97f":"#join the population and testing data\npopulationandweather_australia = populationandweather_australia.merge(australia, left_on='Province_State', right_on='State')\npopulationandweather_australia.head()\n","c052103e":"populationandweather_australia['PopTotal'] = populationandweather_australia['population']\npopulationandweather_australia = populationandweather_australia[['Location', 'PopTotal', 'Province_State', 'Date', 'ConfirmedCases',\n       'Fatalities', 'Lat', 'Long', 'day_from_jan_first', 'temp', 'min', 'max',\n       'stp', 'slp', 'dewp', 'rh', 'ah', 'wdsp', 'prcp', 'fog']]\npopulationandweather_australia.head()\n","77b123fb":"#test to see if we got them \nset(populationandweather_australia.Province_State) - set(australia.State)","a229c83e":"canada = canada[['Province','Population']]\ncanada.head()","471feb69":"#join the population and testing data\npopulationandweather_canada = populationandweather_canada.merge(canada, left_on='Province_State', right_on='Province')\npopulationandweather_canada.head()\n","9efef6bc":"populationandweather_canada['PopTotal'] = populationandweather_canada['Population']\npopulationandweather_canada = populationandweather_canada[['Location', 'PopTotal', 'Province_State', 'Date', 'ConfirmedCases',\n       'Fatalities', 'Lat', 'Long', 'day_from_jan_first', 'temp', 'min', 'max',\n       'stp', 'slp', 'dewp', 'rh', 'ah', 'wdsp', 'prcp', 'fog']]\npopulationandweather_canada.head()\n","9315b056":"finalweather = populationandweather.append([populationandweather_canada,populationandweather_usa,\n                                           populationandweather_australia,populationandweather_china])\nfinalweather.head()","d35fc174":"finalweather['PopTotal']  = finalweather['PopTotal'].replace(',','', regex=True)\nfinalweather['PopTotal'] = finalweather['PopTotal'].astype(float)\nfinalweather['ConfirmedCasesPerCapita'] = finalweather['ConfirmedCases']\/finalweather['PopTotal']\nfinalweather['DeathsPerCapita'] = finalweather['Fatalities']\/finalweather['PopTotal']","531fdb01":"finalweather['DTDate'] = finalweather.Date.apply(lambda x: parser.parse(x))","6b1045ec":"finalweather.columns","1a995413":"finalweather = finalweather[['PopTotal', 'ConfirmedCases','Fatalities', \n                             'Lat', 'Long', 'day_from_jan_first', 'temp', 'min', 'max',\n                             'stp', 'slp', 'dewp', 'rh', 'ah', 'wdsp', 'prcp', 'fog',\n       'ConfirmedCasesPerCapita', 'DeathsPerCapita']]","cf4ef7e6":"#impute missing data\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer","1cdbdb0d":"finalweather = finalweather.replace([np.inf, -np.inf], np.nan)\n","30ec72a6":"imp = IterativeImputer(max_iter=10, random_state=0)\nimp.fit(finalweather)\nIterativeImputer(random_state=0)\nx = imp.fit_transform(finalweather)\n# x.head()\ntemp = pd.DataFrame(x, columns=finalweather.columns)\ntemp.head()","da899f97":"from sklearn.preprocessing import MinMaxScaler\n#load scaler\nscaler = MinMaxScaler()\nscaler.fit(temp)\nscaled =scaler.fit_transform(temp) ","662d6a21":"scaleddf = pd.DataFrame(scaled, columns=finalweather.columns)\nscaleddf.head()","16d56e5d":"scaleddf.dtypes","5736446c":"for i in scaleddf.columns:\n    scaleddf[i] = scaleddf[i].astype(int)","8ecc2b9b":"scaleddf.dtypes","916bf102":"\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import ExtraTreesClassifier\n\ny = scaleddf['DeathsPerCapita'] \nX = scaleddf[['ConfirmedCases', 'Lat', 'Long', \n                  'day_from_jan_first', 'temp', 'min',\n                  'max','stp', 'slp', 'dewp', 'rh', 'ah',\n                  'wdsp', 'prcp', 'fog']]\n\n# Build a forest and compute the feature importances\nforest = ExtraTreesClassifier(n_estimators=250,\n                              random_state=0)\n\nforest.fit(X, y)\nimportances = forest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances[indices],\n       color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X.shape[1]), indices)\nplt.xlim([-1, X.shape[1]])\nplt.show()","708c95c8":"#lets do the same check but for cases\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import ExtraTreesClassifier\n\ny = scaleddf['ConfirmedCases'] \nX = scaleddf[['DeathsPerCapita', 'Lat', 'Long', \n                  'day_from_jan_first', 'temp', 'min',\n                  'max','stp', 'slp', 'dewp', 'rh', 'ah',\n                  'wdsp', 'prcp', 'fog']]\n\n# Build a forest and compute the feature importances\nforest = ExtraTreesClassifier(n_estimators=250,\n                              random_state=0)\n\nforest.fit(X, y)\nimportances = forest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances[indices],\n       color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X.shape[1]), indices)\nplt.xlim([-1, X.shape[1]])\nplt.show()","83000246":"scaleddf = pd.DataFrame(scaled, columns=finalweather.columns)\nscaleddf.head()","bcd68ab4":"scaleddf.columns","928f10be":"scaleddf.head()","71cf2af7":"#create a testing and training model \nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(scaleddf.drop(['DeathsPerCapita'],axis=1), scaleddf['DeathsPerCapita'], test_size=0.2, random_state=42)\n\n","96ee4dfe":"from sklearn.linear_model import LinearRegression\n","3adac31e":"reg = LinearRegression().fit(X_train, y_train)\npred = reg.predict(X_test)","6faca444":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import cross_validate\nimport statistics \n\nprint(mean_absolute_error(pred, y_test))\nscores = cross_validate(reg, X_test, y_test, cv=10,\n                        scoring=('neg_root_mean_squared_error'),\n                        return_train_score=True)  \nstatistics.mean(abs(scores['test_score']))","d484446f":"#multi layer perceptron\n\nfrom sklearn.neural_network import MLPRegressor\nneuralNetwork = MLPRegressor(\n    hidden_layer_sizes=(550, 550),\n    shuffle=True, activation='relu',\n    learning_rate='adaptive')\n\nneuralNetwork.fit(X_train, y_train)\n\npred_y_test = neuralNetwork.predict(X_test)\npred_y_train = neuralNetwork.predict(X_train)","275c8cf9":"print(mean_absolute_error(pred_y_test, y_test))\nscores = cross_validate(neuralNetwork, X_test, y_test, cv=10,\n                        scoring=('neg_root_mean_squared_error'),\n                        return_train_score=True)  \nstatistics.mean(abs(scores['test_score']))","66d9d694":"import tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nprint(tf.__version__)\n","bd73079a":"def build_model():\n  model = keras.Sequential([\n    layers.Dense(90, activation='relu', input_shape=[len(X_train.keys())]),\n    layers.Dense(90, activation='relu'),\n    layers.Dense(1)\n  ])\n\n  optimizer = tf.keras.optimizers.RMSprop(0.001)\n\n  model.compile(loss='mse',\n                optimizer=optimizer,\n                metrics=['mae', 'mse'])\n  return model","4ea8b32c":"model = build_model()\nhistory = model.fit(X_train, y_train,\n                    batch_size=64,\n                    epochs=50)","bb938b24":"model.summary()\n","7541a824":"# test_predictions = history.predict(X_test).flatten()\ntest_predictions = model.predict(X_test).flatten()\n\nprint(mean_absolute_error(test_predictions, y_test))\n","401049f1":"## Lets throw some machine learning in here for fun","1ffb0d80":"# The Sweden situation\n\nThe way that Sweden has handled the lockdown has been significanltly different from most countries. They haven't shutdown their economy. And though the US president has said they are 'paying heavily' for this decision. It is not obvious from the data that they are. So lets take a deeper dive and do a statistical test to see if Sweden is fairing marketabley different from other countries. ","8bc0a22e":"## Wow thats pretty accurate","af9c7dcd":"It seems that oddly population density didnt have the same correlation on deaths as we predicted lets look at the full data set to see if this is still the case","3a7a41d6":"Sweden does seem to have a different death rate according to our ANOVA and graph than most other developed nations. We can say that we are fairly certain the group mean of Swedens death rate isn't the same as other develped nations. So perhaps their decision to not lockdown wasn't a good one.","c27d545a":"Wow that is one ugly graph (Hiding the output for visibility purposes)\n\nLets get rid of a lot of the smaller countries for our analysis\n\nWe're only going to take the Top 25 most populous countries and base our anaylsis on those\n","ef833243":"# The Data\n\nFor this analysis we will be relying on three major data sources \n\nPopulation data - Provided from the UN\n\nhttps:\/\/population.un.org\/wpp\/Download\/Standard\/CSV\/\n\nTesting data and Cases\/Deaths data - Provided from ourworldindata.org (their sources are shown in their github)\n\nhttps:\/\/github.com\/owid\/covid-19-data\/tree\/master\/public\/data\n\nCovid cases and weather data - Provided from Kaggle and NOAA\n\nhttps:\/\/www.kaggle.com\/davidbnn92\/weather-data-for-covid19-data-analysis\n\nhttps:\/\/www.kaggle.com\/c\/covid19-global-forecasting-week-4\n\nhttps:\/\/www.kaggle.com\/noaa\/gsod\n","19254bf2":"# Testing feature importance\n\nFirst lets test fatalities per capita","4313192f":"Interesting, we have world data. Lets save that for later","9117100e":"It is clear that the US has vastly more tests than any other country.\n","18d4b7aa":"# USA analysis\n\nThere has been a lot of media attention about how the US has handled the COVID situation. Many people seem to say that the death toll for COVID patients has been significantly higher in the US than in other countries. And while looking at raw numbers one can say that but, if we standardize our data we see a different story. There does seem to be a high degreee in testing data scarcity. Combine that with the fact countries are self reporting all these numbers, we can have a degree of skepticism when looking at this information. That being said we can paint a much different picture with this information than is being portrayed to us. ","c1fde59f":"Does seem oddly random. ","0e960ef8":"### Lets try tensorflow","36c0c4ba":"# Variable Creation\n\n","23e49047":"Interesting, population density isnt corelated with cases and deaths as strongly as we would have predicted. \nThis could be a result of testing data scarcity.","61dd3e17":"It seems like the us has an extreme amount of cases however \n\nthis is decieving. The us is 3rd in population. But more importantly\n\nlets look at the testing","3112ac4c":"# Anova Assumptions\n\nLooking at this we have to take into consideration the assumptions of an anova test.\n\nThe samples are independent.\n\nEach sample is from a normally distributed population.\n\nThe population standard deviations of the groups are all equal. This property is known as homoscedasticity.\n\nUnfortunately we cannot say that we meet all of these assumptions. However considering the sample size we can look passed some of them","396a3c70":"### So again the most important feature is days since january first.\n\n### We had the same result as before but stp is significantly less than it was before","1c328f8f":"# Fit to a predictive model","b0b92ab1":"# Scale our data ","fccb39f9":"# Weather\n\nThere is a current theory going around that the weather has an impact on the Coronavirus. That warm and humid places tend make the virus live a shorter period and and therby safer. We're going to test these variables importance on growth of the virus. We're also going to attempt to fit a model based on this. \n","a79dd891":"Populations do look different here but lets take into consideration that Sweden is a highly developed country. So comparing it's healthcare system to all the other countries in the world isn't necessarily fair. We could have a significan difference based purely on the fact that they have a better health care system than the majority of other countries.","ceb3818d":"# COVID-19 Analysis\n\nCOVID-19 is perhaps the biggest global event of the past 20 years. The deaths, the entire shutdown of the global economy, and what are most likely lasting changes in day to day life have had a profound effect on all of us. Through all of this, one of the most dangerous aspects in our stay at home lives is the non-data driven opinions we are being fed. So this notebook is an attempt at addressing some of these points and looking at a data driven view to give some clarity in murkiness that is our new normal. \n\nSpecifically we will address the following four questions:\n\n    1. How is the USA dealing with the virus compared to other countries ?\n    2. Is Sweden's approach to not lockdown effective ?\n    3. Does weather have an effect on deaths\/cases ?\n    4. Can we build a predictive model?\n   ","e23fb7a1":"# Impute missing data ","424335b0":"### So obviously the most important feature is days since january first because deaths are going to be higher as time goes on.\n\n### Interestingly stp- or maximum temperature reported during the day in Fahrenheit to tenths--time of max temp report varies by country and region, so this will sometimes not be the max for the calendar day. Also seemed important","09eccc8e":"## Decent performance\n0.01104385921680685\naverage of root mean squared error on testing data after 10 fold cross validation","74f19b53":"### Start with simple regression model","7e5d1e61":"# Data cleaning\n\nObviously in it's current state we can't use any of this data so we'll have to clean it."}}