{"cell_type":{"58243a49":"code","afa23b5e":"code","90059a7b":"code","49512e56":"code","aeba71ec":"code","a77dfe83":"code","ad473008":"code","d96dfbaa":"code","1f0c7854":"code","9b01a4a4":"code","188279e7":"code","e98fce88":"code","3d7f072f":"code","f19dc383":"code","14250496":"code","24a930d7":"code","892788f5":"code","b1e22b11":"code","6dce32f0":"code","5bc3d86b":"code","46c4d43e":"code","5ea4d460":"markdown","371a0cd1":"markdown","0558cb70":"markdown","ac90e262":"markdown","b84c3fee":"markdown","fa54f2e1":"markdown","83da95ec":"markdown","475bae9a":"markdown","ed032381":"markdown","5ddbca5f":"markdown","5ef01a8f":"markdown","e599fdd1":"markdown","fa080020":"markdown"},"source":{"58243a49":"import os\nimport random\nimport time\nfrom contextlib import contextmanager\n\nimport fasttext\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold","afa23b5e":"def set_seed(seed: int = 42):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    \n    \n@contextmanager\ndef timer(name: str):\n    t0 = time.time()\n    print(f\"[{name}] start\")\n    yield\n    print(f\"[{name}] done in {time.time() - t0:.2f} s\")","90059a7b":"set_seed(1213)","49512e56":"with timer(\"Load fastText model\"):\n    ft_model = fasttext.load_model(\n        \"..\/input\/japanese-word-embeddings\/cc.ja.300.bin\")","aeba71ec":"with timer(\"Load dataset\"):\n    train = pd.read_csv(\"..\/input\/rkcup-1\/train.csv\")\n    test = pd.read_csv(\"..\/input\/rkcup-1\/test.csv\")\ntrain.head()","a77dfe83":"class SimpleTokenizer:\n    def tokenize(self, text: str):\n        return text.split()\n\n\nclass SWEM():\n    \"\"\"\n    Simple Word-Embeddingbased Models (SWEM)\n    https:\/\/arxiv.org\/abs\/1805.09843v1\n    \"\"\"\n\n    def __init__(self, ft_model, tokenizer):\n        self.ft_model = ft_model\n        self.tokenizer = tokenizer\n        self.embedding_dim = ft_model.get_dimension()\n\n    def get_word_embeddings(self, text):\n        np.random.seed(abs(hash(text)) % (10 ** 8))\n\n        vectors = []\n        for word in self.tokenizer.tokenize(text):\n            vectors.append(self.ft_model.get_word_vector(word))\n        return np.array(vectors)\n\n    def average_pooling(self, text):\n        word_embeddings = self.get_word_embeddings(text)\n        return np.mean(word_embeddings, axis=0)\n\n    def max_pooling(self, text):\n        word_embeddings = self.get_word_embeddings(text)\n        return np.max(word_embeddings, axis=0)\n\n    def concat_average_max_pooling(self, text):\n        word_embeddings = self.get_word_embeddings(text)\n        return np.r_[np.mean(word_embeddings, axis=0), np.max(word_embeddings, axis=0)]\n\n    def hierarchical_pooling(self, text, n):\n        word_embeddings = self.get_word_embeddings(text)\n\n        text_len = word_embeddings.shape[0]\n        if n > text_len:\n            return np.mean(word_embeddings, axis=0)\n        window_average_pooling_vec = [np.mean(word_embeddings[i:i + n], axis=0) for i in range(text_len - n + 1)]\n\n        return np.max(window_average_pooling_vec, axis=0)","ad473008":"swem = SWEM(ft_model=ft_model,\n            tokenizer=SimpleTokenizer())\n\n\ndef swem_vector(series: pd.Series, mode=\"aver\") -> np.ndarray:\n    if mode == \"aver\":\n        return np.stack(\n            series.fillna(\"\").str.replace(\"\\n\", \" \").map(\n                lambda x: swem.average_pooling(x)).values\n        )\n    elif mode == \"max\":\n        return np.stack(\n            series.fillna(\"\").str.replace(\"\\n\", \" \").map(\n                lambda x: swem.max_pooling(x)).values\n        )\n    elif mode == \"concat\":\n        return np.stack(\n            series.fillna(\"\").str.replace(\"\\n\", \" \").map(\n                lambda x: swem.concat_average_max_pooling(x)).values\n        )\n    else:\n        return np.stack(\n            series.fillna(\"\").str.replace(\"\\n\", \" \").map(\n                lambda x: swem.hierarchical_pooling(x, n=2)).values\n        )","d96dfbaa":"with timer(\"Vectorize text with SWEM aver\"):\n    X_train_aver = swem_vector(train[\"separated_text\"], \"aver\")\n    X_test_aver = swem_vector(test[\"separated_text\"], \"aver\")\n    \nwith timer(\"Vectorize text with SWEM max\"):\n    X_train_max = swem_vector(train[\"separated_text\"], \"max\")\n    X_test_max = swem_vector(test[\"separated_text\"], \"max\")\n    \nwith timer(\"Vectorize text with SWEN concat\"):\n    X_train_concat = swem_vector(train[\"separated_text\"], \"concat\")\n    X_test_concat = swem_vector(test[\"separated_text\"], \"concat\")\n    \nwith timer(\"Vectorize text with SWEM hier\"):\n    X_train_hier = swem_vector(train[\"separated_text\"], \"hier\")\n    X_test_hier = swem_vector(test[\"separated_text\"], \"hier\")","1f0c7854":"print(f\"aver shape: {X_train_aver.shape} {X_test_aver.shape}\")\nprint(f\"max shape: {X_train_max.shape} {X_test_max.shape}\")\nprint(f\"concat shape: {X_train_concat.shape} {X_test_concat.shape}\")\nprint(f\"hier shape: {X_train_hier.shape} {X_test_hier.shape}\")","9b01a4a4":"def train_kfold(X_train: np.ndarray, X_test: np.ndarray, train: pd.DataFrame, n_splits=5):\n    # Out-of-fold\u306a\u4e88\u6e2c\u3092\u5165\u308c\u3066\u3044\u304f\u305f\u3081\u306e\u914d\u5217\n    oof = np.zeros(len(X_train), dtype=float)\n    # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u63a8\u8ad6\u7d50\u679c\u3092\u5165\u308c\u3066\u3044\u304f\u305f\u3081\u306e\u914d\u5217\n    test_pred = np.zeros(len(X_test), dtype=float)\n    \n    kf = KFold(n_splits=n_splits, random_state=42, shuffle=True)\n    y = train[\"target\"].values\n    params = {\n        \"boosting_type\": \"gbdt\",\n        \"objective\": \"binary\",\n        \"learning_rate\": 0.01,\n        \"num_leaves\": 31,\n        \"subsample\": 0.7,\n        \"subsample_freq\": 1,\n        \"colsample_bytree\": 0.7,\n        \"metric\": \"auc\"\n    }\n    for i, (trn_idx, val_idx) in enumerate(kf.split(train)):\n        with timer(f\"Fold{i + 1} training\"):\n            X_trn = X_train[trn_idx]\n            X_val = X_train[val_idx]\n            y_trn = y[trn_idx]\n            y_val = y[val_idx]\n            \n            lgb_train = lgb.Dataset(X_trn, y_trn)\n            lgb_valid = lgb.Dataset(X_val, y_val)\n            model = lgb.train(params,\n                              lgb_train,\n                              valid_sets=[lgb_valid],\n                              valid_names=[\"valid\"],\n                              num_boost_round=10000,\n                              early_stopping_rounds=100,\n                              verbose_eval=100)\n            y_val_pred = model.predict(X_val)\n            score = roc_auc_score(y_true=y_val, y_score=y_val_pred)\n            print(f\"Fold{i + 1} AUC score: {score:.4f}\")\n            \n            oof[val_idx] = y_val_pred.reshape(-1)\n            \n            y_test_pred = model.predict(X_test)\n            test_pred += y_test_pred.reshape(-1) \/ n_splits\n    return oof, test_pred","188279e7":"oof_aver, test_pred_aver = train_kfold(X_train_aver, X_test_aver, train, n_splits=5)","e98fce88":"score = roc_auc_score(y_true=train[\"target\"].values, y_score=oof_aver)\nprint(f\"OOF AUC: {score:.4f}\")","3d7f072f":"oof_max, test_pred_max = train_kfold(X_train_max, X_test_max, train, n_splits=5)","f19dc383":"score = roc_auc_score(y_true=train[\"target\"].values, y_score=oof_max)\nprint(f\"OOF AUC: {score:.4f}\")","14250496":"oof_concat, test_pred_concat = train_kfold(X_train_concat, X_test_concat, train, n_splits=5)","24a930d7":"score = roc_auc_score(y_true=train[\"target\"].values, y_score=oof_concat)\nprint(f\"OOF AUC: {score:.4f}\")","892788f5":"oof_hier, test_pred_hier = train_kfold(X_train_hier, X_test_hier, train, n_splits=5)","b1e22b11":"score = roc_auc_score(y_true=train[\"target\"].values, y_score=oof_hier)\nprint(f\"OOF AUC: {score:.4f}\")","6dce32f0":"sample = pd.read_csv(\"..\/input\/rkcup-1\/sample_submission.csv\")\nsample.head()","5bc3d86b":"sample[\"target\"] = test_pred_aver\nsample.to_csv(\"tfidf_logistic_submission.csv\", index=False)\nsample.head()","46c4d43e":"pd.read_csv(\"tfidf_logistic_submission.csv\").tail()","5ea4d460":"\u305d\u308c\u305e\u308c\u306e\u624b\u6cd5\u3067\u6587\u7ae0\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u6210\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002","371a0cd1":"### SWEM concat\u3092\u7279\u5fb4\u306b\u3057\u305f\u5834\u5408","0558cb70":"## Test\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u4e88\u6e2c\u306e\u63d0\u51fa","ac90e262":"## \u6587\u7ae0\u30d9\u30af\u30c8\u30eb\u306e\u4f5c\u6210\n\n\u3053\u3053\u3067\u306f\u3001`fastText`\u3068`SWEM`\u306b\u3088\u308a\u6587\u7ae0\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u6210\u3059\u308b\u65b9\u6cd5\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002","b84c3fee":"### SWEM\u306b\u3064\u3044\u3066\n\n\u6587\u7ae0\u3092\u5358\u8a9e\u306e\u4e26\u3073\u3060\u3068\u3057\u3066\u3001\u5404\u5358\u8a9e\u3092\u5bfe\u5fdc\u3059\u308b\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u8868\u73fe\u306b\u5909\u63db\u3057\u305f\u3068\u3057\u3066\u3082\u3001\u305d\u308c\u306f\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u8868\u73fe\u306e\u7cfb\u5217\u306b\u3057\u304b\u306a\u308a\u307e\u305b\u3093\u3002\u3053\u306e\u7cfb\u5217\u306f\u6587\u7ae0\u4e2d\u306e\u5358\u8a9e\u6570\u306b\u3088\u3063\u3066\u7cfb\u5217\u9577\u304c\u5909\u308f\u308b\u306a\u3069\u3001\u6a5f\u68b0\u5b66\u7fd2\u306e\u30e2\u30c7\u30eb\u306e\u5165\u529b\u306b\u4f7f\u3046\u306b\u306f\u3042\u307e\u308a\u76f8\u5fdc\u3057\u304f\u306a\u3044\u305f\u3081\u3001\u306a\u3093\u3089\u304b\u306e\u65b9\u6cd5\u3067\u56fa\u5b9a\u9577\u306e\u6587\u7ae0\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3057\u3066\u3084\u308b\u306e\u304c\u671b\u307e\u3057\u3044\u3067\u3059\u3002\n\nSWEM\u306f\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u8868\u73fe\u306e\u7cfb\u5217\u304b\u3089\u304b\u306a\u308a\u30b7\u30f3\u30d7\u30eb\u306a\u3084\u308a\u65b9\u3067\u6587\u7ae0\u30d9\u30af\u30c8\u30eb\u3092\u5f97\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u624b\u6cd5\u3067\u3059\u3002\n\n[Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms](https:\/\/arxiv.org\/abs\/1805.09843)\n\nSWEM\u306f\u5b9f\u306f\u4e00\u3064\u306e\u624b\u6cd5\u3092\u6307\u3057\u305f\u540d\u79f0\u3067\u306f\u306a\u304f4\u3064\u306e\u65b9\u6cd5\u3092\u307e\u3068\u3081\u305f\u540d\u79f0\u3068\u306a\u3063\u3066\u3044\u307e\u3059\u30024\u3064\u306e\u65b9\u6cd5\u3068\u306f\u305d\u308c\u305e\u308c\n\n1. SWEM-aver: \u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u306e\u7cfb\u5217\u3092\u5217\u65b9\u5411\u306b\u5e73\u5747\u3057\u3001\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u3068\u540c\u3058\u6b21\u5143\u6570\u306e\u6587\u7ae0\u30d9\u30af\u30c8\u30eb\u3092\u5f97\u308b\n2. SWEM-max:  \u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u306e\u7cfb\u5217\u3092\u5217\u65b9\u5411\u306bmax\u3092\u53d6\u308a\u3001\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u3068\u540c\u3058\u6b21\u5143\u6570\u306e\u6587\u7ae0\u30d9\u30af\u30c8\u30eb\u3092\u5f97\u308b\n3. SWEM-concat: SWEM-aver\u3068SWEM-max\u306e\u4e21\u65b9\u3092\u4f5c\u6210\u3057\u305f\u4e0a\u3067\u7d50\u5408\u3057\u3001\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u306e2\u500d\u306e\u6b21\u5143\u6570\u306e\u6587\u7ae0\u30d9\u30af\u30c8\u30eb\u3092\u5f97\u308b\n4. SWEM-hier: \u4e00\u5b9a\u306e\u7a93\u5e45\u5185\u3067\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u306e\u7cfb\u5217\u306e\u5e73\u5747\u3092\u3068\u308a\u3001\u7a93\u3092\u30b9\u30e9\u30a4\u30c9\u3055\u305b\u3066\u3044\u304f\u3053\u3068\u3067\u7a93\u5185\u3067\u306e\u5e73\u5747\u30d9\u30af\u30c8\u30eb\u306e\u7cfb\u5217\u3092\u5f97\u308b\u3002\u305d\u306e\u5e73\u5747\u30d9\u30af\u30c8\u30eb\u306e\u7cfb\u5217\u3092\u5217\u65b9\u5411\u306bmax\u3092\u53d6\u308b\u3053\u3068\u3067\u6700\u7d42\u7684\u306b\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u3068\u540c\u3058\u6b21\u5143\u6570\u306e\u6587\u7ae0\u30d9\u30af\u30c8\u30eb\u3092\u5f97\u308b\n\n\u7279\u306b4\u306eSWEM-hier\u306f\u3042\u308b\u7a0b\u5ea6\u8a9e\u9806\u306a\u3069\u306e\u5f71\u97ff\u3082\u52a0\u5473\u3067\u304d\u308b\u6587\u7ae0\u30d9\u30af\u30c8\u30eb\u4f5c\u6210\u6cd5\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002\n\nSWEM\u306f[\u30c6\u30fc\u30d6\u30eb\u30c7\u30fc\u30bf\u5411\u3051\u81ea\u7136\u8a00\u8a9e\u7279\u5fb4\u62bd\u51fa\u8853](https:\/\/zenn.dev\/koukyo1994\/articles\/9b1da2482d8ba1)\u3067\u7d39\u4ecb\u3057\u305f\u5b9f\u88c5\u3092\u4f7f\u3044\u307e\u3059\u3002","fa54f2e1":"### SWEM aver\u3092\u7279\u5fb4\u306b\u3057\u305f\u5834\u5408","83da95ec":"## \u306f\u3058\u3081\u306b\n\n\u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u306f\u3001`fastText`\u306e\u5358\u8a9e\u57cb\u3081\u8fbc\u307f\u304b\u3089`SWEM`\u3068\u547c\u3070\u308c\u308b\u624b\u6cd5\u306b\u3088\u3063\u3066\u6587\u7ae0\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u6210\u3057\u3001\u305d\u308c\u3092\u7279\u5fb4\u3068\u3057\u3066`LightGBM`\u3067\u30dd\u30b8\u30cd\u30ac\u5206\u6790\u3092\u3059\u308b\u4f8b\u3092\u793a\u3057\u307e\u3059\u3002","475bae9a":"Out-of-fold\u306e\u4e88\u6e2c\u7d50\u679c\u306e\u30b9\u30b3\u30a2\u3067\u306fSWEM-aver\u304c\u6700\u3082\u9ad8\u3044\u3068\u3044\u3046\u7d50\u679c\u306b\u306a\u308a\u307e\u3057\u305f\u306e\u3067\u3053\u308c\u3092\u63d0\u51fa\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002","ed032381":"## KFold\u5b66\u7fd2\n\n\u305d\u308c\u305e\u308c\u306e\u7279\u5fb4\u3092\u7528\u3044\u3066KFold\u5b66\u7fd2\u3092\u884c\u3063\u3066\u307f\u307e\u3057\u3087\u3046\u3002\nKFold\u5b66\u7fd2\u3092\u3059\u308b\u90e8\u5206\u306f\u95a2\u6570\u304b\u3057\u3066\u3057\u307e\u3044\u307e\u3059\u3002","5ddbca5f":"### SWEM hier\u3092\u7279\u5fb4\u306b\u3057\u305f\u5834\u5408","5ef01a8f":"### SWEM max\u3092\u7279\u5fb4\u306b\u3057\u305f\u5834\u5408","e599fdd1":"### fastText\u306b\u3064\u3044\u3066\n\n\u3053\u3053\u3067\u306f\u3001fastText\u3068\u3044\u3046\u624b\u6cd5\u306b\u3064\u3044\u3066\u8efd\u304f\u7d39\u4ecb\u3057\u307e\u3059\u3002\u6280\u8853\u7684\u80cc\u666f\u306a\u3069\u306b\u306f\u89e6\u308c\u306a\u3044\u305f\u3081\u9069\u5b9c\u8ad6\u6587\u3092\u8aad\u3080\u306a\u3069\u3057\u3066\u304f\u3060\u3055\u3044\u3002\nfastText\u306f[\"Enriching Word Vectors with Subword Information\"](https:\/\/aclanthology.org\/Q17-1010\/)\u3068\u3044\u3046\u8ad6\u6587\u3067\u7d39\u4ecb\u3055\u308c\u305f\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u8868\u73fe\u306e\u4f5c\u6210\u624b\u6cd5\u3067\u3059\u3002\n\n\u30bf\u30a4\u30c8\u30eb\u306b\u3082\u3042\u308b\u3088\u3046\u306bsubword\u3059\u306a\u308f\u3061\u5358\u8a9e\u306e\u4e00\u90e8\u3092\u4f7f\u3046\u3053\u3068\u3067\u3001\u5358\u8a9e\u57cb\u3081\u8fbc\u307f\u3092\u9ad8\u7cbe\u5ea6\u304b\u3064\u9ad8\u901f\u306b\u5b66\u7fd2\u3067\u304d\u308b\u3088\u3046\u306b\u3057\u305f\u307b\u304b\u3001\u5358\u8a9e\u57cb\u3081\u8fbc\u307f\u306b\u3064\u304d\u307e\u3068\u3046\u554f\u984c\u3060\u3063\u305fOut-of-vocabulary(\u8a9e\u5f59\u306b\u306a\u3044\u5358\u8a9e\u304c\u73fe\u308c\u305f\u3068\u304d\u306b\u5358\u8a9e\u57cb\u3081\u8fbc\u307f\u3092\u63d0\u4f9b\u3067\u304d\u306a\u3044\u3053\u3068)\u306e\u554f\u984c\u3092\u89e3\u6c7a\u3057\u305f\u624b\u6cd5\u3067\u3059\u3002\u3088\u308a\u5177\u4f53\u7684\u306b\u306f\u5358\u8a9e\u306e\u57cb\u3081\u8fbc\u307f\u8868\u73fe\u3092\u5358\u8a9e\u3092\u69cb\u6210\u3059\u308bngram\u306e\u57cb\u3081\u8fbc\u307f\u8868\u73fe\u306e\u548c\u3068\u3057\u3066\u8868\u73fe\u3059\u308b\u3053\u3068\u3067\u3001\u5358\u8a9e\u306e\u6301\u3064\u5f62\u614b\u5b66\u7684\u306a\u8fd1\u3055(\u4f3c\u305f\u3088\u3046\u306a\u7db4\u308a\u306e\u5358\u8a9e\u306f\u8fd1\u3044\u306f\u305a)\u3092\u8868\u73fe\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\n\nfastText\u306fSkipGram\u304a\u3088\u3073CBOW\u306e\u751f\u307f\u306e\u89aa\u3067\u3082\u3042\u308bTomas Mikolov\u304cFacebook AI Research\u3067\u306e\u7814\u7a76\u6210\u679c\u3068\u3057\u3066\u767a\u8868\u3057\u3066\u304a\u308a\u3001[facebookresearch\/fastText](https:\/\/github.com\/facebookresearch\/fastText)\u3067\u5b9f\u88c5\u3082\u516c\u958b\u3055\u308c\u3066\u3044\u308b\u307b\u304b\u3001[fasttext.cc](https:\/\/fasttext.cc\/)\u3067\u591a\u6570\u306e\u5b66\u7fd2\u6e08\u307f\u91cd\u307f\u3092\u5165\u624b\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\n\n\u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3067\u306f[fasttext.cc](https:\/\/fasttext.cc\/)\u304b\u3089\u65e5\u672c\u8a9e\u306e300\u6b21\u5143\u306e\u5b66\u7fd2\u6e08\u307f\u30d9\u30af\u30c8\u30eb\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u305f\u3082\u306e\u3092\u7528\u3044\u307e\u3059\u3002","fa080020":"## \u4fbf\u5229\u95a2\u6570\u306e\u7528\u610f"}}