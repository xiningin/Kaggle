{"cell_type":{"b6376061":"code","0d485053":"code","09a12c54":"code","42eba348":"code","aff83036":"code","bf5c7d99":"code","7d31d436":"code","358a8334":"code","ff3df676":"code","1f815f07":"code","a3c59206":"code","fa177413":"code","4f50a5e0":"code","9f80c295":"code","044ce4b4":"code","116c761a":"code","3afdf735":"code","dde89e48":"code","e5b40207":"code","cf3e4d9a":"code","1e6bb07c":"code","6a48dd9c":"code","8c34783e":"code","baf28257":"code","4fea0866":"code","0daaa624":"code","5b63a29f":"code","85ddb978":"code","9a0efe6d":"code","58a4e165":"code","8489db2e":"code","eafc2e50":"code","15f4ce7c":"code","43527677":"code","5d17b74d":"code","54482423":"code","54311324":"code","37adfe6c":"code","1956a04a":"markdown","a6629e14":"markdown","ac25acfb":"markdown","209f2535":"markdown","dd013876":"markdown","5bb1ba0b":"markdown","ead171da":"markdown","61cf5c75":"markdown","2a4c6826":"markdown","9e2fe4f8":"markdown","ab07a275":"markdown","420f5662":"markdown","1a62c939":"markdown","8681fe16":"markdown"},"source":{"b6376061":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0d485053":"import pandas as pd\npd.set_option('use_inf_as_na', True)\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport itertools\nfrom fractions import Fraction\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score as acs\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.metrics import f1_score,classification_report\nfrom sklearn.metrics import log_loss\nfrom sklearn.utils import resample\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","09a12c54":"data=pd.read_csv(\"\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv\")\ndata.drop([\"Unnamed: 32\"],axis=1,inplace=True)\ndisplay(data.head())","42eba348":"#New data with \"M\"=1 and \"B\"=0\ndata1=data.copy()\ndef classifier(data1):\n    if data1[\"diagnosis\"]==\"M\":\n        return \"1\"\n    else:\n        return \"0\"\ndata1[\"diagnosis\"] = data1.apply(classifier, axis=1)\ndata1.replace([np.inf, -np.inf], np.nan, inplace=True)\ndata1[\"diagnosis\"]=pd.to_numeric(data1[\"diagnosis\"],errors=\"coerce\")","aff83036":"print(data1.columns,data.shape)\n# print(data1.info())\n# print(data1.describe().T)\n# print(data1.nunique())","bf5c7d99":"sns.set(style=\"whitegrid\")\nprint(data['diagnosis'].value_counts())\nfig = plt.figure(figsize = (10,6))\nsns.countplot('diagnosis', data=data, palette='gist_heat')\nplt.show()","7d31d436":"color = sns.color_palette(\"pastel\")\n\nfig, ax1 = plt.subplots(8,4, figsize=(30,60))\nk = 0\ncolumns = list(data1.columns)\nfor i in range(8):\n    for j in range(4):\n            sns.distplot(data1[columns[k]], ax = ax1[i][j], color = 'red')\n            plt.xlabel(columns[k],size=20)\n            k += 1\nplt.show()","358a8334":"#Log transform\ndef log_transform(col):\n    return np.log(col[0])\n\ndata1[\"compactness_mean\"]=data1[[\"compactness_mean\"]].apply(log_transform, axis=1)\ndata1[\"concavity_mean\"]=data1[[\"concavity_mean\"]].apply(log_transform, axis=1)\ndata1[\"concave points_mean\"]=data1[[\"concave points_mean\"]].apply(log_transform, axis=1)\ndata1[\"radius_se\"]=data1[[\"radius_se\"]].apply(log_transform, axis=1)\ndata1[\"perimeter_se\"]=data1[[\"perimeter_se\"]].apply(log_transform, axis=1)\ndata1[\"smoothness_se\"]=data1[[\"smoothness_se\"]].apply(log_transform, axis=1)\ndata1[\"compactness_se\"]=data1[[\"compactness_se\"]].apply(log_transform, axis=1)\ndata1[\"concavity_se\"]=data1[[\"concavity_se\"]].apply(log_transform, axis=1)\ndata1[\"symmetry_se\"]=data1[[\"symmetry_se\"]].apply(log_transform, axis=1)\ndata1[\"fractal_dimension_se\"]=data1[[\"fractal_dimension_se\"]].apply(log_transform, axis=1)\ndata1[\"area_worst\"]=data1[[\"area_worst\"]].apply(log_transform, axis=1)\ndata1[\"compactness_worst\"]=data1[[\"compactness_worst\"]].apply(log_transform, axis=1)\ndata1[\"concavity_worst\"]=data1[[\"concavity_worst\"]].apply(log_transform, axis=1)","ff3df676":"color = sns.color_palette(\"pastel\")\n\nfig, ax1 = plt.subplots(8,4, figsize=(30,60))\nk = 0\ncolumns = list(data1.columns)\nfor i in range(8):\n    for j in range(4):\n        sns.distplot(data1[columns[k]], ax = ax1[i][j], color = 'green')\n        k += 1\nplt.show()","1f815f07":"plt.figure(figsize=(16,8))\ncorr=data1.drop([\"id\"],axis=1).corr()\nsns.heatmap(corr,annot=True,linewidth=1)\nplt.show()\n\n#Correaltion of features in descending order\nprint(data1.corr()['diagnosis'].sort_values(ascending=False))\n\nplt.figure(figsize=(16,8))\nplt.plot(data1.corr()['diagnosis'].sort_values(ascending=False)[1:],color=\"cyan\")\nplt.title(\"Correlation of different features with 'Diagnosis'\")\nplt.xticks(rotation=90)\nplt.show()","a3c59206":"data_M = data1[data1.diagnosis==1]     #Minority\ndata_B = data1[data1.diagnosis==0]     #Majority\n\ndata_M_upsampled=resample(data_M,replace=True, n_samples=300, random_state=12)\ndata_B_downsampled= data_B.sample(n=300).reset_index(drop=True)\n\n#New dataset for balanced data\nBalanced_df = pd.concat([data_M_upsampled, data_B_downsampled]).reset_index(drop=True)","fa177413":"print(Balanced_df[\"diagnosis\"].value_counts())\nplt.figure(figsize=(10,6))\nsns.countplot(x='diagnosis', data=Balanced_df, palette='gist_heat')\nplt.show()","4f50a5e0":"plt.figure(figsize=(15,15))\nBalanced_df.corr().diagnosis.apply(lambda x: abs(x)).sort_values(ascending=False).iloc[1:21][::-1].plot(kind='barh',color='cyan') \n# calculating the top 20 highest correlated features\n# with respect to the target variable i.e. \"quality\"\nplt.title(\"Top 20 highly correlated features\", size=20, pad=26)\nplt.xlabel(\"Correlation coefficient\")\nplt.ylabel(\"Features\")\nplt.show()","9f80c295":"selected_features=Balanced_df.corr().diagnosis.sort_values(ascending=False).iloc[1:21][::-1].index\n\nX = Balanced_df[selected_features]\nY = Balanced_df.diagnosis","044ce4b4":"X=data.iloc[:,2:32]\nY=data.iloc[:,1]\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=0) \n\n#Feature Scaling\nscaler=StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.fit_transform(X_test)","116c761a":"LR_model=LogisticRegression(random_state=0)\nLR_model.fit(X_train,Y_train)","3afdf735":"Y_pred=LR_model.predict(X_test)","dde89e48":"cm=confusion_matrix(Y_pred,Y_test)\nclass_label = [\"malignant\", \"benign\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\n\nprecision, recall, fscore, train_support = score(Y_test, Y_pred, pos_label=\"M\", average='binary')\nLogistic_Regression_accuracy=round(acs(Y_test,Y_pred), 4)*100\nprint('Precision: {} \\nRecall: {} \\nF1-Score: {} \\nAccuracy: {}'.format(\n    round(precision, 3), round(recall, 3), round(fscore,3), Logistic_Regression_accuracy) +\"% \\n\")\n\nsns.heatmap(df_cm,annot=True,cmap='Pastel1',linewidths=2,fmt='d')\nplt.title(\"Confusion Matrix\",fontsize=15)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()","e5b40207":"svm=SVC(kernel=\"rbf\",random_state=0)\nsvm.fit(X_train,Y_train)","cf3e4d9a":"Y_pred=svm.predict(X_test)","1e6bb07c":"cm=confusion_matrix(Y_pred,Y_test)\nclass_label = [\"malignant\", \"benign\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\n\nprecision, recall, fscore, train_support = score(Y_test, Y_pred, pos_label=\"M\", average='binary')\nSVM_accuracy=round(acs(Y_test,Y_pred), 4)*100\nprint('Precision: {} \\nRecall: {} \\nF1-Score: {} \\nAccuracy: {}'.format(\n    round(precision, 3), round(recall, 3), round(fscore,3), SVM_accuracy) +\"% \\n\")\n\nsns.heatmap(df_cm,annot=True,cmap='Pastel1',linewidths=2,fmt='d')\nplt.title(\"Confusion Matrix\",fontsize=15)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()","6a48dd9c":"tree=DecisionTreeClassifier(random_state=10)\ntree.fit(X_train,Y_train)","8c34783e":"Y_pred=tree.predict(X_test)","baf28257":"cm=confusion_matrix(Y_pred,Y_test)\nclass_label = [\"malignant\", \"benign\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\n\nprecision, recall, fscore, train_support = score(Y_test, Y_pred, pos_label=\"M\", average='binary')\nDecision_Tree_accuracy=round(acs(Y_test,Y_pred), 4)*100\nprint('Precision: {} \\nRecall: {} \\nF1-Score: {} \\nAccuracy: {}'.format(\n    round(precision, 3), round(recall, 3), round(fscore,3), Decision_Tree_accuracy) +\"% \\n\")\n\nsns.heatmap(df_cm,annot=True,cmap='Pastel1',linewidths=2,fmt='d')\nplt.title(\"Confusion Matrix\",fontsize=15)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()","4fea0866":"knn=KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train,Y_train)","0daaa624":"Y_pred=knn.predict(X_test)","5b63a29f":"cm=confusion_matrix(Y_pred,Y_test)\nclass_label = [\"malignant\", \"benign\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\n\nprecision, recall, fscore, train_support = score(Y_test, Y_pred, pos_label=\"M\", average='binary')\nKNN_accuracy=round(acs(Y_test,Y_pred), 4)*100\nprint('Precision: {} \\nRecall: {} \\nF1-Score: {} \\nAccuracy: {}'.format(\n    round(precision, 3), round(recall, 3), round(fscore,3), KNN_accuracy) +\"% \\n\")\n\nsns.heatmap(df_cm,annot=True,cmap='Pastel1',linewidths=2,fmt='d')\nplt.title(\"Confusion Matrix\",fontsize=15)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()","85ddb978":"val=10   #Max value of n_neighbor\nmodel=knn  #Name of model you want to train (I'm training my KNN model)\nfor K in range(val):\n    K_value = K+1\n    model = KNeighborsClassifier(n_neighbors=K_value)\n    model.fit(X_train,Y_train)\n    Y_pred = model.predict(X_test)\n    print(\"Accuracy is : \", acs(Y_test,Y_pred)*100,\"% for n_neighbors: \", K_value)","9a0efe6d":"# Model 1=>Logistic Regression\nLR_model = LogisticRegression(random_state=0)\nLR_model.fit(X_train, Y_train)\nLR_probs = LR_model.predict_proba(X_test)\nscore = log_loss(Y_test, LR_probs)\nprint(score)","58a4e165":"# Model 2=>Support Vector Machine\nSVC_model = SVC(kernel=\"rbf\",random_state=0)\nSVC_model.fit(X_train, Y_train)\nSVC_probs = SVC_model.decision_function(X_test)\nscore = log_loss(Y_test, SVC_probs)\nprint(score)","8489db2e":"# Model 3=>Decision Tree\ntree_model = DecisionTreeClassifier(random_state=10)\ntree_model.fit(X_train, Y_train)\ntree_probs = tree_model.predict_proba(X_test)\nscore = log_loss(Y_test, tree_probs)\nprint(score)","eafc2e50":"# Model 4=>K-Nearest Neighbor (KNN) classification\nKNN_model = KNeighborsClassifier(n_neighbors=5)\nKNN_model.fit(X_train, Y_train)\nKNN_probs = KNN_model.predict_proba(X_test)\nscore = log_loss(Y_test, KNN_probs)\nprint(score)","15f4ce7c":"from sklearn.metrics import jaccard_score","43527677":"# Model 1=>Logistic Regression\nLR_model = LogisticRegression(random_state=0)\nLR_model.fit(X_train, Y_train)\nLR_probs = LR_model.predict_proba(X_test)\nscore = jaccard_score(Y_test, Y_pred, pos_label=\"M\", average='binary')\nprint(score)","5d17b74d":"# Model 2=>Support Vector Machine\nSVC_model = SVC(kernel=\"rbf\",random_state=0)\nSVC_model.fit(X_train, Y_train)\nSVC_probs = SVC_model.decision_function(X_test)\nscore = jaccard_score(Y_test, Y_pred, pos_label=\"M\", average='binary')\nprint(score)","54482423":"# Model 3=>Decision Tree\ntree_model = DecisionTreeClassifier(random_state=10)\ntree_model.fit(X_train, Y_train)\ntree_probs = tree_model.predict_proba(X_test)\nscore = jaccard_score(Y_test, Y_pred, pos_label=\"M\", average='binary')\nprint(score)","54311324":"# Model 4=>K-Nearest Neighbor (KNN) classification\nKNN_model = KNeighborsClassifier(n_neighbors=5)\nKNN_model.fit(X_train, Y_train)\nKNN_probs = KNN_model.predict_proba(X_test)\nscore = jaccard_score(Y_test, Y_pred, pos_label=\"M\", average='binary')\nprint(score)","37adfe6c":"accuracies={\"SVM\": SVM_accuracy,\n            \"KNN\": KNN_accuracy,\n            \"Logistic regression\": Logistic_Regression_accuracy,\n            \"Decision Tree\": Decision_Tree_accuracy}\n\n#Plot accuracy for different models\nplt.figure(figsize=(14,6))\nplt.bar(accuracies.keys(),accuracies.values(),label=\"Accuracy\")\nplt.xlabel(\"Classifier Used\")\nplt.ylabel(\"Accuracy (%)\")\nplt.ylim(90,100)\nplt.legend()\nplt.tight_layout()\nplt.show()","1956a04a":"# Model 4=>K-Nearest Neighbor (KNN) classification","a6629e14":"# Model 1=>Logistic Regression","ac25acfb":"**Now the count for our output variable \"diagnosis\" has been made equal**","209f2535":"**Comparison of Accuracy**","dd013876":"# Model 3=>Decision Tree","5bb1ba0b":"**Correlation plot between the features:**","ead171da":"# Model 2=>Support Vector Machine","61cf5c75":"# __Data Preprocessing__ ","2a4c6826":"**Split data into training and testing sets**","9e2fe4f8":"# Jaccard Index","ab07a275":"# Logloss","420f5662":"# Acknowledgements\n* [Breast cancer classification,8 models(98.25% acc)](https:\/\/www.kaggle.com\/advikmaniar\/breast-cancer-classification-8-models-98-25-acc)","1a62c939":"We will only be using the top 20 correlated features to train our model, this will hellp improve the accuacy. ","8681fe16":"**Method to find the best value of *n_neighbors* based on accuracy**"}}