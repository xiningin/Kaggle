{"cell_type":{"4354a8d5":"code","1891ce1a":"code","5fdbee7e":"code","d8f04cf0":"code","3ed3793f":"code","5cc35ca6":"code","c202364a":"code","874fa0f9":"code","6c4b2cf4":"code","e49f869b":"code","eaa3d372":"code","f324c59f":"code","2d107b65":"code","c646afd0":"code","b0aaa991":"code","099b3118":"code","b99a9841":"code","f786bc39":"code","b4bb783a":"code","66199a38":"code","c58a0096":"code","eee13d8e":"code","506ce2b4":"code","a5159dd5":"code","65439969":"code","f16c33d9":"code","74b81c02":"code","13e52131":"code","ac483fda":"code","cd77859a":"code","62394fb7":"code","096e962f":"code","593c33e3":"code","e6103424":"code","3f13933a":"code","5ea9726d":"code","01b59d68":"code","d3278788":"code","235f9881":"code","b40eb8d8":"code","97a5ae71":"code","b91d39b6":"code","fb657569":"code","e5862805":"code","bca1f045":"code","f539ea6e":"markdown","9d039ae4":"markdown","890359ed":"markdown","101d7ba8":"markdown","6ce49478":"markdown","c3ea8296":"markdown","98020254":"markdown"},"source":{"4354a8d5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1891ce1a":"import warnings\nwarnings.filterwarnings(\"ignore\")","5fdbee7e":"train = pd.read_csv('\/kaggle\/input\/advanced-regression-preprocessed-data\/train_data.csv')\ntest = pd.read_csv('\/kaggle\/input\/advanced-regression-preprocessed-data\/test_data.csv')","d8f04cf0":"train.head()","3ed3793f":"test.index =  np.arange(1461,2920)","5cc35ca6":"X = train.iloc[:, train.columns != 'SalePrice_log1p']\ny = train.SalePrice_log1p","c202364a":"from sklearn.feature_selection import mutual_info_regression\ninformation = mutual_info_regression(X,y)\nvalue_df = pd.DataFrame({'columns':X.columns.values,'value':information})\nvalue_df.sort_values(by= 'value',ascending = False)\n\nmutual_info_useless = value_df[value_df.value == 0]['columns']","874fa0f9":"import xgboost as xgb\nxgbre = xgb.XGBRFRegressor()\nxgbre.fit(X, y)\nxgb_importance = pd.DataFrame({'columns':X.columns.values,'values':xgbre.feature_importances_}).sort_values(by = 'values',ascending=False)\nxgb_useless = xgb_importance[xgb_importance.values==0]['columns']\nxgb_useless","6c4b2cf4":"print(len(mutual_info_useless), len(xgb_useless))\nuseless_features = set(mutual_info_useless).intersection(set(xgb_useless))","e49f869b":"useless_features","eaa3d372":"train.drop(useless_features, axis = 1, inplace = True)\ntest.drop(useless_features, axis = 1, inplace = True)","f324c59f":"print(train.shape, test.shape)","2d107b65":"X = train.iloc[:, train.columns != 'SalePrice_log1p']\ny = train.SalePrice_log1p\n# splitting the data into train and test \nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.2, random_state = 10)","c646afd0":"#Validation function\nimport sklearn\nfrom sklearn.model_selection import cross_val_score,KFold\n\ndef rmse_cv(model):\n    kf = KFold(n_splits = 5, shuffle = False, random_state = 42)\n\n    rmse = np.sqrt(-cross_val_score(model, X_train[:1000], y_train[:1000], scoring ='neg_mean_squared_error', cv = kf,n_jobs =-1))\n    return(np.mean(rmse))","b0aaa991":"\ndef rmse_cv_full(model):\n    kf = KFold(n_splits = 5, shuffle = False, random_state = 42)\n\n    rmse = np.sqrt(-cross_val_score(model, X_train, y_train, scoring ='neg_mean_squared_error', cv = kf,n_jobs =-1))\n    return(np.mean(rmse))","099b3118":"rmse_cv(sklearn.ensemble.GradientBoostingRegressor())","b99a9841":"from sklearn.model_selection import cross_validate,KFold\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import LinearRegression,LassoCV, ElasticNet,Lasso\nfrom sklearn.kernel_ridge import KernelRidge\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.svm import SVR\nimport sklearn\nimport xgboost as xgb\nmodels = {\n        'linear_regression' :{\n            'model': LinearRegression() \n        },\n        \n        'decison_tree_regressor':{\n            'model': DecisionTreeRegressor(splitter='best'),\n    \n        },\n        \n        'random_forest': {\n            'model':RandomForestRegressor()\n        },\n\n         'svc' : {\n            'model' :SVR(gamma= 'auto')\n\n        },\n        'xgboost':{'model':xgb.XGBRegressor()\n               \n                  },\n    \n    \n        'adaboost':{'model':sklearn.ensemble.AdaBoostRegressor() },\n    \n    \n        'gbm':{'model':sklearn.ensemble.GradientBoostingRegressor()},\n    \n    \n        'extra_tree_regressor':{'model':sklearn.ensemble.ExtraTreesRegressor()},\n        \n        'lasso_regression':{'model':make_pipeline(RobustScaler(), Lasso(alpha =0.0005,random_state=1))},\n    \n    'elastic_net':{'model':make_pipeline(RobustScaler(), ElasticNet(alpha = 0.0005,random_state=3))},\n    'Kernal_ridge_regression':{'model': make_pipeline(RobustScaler(),KernelRidge(alpha=0.015, kernel='polynomial', degree=1, coef0=0.5))\n},\n    \n\n    \n        \n    }\n    \nscores = []\n\nfor model_names,model_params in models.items():\n    val_score = rmse_cv(model_params['model'])\n    \n    \n    scores.append({\n            'model':model_names,\n            'rmse': val_score\n        \n    })\n    \n    \npd.DataFrame(scores, columns=['model','rmse'])","f786bc39":"import optuna\nfrom optuna import Trial, visualization\n\nfrom optuna.samplers import TPESampler","b4bb783a":"def objective(trial):\n   \n    param = {\n                \"n_estimators\" : trial.suggest_int('n_estimators', 400, 900),\n                'max_depth':trial.suggest_int('max_depth', 3, 5),\n                'min_samples_split':trial.suggest_int('min_samples_split', 5,12),\n                'max_features': trial.suggest_uniform('max_features', .0,.4),\n                'subsample': trial.suggest_uniform('subsample',.2,.4),\n                'min_samples_leaf':trial.suggest_int('min_samples_leaf', 3,3),\n                'learning_rate':trial.suggest_loguniform('learning_rate',0.005,0.3),\n               'random_state' : 23  ,\n             \n    }\n    \n\n    model = sklearn.ensemble.GradientBoostingRegressor(**param)\n    return(rmse_cv(model))","66199a38":"# calling the optuna study\nstudy_gbm = optuna.create_study(direction='minimize',sampler=TPESampler())\nstudy_gbm.optimize(objective, n_trials= 300,show_progress_bar = True)","c58a0096":"optuna.visualization.plot_slice(study_gbm)","eee13d8e":"study_gbm.best_params\n","506ce2b4":"gbm_tuned = sklearn.ensemble.GradientBoostingRegressor(**study_gbm.best_params)\n","a5159dd5":"def objective(trial):\n   \n    param = {\n               'alpha':trial.suggest_loguniform('alpha',.0000005,.5),\n              \n    }\n    \n    model = make_pipeline(RobustScaler(),Lasso(**param))\n    return(rmse_cv(model))","65439969":"# calling the optuna study\nstudy_lasso = optuna.create_study(direction='minimize',sampler=TPESampler())\nstudy_lasso.optimize(objective, n_trials= 300,show_progress_bar = True)","f16c33d9":"study_lasso.best_params","74b81c02":"lasso_tuned= make_pipeline(RobustScaler(),Lasso(**study_lasso.best_params, random_state = 1))\nrmse_cv(lasso_tuned)","13e52131":"def objective(trial):\n   \n    param = {\n               'alpha':trial.suggest_loguniform('alpha',.0000005,.5),\n        'l1_ratio':trial.suggest_uniform('l1_ratio',.01,.9),\n        'random_state':3\n              \n    }\n    \n    model = make_pipeline(RobustScaler(),ElasticNet(**param))\n    return(rmse_cv(model))","ac483fda":"# calling the optuna study\nstudy_elastic = optuna.create_study(direction='minimize',sampler=TPESampler())\nstudy_elastic.optimize(objective, n_trials= 300,show_progress_bar = True)","cd77859a":"study_elastic.best_params","62394fb7":"elastic_tuned = make_pipeline(RobustScaler(),ElasticNet(**study_elastic.best_params, random_state = 3))","096e962f":"def objective(trial):\n   \n    param = {\n               'alpha':trial.suggest_loguniform('alpha',.0000005,.5),\n        'coef0':trial.suggest_uniform('coef0',.01,.9),\n        'degree': trial.suggest_int('degree',1,6),\n        'kernel':trial.suggest_categorical('kernel',['polynomial','linear']),\n      \n    }\n    model = make_pipeline(RobustScaler(),KernelRidge(**param))\n    return(rmse_cv(model))\n","593c33e3":"# calling the optuna study\nstudy_kkr = optuna.create_study(direction='minimize',sampler=TPESampler())\nstudy_kkr.optimize(objective, n_trials= 300,show_progress_bar = True)","e6103424":"study_kkr.best_params","3f13933a":"kkr_tuned = make_pipeline( RobustScaler(),KernelRidge(**study_kkr.best_params))\nrmse_cv(kkr_tuned)","5ea9726d":"models = {\n        'tuned_kkr':kkr_tuned,\n    'tuned_gbm':gbm_tuned,\n    'tuned_lasso': lasso_tuned,\n    'tuned_elastic': elastic_tuned\n        \n    }\n    \nscores = []\n\nfor model_names,model_params in models.items():\n    val_score = rmse_cv_full(model_params)\n    \n    \n    scores.append({\n            'model':model_names,\n            'rmse': val_score\n        \n    })\n    \n    \npd.DataFrame(scores, columns=['model','rmse'])","01b59d68":"from mlxtend.regressor import StackingCVRegressor\nregressors = [kkr_tuned,lasso_tuned,elastic_tuned,gbm_tuned]\n","d3278788":"from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone","235f9881":"model = StackingCVRegressor(regressors=regressors, meta_regressor=Lasso(alpha = .0019),\n                           cv=10,     random_state=1)\n\n    \nrmse_cv_full(model)","b40eb8d8":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","97a5ae71":"averaged_models = AveragingModels(models = (elastic_tuned, gbm_tuned, lasso_tuned, kkr_tuned))","b91d39b6":"model.fit(X,y)","fb657569":"log_price = model.predict(test)","e5862805":"SalePrice = np.expm1(log_price)","bca1f045":"submission = pd.DataFrame({'Id':test.index.values, 'SalePrice':SalePrice})\nsubmission.to_csv('\/kaggle\/working\/submission4.csv', index = False)","f539ea6e":"## 2. now use the filtered data for training","9d039ae4":"### Tuning Lasso Regression","890359ed":"# Now we will stack gbm, kkr, lasso, elasticnet","101d7ba8":"### Tuning Kernal Ridge Regression","6ce49478":"### Tuning the ElasticNet","c3ea8296":"### ElasticNet lasso regression will have good result along with gbm model so lets tune them","98020254":"## 1.let's do some feature selection"}}