{"cell_type":{"8b379ad1":"code","9c058000":"code","569d87b3":"code","7d2b81bb":"code","471d9f28":"code","0957587a":"code","d986c335":"code","66d22539":"code","77dcc45e":"code","7c51d0fe":"code","368a3f82":"code","dafa0d0d":"code","f295e4f2":"code","670f612d":"code","fcee89d3":"markdown","dce78657":"markdown","f05124d7":"markdown","e92872d4":"markdown","75bc7704":"markdown","79584dda":"markdown","50340e01":"markdown"},"source":{"8b379ad1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9c058000":"train = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv')","569d87b3":"print(train.head())","7d2b81bb":"print(test.head())","471d9f28":"def rmse(targets, predictions):\n    return np.sqrt(((predictions - targets) ** 2).mean())","0957587a":"y = train['target']\nX = train.copy().drop(columns=['id', 'target'])","d986c335":"categorical_cols = [c for c in X.columns if X[c].dtype == 'object']\nnumerical_cols = [c for c in X.columns if X[c].dtype in ['int64', 'float64']]\n\nencoder = OrdinalEncoder()\nX[categorical_cols] = encoder.fit_transform(X[categorical_cols])\n\nscaler = StandardScaler()\nX[numerical_cols] = scaler.fit_transform(X[numerical_cols])","66d22539":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33, random_state=7)","77dcc45e":"# XGBoost\nxgb_model = XGBRegressor(n_estimators=500, learning_rate=0.05, random_state=7)\nxgb_model.fit(X_train, y_train, early_stopping_rounds=50, eval_set=[(X_valid, y_valid)], verbose=False)\nxgb_preds = xgb_model.predict(X_valid)\nxgb_score = rmse(y_valid, xgb_preds)\nprint('RMSE:', xgb_score)","7c51d0fe":"# setting base parameters for all XGBoost models\nparams = {\n    'n_estimators' : 5000,\n    'subsample' : 0.9,\n    'booster' : 'gbtree',\n    'n_jobs' : 2,\n}","368a3f82":"# #Using KFold from scikit learn to split the data\n# kf = KFold(n_splits = 5, shuffle=True, random_state = 6)\n\n# error = []\n# # Using itertools to create a grid search method for tuning the learning_rate and max_depth hyper parameters\n# # This creates 45 models and \n# for learning_rate, max_depth in itertools.product([0.1, 0.5, 0.9], [3, 6, 9]):\n#     xgb = XGBRegressor(**params, learning_rate=learning_rate, max_depth=max_depth)\n#     cv_error = []\n#     preds = []\n#     for train_index, valid_index in kf.split(X):\n#         X_train, X_valid = X.loc[train_index], X.loc[valid_index]\n#         y_train, y_valid = y.loc[train_index], y.loc[valid_index]\n        \n#         xgb.fit(X_train, y_train, early_stopping_rounds=50, eval_set=[(X_valid, y_valid)], verbose=False)\n#         preds = xgb.predict(X_valid)\n#         cv_error = rmse(y_valid, preds)\n#     error.append(\n#     {\n#         'learning_rate' : learning_rate,\n#         'max_depth' : max_depth,\n#         'avg_error' : np.mean(cv_error)\n#     })\n# errors_df = pd.DataFrame(error)        ","dafa0d0d":"# print(errors_df)","f295e4f2":"# XGBoost\nxgb_model_best = XGBRegressor(**params, learning_rate=0.1, max_depth=3)\nxgb_model_best.fit(X_train, y_train, early_stopping_rounds=50, eval_set=[(X_valid, y_valid)], verbose=False)\nxgb_preds_best = xgb_model_best.predict(X_valid)\nxgb_score_best = rmse(y_valid, xgb_preds_best)\nprint('RMSE:', xgb_score_best)","670f612d":"X_test = test.copy().drop(columns=['id'])\nX_test[categorical_cols] = encoder.transform(X_test[categorical_cols])\nX_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\npredictions = xgb_model_best.predict(X_test)\noutput = pd.DataFrame({'id': test.id,\n                       'target': predictions})\noutput.to_csv('submission.csv', index=False)","fcee89d3":"## Training an XGBoost model, with verbose = True to see output!","dce78657":"## Submission!","f05124d7":"This next part of code takes two hours to run so commented out. KFold cross validation with Grid Search yields optimal values learning_rate = 0.1 and max_depth = 3","e92872d4":"## Transforming into X and y","75bc7704":"## This score is ok, can we tune the hyper parameters with K-Fold cross validation?","79584dda":"# **Reading in the data**","50340e01":"## Data preprocessing and inital train test split"}}