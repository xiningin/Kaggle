{"cell_type":{"b28213d1":"code","c3f86339":"code","bb31f3d4":"code","ad655b70":"code","9ef72f5c":"code","447fece8":"code","b7f35db8":"code","c6991b34":"code","198741ee":"code","a6da40a4":"code","6be2f189":"code","25d1d49a":"code","6ecd4acd":"code","968deb75":"code","06cab69b":"code","d03c1edc":"code","878e51ca":"code","fd41dc61":"code","29daf31a":"code","d222977f":"code","048f187b":"code","f97ab6e9":"code","dcaa76dc":"code","ee932cce":"code","f72d6336":"code","57c82d18":"code","03c1cfb3":"code","540878c2":"code","615b737c":"code","d6e92643":"code","e01dd9e2":"code","b41397e7":"code","c47a5767":"code","581468e9":"markdown","cc33bf37":"markdown","eba4784c":"markdown","0d2c4446":"markdown","523f0cc8":"markdown","1fd44816":"markdown","b5083076":"markdown","31c92bc4":"markdown","1dc1b798":"markdown","4a2eae88":"markdown","1c72ac80":"markdown","eb483498":"markdown","7426c1a6":"markdown","bfaf0c62":"markdown","e550a8a9":"markdown","cd7f9666":"markdown","1c15f781":"markdown","21b02ee9":"markdown","e8de7a9c":"markdown","5991aaf0":"markdown","769c13c3":"markdown","233d8c14":"markdown","43391222":"markdown","99dbe600":"markdown","c14491d9":"markdown","a140fce8":"markdown","11d8e7e8":"markdown","878d1631":"markdown","936532c5":"markdown","cb8fbe86":"markdown","922a5f79":"markdown","5ca573e4":"markdown","fd837eb4":"markdown"},"source":{"b28213d1":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nimport statsmodels.api as sm\nimport seaborn as sns\n#import pmdarima as pm\nfrom dateutil.relativedelta import relativedelta\nimport warnings\nwarnings.simplefilter(action='ignore')\nfrom sklearn.metrics import accuracy_score\nimport time\nimport tensorflow as tf\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, SimpleRNN, LSTM, Activation, Dropout\nimport math\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport fbprophet  # if you have an issue (on your workstation) with fbprophet on python3.8 i would suggest you to use python3.7 instead\nfrom fbprophet import Prophet\nfrom numpy import median\nimport math\nimport statistics","c3f86339":"# reading and inserting the csv sheet into DataFrame\ndf = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\ndf.head(3)\n","bb31f3d4":"# No null cells!!\ndf.isnull().sum()","ad655b70":"# reading and inserting the calendar csv sheet into Dataframe, this data will be used as lookup to get the right date for each day in df dataframe\ncalendar = pd.read_csv('..\/input\/m5-forecasting-accuracy\/calendar.csv')\ncalendar.head(3)","9ef72f5c":"df.drop(columns=['item_id', 'dept_id','cat_id','store_id','state_id'],axis=1, inplace=True) # Drop the unnecessary columns\ndata = df.T  # seting the days as index for new dataframe(Data)\ndata.columns = df['id'] # ading product id column to Data \ndata.index.name = None   # remove the name of index\ndata.drop(index='id',inplace=True) # remove the first row\n# change the format and the freq. of index to datetime format\ndata.index = calendar['date'][0:1913]\ndata.index = pd.to_datetime(data.index)\ndata.index.freq= 'd'\ndata.index","447fece8":"max_value =data.max().to_frame()\nmax_prod = max(data.max())\nmax_value.loc[max_value[0] == max_prod]","b7f35db8":"# WE noctice that numbe of purchasments is zero for arround first 10 months which looks justified if product was not linched or avaliable yet in the stores at that time thats why this interval will be ecluded from time series \nplt.figure(figsize=(12,8))\nplt.plot(data.index,data['FOODS_3_090_CA_3_validation'])","c6991b34":"# Using the decompostion method...\nss_decomposition = seasonal_decompose(x=data['FOODS_3_090_CA_3_validation'])\nestimated_trend = ss_decomposition.trend\nestimated_seasonal = ss_decomposition.seasonal\nestimated_residual = ss_decomposition.resid\n\n#fig, ax = plt.subplots(3,1, figsize=(12,8))\nfig, ax = plt.subplots(3,figsize=(12,8) )\n#plt.figure(figsize=(12,5))\n#plt.subplot(3,1,1)\nax[0].plot(data.index[250:],estimated_trend[250:], label='Trend')\nax[0].legend(loc='upper left')\n\n#plt.subplot(3,1,2)\nax[1].plot(data.index[250:300],estimated_seasonal[250:300], label='Seasonal')\nax[1].legend(loc='upper left')\n\n#plt.subplot(3,1,3)\nax[2].plot(data.index,estimated_residual, label='Residual')\nax[2].legend(loc='upper left')\n\nadf, pvalue, usedlag, nobs, critical_values, icbest = adfuller(data['FOODS_3_090_CA_3_validation'])\n\nprint(f'adf is {adf}')\nprint(f'pvalue is {pvalue}')\n","198741ee":"#plt.subplot(3,1,4)\nplot_acf(data['FOODS_3_090_CA_3_validation'],lags=100, zero=False)\n\n\n#plt.subplot(3,1,5)\nplot_pacf(data['FOODS_3_090_CA_3_validation'], zero=False)","a6da40a4":"# diffrencing method\n\ndata['shift'] = data['FOODS_3_090_CA_3_validation'].shift(7)\ndata['week_lag'] = data['FOODS_3_090_CA_3_validation'] - data['shift']\n\nss_decomposition = seasonal_decompose(x=data['week_lag'].dropna())\nlag_trend = ss_decomposition.trend\nlag_seasonal = ss_decomposition.seasonal\nlag_residual = ss_decomposition.resid\n\n#fig, ax = plt.subplots(3,1, figsize=(12,8))\nfig, ax = plt.subplots(3,figsize=(12,8) )\n#plt.figure(figsize=(12,5))\n#plt.subplot(3,1,1)\nax[0].plot(data.index[7:],lag_trend, label='Trend')\nax[0].legend(loc='upper left')\n\n#plt.subplot(3,1,2)\nax[1].plot(data.index[800:830],lag_seasonal[800:830], label='Seasonal')\nax[1].legend(loc='upper left')\n\n#plt.subplot(3,1,3)\nax[2].plot(data.index[:30],lag_residual[:30], label='Residual')\nax[2].legend(loc='upper left')\n\nadf_dif, pvalue_dif,usedlag, nobs, critical_values, icbest = adfuller(data['week_lag'].dropna())\n\nprint(f'adf is {adf_dif}')\nprint(f'pvalue is {pvalue_dif}')\n","6be2f189":"#plt.subplot(3,1,4)\nplot_acf(data['week_lag'][7:],zero=False)\n\n\n#plt.subplot(3,1,5)\nplot_pacf(data['week_lag'][7:],zero=False)","25d1d49a":"start_interval = 250 # removing the intial interval where no sold item data is shown\ntest_sample= 100\ntrain_SARIMA = data['FOODS_3_090_CA_3_validation'][start_interval:-test_sample].astype('float')\ntest_SARIMA = train_SARIMA[-test_sample:].astype('float')\ntrain_LSTM = data['FOODS_3_090_CA_3_validation'][start_interval:-2*test_sample].astype('float')\ntest_LSTM = data['FOODS_3_090_CA_3_validation'][-2*test_sample:-test_sample].astype('float')\ntrain_prophet = data[['FOODS_3_090_CA_3_validation']][start_interval:-test_sample].astype('float')\ntrue = data['FOODS_3_090_CA_3_validation'][-test_sample:].astype('float')\nprint('full data length', data[['FOODS_3_090_CA_3_validation']][start_interval:].shape)\nprint('train_SARIMA', train_SARIMA.shape)\nprint('test_SARIMA', test_SARIMA.shape)\nprint('train_LSTM', train_LSTM.shape)\nprint('test_LSTM', test_LSTM.shape)\nprint(\"train prophet\", train_prophet.shape)\nprint(\"true \", true.shape)","6ecd4acd":"# Custamized grid search\n\n\np_min,p_max = 0,3\nd_min,d_max= 0,2\nq_min,q_max = 0,2\n\nP_min,P_max = 0,3\nD_min, D_max = 0,2\nQ_min,Q_max = 0,2\n\ntrend= 'n' # ['n','c','t','ct']\nm=42\n\np_m = int(round(median([p_min,p_max]),0))\nd_m = int(round(median([d_min,d_max]),0))\nq_m = int(round(median([q_min,q_max]),0))\nP_m = int(round(median([P_min,P_max]),0))\nD_m = int(round(median([D_min,D_max]),0))          \nQ_m = int(round(median([Q_min,Q_max]),0))          \n          \n\nstart_time_init = time.time()\nduppl_order = []\n\nresult = []\n\nif (p_m,d_m,q_m) != (p_max,d_max,q_max):\n    order = (p_m,q_m,d_m)\n    seasonal_order = (P_m,D_m,Q_m,m)\n    \n    \n    if order + seasonal_order in duppl_order:\n        pass\n    else:\n        model2 = sm.tsa.statespace.SARIMAX(train_SARIMA[-400:], order=order, seasonal_order=seasonal_order, trend='n').fit()\n        yhat = round(model2.predict(start = '2016-01-16' , end= '2016-04-24', dynamic=True),0)\n        rmse_m = math.sqrt(sum(pow((yhat - true),2))\/len(true))\n        result.append(({'order': (p_m,d_m,q_m),'seasonal_order': seasonal_order,'rmse':rmse_m}))\n        duppl_order.append(order+seasonal_order)\n        print(order,seasonal_order,'rmse_(p_m,q_m,d_m):', rmse_m)\n\n    \n    order = (p_max,d_max,q_max)\n    if order + seasonal_order in duppl_order:\n        pass\n    else:\n        model2 = sm.tsa.statespace.SARIMAX(train_SARIMA[-400:], order=order, seasonal_order=seasonal_order, trend='n').fit()\n        yhat = round(model2.predict(start = '2016-01-16' , end= '2016-04-24', dynamic=True),0)\n        rmse_max = math.sqrt(sum(pow((yhat - true),2))\/len(true))\n        result.append(({'order': (p_max,d_max,q_max),'seasonal_order': seasonal_order,'rmse':rmse_max}))\n        duppl_order.append(order+seasonal_order)\n        print(order,seasonal_order,'rmse_(p_max,d_max,q_max):',rmse_max)\n\n    order = (p_min,d_min,q_min)\n    if order + seasonal_order in duppl_order:\n        pass\n    else:\n        model2 = sm.tsa.statespace.SARIMAX(train_SARIMA[-400:], order=order, seasonal_order=seasonal_order, trend='n').fit()\n        yhat = round(model2.predict(start = '2016-01-16' , end= '2016-04-24', dynamic=True),0)\n        rmse_min = math.sqrt(sum(pow((yhat - true),2))\/len(true))\n        result.append(({'order': (p_min,q_min,d_min),'seasonal_order': seasonal_order,'rmse':rmse_min}))\n        duppl_order.append(order+seasonal_order)\n        print(order,seasonal_order,'rmse_(p_min,d_min,q_min):',rmse_min)\n    \n   \n    if rmse_max < rmse_min:\n        p_loop = range(p_m,p_max)\n        d_loop = range(d_m,d_max)\n        q_loop = range(q_m,q_max)\n    else:\n        p_loop = range(p_min,p_m)\n        d_loop = range(d_min,d_m)\n        q_loop = range(q_min,q_m)\n        \n        \n    for p_ in p_loop:\n        for d_ in d_loop:\n            for q_ in q_loop:\n                start_time = time.time()\n                order=(p_,d_,q_)\n                seasonal_order = (P_m,D_m,Q_m,m)\n                if order + seasonal_order in duppl_order:\n                    pass\n                else:\n                    model2 = sm.tsa.statespace.SARIMAX(train_SARIMA[-400:], order=order, seasonal_order=seasonal_order, trend='n').fit()\n                    yhat = round(model2.predict(start = '2016-01-16' , end= '2016-04-24', dynamic=True),0)\n                    rmse = math.sqrt(sum(pow((yhat - true),2))\/len(true))\n                    result.append(({'order':order,'seasonal_order':seasonal_order,'rmse':rmse}))\n                    duppl_order.append(order+seasonal_order)\n                    duration = time.time() - start_time\n                    print(order,seasonal_order,'rmse:' ,rmse , 'duration:',duration)\n                    \n    output = pd.DataFrame(result)\n    fit_model = output[output['rmse'] == (output['rmse'].min())]\n    order_fit = fit_model.order.values[0]\n    \n    \n    if (P_m,D_m,Q_m) != (P_max,D_max,Q_max):\n        order = order_fit\n        seasonal_order = (P_m,D_m,Q_m,m)\n        \n        if order + seasonal_order in duppl_order:\n            pass\n        else:\n            model2 = sm.tsa.statespace.SARIMAX(train_SARIMA[-400:], order=order, seasonal_order=seasonal_order, trend='n').fit()\n            yhat = round(model2.predict(start = '2016-01-16' , end= '2016-04-24', dynamic=True),0)\n            Rmse_m = math.sqrt(sum(pow((yhat - true),2))\/len(true))\n            result.append(({'order': order_fit,'seasonal_order': (P_m,D_m,Q_m,m),'rmse':Rmse_m}))\n            duppl_order.append(order+seasonal_order)\n            print(order,seasonal_order,'Rmse_(P_m,D_m,Q_m,m):', Rmse_m)\n        \n        seasonal_order = (P_max,D_Max,Q_Max,m)\n        if order + seasonal_order in duppl_order:\n            pass\n        else:\n            model2 = sm.tsa.statespace.SARIMAX(train_SARIMA[-400:], order=order, seasonal_order=seasonal_order, trend='n').fit()\n            yhat = round(model2.predict(start = '2016-01-16' , end= '2016-04-24', dynamic=True),0)\n            Rmse_max = math.sqrt(sum(pow((yhat - true),2))\/len(true))\n            result.append(({'order': order_fit,'seasonal_order': (P_max,D_max,Q_max,m),'rmse':Rmse_max}))\n            duppl_order.append(order+seasonal_order)\n            print(order,seasonal_order,'Rmse_(P_max,D_max,Q_max,m):',Rmse_max)\n        \n        seasonal_order = (P_min,D_min,Q_min,m)\n        if order + seasonal_order in duppl_order:\n            pass\n        else:\n            model2 = sm.tsa.statespace.SARIMAX(train_SARIMA[-400:], order=order, seasonal_order=seasonal_order, trend='n').fit()\n            yhat = round(model2.predict(start = '2016-01-16' , end= '2016-04-24', dynamic=True),0)\n            Rmse_min = math.sqrt(sum(pow((yhat - true),2))\/len(true))\n            result.append(({'order': order_fit,'seasonal_order': (P_min,D_min,Q_min,m),'rmse':Rmse_min}))\n            duppl_order.append(order+seasonal_order)\n            print(order,seasonal_order,'Rmse_(P_min,D_min,Q_min,m):',Rmse_min)\n\n        \n        if rmse_max < rmse_min:\n            P_loop = range(P_m,P_max)\n            D_loop = range(D_m,D_max)\n            Q_loop = range(Q_m,Q_max)\n        else:\n            P_loop = range(P_min,P_m)\n            D_loop = range(D_min,D_m)\n            Q_loop = range(Q_min,Q_m)\n        \n        \n        for P_ in P_loop:\n            for D_ in D_loop:\n                for Q_ in Q_loop:\n                    start_time = time.time()\n                    order = order_fit\n                    seasonal_order = (P_,D_,Q_,m)\n                    if order + seasonal_order in duppl_order:\n                        pass\n                    else:\n                        model2 = sm.tsa.statespace.SARIMAX(train_SARIMA[-400:], order=order, seasonal_order= seasonal_order, trend='n').fit()\n                        yhat = round(model2.predict(start = '2016-01-16' , end= '2016-04-24', dynamic=True),0)\n                        rmse = math.sqrt(sum(pow((yhat - true),2))\/len(true))\n                        result.append(({'order':order_fit,'seasonal_order':(P_,D_,Q_,m),'rmse':rmse}))\n                        duppl_order.append(order+seasonal_order)\n                        duration = time.time() - start_time\n                        print(order,(P_,D_,Q_,m),'rmse:' ,rmse , 'duration:',duration)\n        output2 = pd.DataFrame(result)\n        fit_model2 = output2[output2['rmse'] == (output2['rmse'].min())]\n        print('best model: ',fit_model2.order.values,fit_model2.seasonal_order.values, 'rmse:', fit_model2.rmse.values)\n    \n    \n    else:\n        order = order_fit\n        seasonal_order_best = (P_m,D_m,Q_m,m)\n        \n        if order + seasonal_order in duppl_order:\n            pass\n        else:\n            model2 = sm.tsa.statespace.SARIMAX(train_SARIMA[-400:], order=order, seasonal_order=seasonal_order, trend='n').fit()\n            yhat = round(model2.predict(start = '2016-01-16' , end= '2016-04-24', dynamic=True),0)\n            rmse = math.sqrt(sum(pow((yhat - true),2))\/len(true))\n            result.append(({'order': order,'seasonal_order': seasonal_order,'rmse':rmse}))\n            duppl_order.append(order+seasonal_order)\n            print(order,seasonal_order,'rmse:',rmse)\n\nelse:\n    order = (p_m,q_m,d_m)\n    if (P_m,D_m,Q_m) != (P_max,D_max,Q_max):\n        \n        \n        seasonal_order = (P_m,D_m,Q_m,m)\n        if order + seasonal_order in duppl_order:\n            pass\n        else:\n            model2 = sm.tsa.statespace.SARIMAX(train_SARIMA[-400:], order=order, seasonal_order= seasonal_order, trend='n').fit()\n            yhat = round(model2.predict(start = '2016-01-16' , end= '2016-04-24', dynamic=True),0)\n            Rmse_m = math.sqrt(sum(pow((yhat - true),2))\/len(true))\n            result.append(({'order': order_fit,'seasonal_order': (P_m,D_m,Q_m,m),'rmse':Rmse_m}))\n            duppl_order.append(order+seasonal_order)\n            print(order,seasonal_order,'Rmse_(P_m,D_m,Q_m,m):', Rmse_m)\n        \n        seasonal_order =  (P_max,D_max,Q_max,m)\n        if order + seasonal_order in duppl_order:\n            pass\n        else:\n            model2 = sm.tsa.statespace.SARIMAX(train_SARIMA[-400:], order=order, seasonal_order=seasonal_order, trend='n').fit()\n            yhat = round(model2.predict(start = '2016-01-16' , end= '2016-04-24', dynamic=True),0)\n            Rmse_max = math.sqrt(sum(pow((yhat - true),2))\/len(true))\n            result.append(({'order': order_fit,'seasonal_order': (P_max,D_max,Q_max,m),'rmse':Rmse_max}))\n            duppl_order.append(order+seasonal_order)\n            print(order,seasonal_order,'Rmse_(P_max,D_max,Q_max,m):',Rmse_max)\n            \n        seasonal_order = (P_min,D_min,Q_min,m)\n        if order + seasonal_order in duppl_order:\n            pass\n        else:\n            model2 = sm.tsa.statespace.SARIMAX(train_SARIMA[-400:], order=order, seasonal_order= seasonal_order, trend='n').fit()\n            yhat = round(model2.predict(start = '2016-01-16' , end= '2016-04-24', dynamic=True),0)\n            Rmse_min = math.sqrt(sum(pow((yhat - true),2))\/len(true))\n            result.append(({'order': order_fit,'seasonal_order': (P_min,D_min,Q_min,m),'rmse':Rmse_min}))\n            duppl_order.append(order+seasonal_order)\n            print(order,seasonal_order,'Rmse_(P_min,D_min,Q_min,m):',Rmse_min)\n        \n        \n            \n        if rmse_max < rmse_min:\n            P_loop = range(P_m,P_max)\n            D_loop = range(D_m,D_max)\n            Q_loop = range(Q_m,Q_max)\n            \n        else:\n            P_loop = range(P_min,P_m)\n            D_loop = range(D_min,D_m)\n            Q_loop = range(Q_min,Q_m)\n            \n        \n        for P_ in P_loop:\n            for D_ in D_loop:\n                for Q_ in Q_loop:\n                    start_time = time.time()\n                    \n                    seasonal_order = (P_,D_,Q_,m)\n                    if order + seasonal_order in duppl_order:\n                        pass\n                    else:\n                        model2 = sm.tsa.statespace.SARIMAX(train_SARIMA[-400:], order=order, seasonal_order= seasonal_order, trend='n').fit()\n                        yhat = round(model2.predict(start = '2016-01-16' , end= '2016-04-24', dynamic=True),0)\n                        rmse = math.sqrt(sum(pow((yhat - true),2))\/len(true))\n                        result.append(({'order':order_fit,'seasonal_order':(P_,D_,Q_,m),'rmse':rmse}))\n                        duppl_order.append(order+seasonal_order)\n                        duration = time.time() - start_time\n                        print(order,(P_,D_,Q_,m),'rmse:' ,rmse , 'duration:',duration)\n        \n        \n        output2 = pd.DataFrame(result)\n        fit_model2 = output2[output2['rmse'] == (output2['rmse'].min())]\n        print('best model: ',fit_model2.order.values,fit_model2.seasonal_order.values, 'rmse:', fit_model2.rmse.values)\n        \n    else:\n        seasonal_order = (P_m,D_m,Q_m,m)\n        \n        if order + seasonal_order in duppl_order:\n            pass\n        else:\n            model2 = sm.tsa.statespace.SARIMAX(train_SARIMA[-400:], order=order, seasonal_order=seasonal_order, trend='n').fit()\n            yhat = round(model2.predict(start = '2016-01-16' , end= '2016-04-24', dynamic=True),0)\n            rmse = math.sqrt(sum(pow((yhat - true),2))\/len(true))\n            result.append(({'order': order,'seasonal_order': seasonal_order,'rmse':rmse}))\n            duppl_order.append(order+seasonal_order)\n            print(order,seasonal_order,'rmse:',rmse)\n \nduration = time.time() - start_time_init\n\nprint('Full total time: ',duration)","968deb75":"start_time = time.time()\nseasonal_order = (2, 1, 1, 42)\norder_m = (2,1,1)\nmodel_sarima = sm.tsa.statespace.SARIMAX(train_SARIMA, order=order_m, seasonal_order=seasonal_order, trend='n').fit()\nduration = (time.time() - start_time) \/ 60\nprint(f'training is done within:{duration} minutes')","06cab69b":"# out of sample prediction for (100 days):\nyhat_sarima = model_sarima.predict(start = '2016-01-16' , end= '2016-04-24', dynamic=True)","d03c1edc":"sarimax_Rmse = []\nfor i in np.arange(10,100,10):\n    sarimax_Rmse.append(math.sqrt(sum(pow((yhat_sarima[:i] - true[:i]),2))\/len(true[:i])))\nsarimax_Rmse","878e51ca":"future_fcst = model_sarima.get_forecast(100)\n# That will have a method to pull in confidence interval \nconfidence_int = future_fcst.conf_int(alpha = 0.01)\nconfidence_int['yhat'] = abs(confidence_int['lower FOODS_3_090_CA_3_validation'] + confidence_int['upper FOODS_3_090_CA_3_validation']\/2)\nconfidence_int","fd41dc61":"plt.figure(figsize=(12,8))\nplt.plot(true, label='Ground Truth')\nplt.plot(yhat_sarima, label='Predicted')\nplt.title('SARIMAX Model --- Ground Truth Vs Predicted(Rmse= 32.42313988')\nplt.ylabel('Sales')\n\n\nplt.legend()","29daf31a":"model_sarima.plot_diagnostics(lags=12,figsize = (20,10),);","d222977f":"# Create x_train,y_train for LSTM model\n\ninput_days = 12\nx_train, y_train = [],[]\nfor i in range(0,train_LSTM.shape[0] - input_days,3):\n    x_train.append(train_LSTM[i:i+input_days])\n    y_train.append(train_LSTM[i+input_days])\n\n    \nx_train = np.array(x_train)\nx_train = x_train.reshape(x_train.shape[0],x_train.shape[1],1)\ny_train = np.asarray(y_train)\nprint('x_train shape', x_train.shape)\nprint('y_train shape', y_train.shape)","048f187b":"# building simple LSMT model:\n\nstart_time = time.time()\ncell_units = 1000\nepochs = 1500\n\nmodel_LSTM = Sequential() \n\nmodel_LSTM.add(LSTM(cell_units,input_shape=(x_train.shape[1],1))) #return_sequences= True))\n    \n\nmodel_LSTM.add(Dense(1))\n    \n\nmodel_LSTM.compile(loss='mean_squared_error', optimizer='adam')\nmodel_LSTM.fit(x_train, y_train, epochs=epochs, batch_size=64, verbose=1)\n\n\nduration = (time.time() - start_time) \/ 60\nprint(f'training is done within:{duration} minutes')","f97ab6e9":"model_LSTM.summary()","dcaa76dc":"# Creating x_test data (its shape should be match with training data and with input_shape)\nx_test = []\nfor i in range(0,test_LSTM.shape[0] - input_days + 1):\n    x_test.append(test_LSTM[i:i + input_days])\n               \nx_test = np.asarray(x_test)    \nx_test = x_test.reshape(x_test.shape[0],x_test.shape[1],1)\nprint('x_test shape', x_test.shape)\n","ee932cce":"# prediction function which goes through each row where each row contains 12 data cells and each cell value will be replaced by predicted value...\npred = []\n\nfor i in range(x_test.shape[0]):\n    for _ in range(x_test.shape[1]):\n        prediction = model_LSTM.predict(x_test[i:i+1,:,:])\n        pred.append(prediction)\n        x_test[i,:-1,:] = x_test[i,1:,:]\n        x_test[i,-1,:] = prediction","f72d6336":"# Reshape the output of precidiction array from (89,12,1) shape to (100,) shape (input_days = 12)\nyhat_lstm = np.arange(0,100,1,'float')\ny_flatten = x_test.flatten()\nyhat_lstm[:input_days] = y_flatten[:input_days] # \nk = 2\nfor i in range(input_days,test_LSTM.shape[0]):\n    yhat_lstm[i] = y_flatten[(k*input_days) - 1]  # {formula:2*input_days - 1}\n    k +=1\n    \nlstm_result = pd.DataFrame(data=yhat_lstm, index=true.index, columns=['yhat'])\nlstm_result['true'] = true","57c82d18":"lstm_Rmse = []\n\nfor i in np.arange(10,100,10):\n    lstm_rmse = math.sqrt(sum(pow((lstm_result['yhat'][:i] - true[:i]),2))\/len(true[:i]))\n    lstm_Rmse.append(lstm_rmse)\nlstm_Rmse\n\n","03c1cfb3":"plt.figure(figsize=(12,8))\nplt.plot(true, label='Ground Truth')\nplt.plot(lstm_result.yhat, label='Predicted')\nplt.title('LSTM Model --- Ground Truth Vs Predicted(Rmse= {})'.format(lstm_rmse))\nplt.legend()","540878c2":"# prepare dataframe for fprophet model\ntrain_prophet['ds'] = train_prophet.index.values\ntrain_prophet.rename(columns={'FOODS_3_090_CA_3_validation':'y'},inplace=True)\ntrain_prophet = train_prophet[['ds','y']]\ntrain_prophet.columns.name = None\ntrain_prophet","615b737c":"#fprophet model without holidays\nstart_time = time.time()\nm = Prophet(weekly_seasonality=True)\nm.fit(train_prophet)\n\nfuture = m.make_future_dataframe(periods=100)\nforecast = m.predict(future)\npredicted_prophet = forecast[['yhat']][-test_sample:]\npredicted_prophet.set_index(true.index,inplace=True)\npredicted_prophet.index\n\nduration = time.time() - start_time\nprint(f'training is done within:{duration} minutes')\n\n\nplt.figure(figsize=(12,8))\nplt.plot(true, label='Ground Truth')\nplt.plot(predicted_prophet, label='Predicted')\nplt.title('fbprophet(Without Holidays) --- Ground Truth Vs Predicted(Rmse= 76')\nplt.ylabel('Sales')\nplt.legend()","d6e92643":"#fprophet model with holidays\n\nholidays = calendar[calendar['event_type_1'].isnull() == False]\nholidays = holidays[['date','event_name_1', 'event_type_1']]\nholidays.rename(columns={'date':'ds','event_name_1':'holiday'},inplace=True)\n\nstart_time = time.time()\n\nm_holi = Prophet(weekly_seasonality=True,holidays=holidays)\nm_holi.fit(train_prophet)\n\nfuture = m_holi.make_future_dataframe(periods=100)\nforecast = m_holi.predict(future)\npredicted_prophet = forecast[['yhat']][-test_sample:]\npredicted_prophet.set_index(true.index,inplace=True)\npredicted_prophet.index\n\nprint(f'training is done within:{duration} minutes')\n\n","e01dd9e2":"prophet_Rmse = []\nfor i in np.arange(10,100,10):\n    prophet_rmse = math.sqrt(sum(pow((predicted_prophet.yhat[:i] - true[:i]),2))\/len(true[:i]))\n    prophet_Rmse.append(prophet_rmse)\nprophet_Rmse","b41397e7":"plt.figure(figsize=(12,8))\nplt.plot(true, label='Ground Truth')\nplt.plot(predicted_prophet, label='Predicted')\nplt.title('fbprophet(With Holidays) --- Ground Truth Vs Predicted(Rmse= {}'.format(prophet_rmse))\nplt.ylabel('Sales')\nplt.legend()","c47a5767":"plt.figure(figsize=(12,8))\nplt.plot(sarimax_Rmse, label='SARIMAX Rmse')\nplt.plot(lstm_Rmse, label='lstm_Rmse')\nplt.plot(prophet_Rmse, label='fbprophet Rmse')\nplt.ylabel('RMSE')\nplt.xlabel('No.Days')\nplt.title('fbprophet Vs SARIMX Vs Lstm')\nplt.xticks(np.arange(10), ['10', '20', '30','40', '50', '60','70', '80', '90','100'])\nplt.legend()","581468e9":"## LSMT Model","cc33bf37":"In general, all Data scientists can benefit from this report however beginner Data scientist who just started their learning journey can benefit more as this study provides enough adequate details on creating time series models from scratch and by using real dataset. In addition to that, normal users who do not have machine learning workstation equipped by powerful tools like GPU can also benefit from this report.","eba4784c":"**Observation:**  \ndifferencing has reduced the varaince as shown Trend plot after differencing with Trend plot befor differencing, so we expect value of d will be in range of(0,2)","0d2c4446":"**Observations:**\n* All significants lags in Partial Autocorrelation chart are positive and largest one is lag1 so we can tuning p parameter in range(0,3) and q in range(0,2).\n* All significants lags in  Autocorrelation chart are positive and largest one is lag1 so we can tuning P parameter in range(0,3) and Q in range(0,2) in seasonal_orders","523f0cc8":"**Creating new Dataframe and set the datetime as index**","1fd44816":"## fbprophet Model","b5083076":"**fbprophet Rmse**","31c92bc4":"## Benefits:","1dc1b798":"# Time Series Comparison Models (SARIMAX Vs LSTM Vs fbprophet)","4a2eae88":"| Model | Training Time (Minutes) }\n| --- | --- |\n| SARIMAX | 5.8 |\n| fbprophet | 3.0 |\n| LSTM | 35.0* , 1.7** |\n*(By using CPU only)\n**(By using GPU)\n\n* SARIMAX model residual is the lowest one and the shape of the predicted values is best match the shape of actual values,So SARIMAX model will be the recommended model consedering its more open for enhancement via tune hyperparameter, the only limitation comes from hardware requirments since powerful workstation is high requiered to perform grid searching . \n\n* Fbprophet will be good option if short term forecasting is required (let us say period of two weeks ), its training time is the lowest and does not need powerful workstation, so its good choice especially if number of time series which need to be forecasted is high,Its performance and ability to fitech the seasonality give its good credit.\n\n* LSTM model could be also selected if dataset is big enough as it can get the pattern properly but \u0624ertainly would require powerful workstation equipped with  GPU and high RAM as well .\n\n","1c72ac80":"Source:\nYou can download the dataset from Kaggle website (https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/data), the source of this data is https:\/\/mofc.unic.ac.cy\/m5-competition\/\nDataset has been lunched as part of The M5 Competition\nAbout Dataset:\nIt used hierarchical sales data, generously made available by Walmart, starting at the item level and aggregating to that of departments, product categories and stores in three geographical areas of the US: California, Texas, and Wisconsin.\n\nBesides the time series data, it also included explanatory variables such as price, promotions, day of the week, and special events (e.g. Super Bowl, Valentine\u2019s Day, and Orthodox Easter) that affect sales which are used to improve forecasting accuracy.\n","eb483498":"## SARIMA MODEL","7426c1a6":"**Since the number of products is so high we will focus on product with highest number of purchasments, Below code will get is this product** ","bfaf0c62":"**Observation**  \n\n* Accuracy of fbprophet for first 10 days is the best, but after then the SARIMAX defeats fbprophet.\n* Accuracy of LSTM comes in the middle between SARIMAX and fbprophet and it does not drop.","e550a8a9":"## Main Objective","cd7f9666":"***\n\n**Checking the type of time series (stationary or non-statinary)**\n\n***","1c15f781":"**Building and fitting SARIMX model**","21b02ee9":"**Tuning  hyparameter by using self developed method aims to reduce the number of iteration in order to enhance the performance of your workstation incase ready grid search tools do not run smoothly**  \nTou can expand the below cell code to review the code","e8de7a9c":"**Calculating the Rmse for SARIMX model for 100 days,The rmse trend going up as forcasting days increases**","5991aaf0":"## Import Libraries ","769c13c3":"The aims of this report is to compare different time series algorithms (SAIMAX, fbprophet and LSTM ), As part of this study; These models will be used alongside with time series for forecasting purpose  to assess the pros and cons for each one of them\nNote: This study has been by run by using Modest workstation (specs):  \n**Intel(R) Core \u2122 - 5i 6400 CPU @ 2.7GHz  \nRAM (8GB)  \nand GPU for LSTM model.**\n","233d8c14":"***\n\n**Forcasting (LSTM model)**\n\n***","43391222":"Dataset contains  4 CSV sheets as shown beknow:\n* calendar.csv - Contains information about the dates on which the products are sold.\n* sales_train_validation.csv - Contains the historical daily unit sales data per product and store [d_1 - d_1913]\n* sample_submission.csv - The correct format for submissions. Reference the Evaluation tab for more info.\n* sell_prices.csv - Contains information about the price of the products sold per store and date.\n* sales_train_evaluation.csv - Includes sales [d_1 - d_1941] (labels used for the Public leaderboard)\n\nEach training dataset has around 30490 rows and 1919 columns, each row represents product, and each column represents day (time series is daily frequency)  \n\nListing all columns is not right option because their number is so high, however and for sake of this report we will shed light on the following columns:\nd_number of days columns which represent days number starting from d_1,d_2,d_3 \u2026d_1913, there are 1913 days which equal to  5 years (End of 2011 to mid of 2016) and two months, type of column is \u2018integer\n  \nIn addition to train dataset, Calendar dataset will be used to convert days number to datetime type and to create holidays dataset for fbprophet model \n","99dbe600":"**Observation:  \nThe first few montths have no data (mabye the product was not luanched at that time or not avaliable in the stores) so exclude these months is good approach**\n","c14491d9":"**Preparing the training and testing data for all models**","a140fce8":"**Observations:**\n* p-value is **0.0007317275745300305** which is much less than .05 so we can reject the \"null hypothesis\" which says time series is not stationary\n* Data has weekly sesonality\n* Data does not have constant varaiance.\n\nlet us plot the acf and pacf as well","11d8e7e8":"## Data Attribute:","878d1631":"## Dataset","936532c5":"## Recomended Model","cb8fbe86":"**Observation:**\n* Histogrham and estimated density: We  see green and orange lines are close and having the same shape which is good indicator that our model has fit the data properly\n* Correlogram: We See that correlation is not significant of course except at lag0 which normal as any data is 100 correlated with itself.\n* Normal Q-Q: We see that red line overlap most of blue dots which is good indiactor.\n","922a5f79":"## Data cleaning \/ feature engineering","5ca573e4":"**Plot the diagnostics**","fd837eb4":"**Rmse for LSTM**"}}