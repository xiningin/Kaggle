{"cell_type":{"2284d97d":"code","39a6bcb8":"code","2f5ca380":"code","301b8c41":"code","92c22e98":"code","bebc60a6":"code","7b12789c":"code","7214e46c":"code","d8260c21":"code","1a41203d":"code","030c0e6c":"code","cf6e9f50":"code","b5ec17da":"code","ada7786c":"code","d65217af":"code","9caf85b2":"code","c0b07576":"code","30b72c9c":"code","c09f6a6d":"code","2a6f3819":"code","95e79bdc":"code","b3723fdf":"code","04acfdac":"code","234f1171":"code","4f99f1c1":"code","7aa14ccc":"code","3587dc1d":"code","ab7b7dcd":"code","3e7ed872":"code","53adfbc4":"code","27e8e14a":"code","b5c63c88":"code","186b20ee":"code","b3f7e458":"code","6f1ba380":"code","4c9bca84":"markdown","ae89d85d":"markdown"},"source":{"2284d97d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","39a6bcb8":"df32K = pd.read_csv('\/kaggle\/input\/uci-us-census-income-original-dataset\/adult.data')\ndf32K.columns = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', '50K']","2f5ca380":"df16K = pd.read_csv('\/kaggle\/input\/uci-us-census-income-original-dataset\/adult.test')\ndf16K = df16K.reset_index()\ndf16K.columns = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', \n                 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', \n                 'native_country', '50K']","301b8c41":"df48K = df32K.append(df16K, ignore_index=True, sort=False) ","92c22e98":"df48K.native_country.value_counts()\/len(df48K)","bebc60a6":"# As you can see United States count for almost 90% of the native_country data,.. \n# ..and won't help that much with our analysis, for which I will drop this column.\n# Will drop as well 'fnlwgt' which is an US State weigh and 'education_num' which is the duplicate of 'education'. \ndf48K = df48K.drop(['native_country', 'fnlwgt', 'education_num'], axis=1) \n# here you have details for final weight fnlwgt: https:\/\/www.kaggle.com\/uciml\/adult-census-income","7b12789c":"# will have just a little bit of fun with the features, just to simplify things a bit:\ndf48K['marital_status'] = df48K['marital_status'].replace({'Never-married','Divorced', 'Separated', 'Widowed'}, 'Single', regex=True)\ndf48K['marital_status'] = df48K['marital_status'].replace({'Married-civ-spouse','Married-spouse-absent','Married-AF-spouse'}, 'Married', regex=True)\n                                                          \ndf48K['education'] = df48K['education'].replace({'Preschool','1st-4th','5th-6th', '7th-8th'}, 'Elementary-School', regex=True)\ndf48K['education'] = df48K['education'].replace({'9th','10th', '11th', '12th', 'HS-grad'}, 'High-School', regex=True)\ndf48K['education'] = df48K['education'].replace({'Masters', 'Doctorate'}, 'Advanced-Studies', regex=True)\ndf48K['education'] = df48K['education'].replace({'Bachelors', 'Some-college'}, 'College', regex=True)\ndf48K['education'] = df48K['education'].replace({'Prof-school', 'Assoc-acdm', 'Assoc-voc'}, 'Professional-School', regex=True)\n\ndf48K['workclass'] = df48K['workclass'].replace({'Self-emp-inc', 'Self-emp-not-inc'}, 'SelfEmployed', regex=True)\ndf48K['workclass'] = df48K['workclass'].replace({'Local-gov', 'State-gov', 'Federal-gov'}, 'Gov-job', regex=True)\ndf48K['workclass'] = df48K['workclass'].replace({'Without-pay','Never-worked'}, 'Unemployed', regex=True)\n\ndf48K['50K'] = df48K['50K'].replace({'<=50K.'}, '<=50K', regex=True)\ndf48K['50K'] = df48K['50K'].replace({'>50K.'}, '>50K', regex=True)","7214e46c":"# having a quick look at our proud cleaning:\ndf48K.head()","d8260c21":"df48K['50K'].value_counts(dropna=False)\/len(df48K)\n# Looks like our set is imbalanced, as most of the data sets out there in the wild.","1a41203d":"# Plotting time! Let`s see how things are:\nfacet = sns.FacetGrid(df48K, hue=\"50K\", aspect=4)\nfacet.map(sns.kdeplot,'age', shade= True)\nfacet.set(xlim=(0, df48K['age'].max()))\nfacet.add_legend()\n# Looks like the income peak is between mid 30`s to late 40`s.","030c0e6c":"facet = sns.FacetGrid(df48K, hue=\"50K\", aspect=4)\nfacet.map(sns.kdeplot,'hours_per_week', shade= True)\nfacet.set(xlim=(0, df48K['hours_per_week'].max()))\nfacet.add_legend()\n# clearly that once you work more than 40 hours\/week you get a higher income","cf6e9f50":"sns.countplot('50K', hue='education', data=df48K)\n# College and Advanced-Studies do get more jobs that pay higher than 50K.","b5ec17da":"sns.countplot('50K', hue='sex', data=df48K)\n# the gender gap of the 90`s..","ada7786c":"sns.countplot('50K', hue='race', data=df48K)","d65217af":"# There are a few cells that hold the value: '?' \n# - which either you can replace it with NaN's - though I don`t recommend since It might capture valuable information\n# - or you add it to the value that occurs the most or whatever else makes sense from case to case \nplt.figure(figsize=(10,8))\nsns.countplot('50K', hue='occupation', data=df48K)\n# this shows that most of the '?' are in the <50K .. for which I will add them to 'Other-service' which shows similar behavior and high numbers. ","9caf85b2":"df48K['occupation'] = df48K['occupation'].replace({r'\\?', 'Other-service'}, 'Other-service', regex=True)","c0b07576":"plt.figure(figsize=(5,8))\nsns.countplot('50K', hue='occupation', data=df48K)\n# I think it looks better now..","30b72c9c":"sns.countplot('50K', hue='workclass', data=df48K)\n# will deal with '?' same as above","c09f6a6d":"df48K['workclass'] = df48K['workclass'].replace({r'\\?', 'Private'}, 'Private', regex=True)","2a6f3819":"sns.countplot('50K', hue='workclass', data=df48K) \n# better, no? ","95e79bdc":"sns.countplot('50K', hue='marital_status', data=df48K)\n# seems that stability and responsibilities offered by having a family could increase your income.","b3723fdf":"plt.figure(figsize=(20,8))\nsns.countplot('occupation', hue='workclass', data=df48K)\n# interesting to see the dynamics between workclasses, this will definitely help our analysis.","04acfdac":"# preparing the data for spliting into train, validation and test with train_test_split\ny = df48K[['50K']]  #pt arrray .values\nX = df48K.drop(['50K'], axis=1)\nX = pd.get_dummies(X, drop_first=True)\ny = pd.get_dummies(y, drop_first=True)","234f1171":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.1, random_state=9)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=9)","4f99f1c1":"# depending on the model you use you might need a little bit of normalization and standardization:\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler() #copy=True, with_mean=True, with_std=True\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_val_scaled = scaler.transform(X_val)\nX_test_scaled = scaler.transform(X_test)","7aa14ccc":"# Now let`s pick some models and see which performs best.\n# I won`t use any for loops or GridSearchCV since would be too cumbersome for this notebook.. \n# ..though I have had on my own system and got to some conclusions that I will input bellow.\n# Let`s look at the first model: knn\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=28)\nknn.fit(X_train_scaled, y_train)","3587dc1d":"# Model Evaluation:\n# a) On train data\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nprint(knn.score(X_train_scaled, y_train))\ny_pred = knn.predict(X_train_scaled)\nprint(confusion_matrix(y_pred, y_train))  \nprint(classification_report(y_pred, y_train, target_names=[\">50K\", \"<50K\"])) ","ab7b7dcd":"# b) On New, unseen data\nprint(knn.score(X_val_scaled, y_val))\ny_pred = knn.predict(X_val_scaled)\nprint(confusion_matrix(y_pred, y_val)) \nprint(classification_report(y_pred, y_val, target_names=[\">50K\", \"<50K\"]))","3e7ed872":"y_pred = knn.predict(X_test_scaled)\nprint(confusion_matrix(y_pred, y_test))\nprint(classification_report(y_pred, y_test, target_names=[\">50K\", \"<50K\"]))","53adfbc4":"# Now let`s try: LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X_train_scaled, y_train)\nprint(logreg.score(X_train_scaled, y_train))\ny_pred = logreg.predict(X_train_scaled)\nprint(confusion_matrix(y_pred, y_train)) \nprint(classification_report(y_pred, y_train))","27e8e14a":"print(logreg.score(X_val_scaled, y_val))\ny_pred = logreg.predict(X_val_scaled)\nprint(confusion_matrix(y_pred, y_val)) \nprint(classification_report(y_pred, y_val))\ny_pred = logreg.predict(X_test_scaled)\nprint(confusion_matrix(y_pred, y_test))\nprint(classification_report(y_pred, y_test))","b5c63c88":"# Last but not least, let`s try: random forest\nfrom sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier(n_estimators=500, random_state=9, verbose=1)\nforest.fit(X_train_scaled, y_train) # if you don`t like the overfit feel free to use max_features and max_depth, you might even get a slightly higher f1 ;) \nprint(forest.score(X_train_scaled, y_train))\ny_pred = forest.predict(X_train_scaled)\nprint(confusion_matrix(y_pred, y_train)) \nprint(classification_report(y_pred, y_train, target_names=[\">50K\", \"<50K\"]))","186b20ee":"# on unseen data\nprint(forest.score(X_val_scaled, y_val))\ny_pred = forest.predict(X_val_scaled)\nprint(confusion_matrix(y_pred, y_val)) \nprint(classification_report(y_pred, y_val))","b3f7e458":"y_pred = forest.predict(X_test_scaled)\nprint(confusion_matrix(y_pred, y_test))\nprint(classification_report(y_pred, y_test, target_names=[\">50K\", \"<50K\"]))","6f1ba380":"# Let's have a look at the scores on X_val:\n# knn: 0.8372\n# logreg: 0.8509\n# randomforest: 0.8467\n\n#.. as well at f1-scores for X_test:\n# knn: 0.59           TP: 614\n# logreg: 0.63        TP: 668\n# randomforest: 0.63  TP: 687 \n\n# Conclusion: if I want high score on X_val I`ll go with logreg or..\n# (f1 being same) if looking for as many true positives as possible (without modifying the recall) will go with random forest... ","4c9bca84":"Hope this helps or at least you enjoyed the colorful plots and the new, complete, data set. And you have also consider the importance of precision, recall and f1-score. ","ae89d85d":"Hi everyone, \n\nIn the 'kernel' bellow I'll have my own take on US Census Income data set. \n\nWill start with getting the data: which is actually the original data from UCI https:\/\/archive.ics.uci.edu\/ml\/datasets\/census+income (adult.data & adult.test = 48K rows) which comes with many other challenges for data wrangling. \n\nWhat next: \n- EDA (including some nice graphs)\n- Feature creation: selecting and transforming the features \n- Model selection: will try knn, logreg and random forest\n- Evaluation: accuracy on X_val, as well having a look at precision, recall and f1-score\n- Conclusions"}}