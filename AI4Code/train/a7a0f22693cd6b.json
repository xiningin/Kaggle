{"cell_type":{"5f2aabcb":"code","982da17c":"code","db0b2eda":"code","ade0a5a9":"code","f4357a3b":"code","435e1dc3":"code","f26095d8":"code","2947da06":"code","ae595205":"code","158148f9":"code","12bceb33":"code","035db030":"code","e296aea1":"code","8890254e":"code","bb5eefca":"code","7705f1ae":"code","74b40b4b":"code","9c4d3e61":"code","e077e3b4":"markdown","ff398421":"markdown","f190401a":"markdown","34a2ff48":"markdown","ee0a3587":"markdown","f251db85":"markdown","08ceabb2":"markdown","53d1a759":"markdown","2b51c127":"markdown","965eac7d":"markdown","8be0d863":"markdown","ce7ca3c6":"markdown","18ca7245":"markdown","99971641":"markdown","5f9f4749":"markdown"},"source":{"5f2aabcb":"import numpy as np \nimport pandas as pd\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nprint(os.listdir(\"..\/input\"))","982da17c":"train=pd.read_csv(\"..\/input\/fashion-mnist_train.csv\")","db0b2eda":"train.head()","ade0a5a9":"train_label=train['label']\ndel train['label']","f4357a3b":"# Batch data\ntrain=train.iloc[0:1000,:]\ntrain_label=train_label.iloc[0:1000]","435e1dc3":"# helper function for plotting\ndef plotting(xdf,ydf,title):\n    xdf=pd.DataFrame(data=xdf, index=train.index)\n    xdf=xdf.iloc[:,0:2]\n    data=pd.concat((xdf,ydf),axis=1)\n    data=data.rename(columns={0:'first',1:'second'})\n    sns.lmplot(data=data,x='first',y='second',hue='label',fit_reg=False)\n    sns.set_style('darkgrid')\n    plt.title(title)","f26095d8":"from sklearn.decomposition import PCA\npca=PCA(n_components=2,random_state=132,whiten=False)\ntrain_pca=pca.fit_transform(train)\nplotting(train_pca,train_label,'PCA')","2947da06":"from sklearn.decomposition import IncrementalPCA\nipca=IncrementalPCA(n_components=3)\nbatch_size=100\nfor train_batch in np.array_split(train,batch_size):\n    ipca.partial_fit(train_batch)\nplotting(ipca.transform(train),train_label,'Incremental PCA')","ae595205":"from sklearn.decomposition import SparsePCA\nsparse_pca=SparsePCA(n_components=2,alpha=0.002)\nplotting(sparse_pca.fit_transform(train),train_label,'Incremental PCA')","158148f9":"from sklearn.decomposition import KernelPCA\nkpca=KernelPCA(n_components=2, kernel='rbf')\nplotting(kpca.fit_transform(train),train_label,'Kernel PCA with RBF')","12bceb33":"from sklearn.decomposition import TruncatedSVD\ntsvd=TruncatedSVD(n_components=2, n_iter=5, algorithm='randomized')\nplotting(tsvd.fit_transform(train),train_label,'SVD')","035db030":"from sklearn.random_projection import GaussianRandomProjection\ngrp=GaussianRandomProjection(n_components='auto', eps=0.5)\nplotting(grp.fit_transform(train),train_label,'Gaussian Random Projection')","e296aea1":"from sklearn.random_projection import SparseRandomProjection\nsrp=SparseRandomProjection(n_components='auto', eps=0.5, dense_output=False)\nplotting(srp.fit_transform(train),train_label,'Gaussian Random Projection')","8890254e":"from sklearn.manifold import Isomap\nisomap= Isomap(n_components=2, n_neighbors=10, n_jobs=-1)\nplotting(isomap.fit_transform(train),train_label,'ISOMAP')","bb5eefca":"from sklearn.manifold import TSNE\ntsne=TSNE(n_components=2, learning_rate=300, early_exaggeration=12, init='random')\nplotting(tsne.fit_transform(train),train_label,'TSNE')","7705f1ae":"from sklearn.manifold import MDS \nmds=MDS(n_components=2, max_iter=100, metric=True, n_jobs=-1)\nplotting(mds.fit_transform(train),train_label,'MDS')","74b40b4b":"from sklearn.decomposition import FastICA","9c4d3e61":"fast_ica=FastICA(n_components=2, max_iter=50, algorithm='parallel')\nplotting(fast_ica.fit_transform(train),train_label,'ICA')","e077e3b4":"## Kernel PCA:\n#### Mainly use when original dataset is not lineraly seperable. Then Kernel use to plot on kernel space then dimension reduction applied on top of it","ff398421":"## Multi-dimensional Scaling (MDS):\n#### It tries to understand the similarity between data points of higher dimension then use that learning to reduce dimension to lower dimenion.","f190401a":"## Random Projection :\n#### 1. Gaussian Projection : In this projection we don't need to specify number of principal components. It can be controlled by hyper parameter eps\n#### 2. Sparse Projection : Same as Gaussian proejction with retain some sparse space from dataset. number of principal components can be controlled parameter eps. If eps high PC low and eps low PC high","34a2ff48":"## Independent Component Analysis(ICA):\n#### One of the most important dimension reduction technique is ICA. Since sometimes many signals are blended together as one feature so in order to seperate these we use ICA. It can also revert all signal back to one feature. mainly use in audio, video, signal processing","ee0a3587":"# Thanks ! Dont forget to upvote this kernel :)\n","f251db85":"## PCA: \n#### It will try to an axis to retain maximum variance. Newly derived axis called Principal component (PC1). Now next principal component will be orthogonal to all previous one","08ceabb2":"## Choosing only some portion of data so that dimension reduction algorithms execute fast","53d1a759":"## t-SNE :\n#### It is mainly use to reduce dimention to 2 or 3 for visualization purpose. It uses probablity distribution of higher order dimension and compare with lower dimension order for dimension reduction","2b51c127":"## Isomap:\n#### Calculate pairwise distance between all points where, distance is not calculated by euclidean distance. Parameter n_neighbors use to specify neighbors to choose for pair wise distance calculation","965eac7d":"# Dimention Reduction Techniques (PCA)","8be0d863":"## Helper function below which we will use to plot figures. \n## Will use to Plotting 1st and 2nd Principal component","ce7ca3c6":"## Incemental PCA\n#### Sometimes it's hard to load full data to perform dimension reduction. We divide data into batches then apply Incremetal PCA one by one","18ca7245":"## Sparse PCA:\n#### Sparse PCA retain some degree of sparsity controlled by hyper parameter aplha. It is slow to train","99971641":"## SVD (Single value Decomposition):\n#### Way in which original data dimension reduced and further we can get original data again with some combination of smaller rank matrix","5f9f4749":"#### A short notebook in which i tried to cover some Linear as well as Non- Linear Dimension Reduction techniques.\n### **Linear** \n#### 1. Principal component analysis \n#### 2. Incremental PCA\n#### 3. SVD (single value decomposition)\n#### 4. Sparse PCA\n#### 5. Random projection (a)-> Gaussian Random Projection  (b)-> Sparse Random Projection\n### **Non-Linear (Manifold)**\n#### 1. kernel PCA\n#### 2. Isomap\n#### 3. t-SNE\n#### 4. Multi dimension Scaling\n#### 5. Independent Principal Component"}}