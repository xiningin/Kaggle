{"cell_type":{"da74f511":"code","300863fb":"code","40c008a2":"code","0b7e5803":"code","a4e19e7e":"code","b0745b21":"code","7f71f602":"code","ff7ee145":"code","e82f6c0f":"code","4f9df39c":"code","510859ca":"code","b99386ae":"code","d83449a0":"code","88d55bc0":"code","4c2c6402":"code","a5b6dc90":"code","ee4face1":"code","16e3fcc6":"code","69c49681":"code","d185063e":"code","4a3a2433":"code","f3980c9f":"code","baee690c":"code","860984cc":"code","523d9b50":"code","210e9b9a":"code","47031526":"code","949ac0d4":"code","067d42d9":"code","b65fb6d2":"code","5dbd02f4":"code","98b0f453":"code","e3d57984":"code","a84d5a50":"code","0eccf2ec":"code","1e9c53a8":"code","5e63ba78":"code","e03aface":"code","214f3c86":"code","a2460e64":"code","6740611a":"markdown","fe7de60b":"markdown","dfef3500":"markdown","c56610f5":"markdown","e3fec94f":"markdown","116f0295":"markdown","abfc6e59":"markdown","446aad60":"markdown","ff695605":"markdown","6a514e9a":"markdown","0261432a":"markdown","42b99f5b":"markdown","b58d0c85":"markdown","e6614333":"markdown","4ac9c1e5":"markdown"},"source":{"da74f511":"import gc\nimport os\nfrom pathlib import Path\nimport random\nimport sys\n\nfrom tqdm import tqdm_notebook as tqdm\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom IPython.core.display import display, HTML\n\n# --- plotly ---\nfrom plotly import tools, subplots\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\n\n# --- models ---\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\n\nfrom sklearn.metrics import mean_squared_error","300863fb":"\n# Original code from https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage by @gemartin\n# Modified to support timestamp type, categorical type\n# Modified to add option to use float16 or not. feather format does not support float16.\nfrom pandas.api.types import is_datetime64_any_dtype as is_datetime\nfrom pandas.api.types import is_categorical_dtype\n\ndef reduce_mem_usage(df, use_float16=False):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n            # skip datetime type or categorical type\n            continue\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","40c008a2":"%%time\nroot = Path('..\/input\/ashrae-feather-format-for-fast-loading')\n\ntrain_df = pd.read_feather(root\/'train.feather')\ntest_df = pd.read_feather(root\/'test.feather')\n#weather_train_df = pd.read_feather(root\/'weather_train.feather')\n#weather_test_df = pd.read_feather(root\/'weather_test.feather')\nbuilding_meta_df = pd.read_feather(root\/'building_metadata.feather')","0b7e5803":"# i'm now using my leak data station kernel to shortcut.\nleak_df = pd.read_feather('..\/input\/ashare-leak-data-station-2\/leak2.feather')\n\nleak_df.fillna(0, inplace=True)\nleak_df = leak_df[(leak_df.timestamp.dt.year > 2016) & (leak_df.timestamp.dt.year < 2019)]\nleak_df.loc[leak_df.meter_reading < 0, 'meter_reading'] = 0 # remove large negative values\nleak_df = leak_df[leak_df.building_id!=245]","a4e19e7e":"leak_df.meter.value_counts()","b0745b21":"print (leak_df.duplicated().sum())","7f71f602":"print (len(leak_df) \/ len(train_df))","ff7ee145":"! ls ..\/input","e82f6c0f":"del train_df\ngc.collect()","4f9df39c":"sample_submission1 = pd.read_csv('..\/input\/ashrae-kfold-lightgbm-without-leak-1-08\/submission.csv', index_col=0)\nsample_submission2 = pd.read_csv('..\/input\/ashrae-half-and-half\/submission.csv', index_col=0)\nsample_submission3 = pd.read_csv('..\/input\/ashrae-highway-kernel-route4\/submission.csv', index_col=0)","510859ca":"test_df['pred1'] = sample_submission1.meter_reading\ntest_df['pred2'] = sample_submission2.meter_reading\ntest_df['pred3'] = sample_submission3.meter_reading\n\ntest_df.loc[test_df.pred3<0, 'pred3'] = 0 \n\ndel  sample_submission1,  sample_submission2,  sample_submission3\ngc.collect()\n\ntest_df = reduce_mem_usage(test_df)\nleak_df = reduce_mem_usage(leak_df)","b99386ae":"leak_df = leak_df.merge(test_df[['building_id', 'meter', 'timestamp', 'pred1', 'pred2', 'pred3', 'row_id']], left_on = ['building_id', 'meter', 'timestamp'], right_on = ['building_id', 'meter', 'timestamp'], how = \"left\")\nleak_df = leak_df.merge(building_meta_df[['building_id', 'site_id']], on='building_id', how='left')","d83449a0":"leak_df['pred1_l1p'] = np.log1p(leak_df.pred1)\nleak_df['pred2_l1p'] = np.log1p(leak_df.pred2)\nleak_df['pred3_l1p'] = np.log1p(leak_df.pred3)\nleak_df['meter_reading_l1p'] = np.log1p(leak_df.meter_reading)","88d55bc0":"leak_df.head()","4c2c6402":"leak_df[leak_df.pred1_l1p.isnull()]","a5b6dc90":"#ashrae-kfold-lightgbm-without-leak-1-08\nsns.distplot(leak_df.pred1_l1p)\nsns.distplot(leak_df.meter_reading_l1p)\n\nleak_score = np.sqrt(mean_squared_error(leak_df.pred1_l1p, leak_df.meter_reading_l1p))\nprint ('score1=', leak_score)","ee4face1":"#ashrae-half-and-half\nsns.distplot(leak_df.pred2_l1p)\nsns.distplot(leak_df.meter_reading_l1p)\n\nleak_score = np.sqrt(mean_squared_error(leak_df.pred2_l1p, leak_df.meter_reading_l1p))\nprint ('score2=', leak_score)","16e3fcc6":"# meter split based\nsns.distplot(leak_df.pred3_l1p)\nsns.distplot(leak_df.meter_reading_l1p)\n\nleak_score = np.sqrt(mean_squared_error(leak_df.pred3_l1p, leak_df.meter_reading_l1p))\nprint ('score3=', leak_score)","69c49681":"# ashrae-kfold-lightgbm-without-leak-1-08 looks best","d185063e":"leak_df['mean_pred'] = np.mean(leak_df[['pred1', 'pred2', 'pred3']].values, axis=1)\nleak_df['mean_pred_l1p'] = np.log1p(leak_df.mean_pred)\nleak_score = np.sqrt(mean_squared_error(leak_df.mean_pred_l1p, leak_df.meter_reading_l1p))\n\n\nsns.distplot(leak_df.mean_pred_l1p)\nsns.distplot(leak_df.meter_reading_l1p)\n\nprint ('mean score=', leak_score)","4a3a2433":"leak_df['median_pred'] = np.median(leak_df[['pred1', 'pred2', 'pred3']].values, axis=1)\nleak_df['median_pred_l1p'] = np.log1p(leak_df.median_pred)\nleak_score = np.sqrt(mean_squared_error(leak_df.median_pred_l1p, leak_df.meter_reading_l1p))\n\nsns.distplot(leak_df.median_pred_l1p)\nsns.distplot(leak_df.meter_reading_l1p)\n\nprint ('meadian score=', leak_score)","f3980c9f":"N = 10\nscores = np.zeros(N,)\nfor i in range(N):\n    p = i * 1.\/N\n    v = p * leak_df['pred1'].values + (1.-p) * leak_df ['pred3'].values\n    vl1p = np.log1p(v)\n    scores[i] = np.sqrt(mean_squared_error(vl1p, leak_df.meter_reading_l1p))","baee690c":"plt.plot(scores)","860984cc":"best_weight = np.argmin(scores) *  1.\/N\nprint (scores.min(), best_weight)","523d9b50":"# and more\nscores = np.zeros(N,)\nfor i in range(N):\n    p = i * 1.\/N\n    v =  p * (best_weight * leak_df['pred1'].values + (1.-best_weight) * leak_df ['pred3'].values) + (1.-p) * leak_df ['pred2'].values\n    vl1p = np.log1p(v)\n    scores[i] = np.sqrt(mean_squared_error(vl1p, leak_df.meter_reading_l1p))","210e9b9a":"plt.plot(scores)","47031526":"best_weight2 = np.argmin(scores) *  1.\/N\nprint (scores.min(), best_weight2)\n# its seams better than simple mean 0.92079717","949ac0d4":"all_combinations = list(np.linspace(0.1,0.5,401))\n\n# for individual weights w1 to w3 as kernel died when too many combinations\nall_combinations1 = [i for i in all_combinations if i > 0.37 and i < 0.4]\nall_combinations2 = [i for i in all_combinations if i > 0.34 and i < 0.385]\nall_combinations3 = [i for i in all_combinations if i > 0.19 and i < 0.22]\n\nall_combinations","067d42d9":"import itertools","b65fb6d2":"l = [all_combinations1, all_combinations2, all_combinations3]\n# remember to do the reverse!\nall_l = list(itertools.product(*l)) + list(itertools.product(*reversed(l)))","5dbd02f4":"filtered_combis = [l for l in all_l if l[0] + l[1] + l[2] > 0.95 and l[0] + l[1] + l[2] < 1]","98b0f453":"print(len(filtered_combis))","e3d57984":"best_combi = [] # of the form (i, score)\nfor i, combi in enumerate(filtered_combis):\n    print(\"Now at: \" + str(i) + \" out of \" + str(len(filtered_combis)))\n    score1 = combi[0]\n    score2 = combi[1]\n    score3 = combi[2]\n    v = score1 * leak_df['pred1'].values + score2 * leak_df['pred3'].values + score3 * leak_df['pred2'].values\n    vl1p = np.log1p(v)\n    curr_score = np.sqrt(mean_squared_error(vl1p, leak_df.meter_reading_l1p))\n    \n    if best_combi:\n        prev_score = best_combi[0][1]\n        if curr_score < prev_score:\n            best_combi[:] = []\n            best_combi += [(i, curr_score)]\n    else:\n        best_combi += [(i, curr_score)]\n            \nscore = best_combi[0][1]\nprint(score)","a84d5a50":"sample_submission = pd.read_feather(os.path.join(root, 'sample_submission.feather'))\n\n# extract best combination\nfinal_combi = filtered_combis[best_combi[0][0]]\nw1 = final_combi[0]\nw2 = final_combi[1]\nw3 = final_combi[2]\nprint(\"The weights are: w1=\" + str(w1) + \", w2=\" + str(w2) + \", w3=\" + str(w3))\n\nsample_submission['meter_reading'] = w1 * test_df.pred1 +  w2 * test_df.pred3  + w3 * test_df.pred2\nsample_submission.loc[sample_submission.meter_reading < 0, 'meter_reading'] = 0","0eccf2ec":"sample_submission.head()","1e9c53a8":"sns.distplot(np.log1p(sample_submission.meter_reading))","5e63ba78":"leak_df = leak_df[['meter_reading', 'row_id']].set_index('row_id').dropna()\nsample_submission.loc[leak_df.index, 'meter_reading'] = leak_df['meter_reading']","e03aface":"sns.distplot(np.log1p(sample_submission.meter_reading))","214f3c86":"sample_submission.head()","a2460e64":"sample_submission.to_csv('submission.csv', index=False, float_format='%.4f')","6740611a":"A one idea how we can use LV usefull is blending. We probably can find best blending method without LB probing and it's means we can save our submission.","fe7de60b":"# Submit","dfef3500":"### Create List of All Possible Combinations of Three Lists","c56610f5":"## Begin the Search For Combination With Lowest Score!","e3fec94f":"# Leak Validation for public kernels(not used leak data)","116f0295":"# Future Work\n\n- Increase the range of weights\n- Vary tolerance for sum of weights (currently tol = 0.95)","abfc6e59":"# All we need is Leak Validation(LV) ?\n\n* **if you like this kernel, please upvote original kernels.**\n* update site-4 and site-15","446aad60":"# Leak Validation for Blending","ff695605":"this kernel is still work in progress, but i hope you can find something usefull from this.","6a514e9a":"# Acknowledgements\n\nOriginal Kernel: https:\/\/www.kaggle.com\/yamsam\/ashrae-leak-validation-and-more\/notebook#Leak-Validation-for-public-kernels(not-used-leak-data)\n\nAdditions: Added a search method to find combination of weights with best score\n\nLatest Addition: Method of seperate constraints for each score\/ratio","0261432a":"### Create List of Possible Combinations","42b99f5b":"Ummm... it looks mean blending is beter than median blending","b58d0c85":"### Filter Combinations to Have Those With Sum of Weights > 0.95 AND Apply Weight Constraints\n\n- Reason being weight sum of 0.96 led to LB score of 0.99\n- With reference to https:\/\/www.kaggle.com\/khoongweihao\/ashrae-leak-validation-bruteforce-heuristic-search, the best weights are: `w1=0.38, w2=0.38, w3=0.2`.\n- So we now impose seperate constraints to each of the weights to reduce `all_l`\n- For e.g., w1 < 0.5, w1 > 0.3","e6614333":"# Find Best Weight","4ac9c1e5":"# Heuristic way"}}