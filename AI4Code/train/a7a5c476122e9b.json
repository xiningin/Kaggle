{"cell_type":{"05be46db":"code","a4db5687":"code","e13f4eab":"code","2fe2dfc3":"code","c8a8498f":"code","8492f7af":"code","18e9e533":"code","a1ebfe28":"code","456cf205":"code","8a7d1b13":"code","57db32ac":"code","e5c82fe5":"code","70ddc456":"code","627ad6c8":"code","71e5114f":"code","cfe4cb97":"code","d4b43881":"code","10fde681":"code","638e9699":"code","3a5aa963":"code","48fc4ef9":"code","29bb33c2":"code","6d0fc045":"code","2414de4d":"code","7800d769":"code","bcd41d70":"markdown","4780c169":"markdown","850d3b03":"markdown","955894e4":"markdown","b1961816":"markdown","6d46bd6f":"markdown","5f624e90":"markdown","20aade79":"markdown","4eb2aab0":"markdown","ee841445":"markdown","16ab6800":"markdown","677339d1":"markdown","63de9172":"markdown","d168e31f":"markdown","334d23a0":"markdown","0516e910":"markdown","b7b7db5d":"markdown","685a916c":"markdown","ef3ea68a":"markdown","37290863":"markdown","db80db9b":"markdown","c205e14a":"markdown","38a58807":"markdown","843746cb":"markdown","1569273e":"markdown","e4bb26ad":"markdown","8f2b6691":"markdown","b0133b0f":"markdown","981b605a":"markdown","20a078f2":"markdown","1eca3832":"markdown","b18b1296":"markdown","6e433192":"markdown","22abd7cb":"markdown","490367da":"markdown","f01eadf5":"markdown","59c47744":"markdown","07d56dcc":"markdown","fac1bf29":"markdown","1c20b561":"markdown","6e1e5e4b":"markdown","2b5d6f00":"markdown","ef221d64":"markdown","b3056d9b":"markdown","b219a371":"markdown","1c517424":"markdown"},"source":{"05be46db":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, accuracy_score, roc_curve, plot_confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","a4db5687":"full_data = pd.read_csv(\"\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv\")","e13f4eab":"print(full_data.head(3))\nprint(\"\\nNumber of rows: %d \\nNumber of columns: %d\" % (full_data.shape[0], full_data.shape[1]))\nprint(\"Column names: \", list(full_data.columns))","2fe2dfc3":"full_data.describe().T.style.bar(subset=['mean'], color='#208ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')","c8a8498f":"# Creating function for outlier check\ndef outlier_check(var):\n    sns.displot(data = full_data, x = var)\n    print('Variable: ', var) \n    print('Minimum value: ', full_data[var].min())\n    print('1st percentile: ', full_data[var].quantile(0.01))\n    print('99th percentile: ', full_data[var].quantile(0.99))\n    print('Maximum value: ', full_data[var].max())\n\n# Outlier treatment, sets minimum value to 1st percentile or maximum value to 99th percentile.\ndef outlier_treatment(var, min_treat, max_treat):\n    min = full_data[var].quantile(0.01)\n    max = full_data[var].quantile(0.99)\n    if min_treat == 1:\n        full_data.loc[full_data[var] <= min, var] =  min\n    if max_treat == 1:\n        full_data.loc[full_data[var] >= max, var] =  max\n","8492f7af":"outlier_check('age')\noutlier_treatment('age', min_treat = 1, max_treat = 0)","18e9e533":"outlier_check('trtbps')\noutlier_treatment('trtbps', min_treat = 1, max_treat = 1)","a1ebfe28":"outlier_check('chol')\noutlier_treatment('chol', min_treat = 0, max_treat = 1)","456cf205":"outlier_check('thalachh')\noutlier_treatment('thalachh', min_treat = 1, max_treat = 0)","8a7d1b13":"outlier_check('oldpeak')\noutlier_treatment('oldpeak', min_treat = 0, max_treat = 1)","57db32ac":"sns.countplot(full_data['output'])\nprint(pd.concat( [full_data['output'].value_counts(),\n                  full_data['output'].value_counts(normalize=True).mul(100).round(2)],\n                 axis = 1,\n                 keys = ('Count', 'Percentage')))","e5c82fe5":"plt.figure(figsize=(8, 12))\nheatmap = sns.heatmap(full_data.corr()[['output']].sort_values(by='output', ascending=False), \n                      vmin=-1, \n                      vmax=1, \n                      annot=True, \n                      cmap=sns.diverging_palette(5, 5, as_cmap=True))\nheatmap.set_title('Features Correlating with Target', fontdict={'fontsize':18}, pad=16);","70ddc456":"low_corr_vars = ['restecg', 'fbs', 'chol', 'trtbps']\nfull_data.drop(low_corr_vars, axis = 1, inplace = True)\nfull_data.head()","627ad6c8":"full_data =pd.get_dummies(full_data, columns = ['cp', 'caa', 'thall'], drop_first = True)\nfull_data.head()","71e5114f":"corr_data = full_data.drop(['output'], axis = 1)\ncorrmat = corr_data.corr()\nf, ax = plt.subplots(figsize=(14, 14))\nsns.heatmap(corrmat, \n            vmin = -1, \n            vmax= 1, \n            square = True, \n            annot = True,\n            cmap=sns.diverging_palette(5, 5, as_cmap=True));","cfe4cb97":"full_data = full_data.drop(['slp'], axis = 1)\nfull_data.head()","d4b43881":"# Splitting into predictor and target variables\nx_full = full_data.drop('output', axis = 1)\ny_full = full_data['output']\n\n# Train test split\nx_train, x_test, y_train, y_test = train_test_split(x_full, y_full, test_size = 0.3, random_state = 1)\n\nprint('Train data records: %d \\nTest data records: %d' % (x_train.shape[0], x_test.shape[0]))","10fde681":"logistic_model =LogisticRegression(max_iter = 1000)\nlogistic_model.fit(x_train, y_train)\ntrain_predictions = logistic_model.predict(x_train)\ntest_predictions = logistic_model.predict(x_test)\nmodel_performance = pd.DataFrame([[ 'Logistic Regression', \n                                    round(accuracy_score(y_train, train_predictions)*100,2),\n                                    round(accuracy_score(y_test, test_predictions)*100,2)]],\n                                   columns = ['Model', 'Train_Accuracy', 'Test_Accuracy'])\nprint(model_performance)\n\n# Plotting the ROC Curve\nfpr,tpr,threshols=roc_curve(y_test,logistic_model.predict_proba(x_test)[:,1])\nplt.plot([0,1],[0,1],\"k--\",'r+')\nplt.plot(fpr,tpr,label=\"Logistic Regression\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Logistric Regression ROC Curve\")\nplt.show()                                        ","638e9699":"# Creating a function to create different models based on values of max_leaf_nodes\ndef dt_score(max_leaf_nodes, x_train, y_train, x_test, y_test):\n    model = DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes, random_state=1)\n    model.fit(x_train, y_train)\n    train_predict = model.predict(x_train)\n    test_predict = model.predict(x_test)\n    ac_score_train = round(accuracy_score(y_train, train_predict)*100,2)\n    ac_score_test = round(accuracy_score(y_test, test_predict)*100,2)\n    return(ac_score_train, ac_score_test)\n\ndt_score_results = pd.DataFrame(columns = ['Max_leaf_nodes', 'Train Accuracy', 'Test Accuracy'])\nfor max_leaf_nodes in [2, 3, 4, 5, 10, 20, 30, 40, 50]:\n    ac_score_train, ac_score_test = dt_score(max_leaf_nodes, x_train, y_train, x_test, y_test)\n    print(\"Max leaf nodes: %d  \\t Train Accuracy:  %.2f \\t Test Accuracy %.2f\" % (max_leaf_nodes, ac_score_train, ac_score_test))   ","3a5aa963":"decision_tree_model = DecisionTreeClassifier(max_leaf_nodes = 2, random_state = 1)\ndecision_tree_model.fit(x_train,y_train)\ntrain_predictions = decision_tree_model.predict(x_train)\ntest_predictions = decision_tree_model.predict(x_test)\nmodel_performance = model_performance.append(pd.DataFrame([[ 'Decison Tree Classifier', \n                                    round(accuracy_score(y_train, train_predictions)*100,2),\n                                    round(accuracy_score(y_test, test_predictions)*100,2)]],\n                                   columns = ['Model', 'Train_Accuracy', 'Test_Accuracy']), ignore_index = True)\nprint(model_performance)\n# Plotting the ROC Curve\nfpr,tpr,threshols=roc_curve(y_test,decision_tree_model.predict_proba(x_test)[:,1])\nplt.plot([0,1],[0,1],\"k--\",'r+')\nplt.plot(fpr,tpr,label=\"Decison Tree\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Decision Tree ROC Curve\")\nplt.show()","48fc4ef9":"# Creating a function to create different models based on values of max_leaf_nodes\ndef rf_score(max_leaf_nodes, x_train, y_train, x_test, y_test):\n    model = RandomForestClassifier(max_leaf_nodes=max_leaf_nodes, random_state=1)\n    model.fit(x_train, y_train)\n    train_predict = model.predict(x_train)\n    test_predict = model.predict(x_test)\n    ac_score_train = round(accuracy_score(y_train, train_predict)*100,2)\n    ac_score_test = round(accuracy_score(y_test, test_predict)*100,2)\n    return(ac_score_train, ac_score_test)\n\nrf_score_results = pd.DataFrame(columns = ['Max_leaf_nodes', 'Train Accuracy', 'Test Accuracy'])\nfor max_leaf_nodes in [2, 3, 4, 5, 10, 20, 30, 40, 50]:\n    ac_score_train, ac_score_test = rf_score(max_leaf_nodes, x_train, y_train, x_test, y_test)\n    print(\"Max leaf nodes: %d  \\t Train Accuracy:  %.2f \\t Test Accuracy %.2f\" % (max_leaf_nodes, ac_score_train, ac_score_test))   ","29bb33c2":"random_forest_model = RandomForestClassifier(max_leaf_nodes = 5, random_state = 1)\nrandom_forest_model.fit(x_train,y_train)\ntrain_predictions = random_forest_model.predict(x_train)\ntest_predictions = random_forest_model.predict(x_test)\nmodel_performance = model_performance.append(pd.DataFrame([[ 'Random Forest Classifier', \n                                    round(accuracy_score(y_train, train_predictions)*100,2),\n                                    round(accuracy_score(y_test, test_predictions)*100,2)]],\n                                   columns = ['Model', 'Train_Accuracy', 'Test_Accuracy']), ignore_index = True)\nprint(model_performance)\n# Plotting the ROC Curve\nfpr,tpr,threshols=roc_curve(y_test,random_forest_model.predict_proba(x_test)[:,1])\nplt.plot([0,1],[0,1],\"k--\",'r+')\nplt.plot(fpr,tpr,label=\"Random Forest\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Random Forest ROC Curve\")\nplt.show()","6d0fc045":"# Creating a function to create different models based on values of max_leaf_nodes\ndef gbm_score(max_leaf_nodes, x_train, y_train, x_test, y_test):\n    model = GradientBoostingClassifier(max_leaf_nodes=max_leaf_nodes, random_state=1)\n    model.fit(x_train, y_train)\n    train_predict = model.predict(x_train)\n    test_predict = model.predict(x_test)\n    ac_score_train = round(accuracy_score(y_train, train_predict)*100,2)\n    ac_score_test = round(accuracy_score(y_test, test_predict)*100,2)\n    return(ac_score_train, ac_score_test)\n\ngbm_score_results = pd.DataFrame(columns = ['Max_leaf_nodes', 'Train Accuracy', 'Test Accuracy'])\nfor max_leaf_nodes in [2, 3, 4, 5, 10, 20, 30, 40, 50]:\n    ac_score_train, ac_score_test = gbm_score(max_leaf_nodes, x_train, y_train, x_test, y_test)\n    print(\"Max leaf nodes: %d  \\t Train Accuracy:  %.2f \\t Test Accuracy %.2f\" % (max_leaf_nodes, ac_score_train, ac_score_test))   ","2414de4d":"gbm_model = GradientBoostingClassifier(max_leaf_nodes = 2, random_state = 1)\ngbm_model.fit(x_train,y_train)\ntrain_predictions = gbm_model.predict(x_train)\ntest_predictions = gbm_model.predict(x_test)\nmodel_performance = model_performance.append(pd.DataFrame([[ 'Gradient Boosting Classifier', \n                                    round(accuracy_score(y_train, train_predictions)*100,2),\n                                    round(accuracy_score(y_test, test_predictions)*100,2)]],\n                                   columns = ['Model', 'Train_Accuracy', 'Test_Accuracy']), ignore_index = True)\nprint(model_performance)\n# Plotting the ROC Curve\nfpr,tpr,threshols=roc_curve(y_test,random_forest_model.predict_proba(x_test)[:,1])\nplt.plot([0,1],[0,1],\"k--\",'r+')\nplt.plot(fpr,tpr,label=\"Gradient Boosting\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Gradient Boosting ROC Curve\")\nplt.show()","7800d769":"full_predictions = random_forest_model.predict(x_full)\nprint( \"Accuracy Score: %.2f \" % (round(accuracy_score(y_full, full_predictions)*100,2)))\n\n# Plotting confusion matrix\nplot_confusion_matrix(random_forest_model,\n                      x_full, \n                      y_full,\n                      cmap=plt.cm.Blues,\n                      normalize= 'true')\n","bcd41d70":"# 7. Conclusion","4780c169":"Our dataset is now ready for model development. Let's go ahead and start developing models on this dataset.","850d3b03":"**Outlier treatment for column: *trtbps***","955894e4":"We can observe that all the variables have a count of 303, hence, no missing data. Following variables have a high standard deviation: *age, trtbps, chol,thalachh.*","b1961816":"## 2.7 Converting categorical variables into dummies","6d46bd6f":"Distribution of column *thalachh* shows that there are outliers on the left side of the distribution. Outliers don't seem to be present on the right side, hence, we will only apply outlier treatment to floor minimum value to 1st percentile.","5f624e90":"Random Forest Model has a higher accuracy than both Decision Tree as well as Logistic Regression. Let's try Gradient Boosting Model now.","20aade79":"We can observe that our model accuracy in test data is best for Max leaf nodes of 5.","4eb2aab0":"Our model shows 86% accuracy with train data and 80% accuracy with test data. Let's try Decision Tree model now.","ee841445":"**Outlier treatment for column: *oldpeak***\n","16ab6800":"Correlation among variables can be a problem, it can reduce the prediction power of the model as it will become very sensitive to changes in the value of any of the correlated variables. \nImagine you fit a regression model and the coefficient values, and even the signs, change dramatically depending on the specific variables that you include in the model. It\u2019s a disconcerting feeling when slightly different models lead to very different conclusions. You don\u2019t feel like you know the actual effect of each variable!\n\nA correlation among predictor variables of greater than 0.4 or less than -0.4 is not good. From the heatmap, we can see that *slp* and *oldpeak* have a correlation of  -0.57 (57%). We need to remove one of these variables. We should keep oldpeak because it has a high correlation with our target variable (-0.44, 44%) as compared to slp (0.35, 35%). ","677339d1":"Our target variable seems to be evenly distributed among 0s and 1s. This is good and will allow the model to predict the values correctly.","63de9172":"Correlation Plot shows the correlation of each variable with output. A value greater than 0.2 (20%) or less than -0.2 (-20%) is a good correlation. We can observe that following variables do not have a good correlation and can be removed from further steps:\n* restecg = 0.14\n* fbs = -0.028\n* chol = -0.1\n* trtbps = -0.14\n\nLet's remove these variables now.","d168e31f":"# 3. Train test split","334d23a0":"## 2.5 Distribution of target variable","0516e910":"We have 5 categorical variables in our dataset:\n* sex - Gender of the person; Values 0, 1\n* cp - Chest pain type; Values - 0,1,2,3\n* exng - Exercise induced angina; Values - 0, 1\n* caa - Number of major vessels; Values - 0, 1, 2, 3, 4\n* thall - Thalium Stress Test result ~ (0,3)\n\nWe need dummy columns for variables which have more than 2 categories. Only 3 variable have more than 2 categories: cp, caa, thall. Let's convert these variables into dummies.","b7b7db5d":"**Outlier treatment for column: *thalachh***\n","685a916c":"# 5. Random forest classifier model","ef3ea68a":"Our dataset has 303 records, where each record represents a person. Target variable is *output* which has a value of 1 if the person had a heart attack.\n\nLet's have a look at our 13 predictor variables:\n* age - Age of the person\n* sex - Gender of the person. This column has values 0 and 1. Let's assume that 1: Male and 0: Female\n* cp - Chest Pain type, 1: typical angina, 2: atypical angina, 3: non-anginal pain, 4: asymptomatic\n* trtbps - resting blood pressure (in mm Hg). High blood pressure is more likely to cause heart attack\n* chol - cholestoral in mg\/dl fetched via BMI sensor\\\n* fbs - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n* restecg - Resting electrocardiographic results ~ 0 = Normal, 1 = ST-T wave normality, 2 = Left ventricular hypertrophy\n* thalachh - Maximum heart rate achieved\n* oldpeak - Previous peak\n* slp - Slope\n* caa - Number of major vessels\n* thall - Thalium Stress Test result ~ (0,3)\n* exng - Exercise induced angina ~ 1 = Yes, 0 = No\n* oldpeak - Previous peak\n* slp - Slope\n* caa - number of major vessels (0-3)\n* thall - Thal rate","37290863":"## 2.4 Outlier treatment","db80db9b":"## 2.8 Collinearity check","c205e14a":"# 1. Introduction\n\nIn this notebook, we will develop a model to predict heart attack, using the dataset: https:\/\/www.kaggle.com\/rashikrahmanpritom\/heart-attack-analysis-prediction-dataset\n\nWe will develop the following models:\n* Logistic Regression\n* Decision Tree\n* Random Forest\n* Gradient Boosting\n\nBased on the performance of these models on training and test dataset, we will select our final model.\n","38a58807":"**Outlier treatment for column: *age***","843746cb":"Distribution of column *age* shows that there are outliers on the left side of the distribution. Outliers don't seem to be present on the right side, hence, we will only apply outlier treatment to floor minimum value to 1st percentile.","1569273e":"We have split the data into train (70%) and test (30%). Now, we can start developing models on train data and check their performance on both train and test data. Let's start with Logistic Regression model.","e4bb26ad":"We can observe that our model accuracy in test data is best for Max leaf nodes of 2.","8f2b6691":"Decision Tree Model accuracy relies on the parameter: max_leaf_nodes. It limit the number of leaf nodes (outputs of a split). The more leaves we allow the model to make, the more we move from the underfitting area to the overfitting area.","b0133b0f":"Distribution of column oldpeak shows that there are outliers on the right side of the distribution. Outliers don't seem to be present on the left side, hence, we will only apply outlier treatment to cap maximum value to 99th percentile.","981b605a":"Our dataset is ready. Before starting the modeling part, we need to make sure that our predictor variables do not have a high correlation with each other. Let's check that now.","20a078f2":"# 4. Logistic regression model","1eca3832":"Gradient Boosting Classifier shows good performance, but Random Forest Classifier has a better test accuracy. Let's build an XGBoost model now.","b18b1296":"# 6. Gradient boosting classifier model","6e433192":"Distribution of column *chol* shows that there are outliers on the right side of the distribution. Outliers don't seem to be present on the left side, hence, we will only apply outlier treatment to cap maximum value to 99th percentile.","22abd7cb":"**Outlier treatment for column: *chol***\n","490367da":"The *outlier_check* function will perform the following steps:\n* Plot a distribution of values of the variable\n* Print the minimum and 1st percentile value of the variable\n* Print the maximum and 99th percentile value of the variable\n\nIf the distribution shows presence of outliers or if there seems to be a significant difference between the minimum (maximum) value and 1st (99th) percentile, then we will apply floor (cap) using our second function *outlier_treat*.","f01eadf5":"# 4. Decision tree classifier model","59c47744":"## 2.3 Understanding data","07d56dcc":"Our Decision Tree model is not performing better than Logistic Regression model. Let's try Random Forest model now.","fac1bf29":"We need to check if there are outliers in the continuous variables and treat the outliers, if any. Following are the continuous variables in the data:\n* age\n* trtbps\n* chol\n* thalachh\n* oldpeak\n\nWe will start by creating some functions for outlier check and treatment. A good treament for outliers is to apply floor on minimum value (set to 1st percentile) and cap on maximum value (set to 99th percentile).","1c20b561":"# Introduction to Heart Attack Prediction \n### **Arpit Verma**\n#### 15-Jun-2021\n\n* **1. Introduction**\n* **2. Data preparation**\n    * 2.1 Importing libraries\n    * 2.2 Loading data\n    * 2.3 Understanding data\n    * 2.4 Outlier treatment\n    * 2.5 Distribution of target variable \n    * 2.6 Correlation of predictors with target variable\n    * 2.7 Converting categorical variables into dummies   \n    * 2.8 Collinearity check \n* **3. Train test split**\n* **4. Logistic regression model**\n* **5. Decision tree classifier model**\n* **6. Random forest classifier model**\n* **7. Gradient boosting classifier model**\n* **8. Conclusion**","6e1e5e4b":"Our final model is Random Forest Classifier. It shows an accuracy of 83.5% with the full data.","2b5d6f00":"As we increase the maximum leaf nodes, our model accuracy on train data increases and that on test data decreases. We need an optimal value of this parameter, which gives us the best model in terms of train as well as test data performance. The best value of maximum leaf nodes is 2.","ef221d64":"## 2.6 Correlation of predictors with target variable","b3056d9b":"# 2. Data preparation\n## 2.1 Importing Libraries","b219a371":"Distribution of column *trtbps* shows outliers on both left as well as right side. Hence, we will apply floor and cap to this variable.","1c517424":"## 2.2 Loading data"}}