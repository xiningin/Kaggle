{"cell_type":{"0dfa1146":"code","635fecd7":"code","c9519e29":"code","926e15d6":"code","875d6544":"code","bcb1b77d":"code","62e74fae":"code","998c4446":"code","911d10f8":"code","953cc3b5":"code","6146e159":"code","8f2dd9ce":"code","6568985d":"code","03b0b49c":"code","102c701a":"code","e0404d14":"code","261ccb01":"code","83085eb1":"code","c6afd3a8":"code","eff0cf0d":"code","56db8c4c":"code","6460b206":"code","cf39ff2a":"code","93e20bff":"code","74ef2098":"code","ff81b3e4":"code","02d7636c":"code","1ef4ab91":"markdown","814c347e":"markdown","067d67ab":"markdown","185ec776":"markdown","6de297c0":"markdown","b3eed9cd":"markdown","fe342f6d":"markdown","9825b97a":"markdown","bbb06a63":"markdown","e639948e":"markdown","2eb92447":"markdown","3c99f459":"markdown","f95ade60":"markdown","65a54c25":"markdown","98dabb0e":"markdown","e950ccc6":"markdown","6ff9e9c9":"markdown","502b565e":"markdown","ea0e1071":"markdown","9f0c3934":"markdown","45dbce45":"markdown","f5a0e9e3":"markdown","e0586917":"markdown"},"source":{"0dfa1146":"from statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.tsa.stattools import acf\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\nsns.set()\nplt.style.use('ggplot')\nplt.rcParams['figure.figsize'] = (14, 8)","635fecd7":"# Draw samples from a standard Normal distribution (mean=0, stdev=1).\npoints = np.random.standard_normal(1000)\n\n# making starting point as 0\npoints[0]=0\n\n# Return the cumulative sum of the elements along a given axis.\nrandom_walk = np.cumsum(points)\nrandom_walk_series = pd.Series(random_walk)","c9519e29":"plt.figure(figsize=[10, 7.5]); # Set dimensions for figure\nplt.plot(random_walk)\nplt.title(\"Simulated Random Walk\")\nplt.show()","926e15d6":"random_walk_acf = acf(random_walk)\nacf_plot = plot_acf(random_walk_acf, lags=20)","875d6544":"random_walk_difference = np.diff(random_walk, n=1)\n\nplt.figure(figsize=[10, 7.5]); # Set dimensions for figure\nplt.plot(random_walk_difference)\nplt.title('Noise')\nplt.show()","bcb1b77d":"cof_plot_difference = plot_acf(random_walk_difference, lags=20);","62e74fae":"from statsmodels.tsa.arima_process import ArmaProcess\n\n# start by specifying the lag\nar3 = np.array([3])\n\n# specify the weights : [1, 0.9, 0.3, -0.2]\nma3 = np.array([1, 0.9, 0.3, -0.2])\n\n# simulate the process and generate 1000 data points\nMA_3_process = ArmaProcess(ar3, ma3).generate_sample(nsample=1000)","998c4446":"plt.figure(figsize=[10, 7.5]); # Set dimensions for figure\nplt.plot(MA_3_process)\nplt.title('Simulation of MA(3) Model')\nplt.show()\nplot_acf(MA_3_process, lags=20);","911d10f8":"ar3 = np.array([1, 0.9, 0.3, -0.2])\nma = np.array([3])\nsimulated_ar3_points = ArmaProcess(ar3, ma).generate_sample(nsample=10000)","953cc3b5":"plt.figure(figsize=[10, 7.5]); # Set dimensions for figure\nplt.plot(simulated_ar3_points)\nplt.title(\"Simulation of AR(3) Process\")\nplt.show()","6146e159":"plot_acf(simulated_ar3_points);","8f2dd9ce":"from statsmodels.graphics.tsaplots import plot_pacf\n\nplot_pacf(simulated_ar3_points);","6568985d":"from statsmodels.tsa.stattools import pacf\n\npacf_coef_AR3 = pacf(simulated_ar3_points)\nprint(pacf_coef_AR3)","03b0b49c":"ar1 = np.array([1, 0.6])\nma1 = np.array([1, -0.2])\nsimulated_ARMA_1_1_points = ArmaProcess(ar1, ma1).generate_sample(nsample=10000)","102c701a":"plt.figure(figsize=[15, 7.5]); # Set dimensions for figure\nplt.plot(simulated_ARMA_1_1_points)\nplt.title(\"Simulated ARMA(1,1) Process\")\nplt.xlim([0, 200])\nplt.show()","e0404d14":"plot_acf(simulated_ARMA_1_1_points);\nplot_pacf(simulated_ARMA_1_1_points);","261ccb01":"ar2 = np.array([1, 0.6, 0.4])\nma2 = np.array([1, -0.2, -0.5])\n\nsimulated_ARMA_2_2_points = ArmaProcess(ar2, ma2).generate_sample(nsample=10000)","83085eb1":"plt.figure(figsize=[15, 7.5]); # Set dimensions for figure\nplt.plot(simulated_ARMA_2_2_points)\nplt.title(\"Simulated ARMA(2,2) Process\")\nplt.xlim([0, 200])\nplt.show()","c6afd3a8":"plot_acf(simulated_ARMA_2_2_points);\nplot_pacf(simulated_ARMA_2_2_points);","eff0cf0d":"np.random.seed(200)\n\nar_params = np.array([1, -0.4])\nma_params = np.array([1, -0.8])\n\nreturns = ArmaProcess(ar_params, ma_params).generate_sample(nsample=1000)\n\nreturns = pd.Series(returns)\ndrift = 100\n\nprice = pd.Series(np.cumsum(returns)) + drift","56db8c4c":"returns.plot(figsize=(15,6), color=sns.xkcd_rgb[\"orange\"], title=\"simulated return series\")\nplt.show()","6460b206":"price.plot(figsize=(15,6), color=sns.xkcd_rgb[\"baby blue\"], title=\"simulated price series\")\nplt.show()","cf39ff2a":"log_return = np.log(price) - np.log(price.shift(1))\nlog_return = log_return[1:]","93e20bff":"_ = plot_acf(log_return,lags=10, title='log return autocorrelation')","74ef2098":"_ = plot_pacf(log_return, lags=10, title='log return Partial Autocorrelation', color=sns.xkcd_rgb[\"crimson\"])","ff81b3e4":"from statsmodels.tsa.arima_model import ARIMA\n\ndef fit_arima(log_returns):\n        ar_lag_p = 1\n        ma_lag_q = 1\n        degree_of_differentiation_d = 0\n\n        # create tuple : (p, d, q)\n        order = (ar_lag_p, degree_of_differentiation_d, ma_lag_q)\n\n        # create an ARIMA model object, passing in the values of the lret pandas series,\n        # and the tuple containing the (p,d,q) order arguments\n        arima_model = ARIMA(log_returns.values, order=order)\n        arima_result = arima_model.fit()\n\n        #TODO: from the result of calling ARIMA.fit(),\n        # save and return the fitted values, autoregression parameters, and moving average parameters\n        fittedvalues = arima_result.fittedvalues\n        arparams = arima_result.arparams\n        maparams = arima_result.maparams\n\n        return fittedvalues,arparams,maparams\n","02d7636c":"fittedvalues,arparams,maparams = fit_arima(log_return)\narima_pred = pd.Series(fittedvalues)\nplt.plot(log_return, color=sns.xkcd_rgb[\"pale purple\"])\nplt.plot(arima_pred, color=sns.xkcd_rgb[\"jade green\"])\nplt.title('Log Returns and predictions using an ARIMA(p=1,d=1,q=1) model');\nprint(f\"fitted AR parameter {arparams[0]:.2f}, MA parameter {maparams[0]:.2f}\")","1ef4ab91":"# Random Walk Model\n\nThe random walk hypothesis is a financial theory stating that stock market prices evolve according to a random walk and thus cannot be predicted. A Random Walk Model beleives that [1]:\n\n1. Changes in stock prices have the same distribution and are independent of each other.\n2. Past movement or trend of a stock price or market cannot be used to predict its future movement.\n3. It's impossible to outperform the market without assuming additional risk.\n4. Considers technical analysis undependable because it results in chartists only buying or selling a security after a move has occurred.\n5. Considers fundamental analysis undependable due to the often-poor quality of information collected and its ability to be misinterpreted.\n\nA random walk model can be expressed as : <br>\n<img src=\"https:\/\/github.com\/purvasingh96\/AI-for-Trading\/blob\/master\/Term%201\/Theorey%20%26%20Quizes\/4.%20Time%20Series%20Modelling\/images\/random_walk_1.png?raw=1\" width=150, height=50><img>\n\nThis formula represents that location at the present time t is the sum of the previous location and noise, expressed by *Z.*\n\n## Simulating returns with Random Walk\n\n### Importing libraries\n\nHere, we are importing important libraries needed for visualization and also for simulating random walk model.","814c347e":"As you can see, we cannot infer the order of the ARMA process by looking at these plots. In fact, looking closely, we can see some sinusoidal shape in both ACF and PACF functions. ","067d67ab":"### Extracting stationary data\n\nOne way to get stationary time-series is by taking difference between points in time-series. This time difference is called *rate of change.*\n\n`rate_of_change = current_price \/ previous_price`\n\nThe corresponding *log return* will become : \n\n`log_returns = log(current_price) - log(previous_price)`","185ec776":"# References\n\n1. https:\/\/towardsdatascience.com\/how-to-model-time-series-in-python-9983ebbf82cf\n2. https:\/\/towardsdatascience.com\/advanced-time-series-analysis-with-arma-and-arima-a7d9b589ed6d\n3. https:\/\/towardsdatascience.com\/time-series-forecasting-with-autoregressive-processes-ba629717401\n4. https:\/\/stackoverflow.com\/questions\/52815990\/valueerror-the-computed-initial-ma-coefficients-are-not-invertible-you-should-i\n5. Udacity's nanodegree course on AI for Trading.","6de297c0":"plotting the ACF and PACF graph:","b3eed9cd":"### Plotting the simulated Random Walk model\nNow, lets plot our dataset","fe342f6d":"Looking at the correlation plot, we can see that the coefficient is slowly decaying. Now lets plot the corresponding partial correlation plot.\n\n### Partial Autocorrelation Plot\n\nThe autocorrelation for an observation and an observation at a prior time step is comprised of both the direct correlation and indirect correlations. These indirect correlations are a linear function of the correlation of the observation, with observations at intervening time steps.\n\nIt is these indirect correlations that the partial autocorrelation function seeks to remove. ","9825b97a":"# Time Series Analysis with ARMA and ARIMA\n\nIn this notebook I have tried the cover the following ways to perform time-series analysis - \n\n1. Random Walk\n2. Moving averages Models (MA Models)\n3. Autoregression Models (AR Models)\n4. Autoregressive Moving Averages (ARMA Models)\n5. Autoregressive Integrated Moving Averages (ARIMA Models)\n\n","bbb06a63":"plotting ACF and PACF plots:","e639948e":"### Simulating ARMA(1,1) Process\n\nHere, we will be simulating an ARMA(1, 1) model whose equation is :<br>\n\n\n<img src=\"https:\/\/github.com\/purvasingh96\/AI-for-Trading\/blob\/master\/Term%201\/Theorey%20%26%20Quizes\/4.%20Time%20Series%20Modelling\/images\/5.%20ARMA_1_1.png?raw=1\" width=400, height=100><\/img><br><br>\n","2eb92447":"# Autoregressive Integrated Moving Average Models (ARIMA Models)\n\nThis model is the combination of autoregression, a moving average model and differencing. In this context, integration is the opposite of differentiation.\n\nDifferentiation is useful to remove the trend in a time series and make it stationary.<br>\nIt simply involves subtracting a point a t-1 from time t. <br>\n\nMathematically, the ARIMA(p,d,q) now requires three parameters:<br>\n1. p: the order of the autoregressive process\n2. d: the degree of differentiation (number of times it was differenced)\n3. q: the order of the moving average process\n\nand the equations is expressed as:\n\n\n<img src=\"https:\/\/github.com\/purvasingh96\/AI-for-Trading\/blob\/master\/Term%201\/Theorey%20%26%20Quizes\/4.%20Time%20Series%20Modelling\/images\/7.%20arima.png?raw=1\" width=800, height=300><\/img><br><br>\n","3c99f459":"# Moving Average Models (MA Models)\n\nIn MA models, we start with average mu, to get the value at time t, we add a linear combination of residuals from previous time stamps. In finance, residual refers to new unpredictable information that can't be captured by past data points. The residuals are difference between model's past prediction and actual values. <br><br>\nMoving average models are defined as MA(q) where q is the lag.\n\n<img src=\"https:\/\/github.com\/purvasingh96\/AI-for-Trading\/blob\/master\/Term%201\/Theorey%20%26%20Quizes\/4.%20Time%20Series%20Modelling\/images\/2.%20MA%20model.png?raw=1\" width=400, height=230><\/img><br><br>\n\nTaking an example of MA model of order 3, denoted as MA(3):<br><br>\n\n<img src=\"https:\/\/github.com\/purvasingh96\/AI-for-Trading\/blob\/master\/Term%201\/Theorey%20%26%20Quizes\/4.%20Time%20Series%20Modelling\/images\/3.%20ma%203.png?raw=1\" width=400, height=250><\/img><br><br>\n\nThe equation above says that the position y at time t depends on the noise at time t, plus the noise at time t-1 (with a certain weight epsilon), plus some noise at time t-2 (with a certain weights), plus some noise at time t-3.\n","f95ade60":"Looking at the corelation plot we can say that the *process is not stationary.* But there is a way to remove this trend. I am going to try to different ways to make this process a stationary one -\n1. Knowing that a random walk adds a random noise to the previous point, if we take the difference between each point with its previous one, we should obtain a purely random stochastic process.\n2. Taking the log return of the prices.\n\n### Difference between 2 points","65a54c25":"As you can see, both plots exhibit the same sinusoidal trend, which further supports the fact that both an AR(p) process and a MA(q) process is in play.","98dabb0e":"# AutoRegression Models (AR Models)\n\nAn auto-regressive models (AR Models) tries to fit in a line that is linear combination of previous values. It includes an intercept, that is indipendent of previous values. It also contains error term to represent movements that cannot be predicted by previous terms.\n\n<img src=\"https:\/\/github.com\/purvasingh96\/AI-for-Trading\/blob\/master\/Term%201\/Theorey%20%26%20Quizes\/4.%20Time%20Series%20Modelling\/images\/1.%20AR%20models.png?raw=1\" width=400, height=250><\/img>\n\nAn AR model is defined by its lag. If an AR model uses only yesterday's value and ignores the rest, its called AR Lag 1, if the model uses two previous days values and ignores the rest, its called AR Lag 2 and so on.\n\n<img src=\"https:\/\/github.com\/purvasingh96\/AI-for-Trading\/blob\/master\/Term%201\/Theorey%20%26%20Quizes\/4.%20Time%20Series%20Modelling\/images\/2.%20Lag.png?raw=1\" width=400, height=250><\/img>\n\nUsually, autoregressive models are applied to stationary time series only. This constrains the range of the parameters phi. For example, an AR(1) model will constrain phi between -1 and 1. Those constraints become more complex as the order of the model increases, but they are automatically considered when modelling in Python. [1]","e950ccc6":"# Autoregressive Moving Average Models (ARMA Models)\n\nThe ARMA model is defined with a *p* and *q*. p is the lag for *autoregression* and q is lag for *moving average*. Regression based training models require data to be stationary. For a non-stationary dataset, the mean, variance and co-variance may change over time. This causes difficulty in predicting future based on past.\n\nLooking back at the equation of Autoregressive Model (AR Model) :<br><br>\n\n<img src=\"https:\/\/github.com\/purvasingh96\/AI-for-Trading\/blob\/master\/Term%201\/Theorey%20%26%20Quizes\/4.%20Time%20Series%20Modelling\/images\/1.%20AR%20models.png?raw=1\" width=400, height=250><\/img>\n\nLooking at the equation of Moving Average Model (MA Model) : <br><br>\n\n<img src=\"https:\/\/github.com\/purvasingh96\/AI-for-Trading\/blob\/master\/Term%201\/Theorey%20%26%20Quizes\/4.%20Time%20Series%20Modelling\/images\/2.%20MA%20model.png?raw=1\" width=400, height=230><\/img><br><br>\n\n\nEquation of ARMA model is simply the combination of the two :<br><br>\n\n\n<img src=\"https:\/\/github.com\/purvasingh96\/AI-for-Trading\/blob\/master\/Term%201\/Theorey%20%26%20Quizes\/4.%20Time%20Series%20Modelling\/images\/4.%20ARMA.png?raw=1\" width=700, height=400><\/img><br><br>\n\nHence, this model can explain the relationship of a time series with both random noise (moving average part) and itself at a previous step (autoregressive part).\n","6ff9e9c9":"### Autocorrelational Plots\nAn autocorrelation plot is designed to show whether the elements of a time series are positively correlated, negatively correlated, or independent of each other.An autocorrelation plot shows the value of the autocorrelation function (acf) on the vertical axis. It can range from \u20131 to 1.\n\nWe can calculate the correlation for time series observations with observations with previous time steps, called *lags*. Because the correlation of the time series observations is calculated with values of the same series at previous times, this is called a serial correlation, or an *autocorrelation*.\n\nA plot of the autocorrelation of a time series by lag is called the AutoCorrelation Function, or the acronym ACF. This plot is sometimes called a correlogram or an autocorrelation plot.","502b565e":"We see that this is the correlogram of a purely random process, where the autocorrelation coefficients drop at lag 1.\n","ea0e1071":"As you can see the coefficients are not significant after lag 3. Therefore, the partial autocorrelation plot is useful to determine the order of an AR(p) process. You can also view these values using the import statement `from statsmodels.tsa.stattools import pacf`","9f0c3934":"## Simulating return series with autoregressive properties\n\nFor simulating a AR(3) process, we will be using [ArmaProcess](https:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.tsa.arima_process.ArmaProcess.html).\n\nFor this, let us take the same example that we used to simulate random walk model :<br><br>\n\n<img src=\"https:\/\/github.com\/purvasingh96\/AI-for-Trading\/blob\/master\/Term%201\/Theorey%20%26%20Quizes\/4.%20Time%20Series%20Modelling\/images\/3.%20ma%203.png?raw=1\" width=400, height=250><\/img><br><br>\n\nSince we are dealing with an autoregressive model of order 3, we need to define the coefficient at lag 0, 1, 2 and 3.\nAlso, we will cancel the effect of a moving average process.\nFinally, we will generate 10000 data points.\n","45dbce45":"As you can see, there is a significant correlation upto lag 3. Afterwards, the correlation is not significant anymore. This makes sense since we specified a formula with a lag of 3.","f5a0e9e3":"Now we generate 1000 random points by adding a degree of randomness to the previous point to generate the next point with 0 as starting point.","e0586917":"### Simulating ARMA(2, 2) Process\n\nHere, we will be simulating an ARMA(2, 2) model whose equation is :<br>\n\n\n<img src=\"https:\/\/github.com\/purvasingh96\/AI-for-Trading\/blob\/master\/Term%201\/Theorey%20%26%20Quizes\/4.%20Time%20Series%20Modelling\/images\/6.%20ARMA_2_2.png?raw=1\" width=500, height=200><\/img><br><br>\n"}}