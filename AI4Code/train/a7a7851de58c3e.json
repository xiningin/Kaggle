{"cell_type":{"29e58077":"code","731d381f":"code","ecc0e7ee":"code","e7263544":"code","b8edc7d3":"code","9c209f38":"code","49190b09":"code","a4ac2609":"code","1128bec7":"code","ca777aad":"code","e1eed177":"code","fc4433c5":"code","7e88ecc5":"code","b2fbced8":"code","727b539b":"code","368a9381":"code","dd2ad787":"code","447130b1":"code","2c956f3c":"code","68691508":"code","3df40a16":"code","ab6641da":"code","b75b951f":"code","7633fbb2":"code","229e276f":"code","aad1c289":"code","4ee95436":"code","87ba8fd1":"code","3f9bff6b":"code","a7247629":"code","f4eb587e":"code","50bcfe58":"code","6604b153":"code","8848ab6e":"code","cc5f1a79":"code","b4a45cff":"code","dd11ffdb":"code","059a2711":"code","853832ff":"code","9c620002":"code","b72dd01f":"code","774df98b":"code","0ad571ad":"code","452c2c40":"code","e3d594e4":"code","185888ea":"markdown","74918fc9":"markdown","6c52fe4a":"markdown","3ffaf042":"markdown","82f725a4":"markdown","561b9f8d":"markdown","e9acc134":"markdown","98464be8":"markdown","919a8800":"markdown","7b065970":"markdown"},"source":{"29e58077":"import pandas\nimport matplotlib.pyplot as plt\nimport numpy\nimport pandas as pd\nimport math\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom pandas.plotting import scatter_matrix\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import SGDRegressor\nfrom keras.models import Sequential\nfrom keras.layers import Dense   \nfrom keras import optimizers\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom keras.models import load_model\nfrom keras import optimizers\nfrom matplotlib import pyplot\nfrom math import sqrt\nfrom keras import optimizers\n\n# Any results you write to the current directory are saved as output.","731d381f":"import tensorflow as tf\nprint(tf.__version__)","ecc0e7ee":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e7263544":"np.random.seed(7)\n\n# load the dataset\ndataframe = pandas.read_csv('\/kaggle\/input\/us-counties-covid-19-dataset\/us-counties.csv')","b8edc7d3":"dataframe.head()","9c209f38":"dataframe['pd_date'] = pd.to_datetime(dataframe.date)\ndataframe=dataframe.sort_values(by='pd_date',ascending=True)","49190b09":"print(dataframe['pd_date'].max())","a4ac2609":"print(dataframe['pd_date'].min())","1128bec7":"dataframe.state.value_counts()[:20]","ca777aad":"dataframe_Tx = dataframe[dataframe.state == 'Texas']\nlen(dataframe_Tx)","e1eed177":"dataframe_Tx.county.value_counts()","fc4433c5":"#dataframe_wton = dataframe[(dataframe.county=='Washington') & (dataframe.state=='Oregon')]\ndataframe_wton = dataframe[(dataframe.county=='El Paso') & (dataframe.state=='Texas')]","7e88ecc5":"len(dataframe_wton)","b2fbced8":"# Filter out only the cases and deaths values\n#dataframe_wton.index=dataframe_wton['pd_date']\ndataframe_wton = dataframe_wton.iloc[:,4:6]","727b539b":"dataframe_wton.tail(20)","368a9381":"dataset_cases = dataframe_wton.values[:,0:1]\ndataset_cases = dataset_cases.astype('float32')\ndataset_deaths = dataframe_wton.values[:,1:2]\ndataset_deaths = dataset_deaths.astype('float32')","dd2ad787":"plt.title(\"Number of COVID 19 cases by day for El Paso County\")\nplt.plot(dataset_cases)\nplt.show()","447130b1":"plt.title(\"Number of COVID 19 deaths by day for El Paso County\")\nplt.plot(dataset_deaths)\nplt.show()","2c956f3c":"dataset = dataset_cases","68691508":"len(dataset)","3df40a16":"train_size = int(len(dataset)) - 29\n#test_size = 14\ntrain, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\nprint(len(train), len(test))","ab6641da":"#create dataframe series for t+1,t+2,t+3, to be used as y values, during Supervised Learning\n#lookback = 5, means 5 values of TimeSeries (x) are used to predict the value at time t+1,t+2,t+3 (y)\ndef createSupervisedTrainingSet(dataset,lookback):\n\n    df = DataFrame()\n    x = dataset\n    \n    len_series = x.shape[0]\n\n    df['t'] = [x[i] for i in range(x.shape[0])]\n    #create x values at time t\n    x=df['t'].values\n    \n    cols=list()\n  \n    df['t+1'] = df['t'].shift(-lookback)\n    cols.append(df['t+1'])\n    df['t+2'] = df['t'].shift(-(lookback+1))\n    cols.append(df['t+2'])\n    df['t+3'] = df['t'].shift(-(lookback+2))\n    cols.append(df['t+3'])\n    agg = concat(cols,axis=1)\n    y=agg.values\n\n    x = x.reshape(x.shape[0],1)\n\n    len_X = len_series-lookback-2\n    X=np.zeros((len_X,lookback,1))\n    Y=np.zeros((len_X,3))\n \n    for i in range(len_X):\n        X[i] = x[i:i+lookback]\n        Y[i] = y[i]\n\n    return X,Y\n\n","b75b951f":"look_back = 3\ntrainX, trainY = createSupervisedTrainingSet(train, look_back)\ntestX,testY = createSupervisedTrainingSet(test, look_back)","7633fbb2":"testY=testY.reshape(testY.shape[0],testY.shape[1])\ntrainY=trainY.reshape(trainY.shape[0],trainY.shape[1])\nprint(trainX.shape)\nprint(trainY.shape)\nprint(testX.shape)\nprint(testY.shape)","229e276f":"#Check the sample train X and train Y, and match with original time series data\nprint1 = trainY[13,:].reshape(1,-1)\nprint(\"Train X at index 13\")\nprint(np.around((trainX[13,:,:])))\nprint(\"Train Y at index 13\")\nprint(np.around((print1)))\nprint(\"Actual Data\")\nprint(np.around((dataset[13:19])))        \n#We used a lookback value of 5\n#We inspect the X,Y values at a random index: 13\n#As can be seen the 5 values of Time Series (Call Volume) from index 13 are being used as X to \n#predict the 3 values coming next (t+1,t+2,t+3)","aad1c289":"model = Sequential()\nmodel.add(LSTM(16,activation='relu',return_sequences=True, input_shape=(look_back, 1)))\nmodel.add(LSTM(8, activation='relu'))\nmodel.add(Dense(3))\nmyOptimizer = optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=0.01, decay=0.0)\nmodel.compile(loss='mean_squared_error', optimizer=myOptimizer)\nhistory = model.fit(trainX, trainY, epochs=200,  validation_data=(testX,testY), batch_size=5, verbose=2)","4ee95436":"odel = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters=32, kernel_size=5,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=[None, 1]),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 200)\n])\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-8 * 10**(epoch \/ 20))\noptimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\nhistory = model.fit(trainX, trainY, epochs=100, callbacks=[lr_schedule])","87ba8fd1":"plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\nplt.axis([1e-8, 1e-4, 0, 30])","3f9bff6b":"\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters=32, kernel_size=3,\n                      strides=1, padding=\"VALID\",\n                      activation=\"relu\",\n                      input_shape=[None, 1]),\n  tf.keras.layers.LSTM(32, return_sequences=True),\n  tf.keras.layers.LSTM(32, return_sequences=True),\n  tf.keras.layers.Dense(1)\n])\n\noptimizer = tf.keras.optimizers.SGD(lr=1e-1, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\nhistory = model.fit(trainX, trainY, epochs=200,  validation_data=(testX,testY), batch_size=5, verbose=2)","a7247629":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'], color=  'red')\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","f4eb587e":"#Once the model is trained, use it to make a prediction on the test data\ntestPredict = model.predict(testX)\npredictUnscaled = np.around((testPredict))\ntestYUnscaled = np.around((testY))\n#print the actual and predicted values at t+3\nprint(\"Actual values of COVID 19 cases\")\nprint(testYUnscaled[:,0])\nprint(\"Predicted values of COVID 19 cases\")\nprint(predictUnscaled[:,0])","50bcfe58":"pyplot.plot(testPredict[:,0], color='red')\npyplot.plot(testY[:,0])\npyplot.legend(['Predicted','Actual'])\npyplot.title('Actual vs Predicted at time t+1')\npyplot.show()","6604b153":"#Evaluate the RMSE values at t+1,t+2,t+3 to compare with other approaches, and select the best approach\ndef evaluate_forecasts(actuals, forecasts, n_seq):\n    \tfor i in range(n_seq):\n            actual = actuals[:,i]\n            predicted = forecasts[:,i]\n            rmse = sqrt(mean_squared_error(actual, predicted))\n            print('t+%d RMSE: %f' % ((i+1), rmse))\n        \nevaluate_forecasts(testYUnscaled, predictUnscaled,3)","8848ab6e":"dataset_cases[:,0].astype(int)","cc5f1a79":"from statsmodels.tsa.stattools import adfuller\nfrom numpy import log\nresult = adfuller(dataframe_wton['cases'])\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])","b4a45cff":"from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nplt.rcParams.update({'figure.figsize':(9,7), 'figure.dpi':120})\n\n# Original Series\nfig, axes = plt.subplots(3, 2, sharex=True)\naxes[0, 0].plot(dataframe_wton['cases']); axes[0, 0].set_title('Original Series')\nplot_acf(dataframe_wton['cases'], ax=axes[0, 1])\n\n# 1st Differencing\naxes[1, 0].plot(dataframe_wton['cases'].diff()); axes[1, 0].set_title('1st Order Differencing')\nplot_acf(dataframe_wton['cases'].diff().dropna(), ax=axes[1, 1])\n\n# 2nd Differencing\naxes[2, 0].plot(dataframe_wton['cases'].diff().diff()); axes[2, 0].set_title('2nd Order Differencing')\nplot_acf(dataframe_wton['cases'].diff().diff().dropna(), ax=axes[2, 1])","dd11ffdb":"# PACF plot of 1st differenced series\nplt.rcParams.update({'figure.figsize':(9,3), 'figure.dpi':120})\n\nfig, axes = plt.subplots(1, 2, sharex=True)\naxes[0].plot(dataframe_wton['cases'].diff()); axes[0].set_title('1st Differencing')\naxes[1].set(ylim=(0,5))\nplot_pacf(dataframe_wton['cases'].diff().dropna(), ax=axes[1])\n\nplt.show()","059a2711":"plt.rcParams.update({'figure.figsize':(9,3), 'figure.dpi':120})\n\nfig, axes = plt.subplots(1, 2, sharex=True)\naxes[0].plot(dataframe_wton['cases'].diff()); axes[0].set_title('1st Differencing')\naxes[1].set(ylim=(0,1.2))\nplot_acf(dataframe_wton['cases'].diff().dropna(), ax=axes[1])\n\nplt.show()","853832ff":"dataframe_wton = dataframe[(dataframe.county=='El Paso') & (dataframe.state=='Texas')]\ndataframe_wton.index=dataframe_wton['pd_date']\ndataframe_wton = dataframe_wton.iloc[:,4:6]","9c620002":"test_range = pd.to_datetime(dataframe_wton.index[160:])","b72dd01f":"from statsmodels.tsa.arima_model import ARIMA\n\n# 1,1,2 ARIMA Model\nmodel = ARIMA(dataframe_wton['cases'].astype(float), order=(1,1,2))\nmodel_fit = model.fit(disp=0)\nprint(model_fit.summary())","774df98b":"# Plot residual errors\nresiduals = pd.DataFrame(model_fit.resid)\nfig, ax = plt.subplots(1,2)\nresiduals.plot(title=\"Residuals\", ax=ax[0])\nresiduals.plot(kind='kde', title='Density', ax=ax[1])\nplt.show()\n","0ad571ad":"# Actual vs Fitted\ntrange = np.arange(160,189)\ntrange\nmodel_fit.plot_predict(dynamic=False)\nplt.show()","452c2c40":"train = dataframe_wton.iloc[0:160, :]\ntest = dataframe_wton.iloc[160:, :]\n\narima = ARIMA(train['cases'].astype(float), order = (1,1,2)).fit(disp = 0)\n","e3d594e4":"prediction = arima.plot_predict(test.index[0], test.index[-1], dynamic = True)","185888ea":"### We see we have data from 21-Jan to 17-Sep ###","74918fc9":"Use a Deep Learning technique with one hidden layer of 20 LSTM cells, outputting into 3 values, ie the predictions at time t+1,t+2,t+3. Input layer being 10 by 1 in size, for the 10 prior values of time series.","6c52fe4a":"### We sort the values by date ascending order ###","3ffaf042":"### The next part is the most important step in creating the train data for a Supervised Learning problem. We shift the data with a lookback of 5 time steps and use this data to predict the value of the Call volume at the next time step.","82f725a4":"### Let us investigate the values for one County (El Paso) within a state ('Texas') ###","561b9f8d":"# We split the train and test data. We have 189 values, worth of few months of data. We take 160 values as train data, and remaining as train data. Since this is time series data, we take the first 160 values as train data and next 29 values as test data","e9acc134":"**Grid Search of parameters**\nWe can improve the DeepLearning approach further by using Grid Search of Neural Network parameters using sklearn wrapper for Keras, KerasRegressor, and GridSearchCV. ","98464be8":"### Let us read the US Counties dataset into a dataframe ###","919a8800":"Plot the predicted and actual values at time t+1,t+2,t+3","7b065970":"### **Approach 1: We use lookback period of 5, and create Supervised Training Data** for applying Deep Learning"}}