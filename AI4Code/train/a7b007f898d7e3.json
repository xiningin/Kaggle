{"cell_type":{"a597ec23":"code","7c7900d9":"code","5c082375":"code","29854fc3":"code","8677838e":"code","c0bd019f":"code","8e71d451":"code","fc4e0138":"code","b4cf644b":"code","12949c75":"code","d8d2e476":"code","4c3ab282":"code","798d83dc":"code","ef49d5fc":"code","eca219d3":"code","2f798273":"code","9eee4eeb":"code","5674dfe7":"code","fa5facb8":"code","2aa12209":"code","6968a121":"code","f4b3a420":"code","bb95765d":"code","b22226f6":"code","241453bf":"code","f478ff88":"code","a7cc4f38":"code","b2a6fcf9":"code","84b84337":"code","8f314c05":"code","f301ead7":"code","48c505fc":"code","a6e94627":"code","268b4791":"markdown","fec908b9":"markdown","d8dc888a":"markdown","13b095e4":"markdown","552cc74a":"markdown","0d69f697":"markdown","c927f834":"markdown","e3ea6fd6":"markdown","c2643cc3":"markdown","a0854122":"markdown","82d7668f":"markdown","f6c4f792":"markdown","de510fa2":"markdown","08363000":"markdown","3ac37127":"markdown","3be33e82":"markdown","f6c7b549":"markdown","52cbcb47":"markdown","e76fa265":"markdown","350c5380":"markdown","6f5bcbcf":"markdown","0c291882":"markdown","8ef94395":"markdown","d69fc754":"markdown","99e581f5":"markdown","0bea823d":"markdown","563a747b":"markdown","600a1a70":"markdown","fc99a19b":"markdown","ab046604":"markdown","8ca62eb8":"markdown","ac68a78b":"markdown","3c1f698e":"markdown","504832a2":"markdown","a628f387":"markdown"},"source":{"a597ec23":"# Packages needed\n\nimport pandas as pd\npd.options.mode.chained_assignment = None  # default='warn'\nimport numpy as np\nimport seaborn as sns\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom matplotlib import pyplot as plt\nimport pylab as plot","7c7900d9":"# path to dataset\n# CHANGE AS NEEDED\ndf=pd.read_csv (r'..\/input\/titanic\/train.csv')","5c082375":"print(\"Size of the dataset :\",df.shape)\ndf.head()","29854fc3":"if np.array_equal(df.index.tolist(),np.array(df[\"PassengerId\"].tolist())-1):\n    print(\"True\")\n    del df[\"PassengerId\"]","8677838e":"sns.countplot('Survived',data=df)","c0bd019f":"df.describe()","8e71d451":"plt.subplots(figsize=(7, 5))\nplt.boxplot(df['Fare'])\nplt.title('Boxplot of Fare')\nplt.show()","fc4e0138":"df[df['Fare']>300]","b4cf644b":"# df[df['Fare']>300]\n# df['Fare'].loc[[258,679,737]]=df[df['Fare']>300]['Fare']\/3","12949c75":"plt.subplots(figsize=(7, 5))\nsns.barplot(x='Sex', y='Survived', data=df, ci=None)\nplt.title('Ratio of survivors based on sex')\nplt.show()","d8d2e476":"sns.countplot('Pclass',hue='Survived',data=df)","4c3ab282":"df['Title']=''\nfor i in range(len(df['Name'].values)):\n    j=0\n    while j<len(df['Name'][i]) and df['Name'][i][j]!=',':\n        j+=1\n    temp=''\n    j+=2\n    while j<len(df['Name'][i]) and df['Name'][i][j]!='.':\n        temp+=df['Name'][i][j]\n        j+=1\n    df['Title'][i]=temp\n    \nprint(df['Title'].unique())","798d83dc":"def map_title(df):\n    title_category = {\n    \"Capt\": \"Officer\",\n    \"Col\": \"Officer\",\n    \"Major\": \"Officer\",\n    \"Jonkheer\": \"Royalty\",\n    \"Don\": \"Royalty\",\n    \"Sir\": \"Royalty\",\n    \"Dr\": \"Officer\",\n    \"Rev\": \"Officer\",\n    \"the Countess\": \"Royalty\",\n    \"Dona\": \"Royalty\",\n    \"Mme\": \"Mrs\",\n    \"Mlle\": \"Miss\",\n    \"Ms\": \"Mrs\",\n    \"Mr\": \"Mr\",\n    \"Mrs\": \"Mrs\",\n    \"Miss\": \"Miss\",\n    \"Master\": \"Master\",\n    \"Lady\": \"Royalty\"\n    }\n    new_title = df['Title'].map(title_category)\n    return new_title\ndf['Title']=map_title(df)","ef49d5fc":"plt.subplots(figsize=(7, 5))\nsns.barplot(x='Title', y='Survived', data=df, ci=None)\nplt.title('Ratio of survivors based on title')\nplt.show()","eca219d3":"plt.figure(figsize=(8,6))\nsns.heatmap(df.isnull(),cmap='viridis')","2f798273":"age_ref = pd.DataFrame(data=[df.groupby('Title')['Age'].mean()],columns=df['Title'].unique())\n\ndef fill_age(title,age):\n    if pd.isnull(age):\n        return float(age_ref[title])\n    else:\n        return age\n\ndf['Age'] = df.apply(lambda x: fill_age(x['Title'],x['Age']), axis=1)","9eee4eeb":"del df['Name']\ndel df['Ticket']\ndel df['Cabin']","5674dfe7":"# Looks at types and change int to floats\nnumbInt=0\nnumbCont=0\nnumbCat=0\nfor x in df.columns:\n    if(df[x].dtype==np.float64):\n        numbCont+=1\n    if(df[x].dtype==np.int64):\n        numbInt+=1\n        if x!='Survived':\n            df[x]=df[x].astype(float) # if we want to transform all the int features to float features\n    elif(df[x].dtype==np.object):\n        numbCat+=1\nprint(\"Number of continuous features : \" , numbCont,\", of int features : \" , numbInt,\", of categorical features : \" , numbCat , \"\\n\")","fa5facb8":"for x in df.columns:\n    if(df[x].dtype==np.object):\n        print(\"Values of feature\", x, \":\",df[x].unique())","2aa12209":"df.head()","6968a121":"# Separate X and y\ny=df[\"Survived\"].values\ndf=df.loc[:, df.columns != \"Survived\"]\n\n# Split train and test\nX=df\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)\nprint(sum(y_train)\/len(y_train),sum(y_test)\/len(y_test))","f4b3a420":"# Mean normalisation for continuous and int columns\ndictMeanStd={}\nfor x in X_train.columns:\n    if(X_train[x].dtype!=np.object):\n        dictMeanStd[x]=[X_train[x].mean(),X_train[x].std()]\n        X_train[x]=(X_train[x]-dictMeanStd[x][0])\/dictMeanStd[x][1]","bb95765d":"# Missing Values\nprint(\"Number of lines where at least one value is missing\", sum([True for idx,row in X_train.iterrows() if any(row.isnull())]))\nprint(\"Which is\",sum([True for idx,row in X_train.iterrows() if any(row.isnull())])\/len(X_train)*100,\" % of the dataset.\")\n\n# We cannot drop the missing values\nprint(X_train.isnull().sum().sort_values())\n\n# Create new category Unknown for missing values in categorical features:\nfor x in X_train.columns:\n    if(X_train[x].dtype==np.object):\n        X_train[x] = np.where(X_train[x].isnull(),\"Unknown\",X_train[x])\n\n# Put 0 where values are missing for cont and int features\nX_train=X_train.replace(np.nan,0)","b22226f6":"# Be careful to suppress also values in y_train\n\"\"\"\nthreshold=5\nX_train['Survived']=y_train\nX_train['outliers']=0\nfor x in X_train.columns:\n    if(X_train[x].dtype!=np.object and x!='outliers' and x!='Survived'):\n        X_train.loc[(X_train[x]-X_train[x].mean()).abs() > threshold*X_train[x].std(),'outliers'] = 1\n\nprint(\"The number of outliers is\",sum(X_train['outliers']))\nX_train=X_train[X_train.outliers==0].reset_index(drop=True)\ny_train=X_train['Survived'].values\ndel X_train['Survived']\ndel X_train['outliers']\n\"\"\"","241453bf":"# Separate cat and float features to do a onehotencoding on cat features\nX_traintemp=X_train.select_dtypes('object')\nenc=OneHotEncoder(handle_unknown = 'ignore')\nenc.fit(X_traintemp)\nX_traintemp=enc.transform(X_traintemp).toarray()\nX_train=np.concatenate([X_traintemp,X_train.select_dtypes('float64').values],axis=1)","f478ff88":"# Choice of the model\nfrom sklearn.linear_model import LogisticRegression\n\nregressor = LogisticRegression(max_iter=10000)","a7cc4f38":"param_grid = {'C': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 3, 4, 5, 10, 20, 30, 40, 50, 75, 100], 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\nregressor = GridSearchCV(regressor, param_grid, cv=5, scoring=\"accuracy\")","b2a6fcf9":"# Mean normalization\nfor x in X_test.columns:\n    if(X_test[x].dtype!=np.object):\n        X_test[x]=(X_test[x]-dictMeanStd[x][0])\/dictMeanStd[x][1]\n# Missing Values\nfor x in X_test.columns:\n    if(X_test[x].dtype==np.object):\n        X_test[x] = np.where(X_test[x].isnull(),\"Unknown\",X_test[x])\nX_test=X_test.replace(np.nan,0)\n# One Hot Encoding\nX_testtemp=X_test.select_dtypes('object')\nX_testtemp=enc.transform(X_testtemp).toarray()\nX_test=np.concatenate([X_testtemp,X_test.select_dtypes('float64').values],axis=1)","84b84337":"# Model\nregressor.fit(X_train, y_train)\nprint('Best parameters :', regressor.best_params_)","8f314c05":"y_train_pred=regressor.predict(X_train)\nprint(\"Accuracy on the train:\", accuracy_score(y_train_pred,y_train))\ny_pred=regressor.predict(X_test)\nprint(\"Accuracy on the test:\", accuracy_score(y_pred,y_test))","f301ead7":"# Import the submission file\n# CHANGE AS NEEDED\nsub=pd.read_csv (r'..\/input\/titanic\/test.csv')","48c505fc":"del sub['PassengerId']\n\n# Create the feature Title\nsub['Title']=''\nfor i in range(len(sub['Name'].values)):\n    j=0\n    while j<len(sub['Name'][i]) and sub['Name'][i][j]!=',':\n        j+=1\n    temp=''\n    j+=2\n    while j<len(sub['Name'][i]) and sub['Name'][i][j]!='.':\n        temp+=sub['Name'][i][j]\n        j+=1\n    sub['Title'][i]=temp\nsub['Title']=map_title(sub)\n\n# Suppress unecessary features\ndel sub['Name']\ndel sub['Cabin']\ndel sub['Ticket']\n\n# Int features to float\nfor x in sub.columns:\n    if(sub[x].dtype==np.int64):\n        numbInt+=1\n        sub[x]=sub[x].astype(float) # if we want to transform all the int features to float features\n\n# Missing Values 1\nsub['Age'] = sub.apply(lambda x: fill_age(x['Title'],x['Age']), axis=1)\n\n# Mean normalization\nfor x in sub.columns:\n    if(sub[x].dtype!=np.object):\n        sub[x]=(sub[x]-dictMeanStd[x][0])\/dictMeanStd[x][1]\n\n# Missing Values 2\nfor x in sub.columns:\n    if(sub[x].dtype==np.object):\n        sub[x] = np.where(sub[x].isnull(),\"Unknown\",sub[x])\nsub=sub.replace(np.nan,0)\n\n# One Hot Encoding\nsubtemp=sub.select_dtypes('object')\nsubtemp=enc.transform(subtemp).toarray()\nsub=np.concatenate([subtemp,sub.select_dtypes('float64').values],axis=1)","a6e94627":"# Predict\ny_submission_pred=regressor.predict(sub)\n# Export\n# CHANGE AS NEEDED\nexport=pd.read_csv (r'..\/input\/titanic\/test.csv')\nexport=pd.DataFrame(export[\"PassengerId\"])\nexport[\"Survived\"]=y_submission_pred\nexport.to_csv (r'submission.csv', index = False, header=True)","268b4791":"- One Hot Encoding on categorical features (nominal)","fec908b9":"- Count of survived","d8dc888a":"- Check is the categorical features look badly filled","13b095e4":"# Submission part","552cc74a":"- The maximum value of Fare looks strange compared to the mean, lets look deeper at this","0d69f697":"- Some description of the numerical features","c927f834":"- Split Train and Test","e3ea6fd6":"- Types of features and change int to floats","c2643cc3":"- Detection of outliers and suppression of them","a0854122":"- Print the tickets with Fare higher than 300","82d7668f":"- Lets look at ratio of survivors based on titles","f6c4f792":"- Cross-validation ","de510fa2":"- Prediction and export","08363000":"- Delete features that dont look useful for the prediction","3ac37127":"- Prediction and accuracy of the model on train and test sets","3be33e82":"- Mean normalisation","f6c7b549":"- Fit of the model","52cbcb47":"- The two first columns look similar, check and delete one.","e76fa265":"- Heatmap of missing values","350c5380":"- Add new feature Title extracted from the feature Name and check the values","6f5bcbcf":"- Load the dataset","0c291882":"## Lecture of the dataset and preprocessing","8ef94395":"#### Model","d69fc754":"# Titanic\n\n#### Objective:\n- Predict survival on the Titanic\n\n#### Model:\n- Logistic Regression","99e581f5":"- Preprocessing","0bea823d":"- They share the same ticket, can we divide by 3 the Fare value?","563a747b":"- Lets look at ratio of survivors based on pclass (class of the ticket)","600a1a70":"### Handling of categorical features","fc99a19b":"- Lets look at ratio of survivors based on sex","ab046604":"- Lets group together some titles","8ca62eb8":"- Handle the rest of the missing values.","ac68a78b":"## Machine Learning\n##### Model: Random Forest","3c1f698e":"- Fill of the feature Age using means over Title","504832a2":"- Size and first lines of the dataframe:","a628f387":"- Preprocess of the test set"}}