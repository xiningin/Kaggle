{"cell_type":{"aaaa56bc":"code","ac643f49":"code","08dd7d07":"code","805e52cf":"code","635e2543":"code","34b247ea":"code","a7907873":"code","0c29e146":"code","b1219fc0":"code","07831db2":"code","0d26b2cb":"code","8a3d50bd":"markdown","4f7a0a08":"markdown","a6ca56bb":"markdown","cfc6dc9e":"markdown","db938cc6":"markdown","95271729":"markdown","bd9d87ea":"markdown"},"source":{"aaaa56bc":"!ls \/kaggle\/input\/covid-nlp-preprocess\/output\/","ac643f49":"!ls \/kaggle\/input\/covid-nlp-preprocess\/output\/paragraphs","08dd7d07":"!ls \/kaggle\/input\/covid-nlp-preprocess\/output\/whole","805e52cf":"!ls \/kaggle\/input\/covid-nlp-preprocess\/output\/whole\/biorxiv_medrxiv | head -n 10","635e2543":"with open(\"\/kaggle\/input\/covid-nlp-preprocess\/output\/whole\/biorxiv_medrxiv\/006df1a5284369a9e2ff2dc7ab267a9f70294d8d.txt\", \"r\") as f:\n    text = f.read()\n    print(text[:400])","34b247ea":"!ls \/kaggle\/input\/CORD-19-research-challenge","a7907873":"import pandas as pd\n\ndf_metadata = pd.read_csv(\"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\")\ndf_metadata.head()","0c29e146":"df_metadata[df_metadata[\"cord_uid\"] == \"zvrfqkol\"]","b1219fc0":"!ls \/kaggle\/input\/covid-nlp-preprocess\/output\/paragraphs","07831db2":"!ls \/kaggle\/input\/covid-nlp-preprocess\/output\/paragraphs\/biorxiv_medrxiv | head -n 10","0d26b2cb":"import json\n\nwith open(\"\/kaggle\/input\/covid-nlp-preprocess\/output\/paragraphs\/biorxiv_medrxiv\/006df1a5284369a9e2ff2dc7ab267a9f70294d8d.json\") as f:\n    d = json.load(f)\n    print(\"doc_id: \"+d[\"doc_id\"])\n    texts = \"\"\n    for paragraph in d[\"body_text\"]:\n        paragraph_text = \" \".join(paragraph[\"text\"])\n        texts += paragraph_text + \"\\n\\n\"\n    print(texts)\n","8a3d50bd":"Under \"paragraphs\" there are the documents for the four input sources used in the COVID19 NLP dataset. Files under this directory are JSON files with each document containing paragraphs in a similar set as in the original dataset. So each document split into multiple parts.","4f7a0a08":"If you look at the above, the \"zvrfqkol\" is the ID from the example \"whole\" files first line as loaded above (few cells up).","a6ca56bb":"An example of how to load one, and how to access the doc id similar to above for the whole doc:","cfc6dc9e":"Check the [preprocessing kernel](https:\/\/www.kaggle.com\/donkeys\/preprocess-input-docs-from-apr-17-upload-dataset\/edit) that produces this for the details on how this is built.\n\nCheck the [Topic Models and Transformer Summaries kernel](https:\/\/www.kaggle.com\/donkeys\/topics-and-summaries-lda-and-transformers) for an example of bigger use case.\n\nThe original input the has been preprocessed is from the [COVID19 NLP dataset](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge).\n\nFollowing is a short example on how to load the data.","db938cc6":"Under \"whole\" are all the documents in one big text file per document. Not splitting or anything.","95271729":"The first line of the \"whole\" documents contains the document ID as present in the COVID metadata:","bd9d87ea":"The is under \"output\" directory. What a name.. There are two txt files to help find words for cleanup. Then two directories for differently processed documents. Mostly you are interested in these directories unless you want to finetune the preprocessing kernel."}}