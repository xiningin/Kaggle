{"cell_type":{"bfe89d5b":"code","fb8ed49a":"code","8f4e5c23":"code","544295ba":"code","228bc72b":"code","1a5b62d1":"code","c90ad910":"code","b6b2a23e":"code","3512a1cc":"code","7d48f140":"code","01e80272":"code","302fc377":"code","ddf66664":"code","2b253a45":"code","ff1ba7d4":"code","132e0242":"code","24f8d8fa":"markdown","8ac7651b":"markdown","167bc3d3":"markdown","0833fcd0":"markdown","dfbbc74a":"markdown","37197fcf":"markdown","41123cda":"markdown","d74c19ee":"markdown","03b4965b":"markdown","2464ed24":"markdown","ab6089af":"markdown","04cfd301":"markdown","930c8440":"markdown","e0ac6608":"markdown"},"source":{"bfe89d5b":"import numpy as np\nimport matplotlib.pyplot as plt \n\nimport pandas as pd  \nimport seaborn as sns \n\n%matplotlib inline","fb8ed49a":"from sklearn.datasets import load_boston\n\nboston_dataset = load_boston()\n\n# boston_dataset is a dictionary\n# let's check what it contains\nboston_dataset.keys()","8f4e5c23":"boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\nboston.head()","544295ba":"boston['MEDV'] = boston_dataset.target","228bc72b":"# check for missing values in all the columns\nboston.isnull().sum()","1a5b62d1":"# set the size of the figure\nsns.set(rc={'figure.figsize':(11.7,8.27)})\n\n# plot a histogram showing the distribution of the target values\nsns.distplot(boston['MEDV'], bins=30)\nplt.show()","c90ad910":"# compute the pair wise correlation for all columns  \ncorrelation_matrix = boston.corr().round(2)","b6b2a23e":"# use the heatmap function from seaborn to plot the correlation matrix\n# annot = True to print the values inside the square\nsns.heatmap(data=correlation_matrix, annot=True)","3512a1cc":"plt.figure(figsize=(20, 5))\n\nfeatures = ['LSTAT', 'RM']\ntarget = boston['MEDV']\n\nfor i, col in enumerate(features):\n    plt.subplot(1, len(features) , i+1)\n    x = boston[col]\n    y = target\n    plt.scatter(x, y, marker='o')\n    plt.title(col)\n    plt.xlabel(col)\n    plt.ylabel('MEDV')","7d48f140":"X = pd.DataFrame(np.c_[boston['LSTAT'], boston['RM']], columns = ['LSTAT','RM'])\nY = boston['MEDV']","01e80272":"from sklearn.model_selection import train_test_split\n\n# splits the training and test data set in 80% : 20%\n# assign random_state to any value.This ensures consistency.\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=5)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)","302fc377":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nlin_model = LinearRegression()\nlin_model.fit(X_train, Y_train)","ddf66664":"# model evaluation for training set\n\ny_train_predict = lin_model.predict(X_train)\nrmse = (np.sqrt(mean_squared_error(Y_train, y_train_predict)))\nr2 = r2_score(Y_train, y_train_predict)\n\nprint(\"The model performance for training set\")\nprint(\"--------------------------------------\")\nprint('RMSE is {}'.format(rmse))\nprint('R2 score is {}'.format(r2))\nprint(\"\\n\")\n\n# model evaluation for testing set\n\ny_test_predict = lin_model.predict(X_test)\n# root mean square error of the model\nrmse = (np.sqrt(mean_squared_error(Y_test, y_test_predict)))\n\n# r-squared score of the model\nr2 = r2_score(Y_test, y_test_predict)\n\nprint(\"The model performance for testing set\")\nprint(\"--------------------------------------\")\nprint('RMSE is {}'.format(rmse))\nprint('R2 score is {}'.format(r2))","2b253a45":"# plotting the y_test vs y_pred\n# ideally should have been a straight line\nplt.scatter(Y_test, y_test_predict)\nplt.show()","ff1ba7d4":"from sklearn.preprocessing import PolynomialFeatures\n\ndef create_polynomial_regression_model(degree):\n  \"Creates a polynomial regression model for the given degree\"\n  poly_features = PolynomialFeatures(degree=degree)\n  \n  # transform the features to higher degree features.\n  X_train_poly = poly_features.fit_transform(X_train)\n  \n  # fit the transformed features to Linear Regression\n  poly_model = LinearRegression()\n  poly_model.fit(X_train_poly, Y_train)\n  \n  # predicting on training data-set\n  y_train_predicted = poly_model.predict(X_train_poly)\n  \n  # predicting on test data-set\n  y_test_predict = poly_model.predict(poly_features.fit_transform(X_test))\n  \n  # evaluating the model on training dataset\n  rmse_train = np.sqrt(mean_squared_error(Y_train, y_train_predicted))\n  r2_train = r2_score(Y_train, y_train_predicted)\n  \n  # evaluating the model on test dataset\n  rmse_test = np.sqrt(mean_squared_error(Y_test, y_test_predict))\n  r2_test = r2_score(Y_test, y_test_predict)\n  \n  print(\"The model performance for the training set\")\n  print(\"-------------------------------------------\")\n  print(\"RMSE of training set is {}\".format(rmse_train))\n  print(\"R2 score of training set is {}\".format(r2_train))\n  \n  print(\"\\n\")\n  \n  print(\"The model performance for the test set\")\n  print(\"-------------------------------------------\")\n  print(\"RMSE of test set is {}\".format(rmse_test))\n  print(\"R2 score of test set is {}\".format(r2_test))","132e0242":"create_polynomial_regression_model(2)","24f8d8fa":"**The target values is missing from the data. Create a new column of target values and add it to dataframe**","8ac7651b":"**Load the data into pandas dataframe**","167bc3d3":"**Split the data into training and testing sets**","0833fcd0":"I**mport the required Libraries**","dfbbc74a":"**Observations**\n\n\n\n\n*   From the above coorelation plot we can see that **MEDV** is strongly correlated to **LSTAT**, **RM**\n\n*  **RAD** and **TAX** are stronly correlated, so we don't include this in our features together to avoid multi-colinearity\n\n\n","37197fcf":"**Prepare the data for training**","41123cda":"**Data preprocessing**","d74c19ee":"**Data Visualization**","03b4965b":"## Polynomial Regression on Boston Housing Dataset\n\n**In this notebook we do a comparative study of Linear Regression and Polynomial Regression accuracy on the Boston Housing Dataset**\n\nThis data was originally a part of UCI Machine Learning Repository and has been removed now. This data also ships with the scikit-learn library. \nThere are 506 samples and 13 feature variables in this data-set. The objective is to predict the value of prices of the house using the given features.\n\nThe description of all the features is given below:\n\n  **CRIM**: Per capita crime rate by town\n\n  **ZN**: Proportion of residential land zoned for lots over 25,000 sq. ft\n\n  **INDUS**: Proportion of non-retail business acres per town\n\n  **CHAS**: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n\n  **NOX**: Nitric oxide concentration (parts per 10 million)\n\n  **RM**: Average number of rooms per dwelling\n\n  **AGE**: Proportion of owner-occupied units built prior to 1940\n\n  **DIS**: Weighted distances to five Boston employment centers\n\n  **RAD**: Index of accessibility to radial highways\n\n  **TAX**: Full-value property tax rate per $10,000\n\n  **B**: 1000(Bk - 0.63)\u00b2, where Bk is the proportion of [people of African American descent] by town\n\n  **LSTAT**: Percentage of lower status of the population\n\n  **MEDV**: Median value of owner-occupied homes in $1000s\n  \n  \n  \n\n","2464ed24":"**Load the Boston Housing DataSet from scikit-learn**","ab6089af":"# **Linear Regression**","04cfd301":"**Correlation matrix**","930c8440":"# **Polynomial Regression**\n\nWe can see that **LSTAT** doesn't vary exactly in a linear way. Let's apply the Polynomial Regression with **degree 2** and test. \n\nTo generate the higher order degrees, we use PolyniomialFeatures class from sklearn library. ","e0ac6608":"**We can observe that the error has reduced after using polynomial regression as compared to linear regression**"}}