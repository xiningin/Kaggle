{"cell_type":{"33ab605f":"code","c6bcbb08":"code","1c5172ca":"code","4486c11b":"code","974c16da":"code","7c50b9ef":"code","4ab83e24":"code","186bba4f":"code","6b7002b8":"code","c906d5af":"code","11a22234":"code","241413e1":"code","82e55c38":"code","8ec941d6":"code","d5fb65ef":"code","eacc3d3c":"code","4a5afe8c":"markdown","9ed3622b":"markdown","dbad82d3":"markdown","ee247370":"markdown","3e5899f6":"markdown","18642adf":"markdown","43fe5df0":"markdown","1912c9dc":"markdown","b89c4e9c":"markdown","32099cf4":"markdown","81651b1e":"markdown","fa85c396":"markdown"},"source":{"33ab605f":"# Take note of what cuda version you have by running either of the following commands\n# !conda list | grep cudatoolkit\n!nvidia-smi\n\n# choices: {cuda92, cuda 100, cuda101, cuda102, cuda110, cuda111, cuda113}\n# replace XXX with the respective number\n# pip install bitsandbytes-cudaXXX\n# I believe Colab is 11.2 so the command would be: pip install bitsandbytes-cuda112\n!pip install bitsandbytes-cuda110 -q\n!pip install -U wandb -q\n!pip install seqeval git+https:\/\/github.com\/huggingface\/transformers.git -q","c6bcbb08":"# This tests if the installation was successful\n!wget https:\/\/gist.githubusercontent.com\/TimDettmers\/1f5188c6ee6ed69d211b7fe4e381e713\/raw\/4d17c3d09ccdb57e9ab7eca0171f2ace6e4d2858\/check_bnb_install.py && python check_bnb_install.py","1c5172ca":"from pathlib import Path\n\ntransformers_path = Path(\"\/opt\/conda\/lib\/python3.7\/site-packages\/transformers\")\n\nimport shutil\n\ninput_dir = Path(\"..\/input\/debertav2xlfasttokenizer\")\n\nconvert_file = input_dir \/ \"convert_slow_tokenizer.py\"\nconversion_path = transformers_path\/convert_file.name\n\nif conversion_path.exists():\n    conversion_path.unlink()\n\nshutil.copy(convert_file, transformers_path)\ndeberta_v2_path = transformers_path \/ \"models\" \/ \"deberta_v2\"\n\nfor filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n    filepath = deberta_v2_path\/filename\n    if filepath.exists():\n        filepath.unlink()\n\n    shutil.copy(input_dir\/filename, filepath)","4486c11b":"import os\nimport json\nimport math\nfrom pathlib import Path\nfrom functools import partial\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom tqdm.notebook import tqdm\nimport datasets\nfrom transformers import (\n    TrainingArguments, \n    Trainer, \n    AutoConfig,\n    AutoModelForTokenClassification,\n    AutoTokenizer,\n    get_scheduler,\n    DataCollatorForTokenClassification,\n)\nimport bitsandbytes as bnb","974c16da":"class CFG:\n    \n    fold = 0\n    \n    model_name = \"..\/input\/debertav2xlfasttokenizer\/microsoft-deberta-v2-xlarge\"\n    \n    max_seq_length = 512\n    text_column = \"text\"\n    label_column = \"labels\"\n    word_id_column = \"word_ids\"\n    adam_bits = 8 # Choose from {8,32}\n\n    training_args = TrainingArguments(\n        output_dir=\"output\",\n        overwrite_output_dir=True,\n        do_train=True,\n        do_eval=True,\n        evaluation_strategy=\"epoch\",\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=8,\n        gradient_accumulation_steps=1,\n        learning_rate=3e-5,\n        weight_decay=0.01,\n        num_train_epochs=1,\n        max_steps=-1, # set >0 to limit\n        lr_scheduler_type=\"linear\",\n        warmup_ratio=0.1,\n        logging_strategy=\"steps\",\n        logging_steps=100,\n        save_strategy=\"no\",\n        save_steps=None,\n        seed=18,\n        fp16=True, \n        eval_steps=None, # change evaluation_strategy to steps to use this\n        dataloader_num_workers=4,\n        run_name=f\"deb-v2-xl-{adam_bits}bitAdam\",\n        group_by_length=True, # This can also help speed training\n        report_to=\"wandb\",\n        resume_from_checkpoint=None,\n        gradient_checkpointing=True, # save memory, but backwards pass slower\n    )\n\n# for convenience\nargs = CFG.training_args","7c50b9ef":"# I use weights and biases to track training.\n# The following code requires attaching a secret to the notebook.\nif \"wandb\" in args.report_to:\n    import wandb\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    key = user_secrets.get_secret(\"wandb\")\n    \n    wandb.login(key=key)\n    os.environ[\"WANDB_PROJECT\"] = \"feedback-prize\"","4ab83e24":"%%time\n\nif not Path(\"full_dataset.dataset\").exists():\n    texts, ids = [], []\n    for file in tqdm(Path(\"..\/input\/feedback-prize-2021\/train\").glob(\"*.txt\"), total=15594, desc=\"Reading train texts\"):\n        ids.append(file.stem)\n\n        with open(file) as fp:\n            texts.append(fp.read())\n        \n    \ndef add_label_information(examples):\n    \n    texts = examples[CFG.text_column]\n    ids = examples[\"id\"]\n    all_labels, folds, words = [], [], []\n    \n    for text, id_ in zip(texts, ids):\n    \n        df = train_df[train_df[\"id\"]==id_]\n\n        text = text.split()\n        num_words = len(text)\n\n        labels = [\"O\"]*num_words\n\n        for discourse_type, predictionstring in df[[\"discourse_type\", \"predictionstring\"]].values:\n\n            first = True\n            for word_id in map(int, predictionstring.split()):\n                prefix = \"I-\"\n                if first:\n                    prefix = \"B-\"\n                    first = False\n                labels[word_id] = prefix+discourse_type\n                \n        all_labels.append(labels)\n        folds.append(df[\"kfold\"].values[0])\n        words.append(text)\n\n    examples[CFG.label_column] = all_labels\n    examples[\"fold\"] = folds\n    examples[CFG.text_column] = words\n    return examples\n    \n\n# Using fold strategy shown by Abhishek https:\/\/www.kaggle.com\/abhishek\/creating-folds-properly-hopefully-p\/\ntrain_df = pd.read_csv(\"..\/input\/creating-folds-properly-hopefully-p\/train_folds.csv\", usecols=[\"id\", \"discourse_type\", \"predictionstring\", \"kfold\"])\n\n\n# This step can take a few minutes\nif not Path(\"full_dataset.dataset\").exists(): # This was for when I was debugging so I wouldn't run this multiple times\n    temp_dataset = datasets.Dataset.from_dict({\"id\": ids, CFG.text_column: texts})\n    temp_dataset = temp_dataset.map(add_label_information, batched=True, num_proc=args.dataloader_num_workers)\n\n    full_dataset = datasets.DatasetDict()\n    full_dataset[\"train\"] =  temp_dataset.filter(lambda x: x[\"fold\"]!=CFG.fold)\n    full_dataset[\"validation\"] =  temp_dataset.filter(lambda x: x[\"fold\"]==CFG.fold)\n    full_dataset.save_to_disk(\"full_dataset.dataset\")\nelse:\n    full_dataset = datasets.DatasetDict.load_from_disk(\"full_dataset.dataset\")\nfull_dataset","186bba4f":"# DebertaV2TokenizerFast is not in the transformers library. This patches it in\n\ndef get_DebertaV2TokenizerFast(model_name):        \n    from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n    return DebertaV2TokenizerFast.from_pretrained(model_name)","6b7002b8":"# https:\/\/github.com\/huggingface\/transformers\/blob\/669e3c50c98ad5b506555a551d2ecbf72ceb3c99\/examples\/pytorch\/token-classification\/run_ner.py#L371\ndef tokenize_and_align_labels(examples, label2id, return_word_ids=False):\n    tokenized_inputs = tokenizer(\n        examples[CFG.text_column],\n        truncation=True,\n        max_length=CFG.max_seq_length,\n        # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n        is_split_into_words=True,\n    )\n    labels = []\n    all_word_ids = []\n    for i, label in enumerate(examples[CFG.label_column]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n            # ignored in the loss function.\n            if word_idx is None:\n                label_ids.append(-100)\n            else:\n                label_ids.append(label2id[label[word_idx]])\n            previous_word_idx = word_idx\n            \n        if return_word_ids:\n            all_word_ids.append(word_ids)\n\n        labels.append(label_ids)\n    \n    tokenized_inputs[CFG.label_column] = labels\n    \n    if return_word_ids:\n        tokenized_inputs[CFG.word_id_column] = all_word_ids\n    \n    return tokenized_inputs","c906d5af":"%%time\n\nlabel_list = ['O', 'B-Claim', 'I-Claim', 'B-Concluding Statement', 'I-Concluding Statement', \n              'B-Counterclaim', 'I-Counterclaim', 'B-Evidence', 'I-Evidence','B-Lead', 'I-Lead', \n              'B-Position', 'I-Position', 'B-Rebuttal', 'I-Rebuttal']\n\nlabel2id = {label:id_ for id_, label in enumerate(label_list)}\nid2label = {id_:label for id_, label in enumerate(label_list)}\n\nif \"deberta-v2\" in CFG.model_name:\n    tokenizer = get_DebertaV2TokenizerFast(CFG.model_name)\nelse:\n    tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, add_prefix_space=True)\n\nif Path(\"train.dataset\").exists():\n    train_dataset = datasets.Dataset.load_from_disk(\"train.dataset\")\nelse:\n    train_dataset = full_dataset[\"train\"].map(\n            partial(\n                tokenize_and_align_labels,\n                label2id=label2id,\n                return_word_ids=False\n        ),\n        batched=True,\n        num_proc=args.dataloader_num_workers,\n        remove_columns=[\"fold\", \"text\", \"id\"]\n    )\n    \n    train_dataset.save_to_disk(\"train.dataset\")\n\n    \nif Path(\"validation.dataset\").exists():\n    validation_dataset = datasets.Dataset.load_from_disk(\"validation.dataset\")\nelse:\n    validation_dataset = full_dataset[\"validation\"].map(\n        partial(\n            tokenize_and_align_labels,\n            label2id=label2id,\n            return_word_ids=True\n        ),\n        batched=True,\n        num_proc=args.dataloader_num_workers,\n            remove_columns=[\"fold\"]\n        )\n\n    validation_dataset.save_to_disk(\"validation.dataset\")\n    \n# bonus points if you can explain why it says\n# Ignored unknown kwarg option direction","11a22234":"model_config = AutoConfig.from_pretrained(\n    CFG.model_name,\n    num_labels=len(label_list),\n    label2id=label2id,\n    id2label=id2label,\n    finetuning_task=\"ner\",\n)\n\nmodel = AutoModelForTokenClassification.from_pretrained(CFG.model_name, config=model_config)","241413e1":"no_decay = [\"bias\", \"LayerNorm.weight\"]\noptimizer_grouped_parameters = [\n    {\n        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n        \"weight_decay\": args.weight_decay,\n    },\n    {\n        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n        \"weight_decay\": 0.0,\n    },\n]\n\n# These are the only changes you need to make\n# The first part sets the optimizer to use 8-bits\n# The for loop sets embeddings to use 32-bits\nif CFG.adam_bits == 32:\n    optimizer = bnb.optim.Adam32bit(optimizer_grouped_parameters, lr=args.learning_rate)\nif CFG.adam_bits == 8:\n    optimizer = bnb.optim.Adam8bit(optimizer_grouped_parameters, lr=args.learning_rate)\n    \n    # Thank you @gregorlied https:\/\/www.kaggle.com\/nbroad\/8-bit-adam-optimization\/comments#1661976\n    for module in model.modules():\n        if isinstance(module, torch.nn.Embedding):\n            bnb.optim.GlobalOptimManager.get_instance().register_module_override(\n                module, 'weight', {'optim_bits': 32}\n            )            \n\nnum_update_steps_per_epoch = len(train_dataset) \/\/ args.per_device_train_batch_size \/\/ args.gradient_accumulation_steps\nif args.max_steps == -1 or args.max_steps is None:\n    args.max_steps = args.num_train_epochs * num_update_steps_per_epoch\nelse:\n    args.num_train_epochs = math.ceil(args.max_steps \/ num_update_steps_per_epoch)\n    \nif args.warmup_ratio is not None:\n    args.num_warmup_steps = int(args.warmup_ratio * args.max_steps)\n\nlr_scheduler = get_scheduler(\n    name=args.lr_scheduler_type,\n    optimizer=optimizer,\n    num_warmup_steps=args.num_warmup_steps,\n    num_training_steps=args.max_steps,\n)\noptimizer","82e55c38":"# Data collator\n# 512 for longformer\n# 1024 for bigbird\n# 8 for short models\nif \"longformer\" in CFG.model_name:\n    pad_to_multiple_of = 512 \nelif \"bigbird\" in CFG.model_name:\n    pad_to_multiple_of = 1024\nelse:\n    pad_to_multiple_of = 8\n    \ndata_collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=pad_to_multiple_of)\n\n# Metrics\nmetric = datasets.load_metric(\"seqeval\")\n\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    # Remove ignored index (special tokens)\n    true_predictions = [\n        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    results = metric.compute(predictions=true_predictions, references=true_labels)\n    \n    # This f1 is not the same as the \n    return {\n        \"token_precision\": results[\"overall_precision\"],\n        \"token_recall\": results[\"overall_recall\"],\n        \"token_f1\": results[\"overall_f1\"],\n        \"token_accuracy\": results[\"overall_accuracy\"],\n    }","8ec941d6":"# Rob Mulla @robikscube\n# https:\/\/www.kaggle.com\/robikscube\/student-writing-competition-twitch\ndef calc_overlap(pred, ground_truth):\n    \"\"\"\n    Calculates the overlap between prediction and\n    ground truth and overlap percentages used for determining\n    true positives.\n    \"\"\"\n    set_pred = set(pred.split(' '))\n    set_gt = set(ground_truth.split(' '))\n    # Length of each and intersection\n    len_gt = len(set_gt)\n    len_pred = len(set_pred)\n    inter = len(set_gt.intersection(set_pred))\n    overlap_1 = inter \/ len_gt\n    overlap_2 = inter\/ len_pred\n    return [overlap_1, overlap_2]\n\n\ndef score_feedback_comp(pred_df, gt_df):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n        \n    Uses the steps in the evaluation page here:\n        https:\/\/www.kaggle.com\/c\/feedback-prize-2021\/overview\/evaluation\n    \"\"\"\n    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df = pred_df[['id','discourse_type','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df['pred_id'] = pred_df.index\n    gt_df['gt_id'] = gt_df.index\n    # Step 1. all ground truths and predictions for a given discourse_type are compared.\n    joined = pred_df.merge(gt_df,\n                           left_on=['id','discourse_type'],\n                           right_on=['id','discourse_type'],\n                           how='outer',\n                           suffixes=('_pred','_gt')\n                          )\n    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n\n    joined['overlaps'] = [calc_overlap(pred, gt) for pred, gt in joined[['predictionstring_pred', 'predictionstring_gt']].values]\n\n    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n\n\n    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n    tp_pred_ids = joined.query('potential_TP') \\\n        .sort_values('max_overlap', ascending=False) \\\n        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n\n    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    #calc microf1\n    denominator = (TP + 0.5*(FP+FN))\n    if denominator == 0:\n        return 0.0\n    my_f1_score = TP \/ denominator\n    return {\n        \"F1\": round(my_f1_score, 4),\n        \"Precision\": TP\/(TP+FP),\n        \"Recall\": TP\/(TP+FN), # This was calculated incorrectly in the runs\n    }\n        \n\nid2label={i: l for l, i in label2id.items()}\n# https:\/\/www.kaggle.com\/zzy990106\/pytorch-ner-infer?scriptVersionId=82677278&cellId=13\ndef get_label_predictions(dataset, preds):\n\n    ids = dataset[\"id\"]\n    word_ids = dataset[CFG.word_id_column]\n    words = dataset[CFG.text_column]\n    \n    all_preds = []\n\n    for id_, sample_preds, sample_word_ids, words in zip(ids, preds, word_ids, words):\n        label_preds = [\"\"]*len(words)\n\n        for pred, w_id in zip(sample_preds, sample_word_ids):\n            if w_id is None:\n                continue\n            if label_preds[w_id] == \"\":\n                label_preds[w_id] = id2label[pred]\n\n        j = 0\n        while j < len(label_preds):\n            label = label_preds[j]\n\n            if label.startswith(\"B\"):\n                label = label.replace(\"B\", \"I\")\n                end = j + 1\n                while end < len(label_preds) and label_preds[end] == label:\n                    end += 1\n\n                if end - j > 7:\n                    all_preds.append((id_, label.lstrip(\"BI-\"), ' '.join(map(str, list(range(j, end))))))\n\n                j = end\n            else:\n                j += 1\n                \n    return all_preds","d5fb65ef":"class FeedbackPrizeTrainer(Trainer):\n    \n    def __init__(self, *args, **kwargs):\n        if \"eval_dataset\" in kwargs:\n            # The Trainer hides some columns that are necessary for CV\n            # This code makes those columns accessible\n            dataset_type = kwargs[\"eval_dataset\"].format[\"type\"]\n            dataset_columns = list(kwargs[\"eval_dataset\"].features.keys())\n            self.cv_dataset = kwargs[\"eval_dataset\"].with_format(type=dataset_type, columns=dataset_columns)\n        super().__init__(*args, **kwargs)\n        \n        \n    def evaluation_loop(\n        self, \n        dataloader,\n        description,\n        prediction_loss_only = None,\n        ignore_keys = None,\n        metric_key_prefix = \"eval\",\n    ):\n        \n        eval_output =  super().evaluation_loop(\n            dataloader,\n            description,\n            prediction_loss_only,\n            ignore_keys,\n            metric_key_prefix\n        )\n        \n        # Custom CV F1 calculation\n        # This same loop gets called during predict, and we can't do CV when predicting\n        if metric_key_prefix == \"eval\":\n            \n            eval_id_preds = eval_output.predictions.argmax(-1)\n            eval_label_preds = get_label_predictions(self.cv_dataset, eval_id_preds)\n            \n            eval_pred_df = pd.DataFrame(eval_label_preds, columns=[\"id\", \"discourse_type\", \"predictionstring\"])\n            \n            eval_gt_df = train_df[train_df[\"id\"].isin(self.cv_dataset[\"id\"])].reset_index(drop=True).copy()\n            \n            classes = ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement', 'Counterclaim', 'Rebuttal']\n            f1_scores = []\n            for class_ in classes:\n                gt_df = eval_gt_df.loc[eval_gt_df['discourse_type'] == class_].copy()\n                pred_df = eval_pred_df.loc[eval_pred_df['discourse_type'] == class_].copy()\n                eval_scores = score_feedback_comp(pred_df, gt_df)\n                for score_name, score in eval_scores.items():\n                    eval_output.metrics[f\"{metric_key_prefix}_{class_}_CV_{score_name}\"] = score\n                f1_scores.append(eval_scores[\"F1\"])\n                \n            eval_output.metrics[f\"{metric_key_prefix}_Overall_CV_F1\"] = np.mean(f1_scores)\n        \n        return eval_output\n\n# Initialize our Trainer\ntrainer = FeedbackPrizeTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=validation_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    optimizers=(optimizer, lr_scheduler)\n)","eacc3d3c":"%env TOKENIZERS_PARALLELISM=true\n\n\ntrain_result = trainer.train()\nmetrics = train_result.metrics\ntrainer.save_model()  # Saves the tokenizer too\n\nmetrics[\"train_samples\"] = len(train_dataset)\n\ntrainer.log_metrics(\"train\", metrics)\ntrainer.save_metrics(\"train\", metrics)\ntrainer.save_state()","4a5afe8c":"# Create custom trainer","9ed3622b":"# Tokenizing data\n\nDon't pad to max length unless you are on a TPU or you really want to extend your training","dbad82d3":"# Weights and Biases Report \u2728\n\n#### Update v25: A more thorough testing of 8-bit vs 32-bit Adam was carried out. [Report available here](https:\/\/wandb.ai\/nbroad\/feedback-prize\/reports\/8-bit-Adam-vs-32-bit-Adam-tests--VmlldzoxNDgwNTc1) tldr: 8bit Adam is slower but saves memory and results in a higher CV.\n\n\n#### (v1 and v3): You can see the lower memory usage and marginally faster training time. The loss curves are nearly identical and CV F1 scores are pretty much the same as well. \n\n<iframe src=\"https:\/\/wandb.ai\/nbroad\/feedback-prize\/reports\/8-bit-Adam-vs-32-bit-Adam--VmlldzoxNDQ5Nzg3\" style=\"border:none;height:1024px;width:100%\">","ee247370":"# Optimizer\n\nHere is the key cell where the 8-bit Adam optimizer gets set. It's pretty much trivially easy...","3e5899f6":"# 8-bit Adam Optimization \ud83d\udc7e\n\n#### The optimizer is responsible for computing the gradient statistics for back propagation. These calculations are typically done on 32-bit values, but this notebook demonstrates how to use an 8-bit optimizer that saves memory and increases speed.\n\n#### The problem with reducing the number of bits is that the precision of each value decreases. Tim Dettmers ([@timdettmers](https:\/\/www.kaggle.com\/timdettmers)) did research at Facebook to figure out how to do stable optimization using 8 bits using a clever quantization trick. For a more detailed look at his research, please [read his paper](https:\/\/arxiv.org\/abs\/2110.02861) or [view his humorous video](https:\/\/www.youtube.com\/watch?v=IxrlHAJtqKE). The GitHub repo, which contains installation instructions for your specific GPU, can be found here: https:\/\/github.com\/facebookresearch\/bitsandbytes \n\n#### It was found that this allows for slightly faster training and for slightly larger models to be loaded into memory without sacrificing performance. \n\n## \ud83d\udd25 v18 Update: Thanks to @gregorlied for noticing that the embeddings were not optimized at 32-bits when using 8-bit Adam. Version 18 fixes that.\n\n## \ud83d\udd25\ud83d\udd25 v24 Update: deberta-v2-xlarge shown to train only with 8-bit Adam and not with 32-bit Adam (Version 22). This notebook also shows how to use fast version of deberta v2 tokenizer (not yet merged to master branch)\n\n## \ud83d\udd25\ud83d\udd25\ud83d\udd25 v25 Update: A more thorough testing of 8-bit vs 32-bit Adam was carried out. [Report available here](https:\/\/wandb.ai\/nbroad\/feedback-prize\/reports\/8-bit-Adam-vs-32-bit-Adam-tests--VmlldzoxNDgwNTc1) tldr: 8bit Adam is slower but saves memory and results in a higher CV.\n\n#### In this notebook, I compare the training times between the regular 32-bit Adam and the 8-bit Adam optimizer when training longformer-large for 1 epoch using a maximum sequence length of 2048. To use 8-bit Adam, you need to install the library and then change the one [line where the optimizer gets created](#Optimizer). In some cases using 8-bit Adam allows for larger batch sizes. Not this time, though \ud83d\ude05\n\n#### If you want to jump straight to the results (1 run, 1 epoch comparison), [click here](#Weights-and-Biases-Report-\u2728)\n\n#### If you want to see the notebook during the actual runs, version 1 has 8-bit Adam and version 3 has 32-bit Adam.\n\n#### I think 8-bit Adam is mostly useful for training large language models from scratch, and less for finetuning models with < 1B parameters. Perhaps the best use-case for Kaggle would be for the users who don't have any other compute and batch size of 1 just barely doesn't fit using 32-bit Adam. In that instance, 8-bit Adam would allow people to use Kaggle GPUs to train models that wouldn't fit otherwise. (Like in v24)\n\n#### The speed-ups are modest as seen in this image from the paper below.\n![8bit speed table](https:\/\/pbs.twimg.com\/media\/FBLeOZnVEAkt-Ij?format=png&name=small)\n\n#### 8 bit optimization also enables fitting bigger models on smaller GPUs: \n![8bit fit big models](https:\/\/pbs.twimg.com\/media\/FBLeMS_VIBEettR?format=png&name=900x900)\n\n\n#### [Tim's announcement tweet](https:\/\/twitter.com\/Tim_Dettmers\/status\/1446472128979562499?s=20) ","18642adf":"# Data Collator and Metrics \ud83d\udccf \n\nThis collator is really handy because I can tell it to pad to a multiple of a number. Longformer likes to have inputs in multiples of 512, so it will handle the padding for me!","43fe5df0":"# Install necessary libraries \ud83d\udcda\n\nYou must install the right version of `bitsandbytes` according to the GPU's CUDA version.","1912c9dc":"The following cell is only necessary if you want to use the fast tokenizer for deberta-v2","b89c4e9c":"# Create dataset\n\nThis is a little slow so it would probably be wise to create your dataset in another notebook and then load it in to the training notebook.","32099cf4":"# Train! \ud83d\ude86","81651b1e":"# Config ","fa85c396":"# CV calculation functions"}}