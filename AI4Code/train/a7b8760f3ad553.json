{"cell_type":{"07c04a88":"code","923c9c8a":"code","62be8456":"code","42efb9b4":"code","a0a5116e":"code","9127e3e4":"code","020f1105":"code","9f1c33a5":"markdown","1eb6b566":"markdown","73c25f05":"markdown","9c06399e":"markdown","c3ad48c7":"markdown","b19aa6e1":"markdown","1ccd1c90":"markdown","f6a43063":"markdown","d4f69531":"markdown","ac3bd548":"markdown","2cfbba83":"markdown","1cd5328d":"markdown"},"source":{"07c04a88":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n\ndf = pd.read_csv(\"..\/input\/Shopify 2019 Winter Data Science Intern Challenge Data Set - Sheet1.csv\")","923c9c8a":"df.order_amount.describe()","62be8456":"df.boxplot(column='order_amount')","42efb9b4":"unique_amounts = df.groupby(['order_amount']).size().reset_index(name='count').sort_values(by='order_amount', ascending=False)\nunique_amounts.head(10)","a0a5116e":"df.loc[df['order_amount'].isin([704000, 51450, 25725])].sort_values(by='order_amount', ascending=False)","9127e3e4":"q1 = df.order_amount.quantile(q=0.25)\nq2 = df.order_amount.quantile(q=0.5)\nq3 = df.order_amount.quantile(q=0.75)\nIQR = q3 - q1\n\ndf_truncated = df[(df.order_amount < q2 + IQR * 1.5) & (df.order_amount > q2 - IQR * 1.5)]\ndf_truncated.boxplot(column='order_amount')","020f1105":"df_truncated.order_amount.describe()","9f1c33a5":"**b. What metric would you report for this dataset? **\n\nLooking at the box plot for `df_truncated`, it looks like the distribution is skewed towards lower values.  With this in mind, I would report the median value of the truncated dataset. The values above the median will increase the increase the mean a disproportionate amount.\n\n\n**c. What is its value?**\n\nWe can find this value using the `describe()` function to find that it $272. It also shows that the standard deviation is 132.06, a much more reasonable result!","1eb6b566":"# Winter 2019 Data Science Intern Challenge\n\n## Question 1\n>On Shopify, we have exactly 100 sneaker shops, and each of these shops sells only one model of shoe. We want to do some analysis of the average order value (AOV). When we look at orders data over a 30 day window, we naively calculate an AOV of $3145.13. Given that we know these shops are selling sneakers, a relatively affordable item, something seems wrong with our analysis. \n\n**a. Think about what could be going wrong with our calculation. Think about a better way to evaluate this data.**","73c25f05":"The entire box portion of the box plot is a line along 0! It looks like there are lots of outliers. Let's try finding them by grouping the data by order amount, counting the number of entries for each group, and sort it in descending order.","9c06399e":"## Question 2\n> For this question you\u2019ll need to use SQL. Follow this link to access the data set required for the challenge. Please use queries to answer the following questions. Paste your queries along with your final numerical answers below.\n\n**a. How many orders were shipped by Speedy Express in total?**\n\nIf we join the `[Orders]` and `[Shippers]` tables on `ShipperID`, we can filter table by orders performed by `Speedy Express` and count the entries. \n\n```\nSELECT COUNT(*) AS NumberOfOrders\nFROM [Orders]\nJOIN [Shippers]\n\tON [Shippers].ShipperID = [Orders].ShipperID\nWHERE [Shippers].ShipperName = 'Speedy Express'\n```\n\nUsing the above query, the output shows the number of orders is **54**.","c3ad48c7":"Interesting. It looks like some of these high order amounts are repeated, especially 704000, 51450, and 25725. Let's look at these rows from our original dataset. ","b19aa6e1":"I'm interested in where that AOV of 3145.13 came from. I'm guessing it is probably from the mean as high values of order amounts will drag this number up. Using panda's handy `describe()` method on the data frame, we can see that the mean order amount is indeed 3,145.13. Also, there is an incredibly large standard deviation of 41282.54. This means that on average, the values vary 41,282.54 from the mean, making the mean not a very useful representation of the AOV!\n\nThe `describe()` function we used also gave us some valuable information to better understand this data aside from the mean and standard deviation.  We can see the minimum value is 90 and max value is 704,000. We can also see the values for the median and first and third quartile. Comparing these numbers, we can see that the maximum value is much higher than the other values. Considering our dataset has 5000 records, there are probably a few outlier values that are incredibly high dragging up the mean.\n\nTo get a sense of the distribution, we can look at a box plot of the data.","1ccd1c90":"Using the query above shows that the employee with the last name **Peacock** had the most orders at **40**.","f6a43063":"**c. What product was ordered the most by customers in Germany?**\n\nThe data we need to solve this problem is scattered across a few different tables. Our final query should show a list of products ordered by customers in Germay and how many orders of that product there were.\n\nWe can break down the problem into smaller sections and tackle them one at a time. First, let's look at *all orders from Germany*.\n\n```\nSELECT [Orders].OrderID,\n\t[Customers].Country\nFROM [Orders]\nJOIN [Customers]\n\tON [Customers].CustomerID = [Orders].CustomerID\nWHERE [Customers].Country = 'Germany'\n```\nThis gives us a list of all orders to customers in Germany.\n\nThe next part of the problem is figuring out *which item was ordered the most*. We need to incorporate information such as product ID and quantity from the `[OrderDetails]` table to do this. By joining `[OrderDetails]` through the `OrderID` column, we can find the total quantity for each product by summing the quantity column and grouping by product id. \n\n```\nSELECT [Customers].Country,\n\t[OrderDetails].ProductID,\n    SUM([OrderDetails].Quantity) AS \"TotalOrdered\"\nFROM [Orders]\nJOIN [Customers]\n\tON [Customers].CustomerID = [Orders].CustomerID\nJOIN [OrderDetails]\n\tON [OrderDetails].OrderID = [Orders].OrderID\nWHERE [Customers].Country = 'Germany'\nGROUP BY [OrderDetails].ProductID\nORDER BY TotalOrdered DESC -- Show most ordered item at the top.\n```\n\nAt this point, we can see the most ordered item has a product ID of 40 at 160 orders. To make sense of this result, we can join the `[Products]` table on the `ProductID` column to get the name of the most ordered product.\n\n```\nSELECT [Products].ProductName,\n    SUM([OrderDetails].Quantity) AS \"TotalOrdered\"\nFROM [Orders]\nJOIN [Customers]\n\tON [Customers].CustomerID = [Orders].CustomerID\nJOIN [OrderDetails]\n\tON [OrderDetails].OrderID = [Orders].OrderID\nJOIN [Products]\n\tON [Products].ProductID = [OrderDetails].ProductID\nWHERE [Customers].Country = 'Germany'\nGROUP BY [OrderDetails].ProductID\nORDER BY TotalOrdered DESC -- Show most ordered item at the top.\n```\n\nThis final query shows that **Boston Crab Meat** has the most orders at **160 total orders**.\n","d4f69531":"**b. What is the last name of the employee with the most orders?**\n\nTo solve this problem, we need to count the number of orders associated with each employee, see which one has the most orders, and report back their last name.\n\nWith SQL, we could count the number of orders in `[Orders]` and group by `EmployeeID`. This would give us which employee ID has the most orders, but not that employee's last name. Instead, we can use the `JOIN` expression to merge the `[Orders]` table and the `[Employees]` to match the employees to their employee ID and group by their last name to solve the problem.\n","ac3bd548":"Again this is interesting. It appears the order amounts of 704000 occur at the same time each day between the same store and users. The data for order amounts of 51450 and 25725 similar. It appears as though orders of 51450 are just transactions that bought two items worth 25725 as they all come from the same store id 78.\n\nFor the 704000 amounts, it seems like the transactions are probably some sort of supplier purchasing many shoes at once since the order amount is consistently 2000.\n\nTo better evaluate this data, we can clean some of the values. Let's try plotting only values that are the median +\/- 1.5 times the interquartile range.","2cfbba83":"```\nSELECT [Employees].LastName, COUNT(*) AS NumberOfOrders\nFROM [Orders]\nJOIN [Employees]\nON [Orders].EmployeeID = [Employees].EmployeeID\nGROUP BY [Employees].LastName\nORDER BY NumberOfOrders DESC\nLIMIT 1\n```","1cd5328d":"Using `df_truncated` should provide a more accurate representation of the typical order data."}}