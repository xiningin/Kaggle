{"cell_type":{"3f8cf4c4":"code","52b18b54":"code","fbc3fbc6":"code","a128fd3d":"code","cc508696":"code","a27b41fc":"code","89f5635b":"code","21d119fe":"code","675153a6":"code","7f71beb5":"code","b98a2342":"code","14324972":"code","40d17468":"code","92492a05":"code","9e6597a9":"code","1737d2eb":"code","f0c414f3":"code","9d16c9a9":"code","2276139a":"code","a755deb8":"code","2940dc15":"code","4dee6f03":"markdown","9803ff9d":"markdown","e070e294":"markdown","7f22dd3f":"markdown","7ac51ae6":"markdown","620cad99":"markdown","eaea0887":"markdown","acfdb8fe":"markdown","a37c47f6":"markdown","1bf3f5d0":"markdown","affc6ee4":"markdown","9573bcfb":"markdown","f86dbcd9":"markdown","3aa46e37":"markdown","80a68b69":"markdown","7b55cce8":"markdown","b6974208":"markdown","80c6466f":"markdown","e568e685":"markdown","4e9dcb76":"markdown","3a212049":"markdown","0630373d":"markdown","5c63c786":"markdown","935a2459":"markdown"},"source":{"3f8cf4c4":"# Computational imports\nimport numpy as np   # Library for n-dimensional arrays\nimport pandas as pd  # Library for dataframes (structured data)\n\n# Helper imports\nimport os \nimport pandas_datareader as web\nimport datetime as dt\n\n# ML\/DL imports\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, LSTM\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\n# Plotting imports\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as dates\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots","52b18b54":"file_name = '..\/input\/stock-time-series-20050101-to-20171231\/IBM_2006-01-01_to_2018-01-01.csv'\ndataset = pd.read_csv(file_name, index_col='Date', parse_dates=[\"Date\"])\ndataset.head()","fbc3fbc6":"dataset.isnull().sum()","a128fd3d":"dataset_open = dataset['Open'].dropna()\ndataset_open.isnull().sum()","cc508696":"dataset_open.head()","a27b41fc":"training_set = dataset_open[:'2016']\ntest_set = dataset_open['2017':]\nprint(len(training_set))\nprint(len(test_set))","89f5635b":"fig  = make_subplots()\nfig.add_trace(go.Scatter(x=dataset_open.index,y=dataset_open.values,name=\"Zoom\"))\nfig.update_layout(autosize=True,width=900,height=500,title_text=\"IBM\")\nfig.update_xaxes(title_text=\"year\")\nfig.update_yaxes(title_text=\"prices\")\nfig.show()","21d119fe":"fig  = make_subplots()\nfig.add_trace(go.Scatter(x=training_set.index,y=training_set.values,name=\"IBM Train\"))\nfig.add_trace(go.Scatter(x=test_set.index,y=test_set.values,name=\"IBM Test\"))\nfig.update_layout(autosize=False,width=900,height=500,title_text=\"IBM\")\nfig.update_xaxes(title_text=\"year\")\nfig.update_yaxes(title_text=\"prices\")\nfig.show()","675153a6":"scaler = MinMaxScaler(feature_range=(0,1))\nscaled_data = scaler.fit_transform(training_set.values.reshape(-1,1))\nscaled_data.shape","7f71beb5":"prediction_days = 30\n\nx_train = []\ny_train = []\n\n# Goes from 30 to 2769-5=2764 (because we need to predict price 5 days later thus why can't use days past 2764)\nfor day in range(prediction_days, len(scaled_data)-5):      \n    \n    # training data is 30 past days (if day = 50, the past days used for prediction would be DAY 19-49)\n    x_train.append(scaled_data[day-prediction_days:day, 0])\n    \n    # predict 5 days after\n    y_train.append(scaled_data[day+5, 0])                     \n\n# Turning lists into np arrays\nx_train, y_train = np.array(x_train), np.array(y_train)\n\n# Reshaping for efficient modelling to be then fed into LSTM Network \nx_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))","b98a2342":"# Validate the shape of the training data\nprint(x_train.shape)\nprint(y_train.shape)","14324972":"def LSTM_model():\n    \n    # Use Keras sequential model\n    model = Sequential()    \n    \n    # First LSTM layer with Dropout regularisation; Set return_sequences to True to feed outputs to next layer\n    model.add(LSTM(units = 50, return_sequences = True, input_shape = (x_train.shape[1],1))) \n    model.add(Dropout(0.2))\n    \n    # Second LSTM layer with Dropout regularisation; Set return_sequences to True to feed outputs to next layer\n    model.add(LSTM(units = 50, return_sequences = True))                                    \n    model.add(Dropout(0.2))\n    \n    # Final LSTM layer with Dropout regularisation; Set return_sequences to False since now we will be predicting with the output layer\n    model.add(LSTM(units = 50))\n    model.add(Dropout(0.2))\n    \n    # The output layer with linear activation to predict Open stock price\n    model.add(Dense(units=1, activation = \"linear\"))\n    \n    return model","40d17468":"model = LSTM_model()\nmodel.summary()","92492a05":"model.compile(optimizer='adam', loss='mean_squared_error', metrics = ['accuracy'])","9e6597a9":"checkpointer = ModelCheckpoint(filepath = 'best_weights.hdf5', verbose = 1, save_best_only = True)\nhis=model.fit(x_train,y_train,epochs=50,batch_size=32,callbacks=[checkpointer])","1737d2eb":"plt.plot(his.history['loss'])\nplt.plot(his.history['accuracy'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['loss','accuracy'])\nplt.show()","f0c414f3":"# Now to get the test set ready in a similar way as the training set.\n\n# This our the outputs we are going to compare our predictions to... keep them safe\nreal_prices = test_set.values\ntotal_dataset = pd.concat((training_set, test_set), axis=0)\n\n# Let's find the number of days in the training data\nlen_train_data = len(total_dataset)-len(test_set)\n\n# Here we need 30 days prior to predict the first test data. Thus why we are using days from the training set to help predict TEST DAY 1\nmodel_inputs = total_dataset[len_train_data-prediction_days:].values\nmodel_inputs = model_inputs.reshape(-1,1) # -1 here indicated that we keep the same the dimension meaning dimension (3,) becomes here (3,1)\n\n# Normalzing the model inputs as we did previously but now for the test data inputs\nmodel_inputs = scaler.transform(model_inputs)","9d16c9a9":"# Preparing x_test \nx_test = []\nfor day in range(prediction_days,len(model_inputs)):\n    x_test.append(model_inputs[day-prediction_days:day,0])\n\nx_test = np.array(x_test)\nx_test = np.reshape(x_test,(x_test.shape[0],x_test.shape[1],1))\n\n# Predicting the prices\npredicted_prices = model.predict(x_test)\npredicted_prices = scaler.inverse_transform(predicted_prices) # Have to inverse transform to \"un-normalize\" the values","2276139a":"plt.plot(real_prices, color='black', label=f\"Actual price\")\nplt.plot(predicted_prices, color= 'green', label=\"Predicted 5-days-after price\")\nplt.title(\"IBM Stock\")\nplt.xlabel(\"Days in test period\")\nplt.ylabel(\"Price\")\nplt.legend()\nplt.show()","a755deb8":"predicted_prices = predicted_prices.flatten()\npredicted_prices","2940dc15":"fig=make_subplots()\nfig.add_trace(go.Scatter(x=training_set.index,y=training_set.values,name=\"IBM Train\"))\nfig.add_trace(go.Scatter(x=test_set.index,y=test_set.values,name=\"IBM Test\"))\nfig.add_trace(go.Scatter(x=test_set.index,y=predicted_prices,name=\"Predicted 5-days after price\"))\nfig.update_layout(autosize=True,width=900,height=500,title_text=\"IBM\")\nfig.update_xaxes(title_text=\"year\")\nfig.update_yaxes(title_text=\"prices\")\nfig.show()","4dee6f03":"# Imports\nFirst let us start by importing the relevant libraries that we need.","9803ff9d":"# Splitting the data set in training and test sets\nThis is important because we will be using the training set to train the model and the test set to check if the model does well on unseen inputs.","e070e294":"Let us now visualize the loss and accuracy to see how the training went.","7f22dd3f":"# Training\/Fitting time\nWe can finally train our model with our training data. Let's see how it does.","7ac51ae6":"# Visualizing and Plotting our Data\nIt is very important to plot and visualize the data to get a good grasp of what is happening. For this, I have decided to use plotly, but you can use any other visualizaiton library such as seaborn and matplotlib. We are using plotly graph objects since we will need to create subplots later.","620cad99":"# Checking for NaN (missing values) in the dataframe\nThere is multiple ways of doing this, I have chosen do it with the following method:","eaea0887":"We prepare the test inputs `x_test` now. We do so in the same fashion as the `x_train`, however notice that we do not assign a y_test variable here since we will be predicting it and comparing it to `real_prices`","acfdb8fe":"# Final Visualization\nLet's conclude this notebook with some good ole data visualization. We'll start by flattening the 2D array into an 1D array. This is done for ease of plotting.","a37c47f6":"# Introduction \n`V1.0.1`\n### Who am I\nJust a fellow Kaggle learner. I was creating this Notebook as practice and thought it could be useful to some others \n### Who is this for\nThis Notebook is for people that learn from examples. Forget the boring lectures and follow along for some fun\/instructive time :)\n### What can I learn here\nYou learn all the basics needed to create a rudimentary RNN\/LSTM Network. I go over a multitude of steps with explanations. Hopefully with these building blocks,\nyou can go ahead and build much more complex models.\n\n### Thins to remember\n+ Please Upvote\/Like the Notebook so other people can learn from it\n+ Feel free to give any recommendations\/changes. \n+ I will be continuously updating the notebook. Look forward to many more upcoming changes in the future.","1bf3f5d0":"# Table of Contents\n1. [Imports](#Imports)\n2. [Reading and Preparing the Data](#Reading-and-Preparing-the-Data)\n3. [Checking for NaN (missing values) in the dataframe](#Checking-for-NaN-(missing-values)-in-the-dataframe)\n4. [Splitting the data set in training and test sets](#Splitting-the-data-set-in-training-and-test-sets)\n5. [Visualizing and Plotting our Data](#Visualizing-and-Plotting-our-Data)\n6. [Scaling the training data](#Scaling-the-training-data)\n7. [Training the model](#Training-the-model)\n8. [Creating the LSTM Network](#Creating-the-LSTM-Network)\n9. [Training\/Fitting time](#Training\/Fitting-time)\n10. [Testing Time](#Testing-Time)\n11. [Final Visualization](#Final-Visualization)\n12. [Final Remarks](#Final-Remarks)","affc6ee4":"# Reading and Preparing the Data\nLet's start by preparing our data. We will store it in a dataframe.","9573bcfb":"# Creating the LSTM Network\nWe are going to be creating a three layer LSTM Network with a dense layer at the end. We are using dropout as a regularisation method to combat overffiting.","f86dbcd9":"As you can see, there is not many missing values. Since we will be predicting the Open value, let's take care of the one NaN value that that column contains `Open      1`","3aa46e37":"We will use plotly suplots to plot training data, test data and predicitons on the same plot.","80a68b69":"If you zoom in on Dec 30 2016 to Jan 03 2017, you can see the discontinuity in the prediction since we are only predicting 5 days forward.","7b55cce8":"Now to plot the training and test data on the same plot.","b6974208":"# Final Remarks\nThank you\n\nMerci\n\nArigato\n\nGracias\n\nGood lucks peeps","80c6466f":"# Training the model\nThe models uses 30 past days to predict the Open price 5 days later. \n\nIf today we are DAY 50, we will use DAYS 19-49 to predict DAY 54","e568e685":"Let us now use summary method to validate our network.","4e9dcb76":"# Testing Time\nNow that we have validated that the model does pretty well on our training data, we can move to some more serious stuff... TEST DATA\nWhy is this important you might ask? Well if you score plenty of goals in your practices (good job I guess), but none on the real game... there must be something wrong.\n\nThat is why we need test data to confirm that our model does well on unseen data.","3a212049":"Now we set our compiler and our optimatization mechanism. We will be using the Adam optimazation method since it is widely used and performs much better than regular gradient descent.","0630373d":"Let us finaly visualize the data that we will be using after removing the NaN values.","5c63c786":"WOW WHAT GREAT RESULTS. Please hire me...\n\nActually whenever it is too good to be true, there is usually something wrong. \n\nIn this case, we used `prediction_days = 30`, therefore 30 days in the past to predict the first datapoint in the test dataset (let's call it #1). This is fundamentally biased since you are using training data to predict test data (it's like bringing your trainer golf club to the real game... sheesh unprofessional). This makes it very easy for the model to predict #1 since it is using 1-30 Days of past training data which it has been trained on. What happens if the 30 days past data was NOT training data... You guessed it, much worse predictions. ","935a2459":"# Scaling the training data\nHere we will use MinMaxScaler from the Sci-kit learn library to standardize and scale our input data between 0-1."}}