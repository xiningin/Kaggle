{"cell_type":{"24b919b2":"code","4f62a037":"code","87b93f49":"code","853959a8":"code","b8748c54":"code","2d1c58b6":"code","b8102213":"code","b6bf18fd":"code","42dcb583":"code","f6658edc":"code","dc3e8681":"code","7b76d5f9":"code","52052483":"code","ee482c29":"code","f35a3860":"code","4303f972":"code","74137201":"code","8be30594":"code","c1d39926":"code","9165b51c":"code","aa5ce098":"code","74488b05":"code","af25a470":"code","14a8a148":"code","140c6136":"code","6d949cce":"code","16e2b8ce":"code","a15cc2b8":"code","ba535213":"code","860070f2":"code","180bba8f":"code","9f194277":"code","bf0b2324":"code","9d4e9d79":"code","c62dd5b4":"markdown","987c7918":"markdown","bce6011c":"markdown","ef06f6b4":"markdown","ececd4e0":"markdown","d666ae3b":"markdown","a9ac2766":"markdown","b01700ca":"markdown","d4aba04d":"markdown","720a8790":"markdown","98e230b3":"markdown","66493e65":"markdown","bba868d7":"markdown","629ab43b":"markdown","a76dd4eb":"markdown","26f284c7":"markdown","8f10f7d2":"markdown","c289f7a5":"markdown","5e0e29bf":"markdown","449b69f6":"markdown","89577c9a":"markdown","f0ffcfe6":"markdown","fdc63c44":"markdown","0a9a30a4":"markdown","3a8c5d03":"markdown","2580b786":"markdown","bbdc8d1f":"markdown","ec8b1506":"markdown"},"source":{"24b919b2":"# example of binary classification task\nfrom numpy import where\nfrom collections import Counter\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot\n# define dataset\n#n_features=3 by default the feature =2 \nX, y = make_blobs(n_samples=1000, centers=2, random_state=1)\n# summarize dataset shape\n#print(X.shape, y.shape)\n# summarize observations by class label\ncounter = Counter(y)\n#print(counter)\n# summarize first few examples\nfor i in range(10):\n\tprint(X[i], y[i])\n# plot the dataset and color the by class label\nfor label, _ in counter.items():\n\trow_ix = where(y == label)[0]\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\npyplot.legend()\npyplot.show()","4f62a037":"# example of binary classification task\nfrom numpy import where\nfrom collections import Counter\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot\n# define dataset\n#n_features=3 by default the feature =2 \nX, y = make_blobs(n_samples=1000, centers=3, random_state=1)\n# summarize dataset shape\n#print(X.shape, y.shape)\n# summarize observations by class label\ncounter = Counter(y)\n#print(counter)\n# summarize first few examples\nfor i in range(10):\n\tprint(X[i], y[i])\n# plot the dataset and color the by class label\nfor label, _ in counter.items():\n\trow_ix = where(y == label)[0]\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\npyplot.legend()\npyplot.show()","87b93f49":"# example of a multi-label classification task\nfrom sklearn.datasets import make_multilabel_classification\n# define dataset\nX, y = make_multilabel_classification(n_samples=1000, n_features=2, n_classes=3, n_labels=2, random_state=1)\n# summarize dataset shape\nprint(X.shape, y.shape)\n# summarize first few examples\nfor i in range(10):\n\tprint(X[i], y[i])","853959a8":"# logistic regression for multi-class classification using a one-vs-rest\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=1)\n# define model\nmodel = LogisticRegression()\n# define the ovr strategy\novr = OneVsRestClassifier(model)\n# fit model\novr.fit(X, y)\n# make predictions\nyhat = ovr.predict(X)","b8748c54":"pip install imbalanced-learn\n","2d1c58b6":"import imblearn\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler","b8102213":"# define oversampling strategy\noversample = RandomOverSampler(sampling_strategy='minority')","b6bf18fd":"# define oversampling strategy\noversample = RandomOverSampler(sampling_strategy=0.5)","42dcb583":"# define dataset\nX, y = make_classification(n_samples=10000, weights=[0.99], flip_y=0)\n# summarize class distribution\nprint(Counter(y))\n# define oversampling strategy\noversample = RandomOverSampler(sampling_strategy='minority')\n# fit and apply the transform\nX_over, y_over = oversample.fit_resample(X, y)\n# summarize class distribution\nprint(Counter(y_over))","f6658edc":"# define dataset\nX, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n\tn_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)","dc3e8681":"# summarize class distribution\ncounter = Counter(y)\nprint(counter)","7b76d5f9":"# scatter plot of examples by class label\nfor label, _ in counter.items():\n\trow_ix = where(y == label)[0]\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\npyplot.legend()\npyplot.show()","52052483":"from imblearn.over_sampling import SMOTE\n# transform the dataset\noversample = SMOTE()\nX, y = oversample.fit_resample(X, y)\n\n# summarize the new class distribution\ncounter = Counter(y)\nprint(counter)","ee482c29":"# Oversample and plot imbalanced dataset with SMOTE\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nfrom imblearn.over_sampling import SMOTE\nfrom matplotlib import pyplot\nfrom numpy import where\n# define dataset\nX, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n\tn_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)\n# summarize class distribution\ncounter = Counter(y)\nprint(counter)\n# transform the dataset\noversample = SMOTE()\nX, y = oversample.fit_resample(X, y)\n# summarize the new class distribution\ncounter = Counter(y)\nprint(counter)\n# scatter plot of examples by class label\nfor label, _ in counter.items():\n\trow_ix = where(y == label)[0]\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\npyplot.legend()\npyplot.show()","f35a3860":"over = SMOTE(sampling_strategy=0.1)\nunder = RandomUnderSampler(sampling_strategy=0.5)","4303f972":"# Oversample with SMOTE and random undersample for imbalanced dataset\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\nfrom matplotlib import pyplot\nfrom numpy import where\n# define dataset\nX, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n\tn_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=1)\n# summarize class distribution\ncounter = Counter(y)\nprint(counter)\n# define pipeline\nover = SMOTE(sampling_strategy=0.1)\nunder = RandomUnderSampler(sampling_strategy=0.5)\nsteps = [('o', over), ('u', under)]\npipeline = Pipeline(steps=steps)\n# transform the dataset\nX, y = pipeline.fit_resample(X, y)\n# summarize the new class distribution\ncounter = Counter(y)\nprint(counter)\n# scatter plot of examples by class label\nfor label, _ in counter.items():\n\trow_ix = where(y == label)[0]\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\npyplot.legend()\npyplot.show()","74137201":"print(Counter(y))\nprint(Counter(y_over))","8be30594":"# example of an imbalanced binary classification task\nfrom numpy import where\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nfrom matplotlib import pyplot\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_classes=2, n_clusters_per_class=1, weights=[0.99,0.01], random_state=1)\n# summarize dataset shape\nprint(X.shape, y.shape)\n# summarize observations by class label\ncounter = Counter(y)\nprint(counter)\n# summarize first few examples\nfor i in range(10):\n\tprint(X[i], y[i])\n# plot the dataset and color the by class label\nfor label, _ in counter.items():\n\trow_ix = where(y == label)[0]\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\npyplot.legend()\npyplot.show()","c1d39926":"import pandas as pd\n\n# Import the data using the file path\ndata = pd.read_csv('..\/input\/orangetelecomchurn\/Orange_Telecom_Churn_Data.csv') ","9165b51c":"data.head(1).T","aa5ce098":"# Remove extraneous columns\ndata.drop(['state', 'area_code', 'phone_number'], axis=1, inplace=True)","74488b05":"data.columns","af25a470":"from sklearn.preprocessing import LabelBinarizer\n\nlb = LabelBinarizer()\n\nfor col in ['intl_plan', 'voice_mail_plan', 'churned']:\n    data[col] = lb.fit_transform(data[col])","14a8a148":"# Mute the sklearn warning\nimport warnings\nwarnings.filterwarnings('ignore', module='sklearn')\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nmsc = MinMaxScaler()\n\ndata = pd.DataFrame(msc.fit_transform(data),  # this is an np.array, not a dataframe.\n                    columns=data.columns)","140c6136":"# Get a list of all the columns that don't contain the label\nx_cols = [x for x in data.columns if x != 'churned']\n\n# Split the data into two dataframes\nX_data = data[x_cols]\ny_data = data['churned']\n\n# # alternatively:\n# X_data = data.copy()\n# y_data = X_data.pop('churned')","6d949cce":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=3)\n\nknn = knn.fit(X_data, y_data)\n\ny_pred = knn.predict(X_data)","16e2b8ce":"# Function to calculate the % of values that were correctly predicted\n\ndef accuracy(real, predict):\n    return sum(y_data == y_pred) \/ float(real.shape[0])","a15cc2b8":"print(accuracy(y_data, y_pred))","ba535213":"knn = KNeighborsClassifier(n_neighbors=5, weights='distance')\n\nknn = knn.fit(X_data, y_data)\n\ny_pred = knn.predict(X_data)\n\nprint(accuracy(y_data, y_pred))","860070f2":"knn = KNeighborsClassifier(n_neighbors=5, p=1)\n\nknn = knn.fit(X_data, y_data)\n\ny_pred = knn.predict(X_data)\n\nprint(accuracy(y_data, y_pred))","180bba8f":"# Fit the K-nearest neighbors model with different values of k\n# Store the accuracy measurement for each k\n\nscore_list = list()\n\nfor k in range(1, 21):\n    \n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn = knn.fit(X_data, y_data)\n    \n    y_pred = knn.predict(X_data)\n    score = accuracy(y_data, y_pred)\n    \n    score_list.append((k, score))\n    \nscore_df = pd.DataFrame(score_list, columns=['k', 'accuracy'])","9f194277":"    score_df","bf0b2324":"# Import libraries to make the plot\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","9d4e9d79":"sns.set_context('talk')\nsns.set_style('ticks')\nsns.set_palette('dark')\n\nax = score_df.set_index('k').plot()\n\nax.set(xlabel='k', ylabel='accuracy')\nax.set_xticks(range(1, 21));","c62dd5b4":"A scatter plot of the transformed dataset can also be created and we would expect to see many more examples for the minority class on lines between the original examples in the minority class.\n\nTying this together, the complete examples of applying SMOTE to the synthetic dataset and then summarizing and plotting the transformed result is listed below.","987c7918":"<h3> Link to study the theory for MTT2 and TEE<\/h3>\n[make_classification  ](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.make_classification.html)\nLets use the make_classification()scikit-learn function to create a synthetic binary classification dataset with 10,000 examples and a 1:100 class distribution.","bce6011c":"This would ensure that the minority class was oversampled to have half the number of examples as the majority class, for binary classification problems. This means that if the majority class had 1,000 examples and the minority class had 100, the transformed dataset would have 500 examples of the minority class","ef06f6b4":"## Introduction\n\nWe will be using customer churn data from the telecom industry for the first week's exercises. The data file is called \n`Orange_Telecom_Churn_Data.csv`. We will load this data together, do some preprocessing, and use K-nearest neighbors to predict customer churn based on account characteristics.","ececd4e0":"[KNeighborsClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html)","d666ae3b":"The class can be defined and takes a sampling_strategy argument that can be set to \u201cminority\u201d to automatically balance the minority class with majority class or classes.","a9ac2766":"<h1> SMOTE for Imbalanced Classification with Python <\/h1>\n\nImbalanced classification involves developing predictive models on classification datasets that have a severe class imbalance.\n\nThe challenge of working with imbalanced datasets is that most machine learning techniques will ignore, and in turn have poor performance on, the minority class, although typically it is performance on the minority class that is most important.\n\nOne approach to addressing imbalanced datasets is to oversample the minority class. The simplest approach involves duplicating examples in the minority class, although these examples don\u2019t add any new information to the model. Instead, new examples can be synthesized from the existing examples. This is a type of data augmentation for the minority class and is referred to as the Synthetic Minority Oversampling Technique, or SMOTE for short.\n\nIn this tutorial, you will discover the SMOTE for oversampling imbalanced classification datasets.\n\nAfter completing this tutorial, you will know:\n\nHow the SMOTE synthesizes new examples for the minority class.\nHow to correctly fit and evaluate machine learning models on SMOTE-transformed training datasets.\nHow to use extensions of the SMOTE that generate synthetic examples along the class decision boundary.\nKick-start your project with my new book Imbalanced Classification with Python, including step-by-step tutorials and the Python source code files for all examples.\n\nLet\u2019s get started.","b01700ca":"## Question 5\n\n* Fit the K-nearest neighbors model again with `n_neighbors=3` but this time use distance for the weights. Calculate the accuracy using the function you created above. \n* Fit another K-nearest neighbors model. This time use uniform weights but set the power parameter for the Minkowski distance metric to be 1 (`p=1`) i.e. Manhattan Distance.\n\nWhen weighted distances are used for part 1 of this question, a value of 1.0 should be returned for the accuracy. Why do you think this is? *Hint:* we are predicting on the data and with KNN the model *is* the data. We will learn how to avoid this pitfall in the next lecture.","d4aba04d":"<h1>**Multi-Label Classification** <\/h1>","720a8790":"## Question 4\n\nWays to measure error haven't been discussed in class yet, but accuracy is an easy one to understand--it is simply the percent of labels that were correctly predicted (either true or false). \n\n* Write a function to calculate accuracy using the actual and predicted labels.\n* Using the function, calculate the accuracy of this K-nearest neighbors model on the data.","98e230b3":"Popular algorithms that can be used for multi-class classification include:\n\n* k-Nearest Neighbors.\n* Decision Trees.\n* Naive Bayes.\n* Random Forest.\n* Gradient Boosting.","66493e65":"Logistic Regression and Support Vector Machines algorithms are specifically designed for binary classification and do not natively support more than two classes.","bba868d7":"## Question 2\n\n* Notice that some of the columns are categorical data and some are floats. These features will need to be numerically encoded using one of the methods from the lecture.\n* Finally, remember from the lecture that K-nearest neighbors requires scaled data. Scale the data using one of the scaling methods discussed in the lecture.","629ab43b":"We would expect some SMOTE oversampling of the minority class, although not as much as before where the dataset was balanced. We also expect fewer examples in the majority class via random undersampling.\n\nTying this all together, the complete example is listed below","a76dd4eb":"We can update the example to first oversample the minority class to have 10 percent the number of examples of the majority class (e.g. about 1,000), then use random undersampling to reduce the number of examples in the majority class to have 50 percent more than the minority class (e.g. about 2,000).\n\nTo implement this, we can specify the desired ratios as arguments to the SMOTE and RandomUnderSampler classes; for example:","26f284c7":"Multi-label classification refers to those classification tasks that have two or more class labels, where one or more class labels may be predicted for each example.\n\nConsider the example of **photo classification**, where a given photo may have multiple objects in the scene and a model may predict the presence of multiple known objects in the photo, such as \u201cbicycle,\u201d \u201capple,\u201d \u201cperson,\u201d etc.\n\nThis is unlike binary classification and multi-class classification, where a single class label is predicted for each example.\n\nIt is common to model multi-label classification tasks with a model that predicts multiple outputs, with each output taking predicted as a Bernoulli probability distribution. This is essentially a model that makes multiple binary classification predictions for each example.\n\nClassification algorithms used for binary or multi-class classification cannot be used directly for multi-label classification. Specialized versions of standard classification algorithms can be used, so-called multi-label versions of the algorithms, including:\n\n* Multi-label Decision Trees\n* Multi-label Random Forests\n* Multi-label Gradient Boosting\nAnother approach is to use a separate classification algorithm to predict the labels for each class.\n\nNext, let\u2019s take a closer look at a dataset to develop an intuition for multi-label classification problems.\n\nWe can use the make_multilabel_classification() function to generate a synthetic multi-label classification dataset.\n\nThe example below generates a dataset with 1,000 examples, each with two input features. There are three classes, each of which may take on one of two labels (0 or 1).","8f10f7d2":"<h1> Multiclass Classification <\/h1>\n**Link to study the theory for MTT2 and TEE**\n\n[Multiclass Classification](https:\/\/scikit-learn.org\/stable\/modules\/multiclass.html)","c289f7a5":"## Question 1\n\n* Begin by importing the data. Examine the columns and data.\n* Notice that the data contains a state, area code, and phone number. Do you think these are good features to use when building a machine learning model? Why or why not? \n\nWe will not be using them, so they can be dropped from the data.","5e0e29bf":"we can oversample the minority class using SMOTE and plot the transformed dataset.\n\nWe can use the SMOTE implementation provided by the imbalanced-learn Python library in the SMOTE class.\n\nThe SMOTE class acts like a data transform object from scikit-learn in that it must be defined and configured, fit on a dataset, then applied to create a new transformed version of the dataset.\n\nFor example, we can define a SMOTE instance with default parameters that will balance the minority class and then fit and apply it in one step to create a transformed version of our dataset.","449b69f6":"<h1>Imbalanced Classification<\/h1>\n","89577c9a":"## Question 3\n\n* Separate the feature columns (everything except `churned`) from the label (`churned`). This will create two tables.\n* Fit a K-nearest neighbors model with a value of `k=3` to this data and predict the outcome on the same data.","f0ffcfe6":"**Multi-Class Classification**\nMulti-class classification refers to those classification tasks that have more than two class labels.\n\nExamples include:\n\n* Face classification.\n* Plant species classification.\n* Optical character recognition.\n\nUnlike binary classification, multi-class classification does not have the notion of normal and abnormal outcomes. Instead, examples are classified as belonging to one among a range of known classes.\n\nThe number of class labels may be very large on some problems. For example, a model may predict a photo as belonging to one among thousands or tens of thousands of faces in a face recognition system.\n\nProblems that involve predicting a sequence of words, such as text translation models, may also be considered a special type of multi-class classification. Each word in the sequence of words to be predicted involves a multi-class classification where the size of the vocabulary defines the number of possible classes that may be predicted and could be tens or hundreds of thousands of words in size.\n\nIt is common to model a multi-class classification task with a model that predicts a Multinoulli probability distribution for each example.\n\nThe Multinoulli distribution is a discrete probability distribution that covers a case where an event will have a categorical outcome, e.g. K in {1, 2, 3, \u2026, K}. For classification, this means that the model predicts the probability of an example belonging to each class label.\n\nMany algorithms used for binary classification can be used for multi-class classification.\n\nPopular algorithms that can be used for multi-class classification include:\n\n* k-Nearest Neighbors.\n* Decision Trees.\n* Naive Bayes.\n* Random Forest.\n* Gradient Boosting.\nAlgorithms that are designed for binary classification can be adapted for use for multi-class problems.\n\nThis involves using a strategy of fitting multiple binary classification models for each class vs. all other classes (called one-vs-rest) or one model for each pair of classes (called one-vs-one).\n\n**One-vs-Rest:** Fit one binary classification model for each class vs. all other classes.\n**One-vs-One: ** Fit one binary classification model for each pair of classes.\nBinary classification algorithms that can use these strategies for multi-class classification include:\n\n* Logistic Regression.\n* Support Vector Machine.\nNext, let\u2019s take a closer look at a dataset to develop an intuition for multi-class classification problems.\n\nWe can use the make_blobs() function to generate a synthetic multi-class classification dataset.\n\nThe example below generates a dataset with 1,000 examples that belong to one of three classes, each with two input features.","fdc63c44":"# Supervised Learning and K Nearest Neighbors Exercises","0a9a30a4":"**Imbalanced classification** refers to classification tasks where the number of examples in each class is unequally distributed.\n\nTypically, imbalanced classification **(An imbalanced classification problem is an example of a classification problem where the distribution of examples across the known classes is biased or skewed)** tasks are binary classification tasks where the majority of examples in the training dataset belong to the normal class and a minority of examples belong to the abnormal class.\n\nExamples include:\n\n* Fraud detection.\n* Outlier detection.\n* Medical diagnostic tests.\n\n**Link to study the theory for MTT2 and TEE** [Imbalanced classification](https:\/\/machinelearningmastery.com\/imbalanced-classification-with-python\/)\n\nThese problems are modeled as binary classification tasks, although may require specialized techniques.\n\nSpecialized techniques may be used to change the composition of samples in the training dataset by undersampling the majority class or oversampling the minority class.\n\nExamples include:\n\n* Random Undersampling.\n* SMOTE Oversampling. (Synthetic Minority Oversampling Technique)\nSpecialized modeling algorithms may be used that pay more attention to the minority class when fitting the model on the training dataset, such as cost-sensitive machine learning algorithms.\n\nExamples include:\n\n* Cost-sensitive Logistic Regression.\n* Cost-sensitive Decision Trees.\n* Cost-sensitive Support Vector Machines.\nFinally, alternative performance metrics may be required as reporting the classification accuracy may be misleading.\n\nExamples include:\n\n* Precision.\n* Recall.\n* F-Measure.\nNext, let\u2019s take a closer look at a dataset to develop an intuition for imbalanced classification problems.\n\nWe can use the make_classification() function to generate a synthetic imbalanced binary classification dataset.\n\nThe example below generates a dataset with 1,000 examples that belong to one of two classes, each with two input features.","3a8c5d03":"## Question 6\n\n* Fit a K-nearest neighbors model using values of `k` (`n_neighbors`) ranging from 1 to 20. Use uniform weights (the default). The coefficient for the Minkowski distance (`p`) can be set to either 1 or 2--just be consistent. Store the accuracy and the value of `k` used from each of these fits in a list or dictionary.\n* Plot (or view the table of) the `accuracy` vs `k`. What do you notice happens when `k=1`? Why do you think this is? *Hint:* it's for the same reason discussed above.","2580b786":"Next, the dataset is transformed, first by oversampling the minority class, then undersampling the majority class. The final class distribution after this sequence of transforms matches our expectations with a 1:2 ratio or about 2,000 examples in the majority class and about 1,000 examples in the minority class.","bbdc8d1f":"Link to study [make_blobs](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.make_blobs.html), to generate a synthetic binary classification dataset.","ec8b1506":"Popular algorithms that can be used for binary classification include:\n\n* Logistic Regression\n* k-Nearest Neighbors\n* Decision Trees\n* Support Vector Machine\n* Naive Bayes"}}