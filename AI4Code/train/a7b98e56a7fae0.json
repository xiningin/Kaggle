{"cell_type":{"e534a61d":"code","bebeb0aa":"code","f03132a7":"code","6065bba4":"code","4e4c0914":"code","eeece371":"code","8eec7fa7":"code","2bf174b1":"code","2d818da2":"markdown","69f9dd9b":"markdown","02c74b73":"markdown"},"source":{"e534a61d":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt","bebeb0aa":"FEATURES = ['f190486d6', '58e2e02e6', 'eeb9cd3aa', '9fd594eec', '6eef030c1', \n            '15ace8c9f', 'fb0f5dbfe', '58e056e12', '20aa07010', '024c577b9', \n            'd6bb78916', 'b43a7cfd5', '58232a6fb', '1702b5bf0', '324921c7b', \n            '62e59a501', '2ec5b290f', '241f0f867', 'fb49e4212', '66ace2992', \n            'f74e8f13d', '5c6487af1', '963a49cdc', '26fc93eb7', '1931ccfdd', \n            '703885424', '70feb1494', '491b9ee45', '23310aa6f', 'e176a204a', \n            '6619d81fc', '1db387535', 'fc99f9426', '91f701ba2', '0572565c2', \n            '190db8488', 'adb64ff71', 'c47340d97', 'c5a231d81', '0ff32eb98']\n    \ndef get_pred(data, lag=2):\n    d1 = data[FEATURES[:-lag]].apply(tuple, axis=1).to_frame().rename(columns={0: 'key'})\n    d2 = data[FEATURES[lag:]].apply(tuple, axis=1).to_frame().rename(columns={0: 'key'})\n    d2['pred'] = data[FEATURES[lag - 2]]\n    d3 = d2[~d2.duplicated(['key'], keep=False)]\n    return d1.merge(d3, how='left', on='key').pred.fillna(0)","f03132a7":"def get_all_pred(data, max_lag):\n    target = pd.Series(index=data.index, data=np.zeros(data.shape[0]))\n    for lag in range(2, max_lag + 1):\n        pred = get_pred(data, lag)\n        mask = (target == 0) & (pred != 0)\n        target[mask] = pred[mask]\n    return target","6065bba4":"train = pd.read_csv('..\/input\/train.csv')","4e4c0914":"for max_lag in range(2, 33):\n    pred_train = get_all_pred(train, max_lag)\n    have_data = pred_train != 0\n    print(f'Max lag {max_lag}: Score = {sqrt(mean_squared_error(np.log1p(train.target[have_data]), np.log1p(pred_train[have_data])))} on {have_data.sum()} out of {train.shape[0]} training samples')","eeece371":"test = pd.read_csv('..\/input\/test.csv')","8eec7fa7":"pred_test = get_all_pred(test, 25)\nhave_data = pred_test != 0\nprint(f'Have predictions for {have_data.sum()} out of {test.shape[0]} test samples')","2bf174b1":"# TODO: USE YOUR OLD SUBMISSION!!!\nsub = pd.read_csv('..\/input\/sample_submission.csv')\nsub.loc[have_data, 'target'] = pred_test[have_data]\nsub.to_csv(f'new_submission.csv', index=False)","2d818da2":"Inspired by:\n* https:\/\/www.kaggle.com\/titericz\/the-property-by-giba\n* https:\/\/www.kaggle.com\/johnfarrell\/giba-s-property-extended-extended-result\n* https:\/\/www.kaggle.com\/johnfarrell\/breaking-lb-fresh-start-with-lag-selection\n* https:\/\/www.kaggle.com\/tezdhar\/breaking-lb-fresh-start\n* https:\/\/www.kaggle.com\/dfrumkin\/a-simple-way-to-use-giba-s-features (my own)\n\nThis kernel is **probably worse** than those presented by the (grand)masters.\nMy main goal was to have simple code that **runs fast**.   It easily achieves 0.69 when combined with my old submission.\n\nOf course, much deeper analysis is needed to achieve better results.\nHere, we are taking advantage of newer records containing the \"target\" value for older (historical) records.\nBut, of course, not all records are \"historic\", especially in the test set, and even for them we need to work harder.\n\n**UPDATE**: With Jiazhen's fix and increasing the max lag to 25, it easily gets me the score of 0.64.  Of course, this is not real ML and may not be reflected in the private LB.  The point is speed.\n","69f9dd9b":"# Training Set Analysis (can be skipped)","02c74b73":"# Test Set Predictions"}}