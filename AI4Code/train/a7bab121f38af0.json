{"cell_type":{"fa80b212":"code","9b446fd1":"code","468048a8":"code","826389f7":"code","2fefe1c1":"code","e192b2bc":"code","0ef081b7":"code","9139089c":"code","4a54cd0c":"code","a87a59f4":"code","30015a76":"code","3ff6d9b2":"code","f8ac8518":"code","d09b0180":"code","05e6b3d4":"code","1d6a50fc":"code","fc142cc3":"markdown","0d19516a":"markdown","f8735b4b":"markdown","b2488210":"markdown","bdf45bfb":"markdown","3bd41cb4":"markdown","be6a2389":"markdown","8240f264":"markdown","aec7ce34":"markdown","94636866":"markdown","19b4017c":"markdown","4122ffec":"markdown","55f22708":"markdown","81c413ba":"markdown","bda0e4d7":"markdown","cfa70b72":"markdown","e5d2d09e":"markdown"},"source":{"fa80b212":"# General libraries for scientific purposes\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\nimport matplotlib\nimport sklearn\nimport os\nimport sys\n\n# Preprocessing\nfrom sklearn.preprocessing import LabelEncoder\n\n# Plotting and visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Data models\nfrom sklearn.linear_model import LinearRegression, LogisticRegressionCV, Perceptron\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\nfrom sklearn.model_selection import cross_validate, ShuffleSplit, GridSearchCV\nfrom xgboost import XGBClassifier\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Libraries versions\nprint('Python: {}'.format(sys.version))\nprint('numpy: {}'.format(np.__version__))\nprint('pandas: {}'.format(pd.__version__))\nprint('scipy: {}'.format(sp.__version__))\nprint('matplotlib: {}'.format(matplotlib.__version__))\nprint('sklearn: {}'.format(sklearn.__version__))\nprint('-'*30)\n\n# Print local folder content\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","9b446fd1":"# Importing local datasets\ntrain = pd.read_csv('..\/input\/train.csv')\ntest  = pd.read_csv('..\/input\/test.csv')\n\n# Reference of both train and test, for cleaning purposes\ndatasets = [train, test]","468048a8":"train.head()","826389f7":"train.info()\nprint('-'*30)\ntest.info()\ntrain.describe(include = 'all')","2fefe1c1":"print('Null training values:\\n', train.isnull().sum())\nprint(\"-\"*30)\nprint('Test\/Validation columns with null values:\\n', test.isnull().sum())","e192b2bc":"drop_column = ['Cabin', 'Ticket']\n\nfor d in datasets:    \n    d['Age'].fillna(d['Age'].median(), inplace = True)\n    d['Fare'].fillna(d['Fare'].median(), inplace = True)\n    d['Embarked'].fillna(d['Embarked'].mode()[0], inplace = True)\n    d.drop(drop_column, axis=1, inplace = True)    \n\nprint(train.isnull().sum())\nprint(\"-\"*30)\nprint(test.isnull().sum())","0ef081b7":"for d in datasets:\n    d['FamilySize'] = d['SibSp'] + d['Parch'] + 1\n\n    d['IsAlone'] = 1\n    d['IsAlone'].loc[d['FamilySize'] > 1] = 0\n    \n    # Extract titles from names\n    d['Title'] = d['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n    \n    # Group uncommon titles under \"other\" label\n    d['Title'] = d['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Other')\n    \n    # Group synonyms together \n    d['Title'] = d['Title'].replace('Mlle', 'Miss')\n    d['Title'] = d['Title'].replace('Ms', 'Miss')\n    d['Title'] = d['Title'].replace('Mme', 'Mrs')\n\n    # Grouping continuous values into categories\n    d['FareBand'] = pd.qcut(d['Fare'], 4)\n    d['AgeBand'] = pd.cut(d['Age'].astype(int), 5)\n\ntrain.info()\ntest.info()\ntrain.head()","9139089c":"le = LabelEncoder()\n\ntitanic_train = train.copy(deep = 'True')\ntitanic_test = test.copy(deep = 'True')\ntitanic_train['Age'] = titanic_train['AgeBand']\ntitanic_train['Fare'] = titanic_train['FareBand']\ntitanic_test['Age'] = titanic_test['AgeBand']\ntitanic_test['Fare'] = titanic_test['FareBand']\n\ncolumn_transformed = ['Sex', 'Embarked', 'Title', 'AgeBand', 'FareBand']\ncolumn_transform = ['Sex', 'Embarked', 'Title', 'Age', 'Fare']\n\nfor d in datasets:\n    for i in range(len(column_transform)):\n        d[column_transform[i]] = le.fit_transform(d[column_transformed[i]])\n\ndatasets.append(titanic_train)\ndatasets.append(titanic_test)","4a54cd0c":"drop_column = ['Name', 'SibSp', 'Parch', 'FareBand', 'AgeBand']\nfor d in datasets:\n        d.drop(drop_column, axis=1, inplace = True)\ntrain.drop('PassengerId', axis=1, inplace = True)\ntitanic_train.drop('PassengerId', axis=1, inplace = True)","a87a59f4":"train.head()","30015a76":"titanic_train.head()","3ff6d9b2":"feature_names = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'FamilySize', 'IsAlone', 'Title']\ncorr_matrix = train.corr()\n\nprint(corr_matrix[\"Survived\"].sort_values(ascending=False))\nprint(\"-\"*30)\n\nfor feature in feature_names:\n    print('Correlation between Survived and', feature)\n    print(titanic_train[[feature, 'Survived']].groupby([feature], as_index=False).mean())\n    print(\"-\"*30)","f8ac8518":"X_train = train.drop(\"Survived\", axis=1)\nY_train = train[\"Survived\"]\nX_test  = test.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","d09b0180":"# The set of models I am going to compare\nmodels = [\n            LinearRegression(),\n            LogisticRegressionCV(),\n            Perceptron(),\n            GaussianNB(),\n            KNeighborsClassifier(),\n            SVC(probability=True),\n            DecisionTreeClassifier(),\n            AdaBoostClassifier(),\n            RandomForestClassifier(),\n            XGBClassifier()    \n        ]\n\n# Create a table of comparison for models\nmodels_columns = ['Name', 'Parameters','Train Accuracy', 'Validation Accuracy', 'Execution Time']\nmodels_df = pd.DataFrame(columns = models_columns)\npredictions = pd.DataFrame(columns = ['Survived'])\n\ncv_split = ShuffleSplit(n_splits = 10, test_size = .2, train_size = .8, random_state = 0 )\n\nindex = 0\nfor model in models:\n    models_df.loc[index, 'Name'] = model.__class__.__name__\n    models_df.loc[index, 'Parameters'] = str(model.get_params())\n    \n    scores = cross_validate(model, X_train, Y_train, cv= cv_split)\n\n    models_df.loc[index, 'Execution Time'] = scores['fit_time'].mean()\n    models_df.loc[index, 'Train Accuracy'] = scores['train_score'].mean()\n    models_df.loc[index, 'Validation Accuracy'] = scores['test_score'].mean()   \n    \n    index += 1\n\nmodels_df.sort_values(by = ['Validation Accuracy'], ascending = False, inplace = True)\nmodels_df","05e6b3d4":"param_grid = {\n              'criterion': ['gini', 'entropy'],\n              'max_depth': [2,4,6,8,10,None],\n              'random_state': [0]\n             }\n\ntree = DecisionTreeClassifier(random_state = 0)\nscore = cross_validate(tree, X_train, Y_train, cv  = cv_split)\ntree.fit(X_train, Y_train)\n\ngrid_search = GridSearchCV(DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\ngrid_search.fit(X_train, Y_train)\n\nprint('Before GridSearch:')\nprint('Parameters:', tree.get_params())\nprint(\"Training score:\", score['train_score'].mean()) \nprint(\"Validation score\", score['test_score'].mean())\nprint('-'*30)\nprint('After GridSearch:')\nprint('Parameters:', grid_search.best_params_)\nprint(\"Training score:\", grid_search.cv_results_['mean_train_score'][grid_search.best_index_]) \nprint(\"Validation score\", grid_search.cv_results_['mean_test_score'][grid_search.best_index_])","1d6a50fc":"final_tree = GridSearchCV(DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\nfinal_tree.fit(X_train, Y_train)\ntest['Survived'] = final_tree.predict(X_test)\n\nsubmission = test[['PassengerId','Survived']]\nsubmission.to_csv(\"submission.csv\", index=False)","fc142cc3":"## 6 - Parameter Tuning <a name=\"tune\"><\/a>\n\n### Grid Search Tuning","0d19516a":"### First observations\n\nI use the functions `head`, `info` and `describe` to acquire some basic useful information about the training set.\n","f8735b4b":"We are interested in particular in the null values present inside both the train and the test dataset, since we will have to adjust them in order to perform our analysis.","b2488210":"## 7 - Final Results <a name=\"end\"><\/a>\n\nThe final achieved result using a `DecisionTreeClassifier` is 79.425 % of accuracy on the test set.","bdf45bfb":"### Load data\n\nBoth the datasets are found inside the **input** folder, accessible as it follows.\n\nI create a copy of the training set in order to set up a data cleaning pipeline and try different transformations.","3bd41cb4":"We can recap the data dictionary by adding this new information:\n\n| Type    | Variable | Definition                                  | Key                                            | # train values | # train null values | # test values | # test null values |\n|---------|----------|---------------------------------------------|------------------------------------------------|----------------|---------------------|---------------|--------------------|\n| Categorical   | survived | Survival                                    | 0 = No, 1 = Yes                                | 891            | 0                   | 418           | 0                  |\n| Ordinal   | pclass   | Ticket class                                | 1 = 1st, 2 = 2nd, 3 = 3rd                      | 891            | 0                   | 418           | 0                  |\n| String  | name     | Name of the passenger                       |                                                | 891            | 0                   | 418           | 0                  |\n| Categorical  | sex      | Sex                                         |                                                | 891            | 0                   | 418           | 0                  |\n| Continuous | age      | Age in years                                |                                                | 712            | 177                 | 332           | 86                 |\n| Categorical  | sibsp    | # of siblings \/ spouses aboard the  Titanic |                                                | 891            | 0                   | 418           | 0                  |\n| Categorical  | parch    | # of parents \/ children aboard the  Titanic |                                                | 891            | 0                   | 418           | 0                  |\n| String  | ticket   | Ticket number                               |                                                | 891            | 0                   | 418           | 0                  |\n| Continuous | fare     | Passenger fare                              |                                                | 891            | 0                   | 417           | 1                  |\n| String  | cabin    | Cabin number                                |                                                | 204            | 687                 | 91            | 327                |\n| Categorical  | embarked | Port of embarkation                         | C = Cherbourg, Q = Queenstown, S = Southampton | 889            | 2                   | 418           | 0                  |\n\n#### Variable notes:\n\n\n* **pclass:** A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\n* **age:** Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n* **sibsp:** The dataset defines family relations in this way...\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n* **parch:** The dataset defines family relations in this way...\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.","be6a2389":"## 3 - Data exploration <a name=\"explore\"><\/a>\n\n### Looking for correlations\n\nWe'll use the `corr` method to compute the standard correlation coefficient between the numerical features and the survival of our passengers.\n\nFor categorical values, we'll simply observe the survival rate compared to each category.","8240f264":"# Titanic Competition - An Introduction to Data Analytics\n## By Gabriele Sarti\n\n* URL of the Kaggle competition: [https:\/\/www.kaggle.com\/c\/titanic](https:\/\/www.kaggle.com\/c\/titanic)\n\n* Find it here on Github: **_TODO_**\n\n## Table of Contents\n\n0. [Defining the problem](#problem)\n1.  [Preliminary steps](#first-steps)\n2. [Data cleaning](#clean)\n3. [Data exploration](#explore)\n4. [Data visualization](#viz)\n5. [Data modeling](#model)\n6. [Parameter tuning](#tune)\n7. [Final results](#end)","aec7ce34":"## 2 - Data Cleaning <a name=\"clean\"><\/a>\n\n### Removing null values\n\nThe approach I choose for dealing with those variables is the following:\n* Since age and fare are a float values, I will fill null entries with the respective medians across the dataset.\n* Since embarked is categorical, I will fill null embarked entries with the mode of the variable across the dataset.\n* Since cabin is not relevant for my analysis, I will drop it along with `PassengerId` and `Ticket` to reduce the noise in my training set.","94636866":"### Converting formats\n\nWe can now convert all the categorical literal values in numerical values thanks to the `LabelEncoder` class.\n\nWe create two clones before applying the transformation for plotting and understanding purposes.","19b4017c":"### Some considerations from preliminary phase\n\nSince some of the models I will benchmark for this competition require the entries to be non-null, I must take care of null values. \n\nThe most prominent categories having null values are `age` and `cabin`, with a consistent number of null values, followed by `embarked` (only in the train set) and `fare` (only in the test set), having just a couple of nulls.\n\n","4122ffec":"## 1 - Preliminary steps<a name=\"first-steps\"><\/a>\n\n### Load data analysis libraries\n\n\nI will use the standard tools for performing data analysis in Python, namely **numpy** for linear algebra and arrays,  **pandas** for managing the dataset and I\/O data operations, **scipy** for advanced mathematics operations and **matplotlib** for plotting the results and gaining insights.\n\nI will also use **sklearn** for data cleaning and data modeling, along with **seaborn** for additional visualization purposes.","55f22708":"### Experimenting with feature combinations\n\nBy observing the data and other people kernels, I got some ideas for performing feature combination:\n\n* Merging `Sibsp` and `Parch` in a single numerical discrete attribute, called `FamilySize`.\n* Creating a `IsAlone` categorical attribute that is true when `FamilySize = 0`.\n* Creating a `Title` categorical attribute, with titles extracted from passengers' names.\n* Converting `Fare` and `Age` attributes into ordinal ones by grouping values into categories.","81c413ba":"## 4 - Data Visualization <a name=\"viz\"><\/a>\n\n# TODO","bda0e4d7":"Thoughts on the correlation patterns:\n* Being female is definitely a significant factor, shown both by `Sex` and `Title` attributes.\n* Being married, or at least in company of someone, seems relevant, as both shown by `FamiliSize` and `Title` attributes\n* Social rank and financial capability seems to play a role, as we can see in `Title` (Master), `Pclass` and `Fare`. \n\nAll those points are quite intuitive.\n","cfa70b72":"## 5 - Data Modeling <a name=\"model\"><\/a>\n\n### Creating train and test copies for models","e5d2d09e":"### Remove unnecessary columns\n\nNow that we have engineered our new columns and converted the old ones, we can finally drop unnecessary columns\n* `Name`, since it is a simple string from which we already extracted the title\n* `SibSp` and `Parch` since they have been merged in `FamilySize`, and they don't seem very correlated with the survival when taken alone.\n* `FareBand` and `AgeBand`, since we used them to change `Age` and `Fare` values.\n* For the training set, we also want to drop the `PassengerId`"}}