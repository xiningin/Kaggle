{"cell_type":{"00fe971e":"code","960c3a50":"code","94ff9a24":"code","db86c997":"code","71145eb5":"code","4d33ecd4":"code","4c8680e7":"code","3ee675bf":"code","a0db1e6a":"code","09871279":"code","506bf37b":"code","c72f084e":"code","a01b6c9d":"code","cd9deb6d":"code","2b98a694":"code","48244ff2":"code","c561f409":"code","8c48dae9":"code","d2c8e7e2":"code","1cc2922a":"code","bf290f39":"code","c932b197":"code","a305b3ae":"code","ff88a65c":"code","80b031bd":"code","b881cc18":"code","d90cb1b7":"code","0a108a24":"code","bc396ffe":"code","5988c0d2":"markdown","fcdf876d":"markdown","6ddd1626":"markdown","f6896512":"markdown","d6cf2c27":"markdown","84158978":"markdown","fc171a13":"markdown","a7cf0ad9":"markdown","a68af4a5":"markdown","c3dae0d9":"markdown","dac5f1ed":"markdown","fa9006a4":"markdown","7bea5852":"markdown","c1986c44":"markdown","78c22bd8":"markdown","2e1c3f94":"markdown","9d073e21":"markdown","1eaa3f6a":"markdown","6b551b1c":"markdown","bb1de149":"markdown","1b0df21f":"markdown","86c08762":"markdown","541e876e":"markdown","2a1b2dc6":"markdown","6d1dbff7":"markdown","818f6f6b":"markdown","1fd823e0":"markdown","02b2fdf3":"markdown","a354abb8":"markdown","ea2114ee":"markdown"},"source":{"00fe971e":"# Biblotecas b\u00e1sicas\nimport numpy as np              #Biblioteca para algebra linear e calculos\nimport pandas as pd             #Biblioteca para manipula\u00e7\u00e3o de dados\nimport pandas_profiling         #Biblioteca para analise exploratoria de dados\nimport matplotlib.pyplot as plt #Biblioteca para gr\u00e1ficos\nimport seaborn as sns           #Biblioteca para gr\u00e1ficos mais complexos\nimport h2o                      #Biblioteca para conex\u00e3o do framework H2O \nimport lightgbm as lgb          #Biblioteca para conex\u00e3o do framework lightGBM\n\n# Pacotes para realiza\u00e7\u00e3o das modelagens de aprendizado de m\u00e1quina.\nfrom pylab import rcParams\nimport matplotlib.gridspec as gridspec\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nfrom sklearn.feature_selection import SelectKBest, f_classif,mutual_info_classif\nfrom sklearn.model_selection import KFold, cross_val_score, GridSearchCV, train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,roc_curve, \n                             recall_score, classification_report, f1_score, precision_recall_fscore_support)\nfrom h2o.estimators.deeplearning import H2OAutoEncoderEstimator, H2ODeepLearningEstimator","960c3a50":"%%time\ndf = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","94ff9a24":"%%time\n# Report Pandas Profiling\nreport = df.profile_report(title='Report - Ccard Fraud',minimal=True)\nreport","db86c997":"# Estat\u00edsticas Descritivas do dataset\nround(df.describe(),5)","71145eb5":"# Histogramas com foco no target\nv_features = df.iloc[:,1:29].columns\nplt.figure(figsize=(12,28*4))\ngs = gridspec.GridSpec(28, 1)\nfor i, cn in enumerate(df[v_features]):\n    ax = plt.subplot(gs[i])\n    sns.distplot(df[cn][df.Class == 1], bins=50)\n    sns.distplot(df[cn][df.Class == 0], bins=50)\n    ax.set_xlabel('')\n    ax.set_title('Histograma rela\u00e7\u00e3o de vari\u00e1veis com o Target: ' + str(cn))\nplt.show()","4d33ecd4":"# Retirar as duplicatas\ndf = df.drop_duplicates()","4c8680e7":"# Retirando a coluna Time\ndf = df.drop(['Time'], axis=1)","3ee675bf":"x = df.drop('Class',axis=1)\ny = df['Class']\nxtr, xval, ytr, yval = train_test_split(x,y,test_size=0.2,random_state=67)","a0db1e6a":"# Selecionar um ter\u00e7o das vari\u00e1veis do dataset\nsel = SelectKBest(mutual_info_classif, k=10).fit(xtr,ytr)\nselecao = list(xtr.columns[sel.get_support()])\nprint(selecao)","09871279":"# Dados de treino e teste com a sele\u00e7\u00e3o de vari\u00e1veis\nxtr = xtr.filter(selecao)\nxval = xval.filter(selecao)","506bf37b":"%%time\n# Pesquisa Aleat\u00f3ria\n# Par\u00e2metros para pesquisa aleat\u00f3ria\nlogistic = LogisticRegression()\nC = np.logspace(0, 4, num=10)\npenalty = ['l1', 'l2']\nsolver = ['liblinear', 'saga']\nweights = np.linspace(0.01, 0.99, 10)\nhyperparameters = dict(C=C, penalty=penalty, solver=solver, class_weight = weights)\n\n# Treinamento e sele\u00e7\u00e3o dos melhores hiperparamentros\nrandomizedsearch = RandomizedSearchCV(logistic, hyperparameters,cv = 3,verbose=2,random_state=42,n_jobs = -1,scoring=\"recall\")\nbest_model_random = randomizedsearch.fit(xtr, ytr)\nprint(best_model_random.best_estimator_)","c72f084e":"baseline = LogisticRegression(C=1.0, class_weight=0.8811111111111111, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l1',\n                   random_state=None, solver='saga', tol=0.0001, verbose=0,\n                   warm_start=False)\nbaseline.fit(xtr,ytr)\np = baseline.predict(xval)","a01b6c9d":"# Calculo das probabilidade das classes e as m\u00e9tricas para curva ROC.\ny_pred_prob = baseline.predict_proba(xval)[:,1]\nfpr,tpr,thresholds = roc_curve(yval,y_pred_prob)\nroc_auc = auc(fpr, tpr)\nprint(\"Area under the ROC curve : %f\" % roc_auc)\n\n# Curva ROC.\nplt.plot(fpr, tpr)\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('Curva ROC - Credit Card Detection')\nplt.xlabel('False Positive Rate (1 \u2014 Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)\nplt.show()\n\n# Calculo de metricas e thresholds.\ni = np.arange(len(tpr)) # index for df\nroc = pd.DataFrame({'fpr' : pd.Series(fpr, index=i),\n                    'tpr' : pd.Series(tpr, index = i), \n                    '1-fpr' : pd.Series(1-fpr, index = i), \n                    'tf' : pd.Series(tpr - (1-fpr), index = i), \n                    'threshold' : pd.Series(thresholds, index = i)})\ntab_metricas = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\nprint(tab_metricas)\n\n# Threshold: O corte ideal seria onde tpr \u00e9 alto e fpr \u00e9 baixo tpr - (1-fpr) \u00e9 zero ou quase zero \u00e9 o ponto de corte ideal.\nt = tab_metricas.iloc[0].values[4]\ny_pred = [1 if e > t else 0 for e in y_pred_prob]\n\n# Constru\u00e7\u00e3o do plot da matriz de confus\u00e3o\nLABELS = ['Normal', 'Fraud']\nconf_matrix = confusion_matrix(yval, p)\nplt.figure(figsize=(8,6))\nsns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\nplt.title(\"Matriz de confus\u00e3o\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.grid(False)\nplt.show()\n\n# Report de m\u00e9tricas\nreport = classification_report(yval, p)\nprint(report)","cd9deb6d":"%%time\n# Pesquisa Aleat\u00f3ria\n# Paramentros para pesquisa aleat\u00f3ria\nparam_test ={'num_leaves': sp_randint(6, 50), \n             'min_child_samples': sp_randint(100, 500), \n             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n             'subsample': sp_uniform(loc=0.2, scale=0.8), \n             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\nn_HP_points_to_test = 100\nclf = lgb.LGBMClassifier(max_depth=-1, random_state=314, silent=True, metric='None', n_jobs=4, n_estimators=100)\nrandomizedsearch = RandomizedSearchCV(\n    estimator=clf, param_distributions=param_test, \n    n_iter=n_HP_points_to_test,\n    scoring='roc_auc',\n    cv=3,\n    refit=True,\n    random_state=314,\n    verbose=True)\n\n# Treinamento e sele\u00e7\u00e3o dos melhores hiperpar\u00e2mentros\nbest_model_random = randomizedsearch.fit(xtr, ytr)\nprint(best_model_random.best_estimator_)","2b98a694":"baseline = lgb.LGBMClassifier(boosting_type='gbdt', class_weight=None,\n               colsample_bytree=0.43473068607216775, importance_type='split',\n               learning_rate=0.1, max_depth=-1, metric='None',\n               min_child_samples=478, min_child_weight=0.01, min_split_gain=0.0,\n               n_estimators=100, n_jobs=4, num_leaves=9, objective=None,\n               random_state=314, reg_alpha=1, reg_lambda=5, silent=True,\n               subsample=0.4261926450859534, subsample_for_bin=200000,\n               subsample_freq=0)\nbaseline.fit(xtr,ytr)\np = baseline.predict(xval)","48244ff2":"# Calculo das probabilidade das classes e as m\u00e9tricas para curva ROC.\ny_pred_prob = baseline.predict_proba(xval)[:,1]\nfpr,tpr,thresholds = roc_curve(yval,y_pred_prob)\nroc_auc = auc(fpr, tpr)\nprint(\"Area under the ROC curve : %f\" % roc_auc)\n\n# Curva ROC.\nplt.plot(fpr, tpr)\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('ROC for Credit Card Detection')\nplt.xlabel('False Positive Rate (1 \u2014 Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)\nplt.show()\n\n# Calculo de metricas e thresholds.\ni = np.arange(len(tpr)) # index for df\nroc = pd.DataFrame({'fpr' : pd.Series(fpr, index=i),\n                    'tpr' : pd.Series(tpr, index = i), \n                    '1-fpr' : pd.Series(1-fpr, index = i), \n                    'tf' : pd.Series(tpr - (1-fpr), index = i), \n                    'threshold' : pd.Series(thresholds, index = i)})\ntab_metricas = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\nprint(tab_metricas)\n\n# Threshold: O corte ideal seria onde tpr \u00e9 alto e fpr \u00e9 baixo tpr - (1-fpr) \u00e9 zero ou quase zero \u00e9 o ponto de corte ideal.\nt = tab_metricas.iloc[0].values[4]\ny_pred = [1 if e > t else 0 for e in y_pred_prob]\n\n# Constru\u00e7\u00e3o do plot da matriz de confus\u00e3o\nLABELS = ['Normal', 'Fraud']\nconf_matrix = confusion_matrix(yval, p)\nplt.figure(figsize=(8,6))\nsns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\nplt.title(\"Matriz de confus\u00e3o\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.grid(False)\nplt.show()\n\n# Report de m\u00e9tricas\nreport = classification_report(yval, p)\nprint(report)","c561f409":"h2o.init(max_mem_size = 4)\nh2o.remove_all()","8c48dae9":"creditData_df = h2o.import_file(\"..\/input\/creditcardfraud\/creditcard.csv\")","d2c8e7e2":"features= creditData_df.drop(['Time'], axis=1)\ntrain, test = features.split_frame([0.8])\nprint(train.shape)\nprint(test.shape)","1cc2922a":"# Convers\u00e3o para dataframe pandas\ntrain_df = train.as_data_frame()\ntest_df = test.as_data_frame()\ntrain_df = train_df[train_df['Class'] == 0]\n\n# Drop a vari\u00e1vel de target\ntrain_df = train_df.drop(['Class'], axis=1)\nY_test_df = test_df['Class'] # true labels of the testing set\ntest_df = test_df.drop(['Class'], axis=1)\ntrain_df.shape\n\ntrain_h2o = h2o.H2OFrame(train_df) # converting to h2o frame\ntest_h2o = h2o.H2OFrame(test_df)\nx = train_h2o.columns","bf290f39":"%%time\nanomaly_model = H2ODeepLearningEstimator(activation = \"Tanh\",\n                               hidden = [29,15,15,29],\n                               epochs = 500,\n                               standardize = True,\n                               stopping_metric = 'MSE', \n                               loss = 'automatic',\n                               train_samples_per_iteration = 32,\n                               shuffle_training_data = True,     \n                               autoencoder = True,\n                               l1 = 10e-5)\nanomaly_model.train(x=x, training_frame = train_h2o)\nanomaly_model._model_json['output']['variable_importances'].as_data_frame()","c932b197":"rcParams['figure.figsize'] = 14, 8\n#plt.rcdefaults()\nfig, ax = plt.subplots()\nvariables = anomaly_model._model_json['output']['variable_importances']['variable']\nvar = variables[0:10]\ny_pos = np.arange(len(var))\nscaled_importance = anomaly_model._model_json['output']['variable_importances']['scaled_importance']\nsc = scaled_importance[0:10]\nax.barh(y_pos, sc, align='center', color='green', ecolor='black')\nax.set_yticks(y_pos)\nax.set_yticklabels(variables)\nax.invert_yaxis()\nax.set_xlabel('Importancia relativa')\nax.set_title('Importancia das vari\u00e1veis')\nplt.show()","a305b3ae":"scoring_history = anomaly_model.score_history()\n%matplotlib inline\nrcParams['figure.figsize'] = 14, 8\nplt.plot(scoring_history['training_mse'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\ntest_rec_error = anomaly_model.anomaly(test_h2o) \n\n# anomaly is a H2O function which calculates the error for the dataset\n# converting to pandas dataframe\ntest_rec_error_df = test_rec_error.as_data_frame()\n# plotting the testing dataset against the error\ntest_rec_error_df['id']=test_rec_error_df.index\nrcParams['figure.figsize'] = 14, 8\ntest_rec_error_df.plot(kind=\"scatter\", x='id', y=\"Reconstruction.MSE\")\nplt.show()","ff88a65c":"predictions = anomaly_model.predict(test_h2o)\nerror_df = pd.DataFrame({'reconstruction_error': test_rec_error_df['Reconstruction.MSE'],\n                        'true_class': Y_test_df})\nerror_df.describe()","80b031bd":"fig = plt.figure()\nax = fig.add_subplot(111)\nrcParams['figure.figsize'] = 14, 8\nnormal_error_df = error_df[(error_df['true_class']== 0) & (error_df['reconstruction_error'] < 10)]\n_ = ax.hist(normal_error_df.reconstruction_error.values, bins=20)","b881cc18":"fig = plt.figure()\nax = fig.add_subplot(111)\nrcParams['figure.figsize'] = 14, 8\nfraud_error_df = error_df[error_df['true_class'] == 1]\n_ = ax.hist(fraud_error_df.reconstruction_error.values, bins=20)","d90cb1b7":"fpr, tpr, thresholds = roc_curve(error_df.true_class, error_df.reconstruction_error)\nroc_auc = auc(fpr, tpr)\n\n# Calculo de metricas e thresholds.\ni = np.arange(len(tpr)) # index for df\nroc = pd.DataFrame({'fpr' : pd.Series(fpr, index=i),\n                    'tpr' : pd.Series(tpr, index = i), \n                    '1-fpr' : pd.Series(1-fpr, index = i), \n                    'tf' : pd.Series(tpr - (1-fpr), index = i), \n                    'threshold' : pd.Series(thresholds, index = i)})\ntab_metricas = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\nprint(tab_metricas)\n\n# Threshold: O corte ideal seria onde tpr \u00e9 alto e fpr \u00e9 baixo tpr - (1-fpr) \u00e9 zero ou quase zero \u00e9 o ponto de corte ideal.\nt = tab_metricas.iloc[0].values[4]\n\nplt.title('ROC for Credit Card Detection')\nplt.plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.001, 1])\nplt.ylim([0, 1.001])\nplt.xlabel('False Positive Rate (1 \u2014 Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.show();","0a108a24":"threshold = t\ngroups = error_df.groupby('true_class')\nfig, ax = plt.subplots()\nfor name, group in groups:\n    ax.plot(group.index, group.reconstruction_error, marker='o', ms=3.5, linestyle='',\n            label= \"Fraud\" if name == 1 else \"Normal\")\nax.hlines(threshold, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')\nax.legend()\nplt.title(\"Reconstruction error for different classes\")\nplt.ylabel(\"Reconstruction error\")\nplt.xlabel(\"Data point index\")\nplt.show();","bc396ffe":"LABELS = ['Normal', 'Fraud']\ny_pred = [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\nconf_matrix = confusion_matrix(error_df.true_class, y_pred)\nplt.figure(figsize=(8,6))\nsns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\nplt.title(\"Matriz de confus\u00e3o\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()\n\ncsr = classification_report(error_df.true_class, y_pred)\nprint(csr)","5988c0d2":"Gr\u00e1fico de import\u00e2ncias de vari\u00e1veis","fcdf876d":"Transforma\u00e7\u00e3o de dados","6ddd1626":"## Modelagem supervisionada com regress\u00e3o log\u00edstica otimizando par\u00e2metros ajustando o threshold.","f6896512":"## Conclus\u00e3o\n\nNa proxima atualiza\u00e7\u00e3o","d6cf2c27":"Divis\u00e3o do dataset: 80% para treino e 20% para teste.","84158978":"Curva ROC","fc171a13":"## Modelagem supervisionada com lightGBM otimizando paramentros com o m\u00e9todo de pesquisa aleat\u00f3ria.\n1. Random Search \n2. Treino do modelo com os melhores par\u00e2metros\n3. Previs\u00f5es\n4. Curva ROC, Avalia\u00e7\u00e3o do modelo e Matriz de confus\u00e3o","a7cf0ad9":"Gr\u00e1fico da fun\u00e7\u00e3o de perda","a68af4a5":"### Analise dos resultados.\nO Recall \u00e9 a medida que entre todas as situa\u00e7\u00f5es de classe Positivo como valor esperado, quantas est\u00e3o corretas e neste caso 76% da classe positiva foi classificado corretamente, selecionando as melhores vari\u00e1veis, otimizando os paramentros e o threshold o modelo lightGBM foi muito bem para esse tipo de problema, uma abordagem de balanceamento de amostra(undersampling) poderia trazer melhores resultados.","c3dae0d9":"Curva ROC, Matriz de confus\u00e3o e metricas","dac5f1ed":"### Avalia\u00e7\u00e3o de resultados\nO Recall \u00e9 a medida que entre todas as situa\u00e7\u00f5es de classe Positivo como valor esperado, quantas est\u00e3o corretas e neste caso 92% da classe positiva foi classificado corretamente, selecionando as melhores vari\u00e1veis, otimizando os paramentros e o threshold a rede neural Autoenconder foi razoavelmente bem para esse tipo de problema, uma abordagem de balanceamento de amostra(undersampling) poderia trazer melhores resultados.\n","fa9006a4":"Sele\u00e7\u00e3o de dados: M\u00e9todo de Sele\u00e7\u00e3o k vari\u00e1veis com informa\u00e7\u00e3o m\u00fatua.\n\n* As informa\u00e7\u00f5es m\u00fatuas entre duas vari\u00e1veis medem a depend\u00eancia de uma vari\u00e1vel para outra e facilita a rela\u00e7\u00e3o n\u00e3o linear entre o recurso e a vari\u00e1vel de destino.","7bea5852":"Deep Learning com H2O: Autoenconder com fun\u00e7\u00e3o de ativa\u00e7\u00e3o tangente hiperb\u00f3lica com 500 epochs ","c1986c44":"Divis\u00e3o dos dados","78c22bd8":"Report da an\u00e1lise explor\u00e1toria de dados:\n1. As caracter\u00edsticas das vari\u00e1veis resultantes de uma modelagem PCA as estat\u00edsticas de correla\u00e7\u00e3o e m\u00e9dia entre eles \u00e9 proximo de zero.\n2. N\u00famero de observa\u00e7\u00f5es de 284807.\n3. O dataset tem 1081 observa\u00e7\u00f5es duplicadas (0,4%).\n4. N\u00e3o existe valores nulos.\n5. Existem outliers em algumas vari\u00e1veis e n\u00e3o ser\u00e3o retirados, pois corremos o risco de retirar linhas da classe de fraude.","2e1c3f94":"Reconstruction error for the fraud transactions in the testing dataset","9d073e21":"Entrada de dados","1eaa3f6a":"Reconstruction error for the normal transactions in the testing dataset","6b551b1c":"## Modelagem n\u00e3o-supervisionada com rede neural Autoenconder do framework H2O.\n\nAutoencoders \u00e9 uma rede neural n\u00e3o supervisionada. \u00c9 um algoritmo de compacta\u00e7\u00e3o de dados que recebe a entrada e passa por uma representa\u00e7\u00e3o compactada e fornece a sa\u00edda reconstru\u00edda.\n\nIniciando o h2o server:","bb1de149":"Transforma\u00e7\u00f5es das dataset de acordo com an\u00e1lise explor\u00e1toria de dados","1b0df21f":"## Modeling - Credit Card Fraud Detection\n\nO dataset cont\u00e9m apenas vari\u00e1veis n\u00famericas resultado de uma modelagem PCA e variavel target est\u00e1 desbalanceada e a classe positiva (fraudes) representa 0,172% de todas as transa\u00e7\u00f5es.\n\nAs tr\u00eas abordagens de modelagem \u00e9 para verificar as seguintes hipoteses um modelo de regress\u00e3o logistica com ajuste do threshold para minimizar a taxa de falsos positivos, a segunda hipotese \u00e9 realizar uma modelagem com um modelo lightGBM que \u00e9 um modelo boosting muito utilizado em competi\u00e7\u00f5es do Kaggle e ele vai ser otimizado via procura aleatoria e a ultima hipotese \u00e9 utilizar uma modelagem de uma rede neural n\u00e3o-supervisionada Autoenconder via framework H2O e ao final do notebook iremos comparar os resultados e eleger uma abordagem para possivel implementa\u00e7\u00e3o.\n\n### Etapas da modelagem\n\n1. Entrada e tratamento de dados\n2. Analise exploratoria de dados\n3. Modelagem supervisionada com regress\u00e3o logistica otimizando paramentros com o m\u00e9todo de pesquisa aleat\u00f3ria e ajustando o threshold.\n4. Modelagem supervisionada com lightGBM otimizando paramentros com o m\u00e9todo de pesquisa aleat\u00f3ria e ajustando o threshold.\n5. Modelagem n\u00e3o-supervisionada com rede neural autoenconder do framework h2o.\n6. Avalia\u00e7\u00e3o de resultados e conclus\u00f5es","86c08762":"Calculo de previs\u00f5es","541e876e":"Curva ROC, Matriz de confus\u00e3o e M\u00e9tricas","2a1b2dc6":"## Entrada e tratamento de dados","6d1dbff7":"### Analise dos resultados.\nO Recall \u00e9 a medida que entre todas as situa\u00e7\u00f5es de classe Positivo como valor esperado, quantas est\u00e3o corretas e neste caso apenas 46% da classe positiva foi classificado corretamente, mesmo selecionando as melhores vari\u00e1veis, otimizando os paramentros e o threshold o modelo de regress\u00e3o log\u00edstica foi bem abaixo para esse tipo de problema, uma abordagem de balanceamento de amostra(undersampling) poderia trazer melhores resultados.","818f6f6b":"## Analise explorat\u00f3ria de dados","1fd823e0":"Modelagem de regress\u00e3o log\u00edstica:\n1. Random Search \n2. Treino do modelo com os melhores par\u00e2metros\n3. Previs\u00f5es\n4. Curva ROC, Avalia\u00e7\u00e3o do modelo e Matriz de confus\u00e3o","02b2fdf3":"Bibliotecas python para modelagem e an\u00e1lise de dados","a354abb8":"Matriz de confus\u00e3o","ea2114ee":"Gr\u00e1fico da base de teste com o threshold"}}