{"cell_type":{"070d4e9e":"code","66c995f7":"code","7cdd681e":"code","0b42c1ca":"code","bba2419f":"code","adec0343":"code","d2059915":"code","24f12553":"code","802f8d1a":"code","05aaaabf":"code","3d4cf7d3":"code","831d1380":"code","4a5fa111":"code","9c5b1a13":"code","9d4ad889":"code","0b3d1f1f":"code","bb00e6ca":"code","2ec7fa6f":"code","6eeb7530":"code","5783f3e6":"code","31683c72":"code","47c47e8a":"code","72564a08":"code","19127fa4":"code","9ea4f38a":"code","88339912":"code","cce4d282":"code","610c6153":"code","6b1db422":"code","c520669b":"code","763a5679":"code","462569c5":"code","d285bcee":"code","5a64c20b":"code","384b8ebb":"code","11b6a5ee":"code","db5bb820":"code","46413e4a":"code","1711368b":"code","ac0ed1a6":"code","ae7327db":"code","a93f5bd6":"code","9b1a5d8a":"code","e00b74ed":"code","5776b42b":"code","9979cc44":"code","d7321e85":"code","ed6b4ed9":"code","cf3e4ba2":"code","3ed99352":"code","fc06fc1c":"code","5d44aa0c":"code","34526f8c":"code","66da1ceb":"code","9e531bdc":"code","e7df25fc":"code","3712446f":"code","c1d4982f":"code","38b6e65c":"code","67809266":"code","5266a5bf":"code","17742045":"code","25e47bd6":"code","58283df2":"code","8d444b00":"code","cc9e6bd9":"code","98ed5b5b":"code","a0a1b6e0":"code","f822916f":"code","ecab3a51":"code","0bad9fa0":"code","fc626138":"code","c6125964":"code","398069da":"code","dc73c506":"code","9f77099e":"code","d93d15f0":"code","10b225d2":"code","c06a9e9e":"code","f1940e7a":"code","705559d9":"code","45a3c27a":"code","433e88a0":"code","d469c422":"code","eef2aed4":"code","f237beb0":"code","5332b5b7":"code","d9cfa5f5":"code","2ddcf101":"code","20ea3a90":"code","e43922e7":"code","c4aef1eb":"code","0059dbb5":"code","6c39829e":"code","fd6b97c0":"code","fb9f3e1e":"code","92c93f16":"code","e177a5c8":"markdown","6d3983d8":"markdown","d7b772ff":"markdown","20543a19":"markdown","378ea3ff":"markdown","770e80f9":"markdown","75778ce6":"markdown","c9c3b95f":"markdown","a615d7f0":"markdown","35f62369":"markdown","48e9eaff":"markdown","fe8a8e04":"markdown","b95f1653":"markdown","6a4a51ca":"markdown","32cf3320":"markdown","f05a24fd":"markdown","073cf22a":"markdown","c8612179":"markdown","51624e07":"markdown","374ffd66":"markdown","f585b131":"markdown","070125bc":"markdown","5ebf2e69":"markdown","f9954751":"markdown","0606afb8":"markdown"},"source":{"070d4e9e":"import warnings\nwarnings.filterwarnings('ignore') #to ignore unnecessary warnings\n\nimport numpy as np\nimport pandas as pd\n\n#plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\n#data preprocessing\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import StandardScaler\n\n# ML algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\n\n#cross validation\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import StratifiedKFold\n\n# evaluation metrics\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score, classification_report\n\n\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","66c995f7":"#raw train file\n\ntrain_raw = pd.read_csv('\/kaggle\/input\/system-hack-highly-imbalanced-data\/Train.csv')\ntrain_raw.head(10)","7cdd681e":"#checking distributuion of class 0\n\ntrain_raw.loc[train_raw['MULTIPLE_OFFENSE'] == 0]","0b42c1ca":"# raw test file\n\ntest_raw = pd.read_csv('\/kaggle\/input\/system-hack-highly-imbalanced-data\/Test.csv')\ntest_raw.head(10)","bba2419f":"#Shape of the dataframe\n\nprint('No. of rows in train_raw = ', train_raw.shape[0],'\\n','No. of columns in train_raw = ', train_raw.shape[1] )","adec0343":"print('No. of rows in test_raw = ', test_raw.shape[0],'\\n','No. of columns in test_raw = ', test_raw.shape[1] )","d2059915":"# General information - train_raw\n\ntrain_raw.info()","24f12553":"# Unique value count for train_raw \n\nfor i in train_raw.columns:\n    print(train_raw[i].value_counts(dropna=False))","802f8d1a":"train_raw['X_1'].unique()","05aaaabf":"# Converting Date from object type to pd.datetime\n\ntrain_raw['DATE'] = pd.to_datetime(train_raw['DATE'])\ntest_raw['DATE'] = pd.to_datetime(test_raw['DATE'])\n\nprint('Lower Date: ' ,train_raw['DATE'].min(), '\\n', 'Higher date : ', train_raw['DATE'].max())","3d4cf7d3":"#General information: test_raw\n\ntest_raw.info()","831d1380":"# Unique value count for test_raw \n\nfor i in test_raw.columns:\n    print(test_raw[i].value_counts(dropna=False))","4a5fa111":"#Checking Data Imbalance \/\/ Uniques value counts in 'MULTIPLE_OFFENSE' \/\/ \n\nuv = pd.DataFrame(train_raw['MULTIPLE_OFFENSE'].value_counts(dropna =False))\nuv  # 1 - Hack\n    # 0 - Not a Hack","9c5b1a13":"# Function to print height of barcharts on the bars\n\ndef barh(ax):\n    \n    for p in ax.patches:\n        val = p.get_height() #height of the bar\n        x = p.get_x()+ p.get_width()\/2 # x- position \n        y = p.get_y() + p.get_height() + 500 #y-position\n        ax.annotate(round(val,2),(x,y))\n","9d4ad889":"# Ploting imbalance in dataframe \n\nplt.figure(figsize = (15,7))\n\ncols = ['r','b']\n\nplt.subplot(1,2,1)\nax0 = sns.countplot(x = 'MULTIPLE_OFFENSE',data = train_raw)\nbarh(ax0)\nplt.title('Data Imbalance in counts')\n\nplt.subplot(1,2,2)\nlabels = ['Hack','Not a Hack' ]\n\nplt.pie(uv['MULTIPLE_OFFENSE'], labels=labels, autopct='%1.1f%%',explode = (0,1),colors = cols)\nplt.title('Data Imbalance in Percentages (%)')\n\n\nplt.show()\n\n# 1 - Hack\n    # 0 - Not a Hack","0b3d1f1f":"# All columns of train_raw dataframe\n\ncol = train_raw.columns\ncol ","bb00e6ca":"# Numerical columns\n\nnum_col = [i for i in col if i not in ['INCIDENT_ID', 'DATE','MULTIPLE_OFFENSE']]\n","2ec7fa6f":"# Plotting boxplots to see the distribution of each variable with 'MULTIPLE_OFFENSE'\n\nfor i in num_col:\n    plt.figure(figsize=(15,5))\n    sns.boxplot(x='MULTIPLE_OFFENSE',y=i,data=train_raw)\n    plt.show()","6eeb7530":"train_raw.describe(percentiles = [0.05,0.25,0.5,0.75,0.8,0.85,0.95,0.99,0.995])","5783f3e6":"#Removing outliers\n\ntrain_df = train_raw.loc[(train_raw['X_8'] <=train_raw['X_8'].quantile(0.995)) & (train_raw['X_10'] <=train_raw['X_10'].quantile(0.995))\n                          & (train_raw['X_12'] <=train_raw['X_12'].quantile(0.995))]\ntrain_df.describe(percentiles = [0.05,0.25,0.5,0.75,0.8,0.85,0.95,0.99,0.995])","31683c72":"# Checking outliers for test_raw dataframe\n\nfor i in num_col:\n    plt.figure(figsize=(15,5))\n    sns.boxplot(x=i,data=test_raw)\n    plt.show()","47c47e8a":"test_raw.describe(percentiles = [0.05,0.25,0.5,0.75,0.8,0.85,0.95,0.99,0.995])","72564a08":"# Checking for null values\n\ntrain_df.isnull().sum()","19127fa4":"#Searching null values in test_df\n\npd.DataFrame({\"Null value count\": test_raw.isnull().sum(), \"Null value in %\": round(100*(test_raw.isnull().sum()\/test_raw.shape[0]),2)})","9ea4f38a":"# Treating null values\n\ntest_raw['X_12']= test_raw['X_12'].fillna(test_raw['X_12'].median())\ntest_raw.isnull().sum()","88339912":"test_df = test_raw #NEW NAME ","cce4d282":"#Checking skewness \n\nfor i in num_col:\n    plt.figure(figsize = (20,7))\n    sns.distplot(train_df[i],kde_kws={'bw':0.05})\n    plt.show()","610c6153":"#Creating function to extract features from date column\n\ndef date_feature(df):\n    df['Day'] = df['DATE'].dt.day\n    df['Month'] = df['DATE'].dt.month\n    df['Year'] = df['DATE'].dt.year\n    \n    return df","6b1db422":"# Extracting features\n\ntrain_df = date_feature(train_df)\ntest_df = date_feature(test_df)\ntrain_df","c520669b":"# Dropping columns 'INCIDENT_ID', 'DATE'\n\ntrain_df.drop(['INCIDENT_ID', 'DATE'],axis=1, inplace=True)\n\ntest_df_id = test_df.pop('INCIDENT_ID') # we need id column for submission \ntest_df.drop(['DATE'],axis=1,inplace=True)\n\ntrain_df.head()","763a5679":"test_df.head()","462569c5":"# Spliting into X and y\n\ny = train_df.pop('MULTIPLE_OFFENSE')\nX = train_df\nX.head()","d285bcee":"#Spiliting data set into train and test using strtify= y as the data set is highly imbalanced\n\nX_train,X_test,y_train,y_test = train_test_split(train_df,y, train_size = 0.8, random_state = 100,stratify=y) #stratitify y as the data set is highly imbalanced\n\nprint('Train data : \\n',y_train.value_counts())\nprint('Test data : \\n',y_test.value_counts())","5a64c20b":"#Variations of different classes in train & test data set\n\nprint('Train data : \\n',(y_train.value_counts()\/len(y_train))*100,'\\n \\n')\nprint('Test data : \\n',(y_test.value_counts()\/len(y_test))*100)","384b8ebb":"# Checking dimension after spliting\n\nprint('len of X = ', len(X),', len of X_train + X_test = ', len(X_train)+len(X_test),', X_train = ',len(X_train),', X_test = ',len(X_test))","11b6a5ee":"#Standardizing\n\nscaler = StandardScaler() #initialization of StandardScaler\n\ncolx = X_train.columns\n\nX_train[colx] = scaler.fit_transform( X_train[colx])\nX_train.head()","db5bb820":"# Standardizing X_test\n\nX_test[colx] = scaler.transform(X_test[colx])\nX_test.head()","46413e4a":"#Standardizing TRUE test data\n\ntest_df[colx] = scaler.transform(test_df[colx])\ntest_df.head()","1711368b":"# Using RandomUnderSampler\nfrom imblearn.under_sampling import RandomUnderSampler #undersampling\n\nusm = RandomUnderSampler(random_state =25)\n\nX_train_us,y_train_us = usm.fit_resample(X_train,y_train)\n\nX_train_us =pd.DataFrame(X_train_us,columns = colx)\n\nprint('Shape of X_train after random undersampling : {}'.format(X_train_us.shape))\nprint('Shape of original X_train : {}'.format(X_train.shape))\nX_train_us.head()\nX_train_us","ac0ed1a6":"np.bincount(y_train_us)","ae7327db":"np.bincount(y_train) # original","a93f5bd6":"# HYPERPARAMETER TUNNING \n\n# Tuning logistic regression\n\nparam_log = {'penalty':['l1','l2'],'C':[0.1,.2,.3,.4,.5]}\n\nlog = LogisticRegression(class_weight ='balanced',random_state=5)\n\nfolds= StratifiedKFold(n_splits = 3, shuffle = True, random_state = 90)\ngrid_log = GridSearchCV(estimator = log, param_grid = param_log, \n                          cv = folds, n_jobs = -1,verbose = 1, scoring = 'roc_auc')\n\ngrid_log.fit(X_train_us,y_train_us)\n\ngrid_log.best_params_","9b1a5d8a":"#building model on best params\n\nlogistic = LogisticRegression(class_weight ='balanced',random_state=5,C = 0.5, penalty='l2')\n\nlogistic.fit(X_train_us,y_train_us)\n\n#prediction\npred_log_sm_train = logistic.predict_proba(X_train_us)[:,1]\npred_log_sm_test = logistic.predict_proba(X_test)[:,1]\n\ny_pred = logistic.predict(X_test)\n# Score\n\nprint ( 'Train auc score : ', roc_auc_score(y_train_us,pred_log_sm_train))\nprint ( 'Test auc score : ', roc_auc_score(y_test,pred_log_sm_test))\n\nprint(classification_report(y_test,y_pred))\n\nlog_us_auc_test_cv = roc_auc_score(y_test, pred_log_sm_test)\nprecision_log_us_cv = precision_score(y_test,y_pred)\nrecall_log_us_cv = recall_score(y_test,y_pred)\nf1_log_us_cv = f1_score(y_test,y_pred)","e00b74ed":"# plotting ROC curve on test data\nfpr, tpr, thresholds = roc_curve(y_test, pred_log_sm_test)\nplt.figure(figsize =(7,5))\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.5f)' % log_us_auc_test_cv)\nplt.plot([0, 1], [0, 1])\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","5776b42b":"# Random forest\n\n# Hyperparameter tuning for random forest\n\nparam_rf = {\n    'max_depth': [8,10],\n    'min_samples_leaf': range(50, 200, 50),\n    'min_samples_split':range(50, 200, 50),\n    'n_estimators': [100,150,200,300], \n    'max_features': [5, 10,15,20]\n    \n}\n\nrf = RandomForestClassifier(n_jobs=-1,class_weight ='balanced',random_state=105)\n\nfolds= StratifiedKFold(n_splits = 3, shuffle = True, random_state = 90)\n# Instantiate \ngrid_rf = RandomizedSearchCV(estimator = rf, param_distributions = param_rf, \n                          cv = folds, n_jobs = -1,verbose = 1,scoring = 'roc_auc',random_state =100)\n\n\n#fitting\ngrid_rf.fit(X_train_us,y_train_us)\n\n","9979cc44":"#best params\n\ngrid_rf.best_params_","d7321e85":"#using best params\n\nforest_cv = RandomForestClassifier(n_estimators=300,\n                                   min_samples_split=100, min_samples_leaf=50,\n                                   max_features=15,max_depth=10,\n                                   n_jobs=-1,class_weight ='balanced',random_state=1055)\n\nforest_cv.fit(X_train_us,y_train_us)\n\n\n#prediction\n\npred_rf_train_cv= forest_cv.predict_proba(X_train_us)[:,1]\npred_rf_test_cv = forest_cv.predict_proba(X_test)[:,1]\n\ny_pred = forest_cv.predict(X_test)\n#score\nprint ( 'Train auc score : ', roc_auc_score(y_train_us,pred_rf_train_cv))\nprint ( 'Test auc score : ', roc_auc_score(y_test,pred_rf_test_cv))\n\nprint(classification_report(y_test,y_pred))\n\nforest_us_auc_test_cv = roc_auc_score(y_test,pred_rf_test_cv)\nprecision_rf_us_cv = precision_score(y_test,y_pred)\nrecall_rf_us_cv = recall_score(y_test,y_pred)\nf1_rf_us_cv = f1_score(y_test,y_pred)","ed6b4ed9":"# plotting ROC curve on test data \nfpr, tpr, thresholds = roc_curve(y_test, pred_rf_test_cv)\nplt.figure(figsize =(7,5))\nplt.plot(fpr, tpr, label='Random Forest (area = %0.5f)' % forest_us_auc_test_cv )\nplt.plot([0, 1], [0, 1])\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","cf3e4ba2":"# Hypertunning the xgb\n\nparam_xgb = {\n    'max_depth': [4,6,8],\n    'learning_rate': [0.1,0.3,0.5,0.75],\n    'n_estimators': [100,150,200],\n    'subsample':[0.3,0.50,.75]\n    \n}\n\nxgb= XGBClassifier(booster='gbtree',\n       n_jobs=-1, objective='binary:logistic', random_state=20,\n       reg_alpha=1, reg_lambda=0)\n\nfolds= StratifiedKFold(n_splits = 3, shuffle = True, random_state = 90)\n# Instantiate\ngrid_xgb = RandomizedSearchCV(estimator = xgb, param_distributions = param_xgb, \n                          cv = folds, n_jobs = -1,verbose = 1,scoring = 'roc_auc',random_state =100)\n\ngrid_xgb.fit(X_train_us,y_train_us)","3ed99352":"#best parameters\n\ngrid_xgb.best_params_","fc06fc1c":"xgb= XGBClassifier(booster='gbtree',subsample=0.75,\n                   n_estimators = 100,\n                   max_depth = 8,\n                   learning_rate=0.3,\n                   n_jobs=-1,objective='binary:logistic', random_state=20,\n                   reg_alpha=0, reg_lambda=1)   \n\nxgb.fit(X_train_us,y_train_us)\n#prediction \n\npred_xgb_train_cv= xgb.predict_proba(X_train_us)[:,1]\npred_xgb_test_cv = xgb.predict_proba(X_test)[:,1]\n\ny_pred = xgb.predict(X_test)\n#score\nprint ( 'Train auc score : ', roc_auc_score(y_train_us,pred_xgb_train_cv))\nprint ( 'Test auc score : ', roc_auc_score(y_test,pred_xgb_test_cv))\nprint(classification_report(y_test,y_pred))\n\n\nxgb_us_auc_test_cv = roc_auc_score(y_test,pred_xgb_test_cv)\nprecision_xgb_us_cv = precision_score(y_test,y_pred)\nrecall_xgb_us_cv = recall_score(y_test,y_pred)\nf1_xgb_us_cv = f1_score(y_test,y_pred)","5d44aa0c":"# plotting ROC curve on test data \nfpr, tpr, thresholds = roc_curve(y_test, pred_xgb_test_cv )\nplt.figure(figsize =(7,5))\nplt.plot(fpr, tpr, label='XGBClassifier (area = %0.5f)' % xgb_us_auc_test_cv )\nplt.plot([0, 1], [0, 1])\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","34526f8c":"#SCORE after RandomUnderSampling\n\nauc_score_all =[log_us_auc_test_cv,forest_us_auc_test_cv,xgb_us_auc_test_cv]\nrecall_all = [recall_log_us_cv,recall_rf_us_cv,recall_xgb_us_cv]\nprecision_all = [precision_log_us_cv,precision_rf_us_cv,precision_xgb_us_cv]\nf1_all =[f1_log_us_cv,f1_rf_us_cv,f1_xgb_us_cv]\n\nus_cv = pd.DataFrame({'auc_score':auc_score_all,'recall':recall_all,'precision':precision_all,'f1_score':f1_all},index =['Logistic Regression','Random Forest','XGBoost'])\nus_cv","66da1ceb":"# USING ADASYN - oversampling tech\n\nfrom imblearn.over_sampling import ADASYN\n\nada = ADASYN (random_state=50)\nX_train_adasyn, y_train_ada = ada.fit_resample(X_train, y_train)\n\nX_train_ada =pd.DataFrame(X_train_adasyn,columns = colx) #we are creating a dataframe after the resampling\nX_train_ada.head() \n\n","9e531bdc":"#vizualisation\nX_train_adasyn_0 = X_train_adasyn[X_train.shape[0]:]  #Synthetic data points are appended after the original datapoints in the dataframe.\n                                                # Hence X_train.shape[0] - original data points and ater this length all are synthetic\n\n\n# Creating different dataframe for class 0 and 1 separately\n\nX_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\nX_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n\n\n\n\nplt.rcParams['figure.figsize'] = [20, 20]\nfig = plt.figure()\n\n#Scatter plot to show orignal class-0 data points (two columns of the same dataframe are taken for scatter plot)\nplt.subplot(3, 1, 1)\nplt.scatter(X_train_0[:, 0], X_train_0[:, 1], label='Actual Class-0 Examples') \nplt.legend()\n\n#Scatter plot for original data vs synthetic data \n\nplt.subplot(3, 1, 2)\nplt.scatter(X_train_0[:, 0], X_train_0[:, 1], label='Actual Class-0 Examples')\nplt.scatter(X_train_adasyn_0.iloc[:X_train_0.shape[0], 0], X_train_adasyn_0.iloc[:X_train_0.shape[0], 1],\n            label='Artificial ADASYN Class-0 Examples')  # X_train_0.shape[0] = 804 data points \n                                                        # X_train_adasyn_0.shape[0] = 17148 data points\n                                                        # X_train_adasyn_0[:X_train_0.shape[0], 0] - so that only 804 data points will be considered for the scatterplot\n        \nplt.legend()\n\n# Scatter plot to show distribution of original class-0 and class-1 data points\nplt.subplot(3, 1, 3)\nplt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\nplt.scatter(X_train_0[:, 0], X_train_0[:, 1], label='Actual Class-0 Examples')\nplt.legend()","e7df25fc":"X_train_ada.shape","3712446f":"X_train.shape","c1d4982f":"np.bincount(y_train_ada)","38b6e65c":"np.bincount(y_train)","67809266":"\n# Tuning logistic regression\n\nparam_log = {'penalty':['l1','l2'],'C':[0.1,.2,.3,.4,.5]}\n\nlog = LogisticRegression(class_weight ='balanced',random_state=5)\n\nfolds= StratifiedKFold(n_splits = 3, shuffle = True, random_state = 90)\ngrid_log = GridSearchCV(estimator = log, param_grid = param_log, \n                          cv = folds, n_jobs = -1,verbose = 1, scoring = 'roc_auc')\n\ngrid_log.fit(X_train_ada,y_train_ada)\n\ngrid_log.best_params_","5266a5bf":"#building model on best params\n\nlogistic = LogisticRegression(class_weight ='balanced',random_state=5,C = 0.5, penalty='l2')\n\nlogistic.fit(X_train_ada,y_train_ada)\n\n#prediction\npred_log_ada_train = logistic.predict_proba(X_train_ada)[:,1]\npred_log_ada_test = logistic.predict_proba(X_test)[:,1]\n\ny_pred = logistic.predict(X_test)\n# Score\n\nprint ( 'Train auc score : ', roc_auc_score(y_train_ada,pred_log_ada_train))\nprint ( 'Test auc score : ', roc_auc_score(y_test,pred_log_ada_test))\n\nprint(classification_report(y_test,y_pred))\n\nlog_ada_auc_test_cv = roc_auc_score(y_test, pred_log_ada_test)\nprecision_log_ada_cv = precision_score(y_test,y_pred)\nrecall_log_ada_cv = recall_score(y_test,y_pred)\nf1_log_ada_cv = f1_score(y_test,y_pred)","17742045":"# plotting ROC curve on test data\nfpr, tpr, thresholds = roc_curve(y_test, pred_log_ada_test)\nplt.figure(figsize =(7,5))\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.5f)' % log_ada_auc_test_cv)\nplt.plot([0, 1], [0, 1])\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","25e47bd6":"# Random forest\n\n# Hyperparameter tuning for random forest\n\nparam_rf = {\n    'max_depth': [8,10],\n    'min_samples_leaf': range(50, 150, 50),\n    'min_samples_split':range(50, 200, 50),\n    'n_estimators': [100,150,200], \n    'max_features': [5, 10]\n    \n}\n\nrf = RandomForestClassifier(n_jobs=-1,class_weight ='balanced',random_state=105)\n\nfolds= StratifiedKFold(n_splits = 3, shuffle = True, random_state = 90)\n# Instantiate\ngrid_rf = RandomizedSearchCV(estimator = rf, param_distributions = param_rf, \n                          cv = folds, n_jobs = -1,verbose = 1,scoring = 'roc_auc',random_state =100)\n\n\n# Fitting\ngrid_rf.fit(X_train_ada,y_train_ada)\n\n","58283df2":"#best params\n\ngrid_rf.best_params_","8d444b00":"#using best params\n\nforest_cv = RandomForestClassifier(n_estimators=150,\n                                   min_samples_split=50, min_samples_leaf=50,\n                                   max_features=10,max_depth=10,\n                                   n_jobs=-1,class_weight ='balanced',random_state=1055)\n\nforest_cv.fit(X_train_ada,y_train_ada)\n\n\n#prediction\n\npred_rf_train_cv= forest_cv.predict_proba(X_train_ada)[:,1]\npred_rf_test_cv = forest_cv.predict_proba(X_test)[:,1]\n\ny_pred = forest_cv.predict(X_test)\n#score\nprint ( 'Train auc score : ', roc_auc_score(y_train_ada,pred_rf_train_cv))\nprint ( 'Test auc score : ', roc_auc_score(y_test,pred_rf_test_cv))\n\nprint(classification_report(y_test,y_pred))\n\nforest_ada_auc_test_cv = roc_auc_score(y_test,pred_rf_test_cv)\nprecision_rf_ada_cv = precision_score(y_test,y_pred)\nrecall_rf_ada_cv = recall_score(y_test,y_pred)\nf1_rf_ada_cv = f1_score(y_test,y_pred)","cc9e6bd9":"# plotting ROC curve on test data \nfpr, tpr, thresholds = roc_curve(y_test, pred_rf_test_cv)\nplt.figure(figsize =(7,5))\nplt.plot(fpr, tpr, label='Random Forest (area = %0.5f)' % forest_ada_auc_test_cv )\nplt.plot([0, 1], [0, 1])\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","98ed5b5b":"# Hypertunning the xgb\nparam_xgb = {\n    'max_depth': [4,6,8],\n    'learning_rate': [0.1,0.3,0.5,0.75],\n    'n_estimators': [100,150,200],\n    'subsample':[0.3,0.50,.75]\n    \n}\n\nxgb= XGBClassifier(booster='gbtree',\n       n_jobs=-1, objective='binary:logistic', random_state=20,\n       reg_alpha=1, reg_lambda=0)\n\nfolds= StratifiedKFold(n_splits = 3, shuffle = True, random_state = 90)\n# Instantiate\ngrid_xgb = RandomizedSearchCV(estimator = xgb, param_distributions = param_xgb, \n                          cv = folds, n_jobs = -1,verbose = 1,scoring = 'roc_auc',random_state =100)\n\ngrid_xgb.fit(X_train_ada,y_train_ada)","a0a1b6e0":"grid_xgb.best_params_","f822916f":"xgb_ada= XGBClassifier(booster='gbtree',subsample=0.75,\n                   n_estimators = 100,\n                   max_depth = 4,\n                   learning_rate=0.75,\n                   n_jobs=-1,objective='binary:logistic', random_state=20,\n                   reg_alpha=0, reg_lambda=1)   \n\nxgb_ada.fit(X_train_ada,y_train_ada)\n#prediction \n\npred_xgb_train_cv= xgb_ada.predict_proba(X_train_ada)[:,1]\npred_xgb_test_cv = xgb_ada.predict_proba(X_test)[:,1]\n\ny_pred = xgb_ada.predict(X_test)\n#score\nprint ( 'Train auc score : ', roc_auc_score(y_train_ada,pred_xgb_train_cv))\nprint ( 'Test auc score : ', roc_auc_score(y_test,pred_xgb_test_cv))\nprint(classification_report(y_test,y_pred))\n\n\nxgb_ada_auc_test_cv = roc_auc_score(y_test,pred_xgb_test_cv)\nprecision_xgb_ada_cv = precision_score(y_test,y_pred)\nrecall_xgb_ada_cv = recall_score(y_test,y_pred)\nf1_xgb_ada_cv = f1_score(y_test,y_pred)","ecab3a51":"# plotting ROC curve on test data \nfpr, tpr, thresholds = roc_curve(y_test, pred_xgb_test_cv )\nplt.figure(figsize =(7,5))\nplt.plot(fpr, tpr, label='XGBClassifier (area = %0.5f)' % xgb_ada_auc_test_cv )\nplt.plot([0, 1], [0, 1])\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","0bad9fa0":"#SCORE after ADASYN\n\nauc_score_all =[log_ada_auc_test_cv,forest_ada_auc_test_cv,xgb_ada_auc_test_cv]\nrecall_all = [recall_log_ada_cv,recall_rf_ada_cv,recall_xgb_ada_cv]\nprecision_all = [precision_log_ada_cv,precision_rf_ada_cv,precision_xgb_ada_cv]\nf1_all =[f1_log_ada_cv,f1_rf_ada_cv,f1_xgb_ada_cv]\n\nada_cv = pd.DataFrame({'auc_score':auc_score_all,'recall':recall_all,'precision':precision_all,'f1_score':f1_all},index =['Logistic Regression','Random Forest','XGBoost'])\nada_cv","fc626138":"#Importing SMOTE - oversampling tech\n\nfrom imblearn.over_sampling import SMOTE\n\nsmt = SMOTE(random_state=42)\nX_train_sm, y_train_sm = smt.fit_resample(X_train, y_train)","c6125964":"X_train_sm","398069da":"# Visualising the distribution of actual and synthetic datapoints\n\nX_train_smote_0 = X_train_sm[X_train.shape[0]:] # Synthetic data points are appended after the original datapoints in the dataframe.\n                                                # Hence X_train.shape[0] - original data points and ater this length all are synthetic\n\n    \n# Creating different dataframe for class 0 and 1 separately\n\nX_train_1 = X_train.to_numpy()[np.where(y_train==1.0)] \nX_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n\n\nplt.rcParams['figure.figsize'] = [20, 20]\nfig = plt.figure()\n\n\n#Scatter plot to show orignal class-0 data points (two columns of the same dataframe are taken for scatter plot)\nplt.subplot(3, 1, 1)\nplt.scatter(X_train_0[:, 0], X_train_0[:, 1], label='Actual Class-0 Examples') \nplt.legend()\n\n#Scatter plot for original data vs synthetic data \nplt.subplot(3, 1, 2)\nplt.scatter(X_train_0[:, 0], X_train_0[:, 1], label='Actual Class-0 Examples')\nplt.scatter(X_train_smote_0.iloc[:X_train_0.shape[0], 0], X_train_smote_0.iloc[:X_train_0.shape[0], 1],\n            label='Artificial SMOTE Class-0 Examples')  # X_train_0.shape[0] = 804 data points \n                                                        # X_train_smote_0.shape[0] = 17148 data points\n                                                        # X_train_smote_0[:X_train_0.shape[0], 0] - so that only 804 data points will be considered for the scatterplot\n        \nplt.legend()\n\n\n# Scatter plot to show distribution of original class-0 and class-1 data points\nplt.subplot(3, 1, 3)\nplt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\nplt.scatter(X_train_0[:, 0], X_train_0[:, 1], label='Actual Class-0 Examples')\nplt.legend()","dc73c506":"X_train_sm = pd.DataFrame(X_train_sm,columns = colx) #colx = columns of X_train\nX_train_sm.shape #after SMOTE","9f77099e":"X_train.shape #original","d93d15f0":"np.bincount(y_train_sm) #after SMOTE","10b225d2":"np.bincount(y_train) #original","c06a9e9e":"# Hyperparameters Tunning\n#Logistic regression\n\nparam_log = {'penalty':['l1','l2'],'C':[0.1,.2,.3,.4,.5]}\n\nlog = LogisticRegression(class_weight ='balanced',random_state=5)\n\nfolds= StratifiedKFold(n_splits = 3, shuffle = True, random_state = 90)\ngrid_log = GridSearchCV(estimator = log, param_grid = param_log, \n                          cv = folds, n_jobs = -1,verbose = 1, scoring = 'roc_auc')\n\ngrid_log.fit(X_train_sm,y_train_sm)\n\ngrid_log.best_params_","f1940e7a":"#building model on best params\n\nlogistic = LogisticRegression(class_weight ='balanced',random_state=5,C = 0.5, penalty='l2')\n\nlogistic.fit(X_train_sm,y_train_sm)\n\n#prediction\npred_log_sm_train = logistic.predict_proba(X_train_sm)[:,1]\npred_log_sm_test = logistic.predict_proba(X_test)[:,1]\n\ny_pred = logistic.predict(X_test)\n\n# Score\n\nprint ( 'Train auc score : ', roc_auc_score(y_train_sm,pred_log_sm_train))\nprint ( 'Test auc score : ', roc_auc_score(y_test,pred_log_sm_test))\nprint(classification_report(y_test,y_pred))\n\nlog_sm_auc_test_cv = roc_auc_score(y_test, pred_log_sm_test)\nprecision_log_sm_cv = precision_score(y_test,y_pred)\nrecall_log_sm_cv = recall_score(y_test,y_pred)\nf1_log_sm_cv = f1_score(y_test,y_pred)","705559d9":"# plotting ROC curve on test data\nfpr, tpr, thresholds = roc_curve(y_test, pred_log_sm_test)\nplt.figure(figsize =(7,5))\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.5f)' % log_sm_auc_test_cv)\nplt.plot([0, 1], [0, 1])\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","45a3c27a":"# RandomForestClassifier\n# Hyperparameter tuning for random forest\n\nparam_rf = {\n    'max_depth': [8,10],\n    'min_samples_leaf': range(50, 150, 50),\n    'min_samples_split':range(50, 200, 50),\n    'n_estimators': [100,150,200], \n    'max_features': [5, 10]\n    \n}\n\nrf = RandomForestClassifier(n_jobs=-1,class_weight ='balanced',random_state=105)\n\nfolds= StratifiedKFold(n_splits = 3, shuffle = True, random_state = 90)\n\ngrid_rf = RandomizedSearchCV(estimator = rf, param_distributions = param_rf, \n                          cv = folds, n_jobs = -1,verbose = 1,scoring = 'roc_auc',random_state =100)\n\n\n# Fitting\ngrid_rf.fit(X_train_sm, y_train_sm)\n\n","433e88a0":"#best params\n\ngrid_rf.best_params_","d469c422":"#using best params\n\nforest_cv = RandomForestClassifier(n_estimators=150,\n                                   min_samples_split=50, min_samples_leaf=50,\n                                   max_features=10,max_depth=10,\n                                   n_jobs=-1,class_weight ='balanced',random_state=105)\n\nforest_cv.fit(X_train_sm, y_train_sm)\n\n\n#prediction\n\npred_rf_train_cv= forest_cv.predict_proba(X_train_sm)[:,1]\npred_rf_test_cv = forest_cv.predict_proba(X_test)[:,1]\n\ny_pred = forest_cv.predict(X_test)\n\n#score\nprint ( 'Train auc score : ', roc_auc_score(y_train_sm,pred_rf_train_cv))\nprint ( 'Test auc score : ', roc_auc_score(y_test,pred_rf_test_cv))\nprint(classification_report(y_test,y_pred))\n\nforest_sm_auc_test_cv = roc_auc_score(y_test,pred_rf_test_cv)\nprecision_rf_sm_cv = precision_score(y_test,y_pred)\nrecall_rf_sm_cv = recall_score(y_test,y_pred)\nf1_rf_sm_cv = f1_score(y_test,y_pred)\n","eef2aed4":"# plotting ROC curve on test data \nfpr, tpr, thresholds = roc_curve(y_test, pred_rf_test_cv)\nplt.figure(figsize =(7,5))\nplt.plot(fpr, tpr, label='Random Forest (area = %0.5f)' % forest_sm_auc_test_cv )\nplt.plot([0, 1], [0, 1])\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","f237beb0":"#XGBClassifier\n# Hyperparameter tuning \n\nparam_xgb = {\n    'max_depth': [4,6,8],\n    'learning_rate': [0.1,0.3,0.5,0.75],\n    'n_estimators': [100,150,200],\n    'subsample':[0.3,0.50,.75]\n    \n}\n\nxgb= XGBClassifier(booster='gbtree',\n       n_jobs=-1, objective='binary:logistic', random_state=20,\n       reg_alpha=1, reg_lambda=0)\n\nfolds= StratifiedKFold(n_splits = 3, shuffle = True, random_state = 90)\n# Instantiate \ngrid_xgb = RandomizedSearchCV(estimator = xgb, param_distributions = param_xgb, \n                          cv = folds, n_jobs = -1,verbose = 1,scoring = 'roc_auc',random_state =100)\n\ngrid_xgb.fit(X_train_sm, y_train_sm)","5332b5b7":"#best parameters\n\ngrid_xgb.best_params_","d9cfa5f5":"xgb= XGBClassifier(booster='gbtree',subsample=0.75,\n                   n_estimators = 150,\n                   max_depth = 4,\n                   learning_rate=0.3,\n                   n_jobs=-1,objective='binary:logistic', random_state=20,\n                   reg_alpha=1, reg_lambda=0)   \n\nxgb.fit(X_train_sm, y_train_sm)\n#prediction \n\npred_xgb_train_cv= xgb.predict_proba(X_train_sm)[:,1]\npred_xgb_test_cv = xgb.predict_proba(X_test)[:,1]\n\ny_pred = xgb.predict(X_test)\n\n#score\nprint ( 'Train auc score : ', roc_auc_score(y_train_sm,pred_xgb_train_cv))\nprint ( 'Test auc score : ', roc_auc_score(y_test,pred_xgb_test_cv))\nprint(classification_report(y_test,y_pred))\n\nxgb_sm_auc_test_cv = roc_auc_score(y_test,pred_xgb_test_cv)\nprecision_xgb_sm_cv = precision_score(y_test,y_pred)\nrecall_xgb_sm_cv = recall_score(y_test,y_pred)\nf1_xgb_sm_cv = f1_score(y_test,y_pred)\n","2ddcf101":"# plotting ROC curve on test data \nfpr, tpr, thresholds = roc_curve(y_test, pred_xgb_test_cv )\nplt.figure(figsize =(7,5))\nplt.plot(fpr, tpr, label='XGBClassifier (area = %0.5f)' % xgb_sm_auc_test_cv)\nplt.plot([0, 1], [0, 1])\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","20ea3a90":"# scores \n\nauc_score_all =[log_sm_auc_test_cv,forest_sm_auc_test_cv,xgb_sm_auc_test_cv]\nrecall_all = [recall_log_sm_cv,recall_rf_sm_cv,recall_xgb_sm_cv]\nprecision_all = [precision_log_sm_cv,precision_rf_sm_cv,precision_xgb_sm_cv]\nf1_all =[f1_log_sm_cv,f1_rf_sm_cv,f1_xgb_sm_cv]\n\nsm_cv = pd.DataFrame({'auc_score':auc_score_all,'recall':recall_all,'precision':precision_all,'f1_score':f1_all},index =['Logistic Regression','Random Forest','XGBoost'])\nsm_cv","e43922e7":"us_cv","c4aef1eb":"ada_cv","0059dbb5":"\nhack= xgb_ada.predict(test_df)\n\nresult = pd.DataFrame({'INCIDENT_ID':test_df_id,'MULTIPLE_OFFENSE':hack})\nresult.head()","6c39829e":"#Important features\nxgb_ada.feature_importances_\n","fd6b97c0":"# Important features\n\nplot_importance(xgb,importance_type='gain')\nplt.show()","fb9f3e1e":"result.to_csv ('subx.csv', index = None, header=True)\npd.read_csv('subx.csv').head()","92c93f16":"#saving the final model as pickle file\n\nimport pickle\n\n#open a file where you want to store the model\nfile = open('xgb.pkl','wb')\n\npickle.dump(xgb,file) #dumping \/ saving the model inside file xgb.pkl","e177a5c8":"### 3.3. Checking Skewness :","6d3983d8":"### Observations:\n\n#### 1. Detail explanation is not given for variables X_1 to X_15 ( Original nature of these variables is hidden to ensure confidentiality)\n\n#### 2. From the above value count, it is observed that variables like X_1,X_4,X_5,X_9,etc. could be categorical variables encoded as integers\n#### 3. There are total 4772 number of data points available\n#### 4. The column X_12 has some missing values","d7b772ff":"### 10. Submission:","20543a19":"### Observation :\n\n#### 1. Variables X_8, X_10 and X_12 have extreme values. Clearly these are outliers. It is recommended to remove rows where values of each column are greater than their respective 99.5 percentile. But, here we can't alter the number of rows for test_raw dataframe as we need the exact number for submission. Hence it is decided not to remove outliers.\n\n#### 2. Outliers from train_df were removed in order to get a good dataframe to build ML models","378ea3ff":"#### Observation :\n##### 1.Variable X_12 has 0.8 % null values. \n##### 2. X_12 also has outliers (From earlier outlier analysis)\n##### 3. As we can not alter the number of rows in test_raw dataframe and X_12 has outliers, it is decided to impute null values with median of X_12\n","770e80f9":"#### Observation :\n##### 1.Above plots show that many variables are categorical in nature.\n##### 2. No skewness treatment is required\n","75778ce6":"### Observations:\n\n#### 1. Detail explanation is not given for variables X_1 to X_15 ( Original nature of these variables is hidden to ensure confidentiality)\n\n#### 2. From the above value count, it is observed that variables like X_1,X_4,X_5,X_9,etc. could be categorical variables encoded as integers\n#### 3. There are total 19084 number of data points available\n#### 4. The column X_12 has some missing values","c9c3b95f":"### 5. Spiliting data set into train and test","a615d7f0":"#Kaggle link\n\nhttps:\/\/www.kaggle.com\/c\/system-hack-highly-imbalanced-data","35f62369":"#### Dataframe is highly imbalanced ","48e9eaff":" #### Please note that after hyperparameter tunning using cross validation, certain parameters are tunned manually by taking different values. Hence final model contains parameters either from the result of cross validation or manual tunning.\n    ","fe8a8e04":"### Observation :\n\n#### Variables X_8, X_10 and X_12 have extreme values. Clearly these are outliers. It is decided to remove rows where values of each column are greater than their respective 99.5 percentile (as number of records are available are less).","b95f1653":"### 3.2. Searching for missing values \/ null values :","6a4a51ca":"\n### 1. Importing Required Libraries","32cf3320":"### Table of contents:\n#### 1. Importing required libraries\n#### 2. Reading raw csv files\n#### 3. Exploratory data analysis & data treatment\n#### 4. Feature engineering\n#### 5. Spiliting data set into train and test\n#### 6. Standardizing\n#### 7. Treating Imbalance \n#### 8. Model building\n#### 9. Prediction on test data\n#### 10. Submission","f05a24fd":"### 7. Treating Imbalance + Model Building :","073cf22a":"#### Observation : No null value present in the dataframe train_df . This is a result of earlier outlier removal.\n","c8612179":"### 3.1. Searching for outliers\n","51624e07":"### Observations:\n\n#### Score of XGBClassifier is better than that of other two models. Hence XGBClassifier is selected as the final model.","374ffd66":"#### What is imbalanced data ?\nhttps:\/\/www.youtube.com\/watch?v=FjGvdvK77vo","f585b131":"### 6. Standardizing","070125bc":"### 4. Feature engineering:","5ebf2e69":"### 3. Exploratory Data Analysis & Data Treatment","f9954751":"### 9. Prediction on test data:","0606afb8":"### 2. Reading raw csv files"}}