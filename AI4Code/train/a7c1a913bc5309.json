{"cell_type":{"6bc55107":"code","3990c811":"code","63e11d3e":"code","fef3b4e2":"code","36e1721d":"code","1727de3a":"code","264e6bde":"code","2aa6df1f":"code","f353c55f":"code","7f98ab8f":"code","320c3d2c":"code","1f9ba404":"code","a92c2c0b":"code","4a1e3bc5":"code","f9ddb285":"code","bb59eb28":"code","235889e4":"code","fa4d67ef":"code","c8c2037d":"markdown"},"source":{"6bc55107":"import pandas as pd\nimport numpy as np\nimport json\nimport tensorflow.keras.layers as L\nimport tensorflow as tf\nimport plotly.express as px","3990c811":"train = pd.read_json('\/kaggle\/input\/stanford-covid-vaccine\/train.json', lines=True)\ntest = pd.read_json('\/kaggle\/input\/stanford-covid-vaccine\/test.json', lines=True)\nsample_df = pd.read_csv('\/kaggle\/input\/stanford-covid-vaccine\/sample_submission.csv')","63e11d3e":"train.head()","fef3b4e2":"test.head()","36e1721d":"sample_df.head()","1727de3a":"sample_df.shape","264e6bde":"pred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']","2aa6df1f":"def gru_layer(hidden_dim, dropout):\n    return L.Bidirectional(L.GRU(hidden_dim, dropout=dropout, return_sequences=True))\n\ndef build_model(seq_len=107, pred_len=68, dropout=0.5, embed_dim=100, hidden_dim=128):\n    inputs = L.Input(shape=(seq_len, 3))\n\n    embed = L.Embedding(input_dim=len(token2int), output_dim=embed_dim)(inputs)\n    reshaped = tf.reshape(\n        embed, shape=(-1, embed.shape[1],  embed.shape[2] * embed.shape[3]))\n\n    hidden = gru_layer(hidden_dim, dropout)(reshaped)\n    hidden = gru_layer(hidden_dim, dropout)(hidden)\n    hidden = gru_layer(hidden_dim, dropout)(hidden)\n    \n    # Since we are only making predictions on the first part of each sequence, we have\n    # to truncate it\n    truncated = hidden[:, :pred_len]\n    \n    out = L.Dense(5, activation='linear')(truncated)\n\n    model = tf.keras.Model(inputs=inputs, outputs=out)\n\n    model.compile(tf.keras.optimizers.Adam(), loss='mse')\n    \n    return model","f353c55f":"token2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}\n\ndef preprocess_inputs(df, cols=['sequence', 'structure', 'predicted_loop_type']):\n    return np.transpose(\n        np.array(\n            df[cols]\n            .applymap(lambda seq: [token2int[x] for x in seq])\n            .values\n            .tolist()\n        ),\n        (0, 2, 1)\n    )","7f98ab8f":"train_inputs = preprocess_inputs(train)\ntrain_labels = np.array(train[pred_cols].values.tolist()).transpose((0, 2, 1))","320c3d2c":"model = build_model()\nmodel.summary()","1f9ba404":"history = model.fit(\n    train_inputs, train_labels, \n    batch_size=64,\n    epochs=60,\n    callbacks=[\n        tf.keras.callbacks.ReduceLROnPlateau(),\n        tf.keras.callbacks.ModelCheckpoint('model.h5')\n    ],\n    validation_split=0.3\n)","a92c2c0b":"fig = px.line(\n    history.history, y=['loss', 'val_loss'], \n    labels={'index': 'epoch', 'value': 'Mean Squared Error'}, \n    title='Training History')\nfig.show()","4a1e3bc5":"public_df = test.query(\"seq_length == 107\").copy()\nprivate_df = test.query(\"seq_length == 130\").copy()\n\npublic_inputs = preprocess_inputs(public_df)\nprivate_inputs = preprocess_inputs(private_df)","f9ddb285":"# Caveat: The prediction format requires the output to be the same length as the input,\n# although it's not the case for the training data.\nmodel_short = build_model(seq_len=107, pred_len=107)\nmodel_long = build_model(seq_len=130, pred_len=130)\n\nmodel_short.load_weights('model.h5')\nmodel_long.load_weights('model.h5')\n\npublic_preds = model_short.predict(public_inputs)\nprivate_preds = model_long.predict(private_inputs)","bb59eb28":"print(public_preds.shape, private_preds.shape)\n","235889e4":"preds_ls = []\n\nfor df, preds in [(public_df, public_preds), (private_df, private_preds)]:\n    for i, uid in enumerate(df.id):\n        single_pred = preds[i]\n\n        single_df = pd.DataFrame(single_pred, columns=pred_cols)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n        preds_ls.append(single_df)\n\npreds_df = pd.concat(preds_ls)","fa4d67ef":"submission = sample_df[['id_seqpos']].merge(preds_df, on=['id_seqpos'])\nsubmission.to_csv('submission.csv', index=False)","c8c2037d":"# GRU\nThe GRU is the newer generation of Recurrent Neural networks and is pretty similar to an LSTM. GRU\u2019s got rid of the cell state and used the hidden state to transfer information. It also only has two gates, a reset gate and update gate.\n\nThe GRU, known as the Gated Recurrent Unit is an RNN architecture, which is similar to LSTM units. The GRU comprises of the reset gate and the update gate instead of the input, output and forget gate of the LSTM.\nThe reset gate determines how to combine the new input with the previous memory, and the update gate defines how much of the previous memory to keep around. If we set the reset to all 1\u2019s and update gate to all 0\u2019s we again arrive at our plain RNN model.\n\nFor the GRU the hidden state h\u209c can be calculated as follows:\n![img](https:\/\/miro.medium.com\/max\/490\/1*VCgvWe30mFdSvK93p7zUJg.png)\n![img](https:\/\/miro.medium.com\/max\/875\/1*jhi5uOm9PvZfmxvfaCektw.png)\n## Update Gate\nThe update gate acts similar to the forget and input gate of an LSTM. It decides what information to throw away and what new information to add.\n## Reset Gate\nThe reset gate is another gate is used to decide how much past information to forget.\nAnd that\u2019s a GRU. GRU\u2019s has fewer tensor operations; therefore, they are a little speedier to train then LSTM\u2019s. There isn\u2019t a clear winner which one is better. Researchers and engineers usually try both to determine which one works better for their use case."}}