{"cell_type":{"65892304":"code","36eeb9cf":"code","afe66292":"code","5c225ddb":"code","188a900e":"code","1e5b3ecc":"code","dfed375e":"markdown","0ca87633":"markdown","c677dba7":"markdown","78592cde":"markdown","4708917d":"markdown"},"source":{"65892304":"import torch","36eeb9cf":"x = torch.randn(3, requires_grad=True)\nprint(x)\n\ny = x+2\nprint(y)\n\nz = y*y*2\nprint(z)\n\nz = z.mean()\nprint(z)\n\n# Calculate the gradient\nz.backward() # dz\/dx\nprint(x.grad)","afe66292":"x = torch.randn(3, requires_grad=True)\nprint(x)\n\ny = x+2\nprint(y)\n\nz = y*y*2\nprint(z)\n\n# Because backward is Vector Jacobian Product, you need a vector ! \nv = torch.tensor([0.1,1.0,0.001], dtype = torch.float32)\n\n# Calculate the gradient\nz.backward(v) # dz\/dx\nprint(x.grad)","5c225ddb":"print(x)\n\n# Prevent the gradient to calculate\nv = x.detach()\nb = x.requires_grad_(False)\n# x.requires_grad_(False)\n\nprint(b)\nprint(v)\n\n# Prevent the gradient to calculate\nwith torch.no_grad():\n    g = y+2\n    print(g)\n\n# Still calculate the gradient\ng = y+2 \nprint(g)","188a900e":"weights = torch.ones(4, requires_grad=True)\n\nfor epoch in range(3):\n    model_output = (weights*3).sum()\n    \n    model_output.backward()\n    print(weights.grad)\n    \n    #weights.grad.zero_() # You need to re-assgin to zero\n    #print(weights.grad)","1e5b3ecc":"x = torch.tensor(1.0)\ny = torch.tensor(2.0)\n\nw = torch.tensor(1.0, requires_grad=True)\n\n# forward pass and compute the loss\ny_hat = w * x\nloss = (y_hat-y)**2\n\nprint(loss)\n\n# backward pass\nloss.backward()\nprint(w.grad)\n\n## update weights\n## next forwards and backwards","dfed375e":"## grad in looping","0ca87633":"## Calculate Gradient","c677dba7":"## Backpropagation","78592cde":"## If the input are not scalar","4708917d":"## 3 Ways to prevent for tracking the gradients\n\n```python\n1. x.requires_grad_(False) # Inplacing the gradient to false\n2. x.detach() # Create new tensor that doesn't required the gradient\n3. with torch.no_grad(): \n    pass\n```"}}