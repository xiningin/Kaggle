{"cell_type":{"71b0e3ba":"code","12091d47":"code","1145dae8":"code","3e4ffce8":"code","980f2c4a":"code","13f67995":"code","b2c5796f":"code","c1819b82":"code","45c47fa8":"code","42597374":"code","46dadb03":"code","c225e324":"code","c8e598f5":"code","6e02aa9b":"code","c449b4e9":"code","3efe4313":"code","16d434f7":"code","ce2d8ebd":"code","66de7caf":"code","e900b6ea":"code","0222ead4":"code","95cdde2d":"code","ac3c1733":"code","7a97c962":"code","b2d3d24b":"markdown","ad357f92":"markdown","47d7a532":"markdown","fd8f3148":"markdown","e891565a":"markdown","b74284a8":"markdown","e2ae08ce":"markdown","b8c6c20f":"markdown","d2852471":"markdown","ea0111c8":"markdown","df38fda3":"markdown","9ef7b5ab":"markdown","263fbc9e":"markdown","8710a829":"markdown","8b64f82f":"markdown","fce199a9":"markdown","5f1ad25d":"markdown","75450475":"markdown","50e3b732":"markdown","f9cc9c54":"markdown","e367ec23":"markdown"},"source":{"71b0e3ba":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","12091d47":"df = pd.read_csv(\"..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv\") # Include of the dataset","1145dae8":"print(\"Data shape is: \",df.shape)","3e4ffce8":"df.head() # First 5 value","980f2c4a":"df.info() # Getting info of data","13f67995":"df.describe().transpose() # We are getting statistical values of data.","b2c5796f":"# For data visualization\ncategorical = ['sex','exng','caa','cp','fbs','restecg','slp','thall']\nnumerical = [\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]","c1819b82":"#Correlation Matrix\n\nf = plt.figure(figsize=(18, 10))\nplt.matshow(df.corr(), fignum=f.number)\nplt.xticks(range(df.select_dtypes(['number']).shape[1]), df.select_dtypes(['number']).columns, fontsize=16, rotation=45)\nplt.yticks(range(df.select_dtypes(['number']).shape[1]), df.select_dtypes(['number']).columns, fontsize=16)\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=16)\nplt.title('Correlation Matrix', fontsize=16);","45c47fa8":"# We have 9 collon so we create 9 figures.(ax0, ax1, ..., ax8)\n\nfig = plt.figure(figsize=(20,20))\n\ngr = fig.add_gridspec(3,3)\n\nax0 = fig.add_subplot(gr[0,0])\nax1 = fig.add_subplot(gr[0,1])\nax2 = fig.add_subplot(gr[0,2])\nax3 = fig.add_subplot(gr[1,0])\nax4 = fig.add_subplot(gr[1,1])\nax5 = fig.add_subplot(gr[1,2])\nax6 = fig.add_subplot(gr[2,0])\nax7 = fig.add_subplot(gr[2,1])\nax8 = fig.add_subplot(gr[2,2])\n\naxxes = [ax0,ax1,ax2,ax3,ax4,ax5,ax6,ax7]\n\nfor i in range(0,len(categorical)):\n    axxes[i].grid(color='#000000', linestyle=':', axis='y')\n    sns.countplot(ax=axxes[i],data=df,x=df[categorical[i]],palette=[\"#\"+str(i)+str(i)+\"5353\"]) #so that the figures change colors in each cycle\n#NOT: if str(i) is greater than 9 it will throw an error, be careful\n    \n#ax8 is target colon \nax8.grid(color='#000000', linestyle=':', axis='y')\nsns.countplot(ax=ax8,data=df,x=df[\"output\"],palette=[\"#53535353\"])\nax8.set_xticklabels([\"Low chances of attack(0)\",\"High chances of attack(1)\"])\nax8.set_facecolor(\"#FF5353\") ","42597374":"fig = plt.figure(figsize=(20,20))\n\ngr = fig.add_gridspec(2,3)\n\nax0 = fig.add_subplot(gr[0,0])\nax1 = fig.add_subplot(gr[0,1])\nax2 = fig.add_subplot(gr[0,2])\nax3 = fig.add_subplot(gr[1,0])\nax4 = fig.add_subplot(gr[1,1])\naxxes = [ax0,ax1,ax2,ax3,ax4]\n\nfor i in range(0,len(numerical)):\n    axxes[i].grid(color='#000000', linestyle=':', axis='y')\n    sns.boxenplot(ax=axxes[i],y=df[numerical[i]],palette=[\"#\"+str(i*2)+\"00000\"],width=0.6)\n#if str(i)*2 is greater than 9 it will throw an error, be careful","46dadb03":"X = df.iloc[:,:-1] # We get all but the output column\ny = pd.DataFrame(df[\"output\"]) # We get output column\n\nprint(\"Shape of X:\",X.shape,\"\\nShape of Y:\",y.shape)","c225e324":"#Splitting of data.(train and test)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X,y, test_size = 0.15, random_state = 53)","c8e598f5":"print(\"Shape of x train:\",x_train.shape,\"\\nShape of y train:\", y_train.shape)\nprint(\"Shape of x test:\",x_test.shape,\"\\nShape of y test:\", y_test.shape)","6e02aa9b":"# Importing metrics libraries of sklearn\nfrom sklearn.metrics import accuracy_score, roc_auc_score, f1_score","c449b4e9":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)","3efe4313":"acc_df = pd.DataFrame(columns=[\"Name\", \"Accuracy_score\", \"AUC_score\", \"F1_score\"])","16d434f7":"from sklearn.svm import SVC\n\nmodel = SVC().fit(x_train,y_train)\ny_pred = model.predict(x_test)\n\nacc = accuracy_score(y_test, y_pred)\nauc = roc_auc_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nprint(\"The test accuracy score is \", acc)\nprint(\"The test AUC score is\", auc)\nprint(\"The test F1 score is\", f1)\n\na_series = pd.Series([\"SVC\", acc, auc, f1], index = acc_df.columns)\nacc_df = acc_df.append(a_series, ignore_index=True)","ce2d8ebd":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression().fit(x_train, y_train)\ny_pred = model.predict(x_test)\n\nacc = accuracy_score(y_test, y_pred)\nauc = roc_auc_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(\"The test accuracy score is \", acc)\nprint(\"The test AUC score is\", auc)\nprint(\"The test F1 score is\", f1)\n\na_series = pd.Series([\"LogisticRegression\", acc, auc, f1], index = acc_df.columns)\nacc_df = acc_df.append(a_series, ignore_index=True)","66de7caf":"from sklearn.naive_bayes import GaussianNB\n\nmodel = GaussianNB().fit(x_train, y_train)\ny_pred = model.predict(x_test)\n\nacc = accuracy_score(y_test, y_pred)\nauc = roc_auc_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(\"The test accuracy score is \", acc)\nprint(\"The test AUC score is\", auc)\nprint(\"The test F1 score is\", f1)\n\na_series = pd.Series([\"GaussianNB\", acc, auc, f1], index = acc_df.columns)\nacc_df = acc_df.append(a_series, ignore_index=True)","e900b6ea":"from sklearn.naive_bayes import BernoulliNB\n\nmodel = BernoulliNB().fit(x_train, y_train)\ny_pred = model.predict(x_test)\n\nacc = accuracy_score(y_test, y_pred)\nauc = roc_auc_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(\"The test accuracy score is \", acc)\nprint(\"The test AUC score is\", auc)\nprint(\"The test F1 score is\", f1)\n\na_series = pd.Series([\"BernoulliNB\", acc, auc, f1], index = acc_df.columns)\nacc_df = acc_df.append(a_series, ignore_index=True)","0222ead4":"from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(n_estimators = 100, max_depth=4).fit(x_train, y_train)  \ny_pred = model.predict(x_test)\n\n\nacc = accuracy_score(y_test, y_pred)\nauc = roc_auc_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(\"The test accuracy score is \", acc)\nprint(\"The test AUC score is\", auc)\nprint(\"The test F1 score is\", f1)\n\na_series = pd.Series([\"RandomForest\", acc, auc, f1], index = acc_df.columns)\nacc_df = acc_df.append(a_series, ignore_index=True)","95cdde2d":"from sklearn.neighbors import KNeighborsClassifier\n\nmodel = KNeighborsClassifier(n_neighbors= 4).fit(x_train, y_train)  \ny_pred = model.predict(x_test)\n\nacc = accuracy_score(y_test, y_pred)\nauc = roc_auc_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(\"The test accuracy score is \", acc)\nprint(\"The test AUC score is\", auc)\nprint(\"The test F1 score is\", f1)\n\na_series = pd.Series([\"KNN\", acc, auc, f1], index = acc_df.columns)\nacc_df = acc_df.append(a_series, ignore_index=True)","ac3c1733":"from sklearn.ensemble import GradientBoostingClassifier\n\nmodel = GradientBoostingClassifier(n_estimators=200, learning_rate=0.05, max_depth=1).fit(x_train, y_train) \ny_pred = model.predict(x_test)\n\nacc = accuracy_score(y_test, y_pred)\nauc = roc_auc_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(\"The test accuracy score is \", acc)\nprint(\"The test AUC score is\", auc)\nprint(\"The test F1 score is\", f1)\n\na_series = pd.Series([\"GradientBoosting\", acc, auc, f1], index = acc_df.columns)\nacc_df = acc_df.append(a_series, ignore_index=True)","7a97c962":"acc_df","b2d3d24b":"In general, the models have good predictive values. The best guess among them is the BernoulliNB model, as it seems.\n\n# I tried to explain as best I could. If you've come this far and liked this review, don't forget to hit the arrow in the upper right corner. Hope to see you in different reviews.","ad357f92":"# **Modelling and Predictions of models**","47d7a532":"NOT: We are creating a new dataset because we will examine many models and there will be many metric scoring to evaluate them collectively.","fd8f3148":"Naive Bayes classifier for multivariate Bernoulli models.\n\nLike **MultinomialNB**, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary\/boolean features.","e891565a":"> Our task is to make a prediction about whether the patient will have a heart attack with the given variables. Let's start.","b74284a8":"# When it comes to being beaten to your heart, I always think of good things, not a heart attack: compassion, goodness, contentment, humility, truth and fairness... And a wish: May your heart be whole.","e2ae08ce":"**StandardScaler**\n\nStandardize features by removing the mean and scaling to unit variance.\n\nThe standard score of a sample x is calculated as:\n\nz = (x - u) \/ s\n\nwhere u is the mean of the training samples or zero if with_mean=False, and s is the standard deviation of the training samples or one if with_std=False.","b8c6c20f":"# **Information About Data**\n\n**Age :** Age of the patient\n\n**Sex :** Sex of the patient \n\n**exang:** exercise induced angina (1 = yes; 0 = no)\n\n**ca:** number of major vessels (0-3)\n\n**cp :** Chest Pain type chest pain type,\n\n*   Value 1: typical angina\n   \n*   Value 2: atypical angina\n   \n*   Value 3: non-anginal pain\n   \n*   Value 4: asymptomatic\n   \n**trtbps :** resting blood pressure (in mm Hg)\n\n**chol :** cholestoral in mg\/dl fetched via BMI sensor\n\n**fbs :** (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n\n**rest_ecg :** resting electrocardiographic results\n   \n*   0= normal\n   \n*   1= having ST-T wave abnormality. (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n   \n*   2= showing probable or definite left ventricular hypertrophy by Estes' criteria.\n\n**thalach :** maximum heart rate achieved.\n\n**target :** 0= less chance of heart attack, 1= more chance of heart attack.","d2852471":"**Logistic Regression** is a classification algorithm. It is used to predict a binary outcome (1 \/ 0, Yes \/ No, True \/ False) given a set of independent variables. To represent binary\/categorical outcome, we use dummy variables. You can also think of logistic regression as a special case of linear regression when the outcome variable is categorical, where we are using log of odds as dependent variable. In simple words, it predicts the probability of occurrence of an event by fitting data to a logit function.","ea0111c8":"Classifier implementing the **k-nearest neighbors** vote.","df38fda3":"**Gradient Boosting** for classification.\n\nGB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.","9ef7b5ab":"# If the discrete values were more, we would get rid of the discrete data by making use of the statistical information (df.describe) of the data,but it is not necessary because there are very few discrete values.","263fbc9e":"# **Data pre-processing**","8710a829":"**NOT:** As we can see don't have non values and our job gets easier as there are no null values. All value types is numerical but we know some values is categorical.We will use this information in the data visualization process. Now we can continue this adventure...\n\n**Categorical values:** \n* sex \n* exng\n* caa\n* cp\n* fbs\n* restecg\n* slp\n* thall\n","8b64f82f":"![Hello](https:\/\/iasbh.tmgrup.com.tr\/8007eb\/1200\/627\/0\/0\/800\/417?u=https:\/\/isbh.tmgrup.com.tr\/sbh\/2019\/11\/22\/erkeklerde-ve-kadinlarda-kalp-krizi-belirtileri-nelerdir-belirtiler-ne-zaman-baslar-1574422889186.jpg)","fce199a9":"**Random Forest Classifier.**\n\nA random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree.","5f1ad25d":"# **Conclusion**\n","75450475":"The advantages of support vector machines are:\n\n* Effective in high dimensional spaces.\n\n* Still effective in cases where number of dimensions is greater than the number of samples.\n\n* Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n\n* Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n\nThe disadvantages of support vector machines include:\n\n* If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n\n* SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below).","50e3b732":"# **Importing libraries**","f9cc9c54":"**Gaussian Naive Bayes** supports continuous valued features and models each as conforming to a Gaussian (normal) distribution.\nAn approach to create a simple model is to assume that the data is described by a Gaussian distribution with no co-variance (independent dimensions) between dimensions. This model can be fit by simply finding the mean and standard deviation of the points within each label.","e367ec23":"# **Data Visualization**"}}