{"cell_type":{"5928e5ec":"code","4c88d83b":"code","13040085":"code","fa67f3e6":"code","c4a844aa":"code","458ed554":"code","dcc53ccd":"code","b843ac2d":"code","0a6a28c0":"code","27dc859a":"code","f388249f":"code","3bd1ddfb":"code","9de4349e":"code","8a7417c6":"code","6398085f":"code","5d51b308":"code","42e90730":"code","bf178d23":"code","aecb44fe":"code","41d24be7":"code","f25beeb3":"code","a43a85c0":"code","2aa4631e":"code","74cb6392":"code","a5384ef3":"code","f8647769":"code","c82f3d55":"code","5dcb5fbd":"code","03b4f573":"code","739c0c00":"code","f65e9847":"code","b31daa09":"code","230af46f":"code","1091e70c":"code","99222637":"code","8e579b74":"code","d47a9d0b":"code","074cb880":"code","9b888960":"code","636c1a65":"code","9eb4aed9":"code","8d3c78d2":"code","898d52b4":"code","33bcb54e":"code","1a422324":"code","8d2f228f":"code","5686cce7":"code","f931c57f":"code","668d6079":"code","a1653731":"code","01eaea3e":"code","71c61c52":"code","9d409215":"code","95021ce7":"code","c0415afb":"code","c1e5831e":"code","678cbd7d":"code","7dd2ea14":"code","48a9434b":"code","2a391e6c":"code","b714cd41":"markdown","c7f3b128":"markdown","05e8df6f":"markdown","168f6ee7":"markdown","8f51764c":"markdown","3221c8b5":"markdown","39b9934c":"markdown","5c78d087":"markdown","30898175":"markdown","7434d647":"markdown","0c4429e6":"markdown","28978e89":"markdown","16337269":"markdown","6ba65001":"markdown","5ba6f4e2":"markdown","449f4635":"markdown","ed3dd9e9":"markdown","e9a5b719":"markdown","bae6b794":"markdown","78972b41":"markdown","8e75864b":"markdown","4a7a1069":"markdown","11746743":"markdown","564ac402":"markdown"},"source":{"5928e5ec":"import numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns","4c88d83b":"pd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","13040085":"# Load Data\ndf_train = pd.read_csv('..\/input\/osic-pulmonary-fibrosis-progression\/train.csv')\ndf_test = pd.read_csv('..\/input\/osic-pulmonary-fibrosis-progression\/test.csv')","fa67f3e6":"# Print Data Shape\nprint(f'Training Set Shape = {df_train.shape} - Patients = {df_train[\"Patient\"].nunique()}')\nprint(f'Training Set Memory Usage = {df_train.memory_usage().sum() \/ 1024 ** 2:.2f} MB')\nprint(f'Test Set Shape = {df_test.shape} - Patients = {df_test[\"Patient\"].nunique()}')\nprint(f'Test Set Memory Usage = {df_test.memory_usage().sum() \/ 1024 ** 2:.2f} MB')","c4a844aa":"# Training Set FVC Measurements Per Patient\ntraining_sample_counts = df_train.rename(columns={'Weeks': 'Samples'}).groupby('Patient').agg('count')['Samples'].value_counts()\nprint(f'Training Set FVC Measurements Per Patient \\n{(\"-\") * 41}\\n{training_sample_counts}')","458ed554":"df_test.head(2)","dcc53ccd":"df_submission = pd.read_csv( '..\/input\/osic-pulmonary-fibrosis-progression\/sample_submission.csv' )\ndf_submission.head()","b843ac2d":"print(f'FVC Statistical Summary\\n{\"-\" * 23}')\n\nprint(f'Mean: {df_train[\"FVC\"].mean():.6}  -  Median: {df_train[\"FVC\"].median():.6}  -  Std: {df_train[\"FVC\"].std():.6}')\nprint(f'Min: {df_train[\"FVC\"].min()}  -  25%: {df_train[\"FVC\"].quantile(0.25)}  -  50%: {df_train[\"FVC\"].quantile(0.5)}  -  75%: {df_train[\"FVC\"].quantile(0.75)}  -  Max: {df_train[\"FVC\"].max()}')\nprint(f'Skew: {df_train[\"FVC\"].skew():.6}  -  Kurtosis: {df_train[\"FVC\"].kurtosis():.6}')\nmissing_values_count = df_train[df_train[\"FVC\"].isnull()].shape[0]\ntraining_samples_count = df_train.shape[0]\nprint(f'Missing Values: {missing_values_count}\/{training_samples_count} ({missing_values_count * 100 \/ training_samples_count:.4}%)')\n\nfig, axes = plt.subplots(ncols=2, figsize=(18, 6), dpi=150)\n\nsns.distplot(df_train['FVC'], label='FVC', ax=axes[0])\nstats.probplot(df_train['FVC'], plot=axes[1])\n\nfor i in range(2):\n    axes[i].tick_params(axis='x', labelsize=12)\n    axes[i].tick_params(axis='y', labelsize=12)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    \naxes[0].set_title(f'FVC Distribution in Training Set', size=15, pad=15)\naxes[1].set_title(f'FVC Probability Plot', size=15, pad=15)\n\nplt.show()","0a6a28c0":"def plot_fvc(df, patient):\n        \n    df[['Weeks', 'FVC']].set_index('Weeks').plot(figsize=(30, 6), label='_nolegend_')\n    \n    plt.tick_params(axis='x', labelsize=20)\n    plt.tick_params(axis='y', labelsize=20)\n    plt.xlabel('')\n    plt.ylabel('')\n    plt.title(f'Patient: {patient} - {df[\"Age\"].tolist()[0]} - {df[\"Sex\"].tolist()[0]} - {df[\"SmokingStatus\"].tolist()[0]} ({len(df)} Measurements in {(df[\"Weeks\"].max() - df[\"Weeks\"].min())} Weeks Period)', size=25, pad=25)\n    plt.legend().set_visible(False)\n    plt.show()\n\nfor patient, df in list(df_train.groupby('Patient')):\n    \n    df['FVC_diff-1'] = np.abs(df['FVC'].diff(-1))\n    \n    print(f'Patient: {patient} FVC Statistical Summary\\n{\"-\" * 58}')\n    print(f'Mean: {df[\"FVC\"].mean():.6}  -  Median: {df[\"FVC\"].median():.6}  -  Std: {df[\"FVC\"].std():.6}')\n    print(f'Min: {df[\"FVC\"].min()} -  Max: {df[\"FVC\"].max()}')\n    print(f'Skew: {df[\"FVC\"].skew():.6}  -  Kurtosis: {df[\"FVC\"].kurtosis():.6}')\n    print(f'Change Mean: {df[\"FVC_diff-1\"].mean():.6}  - Change Median: {df[\"FVC_diff-1\"].median():.6}  - Change Std: {df[\"FVC_diff-1\"].std():.6}')\n    print(f'Change Min: {df[\"FVC_diff-1\"].min()} -  Change Max: {df[\"FVC_diff-1\"].max()}')\n    print(f'Change Skew: {df[\"FVC_diff-1\"].skew():.6} -  Change Kurtosis: {df[\"FVC_diff-1\"].kurtosis():.6}')\n    \n    plot_fvc(df, patient)","27dc859a":"# Tabular Data\n\n# There are four continuous features along with `FVC` in tabular data. Those features are:\n\n# * `Weeks`: The relative number of weeks pre\/post the baseline CT (may be negative). It doesn't have any significant relationship with other features because patients got both better or worse over the course of time regardless of their `Age`.\n# * `Percent`: A computed field which approximates the patient's `FVC` as a percent of the typical `FVC` for a person of similar characteristics. This feature has a strong relationship with `FVC` because it is derived from it, but it doesn't have any significant relationship with other features.\n# * `Age`: Age of the patient. `Age` has a slight relationship with `FVC` and `Percent` since younger patients have higher lung capacity.\n\n# Distributions of `FVC`, `Percent` and `Age` are very similar but `Weeks` is different than those features.","f388249f":"g = sns.pairplot(df_train[['FVC', 'Weeks', 'Percent', 'Age']], aspect=1.4, height=5, diag_kind='kde', kind='reg')\n\ng.axes[3, 0].set_xlabel('FVC', fontsize=20)\ng.axes[3, 1].set_xlabel('Weeks', fontsize=20)\ng.axes[3, 2].set_xlabel('Percent', fontsize=20)\ng.axes[3, 3].set_xlabel('Age', fontsize=20)\ng.axes[0, 0].set_ylabel('FVC', fontsize=20)\ng.axes[1, 0].set_ylabel('Weeks', fontsize=20)\ng.axes[2, 0].set_ylabel('Percent', fontsize=20)\ng.axes[3, 0].set_ylabel('Age', fontsize=20)\n\ng.axes[3, 0].tick_params(axis='x', labelsize=15)\ng.axes[3, 1].tick_params(axis='x', labelsize=15)\ng.axes[3, 2].tick_params(axis='x', labelsize=15)\ng.axes[3, 3].tick_params(axis='x', labelsize=15)\ng.axes[0, 0].tick_params(axis='y', labelsize=15)\ng.axes[1, 0].tick_params(axis='y', labelsize=15)\ng.axes[2, 0].tick_params(axis='y', labelsize=15)\ng.axes[3, 0].tick_params(axis='y', labelsize=15)\n\ng.fig.suptitle('Tabular Data Feature Distributions and Interactions', fontsize=25, y=1.08)\n\nplt.show()","3bd1ddfb":"# The first categorical feature in tabular data is `Sex` which is basically gender of the patient.\n\n# * `FVC` distributions of males and females are very different from each other. Females have lower lung capacity compared to males due to genetics. `FVC` relationships with other features are also very different for males and females. `FVC` of males have a stronger relationship with `Percent` and `Age` compared to `FVC` of females.\n# * Comparing `Weeks` for different genders is not logical but females have a decent `FVC` improvement over the course weeks compared to males.\n# * `Percent` distributions of males and females are very different from each other just like `FVC` distributions because `Percent` is derived from it.\n# * `Age` has no differences between males and females in terms of relationships and distributions except female's `Age` distribution have slightly longer tails and a shorter peak.\ng = sns.pairplot(df_train[['FVC', 'Weeks', 'Percent', 'Age', 'Sex']], hue='Sex', aspect=1.4, height=5, diag_kind='kde', kind='reg')\n\ng.axes[3, 0].set_xlabel('FVC', fontsize=20)\ng.axes[3, 1].set_xlabel('Weeks', fontsize=20)\ng.axes[3, 2].set_xlabel('Percent', fontsize=20)\ng.axes[3, 3].set_xlabel('Age', fontsize=20)\ng.axes[0, 0].set_ylabel('FVC', fontsize=20)\ng.axes[1, 0].set_ylabel('Weeks', fontsize=20)\ng.axes[2, 0].set_ylabel('Percent', fontsize=20)\ng.axes[3, 0].set_ylabel('Age', fontsize=20)\n\ng.axes[3, 0].tick_params(axis='x', labelsize=15)\ng.axes[3, 1].tick_params(axis='x', labelsize=15)\ng.axes[3, 2].tick_params(axis='x', labelsize=15)\ng.axes[3, 3].tick_params(axis='x', labelsize=15)\ng.axes[0, 0].tick_params(axis='y', labelsize=15)\ng.axes[1, 0].tick_params(axis='y', labelsize=15)\ng.axes[2, 0].tick_params(axis='y', labelsize=15)\ng.axes[3, 0].tick_params(axis='y', labelsize=15)\n\nplt.legend(prop={'size': 20})\ng._legend.remove()\ng.fig.suptitle('Tabular Data Feature Distributions and Interactions Between Sex Groups', fontsize=25, y=1.08)\n\nplt.show()","9de4349e":"# The second categorical feature in tabular data is `SmokingStatus` which is also self-explanatory.\n\n# * `FVC` distributions of `SmokingStatus` groups are quite unexpected. Mean `FVC` of smokers is  higher than mean `FVC` of ex-smokers and patients who had never smoked.\n# * Distribution of `Weeks` is similar for different `SmokingStatus`. Smokers have the strongest positive linear relationship between `FVC` and `Weeks` which is also another unexpected phenomenon.\n# * `Percent` distributions of different `SmokingStatus` groups is very similar to `FVC` distributions but peaks are taller. The linear relationship between `Percent` and `Weeks` is also stronger compared to `FVC` and `Weeks`.\n# * `Age` has no relationship with `SmokingStatus`.\ng = sns.pairplot(df_train[['FVC', 'Weeks', 'Percent', 'Age', 'SmokingStatus']], hue='SmokingStatus', aspect=1.4, height=5, diag_kind='kde', kind='reg')\n\ng.axes[3, 0].set_xlabel('FVC', fontsize=20)\ng.axes[3, 1].set_xlabel('Weeks', fontsize=20)\ng.axes[3, 2].set_xlabel('Percent', fontsize=20)\ng.axes[3, 3].set_xlabel('Age', fontsize=20)\ng.axes[0, 0].set_ylabel('FVC', fontsize=20)\ng.axes[1, 0].set_ylabel('Weeks', fontsize=20)\ng.axes[2, 0].set_ylabel('Percent', fontsize=20)\ng.axes[3, 0].set_ylabel('Age', fontsize=20)\n\ng.axes[3, 0].tick_params(axis='x', labelsize=15)\ng.axes[3, 1].tick_params(axis='x', labelsize=15)\ng.axes[3, 2].tick_params(axis='x', labelsize=15)\ng.axes[3, 3].tick_params(axis='x', labelsize=15)\ng.axes[0, 0].tick_params(axis='y', labelsize=15)\ng.axes[1, 0].tick_params(axis='y', labelsize=15)\ng.axes[2, 0].tick_params(axis='y', labelsize=15)\ng.axes[3, 0].tick_params(axis='y', labelsize=15)\n\nplt.legend(prop={'size': 20})\ng._legend.remove()\ng.fig.suptitle('Tabular Data Feature Distributions and Interactions Between SmokingStatus Groups', fontsize=25, y=1.08)\n\nplt.show()","8a7417c6":"# As seen from the plots above, the only strong correlation is between FVC and Percent. The other features' correlations are between -0.1 and 0.1.\nfig = plt.figure(figsize=(10, 10), dpi=100)\n\nsns.heatmap(df_train.corr(), annot=True, square=True, cmap='coolwarm', annot_kws={'size': 15},  fmt='.2f')   \n\nplt.tick_params(axis='x', labelsize=18, rotation=75)\nplt.tick_params(axis='y', labelsize=18, rotation=0)\nplt.title('Tabular Data Feature Correlations', size=20, pad=20)\n\nplt.show()","6398085f":"import os\nimport numpy as np\nimport pandas as pd\nimport random\nimport math\n\nfrom tqdm.notebook import tqdm\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nfrom sklearn.metrics import mean_squared_error\nimport category_encoders as ce\n\nfrom sklearn.linear_model import Ridge, ElasticNet\nfrom functools import partial\nimport scipy as sp\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5d51b308":"def seed_everything(seed=777):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)","42e90730":"OUTPUT_DICT = '.\/'\n\nID = 'Patient_Week'\nTARGET = 'FVC'\nSEED = 777\nseed_everything(seed=SEED)\n\nN_FOLD = 7","bf178d23":"train = pd.read_csv('..\/input\/osic-pulmonary-fibrosis-progression\/train.csv')\notest = pd.read_csv('..\/input\/osic-pulmonary-fibrosis-progression\/test.csv')","aecb44fe":"# construct train input\ntrain = pd.concat([train,otest])\noutput = pd.DataFrame()\ngb = train.groupby('Patient')\ntk0 = tqdm(gb, total=len(gb))\nfor _, usr_df in tk0:\n    usr_output = pd.DataFrame()\n    for week, tmp in usr_df.groupby('Weeks'):\n        rename_cols = {'Weeks': 'base_Week', 'FVC': 'base_FVC', 'Age': 'base_Age'}\n        tmp = tmp.rename(columns=rename_cols)\n        drop_cols = ['Age', 'Sex', 'SmokingStatus', 'Percent']\n        _usr_output = usr_df.drop(columns=drop_cols).rename(columns={'Weeks': 'predict_Week'}).merge(tmp, on='Patient')\n        _usr_output['Week_passed'] = _usr_output['predict_Week'] - _usr_output['base_Week']\n        usr_output = pd.concat([usr_output, _usr_output])\n    output = pd.concat([output, usr_output])\n    \ntrain = output[output['Week_passed']!=0].reset_index(drop=True)","41d24be7":"# construct test input\ntest = otest.rename(columns={'Weeks': 'base_Week', 'FVC': 'base_FVC', 'Age': 'base_Age'})\nsubmission = pd.read_csv('..\/input\/osic-pulmonary-fibrosis-progression\/sample_submission.csv')\nsubmission['Patient'] = submission['Patient_Week'].apply(lambda x: x.split('_')[0])\nsubmission['predict_Week'] = submission['Patient_Week'].apply(lambda x: x.split('_')[1]).astype(int)\ntest = submission.drop(columns=['FVC', 'Confidence']).merge(test, on='Patient')\ntest['Week_passed'] = test['predict_Week'] - test['base_Week']\ntest.set_index('Patient_Week', inplace=True)","f25beeb3":"folds = train[['Patient', TARGET]].copy()\nFold = GroupKFold(n_splits=N_FOLD)\ngroups = folds['Patient'].values\nfor n, (train_index, val_index) in enumerate(Fold.split(folds, folds[TARGET], groups)):\n    folds.loc[val_index, 'fold'] = int(n)\nfolds['fold'] = folds['fold'].astype(int)","a43a85c0":"#===========================================================\n# model\n#===========================================================\ndef run_single_model(clf, train_df, test_df, folds, features, target, fold_num=0):\n    \n    trn_idx = folds[folds.fold!=fold_num].index\n    val_idx = folds[folds.fold==fold_num].index\n    \n    y_tr = target.iloc[trn_idx].values\n    X_tr = train_df.iloc[trn_idx][features].values\n    y_val = target.iloc[val_idx].values\n    X_val = train_df.iloc[val_idx][features].values\n    \n    oof = np.zeros(len(train_df))\n    predictions = np.zeros(len(test_df))\n    clf.fit(X_tr, y_tr)\n    \n    oof[val_idx] = clf.predict(X_val)\n    predictions += clf.predict(test_df[features])\n    return oof, predictions\n\n\ndef run_kfold_model(clf, train, test, folds, features, target, n_fold=7):\n    \n    oof = np.zeros(len(train))\n    predictions = np.zeros(len(test))\n    feature_importance_df = pd.DataFrame()\n\n    for fold_ in range(n_fold):\n\n        _oof, _predictions = run_single_model(clf,\n                                              train, \n                                              test,\n                                              folds,  \n                                              features,\n                                              target, \n                                              fold_num=fold_)\n        oof += _oof\n        predictions += _predictions\/n_fold\n    \n    return oof, predictions","2aa4631e":"target = train[TARGET]\ntest[TARGET] = np.nan\n\n# features\ncat_features = ['Sex', 'SmokingStatus']\nnum_features = [c for c in test.columns if (test.dtypes[c] != 'object') & (c not in cat_features)]\nfeatures = num_features + cat_features\ndrop_features = [TARGET, 'predict_Week', 'Percent', 'base_Week']\nfeatures = [c for c in features if c not in drop_features]\n\nif cat_features:\n    ce_oe = ce.OrdinalEncoder(cols=cat_features, handle_unknown='impute')\n    ce_oe.fit(train)\n    train = ce_oe.transform(train)\n    test = ce_oe.transform(test)","74cb6392":"'''for alpha1 in (1,0.3,0.1,0.03,0.01):\n    for l1s in (0.01,0.03,0.1,0.2,0.5,0.8,0.9,0.97,0.99):\n        \n        print(\" For alpha:\",alpha1,\"& l1_ratio:\",l1s)\n        clf = ElasticNet(alpha=alpha1, l1_ratio = l1s)\n        oof, predictions = run_kfold_model(clf, train, test, folds, features, target, n_fold=N_FOLD)\n\n        train['FVC_pred'] = oof\n        test['FVC_pred'] = predictions\n\n        # baseline score\n        train['Confidence'] = 100\n        train['sigma_clipped'] = train['Confidence'].apply(lambda x: max(x, 70))\n        train['diff'] = abs(train['FVC'] - train['FVC_pred'])\n        train['delta'] = train['diff'].apply(lambda x: min(x, 1000))\n        train['score'] = -math.sqrt(2)*train['delta']\/train['sigma_clipped'] - np.log(math.sqrt(2)*train['sigma_clipped'])\n        score = train['score'].mean()\n        print(score)\n\n        def loss_func(weight, row):\n            confidence = weight\n            sigma_clipped = max(confidence, 70)\n            diff = abs(row['FVC'] - row['FVC_pred'])\n            delta = min(diff, 1000)\n            score = -math.sqrt(2)*delta\/sigma_clipped - np.log(math.sqrt(2)*sigma_clipped)\n            return -score\n\n        results = []\n        tk0 = tqdm(train.iterrows(), total=len(train))\n        for _, row in tk0:\n            loss_partial = partial(loss_func, row=row)\n            weight = [100]\n            result = sp.optimize.minimize(loss_partial, weight, method='SLSQP')\n            x = result['x']\n            results.append(x[0])\n\n        # optimized score\n        train['Confidence'] = results\n        train['sigma_clipped'] = train['Confidence'].apply(lambda x: max(x, 70))\n        train['diff'] = abs(train['FVC'] - train['FVC_pred'])\n        train['delta'] = train['diff'].apply(lambda x: min(x, 1000))\n        train['score'] = -math.sqrt(2)*train['delta']\/train['sigma_clipped'] - np.log(math.sqrt(2)*train['sigma_clipped'])\n        score = train['score'].mean()\n        print(score)'''","a5384ef3":"for alpha1 in [0.3]:\n    for l1s in [0.8]:\n        \n        print(\" For alpha:\",alpha1,\"& l1_ratio:\",l1s)\n        clf = ElasticNet(alpha=alpha1, l1_ratio = l1s)\n        oof, predictions = run_kfold_model(clf, train, test, folds, features, target, n_fold=N_FOLD)\n\n        train['FVC_pred'] = oof\n        test['FVC_pred'] = predictions\n\n        # baseline score\n        train['Confidence'] = 100\n        train['sigma_clipped'] = train['Confidence'].apply(lambda x: max(x, 70))\n        train['diff'] = abs(train['FVC'] - train['FVC_pred'])\n        train['delta'] = train['diff'].apply(lambda x: min(x, 1000))\n        train['score'] = -math.sqrt(2)*train['delta']\/train['sigma_clipped'] - np.log(math.sqrt(2)*train['sigma_clipped'])\n        score = train['score'].mean()\n        print(score)\n\n        def loss_func(weight, row):\n            confidence = weight\n            sigma_clipped = max(confidence, 70)\n            diff = abs(row['FVC'] - row['FVC_pred'])\n            delta = min(diff, 1000)\n            score = -math.sqrt(2)*delta\/sigma_clipped - np.log(math.sqrt(2)*sigma_clipped)\n            return -score\n\n        results = []\n        tk0 = tqdm(train.iterrows(), total=len(train))\n        for _, row in tk0:\n            loss_partial = partial(loss_func, row=row)\n            weight = [100]\n            result = sp.optimize.minimize(loss_partial, weight, method='SLSQP')\n            x = result['x']\n            results.append(x[0])\n\n        # optimized score\n        train['Confidence'] = results\n        train['sigma_clipped'] = train['Confidence'].apply(lambda x: max(x, 70))\n        train['diff'] = abs(train['FVC'] - train['FVC_pred'])\n        train['delta'] = train['diff'].apply(lambda x: min(x, 1000))\n        train['score'] = -math.sqrt(2)*train['delta']\/train['sigma_clipped'] - np.log(math.sqrt(2)*train['sigma_clipped'])\n        score = train['score'].mean()\n        print(score)","f8647769":"TARGET = 'Confidence'\n\ntarget = train[TARGET]\ntest[TARGET] = np.nan\n\n# features\ncat_features = ['Sex', 'SmokingStatus']\nnum_features = [c for c in test.columns if (test.dtypes[c] != 'object') & (c not in cat_features)]\nfeatures = num_features + cat_features\ndrop_features = [ID, TARGET, 'predict_Week', 'base_Week', 'FVC', 'FVC_pred']\nfeatures = [c for c in features if c not in drop_features]\n\noof, predictions = run_kfold_model(clf, train, test, folds, features, target, n_fold=N_FOLD)","c82f3d55":"train['Confidence'] = oof\ntrain['sigma_clipped'] = train['Confidence'].apply(lambda x: max(x, 70))\ntrain['diff'] = abs(train['FVC'] - train['FVC_pred'])\ntrain['delta'] = train['diff'].apply(lambda x: min(x, 1000))\ntrain['score'] = -math.sqrt(2)*train['delta']\/train['sigma_clipped'] - np.log(math.sqrt(2)*train['sigma_clipped'])\nscore = train['score'].mean()\nprint(score)","5dcb5fbd":"test['Confidence'] = predictions\ntest = test.reset_index()","03b4f573":"sub = submission[['Patient_Week']].merge(test[['Patient_Week', 'FVC_pred', 'Confidence']], on='Patient_Week')\nsub = sub.rename(columns={'FVC_pred': 'FVC'})\n\nfor i in range(len(otest)):\n    sub.loc[sub['Patient_Week']==otest.Patient[i]+'_'+str(otest.Weeks[i]), 'FVC'] = otest.FVC[i]\n    sub.loc[sub['Patient_Week']==otest.Patient[i]+'_'+str(otest.Weeks[i]), 'Confidence'] = 0.1\n    \nsub[sub.Confidence<1]\n\nsub.to_csv('submission_2.csv', index=False, float_format='%.1f')","739c0c00":"import numpy as np\nimport pandas as pd\nimport pydicom\nimport os\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold","f65e9847":"ROOT = \"..\/input\/osic-pulmonary-fibrosis-progression\"\n#DESIRED_SIZE = 256 # Memory issue\nDESIRED_SIZE = 128","b31daa09":"tr = pd.read_csv(f\"{ROOT}\/train.csv\")\ntr.drop_duplicates(keep=False, inplace=True, subset=['Patient','Weeks'])\nchunk = pd.read_csv(f\"{ROOT}\/test.csv\")\n\nprint(\"add infos\")\nsub = pd.read_csv(f\"{ROOT}\/sample_submission.csv\")\nsub['Patient'] = sub['Patient_Week'].apply(lambda x:x.split('_')[0])\nsub['Weeks'] = sub['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\nsub =  sub[['Patient','Weeks','Confidence','Patient_Week']]\nsub = sub.merge(chunk.drop('Weeks', axis=1), on=\"Patient\")","230af46f":"tr['WHERE'] = 'train'\nchunk['WHERE'] = 'val'\nsub['WHERE'] = 'test'\ndata = tr.append([chunk, sub])","1091e70c":"print(tr.shape, chunk.shape, sub.shape, data.shape)\nprint(tr.Patient.nunique(), chunk.Patient.nunique(), sub.Patient.nunique(), \n      data.Patient.nunique())","99222637":"data['min_week'] = data['Weeks']\ndata.loc[data.WHERE=='test','min_week'] = np.nan\ndata['min_week'] = data.groupby('Patient')['min_week'].transform('min')","8e579b74":"base = data.loc[data.Weeks == data.min_week]\nbase = base[['Patient','FVC']].copy()\nbase.columns = ['Patient','min_FVC']\nbase['nb'] = 1\nbase['nb'] = base.groupby('Patient')['nb'].transform('cumsum')\nbase = base[base.nb==1]\nbase.drop('nb', axis=1, inplace=True)","d47a9d0b":"data = data.merge(base, on='Patient', how='left')\ndata['base_week'] = data['Weeks'] - data['min_week']\ndel base","074cb880":"COLS = ['Sex','SmokingStatus']\nFE = []\nfor col in COLS:\n    for mod in data[col].unique():\n        FE.append(mod)\n        data[mod] = (data[col] == mod).astype(int)","9b888960":"data['age'] = (data['Age'] - data['Age'].min() ) \/ ( data['Age'].max() - data['Age'].min() )\ndata['BASE'] = (data['min_FVC'] - data['min_FVC'].min() ) \/ ( data['min_FVC'].max() - data['min_FVC'].min() )\ndata['week'] = (data['base_week'] - data['base_week'].min() ) \/ ( data['base_week'].max() - data['base_week'].min() )\ndata['percent'] = (data['Percent'] - data['Percent'].min() ) \/ ( data['Percent'].max() - data['Percent'].min() )\nFE += ['age','percent','week','BASE']","636c1a65":"tr = data.loc[data.WHERE=='train']\nchunk = data.loc[data.WHERE=='val']\nsub = data.loc[data.WHERE=='test']\ndel data","9eb4aed9":"tr.shape, chunk.shape, sub.shape","8d3c78d2":"def get_images(df, how=\"train\"):\n    xo = []\n    p = []\n    w  = []\n    for i in tqdm(range(df.shape[0])):\n        patient = df.iloc[i,0]\n        week = df.iloc[i,1]\n        try:\n            img_path = f\"{ROOT}\/{how}\/{patient}\/{week}.dcm\"\n            ds = pydicom.dcmread(img_path)\n            im = Image.fromarray(ds.pixel_array)\n            im = im.resize((DESIRED_SIZE,DESIRED_SIZE)) \n            im = np.array(im)\n            xo.append(im[np.newaxis,:,:])\n            p.append(patient)\n            w.append(week)\n        except:\n            pass\n    data = pd.DataFrame({\"Patient\":p,\"Weeks\":w})\n    return np.concatenate(xo, axis=0), data","898d52b4":"import tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M","33bcb54e":"C1, C2 = tf.constant(70, dtype='float32'), tf.constant(1000, dtype=\"float32\")\n#=============================#\ndef score(y_true, y_pred):\n    tf.dtypes.cast(y_true, tf.float32)\n    tf.dtypes.cast(y_pred, tf.float32)\n    sigma = y_pred[:, 2] - y_pred[:, 0]\n    fvc_pred = y_pred[:, 1]\n    \n    #sigma_clip = sigma + C1\n    sigma_clip = tf.maximum(sigma, C1)\n    delta = tf.abs(y_true[:, 0] - fvc_pred)\n    delta = tf.minimum(delta, C2)\n    sq2 = tf.sqrt( tf.dtypes.cast(2, dtype=tf.float32) )\n    metric = (delta \/ sigma_clip)*sq2 + tf.math.log(sigma_clip* sq2)\n    return K.mean(metric)\n#============================#\ndef qloss(y_true, y_pred):\n    # Pinball loss for multiple quantiles\n    qs = [0.2, 0.50, 0.8]\n    q = tf.constant(np.array([qs]), dtype=tf.float32)\n    e = y_true - y_pred\n    v = tf.maximum(q*e, (q-1)*e)\n    return K.mean(v)\n#=============================#\ndef mloss(_lambda):\n    def loss(y_true, y_pred):\n        return _lambda * qloss(y_true, y_pred) + (1 - _lambda)*score(y_true, y_pred)\n    return loss\n#=================\ndef make_model():\n    z = L.Input((9,), name=\"Patient\")\n    x = L.Dense(100, activation=\"relu\", name=\"d1\")(z)\n    x = L.Dense(100, activation=\"relu\", name=\"d2\")(x)\n    #x = L.Dense(100, activation=\"relu\", name=\"d3\")(x)\n    p1 = L.Dense(3, activation=\"linear\", name=\"p1\")(x)\n    p2 = L.Dense(3, activation=\"relu\", name=\"p2\")(x)\n    preds = L.Lambda(lambda x: x[0] + tf.cumsum(x[1], axis=1), \n                     name=\"preds\")([p1, p2])\n    \n    model = M.Model(z, preds, name=\"CNN\")\n    #model.compile(loss=qloss, optimizer=\"adam\", metrics=[score])\n    model.compile(loss=mloss(0.8), optimizer=\"adam\", metrics=[score])\n    return model","1a422324":"net = make_model()\nprint(net.summary())\nprint(net.count_params())","8d2f228f":"y = tr['FVC'].values\nz = tr[FE].values\nze = sub[FE].values\npe = np.zeros((ze.shape[0], 3))\npred = np.zeros((z.shape[0], 3))","5686cce7":"NFOLD = 7\nkf = KFold(n_splits=NFOLD)","f931c57f":"%%time\ncnt = 0\nfor tr_idx, val_idx in kf.split(z):\n    cnt += 1\n    print(f\"FOLD {cnt}\")\n    net = make_model()\n    net.fit(z[tr_idx], y[tr_idx], batch_size=200, epochs=1000, \n            validation_data=(z[val_idx], y[val_idx]), verbose=0) #\n    print(\"train\", net.evaluate(z[tr_idx], y[tr_idx], verbose=0, batch_size=500))\n    print(\"val\", net.evaluate(z[val_idx], y[val_idx], verbose=0, batch_size=500))\n    print(\"predict val...\")\n    pred[val_idx] = net.predict(z[val_idx], batch_size=500, verbose=0)\n    print(\"predict test...\")\n    pe += net.predict(ze, batch_size=500, verbose=0) \/ NFOLD","668d6079":"sigma_opt = mean_absolute_error(y, pred[:, 1])\nunc = pred[:,2] - pred[:, 0]\nsigma_mean = np.mean(unc)\nprint(sigma_opt, sigma_mean)","a1653731":"idxs = np.random.randint(0, y.shape[0], 100)\nplt.plot(y[idxs], label=\"ground truth\")\nplt.plot(pred[idxs, 0], label=\"q25\")\nplt.plot(pred[idxs, 1], label=\"q50\")\nplt.plot(pred[idxs, 2], label=\"q75\")\nplt.legend(loc=\"best\")\nplt.show()","01eaea3e":"print(unc.min(), unc.mean(), unc.max(), (unc>=0).mean())","71c61c52":"plt.hist(unc)\nplt.title(\"uncertainty in prediction\")\nplt.show()","9d409215":"sub.head()","95021ce7":"sub['FVC1'] = pe[:, 1]\nsub['Confidence1'] = pe[:, 2] - pe[:, 0]","c0415afb":"subm = sub[['Patient_Week','FVC','Confidence','FVC1','Confidence1']].copy()","c1e5831e":"subm.loc[~subm.FVC1.isnull()].head(10)","678cbd7d":"subm.loc[~subm.FVC1.isnull(),'FVC'] = subm.loc[~subm.FVC1.isnull(),'FVC1']\nif sigma_mean<70:\n    subm['Confidence'] = sigma_opt\nelse:\n    subm.loc[~subm.FVC1.isnull(),'Confidence'] = subm.loc[~subm.FVC1.isnull(),'Confidence1']","7dd2ea14":"subm.head()","48a9434b":"subm.describe().T","2a391e6c":"subm[[\"Patient_Week\",\"FVC\",\"Confidence\"]].to_csv(\"submission.csv\", index=False)","b714cd41":"#### Load Submission Data","c7f3b128":"### ------------------------END OSIC Pulmonary Fibrosis Progression Analysis-----------------------------------","05e8df6f":"# Description of the Competetion\nImagine one day, your breathing became consistently labored and shallow. Months later you were finally diagnosed with pulmonary fibrosis, a disorder with no known cause and no known cure, created by scarring of the lungs. If that happened to you, you would want to know your prognosis. That\u2019s where a troubling disease becomes frightening for the patient: outcomes can range from long-term stability to rapid deterioration, but doctors aren\u2019t easily able to tell where an individual may fall on that spectrum. Your help, and data science, may be able to aid in this prediction, which would dramatically help both patients and clinicians.\n\n![image.png](attachment:image.png)\n\nCurrent methods make fibrotic lung diseases difficult to treat, even with access to a chest CT scan. In addition, the wide range of varied prognoses create issues organizing clinical trials. Finally, patients suffer extreme anxiety\u2014in addition to fibrosis-related symptoms\u2014from the disease\u2019s opaque path of progression.\n\n[Open Source Imaging Consortium (OSIC)](https:\/\/www.osicild.org\/) is a not-for-profit, co-operative effort between academia, industry and philanthropy. The group enables rapid advances in the fight against Idiopathic Pulmonary Fibrosis (IPF), fibrosing interstitial lung diseases (ILDs), and other respiratory diseases, including emphysematous conditions. Its mission is to bring together radiologists, clinicians and computational scientists from around the world to improve imaging-based treatments.\n\nIn this competition, you\u2019ll predict a patient\u2019s severity of decline in lung function based on a CT scan of their lungs. You\u2019ll determine lung function based on output from a spirometer, which measures the volume of air inhaled and exhaled. The challenge is to use machine learning techniques to make a prediction with the image, metadata, and baseline FVC as input.\n\nIf successful, patients and their families would better understand their prognosis when they are first diagnosed with this incurable lung disease. Improved severity detection would also positively impact treatment trial design and accelerate the clinical development of novel treatments.","168f6ee7":"Training Set has 176 Patients","8f51764c":"# ----------OSIC Pulmonary Fibrosis Progression Analysis------------","3221c8b5":"# Another Way for OSIC Pulmonary Fibrosis Progression","39b9934c":"## Load Train and Test Data","5c78d087":"# FIRST OSIC Pulmonary Fibrosis Progression","30898175":"##  Submission Data Processing","7434d647":"## Data Modeling","0c4429e6":"### -------------------------END SECOND OSIC Pulmonary Fibrosis Progression------------------------------------","28978e89":"## Utilities ","16337269":"### construct test input","6ba65001":"### -------------------END FIRST OSIC Pulmonary Fibrosis Progression-----------------------","5ba6f4e2":"## Predict Confidence","449f4635":"## BUILD BASELINE CNN","ed3dd9e9":"### Divide the dataset in some fold for cross validation","e9a5b719":"## Image Processing","bae6b794":"## Laplace Log Likelihood\n\n\n`Confidence` values smaller than 70 are clipped.\n\n$\\large \\sigma_{clipped} = max(\\sigma, 70),$\n\nErrors greater than 1000 are also clipped in order to avoid large errors.\n\n$\\large \\Delta = min ( |FVC_{true} - FVC_{predicted}|, 1000 ),$\n\nThe metric is defined as:\n\n$\\Large metric = -   \\frac{\\sqrt{2} \\Delta}{\\sigma_{clipped}} - \\ln ( \\sqrt{2} \\sigma_{clipped} ).$","78972b41":"### construct train input","8e75864b":"## Model Training\n","4a7a1069":"## Predict Dataset","11746743":"## Configuration","564ac402":"## Import Libraries"}}