{"cell_type":{"6f95ec25":"code","ef6b01a8":"code","fe47dd54":"code","a9047f3d":"code","0f1937ab":"code","ce33269a":"code","b8e1edb3":"code","efcc1331":"code","f4c47aaa":"code","4ae6fc95":"code","1a39f5fd":"code","3d419ce4":"code","b399cafd":"code","3c8d9dec":"code","dedc49d7":"code","699c7ef2":"code","f31b88bf":"code","0b43681e":"code","1830e30d":"code","7c7002a2":"code","c4999e09":"code","d62d8cde":"code","fe4f9b82":"markdown"},"source":{"6f95ec25":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statistics\nimport math\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import r2_score, mean_squared_error\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ef6b01a8":"data = pd.read_csv('\/kaggle\/input\/melbourne-housing-market\/MELBOURNE_HOUSE_PRICES_LESS.csv')","fe47dd54":"print(data.shape, '\\n', data.isnull().sum())","a9047f3d":"# dropping missing values from price. \n\ndata.dropna(subset=['Price'], inplace=True)","0f1937ab":"# Create Month and day feature. \n\n# converts to a datetime feature\ndata['Date'] = pd.to_datetime(data['Date'])\n\n# Create a month column\ndata['Month'] = pd.DatetimeIndex(data['Date']).month\n\n# Creating a day of week column \ndata['Day'] = pd.DatetimeIndex(data['Date']).day_name()\n\n# Drop date column\ndata.drop(['Date'], axis=1, inplace=True)","ce33269a":"# Creating a 'Street Name' feature\n\n# Lambda expression that removes the number portion of an address, leaving the street name.\ndata['Address'] = data['Address'].apply(lambda x: \" \".join(x.split()[1:]))\n\n# Renaming the column \ndata.rename(columns={'Address': 'Street'}, inplace=True)","b8e1edb3":"sns.pairplot(data)\nplt.show()","efcc1331":"sns.set(rc={'figure.figsize':(20,15)})\nsns.heatmap(data.corr(), cmap='YlGnBu', linewidths =.01, annot=True)\nplt.xticks(rotation=80)\nplt.show()","f4c47aaa":"# Removing Outliers\n\nprint(data.shape)\n\ncols = ['Rooms', 'Price', 'Distance']\n\nfor col in cols:\n    mean = data[col].mean()\n    cut_off = data[col].std() * 3\n    high = mean + cut_off\n    low = mean - cut_off\n    data = data[(data[col]<high) & (data[col]>low)]\n\nprint(data.shape)","4ae6fc95":"# Getting columns for encoding. \n\nencode_columns = ['Suburb', 'Street', 'SellerG', 'CouncilArea', 'Postcode']\ndummy_columns = ['Method', 'Type', 'Regionname', 'Day']\n\n\nencoder = preprocessing.LabelEncoder() \n\n# Applying encoder to each of the specified columns\nfor col in encode_columns:\n    data[col] = encoder.fit_transform(data[col])\n\n    \n# Creating dummy columns \n\nfor x in dummy_columns:\n    dummies = pd.get_dummies(data[x])\n    data = pd.concat([data, dummies], axis=1)\n    data.drop(x, axis=1, inplace=True)","1a39f5fd":"data.columns","3d419ce4":"y = data['Price']\nX = data.drop(['Price'], axis=1)\n\n# Applying scalar to feature values \nscaler = StandardScaler()\nX = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)","b399cafd":"# Logging Y values because of skew \n\nlog_y = y.copy()\nlog_y = np.log10(log_y)","3c8d9dec":"def get_accuracy (predictions, test_set):\n    \n    index = 0\n    differences = []\n    diff_percent = []\n    y = list(test_set)\n    \n    sum_diff = 0\n    \n    for prediction in predictions:\n        \n        norm_diff = (10**prediction) - (10**y[index])\n        sum_diff += norm_diff\n        \n        diff = (  abs((10**prediction) - (10**y[index]))  )\n        differences.append(diff)\n\n        percent = (diff\/10**y[index]) * 100\n        diff_percent.append(percent)\n        index += 1\n\n    print('mean difference', statistics.mean(differences))\n    print('mean difference percentage', statistics.mean(diff_percent))\n    return [differences, diff_percent, sum_diff]","dedc49d7":"state = 1\n\n# Splitting data\nx_train, x_test, y_train, y_test = train_test_split(X, log_y, test_size=0.3, random_state=state)\n\n\n# Fitting XGBoost Regressor algorithm \nfrom xgboost import XGBRegressor\n\nxgb = XGBRegressor(random_state=state)\n\nxgb.fit(x_train, y_train)\n\nxgb_pred = xgb.predict(x_test)\n","699c7ef2":"# Evaluating \n\nprint('R^2: ', r2_score(y_test, xgb_pred))","f31b88bf":"differences, diff_percent, z = get_accuracy(xgb_pred, y_test)","0b43681e":"# Parameter Tuning \n\nrates = np.arange(0.05, 1, 0.05)\ndepths = list(range(1, 10))\ngamma = list(range(0,20))\nsample = np.arange(0.1, 1.1, 0.1)\nL2 = np.arange(0.5, 10, 0.5)\nL1 = np.arange(0.5, 10, 0.5)\nestimators = list(range(50, 200, 10))\n\n# for rate in rates:\n#     xgb = XGBRegressor(random_state=state, learning_rate=rate)\n#     print(np.mean(cross_val_score(xgb, X, log_y, cv=5)),  rate, '\\n')\n\n# for depth in depths:\n#     xgb = XGBRegressor(random_state=state, max_depth=depth)\n#     print(np.mean(cross_val_score(xgb, X, log_y, cv=5)),  depth, '\\n')\n\n# for g in gamma:\n#     xgb = XGBRegressor(random_state=state, gamma=g)\n#     print(np.mean(cross_val_score(xgb, X, log_y, cv=5)),  g, '\\n')\n\n# for s in sample:\n#     xgb = XGBRegressor(random_state=state, subsample=s)\n#     print(np.mean(cross_val_score(xgb, X, log_y, cv=5)),  s, '\\n')\n\n# for x in L2:\n#     xgb = XGBRegressor(random_state=state, reg_lambda=x)\n#     print(np.mean(cross_val_score(xgb, X, log_y, cv=5)),  x, '\\n')\n\n# for y in L1:\n#     xgb = XGBRegressor(random_state=state, alpha=y)\n#     print(np.mean(cross_val_score(xgb, X, log_y, cv=5)),  y, '\\n')\n\n# for z in estimators:\n#     xgb = XGBRegressor(random_state=state, n_estimators=z)\n#     print(np.mean(cross_val_score(xgb, X, log_y, cv=5)),  z, '\\n')","1830e30d":"# Tuned Model\n\nxgb = XGBRegressor(random_state=state, n_estimators=200, alpha=0.5, reg_lambda=7, subsample=1, gamma=0, \n                   max_depth=7, learning_rate=0.2)\n\nxgb.fit(x_train, y_train)\n\nxgb_pred = xgb.predict(x_test)\n\nprint(r2_score(y_test, xgb_pred))\ndifferences, diff_percent, sum_diff = get_accuracy(xgb_pred, y_test)","7c7002a2":"# Coefficients \n\ncoef_dict = {}\nfor coef, column in zip(xgb.feature_importances_, X.columns):\n        coef_dict[column] = coef\n\nsorted_coef = dict(sorted(coef_dict.items(), key=lambda x: x[1], reverse=True))\nfor i in sorted_coef.items():\n    print(i[0], ': ', i[1])","c4999e09":"# Predicted vs Actual house price (on log scale)\n\nplt.figure(figsize=(16,10))\nplt.hist(xgb_pred, bins=50, alpha=1, label='Predicted')\nplt.hist(y_test, bins=50, alpha=0.5, label='Actual')\nplt.legend(loc='upper right')\nplt.show()","d62d8cde":"plt.figure(figsize=(16,10))\nplt.scatter(xgb_pred, y_test)","fe4f9b82":"\nThe aim is to predict property prices in the Melbourne metropolitan area various property features. I found that a XGBoost Regressor to give that best accuracy.\n\nVarious tools\/techniques were used, including a label encoding, scalars, feature creation, dummy columns and log transformations. \n\nThe final model had an R^2 of 0.81. "}}