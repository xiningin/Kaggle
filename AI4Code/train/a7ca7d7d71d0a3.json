{"cell_type":{"4f819791":"code","154d648e":"code","fead3bc1":"code","e3f4b638":"code","556787d5":"code","dad841b8":"code","e1bce8c9":"code","aeef7c40":"code","60f21863":"code","5956e4ad":"code","7eb48c4e":"code","24d8b343":"code","f2dcb15d":"code","c8ce4448":"code","d47c409c":"code","1652958a":"code","64968b6d":"code","7a0a3483":"code","d83f09f8":"code","f19928bb":"code","b115c8c7":"code","cab7d71f":"code","c844e97a":"code","b589269a":"code","bee7ecf6":"code","b681d1dc":"code","6686fe0f":"code","acdfbcbc":"code","0824dbb2":"code","2fd5311d":"code","c5a5cab8":"markdown","4da60909":"markdown","556df8cc":"markdown","9a7abd90":"markdown","746bfcfd":"markdown","b2d4954f":"markdown","74761ad5":"markdown","aa7352da":"markdown","10c80caa":"markdown","e9816135":"markdown","b6274ee9":"markdown","7de5c18d":"markdown","f6ffc549":"markdown","e5b3760e":"markdown"},"source":{"4f819791":"import numpy as np\nimport pandas as pd\nfrom google.colab import drive\ndrive.mount('\/content\/drive')","154d648e":"app_train = pd.read_csv('\/content\/drive\/MyDrive\/application_train.csv')\napp_test = pd.read_csv('\/content\/drive\/MyDrive\/application_test.csv')\nprint(app_train.shape)\nprint(app_test.shape)","fead3bc1":"app_train.info()","e3f4b638":"app_test.head(3)","556787d5":"missing = app_train.isnull().sum()\nprint(\"Max number of missing values in a certain feature:\",missing.max())\nprint(\"Total number of missing values:\",missing.sum())","dad841b8":"train = app_train.loc[:,missing.index[missing<10000]]\ntrain.shape","e1bce8c9":"test = app_test.loc[:,(missing.index[missing<10000]).drop('TARGET')]\ntest.shape","aeef7c40":"test_id=test['SK_ID_CURR']\ntest_id","60f21863":"miss = test.isnull().sum()\nprint(\"Max number of missing values in a certain feature:\",miss.max())\nprint(\"Total number of missing values:\",miss.sum())","5956e4ad":"y_train = train.TARGET\nx_train = train.drop(['TARGET'],axis=1)\nprint(y_train.shape)\nx_train.shape","7eb48c4e":"fdata = pd.concat([x_train,test])\nwdata = pd.concat([x_train,test])\nwdata.shape","24d8b343":"col_obj = wdata.columns[wdata.dtypes == 'object']\nfor i in col_obj:\n    wdata.loc[:,i],_ = pd.factorize(wdata.loc[:,i])\nwdata.head(3) ","f2dcb15d":"data = wdata.fillna(0)","c8ce4448":"data = data.drop(['SK_ID_CURR'],axis=1)\nxtrain = data.iloc[0:len(x_train),:]\nxtest = data.iloc[len(x_train):len(data),:] \nprint(xtrain.shape)\nprint(xtest.shape)","d47c409c":"from sklearn import preprocessing\nscaler = preprocessing.StandardScaler().fit(xtrain)\nx_scaled = scaler.transform(xtrain)\nx_scaled = pd.DataFrame(x_scaled)\nx_scaled.isna().sum().sum()","1652958a":"from sklearn.model_selection import train_test_split\nX_train, X_val, Y_train, Y_val = train_test_split(x_scaled, y_train, test_size=0.2, random_state=18)\nfrom sklearn.linear_model import LogisticRegression\nlreg = LogisticRegression(max_iter=1000).fit(X_train, Y_train)","64968b6d":"y_pred = lreg.predict_proba(X_val)\nfrom sklearn.metrics import roc_auc_score\nprint(\"Validation ROC AUC:\",roc_auc_score(Y_val, y_pred[:,1]))","7a0a3483":"xs_test = scaler.transform(xtest)\ny_pred = lreg.predict_proba(xs_test)\ny=pd.DataFrame(y_pred[:,1])\ny.columns=['TARGET']","d83f09f8":"outcome = pd.concat([test_id,y],axis=1)\noutcome.head(3)","f19928bb":"outcome.to_csv (r'\/content\/drive\/MyDrive\/pred_test.csv', index = False, header=True)","b115c8c7":"import lightgbm as lgb\nlgb_train = lgb.Dataset(data=X_train, label=Y_train)\nlgb_eval = lgb.Dataset(data=X_val, label=Y_val)\nparams = {'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', \n          'learning_rate': 0.01, 'num_leaves': 48, 'num_iteration': 5000, 'verbose': 0 ,\n          'colsample_bytree':.8, 'subsample':.9, 'max_depth':7, 'reg_alpha':.1, 'reg_lambda':.1, \n          'min_split_gain':.01, 'min_child_weight':1}\nmodel = lgb.train(params, lgb_train, valid_sets=lgb_eval, early_stopping_rounds=150, verbose_eval=200)","cab7d71f":"xtrain0 = wdata.iloc[0:len(x_train),:]\nxtrain0=xtrain0.drop(['SK_ID_CURR'],axis=1)\nfd = pd.concat([y_train,xtrain0],axis=1)\ncormat = fd.corr()\nvars = cormat.index[abs(cormat.TARGET)>0.01]\nvars = vars.drop('TARGET') \nprint(vars)\nprint(len(vars))","c844e97a":"fdata = wdata.loc[:,vars]\nfdata = fdata.fillna(0)\nxtrain = fdata.iloc[0:len(x_train),:]\nxtest = fdata.iloc[len(x_train):len(fdata),:] \nprint(xtrain.shape)\nprint(xtest.shape)\nscaler = preprocessing.StandardScaler().fit(xtrain)\nx_scaled = scaler.transform(xtrain)\nx_scaled = pd.DataFrame(x_scaled)","b589269a":"X_train, X_val, Y_train, Y_val = train_test_split(x_scaled, y_train, test_size=0.2, random_state=18)\nlreg = LogisticRegression(max_iter=1000).fit(X_train, Y_train)\ny_pred = lreg.predict_proba(X_val)\nprint(\"Validation ROC AUC:\",roc_auc_score(Y_val, y_pred[:,1]))","bee7ecf6":"scaler = preprocessing.MaxAbsScaler().fit(xtrain)\nx_scaled = scaler.transform(xtrain)\nx_scaled = pd.DataFrame(x_scaled)\nX_train, X_val, Y_train, Y_val = train_test_split(x_scaled, y_train, test_size=0.2, random_state=18)\nlreg = LogisticRegression(max_iter=1000).fit(X_train, Y_train)\ny_pred = lreg.predict_proba(X_val)\nprint(\"Validation ROC AUC:\",roc_auc_score(Y_val, y_pred[:,1]))","b681d1dc":"scaler = preprocessing.MinMaxScaler().fit(xtrain)\nx_scaled = scaler.transform(xtrain)\nx_scaled = pd.DataFrame(x_scaled)\nX_train, X_val, Y_train, Y_val = train_test_split(x_scaled, y_train, test_size=0.2, random_state=18)\nlreg = LogisticRegression(max_iter=1000).fit(X_train, Y_train)\ny_pred = lreg.predict_proba(X_val)\nprint(\"Validation ROC AUC:\",roc_auc_score(Y_val, y_pred[:,1]))","6686fe0f":"wdata = pd.concat([x_train,test])\ncol_obj = wdata.columns[wdata.dtypes == 'object']\nfor i in col_obj:\n    wdata.loc[:,i],_ = pd.factorize(wdata.loc[:,i])\nwdata = wdata.drop(['SK_ID_CURR'],axis=1)\nfdata = wdata.loc[:,vars]\nprint(fdata.isna().sum().sum())\nfdata.head(3)","acdfbcbc":"from sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values=np.nan, strategy='mean').fit(fdata)\ncols = fdata.columns\nidata = imp.transform(fdata)\nidata = pd.DataFrame(idata,columns=cols)","0824dbb2":"xtrain = idata.iloc[0:len(x_train),:]\nxtest = idata.iloc[len(x_train):len(idata),:] \nscaler = preprocessing.StandardScaler().fit(xtrain)\nx_scaled = scaler.transform(xtrain)\nx_scaled = pd.DataFrame(x_scaled)\nX_train, X_val, Y_train, Y_val = train_test_split(x_scaled, y_train, test_size=0.2, random_state=18)\nlreg = LogisticRegression(max_iter=1000).fit(X_train, Y_train)\ny_pred = lreg.predict_proba(X_val)\nprint(\"Validation ROC AUC:\",roc_auc_score(Y_val, y_pred[:,1]))","2fd5311d":"from sklearn.ensemble import RandomForestClassifier\nRF = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0).fit(X_train, Y_train)\ny_pred = RF.predict_proba(X_val)\nprint(\"Validation ROC AUC:\",roc_auc_score(Y_val, y_pred[:,1]))","c5a5cab8":"# [Problem 4] Feature engineering","4da60909":"Uploaded csv file to Kaggle and resulted score of 0.68725","556df8cc":"307k data for training and 48k data for testing.","9a7abd90":"Method 4. Used SimpleImputer on missing values","746bfcfd":"For training accuracy, it shows similar result compared to using 63 variables. <br\/>\n","b2d4954f":"Method 2. use features with high corr coeffs","74761ad5":"# [Problem 1] Confirmation of competition contents","aa7352da":"# [Problem 2] Learning and verification","10c80caa":"<b> 65 features with floating point number <\/b> <br\/>\n<b> 41 features with integer number <\/b> <br\/>\n<b> 16 features with object type <\/b> <br\/>","e9816135":"<a href=\"https:\/\/colab.research.google.com\/github\/otgontstsg\/diveintocode-ml\/blob\/master\/home_credit_risk_prediction.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>","b6274ee9":"- **What to learn and what to predict?**\n\nHome Credit is the international finance provider company which provides loans to people with little or no credit history.\nThe company expects prediction result that can precisely evaluate the payment abilities of the loan takers, especially people with little or no credit history. \n\n\n- **What kind of file to create and submit to Kaggle?** \n\nSubmission should be in CVS file that contains predicted probabilities of the test samples. \n\n\n- **What kind of index value will be used to evaluate the submissions?**\n\nAccording to the prediction area under ROC curve will be used to evaluate the performance of the prediction. <br\/>\nTrue positive and false positive rate is used. ","7de5c18d":"Method 5. other algorithm (I have no other preprocessing or feature choosing ideas)","f6ffc549":"Method 3. Use different scaler for preprocessing. ","e5b3760e":"[Problem 3] Estimation on test data"}}