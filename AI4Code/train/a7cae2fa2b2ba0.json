{"cell_type":{"794025cc":"code","a02b12c2":"code","e7241007":"code","026acb42":"code","268a657b":"code","d397cc96":"code","03ddc7fc":"code","f1c0d89f":"code","cab11958":"code","485f5cb8":"code","20ee184d":"code","017af924":"code","0e6f0ee9":"code","6460c0cb":"code","91d554db":"code","6398a096":"code","9f71d37a":"code","0c169e4e":"code","74c5c6b4":"code","38620e9b":"code","4aa5f7a7":"code","7807f722":"code","05879add":"code","0a91fe3a":"code","ff4f42ff":"code","453163a3":"code","6eafe834":"code","442069bb":"code","ccf10bfc":"code","0b3b8425":"code","6516b986":"code","139fdf5d":"code","bc1dec96":"code","648d4a72":"code","2c7595db":"code","8ea6a1bf":"code","a1ced286":"code","c4591a77":"code","8bab0df2":"code","59c7345a":"code","6c075ed6":"code","c753b82e":"code","016450e2":"code","de50202c":"code","51800665":"code","2d3f8be3":"code","45dc5d1a":"code","f7aeeb57":"code","5ce9c336":"code","40bb8d6f":"code","3aa87b0b":"code","b2dcce8a":"code","1cc936f2":"code","9f2733e5":"markdown","8eeb9e9b":"markdown","e872a8ca":"markdown","7703f490":"markdown","c6a2a7ff":"markdown","f7cc7855":"markdown","70efb5ba":"markdown","bffbcfc9":"markdown","177adcd7":"markdown","6a44f34b":"markdown","00c0ccdf":"markdown","b6c4ebeb":"markdown","05e173b0":"markdown","061c4cb4":"markdown","cd9637f2":"markdown","8b667d56":"markdown","47107777":"markdown","7acd7243":"markdown","2ab96de5":"markdown","5768d19b":"markdown","fe6d2ccd":"markdown","43a1ae83":"markdown","b276615f":"markdown","37d7f9b1":"markdown","a5ef129d":"markdown","1b576e3f":"markdown","f7e51ee0":"markdown","e566cbc1":"markdown","36a88e04":"markdown","239959aa":"markdown","48c4cd91":"markdown","d8de86bc":"markdown","344e580d":"markdown"},"source":{"794025cc":"import sys, os, re, csv, codecs, numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport string\nimport re\nimport nltk\nimport matplotlib.gridspec as gridspec \nps = nltk.PorterStemmer()\nwn = nltk.WordNetLemmatizer()\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.sparse import hstack","a02b12c2":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nall_data =[train, test]\ntrain.sample(20)","e7241007":"print(\"out of {} rows {} are toxic ,{} are severe_toxic, {} are obscene, {} are threat,{} are insult and {} are identity_hate\".format(len(train),len(train[train.toxic==1]),len(train[train.severe_toxic==1]),len(train[train.obscene==1]),len(train[train.threat==1]),len(train[train.insult==1]),len(train[train.identity_hate==1])))","026acb42":"print(\"See the data types of the training set\")\ntrain.dtypes","268a657b":"print(\"Print the statistical review of the training set\")\ntrain.describe()","d397cc96":"print(\"toxic examples:\")\ntrain[train['toxic']==1]['comment_text']","03ddc7fc":"print(\"severe_toxic examples:\")\ntrain[train['severe_toxic']==1]['comment_text']","f1c0d89f":"print(\"obscene examples:\")\ntrain[train['obscene']==1]['comment_text'][:5]","cab11958":"print(\"threatthreat examples:\")\ntrain[train['threat']==1]['comment_text'][:5]","485f5cb8":"print(\"insult examples:\")\ntrain[train['insult']==1]['comment_text'][:5]","20ee184d":"train.head()","017af924":"rowsums=train.iloc[:,2:].sum(axis=1) # sum the rows from column 2 to the end\ntrain['clean']=(rowsums==0)# put the sum = 0 which is consider as a clean comment to clean","0e6f0ee9":"train.isnull().any(),test.isnull().any()","6460c0cb":"print(\"display variations in the length of the comments as you can see a very large std value indicates that\")\nlens = train.comment_text.str.len()\nlens.mean(), lens.std(), lens.max()","91d554db":"lens.hist();","6398a096":"\"I love NLP\" == \"I love NLP.\"","9f71d37a":"string.punctuation","0c169e4e":"test.head()","74c5c6b4":"print(\"Create a fn that remove any sort of punctuation\")\ndef remove_pun(text):\n    text_nopunc = \"\".join([char for char in text if char not in string.punctuation]) # as we are interating through char we need join to join words together without spacing\n    return text_nopunc\nfor data in all_data:\n    data['clean_text']=data['comment_text'].apply(lambda x:remove_pun(x))\ntrain.head()","38620e9b":"corpus = train.clean_text\ncorpus","4aa5f7a7":"def clean(comment):\n    \"\"\"\n    This function receives comments and returns clean word-list\n    \"\"\"\n    #Convert to lower case , so that Hi and hi are the same\n    comment=comment.lower()\n    #remove \\n\n    comment=re.sub(\"\\\\n\",\"\",comment)    \n    tokens = re.split('\\W+',comment)\n    return tokens","7807f722":"print(\"let's give it a try\")\ncorpus.iloc[12235]","05879add":"print(\"See the change\")\nclean(corpus.iloc[12235])","0a91fe3a":"train['body_text_tokenized']=corpus.apply(lambda x :clean(x))\n","ff4f42ff":"print(\"let's have a look on the nexamples of stop words\")\nstopwords = nltk.corpus.stopwords.words('english')\nstopwords","453163a3":"print('as always lets write our function that removes stop words')\ndef remove_stop_words(text):\n      words = [word for word in text if word not in  stopwords]\n      return words\nprint(\"calling the fn on our tokenized column\")\ntrain['tokens_without_stopw'] = train['body_text_tokenized'].apply(lambda x :remove_stop_words(x))","6eafe834":"train.head(15)","442069bb":"print(\"have an example to make things clear\")\nprint(ps.stem('grows'))\nprint(ps.stem('growing'))\nprint(ps.stem('grow'))","ccf10bfc":"#print(\"lets implement our fn to stem the words in our comments\")\n#   text = [ps.stem(word) for word in text]\n #   return text\n#print(\"calling our fn on our tokens_without_stopw column\")\n#train['text_stemming'] = train['tokens_without_stopw'].apply(lambda x:stemming(x))","0b3b8425":"train.head()","6516b986":"def lemmatizing(text):\n    text = [wn.lemmatize(word) for word in text]\n    return text\nprint(\"calling our fn on our tokens_without_stopw column\")\ntrain['text_lemmatize'] = train['tokens_without_stopw'].apply(lambda x:lemmatizing(x))","139fdf5d":"train.text_lemmatize","bc1dec96":"print(\"our data shape\")\ntrain.shape","648d4a72":"train.T","2c7595db":"#the range of ngram we would like to look for\n#ngram should be dealing with the full sentence not the tokenized one\nngram_vect=CountVectorizer(ngram_range=(2,2))\nx_counts=ngram_vect.fit_transform(train[:25].clean_text)\nprint(x_counts.shape)\nprint(ngram_vect.get_feature_names())\n","8ea6a1bf":"x_counts_df = pd.DataFrame(x_counts.toarray())\nx_counts_df.columns = ngram_vect.get_feature_names()\nx_counts_df.T","a1ced286":"for data in all_data:\n    data['total_length'] = data['comment_text'].apply(len)\n    data['capitals'] = data['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n    data['caps_vs_length'] = data.apply(lambda row: float(row['capitals'])\/float(row['total_length']),\n                                axis=1)\n    data['num_exclamation_marks'] = data['comment_text'].apply(lambda comment: comment.count('!'))\n    data['num_question_marks'] = data['comment_text'].apply(lambda comment: comment.count('?'))\n    data['num_punctuation'] = data['comment_text'].apply(\n        lambda comment: sum(comment.count(w) for w in '.,;:'))\n    data['num_symbols'] = data['comment_text'].apply(\n        lambda comment: sum(comment.count(w) for w in '*&$%'))\n    data['num_words'] = data['comment_text'].apply(lambda comment: len(comment.split()))\n    data['num_unique_words'] = data['comment_text'].apply(\n        lambda comment: len(set(w for w in comment.split())))\n    data['words_vs_unique'] = data['num_unique_words'] \/ data['num_words']\n    data['num_smilies'] = data['comment_text'].apply(\n        lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))","c4591a77":"train.head().T","8bab0df2":"target = train.loc[:,['clean','toxic','severe_toxic','obscene','threat','insult','identity_hate','total_length','capitals','caps_vs_length','num_exclamation_marks','num_question_marks','num_punctuation','num_symbols','num_words','num_unique_words','words_vs_unique','num_smilies']] ","59c7345a":"target.clean=target.clean.map({True:1,False:0})","6c075ed6":"colormap = plt.cm.plasma\nplt.figure(figsize=(20,20))\nplt.title('Correlation of features & targets',y=1.05,size=14)\nsns.heatmap(train.corr(),linewidths=0.1,vmax=1.0,square=True,cmap=colormap,\n           linecolor='white',annot=True)","c753b82e":"bins = np.linspace(0,500,40)\n\nplt.hist(target[target['clean']==1]['total_length'],bins,alpha=0.5,normed=True,label='Clean')\nplt.hist(target[target['clean']==0]['total_length'],bins,alpha=0.5,normed=True,label='Not Clean')\nplt.legend(loc='upper left')\nplt.show()\n# Look at the X-axis both labels got the same distribution on x-axis so this feature won't help us ","016450e2":"bins = np.linspace(0,110,40)\n\nplt.hist(target[target['clean']==1]['capitals'],bins,alpha=0.5,normed=True,label='Clean')\nplt.hist(target[target['clean']==0]['capitals'],bins,alpha=0.5,normed=True,label='Not Clean')\nplt.legend(loc='upper left')\nplt.show()","de50202c":"bins = np.linspace(0,100,40)\n\nplt.hist(target[target['clean']==1]['caps_vs_length'],bins,alpha=0.5,normed=True,label='Clean')\nplt.hist(target[target['clean']==0]['caps_vs_length'],bins,alpha=0.5,normed=True,label='Not Clean')\nplt.legend(loc='upper left')\nplt.show()\n#This feature will help our model as you can see all values of caps_vs_length are labelled as not clean comments","51800665":"bins = np.linspace(0,100,40)\n\nplt.hist(target[target['clean']==1]['num_exclamation_marks'],bins,alpha=0.5,normed=True,label='Clean')\nplt.hist(target[target['clean']==0]['num_exclamation_marks'],bins,alpha=0.5,normed=True,label='Not Clean')\nplt.legend(loc='upper left')\nplt.show()\n#This feature will help us out as all values from this feature is labelled as Not Clean comments","2d3f8be3":"bins = np.linspace(0,100,40)\n\nplt.hist(target[target['clean']==1]['num_question_marks'],bins,alpha=0.5,normed=True,label='Clean')\nplt.hist(target[target['clean']==0]['num_question_marks'],bins,alpha=0.5,normed=True,label='Not Clean')\nplt.legend(loc='upper left')\nplt.show()\n# what do you think of this feature? \n# it's useful you are right :D","45dc5d1a":"bins = np.linspace(0,100,40)\n\nplt.hist(target[target['clean']==1]['num_punctuation'],bins,alpha=0.5,normed=True,label='Clean')\nplt.hist(target[target['clean']==0]['num_punctuation'],bins,alpha=0.5,normed=True,label='Not Clean')\nplt.legend(loc='upper left')\nplt.show()\n#They got the same distribution so it won't help our model","f7aeeb57":"bins = np.linspace(0,4,40)\n\nplt.hist(target[target['clean']==1]['num_symbols'],bins,alpha=0.5,normed=True,label='Clean')\nplt.hist(target[target['clean']==0]['num_symbols'],bins,alpha=0.5,normed=True,label='Not Clean')\nplt.legend(loc='upper left')\nplt.show()","5ce9c336":"bins = np.linspace(0,100,40)\n\nplt.hist(target[target['clean']==1]['num_words'],bins,alpha=0.5,normed=True,label='Clean')\nplt.hist(target[target['clean']==0]['num_words'],bins,alpha=0.5,normed=True,label='Not Clean')\nplt.legend(loc='upper left')\nplt.show()","40bb8d6f":"bins = np.linspace(0,300,40)\n\nplt.hist(target[target['clean']==1]['num_unique_words'],bins,alpha=0.5,normed=True,label='Clean')\nplt.hist(target[target['clean']==0]['num_unique_words'],bins,alpha=0.5,normed=True,label='Not Clean')\nplt.legend(loc='upper left')\nplt.show()","3aa87b0b":"bins = np.linspace(0,10,40)\n\nplt.hist(target[target['clean']==1]['words_vs_unique'],bins,alpha=0.5,normed=True,label='Clean')\nplt.hist(target[target['clean']==0]['words_vs_unique'],bins,alpha=0.5,normed=True,label='Not Clean')\nplt.legend(loc='upper left')\nplt.show()","b2dcce8a":"bins = np.linspace(0,0.5,40)\n\nplt.hist(target[target['clean']==1]['num_smilies'],bins,alpha=0.5,normed=True,label='Clean')\nplt.hist(target[target['clean']==0]['num_smilies'],bins,alpha=0.5,normed=True,label='Not Clean')\nplt.legend(loc='upper left')\nplt.show()","1cc936f2":"train.head().T","9f2733e5":"## Linear relationship","8eeb9e9b":"<a id=\"p1\"><\/a>\n# 1. Importing Libraries and Packages","e872a8ca":"## Remove stop words","7703f490":"<a id=\"p2\"><\/a>\n# 2. Loading and Viewing Data Set\nWith Pandas, we can load both csv files including the training and testing set that we wil later use to train and test our model. Before we begin, we should take a look at our data table to see the values that we'll be working with. We can use the head and describe function to look at some sample data and statistics as mean,std and count values . We can also look at its keys and column names.","c6a2a7ff":"**# Contents\n1. [Importing Libraries and Packages](#p1)\n2. [Loading and Viewing Data Set](#p2)\n3. [Data Cleaning](#p3)\n4. [Feature Engineering](#p4)","f7cc7855":"# Toxic Comment Classification Challenge\nThe Conversation AI team, a research initiative founded by Jigsaw and Google (both a part of Alphabet) are working on tools to help improve online conversation. One area of focus is the study of negative online behaviors, like toxic comments (i.e. comments that are rude, disrespectful or otherwise likely to make someone leave a discussion). So far they\u2019ve built a range of publicly available models served through the Perspective API, including toxicity. But the current models still make errors, and they don\u2019t allow users to select which types of toxicity they\u2019re interested in finding (e.g. some platforms may be fine with profanity, but not with other types of toxic content).\n\nIn this competition, you\u2019re challenged to build a multi-headed model that\u2019s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate better than Perspective\u2019s current models. You\u2019ll be using a dataset of comments from Wikipedia\u2019s talk page edits. Improvements to the current model will hopefully help online discussion become more productive and respectful. so in other words we are challenged to classify text to **Toxic, Non Toxic, Threat and so on**\n\nPlease upvote and share if this helps you!! Also, feel free to fork this kernel to play around with the code and test it for yourself. If you plan to use any part of this code, please reference this kernel! I will be glad to answer any questions you may have in the comments. Thank You!\n\nMake sure to follow me for Future Kernels even better than this one","70efb5ba":"Movng on, as you can see from the sneak peek, the dependent variables are in the training set itself so we need to split them up, into X and Y sets.","bffbcfc9":"# Evaluate created features\nlet's see if the features we picked are worth and will make our learning process is easier or not by actually visualizing them using Correlation map to visualize the linear relation between our newly created features if this didn''t help us out we should try scatter plot to visualize the non linear relation","177adcd7":"## Non linear relationship","6a44f34b":"No !! so we will need to remove any sort of punctuation in our text data in order not to confuse Python, let's first what a punctuation looks like in ptyhon\n","00c0ccdf":"Looks like we don't need to deal with the null values after all!\n\nNote that: There are tons of preprocessing and feature engineering steps you could do for the dataset, but our focus today is not about the preprocessing task so what we are doing here is the minimal that could get the rest of the steps work well.","b6c4ebeb":"## Vectorization\n### Count Vectorization\nconverting a sentence to a numerical representation to be understood by our model so let me wrap up what we have done so far\n![9.PNG](attachment:9.PNG)\nthen after that will have each word contained in the text as a column and 1 indicating the text having a word and 0 otherwise and will end up with a matrix like this\n![10.PNG](attachment:10.PNG)\nso our model roughly check for the number of occurance of some words, sentence and correlates that to any of our classes through creating relations between the occurence of some words and the classes choosed so let's see an example\n![11.PNG](attachment:11.PNG)\nwe are now have that intution that word offer is related with a spam mails and word lol is not \n### N-gram \nit's actually working the same as count vectorization but regards our columns instead of having one word per column we are having all the combination of words for example \"i love nlp so much\" will break to \"i love\", \"love nlp\",\"nlp so\",\"so much\" if we are having bigram(2 words) there are another examples of trigrams(3 words per column) so the common question , what to choose , actually we will tune these parameters and pick what works the best for our model","05e173b0":"For Memory sake i will be using TF-IDF in another kernel, don't worry people i will complete solving the problem but let's consider this kernel for feature analysis and data visualization.","061c4cb4":"## Evaluation \nthis is actually how kaggle would evaluate our modeling so we should be sure we understand it clearly before we proceed.\nFor each id in the test set, you must predict a probability for each of the six possible types of comment toxicity (toxic, severe_toxic, obscene, threat, insult, identity_hate). The columns must be in the same order as shown below. The file should contain a header and have the following format:\n\nid,toxic,severe_toxic,obscene,threat,insult,identity_hate\n00001cee341fdb,12,0.5,0.5,0.5,0.5,0.5,0.5\n0000247867823ef7,0.5,0.5,0.5,0.5,0.5,0.5\netc.\n","cd9637f2":"Hi, I am a graduate Electronics and Communications Engineer who decided to shift to Data sciecne and Machine learning and in this kernel i will be disscussing explicitly the problem of prediction the toxic\/non toxic\/threat people on board of titanic, i will come across several concepts and techniques spanning data visualization, Machine learning and Deep learning trying to state the best model working on this specific dataset.\n\n*Please upvote and share if this helps you!! Also, feel free to fork this kernel to play around with the code and test it for yourself. If you plan to use any part of this code, please reference this kernel!* I will be glad to answer any questions you may have in the comments. Thank You! \n\n*Make sure to follow me for Future Kernels even better than this one!*\n","8b667d56":"## Remove Punctuation\nFor us human the words \"I love NLP\" and \"I love NLP. \" is the same but are they having the same meaning to python let's figure this out","47107777":"**If you made it this far, congratulations!! You have gotten a glimpse at an introduction to data visualization, analysis and Deep Learning. You are well on your way to become a Data Science expert! Keep learning and trying out new things, as one of the most important things for Data Scientists is to be creative and perform analysis hands-on. Please upvote and share if this kernel helped you!**","7acd7243":"## Stemming\nProcess of reducing inflected (or sometimes derived) words to their word stem or root, crudely chopping off the end of the word to leave only the base so that our model could learn less things than before for example Electricity\/electrical goes down to just electric\nbut there is a problem with this approach that for soe words as Meanness\/meaning these words will boils down to mean and Python will deal with them as they are having the same meaning.","2ab96de5":"## Tokenize","5768d19b":"# NLP\n## Can Computers Understand Language?\n\nAs long as computers have been around, programmers have been trying to write programs that understand languages like English. The reason is pretty obvious\u200a\u2014\u200ahumans have been writing things down for thousands of years and it would be really helpful if a computer could read and understand all that data.\n\nComputers can\u2019t yet truly understand English in the way that humans do\u200a\u2014\u200abut they can already do a lot! In certain limited areas, what you can do with NLP already seems like magic. You might be able to save a lot of time by applying NLP techniques to your own projects.\n\nAnd even better, the latest advances in NLP are easily accessible through open source Python libraries like spaCy, textacy, and neuralcoref. What you can do with just a few lines of python is amazing.\nExtracting Meaning from Text is Hard\n\nThe process of reading and understanding English is very complex\u200a\u2014\u200aand that\u2019s not even considering that English doesn\u2019t follow logical and consistent rules. For example, what does this news headline mean?\n\n    \u201cEnvironmental regulators grill business owner over illegal coal fires.\u201d\n\nAre the regulators questioning a business owner about burning coal illegally? Or are the regulators literally cooking the business owner? As you can see, parsing English with a computer is going to be complicated.\n\nDoing anything complicated in machine learning usually means building a pipeline. The idea is to break up your problem into very small pieces and then use machine learning to solve each smaller piece separately. Then by chaining together several machine learning models that feed into each other, you can do very complicated things.\n\nAnd that\u2019s exactly the strategy we are going to use for NLP. We\u2019ll break down the process of understanding English into small chunks and see how each one works.\nBuilding an NLP Pipeline, Step-by-Step\n## Application of NLP in real life\n![Capture7.PNG](attachment:Capture7.PNG)\n\n# Unstructred Data\n![Capture8.PNG](attachment:Capture8.PNG)\nSo there is nothing to indicate for example if you are having an email nothing to indicate where is the body, subject and the end\nthe text is considered as a semi structured or unstructure data but since Kaggle is putting our data in a tables so it gives our data a sense of structure so THANKS Kaggle.\n## Word Representations\n### One Hot\nIn machine learning vector space terms, this is a vector with one 1 and a lot of zeroes [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\nDeep learning people call this a \u201cone hot\u201d representation so in other words let's say we have a dictionary of all words, in order to represent a certain word we are putting 1 infront of the word in the dictionary and other wise zero to build up the vector of this word, but there is a problem with that representation that i can't relate any words to similar other words so let's have a real world example if you are searching for f user searches for [Dell notebook battery size], we would like to match documents with \n\u201c\nDell laptop battery capacity\n\u201d\nand actually this couldn't be taken place here as \n![Capture.PNG](attachment:Capture.PNG))\n\nOur query and document vectors are orthogonal There is no natural notion of similarity in a set of one hot vectors\nSo a solution to that is using  distributional similarity based representations, so instead of having only a vector representation of just the word you are no having 2 vectors 1 for the main word and other for the context around it (before and after words)\nso instead we are having a model that could predict the context given the word.\n![Capture1.PNG](attachment:Capture1.PNG)\nActually word2vec model came out from this idea of predicting the context of a word through it's distributional representation\n\n# Word2Vec\n## skip-grams\nPredict the context given target, so for each step your model will consider a certain word as the center word and will predicts context words (after and before that word) \n![Capture3.PNG](attachment:Capture3.PNG)\nso as it appears we pick a center word and predict its context (the words before and after it) maximizing some words vectors that we think they might be in its context and then compute the loss\n![Capture4.PNG](attachment:Capture4.PNG)\nJ: loss fn\nwt: center word\nwt+j: the context so we maximize the probability of this prediction and compute the loss aftewards\n","fe6d2ccd":"## Implementing an N-grams\n","43a1ae83":"We need then to split our sentences to words and then to a numeric representation so our model could deal with it as i mentioned in my previous kernel [here](https:\/\/www.kaggle.com\/omarayman\/the-home-for-future-data-scientists) that any machine learning model will have to deal with numbers let's first clean our text data as we have words like haven't , im and \\n and our models won't understand that these are two different words so will have to split them","b276615f":"As you can deduce the squares that tend to be yellow are an indication that the two features connected to it are highly correlated, for further info see [link](https:\/\/www.dummies.com\/education\/math\/statistics\/how-to-interpret-a-correlation-coefficient-r\/) ","37d7f9b1":"### Why do we care \nit is heavly reducing what our models should look at actually if you think of example of text having grow, grows,growing,grown these are 4 tokens and python won't be able to relate those tokens to each other but if we instead we will just care about grow so it will save the memory with 4 times !!","a5ef129d":"<a id=\"p4\"><\/a>\n# 4. Feature engineering\nfeature engineering is the process is as simple as that creating a new features from the data we are having in our case may be it could be a certain % of capital letters in word might consider more toxic, we begin with a hypothesis that the toxic comments is has more characters than others and will see in a couple of minutes if this hypothesis is justifying or not, let's create some extra feature that could help us in our model, Thanks to  [this](https:\/\/www.kaggle.com\/eikedehling\/feature-engineering) kernel for figuring this out for us","1b576e3f":"I have borrowed the follwing function from this incredible kernel ","f7e51ee0":"## Columns\nid : the comment id\nComment_text: the raw words of the comment\n\n    toxic  if true 1 otherwise 0\n    severe_toxic\n    obscene\n    threat\n    insult\n    identity_hate\n","e566cbc1":"Let's create a feature called \"clean\" for clean comments that are neither toxi nor a threat n, and so on , this step will help us alot to see what features are making the comment to be toxic","36a88e04":"## lemmatizing\nActually does the same job as stemming BUT lemmatizing using the vocabulary analysis of words aiming to remove inflectional endings to return the dictionary form of a word, if you are confused i dont blame you, let's dive into it ,stemming typically faster as it simply chops off the end of a word without any understanding of the context in which a word is used so it's less acurate as because of that maybe it crashed and didn't work with me upwards HAHAHA :D while lemmatizing is typically more accurate as it's creating groups with similar meaning based on the context around the word, it will always return a dictionary word but it may be computationally expensive.","239959aa":"Now is the step of removing some of the stop words that doesnot contribute much to the meaning of the sentence and won't help our model much as I,the,me, if so we will need to remove it to limit the number of tokens Python actually has to look at when learning for example if we have the sentence: I am learning NLP, this would be 4 tokens but after cleaning and removing the stop words we will have just 2 tokens which are learning NLP.","48c4cd91":"<a id=\"p3\"><\/a>\n# 3. Data Cleaning(Handling missing values) \nThe first thing I do when I get a new dataset is take a look at some of it. This lets me see that it all read in correctly and get an idea of what's going on with the data. In this case, I'm looking to see if I see any missing values, which will be reprsented with NaN or None.\n## See how many missing data points we have\n","d8de86bc":"So this is the N-gram looking for a sample of data so you can see the counts of the ngrams we have built on our data\n## TF-IDF \n![Capture.PNG](attachment:Capture.PNG)\nthe term here is actually following different approach instead of showing how many times a word or a combination of words is occuring we are showing the weigh of the word in the data by a simple equation above,note(df is the number of occurence)\n![Captur.PNG](attachment:Captur.PNG)","344e580d":"\nWe need to build fn that allows us to see the full rows\/columns in data when we hit .head() or .tail()\n"}}