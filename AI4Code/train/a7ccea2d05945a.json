{"cell_type":{"1343ae21":"code","79b27861":"code","c4e50685":"code","7552f60a":"code","4c9291f6":"code","37a40489":"code","ff622f83":"code","c4a54817":"code","a47cc51b":"markdown","c034bab6":"markdown","216bc2bc":"markdown","e4cd6547":"markdown","c448dfd8":"markdown","a2131281":"markdown","2974f549":"markdown","b2d82dd2":"markdown","bbc4974e":"markdown","5d0b1c7d":"markdown","2516d9c7":"markdown","8c03e9c3":"markdown","6d093eb0":"markdown"},"source":{"1343ae21":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","79b27861":"import pickle\nimport gensim\nimport matplotlib\nimport gc","c4e50685":"#no_n_below should be uint, ex: no_n_below = 3 or no_n_below = 5\n#no_freq_above should be float [0,1], ex: no_freq_above = 0.5\n#n_feats should be uint, ex: n_feats = 1024 or n_feats = 2048\ndef create_dictionary(tokenized_documents, n_feats, no_n_below = 3, no_freq_above = 0.5):\n    print(\"creating dictionary\")\n    id2word_dict = gensim.corpora.Dictionary(tokenized_documents)\n    print(\"done creating dictionary\")\n    \n    print(\"prior dictionary len %i\" % len(id2word_dict))\n    id2word_dict.filter_extremes(no_below = no_n_below, no_above = no_freq_above, keep_n = n_feats, keep_tokens = None)\n    print(\"current dictionary len %i\" % len(id2word_dict))\n    \n    return id2word_dict   \n    ","7552f60a":"def corpus_tf(id2word_dict, tokenized_documents):\n    return [id2word_dict.doc2bow(document) for document in tokenized_documents]","4c9291f6":"def try_parameters(tokenized_documents, n_feats, n_topics):\n    id2word_dict = create_dictionary(tokenized_documents, n_feats = n_feats)\n    tfcorpus = corpus_tf(id2word_dict, tokenized_documents)\n    print(\"training lda model with %i features and %i topics\" % (n_feats, n_topics))\n    lda_model = gensim.models.ldamodel.LdaModel(corpus = tfcorpus, num_topics = n_topics, id2word = id2word_dict, per_word_topics = False)\n    coherence_model = gensim.models.CoherenceModel(model = lda_model, texts = tokenized_documents, dictionary = id2word_dict, coherence = \"c_v\")\n    coherence_score = coherence_model.get_coherence()\n    print(\"coherence for unknown ngram with %i features and %i topics: %f\" % (n_feats, n_topics, coherence_score))\n    gc.collect()\n    return coherence_score","37a40489":"def loop_lda(tokenized_documents, \n                     tfcorpus, \n                     id2word_dict,\n                     start, #suggest 2 or something\n                     stop, # suggest 20 or similar\n                     step,\n                     per_word_topics = False): #compute list of topics for each word\n    topic_counts = []\n    coherence_scores = []\n    for n_topics in range (start, stop, step):\n        lda_model = gensim.models.ldamodel.LdaModel(corpus = tfcorpus, num_topics = n_topics, id2word = id2word_dict, per_word_topics = per_word_topics)\n        coherence_model = gensim.models.CoherenceModel(model = lda_model, texts = tokenized_documents, dictionary = id2word_dict, coherence = \"c_v\")\n        coherence_score = coherence_model.get_coherence()\n        coherence_scores.append(coherence_score)\n        topic_counts.append(n_topics)\n        print(\"coherence of %f with %i topics\" % (coherence_score, n_topics))\n              \n    return topic_counts, coherence_scores;\n        \ndef loop_ntopics_lda(tokenized_documents, n_feats, start, stop, step):\n    id2word_dict = create_dictionary(tokenized_documents, n_feats = n_feats)\n    tfcorpus = corpus_tf(id2word_dict, tokenized_documents)\n    topic_counts, coherence_scores = loop_lda(tokenized_documents, tfcorpus, id2word_dict, start, stop, step)\n    gc.collect()\n    return topic_counts, coherence_scores","ff622f83":"ngram_bounds = (1,2)\nn_feats_bounds = (512,2048)\nn_topics_bounds = (1,20)\n\nbounds = [ngram_bounds, n_feats_bounds, n_topics_bounds]\n\ndef lda_objective(X, tokenized_documents, tokenized_bigram_documents):\n    ngram = int(round(X[0])) #bound should be [1,2]\n    n_feats = int(round(X[1])) #bounds should be [512, 2048]\n    n_topics = int(round(X[2])) #bouns should be [1,20]\n    \n    if ngram == 2:\n        documents = tokenized_bigram_documents\n        type_string = \"tokenized_bigram_documents\"\n    else:\n        documents = tokenized_documents\n        type_string = \"tokenized_documents\"\n\n    print(\"creating dictionary with %s for: %i %i %i\" % (type_string, ngram, n_feats, n_topics))\n    id2word_dict = create_dictionary(documents, n_feats = n_feats)\n\n    print(\"done creating dictionary.  creating corpus for: %i %i %i\" % (ngram, n_feats, n_topics))\n    tfcorpus = corpus_tf(id2word_dict, documents)\n\n    print(\"done creating corpus.  Building model for: %i %i %i\" % (ngram, n_feats, n_topics))\n    lda_model = gensim.models.ldamodel.LdaModel(corpus = tfcorpus, num_topics = n_topics, id2word = id2word_dict, per_word_topics = False)\n\n    print(\"calculating coherence for: %i %i %i\" % (ngram, n_feats, n_topics))\n    coherence_model = gensim.models.CoherenceModel(model = lda_model, texts = documents, dictionary = id2word_dict, coherence = \"c_v\")\n    coherence = coherence_model.get_coherence()\n    #we want to MAX coherence.  but we will be using a \n    value2minimize = 1 - coherence\n    return value2minimize","c4a54817":"tokenized_path = \"\/kaggle\/input\/preprocess-cord19\/tokenized_documents.pkl\"\nprint(\"opening %s\" % str(tokenized_path)) \nwith open(tokenized_path, \"rb\") as f:\n    tokenized_documents = pickle.load(f)\nprint(\"done opening tokenized documents.  Optimizing\")\n\nstart = 2\nstop = 20\nstep = 1\n\nn_feats = 512\n#coherence_1gram_512featurs_10topics = try_parameters(tokenized_documents, n_feats, n_topics)\ntopic_counts, coherence_1gram_512features = loop_ntopics_lda(tokenized_documents, n_feats, start, stop, step) #compute list of topics for each word\n    \nn_feats = 1024\ntopic_counts, coherence_1gram_1024features = loop_ntopics_lda(tokenized_documents, n_feats, start, stop, step) \n\nbigram_path = \"\/kaggle\/input\/preprocess-cord19\/bigram_model.pkl\"\nprint(\"opening %s\" % str(bigram_path))\nwith open(bigram_path, \"rb\") as f:\n    bigram_model = pickle.load(f)\nprint(\"creating bigram documents\")\ntokenized_document = [bigram_model[document] for document in tokenized_documents]\nprint(\"done retrieving documents. lets optimize\")\n\ngc.collect()\n\nn_feats = 256\ntopic_counts, coherence_2gram_256features = loop_ntopics_lda(tokenized_documents, n_feats, start, stop, step) \n\nn_feats = 512\ntopic_counts, coherence_2gram_512features = loop_ntopics_lda(tokenized_documents, n_feats, start, stop, step) \n\nn_feats = 1024\ntopic_counts, coherence_2gram_1024features = loop_ntopics_lda(tokenized_documents, n_feats, start, stop, step) \n\ncoherence_dict = {\"topic_counts\" : topic_counts,\n                  \"coherence_1gram_512features\" : coherence_1gram_512features,\n                  \"coherence_1gram_1024features\" : coherence_1gram_1024features,\n                  \"coherence_2gram_256features\" : coherence_2gram_256features, \n                  \"coherence_2gram_512features\" : coherence_2gram_512features,\n                  \"coherence_2gram_1024features\" : coherence_2gram_1024features}\n\ndict_path = \"coherence_dict.pkl\"\nprint(\"saving %s\" % (dict_path))\nwith open(dict_path, 'wb') as f:\n    pickle.dump(coherence_dict, f)\n\n#x = range(start, stop, step)\n#matplotlib.pyplot.plot(x, coherence_scores_1gram_512feats)\n#matplotlib.pyplot.plot(x, coherence_scores_1gram_1024feats)\n#matplotlib.pyplot.plot(x, coherence_scores_1gram_2048feats)\n#matplotlib.pyplot.xlabel(\"Number of topics\")\n#matplotlib.pyplot.ylabel(\"Coherence score\")\n#matplotlib.pyplot.legend((coherence_scores_2gram_256feats, coherence_scores_2gram_512feats), (\"256feats\", \"512feats\"))\n#matplotlib.pyplot.title(\"Coherence values for ngram = 2\")\n#matplotlib.pyplot.show()\n","a47cc51b":"# Optimize LDA stand-in","c034bab6":"This is the definition for creating a dictionary for our corpus.  Number of features (word stems of ngrams) will be the variable optimized for using this function.","216bc2bc":"Definition to train a LDA Model and the compute the coherence score.","e4cd6547":"# Objective function ","c448dfd8":"In this portion of our effort to tune some of the LDA parameters, we will try configurations of several hyperparameters.\n\nA topic model learns a topic-feature matrix of abstract topics and features (word or ngrams) and a document-topic matrix of documents and topics, from a document-feature matrix of documents and features.  From this factorization we achieve statistical feature vectors for each topic and topic vectors for each document in the training corpus.  We can then find topic vectors for the questions we would like to ask the corpus of documents.  We will use the closest matching documents in the CORD-19 dataset in an attempt to answer the task questions.  LDA assumes a Dirichlet prior on topic-feature and document-topic distributions.  In other words it assumes each topic is defined by and small collection of words or ngrams and that each documnent consists of a small number of topics. ","a2131281":"# Try model configuration","2974f549":"# create dictionary definition","b2d82dd2":"# Create term frequency corpus definition","bbc4974e":"# Additional Imports","5d0b1c7d":"Pickle will be used to open data from the preprocessing step; gensim will be used for all LDA tasks; matplotlib will be used for visualization;","2516d9c7":"# Try ngram, n_feats, and n_topics configurations","8c03e9c3":"Definition for creating the corpus from the dictionary and documents.","6d093eb0":"# Introduction"}}