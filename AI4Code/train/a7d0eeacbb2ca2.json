{"cell_type":{"ebd0677f":"code","5f7be4f8":"code","1d93410e":"code","b64de4fa":"code","903f0782":"code","e8f237fc":"code","54051c6f":"code","fea97dab":"code","9915ff40":"code","bc2d0e18":"code","1d150fcc":"code","201387e4":"code","d19de823":"code","5ec7bcb1":"code","8f85d9c0":"code","65ccf5ca":"code","6fb7b6b7":"code","0ee904df":"code","391ed2ae":"code","24cffd75":"code","779fe76b":"code","1fe919e0":"code","c43cdcb7":"code","765b6fcf":"code","498cc6cf":"code","2f9ebcb6":"code","999fc5c1":"code","afaea8db":"code","36173e3c":"code","3296d952":"code","b4a9e776":"code","8075fb1a":"code","d13f1d3e":"code","f666c8f9":"code","5a4ab394":"code","303f8037":"code","f3fe275e":"code","f2d3c5fe":"code","1d13f1b6":"code","a9b9b1a6":"code","7e9aff76":"code","a1d690a4":"code","dec8c77e":"code","48fe6510":"code","7c71eedf":"code","f6044438":"code","e1f43254":"code","bc1722c6":"code","fef6df76":"code","90f63141":"code","bff77837":"code","fcdc162a":"code","788d14e9":"code","a6077390":"code","cc07f604":"code","a06b8b05":"code","6eef7ccc":"code","a83b683a":"code","3d4a666e":"code","eae662a1":"code","d50a70df":"code","c5692739":"code","130451a0":"code","e92fa587":"code","a84f26a2":"code","d7c5e2cf":"code","a9978061":"code","58e0edb5":"code","573d2da4":"code","b81df393":"code","5c4bac0a":"markdown","b65f4171":"markdown","18744c5d":"markdown","befae60a":"markdown","9fc3f633":"markdown","d0266f4a":"markdown","bb13369b":"markdown","1abe7574":"markdown","88fd0351":"markdown","2e803ef3":"markdown","b578bcf9":"markdown","7185f693":"markdown","727dc10d":"markdown","9cb637de":"markdown","282b7be9":"markdown","31be489a":"markdown","4d1a85c8":"markdown","41e37a0c":"markdown","4993b529":"markdown","cc6658c6":"markdown","6e1081fe":"markdown","db22c16d":"markdown","64b96acd":"markdown","5fd696c8":"markdown","8cb41458":"markdown","bb9544ec":"markdown","67baa47e":"markdown","7380bae8":"markdown","9dcfd743":"markdown","b9b882a3":"markdown","c56d5106":"markdown","7a79264a":"markdown","8bd7ac90":"markdown","6a4fd7e1":"markdown","886ca741":"markdown","8630f84f":"markdown","f133496a":"markdown"},"source":{"ebd0677f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nimport os\npath_list = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        path_list.append(os.path.join(dirname, filename))\n        print(os.path.join(dirname, filename))","5f7be4f8":"train_df = pd.read_csv(\"\/kaggle\/input\/loan-prediction-based-on-customer-behavior\/Training Data.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/loan-prediction-based-on-customer-behavior\/Test Data.csv\")\ntargets = pd.read_csv(\"\/kaggle\/input\/loan-prediction-based-on-customer-behavior\/Sample Prediction Dataset.csv\")","1d93410e":"train_df.head()","b64de4fa":"test_df.head()","903f0782":"targets.head()","e8f237fc":"print(f\"The training data set size: {train_df.shape}\")\nprint(f\"The test data set size: {test_df.shape}\")","54051c6f":"train_df.describe()","fea97dab":"train_df.info()","9915ff40":"# get numeric columns\ntrain_num = train_df.select_dtypes(include=[\"int64\"]).set_index(\"Id\")\ntrain_num.head()","bc2d0e18":"def num_sub_plots_hist(df):\n    \"\"\"\n    Input: numerical data\n    Output: plots histograms of the features compared to the label\n    \"\"\"\n    num_plots = len(df.columns)\n    fig, axs = plt.subplots(num_plots, 1, figsize=(8,3*num_plots))\n    fig.tight_layout()\n    for col, ax in enumerate(axs):\n        sns.histplot(df, x=df.columns[col], hue=\"Risk_Flag\",ax=ax)\n        \ndef num_sub_plots_box(df):\n    \"\"\"\n    Input: numerical data\n    Output: plots boxplots of the features compared to the label\n    \"\"\"\n    num_plots = len(df.columns)\n    fig, axs = plt.subplots(num_plots-1, 1, figsize=(6,22))\n    fig.tight_layout()\n    for col, ax in enumerate(axs):\n        if df.columns[col] != \"Risk_Flag\":\n            sns.boxplot(data=df, y=df.columns[col], x=\"Risk_Flag\", ax=ax)","1d150fcc":"num_sub_plots_hist(train_num)","201387e4":"num_sub_plots_box(train_num)","d19de823":"sns.pairplot(train_num.iloc[:10000,:], hue=\"Risk_Flag\", corner=True);","5ec7bcb1":"def names_for_poly_df(df):\n    \"\"\"\n    Input: a numerical dataframe\n    Output: returns a list of the polynomial feature column names in a more readable format\n    \"\"\"\n    \n    poly_cols = create_poly_df(df).columns\n    poly_col_dic = {k:v for k,v in zip(poly_cols[:len(df.columns)-1], df.columns[:-1])}\n    \n    new_cols = []\n    for col in poly_cols[:-1]:\n        split_col_space = col.split(\" \")\n        temp_scol = []\n        for scol in split_col_space:\n            if len(split_col_space) > 1:\n                temp_scol.append(scol.replace(scol[:2], poly_col_dic[scol[:2]]))\n            else:\n                new_cols.append(scol.replace(scol[:2], poly_col_dic[scol[:2]]))\n        new_cols.append(\" * \".join(temp_scol))\n        \n    names = [name for name in new_cols if name != \"\"]\n    names.append(\"Risk_Flag\")\n\n    return names","8f85d9c0":"def create_poly_df(df, poly=2):\n    \"\"\"\n    Inputs: a numerical dataframe and number of polynomal features\n    Output: a dataframe with polynomical feature combinations\n    \"\"\"\n    \n    polyclass = PolynomialFeatures(poly)\n    train_poly = pd.DataFrame(data=polyclass.fit_transform(train_num.drop(labels=\"Risk_Flag\", axis=1)),\n                 columns=polyclass.get_feature_names(), index=train_num.index)\n    if train_poly.columns[-1] != \"Risk_Flag\":\n        train_poly = pd.concat([train_poly, train_num[\"Risk_Flag\"]],axis=1).drop(labels=\"1\", axis=1)\n        \n        \n    return train_poly","65ccf5ca":"create_poly_df(train_num).head()","6fb7b6b7":"print(names_for_poly_df(train_num))","0ee904df":"test_df = create_poly_df(train_num)\ntest_df.columns = names_for_poly_df(train_num)\ntest_df.head()","391ed2ae":"num_sub_plots_hist(test_df)","24cffd75":"train_cat = pd.concat([train_df.select_dtypes(include=[\"object\"]), train_df[\"Risk_Flag\"]], axis=1)\ntrain_cat.head()","779fe76b":"len(train_cat[\"STATE\"].value_counts())","1fe919e0":"def plot_cat_features(feature_name, limit=5, ascending=False):\n    \"\"\"\n    Input: Feature names, the total number of categoricals looked at, and the sorting prefered\n    Output: returns a plot of the categoricals vs loan default, returns the highest percentage defaults\n    \"\"\"\n    # creates a df of categorical as index and risk_flag as columns, the values are the total counts.\n    group_risk = train_cat.groupby([feature_name,\"Risk_Flag\"])[feature_name].count().unstack()\n    \n    # loops through each row and updates the values as a percentage\n    for row in group_risk.index:\n        group_risk.loc[row] = group_risk.loc[row]\/group_risk.loc[row].sum()\n    \n    # orders the values by highest percentage loan defaults first\n    group_risk = group_risk.sort_values(by=1, ascending=ascending)[:limit]\n    \n    # plots the data as a bar plot\n    group_risk.plot(kind=\"bar\", figsize=(2*len(group_risk.index), 6))\n    plt.title(f\"{feature_name} Risk_Flag Count\")\n    plt.ylabel(\"Percentage\");","c43cdcb7":"for col in train_cat.columns[:-1]:\n    plot_cat_features(col)","765b6fcf":"train_cat[(train_cat[\"Married\/Single\"] == \"single\")\n          & (train_cat[\"House_Ownership\"] == \"rented\")\n          & (train_cat[\"Car_Ownership\"] == \"no\")\n          & (train_cat[\"Profession\"] == \"Police_officer\")\n          & (train_cat[\"CITY\"] == \"Bhubaneswar\")][\"Risk_Flag\"].value_counts()","498cc6cf":"mean_default = 100*train_cat[\"Risk_Flag\"].value_counts().values[1]\/train_cat[\"Risk_Flag\"].value_counts().sum()\nprint(\"The mean default rate for this data is: {}%\".format(mean_default))","2f9ebcb6":"# shows the best proffesions, cities, and states to live in for loan defaults:\n\nfor col in train_cat.columns[-4:-1]:\n    plot_cat_features(col, ascending=True)","999fc5c1":"print(\"Total number of unique professions: {}\".format(train_cat[\"Profession\"].nunique()))\nprint(\"Total number of unique cities: {}\".format(train_cat[\"CITY\"].nunique()))\nprint(\"Total number of unique states: {}\".format(train_cat[\"STATE\"].nunique()))","afaea8db":"# creates a df of categorical as index and risk_flag as columns, the values are the total counts.\ndef risk_flag_ranked_dictionary(df, feature):\n    group_risk = df.groupby([feature,\"Risk_Flag\"])[feature].count().unstack()\n\n    # loops through each row and updates the values as a percentage\n    for row in group_risk.index:\n        group_risk.loc[row] = group_risk.loc[row]\/group_risk.loc[row].sum()\n\n    # orders the values by highest percentage loan defaults first\n    group_risk = group_risk.sort_values(by=1, ascending=True)\n    group_risk[\"Risk_Rank\"] = range(1,len(group_risk)+1)\n    \n    return group_risk[\"Risk_Rank\"].to_dict()","36173e3c":"# example of output:\nrisk_flag_ranked_dictionary(train_cat, \"Profession\")","3296d952":"def transform_categorical_to_risk_rank(row, dic):\n    return dic[row]","b4a9e776":"for feature in train_cat.columns[-4:-1]:\n    dic = risk_flag_ranked_dictionary(train_cat, feature)\n    train_cat[feature] = train_cat[feature].apply(lambda row: transform_categorical_to_risk_rank(row, dic))","8075fb1a":"train_cat.head()","d13f1d3e":"def features_to_one_hot(df):\n    \"\"\"\n    Input: The categorical data frame.\n    Output: The dataframe with one-hot encoded features, drop_first is used to remove redundant features and prevent\n            multicollinearity. \n    \"\"\"\n    \n    for feature in df.columns[0:3]:\n        df = pd.concat([pd.get_dummies(df[feature], drop_first=True), df.drop(labels=feature,axis=1)],axis=1)\n        \n    return df","f666c8f9":"train_cat = features_to_one_hot(train_cat)\ntrain_cat.head()","5a4ab394":"train_num.head()","303f8037":"train_cat.head()","f3fe275e":"def combine_dfs(df1, df2):\n    if df1.index[0] > 0:\n        df1.index -= 1 # fix index shift\n    train_df = pd.concat([df1, df2], axis=1)\n    new_col_names = [col.lower().capitalize() for col in train_df.columns]\n    train_df.columns = new_col_names\n    return train_df","f2d3c5fe":"combine_dfs(train_num, train_cat).head()","1d13f1b6":"# this code repeats a lot from above, its just used to quickly pull the data from csv and have it in the correct format\n# by running main()\n\ndef import_data(train=True):\n    \"\"\"\n    Input: Bool asking for either train or test data to be returned\n    Output: Either the training data or the test data\n    \"\"\"\n    \n    if train:\n        return pd.read_csv(\"\/kaggle\/input\/loan-prediction-based-on-customer-behavior\/Training Data.csv\")\n    else:\n        df_test = pd.read_csv(\"\/kaggle\/input\/loan-prediction-based-on-customer-behavior\/Test Data.csv\")\n        df_test.rename(columns={\"ID\":\"Id\"}, inplace=True)\n        targets = pd.read_csv(\"\/kaggle\/input\/loan-prediction-based-on-customer-behavior\/Sample Prediction Dataset.csv\")\n        targets.rename(columns={\"id\":\"Id\", \"risk_flag\":\"Risk_Flag\"}, inplace=True)\n        \n        return pd.concat([df_test,targets.drop(\"Id\",axis=1)],axis=1)\n\n    \ndef split_num_cat(df, num=True):\n    \"\"\"\n    Input: the whole dataframe, if the user wants to return just the numerical to categorical features\n    Output: Either a df with numerical features or categroical features\n    \"\"\"\n    \n    if num:\n        return df.select_dtypes(include=[\"int64\"]).set_index(\"Id\")\n    else:\n        return pd.concat([df.select_dtypes(include=[\"object\"]), df[\"Risk_Flag\"]], axis=1)\n\n    \ndef risk_flag_ranked_dictionary(df, feature):\n    \"\"\"\n    Input: a categorical dataframe, the feature that is being ranked interms of risk flag\n    Output: a dictionary of the unique categorical values, with items as the relative risk\n    \"\"\"\n    \n    group_risk = df.groupby([feature,\"Risk_Flag\"])[feature].count().unstack()\n\n    # loops through each row and updates the values as a percentage\n    for row in group_risk.index:\n        group_risk.loc[row] = group_risk.loc[row]\/group_risk.loc[row].sum()\n\n    # orders the values by highest percentage loan defaults first\n    group_risk = group_risk.sort_values(by=1, ascending=True)\n    group_risk[\"Risk_Rank\"] = range(1,len(group_risk)+1)\n    \n    return group_risk[\"Risk_Rank\"].to_dict()\n\n\ndef transform_categorical_to_risk_rank(row, dic):\n    \"\"\"\n    Input: the row of the dataframe, the dictionary from risk_flag_ranked_dictionary for a feature\n    Output: updates the dataframes unique values with relative risk rankings\n    \"\"\"\n    \n    return dic[row]\n\n\ndef cat_to_num(df):\n    \"\"\"\n    Input: \n    \"\"\"\n    for feature in df.columns[-4:-1]:\n        dic = risk_flag_ranked_dictionary(df, feature)\n        df[feature] = df[feature].apply(lambda row: transform_categorical_to_risk_rank(row, dic))\n        \n    return df.drop(\"Risk_Flag\", axis=1)\n        \n    \ndef features_to_one_hot(df):\n    \"\"\"\n    Input: The categorical data frame.\n    Output: The dataframe with one-hot encoded features, drop_first is used to remove redundant features and prevent\n            multicollinearity. \n    \"\"\"\n    \n    for feature in df.columns[0:3]:\n        df = pd.concat([pd.get_dummies(df[feature], drop_first=True), df.drop(labels=feature,axis=1)], axis=1)\n        \n    return df\n\n\ndef combine_dfs(df_num, df_cat):\n    \"\"\"\n    Input: both the numerical and categorical dataframes\n    Output: Both dataframes concatenated into one\n    \"\"\"\n    \n    if df_num.index[0] > 0:\n        df_num.index -= 1 # fix index shift\n    train_df = pd.concat([df_num, df_cat], axis=1)\n    new_col_names = [col.lower().capitalize() for col in train_df.columns]\n    train_df.columns = new_col_names\n    \n    return train_df\n\n\ndef main(train=True):\n    \"\"\"\n    Input: train - used as a control flow for users to either return train or test datasets\n    Ouput: takes a raw csv data and returns a dataframe read for ML\n    \"\"\"\n    \n    if train:\n        df = import_data(train=train)\n    else:\n        df = import_data(train=False)\n    df_num = split_num_cat(df)\n    df_cat = split_num_cat(df, num=False)\n    df_cat = cat_to_num(df_cat)\n    df_cat = features_to_one_hot(df_cat)\n    df = combine_dfs(df_num, df_cat)\n    \n    return df","a9b9b1a6":"test_df = main(train=False)\ntest_df.head()","7e9aff76":"train_df = main()\ntrain_df.head()","a1d690a4":"# split train into train-validation\n# split test into X,y\n# scale data\n# run initial models\n# evaluate models, including tests for bias\/variance\n# error analysis on best model\n# possible more feature analysis\n# final model","dec8c77e":"X = train_df.drop(\"Risk_flag\", axis=1)\ny = train_df[\"Risk_flag\"]\nX_test = test_df.drop(\"Risk_flag\", axis=1)\ny_test = test_df[\"Risk_flag\"]\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n\nprint(\"X_train shape: {}\".format(X_train.shape))\nprint(\"X_val shape: {}\".format(X_val.shape))\nprint(\"X_test shape: {}\".format(X_test.shape))","48fe6510":"scaler = StandardScaler().fit(X_train)\nX_train = scaler.transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)","7c71eedf":"def evaluate(model, train=False, val=False, test=False):\n    # returns train results    \n    if train == True:\n        name = model\n        pred = model.predict(X_train)\n        plt.figure(figsize=(4,4))\n        sns.heatmap(confusion_matrix(y_train, pred),\n                    annot=True, fmt='g', cmap=\"BuGn\", cbar=False)\n\n        plt.xlabel(\"Predicted Values\", fontsize=14)\n        plt.ylabel(\"True Values\", fontsize=14)\n        plt.title(\"Confusion Matrix Train\", fontsize=18);\n        print(\"-----------------------TRAINING SCORES-----------------------\")\n        print(\"\")\n        print(str(model)+\" accuracy score: {:.2f}\".format(accuracy_score(y_train, pred)))\n        print(str(model)+\" precision score: {:.2f}\".format(precision_score(y_train, pred, zero_division=0)))\n        print(str(model)+\" recall score: {:.2f}\".format(recall_score(y_train, pred)))\n        print(\"\")\n        \n    # returns val results\n    if val == True:\n        name = model\n        pred = model.predict(X_val)\n        plt.figure(figsize=(4,4))\n        sns.heatmap(confusion_matrix(y_val, pred),\n                    annot=True, fmt='g', cmap=\"BuGn\", cbar=False)\n\n        plt.xlabel(\"Predicted Values\", fontsize=14)\n        plt.ylabel(\"True Values\", fontsize=14)\n        plt.title(\"Confusion Matrix Val\", fontsize=18);\n        print(\"----------------------VALIDATION SCORES---------------------\")\n        print(\"\")\n        print(str(model)+\" accuracy score: {:.2f}\".format(accuracy_score(y_val, pred)))\n        print(str(model)+\" precision score: {:.2f}\".format(precision_score(y_val, pred, zero_division=0)))\n        print(str(model)+\" recall score: {:.2f}\".format(recall_score(y_val, pred)))\n        print(\"\")\n        \n    # completely unseen data\n    if test == True:\n        name = model\n        pred = model.predict(X_test)\n        plt.figure(figsize=(4,4))\n        sns.heatmap(confusion_matrix(y_test, pred),\n                    annot=True, fmt='g', cmap=\"BuGn\", cbar=False)\n\n        plt.xlabel(\"Predicted Values\", fontsize=14)\n        plt.ylabel(\"True Values\", fontsize=14)\n        plt.title(\"Confusion Matrix Test\", fontsize=18);\n        print(\"-------------------------TEST SCORES-----------------------\")\n        print(\"\")\n        print(str(model)+\" accuracy score: {:.2f}\".format(accuracy_score(y_test, pred)))\n        print(str(model)+\" precision score: {:.2f}\".format(precision_score(y_test, pred, zero_division=0)))\n        print(str(model)+\" recall score: {:.2f}\".format(recall_score(y_test, pred)))","f6044438":"clf_lr = LogisticRegression().fit(X_train, y_train)\nevaluate(clf_lr, train = True, val = True)","e1f43254":"clf_rf = RandomForestClassifier().fit(X_train, y_train)\nevaluate(clf_rf, train=True, val=True)","bc1722c6":"clf_gnb = GaussianNB().fit(X_train, y_train)\nevaluate(clf_gnb, train=True, val=True)","fef6df76":"clf_knn = KNeighborsClassifier().fit(X_train, y_train)\nevaluate(clf_knn, train=True, val=True)","90f63141":"clf_adb = AdaBoostClassifier().fit(X_train, y_train)\nevaluate(clf_adb, train=True, val=True)","bff77837":"clf_nn = MLPClassifier(hidden_layer_sizes=(64,64,64,64,64,32), max_iter=100).fit(X_train,y_train)\nevaluate(clf_nn, train=True, val=True)","fcdc162a":"feature_importance = pd.Series(data=clf_rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n\nsns.barplot(x=feature_importance.index, y=feature_importance.values)\nplt.ylabel(\"Feature Importance\")\nplt.title(\"Feature Importance from a RandomForest\")\nplt.xticks(rotation='60');","788d14e9":"pred = clf_rf.predict(X_val)","a6077390":"error_df = pd.DataFrame({\"Predicted Value\": pred, \"True Value\": y_val})\n\ndef error(row):\n    if row[0] == 0 and row[1] == 1:\n        return 1\n    if row[0] == 1 and row[1] == 0:\n        return 1\n\nerror_df[\"Check\"] = error_df.apply(lambda row: error(row), axis=1)","cc07f604":"error_df.head()","a06b8b05":"all_error = error_df[error_df.Check == 1]\nall_error.head()","6eef7ccc":"# all the data that the model predicted incorrect on (validation set only)!\n\nval_error_df = train_df.loc[all_error.index,:]\nval_error_df.rename(columns={\"Risk_flag\":\"Risk_Flag\"}, inplace =True)\nval_error_df.head()","a83b683a":"## Random Forest:\n#\n#from sklearn.model_selection import cross_val_score, GridSearchCV\n#\n#grid = {\"n_estimators\": [10,50,100,300,600,1000],\n#       \"max_depth\":[3,5,10,30,None]}\n#\n#grid_rf_model = GridSearchCV(RandomForestClassifier(), param_grid = grid, n_jobs=-2)\n#grid_rf_model.fit(X_train, y_train)\n#\n#params_rf = grid_rf_model.best_params_\n#\n#rf_best_model = RandomForestClassifier(n_estimators=params_rf[\"n_estimators\"], max_depth=params_rf[\"max_depth\"])\n#\n#cross_val_score_rf_best = cross_val_score(rf_best_model, X, y, cv=5, n_jobs=-2)","3d4a666e":"# the grid search above takes a long time, the below was the best found model.\nrf_best_model = RandomForestClassifier(n_estimators=50, max_depth=None)","eae662a1":"rf_best_model.fit(X_train, y_train)\nevaluate(rf_best_model, train = True, val= True, test= True)","d50a70df":"evaluate(clf_rf, train=True, val=True, test=True)","c5692739":"X = train_df.drop(\"Risk_flag\", axis=1)\ny = train_df[\"Risk_flag\"]\nX_test = test_df.drop(\"Risk_flag\", axis=1)\ny_test = test_df[\"Risk_flag\"]\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n\nprint(\"X_train shape: {}\".format(X_train.shape))\nprint(\"X_val shape: {}\".format(X_val.shape))\nprint(\"X_test shape: {}\".format(X_test.shape))\n\nscaler = StandardScaler().fit(X_train)\nX_train = scaler.transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)","130451a0":"rf_best_model.fit(X_train, y_train)\nevaluate(rf_best_model, train = True, val= True, test= True)","e92fa587":"train_df = main()\ntest_df = main(train=False)\n\ntop_5_features = list(feature_importance.index[:1])\ntop_5_features.append(\"Risk_flag\")\ntrain_df = train_df[top_5_features]\ntest_df = test_df[top_5_features]\n\nX = train_df.drop(\"Risk_flag\", axis=1)\ny = train_df[\"Risk_flag\"]\nX_test = test_df.drop(\"Risk_flag\", axis=1)\ny_test = test_df[\"Risk_flag\"]\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n\nprint(\"X_train shape: {}\".format(X_train.shape))\nprint(\"X_val shape: {}\".format(X_val.shape))\nprint(\"X_test shape: {}\".format(X_test.shape))\n\nscaler = StandardScaler().fit(X_train)\nX_train = scaler.transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)","a84f26a2":"rf_best_model.fit(X_train, y_train)\nevaluate(rf_best_model, train = True, val= True, test= True)","d7c5e2cf":"X_poly2 = create_poly_df(X)\nX_poly2.head()","a9978061":"train_df = create_poly_df(main())\ntest_df = create_poly_df(main(train=False))\ntest_df.head()","58e0edb5":"train_df.head()","573d2da4":"train_df = create_poly_df(main())\ntest_df = create_poly_df(main(train=False))\n\nX = train_df.drop(\"Risk_Flag\", axis=1)\ny = train_df[\"Risk_Flag\"]\nX_test = test_df.drop(\"Risk_Flag\", axis=1)\ny_test = test_df[\"Risk_Flag\"]\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n\nprint(\"X_train shape: {}\".format(X_train.shape))\nprint(\"X_val shape: {}\".format(X_val.shape))\nprint(\"X_test shape: {}\".format(X_test.shape))\n\nscaler = StandardScaler().fit(X_train)\nX_train = scaler.transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(X_test)","b81df393":"rf_best_model.fit(X_train, y_train)\nevaluate(rf_best_model, train = True, val= True, test= True)","5c4bac0a":"#### Try less features:","b65f4171":"**The best model seems to be the RandomForestClassifier, however, as per usualy with RFs it has some overtraining\/variance problems**","18744c5d":"**Note: Error analysis takes a long time... I will not be pursuing it this time**","befae60a":"#### Trying a larger training set:","9fc3f633":"**Adding in Polynomial Features improved Test Score Performance massively!**","d0266f4a":"## Categorical Features:","bb13369b":"## EDA","1abe7574":"#### Models:","88fd0351":"## Transform Categorical Values into One Hot Encodings:","2e803ef3":"If i turned all the categoricals into columns my dataset would end up with 408 columns! At the moment it has 12.","b578bcf9":"**No difference**","7185f693":"## GridSearch on RandomForest","727dc10d":"**According to the data the worst thing you could do for your loan application would be (in order of worst):**\n- live in Bhubaneswar\n- Be in the state of Manipur\n- Be a police officer\n- rent\n- not have a car\n- be single","9cb637de":"**Plotting polynomial feature combinations:**","282b7be9":"## Predicting if someone will make a default on their loan","31be489a":"### Error Analysis","4d1a85c8":"### Load Data","41e37a0c":"## Building DataPipeline for Train\/Test Data","4993b529":"**Polynomical combinations dont seem to yeild better seperation either**","cc6658c6":"**The model performas poorly on unseen data**\n\n- The best metrics for this data would be either recall or precision (f1 score for a balance between the two)\n- All models show high variance for precision and recall.\n- High precision could be fixed by getting more data, trying a smaller set of features, increasing regularization (where possible).","6e1081fe":"**No NAN values!**","db22c16d":"**small improvement when only using 2 features (income and city)**","64b96acd":"## Machine Learning Section:","5fd696c8":"**Group values in the Proffession, City, and State into bins to reduce number of columns**","8cb41458":"### Numerical Columns","bb9544ec":"**I will try changing the unique names to numbers where a low number shows a low Risk_Flag rate**\n\nThis will be done base on the above % Risk Flag for each unique value","67baa47e":"## Review:","7380bae8":"#### Train Validation Test Split:","9dcfd743":"### One-hot encode the other features:","b9b882a3":"**No single variabel stands out as having good seperation of the target variable, perhaps linear and polynomical combinations will heald some seperation**","c56d5106":"**No clear linear combination of features shows good seperation of target variable**","7a79264a":"#### Scaling Data","8bd7ac90":"## Combine Datasets:","6a4fd7e1":"**39%** of people who are singel, rent, dont have a car, are a police officer and live in Bhubaneswar make loan defaults!","886ca741":"### Try more Features...","8630f84f":"#### Polynomial Combinations of Features","f133496a":"**How**: First I need to create a dictionaries with keys as unique values and items as the relative risk_flag score for state, city, profession features.\n\nI will then loop through the data set and use the dictionary to update the unique categorical values to these dictionary item values"}}