{"cell_type":{"4d731dab":"code","6daa633a":"code","226368cf":"code","8cbbb7f1":"code","bfea04b6":"code","9f774688":"code","17946d7e":"code","74ab01a1":"code","b5ac9e63":"code","ba32f40e":"code","f13bcb13":"code","54e8aee8":"code","a3158ea5":"code","ec23e2ff":"code","4415610a":"code","a54d01ad":"code","dcdcecaf":"code","a72560f2":"code","5ebc2591":"code","afe93757":"code","6016e712":"code","54ea7e64":"code","d49db46b":"code","7cce7121":"code","a2718126":"code","25932d9b":"code","5a9cd766":"code","4bd9b0f6":"code","d9d52092":"code","67e4afcb":"code","22e3b062":"code","23a0a411":"code","dc9a50df":"code","31665f1a":"code","48c3c7c5":"code","417aea8c":"code","5280d2a0":"code","4b2a0048":"code","04782392":"code","177a8e24":"code","0b1a63df":"code","f1369029":"code","b8d8e1d2":"code","3a0f64e0":"code","7dc9b369":"code","eb93a820":"code","d12403a1":"code","436aabf4":"code","25787b9b":"code","67f060d3":"code","76df8ad2":"code","132a7088":"code","1039d66b":"code","cd1bbf75":"code","1ac57312":"code","9bee9353":"code","bb9a3fe1":"code","500ebdc4":"code","d5ddef8d":"code","5c8fb4e9":"code","a5cf1151":"code","b95a4d75":"code","c3bc0a6b":"code","449c9231":"code","de817573":"code","8311a6ba":"code","fc22a036":"code","7de3a00e":"code","064b4d25":"code","c59dce59":"code","b3aa6173":"code","9a6fff67":"code","9d2be63b":"code","58a36700":"markdown","ee78879c":"markdown","4506dd0b":"markdown","1202905b":"markdown","de044685":"markdown","dc6f976d":"markdown","dea2aad8":"markdown","97bab140":"markdown","f82bcd6a":"markdown","174166e3":"markdown","3cc630c4":"markdown","947b780f":"markdown","4e6cb0b8":"markdown","6660174b":"markdown","3e444f91":"markdown","082f0007":"markdown","af6480ee":"markdown","99e4a859":"markdown","4fd5c2cc":"markdown","5ba4b611":"markdown","21526171":"markdown","17b9ecfc":"markdown","fb646676":"markdown","f7ec4203":"markdown","07b0fbfa":"markdown","6099b390":"markdown","a40ca53e":"markdown","dd466f1c":"markdown","389e1a01":"markdown","a96d9d4b":"markdown","04ce591c":"markdown","6e9f362e":"markdown","c981b02c":"markdown","be857861":"markdown","aa266591":"markdown","27115deb":"markdown","6bb34f44":"markdown","02577913":"markdown","c49117c1":"markdown","f8b7adcf":"markdown","f91c6df7":"markdown","d865722e":"markdown","2a037b2c":"markdown","580365f7":"markdown","ab368032":"markdown","6cf9aa05":"markdown","d4f6c834":"markdown","014d14b6":"markdown","8ca18b58":"markdown"},"source":{"4d731dab":"# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom sklearn.metrics import classification_report, mean_squared_error, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import KFold,cross_val_score, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom scipy.special import boxcox1p\nfrom xgboost import XGBRegressor","6daa633a":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","226368cf":"print(\"Training set size:\", train.shape)\nprint(\"Test Set size: \", test.shape)","8cbbb7f1":"print(\"Train set columns not  in test set\")\nprint([train_col for train_col in train.columns if train_col not in test.columns ])\nprint (\"Test set columns not in test set\")\nprint ([test_col for test_col in test.columns if test_col not in train.columns])","bfea04b6":"#View first 10 rows of data in test and training\nprint(train.head(10))\nprint(test.head(10))","9f774688":"train.describe()","17946d7e":"train_ID = train['Id']\ntest_ID = test['Id']","74ab01a1":"train.drop(['Id'], axis = 1, inplace = True)\ntest.drop(['Id'], axis = 1, inplace = True)","b5ac9e63":"cormatrix = train.corr()\ncorplot = plt.subplots(figsize =(15,12))\nsns.heatmap(cormatrix, vmin = -1, vmax = 1,cbar = True, square = True, cmap = 'coolwarm')","ba32f40e":"k = 10\ncorr_cols = cormatrix.nlargest(k, 'SalePrice')['SalePrice'].index\ncormatrix2 = np.corrcoef(train[corr_cols].values.T)\nsns.set(font_scale = 1)\nsns.heatmap(cormatrix2,cbar = True, square = True , cmap = 'coolwarm',annot_kws={'size': 10}, annot = True , xticklabels = corr_cols.values, yticklabels = corr_cols.values)","f13bcb13":"#ScatterPlot Matrix of most correlated features\nsns.set(style = 'ticks', color_codes= True)\npltmatrix = sns.pairplot(train, vars = ['SalePrice','OverallQual', 'GrLivArea','GarageCars','TotalBsmtSF','FullBath','YearBuilt'])\n","54e8aee8":"#Deleting outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)","a3158ea5":"sns.distplot(train['SalePrice'] , fit=norm)","ec23e2ff":"train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])","4415610a":"sns.distplot(train['SalePrice'] , fit=norm);","a54d01ad":"train_num = train.shape[0]\ntest_num = test.shape[0]\ny_train = train.SalePrice.values","dcdcecaf":"data = pd.concat((train, test)).reset_index(drop=True)\ndata.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"Concatenated dataframe size is :\", (data.shape))","a72560f2":"#sum missing data calculate percentage missing in each column \nmissing_df = pd.DataFrame({'Total':train.isnull().sum(), 'Percentage':(train.isnull().sum())\/1460*100})\nmissing_df = missing_df.sort_values(by = 'Total',ascending = False)\nmissing_df = missing_df.loc[missing_df['Total'] > 0]\nprint(missing_df)","5ebc2591":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=missing_df.index, y=missing_df['Percentage'].values)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","afe93757":"for col in ('PoolQC', 'MiscFeature', 'Alley', 'Fence'):\n    data.drop([col], axis = 1, inplace = True)","6016e712":"data.drop(['GarageArea','TotRmsAbvGrd', '1stFlrSF'], axis = 1, inplace = True)","54ea7e64":"##Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\ndata[\"LotFrontage\"] = data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median())) ","d49db46b":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    data[col] = data[col].fillna('None')","7cce7121":"for col in ('GarageYrBlt', 'GarageCars'):\n    data[col] = data[col].fillna(0)    \n","a2718126":"for col in ('BsmtFinSF1', 'BsmtFinSF2','BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    data[col] = data[col].fillna(0)   \n","25932d9b":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    data[col] = data[col].fillna('None')","5a9cd766":"data[\"FireplaceQu\"] = data[\"FireplaceQu\"].fillna(\"None\")","4bd9b0f6":"data[\"MasVnrType\"] = data[\"MasVnrType\"].fillna(\"None\")\ndata[\"MasVnrArea\"] = data[\"MasVnrArea\"].fillna(0)    ","d9d52092":"data['MSZoning'] = data['MSZoning'].fillna(data['MSZoning'].mode()[0])\n","67e4afcb":"data.drop(['Utilities'], axis=1, inplace = True)","22e3b062":"data[\"Functional\"] = data[\"Functional\"].fillna(\"Typ\")","23a0a411":"data['Electrical'] = data['Electrical'].fillna(data['Electrical'].mode()[0], inplace = True)\n\n","dc9a50df":"data['KitchenQual'] = data['KitchenQual'].fillna(data['KitchenQual'].mode()[0])","31665f1a":"data['Exterior1st'] = data['Exterior1st'].fillna(data['Exterior1st'].mode()[0])\ndata['Exterior2nd'] = data['Exterior2nd'].fillna(data['Exterior2nd'].mode()[0])\n","48c3c7c5":"data['SaleType'] = data['SaleType'].fillna(data['SaleType'].mode()[0])","417aea8c":"data.select_dtypes(include=['int64','float64']).columns","5280d2a0":"data.select_dtypes(include=['object']).columns","4b2a0048":"#MSSubClass \ndata['MSSubClass'] = data['MSSubClass'].apply(str)","04782392":"data['YrSold'] = data['YrSold'].astype(str)\ndata['MoSold'] = data['MoSold'].astype(str)","177a8e24":"#Counting unique values in each of the categorical features with ordered categorical values\nfor col in ('ExterQual','ExterCond','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','HeatingQC','KitchenQual','FireplaceQu','GarageQual','GarageCond'):\n    print(col)\n    print(data[col].value_counts())\n","0b1a63df":"\nmap1 = {'Po': 1, 'Fa': 2, 'TA': 3,'Gd': 4, 'Ex': 5}\n\nfor col in ('ExterCond', 'HeatingQC'):\n    data[col] = data[col].map(map1)","f1369029":"map2 = {'None': 1, 'Po': 2, 'Fa': 3, 'TA': 4,'Gd': 5, 'Ex': 6}\n\nfor col in ('GarageCond', 'GarageQual','FireplaceQu'):\n    data[col] = data[col].map(map2)","b8d8e1d2":"map3 = {'None':1, 'Unf': 2, 'LwQ': 3, 'Rec': 4, 'BLQ': 5,'ALQ': 6, 'GLQ': 7}\n\nfor col in ('BsmtFinType2', 'BsmtFinType1'):\n    data[col] = data[col].map(map3)","3a0f64e0":"map4 = { 'Fa': 1, 'TA': 2,'Gd': 3, 'Ex': 4}\n\nfor col in ('ExterQual', 'KitchenQual'):\n    data[col] = data[col].map(map4)","7dc9b369":"map5 = {'None': 1, 'Fa': 2, 'TA': 3,'Gd': 4, 'Ex': 5}\ndata['BsmtQual'] = data['BsmtQual'].map(map5)  ","eb93a820":"map6 = {'None': 1,'Po': 2, 'Fa': 3, 'TA': 4,'Gd': 5,}\ndata['BsmtCond'] = data['BsmtCond'].map(map6) ","d12403a1":"map7 = {'None': 1,'No': 2, 'Mn': 3, 'Av': 4,'Gd': 5,}\ndata['BsmtExposure'] = data['BsmtExposure'].map(map7) ","436aabf4":"cat_cols = ['BldgType', 'CentralAir', 'Condition1', 'Condition2', 'Electrical',\n            'Exterior1st','Exterior2nd','Foundation', 'Functional','GarageFinish','GarageType','Heating', 'HouseStyle',\n            'LandContour', 'LandSlope', 'LotConfig', 'LotShape', 'MSZoning','MasVnrType', 'Neighborhood', 'PavedDrive', \n            'RoofMatl', 'RoofStyle','SaleCondition', 'SaleType', 'Street','MoSold', 'YrSold',\n             'MSSubClass']","25787b9b":"data = pd.get_dummies(data, columns=cat_cols,prefix=cat_cols,drop_first=True )","67f060d3":"num_feature = ['2ndFlrSF', '3SsnPorch', 'BsmtFinSF1', 'BsmtFinSF2','BsmtUnfSF',\n               'EnclosedPorch','GrLivArea', 'GarageYrBlt','LotArea', 'LotFrontage', 'LowQualFinSF', 'MasVnrArea',\n               'MiscVal', 'OpenPorchSF', 'PoolArea', 'ScreenPorch', 'TotalBsmtSF', 'WoodDeckSF','YearBuilt', 'YearRemodAdd']","76df8ad2":"# Check the skew of all numerical features\nskewness = data[num_feature].skew()\nskewness = skewness[abs(skewness) > 0.75]\nskewed_feature = skewness.index","132a7088":"for feature in skewed_feature:\n    data[feature] = np.log1p(data[feature])","1039d66b":"X_train = data[:1458]\nX_test = data[1458:]","cd1bbf75":"def print_parameters(select_param, select_param_name, parameters):\n    grid_search = GridSearchCV(estimator = xgb_model,\n                            param_grid = parameters,\n                            scoring = 'neg_mean_squared_error',\n                            cv = 5,\n                            n_jobs = -1)\n\n    grid_result = grid_search.fit(X_train, y_train)\n\n    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n    means = grid_result.cv_results_['mean_test_score']\n    stds = grid_result.cv_results_['std_test_score']\n    params = grid_result.cv_results_['params']\n    for mean, stdev, param in zip(means, stds, params):\n        print(\"%f (%f) with: %r\" % (mean, stdev, param))","1ac57312":"xgb_model = XGBRegressor()","9bee9353":"learning_rate = np.arange(0.01, 0.5, 0.02)\nparameters = dict(learning_rate=learning_rate)\n\nprint_parameters(learning_rate, 'learning_rate', parameters)","bb9a3fe1":"n_estimators = range(100, 1000, 100)\nparameters = dict(n_estimators=n_estimators)\n\nprint_parameters(n_estimators, 'n_estimators', parameters)","500ebdc4":"max_depth = range(0, 5)\nparameters = dict(max_depth=max_depth)\n\nprint_parameters(max_depth, 'max_depth', parameters)","d5ddef8d":"subsample = np.arange(0.2, 1., 0.2)\nparameters = dict(subsample=subsample)\n\nprint_parameters(subsample, 'subsample', parameters)","5c8fb4e9":"colsample_bytree = np.arange(0.2, 1.2, 0.2)\nparameters = dict(colsample_bytree=colsample_bytree)\n\nprint_parameters(colsample_bytree, 'colsample_bytree', parameters)","a5cf1151":"gamma = np.arange(0.001, 0.1, 0.02)\nparameters = dict(gamma=gamma)\n\nprint_parameters(gamma, 'gamma', parameters)","b95a4d75":"min_child_weight = np.arange(0.5, 2.0, 0.2)\nparameters = dict(min_child_weight=min_child_weight)\n\nprint_parameters(min_child_weight, 'min_child_weight', parameters)","c3bc0a6b":"parameters = {  \n                'colsample_bytree':[1],\n                'subsample':[0.4,0.6],\n                'gamma':[0.041],\n                'min_child_weight':[1.1,1.3],\n                'max_depth':[3,5],\n                'learning_rate':[0.2, 0.25],\n                'n_estimators':[400],                                                                    \n                'reg_alpha':[0.75],\n                'reg_lambda':[0.45],\n                'seed':[10]\n}","449c9231":"\ngrid_search = GridSearchCV(estimator = xgb_model,\n                        param_grid = parameters,\n                        scoring = 'neg_mean_squared_error',\n                        cv = 5,\n                        n_jobs = -1)","de817573":"xgb_model = grid_search.fit(X_train, y_train)","8311a6ba":"best_score = grid_search.best_score_\n","fc22a036":"best_parameters = grid_search.best_params_","7de3a00e":"accuracies = cross_val_score(estimator=xgb_model, X=X_train, y=y_train, cv=10)","064b4d25":"accuracies.mean()","c59dce59":"y_pred = xgb_model.predict(X_test)\ny_pred = np.floor(np.expm1(y_pred))","b3aa6173":"submission = pd.concat([test_ID, pd.Series(y_pred)], \n                        axis=1,\n                        keys=['Id','SalePrice'])","9a6fff67":"submission.to_csv('sample_submission.csv', index = False)","9d2be63b":"submission","58a36700":"Lets visualise correlation data . Why is that important ....correlation is a measure of how strongly variables are related.Understandinghow variables are correlated is useful because we can use the value of one variable to predict the value of the other variable. The correlation coefficient ranges from -1.0 to +1.0. The closer it is to +1 or -1, the more closely the two variables are related. Valuea close to 0 mean there is a weak relationship between the variables or none at all. A positive correlation coefficient means that as one variable gets larger the other gets larger and a negative one means that as one gets larger, the other gets smaller. So lets see","ee78879c":"Lets split the data frame again using the number of rows in the previous train test as index","4506dd0b":"**PoolQC** has the highest missing values. From the description NA = the house has no pool and because most houses have no pool (99%) we will drop the column\n\n**MiscFeature** : data description says NA means \"no misc feature\"\nSince approximately 96% of house dont have a misc feature the coulumn  will be dropped\n\nMost houses have no **alley access** and so this feature will be dropped as it wont be helpful \nwith predictions. The same logic is applied to the **fence**  feature which has 80% missing values\n","1202905b":"If you log transform the response\/target variable, it is required to also log transform feature variables that are skewed. For the numerical features , features with a skew value greater than 0.75 with be log transformed","de044685":"Save the Id Columns important for submission at the very end.","dc6f976d":"First lets save the number of observations in the train and test set for when we have to split the concatenated data set .SalePrice is assigned a variable and then dropped from the joined data sets. ","dea2aad8":"**LotFrontage** : Most houses in the same neighbourhood have the same size lot frontage area so we will fill in missing values by the median LotFrontage of the neighborhood.","97bab140":" **GarageYrBlt, GarageArea and GarageCars** : Replacing missing data with 0 \n(Since No garage = no cars in such garage.)","f82bcd6a":"And now the Categorical Features...","174166e3":"One scatterplot looks interesting. From the scatter plot matrix above the matrix for Sale Price and the Living area GrLivArea there are points that are not following the observable trend because they have a large area but low price.These are outliers .\n**An outlier is an observation that lies an abnormal distance from other values in a random sample from a population or data set**\nData outliers can mislead the training process because some machine learning models are sensitive to range and distribution of values and this can result in poor results and less accurate models...WOMP WOMP .So they have to be deleted.... ","3cc630c4":"mostcor = pd.DataFrame(corr_cols)\nmostcor.columns = ['Most Correlated Features']\nmostcor","947b780f":"Lets take a look at how much missing data is in the data set . ","4e6cb0b8":" **Utilities** : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\"  \nand 2 NA . Since the house with 'NoSewa' is in the training set, **this feature won't help in predictive modelling**. We can then safely  remove it.","6660174b":"Training set has one extra column than the test set .Check which columns match and which dont. The mismatch is most likely to be because the test set doesnt have the target feature BUT just to be sure...","3e444f91":"Even though MSSubClass, MoSold, YrSold, YearBuilt, YearRemodAdd , and GarageYrBlt have numerical values they are categorical features and need to be transformed.","082f0007":"Lets see what that looks like with the aid of a bar plot.","af6480ee":"Lets see if it worked . ","99e4a859":"Assign the best values or ranges of values to the parameters obtained above to use with model and then use GridSearch to find the best model with the best score ie the lowest mean squared ","4fd5c2cc":"**BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath** :\nmissing values are likely zero for having no basement\n","5ba4b611":"Lets look at **Numerical** Features in tha data set","21526171":"Load Data","17b9ecfc":" **MSZoning (The general zoning classification)** :  'RL' is by far  \nthe most common value.  So we can fill in missing values with 'RL' \n","fb646676":"Can we observe anything interesting between the features most correlated to SalePrice with scatterplots.... A scatter plot matrix could be a great idea for these features . Lets see.","f7ec4203":"Now we define a function using GridSearch that will pick the best values of XGBoost based on the range of values of parameters we will use with the model. Using the optimal values of parameters will help make better predictions as opposed to using default values of parameters.","07b0fbfa":"**HOUSE PRICE PREDICTION USING XGBOOST**","6099b390":"**SaleType** : Fill in again with most frequent which is \"WD\"","a40ca53e":"**GarageType, GarageFinish, GarageQual and GarageCond** : Replacing missing data with None","dd466f1c":"**Features engineering** <br>\nLet's join the train and test data into one dataframe to transform the datasets simultaneously","389e1a01":"Drop The ID Columns from the data sets as they wont add value to our model...","a96d9d4b":"**MasVnrArea and MasVnrType** : NA most likely means no masonry veneer \nfor these houses. We can fill 0 for the area and None for the type.\n","04ce591c":"**MISSING DATA**","6e9f362e":"And now a quick statistical summary of the training data...","c981b02c":"Tunning Parameters","be857861":"**OUTLIERS**","aa266591":" **KitchenQual**: Only one NA value, and same as Electrical, we set 'TA' \n(which is the most frequent)  for the missing value in KitchenQual.","27115deb":"Lets analyse the target variable Sale Price by looking at its normal distribution plot. A quick note on normal distribution....The normal distribution is a probability function that describes how the values of a variable are distributed. So why are we interested in it....many algorithms in data science assume that the data follows a  normal distibution and make various calculations with this assumption. So the more the data is close to a normal distribution the more it fits the assumption. The errors your model make should have the same variance meaning the difference between predicted values  and the actual values of the target variable should be constant. This can be ensured by making sure that target variable follows a normal distribution. Lets see how SalePrice is distributed","6bb34f44":"List most correlated columns","02577913":"The rest of the categorical features need to be transformed to numerical and this is achieved by using dummy variables.","c49117c1":"The features are described as follows. \n\n* OverallQual: Rates the overall material and finish of the house (1 = Very Poor, 10 = Very Excellent)\n* GrLivArea: Above grade (ground) living area square feet\n* GarageCars: Size of garage in car capacity\n* GarageArea: Size of garage in square feet\n* TotalBsmtSF: Total square feet of basement area\n* 1stFlrSF: First Floor square feet\n* FullBath: Full bathrooms above grade\n* TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n* YearBuilt: Original construction date\n* GarageCars and GarageArea are strongly correlated because the more cars that fit into a garage the more area there is in the garage.\n* We shall only use one Garage value in our model which is GarageCars since has a higher correlation value to SalePrice\n* OverallQual Has the highest correlation value to SalePrice because the overall quality of a house determines the selling price.\n* TotRmsAbvGrd is strongly correlated to GrLivArea because the area of living area above ground is related or determined mostly by the number of rooms above ground.Only one of these will be used\n* TotalBsmtSF and 1stFlrSF are strongly correlated because its likely that the size in area of the first floor determines the size of the basement.We will use TotalBsmtSF ","f8b7adcf":"From correlation matrix of features most related to SalePrice : drop columns mostly correlated to other features.","f91c6df7":"Lets take a look at the ten Features Most Related to Sale Price from the correlation matrix we just plotted.","d865722e":"Confirmed.....The test set has the SalePrice feature missing. Let take a look at the first few rows of the training and test data sets.","2a037b2c":"That does not look normal...\nSale Price is skewed right  (positively skewed) . Skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean . SalePrice will need to be transformed and make it normal distributed. Let's log transform this variable and see if this variable distribution can get any closer to normal.We use the numpy fuction log1p which  applies log(1+x) to all elements of the column.\n","580365f7":"There are ordered categorical features categorical values . These need to be transformed to numerical features with the values changed according to their order . For example ExterQual which is an evaluation of the quality of the material on the exterior <br>\n    Ex\tExcellent  <br>\n   Gd\tGood<br>\n   TA\tAverage\/Typical<br>\n   Fa\tFair<br>\n   Po\tPoor<br>\n   The number of unique values in each column are counted and numerical values are mapped according to the order of the categorical values.","ab368032":"Check the numbers of samples and features.","6cf9aa05":"**Electrical** : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.'''\n","d4f6c834":"For MoSold  replace month number with month name. Year and month sold are transformed into categorical features.","014d14b6":"**Exterior1st and Exterior2nd** : Again Both Exterior 1 & 2 have \nonly one missing value. We will just substitute in the most common string","8ca18b58":"**Functional** : data description says NA means typical"}}