{"cell_type":{"aafec5c7":"code","9f71afae":"code","96a2a96e":"code","b715aee3":"code","2e9e8f2e":"code","1da9b118":"code","74906d80":"code","6396bd7b":"code","bb0714b8":"code","201da3d3":"code","b00991e6":"code","09780beb":"code","a5618b1f":"code","bb0a13f3":"markdown","e39da157":"markdown","f93d4025":"markdown","bf06e7e2":"markdown","03f0bb5a":"markdown","04b3aa10":"markdown","eab066c1":"markdown","5c094cc2":"markdown","3ba4964b":"markdown","0eb686da":"markdown","2f0187eb":"markdown","5d81e659":"markdown","cbefc175":"markdown"},"source":{"aafec5c7":"#from google.colab import drive\n#drive.mount('\/content\/gdrive')","9f71afae":"import matplotlib\nimport sklearn\nimport numpy as np\nimport pandas as pd\nimport sklearn.metrics as metrics\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport os\n\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint,CSVLogger\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.utils import plot_model\n\n\nprint(\"Versions of key libraries\")\nprint(\"---\")\nprint(\"tensorflow: \", tf.__version__)\nprint(\"numpy:      \", np.__version__)\nprint(\"matplotlib: \", matplotlib.__version__)\nprint(\"sklearn:    \", sklearn.__version__)","96a2a96e":"def grayplt(img,title=''):\n    plt.axis('off')\n    if np.size(img.shape) == 3:\n        plt.imshow(img[:,:,0],cmap='gray',vmin=0,vmax=1)\n    else:\n        plt.imshow(img,cmap='gray',vmin=0,vmax=1)\n    plt.title(title, fontproperties=prop)\n    plt.show()\n\nprint(grayplt)","b715aee3":"                                          # Setting up the font manager, so that\n                                          # it can show japanese characters correctly\nfrom matplotlib import font_manager as fm\nfpath       = os.path.join(\"..\/input\/sivakmnist\/\", \"ipam.ttf\")\nprop        = fm.FontProperties(fname=fpath)\n\nplt.style.use('seaborn') \nplt.rcParams['ytick.right']     = True\nplt.rcParams['ytick.labelright']= True\nplt.rcParams['ytick.left']      = False\nplt.rcParams['ytick.labelleft'] = False\nplt.rcParams['figure.figsize']  = [7,7]   # Set the figure size to be 7 inch for (width,height)\n\nprint(\"Matplotlib setup completes.\")","2e9e8f2e":"                                                                                # Step 1\ntrDat = np.load('..\/input\/sivakmnist\/kmnist-train-imgs.npz')['arr_0']\ntrLbl = np.load('..\/input\/sivakmnist\/kmnist-train-labels.npz')['arr_0']\ntsDat = np.load('..\/input\/sivakmnist\/kmnist-test-imgs.npz')['arr_0']\ntsLbl  = np.load('..\/input\/sivakmnist\/kmnist-test-labels.npz')['arr_0']\n\n                                                                                # Step 2\nprint(\"The shape of trDat is\", trDat.shape, \"and the type of trDat is\", trDat.dtype)\nprint(\"The shape of tsDat is\", tsDat.shape, \"and the type of tsDat is\", tsDat.dtype)\nprint(\"\")\nprint(\"The shape of trLbl is\", trLbl.shape, \"and the type of trLbl is\", trLbl.dtype)\nprint(\"The shape of tsLbl is\", tsLbl.shape, \"and the type of tsLbl is\", tsLbl.dtype)\nprint(\"\")\ngrayplt(trDat[132])\n\n                                                                                # Step 3\ntrDat           = trDat.astype('float32')\/255\ntsDat           = tsDat.astype('float32')\/255\n\n                                                                                # Step 4\nimgrows         = trDat.shape[1]\nimgclms         = trDat.shape[2]\n\n                                                                                # Step 5\ntrDat       = trDat.reshape(trDat.shape[0],\n                            imgrows,\n                            imgclms,\n                            1)\ntsDat       = tsDat.reshape(tsDat.shape[0],\n                            imgrows,\n                            imgclms,\n                            1)\n\n                                                                                # Step 6\ntrLbl           = to_categorical(trLbl)\ntsLbl           = to_categorical(tsLbl)\n                               \nnum_classes     = tsLbl.shape[1]                                                # Step 7","1da9b118":"modelname   = 'wks5_5'                                                          # Step 1\n\n                                                                                # Step 2\ndef createModel():\n  model =Sequential()\n  model.add(Conv2D(20,(5,5), input_shape=(28,28,1), activation='relu'))\n  model.add(MaxPooling2D(pool_size=(2,2)))\n  model.add(Conv2D(40,(5,5), activation='relu'))\n  model.add(MaxPooling2D(pool_size=(2 , 2 )))\n  model.add(Dropout(0.2))\n  model.add(Flatten())\n  model.add(Dense(512 , activation='relu'))\n  model.add(Dense(128 , activation='relu'))\n  model.add(Dense(64 , activation='relu'))\n  model.add(Dense(num_classes,activation='softmax'))\n  model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n  return model\n\n                                                                                # Step 3\nmodel       = createModel() # This is meant for training\nmodelGo     = createModel() # This is used for final testing\n\nmodel.summary()                                                                 # Step 4","74906d80":"                                                                                # Step 1\nfolderpath      = '.\/'\nfilepath        = folderpath + modelname + \".hdf5\"\ncheckpoint      = ModelCheckpoint(filepath, \n                                  monitor='val_accuracy', \n                                  verbose=0, \n                                  save_best_only=True, \n                                  mode='max')\n\ncsv_logger      = CSVLogger(folderpath+modelname +'.csv')                       # Step 2\ncallbacks_list  = [checkpoint,csv_logger]                                       # Step 3\n\nprint(\"Callbacks created:\")\nprint(callbacks_list[0])\nprint(callbacks_list[1])\nprint('')\nprint(\"Path to model:\", filepath)\nprint(\"Path to log:  \", folderpath+modelname+'.csv')","6396bd7b":"model.fit(trDat,                            # Training data\n          trLbl,                            # Training label\n          validation_data=(tsDat, tsLbl),   # Validation data and label\n          epochs=2,                       # The amount of epochs to be trained\n          batch_size=128,                   \n          shuffle=True,                     # To shuffle the training data\n          callbacks=callbacks_list)         # Callbacks to execute the checkpoints","bb0714b8":"                                                                                # Step 1\nmodelGo.load_weights(filepath)\nmodelGo.compile(loss='categorical_crossentropy', \n                optimizer='adam', \n                metrics=['accuracy'])\n\npredicts    = modelGo.predict(tsDat)                                            # Step 2\nprint(\"Prediction completes.\")","201da3d3":"                                                                                # Step 1\nlabelname   = ['\u304a O','\u304d Ki','\u3059 Su','\u3064 Tsu','\u306a Na','\u306f Ha','\u307e Ma','\u3084 Ya','\u308c Re','\u3092 Wo']\n                                                                                # Step 2\npredout     = np.argmax(predicts,axis=1)\ntestout     = np.argmax(tsLbl,axis=1)\n\ntestScores  = metrics.accuracy_score(testout,predout)                           # Step 3\n\n                                                                                # Step 4\nprint(\"Best accuracy (on testing dataset): %.2f%%\" % (testScores*100))\nprint(metrics.classification_report(testout,\n                                    predout,\n                                    target_names=labelname,\n                                    digits=4))","b00991e6":"confusion   = metrics.confusion_matrix(testout,predout)\nprint(confusion)","09780beb":"records     = pd.read_csv(folderpath+modelname +'.csv')\nplt.figure()\nplt.subplot(211)\nplt.plot(records['val_loss'], label=\"validation\")\nplt.plot(records['loss'],label=\"training\")\nplt.yticks([0.10,0.30,0.50,0.70])\nplt.title('Loss value',fontsize=12)\n\nax          = plt.gca()\nax.set_xticklabels([])\n\nplt.subplot(212)\nplt.plot(records['val_accuracy'],label=\"validation\")\nplt.plot(records['accuracy'],label=\"training\")\nplt.yticks([0.7,0.8,0.9,1.0])\nplt.title('Accuracy',fontsize=12)\nax.legend()\nplt.show()","a5618b1f":"plotpath  = folderpath+modelname+'_plot.png'\nplot_model(model, \n           to_file=plotpath, \n           show_shapes=True, \n           show_layer_names=False,\n           rankdir='TB')\n\nprint(\"Path to plot:\", plotpath)","bb0a13f3":"## **10. Report classification metrics**\n---\n* Step 1: Setup the label\n* Step 2: Convert label from one-hot to integer\n* Step 3: Calculate the accuracy score\n* Step 4: Generate classification report","e39da157":"## **8. Train the deep learning model**\n___","f93d4025":"## **9. Validate the deep learning model**\n---\n* Step 1: Load the trained weights and compile the model\n* Step 2: Make prediction\n","bf06e7e2":"## **2. Import the necessary libraries**\n---","03f0bb5a":"## **1. Mount google drive**\n---","04b3aa10":"## **7. Create the callbacks to be applied during training**\n---\n* Step 1: Create a callback to save the model from an epoch when validation accuracy is the highest\n* Step 2: Create a callback to save the training loss, training accuracy, validation loss and validation accuracy of each epoch into a csv file\n* Step 3: Put the two callbacks objects into a list","eab066c1":"## **13. Save the model plot**\n---","5c094cc2":"## **12. Plot curves on validation loss and accuracy**\n---","3ba4964b":"## **6. Define deep learning model (to be completed)**\n___\n* Step 1: Set a name for the coming model (required for saving)\n* Step 2: Define the convolutional neural network model (to be completed)\n* Step 3: Create models for training and testing\n* Step 4: Display the summary of the model of interest \n\n**You may trial and error various structures (for input or number of channels or kernels sizes or pooling sizes) and see what happens to the performance**","0eb686da":"## **11. Print confusion matrix**\n---","2f0187eb":"## **5. Prepare data for training and testing**\n---\n* Step 1: Load the dataset \n* Step 2: Check the shape and type of the data, plot a sample for observation\n* Step 3: Convert the data into float32 and rescale the values from the range of 0\\~255 into 0\\~1\n* Step 4: Retrieve the row size and the column size of each image\n* Step 5: Reshape training and testing data to be in the form of `[samples,rows,columns,channel]`. This is required by Keras framework\n* Step 6: Perform one-hot enconding on the labels\n* Step 7: Retrieve the number of classes in this problem","5d81e659":"## **4. Setup matplotlib**\n---","cbefc175":"## **3.Create a function to plot the japanese character correctly**\n---"}}