{"cell_type":{"49f49f76":"code","c1bab05b":"code","22fdf319":"code","09ddd2ed":"code","ca1fa567":"code","438eb792":"code","5ec254aa":"code","e0818796":"code","e0a3c748":"code","a41fa74f":"code","ee0ea469":"code","93097299":"code","bacd8b57":"code","21d13779":"code","96c116c5":"code","995a5cb7":"code","ef44f064":"code","0e2c547f":"code","fd1bf580":"code","b20e6dbd":"code","1ddc93a8":"code","c9d31311":"code","6f9ab61f":"code","bde6bae6":"code","dc648c18":"code","a58aade0":"code","1358f9a7":"code","d77b2119":"code","ee7e27d4":"code","cccfe162":"code","bf138ca5":"code","1b5e49da":"code","324c0984":"code","dd186931":"code","cd42e6a3":"code","d4638eec":"code","1184d858":"code","f9387ffa":"code","2fb6b896":"code","78fb3c0e":"code","bb16d235":"code","adcedd54":"code","ecc27fc6":"code","ba39e066":"code","36e44a3f":"code","0a3efabc":"code","96b283ae":"code","7a962f77":"code","d8c60a20":"code","a5f9d794":"code","1afcd859":"code","8148e974":"code","2806ad5a":"code","607ce964":"code","7c8cb20b":"code","65de88e7":"code","4b0fdadc":"code","4ed83c37":"code","28f985a7":"code","83edb7af":"code","6849ac1c":"code","b3c32f2a":"code","c6843fc3":"code","adab1aaf":"code","321e7b39":"code","bcab866a":"code","5e5d92ad":"markdown","d776b8cf":"markdown","52fe4c23":"markdown","4c375152":"markdown","faab888a":"markdown","6e85a90c":"markdown","b8158c85":"markdown","7cf0416d":"markdown","f9a42d4e":"markdown","c7fe5241":"markdown","f9d08491":"markdown","d7e56a91":"markdown","0e1c394d":"markdown","792ce5bf":"markdown"},"source":{"49f49f76":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))","c1bab05b":"dataPath = '..\/input'","22fdf319":"setNumber = 1","09ddd2ed":"id_col = ['id']\ncycle_col = ['cycle']\nsetting_cols = ['setting1', 'setting2', 'setting3']\nsensor_cols = ['sensor' + str(i) for i in range(1, 22)]\nrul_col = ['RUL']\nall_cols = id_col + cycle_col + setting_cols + sensor_cols + rul_col","ca1fa567":"# This section is to load data\ndef loadData(fileName):\n    data = pd.read_csv(fileName, sep=\" \", header=None)\n    data.drop([26, 27], axis = 1, inplace=True)\n    data.columns = id_col + cycle_col + setting_cols +sensor_cols\n    return data","438eb792":"# load train RUL also returns the max cycle, and this max cycle is also the life cylce\ndef addTrainRul(data, decrease_threshold=None):\n    lifeCycles = {mcId: data[data['id']==mcId]['cycle'].max() for mcId in data['id'].unique()}\n    if decrease_threshold == None: decrease_threshold = 1\n    ruls = [lifeCycles[row[0]] - decrease_threshold if row[1] < decrease_threshold else lifeCycles[row[0]] - row[1] for row in data.values]\n    data['RUL'] = ruls\n    return lifeCycles\n    \n# use this last one only, return the data as well as the max life cycles\ndef loadTrainData(setNumber, decrease_threshold=None):\n    fileName = dataPath + '\/train_FD00' + str(setNumber) + '.txt'\n    data = loadData(fileName)\n    lifeCycles = addTrainRul(data, decrease_threshold)\n    return data, lifeCycles","5ec254aa":"decrease_threshold = None\ntrain, trainLifeCycles = loadTrainData(setNumber, decrease_threshold)","e0818796":"def loadTestRul(fileName):\n    data = pd.read_csv(fileName, sep = \" \", header=None)\n    data.drop([1], axis=1, inplace=True)\n    data.columns = ['RUL']\n    return data\ndef addTestRul(data, rulData, decrease_threshold=None):\n    testRuls = {i+1: rulData.iloc[i, 0] for i in range(len(rulData))}\n    lifeCycles = {mcId: data[data['id']==mcId]['cycle'].max() + testRuls[mcId] for mcId in data['id'].unique()}\n    if decrease_threshold == None: decrease_threshold = 1\n    ruls = [lifeCycles[row[0]] - decrease_threshold if row[1] < decrease_threshold else lifeCycles[row[0]] - row[1] for row in data.values]\n    data['RUL'] = ruls\n    return lifeCycles\n# Use this last one only => return data as well as the max life cycles for each machine\ndef loadTestData(setNumber, decrease_threshold=None):\n    data = loadData(dataPath + '\/test_FD00' +str(setNumber)+'.txt')\n    rulData = loadTestRul(dataPath + '\/RUL_FD00' + str(setNumber)+'.txt')\n    lifeCycles = addTestRul(data, rulData, decrease_threshold)\n    return data, lifeCycles","e0a3c748":"# Also make test RUL becomes piecewise\ntest, testLifeCycles = loadTestData(setNumber, decrease_threshold)","a41fa74f":"from matplotlib import pyplot as plt\nimport seaborn as sns","ee0ea469":"# As of feature selection they often select: 7, 8, 9, 12, 16, 17, 20  (manual selection based on sensor trends)\ndef plotSensorDataOfId(data, mcId):\n    plt.figure(figsize=(30, 20))\n    for i in range(21):\n        sensor = 'sensor'+str(i+1)\n        plt.subplot(10, 3, i+1).set_title(sensor)\n        ssdata = data[data['id']==mcId]\n        plt.plot(ssdata['cycle'], ssdata[sensor])\n    plt.tight_layout()","93097299":"plotSensorDataOfId(train, 1)","bacd8b57":"def plotDataDistribution(data):\n    sensors = []\n    plt.figure(figsize=(30, 10))\n    for i in range(21):\n        sensor = 'sensor'+str(i+1)\n        if(len(data[sensor].unique())>1):\n            sensors.append(sensor)\n            plt.subplot(3, 10, i+1)\n            sns.distplot(data[sensor])\n    plt.tight_layout()\n    return sensors","21d13779":"# As of feature selection they often select: 7, 8, 9, 12, 16, 17, 20 => Why 16?\ncols = plotDataDistribution(train)","96c116c5":"def plotCorrelation(data):\n    corr = data.corr()\n    # Generate a mask for the upper triangle\n    mask = np.zeros_like(corr, dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True\n    plt.figure(figsize=(10, 10))\n    sns.heatmap(data.corr(), square=True, mask=mask, cbar_kws={\"shrink\": 0.5})","995a5cb7":"plotCorrelation(train[cols])","ef44f064":"plt.scatter(train['sensor15'].values, train['sensor14'].values)","0e2c547f":"def plotCorrelationOfID(data, mcId):\n    data1 = data[data['id']==mcId]\n    data1 = data1.drop(['id'], axis = 1)\n    corr = data1.corr()\n    # Generate a mask for the upper triangle\n    mask = np.zeros_like(corr, dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True\n    plt.figure(figsize=(10, 10))\n    sns.heatmap(data1.corr(), square=True, mask=mask, cbar_kws={\"shrink\": 0.5})","fd1bf580":"plotCorrelationOfID(train[['id']+cols], 1)","b20e6dbd":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler","1ddc93a8":"# Scale the data and return the scaled data in form of a df and the scaler (will generate the scaler if doesn't pass it)\ndef scaleData(data, scaler=None):\n    scaled_fields = setting_cols+sensor_cols\n    if scaler == None:\n        scaler = StandardScaler().fit(data[scaled_fields].values)\n#         scaler = MinMaxScaler().fit(data[scaled_fields].values)\n    scaled_data = scaler.transform(data[scaled_fields].values)\n    scaled_df0 = pd.DataFrame(scaled_data)\n    scaled_df0.columns = scaled_fields\n    scaled_df1 = data.copy()\n    for i in range(len(scaled_fields)):\n        theField = scaled_fields[i]\n        scaled_df1[theField] = scaled_df0[theField]\n    return scaled_df1, scaler","c9d31311":"# Scaled train\nscaled_train, scaler = scaleData(train)\n# Scaled test\nscaled_test, scaler = scaleData(test, scaler)","6f9ab61f":"# plot to make sure that the scaled data still keep its shape.\ncols = plotDataDistribution(scaled_train)","bde6bae6":"#plot to see if the data keeps its distribution\ncols = plotDataDistribution(scaled_test)","dc648c18":"plotSensorDataOfId(scaled_train, 1)","a58aade0":"import random\ndef getPieceWiseData(data, augmentStartCycle=None, augmentEndCycle=None, movingAverage=None):\n    uniqueIds = data['id'].unique()\n    if movingAverage==None:\n        result = [data[data['id']==mId].values for mId in uniqueIds]\n    else:\n        result = [data[data['id']==mId].rolling(movingAverage).mean().dropna().values for mId in uniqueIds]\n    maxlen = np.max([len(x) for x in result])\n    #Augment the data now\n    if(augmentStartCycle!=None and augmentEndCycle!= None):\n        result1 = []\n        for mc in result:\n            maxCycle = len(mc)\n            for i in range(50):\n                idx = random.randint(max([maxCycle-145, 10]), max([maxCycle-10, 10]))\n                if(len(mc[:idx, :])>0):\n                    result1.append(mc[:idx, :])\n            #Also add the complete sequence.\n#             result1.append(mc)\n        result = result1\n    # calculate the ruls (-1) is the last column for RUL\n    ruls = [min(mc[:, -1]) for mc in result]\n    return result,ruls, maxlen\n# Use this last one only (prev one is a helper)\nfrom keras.preprocessing.sequence import pad_sequences\ndef getPaddedSequence(data, pad_type='pre', maxlen=None, augmentStartCycle=None, augmentEndCycle=None, movingAverage=None):\n    piece_wise, ruls, ml = getPieceWiseData(data, augmentStartCycle, augmentEndCycle, movingAverage)\n    if(maxlen==None): maxlen = ml\n    padded_sequence = pad_sequences(piece_wise, padding=pad_type, maxlen=maxlen, dtype='float32')\n    return padded_sequence, ruls, maxlen","1358f9a7":"augmentStartCycle = 130\naugmentEndCycle = 362\nmaxlen=200\nmovingAverage = None\npadded_train, train_ruls, maxlen = getPaddedSequence(scaled_train, maxlen=maxlen, augmentStartCycle=augmentStartCycle, augmentEndCycle=augmentEndCycle, movingAverage=movingAverage)\npadded_test, test_ruls, maxlen = getPaddedSequence(scaled_test, maxlen=maxlen, movingAverage=movingAverage)","d77b2119":"sns.distplot(train_ruls)","ee7e27d4":"def plotDataForIndex(data, theIndex):\n    plt.figure(figsize=(30, 30))\n    for i in range(5, 26):\n        plt.subplot(7, 3, i-4)\n        values = data[theIndex][:, i]\n        plt.plot(range(len(values)) ,values)\n        plt.title('sensor'+str(i-4))\n        plt.tight_layout()","cccfe162":"plotDataForIndex(padded_train, 450)","bf138ca5":"# selected_sensors = [7, 8, 9, 12, 16, 17, 20]\nselected_sensors = [2, 3, 4, 6, 7, 8, 9, 11, 12, 13, 14, 15, 17, 20, 21]\n# selected_sensors = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\nselected_sensors_indices = [x-1 for x in selected_sensors] # -1 because the index starts from 1","1b5e49da":"# X_train = padded_train[:, :, 5:26]\nX_train = padded_train[:, :, 5:26][:, :, selected_sensors_indices]","324c0984":"# X_test = padded_test[:, :, 5:26]\nX_test = padded_test[:, :, 5:26][:, :, selected_sensors_indices]","dd186931":"y_train = np.array(train_ruls).reshape(-1,1)\ny_test = np.array(test_ruls).reshape(-1,1)","cd42e6a3":"numOfSensors = len(X_train[0][0])","d4638eec":"import codecs, json\ndef exportNPArrayToJSON(a, fileName):\n    b = a.tolist() # nested lists with same data, indices\n    json.dump(b, codecs.open(fileName, 'w', encoding='utf-8')) ### this saves the array in .json format","1184d858":"test_FD = 'test_FD00' + str(setNumber) + \".json\"\ntrain_FD = 'train_FD00' + str(setNumber) + \".json\"\ntest_RUL_FD = 'test_RUL_FD00' + str(setNumber) + \".json\"\ntrain_RUL_FD = 'train_RUL_FD00' + str(setNumber) + \".json\"\n# exportNPArrayToJSON(X_train, train_FD)\nexportNPArrayToJSON(X_test, test_FD)\n# exportNPArrayToJSON(y_train, train_RUL_FD)\nexportNPArrayToJSON(y_test, test_RUL_FD)","f9387ffa":"from IPython.display import FileLink","2fb6b896":"FileLink(test_FD)","78fb3c0e":"FileLink(train_FD)","bb16d235":"FileLink(test_RUL_FD)","adcedd54":"FileLink(train_RUL_FD)","ecc27fc6":"from keras import regularizers\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import Activation\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten\n\n# from keras import backend as K\n# K.set_session(K.tf.Session(config=K.tf.ConfigProto(intra_op_parallelism_threads=36, inter_op_parallelism_threads=36)))\n\n\ndef createModel(l1Nodes, l2Nodes, d1Nodes, d2Nodes, inputShape):\n    # input layer\n    lstm1 = LSTM(l1Nodes, input_shape=inputShape, return_sequences=True, kernel_regularizer=regularizers.l2(0.1))\n    do1 = Dropout(0.2)\n    \n    lstm2 = LSTM(l2Nodes, return_sequences=True, kernel_regularizer=regularizers.l2(0.1))\n    do2 = Dropout(0.2)\n    \n    flatten = Flatten()\n    \n    dense1 = Dense(d1Nodes, activation='relu', kernel_regularizer=regularizers.l2(0.1))\n    do3 = Dropout(0.2)\n    \n    dense2 = Dense(d2Nodes, activation='relu', kernel_regularizer=regularizers.l2(0.1))\n    do4 = Dropout(0.2)\n    \n    # output layer\n    outL = Dense(1, activation='relu', kernel_regularizer=regularizers.l2(0.1))\n    # combine the layers\n#     layers = [lstm1, do1, lstm2, do2, dense1, do3, dense2, do4, outL]\n    layers = [lstm1, lstm2, do2, flatten,  dense1, dense2, outL]\n    # create the model\n    model = Sequential(layers)\n    model.compile(optimizer='adam', loss='mse')\n    return model","ba39e066":"model = createModel(64, 64, 64, 8, (maxlen, numOfSensors))","36e44a3f":"model.summary()","0a3efabc":"from keras.models import Sequential\nfrom keras.layers import Convolution1D\nfrom keras.layers import MaxPooling1D\nfrom keras.layers import Flatten\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import Activation\nfrom keras.layers import Dropout","96b283ae":"def createCNNLSTMModel(inputShape):\n    cv1 = Convolution1D(input_shape=inputShape, filters=18, kernel_size=2, strides=1, padding='same', activation='relu', name='cv1')\n    mp1 = MaxPooling1D(pool_size=2, strides=2, padding='same', name = 'mp1')\n    \n    cv2 = Convolution1D(filters=36, kernel_size=2, strides=1, padding='same', activation='relu', name='cv2')\n    mp2 = MaxPooling1D(pool_size=2, strides=2, padding='same', name= 'mp2')\n    \n    cv3 = Convolution1D(filters=72, kernel_size=2, strides=1, padding='same', activation='relu', name='cv3')\n    mp3 = MaxPooling1D(pool_size=2, strides=2, padding='same', name= 'mp3')\n    \n    d4 = Dense(inputShape[0]*inputShape[1], activation='relu')\n    do4 = Dropout(0.2)\n    \n    lstm5 = LSTM(inputShape[1]*3, return_sequences=True)\n    do5 = Dropout(0.2)\n    \n    lstm6 = LSTM(inputShape[1]*3)\n    do6 = Dropout(0.2)\n    \n    d7 = Dense(50, activation='relu')\n    do7 = Dropout(0.2)\n    \n    dout = Dense(1)\n    \n    model = Sequential([cv1, mp1, cv2, mp2, cv3, mp3, d4, do4, lstm5, do5, lstm6, do6, d7, do7, dout])\n    model.compile(optimizer='rmsprop', loss='mse')\n    return model","7a962f77":"# model = createCNNLSTMModel((maxlen, numOfSensors))","d8c60a20":"# model.summary()","a5f9d794":"# from keras.models import Sequential\n# from keras.layers import Convolution1D\n# from keras.layers import MaxPooling1D\n# from keras.layers import AveragePooling1D\n# from keras.layers import Flatten\n# from keras.layers import LSTM\n# from keras.layers import Dense\n# from keras.layers import Activation\n# from keras.layers import Dropout","1afcd859":"# def createAVGLSTMModel(inputShape):\n#     a1 = AveragePooling1D(pool_size=5,stride=1, padding='same', input_shape=inputShape)  \n# #     d4 = Dense(inputShape[0]*inputShape[1], activation='relu')\n# #     do4 = Dropout(0.5)\n    \n#     lstm5 = LSTM(64, return_sequences=True)\n#     do5 = Dropout(0.5)\n    \n#     lstm6 = LSTM(64)\n#     do6 = Dropout(0.5)\n    \n#     d7 = Dense(8, activation='relu')\n#     do7 = Dropout(0.5)\n    \n#     d8 = Dense(8, activation='relu')\n#     do8 = Dropout(0.5)\n\n    \n#     dout = Dense(1)\n    \n#     model = Sequential([a1, lstm5, do5, lstm6, do6, d7, do7, d8, do8, dout])\n#     model.compile(optimizer='adam', loss='mse')\n#     return model","8148e974":"# model = createAVGLSTMModel((maxlen, numOfSensors))\n# model.summary()","2806ad5a":"from keras.callbacks import ModelCheckpoint\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import load_model\n# ten fold\nfrom sklearn.model_selection import StratifiedKFold\nkfold = StratifiedKFold(n_splits=3, shuffle=True)\nfrom keras.models import load_model\nmsescores = []\ncounter= 0\nfor trainIdx, testIdx in kfold.split(X_train, y_train):\n    counter = counter + 1\n    # create callbacks\n    model_path = 'best_model_set'+str(setNumber)+'fold'+str(counter)+'.h5'\n    mc = ModelCheckpoint(model_path, monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n    es = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=1)\n    # create model\n    # model = createModel(64, 64, 8, 8, (maxlen, numOfSensors))\n    model = createCNNLSTMModel((maxlen, numOfSensors))\n    model.fit(X_train[trainIdx], y_train[trainIdx], validation_data=(X_train[testIdx], y_train[testIdx]), batch_size=32, epochs=4, callbacks=[mc, es])\n    # Done load the best model of this fold\n    saved_model = load_model(model_path)\n    msescores.append({'path': model_path, 'mse': saved_model.evaluate(X_train[testIdx], y_train[testIdx])})","607ce964":"msescores","7c8cb20b":"\nfor md in msescores:\n    saved_model = load_model(md['path'])\n    print(saved_model.evaluate(X_test, y_test))","65de88e7":"predicted = saved_model.predict(X_test)","4b0fdadc":"plt.figure(figsize=(50, 10))\nplt.plot(range(len(predicted)), predicted, '-x', label='predicted')\nplt.plot(range(len(y_test)), y_test, '-o', label='actual')\nplt.legend()","4ed83c37":"from keras.models import Model\ndef getVizModel(model):\n    output_layers = [l.output for l in model.layers]\n    viz_model = Model(saved_model.input, output_layers)\n    return viz_model","28f985a7":"viz_model = getVizModel(saved_model)","83edb7af":"layer_outputs = viz_model.predict(X_train)","6849ac1c":"layer_outputs[0].shape","b3c32f2a":"import math\ndef plotLayerData(layer_data, mcIndex):\n    mcData = layer_data[mcIndex]\n    plt.figure(figsize=(30, 30))\n    nCols = 2\n    nRows = math.ceil(len(mcData[0])\/nCols)\n    for i in range(len(mcData[0])):\n        plt.subplot(nRows, nCols, i+1)\n        plt.plot(range(len(mcData[:, i])), mcData[:, i])\n        plt.tight_layout()","c6843fc3":"plotLayerData(layer_outputs[1], 100)","adab1aaf":"import seaborn as sns\ndef plotLayerHeatmap(layer_data, mcIndex):\n    mcData = layer_data[mcIndex]\n    plt.figure(figsize=(30, 10))\n    sns.heatmap(mcData.transpose())","321e7b39":"plotSensorDataOfId(train, 100)","bcab866a":"plotLayerHeatmap(layer_outputs[1], 100)","5e5d92ad":"# LSTM Model","d776b8cf":"# Average model","52fe4c23":"# Plot data to see if it is smoothened.","4c375152":"# Exporting and downloading data","faab888a":"# Train test data","6e85a90c":"# Load train\/test data with RUL","b8158c85":"# Select sensors","7cf0416d":"# Train\/Fit model","f9a42d4e":"# Plot to check the distribution are still the same after scaling","c7fe5241":"# CNN + LSTM","f9d08491":"# Visualize intermediate outputs","d7e56a91":"# Scale","0e1c394d":"# Plot data to see its distribution","792ce5bf":"# Piece-wise data (all columns in order) with augmentation and padded sequence"}}