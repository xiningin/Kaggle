{"cell_type":{"3e86a21b":"code","b8a081c6":"code","465ed87f":"code","8ece25bb":"code","5984d467":"code","9358f51f":"code","fbd71c25":"code","81bff418":"code","946f0040":"code","dee4e951":"code","7bb6f27a":"code","6d2422a2":"code","fb0c44c7":"code","b1b816b4":"code","991e8cf0":"code","a0ad449b":"code","b54487b5":"code","6c66ae66":"code","b6ea00e4":"code","832ebb5d":"code","f28ef5b2":"code","203849af":"code","808c7534":"code","86470434":"code","6f164380":"code","a626b6ab":"code","a41f43c1":"code","4fda6a95":"code","87456862":"code","07d58bb0":"code","1b02493a":"markdown","b6b841e8":"markdown","51622bda":"markdown","533b2582":"markdown","ddc81c47":"markdown","708fff19":"markdown","abf4fba2":"markdown","15bc0ebf":"markdown","924fc577":"markdown","5991505c":"markdown","cc24f825":"markdown","98030dd9":"markdown","86942b3a":"markdown","a3bf2ecf":"markdown","959e0e71":"markdown","3133f4d8":"markdown","422b11d0":"markdown","8bd5f0e3":"markdown"},"source":{"3e86a21b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b8a081c6":"df1= pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf1.head()","465ed87f":"df2 = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ndf2.head()","8ece25bb":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom xgboost import XGBClassifier\nimport statsmodels.api as sms","5984d467":"df1.describe().transpose()","9358f51f":"sns.heatmap(df1.notnull(),cbar=False,yticklabels=False)","fbd71c25":"Train_Age_avail=df1['Age'].count()\/len(df1['Age'])\nTrain_Cabin_avail=df1['Cabin'].count()\/len(df1['Cabin'])\nprint('Percentage of available values of Age and Cabin in Train ', Train_Age_avail, Train_Cabin_avail)\nTest_Age_avail=df2['Age'].count()\/len(df2['Age'])\nTest_Cabin_avail=df2['Cabin'].count()\/len(df2['Cabin'])\nprint('Percentage of available values of Age and Cabin in Train ', Test_Age_avail, Test_Cabin_avail)","81bff418":"sns.heatmap(df1.corr(), annot=True)","946f0040":"df1['Age'].hist(bins=10)","dee4e951":"col_list=list(df1.columns)\ncol_list.remove('Survived')\ncol_list.remove('PassengerId')\ncol_list.remove('Name')\ncol_list.remove('Cabin')\nfor i in col_list:\n    plt.figure()\n    sns.countplot(x=df1[i], hue=df1['Survived'])","7bb6f27a":"#Fill missing values in Age with median of the group\ndf1['Age'].fillna(df1['Age'].median(),inplace=True)\ndf2['Age'].fillna(df2['Age'].median(),inplace=True)","6d2422a2":"#Box the ages in groups\ncat_age=pd.cut(df1['Age'],bins=[0,20,30,40,60,80], labels=[1,2,3,4,5])\ndf1.insert(6,'AgeBox',cat_age)\ncat_age=pd.cut(df2['Age'],bins=[0,20,30,40,60,80], labels=[1,2,3,4,5])\ndf2.insert(6,'AgeBox',cat_age)","fb0c44c7":"df1.groupby(['Pclass','Sex','Survived'])['Name'].count()","b1b816b4":"df1['Dependents'] = df1['SibSp']+df1['Parch']\ndf2['Dependents'] = df2['SibSp']+df2['Parch']\ndf1.drop(['SibSp','Parch'], axis=1,inplace=True)","991e8cf0":"sns.countplot(df1['Dependents'],hue=df1['Survived'])","a0ad449b":"#Convert Embarked by Label encoding\nfrom sklearn.preprocessing import LabelEncoder\nlbe = LabelEncoder()\ndf1['Embarked_Code']=lbe.fit_transform(df1['Embarked'].astype(str))\ndf2['Embarked_Code']=lbe.fit_transform(df2['Embarked'].astype(str))\n#Convert Sex into categories\ngender={\"male\":1,\"female\":2}\ndf1['Sex']=[gender[x] for x in df1['Sex']]\ndf2['Sex']=[gender[x] for x in df2['Sex']]","b54487b5":"sns.boxplot(df1['Fare'])","6c66ae66":"print(df1[df1['Fare']>100]['Pclass'].value_counts())\nprint(df2[df2['Fare']>100]['Pclass'].value_counts())","b6ea00e4":"df1.loc[df1['Fare']>100,'Fare']=df1[df1['Pclass']==1]['Fare'].median()\ndf2.loc[df2['Fare']>100,'Fare']=df2[df2['Pclass']==1]['Fare'].median()","832ebb5d":"sns.heatmap(df1.corr(),annot=True)","f28ef5b2":"df1.drop(['Name','Age','Ticket','Cabin','Embarked','Fare'], axis=1,inplace=True)\ndf2.drop(['Name','Age','Ticket','Cabin','Embarked','Fare'], axis=1,inplace=True)","203849af":"#Re-arranging the columns\ndf1=df1[['PassengerId','Pclass','Sex','AgeBox','Dependents','Embarked_Code','Survived']]\ndf2=df2[['PassengerId','Pclass','Sex','AgeBox','Dependents','Embarked_Code']]\ny=df1.iloc[:,-1]\nX=df1.iloc[:,:-1]","808c7534":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)","86470434":"#Logical Regression and check P values\nlogReg=LogisticRegression()\nlogReg.fit(X_train,y_train)\ny1_pred=logReg.predict(X_test)\nprint(\"Accuracy Score for Logical Regression is \",accuracy_score(y1_pred,y_test))\nprint(\"Confusion Matrix for Logical Regression is \",confusion_matrix(y1_pred,y_test))","6f164380":"logres=sms.Logit(y_train,X_train.astype(float))\nres=logres.fit()\nprint(res.summary())","a626b6ab":"rfcls= RandomForestClassifier(n_estimators=50,criterion='gini',min_samples_leaf=3,max_depth=20,random_state=42)\nrfcls.fit(X_train, y_train)\ny2_pred=rfcls.predict(X_test)\nprint(\"Accuracy Score for Random Forest Classifier is \",accuracy_score(y2_pred,y_test))\nprint(\"Confusion Matrix for Random Forest Classifier is \",confusion_matrix(y2_pred,y_test))","a41f43c1":"dtree= DecisionTreeClassifier(random_state=42)\ndtree.fit(X_train, y_train)\ny3_pred=rfcls.predict(X_test)\nprint(\"Accuracy Score for Decision Tree Classifier is \",accuracy_score(y3_pred,y_test))\nprint(\"Confusion Matrix for Decision Tree Classifier is \",confusion_matrix(y3_pred,y_test))","4fda6a95":"grdbst=GradientBoostingClassifier(n_estimators=50,min_samples_leaf=3,max_depth=20,random_state=42)\ngrb_mod=grdbst.fit(X_train,y_train)\ny6_pred=grb_mod.predict(X_test)\nprint(\"Accuracy Score for Gradient Boost Classifier is \",accuracy_score(y6_pred,y_test))\nprint(\"Confusion Matrix for Gradient Boost Classifier is \",confusion_matrix(y6_pred,y_test))","87456862":"y_test_pred=rfcls.predict(df2)\nprint(y_test_pred)","07d58bb0":"output=pd.DataFrame({'PassengerId':df2['PassengerId'],'Survived':y_test_pred})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","1b02493a":"# Decision Tree Classifier","b6b841e8":"**Final Observation on Model - Now that Random Forest & Decsision Tree Classifiers  has provided a good output.\nZero-in Random Forest to apply on test data**","51622bda":"# GradientBoost Classifier","533b2582":"# Logistic Regression","ddc81c47":"# Feature Engineering","708fff19":"# Data Analysis","abf4fba2":"\nObservation 5:\n\n* Add the SibSp and Parch into a single field, considering its correlation \n* Drop the 2 separate fields","15bc0ebf":"Observation 8:\n* There is no need to split the names and group the Lastname and compare, as we have already created Dependents feature. So can drop Name\n* Age could also be dropped as we have AgeBox created, and so is Embarked as Embarked_Code is created\n* Cabin with only 20% of data, could be dropped\n* Ticket could also be dropped, as not significant with Alphanumeric or just Alphabets\n* PassengerId - retaining for now, as not sure on the significance in enuermating thr results. But for model building its of no significance \n* Fare and Pclass are also negatively correlated. So we can drop Fare too [Effort spent on treaing Fare is wasted]","924fc577":"# Random Forest Classifier","5991505c":"\nObservation 2:\n1. No major correlation between most of the features except few\n2. As expected Pclass and Fare are inversely proportional (higher the fare, higher the class (denoted as less numeric value)\n3. Parch and Sibsp has some correlation, but not the point high significance to remove any feature","cc24f825":"Observation 6:\n\n* People who had dependents had high Survival rate, compared to people without dependents\n* Of that people who had less dependents <3, had better survival rate","98030dd9":"Observation 7:\n* There are quite outliers in Fare and as expected the higher outlier is in PClass1\n* Let us replace the outliers with median of Pclass1 to avoid any impurity of data","86942b3a":"# Logit","a3bf2ecf":"Observation 4:\n* Pclass 1 - Survival rate-62.96%, Female(94) Survival rate-96.80%, Male(122) Survival rate-36.88%\n* Pclass 2 - Survival rate-47.28%, Female(76) Survival rate-92.10%, Male(108) Survival rate-15.74%\n* Pclass 3 - Survival rate-24.23%, Female(144) Survival rate-50.0%, Male(347) Survival rate-13.54%","959e0e71":"Based on Observation3, we are performing multivariate analysis on the data with reference to multiple features","3133f4d8":"Observation 9: \n* As expected PassengerId has high P value. We can remove them in later instance(if required)\n* Remaining features are valid","422b11d0":"Observation 1:\n1. Age has ~80% values and cabin ~21-22% values in both Train and test\n2. Checking the Age distribution, we can fill the missing values through central tendency approach\n3. Filling ~80% of values for missing Cabin would not be valid. So we can think of dropping the column cabin if that is not significant","8bd5f0e3":"Observation 3: \nOn performimg Bivariate analysis against our target (Survived feature) against other features (ignore the obvious - PassengerId, Name, Cabin, Ticket, Fare etc)\n* More Females survived than Males\n* Higher the class levels (1 and 2) more Survived\n* People with 1,2,3 SibSp or Parch had more Survived, compared to People with 0 or greater than 3 Sibsp\/Pacrh\n* People with high Fare had Survived - obvious it should have been from High Pclass\n* Visual check shows people in the median age group (20-30's) had less survived compared to other Age groups - we can box the Ages"}}