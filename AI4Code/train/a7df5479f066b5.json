{"cell_type":{"bfe0e11b":"code","f79b7417":"code","b114f858":"code","b0f6490f":"code","77f7c2aa":"code","b7eda5d5":"code","b9ee4339":"code","e465691b":"code","1022e779":"code","c6cc0285":"code","808b0975":"code","f6f8df6a":"code","9f895931":"code","9118462f":"code","fc29eba4":"code","f08ccd29":"code","586e5431":"code","e6c62ff7":"code","dc8aa501":"code","981968e9":"code","6854a282":"code","7debccaa":"code","f8c7f6c7":"code","44d7d878":"code","14cc2c48":"code","9c8a8ac7":"code","fb2cf2d2":"code","a7621452":"code","1756405b":"code","a3915934":"code","de0a71e3":"code","65c25eac":"code","64ed219e":"code","aad5f411":"code","0e9a27a7":"code","1144e442":"code","15a67ac6":"code","9257b23f":"code","a7298fe7":"code","5dfa7c1f":"code","cf25d02b":"code","d8baf736":"code","e386ea38":"code","7535f0ef":"code","e1789b97":"code","829f42d6":"code","15164165":"code","f660b7d0":"code","4e2c3878":"code","4766ad2a":"code","d731fc8e":"code","4ef06a7e":"code","e5034f02":"code","ca30b5e5":"code","428847d7":"code","4d0ab1db":"code","c56eeba2":"code","edc82ccb":"code","54d76726":"code","35ea9b57":"code","315ab93f":"code","649ee8ad":"code","f56368e9":"code","6bfb3178":"code","56e843cb":"code","b624d454":"code","4f0ca554":"code","22a7e6cf":"code","415f8ec9":"code","ac88371f":"code","c4ff6146":"code","ad818d07":"code","52744c08":"markdown","044331c5":"markdown","7bb68fb4":"markdown","211f3cf5":"markdown","b399b8cd":"markdown","4a6d4c1d":"markdown","7ad3ac11":"markdown","dc16af12":"markdown","b80dda22":"markdown","eddbf759":"markdown","4ad46f88":"markdown","076d5250":"markdown","b7ba9be7":"markdown","e33b95cc":"markdown","05f1f8d5":"markdown","e4861f10":"markdown","804076ef":"markdown"},"source":{"bfe0e11b":"import numpy as np  \nimport pandas as pd  \nimport os\nfrom typing import Tuple\nimport pickle\nimport seaborn as sns\nimport time\nfrom collections import Counter, OrderedDict\n\nimport gc\ngc.enable()\n\nimport matplotlib.pyplot as plt\nimport plotly.io as plt_io\nimport plotly.graph_objects as go\nfrom matplotlib.pyplot import figure\n%matplotlib inline\nfrom mpl_toolkits.basemap import Basemap\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.metrics import precision_recall_curve, average_precision_score, auc\n\nfrom sklearn import model_selection, preprocessing\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV,KFold, cross_val_predict, StratifiedKFold, train_test_split, learning_curve, ShuffleSplit\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC \nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, ShuffleSplit\nfrom sklearn.model_selection import cross_val_predict\n\nimport tensorflow as tf\nfrom keras import models, regularizers, layers, optimizers, losses, metrics\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding\nfrom keras.utils import np_utils\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\npd.options.display.max_columns =999\npd.options.display.max_rows = 999\n\nfrom pdpbox import pdp, get_dataset, info_plots\nimport shap \nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', None)\nlistFiles = os.listdir('\/kaggle\/input\/syntheacovid100k\/100k_synthea_covid19_csv')\nlistFiles","f79b7417":"# Function for a nice CM\n\ndef plot_confusion_matrix(cm,target_names,title='Confusion matrix',cmap=None,\n                          normalize=False):\n    import itertools\n    accuracy = np.trace(cm) \/ float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    \n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        \n    thresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()","b114f858":"patients = pd.read_csv('\/kaggle\/input\/syntheacovid100k\/100k_synthea_covid19_csv\/patients.csv')\nprint(patients.shape)\npatients.sample()","b0f6490f":"len(patients.Id.unique())","77f7c2aa":"# Mortality \n\npatients.DEATHDATE.value_counts().sum() \/ len(patients.Id.unique())","b7eda5d5":"RawData4ML = patients.copy()\nRawData4ML = RawData4ML[['Id', 'BIRTHDATE', 'DEATHDATE','MARITAL', 'RACE', 'ETHNICITY',\n       'GENDER','LAT', 'LON', 'HEALTHCARE_EXPENSES', 'HEALTHCARE_COVERAGE']]\nRawData4ML.sample()","b9ee4339":"%%time\n\n# assign dataset names\nlist_of_names = ['observations.csv',\n 'careplans.csv',\n 'conditions.csv',\n 'encounters.csv',\n 'procedures.csv',\n 'allergies.csv',\n 'medications.csv',\n 'payers.csv',\n 'organizations.csv',\n 'imaging_studies.csv',\n 'supplies.csv',\n 'patients.csv',\n 'devices.csv',\n 'payer_transitions.csv',\n 'providers.csv',\n 'immunizations.csv']\n\n# create empty list\ndataframes_list = []\n  \nfor name in list_of_names:\n    name = name[:-4]\n    print(name, end=' ')\n    name = pd.read_csv(\"\/kaggle\/input\/syntheacovid100k\/100k_synthea_covid19_csv\/\"+str(name)+\".csv\")\n    dataframes_list.append(name)\n    print(name.shape)","e465691b":"observations = dataframes_list[0]\ncareplans = dataframes_list[1]\nconditions = dataframes_list[2]\nencounters = dataframes_list[3]\nprocedures = dataframes_list[4]\nallergies = dataframes_list[5]\nmedications = dataframes_list[6]\npayers = dataframes_list[7]\norganizations = dataframes_list[8]\nimaging_studies = dataframes_list[9]\nsupplies = dataframes_list[10]\npatients = dataframes_list[11]\ndevices = dataframes_list[12]\npayer_transitions = dataframes_list[13]\nproviders = dataframes_list[14]\nimmunizations = dataframes_list[15]\n\nprint(observations.shape)\nprint(careplans.shape)\nprint(conditions.shape)\nprint(encounters.shape)\nprint(procedures.shape)\nprint(allergies.shape)\nprint(medications.shape)\nprint(payers.shape)\nprint(organizations.shape)\nprint(imaging_studies.shape)\nprint(supplies.shape)\nprint(patients.shape)\nprint(devices.shape)\nprint(payer_transitions.shape)\nprint(providers.shape)\nprint(immunizations.shape)\n","1022e779":"del dataframes_list\ngc.collect()","c6cc0285":"# observations has 18,134 Cause of Death\nprint(observations.shape)\nobservations = observations.loc[~observations['DESCRIPTION'].str.contains(\"death\", case=False)]\nprint(observations.shape)\n\n# encounters 18,134 Death certificate\nprint(encounters.shape)\nencounters = encounters.loc[~encounters['DESCRIPTION'].str.contains(\"death\", case=False)]\nprint(encounters.shape)\n\n# conditions , procedures ...No label leakage\nprint(conditions.loc[conditions['DESCRIPTION'].str.contains(\"death\", case=False)].shape)\nprocedures.loc[procedures['DESCRIPTION'].str.contains(\"death\", case=False)].shape","808b0975":"%%time\n\nconditionsCount = conditions.groupby('PATIENT').size().reset_index(name='NumConditions')\nconditionsCount.columns = ['Id','NumConditions']\n\nproceduresCount = procedures.groupby('PATIENT').size().reset_index(name='NumProcedures')\nproceduresCount.columns = ['Id','NumProcedures']\n\nmedicationsCount = medications.groupby('PATIENT').size().reset_index(name='NumMedications')\nmedicationsCount.columns = ['Id','NumMedications']\n\nobservationsCount = observations.groupby('PATIENT').size().reset_index(name='NumObservations')\nobservationsCount.columns = ['Id','NumObservations']\n\nencountersCount = encounters.groupby('PATIENT').size().reset_index(name='NumEncounters')\nencountersCount.columns = ['Id','NumEncounters']\n\nimaging_studiesCount = imaging_studies.groupby('PATIENT').size().reset_index(name='NumImages')\nimaging_studiesCount.columns = ['Id','NumImages']\n\ncareplansCount = careplans.groupby('PATIENT').size().reset_index(name='NumCareplans')\ncareplansCount.columns = ['Id','NumCareplans']\n\nallergiesCount = allergies.groupby('PATIENT').size().reset_index(name='NumAllergies')\nallergiesCount.columns = ['Id','NumAllergies']\n\npayersCount = payers.groupby('Id').size().reset_index(name='NumPayers')\npayersCount.columns = ['Id','NumPayers']\n\norganizationsCount = organizations.groupby('Id').size().reset_index(name='NumOrganizations')\norganizationsCount.columns = ['Id','NumOrganizations']\n\nsuppliesCount = supplies.groupby('PATIENT').size().reset_index(name='NumSupplies')\nsuppliesCount.columns = ['Id','NumSupplies']\n\ndevicesCount = devices.groupby('PATIENT').size().reset_index(name='NumDevices')\ndevicesCount.columns = ['Id','NumDevices']\n\npayer_transitionsCount = payer_transitions.groupby('PATIENT').size().reset_index(name='NumPayer_transitions')\npayer_transitionsCount.columns = ['Id','NumPayer_transitions']\n\nprovidersCount = providers.groupby('Id').size().reset_index(name='NumProviders')\nprovidersCount.columns = ['Id','NumProviders']\n\nimmunizationsCount = immunizations.groupby('PATIENT').size().reset_index(name='NumImmunizations')\nimmunizationsCount.columns = ['Id','NumImmunizations']\n\nRawData4ML = RawData4ML.merge(immunizationsCount, how='left', on='Id')\nRawData4ML = RawData4ML.merge(providersCount, how='left', on='Id')\nRawData4ML = RawData4ML.merge(payer_transitionsCount, how='left', on='Id')\nRawData4ML = RawData4ML.merge(devicesCount, how='left', on='Id')\nRawData4ML = RawData4ML.merge(suppliesCount, how='left', on='Id')\nRawData4ML = RawData4ML.merge(organizationsCount, how='left', on='Id')\nRawData4ML = RawData4ML.merge(payersCount, how='left', on='Id')\nRawData4ML = RawData4ML.merge(allergiesCount, how='left', on='Id')\nRawData4ML = RawData4ML.merge(careplansCount, how='left', on='Id')\nRawData4ML = RawData4ML.merge(imaging_studiesCount, how='left', on='Id')\nRawData4ML = RawData4ML.merge(encountersCount, how='left', on='Id')\nRawData4ML = RawData4ML.merge(observationsCount, how='left', on='Id')\nRawData4ML = RawData4ML.merge(medicationsCount, how='left', on='Id')\nRawData4ML = RawData4ML.merge(proceduresCount, how='left', on='Id')\nRawData4ML = RawData4ML.merge(conditionsCount, how='left', on='Id')","f6f8df6a":"# Mortality\n\nRawData4ML['Dead'] = 0\nRawData4ML['Dead'] = ~RawData4ML.DEATHDATE.isnull()\nRawData4ML.drop('DEATHDATE', axis=1, inplace=True)\nRawData4ML.Dead.value_counts(normalize=True)","9f895931":"# Categorical data\n\nprint(RawData4ML.shape)\ncols4OHE = ['MARITAL', 'RACE', 'ETHNICITY']\n\nRawData4ML = pd.get_dummies(RawData4ML, columns = cols4OHE)\nprint(RawData4ML.shape)\nRawData4ML.isnull().sum()","9118462f":"# Gender and fillna\n\nRawData4ML['GENDER']=RawData4ML['GENDER'].apply(lambda x: 1 if x =='M'  else 0)\nRawData4ML.fillna(value=0, inplace=True)\nRawData4ML.isnull().sum()","fc29eba4":"# Age\n\nRawData4ML['BIRTHDATE'] = pd.to_datetime(RawData4ML['BIRTHDATE'])\nRawData4ML['BIRTHDATE']\n\nnow = pd.to_datetime('now')\n\nRawData4ML['Age'] = (now - RawData4ML['BIRTHDATE']).astype('<m8[Y]') \nRawData4ML.drop('BIRTHDATE', axis=1, inplace=True)\nRawData4ML.head(1)","f08ccd29":"# Separate X,y for normalizing only X\n\n# RawData4ML = RawData4ML.sample(frac=1, random_state=42) # shuffles df\n\ny = RawData4ML.Dead.values\nprint(y.shape)\n#X = RawData4ML.drop(['Dead','Id', 'Conditions'], axis=1)\nX = RawData4ML.drop(['Dead','Id'], axis=1)\nprint(X.shape)\n\n# Normalize X\n\nx = X.values #returns a numpy array\nscaler = preprocessing.StandardScaler()\nx_scaled = scaler.fit_transform(x)\nX = pd.DataFrame(x_scaled, columns=X.columns)\nprint(X.shape)\nX.tail(1)","586e5431":"numFeat = 20","e6c62ff7":"# Concepts are separated by comma ,\n\nconditions.sort_values(by =['PATIENT', 'START'],ascending=False, inplace=True)\nconditions.DESCRIPTION = conditions.DESCRIPTION.str.lower()\nconditions.DESCRIPTION = conditions.DESCRIPTION.str.replace('(', '')\nconditions.DESCRIPTION = conditions.DESCRIPTION.str.replace(')', '')\nconditions.DESCRIPTION = conditions.DESCRIPTION.replace(' ', '_', regex=True)\nconditionsGr = conditions.groupby('PATIENT')['DESCRIPTION'].apply(lambda x: ' '.join(x)).reset_index()\nconditionsGr.columns = ['Id','Conditions']\nconditionsGr.head()\nRawData4ML = RawData4ML.merge(conditionsGr, how='left', on='Id')\n\n# Remove duplicates\n\nRawData4ML['Conditions'] = (RawData4ML['Conditions'].astype(str).str.split()\n                              .apply(lambda x: OrderedDict.fromkeys(x).keys())\n                              .str.join(' '))\n\nRawData4ML.head(1)","dc8aa501":"texts = RawData4ML['Conditions'].astype(str).tolist()\nprint(len(texts))\n# Number of concepts: max, mean, std\n\nRawData4ML.Conditions.str.split().str.len().max(),RawData4ML.Conditions.str.split().str.len().mean(), RawData4ML.Conditions.str.split().str.len().std()","981968e9":"# Tokenize from words to integers (sequences) ... removed underscore _ the dot . and minus - from the filters\n\nmaxlen = numFeat # cut off after this number of words in a text...\nmax_words = 200 # considers only the top number of words\nmax_features = max_words\n\ntokenizer = Tokenizer(num_words=max_words, \n                     filters='!\"#$%&()*+,\/:;<=>?@[\\\\]^`{|}~\\t\\n',)\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\ndata = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='post', value=0.0)\n\nprint(data.shape)","6854a282":"rowNum = 123\n\nprint(texts[rowNum])\nprint(sequences[rowNum])\nprint(data[rowNum])","7debccaa":"# Embedding model: from integers to floating point vectors\n\nEmodel = Sequential()\nEmodel.add(Embedding(max_features, 1, input_length=maxlen))\n\noutput_array = Emodel.predict(data)\noutput_array.shape","f8c7f6c7":"# The DICTIONARY between vectors and their integers is in the weights of the Embedding layer\n\nembeddings = Emodel.layers[0].get_weights()[0]\nembeddings.shape","44d7d878":"# Back from vectors to one review - whole text\n# Create a new list fromVsequences of integers from the VECTORIZED embeddings\n# NOTE only ONE sample \/ review - rowNum  - is translated at a time from vectors to words\n\nrowNum=0\n\nreverse_word_index = dict(\n[(value, key) for (key, value) in word_index.items()])\n\n\nfromVsequences = []\n\nfor wordNum in range(len(output_array[rowNum])):\n    itemindex = np.where(embeddings == output_array[rowNum][wordNum])\n    wordInteger = itemindex[0][0]\n    if wordInteger != 0: # remove padding w 0s done above to data\n        fromVsequences.append(wordInteger)\n\n#print(fromVsequences)\n\nVdecoded_review = ' '.join(\n[reverse_word_index.get(i) for i in fromVsequences])\n\n\nprint(texts[rowNum])\nprint(sequences[rowNum])\nprint(data[rowNum])\nprint(output_array[rowNum])\nprint(Vdecoded_review)","14cc2c48":"VectFeat = []\n\nfor rowNum in range(output_array.shape[0]):\n    VectFeat.append(output_array[rowNum].flatten())\n\nVectFeat = pd.DataFrame(VectFeat)\nVectFeat.shape","9c8a8ac7":"VectFeat.columns=[\"Condition\"+str(i) for i in range(0, numFeat)]\nVectFeat","fb2cf2d2":"# Add the embeddings as features for RF (representation learning)\n\nprint(X.shape)\n\nX = pd.concat([X, VectFeat], axis=1)\n\nprint(X.shape)\nX.head(1)","a7621452":"# Only meds from 2019 and not stopped\n\nmedications.sort_values(by =['PATIENT', 'START'],ascending=False, inplace=True)\nmedications['START'] = pd.to_datetime(medications['START'])\nmedications = medications[(medications.START > '2019-01-01') & (medications.STOP.isnull())]\nprint(medications.shape)\nmedications.sample(1)","1756405b":"medications.DESCRIPTION = medications.DESCRIPTION.str.lower()\nmedications.DESCRIPTION = medications.DESCRIPTION.replace(' ', '_', regex=True)\n\nmedicationsGr = medications.groupby('PATIENT')['DESCRIPTION'].apply(lambda x: ' '.join(x)).reset_index()\nmedicationsGr.columns = ['Id','Medications']\n\nRawData4ML = RawData4ML.merge(medicationsGr, how='left', on='Id')\n\n# Remove duplicates\n\nRawData4ML['Medications'] = (RawData4ML['Medications'].astype(str).str.split()\n                              .apply(lambda x: OrderedDict.fromkeys(x).keys())\n                              .str.join(' '))\n\nRawData4ML.head(1)","a3915934":"texts = RawData4ML['Medications'].astype(str).tolist()\nprint(len(texts))\n# Number of concepts: max, mean, std\n\nRawData4ML.Medications.str.split().str.len().max(),RawData4ML.Medications.str.split().str.len().mean(), RawData4ML.Medications.str.split().str.len().std()","de0a71e3":"# Tokenize from words to integers (sequences) ... removed underscore _ the dot . and minus - from the filters\n\nmaxlen = numFeat # cut off after this number of words in a text\nmax_words = 160 # considers only the top number of words\nmax_features = max_words\n\ntokenizer = Tokenizer(num_words=max_words, \n                     filters='!\"#$%&()*+,\/:;<=>?@[\\\\]^`{|}~\\t\\n',)\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\ndata = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='post', value=0.0)\n\nprint(data.shape)","65c25eac":"# Embedding model: from integers to floating point vectors\n\nrowNum=112\n\nEmodel = Sequential()\nEmodel.add(Embedding(max_features, 1, input_length=maxlen))\n\noutput_array = Emodel.predict(data)\nprint(output_array.shape)\n\n# The DICTIONARY between vectors and their integers is in the weights of the Embedding layer\n\nembeddings = Emodel.layers[0].get_weights()[0]\nprint(embeddings.shape)\n\nreverse_word_index = dict(\n[(value, key) for (key, value) in word_index.items()])\n\nfromVsequences = []\n\nfor wordNum in range(len(output_array[rowNum])):\n    itemindex = np.where(embeddings == output_array[rowNum][wordNum])\n    wordInteger = itemindex[0][0]\n    if wordInteger != 0: # remove padding w 0s done above to data\n        fromVsequences.append(wordInteger)\n\nVdecoded_review = ' '.join(\n[reverse_word_index.get(i) for i in fromVsequences])\n\nprint(texts[rowNum])\n#print(sequences[rowNum])\n#print(data[rowNum])\n#print(output_array[rowNum])\nprint(Vdecoded_review)","64ed219e":"VectFeat = []\n\nfor rowNum in range(output_array.shape[0]):\n    VectFeat.append(output_array[rowNum].flatten())\n\nVectFeat = pd.DataFrame(VectFeat)\nprint(VectFeat.shape)\n\nVectFeat.columns=[\"Medication\"+str(i) for i in range(0, numFeat)]\nVectFeat","aad5f411":"# Add the embeddings as features for RF (representation learning)\n\nprint(X.shape)\n\nX = pd.concat([X, VectFeat], axis=1)\nprint(X.shape)\nX.head(1)","0e9a27a7":"# Only observations from 2020\n\nprint(observations.shape)\nobservations['DATE'] = pd.to_datetime(observations['DATE'])\nobservations = observations[observations.DATE > '2020-01-01']\nobservations['DescVal'] = observations.DESCRIPTION.astype(str) +'_'+ observations.VALUE.astype(str)\nprint(observations.shape)\nobservations.sort_values(by =['PATIENT', 'DATE'],ascending=False, inplace=True)\nobservations.head()","1144e442":"observations.DescVal = observations.DescVal.str.lower()\nobservations.DescVal = observations.DescVal.replace(' ', '_', regex=True)\n\nobservationsGr = observations.groupby('PATIENT')['DescVal'].apply(lambda x: ' '.join(x)).reset_index()\nobservationsGr.columns = ['Id','Observations']\n\nRawData4ML = RawData4ML.merge(observationsGr, how='left', on='Id')\n\n# Remove duplicates\n\nRawData4ML['Observations'] = (RawData4ML['Observations'].astype(str).str.split()\n                              .apply(lambda x: OrderedDict.fromkeys(x).keys())\n                              .str.join(' '))\n\nRawData4ML.head(1)","15a67ac6":"texts = RawData4ML['Observations'].astype(str).tolist()\nprint(len(texts))\n# Number of concepts: max, mean, std\n\nRawData4ML.Observations.str.split().str.len().max(),RawData4ML.Observations.str.split().str.len().mean(), RawData4ML.Observations.str.split().str.len().std()","9257b23f":"# Tokenize from words to integers (sequences) ... removed underscore _ the dot . and minus - from the filters\n\nmaxlen = numFeat # cut off after this number of words in a text\nmax_words = 49000 # considers only the top number of words\nmax_features = max_words\n\ntokenizer = Tokenizer(num_words=max_words, \n                     filters='!\"#$%&()*+,\/:;<=>?@[\\\\]^`{|}~\\t\\n',)\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\ndata = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='post', value=0.0)\n\nprint(data.shape)","a7298fe7":"# Embedding model: from integers to floating point vectors\n\nrowNum=112\n\nEmodel = Sequential()\nEmodel.add(Embedding(max_features, 1, input_length=maxlen))\n\noutput_array = Emodel.predict(data)\nprint(output_array.shape)\n\n# The DICTIONARY between vectors and their integers is in the weights of the Embedding layer\n\nembeddings = Emodel.layers[0].get_weights()[0]\nprint(embeddings.shape)\n\nreverse_word_index = dict(\n[(value, key) for (key, value) in word_index.items()])\n\nfromVsequences = []\n\nfor wordNum in range(len(output_array[rowNum])):\n    itemindex = np.where(embeddings == output_array[rowNum][wordNum])\n    wordInteger = itemindex[0][0]\n    if wordInteger != 0: # remove padding w 0s done above to data\n        fromVsequences.append(wordInteger)\n\nVdecoded_review = ' '.join(\n[reverse_word_index.get(i) for i in fromVsequences])\n\nprint(texts[rowNum])\n#print(sequences[rowNum])\n#print(data[rowNum])\n#print(output_array[rowNum])\nprint(Vdecoded_review)","5dfa7c1f":"VectFeat = []\n\nfor rowNum in range(output_array.shape[0]):\n    VectFeat.append(output_array[rowNum].flatten())\n\nVectFeat = pd.DataFrame(VectFeat)\nprint(VectFeat.shape)\n\nVectFeat.columns=[\"Obs\"+str(i) for i in range(0, numFeat)]\nVectFeat","cf25d02b":"# Add the embeddings as features for RF (representation learning)\n\nprint(X.shape)\n\nX = pd.concat([X, VectFeat], axis=1)\nprint(X.shape)\nX.head(1)","d8baf736":"del observations\ngc.collect()","e386ea38":"# Procedures\n\nprocedures.sort_values(by =['PATIENT', 'DATE'],ascending=False, inplace=True)\nprint(procedures.shape)\nprocedures.sample(1)","7535f0ef":"procedures.DESCRIPTION = procedures.DESCRIPTION.str.lower()\nprocedures.DESCRIPTION = procedures.DESCRIPTION.replace(' ', '_', regex=True)\n\nproceduresGr = procedures.groupby('PATIENT')['DESCRIPTION'].apply(lambda x: ' '.join(x)).reset_index()\nproceduresGr.columns = ['Id','Procedures']\n\nRawData4ML = RawData4ML.merge(proceduresGr, how='left', on='Id')\n# Remove duplicates\n\nRawData4ML['Procedures'] = (RawData4ML['Procedures'].astype(str).str.split()\n                              .apply(lambda x: OrderedDict.fromkeys(x).keys())\n                              .str.join(' '))\n\nRawData4ML.head(1)","e1789b97":"texts = RawData4ML['Procedures'].astype(str).tolist()\nprint(len(texts))\n# Number of concepts: max, mean, std\n\nRawData4ML.Procedures.str.split().str.len().max(),RawData4ML.Procedures.str.split().str.len().mean(), RawData4ML.Procedures.str.split().str.len().std()","829f42d6":"# Tokenize from words to integers (sequences) ... removed underscore _ the dot . and minus - from the filters\n\nmaxlen = numFeat # cut off after this number of words in a text\nmax_words = 200 # considers only the top number of words\nmax_features = max_words\n\ntokenizer = Tokenizer(num_words=max_words, \n                     filters='!\"#$%&()*+,\/:;<=>?@[\\\\]^`{|}~\\t\\n',)\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\ndata = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='post', value=0.0)\n\nprint(data.shape)","15164165":"# Embedding model: from integers to floating point vectors\n\nrowNum=112\n\nEmodel = Sequential()\nEmodel.add(Embedding(max_features, 1, input_length=maxlen))\n\noutput_array = Emodel.predict(data)\nprint(output_array.shape)\n\n# The DICTIONARY between vectors and their integers is in the weights of the Embedding layer\n\nembeddings = Emodel.layers[0].get_weights()[0]\nprint(embeddings.shape)\n\nreverse_word_index = dict(\n[(value, key) for (key, value) in word_index.items()])\n\nfromVsequences = []\n\nfor wordNum in range(len(output_array[rowNum])):\n    itemindex = np.where(embeddings == output_array[rowNum][wordNum])\n    wordInteger = itemindex[0][0]\n    if wordInteger != 0: # remove padding w 0s done above to data\n        fromVsequences.append(wordInteger)\n\nVdecoded_review = ' '.join(\n[reverse_word_index.get(i) for i in fromVsequences])\n\nprint(texts[rowNum])\n#print(sequences[rowNum])\n#print(data[rowNum])\n#print(output_array[rowNum])\nprint(Vdecoded_review)","f660b7d0":"VectFeat = []\n\nfor rowNum in range(output_array.shape[0]):\n    VectFeat.append(output_array[rowNum].flatten())\n\nVectFeat = pd.DataFrame(VectFeat)\nprint(VectFeat.shape)\n\nVectFeat.columns=[\"Procedure\"+str(i) for i in range(0, numFeat)]\nVectFeat","4e2c3878":"# Add the embeddings as features for RF (representation learning)\n\nprint(X.shape)\n\nX = pd.concat([X, VectFeat], axis=1)\nprint(X.shape)\nX.head(1)","4766ad2a":"encounters.sort_values(by =['PATIENT', 'START'],ascending=False, inplace=True)\nprint(encounters.shape)\nencounters.sample(1)","d731fc8e":"encounters.DESCRIPTION = encounters.DESCRIPTION.str.lower()\nencounters.DESCRIPTION = encounters.DESCRIPTION.replace(' ', '_', regex=True)\n\nencountersGr = encounters.groupby('PATIENT')['DESCRIPTION'].apply(lambda x: ' '.join(x)).reset_index()\nencountersGr.columns = ['Id','Encounters']\n\nRawData4ML = RawData4ML.merge(encountersGr, how='left', on='Id')\n# Remove duplicates\n\nRawData4ML['Encounters'] = (RawData4ML['Encounters'].astype(str).str.split()\n                              .apply(lambda x: OrderedDict.fromkeys(x).keys())\n                              .str.join(' '))\n\nRawData4ML.head(1)","4ef06a7e":"texts = RawData4ML['Encounters'].astype(str).tolist()\nprint(len(texts))\n# Number of concepts: max, mean, std\n\nRawData4ML.Encounters.str.split().str.len().max(),RawData4ML.Encounters.str.split().str.len().mean(), RawData4ML.Encounters.str.split().str.len().std()","e5034f02":"# Tokenize from words to integers (sequences) ... removed underscore _ the dot . and minus - from the filters\n\nmaxlen = numFeat # cut off after this number of words in a text\nmax_words = 62 # considers only the top number of words\nmax_features = max_words\n\ntokenizer = Tokenizer(num_words=max_words, \n                     filters='!\"#$%&()*+,\/:;<=>?@[\\\\]^`{|}~\\t\\n',)\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\ndata = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='post', value=0.0)\n\nprint(data.shape)","ca30b5e5":"# Embedding model: from integers to floating point vectors\n\nrowNum=112\n\nEmodel = Sequential()\nEmodel.add(Embedding(max_features, 1, input_length=maxlen))\n\noutput_array = Emodel.predict(data)\nprint(output_array.shape)\n\n# The DICTIONARY between vectors and their integers is in the weights of the Embedding layer\n\nembeddings = Emodel.layers[0].get_weights()[0]\nprint(embeddings.shape)\n\nreverse_word_index = dict(\n[(value, key) for (key, value) in word_index.items()])\n\nfromVsequences = []\n\nfor wordNum in range(len(output_array[rowNum])):\n    itemindex = np.where(embeddings == output_array[rowNum][wordNum])\n    wordInteger = itemindex[0][0]\n    if wordInteger != 0: # remove padding w 0s done above to data\n        fromVsequences.append(wordInteger)\n\nVdecoded_review = ' '.join(\n[reverse_word_index.get(i) for i in fromVsequences])\n\nprint(texts[rowNum])\n#print(sequences[rowNum])\n#print(data[rowNum])\n#print(output_array[rowNum])\nprint(Vdecoded_review)","428847d7":"VectFeat = []\n\nfor rowNum in range(output_array.shape[0]):\n    VectFeat.append(output_array[rowNum].flatten())\n\nVectFeat = pd.DataFrame(VectFeat)\nprint(VectFeat.shape)\n\nVectFeat.columns=[\"Encounter\"+str(i) for i in range(0, numFeat)]\nVectFeat","4d0ab1db":"# Add the embeddings as features for RF (representation learning)\n\nprint(X.shape)\n\nX = pd.concat([X, VectFeat], axis=1)\nprint(X.shape)\nX.head(1)","c56eeba2":"# SPLIT into Train & Test \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint ('X_train: ', X_train.shape)\nprint ('X_test: ', X_test.shape)\nprint ('y_train: ', y_train.shape)\nprint ('y_test: ', y_test.shape)","edc82ccb":"%%time\n\n# Predict on test\n\nmodel = RandomForestClassifier()\n\nmodel.fit(X_train, y_train)\nPreds = model.predict(X_test)\n\n# Classification report\nprint(classification_report(y_test, Preds))","54d76726":"# CM\n\nCM = confusion_matrix(y_test, Preds)\ntn, fp, fn, tp = confusion_matrix(y_test, Preds).ravel()\n\nprint(CM)\nprint(\"_\"*50)\nprint(\"TP \", tp)\nprint(\"FP \", fp)\nprint(\"TN \", tn)\nprint(\"FN \", fn)","35ea9b57":"plot_confusion_matrix(CM, \n                      normalize    = False,\n                      target_names = ['0', '1'],\n                      title        = \"Confusion Matrix\")","315ab93f":"print ('precision ',round(precision_score(y_test, Preds),3))\nprint ('recall ',round(recall_score(y_test, Preds),3 ))\nprint ('accuracy ',round(accuracy_score(y_test, Preds),3))\nprint ('F1 score ',round(f1_score(y_test, Preds),3))","649ee8ad":"# AUC\/ROC curves should be used when there are roughly equal numbers of observations for each class\n# Precision-Recall curves should be used when there is a moderate to large class imbalance\n\n# calculate AUC\nauc = roc_auc_score(y_test, Preds)\nprint('AUC: %.3f' % auc)\n# calculate roc curve\nfpr, tpr, thresholds = roc_curve(y_test, Preds)\n# plot no skill\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(fpr, tpr, marker='.')\nplt.title('ROC ')\n# show the plot\nplt.show()","f56368e9":"# calculate precision-recall curve\nprecision, recall, thresholds = precision_recall_curve(y_test, Preds)\n# calculate F1 score\nf1 = f1_score(y_test, Preds)\n# calculate precision-recall AUC\n#auc = auc(recall, precision)\n# calculate average precision score\nap = average_precision_score(y_test, Preds)\nprint('f1=%.3f ap=%.3f' % (f1, ap))\n# plot no skill\nplt.plot([0, 1], [0.5, 0.5], linestyle='--')\n# plot the roc curve for the model\nplt.plot(recall, precision, marker='.')\n# show the plot\nplt.show()","6bfb3178":"# Feature importance\n\nimportance = model.feature_importances_\nFIdf = pd.DataFrame()\nFIdf['Feature'] = X.columns\nFIdf['Importance'] = importance\n\nFIdf.sort_values(by=['Importance'], ascending=False, inplace=True)\nFIdf.head(20)","56e843cb":"FIdf.sort_values(by=['Importance'], ascending=True, inplace=True)\nfigure(figsize=(12, 60), dpi=80)\n\nplt.barh(FIdf['Feature'].astype(str), FIdf['Importance'])\nplt.show()","b624d454":"# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=model, dataset=X, model_features=X.columns, feature='Age')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'Age')\nplt.show()","4f0ca554":"# Create the data for PDP\npdp_goals = pdp.pdp_isolate(model=model, dataset=X, model_features=X.columns, feature='HEALTHCARE_EXPENSES')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'HEALTHCARE_EXPENSES')\nplt.show()","22a7e6cf":"row_to_show = np.where(y==True)[0][1234]\n\ndata_for_prediction = X.iloc[row_to_show]  \ndata_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n\nmodel.predict_proba(data_for_prediction_array)","415f8ec9":" # SHAP\n\nexplainer = shap.TreeExplainer(model)\n\n# Calculate Shap values\nshap_values = explainer.shap_values(data_for_prediction)\nshap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)","ac88371f":"# Following will work ONLY with the LAST embeddings\n\n# Reverse from ONE vector to ONE concept\n\ndef find_nearest(array, value):\n    array = np.asarray(array)\n    idx = (np.abs(array - value)).argmin()\n    return idx, array[idx]\n\nmyIx = find_nearest(embeddings, -0.02698)[0]\nprint(reverse_word_index.get(myIx))","c4ff6146":"row_to_show = np.where(y==False)[0][1234]\n\ndata_for_prediction = X.iloc[row_to_show]  \ndata_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n\nmodel.predict_proba(data_for_prediction_array)","ad818d07":" # SHAP\n\nexplainer = shap.TreeExplainer(model)\n\n# Calculate Shap values\nshap_values = explainer.shap_values(data_for_prediction)\nshap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)","52744c08":"### Encounters","044331c5":"#Prep ONLY num of interactions - REMOVE demographics\n\nRawData4ML = RawData4ML[['Id', 'NumImmunizations', 'NumProviders',\n       'NumPayer_transitions', 'NumDevices', 'NumSupplies', 'NumOrganizations',\n       'NumPayers', 'NumAllergies', 'NumCareplans', 'NumImages',\n       'NumEncounters', 'NumObservations', 'NumMedications', 'NumProcedures',\n       'NumConditions', 'Dead']]\n","7bb68fb4":"# Data","211f3cf5":"# Prevent data leakage ... Feature contains word DEATH","b399b8cd":"# Count number of interactions between patient and system","4a6d4c1d":"### Observations","7ad3ac11":"## Feature importance - What features have the biggest impact on predictions","dc16af12":"### Procedures","b80dda22":"## Partial Dependence Plots - How ONE feature affects predictions\n\nWhile feature importance shows what variables most affect predictions, partial dependence plots show how a feature affects predictions.","eddbf759":"### Medications","4ad46f88":"# Goal: Predict Mortality\n\n* The task is a ML supervised, binary classification of mortality\n* The synthetic data is  from\u00a0http:\/\/hdx.mitre.org\/downloads\/syntheticmass\/100k_synthea_covid19_csv.zip \n* The data is unbalanced as mortality = 19.5% and the majority voting \/ guess accuracy = 80.5%  \n* The best performance metric to monitor is F1 score with accuracy, precision, recall and AUC\n\n\n## Problems \n\n* Original data is in 16 tables in a relational format \u2013 needs to be flattened\n* While there are a several numerical features (age, longitude, latitude, etc.)\u2026\n* Data is mostly categorical and highly dimensional (many meds, conditions, observations, etc.)\n* One Hot Encoding the categorical features will create a sparse matrix (aka curse of dimensionality)\n\n## Plan\n* Categorical concepts embedding \/ vectorization instead of OHE\n* Use these vectors as features (aka representational learning) together with the other numerical features\n* Create new features as the number of all interactions between patient \u2013 system (number of meds, labs, care-plans, providers, etc.)\n* ALL the above categories of features - at once\n\n## Results \n![Screen Shot 2021-09-10 at 6.05.02 PM.png](attachment:ad466935-0aba-4c93-91e4-4e307428db20.png)\n\n### Future\n* Keras double input NN (simple NN on numericals and RNN on emebeddings, concatenated before the last sigmoid layer)","076d5250":"# Prep concepts for embedding\n\n* Arrange descending order - pick only last, 20 most recent\n* Concatenate words into concepts - chronic sinusitis = chronic_sinusitis\n* Remove duplicates\n* Tokenize\n* Vectorize into one dimension\n* Add the vectors as features","b7ba9be7":"# Split into train and test","e33b95cc":"### Conditions","05f1f8d5":"# Models","e4861f10":"## SHAP - Impact of each feature on ONE prediction\n\nSHAP Values (an acronym from SHapley Additive exPlanations) break down a prediction to show the impact of each feature. ","804076ef":"#ONLY when ONE category of features at a time\n\nX = VectFeat\nX.shape"}}