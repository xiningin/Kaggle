{"cell_type":{"09680c85":"code","8cdba10a":"code","15937c2f":"code","36a18355":"code","336ff79e":"code","2c3c4cfe":"code","4ddc402e":"code","a6c33f70":"code","6c310a0a":"code","64f3e8f5":"code","d8145bb6":"code","8aabddcc":"code","3dbb5acd":"code","4dcc951e":"code","32c6f1b2":"code","97eb3773":"code","fa724029":"code","b5c35dc8":"code","0d2fd937":"code","971e256b":"code","6366e724":"code","d88af45b":"code","c92b724f":"code","b8fb327e":"code","e150a7ce":"code","69c93092":"code","0e27e1b2":"code","11939c9c":"code","a9c3d0eb":"code","a350105a":"code","39427738":"code","d7be83af":"code","4af97711":"code","f2704993":"code","99f5d208":"code","9f3274bf":"code","f16c254b":"code","464b9818":"code","86b70663":"code","0fd59fdf":"code","ef632b29":"code","9c808a6f":"code","4306a33a":"code","774727f5":"code","f799b32b":"code","ef4a5d5e":"code","89bb4746":"code","ba710636":"code","742f3335":"code","df8910f6":"code","1cfbc61f":"code","0774ab7c":"code","f5957503":"code","e32bc6db":"code","951520ab":"code","3e8f0297":"code","7e56e3e4":"markdown","c3c5910a":"markdown","9f97179b":"markdown","d432fa30":"markdown"},"source":{"09680c85":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8cdba10a":"#Import Libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\n\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\nfrom scipy.stats.stats import pearsonr\nnp.random.seed(10)\n\n%config InlineBackend.figure_format = 'retina' #set 'png' here when working on notebook\n%matplotlib inline","15937c2f":"#Import Dataset\ntrain_df = pd.read_csv('..\/input\/atm-data\/ATM_training.csv')\ntest_df = pd.read_csv('..\/input\/atm-data\/ATM_test.csv')","36a18355":"# Histogram of the Dependent Variable\nsns.distplot(train_df['Withdraw']);","336ff79e":"#scatter plot of Shops and Withdraw\nvar = 'Shops'\ndata = pd.concat([train_df['Withdraw'], train_df[var]], axis=1)\ndata.plot.scatter(x=var, y='Withdraw', ylim=(0,150));","2c3c4cfe":"# Box plot of ATMs and Withdraw\nvar = 'ATMs'\ndata = pd.concat([train_df['Withdraw'], train_df[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"Withdraw\", data=data)\nfig.axis(ymin=0, ymax=150);","4ddc402e":"# Boxplot of Weekday and Withdraw\nvar = 'Weekday'\ndata = pd.concat([train_df['Withdraw'], train_df[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"Withdraw\", data=data)\nfig.axis(ymin=0, ymax=150);","a6c33f70":"#skewness and kurtosis\nprint(\"Skewness: %f\" % train_df['Withdraw'].skew())\nprint(\"Kurtosis: %f\" % train_df['Withdraw'].kurt())","6c310a0a":"#correlation matrix\ncorrmat = train_df.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","64f3e8f5":"#Withdraw correlation matrix\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'Withdraw')['Withdraw'].index\ncm = np.corrcoef(train_df[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","d8145bb6":"# Scatter plot\nsns.set()\ncols = ['Shops', 'ATMs', 'Downtown', 'Weekday', 'Center', 'High', 'Withdraw']\nsns.pairplot(train_df[cols], size = 2.5)\nplt.show();","8aabddcc":"# Checking missing data\ntotal = train_df.isnull().sum().sort_values(ascending=False)\npercent = (train_df.isnull().sum()\/train_df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","3dbb5acd":"#standardizing data\nfrom sklearn.preprocessing import StandardScaler\n\nsaleprice_scaled = StandardScaler().fit_transform(train_df['Withdraw'][:,np.newaxis]);\nlow_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10]\nhigh_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]\nprint('outer range (low) of the distribution:')\nprint(low_range)\nprint('\\nouter range (high) of the distribution:')\nprint(high_range)","4dcc951e":"#histogram and normal probability plot\nfrom scipy.stats import norm, stats\nfrom scipy import stats\n\nsns.distplot(train_df['Withdraw'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train_df['Withdraw'], plot=plt)","32c6f1b2":"train_df.head()","97eb3773":"matplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\nprices = pd.DataFrame({\"Withdraw\":train_df[\"Withdraw\"], \"log(Withdraw + 1)\":np.log1p(train_df[\"Withdraw\"])})\nprices.hist()","fa724029":"all_data = pd.concat((train_df.loc[:,'Shops':'High'],\n                      test_df.loc[:,'Shops':'High']))","b5c35dc8":"#log transform the target:\ntrain_df[\"Withdraw\"] = np.log1p(train_df[\"Withdraw\"])\n\n#log transform skewed numeric features:\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\nskewed_feats = train_df[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\n\nall_data[skewed_feats] = np.log1p(all_data[skewed_feats])","0d2fd937":"all_data = pd.get_dummies(all_data)\n","971e256b":"all_data.head()","6366e724":"#creating matrices for sklearn:\nX_train = all_data[:train_df.shape[0]]\nX_test = all_data[train_df.shape[0]:]\ny = train_df.Withdraw\ny_test = test_df.iloc[:,-1]","d88af45b":"# Import libraries for Logic-based algorithms\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.metrics import mean_absolute_error, explained_variance_score\nfrom sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV, Lasso\nfrom sklearn.model_selection import cross_val_score\nimport lightgbm as lgb\nfrom sklearn.kernel_ridge import KernelRidge\n\ndef mse_cv(model):\n    mse= -cross_val_score(model, X_train, y, scoring=\"neg_mean_squared_error\", cv = 5)\n    return(mse)","c92b724f":"# Cross validation and hyperparmater tuning for ridge regression so we can visualize\n# MSE in a plot\nalphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\ncv_ridge = [mse_cv(Ridge(alpha = alpha)).mean() \n            for alpha in alphas]","b8fb327e":"# Plot MSE mean for each cross validation and hyperparameter\ncv_ridge = pd.Series(cv_ridge, index = alphas)\ncv_ridge.plot(title = \"Hypertuning-Ridge Regression\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"MSE\")","e150a7ce":"cv_ridge.min()","69c93092":"# Fit ridge regression cross validation\nmodel_ridge = RidgeCV(alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]).fit(X_train, y)\n","0e27e1b2":"# predict on test set\npred_ridge_all =  np.expm1(model_ridge.predict(X_test))\n","11939c9c":"# compare to true results\nmse = MSE(y_test, pred_ridge_all)\nprint(\"MSE : % f\" %(mse))","a9c3d0eb":"# Check differences between true value and predicted value\ndifference = y_test - pred_ridge_all","a350105a":"data = {'y_test': y_test, 'values': pred_ridge_all, 'Error': difference}\ndf = pd.DataFrame(data=data)\ndf","39427738":"abs(df['Error']).min(), abs(df['Error']).max()","d7be83af":"# plot coefficients\ncoef = pd.Series(model_ridge.coef_, index = X_train.columns)\nprint(\"Ridge picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\nimp_coef = pd.concat([coef.sort_values().head(10)])\nmatplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Ridge Model\")","4af97711":"# Cross validation and hyperparmater tuning for LASSO regression so we can visualize\n# MSE in a plot\nalphas = [1, 0.1, 0.001, 0.0005]\ncv_lasso = [mse_cv(Lasso(alpha = alpha)).mean() \n            for alpha in alphas]","f2704993":"# Plot MSE mean for each cross validation and hyperparameter\ncv_lasso = pd.Series(cv_lasso, index = alphas)\ncv_lasso.plot(title = \"Hypertuning-Lasso\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"MSE\")","99f5d208":"cv_lasso.min()","9f3274bf":"# Fit LASSO regression cross validation\nmodel_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, y)\n","f16c254b":"mse_cv(model_lasso).mean()","464b9818":"coef = pd.Series(model_lasso.coef_, index = X_train.columns)","86b70663":"print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","0fd59fdf":"imp_coef = pd.concat([coef.sort_values().head(10)])","ef632b29":"# visualize the weighted coefficients\nmatplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")","9c808a6f":"#let's look at the residuals as well:\nmatplotlib.rcParams['figure.figsize'] = (6.0, 6.0)\n\npreds = pd.DataFrame({\"preds\":model_lasso.predict(X_train), \"true\":y})\npreds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\npreds.plot(x = \"preds\", y = \"residuals\",kind = \"scatter\")","4306a33a":"# Predict using LASSO on test set\nlasso_preds = np.expm1(model_lasso.predict(X_test))","774727f5":"# Calculate MSE for test set\nmse = MSE(y_test, lasso_preds)\nprint(\"MSE : % f\" %(mse))","f799b32b":"min(abs(y_test - lasso_preds)), max(abs(y_test - lasso_preds))","ef4a5d5e":"# Import for ANN\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.regularizers import l1\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split","89bb4746":"#creating matrices for sklearn:\nX_train = all_data[:train_df.shape[0]]\nX_test = all_data[train_df.shape[0]:]\ny_train = train_df.Withdraw\ny_test = test_df.iloc[:,-1]","ba710636":"# Scaler Transform\nscalerX = StandardScaler().fit(X_train)\nscalery = StandardScaler().fit(pd.array(y_train).reshape(-1, 1))\nX_train = scalerX.transform(X_train)\ny_train = scalery.transform(pd.array(y_train).reshape(-1, 1))","742f3335":"X_train, X_val, y_train, y_val = train_test_split(X_train, y, random_state = 3, train_size = 0.8)","df8910f6":"# Creating the model\nmodel = Sequential()\n\n# input layer\nmodel.add(Dense(19,activation='relu'))\n\n# hidden layers\nmodel.add(Dense(19,activation='relu'))\nmodel.add(Dense(19,activation='relu'))\nmodel.add(Dense(19,activation='relu'))\n\n# output layer\nmodel.add(Dense(1))\n\nmodel.compile(optimizer='adam',loss='mse')","1cfbc61f":"hist = model.fit(X_train, y_train, validation_data = (X_val, y_val))","0774ab7c":"pd.Series(model.predict(X_test)[:,0]).hist()\n","f5957503":"predictions = pd.Series(model.predict(X_test)[:,0])\n","e32bc6db":"predictions = scalery.inverse_transform(predictions)","951520ab":"# predictions on the test set\n\nprint('MAE: ',mean_absolute_error(y_test,predictions))\nprint('MSE: ',MSE(y_test,predictions))\nprint('RMSE: ',np.sqrt(MSE(y_test,predictions)))\nprint('Variance Regression Score: ',explained_variance_score(y_test,predictions))\n\nprint('\\n\\nDescriptive Statistics:\\n',train_df['Withdraw'].describe())","3e8f0297":"f, axes = plt.subplots(1, 2,figsize=(15,5))\n\n# Our model predictions\nplt.scatter(y_test,predictions)\n\n# Perfect predictions\nplt.plot(y_test,y_test,'r')\n\nerrors = y_test.values.reshape(20, 1) - predictions\nsns.distplot(errors, ax=axes[0])\n\nsns.despine(left=True, bottom=True)\naxes[0].set(xlabel='Error', ylabel='', title='Error Histogram')\naxes[1].set(xlabel='Test True Y', ylabel='Model Predictions', title='Model Predictions vs Perfect Fit')","7e56e3e4":"# Ridge Regression","c3c5910a":"# ANN","9f97179b":"# LASSO","d432fa30":"# Exploratory Data Analysis"}}