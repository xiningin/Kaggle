{"cell_type":{"c998cccc":"code","bde521f5":"code","ce130094":"code","42515a5a":"code","f018cd35":"code","1decc72d":"code","9a17475b":"code","ed5116a1":"code","8a147622":"code","63c12f49":"code","dec9a32d":"code","a1170d64":"code","37c256d1":"code","1d9ac4e6":"code","96d097b7":"code","03719736":"code","247ce7f8":"code","520aeb27":"code","3d9f3c95":"code","8a07069f":"code","5ec255d1":"code","d6502ec7":"code","f8060e46":"code","5eaa898d":"code","1f1ba5de":"code","2d14dfce":"code","f612d0d2":"code","a56290b8":"code","6b35d259":"code","e04e57f7":"code","a310622b":"code","86df65ab":"markdown","219baadc":"markdown","ed66a853":"markdown","299ff2a3":"markdown","937c21b3":"markdown","58d19ffc":"markdown","5cb342ef":"markdown","82ceb262":"markdown","c52f348f":"markdown","81673729":"markdown","be5083f5":"markdown","49b66c9a":"markdown","3bda847f":"markdown","8d1449c0":"markdown","cd1eb287":"markdown","3cff4f89":"markdown","d1ef213f":"markdown","e96f52c0":"markdown","a0b90fe5":"markdown","d31f72b9":"markdown","c821ebb6":"markdown","067069a2":"markdown","5f265e59":"markdown","f3843c8e":"markdown"},"source":{"c998cccc":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.base import BaseEstimator\nfrom sklearn.utils.estimator_checks import check_estimator\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\nfrom typing import Optional, Union\n\ndata = pd.read_csv('..\/input\/fpl-dataset\/data.csv', index_col='entry_id')\nelement_types = pd.read_csv('..\/input\/fpl-dataset\/element_types.csv', index_col='id')\nplayers = pd.read_csv('..\/input\/fpl-dataset\/players.csv', index_col='id')\nteams = pd.read_csv('..\/input\/fpl-dataset\/teams.csv', index_col='id')","bde521f5":"print(data.info())\n\ndata.head()","ce130094":"print(element_types.info())\n\nelement_types.head()","42515a5a":"print(players.info())\n\nplayers.head()","f018cd35":"print(teams.info())\n\nteams.head()","1decc72d":"train_data = data[data.event_id < 38]\ntest_data = data[data.event_id == 38]\n\nprint(\"Training data entries:\", train_data.shape[0])\nprint(\"Test data entries:\", test_data.shape[0])","9a17475b":"removed_cols = ['timestamp', 'fixture_code', 'kickoff_time', 'opposition', 'event_id']\n\nfor col in removed_cols:\n    del train_data[col]\n    del test_data[col]","ed5116a1":"plt.figure(figsize=(10, 10))\nsns.histplot(x=train_data.response, hue=train_data.status, binwidth=1)\n","8a147622":"played_match_1 = train_data[train_data.status == 'a'][\"minutes_1\"] > 0\nplayed_match_2 = train_data[train_data.status == 'a'][\"minutes_2\"] > 0\nplayed_match_3 = train_data[train_data.status == 'a'][\"minutes_3\"] > 0\n\nfig, ax = plt.subplots(1, 3, figsize=(20, 5), sharey=True)\nsns.kdeplot(x=train_data[train_data.status == 'a'].response, hue=played_match_1, shade=True, ax=ax[0])\nsns.kdeplot(x=train_data[train_data.status == 'a'].response, hue=played_match_2, shade=True, ax=ax[1])\nsns.kdeplot(x=train_data[train_data.status == 'a'].response, hue=played_match_3, shade=True, ax=ax[2])","63c12f49":"available_players = train_data[train_data.status == 'a'].copy()\n\nsns.jointplot(x=available_players.form, y=available_players.response, kind='kde', fill=True)\nsns.jointplot(x=available_players.points_1, y=available_players.response, kind='kde', fill=True)\nsns.jointplot(x=available_players.points_2, y=available_players.response, kind='kde', fill=True)\nsns.jointplot(x=available_players.points_3, y=available_players.response, kind='kde', fill=True)","dec9a32d":"sns.histplot(x=available_players.response, hue=available_players.is_home, binwidth=1)","a1170d64":"train_data.info()","37c256d1":"train_data = pd.get_dummies(train_data)\ntest_data  = pd.get_dummies(test_data)\n\ntrain_data.info()\ntest_data.info()","1d9ac4e6":"def fill_values(df):\n    df['chance_of_playing_this_round'] = df['chance_of_playing_this_round'].fillna(100)\n    df.fillna(0, inplace=True)\n    \nfill_values(train_data)\nfill_values(test_data)\n\ntrain_data.head()","96d097b7":"train_data.info()","03719736":"del train_data['player_id']","247ce7f8":"features = list(train_data.columns)\nfeatures.remove('response')\nX_train, y_train = train_data[features].values, train_data['response'].values\nprint(X_train.shape)\nprint(y_train.shape)","520aeb27":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nprint(X_train)","3d9f3c95":"model1 = LinearRegression()\nmodel1.fit(X_train, y_train)","8a07069f":"player_ids = test_data['player_id'].values \ndel test_data['player_id']\nX_test, y_test = test_data[features].values, test_data['response'].values\nX_test = scaler.transform(X_test)","5ec255d1":"def model_to_predictions(model):\n    predictions = model.predict(X_test)\n    predictions = [[player_ids[i], predictions[i], y_test[i]] for i in range(len(player_ids))]\n    predictions = pd.DataFrame(predictions, columns=['id', 'prediction', 'actual']).set_index('id')\n    predictions = predictions.join(players, on='id')\n    return predictions\n\nmodel_to_predictions(model1).sort_values('prediction', ascending=False)","d6502ec7":"model1.score(X_test, y_test)","f8060e46":"def plot_predictions(model):\n    sns.regplot(x=model.predict(X_test), y=y_test)\n    plt.xlabel('Prediction')\n    plt.ylabel('Response')\n    \nplot_predictions(model1)","5eaa898d":"model2 = RandomForestRegressor(n_estimators=500, max_depth=10, min_samples_split=2, min_samples_leaf=5)\nmodel2.fit(X_train, y_train)","1f1ba5de":"model_to_predictions(model2).sort_values('prediction', ascending=False)","2d14dfce":"model2.score(X_test, y_test)","f612d0d2":"plot_predictions(model2)","a56290b8":"class CustomRegressor(BaseEstimator):\n    \"\"\" Regression model which handles excessive zeros by fitting a two-part model and combining predictions:\n            1) binary classifier\n            2) continuous regression\n    Implementeted as a valid sklearn estimator, so it can be used in pipelines and GridSearch objects.\n    Args:\n        clf_name: currently supports either 'logistic' or 'LGBMClassifier'\n        reg_name: currently supports either 'linear' or 'LGBMRegressor'\n        clf_params: dict of parameters to pass to classifier sub-model when initialized\n        reg_params: dict of parameters to pass to regression sub-model when initialized\n    \"\"\"\n\n    def __init__(self,\n                 clf_name: str = 'logistic',\n                 reg_name: str = 'linear',\n                 clf_params: Optional[dict] = None,\n                 reg_params: Optional[dict] = None):\n\n        self.clf_name = clf_name\n        self.reg_name = reg_name\n        self.clf_params = clf_params\n        self.reg_params = reg_params\n\n    @staticmethod\n    def _resolve_estimator(func_name: str):\n        \"\"\" Lookup table for supported estimators.\n        This is necessary because sklearn estimator default arguments\n        must pass equality test, and instantiated sub-estimators are not equal. \"\"\"\n\n        funcs = {'linear': RandomForestRegressor(n_estimators=500, random_state=0),\n                 'logistic': LogisticRegression(random_state=0)}\n\n        return funcs[func_name]\n\n    def fit(self,\n            X: Union[np.ndarray],\n            y: Union[np.ndarray]):\n        X, y = check_X_y(X, y, dtype=None,\n                         accept_sparse=False,\n                         accept_large_sparse=False,\n                         force_all_finite='allow-nan')\n\n        if X.shape[1] < 2:\n            raise ValueError('Cannot fit model when n_features = 1')\n\n        self.clf_ = self._resolve_estimator(self.clf_name)\n        if self.clf_params:\n            self.clf_.set_params(**self.clf_params)\n        self.clf_.fit(X, y != 0)\n\n        self.reg_ = self._resolve_estimator(self.reg_name)\n        if self.reg_params:\n            self.reg_.set_params(**self.reg_params)\n        self.reg_.fit(X[y != 0], y[y != 0])\n\n        self.is_fitted_ = True\n        return self\n\n    def predict(self, X: Union[np.ndarray]):\n        \"\"\" Predict combined response using binary classification outcome \"\"\"\n        X = check_array(X, accept_sparse=False, accept_large_sparse=False)\n        check_is_fitted(self, 'is_fitted_')\n        return self.clf_.predict_proba(X)[:, 1] * self.reg_.predict(X)","6b35d259":"model3 = CustomRegressor()\nmodel3.fit(X_train, y_train)\n\nmodel_to_predictions(model3).sort_values('prediction', ascending=False)\n","e04e57f7":"r2_score(y_test, model3.predict(X_test))","a310622b":"plot_predictions(model3)","86df65ab":"### Linear Regression","219baadc":"We can see that all columns are now numeric and completely filled with data. For the purposes of training a model, also notice that player_id is no longer required. This will be useful later but only in the test set where we look at our predictions again.","ed66a853":"## Preparing and Analysing the Data\n\nOur aim for this notebook is to use the data collected in gameweeks 33-37 to predict the scores in gameweek 38. Therefore, we should split the 'data' table into training and testing subsets based on these gameweeks.","299ff2a3":"Now we scale the features so that they are comparable - we do this using scikit-learn's StandardScaler class. First, we separate out the response column and the features, into numpy arrays X_train and y_train. Then we scale X_train based on the mean and standard deviation of each column.","937c21b3":"## Training a Model\n\nThe data is now fully prepared for modelling - all features are numeric, with no gaps, and scaled to be comparable. We have also extracted the target variable, 'response' into y_train for training the model. So, let's try a basic Linear Regression first.","58d19ffc":"### Random Forest Regression\n\nNow we try a more sophisticated model, called random forest regression. This takes the average of many different decision trees in order to make a prediction.","5cb342ef":"## Preprocessing and Data Cleaning\n\nAhead of the modelling phase, we should clean up the data so that:\n\n- Dummy fields are created for categorical variables\n- Any missing data is filled in\n- Erroneous data entries are removed\n- Features are scaled so as to be comparable","82ceb262":"Again, opting for an even more complex model doesn't actually give us a great benefit. This suggests that we need to revisit the data - can features be engineered, or the data manipulated in a different way to achieve better results? Of course, more data will also be helpful to such a model - as more games are played in the seasons to come, we will be able to collect more of this data and refine our predictions.","c52f348f":"Our target column to predict is going to be the 'response' of the player, i.e. how many points they went on to score in that gameweek. Let's explore how some of the columns influence this value in the training set. A first obvious column is the player's status: if a player is injured or unavailable, they are almost certain to score 0, while if they are fully fit and available, they are much more likely to score points. This is illustrated below. ","81673729":"This function successfully splits the training and testing data into 6 diffrent boolean columns based on the status. Now let's fill in those missing values. For chance of playing next round, it makes sense to fill this column with 100%, since this is based on a player's availability, and more often than not a player is available. For the remaining data points, it is safe to assume that if values have not been found, it is because the player has not played enough games to have a value. Therefore, we will fill those values with zeros. ","be5083f5":"Prepare the test data in the same way:","49b66c9a":"We see that the basic linear regression model does fairly well, giving an $R^2$ coefficient of 26% of predictions vs actual scores. Let's visualise this below.","3bda847f":"## Conclusions\n\nThis data allows us to make a nice start in making predictions in FPL. Ultimately, how well a human-being will perform on any given matchday is highly unpredictable, but an $R^2$ value of 26% at least gives us hope that as FPL managers, we can use this as a guide to help us out.","8d1449c0":"In terms of categorical variables, there is just one - status. We already explored the different values here, so we will use pandas get_dummies function to create dummy columns","cd1eb287":"# Custom Regression","3cff4f89":"Now test the model's output:","d1ef213f":"## Initial Steps\n\nLet's first do all imports that we will need, and read the data in from the csv files","e96f52c0":"This seems like a good split. First notice that the columns timestamp, event_id, opposition, fixture_code and kickoff_time are purely for administrative purposes. They will not help us with our predictions, so we can safely delete these.","a0b90fe5":"We now look at the columns of each dataframe:","d31f72b9":"# Predicting FPL Scores\n\nThis is an example of how to use the FPL dataset to predict players scores.","c821ebb6":"We see that a player is much more likely to score points if they are available and played > 0 minutes in the last three matches. Pretty self-explanatory right? So we've now explored what makes a player more likely to play, but what affects their score once they're on the pitch? The easiest features to look at are the points they scored in their last three - their **form**.","067069a2":"As this shows that if a player's status is not 'available', then they are very likely to score 0 points, it is therefore only interesting to explore the data for those whose status is available. This is based around the number of minutes a player plays - if a player is not available, they will play 0 minutes and therefore score 0. How else can we predict minutes? Well, we have the data for minutes for the last three matches.","5f265e59":"We see that using this more sophisticated model only achieves a similar $R^2$ value to linear regression.","f3843c8e":"Notice that so far, a lot of players who went on to score zero were predicted more using these models. This is because the model is not quite 'detecting' whether a player is likely to play or not. In effect, it is hard to predict such a skewed distribution without overfitting. So, we aim to combat this using 'hurdle' regression. I.e. we train a classifier to predict the probability that a player scores a non-zero amount of points, then multiply the output of this by another regression model. Note that the regression model must only be trained on data points with a non-zero response. This uses the fact that for any random variable $X$:\n\n$E(X) = E(X | X \\neq 0) P(X \\neq 0)$"}}