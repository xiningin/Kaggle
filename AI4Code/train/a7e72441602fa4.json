{"cell_type":{"428c1999":"code","4c5b833f":"code","eb296621":"code","ca95e69c":"code","55f80144":"code","077d5467":"code","591e9e28":"code","0e80ec8d":"code","7ee6d2aa":"code","d5481c9b":"code","27a3331b":"markdown","6552a720":"markdown","beb3fc8b":"markdown"},"source":{"428c1999":"from IPython.core.display import display, HTML\n\nimport glob\nimport os\nimport gc\nfrom functools import reduce\nfrom joblib import Parallel, delayed\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport numpy.matlib\nfrom numpy.random import seed\n\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.cluster import KMeans\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import backend as K\nfrom keras.backend import sigmoid\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras.layers import Activation\n\nimport lightgbm as lgb\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 300)\n\n#https:\/\/bignerdranch.com\/blog\/implementing-swish-activation-function-in-keras\/\ndef swish(x, beta = 1):\n    return (x * sigmoid(beta * x))\nget_custom_objects().update({'swish': Activation(swish)})","4c5b833f":"# data preparation\n# Function to calculate first WAP\ndef calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n\n# Function to calculate second WAP\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# Function to calculate third WAP\ndef calc_wap3(df):\n    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap \n\n# Function to calculate fourth WAP\ndef calc_wap4(df):\n    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap  \n\n# Function to calculate the log of the return\n# Remember that logb(x \/ y) = logb(x) - logb(y)\ndef log_return(series):\n    return np.log(series).diff()\n\n# Calculate the realized volatility\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\n# Function to count unique elements of a series\ndef count_unique(series):\n    return len(np.unique(series))\n\n#Function to calculate the range of the series\ndef spread(series):\n    return np.max(series) - np.min(series)\n\ndef sum_square(series):\n    return np.sum(series ** 2)\n\ndef imr(series):\n    return np.percentile(series.values,75) - np.percentile(series.values,25)\n\n#function to calculate tendency\ndef tendency(price, vol):    \n    df_diff = np.diff(price)\n    val = (df_diff\/price[1:])*100\n    power = np.sum(val*vol[1:])\n    return(power)\n\n# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\n# Function to calculate the root mean squared percentage error for NN model\ndef root_mean_squared_per_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square( (y_true - y_pred)\/ y_true )))\n\n# Function to early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\n\n# Function to preprocess book data (for each stock id)\ndef book_preprocessor(file_path, stock_id, seconds_in_bucket_groups, book_feature_dict, book_feature_dict_time):\n    df = pd.read_parquet(file_path)\n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    df['wap3'] = calc_wap3(df)\n    df['wap4'] = calc_wap3(df)\n    # Calculate log returns\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) \/ ((df['ask_price2'] + df['bid_price2']) \/ 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket):\n        # Group by the window\n        if seconds_in_bucket:\n            df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(book_feature_dict_time).reset_index()\n        else:\n            df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(book_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) + '_' + str(seconds_in_bucket) if col[1] else col[0] for col in df_feature.columns]\n        return df_feature\n    \n    # Get and merge the stats for different windows\n    df_features = [get_stats_window(seconds_in_bucket = second) for second in seconds_in_bucket_groups]\n    df_feature = reduce(lambda  left,right: pd.merge(left,right, on='time_id', how='left'), df_features)\n    df_feature['stock_id'] = stock_id\n    return df_feature\n\n# Function to preprocess trade data (for each stock id)\ndef trade_preprocessor(file_path, stock_id, seconds_in_bucket_groups, trade_feature_dict, trade_feature_dict_time, seconds_in_bucket_groups2):\n    df = pd.read_parquet(file_path)\n    df['trade_log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    df['amount']=df['price']*df['size']\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(trade_feature_dict_time).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) + '_' + str(seconds_in_bucket) if col[1] else col[0] for col in df_feature.columns]\n        return df_feature\n    \n    # Get and merge the stats for different windows\n    df_features = [get_stats_window(seconds_in_bucket = second) for second in seconds_in_bucket_groups2]\n    df_feature = reduce(lambda  left,right: pd.merge(left,right,on='time_id', how='left'), df_features)\n    #add size_tau for each buckets\n    for second in seconds_in_bucket_groups2:\n        df_feature['size_tau_'+str(second)] = np.sqrt(1\/ df_feature['seconds_in_bucket_count_unique_'+str(second)])\n        df_feature['size_tau2_'+str(second)] = np.sqrt(((600 - second) \/ 600) \/ df_feature['order_count_sum_'+str(second)])\n        if second == 400:\n            df_feature['size_tau2_d_' + str(second)] = df_feature['size_tau2_'+ str(second)] - df_feature['size_tau2_' + '0']\n        \n    #create additional features\n    def additional_features(df):\n        lis = []\n        for n_time_id in df['time_id'].unique():\n            df_id = df[df['time_id'] == n_time_id]        \n            tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n            f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n            f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n            df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n            df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n            # new\n            abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n            energy = np.mean(df_id['price'].values**2)\n            iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n            # vol vars\n            abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n            energy_v = np.sum(df_id['size'].values**2)\n            iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n            lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                       'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n        return pd.DataFrame(lis)\n    df = df.merge(additional_features(df), on='time_id', how='left')\n    df_feature['stock_id'] = stock_id\n    return df_feature\n\n# Function to get group stats for the stock_id and time_id\ndef get_time_stock(df, seconds_in_bucket_groups, vol_groups):\n    # Get realized volatility columns\n    vol_cols = [ret + '_realized_volatility_' + str(second) for ret in ['log_return1', 'log_return2', 'trade_log_return'] \n                for second in seconds_in_bucket_groups]\n    \n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(vol_groups).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) + '_stock' if col[1] else col[0] for col in df_stock_id.columns]\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(vol_groups).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) + '_time' if col[1] else col[0] for col in df_time_id.columns]\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', on = ['stock_id'])\n    df = df.merge(df_time_id, how = 'left', on = ['time_id'])\n    return df\n\n#quantile_transform\ndef quantile_transform(train, test, quantiles_no):\n    colNames = list(train)\n    cols_remove = ['time_id', 'target', 'row_id', 'stock_id']\n    for col in cols_remove:\n        colNames.remove(col)\n    train.replace([np.inf, -np.inf], np.nan,inplace=True)\n    test.replace([np.inf, -np.inf], np.nan,inplace=True)\n    for col in colNames:\n        qt = QuantileTransformer(random_state=seed_no, n_quantiles=quantiles_no, output_distribution='normal')\n        train[col] = qt.fit_transform(train[[col]])\n        test[col] = qt.transform(test[[col]])\n    return train, test\n\n#make aggregate features by Kmeans\ndef aggregate_features(train, test, selected_features, selected_second_buckets, selected_clusters, clusters_no = 7):\n    train_p = pd.read_csv(PATH + 'train.csv')\n    train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n    corr = train_p.corr()\n    ids = corr.index\n    kmeans = KMeans(n_clusters=clusters_no, random_state=0).fit(corr.values)\n    l = []\n    for n in range(7):\n        l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0])\n    mat = []\n    matTest = []\n\n    n = 0\n    for ind in l:\n        newDf = train.loc[train['stock_id'].isin(ind) ]\n        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n        newDf.loc[:,'stock_id'] = str(n)+'c1'\n        mat.append(newDf)\n\n        newDf = test.loc[test['stock_id'].isin(ind) ]    \n        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n        newDf.loc[:,'stock_id'] = str(n)+'c1'\n        matTest.append(newDf)\n        n+=1\n\n    mat1 = pd.concat(mat).reset_index()\n    mat1.drop(columns=['target'],inplace=True)\n    mat2 = pd.concat(matTest).reset_index()\n    \n    matTest = []\n    mat = []\n    kmeans = []\n    #mat2 #= mat1.pivot(index='time_id', columns='stock_idmat2\n    mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n    \n    mat1 = mat1.pivot(index='time_id', columns='stock_id')\n    mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n    mat1.reset_index(inplace=True)\n\n    mat2 = mat2.pivot(index='time_id', columns='stock_id')\n    mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n    mat2.reset_index(inplace=True)\n    \n    selected_features_groups = ['time_id'] + [f'{feature}_{second}_{cluster}c1' for feature in selected_features for second in selected_second_buckets \n                         for cluster in selected_clusters]\n\n    train = pd.merge(train,mat1[selected_features_groups],how='left',on='time_id')\n    test = pd.merge(test,mat2[selected_features_groups],how='left',on='time_id')\n    del mat1,mat2\n    gc.collect()\n    return train, test\n    \n# Funtion to make preprocessing function in parallel (for each stock id)\ndef preprocessor(list_stock_ids, seconds_in_bucket_groups, book_feature_dict, trade_feature_dict, is_train = True):\n    # Parrallel for loop\n    def for_joblib(stock_id):\n        df_name = 'train' if is_train else 'test'\n        file_path_book = PATH + f'book_{df_name}.parquet\/stock_id=' + str(stock_id)\n        file_path_trade = PATH + f'trade_{df_name}.parquet\/stock_id=' + str(stock_id)\n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book, stock_id, seconds_in_bucket_groups, book_feature_dict, book_feature_dict_time), \n                          trade_preprocessor(file_path_trade, stock_id, seconds_in_bucket_groups, trade_feature_dict, trade_feature_dict_time, seconds_in_bucket_groups2),\n                          on = ['stock_id', 'time_id'], how = 'left')\n        return df_tmp\n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    return df\n\n# Function to read our base train and test set\ndef read_data(PATH, is_train = True):\n    df_name = 'train' if is_train else 'test'\n    df = pd.read_csv(PATH + f'{df_name}.csv')\n    # Create a key to merge with book and trade data\n    df['row_id'] = df['stock_id'].astype(str) + '-' + df['time_id'].astype(str)\n    print(f'Our {df_name}ing set has {df.shape[0]} rows')\n    return df\n\ndef prepare_data(is_debug = False, add_aggregate_features = False):\n    train, test = read_data(PATH, True), read_data(PATH, False)\n    if add_aggregate_features:\n        stock_list = [1, 0, 3, 27, 2, 81, 8]\n    else:\n        stock_list = [0]\n    if is_debug:\n        train, test = train[train['stock_id'].isin(stock_list)], test[test['stock_id'] == 0]\n        train_stock_ids, test_stock_ids = stock_list, [0]\n    else:\n        train_stock_ids, test_stock_ids = train['stock_id'].unique(), test['stock_id'].unique()\n    train_ = preprocessor(train_stock_ids, seconds_in_bucket_groups, book_feature_dict, trade_feature_dict, is_train = True)\n    test_ = preprocessor(test_stock_ids, seconds_in_bucket_groups, book_feature_dict, trade_feature_dict, is_train = False)\n    train = train.merge(train_, on = ['stock_id', 'time_id'], how = 'left')\n    test = test.merge(test_, on = ['stock_id', 'time_id'], how = 'left')\n    train = get_time_stock(train, seconds_in_bucket_groups2, vol_groups)\n    test = get_time_stock(test, seconds_in_bucket_groups2, vol_groups)\n    return train, test","eb296621":"## NN model\ndef NN_model(train, test, nfolds, no_quantile, hidden_units, dropout_rate, stock_embedding_size, callbacks, seed_no, learning_rate, cat_variables, aggregate_parameters, \n             is_quan_trans = False, add_aggregate_features = False):\n    # kfold based on the knn++ algorithm\n    def k_fold(nfolds):\n        out_train = pd.read_csv(PATH + 'train.csv')\n        out_train = out_train.pivot(index='time_id', columns='stock_id', values='target')\n        out_train = out_train.fillna(out_train.mean())\n        # code to add the just the read data after first execution\n        # data separation based on knn ++\n        index, totDist, values = [], [], []\n        # generates a matriz with the values of \n        mat = MinMaxScaler(feature_range=(-1, 1)).fit_transform(out_train.values)\n        # adds index in the last column\n        mat = np.c_[mat,np.arange(mat.shape[0])]\n        nind = int(mat.shape[0]\/nfolds) # number of individuals\n        lineNumber = np.random.choice(np.array(mat.shape[0]), size=nfolds, replace=False)\n        lineNumber = np.sort(lineNumber)[::-1]\n        for n in range(nfolds):\n            totDist.append(np.zeros(mat.shape[0]-nfolds))\n            values.append([lineNumber[n]])     \n        s=[]\n        for n in range(nfolds):\n            s.append(mat[lineNumber[n],:])\n            mat = np.delete(mat, obj=lineNumber[n], axis=0)\n        for n in range(nind-1):    \n            luck = np.random.uniform(0,1,nfolds)\n            for cycle in range(nfolds):\n                 # saves the values of index           \n                s[cycle] = np.matlib.repmat(s[cycle], mat.shape[0], 1)\n                sumDist = np.sum((mat[:,:-1] - s[cycle][:,:-1])**2 , axis=1)   \n                totDist[cycle] += sumDist        \n                # probabilities\n                f = totDist[cycle]\/np.sum(totDist[cycle]) # normalizing the totdist\n                j = 0\n                kn = 0\n                for val in f:\n                    j += val        \n                    if (j > luck[cycle]): # the column was selected\n                        break\n                    kn +=1\n                lineNumber[cycle] = kn\n                # delete line of the value added    \n                for n_iter in range(nfolds):\n                    totDist[n_iter] = np.delete(totDist[n_iter],obj=lineNumber[cycle], axis=0)\n                    j= 0\n                s[cycle] = mat[lineNumber[cycle],:]\n                values[cycle].append(int(mat[lineNumber[cycle],-1]))\n                mat = np.delete(mat, obj=lineNumber[cycle], axis=0)\n        for n_mod in range(nfolds):\n            values[n_mod] = out_train.index[values[n_mod]]\n        return values\n    \n    def base_model(no_of_col, hidden_units, stock_embedding_size):\n        # Each instance will consist of two inputs: a single user id, and a single movie id\n        stock_id_input = keras.Input(shape=(1,), name='stock_id')\n        num_input = keras.Input(shape=(no_of_col,), name='num_data')\n        #embedding, flatenning and concatenating\n        stock_embedded = keras.layers.Embedding(max(train['stock_id'])+1, stock_embedding_size, \n                                               input_length=1, name='stock_embedding')(stock_id_input)\n        stock_flattened = keras.layers.Flatten()(stock_embedded)\n        out = keras.layers.Concatenate()([stock_flattened, num_input])\n        # Add one or more hidden layers\n        for n_hidden in hidden_units:\n            out = keras.layers.Dense(n_hidden, activation='swish')(out)\n            out = keras.layers.Dropout(dropout_rate)(out)\n        # A single output: our predicted rating\n        out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n        model = keras.Model(inputs = [stock_id_input, num_input],outputs = out)\n        return model\n    \n    values = k_fold(nfolds)\n    if is_quan_trans:\n        train, test, quantile_transform(train, test, quantiles_no = no_quantile)\n    if add_aggregate_features:\n        selected_features, selected_second_buckets, selected_clusters = aggregate_parameters\n        train, test = aggregate_features(train, test, selected_features, selected_second_buckets, selected_clusters)\n\n    train_scores_folds, val_scores_folds = [], []\n    features_to_consider = list(train)\n    if 'time_id' not in cat_variables:\n        features_to_consider.remove('time_id')\n    features_to_consider.remove('target')\n    features_to_consider.remove('row_id')\n\n    train[features_to_consider] = train[features_to_consider].fillna(train[features_to_consider].mean()).fillna(0)\n    test[features_to_consider] = test[features_to_consider].fillna(train[features_to_consider].mean()).fillna(0)\n    y = train['target']\n    \n    no_of_col = len(list(train)) - 4\n    # Create out of folds array\n    oof_predictions = np.zeros(train.shape[0])\n    \n    test_predictions = np.zeros(test.shape[0])\n    for n_count in range(nfolds):\n        print('CV {}\/{}'.format(n_count + 1, nfolds))\n        indexes = np.arange(nfolds).astype(int)    \n        indexes = np.delete(indexes,obj=n_count, axis=0)\n        indexes = np.r_[tuple([values[indexes[i]] for i in range(nfolds - 1)])]\n\n        X_train = train.loc[train.time_id.isin(indexes), features_to_consider]\n        y_train = train.loc[train.time_id.isin(indexes), 'target']\n        X_val = train.loc[train.time_id.isin(values[n_count]), features_to_consider]\n        y_val = train.loc[train.time_id.isin(values[n_count]), 'target']\n\n        model = base_model(no_of_col, hidden_units, stock_embedding_size)\n        model.compile(\n            keras.optimizers.Adam(learning_rate=learning_rate),\n            loss=root_mean_squared_per_error\n        )\n        for cat in cat_variables:\n            features_to_consider.remove(cat)\n        num_data = X_train[features_to_consider]\n        scaler = MinMaxScaler(feature_range=(-1, 1))         \n        num_data = scaler.fit_transform(num_data.values)    \n        cat_data = X_train[cat_variables]    \n        target =  y_train\n        num_data_val = X_val[features_to_consider]\n        num_data_val = scaler.transform(num_data_val.values)\n        cat_data_val = X_val[cat_variables]\n        model.fit([cat_data, num_data], \n                  target,               \n                  batch_size=1024,\n                  epochs=1000,\n                  validation_data=([cat_data_val, num_data_val], y_val),\n                  callbacks=[es, plateau],\n                  validation_batch_size=len(y_val),\n                  shuffle=True,\n                  verbose = 0)\n        #training scores\n        train_preds = model.predict([cat_data, num_data]).reshape(1,-1)[0]\n        train_score = rmspe(y_true = y_train, y_pred = train_preds)\n        train_scores_folds.append(train_score)\n        #validating scores\n        val_preds = model.predict([cat_data_val, num_data_val]).reshape(1,-1)[0]\n        oof_predictions[train.time_id.isin(values[n_count])] = val_preds\n        val_score = rmspe(y_true = y_val, y_pred = val_preds)\n        val_scores_folds.append(val_score)\n        print(f'Fold {n_count + 1}: trainscore: {train_score} valscore: {val_score}')\n        #test\n        tt =scaler.transform(test[features_to_consider].values)\n        test_predictions += model.predict([test['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10) \/ nfolds\n        for cat in cat_variables:\n            features_to_consider.append(cat)\n    rmspe_val_score = rmspe(y, oof_predictions)\n    print(f'Training RMSPE: {np.mean(train_scores_folds)} Out of folds RMSPE is {rmspe_val_score}')\n    return  train_scores_folds, val_scores_folds, rmspe_val_score, test_predictions","ca95e69c":"#lightgbm model\ndef lightgbm_model(train, test, nfolds, no_quantile, params, seed_no, aggregate_parameters, is_quan_trans = False, add_aggregate_features = False):\n    if is_quan_trans:\n        train, test = quantile_transform(train, test, quantiles_no = no_quantile)\n    if add_aggregate_features:\n        selected_features, selected_second_buckets, selected_clusters = aggregate_parameters\n        train, test = aggregate_features(train, test, selected_features, selected_second_buckets, selected_clusters)\n    # Split features and target\n    x = train.drop(['row_id', 'target', 'time_id'], axis = 1)\n    y = train['target']\n    x_test = test.drop(['row_id', 'time_id'], axis = 1)\n    # Transform stock id to a numeric value\n    x['stock_id'] = x['stock_id'].astype(int)\n    x_test['stock_id'] = x_test['stock_id'].astype(int)\n    # Create out of folds array\n    oof_predictions = np.zeros(x.shape[0])\n    # Create test array to store predictions\n    test_predictions = np.zeros(x_test.shape[0])\n    # Create a KFold object\n    kfold = KFold(n_splits = nfolds, random_state = seed_no, shuffle = True)\n    # Iterate through each fold\n    train_scores_folds, val_scores_folds = [], []\n    for fold, (trn_ind, val_ind) in enumerate(kfold.split(x)):\n        print(f'Training fold {fold + 1}')\n        x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        # Root mean squared percentage error weights\n        train_weights = 1 \/ np.square(y_train)\n        val_weights = 1 \/ np.square(y_val)\n        train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights, categorical_feature = ['stock_id'])\n        val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights, categorical_feature = ['stock_id'])\n        model = lgb.train(params = params, \n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, val_dataset], \n                          num_boost_round = 1000, \n                          early_stopping_rounds = 100, \n                          verbose_eval = 250,\n                          feval = feval_rmspe)\n\n        # Add predictions to the out of folds array\n        train_preds = model.predict(x_train)\n        train_score = rmspe(y_true = y_train, y_pred = train_preds)\n        train_scores_folds.append(train_score)\n        \n        val_preds = model.predict(x_val)\n        val_score = rmspe(y_true = y_val, y_pred = val_preds)\n        val_scores_folds.append(val_score)\n        oof_predictions[val_ind] = val_preds\n        print(f'Fold {fold + 1}: trainscore: {train_score} valscore: {val_score}')\n        # Predict the test set\n        test_predictions += model.predict(x_test) \/ nfolds\n    #plot importance\n    plt.figure(figsize=(12,6))\n    lgb.plot_importance(model, max_num_features=20)\n    plt.title(\"Feature importance\")\n    plt.show()\n    rmspe_val_score = rmspe(y, oof_predictions)\n    print(f'Training RMSPE: {np.mean(train_scores_folds)} Out of folds RMSPE is {rmspe_val_score}')\n    return train_scores_folds, val_scores_folds, rmspe_val_score, test_predictions","55f80144":"#Parameters for features engineering\nPATH = '..\/input\/optiver-realized-volatility-prediction\/'\n# Book for aggregations\nbook_feature_dict = {\n        'wap1': [np.sum, np.std],\n        'wap2': [np.sum, np.std],\n        'wap3': [np.sum, np.std],\n        'wap4': [np.sum, np.std],\n        'log_return1': [realized_volatility],\n        'log_return2': [realized_volatility],\n        'log_return3': [realized_volatility],\n        'log_return4': [realized_volatility],\n        'wap_balance': [np.sum, np.max],\n        'price_spread':[np.sum, np.max],\n        'price_spread2':[np.sum, np.max],\n        'bid_spread':[np.sum, np.max],\n        'ask_spread':[np.sum, np.max],\n        'total_volume':[np.sum, np.max],\n        'volume_imbalance':[np.sum, np.max],\n        \"bid_ask_spread\":[np.sum, np.max],\n}\nbook_feature_dict_time = {\n        'log_return1': [realized_volatility],\n        'log_return2': [realized_volatility],\n        'log_return3': [realized_volatility],\n        'log_return4': [realized_volatility],\n    }\n\n# Trade for aggregations\ntrade_feature_dict = {\n    'trade_log_return':[realized_volatility],\n    'seconds_in_bucket':[count_unique],\n    'size':[np.sum, spread],\n    'order_count':[np.sum, spread],\n    'amount':[np.sum, spread],\n}\ntrade_feature_dict_time = {\n    'trade_log_return':[realized_volatility],\n    'seconds_in_bucket':[count_unique],\n    'size':[np.sum],\n    'order_count':[np.sum],\n}\n\nvol_groups = ['mean', 'std', 'min', 'max']\n\nseconds_in_bucket_groups = [0, 100, 200, 300, 400, 500]\n\nseconds_in_bucket_groups2 = [0, 200, 300, 400]\n\nselected_features = ['log_return1_realized_volatility',\n                     'total_volume_sum', 'size_sum', 'order_count_sum', 'price_spread_sum',\n                    'bid_spread_sum', 'ask_spread_sum', 'volume_imbalance_sum', 'bid_ask_spread_sum', 'size_tau2']\n#                      , 'trade_log_return_realized_volatility', 'seconds_in_bucket_count_unique', 'size_sum', 'order_count_mean']\n\nselected_second_buckets = [0]\n\nselected_clusters = [0, 1, 3, 4, 6]\n\naggregate_parameters = [selected_features, selected_second_buckets, selected_clusters]\n\nis_debug = False\nis_quan_trans = True\nadd_aggregate_features = True\ntrain, test = prepare_data(is_debug = is_debug, add_aggregate_features = add_aggregate_features)","077d5467":"nfolds = 10\nseed_no = 1111\nno_quantile = 500\nhidden_units = (256,128,64)\nstock_embedding_size = 24\n\nseed(seed_no)\ntf.random.set_seed(seed_no)\n\nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=100, verbose=0,\n    mode='min', restore_best_weights=True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.2, patience=7, verbose=0,\n    mode='min')\n\nlearning_rate = 0.005\n\ndropout_rate = 0\n\ncallbacks = [es, plateau]\n\ncat_variables = ['stock_id']\n\ntrain_scores_folds, scores_folds, rmspe_score, test_predictions = NN_model(train, test, nfolds, no_quantile, \n                     hidden_units, dropout_rate, stock_embedding_size, callbacks, seed_no, learning_rate, cat_variables, aggregate_parameters,\n                    is_quan_trans = is_quan_trans, add_aggregate_features = add_aggregate_features)","591e9e28":"params = {\n    'learning_rate': 0.05,        \n    'lambda_l1': 2,\n    'lambda_l2': 7,\n    'num_leaves': 800,\n    'min_sum_hessian_in_leaf': 20,\n    'feature_fraction': 0.8,\n    'feature_fraction_bynode': 0.8,\n    'bagging_fraction': 0.9,\n    'bagging_freq': 42,\n    'min_data_in_leaf': 700,\n    'max_depth': 4,\n    'seed': seed_no,\n    'feature_fraction_seed': seed_no,\n    'bagging_seed': seed_no,\n    'drop_seed': seed_no,\n    'data_random_seed': seed_no,\n    'objective': 'rmse',\n    'boosting': 'gbdt',\n    'verbosity': -1,\n    'n_jobs': -1,\n}  \nparams0 = {\n    'objective': 'rmse',\n    'boosting_type': 'gbdt',\n    'max_depth': -1,\n    'max_bin':100,\n    'min_data_in_leaf':500,\n    'learning_rate': 0.05,\n    'subsample': 0.72,\n    'subsample_freq': 4,\n    'feature_fraction': 0.5,\n    'lambda_l1': 0.5,\n    'lambda_l2': 1.0,\n    'categorical_column':[0],\n    'seed':seed_no,\n    'feature_fraction_seed': seed_no,\n    'bagging_seed': seed_no,\n    'drop_seed': seed_no,\n    'data_random_seed': seed_no,\n    'n_jobs':-1,\n    'verbose': -1}\n\ntrain_scores_folds2, val_scores_folds2, rmspe_val_score2, test_predictions2 = lightgbm_model(train, test, nfolds, no_quantile, params0, seed_no,aggregate_parameters,\n                                                                            is_quan_trans = False, add_aggregate_features = True)","0e80ec8d":"def predict(test_predictions, test_predictions2, weight = False, rmspe_val_score = None, rmspe_val_score2 = None):\n    if weight:\n        rmspe_val_score = 1 \/ rmspe_val_score\n        rmspe_val_score2 = 1 \/ rmspe_val_score2\n        return (rmspe_val_score * test_predictions + rmspe_val_score2 * test_predictions2)\/(rmspe_val_score + rmspe_val_score2)\n    else:\n        return (test_predictions + test_predictions2)\/2\ntest['target'] = predict(test_predictions, test_predictions2, weight = False, rmspe_val_score = rmspe_score, rmspe_val_score2 = rmspe_val_score2)","7ee6d2aa":"test[['row_id', 'target']].to_csv('submission.csv',index = False)","d5481c9b":"'''\nLog: Add aggregate features in KNN only, [0, 200, 300 ,400], no_quantile = 500, add trade spread\nTrial20:Training KNN:0.2001754207356333,  lgbm:0.18897182891288467   KNN:0.2112148779306615,  lgbm:0.20338421694021552,  Public Score: 0.20023\nLog: Modified based on new kernal, selected_clusters = [0, 1, 2, 3, 4]\nTrial24:Training KNN:0.20202140031573315, lgbm:0.17893973303582508   KNN:0.2102177518533855,  lgbm:0.19493261465281048,  Public Score: 0.19969\nLog: selected_clusters = [0, 1, 2, 3, 4, 5, 6]\nTrial25:Training KNN:0.20056520864481855, lgbm:0.17862442156084907   KNN:0.21080988078150428, lgbm:0.19457209713111165,  Public Score: 0.19986\nLog: add wap3 and wap4 and related log return\nTrial26:Training KNN:0.20181446860339824, lgbm:0.17980917806218497   KNN:0.20995577500704568, lgbm:0.1949107281631906,   Public Score: 0.19904\nLog: add wap3 and wap4 and related log return, selected_clusters = [0, 1, 3, 4, 6], no spread\nTrial27:Training KNN:0.20221566413103273, lgbm:0.17897978725266786   KNN:0.21066830921449906, lgbm:0.19478845606184658,  Public Score: 0.19863\nlog: size_tau2_d only 400\nTrial29:Training KNN:0.2018674377866964,  lgbm:0.17918047277983357   KNN:0.2101467689834155,  lgbm:0.19501864463388588,  Public Score: 0.19880\n'''\n\n'''\nFast Trial\nLog: Add aggregate features in KNN only, [0, 200, 300 ,400], no_quantile = 500, add trade spread\nTrial20:Training KNN:0.21477605084969267,  lgbm:0.2112767456735686   KNN:0.23647947146162088, lgbm:0.232755165366543,    Public Score: 0.20023\nLog: Modified based on new kernal, selected_clusters = [0, 1, 2, 3, 4]\n        Training KNN:0.21734438390621938,  lgbm:0.17682403414225137  KNN:0.23179466773975121, lgbm:0.22290430972939718,  Public Score: 0.19969\nLog: selected_clusters = [0, 1, 2, 3, 4, 5, 6]\n        Training KNN:0.21528366042125913,  lgbm:0.17623016138257902  KNN:0.23120720093921826, lgbm:0.22185254969975954\nLog: seconds_in_bucket_groups = [0, 100, 200, 300, 400, 500] - > [0, 200, 300, 400]\n        Training KNN:0.2149028630710367,   lgbm:0.1755718158339396   KNN:0.2332897817467045,  lgbm:0.2218928427951056\nLog: add wap3 and wap4 and related log return\n        Training KNN:0.2154105796076588,   lgbm:0.1746446745025871   KNN:0.23236514724791815, lgbm:0.22147884880810506\nLog: change book max to spread\n        Training KNN:0.21754382246940768,  lgbm:0.1787300836034825   KNN:0.23278519803416653, lgbm:0.2249218172831521\nLog: lbgm add_aggregate_features = False, add wap3 and wap4 and related log return\n        Training KNN:0.2181941671317213,   lgbm:0.18681903260555696  KNN:0.23353375861830414, lgbm:0.22792100702016171\nLog: add wap3 and wap4 and related log return, selected_clusters = [0, 1, 3, 4, 6]\nTrial27:Training KNN:0.21642869439818804,  lgbm:0.1742123982976963   KNN:0.2318034023014281,  lgbm:0.2226242440767707,   Public Score: 0.19863\nlog: get time stock add logreturn3 and 4 (reversed)\n        Training KNN:0.21836248916538498,  lgbm:0.17416614843171563  KNN:0.23257533999906102, lgbm:0.22303301056885588\nlog: selected_features add ['log_return2_realized_volatility', 'trade_return_realized_volatility'] (reversed)\n        Training KNN:0.21906564563645076,  lgbm:0.17243249914765277  KNN:0.23425487932426461, lgbm:0.222073366135514\nlog: selected_clusters = [0, 1, 2, 3, 4, 5, 6]\n        Training KNN:0.2176840245308933,   lgbm:0.17674364914662366  KNN:0.23292574600788693, lgbm:0.2225863285558057\nlog: no_quantile 500 -> 2000\n        Training KNN:0.2159818277245494,   lgbm:0.17857807833774592  KNN:0.23232437011450735, lgbm:0.22285963617022936\nlog: book feature delete logreturn3 and 4\n        Training KNN:0.216430252174648,    lgbm:0.1787822359316717   KNN:0.23318504064283288, lgbm:0.2229118038742619\nlog: trade size and amount replace min max by spread (same as min max)\n        Training KNN:0.21642869439818804,  lgbm:0.1742123982976963   KNN:0.2318034023014281,  lgbm:0.2226242440767707,   Public Score: 0.19863\nlog: order_count max -> spread (same as max)\n        Training KNN:0.21642869439818804,  lgbm:0.1742123982976963   KNN:0.2318034023014281,  lgbm:0.2226242440767707,   Public Score: 0.19863\nlog: trade_feature_dict_time add amount sum (reversed)\n        Training KNN:0.21841185989276046,  lgbm:0.1779921805085564   KNN:0.23424890396258807, lgbm:0.2234330406640632\nlog: vol_groups min max -> spread (reversed)\n        Training KNN:0.218744304158386,    lgbm:0.17953000379961834  KNN:0.2334280726091211,  lgbm:0.2237829243514391\nlog: vol_groups add spread\n        Training KNN:0.21701558180105923,  lgbm:0.17572965238162686  KNN:0.23157669125800778, lgbm:0.22317242219920877\nlog: size_tau2_d only 400\n        Training KNN:0.2176399078888894,   lgbm:0.17892425974989284  KNN:0.22989670347607916, lgbm:0.22360151758606328,  Public Score: 0.19880      \nlog: book_feature_dict_time add 'volume_imbalance':[np.sum] (reversed)\n        Training KNN:0.21901899204719757,  lgbm:0.17634415875366674  KNN:0.23469634182000773, lgbm:0.2228152130397425  \nlog: 'volume_imbalance' add std\n        Training KNN:0.2185474348107188,   lgbm:0.17465596873417216  KNN:0.23380060015120852, lgbm:0.22252130655681454 \nlog: early stopping, 50 -> 100\n        Training KNN:0.2176399078888894,   lgbm:0.171026447690265    KNN:0.22989670347607916, lgbm:0.22298375912867413 \nhidden_units = (256,128,64)\n        Training KNN:0.21648116527683733,  lgbm:0.17892425974989284  KNN:0.22947559570636517, lgbm:0.22360151758606328   \n'''\nTraining RMSPE: 0.21648116527683733 Out of folds RMSPE is 0.22947559570636517","27a3331b":"# Acknowledgements\n\nStock Embedding - FFNN - upgrade & 3D\nhttps:\/\/www.kaggle.com\/vbmokin\/stock-embedding-ffnn-upgrade-3d\n\nStock Embedding - FFNN - My features\nhttps:\/\/www.kaggle.com\/alexioslyon\/stock-embedding-ffnn-my-features\n\nlgbm baseline\nhttps:\/\/www.kaggle.com\/alexioslyon\/lgbm-baseline","6552a720":"# Training model and making predictions","beb3fc8b":"I have reorganised all the codes for my own convenience in modifying, but the ideas are mostly based on the above kagglers. Thank you to all of you inspried me a lot."}}