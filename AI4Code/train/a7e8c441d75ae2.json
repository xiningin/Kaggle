{"cell_type":{"11172c46":"code","c5844196":"code","9e05b001":"code","2ca0891b":"code","30e69b54":"code","f6817f4b":"code","8f8bb62a":"code","b7da60c0":"code","c126f312":"code","d17ffeb2":"code","75e23c12":"code","0e9943a9":"code","3f058003":"code","29de5df6":"code","995c82ae":"code","31ee8d7f":"code","d1f47d25":"code","64805d21":"code","68e116c5":"code","3cf54877":"code","76757a95":"code","944815e6":"code","77e0b15b":"code","5efdb0dc":"code","2cfdb4f6":"code","c064ea63":"code","115d2f82":"code","bca02266":"code","7cc195db":"code","6cf17fc7":"code","0f179c7a":"code","90b78458":"code","8c6b68fd":"code","0007491d":"code","5fcbcd6c":"code","30d7726e":"code","7767152d":"code","f14d7039":"code","ea9d2749":"code","35f8b307":"code","f8717a0b":"code","5d381371":"code","fe75e27e":"code","26a60dd6":"code","c3b26a82":"code","dfe1843b":"code","50271a7f":"code","55bd5c46":"code","84db25ed":"code","1349f2cf":"code","1277fe69":"code","697a7824":"code","f9fb5816":"code","1f48a6f1":"code","5955ad25":"code","278f2c6d":"code","94965e46":"code","19dca784":"code","b60841f5":"code","3f3de4a6":"code","98230112":"code","f6ed144f":"code","201dad9a":"markdown"},"source":{"11172c46":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom collections import Counter\n\nimport os\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_colwidth',500)\npd.set_option('display.max_columns',5000)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c5844196":"#reading files\ntrain = pd.read_csv('\/kaggle\/input\/train.csv')\ncoupons = pd.read_csv('\/kaggle\/input\/coupon_item_mapping.csv')\ncampaign = pd.read_csv('\/kaggle\/input\/campaign_data.csv')\ncust_tran = pd.read_csv('\/kaggle\/input\/customer_transaction_data.csv')\ncust_demo = pd.read_csv('\/kaggle\/input\/customer_demographics.csv')\nitems = pd.read_csv('\/kaggle\/input\/item_data.csv')\n\ntest = pd.read_csv('\/kaggle\/input\/test_QyjYwdj.csv')\nsample = pd.read_csv('\/kaggle\/input\/sample_submission_Byiv0dS.csv')","9e05b001":"train.head()","2ca0891b":"########################### Campaign #############################","30e69b54":"campaign.head()","f6817f4b":"#todatetime\ncampaign['start_date'] = pd.to_datetime(campaign['start_date'], format = '%d\/%m\/%y')\ncampaign['end_date'] = pd.to_datetime(campaign['end_date'], format = '%d\/%m\/%y')","8f8bb62a":"#adding campaign type to train and test\ntrain['campaign_type'] = train.campaign_id.map(campaign.groupby('campaign_id').campaign_type.apply(lambda x: x.unique()[0]))\ntest['campaign_type'] = test.campaign_id.map(campaign.groupby('campaign_id').campaign_type.apply(lambda x: x.unique()[0]))","b7da60c0":"############################ Customer demographics ##############################","c126f312":"cust_demo.head()","d17ffeb2":"#type of family size, no of children = int64\ncust_demo['family_size'] = cust_demo.family_size.apply(lambda x: int(re.sub('\\+','',x)))\ncust_demo['no_of_children'] = cust_demo.no_of_children.apply(lambda x: float(re.sub('\\+','',x)) if pd.notna(x) else x)","75e23c12":"#Filling nans marital_status\n\n#customers with family size =1 will be single\ncust_demo.loc[pd.isnull(cust_demo.marital_status) & (cust_demo.family_size == 1),'marital_status'] = 'Single'\n\n#customers whos fam size - no of childrens == 1, will also be single\ncust_demo.loc[(cust_demo.family_size - cust_demo.no_of_children == 1) & pd.isnull(cust_demo.marital_status),'marital_status'] = 'Single'\n\n#from the orignal data we have 142 of 152 customers with diff of 2 in their fam size and #childrens are Married\ncust_demo.loc[(pd.isnull(cust_demo.marital_status)) & ((cust_demo.family_size - cust_demo.no_of_children) == 2)  & (pd.notnull(cust_demo.no_of_children)),'marital_status'] = 'Married'\n\n#original data shows customers with fam size == 2, and nans in no of childrens are majorly Married\ncust_demo.loc[pd.isnull(cust_demo.marital_status) & (pd.isnull(cust_demo.no_of_children)) & (cust_demo.family_size ==2),'marital_status'] = 'Married'","0e9943a9":"#Filling nans in no of children\n\n#Married people with family_size ==2 will have 0 childrens\ncust_demo.loc[pd.isnull(cust_demo.no_of_children) & (cust_demo.marital_status == 'Married') & (cust_demo.family_size == 2),'no_of_children'] = 0.0\n\n#customers with family size 1 will have zero childrens\ncust_demo.loc[pd.isnull(cust_demo.no_of_children) & (cust_demo.family_size == 1), 'no_of_children'] = 0.0\n\n#singles with family size == 2, will probably have 1 child\ncust_demo.loc[pd.isnull(cust_demo.no_of_children) & (cust_demo.family_size == 2),'no_of_children'] = 1.0","3f058003":"############################ Customer transactions ############################","29de5df6":"cust_tran.head()","995c82ae":"#to datetime\ncust_tran['date'] = pd.to_datetime(cust_tran['date'])","31ee8d7f":"############################# common ############################### ","d1f47d25":"#merging train and test with cust_demo on campaign_id\ntrain = pd.merge(train,cust_demo, on='customer_id', how='left')\ntest = pd.merge(test,cust_demo, on='customer_id', how='left')","64805d21":"train.head()","68e116c5":"# \"bought_X_vailable\" =  Intersection between Items bought by customer previously(from cust_tran) and all items available in coupon provided(from coupons)","3cf54877":"#cust2items - dictionary mapping customer_ids to all items bought by them\ncust_tran['str_item'] = cust_tran.item_id.apply(lambda x: str(x)) #did this to calculate d_cust2items, no need further\nd_cust2items = cust_tran.groupby('customer_id').str_item.apply(lambda x: ' '.join(x)).to_dict()\ncust_tran.drop('str_item',axis=1,inplace=True)","76757a95":"#coupon2items - dictionary mapping coupon_ids to all items under them\nd_coupon2items = coupons.groupby('coupon_id').item_id.apply(lambda x: ' '.join(list(x.apply(lambda x: str(x))))).to_dict()","944815e6":"#intersect of cust2items and coupon2items (increased score by 0.14)\ntrain['bought_X_vailable'] = train[['coupon_id','customer_id']].apply(lambda x : len(np.intersect1d(d_cust2items[x[1]].split() , d_coupon2items[x[0]].split())) , axis=1)\ntest['bought_X_vailable'] = test[['coupon_id','customer_id']].apply(lambda x : len(np.intersect1d(d_cust2items[x[1]].split() , d_coupon2items[x[0]].split())) , axis=1)","77e0b15b":"#############","5efdb0dc":"#item2coupons - dictionary mapping item_ids to all coupons applicable to them\nd_item2coupons = coupons.groupby('item_id').coupon_id.apply(lambda x: ' '.join(list(x.apply(lambda x: str(x))))).to_dict()","2cfdb4f6":"#adding col for whether coupon was applied on that item (i.e redeemed or not)\ncust_tran['redeem'] = cust_tran.coupon_discount.apply(lambda x: 1 if x<0 else 0)","c064ea63":"##############  1.) Calculating redeemed % per item from cust_tran\n#               2.) Summing all those %'s for items in a coupon, take mean finally\n#               3.) map it to coupons","115d2f82":"#per_item_redeemed_history = dict mapping item_ids to redeemed %\nd_per_item_redeemed_history = ((cust_tran.groupby('item_id').redeem.sum() \/ cust_tran.groupby('item_id').redeem.count()) *100).to_dict()","bca02266":"#some items corresponding to test coupons are not in d_per_item_redeemed_hist hence need for this func\ndef item_redeem_func(x):\n    for item in d_coupon2items[x].split():\n        per = []\n        try:\n            per.append(d_per_item_redeemed_history[int(item)])\n\n        except:\n            pass\n    k = [np.mean(per) if pd.isna(np.mean(per)) == False else 0]\n    return k[0]","7cc195db":"#applying the above func to coupon_id\ntrain['item_redeem'] = train.coupon_id.apply(item_redeem_func)\ntest['item_redeem'] = test.coupon_id.apply(item_redeem_func)","6cf17fc7":"###############","0f179c7a":"##### 1.) Calculating redeemed % per customer from cust_tran\n#     2.) map it to customer_ids in train and tests","90b78458":"#per_cust_redeem_history - dict mapping customer_id to redemmed %\nd_per_cust_redeem_history = ((cust_tran.groupby('customer_id').redeem.sum() \/ cust_tran.groupby('customer_id').redeem.count())*100).to_dict()","8c6b68fd":"#adding a col for cust redeem #increased score by 0.03\ntrain['cust_redeem'] = train.customer_id.map(d_per_cust_redeem_history)\ntest['cust_redeem'] = test.customer_id.map(d_per_cust_redeem_history)","0007491d":"###############","5fcbcd6c":"#adding net price\n#cust_tran['net_price'] = cust_tran['selling_price'] - cust_tran['other_discount'] - cust_tran['coupon_discount']\n\n#dict for cust_id to income bracket\n#d_cust2_incomebrac = cust_demo[['customer_id','income_bracket']].set_index('customer_id').to_dict()['income_bracket']\n\n#adding income bracket col in customer trans\n#cust_tran['income_bracket'] = cust_tran.customer_id.map(d_cust2_incomebrac)\n\n#merging cust_trans with items on item_id\ncust_tran = pd.merge(cust_tran, items, how='left', on='item_id')","30d7726e":"cust_tran.head()","7767152d":"#################### 1.) Calculating redeemed % per category ---> per_cat_redeem_history\n#                    2.) Calculating redeemed % per customer based on cat using (1) ---> per_cust_redeem_history_catwali\n#                    3.) map (2) to customer_ids in train and test","f14d7039":"#redeem history based on category\nd_per_cat_redeem_history = (cust_tran.groupby('category').redeem.sum() \/ cust_tran.groupby('category').redeem.count()*1000).to_dict()","ea9d2749":"#(increased score by 0.0001)\nd_per_cust_redeem_history_catwali = cust_tran.groupby('customer_id').category.apply(lambda x: np.mean([d_per_cat_redeem_history[k] for k in x.values]))\n\ntrain['cat_cust_redeem'] = train.customer_id.map(d_per_cust_redeem_history_catwali)\ntest['cat_cust_redeem'] = test.customer_id.map(d_per_cust_redeem_history_catwali)","35f8b307":"############","f8717a0b":"############ if for a customer, brands bought by him previously are available in the coupon given, high chance of redeem","5d381371":"#cust2brands - dict mapping customer_ids to all brands bought by them\nd_cust2brands = cust_tran.groupby('customer_id').brand.apply(lambda x: ' '.join([str(k) for k in x.unique()])).to_dict()","fe75e27e":"#item2brand - dict mapping items to their respective brands\nd_item2brand = cust_tran.groupby('item_id').brand.apply(lambda x: x.unique()[0]).to_dict()","26a60dd6":"#filling nans in brand of which we have no prior info\ncoupons['brand'] = coupons.item_id.map(d_item2brand).fillna('99999999999')","c3b26a82":"#coupon2brands - dict mapping coupons to all brands available in them to purchase\nd_coupon2brands = coupons.groupby('coupon_id').brand.apply(lambda x: ' '.join([str(int(k)) for k in x.unique()])).to_dict()","dfe1843b":"#getting no of common brands in cust2brands and coupon2brands\ntrain['brand_bot'] = train[['customer_id','coupon_id']].apply(lambda x: len(np.intersect1d(d_cust2brands[x[0]].split(), d_coupon2brands[x[1]].split())), axis=1)\ntest['brand_bot'] = test[['customer_id','coupon_id']].apply(lambda x: len(np.intersect1d(d_cust2brands[x[0]].split(), d_coupon2brands[x[1]].split())), axis=1)","50271a7f":"#########","55bd5c46":"######### Filling some nans in rented, age_range","84db25ed":"#filling nans in train.rented with 2\ntrain.rented.fillna(2,inplace=True)\ntest.rented.fillna(2,inplace=True)","1349f2cf":"#imputing age_range based on campaign_id\n\ndef d_age(df):\n    k = df.groupby('campaign_id').age_range.value_counts()\n    k = k.reset_index(name='value').sort_values(['campaign_id','value'], ascending=[True,False])\n    d_age = {}\n    for i in list(df.campaign_id.unique()):\n        df = k.loc[k.campaign_id == i,['age_range','value']]\n        df = df.set_index('age_range')\n        max_val_per_campaign = df.idxmax().value\n        d_age[i] = max_val_per_campaign\n        \n    return d_age\n\n    \n#filling nans with d_age\ntrain.loc[(pd.isnull(train.age_range)),'age_range'] = train.loc[(pd.isnull(train.age_range)),'campaign_id'].map(d_age(train))\ntest.loc[(pd.isnull(test.age_range)),'age_range'] = test.loc[(pd.isnull(test.age_range)),'campaign_id'].map(d_age(test))\n","1277fe69":"###############","697a7824":"#adding brand (most frequent) per coupon_id\ntrain['brand'] = train.coupon_id.map(coupons.groupby('coupon_id').brand.apply(lambda x: x.values[0]).to_dict())\ntest['brand'] = test.coupon_id.map(coupons.groupby('coupon_id').brand.apply(lambda x: x.values[0]).to_dict())","f9fb5816":"############### val set","1f48a6f1":"#array's containing common customer_ids and coupon_ids in train,test ---> (in order to make val set)\ncommom_cust = np.intersect1d(train.customer_id.unique(),test.customer_id.unique())\ncommom_coup = np.intersect1d(train.coupon_id.unique(),test.coupon_id.unique())","5955ad25":"#adding col to see whether cust, coup is in test or not \ntrain['test_cust'] = train.customer_id.apply(lambda x: 1 if x in commom_cust else 0)\ntrain['test_coup'] = train.coupon_id.apply(lambda x: 1 if x in commom_coup else 0)","278f2c6d":"####Validation set\n\n#(len(train[pd.isnull(train.family_size) & (train.redemption_status == 1)]) \/ len(train)) * 7837 #16\nindex1 = train[pd.isnull(train.family_size) & (train.redemption_status == 1) & (train.test_cust == 1) & (train.test_coup == 1)].sample(16, random_state=1996).index\n\n#(len(train[pd.notnull(train.family_size) & (train.redemption_status == 1)]) \/ len(train) ) * 7837 #57\nindex2 = train[pd.notnull(train.family_size) & (train.redemption_status == 1) & (train.test_cust == 1) & (train.test_coup == 1)].sample(57, random_state=1996).index\n\n#(len(train[pd.isnull(train.family_size) & (train.redemption_status == 0)]) \/ len(train)) * 7837 #3455\nindex3 = train[pd.isnull(train.family_size) & (train.redemption_status == 0) & (train.test_cust == 1) & (train.test_coup == 1)].sample(3366, random_state=1996).index\n\n#(len(train[pd.notnull(train.family_size) & (train.redemption_status == 0)]) \/ len(train)) * 7837 #4309\nindex4 = train[pd.notnull(train.family_size) & (train.redemption_status == 0) & (train.test_cust == 1) & (train.test_coup == 1)].sample(4309, random_state=1996).index\n\n\n\n#new train and val set\nval_index = []\nfor i in [index1,index2, index3, index4]:\n    val_index.extend(i)#main val_index\n    \ntrain_index = set(train.index)\ntrain_index = train_index.symmetric_difference(val_index)#main train index\n\nnew_train = train.loc[train_index]\nval = train.loc[val_index].sample(frac=1, random_state = 1996)\nnew_test = test","94965e46":"#final_train = new_train.dropna(axis=1).drop(['test_cust','test_coup'], axis=1)\n#final_test = new_test.dropna(axis=1)#.drop(['coup_redeem'], axis=1)\n#val = val.dropna(axis=1).drop(['test_cust','test_coup'], axis=1)\n\nfinal_train = train.dropna(axis=1).drop(['test_cust','test_coup'], axis=1)\nfinal_test = test.dropna(axis=1)\n\n################# Label Encoding\n\n#label encoding features\nfinal_train['campaign_type'] = final_train.campaign_type.map({'X':0,'Y':1})\n#val['campaign_type'] = val.campaign_type.map({'X':0,'Y':1})\nfinal_test['campaign_type'] = final_test.campaign_type.map({'X':0,'Y':1})\n\nfinal_train['age_range'] = final_train.age_range.map({'46-55':0,'36-45':1,'18-25':2,'26-35':3,'56-70':4,'70+':5})\n#val['age_range'] = val.age_range.map({'46-55':0,'36-45':1,'18-25':2,'26-35':3,'56-70':4,'70+':5})\nfinal_test['age_range'] = final_test.age_range.map({'46-55':0,'36-45':1,'18-25':2,'26-35':3,'56-70':4,'70+':5})\n\n###############\n\n############## train_test\n\n#preparing data\nX_train = final_train.drop(['redemption_status'],axis=1)\ny_train = final_train.redemption_status\n\n#val_x = val.drop(['redemption_status'],axis=1)\n#val_y = val.redemption_status\n\nX_test = final_test","19dca784":"from sklearn.metrics import roc_auc_score","b60841f5":"from xgboost import XGBClassifier\n\npreds1= pd.DataFrame()\nauc_roc1 = []\nval_auc = []\n\nfor k in range(0,10):\n    df_1 = final_train[final_train.redemption_status == 1]\n    df_0 = final_train[final_train.redemption_status == 0].sample(1000, random_state =2*k*k*k)\n    \n    df = pd.concat([df_0,df_1],axis=0).sample(frac=1)\n\n    X_train = df.drop('redemption_status',axis=1)\n    y_train = df.redemption_status\n\n    model1 = XGBClassifier(n_estimators=100, scale_pos_weight=2)\n    model1.fit(X_train,y_train)\n\n    #pred by model1\n    auc_roc1.append(roc_auc_score(y_train, model1.predict_proba(X_train)[:,1].round(3)))\n    preds1['k'+str(k)] = model1.predict_proba(X_test)[:,1].round(3)\n    \n\n\n    print(k, end='')","3f3de4a6":"#pred = preds1.mean(axis=1)\n#roc_auc_score(val_y, pred)","98230112":"##############","f6ed144f":"pred_test = preds1.mean(axis=1)\n\nsample['redemption_status'] = pred_test\n\nname = 'final.csv'\nsample.to_csv(name,index=False)","201dad9a":"Problem Statement Predicting Coupon Redemption XYZ Credit Card company regularly helps it\u2019s merchants understand their data better and take key business decisions accurately by providing machine learning and analytics consulting. ABC is an established Brick & Mortar retailer that frequently conducts marketing campaigns for its diverse product range. As a merchant of XYZ, they have sought XYZ to assist them in their discount marketing process using the power of machine learning. Can you wear the AmExpert hat and help out ABC?\n\nDiscount marketing and coupon usage are very widely used promotional techniques to attract new customers and to retain & reinforce loyalty of existing customers. The measurement of a consumer\u2019s propensity towards coupon usage and the prediction of the redemption behaviour are crucial parameters in assessing the effectiveness of a marketing campaign.\n\nABC\u2019s promotions are shared across various channels including email, notifications, etc. A number of these campaigns include coupon discounts that are offered for a specific product\/range of products. The retailer would like the ability to predict whether customers redeem the coupons received across channels, which will enable the retailer\u2019s marketing team to accurately design coupon construct, and develop more precise and targeted marketing strategies.\n\nThe data available in this problem contains the following information, including the details of a sample of campaigns and coupons used in previous campaigns -\n\nUser Demographic Details Campaign and coupon Details Product details Previous transactions Based on previous transaction & performance data from the last 18 campaigns, predict the probability for the next 10 campaigns in the test set for each coupon and customer combination, whether the customer will redeem the coupon or not?\n\nDataset Description Here is the schema for the different data tables available. The detailed data dictionary is provided next.\n\nYou are provided with the following files in train.zip:\n\ntrain.csv: Train data containing the coupons offered to the given customers under the 18 campaigns\n\nVariable Definition id Unique id for coupon customer impression campaign_id Unique id for a discount campaign coupon_id Unique id for a discount coupon customer_id Unique id for a customer redemption_status (target) (0 - Coupon not redeemed, 1 - Coupon redeemed) campaign_data.csv: Campaign information for each of the 28 campaigns\n\nVariable Definition campaign_id Unique id for a discount campaign campaign_type Anonymised Campaign Type (X\/Y) start_date Campaign Start Date end_date Campaign End Date coupon_item_mapping.csv: Mapping of coupon and items valid for discount under that coupon\n\nVariable Definition coupon_id Unique id for a discount coupon (no order) item_id Unique id for items for which given coupon is valid (no order) customer_demographics.csv: Customer demographic information for some customers\n\nVariable Definition customer_id Unique id for a customer age_range Age range of customer family in years marital_status Married\/Single rented 0 - not rented accommodation, 1 - rented accommodation family_size Number of family members no_of_children Number of children in the family income_bracket Label Encoded Income Bracket (Higher income corresponds to higher number) customer_transaction_data.csv: Transaction data for all customers for duration of campaigns in the train data\n\nVariable Definition date Date of Transaction customer_id Unique id for a customer item_id Unique id for item quantity quantity of item bought selling_price Sales value of the transaction other_discount Discount from other sources such as manufacturer coupon\/loyalty card coupon_discount Discount availed from retailer coupon item_data.csv: Item information for each item sold by the retailer\n\nVariable Definition item_id Unique id for item brand Unique id for item brand brand_type Brand Type (local\/Established) category Item Category test.csv: Contains the coupon customer combination for which redemption status is to be predicted\n\nVariable Definition id Unique id for coupon customer impression campaign_id Unique id for a discount campaign coupon_id Unique id for a discount coupon customer_id Unique id for a customer *Campaign, coupon and customer data for test set is also contained in train.zip\n\nsample_submission.csv: This file contains the format in which you have to submit your predictions.\n\nTo summarise the entire process:\n\nCustomers receive coupons under various campaigns and may choose to redeem it. They can redeem the given coupon for any valid product for that coupon as per coupon item mapping within the duration between campaign start date and end date Next, the customer will redeem the coupon for an item at the retailer store and that will reflect in the transaction table in the column coupon_discount.\n\nEvaluation Metric Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.\n\nPublic and Private Split Test data is further randomly divided into Public (40%) and Private data (60%) Your initial responses will be checked and scored on the Public data. The final rankings would be based on your private score which will be published once the competition is over. Hackathon Rules Setting the final submission is mandatory. Without a final submission, the submission corresponding to best public score will be taken as final submission Use of external datasets is not allowed Use of id variable as a part of making predictions is not allowed You can only make 10 submissions per day The code file is mandatory while setting final submission. For GUI based tools, please upload a zip file of snapshots of steps taken by you, else upload code file. The code file uploaded should be pertaining to your final submission. No submission will be accepted after the contest deadline"}}