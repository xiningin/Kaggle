{"cell_type":{"e57e0a76":"code","aa4a716e":"code","d0fb0d6d":"code","4a5c595c":"code","a8db1ce5":"code","279986ef":"code","2c8a1cf6":"code","428b9cb9":"code","2260d69e":"code","c4ddc102":"code","032b7c7d":"code","a86a9477":"code","ee123f72":"code","14e4a767":"code","55a519b6":"code","0a65e153":"code","b03c1403":"code","b247f675":"code","daa63e1a":"code","fcc21995":"code","35a0fd29":"code","f9936a60":"code","b5f90d1f":"code","b154d961":"code","78d8cecb":"code","646b0981":"code","66cc421d":"code","d75dd877":"code","8607295d":"markdown","e5474347":"markdown","c8dc7c17":"markdown","28c0d17d":"markdown","2189c0fc":"markdown","f6a168a2":"markdown","6bef15d9":"markdown","bf544323":"markdown","3e5adecf":"markdown","384d4e8b":"markdown","868430a2":"markdown","b16d968d":"markdown","135e7e97":"markdown","327b020a":"markdown","b3c54f8b":"markdown","7cb7ef6b":"markdown","41c450ff":"markdown","6784a6ad":"markdown","7fc982ea":"markdown","72401fe6":"markdown","8b466e36":"markdown"},"source":{"e57e0a76":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport glob\nimport os\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBClassifier, plot_importance\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split, KFold\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfilename1 = r'\/kaggle\/input\/tabular-playground-series-nov-2021\/train.csv'\nfilename2 = r'\/kaggle\/input\/tabular-playground-series-nov-2021\/test.csv'\n\n\ndf_train = pd.read_csv(filename1, index_col=None, header=0)\ndf_test = pd.read_csv(filename2, index_col=None, header=0)\n#df=df.iloc[1:,:]\nprint(len(df_train), 'rows in training dataset')\nprint(df_train.head())\nprint(len(df_test), 'rows in test dataset')\nprint(df_test.head())","aa4a716e":"df_train.describe()","d0fb0d6d":"df_test.describe()","4a5c595c":"df_train.isnull().sum()","a8db1ce5":"df_test.isnull().sum()","279986ef":"Y=df_train['target']","2c8a1cf6":"mask = df_train.dtypes == np.float\nfloat_cols = df_train.columns[mask]\nfig, axes = plt.subplots(len(float_cols) \/\/ 4, 4, figsize=(22, 40))\n#df[float_cols].hist(figsize=(20, 20), bins=50, xlabelsize=12, ylabelsize=12); \nfor col,axis in zip(float_cols,axes.reshape(-1)):\n    sns.histplot(df_train[col], ax=axis, kde=True,bins=100, label=f'train_{col}')\n    sns.histplot(df_test[col], color ='red' ,ax=axis, kde=True,bins=100, label=f'test_{col}')\n    axis.legend()\n    \n","428b9cb9":"dfcorr = df_train.corr()\nndf = dfcorr.loc[dfcorr.max(axis=1) > 0.50, dfcorr.max(axis=0) > 0.50]\n\nsns.heatmap(ndf)\nplt.show()","2260d69e":"dfcorr['target']","c4ddc102":"Y_train=df_train['target']\n","032b7c7d":"## Target distibution\npie, ax = plt.subplots(figsize=[18,8])\ndf_train.groupby('target').size().plot(kind='pie',autopct='%.1f',colors=sns.color_palette('pastel')[0:2], x=ax,title='Target distibution')","a86a9477":"# Create a list of float colums to check for skewing\nmask = df_train.dtypes == np.float\nfloat_cols = df_train.columns[mask]\nskew_limit = 0.75\nskew_vals = df_train[float_cols].skew()\n\nskew_cols = (skew_vals\n             .sort_values(ascending=False)\n             .to_frame()\n             .rename(columns={0:'Skew'})\n             .query('abs(Skew) > {0}'.format(skew_limit)))\n\nskew_cols = skew_cols.index.to_list()","ee123f72":"from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n\n# apply standard scaler to the data\nscaler = RobustScaler()\ndf_train[skew_cols] = scaler.fit_transform(df_train[skew_cols])\ndf_test[skew_cols] = scaler.transform(df_test[skew_cols])","14e4a767":"df_train=df_train.drop('id', axis=1)\ndf_test=df_test.drop('id', axis=1)","55a519b6":"xgb_classifier = XGBClassifier(booster='gbtree', \n                             max_depth = 4,\n                    objective = 'binary:logistic',         \n                    n_estimators=1000, \n                    learning_rate = .1)\n\nKFoldseed = 1\ncv = KFold(n_splits=5, shuffle=True, random_state=KFoldseed)","0a65e153":"#separate predictors and targets in data frame\n#remove id column as it is simply numbering the rows\nx_df = df_train[df_train.columns[:-1]]\ny_df = df_train[df_train.columns[-1:]]\nprint(x_df.shape)\nprint(y_df.shape)","b03c1403":"y_df.columns\n","b247f675":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\ncolumn='target'\nsigma_target_list = []\nsigma_target = np.std(y_df[column])\nsigma_target_list.append(sigma_target)\n\ncv_train_accuracy= []\ncv_test_accuracy = []\ncv_train_roc_auc= []\ncv_test_roc_auc = []\n\nfor train_idx, test_idx in cv.split(x_df):\n\nx_train, x_test = x_df.iloc[train_idx], x_df.iloc[test_idx]\ny_train, y_test = y_df[column].iloc[train_idx], y_df[column].iloc[test_idx]\n\n\nfitted_model = xgb_classifier.fit(x_train, y_train, \n                            eval_set=[(x_train, y_train), (x_test, y_test)],\n                            eval_metric='auc',\n                            early_stopping_rounds=250, \n                            verbose=False)\n\npred_train = fitted_model.predict(x_train)\npred_train_prob = fitted_model.predict_proba(x_train)[:,1]\n\ncv_train_accuracy.append(accuracy_score(y_train.values, pred_train))\ncv_train_roc_auc.append(roc_auc_score(y_train, pred_train_prob))\n\npred_test = fitted_model.predict(x_test)\npred_test_prob = fitted_model.predict_proba(x_test)[:,1]\ncv_test_accuracy.append(accuracy_score(y_test.values, pred_test))\ncv_test_roc_auc.append(roc_auc_score(y_test.values, pred_test_prob))\n\n\n    ","daa63e1a":"evals_result = fitted_model.evals_result()\nplt.plot(np.arange(len(evals_result['validation_0']['auc'])), evals_result['validation_0']['auc'], label='Training Set')\nplt.plot(np.arange(len(evals_result['validation_1']['auc'])), evals_result['validation_1']['auc'], label='Testing Set')\nplt.xlabel('Iteration')\nplt.ylabel('AUC')\nplt.title('Learning Curve for Target', fontweight='bold')\nplt.legend()\n\n#plot feature importance per target\nfeature_importances = pd.DataFrame(fitted_model.feature_importances_, index = x_df.columns, columns=['importance']).sort_values('importance')\nprint(feature_importances[feature_importances['importance']>0.02])\npos_importance = feature_importances[feature_importances['importance']>0.02]\npos_importance.plot(kind = 'barh',title=f'Target')\nplt.show()\nplt.clf()","fcc21995":"#Print model report:\nprint(\"\\nModel Report\")\nprint(\"XGBoost Mean Accuracy (Train) : %.4g\" % np.mean(cv_train_accuracy))\nprint(\"XGBoost Mean AUC Score (Train): %f\" % np.mean(cv_train_roc_auc))\n\nprint(\"XGBoost Mean Accuracy (Test) : %.4g\" % np.mean(cv_test_accuracy))\nprint(\"XGBoost Mean AUC Score (Test): %f\" % np.mean(cv_test_roc_auc))\n\n","35a0fd29":"from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n# Creates a confusion matrix\ncm = confusion_matrix(y_test, pred_test) \n\n# Transform to df for easier plotting\ncm_df = pd.DataFrame(cm,\n                     index = ['0','1',], \n                     columns = ['0','1'])\n\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_df, annot=True,fmt='g')\nplt.title('XGBoost \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, pred_test)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","f9936a60":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\ncolumn='target'\nsigma_target_list = []\nsigma_target = np.std(y_df[column])\nsigma_target_list.append(sigma_target)\n    \ncv_train_accuracy= []\ncv_test_accuracy = []\ncv_train_roc_auc= []\ncv_test_roc_auc = []\npreds = np.zeros(len(df_test))\nlgb = lgb.LGBMClassifier(learning_rate=0.04,max_depth=3, n_estimators=5000)\n\nfor train_idx, test_idx in cv.split(x_df):\n\n    x_train, x_test = x_df.iloc[train_idx], x_df.iloc[test_idx]\n    y_train, y_test = y_df[column].iloc[train_idx], y_df[column].iloc[test_idx]\n    \n    \n    fitted_model = lgb.fit(x_train, y_train, \n                                eval_set=[(x_train, y_train), (x_test, y_test)],\n                                eval_metric='auc',\n                                early_stopping_rounds=250, \n                                verbose=False)\n\n    pred_train = fitted_model.predict(x_train)\n    pred_train_prob = fitted_model.predict_proba(x_train)[:,1]\n   \n    cv_train_accuracy.append(accuracy_score(y_train.values, pred_train))\n    cv_train_roc_auc.append(roc_auc_score(y_train, pred_train_prob))\n   \n    pred_test = fitted_model.predict(x_test)\n    pred_test_prob = fitted_model.predict_proba(x_test)[:,1]\n    preds += fitted_model.predict_proba(df_test)[:,1]\/5\n    cv_test_accuracy.append(accuracy_score(y_test.values, pred_test))\n    cv_test_roc_auc.append(roc_auc_score(y_test.values, pred_test_prob))","b5f90d1f":"#Print model report:\nprint(\"\\nModel Report\")\nprint(\"LightGBM Mean Accuracy (Train) : %.4g\" % np.mean(cv_train_accuracy))\nprint(\"LightGBM Mean AUC Score (Train): %f\" % np.mean(cv_train_roc_auc))\n\nprint(\"LightGBM Mean Accuracy (Test) : %.4g\" % np.mean(cv_test_accuracy))\nprint(\"LightGBM Mean AUC Score (Test): %f\" % np.mean(cv_test_roc_auc))","b154d961":"from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n# Creates a confusion matrix\ncm = confusion_matrix(y_test, pred_test) \n\n# Transform to df for easier plotting\ncm_df = pd.DataFrame(cm,\n                     index = ['0','1',], \n                     columns = ['0','1'])\n\nplt.figure(figsize=(5.5,4))\nsns.heatmap(cm_df, annot=True,fmt='g')\nplt.title('LightGBM \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, pred_test)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","78d8cecb":"!pip install pytorch-tabnet wget\n","646b0981":"import pytorch_tabnet\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nimport numpy as np\nimport torch\nnp.random.seed(8)\ntabnetclass = TabNetClassifier(optimizer_fn=torch.optim.Adam,\n                       optimizer_params=dict(lr=2e-2),\n                       scheduler_params={\"step_size\":50, # how to use learning rate scheduler\n                                         \"gamma\":0.8},\n                       scheduler_fn=torch.optim.lr_scheduler.StepLR,\n                       mask_type='entmax' # \"sparsemax\"\n                      )\n\n","66cc421d":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_df, y_df, test_size=0.30, random_state=8)\nx_train_np= x_train.to_numpy()\ny_train_np= y_train.to_numpy().ravel()\nx_val_np = x_val.to_numpy()\ny_val_np = y_val.to_numpy().ravel()\n\ntabnetclass.fit(\n    x_train_np,y_train_np,\n    eval_set=[(x_train_np, y_train_np), (x_val_np, y_val_np)],\n    eval_name=['train', 'valid'],\n    eval_metric=['auc','accuracy'],\n    max_epochs=200 , patience=20,\n    batch_size=1024, virtual_batch_size=128,\n    num_workers=0,\n    weights=1,\n    drop_last=False\n)","d75dd877":"filename3 = r'\/kaggle\/input\/tabular-playground-series-nov-2021\/sample_submission.csv'\n\ndf3 = pd.read_csv(filename3, index_col=None, header=0)\n\ndf3['target']=preds\ndf3.to_csv('submission.csv', index=False)\ndf3\n","8607295d":"### Distribution and correlation checks\n","e5474347":"We can see that there is no strong correlation between features to be detected.","c8dc7c17":"The preliminary checks detect no missing values hence the next step of analysis: distribution and feature correlation checks are performed.","28c0d17d":"### Skewness Check and Normalisation","2189c0fc":"Other than results we are also interested in feature importance. This is important since if in the future hyperarameter optimization is performed and the goal is to improve the results elimination of some features can play an important role.","f6a168a2":"# Kaggle Tabular Playground Nov Data: Logistic regression","6bef15d9":"XGBoost in this example takes a long time to run hence to LightGBM Classifier was tested to improve the execution times. Not only LightGBM rans faster but also it provides a slightly improved result both on training and testing sets.","bf544323":"It is easy to see that data is continuous and is not normalised. Note that feature 'f2' gets significantly higher values compared to other features and depending on modelling technique adopted scaling could be required.","3e5adecf":"## Modelling and splitting the data into 5 KFolds","384d4e8b":"## EDA","868430a2":"Introduced in 2019 by Google TabNet (https:\/\/arxiv.org\/pdf\/1908.07442.pdf) is a Neural Network that was able to outperform the leading tree based models across a variety of benchmarks. It is also considered more explainable than boosted tree models and can be used without any feature preprocessing.  Hence it was interesting to try TabNet for this problem and see whether score can be improved. To test the labelled data set was split into 2 sets -training and test for TabNet to train.","b16d968d":"## Improvements : LightGBM Model","135e7e97":"For modelling XGBoost was chosen as this is a powerful classifier to be used for logistic regression problems. The training set is split into 5 Folds and XGBClassifier is trained on that data. **Note** that tree based models do not require data scaling and normalising hence no data scaling\/normalisation is performed.","327b020a":"This work looks at Kaggle Tabular Playground Nov. Data and produces predictions for 'target' variable. \nThe data is synthetically generated by a GAN that was trained on a real-world dataset used to identify spam emails via various extracted features from the email.It contains 100 features with continuous values and one target variable (0,1).\nThe following steps were followed:\n\n- detailed exploratory data analysis\n- model construction and evaluation\n- hyperparameter tuning\n- producing predictions\n\n","b3c54f8b":"## Improvements TabNet Model","7cb7ef6b":"## Submission","41c450ff":"## Confusion Matrix","6784a6ad":"The next step is checking the columns for null values","7fc982ea":"### Target Disribution check\n","72401fe6":"In this section train and test data sets are read, analysed and data checks are performed.","8b466e36":"As we see using LightGBM model has imporved the score and decreased the difference of AUC scores between the traning and testing sets."}}