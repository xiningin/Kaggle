{"cell_type":{"d6375a42":"code","dca00b7f":"code","41ef2965":"code","32e3c6e6":"code","84a88506":"code","31a08acd":"code","7174c050":"code","7eb013a1":"code","e18ea68d":"code","4b86aec7":"code","20c0b6a1":"code","b162bee8":"code","316c9f52":"code","e988bf8b":"code","b1b15f20":"code","bf6330f0":"code","689e4ca2":"code","76eb5138":"code","7fdfc95b":"markdown","5f232a20":"markdown","ccaba4a9":"markdown","29c8928c":"markdown","3100c4a5":"markdown","ef8c83d3":"markdown","0d6cf1af":"markdown","ab5067ba":"markdown","c9713a47":"markdown","56b4df20":"markdown","4b5b84d6":"markdown"},"source":{"d6375a42":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dca00b7f":"nRowsRead = 1000 # specify 'None' if want to read whole file\ndf = pd.read_csv('..\/input\/cusersmarildownloadswinecsv\/wine.csv', delimiter=';', encoding = \"utf8\", nrows = nRowsRead)\ndf.dataframeName = 'wine.csv'\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')\ndf.head()","41ef2965":"#The one before the last was alcohol (since the encoding didn't work I changed to sulphates 2 before the last)\n\nX = df.loc[:,'fixed_acidity':'sulphates']\ny = df['quality']","32e3c6e6":"X.columns","84a88506":"y.name","31a08acd":"from sklearn.preprocessing import LabelEncoder\n\n#fill in mean for floats\nfor c in df.columns:\n    if df[c].dtype=='float16' or  df[c].dtype=='float32' or  df[c].dtype=='float64':\n        df[c].fillna(df[c].mean())\n\n#fill in -999 for categoricals\ndf = df.fillna(-999)\n# Label Encoding\nfor f in df.columns:\n    if df[f].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(df[f].values))\n        df[f] = lbl.transform(list(df[f].values))\n        \nprint('Labelling done.')","7174c050":"df = pd.get_dummies(df)","7eb013a1":"from sklearn import preprocessing\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = preprocessing.StandardScaler().fit(X)\nscaler","e18ea68d":"StandardScaler(copy=True, with_mean=True, with_std=True)","4b86aec7":"print('Mean of each variable:')\nprint(scaler.mean_)\nprint('\\nStd of each variable:')\nprint(scaler.scale_)","20c0b6a1":"#Perform transformation\n\nX = scaler.transform(X)","b162bee8":"from sklearn.decomposition import PCA\n\npca = PCA() # creates an instance of PCA class\nresults = pca.fit(X) # applies PCA on predictor variables\nZ = results.transform(X) # create a new array of latent variables","316c9f52":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set()\n\n\nplt.plot(results.explained_variance_ratio_*100) # scree plot\nplt.show()","e988bf8b":"pd.DataFrame(results.components_)","b1b15f20":"pd.DataFrame(Z[:,:6], columns=list(\n[u'Acidity', u'Sulfides', u'More alcohol', u'Chlorides', u'More residual sugar', u'Less pH'])).head(10)","bf6330f0":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nneigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(X,y)\npred = neigh.predict(X)\nprint('Confusion matrix:')\nprint(confusion_matrix(pred,y))\nprint('\\nAccuracy:')\nprint(accuracy_score(pred,y))","689e4ca2":"neigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(Z[:,:6],y)\npred = neigh.predict(Z[:,:6])\nprint('Confusion matrix:')\nprint(confusion_matrix(pred,y))\nprint('\\nAccuracy:')\nprint(accuracy_score(pred,y))","76eb5138":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n    \n    \ndhtml('Thanks Sinan Gok for the script' )","7fdfc95b":"#Double check:","5f232a20":"#Using the first 6 PCs","ccaba4a9":"#Perform transformation","29c8928c":"The above is called a scree plot. It shows the variances explained by each latent variable. The first component explains approx. 28% of the variance in the whole dataset.\n\nIdeally, we would like to see an elbow shape in order to decide which PCs to keep and which ones to disregard. In practice, this rarely happens. Most of the time, we use enough PCs so that they explain 95% or 99% of the variation in the data.\n\nBy examining the above figure, we can conclude that first 6 variables contain most of the information inside the data.\nhttps:\/\/goksinan.github.io\/machine\/pca-on-wine-data\/","3100c4a5":"#Use PCA and take a closer look at the latent variables.","ef8c83d3":"#Slice from first column to one before the last.","0d6cf1af":"#PCA on Wine Quality Dataset by Sinan Gok https:\/\/goksinan.github.io\/machine\/pca-on-wine-data\/\n\nUnsupervised learning (principal component analysis)","ab5067ba":"#The dataset has to be standardized (i.e. subtracting mean, dividing by the standard deviation) The scikit-learn PCA package probably performs this internally, but the author (Sinan Gok) did it anyway.","c9713a47":"#Interpreting the results\n\nOnce the author applied the PCA, he was no longer in his familiar domain. A different domain in which the latents are the linear combinations of the original variables, but they don\u2019t represent any meaningful properties. Thus, it is impossible to interpret them by themselves.\n\nLook at the correlation between the latent variable and original variables. If any of the original variables correlate well with the first few PCs, conclude that the PCs are mainly influenced by the said variables, thus they must be the important ones.\n\nAnother approach, look at the PCA coefficients. These coefficients tell how much of the original variables are used in creating the PCs. The higher the coefficient, the more important is the related variable.\nhttps:\/\/goksinan.github.io\/machine\/pca-on-wine-data\/","56b4df20":"#Using 6 variables instead of 11, he achive almost the same accuracy in his prediction.","4b5b84d6":"#Predictive model"}}