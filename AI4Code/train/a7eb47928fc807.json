{"cell_type":{"c4e0717d":"code","02758782":"code","3cd69c62":"code","1ba83a8e":"code","6f233fc9":"code","9eb51108":"code","f716d0ed":"code","c75c35b7":"code","87094e55":"code","78389b05":"code","22dac117":"code","f8212c42":"code","c009905e":"code","d49627c0":"code","73c40658":"code","5f82f227":"code","49e0433a":"code","03e1c1b9":"code","d23dcd5a":"code","a7f53bad":"code","79f9b874":"code","a9aa7d2e":"code","8e13fca4":"code","3bb617f7":"code","e1aef7ad":"code","8a9814b9":"code","737058fd":"code","46ddb174":"code","8478e428":"code","a46e1088":"code","bc547cd5":"code","d17a56ca":"code","782dbaba":"code","e1704dfb":"markdown","ae30d59f":"markdown","561425a8":"markdown","4a872ca5":"markdown","966fb3a4":"markdown","29dd9bf5":"markdown","204f7bb2":"markdown","e948aa34":"markdown","bd10dfac":"markdown","7ca592d8":"markdown","abf54814":"markdown","8447954c":"markdown","aa57fbde":"markdown","8a71625b":"markdown","bfe888ee":"markdown","6343153d":"markdown","5cc493ec":"markdown","ed514d37":"markdown","5f22caf9":"markdown","42e4d7e2":"markdown"},"source":{"c4e0717d":"import os\nprint(os.listdir(\"..\/input\"))","02758782":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\n%matplotlib inline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('ggplot')\nplt.rcParams['figure.figsize'] = 10, 5","3cd69c62":"train = pd.read_csv('..\/input\/train_data.csv', encoding='utf', engine='python', index_col=0)\ntest = pd.read_csv('..\/input\/test_data.csv', encoding='utf', engine='python', index_col=0)","1ba83a8e":"train.shape, test.shape","6f233fc9":"targ_distr = pd.DataFrame([train.score.value_counts().rename(\"Abs\"), \n                           (train.score.value_counts()\/train.shape[0]).rename(\"Rel\")])","9eb51108":"display(targ_distr.T)\n_ = train.score.value_counts().plot.bar()\nplt.title('Score distribution')\nplt.xticks(rotation=0)\nplt.show()","f716d0ed":"train['text_len'] = train.text.str.len()\ntest['text_len'] = test.text.str.len()\n\ntrain['title_len'] = train.title.str.len()\ntest['title_len'] = test.title.str.len()","c75c35b7":"prsntls = [.25, .5, .75, 0.9, 0.95, 0.99]\ntext_length = pd.DataFrame([train['text_len'].describe(percentiles=prsntls).rename(\"train\"), \n              test['text_len'].describe(percentiles=prsntls).rename(\"test\")])\ndisplay(text_length)","87094e55":"text_length = pd.DataFrame([train['title_len'].describe(percentiles=prsntls).rename(\"train\"), \n              test['title_len'].describe(percentiles=prsntls).rename(\"test\")])\ndisplay(text_length)","78389b05":"fig, ax = plt.subplots(nrows=1, ncols=2)\nax[0].set_title(\"Lens text\")\ntrain['text_len'].plot.hist(alpha=0.7, ax=ax[0])\ntest['text_len'].plot.hist(alpha=0.7, ax=ax[0])\nax[0].legend([\"Train\", \"Test\"]);\n\nax[1].set_title(\"Lens titles\")\ntrain['title_len'].plot.hist(alpha=0.7, ax=ax[1])\ntest['title_len'].plot.hist(alpha=0.7, ax=ax[1])\nax[1].legend([\"Train\", \"Test\"]);\nplt.tight_layout()","22dac117":"display(train.groupby('score')['text_len'].describe(percentiles=prsntls))\ndisplay(train.groupby('score')['title_len'].describe(percentiles=prsntls))\nfig, ax = plt.subplots(nrows=1, ncols=2)\n_ = sns.catplot(x=\"score\", y=\"text_len\", data=train, kind=\"bar\", ax=ax[0])\nplt.close(_.fig)\n_ = sns.catplot(x=\"score\", y=\"title_len\", data=train, kind=\"bar\", ax=ax[1])\nplt.close(_.fig)\nplt.show()","f8212c42":"# https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-qiqc\nfrom wordcloud import WordCloud, STOPWORDS\n\n# Thanks : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, \n                   #max_words=200, \n                   max_font_size=100, \n                   title = None, image_color=False, ax = None):\n    stopwords = set(STOPWORDS)\n    # more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    # stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    #max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    ax.imshow(wordcloud);\n    ax.set_title(title)\n    ax.axis('off');\n    plt.tight_layout()","c009905e":"fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(20, 16))\nplot_wordcloud(train[\"text\"], title=\"Word Cloud Train Text\", ax=ax[0][0])\nplot_wordcloud(test[\"text\"], title=\"Word Cloud Test Text\", ax=ax[0][1])\nplot_wordcloud(train[\"title\"], title=\"Word Cloud Train Title\", ax=ax[1][0])\nplot_wordcloud(test[\"title\"], title=\"Word Cloud Test Title\", ax=ax[1][1])\nplt.tight_layout()","d49627c0":"mask_pos = train['score'] == '\u041f\u043e\u0437\u0438\u0442\u0438\u0432\u043d\u044b\u0439'\nmask_neg = train['score'] == '\u041d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u044b\u0439'","73c40658":"fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(20, 16))\nplot_wordcloud(train['text'], title=\"Word Cloud Train Text\", ax=ax[0][0])\nplot_wordcloud(train['text'][mask_pos], title=\"Word Cloud Score Pos Text\", ax=ax[0][1])\nplot_wordcloud(train['text'][mask_neg], title=\"Word Cloud Score Neg Text\", ax=ax[0][2])\n\nplot_wordcloud(train['title'], title=\"Word Cloud Train Text\", ax=ax[1][0])\nplot_wordcloud(train['title'][mask_pos], title=\"Word Cloud Score Pos Title\", ax=ax[1][1])\nplot_wordcloud(train['title'][mask_neg], title=\"Word Cloud Score Neg Title\", ax=ax[1][2])\nplt.tight_layout()","5f82f227":"train[train.text.str.lower().str.contains('\u0431\u043b\u0430\u0433\u043e\u0434\u0430\u0440\u043d\u043e\u0441\u0442\u044c')].groupby('score').count()","49e0433a":"# https:\/\/www.kaggle.com\/ogrellier\/lgbm-with-words-and-chars-n-gram\nimport re \n\ndef count_regexp_occ(regexp=\"\", text=None):\n    \"\"\" Simple way to get the number of occurence of a regex\"\"\"\n    return len(re.findall(regexp, text))","03e1c1b9":"def get_indicators_and_clean_comments(df):\n    \"\"\"\n    Check all sorts of content as it may help find toxic comment\n    Though I'm not sure all of them improve scores\n    \"\"\"\n    # Get length in words and characters\n    df[\"raw_word_len\"] = df[\"text\"].apply(lambda x: len(x.split()))\n    # Check number of upper case, if you're angry you may write in upper case\n    df[\"nb_upper\"] = df[\"text\"].apply(lambda x: count_regexp_occ(r\"[\u0410-\u042f]\", x))\n    # Check for time stamp\n    df[\"has_timestamp\"] = df[\"text\"].apply(lambda x: count_regexp_occ(r\"\\d{2}|:\\d{2}\", x))","d23dcd5a":"%%time\ntrain_prepr = train.copy()\ntest_prepr = test.copy()\n\nget_indicators_and_clean_comments(train_prepr)\nget_indicators_and_clean_comments(test_prepr)","a7f53bad":"tf_idf_word = TfidfVectorizer(ngram_range=(1, 2),\n                         stop_words=stopwords.words('russian'), \n                         token_pattern=('\\w{1,}'),\n                         #preprocessor=None,\n                         analyzer='word',\n                         #max_df=0.8, \n                         #min_df=10,\n                         max_features=20000,\n                         sublinear_tf=True,\n                        )","79f9b874":"%%time\ntf_idf_model = tf_idf_word.fit(np.concatenate([train['text'], test['text']]))","a9aa7d2e":"%%time\ntrain_tf_idf_vec = tf_idf_model.transform(train['text'])\ntest_tf_idf_vec = tf_idf_model.transform(test['text'])","8e13fca4":"from scipy.sparse import hstack","3bb617f7":"train_tf_idf_vec = hstack([train_tf_idf_vec, \n                           train_prepr[train_prepr.columns.difference(['title', 'text', 'score'])]])\ntest_tf_idf_vec = hstack([test_tf_idf_vec, \n                           test_prepr[test_prepr.columns.difference(['title', 'text', 'score'])]])","e1aef7ad":"train_tf_idf_vec, test_tf_idf_vec","8a9814b9":"lm = LogisticRegression(#solver='newton-cg', \n                        #n_jobs=-1,\n                        #solver='lbfgs',\n                        # penalty='l2',\n                        #tol=0.000000001,\n                        random_state=42,\n                        C=10, \n                        max_iter=100000)","737058fd":"lm_params = {'penalty':['l1', 'l2'],\n             'C':[0.001, 0.01, 0.1, 1, 2, 5, 10, 20, 100],\n             #'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n             #'tol' : [10, 1, 0.1, 0.01, 0.001, 0.0001, 0.0001]\n    \n    \n}\nlm_search = GridSearchCV(estimator=lm, \n                         param_grid=lm_params, \n                         scoring ='roc_auc', \n                         cv=StratifiedKFold(10), \n                         n_jobs=-1,\n                         verbose=1)","46ddb174":"%%time\nlm_search_fitted = lm_search.fit(X=train_tf_idf_vec, y=pd.factorize(train.score)[0])","8478e428":"lm_search_fitted.best_estimator_","a46e1088":"pred_scores = cross_val_score(estimator=lm_search_fitted.best_estimator_, X=train_tf_idf_vec, y=pd.factorize(train.score)[0],\n                scoring='roc_auc',  \n                cv=10, #stratified by default\n                n_jobs=-1)\ndisplay(np.mean(pred_scores))","bc547cd5":"predicts = lm_search_fitted.best_estimator_.predict_proba(test_tf_idf_vec)[:, 0]","d17a56ca":"sub = pd.DataFrame({'index': range(0, len(predicts)),\n                    'score':predicts})\nsub.to_csv('LRandTFIDF_sample_submission.csv', index=False)","782dbaba":"pd.read_csv('LRandTFIDF_sample_submission.csv').head()","e1704dfb":"## Best model local validation","ae30d59f":"\u0418\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u043d\u0443\u0436\u043d\u044b\u0435 \u043f\u0430\u043a\u0435\u0442\u044b","561425a8":"\u041f\u043e\u0441\u0442\u0440\u043e\u0438\u043c \"\u043e\u0431\u0449\u0438\u0435\" \u043e\u0431\u043b\u0430\u043a\u0430 \u0441\u043b\u043e\u0432.","4a872ca5":"\u0423 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0431\u043e\u043b\u044c\u0448\u0438\u0439 \u0440\u0430\u0437\u0431\u0440\u043e\u0441,","966fb3a4":"# EDA","29dd9bf5":"\u0414\u043b\u0438\u043d\u0430 \u0437\u0430\u0433\u043e\u043b\u043e\u0432\u043a\u043e\u0432 \u043f\u0440\u0438\u043c\u0435\u0440\u043d\u043e \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u044b\u0435.","204f7bb2":"\u0418\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u043e,\u0447\u0442\u043e \u0442\u0435\u043a\u0441\u0442\u044b \u043d\u0435\u043f\u043b\u043e\u0445\u043e \u0445\u043e\u0440\u043e\u0448\u043e \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u0446\u0438\u0440\u0443\u044e\u0442\u0441\u044f \u043f\u043e \u0441\u043b\u043e\u0432\u0443 \u0431\u043b\u0430\u0433\u043e\u0434\u0430\u0440\u043d\u043e\u0441\u0442\u044c \u0438\u043b\u0438 \u0435\u0433\u043e \u0447\u0430\u0441\u0442\u0438 (\u0431\u043b\u0430\u0433\u043e\u0434\u0430\u0440\/\u0431\u043b\u0430\u0433\u043e\u0434\u0430\u0440\u043d)","e948aa34":"# \u0412\u0435\u043a\u0442\u043e\u0440\u043d\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435","bd10dfac":" **\u0414\u043b\u0438\u043d\u0430 \u043d\u0430 \u043a\u043b\u0430\u0441\u0441**\n \n \u041d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u044b\u0435 \u0434\u043b\u0438\u043d\u043d\u0435\u0435 \u0438 \u043f\u043e \u0442\u0435\u043a\u0441\u0442\u0443, \u0438 \u043f\u043e \u0437\u0430\u0433\u043e\u043b\u043e\u0432\u043a\u0443.","7ca592d8":"**\u0414\u043b\u0438\u043d\u0430 \u0442\u0435\u043a\u0441\u0442\u0430 \u0438 \u0434\u043b\u0438\u043d\u0430 \u0437\u0430\u0433\u043e\u043b\u043e\u0432\u043a\u0430**","abf54814":"\u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u0438\u043c \u0444\u0430\u0439\u043b \u043e\u0442\u043f\u0440\u0430\u0432\u043a\u0438.","8447954c":"# Model fitting","aa57fbde":"## Params optimisation","8a71625b":"\u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c tf_idf \u043c\u043e\u0434\u0435\u043b\u044c \u0432\u0435\u043a\u0442\u043e\u0440\u043d\u043e\u0433\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0442\u0435\u043a\u0441\u0442\u0430","bfe888ee":"\u041f\u043e\u0434\u0433\u0440\u0443\u0437\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435","6343153d":"\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0442\u0430\u0440\u0433\u0435\u0442\u0430","5cc493ec":"\u041f\u043e\u0441\u0442\u0440\u043e\u0438\u043c \u044d\u0442\u0443 \u043c\u043e\u0434\u0435\u043b\u044c.","ed514d37":"# Some FE","5f22caf9":"\u041f\u043e\u0441\u0442\u0440\u043e\u0438\u043c \u043e\u0431\u043b\u0430\u043a\u0430 \u0441\u043b\u043e\u0432 \u043f\u043e \u043a\u043b\u0430\u0441\u0441\u0443","42e4d7e2":"\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0445\u0430\u0440\u0430\u043a\u0442\u0435\u0440\u0438\u0441\u0442\u0438\u043a\u0438 \u0442\u0435\u043a\u0441\u0442\u043e\u0432 \u0432 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u0439 \u0438 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0430\u0445."}}