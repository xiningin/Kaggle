{"cell_type":{"078ab6a4":"code","da1c43c1":"code","3ba8c2d9":"code","11ba10b9":"code","589751d3":"code","c5f1daec":"code","eaf3be65":"code","f3f9c3b9":"code","48c62847":"code","34c0441c":"code","9f2703af":"code","3fd07bea":"code","4a8a8e1c":"markdown","f802799d":"markdown","2f84872b":"markdown","fe78f0df":"markdown","502fbc72":"markdown","86fbd4a2":"markdown","e35f1a57":"markdown"},"source":{"078ab6a4":"from os.path import join, isfile\nfrom os import path, scandir, listdir\n\n# standard\nimport pandas as pd\nimport numpy as np\n\nimport gc","da1c43c1":"def list_all_files(location='..\/input', pattern=None, recursive=True):\n    subdirectories= [f.path for f in scandir(location) if f.is_dir()]\n    files = [join(location, f) for f in listdir(location) if isfile(join(location, f))]\n    if recursive:\n        for directory in subdirectories:\n            files.extend(list_all_files(directory))\n    if pattern:\n        files = [f for f in files if pattern in f]\n    return files","3ba8c2d9":"list_all_files(pattern='_ACS_income')","11ba10b9":"root = '..\/input\/cpe-data\/'\ndept_list = listdir(root)\ndept_list","589751d3":"list_all_files(root + dept_list[0])","c5f1daec":"def consistency(location):\n    dept_list = [d.path for d in scandir(location) if d.is_dir()]\n    for dept in dept_list:\n        dept_num = dept.split('_')[1]\n        print('Checking department {}'.format(dept_num))\n        # Check if we have some kind of data about crime or police\n        crime_files = list_all_files(dept, pattern='.csv', recursive=False)\n        if len(crime_files) < 1:\n            print(\"Department {} does not have data about police interventions\".format(dept_num))\n        # Check the ACS data consistency\n        data_path = dept + '\/' + dept_num + '_ACS_data\/'\n        # Check if we have all the topics (poverty, education, etc)\n        topics = [d.path for d in scandir(data_path) if d.is_dir()]\n        if len(topics) < 7:\n            print('Department {} does not have all the 7 ACS categories'.format(dept_num))\n        # Check if the data have consistent id's and columns\n        files = list_all_files(data_path, pattern='_with_ann.csv')\n        ids = []\n        for file in files:\n            meta = file.replace('_with_ann.csv', '_metadata.csv')\n            data = pd.read_csv(file, skiprows=[1], low_memory=False)\n            metadata = pd.read_csv(meta, header=None, names=['key', 'description'])\n            if not data.columns.all() in list(metadata['key']):\n                print(\"In {} inconsistent metadata\".format(file))\n            tmp_ids = data['GEO.id2'].unique()\n            if len(tmp_ids) != data.shape[0]:\n                print(\"In {} inconsistent id's\".format(file))\n            if len(ids) < 1: # the first time it creates the \"base\" of id's\n                ids = tmp_ids\n            if set(tmp_ids) != set(ids):\n                print(\"In {} inconsistent id's with the other files\".format(file))\n    print(\"Done\")","eaf3be65":"consistency(root)","f3f9c3b9":"def import_topic(path, tolerance=0.7):\n    # find the file with the ACS data and load it\n    datafile = list_all_files(path, pattern='_with_ann.csv')[0]\n    data = pd.read_csv(datafile, skiprows=[1], low_memory=False)\n    # take out the ids momentarily\n    ids = data[[col for col in data.columns if 'GEO' in col]]\n    rest = data[[col for col in data.columns if 'GEO' not in col]]\n    # convert to numeric and force na's if necessary\n    rest = rest.apply(pd.to_numeric, errors='coerce')\n    # put data together again\n    data = ids.join(rest)\n    print('Shape: {}'.format(data.shape))\n    cols = data.columns\n    nrows = data.shape[0]\n    removed = 0\n    for col in cols:\n        mis = data[col].isnull().sum() \/ nrows\n        if mis > tolerance:\n            removed += 1\n            del data[col]\n    if removed > 0:\n        print(\"Removed {} columns because more than {}% of the values are missing\".format(removed, \n                                                                                      tolerance*100))\n        print(\"New shape: {}\".format(data.shape))\n    meta = datafile.replace('_with_ann.csv', '_metadata.csv')\n    metadata = pd.read_csv(meta, header=None, names=['key', 'description'])\n    return data, metadata\n\n\ndef import_dept(location):\n    dept_num = location.split('_')[1]\n    print('Importing department {}'.format(dept_num))\n    print('\\n')\n    data_path = location + '\/' + dept_num + '_ACS_data\/'\n    data_list = {}\n    topics = listdir(data_path)\n    for topic in topics:\n        topic_name = topic.split('_')[-1]\n        print('Importing {}'.format(topic_name))\n        data, meta = import_topic(data_path + topic)\n        data_list[topic_name] = data\n        data_list[topic_name + '_meta'] = meta\n    gc.collect() # in case some of the files were really big\n    return data_list","48c62847":"test = root + dept_list[4]\nprint(test)\nprint(\"_\"*40)\nprint('\\n')\nresult = import_dept(test)","34c0441c":"result['poverty'].sample(10)","9f2703af":"result['poverty_meta'].sample(10)","3fd07bea":"result.keys()","4a8a8e1c":"The next step would be to do the same with shape files, automating the matching between those and the ACS, checking if there are problematic entries, etc.\n\nI hope this can speed up some of your process or at least helped you to come up with better ideas.\n\nCheers.","f802799d":"This notebook will not help you in getting insights out of your data but will give you a fast way of checking if everything is in order and import efficiently the ACS files.\n\nI think that, in an automated solution, being sure that your data are in a good shape is crucial for working efficiently.\n\nWhat follows is just a simple routine that hopefully you will find useful or, even better, will inspire you to build your own automated data quality check system.\n\nHere I just focus on the ACS data which somehow are very much ignored by most of the kernels I found so far.","2f84872b":"The first function is a recursive function that finds every file at a given location. Useful to import everything at once or to spot inconsistencies in your folder structure.","fe78f0df":"For example, to import everything we have about a department, we do simply","502fbc72":"Another simple example of usage is:","86fbd4a2":"With this simple function we can explore the entire input directory and be sure that:\n* every department has some file with the police data\n* All the 7 topics (poverty, income, education, etc) are present for each department\n* All the columns have a corresponding description in the metadata file\n* Across the topics, the Id's are consistent\n\nOne may argue that is easy to check by hand such things (maybe not the id's) but imagine what would you do if you have to scale up your solution. \n\nHere I just print stuff out. A better way of approaching such check would be to raise warnings, trigger responses, etc.","e35f1a57":"We see that there is something odd going on in Dept_11-00091 and a quick check reveals (thanks Chris for pointing it out in this [discussion](https:\/\/www.kaggle.com\/center-for-policing-equity\/data-science-for-good\/discussion\/69326#post410204)) that the department contains ACS files from different years across the topics.\n\n# Importing ACS data quickly\n\nThe next two functions allow you to import ACS data and the corresponding metadata. The output is a dictionary in the format, for example, `{'poverty' : data, 'poverty_meta' : metadata}`.\n\nI do two debatable things:\n\n* I don't like to see `(X)` or other symbols to indicate a missing data, so I put everything to NA\n* I drop every column with more than 70% of the data missing, but this is a tunable parameter"}}