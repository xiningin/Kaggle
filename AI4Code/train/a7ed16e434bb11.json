{"cell_type":{"ff363524":"code","d2e2d758":"code","707b923d":"code","00e25977":"code","aee6f4a3":"code","828075c7":"code","ec4e1252":"code","02ea43bc":"code","df5b99f9":"code","43ee5307":"code","a45b897b":"code","bbfd6a15":"code","0b5cbeee":"code","eeb0a302":"code","3e31397e":"code","7fd898dd":"code","f1f85ca9":"code","3ff4bc8a":"code","87ea987d":"code","1561c9f2":"code","d0306fa3":"code","6e67f3f5":"code","6f15ce9d":"code","616ef10c":"code","c6ed13de":"code","c50140a8":"code","fa531cf7":"code","e48e4081":"code","00d98127":"code","f923fef4":"code","4eaf10b0":"code","d86d629d":"code","a29b5ef2":"code","95bc528a":"code","5f89582b":"code","42916ae3":"code","4d5706ab":"code","49d06fd2":"code","33a9e4b1":"code","cc4d39c1":"code","7a42eb28":"code","fe960400":"code","d4280a74":"code","b2c4efae":"code","eeb93de9":"code","bc240ad9":"code","d35a6214":"code","2f5254ec":"code","0cb5f06c":"code","0c35da01":"code","d8ed5220":"code","6ad3243a":"code","ec0a7a82":"code","a46a14a6":"markdown","860694c4":"markdown","12968b91":"markdown","cb094b59":"markdown","b9f8ec3b":"markdown","d95d41fa":"markdown","f402787c":"markdown","2cf9f0d8":"markdown","4fa709fc":"markdown","2f6b01f1":"markdown","174ce96a":"markdown","ca5b3619":"markdown","510be173":"markdown","93512be0":"markdown","c0a6c142":"markdown","06684832":"markdown","721f03b2":"markdown","3a6a88c6":"markdown","76341254":"markdown","e12cf424":"markdown","f888390e":"markdown","d4937971":"markdown","aa59957b":"markdown","f60017ff":"markdown","2918a794":"markdown","276a467f":"markdown","55a60037":"markdown","f45b3c65":"markdown","87208dd6":"markdown","a8e72687":"markdown","904f5259":"markdown","8d3ea7bd":"markdown","b3236ae4":"markdown","86fa7f10":"markdown","51e22194":"markdown","2e86a0a4":"markdown","8ea254ef":"markdown","995104b7":"markdown","38c77d01":"markdown","8d8772c2":"markdown","ddf1ba5c":"markdown","ed85846a":"markdown","ad8a7e9b":"markdown","55a2a824":"markdown","78a25fe1":"markdown","23363c1e":"markdown","63c57414":"markdown","a8407ff2":"markdown","8aff4461":"markdown","53b6be08":"markdown","c2c7d425":"markdown","cb570b3e":"markdown","8828c319":"markdown","9307889f":"markdown"},"source":{"ff363524":"import pandas as pd\nimport numpy as np\n#from bs4 import BeautifulSoup\nimport requests\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords,wordnet\nfrom nltk.tokenize import word_tokenize\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport re\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nfrom nltk import TreebankWordTokenizer, SnowballStemmer, pos_tag\nimport os\nfrom sklearn.model_selection import GridSearchCV","d2e2d758":"train = pd.read_csv('..\/input\/train.csv')\ntest  = pd.read_csv('..\/input\/test.csv')","707b923d":"plt.figure(figsize=(17, 5))\nplt.title(\"Count of Personality Types\")\nplt.xlabel(\" \")\nplt.ylabel(\" \")\nsns.barplot(train['type'].value_counts().index, train['type'].value_counts(),palette = 'winter')\nplt.show()","00e25977":"from sklearn.utils import resample\ns = ['ENTJ', 'ENFJ','ESFP',\n       'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESTJ', 'ESFJ']\ntrain_major = train[train['type'] == 'ENFP']\nfor a in s:\n    train_min = train[train['type'] == a ]\n    train_sam = resample(train_min ,replace = True, n_samples =  int(train['type'].value_counts().mean()),random_state = 123)\n    train_major = pd.concat([train_major,train_sam])\n    \nt = ['INTJ','ENTP','INTP','INFP','INFJ'] \nfor b in t :\n    train_major_ds  = train[train['type'] == b ]\n    train_sam = resample(train_major_ds ,replace = False, n_samples =  int(train['type'].value_counts().mean()),random_state = 123)\n    train_major = pd.concat([train_major,train_sam])\n    \n# Upsampling\n\nplt.figure(figsize=(17, 5))\nplt.title(\"Resampling the Personality\")\nplt.xlabel(\" \")\nplt.ylabel(\" \")\nsns.barplot(train_major['type'].value_counts().index, train_major['type'].value_counts(),palette = 'winter')\nplt.show()","aee6f4a3":"# Mind class\ntrain['Mind']   = train['type'].apply(lambda s : s[0])\ntrain['Mind']   = train['Mind'].map({'I': 0,'E':1})\n\n#Energy class\ntrain['Energy'] = train['type'].apply(lambda s : s[1])\ntrain['Energy'] = train['Energy'].map({'S': 0,'N':1})\n\n#Nature\ntrain['Nature'] = train['type'].apply(lambda s : s[2])                      \ntrain['Nature'] = train['Nature'].map({'F': 0,'T':1})\n\n#Tactic class\ntrain['Tactic'] = train['type'].apply(lambda s : s[3])\ntrain['Tactic'] = train['Tactic'].map({'P': 0,'J':1})","828075c7":"#Split the train data\ntrain['posts'] = train['posts'].apply(lambda s: ' '.join(s.split('|||')))\n#split the test data\ntest['posts'] = test['posts'].apply(lambda s: ' '.join(s.split('|||')))","ec4e1252":"train['number_of_links'] = train['posts'].str.count('http|https')","02ea43bc":"train['social_media_presence'] = train['posts'].str.count('facebook.com|face.bo|twitter.com|instagram.com|tumblr.com')","df5b99f9":"train['number_of_videos'] = train['posts'].str.count('youtube|vimeo|videobash')","43ee5307":"train['number_of_blogs'] = train['posts'].str.count('blog|wordpress')","a45b897b":"d = ('.png.|.jpg.|.gif|.tiff|.psd|.raw|.indd|.pdf|tinypic|.imageshack|')\ntrain['number_of_images'] = train['posts'].str.count(d)","bbfd6a15":"train.head()","0b5cbeee":"plt.figure(figsize = (15,10))\nsns.swarmplot('type','number_of_images', data = train,palette = 'winter')","eeb0a302":"plt.figure(figsize = (15,10))\nsns.swarmplot('type','social_media_presence', data = train,palette = 'winter')","3e31397e":"plt.figure(figsize = (15,10))\nsns.swarmplot('type','number_of_videos', data = train,palette = 'winter')","7fd898dd":"plt.figure(figsize = (15,10))\nsns.swarmplot('type','number_of_blogs', data = train,palette = 'winter')","f1f85ca9":"plt.figure(figsize = (15,12))\nsns.swarmplot('type','number_of_links', data = train,palette = 'winter')","3ff4bc8a":"train['posts'] = train['posts'].apply(lambda b : ''.join([ 'Photos' if s.endswith(d) else s for s in b]))\ntest['posts'] = test['posts'].apply(lambda b : ''.join([ 'photos' if s.endswith(d) else s for s in b]))","87ea987d":"# Replace Web links\ntrain['posts'] = train['posts'].apply(lambda s : ''.join([re.sub(r'http\\S+', r'link', s)]))\ntest['posts'] = test['posts'].apply(lambda s : ''.join([re.sub(r'http\\S+', r'link', s)]))","1561c9f2":"train['posts'] = train['posts'].apply(lambda s: ' '.join(s.split('::')))\ntrain['posts'] = train['posts'].apply(lambda s: ' '.join(s.split('-')))\n\ntest['posts'] = test['posts'].apply(lambda s: ' '.join(s.split('::')))\ntest['posts'] = test['posts'].apply(lambda s: ' '.join(s.split('-')))","d0306fa3":"def preprocessing(text):\n    text2 = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in text]).split())\n    text2 = \" \".join(\"\".join([\" \" if ch in string.digits else ch for ch in text2]).split())\n    tokens = [word for sent in nltk.sent_tokenize(text2) for word in nltk.word_tokenize(sent)]\n    tokens = [word.lower() for word in tokens]\n    stopwds = stopwords.words('english')\n    tokens = [token for token in tokens if token not in stopwds]\n    tokens = [word for word in tokens if len(word)>=3]\n    stemmer = SnowballStemmer('english')\n    tokens = [stemmer.stem(word) for word in tokens]\n    tagged_corpus = pos_tag(tokens)\n    Noun_tags = ['NN','NNP','NNPS','NNS']\n    Verb_tags = ['VB','VBD','VBG','VBN','VBP','VBZ']\n    lemmatizer = WordNetLemmatizer()\n    def prat_lemmatize(token,tag):\n        if tag in Noun_tags:\n            return lemmatizer.lemmatize(token,'n')\n        elif tag in Verb_tags:\n            return lemmatizer.lemmatize(token,'v')\n        else:\n            return lemmatizer.lemmatize(token,'n')\n    pre_proc_text = \" \".join([prat_lemmatize(token,tag) for token,tag in tagged_corpus])\n    return pre_proc_text","6e67f3f5":"train.posts = train.posts.apply(preprocessing)\ntest.posts = test.posts.apply(preprocessing)","6f15ce9d":"intro = ['INFJ', 'INTP', 'INTJ', 'INFP', 'ISFP', 'ISTP', 'ISFJ', 'ISTJ']\n\nfor i in range(1, 9):\n    for a in intro:\n        plt.figure(figsize=(12, 8))\n        \n        df_intro = train[train.type == a]\n        wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(str(df_intro))\n        plt.title(a)\n        plt.imshow(wordcloud, interpolation='bilinear', aspect='equal')\n        plt.axis(\"off\")\n        plt.show()\n    break","616ef10c":"entro = ['ENFJ', 'ENTP', 'ENTJ', 'ENFP', 'ESFP', 'ESTP', 'ESFJ', 'ESTJ']\n\nfor i in range(1, 9):\n    for a in entro:\n        plt.figure(figsize=(12, 8))\n        \n        df_entro = train[train.type == a]\n        wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(str(df_entro))\n        plt.title(a)\n        plt.imshow(wordcloud, interpolation='bilinear', aspect='equal')\n        plt.axis(\"off\")\n        plt.show()\n    break","c6ed13de":"train.head()","c50140a8":"# tfidvect_sub = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_df=0.7)#, min_df=0.05)\n# X_train_tfidvect_sub = tfidvect_sub.fit_transform(train['stem'])\n# X_test_tfidvect_sub = tfidvect_sub.transform(test['stem'])","fa531cf7":"train.head(n=1)","e48e4081":"# Importing TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# Instatiating TfidfVectorizer\ntfidfvectorizer = TfidfVectorizer(max_df= 0.7,stop_words='english')\n# Fit transform\nX_tfi = tfidfvectorizer.fit_transform(train['posts'])\n# Setting the four classes\ny_mind = train['Mind']\ny_enegry = train['Energy']\ny_nature = train['Nature']\ny_tactic = train['Tactic']","00d98127":"# Importing CountVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n# Instatiating CountVectorizer\ncountvectorizer = CountVectorizer(max_df= 0.7,stop_words='english')\n# Fit transform\nX_countvectorizer = countvectorizer.fit_transform(train['posts'])","f923fef4":"logistic_reg = LogisticRegression()\nnbayes = MultinomialNB()\nprint('logistic Regression Parameters: ', logistic_reg.get_params())\nprint()\nprint('MultinomialNB: ', nbayes.get_params())","4eaf10b0":"# Instantiate the GridSearchCV for Logistic Regression\nlogr_parameters = {'penalty' : ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10],'class_weight': [None, 'balanced']}\ngscv_log = GridSearchCV(logistic_reg,logr_parameters)\n# Instantiate the GridSearchCV for MultinomialNB\nnbayes_parameters = {'alpha' : [0.1, 1, 0.1]}\ngscv_naive = GridSearchCV(nbayes,nbayes_parameters)","d86d629d":"X_train,X_test,y_train,y_test = train_test_split(X_tfi,y_enegry, test_size = 0.30,random_state = 0)","a29b5ef2":"# LOGISTIC REGRESSION\nlogistic_reg.fit(X_train,y_train) \n# Evaluating Performance\nprint( 'Train score Logistic Regression ',logistic_reg.score(X_train,y_train))\ny_p = logistic_reg.predict(X_test)\nprint('Logr Classification Report: ', classification_report(y_test,y_p))\n\n# MULTINOMIALND\nnbayes.fit(X_train,y_train)\n# Evaluating Performance\nprint('Train score Random Forest ',nbayes.score(X_train,y_train))\ny_p = nbayes.predict(X_test)\nprint('RF Classification Report: ', classification_report(y_test,y_p))","95bc528a":"# LOGISTIC REGRESSION\ngscv_log.fit(X_train,y_train) \nprint('Logr Train score :',gscv_log.score(X_train,y_train))\ny_p = gscv_log.predict(X_test)\nprint('Logr Accuracy :',accuracy_score(y_test,y_p))\nprint('Logr Best Parameters: ', gscv_log.best_params_)\n\n# MUILTINOMILNB\ngscv_naive.fit(X_train,y_train) \nprint('nbayes Train score :',gscv_naive.score(X_train,y_train))\ny_p = gscv_naive.predict(X_test)\nprint('nbayes Accuracy :',accuracy_score(y_test,y_p))\nprint('nbayes Best Parameters: ', gscv_naive.best_params_)","5f89582b":"X_train,X_test,y_train,y_test = train_test_split(X_tfi,y_enegry, test_size = 0.30,random_state = 0)\n# fitting the regressor\nlogistic_reg_tuned_E = LogisticRegression(penalty='l2', class_weight = 'balanced', C=1)\nlogistic_reg_tuned_E.fit(X_train,y_train) \n# Evaluating Performance\nprint( 'Train score Logistic Regression ',logistic_reg_tuned_E.score(X_train,y_train))\ny_p = logistic_reg_tuned_E.predict(X_test)\nprint('Logr Classification Report: ', classification_report(y_test,y_p))\n\n# fitting the random forest\nnbayes_tuned = MultinomialNB(alpha=1)\nnbayes_tuned.fit(X_train,y_train)\n# Evaluating Performance\nprint( 'Train score Random Forest ',nbayes_tuned.score(X_train,y_train))\ny_p = nbayes.predict(X_test)\nprint('RF Classification Report: ', classification_report(y_test,y_p))","42916ae3":"X_train,X_test,y_train,y_test = train_test_split(X_tfi,y_mind, test_size = 0.30,random_state = 0)","4d5706ab":"# LOGISTIC REGRESSION\nlogistic_reg.fit(X_train,y_train) \n# Evaluating Performance\nprint( 'Train score Logistic Regression ',logistic_reg.score(X_train,y_train))\ny_p = logistic_reg.predict(X_test)\nprint('Logr Classification Report: ', classification_report(y_test,y_p))\n\n# MULTINOMIALND\nnbayes.fit(X_train,y_train)\n# Evaluating Performance\nprint('Train score Random Forest ',nbayes.score(X_train,y_train))\ny_p = nbayes.predict(X_test)\nprint('RF Classification Report: ', classification_report(y_test,y_p))","49d06fd2":"# LOGISTIC REGRESSION\ngscv_log.fit(X_train,y_train) \nprint('Logr Train score :',gscv_log.score(X_train,y_train))\ny_p = gscv_log.predict(X_test)\nprint('Logr Accuracy :',accuracy_score(y_test,y_p))\nprint('Logr Best Parameters: ', gscv_log.best_params_)\n\n# MUILTINOMILNB\ngscv_naive.fit(X_train,y_train) \nprint('nbayes Train score :',gscv_naive.score(X_train,y_train))\ny_p = gscv_naive.predict(X_test)\nprint('nbayes Accuracy :',accuracy_score(y_test,y_p))\nprint('nbayes Best Parameters: ', gscv_naive.best_params_)","33a9e4b1":"X_train,X_test,y_train,y_test = train_test_split(X_tfi,y_mind, test_size = 0.30,random_state = 0)\n# fitting the regressor\nlogistic_reg_tuned_M = LogisticRegression(penalty='l2',class_weight ='balanced', C=10)\nlogistic_reg_tuned_M.fit(X_train,y_train) \n# Evaluating Performance\nprint( 'Train score Logistic Regression ',logistic_reg_tuned_M.score(X_train,y_train))\ny_p = logistic_reg_tuned_M.predict(X_test)\nprint('Logr Classification Report: ', classification_report(y_test,y_p))\n\n# fitting the random forest\nnbayes_tuned = MultinomialNB(alpha=1)\nnbayes_tuned.fit(X_train,y_train)\n# Evaluating Performance\nprint( 'Train score Random Forest ',nbayes_tuned.score(X_train,y_train))\ny_p = nbayes.predict(X_test)\nprint('RF Classification Report: ', classification_report(y_test,y_p))","cc4d39c1":"X_train,X_test,y_train,y_test = train_test_split(X_tfi,y_nature, test_size = 0.30,random_state = 0)","7a42eb28":"# LOGISTIC REGRESSION\nlogistic_reg.fit(X_train,y_train) \n# Evaluating Performance\nprint( 'Train score Logistic Regression ',logistic_reg.score(X_train,y_train))\ny_p = logistic_reg.predict(X_test)\nprint('Logr Classification Report: ', classification_report(y_test,y_p))\n\n# MULTINOMIALND\nnbayes.fit(X_train,y_train)\n# Evaluating Performance\nprint('Train score Random Forest ',nbayes.score(X_train,y_train))\ny_p = nbayes.predict(X_test)\nprint('RF Classification Report: ', classification_report(y_test,y_p))","fe960400":"# LOGISTIC REGRESSION\ngscv_log.fit(X_train,y_train) \nprint('Logr Train score :',gscv_log.score(X_train,y_train))\ny_p = gscv_log.predict(X_test)\nprint('Logr Accuracy :',accuracy_score(y_test,y_p))\nprint('Logr Best Parameters: ', gscv_log.best_params_)\n\n# MUILTINOMILNB\ngscv_naive.fit(X_train,y_train) \nprint('nbayes Train score :',gscv_naive.score(X_train,y_train))\ny_p = gscv_naive.predict(X_test)\nprint('nbayes Accuracy :',accuracy_score(y_test,y_p))\nprint('nbayes Best Parameters: ', gscv_naive.best_params_)","d4280a74":"X_train,X_test,y_train,y_test = train_test_split(X_tfi,y_nature, test_size = 0.30,random_state = 0)\n# fitting the regressor\nlogistic_reg_tuned_N = LogisticRegression(penalty='l2',class_weight = 'balanced' , C=10)\nlogistic_reg_tuned_N.fit(X_train,y_train) \n# Evaluating Performance\nprint( 'Train score Logistic Regression ',logistic_reg_tuned_N.score(X_train,y_train))\ny_p = logistic_reg_tuned_N.predict(X_test)\nprint('Logr Classification Report: ', classification_report(y_test,y_p))\n\n# fitting the random forest\nnbayes_tuned = MultinomialNB(alpha=1)\nnbayes_tuned.fit(X_train,y_train)\n# Evaluating Performance\nprint( 'Train score Random Forest ',nbayes_tuned.score(X_train,y_train))\ny_p = nbayes.predict(X_test)\nprint('RF Classification Report: ', classification_report(y_test,y_p))","b2c4efae":"X_train,X_test,y_train,y_test = train_test_split(X_tfi,y_tactic, test_size = 0.30,random_state = 0)","eeb93de9":"# LOGISTIC REGRESSION\nlogistic_reg.fit(X_train,y_train) \n# Evaluating Performance\nprint( 'Train score Logistic Regression ',logistic_reg.score(X_train,y_train))\ny_p = logistic_reg.predict(X_test)\nprint('Logr Classification Report: ', classification_report(y_test,y_p))\n\n# MULTINOMIALND\nnbayes.fit(X_train,y_train)\n# Evaluating Performance\nprint('Train score Random Forest ',nbayes.score(X_train,y_train))\ny_p = nbayes.predict(X_test)\nprint('RF Classification Report: ', classification_report(y_test,y_p))","bc240ad9":"# LOGISTIC REGRESSION\ngscv_log.fit(X_train,y_train) \nprint('Logr Train score :',gscv_log.score(X_train,y_train))\ny_p = gscv_log.predict(X_test)\nprint('Logr Accuracy :',accuracy_score(y_test,y_p))\nprint('Logr Best Parameters: ', gscv_log.best_params_)\n\n# MUILTINOMILNB\ngscv_naive.fit(X_train,y_train) \nprint('nbayes Train score :',gscv_naive.score(X_train,y_train))\ny_p = gscv_naive.predict(X_test)\nprint('nbayes Accuracy :',accuracy_score(y_test,y_p))\nprint('nbayes Best Parameters: ', gscv_naive.best_params_)","d35a6214":"X_train,X_test,y_train,y_test = train_test_split(X_tfi,y_tactic, test_size = 0.30,random_state = 0)\n# fitting the regressor\nlogistic_reg_tuned_T = LogisticRegression(penalty='l2', class_weight = 'balanced', C=1)\nlogistic_reg_tuned_T.fit(X_train,y_train) \n# Evaluating Performance\nprint( 'Train score Logistic Regression ',logistic_reg_tuned_T.score(X_train,y_train))\ny_p = logistic_reg_tuned_T.predict(X_test)\nprint('Logr Classification Report: ', classification_report(y_test,y_p))\n\n# fitting the random forest\nnbayes_tuned = MultinomialNB(alpha=1)\nnbayes_tuned.fit(X_train,y_train)\n# Evaluating Performance\nprint( 'Train score Random Forest ',nbayes_tuned.score(X_train,y_train))\ny_p = nbayes.predict(X_test)\nprint('RF Classification Report: ', classification_report(y_test,y_p))","2f5254ec":"logistic_reg = LogisticRegression(class_weight = 'balanced', )","0cb5f06c":"# Main Prediction\nX_test_m = tfidfvectorizer.transform(test['posts'])\n# Mind\nlogistic_reg.fit(X_tfi,y_mind)\ny_mind_pred = logistic_reg.predict(X_test_m)","0c35da01":"# Main Prediction\n\n# Energy\nlogistic_reg.fit(X_tfi,y_enegry)\ny_energy_pred = logistic_reg.predict(X_test_m)","d8ed5220":"# Main Prediction\n\n# Tactics\nlogistic_reg.fit(X_tfi,y_tactic)\ny_tactic_pred = logistic_reg.predict(X_test_m)","6ad3243a":"#Main Prediction\n# Nature\nlogistic_reg.fit(X_tfi,y_nature)\ny_nature_pred = logistic_reg.predict(X_test_m)","ec0a7a82":"df = pd.DataFrame({'Id':test['id'],'mind':y_mind_pred,'energy':y_energy_pred,'nature':y_nature_pred,'tactics':y_tactic_pred})\n\ndf.to_csv('out_csv.csv',index = False)","a46a14a6":"### Instantiating GridSearchCV\nhyperparameter tuning refers to choosing a set of optimal hyperparameters for a machine learning algorithm. \nGrid search is a common tuning Strategy used to perform hyperparameter optimization. It works by searching exhaustively through a specified subset of hyperparameters.\nThe benefit of grid search is that it is guaranteed to find the optimal combination of parameters supplied.\n\nSource: https:\/\/towardsdatascience.com\/hyperparameter-tuning-c5619e7e6624","860694c4":"###  Instatiating Logistic regressor and MultinomialNB\nGet the parameters","12968b91":"#### Social Media","cb094b59":"#### Base Model","b9f8ec3b":"#### Train with parameters","d95d41fa":"Data not normally distrubuted across all personality types, we might consider resampling the data  (***upsample or downsample***).","f402787c":"##### Spliting texts in the posts column","2cf9f0d8":"1. Keeping only the words with length greater than 3 in the following code for removing small\n    words which hardly consists of much of a meaning to carry;\n    \n2. Stemming applied on the words using Porter stemmer which stems the extra suffixes from\n    the words\n    \n3. POS tagging is a prerequisite for lemmatization, based on whether word is noun or verb or\n    and so on. it will reduce it to the root word\n   \n4. pos_tag function returns the part of speed in four formats for Noun and six formats for\n    verb. NN - (noun, common, singular), NNP - (noun, proper, singular), NNPS - (noun,\n    proper, plural), NNS - (noun, common, plural), VB - (verb, base form), VBD - (verb, past\n    tense), VBG - (verb, present participle), VBN - (verb, past participle), VBP - (verb, present\n    tense, not 3rd person singular), VBZ - (verb, present tense, third person singular)\n\n5. The following function, prat_lemmatize, has been created only for the reasons of\n    mismatch between the pos_tag function and intake values of lemmatize function. If the\n    tag for any word falls under the respective noun or verb tags category, n or v will be\n    applied accordingly in lemmatize function:","4fa709fc":"###### Resampling data will not be considered going forward since it yield negative results","2f6b01f1":"# 3. Exploratory Data Visualization","174ce96a":"# 1. Import Libraries","ca5b3619":"#### Train , Test and Evaluate with best parameters","510be173":"#### Stem and lematize\n\nStemming is a normalization process that involves reducing a word to its root form. for example, the root word for the following words \"programs,pogramer programing programers\"is program.\nStemming is desirable as it may reduce redundancy as most of the time the word stem and their inflected\/derived words mean the same.\n\nSource:https:\/\/www.geeksforgeeks.org\/python-stemming-words-with-nltk\/\n\n\nLemmatization is the process of converting a word to its base form. Unlike stemming, \nlemmatization considers the context and converts the word to its meaningful base form\n\nSource: https:\/\/www.machinelearningplus.com\/nlp\/lemmatization-examples-python\/","93512be0":"### Number of links","c0a6c142":"#### Train with parameters","06684832":"# Fit Classification Models","721f03b2":"#### Train , Test and Evaluate with best parameters","3a6a88c6":"Text data for 8600 individuals, as well as each of their MBTI designations was scraped from the PersonalityCafe forum, followed by several attempts to manipulate the data according to natural language processing rules and norms.\n\nInitially, the independent 'type' column was split into 4 sub-classes: 'mind', 'nature', 'energy' and 'tactics' to sub-divide the classification system into its' individual components and allow for model predictions to be based on binary target variable classification.\n\nThis allowed for more detailed prediction of the underlying psychological conditions as well as lowering computational time and resource requirements.  \n\nData was then stemmed and lemmatized and passed to a tfidVectorizer to calculate the frequency of words in each document in relation to the total corpus. \n\nPredictions were then performed using both a logistic regression model and multinomial Naive Bayes, and hyperparamater tuning was performed for both models. Using this approach, we conclude that a base logistic regression model using default paramaters was able to provide the most accurate prediction of personality type.","76341254":"### Resampling Data \n","e12cf424":"#### Train with parameters","f888390e":"#### CountVectorizer\nEvery column is term from the corpus and, evry cell represents the frequency count of a term in each document. {formula}","d4937971":"#### Find web links","aa59957b":"### Nature Class","f60017ff":"![NLP](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcR9hkA2TL95UvxuNBOj5t_olTilEKeuri7OG5acD-jq4pLvI_fUFQ)","2918a794":"### Tactic Class","276a467f":"#### Base Model","55a60037":"#### Find videos","f45b3c65":"#### Find images and documents","87208dd6":"#### Train with parameters","a8e72687":"# Conclusion","904f5259":"The Myers-Briggs challenge requires the prediction of a 4 code designation representing personality type from posts and comments made by 8600 indivduals on the PersonalityCafe forum. The posts are a combination of personal texts as well as links visited and shared with others. \n\nThe tested assumption is that a persons' choice of language and preference in specific types of social media activity can reflect their underlying psychological disposition.\n\nThe Myers-Briggs test classifies individuals into 1 of 16 personality types based on 4 contribtuting factors:\n    1. Their degree of interaction with others and their surroundings, being either introverted (I) or extraverted (E).\n    2. Their tendency to focus on objective, fact-based reality (S) or imaginative, subjective experience (N).\n    3. Their tendency towards either logical (T) or emotional (F) behaviour.\n    4. Their need for structure order (J) or for improvisation and flexibility (P).\n\nThe text will be manipulated according to natural language processing(NLP) norms. NLP is an attempt to decipher the vocabulary, syntax, grammar and ultimately the meaning of human speech in a way that can be interpreted by a computer.\n\nOnce the text has been sufficiently modified, predictions will be made using classification machine learning techniques. Classification is the process of predicting a particular class that categorical target variables can fall into (in this case personality type) based on a number of independent variables.\n \nThe following process will be followed to predict the MBTI types accordingly:\n\n    1. EDA to understand the data.\n    2. Cleaning and NLP preparation of text data for modelling.\n    3. Fitting models and evaluating performance.","8d3ea7bd":"### Mind class","b3236ae4":"![Intro](https:\/\/i0.wp.com\/www.careercontinuum.com\/wp-content\/uploads\/2017\/11\/myersbriggs4.jpg?w=750&ssl=1)","86fa7f10":"The above visual shows how the data was resampled","51e22194":"#### TfidfVectorizer (TF-IDF):\nTerm Frequent-Inverse Data Frequency, Term Frequent : gives us the frequency of the word in each document in the corpus. It is the ratio of number of time the word appears in a document compared to the total number of words in that document.It increases as the number of occurrences of that word within the documnents increase. {formula}. IDF : used to calculate the weight of rare words across all documents in the corpus. The words that occur rarely in the corpus have a high IDF score.{formula}","2e86a0a4":"#### Find blogs platforms","8ea254ef":"##### *Generating Classes*","995104b7":"# NLP Feature Engineering\nUnder feature_extraction.text class we import TfidfVectorizer and Countvectorizer","38c77d01":"#### Find images and documents","8d8772c2":"#### Find social media platform","ddf1ba5c":"####  Number of blogs","ed85846a":"#### Base model","ad8a7e9b":"### Energy class","55a2a824":"# Table of content\n1. Import libraries\n2. Import datasets\n3. Data Preprocessing\n    - 1. Resampling Data\n    - 2. Feature Engineering\n4. Exploratory Data Visualization\n5. NLP data Preprocessing\n    - 1. Tokenizing\n    - 2. Stemming and Lemmatizing\n6. NLP Feature Engineering\n7. Missing Data\n8. Feature Engineering\n9. Fit Classification Models\n10. Main Prediction","78a25fe1":"# Introduction\n","23363c1e":"# Main Prediction","63c57414":"### Number of video links","a8407ff2":"#### Logistic Regression \nLogistic Regression is used when the dependent variable is categorical. Types of Logistic Regression : 1. Binary Logistic Regression 2. Multinomial Logistic Regression 3. Ordinal Logistic Regression.\n\nSince we are modelling the probability, $P(X)$ should be greater or equal to 0 and smaller or equal to 1 for it to make sense. We therefore require a function that gives outputs between 0 and 1 for all input values of $X$. For this we use the **logistic function** displayed graphical below:\n\n<img src=\"https:\/\/github.com\/Samantha-movius\/hello-world\/blob\/master\/logistic_reg.png?raw=true\" alt=\"Drawing\" style=\"width: 500px;\"\/>\n\nWhich is defined by the function:\n\n$$P(X) = \\displaystyle \\frac{e^{\\beta_0 + \\beta_1 X}}{1+e^{\\beta_0 + \\beta_1 X}}$$\n\nAfter a bit of manipulation we arrive at:\n\n\\begin{align}\n1 - P(X) &= \\displaystyle \\frac{1}{1+e^{\\beta_0 + \\beta_1 X}} \\\\\n\\therefore \\log \\left( \\frac{P(X)}{1-P(X)} \\right) &= {\\beta_0 + \\beta_1 X}\n\\end{align}\n\nSo the fraction on the left is being modelled as a linear function of the observations $X$, and this is known as the **log odds ratio**. Without the log sign in front of it, it is known simply as the odds ratio. While $P(X)$ is bounded between 0 and 1, the odds ratio is bounded between 0 and $\\infty$. ","8aff4461":"#### Train , Test and Evaluate with best parameters","53b6be08":"#### Train , Test and Evaluate with best parameters","c2c7d425":"### NLP data Preprocessing","cb570b3e":"Creating classes : creating mind class based on  introvert or extrovert, Energy based on Sense and intuition, Nature based on feeling and thinkin and Tactic based perception and judge","8828c319":"# 2. Data Preprocessing","9307889f":"#### Base model"}}