{"cell_type":{"6086d489":"code","46b458c0":"code","2884611b":"code","5d8e488e":"code","045bf916":"code","1a909b5c":"code","77340b21":"code","4a16f8e6":"code","4aa93fd9":"code","3da8598a":"code","a4b7d38b":"code","515355e2":"code","d9a4a228":"markdown","e8aba9d2":"markdown","ac53e2da":"markdown","b9ff3be4":"markdown","2b538759":"markdown"},"source":{"6086d489":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.tree import DecisionTreeClassifier","46b458c0":"class BaggedDecisonTreeClassifier:\n    \n    def __init__(self,num_of_bagged=5):\n        # Initialised with number of bagged models\n        self.num_of_bagged=num_of_bagged\n        \n    def fit(self,X,y):\n        # to store the models\n        self.models=[]\n        for i in range(self.num_of_bagged):\n            indexs=np.random.choice(len(X),size=len(X))# sample with replacement\n            Xi=X[indexs]# Chossing random samples\n            Yi=y[indexs]\n            # Training for each sample bunch by Decision Tree Classifier\n            model=DecisionTreeClassifier()\n            model.fit(Xi,Yi)\n            # Storing the models\n            self.models.append(model)\n            \n    def predict(self,X):\n        pred=np.zeros(len(X))\n        # predicting with each stored models\n        for model in self.models:\n            pred=pred+model.predict(X)\n        return np.round(pred\/self.num_of_bagged) # Model averaging\n    \n    def acc(self,y_true,y_pred):\n        return np.mean(y_true==y_pred)\n        ","2884611b":"from sklearn.datasets import load_breast_cancer","5d8e488e":"data = load_breast_cancer()","045bf916":"X=data.data\ny=data.target","1a909b5c":"from sklearn.model_selection import train_test_split","77340b21":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17)","4a16f8e6":"# Scaling\n# It could be done manually, I used sci-kit learn library\n#from sklearn.preprocessing import StandardScaler\n#sc = StandardScaler()\n#X_train = sc.fit_transform(X_train)\n#X_test=sc.fit_transform(X_test)","4aa93fd9":"# Calling with 10 Decision Trees\nbdtc=BaggedDecisonTreeClassifier(10)","3da8598a":"# Fitting the model\nbdtc.fit(X_train,y_train)","a4b7d38b":"# Predicting with model\ny_pred=bdtc.predict(X_test)","515355e2":"# Calculating Accuracy\nbdtc.acc(y_test,y_pred)","d9a4a228":"### I added the column so, that you can check it out for yourselves.","e8aba9d2":"## Test out code with breast Cancer dataset from sklearn","ac53e2da":"### To get better understanding of Ensemble method, you can checkout Lazyprogrammer course from Udmey\n#### It is not a paid promotion. Just suggestion.","b9ff3be4":"### Tree- based algorithm are scale invariant. This the reason normalizing or standardizing doesnot effect them. \n#### Reason: Intuitively, what decision tree does, it separates data based on condition. So, if one feature is in 10000's scale and other on 10's scale. Decision boundary wont get effeceted by bringing both of them on equal scale.","2b538759":"## Intention of these Notebook is to implement Ensemble method\n### Decision Tree Classifier is not implemented from Scratch"}}