{"cell_type":{"d194b29c":"code","2dfa2955":"code","0e324574":"code","ebf96931":"code","7a76e47b":"code","5659e2ed":"code","943f21f9":"code","7840e4f6":"code","15805713":"code","8b9906e3":"code","1cd80f5e":"code","ef2ea39d":"code","a7b4fea6":"code","e9770bae":"code","71417455":"code","d50b5084":"code","76f427ef":"code","8f3e41bd":"code","f7924e88":"code","cf041095":"code","be7842ba":"code","72b19d71":"code","35801e19":"code","a6891a21":"code","c911ceec":"code","98f4de0f":"code","f8cccbfe":"code","874e5aeb":"code","b13660ed":"code","1d2da573":"code","72f19058":"code","c4f5fc81":"code","5bf2a5b9":"code","273b8e8c":"code","aa330898":"code","5704291e":"code","e917cd2e":"code","a5a6cd6a":"code","5a3ced61":"code","8c9dcc75":"code","0005e60e":"code","109542be":"code","213196c6":"code","c8a7ee47":"code","796b4624":"code","901dca12":"code","8961712a":"code","edb5945c":"code","1f9bb099":"markdown","0b64182b":"markdown","35e69bef":"markdown","41011005":"markdown","e1ccf313":"markdown","7d90209b":"markdown","a8efde43":"markdown","1983704f":"markdown","39df291e":"markdown","35a7606e":"markdown","80da33ad":"markdown","245033d9":"markdown","0252865d":"markdown","447f8ef1":"markdown","33b304d4":"markdown","d30d83c9":"markdown","c550198b":"markdown","f8a75355":"markdown","76aa4b71":"markdown","ec061258":"markdown","c8f2e6c4":"markdown","7d5c4d24":"markdown","a557ee0f":"markdown","360a278b":"markdown","24d4a288":"markdown","fabacd71":"markdown","6cb8148f":"markdown","b7094333":"markdown"},"source":{"d194b29c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Any results you write to the current directory are saved as output.","2dfa2955":"test = pd.read_csv('\/kaggle\/input\/dl50-project1\/Test.csv')\ntrain = pd.read_csv('\/kaggle\/input\/dl50-project1\/Train.csv')\nsample = pd.read_csv('\/kaggle\/input\/dl50-project1\/sample.csv')","0e324574":"import matplotlib.pyplot as plt\nfrom sklearn import preprocessing\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix,accuracy_score\n\n\nimport collections\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom keras.wrappers.scikit_learn import KerasClassifier","ebf96931":"sample.head()","7a76e47b":"train.isna().sum()","5659e2ed":"train[train.isna().any(axis=1)]","943f21f9":"train.duplicated().sum()","7840e4f6":"data = train\ndata.Month.unique()","15805713":"data.VisitorType.unique()","8b9906e3":"def get_dummies(df,test = False):\n    df.Month = df['Month'].map({'Feb' : 2, 'Mar' : 3, 'May' : 5, 'Oct': 10, 'June' : 6, 'Jul' : 7, 'Aug' : 8, 'Nov' : 11, 'Sep' : 9,'Dec' : 12}).astype(int)\n    df.VisitorType = df['VisitorType'].map({'Returning_Visitor' : 2, 'New_Visitor' : 1, 'Other' : 3}).astype(int)\n    df.Weekend = df['Weekend'].map( {True: 1, False: 0} ).astype(int)\n    if test == False:\n        df.Revenue = df['Revenue'].map( {True: 1, False: 0} ).astype(int)\n\nget_dummies(data)\ndata.head()","1cd80f5e":"data = data.drop(['id'], axis=1)","ef2ea39d":"data.max()","a7b4fea6":"def plot_df(data):\n    plt.hist(data.Administrative_Duration,alpha=0.8, label='Administrative Duration')\n    plt.hist(data.Informational_Duration, alpha=0.7, label='Informational Duration')\n    plt.hist(data.ProductRelated_Duration, alpha=0.6, label='Product Related Duration')\n    plt.hist(data.PageValues,alpha=0.5, label='Page Values')\n    plt.legend(loc='upper right')\n    plt.ylabel('Visitor')    ","e9770bae":"plot_df(data)","71417455":"columns = ['Administrative_Duration' , 'Informational_Duration' , 'ProductRelated' , 'ProductRelated_Duration' , 'PageValues']\ndata_scaler = data[columns]\nscaler = preprocessing.MinMaxScaler()\nstd_data = scaler.fit_transform(data_scaler)\nstd_data = pd.DataFrame(std_data,columns=columns)","d50b5084":"plot_df(std_data)","76f427ef":"data[columns] = std_data","8f3e41bd":"imp = IterativeImputer(random_state=0)\ndata_clean = imp.fit_transform(data)\ndata_clean = pd.DataFrame(data_clean , columns =  ['Administrative','Administrative_Duration','Informational','Informational_Duration','ProductRelated','ProductRelated_Duration','BounceRates','ExitRates','PageValues','SpecialDay','Month','OperatingSystems','Browser','Region','TrafficType','VisitorType','Weekend','Revenue'])\ndata_clean[data_clean.isna().any(axis=1)]","f7924e88":"X = data_clean.drop(['Revenue'], axis=1)\ny = data_clean[['Revenue']]\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20)","cf041095":"model = tf.keras.Sequential()\nmodel.add(layers.Dense(128, activation='relu' , kernel_initializer='random_normal', input_dim=17))\nmodel.add(layers.Dense(256, activation='relu' , kernel_initializer='random_normal'))\nmodel.add(layers.Dense(128, activation='relu' , kernel_initializer='random_normal'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.summary() \nmodel.compile(loss='binary_crossentropy', optimizer='Nadam',metrics=['accuracy'])","be7842ba":"model.fit(X_train, y_train,validation_data=(X_test, y_test),batch_size=200,epochs=20)\neval_model=model.evaluate(X_train, y_train)\neval_model","72b19d71":"clstm = tf.keras.Sequential()\nclstm.add(layers.Embedding(300, 300, input_length=17))\nclstm.add(layers.LSTM(200))\nclstm.add(layers.Dense(1, activation='sigmoid'))\nclstm.summary() \nclstm.compile(loss='binary_crossentropy', optimizer='Nadam',metrics=['accuracy'])","35801e19":"clstm.fit(X_train, y_train,validation_data=(X_test, y_test),batch_size=64,epochs=5)\neval_model=clstm.evaluate(X_train, y_train)\neval_model","a6891a21":"dtree = DecisionTreeClassifier()\ndtree = dtree.fit(X_train, y_train)\n\ny_pred = dtree.predict(X_test)\naccuracy = accuracy_score(y_test,y_pred)\naccuracy","c911ceec":"rforest = RandomForestClassifier()\nrforest = rforest.fit(X_train, y_train)\n\ny_pred = rforest.predict(X_test)\naccuracy = accuracy_score(y_test,y_pred)\naccuracy","98f4de0f":"k_scores = []\nfor i in range(1,25) :   \n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn = knn.fit(X_train,y_train)\n    y_pred = knn.predict(X_test)\n    accuracy = accuracy_score(y_test,y_pred)\n    k_scores.append(accuracy)\nnp.mean(k_scores)","f8cccbfe":"svm = SVC()\nsvm = svm.fit(X_train,y_train)\ny_pred = svm.predict(X_test)\naccuracy = accuracy_score(y_test,y_pred)\naccuracy","874e5aeb":"k_fold = KFold(n_splits = 10, shuffle = True, random_state  =0)\nscoring = 'accuracy'","b13660ed":"kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\ncvscores = []\n_X = X.values\n_y = y.values\n\nfor train, test in kfold.split(_X,_y):\n    \n    nn_best = tf.keras.Sequential()\n    nn_best.add(layers.Dense(128, activation='relu' , kernel_initializer='random_normal', input_dim=17))\n    nn_best.add(layers.Dense(256, activation='relu' , kernel_initializer='random_normal'))\n    nn_best.add(layers.Dense(128, activation='relu' , kernel_initializer='random_normal'))\n    nn_best.add(layers.Dense(1, activation='sigmoid'))\n    nn_best.compile(loss='binary_crossentropy', optimizer='Nadam',metrics=['accuracy'])\n    nn_best.fit(_X[train], _y[train],validation_data=(_X[test], _y[test]),batch_size=200,epochs=20,verbose=0)\n    scores=nn_best.evaluate(_X[test], _y[test])\n    print(\"%s: %.2f%%\" % (nn_best.metrics_names[1], scores[1]*100))\n    cvscores.append(scores[1] * 100)\n\n    \nprint(\"%.2f%% (+\/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n\n\n\n","1d2da573":"cvscores","72f19058":"def create_model(optimizer='adam'):\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(128, activation='relu' , kernel_initializer='random_normal', input_dim=17))\n    model.add(layers.Dense(256, activation='relu' , kernel_initializer='random_normal'))\n    model.add(layers.Dense(128, activation='relu' , kernel_initializer='random_normal'))\n    model.add(layers.Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])\n    return model\n\nmodel = KerasClassifier(build_fn=create_model, epochs=20, batch_size=200, verbose=0)\noptimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\nparam_grid = dict(optimizer=optimizer)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid,  cv=3)\ngrid_result = grid.fit(X, y)\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","c4f5fc81":"tree = DecisionTreeClassifier()\nscore = cross_val_score(tree, X, y, cv= k_fold, n_jobs=1, scoring=scoring)\nscore","5bf2a5b9":"rforest = RandomForestClassifier()\nscore = cross_val_score(rforest, X, y, cv= k_fold, n_jobs=1, scoring=scoring)\nscore","273b8e8c":"k_scores = []\nfor i in range(10,50) :   \n    knn = KNeighborsClassifier(n_neighbors = i)\n    score = cross_val_score(knn, X, y, cv = k_fold, scoring = scoring)  \n    k_scores.append(np.mean(score))\nnp.mean(k_scores)","aa330898":"k_range = range(10, 50)\nplt.plot(k_range, k_scores)\nplt.xlabel('Value of K for KNN')\nplt.ylabel('Cross-Validated Accuracy')","5704291e":"svm = SVC()\nscore = cross_val_score(svm, X, y, cv= k_fold, n_jobs=1, scoring=scoring)\nscore","e917cd2e":"param_grid = {'n_neighbors': np.arange(10,20)}\nknn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, param_grid, cv=10,scoring = scoring)\nknn_cv.fit(X, y)\nknn_cv.best_params_","a5a6cd6a":"knn_best = KNeighborsClassifier(n_neighbors = 18)\nscore = cross_val_score(knn_best, X, y, cv = k_fold, n_jobs = 1, scoring = scoring)\nscore","5a3ced61":"# param_grid = {'kernel': ('linear', 'rbf','poly'),'gamma': [0.001, 0.01, 0.1, 1],'C':[0.001, 0.01, 0.1, 1, 10]}\n# svm = SVC()\n# svm_cv = GridSearchCV(svm, param_grid, cv=10)\n# svm_cv.fit(X, y) \n# svm_cv.best_params_","8c9dcc75":"param_grid = {\"max_depth\": [3, None],\"max_features\": [1, 3, 10],\"min_samples_split\": [2, 3, 10],\"bootstrap\": [True, False],\"criterion\": [\"gini\", \"entropy\"]}\nrforest = RandomForestClassifier()\nforest_cv = GridSearchCV(rforest,param_grid,cv=10)\nforest_cv = forest_cv.fit(X,y)\nforest_cv.best_params_","0005e60e":"forest_best = RandomForestClassifier(bootstrap = True,criterion = 'gini',max_depth = 3,max_features =  10,min_samples_split = 10)\nscore = cross_val_score(forest_best, X, y, cv = k_fold, n_jobs = 1, scoring = scoring)\nnp.mean(score)","109542be":"depths = np.arange(1, 21)\nnum_leafs = [1, 5, 10, 20, 50, 100]\nparam_grid = [{'max_depth':depths,'min_samples_leaf':num_leafs}]\ntree = DecisionTreeClassifier()\ntree_cv = GridSearchCV(tree, param_grid, cv=10)\ntree_cv = tree_cv.fit(X,y)\ntree_cv.best_params_","213196c6":"tree_best = DecisionTreeClassifier(max_depth = 4 , min_samples_leaf = 50)\nscore = cross_val_score(tree_best, X, y, cv = k_fold, n_jobs = 1, scoring = scoring)\nscore\nnp.mean(score)","c8a7ee47":"test = pd.read_csv('\/kaggle\/input\/dl50-project1\/Test.csv')\ndata_test = test\n\ndata_test = data_test.drop(['id'], axis=1)\n\nget_dummies(data_test,test = True)\ndata_test.head()","796b4624":"columns = ['Administrative_Duration' , 'Informational_Duration' , 'ProductRelated' , 'ProductRelated_Duration' , 'PageValues']\ndata_scaler = data_test[columns]\nscaler = preprocessing.MinMaxScaler()\ntest_std_data = scaler.fit_transform(data_scaler)\ntest_std_data = pd.DataFrame(test_std_data,columns=columns)\ndata_test[columns] = test_std_data\ndata_test.head()","901dca12":"tree_best.fit(X, y)\nprediction = tree_best.predict(data_test)\nsubmission = test[['id']]\nsubmission['Revenue'] = prediction\n#submission.Revenue = submission['Revenue'].map( {1: True, 0 : False} ).astype(bool)\nsubmission.to_csv(\"submission_tree.csv\", index = False)","8961712a":"forest_best.fit(X, y)\nprediction = forest_best.predict(data_test)\n\nsubmission = test[['id']]\nsubmission['Revenue'] = prediction\n#submission.Revenue = submission['Revenue'].map( {1: True, 0 : False} ).astype(bool)\nsubmission.to_csv(\"submission_forest.csv\", index = False)","edb5945c":"nn_best = tf.keras.Sequential()\nnn_best.add(layers.Dense(128, activation='relu' , kernel_initializer='random_normal', input_dim=17))\nnn_best.add(layers.Dense(256, activation='relu' , kernel_initializer='random_normal'))\nnn_best.add(layers.Dense(128, activation='relu' , kernel_initializer='random_normal'))\nnn_best.add(layers.Dense(1, activation='sigmoid'))\nnn_best.compile(loss='binary_crossentropy', optimizer='Nadam',metrics=['accuracy'])\nnn_best.fit(X,y,batch_size=200,epochs=20,verbose=0)\n    \nnn_best.fit(X, y)\nprediction = nn_best.predict(data_test)\nsubmission = test[['id']]\nsubmission['Revenue'] = prediction\n#submission.Revenue = submission['Revenue'].map( {1: True, 0 : False} ).astype(bool)\nsubmission.to_csv(\"submission_nn.csv\", index = False)","1f9bb099":"Normalisation with sklearn for Administrative_Duration , Informational_Duration , ProductRelated , ProductRelated_Duration , PageValues","0b64182b":"## SVM","35e69bef":"# Best Classification is Random Forest","41011005":"### fix missing data","e1ccf313":"## Random Forest","7d90209b":"### Spliting data to train 80% test 20%","a8efde43":"### Read data","1983704f":"### knn best 18 neighbors","39df291e":"## Encoding dummy variables\n### Month ('Feb' : 2, 'Mar' : 3, 'May' : 5, 'Oct': 10, 'June' : 6, 'Jul' : 7, 'Aug' : 8, 'Nov' : 11, 'Sep' : 9,'Dec' : 12)\n### VisitorType  ('Returning_Visitor' : 2, 'New_Visitor' : 1, 'Other' : 3)\n### boolean True = 1 and False = 0","35a7606e":"## KNN","80da33ad":"# GridSearchCV","245033d9":"## KNN","0252865d":"## normalize test","447f8ef1":"## two hidden layer ","33b304d4":"## Tree","d30d83c9":"## Random Forest","c550198b":"## clean test for prediction","f8a75355":"## k-folds","76aa4b71":"# Neural Networks","ec061258":"## Tree","c8f2e6c4":"## SVM","7d5c4d24":"## test NN models","a557ee0f":"## Decision tree","360a278b":"## KNN","24d4a288":"## save submission","fabacd71":"## SVM","6cb8148f":"## LSTM","b7094333":"## Random Forest"}}