{"cell_type":{"5e3bf9e1":"code","761d64d7":"code","9820f314":"code","1423b0c9":"code","8728b486":"code","aefb3128":"code","ad579dcb":"code","dab9a9dc":"code","c3c9b2be":"markdown","ce244d76":"markdown","e9de83d4":"markdown","f5ee0ae6":"markdown","baf16bdb":"markdown","abc64499":"markdown","64f75019":"markdown"},"source":{"5e3bf9e1":"# importing the requirements\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nfrom urllib.request import Request, urlopen\nimport time\nimport os\nimport shutil","761d64d7":"base_link1 = \"https:\/\/myanimelist.net\/character.php?letter=\"   # base path for every letter (A-Z)\nbase_link2 = \"&show=\"             # base path for every sub pages of given letter (50 anime characters per page)\nletters_ls = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\nbase_links = [base_link1 + i for i in letters_ls]\n\n# these are the number of pages per each alphabet. \n#For exmaple the anime characters with starting letter \"Z\" are present in 7 pages \n#(50 per each page and last page may not contain 50 images)\nnum_dict = {}\nnum_dict[\"Z\"] = 7\nnum_dict[\"Y\"] = 67\nnum_dict[\"X\"] = 2\nnum_dict[\"W\"] = 22\nnum_dict[\"V\"] = 13\nnum_dict[\"U\"] = 23\nnum_dict[\"T\"] = 122\nnum_dict[\"S\"] = 186\nnum_dict[\"R\"] = 29\nnum_dict[\"Q\"] = 2\nnum_dict[\"P\"] = 12\nnum_dict[\"O\"] = 62\nnum_dict[\"N\"] = 74\nnum_dict[\"M\"] = 146\nnum_dict[\"L\"] = 26\nnum_dict[\"K\"] = 206\nnum_dict[\"J\"] = 16\nnum_dict[\"I\"] = 61\nnum_dict[\"H\"] = 115\nnum_dict[\"G\"] = 29\nnum_dict[\"F\"] = 41\nnum_dict[\"E\"] = 20\nnum_dict[\"D\"] = 26\nnum_dict[\"C\"] = 31\nnum_dict[\"B\"] = 32\nnum_dict[\"A\"] = 103\n\nprint(sum(list(num_dict.values())))","9820f314":"final_links = []\n\nfor num,letter in enumerate(letters_ls):\n    for i in range(num_dict[letter]):\n        if i==0:\n            final_links.append(base_links[num])\n        else:\n            final_links.append(base_links[num]+base_link2+str(i*50))\nlen(final_links)","1423b0c9":"a_tags = []\ntime.sleep(180)\nfor i,link in enumerate(final_links):\n    headers = {'User-Agent': 'Mozilla\/5.0 (X11; Linux x86_64) AppleWebKit\/537.11 (KHTML, like Gecko) Chrome\/23.0.1271.64 Safari\/537.11',\n         'Accept': 'text\/html,application\/xhtml+xml,application\/xml;q=0.9,*\/*;q=0.8',\n         'Referer': 'https:\/\/cssspritegenerator.com',\n         'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n         'Accept-Encoding': 'none',\n         'Accept-Language': 'en-US,en;q=0.8',\n         'Connection': 'keep-alive'}\n    req = Request(link , headers=headers)\n    webpage = urlopen(req).read()\n    soup = BeautifulSoup(webpage, \"html.parser\")\n    links = soup.findAll(\"a\")\n    for l in links:\n        ls = []\n        if l.img:\n            a_tags.append(l)\n    if i%150==0 and i!=0:\n        print(\"Completed {} links and waiting for 300 seconds\".format(i))\n        time.sleep(300)\n        print(\"Waiting completed, going for next set of 150 links\")","8728b486":"# This code gives you the actual links form a tag that we got from previous csv file\nanime_links = []\nfor link in a_tags:\n    if \"onclick\" in str(link):\n        a_tags.remove(link)\n    else:\n        anime_links.append(link.attrs[\"href\"])","aefb3128":"# Creating a new folder where we store the images\n\nif os.path.isdir(\".\/dataset\/\") == False:\n    os.mkdir(\".\/dataset\/\")","ad579dcb":"df_links = pd.read_csv(\"..\/input\/anime-names-and-image-generation\/anime_links.csv\")\nanime_links = list(df_links[\"Link\"])\nlen(anime_links)     #72992","dab9a9dc":"\nanime_names = []\n\nfor i,src in enumerate(anime_links):\n    src_new = src.encode('ascii', 'ignore').decode('ascii')       #there are some names where there are characters other than ascii. To reduce complexity iam excluding those\n    if src_new != src:\n        continue\n    req = Request(src , headers=headers)\n    webpage = urlopen(req).read()\n    soup = BeautifulSoup(webpage, \"html.parser\")\n    links = soup.findAll(\"a\")\n    for link in links:\n        if link.img and \"border\" not in str(link) and \"onclick\" not in str(link):  # actual image links\n            split= str(link.img).split(\"\\\"\")\n            img_src = split[5]\n            name = split[1]\n            \n            if img_src.split(\".\")[-1] == \"jpg\" and len(name.split(\"\/\"))==1:\n                split = str(link.img).split(\"\\\"\")\n                response = requests.get(img_src, headers=headers)\n                image_name = \".\/dataset\/\"+name + \".jpg\"\n                file = open(image_name, \"wb\")\n                file.write(response.content)\n                file.close()\n\n    if i%120==0 and i!=0:  # downloading 120 images and waiting for 4 min and then the loop continues.\n        print(\"Completed {} links and waiting for 240 seconds\".format(i))\n        time.sleep(240)\n        print(\"Waiting completed, going for next set of 120 links\\n\\n\")","c3c9b2be":"# ![Anime World](https:\/\/cache.desktopnexus.com\/thumbseg\/2305\/2305029-bigthumbnail.jpg)Welcome to the world of Anime. This code requieres the dataset I created [Anime characters](https:\/\/www.kaggle.com\/shanmukh05\/anime-names-and-image-generation) . You can fork the notebook and can use to extract any other parts of the above mentioned site. Let's dive in.","ce244d76":"Following block gives you final images stored in dataset folder (.\/dataset). You can also extract names of characters form name of each anime image.","e9de83d4":"What I am doing is  othing but web scraping. This notebook is also helpful for learning the web scraping.","f5ee0ae6":"This block will give you links to each page of each letter (ie., 7 links for \"Z\" letter)","baf16bdb":"On running below block you will get final links ie., each link contains details of one anime character. Basically I used \n\n> time.sleep()\n\nto  make sure that there is no much traffic from our side during the web scraping. As we are going through a lot of pages. it is better to use the sleep to make sure that we do not get the [Error 403](https:\/\/stackoverflow.com\/questions\/16627227\/http-error-403-in-python-3-web-scraping\/31758803).","abc64499":"So, the above code gives you the output as a csv file containg the links to every anime character present in [Anime and Manga](https:\/\/myanimelist.net\/character.php). Below code helps you to get the names and images of the anime characters using the above links.","64f75019":"The following cells of code will help you to extract the links to each anime character present in [Anime and Manga](https:\/\/myanimelist.net\/character.php) website."}}