{"cell_type":{"60c90ff7":"code","a72e5e5c":"code","c660a0a6":"code","4a300001":"code","6b06589a":"code","42ce8f5c":"code","8bd1a45c":"code","65c17038":"code","b091382d":"code","8e0f5738":"code","99132ad5":"code","de5967ce":"code","af92cb5d":"code","d98f1d08":"code","df74fe39":"code","89387268":"code","734742b7":"code","96adb00f":"code","031402a4":"code","c1888b9f":"code","45c1f9ee":"code","353b61e7":"code","a71cd9cb":"markdown","0c057f94":"markdown","9cdea968":"markdown","1e4a37d8":"markdown","a0a46ce8":"markdown","cafae8dd":"markdown","3acbd6cf":"markdown","fe232956":"markdown","38223e6e":"markdown","72056827":"markdown","e450a4e2":"markdown","5fa4550b":"markdown","85aacda0":"markdown"},"source":{"60c90ff7":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport torch \ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(device)\n\nimport os, gc, random\nif device == 'cuda':\n    import cudf\n    import cupy as cp\nimport datatable as dtable\nimport pandas as pd\nimport numpy as np\nimport janestreet\nfrom numba import njit\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom joblib import dump, load\n\nimport tensorflow as tf\ntf.random.set_seed(42)\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n\nQUICK_TEST = False","a72e5e5c":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(42)","c660a0a6":"print('Loading...')\nif QUICK_TEST:\n    train = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv', nrows = 10000)\nelse:\n    train = dtable.fread('..\/input\/jane-street-market-prediction\/train.csv').to_pandas()\nfeatures = [c for c in train.columns if 'feature' in c]\n\nprint('Forward-Filling...')\ntrain = train.query('weight > 0').reset_index(drop = True)\ntrain[features] = train[features].fillna(method = 'ffill').fillna(0)\ntrain['action'] = (train['resp'] > 0).astype('int')\n\nprint('Finish.')","4a300001":"def utility_score_loop(date, weight, resp, action):\n    count_i = len(np.unique(date))\n    Pi = np.zeros(count_i)\n    for i, day in enumerate(np.unique(date)):\n        Pi[i] = np.sum(weight[date == day] * resp[date == day] * action[date == day])\n    t = np.sum(Pi) \/ np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 \/ count_i)\n    u = np.clip(t, 0, 6) * np.sum(Pi)\n    return u\n\ndef utility_score_bincount(date, weight, resp, action):\n    count_i = len(np.unique(date))\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) \/ np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 \/ count_i)\n    u = np.clip(t, 0, 6) * np.sum(Pi)\n    return u","6b06589a":"def utility_score_pd(date, weight, resp, action):\n    count_i = len(pd.unique(date))\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) \/ np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 \/ count_i)\n    u = np.clip(t, 0, 6) * np.sum(Pi)\n    return u\n\ndef utility_score_max(date, weight, resp, action):\n    count_i = date.max() + 1\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) \/ np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 \/ count_i)\n    u = np.clip(t, 0, 6) * np.sum(Pi)\n    return u\n\ndef utility_score_last(date, weight, resp, action):\n    count_i = date[-1] + 1\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) \/ np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 \/ count_i)\n    u = np.clip(t, 0, 6) * np.sum(Pi)\n    return u","42ce8f5c":"@njit(fastmath = True)\ndef utility_score_numba(date, weight, resp, action):\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) \/ np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 \/ len(Pi))\n    u = min(max(t, 0), 6) * np.sum(Pi)\n    return u","8bd1a45c":"# LDMTWO's\ndef utility_score_LDMTWO(df, labels='action,.r0,.weight,.date'.split(',')):\n    \"\"\"Calculate utility score of a dataframe according to formulas defined at\n    https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/overview\/evaluation\n    \"\"\"\n    action,resp,weight,date = labels\n    df = df.set_index(date)\n    p = df[weight]  * df[resp] * df[action]\n    p_i = p.groupby(date).sum()\n    t = (p_i.sum() \/ np.sqrt((p_i**2).sum())) * (np.sqrt(250 \/ p_i.index.size))\n    return np.clip(t,0,6) * p_i.sum()\n\n# Jorijn Jacko Smit's\ndef utility_score_Jorijn(df):\n    \"\"\"Calculate utility score of a dataframe according to formulas defined at\n    https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/overview\/evaluation\n    \"\"\"\n\n    df['p'] = df['weight']  * df['resp'] * df['action']\n    p_i = df.set_index('date')['p'].groupby('date').sum()\n    t = (p_i.sum() \/ np.sqrt((p_i**2).sum())) * (np.sqrt(250 \/ p_i.index.size))\n    return min(max(t, 0), 6) * p_i.sum()","65c17038":"date = train['date'].values\nweight = train['weight'].values\nresp = train['resp'].values\naction = train['action'].values","b091382d":"print('numpy for-loop:')\n%timeit utility_score_loop(date, weight, resp, action)\nprint('-' * 70)\nprint('numpy.bincount():')\n%timeit utility_score_bincount(date, weight, resp, action)\nprint('-' * 70)\nprint('numpy.bincount() + pandas.unique():')\n%timeit utility_score_pd(date, weight, resp, action)\nprint('-' * 70)\nprint('numpy.bincount() + date.max() + 1:')\n%timeit utility_score_max(date, weight, resp, action)\nprint('-' * 70)\nprint('numpy.bincount() + date[-1] + 1:')\n%timeit utility_score_last(date, weight, resp, action)\nprint('-' * 70)\nprint('numba:')\n%timeit utility_score_numba(date, weight, resp, action)\nprint('-' * 70)\nprint('LDMTWO\\'s:')\n%timeit utility_score_LDMTWO(train, labels = 'action,resp,weight,date'.split(','))\nprint('-' * 70)\nprint('Jorijn\\'s:')\n%timeit utility_score_Jorijn(train)","8e0f5738":"print(utility_score_numba(date, weight, resp, action))","99132ad5":"X_tr = train.loc[(train['date'] > 80) & (train['date'] <= 370), features]\ny_tr = train.loc[(train['date'] > 80) & (train['date'] <= 370), 'action']\n\nX_tr2 = train.loc[(train['date'] > 370) & (train['date'] <= 400), features]\ny_tr2 = train.loc[(train['date'] > 370) & (train['date'] <= 400), 'action']\n\nX_val = train.loc[train['date'] > 400, features]\ny_val = train.loc[train['date'] > 400, 'action']\n\ndate = train.loc[train['date'] > 400, 'date'].values\nweight = train.loc[train['date'] > 400, 'weight'].values\nresp = train.loc[train['date'] > 400, 'resp'].values\n\nrubbish = gc.collect()","de5967ce":"def create_mlp(num_columns, num_labels, hidden_units, dropout_rates, learning_rate):\n    \n    inp = tf.keras.layers.Input(shape = (num_columns, ))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n    for i in range(len(hidden_units)):\n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i+1])(x)    \n        \n    x = tf.keras.layers.Dense(num_labels)(x)\n    out = tf.keras.layers.Activation('sigmoid')(x)\n    \n    model = tf.keras.models.Model(inputs = inp, outputs = out)\n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate),\n                  loss = tf.keras.losses.BinaryCrossentropy(), \n                  metrics = tf.keras.metrics.AUC(name = 'AUC'), \n                 )\n    \n    return model","af92cb5d":"hidden_units = [384, 896, 384]\ndropout_rates = [0.10143786981358652, 0.19720339053599725, 0.2703017847244654, 0.2357768967777311]\n\nckp_path = 'JSModel.hdf5'\nmodel = create_mlp(X_tr.shape[1], 1, hidden_units, dropout_rates, 1e-3)\nrlr = ReduceLROnPlateau(monitor = 'val_AUC', factor = 0.1, patience = 3, verbose = 1, \n                        min_delta = 1e-4, mode = 'max')\nckp = ModelCheckpoint(ckp_path, monitor = 'val_AUC', verbose = 0, \n                      save_best_only = True, save_weights_only = True, mode = 'max')\nes = EarlyStopping(monitor = 'val_AUC', min_delta = 1e-4, patience = 5, mode = 'max', \n                   baseline = None, restore_best_weights = True, verbose = 0)\nhistory = model.fit(X_tr.values, y_tr.values, validation_data = (X_val.values, y_val.values), epochs = 100, \n                    batch_size = 4096, callbacks = [rlr, ckp, es], verbose = 1)\nhist = pd.DataFrame(history.history)\nprint(hist['val_AUC'].max())\n\ndel model\nK.clear_session()\nrubbish = gc.collect()","d98f1d08":"@njit(fastmath = True)\ndef decision_threshold_optimisation(preds, date, weight, resp, low = 0, high = 1, bins = 100, eps = 1):\n    opt_threshold = low\n    gap = (high - low) \/ bins\n    action = np.where(preds >= opt_threshold, 1, 0)\n    opt_utility = utility_score_numba(date, weight, resp, action)\n    for threshold in np.arange(low, high, gap):\n        action = np.where(preds >= threshold, 1, 0)\n        utility = utility_score_numba(date, weight, resp, action)\n        if utility - opt_utility > eps:\n            opt_threshold = threshold\n            opt_utility = utility\n    print('Optimal Decision Threshold:', opt_threshold)\n    print('Optimal Utility Score:', opt_utility)\n    return opt_threshold, opt_utility","df74fe39":"# Optimise Decision Threshold on the Validation Set\nmodel = create_mlp(X_tr.shape[1], 1, hidden_units, dropout_rates, 1e-5)\nmodel.load_weights(ckp_path)\npreds = model.predict(X_val, batch_size = 4096, verbose = 1).ravel()\nopt_threshold, opt_utility = decision_threshold_optimisation(preds, date, weight, resp, preds.min(), preds.max(), 1000, 1)\n\nrubbish = gc.collect()","89387268":"# # Finetune 3 epochs\n# model.fit(np.concatenate((X_tr2.values, X_val.values)), np.concatenate((y_tr2.values, y_val.values)), \n#           epochs = 3, batch_size = 4096, verbose = 1)\n# model.save_weights(ckp_path)","734742b7":"X_tr = train.loc[train['date'] > 80, features]\ny_tr = train.loc[train['date'] > 80, 'action']\nmodel = create_mlp(X_tr.shape[1], 1, hidden_units, dropout_rates, 1e-4)\nmodel.fit(X_tr.values, y_tr.values, batch_size = 4096, epochs = 10, verbose = 1)\nmodel.save_weights(ckp_path)","96adb00f":"example_test = pd.read_csv('..\/input\/jane-street-market-prediction\/example_test.csv')\nexample_test = example_test.query('weight > 0').reset_index(drop = True)\nexample_test[features] = example_test[features].fillna(method = 'ffill').fillna(0)\ntest_preds = model.predict(example_test[features].values, batch_size = 4096, verbose = 1).ravel()\nprint(test_preds.min())\nprint(test_preds.max())\nprint(test_preds.mean())\nprint(test_preds.std())\nplt.hist(test_preds, bins = 100)\nplt.show()","031402a4":"@njit\ndef fast_fillna(array, values):\n    if np.isnan(array.sum()):\n        array = np.where(np.isnan(array), values, array)\n    return array","c1888b9f":"env = janestreet.make_env()\nenv_iter = env.iter_test()","45c1f9ee":"# Try 0.5 threshold\nopt_threshold = 0.5\ntmp = np.zeros(len(features))\nfor (test_df, pred_df) in tqdm(env_iter):\n    if test_df['weight'].item() > 0:\n        x_tt = test_df.loc[:, features].values\n        x_tt[0, :] = fast_fillna(x_tt[0, :], tmp)\n        tmp = x_tt[0, :]\n        pred = model(x_tt, training = False).numpy().item()\n        pred_df.action = np.where(pred >= opt_threshold, 1, 0).astype(int)\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","353b61e7":"import numpy as np\nfrom numba import njit\n\n@njit(fastmath = True)\ndef utility_score_numba(date, weight, resp, action):\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) \/ np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 \/ len(Pi))\n    u = min(max(t, 0), 6) * np.sum(Pi)\n    return u","a71cd9cb":"We also compare some of the functions in pandas from the discussion forum.","0c057f94":"# Perfect Prediction on Train\n\nWow, we can get a utility score of **224162** if we perfectly predict every action in the train set.","9cdea968":"# Preprocessing","1e4a37d8":"# Conclusion\n\nSo far, the fastest version is the numba version! Tribute to [Calibrator][1]!\n\n[1]: https:\/\/www.kaggle.com\/calibrator","a0a46ce8":"The optimised threshold is very high, it seems the market trend in the last 100 days is decreasing dramatically so the model needs to take fewer actions. We may use cross-validation score instead of one leave-out validation score for a better threshold optimisation result.","cafae8dd":"Calibrator has suggested using `len(Pi)` and `@njit(fastmath = True)` for acceleration. Let's check how it performs!","3acbd6cf":"For-loop version is very slow. We would better replace it with a magic numpy function called `numpy.bincount()`.","fe232956":"Try training on the entire train set.","38223e6e":"Further improvement by changing `numpy.unique()` to `pandas.unique()` because it does not sort the values. However, if your date values are consecutive and chronological, using `date.max() + 1` or `date[-1] + 1` is the optimal solution.","72056827":"# Optimisation Based On Validation","e450a4e2":"# Time-Consumption Comparison","5fa4550b":"# Jane Street: Super Fast Utility Score Function\n\nIn this notebook, I compare the time-consumption of different utility score function implementations from discussion forum [Super Fast Utility Score Function Implementation\n][1].\n\n[1]: https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/discussion\/201257","85aacda0":"# Utility Score Functions"}}