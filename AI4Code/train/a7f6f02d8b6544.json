{"cell_type":{"23b283f2":"code","b759c025":"code","2640276d":"code","e28fcbed":"code","3cf814b8":"code","1b529b4b":"code","86d2ff56":"code","c904009c":"code","80178b58":"code","acc81731":"code","9bf2b3c6":"code","73b0af4e":"code","79f0e781":"code","be5b756b":"code","d47f0eb9":"code","ff703e73":"markdown","1148ad3c":"markdown","4a96f210":"markdown","f7fe3019":"markdown","1e12dc16":"markdown","c9f21f65":"markdown","e55f4936":"markdown","b5fd34f1":"markdown","a857d62b":"markdown"},"source":{"23b283f2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LogNorm\nfrom matplotlib.ticker import PercentFormatter, FuncFormatter\nimport matplotlib.patches as mpatches\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\ndef pre_processing(df,value_for_na = -1):\n    df[\"class\"] = df[\"class\"].map({\"neg\":0,\"pos\":1})\n    df = df.replace(\"na\",value_for_na)\n    for col in df.columns:\n        if col != \"origin\":\n            df[col] = pd.to_numeric(df[col])\n    return df\n\ndef get_tag(name):\n    return name.split(\"_\")[0]","b759c025":"train = pd.read_csv('..\/input\/aps_failure_training_set.csv',)\ntest = pd.read_csv('..\/input\/aps_failure_test_set.csv',)\ntrain[\"origin\"] = \"train\"\ntest[\"origin\"] = \"test\"\ndata= pd.concat([train,test],sort=True)\n#to avoid double index\ndata = data.reset_index()","2640276d":"columns_list = train.columns\n\nall_columns_with_tags = [a for a in columns_list if \"_\" in a]\nall_tags = [get_tag(a) for a in all_columns_with_tags]\nhists = [k for k,v in Counter(all_tags).items() if v == 10]\nhists_columns = [k for k in all_columns_with_tags if get_tag(k) in hists]\nhists_dict = {k:[col for col in hists_columns if k in col] for k in hists if get_tag(k) in hists}\ncounter_columns = [k for k in all_columns_with_tags if get_tag(k) not in hists]","e28fcbed":"data = pre_processing(data,-1)\ndata = data.rename(columns={\"class\":\"Class\"}) #to avoid name collision with the class\n\n","3cf814b8":"fig , axs = plt.subplots(10,7,sharex=True,  sharey='row',figsize=(15,20))\ncmap=plt.cm.get_cmap('Set1', 7)\ndf_top = data[(data.Class==0)&(data.ag_000>0)]\nfor axis,r in zip(axs,df_top.sample(5).iterrows()):\n    for i,(ax,hist) in enumerate(zip(axis,hists)):\n        ax.step(range(10),r[1][f\"{hist}_000\":f\"{hist}_009\"],where=\"mid\",color=cmap(i))\n        ax.tick_params(labelbottom=False)\n        ax.yaxis.set_major_formatter(FuncFormatter(lambda x,pos: \"%.1E\"%x))\n        ax.tick_params(axis='y', which='major', labelsize=10)\ndf_bottom = data[(data.Class==1)&(data[\"ag_000\"]>0)]\nfor axis,r in zip(axs[5:],df_top.sample(5).iterrows()):\n    for i,(ax,hist) in enumerate(zip(axis,hists)):\n        ax.step(range(10),r[1][f\"{hist}_000\":f\"{hist}_009\"],where=\"mid\",color=cmap(i))\n        ax.tick_params(labelbottom=False)\n        ax.yaxis.set_major_formatter(FuncFormatter(lambda x,pos: \"%.1E\"%x))\n        ax.tick_params(axis='y', which='major', labelsize=10)\n\nfor ax,hist in zip(axs[0],hists):\n    ax.set_xlabel(hist.upper())    \n    ax.xaxis.set_label_position('top') \nplt.tight_layout()","1b529b4b":"for hist in hists:\n    data[f\"{hist}_total\"] = sum(data[col] for col in hists_dict[hist])\ndata[\"system_age\"] = data[[f\"{hist}_total\" for hist in hists]].max(axis=1)\ndata = data.replace(-10,-1) #Some totals of -1 to make it easy to detect NA\n#data = data.drop([f\"{hist}_total\" for hist in hists],axis=1)\n\n\nplt.figure(figsize=(15,5));\nfor_plotting = data[data.system_age>=0]\n_,bins,_ = plt.hist(np.log(for_plotting[for_plotting.Class==0].system_age+1),bins=100,density=True,alpha=0.5,label=\"Class 0\");\nplt.hist(np.log(for_plotting[for_plotting.Class==1].system_age+1),bins=bins,density=True,alpha=0.5,label=\"Class 1\");\nplt.legend();\nplt.ylabel(\"Percentage per categorie\");\nplt.xlabel(\"Number of measurements (a.k.a. System Age) for physical (logarithmic scale)\");\nplt.xlim(0,21);\n\n\n\n","86d2ff56":"font = {'family' : 'normal',\n        'weight' : 'normal',\n        'size'   : 12}\nplt.rc('font', **font)\ndf_dist = {}\nfor dist in hists:\n    pds_to_concat = []\n    for i,col in enumerate(hists_dict[dist]):\n        temp = data[[col,'Class']]\n        temp[\"ref\"] = col\n        temp.loc[temp[col]>=0,col] = np.log(temp[col]+1)\n        temp.columns = [\"data\",\"Class\",\"ref\"]\n        pds_to_concat.append(temp)\n    df_dist[dist] = pd.concat(pds_to_concat)\n    df_dist[dist] = df_dist[dist][df_dist[dist].data>=0] # No NA values\n#ag_dist.data = np.log(ag_dist.data+1)\nfig , axs = plt.subplots(7)\nfig.set_figheight(15)\nfig.set_figwidth(15)\n#sns.boxenplot(data=ag_dist,x=\"ref\",y=\"data\",hue=\"class\",outlier_prop=0.00000000001)\nfor i,hist in enumerate(hists):\n    sns.violinplot(data=df_dist[hist],x=\"ref\",y=\"data\",hue=\"Class\",scale=\"width\",scale_hue=True\n                   , split=True,ax=axs[i],legend=(i==0));\nfig.text(0.04, 0.5, 'Distribution of each feature per outcome', va='center', rotation='vertical');\n\n","c904009c":"variable =  \"ag_002\"\nbucket_nb = 20 \n\n_ , bins = pd.qcut(np.log(data[data[variable]>0][variable]),bucket_nb,retbins=True)\ndata[f\"{variable}_buckets\"] = pd.cut(np.log(data[variable]+1),[-0.1] + list(bins),labels=range(bucket_nb+1))\n#pd.crosstab(data[f\"{variable}_buckets\"],data.Class,margins=True)\nplt.figure(figsize=(15,10));\nax = sns.barplot(data=data,x=f\"{variable}_buckets\", y=\"Class\")\nax.yaxis.set_major_formatter(PercentFormatter(1))\nplt.xlabel(\"Distibutions of the feature AG_002\");\nplt.ylabel(\"Percentage of failure per range\");\n","80178b58":"variable =  \"cn_001\"\nbucket_nb = 20 \n_ , bins = pd.qcut(np.log(data[data[variable]>0][variable]),bucket_nb,retbins=True)\ndata[f\"{variable}_buckets\"] = pd.cut(np.log(data[variable]+1),[-0.1] + list(bins),labels=range(bucket_nb+1))\n#pd.crosstab(data[f\"{variable}_buckets\"],data.Class,margins=True)\nplt.figure(figsize=(15,10));\nax = sns.barplot(data=data,x=f\"{variable}_buckets\", y=\"Class\")\nax.yaxis.set_major_formatter(PercentFormatter(1))\nplt.xlabel(\"Distibutions of the feature CN_001\");\nplt.ylabel(\"Percentage of failure per range\");\n\n","acc81731":"\n\nplt.figure(figsize=(20,10))\n\ndf = data[(data.Class==0)&(data.ag_002>0)&(data.cn_001>0)]\ncmap = sns.light_palette(\"blue\", as_cmap=True)\n_,xedges,yedges, _ = plt.hist2d(np.log(df[\"ag_002\"]+2), np.log(df[\"cn_001\"]+1),norm=LogNorm()\n                                , bins=40,cmap=cmap, cmin=1,alpha=0.5,label=\"Sound APS\");\ndf = data[(data.Class==1)&(data.ag_002>0)&(data.cn_001>0)]\n\nclass_0 = mpatches.Patch(color='blue',alpha=0.3, label='Class 0')\nclass_1 = mpatches.Patch(color='red',alpha=0.3, label='Class 1')\nplt.legend(handles=[class_0,class_1],loc=2)\n\ncmap = sns.light_palette(\"red\", as_cmap=True)\nplt.hist2d(np.log(df[\"ag_002\"]+1)\n                , np.log(df[\"cn_001\"]+1), cmap=cmap,bins=[xedges,yedges], cmin=1\n                ,alpha=0.5\n                ,label=\"Failing APS\");\nplt.ylabel(\"AG_002 distribution using a logarithmic scale\");\nplt.xlabel(\"CN_001 distribution using a logarithmic scale\");\n\n","9bf2b3c6":"for hist in hists:\n    data[f\"{hist}_total\"] = sum(data[col] for col in hists_dict[hist])\ndata = data.replace(-10,-1) #Some totals of -1 to make it easy to detect NA\n\n\nfor hist in hists:\n    data[f\"{hist}_avg\"] = 0\n    for col in hists_dict[hist]:\n        data[f\"{col}_density\"] = data[col]\/data[f\"{hist}_total\"]\n        data.loc[data[f\"{hist}_total\"] == -10, f\"{col}_density\"] = -1\n        data.loc[data[f\"{hist}_total\"] == 0, f\"{col}_density\"] = 0\n        data[f\"{hist}_avg\"] += int(col[3:])*data[col]\n    data[f\"{hist}_avg\"] = data[f\"{hist}_avg\"]\/data.system_age\n    data.loc[data[f\"{hist}_total\"] == 0, f\"{hist}_avg\"] = 0\n    data.loc[data[f\"{hist}_total\"] == -1, f\"{hist}_avg\"] = 0\n\ndata = data.drop([f\"{hist}_total\" for hist in hists],axis=1)\n","73b0af4e":"_, bins_for_total_feature = pd.qcut(data[(data[\"origin\"]==\"train\")&(data[\"Class\"]==1)&(data.system_age>0)].system_age,3,retbins=True)\nbins_for_total_feature[3] = np.max(data.system_age)\ndata[\"total_cat\"] = pd.cut(data.system_age.replace(np.nan,-1),[-10.1] + list(bins_for_total_feature)\n                        ,labels=[\"null\",\"low\", \"medium\", \"high\"])\n\n","79f0e781":"pds_to_concat = []\nfor i,hist in enumerate(hists):\n    temp = data[data.total_cat!=\"null\"][[f\"{hist}_avg\",'Class',\"total_cat\"]]\n    temp[\"ref\"] = f\"{hist}_avg\"\n    #temp.loc[temp[f\"{hist}_avg\"]>=0,f\"{hist}_avg\"] = np.log(temp[f\"{hist}_avg\"]+1)\n    temp.columns = [\"data\",\"Class\",\"total_cat\",\"ref\"]\n    pds_to_concat.append(temp)\nall_avg_vals = pd.concat(pds_to_concat)\n","be5b756b":"categories_to_display = data.total_cat.cat.categories[1:]\nplt.rcParams.update({'font.size': 12})\n\n\nfig , axs = plt.subplots(len(categories_to_display))\nfig.set_figheight(15)\nfig.set_figwidth(15)\n#sns.boxenplot(data=ag_dist,x=\"ref\",y=\"data\",hue=\"class\",outlier_prop=0.00000000001)\nfor ax,cat in zip(axs,categories_to_display):\n    g = sns.violinplot(data=all_avg_vals[all_avg_vals.total_cat==cat]\n                  , x=\"ref\",y=\"data\",hue=\"Class\",split=True\n                  , ax = ax)\n    g.set_xlabel(f\"For system_age {cat} values\")\n#fig.text(0.04, 0.5, 'Distribution of each feature per outcome', va='center', rotation='vertical');\n\n","d47f0eb9":"categories_to_diplay = data.total_cat.cat.categories[1:]\nfig , axs = plt.subplots(len(categories_to_diplay))\nfig.set_figheight(15)\nfig.set_figwidth(15)\n#sns.boxenplot(data=ag_dist,x=\"ref\",y=\"data\",hue=\"class\",outlier_prop=0.00000000001)\nfor ax,cat in zip(axs,categories_to_diplay):\n    g = sns.pointplot(data=all_avg_vals[all_avg_vals.total_cat==cat]\n                  , x=\"ref\",y=\"data\",hue=\"Class\"\n                  , ax = ax)\n    g.set_xlabel(f\"For total_max {cat} values\")\n#fig.text(0.04, 0.5, 'Distribution of each feature per outcome', va='center', rotation='vertical');\n\n","ff703e73":"# Scania Database : Histograms\n\nThe database was released in 2016 at the IDA conference and is available on UCI. It contains anonymised data from an Air Pressure System (APS), with the target of detecting future failures.\n\nTo avoid confusion, during this note, we will use the following norm:\n- positive classification or class 1 means there is component failures of specific components of the APS system.\n- negative classification or class 0 means there is a failure but not related to the APS\n\nAll the feature are positive integers :\n- 100 are \"counters\" that counts the occurrence of events known by a two-letter code.\n- 7 \"histograms\" of 10 bins, which means that here this is a counting of a particular physical value being in a particular range (also known as \"bucket\").\n\nWe do not know what those are or the values of the ranges, but the presentation of the paper suggest they are open-ended.\n\nThe 7 histograms of physical histograms are known as AG, AY, AZ, BA, CN, CS and EE.\n\nThe aim of the code below is to better understand those histograms.","1148ad3c":"\n## Potential interaction between AG_002 and CN_001\n\nWe have looked at the potential of using AG_002 and CN_001 for detecting failing system, not let's look at the possibility of looking at those 2 variables at the same time.\n\nGiven the distribution of AG_002 and CN_001, the diagramme below is also using a logarithmic scale for both AG_002 ad CN_001.\n\nIn different of blue is the \"sound\" system.\n\nIn the different red is the failing system.\n\nWhen you have a red cell, you have a majority of class 1 APS.\n","4a96f210":"\n## Histogram features\n\nWhen looking at the histograms, we notice they have the same surface. Indeed if we add the histograms values for each measurement that are the same (when there are not missing).\n\nThis means that all measurements are taken at the same moment.\n\nThis is also a way to measure the age of the APS .\n\nWe add a feature called 'system_age' that is the sum for features for one of the histograms (they are in practice equal).\n\nNow let's look at the distribution of this \"system_age\" feature depending on whether the system is failing or sound.\n\nApart from a group of APS that have no measurements, we notice that failing systems have more measures than none failing system. Assuming the frequency for those measurements is the same across systems, this means that failing systems tend to be older systems.\n\nThe graph is scaled to have the same surface, we cannot compare the value between the figure, but we can already notice that the system that fails are usually older than \"younger\" system which should not come are a surprise.\n","f7fe3019":"## Distribution of the measurements\n\nTo make the graph readable, it has been scaled using the width.\n\nThere is the same number of points per categories; this means that when there is a spike, this does not mean that there is fewer values, but that all values are the same.\n\nThis means that for class 0, they are as many points for ay_000 and ay_008, but almost ay_00 values are 0.\n\nAlso given the range of the distribution for the number of occurrence in each category, the number of events is using a logarithmic scale.\n\nFor example, we notice some distribution difference between the class for the variables ag_002 and cn_002; we will zoom in into those 2 features.\n","1e12dc16":"\n\nAs we can notice, there is a strong correlation between the age of the system and its risk of failing, which makes sense intuitively, but will not help us with the detection....","c9f21f65":"\n## Feature CN_001 distribution\n\nWe will do the same with the same with the feature CN_001.\n\nFirst, we select all the system for which there is no occurrence of CN in the range 1 (the second interval), said differently those are the system for which CN_001 is 0.\n\nThey represent in practice the majority of the systems here again.\n\nThe rest of the APS are divided into 20 buckets of equal size (200 each).\n\nThe percentage of failure goes from a few percents for the lower values to more than 70\\% for the top 200 values, still very useful for detecting failing system but not as precise as AG_002.\n","e55f4936":"## Feature engineering for histograms\n\nAs we have an histograms, we can infer the average of each of the variable.\n\nThe advantage of doing that is to see if there is a particular value that is indicative of early failure of the system early in the life of the system.\n\nLooking at the distributions and average, we notice that failing system for \"low\" total feature have a higher BA average value (ba_avg) and a lower CS average (cs_avg).\n\n","b5fd34f1":"\n## Feature AG_002 distribution\n\nLet's look at the variation of the failure rate depending of the number of occurence of AG_002.\n\nFirst we select all the system for which there is no occurence of AG in the range 2 (the thirds range), said differently those are the system for which AG_002 is 0.\n\nThey represent in practice the majority of the system.\n\nThe of the APS are divided in 20 buckets of equal size (200 each).\n\nThe percentage of failure goes from a few percent for the lower values, to more than 90\\% for the top 200 values.\n","a857d62b":"## The histograms\n\nThere are 860 missing values, but when there is a missing value in a histogram, all the values in the histograms are missing.\n\nFirst let's have a look at ten histograms, five from the positive class and five from the negative class.\n"}}