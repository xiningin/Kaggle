{"cell_type":{"edfe5431":"code","c2f77cc6":"code","193bbb88":"code","544c438e":"code","7cae43ec":"code","37ac1843":"code","1e948cc3":"code","8da42014":"code","057d6db4":"code","05189630":"code","f9bdebb2":"code","a8332a69":"code","b5a35980":"code","1fe3fa1b":"code","0580e141":"code","6617becd":"code","62827ebc":"code","b4e50bfa":"markdown","0bce9f13":"markdown","909e18ce":"markdown","a475843f":"markdown","42569d51":"markdown","054883fb":"markdown","b15648b9":"markdown","326b8cc5":"markdown","4b6114a9":"markdown","2ba6676b":"markdown","eb0fd6a8":"markdown","0d1bb272":"markdown","3b96e452":"markdown","2d995801":"markdown"},"source":{"edfe5431":"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nfrom time import time\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom matplotlib import rcParams\n# %matplotlib inline\nle = preprocessing.LabelEncoder()\nfrom numba import jit\nimport itertools\nfrom seaborn import countplot,lineplot, barplot\nfrom numba import jit\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn import preprocessing\nfrom scipy.stats import randint as sp_randint\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import LeaveOneGroupOut\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import kurtosis, skew\n\nimport matplotlib.style as style\nstyle.use('ggplot')\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport gc\ngc.enable()","c2f77cc6":"#\ndef sq_dist(a, b):\n    ''' the squared euclidean distance between two samples '''\n\n    return np.sum((a - b) ** 2, axis=1)\n\n\ndef find_run_edges(data, edge):\n    ''' examine links between samples. left\/right run edges are those samples which do not have a link on that side. '''\n\n    if edge == 'left':\n        border1 = 0\n        border2 = -1\n    elif edge == 'right':\n        border1 = -1\n        border2 = 0\n    else:\n        return False\n\n    edge_list = []\n    linked_list = []\n\n    for i in range(len(data)):\n        dist_list = sq_dist(data[i, border1, :4], data[:, border2, :4])  # distances to rest of samples\n        min_dist = np.min(dist_list)\n        closest_i = np.argmin(dist_list)  # this is i's closest neighbor\n        if closest_i == i:  # this might happen and it's definitely wrong\n            print('Sample', i, 'linked with itself. Next closest sample used instead.')\n            closest_i = np.argsort(dist_list)[1]\n        dist_list = sq_dist(data[closest_i, border2, :4], data[:, border1, :4])  # now find closest_i's closest neighbor\n        rev_dist = np.min(dist_list)\n        closest_rev = np.argmin(dist_list)  # here it is\n        if closest_rev == closest_i:  # again a check\n            print('Sample', i, '(back-)linked with itself. Next closest sample used instead.')\n            closest_rev = np.argsort(dist_list)[1]\n        if (i != closest_rev):  # we found an edge\n            edge_list.append(i)\n        else:\n            linked_list.append([i, closest_i, min_dist])\n\n    return edge_list, linked_list\n\n\ndef find_runs(data, left_edges, right_edges):\n    import time\n    ''' go through the list of samples & link the closest neighbors into a single run '''\n\n    data_runs = []\n\n    for start_point in left_edges:\n        i = start_point\n        run_list = [i]\n        while i not in right_edges:\n            tmp = np.argmin(sq_dist(data[i, -1, :4], data[:, 0, :4]))\n            if tmp == i:  # self-linked sample\n                tmp = np.argsort(sq_dist(data[i, -1, :4], data[:, 0, :4]))[1]\n            i = tmp\n            run_list.append(i)\n        data_runs.append(np.array(run_list))\n\n    return data_runs","193bbb88":"train_X = pd.read_csv('..\/input\/X_train.csv')\ntest_X  = pd.read_csv('..\/input\/X_test.csv' )","544c438e":"print(train_X['series_id'].tail()) # train_X series_id stop in 3809\ntest_X['series_id'] = test_X['series_id'] + 3810 # so test_X's series_id should start from 3810","7cae43ec":"_total = pd.concat([train_X, test_X], axis=0).reset_index(drop=True)\nprint('total series' , len(_total['series_id'].unique()))\ntotal = _total.iloc[:,3:].values.reshape(-1,128,10)","37ac1843":"all_left_edges, all_left_link = find_run_edges(total, edge='left')\nall_right_edges, all_right_link = find_run_edges(total, edge='right')\nprint('Found', len(all_left_edges), 'left edges and', len(all_right_edges), 'right edges.')\n\nall_runs = find_runs(total, all_left_edges, all_right_edges)\n\nflat_list = [series_id for run in all_runs for series_id in run]\nprint(len(flat_list), len(np.unique(flat_list)))","1e948cc3":"lost_samples = np.array([ i for i in range(len(total)) if i not in np.concatenate(all_runs) ])\nprint(lost_samples)\nprint(len(lost_samples))\n\nlost_run = np.array(lost_samples[find_runs(total[lost_samples], [0], [5])[0]])\nall_runs.append(lost_run)","8da42014":"final = pd.DataFrame(index=_total['series_id'].unique())\nfor run_id in range(len(all_runs)):\n    for run_pos in range(len(all_runs[run_id])):\n        series_id = all_runs[run_id][run_pos]\n        final.at[ series_id, 'run_id'  ] = run_id\n        final.at[ series_id, 'run_pos' ] = run_pos\n\ntrain_y = pd.read_csv('..\/input\/y_train.csv')\nfinal['surface'] = train_y['surface']","057d6db4":"for id, surface in zip(final[final['run_id'].notnull()]['run_id'], final[final['surface'].notnull()]['surface']):\n    final.loc[final['run_id']==id, 'surface'] = surface\n\n\nprint(final['run_id'].unique())\nprint(final[final['surface'].isnull()]['run_id'].unique())","05189630":"new_target = final[final['surface'].notnull()]\nnew_train_series = final[final['surface'].notnull()].index\nnew_test_series = final[final['surface'].isnull()]['run_id'].index\nnew_train = _total[_total['series_id'].isin(new_train_series)]\nnew_test = _total[_total['series_id'].isin(new_test_series)]","f9bdebb2":"\ndef feat_eng(data):\n    df = pd.DataFrame()\n    data['totl_anglr_vel'] = (data['angular_velocity_X'] ** 2 + data['angular_velocity_Y'] ** 2 + data[\n        'angular_velocity_Z'] ** 2) ** 0.5\n    data['totl_linr_acc'] = (data['linear_acceleration_X'] ** 2 + data['linear_acceleration_Y'] ** 2 + data[\n        'linear_acceleration_Z'] ** 2) ** 0.5\n    data['acc_vs_vel'] = data['totl_linr_acc'] \/ data['totl_anglr_vel']\n\n    def mean_change_of_abs_change(x):\n        return np.mean(np.diff(np.abs(np.diff(x))))\n\n    for col in data.columns:\n\n        if col in ['row_id', 'series_id', 'measurement_number',\n                   'orientation_X', 'orientation_Y', 'orientation_Z',\n                   'run_id', 'orientation_W']:\n            continue\n        df[col + '_mean'] = data.groupby(['series_id'])[col].mean()\n        df[col + '_median'] = data.groupby(['series_id'])[col].median()\n        df[col + '_max'] = data.groupby(['series_id'])[col].max()\n        df[col + '_min'] = data.groupby(['series_id'])[col].min()\n        df[col + '_std'] = data.groupby(['series_id'])[col].std()\n\n        df[col + '_range'] = df[col + '_max'] - df[col + '_min']\n        df[col + '_maxtoMin'] = df[col + '_max'] \/ df[col + '_min']\n        df[col + '_mean_abs_chg'] = data.groupby(['series_id'])[col].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        df[col + '_mean_change_of_abs_change'] = data.groupby('series_id')[col].apply(mean_change_of_abs_change)\n        df[col + '_abs_max'] = data.groupby(['series_id'])[col].apply(lambda x: np.max(np.abs(x)))\n        df[col + '_abs_min'] = data.groupby(['series_id'])[col].apply(lambda x: np.min(np.abs(x)))\n        df[col + '_abs_avg'] = (df[col + '_abs_min'] + df[col + '_abs_max']) \/ 2\n\n\n        df[col + '_kurtosis'] = data.groupby(['series_id'])[col].apply(lambda x:kurtosis(x))\n\n        df[col + '_skew'] = data.groupby(['series_id'])[col].skew()\n\n    return df","a8332a69":"data = feat_eng(new_train).reset_index(drop=True)\ntest = feat_eng(new_test).reset_index(drop=True)\n\ndata = data.fillna(0)\ntest = test.fillna(0)\ndata = data.replace(-np.inf,0)\ndata = data.replace(np.inf,0)\ntest = test.replace(-np.inf,0)\ntest = test.replace(np.inf,0)\n","b5a35980":"new_target['surface'] = le.fit_transform(new_target['surface'])","1fe3fa1b":"print('modelling')\nmodel = RandomForestClassifier(n_estimators=500, max_depth=10, min_samples_split=5, n_jobs=-1)\nmodel.fit(data.values, new_target['surface'].values)\nmeasured = model.predict(data.values)\npredicted = model.predict_proba(test)\nscore = model.score(data.values, new_target['surface'].values)\nprint(\"score: {}\".format(model.score(data.values, new_target['surface'].values)))\nimportances = model.feature_importances_\nindices = np.argsort(importances)\nfeatures = data.columns","0580e141":"\ngc.collect()\n\nprint('Accuracy RF', score )\n\nresult = pd.DataFrame(data={'surface':le.inverse_transform(predicted.argmax(axis=1))},index= new_test['series_id'].unique())\nnew_target['surface'] = le.inverse_transform(new_target['surface'])\n\ndf = pd.concat([result, new_target[['surface']]], axis=0)\nsub = pd.read_csv('..\/input\/sample_submission.csv')\nsub['surface'] = new_target['surface']","6617becd":"sub.to_csv('my_submission.csv', index=0)","62827ebc":"print(sub.head())","b4e50bfa":"Now we do some feature egineering.\n[\"The Orientation Sensor\" or \"Science vs. Alchemy\" discussion](https:\/\/www.kaggle.com\/c\/career-con-2019\/discussion\/87239#latest-512162)\n[Smart Robots. Complete Notebook](https:\/\/www.kaggle.com\/jesucristo\/1-smart-robots-complete-notebook-0-73).","0bce9f13":"I use RandomForest, and I try Xgboost and have same public score as RF without tunning.\nI should stack this 2 result, but it's too late for me, I ran out my submission chance.","909e18ce":"Here is what I did wrong, each run may contain more than one surface type.\nBut here I made same run equals same surface.\nFrankly, I do not know what to do with same run with different surface.","a475843f":"surface string into class number","42569d51":"we lost 7626-7600=26 series, we find it, and make it into one run, then append it to all_runs","054883fb":"[Markus F](https:\/\/www.kaggle.com\/friedchips)'s work \n[The Missing Link](https:\/\/www.kaggle.com\/friedchips\/the-missing-link) show us there are links between train and test data.\nSame run by robot was recored and splited into train\/test.\nBy calculate the squared euclidean distance between two samples, we can find series that link to each other.","b15648b9":"**This solution was born from those elite scientists's works.**\nPlease accept my deepest aplologies if I missed someone.\n[Markus F](https:\/\/www.kaggle.com\/friedchips), \n[Thomas Rohwer](https:\/\/www.kaggle.com\/trohwer64), \n[Nanashi](https:\/\/www.kaggle.com\/jesucristo) \n[The Missing Link](https:\/\/www.kaggle.com\/friedchips\/the-missing-link)\n[\"The Orientation Sensor\" or \"Science vs. Alchemy\" discussion](https:\/\/www.kaggle.com\/c\/career-con-2019\/discussion\/87239#latest-512162)\n[Smart Robots. Complete Notebook](https:\/\/www.kaggle.com\/jesucristo\/1-smart-robots-complete-notebook-0-73).\n\n\nI'm a true beginner in machine learning, and I'm a self-learner.\nI want to learn, I need to learn, and I really want a job, this competition is 'fit' for me.\n\n","326b8cc5":"There are leakage in orientation's data.So I'm not using it.","4b6114a9":"Now I have new train\/test target","2ba6676b":"Now we load the data.","eb0fd6a8":"now we name a Dataframe 'final',it shows run_id\/run_pos by series_id.","0d1bb272":"we have 7626 series in total, Now we check how many runs we can find, and how many series we used.","3b96e452":"test_X's series_id shoud be considered as continuous number.","2d995801":"combine train and test data, and check how many series_id in total"}}