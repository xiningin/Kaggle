{"cell_type":{"132e03de":"code","dde36a15":"code","91975d53":"code","331670a1":"code","ecbe1747":"code","6433df3d":"code","e15c3444":"code","e95c7165":"code","3d43ff3e":"code","080cd6e6":"code","7edeaad9":"code","513500da":"code","03ce77dc":"code","940d197b":"code","b5749302":"code","404ecfea":"code","0a3ec58b":"code","1a45da01":"code","cb743349":"code","18529778":"code","501e8e59":"code","f7ec91da":"code","0a2a4306":"code","1ed75636":"code","917a8322":"code","6be11a0a":"markdown","98b85964":"markdown","69b60e72":"markdown","f25b71e2":"markdown","81f9696d":"markdown","9866bfdb":"markdown","cfb6f3da":"markdown","2d5374ef":"markdown","90528ad4":"markdown","13eed029":"markdown","a9081e72":"markdown","7dc3d3e2":"markdown","77cdb85d":"markdown","de802c12":"markdown","ad40593b":"markdown","3f69f1ca":"markdown","89b058cd":"markdown","1e8b4624":"markdown","4733b3e8":"markdown","9f7560ff":"markdown","a37e8be7":"markdown","769401c5":"markdown","2d01678f":"markdown","a95190f1":"markdown","a110c06f":"markdown","714d65f9":"markdown","31d20138":"markdown","fce56d9b":"markdown","1ac44037":"markdown","458602d3":"markdown"},"source":{"132e03de":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom sklearn.feature_extraction.text import CountVectorizer\ncount=CountVectorizer()\n","dde36a15":"data=pd.read_csv(\"..\/input\/imdb-dataset-sentiment-analysis-in-csv-format\/Train.csv\")\ndata.head()","91975d53":"data.shape\n","331670a1":"fig=plt.figure(figsize=(5,5))\ncolors=[\"skyblue\",'pink']\npos=data[data['label']==1]\nneg=data[data['label']==0]\nck=[pos['label'].count(),neg['label'].count()]\nlegpie=plt.pie(ck,labels=[\"Positive\",\"Negative\"],\n                 autopct ='%1.1f%%', \n                 shadow = True,\n                 colors = colors,\n                 startangle = 45,\n                 explode=(0, 0.1))","ecbe1747":"df=[\"Hey Jude, refrain Dont carry the world upon your shoulders For well you know that its a fool Who plays it cool By making his world a little colder Na-na-na,a, na Na-na-na, na\"]\nbag=count.fit_transform(df)\nprint(count.get_feature_names())\n","6433df3d":"print(bag.toarray())","e15c3444":"import re\ndef preprocessor(text):\n             text=re.sub('<[^>]*>','',text)\n             emojis=re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text)\n             text=re.sub('[\\W]+',' ',text.lower()) +\\\n                ' '.join(emojis).replace('-','')\n             return text   ","e95c7165":"preprocessor(data.loc[0,'text'][-50:])","3d43ff3e":"preprocessor(\"<a> this is :(  aweomee wohhhh :)\")","080cd6e6":"data['text']=data['text'].apply(preprocessor)","7edeaad9":"from nltk.stem.porter import PorterStemmer\n\nporter=PorterStemmer()","513500da":"def tokenizer(text):\n        return text.split()","03ce77dc":"def tokenizer_porter(text):\n    return [porter.stem(word) for word in text.split()]","940d197b":"tokenizer(\"Haters love Hating as they Hate\")","b5749302":"tokenizer_porter(\"Haters love Hating as they Hate\")","404ecfea":"import nltk\nnltk.download('stopwords')","0a3ec58b":"from nltk.corpus import stopwords\nstop=stopwords.words('english')","1a45da01":"from wordcloud import WordCloud\npositivedata = data[ data['label'] == 1]\npositivedata =positivedata['text']\nnegdata = data[data['label'] == 0]\nnegdata= negdata['text']\n\ndef wordcloud_draw(data, color = 'white'):\n    words = ' '.join(data)\n    cleaned_word = \" \".join([word for word in words.split()\n                              if(word!='movie' and word!='film')\n                            ])\n    wordcloud = WordCloud(stopwords=stop,\n                      background_color=color,\n                      width=2500,\n                      height=2000\n                     ).generate(cleaned_word)\n    plt.figure(1,figsize=(10, 7))\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()\n    \nprint(\"Positive words are as follows\")\nwordcloud_draw(positivedata,'white')\nprint(\"Negative words are as follows\")\nwordcloud_draw(negdata)\n","cb743349":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf=TfidfVectorizer(strip_accents=None,lowercase=False,preprocessor=None,tokenizer=tokenizer_porter,use_idf=True,norm='l2',smooth_idf=True)","18529778":"y=data.label.values\nx=tfidf.fit_transform(data.text)","501e8e59":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test=train_test_split(x,y,random_state=1,test_size=0.5,shuffle=False)","f7ec91da":"\nfrom sklearn.linear_model import LogisticRegressionCV\n\nclf=LogisticRegressionCV(cv=6,scoring='accuracy',random_state=0,n_jobs=-1,verbose=3,max_iter=500).fit(X_train,y_train)\n\ny_pred = clf.predict(X_test)\n","0a2a4306":"from sklearn import metrics\n\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","1ed75636":"from sklearn.linear_model import SGDClassifier\nclf= SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)","917a8322":"print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","6be11a0a":"* **Learn vocabulary and idf, return term-document matrix.This is equivalent to fit followed by transform, but more efficiently implemented.**","98b85964":"* **Stopwords are the most common words in any natural language. For the purpose of analyzing text data and building NLP models, these stopwords might not add much value to the meaning of the document.Generally, the most common words used in a text are \u201cthe\u201d, \u201cis\u201d, \u201cin\u201d, \u201cfor\u201d, \u201cwhere\u201d, \u201cwhen\u201d, \u201cto\u201d, \u201cat\u201d etc.**","69b60e72":"* **So lets call the preprocessing function and pass some text in it and observe the output to understand what exactly is happening when the function is called**","f25b71e2":"* **Lets understand how Count Vectorizer works which will be applied later to the dataset!**","81f9696d":"* **So now we will be using Logistic Regression for our Sentiment Analysis **","9866bfdb":"* **So lets show you some examples to get some clarity incase you did not understand what has happened in the above code**","cfb6f3da":"![source](https:\/\/cdn-images-1.medium.com\/max\/361\/0*ga5rNPmVYBsCm-lz.)","2d5374ef":"* **In the below text that has been passed in the function we can see that it contains HTML tag and emojis.The text returned will not contain the html tag and the emojis will be pushed towards the end of the text!**","90528ad4":"* **Importing important libraries**","13eed029":"* **We are importing a new library PorterStemmer from nltk.stem.porter.It follows an algorithm for suffix stripping i.e it will bring the word to its base meaning like running will be changed to run ,eating will be changed to eat.This is for simplifying the data and removing unnecessary complexities in our text data**","a9081e72":"![](https:\/\/d1sjtleuqoc1be.cloudfront.net\/wp-content\/uploads\/2019\/04\/25112909\/shutterstock_1073953772.jpg)","7dc3d3e2":"* **from sklearn.feature_extraction.text import TfidfVectorizer:Convert a collection of raw documents to a matrix of TF-IDF features.TfidfVectorizer:-Transforms text to feature vectors that can be used as input to estimator.****","77cdb85d":"* **In this next Example we will see the words being reduced to their base words,Haters will be changed to hater,hating is changed to hate**","de802c12":"**So I took a project based course on coursera,the topic being Sentiment Analysis with sckit-learn.**\n\n**This kernel is the application of what I have learnt in the course.\n**\n**This is going to be a very short,easy but effective kernel!**\n\n**I hope you too will learn something new!**","ad40593b":"* **Now from the array we can infer that conversion a collection of text documents to a matrix of token counts has been done below**","3f69f1ca":"* **Logistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.).CV here stands for Cross Validation.The user can pass the number of folds as an argument cv of the function to perform k-fold cross-validation**","89b058cd":"* **get_feature_names():Array mapping from feature integer indices to feature name.**","1e8b4624":"* **So now lets print the Accuracy Score!!**","4733b3e8":"* **Now since we have seen the above examples we will be passing the text data from our train dataset to this preprocessor function to clean the data.**","9f7560ff":"* **from sklearn.model_selection import train_test_split:Split arrays\/matrices into train and test subsets**","a37e8be7":"\n# Perform Sentiment Analysis with scikit-learn(IMDb Reviews)","769401c5":"* **from sklearn.feature_extraction.text import CountVectorizer**:\nConvert a collection of text documents to a matrix of token counts\nThis implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.","2d01678f":"* **Surprisingly both have 'ONE' in common,i guess for the positve category reviews might be like \"One of the best characters\/movies\/films I've watched and for the negative comments it might be like \"I rate this movie one star or one of the worst directions\/scripts etc**","a95190f1":"* **Lets see what will happen to this text that we are passing in the tokenizer function.**","a110c06f":"* **After reading the csv file which contains 40k movie reviews from IMDB,we see that there are two prominent columns.One being TEXT which contains the review and the other being LABEL which contains O's and 1's ,where 0-NEGATIVE and 1-POSITIVE REVIEW!!**","714d65f9":"* **So before we perform stemming we need to split the sentences into words.Again to make you understand this concept of splitting of text we will see some Examples.**","31d20138":"* ** Next we are going to Import RE i.e Regular Expression Operation,we are using this library to remove html tags like '< a >'  or  <head\/>.So whenever we come across these tage we replace them with an empty string ''.Next we will also be altering emojis\/emoticons which can be smiley :) ,sad face :( or even some upset face :\/.We will be shifting the emojis towards the end so that we can get a set of clean text.**","fce56d9b":"* **Negative words that are highlighted are:awful,waste,problem,stupid,horrible,bad,poor**","1ac44037":"* **Positive words that are highlighted are:love,great,perfect,good,beautiful,nice,excellent**","458602d3":"**Linear classifiers (SVM, logistic regression, a.o.) with SGD training.**"}}