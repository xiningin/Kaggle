{"cell_type":{"91184902":"code","7f13e263":"code","8f969ed7":"code","b583c189":"code","1e3229da":"code","7d1c58c9":"code","f4531f96":"code","d7e96554":"code","b18160d6":"code","506d6e34":"code","19017b5a":"code","8521fd71":"code","a0083e8d":"code","ce863207":"code","86981559":"code","69cd8a60":"code","d43e0a33":"code","6448b3d6":"code","2285edad":"code","f58136eb":"code","bc7bc61b":"code","72c19906":"code","bd932a6b":"code","22d68724":"code","8547fd87":"code","650d71e2":"code","23cd808c":"code","c40f5f99":"code","4fa73cdd":"code","8afd29df":"code","0afd8936":"code","a1e85bb7":"code","da091d09":"code","909ac574":"code","a4a492c8":"code","17a00cd7":"code","377c63cd":"code","484ff795":"code","2ca9cf42":"code","9ad91178":"code","f5d6601f":"code","3c8dfc94":"code","341b3894":"code","265f4959":"code","044ae654":"code","50b93008":"code","82c82c29":"code","59902cac":"code","46881faa":"code","07106bf4":"code","041c9286":"code","08b05372":"code","0d82c7eb":"code","f944f4c8":"code","9c63ea2b":"code","b3719df4":"code","d97f5565":"code","eb37a455":"code","ae5ca84d":"markdown","dc4abbeb":"markdown","91b653f0":"markdown","cd6dfdd5":"markdown","81a134b8":"markdown","1e099aae":"markdown","5e9cd86f":"markdown","2341ac6c":"markdown","76fcb231":"markdown","fba8f1fb":"markdown","673c13ac":"markdown","d13e56f5":"markdown","6d3fad0d":"markdown","0137fb18":"markdown","efbd3cbd":"markdown","4c0b50c5":"markdown","759cd10a":"markdown","a24ae77e":"markdown","b1517771":"markdown","f31f642a":"markdown","fe0291f3":"markdown","179b12e2":"markdown","5588839b":"markdown","61eccc88":"markdown","7940b067":"markdown","f233d303":"markdown","7f08c0be":"markdown","5dcc04e8":"markdown","ce7cbf50":"markdown","34104691":"markdown"},"source":{"91184902":"import plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\nfrom colorama import Fore\n\nfrom pandas_profiling import ProfileReport\nimport seaborn as sns\nfrom sklearn import metrics\nfrom scipy import stats\nimport math\n\nfrom tqdm.notebook import tqdm\nfrom copy import deepcopy\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nimport optuna\nfrom optuna import Trial, visualization\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score, mean_squared_error","7f13e263":"# Defining all our palette colours.\nprimary_blue = \"#496595\"\nprimary_blue2 = \"#85a1c1\"\nprimary_blue3 = \"#3f4d63\"\nprimary_grey = \"#c6ccd8\"\nprimary_black = \"#202022\"\nprimary_bgcolor = \"#f4f0ea\"\n\nprimary_green = px.colors.qualitative.Plotly[2]","8f969ed7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nmeta_random_seed = 68\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b583c189":"train_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/test.csv')\nsub_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/sample_submission.csv')\n\ntrain_df.head()","1e3229da":"feature_cols = train_df.drop(['id', 'target'], axis=1).columns\n\n## Getting all the data that are not of \"object\" type. \nnumerical_columns = train_df[feature_cols].select_dtypes(include=['int64','float64']).columns\ncategorical_columns = train_df[feature_cols].select_dtypes(exclude=['int64','float64']).columns\n\nprint(len(numerical_columns), len(categorical_columns))","7d1c58c9":"## Join train and test datasets in order to obtain the same number of features during categorical conversion\ntrain_indexs = train_df.index\ntest_indexs = test_df.index\n\ndf =  pd.concat(objs=[train_df, test_df], axis=0).reset_index(drop=True)\ndf = df.drop(['id', 'target'], axis=1)\n\nprint(df.shape)","f4531f96":"profile = ProfileReport(train_df)","d7e96554":"profile","b18160d6":"fig = px.histogram(train_df, x='target')\nfig.update_layout(\n    title_text='Target distribution', # title of plot\n    xaxis_title_text='Value', # xaxis label\n    yaxis_title_text='Count', # yaxis label\n    bargap=0.2, # gap between bars of adjacent location coordinates\n)\nfig.show()","506d6e34":"num_rows, num_cols = 4,3\nf, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(12, 12))\nf.suptitle('Distribution of Features', fontsize=16)\n\nfor index, column in enumerate(df[numerical_columns].columns):\n    i,j = (index \/\/ num_cols, index % num_cols)\n    sns.kdeplot(train_df.loc[train_df['target'] == 0, column], color=\"m\", shade=True, ax=axes[i,j])\n    sns.kdeplot(train_df.loc[train_df['target'] == 1, column], color=\"b\", shade=True, ax=axes[i,j])\n\nf.delaxes(axes[3, 2])\nplt.tight_layout()\nplt.show()","19017b5a":"corr = df[numerical_columns].corr().abs()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\nfig, ax = plt.subplots(figsize=(14, 14))\n\n# plot heatmap\nsns.heatmap(corr, mask=mask, annot=True, fmt=\".2f\", cmap='coolwarm',\n            cbar_kws={\"shrink\": .8}, vmin=0, vmax=1)\n# yticks\nplt.yticks(rotation=0)\nplt.show()","8521fd71":"# Thanks a lot @dwin183287 for sharing this amazinf function!\n\nbackground_color = \"#f6f5f5\"\n\nfig = plt.figure(figsize=(12, 8), facecolor=background_color)\ngs = fig.add_gridspec(1, 1)\nax0 = fig.add_subplot(gs[0, 0])\n\nax0.set_facecolor(background_color)\nax0.text(-1.1, 0.26, 'Correlation of Continuous Features with Target', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-1.1, 0.24, 'There is no features that pass 0.22 correlation with target', fontsize=13, fontweight='light', fontfamily='serif')\n\nchart_df = pd.DataFrame(train_df[numerical_columns].corrwith(train_df['target']))\nchart_df.columns = ['corr']\nsns.barplot(x=chart_df.index, y=chart_df['corr'], ax=ax0, color=primary_blue, zorder=3, edgecolor='black', linewidth=1.5)\nax0.grid(which='major', axis='y', zorder=0, color='gray', linestyle=':', dashes=(1,5))\nax0.set_ylabel('')\n\nfor s in [\"top\",\"right\", 'left']:\n    ax0.spines[s].set_visible(False)\n\nplt.show()","a0083e8d":"train_0_df = train_df.loc[train_df['target'] == 0]\ntrain_1_df = train_df.loc[train_df['target'] == 1]\n\nnum_rows, num_cols = 5,4\nfig = make_subplots(rows=num_rows, cols=num_cols)\n\nfor index, column in enumerate(df[categorical_columns].columns):\n    i,j = ((index \/\/ num_cols)+1, (index % num_cols)+1)\n    data = train_0_df.groupby(column)[column].count().sort_values(ascending=False)\n    data = data if len(data) < 10 else data[:10]\n    fig.add_trace(go.Bar(\n        x = data.index,\n        y = data.values,\n        name='Label: 0',\n    ), row=i, col=j)\n\n    data = train_1_df.groupby(column)[column].count().sort_values(ascending=False)\n    data = data if len(data) < 10 else data[:10]\n    fig.add_trace(go.Bar(\n        x = data.index,\n        y = data.values,\n        name='Label: 1'\n    ), row=i, col=j)\n    \n    fig.update_xaxes(title=column, row=i, col=j)\n    fig.update_layout(barmode='stack')\n    \nfig.update_layout(\n    autosize=False,\n    width=1600,\n    height=1600,\n    showlegend=False,\n)\nfig.show()","ce863207":"num_rows, num_cols = 10,1\nfig = make_subplots(rows=num_rows, cols=num_cols)\ncont = 1\n\nfor index, column in enumerate(df[categorical_columns].columns):\n    data = train_0_df.groupby(column)[column].count().sort_values(ascending=False)\n    if len(data) < 10:\n        continue\n    # data = data if len(data) < 25 else data[:25]\n    i,j = (cont, 1)\n    cont+=1\n    \n    fig.add_trace(go.Bar(\n        x = data.index,\n        y = data.values,\n        name='Label: 0',\n    ), row=i, col=j)\n    \n    target_0_values = set(deepcopy(data.index))\n    \n    data = train_1_df.groupby(column)[column].count().sort_values(ascending=False)\n    # data = data if len(data) < 25 else data[:25]\n    \n    fig.add_trace(go.Bar(\n        x = data.index,\n        y = data.values,\n        name='Label: 1'\n    ), row=i, col=j)\n    \n    target_1_values = set(deepcopy(data.index))\n    \n    print('----------------------{}----------------------'.format(column))\n    print('Unique values for class 0: {}'.format(target_0_values - target_1_values))\n    print('Unique values for class 1: {}'.format(target_1_values - target_0_values))\n    \n    fig.update_xaxes(title=column, row=i, col=j)\n    fig.update_layout(barmode='stack')\n    \nfig.update_layout(\n    autosize=False,\n    width=900,\n    height=2000,\n    showlegend=False,\n)\nfig.show()","86981559":"# Fix cat5 variable\ntrain_df['cat5'] = train_df['cat5'].apply(lambda x: x if x not in ['AK', 'BX', 'BP', 'AG', 'BM', 'CB', 'B', 'ZZ'] else 'CAT50')\ntest_df['cat5'] = test_df['cat5'].apply(lambda x: x if x not in ['AK', 'BX', 'BP', 'AG', 'BM', 'CB', 'B', 'ZZ'] else 'CAT50')\n\n# Fix cat8 variable\ntrain_df['cat8'] = train_df['cat8'].apply(lambda x: x if x not in ['P', 'AC'] else 'CAT80')\ntest_df['cat8'] = test_df['cat8'].apply(lambda x: x if x not in ['P', 'AC'] else 'CAT80')\n\n# Fix cat10 variable\ntrain_df['cat10'] = train_df['cat10'].apply(lambda x: x if x not in ['AF', 'MR', 'DU', 'AW', 'DL', 'GJ', 'MK', 'MA', 'DT', 'FA', 'GY', 'EN', 'EH', 'JE', 'JF', 'KK', 'LH', 'LK', 'MW', 'FF', 'CH', 'JU', 'HY', 'LR', 'KI', 'IU', 'CM', 'DM', 'BD', 'MU', 'ML', 'EB', 'IQ', 'CF', 'IN', 'CN', 'IM', 'AJ', 'IP', 'MI', 'ED', 'CX', 'FW', 'BS', 'IY', 'MP', 'BX', 'DN', 'MO', 'GH', 'EG', 'BA', 'ME', 'GR', 'KD', 'LT', 'IL', 'GF', 'BO', 'DA', 'MQ', 'KU', 'DX', 'CT', 'HF', 'CQ', 'GG', 'EF', 'HI', 'KN', 'GV', 'JC', 'DK', 'GD'] else 'CAT10')\ntest_df['cat10'] = test_df['cat10'].apply(lambda x: x if x not in ['AF', 'MR', 'DU', 'AW', 'DL', 'GJ', 'MK', 'MA', 'DT', 'FA', 'GY', 'EN', 'EH', 'JE', 'JF', 'KK', 'LH', 'LK', 'MW', 'FF', 'CH', 'JU', 'HY', 'LR', 'KI', 'IU', 'CM', 'DM', 'BD', 'MU', 'ML', 'EB', 'IQ', 'CF', 'IN', 'CN', 'IM', 'AJ', 'IP', 'MI', 'ED', 'CX', 'FW', 'BS', 'IY', 'MP', 'BX', 'DN', 'MO', 'GH', 'EG', 'BA', 'ME', 'GR', 'KD', 'LT', 'IL', 'GF', 'BO', 'DA', 'MQ', 'KU', 'DX', 'CT', 'HF', 'CQ', 'GG', 'EF', 'HI', 'KN', 'GV', 'JC', 'DK', 'GD'] else 'CAT10')\n","69cd8a60":"# try to delete low represented vars\n\ntrain_df['cat4'] = train_df['cat4'].apply(lambda x: x if x in ['E', 'F', 'D', 'G', 'H', 'J', 'K', 'I', 'C'] else 'Z')\ntest_df['cat4'] = test_df['cat4'].apply(lambda x: x if x in ['E', 'F', 'D', 'G', 'H', 'J', 'K', 'I', 'C'] else 'Z')\n\ntrain_df['cat5'] = train_df['cat5'].apply(lambda x: x if x in ['BI', 'AB', 'BU', 'K', 'G', 'BQ', 'N', 'CL', 'CAT50'] else 'Z')\ntest_df['cat5'] = test_df['cat5'].apply(lambda x: x if x in ['BI', 'AB', 'BU', 'K', 'G', 'BQ', 'N', 'CL', 'CAT50'] else 'Z')\n","d43e0a33":"train_0_df = train_df.loc[train_df['target'] == 0]\ntrain_1_df = train_df.loc[train_df['target'] == 1]\n\nnum_rows, num_cols = 10,1\nfig = make_subplots(rows=num_rows, cols=num_cols)\ncont = 1\n\nfor index, column in enumerate(df[categorical_columns].columns):\n    data = train_0_df.groupby(column)[column].count().sort_values(ascending=False)\n    if len(data) < 10:\n        continue\n    data = data if len(data) < 25 else data[:25]\n    i,j = (cont, 1)\n    cont+=1\n    \n    fig.add_trace(go.Bar(\n        x = data.index,\n        y = data.values,\n        name='Label: 0',\n    ), row=i, col=j)\n        \n    data = train_1_df.groupby(column)[column].count().sort_values(ascending=False)\n    data = data if len(data) < 25 else data[:25]\n    \n    fig.add_trace(go.Bar(\n        x = data.index,\n        y = data.values,\n        name='Label: 1'\n    ), row=i, col=j)\n        \n    fig.update_xaxes(title=column, row=i, col=j)\n    fig.update_layout(barmode='stack')\n    \nfig.update_layout(\n    autosize=False,\n    width=900,\n    height=2000,\n    showlegend=False,\n)\nfig.show()","6448b3d6":"from category_encoders import CatBoostEncoder, LeaveOneOutEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom copy import deepcopy\n\nloo_features = []\nle_features = []\n\n    \ndef label_encoder(train_df, test_df, column):\n    le = LabelEncoder()\n    new_feature = \"{}_le\".format(column)\n    le.fit(train_df[column].unique().tolist() + test_df[column].unique().tolist())\n    \n    train_df[new_feature] = le.transform(train_df[column])\n    test_df[new_feature] = le.transform(test_df[column])\n    \n    return new_feature\n\ndef loo_encode(train_df, test_df, column):\n    loo = LeaveOneOutEncoder()\n    new_feature = \"{}_loo\".format(column)\n    loo.fit(train_df[column], train_df[\"target\"])\n    \n    train_df[new_feature] = loo.transform(train_df[column])\n    test_df[new_feature] = loo.transform(test_df[column])\n    \n    return new_feature\n\nfor feature in categorical_columns:\n    loo_features.append(loo_encode(train_df, test_df, feature))\n    le_features.append(label_encoder(train_df, test_df, feature))\n    \nxgb_cat_features = deepcopy(loo_features)\nlgb_cat_features = deepcopy(le_features)\ncb_cat_features = deepcopy(list(categorical_columns))\nridge_cat_features = deepcopy(loo_features)","2285edad":"from lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import roc_auc_score","f58136eb":"y = train_df[\"target\"]\nlgbm_features = lgb_cat_features + list(numerical_columns)\nx = train_df[lgbm_features]\n\nx_train, x_valid, y_train, y_valid=train_test_split(x, y, test_size=0.2, random_state=meta_random_seed)","bc7bc61b":"xgb_cls = XGBClassifier()\n\nxgb_cls.fit(x_train, y_train, verbose=False)\npredictions = xgb_cls.predict_proba(x_valid)[:,1]\n\nauc = roc_auc_score(y_valid, predictions)\n\nprint(f'Baseline Score: {auc}')","72c19906":"lgbm = LGBMClassifier()\n\nlgbm.fit(x_train, y_train, eval_set=(x_valid,y_valid), early_stopping_rounds=150, verbose=False)\npredictions = lgbm.predict_proba(x_valid)[:,1]\n\nauc = roc_auc_score(y_valid, predictions)\n\nprint(f'Baseline Score: {auc}')","bd932a6b":"# I took many ideas from @craigmthomas, so thank you very much bro, your ideas are awesome\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import RidgeClassifier\n\nfrom sklearn.model_selection import StratifiedKFold","22d68724":"random_state = meta_random_seed\nn_folds = 10\nk_fold = StratifiedKFold(n_splits=n_folds, random_state=random_state, shuffle=True)\n\ny = train_df[\"target\"]\n\nxgb_train_preds = np.zeros(len(train_df.index), )\nxgb_test_preds = np.zeros(len(test_df.index), )\nxgb_features = xgb_cat_features + list(numerical_columns)\n\nlgbm_train_preds = np.zeros(len(train_df.index), )\nlgbm_test_preds = np.zeros(len(test_df.index), )\nlgbm_features = lgb_cat_features + list(numerical_columns)\n\ncb_train_preds = np.zeros(len(train_df.index), )\ncb_test_preds = np.zeros(len(test_df.index), )\ncb_features = cb_cat_features + list(numerical_columns)\n\nridge_train_preds = np.zeros(len(train_df.index), )\nridge_test_preds = np.zeros(len(test_df.index), )\nridge_features = ridge_cat_features + list(numerical_columns)","8547fd87":"%%script false --no-raise-error\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(train_df, y)):\n    print(\"--> Fold {}\".format(fold + 1))\n    y_train = y.iloc[train_index]\n    y_valid = y.iloc[test_index]\n\n    ########## Generate train and valid sets ##########\n    xgb_x_train = pd.DataFrame(train_df[xgb_features].iloc[train_index])\n    xgb_x_valid = pd.DataFrame(train_df[xgb_features].iloc[test_index])\n\n    lgbm_x_train = pd.DataFrame(train_df[lgbm_features].iloc[train_index])\n    lgbm_x_valid = pd.DataFrame(train_df[lgbm_features].iloc[test_index])\n    \n    cb_x_train = pd.DataFrame(train_df[cb_features].iloc[train_index])\n    cb_x_valid = pd.DataFrame(train_df[cb_features].iloc[test_index])\n\n    ridge_x_train = pd.DataFrame(train_df[ridge_features].iloc[train_index])\n    ridge_x_valid = pd.DataFrame(train_df[ridge_features].iloc[test_index])\n\n    ########## XGBoost model ##########\n    xgb_model = XGBClassifier(\n        seed=random_state,\n        verbosity=1,\n        eval_metric=\"auc\",\n        tree_method=\"gpu_hist\",\n        gpu_id=0,\n        n_jobs = 12,\n    )\n    xgb_model.fit(\n        xgb_x_train,\n        y_train,\n        eval_set=[(xgb_x_valid, y_valid)], \n        verbose=0,\n        early_stopping_rounds=200\n    )\n\n    train_oof_preds = xgb_model.predict_proba(xgb_x_valid)[:,1]\n    test_oof_preds = xgb_model.predict_proba(test_df[xgb_features])[:,1]\n    xgb_train_preds[test_index] = train_oof_preds\n    xgb_test_preds += test_oof_preds \/ n_folds\n    \n    print(\": XGB - ROC AUC Score = {}\".format(roc_auc_score(y_valid, train_oof_preds, average=\"micro\")))\n    \n    ########## LGBM model ##########\n    lgbm_model = LGBMClassifier(\n        cat_feature=[x for x in range(len(categorical_columns))],\n        random_state=random_state,\n        metric=\"auc\",\n        n_jobs=12,\n    )\n    lgbm_model.fit(\n        lgbm_x_train,\n        y_train,\n        eval_set=[(lgbm_x_valid, y_valid)], \n        verbose=0,\n    )\n\n    train_oof_preds = lgbm_model.predict_proba(lgbm_x_valid)[:,1]\n    test_oof_preds = lgbm_model.predict_proba(test_df[lgbm_features])[:,1]\n    lgbm_train_preds[test_index] = train_oof_preds\n    lgbm_test_preds += test_oof_preds \/ n_folds\n    \n    print(\": LGB - ROC AUC Score = {}\".format(roc_auc_score(y_valid, train_oof_preds, average=\"micro\")))\n\n    ########## CatBoost model ##########\n    cb_model = CatBoostClassifier(\n        verbose=0,\n        eval_metric=\"AUC\",\n        loss_function=\"Logloss\",\n        random_state=random_state,\n        task_type=\"GPU\",\n        devices=\"0\",\n        cat_features=[x for x in range(len(categorical_columns))],\n    )\n    cb_model.fit(\n        cb_x_train,\n        y_train,\n        eval_set=[(cb_x_valid, y_valid)], \n        verbose=0,\n    )\n\n    train_oof_preds = cb_model.predict_proba(cb_x_valid)[:,1]\n    test_oof_preds = cb_model.predict_proba(test_df[cb_features])[:,1]\n    cb_train_preds[test_index] = train_oof_preds\n    cb_test_preds += test_oof_preds \/ n_folds\n    \n    print(\": CB - ROC AUC Score = {}\".format(roc_auc_score(y_valid, train_oof_preds, average=\"micro\")))\n    \n    ########## Ridge model ##########\n    ridge_model = RidgeClassifier(\n        random_state=random_state,\n    )\n    ridge_model.fit(\n        ridge_x_train,\n        y_train,\n    )\n\n    train_oof_preds = ridge_model.decision_function(ridge_x_valid)\n    test_oof_preds = ridge_model.decision_function(test_df[ridge_features])\n    ridge_train_preds[test_index] = train_oof_preds\n    ridge_test_preds += test_oof_preds \/ n_folds\n    \n    print(\": Ridge - ROC AUC Score = {}\".format(roc_auc_score(y_valid, train_oof_preds, average=\"micro\")))\n    print(\"\")","650d71e2":"%%script false --no-raise-error\n\nprint(\"--> Overall metrics\")\nprint(\": XGB - ROC AUC Score = {}\".format(\n    roc_auc_score(y, xgb_train_preds, average=\"micro\")\n))\nprint(\": LGB - ROC AUC Score = {}\".format(\n    roc_auc_score(y, lgbm_train_preds, average=\"micro\")\n))\nprint(\": CB - ROC AUC Score = {}\".format(\n    roc_auc_score(y, cb_train_preds, average=\"micro\")\n))\nprint(\": Ridge - ROC AUC Score = {}\".format(\n    roc_auc_score(y, ridge_train_preds, average=\"micro\")\n))","23cd808c":"%%script false --no-raise-error\n\ny_train_preds = (\n    0.3 * xgb_train_preds +\n    0.4 * lgbm_train_preds +\n    0.3 * cb_train_preds\n)\n\nprint(\": Essemble train test - ROC AUC Score = {}\".format(\n    roc_auc_score(y, y_train_preds, average=\"micro\")\n))","c40f5f99":"%%script false --no-raise-error\n\ny_test_preds = (\n    0.3 * xgb_test_preds +\n    0.4 * lgbm_test_preds +\n    0.3 * cb_test_preds\n)\n\nsub_df['target'] = y_test_preds\nsub_df.to_csv('submission_base_essemble.csv',index=False)","4fa73cdd":"%%script false --no-raise-error\n\nfrom scipy.special import expit\nfrom sklearn.calibration import CalibratedClassifierCV\n\nrandom_state = meta_random_seed\nn_folds = 10\nk_fold = StratifiedKFold(n_splits=n_folds, random_state=random_state, shuffle=True)\n\nl1_train = pd.DataFrame(data={\n    \"xgb\": xgb_train_preds.tolist(),\n    \"lgbm\": lgbm_train_preds.tolist(),\n    \"cb\": cb_train_preds.tolist(),\n    \"ridge\": ridge_train_preds.tolist(),\n    \"target\": y.tolist()\n})\nl1_test = pd.DataFrame(data={\n    \"xgb\": xgb_test_preds.tolist(),\n    \"lgbm\": lgbm_test_preds.tolist(),\n    \"cb\": cb_test_preds.tolist(),\n    \"ridge\": ridge_test_preds.tolist(),    \n})\n\ntrain_preds = np.zeros(len(l1_train.index), )\ntest_preds = np.zeros(len(l1_test.index), )\nfeatures = [\"xgb\", \"lgbm\", \"cb\", \"ridge\"]\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(l1_train, y)):\n    print(\"--> Fold {}\".format(fold + 1))\n    y_train = y.iloc[train_index]\n    y_valid = y.iloc[test_index]\n\n    x_train = pd.DataFrame(l1_train[features].iloc[train_index])\n    x_valid = pd.DataFrame(l1_train[features].iloc[test_index])\n    \n    model = CalibratedClassifierCV(\n        RidgeClassifier(random_state=random_state), \n        cv=3\n    )\n    model.fit(\n        x_train,\n        y_train,\n    )\n\n    train_oof_preds = model.predict_proba(x_valid)[:,-1]\n    test_oof_preds = model.predict_proba(l1_test[features])[:,-1]\n    train_preds[test_index] = train_oof_preds\n    test_preds += test_oof_preds \/ n_folds\n    \n    print(\": ROC AUC Score = {}\".format(roc_auc_score(y_valid, train_oof_preds, average=\"micro\")))\n    print(\"\")\n    \nprint(\"--> Overall metrics\")\nprint(\": ROC AUC Score = {}\".format(roc_auc_score(y, train_preds, average=\"micro\")))","8afd29df":"%%script false --no-raise-error\n\nsub_df[\"target\"] = test_preds.tolist()\nsub_df.to_csv(\"submission_base_l2_classifier.csv\", index=False)","0afd8936":"def objective(trial, X=train_df[xgb_features], y=y):\n\n    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=meta_random_seed)\n\n\n    lgb_params={\n        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2),\n        'max_depth': trial.suggest_int('max_depth', 6, 200),\n        'num_leaves': trial.suggest_int('num_leaves', 31, 120),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0),\n        'random_state': meta_random_seed,\n        'metric': 'auc',\n        'n_estimators': trial.suggest_int('n_estimators', 6, 300000),\n        'n_jobs': 12,\n        'cat_feature': [x for x in range(len(categorical_columns))],\n        'bagging_seed': 2021,\n        'feature_fraction_seed': 2021,\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 0.9),\n        'min_child_samples': trial.suggest_int('min_child_samples', 1, 500),\n        'subsample_freq': trial.suggest_int('subsample_freq', 1, 10),\n        'subsample': trial.suggest_float('subsample', 0.3, 0.9),\n        'max_bin': trial.suggest_int('max_bin', 128, 1024),\n        'min_data_per_group': trial.suggest_int('min_data_per_group', 50, 350),\n        'cat_smooth': trial.suggest_int('cat_smooth', 10, 250),\n        'cat_l2': trial.suggest_int('cat_l2', 1, 20)\n    }\n\n    lgb = LGBMClassifier(\n        **lgb_params\n    )\n    lgb.fit(\n        X_train,\n        y_train,\n        eval_set=(X_test,y_test),\n        eval_metric='auc',\n        early_stopping_rounds=100,\n        verbose=False\n    )\n    predictions=lgb.predict_proba(X_test)[:,1]\n\n    return roc_auc_score(y_test,predictions)","a1e85bb7":"# study = optuna.create_study(direction='maximize')\n# study.optimize(objective, timeout=3600*7, n_trials=15)","da091d09":"# study.best_params","909ac574":"# Thanks to @gaetanlopez for the optimization result\nlgbm_params={\n    'learning_rate': 0.00605886703283976,\n    'max_depth': 42,\n    'num_leaves': 108,\n    'reg_alpha': 0.9140720355379223,\n    'reg_lambda': 9.97396811596188,\n    'colsample_bytree': 0.2629101393563821,\n    'min_child_samples': 61,\n    'subsample_freq': 2,\n    'subsample': 0.8329687190743886,\n    'max_bin': 899,\n    'min_data_per_group': 73,\n    'cat_smooth': 21,\n    'cat_l2': 11,\n    'random_state': 2021,\n    'metric': 'auc',\n    'n_estimators': 20000,\n    'n_jobs': -1,\n    'bagging_seed': 2021,\n    'feature_fraction_seed': 2021\n}","a4a492c8":"from scipy.special import expit\nfrom sklearn.calibration import CalibratedClassifierCV\n\nrandom_state = meta_random_seed\nn_folds = 10\nk_fold = StratifiedKFold(n_splits=n_folds, random_state=random_state, shuffle=True)\n\ny = train_df[\"target\"]\n\nxgb_train_preds = np.zeros(len(train_df.index), )\nxgb_test_preds = np.zeros(len(test_df.index), )\nxgb_features = xgb_cat_features + list(numerical_columns)\n\nlgbm_train_preds = np.zeros(len(train_df.index), )\nlgbm_test_preds = np.zeros(len(test_df.index), )\nlgbm_features = lgb_cat_features + list(numerical_columns)\n\ncb_train_preds = np.zeros(len(train_df.index), )\ncb_test_preds = np.zeros(len(test_df.index), )\ncb_features = cb_cat_features + list(numerical_columns)\n\nridge_train_preds = np.zeros(len(train_df.index), )\nridge_test_preds = np.zeros(len(test_df.index), )\nridge_features = ridge_cat_features + list(numerical_columns)","17a00cd7":"for fold, (train_index, test_index) in enumerate(k_fold.split(train_df, y)):\n    print(\"--> Fold {}\".format(fold + 1))\n    y_train = y.iloc[train_index]\n    y_valid = y.iloc[test_index]\n\n    ########## Generate train and valid sets ##########\n    xgb_x_train = pd.DataFrame(train_df[xgb_features].iloc[train_index])\n    xgb_x_valid = pd.DataFrame(train_df[xgb_features].iloc[test_index])\n\n    lgbm_x_train = pd.DataFrame(train_df[lgbm_features].iloc[train_index])\n    lgbm_x_valid = pd.DataFrame(train_df[lgbm_features].iloc[test_index])\n    \n    cb_x_train = pd.DataFrame(train_df[cb_features].iloc[train_index])\n    cb_x_valid = pd.DataFrame(train_df[cb_features].iloc[test_index])\n\n    ridge_x_train = pd.DataFrame(train_df[ridge_features].iloc[train_index])\n    ridge_x_valid = pd.DataFrame(train_df[ridge_features].iloc[test_index])\n\n    ########## XGBoost model ##########\n    xgb_model = XGBClassifier(\n        seed=random_state,\n        n_estimators=10000,\n        verbosity=1,\n        eval_metric=\"auc\",\n        tree_method=\"gpu_hist\",\n        gpu_id=0,\n        alpha=7.105034571323,\n        colsample_bytree=0.25749283463,\n        gamma=0.5003291821,\n        reg_lambda=0.969826765347235612,\n        learning_rate=0.009823136778823764,\n        max_bin=338,\n        max_depth=8,\n        min_child_weight=2.2834723630466,\n        subsample=0.6200435155855,\n    )\n    xgb_model.fit(\n        xgb_x_train,\n        y_train,\n        eval_set=[(xgb_x_valid, y_valid)], \n        verbose=0,\n        early_stopping_rounds=200\n    )\n\n    train_oof_preds = xgb_model.predict_proba(xgb_x_valid)[:,1]\n    test_oof_preds = xgb_model.predict_proba(test_df[xgb_features])[:,1]\n    xgb_train_preds[test_index] = train_oof_preds\n    xgb_test_preds += test_oof_preds \/ n_folds\n    \n    print(\": XGB - ROC AUC Score = {}\".format(roc_auc_score(y_valid, train_oof_preds, average=\"micro\")))\n    \n    ########## LGBM model ##########\n    lgbm_model = LGBMClassifier(\n        cat_feature=[x for x in range(len(categorical_columns))],\n        random_state=random_state,\n        cat_l2=26.00385242730252,\n        cat_smooth=89.2699690675538,\n        colsample_bytree=0.2557260109926193,\n        early_stopping_round=200,\n        learning_rate=0.00605886703283976,\n        max_bin=899,\n        max_depth=42,\n        metric=\"auc\",\n        min_child_samples=292,\n        min_data_per_group=177,\n        n_estimators=1600000,\n        n_jobs=12,\n        num_leaves=108,\n        reg_alpha=0.9140720355379223,\n        reg_lambda=5.643115293892745,\n        subsample=0.919878341796,\n        subsample_freq=1,\n        verbose=-1,\n    )\n    lgbm_model.fit(\n        lgbm_x_train,\n        y_train,\n        eval_set=[(lgbm_x_valid, y_valid)], \n        verbose=0,\n    )\n\n    train_oof_preds = lgbm_model.predict_proba(lgbm_x_valid)[:,1]\n    test_oof_preds = lgbm_model.predict_proba(test_df[lgbm_features])[:,1]\n    lgbm_train_preds[test_index] = train_oof_preds\n    lgbm_test_preds += test_oof_preds \/ n_folds\n    \n    print(\": LGB - ROC AUC Score = {}\".format(roc_auc_score(y_valid, train_oof_preds, average=\"micro\")))\n\n    ########## CatBoost model ##########\n    cb_model = CatBoostClassifier(\n        verbose=0,\n        eval_metric=\"AUC\",\n        loss_function=\"Logloss\",\n        random_state=random_state,\n        num_boost_round=20000,\n        od_type=\"Iter\",\n        od_wait=200,\n        task_type=\"GPU\",\n        devices=\"0\",\n        cat_features=[x for x in range(len(categorical_columns))],\n        bagging_temperature=1.290192494969795,\n        grow_policy=\"Depthwise\",\n        l2_leaf_reg=9.799870133539244,\n        learning_rate=0.02017982653902465,\n        max_depth=8,\n        min_data_in_leaf=1,\n        penalties_coefficient=2.096787602734,\n    )\n    cb_model.fit(\n        cb_x_train,\n        y_train,\n        eval_set=[(cb_x_valid, y_valid)], \n        verbose=0,\n    )\n\n    train_oof_preds = cb_model.predict_proba(cb_x_valid)[:,1]\n    test_oof_preds = cb_model.predict_proba(test_df[cb_features])[:,1]\n    cb_train_preds[test_index] = train_oof_preds\n    cb_test_preds += test_oof_preds \/ n_folds\n    \n    print(\": CB - ROC AUC Score = {}\".format(roc_auc_score(y_valid, train_oof_preds, average=\"micro\")))\n    \n    ########## Ridge model ##########\n    ridge_model = CalibratedClassifierCV(\n        RidgeClassifier(random_state=random_state),\n        cv=3,\n    )\n    ridge_model.fit(\n        ridge_x_train,\n        y_train,\n    )\n\n    train_oof_preds = ridge_model.predict_proba(ridge_x_valid)[:,-1]\n    test_oof_preds = ridge_model.predict_proba(test_df[ridge_features])[:,-1]\n    ridge_train_preds[test_index] = train_oof_preds\n    ridge_test_preds += test_oof_preds \/ n_folds\n    \n    print(\": Ridge - ROC AUC Score = {}\".format(roc_auc_score(y_valid, train_oof_preds, average=\"micro\")))\n    print(\"\")","377c63cd":"from scipy.special import expit\nfrom sklearn.calibration import CalibratedClassifierCV\n\nrandom_state = meta_random_seed\nn_folds = 10\nk_fold = StratifiedKFold(n_splits=n_folds, random_state=random_state, shuffle=True)\n\nl1_train = pd.DataFrame(data={\n    \"xgb\": xgb_train_preds.tolist(),\n    \"lgbm\": lgbm_train_preds.tolist(),\n    \"cb\": cb_train_preds.tolist(),\n    \"ridge\": ridge_train_preds.tolist(),\n    \"target\": y.tolist()\n})\nl1_test = pd.DataFrame(data={\n    \"xgb\": xgb_test_preds.tolist(),\n    \"lgbm\": lgbm_test_preds.tolist(),\n    \"cb\": cb_test_preds.tolist(),\n    \"ridge\": ridge_test_preds.tolist(),    \n})\n\nl2_ridge_train_preds = np.zeros(len(l1_train.index), )\nl2_ridge_test_preds = np.zeros(len(l1_test.index), )\n\nl2_lgbm_train_preds = np.zeros(len(l1_train.index), )\nl2_lgbm_test_preds = np.zeros(len(l1_test.index), )\n\nfeatures = [\"xgb\", \"lgbm\", \"cb\", \"ridge\"]\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(l1_train, y)):\n    print(\"--> Fold {}\".format(fold + 1))\n    y_train = y.iloc[train_index]\n    y_valid = y.iloc[test_index]\n\n    x_train = pd.DataFrame(l1_train[features].iloc[train_index])\n    x_valid = pd.DataFrame(l1_train[features].iloc[test_index])\n    \n    model = CalibratedClassifierCV(\n        RidgeClassifier(random_state=random_state), \n        cv=3\n    )\n    model.fit(\n        x_train,\n        y_train,\n    )\n\n    train_oof_preds = model.predict_proba(x_valid)[:,-1]\n    test_oof_preds = model.predict_proba(l1_test[features])[:,-1]\n    l2_ridge_train_preds[test_index] = train_oof_preds\n    l2_ridge_test_preds += test_oof_preds \/ n_folds\n    \n    print(\": Ridge - ROC AUC Score = {}\".format(roc_auc_score(y_valid, train_oof_preds, average=\"micro\")))\n    print(\"\")\n    \n    model = LGBMClassifier(\n        random_state=meta_random_seed\n    )\n    model.fit(\n        x_train,\n        y_train,\n    )\n\n    train_oof_preds = model.predict_proba(x_valid)[:,1]\n    test_oof_preds = model.predict_proba(l1_test[features])[:,1]\n    l2_lgbm_train_preds[test_index] = train_oof_preds\n    l2_lgbm_test_preds += test_oof_preds \/ n_folds\n    \n    print(\": LGBM - ROC AUC Score = {}\".format(roc_auc_score(y_valid, train_oof_preds, average=\"micro\")))\n    print(\"\")\n    \nprint(\"--> Overall metrics\")\nprint(\": Ridge - ROC AUC Score = {}\".format(roc_auc_score(y, l2_ridge_train_preds, average=\"micro\")))\nprint(\": LGBM - ROC AUC Score = {}\".format(roc_auc_score(y, l2_lgbm_train_preds, average=\"micro\")))","484ff795":"l1_train.head()","2ca9cf42":"sub_df[\"target\"] = l2_lgbm_test_preds.tolist()\nsub_df.to_csv(\"submission_optimized_l2_lgbm_classifier.csv\", index=False)\n\nsub_df[\"target\"] = l2_ridge_test_preds.tolist()\nsub_df.to_csv(\"submission_optimized_l2_ridge_classifier.csv\", index=False)","9ad91178":"import h2o\nfrom h2o.automl import H2OAutoML\n\nh2o.init(\n    max_mem_size=14,\n    nthreads=12,\n)","f5d6601f":"x = train_df[cb_features]\n\nhf = h2o.H2OFrame(pd.concat([x, y], axis=1))\nx_test_hf = h2o.H2OFrame(test_df[cb_features])\n\nhf.head()","3c8dfc94":"predictors = hf.columns\nlabel = \"target\"\npredictors.remove(label)\n\nhf[label] = hf[label].asfactor()\n\ntrain_hf, valid_hf  = hf.split_frame(ratios=[.8], seed=meta_random_seed)","341b3894":"train_hf.describe()","265f4959":"aml = H2OAutoML(\n    max_runtime_secs=3600, \n    seed=meta_random_seed,\n    exclude_algos = [\"DeepLearning\", \"DRF\"]\n)","044ae654":"aml.train(\n    x=predictors,\n    y=label, \n    training_frame=train_hf,\n    validation_frame=valid_hf,\n)","50b93008":"# View the AutoML Leaderboard\nlb = aml.leaderboard\nlb.head(rows=10)","82c82c29":"aml_leader_test_preds = aml.predict(x_test_hf).as_data_frame()['p1']","59902cac":"sub_df['target'] = list(aml_leader_test_preds)\nsub_df.to_csv(\"submission_aml_leader_30min.csv\", index=False)","46881faa":"# Get model ids for all models in the AutoML Leaderboard\nmodel_ids = list(aml.leaderboard['model_id'].as_data_frame().iloc[:,0])","07106bf4":"tmp_model = h2o.get_model(model_ids[2])\ntmp_model.auc(valid=True)","041c9286":"aml_train_preds = {}\naml_test_preds = {}\n\nfor model_id in model_ids[:5]:\n    tmp_model = h2o.get_model(model_id)\n    \n    aml_train_preds[model_id] = list(tmp_model.predict(hf).as_data_frame()['p1'])\n    aml_test_preds[model_id] = list(tmp_model.predict(x_test_hf).as_data_frame()['p1'])\n\naml_train_preds['target'] = list(y)\n\naml_train_preds_df = pd.DataFrame(data = aml_train_preds)\naml_test_preds_df = pd.DataFrame(data = aml_test_preds)","08b05372":"aml_train_preds_df.head()","0d82c7eb":"aml_train_preds_df.shape, aml_test_preds_df.shape","f944f4c8":"random_state = meta_random_seed\nn_folds = 10\nk_fold = StratifiedKFold(n_splits=n_folds, random_state=random_state, shuffle=True)\n\naml_l2_ridge_train_preds = np.zeros(len(aml_train_preds_df.index), )\naml_l2_ridge_test_preds = np.zeros(len(aml_test_preds_df.index), )\n\naml_l2_lgbm_train_preds = np.zeros(len(aml_train_preds_df.index), )\naml_l2_lgbm_test_preds = np.zeros(len(aml_test_preds_df.index), )\n\nfeatures = aml_test_preds_df.columns\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(aml_train_preds_df, y)):\n    print(\"--> Fold {}\".format(fold + 1))\n    y_train = y.iloc[train_index]\n    y_valid = y.iloc[test_index]\n\n    x_train = pd.DataFrame(aml_train_preds_df[features].iloc[train_index])\n    x_valid = pd.DataFrame(aml_train_preds_df[features].iloc[test_index])\n    \n    model = CalibratedClassifierCV(\n        RidgeClassifier(random_state=random_state), \n        cv=3\n    )\n    model.fit(\n        x_train,\n        y_train,\n    )\n\n    train_oof_preds = model.predict_proba(x_valid)[:,-1]\n    test_oof_preds = model.predict_proba(aml_test_preds_df[features])[:,-1]\n    aml_l2_ridge_train_preds[test_index] = train_oof_preds\n    aml_l2_ridge_test_preds += test_oof_preds \/ n_folds\n    \n    print(\": Ridge - ROC AUC Score = {}\".format(roc_auc_score(y_valid, train_oof_preds, average=\"micro\")))\n    print(\"\")\n    \n    model = LGBMClassifier(\n        random_state=meta_random_seed\n    )\n    model.fit(\n        x_train,\n        y_train,\n    )\n\n    train_oof_preds = model.predict_proba(x_valid)[:,1]\n    test_oof_preds = model.predict_proba(aml_test_preds_df[features])[:,1]\n    aml_l2_lgbm_train_preds[test_index] = train_oof_preds\n    aml_l2_lgbm_test_preds += test_oof_preds \/ n_folds\n    \n    print(\": LGBM - ROC AUC Score = {}\".format(roc_auc_score(y_valid, train_oof_preds, average=\"micro\")))\n    print(\"\")\n    \nprint(\"--> Overall metrics\")\nprint(\": Ridge - ROC AUC Score = {}\".format(roc_auc_score(y, aml_l2_ridge_train_preds, average=\"micro\")))\nprint(\": LGBM - ROC AUC Score = {}\".format(roc_auc_score(y, aml_l2_lgbm_train_preds, average=\"micro\")))","9c63ea2b":"sub_df[\"target\"] = aml_l2_lgbm_test_preds.tolist()\nsub_df.to_csv(\"submission_aml_l2_lgbm_classifier_top5.csv\", index=False)\n\nsub_df[\"target\"] = aml_l2_ridge_test_preds.tolist()\nsub_df.to_csv(\"submission_aml_l2_ridge_classifier_top5.csv\", index=False)","b3719df4":"aml_leader_test_preds = aml.predict(x_test_hf).as_data_frame()['p1']\naml_leader_train_preds = aml.predict(hf).as_data_frame()['p1']","d97f5565":"from scipy.special import expit\nfrom sklearn.calibration import CalibratedClassifierCV\n\nrandom_state = meta_random_seed\nn_folds = 10\nk_fold = StratifiedKFold(n_splits=n_folds, random_state=random_state, shuffle=True)\n\nl1_train = pd.DataFrame(data={\n    \"xgb\": xgb_train_preds.tolist(),\n    \"lgbm\": lgbm_train_preds.tolist(),\n    \"cb\": cb_train_preds.tolist(),\n    \"ridge\": ridge_train_preds.tolist(),\n    \"aml\": list(aml_leader_train_preds),\n    \"target\": y.tolist()\n})\nl1_test = pd.DataFrame(data={\n    \"xgb\": xgb_test_preds.tolist(),\n    \"lgbm\": lgbm_test_preds.tolist(),\n    \"cb\": cb_test_preds.tolist(),\n    \"ridge\": ridge_test_preds.tolist(),\n    \"aml\": list(aml_leader_test_preds)\n})\n\nl2_ridge_train_preds = np.zeros(len(l1_train.index), )\nl2_ridge_test_preds = np.zeros(len(l1_test.index), )\n\nl2_lgbm_train_preds = np.zeros(len(l1_train.index), )\nl2_lgbm_test_preds = np.zeros(len(l1_test.index), )\n\nfeatures = [\"xgb\", \"lgbm\", \"cb\", \"ridge\", \"aml\"]\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(l1_train, y)):\n    print(\"--> Fold {}\".format(fold + 1))\n    y_train = y.iloc[train_index]\n    y_valid = y.iloc[test_index]\n\n    x_train = pd.DataFrame(l1_train[features].iloc[train_index])\n    x_valid = pd.DataFrame(l1_train[features].iloc[test_index])\n    \n    model = CalibratedClassifierCV(\n        RidgeClassifier(random_state=random_state), \n        cv=3\n    )\n    model.fit(\n        x_train,\n        y_train,\n    )\n\n    train_oof_preds = model.predict_proba(x_valid)[:,-1]\n    test_oof_preds = model.predict_proba(l1_test[features])[:,-1]\n    l2_ridge_train_preds[test_index] = train_oof_preds\n    l2_ridge_test_preds += test_oof_preds \/ n_folds\n    \n    print(\": Ridge - ROC AUC Score = {}\".format(roc_auc_score(y_valid, train_oof_preds, average=\"micro\")))\n    print(\"\")\n    \n    model = LGBMClassifier(\n        random_state=meta_random_seed\n    )\n    model.fit(\n        x_train,\n        y_train,\n    )\n\n    train_oof_preds = model.predict_proba(x_valid)[:,1]\n    test_oof_preds = model.predict_proba(l1_test[features])[:,1]\n    l2_lgbm_train_preds[test_index] = train_oof_preds\n    l2_lgbm_test_preds += test_oof_preds \/ n_folds\n    \n    print(\": LGBM - ROC AUC Score = {}\".format(roc_auc_score(y_valid, train_oof_preds, average=\"micro\")))\n    print(\"\")\n    \nprint(\"--> Overall metrics\")\nprint(\": Ridge - ROC AUC Score = {}\".format(roc_auc_score(y, l2_ridge_train_preds, average=\"micro\")))\nprint(\": LGBM - ROC AUC Score = {}\".format(roc_auc_score(y, l2_lgbm_train_preds, average=\"micro\")))","eb37a455":"sub_df[\"target\"] = l2_lgbm_test_preds.tolist()\nsub_df.to_csv(\"submission_optimized_aml_l2_lgbm_classifier.csv\", index=False)\n\nsub_df[\"target\"] = l2_ridge_test_preds.tolist()\nsub_df.to_csv(\"submission_optimized_aml_l2_ridge_classifier.csv\", index=False)","ae5ca84d":"<a id='2'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">2. Feature Engineering \ud83d\udd27<\/p>","dc4abbeb":"<a id='1'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">1. Data visualization \ud83d\udcca<\/p>","91b653f0":"<a id='5'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">5. Models Optimization with Optuna \u26f7<\/p>","cd6dfdd5":"### Label Encoder\n\n![LabelEncoder.png](attachment:LabelEncoder.png)","81a134b8":"<a id='3.1'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center\">3.1 XGBoost<\/p>","1e099aae":"<a id='3'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">3. Base Model \u2699\ufe0f<\/p>","5e9cd86f":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:180%; text-align:center\">Tabular Playground Series \ud83d\udcda - March 2021 \ud83d\udcc8<\/p>\n\n![kaggle-python.png](attachment:kaggle-python.png)","2341ac6c":"Lets analyze the results:\n- **cat1**: We can merge D and E as they have really low representation\n- **cat2**: We can merge all values diferent than: ['A', 'C', 'D', 'G', 'F', 'J', 'I', 'M', 'Q', 'L', 'O'] as they have really low representation.\n- **cat3**: ['I', 'L', 'K'] and ['H', 'J', 'G'] can be merged.\n- **cat4**: ['C', 'S', 'T', 'R'] can be merged into 'C' and others than ['E', 'F', 'D', 'G', 'H', 'J', 'K', 'I', 'C'] can be deleted\n- **cat5**: Other than ['BI', 'AB', 'BU', 'K', 'G', 'BQ', 'N'] can be deleted.\n- **cat7**: ['Y', 'AA', 'R', 'O', 'AP', 'AY'] and ['AL', 'V', 'BA', 'AC', 'AD', 'L'] can be merged.\n- **cat10**: ['GE', 'LN', 'HJ', 'IG', 'EK', 'HB', 'DF'] and ['CD', 'GI', 'HC', 'JR', 'MC', 'FR', 'GK'] can be merged.","76fcb231":"## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center\">Table of Content<\/p>\n\n* [1. Data visualization \ud83d\udcca](#1)\n    * [1.1 Target](#1.1)\n    * [1.2 Numerical Columns](#1.2)\n    * [1.3 Categorical Columns](#1.3)\n* [2. Feature Engineering \ud83d\udd27](#2)\n* [3. Base Model \u2699\ufe0f](#3)\n    * [3.1 XGBoost](#3.1)\n    * [3.2 LGBM](#3.2)\n* [4. Essemble models \ud83c\udfc2](#4)\n    * [4.1 L1 classification \ud83d\udcdd](#4.1)\n    * [4.2 Blended classification \ud83d\udcc8](#4.2)\n    * [4.3 L2 classification \ud83d\udcdd](#4.3)\n* [5. Models Optimization with Optuna \u26f7](#5)\n    * [5.1 L1 classification \ud83d\udcdd](#5.1)\n    * [5.2 L1 classification \ud83d\udcdd](#5.2)\n* [6. H2O AutoML \ud83e\uddee](#6)\n    * [6.1 H2O AutoML Submission \ud83d\udcdd](#6.1)\n    * [6.2 L2 AutoML Classification](#6.2)\n    * [6.3 L2 Optimized + AutoML leader Classification](#6.3)\n* [7. Fianl Submission](#7)","fba8f1fb":"<a id='6.3'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center\">6.3 L2 Optimized + AutoML leader Classification<\/p>","673c13ac":"### Submission","d13e56f5":"<a id='6'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">6. H2O AutoML<\/p>","6d3fad0d":"<a id='5.2'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center\">5.2 L2 Optimized Classification<\/p>","0137fb18":"### High cardinality variables","efbd3cbd":"<a id='5.1'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center\">5.1 L1 Optimized Classification<\/p>","4c0b50c5":"<a id='4.3'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center\">4.3 L2 classification<\/p>\n\nPredict the target over the L1 (level 1) predicted probabilities.","759cd10a":"### Predict with the TOP 5 models","a24ae77e":"<a id='1.3'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center\">1.3 Categorical Variables<\/p>","b1517771":"<a id='4.2'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center\">4.2 Blended classification<\/p>","f31f642a":"As we can see, the numerical columns are low correlated between them and the distribution of all of them doesn't variate over target value.","fe0291f3":"### Aggregate values","179b12e2":"<a id='4.1'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center\">4.1 L1 classification<\/p>\n\nThis will be the first level classification, where many algorithms will predict in the train and test sets. Wi will then essemble them and try a second level classification.","5588839b":"<a id='4'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">4. Essemble models \ud83c\udfc2<\/p>","61eccc88":"### Submission","7940b067":"### Standarize numerical features","f233d303":"<a id='6.2'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center\">6.2 L2 AutoML Classification<\/p>","7f08c0be":"<a id='3.2'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center\">3.2 LGBM<\/p>","5dcc04e8":"<a id='1.2'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center\">1.2 Numerical Variables<\/p>","ce7cbf50":"As we can see, many categorical columns has values that only apply to one category (0 or 1) so we can engineer this variables.","34104691":"<a id='1.1'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center\">1.1 Target Variable<\/p>"}}