{"cell_type":{"e75a3706":"code","50fd0632":"code","649c57bd":"code","825a9819":"code","dca68a72":"code","f29c527e":"code","a5367c58":"code","32e3ac7e":"code","27d804a7":"code","11e356cf":"code","a7c963b3":"code","268dc2da":"code","187aaa04":"code","2c3898b6":"code","f96b77c9":"code","17202cd4":"code","ef751668":"code","99fec421":"code","aca87b72":"code","1ceb5e69":"code","b32b3be0":"code","de1acc11":"code","d066ae93":"code","865bf15a":"code","3e452303":"code","25d7a4e4":"code","a63495b7":"code","caba45ff":"code","b464b110":"code","3e9aec7e":"code","c8eb6490":"code","fab893e3":"code","ea8379af":"code","c32d82f7":"code","3262d9ae":"code","46eb0545":"code","5f5349d6":"code","b43f3cfc":"code","7d1ce3d8":"code","52c4c0ef":"code","70d92105":"code","2353b674":"code","e1f7ebde":"markdown","1ed500ed":"markdown","2a6c5fae":"markdown","d9b86ff0":"markdown","10be5df4":"markdown","95fd81b8":"markdown","66806dba":"markdown","0481f676":"markdown","6e610733":"markdown","ca9f328d":"markdown","ab74b759":"markdown","0996f72e":"markdown","a7a52b6c":"markdown","e15df193":"markdown","d0035f99":"markdown","98ebc8f3":"markdown","b35a1638":"markdown","cb20f55c":"markdown","5140fa52":"markdown","6030675a":"markdown","4397a389":"markdown","51e4abd4":"markdown","87abec7b":"markdown","85206d79":"markdown","0c86747a":"markdown","de147c44":"markdown","e35d7066":"markdown","76183250":"markdown","0e88d412":"markdown","27002c2f":"markdown","f51d7fbd":"markdown","094bf37a":"markdown","cc1d7881":"markdown","943570ee":"markdown","4480e21d":"markdown","a6ef19e1":"markdown","e258d02d":"markdown","65cee5c7":"markdown","d21c52fb":"markdown","4f245fe6":"markdown","7d6aa4c5":"markdown","2d86ffb0":"markdown","d158d9cd":"markdown","1995e8a7":"markdown","eabc6934":"markdown","8fb7b069":"markdown","c0b8d41b":"markdown","92d89f40":"markdown","56bdcaf4":"markdown","90ee1f38":"markdown","ada71979":"markdown","dc2e6447":"markdown","1ff7e39b":"markdown"},"source":{"e75a3706":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","50fd0632":"import numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# get the features from the file features.txt\nfeatures = list()\nwith open('\/kaggle\/input\/uci-har\/UCI HAR Dataset for Kaggle\/UCI HAR Dataset for Kaggle\/features.txt') as f:\n    features = [line.split()[1] for line in f.readlines()]\nprint('No of Features: {}'.format(len(features)))","649c57bd":"# get the data from txt files to pandas dataframe\nX_train = pd.read_csv('\/kaggle\/input\/uci-har\/UCI HAR Dataset for Kaggle\/UCI HAR Dataset for Kaggle\/train\/X_train.txt', delim_whitespace=True, header=None)\nX_train.columns = features\n#delim_whitespace: \uacf5\ubc31\uc73c\ub85c \uad6c\ubd84 \ub41c \uac12\uc73c\ub85c \ud30c\uc77c \uc77d\uae30\n\n# add subject column to the dataframe\nX_train['subject'] = pd.read_csv('\/kaggle\/input\/uci-har\/UCI HAR Dataset for Kaggle\/UCI HAR Dataset for Kaggle\/train\/subject_train.txt', header=None, squeeze=True)\n\ny_train = pd.read_csv('\/kaggle\/input\/uci-har\/UCI HAR Dataset for Kaggle\/UCI HAR Dataset for Kaggle\/train\/y_train.txt', names=['Activity'], squeeze=True)\ny_train_labels = y_train.map({1: 'WALKING', 2:'WALKING_UPSTAIRS',3:'WALKING_DOWNSTAIRS',\\\n                       4:'SITTING', 5:'STANDING',6:'LAYING'})\n#squeeze: DataFrame\uc744 \ub2e8\uc77c \uce7c\ub7fc\uc73c\ub85c \ubc18\ud658\n\n# put all columns in a single dataframe\ntrain = X_train\ntrain['Activity'] = y_train\ntrain['ActivityName'] = y_train_labels\ntrain.sample()","825a9819":"train.shape","dca68a72":"# get the data from txt files to pandas dataffame\nX_test = pd.read_csv('\/kaggle\/input\/uci-har\/UCI HAR Dataset for Kaggle\/UCI HAR Dataset for Kaggle\/test\/X_test.txt', delim_whitespace=True, header=None)\nX_test.columns = features\n\n# add subject column to the dataframe\nX_test['subject'] = pd.read_csv('\/kaggle\/input\/uci-har\/UCI HAR Dataset for Kaggle\/UCI HAR Dataset for Kaggle\/test\/subject_test.txt', header=None, squeeze=True)\n\n# get y labels from the txt file\ny_test = pd.read_csv('\/kaggle\/input\/y-test\/y_test.txt', names=['Activity'], squeeze=True)\ny_test_labels = y_test.map({1: 'WALKING', 2:'WALKING_UPSTAIRS',3:'WALKING_DOWNSTAIRS',\\\n                       4:'SITTING', 5:'STANDING',6:'LAYING'})\n\n\n# put all columns in a single dataframe\ntest = X_test\ntest['Activity'] = y_test\ntest['ActivityName'] = y_test_labels\ntest.sample()","f29c527e":"test.shape","a5367c58":"train.columns","32e3ac7e":"# \ud30c\uc77c\uba85.duplicated()\nprint('No of duplicates in train: {}'.format(sum(train.duplicated())))\nprint('No of duplicates in test : {}'.format(sum(test.duplicated())))","27d804a7":"print('We have {} NaN\/Null values in train'.format(train.isnull().values.sum()))\nprint('We have {} NaN\/Null values in test'.format(test.isnull().values.sum()))","11e356cf":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('whitegrid')","a7c963b3":"plt.figure(figsize=(16,8))\nplt.title('Data provided by each user', fontsize=20)\nsns.countplot(x='subject',hue='ActivityName', data = train)\nplt.show()","268dc2da":"plt.title('No of Datapoints per Activity', fontsize=15)\nsns.countplot(train.ActivityName)\nplt.xticks(rotation=90)\nplt.show()","187aaa04":"columns = train.columns\n\n# Removing '()' from column names\ncolumns = columns.str.replace('[()]','')\ncolumns = columns.str.replace('[-]', '_')\ncolumns = columns.str.replace('[,]','')\n\ntrain.columns = columns\ntest.columns = columns\n\ntest.columns","2c3898b6":"train.to_csv('train.csv', index=False)\ntest.to_csv('test.csv', index=False)","f96b77c9":"# sns.displot: \uc5ec\ub7ec\uac1c \uadf8\ub8f9\uc758 \ucee4\ub110 \ubc00\ub3c4 \uace1\uc120 \uadf8\ub9ac\uae30\nsns.set_palette(\"Set1\", desat=0.80)\nfacetgrid = sns.FacetGrid(train, hue='ActivityName', size=6,aspect=2)\nfacetgrid.map(sns.distplot,'tBodyAccMag_mean', hist=False)\\\n    .add_legend()\nplt.annotate(\"Stationary Activities\", xy=(-0.956,14), xytext=(-0.9, 23), size=20,\\\n            va='center', ha='left',\\\n            arrowprops=dict(arrowstyle=\"simple\",connectionstyle=\"arc3,rad=0.1\"))\n\nplt.annotate(\"Moving Activities\", xy=(0,3), xytext=(0.2, 9), size=20,\\\n            va='center', ha='left',\\\n            arrowprops=dict(arrowstyle=\"simple\",connectionstyle=\"arc3,rad=0.1\"))\nplt.show()","17202cd4":"# for plotting purposes taking datapoints of each activity to a different dataframe\ndf1 = train[train['Activity']==1]\ndf2 = train[train['Activity']==2]\ndf3 = train[train['Activity']==3]\ndf4 = train[train['Activity']==4]\ndf5 = train[train['Activity']==5]\ndf6 = train[train['Activity']==6]\n\nplt.figure(figsize=(14,7))\nplt.subplot(2,2,1)\nplt.title('Stationary Activities(Zoomed in)')\nsns.distplot(df4['tBodyAccMag_mean'],color = 'r',hist = False, label = 'Sitting')\nsns.distplot(df5['tBodyAccMag_mean'],color = 'm',hist = False,label = 'Standing')\nsns.distplot(df6['tBodyAccMag_mean'],color = 'c',hist = False, label = 'Laying')\nplt.axis([-1.01, -0.5, 0, 35])\nplt.legend(loc='center')\n\nplt.subplot(2,2,2)\nplt.title('Moving Activities')\nsns.distplot(df1['tBodyAccMag_mean'],color = 'red',hist = False, label = 'Walking')\nsns.distplot(df2['tBodyAccMag_mean'],color = 'blue',hist = False,label = 'Walking Up')\nsns.distplot(df3['tBodyAccMag_mean'],color = 'green',hist = False, label = 'Walking down')\nplt.legend(loc='center right')\n\n\nplt.tight_layout()\nplt.show()","ef751668":"plt.figure(figsize=(7,7))\nsns.boxplot(x='ActivityName', y='tBodyAccMag_mean',data=train, showfliers=False, saturation=1)\nplt.ylabel('Acceleration Magnitude mean')\nplt.axhline(y=-0.7, xmin=0.1, xmax=0.9,dashes=(5,5), c='g')\nplt.axhline(y=-0.05, xmin=0.4, dashes=(5,5), c='m')\nplt.xticks(rotation=90)\nplt.show()","99fec421":"sns.boxplot(x='ActivityName', y='angleXgravityMean', data=train)\nplt.axhline(y=0.08, xmin=0.1, xmax=0.9,c='m',dashes=(5,3))\nplt.title('Angle between X-axis and Gravity_mean', fontsize=15)\nplt.xticks(rotation = 40)\nplt.show()","aca87b72":"sns.boxplot(x='ActivityName', y='angleYgravityMean', data = train, showfliers=False)\nplt.title('Angle between Y-axis and Gravity_mean', fontsize=15)\nplt.xticks(rotation = 40)\nplt.axhline(y=-0.22, xmin=0.1, xmax=0.8, dashes=(5,3), c='m')\nplt.show()","1ceb5e69":"import numpy as np\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport seaborn as sns","b32b3be0":"# performs t-sne with different perplexity values and their repective plots..\n\ndef perform_tsne(X_data, y_data, perplexities, n_iter=1000, img_name_prefix='t-sne'):\n        \n    for index,perplexity in enumerate(perplexities):\n        # perform t-sne\n        print('\\nperforming tsne with perplexity {} and with {} iterations at max'.format(perplexity, n_iter))\n        X_reduced = TSNE(verbose=2, perplexity=perplexity).fit_transform(X_data)\n        print('Done..')\n        \n        # prepare the data for seaborn         \n        print('Creating plot for this t-sne visualization..')\n        df = pd.DataFrame({'x':X_reduced[:,0], 'y':X_reduced[:,1] ,'label':y_data})\n        \n        # draw the plot in appropriate place in the grid\n        sns.lmplot(data=df, x='x', y='y', hue='label', fit_reg=False, size=8,\\\n                   palette=\"Set1\",markers=['^','v','s','o', '1','2'])\n        plt.title(\"perplexity : {} and max_iter : {}\".format(perplexity, n_iter))\n        img_name = img_name_prefix + '_perp_{}_iter_{}.png'.format(perplexity, n_iter)\n        print('saving this plot as image in present working directory...')\n        plt.savefig(img_name)\n        plt.show()\n        print('Done')","de1acc11":"X_pre_tsne = train.drop(['subject', 'Activity','ActivityName'], axis=1)\ny_pre_tsne = train['ActivityName']\nperform_tsne(X_data = X_pre_tsne,y_data=y_pre_tsne, perplexities =[2,5,10,20,50])","d066ae93":"#train = pd.read_csv('UCI_HAR_Dataset\/csv_files\/train.csv')\n#test = pd.read_csv('UCI_HAR_Dataset\/csv_files\/test.csv')\nprint(train.shape, test.shape)","865bf15a":"train.head(1)","3e452303":"# get X_train and y_train from csv files\nX_train = train.drop(['subject', 'Activity', 'ActivityName'], axis=1)\ny_train = train.ActivityName","25d7a4e4":"\n# get X_test and y_test from test csv file\nX_test = test.drop(['subject', 'Activity', 'ActivityName'], axis=1)\ny_test = test.ActivityName","a63495b7":"print('X_train and y_train : ({},{})'.format(X_train.shape, y_train.shape))\nprint('X_test  and y_test  : ({},{})'.format(X_test.shape, y_test.shape))","caba45ff":"labels=['LAYING', 'SITTING','STANDING','WALKING','WALKING_DOWNSTAIRS','WALKING_UPSTAIRS']","b464b110":"import itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","3e9aec7e":"from datetime import datetime\ndef perform_model(model, X_train, y_train, X_test, y_test, class_labels, cm_normalize=True, \\\n                 print_cm=True, cm_cmap=plt.cm.Greens):\n    \n    # to store results at various phases\n    results = dict()\n    \n    # time at which model starts training \n    train_start_time = datetime.now()\n    print('training the model..')\n    model.fit(X_train, y_train)\n    print('Done \\n \\n')\n    train_end_time = datetime.now()\n    results['training_time'] =  train_end_time - train_start_time\n    print('training_time(HH:MM:SS.ms) - {}\\n\\n'.format(results['training_time']))\n    \n    \n    # predict test data\n    print('Predicting test data')\n    test_start_time = datetime.now()\n    y_pred = model.predict(X_test)\n    test_end_time = datetime.now()\n    print('Done \\n \\n')\n    results['testing_time'] = test_end_time - test_start_time\n    print('testing time(HH:MM:SS:ms) - {}\\n\\n'.format(results['testing_time']))\n    results['predicted'] = y_pred\n   \n\n    # calculate overall accuracty of the model\n    accuracy = metrics.accuracy_score(y_true=y_test, y_pred=y_pred)\n    # store accuracy in results\n    results['accuracy'] = accuracy\n    print('---------------------')\n    print('|      Accuracy      |')\n    print('---------------------')\n    print('\\n    {}\\n\\n'.format(accuracy))\n    \n    \n    # confusion matrix\n    cm = metrics.confusion_matrix(y_test, y_pred)\n    results['confusion_matrix'] = cm\n    if print_cm: \n        print('--------------------')\n        print('| Confusion Matrix |')\n        print('--------------------')\n        print('\\n {}'.format(cm))\n        \n    # plot confusin matrix\n    plt.figure(figsize=(8,8))\n    plt.grid(b=False)\n    plot_confusion_matrix(cm, classes=class_labels, normalize=True, title='Normalized confusion matrix', cmap = cm_cmap)\n    plt.show()\n    \n    # get classification report\n    print('-------------------------')\n    print('| Classifiction Report |')\n    print('-------------------------')\n    classification_report = metrics.classification_report(y_test, y_pred)\n    # store report in results\n    results['classification_report'] = classification_report\n    print(classification_report)\n    \n    # add the trained  model to the results\n    results['model'] = model\n    \n    return results","c8eb6490":"def print_grid_search_attributes(model):\n    # Estimator that gave highest score among all the estimators formed in GridSearch\n    print('--------------------------')\n    print('|      Best Estimator     |')\n    print('--------------------------')\n    print('\\n\\t{}\\n'.format(model.best_estimator_))\n\n\n    # parameters that gave best results while performing grid search\n    print('--------------------------')\n    print('|     Best parameters     |')\n    print('--------------------------')\n    print('\\tParameters of best estimator : \\n\\n\\t{}\\n'.format(model.best_params_))\n\n\n    #  number of cross validation splits\n    print('---------------------------------')\n    print('|   No of CrossValidation sets   |')\n    print('--------------------------------')\n    print('\\n\\tTotal numbre of cross validation sets: {}\\n'.format(model.n_splits_))\n\n\n    # Average cross validated score of the best estimator, from the Grid Search \n    print('--------------------------')\n    print('|        Best Score       |')\n    print('--------------------------')\n    print('\\n\\tAverage Cross Validate scores of best estimator : \\n\\n\\t{}\\n'.format(model.best_score_))","fab893e3":"from sklearn import linear_model\nfrom sklearn import metrics\n\nfrom sklearn.model_selection import GridSearchCV","ea8379af":"# start Grid search\nparameters = {'C':[0.01, 0.1, 1, 10, 20, 30], 'penalty':['l2','l1']}\nlog_reg = linear_model.LogisticRegression()\nlog_reg_grid = GridSearchCV(log_reg, param_grid=parameters, cv=3, verbose=1, n_jobs=8)\nlog_reg_grid_results =  perform_model(log_reg_grid, X_train, y_train, X_test, y_test, class_labels=labels)","c32d82f7":"plt.figure(figsize=(8,8))\nplt.grid(b=False)\nplot_confusion_matrix(log_reg_grid_results['confusion_matrix'], classes=labels, cmap=plt.cm.Greens, )\nplt.show()","3262d9ae":"# observe the attributes of the model \nprint_grid_search_attributes(log_reg_grid_results['model'])","46eb0545":"from sklearn.svm import LinearSVC","5f5349d6":"parameters = {'C':[0.125, 0.5, 1, 2, 8, 16]}\nlr_svc = LinearSVC(tol=0.00005)\nlr_svc_grid = GridSearchCV(lr_svc, param_grid=parameters, n_jobs=8, verbose=1)\nlr_svc_grid_results = perform_model(lr_svc_grid, X_train, y_train, X_test, y_test, class_labels=labels)","b43f3cfc":"print_grid_search_attributes(lr_svc_grid_results['model'])","7d1ce3d8":"\nfrom sklearn.svm import SVC\nparameters = {'C':[2,8,16],\\\n              'gamma': [ 0.0078125, 0.125, 2]}\nrbf_svm = SVC(kernel='rbf')\nrbf_svm_grid = GridSearchCV(rbf_svm,param_grid=parameters,n_jobs=8)\nrbf_svm_grid_results = perform_model(rbf_svm_grid, X_train, y_train, X_test, y_test, class_labels=labels)","52c4c0ef":"from sklearn.tree import DecisionTreeClassifier\nparameters = {'max_depth':np.arange(3,10,2)}\ndt = DecisionTreeClassifier()\ndt_grid = GridSearchCV(dt,param_grid=parameters, n_jobs=8)\ndt_grid_results = perform_model(dt_grid, X_train, y_train, X_test, y_test, class_labels=labels)\nprint_grid_search_attributes(dt_grid_results['model'])","70d92105":"from sklearn.ensemble import RandomForestClassifier\nparams = {'n_estimators': np.arange(10,201,20), 'max_depth':np.arange(3,15,2)}\nrfc = RandomForestClassifier()\nrfc_grid = GridSearchCV(rfc, param_grid=params, n_jobs=8)\nrfc_grid_results = perform_model(rfc_grid, X_train, y_train, X_test, y_test, class_labels=labels)\nprint_grid_search_attributes(rfc_grid_results['model'])","2353b674":"print('\\n                     Accuracy     Error')\nprint('                     ----------   --------')\nprint('Logistic Regression : {:.04}%       {:.04}%'.format(log_reg_grid_results['accuracy'] * 100,\\\n                                                  100-(log_reg_grid_results['accuracy'] * 100)))\n\nprint('Linear SVC          : {:.04}%       {:.04}% '.format(lr_svc_grid_results['accuracy'] * 100,\\\n                                                        100-(lr_svc_grid_results['accuracy'] * 100)))\n\nprint('rbf SVM classifier  : {:.04}%      {:.04}% '.format(rbf_svm_grid_results['accuracy'] * 100,\\\n                                                          100-(rbf_svm_grid_results['accuracy'] * 100)))\n\nprint('DecisionTree        : {:.04}%      {:.04}% '.format(dt_grid_results['accuracy'] * 100,\\\n                                                        100-(dt_grid_results['accuracy'] * 100)))\n\nprint('Random Forest       : {:.04}%      {:.04}% '.format(rfc_grid_results['accuracy'] * 100,\\\n                                                           100-(rfc_grid_results['accuracy'] * 100)))\n#print('GradientBoosting DT : {:.04}%      {:.04}% '.format(rfc_grid_results['accuracy'] * 100,\\\n#                                                        100-(rfc_grid_results['accuracy'] * 100)))","e1f7ebde":"Linear SVC\uac00 Error\uc728\uc774 \uc81c\uc77c \uc801\uace0, Accuracy\uac00 \uc81c\uc77c \ub192\ub2e4.","1ed500ed":"Ensembling\uc5d0\uc11c Random Forest\ub294 Bagging(\uc0d8\ud50c\uc744 \uc5ec\ub7ec\ubc88 \ubf51\uc544 \ubaa8\ub378 \ud559\uc2b5)\uacc4\uc5f4\uc774\ub77c\uba74 Gradient Boosting\uc740 Boosting(\uac00\uc911\uce58\ub97c \ud65c\uc6a9\ud558\uc5ec \ubd84\ub958\uae30 \uc5c5\ub370\uc774\ud2b8)\uacc4\uc5f4","2a6c5fae":"# Apply t-sne on the data","d9b86ff0":"\uace0\ucc28\uc6d0 \ubca1\ud130\ub97c 2\ucc28\uc6d0\uc73c\ub85c \uc2dc\uac01\ud654(\ucc28\uc6d0 \ucd95\uc18c)\n\nT\ubd84\ud3ec \uadf8\ub798\ud504\uc0c1\uc5d0\uc11c T\ubd84\ud3ec\uc758 \uac12\uc744 \uce5c\ubc00\ub3c4\ub85c \uacc4\uc0b0\ud558\uc5ec \uce5c\ubc00\ub3c4\uac00 \uac00\uae4c\uc6b4 \uac12 \ub07c\ub9ac \ubb36\uae30\n\nperplexity: \ud559\uc2b5\uc5d0 \uc601\ud5a5\uc744 \uc8fc\ub294 \uc810\ub4e4\uc758 \uac1c\uc218(perplexity\uac00 \ub192\uc744\uc218\ub85d \ubd84\uc0b0\uc774 \ub192\ub2e4)","10be5df4":"### Final prediction pipeline\n\n1. \uc704\uc5d0\uc11c \uc800\uc7a5\ud55c 3\uac1c\uc758 \ubaa8\ub378(dynamic\/static\ubd84\ub958 \ubaa8\ub378, dynamic \ubd84\ub958 \ubaa8\ub378, static\ubd84\ub958 \ubaa8\ub378)\uacfc scaling\uc5d0 \ud544\uc694\ud55c \ud30c\uc77c\ub4e4 \ubd88\ub7ec\uc624\uae30\n\n2. predict_activity\ud568\uc218 \ub9cc\ub4e4\uae30\n\n    1) model_2class\ub85c X_static, X_dynamic \uace0\ub974\uae30.\n\n    2) static\uacfc dynamic \ubaa8\ub378\uc744 \uc774\uc6a9\ud574\uc11c \uac01 \ub370\uc774\ud130\uc5d0 \uc801\uc6a9\ud6c4 \uc138\ubd80 activity\ubd84\ub958\ud558\uae30. predict\ub85c \ubd84\ub958\ud560\ub54c\ub294 \ucd5c\ub300 \ud655\ub960\uc778 index\uac12 \ubc18\ud658\ud574\uc11c activity label\uc5d0 \ub9de\uac8c\ub054 +1\ud639\uc740 +4\ucd94\uac00\n\n    3) final_pred\ub9ac\uc2a4\ud2b8\uc5d0 \ub450 \ubaa8\ub378 \uacb0\uacfc\ub97c 1)\uacb0\uacfc \uc21c\uc11c\uc5d0 \ub9de\uac8c \ub123\uae30\n\nAccuracy of train data 0.9832698585418934\n\nAccuracy of validation data 0.9684424838819138\n\nDivide and Conquer approach with CNN\uc774 \uc88b\uc740 \uacb0\uacfc \ubcf4\uc5ec\uc900\ub2e4..","95fd81b8":"# Obtain the train and test data","66806dba":"# Obtain the test data","0481f676":"SVC\ub85c\ub3c4 \uc644\ubcbd\ud788 \ubd84\ub958\ud558\uae30 \uc5b4\ub824\uc6b8\ub54c \uc0ac\uc6a9. \uc704\uc5d0\uc11c\ub294 Linear(\uc120\ud615) \ubd84\ub958\uacbd\uacc4\uba74\uc744 \ub9cc\ub4e4\uc5c8\uc9c0\ub9cc, \uc774 \ubaa8\ub378\uc758 Kernel\uc740 rbf(Radial Basis Function or Gaussian Kernel\uc774\ub77c\uace0\ub3c4 \ubd88\ub9bc)\ub85c \ube44\uc120\ud615 \uacbd\uacc4\uba74","6e610733":"# Using raw time series data and deep learning methods:\nApproch 1 - Using LSTM\n\nApproch 2 - Using CNN - CNN are useful to get best features and realtions between sequnce data using convolution.\n\nApproch 3 - Using some cascading techniques.","ca9f328d":"### Method to print the gridsearch Attributes","ab74b759":"# Using some cascading techniques.\n\n### Divide and Conquer-Based:\nDivide and Conquer-Based 1D CNN Human Activity Recognition Using Test Data Sharpening ( https:\/\/www.mdpi.com\/1424-8220\/18\/4\/1055\/pdf )\ub97c \ucc38\uace0\ud558\uc5ec Dynamic\ud55c \ub3d9\uc791(walking, walking_upstairs, walking_downstairs)\uc744 category 0\uc73c\ub85c \ud558\uace0, Static\ud55c \ub3d9\uc791(sitting, standing, laying)\uc744 category 1\ub85c \ud574\uc11c \ub450 class\uc6a9\uc778 2\uac1c\uc758 \ubd84\ub958\uae30\ub97c \ub354 \uc0ac\uc6a9\ud574\uc11c dynamic\/static \ub3d9\uc791\ub4e4 \uc704\ud55c features \ud559\uc2b5\n\n\nStandardScaler\uc801\uc6a9, https:\/\/github.com\/Snehitha17\/human-activity-recognition\/blob\/master\/Scale_2class.p \uc5d0 \uc788\ub294 Scale_2class.p \ub2e4\uc6b4\ubc1b\uc544 scaling \ucd94\uac00","0996f72e":"## 4. Position of GravityAccelerationComponants also matters","a7a52b6c":"## 3. Magnitude of an acceleration can saperate it well","e15df193":"# Using CNN\n\nStandardScaler\uc801\uc6a9\n\n* Base Model\n\n  Conv1D(32filters, 3kernel_size, relu, he_uniform)\n  Conv1D(\uc704\uc640 \ub3d9\uc77c)\n  Dropout(0.6)\n  MaxPooling1D(pool_size=2)\n  Flatten()\n  Dense(50, relu)\n  Dense(softmax)\n  \n  \nloss = 'categorical_crossentropy', optimizer='adam', metrics='accuracy'\n\n\ub9c8\uc9c0\ub9c9 epoch\uc77c\ub54c: loss: 0.0531 - acc: 0.9897 - val_loss: 0.6342 - val_acc: 0.9152\n--> Overfitting\ub418\uc5b4\uc11c \uaddc\uc81c\uc801\uc6a9\ud558\uae30\n\n* \uaddc\uc81c \uc801\uc6a9 Model\n  Conv1D(\uc704\uc640 \ub3d9\uc77c, l2(0.1))\n  Conv1D(16filters, \uc704\uc640 \ub3d9\uc77c, l2(0.06))\n  Dropout(0.65)\n  MaxPooling1D(pool_size=2)\n  Flatten()\n  Dense(32, relu)\n  Dense(softmax)\n  \n\ub9c8\uc9c0\ub9c9 epoch\uc77c\ub54c: loss: 0.1754 - acc: 0.9467 - val_loss: 0.3478 - val_acc: 0.8958","d0035f99":"# https:\/\/github.com\/UdiBhaskar\/Human-Activity-Recognition--Using-Deep-NN\/blob\/master\/Human%20Activity%20Detection.ipynb\n\uc758 \ucf54\ub4dc \uc774\ud574\ud558\uae30.","98ebc8f3":"### Observations:\n\nIf angleX,gravityMean > 0 then Activity is Laying.\n\nWe can classify all datapoints belonging to Laying activity with just a single if else statement.","b35a1638":"# Obtain the train data","cb20f55c":"\uacb0\uc815 \ud2b8\ub9ac. T\/F\uc5d0 \ub530\ub77c \ub098\ubb34 \uae4a\uc774\uac00 \uae4a\uc5b4\uc9c0\uba74\uc11c \ubd84\ub958\uac00 \ub41c\ub2e4.","5140fa52":"-> data\uac00 balanced","6030675a":"### Function to plot the confusion matrix","4397a389":"# 1. Logistic Regression with Grid Search","51e4abd4":"\ubaa8\ub4e0 subjects\uc5d0 \ub300\ud574 \uac70\uc758 \ube44\uc2b7\ud55c \uc22b\uc790.","87abec7b":"### Classification of Dynamic activities :\n\nStandardScaler\uc801\uc6a9, https:\/\/github.com\/Snehitha17\/human-activity-recognition\/blob\/master\/Scale_dynamic.p \uc5d0 \uc788\ub294 Scale_dynamic.p \ub2e4\uc6b4\ubc1b\uc544 scaling \ucd94\uac00\n\n* Baseline Model:\nConv1D, Conv1D, Dropout, MaxPooling1D, Flatten, Dense(30, relu), Dense(3, softmax)\n\n\ub9c8\uc9c0\ub9c9 epoch \uc131\ub2a5: loss: 0.2668 - acc: 0.9341 - val_loss: 0.4545 - val_acc: 0.9173\n\n* Hyper Parmaeter \ud29c\ub2dd:\nConv1D(filter, kernel_size, l2\uaddc\uc81c), Conv1D(filter, kernel_size, l2\uaddc\uc81c), Dropout \uc815\ub3c4, MaxPooling1D\uc758 pool_size, Flatten, Dense(\uc815\ub3c4), Dese\n\n\uc704\uc640 \ub9ce\uc774 \ube44\uc2b7\ud558\ubbc0\ub85c \uc0dd\ub7b5.\n\n\ud559\uc2b5 \uacb0\uacfc\ub85c \ucd5c\uc801\uc758 parameter\ub97c \ucc3e\uc740 \ud6c4, model_hyperas\ud568\uc218 \ubaa8\ub378 parameter\uc5d0 \ub123\uc740 \ud6c4 \ud559\uc2b5.\n\n\ud559\uc2b5 \uacb0\uacfc: Train_accuracy 1.0, test_accuracy 0.9704397981254506\n\n* Model 59\uc640 99\ub294 0.99 accuracy\ub97c \ubcf4\uc5ec\uc11c \ub450 \ubaa8\ub378 \uc790\uc138\ud788 \ubcf4\uae30.\n\nM59\uc758 nb_epoch=70\uc73c\ub85c \ud55c\ud6c4 \ud559\uc2b5 ->  19epoch\uc77c \ub54c \uc88b\uc740 score. ( loss: 0.1709 - acc: 0.9744 - val_loss: 0.2684 - val_acc: 0.9863) -> \ub354 \ub098\uc740 Dynamic activity\ubd84\ub958 \ubaa8\ub378 \uc644\uc131 (..99\ub294 \uc0dd\ub7b5\ud55c \ub4ef\ud558\ub2e4.)","85206d79":"from sklearn.ensemble import GradientBoostingClassifier\nparam_grid = {'max_depth': np.arange(5,8,1), \\\n             'n_estimators':np.arange(130,170,10)}\ngbdt = GradientBoostingClassifier()\ngbdt_grid = GridSearchCV(gbdt, param_grid=param_grid, n_jobs=8)\ngbdt_grid_results = perform_model(gbdt_grid, X_train, y_train, X_test, y_test, class_labels=labels)\nprint_grid_search_attributes(gbdt_grid_results['model'])","0c86747a":"Decision Tree\uac00 overfitting\ub420 \ub192\uc740 \uac00\ub2a5\uc131\uc744 \uac00\uc9c0\uace0 \uc788\uc5b4\uc11c,\nRandom Forest\ub97c \ud1b5\ud574 \ub354 \uc77c\ubc18\ud654\ub41c \ubaa8\ub378 \ub9cc\ub4e6.\n\nRandom forest: Ensemble \uba38\uc2e0\ub7ec\ub2dd \ubaa8\ub378. \uc8fc\uc5b4\uc9c4 train \ub370\uc774\ud130\uc5d0\uc11c n\uac1c \ub370\uc774\ud130 \uc0d8\ud50c \uc120\ud0dd, d\uac1c\uc758 feature \uc120\ud0dd -> \uc758\uc0ac\uacb0\uc815 \ud2b8\ub9ac \uc0dd\uc131 (\uc774 \ubaa8\ub378\uc5d0\uc11c max_features\ub294 auto)\n\nn_estimators: \uc758\uc0ac\uacb0\uc815\ud2b8\ub9ac \uac1c\uc218","de147c44":"## 3. Kernel SVM with GridSearch","e35d7066":">\uc704 \uc815\uc758 \ud6c4 \uc2e4\ud589\ucf54\ub4dc\n\nX_train, Y_train, X_val, Y_val = data()\ntrials = Trials()\n\n* optim.minimize()\uc758 Trials()\uac00 \uc2e4\ud589\ub418\uc5b4 data()\uc640 model()\uc774 \ubc18\ubcf5\uc801\uc73c\ub85c \uc218\ud589\n* Tree of Parzen Estimators\ub77c\ub294 \uc54c\uace0\ub9ac\uc998\uc73c\ub85c optimize\ud55c\ub2e4\ub294\ub370, \uc5b4\ub824\uc6cc\uc11c \uc0dd\ub7b5.\n* 15\ubc88 \uc2dc\ub3c4\n* notebook_name: If running from an ipython notebook, provide filename (not path)\n\nbest_run, best_model, space = optim.minimize(model=model,\n                                      data=data,\n                                      algo=tpe.suggest, \n                                      max_evals=15, \n                                      trials=trials,notebook_name = 'Human Activity Detection',\n                                     return_space = True)\n                                                                    ","76183250":"Linear Support Vector Classification: \uc624\ucc28\ub97c \ucd5c\uc18c\ud654\ud558\ub294 \ubc29\ud5a5\uc73c\ub85c \ud5a5\ud558\ub294 Logistic\uc740 \ub54c\ub860 \ud569\ub9ac\uc801\uc73c\ub85c \ubd84\ub958\ud558\uc9c0 \ubabb\ud560 \uc218 \uc788\uc5b4\uc11c Margin \uac1c\ub150 \uc801\uc6a9(Margin\uc774 \ucd5c\ub300\ud654\ud558\ub294 \ubc29\ud5a5\uc73c\ub85c)","0e88d412":"\ubaa8\ub378\uc5d0\uc11c overfitting\uc774 \uad00\uce21\ub41c\ub2e4. Static\ud55c \ud65c\ub3d9 \ub355\ubd84\uc5d0 \uc88b\uc740 \uacb0\uacfc\uac00 \ub098\uc628\ub4ef \ud558\uc5ec, \uc544\ub798\uc758 \ubaa8\ub378\uc740 \ub2e4\ub978 \ubc29\ubc95 \uc0ac\uc6a9.","27002c2f":"Keras(\ud30c\uc774\uc36c \ub525\ub7ec\ub2dd \ub77c\uc774\ube0c\ub7ec\ub9ac \uc911 \ud558\ub098)\ub97c \uc774\uc6a9\ud574 \ub525\ub7ec\ub2dd \ubaa8\ub378 \ub9cc\ub4e4\uae30.\n\nkeras.layers\uc5d0\uc11c LSTM import\ud558\uc5ec \uc0ac\uc6a9.\nepochs = 30, batch_size = 16, n_hidden(units) = 32\n* Base Model\n\n  layer 3\uac1c(lstm_1(32units), dropout(0.5), dense(activation='sigmoid'))\n  \n  Compile: loss='categorical_crossentropy', optimizer='rmsprop', metrics='accuracy'\n  \n  Model fitting\ud560\ub54c validation set\uc73c\ub85c test data\uc0ac\uc6a9.\n  \n  \ub9c8\uc9c0\ub9c9 epoch\uc77c\ub54c\uc758 \uc131\ub2a5: loss: 0.1598 - acc: 0.9450 - val_loss: 0.4559 - val_acc: 0.9013\n  \n\n* Multi layer LSTM\n\n    layer 5\uac1c(lstm_5(32units), dropout(0.5), lstm_6(28units), dropout(0.6), dense(sigmoid)\n    \n    Compile, Model fitting\uc740 \uc704\uc640 \uac19\ub2e4.\n    \n    \ub9c8\uc9c0\ub9c9 epoch\uc77c\ub54c\uc758 \uc131\ub2a5: loss: 0.1730 - acc: 0.9431 - val_loss: 0.4763 - val_acc: 0.9084\n\n--> 2\uac1c LSTM layer(Multi layer LSTM)\uc758 \uacb0\uacfc\uac00 layer 1\uac1c(Base Model)\uacfc \ube44\uc2b7\ud558\ub2e4.\n\n\n* L2\uaddc\uc81c \uc801\uc6a9\ud55c LSTM\n\n  layer 5\uac1c(lstm_7(32units, 0.003\uaddc\uc81c \uc801\uc6a9), dropout(0.5), lstm_8(28units), dropout(0.6), dense(sigmoid)\n  \n  optimizer='adam'\uc744 \uc81c\uc678\ud558\uace0 Compile, Model fitting\uc740 \uc704\uc640 \ub3d9\uc77c","f51d7fbd":"## 1. Featuring Engineering from Domain Knowledge\nStatic and Dynamic Activities\n\nIn static activities (sit, stand, lie down) motion information will not be very useful.\nIn the dynamic activities (Walking, WalkingUpstairs,WalkingDownstairs) motion info will be significant.\n\n## 2. Stationary and Moving activities are completely different","094bf37a":"## 3. Check for data imbalance","cc1d7881":"## 6. Gradient Boosted Decision Trees With GridSearch","943570ee":"## 7. Comparing all models","4480e21d":"### Hyperparameter Tuning Using Hyperas:\n\n\ucc38\uace0: https:\/\/gdlovehush.tistory.com\/36\n\n* two\uc77c\ub54c:\nLSTM\uc5d0\uc11c units(28,32,38), l2\uaddc\uc81c\uc758 C(0, 0.0002)\nDropout\uc758 \uc815\ub3c4(0.35, 0.65)\nLSTM\uc5d0\uc11c units(26, 32, 36), l2\uaddc\uc81c\uc758 C(0, 0.001)\nDropout\uc758 \uc815\ub3c4(0.5, 0.7)\nDense(sigmoid)\n\n* one\uc77c\ub54c:\nLSTM\uc5d0\uc11c units(28,32,36), l2\uaddc\uc81c\uc758 C(0, 0.001)\nDropout\uc758 \uc815\ub3c4(0.35, 0.55)\nDense(sigmoid)\n\noptimizer = adam(lr=uniform(0.009, 0.025)) or rmsprop(\ub3d9\uc77c)\n\nloss = categorical_crossentropy, metrics=accuracy\nvalidation set\uc73c\ub85c test data\uc0ac\uc6a9","a6ef19e1":"## 2. Checking for NaN\/null values","e258d02d":"## 5. Random Forest Classifier with GridSearch","65cee5c7":"### Hyper Parameter Tuning Using Hyperas\n\n* Conv1D(filters=[28,32,42], kernel_size=[3,5,7], relu, he_uniform, l2=uniform[0, 2.5])\n\n* Conv1D(filters[16,24,32], kernel_size=[3,5,7], relu, he_uniform, l2=uniform[0, 2.5]\n\n* Dropout(uniform[0.45, 0.7])\n\n* MaxPooling1D(pool_size=[2,3])\n\n* Flatten()\n\n* Dense([32,64], relu)\n\n* Dense(softmax)\n\noptimizer = adam or rmsprop (lr=uniform[0.00065, 0.004])\n\nmodel.fit(batch_size[16,32,64], nb_epoch[25,30,35])\n\n* best model\uc77c\ub54c:\nTrain_accuracy 0.963139281828074, test_accuracy 0.9229725144214456","d21c52fb":"## Exploratory Data Analysis","4f245fe6":"## 4. Changing feature names","7d6aa4c5":"## 5. Save this dataframe in a csv files","2d86ffb0":"# Data Cleaning","d158d9cd":"\n### Observations:\n\nIf tAccMean is < -0.8 then the Activities are either Standing or Sitting or Laying.\n\nIf tAccMean is > -0.6 then the Activities are either Walking or WalkingDownstairs or WalkingUpstairs.\n\nIf tAccMean > 0.0 then the Activity is WalkingDownstairs.\n\nWe can classify 75% the Acitivity labels with some errors.","1995e8a7":"# LSTM","eabc6934":"best_run\uc73c\ub85c \uac01 parameter\uc758 \ucd5c\uc801\uac12 \uc54c\uc544\ubcf4\uace0, total_trials['M14']\ub85c \ucd5c\uc801 \ubaa8\ub378 M14\uc758 parameter \ud655\uc778\n\nTrain_accuracy 0.94560663764961915\n\nvalidation accuracy 0.9199185612487275","8fb7b069":"\ucc38\uace0 \uc790\ub8cc: https:\/\/tykimos.github.io\/2017\/04\/09\/RNN_Getting_Started\/","c0b8d41b":"### Model for classifying data into Static and Dynamic activities\n\nCon1D, Conv1D, Dropout, MaxPooling1D, Flatten, Dense(50, relu), Dense(2, softmax)\n\n(\ub098\uba38\uc9c0\ub294 \uc704\uc640 \ube44\uc2b7\ud558\ubbc0\ub85c \uc0dd\ub7b5)\n\n\nTrain_accuracy 1.0 test_accuracy 0.9989820156090939\n\uc774 \ubaa8\ub378\uc740 \uc8fc\uc5b4\uc9c4 data\ub97c dynamic\/static\uc73c\ub85c \uc798 \ubd84\ub958\ud55c\ub2e4.\n\n100\ubc88 \ud559\uc2b5\n\n### Classificaton of Static activities\nStandardScaler\uc801\uc6a9, https:\/\/github.com\/Snehitha17\/human-activity-recognition\/blob\/master\/Scale_static.p \uc5d0 \uc788\ub294 Scale_static.p \ub2e4\uc6b4\ubc1b\uc544 scaling \ucd94\uac00\n\n\n* Baseline Model: \n  Conv1D,\n  Conv1D,\n  Dropout,\n  MaxPooling1D,\n  Flatten,\n  Dense(30, relu),\n  Dense(3, softmax)\n  \n  \ub9c8\uc9c0\ub9c9 epoch \uc131\ub2a5: loss: 0.1927 - acc: 0.9592 - val_loss: 0.2545 - val_acc: 0.9096\n  \n* Hyper Parmaeter \ud29c\ub2dd\n   Conv1D(filter, kernel_size, l2\uaddc\uc81c),\n   Conv1D(filter, kernel_size, l2\uaddc\uc81c),\n   Dropout \uc815\ub3c4,\n   MaxPooling1D\uc758 pool_size,\n   Flatten,\n   Dense(\uc815\ub3c4),\n   Dese\n   \n\uc704\uc640 \ub9ce\uc774 \ube44\uc2b7\ud558\ubbc0\ub85c \uc0dd\ub7b5.\n\n\ud559\uc2b5 \uacb0\uacfc\ub85c \ucd5c\uc801\uc758 parameter\ub97c \ucc3e\uc740 \ud6c4, keras_fmin_fnct\ud568\uc218\uc5d0\uc11c \ubaa8\ub378\uc758 parameter\uc5d0 \ub123\uc740 \ud6c4 \ud559\uc2b5.\n\n\ud559\uc2b5 \uacb0\uacfc: Train_accuracy 0.9628718957462503, test_accuracy 0.9391025641025641\n\n* 23\ubc88\uc9f8 \ubaa8\ub378\uc740 best model\uc774 \uc544\ub2c8\uc9c0\ub9cc, \uaf64 \uc88b\uc740 \uacb0\uacfc\ub77c\uc11c \uc774 \ubaa8\ub378\ub3c4 \ud559\uc2b5\ud574\ubcf4\uae30. \ub2e4\ub9cc,\n-> Train\/Validation loss \uadf8\ub798\ud504\ub97c \uadf8\ub824\uc11c overfitting\ub418\ub294 \uc810 \ud655\uc778.\noverfitting\uc774 \ub35c \ub418\ub294 \uc88b\uc740 accuracy\uc9c0\uc810(57-59\ubc88\uc9f8 nb_epoch)\uc77c\ub54c\ub85c \ubaa8\ub378 \ud559\uc2b5\n\n\ud559\uc2b5 \uacb0\uacfc: Train_accuracy 0.9741824440619621, test_accuracy 0.9544871794871795\n--> \ub354 \ub098\uc740 Static activity\ubd84\ub958 \ubaa8\ub378 \uc644\uc131","92d89f40":"## 4. Decision Trees with GridSearchCV","56bdcaf4":"# Let's model with our data","90ee1f38":"## 1. Check for Duplicates","ada71979":"\uc8fc\uc694 \ucf54\ub4dc\ub294 \ud0c0\uc774\ud551\ud588\uc9c0\ub9cc, \ub098\uba38\uc9c0\ub294 \ubcf5\ubd99\ud568.\n\n\uadf8\ub7f0\ub370 \uc774\ub807\uac8c \ud558\ub294 \uac83\uc774 \uc758\ubbf8\uac00 \uc788\ub098..\uc2f6\uae30\ub3c4\ud558\uace0 \uc2dc\uac04\uc774 \uc624\ub798\uac78\ub9b4\uac83 \uac19\uc544.. \uc5ec\uae30\uc11c\ubd80\ud130\ub294 Approach \uc124\uba85\ud558\ub294 \ubc29\uc2dd\uc73c\ub85c \ubc14\uafbc\ub2e4.","dc2e6447":"### Generic function to run any model specified","1ff7e39b":"## 2. Linear SVC with GridSearch"}}