{"cell_type":{"0ae4afe1":"code","69ad1d96":"code","faff92d5":"code","f16c40d1":"code","a6398f41":"code","1b7385eb":"code","ed7c646d":"code","bdbe4df4":"code","064459fc":"code","fb89d050":"code","008f2ba3":"code","9e363a6d":"code","dabd6dc8":"code","55ce1ac7":"code","91749df8":"code","0ea7f1c0":"code","c174f593":"code","605f7d6b":"code","b37898e3":"code","c95307c3":"code","de6404d0":"code","87c06b58":"code","20346df6":"code","7712ac26":"code","0afb0450":"code","4036b465":"code","42644122":"code","896fba2a":"code","7469f264":"code","b2105713":"code","b303194a":"code","82da96db":"code","8cf09ea5":"code","2d5d6156":"code","6a05279b":"code","8bb6c867":"code","e352b87e":"code","6d1ebd55":"code","c5d84566":"code","b776a3fd":"code","23997e38":"code","cf12130e":"code","9d26f1b1":"code","5a15a085":"code","b409a823":"code","54bf1398":"code","544f56cc":"code","995ae18f":"code","123a2063":"code","500c4508":"code","9f617414":"code","b9944f31":"code","da067fec":"code","726eee8b":"code","e5ec32e2":"code","b2d1b6a4":"markdown","84f1141b":"markdown","e962b7c6":"markdown","6cfd7a55":"markdown","6479ff6c":"markdown","7f7cae24":"markdown","99db4851":"markdown","17d87d46":"markdown","c70df06c":"markdown","93ac7255":"markdown","be3edc19":"markdown","0de7ad55":"markdown","75fc0fa8":"markdown","2b4c959c":"markdown","affc4f4a":"markdown","c2c32701":"markdown","23271f8e":"markdown","49bf7f58":"markdown","633d3566":"markdown","80164d0b":"markdown","b5121c09":"markdown"},"source":{"0ae4afe1":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nfrom sklearn.metrics import classification_report,roc_auc_score,roc_curve\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom mpl_toolkits.mplot3d import Axes3D \nfrom sklearn.linear_model import LogisticRegression\nimport scipy\nfrom scipy.spatial.distance import pdist,cdist\nfrom scipy.cluster.hierarchy import dendrogram,linkage\nfrom scipy.cluster.hierarchy import fcluster\nfrom scipy.cluster.hierarchy import cophenet\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import tree\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import tree\nfrom IPython.display import Image as PImage\nfrom subprocess import check_call\nfrom PIL import Image, ImageDraw, ImageFont\nimport re\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, learning_curve, train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (accuracy_score, log_loss, classification_report,f1_score,confusion_matrix)\nimport xgboost\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,Imputer,LabelEncoder,OneHotEncoder\nimport xgboost as xgb\nfrom sklearn.metrics import roc_auc_score,roc_curve\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nimport scipy\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier","69ad1d96":"df = pd.read_csv('\/kaggle\/input\/onlinenewspop\/OnlineNewsPopularity.csv')","faff92d5":"df.head()","f16c40d1":"df.isnull().sum()","a6398f41":"df.info()","1b7385eb":"df.describe()","ed7c646d":"#droping url\ndf.drop(columns=['url'],inplace=True)","bdbe4df4":"#correlation plot\nplt.figure(figsize=(40,40))\nsns.heatmap(data=df.corr(),annot=True,cmap='BuPu')","064459fc":"df.columns","fb89d050":"#box plot to check outliers\nfor i in df.columns:\n    sns.boxplot(df[i])\n    plt.show()","008f2ba3":"#removing outliers\nQ1 = df.quantile(q=0.25) \n\nQ3 = df.quantile(q=0.75)\n\nIQR = Q3-Q1\nprint('IQR for each column:- ')\nprint(IQR)","9e363a6d":"\nsorted_shares = df.sort_values(' shares') \n\nmedian = sorted_shares[' shares'].median() \n\nq1 = sorted_shares[' shares'].quantile(q=0.25) \n\nq3 = sorted_shares[' shares'].quantile(q=0.75) \n\niqr = q3-q1","dabd6dc8":"Inner_bound1 = q1-(iqr*1.5) \nprint(f'Inner Boundary 1 = {Inner_bound1}')\nInner_bound2 = q3+(iqr*1.5)  \nprint(f'Inner Boundary 2 = {Inner_bound2}')\nOuter_bound1 = q1-(iqr*3)    \nprint(f'Outer Boundary 1 = {Outer_bound1}')\nOuter_bound2 = q3+(iqr*3)   \nprint(f'Outer Boundary 2 = {Outer_bound2}')","55ce1ac7":"Df = df[df[' shares']<=Outer_bound2]","91749df8":"print(f'Data before Removing Outliers = {df.shape}')\nprint(f'Data after Removing Outliers = {Df.shape}')\nprint(f'Number of Outliers = {df.shape[0] - Df.shape[0]}')","0ea7f1c0":"Df.hist(figsize=(30,30))\nplt.show()","c174f593":"#EDA\na,b = Df[' shares'].mean(),Df[' shares'].median()\nprint(f'Mean article shares = {a}')\nprint(f'Median article share = {b}')","605f7d6b":"Wd = Df.columns.values[30:37]\nWd","b37898e3":"Unpop=Df[Df[' shares']<a]\nPop=Df[Df[' shares']>=a]\nUnpop_day = Unpop[Wd].sum().values\nPop_day = Pop[Wd].sum().values\n\nfig = plt.figure(figsize = (13,5))\nplt.title(\"Count of popular\/unpopular news over different day of week (Mean)\", fontsize = 16)\nplt.bar(np.arange(len(Wd)), Pop_day, width = 0.3, align=\"center\", color = 'r', \\\n          label = \"popular\")\nplt.bar(np.arange(len(Wd)) - 0.3, Unpop_day, width = 0.3, align = \"center\", color = 'b', \\\n          label = \"unpopular\")\nplt.xticks(np.arange(len(Wd)), Wd)\nplt.ylabel(\"Count\", fontsize = 12)\nplt.xlabel(\"Days of week\", fontsize = 12)\n    \nplt.legend(loc = 'upper right')\nplt.tight_layout()\nplt.show()","c95307c3":"Unpop2=Df[Df[' shares']<b]\nPop2=Df[Df[' shares']>=b]\nUnpop_day2 = Unpop2[Wd].sum().values\nPop_day2 = Pop2[Wd].sum().values\nfig = plt.figure(figsize = (13,5))\nplt.title(\"Count of popular\/unpopular news over different day of week (Median)\", fontsize = 16)\nplt.bar(np.arange(len(Wd)), Pop_day2, width = 0.3, align=\"center\", color = 'r', \\\n          label = \"popular\")\nplt.bar(np.arange(len(Wd)) - 0.3, Unpop_day2, width = 0.3, align = \"center\", color = 'b', \\\n          label = \"unpopular\")\nplt.xticks(np.arange(len(Wd)), Wd)\nplt.ylabel(\"Count\", fontsize = 12)\nplt.xlabel(\"Days of week\", fontsize = 12)\n    \nplt.legend(loc = 'upper right')\nplt.tight_layout()\nplt.show()","de6404d0":"Dc = Df.columns.values[12:18]","87c06b58":"Unpop3=Df[Df[' shares']<a]\nPop3=Df[Df[' shares']>=a]\nUnpop_day3 = Unpop3[Dc].sum().values\nPop_day3 = Pop3[Dc].sum().values\nfig = plt.figure(figsize = (13,5))\nplt.title(\"Count of popular\/unpopular news over different data channel (Mean)\", fontsize = 16)\nplt.bar(np.arange(len(Dc)), Pop_day3, width = 0.3, align=\"center\", color = 'r', \\\n          label = \"popular\")\nplt.bar(np.arange(len(Dc)) - 0.3, Unpop_day3, width = 0.3, align = \"center\", color = 'b', \\\n          label = \"unpopular\")\nplt.xticks(np.arange(len(Dc)), Dc)\nplt.ylabel(\"Count\", fontsize = 12)\nplt.xlabel(\"Days of week\", fontsize = 12)\n    \nplt.legend(loc = 'upper right')\nplt.tight_layout()\nplt.show()","20346df6":"Unpop4=Df[Df[' shares']<b]\nPop4=Df[Df[' shares']>=b]\nUnpop_day4 = Unpop4[Dc].sum().values\nPop_day4 = Pop4[Dc].sum().values\nfig = plt.figure(figsize = (13,5))\nplt.title(\"Count of popular\/unpopular news over different data channel (Median)\", fontsize = 16)\nplt.bar(np.arange(len(Dc)), Pop_day4, width = 0.3, align=\"center\", color = 'r', \\\n          label = \"popular\")\nplt.bar(np.arange(len(Dc)) - 0.3, Unpop_day4, width = 0.3, align = \"center\", color = 'b', \\\n          label = \"unpopular\")\nplt.xticks(np.arange(len(Dc)), Dc)\nplt.ylabel(\"Count\", fontsize = 12)\nplt.xlabel(\"Days of week\", fontsize = 12)\n    \nplt.legend(loc = 'upper right')\nplt.tight_layout()\nplt.show()","7712ac26":"Df.head()","0afb0450":"mean = Df[' shares'].mean()","4036b465":"#Converting output columns to 0 and 1\nDf[' shares'] = Df[' shares'].apply(lambda x: 0 if x <mean  else 1)","42644122":"Df[' shares'].value_counts()","896fba2a":"#Scaling and Doing SMOTE \nX = Df.drop(' shares',axis=1)\ny = Df[' shares']\n\nscaler=StandardScaler()\nX=scaler.fit_transform(X)\nfrom imblearn.over_sampling import SMOTE\nSMOTE().fit_resample(X, y)\nX,y = SMOTE().fit_resample(X, y)","7469f264":"print(y)","b2105713":"def calculateScore(confMat):\n    TP = confMat[0][0]\n    TN = confMat[1][1]\n    FP = confMat[0][1]\n    FN = confMat[1][0]\n    Sen.append(TP \/ (TP + FN))\n    Spe.append(TN \/ (FP + TN))\n    FPR.append(FP \/ (FP + TN))\n    FNR.append(FN \/ (FN + TP))","b303194a":"train, test, target_train, target_val = train_test_split(X, \n                                                         y, \n                                                         train_size= 0.80,\n                                                         random_state=0);","82da96db":"#Using multiple classifiers\nModel = []\nAccuracy= []\nF1Score = []\nSen = []\nSpe = []\nFPR = []\nFNR = []","8cf09ea5":"LR = LogisticRegression(multi_class='auto')\nLR.fit(train,target_train)\nlr_pred = LR.predict(test)\nModel.append('Logistic Regression')\nAccuracy.append(accuracy_score(target_val,lr_pred))\nF1Score.append(f1_score(target_val,lr_pred,average=None))","2d5d6156":"data = confusion_matrix(target_val,lr_pred)\ncalculateScore(data)\ndf_cm = pd.DataFrame(data, columns=np.unique(target_val), index = np.unique(target_val))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nplt.figure(figsize = (10,7))\nsns.set(font_scale=1.4)\nsns.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})","6a05279b":"seed = 0\nparams = {\n    'n_estimators':range(10,100,10),\n    'criterion':['gini','entropy'],\n}\nrf = RandomForestClassifier()\nrs = RandomizedSearchCV(rf, param_distributions=params, scoring='accuracy', n_jobs=-1, cv=5, random_state=42)\nrs.fit(X,y)","8bb6c867":"rs.best_params_","e352b87e":"rf = RandomForestClassifier(**rs.best_params_)\nrf.fit(train, target_train)\nrf_pred = rf.predict(test)","6d1ebd55":"features = Df.columns\nimportance = rf.feature_importances_\nindices = np.argsort(importance)\nplt.figure(1,figsize=(10,20))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importance[indices], color='lightblue', align='center')\nplt.yticks(range(len(indices)), features[indices])\nplt.xlabel('Relative Importance')","c5d84566":"Model.append('Random Forrest')\nAccuracy.append(accuracy_score(target_val,rf_pred))\nF1Score.append(f1_score(target_val,rf_pred,average=None))","b776a3fd":"data = confusion_matrix(target_val,rf_pred)\ncalculateScore(data)\ndf_cm = pd.DataFrame(data, columns=np.unique(target_val), index = np.unique(target_val))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nplt.figure(figsize = (10,7))\nsns.set(font_scale=1.4)\nsns.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})","23997e38":"params = {\n    \n    'criterion':['gini','entropy'],\n    'splitter':['best','random'],\n    'max_depth':range(1,10,1),\n    'max_leaf_nodes':range(2,10,1),\n}\ndt = DecisionTreeClassifier()\nrs = RandomizedSearchCV(dt, param_distributions=params, scoring='accuracy', n_jobs=-1, cv=5, random_state=42)\nrs.fit(X,y)","cf12130e":"rs.best_params_","9d26f1b1":"dt = DecisionTreeClassifier()\ndt.fit(train, target_train)\ndt_pred = dt.predict(test)\nfeatures = Df.columns\nimportance = dt.feature_importances_\nindices = np.argsort(importance)\nplt.figure(1,figsize=(10,20))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importance[indices], color='lightblue', align='center')\nplt.yticks(range(len(indices)), features[indices])\nplt.xlabel('Relative Importance')","5a15a085":"Model.append('Decision Tree')\nAccuracy.append(accuracy_score(target_val,dt_pred))\nF1Score.append(f1_score(target_val,dt_pred,average=None))","b409a823":"data = confusion_matrix(target_val,dt_pred)\ncalculateScore(data)\ndf_cm = pd.DataFrame(data, columns=np.unique(target_val), index = np.unique(target_val))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nplt.figure(figsize = (10,7))\nsns.set(font_scale=1.4)\nsns.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})","54bf1398":"gb_Boost = GradientBoostingClassifier(n_estimators=100,learning_rate=0.01)\ngb_Boost.fit(train, target_train)\ny_pred = rf.predict(test)","544f56cc":"Model.append('Gradient Boosting')\nAccuracy.append(accuracy_score(target_val,y_pred))\nF1Score.append(f1_score(target_val,dt_pred,average=None))","995ae18f":"data = confusion_matrix(target_val,y_pred)\ncalculateScore(data)\ndf_cm = pd.DataFrame(data, columns=np.unique(target_val), index = np.unique(target_val))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nplt.figure(figsize = (10,7))\nsns.set(font_scale=1.4)\nsns.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})","123a2063":"naiveClassifier=GaussianNB()\nnaiveClassifier.fit(train, target_train)\nnaiveClassifier_pred = naiveClassifier.predict(test)","500c4508":"Model.append('Naive')\nAccuracy.append(accuracy_score(target_val,naiveClassifier_pred))\nF1Score.append(f1_score(target_val,naiveClassifier_pred,average=None))","9f617414":"data = confusion_matrix(target_val,naiveClassifier_pred)\ncalculateScore(data)\ndf_cm = pd.DataFrame(data, columns=np.unique(target_val), index = np.unique(target_val))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nplt.figure(figsize = (10,7))\nsns.set(font_scale=1.4)\nsns.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})","b9944f31":"# knn = KNeighborsClassifier(n_neighbors=8)\n# knn.fit(train, target_train)\n# knn_pred = knn.predict(test)","da067fec":"# Model.append('KNN')\n# Accuracy.append(accuracy_score(target_val,knn_pred))\n# F1Score.append(f1_score(target_val,knn_pred,average=None))","726eee8b":"# data = confusion_matrix(target_val,knn_pred)\n# calculateScore(data)\n# df_cm = pd.DataFrame(data, columns=np.unique(target_val), index = np.unique(target_val))\n# df_cm.index.name = 'Actual'\n# df_cm.columns.name = 'Predicted'\n# plt.figure(figsize = (10,7))\n# sns.set(font_scale=1.4)\n# sns.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})","e5ec32e2":"result = pd.DataFrame({'Model':Model,'Accuracy':Accuracy,'F1Score':F1Score,'Sensitivity':Sen,'Specificity':Spe,'FPR':FPR,'FNR':FNR})\nresult","b2d1b6a4":"**KNN**","84f1141b":"**Gradient Boosting**","e962b7c6":"**Too many outliers.So removing them**","6cfd7a55":"**Naive**","6479ff6c":"**Logistic Regression**","7f7cae24":"From correlation plot we can see most of the features are important","99db4851":"We can clearly see almost all features are important","17d87d46":"**Hist plot**","c70df06c":"**KNN model is taking long time for differnt values of k but optimal value got was 8. We can see that Random forrest ensemble technique gave maximum accuracy and also F1 score is higher for the same. Also we can see gradient boosting gave almost same accuracy but since F1 score of random forrest is more. Hence ensemble technique is most accurate.**","93ac7255":"**Dropping URL as its not making any sense**","be3edc19":"**Box plots**","0de7ad55":"**Scaling and SMOTE**","75fc0fa8":"**Comparing models**","2b4c959c":"We can clearly see data is not properly distributed so we need to do scaling and smote ","affc4f4a":"From confusion matrix we can see there are false positive as well","c2c32701":"Again we can see almost all features have importance. few features like n_stop_words has very less importance when compared to others","23271f8e":"Scaling and SMOTE is needed because data is not properly distributed","49bf7f58":"**Plot feature importance**","633d3566":"**Splitting train and test**","80164d0b":"**Decision Tree****","b5121c09":"**Random Forrest**"}}