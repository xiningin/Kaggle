{"cell_type":{"f8c34fd4":"code","d491dd6d":"code","07f5e8cc":"code","2c3daa18":"code","35466e24":"code","79f8fca6":"code","80c1037f":"code","851aecf2":"code","9d4d32f0":"code","8b1fd893":"code","31dec44c":"code","9f04688e":"code","94d198c2":"code","ecb5d489":"code","574dacc9":"code","2d5bee59":"code","1faeb49d":"code","14559e11":"code","7249ee16":"markdown","0bcbe75c":"markdown","23dd635a":"markdown","eb8ad5a5":"markdown"},"source":{"f8c34fd4":"import scipy as sp\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom tqdm import tqdm_notebook, tqdm\n\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nimport lightgbm as lgb\n\n# so that we can print multiple dataframe in the same cell\nfrom IPython.display import display, HTML\ndef displayer(df, num_rows=2): display(HTML(df.head(num_rows).to_html()))\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 4000)\ntqdm.pandas()  # for progress_apply\n\n!ls ..\/input\/data-science-bowl-2019\/","d491dd6d":"%%time\n# loading dataframes\ntrain_actions = pd.read_csv(\"..\/input\/data-science-bowl-2019\/train.csv\")\nprint(train_actions.shape)\n\ntest_actions = pd.read_csv(\"..\/input\/data-science-bowl-2019\/test.csv\")\nprint(test_actions.shape)\n\ntrain_df = pd.read_csv(\"..\/input\/data-science-bowl-2019\/train_labels.csv\")\nprint(train_df.shape)\ntrain_labels = train_df[\"accuracy_group\"]\n\n# the actual test_df will be sourced from test_actions\ntest_df_reference = pd.read_csv(\"..\/input\/data-science-bowl-2019\/sample_submission.csv\")\nprint(test_df_reference.shape)\n\nspecs = pd.read_csv(\"..\/input\/data-science-bowl-2019\/specs.csv\")\nprint(specs.shape)\n\ndisplayer(train_actions)\ndisplayer(test_actions)\ndisplayer(train_df)\ndisplayer(test_df_reference)\ndisplayer(specs)","07f5e8cc":"def extract_time_features(df):\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n#     df['hour'] = df['timestamp'].dt.hour\n#     df['date'] = df['timestamp'].dt.date\n#     df['month'] = df['timestamp'].dt.month\n#     df['year'] = df['timestamp'].dt.year\n    df['dayofweek'] = df['timestamp'].dt.dayofweek\n    return df\n\ntrain_actions = extract_time_features(train_actions)\ntest_actions = extract_time_features(test_actions)","2c3daa18":"%%time\n# reassign game_session_id for indexing\ngame_session_lst = list(train_actions[\"game_session\"]) + list(test_actions[\"game_session\"]) + list(train_df[\"game_session\"])\ngame_session_dict = {}\ncounter = 10**8\nfor game_session in game_session_lst:\n    if not game_session in game_session_dict:\n        counter += 1\n        game_session_dict[game_session] = counter\n        \ndef reassign(df):\n    arr = []\n    for a,b in zip(df[\"installation_id\"], df[\"game_session\"]):\n        arr.append(a + str(game_session_dict[b]) + \"x\")\n    return arr\n\ntrain_actions[\"game_session\"] = reassign(train_actions)\ntest_actions[\"game_session\"] = reassign(test_actions)\ntrain_df[\"game_session\"] = reassign(train_df)","35466e24":"%%time\n# reindex dataframes and lexsort for segmentations later\ntrain_actions['idx'] = np.arange(train_actions.shape[0])\ntest_actions['idx'] = np.arange(test_actions.shape[0])\ntrain_df['idx'] = np.arange(train_df.shape[0])\n\ntrain_actions.set_index(['installation_id', 'game_session', 'idx'], inplace=True)\ntest_actions.set_index(['installation_id', 'game_session', 'idx'], inplace=True)\ntrain_df.set_index(['installation_id', 'game_session', 'idx'], inplace=True)\n\ntrain_actions = train_actions.sort_index()\ntest_actions = test_actions.sort_index()\ntrain_df = train_df.sort_index()\n\nprint(train_actions.index.is_lexsorted(),\n      test_actions.index.is_lexsorted(),\n      train_df.index.is_lexsorted())\n\ndisplayer(train_actions, 7)\ndisplayer(test_actions, 7)\ndisplayer(train_df, 7)","79f8fca6":"import json\ndef trim_event_data(json_string):\n    d = json.loads(json_string)\n    if \"event_data\" in d: del d[\"event_data\"]\n    if \"event_count\" in d: del d[\"event_count\"]\n    if \"event_code\" in d: del d[\"event_code\"]\n    if \"description\" in d: del d[\"description\"]\n    if \"game_time\" in d: del d[\"game_time\"]\n    json_string = json.dumps(d)\n    return json_string\ntrain_actions[\"event_data\"] = train_actions[\"event_data\"].progress_apply(trim_event_data)\ntest_actions[\"event_data\"] = test_actions[\"event_data\"].progress_apply(trim_event_data)\ndisplayer(train_actions, 7)\ndisplayer(test_actions, 7)","80c1037f":"# drop information currently not aggregated\ntrain_actions = train_actions.drop([\"event_data\", \"game_time\", \"type\", \"world\"], axis=1)\ntest_actions = test_actions.drop([\"event_data\", \"game_time\", \"type\", \"world\"], axis=1)","851aecf2":"# helper functions to aggregate player history\n# we count the number of event_codes and titles the user has been through\ndef aggregate_column_types(columns):\n    aggr = {}\n    for col in columns:\n        aggr[col] = list(set(list(train_actions[col]) +  list(test_actions[col])))\n    return aggr\naggr = aggregate_column_types([\"title\"])\n\ndef aggregate_df(aggr, df):\n    d = {}\n    for col, vals in aggr.items():\n        value_count = df[col].value_counts()\n        for val in vals:\n            d[str(col)+\"__\"+str(val)] = 0\n            if val in value_count:\n                d[str(col)+\"__\"+str(val)] = value_count[val]\n    d[\"length\"] = df.shape[0]\n    return d","9d4d32f0":"# aggregate information from training\ntrain_df_features_lst = []\ntrain_df_features_dfs = []\nfor index, row in tqdm_notebook(train_df.iterrows()):\n    train_df_features = train_actions.loc[([index[0]], slice(None,index[1][:-1]), slice(None)), :]\n    train_df_features_dfs.append(train_df_features)\n    train_df_features_lst.append(aggregate_df(aggr, train_df_features))","8b1fd893":"import pickle\nwith open('train_df_features_dfs.pkl', \"wb\") as f:\n    pickle.dump(len(train_df_features_dfs), f)\n    for train_df_feature in tqdm_notebook(train_df_features_dfs):\n        pickle.dump(train_df_feature.reset_index().set_index(['installation_id', 'game_session', 'idx']).copy(), f)\n        \n# loading code\n# train_df_features_dfs = []\n# with open('train_df_features_dfs.pkl', \"rb\") as f:\n#     for _ in tqdm_notebook(range(pickle.load(f))):\n#         train_df_features_dfs.append(pickle.load(f))","31dec44c":"# aggregate information from testing\ntest_df_features_lst = []\ntest_df_rows = []\nfor index in tqdm_notebook(list(set([i[0] for i in test_actions.index]))):\n    test_df_features = test_actions.loc[([index], slice(None), slice(None)), :]\n    test_df_features_lst.append(aggregate_df(aggr, test_df_features[:-1]))\n    test_df_rows.append(test_df_features.tail(1))\ntest_df = pd.concat(test_df_rows, axis=0)","9f04688e":"# map assessment titles to number\ntrain_df = train_df[[\"title\"]]\ntest_df = test_df[[\"title\"]]\ntitle_lst = list(set(list(train_df[\"title\"])+list(test_df[\"title\"])))\ntrain_df['title'] = train_df['title'].apply(lambda x: title_lst.index(x))\ntest_df['title'] = test_df['title'].apply(lambda x: title_lst.index(x))","94d198c2":"# include aggregated information \ntrain_df = pd.concat([train_df,pd.DataFrame(train_df_features_lst, index=train_df.index)], sort=False, axis=1)\ntest_df = pd.concat([test_df,pd.DataFrame(test_df_features_lst, index=test_df.index)], sort=False, axis=1)\n\ntest_df = test_df.sort_index(axis=0, level=\"idx\")\n\nassert list([i[0] for i in test_df.index]) == list(test_df_reference[\"installation_id\"])\n\ndisplayer(train_df)\ndisplayer(test_df)","ecb5d489":"# defining LightGBM dataset and categorical_features\ncategorical_features = [\"title\"]\ntrain_dataset = lgb.Dataset(train_df, label=train_labels,\n                            free_raw_data=False,\n                            categorical_feature=categorical_features)\ntest_dataset = lgb.Dataset(test_df,\n                           free_raw_data=False,\n                           categorical_feature=categorical_features)","574dacc9":"param = {'num_leaves': 127, \n         'objective': 'multiclass',\n         'num_class': 4}","2d5bee59":"# defined Kfold\nnum_rows = train_labels.shape[0]\nskf = KFold(n_splits=10, shuffle=True, random_state=42)\nfolds = [fold for fold in skf.split(np.arange(num_rows), train_labels)]\n\n# training for each fold\nbst_lst = []\nfor i, (tr_idx, tx_idx) in enumerate(folds):\n    print(\"fold number \", i)\n    bst = lgb.train(param,\n                    train_dataset.subset(tr_idx).construct(), \n                    num_boost_round=500,\n                    early_stopping_rounds=10,\n                    verbose_eval=100,\n                    categorical_feature=categorical_features,\n                    valid_sets=[train_dataset.subset(tr_idx).construct(),\n                                train_dataset.subset(tx_idx).construct()])\n    bst_lst.append(bst)","1faeb49d":"# make prediction with each fold\npreds = []\nfor bst in bst_lst:\n    pred = bst.predict(test_df) # not test_dataset\n    preds.append(pred)\npreds = np.array(preds)\nprint(preds.shape)\n\npreds_mean = np.mean(preds, axis=0)\nprint(preds_mean.shape)\n\nres = [np.argmax(ar) for ar in preds_mean]\nprint(Counter(res))\n\ntest_df_reference[\"accuracy_group\"] = res","14559e11":"# save and loading the submission file\ntest_df_reference.to_csv('submission.csv', index=None)\ndf_read = pd.read_csv('submission.csv')\ndf_read.head()","7249ee16":"# LightGBM training and predicting","0bcbe75c":"# Data aggregation","23dd635a":"Code quality and readability should also apply to data science notebooks.\n\nHere I hope to write a notebook that is understandable and modular, so that other Kagglers can use part or whole of this code.\n\nCredit:\n- I first understood the data with this\n  - https:\/\/www.kaggle.com\/mhviraf\/a-baseline-for-dsb-2019\n- An example of quality starter code, although this is not a notebook\n  - https:\/\/www.kaggle.com\/gpreda\/data-science-bowl-fast-compact-solution\n\nComments on this notebook:\n- This line is slow and takes 38 minutes to compute for the entire dataframe. Speedup is appreciated.\n```\ntrain_df_features = train_actions.loc[([index[0]], slice(None,index[1][:-1]), slice(None)), :]\n```\n- However, this step can be skipped subsequently because you may load the pickle produced. I do not save the test dataset because I suspect all the non-training dataset will be replaced once you sent the notebook for evaluation in the public leaderboard.\n- You can see the input and output dataframe just before the section - LightGBM training and predicting","eb8ad5a5":"# Input Processing"}}