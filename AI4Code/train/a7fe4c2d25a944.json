{"cell_type":{"17e5e11a":"code","bc972d00":"code","dde3864e":"code","00121692":"code","3ad8c41e":"code","62a58939":"code","532092c6":"code","0edd738f":"code","a550c57b":"code","b50b68f3":"code","faa1bbb8":"code","56795ffd":"code","65109468":"code","a6a2b5b4":"code","488e9abc":"code","5234ff40":"code","b05648bf":"code","0a3138de":"code","8dcf825b":"code","3b9c26da":"code","d717a7ce":"code","bae3e7f9":"code","2f7feb8c":"code","a62e3e3a":"code","c64be500":"code","d46986be":"code","6e6b4b2b":"code","59f8758f":"code","d7580d53":"code","a234ebf6":"code","96fdf8cb":"code","00cc9f67":"code","07747ef1":"code","c8ac08b2":"code","6a677ce0":"markdown","6a23eff2":"markdown","a09c1dff":"markdown","7adb434d":"markdown","9a2adc23":"markdown","d933b84c":"markdown","e0d39a1d":"markdown","e3ca4cec":"markdown","503fe84d":"markdown","fc04ca66":"markdown","ecc824f7":"markdown","58e0f122":"markdown","d5184661":"markdown","4c2c73fa":"markdown","70a6e2d6":"markdown","f15e8ac3":"markdown","5288c32d":"markdown","7ddec49f":"markdown","f8af2d69":"markdown","c9a74051":"markdown","217b311e":"markdown","643291c7":"markdown","1b1991f6":"markdown","2f920331":"markdown","1908d430":"markdown","cde23e5c":"markdown","6ae0e151":"markdown","0387e5e1":"markdown","9a3cb0c4":"markdown","0ca877df":"markdown","3fa263cf":"markdown","30cdc19a":"markdown","79164ada":"markdown","44ced56d":"markdown","b4451731":"markdown","c3ddc982":"markdown","c269f524":"markdown","ce85f8c3":"markdown","c92a5705":"markdown","b0005aff":"markdown","2c62ae02":"markdown","0392b04a":"markdown"},"source":{"17e5e11a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nimport nltk\n#nltk.download('stopwords')\n#nltk.download('punkt')\nfrom nltk.corpus import stopwords\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV\nfrom sklearn.svm import SVC\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nfrom nltk.tokenize import word_tokenize\nimport re\nimport warnings \n\nstop=set(stopwords.words('english'))\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","bc972d00":"train_df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_df = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","dde3864e":"print('Train Data size :{}'.format(train_df.shape))\nprint('Test Data size :{}'.format(test_df.shape))","00121692":"train_df.head()","3ad8c41e":"train_df.isna().sum()","62a58939":"test_df.isna().sum()","532092c6":"train_df.groupby('location')['id'].count()","0edd738f":"sns.set_style('darkgrid')\nplt.figure(figsize=(6,6))\nx=train_df['target'].value_counts()\nsns.barplot(x.index,x)","a550c57b":"fig,axes = plt.subplots(1,2,figsize=(12,6))\nchar_len = train_df[train_df['target']==1]['text'].str.len()\nsns.distplot(char_len,ax=axes[0],kde=False)\n\nchar_len = train_df[train_df['target']==0]['text'].str.len()\nsns.distplot(char_len,ax=axes[1],kde=False)","b50b68f3":"fig,axes= plt.subplots(2,1,figsize=(18,12))\n\ntemp = pd.DataFrame(train_df[train_df['target']==0].groupby('keyword')['id'].count())\ntemp.sort_values('id',ascending=False,inplace=True)\nsns.barplot(temp.index[:10],temp['id'][:10],ax=axes[0]).set_title('Normal Tweets')\n\n\ntemp = pd.DataFrame(train_df[train_df['target']==1].groupby('keyword')['id'].count())\ntemp.sort_values('id',ascending=False,inplace=True)\nsns.barplot(temp.index[:10],temp['id'][:10],ax=axes[1]).set_title('Disaster Tweets')","faa1bbb8":"# Referenec : https:\/\/www.analyticsvidhya.com\/blog\/2018\/07\/hands-on-sentiment-analysis-dataset-python\/\n\ndef hashtag_extract(x):\n    hashtags = []\n    # Loop over the words in the tweet\n    for i in x:\n        ht = re.findall(r\"#(\\w+)\", i)\n        hashtags.append(ht)\n\n    return hashtags\n\nHT_regular = hashtag_extract(train_df['text'][train_df['target'] == 0])\n\n# extracting hashtags from racist\/sexist tweets\nHT_disaster = hashtag_extract(train_df['text'][train_df['target'] == 1])\n\n# unnesting list\nHT_regular = sum(HT_regular,[])\nHT_disaster = sum(HT_disaster,[])","56795ffd":"fig,axes = plt.subplots(2,1,figsize=(18,10))\n\na = nltk.FreqDist(HT_regular)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})\n# selecting top 10 most frequent hashtags     \nd = d.nlargest(columns=\"Count\", n = 10) \nplt.figure(figsize=(16,5))\nsns.barplot(data=d, x= \"Hashtag\", y = \"Count\",ax=axes[0]).set_title('Normal Tweets')\n\n\na = nltk.FreqDist(HT_disaster)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})\n# selecting top 10 most frequent hashtags     \nd = d.nlargest(columns=\"Count\", n = 10) \nplt.figure(figsize=(16,5))\nsns.barplot(data=d, x= \"Hashtag\", y = \"Count\",ax=axes[1]).set_title('Disaster Tweets')\n\n","65109468":"Merge_df = train_df.append(test_df,ignore_index=True)","a6a2b5b4":"def remove_pattern(input_txt, pattern):\n    reg_obj = re.compile(pattern)\n    input_txt = reg_obj.sub(r'', input_txt)\n        \n    return input_txt   ","488e9abc":"Merge_df['text'] = Merge_df['text'].apply(lambda x: remove_pattern(x,\"@[\\w]*\"))","5234ff40":"# Reference : https:\/\/www.kaggle.com\/shahules\/tweets-complete-eda-and-basic-modeling\n\nMerge_df['text'] = Merge_df['text'].apply(lambda x: remove_pattern(x,'https?:\/\/\\S+|www\\.\\S+'))\nMerge_df['text'] = Merge_df['text'].apply(lambda x: remove_pattern(x,'<.*?>'))\n    ","b05648bf":"Merge_df['text'] = Merge_df['text'].apply(lambda x: remove_pattern(x,\"[^a-zA-Z# ]\"))","0a3138de":"def remove_stop_words(text):\n    \n    word_tokens = word_tokenize(text) \n  \n    filtered_sentence = [w for w in word_tokens if not w in stop] \n    \n    filtered_tweet = ' '.join(filtered_sentence)\n    \n    return filtered_tweet","8dcf825b":"Merge_df['text'] = Merge_df['text'].apply(lambda x: remove_stop_words(x))","3b9c26da":"from nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\ndef tokenize_stem(text):\n    \n    token_words = word_tokenize(text)\n    stem_words =[]\n    for i in token_words:\n        word = lemmatizer.lemmatize(i)\n        stem_words.append(word)\n        \n    final_tweet = ' '.join(stem_words)\n    \n    return final_tweet","d717a7ce":"Merge_df['text'] = Merge_df['text'].apply(lambda x: tokenize_stem(x))","bae3e7f9":"all_words = ' '.join([text for text in Merge_df['text'][Merge_df['target']==0]])\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n\nplt.figure(figsize=(16, 10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","2f7feb8c":"all_words = ' '.join([text for text in Merge_df['text'][Merge_df['target']==1]])\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n\nplt.figure(figsize=(16, 10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","a62e3e3a":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=300, stop_words='english')\n# TF-IDF feature matrix\ntfidf = tfidf_vectorizer.fit_transform(Merge_df['text'])","c64be500":"tfidf.shape","d46986be":"Final_train = tfidf[:7613]\nFinal_test = tfidf[7613:]","6e6b4b2b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\nxtrain = Final_train[:5331]\nxvalid = Final_train[5331:]\nytrain = Merge_df[:5331]['target']\nyvalid = Merge_df[5331:7613]['target']\n\nparameter = {'solver':['liblinear','lbfgs'],\n            'max_iter':[200,400]}\n\nLogis_clf = LogisticRegression()\n\nlreg = GridSearchCV(Logis_clf, param_grid = parameter, cv = 3, verbose=True, n_jobs=-1)\nlreg.fit(xtrain, ytrain) # training the model\n\nprediction = lreg.predict_proba(xvalid) # predicting on the validation set","59f8758f":"from sklearn.metrics import roc_auc_score,roc_curve,f1_score, confusion_matrix\n\n\n# keep probabilities for the positive outcome only\nlr_probs = prediction[:, 1]\nlr_auc = roc_auc_score(yvalid, lr_probs)\n\n\nprint('ROC AUC=%.3f' % (lr_auc))\n\nlr_fpr, lr_tpr, _ = roc_curve(yvalid, lr_probs)\n\n# plot the roc curve for the model\nplt.figure(figsize=(10,8))\nplt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.show()","d7580d53":"prediction_int = prediction[:,1] >= 0.35\nprediction_int = prediction_int.astype(np.int)\nf1score = f1_score(yvalid,prediction_int)\nprint('F1 Score : %.3f' %(f1score))\nconf = confusion_matrix(yvalid,prediction_int)\nprint(conf)","a234ebf6":"prediction = lreg.predict_proba(Final_test) # predicting on the test set\n\nprediction_int = prediction[:,1] >= 0.35\nprediction_int = prediction_int.astype(np.int)","96fdf8cb":"NB_Clf = MultinomialNB()\nNB_Clf.fit(xtrain, ytrain)\n\npred_naive = NB_Clf.predict(xvalid)\nconf = confusion_matrix(yvalid, pred_naive)\nprint(conf)\n\nf1score = f1_score(yvalid,pred_naive)\nprint('F1 Score : %.3f' %(f1score))\n\npred_naive_test = NB_Clf.predict(Final_test)\npred_naive_test = pred_naive_test.astype(int)\n\n","00cc9f67":"param_grid = {'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf']}\nSVM_Model = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=3, n_jobs=-1).fit(xtrain, ytrain)","07747ef1":"pred_SVM_valid = SVM_Model.predict(xvalid)\npred_SVM_valid = pred_SVM_valid.astype(int)\n\nconf = confusion_matrix(yvalid, pred_SVM_valid)\nprint(conf)\n\nf1score = f1_score(yvalid,pred_SVM_valid)\nprint('F1 Score : %.3f' %(f1score))","c8ac08b2":"test_df['target'] = prediction_int\n\nFinal_submission= test_df[['id','target']]\nFinal_submission.to_csv('submission.csv',index=False)","6a677ce0":"#### Do people go for lengthy tweets in case of disaster?? \nWell it depends on whether the person is talking about the disaster Vs reporting a disaster. Ideally, the first category would have lengthy tweets compared to the later","6a23eff2":"****Please upvote if you find this kernel interesting\/useful**","a09c1dff":"- Based on the top keywords in each category, it can be safely considered that keyword tell us about a disaster tweet to some extent.\n- Tweets in the disaster category seems to be around\n    - Train accident\n    - Oil spill in the ocean, accident\/damage to a ship\n    - Suicide bomb attack\n    - Outbreak of a virus or a disease\n    - Typhoon","7adb434d":"#### Define function to remove patterns","9a2adc23":"## Test against the 'Test Data'","d933b84c":"#### Word cloud for Normal tweets","e0d39a1d":"#### Word cloud for Disaster Tweets","e3ca4cec":"- Location data seems to have place name, coordinates & unreadable characters as well.\n- Seems it can't be used as is & needs good amount of preprocessing.\n- Let's park it aside for now","503fe84d":"#### Let's look at the top 10 keywords","fc04ca66":"### Naive Bayes","ecc824f7":"- Only a few tweets are missing keyword. This can be considered in the analysis\n- Approx. 33% of tweets are missing the location. We will analyze the rest 67% to see if any information can be extracted","58e0f122":"### Keywords mostly used in each category","d5184661":"# Disaster Tweet - Real or Not","4c2c73fa":"Consider 0.4 as cut-off for consider probability as 1","70a6e2d6":"## Import Libraries","f15e8ac3":"## Word Cloud","5288c32d":"### Final Submission File","7ddec49f":"## Time to clean the actual text","f8af2d69":"## Model Building","c9a74051":"![natural-disaster-clipart-2.jpg](attachment:natural-disaster-clipart-2.jpg)","217b311e":"Clearly the word cloud says it all.","643291c7":"### Tokenize & Stemming","1b1991f6":"<font color=blue> Merge the train & test to process everything atonce <\/font>","2f920331":"### Remove punctuations, special characters, numbers","1908d430":"Tweets are a the new way of expressing emotions, sharing news, promoting brands etc. It has also become a strong force that enables people let their voices\/feelings known to a company, Government or to the whole world.\n\nPeople often take to such social media platforms to share something. But given the volume of tweets, its often difficult to quickly identify whether a tweet is alarming something.","cde23e5c":"### Quick Peak into the data","6ae0e151":"Hashtags in twitter are synonymous with the ongoing trends on twitter at any particular point in time.Lets check the hashtags to see if they have something else to say","0387e5e1":"### Remove Stop words","9a3cb0c4":"- Story seems to be different here. While news hashtag can be ignored, certain hashtags seems to tell us about locations like Japan, India. Probably places linked to disasters\n- Hiroshima  - Are they talking about the Hiroshima Nagasaki incident here??","0ca877df":"## Read the files","3fa263cf":"### Logistic Regression","30cdc19a":"- Not so imbalanced dataset","79164ada":"## Hash Tags","44ced56d":"### Remove twitter handles","b4451731":"## TF-IDF","c3ddc982":"- Both categories have more or less same char length , lengthy tweets.\n- Seems people who tweeted about disaster were talking about it than reporting from ground 0.","c269f524":"![Twitter-Hashtags.png](attachment:Twitter-Hashtags.png)","ce85f8c3":"### Target Distribution","c92a5705":"\n\n### Remove URLs, http tags","b0005aff":"### SVM","2c62ae02":"## Tweets Distribution","0392b04a":"#### Tweets missing the keyword & location"}}