{"cell_type":{"dae13f6f":"code","241f550c":"code","2b1d316a":"code","14b91461":"code","53aa7d85":"code","51f06caf":"code","178715d6":"code","0ee811df":"code","04e51c4a":"code","ed79c06d":"code","23a3a684":"code","85ae2c0c":"code","9f12f77b":"code","4bd1b1a4":"code","4f577b07":"code","8c87a025":"code","d1254b86":"code","d6bbc0e0":"code","83f000b3":"code","85948936":"code","837aefb1":"code","a3828c1a":"code","a25ef91d":"code","90dc29a0":"markdown","08c97b9f":"markdown","a09f4aa5":"markdown","ca2304de":"markdown","4c8f6aff":"markdown","bd8c38e9":"markdown","54ee6a4a":"markdown","686da71b":"markdown","bc033766":"markdown","b749fc56":"markdown","fc443800":"markdown","15d41c91":"markdown","fd788df8":"markdown","4a192af5":"markdown","fc5da4bb":"markdown","09899860":"markdown","a4e8c008":"markdown","b6b1ffac":"markdown","3ca3238b":"markdown","b197e266":"markdown","44e6223c":"markdown","cc859a30":"markdown","47849cfb":"markdown","d98a9a57":"markdown","3d6826ad":"markdown","0e5f55fb":"markdown","912b6f6c":"markdown"},"source":{"dae13f6f":"# Quick load dataset and check\nimport pandas as pd\n","241f550c":"pathTrainSet = \".\/templates\/train_set.csv\"\npathTestSet = \".\/templates\/test_set.csv\"","2b1d316a":"filename = pathTrainSet\ndata_train = pd.read_csv(filename)\nfilename = pathTrainSet\ndata_test = pd.read_csv(filename)","14b91461":"data_test.describe()","53aa7d85":"import numpy as np\nimport category_encoders as ce\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn import metrics\nfrom sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_regression\n\n##Each function returns a pandadataframe for train- and testdata\n\n## Function to impute the missing values with most_frequent for bins and cats and mean for else\ndef impute(data, testdata):\n    else_col=data.columns[data.columns.str.contains(pat = '\\d+$')]\n    bin_col=data.columns[data.columns.str.contains(pat = 'bin')]\n    cat_col=data.columns[data.columns.str.contains(pat = 'cat')]\n    imputed_data = data.copy()\n    imputed_testdata = testdata.copy()\n    imp = SimpleImputer(missing_values=-1, strategy='most_frequent')\n    imp.fit(data[cat_col])\n    imputed_data[cat_col] = imp.transform(data[cat_col])\n    imputed_testdata[cat_col] = imp.transform(testdata[cat_col])\n    imp.fit(data[bin_col])\n    imputed_data[bin_col] = imp.transform(data[bin_col])\n    imputed_testdata[bin_col] = imp.transform(testdata[bin_col])\n    \n    imptwo = SimpleImputer(missing_values=-1., strategy='mean')\n    imptwo.fit(data[else_col])\n    imputed_data[else_col] = imptwo.transform(data[else_col])\n    imputed_testdata[else_col] = imptwo.transform(testdata[else_col])\n\n    return imputed_data, imputed_testdata\n\n## Function to encode the cat_columns\ndef encode_cat_columns (data_X, testdata_X, data_Y):\n    traindata = data_X.copy()\n    testdata = testdata_X.copy()\n    cat_col=data_X.columns[data_X.columns.str.contains(pat = 'cat')]\n    target_enc = ce.TargetEncoder(cols=cat_col)\n    target_enc.fit(data_X[cat_col], data_Y)\n    traindata[cat_col] = target_enc.transform(data_X[cat_col])\n    testdata[cat_col] = target_enc.transform(testdata_X[cat_col])\n    return traindata, testdata\n\n### Function to remove highly correlated data\ndef removecorrelateddata(data, testdata):\n    selected_data = data.copy()\n    selected_testdata = testdata.copy()\n    corr = selected_data.corr()\n    columns = np.full((corr.shape[0],), True, dtype=bool)\n    for i in range(corr.shape[0]):\n        for j in range(i+1, corr.shape[0]):\n            if corr.iloc[i,j] >= 0.6:\n                if columns[j]:\n                    columns[j] = False\n    selected_columns = selected_data.columns[columns]\n    selected_data = selected_data[selected_columns]\n    selected_testdata = selected_testdata[selected_columns]\n    return selected_data, selected_testdata\n\n##Oversample minority class\ndef oversampling(datax, datay):\n    smote= SMOTE(sampling_strategy = 'auto')\n    resampled_trainx, resampled_trainy = smote.fit_sample(datax, datay)\n    return resampled_trainx, resampled_trainy\n    \n##Univariate selection of best datafeatures\ndef featureselection(datax, testdatax, datay):\n    feature_cols = datax.columns[:]\n    selector = SelectKBest(f_classif, k=32, score_func=chi2)\n    data_new = selector.fit_transform(datax, datay)\n    selected_features = pd.DataFrame(selector.inverse_transform(data_new), \n                                 index=datax.index, \n                                 columns=feature_cols)\n    selected_columns = selected_features.columns[selected_features.var() != 0]\n    return datax[selected_columns], testdatax[selected_columns]\n\ndef undersampling(datax, datay):\n    undersample = RandomUnderSampler(sampling_strategy = 0.9)\n    resampled_datax, resampled_trainy = undersample.fit_resample(datax, datay)\n    return resampled_datax, resampled_trainy\n\n\ndef prepare_data_MLP(datax, testdatax, datay):\n    datax, testdatax = impute(datax, testdatax)\n    datax, testdatax = encode_cat_columns(datax, testdatax, datay)\n    datax, testdatax = removecorrelateddata(datax, testdatax)\n    datax, testdatax = featureselection(datax, testdatax, datay)\n    datax, datay = oversampling(datax, datay)\n    return datax, testdatax, datay\n    ","51f06caf":"import numpy as np\nimport category_encoders as ce\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn import metrics\nfrom sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_regression\nfrom imblearn.under_sampling import RandomUnderSampler","178715d6":"## Select target and features\nfea_col = data_train.columns[2:]\ndata_Y = data_train['target']\ndata_X = data_train[fea_col]\n\ndata_test_X = data_test.drop(columns=['id'])\n\nind_col = data_X.columns[data_X.columns.str.contains(pat = 'ind')]\nreg_col = data_X.columns[data_X.columns.str.contains(pat = 'reg')]\ncar_col = data_X.columns[data_X.columns.str.contains(pat = 'car')]\ncorrelation_data = data_X[ind_col]\ncorrelation_data[reg_col] = data_X[reg_col]\ncorrelation_data[car_col] = data_X[car_col]\n\nimport seaborn as sns\ncorr = correlation_data.corr()\nsns.heatmap(corr)","0ee811df":"imputed_data, imputed_testdata = impute(data_X, data_test_X)\n\n##-----------------------------------------\n#Test ob noch Nullwerte\nif -1 in imputed_data.to_numpy():\n    print(\"Nullwerte enthalten\")\nif -1 in imputed_testdata.to_numpy():\n    print(\"Nullwerte enthalten\")\n#--------------------------#----------------\n\nencoded_traindata, encoded_testdata = encode_cat_columns(imputed_data, imputed_testdata, data_Y)\n\nremoved_correlated_traindata, removed_correlated_testdata = removecorrelateddata(encoded_traindata, encoded_testdata)\n\nselected_traindata, selected_testdata = featureselection(removed_correlated_traindata, removed_correlated_testdata, data_Y)\n\nselected_traindata","04e51c4a":"\nx_train, x_val, y_train, y_val = train_test_split(selected_traindata, data_Y, test_size = 0.3, shuffle = True)\nx_oversampled, y_oversampled = oversampling(x_train, y_train)\nx_new, y_new = undersampling(x_oversampled, y_oversampled)\n","ed79c06d":"clf = RandomForestClassifier(max_depth=7, random_state=0, class_weight='balanced', n_estimators=300)\nclf = clf.fit(x_new, y_new)\ny_pred = clf.predict(x_val)\n\nprint(\"Percentage der Treffer\", sum(y_pred==y_val)\/len(y_val))\n##-----------------------------------\n\n\ndef extrac_one_label(x_val, y_val, label):\n    X_pos = x_val[y_val == label]\n    y_pos = y_val[y_val == label]\n    return X_pos, y_pos\n\nX_pos, y_pos = extrac_one_label(x_val, y_val, 1)\ny_pospred = clf.predict(X_pos)\nprint(\"Percentage der Treffer des 1-Labels\", sum(y_pospred==y_pos)\/len(y_pos))\n\nX_neg, y_neg = extrac_one_label(x_val, y_val, 0)\ny_negpred = clf.predict(X_neg)\nprint(\"Percentage der Treffer des 0-Labels\", sum(y_negpred==y_neg)\/len(y_neg))\n##--------------------------------------------------------------------------------------\n    \n\nprint(\"truenegative\",sum(y_negpred==0))\nprint(\"truepositive\",sum(y_pospred==1))\nprint(\"falsepostive\",sum(y_pospred==0))\nprint(\"falsenegative\",sum(y_negpred==1))\n\n\n\nprint(metrics.f1_score(y_val, y_pred, average='macro'))\n","23a3a684":"y_target = clf.predict(selected_testdata)\nprint(\"negative outputs: \", sum(y_target==0))\nprint(\"positive outputs: \", sum(y_target==1))","85ae2c0c":"data_out = pd.DataFrame(data_test['id'].copy())\ndata_out.insert(1, \"target\", y_target, True) \ndata_out.to_csv('submission.csv',index=False)","9f12f77b":"from imblearn.over_sampling import SMOTE\n\nsmote= SMOTE(sampling_strategy='auto')\n\nx_train, x_val, y_train, y_val = train_test_split(data_X, data_Y, test_size = 0.3, shuffle = True)\n\nx_new, y_new = smote.fit_sample(x_train, y_train)\n\nprint (x_new.describe())\nprint (y_new.describe())\n\nclf = DecisionTreeClassifier()\nclf = clf.fit(x_new, y_new)\ny_pred = clf.predict(x_val)","4bd1b1a4":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split\n\nprint(\"staring methode\")\n\nfea_col = data_train.columns[2:]\ndata_Y_train = data_train['target']\ndata_X_train = data_train[fea_col]\ndata_X_test = data_test.drop(columns=['id'])\n\n##---------- prepare data\ndata_X_train, data_X_test, data_Y_train = prepare_data_MLP(data_X_train, data_X_test, data_Y_train)\n\n##---------- split into train and test\nx_train, x_val, y_train, y_val = train_test_split(data_X_train, data_Y_train, test_size = 0.3, shuffle = True)\n\nprint(\"Started Training model, takes a while\")\n\nclf = MLPClassifier(verbose = True)\nclf.fit(x_train, y_train)\ny_pred = clf.predict(x_val)\n\nprint(\"Percentage der Treffer\", sum(y_pred==y_val)\/len(y_val))\n##-----------------------------------\n\n\ndef extrac_one_label(x_val, y_val, label):\n    X_pos = x_val[y_val == label]\n    y_pos = y_val[y_val == label]\n    return X_pos, y_pos\n\nX_pos, y_pos = extrac_one_label(x_val, y_val, 1)\ny_pospred = clf.predict(X_pos)\nprint(\"Percentage der Treffer des 1-Labels\", sum(y_pospred==y_pos)\/len(y_pos))\n\nX_neg, y_neg = extrac_one_label(x_val, y_val, 0)\ny_negpred = clf.predict(X_neg)\nprint(\"Percentage der Treffer des 0-Labels\", sum(y_negpred==y_neg)\/len(y_neg))","4f577b07":"y_target = clf.predict(data_X_test)\n\ndata_out = pd.DataFrame(data_test['id'].copy())\ndata_out.insert(1, \"target\", y_target, True) \ndata_out.to_csv('submission.csv',index=False)","8c87a025":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nimport numpy as np\n# reduce data so there are not 96% positive anymore instead 50% positive\ndata_pos = data_train.loc[data_train['target'] == 1]\ndata_neg = data_train.loc[data_train['target'] == 0]\ndata_neg_reduced = data_neg.sample(len(data_pos))\n\n#combine and shuffel those 2 sets\ndata = data_pos.append(data_neg_reduced)\ndata = shuffle(data)\n\n##prepare for training\nfea_col = data_train.columns[2:]\n\ndata_Y = data['target']\ndata_X = data[fea_col]\n\n##train model\nx_train, x_val, y_train, y_val = train_test_split(data_X, data_Y, test_size = 0.3, shuffle = True)\nclf = DecisionTreeClassifier(min_impurity_decrease = 0.001, min_samples_split = 20)\nclf = clf.fit(x_train, y_train)\ny_pred = clf.predict(x_val)\n\nprint(\"overall accuracy\", sum(y_pred==y_val)\/len(y_val))\n\ndef extrac_one_label(x_val, y_val, label):\n    X_pos = x_val[y_val == label]\n    y_pos = y_val[y_val == label]\n    return X_pos, y_pos\n\nX_pos, y_pos = extrac_one_label(x_val, y_val, 1)\ny_pospred = clf.predict(X_pos)\nprint(\"accuracy for positive values:\", sum(y_pospred==y_pos)\/len(y_pos))\n\nX_neg, y_neg = extrac_one_label(x_val, y_val, 0)\ny_pospred = clf.predict(X_neg)\nprint(\"accuracy for negative values:\", sum(y_pospred==y_neg)\/len(y_neg))","d1254b86":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nfea_col = data_train.columns[2:]\ndata_Y_train = data_train['target']\ndata_X_train = data_train[fea_col]\ndata_X_test = data_test.drop(columns=['id'])\n\n##---------- prepare data\ndata_X_train, data_X_test, data_Y_train = prepare_data_MLP(data_X_train, data_X_test, data_Y_train)\n\nx_train, x_val, y_train, y_val = train_test_split(data_X_train, data_Y_train, test_size = 0.3, shuffle = True)\n\nclf = RandomForestClassifier(n_estimators = 19, min_impurity_decrease = 0.001, class_weight=\"balanced\")\nclf = clf.fit(x_train, y_train)\ny_pred = clf.predict(x_val)\n\nprint(\"Percentage der Treffer\", sum(y_pred==y_val)\/len(y_val))\n##-----------------------------------\n\n\ndef extrac_one_label(x_val, y_val, label):\n    X_pos = x_val[y_val == label]\n    y_pos = y_val[y_val == label]\n    return X_pos, y_pos\n\nX_pos, y_pos = extrac_one_label(x_val, y_val, 1)\ny_pospred = clf.predict(X_pos)\nprint(\"Percentage der Treffer des 1-Labels\", sum(y_pospred==y_pos)\/len(y_pos))\n\nX_neg, y_neg = extrac_one_label(x_val, y_val, 0)\ny_negpred = clf.predict(X_neg)\nprint(\"Percentage der Treffer des 0-Labels\", sum(y_negpred==y_neg)\/len(y_neg))\n##--------------------------------------------------------------------","d6bbc0e0":"y_target = clf.predict(data_X_test)\n\ndata_out = pd.DataFrame(data_test['id'].copy())\ndata_out.insert(1, \"target\", y_target, True) \ndata_out.to_csv('submission.csv',index=False)","83f000b3":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n## Select target and features\nfea_col = data_train.columns[2:]\ndata_Y = data_train['target']\ndata_X = data_train[fea_col]\n\n\n##-----------------------------------------------------------------------------------\n\nx_train, x_val, y_train, y_val = train_test_split(data_X, data_Y, test_size = 0.3, shuffle = True)\n\n##------------------------------------------------------------------------------------\n##  Gewichtung der Daten\n\n## imbalanced data werden mit class_weight=balanced gewichtet\nclf = DecisionTreeClassifier(min_impurity_decrease = 0.001, class_weight=\"balanced\")\nclf = clf.fit(x_train, y_train)\ny_pred = clf.predict(x_val)\n\nprint(\"Percentage der Treffer\", sum(y_pred==y_val)\/len(y_val))\n##-----------------------------------\n\n\ndef extrac_one_label(x_val, y_val, label):\n    X_pos = x_val[y_val == label]\n    y_pos = y_val[y_val == label]\n    return X_pos, y_pos\n\nX_pos, y_pos = extrac_one_label(x_val, y_val, 1)\ny_pospred = clf.predict(X_pos)\nprint(\"Percentage der Treffer des 1-Labels\", sum(y_pospred==y_pos)\/len(y_pos))\n\nX_neg, y_neg = extrac_one_label(x_val, y_val, 0)\ny_negpred = clf.predict(X_neg)\nprint(\"Percentage der Treffer des 0-Labels\", sum(y_negpred==y_neg)\/len(y_neg))\n##--------------------------------------------------------------------------------------\n    \n\nprint(\"negative\",sum(y_negpred==0))\nprint(\"positive\",sum(y_pospred==1))\nprint(\"falsenegative\",sum(y_pospred==0))\nprint(\"falsepositive\",sum(y_negpred==1))\n","85948936":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nimport numpy as np\n\n## Oversampling (can be replaced with smote etc.\/could not get it to work on my machine)\ndata_pos = data_train.loc[data_train['target'] == 1]\ndata_neg = data_train.loc[data_train['target'] == 0]\ndata_pos_oversampled = data_pos.sample(len(data_neg), replace=True)\n\ndata = data_pos_oversampled.append(data_neg)\ndata = shuffle(data)\n\n## Select target and features\nfea_col = data_train.columns[2:]\ndata_Y = data['target']\ndata_X = data[fea_col]\n\n##-----------------------------------------------------------------------------------\n\n\nclf = LDA(n_components=None, priors=None, shrinkage='auto', solver='lsqr',\nstore_covariance=False, tol=0.0001)\nclf.fit(x_train, y_train)\ny_pred = clf.predict(x_val)\n\nprint(\"Percentage der Treffer (oversampled)\", sum(y_pred==y_val)\/len(y_val))\n##-----------------------------------\n\n\ndef extrac_one_label(x_val, y_val, label):\n    X_pos = x_val[y_val == label]\n    y_pos = y_val[y_val == label]\n    return X_pos, y_pos\n\nX_pos, y_pos = extrac_one_label(x_val, y_val, 1)\ny_pospred = clf.predict(X_pos)\nprint(\"Percentage der Treffer des 1-Labels\", sum(y_pospred==y_pos)\/len(y_pos))\n\nX_neg, y_neg = extrac_one_label(x_val, y_val, 0)\ny_negpred = clf.predict(X_neg)\nprint(\"Percentage der Treffer des 0-Labels\", sum(y_negpred==y_neg)\/len(y_neg))\n##--------------------------------------------------------------------------------------\n    \n\nprint(\"truenegative\",sum(y_negpred==0))\nprint(\"truepositive\",sum(y_pospred==1))\nprint(\"falsenegative\",sum(y_pospred==0))\nprint(\"falsepositive\",sum(y_negpred==1))","837aefb1":"data_test_X = data_test.drop(columns=['id'])\ny_target = clf.predict(data_test_X)\nprint(\"negative outputs: \", sum(y_target==0))\nprint(\"positive outputs: \", sum(y_target==1))","a3828c1a":"data_out = pd.DataFrame(data_test['id'].copy())\ndata_out.insert(1, \"target\", y_target, True) \ndata_out.to_csv('submission.csv',index=False)","a25ef91d":"data_out","90dc29a0":"# Paths","08c97b9f":"### Submission","a09f4aa5":"### now manipulate data with functions from before","ca2304de":"## RandomDecisionForest with Data Preperation","4c8f6aff":"The prefix, e.g. `ind` and `calc`, indicate the feature belongs to similiar groupings. The postfix `bin` indicates binary features and `cat` indicates categorical features. The features without postfix are ordinal or continuous. Similarly, you can check the statistics for testing data:","bd8c38e9":"### Use train model","54ee6a4a":"## 2. Just Oversampling","686da71b":"## Primitive solution (undersampling)","bc033766":"## Phase 1: 26th May - 9th June\n\n### Data Description\n\nIn order to take a look at the data, you can use the `describe()` method. As you can see in the result, each row has a unique `id`. `Target` $\\in \\{0, 1\\}$ is whether a user will file a claim in his insurance period. The rest of the 57 columns are features regarding customers' profiles. You might also notice that some of the features have minimum values of `-1`. This indicates that the actual value is missing or inaccessible.\n","b749fc56":"# Machine Learning 2020 Course Projects\n\n## Project Schedule\n\nIn this project, you will solve a real-life problem with a dataset. The project will be separated into two phases:\n\n27th May - 10th June: We will give you a training set with target values and a testing set without target. You predict the target of the testing set by trying different machine learning models and submit your best result to us and we will evaluate your results first time at the end of phase 1.\n\n9th June - 24th June: Students stand high in the leader board will briefly explain  their submission in a proseminar. We will also release some general advice to improve the result. You try to improve your prediction and submit final results in the end. We will again ask random group to present and show their implementation.\nThe project shall be finished by a team of two people. Please find your teammate and REGISTER via [here](https:\/\/docs.google.com\/forms\/d\/e\/1FAIpQLSf4uAQwBkTbN12E0akQdxfXLgUQLObAVDRjqJHcNAUFwvRTsg\/alreadyresponded).\n\nThe submission and evaluation is processed by [Kaggle](https:\/\/www.kaggle.com\/t\/b3dc81e90d32436d93d2b509c98d0d71).  In order to submit, you need to create an account, please use your team name in the `team tag` on the [kaggle page](https:\/\/www.kaggle.com\/t\/b3dc81e90d32436d93d2b509c98d0d71). Two people can submit as a team in Kaggle.\n\nYou can submit and test your result on the test set 2 times a day, you will be able to upload your predicted value in a CSV file and your result will be shown on a leaderboard. We collect data for grading at 22:00 on the **last day of each phase**. Please secure your best results before this time.\n\n","fc443800":"# Different Solutions (each box is another solution)","15d41c91":"### Split into test and train data and oversample the minority class and undersample the priority","fd788df8":"## Functions to Prepare Data","4a192af5":"### Submission\nUse standard submission at bottom","fc5da4bb":"### Submission\nUse standard submission at bottom","09899860":"### Submission\nUse standard Submission at bottom","a4e8c008":"## LCA + Manual oversampling","b6b1ffac":"### Submission\nUse standard submission at bottom","3ca3238b":"## Weighting labels","b197e266":"# Submission (standard)\n\nPlease only submit the csv files with predicted outcome with its id and target [here](https:\/\/www.kaggle.com\/t\/b3dc81e90d32436d93d2b509c98d0d71). Your column should only contain `0` and `1`.","44e6223c":"### Submission","cc859a30":"# Data Manipulation","47849cfb":"## Project Description\n\nCar insurance companies are always trying to come up with a fair insurance plan for customers. They would like to offer a lower price to the careful and safe driver while the careless drivers who file claims in the past will pay more. In addition, more safe drivers mean that the company will spend less in operation. However, for new customers, it is difficult for the company to know who the safe driver is. As a result, if a company offers a low price, it bears a high risk of cost. If not, the company loses competitiveness and encourage new customers to choose its competitors.\n\n\nYour task is to create a machine learning model to mitigate this problem by identifying the safe drivers in new customers based on their profiles. The company then offers them a low price to boost safe customer acquirement and reduce risks of costs. We provide you with a dataset (train_set.csv) regarding the profile (columns starting with ps_*) of customers. You will be asked to predict whether a customer will file a claim (`target`) in the next year with the test_set.csv \n\nYou can find the dataset in the `project` folders in the jupyter hub. We also upload dataset to Kaggle and will test your result and offer you a leaderboard in Kaggle:\nhttps:\/\/www.kaggle.com\/t\/b3dc81e90d32436d93d2b509c98d0d71","d98a9a57":"## 1. DecisionTreeClassifier with various Data Preperation (Best Score)","3d6826ad":"### Look how different features are correlated","0e5f55fb":"### Submission","912b6f6c":"## 3. Data Manipulation + MLPClassifier"}}