{"cell_type":{"6f8dec82":"code","ef4f1ee8":"code","c19dcb7f":"code","fe0e9e82":"code","ae17b650":"code","874db141":"code","8fe62592":"code","ad2b7d34":"code","c38ce6ff":"code","901387d9":"code","4c3bb8f9":"code","f44fbae8":"code","540ca0c8":"code","cde94463":"code","d3c705c8":"code","d20724a3":"code","8b195850":"code","de9c81c4":"code","3e8f6cdd":"code","a5e35813":"code","abe46a93":"code","d2ecf0b6":"markdown","d022aee7":"markdown","96c2600d":"markdown","66caf070":"markdown","14d56a16":"markdown","32d4167f":"markdown","9aeb829a":"markdown","3ef27318":"markdown","80a2c365":"markdown"},"source":{"6f8dec82":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","ef4f1ee8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn\nimport warnings\nwarnings.filterwarnings('ignore')","c19dcb7f":"data = pd.read_json('\/kaggle\/input\/news-category-dataset\/News_Category_Dataset_v2.json', lines=True)\ndata.head()","fe0e9e82":"data = data[[\"category\", \"headline\"]]\ndata.category.value_counts().plot.bar(figsize = (20,10))","ae17b650":"mapper = {}\n\nfor i,cat in enumerate(data[\"category\"].unique()):\n        mapper[cat] = i\n\ndata[\"category_target\"] = data[\"category\"].map(mapper)\ndata.head()","874db141":"from sklearn.feature_extraction.text import CountVectorizer","8fe62592":"#toy example\ntext=[\"My name is Paul my life is Jane! And we live our life together\" , \"My name is Guido my life is Victoria! And we live our life together\"]\ntoy = CountVectorizer(stop_words = 'english')\n# https:\/\/docs.python.org\/2\/library\/re.html Token pattern explained token_pattern=r'\\w+|\\,',\ntoy.fit_transform(text)\nprint (toy.vocabulary_)\nmatrix = toy.transform(text)\nprint (matrix)\nfeatures = toy.get_feature_names()\ndf_res = pd.DataFrame(matrix.toarray(), columns=features)\ndf_res","ad2b7d34":"vect = CountVectorizer(stop_words = 'english')\nX_train_matrix = vect.fit_transform(data[\"headline\"]) \n","c38ce6ff":"print (X_train_matrix.shape)","901387d9":"print (\"shape of the matrix \", X_train_matrix.shape)\nprint (\"one example\" , data[\"headline\"][1515])","4c3bb8f9":"column = vect.vocabulary_[\"hollywood\"]\nprint (column)\nvect.get_feature_names()[column]\n","f44fbae8":"y = data[\"category_target\"]","540ca0c8":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_train_matrix, y, test_size=0.3)\nfrom sklearn.naive_bayes import MultinomialNB\nclf=MultinomialNB()\nclf.fit(X_train, y_train)\nprint (clf.score(X_train, y_train))\nprint (clf.score(X_test, y_test))\npredicted_result=clf.predict(X_test)\nfrom sklearn.metrics import classification_report\n#print(classification_report(y_test,predicted_result))","cde94463":"pi = {}\nAll = data[\"category_target\"].value_counts().sum()","d3c705c8":"for i, cat in enumerate (data[\"category_target\"].value_counts(sort = False)):\n    pi[i] = cat \/ All\n\nprint(\"Probability of each class:\")\nprint(\"\\n\".join(\"{}: {}\".format(k, v) for k, v in pi.items()))","d20724a3":"vect = CountVectorizer(stop_words = 'english')\nX_train_matrix = vect.fit_transform(data[\"headline\"]) ","8b195850":"docIdx, wordIdx = X_train_matrix.nonzero()\ncount = X_train_matrix.data","de9c81c4":"classIdx = []\n\nfor idx in docIdx:\n        \n    classIdx.append(data[\"category_target\"].iloc[idx])\n\n    \n    ","3e8f6cdd":"df = pd.DataFrame()\ndf[\"docIdx\"] = np.array(docIdx)\ndf[\"wordIdx\"] = np.array(wordIdx)\ndf[\"count\"] = np.array(count)\ndf[\"classIdx\"] = np.array(classIdx)\ndf.info()","a5e35813":"len(vect.vocabulary_)","abe46a93":"#Alpha value for smoothing\na = 0.001\n#Calculate probability of each word based on class\npb_ij = df.groupby(['classIdx','wordIdx'])\npb_j = df.groupby(['classIdx'])\nPr =  (pb_ij['count'].sum() + a) \/ (pb_j['count'].sum() + len(vect.vocabulary_))    \n#Unstack series\nPr = Pr.unstack()\n\n#Replace NaN or columns with 0 as word count with a\/(count+|V|+1)\nfor c in range(0,41):\n    Pr.loc[c,:] = Pr.loc[c,:].fillna(a\/(pb_j['count'].sum()[c] + 16689))\n\n#Convert to dictionary for greater speed\nPr_dict = Pr.to_dict()\n\nPr","d2ecf0b6":"**EXAMPLE**\n\nwe need to calculate the count of each word.\nNow let say you have got a text like \"My name is Paul, My life is Jane!\"","d022aee7":"### convert to a pandas dataframe is clear but the model uses the matrix","96c2600d":" ## Probability Distribution over Vocabulary","66caf070":"## Class Distribution","14d56a16":"For calculating our probability, we will find the average of each word for a given class. since some words will have 0 counts, we will perform a Laplace Smoothing","32d4167f":"credit to:\nhttps:\/\/towardsdatascience.com\/multinomial-naive-bayes-classifier-for-text-analysis-python-8dd6825ece67","9aeb829a":"# Build the classifier in Scikit Learn","3ef27318":"# Implementation of the algorithm from scratch","80a2c365":"# Vectorisation"}}