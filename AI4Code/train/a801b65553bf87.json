{"cell_type":{"a7b37d0f":"code","8d682760":"code","543388b5":"code","d350c93d":"code","72ff6d48":"code","218582ac":"code","4a0450f6":"code","fc9b6f1b":"code","16132071":"code","9ac8e046":"code","dc5544da":"code","ca323b04":"code","913dcb01":"code","5837478b":"code","aa45a577":"code","cdd4dcf0":"code","ffbf7694":"code","143cd977":"code","7f416d72":"code","cb51ccda":"code","2be2b98a":"code","1b970e15":"code","97d31fdc":"code","6c4b9f3f":"code","89bd3010":"code","541e1f54":"code","4ca0b549":"code","5e9fbd85":"code","1cd5ae3a":"code","81976f35":"code","c1dd4cdf":"code","51c0f6d0":"code","1359818a":"code","98c1c102":"code","9b50ec07":"code","159081ee":"markdown"},"source":{"a7b37d0f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8d682760":"building_df = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/building_metadata.csv\")\nweather_train = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/weather_train.csv\")\ntrain = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/train.csv\")\n\ntrain = train.merge(building_df, left_on = \"building_id\", right_on = \"building_id\", how = \"left\")\ntrain = train.merge(weather_train, left_on = [\"site_id\", \"timestamp\"], right_on = [\"site_id\", \"timestamp\"])\ndel weather_train","543388b5":"train.loc[(train['meter']==0) & (train['site_id']==0) & (train['timestamp']<'2016-05-21 00:00:00'), 'drop'] = True\ntrain = train[train['drop']!=True]","d350c93d":"def average_imputation(df, column_name):\n    imputation = df.groupby(['timestamp'])[column_name].mean()\n    \n    df.loc[df[column_name].isnull(), column_name] = df[df[column_name].isnull()][[column_name]].apply(lambda x: imputation[df['timestamp'][x.index]].values)\n    del imputation\n    return df","72ff6d48":"train = average_imputation(train, 'wind_speed')\n\nbeaufort = [(0, 0, 0.3), (1, 0.3, 1.6), (2, 1.6, 3.4), (3, 3.4, 5.5), (4, 5.5, 8), (5, 8, 10.8), (6, 10.8, 13.9), \n          (7, 13.9, 17.2), (8, 17.2, 20.8), (9, 20.8, 24.5), (10, 24.5, 28.5), (11, 28.5, 33), (12, 33, 200)]\n\nfor item in beaufort:\n    train.loc[(train['wind_speed']>=item[1]) & (train['wind_speed']<item[2]), 'beaufort_scale'] = item[0]","218582ac":"train[\"timestamp\"] = pd.to_datetime(train[\"timestamp\"])\ntrain[\"weekday\"] = train[\"timestamp\"].dt.weekday\ntrain[\"hour\"] = train[\"timestamp\"].dt.hour\ntrain[\"weekday\"] = train['weekday'].astype(np.uint8)\ntrain[\"hour\"] = train['hour'].astype(np.uint8)\ntrain[\"month\"] = train[\"timestamp\"].dt.month\ntrain['year_built'] = train['year_built']-1900\ntrain['square_feet'] = np.log(train['square_feet'])","4a0450f6":"train['group'] = train['month']\ntrain['group'].replace((6, 7, 8), 21, inplace=True)\ntrain['group'].replace((9, 10, 11), 22, inplace=True)\ntrain['group'].replace((3, 4, 5), 23, inplace=True)\ntrain['group'].replace((1, 2, 12), 24, inplace=True)\ntrain['group'].replace((21), 1, inplace=True)\ntrain['group'].replace((22), 2, inplace=True)\ntrain['group'].replace((23), 3, inplace=True)\ntrain['group'].replace((24), 4, inplace=True)","fc9b6f1b":"import gc\ndel train[\"timestamp\"]\ndel train[\"drop\"]\ngc.collect()","16132071":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ntrain[\"primary_use\"] = le.fit_transform(train[\"primary_use\"])\n\ncategoricals = ['building_id',\"primary_use\", \"hour\", \"weekday\", \"meter\"]","9ac8e046":"drop_cols = ['site_id', \"sea_level_pressure\", \"wind_speed\", 'wind_direction', 'month', 'dew_temperature', \"group\"]\n\nnumericals = [\"square_feet\", \"year_built\", \"air_temperature\", \"cloud_coverage\", 'beaufort_scale', 'precip_depth_1_hr', \"floor_count\"]\n\nfeat_cols = categoricals + numericals","dc5544da":"meter_df = [train[train['meter']==i] for i in range(0,4)]\ndel train","ca323b04":"targets = [np.log1p(item[\"meter_reading\"]) for item in meter_df]\n\nfor item in meter_df:\n    del item[\"meter_reading\"]\n    for col in drop_cols:\n        del item[col]","913dcb01":"from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\nparams = {\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': {'rmse'},\n             \"num_leaves\": 1280,\n            \"learning_rate\": 0.05,\n            \"feature_fraction\": 0.85,\n            \"reg_lambda\": 2\n            }\n\nfolds = 3\nseed = 666\n\nkf = KFold(n_splits=folds, shuffle=False, random_state=seed)\nmodels0 = []\nfor train_index, val_index in kf.split(meter_df[0]):\n    train_X = meter_df[0][feat_cols].iloc[train_index]\n    val_X = meter_df[0][feat_cols].iloc[val_index]\n    train_y = targets[0].iloc[train_index]\n    val_y = targets[0].iloc[val_index]\n    lgb_train = lgb.Dataset(train_X, train_y, categorical_feature=categoricals)\n    lgb_eval = lgb.Dataset(val_X, val_y, categorical_feature=categoricals)\n    gbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=10000,\n                valid_sets=(lgb_train, lgb_eval),\n               early_stopping_rounds=100,\n               verbose_eval = 100)\n    models0.append(gbm)","5837478b":"params = {\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': {'rmse'},\n             \"num_leaves\": 1280,\n            \"learning_rate\": 0.05,\n            \"feature_fraction\": 0.85,\n            \"reg_lambda\": 2\n            }\n\nfolds = 3\nseed = 666\n\nkf = KFold(n_splits=folds, shuffle=False, random_state=seed)\nmodels1 = []\nfor train_index, val_index in kf.split(meter_df[1]):\n    train_X = meter_df[1][feat_cols].iloc[train_index]\n    val_X = meter_df[1][feat_cols].iloc[val_index]\n    train_y = targets[1].iloc[train_index]\n    val_y = targets[1].iloc[val_index]\n    lgb_train = lgb.Dataset(train_X, train_y, categorical_feature=categoricals)\n    lgb_eval = lgb.Dataset(val_X, val_y, categorical_feature=categoricals)\n    gbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=1000,\n                valid_sets=(lgb_train, lgb_eval),\n               early_stopping_rounds=100,\n               verbose_eval = 100)\n    models1.append(gbm)","aa45a577":"params = {\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': {'rmse'},\n             \"num_leaves\": 1280,\n            \"learning_rate\": 0.05,\n            \"feature_fraction\": 0.85,\n            \"reg_lambda\": 2\n            }\n\nfolds = 3\nseed = 666\n\nkf = KFold(n_splits=folds, shuffle=False, random_state=seed)\nmodels2 = []\nfor train_index, val_index in kf.split(meter_df[2]):\n    train_X = meter_df[2][feat_cols].iloc[train_index]\n    val_X = meter_df[2][feat_cols].iloc[val_index]\n    train_y = targets[2].iloc[train_index]\n    val_y = targets[2].iloc[val_index]\n    lgb_train = lgb.Dataset(train_X, train_y, categorical_feature=categoricals)\n    lgb_eval = lgb.Dataset(val_X, val_y, categorical_feature=categoricals)\n    gbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=10000,\n                valid_sets=(lgb_train, lgb_eval),\n               early_stopping_rounds=100,\n               verbose_eval = 100)\n    models2.append(gbm)","cdd4dcf0":"params = {\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': {'rmse'},\n             \"num_leaves\": 1280,\n            \"learning_rate\": 0.05,\n            \"feature_fraction\": 0.85,\n            \"reg_lambda\": 2\n            }\n\nfolds = 3\nseed = 666\n\nkf = KFold(n_splits=folds, shuffle=False, random_state=seed)\nmodels3 = []\nfor train_index, val_index in kf.split(meter_df[3]):\n    train_X = meter_df[3][feat_cols].iloc[train_index]\n    val_X = meter_df[3][feat_cols].iloc[val_index]\n    train_y = targets[3].iloc[train_index]\n    val_y = targets[3].iloc[val_index]\n    lgb_train = lgb.Dataset(train_X, train_y, categorical_feature=categoricals)\n    lgb_eval = lgb.Dataset(val_X, val_y, categorical_feature=categoricals)\n    gbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=10000,\n                valid_sets=(lgb_train, lgb_eval),\n               early_stopping_rounds=100,\n               verbose_eval = 100)\n    models3.append(gbm)","ffbf7694":"import gc\ndel meter_df, train_X, val_X, lgb_train, lgb_eval, train_y, val_y, targets\ngc.collect()","143cd977":"#preparing test data\ntest = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/test.csv\")\ntest = test.merge(building_df, left_on = \"building_id\", right_on = \"building_id\", how = \"left\")\ndel building_df\ngc.collect()\ntest[\"primary_use\"] = le.transform(test[\"primary_use\"])\n\nweather_test = pd.read_csv(\"..\/input\/ashrae-energy-prediction\/weather_test.csv\")\ntest = test.merge(weather_test, left_on = [\"site_id\", \"timestamp\"], right_on = [\"site_id\", \"timestamp\"], how = \"left\")\ndel weather_test","7f416d72":"test = average_imputation(test, 'wind_speed')\n\nfor item in beaufort:\n    test.loc[(test['wind_speed']>=item[1]) & (test['wind_speed']<item[2]), 'beaufort_scale'] = item[0]\n    \ntest['beaufort_scale'] = test['beaufort_scale'].astype(np.uint8)\n\ntest[\"timestamp\"] = pd.to_datetime(test[\"timestamp\"])\ntest[\"hour\"] = test[\"timestamp\"].dt.hour\ntest[\"weekday\"] = test[\"timestamp\"].dt.weekday\ntest[\"weekday\"] = test['weekday'].astype(np.uint8)\ntest[\"hour\"] = test['hour'].astype(np.uint8)\ntest[\"month\"] = test[\"timestamp\"].dt.month\ntest['year_built'] = test['year_built']-1900\ntest['square_feet'] = np.log(test['square_feet'])\n\ntest['group'] = test['month']\ntest['group'].replace((6, 7, 8), 21, inplace=True)\ntest['group'].replace((9, 10, 11), 22, inplace=True)\ntest['group'].replace((3, 4, 5), 23, inplace=True)\ntest['group'].replace((1, 2, 12), 24, inplace=True)\ntest['group'].replace((21), 1, inplace=True)\ntest['group'].replace((22), 2, inplace=True)\ntest['group'].replace((23), 3, inplace=True)\ntest['group'].replace((24), 4, inplace=True)\n","cb51ccda":"test = test.drop([\"sea_level_pressure\", \"wind_direction\", \"timestamp\", 'site_id', \"wind_speed\", 'month', 'dew_temperature', \"group\"], axis=1)","2be2b98a":"test_df = [test[test['meter']==i] for i in range(0,4)]\ndel test","1b970e15":"row_ids = [list(item['row_id']) for item in test_df]","97d31fdc":"drop_list = ['row_id', 'meter']\n\nfor item in test_df:\n    for col in drop_list:\n        del item[col]","6c4b9f3f":"from tqdm import tqdm\ni=0\nres0=[]\nstep_size = 50000\nfor j in tqdm(range(int(np.ceil(test_df[0].shape[0]\/50000)))):\n    res0.append(sum(np.expm1([model.predict(test_df[0].iloc[i:i+step_size]) for model in models0])\/folds))\n    i+=step_size","89bd3010":"i=0\nres1=[]\nstep_size = 50000\nfor j in tqdm(range(int(np.ceil(test_df[1].shape[0]\/50000)))):\n    res1.append(sum(np.expm1([model.predict(test_df[1].iloc[i:i+step_size]) for model in models1])\/folds))\n    i+=step_size","541e1f54":"i=0\nres2=[]\nstep_size = 50000\nfor j in tqdm(range(int(np.ceil(test_df[2].shape[0]\/50000)))):\n    res2.append(sum(np.expm1([model.predict(test_df[2].iloc[i:i+step_size]) for model in models2])\/folds))\n    i+=step_size","4ca0b549":"i=0\nres3=[]\nstep_size = 50000\nfor j in tqdm(range(int(np.ceil(test_df[3].shape[0]\/50000)))):\n    res3.append(sum(np.expm1([model.predict(test_df[3].iloc[i:i+step_size]) for model in models3])\/folds))\n    i+=step_size","5e9fbd85":"res0 = np.concatenate(res0)\nres1 = np.concatenate(res1)\nres2 = np.concatenate(res2)\nres3 = np.concatenate(res3)","1cd5ae3a":"del test_df","81976f35":"res0 = pd.DataFrame(data=res0,columns=['meter_reading'])\nres1 = pd.DataFrame(data=res1,columns=['meter_reading'])\nres2 = pd.DataFrame(data=res2,columns=['meter_reading'])\nres3 = pd.DataFrame(data=res3,columns=['meter_reading'])","c1dd4cdf":"res0['row_id'] = row_ids[0]\nres1['row_id'] = row_ids[1]\nres2['row_id'] = row_ids[2]\nres3['row_id'] = row_ids[3]","51c0f6d0":"del row_ids","1359818a":"res = pd.concat([res0, res1, res2, res3])","98c1c102":"del res0, res1, res2, res3","9b50ec07":"submission = pd.read_csv('\/kaggle\/input\/ashrae-energy-prediction\/sample_submission.csv')\nsubmission = submission.merge(res, left_on='row_id', right_on='row_id', how='inner')\nsubmission = submission[['row_id', 'meter_reading_y']]\nsubmission.columns = ['row_id', 'meter_reading']\nsubmission.loc[submission['meter_reading']<0, 'meter_reading'] = 0\nsubmission.to_csv('submission.csv', index=False)\nsubmission","159081ee":"**I decided to split training dataset on 'meter' column and train 4 different LGBM models and check the results.**\n\n**It is just example of the process. I think after optimization of hyperparameters it should provide good results.**"}}