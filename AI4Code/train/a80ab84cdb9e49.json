{"cell_type":{"5fe24b4c":"code","ba6781ca":"code","40886ed6":"code","c1d05a11":"code","cde829c3":"code","4f4ae94f":"code","861c21e3":"code","6d57d69e":"code","ca491423":"code","fb7c1f76":"code","c6748975":"code","01f72813":"code","9f44ccdf":"code","e819215a":"code","6c0e8953":"code","3de78f1f":"code","fd588783":"code","6f3f6509":"code","1dd7c39d":"code","34949827":"code","2b5c799a":"code","dadfff9c":"code","44fd5375":"code","e7319b19":"code","e7dd4613":"code","e5468ff0":"code","346b7145":"code","dc410760":"code","7b50e8ac":"code","df885751":"code","4ba09c84":"code","9bd71fca":"code","78b35e3e":"code","a226d97c":"code","0454d8ca":"code","24f88f84":"code","2b2798ed":"code","d92abb54":"code","03bc1c44":"code","bece6724":"code","8a8a9fa8":"code","ac37b91e":"code","7fd2554e":"code","db37f891":"code","299a2501":"code","851add8c":"code","2638bb3e":"code","02be2f88":"code","a5db7c82":"code","a0f7f28f":"code","e50ed53d":"code","88f18963":"code","28f93091":"code","6ba2f50b":"code","5a2bd8f0":"code","d0d1a63f":"code","20c9132c":"code","55ad255d":"code","ca5ed23d":"code","e0cc6300":"code","f052ad48":"code","d5a585a3":"code","40ab0605":"code","43bcd88f":"code","871e26c7":"code","10ad7c75":"code","3fac92e7":"code","1115defe":"code","3d6a7ed6":"code","354be86d":"code","9bd2f7b1":"code","890f9ae7":"code","aac11686":"markdown","31991ede":"markdown","468c9d51":"markdown","24c3593a":"markdown","99bb0f5a":"markdown","854ae1e9":"markdown","d27a925e":"markdown","822e7fdb":"markdown","4d2436af":"markdown","f6ac95e8":"markdown","9617b12a":"markdown","3f81b721":"markdown","cb8ad3c0":"markdown","340f6a56":"markdown","98859180":"markdown","41c5de8f":"markdown","f54fe7e8":"markdown","7ef0ce73":"markdown","9ac4c27a":"markdown","071e7cb5":"markdown","8d42ac55":"markdown","b7408fda":"markdown","817cabb0":"markdown","f38dc7e5":"markdown","38d7e272":"markdown","c1343846":"markdown","b3a2534d":"markdown","ec38b82a":"markdown","eea65bf1":"markdown","eb8b833b":"markdown","fa3c1060":"markdown","1ef83417":"markdown","c7ab9ec5":"markdown","c76a6b69":"markdown","5c0108d2":"markdown","7a2bd596":"markdown","4623fe47":"markdown","e267c7fa":"markdown","9e2053e2":"markdown","12b0dc15":"markdown","688c9836":"markdown","d13ca509":"markdown","20d7d5d8":"markdown","f2d7e229":"markdown","f3e89985":"markdown","fdbd65b8":"markdown","7586bb21":"markdown","4377a4d2":"markdown","396cf468":"markdown","e4424b9d":"markdown","4362a81e":"markdown","47ff3f8f":"markdown","2c4da3cc":"markdown","2ea93451":"markdown","91d36805":"markdown","7f732450":"markdown","cf2039f2":"markdown","0865466c":"markdown","7b0830d9":"markdown","0ee6ee11":"markdown","d60898a2":"markdown","2d8cb218":"markdown","fee3f756":"markdown","794e0cd0":"markdown","f177a6d7":"markdown","4f882065":"markdown","ce210627":"markdown","cdc2781f":"markdown","64f92ad9":"markdown","44ee7f9d":"markdown"},"source":{"5fe24b4c":"%%HTML\n<style type=\"text\/css\">\ndiv.h1 {\n    background-color: #009C37; \n    color: white; padding: 8px; padding-right: 300px; font-size: 35px; max-width: 1500px; margin: auto; margin-top: 50px;}\ndiv.h2 {\n    background-color: #002277; \n    color: white; padding: 8px; padding-right: 300px; font-size: 25px; max-width: 1500px; margin: auto; margin-top: 50px;}\ndiv.h3 {\n    color: #002277;\n    font-size: 16px; margin-top: 20px; margin-bottom:4px;}\ndiv.h4 {\n    font-size: 15px; margin-top: 20px; margin-bottom: 8px;\nspan.note {\n    font-size: 5; color: #002277;\n    font-style: italic;}\nspan.captiona {\n    font-size: 5; color: dimgray; font-style: italic; margin-left: 130px; vertical-align: top;}\nhr {\n    display: block; color: #002277;\n    height: 1px; border: 0;  border-top: 1px solid;}\nhr.light {\n    display: block; color: lightgray; height: 1px; border: 0; border-top: 1px solid;}\n<\/style>","ba6781ca":"# Suppress warnings \nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nfrom IPython.display import HTML\nHTML('<iframe width=\"800\" height=\"500\" src=\"https:\/\/www.youtube.com\/embed\/zlzU0hytEks\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>')","40886ed6":"# Data processing, metrics and modeling\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom datetime import datetime\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc,precision_recall_curve\nfrom sklearn import metrics\n# Lgbm\nimport lightgbm as lgb\nimport catboost\nfrom catboost import Pool\nimport xgboost as xgb\n\n# Suppr warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")","c1d05a11":"# Import PyStackNet Package\n# Source: https:\/\/www.kaggle.com\/kirankunapuli\/pystacknet\nimport os\nimport sys\npackage_dir = \"\/kaggle\/input\/pystacknet\/h2oai-pystacknet-af571e0\"\nsys.path.append(package_dir)\nimport pystacknet","cde829c3":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nimport matplotlib.patches as patches\n\nfrom plotly import tools, subplots\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\npd.set_option('max_columns', 200)\npd.set_option('max_rows', 200)\nprint(os.listdir(\"..\/input\"))","4f4ae94f":"%%time\ndataset = pd.read_excel('\/kaggle\/input\/covid19\/dataset.xlsx')\ndataset.columns = [x.lower().strip().replace(' ','_') for x in dataset.columns]","861c21e3":"print('Size of the data', dataset.shape)","6d57d69e":"dataset.columns.values","ca491423":"dataset.head()","fb7c1f76":"%%time\ndataset.describe()","c6748975":"dataset['sars-cov-2_exam_result'] = dataset['sars-cov-2_exam_result'].replace(['negative','positive'], [0,1])\nsns.countplot(dataset['sars-cov-2_exam_result'])\n","01f72813":"print(\"There are {}% target values with 1\".format(100 * dataset['sars-cov-2_exam_result'].value_counts()[1]\/dataset.shape[0]))","9f44ccdf":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","e819215a":"%%time\nmissing_data(dataset)","6c0e8953":"import missingno as msno","3de78f1f":"msno.matrix(dataset.head(20000))","fd588783":"%%time\na = msno.heatmap(dataset, sort='ascending')\na","6f3f6509":"%%time\na2 = msno.dendrogram(dataset)\na2","1dd7c39d":"# Number of each type of column\ndataset.dtypes.value_counts()","34949827":"# Number of unique classes in each object column\ndataset.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","2b5c799a":"# Find correlations with the target and sort\ncorrelations = dataset.corr()['sars-cov-2_exam_result'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","dadfff9c":"%%time\nfeatures = dataset.columns.values[2:112]\ncorrs_ = dataset[features].corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrs_ = corrs_[corrs_['level_0'] != corrs_['level_1']]\ncorrs_.head(10)","44fd5375":"corrs_.head(10)","e7319b19":"corrs_.tail(10)","e7dd4613":"%%time\ncorrs = dataset.corr()\nplt.figure(figsize = (20, 8))\n# Heatmap of correlations\nsns.heatmap(corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = False, vmax = 0.8)\nplt.title('Clustermap');","e5468ff0":"def plot_dist_col(column):\n    pos__df = dataset[dataset['sars-cov-2_exam_result'] ==1]\n    neg__df = dataset[dataset['sars-cov-2_exam_result'] ==0]\n\n    '''plot dist curves for train and test weather data for the given column name'''\n    fig, ax = plt.subplots(figsize=(10, 10))\n    sns.distplot(pos__df[column].dropna(), color='green', ax=ax).set_title(column, fontsize=16)\n    sns.distplot(neg__df[column].dropna(), color='purple', ax=ax).set_title(column, fontsize=16)\n    plt.xlabel(column, fontsize=15)\n    plt.legend(['Positive sars cov 2 exam result', 'Negative sars cov 2 exam result'])\n    plt.show()\nplot_dist_col('patient_age_quantile')","346b7145":"%%time\nfig = px.pie( values=dataset.groupby(['sars-cov-2_exam_result']).size().values,names=dataset.groupby(['sars-cov-2_exam_result']).size().index)\nfig.update_layout(\n    font=dict(\n        size=15,\n        color=\"#FEE000\" ) )   \npy.iplot(fig)\n","dc410760":"#fill in mean for floats\nfor c in dataset.columns:\n    if dataset[c].dtype=='float16' or  dataset[c].dtype=='float32' or  dataset[c].dtype=='float64':\n        dataset[c].fillna(dataset[c].mean())\n\n#fill in -999 for categoricals\ndataset = dataset.fillna(-999)\n# Label Encoding\nfor f in dataset.columns:\n    if dataset[f].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(dataset[f].values))\n        dataset[f] = lbl.transform(list(dataset[f].values))\n        \nprint('Labelling done.')    ","7b50e8ac":"# Threshold for removing correlated variables\nthreshold = 0.92\n\n# Absolute value correlation matrix\ncorr_matrix = dataset.corr().abs()\ncorr_matrix.head()","df885751":"# Upper triangle of correlations\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nupper.head()","4ba09c84":"# Select columns with correlations above threshold\nto_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n\nprint('There are %d columns to remove.' % (len(to_drop)))\ndataset = dataset.drop(columns = to_drop)\nprint('Data shape: ', dataset.shape)\nprint('Size of the data', dataset.shape)","9bd71fca":"to_drop","78b35e3e":"## Dataset missing values (in percent)\ndataset_missing = (dataset.isnull().sum() \/ len(dataset)).sort_values(ascending = False)\ndataset_missing.head()","a226d97c":"# Identify missing values above threshold\ndataset_missing_ = dataset_missing.index[dataset_missing > 0.85]\n\nall_missing = list(set(dataset_missing_))\nprint('There are %d columns with more than 85%% missing values' % len(all_missing))\ndataset = dataset.drop(columns = all_missing)\nprint('Data shape: ', dataset.shape)","0454d8ca":"cat_features = [i for i in dataset.columns if str(dataset[i].dtype) in ['object', 'category']]\n\nif len(cat_features) > 0:\n    dataset[cat_features] = dataset[cat_features].astype('category')\n\n\ndf_lgb = dataset.copy()\nfor i in cat_features:\n    df_lgb[i] = dataset[i].cat.codes\n\ndf_lgb.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in df_lgb.columns]\n\ndataset_labels = df_lgb['sars_cov_2_exam_result']\ndf_lgb_ = df_lgb.copy()\ndf_lgb = df_lgb.drop(['patient_id', \n                 'sars_cov_2_exam_result', \n                 'patient_addmited_to_regular_ward__1_yes__0_no_', \n                 'patient_addmited_to_semi_intensive_unit__1_yes__0_no_', \n                 'patient_addmited_to_intensive_care_unit__1_yes__0_no_'\n                ], axis=1)\nx = df_lgb.copy()","24f88f84":"# Initialize an empty array to hold feature importances\nfeature_importances = np.zeros(df_lgb.shape[1])\n\n# Create the model with several hyperparameters\nmodel = lgb.LGBMClassifier(objective='binary', boosting_type = 'goss', n_estimators = 5000, class_weight = 'balanced')\n\n# Fit the model twice to avoid overfitting\nfor i in range(2):\n    # Split into training and validation set\n    dataset_features, valid_features, dataset_features_y, valid_y = train_test_split(x, dataset_labels, test_size = 0.20, random_state = i)\n    \n    # Train using early stopping\n    model.fit(dataset_features, dataset_features_y, early_stopping_rounds=100, eval_set = [(valid_features, valid_y)], \n              eval_metric = 'auc', verbose = 200)\n    \n    # Record the feature importances\n    feature_importances += model.feature_importances_","2b2798ed":"# Make sure to average feature importances! \nfeature_importances = feature_importances \/ 2\nfeature_importances = pd.DataFrame({'feature': list(df_lgb.columns), 'importance': feature_importances}).sort_values('importance', ascending = False)\nfeature_importances.head()","d92abb54":"# Find the features with zero importance\nzero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\nprint('There are %d features with 0.0 importance' % len(zero_features))\nfeature_importances.tail()","03bc1c44":"def plot_feature_importances(df, threshold = 0.9):\n    \"\"\"\n    Plots 15 most important features and the cumulative importance of features.\n    Prints the number of features needed to reach threshold cumulative importance.\n    \n    Parameters\n    --------\n    df : dataframe\n        Dataframe of feature importances. Columns must be feature and importance\n    threshold : float, default = 0.9\n        Threshold for prining information about cumulative importances\n        \n    Return\n    --------\n    df : dataframe\n        Dataframe ordered by feature importances with a normalized column (sums to 1)\n        and a cumulative importance column\n    \n    \"\"\"\n    \n    plt.rcParams['font.size'] = 18\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] \/ df['importance'].sum()\n    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    # Cumulative importance plot\n    plt.figure(figsize = (8, 6))\n    plt.plot(list(range(len(df))), df['cumulative_importance'], 'r-')\n    plt.xlabel('Number of Features'); plt.ylabel('Cumulative Importance'); \n    plt.title('Cumulative Feature Importance');\n    plt.show();\n    \n    importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n    print('%d features required for %0.2f of cumulative importance' % (importance_index + 1, threshold))\n    \n    return df","bece6724":"norm_feature_importances = plot_feature_importances(feature_importances)","8a8a9fa8":"df_lgb = df_lgb.drop(columns = zero_features)\nprint('Dataset shape: ', df_lgb.shape)\n","ac37b91e":"def identify_zero_importance_features(train, train_labels, iterations = 2):\n    \"\"\"\n    Identify zero importance features in a training dataset based on the \n    feature importances from a gradient boosting model. \n    \n    Parameters\n    --------\n    train : dataframe\n        Training features\n        \n    train_labels : np.array\n        Labels for training data\n        \n    iterations : integer, default = 2\n        Number of cross validation splits to use for determining feature importances\n    \"\"\"\n    \n    # Initialize an empty array to hold feature importances\n    feature_importances = np.zeros(train.shape[1])\n\n    # Create the model with several hyperparameters\n    model = lgb.LGBMClassifier(objective='binary', boosting_type = 'goss', n_estimators = 10000, class_weight = 'balanced')\n    \n    # Fit the model multiple times to avoid overfitting\n    for i in range(iterations):\n\n        # Split into training and validation set\n        train_features, valid_features, train_y, valid_y = train_test_split(train, train_labels, test_size = 0.25, random_state = i)\n\n        # Train using early stopping\n        model.fit(train_features, train_y, early_stopping_rounds=100, eval_set = [(valid_features, valid_y)], \n                  eval_metric = 'auc', verbose = 200)\n\n        # Record the feature importances\n        feature_importances += model.feature_importances_ \/ iterations\n    \n    feature_importances = pd.DataFrame({'feature': list(train.columns), 'importance': feature_importances}).sort_values('importance', ascending = False)\n    \n    # Find the features with zero importance\n    zero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\n    print('\\nThere are %d features with 0.0 importance' % len(zero_features))\n    \n    return zero_features, feature_importances","7fd2554e":"second_round_zero_features, feature_importances = identify_zero_importance_features(df_lgb, dataset_labels)","db37f891":"norm_feature_importances = plot_feature_importances(feature_importances, threshold = 0.95)","299a2501":"# # Threshold for cumulative importance\nthreshold = 0.95\n\n# Extract the features to keep\nfeatures_to_keep = list(norm_feature_importances[norm_feature_importances['cumulative_importance'] < threshold]['feature'])\nfeatures_to_keep.append('patient_addmited_to_intensive_care_unit__1_yes__0_no_')\nfeatures_to_keep.append( 'patient_addmited_to_regular_ward__1_yes__0_no_')\nfeatures_to_keep.append( 'patient_addmited_to_semi_intensive_unit__1_yes__0_no_')\nfeatures_to_keep.append( 'patient_id')\nfeatures_to_keep.append( 'sars_cov_2_exam_result')\n                        \n# Create new datasets with smaller features\ndataset_small = df_lgb_[features_to_keep]\n    ","851add8c":"import shap\n%time shap_values = shap.TreeExplainer(model).shap_values(valid_features)\nprint(shap_values[0].shape)","2638bb3e":"%time shap.summary_plot(shap_values, valid_features)","02be2f88":"# summarize the effects of top features\nshap.summary_plot(shap_values[0], valid_features, max_display=30)    ","a5db7c82":"print(\"Total columns to display:\", len(valid_features.columns))\nfor i in valid_features.columns:\n    shap.dependence_plot(i, shap_values[0], valid_features)\n    plt.show()","a0f7f28f":"train_df = dataset_small\nfeatures = list(train_df)\nfeatures.remove( 'patient_id')\nfeatures.remove( 'sars_cov_2_exam_result')\nfeatures.remove( 'patient_addmited_to_intensive_care_unit__1_yes__0_no_')\nfeatures.remove( 'patient_addmited_to_regular_ward__1_yes__0_no_')\nfeatures.remove( 'patient_addmited_to_semi_intensive_unit__1_yes__0_no_')\ntarget = 'sars_cov_2_exam_result'","e50ed53d":"# Confusion matrix \ndef plot_confusion_matrix(cm, classes,\n                          normalize = False,\n                          title = 'Confusion matrix\"',\n                          cmap = plt.cm.Blues) :\n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation = 0)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) :\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment = 'center',\n                 color = 'white' if cm[i, j] > thresh else 'black')\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","88f18963":"import itertools\nfrom scipy import interp\ndef gradient_boosting_model(params, folds, model='LGB',stack = False):    \n    print(str(model)+' modeling...')\n    start_time = timer(None)\n    plt.rcParams[\"axes.grid\"] = True\n    nfold = 5\n    skf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=2020)\n\n    oof = np.zeros(len(train_df))\n    mean_fpr = np.linspace(0,1,100)\n    cms= []\n    tprs = []\n    aucs = []\n    y_real = []\n    y_proba = []\n    recalls = []\n    roc_aucs = []\n    f1_scores = []\n    accuracies = []\n    precisions = []\n    feature_importance_df = pd.DataFrame()\n\n    i = 1\n    for train_idx, valid_idx in skf.split(train_df, train_df['sars_cov_2_exam_result'].values):\n        print(\"\\nfold {}\".format(i))\n            \n        if model == 'XGB':\n            trn_data = xgb.DMatrix(train_df.iloc[train_idx][features], \n                                   label=train_df.iloc[train_idx][target].values)\n            val_data = xgb.DMatrix(train_df.iloc[valid_idx][features], \n                                   label=train_df.iloc[valid_idx][target].values)\n\n            watchlist = [(trn_data, 'train'), (val_data, 'valid')]\n\n            clf = xgb.train(params, dtrain = trn_data, evals=watchlist, early_stopping_rounds=100, maximize=True, verbose_eval=100)\n            oof[valid_idx] = clf.predict(val_data, ntree_limit=clf.best_ntree_limit)\n\n            \n        # Scores \n        roc_aucs.append(roc_auc_score(train_df.iloc[valid_idx][target].values, oof[valid_idx]))\n        accuracies.append(accuracy_score(train_df.iloc[valid_idx][target].values, oof[valid_idx].round()))\n        recalls.append(recall_score(train_df.iloc[valid_idx][target].values, oof[valid_idx].round()))\n        precisions.append(precision_score(train_df.iloc[valid_idx][target].values, oof[valid_idx].round()))\n        f1_scores.append(f1_score(train_df.iloc[valid_idx][target].values, oof[valid_idx].round()))\n\n        # Roc curve by folds\n        f = plt.figure(1)\n        fpr, tpr, t = roc_curve(train_df.iloc[valid_idx][target].values, oof[valid_idx])\n        tprs.append(interp(mean_fpr, fpr, tpr))\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc)\n        plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.4f)' % (i,roc_auc))\n\n        # Precion recall by folds\n        g = plt.figure(2)\n        precision, recall, _ = precision_recall_curve(train_df.iloc[valid_idx][target].values, oof[valid_idx])\n        y_real.append(train_df.iloc[valid_idx][target].values)\n        y_proba.append(oof[valid_idx])\n        plt.plot(recall, precision, lw=2, alpha=0.3, label='P|R fold %d' % (i))  \n\n        i= i+1\n        \n        # Confusion matrix by folds\n        cms.append(confusion_matrix(train_df.iloc[valid_idx][target].values, oof[valid_idx].round()))\n        \n\n    # Metrics\n    print(\n            '\\nCV roc score        : {0:.4f}, std: {1:.4f}.'.format(np.mean(roc_aucs), np.std(roc_aucs)),\n            '\\nCV accuracy score   : {0:.4f}, std: {1:.4f}.'.format(np.mean(accuracies), np.std(accuracies)),\n            '\\nCV recall score     : {0:.4f}, std: {1:.4f}.'.format(np.mean(recalls), np.std(recalls)),\n            '\\nCV precision score  : {0:.4f}, std: {1:.4f}.'.format(np.mean(precisions), np.std(precisions)),\n            '\\nCV f1 score         : {0:.4f}, std: {1:.4f}.'.format(np.mean(f1_scores), np.std(f1_scores))\n    )\n    \n    # Roc plt\n    f = plt.figure(1)\n    plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'grey')\n    mean_tpr = np.mean(tprs, axis=0)\n    mean_auc = auc(mean_fpr, mean_tpr)\n    plt.plot(mean_fpr, mean_tpr, color='blue',\n             label=r'Mean ROC (AUC = %0.4f)' % (np.mean(roc_aucs)),lw=2, alpha=1)\n\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(str(model)+' ROC curve by folds')\n    plt.legend(loc=\"lower right\")\n    \n    # PR plt\n    g = plt.figure(2)\n    plt.plot([0,1],[1,0],linestyle = '--',lw = 2,color = 'grey')\n    y_real = np.concatenate(y_real)\n    y_proba = np.concatenate(y_proba)\n    precision, recall, _ = precision_recall_curve(y_real, y_proba)\n    plt.plot(recall, precision, color='blue',\n             label=r'Mean P|R')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title(str(model)+' P|R curve by folds')\n    plt.legend(loc=\"lower left\")\n\n    # Confusion maxtrix\n    plt.rcParams[\"axes.grid\"] = False\n    cm = np.average(cms, axis=0)\n    class_names = [0,1]\n    plt.figure()\n    plot_confusion_matrix(cm, \n                          classes=class_names, \n                          title= str(model).title()+' Confusion matrix [averaged\/folds]')\n    \n    # Feat imp plt\n    if model != 'XGB':\n        cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n            .groupby(\"Feature\")\n            .mean()\n            .sort_values(by=\"importance\", ascending=False)[:30].index)\n        best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\n        plt.figure(figsize=(10,10))\n        sns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False),\n                edgecolor=('white'), linewidth=2, palette=\"rocket\")\n        plt.title(str(model)+' Features importance (averaged\/folds)', fontsize=18)\n        plt.tight_layout()\n        \n    # Timer end    \n    timer(start_time)\n    \n#Timer\ndef timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('Time taken for Modeling: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))","28f93091":"param_xgb = {\n            'n_jobs' : -1, 'n_estimators' : 500, 'seed' : 1919,\n            'random_state':2020, 'eval_metric':'auc' }","6ba2f50b":"gradient_boosting_model(param_xgb, 5, 'XGB')","5a2bd8f0":"# Machine Learning\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor","d0d1a63f":"# Split Train and Validation\nX_train = train_df.drop('patient_id',axis=1)\nX_train = X_train.drop(['sars_cov_2_exam_result','patient_addmited_to_intensive_care_unit__1_yes__0_no_','patient_addmited_to_regular_ward__1_yes__0_no_','patient_addmited_to_semi_intensive_unit__1_yes__0_no_'],axis=1)\ntarget = train_df['sars_cov_2_exam_result']\n\nX_train, X_val, y_train, y_val = train_test_split(X_train,\n                                                  target,\n                                                  test_size=0.30, \n                                                  random_state=2020, \n                                                  stratify=target)","20c9132c":"# Level 1 are the base models that take the training dataset as input\n\nl1_clf1 = LGBMRegressor(boosting_type='gbdt',\n                        objective=\"binary\",\n                        metric=\"AUC\",\n                        boost_from_average=\"false\",\n                        learning_rate=0.0045,\n                        num_leaves=491,\n                        max_depth=20,\n                        min_child_weight=0.035,\n                        feature_fraction=0.38,\n                        bagging_fraction=0.42,\n                        min_data_in_leaf=100,\n                        max_bin=255,\n                        importance_type='split',\n                        reg_alpha=0.4,\n                        reg_lambda=0.65,\n                        bagging_seed=2020,\n                        random_state=2020,\n                        verbosity=-1,\n                        subsample=0.85,\n                        colsample_bytree=0.8,\n                        min_child_samples=79)\n\nl1_clf3 = CatBoostRegressor(learning_rate=0.1,\n                            bagging_temperature=0.1, \n                            l2_leaf_reg=30,\n                            depth=12, \n                            max_bin=255,\n                            iterations=100,\n                            loss_function='Logloss',\n                            objective='RMSE',\n                            eval_metric=\"AUC\",\n                            bootstrap_type='Bayesian',\n                            random_seed=2020,\n                            early_stopping_rounds=10)\n\n\n# Level 2 models will take predictions from level 1 models as input\n# Remember to keep level 2 models smaller\n# Basic models like Ridge Regression with large regularization or small random forests work well\nl2_clf1 = RandomForestRegressor(n_estimators=250, \n                                max_depth=5, \n                                max_features='sqrt', \n                                random_state=2020)","55ad255d":"# Specify model tree for StackNet\nmodels = [[l1_clf1, l1_clf3], # Level 1\n          [l2_clf1]] # Level 2","ca5ed23d":"from pystacknet.pystacknet import StackNetClassifier\n\n# Specify parameters for stacked model and begin training\nmodel = StackNetClassifier(models, \n                           metric=\"auc\", \n                           folds=5,\n                           restacking=False,\n                           use_retraining=True,\n                           use_proba=True, # To use predict_proba after training\n                           random_state=2020,\n                           n_jobs=-1, \n                           verbose=1)\n\n# Fit the entire model tree\nmodel.fit(X_train, y_train)\n","e0cc6300":"# source: https:\/\/www.kaggle.com\/carlolepelaars\/ensembling-with-stacknet\/notebook\ndef auc_score(y_true, y_pred):\n    \"\"\"\n    Calculates the Area Under ROC Curve (AUC)\n    \"\"\"\n    return roc_auc_score(y_true, y_pred)\ndef plot_curve(y_true_train, y_pred_train, y_true_val, y_pred_val, model_name):\n    \"\"\"\n    Plots the ROC Curve given predictions and labels\n    \"\"\"\n    fpr_train, tpr_train, _ = roc_curve(y_true_train, y_pred_train, pos_label=1)\n    fpr_val, tpr_val, _ = roc_curve(y_true_val, y_pred_val, pos_label=1)\n    plt.figure(figsize=(8, 8))\n    plt.plot(fpr_train, tpr_train, color='black',\n             lw=2, label=f\"ROC train curve (AUC = {round(roc_auc_score(y_true_train, y_pred_train), 4)})\")\n    plt.plot(fpr_val, tpr_val, color='darkorange',\n             lw=2, label=f\"ROC validation curve (AUC = {round(roc_auc_score(y_true_val, y_pred_val), 4)})\")\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([-0.05, 1.05])\n    plt.ylim([-0.05, 1.05])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.xticks(fontsize=14)\n    plt.yticks(fontsize=14)\n    plt.title(f'ROC Plot for {model_name}', weight=\"bold\", fontsize=20)\n    plt.legend(loc=\"lower right\", fontsize=16)","f052ad48":"# Get score on training set and validation set for our StackNetClassifier\ntrain_preds = model.predict_proba(X_train)[:, 1]\nval_preds = model.predict_proba(X_val)[:, 1]\ntrain_score = auc_score(y_train, train_preds)\nval_score = auc_score(y_val, val_preds)","d5a585a3":"print(f\"StackNet AUC on training set: {round(train_score, 4)}\")\nprint(f\"StackNet AUC on validation set: {round(val_score, 4)}\")","40ab0605":"# Plot ROC curve\nplot_curve(y_train, train_preds, y_val, val_preds, \"StackNet Baseline\")","43bcd88f":"df_cov19pos = pd.DataFrame(val_preds, columns=['prob_sars-cov-2_exam_result'])\ndf_cov19pos['sars-cov-2_exam_result'] = y_val.values\ndf_cov19pos['patient_id'] = X_val.index\ndf_cov19pos.head()","871e26c7":"from sklearn.metrics import classification_report\ntrain_cm = confusion_matrix(y_train,train_preds.round())\nprint('Confusion matrix: \\n',train_cm)\nprint('Classification report: \\n',classification_report(y_train, train_preds.round()))","10ad7c75":"# visualize with seaborn library\nsns.heatmap(train_cm,annot=True,fmt=\"d\") \nplt.show()","3fac92e7":"dataset['sars-cov-2_exam_result'].value_counts() \/ dataset.shape[0]\n#  0.098866% of the cases are Positive for SarsCov2 Exam","1115defe":"grouped = df_cov19pos.groupby('prob_sars-cov-2_exam_result').sum().reset_index()\ngrouped.sort_values('prob_sars-cov-2_exam_result', ascending=False)","3d6a7ed6":"HTML('<iframe width=\"800\" height=\"500\" src=\"https:\/\/www.youtube.com\/embed\/HXmt0j1gtDU\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>')","354be86d":"HTML('<iframe width=\"800\" height=\"500\" src=\"https:\/\/www.youtube.com\/embed\/zF2pXXJIAGM\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>')","9bd2f7b1":"HTML('<iframe width=\"800\" height=\"500\" src=\"https:\/\/www.youtube.com\/embed\/7jHgS4yxS0A\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>')","890f9ae7":"HTML('<iframe width=\"800\" height=\"500\" src=\"https:\/\/www.youtube.com\/embed\/2DAIe1SlLMo\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>')","aac11686":"<a id='t2_2'><\/a>\n# <div class=\"h3\"> Examine Missing Values<\/div>\n[Next](#t2_3)\n\nNext we can look at the number and percentage of missing values in each column","31991ede":"<a id='t2'><\/a>\n# <div class=\"h3\"> Examine the Distribution of the Target Column<\/div>\n[Next](#t2_2)\n","468c9d51":"<a id='t6_2'><\/a>\n# <div class=\"h2\">Gradient Boosting Model function<\/div>\n\n[Back to Contents](#top)\n\n[Next](#t6_3)\n","24c3593a":"<a id='t3'><\/a>\n# <div class=\"h1\">Visualizations<\/div>\n\n[Back to Contents](#top)\n\n[Next](#t4)\n","99bb0f5a":"<a id='top'><\/a>\n<div class=\"h1\">Contents<\/div>\n\n1. [Glimpse of Data](#t1)\n\n    1.1 [Imports](#t1_1)\n    1.2 [Read in Data](#t1_2)\n\n2. [Exploratory Data Analysis](#t2)\n    2.1 [Examine the Distribution of the Target Column](#t2_1)\n    2.2 [Examine Missing Values](#t2_2)\n    2.3 [Handling missing values](#t2_3)\n    2.4 [Column Types](#t2_4)\n    2.5 [Correlations](#t2_5)\n    \n3. [Visualizations](#t3)\n\n4. [Encoding Variables](#t4)\n    4.1 [Label Encoding](#t4_1)\n\n5. [Feature Selection](#t5)\n    5.1 [Remove Collinear features](#t5_1)\n    5.2 [Remove features with greater than a threshold percentage of missing values](#t5_2)\n    5.3 [Feature Selection through Feature Importances](#t5_3)\n    5.4 [LGB Shap Values](#t5_4)\n\n6. [Modeling](#t6)\n    6.1 [Confusion Matrix function](#t6_1)\n    6.2 [Gradient Boosting Model function](#t6_2)\n    6.3 [XGB](#t6_3)\n\n7. [Ensembling With StackNet](#t7)\n    7.1 [StackNet Modeling](#t7_1)\n    7.1 [Evaluation StackNet ROC_AUC](#t7_2)\n\n8. [General Findings](#t8)\n\n9. [End Notebooks](#end)\n","854ae1e9":"<a id='t8'><\/a>\n# <div class=\"h1\">General Findings<\/div>\n\n[Back to Contents](#top)\n\n[End Notebook](#end)\n","d27a925e":"It also looks like many of the features we made have literally 0 importance. For the gradient boosting machine, features with 0 importance are not used at all to make any splits. Therefore, we can remove these features from the model with no effect on performance.\n\nLet's remove the features that have zero importance.","822e7fdb":"Specify model tree for StackNet","4d2436af":"Level 1 are the base models that take the training dataset as input","f6ac95e8":"<a id='t5_4'><\/a>\n# <div class=\"h2\">Shap values LGB<\/div>\n\n[Back to Contents](#top)\n\n[Next](#t6)\n\nguidelines:[LightGBM Shap](https:\/\/www.kaggle.com\/rspadim\/eda-first-try-python-lgb-shap\/notebook), [LightGBM Predictions Explained with SHAP ](https:\/\/www.kaggle.com\/hmendonca\/lightgbm-predictions-explained-with-shap-0-796)\n","9617b12a":"<a id='t1_1'><\/a>\n# <div class=\"h3\"> Imports<\/div>\n[Next](#t1_2)\n\nWe are using a typical data science stack: ``numpy``, ``pandas``, ``sklearn``, ``matplotlib``.","3f81b721":"dataset head","cb8ad3c0":"<a id='t2'><\/a>\n# <div class=\"h1\"> Exploratory Data Analysis<\/div>\n\n[Back to Contents](#top)\n\n[Next](#t2_1)\n\n\nExploratory Data Analysis (EDA) is an open-ended process where we calculate statistics and make figures to find trends, anomalies, patterns, or relationships within the data.","340f6a56":"<a id='t6'><\/a>\n# <div class=\"h1\">Modeling<\/div>\n\n[Back to Contents](#top)\n\n[Next](#t6_1)\n","98859180":"### Useful links: \n- [Classification: Precision and Recall](https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/precision-and-recall)\n- [Classification: True vs. False and Positive vs. Negative](https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/true-false-positive-negative)\n- [What is LightGBM, How to implement it? How to fine tune the parameters?](https:\/\/towardsdatascience.com\/boosting-techniques-in-python-predicting-hotel-cancellations-62b7a76ffa6c)\n- [XGBoost and Imbalanced Classes](https:\/\/towardsdatascience.com\/boosting-techniques-in-python-predicting-hotel-cancellations-62b7a76ffa6c)\n <hr>","41c5de8f":"#### Initialize an empty array to hold feature importances","f54fe7e8":"<div class=\"h2\">Task Details<\/div>\nPredict confirmed **COVID-19** cases among suspected cases.\nBased on the results of laboratory tests commonly collected for a suspected **COVID-19** case during a visit to the emergency room, would it be possible to predict the test result for **SARS-Cov-2** (positive\/negative)?","7ef0ce73":"Transform the target variable into numeric","9ac4c27a":"#### Find the features with zero importance","071e7cb5":"probabilities of sarscov2 exam result","8d42ac55":"<div class=\"h3\"> Import PyStackNet Package<\/div>","b7408fda":"Number of unique classes in each object column","817cabb0":"<a id='t7_3'><\/a>\n# <div class=\"h2\">Evaluation StackNet ROC_AUC<\/div>\n\n[Back to Contents](#top)\n\n[Next](#t8)\n\n\nThe blue line signifies the baseline AUC which is 0.5. The final validation score is the area under the orange curve, which is mentioned in the plot","f38dc7e5":"<a id='t2_3'><\/a>\n# <div class=\"h3\">Handling missing values<\/div>\n[Next](#t2_4)\n\nPackage called missingno (https:\/\/github.com\/ResidentMario\/missingno)\n !pip install quilt","38d7e272":"<a class=\"anchor\" id=\"end\"><\/a>\n<a id='end'><\/a>\n# <div class=\"h1\"> End Notebook<\/div>\n[Back to Contents](#top)","c1343846":"The correlation between the features is very small.","b3a2534d":"#### Threshold for cumulative importance\n","ec38b82a":"<a id='t1_2'><\/a>\n# <div class=\"h3\"> Read in Data<\/div>\n[Next](#t2)","eea65bf1":"Correlations clustermap\n","eb8b833b":"<a id='t5_3'><\/a>\n# <div class=\"h2\">Feature Selection through Feature Importances<\/div>\n\n[Back to Contents](#top)\n\n[Next](#t5_4)\n\nTree-based models (and consequently ensembles of trees) can determine an \"importance\" for each feature by measuring the reduction in impurity for including the feature in the model.\nSince the LightGBM model does not need missing values to be imputed, we can directly fit on the training data. We will use Early Stopping to determine the optimal number of iterations and run the model twice, averaging the feature importances to try and avoid overfitting to a certain set of features.\n\nguidelines: [Feature Selection by @willkoehrsen](https:\/\/www.kaggle.com\/willkoehrsen\/introduction-to-feature-selection#Introduction:-Feature-Selection)","fa3c1060":"#### Classification report train","1ef83417":"###  9.8866% of the cases are Positive for SarsCov2 Exam","c7ab9ec5":"<a id='t5_2'><\/a>\n# <div class=\"h2\">Remove Missing Values<\/div>\n\n[Back to Contents](#top)\n\n[Next](#t5_3)\n\nIn this implementation, if any columns have greater than 85% missing values, they will be removed.\n\nguidelines:[LightGBM Advanced Topics](https:\/\/github.com\/Microsoft\/LightGBM\/blob\/master\/docs\/Advanced-Topics.rst)\n","c76a6b69":"<div class=\"h2\">Diagnosis of COVID-19 and its clinical spectrum<\/div>\n### AI and Data Science supporting clinical decisions (from 28th Mar to 1st Apr)\n\n![](https:\/\/img.olhardigital.com.br\/uploads\/acervo_imagens\/2020\/03\/r16x9\/20200320062922_1200_675_-_coronavirus_brasil_arte.jpg)\n[source](https:\/\/img.olhardigital.com.br\/uploads\/acervo_imagens\/2020\/03\/r16x9\/20200320062922_1200_675_-_coronavirus_brasil_arte.jpg)\n\n<div class=\"h3\">Background<\/div>\nThe World Health Organization (WHO) characterized the COVID-19, caused by the SARS-CoV-2, as a pandemic on March 11, while the exponential increase in the number of cases was risking to overwhelm health systems around the world with a demand for ICU beds far above the existing capacity, with regions of Italy being prominent examples.\n\nBrazil recorded the first case of SARS-CoV-2 on February 26, and the virus transmission evolved from imported cases only, to local and finally community transmission very rapidly, with the federal government declaring nationwide community transmission on March 20.\n\n**Until March 27**, the state of S\u00e3o Paulo had recorded 1,223 confirmed cases of COVID-19, with 68 related deaths, while the county of S\u00e3o Paulo, with a population of approximately 12 million people and where Hospital Israelita Albert Einstein is located, had 477 confirmed cases and 30 associated death, as of March 23. Both the state and the county of S\u00e3o Paulo decided to establish quarantine and social distancing measures, that will be enforced at least until early April, in an effort to slow the virus spread.\n\nOne of the motivations for this challenge is the fact that in the context of an overwhelmed health system with the possible limitation to perform tests for the detection of SARS-CoV-2, testing every case would be impractical and tests results could be delayed even if only a target subpopulation would be tested.\n\n<div class=\"h3\">Dataset<\/div>\nThis dataset contains anonymized data from patients seen at the [Hospital Israelita Albert Einstein](https:\/\/www.einstein.br\/Pages\/Home.aspx), at S\u00e3o Paulo, Brazil, and who had samples collected to perform the SARS-CoV-2 RT-PCR and additional laboratory tests during a visit to the hospital.\n\nAll data were anonymized following the best international practices and recommendations. All clinical data were standardized to have a mean of zero and a unit standard deviation.","5c0108d2":"Let's see also the least correlated features.","7a2bd596":"<a id='t6_1'><\/a>\n# <div class=\"h2\">Confusion Matrix function<\/div>\n\n[Back to Contents](#top)\n\n[Next](#t6_2)\n","4623fe47":"**Heatmap**. The missingno correlation heatmap measures nullity correlation: how strongly the presence or absence of one variable affects the presence of another:","e267c7fa":"A situation in Brazil can be better #StayHome","9e2053e2":"**Dendrogram**.The dendrogram allows you to more fully correlate variable completion, revealing trends deeper than the pairwise ones visible in the correlation heatmap:","12b0dc15":"Let's look to the top most correlated features, besides the same feature pairs.","688c9836":"#### Split Train and Validation","d13ca509":"The model is compiled and fitted through the a familiar sklearn-like API. The StackNetClassifier will perform cross-validation (CV) and will output the CV scores for each model. To make sure we can output a probability of pacient result for sarscov2 exam result we specify \"use_proba=True\".","20d7d5d8":"<a id='t4_1'><\/a>\n# <div class=\"h2\">Label Encoding<\/div>\n\n[Back to Contents](#top)\n\n[Next](#t5)\n\nLabel encoding assigns each unique value to a different integer.\n![](https:\/\/raw.githubusercontent.com\/WillKoehrsen\/Machine-Learning-Projects\/master\/label_encoding.png)\n\nThis approach assumes an ordering of the categories: \"Never\" (0) < \"Rarely\" (1) < \"Most days\" (2) < \"Every day\" (3).\nThis assumption makes sense in this example, because there is an indisputable ranking to the categories. Not all categorical variables have a clear ordering in the values, but we refer to those that do as ordinal variables. For tree-based models (like decision trees and random forests), you can expect label encoding to work well with ordinal variables.","f2d7e229":"Live Coronavirus from 03\/20 - What Brazil needs to do in the coming days","f3e89985":"### Get score on training set and validation set for our StackNetClassifier","fdbd65b8":"dataset columns values","7586bb21":"<a id='t6_3'><\/a>\n# <div class=\"h2\">XGBoost<\/div>\n\n[Back to Contents](#top)\n\n[Next](#t6_4)\n","4377a4d2":"<a id='t7_2'><\/a>\n# <div class=\"h2\">StackNet Modeling<\/div>\n\n[Back to Contents](#top)\n\n[Next](#t7_3)\n \nStackNet allows you to define all kinds of models. For example, Sklearn models, LightGBM, XGBoost, CatBoost and Keras models can all be used with StackNet.For the individual models, you are responsible for not overfitting.","396cf468":"#### summarize the effects of top features","e4424b9d":"% of SARS COV2 exam result (0=Negative, 1=Positive)","4362a81e":"#### Average feature importances! ","47ff3f8f":"Applying this on the entire dataset results in **64** collinear features removed.","2c4da3cc":"Patient age quantile by sars cov2 exam result","2ea93451":"<a id='t7'><\/a>\n# <div class=\"h1\">Ensembling With StackNet<\/div>\n\n[Back to Contents](#top)\n\n[Next](#t7_1)\n![](https:\/\/github.com\/kaz-Anova\/StackNet\/raw\/master\/images\/StackNet_Logo.png?raw=true)\nStackNet was created by Kaggle Grandmaster Marios Michailidis ([kazanova](https:\/\/www.kaggle.com\/kazanova)) as part of his PhD. Thanks to [Kiran Kunapuli](https:\/\/www.kaggle.com\/kirankunapuli) for uploading the package as a Kaggle dataset so it can conveniently be used with Kaggle kernels.","91d36805":"Live Coronavirus from 03\/22 - Why is it important to stay home?","7f732450":"<a id='t5_1'><\/a>\n# <div class=\"h2\">Remove Collinear Variables<\/div>\n\n[Back to Contents](#top)\n\n[Next](#t5_2)\n\nCollinear variables are those which are highly correlated with one another. These can decrease the model's availablility to learn, decrease model interpretability, and decrease generalization performance on the test set. \n\nguidelines:[Drop Highly Correlated Features](https:\/\/chrisalbon.com\/machine_learning\/feature_selection\/drop_highly_correlated_features\/)\n","cf2039f2":"<a id='t4'><\/a>\n# <div class=\"h1\">Encoding Variables<\/div>\n\n[Back to Contents](#top)\n\n[Next](#t5)\n\nBefore we go any further, we need to deal with pesky categorical variables. A machine learning model unfortunately cannot deal with categorical variables (except for some models such as [LightGBM](Before we go any further, we need to deal with pesky categorical variables. A machine learning model unfortunately cannot deal with categorical variables (except for some models such as LightGBM. Therefore, we have to find a way to encode (represent) these variables as numbers before handing them off to the model. There are two main ways to carry out this process). Therefore, we have to find a way to encode (represent) these variables as numbers before handing them off to the model. There are two main ways to carry out this process.\nYou can see this kaggle course: [Intermediate Machine Learning Home Page](https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables)","0865466c":"Number of each type of column","7b0830d9":"We calculate now the correlations between the features in dataset.\nThe following table shows the first 10 the least correlated features.","0ee6ee11":"<a id='t2_5'><\/a>\n# <div class=\"h3\">Correlations<\/div>\n[Next](#t3)\n\n\nNow that we have dealt with the categorical variables and the outliers, let's continue with the EDA. One way to try and understand the data is by looking for correlations between the features and the target. We can calculate the Pearson correlation coefficient between every variable and the target using the .corr dataframe method.\n\nThe correlation coefficient is not the greatest method to represent \"relevance\" of a feature, but it does give us an idea of possible relationships within the data. Some [general interpretations of the absolute value of the correlation coefficent are](http:\/\/www.statstutor.ac.uk\/resources\/uploaded\/pearsons.pdf):\n\n- .00-.19 <b>\u201cvery weak\u201d<\/b>\n- .20-.39 <b>\u201cweak\u201d<\/b>\n- .40-.59 <b>\u201cmoderate\u201d<\/b>\n- .60-.79 <b>\u201cstrong\u201d<\/b>\n- .80-1.0 <b>\u201cvery strong\u201d<\/b>","d60898a2":"Live Coronavirus of 03\/18 - How the pandemic can happen in Brazil","2d8cb218":"<a id='t2_4'><\/a>\n# <div class=\"h3\">Column Types<\/div>\n[Next](#t2_5)\n\nLet's look at the number of columns of each data type. int64 and float64 are numeric variables [(which can be either discrete or continuous)](https:\/\/stats.stackexchange.com\/questions\/206\/what-is-the-difference-between-discrete-data-and-continuous-data). object columns contain strings and are [categorical features](https:\/\/stats.stackexchange.com\/questions\/206\/what-is-the-difference-between-discrete-data-and-continuous-data). ","fee3f756":"# Determining the Threshold for sars-cov2 exam positive result \n\nIt is necessary to define a value to help the model make decisions for sars-cov2 exam positive result.\nTo find the ideal threshold that classifies Positive and Negative  sars-cov2 exam.  It's elaborated the heuristic,\n, first, we found the proportion **p** of cases that were for sars-cov2 exam positive result in the entire dataset, **~9.99%**, the final result its the highest prob proportion correspondent. The central idea is that the model should output positive (sars-cov-2_exam_result Positive) cases in the same proportion as **p**.","794e0cd0":"<a id='t5'><\/a>\n# <div class=\"h1\">Feature Selection<\/div>\n\n[Back to Contents](#top)\n\n[Next](#t5_1)\n\nFind the optimal feature subset using an evaluation measure. The choice of evaluation metric distinguish the three main strategies of feature selection algorithms: the wrapper strategy, the filter strategy, and the embedded strategy\n\nFilter methods:\n   - information gain\n   - chi-square test\n   - correlation coefficient\n   - variance threshold\n\nWrapper methods:\n   - recursive feature elimination\n   - sequential feature selection algorithms\n\nEmbedded methods:\n  - L1 (LASSO) regularization\n   - decision tree\n\nIn our case, we remove some useless, redundant variables.\nWe will use three methods for feature selection: Remove collinear features, remove features with greater than a threshold percentage of missing values, keep only the most relevant features using feature importances from a model","f177a6d7":"# Acknowledgments\n[XGB | LGB | CB](https:\/\/www.kaggle.com\/vincentlugat\/fraud-detection-xgb-lgb-cb\/) by @vincentlugat\n[LigthGBM Simple fe](https:\/\/www.kaggle.com\/caesarlupum\/ashrae-ligthgbm-simple-fe) by @caesarlupum\n[Ensembling With Stacknet](https:\/\/www.kaggle.com\/carlolepelaars\/ensembling-with-stacknet\/notebook) by @carlolepelaars\n[Stacknet on gpu lgb xgb cb](https:\/\/www.kaggle.com\/kirankunapuli\/ieee-fraud-stacknet-on-gpu-lgb-xgb-cb\/notebook) by @kirankunapuli\n[eda and prediction](https:\/\/www.kaggle.com\/gpreda\/santander-eda-and-prediction) by @gpreda\n[Introduction to Feature Selection](https:\/\/www.kaggle.com\/willkoehrsen\/introduction-to-feature-selection#Introduction:-Feature-Selection) by @willkoehrsen\n[EDA  lgb shap](https:\/\/www.kaggle.com\/rspadim\/eda-first-try-python-lgb-shap) by @rspadim\n\n<div class=\"h3\">Remember the upvote button is next to the fork button, and it's free too! ;) Don't hesitate to give your suggestions in the comment section, It motivates me to produce more quality content :)\n<\/div>","4f882065":"<div class=\"h3\"> Heroes in Action<\/div>\nCoronavirus: The routine of doctors at Albert Einstein Hospital in the fight against the advance of Covid-19","ce210627":"Specify parameters for stacked model and begin training\n","cdc2781f":"<a id='t1'><\/a>\n# <div class=\"h1\"> Glimpse of Data<\/div>\n[Back to Contents](#top)\n\n[Next](#t1_1)\n","64f92ad9":"At this point, we can re-run the model to see if it identifies any more features with zero importance. In a way, we are implementing our own form of recursive feature elimination. Since we are repeating work, we should probably put the zero feature importance identification code in a function.","44ee7f9d":"**Nullity Matrix**.\nThe msno.matrix nullity matrix is a data-dense display which lets you quickly visually analyse data completion."}}