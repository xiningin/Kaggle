{"cell_type":{"81b62d45":"code","456d43ba":"code","9294eef3":"code","192983ea":"code","900e827a":"code","35f4827f":"code","ef3d566e":"code","c0f4d620":"code","44062bae":"code","f3500e26":"code","43f961be":"code","b4a52421":"markdown","5e87c154":"markdown","58a9fd7e":"markdown","0d689a59":"markdown","a73d7e68":"markdown","b7d81336":"markdown"},"source":{"81b62d45":"import torch\nimport torch.nn as nn\nimport torch.autograd as autograd\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","456d43ba":"# 2 words to the left, 2 to the right\nCONTEXT_SIZE = 2\nEMBEDDING_SIZE = 10","9294eef3":"# The below CBOW implementation is based on a gist by xingjunjie (github @gavinxing), which in turn was based off of a template from http:\/\/pytorch.org\/tutorials\/beginner\/nlp\/word_embeddings_tutorial.html.\nclass CBOW(nn.Module):\n    def __init__(self, context_size=2, embedding_size=100, vocab_size=None):\n        super(CBOW, self).__init__()\n        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n        self.linear1 = nn.Linear(embedding_size, vocab_size)\n\n    def forward(self, inputs):\n        lookup_embeds = self.embeddings(inputs)\n        embeds = lookup_embeds.sum(dim=0)\n        out = self.linear1(embeds)\n        return out\n    \ndef make_context_vector(context, word_to_ix):\n    idxs = [word_to_ix[w] for w in context]\n    tensor = torch.LongTensor(idxs)\n    return autograd.Variable(tensor)","192983ea":"# Part of our loss function will be a penalty term.\n# Pearson Correlation requires us to calculate Covariance:\ndef cov(m, y=None):\n    if y is not None:\n        m = torch.cat((m, y), dim=0)\n    m_exp = torch.mean(m, dim=1)\n    x = m - m_exp[:, None]\n    cov = 1 \/ (x.size(1) - 1) * x.mm(x.t())\n    return cov\n\ndef pcor(m, y=None):\n    x = cov(m, y)\n    \n    # Not interested in positive nor negative correlations; \n    # We're only interested in correlation magnitudes:\n    x = (x*x).sqrt() # no negs\n    \n    stddev = x.diag().sqrt()\n    x = x \/ stddev[:, None]\n    x = x \/ stddev[None, :]\n    return x\n\ndef pcor_err(m, y=None):\n    # Every var is correlated with itself, so subtract that:\n    x = (pcor(m, y) - torch.eye(m.size(0)))\n    return x.mean(dim=0).mean()","900e827a":"raw_text = \"\"\"We are about to study the idea of a computational process.\nComputational processes are abstract beings that inhabit computers.\nAs they evolve, processes manipulate other abstract things called data.\nThe evolution of a process is directed by a pattern of rules\ncalled a program. People create programs to direct processes. In effect,\nwe conjure the spirits of the computer with our spells.\"\"\".split()\n\n# By deriving a set from `raw_text`, we deduplicate the array\nvocab = set(raw_text)\nvocab_size = len(vocab)\n\nword_to_ix = {word: i for i, word in enumerate(vocab)}\ndata = []\nfor i in range(CONTEXT_SIZE, len(raw_text) - CONTEXT_SIZE):\n    context = [raw_text[i - 2], raw_text[i - 1],\n               raw_text[i + 1], raw_text[i + 2]]\n    target = autograd.Variable(torch.LongTensor([word_to_ix[raw_text[i]]]))\n    data.append((context, target))","35f4827f":"def results(net, title):\n    c = np.corrcoef(net.embeddings.weight.data.numpy().T)\n    plt.figure(figsize=(10,10))\n    plt.title(title)\n    plt.imshow(c, interpolation='nearest')\n    plt.show()\n    print(np.abs(c).sum(), 'Feature Correlation Sum')","ef3d566e":"loss_func = nn.CrossEntropyLoss()","c0f4d620":"net = CBOW(CONTEXT_SIZE, embedding_size=EMBEDDING_SIZE, vocab_size=vocab_size)\noptimizer = optim.Adam(net.parameters()) #SGD?\n\nfor epoch in range(100):\n    total_loss = 0\n    for context, target in data:\n        context_var = make_context_vector(context, word_to_ix)\n        net.zero_grad()\n        probs = net(context_var)\n\n        loss = loss_func(probs.view(1,-1), target)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.data\n    print(total_loss)","44062bae":"results(net, 'No Penalty')","f3500e26":"net = CBOW(CONTEXT_SIZE, embedding_size=EMBEDDING_SIZE, vocab_size=vocab_size)\noptimizer = optim.Adam(net.parameters()) #SGD?\n\nfor epoch in range(100):\n    total_loss = 0\n    for context, target in data:\n        context_var = make_context_vector(context, word_to_ix)\n        net.zero_grad()\n        probs = net(context_var)\n\n        loss = loss_func(probs.view(1,-1), target) + pcor_err(torch.transpose(net.embeddings.weight,0,1)) * 6 # (vocab_size\/\/2+1) ?\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.data\n    print(total_loss)","43f961be":"results(net, 'With Penalty')","b4a52421":"# Description","5e87c154":"# Penalty Version","58a9fd7e":"# Next Steps","0d689a59":"Now that we have non-correlated embeddings, we need to dive deeper into EDA. Should we reduce the number of dimensions due to the removal of correlation? How well do these embeddings perform against their non-penalized counterparts? Just like with L1\/L2\/WD regularization schemes, penalizing too greatly can impact performance. We would need to identify an appropriate amount of penalty to tack on for best training results.","a73d7e68":"# No Penalty Version","b7d81336":"I had an idea recently. Coming off the Santander competiton, whenever I saw 200 or 300 variables I start to think they're all I.I.D, lol. That gave me an idea regards to embeddings.\n\nWe're aware embeddings form a vector space where one can perform characteristic operations such as King - Male = Queen; however the **dimensions** of our embeddings are not linearly independent and actually overlap quite a bit, as evidenced by running, e.g. pearson correlation on them. What if during embedding generation, a penalty or regularization term was added which encouraged the columns to be linearly independent for the purpose of hopefully learning something different per dimension? Lets experiment.\n\nIn this kernel, we attempt to learn an embedding using CBOW (Word2Vec). First, we learn in per-usual and visualize the correlation coefficients matrix of each embedding dimension. As expected, there are a lot of correlations, which inform us that even if we've learned 10 or 300 dimensions, some of these dimensions may in fact be learning very similar relationships about our source words (undesireable). In the next block, we introduce a penalty into the loss function during training, which encourages our model to learn independent, non-correlated embedded features. In a follow up kernel, we'll pour the gas onto this model, train it with a larger corpus, and then test the performance of regular self-trained embeddings and lindep penalized self-trained embeddings on the previous Toxic Comment dataset!"}}