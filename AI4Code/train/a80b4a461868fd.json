{"cell_type":{"ce473260":"code","333d564c":"code","bfe795d8":"code","5ffe4adb":"code","8db39304":"code","87c3650c":"code","69b93385":"code","5a10b361":"code","94aeeaa8":"code","31e90ddc":"code","8ee933c5":"code","34e63112":"code","d98a4623":"code","603b6ae6":"code","187c1a10":"code","230ab921":"code","c4aeab69":"code","9725c79d":"code","d7f73725":"code","bd19986a":"code","25642f24":"code","921f88b0":"code","b736295a":"code","f050ec7c":"code","db508ef5":"code","3439de1e":"code","937a4ff2":"code","1ddcf32f":"code","1162c78b":"markdown","5322ef3b":"markdown","ef9e8575":"markdown","656b10b2":"markdown","a9fe80d7":"markdown","76c4592f":"markdown","667455bb":"markdown","9890f22f":"markdown","4472387e":"markdown","8ba85729":"markdown","ee9b7776":"markdown","4d9abb20":"markdown","54dac02c":"markdown","1a7529cb":"markdown"},"source":{"ce473260":"# installing the keras tuner library\n!pip install -q -U keras-tuner","333d564c":"# importing the dependencies\nimport pandas as pd\nimport kerastuner as kt\nfrom sklearn import ensemble\nfrom sklearn import datasets\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier","bfe795d8":"from sklearn.datasets import load_breast_cancer\n\n#loading the dataset\ndata = load_breast_cancer()\n\ndata = load_breast_cancer()\ndf = pd.DataFrame(data['data'], columns=data['feature_names'])\ndf['target'] = data['target']\ndf.info()\ndf.head(3)","5ffe4adb":"# displaying the target names\nlist(data.target_names)","8db39304":"# Selecting few features, for the sake of simplicity\nX = df[['mean radius', 'worst concave points', 'worst area', \n          'mean concavity', 'mean concave points']]\ny = df[['target']]","87c3650c":"# spliting the dataset into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","69b93385":"def build_random_forest(hp):\n  model = ensemble.RandomForestClassifier(\n        n_estimators=hp.Int('n_estimators', 10, 50, step=10),\n        max_depth=hp.Int('max_depth', 3, 10))\n  return model","5a10b361":"tuner = kt.tuners.Sklearn(\n    oracle=kt.oracles.BayesianOptimization(\n        objective=kt.Objective('score', 'max'),\n        max_trials=10),\n    hypermodel= build_random_forest,\n    directory='.',\n    project_name='random_forest')","94aeeaa8":"tuner.search(X_train.values, y_train.values.ravel())\nbest_hp = tuner.get_best_hyperparameters(num_trials=1)[0]","31e90ddc":"model = tuner.hypermodel.build(best_hp)","8ee933c5":"model.fit(X_train.values, y_train.values.ravel())\npredictions = model.predict(X_test)\nprint(accuracy_score(y_test, predictions))","34e63112":"def build_DecisionTreeClassifier(hp):\n    model = DecisionTreeClassifier(max_depth=hp.Int(\"max_depth\", 50, 100, step=10))\n    return model","d98a4623":"tuner = kt.tuners.Sklearn(\n    oracle=kt.oracles.BayesianOptimization(\n        objective=kt.Objective('score', 'max'),\n        max_trials=10),\n    hypermodel= build_DecisionTreeClassifier,\n    directory='.',\n    project_name='DecisionTreeClassifier')","603b6ae6":"model = tuner.hypermodel.build(best_hp)","187c1a10":"model.fit(X_train.values, y_train.values.ravel())\npredictions = model.predict(X_test)\nprint(accuracy_score(y_test, predictions))","230ab921":"def build_ridge_classifier(hp):\n  model = linear_model.RidgeClassifier(\n        alpha=hp.Float('alpha', 1e-3, 1, sampling='log'))\n  return model","c4aeab69":"tuner = kt.tuners.Sklearn(\n    oracle=kt.oracles.BayesianOptimization(\n        objective=kt.Objective('score', 'max'),\n        max_trials=10),\n    hypermodel= build_ridge_classifier,\n    directory='.',\n    project_name='ridge_classifier')","9725c79d":"tuner.search(X_train.values, y_train.values.ravel())\nbest_hp = tuner.get_best_hyperparameters(num_trials=1)[0]","d7f73725":"model = tuner.hypermodel.build(best_hp)\nmodel.fit(X_train.values, y_train.values.ravel())","bd19986a":"predictions = model.predict(X_test)\nprint(accuracy_score(y_test, predictions))","25642f24":"def build_svc(hp):\n  model = SVC(C=hp.Float('c', 1.0, 20.0, step=10),\n              gamma=hp.Float('gamma', 0.01, 1, step=0.01)\n              )\n  return model","921f88b0":"tuner = kt.tuners.Sklearn(\n    oracle=kt.oracles.BayesianOptimization(\n        objective=kt.Objective('score', 'max'),\n        max_trials=10),\n    hypermodel= build_svc,\n    directory='.',\n    project_name='svc')","b736295a":"tuner.search(X_train.values, y_train.values.ravel())\nbest_hp = tuner.get_best_hyperparameters(num_trials=1)[0]","f050ec7c":"model = tuner.hypermodel.build(best_hp)\n\nmodel.fit(X_train.values, y_train.values.ravel())\npredictions = model.predict(X_test)\nprint(accuracy_score(y_test, predictions))","db508ef5":"def build_model(hp):\n  model_type = hp.Choice('model_type', ['random_forest', 'ridge', 'SVC'])\n  if model_type == 'random_forest':\n    model = ensemble.RandomForestClassifier(\n        n_estimators=hp.Int('n_estimators', 10, 50, step=10),\n        max_depth=hp.Int('max_depth', 3, 10))\n  elif model_type == 'ridge':\n    model = linear_model.RidgeClassifier(\n        alpha=hp.Float('alpha', 1e-3, 1, sampling='log'))\n  else:\n    model = SVC(C=hp.Float('c', 1.0, 20.0, step=10),\n              gamma=hp.Float('gamma', 0.01, 1, step=0.01))\n  return model","3439de1e":"\ntuner = kt.tuners.Sklearn(\n    oracle=kt.oracles.BayesianOptimization(\n        objective=kt.Objective('score', 'max'),\n        max_trials=10),\n    hypermodel=build_model,\n    scoring=metrics.make_scorer(metrics.accuracy_score),\n    cv=model_selection.StratifiedKFold(5),\n    directory='.',\n    project_name='my_best_model')","937a4ff2":"tuner.search(X_train.values, y_train.values.ravel())\nbest_model = tuner.get_best_models(num_models=1)[0]","1ddcf32f":"best_model.fit(X_train.values, y_train.values.ravel())\npredictions = best_model.predict(X_test)\nprint(accuracy_score(y_test, predictions))","1162c78b":"The Sklearn class(kerastuner.tuners.sklearn.Sklearn) of keras tuner performs a cross-validated hyperparameter search for Scikit-learn models.\\\n\nThe steps we've to do in order to tune the models are:\n\n***1.*** Building a function which takes ```hp``` arguement and inside it\\\nwe specify the range of the HyperParameters( keras tuner will search the best HyperParameter within the range).\n\n\nFor e.g, suppose we want to tune ```n_estimators``` HyperParameter of the\\\nrandom forest. The value of ```n_estimators``` is of type ```int```.\\\nSo, we can do this by simply ``` n_estimators=hp.Int('n_estimators', 10, 50, step=10)```.\\\nAnd If the value of the HyperParameter is of type Float then you've to use,\\\n```hp.Float```.\n\n\nThis hp.Int or hp.Float takes the following arguements:\n\n* ```name``` It's the required arguement, make sure to name them unquely.\n* ```min_value``` It's the minimum value from which the keras tuner starts finding the best hyperparametrs\n* ```max_value``` It's the maximum value upto which the keras tuner finds the best hyperparameter.\n* ```step``` it's the number of steps skips in order to find the best hyperparameter values.\\\n* ```default``` It's an optional arguement which you can set.\n\n***2.*** Then we've to initiate the ```tuner``` class.\n\n\nThe Arguements it takes are:\n\n*   oracle: An instance of the kerastuner.Oracle class. Note that for this Tuner, the objective for the Oracle should always be set to Objective('score', direction='max'). Also, Oracles that exploit Neural-Network-specific training (e.g. Hyperband) should not be used with this Tuner.\n*   hypermodel: Instance of HyperModel class (or callable that takes a Hyperparameters object and returns a Model instance).\n*    scoring: An sklearn scoring function. For more information, see sklearn.metrics.make_scorer. If not provided, the Model's default scoring will be used via model.score. Note that if you are searching across different Model families, the default scoring for these Models will often be different. In this case you should supply scoring here in order to make sure your Models are being scored on the same metric.\n*   metrics: Additional sklearn.metrics functions to monitor during search. Note that these metrics do not affect the search process.\n*   cv: An sklearn.model_selection Splitter class. Used to determine how samples are split up into groups for cross-validation.\n\n\n***3.*** Then we've to call the tuner's ```search``` method. And in the arguements of search we've to provide the 'training data' and its 'labels'.\n\n***4*** After it searched the best HyperParameter we can obtain the sets of the best HyperParameters by doing,\n\n\n\n    ```best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]``                                                                                                 \n\n***5*** Then we can build the model with the best HyperParametrs obtained from the Keras Tuner by doing,\n\n  model = tuner.hypermodel.build(best_hp) and then we can train the model as same as we're originally doing in the sklearn before.\n  by calling the model.fit method\n","5322ef3b":"<a id=\"subsection-3\"><\/a>\n## [Ridge Classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.RidgeClassifier.html)\n\nHyperParameter to be tuned:\n* ```alpha : float```, Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to 1 \/ (2C) in other linear models such as LogisticRegression or sklearn.svm.LinearSVC.","ef9e8575":"![IMG_20200721_195432.jpg](attachment:IMG_20200721_195432.jpg)","656b10b2":"###  <span style=\"color:red\">If you like this kernel then please upvote. It'll motivate me for making more kernels like this.<\/span>","a9fe80d7":"In this kernel we're going to use the breast cancer dataset.\\\nOur task is to classify whether a person is having 'malignant' or 'benign'\\\ntype of cancers.","76c4592f":"<a id=\"section-three\"><\/a>\n## B. How to automatically tune the Hyperparameters of the models and select the best out of them simultaneously.(recall the very popular \"no-free lunch theorem\")\n\nNow, we'll see how we're gonna do HyperParametr Tuning of the model and selecting best model out od it, simultaneously.\n\n\nThe modeles we're gonna use are:\n* RandomForest\n* RidgeClassifier\n* SVC\n\nThe ```hp.choice``` takes the arguements:\n* ```name``` \n* And a list of choices of models name.","667455bb":"Now, let's start tuning the HyperPramaters of the models.\nIf you are unware of any model or its hyperparameter then feel free to refer the documentation.","9890f22f":"# Table of contents.\n\n* [Hyperparameter tuning intro](#section-one)\n* [A. How to tune the Hyperparameters of the sklearn models.](#secion-two)\n    - [RandomForest](#subsection-1)\n    - [DecisionTree](#subsection-2)\n    - [Ridge Classifier](#subsection-3)\n    - [Support Vector classifier](#subsection-4)\n* [B. How to automatically tune the Hyperparameters of the models and select the best out of them simultaneously.](#section-three)","4472387e":"<a id=\"section-one\"><\/a>\nDo, you know you can tune your sklearn model's hyperparameters with the help of Keras Tuner?\n\n### Hyperparameter tuning\n\"Hyperparameter tuning is searching the hyperparameter space for a set of values that will\noptimize your model architecture\".\n\nIn this kernel we're going to see how we can use the keras tuner to tune your models.\\\nThis kernel is divided into two parts:\n* How to tune the Hyperparameters of the model.\n* How to automatically tune the Hyperparameters of the models and select the best out of them.(recall the very popular \"no-free lunch theorem\")","8ba85729":"<a id=\"section-two\"><\/a>\n## A. How to tune the Hyperparameters of the model.\n\nNow, we gonna try out different models and we'll see how to tune thier hyperparametrs.\\\n\nsklearn models we're gonna try are:\n* [RandomForest](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html).\n* [DecisionTree](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html).\n* [Ridge Classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.RidgeClassifier.html).\n* [Support Vector classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html).\n\nI'm assuming that you guys are already know about the above models.\\\nIf not then models are clickable please click it to see their documentation.","ee9b7776":"<a id=\"subsection-2\"><\/a>  \n## [DecisionTree](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html)\n\nHyperParameter to be tuned:\n* ```max_depth : int```, The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.","4d9abb20":"<a id=\"subsection-4\"><\/a>\n## [Support Vector classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html)\nHyperParameters to be tuned:\n* ```C : float```, Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.\n* ```gamma : float```, Kernel coefficient for \u2018rbf\u2019, \u2018poly\u2019 and \u2018sigmoid\u2019.","54dac02c":"\n###  <span style=\"color:red\">Note: we are searching across different Model families, the default scoring for these Models will often be different. In this case we should supply scoring here in order to make sure your Models are being scored on the same metric.\nThis can be done by passing sklearn metrics to ```scoring``` arguements.<\/span>","1a7529cb":"<a id=\"subsection-1\"><\/a>\n## [RandomForest](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html)\n\nHyperParameters to be tuned,\n\n* ```n_estimators: int```,\nThe number of trees in the forest.\n* ```max_depth: int```,\nThe maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples."}}