{"cell_type":{"f1fb68ef":"code","230d36b9":"code","23ce6874":"code","b3081ef0":"code","712a7934":"code","3e227ab1":"code","e7c62820":"code","477e7c69":"code","75d7d3cb":"code","e77ce134":"code","c490a16f":"code","0a7134fc":"code","2e635381":"code","982199fe":"code","fc447afc":"code","6f8937ee":"code","0ff703fa":"code","d1ade327":"code","c8bb271a":"code","a9c6385a":"code","ba8c9de1":"code","d594a75c":"code","aa1aae57":"code","596dce10":"code","be03220b":"code","ab42e99f":"code","e2dd4fb3":"code","bbe678ad":"code","bfc67036":"code","9d6d1dc5":"code","dd6f9a7c":"code","3123c11d":"code","2fabf7b8":"code","f86c3a88":"code","1dc1f128":"code","bfb05d75":"code","454afb3c":"code","06c9f8e3":"code","4c79b76e":"code","a2758766":"code","dcf2057a":"code","8de20792":"code","08d88e06":"code","da87be8e":"code","078b7dc1":"code","f8ead84c":"code","be02c480":"code","067b9167":"code","f4d436db":"code","9a633fb0":"code","4f43b64d":"code","e6f9ffb6":"code","516dc148":"code","00a859bb":"code","d6d015ee":"code","86aef9be":"code","e1c281e7":"code","38126dce":"code","645f4e95":"code","7f88666f":"code","53391838":"code","58b773ed":"code","ac118a3c":"code","c5c23938":"code","db1f07e6":"code","d9b7c208":"code","6429cc18":"code","15d301b7":"code","93aa46f0":"code","edc6a1b9":"code","70891de6":"code","ab57212d":"code","9babe2c1":"code","884bd209":"markdown","4aa715d5":"markdown","9928def5":"markdown","13aa9ba1":"markdown","24f85816":"markdown","2a221cec":"markdown","d9c39b75":"markdown","32a1ed6a":"markdown","19b2e415":"markdown","da03147f":"markdown","50d4501e":"markdown","89d150f1":"markdown","6cbdcc54":"markdown","ebfa83b7":"markdown","7b89dbe5":"markdown","b3c6c891":"markdown","fa2c452e":"markdown","45d62e8c":"markdown","23c11271":"markdown","2322861b":"markdown","d7f546b5":"markdown","f767e1d2":"markdown","0084ce46":"markdown"},"source":{"f1fb68ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","230d36b9":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.special import boxcox1p\n\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, RidgeCV, LassoCV\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport xgboost as xgb\n\nsns.set(style='whitegrid')\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ntrain_original = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_original = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n","23ce6874":"train.info()\n","b3081ef0":"train = train.drop('Id', axis=1)\ntest = test.drop('Id', axis=1)\n","712a7934":"test.head()\n","3e227ab1":"train['SalePrice'].describe()\n","e7c62820":"sns.distplot(train['SalePrice'])\n","477e7c69":"print(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())\n","75d7d3cb":"train_quantitative = train[[d for d in train.columns if train.dtypes[d] != 'object']].copy()\ntest_quantitative = test[[d for d in test.columns if test.dtypes[d] != 'object']].copy()\n","e77ce134":"train_qualitative = train[[d for d in train.columns if train.dtypes[d] == 'object']].copy()\ntest_qualitative = test[[d for d in test.columns if test.dtypes[d] == 'object']].copy()","c490a16f":"train_quantitative.describe()\n","0a7134fc":"train_qualitative.describe()\n","2e635381":"train_qualitative.columns\n","982199fe":"missing_train = train_qualitative.isnull().sum().sort_values(ascending=False)\npercentage_train = (train_qualitative.isnull().sum()\/train_qualitative.isnull().count()).sort_values(ascending=False)\ntrain_info = pd.concat([missing_train,percentage_train],keys=['Missing','Percentage'],axis=1)\ntrain_info.head(25)\n","fc447afc":"fig = plt.figure(figsize=(10,5))\ntrain_plot = sns.barplot(x=missing_train.index[0:20],y=missing_train[0:20])\ntrain_plot.set_xticklabels(train_plot.get_xticklabels(),rotation=90)\nplt.title('Number of missing values in categorical data(train)')\n","6f8937ee":"missing_test = test_qualitative.isnull().sum().sort_values(ascending=False)\npercentage_test = (test_qualitative.isnull().sum()\/test_qualitative.isnull().count()).sort_values(ascending=False)\ntest_info = pd.concat([missing_test,percentage_test],keys=['Missing','Percentage'],axis=1)\ntest_info.head(25)\n","0ff703fa":"fig = plt.figure(figsize=(10,5))\ntest_plot = sns.barplot(x=missing_test.index[0:20],y=missing_test[0:20])\ntest_plot.set_xticklabels(test_plot.get_xticklabels(),rotation=90)\nplt.title('Number of missing values in categorical data(test)')\n","d1ade327":"for column in train_qualitative.columns:\n    train_qualitative[column] = train_qualitative[column].fillna(\"None\")\nfor column in test_qualitative.columns:\n    test_qualitative[column] = test_qualitative[column].fillna(\"None\")\n","c8bb271a":"train_qualitative['Electrical']=train_qualitative['Electrical'].fillna(method='pad')\ntest_qualitative['SaleType']=test_qualitative['SaleType'].fillna(method='pad')\ntest_qualitative['KitchenQual']=test_qualitative['KitchenQual'].fillna(method='pad')\ntest_qualitative['Exterior1st']=test_qualitative['Exterior1st'].fillna(method='pad')\ntest_qualitative['Exterior2nd']=test_qualitative['Exterior2nd'].fillna(method='pad')\ntest_qualitative['Functional']=test_qualitative['Functional'].fillna(method='pad')\ntest_qualitative['Utilities']=test_qualitative['Utilities'].fillna(method='pad')\ntest_qualitative['MSZoning']=test_qualitative['MSZoning'].fillna(method='pad')\n","a9c6385a":"train_qualitative.isnull().sum().sum()\n","ba8c9de1":"test_qualitative.isnull().sum().sum()\n","d594a75c":"train_qualitative.shape\n","aa1aae57":"test_qualitative.shape\n","596dce10":"#Let's check what really matters in quantiative data. Sns's heatmap will be a great place to start\ntop = 10\ncorr = train_quantitative.corr()\ntop10 = corr.nlargest(top,'SalePrice')['SalePrice'].index\ncorr_top10 = train_quantitative[top10].corr()\nf,ax = plt.subplots(figsize=(10,10))\nsns.heatmap(corr_top10, square=True, ax=ax, annot=True, fmt='.2f', annot_kws={'size':12})\nplt.title('Top correlated quantitative features of dataset')\nplt.show()\n","be03220b":"corr = train_quantitative.corr()['SalePrice'].sort_values(ascending=False)\nprint(corr)\n","ab42e99f":"fig,ax = plt.subplots(2,2,figsize=(15,15))\nsns.scatterplot(data=train_quantitative, x='SalePrice', y='GarageArea', ax=ax[0][0])\nsns.scatterplot(data=train_quantitative, x='SalePrice', y='GarageCars', ax=ax[0][1])\nsns.scatterplot(data=train_quantitative, x='SalePrice', y='TotRmsAbvGrd', ax=ax[1][0])\nsns.scatterplot(data=train_quantitative, x='SalePrice', y='GrLivArea', ax=ax[1][1])\n\nplt.show()\n","e2dd4fb3":"corr = train_quantitative.corr()['SalePrice'].sort_values(ascending=False)\nprint(corr)\n","bbe678ad":"train_quantitative = train_quantitative.drop(['GarageCars','TotRmsAbvGrd'], axis=1)\ntest_quantitative = test_quantitative.drop(['GarageCars','TotRmsAbvGrd'], axis=1)\n","bfc67036":"#To take whole dataset and see what's going on with the data the seaborn library will be very helpful again.\n\n\nfig,ax = plt.subplots(17,2,figsize=(15,60))\n\nfor i in range(len(train_quantitative.columns)-1):\n    #-1 in iterator to avoid regplot between \"SalePrice\" and \"SalePrice\"\n    r=i\/\/2\n    c=i%2\n    sns.scatterplot(data=train_quantitative, x=train_quantitative.columns[i], y='SalePrice', hue='SalePrice', palette='rocket', ax=ax[r][c])\n    \nplt.tight_layout()\nplt.show()\n","9d6d1dc5":"#Missing values\nmissing_train_num = train_quantitative.isnull().sum().sort_values(ascending=False)\npercentage_train_num = (train_quantitative.isnull().sum()\/train_quantitative.isnull().count()).sort_values(ascending=False)\ntrain_info = pd.concat([missing_train_num,percentage_train_num],keys=['Missing','Percentage'],axis=1)\ntrain_info.head(10)\n","dd6f9a7c":"fig = plt.figure(figsize=(10,5))\ntest_plot = sns.barplot(x=missing_train_num.index[0:5],y=missing_train_num[0:5])\ntest_plot.set_xticklabels(test_plot.get_xticklabels(),rotation=90)\nplt.title('Number of missing values in numerical data(test)')\n","3123c11d":"train_quantitative['LotFrontage'] = train_quantitative.groupby(train_qualitative['Neighborhood'])['LotFrontage'].transform(lambda x: x.fillna(x.median()))\ntest_quantitative['LotFrontage'] = test_quantitative.groupby(test_qualitative['Neighborhood'])['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n","2fabf7b8":"train_quantitative['GarageYrBlt']=train_quantitative['GarageYrBlt'].fillna(train_quantitative['GarageYrBlt'].median())\ntest_quantitative['GarageYrBlt']=test_quantitative['GarageYrBlt'].fillna(test_quantitative['GarageYrBlt'].median())\n\nfor column in train_quantitative.columns:\n    train_quantitative[column] = train_quantitative[column].fillna(0)\nfor column in test_quantitative.columns:\n    test_quantitative[column] = test_quantitative[column].fillna(0)\n","f86c3a88":"train_quantitative.isnull().sum().sum()","1dc1f128":"test_quantitative.isnull().sum().sum()\n","bfb05d75":"train_quantitative['TotalSF'] = train_quantitative['TotalBsmtSF']+train_quantitative['1stFlrSF']+train_quantitative['2ndFlrSF']\ntrain_quantitative = train_quantitative.drop(columns={'1stFlrSF', '2ndFlrSF','TotalBsmtSF'})\ntrain_quantitative['YrBltAndRemod']=train_quantitative['YearBuilt']+train_quantitative['YearRemodAdd']\ntrain_quantitative = train_quantitative.drop(columns={'YearBuilt', 'YearRemodAdd'})\ntrain_quantitative['Bsmt'] = train_quantitative['BsmtFinSF1']+ train_quantitative['BsmtFinSF2']\ntrain_quantitative = train_quantitative.drop(columns={'BsmtFinSF1','BsmtFinSF2'})\ntrain_quantitative['TotalBathroom'] = (train_quantitative['FullBath'] + (0.5 * train_quantitative['HalfBath']) +\n                               train_quantitative['BsmtFullBath'] + (0.5 * train_quantitative['BsmtHalfBath']))\ntrain_quantitative = train_quantitative.drop(columns={'FullBath','HalfBath','BsmtFullBath','BsmtHalfBath'})\n\n\ntest_quantitative['TotalSF'] = test_quantitative['TotalBsmtSF']+test_quantitative['1stFlrSF']+test_quantitative['2ndFlrSF']\ntest_quantitative = test_quantitative.drop(columns={'1stFlrSF', '2ndFlrSF','TotalBsmtSF'})\ntest_quantitative['YrBltAndRemod']=test_quantitative['YearBuilt']+test_quantitative['YearRemodAdd']\ntest_quantitative = test_quantitative.drop(columns={'YearBuilt', 'YearRemodAdd'})\ntest_quantitative['Bsmt'] = test_quantitative['BsmtFinSF1']+ test_quantitative['BsmtFinSF2']\ntest_quantitative = test_quantitative.drop(columns={'BsmtFinSF1','BsmtFinSF2'})\ntest_quantitative['TotalBathroom'] = (test_quantitative['FullBath'] + (0.5 * test_quantitative['HalfBath']) +\n                               test_quantitative['BsmtFullBath'] + (0.5 * test_quantitative['BsmtHalfBath']))\ntest_quantitative = test_quantitative.drop(columns={'FullBath','HalfBath','BsmtFullBath','BsmtHalfBath'})\n","454afb3c":"fig,ax = plt.subplots(14,2,figsize=(15,60))\n\nfor i in range(len(train_quantitative.columns)):\n    r=i\/\/2\n    c=i%2\n    sns.scatterplot(data=train_quantitative, x=train_quantitative.columns[i], y='SalePrice', hue='SalePrice', palette='viridis', ax=ax[r][c])\n    \nplt.tight_layout()\nplt.show()\n","06c9f8e3":"numerical_to_categorical = ['TotalBathroom','Fireplaces','MSSubClass','OverallCond','BedroomAbvGr','LowQualFinSF','KitchenAbvGr','MoSold','YrSold','PoolArea','MiscVal','LotArea','3SsnPorch','ScreenPorch']\n","4c79b76e":"numerical_categorical_train=train_quantitative[numerical_to_categorical]\ntrain_quantitative.drop(columns=numerical_to_categorical,inplace=True)\ntrain_quantitative\n","a2758766":"numerical_categorical_test = test_quantitative[numerical_to_categorical]\ntest_quantitative.drop(columns=numerical_to_categorical, inplace=True)\ntest_quantitative\n","dcf2057a":"corr = train_quantitative.corr()['SalePrice'].sort_values(ascending=False)\nprint(corr)\n","8de20792":"train_qualitative = pd.concat([train_qualitative, numerical_categorical_train], axis=1)\ntest_qualitative = pd.concat([test_qualitative, numerical_categorical_test], axis=1)\nqualitative = pd.concat((train_qualitative, test_qualitative), sort=False).reset_index(drop=True)\nqualitative = pd.get_dummies(qualitative)\n","08d88e06":"train_qualitative_final = qualitative[:train_qualitative.shape[0]]\ntest_qualitative_final = qualitative[train_qualitative.shape[0]:]\n","da87be8e":"train_qualitative_final.shape\n","078b7dc1":"test_qualitative_final.shape\n","f8ead84c":"y_pred = np.log1p(train['SalePrice'])\ny_train = np.log1p(train_quantitative['SalePrice'])\n","be02c480":"train_quantitative.drop('SalePrice',axis=1, inplace=True)\n","067b9167":"sns.distplot(y_train)\n","f4d436db":"print('Train quantitative skewness')\nskewed_features_train = []\nfor column in train_quantitative:\n    skew = abs(train_quantitative[column].skew())\n    print('{:15}'.format(column), \n          'Skewness: {:05.2f}'.format(skew))\n    if skew > 0.5:\n        skewed_features_train.append(column)\n","9a633fb0":"skewed_features_train","4f43b64d":"lam = 0.15\nfor feat in skewed_features_train:\n    train_quantitative[feat] = boxcox1p(train_quantitative[feat], lam)\n","e6f9ffb6":"print('Test quantitative skewness')\nskewed_features_test = []\nfor column in test_quantitative:\n    skew = abs(test_quantitative[column].skew())\n    print('{:15}'.format(column), \n          'Skewness: {:05.2f}'.format(skew))\n    if skew > 0.75:\n        skewed_features_test.append(column)\n","516dc148":"skewed_features_test\n","00a859bb":"lam = 0.15\nfor feat in skewed_features_test:\n    test_quantitative[feat] = boxcox1p(test_quantitative[feat], lam)\n","d6d015ee":"#scaling \nscaling = StandardScaler()\ntrain_quantitative_final = pd.DataFrame(scaling.fit_transform(train_quantitative),columns=train_quantitative.columns)\ntest_quantitative_final = pd.DataFrame(scaling.fit_transform(test_quantitative),columns=test_quantitative.columns)\n","86aef9be":"train_final=train_quantitative_final.merge(train_qualitative_final,left_index=True,right_index=True).reset_index(drop=True)\ntrain_final.head()\n","e1c281e7":"test_qualitative_final = test_qualitative_final.reset_index(drop=True)\ntest_final=test_quantitative_final.merge(test_qualitative_final,left_index=True,right_index=True).reset_index(drop=True)\ntest_final.head()\n","38126dce":"train_final.shape\n","645f4e95":"test_final.shape\n","7f88666f":"X_train, X_test, Y_train, Y_test = train_test_split(train_final, y_train, test_size = .3, random_state=0)\n","53391838":"def rmse(actual,predicted):\n    return(str(np.sqrt(mean_squared_error(actual, predicted))))\n","58b773ed":"# linear regression without regularization\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, Y_train)\n\ny_pred_train = lin_reg.predict(X_train)\ny_pred_test = lin_reg.predict(X_test)\n\nprint('RMSE train = ' + rmse(Y_train,y_pred_train))\nprint('RMSE test = ' + rmse(Y_test,y_pred_test)) \nprint()\n","ac118a3c":"lasso_reg =Lasso()\nparameters= {'alpha': [0.0005,0.001,0.1,1,5,10,20]}\n\nlasso_reg=GridSearchCV(lasso_reg, param_grid=parameters)\nlasso_reg.fit(X_train,Y_train)\nalpha = lasso_reg.best_params_\nlasso_score = lasso_reg.best_score_\nprint(\"The best alpha value found is:\",alpha['alpha'],'with score:',lasso_score)\n\nlasso_reg_alpha = Lasso(alpha=alpha['alpha'])\nlasso_reg_alpha.fit(train_final,y_train)\ny_pred_train=lasso_reg_alpha.predict(X_train)\ny_pred_test=lasso_reg_alpha.predict(X_test)\n\nprint('RMSE train = ' + rmse(Y_train,y_pred_train))\nprint('RMSE test = ' + rmse(Y_test,y_pred_test))\n","c5c23938":"ridge=Ridge()\nparameters= {'alpha': [0.0005,0.001,0.1,0.2,0.4,0.5,0.7,0.8,1]}\n\nridge_reg=GridSearchCV(ridge, param_grid=parameters)\nridge_reg.fit(X_train,Y_train)\nalpha = ridge_reg.best_params_\nridge_score = ridge_reg.best_score_\nprint(\"The best alpha value found is:\",alpha['alpha'],'with score:',ridge_score)\n\nridge_reg_alpha=Ridge(alpha=alpha['alpha'])\nridge_reg_alpha.fit(train_final,y_train)\ny_pred_train=ridge_reg_alpha.predict(X_train)\ny_pred_test=ridge_reg_alpha.predict(X_test)\n\nprint('RMSE train = ' + rmse(Y_train,y_pred_train))\nprint('RMSE test = ' + rmse(Y_test,y_pred_test))\n","db1f07e6":"rf_reg = RandomForestRegressor()\nparameters = {\"max_depth\":[5, 8, 15, 25, 30], \"n_estimators\":[25,50,100,200]}\n\nrf_reg_param = GridSearchCV(rf_reg, parameters, cv = 10, n_jobs =10)\nrf_reg_param.fit(X_train, Y_train)\nrf_reg_best=rf_reg_param.best_estimator_\ny_pred_train = rf_reg_best.predict(X_train)\ny_pred_test = rf_reg_best.predict(X_test)\n\nprint('RMSE train = ' + rmse(Y_train,y_pred_train))\nprint('RMSE test = ' + rmse(Y_test,y_pred_test))\n","d9b7c208":"import xgboost as xgb \n\nxgb_reg = xgb.XGBRegressor(n_estimators=1000)\nxgb_reg.fit(X_train, Y_train, early_stopping_rounds=5, \n             eval_set=[(X_test, Y_test)], verbose=False)\n","6429cc18":"xgb_reg_param = xgb.XGBRegressor(learning_rate=0.05,\n                      n_estimators=1000,\n                      max_depth=3)\n\nxgb_reg_param.fit(train_final, y_train)\nxgb_train_pred = xgb_reg_param.predict(X_train)\nxgb_test_pred = xgb_reg_param.predict(X_test)\n\n\nprint('RMSE train = ' + rmse(Y_train,xgb_train_pred))\nprint('RMSE test = ' + rmse(Y_test,xgb_test_pred))\n","15d301b7":"def blended_regression(X):\n    return ((0.3 * ridge_reg_alpha.predict(X)) + (0.7 * xgb_reg_param.predict(X)))\n","93aa46f0":"y_pred_train = blended_regression(X_train)\ny_pred_test = blended_regression(X_test)\nprint('RMSE train = ' + rmse(Y_train,y_pred_train))\nprint('RMSE test = ' + rmse(Y_test,y_pred_test))\n\n","edc6a1b9":"y_test=blended_regression(test_final)\n","70891de6":"final_y_test=np.expm1(y_test)\n","ab57212d":"sample=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsubmission=pd.DataFrame({\"Id\":sample['Id'],\n                         \"SalePrice\":final_y_test})\nsubmission.to_csv('submission.csv',index=False)\n","9babe2c1":"final_y_test\n","884bd209":"# Train test split","4aa715d5":"> For the rest of the missing data we will use the fillna method with the substituted value 0. The exception will be 'GarageYrBlt' which we will replace with median value.\n\n","9928def5":"# XGBoost Regressor","13aa9ba1":"# Random Forest Regressor","24f85816":"> As we can see, we are missing quite a lot of data. A simple and effective solution will be to swap NaN values for None","2a221cec":"# Submission","d9c39b75":"> REF;-\nhttps:\/\/towardsdatascience.com\/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b\nhttps:\/\/medium.com\/swlh\/random-forest-and-its-implementation-71824ced454f\nhttps:\/\/machinelearningmastery.com\/xgboost-for-regression\/\n","32a1ed6a":"> Feature engineering","19b2e415":"****In order to test blended regression I decided to build a simple model and check how it performs by trial and error method. For this purpose I chose the L2 regularization model and XGBoost regressor becouse they achieve best RMSE scores in single run.\n\n","da03147f":"> Let's check how our newly created featurs talks with SalePrice.\n\n","50d4501e":"****ridge regression","89d150f1":"> As we can see from the charts in our data, there are still numerical featuers which in fact are categorical ones. I will move them to categorical data and drop them from numerical data. After that i will concat them with qualitative datasets and then i will use pd.getdummies to obtain final qualitative datasets.\n\n","6cbdcc54":"#RMSE evaluation","ebfa83b7":"> Now as we can see our target - 'SalePrice' is normally distributed. But what about other skewed features? Let's take a deeper look into them.\n\n","7b89dbe5":"****Lasso Regression (L1 regularization)","b3c6c891":"> However, for some of the columns where not much data were missing I used the 'pad' method, which propagate last valid observation forward to next.\n\n","fa2c452e":"# Blended approach","45d62e8c":"> How much categorical data do we lack? Let's find out.\n\n","23c11271":"> Skewed features\nAs I mentioned before, distribution of our target - 'SalePrice' is far away from gaussian distribution. Thus we need to perform relevant transformation to obtain distribution closer to normal. To do this I've used log(1+x) transformation","2322861b":"> Because property areas are usually similar to other houses in its neighborhood, we can supplement the missing values with the median LotFrontage of the area.\n\n","d7f546b5":"> Exploratory data analysis on quantitative data","f767e1d2":"# Exploratory data analysis on categorical data","0084ce46":"> Based on the chart, we can see that SalePrice has:\n\nLeft(positive) skewness\nVisible peak\nDistribution which is far from normal\nLater, log transformation could be helpful. Why log transformation? Log transformation is used to transform skewed data into data which distribution is closer to normal. This allows for better performance of regression models. The next sensible step for further exploratory data analysis is the division of data into qualitative and quantitative. This division will allow for a better understanding of the dataset.\n\n"}}