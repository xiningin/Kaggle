{"cell_type":{"3ba5a75b":"code","596fbd05":"code","1b5000c4":"code","a4acf41f":"code","fd1a1d24":"code","a9dd66b1":"code","b431ef08":"code","2c03f72f":"code","0a93f0e6":"code","da265083":"code","103412b8":"code","d1edef87":"code","7009feaa":"code","ebcbe24a":"code","5cfa1b24":"code","88167923":"code","fa5901ff":"code","17bf7cb6":"code","1c0dbae3":"code","9e311b1e":"code","0d977359":"code","060c967c":"code","3aa9e096":"code","18e9ad18":"code","8a6fa8c6":"code","20785c51":"code","b9226c8a":"code","99c629ba":"code","b6fd5304":"code","88d96eaa":"code","6946ac9a":"code","c5884f3f":"code","e48bc963":"code","ab0d1c0b":"code","e056501d":"code","48096758":"code","394a8879":"code","606ecf36":"code","33a8e7f4":"code","3654bacd":"code","272317a4":"code","e776e2ea":"code","87f54e0c":"code","266fa1d1":"code","dc4c3edb":"code","61f6fad5":"code","87655dc8":"code","32f87f3b":"code","c2afc3c8":"code","dc2e03c4":"code","7111ab2a":"code","8698d245":"code","85fa6ce5":"code","5b102275":"markdown","ea61a4c8":"markdown","418e24ac":"markdown","89bdb8c0":"markdown","b29b31a3":"markdown","c5625768":"markdown","e370c7ac":"markdown","5b2564f4":"markdown","6bbed1a3":"markdown","4dbfd291":"markdown","2aae3589":"markdown","817afbc8":"markdown","c4cfb0a1":"markdown","dac9b9c2":"markdown","e652c5ab":"markdown","d6984f8b":"markdown","b468a20b":"markdown","0d7152d0":"markdown","db99a86a":"markdown","f6c48806":"markdown","920677bc":"markdown","59ff1ebb":"markdown","fa117cf1":"markdown","eb296208":"markdown","e89e7ba3":"markdown","a3db0531":"markdown","1f4e8caa":"markdown","4c6bf05c":"markdown","a4f921b2":"markdown","21e2c20c":"markdown","208dbecf":"markdown","db51121c":"markdown","5ff874e7":"markdown","323cc922":"markdown","00e25783":"markdown","78a4dbfa":"markdown","011b3f23":"markdown","f5823dd8":"markdown","58bfd1da":"markdown","f8fe196d":"markdown","0270472b":"markdown","3cc17fda":"markdown","affcb307":"markdown","118db0df":"markdown"},"source":{"3ba5a75b":"\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport os\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport plotly.express as px\nfrom plotly.offline import init_notebook_mode\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\ninit_notebook_mode(connected=True)\npd.set_option('display.max_columns', 5000)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#os.mkdir('\/kaggle\/working\/individual_charts\/')\nimport matplotlib.pyplot as plt\n# Load the data\n#Will come in handy to wrap the lengthy texts\nimport textwrap\n#useful libraries and functions\nfrom itertools import repeat\n#Libraries that give a different visual possibilities\nfrom pandas import option_context \nfrom plotly.subplots import make_subplots\n\ndef long_sentences_seperate(sentence, width=30):\n    try:\n        splittext = textwrap.wrap(sentence,width)\n        text = '<br>'.join(splittext)#whitespace is removed, and the sentence is joined\n        return text\n    except:\n        return sentence\n\ndef load_csv(base_dir,file_name):\n    \"\"\"Loads a CSV file into a Pandas DataFrame\"\"\"\n    file_path = os.path.join(base_dir,file_name)\n    df = pd.read_csv(file_path,low_memory=False)\n    return df    \n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","596fbd05":"base_dir = '..\/input\/widsdatathon2022\/'\nfile_name = 'train.csv'\ndataset_main = load_csv(base_dir,file_name)","1b5000c4":"#Single Factor Bar Graphs\ndef uni_factor(factor):\n    grp_factor = dataset_main.groupby(factor)['floor_area'].count().reset_index()\n    grp_factor.sort_values(by='floor_area',inplace=True,ascending = False)\n    grp_factor[factor] = grp_factor[factor].astype('category')\n    vis = px.bar(data_frame=grp_factor,y = factor,x ='floor_area',color= factor)\n    vis.update_layout(yaxis = {'categoryorder' : 'total ascending'},\n                      title = 'Number of Buildings based on ' + factor,\n                     height = 800)\n    vis.show()\n\n#Single Factor Histogram Graphs    \ndef uni_hist_plot(independent,dependent):\n    vis = px.histogram(data_frame=dataset_main,x=dependent,color=independent)\n    vis.update_layout(title='Distribution of '+ dependent + ' based on '+ independent)\n    vis.show()\n\n#Single Factor Box Plot Graphs    \ndef uni_box_plot(independent,dependent):\n    vis = px.box(data_frame=dataset_main,x=dependent,color=independent)\n    vis.update_layout(title='Box plot of '+ dependent + ' based on '+ independent)\n    vis.show()\n\n#Two Factor Scatter Plot Graphs \ndef two_factor(factor1, factor2,independent):\n    vis = px.scatter(data_frame=dataset_main,y = factor1,x =factor2,color=independent,\n                     facet_col=independent,facet_col_wrap=3)\n    vis.update_layout(title = 'Relation between ' + factor1 + ' and ' +factor2 + ' in ' + independent + 'condition',\n                     height = 1000)\n    vis.show()\n\n#Single factor target average Bar Graph    \ndef avg_on_factor(factor,target):\n    grp_factor = dataset_main.groupby(factor)[target].mean().reset_index()\n    grp_factor.sort_values(by=target,inplace=True,ascending = False)\n    grp_factor[factor] = grp_factor[factor].astype('category')\n    vis = px.bar(data_frame=grp_factor,y = factor,x =target,color= factor)\n    vis.update_layout(yaxis = {'categoryorder' : 'total ascending'},\n                      title = 'Average of '+target +' on basis of ' + factor,\n                     height = 800)\n    vis.show()\n    \n\ndef corr_heat_map(df,title):\n\n    #Building the dataset with column that are numerical\n    df = df[df.columns[df.dtypes != 'object']]\n    df_corr_mat = df.corr() #building the correlation matrix\n    #Building the lower triagle of the correlation matrix\n    df_corr_mat_lt = df_corr_mat.where(np.tril(np.ones(df_corr_mat.shape)).astype(np.bool))\n    vis = px.imshow(df_corr_mat_lt,text_auto=True, aspect=\"auto\",\n                    height=1000,color_continuous_scale='spectral',width=900)\n    vis.update_layout(title=title)\n    vis.show()","a4acf41f":"#Collecting garbage memory and deleting unwanted Dataframes, that have served their purpose earlier\nimport gc\ngc.collect()","fd1a1d24":"#Let us start checking the missing value columns\n\ndataset_withMissing = dataset_main[['direction_max_wind_speed','direction_peak_wind_speed',\n                                    'max_wind_speed','days_with_fog','energy_star_rating']]\n\n#Need to find an effective way to deal with all the columns, so lets try describe\ndataset_withMissing.describe()\n\n#direction_max_wind_speed,  direction_peak_wind_speed,  max_wind_speed all can be safely filled with 1\n\n#days with fog is best to be filled with median values.","a9dd66b1":"del dataset_withMissing\n\n#Below variables are made 0, since the sensors would have malfunctioned. So forward filling \n\ndataset_main.direction_max_wind_speed = dataset_main.direction_max_wind_speed.fillna(1)\ndataset_main.direction_peak_wind_speed = dataset_main.direction_peak_wind_speed.fillna(1)\ndataset_main.max_wind_speed = dataset_main.max_wind_speed.fillna(1)\ndataset_main.days_with_fog = dataset_main.days_with_fog.fillna(dataset_main.days_with_fog.median())\n\n#That leaves the Energy star rating. We need to check the test set provided for deciding","b431ef08":"#loading test_set\ntest_main = load_csv(base_dir='..\/input\/widsdatathon2022',file_name='test.csv')\n\n#Need to find an effective way to deal with all the columns, so lets try describe\ntest_main.describe()\n\ntest_main.direction_max_wind_speed = test_main.direction_max_wind_speed.fillna(1)\ntest_main.direction_peak_wind_speed = test_main.direction_peak_wind_speed.fillna(1)\ntest_main.max_wind_speed = test_main.max_wind_speed.fillna(1)\n#direction_max_wind_speed,  direction_peak_wind_speed,  max_wind_speed all can be safely filled with 1\ntest_main.days_with_fog = test_main.days_with_fog.fillna(0)","2c03f72f":"print('By removing the row with energy_star_rating null we lose {}% data'.format(100-(49048\/75757)*100))\nprint('Site EUI the target variable is having {}% correlation'.format(dataset_main.corrwith(dataset_main.energy_star_rating)[-2]*100))","0a93f0e6":"#We take only the rows that have energy star rating available for the modeling.\ndataset_A = dataset_main[~dataset_main.energy_star_rating.isna()]\ndataset_A.drop('Year_Factor',axis=1,inplace=True) #Since the test set has only one year factor, so will not be useful","da265083":"# Segregating the columns with categorical value\ndata_Acat = dataset_A[dataset_A.columns[dataset_A.dtypes == 'object']]\n\ndata_Anum = dataset_A[dataset_A.columns[dataset_A.dtypes != 'object']]\n\n#A simple and straight_forward one-hot encoding using Pandas' Get_Dummies\ndata_Acat = pd.get_dummies(data_Acat,columns=data_Acat.columns)\n\ndata_Amodel = pd.merge(left=data_Acat,right=data_Anum,left_index=True,right_index=True)\nprint(data_Amodel.shape)\n","103412b8":"#https:\/\/stackoverflow.com\/a\/46581125\/16388185\ndef clean_dataset(df):\n    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n    df.dropna(inplace=True)\n    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n    return df[indices_to_keep].astype(np.float64)","d1edef87":"#Creating the X the Independent variables and Y the Target or Dependent variables\ndata_Amodel = clean_dataset(data_Amodel)\n#We lost another 1000 entries to the big number and infinity issues\nX = data_Amodel.iloc[:,:-2]\nY = data_Amodel.site_eui","7009feaa":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\n\n# Error Metrics\nfrom sklearn.metrics import mean_squared_error","ebcbe24a":"# split out validation dataset for the end\n\nvalidation_size = 0.2\n\n#In case the data is not dependent on the time series, then train and test split randomly\nseed = 7\n# X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n\n#In case the data is not dependent on the time series, then train and test split should be done based on sequential sample\n#This can be done by selecting an arbitrary split point in the ordered list of observations and creating two new datasets.\n\ntrain_size = int(len(X) * (1-validation_size))\nX_train, X_test = X[0:train_size], X[train_size:len(X)]\nY_train, Y_test = Y[0:train_size], Y[train_size:len(X)]","5cfa1b24":"# test options for regression\nnum_folds = 10\nscoring = 'neg_mean_squared_error'\n#scoring ='neg_mean_absolute_error'\n#scoring = 'r2'","88167923":"# spot check the traditional Machine Learning algorithms\nmodels = []\n#models.append(('LR', LinearRegression()))\nmodels.append(('LASSO', Lasso()))\nmodels.append(('EN', ElasticNet()))\n#models.append(('KNN', KNeighborsRegressor()))\nmodels.append(('CART', DecisionTreeRegressor()))\n#models.append(('SVR', SVR()))\n\n#Ensable Models \n# Boosting methods\n#models.append(('ABR', AdaBoostRegressor()))\n#models.append(('GBR', GradientBoostingRegressor()))\n# Bagging methods\n#models.append(('RFR', RandomForestRegressor()))\n#models.append(('ETR', ExtraTreesRegressor()))","fa5901ff":"names = []\nkfold_results = []\ntest_results = []\ntrain_results = []\nfor name, model in models:\n    names.append(name)\n    \n    ## K Fold analysis:\n    \n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    #converted mean square error to positive. The lower the beter\n    cv_results = -1* cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    kfold_results.append(cv_results)\n    \n\n    # Full Training period\n    print('{} model fit Started'.format(model))\n    res = model.fit(X_train, Y_train)\n    print('{} model fit Completed'.format(model))\n    #The error function is root of mean_squared_error\n    train_result = np.sqrt(mean_squared_error(res.predict(X_train), Y_train))\n    train_results.append(train_result)\n    \n    # Test results\n    #The error function is root of mean_squared_error\n    test_result = np.sqrt(mean_squared_error(res.predict(X_test), Y_test))\n    test_results.append(test_result)\n    \n    msg = \"%s: %f (%f) %f %f\" % (name, cv_results.mean(), cv_results.std(), train_result, test_result)\n    print(msg)","17bf7cb6":"kfold = pd.DataFrame(kfold_results,columns=range(1,11)).T\nkfold.columns = ['LASSO','EN','CART']","1c0dbae3":"visA = go.Figure()\nfor kf in kfold.columns:\n    visA.add_trace(go.Box(x=kfold[kf],name=kf))\nvisA.update_xaxes(type='log')\nvisA.update_layout(title='Kfold Error Results')\nvisA.show()","9e311b1e":"# compare algorithms\nfig = plt.figure()\n\nind = np.arange(len(names))  # the x locations for the groups\nwidth = 0.35  # the width of the bars\n\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.bar(ind - width\/2, train_results,  width=width, label='Train Error')\nplt.bar(ind + width\/2, test_results, width=width, label='Test Error')\nfig.set_size_inches(15,8)\nplt.legend()\nax.set_xticks(ind)\nax.set_xticklabels(names)\nplt.show()","0d977359":"dataset_main.direction_max_wind_speed = dataset_main.direction_max_wind_speed.fillna(1)\ndataset_main.direction_peak_wind_speed = dataset_main.direction_peak_wind_speed.fillna(1)\ndataset_main.max_wind_speed = dataset_main.max_wind_speed.fillna(1)\ndataset_main.days_with_fog = dataset_main.days_with_fog.fillna(dataset_main.days_with_fog.median())\ndataset_main.loc[dataset_main.energy_star_rating.isna(),'energy_star_rating'] = dataset_main.loc[~dataset_main.energy_star_rating.isna(),'energy_star_rating'].median()\n\n#That leaves the Energy star rating. We need to check the test set provided for deciding","060c967c":"# Segregating the columns with categorical value\ndata_Bcat = dataset_main[dataset_main.columns[dataset_main.dtypes == 'object']]\n\ndata_Bnum = dataset_main[dataset_main.columns[dataset_main.dtypes != 'object']]\ndata_Bnum.drop('Year_Factor',axis=1,inplace=True)\n#A simple and straight_forward one-hot encoding using Pandas' Get_Dummies\ndata_Bcat = pd.get_dummies(data_Bcat,columns=data_Bcat.columns)\n\ndata_Bmodel = pd.merge(left=data_Bcat,right=data_Bnum,left_index=True,right_index=True)\nprint(data_Bmodel.shape)\n","3aa9e096":"#Creating the X the Independent variables and Y the Target or Dependent variables\ndata_Bmodel = clean_dataset(data_Bmodel)\n#We lost another 2500 entries to the big number and infinity issues\nX = data_Bmodel.iloc[:,:-2]\nY = data_Bmodel.site_eui","18e9ad18":"# split out validation dataset for the end\n\nvalidation_size = 0.2\n\n#In case the data is not dependent on the time series, then train and test split randomly\nseed = 7\n# X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n\n#In case the data is not dependent on the time series, then train and test split should be done based on sequential sample\n#This can be done by selecting an arbitrary split point in the ordered list of observations and creating two new datasets.\n\ntrain_size = int(len(X) * (1-validation_size))\nX_train, X_test = X[0:train_size], X[train_size:len(X)]\nY_train, Y_test = Y[0:train_size], Y[train_size:len(X)]","8a6fa8c6":"# test options for regression\nnum_folds = 10\nscoring = 'neg_mean_squared_error'\n#scoring ='neg_mean_absolute_error'\n#scoring = 'r2'","20785c51":"# spot check the traditional Machine Learning algorithms\nmodels = []\nmodels.append(('LASSO', Lasso()))\nmodels.append(('EN', ElasticNet()))\nmodels.append(('CART', DecisionTreeRegressor()))","b9226c8a":"names = []\nkfold_results = []\ntest_results = []\ntrain_results = []\nfor name, model in models:\n    names.append(name)\n    \n    ## K Fold analysis:\n    \n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    #converted mean square error to positive. The lower the beter\n    cv_results = -1* cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    kfold_results.append(cv_results)\n    \n\n    # Full Training period\n    print('{} model fit Started'.format(model))\n    res = model.fit(X_train, Y_train)\n    print('{} model fit Completed'.format(model))\n    #The error function is root of mean_squared_error\n    train_result = np.sqrt(mean_squared_error(res.predict(X_train), Y_train))\n    train_results.append(train_result)\n    \n    # Test results\n    #The error function is root of mean_squared_error\n    test_result = np.sqrt(mean_squared_error(res.predict(X_test), Y_test))\n    test_results.append(test_result)\n    \n    msg = \"%s: %f (%f) %f %f\" % (name, cv_results.mean(), cv_results.std(), train_result, test_result)\n    print(msg)","99c629ba":"kfold = pd.DataFrame(kfold_results,columns=range(1,11)).T\nkfold.columns = ['LASSO','EN','CART']","b6fd5304":"visB = go.Figure()\nfor kf in kfold.columns:\n    visB.add_trace(go.Box(x=kfold[kf],name=kf))\nvisB.update_xaxes(type='log')\nvisB.update_layout(title='Kfold Error Results')\nvisB.show()","88d96eaa":"# compare algorithms\nfig = plt.figure()\n\nind = np.arange(len(names))  # the x locations for the groups\nwidth = 0.35  # the width of the bars\n\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.bar(ind - width\/2, train_results,  width=width, label='Train Error')\nplt.bar(ind + width\/2, test_results, width=width, label='Test Error')\nfig.set_size_inches(15,8)\nplt.legend()\nax.set_xticks(ind)\nax.set_xticklabels(names)\nplt.show()","6946ac9a":"alpha_space = np.logspace(-4, 0, 30)   # Checking for alpha from .0001 to 1 and finding the best value for alpha","c5884f3f":"lasso_scores = []\nlasso = Lasso(normalize = True)\nfor alpha in alpha_space:\n    lasso.alpha = alpha\n    val = np.mean(cross_val_score(lasso, X_train, Y_train, cv = 10))\n    lasso_scores.append(val)","e48bc963":"plt.figure(figsize=(8, 8))\nplt.plot(alpha_space, lasso_scores, marker = 'D', label = \"Lasso\")\nplt.legend()\nplt.show()","ab0d1c0b":"# Performing GridSearchCV with Cross Validation technique on Lasso Regression and finding the optimum value of alpha\n\nfrom sklearn.model_selection import GridSearchCV\nnp.set_printoptions(suppress=True)\n\nparams = {'alpha': (np.logspace(-8, 8, 100))} # It will check from 1e-08 to 1e+08\nlasso = Lasso(normalize=True)\nlasso_model = GridSearchCV(lasso, params, cv = 10)\nlasso_model.fit(X_train, Y_train)\nprint(lasso_model.best_params_)\nprint(lasso_model.best_score_)","e056501d":"Training_Accuracy_Before = []\nTesting_Accuracy_Before =[]\n\n# Using value of alpha as 0.0000171 to get best accuracy for Lasso Regression\nlasso = Lasso(alpha = 1.3530477745798075e-07, normalize = True)\nlasso.fit(X_train, Y_train)\n\ntrain_score = lasso.score(X_train, Y_train)\nprint(train_score)\ntest_score = lasso.score(X_test, Y_test)\nprint(test_score)\n\ntest_result = np.sqrt(mean_squared_error(lasso.predict(X_test), Y_test))\nprint(test_result)","48096758":"coefficients = lasso.coef_\n#The factors that has higher influence\ncol1 = X_train.columns[coefficients > 30]\ncol2 = X_train.columns[coefficients < -40]","394a8879":"major_factor = col1.append(col2)\n\n#Creating data using the parameters that have major impact.\nX_lasso = X_train[major_factor]\nX_lasso_test = X_test[major_factor]\n\n#Grid Searching with updated data frame\n\nparams = {'alpha': (np.logspace(-8, 8, 100))} # It will check from 1e-08 to 1e+08\nlasso = Lasso(normalize=True)\nlasso_model = GridSearchCV(lasso, params, cv = 10)\nlasso_model.fit(X_lasso, Y_train)\nprint(lasso_model.best_params_)\nprint(lasso_model.best_score_)","606ecf36":"lasso = Lasso(alpha = 1e-08, normalize = True)\nlasso.fit(X_lasso, Y_train)\n\ntest_result = np.sqrt(mean_squared_error(lasso.predict(X_lasso_test), Y_test))\nprint(test_result)","33a8e7f4":"#Using the same dataset that is created using the major factors from Lasso Coefficients\nX_DT = X_lasso\nX_DT_test = X_lasso_test","3654bacd":"dtm = DecisionTreeRegressor(max_depth=4,\n                           min_samples_split=5,\n                           max_leaf_nodes=10)\n\ndtm.fit(X_DT,Y_train)\nprint(\"R-Squared on train dataset={}\".format(dtm.score(X_DT_test,Y_test)))\n\ndtm.fit(X_DT_test,Y_test)   \nprint(\"R-Squared on test dataset={}\".format(dtm.score(X_DT_test,Y_test)))","272317a4":"param_grid = {\"criterion\": [\"mse\"],\n              \"min_samples_split\": [10, 20, 40],\n              \"max_depth\": [2,6,8], #[2, 6, 8],\n              \"min_samples_leaf\": [20,40,100], #[20, 40, 100],\n              \"max_leaf_nodes\": [5,20,100], #[5, 20, 100],\n              }\n\n## Comment in order to publish in kaggle.\n\ngrid_cv_dtm = GridSearchCV(dtm, param_grid, cv=5)\n\ngrid_cv_dtm.fit(X_DT,Y_train)","e776e2ea":"print(\"R-Squared::{}\".format(grid_cv_dtm.best_score_))\nprint(\"Best Hyperparameters::\\n{}\".format(grid_cv_dtm.best_params_))","87f54e0c":"decision_tree_results = pd.DataFrame(data=grid_cv_dtm.cv_results_)\n\nfig,ax = plt.subplots()\nsns.pointplot(data=decision_tree_results[['mean_test_score',\n                                          'param_max_leaf_nodes',\n                                          'param_max_depth']],\n              y='mean_test_score',x='param_max_depth',\n              hue='param_max_leaf_nodes',ax=ax)\nax.set(title=\"Effect of Depth and Leaf Nodes on Model Performance\")","266fa1d1":"final_dtm = DecisionTreeRegressor(max_depth=8,min_samples_split=20,\n                                  max_leaf_nodes=100,min_samples_leaf= 20)\n\nfinal_dtm.fit(X_train,Y_train)\n\ntest_result = np.sqrt(mean_squared_error(final_dtm.predict(X_test), Y_test))\n\nprint(test_result)","dc4c3edb":"#Libraries for Deep Learning Models\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.neural_network import MLPRegressor","61f6fad5":"#Set the following Flag to 0 if the Deep LEarning Models Flag has to be disabled\ndef create_model(neurons=127, activation='relu', learn_rate = 0.01, momentum=0):\n        # create model\n        model = Sequential()\n        model.add(Dense(neurons, input_dim=X_train.shape[1], activation=activation))\n        #The number of hidden layers can be increased\n        model.add(Dense(64, activation=activation))\n        model.add(Dense(32, activation=activation))\n        # Final output layer\n        model.add(Dense(1, kernel_initializer='normal'))\n        # Compile model\n        optimizer = SGD(lr=learn_rate, momentum=momentum)\n        model.compile(loss='mean_squared_error', optimizer='adam')\n        return model","87655dc8":"#The data set for NN is same as the full data with Energy rating with median values\n\nneural_model = KerasRegressor(build_fn=create_model, epochs=100, batch_size=100, verbose=1)\n\nneural_model_fit = neural_model.fit(X_train, \n                                    Y_train, validation_data=(X_test,Y_test),\n                                    epochs=100, batch_size=72, \n                                    verbose=0, shuffle=False)","32f87f3b":"#Visual plot to check if the error is reducing\nplt.plot(neural_model_fit.history['loss'], label='train')\nplt.plot(neural_model_fit.history['val_loss'], label='test')\nplt.legend()\nplt.show()","c2afc3c8":"error_Train = np.sqrt(mean_squared_error(Y_train, neural_model.predict(X_train)))\npredicted = neural_model.predict(X_test)\nerror_Test = np.sqrt(mean_squared_error(Y_test,predicted))\nerror_Test","dc2e03c4":"#Creating the X the Independent variables and Y the Target or Dependent variables\ndata_XGB = clean_dataset(data_Bmodel)\n#We lost another 2500 entries to the big number and infinity issues\nX = data_XGB.iloc[:,:-2]\nY = data_XGB.site_eui","7111ab2a":"import xgboost as xgb\nfrom xgboost.sklearn import XGBRegressor\n\nxgb1 = XGBRegressor()\nparameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n              'objective':['reg:linear'],\n              'learning_rate': [.03, 0.05, .07], #so called `eta` value\n              'max_depth': [5, 6, 7],\n              'min_child_weight': [4],\n              'silent': [1],\n              'subsample': [0.7],\n              'colsample_bytree': [0.7],\n              'n_estimators': [500]}\n\nxgb_grid = GridSearchCV(xgb1,parameters,cv = 2,n_jobs = 5,verbose=True)\n\nxgb_grid.fit(X,Y) #Give the full dataset for running the grid search\n\nprint(xgb_grid.best_score_)\nprint(xgb_grid.best_params_)\n\nmodel =  xgb_grid.best_estimator_","8698d245":"#loading test_set\ntest_main = load_csv(base_dir='..\/input\/widsdatathon2022',file_name='test.csv')\n\n#Need to find an effective way to deal with all the columns, so lets try describe\ntest_main.describe()\n\ntest_main.direction_max_wind_speed = test_main.direction_max_wind_speed.fillna(1)\ntest_main.direction_peak_wind_speed = test_main.direction_peak_wind_speed.fillna(1)\ntest_main.max_wind_speed = test_main.max_wind_speed.fillna(1)\n#direction_max_wind_speed,  direction_peak_wind_speed,  max_wind_speed all can be safely filled with 1\ntest_main.days_with_fog = test_main.days_with_fog.fillna(0)\ntest_main.loc[test_main.energy_star_rating.isna(),'energy_star_rating'] = np.median(test_main.loc[~test_main.energy_star_rating.isna(),'energy_star_rating'])\ntest_main.loc[test_main.year_built.isna(),'year_built'] = np.median(test_main.loc[~test_main.year_built.isna(),'year_built'])\n\n# Segregating the columns with categorical value\ntest_cat = test_main[test_main.columns[test_main.dtypes == 'object']]\n\ntest_num = test_main[test_main.columns[test_main.dtypes != 'object']]\ntest_num.drop('Year_Factor',axis=1,inplace=True)\n#A simple and straight_forward one-hot encoding using Pandas' Get_Dummies\ntest_cat = pd.get_dummies(test_cat,columns=test_cat.columns)\n\ntest_model = pd.merge(left=test_cat,right=test_num,left_index=True,right_index=True)","85fa6ce5":"y_predict_test = xgb_grid.best_estimator_.predict(test_model)\n\nmy_submission = pd.DataFrame({'id': test_main.id, 'site_eui': y_predict_test})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","5b102275":"#### [Back to Contents](#contents)","ea61a4c8":"#### <a id='modDT'> Setting up the Decision Tree Search <\/a>\n\n\nWe have an opportunity perform a ensemble analysis here. Using the Lasso coefficient, we can select the parameters that have highest impact, and use those for grid searching Decision Tree models.","418e24ac":"##### <a id='Nnet'> Setting Neural Nets <\/a>","89bdb8c0":"#### <a id='misval'> Missing Value assumptions <\/a>\n\nThere are many ways to handle the missing values, out of which we will handle them in 2 ways in this analysis. \n\n    a) Will entirely drop the missing value rows\n    \n    b) Will fill the missing values with \"Median\" values\n    \nWill have to check this for both the test and train datasets, and make appropriate decision. Since in test set, we won't be allowed to drop the entire rows. There some form of imputation is necessary.    \n\n[1) Data prep](#Adata)\n\n[2) Model Set](#Amodset)\n\n[3) Result Visualisation](#Aresvis)\n\n[4) Final prediction](#Afinpre)","b29b31a3":"##### <a id='FinCon'> Lessons of the Exercise <\/a>\n\nThere are so many options to solve a single problem, all with different levels of accuracy and results. The notebook explores the breadth of models, and provides the results \n\n1) Science is challenging, and Data Science is more so. The exercise shows the challenge most of us face.\n\n2) The real world features themselves don't provide insights directly. They need to be engineered to learn more about the data. The next exercise will be exactly to use feature engineering using the KPCA and tSNE. \n\n3) Neural Networks also needs to be setup with proper understanding to use its full power for fitting the data that we throw at it.\n\n4) Be patient, and master the art of Automation using functions and pipelines.","c5625768":"#### <a id='cvlasso'> Searching the Parameters of Lasso model <\/a>\n\nDataset with the missing values updated as per the assumption  \n\n[2) Grid searching Lasso Space](#lassoset)\n\n[3) Grid Searching Result Visualisation](#GresSearch)\n\n[4) Final Observations](#Lfinpre)","e370c7ac":"##### <a id='GresSearch'> Result of Grid Searching <\/a>","5b2564f4":"#### [Back to Contents](#contents)","6bbed1a3":"##### <a id='finDT'> Finalising Decision Tree Model <\/a>","4dbfd291":"##### <a id='Aresvis'> Result Visualisation <\/a>","2aae3589":"##### <a id='lassoset'> Grid searching Lasso Space <\/a>","817afbc8":"#### [Back to Contents](#contents)","c4cfb0a1":"##### <a id='NNres'> Result of Neural Network Grid Searching <\/a>","dac9b9c2":"#### The correlation of energy_star_rating is considerable. So we try the predictive power of the model with the energy_star_rating null rows removed","e652c5ab":"#### [Back to Contents](#contents)","d6984f8b":"##### <a id='Adata'> Data prep<\/a>","b468a20b":"##### <a id='setDT'> Grid searching Decision Tree <\/a>\n","0d7152d0":"#### [Back to Contents](#contents)","db99a86a":"##### <a id='DTres'> Result of Decision Tree Grid Searching <\/a>","f6c48806":"#### [Back to Contents](#contents)","920677bc":"Decision tree regressor and the Lasso Regressor have been grid searched, and the final results have not been a huge improvement. \n\nNext option is to throw Neural Networks on to this dataset for their power to fit the parameters in a different dimension","59ff1ebb":"#### <a id='ModUnd'> Model understanding <\/a>","fa117cf1":"#### [Back to Contents](#contents)","eb296208":"##### <a id='Bresvis'> Result Visualisation <\/a>","e89e7ba3":"## <a id='contents'>  Contents <\/a>\n\n### [Missing Value assumptions](#misval)\n\n   #### [Result Visualisation](#Aresvis)\n    \n   #### [Final prediction](#Afinpre)\n\n### [Filling the Median Values in Energy Rating](#medianval)\n    \n   #### [Result Visualisation](#Bresvis)\n    \n   #### [Final prediction](#Bfinpre)\n   \n### [Lasso Grid Search results](#lassores)\n### [Decision tree Grid Search results](#DTres)\n### [Neural Networks results](#NNres)\n### [XGB GridSearching](#xgb)\n   #### [XGB Results](#xgbres)\n### [Lessons of the Exercise ](#FinCon)\n","a3db0531":"##### <a id='lassores'> Result of Lasso Grid Searching <\/a>\n\nThe result is obtained after taking the most influential parameters. Even after that, the results have not seen significant improvement","1f4e8caa":"#### <a id='medianval'> Updating the Energy Star nulls with Median values <\/a>\n\nWe will prepare the data with everything same, except the energy-star rating missing values updated to median values\n\n[1) Data prep](#Bdata)\n\n[2) Model Set](#Bmodset)\n\n[3) Result Visualisation](#Bresvis)\n\n[4) Final Observations](#Bfinpre)","4c6bf05c":"##### <a id='Bdata'> Data prep<\/a>","a4f921b2":"##### <a id='xgbres'> Result of XGB Grid Searching <\/a>","21e2c20c":"##### <a id='Bmodset'> Model Set <\/a>","208dbecf":"#### [Back to Contents](#contents)","db51121c":"##### <a id='xgb'> Setting XGB <\/a>","5ff874e7":"##### <a id='Bfinpre'> Final Observation <\/a>\n\nThe idea of changing null energy star rating to median values has not improved the results. We have resort to the next grid searching the Decision Tree and Lasso Regressor. ","323cc922":"##### <a id='Afinpre'> Final Observation <\/a>\n\nThe idea of removing the rows with null energy_star_ratings has not significantly improved the results. Time to try next idea. \n\nThe Random forest Regressors are all over fitting very badly. \n\nThe DecisionTree Regressors are having the lowest training error, which is fully overfitting. If the numbers of leaves are pruned, there is a possibility for the validation accuracy to improve. May be a grid search algorithm might help. \n\nSame way Lasso regressor grid search also might lead to better validation accuracy. Before that we will run another analysis with Energy star rating filled with the \"Median Values","00e25783":"#### [Back to Contents](#contents)","78a4dbfa":"#### [Back to Contents](#contents)","011b3f23":"### Purpose of the Notebook:\n\nPurely Data Science \n\nData can be used to generate useful insights, which is only the 1st part. The important part is ensuring the insights are statiscally valid. The best way is to test it on the real data. \n\nWe will be checking multiple ideas in this notebook, and understand how the results change based on that ideas. The modeling and testing harness will remain root mean squared error. [You will learn](#FinCon) that the insights and models that are finally presented are done after lots of analysis. \n\n### What to Expect\n\n\n[Lasso](#lassoset) and [Decision Tree](#modDT) Grid search, followed by [Neural Network modeling](#Nnet) are explored. The notebook tries the bruteforce method of finding the insight out of the WIDS Data. \n\nAfter all this, the root mean square error is still above [**56**](#NNres). \n\n\n### Sneek Peek\n\nThere will [Lasso grid search result](#lassores) and the [Decision Tree Grid Search result](#DTres) was an interesting exercise, without any improvement at this moment. Following the Neural Network was instantiated, which has [given further worse result](#NNres)\n\nPS: Use the blue links to go the exact location of the code and related activity","f5823dd8":"#### [Back to Contents](#contents)","58bfd1da":"##### <a id='Amodset'> Model Set <\/a>","f8fe196d":"#### [Back to Contents](#contents)","0270472b":"#### [Back to Contents](#contents)","3cc17fda":"#### [Back to Contents](#contents)","affcb307":"#### [Back to Contents](#contents)","118db0df":"1) Lasso Grid Searching, and further working on the model parameter optimisation has not provided any improvement the testing validation. \n\n2) Searching for the neural networks to see how they improve the validation result"}}