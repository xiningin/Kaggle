{"cell_type":{"6b2b9eac":"code","65b5624a":"code","4fc6cb31":"code","7e75c203":"code","4fc75ab6":"code","f6eab404":"code","f0647094":"code","ceb60b30":"code","ea72f6c7":"code","57e9439d":"code","0b5b35f2":"code","2fb33d3c":"code","0830b96d":"code","26e1a9d8":"code","15315457":"code","6108b2ef":"code","430f62a1":"code","09a93b93":"code","99184445":"code","b1e1dde5":"markdown","d47fd325":"markdown","f2e5ed80":"markdown","3052aa3c":"markdown","1aa870f0":"markdown","f4df02a5":"markdown","3bb36124":"markdown","88abfea2":"markdown","cc3a16f3":"markdown","9a4352ef":"markdown","bcdce5ec":"markdown","0c62cdeb":"markdown","7dc9b2b1":"markdown"},"source":{"6b2b9eac":"# useful imports\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\n%matplotlib inline\nplt.style.use('seaborn-whitegrid')","65b5624a":"# i'll have a look at the files in the dataset\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4fc6cb31":"# we read from GlobalTemperatures.csv using pandas' read_csv (comma separeted values)\n# we parse as dates the values in the column 'dt' (this will be useful later)\n# the method returns a DataFrame (pandas' two-dimensional data structure akin to a table)\nglobal_temperatures = pd.read_csv('\/kaggle\/input\/climate-change-earth-surface-temperature-data\/GlobalTemperatures.csv', parse_dates=['dt'])\nglobal_temperatures.head(3)","7e75c203":"# for semplicity's sake i'm going to calculate the mean temperature for each year\n# i use dataframe.groupby to which returns a 'DataFrameGroupBy' or 'SeriesGroupBy' i then apply mean() to get the mean temperatures\nmean_temp_year = global_temperatures.groupby(global_temperatures.dt.dt.year).mean()","4fc75ab6":"# making plot bigger\nplt.rcParams['figure.figsize'] = [14, 10]\n\n# showing averages and respective uncertainty bars\nplt.xlabel('year')\nplt.ylabel('temperature (\u00b0C)')\nplt.errorbar(mean_temp_year.index, mean_temp_year.LandAverageTemperature, yerr=mean_temp_year.LandAverageTemperatureUncertainty, \n             fmt='.', color='black', ecolor='lightblue', elinewidth=1)\nplt.show()","f6eab404":"# i want lists to easily package the data for scikit learn\ntemperatures = list(mean_temp_year.LandAverageTemperature)\nyears = list(mean_temp_year.index)\n\nx_train = []\ny_train = temperatures\nfor y in years:\n    x_train.append([y]) # fit wants a list of lists so i make one with the years values.\n\nreg = linear_model.Ridge(alpha=1) # i'll try first with alpha 1, we'll see later how changing alpha changes the result\nreg.fit(x_train, y_train) # i'm training and predicting on the entire set for now\nprediction = reg.predict(x_train)","f0647094":"plt.xlabel('year')\nplt.ylabel('temperature (\u00b0C)')\n\nplt.errorbar(mean_temp_year.index, mean_temp_year.LandAverageTemperature, yerr=mean_temp_year.LandAverageTemperatureUncertainty, fmt='.', color='black', ecolor='lightblue', elinewidth=1)\nplt.plot(x_train, prediction, color='green', linewidth='1')\nplt.show()","ceb60b30":"# i filter out entries with uncertainty > 1 degree celsius\nmean_temp_year = mean_temp_year[mean_temp_year.LandAverageTemperatureUncertainty <= 1]\n\nplt.xlabel('year')\nplt.ylabel('temperature (\u00b0C)')\nplt.errorbar(mean_temp_year.index, mean_temp_year.LandAverageTemperature, yerr=mean_temp_year.LandAverageTemperatureUncertainty, fmt='.', color='black', ecolor='lightblue', elinewidth=1)\nplt.show()","ea72f6c7":"# same as before\ntemperatures = list(mean_temp_year.LandAverageTemperature)\nyears = list(mean_temp_year.index)\n\nx_train = []\ny_train = temperatures\nfor y in years:\n    x_train.append([y])\n\nreg = linear_model.Ridge(alpha=1)\nreg.fit(x_train, y_train)\nprediction = reg.predict(x_train)","57e9439d":"plt.xlabel('year')\nplt.ylabel('temperature (\u00b0C)')\nplt.errorbar(mean_temp_year.index, mean_temp_year.LandAverageTemperature, yerr=mean_temp_year.LandAverageTemperatureUncertainty, fmt='.', color='black', ecolor='lightblue', elinewidth=1)\nplt.plot(x_train, prediction, color='green', linewidth='1')\nplt.show()","0b5b35f2":"temperatures = list(mean_temp_year.LandAverageTemperature)\nyears = list(mean_temp_year.index)\n\nx_train = []\ny_train = temperatures\nfor y in years:\n    x_train.append([y])\n\n# lets try with alpha = 0\nreg = linear_model.Ridge(alpha=0)\nreg.fit(x_train, y_train)\nprediction = reg.predict(x_train)\n\n# lets try with alpha = 50.000\nreg2 = linear_model.Ridge(alpha=50000)\nreg2.fit(x_train, y_train)\nprediction2 = reg2.predict(x_train)\n\n# lets try with alpha = 500.000\nreg3 = linear_model.Ridge(alpha=500000)\nreg3.fit(x_train, y_train)\nprediction3 = reg3.predict(x_train)","2fb33d3c":"plt.xlabel('year')\nplt.ylabel('temperature (\u00b0C)')\n\n# plotting on the same plot the different lines\nplt.errorbar(mean_temp_year.index, mean_temp_year.LandAverageTemperature, fmt='.', color='black', elinewidth=1)\np = plt.plot(x_train, prediction, color='lightblue')\np2 = plt.plot(x_train, prediction2, color='red')\np3 = plt.plot(x_train, prediction3, color='orange')\n\nplt.legend(['alpha = 0','alpha = 50.000','alpha = 500.000'], numpoints=1)\nplt.show()","0830b96d":"# i'll somewhat randomly devide the dataset in 2 sections, a big one and a small one\nimport random\n\ntemperatures = list(mean_temp_year.LandAverageTemperature)\nyears = list(mean_temp_year.index)\n\nsmall_x = []\nsmall_y = []\nbig_x =[]\nbig_y = []\nfor y, t in zip(years, temperatures):\n    choice = random.randint(0,99)\n    if choice <= 10: # ~ 1 in 10 will go here\n        small_x.append([y])\n        small_y.append(t)\n    else:\n        big_x.append([y])\n        big_y.append(t)\n\n# fitting the model on the small set and predicting on the big one\nreg = linear_model.Ridge(alpha=0)\nreg.fit(small_x, small_y)\nprediction = reg.predict(big_x)\nscore = reg.score(small_x, small_y)\n\n# trying a different alpha\nreg2 = linear_model.Ridge(alpha=10000)\nreg2.fit(small_x, small_y)\nprediction2 = reg2.predict(big_x)\nscore2 = reg2.score(small_x, small_y)\n\n# fitting the model on the big set and predicting on the small one\nreg3 = linear_model.Ridge(alpha=0)\nreg3.fit(big_x, big_y)\nprediction3 = reg3.predict(small_x)\nscore3 = reg3.score(big_x, big_y)\n\n# trying a different alpha\nreg4 = linear_model.Ridge(alpha=10000)\nreg4.fit(big_x, big_y)\nprediction4 = reg4.predict(small_x)\nscore4 = reg4.score(big_x, big_y)\n","26e1a9d8":"plt.rcParams['figure.figsize'] = [14, 10]\nplt.xlabel('year')\nplt.ylabel('temperature (\u00b0C)')\n\nplt.scatter(big_x, big_y, marker='o', color='black', s=12)\nplt.scatter(small_x, small_y, marker='s', color='gray', s=10)\np = plt.plot(big_x, prediction, color='blue')\np2 = plt.plot(big_x, prediction2, color='lightblue')\np3 = plt.plot(small_x, prediction3, color='green')\np4 = plt.plot(small_x, prediction4, color='lightgreen')\n\nleg = ['fit small, pred big | alpha = 0           | score = {}'.format(score),\n       'fit small, pred big | alpha = 10.000  | score = {}'.format(score2),\n       'fit big, pred small | alpha = 0           | score = {}'.format(score3), \n       'fit big, pred small | alpha = 10.000  | score = {}'.format(score4)\n      ]\n\nplt.legend(leg, numpoints=1)\nplt.show()","15315457":"# same as before\ntemperatures = list(mean_temp_year.LandAverageTemperature)\nyears = list(mean_temp_year.index)\n\nx_train = []\ny_train = temperatures\nfor y in years:\n    x_train.append([y])\n\nalphas = [0, 1, 50]\npredictions = []\nfor i in range(0, 3):\n    model = make_pipeline(PolynomialFeatures(2), linear_model.Ridge(alpha=alphas[i]))\n    model.fit(x_train, y_train)\n    predictions.append(model.predict(x_train))","6108b2ef":"plt.xlabel('year')\nplt.ylabel('temperature (\u00b0C)')\n\nplt.errorbar(mean_temp_year.index, mean_temp_year.LandAverageTemperature, fmt='.', color='black', elinewidth=1)\np = plt.plot(x_train, predictions[0], color='green')\np1 = plt.plot(x_train, predictions[1], color='red')\np2 = plt.plot(x_train, predictions[2], color='blue')\n\nplt.legend(['alpha = 0','alpha = 1','alpha = 50'], numpoints=1)\nplt.show()","430f62a1":"from sklearn import model_selection\n\n# same as before\ntemperatures = list(mean_temp_year.LandAverageTemperature)\nyears = list(mean_temp_year.index)\n\nx_train = []\ny_train = temperatures\nfor y in years:\n    x_train.append([y])\n\ndegrees = [1, 2, 4]\npredictions = []\n\n\n\nfor i in range(0, 3):\n    model = make_pipeline(PolynomialFeatures(degrees[i]), linear_model.Ridge(alpha=1))\n    model.fit(x_train, y_train)\n    predictions.append(model.predict(x_train))\n\nkfold = model_selection.KFold(n_splits=10, random_state=42)\nscore_model = make_pipeline(PolynomialFeatures(4), linear_model.Ridge(alpha=1))\nresults = model_selection.cross_val_score(score_model, x_train, y_train, cv=kfold, scoring='r2')\nprint(\"score_model {}\".format(results.mean()))","09a93b93":"plt.errorbar(mean_temp_year.index, mean_temp_year.LandAverageTemperature, fmt='.', color='black', elinewidth=1)\np = plt.plot(x_train, predictions[0], color='green')\np1 = plt.plot(x_train, predictions[1], color='red')\n\n\nplt.legend(['degree = 1','degree = 2'], numpoints=1)\nplt.show()","99184445":"plt.errorbar(mean_temp_year.index, mean_temp_year.LandAverageTemperature, fmt='.', color='black', elinewidth=1)\np1 = plt.plot(x_train, predictions[1], color='red')\np2 = plt.plot(x_train, predictions[2], color='blue')\n\nplt.legend(['degree = 2','degree = 4'], numpoints=1)\nplt.show()","b1e1dde5":"I'll now try and make polynomial interpolation using ridge regression, and try different values of alpha with a second degree polynomial","d47fd325":"# Trying out different values for alpha fitting the entire dataset\nLinear Ridge Regression tries to best fit the data with a set of data points with a line. \n\nTo do this the model minimizes the following equation:\n\n1\/n \\* || y - Xw ||^2 + alpha \\* || w ||^2\n\nThe first addend represents the empirical error. The second addend is there to avoid overfitting of the data: it introduces some bias into how the line fits the data.\nThis implies that the line doesn't fit as well the training data (because we are not minimizing the empirical error anymore) but test data may be better fit by this biased line.","f2e5ed80":"As we can see, for the same variation in alpha, a smaller training set gives a bigger difference in the bias shown in the line.","3052aa3c":"This notebook has the main goal of taking a brief look at the temperature data for the last few hundred years, and try out sci-kit learn, in particular the Ridge Regression linear method","1aa870f0":"# Applying Ridge Regression to the polished dataset","f4df02a5":"# First Ridge Regression method application\nI'll now apply the ridge regression model offered by scikit-learn to the data shown before ","3bb36124":"# Taking a first look at the data","88abfea2":"# Trying out different values for alpha fitting part of the dataset\nIn the following, i divide the dataset into two parts, one consists of ~ 90% of the dataset, and the other of the remainder.\n\nI then fit the model to the small set and predict on the big one, once with alpha = 0, once with alpha = 10.000\n\nThen i do the contrary: i fit the model to the big set and predict on the small one, again, once with alpha = 0, once with alpha = 10.000","cc3a16f3":"# Mean temperature for each year\nin the following plot, i show the mean temperature for each year since 1750. The blue bar associated to each data point, represents the uncertainty of the measurement.\nAs you can see, older measurements have a higher margin of error.","9a4352ef":"# Polynomial Ridge Regression","bcdce5ec":"# Removing data with high uncertainty","0c62cdeb":"As we can see the interpolations produced with alpha == 0 and alpha == 1 are almost equivalent\n\nNow i'll try to change degree of the polynomial","7dc9b2b1":"Playing around with alpha, i noticed chancing it by a few integers (eg 0 to 1) seem to give the same line. This may be caused by the scale of the plot."}}