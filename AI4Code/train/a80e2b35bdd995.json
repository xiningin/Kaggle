{"cell_type":{"19f01f7c":"code","e885f3d7":"code","0694e483":"code","af27e035":"code","db424968":"code","169a5d0f":"code","50fe4bbc":"code","162a8f1a":"code","f55b5841":"code","5204f2b1":"code","445cf82d":"code","2938cc84":"code","639a472b":"code","faaba263":"code","d502bfaa":"code","f01dde9d":"code","18dd3562":"code","20fc11b8":"code","7c2c1428":"code","c42d4235":"code","5599303a":"code","2e28a124":"code","d93a482e":"code","765d5be9":"code","5bdcf25d":"code","796b9d47":"code","da4a7770":"code","0e19a32b":"code","579b67eb":"code","912b6541":"code","0e40964b":"code","8e4e9903":"code","d752a4ca":"code","40f58bf6":"code","6cd6af0f":"code","d4572ddf":"code","221b4d5f":"code","7285c4ed":"code","52fadc1b":"code","6070a213":"code","303f90fb":"code","bb964cd2":"code","f877aa1e":"code","4f9f73d2":"code","01dcee6c":"code","d3e576d3":"code","f82fc175":"code","665d18de":"code","fc27732b":"code","33e95f61":"code","ed67c09f":"code","a5373273":"code","97002876":"code","c5e34a4a":"code","faaa34a1":"code","12b27808":"code","90d96897":"code","3bf759b9":"code","132bb5f5":"code","fd7f3f91":"code","83ea4ee5":"code","2b774431":"code","de690b11":"code","4db4857c":"code","9ce380f6":"code","514f00cd":"code","ffd5358f":"code","90886d03":"code","c9dfc978":"code","f7d06b54":"code","7874d370":"code","8489bd40":"code","3576c49a":"code","1fd48302":"code","a8ed4fab":"code","8dfe5dce":"code","2058d22a":"code","6fac090d":"code","a9b5096d":"code","4e40062c":"code","c785361e":"code","5cc0bd56":"code","3581c5db":"code","eca29816":"code","c17a966d":"code","63536fd6":"markdown","7ee702f1":"markdown","00bdd5dd":"markdown","ef3583c3":"markdown","57a20b2e":"markdown","4f8e3a10":"markdown","7e2bd44c":"markdown","2afb9cdd":"markdown","b0ddf345":"markdown","3a3ff739":"markdown","eecc20f7":"markdown","5cbaadde":"markdown","326e1e54":"markdown","91beccbf":"markdown","b35a8209":"markdown","80f44402":"markdown","cd237a34":"markdown","02169927":"markdown","1111e0df":"markdown","933ac806":"markdown","090cca97":"markdown","cc5499b0":"markdown","1ab87512":"markdown","4efe360b":"markdown","f1d35ec5":"markdown","bfa8a594":"markdown","b71cb4bb":"markdown","2796de2a":"markdown","5257f7de":"markdown","50a5b679":"markdown","7d7832d3":"markdown","c0e95331":"markdown","e5c8c7a1":"markdown","b84f8cc3":"markdown","d50138a1":"markdown","f7ebc70c":"markdown","75283238":"markdown","0403b8ab":"markdown","184a8a00":"markdown","eb99aa29":"markdown","1f0affd2":"markdown","483c9532":"markdown","b0b0601a":"markdown","a65ab177":"markdown","985ac6cd":"markdown","abc05689":"markdown","b81bf952":"markdown","4b38db76":"markdown","a35513d1":"markdown","3dbb0500":"markdown","6efab4f5":"markdown","47c282a9":"markdown","f0887d09":"markdown","b3819ced":"markdown","b4c03acc":"markdown","b688aad1":"markdown","043f098f":"markdown","6ca3de82":"markdown","0139b87e":"markdown","9feadf76":"markdown","78c6abe4":"markdown","75ae2181":"markdown","f184eec8":"markdown","8d1fcbaa":"markdown","a76c7d39":"markdown","d9456f9d":"markdown","0dbe660b":"markdown","e4e9ee4d":"markdown","d5e321e0":"markdown","c239a9af":"markdown","3ac7c572":"markdown","f31faf0b":"markdown","3d3e6e62":"markdown","4285900a":"markdown","e460f4ba":"markdown","1668e9bb":"markdown","d1a916b1":"markdown","44ce82dd":"markdown","c0a483db":"markdown","75148149":"markdown","fdf36755":"markdown","ad3049dd":"markdown","2c826c96":"markdown","e094f526":"markdown","b5e7c58a":"markdown","363a0cb3":"markdown","1c75b1c9":"markdown","bf114835":"markdown","f4f53801":"markdown","62b18a65":"markdown","59e010cf":"markdown","a41638c6":"markdown","0442af9a":"markdown","01412b1d":"markdown","abe94397":"markdown","fb92980c":"markdown","46a01533":"markdown","c1823e86":"markdown","6e48c1e1":"markdown","1ae134be":"markdown","23562f49":"markdown","7ef28934":"markdown","0a14037e":"markdown"},"source":{"19f01f7c":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e885f3d7":"!pip install vaderSentiment","0694e483":"!pip install flair","af27e035":"#general purpose packages\nimport numpy as np\nimport pandas as pd\npd.options.mode.chained_assignment = None\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport pycountry\n\nimport re, string\nimport emoji\n\n##sentiment analysis\n\n\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nfrom textblob import TextBlob\nfrom flair.models import TextClassifier\nfrom flair.data import Sentence\nfrom wordcloud import WordCloud, STOPWORDS\n\n\nsns.set_style(\"whitegrid\")\nsns.despine()\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlesize=14, titlepad=10)","db424968":"df = pd.read_csv(\"\/kaggle\/input\/omicron-rising\/omicron.csv\")","169a5d0f":"df.head()","50fe4bbc":"df.info()","162a8f1a":"df[df.duplicated()]","f55b5841":"df['date'] = pd.to_datetime(df['date'])","5204f2b1":"df['user_created'] = pd.to_datetime(df['user_created'])","445cf82d":"df = df.sort_values('date')","2938cc84":"df.head()","639a472b":"tweets_per_day = df['date'].dt.strftime('%m-%d').value_counts().sort_index().reset_index(name='counts')","faaba263":"plt.figure(figsize=(8,5.5))\nax = sns.barplot(x='index', y='counts', data=tweets_per_day,edgecolor = 'black',ci=False, palette='Blues_r')\nplt.title('Tweets count by date')\nplt.yticks([])\nax.bar_label(ax.containers[0])\nplt.ylabel('count')\nplt.xlabel('')\nplt.show()","d502bfaa":"df.tail(1)['date']","f01dde9d":"tweets_per_hour = df['date'].dt.strftime('%H').value_counts().sort_index().reset_index(name='counts')","18dd3562":"plt.figure(figsize=(8,5.5))\nax = sns.barplot(x='index', y='counts', data=tweets_per_hour,edgecolor = 'black',ci=False, palette='mako_r')\nplt.title('Tweets count by hour')\nplt.yticks([])\nax.bar_label(ax.containers[0])\nplt.ylabel('count')\nplt.xlabel('')\nplt.show()","20fc11b8":"df['user_location'].value_counts()","7c2c1428":"countries_list = []\nfor i in list(pycountry.countries):\n    countries_list.append(i.name)","c42d4235":"countries = []\nfor c1 in df.user_location.values:\n    if type(c1) != str:\n        countries.append(np.nan)\n    else:\n        cnt=0 ##counter to check if we arrived at the end of the country list\n        for c2 in countries_list:\n            if c2 in c1:\n                countries.append(c2)\n                break\n            else:\n                cnt+=1\n                if cnt== len(countries_list):\n                    countries.append(np.nan)","5599303a":"countries.count(np.nan)","2e28a124":"df['country'] = countries","d93a482e":"df[['country','user_location']]","765d5be9":"tweets_per_country = df['country'].value_counts().loc[lambda x : x > 30].reset_index(name='counts')","5bdcf25d":"plt.figure(figsize=(16.5,6.5))\nax = sns.barplot(x='index', y='counts', data=tweets_per_country,edgecolor = 'black',ci=False, palette='Spectral')\nplt.title('Tweets count by country')\nplt.xticks(rotation=70)\nplt.yticks([])\nax.bar_label(ax.containers[0])\nplt.ylabel('count')\nplt.xlabel('')\nplt.show()","796b9d47":"fav_tweets = df[['text','favorites']].sort_values('favorites', ascending=False)","da4a7770":"fav_tweets['text'].iloc[:10].values","0e19a32b":"retweets = df[['text','retweets']].sort_values('retweets', ascending=False)","579b67eb":"retweets['text'].iloc[:10].values","912b6541":"sia_vader = SentimentIntensityAnalyzer()","0e40964b":"sentiments = []\nfor tweet in df.text:\n    sentiment_dict = sia_vader.polarity_scores(tweet)\n    sentiment_dict.pop('compound', None)\n    sentiments.append(max(sentiment_dict , key=sentiment_dict.get))","8e4e9903":"df['sentiment'] = sentiments\ndf['sentiment'].value_counts()","d752a4ca":"##CUSTOM DEFINED FUNCTIONS TO CLEAN THE TWEETS\n\n#Clean emojis from text\ndef strip_emoji(text):\n    return re.sub(emoji.get_emoji_regexp(), r\"\", text) #remove emoji\n\n#Remove punctuations, links, mentions and \\r\\n new line characters\ndef strip_all_entities(text): \n    text = text.replace('\\r', '').replace('\\n', ' ').replace('\\n', ' ').lower() #remove \\n and \\r and lowercase\n    text = re.sub(r\"(?:\\@|https?\\:\/\/)\\S+\", \"\", text) #remove links and mentions\n    text = re.sub(r'[^\\x00-\\x7f]',r'', text) #remove non utf8\/ascii characters such as '\\x9a\\x91\\x97\\x9a\\x97'\n    banned_list= string.punctuation + '\u00c3'+'\u00b1'+'\u00e3'+'\u00bc'+'\u00e2'+'\u00bb'+'\u00a7'\n    table = str.maketrans('', '', banned_list)\n    text = text.translate(table)\n    return text\n\n#clean hashtags at the end of the sentence, and keep those in the middle of the sentence by removing just the # symbol\ndef clean_hashtags(tweet):\n    new_tweet = \" \".join(word.strip() for word in re.split('#(?!(?:hashtag)\\b)[\\w-]+(?=(?:\\s+#[\\w-]+)*\\s*$)', tweet)) #remove last hashtags\n    new_tweet2 = \" \".join(word.strip() for word in re.split('#|_', new_tweet)) #remove hashtags symbol from words in the middle of the sentence\n    return new_tweet2\n\n#Filter special characters such as & and $ present in some words\ndef filter_chars(a):\n    sent = []\n    for word in a.split(' '):\n        if ('$' in word) | ('&' in word):\n            sent.append('')\n        else:\n            sent.append(word)\n    return ' '.join(sent)\n\ndef remove_mult_spaces(text): ## remove multiple spaces\n    return re.sub(\"\\s\\s+\" , \" \", text)\n\ndef remove_spam(text):\n    match = re.search(r'subscribe', text)\n    if match:\n        return ''\n    else:\n        return text","40f58bf6":"texts_new = []\nfor t in df.text:\n    texts_new.append(remove_spam(remove_mult_spaces(filter_chars(clean_hashtags(strip_all_entities(strip_emoji(t)))))))","6cd6af0f":"df['text_clean'] = texts_new ","d4572ddf":"df['text_clean'] = df['text_clean'].str.lower() ","221b4d5f":"text_len = []\nfor text in df.text_clean:\n    tweet_len = len(text.split())\n    text_len.append(tweet_len)","7285c4ed":"df['text_len'] = text_len","52fadc1b":"plt.figure(figsize=(10,5))\nsns.histplot(x='text_len', data=df, bins=20)\nplt.title('Cleaned text lenght')\nplt.show()","6070a213":"plt.figure(figsize=(7,5))\nax = sns.countplot(x='text_len', data=df[df['text_len'] < 10], palette='mako')\nplt.title('Teets with less than 10 words')\nplt.yticks([])\nax.bar_label(ax.containers[0])\nplt.ylabel('count')\nplt.xlabel('')\nplt.show()","303f90fb":"df = df[df['text_len'] > 4]","bb964cd2":"sia_vader = SentimentIntensityAnalyzer()","f877aa1e":"sentiments_vader = []\nfor tweet in df.text_clean:\n    sentiment_dict = sia_vader.polarity_scores(tweet)\n    sentiment_dict.pop('compound', None)\n    sentiments_vader.append(max(sentiment_dict , key=sentiment_dict.get))","4f9f73d2":"df['sentiment_vader'] = sentiments_vader\ndf['sentiment_vader'].value_counts()","01dcee6c":"sia_nltk = SentimentIntensityAnalyzer()","d3e576d3":"sentiments_nltk = []\nfor tweet in df.text_clean:\n    sentiment_dict = sia_nltk.polarity_scores(tweet)\n    sentiment_dict.pop('compound', None)\n    sentiments_nltk.append(max(sentiment_dict , key=sentiment_dict.get))","f82fc175":"df['sentiment_nltk'] = sentiments_nltk\ndf['sentiment_nltk'].value_counts()","665d18de":"(df['sentiment_nltk'] == df['sentiment_vader']).sum() == len(df)","fc27732b":"def polarity_to_text(blob):\n    if (blob.sentiment.polarity > 0.1):\n        return 'pos'\n    elif(blob.sentiment.polarity <= 0.1 and  blob.sentiment.polarity >= -0.05):\n        return 'neu'\n    else:\n        return 'neg'","33e95f61":"sentiments_blob = []\nfor tweet in df.text_clean:\n    blob = TextBlob(tweet)\n    sentiments_blob.append(polarity_to_text(blob))","ed67c09f":"df['sentiment_blob'] = sentiments_blob\ndf['sentiment_blob'].value_counts()","a5373273":"flair_clf = TextClassifier.load('sentiment-fast');","97002876":"def emotion_threshold(emo, val):\n    if (emo =='POSITIVE') & (val >= 0.7): #outputs 'pos' only if positive confidence > 70%\n        return 'pos'\n    elif(emo =='NEGATIVE') & (val >= 0.7): #outputs 'neg' only if negative confidence > 70%\n        return 'neg'\n    else:\n        return 'neu' #else outputs neutral emotion","c5e34a4a":"sentiments_flair = []\nfor tweet in df.text_clean:\n    sentence = Sentence(tweet)\n    flair_clf.predict(sentence)\n    emo = re.findall(r\"([A-Z]\\w+)\",str(sentence.labels))[0] #extract the emotion\n    val = float(re.findall(r\"([+-]?[0-9]*[.]?[0-9]+)\",str(sentence.labels))[0]) #extract the confidence value for the emotion\n    sentiments_flair.append(emotion_threshold(emo,val))","faaa34a1":"df['sentiment_flair'] = sentiments_flair\ndf['sentiment_flair'].value_counts()","12b27808":"stopwords = [\"covid19\", \"case\",\"covid\",\"us\", \"will\",\"cases\", \"new\", 'variant', 'omicron','u','s','t'] + list(STOPWORDS)","90d96897":"texts_vader_pos = \" \".join(sentiment for sentiment in df[df['sentiment_vader']=='pos']['text_clean'])\ntexts_nltk_pos = \" \".join(sentiment for sentiment in df[df['sentiment_nltk']=='pos']['text_clean'])\ntexts_blob_pos = \" \".join(sentiment for sentiment in df[df['sentiment_blob']=='pos']['text_clean'])\ntexts_flair_pos = \" \".join(sentiment for sentiment in df[df['sentiment_flair']=='pos']['text_clean'])","3bf759b9":"wordcloud_vader_pos = WordCloud(width=800,\n                      stopwords=stopwords,\n                      height=400,\n                      max_font_size=200,\n                      max_words=50,\n                      collocations=False,\n                      background_color='black').generate(texts_vader_pos)\n\nwordcloud_nltk_pos = WordCloud(width=800,\n                      stopwords=stopwords,\n                      height=400,\n                      max_font_size=200,\n                      max_words=50,\n                      collocations=False,\n                      background_color='black').generate(texts_nltk_pos)\n\nwordcloud_blob_pos = WordCloud(width=800,\n                      stopwords=stopwords,\n                      height=400,\n                      max_font_size=200,\n                      max_words=50,\n                      collocations=False,\n                      background_color='black').generate(texts_blob_pos)\n\nwordcloud_flair_pos = WordCloud(width=800,\n                      stopwords=stopwords,\n                      height=400,\n                      max_font_size=200,\n                      max_words=50,\n                      collocations=False,\n                      background_color='black').generate(texts_flair_pos)","132bb5f5":"fig, ax = plt.subplots(2,2, figsize=(15,8))\nplt.suptitle('WORDCLOUD FOR POSITIVE TWEETS', fontsize=30)\n\nax[0,0].imshow(wordcloud_vader_pos, interpolation=\"bilinear\")\nax[0,0].axis(\"off\")\nax[0,0].set_title('Vader')\n\nax[0,1].imshow(wordcloud_nltk_pos, interpolation=\"bilinear\")\nax[0,1].axis(\"off\")\nax[0,1].set_title('NLTK')\n\nax[1,0].imshow(wordcloud_blob_pos, interpolation=\"bilinear\")\nax[1,0].axis(\"off\")\nax[1,0].set_title('TEXTBLOB')\n\n\nax[1,1].imshow(wordcloud_flair_pos, interpolation=\"bilinear\")\nax[1,1].axis(\"off\")\nax[1,1].set_title('FLAIR')\n\n\nplt.tight_layout()\nplt.show()","fd7f3f91":"df.text_clean[df['sentiment_vader']=='pos'].sample(10).values","83ea4ee5":"df.text_clean[df['sentiment_nltk']=='pos'].sample(10).values","2b774431":"df.text_clean[df['sentiment_blob']=='pos'].sample(10).values","de690b11":"df.text_clean[df['sentiment_flair']=='pos'].sample(10).values","4db4857c":"df_all_pos = df[(df['sentiment_vader']=='pos') & (df['sentiment_nltk']=='pos') & (df['sentiment_blob']=='pos') & (df['sentiment_flair']=='pos')]","9ce380f6":"len(df_all_pos)","514f00cd":"remove_spam('plz for giving us such a great opportunity i am supporting it alwayssuccess for the development')","ffd5358f":"df_all_pos['text_clean'].sample(10).values","90886d03":"texts_pos = \" \".join(sentiment for sentiment in df_all_pos['text_clean'])","c9dfc978":"wordcloud_pos = WordCloud(width=900,\n                      stopwords=stopwords,\n                      height=500,\n                      max_font_size=300,\n                      max_words=100,\n                      collocations=False,\n                      background_color='black').generate(texts_pos)","f7d06b54":"plt.figure(figsize=(9,5))\nplt.imshow(wordcloud_pos, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title('Positive tweets WORDCLOUD', fontsize=25)\nplt.show()","7874d370":"texts_vader_neg = \" \".join(sentiment for sentiment in df[df['sentiment_vader']=='neg']['text_clean'])\ntexts_nltk_neg = \" \".join(sentiment for sentiment in df[df['sentiment_nltk']=='neg']['text_clean'])\ntexts_blob_neg = \" \".join(sentiment for sentiment in df[df['sentiment_blob']=='neg']['text_clean'])\ntexts_flair_neg = \" \".join(sentiment for sentiment in df[df['sentiment_flair']=='neg']['text_clean'])","8489bd40":"wordcloud_vader_neg = WordCloud(width=800,\n                      stopwords=stopwords,\n                      height=400,\n                      max_font_size=200,\n                      max_words=50,\n                      collocations=False,\n                      background_color='black').generate(texts_vader_neg)\nwordcloud_nltk_neg = WordCloud(width=800,\n                      stopwords=stopwords,\n                      height=400,\n                      max_font_size=200,\n                      max_words=50,\n                      collocations=False,\n                      background_color='black').generate(texts_nltk_neg)\nwordcloud_blob_neg = WordCloud(width=800,\n                      stopwords=stopwords,\n                      height=400,\n                      max_font_size=200,\n                      max_words=50,\n                      collocations=False,\n                      background_color='black').generate(texts_blob_neg)\nwordcloud_flair_neg = WordCloud(width=800,\n                      stopwords=stopwords,\n                      height=400,\n                      max_font_size=200,\n                      max_words=50,\n                      collocations=False,\n                      background_color='black').generate(texts_flair_neg)","3576c49a":"fig, ax = plt.subplots(2,2, figsize=(15,8))\nplt.suptitle('WORDCLOUD FOR NEGATIVE TWEETS', fontsize=30)\n\nax[0,0].imshow(wordcloud_vader_neg, interpolation=\"bilinear\")\nax[0,0].axis(\"off\")\nax[0,0].set_title('Vader')\n\nax[0,1].imshow(wordcloud_nltk_neg, interpolation=\"bilinear\")\nax[0,1].axis(\"off\")\nax[0,1].set_title('NLTK')\n\nax[1,0].imshow(wordcloud_blob_neg, interpolation=\"bilinear\")\nax[1,0].axis(\"off\")\nax[1,0].set_title('TEXTBLOB')\n\n\nax[1,1].imshow(wordcloud_flair_neg, interpolation=\"bilinear\")\nax[1,1].axis(\"off\")\nax[1,1].set_title('FLAIR')\n\nplt.tight_layout()\n\nplt.show()","1fd48302":"df.text_clean[df['sentiment_vader']=='neg'].sample(10).values","a8ed4fab":"df.text_clean[df['sentiment_nltk']=='neg'].sample(10).values","8dfe5dce":"df.text_clean[df['sentiment_blob']=='neg'].sample(10).values","2058d22a":"df.text_clean[df['sentiment_flair']=='neg'].sample(10).values","6fac090d":"df_all_neg = df[(df['sentiment_vader']=='neg') & (df['sentiment_nltk']=='neg') & (df['sentiment_blob']=='neg') & (df['sentiment_flair']=='neg')]","a9b5096d":"len(df_all_neg)","4e40062c":"df_all_neg['text_clean'].sample(10).values","c785361e":"texts_neg = \" \".join(sentiment for sentiment in df_all_neg['text_clean'])","5cc0bd56":"wordcloud_neg = WordCloud(width=900,\n                      stopwords=stopwords,\n                      height=500,\n                      max_font_size=200,\n                      max_words=60,\n                      collocations=False,\n                      background_color='black').generate(texts_neg)","3581c5db":"plt.figure(figsize=(9,5))\nplt.imshow(wordcloud_neg, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title('Negative tweets WORDCLOUD', fontsize=25)\nplt.show()","eca29816":"fav_tweets = df[['text','favorites','sentiment_vader','sentiment_nltk','sentiment_blob','sentiment_flair']].sort_values('favorites', ascending=False)\nfav_tweets.iloc[:10]","c17a966d":"retweets = df[['text','retweets','favorites','sentiment_vader','sentiment_nltk','sentiment_blob','sentiment_flair']].sort_values('retweets', ascending=False)\nretweets.iloc[:10]","63536fd6":"**By far india has the most tweets in this dataset.**","7ee702f1":"We have 16323 missing countries: from the dataframe we could see that some tweets only have the city name as location.<br>\nThere are some libraries that allow to extract a country from a city name, however, in this work, for simplicity this will not be implemented.","00bdd5dd":"We note that the user location is not always a country: we can extract the country !","ef3583c3":"In this case, the sentences looks longer compared to those chosen by the previous algorithms, and do not present too many strong positive words. Nonetheless, they do not seem to be too positive, with some sarcasting positive tweets labeled as positive too.","57a20b2e":"# Tweets analysis","4f8e3a10":"Next, we will use TextBlob algorithm. Before instantiating the algorithm, we create a custom functon with threshold values to decide if a text is positive, negative or neutral.","7e2bd44c":"# Country analysis","2afb9cdd":"## NLTK","b0ddf345":"## TOP Retweeted Tweets","3a3ff739":"# Tweets Sentiment Analaysis ","eecc20f7":"The following project is about the Sentiment Analysis for recent tweets about the new Omicron Variant of COVID19. The aim is to understand the feeling of users about the new variant by analyzing the tweets.<BR>","5cbaadde":"# Flair Sentiment Analysis","326e1e54":"## TextBlob","91beccbf":"# Overall Positive Tweets Analysis","b35a8209":"# Vader Sentiment Analysis","80f44402":"We confirm that the sentiments chosen by Vader and NLTK are the same.","cd237a34":"As seen for TextBlob, we can see a large number of positive and negative labeled tweets compared to NLTK and Vader algorithms.<br>\n**We also notice that Flair is the only algorithm to label more negative tweets than positive, which would be expected since the tweets are about the new covid variant.**","02169927":"# Text cleaning","1111e0df":"## Are there duplicate tweets?","933ac806":"In the following, the tweets' text will be cleaned by custom defined functions.","090cca97":"There are 146 tweets labeled as positive by all the algorithms.","cc5499b0":"Next, we will use the NLTK Sentiment Analyzer.","1ab87512":"Moreover, we also make all the tweets to lower case.","4efe360b":"<img src=\"https:\/\/i.imgur.com\/qWPcQru.png\" width=\"1000px\">","f1d35ec5":"It looks like some tweets have very few words: let's analyze these tweets!","bfa8a594":"### How many missing countries?","b71cb4bb":"- Overall, the wordclouds containg the positive and negative tweets voted by all the algorithms seems to contain pretty emotional words (negative or positive). <br>Moreover, we could notice that lots of sentences labeled as positive by some algorithms (NLTK and Vader) are actually sarcastic about the new found cases of Omicron around the world. However, even if they present 'positve' words, their actual meaning is quite negative.\n- For the users with a specified country, India seems to be the country with most tweets. Of course the analysis of country is biased towards the users with a specified country, however it could make sense that India has most of the tweets after 2 cases of the Omicron variant have been found in India recently. The tweets from India could be caused by the panic of a potential crisis due to Omicron, similarly for what happened with the Delta Variant in the last months in India.\n- There is a peak of tweets registered at 11:00 am and 12:00 am, which depends on the GMT of the dataset scraper.","2796de2a":"# NLTK Sentiment Analysis","5257f7de":"**Judging by the wordcloud, it looks like NLTK and Vader identified as positive the sentences with strong emotional words like 'happy', 'great', 'good', 'please', 'friend'. <br>\nIn case of Textblob and flair, we cannot find any particular emotional word.**","50a5b679":"To start the sentiment analysis, we will plot the wordclouds of the tweets labeled as positive by each algorithm.<br>\nEDIT: After performing few tests with wordclouds, we decided to add some stopwords in order not to display common\/obvious words, such as 'covid' or 'omicron', shared by almost all tweets.","7d7832d3":"## Vader","c0e95331":"**As expected we can see that the latest tweet has been extracted on 12-08 at 7:35 AM, which is why there are less tweets about Omicron on 12-08 on the dataset.**","e5c8c7a1":"# Sentiment Analysis of Favorite and Retweeted tweets","b84f8cc3":"# Vader algorithm with cleaned text","d50138a1":"In the following, we will compare four different algorithms for Sentiment Analysis to indentify the sentiment of the tweets' text: Vader, NLTK, TextBLOB and Flair.","f7ebc70c":"It looks like this sentences are quite negative indeed. Let's get the wordcloud.","75283238":"Also in this case, the sentences look negative.<br>\nFinally, what could be done is to analyze the tweets that all the algorithms labeled as negative!","0403b8ab":"**Thank you for reading my notebook ! Let me know if you have some questions or if you want me to check out your works :)**","184a8a00":"It looks like these tweets present positive words even if they seem quite negative (and sarcastic).","eb99aa29":"We apply all the three custom functions to the raw text of the tweets.","1f0affd2":"- NLTK and Vader identified as positive the sentences with strong emotional words like 'happy', 'great', 'good', 'please', 'friend'.<br> In case of Textblob and flair, we cannot find any particular emotional word.\n\n- Judging by the wordcloud, it looks like NLTK and Vader identified as negative the sentences with strong emotional words like 'fear', 'death' and 'scam'. <br> In case of Textblob and flair, we can find less emotional word.\n\nMoreover, after a rough analysis of some tweets labeled as negative or positive by the 4 algorithms, we could confirm that the tweets chosen by Vader and NLTK tends to have more emotional words compared to those chosen by TextBLOB and Flair, which are longer and probably have a deeper emotional meaning, not represented just by some emotional buzzwords.<br>","483c9532":"# Favorite Tweets analysis","b0b0601a":"**We can see an increasing trend from 00:00 to 6:00 AM, a decreasing trend from 6:00 AM to 9:00 AM and a stiff increasing from 9:00 AM to 12:00 PM.<br>**","a65ab177":"## Which are the TOP 10 favorite tweets?","985ac6cd":"It could be interesting to see which sentiment the following algorithms will choose for these tweets!","abc05689":"# TextBlob Sentiment Analysis","b81bf952":"The sentences chosen by TextBLOB are quite longer compared to those chosen by the previous algorithms, and do not present lots of strong positive words. In particular, they do not to be too positive.","4b38db76":"# Negative Tweets Analysis by algorithm","a35513d1":"# Tweet counts per country","3dbb0500":"What we can do now is to check if there are some tweets with cleaned text with too few words! <br>\nTo do so, we create a list which will contain the number of words in each tweet and eventually plot it.","6efab4f5":"It looks like most of these sentences are quite negative by a first look. They present some negative words too.","47c282a9":"In case of NLTK algotihm, the sentences look more negative.","f0887d09":"## Which are the TOP 10 most retweeted tweets?","b3819ced":"**We can indeed see that the countries have been corerctly extracted from the user_location column.**","b4c03acc":"In the following, we will check which sentiment is associated by the 4 algorithms to the top 10 most favorite and retweeted tweets.","b688aad1":"# Wordcloud Results by algorithm","043f098f":"First, we will try to get the emotions from the raw tweets' text, keeping hashtags, external links etc...","6ca3de82":"There are lots of tweets with less than 5 words. We will remove these tweets.","0139b87e":"**NOTE: After performing some tests, I noticed that Flair only classifies a text as either 'positive' or 'negative' (no 'neutral'). For this reason, I decided to define a custom threshold function, so that we can include neutral emotions when the value for negative or positive are not too high (not top confident values).**","9feadf76":"# Main results Dashboard:","78c6abe4":"Next, it could be interesting to select few random sentences labeled as positive by the four algorithms and check how actually 'positive' they are.","75ae2181":"# Retweets  analysis","f184eec8":"First, we create a list of countries by using the pycountry library and a 'for' loop.","8d1fcbaa":"## Vader","a76c7d39":"# Tweet counts per day ","d9456f9d":"## Flair","0dbe660b":"<img src=\"https:\/\/i.imgur.com\/bI1nvvS.png\" width=\"1000px\">","e4e9ee4d":"****EDIT: Updated on December 23th with the new data ! ****","d5e321e0":"Great! the number of positive and negative tweets increased, while still the great majority of tweets are labeled as neutral by the algorithm.","c239a9af":"It looks like most of the tweets have been labeled as neutral, with very few positive and negative labeled tweets.<br>\n**Next, we will remove external entities such as hashtags, emojis and links in order to clean the tweets' text.**","3ac7c572":"It looks like this sentences are quite positive. Let's get the wordcloud.","f31faf0b":"Next, we will use Flair, a simple state-of-the-art natural language processing (NLP) library developed and open-sourced by Zalando Research.<br>\nIn particular, for this work we will use a pre trained model 'en-sentiment' (english sentiments) from the Flair library.","3d3e6e62":"Then, we can create a list 'countries' to host the extracted countries from the user_location column. <br>\nIn particular, nan values will be imputed in case the country can not be found in the column.","4285900a":"## Data Loading","e460f4ba":"# Overall Negative Tweets","1668e9bb":"## This project is still a Work in Progress!","d1a916b1":"We can see lots of positive words in the wordcloud.","44ce82dd":"All these tweets are about new infections of Omicron Variant found in Karnataka (India), Canada, US and Hong Kong.","c0a483db":"## Columns type analysis","75148149":"Good, it looks like there are no duplicate rows.","fdf36755":"# Positive Tweets Analysis by algorithm","ad3049dd":"In the following, we will analyze some random sentences that the algorithm labeled as negative to check the 'quality' of the labeling.","2c826c96":"Judging by the wordcloud, it looks like NLTK and Vader identified as negative the sentences with strong emotional words like 'fear', 'panic' and insults . <br>\nIn case of Textblob and flair, we cannot find any particular emotional word.","e094f526":"Similarly to the top 10 favorite tweets, also here the tweets are about new cases of Omicron (India, US, South Korea, China).","b5e7c58a":"Next, we will check if the Vader sentiment analaysis algorithm performs better with the cleaned tweets.","363a0cb3":"**The highest number of tweets about omicron is on the 7th of December. <br>\nIt is interesting to notice that the number of tweets between dec 13 and dec 23 is constant and equal to 3000:this could be due to tweets scraping limitations per day.**","1c75b1c9":"The senteces look quite negative.","bf114835":"The sentiments chosen by the algorithms are:\n- Vader and NLTK: all neutrals.\n- Textblob: 2 negatives.\n- Flair: 3 negatives","f4f53801":"## TextBLOB","62b18a65":"As before, it looks like these tweets present positive words even if they seem quite negative.","59e010cf":"## TOP Favorite Tweets","a41638c6":"We can notice a very large number of positve and negative labeled tweets compared to the previous algorithms.","0442af9a":"Judging by the number of chosen tweets, it looks like NLTK chose the same tweets as Vader.","01412b1d":"There are 15168 tweets, with some nulls values for the columns user_description, user_location and hashtags.\n- Missing user_description: some users do not have any description.\n- Missing user_location: some users did not specify location\n- Missing hashtags: some posts do not have any hashtag","abe94397":"## NLTK","fb92980c":"There are only 88 tweets labeled as negative by all the algorithms.","46a01533":"Finally, what could be done is to analyze those tweets that all the algorithms labeled as positive!","c1823e86":"We can see that there are two columns ('date', 'user_created') we can convert to pandas datetime type to simplify further analysis.","6e48c1e1":"The sentiments chosen by the algorithms are:\n- Vader and NLTK: all neutrals.\n- Textblob: 3 negatives.\n- Flair:  5 negatives.","1ae134be":"Indeed the words are quite negative.","23562f49":"# Tweet counts per hour","7ef28934":"## Flair","0a14037e":"# Twitter Sentiment Analysis about new Omicron Variant using Vader, NLTK, TextBLOB and Flair"}}