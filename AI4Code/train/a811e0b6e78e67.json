{"cell_type":{"0991d32a":"code","1a0383c8":"code","37a4d74b":"code","8be504d1":"code","bb163ef0":"code","4bf8c3b2":"code","0f0038d1":"code","ce01a40a":"code","81e2d13a":"code","18a82561":"code","47ed7865":"code","a933bca8":"code","444d566e":"code","6e4e9448":"code","7944ae5e":"code","02d21336":"code","82592a8d":"code","3f2cffd6":"code","b6586791":"code","0c54a24d":"code","90bbeb55":"code","8e1121f6":"code","6ec5c57b":"code","bf94cb11":"code","ca16a815":"code","9fdcd293":"code","6a92252f":"code","91b2a1ef":"code","8d4bf9a3":"code","49f2ca5f":"code","ed59624b":"code","ef6fe45a":"code","006d56ee":"code","0560aaaf":"code","13232f40":"code","5e7a2d1c":"code","f4799e75":"code","aa883bd9":"markdown","6d59dd58":"markdown","296063c4":"markdown","a369b34d":"markdown","ac1b2658":"markdown","f8364714":"markdown","07ea262e":"markdown","7000cb63":"markdown","e163b238":"markdown","88f21f2d":"markdown","27a0071e":"markdown","a534db69":"markdown","475c8412":"markdown","b1a39f16":"markdown","44060c10":"markdown","f7ad12b0":"markdown","177dbb23":"markdown","ae4856aa":"markdown","a9e49589":"markdown","aa7a6280":"markdown","77b500b0":"markdown","9e4bd286":"markdown","e37aa9c7":"markdown","da9260bf":"markdown","c1583c9a":"markdown","1e7f8441":"markdown","c88d98de":"markdown","bf3d0718":"markdown","5731da93":"markdown","01a8a908":"markdown","2d4d6015":"markdown","2c2a2700":"markdown","9c161f71":"markdown","4a79e4d9":"markdown","b9fdcf2d":"markdown","010ae14e":"markdown","b39438ad":"markdown","a67a5cec":"markdown","608d7960":"markdown"},"source":{"0991d32a":"import numpy as np, pandas as pd\nimport matplotlib.pyplot as plt, seaborn as sns\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LinearRegression\n\nsns.set()","1a0383c8":"raw_data = pd.read_csv(\"..\/input\/used-car-prices\/used_cars_prices.csv\")\nraw_data.head(4)","37a4d74b":"raw_data.describe(include = \"all\")  # to gain insight of all data","8be504d1":"data_p1 = raw_data.drop([\"Model\"], axis = 1)\ndata_p1.head(1)","bb163ef0":"data_p1.isnull().sum()","4bf8c3b2":"data_p2 = data_p1.dropna()\ndata_p2.describe(include = \"all\")","0f0038d1":"sns.distplot(data_p2.Price)","ce01a40a":"q = data_p2[\"Price\"].quantile(0.99)\ndata_p31 = data_p2[data_p2[\"Price\"] < q]\nsns.distplot(data_p31.Price)\ndata_p31.describe()","81e2d13a":"sns.distplot(data_p31.Mileage)","18a82561":"q = data_p31[\"Mileage\"].quantile(0.99)\ndata_p32 = data_p31[data_p31[\"Mileage\"] < q]\nsns.distplot(data_p32.Mileage)\ndata_p32.describe()","47ed7865":"sns.distplot(data_p32.EngineV)","a933bca8":"data_p33 = data_p32[data_p32[\"EngineV\"] < 6.5]\nsns.distplot(data_p33.EngineV)\ndata_p33.describe()","444d566e":"sns.distplot(data_p33.Year)","6e4e9448":"q = data_p33[\"Year\"].quantile(0.01)\ndata_p34 = data_p33[data_p33[\"Year\"] > q]\nsns.distplot(data_p34.Year)\ndata_p34.describe()","7944ae5e":"preprocessed_data = data_p34.reset_index(drop = True)\npreprocessed_data.head()","02d21336":"preprocessed_data.describe(include = \"all\")","82592a8d":"y, (x1, x2, x3) = plt.subplots(1, 3, sharey=True, figsize =(15,3))  # sharey - sharing what is on y-axis\n\nx1.scatter(preprocessed_data['Mileage'],preprocessed_data['Price'])\nx1.set_title('Price and Mileage')\nx2.scatter(preprocessed_data['EngineV'],preprocessed_data['Price'])\nx2.set_title('Price and EngineV')\nx3.scatter(preprocessed_data['Year'],preprocessed_data['Price'])\nx3.set_title('Price and Year')\n\nplt.show()","3f2cffd6":"preprocessed_data[\"LogPrice\"] = np.log(preprocessed_data[\"Price\"])\nlinear_data = preprocessed_data.drop([\"Price\"], axis = 1)\nlinear_data.head(3)","b6586791":"y, (x1, x2, x3) = plt.subplots(1, 3, sharey=True, figsize =(15,3))  # sharey - sharing what is on y-axis\n\nx1.scatter(linear_data['Mileage'], linear_data['LogPrice'])\nx1.set_title('LogPrice and Mileage')\nx2.scatter(linear_data['EngineV'], linear_data['LogPrice'])\nx2.set_title('LogPrice and EngineV')\nx3.scatter(linear_data['Year'], linear_data['LogPrice'])\nx3.set_title('LogPrice and Year')\n\nplt.show()","0c54a24d":"from statsmodels.stats.outliers_influence import variance_inflation_factor","90bbeb55":"check_variables = linear_data[[\"Mileage\", \"EngineV\", \"Year\"]]\nvif = pd.DataFrame()","8e1121f6":"vif[\"VIF\"] = [variance_inflation_factor(check_variables.values, i) for i in range(check_variables.shape[1])]\nvif[\"Features\"] = check_variables.columns\nvif = vif[[\"Features\", \"VIF\"]]\nvif","6ec5c57b":"qualified_data = linear_data.drop([\"Year\"], axis = 1)\nqualified_data.head(2)","bf94cb11":"data = pd.get_dummies(qualified_data, drop_first = True)\ndata.head(3)","ca16a815":"data.columns","9fdcd293":"data = data[['LogPrice', 'Mileage', 'EngineV', 'Brand_BMW', 'Brand_Mercedes-Benz',\n       'Brand_Mitsubishi', 'Brand_Renault', 'Brand_Toyota', 'Brand_Volkswagen',\n       'Body_hatch', 'Body_other', 'Body_sedan', 'Body_vagon', 'Body_van',\n       'Engine Type_Gas', 'Engine Type_Other', 'Engine Type_Petrol',\n       'Registration_yes']]\ndata.head(3)","6a92252f":"target = data[\"LogPrice\"]\ninputs = data.drop([\"LogPrice\"], axis = 1)","91b2a1ef":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(inputs)\n\nscaled_inputs = scaler.transform(inputs)","8d4bf9a3":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(scaled_inputs, target, test_size = 0.2, random_state = 365)","49f2ca5f":"reg = LinearRegression()  # making an object of LinearRegression()\n\nreg.fit(x_train, y_train)  # making a model to fit out training data\n\ny_hat = reg.predict(x_train)  # predicting prices for same input, to compare","ed59624b":"plt.scatter(y_train, y_hat)\n\nplt.xlabel(\"Actual Prices\", fontsize = 18)\nplt.ylabel(\"Predicted Prices\", fontsize = 18)\n\nplt.xlim(6, 13)  # setting scale on x-axis\nplt.ylim(6, 13)  # setting scale on y-axis\n\nplt.show()","ef6fe45a":"sns.distplot(y_train - y_hat)\nplt.title(\"Residuals PDF\")\nplt.show()","006d56ee":"reg.score(x_train, y_train)","0560aaaf":"reg_summary = pd.DataFrame(inputs.columns, columns = [\"Features\"])\nreg_summary[\"Weight\"] = reg.coef_\nreg_summary","13232f40":"y_hat_test = reg.predict(x_test)","5e7a2d1c":"plt.scatter(y_test, y_hat_test, alpha = 0.3)  # alpha changes tranparency of the points, so we can see where they are concentrating more\n\nplt.xlabel(\"Actual Prices\", fontsize = 18)\nplt.ylabel(\"Predicted Prices\", fontsize = 18)\n\nplt.xlim(6, 13)\nplt.ylim(6, 13)\n\nplt.show()","f4799e75":"pd.options.display.max_rows = 999  # to make each value print\n\npredictions = pd.DataFrame(np.exp(y_hat_test), columns = [\"Predicted Prices\"])\ny_test = y_test.reset_index(drop = True)\npredictions[\"Actual Prices\"] = np.exp(y_test)\npredictions[\"Residual\"] = predictions[\"Actual Prices\"] - predictions[\"Predicted Prices\"]\npredictions[\"Difference %\"] = np.absolute(predictions[\"Residual\"]\/predictions[\"Actual Prices\"]*100)\npredictions = predictions.sort_values(by = [\"Difference %\"], ascending = False)\npredictions","aa883bd9":"We can see the differnce as now maximum price is relatively more close to the mean.\n\nWe will do the same for other numerical data too.","6d59dd58":"Predictions calculated. Now, we can plot actual and predicted prices against each other on the scatter plot. For a good model, all points should be converging towards a straight 45 degree line from the origin of the graph.","296063c4":"The case on engine volume is a little different and here we need some commonsense or vehicle knowledge to address outliers. Upon a little googling we can find out there are no cars with engine volumes as high as 30\/40 litres. Some sports cars have bigger engines and even they are not more than 5-6 litres.\n\nThere is a very bad practice used while filling the data that if there is no data for a certain thing, 99.99 is filled at its place. This can be a possible answer to this problem.\n\nHowever, we will be dropping all observations with engine volumes more than 6.5 litres.","a369b34d":"Now, we can make a DataFrame with Actual Prices and Predicted Prices of the cars. And some more columns to make comparison clear.","ac1b2658":"Now, we have data that is clean and complies with OLS assumptions. We did not check for Autocorrelation because it is often found in time series data and since we do not have time series data here, we won't worry about that. For now, we also assume that we have not left any important variable out, so we can say there is no endogeneity. Normality and Homoscadesticity is present in data, so fifth assumptions is also qualified.","f8364714":"## 8. Train - Test Split:\nTo make sure that our model is not a underfitted or overfitted, we will test it with known targets. For this, we need to split the data into two groups one for train the model and one for testing it.","07ea262e":"### 3.3. Removing of Outliers\n\nThis is a very important step of the process as outliers have the ability to influence the model in great manner. Also, for a good linear regression model, our data should be Normally Distributed and with outliers at any end, it will not be possible.\n\nWe will be using Probability Distribution of the function to look for outliers.","7000cb63":"### 3.2. Dealing with Missing Values","e163b238":"## 2. Loading Raw Data","88f21f2d":"**Insights:**\n- There are 4345 observations.\n- Coloumns 'Price' and \"EngineV\" has some null values.\n- Some coloumns have outliers.","27a0071e":"## 1. Importing Libraries","a534db69":"It tells that around 75% of the variability of the target is defined by the feature that we have included in our model.","475c8412":"### 3.1. Dropping Unwanted Columns\n\nColumn 'Model' has 312 unique categorical values in 4345 observations. Creating a dummy value for each unique value can complicate our model, for now we can drop it.","b1a39f16":"We have trained the model and have made predictions for the same inputs as used for training to compare how close is our predicted prices to the actual prices.\n\nLet's scatter plot of Actual and Predicted Prices to compare. More observations close to a 45 degree line, better the model.","44060c10":"## 5. Dummy Variables for Categorical Data\nThere is just one thing left before we start regression analysis and that is creating dummy variables for categorical data.\n\nWe have get_dummies() function of pandas which takes in a DataFrame and returns one with dummy variables. We will call this regression ready data 'data'.","f7ad12b0":"## 11. Model Testing\n\nWith the model ready to predict, let's test it with the data the model has not seen yet. We can find how good the model performs as we know the actual dependent variables.","177dbb23":"We will only be checking numerical variables as categorical variables do not have interdependencies among them.\n\nFirst, we need to create a DataFrame of to-be checked variables and a DataFrame to insert VIF values in it.","ae4856aa":"In this graph we can clearly see that there are some prices on the higher side. These are our outliers.\n\nOne way to tackle high outliers is to drop top 0.5% or top 1% of the points. We will use 99% quantile for this.","a9e49589":"### 4.1. Linearity\nFirst up, we need to check whehter all of the features of our to be model are in linear relationship with the target or not.\n\nBest way to do this is plotting a scatter chart of each feature with the target individually. We can use plt.scatter() for this purpose but to make it more presentable and comparable we will be pltting all scatters in same line and since 'price' is common for each scatter chart, we will make an equation in which it is shared by all other regressors.","aa7a6280":"## 9. Training the Model\n\nWe are ready to train our model with training set of inputs and target.","77b500b0":"**Findings:** We have got VIF for each factor.\n\nVIF may range from 1 to inf.\n- VIF = 1 means no interdependency with any feature.\n- VIF = 1 - 5  means some interdependency, but okay.\n- VIF < 10 is acceptable for some people under some scenarios.\n- VIF > 10 is troublesome.\n\nYear has VIF > 10. And it seem fair, mileage and year are interdependent. One way to fix this is dropping one, let's drop 'Year'. We will call it qualified data after dropping 'Year'.","9e4bd286":"## 7. Scaling Data (Standardization)\nNext up is data standardization.\n\nWe can or can not standardize dummy variables as it does not change the weightage of them as features.","e37aa9c7":"Seems like a good model, doesn't it?\n\nAnother way to check how good the model is to plot a **Probability Distribution Function of Residuals**. (Residual = Observation - Prediction).","da9260bf":"## 6. Declaring Inputs and Target\nNow, we are ready to apply linear regression analysis on our data. First step in that direction is to declare what are the independent and dependent variables for the model.","c1583c9a":"**Preprocessing Complete!**\n\nWe are done with data preprocessing. We will save the resulted data in preprocessed_data variable and will be resetting index.","1e7f8441":"Now, we can see some linear relationship between target and regressors.","c88d98de":"Looking at these charts, we can say that none of the regressor seem to be in linear relationship with the target. To fix this problem we can transfor one of the variable in the charts. Since, price is common, let's transform it first. We will also be dropping column 'Price'. We will call new data frame linear_data.","bf3d0718":"Prices have been transformed. Let's plot scatter chart again with transformed prices.","5731da93":"### 4.2. Multicollinearity\nTo check whether our features have interdependencies, we use Variance Inflation Factor as its value can measure how much a feature depend on other features.\n\nWe have a method for that in statsmodels, let's import the required module for that method.","01a8a908":"A negative weight implies that more the feature is, the target will get reduced.\n\nFor dummies it a bit different, the dropped variable will have the benchmark weightage and weightage will show of others that how much more\/less they affect the price.","2d4d6015":"For the best case scenario, the residuals should be normally distributed. This graph is mostly okay. But, at the same time it has more residuals on the negative side of the graph. It suggests that our model has overestimated price more often than the time it has underestimated.\n\nAnother way to assess the model is to find its R-Squared.","2c2a2700":"## 4. Checking OLS Assumptions\nNow, that we have clean data, we need to check some assumptions that need to be true before applying Linear Regression.","9c161f71":"## 3. Data Preprocessing","4a79e4d9":"Little rearrangement to make 'LogPrice' first column.","b9fdcf2d":"There are a little over 300 missing values in the data. If missing values are < 5% of the data, the rule of thumb is, we can drop them.","010ae14e":"**Calculating VIF**","b39438ad":"## 10. Creating a Regression Summary\n\nA summary of regression will tell how and how much a feature affects the variability of the target.","a67a5cec":"# Car Price Prediction - Linear Regression\nIn this notebook, we will look at a real life application of linear reggression. We will be predicting used car prices, after making a model and train it with the data that we have.\n\nIn this practical example, we will go through each step like data preprocessing, checking for OLS assumptions, creating dummy variables to incorporate categorical data, model training and model testing.\n\nLet's start the process!","608d7960":"Year have a left skewed distribution, we will just drop bottom 1% for it."}}