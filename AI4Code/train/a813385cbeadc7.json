{"cell_type":{"efacd35a":"code","92cebb3c":"code","ca0c563a":"code","498cd3d0":"code","a0f601e9":"code","14876894":"code","44474f20":"code","35f072d5":"code","fe511c5b":"code","cf8e30ac":"code","b13f58e5":"code","44346f05":"code","8dcbfd9d":"code","81762450":"code","7944eee0":"code","0fb64640":"code","a23ec744":"markdown","71c20e94":"markdown","aaddc854":"markdown","f2cdbe11":"markdown","4d7e845f":"markdown","fc84bc11":"markdown"},"source":{"efacd35a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/hackathon-blossom-flower-classification\"))\n\n# Any results you write to the current directory are saved as output.","92cebb3c":"import matplotlib.pyplot as plt\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torchvision import datasets,transforms,models\nfrom torch import optim\n%matplotlib inline","ca0c563a":"train_transform=transforms.Compose([transforms.RandomRotation(20),\n                                   transforms.Resize((224,224)),\n                                   transforms.ToTensor(),\n                                   transforms.Normalize([0.485,0.456,0.406],\n                                                       [0.229,0.224,0.225])])\ntest_transform=transforms.Compose([transforms.Resize((224,224)),\n                                   transforms.ToTensor(),\n                                   transforms.Normalize([0.485,0.456,0.406],\n                                                       [0.229,0.224,0.225])])","498cd3d0":"dataTrain=datasets.ImageFolder('..\/input\/hackathon-blossom-flower-classification\/flower_data\/flower_data\/train',transform=train_transform)\ndataTest=datasets.ImageFolder('..\/input\/hackathon-blossom-flower-classification\/flower_data\/flower_data\/valid',transform=train_transform)\ntrainLoader=torch.utils.data.DataLoader(dataTrain,batch_size=32,shuffle=True)\ntestLoader=torch.utils.data.DataLoader(dataTest,batch_size=32)","a0f601e9":"def imshow(image, ax=None, title=None, normalize=True):\n    \"\"\"Imshow for Tensor.\"\"\"\n    if ax is None:\n        fig, ax = plt.subplots()\n    image = image.numpy().transpose((1, 2, 0))\n\n    if normalize:\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        image = std * image + mean\n        image = np.clip(image, 0, 1)\n\n    ax.imshow(image)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.tick_params(axis='both', length=0)\n    ax.set_xticklabels('')\n    ax.set_yticklabels('')\n\n    return ax","14876894":"images,labels=next(iter(trainLoader))\nnum=20\nimg=images[num]\nlbl=labels[num]\nprint('This is {}'.format(lbl))\nimshow(img)","44474f20":"model=models.resnet50(pretrained=True)\nmodel.cuda()\nmodel","35f072d5":"from collections import OrderedDict\nclassifier=nn.Sequential(OrderedDict([\n    ('fc1',nn.Linear(2048,1024)),\n    ('relu1',nn.ReLU()),\n    ('drop1',nn.Dropout(p=0.4)),\n    ('fc2',nn.Linear(1024,256)),\n    ('relu2',nn.ReLU()),\n    ('drop2',nn.Dropout(p=0.4)),\n    ('fc3',nn.Linear(256,128)),\n    ('relu3',nn.ReLU()),\n    ('drop3',nn.Dropout(p=0.4)),\n    ('fc4',nn.Linear(128,102)),\n    ('out',nn.LogSoftmax(dim=1))\n]))\nfor param in model.parameters():\n    param.requires_grad=False\nmodel.fc=classifier\nmodel","fe511c5b":"import json\nfile=json.load(open('..\/input\/hackathon-blossom-flower-classification\/cat_to_name.json'))\nNewDict={k: v for k, v in file.items()}\nNewDict","cf8e30ac":"classes=list()\nfor k,v in file.items():\n    classes.append(v)\n    \nclasses","b13f58e5":"criterion=nn.NLLLoss()","44346f05":"model.cuda()","8dcbfd9d":"lr=[0.003]\nminLoss=1000000\nfor i in lr:\n    model=models.resnet50(pretrained=True)\n    classifier=nn.Sequential(OrderedDict([\n        ('fc1',nn.Linear(2048,1024)),\n        ('relu1',nn.ReLU()),\n        ('drop1',nn.Dropout(p=0.4)),\n        ('fc2',nn.Linear(1024,256)),\n        ('relu2',nn.ReLU()),\n        ('drop2',nn.Dropout(p=0.4)),\n        ('fc3',nn.Linear(256,128)),\n        ('relu3',nn.ReLU()),\n        ('drop3',nn.Dropout(p=0.4)),\n        ('fc4',nn.Linear(128,102)),\n        ('out',nn.LogSoftmax(dim=1))\n    ]))\n    for param in model.parameters():\n        param.requires_grad=False\n    model.fc=classifier\n    model.cuda()\n    optimizer=optim.Adam(model.parameters(),i)\n    epochs=30\n    print(\"*********************************************************\")\n    print(\"For lr = {}\".format(i))\n    for j in range(epochs+1):\n        trainLoss=0.0\n        validLoss=0.0\n        for images,labels in trainLoader:\n            model.train()\n            images,labels=images.cuda(),labels.cuda()\n            optimizer.zero_grad()\n            output=model(images)\n            loss=criterion(output,labels)\n            trainLoss+=loss.item()\n            loss.backward()\n            optimizer.step()\n            \n        trainLoss=trainLoss\/len(trainLoader)\n        for imageValid,labelValid in testLoader:\n            model.eval()\n            imageValid,labelValid=imageValid.cuda(),labelValid.cuda()\n            with torch.no_grad():\n                log_ps=model(imageValid)\n                ValidLoss=criterion(log_ps,labelValid)\n                validLoss+=ValidLoss.item()\n                \n        validLoss=validLoss\/len(testLoader)\n        if j%2==0:\n            print(\"***************************************************\")\n            print('Training Loss= {:0.4f} \\t Validation Loss= {:0.4f}'.format(trainLoss,validLoss))\n                \n        if validLoss<minLoss:\n            print(\"************Saving Model**********************\")\n            torch.save(model.state_dict(),'checkpoint.pth')\n            bestlr=i\n            minLoss=validLoss","81762450":"test_loss = 0\nwith torch.no_grad():\n    for images,labels in testLoader:\n        images,labels=images.cuda(),labels.cuda()\n        output = model(images)\n        test_loss += criterion(output,labels).item()\n        pred = output.argmax(1, keepdim=True)\n    test_loss \/= len(testLoader.dataset)\n\n    print('\\nAverage loss: {:.4f}'.format(test_loss))","7944eee0":"def load_checkpoint(filepath, inference = False):\n    checkpoint = torch.load(filepath + 'checkpoint.pth')\n    model = checkpoint['model']\n    if inference:\n        for parameter in model.parameter():\n            parameter.require_grad = False\n        model.eval()\n    model.to(device)\n    return model","0fb64640":"\n\nfrom PIL import Image\n\nfiles = os.listdir(\"..\/input\/hackathon-blossom-flower-classification\/test set\/test set\/\")\n\nprediction_list = []\n\nfor file in files:\n    fullpath = \"..\/input\/hackathon-blossom-flower-classification\/test set\/test set\/\" + str(file)\n    with Image.open(fullpath) as f:\n        try:\n            img = test_transform(f)\n            img = img.unsqueeze(0)\n            with torch.no_grad():\n                img = img.to(device)\n                out = model(img)\n                output = torch.exp(out)\n                probs,top_class = output.topk(1, dim=1)\n                class_name = classes[(top_class.item() - 1)]\n                prediction_list.append([file, class_name[1], class_name[0]])\n                \n        except:\n            None\n            \ndf = pd.DataFrame(prediction_list, columns=['Image', 'Flower Name', 'Class Number'])\npd.set_option('display.max_colwidth', -1)\ndf\n\n","a23ec744":"**Importing Dependencies**","71c20e94":"**Creating Training and Validation Loaders**","aaddc854":"**Visualization**","f2cdbe11":"**Training Model**","4d7e845f":"**Model**","fc84bc11":"**Testing Model**"}}