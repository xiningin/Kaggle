{"cell_type":{"ac35eec2":"code","310abf8c":"code","8f6bea06":"code","8371fcf5":"code","1dbf99eb":"code","b33b8d0e":"code","edfe3f84":"code","debe3353":"code","066de15c":"code","7ee1c068":"code","8eadc7e6":"code","d30950ae":"code","997eafea":"code","8b375006":"code","50635ee1":"code","957979c0":"code","f2fa8ece":"code","ec98a78a":"code","a739c985":"code","9f865c2e":"code","10c756bd":"code","e238b17d":"code","ff6c0aa5":"code","b17936c7":"code","c9ed2476":"code","5d6f2dc5":"code","7e54ba9d":"code","f2866c84":"code","437a1912":"code","ad62ec84":"code","249cdfb9":"code","ebf6db8a":"code","dda2f27a":"code","a86afec6":"code","7f1a1eeb":"code","79e656c0":"code","fbcf4865":"code","700e00f3":"code","d55208a9":"code","a951e9e0":"code","3b182509":"code","c08d4d0a":"code","5ba3247b":"code","5972a7c7":"code","7a1f98df":"code","e965db81":"code","d43ce6a0":"code","3c1f055f":"code","02439b91":"code","6ed60216":"code","fd4c9554":"code","f0177b21":"code","c757820a":"code","e09ba1ec":"code","09e90d57":"code","5f77dfc1":"code","00452934":"code","4da255e9":"code","5abe46a5":"code","84892954":"markdown","e46d6667":"markdown","d4f45586":"markdown","ec7f4094":"markdown","367de7f5":"markdown","3a4f7e9e":"markdown","6e559548":"markdown","08693ed1":"markdown","1bf2eee4":"markdown","769de212":"markdown","b70ec981":"markdown","9602d19a":"markdown","c273bd16":"markdown","5828ed1a":"markdown"},"source":{"ac35eec2":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom skorch import NeuralNetClassifier\nfrom itertools import cycle\nfrom sklearn import svm, datasets\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom scipy import interp\nfrom sklearn.metrics import roc_auc_score","310abf8c":"#loading the dataset \ndf=pd.read_excel('CTG1.xls')","8f6bea06":"df","8371fcf5":"#deleting duplicate values \ndf.duplicated().sum()\ndf.drop_duplicates(keep=False,inplace=True) ","1dbf99eb":"#checking for null values \ndf.isnull().sum()","b33b8d0e":"#dropiing null values\ndf = df.dropna()\ndf.isnull().sum()","edfe3f84":"#dropping columns as they do not contribute to the prediction of fetal class . They have been selected based on domain knowledge \n# for example: columns such as FileName\",\"Date\",\"SegFile\",\"b\",\"e\" are particularly not useful for our classification objective  and are unique case to case\ndf=df.drop([\"FileName\",\"Date\",\"SegFile\",\"b\",\"e\",\"A\", \"B\",\"C\", \"D\" ,\"E\", \"AD\", \"DE\" ,\"LD\", \"FS\", \"SUSP\",\"CLASS\",\"DR\",\"LBE\"],axis=1)","debe3353":"#reference-https:\/\/etav.github.io\/python\/vif_factor_python.html\n#trying to use Vif to eliminate multicollinearity in the dataset \n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor \n\nvif = pd.DataFrame()\nvif[\"Features\"] = df.columns\nvif[\"vif\"] = [variance_inflation_factor(df.values,i) for i in range(len(df.columns))]","066de15c":"vif\n","7ee1c068":"#since the vif values for columns are really high we dropped them \n#dropping columns such as  as it\n#LBE=has duplicate values as LB \n#DR=has only zero as its values \ndf=df.drop([\"Median\",\"Mode\",\"Mean\",\"Width\",\"Min\",\"Max\",\"Nmax\",\"MSTV\",\"DL\"],axis=1)","8eadc7e6":"#Getting the Correlation \ncorr = df.corr(method='spearman')\nplt.figure(figsize=(20,10))\nsns.heatmap(corr, annot=True,cmap=\"inferno\")\ndf.columns","d30950ae":"# Histogram for all features\ndf_X = df.drop(columns=['NSP'])\nfig, ax = plt.subplots(1, 1, figsize=(15, 15))\ndf_X.hist(ax=ax)\nplt.show()","997eafea":"sns.countplot(x='NSP',data=df)","8b375006":"#ref-https:\/\/towardsdatascience.com\/pytorch-tabular-multiclass-classification-9f8211a123ab\n#relabelling the classes as class1 = class 0, Class 2= class 1, class3=class 2\nclass2idx = {\n    1:0,\n    2:1,\n    3:2\n}\n\nidx2class = {v: k for k, v in class2idx.items()}\n\ndf['NSP'].replace(class2idx, inplace=True)","50635ee1":"sns.countplot(x='NSP',data=df)","957979c0":"#defining X and Y \nX=df.drop([\"NSP\"],axis=1)\ny=df[\"NSP\"]","f2fa8ece":"#splitting the data to train(70 %)  and test(30 %) \nfrom sklearn.model_selection import train_test_split\nXtrain,Xtest,Ytrain,Ytest=train_test_split(X,y,test_size=0.3)","ec98a78a":"#Binarize labels in a one-vs-all fashion to use the Y_test_Roc values could be used while plotting the ROC curves \nfrom sklearn.preprocessing import label_binarize\nY_test_ROC = label_binarize(Ytest, classes=[0, 1, 2])\nprint(Y_test_ROC)","a739c985":"Xtrain.info()","9f865c2e":"#ref-https:\/\/machinelearningmastery.com\/smote-oversampling-for-imbalanced-classification\/\n#since the classes in our dataset were disbalanced we balanced them using borderline SMOTE \n# borderline-SMOTE for imbalanced dataset\nfrom collections import Counter\nfrom imblearn.over_sampling import BorderlineSMOTE\nfrom matplotlib import pyplot\nfrom numpy import where\n# summarize class distribution\ncounter = Counter(Ytrain)\nprint(counter)\n# transform the dataset\noversample = BorderlineSMOTE()\nXtrain, Ytrain = oversample.fit_resample(Xtrain, Ytrain)\n# summarize the new class distribution\ncounter = Counter(Ytrain)\nprint(counter)\n","10c756bd":"#WE scale all the features\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn import preprocessing\nimport numpy as np\nimport torch \nscaler = MinMaxScaler()\nXtrain = scaler.fit_transform(Xtrain)\nXtest = scaler.transform(Xtest)\nXtrain, Ytrain = np.array(Xtrain), np.array(Ytrain)\nXtest, Ytest = np.array(Xtest), np.array(Ytest)","e238b17d":"#import torch \nx_train = torch.tensor(Xtrain)\ny_train = torch.tensor(Ytrain)\nx_test = torch.tensor(Xtest)\ny_test = torch.tensor(Ytest)","ff6c0aa5":"#GridSearch for MLP classfier from SKlearn\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import recall_score\n           \nmlp=MLPClassifier(random_state=10)\ngrid_values = {'max_iter':[200],\n              \"hidden_layer_sizes\": [(11,11,11),\n                                     (128,64),\n                                     (32,16,8,4)],\n              \"solver\" : [\"adam\",\"sgd\"],\n              \"activation\": [\"relu\",\"sigmoid\"]}\ngrid_mlp=GridSearchCV(mlp,param_grid=grid_values,scoring='recall_macro',cv=5)","b17936c7":"grid_mlp.fit(Xtrain, Ytrain)","c9ed2476":"#our best parameters after gridsearch \ngrid_mlp.best_params_","5d6f2dc5":"grid_mlp.best_score_","7e54ba9d":"m_best=grid_mlp.best_estimator_\nypred2 = m_best.predict(Xtest)","f2866c84":"#getting the recall_score for oue best estimator\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\n\nprint(\"recall_score:\",recall_score(Ytest, ypred2,average='macro'))\nprint(\"accuracy_score:\",accuracy_score(Ytest, ypred_2))\nfrom sklearn.metrics import classification_report,confusion_matrix\nconfusion_matrix(Ytest, ypred2)","437a1912":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass ContraclassClassification(nn.Module):\n    def __init__(self,\n                dropout=0.5,\n                ):\n        \n        super(ContraclassClassification, self).__init__()\n        self.dropout = nn.Dropout(dropout)\n        \n        self.layer_1 = nn.Linear(12, 128)\n        self.layer_2 = nn.Linear(128, 64)\n        self.layer_out = nn.Linear(64, 3) \n        \n        \n    def forward(self, x):\n        x = F.relu(self.layer_1(x))\n        x = self.dropout(x)      \n        x = F.relu(self.layer_2(x))\n        x = self.dropout(x)\n        return x ","ad62ec84":"from skorch.callbacks import EpochScoring\n\nrecall_sc = EpochScoring(scoring='recall_macro', lower_is_better= False)\nnet = NeuralNetClassifier(\n    ContraclassClassification,\n    lr=0.1,\n    max_epochs=200,\n    criterion = torch.nn.modules.loss.CrossEntropyLoss,   \n    optimizer=torch.optim.Adam,\n    callbacks=[recall_sc]\n)\n ","249cdfb9":"#Grid Search for the below parameters \nfrom sklearn.model_selection import GridSearchCV\nparams={\n        'module__dropout':[0.5,0.1],\n        'lr':[0.01,0.05,0.1],\n        'max_epochs':[200],\n        'batch_size':[50,100],\n        'optimizer__weight_decay':[0.01,0.5]    \n}\ngs=GridSearchCV(net,params,refit=False,cv=3,scoring='recall_macro')\ngs.fit(x_train.float(), y_train.long())#comment it \nprint(gs.best_score_,gs.best_params_)","ebf6db8a":"print(gs.best_score_,gs.best_params_)","dda2f27a":"# Redoing our model with the parameters\n#{'batch_size': 100, 'lr': 0.01, 'max_epochs': 200, 'module__dropout': 0.1, 'optimizer__weight_decay': 0.01}\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ContraclassClassification(nn.Module):\n    def __init__(self,\n                dropout=0.1,\n                ):\n        \n        super(ContraclassClassification, self).__init__()\n        self.dropout = nn.Dropout(dropout)\n        \n        self.layer_1 = nn.Linear(12, 128)\n        self.layer_2 = nn.Linear(128, 64)\n        self.layer_out = nn.Linear(64, 3) \n        \n        \n    def forward(self, x):\n        x = F.relu(self.layer_1(x))\n        x = self.dropout(x)      \n        x = F.relu(self.layer_2(x))\n        x = self.dropout(x)\n        return x ","a86afec6":"#{'batch_size': 100, 'lr': 0.01, 'max_epochs': 200, 'module__dropout': 0.1, 'optimizer__weight_decay': 0.01}\nfrom skorch.callbacks import EpochScoring\n\nrecall_sc = EpochScoring(scoring='recall_macro', lower_is_better= False)\nnet1 = NeuralNetClassifier(\n    ContraclassClassification,\n    module__dropout=0.1,\n    lr=0.01,\n    max_epochs=200,\n    batch_size=100,\n    criterion = torch.nn.modules.loss.CrossEntropyLoss,   \n    optimizer=torch.optim.Adam,\n    optimizer__weight_decay= 0.01,\n    callbacks=[recall_sc]\n  \n)","7f1a1eeb":"#Gridsearch for optimal parameters by varying the learning rate and optimizer weight decay \nfrom sklearn.model_selection import GridSearchCV\nparams={\n    'lr':[0.01,0.001],\n    'optimizer__weight_decay':[0.001,0.01,0.1,1.0]    \n}\ngs_2=GridSearchCV(net1,params,refit=False,cv=3,scoring='recall_macro')\n\ngs_2.fit(x_train.float(), y_train.long())\nprint(gs_2.best_score_,gs_2.best_params_)","79e656c0":"print(gs_2.best_score_,gs_2.best_params_)","fbcf4865":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass ContraclassClassification(nn.Module):\n    def __init__(self,\n                dropout=0.1,\n                ):\n        \n        super(ContraclassClassification, self).__init__()\n        self.dropout = nn.Dropout(dropout)\n        \n        self.layer_1 = nn.Linear(12, 128)\n        self.layer_2 = nn.Linear(128, 64)\n        self.layer_out = nn.Linear(64, 3) \n        \n        \n    def forward(self, x):\n        x = F.relu(self.layer_1(x))\n        x = self.dropout(x)      \n        x = F.relu(self.layer_2(x))\n        x = self.dropout(x)\n        return x ","700e00f3":"#{'lr': 0.01, 'optimizer__weight_decay': 0.001}\nrecall_sc = EpochScoring(scoring='recall_macro', lower_is_better= False)\nnet2 = NeuralNetClassifier(\n       ContraclassClassification,\n       module__dropout=0.1,\n       lr=0.01,\n       max_epochs=200,\n       batch_size=100,\n       criterion = torch.nn.modules.loss.CrossEntropyLoss,   \n       optimizer=torch.optim.Adam,\n       optimizer__weight_decay= 0.001,\n       callbacks=[recall_sc]\n       \n  \n)","d55208a9":"#net2.fit(x_train.float(), y_train.long());","a951e9e0":"scores = cross_validate(net2,x_train.float(), y_train.long(),cv=5,scoring='recall_macro')","3b182509":"scores","c08d4d0a":"net2.fit(x_train.float(), y_train.long())","5ba3247b":"#train\ny_pred_train=net2.predict(x_train.float())\n#test\ny_pred_test=net2.predict(x_test.float())","5972a7c7":"from sklearn.model_selection import cross_val_predict\n#y_sc = cross_val_predict(net2, x_train.float(), y_train.long(), cv=5, method='predict_proba')","7a1f98df":"#getting the recall_score on train \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\n\nprint(\"recall_score:\",recall_score(y_train, y_pred_train,average='macro'))\nprint(\"accuracy_score:\",accuracy_score(y_train, y_pred_train))\nfrom sklearn.metrics import classification_report,confusion_matrix\nconfusion_matrix(y_train, y_pred_train)","e965db81":"#getting the recall_score on test \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\n\nprint(\"recall_score:\",recall_score(y_test, y_pred_test,average='macro'))\nprint(\"accuracy_score:\",accuracy_score(y_test, y_pred_test))\nfrom sklearn.metrics import classification_report,confusion_matrix\nconfusion_matrix(y_test, y_pred_test)\n","d43ce6a0":"#https:\/\/skorch.readthedocs.io\/en\/stable\/history.html\ntrain_loss = net2.history[:, 'train_loss']\nvalid_loss = net2.history[:, 'valid_loss']\nplt.figure(figsize=(9,9))\nplt.plot(train_loss,'-o',label='train')\nplt.plot(valid_loss,'-',label='valid')\nplt.xlabel('not sure-maybe epochs?')\n# Set the y axis label of the current axis.\nplt.ylabel('loss')\n# Set a title of the current axes.\nplt.title('loss curves for training and validation ')\nplt.legend()\nplt.show()","3c1f055f":"#ref-https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_roc.html\ny_pred_test_prob=net2.predict_proba(x_test.float())\n#print(ypred_prob)\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nn_classes=3\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(Y_test_ROC[:, i], y_pred_test_prob[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_test_ROC.ravel(), y_pred_test_prob.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\nlw = 2\n# First aggregate all false positive rates\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n# Then interpolate all ROC curves at this points\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(n_classes):\n    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\n# Finally average it and compute AUC\nmean_tpr \/= n_classes\n\nfpr[\"macro\"] = all_fpr\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n# Plot all ROC curves\nplt.figure()\n\nn_classes=3\ncolors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\nfor i, color in zip(range(n_classes), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n             label='ROC curve of class {0} (area = {1:0.2f})'\n             ''.format(i, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--', lw=lw)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Some extension of Receiver operating characteristic to multi-class')\nplt.legend(loc=\"lower right\")\nplt.show()","02439b91":"# cofusion matrix\nplt.subplots(figsize=(4,4))\ncf_matrix = confusion_matrix(y_test, y_pred_test)\n\ncmap = sns.diverging_palette(250, 10, s=80, l=55, n=9, as_cmap=True)\nsns.heatmap(cf_matrix, cmap=cmap,annot = True, annot_kws = {'size':15})","6ed60216":"from sklearn import svm \nclassifier=svm.SVC(kernel='rbf',gamma=0.1,C=1000)\nclassifier.fit(Xtrain,Ytrain)\npred2=classifier.predict(Xtest)","fd4c9554":"from sklearn.model_selection import GridSearchCV \nfrom sklearn.svm import SVC \n  \n# defining parameter range \nparam_grid = {'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf','linear','poly'],\n              'degree' : [2, 3, 4]\n             }\n               \n  \ngrid = GridSearchCV(SVC(), param_grid,scoring='recall_macro', refit = True,cv=10, verbose = 3) \n  \n# fitting the model for grid search \ngrid.fit(Xtrain,Ytrain) ","f0177b21":"# print best parameter after tuning \nprint(grid.best_params_) \n  \n# print how our model looks after hyper-parameter tuning \nprint(grid.best_estimator_) ","c757820a":"grid_predictions = grid.predict(Xtest) \n\n  \n# print classification report \nprint(classification_report(Ytest, grid_predictions))","e09ba1ec":"from sklearn import svm \nfrom sklearn.model_selection import cross_validate\nclassifier=svm.SVC(kernel='rbf',gamma=0.1,C=100,degree=2,probability=True)\nscores = cross_validate(classifier, Xtrain, Ytrain, scoring='recall_macro',cv=5)#cross validation implementation is it correct \nclassifier.fit(Xtrain,Ytrain)\n#pred2=classifier.cross_val_predict(Xtest)#error\npred_train=classifier.predict(Xtrain)\npred2=classifier.predict(Xtest)","09e90d57":"#getting the recall_score on train \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\n\nprint(\"recall_score:\",recall_score(Ytrain, pred_train,average='macro'))\nprint(\"accuracy_score:\",accuracy_score(Ytrain, pred_train))\nfrom sklearn.metrics import classification_report,confusion_matrix\nconfusion_matrix(Ytrain, pred_train)","5f77dfc1":"#getting the recall_score on test \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\n\nprint(\"recall_score:\",recall_score(Ytest, pred2,average='macro'))\nprint(\"accuracy_score:\",accuracy_score(Ytest, pred2))\nfrom sklearn.metrics import classification_report,confusion_matrix\nconfusion_matrix(Ytest, pred2)\n","00452934":"ypred2_prob=classifier.predict_proba(Xtest)\nprint(ypred_prob)","4da255e9":"\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nn_classes=3\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(Y_test_ROC[:, i], ypred_prob[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Y_test_ROC.ravel(), ypred_prob.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\nlw = 2\n# First aggregate all false positive rates\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n# Then interpolate all ROC curves at this points\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(n_classes):\n    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\n# Finally average it and compute AUC\nmean_tpr \/= n_classes\n\nfpr[\"macro\"] = all_fpr\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n# Plot all ROC curves\nplt.figure()\n\nn_classes=3\ncolors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\nfor i, color in zip(range(n_classes), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n             label='ROC curve of class {0} (area = {1:0.2f})'\n             ''.format(i, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--', lw=lw)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Some extension of Receiver operating characteristic to multi-class')\nplt.legend(loc=\"lower right\")\nplt.show()","5abe46a5":"#y_prob = mlp.predict_proba(X_test)\n#Y_test_ROC[:, i], ypred_prob[:, i])\n\nmacro_roc_auc_ovo = roc_auc_score(Y_test_ROC, Y_test_ROC, multi_class=\"ovo\",\n                                  average=\"macro\")\nweighted_roc_auc_ovo = roc_auc_score(Y_test_ROC, Y_test_ROC, multi_class=\"ovo\",\n                                     average=\"weighted\")\nmacro_roc_auc_ovr = roc_auc_score(Y_test_ROC, Y_test_ROC, multi_class=\"ovr\",\n                                  average=\"macro\")\nweighted_roc_auc_ovr = roc_auc_score(Y_test_ROC, Y_test_ROC, multi_class=\"ovr\",\n                                     average=\"weighted\")\nprint(\"One-vs-One ROC AUC scores:\\n{:.6f} (macro),\\n{:.6f} \"\n      \"(weighted by prevalence)\"\n      .format(macro_roc_auc_ovo, weighted_roc_auc_ovo))\nprint(\"One-vs-Rest ROC AUC scores:\\n{:.6f} (macro),\\n{:.6f} \"\n      \"(weighted by prevalence)\"\n      .format(macro_roc_auc_ovr, weighted_roc_auc_ovr))","84892954":"# ROC CURVES  and confusion matrix for our model","e46d6667":"# ROC Curve for SVM ","d4f45586":"despite of the grid search the final model seemed to overfit so we changed the gamma and the C values manually to get a more optimal scores","ec7f4094":"We have used multiple approaches using \n1)MLP classifier from Sklearn and creating a multilayer perceptron \n2)using pytorch with skorch  for our multilayer perceptron","367de7f5":"# Multi layer Perceptron -using pytorch ","3a4f7e9e":"# Final MLP model","6e559548":"GridSearch for MLP classfier for SKlearn\n1)we have tried to experiment with different parameters to knw our best  hidden layer sizes, activation functions, and solver for weight optimization.\n2)Since False Negatives are important in our case scoring parameter is recall_macro","08693ed1":"Architechure 1 -\n1)We have used activation function Relu  for our 2 hidden layers\n2)we used dropout as 0.5 ","1bf2eee4":"# SVM  implementation ","769de212":"# final model","b70ec981":"# MULTILAYER PERCEPTRON ","9602d19a":"after getting the best parameters from the grid search we have tried to further narrow down our search for optimal parameters by varying the learning rate and optimizer weight decay ","c273bd16":"# 1st MLP classifier approach ","5828ed1a":"Despite of having a multiclass classification we have not used softmax on our output layer because of use of\"crossentropyloss\" \nAccording to the documentation torch.nn.CrossEntropyLoss() combines nn.LogSoftmax() and nn.NLLLoss() in one single class"}}