{"cell_type":{"f07d2e25":"code","a591ce2b":"code","3521bed2":"code","87741288":"code","92ce3021":"code","43fec4f7":"code","0a2ca2db":"code","fc4c5130":"code","b015ae44":"code","c1521636":"code","630511c2":"code","f8fef70d":"code","4c17721c":"code","505e78ce":"code","43d9566c":"code","3658ffc8":"code","8dd0275f":"code","235facd3":"code","5382b3f0":"code","18b87a32":"markdown","50b3cb57":"markdown","763fdf91":"markdown","843d3f8f":"markdown","9cf70f58":"markdown","847c7bd0":"markdown","363cc6b5":"markdown","31f9c160":"markdown","f23ec37f":"markdown","0bd73127":"markdown","527e5b54":"markdown","8115173e":"markdown","278e9d42":"markdown","f5b38d49":"markdown","8bbaa56e":"markdown","d5811b5a":"markdown","f60cc33c":"markdown","dd1bc7f4":"markdown","32c4f821":"markdown","6eaef672":"markdown","2657cc2c":"markdown","f5fb4ba2":"markdown","589c1f12":"markdown","2b30096f":"markdown","73b5c570":"markdown","30c33937":"markdown","493d4010":"markdown","0c7e0064":"markdown","2d7551ab":"markdown","15f48925":"markdown","468a97ec":"markdown","782e05e3":"markdown","82f18a42":"markdown","d68301ab":"markdown","22a6a57b":"markdown","5b7aebf6":"markdown","bf664a14":"markdown","01315ab6":"markdown","e15ce81b":"markdown","fe48459a":"markdown"},"source":{"f07d2e25":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import confusion_matrix, classification_report","a591ce2b":"hfp = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')","3521bed2":"hfp.head()","87741288":"hfp.describe()","92ce3021":"hfp.info()","43fec4f7":"fig = plt.figure(figsize=[16, 9])\nplt.style.use('seaborn-darkgrid')\nsns.heatmap(hfp.corr(), annot=True, mask=np.triu(hfp.corr()), cmap='coolwarm')","0a2ca2db":"hfp.drop('time', axis=1, inplace=True)\nhfp.head()","fc4c5130":"%matplotlib inline\n\n_ = [1, 3, 5, 9, 10]\n_ = list(enumerate([list(hfp.columns)[i] for i in _], start=1))\n\nfig = plt.figure(figsize=[18,12])\nfor index, col_name in _:\n    ax = fig.add_subplot(2, 3, index)\n    sns.countplot(x=col_name, data=hfp, hue=\"DEATH_EVENT\", palette='magma')\n\nax = fig.add_subplot(2, 3, 6)\nsns.countplot(x='DEATH_EVENT', data=hfp, palette='magma')","b015ae44":"%matplotlib inline\n\n_ = [0, 2, 4, 6, 7, 8]\n_ = list(enumerate([list(hfp.columns)[i] for i in _], start=1))\n_\n\nfig = plt.figure(figsize=[18,12])\nfor index, col_name in _:\n    ax = fig.add_subplot(2, 3, index)\n    sns.distplot(hfp[col_name])","c1521636":"%matplotlib inline\n\n_ = [0, 2, 4, 6, 7, 8]\n_ = list(enumerate([list(hfp.columns)[i] for i in _], start=1))\n_\n\nfig = plt.figure(figsize=[18,12])\nfor index, col_name in _:\n    ax = fig.add_subplot(2, 3, index)\n    sns.boxplot(x=col_name, y='DEATH_EVENT', data=hfp, palette=\"coolwarm\", orient='h')","630511c2":"hfp_dfs = [hfp.copy() for i in range(3)]","f8fef70d":"# Classifying as Hyponatremia (1) for Serum Sodium levels of less than 135 mEq\/L or normal (0)\nhfp_dfs[1]['serum_sodium'] = hfp_dfs[1]['serum_sodium'].apply(lambda x: 1 if x < 135 else 0)\n\n# Classifying as elevated Serum Creatinine (1) for levels of more than 1.47 mg\/dl or normal (0)\nhfp_dfs[1]['serum_creatinine'] = hfp_dfs[1]['serum_creatinine'].apply(lambda x: 1 if x >= 1.47 else 0)","4c17721c":"# Creating three data sets, which will be stored as a dictionary \n# of the format {'X_train': df, 'X_test': df, 'y_train': Series, 'y_test': Series}\n\nkey_names = ['X_train', 'X_test', 'y_train', 'y_test']\n\nfor i in range(3):\n    X = hfp_dfs[i].drop('DEATH_EVENT', axis=1)\n    y = hfp_dfs[i]['DEATH_EVENT']\n    \n    _ = train_test_split(X,y,test_size=0.2,random_state=100)\n    _ = dict(zip(key_names, _))\n    \n    hfp_dfs[i] = _","505e78ce":"scaler = RobustScaler()\nscaler.fit(hfp_dfs[2]['X_train'])\n\nhfp_dfs[2]['X_train'] = scaler.transform(hfp_dfs[2]['X_train'])\nhfp_dfs[2]['X_test'] = scaler.transform(hfp_dfs[2]['X_test'])","43d9566c":"hfp_dfs[2]['X_train'] = pd.DataFrame(hfp_dfs[2]['X_train'], columns=['age', 'anaemia', 'creatinine_phosphokinase', 'diabetes', \n                                                                     'ejection_fraction', 'high_blood_pressure', 'platelets', \n                                                                     'serum_creatinine', 'serum_sodium', 'sex', 'smoking'])\n\nhfp_dfs[2]['X_test'] = pd.DataFrame(hfp_dfs[2]['X_test'], columns=['age', 'anaemia', 'creatinine_phosphokinase', 'diabetes', \n                                                                     'ejection_fraction', 'high_blood_pressure', 'platelets', \n                                                                     'serum_creatinine', 'serum_sodium', 'sex', 'smoking'])","3658ffc8":"model_list = []\n\nfor i in range(3):\n    \n    log_model = LogisticRegression(max_iter=100000)\n    log_model.fit(hfp_dfs[i]['X_train'], hfp_dfs[i]['y_train'])\n    \n    predictions = log_model.predict(hfp_dfs[i]['X_test'])\n    \n    print(classification_report(hfp_dfs[i]['y_test'], predictions))\n    print(confusion_matrix(hfp_dfs[i]['y_test'], predictions))\n    \n    model_list.append(log_model)","8dd0275f":"rf_list = []\n\nfor i in range(3):     \n    rfc = RandomForestClassifier(n_estimators=100)\n    rfc.fit(hfp_dfs[i]['X_train'], hfp_dfs[i]['y_train'])\n\n    rfc_pred = rfc.predict(hfp_dfs[i]['X_test'])\n\n    print(classification_report(hfp_dfs[i]['y_test'], rfc_pred))\n    print(confusion_matrix(hfp_dfs[i]['y_test'], rfc_pred))\n    \n    rf_list.append(rfc)","235facd3":"class_w = {0:1, 1:3}\n\nmodel_list_1 = []\n\nfor i in range(3):\n    \n    log_model = LogisticRegression(max_iter=100000, class_weight=class_w)\n    log_model.fit(hfp_dfs[i]['X_train'], hfp_dfs[i]['y_train'])\n    \n    predictions = log_model.predict(hfp_dfs[i]['X_test'])\n    \n    print(classification_report(hfp_dfs[i]['y_test'], predictions))\n    print(confusion_matrix(hfp_dfs[i]['y_test'], predictions))\n    \n    model_list_1.append(log_model)","5382b3f0":"rf_list_1 = []\n\nfor i in range(3):     \n    rfc = RandomForestClassifier(n_estimators=100, class_weight=class_w)\n    rfc.fit(hfp_dfs[i]['X_train'], hfp_dfs[i]['y_train'])\n\n    rfc_pred = rfc.predict(hfp_dfs[i]['X_test'])\n\n    print(classification_report(hfp_dfs[i]['y_test'], rfc_pred))\n    print(confusion_matrix(hfp_dfs[i]['y_test'], rfc_pred))\n    \n    rf_list_1.append(rfc)","18b87a32":"***","50b3cb57":"Transformation 1: Transformation of variables","763fdf91":"<font size=\"3\">Observations:<\/font>\n\n1. Age of patient - The risk of developing heart disease increases with age, as pointed out in the following article - https:\/\/www.nia.nih.gov\/health\/heart-health-and-aging#:~:text=Adults%20age%2065%20and%20older,risk%20of%20developing%20cardiovascular%20disease\n\n\n2. Percentage of blood leaving the heart at each contraction - Heart failure with reduced ejection fraction happens when the muscle of the left ventricle is not pumping as well as normal (ejection fraction is 40% or less), and hence the negative correlation makes sense - https:\/\/www.uofmhealth.org\/health-library\/tx4090abc#:~:text=A%20normal%20ejection%20fraction%20is,fraction%20is%2040%25%20or%20less\n\n\n3. Level of serum creatinine in the blood (mg\/dL) - High serum creatinine levels are associated with heart failure - https:\/\/www.onlinejcf.com\/article\/S1071-9164(02)25410-X\/fulltext#:~:text=A%20prior%20study%20from%20this,of%20stay%20and%20mortality%20risk\n\n\n4. Level of serum sodium in the blood (mEq\/L) - Hyponatremia (serum sodium concentration of <135 mEq\/L) is one of the crucial factors in the clinical prognosis of heart failure patients - https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC6224129\/\n\n\n5. Follow-up period (days) - This is an interesting feature, since it has a high correlation with DEATH_EVENT. However, upon closer inspection this feature essentially reports the days after taking measurements the patient died (1) or lost contact (0). In a scenario where we want to predict a patient outcome, we will not have this variable, and hence it makes sense to drop this variable altogether.","843d3f8f":"<b><font size=\"4\">1. Correlation Matrix<\/font><\/b>","9cf70f58":"<font size=\"3\">Observations:<\/font>\n\n1. This is an unbalanced classification problem i.e. there are fewer cases of deaths due to heart failure as compared to survival (~31% of total)\n\n\n2. Only high_blood_pressure variable seems to have a noticeable effect on the DEATH_EVENT variable\n\n\n3. Other variables do not seem to impart much information. However, we will still keep these variables as we don't have an issue of dimensionality, and the variables are not highly correlated to each other as we observed in the correlation matrix above. Another consideration is that the classification accuracy may reduce if we drop these variables.","847c7bd0":"# Setting up","363cc6b5":"***","31f9c160":"# Data Visualizations","f23ec37f":"We will run Logistic Regression and Random Forest classification algorithms on all of our datasets, comparing performance across different levels of feature engineering and exploring further parameters we can modify to improve our prediction","0bd73127":"***","527e5b54":"Tranformation 2: Applying RobustScaler","8115173e":"<font size=\"3\">Importing data<\/font>","278e9d42":"<font size=\"3\">The above heatmap gives us a sense of what variables could possibly help us predicting the incidence of a cardiac arrest. We can see what these variables mean.<\/font>","f5b38d49":"<b><font size='4'>Next Steps<\/font><\/b>","8bbaa56e":"<b><font size=\"4\">Logistic Regression with class weights<\/font><\/b>","d5811b5a":"<b><font size=\"4\">Random forests with class weights<\/font><\/b>","f60cc33c":"<b><font size=\"4\">Methodology<\/font><\/b>","dd1bc7f4":"<b><font size=\"10\"><center>Heart Failure Prediction<\/center><\/font><\/b>","32c4f821":"The following studies were helpful in getting intuition as to how we can better use some of the features in our dataset\n\n1. Serum creatinine - According to a study (https:\/\/www.ahajournals.org\/doi\/full\/10.1161\/01.str.28.3.557) published in 1997 done on middle-aged men, elevated serum creatinine level is linked to a higher risk of CVD. Higher risk of CVD was observed where serum creatinine levels are higher than 130 \u00b5mol\/L, or 1.47 mg\/dL*\n\n    <font size=\"1\">*Conversion units taken as 1 mg\/dL = 0.0113 \u00b5mol\/L - http:\/\/www.endmemo.com\/medical\/unitconvert\/Creatinine.php<\/font>\n\n\n2. Serum Sodium - A 2018 (https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC6224129\/) study links heart failure to Hyponatremia (serum sodium concentration of less than 135 mEq\/L)","6eaef672":"Making three copies of the data set","2657cc2c":"![banner-918218_1920.jpg](attachment:banner-918218_1920.jpg)","f5fb4ba2":"Creating Random Forest models on our datasets","589c1f12":"<font size=\"3\">Structure of our data<\/font>","2b30096f":"<font size=\"3\">Observations:<\/font>\n\n1. In both the above set of visualizations, we can see that serum_creatinine and creatinine_phosphokinase variables have a lot of outliers. We will see if we can deal with these by scaling. A min max scaler will not work since it is affected by extreme values, and will not do much as far as reducing variance is concerned.\n\n\n2. Another data transformation to explore would be convert serum_creatinine and creatinine_phosphokinase into a binary categorical variables, basis certain cutoff point. A category value of 1 would indicate elevated creatinine levels or Hyponatremia respectively.","73b5c570":"<b><font size=\"4\">Random Forest<\/font><\/b>","30c33937":"<font size=\"4\">This database has been sourced from kaggle (link: https:\/\/www.kaggle.com\/andrewmvd\/heart-failure-clinical-data). The goal is to see whether we can predict heart failure from the features available to us. Since this effort requires a binary categorical prediction, we will use two commonly applied classification algorithms - Logistic Regression and Random Forests. We will also explore if we can improve performance by feature engineering, and compare performance across.<\/font>\n\n\n### References\n\n1. Ahmad, Tanvir; Munir, Assia; Bhatti, Sajjad Haider; Aftab, Muhammad; Ali Raza, Muhammad (2017): DATA_MINIMAL.. PLOS ONE. Dataset. https:\/\/doi.org\/10.1371\/journal.pone.0181001.s001 \n\n\n2. Chicco, D., Jurman, G. Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone. BMC Med Inform Decis Mak 20, 16 (2020). https:\/\/doi.org\/10.1186\/s12911-020-1023-5","493d4010":"***","0c7e0064":"Splitting into testing and training data sets","2d7551ab":"In a real world use case, we would ideally want to go with the Logistic Regression model, as even though the overall accuracy may be less, the high recall on predicted class means that there will be fewer misclassifications of patients who are actually at a risk of heart failure. This will have a positive effect on patient care outcomes.  ","15f48925":"<b><font size=\"4\">Methodology<\/font><\/b>","468a97ec":"1. Get more data points\n\n\n2. Hyperparameter tuning\n\n\n3. Oversampling the imbalanced class with techniques (for e.g. SMOTE)","782e05e3":"<b><font size=\"4\">Logistic Regression<\/font><\/b>","82f18a42":"# Data Pre-Processing","d68301ab":"Creating Logistic Regression Models on our datasets","22a6a57b":"***","5b7aebf6":"# Modeling","bf664a14":"<font size=\"3\">Observations:<\/font>\n\n1. We see that Logistic Regression performs best on dataset with transformed variables. However, even though we have an overall accuracy of 70%, our recall on the predicted class (1) is 48%. Hence, we are only classifying the incidence of heart failure half the time, which is not good performance. This could be due to unbalanced classes, and not enough instances of heart failure for the model to learn from.\n\n\n2. We get considerable improvement from Random Forest over Logistic Regression, both in terms of accuracy and recall. This could be due to Random Forests being inherently better with unbalanced classes due to drawing samples repeatedly.\n\n\n3. Applying class weights to the cost function did not really improve upon the results much in terms of accuracy. However, we achieved around 80% recall on the predicted class (1), which could be worth the trade-off on accuracy since we would want to error on the side of caution.","01315ab6":"We will create 3 separate datasets with different levels of feature engineering\n\n\n- Data set 1: No transformations\n- Data set 2: Converting serum_sodium and serum_creatinine to binary variables\n- Data set 3: Applying RobustScaler","e15ce81b":"<font size=\"3\">Data Processing and Visualization Modules<\/font>\n\n1. Numpy\n2. Pandas\n3. Seaborn\n4. Matplotlib\n\n<font size=\"3\">Machine Learning Modules<\/font>\n\n1. Model Selection\n2. Logistic Regression\n3. Random Forest Classifier\n4. Robust Scaler\n5. Confusion Matrix\n6. Classification Report","fe48459a":"# Conclusion"}}