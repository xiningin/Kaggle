{"cell_type":{"17e7c940":"code","191f9f1f":"code","182561d8":"code","0a636009":"code","babee7ad":"code","fb6e7fb0":"code","cf222924":"code","c7aafd11":"code","02d57c0b":"code","b95bbf45":"code","fb676471":"code","61a6fc85":"code","05197b6a":"code","07d9238e":"code","e46e94ad":"code","5c91d86f":"code","5db40d26":"code","a61334c9":"code","9e9787aa":"code","c094ab05":"code","b7cca804":"code","ee58440a":"code","b1adf953":"code","5f464b83":"code","1cf076d4":"code","31ea76ce":"code","5478b55f":"code","ad2ca856":"code","8b308f9b":"code","d28c95ff":"code","b59c1acd":"code","adf57a95":"code","206d0fc7":"code","bda3a851":"code","1ed1c867":"code","2a111f90":"code","e2f8c015":"code","f633ea31":"code","dffbf6d8":"code","912a2ba8":"code","33cd4518":"code","5c250406":"code","fc268a03":"code","e3bd1a01":"code","d75aff29":"code","82d7bd4c":"code","7647aa3b":"code","7c0f76cf":"code","75cce5ad":"code","3a33c061":"code","3f348db4":"code","06fdf28c":"code","614a3d48":"code","012ccd88":"code","488fbadc":"code","70b34ae0":"code","ee4fa35c":"code","4494533c":"code","d6d5f11a":"code","68c61c7c":"code","500d784a":"code","6c19517d":"code","3de0768b":"code","8ef25fe6":"code","7e0593cc":"code","7ef71da4":"code","56abbd23":"code","bf503184":"code","3e8345c5":"code","6aa24bf4":"code","40875da6":"code","7e5ff75a":"code","bc6b5446":"code","f2dfe255":"code","9324f1da":"code","9ac83643":"code","f7edd3f9":"code","2e4fb057":"code","63ac9a0e":"code","7c2f8a97":"code","d7729d31":"code","1019455c":"code","0944848e":"code","90250963":"code","08b9231e":"code","337105b1":"code","400279a7":"code","5e8f8f04":"code","0fa75c2e":"code","38de7338":"code","1285295d":"code","34e4fb0d":"code","d413c262":"code","47851229":"code","4f663d66":"code","b006d43a":"code","cdaf5ca4":"code","1ff11607":"code","bd5af7ec":"code","af73660b":"code","ed6e6788":"code","20b7b597":"code","c3009a84":"code","f815c958":"code","65f69ba6":"code","172d828d":"code","612e5cd1":"code","6346df62":"code","ddbb288a":"code","2d36e94b":"code","8fc335a5":"code","01f9bbfa":"code","dbac901c":"code","c2a235a5":"code","89114268":"markdown","036d01b8":"markdown","f3512e2f":"markdown","5b1ce814":"markdown","4b752b59":"markdown","4da2e1e0":"markdown","6eb325b4":"markdown","8754ef70":"markdown","ea39e89f":"markdown","f273f11b":"markdown","001697cd":"markdown","e07c74ee":"markdown","fac664bb":"markdown","cd29cf38":"markdown","d2c9150d":"markdown","00c17936":"markdown","2827de8b":"markdown","1254d140":"markdown","92c326f8":"markdown","3eb9bb0b":"markdown","d9811d6a":"markdown","d5bb8674":"markdown","1d681f52":"markdown","cb18a5b5":"markdown","5b500a6c":"markdown","78c40e1f":"markdown","c3656e79":"markdown","673ac43e":"markdown","8d70edbd":"markdown","c8f5aadd":"markdown","683079a7":"markdown","fa662184":"markdown","059e11ff":"markdown","822db2df":"markdown","b25d844f":"markdown","04115e5d":"markdown","743aeaa3":"markdown","29165730":"markdown","b7c10a57":"markdown","857d2d71":"markdown","cf250123":"markdown","fc047280":"markdown","92eab3ae":"markdown","dd282f83":"markdown","78ac3107":"markdown","9e90f3f6":"markdown","f92cef5b":"markdown"},"source":{"17e7c940":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder\nfrom sklearn.neighbors  import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV,cross_val_score\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_curve, auc\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","191f9f1f":"#data_predict=pd.read_csv('Admission_Predict.csv')\ndata_predict=pd.read_csv('..\/input\/Admission_Predict_Ver1.1.csv')","182561d8":"print('Data First 5 Rows\\n')\ndata_predict.head()","0a636009":"print('Data Last 5 Rows\\n')\ndata_predict.tail()","babee7ad":"#random rows in dataset\nprint('Data Random Rows\\n')\ndata_predict.sample(5)","fb6e7fb0":"print(\"Random rows in dataset\\n\")\ndata_predict.sample(frac=.1)","cf222924":"print('Data Show Describe\\n')\ndata_predict.describe()","c7aafd11":"print('Data Show Info\\n')\ndata_predict.info()","02d57c0b":"print('Data Show Columns')\ndata_predict.columns","b95bbf45":"data_predict=data_predict.rename(columns={'Serial No.':'SerialNo','GRE Score':'GREScore','TOEFL Score':'TOEFLScore','LOR ':'LOR','University Rating':'UniversityRating','Chance of Admit ':'ChanceOfAdmit'})","fb676471":"data_predict.dtypes","61a6fc85":"print('Data Show Shape')\ndata_predict.shape","05197b6a":"data_predict.isnull().values.any()","07d9238e":"print('Data Show Is NULL')\ndata_predict.isnull().sum()","e46e94ad":"for col in data_predict.columns:\n    print(data_predict[data_predict[col].isnull()])","5c91d86f":"print('The result is that the value of SeriaNo feature does not have any significance.')\nprint('Because it is an increasing value and personalized does not matter any.')\n\nlen(data_predict['SerialNo'].unique())","5db40d26":"sns.pairplot(data_predict)\nplt.show()","a61334c9":"corr = data_predict.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()","9e9787aa":"plt.figure(figsize=(10, 10))\nsns.heatmap(data_predict.corr(), annot=True, linewidths=0.05, fmt= '.2f',cmap=\"magma\")\nplt.show()","c094ab05":"import numpy as np\ncorr = data_predict.corr()\nfig, ax = plt.subplots(figsize=(8, 8))\ncolormap = sns.diverging_palette(220, 10, as_cmap=True)\ndropSelf = np.zeros_like(corr)\ndropSelf[np.triu_indices_from(dropSelf)] = True\ncolormap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, cmap=colormap, linewidths=.5, annot=True, fmt=\".2f\", mask=dropSelf)\nplt.show()","b7cca804":"data_predict=data_predict.drop(['SerialNo'],axis=1) #1 column 0 row","ee58440a":"data_predict.corr()","b1adf953":"data_predict.iloc[:,:6].corr()","5f464b83":"#show data columns\ndata_predict.columns","1cf076d4":"#Research group 1, ChanceOfAdmit value of all TOEFLScore data by grouping the number of data is performed.\nplt.figure(figsize=(10,5))\nresearch_predict=data_predict[data_predict['Research']==1.0].groupby('ChanceOfAdmit')['TOEFLScore'].count()\nsns.barplot(x=research_predict.index,y=research_predict.values)\nplt.ylabel('Frequency')\nplt.xticks(rotation=90)\nplt.show()","31ea76ce":"#Research group 1, ChanceOfAdmit value of all TOEFLScore data by grouping the number of data is performed.\nplt.figure(figsize=(10,5))\nresearch_predict=data_predict[data_predict['Research']==1.0].groupby('ChanceOfAdmit')['TOEFLScore'].count()\nresearch_predict=research_predict.sort_values(ascending=True)\nsns.barplot(x=research_predict.index,y=research_predict.values)\nplt.ylabel('Frequency')\nplt.xticks(rotation=90)\nplt.show()","5478b55f":"data_predict.head(1)","ad2ca856":"max(data_predict['UniversityRating'])","8b308f9b":"filter_data=data_predict[np.logical_and(data_predict['ChanceOfAdmit']==1.0,data_predict['UniversityRating']==max(data_predict['UniversityRating']))]","d28c95ff":"filter_data.groupby('UniversityRating')[['GREScore','TOEFLScore','CGPA']].mean()","b59c1acd":"plt.figure(figsize=(20,25))\ni = 0\n\nfor item in data_predict.columns:\n    i += 1\n    plt.subplot(4, 2, i)\n    sns.distplot(data_predict[item], rug=True, rug_kws={\"color\": \"b\"},kde=True,\n                 kde_kws={\"color\": \"blue\", \"lw\": 5, \"label\": \"KDE\"},\n                 hist_kws={\"histtype\": \"step\", \"linewidth\": 3,\"alpha\": 1, \"color\": \"blue\"},label=\"{0}\".format(item))\n#     sns.distplot(admission_v1[item], kde=True,label=\"{0}\".format(item))\nplt.show()","adf57a95":"for i,col in enumerate(data_predict.columns):\n    plt.subplot(4,2,i+1)\n    plt.scatter(np.arange(1,501),data_predict[col].values.tolist())\n    plt.title(col)\n    fig, ax = plt.gcf(), plt.gca()\n    fig.set_size_inches(10, 10)\n    plt.tight_layout()\nplt.show()","206d0fc7":"# Basic correlogram\nsns.pairplot(data_predict)\nplt.show()","bda3a851":"sns.violinplot(data_predict, palette=\"Set3\", bw=.2, cut=1, linewidth=1)\n\nax.set(ylim=(-.7, 1.05))\nsns.despine(left=True, bottom=True)\nplt.xlabel(\"Values\")\nplt.show()","1ed1c867":"data_predict.head()","2a111f90":"plt.figure(figsize=(7,7))\nfor i,col in enumerate(data_predict.columns):\n    plt.title(col)\n    plt.subplot(4,2,i+1)\n    sns.swarmplot(data_predict[col])\n    ax,fig=plt.gcf(),plt.gca()\n    plt.tight_layout()\nplt.show()","e2f8c015":"data_lor=data_predict[data_predict['LOR']>4.5]\nsns.relplot(x=\"TOEFLScore\", y=\"UniversityRating\", hue=\"Research\",\n            sizes=(40, 400), alpha=.5,\n            height=6, data=data_lor)","f633ea31":"# Show each distribution with both violins and points\nsns.lineplot(data_lor.Research,data_lor.TOEFLScore,linewidth=2.5)\nplt.xticks(rotation=90)\nplt.show()","dffbf6d8":"data_predict.head()","912a2ba8":"# Show the joint distribution using kernel density estimation\ng = sns.jointplot(data_predict.TOEFLScore, data_predict.UniversityRating, kind=\"kde\", height=7, space=0)\nplt.show()","33cd4518":"# Draw a categorical scatterplot to show each observation\nsns.swarmplot(y=\"TOEFLScore\", x=\"SOP\", hue=\"ChanceOfAdmit\",\n              palette=[\"r\", \"c\", \"y\"], data=data_predict)\nplt.show()","5c250406":"fig = sns.regplot(x=\"GREScore\", y=\"CGPA\", data=data_predict)\nplt.title(\"GRE Score vs CGPA\")\nplt.show()","fc268a03":"#data show first five rows\ndata_predict.head()","e3bd1a01":"#We analyze the status of the CGPA and LOR data.\nplt.scatter(x=np.arange(1,501),y=data_predict['CGPA'],color='b')\nplt.scatter(x=np.arange(1,501),y=data_predict['LOR'],color='r')\nplt.show()","d75aff29":"data_predict.columns","82d7bd4c":"#Although there are exceptions, people with higher CGPA usually have higher GRE scores maybe because they are smart or hard working\nfig = sns.lmplot(x=\"CGPA\", y=\"LOR\", data=data_predict, hue=\"Research\")\nplt.title(\"GRE Score vs CGPA\")\nplt.show()","7647aa3b":"data_predict.describe().plot(kind = \"area\",fontsize=27, figsize = (20,8), table = True,colormap=\"rainbow\")\nplt.xlabel('Statistics',)\nplt.ylabel('Value')\nplt.title(\"General Statistics of Admissions\")\nplt.show()","7c0f76cf":"fig = sns.regplot(x=data_predict[\"GREScore\"], y=data_predict[\"TOEFLScore\"])\nplt.title(\"GRE Score vs TOEFL Score\")\nplt.show()","75cce5ad":"data_predict.columns","3a33c061":"sns.lineplot(x=\"GREScore\", y=\"ChanceOfAdmit\",\n             data=data_predict,color='b',label='ChanceOfAdmit')\nsns.lineplot(x=\"GREScore\", y=\"SOP\",\n             data=data_predict,color='r',label='SOP')\nsns.lineplot(x=\"GREScore\", y=\"LOR\",\n             data=data_predict,color='G',label='LOR')\nplt.legend(loc=2)\nplt.show()","3f348db4":"sns.countplot(data_predict.Research.value_counts())\nplt.show()","06fdf28c":"data_predict[data_predict['ChanceOfAdmit']>0.85].groupby('Research')['UniversityRating'].median()","614a3d48":"sns.countplot(x='Research', hue='UniversityRating', data=data_predict)\nplt.show()","012ccd88":"data_predict.head()","488fbadc":"plt.figure(figsize=(10,10))\nsns.countplot(data_predict.GREScore,hue=data_predict.Research)\nplt.title('GREScore for Hue Research Show')\nplt.xticks(rotation=90)\nplt.show()","70b34ae0":"sns.catplot(y=\"CGPA\", x=\"UniversityRating\", hue=\"Research\", data=data_predict)\nplt.show()","ee4fa35c":"# Plot the responses for different events and regions\nsns.lineplot(y=\"CGPA\", x=\"UniversityRating\",\n             hue=\"Research\",data=data_predict)\nplt.show()","4494533c":"print(\"Max GREScore :\",max(data_predict.GREScore))\nprint(\"Min GREScore :\",min(data_predict.GREScore))\n\n#We will now perform an analysis with GREScore. In this analysis, \n#we will give ourselves a certain range and we will proceed with these intervals.\n","d6d5f11a":"maxvalues=data_predict[data_predict.GREScore>=320]\nmeanvalues=data_predict[(data_predict.GREScore>295)&(data_predict.GREScore<320)]\nminvalues=data_predict[(data_predict.GREScore<=295)]","68c61c7c":"print(len(maxvalues))\nprint(len(meanvalues))\nprint(len(minvalues))","500d784a":"y=np.array([len(maxvalues),len(meanvalues),len(minvalues)])\nx=['Max Values','Mean Values','Min Values']\ncolorlists=['red','blue','black']\nplt.bar(x,y,color=colorlists)\nplt.title('GREScore Analysis')\nplt.xlabel('Analysis')\nplt.ylabel('Score')\nplt.show()","6c19517d":"plt.scatter(x='UniversityRating',y='GREScore',data=data_predict)\nplt.xlabel('University Rating')\nplt.ylabel('GRE Score')\nplt.title('University Rating Vs GREScore')\nplt.show()","3de0768b":"data_predict.groupby('UniversityRating')['GREScore'].mean()\nsns.barplot(x=data_predict.groupby('UniversityRating')['GREScore'].mean().index,y=data_predict.groupby('UniversityRating')['GREScore'].mean().values)\nplt.ylabel('GRE Score Mean')\nplt.title('University Rating Groupby GREScore')\nplt.show()","8ef25fe6":"print(\"Min TOEFLScore :\",min(data_predict.TOEFLScore))\nprint(\"Max TOEFLScore :\",max(data_predict.TOEFLScore))","7e0593cc":"plt.figure(figsize=(10,5))\nsns.barplot(x=data_predict.TOEFLScore.value_counts().index,y=data_predict.TOEFLScore.value_counts().values)\nplt.xticks(rotation=90)\nplt.show()","7ef71da4":"data_predict.UniversityRating.value_counts()","56abbd23":"#The analysis of the university rating values was made. \n#A maximum of 3.0 is shown in the extracted results. All of these analysis results are given in pie.\ncolors = ['lime','blue','red','yellow','green']\nexplode = [0,0.1,0,0,0]\nplt.figure(figsize=(7,7))\nplt.pie(data_predict.UniversityRating.value_counts().values,explode=explode,labels=data_predict.UniversityRating.value_counts().index,colors=colors,autopct='%1.1f%%')\nplt.title('University Rating',color='blue',fontsize=15)\nplt.show()","bf503184":"#Multiple analysis and representation in graphics\nplt.scatter(data_predict.UniversityRating,data_predict.CGPA)\nplt.title(\"CGPA Scores for University Ratings\")\nplt.xlabel(\"University Rating\")\nplt.ylabel(\"CGPA\")\nplt.show()","3e8345c5":"tt = data_predict.groupby(['UniversityRating','Research']).mean().reset_index()\nsns.factorplot(x='UniversityRating', y='GREScore', hue='Research', data=tt, kind='bar')\nplt.show()","6aa24bf4":"data_predict.groupby('UniversityRating')[['SOP','LOR','CGPA']].mean()","40875da6":"sns.scatterplot(y=\"CGPA\", x=\"GREScore\",\n                hue=\"UniversityRating\",\n                data=data_predict)\nplt.show()","7e5ff75a":"sns.scatterplot(x=\"CGPA\", y=\"GREScore\",\n                hue=\"Research\",\n                data=data_predict)\nplt.show()","bc6b5446":"plt.figure(figsize=(15,5))\nsns.boxplot(x=\"CGPA\", y=\"GREScore\", data=data_predict[:60],whis=\"range\", palette=\"vlag\")\nplt.show()","f2dfe255":"#This section shows the status of all students between the GREScore value and CGPA values. \n#In addition, there are some values, which appear to be a bit more lazy among themselves. \n#Thus, the CGPA value is between 330 and 340 and it is seen that the GREScore value is very low. \n#This is for us as sling data or inconsistent data. Of course, we can use this situation instead of deleting this situation.\nplt.scatter(data_predict.GREScore,data_predict.CGPA)\nplt.title(\"CGPA for GRE Scores\")\nplt.xlabel(\"GRE Score\")\nplt.ylabel(\"CGPA\")\nplt.show()","9324f1da":"plt.scatter(x=np.arange(1,501),y=data_predict['UniversityRating'],color='b')\nplt.scatter(x=np.arange(1,501),y=data_predict['SOP'],color='r')\nplt.scatter(x=np.arange(1,501),y=data_predict['CGPA'],color='y')\nplt.title('Rating University,SOP,CGPA')\nplt.show()","9ac83643":"print(\"Min CGPA Score :\",min(data_predict.CGPA))\nprint(\"Max CGPA Score :\",max(data_predict.CGPA))","f7edd3f9":"plt.scatter(x=data_predict[data_predict.ChanceOfAdmit >=0.75].CGPA,y=data_predict[data_predict.ChanceOfAdmit >=0.75].ChanceOfAdmit)\nplt.xlabel(\"CGPA Score\")\nplt.ylabel(\"Chance Of Admit\")\nplt.title(\"Chance Of Admit>=0.75\")\nplt.grid(True)\nplt.show()","2e4fb057":"plt.figure(1, figsize=(7,7))\ndata_predict['ChanceOfAdmit'].value_counts().head(15).plot.pie(autopct=\"%1.1f%%\")\nplt.show()","63ac9a0e":"s = data_predict[data_predict[\"ChanceOfAdmit\"] >= 0.75][\"UniversityRating\"].value_counts().head(5)\nplt.title(\"University Ratings of Candidates with an 75% acceptance chance\")\ncolor_list=['red','blue','yellow','orange','black']\ns.plot(kind='bar',figsize=(10, 5))\nplt.xlabel(\"University Rating\")\nplt.ylabel(\"Candidates\")\nplt.xticks(rotation=360)\nplt.show()","7c2f8a97":"data_predict.head()","d7729d31":"data_predict[(data_predict.TOEFLScore>100) & (data_predict['Research']==0)].head()","1019455c":"sns.relplot(x=\"TOEFLScore\", y=\"CGPA\", hue=\"UniversityRating\",\n            sizes=(40, 400), alpha=.5,\n            height=6, data=data_predict[(data_predict.TOEFLScore>100) & (data_predict['Research']==0)])\nplt.show()","0944848e":"sns.relplot(x=\"GREScore\", y=\"TOEFLScore\", hue=\"Research\",\n            sizes=(40, 400), alpha=.5, palette=\"muted\",\n            height=6, data=data_predict)\nplt.show()","90250963":"sns.boxenplot(x=\"ChanceOfAdmit\", y=\"UniversityRating\",\n              color=\"b\",scale=\"linear\", data=data_predict)\nplt.show()","08b9231e":"index_ChaceOfAdmit=[]\nfor sop in data_predict.SOP.value_counts().index:\n    index_ChaceOfAdmit.append(len(data_predict[(data_predict['SOP']==sop)&(data_predict['ChanceOfAdmit']>0.75)]))","337105b1":"data_predict.iloc[:,:-1].corr()","400279a7":"data_predict.columns","5e8f8f04":"from sklearn.decomposition import PCA\npca=PCA()\npca.n_components=5\npca_data=pca.fit_transform(data_predict)\npercentage_var_explained = pca.explained_variance_ \/ np.sum(pca.explained_variance_)\ncum_var_explained=np.cumsum(percentage_var_explained)\n\nplt.figure(1,figsize=(6,4))\n\nplt.clf()\nplt.plot(cum_var_explained,linewidth=2)\nplt.axis('tight')\nplt.grid()\nplt.xlabel('n_components')\nplt.show()","0fa75c2e":"data_predict['ChanceOfAdmit']=[1 if chance>=0.75  else 0 for chance in data_predict['ChanceOfAdmit']]","38de7338":"data_predict=data_predict.astype(float)","1285295d":"dataX=data_predict.drop('ChanceOfAdmit',axis=1)\ndataY=data_predict['ChanceOfAdmit']","34e4fb0d":"X_train,X_test,y_train,y_test=train_test_split(dataX,dataY,test_size=0.2,random_state=42)","d413c262":"print('X_train',X_train.shape)\nprint('X_test',X_test.shape)\nprint('y_train',y_train.shape)\nprint('y_test',y_test.shape)","47851229":"'''sc=StandardScaler()\nX_train=sc.fit_transform(X_train)\nX_test=sc.transform(X_test)\n'''\nX_train = (X_train - np.min(X_train))\/(np.max(X_train)-np.min(X_train))\nX_test = (X_test - np.min(X_test))\/(np.max(X_test)-np.min(X_test))","4f663d66":"def plot_roc_(false_positive_rate,true_positive_rate,roc_auc):\n    plt.figure(figsize=(5,5))\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],linestyle='--')\n    plt.axis('tight')\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()\n    \ndef plot_feature_importances(gbm):\n    n_features = X_train.shape[1]\n    plt.barh(range(n_features), gbm.feature_importances_, align='center')\n    plt.yticks(np.arange(n_features), X_train.columns)\n    plt.xlabel(\"Feature importance\")\n    plt.ylabel(\"Feature\")\n    plt.ylim(-1, n_features)","b006d43a":"combine_features_list=[\n    ('GREScore','TOEFLScore','UniversityRating'),\n    ('SOP','LOR','CGPA','Research'),\n    ('GREScore','SOP','TOEFLScore','CGPA'),\n    ('UniversityRating','Research')\n]","cdaf5ca4":"parameters=[\n{\n    'penalty':['l1','l2'],\n    'C':[0.1,0.4,0.5],\n    'random_state':[0]\n    },\n]\n\nfor features in combine_features_list:\n    print(features)\n    print(\"*\"*50)\n    \n    X_train_set=X_train.loc[:,features]\n    X_test_set=X_test.loc[:,features]\n    \n    gslog=GridSearchCV(LogisticRegression(),parameters,scoring='accuracy')\n    gslog.fit(X_train_set,y_train)\n    print('Best parameters set:')\n    print(gslog.best_params_)\n    print()\n    predictions=[\n    (gslog.predict(X_train_set),y_train,'Train'),\n    (gslog.predict(X_test_set),y_test,'Test1'),\n    ]\n    \n    for pred in predictions:\n        print(pred[2] + ' Classification Report:')\n        print(\"*\"*50)\n        print(classification_report(pred[1],pred[0]))\n        print(\"*\"*50)\n        print(pred[2] + ' Confusion Matrix:')\n        print(confusion_matrix(pred[1], pred[0]))\n        print(\"*\"*50)\n\n    print(\"*\"*50)    \n    basari=cross_val_score(estimator=LogisticRegression(),X=X_train,y=y_train,cv=12)\n    print(basari.mean())\n    print(basari.std())\n    print(\"*\"*50) \n   ","1ff11607":"from sklearn.linear_model import LogisticRegression\n\nlr=LogisticRegression(C=0.1,penalty='l1',random_state=0)\nlr.fit(X_train,y_train)\n\ny_pred=lr.predict(X_test)\n\n\ny_proba=lr.predict_proba(X_test)\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplot_roc_(false_positive_rate,true_positive_rate,roc_auc)\n\n\nfrom sklearn.metrics import r2_score,accuracy_score\n\n#print('Hata Oran\u0131 :',r2_score(y_test,y_pred))\nprint('Accurancy Oran\u0131 :',accuracy_score(y_test, y_pred))\nprint(\"Logistic TRAIN score with \",format(lr.score(X_train, y_train)))\nprint(\"Logistic TEST score with \",format(lr.score(X_test, y_test)))\nprint()\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)\nsns.heatmap(cm,annot=True)\nplt.show()","bd5af7ec":"parameters=[\n{\n    'n_neighbors':np.arange(2,33),\n    'n_jobs':[2,6]\n    },\n]\nprint(\"*\"*50)\nfor features in combine_features_list:\n    print(\"*\"*50)\n    \n    X_train_set=X_train.loc[:,features]\n    X_test_set=X_test.loc[:,features]\n   \n    gsknn=GridSearchCV(KNeighborsClassifier(),parameters,scoring='accuracy')\n    gsknn.fit(X_train_set,y_train)\n    print('Best parameters set:')\n    print(gsknn.best_params_)\n    print(\"*\"*50)\n    predictions = [\n    (gsknn.predict(X_train_set), y_train, 'Train'),\n    (gsknn.predict(X_test_set), y_test, 'Test1')\n    ]\n    for pred in predictions:\n        print(pred[2] + ' Classification Report:')\n        print(\"*\"*50)\n        print(classification_report(pred[1], pred[0]))\n        print(\"*\"*50)\n        print(pred[2] + ' Confusion Matrix:')\n        print(confusion_matrix(pred[1], pred[0]))\n        print(\"*\"*50)\n        \n    print(\"*\"*50)    \n    basari=cross_val_score(estimator=KNeighborsClassifier(),X=X_train,y=y_train,cv=12)\n    print(basari.mean())\n    print(basari.std())\n    print(\"*\"*50)","af73660b":"knn=KNeighborsClassifier(n_jobs=2, n_neighbors=22)\nknn.fit(X_train,y_train)\n\ny_pred=knn.predict(X_test)\n\ny_proba=knn.predict_proba(X_test)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplot_roc_(false_positive_rate,true_positive_rate,roc_auc)\n\nfrom sklearn.metrics import r2_score,accuracy_score\n\nprint('Accurancy Oran\u0131 :',accuracy_score(y_test, y_pred))\nprint(\"KNN TRAIN score with \",format(knn.score(X_train, y_train)))\nprint(\"KNN TEST score with \",format(knn.score(X_test, y_test)))\nprint()\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)\nsns.heatmap(cm,annot=True)\nplt.show()","ed6e6788":"n_neighbors = range(1, 17)\ntrain_data_accuracy = []\ntest1_data_accuracy = []\nfor n_neigh in n_neighbors:\n    knn = KNeighborsClassifier(n_neighbors=n_neigh,n_jobs=5)\n    knn.fit(X_train, y_train)\n    train_data_accuracy.append(knn.score(X_train, y_train))\n    test1_data_accuracy.append(knn.score(X_test, y_test))\nplt.plot(n_neighbors, train_data_accuracy, label=\"Train Data Set\")\nplt.plot(n_neighbors, test1_data_accuracy, label=\"Test1 Data Set\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Neighbors\")\nplt.legend()\nplt.show()","20b7b597":"n_neighbors = range(1, 17)\nk_scores=[]\nfor n_neigh in n_neighbors:\n    knn = KNeighborsClassifier(n_neighbors=n_neigh,n_jobs=5)\n    scores=cross_val_score(estimator=knn,X=X_train,y=y_train,cv=12)\n    k_scores.append(scores.mean())\nprint(k_scores)","c3009a84":"plt.plot(n_neighbors,k_scores)\nplt.xlabel('Value of k for KNN')\nplt.ylabel(\"Cross-Validated Accurancy\")\nplt.show()","f815c958":"parameters = [\n    {\n        'kernel': ['linear'],\n        'random_state': [2]\n    },\n    {\n        'kernel': ['rbf'],\n        'gamma':[0.9,0.06,0.3],\n        'random_state': [0],\n        'C':[1,2,3,4,5,6],\n        'degree':[2],\n        'probability':[True]\n    },\n]\n\nfor features in combine_features_list:\n    print(\"*\"*50)\n    X_train_set=X_train.loc[:,features]\n    X_test_set=X_test.loc[:,features]\n  \n    svc = GridSearchCV(SVC(), parameters,\n    scoring='accuracy')\n    svc.fit(X_train_set, y_train)\n    print('Best parameters set:')\n    print(svc.best_params_)\n    print(\"*\"*50)\n    predictions = [\n    (svc.predict(X_train_set), y_train, 'Train'),\n    (svc.predict(X_test_set), y_test, 'Test1')\n    ]\n    for pred in predictions:\n        print(pred[2] + ' Classification Report:')\n        print(\"*\"*50)\n        print(classification_report(pred[1], pred[0]))\n        print(\"*\"*50)\n        print(pred[2] + ' Confusion Matrix:')\n        print(confusion_matrix(pred[1], pred[0]))\n        print(\"*\"*50)\n        \n    print(\"*\"*50)    \n    basari=cross_val_score(estimator=SVC(),X=X_train,y=y_train,cv=4)\n    print(basari.mean())\n    print(basari.std())\n    print(\"*\"*50)","65f69ba6":"svc=SVC(C=5,degree=2,gamma=0.06,kernel='rbf',probability=True,random_state=0)\nsvc.fit(X_train,y_train)\n\ny_pred=svc.predict(X_test)\n\ny_proba=svc.predict_proba(X_test)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplot_roc_(false_positive_rate,true_positive_rate,roc_auc)\n\nfrom sklearn.metrics import r2_score,accuracy_score\n\nprint('Accurancy Oran\u0131 :',accuracy_score(y_test, y_pred))\nprint(\"SVC TRAIN score with \",format(svc.score(X_train, y_train)))\nprint(\"SVC TEST score with \",format(svc.score(X_test, y_test)))\nprint()\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)\nsns.heatmap(cm,annot=True)\nplt.show()","172d828d":"parameters = [\n{\n    'learning_rate': [0.01, 0.02, 0.002],\n    'random_state': [0],\n    'n_estimators': np.arange(3, 20)\n    },\n]\nfor features in combine_features_list:\n    print(\"*\"*50)\n    X_train_set=X_train.loc[:,features]\n    X_test1_set=X_test.loc[:,features]\n   \n    gbc = GridSearchCV(GradientBoostingClassifier(), parameters, scoring='accuracy')\n    gbc.fit(X_train_set, y_train)\n    print('Best parameters set:')\n    print(gbc.best_params_)\n    print(\"*\"*50)\n    predictions = [\n    (gbc.predict(X_train_set), y_train, 'Train'),\n    (gbc.predict(X_test1_set), y_test, 'Test1')\n    ]\n    for pred in predictions:\n        print(pred[2] + ' Classification Report:')\n        print(\"*\"*50)\n        print(classification_report(pred[1], pred[0]))\n        print(\"*\"*50)\n        print(pred[2] + ' Confusion Matrix:')\n        print(confusion_matrix(pred[1], pred[0]))\n        print(\"*\"*50)\n        \n    print(\"*\"*50)    \n    basari=cross_val_score(estimator=GradientBoostingClassifier(),X=X_train,y=y_train,cv=4)\n    print(basari.mean())\n    print(basari.std())\n    print(\"*\"*50)","612e5cd1":"gbc=GradientBoostingClassifier(learning_rate=0.02,n_estimators=18,random_state=0)\ngbc.fit(X_train,y_train)\n\ny_pred=gbc.predict(X_test)\n\ny_proba=gbc.predict_proba(X_test)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplot_roc_(false_positive_rate,true_positive_rate,roc_auc)\n\nfrom sklearn.metrics import r2_score,accuracy_score\n\nprint('Accurancy Oran\u0131 :',accuracy_score(y_test, y_pred))\nprint(\"GradientBoostingClassifier TRAIN score with \",format(gbc.score(X_train, y_train)))\nprint(\"GradientBoostingClassifier TEST score with \",format(gbc.score(X_test, y_test)))\nprint()\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)\nsns.heatmap(cm,annot=True)\nplt.show()","6346df62":"plot_feature_importances(gbc)\nplt.show()","ddbb288a":"parameters = [\n    {\n        'max_depth': np.arange(1, 10),\n        'min_samples_split': np.arange(2, 5),\n        'random_state': [3],\n        'n_estimators': np.arange(10, 20)\n    },\n]\n\nfor features in combine_features_list:\n    print(\"*\"*50)\n    \n    X_train_set=X_train.loc[:,features]\n    X_test1_set=X_test.loc[:,features]\n    \n    tree=GridSearchCV(RandomForestClassifier(),parameters,scoring='accuracy')\n    tree.fit(X_train_set, y_train)\n    \n    print('Best parameters set:')\n    print(tree.best_params_)\n    print(\"*\"*50)\n    predictions = [\n        (tree.predict(X_train_set), y_train, 'Train'),\n        (tree.predict(X_test1_set), y_test, 'Test1')\n    ]\n    \n    for pred in predictions:\n        \n        print(pred[2] + ' Classification Report:')\n        print(\"*\"*50)\n        print(classification_report(pred[1], pred[0]))\n        print(\"*\"*50)\n        print(pred[2] + ' Confusion Matrix:')\n        print(confusion_matrix(pred[1], pred[0]))\n        print(\"*\"*50)\n    \n    print(\"*\"*50)    \n    basari=cross_val_score(estimator=RandomForestClassifier(),X=X_train,y=y_train,cv=4)\n    print(basari.mean())\n    print(basari.std())\n    print(\"*\"*50)","2d36e94b":"rfc=RandomForestClassifier(max_depth=7,min_samples_split=4,n_estimators=19,random_state=3)\nrfc.fit(X_train,y_train)\n\ny_pred=rfc.predict(X_test)\n\ny_proba=rfc.predict_proba(X_test)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplot_roc_(false_positive_rate,true_positive_rate,roc_auc)\n\nfrom sklearn.metrics import r2_score,accuracy_score\nprint('Accurancy Oran\u0131 :',accuracy_score(y_test, y_pred))\nprint(\"RandomForestClassifier TRAIN score with \",format(rfc.score(X_train, y_train)))\nprint(\"RandomForestClassifier TEST score with \",format(rfc.score(X_test, y_test)))\nprint()\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)\nsns.heatmap(cm,annot=True)\nplt.show()","8fc335a5":"for i in range(1,11):\n    rf = RandomForestClassifier(n_estimators=i, random_state = 3, max_depth=7)\n    rf.fit(X_train, y_train)\n    print(\"TEST set score w\/ \" +str(i)+\" estimators: {:.5}\".format(rf.score(X_test, y_test)))","01f9bbfa":"plot_feature_importances(rf)\nplt.show()","dbac901c":"parameters = [\n{\n    'random_state': [42],\n    },\n]\nfor features in combine_features_list:\n    print(\"*\"*50)\n    X_train_set=X_train.loc[:,features]\n    X_test1_set=X_test.loc[:,features]\n    \n    dtr = GridSearchCV(DecisionTreeClassifier(), parameters, scoring='accuracy')\n    dtr.fit(X_train_set, y_train)\n    print('Best parameters set:')\n    print(dtr.best_params_)\n    print(\"*\"*50)\n    predictions = [\n    (dtr.predict(X_train_set), y_train, 'Train'),\n    (dtr.predict(X_test1_set), y_test, 'Test1')\n    ]\n    for pred in predictions:\n        print(pred[2] + ' Classification Report:')\n        print(\"*\"*50)\n        print(classification_report(pred[1], pred[0]))\n        print(\"*\"*50)\n        print(pred[2] + ' Confusion Matrix:')\n        print(confusion_matrix(pred[1], pred[0]))\n        print(\"*\"*50)\n        \n    print(\"*\"*50)    \n    basari=cross_val_score(estimator=DecisionTreeClassifier(),X=X_train,y=y_train,cv=4)\n    print(basari.mean())\n    print(basari.std())\n    print(\"*\"*50)  ","c2a235a5":"parameters = [\n{\n    'random_state': [42],\n    },\n]\nfor features in combine_features_list:\n    print(\"*\"*50)\n    X_train_set=X_train.loc[:,features]\n    X_test1_set=X_test.loc[:,features]\n    \n    dtr = GridSearchCV(SVC(), parameters, scoring='accuracy')\n    \n    dtr.fit(X_train_set, y_train)\n    print('Best parameters set:')\n    print(dtr.best_params_)\n    print(\"*\"*50)\n    predictions = [\n    (dtr.predict(X_train_set), y_train, 'Train'),\n    (dtr.predict(X_test1_set), y_test, 'Test1')\n    ]\n    for pred in predictions:\n        print(pred[2] + ' Classification Report:')\n        print(\"*\"*50)\n        print(classification_report(pred[1], pred[0]))\n        print(\"*\"*50)\n        print(pred[2] + ' Confusion Matrix:')\n        print(confusion_matrix(pred[1], pred[0]))\n        print(\"*\"*50)\n        \n    print(\"*\"*50)    \n    basari=cross_val_score(estimator=SVC(),X=X_train,y=y_train,cv=4)\n    print(basari.mean())\n    print(basari.std())\n    print(\"*\"*50)  ","89114268":"<p id='3'><h3>One Visualization to Rule Them All<\/h3><\/p>\n<p>We will perform analysis on the training data. The relationship between the features found in the training data is observed. In this way, comments about the properties can be made.<\/p>","036d01b8":"<p>The above mentioned graphs are visualized on the university rates and the research process that appears in the graphs.<\/p>","f3512e2f":"<table border=2>\n<tr>\n<th>Algorithm<\/th>\n<th>\u00d6zellikler<\/th>\n<th>Parametreler<\/th>\n<th>Train Set<\/th>\n<th>Test Set<\/th>\n<\/tr>\n<tr>\n<td>Logistic Regression<\/td>\n<td>('GREScore', 'SOP', 'TOEFLScore', 'CGPA')<\/td>\n<td>{'C': 0.4, 'penalty': 'l1', 'random_state': 0}<\/td>\n<td>% 89<\/td>\n<td>% 92<\/td>\n<\/tr>\n\n<tr>\n<td>KNN<\/td>\n<td>('GREScore', 'SOP', 'TOEFLScore', 'CGPA')<\/td>\n<td>{'n_jobs': 2, 'n_neighbors': 10}<\/td>\n<td>% 88<\/td>\n<td>% 90<\/td>\n<\/tr>\n\n<tr>\n<td>Gradient Boosting Classifier<\/td>\n<td>('GREScore', 'SOP', 'TOEFLScore', 'CGPA')<\/td>\n<td>{'learning_rate': 0.02, 'n_estimators': 10, 'random_state': 0}<\/td>\n<td>% 91<\/td>\n<td>% 92<\/td>\n<\/tr>\n\n<tr>\n<td>Random Forest<\/td>\n<td>('SOP', 'LOR', 'CGPA', 'Research')<\/td>\n<td>{'max_depth': 4, 'min_samples_split': 2, 'n_estimators': 11, 'random_state': 3}<\/td>\n<td>% 90<\/td>\n<td>% 91<\/td>\n<\/tr>\n\n<tr>\n<td>Random Forest<\/td>\n<td>('GREScore', 'SOP', 'TOEFLScore', 'CGPA')<\/td>\n<td>{'max_depth': 1, 'min_samples_split': 2, 'n_estimators': 10, 'random_state': 3}<\/td>\n<td>% 89<\/td>\n<td>% 92<\/td>\n<\/tr>\n\n<tr>\n<td>Decision tree<\/td>\n<td>('GREScore', 'SOP', 'TOEFLScore', 'CGPA')<\/td>\n<td>{'random_state': 42}<\/td>\n<td>% 1.0<\/td>\n<td>% 88<\/td>\n<\/tr>\n\n<tr>\n<td>Decision tree<\/td>\n<td>('UniversityRating','Research')<\/td>\n<td>{'random_state': 42}<\/td>\n<td>% 80<\/td>\n<td>% 83<\/td>\n<\/tr>\n\n<tr>\n<td>SVM<\/td>\n<td>('SOP', 'LOR', 'CGPA', 'Research')<\/td>\n<td>{'random_state': 42}<\/td>\n<td>% 86<\/td>\n<td>% 90<\/td>\n<\/tr>\n\n<tr>\n<td>SVM<\/td>\n<td>('GREScore', 'SOP', 'TOEFLScore', 'CGPA')<\/td>\n<td>{'random_state': 42}<\/td>\n<td>% 87<\/td>\n<td>% 91<\/td>\n<\/tr>\n\n<\/table>","5b1ce814":"<h1>GRADUATE ADMISSIONS<\/h1>\n\n<p>Our aim here is to conduct a wide variety of analyzes and forecasting operations using the data set here.<\/p>\n\n<p>The dataset contains several parameters which are considered important during the application for Masters Programs. The parameters included are :<\/p>\n\n<p>\n\n<ul>\n\n<li>GRE Scores ( out of 340 )<\/li> \n<li>TOEFL Scores ( out of 120 )<\/li> \n<li>University Rating ( out of 5 ) \n<li>Statement of Purpose and Letter of Recommendation Strength ( out of 5 )  \n<li>Undergraduate GPA ( out of 10 )  \n<li>Research Experience ( either 0 or 1 ) \n<li>Chance of Admit ( ranging from 0 to 1 )\n\n<\/ul>\n\n<\/p>\n\n<p>Dataset: https:\/\/www.kaggle.com\/mohansacharya\/graduate-admissions\/home<\/p>\n\n<p>last updated : <b>05.07.2019<\/b><\/p>\n\n<p>if you like it, please<b> UPVOTED<\/b><\/p>\n","4b752b59":"<p id='6'><h3>TOEFL Score Analysis<\/h3><\/p>","4da2e1e0":"<table border=2>\n<tr>\n<th>\u00d6zellikler<\/th>\n<th>Parametreler<\/th>\n<th>Train Set<\/th>\n<th>Test Set<\/th>\n<\/tr>\n<tr>\n<td>'GREScore', 'TOEFLScore', 'UniversityRating'<\/td>\n<td>{'C': 0.5, 'penalty': 'l1', 'random_state': 0}<\/td>\n<td>% 88<\/td>\n<td>% 87<\/td>\n<\/tr>\n\n<tr>\n<td>('SOP', 'LOR', 'CGPA', 'Research')<\/td>\n<td>{'C': 0.4, 'penalty': 'l1', 'random_state': 0}<\/td>\n<td>% 89<\/td>\n<td>% 90<\/td>\n<\/tr>\n\n<tr>\n<td>('GREScore', 'SOP', 'TOEFLScore', 'CGPA')<\/td>\n<td>{'C': 0.4, 'penalty': 'l1', 'random_state': 0}<\/td>\n<td>% 89<\/td>\n<td>% 92<\/td>\n<\/tr>\n\n<tr>\n<td>('UniversityRating', 'Research')<\/td>\n<td>{'C': 0.4, 'penalty': 'l1', 'random_state': 0}<\/td>\n<td>% 80<\/td>\n<td>% 83<\/td>\n<\/tr>\n\n<\/table>","6eb325b4":"<p id='11'><h1>K-Nearest Neighbors<\/h1><\/p>","8754ef70":"<p id='9'><h1>MODEL, TRAINING and TESTING<\/h1><\/p>\n<p>As a result of our initial evaluations, we have used a number of artificial learning algorithms. These are logistic regression, support vector machine (SVM), k close neighborhood (kNN), GradientBoostingClassifier and RandomForestClassifier algorithms. The first algorithm is logistic regression algorithm. To implement this algorithm model, we need to separate dependent and independent variables within our data sets. In addition, we created a combination of features between different features to make different experiments. While creating these parameters, the process of finding the best results was made by giving hyper parameter values.<\/p>\n\n<p>First, we have 19 properties. By grouping these features we will see which one is the most successful.<\/p>","ea39e89f":"<table border=2>\n<tr>\n<th>\u00d6zellikler<\/th>\n<th>Parametreler<\/th>\n<th>Train Set<\/th>\n<th>Test Set<\/th>\n<\/tr>\n<tr>\n<td>'GREScore', 'TOEFLScore', 'UniversityRating'<\/td>\n<td>{'max_depth': 2, 'min_samples_split': 2, 'n_estimators': 11, 'random_state': 3}<\/td>\n<td>% 87<\/td>\n<td>% 88<\/td>\n<\/tr>\n\n<tr>\n<td>('SOP', 'LOR', 'CGPA', 'Research')<\/td>\n<td>{'max_depth': 4, 'min_samples_split': 2, 'n_estimators': 11, 'random_state': 3}<\/td>\n<td>% 90<\/td>\n<td>% 91<\/td>\n<\/tr>\n\n<tr>\n<td>('GREScore', 'SOP', 'TOEFLScore', 'CGPA')<\/td>\n<td>{'max_depth': 1, 'min_samples_split': 2, 'n_estimators': 10, 'random_state': 3}<\/td>\n<td>% 89<\/td>\n<td>% 92<\/td>\n<\/tr>\n\n<tr>\n<td>('UniversityRating','Research')<\/td>\n<td>{'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 10, 'random_state': 3}<\/td>\n<td>% 81<\/td>\n<td>% 82<\/td>\n<\/tr>\n\n<\/table>","f273f11b":"<p id='4'><h3>Research Analysis<\/h3><\/p>","001697cd":"<p>A visible rating is available for the TOEFL Score feature. The 110 Score value is the most received value. and 92 Score value is the lowest received value. Later on, we will continue our understandings.<\/p>","e07c74ee":"<p id='8'><h3>CGPA vs GRE Score Analysis<\/h3><\/p>","fac664bb":"<h3>.info()<\/h3>\n<p>Then, the data has what informations. We are learning the information for all data<\/p>\n<p>The info function shows the data types and numerical values of the features in our data set. In short, this information about our data set. :)<\/p>","cd29cf38":"<p id='16'><h1>Kernelized SVM<\/h1><\/p>","d2c9150d":"<p>As you can see above, we see that the value of the research property is not evenly distributed.<\/p>\n<p>In addition, when we look at the correlation values, it is seen that the value of Research feature is not too significant.<\/p>","00c17936":"<p id='7'><h3>University Rating Analysis<\/h3><\/p>","2827de8b":"<h1>CONTENT<\/h1>\n<ul>\n    <a href='#1'><li>INTRODUCTION<\/a>\n    <a href='#2'><li>INVESTIGATING THE DATA and EXPLORATORY DATA ANALSIS<\/a>\n<ul>\n    <a href='#3'><li>One Visualization to Rule Them All<\/a>\n    <a href='#4'><li>Research Analysis<\/a>\n    <a href='#5'><li>GRE Score Analysis<\/a>\n    <a href='#6'><li>TOEFL Score Analysis<\/a>\n    <a href='#7'><li>University Rating Analysis<\/a>\n    <a href='#8'> <li>CGPA vs GRE Score Analysis<\/a>\n<\/ul>\n<a href='#9'><li>MODEL, TRAINING and TESTING<\/a>\n<ul>\n    <a href='#10'><li>Logistic Regression<\/a>\n    <a href='#11'><li>K-Nearest Neighbors<\/a>\t\n    <a href='#12'><li>Naive Bayes<\/a>\n    <a href='#13'><li>Gradient Boosting Machine<\/a>\t\n    <a href='#14'><li>Random Forest<\/a>\n    <a href='#15'><li>Decision Tree<\/a>\n    <a href='#16'><li>Kernelized SVM<\/a>\n<\/ul>\n    <a href='#17'><li>CONCLUSION<\/a>\n    <a href='#18'><li>REFERENCES<\/a>\n\n<\/ul>","1254d140":"<p>We will list all the columns for all data. We check all columns. Is there any spelling mistake?<\/p>\n<p>We will now set the headings of the feature values in the data set.<\/p>","92c326f8":"<p>The difference in value between our data is too big. It increases both the volume and the results are very bad. To reduce this situation we need to use the StandardScaler function. A value of -1.1 will be obtained after using it.<\/p>","3eb9bb0b":"<p>So, we need to analyze in-house for each feature value. In order for this analysis to be very robust, we need to index all values between values. There are many different methods for this. But we'il do it. Normalization or z-score. We're going to do a z-score here.<\/p>\n<p>Some analysts perform the train_test_split process first.After ,we need to make scale values.This system that's true.<\/p>","d9811d6a":"<p id='18'><h1>REFERENCES<\/h1><\/p>","d5bb8674":"<p id='5'><h3>GRE Score Analysis<\/h3><\/p>","1d681f52":"<p>Link : https:\/\/www.kaggle.com\/mohansacharya\/graduate-admissions<\/p>\n<h3>Context<h3>\n<p>This dataset is created for prediction of Graduate Admissions from an Indian perspective.\n\n<h3>Content<\/h3>\n<p>The dataset contains several parameters which are considered important during the application for Masters Programs. The parameters included are : 1. GRE Scores ( out of 340 ) 2. TOEFL Scores ( out of 120 ) 3. University Rating ( out of 5 ) 4. Statement of Purpose and Letter of Recommendation Strength ( out of 5 ) 5. Undergraduate GPA ( out of 10 ) 6. Research Experience ( either 0 or 1 ) 7. Chance of Admit ( ranging from 0 to 1 )<\/p>\n\n<h3>Acknowledgements<\/h3>\n<p>This dataset is inspired by the UCLA Graduate Dataset. The test scores and GPA are in the older format. The dataset is owned by Mohan S Acharya.<\/p>\n\n<h3>Inspiration<\/h3>\n<p>This dataset was built with the purpose of helping students in shortlisting universities with their profiles. The predicted output gives them a fair idea about their chances for a particular university.<\/p>\n\n<h3>Citation<\/h3>\n<p>Please cite the following if you are interested in using the dataset : Mohan S Acharya, Asfia Armaan, Aneeta S Antony : A Comparison of Regression Models for Prediction of Graduate Admissions, IEEE International Conference on Computational Intelligence in Data Science 2019<\/p>","cb18a5b5":"<p>And, how many rows and columns are there for all data?<\/p>","5b500a6c":"<h3>.head() and .tail()<\/h3>\n<p>Head and tail functions are capable of 5 rows per time. But you can change this situation. So you can enter the desired value in the parameter section. The first function, ie head (), returns the initial values. The second function returns the last values.<\/p>\n<h3>describe()<\/h3>\n<p>After I get the main intuition, I am investigating further to see some analytical attributes.<\/p>\n<p>Describe function includes analysis of all our numerical data. For this, count, mean, std, min,% 25,% 50,% 75, max values are given. The reason this section is important is that you can estimate the probability that the values found here are deviant data.<\/p>","78c40e1f":"<p>Now,I will check null on all data and If data has null, I will sum of null data's. In this way, how many missing data is in the data.<\/p>","c3656e79":"<table border=2>\n<tr>\n<th>\u00d6zellikler<\/th>\n<th>Parametreler<\/th>\n<th>Train Set<\/th>\n<th>Test Set<\/th>\n<\/tr>\n<tr>\n<td>'GREScore', 'TOEFLScore', 'UniversityRating'<\/td>\n<td>{'random_state': 42}<\/td>\n<td>% 87<\/td>\n<td>% 87<\/td>\n<\/tr>\n\n<tr>\n<td>('SOP', 'LOR', 'CGPA', 'Research')<\/td>\n<td>{'random_state': 42}<\/td>\n<td>% 86<\/td>\n<td>% 90<\/td>\n<\/tr>\n\n<tr>\n<td>('GREScore', 'SOP', 'TOEFLScore', 'CGPA')<\/td>\n<td>{'random_state': 42}<\/td>\n<td>% 87<\/td>\n<td>% 91<\/td>\n<\/tr>\n\n<tr>\n<td>('UniversityRating','Research')<\/td>\n<td>{'random_state': 42}<\/td>\n<td>% 80<\/td>\n<td>% 82<\/td>\n<\/tr>\n\n<\/table>","673ac43e":"<table border=2>\n<tr>\n<th>\u00d6zellikler<\/th>\n<th>Parametreler<\/th>\n<th>Train Set<\/th>\n<th>Test Set<\/th>\n<\/tr>\n<tr>\n<td>'GREScore', 'TOEFLScore', 'UniversityRating'<\/td>\n<td>{'n_jobs': 2, 'n_neighbors': 12}<\/td>\n<td>% 87<\/td>\n<td>% 87<\/td>\n<\/tr>\n\n<tr>\n<td>('SOP', 'LOR', 'CGPA', 'Research')<\/td>\n<td>{'n_jobs': 2, 'n_neighbors': 5}<\/td>\n<td>% 89<\/td>\n<td>% 87<\/td>\n<\/tr>\n\n<tr>\n<td>('GREScore', 'SOP', 'TOEFLScore', 'CGPA')<\/td>\n<td>{'n_jobs': 2, 'n_neighbors': 10}<\/td>\n<td>% 88<\/td>\n<td>% 90<\/td>\n<\/tr>\n\n<tr>\n<td>('UniversityRating', 'Research')<\/td>\n<td>{'n_jobs': 2, 'n_neighbors': 31}<\/td>\n<td>% 80<\/td>\n<td>% 82<\/td>\n<\/tr>\n\n<\/table>","8d70edbd":"<table border=2>\n<tr>\n<th>\u00d6zellikler<\/th>\n<th>Parametreler<\/th>\n<th>Train Set<\/th>\n<th>Test Set<\/th>\n<\/tr>\n<tr>\n<td>'GREScore', 'TOEFLScore', 'UniversityRating'<\/td>\n<td>{'learning_rate': 0.02, 'n_estimators': 14, 'random_state': 0}<\/td>\n<td>% 88<\/td>\n<td>% 85<\/td>\n<\/tr>\n\n<tr>\n<td>('SOP', 'LOR', 'CGPA', 'Research')<\/td>\n<td>{'learning_rate': 0.02, 'n_estimators': 11, 'random_state': 0}<\/td>\n<td>% 89<\/td>\n<td>% 91<\/td>\n<\/tr>\n\n<tr>\n<td>('GREScore', 'SOP', 'TOEFLScore', 'CGPA')<\/td>\n<td>{'learning_rate': 0.02, 'n_estimators': 10, 'random_state': 0}<\/td>\n<td>% 91<\/td>\n<td>% 92<\/td>\n<\/tr>\n\n<tr>\n<td>('UniversityRating','Research')<\/td>\n<td>{'learning_rate': 0.01, 'n_estimators': 15, 'random_state': 0}<\/td>\n<td>% 81<\/td>\n<td>% 89<\/td>\n<\/tr>\n\n<\/table>","c8f5aadd":"<p>We used scatter function in this section. Because each student has a certain CGPA value. We tried this way to show them better. In addition, we can see that the people who are very hardworking are better than CGPA values.<\/p>","683079a7":"<p>isnull () Detect missing values.<\/p>\n\n<p>Return a boolean same-sized object indicating if the values are NA. NA values, such as None or numpy.NaN, gets mapped to True values. Everything else gets mapped to False values. Characters such as empty strings '' or numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True).<\/p>","fa662184":"<p id='1'><h1>INTRODUCTION<\/h1><\/p>\n<p>In this project, there are many exam points taken in various exam systems. A variety of analyzes will be obtained from these exams. We will use python programming language. Our system will consist of three stages. The first stage data will be preprocessed. However, there are various analyzes and graphs. In the next step, a wide variety of analyzes will be made by using many supervised algorithms. Each algorithm will be explained and explained in detail.<\/p>","059e11ff":"<p id='13'><h1>Gradient Boosting Machine<\/h1>\t<\/p>","822db2df":"<p>In the heat map above, we need to delete the serial no value because it does not give any value.<\/p>","b25d844f":"<p>All data contained in our data set have been checked to check for any null values. So there is no problem left for a general analysis.<\/p>\n\n<p>So,We access the unique values of the date on the training data. In this way, we check that there are any data assigned at the same date.<\/p>","04115e5d":"<h3>read_csv function<\/h3>\n<p>The read_csv function is one of the important processes of the pandas library. This process is used for this.<\/p>\n<p>Now, our data is loaded. We're writing the following snippet to see the loaded data. The purpose here is to see the top five of the loaded data.<\/p>","743aeaa3":"<p id='2'><h1>INVESTIGATING THE DATA and EXPLORATORY DATA ANALYSIS<\/h1><\/p>\n\n<p>First, I install all the libraries that I will use in our application. I install all the libraries in the first part because the algorithms I will use later and the analysis I will make more clearly will be done.Furthurmore, I have investigated the data, presented some visualization and analysed features. Let's write it. I will import necessary Python modules and read the data.<\/p>\n","29165730":"<p id='14'><h1>Random Forest<\/h1>\t<\/p>","b7c10a57":"<p id='10'><h1>Logistic Regression<\/h1><\/p>\n<p>First we need parameters to use our data more effectively. Hyperthermatic technique was used for this condition. This technique is used to express different features in the process.<\/p>\n\n<p>Logistic Regression (aka logit, MaxEnt) classifier.\n\nIn the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the \u2018multi_class\u2019 option is set to \u2018ovr\u2019, and uses the cross- entropy loss if the \u2018multi_class\u2019 option is set to \u2018multinomial\u2019. (Currently the \u2018multinomial\u2019 option is supported only by the \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018newton-cg\u2019 solvers.)\n\nThis class implements regularized logistic regression using the \u2018liblinear\u2019 library, \u2018newton-cg\u2019, \u2018sag\u2019 and \u2018lbfgs\u2019 solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied).\n\nThe \u2018newton-cg\u2019, \u2018sag\u2019, and \u2018lbfgs\u2019 solvers support only L2 regularization with primal formulation. The \u2018liblinear\u2019 solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.<\/p>\n\n","857d2d71":"<table border=2>\n<tr>\n<th>\u00d6zellikler<\/th>\n<th>Parametreler<\/th>\n<th>Train Set<\/th>\n<th>Test Set<\/th>\n<\/tr>\n<tr>\n<td>'GREScore', 'TOEFLScore', 'UniversityRating'<\/td>\n<td>{'C': 1, 'degree': 2, 'gamma': 0.3, 'kernel': 'rbf', 'probability': True, 'random_state': 0}<\/td>\n<td>% 87<\/td>\n<td>% 87<\/td>\n<\/tr>\n\n<tr>\n<td>('SOP', 'LOR', 'CGPA', 'Research')<\/td>\n<td>{'C': 4, 'degree': 2, 'gamma': 0.9, 'kernel': 'rbf', 'probability': True, 'random_state': 0}<\/td>\n<td>% 90<\/td>\n<td>% 92<\/td>\n<\/tr>\n\n<tr>\n<td>('GREScore', 'SOP', 'TOEFLScore', 'CGPA')<\/td>\n<td>{'C': 3, 'degree': 2, 'gamma': 0.9, 'kernel': 'rbf', 'probability': True, 'random_state': 0}<\/td>\n<td>% 89<\/td>\n<td>% 91<\/td>\n<\/tr>\n\n<tr>\n<td>('UniversityRating','Research')<\/td>\n<td>{'C': 6, 'degree': 2, 'gamma': 0.06, 'kernel': 'rbf', 'probability': True, 'random_state': 0}<\/td>\n<td>% 80<\/td>\n<td>% 83<\/td>\n<\/tr>\n\n<\/table>","cf250123":"<p id='12'><h1>Naive Bayes<\/h1><\/p>","fc047280":"<p>All correlation values between the data are listed in the previous sections. As a result of this listing, it is aimed to ensure that these properties are used in different places by performing different operations. Thus, the p-value process determines a hypothesis and a hypothesis thesis is presented between each characteristic according to this hypothesis. In this process, after determining the Class property as hypothesis, the relations between all the other properties are checked. This results in a different number for each property. What is important here is that these numbers are not close to 1.00. If the number is close to 1.00 this is very bad.<\/p>","92eab3ae":"<p id='15'><h1>Decision Tree<\/h1>\t<\/p>","dd282f83":"<p id='17'><h1>CONCLUSION<\/h1><\/p>","78ac3107":"<p>Our data appears to be attached in the file with the extension txt. We use pandas library to load all datasets. In this way, we see our data more regularly.So, Reading the data into Pandas DataFrames as Admission_Predict and Admission_Predict_Ver1.1.<\/p>","9e90f3f6":"<p>Our data is a situation that summarizes the numerical relationships between each other. This corr () function reveals the correlation values between the data. As can be seen, the first five relationships are almost 1.0000 between each other. This is good for now. But if we do this for all features, this is not good.<\/p>","f92cef5b":"<table border=2>\n<tr>\n<th>\u00d6zellikler<\/th>\n<th>Parametreler<\/th>\n<th>Train Set<\/th>\n<th>Test Set<\/th>\n<\/tr>\n<tr>\n<td>'GREScore', 'TOEFLScore', 'UniversityRating'<\/td>\n<td>{'random_state': 42}<\/td>\n<td>% 95<\/td>\n<td>% 86<\/td>\n<\/tr>\n\n<tr>\n<td>('SOP', 'LOR', 'CGPA', 'Research')<\/td>\n<td>{'random_state': 42}<\/td>\n<td>% 1.0<\/td>\n<td>% 89<\/td>\n<\/tr>\n\n<tr>\n<td>('GREScore', 'SOP', 'TOEFLScore', 'CGPA')<\/td>\n<td>{'random_state': 42}<\/td>\n<td>% 1.0<\/td>\n<td>% 88<\/td>\n<\/tr>\n\n<tr>\n<td>('UniversityRating','Research')<\/td>\n<td>{'random_state': 42}<\/td>\n<td>% 80<\/td>\n<td>% 83<\/td>\n<\/tr>\n\n<\/table>"}}