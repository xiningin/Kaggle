{"cell_type":{"74c66038":"code","8a804334":"code","cfb59367":"code","908bceef":"code","be8ea456":"code","2c8458f1":"code","6e4781a2":"code","8a3ba6fb":"code","29818280":"code","cacd3c85":"code","aa3fadbd":"code","1e658cb5":"code","fc1c5645":"code","97aaa964":"code","7a08dcb5":"code","5cdc407f":"code","e34f8ec3":"code","2926a398":"code","391291b8":"code","ea258c22":"code","9fb4f1b8":"code","ce413c58":"code","d7354eed":"code","f3718d0d":"markdown","19685e5f":"markdown","d45fa9dc":"markdown","b3da84b4":"markdown","f51f940c":"markdown"},"source":{"74c66038":"# Import Dependencies\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport os\nimport random\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport transformers\n\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.simplefilter('ignore')","8a804334":"# Settings\nconfig = {\n    'train_batch_size': 16,\n    'valid_batch_size': 32,\n    'max_len': 314,\n    'nfolds': 5,\n    'seed': 42,\n}\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'{device} is used')\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.detarministic = True \n    torch.backends.cudnn.benchmark = True \n\nseed_everything(seed=config['seed'])\n\ndef rmse_score(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))","cfb59367":"# Load the data\ntrain_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nsample = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')","908bceef":"# k-fold\nnum_bins = int(np.floor(1 + np.log2(len(train_data))))\ntrain_data.loc[:, 'bins'] = pd.cut(train_data['target'], bins=num_bins, labels=False)\n\ntrain_data['kfold'] = -1\nkfold = StratifiedKFold(n_splits=config['nfolds'],\n                        shuffle=True,\n                        random_state=config['seed'])\nfor k, (train_idx, valid_idx) in enumerate(kfold.split(X=train_data, y=train_data.bins)):\n    train_data.loc[valid_idx, 'kfold'] = k\n","be8ea456":"# Dataset, DataLoader\nclass CLRPDataset(Dataset):\n    def __init__(self, df, tokenizer):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.target = df['target'].to_numpy()\n        self.tokenizer = tokenizer \n\n    def __len__(self):\n        return len(self.excerpt)\n\n    def __getitem__(self, idx):\n        sentence = self.excerpt[idx]\n        sentence = sentence.replace('\\n', ' ')\n        bert_sens = tokenizer.encode_plus(sentence,\n                                          add_special_tokens=True,\n                                          max_length=config['max_len'],\n                                          pad_to_max_length=True,\n                                          truncation=True,\n                                          return_attention_mask=True)\n        ids = torch.tensor(bert_sens['input_ids'])\n        mask = torch.tensor(bert_sens['attention_mask'])\n        targets = torch.tensor(self.target[idx], dtype=torch.float)\n        return {'ids': ids, 'mask': mask, 'targets': targets}\n        ","2c8458f1":"model_path = '..\/input\/roberta-base'\n\ntokenizer = transformers.RobertaTokenizer.from_pretrained(model_path)\n\np_fold = 0\np_train = train_data.query(f'kfold != {p_fold}').reset_index(drop=True)\np_valid = train_data.query(f'kfold == {p_fold}').reset_index(drop=True)\n\ntrain_dataset = CLRPDataset(p_train, tokenizer)\nvalid_dataset = CLRPDataset(p_valid, tokenizer)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=config['train_batch_size'],\n                              shuffle=True, num_workers=4, pin_memory=True)\nvalid_dataloader = DataLoader(valid_dataset, batch_size=config['valid_batch_size'],\n                              shuffle=False, num_workers=4, pin_memory=True)","6e4781a2":"model = transformers.RobertaForSequenceClassification.from_pretrained(model_path, num_labels=1)\nmodel.to(device)","8a3ba6fb":"# freezing parameters\nfor param in model.roberta.embeddings.parameters():\n    param.requires_grad = False\n\nfor param in model.roberta.encoder.parameters():\n    param.requires_grad = False \n    ","29818280":"optimizer = optim.Adam([\n    {'params': model.classifier.parameters(), 'lr': 1e-3},\n], betas=(0.9, 0.98))\n\ncriterion = nn.MSELoss()\n","cacd3c85":"scaler = torch.cuda.amp.GradScaler()\n\ndef training(train_dataloader, model, optimizer):\n\n    model.train()\n    \n    all_preds = []\n    all_targets = []\n    losses = []\n\n    for a in train_dataloader:\n\n        optimizer.zero_grad()\n\n        with torch.cuda.amp.autocast():\n\n            ids = a['ids'].to(device, non_blocking=True)\n            mask = a['mask'].to(device, non_blocking=True)\n\n            output = model(ids, mask)\n            output = output['logits'].squeeze(-1)\n\n            target = a['targets'].to(device, non_blocking=True)\n\n            loss = criterion(output, target)\n\n            losses.append(loss.item())\n            all_preds.append(output.detach().cpu().numpy())\n            all_targets.append(target.detach().squeeze(-1).cpu().numpy())\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        del loss\n        torch.cuda.empty_cache()\n\n    all_preds = np.concatenate(all_preds)\n    all_targets = np.concatenate(all_targets)\n    losses = np.mean(losses)\n    train_score = rmse_score(all_targets, all_preds)\n\n    return losses, train_score","aa3fadbd":"def validating(valid_dataloader, model):\n\n    model.eval()\n\n    all_preds = []\n    all_targets = []\n    losses = []\n\n    for b in valid_dataloader:\n\n        with torch.no_grad():\n\n            ids = b['ids'].to(device, non_blocking=True)\n            mask = b['mask'].to(device, non_blocking=True)\n\n            output = model(ids, mask)\n            output = output['logits'].squeeze(-1)\n\n            target = b['targets'].to(device, non_blocking=True)\n\n            loss = criterion(output, target)\n            losses.append(loss.item())\n            all_preds.append(output.detach().cpu().numpy())\n            all_targets.append(target.detach().squeeze(-1).cpu().numpy())\n            \n            del loss\n            torch.cuda.empty_cache()\n\n    all_preds = np.concatenate(all_preds)\n    all_targets = np.concatenate(all_targets)\n\n    losses = np.mean(losses)\n    valid_score = rmse_score(all_targets, all_preds)\n    \n    return all_preds, losses, valid_score\n","1e658cb5":"def train_and_valid(epochs=5):\n    \n    train_losses = []\n    valid_losses = []\n    best_score = None \n    \n    train_scores = []\n    valid_scores = []\n    \n    for epoch in tqdm(range(epochs)):\n        print(\"---------------\" + str(epoch) + \"start-------------\\n\")\n        train_loss, train_score = training(train_dataloader, model, optimizer)\n        train_losses.append(train_loss)\n        train_scores.append(train_score)\n        print(f'train_score is {train_score}\\n')\n        \n        preds, valid_loss, valid_score = validating(valid_dataloader, model)\n        valid_losses.append(valid_loss)\n        valid_scores.append(valid_score)\n        print(f'valid_score is {valid_score}\\n')\n        \n        if (epoch+1 >= 3) and (best_score is None):\n            best_score = valid_score\n            torch.save(model.state_dict(), 'model0.pth')\n            print('Save the first model')\n            \n        elif (epoch+1 >= 3) and (best_score > valid_score):\n            best_score = valid_score\n            torch.save(model.state_dict(), 'model0.pth')        \n            print('found better point')\n            \n        else:\n            pass\n        \n    return {'train_losses': train_losses,\n            'valid_losses': valid_losses,\n            'train_scores': train_scores,\n            'valid_scores': valid_scores,\n            'best_score': best_score,\n            'preds': preds}\n        ","fc1c5645":"epochs = 5\nresult = train_and_valid(epochs=epochs)","97aaa964":"train_losses = result['train_losses']\nvalid_losses = result['valid_losses']\ntrain_scores = result['train_scores']\nvalid_scores = result['valid_scores']\nbest_score = result['best_score']\npreds = result['preds']\n\n#visualization of the results_1\nplt.scatter(p_valid['target'], preds)","7a08dcb5":"#visualization of the results_2\nx = np.arange(epochs)\nplt.plot(x, train_losses)\nplt.plot(x, valid_losses)\n","5cdc407f":"# un-freezing parameters\nfor param in model.roberta.embeddings.parameters():\n    param.requires_grad = True\n\nfor param in model.roberta.encoder.parameters():\n    param.requires_grad = True\n    \noptimizer = optim.Adam([\n    {'params': model.classifier.parameters(), 'lr': 1e-4},\n    {'params': model.roberta.parameters(), 'lr': 5e-5},\n], betas=(0.9, 0.98))\n\nepochs = 5\nresult = train_and_valid(epochs=epochs)","e34f8ec3":"# remaining k-fold\nbest_scores = []\nbest_scores.append(result['best_score'])\n\nfor p_fold in range(1, 5):\n    # initializing the data, model and optimizer\n    p_train = train_data.query(f'kfold != {p_fold}').reset_index(drop=True)\n    p_valid = train_data.query(f'kfold == {p_fold}').reset_index(drop=True)\n\n    train_dataset = CLRPDataset(p_train, tokenizer)\n    valid_dataset = CLRPDataset(p_valid, tokenizer)\n    train_dataloader = DataLoader(train_dataset, batch_size=config['train_batch_size'],\n                                  shuffle=True, num_workers=4, pin_memory=True)\n    valid_dataloader = DataLoader(valid_dataset, batch_size=config['valid_batch_size'],\n                                  shuffle=False, num_workers=4, pin_memory=True)\n    \n    model = transformers.RobertaForSequenceClassification.from_pretrained(model_path, num_labels=1)\n    model.to(device)\n    \n    for param in model.roberta.embeddings.parameters():\n        param.requires_grad = False   \n    for param in model.roberta.encoder.parameters():\n        param.requires_grad = False \n\n    optimizer = optim.Adam([\n        {'params': model.classifier.parameters(), 'lr': 1e-3},\n    ], betas=(0.9, 0.98))\n    criterion = nn.MSELoss()\n    scaler = torch.cuda.amp.GradScaler()\n    \n    epochs = 5\n    result = train_and_valid(epochs=epochs)\n    \n    for param in model.roberta.embeddings.parameters():\n        param.requires_grad = True   \n    for param in model.roberta.encoder.parameters():\n        param.requires_grad = True \n\n    optimizer = optim.Adam([\n        {'params': model.classifier.parameters(), 'lr': 1e-4},\n        {'params': model.roberta.parameters(), 'lr': 5e-5},\n    ], betas=(0.9, 0.98))\n    \n    epochs = 5\n    result = train_and_valid(epochs=epochs)\n    best_scores.append(result['best_score'])\n    ","2926a398":"tokenizer = transformers.RobertaTokenizer.from_pretrained('..\/input\/roberta-base')\n\ntest_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nsample = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')","391291b8":"# Dataset and DataLoader for inference\nclass CLRPInferenceDataset(Dataset):\n\n    def __init__(self, df, tokenizer):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.excerpt)\n\n    def __getitem__(self, idx):\n        sentence = self.excerpt[idx]\n        sentence = sentence.replace('\\n', ' ')\n        roberta_sens = tokenizer.encode_plus(sentence,\n                                            add_special_tokens=True,\n                                            max_length=config['max_len'],\n                                            pad_to_max_length=True,\n                                            truncation=True,\n                                            return_attention_mask=True)\n        ids = torch.tensor(roberta_sens['input_ids'])\n        mask = torch.tensor(roberta_sens['attention_mask'])\n        return {'ids': ids, 'mask': mask}\n\ntest_dataset = CLRPInferenceDataset(test_data, tokenizer)\ntest_dataloader = DataLoader(test_dataset, batch_size=config['valid_batch_size'],\n                             shuffle=False, num_workers=4, pin_memory=True)\n","ea258c22":"model = transformers.RobertaForSequenceClassification.from_pretrained('..\/input\/roberta-base', num_labels=1)\n\nfinetune_result_path = '.\/'\nmodel_names = [s for s in os.listdir(finetune_result_path) if '.pth' in s]\npthes = [os.path.join(finetune_result_path, s) for s in os.listdir(finetune_result_path) if '.pth' in s]\n\ndef clrp_inference(test_dataloader, model, model_names, pthes):\n    all_preds = []\n    all_models = []\n    for model_name, state in zip(model_names, pthes):\n        model.load_state_dict(torch.load(state))\n        model.to(device)\n        model.eval()\n\n        preds = []\n        all_valid_loss = 0\n\n        with torch.no_grad():\n            for a in test_dataloader:\n                ids = a['ids'].to(device)\n                mask = a['mask'].to(device)\n\n                output = model(ids, mask)\n                output = output['logits'].squeeze(-1)\n\n                preds.append(output.cpu().numpy())\n\n            preds = np.concatenate(preds)\n            all_preds.append(preds)\n            all_models.append(model_name)\n\n    print('\\npredicted!')\n    return all_preds, all_models\n\nall_preds, all_models = clrp_inference(test_dataloader, model, model_names, pthes)","9fb4f1b8":"preds_df = pd.DataFrame(all_preds).T\npreds_df.columns = all_models\n\npreds_df\n","ce413c58":"fin_preds = preds_df.mean(axis=1)\nsample['target'] = fin_preds\nsample\n","d7354eed":"sample.to_csv('submission.csv', index=False)\n","f3718d0d":"## Inference","19685e5f":"## Datasets and DataLoaders","d45fa9dc":"## Training","b3da84b4":"## Model","f51f940c":"**\u3010Points\u3011**\n\n\u30fbI used RobertaForSequenceClassification model for prediction.\n\n\u30fbSome parameters are freezed during training.\n\n\u30fbI focused on a better understanding of RoBERTa than Leaderboard.\n\n---\n\n**Comments**: Thanks to previous great Notebooks.\n\n1. [Pytorch BERT beginner's room][1]\n\n2. [CLRP: Pytorch Roberta Finetune][2]\n\n[1]: https:\/\/www.kaggle.com\/chumajin\/pytorch-bert-beginner-s-room\n\n[2]: https:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-finetune"}}