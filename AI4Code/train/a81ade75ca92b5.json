{"cell_type":{"5806535e":"code","788c52c5":"code","48f07722":"code","e1af942a":"code","5a0b02bb":"code","a102deaf":"code","f62b7325":"code","a0944ae0":"code","6becd36b":"code","6f8173c7":"code","2af2ddc9":"code","7517d9e4":"code","325c8625":"code","1b1faeaa":"code","c58c36fa":"code","c242a14e":"code","2859245a":"code","5b6be057":"code","8663c518":"code","2caa1d19":"code","d5b272e5":"code","8e79797d":"code","824fb2a6":"code","28a2c4c3":"code","bf92bfee":"code","f5177b01":"code","47c304ee":"code","cd5239c8":"code","07ddec70":"code","b4f65986":"code","904fef45":"code","20af124a":"code","97066ac6":"code","08ced62e":"code","79a689f3":"code","36953e67":"code","4393d97d":"code","fb2b0d0e":"code","ef9b66b8":"code","8651a7ad":"code","7e4c52ba":"code","5c5973e8":"code","4200090f":"code","0bae31eb":"code","20f41831":"code","63410f0d":"code","8c9dfc55":"code","1e983c4a":"code","f316eea5":"code","de3fc8c8":"code","c0b30cb9":"code","8a1b4e75":"markdown","2108c9c8":"markdown","01d2a1df":"markdown","555076cf":"markdown","c23af750":"markdown","e4bb050a":"markdown","adb71a1c":"markdown","09b23902":"markdown","7247a6e0":"markdown","080dfd43":"markdown","a300b956":"markdown","79355191":"markdown","a15e2fd6":"markdown","2f05eb11":"markdown","9dba5a5e":"markdown","36efa603":"markdown","ad473a24":"markdown","829eb2eb":"markdown","905cbabb":"markdown","22830e9d":"markdown","1368ddb0":"markdown","ca0d0a97":"markdown","87d2c6ff":"markdown"},"source":{"5806535e":"# Supressing the warning messages\nimport warnings\nwarnings.filterwarnings('ignore')","788c52c5":"# Reading the dataset\nimport pandas as pd\nimport numpy as np\nLoanData=pd.read_csv('..\/input\/loandata\/Loan_Approval_Data.csv', encoding='latin')\nprint('Shape before deleting duplicate values:', LoanData.shape)\n\n# Removing duplicate rows if any\nLoanData=LoanData.drop_duplicates()\nprint('Shape After deleting duplicate values:', LoanData.shape)\n\n# Printing sample data\n# Start observing the Quantitative\/Categorical\/Qualitative variables\nLoanData.head(10)","48f07722":"%matplotlib inline\n# Creating Bar chart as the Target variable is Categorical\nGroupedData=LoanData.groupby('Loan_Status').size()\nGroupedData.plot(kind='bar', figsize=(4,3))","e1af942a":"LoanData.head()","5a0b02bb":"LoanData.info()","a102deaf":"LoanData.describe(include='all')","f62b7325":"LoanData.nunique()","a0944ae0":"UselessColumns = ['Loan_ID']\nLoanData = LoanData.drop(UselessColumns,axis=1)\nLoanData.head()","6becd36b":"def PlotBarCharts(inpData, colsToPlot):\n    %matplotlib inline\n    \n    import matplotlib.pyplot as plt\n    \n    # Generating multiple subplots\n    fig, subPlot=plt.subplots(nrows=1, ncols=len(colsToPlot), figsize=(40,6))\n    fig.suptitle('Bar charts of: '+ str(colsToPlot))\n\n    for colName, plotNumber in zip(colsToPlot, range(len(colsToPlot))):\n        inpData.groupby(colName).size().plot(kind='bar',ax=subPlot[plotNumber])\n\n#####################################################################\n# Calling the function\nPlotBarCharts(inpData=LoanData, colsToPlot=['Gender', 'Married', 'Dependents', 'Education',\n       'Self_Employed','Loan_Amount_Term', 'Credit_History', 'Property_Area'])","6f8173c7":"LoanData.hist(['ApplicantIncome', 'CoapplicantIncome','LoanAmount'], figsize=(18,10))","2af2ddc9":"LoanData['ApplicantIncome'][LoanData['ApplicantIncome']>20000].sort_values()","7517d9e4":"LoanData['ApplicantIncome'][LoanData['ApplicantIncome']>30000] = 23803","325c8625":"LoanData['CoapplicantIncome'][LoanData['CoapplicantIncome']>10000].sort_values()","1b1faeaa":"LoanData['CoapplicantIncome'][LoanData['CoapplicantIncome']>15000] = 11300","c58c36fa":"LoanData.hist(['ApplicantIncome', 'CoapplicantIncome'], figsize=(18,5))","c242a14e":"LoanData.isnull().sum()","2859245a":"# Imputing the missing values\n# Using MODE for categorical columns\nLoanData['Gender'].fillna(LoanData['Gender'].mode()[0], inplace=True)\nLoanData['Married'].fillna(LoanData['Married'].mode()[0], inplace=True)\nLoanData['Dependents'].fillna(LoanData['Dependents'].mode()[0], inplace=True)\nLoanData['Self_Employed'].fillna(LoanData['Self_Employed'].mode()[0], inplace=True)\n# Using Mode value for Loan_Amount_Term since it is a categorical variable\nLoanData['Loan_Amount_Term'].fillna(LoanData['Loan_Amount_Term'].mode()[0], inplace=True)\nLoanData['Credit_History'].fillna(LoanData['Credit_History'].mode()[0], inplace=True)\n\n# Using Median value for continuous columns\nLoanData['LoanAmount'].fillna(LoanData['LoanAmount'].median(), inplace=True)","5b6be057":"LoanData.isnull().sum()","8663c518":"# Box plots for Categorical Target Variable \"Loan_Status\" and continuous predictors\nContinuousColsList=['ApplicantIncome','CoapplicantIncome', 'LoanAmount']\n\nimport matplotlib.pyplot as plt\nfig, PlotCanvas=plt.subplots(nrows=1, ncols=len(ContinuousColsList), figsize=(18,5))\n\n# Creating box plots for each continuous predictor against the Target Variable \"Loan_Status\"\nfor PredictorCol , i in zip(ContinuousColsList, range(len(ContinuousColsList))):\n    LoanData.boxplot(column=PredictorCol, by='Loan_Status', figsize=(5,5), vert=True, ax=PlotCanvas[i])","2caa1d19":"# Defining a function to find the statistical relationship with all the categorical variables\ndef FunctionAnova(inpData, TargetVariable, ContinuousPredictorList):\n    from scipy.stats import f_oneway\n\n    # Creating an empty list of final selected predictors\n    SelectedPredictors=[]\n    \n    print('##### ANOVA Results ##### \\n')\n    for predictor in ContinuousPredictorList:\n        CategoryGroupLists=inpData.groupby(TargetVariable)[predictor].apply(list)\n        AnovaResults = f_oneway(*CategoryGroupLists)\n        \n        # If the ANOVA P-Value is <0.05, that means we reject H0\n        if (AnovaResults[1] < 0.05):\n            print(predictor, 'is correlated with', TargetVariable, '| P-Value:', AnovaResults[1])\n            SelectedPredictors.append(predictor)\n        else:\n            print(predictor, 'is NOT correlated with', TargetVariable, '| P-Value:', AnovaResults[1])\n    \n    return(SelectedPredictors)","d5b272e5":"# Calling the function to check which categorical variables are correlated with target\nContinuousVariables=['ApplicantIncome', 'CoapplicantIncome','LoanAmount']\nFunctionAnova(inpData=LoanData, TargetVariable='Loan_Status', ContinuousPredictorList=ContinuousVariables)","8e79797d":"CrossTabResult=pd.crosstab(index=LoanData['Gender'], columns=LoanData['Loan_Status'])\nCrossTabResult","824fb2a6":"# Visual Inference using Grouped Bar charts\nCategoricalColsList=['Gender', 'Married', 'Dependents', 'Education',\n       'Self_Employed','Loan_Amount_Term', 'Credit_History', 'Property_Area']\n\nimport matplotlib.pyplot as plt\nfig, PlotCanvas=plt.subplots(nrows=len(CategoricalColsList), ncols=1, figsize=(10,50))\n\n# Creating Grouped bar plots for each categorical predictor against the Target Variable \"Loan_Status\"\nfor CategoricalCol , i in zip(CategoricalColsList, range(len(CategoricalColsList))):\n    CrossTabResult=pd.crosstab(index=LoanData[CategoricalCol], columns=LoanData['Loan_Status'])\n    CrossTabResult.plot.bar(color=['red','blue'], ax=PlotCanvas[i])","28a2c4c3":"# Writing a function to find the correlation of all categorical variables with the Target variable\ndef FunctionChisq(inpData, TargetVariable, CategoricalVariablesList):\n    from scipy.stats import chi2_contingency\n    \n    # Creating an empty list of final selected predictors\n    SelectedPredictors=[]\n\n    for predictor in CategoricalVariablesList:\n        CrossTabResult=pd.crosstab(index=inpData[TargetVariable], columns=inpData[predictor])\n        ChiSqResult = chi2_contingency(CrossTabResult)\n        \n        # If the ChiSq P-Value is <0.05, that means we reject H0\n        if (ChiSqResult[1] < 0.05):\n            print(predictor, 'is correlated with', TargetVariable, '| P-Value:', ChiSqResult[1])\n            SelectedPredictors.append(predictor)\n        else:\n            print(predictor, 'is NOT correlated with', TargetVariable, '| P-Value:', ChiSqResult[1])        \n            \n    return(SelectedPredictors)","bf92bfee":"CategoricalVariables=['Gender', 'Married', 'Dependents', 'Education',\n       'Self_Employed','Loan_Amount_Term', 'Credit_History', 'Property_Area']\n\n# Calling the function\nFunctionChisq(inpData=LoanData, \n              TargetVariable='Loan_Status',\n              CategoricalVariablesList= CategoricalVariables)","f5177b01":"SelectedColumns=['Married', 'Education', 'Credit_History', 'Property_Area']\n\n# Selecting final columns\nDataForML=LoanData[SelectedColumns]\nDataForML.head()","47c304ee":"# Saving this final data for reference during deployment\nDataForML.to_pickle('DataForML.pkl')","cd5239c8":"# Treating the binary nominal variables first\nDataForML['Married'].replace({'Yes':1, 'No':0}, inplace=True)\nDataForML['Education'].replace({'Graduate':1, 'Not Graduate':0}, inplace=True)\n\n# Looking at data after nominal treatment\nDataForML.head()","07ddec70":"# Treating all the nominal variables at once using dummy variables\nDataForML_Numeric=pd.get_dummies(DataForML)\n\n# Adding Target Variable to the data\nDataForML_Numeric['Loan_Status']=LoanData['Loan_Status']\n\n# Printing sample rows\nDataForML_Numeric.head()\n","b4f65986":"DataForML_Numeric.columns","904fef45":"# Separate Target Variable and Predictor Variables\nTargetVariable='Loan_Status'\nPredictors=['Married', 'Education', 'Credit_History', 'Property_Area_Rural',\n       'Property_Area_Semiurban', 'Property_Area_Urban']\n\nX=DataForML_Numeric[Predictors].values\ny=DataForML_Numeric[TargetVariable].values","20af124a":"### Sandardization of data ###\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n# Choose either standardization or Normalization\n# On this data Min Max Normalization produced better results\n\n# Choose between standardization and MinMAx normalization\n#PredictorScaler=StandardScaler()\nPredictorScaler=MinMaxScaler()\n\n# Storing the fit object for later reference\nPredictorScalerFit=PredictorScaler.fit(X)\n\n# Generating the standardized values of X\nX=PredictorScalerFit.transform(X)\n\n# Split the data into training and testing set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","97066ac6":"print(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","08ced62e":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n# choose parameter Penalty='l1' or C=1\n# choose different values for solver 'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'\nclf = LogisticRegression(C=1,penalty='l2', solver='newton-cg')\n\n# Printing all the parameters of logistic regression\n# print(clf)\n\n# Creating the model on Training Data\nLOG=clf.fit(X_train,y_train)\nprediction=LOG.predict(X_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.f1_score(y_test, prediction, average='weighted')\nprint('Accuracy of the model on Testing Sample Data:', round(F1_Score*100,2))\n\n# Importing cross validation function from sklearn\nfrom sklearn.model_selection import cross_val_score\n\n# Running 10-Fold Cross validation on a given algorithm\n# Passing full data X and y because the K-fold will split the data and automatically choose train\/test\nAccuracy_Values=cross_val_score(LOG, X , y, cv=10, scoring='f1_weighted')\nprint('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\nprint('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean()*100,2))","79a689f3":"#Decision Trees\nfrom sklearn import tree\n#choose from different tunable hyper parameters\nclf = tree.DecisionTreeClassifier(max_depth=2,criterion='entropy')\n\n# Printing all the parameters of Decision Trees\nprint(clf)\n\n# Creating the model on Training Data\nDTree=clf.fit(X_train,y_train)\nprediction=DTree.predict(X_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.f1_score(y_test, prediction, average='weighted')\nprint('Accuracy of the model on Testing Sample Data:', round(F1_Score*100,2))\n\n# Plotting the feature importance for Top 10 most important columns\n%matplotlib inline\nfeature_importances = pd.Series(DTree.feature_importances_, index=Predictors)\nfeature_importances.nlargest(10).plot(kind='barh')\n\n# Importing cross validation function from sklearn\nfrom sklearn.model_selection import cross_val_score\n\n# Running 10-Fold Cross validation on a given algorithm\n# Passing full data X and y because the K-fold will split the data and automatically choose train\/test\nAccuracy_Values=cross_val_score(DTree, X , y, cv=10, scoring='f1_weighted')\nprint('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\nprint('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean()*100,2))","36953e67":"from dtreeplt import dtreeplt\ndtree = dtreeplt(model=clf, feature_names=Predictors, target_names=TargetVariable)\nfig = dtree.view()\ncurrentFigure=plt.gcf()\ncurrentFigure.set_size_inches(20,10)","4393d97d":"# Random Forest (Bagging of multiple Decision Trees)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(max_depth=2, n_estimators=100,criterion='gini')\n\n# Printing all the parameters of Random Forest\nprint(clf)\n\n# Creating the model on Training Data\nRF=clf.fit(X_train,y_train)\nprediction=RF.predict(X_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.f1_score(y_test, prediction, average='weighted')\nprint('Accuracy of the model on Testing Sample Data:', round(F1_Score*100,2))\n\n# Importing cross validation function from sklearn\nfrom sklearn.model_selection import cross_val_score\n\n# Running 10-Fold Cross validation on a given algorithm\n# Passing full data X and y because the K-fold will split the data and automatically choose train\/test\nAccuracy_Values=cross_val_score(RF, X , y, cv=10, scoring='f1_weighted')\nprint('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\nprint('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean()*100,2))\n\n\n# Plotting the feature importance for Top 10 most important columns\n%matplotlib inline\nfeature_importances = pd.Series(RF.feature_importances_, index=Predictors)\nfeature_importances.nlargest(10).plot(kind='barh')","fb2b0d0e":"# PLotting a single Decision Tree from Random Forest\nfrom dtreeplt import dtreeplt\ndtree = dtreeplt(model=clf.estimators_[4], feature_names=Predictors, target_names=TargetVariable)\nfig = dtree.view()\n","ef9b66b8":"# Adaboost \nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Choosing Decision Tree with 1 level as the weak learner\nDTC=DecisionTreeClassifier(max_depth=1)\nclf = AdaBoostClassifier(n_estimators=500, base_estimator=DTC ,learning_rate=0.01)\n\n# Printing all the parameters of Adaboost\nprint(clf)\n\n# Creating the model on Training Data\nAB=clf.fit(X_train,y_train)\nprediction=AB.predict(X_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.f1_score(y_test, prediction, average='weighted')\nprint('Accuracy of the model on Testing Sample Data:', round(F1_Score*100,2))\n\n# Importing cross validation function from sklearn\nfrom sklearn.model_selection import cross_val_score\n\n# Running 10-Fold Cross validation on a given algorithm\n# Passing full data X and y because the K-fold will split the data and automatically choose train\/test\nAccuracy_Values=cross_val_score(AB, X , y, cv=10, scoring='f1_weighted')\nprint('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\nprint('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean()*100,2))\n\n# Plotting the feature importance for Top 10 most important columns\n%matplotlib inline\nfeature_importances = pd.Series(AB.feature_importances_, index=Predictors)\nfeature_importances.nlargest(10).plot(kind='barh')","8651a7ad":"# Xtreme Gradient Boosting (XGBoost)\nfrom xgboost import XGBClassifier\nclf=XGBClassifier(max_depth=2, learning_rate=0.01, n_estimators=200, objective='binary:logistic', booster='gbtree')\n\n# Printing all the parameters of XGBoost\nprint(clf)\n\n# Creating the model on Training Data\nXGB=clf.fit(X_train,y_train)\nprediction=XGB.predict(X_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.f1_score(y_test, prediction, average='weighted')\nprint('Accuracy of the model on Testing Sample Data:', round(F1_Score*100,2))\n\n# Importing cross validation function from sklearn\nfrom sklearn.model_selection import cross_val_score\n\n# Running 10-Fold Cross validation on a given algorithm\n# Passing full data X and y because the K-fold will split the data and automatically choose train\/test\nAccuracy_Values=cross_val_score(XGB, X , y, cv=10, scoring='f1_weighted')\nprint('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\nprint('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean()*100,2))\n\n# Plotting the feature importance for Top 10 most important columns\n%matplotlib inline\nfeature_importances = pd.Series(XGB.feature_importances_, index=Predictors)\nfeature_importances.nlargest(10).plot(kind='barh')","7e4c52ba":"# K-Nearest Neighbor(KNN)\nfrom sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=3)\n\n# Printing all the parameters of KNN\nprint(clf)\n\n# Creating the model on Training Data\nKNN=clf.fit(X_train,y_train)\nprediction=KNN.predict(X_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.f1_score(y_test, prediction, average='weighted')\nprint('Accuracy of the model on Testing Sample Data:', round(F1_Score*100,2))\n\n# Importing cross validation function from sklearn\nfrom sklearn.model_selection import cross_val_score\n\n# Running 10-Fold Cross validation on a given algorithm\n# Passing full data X and y because the K-fold will split the data and automatically choose train\/test\nAccuracy_Values=cross_val_score(KNN, X , y, cv=10, scoring='f1_weighted')\nprint('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\nprint('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean()*100,2))\n","5c5973e8":"# Support Vector Machines(SVM)\nfrom sklearn import svm\nclf = svm.SVC(C=3, kernel='rbf', gamma=0.1)\n\n# Printing all the parameters of KNN\nprint(clf)\n\n# Creating the model on Training Data\nSVM=clf.fit(X_train,y_train)\nprediction=SVM.predict(X_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.f1_score(y_test, prediction, average='weighted')\nprint('Accuracy of the model on Testing Sample Data:', round(F1_Score*100,2))\n\n# Importing cross validation function from sklearn\nfrom sklearn.model_selection import cross_val_score\n\n# Running 10-Fold Cross validation on a given algorithm\n# Passing full data X and y because the K-fold will split the data and automatically choose train\/test\nAccuracy_Values=cross_val_score(SVM, X , y, cv=10, scoring='f1_weighted')\nprint('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\nprint('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean()*100,2))","4200090f":"# Naive Bays\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\n\n# GaussianNB is used in Binomial Classification\n# MultinomialNB is used in multi-class classification\nclf = GaussianNB()\n#clf = MultinomialNB()\n\n# Printing all the parameters of Naive Bayes\nprint(clf)\n\nNB=clf.fit(X_train,y_train)\nprediction=NB.predict(X_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.f1_score(y_test, prediction, average='weighted')\nprint('Accuracy of the model on Testing Sample Data:', round(F1_Score*100,2))\n\n# Importing cross validation function from sklearn\nfrom sklearn.model_selection import cross_val_score\n\n# Running 10-Fold Cross validation on a given algorithm\n# Passing full data X and y because the K-fold will split the data and automatically choose train\/test\nAccuracy_Values=cross_val_score(NB, X , y, cv=10, scoring='f1_weighted')\nprint('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\nprint('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean()*100,2))","0bae31eb":"# Adaboost \nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Using the Adaboost algorithm with final hyperparamters\n# Choosing Decision Tree with 1 level as the weak learner\nDTC=DecisionTreeClassifier(max_depth=1)\nclf = AdaBoostClassifier(n_estimators=500, base_estimator=DTC ,learning_rate=0.01)\n\n# Training the model on 100% Data available\nFinalAdaboostModel=clf.fit(X,y)","20f41831":"import pickle\nimport os\n\n# Saving the Python objects as serialized files can be done using pickle library\n# Here let us save the Final ZomatoRatingModel\nwith open('FinalAdaboostModel.pkl', 'wb') as fileWriteStream:\n    pickle.dump(FinalAdaboostModel, fileWriteStream)\n    # Don't forget to close the filestream!\n    fileWriteStream.close()\n    \nprint('pickle file of Predictive Model is saved at Location:',os.getcwd())","63410f0d":"# This Function can be called from any from any front end tool\/website\ndef PredictLoanStatus(InputLoanDetails):\n    import pandas as pd\n    Num_Inputs=InputLoanDetails.shape[0]\n    \n    # Making sure the input data has same columns as it was used for training the model\n    # Also, if standardization\/normalization was done, then same must be done for new input\n    \n    # Appending the new data with the Training data\n    DataForML=pd.read_pickle('DataForML.pkl')\n    InputLoanDetails=InputLoanDetails.append(DataForML)\n    \n    # Treating the binary nominal variables first\n    InputLoanDetails['Married'].replace({'Yes':1, 'No':0}, inplace=True)\n    InputLoanDetails['Education'].replace({'Graduate':1, 'Not Graduate':0}, inplace=True)\n    \n    # Generating dummy variables for rest of the nominal variables\n    InputLoanDetails=pd.get_dummies(InputLoanDetails)\n            \n    # Maintaining the same order of columns as it was during the model training\n    Predictors=['Married', 'Education', 'Credit_History', 'Property_Area_Rural',\n       'Property_Area_Semiurban', 'Property_Area_Urban']\n    \n    # Generating the input values to the model\n    X=InputLoanDetails[Predictors].values[0:Num_Inputs]    \n    \n    # Generating the standardized values of X since it was done while model training also\n    X=PredictorScalerFit.transform(X)\n    \n    # Loading the Function from pickle file\n    import pickle\n    with open('FinalAdaboostModel.pkl', 'rb') as fileReadStream:\n        AdaBoost_model=pickle.load(fileReadStream)\n        # Don't forget to close the filestream!\n        fileReadStream.close()\n            \n    # Genrating Predictions\n    Prediction=AdaBoost_model.predict(X)\n    PredictedStatus=pd.DataFrame(Prediction, columns=['Predicted Status'])\n    return(PredictedStatus)","8c9dfc55":"# Calling the function for some loan applications\nNewLoanApplications=pd.DataFrame(\ndata=[['No','Graduate',1,'Urban'],\n     ['No','Graduate',0,'Urban']],\ncolumns=['Married','Education','Credit_History','Property_Area'])\n\nprint(NewLoanApplications)\n\n# Calling the Function for prediction\nPredictLoanStatus(InputLoanDetails= NewLoanApplications)","1e983c4a":"# Creating the function which can take loan inputs and perform prediction\ndef FunctionLoanPrediction(inp_married, inp_education, inp_credit_history, inp_Property_Area):\n    SampleInputData=pd.DataFrame(\n     data=[[inp_married, inp_education , inp_credit_history, inp_Property_Area]],\n     columns=['Married','Education','Credit_History','Property_Area'])\n\n    # Calling the function defined above using the input parameters\n    Predictions=PredictLoanStatus(InputLoanDetails= SampleInputData)\n\n    # Returning the predicted loan status\n    return(Predictions.to_json())\n\n# Function call\nFunctionLoanPrediction(inp_married='Yes', \n                       inp_education='Graduate',\n                       inp_credit_history=1.0,\n                       inp_Property_Area='Urban')","f316eea5":"from flask import Flask,request,jsonify\nimport pickle\nimport pandas as pd\nimport numpy","de3fc8c8":"app = Flask(__name__)\n\n@app.route('\/get_loan_prediction', methods=[\"GET\"])\ndef get_loan_prediction():\n    try:\n        # Getting the paramters from API call\n        married_value = request.args.get('Married')\n        education_value = request.args.get('Education')\n        credit_value=float(request.args.get('CreditHistory'))\n        property_area_value=request.args.get('PropertyArea')\n                \n        # Calling the funtion to get loan approval status\n        prediction_from_api=FunctionLoanPrediction(inp_married=married_value,\n                               inp_education=education_value, \n                               inp_credit_history=credit_value,\n                               inp_Property_Area= property_area_value)\n\n        return (prediction_from_api)\n    \n    except Exception as e:\n        return('Something is not right!:'+str(e))","c0b30cb9":"import os\nif __name__ ==\"__main__\":\n    \n    # Hosting the API in localhost\n    app.run(host='127.0.0.1', port=8080, threaded=True, debug=True, use_reloader=False)\n    # Interrupt kernel to stop the API","8a1b4e75":"# Machine Learning: Splitting the data into Training and Testing sample","2108c9c8":"# Logistic Regression","01d2a1df":"# KNN","555076cf":"# Data Pre-processing for Machine Learning","c23af750":"# XGBoost","e4bb050a":"# Statistical Feature Selection (Categorical Vs Continuous) using ANOVA test","adb71a1c":"# Basic Data Exploration","09b23902":"# Outlier treatment","7247a6e0":"# Relationship exploration: Categorical Vs Continuous -- Box Plots","080dfd43":"# AdaBoost","a300b956":"# Defining the problem statement:\nCreate a Predictive model which can tell weather to approve a loan application or not?\nTarget Variable: Loan_Status\nPredictors: Gender, Married, Dependents, Education, Self_Employed, ApplicantIncome etc.\nLoan_Status=\"N\" means the loan was rejected.\nLoan_Status=\"Y\" means the loan was approved.\n\n# Looking at the distribution of Target variable","79355191":"# Decision Tree","a15e2fd6":"# Missing values treatment","2f05eb11":"# Function for predictions API","9dba5a5e":"# Creating Flask API","36efa603":"# Selecting final predictors for Machine Learning","ad473a24":"# Feature Selection","829eb2eb":"# Random Forest","905cbabb":"# Deployment of the Model\n\nI am choosing Adaboost as the final model since it is very fast and I observe that it is using the predictors better by looking at its variable importance chart. It is not letting a single predictor dominate the decision, which is good.","22830e9d":"# Naive Bayes","1368ddb0":"# Relationship exploration: Categorical Vs Categorical -- Grouped Bar Charts","ca0d0a97":"# SVM","87d2c6ff":"# Starting the API engine"}}