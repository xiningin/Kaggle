{"cell_type":{"397ce202":"code","33336c43":"code","33ef4f06":"code","11552b62":"code","09a7c4a8":"code","6869ccab":"code","6b28bb73":"code","56f861bf":"code","4f0eba24":"code","7aedff00":"code","5681609c":"code","c799a318":"code","24992d55":"code","e3599317":"code","b8a26cde":"code","60830727":"code","bd6c9d65":"code","d9acae86":"code","cf351ed9":"code","abca62ab":"code","6feb048a":"code","92582a9e":"code","1cc927ec":"code","c35f15d4":"code","50744ef2":"markdown","77a2017e":"markdown"},"source":{"397ce202":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import roc_auc_score\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","33336c43":"train = pd.read_csv('..\/input\/train.csv', index_col=False)\ntest = pd.read_csv('..\/input\/test.csv', index_col=False)\nid_test = test['id']\ntest = test.drop(columns=['id'])\ny = train['target']\nX = train.drop(columns=['id', 'target'])","33ef4f06":"X.head()","11552b62":"plt.title('Visualize Target datasets')\nsns.countplot(y)","09a7c4a8":"train.isnull().sum().sort_values(ascending=False), test.isnull().sum().sort_values(ascending=False)","6869ccab":"(train.nunique()\/len(train.columns)).sort_values(ascending=True)","6b28bb73":"sns.pairplot(train.iloc[:, :4], hue='target')","56f861bf":"pca = PCA(n_components=250)\nX = pd.DataFrame(pca.fit_transform(X))\ntest = pd.DataFrame(pca.fit_transform(test))","4f0eba24":"# from sklearn.preprocessing import StandardScaler\n# scaler = StandardScaler()\n# X = scaler.fit_transform(X)","7aedff00":"\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nclf = LogisticRegressionCV(cv=5)\nclf = RandomForestClassifier()\nclf = SVC(C=13.0, probability=True)\nclf.fit(X_train, y_train)","5681609c":"print(\"Accuracy: ({:.2}, {:.2})\".format(roc_auc_score(y_train, clf.predict_proba(X_train)[:, 1]), roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])))\n\n","c799a318":"submit = pd.DataFrame()\nsubmit['id'] = id_test\nsubmit['target'] = clf.predict_proba(test)[:, 1]\nsubmit.to_csv('submit.csv', index=False)","24992d55":"#train.min().sort_values(ascending=False)","e3599317":"#train.max().sort_values(ascending=True)","b8a26cde":"\n\nparams = {'learning_rate': 0.3,\n              'application': 'binary',\n              'num_leaves': 31,\n              'verbosity': -1,\n              'metric': 'auc',\n              'data_random_seed': 2,\n              'bagging_fraction': 0.8,\n              'feature_fraction': 0.6,\n              'nthread': 4,\n              'lambda_l1': 1,\n              'lambda_l2': 1}\n# train_data = lgb.Dataset(X_train, label=y_train)\n# val_data = lgb.Dataset(X_test, label=y_test)\n# watchlist = [train_data, val_data]\n\n# model_lgb = lgb.train(params, train_set=train_data, valid_sets=watchlist)\n\nfrom sklearn.model_selection import KFold, StratifiedKFold\nN_FOLDS = 10\nfolds = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\noof = np.zeros(len(X))\nsub = np.zeros(len(test))\nscores = [0 for _ in range(folds.n_splits)]\nfor fold_, (train_idx, val_idx) in enumerate(folds.split(X.values, y)):\n    X_train, y_train = X.loc[train_idx], y.loc[train_idx]\n    X_val, y_val = X.loc[val_idx], y.loc[val_idx]\n    train_data = lgb.Dataset(X_train, label=y_train)\n    val_data = lgb.Dataset(X_val, label=y_val)\n    watchlist = [train_data, val_data]\n    clf = lgb.train(params, train_set = train_data, valid_sets=watchlist)\n    oof[val_idx] = clf.predict(X_val)\n    sub += clf.predict(test)\/folds.n_splits\n    scores[fold_] = roc_auc_score(y[val_idx], oof[val_idx])\n    print(\"Fold {}: {}\".format(fold_+1, round(scores[fold_],5)))\n    \nprint(\"CV score(auc): {:<8.5f}, (std: {:<8.5f})\".format(roc_auc_score(y, oof), np.std(scores)))\n","60830727":"submit = pd.DataFrame()\nsubmit['id'] = id_test\nsubmit['target'] = sub\nsubmit.to_csv('submit_lgbcv.csv', index=False)","bd6c9d65":"print(type(submit))\nif(isinstance(submit,(pd.core.frame.DataFrame))):\n    print(\"True\")","d9acae86":"params = {\n    'learning_rate': 0.2,\n    'application': 'binary',\n    'num_leaves': 31,\n    'verbosity': -1,\n    'metric': 'auc',\n    'data_random_seed': 2,\n    'bagging_fraction': 0.8,\n    'feature_fraction': 0.6,\n    'nthread': 4,\n    'lambda_l1': 1,\n    'lambda_l2': 1}\ndef lightgbm(train, target, params, test_size=0.2):\n    X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=test_size, random_state=42)\n    train_data = lgb.Dataset(X_train, label=y_train)\n    val_data = lgb.Dataset(X_test, label=y_test)\n    watchlist = [train_data, val_data]\n    model_lgb = lgb.train(params, train_set=train_data, valid_sets=watchlist)\n    return model_lgb\nmodel = lightgbm(train=X, target=y, params=params)\n\nparams = {\n    'learning_rate': 0.2,\n    'application': 'binary',\n    'num_boost_round': 100,\n    'nfold': 5,\n    'num_leaves': 31,\n    'verbosity': -1,\n    'metric': 'auc',\n    'data_random_seed': 2,\n    'bagging_fraction': 0.8,\n    'feature_fraction': 0.6,\n    'nthread': 4,\n    'lambda_l1': 1,\n    'lambda_l2': 1,\n    'early_stopping_rounds': 40,\n}\ndef lightgbmcv(train, target, params):\n    train_data=lgb.Dataset(train, label=target)\n    model_lgbcv = lgb.cv(params, train_set=train_data)\n    return model_lgbcv\nmodel = lightgbmcv(X, y, params=params)\n","cf351ed9":"lightgbmcv(X, y, params)","abca62ab":"import xgboost as xgb\nparams = {\n    'learning_rates': 0.2,\n    'eta': 0.02, \n    'max_depth': 10, \n    'subsample': 0.7, \n    'colsample_bytree': 0.7, \n    'objective': 'binary:logistic', \n    'seed': 99, \n    'silent': 1, \n    'eval_metric':'auc', \n    'nthread':4}\n\ndef xgboost(train, target, early_stopping_rounds=10, test_size=0.2):\n    X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=test_size, random_state=42)\n    xgb_train = xgb.DMatrix(train, label=target)\n    model = xgb.train(params, xgb_train, verbose_eval=1)\n    return model\nxgboost(X, y)\n\nparams = {\n    'learning_rates': 0.2,\n    'eta': 0.02, \n    'max_depth': 10, \n    'subsample': 0.7, \n    'colsample_bytree': 0.7, \n    'objective': 'binary:logistic', \n    'seed': 99, \n    'silent': 1, \n    'eval_metric':'auc', \n    'nthread':4}\n\ndef xgboostcv(train, target, nfold=5, early_stopping_rounds=10):\n    xgb_train = xgb.DMatrix(train, label=target)\n    model = xgb.cv(params, xgb_train, 5000, nfold=nfold, early_stopping_rounds=early_stopping_rounds, verbose_eval=1)\n    return model\n#xgboostcv(X, y)\n","6feb048a":"xgboost(X, y)","92582a9e":"print(\"Accuracy: ({:.2}, {:.2})\".format(accuracy_score(y_train, np.round(model_lgb.predict(X_train))), accuracy_score(y_test, np.round(model_lgb.predict(X_test)))))","1cc927ec":"submit = pd.DataFrame()\nsubmit['id'] = id_test\nsubmit['target'] = model_lgb.predict(test)\nsubmit.to_csv('submit_lgb.csv', index=False)","c35f15d4":"\ndef cross_validation(train, target, test, model, folds, N_FOLDS=10):\n    oof = np.zeros(len(train))\n    sub = np.zeros(len(test))\n    scores = [0 for _ in range(folds.n_splits)]\n    for fold_, (train_index, val_index) in enumerate(folds.split(X, y)):\n        if(isinstance(X,(pd.core.frame.DataFrame))):\n            X_train, y_train = train.loc[train_index], y.loc[train_index]\n            X_val, y_val = train.loc[val_index], y.loc[val_index]\n        else:\n            X_train, y_train = train[train_index], y[train_index]\n            X_val, y_val = train[val_index], y[val_index]\n        model.fit(X_train, y_train)\n        oof[val_idx] = model.predict(X_val)\n        sub += model.predict(test)\/folds.n_splits\n        scores[fold_] = roc_auc_score(y[val_index], oof[val_index])\n        print(\"Fold {}: {}\".format(fold_+1, round(scores[fold_],5)))\n    print(\"CV score(auc): {:<8.5f}, (std: {:<8.5f})\".format(roc_auc_score(y, oof), np.std(scores)))\nfolds = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\nclf = LogisticRegression(C=13.0)\ncross_validation(train=X, target=y, test=test, model=clf, folds=folds)    ","50744ef2":"## Visualize Target of Datasets","77a2017e":"## Check Missing Data"}}