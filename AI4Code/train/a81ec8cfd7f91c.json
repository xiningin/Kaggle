{"cell_type":{"45ae6be8":"code","248c5f8a":"code","b5f7f380":"code","60532b49":"code","4619d09a":"code","30835e9a":"code","e5681bd7":"code","ef0830dd":"code","d337b080":"code","e0140462":"code","42aaa1cc":"code","74162d3a":"code","63232186":"code","69622702":"code","55db3ea3":"code","cde96d30":"code","3a5b34a4":"code","d971db5d":"code","81c54f15":"code","a416479c":"code","ed717441":"code","6ac016bd":"code","141fc3ac":"code","12410716":"code","b156d9a2":"code","c3936ab2":"code","ab7945dd":"code","bc0813f8":"code","1b13e398":"code","cbb02393":"code","3294be0f":"code","048d184d":"code","060cc631":"code","3bed69b4":"code","0fe241e3":"code","2c600a6c":"code","c73173e8":"code","cdea0162":"code","95a09f22":"code","632774ad":"code","feaba93f":"code","696fcd04":"markdown","6697c60d":"markdown","150a7f72":"markdown","618e4b45":"markdown","55e13116":"markdown","dab9fef3":"markdown","5792b2f1":"markdown","5360e318":"markdown","23e14a6a":"markdown","d869d80a":"markdown","269104cf":"markdown","b59c50d7":"markdown","ccdf5bc4":"markdown","6adea129":"markdown","f76fdb5b":"markdown","61a50311":"markdown","b07a1c7a":"markdown","b6f05352":"markdown","da013275":"markdown","f6a9dd27":"markdown"},"source":{"45ae6be8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","248c5f8a":"# loading data \ntrain_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","b5f7f380":"\"\"\"import pandas_profiling\ntrain_df.profile_report()\"\"\"","60532b49":"#duplicated value\ntrain_df.duplicated().sum()","4619d09a":"sns.boxplot(train_df[\"Age\"])\nplt.show()","30835e9a":"#return outlier in list\nout=[]\ndef outliers(train_df):\n    q1=train_df.quantile(.25)\n    q3=train_df.quantile(.75)\n    iqr=q3-q1\n    lower_tail=q1-1.5*iqr\n    upper_tail=q3+1.5*iqr\n    \n    for x in train_df:\n        if x < lower_tail or x > upper_tail:\n            out.append(x)\n    print(\" The outliers are : \",out)\noutliers(train_df[\"Age\"])","e5681bd7":"#remove outlier\nsns.boxplot(train_df[\"Age\"])\nplt.title(\"Box Plot before outlier removing\")\nplt.show()\ndef drop_outliers(train_df, field_name):\n    iqr = 1.5 * (np.percentile(train_df[field_name], 75) - np.percentile(train_df[field_name], 25))\n    train_df.drop(train_df[train_df[field_name] > (iqr + np.percentile(train_df[field_name], 75))].index, inplace=True)\n    train_df.drop(train_df[train_df[field_name] < (np.percentile(train_df[field_name], 25) - iqr)].index, inplace=True)\ndrop_outliers(train_df, 'Age')\nsns.boxplot(train_df[\"Age\"])\nplt.title(\"Box Plot after outlier removing\")\nplt.show()","ef0830dd":"train_df.isna().sum()","d337b080":"train_df.head()","e0140462":"train_df.describe()","42aaa1cc":"plt.figure(figsize = (30, 25))\nsns.heatmap(train_df.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","74162d3a":"\ndata=train_df.drop([\"Survived\"],axis=1)\nlabel=train_df['Survived']\nprint(label)\nlabel.value_counts()","63232186":"sns.heatmap(data.isnull(),yticklabels=False,cbar=False,cmap='viridis')","69622702":"data=data.drop([\"Cabin\"],axis=1)\ndata=data.drop([\"Name\"],axis=1)\n#we can change nan value in age vecteur with mean age\ndata=data.fillna(data.mean())\n\ntest=test_df.drop([\"Cabin\"],axis=1)\ntest=test.drop([\"Name\"],axis=1)\n#we can change nan value in age vecteur with mean age\ntest=test.fillna(test_df.mean())\ntest.head()\n\n\n","55db3ea3":"#change textuelle data to numeric data column Sex\ndata=data.drop(['Sex'],axis=1)\nfrom sklearn.preprocessing import LabelEncoder\nX2=train_df['Sex']\nle2=LabelEncoder()\nX2New=le2.fit_transform(X2)\nX2New=X2New.reshape(-1,1)\nSex=pd.DataFrame(X2New)\ndata=np.concatenate((data,Sex),axis=1)\ndata=pd.DataFrame(data)\nprint(data)\n\n#change textuelle data to numeric data column Embarked\ndata=data.drop([7],axis=1)\nfrom sklearn.preprocessing import LabelEncoder\nX3=train_df['Embarked']\nle3=LabelEncoder()\nX3New=le2.fit_transform(X3)\nX3New=X3New.reshape(-1,1)\nEmbarked=pd.DataFrame(X3New)\ndata=np.concatenate((data,Embarked),axis=1)\ndata=pd.DataFrame(data)\ndata=data.drop([5],axis=1)\n","cde96d30":"#change textuelle data to numeric data column Sex\ntest=test.drop(['Sex'],axis=1)\nfrom sklearn.preprocessing import LabelEncoder\nX2=test_df['Sex']\nle2=LabelEncoder()\nX2New=le2.fit_transform(X2)\nX2New=X2New.reshape(-1,1)\nSex=pd.DataFrame(X2New)\ntest=np.concatenate((test,Sex),axis=1)\ntest=pd.DataFrame(test)\nprint(test)\n\n#change textuelle data to numeric data column Embarked\ntest=test.drop([7],axis=1)\nfrom sklearn.preprocessing import LabelEncoder\nX3=test_df['Embarked']\nle3=LabelEncoder()\nX3New=le2.fit_transform(X3)\nX3New=X3New.reshape(-1,1)\nEmbarked=pd.DataFrame(X3New)\ntest=np.concatenate((test,Embarked),axis=1)\ntest=pd.DataFrame(test)\ntest=test.drop([5],axis=1)","3a5b34a4":"test.info()","d971db5d":"\"\"\"data.head()\nfrom sklearn.model_selection import train_test_split\nx_train1,x_test1,train_label,test_label=train_test_split(data,label,test_size=0.33,random_state=0)\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(x_train1, train_label)\npred=model.predict(test)\"\"\"","81c54f15":"\"\"\"from sklearn.metrics import accuracy_score\nACC=accuracy_score(test_label,pred)*100\nprint('the accurency score is :')\nprint(ACC)\"\"\"","a416479c":"\"\"\"from sklearn import svm\nfrom sklearn.model_selection import train_test_split\nx_train1,x_test1,train_label,test_label=train_test_split(data,label,test_size=0.33,random_state=0)\nfrom sklearn import svm\nclf=svm.SVC(kernel='linear',C=5,decision_function_shape='ovo')\nimport time\ndebut=time.time()\nclf.fit(x_train1,train_label)\nfin=time.time()-debut\npred=clf.predict(test)\"\"\"","ed717441":"\"\"\"from sklearn.metrics import accuracy_score\nACC=accuracy_score(test_label,pred)*100\nprint('the accurency score is :')\nprint(ACC)\n\"\"\"","6ac016bd":"\"\"\"from sklearn.metrics import classification_report\nprint(classification_report(test_label,pred))\n\"\"\"","141fc3ac":"\"\"\"from sklearn.metrics import confusion_matrix\nCM=confusion_matrix(test_label,pred)\nprint(CM)\n\n#heatmap de confusion matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nclass_names=[0,1] # name of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n# create heatmap\n# CM1=pd.DataFrame(CM)\n# print(CM1)\nsns.heatmap(pd.DataFrame(CM), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\n#plt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\"\"\"","12410716":"\"\"\"from sklearn.metrics import roc_curve,auc\nfp, tp, thresholds=roc_curve(test_label,pred,pos_label=1)\nprint(fp, tp)\nAUC=auc(fp, tp)*100\nprint(AUC)\n\n#tracer tp en fonction de fp\nimport matplotlib.pyplot as plt\nplt.plot(fp, tp, color='blue',label = 'AUC = %0.2f' % AUC)\nplt.title('Receiver Operating Characteristic')\n#plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % AUC)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\"\"\"","b156d9a2":"\"\"\"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\ndtree = DecisionTreeClassifier()\nx_train,x_test,y_train,y_test = train_test_split(data,label,test_size = 0.3,random_state = 0)\nimport time\ndebut=time.time()\ndtree.fit(x_train,y_train)\nfin=time.time()-debut\npred = dtree.predict(test)\"\"\"","c3936ab2":"\"\"\"from sklearn.metrics import accuracy_score\nACC=accuracy_score(y_test,prediction)*100\nprint('With decision tree accuracy is: ',ACC) # accuracy\"\"\"\n","ab7945dd":"\"\"\"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nrf = RandomForestClassifier(criterion = 'gini')\nx_train,x_test,y_train,y_test = train_test_split(data,label,test_size = 0.3,random_state =1)\nimport time\ndebut=time.time()\nrf.fit(x_train,y_train)\nfin=time.time()-debut\n\npred = rf.predict(test)\"\"\"","bc0813f8":"\"\"\"from sklearn.metrics import accuracy_score\nACC=accuracy_score(y_test,prediction)*100\nprint('With random forest accuracy is: ',ACC) # accuracy\"\"\"\n","1b13e398":"# check version number\nimport imblearn\nprint(imblearn.__version__)\nfrom imblearn.over_sampling import RandomOverSampler\n\n# define oversampling strategy\noversample = RandomOverSampler(sampling_strategy='minority')\n\n# define oversampling strategy\noversample = RandomOverSampler(sampling_strategy=1.0)\n\n# fit and apply the transform\nX_over, y_over = oversample.fit_resample(data, label)\n\n#compter combien de 1 et de 0 dans dataset\n\nlabel=y_over\ndata=X_over\nlabel.value_counts()","cbb02393":"\"\"\"from sklearn import svm\nfrom sklearn.model_selection import train_test_split\nx_train1,x_test1,train_label,test_label=train_test_split(data,label,test_size=0.33,random_state=0)\nfrom sklearn import svm\nclf=svm.SVC(kernel='linear',C=5,decision_function_shape='ovo')\nimport time\ndebut=time.time()\nclf.fit(x_train1,train_label)\nfin=time.time()-debut\npred=clf.predict(test_df)\"\"\"","3294be0f":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nrf = RandomForestClassifier(criterion=\"gini\")\nx_train,x_test,y_train,y_test = train_test_split(data,label,test_size = 0.3,random_state =1)\nimport time\ndebut=time.time()\nrf.fit(x_train,y_train)\nfin=time.time()-debut\n\npred = rf.predict(test)","048d184d":"\"\"\"from sklearn.metrics import accuracy_score\nACC=accuracy_score(test_label,pred)*100\nprint('the accurency score is :')\nprint(ACC)\"\"\"\n","060cc631":"print(data)","3bed69b4":"\"\"\"from sklearn import preprocessing\nstandard = preprocessing.scale(data)\nprint(standard)\"\"\"","0fe241e3":"\"\"\"from sklearn import svm\nfrom sklearn.model_selection import train_test_split\nx_train1,x_test1,train_label,test_label=train_test_split(data,label,test_size=0.33,random_state=0)\nfrom sklearn import svm\nclf=svm.SVC(kernel='linear',C=5,decision_function_shape='ovo')\nimport time\ndebut=time.time()\nclf.fit(x_train1,train_label)\nfin=time.time()-debut\npred=clf.predict(x_test1)\"\"\"","2c600a6c":"\"\"\"from sklearn.metrics import accuracy_score\nACC=accuracy_score(test_label,pred)*100\nprint('the accurency score is :')\nprint(ACC)\"\"\"\n","c73173e8":"\"\"\"from sklearn.model_selection import cross_val_score, cross_validate\nfrom sklearn.model_selection import GridSearchCV \nrf_params = {'random_state': [1],\n             'max_depth': [10, 11, 12],\n             'max_features': [8],\n             'min_samples_leaf': [1, 2],\n             'min_samples_split': [2, 5, 10],\n             'n_estimators': [113]}\n\ngrid = GridSearchCV(rf, \n                    rf_params,\n                    cv = 10,   \n                    n_jobs = -1)\n\ngrid.fit(x_train,y_train)\"\"\"","cdea0162":"#grid.best_params_","95a09f22":"\"\"\"rf = RandomForestClassifier(**grid.best_params_)\n\ncv_results = cross_val_score(rf, x_train,y_train, cv = 10)\n\nprint(f'All results: {cv_results} \\n\\n' +\n      f'Mean: {cv_results.mean()} \\n\\n' +\n      f'Std: {cv_results.std()} \\n\\n')\"\"\"","632774ad":"\"\"\"rf.fit(x_train,y_train)\npredictions = rf.predict(test)\"\"\"","feaba93f":"data={\"PassengerId\":[],\"Survived\":[]}\nfor id,pred in zip(test[0].unique(),pred):\n  data[\"PassengerId\"].append(id)\n  data[\"Survived\"].append(pred)\n    \noutput=pd.DataFrame(data,columns=[\"PassengerId\",\"Survived\"])\noutput\nprint(output)\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\"\"\"\nsubmission = pd.DataFrame({\n        \"PassengerId\": test[0],\n        \"Survived\": pred\n    })\nsubmission.to_csv('submission1.csv', index=False)\nprint(\"file saved !!!\")\"\"\"","696fcd04":"# standardizer data","6697c60d":"# accurency","150a7f72":"# roc_curve,auc","618e4b45":"# Clean the data","55e13116":"# Random forest","dab9fef3":"# SVM","5792b2f1":"# LabelEncoder","5360e318":"# Decision tree","23e14a6a":"# Confusion matrix","d869d80a":"we check the most correlated features with survived","269104cf":"# check outlier","b59c50d7":"# Classification","ccdf5bc4":"cabin contains a lot of nan that's why we should drop it","6adea129":"# Report","f76fdb5b":"# R\u00e9gression logistic","61a50311":"# SVM after oversampling","b07a1c7a":"# accurecy","b6f05352":"# accurency","da013275":"# ====>we aknowledge that accurency < 80% that why we should improve preprocessing of data","f6a9dd27":"# random forest"}}