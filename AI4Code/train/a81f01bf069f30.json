{"cell_type":{"44575f49":"code","e68d3b1d":"code","852388f4":"code","9745845c":"code","9fb17a50":"code","d47b91b1":"code","006c1482":"code","1423e94f":"code","8032e57c":"code","ee94410c":"code","9df03bb9":"code","639761f5":"code","33b9329d":"code","9bf416dd":"code","35e8021f":"code","6798a107":"code","0a0d0cdc":"code","b35935d4":"code","7ab28e81":"code","9a1b51e1":"code","66c33541":"code","00d737d2":"code","99ae7c76":"code","13573120":"code","7b9f8c35":"code","8b5b0d0b":"code","f610ecf0":"code","91068e8e":"code","3e987136":"code","c12aa9e3":"code","e9d6878c":"code","e84cfa12":"code","f21587ed":"code","79ebfa8f":"code","07527a18":"code","8b1dc784":"code","063a8ac2":"code","9a216f81":"code","f65208b3":"code","149350cd":"code","184e10a4":"code","41baac59":"code","68a765ed":"code","c4a3676b":"code","6267fef7":"code","8aa8a59b":"code","401b59e9":"code","eaa6df92":"markdown","fbb0c5c7":"markdown","578ab534":"markdown","46ce289b":"markdown","2a0ed272":"markdown","50ed4079":"markdown","e890160b":"markdown","849a42e7":"markdown","26cb0db0":"markdown","034e7dfc":"markdown","54ded4de":"markdown","a4d8c5c8":"markdown","8a717bef":"markdown","182e94f4":"markdown","8af8a626":"markdown","2166cc1f":"markdown","4427d12e":"markdown","fd7349f3":"markdown","4fbd5ab7":"markdown","c2ce146f":"markdown","0a492e92":"markdown","7956666c":"markdown","2baa67b8":"markdown","16b69ec9":"markdown"},"source":{"44575f49":"#%tensorflow_version 2.x\nimport tensorflow as tf\ntf.__version__","e68d3b1d":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport scipy.stats as stats \nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\n#from keras.models import Sequential\n#from keras.layers import Dense\n#from sklearn.model_selection import StratifiedKFold\n%matplotlib inline\n#Test Train Split\nfrom sklearn.model_selection import train_test_split\n#Feature Scaling library\nfrom sklearn.preprocessing import StandardScaler\n#import pickle\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Flatten, Dense\nfrom tensorflow.keras import regularizers, optimizers\nfrom sklearn.metrics import r2_score\nfrom tensorflow.keras.models import load_model","852388f4":"# Initialize the random number generator\nimport random\nseed = 7\nnp.random.seed(seed)\n\n# Ignore the warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","9745845c":"#Read the data as a data frame\nmydata = pd.read_csv('..\/input\/part-123-signalcsv\/Part- 123 - Signal.csv')\nmydata.head(20)","9fb17a50":"# Shape of the data \nmydata.shape","d47b91b1":"# Data type of each attribute \nmydata.info()   # it gives information about the data and data types of each attribute","006c1482":"# Checking the presence of missing values\nnull_counts = mydata.isnull().sum()  # This prints the columns with the number of null values they have\nprint (null_counts)","1423e94f":"# 5 point summary of numerical attributes\nmydata.describe()","8032e57c":"# studying the distribution of continuous attributes\ncols = list(mydata)\nfor i in np.arange(len(cols)):\n    sns.distplot(mydata[cols[i]], color='blue')\n    #plt.xlabel('Experience')\n    plt.show()\n    print('Distribution of ',cols[i])\n    print('Mean is:',mydata[cols[i]].mean())\n    print('Median is:',mydata[cols[i]].median())\n    print('Mode is:',mydata[cols[i]].mode())\n    print('Standard deviation is:',mydata[cols[i]].std())\n    print('Skewness is:',mydata[cols[i]].skew())\n    print('Maximum is:',mydata[cols[i]].max())\n    print('Minimum is:',mydata[cols[i]].min())","ee94410c":"sns.countplot(mydata['Signal_Strength'])    # Distibution of the column 'Signal_Strength'\nplt.show()","9df03bb9":"#plt.figure(figsize = (50,50))\nsns.pairplot(mydata,diag_kind='kde')\nplt.show()","639761f5":"# Checking the presence of outliers\nl = len(mydata)\ncol = list(mydata.columns)\n#col.remove('condition')\nfor i in np.arange(len(col)):\n    sns.boxplot(x= mydata[col[i]], color='cyan')\n    plt.show()\n    print('Boxplot of ',col[i])\n    #calculating the outiers in attribute \n    Q1 = mydata[col[i]].quantile(0.25)\n    Q2 = mydata[col[i]].quantile(0.50)\n    Q3 = mydata[col[i]].quantile(0.75) \n    IQR = Q3 - Q1\n    L_W = (Q1 - 1.5 *IQR)\n    U_W = (Q3 + 1.5 *IQR)    \n    print('Q1 is : ',Q1)\n    print('Q2 is : ',Q2)\n    print('Q3 is : ',Q3)\n    print('IQR is:',IQR)\n    print('Lower Whisker, Upper Whisker : ',L_W,',',U_W)\n    bools = (mydata[col[i]] < (Q1 - 1.5 *IQR)) |(mydata[col[i]] > (Q3 + 1.5 * IQR))\n    print('Out of ',l,' rows in data, number of outliers are:',bools.sum())   #calculating the number of outliers","33b9329d":"#  function to treat outliers\ndef detect_treate_outliers(df,operation):\n    cols=[]\n    IQR_list=[]\n    lower_boundary_list=[]\n    upper_boundary_list=[]\n    outliers_count=[]\n    for col in df.columns:\n        #print('col',col)\n        if((df[col].dtype =='int64' or df[col].dtype =='float64') and (col != 'HR')):\n            #print('Inside if')\n            IQR = df[col].quantile(0.75) - df[col].quantile(0.25)\n            lower_boundary = df[col].quantile(0.25) - (1.5 * IQR)\n            upper_boundary = df[col].quantile(0.75) + (1.5 * IQR)\n            up_cnt = df[df[col]>upper_boundary][col].shape[0]\n            #print('Upper count=',up_cnt)\n            lw_cnt = df[df[col]<lower_boundary][col].shape[0]\n            #print('lower count=',lw_cnt)\n            if(up_cnt+lw_cnt) > 0:\n                cols.append(col)\n                IQR_list.append(IQR)\n                lower_boundary_list.append(lower_boundary)\n                upper_boundary_list.append(upper_boundary)\n                outliers_count.append(up_cnt+lw_cnt)\n                if operation == 'update':\n                    df.loc[df[col] > upper_boundary,col] = upper_boundary\n                    df.loc[df[col] < lower_boundary,col] = lower_boundary\n                else:\n                    pass\n            else:\n                pass\n   #print('cols=',cols)\n   # print('IQR_list=',IQR_list)\n   # print('lower_boundary_list=',lower_boundary_list)\n   # print('upper_boundary_list=',upper_boundary_list)\n   # print('outliers_count=',outliers_count)\n    ndf = pd.DataFrame(list(zip(cols,IQR_list,lower_boundary_list,upper_boundary_list,outliers_count)),columns=['Features','IQR','Lower Boundary','Upper Boundary','Outlier Count'])\n    #print('Data=',ndf)\n    #print('Columns having outliers=',cols)\n    if operation == 'update':\n        return (len(cols),df)\n    else:\n        return (len(cols),ndf)","9bf416dd":"#Removing outliers by replacing the data below lower whisker with it and above upper whisker with it respectively.\ncount,df=detect_treate_outliers(mydata,'update')\nif count>0:\n    print('Updating dataset')\n    mydata=df","35e8021f":"# studying correlation between the attributes\nb_corr=mydata.corr()\nplt.subplots(figsize =(12, 7)) \nsns.heatmap(b_corr,annot=True)","6798a107":"X = mydata.drop(\"Signal_Strength\", axis=1)\ny = mydata['Signal_Strength']","0a0d0cdc":"from sklearn.model_selection import train_test_split\n\n# splitting to create test data\nX_vtrain, X_test, y_vtrain, y_test = train_test_split(X, y, test_size=.30, random_state=seed)","b35935d4":"X_vtrain.shape","7ab28e81":"# splitting to create training and validation data\nX_train, X_val, y_train, y_val = train_test_split(X_vtrain, y_vtrain, test_size=.20, random_state=seed)","9a1b51e1":"X_train.shape","66c33541":"# Initialize Sequential model\nmodel_reg = tf.keras.models.Sequential()\n\n# Normalize input data\nmodel_reg.add(tf.keras.layers.BatchNormalization(input_shape=(11,)))\n\n# Add final Dense layer for prediction - Tensorflow.keras declares weights and bias automatically\nmodel_reg.add(tf.keras.layers.Dense(1))","00d737d2":"# Compile the model - add mean squared error as loss and stochastic gradient descent as optimizer\nmodel_reg.compile(optimizer='sgd', loss='mse')\n","99ae7c76":"model_reg.fit(X_train, y_train, validation_data=(X_val,y_val),epochs=100, batch_size=10)","13573120":"# save the model\nmodel_reg.save(\"model_reg.h5\") #using h5 extension\nprint(\"model saved!!!\")","7b9f8c35":"# load the model\nmodel_rr = load_model('model_reg.h5')","8b5b0d0b":"# Save the Modle to file in the current working directory\n\n#Pkl_Filename = \"Pickle_RR_Model.pkl\"  \n#with open(Pkl_Filename, 'wb') as file:  \n#    pickle.dump(model_reg, file)","f610ecf0":"# Load the Model back from file\n\n#with open(Pkl_Filename, 'rb') as file:  \n#    Pickled_RR_Model = pickle.load(file)\n\n#Pickled_RR_Model","91068e8e":"y_pred = model_rr.predict(X_test)","3e987136":"print(y_pred[0])\nprint(y_pred[1])\nprint(y_pred[2])\nprint(y_pred[3])\nprint(y_pred[4])\n","c12aa9e3":"print(y_test.head())","e9d6878c":"score_r = r2_score(y_test,y_pred)\nprint(score_r)","e84cfa12":"#summary of regression model\nmodel_rr.summary()","f21587ed":"# counting the number of classes in output\nmydata['Signal_Strength'].value_counts()","79ebfa8f":"X.shape","07527a18":"y.shape","8b1dc784":"yc = to_categorical(y, num_classes=8)","063a8ac2":"# splitting data for test of categorial \nXcv_train, Xc_test, ycv_train, yc_test = train_test_split(X, yc, test_size=.30, random_state=seed)","9a216f81":"print(\"Shape of y_train:\", ycv_train.shape)\nprint(\"One value of y_train:\", ycv_train[0])","f65208b3":"# splitting data for  train and validation of categorial \nXc_train, Xc_val, yc_train, yc_val = train_test_split(Xcv_train, ycv_train, test_size=.20, random_state=seed)","149350cd":"print(\"Shape of y_train:\", yc_train.shape)\nprint(\"One value of y_train:\", yc_train[0])","184e10a4":"model_class = Sequential()\nmodel_class.add(Dense(11, activation='relu'))\nmodel_class.add(Dense(8, activation='relu'))\nmodel_class.add(Dense(8, activation='softmax'))","41baac59":"# Compile the model\nmodel_class.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=\"sgd\")\n\n# Fit the model\nmodel_class.fit(x=Xc_train, y=yc_train, batch_size=20, epochs=100, validation_data=(Xc_val, yc_val))","68a765ed":"# save the model\nmodel_class.save(\"model_class.h5\") #using h5 extension\nprint(\"model saved!!!\")","c4a3676b":"# load the model\nmodel_cl = load_model('model_class.h5')","6267fef7":"# calculate score of training data\nscore = model_cl.evaluate(Xc_train, yc_train, verbose=0)\nprint(score)","8aa8a59b":"# score of test data\nscore_t = model_cl.evaluate(Xc_test, yc_test, verbose=0)\nprint( score_t)","401b59e9":"#summary of classification model\nmodel_cl.summary()","eaa6df92":"There are no null values in the data","fbb0c5c7":"class 5.0 in 'Signal_Strength' has the highest count.","578ab534":"There are 1599 rows and 12 columns in data","46ce289b":"4. Pickle the model for future use.","2a0ed272":"4. Pickle the model for future use.","50ed4079":"3. Design, train, tune and test a neural network regressor. ","e890160b":"The first 5 elements of y_pred and y_test are close.","849a42e7":"2. Data analysis & visualisation ","26cb0db0":"1.Parameter 6 and Parameter 7 are highly correlated with each other and visce versa and they have almost 0 correlation with other Parameters\n2.Parameter 1 is positively correlated to Parameter 3 and Parameter 8 and negatively correlated to Parameter 2 and Parameter 9.\n3.Parameter 4 is has very low correlation with other Parameters.","034e7dfc":"Since high correlation coefficient value lies between \u00b1 0.50 and \u00b1 1\nParameter 1 is highly correlated with Parameter 3 and Parameter 8, Parameter 9.\nParameter 6 and 7 are highly correlated.\nBut since, the correlation is not too high near 0.8 or above not dropping the features.","54ded4de":"PROJECT OBJECTIVE: The need is to build a regressor which can use these parameters to determine the signal strength or \nquality\nDOMAIN:  Electronics and Telecommunication \nCONTEXT: A communications equipment manufacturing company has a product which is responsible for emitting informative signals. \nCompany wants to build a machine learning model which can help the company to predict the equipment\u2019s signal quality using \nvarious parameters.\nDATA DESCRIPTION: The data set contains information on various signal tests performed: \n        1. Parameters: Various measurable signal parameters. \n        2. Signal_Quality: Final signal strength or quality ","a4d8c5c8":"All the parameters are floating point and the signal strength is an integer.","8a717bef":"# Part 2","182e94f4":"Apart from Signal Strength rest all features are floating point.","8af8a626":"Steps 1 and 2 are same as for the regressor above","2166cc1f":"There are 1599 rows and 12 columns","4427d12e":"1. Import data. ","fd7349f3":"Mean, median and mode are almost overlapping or too close to each other ecept in Parameter 7\nParameter 3 is trimodal and Signal strength is a classification variable.\nAll of them are positively skewed.\nStandard deviation is maximum for Parameter7, it is 32.895324478299074","4fbd5ab7":"error when trying to pickle is - \nTypeError: cannot pickle 'weakref' object\nand to resolve 'weakref' object we need to import dill and weakref butit cannot be saved with pickle, so \nI have used save() to save the model and load_model() to load it.\n","c2ce146f":"Looking the 11 parameters :\nParameter 3 ranges between 0 and 1.\nMaximum value of Parameter 5 is 0.6\nParameter 8 has a very low range between 0.9 and 1.004\nStandard deviation is lowest for Parameter 8, it is 0.001887\n'Signal_Strength' has classes as - 3.5, 4.0,5.0, 6.0, 7.0 and 7.5 ","0a492e92":"3. Design, train, tune and test a neural network classifier.","7956666c":"Parameter 4 has the highest number of outliers which is 155.","2baa67b8":"# PROJECT OBJECTIVE: The need is to build a classifier which can use these parameters to determine the signal strength or quality .","16b69ec9":"# Case Study of equipment\u2019s signal quality"}}