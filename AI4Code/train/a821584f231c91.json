{"cell_type":{"32dfef19":"code","b34fd42d":"code","1b754d57":"code","bc2cc40a":"code","5cd98920":"code","94b7f6f6":"code","8bee0f54":"code","d11cd8b7":"code","1bdad0a1":"code","791b2464":"code","57546a31":"code","db8a24f7":"code","3427b50c":"code","7e19075d":"code","e877dbc2":"code","d383b210":"code","cb700084":"code","b52166f9":"code","9c7daf34":"code","9411259b":"code","b8720b9d":"code","89c90191":"code","035ffa1f":"code","c8a6771a":"code","dd0174a0":"code","6b03256e":"code","efe26a3b":"code","892a84b8":"code","691858fb":"code","10efc9e6":"code","24e5412b":"code","a6e8915d":"code","63ae477b":"code","8235fed7":"code","f323052c":"code","dfd9e1ef":"code","96a729b5":"code","78993f4c":"code","682b27d1":"code","b2815f94":"code","686c8904":"code","8c0a29f5":"code","eac4cbc3":"code","36cd5860":"code","7c2482a4":"code","b1333dab":"markdown","0bc3bf9f":"markdown","fbcf05a8":"markdown","8fcf35f1":"markdown","a501d862":"markdown","7569e589":"markdown","6a7f5362":"markdown","1f5417d9":"markdown","cdc828ac":"markdown","36f179d8":"markdown","64b82c1b":"markdown"},"source":{"32dfef19":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b34fd42d":"# Linear algebra\nimport numpy as np\n\n# Data preprocessing csv\nimport pandas as pd\n\n# Data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n# Splitting data train and validation\nfrom sklearn.model_selection import train_test_split\n\n# Tensorflow for modelling\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout","1b754d57":"print(\"Tensorflow version : {}\".format(tf.__version__))","bc2cc40a":"# Load data training\ndf_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\n\n# Load data testing\ndf_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","5cd98920":"# split data between data train and the label\ntrain = df_train.drop(['SalePrice'], axis=1)\ny = df_train['SalePrice']","94b7f6f6":"# concat between data train and data test\ndata=pd.concat([train, df_test], axis=0)","8bee0f54":"# print each shape of data\nprint(\"The shape of data train : {} \".format(df_train.shape))\nprint(\"The shape of data test : {} \".format(df_test.shape))\nprint(\"The shape of all data (label excluded) : {} \".format(data.shape))","d11cd8b7":"# Check numercial data\ndata.describe()","1bdad0a1":"# Check numercial data\ndf_train.describe(include=['object', 'bool'])","791b2464":"df_train.sort_values(by='SalePrice', ascending=False).head()","57546a31":"# Check total price of each year \ndf_analysis = {'Year Sold':['2006', '2007', '2008', '2009', '2010'],\n               'Mean Sale Price': [round(df_train.query('YrSold == 2006').SalePrice.mean()),\n                            round(df_train.query('YrSold == 2007').SalePrice.mean()),\n                            round(df_train.query('YrSold == 2008').SalePrice.mean()),\n                            round(df_train.query('YrSold == 2009').SalePrice.mean()),\n                            round(df_train.query('YrSold == 2010').SalePrice.mean())],\n               'Min Sale Price': [df_train.query('YrSold == 2006').SalePrice.min(),\n                                  df_train.query('YrSold == 2007').SalePrice.min(),\n                                  df_train.query('YrSold == 2008').SalePrice.min(),\n                                  df_train.query('YrSold == 2009').SalePrice.min(),\n                                  df_train.query('YrSold == 2010').SalePrice.min()],\n               'Max Sale Price': [df_train.query('YrSold == 2006').SalePrice.max(),\n                                  df_train.query('YrSold == 2007').SalePrice.max(),\n                                  df_train.query('YrSold == 2008').SalePrice.max(),\n                                  df_train.query('YrSold == 2009').SalePrice.max(),\n                                  df_train.query('YrSold == 2010').SalePrice.max()]}\ndf_temp = pd.DataFrame(df_analysis, index=['1', '2', '3', '4', '5'])\ndf_temp","db8a24f7":"# average price of house sales\nsns.set(style='whitegrid')\nsns.lineplot(x=\"Year Sold\", y=\"Mean Sale Price\", \n                  data=df_temp)","3427b50c":"# Let's see the correlation between each column (numerical column) \n# give the most correlation with saleprice\nfor i in range(0, len(df_train.columns), 4):\n    sns.pairplot(data=df_train,\n                x_vars=df_train.columns[i:i+4],\n                y_vars=['SalePrice'])","7e19075d":"# Find the percentage of NaN value in data\n# check target nan value in the data \ncombined = data.copy()\nnan_percentage = combined.isnull().sum().sort_values(\n    ascending=False) \/ combined.shape[0]\nmissing_val = nan_percentage[nan_percentage > 0]\n\nplt.figure(figsize=(9,7))\nsns.barplot(x=missing_val.index.values, \n            y=missing_val.values * 100, \n            palette=\"Reds_r\");\nplt.title(\"Percentage of missing values in data\");\nplt.ylabel(\"%\");\nplt.xticks(rotation=90);","e877dbc2":"# Delete columns that has percentage NaN value above 50%\ndata.drop(columns=['PoolQC','MiscFeature','Alley','Fence','FireplaceQu'],\n          axis=1, inplace=True)","d383b210":"# Delete columns 'Id' because that is not important to the training process\ndata.drop(columns=['Id'], inplace=True, axis=1)","cb700084":"# Selecting columns that contain NaN value\nnan_col = data.columns[data.isnull().any()].tolist()\ndata[nan_col].head(5)","b52166f9":"# Fill NaN value with dtype is not Object with the average of the data\ndf=data.fillna(data.mean())","9c7daf34":"# Find the percentage of NaN value in data\n# check target nan value in the data \ncombined = df.copy()\nnan_percentage = combined.isnull().sum().sort_values(\n    ascending=False) \/ combined.shape[0]\nmissing_val = nan_percentage[nan_percentage > 0]\n\nplt.figure(figsize=(9,7))\nsns.barplot(x=missing_val.index.values, \n            y=missing_val.values * 100, \n            palette=\"Reds_r\");\nplt.title(\"Percentage of missing values in data\");\nplt.ylabel(\"%\");\nplt.xticks(rotation=90);","9411259b":"# Selecting columns that still contain NaN value\nnan_col = df.columns[df.isnull().any()].tolist()","b8720b9d":"# Replace NaN value on each categorical column with modus of the data\nfor column in nan_col:\n    df[column].fillna(df[column].mode()[0], inplace=True)","89c90191":"# Select all numerical columns\nnumerical_columns=df.select_dtypes(exclude=['O']).columns","035ffa1f":"# Print 5 random numerical columns that we are going to normalize it into 0-1\ndf[numerical_columns].sample(5, random_state=99)","c8a6771a":"from sklearn.preprocessing import MinMaxScaler","dd0174a0":"# build function to data normalization\ndef normalize_data(df):\n  # Select all numerical columns\n  numerical_columns=df.select_dtypes(exclude=['O']).columns\n\n  scaling=MinMaxScaler()\n  df[numerical_columns]=scaling.fit_transform(df[numerical_columns])\n  return df","6b03256e":"# normalize data\ndf_norm = normalize_data(df)","efe26a3b":"# Check if the data already transformed\ndf_norm[numerical_columns].sample(5, random_state=99)","892a84b8":"df_norm.sample(5, random_state=99)","691858fb":"# Select all categorical columns\ncategorical_columns=df_norm.select_dtypes(include=['O']).columns","10efc9e6":"print(categorical_columns)","24e5412b":"from sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder\nimport sklearn as s","a6e8915d":"# Check sklearn version \nprint(s.__version__)","63ae477b":"# Build object to column_transformer\ncolumn_transformer = make_column_transformer(\n    (OneHotEncoder(), categorical_columns),\n    remainder='passthrough'\n)","8235fed7":"# Transform data \ndf_encoded = column_transformer.fit_transform(df_norm)","f323052c":"df_encoded","dfd9e1ef":"# Change data type from sparse into DataFrame\nimport scipy.sparse\ndf_encode = pd.DataFrame.sparse.from_spmatrix(df_encoded)","96a729b5":"df_encode.sample(5, random_state=99)","78993f4c":"# Bring back data training and data testing \nX_train = df_encode.iloc[:-len(df_test), :]\nX_test = df_encode.iloc[len(df_train):, :]","682b27d1":"# Make sure that we split data with the right shape and place\nprint(\"\\nOld data\")\nprint(\"The shape of data train : {} \".format(df_train.shape))\nprint(\"The shape of data test : {} \".format(df_test.shape))\n\nprint(\"\\nNew data\")\nprint(\"The shape of data train : {} \".format(X_train.shape))\nprint(\"The shape of data test : {} \".format(X_test.shape))\n\nprint(\"The shape of class : {} \".format(y.shape))","b2815f94":"# Make an object to a model\nmodel = Sequential()\n\n# The Input Layer \nmodel.add(Dense(512, kernel_initializer='normal',\n                   input_dim = X_train.shape[1], \n                   activation='relu'))\n\n# The Hidden Layers \nmodel.add(Dense(216, kernel_initializer='normal',activation='relu'))\nmodel.add(Dense(216, kernel_initializer='normal',activation='relu'))\nmodel.add(Dense(128, kernel_initializer='normal',activation='relu'))\nmodel.add(Dense(64, kernel_initializer='normal',activation='relu'))\nmodel.add(Dense(8, kernel_initializer='normal',activation='relu'))\n\n# The Output Layer \nmodel.add(Dense(1, kernel_initializer='normal',activation='linear'))\n\n# Compile the network \nmodel.compile(loss='mean_absolute_error', \n                 optimizer='adam', \n                 metrics=['mean_absolute_error'])\nmodel.summary()","686c8904":"# Build callback object to stop learning\nfilepath = 'model.{epoch:02d}-{val_loss:.2f}.h5'\ncallback = ModelCheckpoint(filepath, \n                             monitor='val_loss', \n                             verbose = 1, \n                             save_best_only = True, \n                             mode ='auto')","8c0a29f5":"# Train a model\nmodel.fit(X_train, y, \n             epochs=1000, \n             batch_size=32, \n             validation_split = 0.25, \n             callbacks=[callback])","eac4cbc3":"# Predict with the data testing \npredictions = model.predict(X_test)","36cd5860":"# Load submission file\nsubmission = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsubmission['SalePrice'] = predictions\nsubmission.head(6)","7c2482a4":"# Save the prediction\nsubmission.to_csv('Submission.csv', index=False)","b1333dab":"# Import Dataset","0bc3bf9f":"# Predict","fbcf05a8":"# Data Analysis","8fcf35f1":"## Taking care NaN value with type is float\n---\nThe strategy : replace with the average of that column\n","a501d862":"# Import Libraries","7569e589":"# Split dataset","6a7f5362":"# Normalize the data","1f5417d9":"## Taking care NaN value with type is Object.\n---\nThe strategy : replace with mode of that column","cdc828ac":"# Categorical Variable","36f179d8":"# Training the model","64b82c1b":"# Data Preparation"}}