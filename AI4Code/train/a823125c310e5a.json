{"cell_type":{"dfbe5742":"code","f1518562":"code","4901ba6b":"code","1917317b":"code","d0300ade":"code","585d1761":"code","3ea833f7":"code","a4b4ffed":"code","69db4eb2":"code","d2bf3200":"code","7bae08cb":"code","87abcee8":"code","18361075":"code","3b4827a2":"code","c823715d":"code","ca37a809":"code","17de723f":"code","cb55f02a":"code","df0bb819":"code","41fb1047":"code","ebb7116f":"code","497aed98":"code","9714ce86":"code","69906e20":"markdown","13f9f9f0":"markdown","c67a8e70":"markdown","8cf6d343":"markdown","94874a34":"markdown","752cc891":"markdown","74f377ed":"markdown","d4641331":"markdown","ff1bebe1":"markdown","e8eaa9c1":"markdown","1499ec7f":"markdown","68f2ec04":"markdown","e641cf23":"markdown","7a064ad1":"markdown","86c6cf7b":"markdown","d227ad24":"markdown","588a80b5":"markdown","feb48e59":"markdown"},"source":{"dfbe5742":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re # regular expression for text matching\nimport json\nimport os","f1518562":"#load the files\nallmeta = pd.read_csv('..\/input\/CORD-19-research-challenge\/metadata.csv', low_memory=False)\nprint('All metadata: ', allmeta.shape)\n\n#We can see here that several columns are almost entirely null, so we could probably drop those\n#allmeta.isna().sum().plot(kind='bar', stacked=True)\n\n#Drop duplicates\ncovmeta = allmeta.drop_duplicates(subset=('cord_uid','title'), keep='first')\n#Drop articles with no full text\ncovmeta = covmeta.dropna(subset=('pdf_json_files','pmc_json_files'), how='all')\n#Drop articles with no abstract\ncovmeta = covmeta.dropna(subset=('abstract',))\n#Drop articles from before 2020\ncovmeta = covmeta[covmeta['publish_time'].str.contains('2020')]\nprint('Clean metadata: ', covmeta.shape)\n\nallmeta = None","4901ba6b":"def search_column(df, column, regex):\n    return df[df[column].str.contains(regex, case=False, flags=re.IGNORECASE)]\n\n#search just for abstracts that mention covid-19\ncovmeta_regex = r'covid|-cov-2|cov2|ncov|sars-cov-2|wuhan|novel\\scoronavirus|coronavirus\\s2019'\ncovmeta = search_column(covmeta, 'abstract', covmeta_regex)\nprint(\"Covid metadata:\", covmeta.shape)\n\ncovmeta.head()","1917317b":"#load all the full text; this will take a while\ncovfiles = []\nfor index, row in covmeta.iterrows():\n    path = ''\n    if isinstance(row['pmc_json_files'], str):\n        #print(\"PMC: \", row['pmc_json_files'])\n        path = row['pmc_json_files'].split(';')[0]\n    else:\n        #print(\"PDF: \", row['pdf_json_files'])\n        if isinstance(row['pdf_json_files'], str):\n            path = row['pdf_json_files'].split(';')[0]\n            \n    if path:\n        with open(os.path.join('\/kaggle\/input\/CORD-19-research-challenge\/', path), 'rb') as file:\n            json_data = json.load(file)\n            json_data['meta_id'] = row['cord_uid']\n            covfiles.append(json_data)\n\nprint(covfiles[0]['body_text'][1]['text'])\ncovtext = pd.DataFrame(covfiles)\nprint(\"Text data:\", covtext.shape)\n\ncovfiles = None\n\n#abstract is always null so we can drop it\n#covtext.isna().sum().plot(kind='bar', stacked=True)\ncovtext = covtext.drop('abstract', axis=1)\n#covtext.head()\ncovtext.head()","d0300ade":"#Note: This function based on https:\/\/www.kaggle.com\/mlconsult\/round-2-working-example-material-studies-covid-19\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    \n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        body += section.replace('\\n', ' ')\n        body += \"\\n\\n\"\n        body += text.replace('\\n', ' ')\n        body += \"\\n\\n\"\n    \n    return body\n\ncovtext['reformatted_body'] = covtext.body_text.apply(format_body)\n\ndef split_sentences(text):\n    reformatted = re.sub(r'\\. |\\.(?!\\d)', '.\\n', text)\n    #reformatted = text.replace('. ', '.\\n')\n    return reformatted.split('\\n')\n\ncovtext['sentences'] = covtext.reformatted_body.apply(split_sentences)\n\ncovtext.head()","585d1761":"def quickprint(text):\n    return print(f'{text}\\n')\n\ndef search_title(df, regex):\n    return df[df['title'].str.contains(regex, case=False, flags=re.IGNORECASE)]\n\ndef search_title_and_abstract(df, regex):\n    return df[df['title'].str.contains(regex, case=False, flags=re.IGNORECASE) | \n              df['abstract'].str.contains(regex, case=False, flags=re.IGNORECASE)]\n\ndef get_sentence_match(regex, sentences):    \n    for s in sentences:\n        if re.search(regex, s, flags=re.IGNORECASE):\n            return s \n        \ndef get_best_match(ranked_regexes, sentences):\n    for regex in ranked_regexes:\n        s = get_sentence_match(regex, sentences)\n        if s != None:\n            return s\n        \ndef get_journal(row):\n    if isinstance(row.journal, str):\n        return row.journal\n    return row.source_x\n\n#returns a single tag for the first match found\ndef get_exclusive_tag_from_text(text, searches_and_tags):\n    for search_and_tag in searches_and_tags:\n        if re.search(search_and_tag[0], text, flags=re.IGNORECASE):\n            return search_and_tag[1]\n    return ''\n\n#returns a list of tags for all matches found\ndef get_tags_from_text(text, searches_and_tags):\n    tags = []\n    for search_and_tag in searches_and_tags:\n        if re.search(search_and_tag[0], text, flags=re.IGNORECASE):\n            tags.append(search_and_tag[1])\n            continue\n    return tags\n\ndef get_tags_string_from_text(text, searches_and_tags):\n    return ', '.join(get_tags_from_text(text, searches_and_tags))\n\ndef regex_shuffle2(term1, term2):\n    return f'({term1}.*{term2})|({term2}.*{term1})'\n\ndef regex_shuffle3(term1, term2, term3):\n    return f'({term1}.*{term2}.*{term3})|({term1}.*{term3}.*{term2})|({term2}.*{term1}.*{term3})|({term2}.*{term3}.*{term1})|({term3}.*{term1}.*{term2})|({term3}.*{term2}.*{term1})'\n\nstudy_search_and_tags = [[r'\\bSystematic review\\b', 'Systematic review and meta-analysis'],\n                         [r'\\bProspective observational\\b', 'Prospective observational study'],\n                         [r'\\bRetrospective observational\\b', 'Retrospective observational study'],\n                         [r'\\bCross.?sectional\\b', 'Cross-sectional study'],\n                         [r'\\bEcological regression\\b', 'Ecological regression'],\n                         [r'\\bCase series\\b', 'Case series'],\n                         [r'\\bExpert review\\b', 'Expert review'],\n                         [r'\\bEditorial\\b', 'Editorial'],\n                         [r'\\bSimulation\\b', 'Simulation']]\n        \ndef get_study_from_row(row):\n    study = get_exclusive_tag_from_text(row.abstract, study_search_and_tags)\n    if study != '' and study != 'Simulation':\n        return study\n    #fulltext = covtext[covtext.meta_id == row.cord_uid].reformatted_body.item()\n    return get_exclusive_tag_from_text(row.reformatted_body, study_search_and_tags)\n\nseverity_search_and_tags = [[r'\\bsevere(?! acute)\\b', 'Severe'],\n                            [r'\\bmild\\b', 'Mild'],\n                            [r'\\bmoderate\\b', 'Moderate'],\n                            [r'\\bvaried\\b', 'Varied'],\n                            [r'(\\bnon.?icu)|(\\bnon.?intensive care unit)', 'Non-ICU'],\n                            [r'(\\b(?! non.?)ICU\\b)|(\\b(?! non.?)intensive care unit\\b)', 'ICU']]\n\ndef get_severity_from_text(text):\n    return get_tags_string_from_text(text, severity_search_and_tags)                            ","3ea833f7":"taskdir = '..\/input\/CORD-19-research-challenge\/Kaggle\/target_tables\/7_therapeutics_interventions_and_clinical_studies\/'\noutput_examples = []\nfor filename in os.listdir(taskdir):\n    example = pd.read_csv(taskdir+filename)\n    print(filename,\" Index: \",len(output_examples), \" Shape: \", example.shape)\n    output_examples.append(example)","a4b4ffed":"q1example = output_examples[0]\nq1output = pd.DataFrame(columns=q1example.columns)\nq1example['Study'].apply(quickprint)\nq1example.head()","69db4eb2":"drug_tags = [\n'\\w+afil',\n'\\w\\w\\w+ast', #two characters to avoid \"fast\", \"east\"\n'\\w+axine',\n'\\w+barb\\w*',\n'\\w+caine',\n'\\w+ciclib',\n'\\w+cillin',\n'\\w+grel.*',\n'\\w+imsu',\n'\\w+icin', '\\w+ilin', '\\w+ycin', #instead of just '-in', which was too vague\n'\\w+amine', '\\w+adine', '\\w+asine', '\\w+crine', '\\w+quine', #instead of just '-ine', which was too vague\n'\\w+lisib',\n'\\w+lukast',\n'\\w+mab',\n'\\w+olol',\n'\\w+oxacin',\n'\\w+oxetine',\n'\\w+parib',\n'\\w+prazole',\n'\\w+pril',\n'\\w+prost\\w*',\n'\\w+sartan',\n'\\w+tide',\n'\\w+tinib',\n'\\w+vastatin',\n'\\w+vec',\n'\\w+vir',\n'\\w+xaban',\n'\\w+ximab',\n'\\w+zumab',\n'cef\\w+',\n'dexa\\w+',  #added to catch things like dexamethasone\n'\\w+metha\\w+'] #added to catch things like dexamethasone\n\nnot_drugs = ['breast', 'contrast', 'april', 'nucleotide', 'dinucleotide', 'peptide', 'examine']\n\ndef generate_drug_regex():\n    words = []\n    for word in drug_tags:\n        words.append(f'\\\\b{word}\\\\b')\n    return \"|\".join(words)\n\ndrug_regex_text = generate_drug_regex()\ndrug_regex = re.compile(drug_regex_text, flags=re.IGNORECASE)\nprint(drug_regex.pattern)\n        \ndef get_drugs(text):\n    drug_matches = re.findall(drug_regex,text)\n    drug_matches = set(map(str.lower, drug_matches))\n    for word in not_drugs:\n        drug_matches.discard(word)\n    return \", \".join(drug_matches)","d2bf3200":"q1search = r'treatment|drug|medicin|therap|trial|' + drug_regex_text\nq1 = search_title(covmeta, q1search)\nprint('Q1 metadata shape: ', q1.shape)\nq1.sample(5).title.apply(quickprint)\nq1 = q1.join(covtext.set_index('meta_id'), on='cord_uid')\nq1.head()","7bae08cb":"q1.head(5).title.apply(quickprint)\nq1.head(5).url.apply(quickprint)\nprint()","87abcee8":"#example\nq1.head(10).reformatted_body.apply(get_drugs).apply(print)\nprint()","18361075":"samplesize_regexes=[r'\\s\\d+\\s.*sample size',\n                    r'sample size.*\\s\\d+\\b',\n                    r'\\s\\d+\\s.*(patient|case|participant|distribution)',\n                    r'\\s\\d+\\s.*(drug|trial|therap|medic|study)',\n                    r'(patient|case|participant|distribution|drug|trial|therap|medic|study).*\\s\\d+\\b']\n                     \ndef get_samplesize(row):\n    return get_best_match(samplesize_regexes, row.sentences)\n\n#example\nq1.head(10).apply(get_samplesize, axis=1).apply(quickprint)\nprint()","3b4827a2":"#example\nq1.head(10).reformatted_body.apply(get_severity_from_text).apply(print)\nprint()\n","c823715d":"term1 = r\"(conclu|demonstrat|effective|succe|fail|finding|outcome|proof)\"\nterm2 = r\"(treat|therap|drug|medic|approach|trial|strategy)\"\nterm3 = f'({drug_regex_text})'\nsolution_regexes=[regex_shuffle3(term1, term2, term3),\n                  regex_shuffle2(term1, term3),\n                  regex_shuffle2(term1, term2),\n                  term1,\n                  regex_shuffle2(term2, term3)]\n                     \ndef get_solutions(row):\n    return get_best_match(solution_regexes, row.sentences)\n\n#example\nq1.head(10).apply(get_solutions, axis=1).apply(quickprint)\nprint()","ca37a809":"term1 = r\"(\\bprimary\\sendpoint\\b)\"\nterm2 = r\"endpoint\"\nendpoint_regexes=[term1,\n                  term2]\n                     \ndef get_primary_endpoint(row):\n    return get_best_match(endpoint_regexes, row.sentences)\n\n#example\nq1.head(10).apply(get_primary_endpoint, axis=1).apply(quickprint)\nprint()","17de723f":"term1 = r'(fail(s|ed|\\b))|(not.*improve)'\nterm2 = r'(patient|case|trail|result)'\nfailure_regexes = [regex_shuffle2(term1, term2),\n                  term1]\n                     \ndef get_failed_to_improve(row, do_print):\n    sentence = get_best_match(failure_regexes, row.sentences)\n    if sentence != None:\n        if do_print:\n            print(sentence)\n        return True\n    return False\n\nterm1 = r'((?! not.*)improve|(?! (not|fail|poor).*)recover)'\nsuccess_regexes = [regex_shuffle2(term1, term2),\n                   term1]\n\ndef get_did_improve(row, do_print):\n    sentence = get_best_match(success_regexes, row.sentences)\n    if sentence != None:\n        if do_print:\n            print(sentence)\n        return True\n    return False\n\ndef get_improvement(row):\n    if get_failed_to_improve(row, False):\n        return 'N'\n    if get_did_improve(row, False):\n        return 'Y'\n    return ''\n\n#example\nq1.head(10).apply(lambda x: get_failed_to_improve(x, True), axis=1).apply(print)\nq1.head(10).apply(lambda x: get_did_improve(x, True), axis=1).apply(print)\nq1.head(10).apply(get_improvement, axis=1).apply(print)\nprint()","cb55f02a":"for index, row in q1.iterrows():\n    newrow = [index, row.publish_time, row.title, row.url, get_journal(row), get_drugs(row.reformatted_body), get_samplesize(row), get_severity_from_text(row.reformatted_body), get_solutions(row), get_primary_endpoint(row), get_improvement(row), get_study_from_row(row), 'Unknown']\n    q1output.loc[index] = newrow\n    \nq1output.head()","df0bb819":"q1output.to_csv('What is the efficacy of novel therapeutics being tested currently_.csv')","41fb1047":"q2example = output_examples[1]\nq2output = pd.DataFrame(columns=q2example.columns)\nq2example['Study'].apply(quickprint)\nq2example.head()","ebb7116f":"q2search = r'coagul|plasm|thromb|hypox|pulmonar|tachycard|\\bcardi[ao]|\\barter|\\bplatelet|\\bblood|\\bclot' #+ drug_regex_text\nq2 = search_title(covmeta, q2search)\nprint('Q2 metadata shape: ', q2.shape)\nq2.sample(5).title.apply(quickprint)\nq2 = q2.join(covtext.set_index('meta_id'), on='cord_uid')\nq2.head()","497aed98":"for index, row in q2.iterrows():\n    newrow = [index, row.publish_time, row.title, row.url, get_journal(row), get_study_from_row(row), get_drugs(row.reformatted_body), get_samplesize(row), get_severity_from_text(row.reformatted_body), get_solutions(row), get_primary_endpoint(row), get_improvement(row), 'Unknown']\n    q2output.loc[index] = newrow\n    \nq2output.head()","9714ce86":"q2output.to_csv('What is the best method to combat the hypercoagulable state seen in COVID-19_.csv')","69906e20":"# Preparing the data\n\nFirst we load the metadata and clean it up; ignoring anything that isn't covid-related, dropping duplicates, and ignoring anything that is missing actual article text.","13f9f9f0":"**Clinical Improvement**\n\nWhether or not the patients showed improvement.","c67a8e70":"**Therapeutics Used**\n\nHere we'll use the drugs list created before as well as some other terms to search for the specific treatments discussed.","8cf6d343":"Here we add a reformatted version of the body text and an array of sentences to make it easier to search.","94874a34":"# Task-Specific Output: Group 7\n\nHere we set up output tables based on the task samples\n","752cc891":"Here we set up some common helper functions to allow us to search for similar task data.\n","74f377ed":"# What is the efficacy of novel therapeutics being tested currently?\n\nFirst we look at the example data and make an output file based on it.","d4641331":"**Sample Size**\n\nUse a numeric and keyword search to try to guess the sample size used.","ff1bebe1":"**Outcome or conclusion**\n\nSearch the text for an excerpt about the conclusion.","e8eaa9c1":"Finally we use the combined functions above to generate the output file.","1499ec7f":"**Severity of Disease**\n\nHere we'll use keywords to search for the severity","68f2ec04":"**Drugs Used**\n\nHere I'm using the drug nomenclature table from wikipedia: https:\/\/en.wikipedia.org\/wiki\/Drug_nomenclature as a regex search to try to find drug names within the text.","e641cf23":"Finally we will create the second output file based on the results.","7a064ad1":"# What is the best method to combat the hypercoagulable state seen in COVID-19?\n\nNow we'll look at the second question and generate a new output file.","86c6cf7b":"**Primary Endpoint**\n\nFind the primary endpoint or measurement.","d227ad24":"Search for articles that might be related to the task.","588a80b5":"# Covid Round 2 - Group 7\n\nAfter trying various advanced data extraction methods like NLP, I found that I was generally able to get better results using simpler, hand-crafted regular expression searches for each question in the study task. For most searches, I used a series of \"ranked regexes\" to scan through the abstracts or full body texts looking for particular patterns, where the regexes were ordered from most restrictive and most likely relevant to more common but less likely relevant.\n\nThere are three main steps:\n\n1. Import the metadata and search for covid synonyms in the titles and abstracts; drop anything that doesn't appear covid-related.\n2. For each particular task or topic, search for specific keywords in titles and abstracts\n3. Using this minimized dataset, use regular expressions to search the full article text and extract the relevant passages\n\nThis is still a work in progress.","feb48e59":"Next we load the full text linked to the remaining metadata."}}