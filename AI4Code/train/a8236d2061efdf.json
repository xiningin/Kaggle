{"cell_type":{"bb8c031d":"code","80b686c3":"code","7c822eb6":"code","9ad018e5":"code","45865981":"code","868549bc":"code","0737c0eb":"code","7000cf11":"code","f6a5c807":"code","c4ca2362":"code","72466585":"code","211c33a5":"code","2403ffe0":"code","93b77439":"code","0e60ec87":"code","23a7ae9b":"code","1bbbd38f":"code","98d9d4c8":"code","87f52a5d":"code","fdd12d78":"code","8e26d9af":"markdown","68c5e7c9":"markdown","abc2f198":"markdown","614a9a06":"markdown","0328e13e":"markdown","558e3377":"markdown","89a8da93":"markdown","b34f53bc":"markdown","e863f3f5":"markdown"},"source":{"bb8c031d":"!pip install keras==2.2.4\n!pip install tensorflow==1.14.0","80b686c3":"import os\nimport sys\nimport random\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport cv2\n\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom itertools import chain\nimport skimage\nfrom PIL import Image\nfrom skimage.io import imread, imshow, imread_collection, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.util import crop, pad\nfrom skimage.morphology import label\nfrom skimage.color import rgb2gray, gray2rgb, rgb2lab, lab2rgb\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\nfrom keras.models import Model, load_model,Sequential\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Dense, UpSampling2D, RepeatVector, Reshape\nfrom keras.layers.core import Dropout, Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras import backend as K\n\nimport tensorflow as tf\n\nwarnings.filterwarnings('ignore', category=UserWarning, module='skimage')\nseed = 42\nrandom.seed = seed\nnp.random.seed = seed","7c822eb6":"IMG_WIDTH = 256\nIMG_HEIGHT = 256\nIMG_CHANNELS = 3\nINPUT_SHAPE=(IMG_HEIGHT, IMG_WIDTH, 1)\nTRAIN_PATH = '..\/input\/art-images-drawings-painting-sculpture-engraving\/dataset\/dataset_updated\/training_set\/painting\/'\n\ntrain_ids = next(os.walk(TRAIN_PATH))[2]","9ad018e5":"%%time\nX_train = np.zeros((len(train_ids)-86, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\nmissing_count = 0\nprint('Getting train images ... ')\nsys.stdout.flush()\nfor n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):\n    path = TRAIN_PATH + id_+''\n    try:\n        img = imread(path)\n        img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n        X_train[n-missing_count] = img\n    except:\n#         print(\" Problem with: \"+path)\n        missing_count += 1\n\nX_train = X_train.astype('float32') \/ 255.\nprint(\"Total missing: \"+ str(missing_count))","45865981":"imshow(X_train[5])\nplt.show()","868549bc":"x_train, x_test = train_test_split(X_train, test_size=20, random_state=seed)","0737c0eb":"inception = InceptionResNetV2(weights=None, include_top=True)\ninception.load_weights('..\/input\/inception-resnet-v2-weights\/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5')\ninception.graph = tf.get_default_graph()","7000cf11":"def Colorize():\n    embed_input = Input(shape=(1000,))\n    \n    #Encoder\n    encoder_input = Input(shape=(256, 256, 1,))\n    encoder_output = Conv2D(128, (3,3), activation='relu', padding='same',strides=1)(encoder_input)\n    encoder_output = MaxPooling2D((2, 2), padding='same')(encoder_output)\n    encoder_output = Conv2D(128, (4,4), activation='relu', padding='same')(encoder_output)\n    encoder_output = Conv2D(128, (3,3), activation='relu', padding='same',strides=1)(encoder_output)\n    encoder_output = MaxPooling2D((2, 2), padding='same')(encoder_output)\n    encoder_output = Conv2D(256, (4,4), activation='relu', padding='same')(encoder_output)\n    encoder_output = Conv2D(256, (3,3), activation='relu', padding='same',strides=1)(encoder_output)\n    encoder_output = MaxPooling2D((2, 2), padding='same')(encoder_output)\n    encoder_output = Conv2D(256, (4,4), activation='relu', padding='same')(encoder_output)\n    encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n    encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n    \n    #Fusion\n    fusion_output = RepeatVector(32 * 32)(embed_input) \n    fusion_output = Reshape(([32, 32, 1000]))(fusion_output)\n    fusion_output = concatenate([encoder_output, fusion_output], axis=3) \n    fusion_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion_output)\n    \n    #Decoder\n    decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(fusion_output)\n    decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n    decoder_output = UpSampling2D((2, 2))(decoder_output)\n    decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(decoder_output)\n    decoder_output = UpSampling2D((2, 2))(decoder_output)\n    decoder_output = Conv2D(64, (4,4), activation='relu', padding='same')(decoder_output)\n    decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n    decoder_output = Conv2D(32, (2,2), activation='relu', padding='same')(decoder_output)\n    decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n    decoder_output = UpSampling2D((2, 2))(decoder_output)\n    return Model(inputs=[encoder_input, embed_input], outputs=decoder_output)\n\nmodel = Colorize()\nmodel.compile(optimizer='adam', loss='mean_squared_error')\nmodel.summary()","f6a5c807":"%%time\n\n# Image transformer\ndatagen = ImageDataGenerator(\n        shear_range=0.2,\n        zoom_range=0.2,\n        rotation_range=20,\n        horizontal_flip=True)\n\n#Create embedding\ndef create_inception_embedding(grayscaled_rgb):\n    def resize_gray(x):\n        return resize(x, (299, 299, 3), mode='constant')\n    grayscaled_rgb_resized = np.array([resize_gray(x) for x in grayscaled_rgb])\n    grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized)\n    with inception.graph.as_default():\n        embed = inception.predict(grayscaled_rgb_resized)\n    return embed\n\n#Generate training data\ndef image_a_b_gen(dataset=x_train, batch_size = 20):\n    for batch in datagen.flow(dataset, batch_size=batch_size):\n        X_batch = rgb2gray(batch)\n        grayscaled_rgb = gray2rgb(X_batch)\n        lab_batch = rgb2lab(batch)\n        X_batch = lab_batch[:,:,:,0]\n        X_batch = X_batch.reshape(X_batch.shape+(1,))\n        Y_batch = lab_batch[:,:,:,1:] \/ 128\n        yield [X_batch, create_inception_embedding(grayscaled_rgb)], Y_batch\n        ","c4ca2362":"# # Set a learning rate annealer\n# learning_rate_reduction = ReduceLROnPlateau(monitor='loss', \n#                                             patience=3, \n#                                             verbose=1, \n#                                             factor=0.5,\n#                                             min_lr=0.00001)\n# filepath = \"Art_Colorization_Model.h5\"\n# checkpoint = ModelCheckpoint(filepath,\n#                              save_best_only=True,\n#                              monitor='loss',\n#                              mode='min')\n\n# model_callbacks = [learning_rate_reduction,checkpoint]","72466585":"# %%time\n# BATCH_SIZE = 20\n# model.fit_generator(image_a_b_gen(X_train,BATCH_SIZE),\n#             epochs=25,\n#             verbose=1,\n#             steps_per_epoch=X_train.shape[0]\/BATCH_SIZE,\n#              callbacks=model_callbacks\n#                    )","211c33a5":"# model.save(filepath)\n# model.save_weights(\"Art_Colorization_Weights.h5\")\n# inception.save(filepath)\n# inception.save_weights(\"Inception.h5\")","2403ffe0":"# from IPython.display import FileLink,FileLinks\n# import pickle \n\n# FileLinks('.')","93b77439":"model = Colorize()\nmodel.load_weights('\/kaggle\/input\/image-coloring-pretrained-models\/Art_Colorization_Weights.h5')","0e60ec87":"sample = x_test\ncolor_me = gray2rgb(rgb2gray(sample))\ncolor_me_embed = create_inception_embedding(color_me)\ncolor_me = rgb2lab(color_me)[:,:,:,0]\ncolor_me = color_me.reshape(color_me.shape+(1,))\n\noutput = model.predict([color_me, color_me_embed])\noutput = output * 128\n\ndecoded_imgs = np.zeros((len(output),256, 256, 3))\n\nfor i in range(len(output)):\n    cur = np.zeros((256, 256, 3))\n    cur[:,:,0] = color_me[i][:,:,0]\n    cur[:,:,1:] = output[i]\n    decoded_imgs[i] = lab2rgb(cur)\n    cv2.imwrite(\"img_\"+str(i)+\".jpg\", lab2rgb(cur))","23a7ae9b":"plt.figure(figsize=(20, 10))\nfor i in range(10):\n    # grayscale\n    plt.title(\"ORIGINAL\")\n    plt.subplot(3, 10, i + 1)\n    plt.imshow(rgb2gray(x_test)[i].reshape(256, 256))\n    plt.gray()\n    plt.axis('off')\n \n    # recolorization\n    plt.title(\"GRAY SCALE\")\n    plt.subplot(3, 10, i + 1 +10)\n    plt.imshow(decoded_imgs[i].reshape(256, 256,3))\n    plt.axis('off')\n    \n    # original\n    plt.title(\"GENERATED\")\n    plt.subplot(3, 10, i + 1 + 20)\n    plt.imshow(x_test[i].reshape(256, 256,3))\n    plt.axis('off')\n \nplt.tight_layout()\nplt.show()","1bbbd38f":"i=4\nplt.title(\"RESULTS\")\nplt.axis('off')\nplt.figure(figsize=(50,10))\n\nplt.subplot(1, 10,1)\nplt.imshow(rgb2gray(x_test)[i].reshape(256, 256))\nplt.gray()\nplt.axis('off')\n\n# recolorization\nplt.title(\"GRAY SCALE\")\nplt.subplot(1, 10,3)\nplt.imshow(decoded_imgs[i].reshape(256, 256,3))\nplt.axis('off')\n\n# original\nplt.title(\"GENERATED\")\nplt.subplot(1, 10,5)\nplt.imshow(x_test[i].reshape(256, 256,3))\nplt.axis('off')\n\nplt.title(\"ORIGINAL\")\n\nplt.tight_layout()\nplt.show()","98d9d4c8":"X_test = np.zeros((1, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\nimg = imread('\/kaggle\/input\/image-coloring-pretrained-models\/test.jpg')\nimg = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant',preserve_range=True)\nX_test[0]= img\nX_test = X_test.astype('float32')\/255\n","87f52a5d":"sample = X_test\ncolor_me = gray2rgb(rgb2gray(sample))\ncolor_me_embed = create_inception_embedding(color_me)\ncolor_me = rgb2lab(color_me)[:,:,:,0]\ncolor_me = color_me.reshape(color_me.shape+(1,))\n\noutput = model.predict([color_me, color_me_embed])\noutput = output * 128\n\ndecoded_imgs = np.zeros((len(output),256, 256, 3))\n\nfor i in range(len(output)):\n    cur = np.zeros((256, 256, 3))\n    cur[:,:,0] = color_me[i][:,:,0]\n    cur[:,:,1:] = output[i]\n    decoded_imgs[i] = lab2rgb(cur)\n    cv2.imwrite(\"img_\"+str(i)+\".jpg\", lab2rgb(cur))","fdd12d78":"plt.title(\"RESULTS\")\nplt.axis('off')\nplt.figure(figsize=(50,10))\n\nplt.subplot(1, 10,1)\nplt.imshow(X_test[0])\nplt.gray()\nplt.axis('off')\n\n# recolorization\nplt.title(\"INPUT\")\nplt.subplot(1, 10,3)\nplt.imshow(decoded_imgs[0].reshape(256, 256,3))\nplt.axis('off')\n\nplt.title(\"OUTPUT\")\n\n\nplt.tight_layout()\nplt.show()","8e26d9af":"# Train the Model","68c5e7c9":"# Read 2000 Classic Paintings' Dataset\n\n","abc2f198":"# Sample the Results","614a9a06":"# Data Generator Functions","0328e13e":"# Checkpoints","558e3377":"<h1>REQUIREMENTS<\/h1>","89a8da93":"(Note that 86 of the train_ids have errors while being loading into our dataset, so we will just skip over them. We don't really need them.)","b34f53bc":"# Create the Model\n\nThe model is a combination of an autoencoder and resnet classifier. The best an autoencoder by itself is just shade everything in a brownish tone. The model uses an resnet classifier to give the neural network an \"idea\" of what things should be colored.","e863f3f5":"# Train\/Test Split\nJust getting a sample of 20 images to test the model when it is done."}}