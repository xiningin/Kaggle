{"cell_type":{"143994df":"code","f6a4ea92":"code","472fb808":"code","f7125e12":"code","0b060733":"code","43f3ab0f":"code","e7dd1819":"code","90275f6e":"code","a210ace9":"code","4d1b16d8":"code","f77396e7":"code","16436907":"code","edf8a4b4":"code","58fde2dd":"code","0dbe9b02":"code","0953ce02":"code","0ec38c0b":"code","958eca62":"code","3eb4f965":"code","5fb4fa72":"code","9c2d865b":"markdown","d85b72ff":"markdown","e75f2982":"markdown","ed93defb":"markdown","623e2501":"markdown","f89938f2":"markdown","498b22d8":"markdown","d0077ba9":"markdown"},"source":{"143994df":"#import the standard data analytics libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n%matplotlib inline\nsns.set() #set style for any plots\n\n#import the data\ndata = pd.read_csv('\/kaggle\/input\/gas-prices-in-brazil\/2004-2019.tsv',sep='\\t')\ndata.columns # output the colums","f6a4ea92":"#drop irrelevant column\ndata.drop('Unnamed: 0',axis=1,inplace=True)","472fb808":"# For easy interpretability, we translate the columns to english\n\ndata.rename(\ncolumns={\n        \"DATA INICIAL\": \"start_date\",\n        \"DATA FINAL\": \"end_date\",\n        \"REGI\u00c3O\": \"region\",\n        \"ESTADO\": \"state\",\n        \"PRODUTO\": \"product\",\n        \"N\u00daMERO DE POSTOS PESQUISADOS\": \"no_gas_stations\",\n        \"UNIDADE DE MEDIDA\": \"unit\",\n        \"PRE\u00c7O M\u00c9DIO REVENDA\": \"avg_price\",\n        \"DESVIO PADR\u00c3O REVENDA\": \"sd_price\",\n        \"PRE\u00c7O M\u00cdNIMO REVENDA\": \"min_price\",\n        \"PRE\u00c7O M\u00c1XIMO REVENDA\": \"max_price\",\n        \"MARGEM M\u00c9DIA REVENDA\": \"avg_price_margin\",\n        \"ANO\": \"year\",\n        \"M\u00caS\": \"month\",\n        \"COEF DE VARIA\u00c7\u00c3O DISTRIBUI\u00c7\u00c3O\": \"coef_dist\",\n        \"PRE\u00c7O M\u00c1XIMO DISTRIBUI\u00c7\u00c3O\": \"dist_max_price\",\n        \"PRE\u00c7O M\u00cdNIMO DISTRIBUI\u00c7\u00c3O\": \"dist_min_price\",\n        \"DESVIO PADR\u00c3O DISTRIBUI\u00c7\u00c3O\": \"dist_sd_price\",\n        \"PRE\u00c7O M\u00c9DIO DISTRIBUI\u00c7\u00c3O\": \"dist_avg_price\",\n        \"COEF DE VARIA\u00c7\u00c3O REVENDA\": \"coef_price\"\n    },\n    inplace=True\n)","f7125e12":"# view data\ndata.head()","0b060733":"# convert columns that should be numeric to numbers\nnames = ['avg_price_margin','coef_price','dist_avg_price','dist_sd_price','dist_min_price','dist_max_price','coef_dist']\n\nfor col in names:\n    data[col]=pd.to_numeric(data[col],errors='coerce')\n\ndata.dtypes","43f3ab0f":"#determine if the are any missing values\n# find the shape of the data\nshape = data.shape\nmissing = data.isnull().any()\n\nprint(shape,'\\n')\nprint(missing)","e7dd1819":"# Check how many null values there are in each of the columns that came up missing values in the previous cell.\nmargins_missing = data['avg_price_margin'].isnull().sum()\navg_price_missing = data['dist_avg_price'].isnull().sum()\nsd_price_missing = data['dist_sd_price'].isnull().sum()\ndist_min_price_missing = data['dist_min_price'].isnull().sum()\ndist_max_price_missing = data['dist_max_price'].isnull().sum()\ncoef_dist_missing = data['coef_dist'].isnull().sum()\n\n# Print the number of missing values in each column\nprint(margins_missing)\nprint(avg_price_missing)\nprint(sd_price_missing)\nprint(dist_min_price_missing)\nprint(dist_max_price_missing)\nprint(coef_dist_missing)\n\n# Drop every entry with missing values\ndata.dropna(axis=0,inplace=True)","90275f6e":"# Determine the number and names of the products sold in Brazil\nproducts = data['product'].unique()\nprint(len(products))\nprint(products)","a210ace9":"# Find the number and names of the states in Brazil\ngnv_data = data[data['product'] == 'GNV']\nstates = gnv_data['state'].unique()\nstates","4d1b16d8":"# Plot the distribution of the average price in each state\n# Although anova is robust to non normally distributed data, It is good to know if the assumption of normality holds.\nfig,ax = plt.subplots(6,4,figsize=(10,10),constrained_layout=True)\nax = ax.ravel()\n\nfor i in range(len(states)):\n    ax[i].hist(gnv_data[gnv_data.state == states[i]]['avg_price'])\n    ax[i].set_title(states[i])\n    ax[i].set_xlabel('avg price')\n","f77396e7":"# Determine how many observations we have from every state\ngnv_state_count = gnv_data.groupby('state')['avg_price'].count()\ngnv_state_count","16436907":"# separate the data into two sets, gnv_data2 has the states with less than 100 obseravtions removed \nlow_count_states = ['AMAPA','DISTRITO FEDERAL','GOIAS','MARANHAO','PARA','PIAUI','TOCANTINS']\nlow_count_states_df = gnv_data[gnv_data['state'].isin(low_count_states)]\ngnv_data2 = gnv_data.drop(low_count_states_df.index,axis=0)\nstates2 = np.setdiff1d(states,low_count_states) # Create a list of the remaining states \nprint(states2)\nprint(len(states2))","edf8a4b4":"# We determine if the data is normally distributed, by way of qqplot \n# beacuse the histogram did not provide any useful information\nfrom statsmodels.graphics.gofplots import qqplot\n\nfig,ax = plt.subplots(6,3,figsize=(10,10),constrained_layout=True)\nax = ax.ravel()\n\nfor i in range(len(states2)):\n    qqplot(gnv_data[gnv_data.state == states2[i]]['avg_price'],line='s',ax=ax[i])\n    ax[i].set_title(states2[i])\n    ax[i].set_xlabel('avg price')\n\n","58fde2dd":"# Run a shapiro normality test at a 5% significance level\nfrom scipy.stats import shapiro\n\nalpha = 0.05\nreject_count = 0 # count of all the states that dont have normally distributed data\nnormal_count = 0 # count of states with normally distributed data\n\nfor i in range(len(states2)):\n    \n    stat, p = shapiro(gnv_data2[gnv_data.state == states2[i]]['avg_price'])\n    #print(states2[i])\n    #print('Statistics=%.3f, p=%.3f' % (stat, p))\n\n    if p > alpha:\n        normal_count += 1\n    else:\n        reject_count += 1\n\nprint('number of rejects =',reject_count)\nprint('number of normally distributed prices =',normal_count)","0dbe9b02":"# Next we run a normaltest to verify the results of the previous test.\nfrom scipy.stats import normaltest\n\nalpha = 0.05\nreject_count = 0\nnormal_count = 0\n\nfor i in range(len(states2)):\n    \n    stat, p = normaltest(gnv_data2[gnv_data.state == states2[i]]['avg_price'])\n    #print(states2[i])\n    #print('Statistics=%.3f, p=%.3f' % (stat, p))\n\n    if p > alpha:\n        normal_count += 1\n    else:\n        reject_count += 1\n\nprint('number of rejects =',reject_count)\nprint('number of normally distributed prices =',normal_count)","0953ce02":"# Next we run anova using the states as treatments and the avg_price as the response variable\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\nsamples = pd.DataFrame(columns=gnv_data2.columns) # Create a DataFrame to store the samples\n\nfor state in states2:\n    sample = gnv_data2[gnv_data2.state == state].sample(100) # Sample 100 values from each state.\n    samples = pd.concat([samples,sample])\n\n\nmodel = ols('avg_price ~ state', data=samples).fit()\nanova_table = sm.stats.anova_lm(model,typ=3)\n\nprint(model.summary())\nprint()\nprint(anova_table)","0ec38c0b":"# Next we attempt to discern more about the r_squared value for the anova test. \n# 5,000 repitions of 100 bootstrapped samples (10,000 waaay too slow) will be used to find a distribution for the value.\n# The calculations will be hard coded for this part of the analysis.\n\nN = 5000\ns = 100\nP = len(states2)\nn = len(states2) - 1\nR_squared = []\nR_squared_a = []\nfor i in range(N):\n    samples = pd.DataFrame(columns=gnv_data2.columns)\n    for state in states2:\n        sample = gnv_data2[gnv_data2.state == state].sample(s,replace=True)\n        samples = pd.concat([samples,sample])\n    \n    state_means = samples.groupby('state')['avg_price'].mean()\n    overall_mean = samples['avg_price'].mean()\n    \n    SSA = (s*((state_means - overall_mean)**2)).sum() # Sum squared treatments\n    MSA = SSA\/n # Mean square treatments\n    SST = ((samples['avg_price']-overall_mean)**2).sum() # Total Sum squares \n    MST = SST\/(P*s - 1)\n    SSE = SST - SSA # Sum squared residuals\n    MSE = SSE\/(P*(s-1))\n    r_2 = SSA\/SST # R_squared\n    r_2a = 1-MSE\/MST # Adjusted R_squared\n    R_squared.append(r_2)\n    R_squared_a.append(r_2a)\n    \nmean_r2 = np.mean(R_squared)\nmean_r2a = np.mean(R_squared_a)\n\nfig,ax = plt.subplots(1,2,figsize=(15,5))\nax[0].hist(R_squared)\nax[0].set_title('Sampling distribution of R_squared')\nax[0].set_xlabel('R_square value')\nax[0].set_ylabel('frequency')\n\nax[1].hist(R_squared_a)\nax[1].set_title('Sampling distribution of adjusted R_squared')\nax[1].set_xlabel('R_square value')\nax[1].set_ylabel('frequency')\n\nprint('R_squared mean value is %.3f and the adjusted R-squared mean value is %.3f' % (mean_r2,mean_r2a))","958eca62":"# Pair wise comparison of the top five states by avg_price\n\npair_comp = model.t_test_pairwise('state')\npair_comp_df = pair_comp.result_frame\n\n#pair_comp_df\n\nresults = pd.DataFrame([pair_comp_df.loc['RIO GRANDE DO SUL-AMAZONAS'],pair_comp_df.loc['PARAIBA-AMAZONAS'],\n                       pair_comp_df.loc['SERGIPE-AMAZONAS'],pair_comp_df.loc['MATO GROSSO DO SUL-AMAZONAS'],\n                       pair_comp_df.loc['RIO GRANDE DO SUL-MATO GROSSO DO SUL'],pair_comp_df.loc['PARAIBA-MATO GROSSO DO SUL'],\n                       pair_comp_df.loc['SERGIPE-MATO GROSSO DO SUL'],pair_comp_df.loc['SERGIPE-PARAIBA'],\n                       pair_comp_df.loc['RIO GRANDE DO SUL-PARAIBA'],pair_comp_df.loc['SERGIPE-RIO GRANDE DO SUL'] ])\nresults","3eb4f965":"for region in gnv_data2['region'].unique():\n    sample = gnv_data2[gnv_data2.region == region].sample(100) # Sample 100 values from each state.\n    samples = pd.concat([samples,sample])\n\n\nmodel = ols('avg_price ~ region', data=samples).fit()\nanova_table = sm.stats.anova_lm(model,typ=3)\n\nprint(model.summary())\nprint()\nprint(anova_table)","5fb4fa72":"# Next we attempt to discern more about the r_squared value for the anova test. \n# 5,000 repitions of 100 bootstrapped samples (10,000 waaay too slow) will be used to find a distribution for the value.\n# The calculations will be hard coded for this part of the analysis.\n\nN = 5000\ns = 100\nP = len(states2)\nn = len(states2) - 1\nR_squared = []\nR_squared_a = []\nfor i in range(N):\n    samples = pd.DataFrame(columns=gnv_data2.columns)\n    for region in gnv_data2['region'].unique():\n        sample = gnv_data2[gnv_data2.region == region].sample(s,replace=True)\n        samples = pd.concat([samples,sample])\n    \n    region_means = samples.groupby('region')['avg_price'].mean()\n    overall_mean = samples['avg_price'].mean()\n    \n    SSA = (s*((region_means - overall_mean)**2)).sum() # Sum squared treatments\n    MSA = SSA\/n # Mean square treatments\n    SST = ((samples['avg_price']-overall_mean)**2).sum() # Total Sum squares \n    MST = SST\/(P*s - 1)\n    SSE = SST - SSA # Sum squared residuals\n    MSE = SSE\/(P*(s-1))\n    r_2 = SSA\/SST # R_squared\n    r_2a = 1-MSE\/MST # Adjusted R_squared\n    R_squared.append(r_2)\n    R_squared_a.append(r_2a)\n    \nmean_r2 = np.mean(R_squared)\nmean_r2a = np.mean(R_squared_a)\n\nfig,ax = plt.subplots(1,2,figsize=(15,5))\nax[0].hist(R_squared)\nax[0].set_title('Sampling distribution of R_squared')\nax[0].set_xlabel('R_square value')\nax[0].set_ylabel('frequency')\n\nax[1].hist(R_squared_a)\nax[1].set_title('Sampling distribution of adjusted R_squared')\nax[1].set_xlabel('R_square value')\nax[1].set_ylabel('frequency')\n\nprint('R_squared mean value is %.3f and the adjusted R-squared mean value is %.3f' % (mean_r2,mean_r2a))","9c2d865b":"**Next, the region of the retailers is checked to see if maybe it has a better chance explaining the variability in the data.**","d85b72ff":"# To what extent does the difference in the average price of GNV depend on the state and or region that it is sold in?\n\n**In this notebook I endevour to answer the question above, This is done by running an anova test on the data after some cleaning has been done.**","e75f2982":"**As can be seen in the Ouput above the North is by far the most exoensive region when it comes to GNV pricing. The central west on the other hand is the cheapest area. Next, as was done before, we will attempt to find out how variability the region actually accounts for. **","ed93defb":"**The results gotten from the anova test suggest a few things:**\n\n1) The p_value of the test for the variable state is approximately zero (4.137e-110) suggesting that the State in which GNV is sold is significant in the differences in mean seen in avg_price. That is, the means of the states are statistically different.\n\n2) The r_squared value for the model fluctuates due to the sampling however it never goes beyond 0.1, meaning that less than 10% of the variation observed in the data is attributable to the states. This is low, suggesting that the differences in avg_prices in the states is not mainly due to the states in which GNV is sold. More will be done find a suitable r_value using a sampling distribution  \n\n3) The top five states in terms of avg_price in the model are (in descending order), RIO GRANDE DO SUL, AMAZONAS, PARAIBA,SERGIPE, and MATO GROSSO DO SUL.\n\n4) For the fitted model about 10 out of 17 states are not significant (p values greater than 0.05). This suggests that their effect on the average price is negligible. This can also be seem by the coefficient of the effect. The other two states that have a significant effect on the model are PARANA, RIO DE JANEIRO, and Sao Paulo all with a pronouced negative effect.","623e2501":"**The above looks more S-curved than normally distributed. However it is not easy to discern at this stage. Numeric tests may be more informative.**","f89938f2":"**The is no clear distribution in the data. Nonetheless we continue are exploration.**","498b22d8":"**The comparison is made between all the states in the top 5. We see that the only states whose means differed a lot are Rio Grande do Sul and Mato Grosso do sul. 1st place and 5th place respectively. The pairwise comparison shows that atleast for the states in which Gnv is highly priced there is no major statistical difference in the price. It seems like it requires a difference in average price of about 0.2 for there to be a statistically significant difference.**","d0077ba9":"**In conclusion, the results of this notebook analysis suggest that although the state is significant in the difference between the mean avgerage price of GNV it only accounts for about 7% of the variability. Thus Although the prices vary from state to state there is one or more other factors that play a bigger role in the mean differences. The Region on the other hand accounts for only 6% of the variability about the same as the state. we do see though that the Northern states are by far the most expensive and that central western states are the cheapest. However this low value also suggests there are other factors at play.**"}}