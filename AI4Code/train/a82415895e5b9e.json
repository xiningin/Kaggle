{"cell_type":{"21b7ab28":"code","be49cd5f":"code","5ba5f11f":"code","00c097ef":"code","9f18c403":"code","7064e4bf":"code","9b447a9a":"code","1841602a":"code","a9e6e061":"code","ad5568d7":"code","48e3b42c":"code","33f6ff9a":"code","c85ef07e":"code","323810d4":"code","1b6f7c14":"code","79a3c2ad":"code","fd52c1e7":"code","c0129281":"code","4e0c6b74":"code","dc08090f":"code","f6f3ed41":"code","a709a72a":"code","f3dda783":"code","1acdecfd":"code","ad01dd1a":"code","1e2b2f8c":"code","8ab6f65f":"code","ec76c851":"code","cf91d5f7":"code","1988157c":"code","ffffd52e":"code","af047497":"markdown","5e232475":"markdown","53aa98a2":"markdown","e7742b81":"markdown","efb6b639":"markdown","08c56e93":"markdown","ee0d8a4c":"markdown","7526f756":"markdown"},"source":{"21b7ab28":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn import metrics\nfrom xgboost import XGBClassifier\n\nrcParams['figure.figsize'] = 15, 20","be49cd5f":"df = pd.read_csv('..\/input\/tour-travels-customer-churn-prediction\/Customertravel.csv')","5ba5f11f":"df.sample(5)","00c097ef":"df.info()","9f18c403":"df.isnull().sum()","7064e4bf":"print(f\"Min Age : {min(df['Age'])}\")\nprint(f\"Max Age : {max(df['Age'])}\")\nsns.displot(data=df, x='Age', bins=6)","9b447a9a":"catcol = ['FrequentFlyer', 'AnnualIncomeClass', 'ServicesOpted','AccountSyncedToSocialMedia', 'BookedHotelOrNot', 'Target']","1841602a":"for i,column in enumerate(catcol):\n    plt.subplot(len(catcol), 2, i+1)\n    plt.suptitle(\"Plot Value Count\", fontsize=20)\n    sns.countplot(data=df, x=column)\n    plt.title(f\"{column}\")\n    plt.tight_layout()","a9e6e061":"for i,column in enumerate(catcol):\n    plt.subplot(len(catcol), 2, i+1)\n    plt.suptitle(\"Plot Value Proportion\", fontsize=20)\n    plt.pie(x=df[column].value_counts(), labels=df[column].unique(), autopct='%.0f%%')\n    plt.title(f\"{column}\")\n    plt.tight_layout()","ad5568d7":"for i,column in enumerate(catcol[:-1]):\n    plt.subplot(len(catcol), 2, i+1)\n    plt.suptitle(\"Plot Value Count VS Target\", fontsize=20)\n    sns.countplot(data=df, x=column, hue='Target')\n    plt.title(f\"{column}\")\n    plt.tight_layout()","48e3b42c":"for i,column in enumerate(['BookedHotelOrNot','FrequentFlyer','ServicesOpted']):\n    plt.subplot(len(catcol), 2, i+1)\n    plt.suptitle(\"Choice Based On Income Class\", fontsize=20)\n    sns.countplot(data=df, x='AnnualIncomeClass', hue=column)\n    plt.title(f\"{column}\")\n    plt.tight_layout()","33f6ff9a":"for i in range(2):\n    plt.subplot(2, 2, i+1)\n    plt.suptitle(\"Churn On Age Distribution\", fontsize=20)\n    sns.histplot(data=df[df.Target==i], x='Age', bins=6)\n    plt.title(f\"{'Churn' if i==1 else 'Does Not Churn'}\")\n    plt.tight_layout()","c85ef07e":"# Create New Column OHE for column with 3 uniques value\ndf = pd.get_dummies(df, columns=['FrequentFlyer', 'AnnualIncomeClass'], drop_first=True)\n# Drop_first because there are 3 uniques value, whereas 2 of them is enough, so we drop 1","323810d4":"# Transform column with 2 uniques value to OHE with LabelEncoder\nfor feature in ['AccountSyncedToSocialMedia','BookedHotelOrNot']:        \n    df[feature] = LabelEncoder().fit_transform(df[feature])","1b6f7c14":"df.head()","79a3c2ad":"# Split column to Feature(X) and Target(Y)\nX = df.drop(columns='Target')\nY = df['Target']","fd52c1e7":"X","c0129281":"Y","4e0c6b74":"# Split data to train and test\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=48)","dc08090f":"# Because the Target is imbalance, i try to balance the data with SMOTE(OverSampling)\nfrom imblearn.over_sampling import SMOTE\n\nsm = SMOTE(random_state=46)\nx_train_res, y_train_res = sm.fit_resample(X_train, Y_train)","f6f3ed41":"print(f\"Before resample : \\n{Y_train.value_counts()} \\nAfter resample : \\n{y_train_res.value_counts()}\")","a709a72a":"# I will try some classifier algorithm and not tune the parameter, let it default\nalgorithm = [\n    LogisticRegression(),\n    KNeighborsClassifier(),\n    RandomForestClassifier(),\n    XGBClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n]","f3dda783":"# Data without resample\nlog_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\nlog = pd.DataFrame(columns = log_cols)\n\nfor cla in algorithm:\n    cla.fit(X_train, Y_train)\n    name = cla.__class__.__name__\n    print(\"=\" * 30)\n    print(name)\n    print('****Results****')\n    \n    train_predictions = cla.predict(X_test)\n    acc = accuracy_score(Y_test, train_predictions)\n    print(\"Accuracy: {:.4%}\".format(acc))\n    \n    train_predictions = cla.predict(X_test)\n    ll = log_loss(Y_test, train_predictions)\n    print(\"Log Loss: {}\".format(ll))\n    print(\"\\n\")\n    \n    log_entry = pd.DataFrame([[name, acc * 100, ll]], columns = log_cols)\n    log = log.append(log_entry)\n    \nprint(\"=\" * 30)","1acdecfd":"# Data with resample\nlog_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\nlog_res = pd.DataFrame(columns = log_cols)\n\nfor cla in algorithm:\n    cla.fit(x_train_res, y_train_res)\n    name = cla.__class__.__name__\n    print(\"=\" * 30)\n    print(name)\n    print('****Results****')\n    \n    train_predictions = cla.predict(X_test)\n    acc = accuracy_score(Y_test, train_predictions)\n    print(\"Accuracy: {:.4%}\".format(acc))\n    \n    train_predictions = cla.predict(X_test)\n    ll = log_loss(Y_test, train_predictions)\n    print(\"Log Loss: {}\".format(ll))\n    print(\"\\n\")\n    \n    log_entry_res = pd.DataFrame([[name, acc * 100, ll]], columns = log_cols)\n    log_res = log_res.append(log_entry_res)\n    \nprint(\"=\" * 30)","ad01dd1a":"log.compare(log_res,align_axis=1).reset_index(drop=True).set_index(log['Classifier'])","1e2b2f8c":"log.compare(log_res,align_axis=1).reset_index(drop=True).set_index(log['Classifier']).plot()","8ab6f65f":"# Compare with only one model\n# I choose GradientBoostingClassifier because gives the best accuracy results on both data\ngbc = GradientBoostingClassifier()\ngbc_Wres = gbc.fit(X_train, Y_train)\ngbc_res = gbc.fit(x_train_res, y_train_res)\n\ntest_pred_Wres = gbc_Wres.predict(X_test)\ntest_pred_res = gbc_res.predict(X_test)","ec76c851":"CF_Wres = metrics.confusion_matrix(Y_test, test_pred_Wres)\nCF_Wres","cf91d5f7":"CF_res = metrics.confusion_matrix(Y_test, test_pred_res)\nCF_res","1988157c":"CR_Wres = metrics.classification_report(Y_test, test_pred_Wres)\nprint(CR_Wres)","ffffd52e":"CR_res = metrics.classification_report(Y_test, test_pred_res)\nprint(CR_res)","af047497":"Conclussion\n- Customer with High Income do churn more than don't churn\n- despite getting frequent flyers, more customers do churn than those who don't\n- Customer with Middle Income are loyal customers\n- ...","5e232475":"<h1 style='background:#CCE2CB; border:0; color:black'><center> Loading Data <\/center><\/h1> ","53aa98a2":"<h1 style='background:#CCE2CB; border:0; color:black'><center> Feature Engineering <\/center><\/h1> ","e7742b81":"<h1 style='background:#CCE2CB; border:0; color:black'><center> Comparisson <\/center><\/h1> ","efb6b639":"<h1 style='background:#CCE2CB; border:0; color:black'><center> Preprocessing <\/center><\/h1> ","08c56e93":"<h1 style='background:#CCE2CB; border:0; color:black'><center> EDA <\/center><\/h1> ","ee0d8a4c":"<h1 style='background:#CCE2CB; border:0; color:black'><center> Importing Libraries <\/center><\/h1> ","7526f756":"<h1 style='background:#CCE2CB; border:0; color:black'><center> Modelling <\/center><\/h1> "}}