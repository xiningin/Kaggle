{"cell_type":{"9992638c":"code","fbe6331c":"code","35f07228":"code","f99254ed":"code","beafac6b":"code","855f803d":"code","7de79bde":"code","8fc0d01a":"code","e07e432a":"code","4e0b2426":"code","a0adf96e":"code","2ed80d6e":"code","4dc204bd":"code","228b8873":"code","493f243e":"code","07d15960":"code","c8f3dbcf":"code","8da2d232":"code","d34c3279":"code","b02b76d4":"code","6f687527":"code","3391e0bf":"code","466b3f2d":"code","d2945aff":"code","27743e98":"code","4d548b06":"code","9424d869":"code","9433e56b":"code","00751779":"code","4b52820f":"code","2888d2d0":"code","9b4c8fd6":"code","051173eb":"code","e5338984":"code","755d7e6e":"code","754f3544":"code","21797a3a":"code","e52f7d0d":"markdown","124ddc43":"markdown","b5517fd4":"markdown","10f3266a":"markdown","9d5718c2":"markdown","06a68794":"markdown","0695c4a1":"markdown","cf3f3c88":"markdown","40320bb0":"markdown","586dda28":"markdown","5b93d39d":"markdown","fddb8af3":"markdown","2d8559bf":"markdown","28647cb1":"markdown","f40ac45e":"markdown","91d7ff81":"markdown","40549035":"markdown"},"source":{"9992638c":"# My custom plotting style\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import style\nstyle.use('https:\/\/raw.githubusercontent.com\/JoseGuzman\/minibrain\/master\/minibrain\/paper.mplstyle')\n\n\n# Load necessary modules\nimport numpy as np\nimport pandas as pd\n\n# load train and test dataset\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\", index_col='PassengerId')\ntest  = pd.read_csv('..\/input\/titanic\/test.csv',  index_col='PassengerId')","fbe6331c":"# take a brief look to the train dataset\ntrain.head(n=4)","35f07228":"# take a look to the test dataset\ntest.head(n=4)","f99254ed":"mydf = train['Survived'].value_counts() # the data is umbalanced!\nprint(mydf)\nfig, ax = plt.subplots(1,2, figsize =(6,3))\nfig.tight_layout()\n\nmylabels = ['Deceased', 'Survived']\nmycolors = ['orange', 'royalblue']\n\nax[0].bar(x=mylabels, height = mydf.values, color = mycolors, width = 0.75)\nax[0].set_xlim(-1,2)\nfor tick in ax[0].get_xticklabels():\n    tick.set_rotation(90)\nax[0].set_ylabel('counts'), ax[0].set_yticks(np.arange(0,600,250))\n\nax[1].pie(mydf.values, labels = mylabels, colors = mycolors, autopct='%1.1f%%',shadow=True, startangle=90);","beafac6b":"# see average values upon surival\nmydf = train.groupby('Survived').mean()\nmydf","855f803d":"fig, ax = plt.subplots(2, 3, figsize = (12,6))\nfig.tight_layout()\nmylabels = ['Deceased', 'Survived']\nmycolors = ['orange', 'royalblue']\n\nax[0,0].bar(x=mylabels, height = mydf.Pclass, color = mycolors, width = 0.75)\nax[0,0].set_title('Pclass')\n\nax[0,1].bar(x=mylabels, height = mydf.Age, color = mycolors, width = 0.75)\nax[0,1].set_title('Age')\n\nax[0,2].bar(x=mylabels, height = mydf.SibSp, color = mycolors, width = 0.75)\nax[0,2].set_title('SibSp')\n\nax[1,0].bar(x=mylabels, height = mydf.Parch, color = mycolors, width = 0.75)\nax[1,0].set_title('Parch')\n\nax[1,1].bar(x=mylabels, height = mydf.Fare, color = mycolors, width = 0.75)\nax[1,1].set_title('Fare')\n\n\nfor myax in ax.flatten():\n    myax.set_ylabel('Mean value')\n    \nmydf = train.groupby('Sex')['Survived'].mean()\n\nax[1,2].bar(x=['Female', 'Male'], height = [mydf.female, mydf.male], color = ['violet', 'royalblue'], width = 0.75)\nax[1,2].set_title('Gender');\nax[1,2].set_ylabel('Survival (prop.)');","7de79bde":"#train.groupby('Sex')['Survived'].value_counts()\ntrain.groupby('Sex')['Survived'].mean()","8fc0d01a":"from sklearn.pipeline import Pipeline","e07e432a":"class FeaturesTransformer():\n    \"\"\"\n    This transformer counts family members using data in SibSp and Parch, \n    and creates a title based on the string before the dot in the name,\n    and fills the cabin \n    \"\"\"\n    def __init__(self, create_family=True, dissect_cabin = True):\n        \"\"\"\n        create_family (bool)\n            If True, a column with 'FamilySize' is added\n            \n        dissect_cabin (bool)\n            If true, a  'Room', 'nRooms' and 'CabinDeck'\n        \"\"\"\n        self.create_family = create_family\n        self.dissect_cabin = dissect_cabin\n\n    def fit(self, X, y=None, **fit_params):\n        \"\"\"\n        Return the transfomer object, nothing else to do here\n        \"\"\"\n        return self\n    \n    def transform(self, X, **transform_params):\n        \"\"\"\n        It computes family size and returns a new Pandas Object.\n        X is a panda dataframe.\n        \"\"\"\n        mydf = X.copy() # create a copy of the dataframe\n        \n        # 1. Substitute Name by title\n        mydf['Title'] = X.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n        mydf.drop(['Name'], axis = 1, inplace = True)\n        \n        mydf['Title'].replace(['Ms','Lady', 'Countess','Dona'],'Mrs', inplace=True)\n        mydf['Title'].replace(['Mme','Mlle'], 'Miss', inplace = True)\n        mydf['Title'].replace(['Major', 'Sir', 'Jonkheer', 'Dr','Col','Don', 'Capt','Rev'],'Mr', inplace=True)\n\n        # 2. Count the family size\n        if self.create_family:\n            mydf['FamilySize'] = X['SibSp'] + X['Parch'] + 1 # one means traveling alone\n            mydf.drop(['SibSp', 'Parch'], axis=1, inplace=True)\n        \n        # 3. Set CabinDeck, Room, and number of Rooms\n        # https:\/\/www.kaggle.com\/ccastleberry\/titanic-cabin-features\n        if self.dissect_cabin:\n            mydf['Cabin'].fillna('N', inplace=True) # No cabin labels as 'N'            \n            mydf['Room'] = X['Cabin'].str.extract(\"([0-9]+)\", expand=False).astype(\"float\")\n            mydf['nRooms'] = mydf.Cabin.apply( lambda x: len(str(x).split()))\n            mydf.loc[mydf.Cabin == 'N', 'nRooms'] = 0\n            mydf['CabinDeck'] = mydf.Cabin.apply( lambda x: str(x[0]))\n            mydf['CabinDeck'].replace(['T'], 'A', inplace=True) # T is A\n            del mydf['Cabin']\n\n        return mydf","4e0b2426":"# create a pipeline with a single transformer to have fit_transform\npreprocess = Pipeline([\n    ('Preprocessing', FeaturesTransformer(create_family=True))\n])\n\nmytrain = preprocess.fit_transform(train)","a0adf96e":"# Survival is better in larger families\nmytrain.groupby('Survived')['FamilySize'].median()","2ed80d6e":"# Survival rate is higher in Miss and Mrs and Master\nmytrain.groupby('Survived')['Title'].value_counts()","4dc204bd":"class FillerTransformer():\n    \"\"\"\n    This transformer to fill age according to the title in the ticket.\n    \"\"\"\n    def __init__(self, mean=True):\n        \"\"\"\n        \"\"\"\n        self.mean = mean\n       \n    def fit(self, X, y=None, **fit_params):\n        \"\"\"\n        Return the transfomer object, nothing else to do here\n        \"\"\"\n        return self\n    \n    def transform(self, X, **transform_params):\n        \"\"\"\n        It computes family size and returns a new Pandas Object.\n        X is a panda dataframe.\n        \"\"\"\n        mydf = X.copy() # create a copy of the dataframe\n        \n        # 1. Two missing embarqued. We choose  Southampton (C) \n        # because 80 $ is closer to average ticket prize \n        # https:\/\/www.kaggle.com\/masumrumi\/a-statistical-analysis-ml-workflow-of-titanic\n        mydf['Embarked'] = mydf['Embarked'].fillna('C')\n        \n        # 2. One Fare missing in training set\n        # complete missing fare with median\n        mydf['Fare'].fillna(mydf['Fare'].median(), inplace = True)\n                      \n        # 3. Replace Age with data from passenger with the\n        # same class\/sex\/family\n        if self.mean:\n            mydf['Age'] = mydf.groupby(['Pclass','Title'])['Age'].apply(lambda x: x.fillna(x.mean()))    \n        else:\n            mydf['Age'] = mydf.groupby(['Pclass','Title'])['Age'].apply(lambda x: x.fillna(x.median()))    \n\n        return mydf","228b8873":"# create a pipeline with a single transformer to have fit_transform\npreprocess = Pipeline([\n    ('Preprocessing', FeaturesTransformer(create_family=True)),\n    ('FillingValues', FillerTransformer(mean=True))\n])\n\nmytrain = preprocess.fit_transform(train)\n\nmytrain.Age.isnull().sum()","493f243e":"class ColumnsDelete():\n    \"\"\"\n    Deletes a column of a dataframe. \n    \"\"\"\n    def __init__(self, columns=None):\n        self.columns = columns\n       \n    def fit(self, X, y=None, **fit_params):\n        \"\"\"\n        Return the transfomer object, nothing else to do here\n        \"\"\"\n        return self\n    \n    def transform(self, X, **transform_params):\n        \"\"\"\n        It deletes the columns from the dataframe.\n        \"\"\"\n        mydf = X.copy() # create a copy of the dataframe\n        \n        if self.columns:\n            mydf.drop(self.columns, axis=1, inplace=True)\n        \n        return mydf","07d15960":"# create a pipeline with a single transformer to have fit_transform\npreprocess = Pipeline([\n    ('Preprocessing', FeaturesTransformer(create_family=True)),\n    ('FillingValues', FillerTransformer(mean=True)),\n    ('DeleteColumns', ColumnsDelete(['Room', 'Ticket']))\n])\n\nmytrain = preprocess.fit_transform(train)\nmytest  = preprocess.fit_transform(test)\nmytrain.info(), mytest.info()","c8f3dbcf":"class CategoryEncoder():\n    \"\"\"\n    This transformer encodes categorial variables as numeric.\n    \"\"\"\n    def __init__(self, columns=None):\n        self.columns = columns\n       \n    def fit(self, X, y=None, **fit_params):\n        \"\"\"\n        Return the transfomer object, nothing else to do here\n        \"\"\"\n        return self\n    \n    def transform(self, X, **transform_params):\n        \"\"\"\n        It encodes the Age as 0: female, and 1:male in the dataframe.\n        \"\"\"\n        mydf = X.copy() # create a copy of the dataframe\n        \n        # 1. Encode sex as binary\n        mydf['Sex'] = mydf['Sex'].map( {'male': 1, 'female': 0})\n        \n        # 2. Encode categorial variables\n        if self.columns:\n            \n            #pd.get_dummies(mytrain, columns=['Embarked', 'CabinDeck'], prefix=['Embarked', 'CabinDeck'] )\n            mydf = pd.get_dummies(mydf, columns=self.columns, prefix=self.columns )\n            \n        return mydf","8da2d232":"preprocess = Pipeline([\n    ('Preprocessing', FeaturesTransformer(create_family=True)   ),\n    ('FillingValues', FillerTransformer(mean=True)              ),\n    ('DeleteColumns', ColumnsDelete( ['Room', 'Ticket'] )       ),\n    ('CategoEncoder', CategoryEncoder(['Embarked', 'CabinDeck', 'Title'])),\n])\n\nmytrain = preprocess.fit_transform(train)\nmytrain","d34c3279":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass myZScaler(BaseEstimator, TransformerMixin):\n    \"\"\"\n    This transformer select whic column names to scale \n    and gets the result in transform as a pandas DataFrame object. \n    Adapted from https:\/\/stackoverflow.com\/questions\/37685412\n    \"\"\"\n    def __init__(self, columns = all, **init_params):\n        \n        self.columns = columns\n        self.scaler = StandardScaler(**init_params)\n       \n    def fit(self, X, y=None):\n        \"\"\"\n        Return a matrix with the z-scored values\n        \"\"\"\n        self.scaler.fit( X[self.columns], y )\n        return self\n    \n    def transform(self, X: pd):\n        \"\"\"\n        Returns a new DataFrame object with the scaled columns.\n        \"\"\"\n        if self.columns is all:\n            self.columns = X.columns\n        init_col_order = X.columns\n        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns, index = X.index)\n        X_not_scaled = X.loc[:,~X.columns.isin(self.columns)]\n        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]\n    \nmyscaler = myZScaler(['Age', 'Fare'], copy=True)\nmyfoo = myscaler.fit_transform(mytrain)\nmyfoo.head()","b02b76d4":"# Complete Pipeline\npreprocess = Pipeline([\n    ('Preprocessing', FeaturesTransformer(create_family=True)             ),\n    ('FillingValues', FillerTransformer(mean=False)                        ),\n    ('DeleteColumns', ColumnsDelete( ['Room', 'Ticket'] )                 ),\n    ('CategoEncoder', CategoryEncoder(['Embarked', 'CabinDeck', 'Title']) ),\n    ('z-score'      , myZScaler( ['Age', 'Fare', 'FamilySize', 'Pclass'], copy=True ))\n])\n\ntrain.loc[797,'Name'] = \"Leader, Ms. Alice (Farnham)\"\nmytrain = preprocess.fit_transform(train)","6f687527":"mytrain = preprocess.fit_transform(train)\nmytest =  preprocess.fit_transform(test)","3391e0bf":"mytrain.head()","466b3f2d":"target = mytrain['Survived']\nmytrain.drop(['Survived'], axis=1, inplace=True)","d2945aff":"from sklearn.ensemble import RandomForestClassifier\n\n# Create the model with 100 trees\n#RFCmodel = RandomForestClassifier(n_estimators=1000,max_features='sqrt')\nRFCmodel = RandomForestClassifier(random_state=42)\n# Fit on training data\nRFCmodel.fit(mytrain, target)\n\nprint(f'Accuracy: {RFCmodel.score(mytrain,target)*100:.2f}%')","27743e98":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\n\nmyprediction = RFCmodel.predict(mytrain)\nprint(f'Accuracy  : {accuracy_score(myprediction,target)*100:.2f}%')\nprint(f'Precission: {precision_score(myprediction, target)*100:.2f}%')\nprint(f'Recall    : {recall_score(myprediction, target)*100:.2f}%')\nprint(f'F1-score  : {f1_score(myprediction, target)*100:.2f}%')","4d548b06":"\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import cross_val_score\n\ndef evaluate_model(model):\n    \"\"\"\n    Evaluates the precision of the specified model using 5-fold cross-validation.\n    \"\"\"\n\n    accuracy = cross_val_score(model, mytrain, target, cv=5, scoring='accuracy').mean()\n\n    return accuracy\n\nmodels = {\n    'Logistic regression':LogisticRegression(random_state = 42),\n    'Decision tree':DecisionTreeClassifier(random_state = 42),\n    'Random forest':RandomForestClassifier(random_state = 42)\n}\n\nfor key, mymodel in models.items():    \n    accuracy = evaluate_model(model = mymodel)\n    print('{:20}: {:2.2f}%'.format(key, accuracy*100))","9424d869":"# Grid Search\nfrom sklearn.model_selection import GridSearchCV","9433e56b":"#{'model': LogisticRegression(random_state = 42), 'params' :{'solver':['newton-cg', 'lbfgs', 'liblinear']} }\nmodels = [\n    {\n     'name':'Logistic regression',\n     'estimator':LogisticRegression(random_state = 42),\n     'hyperparameters':{\n         'solver': ['newton-cg', 'lbfgs', 'liblinear']\n         }\n    },\n    {\n     'name':'Decision tree',\n     'estimator':DecisionTreeClassifier(random_state = 42),\n     'hyperparameters':{\n         'criterion':['entropy','gini'],\n         'splitter':['best','random'],\n         'max_depth':range(4,9),\n         'max_features':['auto','sqrt','log2'],\n         'min_samples_split':[3,4,5]\n         }\n    },    {\n     'name':'RandomForestClassifier',\n     'estimator':RandomForestClassifier(random_state = 42),\n     'hyperparameters':{\n         'n_estimators' : [10, 15, 18, 19, 20, 21, 22],\n         'criterion':['entropy','gini'],\n         'max_depth':range(4,9),\n         'max_features':['auto','sqrt','log2'],\n         'min_samples_split':[3,4,5]\n         }\n    }\n]\n\n\ndef tune_models(models):\n\n    \n    for model in models:\n\n        print(model['name'])\n        print('-' * len(model['name']))\n\n        grid = GridSearchCV(model['estimator'], param_grid=model['hyperparameters'], cv=5, scoring='accuracy')\n        grid.fit(mytrain, target)\n\n        model['best_score'] = grid.best_score_\n        model['best_params'] = grid.best_params_\n        model['best_model'] = grid.best_estimator_\n\n        print('Best accuracy: {:.2f}%'.format(model['best_score']*100))\n        print('Best parameters: {}\\n'.format(model['best_params']))\n\n\n    return models\n\nmodels = tune_models(models)","00751779":"# The RandomForestClassifier regression works better","4b52820f":"#mymodel = DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_split = 4, splitter = 'best', random_state=42)","2888d2d0":"\nmymodel = RandomForestClassifier(criterion='entropy', max_depth=7, max_features = 'auto', min_samples_split = 4, n_estimators = 18, random_state=42)","9b4c8fd6":"# without train\/test split\nmymodel.fit(mytrain, target)\nmyprediction = mymodel.predict(mytrain)\nprint(f'Precision: {precision_score(myprediction, target)*100:.2f}%')\nprint(f'Accuracy: {accuracy_score(myprediction, target)*100:.2f}%')\n#print(f'Score: {mymodel.score(mytrain,target)*100:.2f}%')","051173eb":"myprediction = mymodel.predict(mytest)\nsubmission =  pd.DataFrame({'PassengerId':mytest.index,'Survived':myprediction})\nsubmission.to_csv('submissionRFC_nosplit.csv',index=False)","e5338984":"from sklearn.model_selection import train_test_split","755d7e6e":"train_x, test_x, train_y, test_y = train_test_split(mytrain, target, test_size = 0.2, random_state=42)\nmymodel = RandomForestClassifier(criterion='entropy', max_depth=7, max_features = 'auto', min_samples_split = 4, n_estimators = 18, random_state=42)\n# Fit on training data\nmymodel.fit(train_x, train_y)\nmymodel.score(test_x, test_y)\n#myprediction = mymodel.predict(mytest)\n#print(f'Precission: {precision_score(myprediction, mytest)*100:.2f}%')\n#print(f'Accuracy: {accuracy_score(myprediction, mytest)*100:.2f}%')\n#print(f'Score: {mymodel.score(test_x,test_y)*100:.2f}%')","754f3544":"myprediction = mymodel.predict(mytest)","21797a3a":"submission =  pd.DataFrame({'PassengerId':mytest.index,'Survived':myprediction})\nsubmission.to_csv('submissionRF_optim_param.csv',index=False)","e52f7d0d":"<a id=\"section2.1\"><\/a>\n## 2.1 To create FamilySize and number of rooms\n\nWe will have a transformer that uses SibSp and Parch to create a title (Mr, Mrs or Miss) to identify the pasenger later. We will create a FamilySize and number of rooms.","124ddc43":"<a id=\"section2.2\"><\/a>\n## 2.2 To fill Ages according to sex","b5517fd4":"<a id=\"section2\"><\/a>\n## 2. Transfomers and Pipelines\n\nBecause cleaning and preprocessing of the data will be done in both the training and the test dataset, we will create custom transformers that we collect into Pipelines. \n\n\nTo learn more about Pipelies look [here](https:\/\/towardsdatascience.com\/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65) for implementation details.","10f3266a":"<a id=\"section2.3\"><\/a>\n## 2.3 A simple column deleter","9d5718c2":"Withouth parameter optimization, the simple logistic regression model would perform better.","06a68794":"<a id=\"section2.4\"><\/a>\n## 2.4 Transformer for a sex and category encoder","0695c4a1":"<a id=\"section3.1\"><\/a>\n## 3.1 Random Forest Classifier","cf3f3c88":"Being a male implies having a lower survival probability","40320bb0":"When grouping the features upon survival, we can select which variables are most relevant.","586dda28":"One last transformer to perform z-scoring on defined columns","5b93d39d":"<a id=\"section3.3\"><\/a>\n## 3.3. Submit prediction","fddb8af3":"# Table of Contents\n\n1. [Introduction](#section1)\n      * 1.1 [Exploratory Data Analysis (EDA)](#section1.1)\n2. [Transformers and Pipelines](#section2)\n      * 2.1 [To create FamilySize and number of rooms](#section2.1)\n      * 2.2 [To fill Ages according to sex](#section2.1)\n      * 2.3 [A simple column deleter](#section2.3)\n      * 2.4 [Transformer for a sex and category encoder](#section2.4)\n3. [Evaluate models](#section4)\n      * 3.1 [Random Forrest classifier](#section3.1)\n      * 3.2 [Grid Search](#section3.1)\n      * 3.3 [Submit prediction](#section3.3)\n","2d8559bf":"Score of 98% is too accurate, and we may run into overfitting when submitting the results. Lets try different models with 5-k-fold validation.","28647cb1":"<a id=\"section1.1\"><\/a>\n## 1.1 Exploratory Data Analysis (EDA)","f40ac45e":"<a id=\"section3.2\"><\/a>\n## 3.2 Grid search","91d7ff81":"<a id=\"section3\"><\/a>\n# 3. Evaluate models","40549035":"<a id=\"section1\"><\/a>\n# 1. Introduction\n\nA key aspect of the success in the [Titanic competition](https:\/\/www.kaggle.com\/c\/titanic) is to preprocess the dataset after a exploration of the data (Exploratory Data Analysis, EDA). With some feature engineering we can a transformed dataset with better application of machine learning models. \n\nWe can use [Pipelines](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html) to automatize transformations of datasets, which permits identical transformations in the both the training and the test sets."}}