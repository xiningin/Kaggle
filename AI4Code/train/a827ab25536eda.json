{"cell_type":{"057401a2":"code","48f92ff3":"code","51df6290":"code","bc18a6e1":"code","ad8c029c":"code","6441457f":"code","d02f5f00":"code","c6449215":"code","8d52a992":"code","6d5c5b00":"code","6273ec43":"code","0a336809":"code","990443c3":"code","946a4ba3":"code","ded2a092":"code","3819498c":"code","789a3f6b":"code","81d407d6":"code","8bc8a37b":"code","dbd891d2":"code","c1feeccb":"code","a247e1f6":"code","2db6fe14":"code","6aa0ae7c":"code","10af0933":"code","5504abb0":"code","893b9cb3":"code","9ecc1548":"code","74cbab40":"code","6e6e867e":"code","2b91f408":"code","c3e1dc6b":"code","a1a3383e":"code","cc38150a":"code","7a33a2d3":"code","df986adf":"code","1da47b71":"code","b02d23c8":"code","f0587afe":"code","64aab2bc":"code","80c74bc2":"code","44061e79":"code","8822f6e9":"code","ff6917cd":"code","cb0b2ad4":"code","0286611c":"markdown","39376e0c":"markdown","4fb180af":"markdown","4a41f7d0":"markdown","da20a71a":"markdown","77e77188":"markdown","e7a77d5b":"markdown","39383aa5":"markdown","6a19fff9":"markdown","c5f261ec":"markdown","4a9ee003":"markdown","dae95ff4":"markdown","b48d83b2":"markdown","8976675d":"markdown","bf1f7c17":"markdown","43d4df90":"markdown","bc107da0":"markdown","c8ea65de":"markdown","94590227":"markdown","2c55d500":"markdown","e0d6ef49":"markdown","77a6eba2":"markdown","3f24fd38":"markdown","6f4fbff5":"markdown","ffb595bd":"markdown","1da7a435":"markdown","d531ead5":"markdown","8fe9102c":"markdown","c0328356":"markdown","db92af21":"markdown","70736283":"markdown","dca7edd9":"markdown","4cdf614c":"markdown","f647e9e1":"markdown","16fc9d3f":"markdown","8b71662b":"markdown","8ca2b390":"markdown","2a0844a8":"markdown","9f72918f":"markdown","f7f786b1":"markdown","e08db35e":"markdown","b5b2faa9":"markdown","ad89b984":"markdown","ea32a9d0":"markdown","8f2318c5":"markdown"},"source":{"057401a2":"!pip install pyreadr\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pyreadr\nimport seaborn as sns\nimport scipy.stats as stats\nimport torch\nimport lightgbm as lgb\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_squared_error\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nimport tensorflow as tf\n\nsns.set()","48f92ff3":"train_normal_path = '\/kaggle\/input\/tennessee-eastman-process-simulation-dataset\/TEP_FaultFree_Training.RData'\ntrain_fault_path = '\/kaggle\/input\/tennessee-eastman-process-simulation-dataset\/TEP_Faulty_Training.RData'\n\ntrain_df = pyreadr.read_r(train_normal_path)['fault_free_training']\ntest_df = pyreadr.read_r(train_fault_path)['faulty_training']","51df6290":"train_df.head()","bc18a6e1":"train_df.faultNumber.value_counts()","ad8c029c":"test_df.head()","6441457f":"test_df.faultNumber.value_counts()","d02f5f00":"def qqplot_by_fault(data_df, cols, fault_number):\n    plt.figure(figsize=(14,14))\n    \n    for i in range(len(cols)):\n        ax = plt.subplot(4, 4, i+1)\n        data = data_df[(data_df.faultNumber==fault_number) & (data_df.simulationRun.isin(range(10)))][cols[i]]\n        \n        #data = np.log(data)\n        \n        stats.probplot(x=data, plot=plt)\n        ax.set_title(cols[i])\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n        shapiro = stats.shapiro(data)\n        ax.text(0.99, 0.01, '{0:.4f}'.format(shapiro[1]),\n            verticalalignment='bottom', horizontalalignment='right',\n            transform=ax.transAxes,\n            color='green', fontsize=15)\n        \n    plt.show()","c6449215":"qqplot_by_fault(train_df, train_df.columns[3:19], fault_number=0)","8d52a992":"qqplot_by_fault(train_df, train_df.columns[19:35], fault_number=0)","6d5c5b00":"qqplot_by_fault(train_df, train_df.columns[35:51], fault_number=0)","6273ec43":"qqplot_by_fault(train_df, train_df.columns[51:], fault_number=0)","0a336809":"qqplot_by_fault(test_df, test_df.columns[3:19], fault_number=1)","990443c3":"qqplot_by_fault(test_df, test_df.columns[3:19], fault_number=6)","946a4ba3":"qqplot_by_fault(test_df, test_df.columns[3:19], fault_number=17)","ded2a092":"alpha = 0.05\n\ndef get_shapiro(data_df, cols, fault_number):\n    shapiro_history = []\n    for i in range(len(cols)):\n        data = data_df[(data_df.faultNumber==fault_number) & (data_df.simulationRun.isin(range(9)))][cols[i]]\n        #data = np.log(data)\n        shapiro = stats.shapiro(data)\n        shapiro_history.append(shapiro)\n    return shapiro_history\n\ndef plot_shapiro(shapiro_history):\n    W,p = zip(*shapiro_history)\n    plt.plot(p)\n    plt.axhline(y=alpha, color='r', linestyle='-')\n    plt.title('p_value >= {0} for {1} features'.format(alpha, len([i for i in p if i >= alpha])))\n    plt.xlabel('feature')\n    plt.ylabel('p_value')\n    plt.show()","3819498c":"faultless_shapiro = get_shapiro(train_df, train_df.columns[3:55], fault_number=0)\nplot_shapiro(faultless_shapiro)\n\nw,p_values = zip(*faultless_shapiro)\nfaultless_cond = pd.DataFrame(index=train_df.columns[3:55],columns=['isNormal']).fillna(False)\ni=0\nfor p in p_values:\n    if p >= 0.05:\n        faultless_cond.iloc[i] = True\n    i+=1\n","789a3f6b":"all_shap = []\nfor fault in range(1,21):\n    shap = get_shapiro(test_df, test_df.columns[3:55], fault_number=fault)\n    all_shap.append(shap)","81d407d6":"fault_numbers = []\nskewed_features = pd.DataFrame(index=train_df.columns[3:55],columns=['frequency']).fillna(0)\n\nfor i in range(20):\n    w,p_values = zip(*all_shap[i])\n    p_count = len([a for a in p_values if a >= 0.05])\n    #print(\"%d -> %d\" % (i + 1, p_count))\n    if (p_count < 20):\n        fault_numbers.append(i+1)\n        \n    feature_number = 0\n    for p in p_values:\n        if p < 0.05:\n            skewed_features.iloc[feature_number] += 1\n        feature_number+=1\n\n#print(\"------------\")\n#print(len(fault_numbers))","8bc8a37b":"faulty_shapiro = get_shapiro(test_df, train_df.columns[3:55], fault_number=2)\nplot_shapiro(faulty_shapiro)","dbd891d2":"faulty_shapiro = get_shapiro(test_df, train_df.columns[3:55], fault_number=6)\nplot_shapiro(faulty_shapiro)","c1feeccb":"faulty_shapiro = get_shapiro(test_df, train_df.columns[3:55], fault_number=20)\nplot_shapiro(faulty_shapiro)","a247e1f6":"plt.figure(figsize=(12,6))\n\nskewed_features[faultless_cond.isNormal.values].frequency.sort_values(ascending=False).plot(kind='bar');","2db6fe14":"n_prob_features=10\n\nfeature_cols = skewed_features[faultless_cond.isNormal].frequency.sort_values(ascending=False).head(n_prob_features).index.values\nprint(feature_cols)","6aa0ae7c":"scaler = preprocessing.MinMaxScaler()\ndata = pd.DataFrame(scaler.fit_transform(X = train_df[(train_df.faultNumber == 0) & (train_df.simulationRun.isin(range(400)))].loc[:,feature_cols]))\n\nmeans = data.mean()\nvariances = data.std()\n\nmodel_prob = pd.DataFrame(index = feature_cols, data = {'mean': means.values, 'variance': variances.values})\nmodel_prob.head(n_prob_features)","10af0933":"def gaussian(x, mu, sig):\n    return 1.\/(np.sqrt(2.*np.pi)*sig)*np.exp(-np.power((x - mu)\/sig, 2.)\/2)\n\ndef get_probability(x, model):\n    res = gaussian(x.values, model['mean'].values, model['variance'].values)\n    return res.prod()\n\ndef alert_condition(x):\n    return np.max(x)\n\ndef metric_prob(probs, window_minutes, eps):    \n    #samples are sampled each 3 minutes\n    window_points = window_minutes \/\/ 3\n    rolled = probs.rolling(window=window_points).apply(alert_condition, raw=True)    \n    ind = np.where(rolled < eps)\n    \n    if len(ind[0])>0:\n        return ind[0][0]\n\n    return 0","5504abb0":"for fault in range(21):\n    plt.figure(figsize=(12,4))\n    if fault==0:\n        scaled = pd.DataFrame(scaler.transform(train_df[(train_df.faultNumber == fault) & (train_df.simulationRun == 401)].loc[:, feature_cols]))\n        plt.title('no faults')\n    else:\n        scaled = pd.DataFrame(scaler.transform(test_df[(test_df.faultNumber == fault) & (test_df.simulationRun == np.random.randint(500))].loc[:, feature_cols]))\n        plt.title('fault type {0}'.format(fault))\n    probs = scaled.apply(lambda x: get_probability(x, model_prob), axis=1)\n    \n    plt.plot(np.linspace(0,300,300), probs[:300])    \n    plt.axvline(x=20, color='r', linestyle='-')\n\n    #alert_point = metric_prob(probs, 30, 1e+05)\n    #window_points = 10\n    #plt.axvline(x=alert_point, color='y', linestyle='--')\n    #plt.axvline(x=alert_point-window_points, color='y', linestyle='--')\n\n    plt.show()","893b9cb3":"scaler = preprocessing.MinMaxScaler()\n\nfeatures_df = train_df[train_df.faultNumber==0].iloc[:,3:]\nfeatures_df = pd.DataFrame(scaler.fit_transform(X = features_df), columns = features_df.columns)","9ecc1548":"def train(df, cols_to_predict):\n    models = {}\n    for col in cols_to_predict:\n        print('training model for', col)\n        model = lgb.LGBMRegressor(learning_rate=0.1)\n        tr_x = features_df.drop([col],axis=1)\n        target = features_df[col]\n        \n        model.fit(X=tr_x, y=target)\n        models[col] = model\n    \n    return models\n\ndef predict(models, df, cols_to_predict):\n    preds = []\n    for col in cols_to_predict:\n        test_x = df.drop([col],axis=1)\n        test_y = df[col]\n        pred = models[col].predict(test_x)\n        preds.append(pred)\n        \n        #err.append(np.square((test_y - pred)**2).values)\n        \n        #plt.figure(figsize=(15,10))\n        #x = np.linspace(0,len(test_y),len(test_y))\n        #plt.plot(x, test_y, label='Actual value')\n        #plt.plot(x, pred, label='Prediction')\n        #plt.legend(loc='best')\n        #plt.title(col)\n        #plt.plot()\n        #plt.show()\n        \n        #qqplot_data(pred)\n    \n    return preds","74cbab40":"features_to_predict = train_df.columns[3:]\nmodels = train(train_df[(train_df.faultNumber==0) & (train_df.simulationRun.isin(range(400)))], features_to_predict)","6e6e867e":"def get_mse(sample, preds):\n    return np.square((sample.loc[:,features_to_predict] - np.transpose(preds))**2).mean(axis=1)\n\nplt.figure(figsize=(14,6))\n\nnormal_sample = pd.DataFrame(scaler.transform(train_df[(train_df.simulationRun==np.random.randint(500)) & (train_df.faultNumber==0)].iloc[:,3:]), columns = features_df.columns)\nnormal_preds = predict(models, normal_sample, features_to_predict)\n\nfaulty_sample = pd.DataFrame(scaler.transform(test_df[(test_df.simulationRun==np.random.randint(500)) & (test_df.faultNumber==1)].iloc[:,3:]), columns = features_df.columns)\nfaulty_preds = predict(models, faulty_sample, features_to_predict)\n\nplt.axvline(x=20,color='r',linestyle='--')\n\nplt.title('MSE for a normal and fault samples')\nplt.plot(get_mse(normal_sample, normal_preds));\nplt.plot(get_mse(faulty_sample, faulty_preds));","2b91f408":"plt.figure(figsize=(14,6))\nplt.yscale('log')\n\nplt.axvline(x=20,color='r',linestyle='--')\nplt.axhline(y=1e-2,color='g',linestyle='--')\n\nplt.title('MSE for a normal and fault samples')\nplt.plot(get_mse(normal_sample, normal_preds));\nplt.plot(get_mse(faulty_sample, faulty_preds));","c3e1dc6b":"plt.figure(figsize=(16,10))\nplt.yscale('log')\n\nfor i in [0,1,2,4,6,7,8]:\n    if i == 0:\n        tree_sample = train_df[(train_df.simulationRun==500) & (train_df.faultNumber==i)].iloc[:,3:]\n    else:\n        tree_sample = test_df[(test_df.simulationRun==np.random.randint(500)) & (test_df.faultNumber==i)].iloc[:,3:]\n    tree_sample = pd.DataFrame(scaler.transform(tree_sample), columns = features_df.columns)\n    tree_preds = predict(models, tree_sample, features_to_predict)\n    plt.axvline(x=20,color='r',linestyle='--')\n    if i==0:\n        label='normal conditions'\n    else:\n        label='fault type %d' % i\n    plt.plot(get_mse(tree_sample,tree_preds),label=label)\n    plt.title('MSE for normal conditions and faults')\n    plt.legend()","a1a3383e":"plt.figure(figsize=(16,10))\nplt.yscale('log')\n\n#11,12,13\nfor i in [0,5,14,17,18]:\n    if i == 0:\n        tree_sample = train_df[(train_df.simulationRun==500) & (train_df.faultNumber==i)].iloc[:,3:]\n    else:\n        tree_sample = test_df[(test_df.simulationRun==np.random.randint(500)) & (test_df.faultNumber==i)].iloc[:,3:]\n    tree_sample = pd.DataFrame(scaler.transform(tree_sample), columns = features_df.columns)\n    tree_preds = predict(models, tree_sample, features_to_predict)\n    plt.axvline(x=20,color='r',linestyle='--')\n    if i==0:\n        label='normal conditions'\n    else:\n        label='fault type %d' % i\n    plt.plot(get_mse(tree_sample,tree_preds),label=label)\n    plt.title('MSE for normal conditions and faults')\n    plt.legend()","cc38150a":"plt.figure(figsize=(16,10))\n#plt.yscale('log')\n\nfor i in [0,11,19,20]:\n    if i == 0:\n        tree_sample = train_df[(train_df.simulationRun==500) & (train_df.faultNumber==i)].iloc[:,3:]\n    else:\n        tree_sample = test_df[(test_df.simulationRun==np.random.randint(500)) & (test_df.faultNumber==i)].iloc[:,3:]\n    tree_sample = pd.DataFrame(scaler.transform(tree_sample), columns = features_df.columns)\n    tree_preds = predict(models, tree_sample, features_to_predict)\n    plt.axvline(x=20,color='r',linestyle='--')\n    if i==0:\n        label='normal conditions'\n    else:\n        label='fault type %d' % i\n    plt.plot(get_mse(tree_sample,tree_preds),label=label)\n    plt.title('MSE for normal conditions and faults')\n    plt.legend()","7a33a2d3":"def series_to_lstm(data, n_in=1, n_out=1, dropnan=True):\n    n_vars = 1 if type(data) is list else data.shape[1]\n    df = pd.DataFrame(data)\n    cols, names = list(), list()\n    #columns = data.columns\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i))\n        names += [('%d(t-%d)' % (j, i)) for j in range(n_vars)]\n    # forecast sequence (t, t+1, ... t+n)\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n        if i == 0:\n            names += [('%d(t)' % (j)) for j in range(n_vars)]\n        else:\n            names += [('%d(t+%d)' % (j, i)) for j in range(n_vars)]\n    # put it all together\n    agg = pd.concat(cols, axis=1)\n    agg.columns = names\n    # drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg","df986adf":"scaler = preprocessing.MinMaxScaler()\n\ndat = train_df[(train_df.faultNumber==0) & (train_df.simulationRun.isin(range(200)))]\ndat = dat.iloc[:,3:]\ndat = scaler.fit_transform(dat)\nprint(dat.shape)\n#dat.head()\n\ntest = train_df[(train_df.faultNumber==0) & (train_df.simulationRun.isin(range(400,500)))]\ntest = test.iloc[:,3:]\ntest = scaler.transform(test)\nprint(test.shape)","1da47b71":"time_steps = 10\n\nref = series_to_lstm(dat,time_steps,1)\nprint(ref.shape)\n\nref_test = series_to_lstm(test,time_steps,1)\nprint(ref_test.shape)","b02d23c8":"train_x = ref.values[:,:-52]\ntrain_y = ref.values[:,-52:]\ntrain_x = train_x.reshape(train_x.shape[0],time_steps,train_x.shape[1]\/\/time_steps)\n\ntest_x = ref_test.values[:,:-52]\ntest_y = ref_test.values[:,-52:]\ntest_x = test_x.reshape(test_x.shape[0],time_steps,test_x.shape[1]\/\/time_steps)","f0587afe":"print(train_x.shape)\nprint(train_y.shape)\nprint(test_x.shape)\nprint(test_y.shape)","64aab2bc":"model_lstm = Sequential()\nmodel_lstm.add(LSTM(50, input_shape=(train_x.shape[1], train_x.shape[2])))\nmodel_lstm.add(Dense(52))\nmodel_lstm.compile(loss='mse', optimizer='adam')\n\n#with tf.device('\/gpu:0'):\nhistory = model_lstm.fit(train_x, train_y, epochs=25, batch_size=50, validation_data=(test_x, test_y), verbose=2, shuffle=False)\n\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show()","80c74bc2":"plt.figure(figsize=(16,10))\nplt.yscale('log')\n\nfor i in [0,1,2,4,6,7]:    \n    if i == 0:\n        test_sample = train_df[(train_df.simulationRun==500) & (train_df.faultNumber==i)].iloc[:,3:]\n    else:\n        test_sample = test_df[(test_df.simulationRun==np.random.randint(500)) & (test_df.faultNumber==i)].iloc[:,3:]\n        \n    test_sample = scaler.transform(test_sample)\n    test_sample = series_to_lstm(test_sample,time_steps,1)\n    \n    test_x = test_sample.iloc[:,:-52]\n    test_y = test_sample.iloc[:,-52:]\n    test_x = test_x.values.reshape(test_x.shape[0],time_steps,test_x.shape[1]\/\/time_steps)\n    \n    pred = model_lstm.predict(test_x)    \n    \n    plt.axvline(x=20,color='r',linestyle='--')\n    if i==0:\n        label='normal conditions'\n    else:\n        label='fault type %d' % i\n    plt.plot(np.square((test_y.iloc[:,:]-pred[:,:])**2).mean(axis=1), label=label)\n    plt.legend()\n    \n","44061e79":"plt.figure(figsize=(16,10))\nplt.yscale('log')\n\nfor i in [0,5,14,17,18]:    \n    if i == 0:\n        test_sample = train_df[(train_df.simulationRun==500) & (train_df.faultNumber==i)].iloc[:,3:]\n    else:\n        test_sample = test_df[(test_df.simulationRun==np.random.randint(500)) & (test_df.faultNumber==i)].iloc[:,3:]\n        \n    test_sample = scaler.transform(test_sample)\n    test_sample = series_to_lstm(test_sample,time_steps,1)\n    \n    test_x = test_sample.iloc[:,:-52]\n    test_y = test_sample.iloc[:,-52:]\n    test_x = test_x.values.reshape(test_x.shape[0],time_steps,test_x.shape[1]\/\/time_steps)\n    \n    pred = model_lstm.predict(test_x)    \n    \n    plt.axvline(x=20,color='r',linestyle='--')\n    if i==0:\n        label='normal conditions'\n    else:\n        label='fault type %d' % i\n    plt.plot(np.square((test_y.iloc[:,:]-pred[:,:])**2).mean(axis=1), label=label)\n    plt.legend()","8822f6e9":"plt.figure(figsize=(16,10))\n\nfor i in [0,11,19,20]:\n    if i == 0:\n        test_sample = train_df[(train_df.simulationRun==500) & (train_df.faultNumber==i)].iloc[:,3:]\n    else:\n        test_sample = test_df[(test_df.simulationRun==np.random.randint(500)) & (test_df.faultNumber==i)].iloc[:,3:]\n        \n    test_sample = scaler.transform(test_sample)\n    test_sample = series_to_lstm(test_sample,time_steps,1)\n    \n    test_x = test_sample.iloc[:,:-52]\n    test_y = test_sample.iloc[:,-52:]\n    test_x = test_x.values.reshape(test_x.shape[0],time_steps,test_x.shape[1]\/\/time_steps)\n    \n    pred = model_lstm.predict(test_x)    \n    \n    plt.axvline(x=20,color='r',linestyle='--')\n    if i==0:\n        label='normal conditions'\n    else:\n        label='fault type %d' % i\n    plt.plot(np.square((test_y.iloc[:,:]-pred[:,:])**2).mean(axis=1), label=label)","ff6917cd":"plt.figure(figsize=(16,10))\nplt.yscale('log')\n\nfor i in [0,12,13]:    \n    if i == 0:\n        test_sample = train_df[(train_df.simulationRun==500) & (train_df.faultNumber==i)].iloc[:,3:]\n    else:\n        test_sample = test_df[(test_df.simulationRun==np.random.randint(500)) & (test_df.faultNumber==i)].iloc[:,3:]\n        \n    test_sample = scaler.transform(test_sample)\n    test_sample = series_to_lstm(test_sample,time_steps,1)\n    \n    test_x = test_sample.iloc[:,:-52]\n    test_y = test_sample.iloc[:,-52:]\n    test_x = test_x.values.reshape(test_x.shape[0],time_steps,test_x.shape[1]\/\/time_steps)\n    \n    pred = model_lstm.predict(test_x)    \n    \n    plt.axvline(x=20,color='r',linestyle='--')\n    if i==0:\n        label='normal conditions'\n    else:\n        label='fault type %d' % i\n    plt.plot(np.square((test_y.iloc[:,:]-pred[:,:])**2).mean(axis=1), label=label)\n    plt.legend()","cb0b2ad4":"plt.figure(figsize=(16,10))\n\nfor i in [0,19]:    \n    if i == 0:\n        test_sample = train_df[(train_df.simulationRun==500) & (train_df.faultNumber==i)].iloc[:,3:]\n    else:\n        test_sample = test_df[(test_df.simulationRun==np.random.randint(500)) & (test_df.faultNumber==i)].iloc[:,3:]\n        \n    test_sample = scaler.transform(test_sample)\n    test_sample = series_to_lstm(test_sample,time_steps,1)\n    \n    test_x = test_sample.iloc[:,:-52]\n    test_y = test_sample.iloc[:,-52:]\n    test_x = test_x.values.reshape(test_x.shape[0],time_steps,test_x.shape[1]\/\/time_steps)\n    \n    pred = model_lstm.predict(test_x)    \n    \n    plt.axvline(x=20,color='r',linestyle='--')\n    if i==0:\n        label='normal conditions'\n    else:\n        label='fault type %d' % i\n    plt.plot(np.square((test_y.iloc[:,:]-pred[:,:])**2).mean(axis=1), label=label)\n    plt.legend()","0286611c":"At first let's check how similar are distributions of the features to the normal. The most convenient way to do it visually is the quantile-quantile plot. To make the result more objective we'll do a Shapiro-Wilk test for our data versus normal distribution and print the p_value.","39376e0c":"For about a half of our features we can accept a __null hypothesis__ that the distribution is normal. It will be quite good if the faults are skewing the distribution significantly. ","4fb180af":"## Quantile-quantile plot","4a41f7d0":"# EDA\n\nLet's do some exploratory data analysis. What kind of data we are dealing with, is it normaly distributed, etc.","da20a71a":"Let's do Shapiro-Wilk test for every feature on every condition and count the amount of features that we can consider as normal.","77e77188":"### Faulty conditions","e7a77d5b":"Plot the same chart, but with the logarithmic scale to consider which treshold we should use for the anomaly detection.","39383aa5":"# LSTM","6a19fff9":"As earlier we will scale our data first using the MinMaxScaler. Then we will take all our features because","c5f261ec":"### Faulty conditions","4a9ee003":"## Feature selection\n\n","dae95ff4":"## Results","b48d83b2":"After doing Shapiro-Wilk test we can say that some faults can be easily detected with the statistical methods and other not. We can try to detect the anomalies with the probabalistic model first.","8976675d":"# Data preparation\n\nThe original dataset consists of RData files with normal and faulty processes datapoints. Assuming that in the real world we mostly have a big amount of data under the normal conditions we will train our models on it and test it on the data with faults. \n\nLet's convert this files in order to get two dataframes for train and test data. ","bf1f7c17":"Now let's try the different types of to explore how our model reacting on them.","43d4df90":"Using the Shapiro-Wilk test with an alpha level 0.05 let's count the number of features that have almost normal distribution. If the statistics is above the given alpha level then we should accept null hypothesis (distribution is normal) and reject otherwise.","bc107da0":"If we investigate all the faults we will see that sometimes our data is significantly skewed from the normal distribution, but sometimes it's not true and faulty data remains to look like normaly distributed. At first let's find the faults which have skewed distribution and detect them using probabalistic model.","c8ea65de":"As we can see our ensembled model based on the simple decision tree models (it can be any other kind of models) can deal with almost all error types in this dataset and we didn't tweak the LGBM parameters yet. The downside of it that it didn't use the time series information from the past and it can be not reliable on the non stationary sequences.","94590227":"#### Fault 2 \u2014 12 features are still normal","2c55d500":"## How many feature distributions we can consider as normal?","e0d6ef49":"So, with this model we're able to detect faults when the feature distributions distantiating from normal. We're calculating the probability to meet the new data and if it's below some threshold we can show the alarm. In case when fault didn't skew the distribution from normal this method won't be robust.\n\nThe further development of this method is to use the multivariate normal distribution. It will automatically capture the correlations between features.","77a6eba2":"As earlier we're taking random samples for a normal and faulty cases, infer the predictions and plot the MSE values. We will see that the MSE amount is decreasing after the data becoming faulty.","3f24fd38":"## Results","6f4fbff5":"#### Fault 20 \u2014 24 features are still normal","ffb595bd":"#### Fault 6 \u2014 0 features are normal","1da7a435":"LSTM stands for long short-term memory. It's a recurrent type of neural network that keeping the information of the previous states which help it catch the sequence patterns.\n\nUsing some pandas tricks we prepare the data for feeding to the training loop. We're using only clean data for training and validating purposes, because in real cases there might be no or not much data with fauly datapoints.","d531ead5":"Now we have train_df and test_df dataframes which have data for normal working condition and data for fault 1 to 20. Total number of datapoints for normal data are 250 000 and 5 000 000 for the falty one with 55 features in both.\n\nTotal number of simulation runs is 500 for every condition. For the fault conditions faulty data is introduced after 1 hour of normal observations.","8fe9102c":"## Steps\n\n- Prepare the data\n- Train N models to predict 1 feature by other N-1 features on train data\n- Predict features on the test data points\n- Calculate MSE","c0328356":"Under the normal conditions our data seems to be normaly distributed. Notice that __xmeas_9__ variable gets only discret values.\n\nNow we'll observe some data during the different faults in order to understand how strong they can skew the initial distribution.","db92af21":"# Probabalistic model","70736283":"We see that LSTM model does a pretty decent job in a novelty detecion task. It can be further improved by tweaking the model architecture and hyperparameters. We can also do a multiple timesteps predictions. ","dca7edd9":"As far as we have the faulty data we can investigate which features will provide the strongest signal. Earlier we saved the features that are normally distributed under the regular conditions and we counted how often Shapiro-Wilk test gives the p_value below 0.05 during the faults for each feature (which means that the points are not normally distributed). Let's sort them and take the top n features.","4cdf614c":"## Results","f647e9e1":"> As we said the faulty conditions are described in the test_df (faultNumber > 0).","16fc9d3f":"Parameters of our probalistic model will be the means and the variances of the feature distributions. Assuming that regular observations will be closer to the feature mean than faulty data we will receive a very small probability in the case of attack. It will be a signal that something is wrong.","8b71662b":"# Introduction\n\nThe purpose of this kernel is to build a predictive model in domain of anomaly detection using Tennessee Eastman Process Simulation Dataset.\n\nWe will try several approaches such as statistical modelling, model based on predicting every feature separately and forecasting using LSTM to detect anomalies in the faulted data.","8ca2b390":"# Ensembling","2a0844a8":"### Normal conditions","9f72918f":"* Investigate you data first (normal distribution, seasonability, etc.).\n* Probabalistic methods can be implemented if you have not much data.\n* LSTM method need a lot of data to implement a robust model.\n* Ensembling of many weak models can give a good results.","f7f786b1":"Let's take random samples for a normal and faulty cases, infer the predictions and plot the MSE values.","e08db35e":"Now let's scale the features to get more stable parameters and train the model \u2014 calculate means and variances.","b5b2faa9":"In this approach we will train N simple models for N features. Every model will be able to predict a single feature by the others. On faulty conditions the differenece between target and prediction should increase. After that we will calculate the MSE along all the predictions for the new data.","ad89b984":"### Normal conditions","ea32a9d0":"To evaluate the new datapoint we calculate the probability for each feature included in our model and return the product of probabilities. It will be the probability to get the given data point.","8f2318c5":"# Take away"}}