{"cell_type":{"56528688":"code","9e9cd82d":"code","3623cc96":"code","e4eb15f8":"code","77a31595":"code","97e121a0":"code","65798d66":"code","bfb6ddb6":"code","1b229111":"code","6ecce2b9":"code","4cb6b745":"code","85bfb548":"code","8df49b04":"code","934047f8":"code","79970e19":"code","7adac3f2":"code","f3053bee":"code","c175a221":"code","9c27b456":"code","19b8faae":"markdown","c47a9329":"markdown","1b78bc0f":"markdown","ffd9206f":"markdown","235a34ab":"markdown","81199d82":"markdown"},"source":{"56528688":"# First timer read here on [loading zip data issue](https:\/\/www.kaggle.com\/dansbecker\/finding-your-files-in-kaggle-kernels) \n# DO NOT RUN THIS CELL TOO OFTEN, WILL TAKE 2 MINS+ TO RELOAD DATA\n\n# basic packages\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# garbage collector to free up memory\nimport gc\ngc.enable()\n\n\n# ori, as in this original variable is to be preserved \n# ori_aisles = pd.read_csv('..\/input\/d\/psparks\/instacart-market-basket-analysis\/aisles.csv')\nori_dept = pd.read_csv('..\/input\/d\/psparks\/instacart-market-basket-analysis\/departments.csv')\nori_order_prod_prior = pd.read_csv('..\/input\/d\/psparks\/instacart-market-basket-analysis\/order_products__prior.csv')\n# ori_order_prod_train = pd.read_csv('..\/input\/d\/psparks\/instacart-market-basket-analysis\/order_products__train.csv')\n# ori_orders = pd.read_csv('..\/input\/d\/psparks\/instacart-market-basket-analysis\/orders.csv')\nori_products = pd.read_csv('..\/input\/d\/psparks\/instacart-market-basket-analysis\/products.csv')\n\n\n","9e9cd82d":"# additional packages needed\nimport sys\nfrom itertools import combinations, groupby\nfrom collections import Counter\nfrom IPython.display import display\nimport pdb\n\n# load data needed\norders_prod_prior = ori_order_prod_prior\nproducts = ori_products\ndept = ori_dept\n\n# aisles = ori_aisles\n# orders_prod_train = ori_order_prod_train\n# orders = ori_orders\n\n","3623cc96":"def size(obj):\n    '''Input a variable object, Output size of the variable in MB '''\n    \n    return \"{0:2f} MB\".format(sys.getsizeof(obj) \/ (1000 * 1000))\n\n\n# sneak peek of order data\nprint(f\"orders_prod_prior dimensions : {orders_prod_prior.shape};   size : {size(orders_prod_prior)}\")\nprint(orders_prod_prior.head(3))\n\nprint(\"------------------\")\n\n# sneak peek of product data\nprint(f\"products dimensions : {products.shape};   size : {size(products)}\")\nprint(products.head(3))\n\nprint(\"------------------\")\n\n# sneak peek of product data\nprint(f\"departments dimensions : {dept.shape};   size : {size(dept)}\")\nprint(dept.head(3))","e4eb15f8":"# since every order have UNIQUE product_id (since add_to_cart_order represent qty of that product_id being ordered), \n# set every order have UNIQUE dept_id too (with add_to_cart_order as qty of that particular dept_id being ordered)\n\n\n# merge orders_prod_prior & products df, to get dept_id\norders_products = orders_prod_prior.merge(products, left_on='product_id', right_on='product_id')\n\nprint(orders_products.head(3))\nprint(f\"dimensions : {orders_products.shape};   size : {size(orders_products)}\")   ","77a31595":"# get order_id & dept_id only\norders_products = orders_products[['order_id', 'department_id']]","97e121a0":"# removing duplicates idea\nprint(orders_products[orders_products['order_id'] == 4]) # what we have now\nprint(orders_products[orders_products['order_id'] == 4].drop_duplicates(subset=['department_id'])) # what we hope to have as final result","65798d66":"# drop dept_id duplicates using numpy vectorization\n\n# # sort df by order_id\n# orders_products = orders_products.sort_values(by=['order_id'])\n\n# set order_id as index\norders_products_array = orders_products.set_index('order_id')\n\n# convert the whole dataframe to numpy array pairs\norders_products_array = orders_products_array.reset_index().to_numpy()\n","bfb6ddb6":"# %%timeit   result : 41 s \u00b1 13.5 s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n\n# drop duplicates\norders_products_array_unique = np.unique(orders_products_array, axis=0)","1b229111":"orders_products_array_unique","6ecce2b9":"# convert numpy array back to pandas df\norders_departments_id = pd.DataFrame(orders_products_array_unique, \n             columns=['order_id', \n                      'department_id'])","4cb6b745":"# set orders_departments_id that is suitable for 'associate rules' analysis\norders_departments_id = orders_departments_id.set_index('order_id')['department_id'].rename('item_id')\nprint(orders_departments_id.head(5))\nprint(f\"dimensions: {orders_departments_id.shape};  \\nsize: {size(orders_departments_id)};   \\nunique_orders: {len(orders_departments_id.index.unique())};   \\nunique_items: {len(orders_departments_id.value_counts())}; \\ntype: {type(orders_departments_id)};\")\n\n","85bfb548":"# convert order df to series. order_id as index & product_id (renamed to item_id) as value\norders_products_id = ori_order_prod_prior.set_index('order_id')['product_id'].rename('item_id')   # take data from orders_products will error in get_item_pairs(). must take from orders.\nprint(orders_products_id.head(5))\nprint(f\"dimensions: {orders_products_id.shape};  \\nsize: {size(orders_products_id)};   \\nunique_orders: {len(orders_products_id.index.unique())};   \\nunique_items: {len(orders_products_id.value_counts())}; \\ntype: {type(orders_products_id)};\")\n\n","8df49b04":"# helper function ensure we can calculate lift with no buffer due to dealing large dataset (data mgmt)\n\n\ndef freq(iterable):\n    '''Returns frequency counts for items & item pairs'''\n    \n    if type(iterable) == pd.core.series.Series:\n        return iterable.value_counts().rename(\"freq\")\n    else: \n        return pd.Series(Counter(iterable)).rename(\"freq\")\n\n\n    \ndef order_count(order_item):\n    '''Return no of unique orders'''\n    return len(set(order_item.index))\n\n\n\ndef get_item_pairs(order_item):\n    '''Return generator that yields item pairs, one at a time. (helps to facilitate large dataset)'''\n    \n    # input\n    # order_id     product_id\n    # 1             222\n    # 1             223\n    # 2             222\n    # 2             192\n    # output : array([1, 222], [1, 223], [2,222], [2,192])\n    order_item = order_item.reset_index().to_numpy()\n    \n    \n    for order_id, order_object in groupby(order_item, lambda x: x[0]):\n        item_list = [item[1] for item in order_object]\n\n        for item_pair in combinations(item_list, 2):\n            yield item_pair\n        \n\n        \ndef merge_item_stats(item_pairs, item_stats):\n    '''Returns frequency & support associated with items'''\n    \n    return (item_pairs\n                .merge(item_stats.rename(columns={'freq': 'freqA', 'support': 'supportA'}), left_on='item_A', right_index=True)\n                .merge(item_stats.rename(columns={'freq': 'freqB', 'support': 'supportB'}), left_on='item_B', right_index=True)\n           )\n\n\n\ndef merge_targeted_category(rules, targeted_category):\n    '''Returns name associated with item'''\n    \n    columns = ['itemA','itemB','freqAB','supportAB','freqA','supportA','freqB','supportB', \n               'confidenceAtoB','confidenceBtoA','lift']\n    rules = (rules\n                .merge(targeted_category.rename(columns={'targeted_category': 'itemA'}), left_on='item_A', right_on='item_id')\n                .merge(targeted_category.rename(columns={'targeted_category': 'itemB'}), left_on='item_B', right_on='item_id')\n            )\n    return rules[columns]               ","934047f8":"# Association Rules function, to get lift() of all pairs\n\ndef association_rules(order_item, min_support):\n\n    print(\"Starting order_item: {:22d}\".format(len(order_item)))\n\n    # 1) Calculate item frequency and support of each item (support(A))\n    item_stats             = freq(order_item).to_frame(\"freq\") \n    item_stats['support']  = item_stats['freq'] \/ order_count(order_item) # * 100  \n    \n\n\n\n    \n    # 2) Remove items from order_item df that items is below min support \n    qualifying_items       = item_stats[item_stats['support'] >= min_support].index\n    order_item             = order_item[order_item.isin(qualifying_items)]\n     \n \n    print(\"Items with support >= {}: {:15d}\".format(min_support, len(qualifying_items)))\n    print(\"Remaining order_item: {:21d}\".format(len(order_item)))\n\n\n    \n    \n    \n    # 3) Since we want to know what PAIRS happen the most from EVERY ORDERS, there is no point including any orders data that has only 1 item inside it.\n    # Hence, Remove orders from order_item df that order with less than 2 items (remove any single-orders)\n    order_size             = freq(order_item.index)\n    qualifying_orders      = order_size[order_size >= 2].index\n    order_item             = order_item[order_item.index.isin(qualifying_orders)]\n\n    print(\"Remaining orders with 2+ items: {:11d}\".format(len(qualifying_orders)))\n    print(\"Remaining order_item: {:21d}\".format(len(order_item)))\n\n\n\n    \n    \n    # 4) Since all single orders has been removed, Recalculate item frequency and support (repeat of step1, optimisation to remove any useless data & make code run faster)\n    item_stats             = freq(order_item).to_frame(\"freq\")\n    item_stats['support']  = item_stats['freq'] \/ order_count(order_item) # * 100\n\n    \n    \n    \n    \n    # 5) Get all unique items from all orders, pair them up, and count the frequency of each pair happen from all orders\n    # run \"Counter(item_pair_gen)\"\n    item_pair_gen          = get_item_pairs(order_item)  \n\n\n    \n    \n    \n    # 6) Calculate item pair frequency and support of each pair (support(A,B))\n    item_pairs              = freq(item_pair_gen).to_frame(\"freqAB\")\n    item_pairs['supportAB'] = item_pairs['freqAB'] \/ len(qualifying_orders) # * 100\n\n    print(\"Item pairs: {:31d}\".format(len(item_pairs)))\n\n\n    \n    \n    \n    # 7) Remove pairs from item_pairs that the pair supportAB  below min support (repeat of step2, optimisation to remove any useless data & make code run faster)\n    item_pairs              = item_pairs[item_pairs['supportAB'] >= min_support]\n\n    print(\"Item pairs with support >= {}: {:10d}\\n\".format(min_support, len(item_pairs)))\n            \n\n\n        \n        \n    # 8) Create table of association rules and compute relevant metrics (confidence & lift)\n    item_pairs = item_pairs.reset_index().rename(columns={'level_0': 'item_A', 'level_1': 'item_B'})\n    item_pairs = merge_item_stats(item_pairs, item_stats)\n    \n    item_pairs['confidenceAtoB'] = item_pairs['supportAB'] \/ item_pairs['supportA']\n    item_pairs['confidenceBtoA'] = item_pairs['supportAB'] \/ item_pairs['supportB']\n    item_pairs['lift']           = item_pairs['supportAB'] \/ (item_pairs['supportA'] * item_pairs['supportB'])\n    \n    \n    # Return association rules sorted by lift in descending order\n    return item_pairs.sort_values('lift', ascending=False)\n    ","79970e19":"# 0.01 min threshold is fine, meaning, out of 100 unique orders, 1 order has that pair \n\n# associate rules : products\nproducts_rules = association_rules(orders_products_id, 0.01)  \n","7adac3f2":"# 1) Which products that's always bought together ?\n\n# Replace item ID with item name and display association rules\ntargeted_category   = products.rename(columns={'product_id':'item_id', 'product_name':'targeted_category'})\nproducts_rules_final = merge_targeted_category(products_rules, targeted_category).sort_values('lift', ascending=False)\n\n# display result on item pair, ascending order by \"lift\", only show lift > 1\nprint(products_rules_final[products_rules_final['lift'] > 1][['itemA', 'itemB', 'lift']].head(10))\nprint(f\"dimensions: {products_rules_final.shape};  \\nsize: {size(products_rules_final)};   \\nunique_orders: {len(products_rules_final.index.unique())};   \\nunique_items: {len(products_rules_final.value_counts())};\")","f3053bee":"# associate rules : departments\ndept_rules = association_rules(orders_departments_id, 0.01)  ","c175a221":"# 2) Which dept that's always bought together ?\n\n# Replace item ID with item name and display association rules\ntargeted_category   = dept.rename(columns={'department_id':'item_id', 'department':'targeted_category'})\ndept_rules_final = merge_targeted_category(dept_rules, targeted_category).sort_values('lift', ascending=False)\n\n\n# display result on item pair, ascending order by \"lift\"\nprint(dept_rules_final[dept_rules_final['lift'] > 1][['itemA', 'itemB', 'lift']].head(10))\nprint(f\"dimensions: {dept_rules_final.shape};  \\nsize: {size(dept_rules_final)};   \\nunique_orders: {len(dept_rules_final.index.unique())};   \\nunique_items: {len(dept_rules_final.value_counts())};\")","9c27b456":"# # Scrub process \n# # naive way. DO NOT RUN THIS! here to just explain concept on how to tackele this problem the most reckless way\n\n# # empty df to store non-duplicate dept\n# unique_dept_tes = pd.DataFrame(columns = ['order_id', 'department_id'])\n\n\n# # function to remove duplicate\n# def remove_duplicate_depts_given_orderid(df, order_id):\n#     '''input dataframe & order_id, output that given order_id with UNIQUE dept_id'''\n    \n#     return df[df['order_id'] == order_id].drop_duplicates(subset=['department_id'])\n\n\n\n# # small version for order 2,3,4 only\n# i = 2\n# # while i <= tes['order_id'].max():\n# while i <= 4:    \n    \n#     # remove duplicate in that selected order_id\n#     temp_df = remove_duplicate_depts_given_orderid(tes, i)\n    \n#     # append temp_df to new df \n#     unique_dept_tes = unique_dept_tes.append(temp_df, ignore_index=True)\n\n    \n#     i = i + 1\n    \n#     if i == 10 or i == 100 or i == 1000 or i == 10000 or i == 100000 or i == 1000000 or i == 3000000 or i == 3421000 or i == 3421080:\n#         print(f\" order_id up to {i} out of {tes['order_id'].max()} is done, continuing............\")","19b8faae":"## Explore\n\n* Skipped entirely, as this analysis itself almost eats up 15GB ram\n* Will be covered [here](https:\/\/www.kaggle.com\/dwihdyn\/mkt-bskt-prediction\/)\n","c47a9329":"## Obtain\n* load dataset & see their size","1b78bc0f":"## iNterpret\n* Jump to \"Call-To-Action\" section [here](https:\/\/dwihdyn.github.io\/journals\/6-mba-retail.html)\n","ffd9206f":"## Scrub\n* merge orders & products df first\n* Convert dept dataframe to format suitable for association rules (df to series), which is order_id as index & dept_id as value\n* Convert products dataframe to format suitable for association rules (df to series), which is order_id as index & product_id as value","235a34ab":"## Model\n* Functions to calculate Association Rule & its helpers\n* Input df that inside is order_id & product_id, Output every pair of product_id that has high chance it buys together (lift), same procedure for dept\n","81199d82":"# Market Basket Analysis\n-----\n### Goal : \n* Given all past orders, which dept that always come together ?\n* once known, which products to be specific on each od these dept are always bought together ?\n\n\n### Why?\n* So business can reorganize the store layout & run promotional campaign to bundle these item together. [Reference](https:\/\/www.kaggle.com\/datatheque\/association-rules-mining-market-basket-analysis)\n\n\n<br\/>\n\n-----\n\n### Proposed Solution :\n1. We can answer part1 question using concept called \"Association Rules\"\n    * In short, calculate each pair of items relationships (lift). say we have item A & B.\n        * if lift > 1, A & B occur together more often than random\n        * if lift = 1, A & B occur together only by chance (random)\n        * if lift < 1, A & B occur together less often than random\n        \n    * calculate lift(A,B) = Pr(A & B bought together) \/ (Pr(A bought) * Pr(B bought))\n\n<br\/>\n\n2. Formal answer : \n    * Pr : Probability of...\n\n```\nStep1 : \ncalculate support(A,B), support(A), support(B)\n\nsup(A,B) = Pr(A & B bought together) = Pr(A n B)\nsup(A)   = Pr(A is bought) = Pr(A)\nNote : sup(A,B) =\/= sup(B,A)\n```\n\n\n```\nStep2 : \nuse support(A,B) to get Confidence(A,B) = Pr(B bought, given A alrd bought)\n\nCo(A,B) = Pr(B|A)\n        = Pr(B n A) \/ P(A).  https:\/\/images.app.goo.gl\/Cb8Z6aQrtpBpeWTC8\n        = sup(A,B) \/ sup(A)\n        \nNote : Co(A,B) =\/= Co(B,A)\n```\n\n```\nStep3 :\nuse Confidence(A,B) to get lift(A,B) = Likelihood of A & B bought together)\nLi(A,B) = Pr(A & B bought together) \/ ( Pr(A bought) * Pr(B bought) )\n        = Pr(A & B bought together) \/ Pr(A bought) \/ Pr(B bought)\n        = Pr(A n B) \/ Pr(A) \/ Pr(B)\n        = Pr(A n B) \/ Pr(B) \/ Pr(A)\n        = Pr(A|B) \/ Pr(A)\n        = Co(B,A) \/ sup(A)\n```\n\n```\n0 < sup(A,B) < 1\n0 < Co(A,B) < 1\n0 < Li(A,B) < inf\n```\n\n<br\/>\n\n3. 0.01 min threshold is fine, meaning, out of 100 unique orders, 1 order has that pair \n* see [Apriori Algorithm](https:\/\/www.kaggle.com\/datatheque\/association-rules-mining-market-basket-analysis#Apriori-Algorithm) small example. we have 5 orders, and we measure by pair, hence the least pair (next to 0) is that \"it happen in onc order, out of 5\", hence 1\/5\n\n<br\/>\n\n4. How \"Association Rules\" works in order to find pair items that happen to be come together more frequent than random ? (simplified terms)\n* get each orders (where 1 order has many products inside)\n* compare 1 order with another order\n* does a particular pair of products always exist in each of the orders ?\n    * if yes, record. the more frequent this happens, the higher the 'lift' value \n    * if no, if its very less frequent than min_threshold, we throw that pair entirely\n* repeat to all order_id pairs.\n    * hence \"0 < max number of rows in output result < len(order_id) C 2\" . (if len(order_id)=10, -> 10C2)\n\n<br\/>\n\n-----\n\n### Useful reads :\n* [Association Rules Explained](https:\/\/towardsdatascience.com\/a-gentle-introduction-on-market-basket-analysis-association-rules-fa4b986a40ce)"}}