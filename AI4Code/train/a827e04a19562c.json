{"cell_type":{"4f60206d":"code","49943083":"code","14d61d8c":"code","c09b4366":"code","d4daf9c8":"code","da8cd920":"code","15f0a36a":"code","04f0cc81":"code","da11b6d2":"code","807dd981":"markdown","a3433344":"markdown","e172807e":"markdown"},"source":{"4f60206d":"import numpy as np\nimport pandas as pd\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')","49943083":"G = nx.karate_club_graph()\n\ndef make_graph(G):\n    # check if Graph is directed\n    print('Directed:', nx.is_directed(G))\n\n    # check if Graph is weighted\n    print('Weighted:', nx.is_weighted(G))\n    print()\n    \n    # converting to directed Graph for PageRank\n    if not nx.is_directed(G):\n        print('Graph converted to directed..')\n        G = G.to_directed()    \n\n    print('Directed:', nx.is_directed(G))\n    print()\n\n    # labelling nodes as integers\n    print('Relabelling nodes to integers..')\n    n_unique_nodes = len(set(G.nodes()))\n    node2int = dict(zip(set(G.nodes()), range(n_unique_nodes)))\n    int2node = {v:k for k,v in node2int.items()}\n\n    G = nx.relabel_nodes(G, node2int)\n\n    # remove isolated nodes\n    print('Removing isolated nodes..')\n    nodes = G.nodes()\n    for node in nodes:\n        if len(G.edges(node))==0:\n            G.remove_node(node)\n    return G, int2node           ","14d61d8c":"def plot_graph(G, final_probs, int2node, bool_final_probs=False):\n    \n    # defining labels\n    labels = int2node\n\n    # zachary karate club\n    try:\n        clubs = np.array(list(map(lambda x: G.nodes[x]['club'], G.nodes())))\n        labels = dict(zip(G.nodes(), clubs)) \n    except:\n        pass   \n\n    if not bool_final_probs:\n        nx.draw(G, with_labels=True, alpha=0.8, arrows=False, labels=labels)\n    else:\n        nx.draw(G, with_labels=True, alpha=0.8, arrows=False, node_color = final_probs, \\\n                                                                                        cmap=plt.get_cmap('viridis'), labels=labels)\n\n        # adding color bar for pagerank importances\n        sm = plt.cm.ScalarMappable(cmap=plt.get_cmap('viridis'), norm=plt.Normalize(vmin = min(final_probs), vmax=max(final_probs)))\n        sm._A = []\n        plt.colorbar(sm)  \n    return plt","c09b4366":"def make_pagerank_matrix(G, alpha):\n    n_nodes = len(G.nodes())\n\n    # building adjacent matrix\n    adj_matrix = np.zeros(shape=(n_nodes, n_nodes))\n    for edge in G.edges():\n        adj_matrix[edge[0], edge[1]] = 1\n\n    # building transition probability matrix\n    tran_matrix = adj_matrix \/ np.sum(adj_matrix, axis=1).reshape(-1,1)\n\n    # building random surfer matrix\n    random_surf = np.ones(shape = (n_nodes, n_nodes)) \/ n_nodes    \n\n    # building transition matrix for absorbing nodes\n    absorbing_nodes = np.zeros(shape = (n_nodes,))\n    for node in G.nodes():\n        if len(G.out_edges(node))==0:\n            absorbing_nodes[node] = 1\n    absorbing_node_matrix = np.outer(absorbing_nodes, np.ones(shape = (n_nodes,))) \/ n_nodes\n\n    # stochastic matrix\n    stochastic_matrix = tran_matrix + absorbing_node_matrix\n\n    # pagerank matrix\n    pagerank_matrix = alpha * stochastic_matrix + (1-alpha) * random_surf\n    return pagerank_matrix","d4daf9c8":"def random_walk(G, alpha, n_iter):\n    n_nodes = len(G.nodes())\n    initial_state = np.ones(shape=(n_nodes,)) \/ n_nodes\n    pagerank_matrix = make_pagerank_matrix(G, alpha)\n\n    new_initial_state = initial_state\n    print('Running random walk..')\n    NORM = []\n    for i in range(n_iter):\n        final_state = np.dot(np.transpose(pagerank_matrix), new_initial_state)\n        \n        prev_initial_state = new_initial_state\n        new_initial_state = final_state\n        L2 = np.linalg.norm(new_initial_state-prev_initial_state)\n        NORM.append(L2)\n        if np.allclose(new_initial_state, prev_initial_state):\n            print(f'Converged at {i+1} iterations..')\n            break\n\n    plt.figure(figsize=(5,4))\n    plt.plot(range(i+1), NORM)\n    plt.xlabel('iterations')\n    plt.ylabel('Euclidean Norm')\n    plt.title('Convergence plot')\n    plt.show()\n    return final_state","da8cd920":"def run(G, alpha, n_iter):\n\n    G, int2node = make_graph(G)\n    print()\n    print('Number of nodes: ', len(G.nodes()))\n    print('Number of edges: ', len(G.edges())) \n    print()    \n\n    final_probs = random_walk(G, alpha, n_iter)\n\n    # ensuring pagerank importance for each node\n    assert len(final_probs) == len(G.nodes())\n\n    # ensuring probabilities sum to 1\n    assert np.allclose(np.sum(final_probs), 1)\n\n    print()\n    print('Pagerank importances..')\n    print(final_probs)\n\n    plt.figure(figsize=(25,8))\n    plt.subplot(121)\n    plot_graph(G, None, int2node, bool_final_probs=False)\n    plt.subplot(122)\n    plot_graph(G, final_probs, int2node, bool_final_probs=True)\n    plt.show()\n    return final_probs","15f0a36a":"alpha = 0.8\nn_iter = 1000\n\nG = nx.karate_club_graph()\nfinal_probs = run(G, alpha, n_iter)","04f0cc81":"G = nx.davis_southern_women_graph()\nfinal_probs = run(G, alpha, n_iter)","da11b6d2":"G = nx.florentine_families_graph()\nfinal_probs = run(G, alpha, n_iter)","807dd981":"### Random Walk\n\nWe start with an initial state where the surfer has equal probability to start at any of the pages. We have modelled the transition matrix (pagerank matrix) which helps in finding the transition probabilites for the next state given current state.\n\nThe algorithm will converge when there is approximately no difference between two consecutive states. At this point, the vector of probabilities can be seen as probabilities a user is likely to end up in general, and these can be treated as PageRank scores. A higher probability will mean that the webpage\/link is more important and vice versa.","a3433344":"### Introduction\n\nWe will implement the PageRank algorithm from scratch and explore it on famous social networks available in Networkx.\nThere are few preprocessing steps:\n1. Converting the graph to a directed graph (if the graph is undirected, change it to bi-directed graph)\n2. Relabelling nodes as integers (encoding string names to unique node IDs)\n3. Remove isolated nodes (if there are nodes which are not connected to other nodes howsover, remove them from graph)","e172807e":"### PageRank algorithm\n\nAssume a random surfer who starts surfing from one of the webpages available and keeps surfing from one webpage to another by clicking hyperlinks, until he restarts from any of the webpages and follows the same process.\n\nThe idea behind PageRank algorithm is that people are more likely to end up on pages which are more important than others. And good links will have more of good\/important links connecting to them than bad links\/spams. Lastly, spams will have the least number of connections and people are least likely to end up there.\n\nNow the random surfer model can be modelled as a Markov Chain where the next state of surfer depends only on the current state of surfer. This is also known as Random Walk. However, there are some nuances of our network that PageRank deals with:\n\n1. **Absorbing nodes**: Nodes which have an incoming link but not outgoing link. In this case, we assume that if random surfer happens to land at such node, they can migrate to any of the all nodes with equal probability. This helps the algorithm to converge.\n\n2. **Restart probability (alpha)**: Often a random surfer might not want to keep following hyperlinks but restart the search process. To model this, we say that there is a certain probability for random surfer to restart."}}