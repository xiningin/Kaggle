{"cell_type":{"2f927a8c":"code","cc333438":"code","9b547832":"code","67200ca5":"code","8e13e8c7":"code","b087f78d":"code","5090ddb5":"code","57a27c8d":"code","f4e5f215":"markdown","04b52e54":"markdown","441c1029":"markdown"},"source":{"2f927a8c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport cloudpickle\nfrom skimage import measure\nfrom matplotlib import colors\nfrom numpy.lib.stride_tricks import as_strided\nimport json\nfrom pathlib import Path\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n#  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ INITIALISE ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n# now get the tasks\ndef startup(dataset='train', printsample=False):\n\n    data_path = Path('\/kaggle\/input\/abstraction-and-reasoning-challenge')\n\n    # Any results you write to the current directory are saved as output.\n\n    training_path = data_path \/ 'training'\n    evaluation_path = data_path \/ 'evaluation'\n    test_path = data_path \/ 'test'\n\n    training_tasks = sorted(os.listdir(training_path))\n    evaluation_tasks = sorted(os.listdir(evaluation_path))\n    test_tasks = sorted(os.listdir(test_path))\n\n    alltasks = []\n    tasknames = []\n    \n    if dataset is 'train':       \n        for i in range(len(training_tasks)):\n            task_file = str(training_path \/ training_tasks[i])\n            tasknames.append(training_tasks[i].split('.')[0])\n\n            with open(task_file, 'r') as f:\n                nexttask = json.load(f)\n                alltasks.append(nexttask)\n    elif dataset is 'eval':\n        for i in range(len(evaluation_tasks)):\n            task_file = str(evaluation_path \/ evaluation_tasks[i])\n            tasknames.append(evaluation_tasks[i].split('.')[0])\n\n            with open(task_file, 'r') as f:\n                nexttask = json.load(f)\n                alltasks.append(nexttask)\n    elif dataset is 'test':\n        for i in range(len(test_tasks)):\n            task_file = str(test_path \/ test_tasks[i])\n            tasknames.append(test_tasks[i].split('.')[0])\n\n            with open(task_file, 'r') as f:\n                nexttask = json.load(f)\n                alltasks.append(nexttask)\n    else:\n        print('dataset assigned to non-existent string')\n        alltasks = 0\n\n    return alltasks, tasknames","cc333438":"#  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ARC CLASSES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nimport numpy as np\nimport cv2\nimport scipy.ndimage\nfrom copy import deepcopy\n\n\nclass SameColourObject:\n    width = None\n    height = None\n    colour = None\n    elementarr = None\n    distrelotherobjs = None\n    positionabsx = None\n    positionabsy = None\n    holecount = 0\n    holes = None\n\n    def findholes(self, obj):\n        filledobj = scipy.ndimage.morphology.binary_fill_holes(obj).astype(int)\n        holes = filledobj - obj\n        holes = np.uint8(holes)\n\n        if np.count_nonzero(holes) > 0:\n            #  labels is the shape of the hole. Potentially could do something with this later\n            retval, labels = cv2.connectedComponents(holes)\n            self.holecount = retval - 1\n\n    def __init__(self, labels, specificcolour):\n        # cut away where object isn't in the image\n        validcols = labels.sum(axis=0) > 0  # logical index col\n        validrows = labels.sum(axis=1) > 0  # logical index row\n        firstcol = min(np.where(validcols)[0])\n        firstrow = min(np.where(validrows)[0])\n        lastcol = max(np.where(validcols)[0])\n        lastrow = max(np.where(validrows)[0])\n        validcols[firstcol:lastcol] = True\n        validrows[firstrow:lastrow] = True\n\n        # assign the top left corner\n        self.positionabsx = int(firstcol)\n        self.positionabsy = int(firstrow)\n\n        self.elementarr = labels[np.ix_(validrows, validcols)]\n\n        self.colour = specificcolour\n\n        self.height = np.size(self.elementarr, axis=0)\n        self.width = np.size(self.elementarr, axis=1)\n\n        self.findholes(self.elementarr)\n\n\nclass MultiColourObject:\n    width = None\n    height = None\n    elementarr = None\n    distrelotherobjs = None\n    positionabsx = None\n    positionabsy = None\n    samecolourobjs = []\n    multicolourobjs = []\n\n    def __init__(self, labels):\n        \"\"\" takes a full-sized image, finds the object in the image, finds the top\n        left hand corner of the bounding box around that object within the image and\n        saves that to positionAbs, then places the boxed obj into elementarr. \n        \"\"\"\n\n        # cut away where object isn't in the image\n        validcols = labels.sum(axis=0) > 0  # logical index col\n        validrows = labels.sum(axis=1) > 0  # logical index row\n        firstcol = min(np.where(validcols)[0])\n        firstrow = min(np.where(validrows)[0])\n        lastcol = max(np.where(validcols)[0])\n        lastrow = max(np.where(validrows)[0])\n        validcols[firstcol:lastcol] = True\n        validrows[firstrow:lastrow] = True\n\n        # assign the top left corner\n        self.positionabsx = firstrow\n        self.positionabsy = firstcol\n\n        self.elementarr = labels[np.ix_(validrows, validcols)]\n\n        self.height = np.size(self.elementarr, axis=0)\n        self.width = np.size(self.elementarr, axis=1)\n\n\nclass SingleImagePair:\n    fullinputimg = None\n    fulloutputimg = None\n    fullpredimg = None\n    inputsamecolourobjs = []\n    predoutputsamecolobjs = []  # predicted same colour objects\n    predoutputmulticolobjs = []\n    predoutputcanvas = None  # output canvas\n    outputsamecolourobjs = []\n    backgroundcol = None\n\n    def gridrefactorobjs(self):\n        \"\"\"looks for periodicity and shape similarity. If something there, refactor all objs so they conform\n        with this pattern\n        \"\"\"\n        objwidths, objheights, shortestx, shortesty = [], [], [], []\n        furtheestleft, furthestup, shortestxt2, shortestyt2 = 100, 100, 100, 100\n        for objno, obj1 in enumerate(self.predoutputsamecolobjs):\n            if len(self.predoutputsamecolobjs[objno+1:]) != 0:\n                for obj2 in self.predoutputsamecolobjs[objno+1:]:\n                    shortestxt1 = obj2.positionabsx - obj1.positionabsx\n                    shortestyt1 = obj2.positionabsy - obj1.positionabsy\n\n                    if (shortestxt1 > 0) & (shortestxt1 < shortestxt2) & (obj2.positionabsy == obj1.positionabsy):\n                        shortestxt2 = shortestxt1\n\n                    if (shortestyt1 > 0) & (shortestyt1 < shortestyt2) & (obj2.positionabsx == obj1.positionabsx):\n                        shortestyt2 = shortestyt1\n\n                shortestx = shortestx + [shortestxt2]\n                shortesty = shortesty + [shortestyt2]\n            objwidths = objwidths + [obj1.elementarr.shape[1]]\n            objheights = objheights + [obj1.elementarr.shape[0]]\n\n            if obj1.positionabsx < furtheestleft:\n                furtheestleft = obj1.positionabsx\n                topleftx = obj1.positionabsx\n                toplefty = obj1.positionabsy\n\n            if obj1.positionabsy < furthestup:\n                furthestup = obj1.positionabsy\n                toplefty = obj1.positionabsy\n\n        mostfreqwidth = max(set(objwidths), key=objwidths.count)\n        mostfreqheight = max(set(objheights), key=objheights.count)\n        mostfreqx = max(set(shortestx), key=shortestx.count)\n        mostfreqy = max(set(shortesty), key=shortesty.count)\n\n        # sense-check at this point\n        if (mostfreqwidth >= mostfreqx) or (mostfreqheight >= mostfreqy):\n            self.gridapplied = 0\n            return\n\n        # use these numbers to set your grid & rep obj size. If you can account for all pixels in each obj: good.\n        # start at top left obj\n        bwfullimg = (self.fullpredimg != self.backgroundcol) * 1\n        pixelscounted = 0\n        xpos = topleftx\n        ypos = toplefty\n        newobjrefactor = []\n        rowsize = 0\n        objlist = []\n        counter = 0\n        outputcanvas = np.zeros([bwfullimg.shape[0], bwfullimg.shape[1]])\n\n        while (ypos + mostfreqheight) <= bwfullimg.shape[0]:\n            while (xpos + mostfreqwidth) <= bwfullimg.shape[1]:\n                bwarr1 = bwfullimg[ypos:ypos+mostfreqheight, xpos:xpos+mostfreqwidth]  # bw array for counting obj pixels\n                bwarr = deepcopy(outputcanvas)\n                bwarr[ypos:ypos+mostfreqheight, xpos:xpos+mostfreqwidth] = bwarr1\n                colarr = self.fullpredimg[ypos:ypos+mostfreqheight, xpos:xpos+mostfreqwidth]  # colarr for making newobj\n                pixelscounted = pixelscounted + bwarr.sum()  # count the pixels\n\n                if len(np.delete(np.unique(colarr), np.where(np.unique(colarr) == 0))) == 1:  # rem backcol & chek 1 col left\n                    specificcolour = np.delete(np.unique(colarr), np.where(np.unique(colarr) == 0))[0]\n                    newobj = SameColourObject(bwarr, specificcolour)\n                    newobjrefactor = newobjrefactor + [newobj]\n                    objlist = objlist + [counter]\n                else:\n                    objlist = objlist + [None]\n                xpos = xpos + mostfreqx\n                counter += 1\n            rowsize += 1\n            xpos = topleftx\n            ypos = ypos + mostfreqy\n\n        objgrid = np.array(objlist).reshape([rowsize, int(counter\/rowsize)])\n\n        if pixelscounted == bwfullimg.sum():  # if all objs are accounted for, re-factor objs into grid format\n            self.predoutputsamecolobjs = newobjrefactor\n            self.gridapplied = 1\n            # self.objgrid = objgrid\n            print('refactored by seperating by obj grid')\n        else:\n            self.gridapplied = 0\n\n    def findscobjects(self, side, forgroundcols):\n        \"\"\"Find same colour objects in either the input or output and place that into the image\n        \"\"\"\n\n        for specificcolour in forgroundcols:\n            # process so we can find connected components (individual obs) in image\n            if side == 'output':\n                cvimg = self.fulloutputimg == specificcolour\n            elif side == 'input':\n                cvimg = self.fullinputimg == specificcolour\n\n            cvimg = cvimg * 1\n            cvimg = np.uint8(cvimg)\n\n            # find individual objects\n            retval, labels = cv2.connectedComponents(cvimg)\n            for objs in range(1, retval):\n                newobj = SameColourObject((labels == objs) * 1, specificcolour)\n                if side == 'output':\n                    self.outputsamecolourobjs = self.outputsamecolourobjs + [newobj]\n                elif side == 'input':\n                    self.inputsamecolourobjs = self.inputsamecolourobjs + [newobj]\n\n            self.predoutputsamecolobjs = deepcopy(self.inputsamecolourobjs)\n\n    def findbackground(self):\n        countforcols = []\n\n        # rule 1: the colour needs to be shared in both the input and the output\n        uniquesin = np.unique(self.fullinputimg)\n        uniquesout = np.unique(self.fulloutputimg)\n        uniques = list(set(uniquesin) & set(uniquesout))\n\n        if 0 in uniques:  # just make back background: 99% of the time it is (bad but need to write rest of code!!)\n            self.backgroundcol = 0\n        elif not uniques:  # empty list (how can this ever happen?! 0 - black - is a col. So...\n            self.backgroundcol = None\n        else:\n            # rule 2: the colour is the dominent colour of the input\n            for specificcolour in uniques:\n                countforcols.append(np.count_nonzero(self.fullinputimg == specificcolour))\n\n            self.backgroundcol = uniques[countforcols.index(max(countforcols))]\n\n    def extraobjattrs(self):\n        # make a list of x co-ords & y co-ords\n        xstartcoords = []\n        ystartcoords = []\n\n        for obj in self.inputsamecolourobjs:\n            xstartcoords = xstartcoords + [obj.positionabsx]\n            ystartcoords = ystartcoords + [obj.positionabsy]\n\n        xstartcoords.sort()\n        ystartcoords.sort()\n\n        for obj in self.inputsamecolourobjs:\n            obj.xorder = next(i for i, x in enumerate(xstartcoords) if x == obj.positionabsx)\n            obj.yorder = next(i for i, y in enumerate(ystartcoords) if y == obj.positionabsy)\n\n    def __init__(self, tip, traintest, backgroundcol=None):\n        \"\"\"Takes a task image pair and populates all\n        properties with it\n        tip ---   dict, where tip['input'] is the input and\n                  taskImgPair['output'] is the output\n        \"\"\"\n        # inputs first\n        self.fullinputimg = np.array(tip[\"input\"])\n\n        if traintest == 'train':  # need to do this now as it's used later\n            self.fulloutputimg = np.array(tip[\"output\"])\n\n        # find unique colours\n        inuniques = np.unique(self.fullinputimg)\n\n        # assuming background is the prominent colour: find background & assign the property\n        if traintest == 'train':  # if test: we'll need to get it from a train imgpair, can't do it here\n            self.findbackground()\n        else:\n            self.backgroundcol = backgroundcol\n\n        # find all colours other than background colours\n        inforgroundcols = inuniques.tolist()\n        if self.backgroundcol in inforgroundcols:\n            inforgroundcols.remove(self.backgroundcol)\n\n        # find same colour invididual objects in image\n        self.findscobjects('input', inforgroundcols)\n\n        if traintest == 'train':\n            outuniques = np.unique(self.fulloutputimg)\n\n            outforgroundcols = outuniques.tolist()\n            if self.backgroundcol in outforgroundcols:\n                outforgroundcols.remove(self.backgroundcol)\n\n            self.findscobjects('output', outforgroundcols)\n\n        # add extra attributes\n        self.extraobjattrs()\n\n        # create the first predoutputimg\n        self.fullpredimg = deepcopy(self.fullinputimg)\n\n        try:\n            self.gridrefactorobjs()\n        except:\n            None\n\n\nclass FullTask:\n    trainsglimgprs = []    # list of single image pairs for train set\n    testinputimg = []      # list of single image pairs for test set\n    testpred = None        # list containing a numpy array for a final prediction, 1 array for each input in test\n\n    def seperatinglinerefactorobjs(self):\n        \"\"\"looks for line(s) which run through the entire input, separating the input into equal sized smaller portions,\n        which also equal the size of the output image, suggesting some combo of those input objs to be had. Need to\n        do this on a fulltask scale\n        \"\"\"\n        for imgpair in self.trainsglimgprs:\n            if imgpair.gridapplied:  # the grid obj structure messes with this one\n                del imgpair.gridapplied\n                return\n            else:\n                del imgpair.gridapplied\n\n            stillval = 0\n\n            # look for lines\n            linesx = []\n            linesy = []\n            horzorvert = []\n            linecol = []\n            for obj in imgpair.predoutputsamecolobjs:\n                if (obj.height == imgpair.fullinputimg.shape[0]) & (obj.width == 1):\n                    # vert line\n                    linesx = linesx + [obj.positionabsx]\n                    linesy = linesy + [obj.positionabsy]\n                    linecol = linecol + [obj.colour]\n                    horzorvert = horzorvert + ['vert']\n\n                if (obj.width == imgpair.fullinputimg.shape[1]) & (obj.height == 1):\n                    # horz line\n                    linesx = linesx + [obj.positionabsx]\n                    linesy = linesy + [obj.positionabsy]\n                    linecol = linecol + [obj.colour]\n                    horzorvert = horzorvert + ['horz']\n\n            if len(horzorvert) > 0:  # there are lines that run through the whole img\n                # find the objects created by the seperating lines\n                if horzorvert.count(horzorvert[0]) == len(horzorvert):\n                    # all the same vals: either all vert or all horz\n                    linesx = linesx + [imgpair.fullpredimg.shape[1]]\n                    linesy = linesy + [imgpair.fullpredimg.shape[0]]\n                    subimgs = []\n                    subimgspositions = []\n                    colsinobjs = []\n                    if horzorvert[0] == 'vert':\n                        startx = 0\n                        # see if all objs make by splitting full img up are the same\n                        for xno in linesx:\n                            subimgs = subimgs + [imgpair.fullpredimg[:, startx:xno]]\n                            subimgspositions = subimgspositions + [(0, imgpair.fullpredimg.shape[0], startx, xno)]\n                            colsinobjs = colsinobjs + list(np.unique(subimgs))\n                            startx = xno + 1\n\n                    else:\n                        starty = 0\n                        # see if all objs make by splitting full img up are the same\n                        for yno in linesy:\n                            subimgs = subimgs + [imgpair.fullpredimg[starty:yno, :]]\n                            subimgspositions = subimgspositions + [(starty, yno, 0, imgpair.fullpredimg.shape[1])]\n                            colsinobjs = colsinobjs + list(np.unique(subimgs))\n                            starty = yno + 1\n\n                else:\n                    # combo of vert & horz lines\n                    vertlines = np.array(horzorvert) == 'vert'\n                    linesy = np.array(linesy)\n                    linesx = np.array(linesx)\n                    vertlinesx = np.append(linesx[vertlines], imgpair.fullpredimg.shape[0])\n                    horzlinesy = np.append(linesy[vertlines == False], imgpair.fullpredimg.shape[1])\n                    startx, starty = 0, 0\n                    for vlines in vertlinesx:\n                        for hlines in horzlinesy:\n                            subimgs = subimgs + [imgpair.fullpredimg[starty:hlines, startx:vlines]]  # last one\n                            subimgspositions = subimgspositions + [(starty, hlines, startx, vlines)]\n                            colsinobjs = colsinobjs + list(np.unique(subimgs))\n                            starty = hlines + 1\n                        starty = 0\n                        startx = vlines + 1\n\n                # see if the objects can be used for some sort of comparison\n                backgroundcol = max(set(colsinobjs), key=colsinobjs.count)\n\n                stillval = 1\n                multicolour = 0\n                ipbackcanvas = np.zeros([imgpair.fullpredimg.shape[0], imgpair.fullpredimg.shape[1]])\n                for objno, eachobj in enumerate(subimgs):\n                    if not ((eachobj.shape[0] == imgpair.fulloutputimg.shape[0]) &\n                            (eachobj.shape[1] == imgpair.fulloutputimg.shape[1])):\n                        stillval = 0\n\n                    if len(np.unique(eachobj)) > 2:\n                        multicolour = 1\n                    else:\n                        cols = np.unique(imgpair.fulloutputimg)\n                        col = np.delete(cols, np.argwhere(cols == backgroundcol))\n                        imgpair.outputsamecolourobjs = [SameColourObject((imgpair.fulloutputimg == col)*1, col[0])]\n                        newlabel = deepcopy(ipbackcanvas)\n                        poss = subimgspositions[objno]\n                        newlabel[poss[0]:poss[1], poss[2]:poss[3]] = eachobj\n                        cols = np.unique(eachobj)\n                        col = np.delete(cols, np.argwhere(cols == backgroundcol))\n                        subimgs[objno] = SameColourObject(newlabel, col[0])\n\n                if stillval & multicolour:\n                    imgpair.predoutputmulticolobjs = subimgs\n                elif stillval & (not multicolour):\n                    imgpair.predoutputsamecolobjs = subimgs\n\n        if stillval:\n            for imgpair in self.testinputimg:\n                ipbackcanvas = np.zeros([imgpair.fullpredimg.shape[0], imgpair.fullpredimg.shape[1]])\n                subimgstest = deepcopy(subimgs)\n                for objno, poss in enumerate(subimgspositions):\n                    eachobj = imgpair.fullpredimg[poss[0]:poss[1], poss[2]:poss[3]]\n                    newlabel = deepcopy(ipbackcanvas)\n                    newlabel[poss[0]:poss[1], poss[2]:poss[3]] = eachobj\n                    cols = np.unique(eachobj)\n                    col = np.delete(cols, np.argwhere(cols == backgroundcol))\n                    subimgstest[objno] = SameColourObject(newlabel, col[0])\n\n                if stillval & multicolour:\n                    imgpair.predoutputmulticolobjs = subimgstest\n                elif stillval & (not multicolour):\n                    imgpair.predoutputsamecolobjs = subimgstest\n                    print('seperating by lines, samecolobj, succeeded')\n\n    def createoutputcanvas(self):\n        \"\"\"creates the 'canvas' for the test output: i.e. size of output & background col\n        \"\"\"\n        # see if the output image is a certain scale \/ size relative to the input\n        stillvalid = 1\n        for transpose in [0, 1]:\n            rm = 0\n            for imgpair in self.trainsglimgprs:\n                outrow = imgpair.fulloutputimg.shape[0]\n                outcol = imgpair.fulloutputimg.shape[1]\n                inrow = imgpair.fullinputimg.shape[transpose]\n                incol = imgpair.fullinputimg.shape[1 - transpose]\n\n                if rm == 0:  # this is the first image\n                    rm = outrow \/\/ inrow\n                    rc = outrow % inrow\n                    cm = outcol \/\/ incol\n                    cc = outcol % incol\n                else:\n                    if not (((inrow * rm + rc) == outrow) & ((incol * cm + cc) == outcol)):\n                        stillvalid = 0\n\n            if stillvalid:\n                for trainortest in [self.trainsglimgprs, self.testinputimg]:\n                    for eachtask in trainortest:\n                        if eachtask.backgroundcol is None:\n                            # set to 0\n                            eachtask.backgroundcol = 0\n\n                        inrow = eachtask.fullinputimg.shape[transpose]\n                        incol = eachtask.fullinputimg.shape[1 - transpose]\n                        eachtask.predoutputcanvas = \\\n                            np.ones([int(inrow * rm + rc), int(incol * cm + cc)]) * eachtask.backgroundcol\n                return\n\n        # see if it's a fixed size:\n        stillvalid = 1\n        for ii, imgpair in enumerate(self.trainsglimgprs):\n            if ii == 0:\n                outputshape = imgpair.fulloutputimg.shape\n            else:\n                if imgpair.fulloutputimg.shape != outputshape:\n                    stillvalid = 0\n\n        if stillvalid:\n            print('refactored by seperating by line')\n            for trainortest in [self.trainsglimgprs, self.testinputimg]:\n                for eachtask in trainortest:\n                    if eachtask.backgroundcol is None:\n                        # set to 0\n                        eachtask.backgroundcol = 0\n\n                        eachtask.predoutputcanvas = np.ones([outputshape[0], outputshape[1]])\n\n            return\n\n    def findtestbackground(self):\n        backgroundcols = []\n        # make a list of background cols for all train sets\n        for trainimgpair in self.trainsglimgprs:\n            backgroundcols = backgroundcols + [trainimgpair.backgroundcol]\n\n        # if the background is the same in all sets, assign this to the test\n        if len(backgroundcols) == backgroundcols.count(backgroundcols[0]):\n            return backgroundcols[0]\n        else:\n            return None\n\n    def __init__(self, task_file):\n        import json\n\n        if isinstance(task_file, str):\n            with open(task_file, 'r') as f:\n                task = json.load(f)  # tasks is a dict\n        else:  # assume we've entered the task from alltasks: dict\n            task = task_file\n\n        trainset = task['train']  # trainset is a list\n        for ii in range(len(trainset)):\n            ntpis = SingleImagePair(trainset[ii], 'train')\n            self.trainsglimgprs = self.trainsglimgprs + [ntpis]\n\n        testset = task['test']  # testnset is a list\n        for ii in range(len(testset)):\n            backgroundcol = self.findtestbackground()\n            ntpis = SingleImagePair(testset[ii], 'test', backgroundcol=backgroundcol)\n\n            self.testinputimg = self.testinputimg + [ntpis]\n\n            self.createoutputcanvas()\n\n            try:\n                self.seperatinglinerefactorobjs()\n            except:\n                print('seperating objs by line failed')\n\n\nclass FullTaskFromClass(FullTask):\n    def __init__(self, fulltask):\n        # need to pack back into tips\n        for imgpair in fulltask.trainsglimgprs:\n            trainset = {'input': imgpair.fullpredimg.astype(int), 'output': imgpair.fulloutputimg}\n            ntpis = SingleImagePair(trainset, 'train')\n            ntpis.fullinputimg = imgpair.fullinputimg\n            self.trainsglimgprs = self.trainsglimgprs + [ntpis]\n\n        for imgpair in fulltask.testinputimg:\n            backgroundcol = self.findtestbackground()\n            testset = {'input': imgpair.fullpredimg.astype(int), 'output': imgpair.fulloutputimg}\n            ntpis = SingleImagePair(testset, 'test', backgroundcol=backgroundcol)\n            ntpis.fullinputimg = imgpair.fullinputimg\n\n            self.testinputimg = self.testinputimg + [ntpis]\n\n            self.createoutputcanvas()\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~ PATTERN CLASSES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nclass SinglePatImagePair(SingleImagePair):\n    def __init__(self, tip, test_pred_one=0):\n        # input - the un-processed input pattern\n        self.fullinputimg = np.array(tip[\"input\"])\n\n        # output - the processed input\n        if len(test_pred_one) != 1:   # got a testset\n            self.fulloutputimg = test_pred_one\n\n\nclass FullPatTask(FullTask):\n    def __init__(self, task_file, test_pred_list):\n        task = task_file\n\n        trainset = task['train']  # trainset is a list\n        for ii in range(len(trainset)):\n            ntpis = SinglePatImagePair(trainset[ii])\n            self.trainsglimgprs = self.trainsglimgprs + [ntpis]\n\n        testset = task['test']  # testnset is a list\n        for ii in range(len(testset)):\n            backgroundcol = self.findtestbackground()\n            ntpis = SinglePatImagePair(testset[ii], test_pred_one=test_pred_list[ii])\n\n            self.testinputimg = self.testinputimg + [ntpis]","9b547832":"#  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ PAT DETECT  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\ndef initialisedataset():\n    \"\"\"This creates the X & Y dataset that we'll use to train the CNN on whether the task is pattern related or not.\n    X - 2 channel image, first channel is the input picture. Second channel is the output picutre\n    Y - List of booleans stating whether the problem is pattern related or not (this was manually labelled)\"\"\"\n    import initialdefs\n    import math\n\n    task_file, alltasks = initialdefs.starup()\n\n    X = np.array([[np.zeros([32, 32]), np.zeros([32, 32])]])\n    Y = [0]\n\n    # make prelim Y's - labels for which problems are patterns. Prelim because we'll make more samples from each problem\n    # so we'll only use these to inform us what label we should use\n    Yprelim = [0] * 400\n\n    # from manually going through and seeing what tasks were filling in repeating patterns \/ mosaics\n    for i in [16, 60, 73, 109, 241, 286, 304, 312, 350, 399]:\n        Yprelim[i] = 1\n\n    for taskno in range(len(alltasks)):\n        print(taskno)\n        task = alltasks[taskno]\n        train = task['train']\n\n        # check the input & output are the same size: if not, don't use (too different, would cause too many problems)\n        check = train[0]\n        checkinput = np.array(check['input'])\n        checkoutput = np.array(check['output'])\n\n        # if they are the same, we can use as sample for the model.\n        if checkoutput.shape == checkinput.shape:\n            for trainno in range(len(train)):\n                # dim0: samples dim1: channels (2: input, out), dim3: x dim4: y\n                imagepair = train[trainno]\n                imageinput = imagepair['input']\n                imageoutput = imagepair['output']\n                sz0l = math.floor((32 - np.size(imageinput, 0))\/2)  # padding for the left of dimension 0\n                sz0r = math.ceil((32 - np.size(imageinput, 0))\/2)  # padding for the right of dimension 0\n                sz1l = math.floor((32 - np.size(imageinput, 1))\/2)  # padding for the left of dimension 1\n                sz1r = math.ceil((32 - np.size(imageinput, 1))\/2)  # padding for the right of dimension 1\n                ippad = np.pad(imageinput, ((sz0l, sz0r), (sz1l, sz1r)), constant_values=(0, 0))\n                oppad = np.pad(imageoutput, ((sz0l, sz0r), (sz1l, sz1r)), constant_values=(0, 0))\n\n                newsample = np.array([[ippad, oppad]])\n\n                X = np.concatenate((X, newsample), axis=0)\n                Y.append(Yprelim[taskno])\n\n                # create more images from the rotated versions\n                for i in range(3):\n                    ippad = np.rot90(ippad)\n                    oppad = np.rot90(oppad)\n\n                    newsample = np.array([[ippad, oppad]])\n\n                    X = np.concatenate((X, newsample), axis=0)\n                    Y.append(Yprelim[taskno])\n\n                # create more images from the transposed & rotated versions\n                ippad = ippad.T\n                oppad = oppad.T\n\n                newsample = np.array([[ippad, oppad]])\n\n                X = np.concatenate((X, newsample), axis=0)\n                Y.append(Yprelim[taskno])\n\n                for i in range(3):\n                    ippad = np.rot90(ippad)\n                    oppad = np.rot90(oppad)\n\n                    newsample = np.array([[ippad, oppad]])\n\n                    X = np.concatenate((X, newsample), axis=0)\n                    Y.append(Yprelim[taskno])\n\n    X = np.delete(X, 0, axis=0)\n    Y.__delitem__(0)\n\n    #  make channel the last dim\n    X = np.moveaxis(X, 1, -1)\n\n    return X, Y\n\n\ndef modelbuildtrain(X, Y):\n    from keras.models import Sequential\n    from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n\n    buildcomplex = True\n\n    if buildcomplex:\n        #  build model - complex\n        model = Sequential()\n        model.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n                         activation='relu',\n                         input_shape=(32, 32, 2)))\n        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n        model.add(Conv2D(64, (5, 5), activation='relu'))\n        model.add(MaxPooling2D(pool_size=(2, 2)))\n        model.add(Flatten())\n        model.add(Dense(1000, activation='relu'))\n        model.add(Dense(1, activation='sigmoid'))\n    else:\n        # Build the model - simple\n        model = Sequential([\n            Conv2D(8, 3, input_shape=(32, 32, 2)),\n            MaxPooling2D(pool_size=2),\n            Flatten(),\n            Dense(1, activation='sigmoid'),\n        ])\n\n    model.compile(\n        'adam',\n        loss='binary_crossentropy',\n        metrics=['accuracy'],\n    )\n\n    # make data\n    Xtrain = X[0:5500, :, :, :]\n    Xtest = X[5501:, :, :, :]\n    Ytrain = Y[0:5500]\n    Ytest = Y[5501:]\n\n    # Train the model.\n    model.fit(\n        Xtrain,\n        Ytrain,\n        epochs=3,\n        validation_data=(Xtest, Ytest)\n    )\n\n    return model\n\n\ndef preparetaskformodel(task):\n    \"\"\"takes one image pair and transforms it into the correct format to be presented to model\n    imagepair --    an image pair example from a task\n    newsample --    a sample to be presented to the model\n    \"\"\"\n    import math\n\n    tasktrain = task['train']\n    imagepair = tasktrain[0]\n\n    imageinput = np.array(imagepair['input'])\n    imageoutput = np.array(imagepair['output'])\n\n    #  check that these are the same size\n    if not imageinput.shape == imageoutput.shape:\n        #  print('Input and output not the same size so we know its not pattern')\n        return 0\n\n    sz0l = math.floor((32 - np.size(imageinput, 0)) \/ 2)  # padding for the left of dimension 0\n    sz0r = math.ceil((32 - np.size(imageinput, 0)) \/ 2)  # padding for the right of dimension 0\n    sz1l = math.floor((32 - np.size(imageinput, 1)) \/ 2)  # padding for the left of dimension 1\n    sz1r = math.ceil((32 - np.size(imageinput, 1)) \/ 2)  # padding for the right of dimension 1\n    ippad = np.pad(imageinput, ((sz0l, sz0r), (sz1l, sz1r)), constant_values=(0, 0))\n    oppad = np.pad(imageoutput, ((sz0l, sz0r), (sz1l, sz1r)), constant_values=(0, 0))\n\n    newsample = np.array([[ippad, oppad]])\n\n    #  make channel the last dim\n    newsample = np.moveaxis(newsample, 1, -1)\n\n    return newsample\n\n\n# def modelpresrecall(model, X, Y):\n#     \"\"\"gives metrics on the precision and recall of our model designed to label mosaic\/symmetry tasks\n#     \"\"\"\n#     from sklearn.metrics import classification_report\n#\n#     #  get precision & recall\n#     y_pred = model.predict(X, batch_size=64, verbose=1)\n#     y_pred_bool = np.argmax(y_pred, axis=1)\n#\n#     print(classification_report(Y, y_pred_bool))\n\n\ndef makepredictions(task, model):\n    newsample = preparetaskformodel(task)\n\n    if newsample is 0:\n        return False\n    else:\n        prediction = float(model.predict(newsample))\n        return prediction > 0.5\n\n\ndef checktranssymmetry(image, repunit):\n    \"\"\"Once a translational repeating unit has been created, this checks whether the repeating unit can be used\n    to describe the whole image\n    image   --      output image\n    repunit --      repeating unit\n    return  --      boolean whether repunit creates full pattern or not\n    \"\"\"\n    # raster-scan in any possible increments for repeating unit\n    for rasterrow in range(1, np.size(repunit, axis=0)):\n        for rastercol in range(1, np.size(repunit, axis=1)):\n            newrepunit = image[0:rasterrow, 0:rastercol]\n\n            if checktranssymmetry(image, newrepunit):\n                #  found it!\n                foundsol = 1\n                return foundsol, newrepunit\n\n    return 1\n\n\ndef findtranssymmetries(imageoutput):\n    \"\"\"There may be a repeating unit which is translated (raster scanned) across the image. This finds that symmetry\n    task    --      full task\n    return:\n    testout --      output for the test pattern\n    cache   --      parameters for how the task was solved\n    \"\"\"\n\n    foundsol = 0\n\n    # create a repeating pattern\n    for reprow in range(2, np.size(imageoutput, axis=0) \/ 2):\n        for repcol in range(2, np.size(imageoutput, axis=1) \/ 2):\n            newrepunit = imageoutput[0:reprow, 0:repcol]\n\n            if checktranssymmetry(imageoutput, newrepunit):\n                #  found it!\n                foundsol = 1\n                return foundsol, newrepunit\n\n    return foundsol, newrepunit\n\n\ndef findrotsymmetries(imageoutput):\n        \"\"\"Othe type of possible symmetry is rotational symmetry. This finds any rotational symmetry\n    task    --      full task\n    return:\n    testout --      output for the test pattern\n    cache   --      parameters for how the task was solved\n    \"\"\"\n\n\ndef findsymmetries(task):\n    \"\"\"Once a task has been ascertained as a pattern task, this is how to solve it\n    task    --      full task\n    return:\n    testout --      output for the test pattern\n    cache   --      parameters for how the task was solved\n    \"\"\"\n\n    import numpy as np\n\n    tasktrain = task['train']\n    imagepair = tasktrain[0]\n\n    imageinput = np.array(imagepair['input'])\n    imageoutput = np.array(imagepair['output'])\n\n    # find translational symmetries\n    foundsol, newrepunit = findtranssymmetries(imageoutput)\n\n    #  find rotational symmetries if no translational symmetries are present\n    if foundsol is not 1:\n        foundsol, newrepunit = findrotsymmetries(imageoutput)\n\n    #  if a pattern has been found, see if it's the same for the others in the set\n\n\n#  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ FROM KAGGLE ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ FROM KAGGLE ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ FROM KAGGLE ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ FROM KAGGLE ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ FROM KAGGLE ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ FROM KAGGLE ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nfrom skimage import measure\nfrom matplotlib import colors\nfrom numpy.lib.stride_tricks import as_strided\n\n\ndef in_out_diff(t_in, t_out):\n    x_in, y_in = t_in.shape\n    x_out, y_out = t_out.shape\n    diff = np.zeros((max(x_in, x_out), max(y_in, y_out)))\n    diff[:x_in, :y_in] -= t_in\n    diff[:x_out, :y_out] += t_out\n    return diff\n\n\ndef check_symmetric(a):\n    try:\n        sym = 1\n        if np.array_equal(a, a.T):\n            sym *= 2  # Check main diagonal symmetric (top left to bottom right)\n        if np.array_equal(a, np.flip(a).T):\n            sym *= 3  # Check antidiagonal symmetric (top right to bottom left)\n        if np.array_equal(a, np.flipud(a)):\n            sym *= 5  # Check horizontal symmetric of array\n        if np.array_equal(a, np.fliplr(a)):\n            sym *= 7  # Check vertical symmetric of array\n        return sym\n    except:\n        return 0\n\n\ndef bbox(a):\n    try:\n        r = np.any(a, axis=1)\n        c = np.any(a, axis=0)\n        rmin, rmax = np.where(r)[0][[0, -1]]\n        cmin, cmax = np.where(c)[0][[0, -1]]\n        return rmin, rmax, cmin, cmax\n    except:\n        return 0,a.shape[0],0,a.shape[1]\n\n\ndef cmask(t_in):\n    cmin = 999\n    cm = 0\n    for c in range(10):\n        t = t_in.copy().astype('int8')\n        t[t==c],t[t>0],t[t<0]=-1,0,1\n        b = bbox(t)\n        a = (b[1]-b[0])*(b[3]-b[2])\n        s = (t[b[0]:b[1],b[2]:b[3]]).sum()\n        if a>2 and a<cmin and s==a:\n            cmin=a\n            cm=c\n    return cm\n\n\ndef mask_rect(a):\n    r,c = a.shape\n    m = a.copy().astype('uint8')\n    for i in range(r-1):\n        for j in range(c-1):\n            if m[i,j]==m[i+1,j]==m[i,j+1]==m[i+1,j+1]>=1:m[i,j]=2\n            if m[i,j]==m[i+1,j]==1 and m[i,j-1]==2:m[i,j]=2\n            if m[i,j]==m[i,j+1]==1 and m[i-1,j]==2:m[i,j]=2\n            if m[i,j]==1 and m[i-1,j]==m[i,j-1]==2:m[i,j]=2\n    m[m==1]=0\n    return (m==2)\n\n\ndef crop_min(t_in):\n    try:\n        b = np.bincount(t_in.flatten(),minlength=10)\n        c = int(np.where(b==np.min(b[np.nonzero(b)]))[0])\n        coords = np.argwhere(t_in==c)\n        x_min, y_min = coords.min(axis=0)\n        x_max, y_max = coords.max(axis=0)\n        return t_in[x_min:x_max+1, y_min:y_max+1]\n    except:\n        return t_in\n\n\ndef call_pred_train(t_in, t_out, pred_func):\n    import inspect\n\n    feat = {}\n    feat['s_out'] = t_out.shape\n    if t_out.shape==t_in.shape:\n        diff = in_out_diff(t_in,t_out)\n        feat['diff'] = diff\n        feat['cm'] = t_in[diff!=0].max()\n    else:\n        feat['diff'] = (t_in.shape[0]-t_out.shape[0],t_in.shape[1]-t_out.shape[1])\n        feat['cm'] = cmask(t_in)\n    feat['sym'] = check_symmetric(t_out)\n    args = inspect.getargspec(pred_func).args\n    if len(args)==1:\n        return pred_func(t_in)\n    elif len(args)==2:\n        t_pred = pred_func(t_in,feat[args[1]])\n    elif len(args)==3:\n        t_pred = pred_func(t_in,feat[args[1]],feat[args[2]])\n    feat['sizeok'] = len(t_out)==len(t_pred)\n    t_pred = np.resize(t_pred,t_out.shape)\n    acc = (t_pred==t_out).sum()\/t_out.size\n    return t_pred, feat, acc\n\n\ndef call_pred_test(t_in, pred_func, feat):\n    import inspect\n\n    args = inspect.getargspec(pred_func).args\n    if len(args)==1:\n        return pred_func(t_in)\n    elif len(args)==2:\n        t_pred = pred_func(t_in,feat[args[1]])\n    elif len(args)==3:\n        t_pred = pred_func(t_in,feat[args[1]],feat[args[2]])\n    return t_pred\n\n\n# from: https:\/\/www.kaggle.com\/nagiss\/manual-coding-for-the-first-10-tasks\ndef get_data(task_filename):\n    import json\n    with open(task_filename, 'r') as f:\n        task = json.load(f)\n    return task\n\n# from: https:\/\/www.kaggle.com\/boliu0\/visualizing-all-task-pairs-with-gridlines\ncmap = colors.ListedColormap(\n    ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n     '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\nnorm = colors.Normalize(vmin=0, vmax=9)\n\nnum2color = [\"black\", \"blue\", \"red\", \"green\", \"yellow\", \"gray\", \"magenta\", \"orange\", \"sky\", \"brown\"]\ncolor2num = {c: n for n, c in enumerate(num2color)}\n\n\ndef plot_one(ax, input_matrix, title_text):\n    ax.imshow(input_matrix, cmap=cmap, norm=norm)\n    ax.grid(True,which='both',color='lightgrey', linewidth=0.5)\n    ax.set_yticks([x-0.5 for x in range(1+len(input_matrix))])\n    ax.set_xticks([x-0.5 for x in range(1+len(input_matrix[0]))])\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    ax.set_title(title_text)\n\n\ndef check_p(task, pred_func):\n    import matplotlib.pyplot as plt\n\n    n = len(task[\"train\"]) + len(task[\"test\"])\n    fig, axs = plt.subplots(3, n, figsize=(4*n,12), dpi=50)\n    plt.subplots_adjust(wspace=0.3, hspace=0.3)\n    fnum = 0\n    t_acc = 0\n    t_pred_test_list = []\n    for i, t in enumerate(task[\"train\"]):\n        t_in, t_out = np.array(t[\"input\"]).astype('uint8'), np.array(t[\"output\"]).astype('uint8')\n        t_pred, feat, acc = call_pred_train(t_in, t_out, pred_func)\n        plot_one(axs[0,fnum],t_in,f'train-{i} input')\n        plot_one(axs[1,fnum],t_out,f'train-{i} output')\n        plot_one(axs[2,fnum],t_pred,f'train-{i} pred')\n        t_acc+=acc\n        fnum += 1\n    for i, t in enumerate(task[\"test\"]):\n        # removed t_out as there should be no output in the test\n        t_in= np.array(t[\"input\"]).astype('uint8')\n        t_pred_test = call_pred_test(t_in, pred_func, feat)\n        plot_one(axs[0,fnum],t_in,f'test-{i} input')\n        #plot_one(axs[1,fnum],t_out,f'test-{i} output')\n        plot_one(axs[2,fnum],t_pred_test,f'test-{i} pred')\n        t_pred = np.resize(t_pred,t_in.shape) # assume same shape. used to be: t_pred = np.resize(t_pred,t_out.shape)\n        # if len(t_out)==1:\n        #     acc = int(t_pred==t_out)\n        # else:\n        #     acc = (t_pred==t_out).sum()\/t_out.size\n        # t_acc += acc\n        # fnum += 1\n        t_pred_test_list.append(t_pred_test)\n    # plt.show()\n    return t_acc\/fnum, t_pred_test_list\n\n\ndef get_tile(img ,mask):\n    try:\n        m,n = img.shape\n        a = img.copy().astype('int8')\n        a[mask] = -1\n        r=c=0\n        for x in range(n):\n            if np.count_nonzero(a[0:m,x]<0):continue\n            for r in range(2,m):\n                if 2*r<m and (a[0:r,x]==a[r:2*r,x]).all():break\n            if r<m:break\n            else: r=0\n        for y in range(m):\n            if np.count_nonzero(a[y,0:n]<0):continue\n            for c in range(2,n):\n                if 2*c<n and (a[y,0:c]==a[y,c:2*c]).all():break\n            if c<n:break\n            else: c=0\n        if c>0:\n            for x in range(n-c):\n                if np.count_nonzero(a[:,x]<0)==0:\n                    a[:,x+c]=a[:,x]\n                elif np.count_nonzero(a[:,x+c]<0)==0:\n                    a[:,x]=a[:,x+c]\n        if r>0:\n            for y in range(m-r):\n                if np.count_nonzero(a[y,:]<0)==0:\n                    a[y+r,:]=a[y,:]\n                elif np.count_nonzero(a[y+r,:]<0)==0:\n                    a[y,:]=a[y+r,:]\n        return a[r:2*r,c:2*c]\n    except:\n        return a[0:1,0:1]\n\n\ndef patch_image(t_in,s_out,cm=0):\n    try:\n        t = t_in.copy()\n        ty,tx=t.shape\n        if cm>0:\n            m = mask_rect(t==cm)\n        else:\n            m = (t==cm)\n        tile = get_tile(t ,m)\n        if tile.size>2 and s_out==t.shape:\n            rt = np.tile(tile,(1+ty\/\/tile.shape[0],1+tx\/\/tile.shape[1]))[0:ty,0:tx]\n            if (rt[~m]==t[~m]).all():\n                return rt\n        for i in range(6):\n            m = (t==cm)\n            t -= cm\n            if tx==ty:\n                a = np.maximum(t,t.T)\n                if (a[~m]==t[~m]).all():t=a.copy()\n                a = np.maximum(t,np.flip(t).T)\n                if (a[~m]==t[~m]).all():t=a.copy()\n            a = np.maximum(t,np.flipud(t))\n            if (a[~m]==t[~m]).all():t=a.copy()\n            a = np.maximum(t,np.fliplr(t))\n            if (a[~m]==t[~m]).all():t=a.copy()\n            t += cm\n            m = (t==cm)\n            lms = measure.label(m.astype('uint8'))\n            for l in range(1,lms.max()+1):\n                lm = np.argwhere(lms==l)\n                lm = np.argwhere(lms==l)\n                x_min = max(0,lm[:,1].min()-1)\n                x_max = min(lm[:,1].max()+2,t.shape[0])\n                y_min = max(0,lm[:,0].min()-1)\n                y_max = min(lm[:,0].max()+2,t.shape[1])\n                gap = t[y_min:y_max,x_min:x_max]\n                sy,sx=gap.shape\n                if i==1:\n                    sy\/\/=2\n                    y_max=y_min+sx\n                gap = t[y_min:y_max,x_min:x_max]\n                sy,sx=gap.shape\n                allst = as_strided(t, shape=(ty,tx,sy,sx),strides=2*t.strides)\n                allst = allst.reshape(-1,sy,sx)\n                allst = np.array([a for a in allst if np.count_nonzero(a==cm)==0])\n                gm = (gap!=cm)\n                for a in allst:\n                    if sx==sy:\n                        fpd = a.T\n                        fad = np.flip(a).T\n                        if i==1:gm[sy-1,0]=gm[0,sx-1]=False\n                        if (fpd[gm]==gap[gm]).all():\n                            gm = (gap!=cm)\n                            np.putmask(gap,~gm,fpd)\n                            t[y_min:y_max,x_min:x_max] = gap\n                            break\n                        if i==1:gm[0,0]=gm[sy-1,sx-1]=False\n                        if (fad[gm]==gap[gm]).all():\n                            gm = (gap!=cm)\n                            np.putmask(gap,~gm,fad)\n                            t[y_min:y_max,x_min:x_max] = gap\n                            break\n                    fud = np.flipud(a)\n                    flr = np.fliplr(a)\n                    if i==1:gm[sy-1,0]=gm[0,sx-1]=gm[0,0]=gm[sy-1,sx-1]=False\n                    if (a[gm]==gap[gm]).all():\n                        gm = (gap!=cm)\n                        np.putmask(gap,~gm,a)\n                        t[y_min:y_max,x_min:x_max] = gap\n                        break\n                    elif (fud[gm]==gap[gm]).all():\n                        gm = (gap!=cm)\n                        np.putmask(gap,~gm,fud)\n                        t[y_min:y_max,x_min:x_max] = gap\n                        break\n                    elif (flr[gm]==gap[gm]).all():\n                        gm = (gap!=cm)\n                        np.putmask(gap,~gm,flr)\n                        t[y_min:y_max,x_min:x_max] = gap\n                        break\n        if s_out==t.shape:\n            return t\n        else:\n            m = (t_in==cm)\n            return np.resize(t[m],crop_min(m).shape)\n    except:\n        return np.resize(t_in, s_out)\n    \n #  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ MISC FROM KAGGLE~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~   \n\ndef Defensive_Copy(A):\n    n = len(A)\n    k = len(A[0])\n    L = np.zeros((n, k), dtype=int)\n    for i in range(n):\n        for j in range(k):\n            L[i, j] = 0 + A[i][j]\n    return L.tolist()\n\n\ndef Create(task, task_id=0):\n    n = len(task['train'])\n    Input = [Defensive_Copy(task['train'][i]['input']) for i in range(n)]\n    Output = [Defensive_Copy(task['train'][i]['output']) for i in range(n)]\n    Input.append(Defensive_Copy(task['test'][task_id]['input']))\n    return Input, Output\n\n\ndef Recolor(task):\n    Input = task[0]\n    Output = task[1]\n    Test_Picture = Input[-1]\n    Input = Input[:-1]\n    N = len(Input)\n\n    for x, y in zip(Input, Output):\n        if len(x) != len(y) or len(x[0]) != len(y[0]):\n            return -1\n\n    Best_Dict = -1\n    Best_Q1 = -1\n    Best_Q2 = -1\n    Best_v = -1\n    # v ranges from 0 to 3. This gives an extra flexibility of measuring distance from any of the 4 corners\n    Pairs = []\n    for t in range(15):\n        for Q1 in range(1, 8):\n            for Q2 in range(1, 8):\n                if Q1 + Q2 == t:\n                    Pairs.append((Q1, Q2))\n\n    for Q1, Q2 in Pairs:\n        for v in range(4):\n\n            if Best_Dict != -1:\n                continue\n            possible = True\n            Dict = {}\n\n            for x, y in zip(Input, Output):\n                n = len(x)\n                k = len(x[0])\n                for i in range(n):\n                    for j in range(k):\n                        if v == 0 or v == 2:\n                            p1 = i % Q1\n                        else:\n                            p1 = (n - 1 - i) % Q1\n                        if v == 0 or v == 3:\n                            p2 = j % Q2\n                        else:\n                            p2 = (k - 1 - j) % Q2\n                        color1 = x[i][j]\n                        color2 = y[i][j]\n                        if color1 != color2:\n                            rule = (p1, p2, color1)\n                            if rule not in Dict:\n                                Dict[rule] = color2\n                            elif Dict[rule] != color2:\n                                possible = False\n            if possible:\n\n                # Let's see if we actually solve the problem\n                for x, y in zip(Input, Output):\n                    n = len(x)\n                    k = len(x[0])\n                    for i in range(n):\n                        for j in range(k):\n                            if v == 0 or v == 2:\n                                p1 = i % Q1\n                            else:\n                                p1 = (n - 1 - i) % Q1\n                            if v == 0 or v == 3:\n                                p2 = j % Q2\n                            else:\n                                p2 = (k - 1 - j) % Q2\n\n                            color1 = x[i][j]\n                            rule = (p1, p2, color1)\n\n                            if rule in Dict:\n                                color2 = 0 + Dict[rule]\n                            else:\n                                color2 = 0 + y[i][j]\n                            if color2 != y[i][j]:\n                                possible = False\n                if possible:\n                    Best_Dict = Dict\n                    Best_Q1 = Q1\n                    Best_Q2 = Q2\n                    Best_v = v\n\n    if Best_Dict == -1:\n        return -1  # meaning that we didn't find a rule that works for the traning cases\n\n    # Otherwise there is a rule: so let's use it:\n    n = len(Test_Picture)\n    k = len(Test_Picture[0])\n\n    answer = np.zeros((n, k), dtype=int)\n\n    for i in range(n):\n        for j in range(k):\n            if Best_v == 0 or Best_v == 2:\n                p1 = i % Best_Q1\n            else:\n                p1 = (n - 1 - i) % Best_Q1\n            if Best_v == 0 or Best_v == 3:\n                p2 = j % Best_Q2\n            else:\n                p2 = (k - 1 - j) % Best_Q2\n\n            color1 = Test_Picture[i][j]\n            rule = (p1, p2, color1)\n            if (p1, p2, color1) in Best_Dict:\n                answer[i][j] = 0 + Best_Dict[rule]\n            else:\n                answer[i][j] = 0 + color1\n\n    return answer.tolist()\n\n\ndef toplevel1(task):\n    Function = Recolor\n\n    basic_task = Create(task, 0)\n    a = Function(basic_task)\n\n    return a        ","67200ca5":"#  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ARCRULES ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n#  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nimport numpy as np\nfrom copy import deepcopy\nimport datetime\nimport time\nimport xgboost as xgb\n\n# ~~~~~~~~~~~~~~~~~~~~ functions used by entry requirements ~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ~~~~~~~~~~~~~~~~~~~~ functions used by entry requirements ~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ~~~~~~~~~~~~~~~~~~~~ functions used by entry requirements ~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ~~~~~~~~~~~~~~~~~~~~ functions used by entry requirements ~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~ entry requirements ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~ entry requirements ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~ entry requirements ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~ entry requirements ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\ndef colouriner(imgpair):\n    \"\"\"if the input & output is the same other than objects being a different colour, then yes\n    imgpair is a SingleImagePair object\n    \"\"\"\n    # look for multi-colour objs against a background\n    if imgpair.backgroundcol is not None:\n        inputbw = (imgpair.fullpredimg == imgpair.backgroundcol) * 1\n        outputbw = (imgpair.fulloutputimg == imgpair.backgroundcol) * 1\n\n        if np.array_equal(inputbw, outputbw) & (not np.array_equal(imgpair.fullpredimg, imgpair.fulloutputimg)):\n            # this says that the objects are the same... but:\n            # is one object a multicolour object?\n            # is the background incorrect?\n\n            # easy win:\n            if len(imgpair.predoutputsamecolobjs) != len(imgpair.outputsamecolourobjs):\n                return 2\n\n    # look for same-colour objs\n    if len(imgpair.predoutputsamecolobjs) == len(imgpair.outputsamecolourobjs):\n        identitycount = 0\n        for objin in imgpair.outputsamecolourobjs:\n            for objout in imgpair.predoutputsamecolobjs:\n                if (np.array_equal(objin.elementarr, objout.elementarr)) & (objin.positionabsx == objout.positionabsx) \\\n                        & (objin.positionabsy == objout.positionabsy):\n                    identitycount += 1\n\n        if identitycount == len(imgpair.outputsamecolourobjs):\n            # all objects in input can be found in output in the same location\n            return 1\n        else:\n            return 2\n    else:\n        return 0\n\n\ndef zoominer(imgpair, returnsz=1):\n    \"\"\"if the output is a zoomed in version of the input, return 1\n    imgpair is a SingleImagePair object\n    \"\"\"\n    inimg = imgpair.fullinputimg\n    outimg = imgpair.fulloutputimg\n\n    # raster-scan an \"outimg\" sized image across inimg, see if any of the segments are equal to outimg\n    if (inimg.shape[0] > outimg.shape[0]) & (inimg.shape[1] > outimg.shape[1]):\n        for ii in range(inimg.shape[0] - outimg.shape[0]):\n            for jj in range(inimg.shape[1] - outimg.shape[1]):\n                rows = np.repeat(list(range(ii, ii + outimg.shape[0])), outimg.shape[1])\n                cols = list(range(jj, jj + outimg.shape[1])) * outimg.shape[0]\n                inzoom = inimg[rows, cols]\n                if np.array_equal(inzoom, outimg.flatten()):\n                    if returnsz == 1:\n                        return 1\n                    else:\n                        return 1, ii, jj\n\n    return 0\n\n\ndef zoomonobject(imgpair):\n    for obj in imgpair.inputsamecolourobjs:\n        if np.array_equal(obj.elementarr * obj.colour, imgpair.fulloutputimg):\n            return 1\n\n    return 0\n\n\ndef objremer(imgpair):\n    \"\"\"finds if an object(s) from the input is removed from the output\n    \"\"\"\n    if len(imgpair.inputsamecolourobjs) <= len(imgpair.outputsamecolourobjs):\n        return 0\n\n    if len(imgpair.inputsamecolourobjs) > 100:\n        return 0\n\n    outobjcount = [0] * len(imgpair.outputsamecolourobjs)\n    for outobjno, outobj in enumerate(imgpair.outputsamecolourobjs):\n        for inobj in imgpair.inputsamecolourobjs:\n            if np.array_equal(inobj.elementarr, outobj.elementarr):\n                outobjcount[outobjno] = 1\n\n    if len(outobjcount) == outobjcount.count(1):\n        return 1\n    else:\n        return 0\n\n\ndef listofuniqueshapes(objlist):\n    objswiththatshape = []  # list of [list for each shape: which contains obj nos associated with that shape]\n    listofshapes = []  # list of np arrays, each of which is a unique shape\n    for objno, eachobj in enumerate(objlist):\n        newshape = 1\n        for shapeno, shape in enumerate(listofshapes):  # list of all unique symbols for this task\n            if np.array_equal(eachobj.elementarr, shape):\n                newshape = 0\n                objswiththatshape[shapeno] = objswiththatshape[shapeno] + [objno]\n                break\n\n        if newshape:  # made it to the end of listofshapes, not in there, add it\n            listofshapes = listofshapes + [eachobj.elementarr]\n            objswiththatshape = objswiththatshape + [[objno]]\n\n    return {'shapes': listofshapes, 'objswithshape': objswiththatshape}\n\n\ndef symbolser(fulltask):\n    \"\"\"look for re-occuring symbols across all the tasks\n    \"\"\"\n    # question is: what constitutes a symbol, what constitutes just a normal object?\n    # very basic: let's turn everything into a symbol\n    listofsymbols = []\n    allsymbolnumbers = []  # unique number for each symbol found\n    for traintest in [fulltask.trainsglimgprs, fulltask.testinputimg]:\n        for eachtest in traintest:\n            symbolsinimg = []\n            for eachobj in eachtest.predoutputsamecolobjs:\n                stillvalid = 1\n                counter = 0\n                for symbolno, symbol in enumerate(listofsymbols):  # list of all symbols for this task\n                    if np.array_equal(eachobj.elementarr, symbol):\n                        stillvalid = 0\n                        symbolsinimg = symbolsinimg + [symbolno]\n                        break\n\n                    counter += 1\n\n                if stillvalid:  # made it to the end of listofsymbols, not in there, add it\n                    listofsymbols = listofsymbols + [eachobj.elementarr]\n\n                    # add this symbol number to symbols in this image:\n                    allsymbolnumbers = allsymbolnumbers + [counter]\n                    symbolsinimg = symbolsinimg + [counter]\n\n            # when we've gone through each object in the set, we should add these to every obj\n            for eachobj in eachtest.predoutputsamecolobjs:\n                for symbolno, eachsymbol in enumerate(symbolsinimg):\n                    setattr(eachobj, 'symbol' + str(symbolno), eachsymbol)\n\n    return fulltask\n\n\ndef booleannoter(fulltask):\n    boolnottask = 0\n    for imgpair in fulltask.trainsglimgprs:\n        toomanyobjs = len(imgpair.predoutputsamecolobjs) > 40\n        if not toomanyobjs:\n            for objpred in imgpair.predoutputsamecolobjs:\n                newelemarr = 1 - objpred.elementarr\n                objnottoosmall = newelemarr.sum() > 1\n                if objnottoosmall:\n                    # remove cols\/rows of all zeros\n                    validcols = newelemarr.sum(axis=0) > 0  # logical index col\n                    validrows = newelemarr.sum(axis=1) > 0  # logical index row\n                    firstcol = min(np.where(validcols)[0])\n                    firstrow = min(np.where(validrows)[0])\n                    lastcol = max(np.where(validcols)[0])\n                    lastrow = max(np.where(validrows)[0])\n                    validcols[firstcol:lastcol] = True\n                    validrows[firstrow:lastrow] = True\n                    newelemarr = newelemarr[np.ix_(validrows, validcols)]\n\n                    for objout in imgpair.outputsamecolourobjs:\n\n                        # if any one obj is the same, should apply a not to at least 1 of these objects\n                        if np.array_equal(newelemarr, objout.elementarr):\n                            boolnottask = 1\n\n    return boolnottask\n\n\ndef booleanlogicer(imgpair):\n    \"\"\"need only 2 input shapes and they both need to be the same size as the output img\n    \"\"\"\n    stillvalid = 1\n    if len(imgpair.predoutputsamecolobjs) != 2:\n        stillvalid = 0\n\n    for obj in imgpair.predoutputsamecolobjs:\n        if (obj.elementarr.shape[0] != imgpair.fulloutputimg.shape[0]) or \\\n                (obj.elementarr.shape[1] != imgpair.fulloutputimg.shape[1]):\n            stillvalid = 0\n\n    return stillvalid\n\n\ndef movingobjectser(imgpair):\n    \"\"\"Prelim requirements for moving objs around. check that all objs can be mapped from in to out and they're\n    in different locations at the in than they are at the out\n    \"\"\"\n    intooutobjs, warnings = linkinobjtooutobj(imgpair)\n    if len(warnings) > 0:  # we need 1:1 mapping from in:out, as in test, won't know what to map\n        return 0\n\n    inobjs = intooutobjs['inobjs']\n\n    if (len(inobjs) == len(imgpair.predoutputsamecolobjs)) and (len(inobjs) == len(imgpair.outputsamecolourobjs)):\n        return 1\n    else:\n        return 0\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~ rules to apply if passes entry requirements ~~~~~~~~~~~~~~~~~~~~\ndef accbyinputpixtooutput(fulltask):\n    \"\"\"returns a pixel-wise comparison of matching pixels, comparing input to output\n    \"\"\"\n    allaccs = []\n\n    for imgpair in fulltask.trainsglimgprs:\n        if imgpair.fullpredimg.shape == imgpair.fulloutputimg.shape:\n            # can compare on a pixel-wise basis\n            samepix = np.equal(imgpair.fullpredimg, imgpair.fulloutputimg) * 1\n            unique, count = np.unique(samepix, return_counts=True)\n            if (1 in unique) and (0 in unique):\n                oneidx = np.where(unique == 1)[0][0]\n                acc = count[oneidx] \/ (sum(count))\n            elif (1 in unique) and (0 not in unique):\n                acc = 1\n            else:\n                acc = 0\n        else:\n            # should compare on an object-wise basis\n            linkedobjs, warning = linkinobjtooutobj(imgpair)  # outobjs in dict are outobjs linked\n            # if there are same amount of linked objs to output objs: all objects are accounted for. max acc = 0.9\n            acc = 0.9 - np.tanh(abs(len(linkedobjs['outobjs']) - len(imgpair.outputsamecolourobjs)) +\n                                abs(len(linkedobjs['outobjs']) - len(imgpair.predoutputsamecolobjs)))\n\n        allaccs = allaccs + [acc]\n\n    acc = sum(allaccs) \/ len(allaccs)\n\n    return acc\n\n\ndef subtaskvalidation(fulltaskold, fulltasknew, taskname):\n    fulltasknew = placeobjsintofullimg(fulltasknew)\n\n    accnew = accbyinputpixtooutput(fulltasknew)\n    accold = accbyinputpixtooutput(fulltaskold)\n    print('{} - acc before: {}, acc after: {}'.format(taskname, accold, accnew))\n\n    if accnew == 1:\n        fulltasknew = placeobjsintofullimg(fulltasknew)\n        fulltasknew.testpred = []\n        for testimgpairs in fulltasknew.testinputimg:\n            fulltasknew.testpred = fulltasknew.testpred + [testimgpairs.fullpredimg]\n\n    if accnew > accold:\n        return accnew, fulltasknew\n    else:\n        return accold, fulltaskold\n\n\ndef findintattrs(fulltask):\n    \"\"\"returns the attributes of the fulltask which are int, as these can be unpacked easily as\n    features for an NN\n    \"\"\"\n    attrvals = vars(fulltask.trainsglimgprs[0].predoutputsamecolobjs[0])\n    samecolobjattrs = list(vars(fulltask.trainsglimgprs[0].predoutputsamecolobjs[0]).keys())\n    isint = []\n\n    for attr in samecolobjattrs:\n        if isinstance(attrvals[attr], int):\n            isint.append(attr)\n\n    return isint\n\n\ndef resultsfrommodel(xtest, ylabels, model):\n    \"\"\"passes xtest through a model to return a set of y predictions\n    \"\"\"\n    model, modeltype = model\n\n    if modeltype == 'nn':\n        predictions = model.predict(xtest)\n        results = np.dot(np.round(predictions), ylabels)  # put them back into an array which holds the colours\n    elif modeltype == 'otomap':\n        otomap, col = model\n        results = []\n        for ii in range(xtest.shape[0]):\n            xval = str(int(xtest[ii, col]))  # sometimes leave .0 on so need to do int\n            results = results + [otomap[xval]]\n    elif modeltype == 'xgb':\n        predictions = model.predict(xtest)\n        predictions2, _ = ylabelstooh(predictions)\n        results = np.dot(np.round(predictions2), ylabels)  # put them back into an array which holds the colours\n\n    return results\n\n\ndef createxfeatures(imgpair, objno, isint, maxobjno):\n    \"\"\"creates all the x features for one sample. Many samples in an xsamples, which calls this fun.\n    \"\"\"\n    features = [0] * maxobjno * len(isint)\n    attrcount = 0\n\n    objlist = imgpair.predoutputsamecolobjs\n\n    # features for this obj go at the beginning\n    for attr in isint:\n        features[attrcount] = getattr(objlist[objno], attr)\n        attrcount += 1\n\n    # make a list of numbers for all objects other than main obj\n    otherobjs = list(range(len(objlist)))\n    otherobjs.remove(objno)\n\n    # loop through list, each loop looping through attrs like above\n    for otherobj in otherobjs:\n        for attr in isint:\n            features[attrcount] = getattr(objlist[otherobj], attr)\n            attrcount += 1\n\n    return np.array(features).reshape(1, maxobjno * len(isint))\n\n\ndef findmaxobjno(fulltask):\n    \"\"\"finds the max no of objs in both train & test so that the correct size for xtrain is given\n    \"\"\"\n    maxobjno = 0\n\n    # make the maxobjno size the size of imgpair with the most objs\n    for traintest in [fulltask.trainsglimgprs, fulltask.testinputimg]:\n        for imgpair in traintest:\n            objlist = imgpair.predoutputsamecolobjs\n\n            if len(objlist) > maxobjno:\n                maxobjno = len(objlist)\n\n    return maxobjno\n\n\ndef createxsamples(imgpairlist, isint, maxobjno):\n    \"\"\"creates an array which creates x samples to train the NN on.\n    Input (imgpairlist): testinputimg or trainsglimgprs\n    samples is np array. each row is 1 sample. Each col is 1 feature.\n    \"\"\"\n    for imgpair in imgpairlist:\n        objlist = imgpair.predoutputsamecolobjs\n\n        for objno in range(len(objlist)):\n            if 'samples' in locals():\n                samples = np.vstack((samples, createxfeatures(imgpair, objno, isint, maxobjno)))\n            else:\n                samples = createxfeatures(imgpair, objno, isint, maxobjno)\n\n    # remove cols which don't have any variation\n    # colsallsame = np.where(np.all(samples == samples[0, :], axis=0))[0]\n\n    # if colsallsame.size != 0:\n    #     samples = np.delete(samples, colsallsame, 1)\n\n    return samples\n\n\ndef makesimplemodel(xtrain, ytrain, ylabels, isint, xtest):\n    from keras.models import Sequential\n    from keras.layers import Dense\n    from sklearn.metrics import accuracy_score\n\n    # try a one-to-one mapping first\n    revertedy = np.dot(ytrain, ylabels)\n\n    symbolcols = [i for i, name in enumerate(isint) if not name.find('symbol')]\n    acc = 0\n\n    for allcols in [symbolcols, range(len(isint))]:\n        for cols in allcols:\n\n            otomap = {}\n            otomaprev = {}  # make sure there ins't just a list of unique xvals mapping to same val of y's\n            for rows in range(xtrain.shape[0]):\n                xval = str(xtrain[rows, cols])\n                yval = str(revertedy[rows])\n                if xval in otomap:\n                    if (otomap[xval] != revertedy[rows]) or (otomaprev[yval] != xtrain[rows, cols]):\n                        break\n                else:\n                    otomap[xval] = revertedy[rows]\n                    otomaprev[yval] = xtrain[rows, cols]\n\n                if rows == xtrain.shape[0] - 1:\n                    acc = 1\n                    break\n\n            if acc:\n                break\n        if acc:\n            break\n\n    if acc == 1:  # first if acc == 1: need to now check that this oto mapping is still valid for test\n        for rows in range(xtest.shape[0]):\n            if not str(xtest[rows, cols]) in otomap.keys():\n                # oto map will err\n                acc = 0\n\n    if acc == 1:\n        model = otomap, cols  # the correct mapping & the col from x train that it was from\n        modeltype = 'otomap'\n        print(isint[cols])\n    else:\n        yclasses = np.size(ytrain, axis=1)\n        usexgboost = 1\n\n        if usexgboost:\n            model = xgb.XGBClassifier(max_depth=3, eta=1, reg_lambda=5)\n            model.fit(xtrain, revertedy)\n            prediction = model.predict(xtrain)\n            prediction2, _ = ylabelstooh(prediction)\n            ypred = np.dot(np.round(prediction2), ylabels)\n            acc = accuracy_score(revertedy, ypred)\n            modeltype = 'xgb'\n        else:\n            # Neural network\n            model = Sequential()\n            model.add(Dense(16, input_dim=np.size(xtrain, axis=1), activation='relu'))\n            model.add(Dense(12, activation='relu'))\n            model.add(Dense(12, activation='relu'))\n            model.add(Dense(yclasses, activation='softmax'))\n\n            model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n            model.fit(xtrain, ytrain, epochs=800, verbose=0)\n\n            # evaluate the keras model\n            _, acc = model.evaluate(xtrain, ytrain)\n\n            modeltype = 'nn'\n\n    return (model, modeltype), acc\n\n\ndef ylabelstooh(ylabel):\n    from sklearn.preprocessing import OneHotEncoder\n\n    # turn into a ohe\n    ohe = OneHotEncoder()\n    ylabel = np.array(ylabel)\n    ylabelunique = np.unique(ylabel)\n    ylabel = ylabel.reshape(-1, 1)\n    y = ohe.fit_transform(ylabel).toarray()\n\n    return y, ylabelunique\n\n\ndef createcolysamples(imgpairlist):\n    ylabel = []\n    for imgpair in imgpairlist:\n        inobjs = imgpair.predoutputsamecolobjs\n        outobjs = imgpair.outputsamecolourobjs\n\n        for inobj in inobjs:\n            xpos = inobj.positionabsx\n            ypos = inobj.positionabsy\n\n            # the same object in output might not be in the same index so will need to find it\n            for outobj in outobjs:\n                if (outobj.positionabsx == xpos) & (outobj.positionabsy == ypos):\n                    break\n\n            ylabel = ylabel + [outobj.colour]\n\n    y, ylabelunique = ylabelstooh(ylabel)\n\n    return y, ylabelunique\n\n\ndef makecolourpredictions(fulltask, model, isint, ylabels, maxobjno):\n    altfulltask = deepcopy(fulltask)\n\n    for traintest in [altfulltask.trainsglimgprs, altfulltask.testinputimg]:\n        for testno, eachtest in enumerate(traintest):\n            # find predictions from model\n            xtest = createxsamples([eachtest], isint, maxobjno)\n            # print(xtest.shape)\n\n            results = resultsfrommodel(xtest, ylabels, model)\n\n            # put the predictions into a final image\n            for objno in range(len(eachtest.predoutputsamecolobjs)):\n                eachtest.predoutputsamecolobjs[objno].colour = results[objno]\n\n    return altfulltask\n\n\ndef placeobjsintosingleimgpair(imgpair):\n    # make a blank canvas\n    outputimg = np.zeros([imgpair.fullinputimg.shape[0], imgpair.fullinputimg.shape[1]])\n\n    for objno, obj in enumerate(imgpair.inputsamecolourobjs):\n        rowsalt = np.repeat(list(range(obj.positionabsx, obj.positionabsx + obj.height)), obj.width)\n        colsalt = list(range(obj.positionabsy, obj.positionabsy + obj.width)) * obj.height\n        vals = obj.elementarr.flatten() * obj.colour\n\n        outputimg[rowsalt, colsalt] = vals\n\n    return outputimg\n\n\ndef placeobjsintofullimg(fulltask):\n    \"\"\"takes objects from predoutputsamecolobjs and places them back into a fullpredimg image.\n    We do this after every sub-task so that we can see if we've completed the task.\n    \"\"\"\n    fulltask.testpred = []\n\n    for traintest in [fulltask.trainsglimgprs, fulltask.testinputimg]:\n        for testno, eachtest in enumerate(traintest):\n            outputimg = np.copy(eachtest.predoutputcanvas)\n            # need another incase obj locations are out of bounds for new canvas (need to move objs around for new\n            # canvas, still, but still wanna keep previous changes)\n            outputimgorig = np.zeros([eachtest.fullpredimg.shape[0], eachtest.fullpredimg.shape[1]])\n            returnorigimgsize = 0\n\n            for objno, obj in enumerate(eachtest.predoutputsamecolobjs):\n                # colsalt = list(np.repeat(list(range(obj.positionabsx, obj.positionabsx + obj.height)), obj.width))\n                # rowsalt = list(range(obj.positionabsy, obj.positionabsy + obj.width)) * obj.height\n                colsalt = list(range(obj.positionabsx, obj.positionabsx + obj.width)) * obj.height\n                rowsalt = list(np.repeat(list(range(obj.positionabsy, obj.positionabsy + obj.height)), obj.width))\n\n                vals = list(obj.elementarr.flatten() * obj.colour)\n\n                # remove values which are 0, so the background val is maintained  rowsalt\n                indices = [i for i, x in enumerate(vals) if x == obj.colour]\n                rowsalt = [d for (i, d) in enumerate(rowsalt) if i in indices]\n                colsalt = [d for (i, d) in enumerate(colsalt) if i in indices]\n                vals = [d for (i, d) in enumerate(vals) if i in indices]\n\n                outputimgorig[rowsalt, colsalt] = vals\n                try:\n                    outputimg[rowsalt, colsalt] = vals\n                except IndexError:\n                    # got an err therefore we should return the other canvas\n                    returnorigimgsize = 1\n\n            if returnorigimgsize:\n                eachtest.fullpredimg = outputimgorig\n            else:\n                eachtest.fullpredimg = outputimg\n\n    return fulltask\n\n\ndef colourchange(fulltaskin):\n    print('colourchange accessed')\n    fulltask = deepcopy(fulltaskin)\n\n    # unroll all the features & samples & put into array\n    isint = findintattrs(fulltask)  # finds all attribues which are ints & can be used as features\n    maxobjno = findmaxobjno(fulltask)\n    xtrain = createxsamples(fulltask.trainsglimgprs, isint, maxobjno)  # np array with samples=rows, features=cols\n    # print(xtrain.shape)\n    ytrain, ylabels = createcolysamples(fulltask.trainsglimgprs)  # makes y_set. one-hot np.\n    # print(ytrain.shape)\n\n    # special case: only 1 y output type\n    ytrain2 = ytrain.tolist()\n    if ytrain2.count(ytrain2[0]) == len(ytrain2):  # all 1's & will err out\n        for traintest in [fulltask.trainsglimgprs, fulltask.testinputimg]:\n            for imgno, imgpair in enumerate(traintest):\n                for objno, obj in enumerate(imgpair.predoutputsamecolobjs):\n                    obj.colour = fulltask.trainsglimgprs[0].outputsamecolourobjs[0].colour\n        fulltask = placeobjsintofullimg(fulltask)\n        acc, fulltaskfinal = subtaskvalidation(fulltaskin, fulltask, 'colourchange')\n        return acc, fulltaskfinal\n\n    # make & train a model on prorperties from train set\n    xtest = createxsamples(fulltask.testinputimg, isint, maxobjno)\n    model, acc = makesimplemodel(xtrain, ytrain, ylabels, isint, xtest)\n\n    # use trained NN to predict colours for test set(s) + change the colours\n    fulltask = makecolourpredictions(fulltask, model, isint, ylabels, maxobjno)\n\n    # find acc of new iteration\n    acc, fulltaskfinal = subtaskvalidation(fulltaskin, fulltask, 'colourchange')\n\n    return acc, fulltaskfinal\n\n\ndef multicolourchange(fulltask):\n    \"\"\"this could be so complicated & could come in so many guises, I wouldn't know where to start with a\n    hard-coded solutions. I'm just going to throw a NN at it.\n    \"\"\"\n    print('multicolourchange accessed')\n    acc = 0\n    return acc, fulltask\n\n\ndef createyzoomsamples(imgpairlist):\n    \"\"\"Outputs a list of booleans. 1 if the object in question was what was zoomed in on. 0 if not\n    \"\"\"\n    ytrain = []\n\n    for imgpair in imgpairlist:\n        for obj in imgpair.predoutputsamecolobjs:\n            if np.array_equal(obj.elementarr * obj.colour, imgpair.fulloutputimg):\n                ytrain = ytrain + [1]\n            else:\n                ytrain = ytrain + [0]\n\n    y, ylabelunique = ylabelstooh(ytrain)\n\n    return y\n\n\ndef makezoomobjpredictions(fulltask, model, isint, maxobjno):\n    fulltask.testpred = []\n\n    for testno, eachtest in enumerate(fulltask.testinputimg):\n        # find predictions from model\n        xtest = createxsamples([eachtest], isint, maxobjno)\n        ylabels = [0, 1]\n\n        results = resultsfrommodel(xtest, ylabels, model)\n\n        objno = np.where(results == 1)[0][0]\n\n        predobj = fulltask.testinputimg[testno].predoutputsamecolobjs[objno].elementarr * \\\n                  fulltask.testinputimg[testno].predoutputsamecolobjs[objno].colour\n\n        fulltask.testpred = fulltask.testpred + [predobj]\n\n    return fulltask\n\n\ndef zoomspecialrulecheck(fulltask):\n    objsinimg = []\n    for imgpairno in range(len(fulltask.trainsglimgprs)):\n        objsinimg = objsinimg + [len(fulltask.trainsglimgprs[imgpairno].predoutputsamecolobjs)]\n\n    if (len(objsinimg) == objsinimg.count(objsinimg[0])) & (objsinimg[0] == 1):\n        return 1\n    else:\n        return 0\n\n\ndef zoomobjrules(fulltask):\n    print('zoomobjrules accessed')\n\n    # special, easy rule if there's only one obj to choose from:\n    if zoomspecialrulecheck(fulltask):\n        fulltask.testpred = []\n        for istrain, traintest in enumerate([fulltask.trainsglimgprs, fulltask.testinputimg]):\n            for testno, eachtest in enumerate(traintest):\n                colour = eachtest.predoutputsamecolobjs[0].colour\n                objimg = eachtest.predoutputsamecolobjs[0].elementarr\n                eachtest.fullpredimg = objimg * colour\n                if not istrain:\n                    fulltask.testpred = fulltask.testpred + [objimg * colour]\n\n        return 1, fulltask\n\n    isint = findintattrs(fulltask)  # finds all attribues which are ints & can be used as features\n    maxobjno = findmaxobjno(fulltask)\n    xtrain = createxsamples(fulltask.trainsglimgprs, isint, maxobjno)  # np array with samples=rows, features=cols\n    ytrain = createyzoomsamples(fulltask.trainsglimgprs)\n\n    # train NN on prorperties from train set\n    ylabels = [0, 1]  # boolean of \"was this zoomed in on\"\n    xtest = createxsamples(fulltask.testinputimg, isint, maxobjno)\n    model, acc = makesimplemodel(xtrain, ytrain, ylabels, isint, xtest)\n\n    # use trained NN to predict colours for test set(s)\n    fulltask = makezoomobjpredictions(fulltask, model, isint, maxobjno)\n\n    return acc, fulltask\n\n\ndef zoomnoobjrules(fulltask):\n    acc = 0\n    return acc, fulltask\n\n\ndef zoomrules(fulltask):\n    \"\"\"looks for rules determining what section to zoom in on, in the input, and why\n    \"\"\"\n    print('zoomrules accessed')\n    if checkforallimages(fulltask, zoomonobject):\n        acc, fulltask = zoomobjrules(fulltask)  # zoomed on a specific object we have in input\n    else:\n        acc, fulltask = zoomnoobjrules(fulltask)  # zoomed on a specific area in input but not exactly around an obj\n\n    return acc, fulltask\n\n\ndef createyobjremsamples(imgpairlist):\n    \"\"\"outputs a list of booleans. 1 for if the input img exists in the output. 0 if not\n    \"\"\"\n    ytrain = []\n\n    for imgpair in imgpairlist:\n        for inobj in imgpair.predoutputsamecolobjs:\n            objexists = 0\n            for outobj in imgpair.outputsamecolourobjs:\n                if np.array_equal(inobj.elementarr, outobj.elementarr):\n                    objexists = 1\n\n            ytrain = ytrain + [objexists]\n\n    y, ylabelunique = ylabelstooh(ytrain)\n\n    return y\n\n\ndef makeobjrempredictions(fulltask, model, isint, maxobjno):\n    \"\"\"predict which objects are to be removed in the train & test input image(s) & remove them, then check\n    the prediction with the train images\n    \"\"\"\n    # make a copy of the fulltask - gonna make some deletions\n    newfulltask = deepcopy(fulltask)\n\n    # remove the objects from inputsamecolobjs in train\n    for traintest in [newfulltask.trainsglimgprs, newfulltask.testinputimg]:\n        for testno, eachtest in enumerate(traintest):\n            xtrain = createxsamples([eachtest], isint, maxobjno)  # np arr with samples=rows, features=cols\n            # print('xtrain no {} has {} rows and {} cols'.format(testno, xtrain.shape[0], xtrain.shape[1]))\n            ylabels = [0, 1]\n\n            results = resultsfrommodel(xtrain, ylabels, model)\n\n            # this is the first obj manipulation task\n            if len(eachtest.predoutputsamecolobjs) == 0:\n                eachtest.predoutputsamecolobjs = deepcopy(eachtest.fullinputimg)\n\n            objs = eachtest.predoutputsamecolobjs\n\n            noofobjs = len(objs)\n            for objno in range(noofobjs - 1, -1, -1):  # go backwards as if we del, all idxs will shift down one\n                if results[objno] == 1:  # del this\n                    del (objs[objno])\n\n    # let's see if that's been positive\n    acc, finalfulltask = subtaskvalidation(fulltask, newfulltask, 'objrem')\n\n    return acc, finalfulltask\n\n\ndef objremrules(fulltask):\n    \"\"\"Looks for rules determining what objects are removed from the input and why\n    \"\"\"\n    print('object remove rules accessed')\n    isint = findintattrs(fulltask)  # finds all attribues which are ints & can be used as features\n    maxobjno = findmaxobjno(fulltask)\n    xtrain = createxsamples(fulltask.trainsglimgprs, isint, maxobjno)  # np array with samples=rows, features=cols\n    ytrain = createyobjremsamples(fulltask.trainsglimgprs)\n\n    # train NN on prorperties from train set\n    ylabels = [1, 0]  # objects we want to del\n    xtest = createxsamples(fulltask.testinputimg, isint, maxobjno)\n    model, acc = makesimplemodel(xtrain, ytrain, ylabels, isint, xtest)\n\n    # use trained NN to predict colours for test set(s)\n    acc, fulltask = makeobjrempredictions(fulltask, model, isint, maxobjno)\n\n    return acc, fulltask\n\n\ndef linkinobjtooutobj(imgpair, maptype='oto'):\n    \"\"\"for each in object, see it exists as an output. If so, return it's array list number. Return\n    the output object's x position and y position\"\"\"\n    warning = ''\n\n    inshapesall = listofuniqueshapes(imgpair.predoutputsamecolobjs)\n    outshapesall = listofuniqueshapes(imgpair.outputsamecolourobjs)\n    inshapes = inshapesall['shapes']\n    outshapes = outshapesall['shapes']\n    inobjswshapes = inshapesall['objswithshape']\n    outobjswshapes = outshapesall['objswithshape']\n    if len(inobjswshapes) != len(outobjswshapes):\n        warning = 'different shapes in as out \/n'\n\n    inobj = []\n    outobj = []\n\n    for inshapeno, eachinshape in enumerate(inshapes):\n        for outshapeno, eachoutshape in enumerate(outshapes):\n            if np.array_equal(eachinshape, eachoutshape):  # got a matching pair, make sure they're okay\n                if len(inobjswshapes[inshapeno]) == len(outobjswshapes[outshapeno]):  # same no of ins to outs:\n                    for objno in range(len(inobjswshapes[inshapeno])):\n                        if maptype == 'oto':  # one to one\n                            # just assign x in to x out so we get a 1:1 mapping\n                            inobj = inobj + [inobjswshapes[inshapeno][objno]]\n                            outobj = outobj + [outobjswshapes[outshapeno][objno]]\n\n                        if maptype == 'otm':  # one to many\n                            None  # for now\n\n                else:\n                    warning = warning + 'different number of ins to outs \/n'\n\n    if len(inobj) != len(imgpair.predoutputsamecolobjs):\n        warning = warning + 'not all in shapes accounted for: need to remove some first'\n\n    return {'inobjs': inobj, 'outobjs': outobj}, warning\n\n\ndef createymovesamples(imgpair, axis):\n    positionlist = []\n    for obj in imgpair.outputsamecolourobjs:\n        positionlist = positionlist + [getattr(obj, 'positionabs' + axis)]\n\n    return positionlist\n\n\ndef creatingmovingobjsxset(imgpairlist, isint, maxobjno, traintestno, xyaxis):\n    for imgpairno, imgpair in enumerate(imgpairlist):\n        intooutobjs, warnings = linkinobjtooutobj(imgpair)\n        if len(warnings) > 0:  # we need 1:1 mapping from in:out, as in test, won't know what to map\n            return 0, 0\n\n        inobjs = intooutobjs['inobjs']\n        outobjs = intooutobjs['outobjs']\n\n        xtrainraw = createxsamples([imgpair], isint, maxobjno)  # np array with samples=rows, features=cols\n\n        if imgpairno == 0:\n            xtrain = xtrainraw[inobjs[0], :]\n            for objno in inobjs[1:]:\n                xtrain = np.vstack([xtrain, xtrainraw[objno, :]])\n\n            if traintestno == 0:\n                ytrainraw = createymovesamples(imgpair, xyaxis)\n                ytrainraw2 = [ytrainraw[outobjs[0]]]\n                for objno in outobjs[1:]:\n                    ytrainraw2 = ytrainraw2 + [ytrainraw[objno]]\n            else:\n                ytrainraw2 = 0\n        else:\n            for objno in inobjs:\n                xtrain = np.vstack([xtrain, xtrainraw[objno, :]])\n\n            if traintestno == 0:\n                ytrainraw = createymovesamples(imgpair, xyaxis)\n                for objno in outobjs:\n                    ytrainraw2 = ytrainraw2 + [ytrainraw[objno]]\n\n    return xtrain, ytrainraw2\n\n\ndef movingobjects(fulltask):\n    \"\"\"looks to determine rules for where to move each object if they need moving\n    \"\"\"\n    print('movingobjects accessed')\n    newfulltask = deepcopy(fulltask)\n\n    isint = findintattrs(newfulltask)  # finds all attribues which are ints & can be used as features\n    maxobjno = findmaxobjno(newfulltask)\n\n    for xyaxis in ['x', 'y']:\n        xtrain, ytrainraw2 = creatingmovingobjsxset(newfulltask.trainsglimgprs, isint, maxobjno, 0, xyaxis)\n        xtest = createxsamples(fulltask.testinputimg, isint, maxobjno)\n\n        # train\n        ytrain, ylabels = ylabelstooh(ytrainraw2)\n        model, acc = makesimplemodel(xtrain, ytrain, ylabels, isint, xtest)\n\n        for traintestno, traintest in enumerate([newfulltask.trainsglimgprs, newfulltask.testinputimg]):\n            for imgpairno, imgpair in enumerate(traintest):\n                # now make predictions with the model\n                xset = createxsamples([imgpair], isint, maxobjno)\n                results = resultsfrommodel(xset, ylabels, model)\n\n                # assign the new val\n                for objno in range(len(imgpair.predoutputsamecolobjs)):\n                    setattr(imgpair.predoutputsamecolobjs[objno], 'positionabs' + xyaxis, int(results[objno]))\n\n    acc, finalfulltask = subtaskvalidation(fulltask, newfulltask, 'moveobjs')\n\n    return acc, finalfulltask\n\n\ndef booleannot(fulltask):\n    \"\"\"applies a boolean not to each object in turn. If accuracy goes up, keep the not\n    \"\"\"\n    print('booleannot accessed')\n    newfulltask = deepcopy(fulltask)\n\n    # make the y's\n    boolnotobjs = []\n    for imgpair in newfulltask.trainsglimgprs:\n        for obj in imgpair.predoutputsamecolobjs:\n            newelemarr = 1 - obj.elementarr\n\n            # check this elemarr against output img\n            y1 = obj.positionabsy\n            y2 = obj.positionabsy + obj.elementarr.shape[0]\n            x1 = obj.positionabsx\n            x2 = obj.positionabsx + obj.elementarr.shape[1]\n            outputimg = (imgpair.fulloutputimg[y1:y2, x1:x2] != imgpair.backgroundcol) * 1\n\n            # add to list saying if it is or ins't a not\n            boolnotobjs = boolnotobjs + [np.array_equal(newelemarr, outputimg)]\n\n    isint = findintattrs(newfulltask)  # finds all attribues which are ints & can be used as features\n    maxobjno = findmaxobjno(newfulltask)\n\n    xtrain = createxsamples(newfulltask.trainsglimgprs, isint, maxobjno)  # np array with samples=rows, features=cols\n    ytrain, ylabels = ylabelstooh(boolnotobjs)\n\n    xtest = createxsamples(fulltask.testinputimg, isint, maxobjno)\n    model, acc = makesimplemodel(xtrain, ytrain, ylabels, isint, xtest)\n\n    for traintest in [newfulltask.trainsglimgprs, newfulltask.testinputimg]:\n        for imgpair in traintest:\n            xtrain = createxsamples([imgpair], isint, maxobjno)\n            results = resultsfrommodel(xtrain, ylabels, model)\n            for objno, obj in enumerate(imgpair.predoutputsamecolobjs):\n                if results[objno]:\n                    obj.elementarr = 1 - obj.elementarr\n\n    # turn this into a new class as we might have gained\/lost objects\n    newfulltask = placeobjsintofullimg(newfulltask)\n    newfulltask = FullTaskFromClass(newfulltask)\n\n    acc, finalfulltask = subtaskvalidation(fulltask, newfulltask, 'booleannot')\n\n    return acc, finalfulltask\n\n\ndef booltests(newfulltask, test):\n    for traintest in [newfulltask.trainsglimgprs, newfulltask.testinputimg]:\n        for imgpair in traintest:\n            if test == 0:  # logical and\n                taskname = 'logical and'\n                logicalarr = np.logical_and(imgpair.predoutputsamecolobjs[0].elementarr, imgpair.predoutputsamecolobjs[1].elementarr) * 1\n                imgpair.predoutputsamecolobjs = [SameColourObject(logicalarr, 1)]\n                imgpair.fullpredimg = logicalarr\n            elif test == 1:  # logical or\n                taskname = 'logical or'\n                logicalarr = np.logical_or(imgpair.predoutputsamecolobjs[0].elementarr, imgpair.predoutputsamecolobjs[1].elementarr) * 1\n                imgpair.predoutputsamecolobjs = [SameColourObject(logicalarr, 1)]\n                imgpair.fullpredimg = logicalarr\n            elif test == 2:  # logical nand\n                taskname = 'logical nand'\n                logicalarr = np.logical_not(np.logical_or(imgpair.predoutputsamecolobjs[0].elementarr, imgpair.predoutputsamecolobjs[1].elementarr)) * 1\n                imgpair.predoutputsamecolobjs = [SameColourObject(logicalarr, 1)]\n                imgpair.fullpredimg = logicalarr\n            elif test == 3:  # logical nor\n                taskname = 'logical nor'\n                logicalarr = np.logical_not(np.logical_or(imgpair.predoutputsamecolobjs[0].elementarr, imgpair.predoutputsamecolobjs[1].elementarr)) * 1\n                imgpair.predoutputsamecolobjs = [SameColourObject(logicalarr, 1)]\n                imgpair.fullpredimg = logicalarr\n            elif test == 4:  # logical xor\n                taskname = 'logical xor'\n                logicalarr = np.logical_xor(imgpair.predoutputsamecolobjs[0].elementarr, imgpair.predoutputsamecolobjs[1].elementarr) * 1\n                imgpair.predoutputsamecolobjs = [SameColourObject(logicalarr, 1)]\n                imgpair.fullpredimg = logicalarr\n            elif test == 5:  # logical xnor\n                taskname = 'logical xnor'\n                logicalarr = np.logical_not(np.logical_xor(imgpair.predoutputsamecolobjs[0].elementarr, imgpair.predoutputsamecolobjs[1].elementarr)) * 1\n                imgpair.predoutputsamecolobjs = [SameColourObject(logicalarr, 1)]\n                imgpair.fullpredimg = logicalarr\n\n    # elif test == 6:\n    #     for traintest in [newfulltask.trainsglimgprs, newfulltask.testinputimg]:\n    #         for imgpair in traintest:\n    return newfulltask, taskname\n\n\ndef booleanlogic(fulltask):\n    print('boolean logic accessed')\n    for imgpair in fulltask.trainsglimgprs:\n        imgpair.fullpredimg = imgpair.predoutputsamecolobjs[0].elementarr\n    accold = accbyinputpixtooutput(fulltask)\n    accbest = accold\n    for test in range(5):\n        newfulltask = deepcopy(fulltask)\n        newfulltask, taskname = booltests(newfulltask, test)\n        accnew, fulltasknew = subtaskvalidation(fulltask, newfulltask, taskname)\n        if accnew > accbest:\n            accbest = accnew\n            toptest = test\n\n    if accbest > accold:\n        newfulltask = deepcopy(fulltask)\n        newfulltask, taskname = booltests(newfulltask, toptest)\n\n        # to re-order the class\n        accnew, newfulltask = subtaskvalidation(fulltask, newfulltask, taskname)\n        return accbest, newfulltask\n    else:\n        # no luck, return the old, unchanged fulltask\n        return 0, fulltask\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~ looping through entry requirements ~~~~~~~~~~~~~~~~~~~~~~~~~\ndef checkforallimages(fulltask, function):\n    truthlist = []\n    for ii in range(len(fulltask.trainsglimgprs)):\n        truthlist.append(function(fulltask.trainsglimgprs[ii]))\n\n    if truthlist.count(truthlist[0]) == len(truthlist):  # all are the same\n        return truthlist[0]\n    else:\n        return 0\n\n\ndef timeout_handler(signum, frame):\n    raise TimeoutException\n\n\nclass TimeoutException(Exception):\n    pass\n\n\ndef findnextrule(fulltask, subtaskdonelist, symbols=True):\n    \"\"\"This loops through all rule entry requirements and looks for a rule which satisfies requirements. If requirements\n    are met, it then looks for rules to define that type of behaviour. e.g. see if objects just need to be coloured in\n    (entry requirements). If so: what determines the rule of colouring in?\n    fulltask is a FullTask: if it has been input as an argument to second or greater depth calls for findnextrule, this\n    may be adjusted from original fulltask.\n    subtaskdonelist is a list of subtasks done, in case we keep recognising a task as needing that transform done on it,\n    so we don't end up in an endless loop\n    \"\"\"\n    startprocess = datetime.datetime.now()\n    if (0 not in subtaskdonelist) & symbols:\n        fulltask = symbolser(fulltask)\n        subtaskdonelist = subtaskdonelist + [0]\n\n    if (checkforallimages(fulltask, colouriner) == 1) & (1 not in subtaskdonelist):\n        # do colour stuff\n        acc, fulltask = colourchange(fulltask)\n        subtaskdonelist = subtaskdonelist + [1]\n    elif (checkforallimages(fulltask, colouriner) == 2) & (2 not in subtaskdonelist):\n        # do multicolour stuff\n        acc, fulltask = multicolourchange(fulltask)\n        subtaskdonelist = subtaskdonelist + [2]\n    elif (checkforallimages(fulltask, zoominer) == 1) & (3 not in subtaskdonelist):\n        acc, fulltask = zoomrules(fulltask)\n        subtaskdonelist = subtaskdonelist + [3]\n    elif (checkforallimages(fulltask, objremer) == 1) & (4 not in subtaskdonelist):\n        acc, fulltask = objremrules(fulltask)\n        subtaskdonelist = subtaskdonelist + [4]\n    elif (booleannoter(fulltask) == 1) & (5 not in subtaskdonelist):\n        acc, fulltask = booleannot(fulltask)\n        subtaskdonelist = subtaskdonelist + [5]\n    elif (checkforallimages(fulltask, booleanlogicer) == 1) & (6 not in subtaskdonelist):\n        acc, fulltask = booleanlogic(fulltask)\n        subtaskdonelist = subtaskdonelist + [6]\n    elif (checkforallimages(fulltask, movingobjectser) == 1) & (7 not in subtaskdonelist):\n        acc, fulltask = movingobjects(fulltask)\n        subtaskdonelist = subtaskdonelist + [7]\n    else:\n        # no more rules to apply\n        acc = 0\n        return acc, fulltask\n\n    endprocess = datetime.datetime.now()\n    print('Time spent on this process was: {}'.format(endprocess - startprocess))\n\n    if acc == 1:\n        for testno, onetestpred in enumerate(fulltask.testpred):\n            if isinstance(onetestpred, list):\n                fulltask.testpred[testno] = [int(x) for x in onetestpred]\n            else:  # assume numpy array\n                fulltask.testpred[testno] = onetestpred.astype(int)\n                fulltask.testpred[testno] = fulltask.testpred[testno].tolist()\n\n        return acc, fulltask\n    else:\n        # go again to see if we can find the next step\n        acc, fulltask = findnextrule(fulltask, subtaskdonelist, symbols)\n        return acc, fulltask","8e13e8c7":"############################# PLOTTING FUNCTIONS ###############################\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\n\n\ndef plot_one(ax, task, i, traintest, input_or_output):\n    cmap = colors.ListedColormap(\n        ['#000000', '#0074D9', '#FF4136', '#2ECC40', '#FFDC00',\n         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n    norm = colors.Normalize(vmin=0, vmax=9)\n\n    input_matrix = task[traintest][i][input_or_output]\n    ax.imshow(input_matrix, cmap=cmap, norm=norm)\n    ax.grid(True, which='both', color='lightgrey', linewidth=0.5)\n    ax.set_yticks([x - 0.5 for x in range(1 + len(input_matrix))])\n    ax.set_xticks([x - 0.5 for x in range(1 + len(input_matrix[0]))])\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    ax.set_title(traintest + ' ' + input_or_output)\n\n\ndef plot_one_class(ax, task, i, traintest, inoutpred):\n    cmap = colors.ListedColormap(\n        ['#000000', '#0074D9', '#FF4136', '#2ECC40', '#FFDC00',\n         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n    norm = colors.Normalize(vmin=0, vmax=9)\n\n    if traintest == 'train':\n        if inoutpred == 'input':\n            input_matrix = task.trainsglimgprs[i].fullinputimg\n        elif inoutpred == 'output':\n            input_matrix = task.trainsglimgprs[i].fulloutputimg\n        else:\n            if task.trainsglimgprs[i].fullpredimg is None:\n                return\n\n            input_matrix = task.trainsglimgprs[i].fullpredimg\n    else:\n        if inoutpred == 'input':\n            input_matrix = task.testinputimg[i].fullinputimg\n        elif inoutpred == 'pred':\n            if task.testinputimg[i].fullpredimg is None:\n                return\n\n            input_matrix = task.testinputimg[i].fullpredimg\n\n        else:\n            return\n\n    ax.imshow(input_matrix, cmap=cmap, norm=norm)\n    ax.grid(True, which='both', color='lightgrey', linewidth=0.5)\n    ax.set_yticks([x - 0.5 for x in range(1 + len(input_matrix))])\n    ax.set_xticks([x - 0.5 for x in range(1 + len(input_matrix[0]))])\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    ax.set_title(traintest + ' ' + inoutpred)\n\n\ndef plot_task(task):\n    \"\"\"\n    Plots the first train and test pairs of a specified task,\n    using same color scheme as the ARC app\n    \"\"\"\n    if isinstance(task, FullTask) or isinstance(task, FullTaskFromClass):\n        plotarcclass(task)\n        return\n\n    num_train = len(task['train'])\n    fig, axs = plt.subplots(2, num_train, figsize=(3 * num_train, 3 * 2))\n    for i in range(num_train):\n        plot_one(axs[0, i], task, i, 'train', 'input')\n        plot_one(axs[1, i], task, i, 'train', 'output')\n    plt.tight_layout()\n    plt.show()\n\n    num_test = len(task['test'])\n    fig, axs = plt.subplots(2, num_test, figsize=(3 * num_test, 3 * 2))\n    if num_test == 1:\n        plot_one(axs[0], task, 0, 'test', 'input')\n        plot_one(axs[1], task, 0, 'test', 'output')\n    else:\n        for i in range(num_test):\n            plot_one(axs[0, i], task, i, 'test', 'input')\n            plot_one(axs[1, i], task, i, 'test', 'output')\n    plt.tight_layout()\n    plt.show()\n\n\ndef print_numpy_arr(task):\n    num_train = len(task['train'])\n    for i in range(num_train):\n        print()\n\n\ndef plotarcclass(task):\n    \"\"\"\n    Plots the first train and test pairs of a specified task,\n    using same color scheme as the ARC app\n    \"\"\"\n    num_train = len(task.trainsglimgprs)\n    fig, axs = plt.subplots(3, num_train, figsize=(3 * num_train, 3 * 2))\n    for i in range(num_train):\n        plot_one_class(axs[0, i], task, i, 'train', 'input')\n        plot_one_class(axs[1, i], task, i, 'train', 'output')\n        plot_one_class(axs[2, i], task, i, 'train', 'pred')\n    plt.tight_layout()\n    plt.show()\n\n    num_test = len(task.testinputimg)\n    fig, axs = plt.subplots(3, num_test, figsize=(3 * num_test, 3 * 2))\n    if num_test == 1:\n        plot_one_class(axs[0], task, 0, 'test', 'input')\n        plot_one_class(axs[1], task, 0, 'test', 'output')\n        plot_one_class(axs[2], task, 0, 'test', 'pred')\n    else:\n        for i in range(num_test):\n            plot_one_class(axs[0, i], task, i, 'test', 'input')\n            plot_one_class(axs[1, i], task, i, 'test', 'output')\n            plot_one_class(axs[2, i], task, i, 'test', 'pred')\n    plt.tight_layout()\n    plt.show()\n","b087f78d":"def singlesolution(task, patmodel):\n    acc = 0\n    patterntask = makepredictions(task, patmodel)\n\n    if patterntask:\n        # t_pred_test_list is a list containing numpy array, 1 element for each input in test\n        acc, t_pred_test_list = check_p(task, patch_image)\n\n        if acc != 1:\n            None  # make a pattern class here and see if we can get results from that\n        else:\n            sol = 'pattern'\n\n    if acc != 1:\n        subtaskdonelist = []\n        taskclass = FullTask(task)\n        try:\n            acc, fulltask = findnextrule(taskclass, subtaskdonelist)\n            t_pred_test_list = fulltask.testpred\n\n            if acc == 1:\n                sol = 'arcrule'\n        except Exception as inst:\n            print('errored with symbols')\n            print(type(inst))  # the exception instance\n            print(inst.args)  # arguments stored in .args\n            print(inst)\n            acc = 0\n\n    if acc != 1:\n        subtaskdonelist = []\n        taskclass = FullTask(task)\n        try:\n            taskclass = FullTask(task)  # reload as symbols will stay\n            acc, fulltask = findnextrule(taskclass, subtaskdonelist, symbols=False)\n            t_pred_test_list = testpred\n\n            if acc == 1:\n                sol = 'arcrule'\n            else:\n                fulltask.testpred = []\n                for testno, onetestpred in enumerate(fulltask.testinputimg):\n                    if isinstance(onetestpred.fullpredimg, list):\n                        fulltask.testpred = fulltask.testpred + [int(x) for x in onetestpred.fullpredimg]\n                    else:  # assume numpy array\n                        ontestpredlist = onetestpred.fullpredimg.astype(int)\n                        fulltask.testpred = fulltask.testpred + [ontestpredlist.tolist()]\n                        \n        except Exception as inst:\n            print('errored without symbols')\n            print(type(inst))  # the exception instance\n            print(inst.args)  # arguments stored in .args\n            print(inst)\n            acc = 0\n\n    if acc != 1:\n        try:\n            a = toplevel1(task)\n        except Exception as inst:\n            print('misckaggle errored')\n            print(type(inst))  # the exception instance\n            print(inst.args)  # arguments stored in .args\n            print(inst)\n            a = -1\n            acc = 0\n\n        if a != -1:\n            print('misc kaggle: {} to 1'.format(acc))\n            acc = 1\n            t_pred_test_list = [a]\n            fulltask.testinputimg[0].fullpredimg = np.array(t_pred_test_list[0])\n            sol = 'misckaggle'\n        else:\n            sol = 'arcrule'\n\n    return fulltask, t_pred_test_list, sol\n\ndef testatask(task):\n    # grab the pattern model\n    f = open('\/kaggle\/working\/patternmodel.pckl', 'rb')\n    model = cloudpickle.load(f)\n    \n    try:\n        fulltask, t_pred_test_list, sol = singlesolution(task, model)\n    except Exception as inst:\n        print(type(inst))  # the exception instance\n        print(inst.args)  # arguments stored in .args\n        print(inst)\n        acc = 0\n\n    # if acc != 1:\n    #     taskclass = arcclasses.FullTask(task)\n    #     acc, taskclass = arcrules.findnextrule(taskclass, [], symbols=False)  # trying again without symbols\n\n    try:\n        if (not sol == 'pattern') or (not sol == 'misckaggle'):\n            plot_task(fulltask)\n    except Exception as inst:\n        print(type(inst))  # the exception instance\n        print(inst.args)  # arguments stored in .args\n        print(inst)\n        acc = 0\n        \n        print('Something went wrong with plotting... sorry!')\n        \n# save the pattern detect NN model:\nif os.path.isfile('\/kaggle\/input\/patmodel\/patternmodel.pckl'):\n    f = open('\/kaggle\/input\/patmodel\/patternmodel.pckl', 'rb')\n    model = cloudpickle.load(f)\n\n    fl = open('\/kaggle\/working\/patternmodel.pckl', 'wb')\n    cloudpickle.dump(model, fl)","5090ddb5":"from ipywidgets import Layout, Button, VBox, Label, Box, Output\n\noutput_task = Output()\n\nalltasks, tasknames = startup(dataset='test')\n\nbuttonstyle = ['danger'] * len(tasknames)\nfor taskcomplete in [1, 12, 14, 17, 39, 40, 46, 48, 86, 89]:\n    buttonstyle[taskcomplete] = 'success'\n\ndef btn_eventhandler(obj):\n    with output_task:\n        print(obj.description)\n        output_task.clear_output()\n        tnindex = tasknames.index(obj.description)\n        print(tnindex)\n        testatask(alltasks[tnindex])\n\nitem_layout = Layout(height='50px', min_width='490px')\nitems = [Button(layout=item_layout, description=str(tasknames[taskno]), button_style=buttonstyle[taskno]) for taskno in range(len(tasknames))]\nfor eachbutton in items:\n    eachbutton.on_click(btn_eventhandler)\n    \nbox_layout = Layout(overflow_y='auto',\n                    border='3px solid black',\n                    width='500px',\n                    height='500px',\n                    flex_flow='column',\n                    display='block')\ncarousel = Box(children=items, layout=box_layout)\n\nVBox([Label('Tasks to select:'), carousel])\n\n","57a27c8d":"display(output_task)","f4e5f215":"# The fun interactive dropdown:","04b52e54":"# The inner workings: ignore the next 6 cells unless you'd like to get into the guts of the code:","441c1029":"# The Notebook\nThis notebook takes the solution I had created for the ARC challenge and gives users an interface to select different tasks, to see how it coped with each one. Spoiler: not well.\n\nAlthough I was scoring about 10% success in the train and task sets, I only achieved an LB score of 0.99 with this. I write \"DSL\" in quotation marks in the title because although my solution loosely alligns with DSL solutions, it may not be a strict example of a DSL. Essentially, I wrote classes to contain embeddings of the images, and functions to operate on these.\n\n\n\n\n# How it works\nI broke the whole challenge into 2 sections. Some tasks were related to patterns, some to the arrangement of objects. \n\n**Patterns:** I had trained a NN to detect tasks which involved filling in repeating patterns. Once these had been identified, I used Paulo Pinto's [+28 Tasks notebook](https:\/\/www.kaggle.com\/paulorzp\/28-tasks-tiles-and-symmetry) to get a solution for these.\n\n**Objects:** This is where the heavy lifting was done. First I created a class which contained the whole task, then within that, an \"imgpair\" class comprising of an input + output: within each input and output I had a \"singleimg\" class comprising of any objects found in that image. Finally, each object listed had various properties found: size, height, x & y location, colour, amount of holes. These were used later by a model to work out what was important for deciding how a certain rule happened. This formed the basis of object manipulation. I then created functions which identified: 1. if a certain rule took place, 2. What objects that rule applied to, 3: the specifics of that rule. For example, if objects moved around from input -> output, we'd need to know what objects moved, and the specifics of where they moved and the rules for how they move. In total, I did rules for: object colour changes, object removal, object movements, logical operations between 2 objects (and, nand, or, nor, xor, xnor) and not operators for single objects. I guess, in the end, though, this approach didn't generalise very well.\n\n\n\n\n# Success Stories\nThe following tasks in the test set were completed by this approach:\n* 009d5c81.json\n* 0a2355a6.json\n* 0bb8deee.json\n* 0c9aba6e.json\n* 195ba7dc.json\n* 1990f7a8.json\n* 1c0d0a4b.json\n* 1d0a4b61.json\n* 1e97544e.json\n* 332efdb3.json\n* 34b99a2b.json\n* 37d3e8b2.json\n\nYou'll need to click \"edit\" in the top right to get the dropdown to work and then scroll all the way to the bottom (ignore all code other than the dropdown)."}}