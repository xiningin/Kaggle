{"cell_type":{"e9538c7c":"code","3697ab22":"code","ef437827":"code","4a33ae9e":"code","db862630":"code","7d1e5970":"code","077a9149":"code","7def1c7d":"code","31da2169":"code","b81bcdf6":"code","22c14411":"markdown"},"source":{"e9538c7c":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version 20210331 --apt-packages libomp5 libopenblas-dev\n!rm -rf \/kaggle\/working\/*.whl\n!rm -rf \/kaggle\/working\/*.py+\n!pip install accelerate","3697ab22":"import os\nimport gc\nimport sys\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold,StratifiedKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\n\nfrom accelerate import Accelerator\nfrom transformers import (AutoModel, AutoTokenizer,AutoConfig,\n                          get_cosine_schedule_with_warmup)\n\n\n\nfrom colorama import Fore, Back, Style\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nc_ = Fore.CYAN\nsr_ = Style.RESET_ALL","ef437827":"train_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\n\nsample = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\n\ntrain_data['excerpt'] = train_data['excerpt'].apply(lambda x: x.replace('\\n',''))\n\nnum_bins = int(np.floor(1 + np.log2(len(train_data))))\ntrain_data.loc[:,'bins'] = pd.cut(train_data['target'],bins=num_bins,labels=False)\n\nbins = train_data.bins.to_numpy()\ntarget = train_data.target.to_numpy()\n","4a33ae9e":"config = {\n    'lr': 2e-5,\n    'wd':0.01,\n    'batch_size':16,\n    'valid_step':50,\n    'max_len':300,\n    'epochs':10,\n    'nfolds':5,\n    'seed':42,\n    \n}\n\nfor i in range(config['nfolds']):\n    os.makedirs(f'model{i}',exist_ok=True)\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=config['seed'])\n\ntrain_data['Fold'] = -1\nkfold = StratifiedKFold(n_splits=config['nfolds'],shuffle=True,random_state=config['seed'])\nfor k , (train_idx,valid_idx) in enumerate(kfold.split(X=train_data,y=bins)):\n    train_data.loc[valid_idx,'Fold'] = k","db862630":"def rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))","7d1e5970":"class CLRPDataset(Dataset):\n    def __init__(self,df,tokenizer,max_len=256):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.targets = df['target'].to_numpy()\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],\n                                return_tensors='pt',\n                                max_length=self.max_len,\n                                padding='max_length',\n                                truncation=True)\n        \n        target = torch.tensor(self.targets[idx],dtype=torch.float) \n        return encode, target # encode has format {'input_ids': tensor, 'attention_mask'}\n    \n    def __len__(self):\n        return len(self.excerpt)","077a9149":"class AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector","7def1c7d":"class AttentionModel(nn.Module):\n    def __init__(self):\n        super(AttentionModel,self).__init__()\n        self.roberta =  AutoModel.from_pretrained('roberta-base') \n        self.config = AutoConfig.from_pretrained('roberta-base')\n        self.head = AttentionHead(self.config.hidden_size,self.config.hidden_size) # self attention\n        self.mlp = nn.Sequential(\n            nn.Dropout(0.1),\n            nn.Linear(self.config.hidden_size,128),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(128,1)\n        )\n\n    def forward(self,**xb):\n        x = self.roberta(**xb)[0] # last_hidden_state \n        x = self.head(x)\n        x = self.mlp(x)\n        return x\n    \n\nclass MeanPoolingModel(nn.Module):\n    \n    def __init__(self):\n        super(MeanPoolingModel,self).__init__()\n        \n        self.config = AutoConfig.from_pretrained('roberta-base')\n        self.roberta = AutoModel.from_pretrained('roberta-base')\n        self.layer_norm = nn.LayerNorm(self.config.hidden_size)\n        self.linear = nn.Linear(self.config.hidden_size, 1)\n        \n    def forward(self, **xb):\n        attention_mask = xb['attention_mask']\n        \n        outputs = self.roberta(**xb)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings \/ sum_mask\n        norm_mean_embeddings = self.layer_norm(mean_embeddings)\n        logits = self.linear(norm_mean_embeddings)\n        \n        preds = logits.squeeze(-1).squeeze(-1)\n        \n        return preds.view(-1).float()","31da2169":"def create_optimizer(model, model_name):\n    \n    parameters = []\n    if model_name == 'attention':\n        attention_group = [params for params in model.head.parameters()]\n        regressor_group = [params for params in model.mlp.parameters()]\n        parameters.append({\"params\": attention_group})\n        parameters.append({\"params\": regressor_group})\n    \n    elif model_name == 'mean':\n        layer_norm_group = [params for params in model.layer_norm.parameters()]\n        regressor_group = [params for params in model.linear.parameters()]\n        parameters.append({\"params\": layer_norm_group})\n        parameters.append({\"params\": regressor_group})\n    \n    for layer_num, (name, params) in enumerate(model.roberta.named_parameters()):\n        weight_decay = 0.0 if \"bias\" in name else 0.01\n        \n        lr = 2e-5\n\n        if layer_num >= 69:        \n            lr = 5e-5\n\n        if layer_num >= 133:\n            lr = 1e-4\n\n        parameters.append({\"params\": params,\n                           \"weight_decay\": weight_decay,\n                           \"lr\": lr})\n       \n\n    return optim.AdamW(parameters)\n\ndef run(fold, model_name, verbose=True):\n    \n    def loss_fn(outputs,targets):\n        outputs = outputs.view(-1)\n        targets = targets.view(-1)\n        return torch.sqrt(nn.MSELoss()(outputs,targets))\n   \n    def train_and_evaluate_loop(model_name, train_loader,valid_loader,model, loss_fn,optimizer,epoch,fold,best_loss,valid_step=20,lr_scheduler=None):\n        train_loss = 0\n\n        for i, (inputs1,targets1) in enumerate(train_loader):\n            \n            model.train()\n            optimizer.zero_grad()\n            inputs1 = {key:val.reshape(val.shape[0],-1) for key,val in inputs1.items()}\n            outputs1 = model(**inputs1)\n            loss1 = loss_fn(outputs1,targets1)\n            loss1.backward()\n            optimizer.step()\n            gc.collect()\n            train_loss += loss1.item()\n            \n            if lr_scheduler:\n                lr_scheduler.step()\n            \n            #evaluating for every valid_step\n            if (i % valid_step == 0) or (i == (len(train_loader)-1)):\n                model.eval()\n                valid_loss = 0\n                with torch.no_grad():\n                    for j, (inputs2,targets2) in enumerate(valid_loader):\n                        inputs2 = {key:val.reshape(val.shape[0],-1) for key,val in inputs2.items()}\n                        outputs2 = model(**inputs2)\n                        loss2 = loss_fn(outputs2,targets2)\n                        valid_loss += loss2.item()\n                     \n                    valid_loss \/= len(valid_loader)\n                    if valid_loss <= best_loss:\n                        if verbose:                            \n                            xm.master_print(f\"epoch:{epoch} | Train Loss:{train_loss\/(i+1)} | Validation loss:{valid_loss}\")\n                            xm.master_print(f\"{g_}Validation loss Decreased from {best_loss} to {valid_loss}{sr_}\")\n                        best_loss = valid_loss\n                        xm.save(model.state_dict(),f'.\/{model_name}\/model{fold}\/model{fold}.bin')\n                        tokenizer.save_pretrained(f'.\/{model_name}\/model{fold}')\n                        \n        return best_loss\n        \n    accelerator = Accelerator()\n    xm.master_print(f\"{accelerator.device} is used\")\n    x_train,x_valid = train_data.query(f\"Fold != {fold}\"),train_data.query(f\"Fold == {fold}\")\n    \n    model = None\n    if  model_name == 'attention':\n        model = AttentionModel()\n    elif model_name == 'mean':\n        model = MeanPoolingModel()    \n    tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n    optimizer = create_optimizer(model, model_name)\n    \n    train_ds = CLRPDataset(x_train,tokenizer,config['max_len'])\n    train_dl = DataLoader(train_ds,\n                        batch_size = config[\"batch_size\"],\n                        shuffle=True,\n                        num_workers = 4,\n                        pin_memory=True,\n                        drop_last=False)\n\n    valid_ds = CLRPDataset(x_valid,tokenizer,config['max_len'])\n    valid_dl = DataLoader(valid_ds,\n                        batch_size = config[\"batch_size\"],\n                        shuffle=False,\n                        num_workers = 4,\n                        pin_memory=True,\n                        drop_last=False)\n\n    num_training_steps = config['epochs'] * len(train_dl)\n    lr_scheduler = get_cosine_schedule_with_warmup(optimizer,num_warmup_steps=200,\n                                                   num_training_steps =num_training_steps)\n\n    model,train_dl,valid_dl,optimizer,lr_scheduler = accelerator.prepare(model,train_dl,valid_dl,optimizer,lr_scheduler)\n\n    xm.master_print(f\"Fold: {fold}\") \n    best_loss = 9999\n    \n    for epoch in range(config[\"epochs\"]):\n        xm.master_print(f\"Epoch Started:{epoch}\")\n        \n        best_loss = train_and_evaluate_loop(model_name, train_dl,valid_dl,model,loss_fn,optimizer,epoch,fold,best_loss,\n                                            valid_step=config['valid_step'],lr_scheduler=lr_scheduler)\n    return best_loss","b81bcdf6":"import gc\nperformance = {\n    'mean': [],\n    'attention': []\n}\nfor model_name in ['mean', 'attention']:\n    try:\n        os.mkdir(f'.\/{model_name}', mode = 0o666)\n    except OSError as error:\n        pass\n    for f in range(config['nfolds']):\n        try:\n            os.mkdir(f'.\/{model_name}\/model{f}', mode = 0o666)\n        except OSError as error:\n            pass\n        fold_err = run(f, model_name)\n        performance[model_name].append(fold_err)\n        gc.collect()","22c14411":"# Introduction\n* I use pretrained RoBERTa-base model on downstream task\n* I use two models: mean pooling and attention on top of RoBERTa. Each model is trained on 5 folds. \n* To make the final predictions, I blend the predictions of each model of each fold\n\n## Inference Notebook\n### RoBERTa-large 5-fold single model (MeanPooling): \nhttps:\/\/www.kaggle.com\/jcesquiveld\/roberta-large-5-fold-single-model-meanpooling\n### CLRP: Pytorch Roberta Finetune\nhttps:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-finetune\n\nGive an upvote for this notebook and these notebooks are useful!"}}