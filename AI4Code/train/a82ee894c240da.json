{"cell_type":{"56e0209f":"code","15a7f5f1":"code","1ba03d55":"code","2d6944fd":"code","254a1cea":"code","ab7f81ed":"code","227e2cf8":"code","b7dc49f9":"code","4714df87":"code","b445dc6c":"markdown","502d7cd9":"markdown","6ba108c1":"markdown"},"source":{"56e0209f":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\n# Any results you write to the current directory are saved as output.","15a7f5f1":"df=pd.read_csv('..\/input\/fda_breast_cancer_kaggle.csv')\ndf.head()","1ba03d55":"plt.xlabel('weight (kg)') \nplt.ylabel('frequency') \nplt.title('Histogram of weight') \n#df['wt'].hist(figsize=[10,10] ,bins=25)\nparams = {'axes.titlesize':'25',\n          \n         'axes.labelsize': '25'}\nplt.rcParams.update(params)\ndf['wt'].hist(figsize=[10,10],bins=[0,20,40,50,60,70,80,100,120,140,160,200])  \nplt.grid(False)","2d6944fd":"plt.xlabel('age') \nplt.ylabel('frequency') \nplt.title('Histogram of age') \nparams = {'axes.titlesize':'25',\n          \n         'axes.labelsize': '25'}\nplt.rcParams.update(params)\ndf['age'].hist(figsize=[10,10],bins=[0,30,40,45,50,55,60,65,70,75,80,85,90,110])\nplt.grid(False)","254a1cea":"plt.xlabel('gender') \nplt.ylabel('frequency') \nplt.title('Histogram of gender') \nparams = {'axes.titlesize':'25',\n          \n         'axes.labelsize': '25'}\nplt.rcParams.update(params)\ndf['gndr_cod'].hist(figsize=[10,10])\nplt.grid(False)","ab7f81ed":"dummy_col = pd.get_dummies(df[['drug']])\ndf=df.drop(['drug','reporter_country','reaction'] , axis=1)\ndf = pd.concat([df, dummy_col], axis=1)\ndf.head()","227e2cf8":"y=df['de']\nX=df.drop(['de','lt','ho','ds','ca','ri','ot'], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0, stratify=y)\nclf1 = RandomForestClassifier(n_estimators=100, class_weight='balanced', n_jobs=-1, random_state=0)    #building 100 decision trees\nclf1.fit(X_train, y_train)\nprint (\"Death model\")\n#print \"oob score:\", clf1.oob_score_\nprint (metrics.accuracy_score(y_test, clf1.predict(X_test)))\n\nprint (metrics.confusion_matrix(y_test, clf1.predict(X_test)))\nTP=metrics.confusion_matrix(y_test, clf1.predict(X_test))[0,0]\nTN=metrics.confusion_matrix(y_test, clf1.predict(X_test))[1,1]\nFP=metrics.confusion_matrix(y_test, clf1.predict(X_test))[1,0]\nFN=metrics.confusion_matrix(y_test, clf1.predict(X_test))[0,1]\nprint (\"Sensitivity=\", TP\/(TP+FN))\nprint (\"Specificity=\", TN\/(TN+FP))\nprint (\"=======================================================\")\ny=df['lt']\nX=df.drop(['de','lt','ho','ds','ca','ri','ot'], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0, stratify=y)\nclf2 = RandomForestClassifier(n_estimators=100, class_weight='balanced', n_jobs=-1, random_state=0)    #building 100 decision trees\nclf2.fit(X_train, y_train)\nprint (\"Life threating model\")\n#print \"oob score:\", clf2.oob_score_\nprint (metrics.accuracy_score(y_test, clf2.predict(X_test)))\n\nprint (metrics.confusion_matrix(y_test, clf2.predict(X_test)))\nTP=metrics.confusion_matrix(y_test, clf2.predict(X_test))[0,0]\nTN=metrics.confusion_matrix(y_test, clf2.predict(X_test))[1,1]\nFP=metrics.confusion_matrix(y_test, clf2.predict(X_test))[1,0]\nFN=metrics.confusion_matrix(y_test, clf2.predict(X_test))[0,1]\nprint (\"Sensitivity=\", TP\/(TP+FN))\nprint (\"Specificity=\", TN\/(TN+FP))\nprint (\"=======================================================\")\n\ny=df['ho']\nX=df.drop(['de','lt','ho','ds','ca','ri','ot'], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0, stratify=y)\nclf3 = RandomForestClassifier(n_estimators=100, class_weight='balanced', n_jobs=-1, random_state=0)    #building 100 decision trees\nclf3.fit(X_train, y_train)\nprint (\"Hospitalization model\")\n#print \"oob score:\", clf3.oob_score_\nprint (metrics.accuracy_score(y_test, clf3.predict(X_test)))\n\nprint (metrics.confusion_matrix(y_test, clf3.predict(X_test)))\nTP=metrics.confusion_matrix(y_test, clf3.predict(X_test))[0,0]\nTN=metrics.confusion_matrix(y_test, clf3.predict(X_test))[1,1]\nFP=metrics.confusion_matrix(y_test, clf3.predict(X_test))[1,0]\nFN=metrics.confusion_matrix(y_test, clf3.predict(X_test))[0,1]\nprint (\"Sensitivity=\", TP\/(TP+FN))\nprint (\"Specificity=\", TN\/(TN+FP))\nprint (\"=======================================================\")\n\ny=df['ds']\nX=df.drop(['de','lt','ho','ds','ca','ri','ot'], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0, stratify=y)\nclf4 = RandomForestClassifier(n_estimators=100, class_weight='balanced', n_jobs=-1, random_state=0)    #building 100 decision trees\nclf4.fit(X_train, y_train)\nprint (\"Disability model\")\n#print \"oob score:\", clf4.oob_score_\nprint (metrics.accuracy_score(y_test, clf4.predict(X_test)))\n\nprint (metrics.confusion_matrix(y_test, clf4.predict(X_test)))\nTP=metrics.confusion_matrix(y_test, clf4.predict(X_test))[0,0]\nTN=metrics.confusion_matrix(y_test, clf4.predict(X_test))[1,1]\nFP=metrics.confusion_matrix(y_test, clf4.predict(X_test))[1,0]\nFN=metrics.confusion_matrix(y_test, clf4.predict(X_test))[0,1]\nprint (\"Sensitivity=\", TP\/(TP+FN))\nprint (\"Specificity=\", TN\/(TN+FP))\nprint (\"=======================================================\")\n\ny=df['ot']\nX=df.drop(['de','lt','ho','ds','ca','ri','ot'], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0, stratify=y)\nclf5 = RandomForestClassifier(n_estimators=100, class_weight='balanced', n_jobs=-1, random_state=0)    #building 100 decision trees\nclf5.fit(X_train, y_train)\nprint (\"Other Serious issues model\")\n#print \"oob score:\", clf5.oob_score_\nprint (metrics.accuracy_score(y_test, clf5.predict(X_test)))\n\nprint (metrics.confusion_matrix(y_test, clf5.predict(X_test)))\nTP=metrics.confusion_matrix(y_test, clf5.predict(X_test))[0,0]\nTN=metrics.confusion_matrix(y_test, clf5.predict(X_test))[1,1]\nFP=metrics.confusion_matrix(y_test, clf5.predict(X_test))[1,0]\nFN=metrics.confusion_matrix(y_test, clf5.predict(X_test))[0,1]\nprint (\"Sensitivity=\", TP\/(TP+FN))\nprint (\"Specificity=\", TN\/(TN+FP))\nprint (\"=======================================================\")\n\nX_test=X_test.reset_index(drop=True)","b7dc49f9":"\ndef drug_recommender(age,wt,gndr):\n    x=np.zeros(X_test.loc[0].shape)\n    out=pd.DataFrame(columns={'drug','score'})\n    for i in range(3,365):\n        x = np.zeros(x.shape)\n        prob=0.0\n        x[0]=age\n        x[1]=wt\n        x[2]=gndr\n        x[i]=1\n        val1=clf1.predict_proba(x.reshape(1, -1))[0][1]\n        val2=clf2.predict_proba(x.reshape(1, -1))[0][1]\n        val3=clf3.predict_proba(x.reshape(1, -1))[0][1]\n        val4=clf4.predict_proba(x.reshape(1, -1))[0][1]\n        val5=clf5.predict_proba(x.reshape(1, -1))[0][1]\n        for j in range(0,50):\n            w1=round(np.random.triangular(5,9,10))\n            w2=round(np.random.triangular(5,7,10))\n            w3=round(np.random.triangular(3,5,7))\n            w4=round(np.random.triangular(1,3,5))\n            w5=round(np.random.triangular(1,1,5))\n            pred=(w1*val1+w2*val2+w3*val3+w4*val4+w5*val5)\n            prob=prob+pred\n        out=out.append(pd.DataFrame({'drug':[X_test.columns[i]],'score':[prob\/50]}))\n    out=out.sort_values(['score'])\n    out['drug']=out['drug'].str.replace(\"drug_\", \"\")\n    print (\"==============================================\")\n    return out.head()","4714df87":"drug_recommender(57,90,0)","b445dc6c":"**Drug Recommender System for Patients Diagnosed with Breast Cancer**\nIn this notebook, I like to share the initial work on a simple dataset. The dataset, is downloaded from FAERS system provided by FDA. It consists of visits patients have made and used a certain type of drug and have reported symptoms (reactions) after using the drugs. It also consists of several binary outcomes such as death, hospitalization, disability , ... Predictive models can help to predict the single outcome for patients considering different features such as age, gender, weight, and drug. \nHere, we have performed several predictive models on outcomes with random forest classifier. Then, a recommender system has been made by using the classifiers as engine. The desire goal of the recommender system is to reduce the risk level patient may have. Therefore, it searches through the drug database and applies each in the system given the patients data. Then, sorts the drugs with the associated score. Score is coming from the positive class of each predictive model multiplied by a weight. The weight actually is subjective for each outcome. For example, death is more serious than hospitalization. Therefore, it has a larger weight. However, the weights have been considered in a triangular distribution to perform a simulation. Then, in a simulation, each time a weight is given considering an specific distribution. ","502d7cd9":"Data PreProcessing\nWe need to identify the features and our targets! Here we do not need the countries and reactions to predict the survivability risk of patients.","6ba108c1":"Drug Recommender System with Associated Weights on each Outcome to reduce the total risk level"}}