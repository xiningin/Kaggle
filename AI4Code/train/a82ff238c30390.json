{"cell_type":{"47caba53":"code","a7b99f32":"code","1b386012":"code","446b0c9f":"code","26e57450":"code","41488b0c":"code","3ad7a58b":"code","7a8e3068":"code","8edc2df0":"code","8e66f1bc":"code","f5e4d67e":"code","e1599f5f":"code","537813a9":"code","6a819411":"code","1d8e1a6f":"code","8e2402d5":"code","3c6d0e6a":"code","f83be0cd":"code","63c8529c":"code","371998e2":"code","992132a1":"code","5aa5a9c3":"code","babdb97a":"code","0ed9d313":"code","1c5b3493":"code","db521138":"code","3778ade5":"code","a2d9a005":"code","358c4200":"code","5f28f017":"code","f78e05fb":"code","6884916a":"code","f7aef727":"code","4721bb25":"code","d0bb01e9":"code","5f84784d":"code","146309ed":"code","5fcb2a8d":"code","edc7db74":"code","e9ef6186":"code","88cf99e8":"code","37216adc":"code","0861fecd":"code","fa7844e7":"code","0d4e9f37":"code","2f48cd6a":"code","7220e360":"code","29b7c0eb":"code","6dbf941d":"code","ed6ebbbb":"code","cd8b1575":"code","330102ca":"code","03be353a":"code","523c908e":"code","5bde8394":"code","13e1c07b":"code","eab1e78d":"code","220971c8":"code","1e10661d":"code","b54e1618":"code","cc0c3ba9":"code","f9a125ef":"code","e9073e6c":"code","a7c86113":"code","a65f9857":"code","f3060f5e":"code","987c20f7":"code","bd7e16bd":"code","5daacd81":"code","c60c08a3":"code","077fe10d":"code","8c24cb9d":"code","e58b2610":"code","a9fffa97":"code","1b742e98":"markdown","c4ec6571":"markdown","4dff7aaa":"markdown","da811546":"markdown","901074fe":"markdown","59282eda":"markdown","bbae7f78":"markdown","150c0343":"markdown","182b0801":"markdown","2e44605c":"markdown","dd2f6a80":"markdown","1170a7c4":"markdown","7de41811":"markdown","37ba5217":"markdown","f956ff17":"markdown","e0448f6c":"markdown","74f0783c":"markdown","e34a8af6":"markdown","81f4cd26":"markdown","fe43d395":"markdown"},"source":{"47caba53":"import os\nimport cv2\nimport json\nimport pickle\nimport random\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\n\nimport torchvision.models as models\nimport torchvision.transforms as transforms\n\nfrom pathlib import Path\nfrom datetime import datetime as dt\nfrom functools import partial\nfrom collections import Counter\n\nfrom PIL import Image\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.preprocessing import StandardScaler\n\nfrom tqdm import tqdm\n\npd.options.display.max_columns = 128\ntorch.multiprocessing.set_start_method(\"spawn\")","a7b99f32":"!cp ..\/input\/pytorch-pretrained-image-models\/* .\/\n!ls ","1b386012":"def jopen(path):\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        json_file = json.load(f)\n    return json_file\n\n\ndef parse_sentiment_file(path):\n    file = jopen(path)\n    language: str = file[\"language\"]\n\n    sentiment: list = file[\"documentSentiment\"]\n    entities: list = [x[\"name\"] for x in file[\"entities\"]]\n    entity = \" \".join(entities)\n\n    sentence_sentiment: list = [x[\"sentiment\"] for x in file[\"sentences\"]]\n    magnitude: np.ndarray = np.array(\n        [x[\"magnitude\"] for x in sentence_sentiment])\n    score: np.ndarray = np.array([x[\"score\"] for x in sentence_sentiment])\n\n    return_js = {\n        \"magnitude_sum\": magnitude.sum(),\n        \"magnitude_mean\": magnitude.mean(),\n        \"magnitude_var\": magnitude.var(),\n        \"score_sum\": score.sum(),\n        \"score_mean\": score.mean(),\n        \"score_var\": score.var(),\n        \"language\": language,\n        \"entity\": entity,\n        \"document_magnitude\": sentiment[\"magnitude\"],\n        \"document_score\": sentiment[\"score\"]\n    }\n    return return_js\n\n\ndef parse_metadata(path):\n    file: dict = jopen(path)\n    file_keys = list(file.keys())\n    if \"labelAnnotations\" in file_keys:\n        file_annots = file[\"labelAnnotations\"]\n        file_mean_score = np.asarray([x[\"score\"] for x in file_annots]).mean()\n        file_desc = \" \".join([x[\"description\"] for x in file_annots])\n    else:\n        file_mean_score = np.nan\n        file_desc = \"\"\n\n    file_colors: list = file[\"imagePropertiesAnnotation\"][\"dominantColors\"][\n        \"colors\"]\n    file_crops: list = file[\"cropHintsAnnotation\"][\"cropHints\"]\n\n    color_score = np.asarray([x[\"score\"] for x in file_colors]).mean()\n    pixel_frac = np.asarray([x[\"pixelFraction\"] for x in file_colors]).mean()\n    crop_conf = np.asarray([x[\"confidence\"] for x in file_crops]).mean()\n\n    if \"importanceFraction\" in file_crops[0].keys():\n        crop_importance = np.asarray(\n            [x[\"importanceFraction\"] for x in file_crops]).mean()\n    else:\n        crop_importance = np.nan\n    metadata = {\n        \"annot_score\": file_mean_score,\n        \"color_score\": color_score,\n        \"pixel_frac\": pixel_frac,\n        \"crop_conf\": crop_conf,\n        \"crop_importance\": crop_importance,\n        \"desc\": file_desc\n    }\n    return metadata\n\n\ndef additinal_features_per_id(pet_id, sentiment_path: Path, meta_path: Path):\n    sentiment_path = sentiment_path \/ f\"{pet_id}.json\"\n    try:\n        sentiment = parse_sentiment_file(sentiment_path)\n        sentiment[\"pet_id\"] = pet_id\n    except FileNotFoundError:\n        sentiment = {}\n\n    meta_files = sorted(meta_path.glob(f\"{pet_id}*.json\"))\n    metadata_list = []\n    if len(meta_files) > 0:\n        for f in meta_files:\n            metadata = parse_metadata(f)\n            metadata[\"pet_id\"] = pet_id\n            metadata_list.append(metadata)\n    return sentiment, metadata_list\n\n\ndef load_additional_features(ped_ids: list, sentiment_path: Path,\n                             meta_path: Path):\n    features = Parallel(\n        n_jobs=-1, verbose=1)(\n            delayed(additinal_features_per_id)(i, sentiment_path, meta_path)\n            for i in ped_ids)\n    sentiments = [x[0] for x in features if len(x[0]) > 0]\n    metadatas = [x[1] for x in features if len(x[1]) > 0]\n    sentiment_keys = sentiments[0].keys()\n    metadata_keys = metadatas[0][0].keys()\n    sentiment_dict = {}\n    metadata_dict = {}\n    for key in sentiment_keys:\n        sentiment_dict[key] = [x[key] for x in sentiments]\n\n    for key in metadata_keys:\n        meta_list = []\n        for meta_per_pid in metadatas:\n            meta_list += [meta[key] for meta in meta_per_pid]\n        metadata_dict[key] = meta_list\n\n    sentiment_df = pd.DataFrame(sentiment_dict)\n    metadata_df = pd.DataFrame(metadata_dict)\n    return sentiment_df, metadata_df\n\n\ndef aggregate_metadata(metadata_df: pd.DataFrame,\n                       aggregates=[\"sum\", \"mean\", \"var\"]):\n    meta_desc: pd.DataFrame = metadata_df.groupby([\"pet_id\"])[\"desc\"].unique()\n    meta_desc = meta_desc.reset_index()\n    meta_desc[\"desc\"] = meta_desc[\"desc\"].apply(lambda x: \" \".join(x))\n\n    meta_gr: pd.DataFrame = metadata_df.drop([\"desc\"], axis=1)\n    for i in meta_gr.columns:\n        if \"pet_id\" not in i:\n            meta_gr[i] = meta_gr[i].astype(float)\n    meta_gr = meta_gr.groupby([\"pet_id\"]).agg(aggregates)\n    meta_gr.columns = pd.Index(\n        [f\"{c[0]}_{c[1].upper()}\" for c in meta_gr.columns.tolist()])\n    meta_gr = meta_gr.reset_index()\n    return meta_gr, meta_desc\n\n\ndef aggregate_sentiment(sentiment_df: pd.DataFrame, aggregates=[\"sum\"]):\n    sentiment_desc: pd.DataFrame = sentiment_df.groupby(\n        [\"pet_id\"])[\"entity\"].unique()\n    sentiment_desc = sentiment_desc.reset_index()\n    sentiment_desc[\"entity\"] = sentiment_desc[\"entity\"].apply(\n        lambda x: \" \".join(x))\n    sentiment_lang = sentiment_df.groupby(\n        [\"pet_id\"])[\"language\"].unique()\n    sentiment_lang = sentiment_lang.reset_index()\n    sentiment_lang[\"language\"] = sentiment_lang[\"language\"].apply(\n        lambda x: \" \".join(x))\n    sentiment_desc = sentiment_desc.merge(\n        sentiment_lang, how=\"left\", on=\"pet_id\")\n    \n\n    sentiment_gr: pd.DataFrame = sentiment_df.drop([\"entity\", \"language\"],\n                                                   axis=1)\n    for i in sentiment_gr.columns:\n        if \"pet_id\" not in i:\n            sentiment_gr[i] = sentiment_gr[i].astype(float)\n    sentiment_gr = sentiment_gr.groupby([\"pet_id\"]).agg(aggregates)\n    sentiment_gr.columns = pd.Index(\n        [f\"{c[0]}\" for c in sentiment_gr.columns.tolist()])\n    sentiment_gr = sentiment_gr.reset_index()\n    return sentiment_gr, sentiment_desc","446b0c9f":"input_dir = Path(\"..\/input\/petfinder-adoption-prediction\/\")\ntrain = pd.read_csv(input_dir \/ \"train\/train.csv\")\ntest = pd.read_csv(input_dir \/ \"test\/test.csv\")\nsample_submission = pd.read_csv(input_dir \/ \"test\/sample_submission.csv\")","26e57450":"sp_train = input_dir \/ Path(\"train_sentiment\/\")\nmp_train = input_dir \/ Path(\"train_metadata\/\")\nsp_test = input_dir \/ Path(\"test_sentiment\/\")\nmp_test = input_dir \/ Path(\"test_metadata\/\")","41488b0c":"train_pet_ids = train.PetID.unique()\ntest_pet_ids = test.PetID.unique()","3ad7a58b":"train_sentiment_df, train_metadata_df = load_additional_features(\n    train_pet_ids, sp_train, mp_train)\n\ntest_sentiment_df, test_metadata_df = load_additional_features(\n    test_pet_ids, sp_test, mp_test)","7a8e3068":"train_meta_gr, train_meta_desc = aggregate_metadata(train_metadata_df)\ntest_meta_gr, test_meta_desc = aggregate_metadata(test_metadata_df)\ntrain_sentiment_gr, train_sentiment_desc = \\\n    aggregate_sentiment(train_sentiment_df)\ntest_sentiment_gr, test_sentiment_desc = \\\n    aggregate_sentiment(test_sentiment_df)","8edc2df0":"train_proc = train.copy()\ntrain_proc = train_proc.merge(\n    train_sentiment_gr, how=\"left\", left_on=\"PetID\", right_on=\"pet_id\")\ntrain_proc = train_proc.merge(\n    train_meta_gr, how=\"left\", left_on=\"PetID\", right_on=\"pet_id\")\ntrain_proc = train_proc.merge(\n    train_sentiment_desc, how=\"left\", left_on=\"PetID\", right_on=\"pet_id\")\ntrain_proc = train_proc.merge(\n    train_meta_desc, how=\"left\", left_on=\"PetID\", right_on = \"pet_id\")\n\ntest_proc = test.copy()\ntest_proc = test_proc.merge(\n    test_sentiment_gr, how=\"left\", left_on=\"PetID\", right_on=\"pet_id\")\ntest_proc = test_proc.merge(\n    test_meta_gr, how=\"left\", left_on=\"PetID\", right_on=\"pet_id\")\ntest_proc = test_proc.merge(\n    test_sentiment_desc, how=\"left\", left_on=\"PetID\", right_on=\"pet_id\")\ntest_proc = test_proc.merge(\n    test_meta_desc, how=\"left\", left_on=\"PetID\", right_on = \"pet_id\")","8e66f1bc":"print(train_proc.shape, test_proc.shape)\nassert train_proc.shape[0] == train.shape[0]\nassert test_proc.shape[0] == test.shape[0]","f5e4d67e":"train_proc.drop(train_proc.filter(\n    regex=\"pet_id\", axis=1).columns.tolist(), \n    axis=1, \n    inplace=True)\n\ntest_proc.drop(test_proc.filter(\n    regex=\"pet_id\", axis=1).columns.tolist(),\n    axis=1,\n    inplace=True)\n\ntrain_proc.head()","e1599f5f":"train_proc.language.fillna(\"\", inplace=True)\ntest_proc.language.fillna(\"\", inplace=True)\n\nlangs = train_proc.language.unique()\nencode_dict = {k: i for i, k in enumerate(langs)}\n\ntrain_proc.language = train_proc.language.map(encode_dict)\ntest_proc.language = test_proc.language.map(encode_dict)","537813a9":"X = pd.concat([train_proc, test_proc], ignore_index=True, sort=False)\nX_temp = X.copy()\n\ntext_columns = [\n    \"Description\",\n    \"entity\",\n    \"desc\"]\ncategorical_columns = [\n    \"Type\", \"Breed1\", \"Breed2\", \"Gender\",\n    \"Color1\", \"Color2\", \"Color3\", \"MaturitySize\",\n    \"FurLength\", \"Vaccinated\", \"Dewormed\", \"Sterilized\",\n    \"State\", \"language\"\n]\ndrop_columns = [\n    \"PetID\", \"Name\", \"RescuerID\"\n]","6a819411":"rescuer_count = X.groupby([\"RescuerID\"])[\"PetID\"].count().reset_index()\nrescuer_count.columns = [\"RescuerID\", \"RescuerID_COUNT\"]\n\nX_temp = X_temp.merge(rescuer_count, how=\"left\", on=\"RescuerID\")","1d8e1a6f":"X_text = X_temp[text_columns]\nfor i in X_text.columns:\n    X_text.loc[:, i] = X_text.loc[:, i].fillna(\"none\")","8e2402d5":"X_temp[\"len_description\"] = X_text[\"Description\"].map(len)\nX_temp[\"len_meta_desc\"] = X_text[\"desc\"].map(len)\nX_temp[\"len_entity\"] = X_text[\"entity\"].map(len)","3c6d0e6a":"n_components = 16\ntext_features = []\n\nfor i in X_text.columns:\n    print(f\"generating features from: {i}\")\n    tfv = TfidfVectorizer(\n        min_df=2,\n        strip_accents=\"unicode\",\n        analyzer=\"word\",\n        token_pattern=r\"(?u)\\b\\w+\\b\",\n        ngram_range=(1, 3),\n        use_idf=1,\n        smooth_idf=1,\n        sublinear_tf=1)\n    svd = TruncatedSVD(\n        n_components=n_components,\n        random_state=1337)\n    tfidf_col = tfv.fit_transform(X_text.loc[:, i].values)\n    svd_col = svd.fit_transform(tfidf_col)\n    svd_col = pd.DataFrame(svd_col)\n    svd_col = svd_col.add_prefix(\"Tfidf_{}_\".format(i))\n    \n    text_features.append(svd_col)\n\ntext_features = pd.concat(text_features, axis=1)\nX_temp = pd.concat([X_temp, text_features], axis=1)\n\nfor i in X_text.columns:\n    X_temp.drop(i, axis=1, inplace=True)","f83be0cd":"import os\nimport glob\n\ntrain_image_files = sorted(\n    glob.glob(\"..\/input\/petfinder-adoption-prediction\/train_images\/*.jpg\"))\ntest_image_files = sorted(\n    glob.glob(\"..\/input\/petfinder-adoption-prediction\/test_images\/*.jpg\"))\n\ntrain_df_imgs = pd.DataFrame(train_image_files)\ntest_df_imgs = pd.DataFrame(test_image_files)\ntrain_df_imgs.columns = [\"image_file_name\"]\ntest_df_imgs.columns = [\"image_file_name\"]\n\ntrain_imgs_pets = train_df_imgs[\"image_file_name\"].apply(\n    lambda x: x.split(\"\/\")[-1].split(\"-\")[0])\ntest_imgs_pets = test_df_imgs[\"image_file_name\"].apply(\n    lambda x: x.split(\"\/\")[-1].split(\"-\")[0])\ntrain_df_imgs = train_df_imgs.assign(PetID=train_imgs_pets)\ntest_df_imgs = test_df_imgs.assign(PetID=test_imgs_pets)\n\ndef get_size(filename):\n    st = os.stat(filename)\n    return st.st_size\n\ndef get_dimensions(filename):\n    img_size = Image.open(filename).size\n    return img_size\n\ntrain_df_imgs[\"image_size\"] = train_df_imgs[\"image_file_name\"].apply(get_size)\ntest_df_imgs[\"image_size\"] = test_df_imgs[\"image_file_name\"].apply(get_size)\ntrain_df_imgs[\"temp_size\"] = train_df_imgs[\"image_file_name\"].apply(get_dimensions)\ntest_df_imgs[\"temp_size\"] = test_df_imgs[\"image_file_name\"].apply(get_dimensions)\ntrain_df_imgs[\"width\"] = train_df_imgs[\"temp_size\"].apply(lambda x: x[0])\ntest_df_imgs[\"width\"] = test_df_imgs[\"temp_size\"].apply(lambda x: x[0])\ntrain_df_imgs[\"height\"] = train_df_imgs[\"temp_size\"].apply(lambda x: x[1])\ntest_df_imgs[\"height\"] = test_df_imgs[\"temp_size\"].apply(lambda x: x[1])\ntrain_df_imgs.drop([\"temp_size\"], axis=1, inplace=True)\ntest_df_imgs.drop([\"temp_size\"], axis=1, inplace=True)\n\naggs = {\n    \"image_size\": [\"sum\", \"mean\", \"var\"],\n    \"width\": [\"sum\", \"mean\", \"var\"],\n    \"height\": [\"sum\", \"mean\", \"var\"]\n}\nagg_train_imgs = train_df_imgs.groupby(\"PetID\").agg(aggs)\nnew_columns = [\n    k + \"_\" + agg for k in aggs.keys() for agg in aggs[k]\n]\nagg_test_imgs = test_df_imgs.groupby(\"PetID\").agg(aggs)\nagg_train_imgs.columns = new_columns\nagg_train_imgs = agg_train_imgs.reset_index()\n\nagg_test_imgs.columns = new_columns\nagg_test_imgs = agg_test_imgs.reset_index()\n","63c8529c":"agg_imgs = pd.concat([agg_train_imgs, agg_test_imgs], axis=0).reset_index(drop=True)\nX_temp = X_temp.merge(agg_imgs, how=\"left\", on=\"PetID\")\nX_temp.head()","371998e2":"X_temp.drop(drop_columns, axis=1, inplace=True)\nX_temp.head()","992132a1":"for c in new_columns:\n    X_temp.loc[:, c] = X_temp[c].map(lambda x: np.log1p(x))\n    \nX_temp.head()","5aa5a9c3":"X_train = X_temp.loc[np.isfinite(X_temp.AdoptionSpeed), :]\nX_test = X_temp.loc[~np.isfinite(X_temp.AdoptionSpeed), :]\nX_test.drop([\"AdoptionSpeed\"], axis=1, inplace=True)\n\nassert X_train.shape[0] == train.shape[0]\nassert X_test.shape[0] == test.shape[0]","babdb97a":"train_cols = X_train.columns.tolist()\ntrain_cols.remove(\"AdoptionSpeed\")\n\ntest_cols = X_test.columns.tolist()\nassert np.all(train_cols == test_cols)","0ed9d313":"X_train_non_null = X_train.fillna(-1)\nX_test_non_null = X_test.fillna(-1)\n\nX_train_non_null.isnull().any().any(), X_test_non_null.isnull().any().any()","1c5b3493":"X_train_non_null.Fee = X_train_non_null.Fee.map(lambda x: np.log1p(x))\nX_test_non_null.Fee = X_test_non_null.Fee.map(lambda x: np.log1p(x))\n\nX_train_non_null.RescuerID_COUNT = X_train_non_null.RescuerID_COUNT.map(lambda x: np.log(x))\nX_test_non_null.RescuerID_COUNT = X_test_non_null.RescuerID_COUNT.map(lambda x: np.log(x))\n","db521138":"cat_features = [\"Type\", \"Breed1\", \"Breed2\", \"Gender\", \"Color1\", \"Color2\", \"Color3\",\n                \"MaturitySize\", \"FurLength\", \"Vaccinated\", \"Dewormed\", \"Sterilized\",\n                \"Health\", \"Quantity\", \"State\", \"language\"]\n\nX_train_cat = X_train_non_null.loc[:, cat_features]\nX_test_cat = X_test_non_null.loc[:, cat_features]","3778ade5":"y = X_train_non_null.AdoptionSpeed\nX_train_cat_y = X_train_cat.copy()\nX_train_cat_y[\"AdoptionSpeed\"] = y\n\nkfold = GroupKFold(n_splits=5)\nX_train_cat_encoded = np.zeros((X_train_cat.shape[0], len(cat_features) * 2))\nX_test_cat_encoded = np.zeros((X_test_cat.shape[0], len(cat_features) * 2))\n\nfor trn_idx, val_idx in kfold.split(X_train_cat, y, train_pet_ids):\n    X_trn, X_val = X_train_cat_y.loc[trn_idx, :], X_train_cat_y.loc[val_idx, :]\n    for j, c in enumerate(cat_features):\n        X_trn_enc = X_trn.groupby(c).agg({\n            \"AdoptionSpeed\": [\"mean\", \"std\"]\n        })\n        cte_columns = [f\"{c}_{x}\" for x in [\"mean\", \"std\"]]\n        X_trn_enc.columns = cte_columns\n        X_temp = np.zeros((X_test_cat.shape[0], 2))\n        X_temp_df = pd.DataFrame(data=X_temp, columns=cte_columns)\n        for x in X_trn_enc.columns:\n            X_val[x] = X_val[c].map(X_trn_enc[x])\n            X_temp_df[x] = X_test_cat[c].map(X_trn_enc[x]).reset_index(drop=True)\n        X_train_cat_encoded[val_idx, 2 * j:2 * (j + 1)] = X_val[X_trn_enc.columns].values\n        X_test_cat_encoded[:, 2 * j: 2 * (j + 1)] += X_temp_df.values \/ 5","a2d9a005":"X_train_cat_encoded.shape, X_test_cat_encoded.shape","358c4200":"columns = [f\"{c}_{x}\" for c in cat_features for x in [\"mean\", \"std\"]]\nX_train_cat_encoded_df = pd.DataFrame(data=X_train_cat_encoded, columns=columns)\nX_test_cat_encoded_df = pd.DataFrame(data=X_test_cat_encoded, columns=columns)\nX_test_cat_encoded_df.head()","5f28f017":"X_train_num = X_train_non_null.drop(cat_features + [\"AdoptionSpeed\"], axis=1)\nX_test_num = X_test_non_null.drop(cat_features, axis=1)\n\ntarget = X_train_non_null[\"AdoptionSpeed\"]","f78e05fb":"X_train_num = pd.concat([X_train_num, X_train_cat_encoded_df], axis=1)\n\nX_test_cat_encoded_df.index = X_test_num.index\nX_test_num = pd.concat([X_test_num, X_test_cat_encoded_df], axis=1)","6884916a":"X_all_num = pd.concat([X_train_num, X_test_num], axis=0)\nss = StandardScaler()\nss.fit(X_all_num)\n\nX_train_ss = ss.transform(X_train_num)\nX_test_ss = ss.transform(X_test_num)","f7aef727":"X_train_ss[np.isnan(X_train_ss)] = 0.0\nX_test_ss[np.isnan(X_test_ss)] = 0.0","4721bb25":"cat_cat = pd.concat([X_train_cat, X_test_cat])\n\nn_breed1 = cat_cat[\"Breed1\"].nunique()\nn_breed2 = cat_cat[\"Breed2\"].nunique()\nn_langs = cat_cat[\"language\"].nunique()\nn_color1 = cat_cat[\"Color1\"].nunique()\nn_color2 = cat_cat[\"Color2\"].nunique()\nn_color3 = cat_cat[\"Color3\"].nunique()","d0bb01e9":"from sklearn.preprocessing import LabelEncoder\nfor c in X_train_cat.columns:\n        le = LabelEncoder()\n        le.fit(cat_cat[c])\n        X_train_cat[c] = le.transform(X_train_cat[c])\n        X_test_cat[c] = le.transform(X_test_cat[c])","5f84784d":"class OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n    \n    def _kappa_loss(self, coef, X, y):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return -cohen_kappa_score(y, preds, weights='quadratic')\n    \n    def fit(self, X, y, initial_coef=[]):\n        loss_partial = partial(self._kappa_loss, X = X, y = y)\n        initial_coef = initial_coef\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n    \n    def predict(self, X, coef):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return preds\n    \n    def coefficients(self):\n        return self.coef_['x']\n    \ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    assert len(rater_a) == len(rater_b)\n\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_rating = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_rating)] for j in range(num_rating)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n    rater_a = y\n    rater_b = y_pred\n    min_rating = None\n    max_rating = None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n\n    assert len(rater_a) == len(rater_b)\n\n    min_rating = min(min(rater_a), min(rater_b))\n    max_rating = max(max(rater_a), max(rater_b))\n\n    conf_mat = confusion_matrix(rater_a, rater_b, min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (\n                hist_rater_a[i] * hist_rater_b[j]) \/ num_scored_items\n            d = pow(i - j, 2.0) \/ pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] \/ num_scored_items\n            denominator += d * expected_count \/ num_scored_items\n\n    return (1.0 - numerator \/ denominator)","146309ed":"class PetDataset(data.Dataset):\n    def __init__(self, \n                 pet_ids, \n                 cat_features, \n                 num_features, \n                 labels, \n                 root_dir, \n                 subset=False, \n                 transform=None):\n        self.pet_ids = pet_ids\n        self.cat_features = cat_features\n        self.num_features = num_features\n        if labels is not None:\n            self.labels = labels\n        else:\n            self.labels = None\n        self.root_dir = root_dir\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.pet_ids)\n    \n    def __getitem__(self, idx):\n        img_name = f\"{self.pet_ids[idx]}-1.jpg\"\n        fullname = self.root_dir \/ Path(img_name)\n        try:\n            image = Image.open(fullname).convert(\"RGB\")\n        except FileNotFoundError:\n            image = np.zeros((3, 224, 224), dtype=np.uint8).transpose(1, 2, 0)\n            image = Image.fromarray(np.uint8(image))\n        cat_feature = self.cat_features[idx]\n        num_feature = self.num_features[idx]\n        if self.transform:\n            image = self.transform(image)\n\n        if self.labels is not None:\n            label = self.labels[idx]\n            return [image, cat_feature, num_feature, label]\n        else:\n            return [image, cat_feature, num_feature]","5fcb2a8d":"normalize = transforms.Normalize(\n    mean=[0.485, 0.456, 0.406],\n    std=[0.229, 0.224, 0.225]\n)\nds_trans = transforms.Compose([transforms.Resize(224),\n                               transforms.CenterCrop(224),\n                               transforms.ToTensor(),\n                               normalize])","edc7db74":"def get_n_params(model):\n    pp=0\n    for p in list(model.parameters()):\n        nn=1\n        for s in list(p.size()):\n            nn = nn*s\n        pp += nn\n    return pp\n\ndef seed_torch(seed=1029):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\ndef run_xgb(params,\n            X,\n            y,\n            X_test,\n            resc,\n            n_splits=10,\n            num_rounds=60000,\n            early_stop=500,\n            verbose_eval=1000):\n    fold = GroupKFold(n_splits=n_splits)\n    oof_train = np.zeros((X.shape[0]))\n    oof_test = np.zeros((X_test.shape[0], n_splits))\n\n    for i, (trn_index, val_index) in enumerate(fold.split(X, y, train_pet_ids)):\n        X_tr = X.iloc[trn_index, :]\n        X_val = X.iloc[val_index, :]\n\n        y_tr = y[trn_index]\n        y_val = y[val_index]\n        d_train = xgb.DMatrix(\n            data=X_tr, label=y_tr, feature_names=X_tr.columns)\n        d_valid = xgb.DMatrix(\n            data=X_val, label=y_val, feature_names=X_val.columns)\n\n        watchlist = [(d_train, \"train\"), (d_valid, \"valid\")]\n        model = xgb.train(\n            params=params,\n            dtrain=d_train,\n            num_boost_round=num_rounds,\n            evals=watchlist,\n            early_stopping_rounds=early_stop,\n            verbose_eval=verbose_eval)\n        valid_pred = model.predict(\n            xgb.DMatrix(X_val, feature_names=X_val.columns),\n            ntree_limit=model.best_ntree_limit)\n        test_pred = model.predict(\n            xgb.DMatrix(X_test, feature_names=X_test.columns),\n            ntree_limit=model.best_ntree_limit)\n        oof_train[val_index] = valid_pred\n        oof_test[:, i] = test_pred\n    return model, oof_train, oof_test\n\n\nclass Trainer:\n    def __init__(self, \n                 model,\n                 resc,\n                 n_splits=5, \n                 seed=42, \n                 device=\"cuda:0\", \n                 train_batch=16,\n                 val_batch=32,\n                 kwargs={}):\n        self.model = model\n        self.resc = resc\n        self.n_splits = n_splits\n        self.seed = seed\n        self.device = device\n        self.train_batch = train_batch\n        self.val_batch = val_batch\n        self.kwargs = kwargs\n        \n        self.fold = GroupKFold(\n            n_splits=n_splits)\n        self.best_score = None\n        self.tag = dt.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n        \n        self.loss_fn = nn.MSELoss(reduction=\"mean\").to(self.device)\n        path = Path(f\"bin\/{self.tag}\")\n        path.mkdir(exist_ok=True, parents=True)\n        self.path = path\n        \n    def fit(self, pet_ids, cat_feats, num_feats, answer, n_epochs=30):\n        self.train_preds = np.zeros((train.shape[0]))\n        answer = answer.values\n        cat_feats = cat_feats.values\n        for i, (trn_idx, val_idx) in enumerate(self.fold.split(pet_ids, answer, self.resc)):\n            self.fold_num = i\n            print(f\"Fold: {i+1}\")\n            pid_train, pid_val = pet_ids[trn_idx], pet_ids[val_idx]\n            cat_train, cat_val = cat_feats[trn_idx], cat_feats[val_idx]\n            num_train, num_val = num_feats[trn_idx], num_feats[val_idx]\n            y_train, y_val = answer[trn_idx] \/ 4, answer[val_idx] \/ 4\n            \n            valid_preds = self._fit(pid_train, \n                                    cat_train, \n                                    num_train, \n                                    y_train,\n                                    n_epochs,\n                                    pid_val,\n                                    cat_val,\n                                    num_val,\n                                    y_val\n                                   )\n            self.train_preds[val_idx] = valid_preds\n        \n    def _fit(self, pid, cat, num, y, n_epochs, pid_val, cat_val, num_val, y_val):\n        seed_torch(self.seed)\n        cat_tensor = torch.tensor(cat, dtype=torch.long).to(self.device)\n        num_tensor = torch.tensor(num, dtype=torch.float32).to(self.device)\n        y_tensor = torch.tensor(y[:, np.newaxis], dtype=torch.float32).to(self.device)\n        train = PetDataset(pid, \n                           cat_tensor, \n                           num_tensor, \n                           y_tensor,\n                           \"..\/input\/petfinder-adoption-prediction\/train_images\/\",\n                           transform=ds_trans)\n        train_loader = data.DataLoader(train, \n                                       batch_size=self.train_batch, shuffle=True)\n        cat_eval = torch.tensor(cat_val, dtype=torch.long).to(self.device)\n        num_eval = torch.tensor(num_val, dtype=torch.float32).to(self.device)\n        y_eval = torch.tensor(y_val[:, np.newaxis], dtype=torch.float32).to(self.device)\n        eval_ = PetDataset(pid_val,\n                           cat_eval,\n                           num_eval,\n                           y_eval,\n                           \"..\/input\/petfinder-adoption-prediction\/train_images\/\",\n                           transform=ds_trans)\n        eval_loader = data.DataLoader(eval_,\n                                      batch_size=self.val_batch, shuffle=False)\n        \n        model = self.model(**self.kwargs)\n        model = model.to(self.device)\n        optimizer = optim.Adam(model.parameters())\n        best_score = np.inf\n        \n        for epoch in range(n_epochs):\n            model.train()\n            avg_loss = 0.\n            for i_batch, c_batch, n_batch, y_batch in tqdm(train_loader):\n                i_batch = i_batch.to(self.device)\n                y_pred = model(i_batch, c_batch, n_batch)\n                loss = self.loss_fn(y_pred, y_batch)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                avg_loss += loss.item() \/ len(train_loader)\n            valid_preds, avg_val_loss = self._val(eval_loader, model)\n            print(f\"epoch {epoch+1}\/{n_epochs}\")\n            print(f\"avg_loss: {avg_loss:.4f}\")\n            print(f\"avg_val_loss: {avg_val_loss:.4f}\")\n            if best_score > avg_val_loss:\n                torch.save(model.state_dict(),\n                           self.path \/ f\"best{self.fold_num}.pt\")\n                print(f\"Save model on epoch {epoch + 1}\")\n                best_score = avg_val_loss\n        model.load_state_dict(torch.load(self.path \/ f\"best{self.fold_num}.pt\"))\n        valid_preds, avg_val_loss = self._val(eval_loader, model)\n        print(f\"Validation loss: {avg_val_loss}\")\n        return valid_preds\n    \n    def _val(self, loader, model):\n        model.eval()\n        valid_preds = np.zeros(loader.dataset.cat_features.size(0))\n        avg_val_loss = 0.\n\n        for i, (i_batch, c_batch, n_batch, y_batch) in enumerate(loader):\n            with torch.no_grad():\n                i_batch = i_batch.to(self.device)\n                y_pred = model(i_batch, c_batch, n_batch).detach()\n                avg_val_loss += self.loss_fn(y_pred,\n                                             y_batch).item() \/ len(loader)\n                valid_preds[i * self.val_batch:(i + 1) * self.val_batch] = y_pred.cpu().numpy()[:, 0]\n        return valid_preds, avg_val_loss\n        ","e9ef6186":"class NeuralNet(nn.Module):\n    def __init__(self, path, emb_dims, num_dims, img_linear, linear_size):\n        super(NeuralNet, self).__init__()\n        self.densenet121 = models.densenet121()\n        self.densenet121.load_state_dict(torch.load(path))\n        self.densenet121.classifier = nn.Linear(1024, img_linear)\n        dense = nn.Sequential(*list(self.densenet121.children())[:-1])\n        for param in dense.parameters():\n            param.requires_grad = False\n\n        self.embeddings = nn.ModuleList(\n            [nn.Embedding(x, y) for x, y in emb_dims])\n        n_emb_out = sum([y for x, y in emb_dims])\n        self.fc1 = nn.Linear(img_linear + n_emb_out + num_dims, linear_size)\n        self.bn1 = nn.BatchNorm1d(linear_size)\n        self.fc2 = nn.Linear(linear_size, 1)\n        self.drop = nn.Dropout(0.2)\n        self.relu = nn.ReLU()\n\n    def forward(self, i, c, n):\n        img_feats = self.densenet121(i)\n        emb = [\n            emb_layer(c[:, j]) for j, emb_layer in enumerate(self.embeddings)\n        ]\n        emb = torch.cat(emb, 1)\n        data = torch.cat([img_feats, emb, n], 1)\n        out = self.relu(self.fc1(data))\n        out = self.bn1(out)\n        out = self.drop(out)\n        out = self.fc2(out)\n        return out","88cf99e8":"emb_dims = [(2, 1), (n_breed1, 3), (n_breed2, 3), (3, 1), (n_color1, 1),\n            (n_color2, 1), (n_color3, 1), (4, 1), (3, 1), (3, 1),\n            (3, 1), (3, 1), (3, 1), (19, 1), (14, 1),(n_langs, 1)]\nnum_dims = X_test_ss.shape[1]\ntrainer = Trainer(\n    NeuralNet,\n    resc=train_pet_ids,\n    n_splits=4,\n    train_batch=128,\n    val_batch=128,\n    kwargs={\n        \"path\": \"densenet121.pth\",\n        \"emb_dims\": emb_dims,\n        \"num_dims\": num_dims,\n        \"img_linear\": 48,\n        \"linear_size\": 120\n    })","37216adc":"trainer.fit(train_pet_ids, X_train_cat, X_train_ss, target, 5)","0861fecd":"bin_path = trainer.path\ntest_preds = np.zeros((X_test_cat.shape[0]))\nc_tensor = torch.tensor(X_test_cat.values, dtype=torch.long).to(trainer.device)\nn_tensor = torch.tensor(X_test_ss, dtype=torch.float32).to(trainer.device)\ntest_dataset = PetDataset(test_pet_ids, \n                          c_tensor, \n                          n_tensor, \n                          labels=None, \n                          root_dir=\"..\/input\/petfinder-adoption-prediction\/test_images\/\",\n                          transform=ds_trans)\ntest_loader = data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n\nfor path in bin_path.iterdir():\n    print(f\"using {str(path)}\")\n    model = NeuralNet(**trainer.kwargs)\n    model.to(\"cuda:0\")\n    model.load_state_dict(torch.load(path))\n\n    model.eval()\n    temp = np.zeros((X_test_cat.shape[0]))\n    for i, (i_batch, c_batch, n_batch) in enumerate(test_loader):\n        i_batch = i_batch.to(trainer.device)\n        with torch.no_grad():\n            y_pred = model(i_batch, c_batch, n_batch).detach()\n            temp[i * 128:(i + 1) * 128] = y_pred.cpu().numpy()[:, 0]\n    test_preds += temp \/ trainer.n_splits","fa7844e7":"class ImageDataset(data.Dataset):\n    def __init__(self, pet_ids, root_dir, transform=None):\n        self.pet_ids = pet_ids\n        self.root_dir = root_dir\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.pet_ids)\n    \n    def __getitem__(self, idx):\n        img_name = f\"{self.pet_ids[idx]}-1.jpg\"\n        fullname = self.root_dir \/ Path(img_name)\n        try:\n            image = Image.open(fullname).convert(\"RGB\")\n        except FileNotFoundError:\n            image = np.zeros((3, 224, 224), dtype=np.uint8).transpose(1, 2, 0)\n            image = Image.fromarray(np.uint8(image))\n        if self.transform:\n            image = self.transform(image)\n            \n        return [image]","0d4e9f37":"train_dataset = ImageDataset(\n    train_pet_ids,\n    \"..\/input\/petfinder-adoption-prediction\/train_images\/\",\n    transform=ds_trans)\nbatch = 256\nn_img_dim = 48\ntrain_loader = data.DataLoader(train_dataset,\n                               batch_size=batch,\n                               shuffle=False)\nX_train_img = np.zeros((len(train_pet_ids), n_img_dim))\n\ntest_dataset = ImageDataset(\n    test_pet_ids,\n    \"..\/input\/petfinder-adoption-prediction\/test_images\/\",\n    transform=ds_trans)\ntest_loader = data.DataLoader(test_dataset,\n                              batch_size=batch,\n                              shuffle=False)\nX_test_img = np.zeros((len(test_pet_ids), n_img_dim))\nbin_path = trainer.path\nfor path in bin_path.iterdir():\n    model = NeuralNet(**trainer.kwargs)\n    model.to(\"cuda:0\")\n    model.load_state_dict(torch.load(path))\n    model.eval()\n    temp = np.zeros((len(train_pet_ids), n_img_dim))\n    \n    for i, (i_batch, ) in tqdm(enumerate(train_loader)):\n        with torch.no_grad():\n            i_batch = i_batch.to(\"cuda:0\")\n            y_pred = model.densenet121(i_batch).detach()\n            temp[i * batch:(i + 1) * batch, :] = y_pred.cpu().numpy()\n    X_train_img += temp \/ trainer.n_splits\n    \n    temp = np.zeros((len(test_pet_ids), n_img_dim))\n    for i, (i_batch, ) in tqdm(enumerate(test_loader)):\n        with torch.no_grad():\n            i_batch = i_batch.to(\"cuda:0\")\n            y_pred = model.densenet121(i_batch).detach()\n            temp[i * batch:(i + 1) * batch, :] = y_pred.cpu().numpy()\n    X_test_img += temp \/ trainer.n_splits","2f48cd6a":"X_train_img.shape, X_test_img.shape","7220e360":"train_img = pd.DataFrame(data=X_train_img, columns=[\n    f\"img{i}\" for i in range(X_train_img.shape[1])\n])\ntest_img = pd.DataFrame(data=X_test_img, columns=[\n    f\"img{i}\" for i in range(X_test_img.shape[1])\n])","29b7c0eb":"num_columns = X_train_num.columns\nX_train_num_df = pd.DataFrame(data=X_train_ss, columns=num_columns)\nX_test_num_df = pd.DataFrame(data=X_test_ss, columns=num_columns)\n\nX_test_num_df.index = X_test_cat.index\ntest_img.index = X_test_cat.index\n\nX_train_all = pd.concat([X_train_num_df, X_train_cat, train_img], axis=1)\nX_test_all = pd.concat([X_test_num_df, X_test_cat, test_img], axis=1)\n\nprint(X_train_all.shape, X_test_all.shape)\nX_train_all.head()","6dbf941d":"\"AdoptionSpeed\" in X_train_all.columns, \"AdoptionSpeed\" in X_test_all.columns","ed6ebbbb":"X_train_all.columns.tolist() == X_test_all.columns.tolist()","cd8b1575":"class CategoryDataset(data.Dataset):\n    def __init__(self, category):\n        self.category = category\n        \n    def __len__(self):\n        return len(self.category)\n    \n    def __getitem__(self, idx):\n        category = self.category[idx, :]\n        return [category]","330102ca":"c_train = torch.tensor(X_train_cat.values, dtype=torch.long).to(\"cuda:0\")\nc_test = torch.tensor(X_test_cat.values, dtype=torch.long).to(\"cuda:0\")\ntrain_dataset = CategoryDataset(c_train)\ntest_dataset = CategoryDataset(c_test)\ntrain_loader = data.DataLoader(train_dataset, batch_size=128, shuffle=False)\ntest_loader = data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n\nX_train_cat_ = np.zeros((len(train_pet_ids), 20))\nfor i, (c_batch, ) in tqdm(enumerate(train_loader)):\n    with torch.no_grad():\n        y_pred = [model.embeddings[i](c_batch[:, i]) for i in range(len(model.embeddings))]\n        y_pred =torch.cat(y_pred, 1).detach()\n        X_train_cat_[i * 128:(i + 1) * 128, :] = y_pred.cpu().numpy()\n        \nX_test_cat_ = np.zeros((len(test_pet_ids), 20))\nfor i, (c_batch, ) in tqdm(enumerate(test_loader)):\n    with torch.no_grad():\n        y_pred = [model.embeddings[i](c_batch[:, i]) for i in range(len(model.embeddings))]\n        y_pred = torch.cat(y_pred, 1).detach()\n        X_test_cat_[i * 128:(i + 1) * 128, :] = y_pred.cpu().numpy()","03be353a":"X_train_cat_.shape, X_test_cat_.shape","523c908e":"train_emb = pd.DataFrame(data=X_train_cat_, columns=[\n    f\"emb{i}\" for i in range(X_train_cat_.shape[1])\n])\ntest_emb = pd.DataFrame(data=X_test_cat_, columns=[\n    f\"emb{i}\" for i in range(X_test_cat_.shape[1])\n])","5bde8394":"test_emb.index = X_test_all.index\n\nX_train_all = pd.concat([X_train_all, train_emb], axis=1)\nX_test_all = pd.concat([X_test_all, test_emb], axis=1)\n\nprint(X_train_all.shape, X_test_all.shape)\nX_train_all.head()","13e1c07b":"xgb_params = {\n    \"eval_metric\": \"rmse\",\n    \"seed\": 1337,\n    \"eta\": 0.01,\n    \"subsample\": 0.75,\n    \"colsample_bytree\": 0.85,\n    \"tree_method\": \"gpu_hist\",\n    \"device\": \"gpu\",\n    \"silent\": 1\n}\n\nxgb_X = X_train_all\nxgb_y = target\nxgb_X_test = X_test_all\n\nmodel, oof_train_xgb, oof_test_xgb= run_xgb(\n    xgb_params, \n    xgb_X, \n    xgb_y, \n    xgb_X_test,\n    resc=train_pet_ids,\n    n_splits=5,\n    num_rounds=10000)","eab1e78d":"def run_lgb(params,\n            X,\n            y,\n            X_test,\n            resc,\n            cat_features,\n            n_splits=10,\n            early_stop=500):\n    fold = GroupKFold(n_splits=n_splits)\n    oof_train = np.zeros((X.shape[0]))\n    oof_test = np.zeros((X_test.shape[0], n_splits))\n\n    for i, (trn_index, val_index) in enumerate(fold.split(X, y, resc)):\n        X_tr = X.iloc[trn_index, :]\n        X_val = X.iloc[val_index, :]\n\n        y_tr = y[trn_index]\n        y_val = y[val_index]\n        model = lgb.LGBMRegressor(**params)\n        model.fit(X_tr, \n                  y_tr, \n                  eval_set=(X_val, y_val),\n                  verbose=500,\n                  early_stopping_rounds=early_stop,\n                  categorical_feature=cat_features)\n        valid_pred = model.predict(X_val, num_iteration=model.best_iteration_)\n        test_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n        oof_train[val_index] = valid_pred\n        oof_test[:, i] = test_pred\n    return model, oof_train, oof_test","220971c8":"lgb_params = {\n    \"boosting_type\": \"gbdt\",\n    \"num_leaves\": 127,\n    \"learning_rate\": 0.01,\n    \"n_estimators\": 10000,\n    \"subsample\": 0.75,\n    \"subsample_freq\": 8,\n    \"colsample_bytree\": 0.75,\n    \"reg_alpha\": 0.01,\n    \"reg_lambda\": 0.01,\n    \"n_jobs\": -1\n}\n\nmodel, oof_train, oof_test = run_lgb(\n    lgb_params, \n    xgb_X, \n    xgb_y, \n    xgb_X_test,\n    train_pet_ids,\n    cat_features,\n    n_splits=5,\n    early_stop=500)","1e10661d":"def plot_pred(pred):\n    sns.distplot(pred, kde=True, hist_kws={\"range\": [0, 5]})","b54e1618":"plot_pred(oof_train)","cc0c3ba9":"plot_pred(oof_train_xgb)","f9a125ef":"oof_train.shape","e9073e6c":"plot_pred(oof_test.mean(axis=1))","a7c86113":"plot_pred(oof_test_xgb.mean(1))","a65f9857":"plot_pred(trainer.train_preds * 4)","f3060f5e":"plot_pred(test_preds * 4)","987c20f7":"lgb_xgb = 0.5 * oof_train + 0.5 * oof_train_xgb\nlgb_xgb_test = 0.5 * oof_test + 0.5 * oof_test_xgb\nplot_pred(lgb_xgb)\nplot_pred(lgb_xgb_test.mean(1))","bd7e16bd":"nn_preds = np.clip(trainer.train_preds, a_min=0.0, a_max=1.0)\nnn_preds_test = np.clip(test_preds, a_min=0.0, a_max=1.0)\nplot_pred(nn_preds * 4)\nplot_pred(nn_preds_test * 4)","5daacd81":"lgb_xgb_nn = 0.6 * lgb_xgb + 0.4 * (nn_preds * 4)\nlgb_xgb_nn_test = 0.6 * lgb_xgb_test.mean(1) + 0.4 * nn_preds_test * 4","c60c08a3":"plot_pred(lgb_xgb_nn)","077fe10d":"plot_pred(lgb_xgb_nn_test)","8c24cb9d":"opt = OptimizedRounder()\nopt.fit(lgb_xgb, target, [1.5, 2.0, 2.5, 3.5])\ncoeff = opt.coefficients()\nvalid_pred = opt.predict(lgb_xgb, coeff)\nqwk = quadratic_weighted_kappa(xgb_y, valid_pred)\nprint(\"QWK = \", qwk)\ncoeffs = coeff.copy()\ntrain_predictions = opt.predict(lgb_xgb, coeffs).astype(np.int8)\nprint(f\"train_preds: {Counter(train_predictions)}\")\ntest_predictions = opt.predict(lgb_xgb_test.mean(1), coeffs).astype(np.int8)\nprint(f\"test_preds: {Counter(test_predictions)}\")\nsubmission = pd.DataFrame({\"PetID\": test.PetID.values, \"AdoptionSpeed\": test_predictions})\nsubmission.to_csv(\"submission_xgb_lgb.csv\", index=False)\nsubmission.head()","e58b2610":"opt = OptimizedRounder()\nopt.fit(lgb_xgb_nn, target, [1.5, 2.0, 2.5, 3.5])\ncoeff = opt.coefficients()\nvalid_pred = opt.predict(lgb_xgb_nn, coeff)\nqwk = quadratic_weighted_kappa(xgb_y, valid_pred)\nprint(\"QWK = \", qwk)\ncoeffs = coeff.copy()\ntrain_predictions = opt.predict(lgb_xgb_nn, coeffs).astype(np.int8)\nprint(f\"train_preds: {Counter(train_predictions)}\")\ntest_predictions = opt.predict(lgb_xgb_nn_test, coeffs).astype(np.int8)\nprint(f\"test_preds: {Counter(test_predictions)}\")\nsubmission = pd.DataFrame({\"PetID\": test.PetID.values, \"AdoptionSpeed\": test_predictions})\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head()","a9fffa97":"opt = OptimizedRounder()\nopt.fit(nn_preds * 4, target, [1.5, 2.0, 2.5, 3.5])\ncoeff = opt.coefficients()\nvalid_pred = opt.predict(nn_preds * 4, coeff)\nqwk = quadratic_weighted_kappa(target, valid_pred)\nprint(\"QWK = \", qwk)\ncoeffs = coeff.copy()\ntrain_predictions = opt.predict(nn_preds * 4, coeffs).astype(np.int8)\nprint(f\"train_preds: {Counter(train_predictions)}\")\ntest_predictions = opt.predict(nn_preds_test * 4, coeffs).astype(np.int8)\nprint(f\"test_preds: {Counter(test_predictions)}\")\nsubmission = pd.DataFrame({\"PetID\": test.PetID.values, \"AdoptionSpeed\": test_predictions})\nsubmission.to_csv(\"submission_nn.csv\", index=False)\nsubmission.head()","1b742e98":"## Numerical Features","c4ec6571":"## Aggregate sentiment data and metadata","4dff7aaa":"## Image size features","da811546":"## Trainer","901074fe":"## Image model loading","59282eda":"## Libraries","bbae7f78":"## Load data","150c0343":"## Run LGBM","182b0801":"## Category Embedding","2e44605c":"## Merge processed DataFrames with base train\/test DataFrame","dd2f6a80":"## Metadata and Sentiment data","1170a7c4":"## Drop columns","7de41811":"## Train","37ba5217":"## DataLoader","f956ff17":"## Image","e0448f6c":"## Feature engineering","74f0783c":"## Tfidf","e34a8af6":"## Categorical Target Encoding","81f4cd26":"## Post process","fe43d395":"## Metrics"}}