{"cell_type":{"f4124580":"code","9ea91ad0":"code","f95d1dd8":"code","a693a1bf":"code","0c8211f4":"code","e86564ee":"code","0e0c395d":"code","3c3f6415":"code","c9d940e7":"code","30868322":"code","b27ee998":"code","75c93994":"code","252586d7":"code","05e99d28":"code","43ba8380":"code","b72df4e0":"code","29a97778":"code","d287c96a":"code","cccf8645":"code","c3506fc1":"code","68741fa2":"code","ac55d393":"code","c2cefc94":"code","90569e36":"code","3e9d0c33":"code","c5b5e37e":"code","88f77e41":"code","efebaa2f":"code","1863b574":"code","9d759277":"code","8c013d13":"code","3da1a103":"code","a750d641":"code","326a5e53":"code","faa644d3":"code","1d500844":"code","2522a273":"code","802c912b":"code","02eb08ec":"code","b59faae7":"code","4e97ba4f":"code","b7baa928":"code","bbbef7d2":"code","563021ef":"code","8e8bc320":"code","adf4e7f7":"code","c3f5eb50":"code","4dc5cd93":"code","36a9495d":"code","35b029e3":"code","480c8944":"code","be717e74":"code","f76d8736":"code","76da19d2":"code","e32ceb4b":"code","2af396a1":"code","f86017d8":"code","d8e97f3a":"code","73f31cb6":"code","bd3d4a3f":"code","a7cf4eba":"code","a61a88e7":"code","96ff7895":"code","e5092c26":"code","47eb8038":"code","8a770600":"code","b10247ed":"code","19164304":"code","6893f76c":"code","2b6c5dc3":"code","45835e95":"code","54bcd7f4":"code","526be61c":"code","614f3ba2":"code","2ae73bb4":"code","04d20a72":"markdown","cbc8600c":"markdown","a0ea7026":"markdown","c3e06573":"markdown","2cf2a96a":"markdown","bced4d0a":"markdown","2faa659f":"markdown","141c6a0f":"markdown","b9bb5442":"markdown","4a005b9c":"markdown","195b2a26":"markdown","b48f1c1a":"markdown","70b1084a":"markdown","e20792d5":"markdown","b5b4887c":"markdown","a6623a8f":"markdown","470ab45d":"markdown","8c4ebe49":"markdown","5a963895":"markdown","d8d452c8":"markdown","8a2eeab9":"markdown","a5336bb5":"markdown","569cff95":"markdown"},"source":{"f4124580":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9ea91ad0":"# Ignore  the warnings\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\n# data visualisation and manipulation\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\n \n#configure\n# sets matplotlib to inline and displays graphs below the corressponding cell.\n%matplotlib inline  \nstyle.use('fivethirtyeight')\nsns.set(style='whitegrid',color_codes=True)\n\n#model selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,roc_curve,roc_auc_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\n\n#preprocess.\nfrom keras.preprocessing.image import ImageDataGenerator\n\n#dl libraraies\nimport keras\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense , merge\nfrom keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\nfrom keras.utils import to_categorical\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.callbacks import ReduceLROnPlateau\n\n\nfrom keras.layers.merge import dot\nfrom keras.models import Model\n\n\n# specifically for deeplearning.\nfrom keras.layers import Dropout, Flatten,Activation,Input,Embedding\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nimport tensorflow as tf\nimport random as rn\nfrom IPython.display import SVG\n \n# specifically for manipulating zipped images and getting numpy arrays of pixel values of images.\nimport cv2                  \nimport numpy as np  \nfrom tqdm import tqdm\nimport os                   \nfrom random import shuffle  \nfrom zipfile import ZipFile\nfrom PIL import Image\n\n\n\nfrom surprise import Reader, Dataset, SVD\nfrom surprise.model_selection import cross_validate\n\nimport warnings; warnings.simplefilter('ignore')","f95d1dd8":"df = pd.read_csv('\/kaggle\/input\/news_articles.csv')","a693a1bf":"df.head(10)","0c8211f4":"#total articles\nlen(df['Article_Id'])","e86564ee":"#unique aticles\nlist_of_articleid = []\nq = df['Article_Id'].unique()\nlist_of_articleid = list_of_articleid.append(q)\nlist_of_articleid","0e0c395d":"import scipy\nimport random\nfrom scipy import stats","3c3f6415":"random.seed(15)\n\nuser_session = stats.geom.rvs(size=4831,  # Generate geometric data\n                                  p=0.3)       # With success prob 0.5","c9d940e7":"user_session.size,user_session.max(),user_session.min()","30868322":"user_session[:10],sum(user_session)","b27ee998":"\ncount_dict = {x : list(user_session).count(x) for x in user_session}\ncount_dict","75c93994":"#depicts number of users per number of sessions\n    \nbins = np.arange(0, 10, 1) # fixed bin size\n\nplt.xlim([min(user_session)-1, max(user_session) +1])\n\nplt.hist(user_session, bins=bins, alpha=0.5)\nplt.title(\"Count of Number of users per session\")\nplt.xlabel('Number of sessions (bin size = 1)')\nplt.ylabel('count')\n\nplt.show()","252586d7":"import numpy as np\n\nuser_Id = range(1,4831)","05e99d28":"userId_session = list(zip(user_Id,[10*i for i in user_session]))","43ba8380":"type(userId_session), userId_session[:5]","b72df4e0":"#Calculating total number of articles served in a day in all sessions (may be clicked or not)\n\nsum1 = 0\nfor i in range(len(userId_session)):\n    \n    sum1 += userId_session[i][1]\n    \nsum1","29a97778":"UserIDs = []\n\nfor i in range(len(userId_session)):\n    \n    for j in range(userId_session[i][1]):\n        UserIDs.append(userId_session[i][0])","d287c96a":"len(UserIDs)   #matches with sum1 above","cccf8645":"UserIDs[:20]   # UserIds generated for all sessions the user opens","c3506fc1":"session_list = list(user_session)\nsession_list[:10]","68741fa2":"session_Id =[]\n\nfor i in session_list:\n    \n    for j in range(1,i+1):\n#         print j\n        session_Id.append([j for i in range(10)])","ac55d393":"session_Id = np.array(session_Id).flatten()","c2cefc94":"\nsession_Id.shape","90569e36":"User_session = list(zip(UserIDs,session_Id ))","3e9d0c33":"\nlen(User_session),type(User_session)","c5b5e37e":"import pandas as pd\n\ndf = pd.DataFrame(User_session, columns=['UserId', 'SessionId'])","88f77e41":"df.tail(20)","efebaa2f":"Article_Id = list(range(4831))","1863b574":"type(Article_Id)","9d759277":"161730\/4831","8c013d13":"Article_Id = Article_Id*int(161730\/4831)  \n\n","3da1a103":"len(Article_Id)","a750d641":"import random\nfor x in range(len(User_session)-len(Article_Id)):\n    Article_Id.append(random.randint(1,4831))","326a5e53":"len(Article_Id)","faa644d3":"from random import shuffle\nshuffle(Article_Id)","1d500844":"c = len(df['UserId'])","2522a273":"Article_Id = Article_Id[:c]","802c912b":"df['ArticleId_served'] = Article_Id","02eb08ec":"df.tail()","b59faae7":"len(df['UserId'].unique())","4e97ba4f":"df","b7baa928":"p = len(df['UserId'])","bbbef7d2":"import random\nnumLow = 1 \nnumHigh = 6\nx = []\nfor i in range (0,p):\n    m = random.sample(range(numLow, numHigh), 1)\n    x.append(m)","563021ef":"x[:3]","8e8bc320":"flat_list = []\nfor sublist in x:\n    for item in sublist:\n        flat_list.append(item)","adf4e7f7":"len(flat_list)","c3f5eb50":"df.head()","4dc5cd93":"df['rating'] = flat_list","36a9495d":"len(df['rating'])","35b029e3":"df.head()","480c8944":"# df.to_csv('file1.csv') \n# saving the dataframe \ndf.to_csv('file3.csv', index=False)\n\n","be717e74":"index=list(df['UserId'].unique())\ncolumns=list(df['ArticleId_served'].unique())\nindex=sorted(index)\ncolumns=sorted(columns)\n \nutil_df=pd.pivot_table(data=df,values='rating',index='UserId',columns='ArticleId_served')","f76d8736":"util_df","76da19d2":"util_df.fillna(0)","e32ceb4b":"# x_train,x_test,y_train,y_test=train_test_split(df[['UserId','ArticleId_served']],df[['rating']],test_size=0.20,random_state=42)\nusers = df.UserId.unique()\nmovies = df.ArticleId_served.unique()\n\nuserid2idx = {o:i for i,o in enumerate(users)}\nmovieid2idx = {o:i for i,o in enumerate(movies)}","2af396a1":"users","f86017d8":"df['ArticleId_served'].head(70)","d8e97f3a":"df['UserId'] = df['UserId'].apply(lambda x: userid2idx[x])\ndf['ArticleId_served'] = df['ArticleId_served'].apply(lambda x: movieid2idx[x])\nsplit = np.random.rand(len(df)) < 0.8\ntrain = df[split]\nvalid = df[~split]\nprint(train.shape , valid.shape)","73f31cb6":"df['ArticleId_served'].head(70)","bd3d4a3f":"n_article=len(df['ArticleId_served'].unique())\nn_users=len(df['UserId'].unique())\nn_latent_factors=64  # hyperparamter to deal with. ","a7cf4eba":"user_input=Input(shape=(1,),name='user_input',dtype='int64')","a61a88e7":"user_input.shape","96ff7895":"# tf.keras.layers.Embedding(\n#      input_dim,\n#      output_dim,\n#      embeddings_initializer=\"uniform\",\n#      embeddings_regularizer=None,\n#      activity_regularizer=None,\n#      embeddings_constraint=None,\n#      mask_zero=False,\n#      input_length=None,\n#      **kwargs\n#  )","e5092c26":"user_embedding=Embedding(n_users,n_latent_factors,name='user_embedding')(user_input)\nuser_embedding.shape","47eb8038":"user_vec =Flatten(name='FlattenUsers')(user_embedding)\nuser_vec.shape","8a770600":"article_input=Input(shape=(1,),name='article_input',dtype='int64')\narticle_embedding=Embedding(n_article,n_latent_factors,name='article_embedding')(article_input)\narticle_vec=Flatten(name='FlattenArticles')(article_embedding)\n# article_vec","b10247ed":"article_vec","19164304":"sim=dot([user_vec,article_vec],name='Simalarity-Dot-Product',axes=1)\nmodel =keras.models.Model([user_input, article_input],sim)\nmodel.summary()","6893f76c":"# Model.compile(\n#     optimizer=\"rmsprop\",\n#     loss=None,\n#     metrics=None,\n#     loss_weights=None,\n#     weighted_metrics=None,\n#     run_eagerly=None,\n#     **kwargs\n# )\n","2b6c5dc3":"model.compile(optimizer=Adam(lr=1e-4),loss='mse')","45835e95":"train.shape","54bcd7f4":"train.shape\nbatch_size=128\nepochs=50","526be61c":"\n# Model.fit(\n#     x=None,\n#     y=None,\n#     batch_size=None,\n#     epochs=1,\n#     verbose=1,\n#     callbacks=None,\n#     validation_split=0.0,\n#     validation_data=None,\n#     shuffle=True,\n#     class_weight=None,\n#     sample_weight=None,\n#     initial_epoch=0,\n#     steps_per_epoch=None,\n#     validation_steps=None,\n#     validation_batch_size=None,\n#     validation_freq=1,\n#     max_queue_size=10,\n#     workers=1,\n#     use_multiprocessing=False,\n# )","614f3ba2":"History = model.fit([train.UserId,train.ArticleId_served],train.rating, batch_size=batch_size,\n                              epochs =epochs, validation_data = ([valid.UserId,valid.ArticleId_served],valid.rating),\n                              verbose = 1)","2ae73bb4":"from pylab import rcParams\nrcParams['figure.figsize'] = 10, 5\nimport matplotlib.pyplot as plt\nplt.plot(History.history['loss'] , 'g')\nplt.plot(History.history['val_loss'] , 'b')\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.grid(True)\nplt.show()","04d20a72":"#### Dot Layer\nLayer that computes a dot product between samples in two tensors.\n\nE.g. if applied to a list of two tensors a and b of shape (batch_size, n), the output will be a tensor of shape (batch_size, 1) where each entry i will be the dot product between a[i] and b[i].","cbc8600c":"### Compiling the Model","a0ea7026":"During the last few decades, with the rise of Youtube, Amazon, Netflix and many other such web services, recommender systems have taken more and more place in our lives. From e-commerce (suggest to buyers articles that could interest them) to online advertisement (suggest to users the right contents, matching their preferences), recommender systems are today unavoidable in our daily online journeys.\nIn a very general way, recommender systems are algorithms aimed at suggesting relevant items to users (items being movies to watch, text to read, products to buy or anything else depending on industries).","c3e06573":"UNDERSTANDING--\n\n1) First we need to create embeddings for both the user as well as the item or article. For this I have used the Embedding layer from keras.\n\n2) Specify the input expected to be embedded (Both in user and item embedding). The use a Embedding layer which expects the no of latent factors in the resulting embedding and also the no of users or items.\n\n3) Then we take the 'Dot-Product' of both the embeddings using the 'merge' layer. Note that 'dot-product' is just a measure of simalrity and we can use any other mode like 'mulitply' or 'cosine simalarity' or 'concatenate' etc...\n\n4) Lastly we make a Keras model from the specified details.\n\n","2cf2a96a":"#### fit method\nTrains the model for a fixed number of epochs (iterations on a dataset).\n\n***Returns***\n\nA History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable).","bced4d0a":"# creating rating ","2faa659f":"#### Input Object \n\nInput() is used to instantiate a Keras tensor","141c6a0f":"# Fitting on Training set & Validating on Validation Set.","b9bb5442":"#### *compile* method \n\nConfigures the model for training.\n\n","4a005b9c":"> Now you can see length of user session and article_id is same","195b2a26":"### Creating the Embeddings ,Merging and Making the Model from Embeddings","b48f1c1a":" # Creating the Utility Matrix","70b1084a":"![image](https:\/\/miro.medium.com\/max\/2000\/1*m_Z6Da5FZ62KN2yH-x_GOQ@2x.png) ","e20792d5":"# Generating data for Number of users per Article","b5b4887c":"UNDERSTANDING--\n\n1) This is the utility matrix; for each of the 4830 users arranged rowwise; each column shows the rating of the article given by a particular user.\n\n2) Note that majority of the matrix is filled with 'Nan' which shows that majority of the articles are unrated by many users.\n\n3) For each article-user pair if the entry is NOT 'Nan' the vaue indicates the rating given by user to that corressponding article.\n\n4) For now I am gonna fill the 'Nan' value with value '0'. But note that this just is just indicative, a **0 implies NO RATING** and doesn't mean that user has rated 0 to that article. It doesn't at all represent any rating.\n\nRATING SCALE IS [1 2 3 4 5]","a6623a8f":"[link to part2 ](https:\/\/www.kaggle.com\/bavalpreet26\/recommender-system-part2)","470ab45d":"#### Embedding layer\nTurns positive integers (indexes) into dense vectors of fixed size.\n\ne.g. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n\n**Input shape**\n\n2D tensor with shape: (batch_size, input_length).\n\n**Output shape**\n\n3D tensor with shape: (batch_size, input_length, output_dim).","8c4ebe49":">  Nan implies that user has not rated the corressponding Article.","5a963895":"## Creating Training and Validation Sets.","d8d452c8":" totla article served in one day \/ no of unique articles = (161730\/4831)","8a2eeab9":"# Matrix Factorization\n\nHere comes the main part!!!\n\n1) Now we move on to the crux of the notebook ie Matrix Factorization. In matrix facorization, we basically break a matrix into usually 2 smaller matrices each with smaller dimensions. these matrices are oftem called 'Embeddings'. We can have variants of Matrix Factorizartion-> 'Low Rank MF' , 'Non-Negaive MF' (NMF) and so on..\n\n2) Here I have used the so called 'Low Rank Matrix Factorization'. I have created embeddings for both user as well as the item; articles in our case. The number of dimensions or the so called 'Latent Factors' in the embeddings is a hyperparameter to deal with in this implementation of Collaborative Filtering.","a5336bb5":"#### Flatten layer\nFlattens the input. Does not affect the batch size.\n\nNote: If inputs are shaped (batch,) without a feature axis, then flattening adds an extra channel dimension and output shape is (batch, 1).","569cff95":"to make a square matrix "}}