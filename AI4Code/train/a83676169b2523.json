{"cell_type":{"6c9230e7":"code","88651dfe":"code","038599c8":"code","e94d80e5":"code","1ff74206":"code","6c6e15e8":"code","911f2922":"code","acc732a1":"code","d8fbf317":"code","e5d0bed2":"code","2579cd82":"code","b145ce28":"markdown","3af3df8c":"markdown","8483087b":"markdown","47d86004":"markdown","68284ceb":"markdown","1072b852":"markdown","5584023d":"markdown","deb1d682":"markdown","60856445":"markdown","1770b774":"markdown","e7ce1559":"markdown","1064dd62":"markdown"},"source":{"6c9230e7":"import numpy as np\nimport os\nimport sys\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nfrom PIL import Image\n\nimport keras\nfrom keras.preprocessing.image import load_img\nfrom keras.models import Model, Sequential\nfrom keras.layers import Input, Dense, Dropout, Activation, BatchNormalization\nfrom keras.layers import Reshape, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.utils import to_categorical\nfrom keras.utils import np_utils\nfrom keras.regularizers import l1_l2\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\n%matplotlib inline\n%env JOBLIB_TEMP_FOLDER=\/tmp","88651dfe":"dataset = 'notMNIST_large'\nDATA_PATH = '..\/input\/' + dataset + '\/' + dataset\n\ntest = 'notMNIST_small'\nTEST_PATH = '..\/input\/' + dataset + '\/' + dataset","038599c8":"max_images = 100\ngrid_width = 10\ngrid_height = int(max_images \/ grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nclasses = os.listdir(DATA_PATH)\nfor j, cls in enumerate(classes):\n    figs = os.listdir(DATA_PATH + '\/' + cls)\n    for i, fig in enumerate(figs[:grid_width]):\n        ax = axs[j, i]\n        ax.imshow(np.array(load_img(DATA_PATH + '\/' + cls + '\/' + fig)))\n        ax.set_yticklabels([])\n        ax.set_xticklabels([])","e94d80e5":"X = []\nlabels = []\n# for each folder (holding a different set of letters)\nfor directory in os.listdir(DATA_PATH):\n    # for each image\n    for image in os.listdir(DATA_PATH + '\/' + directory):\n        # open image and load array data\n        try:\n            file_path = DATA_PATH + '\/' + directory + '\/' + image\n            img = Image.open(file_path)\n            img.load()\n            img_data = np.asarray(img, dtype=np.int16)\n            # add image to dataset\n            X.append(img_data)\n            # add label to labels\n            labels.append(directory)\n        except:\n            None # do nothing if couldn't load file\nN = len(X) # number of images\nimg_size = len(X[0]) # width of image\nX = np.asarray(X).reshape(N, img_size, img_size,1) # add our single channel for processing purposes\nlabels_cat = to_categorical(list(map(lambda x: ord(x)-ord('A'), labels)), 10) # convert to one-hot\nlabels = np.asarray(list(map(lambda x: ord(x)-ord('A'), labels)))\n\nX_test = []\ny_test = []\n# for each folder (holding a different set of letters)\nfor directory in os.listdir(TEST_PATH):\n    # for each image\n    for image in os.listdir(TEST_PATH + '\/' + directory):\n        # open image and load array data\n        try:\n            file_path = DATA_PATH + '\/' + directory + '\/' + image\n            img = Image.open(file_path)\n            img.load()\n            img_data = np.asarray(img, dtype=np.int16)\n            # add image to dataset\n            X_test.append(img_data)\n            # add label to labels\n            y_test.append(directory)\n        except:\n            None # do nothing if couldn't load file\nN = len(X_test) # number of images\nimg_size = len(X_test[0]) # width of image\nX_test = np.asarray(X_test).reshape(N, img_size, img_size,1) # add our single channel for processing purposes\ny_test_cat = to_categorical(list(map(lambda x: ord(x)-ord('A'), y_test)), 10) # convert to one-hot\ny_test = np.asarray(list(map(lambda x: ord(x)-ord('A'), y_test)))","1ff74206":"cls_s = np.sum(labels,axis=0)\n\nfig, ax = plt.subplots()\nplt.bar(np.arange(10), cls_s)\nplt.ylabel('No of pics')\nplt.xticks(np.arange(10), np.sort(classes))\nplt.title('Checking balance for data set..')\nplt.show()","6c6e15e8":"from sklearn.cross_validation import train_test_split\nX_train,X_valid,y_train,y_valid=train_test_split(X,labels,test_size=0.2)\nX_train_cat,X_valid_cat,y_train_cat,y_valid_cat=train_test_split(X,labels_cat,test_size=0.2)\n\nprint('Training:', X_train.shape, y_train.shape)\nprint('Validation:', X_valid.shape, y_valid.shape)\nprint('Test:', X_test.shape, y_test.shape)","911f2922":"fig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor j in range(max_images):\n    ax = axs[int(j\/grid_width), j%grid_width]\n    ax.imshow(X_train[j,:,:,0])\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])","acc732a1":"# helper functions\ndef plot_training_curves(history):\n    \"\"\"\n    Plot accuracy and loss curves for training and validation sets.\n    Args:\n        history: a Keras History.history dictionary\n    Returns:\n        mpl figure.\n    \"\"\"\n    fig, (ax_acc, ax_loss) = plt.subplots(1, 2, figsize=(8,2))\n    if 'acc' in history:\n        ax_acc.plot(history['acc'], label='acc')\n        if 'val_acc' in history:\n            ax_acc.plot(history['val_acc'], label='Val acc')\n        ax_acc.set_xlabel('epoch')\n        ax_acc.set_ylabel('accuracy')\n        ax_acc.legend(loc='upper left')\n        ax_acc.set_title('Accuracy')\n\n    ax_loss.plot(history['loss'], label='loss')\n    if 'val_loss' in history:\n        ax_loss.plot(history['val_loss'], label='Val loss')\n    ax_loss.set_xlabel('epoch')\n    ax_loss.set_ylabel('loss')\n    ax_loss.legend(loc='upper right')\n    ax_loss.set_title('Loss')\n\n    sns.despine(fig)\n    return\n\n# parameters\nbatch_size = 128\nnb_classes = 10\nnb_epoch = 200\ninput_dim = 784\nresolution = 28\nreg = l1_l2(l1=0, l2=0.02)","d8fbf317":"# input image dimensions\nimg_rows, img_cols = 28, 28\n# number of convolutional filters to use\nnb_filters = 64\n# size of pooling area for max pooling\npool_size = [2, 2]\n# convolution kernel size\nkernel_size = [3, 3]\n\ninput_shape = [img_rows, img_cols, 1]","e5d0bed2":"# define path to save model\nmodel_path = '.\/cnn_notMNIST.h5'\n# prepare callbacks\ncallbacks = [\n    EarlyStopping(\n        monitor='val_acc', \n        patience=20,\n        mode='max',\n        verbose=1),\n    ModelCheckpoint(model_path,\n        monitor='val_acc', \n        save_best_only=True, \n        mode='max',\n        verbose=1),\n    ReduceLROnPlateau(\n        factor=0.1, \n        patience=5, \n        min_lr=0.00001, \n        verbose=1)\n]\n\n# model layers\ncnn_ad = Sequential()\ncnn_ad.add(Conv2D(nb_filters, kernel_size, padding='same', input_shape=input_shape))\ncnn_ad.add(Activation('relu'))\ncnn_ad.add(BatchNormalization())\ncnn_ad.add(Conv2D(nb_filters, kernel_size, padding='same'))\ncnn_ad.add(Activation('relu'))\ncnn_ad.add(BatchNormalization())\ncnn_ad.add(MaxPooling2D(pool_size=pool_size))\ncnn_ad.add(Dropout(0.25))\ncnn_ad.add(Flatten())\ncnn_ad.add(Dense(128, kernel_regularizer=reg))\ncnn_ad.add(Activation('relu'))\ncnn_ad.add(BatchNormalization())\ncnn_ad.add(Dropout(0.5))\ncnn_ad.add(Dense(nb_classes, kernel_regularizer=reg))\ncnn_ad.add(Activation('softmax'))\n\ncnn_ad.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n\nhistory = cnn_ad.fit(X_train_cat, y_train_cat,\n                 batch_size=batch_size, epochs=nb_epoch,\n                 verbose=0, validation_data=(X_valid_cat, y_valid_cat),\n                 shuffle=True, callbacks=callbacks)\nscore = cnn_ad.evaluate(X_test, y_test_cat, verbose=0)","2579cd82":"print('Test score:', score[0])\nprint('Test accuracy:', score[1])\n\nplot_training_curves(history.history)","b145ce28":"Select which data to use.","3af3df8c":"Load images and make them ready for fitting a model.","8483087b":"CNN structure.","47d86004":"Check balance of classes.","68284ceb":"Let's check the accuracy on the test set.","1072b852":"And then adding some more advance features to make our training process smarter.","5584023d":"This notebook uses different techniques to create models for classification of the images of letters in the dataset.\n\nFirstly we import the libraries we'll need.","deb1d682":"So, with this notebook I've explored different architectures and strategies for classifying the notMNIST dataset, starting from the basics, till state-of-art architectures.\nThe best result is obtained by the CNN (accuracy ~95%) with further possible improvements related to hyper parameters tuning.\nIf you've any comment, question or advice, please do not hesitate to type it down in the comments.","60856445":"Sanity check of the final dataset.","1770b774":"Check some data from the training dataset","e7ce1559":"Divide data into train\/test datasets.","1064dd62":"Let's start using TensorFlow, specifically I'll use Keras as its wrapper."}}