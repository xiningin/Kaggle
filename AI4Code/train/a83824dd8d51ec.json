{"cell_type":{"896687d8":"code","23897236":"code","40c26067":"code","1a8ce240":"code","80ce3514":"code","269f433f":"code","1d9b9316":"code","7dc94290":"code","ce1b345f":"code","709ad491":"code","632c9e32":"code","9c228fa8":"code","4cadb0ea":"code","12e16e1d":"code","bb066a58":"code","79dc4a62":"code","55c4d2d0":"code","9415cf02":"code","2911b717":"code","49afde42":"markdown","ad769e5f":"markdown","d12d65de":"markdown","d8e017e0":"markdown","f424df75":"markdown","7f143242":"markdown"},"source":{"896687d8":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\n%matplotlib inline","23897236":"cancer = datasets.load_breast_cancer()","40c26067":"df = pd.DataFrame(\n                  cancer['data'],\n                  columns=cancer['feature_names']\n                  )\ndf['target'] = pd.Series(cancer['target'])\ndf.tail()","1a8ce240":"df.describe().T","80ce3514":"df.info()","269f433f":"df.isna().sum()","1d9b9316":"df1 = df.drop(\"target\", axis=1)\ndf1.hist(bins=20, figsize=(30,30), layout=(6,6));","7dc94290":"plt.figure(figsize=(45,20))\nsns.heatmap(df1.corr(), vmax=1, square=True,annot=True,cmap='viridis')\nplt.title('Correlation among variables')\nplt.show()","ce1b345f":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nnp.random.seed(42)\n\n#Data \nX = df.drop(\"target\", axis=1)\ny = df[\"target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nmodel1 = RandomForestClassifier()\nmodel1.fit(X_train, y_train)\nmodel1.score(X_test, y_test)","709ad491":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\ny_train_pred = model1.predict(X_train)\ny_test_pred = model1.predict(X_test)\n\nprint(\"Accuracy from RandomForestClassifier on training dataset is - \", accuracy_score(y_train, y_train_pred))\nprint(\"Accuracy from RandomForestClassifier on testing dataset is - \", accuracy_score(y_test, y_test_pred))\nprint('----')\ndisplay(confusion_matrix(y_test, y_test_pred))\nconf_mat1 = confusion_matrix(y_test, y_test_pred)","632c9e32":"def plot_conf_mat(conf_mat):\n  \"\"\"\n  Plot a confusion matrix using Seaborn's heatmap().\n  \"\"\"\n  fig, ax = plt.subplots(figsize=(3,3))\n  ax = sns.heatmap(conf_mat,\n                    annot=True, # Annotate the boxes with conf_mat info\n                    cbar=False)\n  plt.xlabel(\"True label\")\n  plt.ylabel(\"Predicted label\")\n  plt.show()\nplot_conf_mat(conf_mat1)","9c228fa8":"df3 = pd.DataFrame(data={\"actual values\": y_test,\n                        \"predicted values\": y_preds1})\ndf3[\"differences\"] = df3[\"predicted values\"] - df3[\"actual values\"]\ndf3.head(10)","4cadb0ea":"from sklearn.svm import LinearSVC\nfrom sklearn.model_selection import train_test_split\nnp.random.seed(42)\n\n#Data \nX = df.drop(\"target\", axis=1)\ny = df[\"target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nmodel2 = LinearSVC(max_iter=10000)\nmodel2.fit(X_train, y_train)\nmodel2.score(X_test, y_test)","12e16e1d":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\ny_preds2 = model2.predict(X_test)\n\nprint(\"Accuracy from LinearSVC is - \",accuracy_score(y_test, y_preds2))\ndisplay(confusion_matrix(y_test, y_preds2))\nconf_mat2 = confusion_matrix(y_test, y_preds2)","bb066a58":"def plot_conf_mat(conf_mat):\n  \"\"\"\n  Plot a confusion matrix using Seaborn's heatmap().\n  \"\"\"\n  fig, ax = plt.subplots(figsize=(3,3))\n  ax = sns.heatmap(conf_mat,\n                    annot=True, # Annotate the boxes with conf_mat info\n                    cbar=False)\n  plt.xlabel(\"True label\")\n  plt.ylabel(\"Predicted label\")\n  plt.show()\nplot_conf_mat(conf_mat2)","79dc4a62":"df4 = pd.DataFrame(data={\"actual values\": y_test,\n                        \"predicted values\": y_preds2})\ndf4[\"differences\"] = df4[\"predicted values\"] - df4[\"actual values\"]\ndf4.head(10)","55c4d2d0":"from sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(random_state=0)\nclf.fit(X_train,y_train)\n","9415cf02":"from sklearn.metrics import accuracy_score\n\n#training accuracy \ntrain_pred = clf.predict(X_train)\nprint(\"Accuracy from Decision Tree Classifier on training dataset is - \", accuracy_score(y_train, train_pred))\n\n#testing accuracy \ntest_pred = clf.predict(X_test)\n\nprint(\"Accuracy from Decision Tree Classifier on testing dataset is - \", accuracy_score(y_test, test_pred))","2911b717":"# from sklearn import tree\n# plt.figure(figsize=(15,10))\n# tree.plot_tree(clf, filled=True)","49afde42":"EDA","ad769e5f":"DecisionTreeClassifier","d12d65de":"RandomForestClassifier","d8e017e0":"Implementing the following algorithms using the Breast Cancer dataset -\n\n* Random Forest Classifier\n* LinearSVC\n* DecisionTreeClassifier\n\n*will add more algorithms as I learn :)\n\nGoals: \n* Comparision of Accuracy between models,\n* Gaining insights from the dataset\n* Comparision of actual and predicted target values\n","f424df75":"LinearSVC","7f143242":"Observation: \n\nRandomForestClassifier gives better results when compared with LinearSVC and DecisionTreeClassifier"}}