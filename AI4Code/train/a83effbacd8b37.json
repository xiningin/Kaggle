{"cell_type":{"87dfdb70":"code","4cd74216":"code","9324efe7":"code","319e2030":"code","9589c6ee":"code","4ddb3713":"code","4d87e1fd":"code","4694e0d7":"code","c8a899de":"code","96af7586":"code","fc8ad7cd":"code","421b1abb":"code","ce3d071b":"code","38bef757":"code","341157f8":"code","7107ff3b":"code","b7c9be3b":"code","b0d463ef":"code","33e41982":"code","6a9a3a2b":"code","9a2cc11d":"markdown","97c3c0c2":"markdown","7dbcf7af":"markdown","3726f837":"markdown","94ad62e5":"markdown","38dc4390":"markdown","8b1e2569":"markdown","14f6cd68":"markdown","45a2fc7d":"markdown","32f42e87":"markdown","f2cee5b7":"markdown","62a72d65":"markdown","f646c865":"markdown","0d7153ae":"markdown","85919d92":"markdown","488d78d0":"markdown","a12549f3":"markdown","68fd6abc":"markdown","90157cfe":"markdown","85a43c76":"markdown","849210cc":"markdown","c3e9778d":"markdown","11824762":"markdown","5b9fb0ae":"markdown"},"source":{"87dfdb70":"import torch\nfrom tqdm import tqdm\nimport numpy as np\nfrom torchvision import transforms,models\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset,ConcatDataset\nfrom torch import nn","4cd74216":"train=pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest=pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\n","9324efe7":"class MnistDataset(Dataset):\n    \n    def __init__(self, dataframe, \n                 transform = transforms.Compose([transforms.ToTensor()])):\n        \n        df = dataframe\n        self.n_pixels = 784\n        \n        if len(df.columns) == self.n_pixels:\n            # validation data\n            self.X = df.values.reshape((-1,28,28)).astype(np.uint8)[:,:,:,None]\n            self.y = None\n        else:\n            # training data\n            self.X = df.iloc[:,1:].values.reshape((-1,28,28)).astype(np.uint8)[:,:,:,None]\n            self.y = torch.from_numpy(df.iloc[:,0].values)\n            \n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        if self.y is not None:\n            return self.transform(self.X[idx]), self.y[idx]\n        else:\n            return self.transform(self.X[idx])","319e2030":" img_tform_1 = transforms.Compose([\n    transforms.ToPILImage(),transforms.ToTensor(),transforms.Normalize((0.5),(0.5))])\n\nimg_tform_2 = transforms.Compose([\n    transforms.ToPILImage(),transforms.RandomRotation(10),transforms.ToTensor(),transforms.Normalize((0.5),(0.5))])\n\nimg_tform_3 = transforms.Compose([\n    transforms.ToPILImage(),transforms.RandomRotation(20),transforms.ToTensor(),transforms.Normalize((0.5),(0.5))])\n\nimg_tform_4 = transforms.Compose([\n    transforms.ToPILImage(),transforms.RandomAffine(degrees=15, translate=(0.1,0.1), scale=(0.85,0.85)),\\\n    transforms.ToTensor(),transforms.Normalize((0.5),(0.5))])\n\nimg_tform_5 = transforms.Compose([\n    transforms.ToPILImage(),transforms.RandomAffine(0,shear=30,scale=[1.15,1.15]),\\\n    transforms.ToTensor(),transforms.Normalize((0.5),(0.5))])\n\nimg_tform_6 = transforms.Compose([\n    transforms.ToPILImage(),transforms.RandomAffine(0,shear=20,scale=[0.8,0.8]),\\\n    transforms.ToTensor(),transforms.Normalize((0.5),(0.5))])\n\nimg_tform_7 = transforms.Compose([\n    transforms.ToPILImage(),transforms.RandomAffine(degrees=30, scale=(1.2,1.2)),\\\n    transforms.ToTensor(),transforms.Normalize((0.5),(0.5))])\n\n","9589c6ee":"from sklearn.model_selection import train_test_split\nseed=42\ndef create_dataloaders(seed, test_size=0.1, df=train, batch_size=64):\n    # Create training set and validation set\n    train_df, val_df = train_test_split(df,test_size=test_size,random_state=seed)\n    \n    # Create Datasets\n    train_data_1 = MnistDataset(train_df)\n    train_data_2 = MnistDataset(train_df, img_tform_2)\n    train_data_3 = MnistDataset(train_df, img_tform_3)\n    train_data_4 = MnistDataset(train_df, img_tform_4)\n    train_data_5 = MnistDataset(train_df, img_tform_5)\n    train_data_6 = MnistDataset(train_df, img_tform_6)\n    train_data_7 = MnistDataset(train_df, img_tform_7)\n    train_final = ConcatDataset([train_data_1, train_data_2, train_data_3, train_data_4, train_data_5,\\\n                                   train_data_6,train_data_7])\n\n    val_data = MnistDataset(val_df)\n    \n    # Create Dataloaders\n    train_loader = torch.utils.data.DataLoader(train_final, batch_size=batch_size, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n\n    return train_loader, valid_loader\n\n","4ddb3713":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(device)","4d87e1fd":"classes = [i for i in range(0,10)]","4694e0d7":"model = models.resnet34(pretrained=True)\nmodel.conv1 = torch.nn.Conv1d(1, 64, (3, 3), (1, 1), (1, 1), bias=False)\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 10)","c8a899de":"model","96af7586":"if torch.cuda.is_available():\n    model.cuda()\n    \nimport torch.optim as optim\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(),amsgrad=True)\nxp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1, verbose=True)","fc8ad7cd":"epochs= 20\nvalid_loss_min = np.Inf\ntrain_epoch=[]\ntrain_loss_vals=[]\ntrain_acc_vals=[]\nvalid_epoch=[]\nvalid_loss_vals=[]\nvalid_acc_vals=[]\ntest_loss_val=[]\ntest_epoch=[]\ntrain_loader, valid_loader= create_dataloaders(seed=seed)\nfor i in range(epochs):\n    model.train()\n    train_acc=0\n    valid_acc=0\n    total=0\n    with tqdm(train_loader, unit=\"batch\") as tepoch:\n        for data, target in tepoch:\n            if torch.cuda.is_available():\n                data,target = data.cuda(), target.cuda()\n            optimizer.zero_grad()\n            output=model(data)\n            _, predicted = torch.max(output.data, 1)\n            train_acc+=((predicted==target).sum().item())\n            total += target.size(0)\n            loss = criterion(output, target)\n            loss.backward()\n            train_epoch.append(loss.item())\n            optimizer.step()\n        \n    xp_lr_scheduler.step()\n    train_loss_vals.append(sum(train_epoch)\/len(train_epoch))\n    train_acc_vals.append(100 * train_acc\/ total)\n    model.eval()\n    total=0\n    with tqdm(valid_loader, unit=\"batch\") as tepoch:\n        for data, target in tepoch:\n            if torch.cuda.is_available():\n                data,target= data.cuda(),target.cuda()\n            output=model(data)\n            _, predicted = torch.max(output.data, 1)\n            valid_acc+=((predicted==target).sum().item())\n            total += target.size(0)\n            loss= criterion(output,target)\n            valid_epoch.append(loss.item())\n    valid_loss_vals.append(sum(valid_epoch)\/len(valid_epoch))\n    valid_acc_vals.append(100 * valid_acc\/ total)\n    \n    print(\"epoch:{}\\t  training_loss:{}\\t  validation_loss:{}\\t  train_accuracy:{}\\t  validation_accuracy:{}\"\n          .format(i,train_loss_vals[i],valid_loss_vals[i],train_acc_vals[i],valid_acc_vals[i]))\n    if valid_loss_vals[i] <= valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss_vals[i]))\n        torch.save(model.state_dict(), 'model_cifar.pt')\n        valid_loss_min = valid_loss_vals[i]","421b1abb":"model.load_state_dict(torch.load('model_cifar.pt'))","ce3d071b":"plt.plot(np.linspace(1, epochs, epochs).astype(int), train_acc_vals,label='train_accuracy')\nplt.plot(np.linspace(1, epochs, epochs).astype(int), valid_acc_vals,label='valid_accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\nplt.title('Accuracy curve')","38bef757":"plt.plot(np.linspace(1, epochs, epochs).astype(int), train_loss_vals,label='train_loss')\nplt.plot(np.linspace(1, epochs, epochs).astype(int), valid_loss_vals,label='valid_loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()\nplt.title('loss functions')","341157f8":"\ntest_images = test.values.reshape((-1, 1, 28, 28)) \/ 255.0\nprint(test_images.shape)\n\ntest_image_tensor = torch.tensor(test_images, dtype=torch.float32)","7107ff3b":"model.eval()\nresult = np.zeros(test_images.shape[0], dtype=np.int64)\n\nwith torch.no_grad():\n    for i in range(test_images.shape[0]):\n        image = test_image_tensor[i, 0, :, :].view(1, 1, 28, 28)\n        output=model(image.cuda())\n        _, pred = torch.max(output, 1) \n        result[i] = classes[pred.item()]","b7c9be3b":"result[:10]","b0d463ef":"sample_submission=pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')\nsample_submission['Label']=result","33e41982":"sample_submission","6a9a3a2b":"sample_submission.to_csv('submission1.csv', index=False)","9a2cc11d":"We will be training our network on 10 epochs.","97c3c0c2":"# 3. Creating a custom class to load data.","7dbcf7af":"# 14. Converting predictions to CSV file and submitting.","3726f837":"ResNet, which was proposed in 2015 by researchers at Microsoft Research introduced a new architecture called Residual Network.\n\nResidual Block:\nIn order to solve the problem of the vanishing\/exploding gradient, this architecture introduced the concept called Residual Network. In this network we use a technique called skip connections . The skip connection skips training from a few layers and connects directly to the output.\n\nThis network uses a 34-layer plain network architecture inspired by VGG-19 in which then the shortcut connection is added. These shortcut connections then convert the architecture into residual network. ","94ad62e5":"# 12. Loading the test data and preprocessing the images.","38dc4390":"# 8. Training the defined network.","8b1e2569":"# 6. Defining the pretrained model.","14f6cd68":"# MNIST DATASET","45a2fc7d":"# 1. Importing necessary libraries. ","32f42e87":"# 2. Using pandas library to read the datasets.","f2cee5b7":"# 10. Plotting the train and validation accuracy curves.","62a72d65":"The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems.The database is also widely used for training and testing in the field of machine learning.It was created by \"re-mixing\" the samples from NIST's original datasets.","f646c865":"We will be using the batch-size as 64 and num_workers as 4. We use the customized \"MNISTDataset\" class to convert the data from csv files into a format which is loaded into the dataloader. This is done for both train and validation datasets.","0d7153ae":"# 9. Saving the model.","85919d92":"We tune the number of channels in the first layer of the ResNet to suit the images present in the dataset.\nWe also add a fully connected layer at the end of the network architecture to make the prediction.","488d78d0":"# 4. Defining the transforms.","a12549f3":"We will be defining the classes on which we will be predicting. We are having 10 classes consisting of numbers 0-9.","68fd6abc":"# 5. Using the defined class and DataLoader to load data into pytorch datasets.","90157cfe":"# 7. Defining the loss function and optimizer.","85a43c76":"# 13. Making the predictions.","849210cc":"![](https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/20200424011138\/ResNet.PNG)","c3e9778d":"# 11. Plotting the train and validation loss curves.","11824762":"We are using a \"CrossEntropy\" loss function and an \"Adam\" optimizer while training the model.\nWe also use a scheduler which decays the learning rate of each parameter group by gamma every step_size epochs. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler.","5b9fb0ae":"**With this, we come to the end of the notebook.\nPlease upvote if you found it useful :)\nIt motivates me a lot to share more such stuff.**"}}