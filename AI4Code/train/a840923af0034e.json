{"cell_type":{"292e1fdd":"code","c4dbe17c":"code","007febb2":"code","22d6506c":"code","6110ae02":"code","dadc5741":"code","19e588b0":"code","f732b230":"code","33ee71d4":"code","805d2f49":"code","e4816df6":"code","91d566f2":"code","b03d0999":"code","e9704b4b":"code","3ceae29b":"code","c8ba7e5c":"code","320b660c":"code","3bc0f361":"code","6a4cd9fa":"code","cd5a4b6a":"code","8996bd04":"code","0d37879b":"code","04de7f27":"code","280448f1":"code","73439208":"code","4c1d6047":"code","43997316":"code","0f90bcb5":"code","3f33cfc3":"code","9d1ecac8":"code","7389979b":"code","e28426cc":"code","ae21a61e":"code","feafc227":"code","5348ae99":"code","0a031482":"code","67bc3017":"code","ffa536cf":"code","dcf1b4e6":"code","63e265b1":"code","b17c146c":"markdown","c3beea05":"markdown","e3693350":"markdown","d4a471ab":"markdown","db220a4e":"markdown","968f4770":"markdown","647805e1":"markdown","7ab74041":"markdown","10031a02":"markdown","8d777296":"markdown","e4672672":"markdown","5f2e84c8":"markdown","3428e188":"markdown","925153f9":"markdown","afc04eeb":"markdown","24ee359a":"markdown","88bdaefb":"markdown","eadeaaac":"markdown","286e5e8e":"markdown","aee6f216":"markdown","5139418b":"markdown","67251d5d":"markdown","46cf58e6":"markdown","89dabbe6":"markdown","03adf320":"markdown","0e1a5c02":"markdown","2aff48f9":"markdown","ccbf2ec6":"markdown","da50017a":"markdown","76c9ab36":"markdown","2d8112ac":"markdown","bb24ec72":"markdown","ad86ddec":"markdown","e433b1bc":"markdown","c674d3f5":"markdown","2f7b2982":"markdown","b4a9f6fe":"markdown","4f328649":"markdown","600f824c":"markdown","04a5827a":"markdown","cad4f5dd":"markdown","3e0d5e6e":"markdown","91b3df1f":"markdown","8919a8eb":"markdown","2a38f556":"markdown","e27c68e1":"markdown","adcf3a48":"markdown","6c17bb1f":"markdown","53b94e92":"markdown","68771580":"markdown","63b39e9e":"markdown","08ec55da":"markdown","bf8a3240":"markdown","467dcace":"markdown","5ebfc344":"markdown"},"source":{"292e1fdd":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score as AUC\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.preprocessing import LabelEncoder, LabelBinarizer\nfrom sklearn.model_selection import cross_val_score\n\nfrom scipy import stats\nimport seaborn as sns\nfrom copy import deepcopy\n\n#model fitting\nimport xgboost as xgb\nimport pickle\nimport sys\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error, make_scorer\nfrom scipy.sparse import csr_matrix, hstack\nfrom sklearn.model_selection import KFold, train_test_split\nfrom xgboost import XGBRegressor\n\n%matplotlib inline","c4dbe17c":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","007febb2":"train.shape","22d6506c":"print ('First 20 columns:'), list(train.columns[:20])","6110ae02":"print ('Last 20 columns:'), list(train.columns[-20:])","dadc5741":"train.describe()","19e588b0":"train['loss'].describe()","f732b230":"pd.isnull(train).values.any()","33ee71d4":"train.info()","805d2f49":"cat_features = list(train.select_dtypes(include=['object']).columns)\nprint (\"Categorical features:\",len(cat_features))","e4816df6":"cont_features = [cont for cont in list(train.select_dtypes(\n                 include=['float64', 'int64']).columns) if cont not in ['loss', 'id']]\nprint (\"Continuous features:\", len(cont_features))\nprint(cont_features)","91d566f2":"id_col = list(train.select_dtypes(include=['int64']).columns)\nprint (\"A column of int64: \", id_col)","b03d0999":"cat_uniques = []\nfor cat in cat_features:\n    cat_uniques.append(len(train[cat].unique()))\n    \nuniq_values_in_categories = pd.DataFrame.from_items([('cat_name', cat_features), ('unique_values', cat_uniques)])","e9704b4b":"uniq_values_in_categories.head()","3ceae29b":"fig, (ax1, ax2) = plt.subplots(1,2)\nfig.set_size_inches(16,5)\nax1.hist(uniq_values_in_categories.unique_values, bins=50)\nax1.set_title('Amount of categorical features with X distinct values')\nax1.set_xlabel('Distinct values in a feature')\nax1.set_ylabel('Features')\nax1.annotate('A feature with 326 vals', xy=(322, 2), xytext=(200, 38), arrowprops=dict(facecolor='black'))\n\nax2.set_xlim(2,30)\nax2.set_title('Zooming in the [0,30] part of left histogram')\nax2.set_xlabel('Distinct values in a feature')\nax2.set_ylabel('Features')\nax2.grid(True)\nax2.hist(uniq_values_in_categories[uniq_values_in_categories.unique_values <= 30].unique_values, bins=30)\nax2.annotate('Binary features', xy=(3, 71), xytext=(7, 71), arrowprops=dict(facecolor='black'))","c8ba7e5c":"# Another option is to use Series.value_counts() method, but its\n# output is not that nice\n\nuniq_values = uniq_values_in_categories.groupby('unique_values').count()\nuniq_values = uniq_values.rename(columns={'cat_name': 'categories'})\nuniq_values.sort_values(by='categories', inplace=True, ascending=False)\nuniq_values.reset_index(inplace=True)\nprint (uniq_values)","320b660c":"plt.figure(figsize=(16,8))\nplt.plot(train['id'], train['loss'])\nplt.title('Loss values per id')\nplt.xlabel('id')\nplt.ylabel('loss')\nplt.legend()\nplt.show()","3bc0f361":"stats.mstats.skew(train['loss']).data","6a4cd9fa":"stats.mstats.skew(np.log(train['loss'])).data","cd5a4b6a":"fig, (ax1, ax2) = plt.subplots(1,2)\nfig.set_size_inches(16,5)\nax1.hist(train['loss'], bins=50)\nax1.set_title('Train Loss target histogram')\nax1.grid(True)\nax2.hist(np.log(train['loss']), bins=50, color='g')\nax2.set_title('Train Log Loss target histogram')\nax2.grid(True)\nplt.show()","8996bd04":"train[cont_features].hist(bins=50, figsize=(16,12))","0d37879b":"plt.subplots(figsize=(16,9))\ncorrelation_mat = train[cont_features].corr()\nsns.heatmap(correlation_mat, annot=True)","04de7f27":"# Simple data preparation\n\ntrain_d = train.drop(['id','loss'], axis=1)\ntest_d = test.drop(['id'], axis=1)\n\n# To make sure we can distinguish between two classes\ntrain_d['Target'] = 1\ntest_d['Target'] = 0\n\n# We concatenate train and test in one big dataset\ndata = pd.concat((train_d, test_d))\n\n# We use label encoding for categorical features:\ndata_le = deepcopy(data) # creates a same copy which can be used for other operations without \n#modifying the dataframe\n\n#`data label encoding`\nfor c in range(len(cat_features)):\n    data_le[cat_features[c]] = data_le[cat_features[c]].astype('category').cat.codes\n\n# We use one-hot encoding for categorical features:\ndata = pd.get_dummies(data=data, columns=cat_features)","280448f1":"# randomize before splitting them up into train and test sets\ndata = data.iloc[np.random.permutation(len(data))]\ndata.reset_index(drop = True, inplace = True)\n\nx = data.drop(['Target'], axis = 1)\ny = data.Target\n\ntrain_examples = 100000\n\nx_train = x[:train_examples]\nx_test = x[train_examples:]\ny_train = y[:train_examples]\ny_test = y[train_examples:]","73439208":"x.head()\ny.head()","4c1d6047":"# Logistic Regression:\nclf = LogisticRegression()\nclf.fit(x_train, y_train)\npred = clf.predict_proba(x_test)[:,1]\nauc = AUC(y_test, pred)\nprint(\"Logistic Regression AUC: \",auc)\n\n# Random Forest, a simple model (100 trees) trained in parallel\nclf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\nclf.fit(x_train, y_train)\npred = clf.predict_proba(x_test)[:,1]\nauc = AUC(y_test, pred)\nprint (\"Random Forest AUC: \",auc)\n\n# Finally, CV our results (a very simple 2-fold CV):\nscores = cross_val_score(LogisticRegression(), x, y, scoring='roc_auc', cv=2) \nprint (\"Mean AUC: {:.2%}, std: {:.2%} \\n\",scores.mean(),scores.std())","43997316":"# A possibility to use pretrained models to limit the computational time.\nUSE_PRETRAINED = True","0f90bcb5":"train['log_loss'] = np.log(train['loss'])","3f33cfc3":"train['log_loss'].sample(5)","9d1ecac8":"features = [x for x in train.columns if x not in ['id','loss', 'log_loss']]\n\ncat_features = [x for x in train.select_dtypes(\n        include=['object']).columns if x not in ['id','loss', 'log_loss']]\nnum_features = [x for x in train.select_dtypes(\n        exclude=['object']).columns if x not in ['id','loss','log_loss']]\n\nprint (\"Categorical features:\", len(cat_features))\nprint (\"Numerical features:\", len(num_features))","7389979b":"ntrain = train.shape[0]\n\ntrain_x = train[features]\ntrain_y = train['log_loss'] #target variable\ntest_x = test[features]\n\nfor c in range(len(cat_features)):\n    train_x[cat_features[c]] = train_x[cat_features[c]].astype('category').cat.codes\n    test_x[cat_features[c]] = test_x[cat_features[c]].astype('category').cat.codes\nprint (\"Xtrain:\", train_x.shape)\nprint (\"ytrain:\", train_y.shape)\nprint(\"Xtest:\", test_x.shape)","e28426cc":"train_x.head()","ae21a61e":"test_x.head()","feafc227":"pred_model = xgb.XGBRegressor() #As target variable is a continuous variable\npred_model.fit(train_x, train_y)\ny_pred = pred_model.predict(test_x)\npredictions = [value for value in y_pred]\n\n#loss-prediction log-transformed\n# print(predictions)\n\n# evaluate predictions\n# accuracy = accuracy_score(y_test, predictions)\n# print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","5348ae99":"loss_value = np.exp(predictions)\nprint(loss_value[:5])","0a031482":"train['loss'].sample(5)","67bc3017":"def xg_eval_mae(yhat, dtrain):\n    y = dtrain.get_label()\n    return 'mae', mean_absolute_error(np.exp(y), np.exp(yhat))","ffa536cf":"dtrain = xgb.DMatrix(train_x, train['log_loss'])","dcf1b4e6":"#We use some average set of parameters to make XGBoost work:\nxgb_params = {\n    'seed': 0,\n    'eta': 0.1,\n    'colsample_bytree': 0.5,\n    'silent': 1,\n    'subsample': 0.5,\n    'objective': 'reg:linear',\n    'max_depth': 5,\n    'min_child_weight': 3\n}\n# to be explored in detail for tuning and optimizing the model","63e265b1":"bst_cv1 = xgb.cv(xgb_params, dtrain, num_boost_round=100, nfold=3, seed=0, \n                    feval=xg_eval_mae, maximize=False, early_stopping_rounds=10)\n\nprint ('CV score:', bst_cv1.iloc[-1,:]['test-mae-mean'])\n#bst_cv1","b17c146c":"There are several distinctive peaks in the loss values representing severe accidents. Such data distribution makes this feature very skewed and can result in suboptimal performance of the regressor.\n\nBasically, skewness measures the asymmetry of the probability distribution of a real-valued random variable about its mean. Let's calculate the skewness of `loss`:","c3beea05":"We see a high correlation among several features. This may be a result of a [data-based multicollinearity](https:\/\/statisticsbyjim.com\/regression\/multicollinearity-in-regression-analysis\/) as two or more predictors are highly correlated. There are many [problems](https:\/\/onlinecourses.science.psu.edu\/stat501\/node\/346) it causes, so we should be very careful while [implementing](https:\/\/github.com\/akshayreddykotha\/regression-analysis-in-excel#evaluation-of-fit-model) linear regression models on current dataset.","e3693350":"#### Recreating training and test sets:","d4a471ab":"#### What does the log-transformed data looks like (once again)?","db220a4e":"### Target Feature ('loss') Plots","968f4770":"Next, let's see a quick statistic summary of our continuous features:","647805e1":"### Testing on missing values","7ab74041":"### Feature Correlation (Pearson)","10031a02":"We see that there are probably 116 categorical columns (as their names suggest) and 14 continuous (numerical) columns. Also, there's `id` and `loss` columns. This sums up to 132 columns total.","8d777296":"## Training a model for the ultimate goal of loss amount prediction","e4672672":"When we apply `np.log` to this vector, we get better results:","5f2e84c8":"*Any inputs and feedback are accepted with great interest.*","3428e188":"One thing we can do is to plot histogram of the numerical features and analyze their distributions:","925153f9":"In order to get reliable predictions on test set, we need to make sure that the test data is distributed the same way as the training data does. If we do confirm that the data is equally distributed, this allows us to cross-validate on training set as well as test set.","afc04eeb":"We have got an MAE to start with, MAE = 1172.098958. With a scope for improvement by model tuning and optimizing, the score can further be improved but when it comes to real-world, there is always a trade-off between predictive power and computational capacity. It's highly recommended to strike a balance between the two in practice to bring your analysis closer to a real-world problem as it is really beneficial if every problem is thought in a practical manner rather to just fit the perfect model with a great score.","24ee359a":"### Unique values in categorical features","88bdaefb":"First, let's get accustomed with dataset and get some basic data about it:","eadeaaac":"### Continuous vs Categorical features","286e5e8e":"### Comparing train and test set distributions","aee6f216":"### Model evaluation:","5139418b":"We see plots with many spikes which don't follow any reasonable PDF (probability distribution function). **Such plots point out that the data might have been converted from categorical to continuous to be represented as a real number.** No idea if that's really true as all the data was pre-processed by [Allstate Corporation](https:\/\/www.allstate.com\/).\n\nI'd still transform these features making their distribution more gaussian (a.k.a Normal), but I don't consider it can dramatically improve the model's performance.","67251d5d":"*Predict the loss amount for the insurance policies using the historical trends and features.*","46cf58e6":"188K training examples, 132 columns \u2014\u00a0this does not look like a massive dataset. We can definitely train most of our models on a local machine.","89dabbe6":"### Summary of target variable","03adf320":"Now we train two classifiers: 1) logistic regression and 2) random forest \u2014 and use them to predict our test:","0e1a5c02":"Another way to see the division to categorical and continuous features is to run `pd.DataFrame.info` method:","2aff48f9":"The idea is to mix training and test sets and to see if a classifier (a logistic regression or a decision tree) can separate one from the other.","ccbf2ec6":"#### Why XGBoost?","da50017a":"In this part, we do a short data exploration to see what dataset we have and whether we can find any patterns in it.\n\nAlso, we'll discover that some data transformations are needed to prepare the dataset for our modelling. A very important section of data discovery phase is to make sure that train and test sets are taken from the same distribution. **This is a required step if we want to use cross-validation on the training set and be sure that our models' generalizations will work the same way on test.**","76c9ab36":"We should always dedicate a part of our research on dealing with missing values. Pandas provides an easy way to detect them:","2d8112ac":"## 1. Data Exploration","bb24ec72":"We use a custom evaluation function `xg_eval_mae` which calculates MAE (Mean absolute error), but works with our log-transformed data and uses XGBoost's DMatrix (native format to apply XGBoost):","ad86ddec":"To start with, we train a baseline XGBoost model just to understand how well the whole training goes. To make sure  PC can handle this, we now limit ourselves to 3-fold CV. Higher fold is needed when the error doesn't decrease for each 10 rounds.\n\n**k-fold CV**: *It is a popular method because it is simple to understand and because it generally results in a less biased or less optimistic estimate of the model skill than other methods, such as a simple train\/test split.*","e433b1bc":"I tried to replicate the problem statement and found the [data set](https:\/\/www.kaggle.com\/c\/allstate-claims-severity\/data) on kaggle to present my analysis which is about measurement of severity of an insurance claim made by a client. As I am new to the industry, I thought it'd be a good idea to find a data set rather than create one my own. I look forward to work on data generation and using the same for solving problems in the near future.","c674d3f5":"Now we run XGBoost and cross validate the results via the built-in `xgb.cv` function. We use our `xg_eval_mae` function for calculating the loss (MAE). [MAE](https:\/\/en.wikipedia.org\/wiki\/Mean_absolute_error) is easy to understand and typically starting point as a score to estimate skill of a model.","2f7b2982":"As we see, most of the categorical features (72 \/ 116) are binary, the vast majority of the features (88 \/ 116) have up to four values, there's one feature with 326 values.","b4a9f6fe":"Let's plot a histogram visualizing the unique categories under each variable. ","4f328649":"I am choosing because it is the most popular when it comes to applied machine learning and most importantly tabular data and suitable for limited training data (I feel it is limited as a 4GB RAM could handle it :P), little training time and little expertise for parameter tuning (this is basically my first implementation of XGBoost regressor) which is my scenario here.","600f824c":"As we see from the results above, [ROC AUC score](https:\/\/towardsdatascience.com\/understanding-auc-roc-curve-68b2303cc9c5) is close to 0.5 (50%). We may conclude that neither a LR, nor a RF could find a difference between training and test set. This leads us to the thought that the training and test sets are drawn from the same distribution. A simple way to do so is just to use `train_test_split` in cross-validation module.","04a5827a":"*This data set is loaded using kaggle kernels. Please change the file paths accordingly for reproducibility.*","cad4f5dd":"To make sure our analysis is correct, we can plot a histogram of the loss. We also log-transform the target and plot its updated histogram. log-transform helps us interpret patterns easily and transformation doesn't mean changing the data. You also have the opportunity to back-transform the data without which it makes no sense while understanding the final results.","3e0d5e6e":"There are **no missing values at all.** This is a great relief and this definitely allows us to focus on algorithms and not on data cleansing.","91b3df1f":"It is important to keep in mind that `train_x` and `train_y` were created again but this time for the ultimate preditive model. The main difference between the earlier creation is the target variables are different in both cases. In the earlier case where RF and LG were fit it was a classification problem (to understand if the distibutions are alike) and the target was 0 or 1 i.e. train and test data points. But now the target\n variable is a log-transform of the `loss` variable.","8919a8eb":"### Continuous features","2a38f556":"We can definitely visualize [correlations](https:\/\/stackoverflow.com\/questions\/42128462\/in-python-how-to-do-correlation-between-multiple-columns-more-than-2-variables?rq=1) among all numerical features. We use an out-of-the-box solution (`pd.corr`) which relies on Pearson coefficient.","e27c68e1":"Yes, the data is skewed. Why? - [Because it is heavily greater than 1](https:\/\/help.gooddata.com\/doc\/en\/reporting-and-dashboards\/maql-analytical-query-language\/maql-expression-reference\/aggregation-functions\/statistical-functions\/predictive-statistical-use-cases\/normality-testing-skewness-and-kurtosis). We can measure the skewness with `stats.mstats.skew`:","adcf3a48":"### Aggregated and basic statistics","6c17bb1f":"In here, `float64(15), int64(1)` are our continuous features (the one with `int64` is probably `id`) while `object(116)` are categorical features. We may confirm this:","53b94e92":"Label encoding for categorical variables among all the features:","68771580":"### Actual loss value prediction","63b39e9e":"First, we just plot the target:","08ec55da":"## Problem Statement - Task B","bf8a3240":"#### Important note:","467dcace":"### Load data","5ebfc344":"As we see, all continuous features have been scaled to `[0,1]` interval and have means around 0.5. This is the result of anonymization and some sort of data preprocessing that was done by Allstate.\n\nOn the other hand, `loss` values are not scaled (though they might have been preprocessed as well)."}}