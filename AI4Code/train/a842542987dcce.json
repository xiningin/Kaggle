{"cell_type":{"50db14b7":"code","f70d87d0":"code","c5e53ac6":"code","32345a77":"code","0a87ce44":"code","79fc5072":"code","2a0370ed":"code","78037955":"code","7863eb23":"code","6962262d":"code","04988cae":"code","bdd61446":"code","866764f1":"code","27560146":"code","e6fd1063":"code","b42f11be":"code","a582cd81":"code","ef52fa5b":"code","61af38e9":"code","b8542d3b":"code","848925bf":"code","0b05cceb":"code","c51fb204":"code","0c0e46ba":"code","20002c63":"code","0c1fb953":"code","04342b25":"code","c18285e3":"code","7d2c587f":"code","1af66bca":"code","42422e50":"code","8bd6f840":"code","0c24af1e":"code","ae730953":"code","f0343051":"code","ae50dd91":"code","ab558a16":"code","c456444b":"code","55645d0f":"code","3f07c6c7":"code","8eb17d30":"code","c1fd950c":"code","3aa3c551":"code","f1455d42":"code","3a9b2a98":"code","a5a07eb0":"code","89ac504c":"code","54a8c6a1":"code","177fbca1":"code","e60e0fee":"code","0475f5f7":"code","8cfc3975":"code","69ca68d7":"code","97680c34":"code","b7910382":"code","d8f7584e":"code","4a6209df":"code","2e9543ea":"code","856fa872":"code","9cfa19fa":"code","b80df732":"code","3f1c1074":"code","30f9af78":"code","4c77c174":"code","8f15104e":"code","28f48443":"code","54c03a55":"code","d6b649c8":"code","f0971b3a":"code","77c9618a":"code","b1f449b2":"code","ebfafe49":"code","d3b013f1":"code","fbdffb86":"code","23de2076":"code","c258daa1":"code","190af36f":"code","4f5b888c":"code","c694114c":"code","0ec13ab3":"code","98e1155c":"code","c9eef95c":"code","f2d3631a":"code","34069a36":"code","ad21f662":"code","96baa567":"code","1b0eb309":"code","98b588a0":"code","29001c76":"code","42cba9c8":"code","7e414cf6":"code","949804cb":"code","e5584410":"code","38cbf4b3":"code","781e4acd":"code","e823679c":"code","19d6c080":"code","be9490e3":"code","8409025a":"code","d2a3d2e2":"code","3b895c51":"code","0ff3a08e":"code","98d96c75":"code","bec0b99b":"code","8d91fc90":"code","436e0e77":"code","48b7b5a7":"code","aedcf1cf":"code","ca6c13a4":"code","4c4b1277":"code","883b77b1":"code","19adc239":"code","1cb250f8":"code","921db4e7":"code","303acd47":"code","4ea30be2":"code","bfff8359":"code","6a52be18":"code","e089b576":"code","401a2382":"code","845202db":"code","218ccb8e":"code","86bcc279":"code","94d59e67":"code","70587dc0":"code","72d2145f":"code","3dd5bc17":"code","ac7ffee3":"code","3b1f7067":"code","550e6232":"code","72c65171":"code","85030194":"code","b0a91e74":"code","7a358e2c":"code","cd268474":"code","3910126f":"code","53141bab":"code","4681d0a6":"code","b626bbf5":"code","caf93434":"code","26122f32":"code","d3f5bc2c":"code","8e91950f":"code","156294af":"code","44c71c48":"code","a5efc5e3":"code","7953d54f":"code","360de9d3":"code","d4fdf99d":"code","6cfd1b86":"code","0b2fb4ac":"code","6430b822":"code","117f11d5":"code","1d6bd492":"code","06fcd8b6":"code","ac55ef05":"code","4f767baf":"code","0e0aa125":"code","a8df08b5":"code","587e9176":"code","d5c87a8a":"code","0ca8a112":"code","f4ea978a":"code","78ac2b1e":"code","408e440f":"code","df143808":"code","878b57b0":"code","7fb27fec":"code","c46f3a1d":"code","ed87bb23":"code","28002d89":"markdown","055d0c4c":"markdown","4d16eedf":"markdown","caefaeb1":"markdown","02d4b89a":"markdown","e335e482":"markdown","4c212cb7":"markdown","4b2254fa":"markdown","1d2ee573":"markdown","5c026feb":"markdown","9dd4cf0b":"markdown","c307a1fa":"markdown","cc0b672d":"markdown","a38dfc14":"markdown","d63bc804":"markdown","0bcf190c":"markdown","e1630216":"markdown","92348ff8":"markdown","e8f2af08":"markdown","b1298d9f":"markdown","e5476a8d":"markdown","819e4f4a":"markdown","c83422f8":"markdown","51dfdf51":"markdown","058aa046":"markdown","42faecda":"markdown","e40b138c":"markdown","41a4dc53":"markdown","0f081082":"markdown","6b363d7d":"markdown","b2961d1d":"markdown","5f4f9046":"markdown","14c86cd0":"markdown"},"source":{"50db14b7":"from google.cloud import bigquery\nimport pandas as pd\nimport bq_helper \nimport numpy as np\n%matplotlib inline\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nimport spacy\nnlp = spacy.load('en')\n\n# create a helper object for our bigquery dataset\nbq_hacker_news = bq_helper.BigQueryHelper(active_project= \"bigquery-public-data\", \n                                       dataset_name = \"hacker_news\")\n\nclient = bigquery.Client()\n%matplotlib inline","f70d87d0":"bq_hacker_news.list_tables()","c5e53ac6":"bq_hacker_news.table_schema('comments')","32345a77":"bq_hacker_news.table_schema('stories')","0a87ce44":"bq_hacker_news.head('stories')","79fc5072":"query =\"\"\"\nSELECT score, author, time, title\nFROM `bigquery-public-data.hacker_news.stories`\nWHERE time > 1387536270 AND score >= 0\n\"\"\"\nbq_hacker_news.estimate_query_size(query)","2a0370ed":"# df_hn = bq_hacker_news.query_to_pandas_safe(query, max_gb_scanned=0.5)","78037955":"# df_hn.to_csv(\"hn_scores_w_title.csv\") ","7863eb23":"# len(df_hn)","6962262d":"# bq_hacker_news.head('full', selected_columns=\"type\", num_rows=100).type.unique()","04988cae":"# bq_hacker_news.head('full')","bdd61446":"import re\ndef add_datepart(df, fldname, drop=True, time=False, errors=\"raise\"):\t\n    \"\"\"add_datepart converts a column of df from a datetime64 to many columns containing\n    the information from the date. This applies changes inplace.\n    Parameters:\n    -----------\n    df: A pandas data frame. df gain several new columns.\n    fldname: A string that is the name of the date column you wish to expand.\n        If it is not a datetime64 series, it will be converted to one with pd.to_datetime.\n    drop: If true then the original date column will be removed.\n    time: If true time features: Hour, Minute, Second will be added.\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({ 'A' : pd.to_datetime(['3\/11\/2000', '3\/12\/2000', '3\/13\/2000'], infer_datetime_format=False) })\n    >>> df\n        A\n    0   2000-03-11\n    1   2000-03-12\n    2   2000-03-13\n    >>> add_datepart(df, 'A')\n    >>> df\n        AYear AMonth AWeek ADay ADayofweek ADayofyear AIs_month_end AIs_month_start AIs_quarter_end AIs_quarter_start AIs_year_end AIs_year_start AElapsed\n    0   2000  3      10    11   5          71         False         False           False           False             False        False          952732800\n    1   2000  3      10    12   6          72         False         False           False           False             False        False          952819200\n    2   2000  3      11    13   0          73         False         False           False           False             False        False          952905600\n    \"\"\"\n    fld = df[fldname]\n    fld_dtype = fld.dtype\n    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n        fld_dtype = np.datetime64\n\n    if not np.issubdtype(fld_dtype, np.datetime64):\n        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True, errors=errors)\n    targ_pre = re.sub('[Dd]ate$', '', fldname)\n    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear', 'Hour']\n    if time: attr = attr + ['Hour', 'Minute', 'Second']\n    for n in attr: df[n] = getattr(fld.dt, n.lower())\n    if drop: df.drop(fldname, axis=1, inplace=True)","866764f1":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","27560146":"df_hn = pd.read_csv('..\/input\/hacker-news\/hn_scores.csv', index_col=0)","e6fd1063":"df_hn = pd.read_csv('..\/input\/hacker-news\/hn_scores_w_title.csv', index_col=0)","b42f11be":"df_hn.head(10)","a582cd81":"df_hn['datetime'] = pd.to_datetime(df_hn.time, unit='s').dt.tz_localize('GMT').dt.tz_convert('US\/Pacific') # in GMT","ef52fa5b":"\ndf_hn.head()","61af38e9":"df_hn = df_hn.sort_values('time').reset_index(drop=True)","b8542d3b":"print(f\"dates from {df_hn.iloc[0].datetime} to {df_hn.iloc[-1].datetime}\")\n\n","848925bf":"df_hn[\"delta_s\"]=df_hn.time.diff()\ndf_hn.delta_s.iloc[0] = int(df_hn.delta_s.mean())","0b05cceb":"df_hn.head()","c51fb204":"df_hn = df_hn[df_hn.delta_s<5000]","0c0e46ba":"plt.plot(df_hn.time)","20002c63":"from scipy.stats import expon\n(loc,scale) = expon.fit(df_hn.delta_s[df_hn.delta_s<1000])","0c1fb953":"plt.hist(df_hn.delta_s[df_hn.delta_s<1000],100, density=True)\nstory_rate = 1\/(df_hn.delta_s.mean()) # 1\/100 sec\nt = np.arange(1000)\ny = story_rate*np.exp(-story_rate*t)\nplt.plot(t,y)\ny = expon.pdf(x=t,loc=loc,scale = scale)\nplt.plot(t,y)\nplt.title('fitting the exponential function')","04342b25":"story_rate = 1\/(df_hn.delta_s[df_hn.delta_s<1000].mean()) # 1\/100 sec\nt = np.arange(1000)\ny = story_rate*np.exp(-story_rate*t)\nplt.plot(t,y)","c18285e3":"import seaborn as sns\nsns.distplot(df_hn.delta_s[df_hn.delta_s<1000])\n","7d2c587f":"add_datepart(df_hn,\"datetime\")\ndf_hn.head()","1af66bca":"import calendar\ndow_dic =dict(enumerate(calendar.day_name))\nfig,ax=plt.subplots(figsize=(12,6))\nfor d in range(7):\n    df_hn[df_hn.Dayofweek == d].groupby(\"Hour\").mean().delta_s.plot()\nplt.legend(list(dow_dic.values()))\nplt.ylabel(\"stories intervals\")","42422e50":"# Let's plot the score distribution","8bd6f840":"sns.distplot(df_hn.score[df_hn.score<100],kde=False,norm_hist=True)","0c24af1e":"max_score = 20 \ndf_tmp = df_hn[df_hn.score<max_score]\ndf_tmp.score.plot(kind=\"hist\", bins=max_score-1, density=True)\nplt.xticks(0.5+np.array(range(max_score)),range(max_score))\nplt.xlim(1)\nplt.xlabel(\"score\")\nplt.ylabel(\"density\")\nplt.title(\"distribution of the score\")","ae730953":"df_tmp.size","f0343051":"print(f\"There is {df_tmp.score[df_tmp.score<=1].count() \/ df_tmp.score.count()*100:.1f}% of stories not upvoted at all\")","ae50dd91":"fig,ax=plt.subplots(figsize=(12,6))\ndf_hn.groupby(\"Dayofweek\").delta_s.mean().plot(kind=\"bar\")\nplt.legend(list(dow_dic.values()))\nplt.ylabel(\"average time between stories\")","ab558a16":"fig, axtmp = plt.subplots(1,2, figsize=(10,6))\ndf_tmp = df_hn[df_hn.score<100].groupby(\"Dayofweek\").mean()\nsns.jointplot(x=df_tmp.delta_s, y=df_tmp.score, kind=\"reg\",ax=axtmp[1])\nplt.ylabel('avg score');plt.xlabel('avg time between stories')\nsns.regplot(x=df_tmp.delta_s, y=df_tmp.score,ax=axtmp[0])\nplt.ylabel('avg score');plt.xlabel('avg time between stories')","c456444b":"dow_dic","55645d0f":"r_scores = df_hn[(df_hn.score<100)&(df_hn.Dayofweek==5)].score.mean() \/ df_hn[(df_hn.score<100)&(df_hn.Dayofweek==4)].score.mean()\nr_deltas = df_hn[(df_hn.score<100)&(df_hn.Dayofweek==5)].delta_s.mean() \/ df_hn[(df_hn.score<100)&(df_hn.Dayofweek==4)].delta_s.mean()\nr_scores \/ r_deltas","3f07c6c7":"dow=5\nupvratio = 100*df_hn[(df_hn.score<100)&(df_hn.Dayofweek==dow)&(df_hn.score>1)].score.count() \/ df_hn[(df_hn.score<100)&(df_hn.Dayofweek==dow)].score.count()\nprint(f\"upvote ratio on Saturday: {upvratio:.1f}%\")\ndow=2\nupvratio = 100*df_hn[(df_hn.score<100)&(df_hn.Dayofweek==dow)&(df_hn.score>1)].score.count() \/ df_hn[(df_hn.score<100)&(df_hn.Dayofweek==dow)].score.count()\nprint(f\"upvote ratio on Friday: {upvratio:.1f}%\")","8eb17d30":"def hexbin(x, y, color, **kwargs):\n    cmap = sns.light_palette(color, as_cmap=True)\n    plt.hexbin(x, y, gridsize=10, cmap=cmap, **kwargs)\ng = sns.FacetGrid(df_hn[(df_hn.score<25) & (df_hn.delta_s<600)], col=\"Dayofweek\")\ng = (g.map(hexbin, \"delta_s\", \"score\", edgecolor=\"w\").add_legend())","c1fd950c":"from wordcloud import WordCloud\nall_titles = \" \".join(str(t) for t in df_hn.title)\n# Create and generate a word cloud image:\nwordcloud = WordCloud(background_color=\"white\").generate(all_titles)\n\n# # Display the generated image:\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","3aa3c551":"from tqdm import tqdm\ntqdm.pandas()\ndf_hn[\"has_ask_hn\"]=df_hn.apply(lambda x: x.title.lower().startswith(\"ask hn:\") if isinstance(x.title, str) else False,axis=1)\ndf_hn[\"has_show_hn\"]=df_hn.apply(lambda x: x.title.lower().startswith(\"show hn:\") if isinstance(x.title, str) else False,axis=1)\n\n","f1455d42":"has_show_hn_stats = df_hn.groupby(\"has_show_hn\").has_show_hn.count()\nhas_ask_hn_stats = df_hn.groupby(\"has_ask_hn\").has_show_hn.count()","3a9b2a98":"print(f\"There are {100* has_show_hn_stats[1]\/(has_show_hn_stats[1]+has_show_hn_stats[0]):.1f}% of Show HN stories\")\nprint(f\"There are {100* has_ask_hn_stats[1]\/(has_ask_hn_stats[1]+has_ask_hn_stats[0]):.1f}% of Ask HN stories\")","a5a07eb0":"## Remove Show HN and Show HN","89ac504c":"df_hn[\"title\"]=df_hn.progress_apply(lambda x: x.title.rsplit(\"Ask HN:\")[-1].rsplit(\"Show HN:\")[-1] if isinstance(x.title, str) else x.title,axis=1)","54a8c6a1":"from wordcloud import WordCloud\nall_titles = \" \".join(str(t) for t in df_hn.title)\n# Create and generate a word cloud image:\nwordcloud = WordCloud(background_color=\"white\").generate(all_titles)\n\n# # Display the generated image:\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","177fbca1":"# Word cloud for low rated stories\nfrom wordcloud import WordCloud\nall_titles = \" \".join(str(t) for t in df_hn[df_hn.score<=1].title)\n# Create and generate a word cloud image:\nwordcloud = WordCloud(background_color=\"white\").generate(all_titles)\n\n# # Display the generated image:\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","e60e0fee":"from wordcloud import WordCloud\nall_titles = \" \".join(str(t) for t in df_hn[df_hn.score>5].title)\n# Create and generate a word cloud image:\nwordcloud = WordCloud(background_color=\"white\").generate(all_titles)\n\n# # Display the generated image:\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","0475f5f7":"df_hn.head()","8cfc3975":"def translate_type(s):\n    if s.has_ask_hn:\n        return \"Ask HN\"\n    elif s.has_show_hn:\n        return \"Show HN\"\n    else:\n        return \"Regular\"\ndf_hn[\"post_type\"]=df_hn.progress_apply(lambda x: translate_type(x), axis=1)\n                        ","69ca68d7":"kwargs = dict(alpha=0.5, bins=100, density=True, stacked=True)\ndf_tmp = df_hn[(df_hn.score<100) & (df_hn.post_type==\"Regular\")]\nplt.hist(df_tmp.score, **kwargs, color='g', label='Regular')\nper_not_upvoted = df_tmp.score[df_tmp.score<=1].count() \/ df_tmp.score.count()*100\nprint(f\"There is {per_not_upvoted:.1f}% of regular stories not upvoted at all\")\nprint(f\"The average score is {df_tmp.score.mean():.1f}\")\n\n\ndf_tmp = df_hn[(df_hn.score<100) & (df_hn.post_type==\"Ask HN\")]\nplt.hist(df_tmp.score, **kwargs, color='r', label='Ask HN')\nper_not_upvoted = df_tmp.score[df_tmp.score<=1].count() \/ df_tmp.score.count()*100\nprint(f\"There is {per_not_upvoted:.1f}% of Ask HN stories not upvoted at all\")\nprint(f\"The average score is {df_tmp.score.mean():.1f}\")\n\ndf_tmp = df_hn[(df_hn.score<100) & (df_hn.post_type==\"Show HN\")]\nplt.hist(df_tmp.score, **kwargs, color='b', label='Show HN')\nper_not_upvoted = df_tmp.score[df_tmp.score<=1].count() \/ df_tmp.score.count()*100\nprint(f\"There is {per_not_upvoted:.1f}% of Show HN stories not upvoted at all\")\nprint(f\"The average score is {df_tmp.score.mean():.1f}\")\n\nplt.gca().set(title='Score vs Type of story', ylabel='Frequency')\nplt.xlim(0,20)\n\nplt.legend();\n\n","97680c34":"sns.violinplot(x=\"post_type\", y=\"score\", data=df_hn[df_hn.score<50])","b7910382":"df_tmp = df_hn[(df_hn.score<100)]\n\nsns.boxenplot(x=\"post_type\", y=\"score\", data=df_tmp)","d8f7584e":"df_hn[\"mentions_hn\"]=df_hn.apply(lambda x: \"hacker news\" in x.title.lower() if isinstance(x.title, str) else False,axis=1)\n","4a6209df":"mentions_hn_stats = df_hn.groupby(\"mentions_hn\").mentions_hn.count()\nprint(f\"There are {100* mentions_hn_stats[1]\/(mentions_hn_stats[1]+mentions_hn_stats[0]):.1f}% of Hacker News mentions stories\")","2e9543ea":"df_hn[df_hn.score<100].groupby(\"mentions_hn\").score.mean()","856fa872":"df_hn[df_hn.score<=1].groupby(\"mentions_hn\").score.count() \/ df_hn[df_hn.score<=100].groupby(\"mentions_hn\").score.count()","9cfa19fa":"from wordcloud import WordCloud\ndf_tmp = df_hn[(df_hn.score<100) & (df_hn.mentions_hn==True)]\nall_titles = \" \".join(str(t) for t in df_tmp.title)\n# Create and generate a word cloud image:\nwordcloud = WordCloud(background_color=\"white\").generate(all_titles)\n\n# # Display the generated image:\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","b80df732":"def plot_corr(corr_mat,size=10):\n    '''Function plots a graphical correlation matrix for each pair of columns in the dataframe.\n\n    Input:\n        df: pandas DataFrame\n        size: vertical and horizontal size of the plot'''\n\n    fig, ax = plt.subplots(figsize=(size, size))\n    ax.matshow(corr_mat)\n    plt.xticks(range(len(corr_mat.columns)), corr_mat.columns);\n    plt.yticks(range(len(corr_mat.columns)), corr_mat.columns);","3f1c1074":"df_hn.head()","30f9af78":"corr_matrix = df_hn[df_hn.score<100].corr() # pearson correlacti","4c77c174":"corr_matrix","8f15104e":"corr_matrix.iloc[2:,0] # skipping time and self","28f48443":"corr_matrix.iloc[2:,0].abs().sort_values(ascending=False).plot(kind='bar')","54c03a55":"df_hn_work = df_hn[df_hn.score<100]\nimport calendar\ndow_dic =dict(enumerate(calendar.day_name))\nm_dic = dict(enumerate(calendar.month_name))\ndf_hn_work.groupby('Dayofweek').score.mean().rename(dow_dic).plot(kind=\"bar\")\nplt.ylabel(\"mean score\")","d6b649c8":"df_hn_work.groupby('Dayofweek').score.count().rename(dow_dic).plot(kind=\"bar\")\nplt.ylabel(\"post count\")","f0971b3a":"df_hn_work.groupby('Month').score.mean().rename(m_dic).plot(kind=\"bar\")","77c9618a":"df_hn_work.groupby('Month').score.count().rename(m_dic).plot(kind=\"bar\")","b1f449b2":"df_hn_work.groupby('Dayofweek').score.count().rename(dow_dic).plot(kind=\"bar\")","ebfafe49":"dow_dic","d3b013f1":"df_hn_work[df_hn_work.Dayofweek==5].delta_s.mean()\/df_hn_work[df_hn_work.Dayofweek==0].delta_s.mean()","fbdffb86":"df_hn_work[df_hn_work.Dayofweek==5].delta_s.mean()","23de2076":"df_dens= df_hn[df_hn.score<20]\ndf_dens = df_dens[df_hn.delta_s<400]","c258daa1":"sns.jointplot(df_dens.delta_s, df_dens.score, kind=\"hex\")","190af36f":"\nplt.figure(figsize=(10,10))\nsns.heatmap(df_dens.corr(), vmax=1, square=True,annot=True,cmap='viridis')\n","4f5b888c":"df_hn = pd.read_csv('..\/input\/hacker-news\/hn_scores_w_title.csv', index_col=0)","c694114c":"def process_hn(df_hn):\n    df_hn['datetime'] = pd.to_datetime(df_hn.time, unit='s').dt.tz_localize('GMT').dt.tz_convert('US\/Pacific') # in GMT\n    df_hn = df_hn.sort_values('time').reset_index(drop=True)\n    df_hn[\"delta_s\"]=df_hn.time.diff()\n    df_hn = df_hn.dropna()\n    df_hn = df_hn[df_hn.delta_s<5000]\n    df_hn = df_hn[df_hn.score<100]\n    add_datepart(df_hn,\"datetime\")\n    return df_hn","0ec13ab3":"df_hn = process_hn(df_hn)","98e1155c":"df_hn.head()","c9eef95c":"df_gpt = df_hn.groupby(\"title\").count()\ndf_gpta = df_hn.groupby(\"title\").author.nunique()","f2d3631a":"df_gpt_tmp = df_hn.groupby(\"title\").score\ndf_gpt_score_mean = df_gpt_tmp.mean()\ndf_gpt_score_sum = df_gpt_tmp.sum()\ndf_gpt_score_std = df_gpt_tmp.std()\ndf_gpt_score_min = df_gpt_tmp.min()\ndf_gpt_score_max = df_gpt_tmp.max()","34069a36":"df_gpt[\"score_mean\"] = df_gpt_score_mean\ndf_gpt[\"score_sum\"] = df_gpt_score_sum\ndf_gpt[\"score_std\"] = df_gpt_score_std\ndf_gpt[\"score_min\"] = df_gpt_score_min\ndf_gpt[\"score_max\"] = df_gpt_score_max\ndf_gpt[\"score_delta\"] = df_gpt_score_max - df_gpt_score_min\ndf_gpt[\"score_delta_ratio\"] = df_gpt[\"score_delta\"]\/df_gpt_score_mean\ndf_gpt[\"n_authors\"] = df_gpta.values","ad21f662":"df_tmp = df_hn.groupby(\"title\").agg(\n    score_mean=('score', 'mean'),\n    score_sum=('score', 'sum'),\n)","96baa567":"df_gpt = df_gpt.drop(columns=['author','time', 'delta_s', 'Year', 'Month', 'Week', 'Day',\n       'Dayofweek', 'Dayofyear', 'Hour'])","1b0eb309":"df_gpt.rename(columns={\"score\": \"post_count\"}, inplace=True)","98b588a0":"df_gpt.head()","29001c76":"print((df_gpt.post_count>1).value_counts(normalize=True))\nprint((df_gpt.post_count>1).value_counts()) # that's 19100","42cba9c8":"df_gpt[(df_gpt.post_count<20)].groupby(\"post_count\").mean().score_mean.plot(kind='bar')\nplt.xlabel(\"number of posts\")\nplt.ylabel(\"mean score for the stories\")","7e414cf6":"df_gpt[df_gpt.post_count>20].sort_values(by=[\"post_count\"], ascending=False).head(10)","949804cb":"import seaborn as sns\ndf_kde = df_gpt[(df_gpt.post_count>1) & (df_gpt.post_count<6)]\nsns.jointplot(x=\"post_count\", y=\"n_authors\",data=df_kde, \n              marginal_kws=dict(bins=5), \n              kind=\"reg\")","e5584410":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import axes3d, Axes3D #<-- Note the capitalization! \nfig = plt.figure()\nxedges = [2,3,4,5];yedges = [1,2,3,4,5]\nax = Axes3D(fig) \nhist, xedges, yedges = np.histogram2d(df_kde.post_count, df_kde.n_authors,bins=(xedges, yedges))\n\n# Construct arrays for the anchor positions of the 16 bars.\nxpos, ypos = np.meshgrid(xedges[:-1] - 0.25, yedges[:-1] - 0.25, indexing=\"ij\")\nxpos = xpos.ravel()\nypos = ypos.ravel()\nzpos = 0\n\n# Construct arrays with the dimensions for the 16 bars.\ndx = dy = 0.5 * np.ones_like(zpos)\ndz = hist.ravel()\nax.bar3d(xpos, ypos, zpos, dx, dy, dz, zsort='average')\nax.set_xlabel(\"n posts\")\nax.set_ylabel(\"n authors\")\nax.set_zlabel(\"count\")\nax.set_xticks([2,3,4])\nax.set_yticks([1,2,3,4])\nplt.title(\"Duplicate posts vs. Number of Authors\")\nplt.show()","38cbf4b3":"df_sample = df_gpt[(df_gpt.post_count==2) & (df_gpt.n_authors==2)]\ndf_sample.head(3)","781e4acd":"df_sample = df_gpt[(df_gpt.post_count>=1) & (df_gpt.n_authors<=5) & (df_gpt.post_count<=10) &(df_gpt.score_mean<=20)  ]\ndf_sample.groupby(\"n_authors\").score_mean.mean().plot()\n","e823679c":"df_hn[\"post_count_for_story\"]=df_hn.groupby(\"title\").score.transform(\"count\")\ndf_hn[\"story_id\"] = df_hn.groupby(\"title\").ngroup()\ndf_hn[\"first_post_score\"]=df_hn[df_hn[\"post_count_for_story\"]>1].groupby(\"title\")[\"score\"].transform(lambda x:  x.iloc[0])\ndf_hn[\"post_upscore\"]=df_hn[df_hn[\"post_count_for_story\"]>1].groupby(\"title\")[\"score\"].transform(lambda x:  (x - x.iloc[0]))\ndf_hn[\"post_upscore\"] = df_hn[\"post_upscore\"].clip(lower=0)\ndf_gpt[\"post_upscore_max\"]=df_hn.groupby(\"title\").post_upscore.max()\ndf_gpt[\"first_post_score\"]=df_hn.groupby(\"title\").first_post_score.mean()\n\n","19d6c080":"df_oneauthor.groupby(\"post_count\").count()","be9490e3":"#cutting mas score at 6 seen the number of samples beyond that.\ndf_oneauthor = df_gpt[(df_gpt.n_authors==1)&((df_gpt.post_count<=6)&(df_gpt.score_mean<=200))]","8409025a":"df_oneauthor.score_mean.max()","d2a3d2e2":"df_oneauthor.boxplot(column='score_mean',by='post_count')","3b895c51":"yerr = df_oneauthor.groupby(\"post_count\").score_std.mean().values\ndf_oneauthor.groupby(\"post_count\").mean().score_mean.plot(kind='bar', yerr=yerr, capsize=3)\nplt.xlabel(\"number of posts\")\nplt.ylabel(\"mean score for the stories\")\nplt.title(\"Reposts by the same author\")","0ff3a08e":"fps_series = df_oneauthor.groupby(\"post_count\").mean().first_post_score\nmps_series = df_oneauthor.groupby(\"post_count\").mean().post_upscore_max\n\np1 = plt.bar(fps_series.index, height=fps_series.values)\np2 = plt.bar(mps_series.index, height=mps_series.values, bottom=fps_series.values)\nplt.ylabel(\"Mean Score\")\nplt.xlabel(\"Number of posts\")\nplt.title(\"How much additional score can be gained over the initial score\")\nplt.legend((p1[0], p2[0]), ('Initial score', 'Max extra score'))\n","98d96c75":"df_oneauthor.groupby(\"post_count\").score_delta.mean().plot(kind='bar')\nplt.xlabel(\"number of posts\")\nplt.ylabel(\"Mean delta on scores for reposts\")","bec0b99b":"df_oneauthor.groupby(\"post_count\").score_delta.mean().plot(kind='bar')\nplt.xlabel(\"number of posts\")\nplt.ylabel(\"Mean delta on scores for reposts\")","8d91fc90":"df_oneauthor[(df_oneauthor.post_count==2)].head(3)","436e0e77":"# df_hn[\"post_count_for_story\"]=df_hn.groupby(\"title\").score.transform(\"count\")\n# df_hn[\"story_id\"] = df_hn.groupby(\"title\").ngroup()","48b7b5a7":"df_hn.head()","aedcf1cf":"df_hn[\"perc_from_mean\"] = df_hn[df_hn[\"post_count_for_story\"]>1].groupby(\"title\")[\"score\"].transform(lambda x: (x - x.mean()) \/ x.mean())","ca6c13a4":"df_hn.head()","4c4b1277":"import calendar \ndf_dup = df_hn[(df_hn.post_count_for_story>1)&(df_hn.post_count_for_story<25)].groupby('Dayofweek').perc_from_mean\ndow_dic =dict(enumerate(calendar.day_name))\ndf_dup.mean().rename(dow_dic).plot.bar()\nplt.xlabel(\"week day\")\nplt.ylabel(\"ratio from the mean\")","883b77b1":"df_dup = df_hn[(df_hn.post_count_for_story>1)&(df_hn.post_count_for_story<10)].groupby('Dayofweek').perc_from_mean\ndf_dup.mean().rename(dow_dic).plot.bar(yerr=df_dup.std().rename(dow_dic))","19adc239":"df_hn[\"score_greater_than_mean\"] = df_hn.perc_from_mean > 0 \ndf_dup = df_hn[(df_hn.post_count_for_story>1)&(df_hn.post_count_for_story<7)].groupby('Dayofweek').score_greater_than_mean\n","1cb250f8":"df_dup.mean().rename(dow_dic).plot(kind=\"bar\")","921db4e7":"df_tmp = df_hn.groupby(\"title\").agg(\n    post_count=('post_count_for_story', 'mean'),\n    score_std=('score', \"std\"),\n    score_mean=('score', \"mean\"),\n    score_max=('score', \"max\"),\n    score_min=('score', \"min\"),\n    unique_authors=('author','nunique'),\n\n\n)\n# df_hn[\"perc_spread\"] = df_hn[df_hn[\"post_count_for_story\"]>1].groupby(\"title\")[\"score\"].transform(lambda x: (x.max() - x.min()) \/ x.mean())","303acd47":"df_dup = df_tmp[(df_tmp.post_count>1)&(df_tmp.post_count<10)]","4ea30be2":"sns.jointplot(df_dup.post_count, df_dup.score_std, kind=\"reg\")","bfff8359":"sns.jointplot(df_dup.post_count, df_dup.score_mean, kind=\"reg\")","6a52be18":"sns.jointplot(df_dup.post_count, df_dup.score_max-df_dup.score_min, kind=\"reg\")","e089b576":"df_tmp[(df_tmp.post_count==9)]","401a2382":"df_hn[df_hn.title==\"As Tech Booms, Workers Turn to Coding for Career Change\"]","845202db":"df_hn[df_hn.title==\"35 Growth Hacking Tools for Marketers Who Don\u2019t Code\"]","218ccb8e":"# Plot number of reposts vs number of users","86bcc279":"sns.jointplot(df_dup.unique_authors, df_dup.score_max, kind=\"reg\")","94d59e67":"### Cleanup?","70587dc0":"df_hn_clean = df_hn[df_hn.post_count_for_story>1]\nsns.violinplot(x=\"Dayofweek\", y=\"score\", data=df_hn_clean)","72d2145f":"df_hn_clean.describe()","3dd5bc17":"#Should Ask HN, Show HN be removed?\ndf_hn_clean[df_hn_clean.title.str.contains(\"Show HN\")].count().max()","ac7ffee3":"df_hn_clean[df_hn_clean.score>90]","3b1f7067":"df_hn_clean.groupby('Dayofweek').perc_from_mean.mean().rename(dow_dic).plot(kind=\"bar\")\nplt.title(\"Variation from mean score of repost\")\nplt.ylabel(\"Ratio from the mean\")","550e6232":"df_hn_clean.groupby('Hour').perc_from_mean.count().plot(kind=\"bar\")","72c65171":"df_hn_clean[df_hn_clean.Dayofweek==5].groupby('Hour').perc_from_mean.mean().plot(kind=\"bar\")","85030194":"## So Saturday at 12 (pacific time)! Lunch time, nobody posts? Or Lunch time everyboday reads?","b0a91e74":"corr_matrix = df_hn.corr() # pearson correlacti","7a358e2c":"corr_matrix.iloc[:,11]","cd268474":"corr_matrix.iloc[:,11].abs().sort_values(ascending=False).plot(kind='bar')\nplt.ylabel(\"percentage from mean\")","3910126f":"!pip install bert-embedding","53141bab":"df_hn = pd.read_csv('..\/input\/hacker-news\/hn_scores_w_title.csv', index_col=0)\ndf_hn[\"post_count_for_story\"]=df_hn.groupby(\"title\").score.transform(\"count\")\ndf_hn = df_hn[(df_hn.score<100)&(df_hn.post_count_for_story<5)]","4681d0a6":"from tqdm import tqdm\ntqdm.pandas()\ndf_hn[\"title\"]=df_hn.progress_apply(lambda x: x.title.rsplit(\"Ask HN:\")[-1].rsplit(\"Show HN:\")[-1] if isinstance(x.title, str) else x.title,axis=1)","b626bbf5":"df_hn = df_hn.groupby(\"title\").first().reset_index()","caf93434":"df_hn.title.replace('', np.nan, inplace=True)","26122f32":"df_hn.dropna(inplace=True)","d3f5bc2c":"df_hn[\"title\"]=df_hn.progress_apply(lambda x: x.title.rsplit(\"\\t\")[-1] if isinstance(x.title, str) else x.title,axis=1)","8e91950f":"df_hn.head()","156294af":"def isLatin(s):\n    try:\n        str(s).encode(encoding='utf-8').decode('ascii')\n    except UnicodeDecodeError:\n        return False\n    else:\n        return True","44c71c48":"df_hn[\"isLatin\"] = df_hn.title.apply(isLatin)","a5efc5e3":"# np.savetxt('data_exp.txt', df_hn[df_hn.isLatin == True].title,fmt='%s')","7953d54f":"# Sample size\nn_sample = 50\n# df_tmp = df_hn[df_hn.isLatin == True].sample()\ndf_tmp = df_hn[df_hn.isLatin == True].iloc[:n_sample]\n","360de9d3":"from bert_embedding import BertEmbedding\ndef get_sentence_embedding(x_emb): \n    if not x_emb[1] :\n        return np.zeros(768)\n    else: return np.vstack(x_emb[1]).mean(axis=0)\n\nbert_embedding = BertEmbedding()","d4fdf99d":"# from tqdm import tqdm\n# tqdm.pandas()\n# df_tmp[\"sent_emb\"]=df_tmp.progress_apply(lambda x: get_sentence_embedding(bert_embedding(x.title)[0]),axis=1)","6cfd1b86":"from tqdm import tqdm\nimport time\nbs = 25\ndf_emb = pd.Series([])\n\nfor i in tqdm(range(0, len(df_tmp), bs)):\n    tmp_emb = bert_embedding(df_tmp.iloc[i:i+bs].title.values)\n    df_emb = df_emb.append(pd.Series([get_sentence_embedding(i) for i in tmp_emb]))\n","0b2fb4ac":"from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\nfrom scipy import sparse\nfrom sklearn.cluster import DBSCAN\nimport sklearn\nimport numpy as np\n\nA_sparse = sparse.csr_matrix(df_emb.values.tolist())\n\nsimilarities = cosine_similarity(A_sparse)\ndistances = cosine_distances(A_sparse)","6430b822":"nlabels = []\nfor e in tqdm(range(1,100)):\n    db = DBSCAN(metric=\"precomputed\",eps=e\/100).fit(distances)\n    nlabels.append(len(set(db.labels_)))\n#     print(f\"Number of labels for eps: {e\/100} is {len(set(db.labels_))}\")","117f11d5":"plt.scatter(range(1,100),nlabels)\nmax_eps = np.argmax(nlabels)\/100\nprint(f\"argmax epsilon is {max_eps}\")","1d6bd492":"db = DBSCAN(metric=\"precomputed\",eps=max_eps).fit(distances)","06fcd8b6":"df_tmp[\"topic\"] = db.labels_","ac55ef05":"len(set(db.labels_))","4f767baf":"from wordcloud import WordCloud\nfor topic in set(db.labels_):\n\n    all_titles = \" \".join(str(t) for t in df_tmp[df_tmp.topic==topic].title)\n    # Create and generate a word cloud image:\n    wordcloud = WordCloud(background_color=\"white\").generate(all_titles)\n\n    # # Display the generated image:\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    tcount = len(df_tmp[df_tmp.topic==topic])\n    tscore = df_tmp[df_tmp.topic==topic].score.mean()\n    plt.title(f\"Topic{topic}: count {tcount}, score {tscore:.1f}\")\n    plt.show()\n","0e0aa125":"from absl import logging\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport re\nimport seaborn as sns\nimport tqdm, time\n","a8df08b5":"n_sample = 20000\n# df_tmp = df_hn[df_hn.isLatin == True].iloc[:n_sample]\ndf_tmp = df_hn[df_hn.isLatin == True].sample(n_sample)","587e9176":"module_url = \"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/2\"\n# Import the Universal Sentence Encoder's TF Hub module\nstart_time = time.time()\nembed = hub.Module(module_url)\n\n# Reduce logging output.\nlogging.set_verbosity(logging.ERROR)\n","d5c87a8a":"from tqdm import tqdm\nimport gc\nimport time\nbs = n_sample\ndf_emb = pd.Series([])\n\nwith tf.Session() as session:\n    print(f\"Initiating...\"); start_time = time.time()\n    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n    print(f\"init: {time.time() - start_time}\"); start_time = time.time()\n    for i in tqdm(range(0, len(df_tmp), bs)):\n        messages = df_tmp.iloc[i:i+bs].title.values\n        message_embeddings = session.run(embed(messages))\n        df_emb = df_emb.append(pd.Series(list(message_embeddings)))\n        print(f\"run {i}:{time.time() - start_time:.1f}\"); start_time = time.time()\n        gc.collect()","0ca8a112":"from sklearn.metrics.pairwise import cosine_similarity, cosine_distances, linear_kernel, euclidean_distances, manhattan_distances\nfrom scipy import sparse\nfrom scipy.spatial.distance import pdist\nfrom sklearn.cluster import DBSCAN\nimport sklearn\nimport numpy as np\n\nprint(f\"Init compute distances\"); start_time = time.time()\n# A_sparse = sparse.csr_matrix(df_emb.values.tolist())\n# # distances_cos = cosine_distances(A_sparse)\n# print(f\"Sparse matrix created in: {time.time() - start_time}\")\n# # dot_prods_distance = np.clip((1-linear_kernel(A_sparse)),a_min=0, a_max=None)\n# distances_euc = euclidean_distances(A_sparse)\n# print(f\"Euclidian done in: {time.time() - start_time}\"); start_time = time.time()\ndistances_euc2 = pdist(df_emb.values.tolist(),\"euclidean\")\nprint(f\"Euclidian done in: {time.time() - start_time}\"); start_time = time.time()\n\n# distances_cos = cosine_distances(A_sparse)\n# print(f\"Cosine done in: {time.time() - start_time}\"); start_time = time.time()\n# distances_man = manhattan_distances(A_sparse)\n# print(f\"Manhattan done  in: {time.time() - start_time}\"); start_time = time.time()\n# dot_prods_distance = np.clip((1-linear_kernel(A_sparse)),a_min=0, a_max=None)\n# print(f\"Dot distance done  in: {time.time() - start_time}\"); start_time = time.time()\ngc.collect()\n","f4ea978a":"# distances = distances_euc\n# start_time = time.time()\n# db = DBSCAN(metric=\"precomputed\",eps=0.6).fit(distances)\n# print(f\"Clustering done in: {time.time() - start_time}\"); \n\n# start_time = time.time()\n# db = DBSCAN(eps=0.2, metric=\"euclidean\").fit(df_emb.values.tolist())\n# print(f\"Clustering with metric done in: {time.time() - start_time}\"); start_time = time.time()\n\n","78ac2b1e":"# Eps search\nfrom scipy.spatial.distance import squareform\ndistances = squareform(distances_euc2)\nnlabels = []\nfor e in tqdm(range(1,100)):\n    db = DBSCAN(metric=\"precomputed\",eps=e\/100, min_samples = 50).fit(distances)\n    nlabels.append(len(set(db.labels_)))\nplt.scatter(range(1,100),nlabels)\n","408e440f":" np.argmax(nlabels)\/100","df143808":"# nclusters = 5\nmax_eps = 0.61 # np.argmax(nlabels)\/100\n# max_eps = np.min(np.argwhere(np.array(nlabels) > nclusters))\/100\nprint(f\"argmax epsilon is {max_eps}\")\ndb = DBSCAN(metric=\"precomputed\",eps=max_eps,min_samples=10).fit(distances)\ndf_tmp[\"topic\"] = db.labels_\n\nfrom wordcloud import WordCloud\nfor topic in set(db.labels_):\n\n    all_titles = \" \".join(str(t) for t in df_tmp[df_tmp.topic==topic].title)\n    # Create and generate a word cloud image:\n    wordcloud = WordCloud(background_color=\"white\").generate(all_titles)\n\n    # # Display the generated image:\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    tcount = len(df_tmp[df_tmp.topic==topic])\n    tscore = df_tmp[df_tmp.topic==topic].score.mean()\n    plt.title(f\"Topic{topic}: count {tcount}, score {tscore:.1f}\")\n\n    plt.show()","878b57b0":"db = DBSCAN(metric=\"precomputed\",eps=max_eps,min_samples=20).fit(distances)\ndf_tmp[\"topic\"] = db.labels_\n\nfrom wordcloud import WordCloud\nfor topic in set(db.labels_):\n\n    all_titles = \" \".join(str(t) for t in df_tmp[df_tmp.topic==topic].title)\n    # Create and generate a word cloud image:\n    wordcloud = WordCloud(background_color=\"white\").generate(all_titles)\n\n    # # Display the generated image:\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    tcount = len(df_tmp[df_tmp.topic==topic])\n    tscore = df_tmp[df_tmp.topic==topic].score.mean()\n    plt.title(f\"Topic{topic}: count {tcount}, score {tscore:.1f}\")\n\n    plt.show()","7fb27fec":"db = DBSCAN(metric=\"precomputed\",eps=max_eps,min_samples=30).fit(distances)\ndf_tmp[\"topic\"] = db.labels_\n\nfrom wordcloud import WordCloud\nfor topic in set(db.labels_):\n\n    all_titles = \" \".join(str(t) for t in df_tmp[df_tmp.topic==topic].title)\n    # Create and generate a word cloud image:\n    wordcloud = WordCloud(background_color=\"white\").generate(all_titles)\n\n    # # Display the generated image:\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    tcount = len(df_tmp[df_tmp.topic==topic])\n    tscore = df_tmp[df_tmp.topic==topic].score.mean()\n    plt.title(f\"Topic{topic}: count {tcount}, score {tscore:.1f}\")\n\n    plt.show()","c46f3a1d":"from sklearn.manifold import TSNE\n\n# Using cosine distance\ntsne_model_en_2d = TSNE(perplexity=45, n_components=2, n_iter=2500, random_state=32,metric=\"precomputed\")\nembeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(distances))","ed87bb23":"palette = np.array(sns.color_palette(\"hls\", len(set(db.labels_))))\nplt.scatter(embeddings_en_2d[:,0],embeddings_en_2d[:,1], c=palette[db.labels_])","28002d89":"#### High reposts could either indicate crappy story quality or great story. Depending on whether reposted by the same user or many different ones","055d0c4c":"- filter by single author and multi post\n- look at the mean score per day and evaluate the delta for each post\n- groupby day","4d16eedf":"Most often, there are only 2 reposts and they are coming from different authors, hinting to not necessarily self promotion but an important news spreading. Let's check a couple for forms","caefaeb1":"Average score is getting down with post count. Also spread, is seems to indicate that the story quality goes down with post count up. Let's look at a couple of high count respots","02d4b89a":"Score is higher with rate","e335e482":"## Data cleanup","4c212cb7":"Accompanying notebook from \"[How To Get Upvoted on Hacker News](https:\/\/medium.com\/@jfrederic.plante\/how-to-get-upvoted-on-hacker-news-692360e85ef8)\". It leverages code from [Mentions of Kaggle on Hacker News](https:\/\/www.kaggle.com\/mrisdal\/mentions-of-kaggle-on-hacker-news) and [SQL Scavenger Hunt Handbook](https:\/\/www.kaggle.com\/rtatman\/sql-scavenger-hunt-handbook) and [Analyzing Hacker News Stories](https:\/\/www.kaggle.com\/vksbhandary\/big-data-analysis-analyzing-hacker-news-stories#Y-Combinator) kernels.","4b2254fa":"Reposting twice increases the likelyhood of being upvoted dramatically? Let's look at a stack bar graph of first post mean score and max additional score from subsequent posts.","1d2ee573":"### Word clouds","5c026feb":"### Max score increase vs. number of resposts","9dd4cf0b":"The notebook is roughly organized as \n - data loading from big query\n - exploratory data analysis (what correlates with the score)\n - how helpful is repeat posting\n - topic discovery with BERT and Google sentence embedder + clustering","c307a1fa":"# What correlates with the score","cc0b672d":"## Score vs days","a38dfc14":"### Correlation duplicate stories w\/ day of the week","d63bc804":"# Data exploration and cleanup","0bcf190c":"## Using BERT embeddings","e1630216":"### How about when by only one contributor","92348ff8":"# How helpful is multi-posting","e8f2af08":"It is unclear from that graph if the repost drive score higher or high value stories tend to get reposted","b1298d9f":"## Number of post counts vs authors","e5476a8d":"Looks like a beautiful exponential distribution, as would be expected from a Bernoulli distribution. See related article [the incredible shrinking Bernoulli](https:\/\/towardsdatascience.com\/the-incredible-shrinking-bernoulli-de16aac524a)","819e4f4a":"# Hacker news. Better days to post\n\n","c83422f8":"What percentage are repost according to this: around 4% or 19K\n\n","51dfdf51":"## Flag Show HN and Ask HN stories","058aa046":"# Topic discovery and average scoring by topic","42faecda":"### Is the spread depending on the time it was reposted?","e40b138c":"## Rate or story count by day","41a4dc53":"This seems to indicate really 4 modes: week-end vs week and Day pattern. Weird spike on Monday?\n\nWeek-end are overall slower in terms of stories posted. Saturday and Sunday are equivalent","0f081082":"## Flag stories that contain hacker news in the title","6b363d7d":"So if the calculation is not wrong, that trend of posting on week-end is not statistically significant (less than 5%)","b2961d1d":"Using [BERT as a service](https:\/\/github.com\/hanxiao\/bert-as-service) not an option as Kaggle doesn't allow background services...","5f4f9046":"## Using Google sentence embedder\nLeveraging code from https:\/\/colab.research.google.com\/github\/tensorflow\/hub\/blob\/master\/examples\/colab\/semantic_similarity_with_tf_hub_universal_encoder.ipynb#scrollTo=MSeY-MUQo2Ha","14c86cd0":"## Loading the data from BigQuery"}}