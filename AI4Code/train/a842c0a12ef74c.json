{"cell_type":{"2469bb23":"code","cac03116":"code","e90e4eb7":"code","d8b9eaad":"code","9e88897e":"code","8d87d899":"code","82b87d6f":"code","7a152bfb":"code","406735dd":"code","92423fe8":"code","c2798e65":"code","706e0493":"code","a91fa59d":"code","68d65ede":"code","9c439425":"code","d9982bca":"code","ac124bcc":"code","9eae5546":"code","6e7565e9":"code","5cc8da87":"code","50082aee":"code","68291b93":"code","de6cde4b":"code","da16a44a":"code","e6ffffd8":"code","363915f4":"code","7374c053":"code","657d6139":"code","8565da79":"code","df6d4e9b":"code","7d883d79":"code","c26119ca":"code","237d2ea1":"code","fdf8389f":"code","47973018":"code","b4adb849":"code","23777457":"code","e6817b1f":"code","81bbf79f":"markdown","b3de0f97":"markdown","e9fb2f06":"markdown","61ffc6b0":"markdown","a6486e25":"markdown","44ea94a2":"markdown","5efa7220":"markdown","f3563e92":"markdown","94403fd4":"markdown","de43473e":"markdown","7b04d4b3":"markdown","27896b4c":"markdown","54289c35":"markdown","a8e76acf":"markdown","81238225":"markdown","64e90270":"markdown","230044fa":"markdown","34518c9b":"markdown","9aabdfa1":"markdown"},"source":{"2469bb23":"import pandas as pd\nimport numpy as np\nfrom scipy import stats\n\nfrom datetime import date\n\nfrom sklearn.preprocessing import OrdinalEncoder, MinMaxScaler, StandardScaler, KBinsDiscretizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import KMeans, DBSCAN, MeanShift\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score\n\nimport matplotlib.pylab as plt # plotting\nimport plotly.graph_objs as go\nimport seaborn as sns\n\nimport ipywidgets as widgets\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npd.set_option('display.max_columns', None)\n#pd.set_option('display.max_rows', None)","cac03116":"df = pd.read_csv('\/kaggle\/input\/customer-personality-analysis\/marketing_campaign.csv', sep='\\t')\ndf.head(2)","e90e4eb7":"df.describe()","d8b9eaad":"#Get today's date\ntoday = date.today()\n\ndf['age'] = today.year - df['Year_Birth']\ndf['children'] = df['Kidhome'] + df['Teenhome']\n\ndf['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'], errors='coerce')\ndf['seniority'] = today.year - df['Dt_Customer'].dt.year","9e88897e":"plt.figure(figsize=(20,20))\nfilter_out = [\n 'ID',\n 'Year_Birth',\n 'Kidhome',\n 'Teenhome',\n #'AcceptedCmp3',\n #'AcceptedCmp4',\n #'AcceptedCmp5',\n #'AcceptedCmp1',\n #'AcceptedCmp2',\n #'Complain',\n 'Z_CostContact',\n 'Z_Revenue',\n #'Response'\n]\n# select columns with numeric values\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnum_df = df.select_dtypes(include=numerics)\n\n# drop by Name\nnum_df = num_df.drop(filter_out, axis=1)\ncolumns = num_df.columns.to_list()\nfor i,column in enumerate(columns):\n    i += 1\n    plt.subplot(5,5,i)\n    num_df[column].plot.kde()\n    plt.title(column)\n    plt.ticklabel_format(style='plain', axis='x', scilimits=(0,0))\n    #plt.legend()","8d87d899":"df.info()","82b87d6f":"cat_df = df.select_dtypes(include=[\"object\"])   \ncolumns = cat_df.columns.to_list()\n\nfor i,column in enumerate(columns):\n    i += 1\n    plt.subplot(1,2,i)\n    cat_df[column].value_counts().plot(kind='bar')\n    plt.title(column)\n    #plt.ticklabel_format(style='plain', axis='x', scilimits=(0,0))\n    #plt.legend()","7a152bfb":"num_df.columns.to_list()","406735dd":"df['sum_promotions'] = df['AcceptedCmp1']+df['AcceptedCmp2']+df['AcceptedCmp3']+df['AcceptedCmp4']+df['AcceptedCmp5']+df['Response']\ndf['accepted_promotions'] = np.where(df['sum_promotions']>=1, 1, 0)\n#df['MntFood'] = df['MntFruits'] + df['MntFishProducts'] + df['MntMeatProducts'] + df['MntSweetProducts']\n\n\nto_drop = [\n 'ID',\n 'Year_Birth',\n 'Kidhome',\n 'Teenhome',\n 'Dt_Customer',\n 'Z_CostContact',\n 'Z_Revenue',\n 'AcceptedCmp3',\n 'AcceptedCmp4',\n 'AcceptedCmp5',\n 'AcceptedCmp1',\n 'AcceptedCmp2',\n 'Complain',\n 'Response',\n 'sum_promotions'\n]\n\ndf.drop(columns=to_drop, inplace=True)\n\ndf.isnull().sum()","92423fe8":"df.dropna(subset=['Income'], inplace=True)\ndf = df[df.age <= 100]\ndf['Marital_Status'] = np.where(df['Marital_Status'].isin(['Absurd', 'Alone', 'YOLO']), 'other', df['Marital_Status'])\n\n","c2798e65":"# OrdinalEncoder to the Education column\nencoder = OrdinalEncoder(categories=[['Basic','2n Cycle', 'Graduation', 'Master', 'PhD']])\n\ndf[['Education_encoded']] = encoder.fit_transform(df[['Education']])\ndf","706e0493":"# Bin the numerical features that have a wide range of values, being careful about outliers.\n\nnum_df = df.select_dtypes(include=numerics)\nd = {}\nfor col in num_df.columns.to_list():\n    diff = max(num_df[col]) - min(num_df[col])\n    d[col] = diff\nd","a91fa59d":"col2bin = list({k: v for k, v in d.items() if v > 10}.keys())\ncolnot2bin = list({k: v for k, v in d.items() if v <= 10}.keys())\ndf[colnot2bin].head(2)","68d65ede":"plt.figure(figsize=(20,20))\nfor i,column in enumerate(col2bin):\n    i += 1\n    plt.subplot(4,4,i)\n    plt.boxplot(df[column])\n    plt.title(column)\n    #plt.ticklabel_format(style='plain', axis='x', scilimits=(0,0))\n    #plt.legend()","9c439425":"# Calculating the Z-Score of each numerical column (it helps to identify the outliers)\nz = np.abs(stats.zscore(num_df))\nthreshold = 3\n# print(np.where(z > threshold), len(np.where(z > threshold)[0]))\n\n# I will delete the rows that have a value higher than the threshold, hence outliers.\ndf_o = df[(z <= threshold).all(axis=1)]\n\ndf_o.shape, df.shape","d9982bca":"plt.figure(figsize=(20,20))\nfor i,column in enumerate(col2bin):\n    i += 1\n    plt.subplot(4,4,i)\n    plt.boxplot(df_o[column])\n    plt.title(column)\n    #plt.ticklabel_format(style='plain', axis='x', scilimits=(0,0))\n    #plt.legend()","ac124bcc":"# Finding the correlation between the feature column\n\nplt.figure(figsize=(20,20))\nsns.heatmap(df_o.corr(), annot=True)\nplt.show()","9eae5546":"X = df_o[col2bin].values\ndisc = KBinsDiscretizer(n_bins=10, encode='ordinal', \n                        strategy='uniform')\nbinned_df = pd.DataFrame(disc.fit_transform(X), columns=col2bin)\nbinned_df.head(2)","6e7565e9":"dummy_df = pd.get_dummies(df_o['Marital_Status'])\ndummy_df.head(2)","5cc8da87":"df_o[colnot2bin].head(2)","50082aee":"print(binned_df.shape,  dummy_df.shape, df_o[colnot2bin].shape)\nbinned_df.reset_index(drop=True, inplace=True)\ndummy_df.reset_index(drop=True, inplace=True)\nnotbinned_df = df_o[colnot2bin]\nnotbinned_df.reset_index(drop=True, inplace=True)\ntemp_df = pd.concat([binned_df, dummy_df], axis=1)\ndf_final = pd.concat([temp_df, notbinned_df], axis=1)\ndf_final.head(2)","68291b93":"scaler = MinMaxScaler()\nscaled_df = round(pd.DataFrame(scaler.fit_transform(df_final), columns=df_final.columns.to_list()), 2)\nscaled_df.head(2)","de6cde4b":"#Compute inertia for different values of k (# of clusters)\nsum_of_squared_distances = []\nK = range(1,10)\nfor k in K:\n    km = KMeans(n_clusters=k, random_state=42)\n    km = km.fit(scaled_df)\n    sum_of_squared_distances.append(km.inertia_)\n    \n# Plot results\nplt.plot(K, sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()","da16a44a":"# Instantiate k-means algorithm\nkmeans = KMeans(n_clusters=5)\n# Fit the algorithm to the features\nkmeans.fit(scaled_df)\n\n# Compute the silhouette score\nkmeans_silhouette = silhouette_score(\n    scaled_df, kmeans.labels_\n).round(2)\nkmeans_silhouette","e6ffffd8":"# countplot to check the number of clusters and number of customers in each cluster\ny_clusters = kmeans.predict(scaled_df)\nsns.countplot(y_clusters)","363915f4":"df_o['cluster'] = y_clusters\ndf_o.head(2)","7374c053":"# 3d scatterplot using plotly\nScene = dict(xaxis = dict(title  = 'Income -->'),yaxis = dict(title  = 'NumDealsPurchases--->'),zaxis = dict(title  = 'MntMeatProducts-->'))\n\n# model.labels_ is nothing but the predicted clusters i.e y_clusters\nlabels = kmeans.labels_\ntrace = go.Scatter3d(x=df_o['Income'], y=df_o['NumDealsPurchases'], z=df_o['MntMeatProducts'], mode='markers',marker=dict(color = labels, size= 10, line=dict(color= 'black',width = 10)))\nlayout = go.Layout(margin=dict(l=0,r=0),scene = Scene,height = 800,width = 800)\ndata = [trace]\nfig = go.Figure(data = data, layout = layout)\nfig.show()","657d6139":"df_o.info()","8565da79":"# define the model\ndbscan = DBSCAN(eps=0.50, min_samples=10)\n# fit model and predict clusters\nyhat = dbscan.fit_predict(scaled_df)\n# retrieve unique clusters\nclusters = np.unique(yhat)\nlen(clusters)","df6d4e9b":"# Compute the silhouette score\ndbscan_silhouette = silhouette_score(\n    scaled_df, dbscan.labels_\n).round(2)\ndbscan_silhouette","7d883d79":"# countplot to check the number of clusters and number of customers in each cluster\nsns.countplot(yhat)","c26119ca":"# define the model\nmeanshift = MeanShift()\n# fit model and predict clusters\nyhat = meanshift.fit_predict(scaled_df)\n# retrieve unique clusters\nclusters = np.unique(yhat)\nlen(clusters)","237d2ea1":"pca = PCA().fit(scaled_df)\n\nnum_cols = len(scaled_df.columns.to_list())\n\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\nfig, ax = plt.subplots()\nxi = np.arange(1, num_cols+1, step=1)\ny = np.cumsum(pca.explained_variance_ratio_)\n\nplt.ylim(0.0,1.1)\nplt.plot(xi, y, marker='o', linestyle='--', color='b')\n\nplt.xlabel('Number of Components')\nplt.xticks(xi) #change from 0-based array index to 1-based human-readable label\nplt.ylabel('Cumulative variance (%)')\nplt.title('The number of components needed to explain variance')\n\nplt.axhline(y=0.95, color='r', linestyle='-')\nplt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n\nax.grid(axis='x')\nplt.show()","fdf8389f":"X = scaled_df.copy()\n\npca = PCA(n_components=16)\npca.fit(X)\nX_pca = pca.transform(X)\nprint(\"original shape:   \", X.shape)\nprint(\"transformed shape:\", X_pca.shape)","47973018":"#Compute inertia for different values of k (# of clusters)\nsum_of_squared_distances = []\nK = range(1,10)\nfor k in K:\n    km = KMeans(n_clusters=k, random_state=42)\n    km = km.fit(X_pca)\n    sum_of_squared_distances.append(km.inertia_)\n    \n# Plot results\nplt.plot(K, sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()","b4adb849":"# Instantiate k-means algorithm\nkmeans = KMeans(n_clusters=5)\n# Fit the algorithm to the features\nkmeans.fit(X_pca)\n\n# Compute the silhouette score\nkmeans_silhouette = silhouette_score(\n    scaled_df, kmeans.labels_\n).round(2)\nkmeans_silhouette","23777457":"y_clusters = kmeans.predict(X_pca)\ndf_o['cluster_pca'] = y_clusters","e6817b1f":"# 3d scatterplot using plotly\nScene = dict(xaxis = dict(title  = 'Income -->'),yaxis = dict(title  = 'NumDealsPurchases--->'),zaxis = dict(title  = 'MntMeatProducts-->'))\n\n# model.labels_ is nothing but the predicted clusters i.e y_clusters\nlabels = kmeans.labels_\ntrace = go.Scatter3d(x=df_o['Income'], y=df_o['NumDealsPurchases'], z=df_o['MntMeatProducts'], mode='markers',marker=dict(color = labels, size= 10, line=dict(color= 'black',width = 10)))\nlayout = go.Layout(margin=dict(l=0,r=0),scene = Scene,height = 800,width = 800)\ndata = [trace]\nfig = go.Figure(data = data, layout = layout)\nfig.show()","81bbf79f":"I will now get the dummies of each categorical feature","b3de0f97":"End of the preprocessing part. The atrifacts that have been created are:\n- encoder: OrdinalEncoder\n- disc (binner): KBinsDiscretizer\n- scaler: MaxMinScaler\n\n\n\nI will know apply some clustering technique and will compare the results to pick the one that performs best","e9fb2f06":"We will reduce to 16 components.","61ffc6b0":"As expected, they show multiple outliers. Let's filter them out.","a6486e25":"## Mean Shift","44ea94a2":"I will now check how the data has changed after treating the outliers.","5efa7220":"#                    Thank you!\n### If you found this kernel helpful please upvote! It pushes me to make more kernels\n### Other kernels:\n- https:\/\/www.kaggle.com\/simomatt\/decision-tree-svm-and-random-forest\/notebook","f3563e92":"# Reading, Visualization and Preprocessing","94403fd4":"I will bin all the features that have a difference between max and min > 10.\n\nBut first, I will boxplot those features to check whether there are outliers.","de43473e":"# 3D plot of the clusters","7b04d4b3":"I will now scale the final dataframe.","27896b4c":"I will now bin the columns I picked earlier","54289c35":"# KMeans applied to PCA","a8e76acf":"I will apply the following transformations to the features:\n- OrdinalEncoder to the Education column\n- Bin the numerical features that have a wide range of values, being careful to remove the outliers first.\n- Dummies to the remaining categorical features \n- MinMaxScaler to the features \n\nBefore applying any scaling transformations it is very important to split your data into a train set and a test set. If you start scaling before, your training (and test) data might end up scaled around a mean value that is not actually the mean of the train or test data, and go past the whole reason why you\u2019re scaling in the first place.","81238225":"# Problem Statement\n\nCustomer Personality Analysis is a detailed analysis of a company\u2019s ideal customers. It helps a business to better understand its customers and makes it easier for them to modify products according to the specific needs, behaviors and concerns of different types of customers.\n\nCustomer personality analysis helps a business to modify its product based on its target customers from different types of customer segments. For example, instead of spending money to market a new product to every customer in the company\u2019s database, a company can analyze which customer segment is most likely to buy the product and then market the product only on that particular segment.\n\n\n## Attributes\n\n### People\n\n- ID: Customer's unique identifier\n- Year_Birth: Customer's birth year\n- Education: Customer's education level\n- Marital_Status: Customer's marital status\n- Income: Customer's yearly household income\n- Kidhome: Number of children in customer's household\n- Teenhome: Number of teenagers in customer's household\n- Dt_Customer: Date of customer's enrollment with the company\n- Recency: Number of days since customer's last purchase\n- Complain: 1 if customer complained in the last 2 years, 0 otherwise\n\n### Products\n\n- MntWines: Amount spent on wine in last 2 years\n- MntFruits: Amount spent on fruits in last 2 years\n- MntMeatProducts: Amount spent on meat in last 2 years\n- MntFishProducts: Amount spent on fish in last 2 years\n- MntSweetProducts: Amount spent on sweets in last 2 years\n- MntGoldProds: Amount spent on gold in last 2 years\n\n### Promotion\n\n- NumDealsPurchases: Number of purchases made with a discount\n- AcceptedCmp1: 1 if customer accepted the offer in the 1st campaign, 0 otherwise\n- AcceptedCmp2: 1 if customer accepted the offer in the 2nd campaign, 0 otherwise\n- AcceptedCmp3: 1 if customer accepted the offer in the 3rd campaign, 0 otherwise\n- AcceptedCmp4: 1 if customer accepted the offer in the 4th campaign, 0 otherwise\n- AcceptedCmp5: 1 if customer accepted the offer in the 5th campaign, 0 otherwise\n- Response: 1 if customer accepted the offer in the last campaign, 0 otherwise\n\n### Place\n\n- NumWebPurchases: Number of purchases made through the company\u2019s web site\n- NumCatalogPurchases: Number of purchases made using a catalogue\n- NumStorePurchases: Number of purchases made directly in stores\n- NumWebVisitsMonth: Number of visits to company\u2019s web site in the last month\n\n## Target\nNeed to perform clustering to summarize customer segments.","64e90270":"## DBSCAN","230044fa":"# K-means\nHow to identify the optimal number of clusters? <br>\nFor each k value, we will initialise k-means and use the inertia attribute to identify the sum of squared distances of samples to the nearest cluster centre.","34518c9b":"# PCA Principal Component Analysis\n## How to select the right number of components\nNow, we know that the principal components explain a part of the variance. From the Scikit-learn implementation, we can get the information about the explained variance and plot the cumulative variance.","9aabdfa1":"What I want to do first, is to get to know better the data at hand. <br>\n- I'll create 3 more columns, age, number of children and seniority\n- I'll plot the numerical and categorical variables"}}