{"cell_type":{"00f5709a":"code","0d9828cc":"code","80fec0d8":"code","dc1c9c4a":"code","bc353c85":"code","ffae5271":"code","9dd51e71":"code","f4c9605d":"code","b29a3986":"code","5935d950":"code","530587e7":"code","57fd79de":"code","ba61a072":"code","a1e6e652":"code","a99987d2":"code","8e7deb45":"code","05448e91":"code","6e07851d":"code","251115c9":"code","26b07e01":"code","c76562e5":"code","90dad989":"code","1a2ab4b4":"code","232417ac":"code","fdd35f4f":"code","72485888":"code","dbb9a36b":"code","ade58538":"code","6df2ad64":"markdown","9b5f9727":"markdown","99e2d170":"markdown","fbfb26d3":"markdown","e934247d":"markdown","5f9dc54e":"markdown","22c3d67a":"markdown","f52495f5":"markdown"},"source":{"00f5709a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0d9828cc":"#load-data\ndata = pd.read_csv(\"\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")","80fec0d8":"data.head()","dc1c9c4a":"data.info()","bc353c85":"data.columns","ffae5271":"data.describe()","9dd51e71":"#correlation \ndata.corr()","f4c9605d":"#correlation map\n\nf,ax = plt.subplots(figsize=(10,10))\nsns.heatmap(data.corr(),annot = True , linewidth = .5, fmt = '.2f',ax = ax)\nplt.show()","b29a3986":"#counts of quality types\ndata.quality.value_counts()","5935d950":"#visualization\n\n#pie_graph\nplt.figure(1, figsize=(8,8))\ndata.quality.value_counts().plot.pie(autopct=\"%1.1f%%\")","530587e7":"sns.countplot(x=\"quality\", data=data)\ndata.loc[:,'quality'].value_counts()","57fd79de":"#scatter plot\ndata.plot(kind = \"scatter\" , x = \"alcohol\" , y = \"quality\" , color = \"red\")\nplt.xlabel(\"alcohol\")\nplt.ylabel(\"quality\")\nplt.title(\"alcohol-quality Scatter Plot\")\nplt.show()","ba61a072":"#line plot \ndata.sulphates.plot(kind = \"line\" , color = \"red\" , alpha = 0.5 , grid = True , linestyle = \":\",label = \"sulphates\")\ndata.chlorides.plot(kind = \"line\" , color = \"green\" , alpha = 0.5 , linestyle = \"-\",label = \"chlorides\")\nplt.legend(loc = \"upper right\")\nplt.xlabel(\"x axis\")\nplt.ylabel(\"y axis\")\nplt.title(\"sulphates-chlorides Scatter Plot\")\nplt.show()","a1e6e652":"y = data.quality.values\nx_data = data.drop([\"quality\"],axis = 1)\n","a99987d2":"#normalization\nx = (x_data - np.min(x_data))\/(np.max(x_data) - np.min(x_data))","8e7deb45":"#train-test split\nfrom sklearn.model_selection import train_test_split\nx_train , x_test , y_train , y_test = train_test_split(x,y,test_size = 0.2,random_state = 42 )","05448e91":"#predict with 3 neighbor\nfrom sklearn.neighbors import KNeighborsClassifier \nknn = KNeighborsClassifier(n_neighbors = 10) #n_neighbors = k\nknn.fit(x_train,y_train)\nprint(\"{} nn score : {}\".format(3,knn.score(x_test,y_test)))","6e07851d":"#finding best accuracy for n-neighbor\nscore_list = []\nfor each in range(1,15):\n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n    \nplt.plot(range(1,15),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")    \nplt.show()","251115c9":"#confusion matrix for KNN\nfrom sklearn.metrics import confusion_matrix\n\ny_pred = knn.predict(x_test)\ny_true = y_test\ncm = confusion_matrix(y_true,y_pred)\nf,ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm,annot = True,linewidth = 0.5,linecolor = \"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()\n","26b07e01":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\n\nprint(\"score : {}\".format(dt.score(x_test,y_test)))","c76562e5":"#confusion matrix for DecisionTree\nfrom sklearn.metrics import confusion_matrix\n\ny_pred = dt.predict(x_test)\ny_true = y_test\ncm = confusion_matrix(y_true,y_pred)\nf,ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm,annot = True,linewidth = 0.5,linecolor = \"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","90dad989":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 100 , random_state = 1)\nrf.fit(x_train,y_train)\n\nprint(\"score : {}\".format(rf.score(x_test,y_test)))","1a2ab4b4":"#finding best accuracy for n_estimators\nscore_list = []\nfor each in range(1,50):\n    rf2 = RandomForestClassifier(n_estimators = each)\n    rf2.fit(x_train,y_train)\n    score_list.append(rf2.score(x_test,y_test))\n    \nplt.plot(range(1,50),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")    \nplt.show()","232417ac":"#confusion matrix for RandomForestClassification\nfrom sklearn.metrics import confusion_matrix\n\ny_pred = rf.predict(x_test)\ny_true = y_test\ncm = confusion_matrix(y_true,y_pred)\nf,ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm,annot = True,linewidth = 0.5,linecolor = \"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","fdd35f4f":"from sklearn.svm import SVC\n\nsvm = SVC(random_state = 1)\nsvm.fit(x_train,y_train)\nprint(\"Accuracy of svm :  {}\".format(svm.score(x_test,y_test)))","72485888":"#confusion matrix for SVM\nfrom sklearn.metrics import confusion_matrix\n\ny_pred = svm.predict(x_test)\ny_true = y_test\ncm = confusion_matrix(y_true,y_pred)\nf,ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm,annot = True,linewidth = 0.5,linecolor = \"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","dbb9a36b":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\nprint(\"Accuracy of naive-bayes : {}\".format(nb.score(x_test,y_test)))","ade58538":"#confusion matrix for Naive-Bayes\nfrom sklearn.metrics import confusion_matrix\n\ny_pred = nb.predict(x_test)\ny_true = y_test\ncm = confusion_matrix(y_true,y_pred)\nf,ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm,annot = True,linewidth = 0.5,linecolor = \"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","6df2ad64":"5. **Naive-Bayes Classification**","9b5f9727":"**1. K-Nearest Neighbors(KNN) Classification**","99e2d170":"> **ANALYZING DATA**","fbfb26d3":"2. **Decision Tree Classification**","e934247d":"4. **SUPPORT VECTOR MACHINE(SVM)**","5f9dc54e":"3. **Random Forest Classification**","22c3d67a":"> > **ML Algortihms**","f52495f5":"> **Visualization**"}}