{"cell_type":{"e713093a":"code","90c460ed":"code","8eceaaed":"code","55d692f8":"code","3c1c40ac":"code","c176dc1a":"code","5712bfe8":"code","eb00d688":"code","e8e8bb8c":"code","4f9ed915":"code","fcf89397":"code","08eb7289":"code","9532c65a":"code","68a238f9":"code","51081e3e":"code","e3c5b1c3":"code","de03e776":"code","e8a0a5a3":"code","98091cc1":"code","01d89f1d":"code","6cd776c8":"code","589c884d":"code","8a83d6f7":"code","423bf661":"code","d117faea":"markdown","a630d0d6":"markdown","de00d56f":"markdown","919e2e00":"markdown","5eb8996a":"markdown","63252112":"markdown","4b4500aa":"markdown","fbcb67c4":"markdown","aa1654f0":"markdown","c4460e95":"markdown","0c1b1032":"markdown","85902100":"markdown","f9971b76":"markdown","2946d74a":"markdown","d8ef0332":"markdown","edc19e17":"markdown","d326c3ac":"markdown","41ba7304":"markdown","6accc0b2":"markdown","3ed32b3d":"markdown","631bdf60":"markdown"},"source":{"e713093a":"import numpy as np\nimport pandas as pd\nimport sklearn\nimport re\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import (RandomForestClassifier,\n                             AdaBoostClassifier,\n                             GradientBoostingClassifier,\n                             ExtraTreesClassifier)\n\nfrom sklearn.svm import SVC\nfrom sklearn.cross_validation import KFold","90c460ed":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\nPassengerId = test['PassengerId']\ntrain.head(3)","8eceaaed":"full_data = [train, test]\ntrain['Name_length'] = train['Name'].apply(len)\ntest['Name_length'] = test['Name'].apply(len)\n\n#Create a new feature reserve cabin\ntrain['Has_Cabin'] = train['Cabin'].apply(lambda x:0 if type(x) == float else 1)\ntest['Has_Cabin'] = test['Cabin'].apply(lambda x:0 if type(x) == float else 1)\n\n#Create a new feature FamilySize\nfor dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\n#Create a new feature IsAlone y=1\/n=0\nfor dataset in full_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\n#Remove all NULLs in Embarked columns\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n\n#Remove all NULLs in Fare columns\nfor dataset in full_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n\n#Create a new feature CategoricalAge\nfor dataset in full_data:\n    age_avg = dataset['Age'].mean()\n    age_std = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    \n    #\u7121\u52b9\u306e\u5024\u306b\u5bfe\u3057\u3066\u3001\u5e73\u5747\u3068\u6a19\u6e96\u504f\u5dee\u3092\u4f7f\u7528\u3057\u305f\u30e9\u30f3\u30c0\u30e0\u5024\u3092\u5272\u308a\u632f\u308b\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)\n\n#\u4e57\u5ba2\u306e\u540d\u524d\u304b\u3089\u656c\u79f0(Mr.Ms\u306a\u3069)\u3092\u53d6\u5f97\u3059\u308b\u95a2\u6570\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    if title_search:\n        return title_search.group(1)\n    return\n\n#Create a new feature Title\nfor dataset in full_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n\n#\u5909\u308f\u3063\u305f\u656c\u79f0\u3092\u7ba1\u7406\u3057\u3084\u3059\u304f\u3059\u308b\u305f\u3081\u306b\u30ab\u30c6\u30b4\u30e9\u30a4\u30ba\u3001\u4e00\u822c\u7684\u306a\u3082\u306e\u306b\u5909\u66f4\nfor dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady','Countess','Capt','Col','Don','Dr','Major','Rev','Sir','Jonkheer','Dona'],'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\nfor dataset in full_data:\n    #Mapping Sex\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n    \n    #Mapping titles\n    title_mapping = {'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Rare': 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \n    #Mapping Embarked\n    dataset['Embarked'] = dataset['Embarked'].map({'S': 0, 'C': 1, 'Q': 2}).astype(int)\n    \n    #Mapping Fare\n    dataset.loc[dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91 ) &\n                (dataset['Fare'] <= 14.454 ) , 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454 ) &\n                (dataset['Fare'] <= 31 ) , 'Fare'] = 2\n    dataset.loc[dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\n    #Mapping Age\n    dataset.loc[dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[dataset['Age'] > 64, 'Age'] = 4;","55d692f8":"#Feature selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\ntest = test.drop(drop_elements, axis = 1)","3c1c40ac":"train.head(3)","c176dc1a":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Feature', y=1.05, size=15)\n\n#\u5404\u30c7\u30fc\u30bf\u306e\u76f8\u95a2\u6307\u6570\u3092\u51fa\u3059\u95a2\u6570\u3000\u3000train.corr()\n\nsns.heatmap(train.astype(float).corr(), linewidth=0.1, vmax=1.0,\n           square=True, cmap=colormap, linecolor='white',annot=True)\n","5712bfe8":"g = sns.pairplot(train[ [u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked', u'FamilySize', u'Title'] ],\n                hue='Survived', palette='seismic', size=1.2, diag_kind='kde', diag_kws=dict(shade=True), plot_kws=dict(s=10))\n\ng.set(xticklabels=[])","eb00d688":"# \u4f7f\u3044\u3084\u3059\u3044\u3088\u3046\u306b\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u306e\u5909\u6570\u3092\u7528\u610f\u3059\u308b\nntrain = train.shape[0]\nntest = test.shape[0]\n#\u4e71\u6570\u306e\u56fa\u5b9a\u5024\nSEED = 0\n#\u4ea4\u5dee\u691c\u8a3c\u306e\u56de\u6570\nNFOLDS = 5\n#\u4ea4\u5dee\u691c\u8a3c(\u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3)\u3067\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u691c\u8a3c\u3057\u3066\u3001\u904e\u5b66\u7fd2\u304c\u8d77\u3053\u3089\u306a\u3044\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u6c7a\u5b9a\u3059\u308b\nkf = KFold(ntrain, n_folds=NFOLDS, random_state=SEED)\n\n# Sklearn\u306e\u30d8\u30eb\u30d1\u30fc\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n\n    \n    def feature_importances(self,x,y):\n        return self.clf.fit(x,y).feature_importances_\n    \n    def afeature_importances(self,x,y):\n        print(self.clf.fit(x,y).feature_importances_)","e8e8bb8c":"def get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","4f9ed915":"# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_estimators': 500,\n     #'max_features': 0.2,\n    'max_depth': 5,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# Support Vector Classifier parameters \nsvc_params = {\n    'kernel' : 'linear',\n    'C' : 0.025\n    }","fcf89397":"#create 5 objects that represent our 4 models\nrf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\net = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\nada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\ngb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\nsvc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)","08eb7289":"#\u751f\u5b58\u3057\u3066\u3044\u308b\u304b\u306e\u884c\u5217\u3092\u4e00\u6b21\u5143\u914d\u5217\u306b\u3059\u308b\ny_train = train['Survived'].ravel()\ntrain = train.drop(['Survived'], axis=1)\nx_train = train.values #train\u30c7\u30fc\u30bf\u306e\u914d\u5217\nx_test = test.values #test\u30c7\u30fc\u30bf\u306e\u914d\u5217","9532c65a":"#Create our OOF train and test predictions. These base results will be used as new features\n\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test)  #Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf, x_train, y_train, x_test) #Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) #AdaBoost\ngb_oof_train, gb_oof_test = get_oof(gb, x_train, y_train, x_test) #Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc, x_train, y_train, x_test) #Support Vector Classifier\n\nprint('Training is complete')\n","68a238f9":"rf_feature = rf.feature_importances(x_train,y_train)\net_feature = et.feature_importances(x_train, y_train)\nada_feature = ada.feature_importances(x_train, y_train)\ngb_feature = gb.feature_importances(x_train,y_train)","51081e3e":"#\u3053\u3053\u3067\u30ea\u30b9\u30c8\u306b\u3057\u3066\u5909\u6570\u306b\u4ee3\u5165\u3057\u3066\u3044\u308b\u304c\u3001\u5024\u304c\u5909\u308f\u3063\u3066\u3044\u308b\u7406\u7531\u304c\u308f\u304b\u3089\u306a\u3044\nrf_features = [0.10474135,  0.21837029,  0.04432652,  0.02249159,  0.05432591,  0.02854371\n  ,0.07570305,  0.01088129 , 0.24247496,  0.13685733 , 0.06128402]\net_features = [ 0.12165657,  0.37098307  ,0.03129623 , 0.01591611 , 0.05525811 , 0.028157\n  ,0.04589793 , 0.02030357 , 0.17289562 , 0.04853517,  0.08910063]\nada_features = [0.028 ,   0.008  ,      0.012   ,     0.05866667,   0.032 ,       0.008\n  ,0.04666667 ,  0.     ,      0.05733333,   0.73866667,   0.01066667]\ngb_features = [ 0.06796144 , 0.03889349 , 0.07237845 , 0.02628645 , 0.11194395,  0.04778854\n  ,0.05965792 , 0.02774745,  0.07462718,  0.4593142 ,  0.01340093]","e3c5b1c3":"cols = train.columns.values\n\nfeature_dataframe = pd.DataFrame( {'features': cols,\n     'Random Forest feature importances': rf_feature,\n     'Extra Trees feature importances': et_feature,\n      'AdaBoost feature importances': ada_feature,\n    'Gradient Boost feature importances': gb_feature\n    })\n","de03e776":"#Scatter plot\n\n#Random Forest\ntrace = go.Scatter(\n    y = feature_dataframe['Random Forest feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n#       size= feature_dataframe['AdaBoost feature importances'].values,\n        #color = np.random.randn(500), #set color equal to a variable\n        color = feature_dataframe['Random Forest feature importances'].values,\n        colorscale = 'Portland',\n        showscale = True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Random Forest Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis = dict(\n        title = 'Feature Importance',\n        ticklen = 5,\n        gridwidth = 2\n    ),\n    showlegend = False\n)\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig,filename = 'scatter2010')\n\n\n#Extra Trees\ntrace = go.Scatter(\n    y = feature_dataframe['Extra Trees feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode = 'markers',\n    marker = dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n        color = feature_dataframe['Extra Trees feature importances'].values,\n        colorscale = 'Portland',\n        showscale = True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Extra Trees Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig, filename = 'scatter2010')\n\n\n#AdaBoost\ntrace = go.Scatter(\n    y = feature_dataframe['AdaBoost feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode = 'markers',\n    marker = dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n        color = feature_dataframe['AdaBoost feature importances'].values,\n        colorscale = 'Portland',\n        showscale = True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout = go.Layout(\n    autosize = True,\n    title = 'AdaBoost Feature Importance',\n    hovermode = 'closest',\n    yaxis = dict(\n        title = 'Feature Importance',\n        ticklen = 5,\n        gridwidth = 2\n    ),\n    showlegend = False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='scatter2010')\n\n\n#Gradient Boost\ntrace = go.Scatter(\n    y = feature_dataframe['Gradient Boost feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode = 'markers',\n    marker = dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n        color = feature_dataframe['Gradient Boost feature importances'].values,\n        colorscale = 'Portland',\n        showscale = True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout = go.Layout(\n    autosize = True,\n    title = 'Gradient Boosting Feature Importances',\n    hovermode = 'closest',\n    yaxis = dict(\n        title = 'Feature',\n        ticklen = 5,\n        gridwidth = 2\n    ),\n    showlegend = False\n)\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig, filename='scatter2010')","e8a0a5a3":"feature_dataframe['mean'] = feature_dataframe.mean(axis = 1)\nfeature_dataframe.head(11)","98091cc1":"y = feature_dataframe['mean'].values\nx = feature_dataframe['features'].values\n\ndata = [go.Bar(\n    x = x,\n    y = y,\n    width = 0.5,\n    marker = dict(\n        color = feature_dataframe['mean'].values,\n        colorscale = 'Portland',\n        showscale = True,\n        reversescale = False\n    ),\n    opacity = 0.6\n)]\n\nlayout = go.Layout(\n    autosize = True,\n    title = 'Barplots of Mean Fearure Importance',\n    hovermode = 'closest',\n    yaxis = dict(\n        title = 'Feature Importance',\n        ticklen = 5,\n        gridwidth = 2\n    ),\n    showlegend = False\n)\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig, filename = 'bar-direct-labels')","01d89f1d":"base_predictions_train = pd.DataFrame({\n    'RandomForest': rf_oof_train.ravel(),\n    'ExtraTrees': ada_oof_train.ravel(),\n    'AdaBoost': ada_oof_train.ravel(),\n    'GradientBoost': gb_oof_train.ravel()\n})\n\nbase_predictions_train.head()","6cd776c8":"data = [\n    go.Heatmap(\n        z = base_predictions_train.astype(float).corr().values,\n        x = base_predictions_train.columns.values,\n        y = base_predictions_train.columns.values,\n        colorscale = 'Viridis',\n        showscale = True,\n        reversescale = True\n    )\n]\n\npy.iplot(data, filename = 'labelled-heatmap')","589c884d":"x_train = np.concatenate(\n    ( \n        et_oof_train,\n        rf_oof_train,\n        ada_oof_train,\n        gb_oof_train,\n        svc_oof_train\n    ),\n    axis = 1\n)\n\nx_test = np.concatenate(\n    (\n        et_oof_test,\n        rf_oof_test,\n        ada_oof_test,\n        gb_oof_test,\n        svc_oof_test\n    ),\n    axis = 1\n)","8a83d6f7":"gbm = xgb.XGBClassifier(\n    n_estimators = 2000,\n    max_depth = 4,\n    min_child_weight = 2,\n    gamma = 0.9,\n    subsample = 0.8,\n    colsample_bytree=0.8,\n    objective= 'binary:logistic',\n    nthread= -1,\n    scale_pos_weight = 1\n).fit(x_train, y_train)\n\npredictions = gbm.predict(x_test)","423bf661":"StackingSubmission = pd.DataFrame({\n    'PassengerID': PassengerId,\n    'Survived': predictions\n})\n\nStackingSubmission.to_csv('StackingSubmission.csv', index = False)","d117faea":"Sklearn\u306e\u6a5f\u80fd\u3067\u7279\u5fb4\u306e\u91cd\u8981\u5ea6\u3092\u7b97\u51fa\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002","a630d0d6":"\u7279\u5fb4\u306e\u91cd\u8981\u5ea6\u306e\u95a2\u308f\u308a\u3092\u6563\u5e03\u56f3\u306b\u3057\u3066\u8868\u793a","de00d56f":"\u7b2c\u4e00\u30ec\u30d9\u30eb\u306e\u4e88\u6e2c\u7d50\u679c\u3092\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u7d50\u5408\u3057\u3066\u3001\u7b2c\u4e8c\u30ec\u30d9\u30eb\u4e88\u6e2c\u306b\u4f7f\u7528\u3059\u308b\u3002","919e2e00":"**Reference Kernel**\n[Introduction to Ensembling\/Stacking in Python](https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python)\n\n\n","5eb8996a":"\u6b21\u306b\u3000\u7279\u5fb4\u306e\u91cd\u8981\u5ea6\u3092\u8a08\u7b97\u3057\u3001\u30c7\u30fc\u30bf\u30d5\u30ec\u30fc\u30e0\u306b\u4fdd\u5b58\u3057\u307e\u3059\u3002","63252112":"### \u7b2c\u4e8c\u30ec\u30d9\u30eb\u4e88\u6e2c\n\u7b2c\u4e00\u30ec\u30d9\u30eb\u306e\u4e88\u6e2c\u7d50\u679c\u3092\u65b0\u3057\u3044\u7279\u5fb4\u3068\u3057\u3066\u4f7f\u7528\u3059\u308b\u3002  \n\u5404\u30e2\u30c7\u30eb\u306e\u9805\u76ee\u3092\u4f5c\u6210\u3057\u3001\u7d50\u679c\u3092\u683c\u7d0d\u3059\u308b","4b4500aa":"\u304a\u4e92\u3044\u306b\u7121\u95a2\u4fc2\u3067\u3001\u3044\u3044\u30b9\u30b3\u30a2\u3092\u51fa\u3059\u30e2\u30c7\u30eb\u3092\u6301\u3064\u3053\u3068\u306b\u306f\u30e1\u30ea\u30c3\u30c8\u304c\u3042\u308b\u3068\u3001\u30b3\u30f3\u30c6\u30b9\u30c8\u306e\u52dd\u8005\u3084\u304b\u306a\u308a\u306e\u6570\u306e\u8a18\u4e8b\u3067\u8a00\u308f\u308c\u3066\u3044\u308b\u3089\u3057\u3044\u3002","fbcb67c4":"## Pearson Correlation Heatmap ( \u30d4\u30a2\u30bd\u30f3\u76f8\u95a2\u30d2\u30fc\u30c8\u30de\u30c3\u30d7 )\n\n### \u76f8\u95a2\u95a2\u4fc2\u306b\u3064\u3044\u3066\n\u76f8\u95a2\u95a2\u4fc2( correlation coefficient )\u3068\u306f\u3001\u4e8c\u3064\u306e\u5909\u6570\u304c\u9023\u52d5\u3057\u3066\u5909\u5316\u3059\u308b\u5ea6\u5408\u3044\u3092\u793a\u3059\u7d71\u8a08\u5b66\u7684\u6307\u6a19\u306e\u3053\u3068\u3002\n\u3053\u3053\u3067\u3044\u3046\u76f8\u95a2\u3068\u306f\u3001\u30d4\u30a2\u30bd\u30f3\u306e\u7a4d\u7387\u76f8\u95a2\u4fc2\u6570\u3092\u6307\u3057\u3066\u3044\u308b\u3002\n\n### \u30d2\u30fc\u30c8\u30de\u30c3\u30d7\n\u500b\u3005\u306e\u5024\u306e\u30c7\u30fc\u30bf\u884c\u5217\u3092\u8272\u3068\u3057\u3066\u8868\u73fe\u3057\u305f\u53ef\u8996\u5316\u30b0\u30e9\u30d5\u306e\u4e00\u7a2e\u3002\n\u8272\u306b\u3088\u3063\u3066\u30c7\u30fc\u30bf\u91cf\u3092\u8996\u899a\u5316\u3059\u308b\u3053\u3068\u3067\u76f4\u611f\u7684\u306b\u628a\u63e1\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b\u3002\n\n\n\n\u95a2\u9023\u3059\u308b\u4e00\u3064\u306e\u7279\u5fb4\u304c\u6b21\u306e\u7279\u5fb4\u3068\u3069\u306e\u3088\u3046\u306b\u95a2\u9023\u3057\u3066\u3044\u308b\u304b\u78ba\u8a8d\u3059\u308b\u305f\u3081\u306b\u76f8\u95a2\u30d7\u30ed\u30c3\u30c8\u3092\u4f5c\u6210\u3059\u308b\u3002\n\u305d\u306e\u305f\u3081\u306b\u3001Seaborn\u3068\u3044\u3046\u30b0\u30e9\u30d5\u63cf\u753b\u304c\u3067\u304d\u308b\u30d1\u30c3\u30b1\u30fc\u30b8\u3092\u4f7f\u7528\u3059\u308b\u3002\n\n\n\n\u53c2\u8003\u306b\u306a\u308a\u305d\u3046\u306a\u30ea\u30f3\u30af\n\nhttps:\/\/seaborn.pydata.org\/\n\nhttps:\/\/myenigma.hatenablog.com\/entry\/2015\/10\/09\/223629\n\n\n","aa1654f0":"### \u7b2c\u4e8c\u30ec\u30d9\u30eb\u4e88\u6e2c\u306b\u306fXGBoost\u30e2\u30c7\u30eb\u3092\u4f7f\u7528\u3059\u308b\nXGBoost\u306f\u5927\u898f\u6a21\u306a\u30d6\u30fc\u30b9\u30c8\u30c4\u30ea\u30fc\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u6700\u9069\u304b\u3059\u308b\u305f\u3081\u306b\u69cb\u7bc9\u3055\u308c\u305f\u3002\n    \nhttps:\/\/xgboost.readthedocs.io\/en\/latest\/\n  ","c4460e95":"\u5404\u30c7\u30fc\u30bf\u540c\u58eb\u306e\u76f8\u95a2\u306f\u305d\u3053\u307e\u3067\u5f37\u304f\u306a\u3044\u3053\u3068\u304c\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u304b\u3089\u308f\u304b\u308b\u3002\n\n\u3053\u308c\u306f\u5b66\u7fd2\u30e2\u30c7\u30eb\u306b\u3053\u308c\u3089\u306e\u7279\u5fb4\u3092\u8ffd\u52a0\u3059\u308b\u3068\u3044\u3046\u89b3\u70b9\u304b\u3089\u306f\u512a\u308c\u3066\u3044\u308b\u3089\u3057\u3044\u3002\n\u62e1\u5f35\u306b\u306f\u4f59\u5206\u306a\u60c5\u5831\u306f\u306a\u304f\u3001\u304b\u3064\u56fa\u6709\u306e\u60c5\u5831\u3092\u6301\u3063\u3066\u3044\u308b\u3053\u3068\u3092\u610f\u5473\u3057\u3066\u3044\u308b\u3089\u3057\u3044\u3002","0c1b1032":"# Visualisations\n\n\u3053\u3053\u304b\u3089\u8996\u899a\u7684\u306b\u308f\u304b\u308a\u3084\u3059\u3044\u3088\u3046\u306b\u8868\u3084\u30b0\u30e9\u30d5\u3092\u8868\u793a\u3059\u308b","85902100":"5\u3064\u306e\u30e2\u30c7\u30eb\u304b\u3089\u51fa\u305f\u7279\u5fb4\u306e\u91cd\u8981\u5ea6\u3092\u5e73\u5747\u3057\u3001\u68d2\u30b0\u30e9\u30d5\u306b\u3057\u3066\u8868\u793a\u3059\u308b","f9971b76":"### \u7b2c\u4e8c\u30ec\u30d9\u30eb\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30bb\u30c3\u30c8\u306e\u76f8\u95a2\u30d2\u30fc\u30c8\u30de\u30c3\u30d7","2946d74a":"## Out-of-Fold Predictinons\n\n\u30aa\u30fc\u30d0\u30fc\u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\uff08\u904e\u5b66\u7fd2\uff09\u3092\u9632\u3050\u305f\u3081\n\u30e2\u30c7\u30eb\u3068\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30bb\u30c3\u30c8\u3092\u5f15\u6570\u306b\u3001\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30bb\u30c3\u30c8\u3067\u30e2\u30c7\u30eb\u3092\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3057\u3001\u8ffd\u52a0\u3055\u308c\u305f\u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u3067\u65b0\u3057\u3044\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30bb\u30c3\u30c8\u3092\u8fd4\u3059\u30d8\u30eb\u30d1\u95a2\u6570\u3002","d8ef0332":"# Ensembling & Stacking models\n\nscikit-learn(Sklearn)\u3068\u3044\u3046\u6a5f\u68b0\u5b66\u7fd2\u306e\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u4f7f\u7528\u3057\u3066\u3044\u304f\u306e\u3067\u3001\u4f7f\u3044\u3084\u3059\u304f\u3059\u308b\u305f\u3081\u306b\u30d8\u30eb\u30d1\u30fc\u30af\u30e9\u30b9\u3092\u4f5c\u6210\u3059\u308b\u3002","edc19e17":"## Pairplots( \u30da\u30a2\u30d7\u30ed\u30c3\u30c8\u56f3\uff08\u6563\u5e03\u56f3\u884c\u5217\uff09)\n\n\u30da\u30a2\u30d7\u30ed\u30c3\u30c8\u3068\u691c\u7d22\u3057\u3066\u3082\u3001\u3069\u306e\u3088\u3046\u306a\u56f3\u3092\u793a\u3059\u306e\u304b\u306f\u4f55\u3082\u3067\u3066\u3053\u306a\u304b\u3063\u305f\u3002\n\n\u30d7\u30ed\u30c3\u30c8\u56f3\u3068\u6563\u5e03\u56f3\u306b\u3064\u3044\u3066\u306f\u3067\u3066\u304d\u305f\u306e\u3067\u3001\u30e1\u30e2\u3002\n\n\u30d7\u30ed\u30c3\u30c8\u56f3\u30fb\u30fb\u30fb\u30c7\u30fc\u30bf\u96c6\u5408\u306e\u63cf\u753b\u624b\u6cd5\u306e\u4e00\u7a2e\u3067\u3001\u4e8c\u7a2e\u985e\u4ee5\u4e0a\u306e\u5909\u6570\u306e\u95a2\u4fc2\u3092\u30b0\u30e9\u30d5\u3067\u8868\u3059\u76ee\u7684\u3067\u4f7f\u308f\u308c\u308b\u3002\n\u65b9\u6cd5\u306e\u3053\u3068\u3067\u56f3\u5f62\u81ea\u4f53\u306f\u3044\u308d\u3044\u308d\u306a\u7a2e\u985e\u304c\u3042\u3063\u305f\u3002\n\u7bb1\u3072\u3052\u56f3\u3001\u7b49\u5024\u7dda\u3001\u6563\u5e03\u56f3\u306a\u3069\u3092\u30d7\u30ed\u30c3\u30c8\u56f3\u3068\u3044\u3046\u3002\n\n\u6563\u5e03\u56f3\u30fb\u30fb\u30fb\u5206\u5e03\u56f3\u3068\u3082\u3044\u3046\u3002\u7e26\u8ef8\u3001\u6a2a\u5b9f\u306b\u4e8c\u9805\u76ee\u306e\u91cf\u3084\u5927\u304d\u3055\u3092\u5bfe\u5fdc\u3055\u305b\u3001\u30c7\u30fc\u30bf\u3092\u70b9\u3067\u63cf\u753b\u3057\u305f\u3082\u306e\u3002\u76f8\u95a2\u95a2\u4fc2\u3092\u628a\u63e1\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u3066\u3001\u30c7\u30fc\u30bf\u7fa4\u304c\u53f3\u4e0a\u308a\u306a\u3089\u6b63\u306e\u76f8\u95a2\u3001\u53f3\u4e0b\u304c\u308a\u306a\u3089\u8ca0\u306e\u76f8\u95a2\u3092\u8868\u3059\u3002","d326c3ac":"### \u7b2c\u4e00\u30ec\u30d9\u30eb\u4e88\u6e2c\n5\u3064\u306e\u5206\u985e\u5668\u3092\u5143\u306bOut-of-fold prediction\u95a2\u6570\u3092\u5b9f\u884c\u3057\u3001\u7b2c\u4e00\u30ec\u30d9\u30eb\u4e88\u6e2c\u3092\u51fa\u3059\u3002","41ba7304":" \u63d0\u51fa\u3059\u308b\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210\u3059\u308b\n","6accc0b2":"# Feature Exploration, Engineering and Cleaning\n\n\u5206\u6790\u3092\u884c\u3046\u305f\u3081\u306b\u30c7\u30fc\u30bf\u3092\u6574\u7406\u3057\u3001\u4f7f\u7528\u3067\u304d\u308b\u72b6\u614b\u306b\u3057\u3066\u3044\u304f\u3002\n\u7279\u5fb4\u30a8\u30f3\u30b8\u30cb\u30a2\u30ea\u30f3\u30b0\u3092\u5b9f\u884c\u3059\u308b\u306e\u3067\u3001\u65e2\u5b58\u306e\u7279\u5fb4\u304b\u3089\u95a2\u9023\u3059\u308b\u7279\u5fb4\u3092\u4f5c\u6210\u3059\u308b\u3002","3ed32b3d":"## \u7b2c\u4e00\u30ec\u30d9\u30eb\u30e2\u30c7\u30eb\u306e\u751f\u6210\n1. Random Forest classifier\n2.Extra Trees classifier\n3.AdaBoost classifier\n4.Gradinet Boosting classifier\n5.Support Vector Machine\n\n### \u30d1\u30e9\u30e1\u30fc\u30bf\nn_jobs\uff1a\u30c8\u30ec\u30fc\u30cb\u30b0\u30d7\u30ed\u30bb\u30b9\u306b\u4f7f\u7528\u3055\u308c\u308b\u30b3\u30a2\u306e\u6570\u3002-1\u306b\u8a2d\u5b9a\u3059\u308b\u3068\u3001\u3059\u3079\u3066\u306e\u30b3\u30a2\u304c\u4f7f\u7528\u3055\u308c\u308b\u3002\n\nn_estimators\uff1a\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30d7\u30ed\u30bb\u30b9\u3067\u4f7f\u7528\u3055\u308c\u308b\u5206\u985e\u6728\u306e\u6570\u300210\u304c\u30c7\u30d5\u30a9\u30eb\u30c8\n\nmax_depth\uff1a\u5206\u985e\u6728\u306e\u30ce\u30fc\u30c9\u6570\u3001\u3069\u306e\u304f\u3089\u3044\u62e1\u5f35\u3059\u308b\u304b\u3092\u6c7a\u5b9a\u3059\u308b\u3002\u6570\u304c\u5927\u304d\u3059\u304e\u308b\u3068\u3001\u904e\u5b66\u7fd2\u306b\u306a\u308b\u5371\u967a\u304c\u3042\u308b\u3002\n\nverbose\uff1a\u5b66\u7fd2\u30d7\u30ed\u30bb\u30a8\u30b9\u4e2d\u306b\u30c6\u30ad\u30b9\u30c8\u3092\u51fa\u529b\u3059\u308b\u304b\u3069\u3046\u304b\u3092\u5236\u5fa1\u3057\u307e\u3059\u30020\u306e\u5024\u306f\u3059\u3079\u3066\u306e\u30c6\u30ad\u30b9\u30c8\u3092\u6291\u5236\u3057\u30013\u306e\u5024\u306f\u5404\u7e70\u308a\u8fd4\u3057\u3067\u30c4\u30ea\u30fc\u5b66\u7fd2\u30d7\u30ed\u30bb\u30b9\u3092\u51fa\u529b\u3057\u307e\u3059\u3002\n\n","631bdf60":"max_depth:\n  \ngamma\uff1a\n  \neta\uff1a"}}