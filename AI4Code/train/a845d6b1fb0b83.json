{"cell_type":{"f0b0cd3d":"code","449454ee":"code","115d33e0":"code","79dd074e":"code","174d2db0":"code","d9a94429":"code","b3e371f2":"code","f21503cf":"code","697f6453":"code","c2ee940f":"code","6ea7b244":"code","46e1cf52":"code","f457f0d5":"code","373cce2e":"code","f65a2853":"code","cee8321c":"code","6cd62a90":"code","7295f203":"code","efc67875":"code","c3bc4d20":"code","b59df449":"code","aeeba516":"markdown","0351edb5":"markdown","6de78dc0":"markdown","49747d26":"markdown","de9da836":"markdown","4d6e6f6f":"markdown","fa696091":"markdown","f8391bc7":"markdown","a9057d6b":"markdown","67ede8ef":"markdown","2c26bcc1":"markdown","97911fb8":"markdown","aeaf31aa":"markdown","534ae6c6":"markdown","0310a761":"markdown","245b943f":"markdown","402ea4a1":"markdown","75aa6a26":"markdown","59f37554":"markdown","40b30962":"markdown","75a8a42a":"markdown","cfd56080":"markdown","55f84941":"markdown","4f708675":"markdown"},"source":{"f0b0cd3d":"!pip3 install tensorflow==2.4.0\n!pip install keras==2.4.3 numpy==1.19.3 pillow==7.0.0 scipy==1.4.1 h5py==2.10.0 matplotlib==3.3.2 opencv-python keras-resnet==0.2.0\n!pip install scikit-image\n!pip uninstall imageai\n!pip install imageai --upgrade","449454ee":"import cv2 # To deal with image manipulation.\nimport os # To deal with files and directories.\nimport numpy as np # To deal with numbers.\nfrom lxml import etree # To deal with annotation files.\nfrom matplotlib import pyplot as plt # To deal with plots (displaying images).\nfrom imageai.Detection.Custom import DetectionModelTrainer, CustomObjectDetection # For object detection.","115d33e0":"# Function to show image given an \"img\" that is already read.\ndef showImage(img, cmap=None):\n    plt.figure(figsize=(10, 10))\n    plt.imshow(img, cmap=cmap)\n    plt.axis('off')\n    plt.show()\n\n# Function to read and image and show it.\ndef readAndShowImage(path, cmap=None):\n    img = plt.imread(path)\n    showImage(img, cmap)","79dd074e":"os.listdir('.\/')","174d2db0":"os.listdir('..\/')","d9a94429":"os.listdir('..\/input')","b3e371f2":"# Before copying, make the folders first.\nos.mkdir('.\/dataset')\nos.mkdir('.\/dataset\/train')","f21503cf":"from distutils.dir_util import copy_tree\n\ncopy_tree('..\/input\/car-plate-detection\/', '.\/dataset\/train')\n# cp -r ..\/input\/car-plate-detection\/annotations .\/dataset\/train && cp -r ..\/input\/car-plate-detection\/images .\/dataset\/train","697f6453":"RAW_FILE_PATH = \"..\/input\/car-plate-detection\/\"\nFILE_PATH = \".\/dataset\/train\"\n\nRAW_IMAGES_DIR = RAW_FILE_PATH + '\/images\/'\nIMAGES_DIR = FILE_PATH + '\/images\/'\n\nRAW_ANNOTATIONS_DIR = RAW_FILE_PATH + '\/annotations\/'\nANNOTATIONS_DIR = FILE_PATH + '\/annotations\/'","c2ee940f":"NEW_WIDTH = 200\nNEW_HEIGHT = 200","6ea7b244":"# This function is simply to make the code more readable.\ndef convertToIntThenString(number):\n    return str(int(number))","46e1cf52":"# Loop through all images within the raw dataset.\nfor filename in os.listdir(RAW_IMAGES_DIR):\n\n    # Convert Images to Grayscale & Resize to NEW_WIDTH and NEW_HEIGHT.\n    img = cv2.imread(RAW_IMAGES_DIR + filename)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    img = cv2.resize(img, (NEW_WIDTH, NEW_HEIGHT))\n\n    # Get Annotation Files.\n    cv2.imwrite(IMAGES_DIR + filename, img)\n    raw_tree = etree.parse(RAW_ANNOTATIONS_DIR + filename.replace('png', 'xml'))\n    tree = etree.parse(ANNOTATIONS_DIR + filename.replace('png', 'xml'))\n    \n    # Get Image Size.\n    raw_size = raw_tree.xpath(\"size\")[0]\n    width = int(raw_size.xpath(\"width\")[0].text)\n    height = int(raw_size.xpath(\"height\")[0].text)\n\n    # Calculate the Scale\/Ratio between Old\/New Width and Heights.\n    width_scale = NEW_WIDTH\/width\n    height_scale = NEW_HEIGHT\/height\n    \n    # Update the Image's Sizes in the XML File.\n    size = tree.xpath(\"size\")[0]\n    size.xpath(\"width\")[0].text = str(NEW_WIDTH)\n    size.xpath(\"height\")[0].text = str(NEW_HEIGHT)\n\n    # Update the Object's Bounding Box to Scale with the New Image Size.\n    i = 0\n    while i < len(raw_tree.xpath(\"object\/bndbox\")):\n        raw_bb = raw_tree.xpath(\"object\/bndbox\")[i]\n        bb = tree.xpath(\"object\/bndbox\")[i]\n\n        bb.xpath(\"xmin\")[0].text = convertToIntThenString(int(raw_bb.xpath(\"xmin\")[0].text) * width_scale)\n        bb.xpath(\"ymin\")[0].text = convertToIntThenString(int(raw_bb.xpath(\"ymin\")[0].text) * height_scale)\n        bb.xpath(\"xmax\")[0].text = convertToIntThenString(int(raw_bb.xpath(\"xmax\")[0].text) * width_scale)\n        bb.xpath(\"ymax\")[0].text = convertToIntThenString(int(raw_bb.xpath(\"ymax\")[0].text) * height_scale)\n        i+=1\n  \n    # Write These Changes to the XML File\n    tree.write(ANNOTATIONS_DIR + filename.replace('png', 'xml'))","f457f0d5":"# The path where the dataset is located.\nDATA_DIR = '.\/dataset'\n\n# Batch size that is used for training.\nBATCH_SIZE = 16 \n\n# Number of times the network will train on all the data.\nEPOCHS = 50 \n\n# The objects that we want to detect.\nOBJECTS = [\"licence\"]\n\n# Path to the pretrained model.\nPRETRAINED_MODEL_PATH = '..\/input\/content\/models\/detection_model-ex-026--loss-0014.603.h5'","373cce2e":"trainer = DetectionModelTrainer()\ntrainer.setModelTypeAsYOLOv3()\ntrainer.setDataDirectory(data_directory=DATA_DIR)\n\n# Uncomment\/comment this line if we want to train a fresh model (not using a pretrained model).\ntrainer.setTrainConfig(object_names_array=OBJECTS, batch_size=BATCH_SIZE, num_experiments=EPOCHS)\n\n# Uncomment\/comment this line if we want to train a pretrained model.\n# trainer.setTrainConfig(object_names_array=OBJECTS, batch_size=BATCH_SIZE, num_experiments=EPOCHS, train_from_pretrained_model=PRETRAINED_MODEL_PATH)\n\ntrainer.trainModel()","f65a2853":"# Uncomment the next two lines to evalute ALL of the models yielded by the current training.\n# TRAINED_MODELS_PATH = \"..\/dataset\/models\"\n# JSON_PATH = \".\/dataset\/json\/detection_config.json\"\n\n# Uncomment the next two lines to evaluate the pre-trained model(s) in the content folder.\nTRAINED_MODELS_PATH = \"..\/input\/content\/models\"\nJSON_PATH = \"..\/input\/content\/json\/detection_config.json\"","cee8321c":"trainer = DetectionModelTrainer()\ntrainer.setModelTypeAsYOLOv3()\ntrainer.setDataDirectory(data_directory=DATA_DIR)\ntrainer.evaluateModel(model_path=TRAINED_MODELS_PATH, json_path=JSON_PATH, iou_threshold=0.6)","6cd62a90":"# Define the input images path.\nINPUT_IMAGE_DIR = '..\/input\/content\/input_images\/'\n\n# Define the output image paths (this will be the image that has its licence plate blurred).\nOUTPUT_IMAGE_DIR = '.\/result_images\/'\nos.mkdir(OUTPUT_IMAGE_DIR)","7295f203":"# Make an empty list to save colored and grayscaled images.\ninput_color = []\ninput_grayscale = []\n\n# Loop through all images within the input folder.\nfor filename in sorted(os.listdir(INPUT_IMAGE_DIR)):\n\n    # Read the image from the specified path previously.\n    image = cv2.imread(INPUT_IMAGE_DIR + filename)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    # Show the image.\n    showImage(image)\n    \n    # Append the image to input_color.\n    input_color.append(image)\n    \n    # This is to get a grayscale version of the image, while keeping the original image.\n    grayscale = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n\n    # Append the grayscaled image to input_grayscale.\n    input_grayscale.append(grayscale)\n    \n    # Save the image to the previously defined output path.\n    # This is the image that the detection model uses.\n    cv2.imwrite(OUTPUT_IMAGE_DIR + filename, grayscale)","efc67875":"MODEL_FOR_DETECTION = '..\/input\/content\/models\/detection_model-ex-051--loss-0025.242.h5'\nCONFIG_FOR_DETECTION = '..\/input\/content\/json\/detection_config.json'","c3bc4d20":"detect = CustomObjectDetection()\ndetect.setModelTypeAsYOLOv3()\ndetect.setModelPath(MODEL_FOR_DETECTION) \ndetect.setJsonPath(CONFIG_FOR_DETECTION)\ndetect.loadModel()\n\n# Loop through All Images\ni = 0\nfor filename in sorted(os.listdir(OUTPUT_IMAGE_DIR)):\n    \n    # Detect Car Plates within the Preprocessed Image\n    detections = detect.detectObjectsFromImage(input_image = OUTPUT_IMAGE_DIR + filename, output_image_path = OUTPUT_IMAGE_DIR + filename)\n\n    # Get the Current Colored Version of Image from the List from Before\n    current_color = input_color[i]\n    \n    # For each detected car plates,\n    for detection in detections:\n\n        #  Get the bounding box points of the detected image.\n        x1, y1, x2, y2 = detection[\"box_points\"]\n        \n        # print(x1, y1, x2, y2)\n        \n        # Crop this particular plate from a colored version of the image.\n        plate = current_color[y1:y2, x1:x2]\n\n        # Show the cropped plate.\n        showImage(plate)\n        \n        # Determine the size of the Gaussian Blur matrix based on the plate's size.\n        matrixSize = int(y2 - y1)*2 + 1\n\n        # Blur the plate.\n        plate = cv2.GaussianBlur(plate, (matrixSize, matrixSize), 0)\n\n        # Put back the blurred plate to the colored version of the image.\n        current_color[y1:y2, x1: x2] = plate\n\n        # Show the blurred plate.\n        # showImage(plate)\n\n    input_color[i] = current_color\n    i += 1","b59df449":"# Loop through all images within the raw dataset.\ni = 0\nwhile i < len(os.listdir(OUTPUT_IMAGE_DIR)):\n    showImage(input_color[i])\n    i += 1","aeeba516":"## 3.1. Evaluation Configurations\n\nJust like before in training, let us first define several variables that will be used in the evaluation to make the code more organized.","0351edb5":"## 2.1. Train Configurations\n\nLet us first define several variables to make the code more organized. Since we are going to use a pretrained model, let us define the name and path of the model.  \n\nSome sources mention that a batch size is usually a power of 2, and a good starting point is 32. Although, when we set the batch size to 32, the notebook crashed upon execution due to lack of resource. Therefore, we decided to just use 16 since it is a power of 2 and should require less computing power than 32.  \n\nNext, since we only want to detect car plates, `licence` is the only object within `OBJECTS`. This keyword is specifically mentioned in all of the annotation files, hence why we did not use `car_plates`, `plates`, etc.\n\nhttps:\/\/ai.stackexchange.com\/questions\/8560\/how-do-i-choose-the-optimal-batch-size  \nhttps:\/\/www.quora.com\/Is-there-a-good-standard-batch-size-that-people-use-when-training-on-the-MNIST-data-set  \nhttps:\/\/stats.stackexchange.com\/questions\/164876\/what-is-the-trade-off-between-batch-size-and-number-of-iterations-to-train-a-neu#:~:text=In%20general%2C%20batch%20size%20of,best%20to%20start%20experimenting%20with.","6de78dc0":"## 2.2. Training the Model\nThere are a few things that have to be done before we can train the model:\n\n*  First, we will create the object detection model trainer with `DetectionModelTrainer()`. Next, we will use YOLOv3 as the object detection algorithm. ImageAI supports these three algorithms: RetinaNet, YOLOv3 and TinyYOLOv3. However, since we want to detect custom objects (not the default ones provided by ImageAI), it appears that this library only allows us to use YOLOv3 algorithm for detecting custom objects, hence why we had to choose this specifically.\n\n* For the remaining parameters, we simply just use the variables we previously defined.\n","49747d26":"### 1.4.2. Declare Paths to Images\n\nWe will declare some variables containing path names of the images for simplicity. We will also define two types of file directory here:\n* The first type is the **raw** directory, which is where the original dataset is located.\n* The second type of directory is meant to save all preprocessed dataset.  \n\nAs previously mentioned, the input folder, which contains the original dataset, is **read only**. In order to make it possible for the ImageAI library to read and write the directory where the dataset is located, we have copied the original dataset to another place that allows file writes. The copied dataset will be used for Image Preprocessing, so these files will be modified later on.","de9da836":"### 4.1.2. Preprocessing the Input Image\n\nWe will first read the image using the path which is defined in the previous step. Then, we create a variable to contain the grayscale version of the image. This image is then saved within `OUTPUT_IMAGE_DIR`. ","4d6e6f6f":"## 4.1. Handling the Input Images\n\nThis step will handle the input images, that is, the images with a car plate that we want to blur.  \n\nBy default, the notebook will use the following 10 images obtained from:\n* https:\/\/imageai.readthedocs.io\/en\/latest\/\n* https:\/\/stackoverflow.com\/questions\/48037513\/implementing-a-license-plate-detection-algorithm\n* https:\/\/www.carscoops.com\/wp-content\/uploads\/2020\/06\/2006-Alfa-Romeo-8C-Competiz.jpg\n* https:\/\/ctbythenumbers.news\/ctnews\/seven-digit-license-plate-combos-beginning-with-a-are-overtaking-ct-roads\n* https:\/\/www.flickr.com\/photos\/sven_designs\/5073179812\n* https:\/\/www.google.com\/url?sa=i&url=https%3A%2F%2Fwww.forbes.com%2Fsites%2Fzakdoffman%2F2019%2F08%2F14%2Fhacker-gets-12000-in-parking-tickets-after-null-license-plate-trick-backfires%2F&psig=AOvVaw1_rtKHJpdHsbGVAmededu8&ust=1641311650990000&source=images&cd=vfe&ved=0CAwQjhxqFwoTCODqlcL4lfUCFQAAAAAdAAAAABAD\n* https:\/\/www.weathertech.com\/titanium-license-plate-frame\/\n* https:\/\/www.platepuller.com\/\n* https:\/\/www.ocregister.com\/wp-content\/uploads\/2019\/03\/fire-1.jpg\n* https:\/\/cdn.shopify.com\/s\/files\/1\/0014\/2527\/6991\/files\/IMG_0147_05fedcb8-992a-42e2-bf4b-880e15719f20_large.jpg?v=1555395492","fa696091":"## 4.3. Using the Model to Detect, then Blur the Plates\n\nUsing the constants that are previously defined, we can immediately use the functions to detect car plates within an image.","f8391bc7":"# 1. Preparation\n\nThis section will contain several steps before the next section (training) starts.","a9057d6b":"# 4. Testing a Model\n\nNow, with a model, we will detect car plate(s) in an image, then attempt to blur them, if any.\n","67ede8ef":"## 1.1. Install Necessary Libraries for ImageAI and ImageAI Itself\n\nThis step will install all libraries needed by the ImageAI library. Then, the ImageAI library itself will be installed.\n","2c26bcc1":"# 3. Evaluating the Model(s)\n\nThe first three line of code below are the same as when we trained our code. The last line is self-explanatory as well; we have to define the path where the models and the `detection_config.json` files are.  \n  \nAccording to the documentation, however, the `evaluateModel` function actually takes several optional parameters:\n*  `iou_threshold`, which is the minimum value for intersection over union. The default value is 0.5,  but we decided to use 0.6, which is slightly above the default value. If the iou_threshold is too low, this means that the bounding box for the license plate may be way bigger than the ground truth bounding box. In order to avoid this, we decided to slightly increase the iou_threshold.\n\n* `object_threshold`, which is the minimum value for confidence score. The default value is 0.2.\n\n* `nms_threshold`, which is a non maximum suppression\u2019s threshold. The default value is 0.45.\n\nFor all of the variables other than `iou_threshold`, we intentionally used the default value, which may seem low for some people. However, we used these default values because we believe that even though with low minimum values, which means that the model may detect some other objects as a licence plate when it is not actually a licence plate. We think that this would not be a problem. Instead, if we put all these values higher, the model may ignore some car plates and leave them unblurred.","97911fb8":"## 1.4. Preprocessing the Dataset","aeaf31aa":"# 2. Training the Model\n\n**NOTE:**  \nWe can either train an existing model or a new one. In this case, we performed extra two trainings on our first very first model, and this model exists within the `'..\/input\/content\/models\/detection_model-ex-026--loss-0014.603.h5'` path. During Section 2.2, the appropriate lines can be comment\/uncommented to train a new\/existing model.","534ae6c6":"## 4.4. View the Results\n\nLet us see the images with the blurred car plate.","0310a761":"# 0. Introduction\n\nThis notebook will use [the Car License Plate dataset](https:\/\/www.kaggle.com\/andrewmvd\/car-plate-detection). It is a dataset containing `433` pictures of car with a licence plate each. Each picture comes with one annotation file (`.xml`) that contains the bounding box location of where the licence plate is within an image. This annotation file will be used later on in the training process of the object detection model.\n  \nTo be brief, this notebook has two goals: to **detect** car plate in an image and to **blur** it. However, in order to achieve that, we will have to **create** and **train** an object detection model. We will use the ImageAI python library for this.\n\n## 0.1. [README!] Additional Information [README!]\n\n**If this notebook is to be executed outside Kaggle, make sure the directory of the notebook looks like the following:**\n```\ninput\n|-> car-plate-detection \n|  |-> annotations \n|  |-> images\n|-> content\n    |-> input_images (place input images here)\n    |-> json (place detection_config.json here, if any)\n    |-> models (place models to be used here, if any)\nworking\n|-> ipynb file (this is the notebook)\n```\n**Do not forget to download the initial dataset.**","245b943f":"Our current position (this notebook's position) within the directory is under the `working` folder. As we can see from the above commands, currently, the directory looks as follows:\n```\ninput\n-> car-plate-detection Folder\n-> content Folder\nlib\nworking\n-> ipynb File (we are here)\n```\n\nSince the input directory is read only, we will have to move all the dataset files to a place where **file writes are allowed** (i.e. the working directory). We have to do this step since ImageAI's training process will generate some files in the same directory as the dataset. We can use the `cp` command or `copy_tree` function (depends if you can use linux kernel,`cp` was initially used here since this notebook was made in Kaggle) to copy the entire dataset from `..\/input\/car-plate-detection` to `.\/dataset\/train`. In other words, the directory would look like the following:\n```\ninput\n-> car-plate-detection Folder\nlib\nworking\n-> dataset Folder\n |-> train Folder (under dataset)\n-> ipynb file (we are here)\n```\n","402ea4a1":"## 4.2. Detection Configuration\nIn order to use ImageAI's function for testing object detection, we have to define the paths to the model and the detection config file, and they are defined in the following variables.\n","75aa6a26":"## 1.3. Utility Functions\n\nSince we cannot use `cv2.imshow` here, we will create some reusable functions to display images easily.","59f37554":"# COMPUTER VISION PROJECT\n\nGiustino Redondo  \nJose Jovian  \nSena Kumara","40b30962":"### 4.1.1. Declare Image Paths (the Input and Output)","75a8a42a":"## 1.2. Import Libraries","cfd56080":"## 3.2. Actually Evaluating the Model\nUsing the configuration that we previously define, we will evaluate the model.\n","55f84941":"### 1.4.1. Copy Files from Kaggle Input Directory (Read Only) to Working Directory","4f708675":"### 1.4.3. Preprocessing the Image\n\nWe will perform some image preprocessing, such that the processing time required to process these images later is reduced:\n* All images will be resized to 200 x 200, though this can be modified by changing the values of `NEW_WIDTH` and `NEW_HEIGHT`.\n* All images are converted into grayscale with `cvtColor`."}}