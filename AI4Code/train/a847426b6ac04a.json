{"cell_type":{"0fddbc2c":"code","7431eb7f":"code","574481d4":"code","87e72991":"code","e66ad504":"code","0b2a0039":"code","0285337d":"code","26f67bd2":"code","a37dfc3c":"code","96e8e735":"code","48494ed4":"code","7b01102b":"code","6c9f50ba":"code","7cf2beb3":"code","8e800132":"code","d6739917":"code","c53913b9":"code","c9a9862d":"code","a950c604":"code","eb942ef8":"code","ffcfffcb":"code","fc9a5c5d":"code","701f2b54":"code","22562733":"code","02ba08df":"code","55fe6c06":"code","6f03ddcf":"code","5d20304e":"code","904c5be9":"code","2e6b2d8c":"markdown","de5306c2":"markdown","4904fc60":"markdown","e642d6fd":"markdown","518cbb5a":"markdown","9f8f93d9":"markdown","c3c40dc4":"markdown"},"source":{"0fddbc2c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom datetime import datetime\n        \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nfrom lightgbm import LGBMRegressor\nfrom sklearn import metrics,model_selection\nfrom optuna.samplers import TPESampler\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nfrom scipy.stats import skew\nimport optuna\nfrom xgboost import XGBRegressor\nfrom tqdm import tqdm\nsampler = TPESampler(seed=10)\nfrom sklearn.svm import SVR\n\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\nimport warnings\npd.options.display.max_columns = 250\npd.options.display.max_rows = 250\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')\nfrom scipy.special import boxcox1p\nfrom scipy.stats import skew, boxcox_normmax, norm\n\n\nfrom sklearn.model_selection import cross_val_score, KFold, cross_validate\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV, TweedieRegressor,LinearRegression\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom mlxtend.regressor import StackingCVRegressor ","7431eb7f":"BASE_PATH = '..\/input\/tabular-playground-series-jan-2021'","574481d4":"# Reading CSV\ntrain_df = pd.read_csv(os.path.join(BASE_PATH,'train.csv'))\ntest_df = pd.read_csv(os.path.join(BASE_PATH,'test.csv'))\nsample_df = pd.read_csv(os.path.join(BASE_PATH,'sample_submission.csv'))","87e72991":"# Checking Shapes\ntrain_df.shape,test_df.shape,sample_df.shape","e66ad504":"# Lets Check for Missing Values\ndef missing_percentage(df):\n    \n    \"\"\"A function for returning missing ratios.\"\"\"\n    \n    total = df.isnull().sum().sort_values(\n        ascending=False)[df.isnull().sum().sort_values(ascending=False) != 0]\n    percent = (df.isnull().sum().sort_values(ascending=False) \/ len(df) *\n               100)[(df.isnull().sum().sort_values(ascending=False) \/ len(df) *\n                     100) != 0]\n    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])","0b2a0039":"missing = missing_percentage(train_df)\nif (len(missing)>0):\n    fig, ax = plt.subplots(figsize=(20, 5))\n    sns.barplot(x=missing.index, y='Percent', data=missing, palette='Reds_r')\n    plt.xticks(rotation=90)\n    display(missing.T.style.background_gradient(cmap='Reds', axis=1))\nelse :\n    pass","0285337d":"# Checking Sample Data\ntrain_df.head()","26f67bd2":"# Lets describe the dataframe and check for different statistics\ntrain_df.describe().T","a37dfc3c":"# Lets Check for outliers and remove if any\ni=1\nplt.figure(figsize=(20,10))\nfor col in train_df.iloc[:,1:15]:\n    plt.subplot(4,4,i)\n    sns.boxplot(y = train_df[col],color='Blue')\n    i+=1\n\n    ","96e8e735":"train_df.shape","48494ed4":"# Removing Outliers\nQ1 = train_df.cont7.quantile(0.25)\nQ3 = train_df.cont7.quantile(0.75)\nIQR = Q3 - Q1\ntrain_df = train_df[(train_df.cont7 >= Q1 - 1.5*IQR) & (train_df.cont7 <= Q3 + 1.5*IQR)]\n\nQ1 = train_df.cont9.quantile(0.25)\nQ3 = train_df.cont9.quantile(0.75)\nIQR = Q3 - Q1\ntrain_df = train_df[(train_df.cont9 >= Q1 - 1.5*IQR) & (train_df.cont9 <= Q3 + 1.5*IQR)]","7b01102b":"train_df.shape","6c9f50ba":"# Lets check the multicollinearity\nsns.set(font_scale=1.1)\ncorrelation_train = train_df.iloc[:,1:15].corr()\nmask = np.triu(correlation_train.corr())\nplt.figure(figsize=(20, 20))\nsns.heatmap(correlation_train,\n            annot=True,\n            fmt='.1f',\n            cmap='coolwarm',\n            square=True,\n            mask=mask,\n            linewidths=1,\n            cbar=False)\n\nplt.show()","7cf2beb3":"# Lets create scatter plot to check corellation with target\n\ni=1\nplt.figure(figsize=(20,10))\nfor col in train_df.iloc[:,1:15]:\n    plt.subplot(4,4,i)\n    sns.scatterplot(train_df[col],train_df.target)\n    i+=1\n","8e800132":"def srt_reg(y, df):\n    fig, axes = plt.subplots(4, 4, figsize=(25, 80))\n    axes = axes.flatten()\n\n    for i, j in zip(df.select_dtypes(include=['number']).columns, axes):\n\n        sns.regplot(x=i,\n                    y=y,\n                    data=df,\n                    ax=j,\n                    order=3,\n                    ci=None,\n                    color='#e74c3c',\n                    line_kws={'color': 'black'},\n                    scatter_kws={'alpha':0.4})\n        j.tick_params(labelrotation=45)\n        j.yaxis.set_major_locator(MaxNLocator(nbins=10))\n\n        plt.tight_layout()","d6739917":"srt_reg('target',train_df.iloc[:,1:16])","c53913b9":"skewed = ['cont3', 'cont4', 'cont5', 'cont6',\n       'cont8', 'cont11', 'cont12', 'cont13', 'cont14',\n       'target']","c9a9862d":"# Finding skewness of the numerical features.\nskew_features = np.abs(train_df[skewed].apply(lambda x: skew(x)).sort_values(\n    ascending=False))\n\n# Filtering skewed features.\n\nhigh_skew = skew_features[skew_features > 0.3]\n\n# Taking indexes of high skew.\n\nskew_index = high_skew.index\n\n# Applying boxcox transformation to fix skewness.\n\nfor i in skew_index:\n    train_df[i] = boxcox1p(train_df[i], boxcox_normmax(train_df[i] + 1))\n    \nfor i in skew_index:\n    test_df[i] = boxcox1p(test_df[i], boxcox_normmax(test_df[i] + 1))","a950c604":"# Split the dataset\ny = train_df.target\nX = train_df.iloc[:,1:15]","eb942ef8":"kf = KFold(10, random_state=42)","ffcfffcb":"# Some parameters for ridge, lasso and elasticnet.\nalphas_alt = [15.5, 15.6, 15.7, 15.8, 15.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [    0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\n# ridge_cv\n\nridge = RidgeCV(alphas=alphas_alt,cv=kf,)\n\n# lasso_cv:\n\n# lasso = LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kf)\n\n# elasticnet_cv:\n\n# elasticnet = ElasticNetCV(max_iter=1e7,\n#                  alphas=e_alphas,\n#                  cv=kf,\n#                  random_state=42,\n#                  l1_ratio=e_l1ratio)\n\n# svr:\n\n# svr = SVR(C=21, epsilon=0.0099, gamma=0.00017, tol=0.000121)\n\n# gradientboosting:\n\ngbr = GradientBoostingRegressor(n_estimators=2900,\n                                learning_rate=0.0161,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=17,\n                                loss='huber',\n                                random_state=42)\n\n# lightgbm:\n\nlightgbm = LGBMRegressor(objective='regression',\n                         n_estimators=3500,\n                         num_leaves=5,\n                         learning_rate=0.00721,\n                         max_bin=163,\n                         bagging_fraction=0.35711,\n                         n_jobs=-1,\n                         bagging_seed=42,\n                         feature_fraction_seed=42,\n                         bagging_freq=7,\n                         feature_fraction=0.1294,\n                         min_data_in_leaf=8)\n\n# xgboost:\n\nxgboost = XGBRegressor(\n    learning_rate=0.0139,\n    n_estimators=4500,\n    max_depth=4,\n    min_child_weight=0,\n    subsample=0.7968,\n    colsample_bytree=0.4064,\n    nthread=-1,\n    scale_pos_weight=2,\n    seed=42,\n)\n\n\n# hist gradient boosting regressor:\n\n# hgrd= HistGradientBoostingRegressor( \n#     loss= 'least_squares',\n#     max_depth= 2,\n#     min_samples_leaf= 40,\n#     max_leaf_nodes= 29,\n#     learning_rate= 0.15,\n#     max_iter= 225,\n#     random_state=42\n# )\n\n# tweedie regressor:\n \n# tweed = TweedieRegressor(alpha=0.005)\n\n\n# stacking regressor:\n\n# stack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr,\n#                                             xgboost, lightgbm,hgrd, tweed),\n#                                 meta_regressor=xgboost,\n#                                 use_features_in_secondary=True)","fc9a5c5d":"def model_check(X, y, estimators, cv):    \n    model_table = pd.DataFrame()\n\n    row_index = 0\n    for est, label in zip(estimators, labels):\n\n        MLA_name = label\n        model_table.loc[row_index, 'Model Name'] = MLA_name\n\n        cv_results = cross_validate(est,\n                                    X,\n                                    y,\n                                    cv=cv,\n                                    scoring='neg_root_mean_squared_error',\n                                    return_train_score=True,\n                                    n_jobs=-1)\n\n        model_table.loc[row_index, 'Train RMSE'] = -cv_results[\n            'train_score'].mean()\n        model_table.loc[row_index, 'Test RMSE'] = -cv_results[\n            'test_score'].mean()\n        model_table.loc[row_index, 'Test Std'] = cv_results['test_score'].std()\n        model_table.loc[row_index, 'Time'] = cv_results['fit_time'].mean()\n\n        row_index += 1\n\n    model_table.sort_values(by=['Test RMSE'],\n                            ascending=True,\n                            inplace=True)\n\n    return model_table","701f2b54":"# Setting list of estimators and labels for them:\n\n# estimators = [ridge, lasso, elasticnet, gbr, xgboost, lightgbm, svr, hgrd, tweed]\n# labels = [\n#     'Ridge', 'Lasso', 'Elasticnet', 'GradientBoostingRegressor',\n#     'XGBRegressor', 'LGBMRegressor', 'SVR', 'HistGradientBoostingRegressor','TweedieRegressor'\n# ]\n\nestimators = [ridge, gbr, xgboost, lightgbm]\n\nlabels = ['Ridge',  'GradientBoostingRegressor','XGBRegressor', 'LGBMRegressor']\n\n# Executing cross validation.\n\nraw_models = model_check(X, y, estimators, kf)\ndisplay(raw_models.style.background_gradient(cmap='summer_r'))","22562733":"# Fitting the models on train data.\n\nprint('=' * 20, 'START Fitting', '=' * 20)\nprint('=' * 55)\n\n# print(datetime.now(), 'StackingCVRegressor')\n# stack_gen_model = stack_gen.fit(X.values, y.values)\n# print(datetime.now(), 'Elasticnet')\n# elastic_model_full_data = elasticnet.fit(X, y)\n# print(datetime.now(), 'Lasso')\n# lasso_model_full_data = lasso.fit(X, y)\n# print(datetime.now(), 'Ridge')\n# ridge_model_full_data = ridge.fit(X, y)\n# print(datetime.now(), 'SVR')\n# svr_model_full_data = svr.fit(X, y)\nprint(datetime.now(), 'GradientBoosting')\ngbr_model_full_data = gbr.fit(X, y)\n# print(datetime.now(), 'XGboost')\n# xgb_model_full_data = xgboost.fit(X, y)\n# print(datetime.now(), 'Lightgbm')\n# lgb_model_full_data = lightgbm.fit(X, y)\n# print(datetime.now(), 'Hist')\n# hist_full_data = hgrd.fit(X, y)\n# print(datetime.now(), 'Tweed')\n# tweed_full_data = tweed.fit(X, y)\nprint('=' * 20, 'FINISHED Fitting', '=' * 20)\nprint('=' * 58)","02ba08df":"# Blending models by assigning weights:\n'''\ndef blend_models_predict(X):\n    return (\n#             (0.1 * elastic_model_full_data.predict(X)) +\n#             (0.1 * lasso_model_full_data.predict(X)) +\n#             (0.1 * ridge_model_full_data.predict(X)) +\n#             (0.1 * svr_model_full_data.predict(X)) +\n            (0.05 * gbr_model_full_data.predict(X)) +\n            (0.1 * xgb_model_full_data.predict(X)) +\n            (0.05 * lgb_model_full_data.predict(X)) \n#             (0.05 * hist_full_data.predict(X)) +\n#             (0.1 * tweed_full_data.predict(X)) +\n#             (0.25 * stack_gen_model.predict(X.values))\n            )\n'''","55fe6c06":"submission = test_df\nsubmission = submission.drop('id',axis=1)","6f03ddcf":"# sample_df.iloc[:,1:] = np.floor(np.expm1(blend_models_predict(submission)))\nsample_df.iloc[:,1:] = (gbr_model_full_data.predict(submission)\n\n# Defining outlier quartile ranges\n# q1 = sample_df['target'].quantile(0.0050)\n# q2 = sample_df['target'].quantile(0.99)\n\n# Applying weights to outlier ranges to smooth them\n# sample_df['target'] = sample_df['target'].apply(lambda x: x if x > q1 else x * 0.77)\n# sample_df['target'] = sample_df['target'].apply(lambda x: x if x < q2 else x * 1.1)","5d20304e":"sample_df.head()","904c5be9":"sample_df.to_csv('submission.csv', index=False)\nprint(\n    'Save submission',\n    datetime.now(),\n)","2e6b2d8c":"### Import Required Libraries","de5306c2":"### Reading the required files","4904fc60":"### Performing EDA on the data","e642d6fd":"Cross Validation","518cbb5a":"Here we can note that cont7 and cont9 is having some amount of outliers","9f8f93d9":"As such there is no correlation present between independent variable and target","c3c40dc4":"Numerical Data plotting"}}