{"cell_type":{"5afaf879":"code","10b09021":"code","39ff9e48":"code","d034f482":"code","48c5a149":"code","832f0e86":"code","2ceeb85e":"code","638f6685":"code","4b52d1b8":"code","18eb9ba2":"code","45443530":"code","fa5ceffa":"code","4b7250f7":"code","5ebcdb10":"code","9f35c0a2":"code","22b7b247":"code","2073ab11":"code","b0201f46":"code","a066a663":"code","5b295e77":"code","488104ab":"markdown","8c19ece1":"markdown","12373e0f":"markdown","621dbc3f":"markdown","99721451":"markdown","769a3b5f":"markdown","1ec30da6":"markdown","79f7e3c0":"markdown","e362285a":"markdown","663c0908":"markdown","9336aa13":"markdown","3eddd29c":"markdown","acc2a3ed":"markdown","6ff2d904":"markdown","8b011445":"markdown","a89eabd5":"markdown","aa391a62":"markdown","a3b98a42":"markdown","e9706fbc":"markdown","3aea9707":"markdown","f37182c4":"markdown"},"source":{"5afaf879":"!pip install feyn","10b09021":"import feyn\nimport pandas as pd\nimport numpy as np","39ff9e48":"data = pd.read_csv(\"..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")\ndata.drop('id', axis = 1, inplace = True)\ndata","d034f482":"missing_col = ['bmi']\n\n#Technique 2: Using median to impute the missing values\nfor i in missing_col:\n data.loc[data.loc[:,i].isnull(),i]=data.loc[:,i].median()\n","48c5a149":"data.stroke.value_counts()","832f0e86":"from seaborn import pairplot\npairplot(data, hue = 'stroke')","2ceeb85e":"pd.crosstab(data['smoking_status'], data['stroke'], normalize = 'index')","638f6685":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(data, test_size = 0.3, stratify = data.stroke, random_state = 42)","4b52d1b8":"sample_weights = np.where(train.stroke == 1, data.stroke.value_counts()[0]\/data.stroke.value_counts()[1], 1)","18eb9ba2":"ql = feyn.connect_qlattice()\nql.reset(42)","45443530":"stypes = {}\nfor f in data.columns:\n    if data[f].dtype =='object':\n        stypes[f] = 'c'","fa5ceffa":"models = ql.auto_run(train,\n                     output_name = 'stroke',\n                     kind = 'classification',\n                     n_epochs = 10,\n                     threads = 6,\n                     criterion='bic',\n                     stypes = stypes,\n                     sample_weights=sample_weights\n                    )","4b7250f7":"my_model = models[0]","5ebcdb10":"my_model.plot(train, test)","9f35c0a2":"my_model.sympify(2, symbolic_lr=True)","22b7b247":"my_model.plot_confusion_matrix(test)","2073ab11":"my_model.plot_partial(train, by = 'age', fixed = {'avg_glucose_level': [50, 150, 250]})","b0201f46":"# Do one hot encoding for compatibility\ndata_ohe = pd.get_dummies(data)\n\n# Perform same train-test-split on prepared data\ntrain_ohe, test_ohe = train_test_split(data_ohe, test_size = 0.3, stratify = data.stroke, random_state=42)","a066a663":"rf = feyn.reference.RandomForestClassifier(train_ohe, output_name='stroke')\ngb = feyn.reference.GradientBoostingClassifier(train_ohe, output_name='stroke')\nlr = feyn.reference.LogisticRegressionClassifier(train_ohe, output_name='stroke', max_iter=10000)","5b295e77":"rf.plot_roc_curve(test_ohe, label=\"Random Forest\")\ngb.plot_roc_curve(test_ohe, label=\"Gradient Boosting\")\nlr.plot_roc_curve(test_ohe, label=\"Logistic Regression\")\nmy_model.plot_roc_curve(test, label = \"QLattice\")","488104ab":"## Evaluate model\nInspect the model best fitting the assigned bic-criterion. We paint the graph with pearson correlation in that way displaying the signal flow through the model. Also, we look at performance measures across train and test split. ","8c19ece1":"## Sample weights\nHow to deal with the imbalance? I choose to go with applying sample weighting to my algorithm. I assign a weight to each observation according to the prevalance of stroke. In practical terms, this means that stroke-individuals will weight about 20 times more non-stroke individuals since stroke-individuals constitue around 5% of the population. When we apply stochastic gradient descend to our models correctly specifying a stroke individual rewards the model 20 times as much as a non-stroke individual.","12373e0f":"# Risk of Stroke Explained by QLattice\nThe QLattice is a supervised machine learning tool for symbolic regression developed by Abzu . It is inspired by Richard Feynman's path integral formulation. That's why the python module to use it is called Feyn, and the Q in QLattice is for Quantum.\n\nAbzu provides free QLattices for non-commercial use to anyone. These free community QLattices gets allocated for us automatically if we use Feyn without an active subscription, as we will do in this notebook. Read more about how it works here: https:\/\/docs.abzu.ai\/docs\/guides\/getting_started\/community.html\n\nThe feyn Python module is not installed on Kaggle by default so we have to pip install it first.","621dbc3f":"Looking at categorical features you might look at crosstab of occurence. Here we look at the occurence of stroke for the given smoking statuses: Current and has been smokers are overrepresented among stroke events with occurences of 5.3% and 7.9%, respectively. While for never smokers and unknowns we see 4.7% and 3% occurence. Let's move from exploration mode into modelling mode.","99721451":"## Comparing predictive power to other go-to machine learning techniques\nWe ask: Can such a simple model really compete?","769a3b5f":"Thanks for reading and if you are interested please go and apply the QLattice to your problems and share your experience.","1ec30da6":"## Data Exploration\nConsider the balance in the dataset. We're dealing with a heavily skewed outcome variable. We need to deal with this.","79f7e3c0":"### Impute missing values\n.. in 'bmi' column","e362285a":"## Model Search\nStart the flow of models from the QLattice to your PC. Define the rules of the game: Which models you are interested in?","663c0908":"How would we predict risk of stroke out-of-sample individuals. Let's have a look at confusion matrix. With this model we correctly predict 60 out of 75 actual stroke-individuals, a recall of 80%. The precision of the model on the other hand is low. When we predict stroke we are only right 12% of the time, or 60 of 520 predicted. Depending on the application of the model you could weigh recall and precision differently by changing the threshold for when to predict stroke (the default here being 50%, heavily affected by our sample weights-setting).","9336aa13":"## Assign semantic types\nWe distinguish between categorical and numerical features and the QLattice needs to be informed of the semantics of your features. I split by dtypes of my training features, objects being categorical features.","3eddd29c":"## Interpretability\nOur models are inherently interpretable since we are dealinh with simple math. However to further simplifying the model functionality to the user a set of partial plots are available. Here we look at the effect of age on stroke risk with three different values of average glucose level. It becomes obvious that age is strongly linked to risk of stroke, increasing from the age of 20 to 30 (depending on glucose level) all the way up to the oldest individuals in the sample around 80. One stroke happening to a young child means that the algorithm has tried to grasp that, and that is why we see the small heightening of risk for young children.","acc2a3ed":"# QLattice application\nConnect to a Community QLattice available for non-commercial users. For more questions about this method visit the [docs page](https:\/\/docs.abzu.ai\/).","6ff2d904":"## Import packages","8b011445":"Let's have a look at the numerical variation in this dataset. We are seeing that stroke is closely linked to age; higher age higher risk of stroke. The imbalance makes it difficult to eyeball the differences in distributions across stroke or no stroke for most of the other features. However, it could look like both hypertension, heart disease, and average glucose level play a role. Contrary to my expectation bmi does not seem to play a major role here.","a89eabd5":"The resulting AUC's are all around the same level Logistic Regression being slightly higher than the QLattice (0.84 vs. 0.83). Now consider that the model provided by the QLattice is a two-feature model compared to the ten features otherwise used by the other models. This makes for a much easier grasp of the functionality of the model. We often see that you can boil things down to only a couple of features and still acheive competitive predictive power of your model.","aa391a62":"## Pick preferred model\n'models' is a list of differentiated models explaining your output. Go and inspect them.","a3b98a42":"## Import data\n.. and drop ID column","e9706fbc":"## Train-test-split\nFirst we split our data into train and test. To keep things simple I omit the otherwise important validation set for now. Due to the heavy imbalance in the output variable I stratify the split with 'stroke' to fix the proportion of stroke events.","3aea9707":"## These models are math really\nCall the matematical expression of your model with 'sympify'","f37182c4":"Apply Random Forest, Gradient Boosting and Logistic Regression readily available for benchmarking purposes in the feyn library."}}