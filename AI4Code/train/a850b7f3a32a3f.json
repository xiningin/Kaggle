{"cell_type":{"02ec954e":"code","4def4762":"code","762b40e6":"code","530d02d5":"code","2402781f":"code","03a183c4":"code","882121be":"code","d44ecf5d":"code","9aff4103":"code","8a502010":"code","dcd25ad5":"code","cdd23715":"code","45efb4ba":"code","900fbe1f":"code","8381e44e":"code","5fb5c855":"code","56c15e8d":"markdown","921d3368":"markdown"},"source":{"02ec954e":"import os\n\nimport pandas as pd\nimport numpy as np\n\nimport pyarrow.parquet as parquet\n# Used to train document embeddings\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\n# Used to train the baseline model\nfrom sklearn.linear_model import LogisticRegression\n\n\ninput_path = '..\/input'\nprint(os.listdir(input_path))","4def4762":"%%sh\nls -lhR ..\/input | awk '\n\/:$\/&&f{s=$0;f=0}\n\/:$\/&&!f{sub(\/:$\/,\"\");s=$0;f=1;next}\nNF&&f{ print s\"\/\"$0 }'","762b40e6":"test_texts = parquet.read_table(\n    input_path + '\/texts\/test\/', \n    columns = ['objectId','preprocessed']\n).to_pandas()\ntest_texts.head(10)","530d02d5":"%%time\n# Build document embeddings for text documents\ndoc2vec = Doc2Vec(\n    [TaggedDocument(lines,'tag') for lines in test_texts.preprocessed], \n    vector_size=5, \n    window=2, \n    min_count=1, \n    workers=4\n)","2402781f":"# Read a single day to train model on as Pandas dataframe\ndata = parquet.read_table(\n    input_path + '\/train\/date=2018-02-07',\n    columns = ['instanceId_objectId','feedback']\n).to_pandas()\ndata.rename(columns = {'instanceId_objectId':'objectId'}, inplace = True)\ndata['label'] = data['feedback'].apply(lambda x: 1.0 if(\"Liked\" in x) else 0.0).values\ndata = data[['objectId','label']]\ndata.head(10)","03a183c4":"%%time\n\nparts = []\n# Get unique object ids\nids = data.groupby('objectId').count()\n\n# In order to save memory iterate part by part\nfor (dirpath, dirnames, filenames) in os.walk(input_path + '\/texts\/train\/'):\n    for name in filenames:\n        if name.startswith('part'):\n            # Read single part\n            texts = parquet.read_table(\n                input_path + '\/texts\/train\/' + name,\n                columns = ['objectId','preprocessed']\n            ).to_pandas()            \n            # Filter documents we need\n            joined = ids.join(texts.set_index('objectId'), how='inner', on='objectId')\n            # Evaluate embeddings\n            joined['embedding'] = joined.preprocessed.apply(doc2vec.infer_vector)\n            # Memorize\n            parts.append(joined[['embedding']])\n            print('Done with ' + name)","882121be":"# Combine all the parts\ntrain = data.join(pd.concat(parts), on='objectId')\ntrain.head(10)","d44ecf5d":"# Construct the label (liked objects)\ny = train['label'].values","9aff4103":"# Extract the most interesting features\nX = np.stack(train['embedding'].values)","8a502010":"# Fit the model and check the weights\nmodel = LogisticRegression(C=0.01, random_state=23, solver='lbfgs').fit(X, y)\nmodel.coef_","dcd25ad5":"%%time\n# Weight the test documents\ntest_texts['weight'] = model.predict_proba(\n    np.stack(\n        test_texts.preprocessed.apply(lambda x : doc2vec.infer_vector(x))\n    )\n)[:, 1]\ntest_texts.head(10)","cdd23715":"# Read the test data\ntest = parquet.read_table(\n    input_path + '\/test',\n    columns = ['instanceId_userId','instanceId_objectId']\n).to_pandas()\ntest.rename(columns={'instanceId_objectId': 'objectId'}, inplace=True)\ntest.head(10)","45efb4ba":"# Join test documents and elliminate possible duplicates\nscores = test.join(\n    test_texts[['objectId','weight']].set_index('objectId'), \n    how='inner', \n    on='objectId'\n).groupby(['instanceId_userId','objectId']).max()\nscores.head(10)","900fbe1f":"#  Sort for each user\nresult = scores.sort_values(by=['instanceId_userId', 'weight'], ascending=False).reset_index()\nresult.head(10)","8381e44e":"# Collect predictions for each user\nsubmit = result.groupby('instanceId_userId')['objectId'].apply(list)\nsubmit.head(10)","5fb5c855":"# Persist the first submit\nsubmit.to_csv('text_submit.csv.gz', header=False, compression='gzip')","56c15e8d":"\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438 \u043d\u0430 \u0432\u0441\u0435 \u0434\u0430\u043d\u043d\u044b\u0435, \u0447\u0442\u043e \u0443 \u043d\u0430\u0441 \u0435\u0441\u0442\u044c (\u0430\u043a\u043a\u0443\u0440\u0430\u0442\u043d\u043e, \u043e\u0447\u0435\u043d\u044c \u0431\u043e\u043b\u044c\u0448\u043e\u0439 \u0441\u043f\u0438\u0441\u043e\u043a!)","921d3368":"# \u0411\u0430\u0437\u043e\u0432\u044b\u0439 \u043f\u0440\u0438\u043c\u0435\u0440 \u0441 \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0435\u0439"}}