{"cell_type":{"f5c86f40":"code","9950fbde":"code","005494f9":"code","bc350db9":"code","ac3b63e4":"code","8d7b2756":"code","1cedf883":"code","d196fe74":"code","fa18fe95":"code","03f6afaf":"code","915bf536":"code","1a97a59d":"code","dbe88988":"code","c88b07ac":"code","d571ed78":"code","507d283d":"code","2ad79532":"code","63e5bd19":"code","4803233e":"code","0a10819d":"code","7d26be9c":"code","74887fc0":"code","74037942":"code","d7eee7fe":"code","2cf47c12":"code","82c49a0c":"code","e54f64bd":"code","d8373e19":"code","22cfd3c6":"code","5731b5bc":"code","98b16171":"code","c89550f9":"code","57dcdfd6":"code","d958973a":"code","37538f9b":"code","a46ac0dc":"code","f699885b":"code","6693d5cc":"code","ab3e43ed":"code","235a7f7b":"code","061d0f33":"code","aa132fe9":"code","6eea94a2":"code","a64ab5ec":"code","ff12b21e":"code","082f8b18":"code","c78f8e54":"code","1131035c":"code","d3e14044":"code","a9123a4c":"code","0d059dcc":"code","6704061b":"code","b50badc0":"code","93e1f2a4":"code","c461c254":"code","ec9fd048":"code","37de54b7":"code","129c7493":"code","7b5a61e6":"code","663032e1":"code","af6294ee":"code","192caafc":"code","7ebd3da9":"code","269b9766":"code","e37fc05a":"code","243a7400":"code","84ff24a3":"code","ceddcfc6":"code","2cd96b86":"code","2096c25b":"code","98e299d9":"code","6f3ca9f5":"code","5dd46daf":"code","fab94d23":"code","b7df32a5":"code","58da3c36":"code","3582d1b9":"code","2617d0a0":"markdown","6887e138":"markdown","2742d24f":"markdown","fb2b0dac":"markdown","fcab391e":"markdown","25533316":"markdown","b10f1dd0":"markdown","9b580a65":"markdown","7e19ee0f":"markdown","acaa54fa":"markdown","48cc9a9c":"markdown","18517e5d":"markdown","68ade00d":"markdown","ac1111c9":"markdown","eee47981":"markdown","9dd08459":"markdown","cbbb82ec":"markdown","75f70a9c":"markdown","64d6ecaf":"markdown","5b42cd3f":"markdown","8f72bda9":"markdown","ea02a23a":"markdown","5a1679db":"markdown","fa356e85":"markdown","2aca8aa8":"markdown"},"source":{"f5c86f40":"import pandas as pd\nimport numpy as np\n\npd.set_option('display.max_rows', 300) # specifies number of rows to show\npd.options.display.float_format = '{:40,.2f}'.format # specifies default number format to 4 decimal places\npd.options.display.max_colwidth\npd.options.display.max_colwidth = 1000","9950fbde":"# This line tells the notebook to show plots inside of the notebook\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sb","005494f9":"import string\nimport nltk\n\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize","bc350db9":"import sklearn\nimport gensim","ac3b63e4":"# # Install Gensim Library pip package in the current Jupyter kernel\n# import sys\n# !{sys.executable} -m pip install --upgrade gensim","8d7b2756":"# # Install Plotly pip package in the current Jupyter kernel\n# import sys\n# !{sys.executable} -m pip install plotly","1cedf883":"# from plotly import __version__\n# from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\n# import plotly.graph_objs as go\n# from plotly.graph_objs import *\n\n# #You can also plot your graphs offline inside a Jupyter Notebook Environment. \n# #First you need to initiate the Plotly Notebook mode as below:\n# init_notebook_mode(connected=True)\n\n# #Run at the start of every ipython notebook to use plotly.offline. \n# #This injects the plotly.js source files into the notebook.\n\n# #https:\/\/plot.ly\/python\/offline\/#generating-offline-graphs-within-jupyter-notebook'","d196fe74":"#load in twitter data csv file\n\ntwitter_data = pd.read_csv('..\/input\/ldf18_twitter_clean.csv')\ntwitter_data.head(2)","fa18fe95":"#import nltk and download all stopwords and punctuations\nnltk.download('stopwords')\nnltk.download('punkt')\n\nimport re","03f6afaf":"twittertext = twitter_data","915bf536":"#remove 'http' links and remove all '\\n' line-brakes\n\ntwittertext.Text = twittertext.Text.str.replace(r'http\\S+','')\ntwittertext.Text = twittertext.Text.str.replace(r'\\n','')","1a97a59d":"#remove 'non-ascii' characters\n\ntwittertext.Text = twittertext.Text.str.replace(r'[^\\x00-\\x7F]','')\n# twittertext.head(2)","dbe88988":"#make all the text lowercase\n\ntwittertext.Text = twittertext.Text.str.lower()","c88b07ac":"#create a column for all #hasthtags that appears in the text\ntwittertext['hashtags'] = twittertext.Text.str.findall(r'#\\S+')","d571ed78":"#clean the text by removing all punctuation\n\n# https:\/\/stackoverflow.com\/questions\/23175809\/str-translate-gives-typeerror-translate-takes-one-argument-2-given-worked-i\n\ntwittertext['cleanText'] = twittertext.Text.apply(lambda x: x.translate(\n                                                str.maketrans('','', string.punctuation)))\n\n# twittertext['cleanText'] = twittertext.Text.str.replace(r'[\/.!$%^&*():@]','')\n\ntwittertext.head(2)","507d283d":"#tokenise all of the clean text\n\ntwittertext['tokenText'] = twittertext.cleanText.apply(word_tokenize)\ntwittertext.head(2)","2ad79532":"#count the number of words in the post\n\ntwittertext['tokenCount'] = twittertext.tokenText.apply(len)\n# twittertext.head(2)","63e5bd19":"# Import stopwords with nltk.\nstop = stopwords.words('english')\n\n# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\ntwittertext['noStopWords'] = twittertext['cleanText'].apply(\n    lambda x: ' '.join([word for word in x.split() if word not in (stop)]))","4803233e":"#tokenise all of the text with Stop words\n\ntwittertext['tokenTextnoStop'] = twittertext.noStopWords.apply(word_tokenize)\n# twittertext.head(2)","0a10819d":"twittertext.head(2)","7d26be9c":"twittertext.shape","74887fc0":"#google's langdetect: https:\/\/stackoverflow.com\/questions\/43377265\/determine-if-text-is-in-english\n\n#import Google's langdetect library to check where tweets are english or not\nfrom langdetect import detect","74037942":"lang = []\n\ntry:\n    for index, row in twittertext['cleanText'].iteritems():\n        lang = detect(row)\n        twittertext.loc[index, 'Languagereveiw'] = lang\n\nexcept Exception:\n    pass","d7eee7fe":"twittertext.Languagereveiw.value_counts()\n\n#9449 tweets are English","2cf47c12":"twittertext = twittertext.loc[twittertext['Languagereveiw'] == 'en']\ntwittertext.shape","82c49a0c":"twittertext_nodup = twittertext.drop_duplicates(subset='cleanText')\ntwittertext_nodup.shape","e54f64bd":"twittertext_nodup.head(2)","d8373e19":"# Load in the sentiment analysis csv file\n\nsentiment = pd.read_csv('..\/input\/ldf-sentiment-nodup.csv')\nsentiment.head(1)","22cfd3c6":"sentiment.DateTime.dtypes","5731b5bc":"sentiment.DateTime = pd.to_datetime(sentiment.DateTime)\nsentiment.DateTime.dtypes","98b16171":"sentiment.shape","c89550f9":"score = sentiment.score\ndate_time = sentiment.DateTime\n\nDF = pd.DataFrame()\nDF['score'] = score\nDF = DF.set_index(date_time)\n\nfig, ax = plt.subplots()\nplt.plot(DF)","57dcdfd6":"x = pd.DataFrame(sentiment.groupby('Day')['score'].mean())\nx.columns = ['mean']","d958973a":"x['median'] = sentiment.groupby('Day')['score'].median()","37538f9b":"x = x.reset_index()\nx.head()","a46ac0dc":"people = pd.DataFrame(sentiment.groupby('UserHandle')['score'].mean())\npeople.columns = ['mean']\npeople.head()","f699885b":"people['median'] = sentiment.groupby('UserHandle')['score'].median()\npeople.head()","6693d5cc":"people['count'] = sentiment.groupby('UserHandle')['score'].count()\npeople.head()","ab3e43ed":"people = people.reset_index()","235a7f7b":"people.dtypes","061d0f33":"people = people[people['count'] > 4]","aa132fe9":"people.sort_values(by = ['mean'],\n                   ascending = False).head(10)","6eea94a2":"twittertext_nodup.head(2)","a64ab5ec":"#flatten list of lists of no stop words (TextnoStop) into one large list\n\ntokens = []\n\nfor sublist in twittertext_nodup.tokenTextnoStop:\n    for word in sublist:\n        tokens.append(word)","ff12b21e":"tokens_df = pd.DataFrame(tokens)\ntokens_df.columns = ['words']\ntokens_df['freq'] = tokens_df.groupby('words')['words'].transform('count')\n\ntokens_df.shape","082f8b18":"tokens_df.head()","c78f8e54":"tokens_df.words.value_counts()[:20]","1131035c":"word_count = pd.DataFrame(tokens_df.words.value_counts()[:20])\nword_count.reset_index(inplace=True)\nword_count.columns = ['word', 'count_words']","d3e14044":"twittertext_nodup.head(1)","a9123a4c":"hashtags = []\n\nfor sublist in twittertext_nodup.hashtags:\n    for word in sublist:\n        hashtags.append(word)","0d059dcc":"hashtags_df = pd.DataFrame(hashtags)\nhashtags_df.columns = ['words']\nhashtags_df['freq'] = hashtags_df.groupby('words')['words'].transform('count')\n\nhashtags_df.shape","6704061b":"hashtags_df.words.value_counts()[:20]","b50badc0":"hash_count = pd.DataFrame(hashtags_df.words.value_counts()[:20])\nhash_count.reset_index(inplace=True)\nhash_count.columns = ['hashtag', 'count_hashtag']","93e1f2a4":"count = pd.concat([word_count, hash_count], axis=1)\ncount","c461c254":"from nltk.stem import WordNetLemmatizer\nfrom collections import Counter","ec9fd048":"#import word lemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()","37de54b7":"#group the text by day\nday_corpus = twittertext_nodup.groupby('Day').apply(lambda x: x['noStopWords'].str.cat())\n\n#tokenize the text for that day using word lemmatizer and then count\nday_corpus = day_corpus.apply(lambda x: Counter([\n    wordnet_lemmatizer.lemmatize(w) \n    for w in word_tokenize(x) \n    if w.lower() not in stop and not w.isdigit()]))","129c7493":"#count the five most frequent words per day\nday_freq = day_corpus.apply(lambda x: x.most_common(5))\n\n# create a dataframe of the five most frequent words per day\nday_freq = pd.DataFrame.from_records(\n    day_freq.values.tolist()).set_index(day_freq.index)","7b5a61e6":"day_freq","663032e1":"def normalize_row(x):\n    label, repetition = zip(*x)\n    t = sum(repetition)\n    r = [n\/t for n in repetition]\n    return list(zip(label,r))\n\nday_freq = day_freq.apply(lambda x: normalize_row(x), axis=1)","af6294ee":"twittertext_nodup.head(1)","192caafc":"# create a corpus of all the text which does not have stop words in it\n# this corupus will be a list of lists\n\ncorpus = list(twittertext_nodup['tokenTextnoStop'])","7ebd3da9":"len(corpus)","269b9766":"from gensim.models import word2vec\n\nmodel = word2vec.Word2Vec(corpus, \n                          size=20, \n                          window=3, \n                          min_count=40, \n                          workers=10)\nmodel.wv['ldf18']\n\n# model.train(corpus, total_examples=len(corpus), epochs=10)","e37fc05a":"from sklearn.decomposition import PCA\n\nvocab = list(model.wv.vocab)\nX = model[model.wv.vocab]\n\npca = PCA(n_components = 2)\nresult = pca.fit_transform(X)","243a7400":"import matplotlib.pyplot as pyplot\n\npyplot.scatter(result[:, 0], result[:, 1])","84ff24a3":"pyplot.scatter(result[:, 0], result[:, 1])\nwords = list(model.wv.vocab)\nfor i, word in enumerate(words):\n    pyplot.annotate(word, xy=(result[i,0], result[i, 1]))\npyplot.show()","ceddcfc6":"wrds = list(model.wv.vocab)\nlen(wrds)","2cd96b86":"#zip the two lists containing vectors and words\nzipped = zip(model.wv.index2word, model.wv.vectors)\n\n#the resulting list contains `(word, wordvector)` tuples. We can extract the entry for any `word` or `vector` (replace with the word\/vector you're looking for) using a list comprehension:\nwordresult = [i for i in zipped if i[0] == word]\nvecresult = [i for i in zipped if i[1] == vector]","2096c25b":"model.wv.most_similar('ldf18')","98e299d9":"model.wv.most_similar('ldf18')","6f3ca9f5":"#dataframe of similar words\n# https:\/\/stackoverflow.com\/questions\/43776572\/visualise-word2vec-generated-from-gensim\n\nword_frame = pd.DataFrame(result, index=vocab, columns=['x', 'y'])\nword_frame.reset_index(inplace=True)\nword_frame.columns = ['words', 'x', 'y']\n# word_frame.sort_values(by=['x','y'])","5dd46daf":"nearest = word_frame[word_frame.words.isin(['ldf18','textiles', 'designs', 'work', \n                                         'innovative', 'craft', 'products', 'beautiful',\n                                         'designthinking', 'showroom', \n                                         'architect'])]","fab94d23":"from sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef tsne_plot(model):\n    \"Creates and TSNE model and plots it\"\n    labels = []\n    tokens = []\n\n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=40, \n                      n_components=2, \n                      init='pca', \n                      n_iter=1000, \n                      random_state=23, \n                      learning_rate=200)\n    \n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(50, 50)) \n    \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()","b7df32a5":"tsne_plot(model)\n\n#model","58da3c36":"vocab = list(model.wv.vocab)\nX = model[vocab]\n\ntsne = TSNE(n_components=2)\nX_tsne = tsne.fit_transform(X)\n\ndf = pd.DataFrame(X_tsne, index=vocab, columns=['x', 'y'])","3582d1b9":"fig = plt.figure()\nax = fig.add_subplot(1,1,1)\n\nax.scatter(df['x'], df['y'])\n\nfor word, pos in df.iterrows():\n    ax.annotate(word, pos)\n    \nplt.show()","2617d0a0":"###\u00a0TSNE","6887e138":"### Principal Component Analysis","2742d24f":"### t-SNE visualisation\n\nI now have a vectorised representation of `corpusS` in relation to the word \"LDF18\" having used `word2vec` and the parameters above. I now want to visualise these findings to see which words in the corpus are closely linked to the word \"LDF18\".\n\nJeff (and also the Tensorflow documentations) explains that a t-SNE is great way to visualise these vectorised reprentations. From the `scikit-learn` library, I have imported TSNE. Here is a definition from the <a href=\"http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.manifold.TSNE.html\"> official documentation<\/a>: \"t-SNE converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data.\"\n\nBelow, I use Jeff's function to create a t-SNE plot using the word vectors in `model.wv`. I will use the same parameters as Jeff for the t-SNE : `perplexity`, `n_components`, `n_iter`, `random_state` and `learning_rate`.\n\n`perplexity` - The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms.\n\n`n_components` - Dimension of the embedded space.\n\n`n_iter`- Maximum number of iterations for the optimization. Should be at least 250.\n\n`random_state` - is the seed used by the random number generator\n\n`learning_rate` - the learning rate for t-SNE is usually in the range [10.0, 1000.0], default 200.\n\n<a href=\"https:\/\/distill.pub\/2016\/misread-tsne\/\"> This article<\/a>, *\"How to use t-SNE effectively\"*, is helpful to show how different parameters for the same dataset can have astonishingly different results.","fb2b0dac":"### Twitter Data","fcab391e":"#### Stop words: remove stop words and then create tokens","25533316":"### Clean text data and toeknize","b10f1dd0":"### ! plot this better !","9b580a65":"## Genism Word2Vec Model\n\nRun Word2Vec to get word embedding for the entire corpus of text from the tweets.\n\nGood tutorial: https:\/\/rare-technologies.com\/word2vec-tutorial\/","7e19ee0f":"### Words per day\n\nhttps:\/\/www.kaggle.com\/umutto\/preliminary-data-analysis-using-word2vec","acaa54fa":"https:\/\/www.kaggle.com\/umutto\/preliminary-data-analysis-using-word2vec","48cc9a9c":"However, other specilaised libraries will be installed throughout this notebook in order to undertake deeper and more advanced analysis. These will be installed and then imported within the notebook only when neccessary.\n\n1. Pip: a package management system used to install and manage software packages written in Python.\n2. Plotly: a graphing library to make interactive, publication-quality graphs.\n7. Google Cloud Natural Langauage API\n8. langdect\n8. gensim","18517e5d":"#### Hashtags: create column for hashtags","68ade00d":"## Sentiment analysis\n\nThe sentiment analysis for the tweets was performed using the Google Cloud NLP API in a seperate jupyter notebook. I have added the path to the csv file for this notebook","ac1111c9":"### Hashtags","eee47981":"Many of the tweets are retweets, therefore, they are duplicates. When performing text analysis, it is important to qualify that these have been removed.","9dd08459":"# London Design Festival 2018 - Natural Language Processing\n\n#### by Vishal Kumar\n","cbbb82ec":"# <a id='5'>Required Libraries <\/a>\n\nThe primary libraries that will be used to undertake the analysis are:\n\n1. NumPy: Provides a fast numerical array structure and helper functions.\n2. pandas: Provides a DataFrame structure to store data in memory and work with it easily and efficiently.\n3. matplotlib: Basic plotting library in Python; most other Python plotting libraries are built on top of it.\n5. Natural Language Toolkit: a library used to work with human language data and to conduct text processing.\n6. scikit-learn: The essential Machine Learning package in Python.\n7. String - useful for text manipulations\n8. nltk - natural language tool kit for text manipulations\n","75f70a9c":"## Count words\n\nCount and the plot the frequency of words for LDF18. What was the top 10 most popular words?","64d6ecaf":"#### Clean text: remove punctuation and then create tokens","5b42cd3f":"# <a id='6'>Checking and cleaning the Data <\/a>\n\nNow that the relevant packages are installed, the next step is to take a look at the data we are working with. It's important to do this so that we can spot errors and clean the data to make subsequent analysis is easier and smoother.\n\nWe will start by reading all the data as Pandas DataFrames.","8f72bda9":"To install these more advanced libraries, we can use the `sys` library and the `executable` function in order to install libraries using a `pip install` directly into this Jupyter kernal.","ea02a23a":"#  <a id='1'>Introduction<\/a>\n\nThis notebook provides a natural language processing analysis of 11,000 tweets about the London Design Festival 2018 light festival which took place in January 2018.\n","5a1679db":" We are fist going to import Plotly, a data visualisation tool to help us interactively visualise the data.","fa356e85":"## Installing advanced libraries using `pip`","2aca8aa8":"#### Remove all Non-English Tweets"}}