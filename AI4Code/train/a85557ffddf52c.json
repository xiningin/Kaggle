{"cell_type":{"870a05a9":"code","31acdfc2":"code","75e9a78e":"code","b9afe9b8":"code","d0f16395":"code","9f6d377a":"code","418f9720":"code","37f98994":"code","13994c60":"code","46fedf88":"code","e91a7052":"code","9f29a226":"code","33939a55":"code","42ea5920":"code","65a98a12":"code","e79c7d58":"code","706b18a8":"code","2426c4ff":"code","7e27cdfc":"code","2706ea7c":"code","a1239a07":"code","3d62a5a5":"code","bec6be85":"code","a19b64a6":"code","48c85224":"code","348df732":"code","f591fcda":"code","3d536b7f":"code","90331308":"code","23b3cb8d":"code","d28f06e9":"code","48517120":"code","22431524":"code","488cee24":"code","b455ca25":"code","6ea2460d":"code","cc3aef49":"code","c2774995":"code","e5dab368":"code","402d9033":"code","cef36cb0":"code","2e5328e0":"code","6c57a3fe":"code","b9fa263c":"code","ddcc9228":"code","20341c2b":"code","2e893dfb":"code","6256ecf6":"code","992737f4":"code","b90bf73d":"code","4ebb02d8":"code","1d0ecd72":"code","7457d6fb":"code","af50cf55":"code","fb6eb7bc":"code","833ec044":"code","eeb04f98":"code","7d9e46df":"code","71c498b8":"code","cbfc466f":"markdown","735c9b14":"markdown","9a07544a":"markdown","35e84f95":"markdown","95f89b44":"markdown","d34e2174":"markdown","9e933b5d":"markdown","598993b2":"markdown","85df34a0":"markdown","599a59a0":"markdown","0e0a19ff":"markdown","83e4680d":"markdown","e0d9cb82":"markdown","0b5fd8fd":"markdown","6f2029f2":"markdown","2d859592":"markdown","ba4a2df6":"markdown","aca87cad":"markdown","1376b9b6":"markdown","6c3b9f66":"markdown","7d1dae07":"markdown","c977249a":"markdown","958be3bd":"markdown","f67ea816":"markdown","56a08734":"markdown","e6ccdae4":"markdown","cd70863b":"markdown","23b232f8":"markdown","e8b0d9b7":"markdown","8eaba7ad":"markdown","b3043ef9":"markdown","6986bc01":"markdown","df700ad1":"markdown","fc7468a3":"markdown","4dbb7ea9":"markdown","08b29232":"markdown","954008e1":"markdown"},"source":{"870a05a9":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\n\n","31acdfc2":"import os\n#os.listdir('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt')","75e9a78e":"tweet= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntweet.head(3)","b9afe9b8":"print('There are {} rows and {} columns in train'.format(tweet.shape[0],tweet.shape[1]))\nprint('There are {} rows and {} columns in test'.format(test.shape[0],test.shape[1]))","d0f16395":"x=tweet.target.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('samples')","9f6d377a":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=tweet[tweet['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=tweet[tweet['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()\n","418f9720":"tweet_len=tweet[tweet['target']==1]['text']\ntype(tweet_len)","37f98994":"len(tweet.iloc[0]['text'])","13994c60":"pd.Series(tweet.iloc[0]['text']).str.len()","46fedf88":"tweet.dtypes","e91a7052":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=tweet[tweet['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=tweet[tweet['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Words in a tweet')\nplt.show()\n","9f29a226":"tweet_len=tweet[tweet['target']==1]['text'].str.split().map(lambda x: len(x))\ntweet_len","33939a55":"tweet.iloc[0]['text'].split()","42ea5920":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword1=tweet[tweet['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word1.map(lambda x: np.mean(x)),ax=ax1,color='red')\nax1.set_title('disaster')\nword2=tweet[tweet['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word2.map(lambda x: np.mean(x)),ax=ax2,color='green')\nax2.set_title('Not disaster')\nfig.suptitle('Average word length in each tweet')","65a98a12":"word1","e79c7d58":"word2","706b18a8":"def create_corpus(target):\n    corpus=[]\n    \n    for x in tweet[tweet['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","2426c4ff":"corpus=create_corpus(0)\nstop=set(stopwords.words('english'))\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]","7e27cdfc":"top=sorted(dic.items(), key=lambda x:x[1],reverse=False)[:10]","2706ea7c":"top","a1239a07":"dic.items()","3d62a5a5":"top","bec6be85":"dic","a19b64a6":"x,y=zip(*top)\nplt.bar(x,y)","48c85224":"corpus=create_corpus(1)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n    \n\n\nx,y=zip(*top)\nplt.bar(x,y)","348df732":"plt.figure(figsize=(10,5))\ncorpus=create_corpus(1)\n\ndic=defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y)","f591fcda":"plt.figure(figsize=(10,5))\ncorpus=create_corpus(0)\n\ndic=defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y,color='green')","3d536b7f":"\ncounter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)","90331308":"sns.barplot(x=y,y=x)","23b3cb8d":"def get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","d28f06e9":"plt.figure(figsize=(10,5))\ntop_tweet_bigrams=get_top_tweet_bigrams(tweet['text'])[:10]\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x)","48517120":"df=pd.concat([tweet,test])\ndf.shape","22431524":"example=\"New competition launched :https:\/\/www.kaggle.com\/c\/nlp-getting-started\"","488cee24":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\nremove_URL(example)","b455ca25":"df['text']=df['text'].apply(lambda x : remove_URL(x))","6ea2460d":"example = \"\"\"<div>\n<h1>Real or Fake<\/h1>\n<p>Kaggle <\/p>\n<a href=\"https:\/\/www.kaggle.com\/c\/nlp-getting-started\">getting started<\/a>\n<\/div>\"\"\"","cc3aef49":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\nprint(remove_html(example))","c2774995":"df['text']=df['text'].apply(lambda x : remove_html(x))","e5dab368":"# Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nremove_emoji(\"Omg another Earthquake \ud83d\ude14\ud83d\ude14\")","402d9033":"df['text']=df['text'].apply(lambda x: remove_emoji(x))\n","cef36cb0":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\nexample=\"I am a #king\"\nprint(remove_punct(example))","2e5328e0":"df['text']=df['text'].apply(lambda x : remove_punct(x))","6c57a3fe":"!pip install pyspellchecker","b9fa263c":"from spellchecker import SpellChecker\n\nspell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n        \ntext = \"corect me plese\"\ncorrect_spellings(text)","ddcc9228":"#df['text']=df['text'].apply(lambda x : correct_spellings(x)#)","20341c2b":"\ndef create_corpus(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus\n        \n        ","2e893dfb":"corpus=create_corpus(df)","6256ecf6":"embedding_dict={}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","992737f4":"MAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","b90bf73d":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","4ebb02d8":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec\n            ","1d0ecd72":"model=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=1e-5)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])\n\n","7457d6fb":"model.summary()","af50cf55":"train=tweet_pad[:tweet.shape[0]]\ntest=tweet_pad[tweet.shape[0]:]","fb6eb7bc":"X_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.15)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","833ec044":"history=model.fit(X_train,y_train,batch_size=4,epochs=15,validation_data=(X_test,y_test),verbose=2)","eeb04f98":"sample_sub=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","7d9e46df":"y_pre=model.predict(test)\ny_pre=np.round(y_pre).astype(int).reshape(3263)\nsub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})\nsub.to_csv('submission.csv',index=False)\n","71c498b8":"sub.head()","cbfc466f":"### Removing punctuations","735c9b14":"## Class distribution","9a07544a":"### Spelling Correction\n","35e84f95":"<font size='5' color='red'>  if you like this kernel,please do an upvote.<\/font>","95f89b44":"### Basic Intro \n\nIn this competition, you\u2019re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one\u2019s aren\u2019t.\n\n![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcTigQWzoYCNiDyrz1BN4WTf2X2k9OZ_yvW-FsmcIMsdS9fppNmh)","d34e2174":"### Number of words in a tweet","9e933b5d":"### Common stopwords in tweets","598993b2":"### Number of characters in tweets","85df34a0":"### Importing required Libraries.","599a59a0":"Lot of cleaning needed !","0e0a19ff":"Now,we will analyze tweets with class 1.","83e4680d":"### Romoving Emojis","e0d9cb82":"We will need lot of cleaning here..","0b5fd8fd":"First let's check tweets indicating real disaster.","6f2029f2":"### Ngram analysis","2d859592":"### Removing urls","ba4a2df6":"In both of them,\"the\" dominates which is followed by \"a\" in class 0 and \"in\" in class 1.","aca87cad":"## Data Cleaning\nAs we know,twitter tweets always have to be cleaned before we go onto modelling.So we will do some basic cleaning such as spelling correction,removing punctuations,removing html tags and emojis etc.So let's start.","1376b9b6":"### Removing HTML tags","6c3b9f66":"Before we begin with anything else,let's check the class distribution.There are only two classes 0 and 1.","7d1dae07":"## GloVe for Vectorization","c977249a":"First we  will analyze tweets with class 0.","958be3bd":"The distribution of both seems to be almost same.120 t0 140 characters in a tweet are the most common among both.","f67ea816":"## Loading the data and getting basic idea ","56a08734":"Now,we will move on to class 0.","e6ccdae4":"###  Average word length in a tweet","cd70863b":"## Baseline Model","23b232f8":"Even if I'm not good at spelling I can correct it with python :) I will use `pyspellcheker` to do that.","e8b0d9b7":"## What's in this kernel?\n- Basic EDA\n- Data Cleaning\n- Baseline Model","8eaba7ad":"we will do a bigram (n=2) analysis over the tweets.Let's check the most common bigrams in tweets.","b3043ef9":"### Analyzing punctuations.","6986bc01":"## Making our submission","df700ad1":"## Exploratory Data Analysis of tweets","fc7468a3":"Here we will use GloVe pretrained corpus model to represent our words.It is available in 3 varieties :50D ,100D and 200 Dimentional.We will try 100 D here.","4dbb7ea9":"### Common words ?","08b29232":"ohh,as expected ! There is a class distribution.There are more tweets with class 0 ( No disaster) than class 1 ( disaster tweets)","954008e1":"First,we will do very basic analysis,that is character level,word level and sentence level analysis."}}