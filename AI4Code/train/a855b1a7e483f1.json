{"cell_type":{"f5c4efa7":"code","ddee2e71":"code","ba7d27c5":"code","4bb74d50":"code","3e4b0a17":"code","6c880018":"code","0d56fdfb":"code","849a3e63":"code","7295026a":"code","7fb6edca":"code","7715c304":"code","d8d846a2":"code","5a68e5b7":"code","8626e8da":"code","1e774cc6":"code","aebca3c3":"code","17a8acb6":"code","dd36d457":"code","42850306":"code","77751d14":"code","27f7fca9":"code","5c3f6372":"code","bfffa664":"code","ba16f2ce":"code","eb13aabf":"code","dbd8a0dd":"code","508b9c1e":"code","6ab3a515":"code","bd7de3f4":"code","55d3fcc4":"code","52aa986c":"code","7c008955":"code","7f321c11":"code","02418351":"code","79e34f83":"code","1e10f93f":"code","9117aef1":"code","33505120":"code","9274fd58":"code","797d2941":"code","61320ed7":"code","90ec42a6":"code","e3935451":"code","268bb424":"code","b5bf02f4":"code","a1a6c292":"code","b7315e49":"code","89ee1e15":"code","966b05a0":"code","80e673e0":"code","500ebf49":"code","81c665ca":"code","adc7d91a":"code","af9a0b5f":"code","4c274682":"code","de27dc5a":"code","7bba7129":"code","6584fc44":"code","e32d6904":"code","a830b0eb":"code","dd71bfa0":"markdown","6ecd717d":"markdown","eeef71c1":"markdown","6aafb5a2":"markdown","0abbeb63":"markdown","0a63ba3b":"markdown","bd18b35a":"markdown","f776150f":"markdown","9e6056e0":"markdown","53525680":"markdown","0940c39b":"markdown","d68fb53d":"markdown","3a95fde5":"markdown","85bdc4e3":"markdown","8c9475ca":"markdown","d8356d2a":"markdown","2ca768e3":"markdown"},"source":{"f5c4efa7":"import numpy as np\nimport pandas as pd","ddee2e71":"dataset = pd.read_csv('..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')","ba7d27c5":"dataset.shape\n# shape of the dataset","4bb74d50":"dataset.columns\n# columns in the dataset","3e4b0a17":"dataset.head(n=10)\n# first 10 rows of the dataset","6c880018":"import seaborn as sns","0d56fdfb":"sns.countplot(x=dataset['sentiment'])","849a3e63":"dataset['sentiment'].value_counts()\n# count of each sentiment","7295026a":"from nltk.corpus import stopwords\n# to dealing with stopwords\n\nfrom nltk.stem import PorterStemmer\n# PorterStemmer --> for stemming the text\n\nimport re\n# re --> regular expression","7fb6edca":"sw = stopwords.words('english') # as we needs to remove english stopwords\nps = PorterStemmer() # creating an object of PorterStemmer","7715c304":"def clean_text(sample):\n    sample = sample.lower()\n    # lowering the entire text\n    \n    sample = sample.replace(\"<br \/><br \/>\",\"\")\n    # as this is an html text. hence it is containing '<br>'\n    # So, we are replacing the <br> with \"\", ie, remove this <br> tags\n    \n    sample = re.sub(\"[^a-zA-Z]+\",\" \",sample)\n    # to remove those characters which are not the alphabets and replacing them with \" \".\n    \n    sample = sample.split()\n    # to apply stopword removal and stemming, we needs to iterate over the text, which is only possible \n    # if we convert this text into a list. Hence, converting this into a list.\n    \n    sample = [ps.stem(s) for s in sample if s not in sw]\n    # iterating over the list to perform stemming\n    \n    sample = \" \".join(sample)\n    # after stemming, re join the list back into a text.\n    \n    return sample","d8d846a2":"dataset['review'][0]\n# first review with stopwords and without stemming","5a68e5b7":"clean_text(dataset['review'][0])\n# first review without stopwords and after stemming","8626e8da":"dataset['review'] = dataset['review'].apply(clean_text)","1e774cc6":"dataset.head(n=10)\n# dataset after the removal of stopwords","aebca3c3":"max_features = 10000\n# the number of words in the vocab = 10000\n# 10000 is basically the number of unique words, ie. vocabulary size is 10000\n# so the first 10000 relevant words will be used.","17a8acb6":"from keras.preprocessing.text import Tokenizer","dd36d457":"tokenizer = Tokenizer(num_words=max_features, split=' ')\n# tokenizer --> a vocab of 10000 words\n\ntokenizer.fit_on_texts(dataset['review'].values)\n# applying tokenization on the dataset, it will take first 10000 words","42850306":"tokenizer.index_word\n# it is saying that the first word in vocab is 'movi'\n# second word is 'film'","77751d14":"tokenizer.word_counts\n# it means 'one' has occured 55435 times in entire dataset","27f7fca9":"import pickle","5c3f6372":"# now saving this tokenizer\nwith open('tokenizer.pickle', 'wb') as handle:\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)","bfffa664":"X = tokenizer.texts_to_sequences(dataset['review'].values)\n# X --> in each row it basically contains index of those words which are there in the review","ba16f2ce":"len(X)\n# ie, it has 50000 reviews","eb13aabf":"len(X[0])\n# ie, first review has 153 words","dbd8a0dd":"len(X[1])\n# ie, second review has 79 words","508b9c1e":"print(X[0])\n# they are the indexes of 153 words of first review","6ab3a515":"XT = X[:25000] # XT --> taking first 25000 reviews into the training set\nXt = X[25000:] # Xt --> taking last 25000 reviews into the testing set\n# NOTE --> ie, we have divided X into two parts --> i) XT and ii) Xt","bd7de3f4":"print(len(XT))\nprint(len(Xt))","55d3fcc4":"def vectorize_sentences(sentences,dim=10000):\n    outputs=np.zeros((len(sentences),dim)) \n    # outputs --> (25000 x 10000) matrix\n    \n    for i,idx in enumerate (sentences):\n        outputs[i,idx]=1\n    return outputs","52aa986c":"X_train = vectorize_sentences(XT)\nX_test = vectorize_sentences(Xt)","7c008955":"print(X_train.shape)\nprint(X_test.shape)\n# basically in each row we have one review and for that review we have 10000 columns\n# and if that index word is present in review then it will be 1 otherwise it will be 0","7f321c11":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n# we have two types of sentiment, so to encode them with numbers","02418351":"y = dataset['sentiment'].values","79e34f83":"y = le.fit_transform(y)","1e10f93f":"y[:100]\n# 1--> positive\n# 0--> negative\n# value of first 100 reviews","9117aef1":"Y_train = y[:25000] # YT --> training set of y\nY_test = y[25000:] # Yt --> test set of y","33505120":"print(len(Y_train))\nprint(len(Y_test))","9274fd58":"x_train_new = X_train[:20000]\nx_val = X_train[20000:]\n\ny_train_new = Y_train[:20000]\ny_val = Y_train[20000:]","797d2941":"print(x_val.shape)\nprint(x_train_new.shape)\nprint(y_val.shape)\nprint(y_train_new.shape)","61320ed7":"from keras import models\nfrom keras.layers import Dense","90ec42a6":"model=models.Sequential()\nmodel.add(Dense(16,activation='relu',input_shape=(10000,))) # first hidden layer having 16 neurons.\nmodel.add(Dense(16,activation='relu')) # second hidden layer having 16 neurons.\nmodel.add(Dense(1,activation='sigmoid')) # output layer having only 1 neuron which can be used for binary classification.","e3935451":"model.summary()\n# parameters are nothing but weights and biases\n# 160016 = (10000 * 16) + 16, 10000-->features || 16-->neurons in H1 layer || 16-->bias term\n# 272 = (16 * 16) + 16, 16-->neurons in H1 layer || 16-->neurons in H2 layer || 16-->bias term\n# 17 = (16 * 1) + 1, 16-->neurons in H2 layer || 1-->neurons in O\/P layer || 1-->bias term","268bb424":"# Compile the model\nmodel.compile(\n    optimizer='rmsprop',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n# loss function --> binary cross entropy\n# optimizer --> rmsprop (it basically helps to reduce the loss function)\n# metrics --> after each epoch, we can judge our training procedure, for that judgement we are using\n#             'accuracy' parameter here.","b5bf02f4":"# Executing the model\nhist = model.fit(\n    x_train_new, \n    y_train_new,\n    epochs=20,\n    batch_size=512,\n    validation_data=(x_val,y_val)\n)","a1a6c292":"import matplotlib.pyplot as plt","b7315e49":"plt.style.use('seaborn')","89ee1e15":"h = hist.history\n# it is a dictionary having keys as 'accuracy' , 'loss', 'val_accuracy' and their values","966b05a0":"plt.plot(h['val_loss'],label=\"Validation Loss\")\nplt.plot(h['loss'],label=\"Training Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n# ie, after some epochs our validation loss decreases but after that it starts to increase \n# which basically means overfitting","80e673e0":"plt.plot(h['val_accuracy'],label=\"Validation Acc\")\nplt.plot(h['accuracy'],label=\"Training Acc\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n# after some epochs, validation accuracy starts to decrease---> overfitting\n# so now we will stop after some 3-4 epochs called as stop early.","500ebf49":"# when epochs = 4\nhist = model.fit(\n    x_train_new,\n    y_train_new,epochs=4,\n    batch_size=512,\n    validation_data=(x_val,y_val)\n)","81c665ca":"h = hist.history","adc7d91a":"plt.plot(h['val_loss'],label=\"Validation Loss\")\nplt.plot(h['loss'],label=\"Training Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","af9a0b5f":"plt.plot(h['val_accuracy'],label=\"Validation Acc\")\nplt.plot(h['accuracy'],label=\"Training Acc\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","4c274682":"x = model.evaluate(X_test,Y_test)[1]*100\nx = round(x,2)\nprint(\"Accuracy on test set = \",x,\"%\")","de27dc5a":"x = model.evaluate(X_train,Y_train)[1]*100\nx = round(x,2)\nprint(\"Accuracy on training set = \",x,\"%\")","7bba7129":"result = model.predict(X_test)\n# applying on test dataset\n# result basically has the probability of each reviews that by what probability it can be positive(ie,1)","6584fc44":"print(result)\n# first review has 0.04 probability that it is positive and so on...","e32d6904":"result_train = model.predict(X_train)\n# applying model on training dataset","a830b0eb":"print(result_train)\n# first review has 0.97 probability that it is positive and so on...","dd71bfa0":"#### Loss vs Epoch","6ecd717d":"#### (9.2) Accuracy vs Epoch","eeef71c1":"### (3) Creating the Vocab","6aafb5a2":"### (4) Creating the X, \n### X --> rows = 50000\n### X --> columns = not specific, it contains different columns for each row, where column number is equals to number of words in that review (or row)","0abbeb63":"### (2) Stop word Removal and stemming","0a63ba3b":"#### Applying the stopword removal and stemming over entire dataset","bd18b35a":"### (9) Visualizing the validation accuracy and loss","f776150f":"### (5) XT --> taking first 25000 reviews into the training set\n###       Xt --> taking last 25000 reviews into the testing set","9e6056e0":"### (8) Defining the model architecture\n#### Using Fully Connected\/ Dense Layers with ReLu activation\n#### 2 Hidden layers with 16 units each\n#### 1 output layer with 1 unit (Sigmoid activation)","53525680":"#### Accuracy vs Epoch","0940c39b":"### (6) Vectorization of reviews, ie, making each review of size 10000\n#### (6.1) X_train and X_test","d68fb53d":"#### (9.1) Loss vs Epoch","3a95fde5":"### (11) Analyzing the developed model","85bdc4e3":"# ------------------------------END----------------------------------","8c9475ca":"#### (6.2) Y_train and Y_test\n#### Encoding positive as 1 and negative as 0","d8356d2a":"### (10) So now we will run our model for 4 epochs","2ca768e3":"### (7) Spliting the training set (25000) into \n### i) train_new (20000)\n### ii) validation (5000)"}}