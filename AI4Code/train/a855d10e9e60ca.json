{"cell_type":{"857c5dd3":"code","5c0e6287":"code","d069fb46":"code","1128c6da":"code","c876e604":"code","bd649df7":"code","4188479c":"code","b4076ead":"code","3e142395":"code","0df112ea":"code","9414e7c4":"code","5f8f0ef4":"code","646fca08":"code","f00a4c23":"markdown","93aed4ed":"markdown","9408c890":"markdown","c41ac14b":"markdown","36fcbbce":"markdown","3e012a73":"markdown","25b27d8a":"markdown","a9f95e8b":"markdown","a0c162e8":"markdown","747e149a":"markdown","276d0f6b":"markdown","41f0202d":"markdown","c5f40769":"markdown","08d020d9":"markdown","a9eb9819":"markdown","27c6453a":"markdown","0641eb3d":"markdown","5befcc3c":"markdown","81835c1f":"markdown"},"source":{"857c5dd3":"import numpy as np\nimport pandas as pd","5c0e6287":"class Layer:\n    def __init__(self):\n        self.input = None\n        self.output = None\n\n    # computes the output Y of a layer for a given input X\n    def forward_propagation(self, input):\n        raise NotImplementedError\n\n    # computes dE\/dX for a given dE\/dY (and update parameters if any)\n    def backward_propagation(self, output_error, learning_rate):\n        raise NotImplementedError","d069fb46":"# inherit from base class Layer\nclass FCLayer(Layer):\n    # input_size = number of input neurons\n    # output_size = number of output neurons\n    def __init__(self, input_size, output_size):\n        self.weights = np.random.rand(input_size, output_size) - 0.5\n        self.bias = np.random.rand(1, output_size) - 0.5\n\n    # returns output for a given input\n    def forward_propagation(self, input_data):\n        self.input = input_data\n        self.output = np.dot(self.input, self.weights) + self.bias\n        return self.output\n\n    # computes dE\/dW, dE\/dB for a given output_error=dE\/dY. Returns input_error=dE\/dX.\n    def backward_propagation(self, output_error, learning_rate):\n        input_error = np.dot(output_error, self.weights.T)\n        weights_error = np.dot(self.input.T, output_error)\n        # dBias = output_error\n\n        # update parameters\n        self.weights -= learning_rate * weights_error\n        self.bias -= learning_rate * output_error\n        return input_error","1128c6da":"#loss function and its derivative\ndef mse(y_true, y_pred):\n    return(np.mean(np.power(y_true-y_pred, 2)));\n\ndef mse_prime(y_true, y_pred):\n    return(2*(y_pred-y_true)\/y_true.size);","c876e604":"#activation function and derivative\n\ndef tanh(x):\n    return(np.tanh(x));\n\ndef tanh_prime(x):\n    return(1-np.tanh(x)**2);","bd649df7":"#inherit from base class\nclass ActivationLayer(Layer):\n    def __init__(self, activation, activation_prime):\n        self.activation = activation\n        self.activation_prime = activation_prime\n        \n    #return the activation input\n    def forward_propagation(self, input_data):\n        self.input = input_data\n        self.output = self.activation(self.input)\n        return(self.output)\n    \n    #return input_error = dE\/dX for a given output_error=dE\/dY\n    def backward_propagation(self, output_error, learning_rate):\n        return(self.activation_prime(self.input) * output_error)","4188479c":"class Network:\n    def __init__(self):\n        self.layers = []\n        self.loss = None\n        self.loss_prime = None\n\n    # add layer to network\n    def add(self, layer):\n        self.layers.append(layer)\n\n    # set loss to use\n    def use(self, loss, loss_prime):\n        self.loss = loss\n        self.loss_prime = loss_prime\n\n    # predict output for given input\n    def predict(self, input_data):\n        # sample dimension first\n        samples = len(input_data)\n        result = []\n\n        # run network over all samples\n        for i in range(samples):\n            # forward propagation\n            output = input_data[i]\n            for layer in self.layers:\n                output = layer.forward_propagation(output)\n            result.append(output)\n\n        return result\n\n    # train the network\n    def fit(self, x_train, y_train, epochs, learning_rate):\n        # sample dimension first\n        samples = len(x_train)\n        \n        #saving epoch and error in list\n        epoch_list = []\n        error_list = []\n\n        # training loop\n        for i in range(epochs):\n            err = 0\n            for j in range(samples):\n                # forward propagation\n                output = x_train[j]\n                for layer in self.layers:\n                    output = layer.forward_propagation(output)\n\n                # compute loss (for display purpose only)\n                err += self.loss(y_train[j], output)\n\n                # backward propagation\n                error = self.loss_prime(y_train[j], output)\n                for layer in reversed(self.layers):\n                    error = layer.backward_propagation(error, learning_rate)\n\n            # calculate average error on all samples\n            err \/= samples\n            print('epoch %d\/%d   error=%f' % (i+1, epochs, err))\n            \n            epoch_list.append(i+1)\n            error_list.append(err)\n        \n            #creating dataframe of epoch and error\n            df = pd.DataFrame()\n            df['epoch'] = epoch_list\n            df['loss'] = error_list\n        return df","b4076ead":"from keras.datasets import mnist\nfrom keras.utils import np_utils","3e142395":"#load MNIST from server\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\n\n#training data\n#reshape and normalize input data\nx_train = x_train.reshape(x_train.shape[0],1,28*28)\nx_train = x_train.astype('float32')\nx_train \/= 255\n\n#ecnoding output\ny_train = np_utils.to_categorical(y_train)\n\n#same for test data\nx_test = x_test.reshape(x_test.shape[0],1,28*28)\nx_test = x_test.astype('float32')\nx_test \/= 255\n\n#ecnoding output\ny_test = np_utils.to_categorical(y_test)","0df112ea":"#Network\nnet = Network()\nnet.add(FCLayer(28*28, 50))\nnet.add(ActivationLayer(tanh, tanh_prime))\nnet.add(FCLayer(50, 10))\nnet.add(ActivationLayer(tanh, tanh_prime))\n\nnet.use(mse, mse_prime)","9414e7c4":"df = net.fit(x_train[:1000], y_train[:1000],epochs=50,learning_rate=0.1)","5f8f0ef4":"import plotly.express as px\n\nfig = px.line(df, x='epoch', y='loss',title='Change in loss with respect to Epochs')\nfig.show()","646fca08":"#test on 3 samples\nout = net.predict(x_test[:1])\n\nprint('true values: ')\nprint(y_test[0:1])\n\nprint('\\n')\nprint('predicted values: ')\nout_int = [abs(np.round(x)) for x in out]\nprint(out_int)","f00a4c23":"# Loss and derivative","93aed4ed":"# Activation function and derivative","9408c890":"# Visualizing Epoch vs Loss","c41ac14b":"# Fully Connected Layer","36fcbbce":"<img src=\"http:\/\/ronny.rest\/media\/blog\/2017\/2017_08_16_tanh\/tanh_and_derivative_formulas.jpg\" width=\"500\" height=\"500\">","3e012a73":"# Creating dataset for training and preprocessing","25b27d8a":"# Defining complete Network","a9f95e8b":"# Training Network","a0c162e8":"* Parameter gets updated by optimizer\n* In our case we use Gradient descent ","747e149a":"<img src=\"https:\/\/thumbs.gfycat.com\/DeadlyDeafeningAtlanticblackgoby-max-1mb.gif\" width=\"1024\" height=\"1024\">","276d0f6b":"# **Steps Followed**\n\n1. Create a skeleton class for Network\n\n2. Inherit the skeleton class to make a FCLayer class\n   * Forward Propagation\n   * Backward Propagation\n\n3. Activation Layer\n   * Tanh\n   * Tanh derivative\n\n4. Calculate Loss\n   * MSE\n\n5. Data Exraction and preprocessing part\n\n6. Model Creation Part\n\n7. Model training part\n\n8. Visualizing Epochs VS Loss\n\n9. Making Prediction","41f0202d":"<img src=\"https:\/\/i.ytimg.com\/vi\/An5z8lR8asY\/hqdefault.jpg\" width=\"1024\" height=\"1024\">","c5f40769":"# Skeleton for FCLayer","08d020d9":"# Creating Network","a9eb9819":"**Forward propagation**\n\n*As the name suggests, the input data is fed in the forward direction through the network. Each hidden layer accepts the input data, processes it as per the activation function and passes to the successive layer.*\n\nAt each neuron in a hidden or output layer, the processing happens in two steps:\n\n    Preactivation: it is a weighted sum of inputs i.e. the linear transformation of weights w.r.t to inputs available. Based on this aggregated sum and activation function the neuron makes a decision whether to pass this information further or not.\n    \n    Activation: the calculated weighted sum of inputs is passed to the activation function. An activation function is a mathematical function which adds non-linearity to the network. There are four commonly used and popular activation functions \u2014 sigmoid, hyperbolic tangent(tanh), ReLU and Softmax.\n    \n\n**BackPropagation**\n\n* The algorithm is used to effectively train a neural network through a method called chain rule. In simple terms, after each forward pass through a network, backpropagation performs a backward pass while adjusting the model\u2019s parameters (weights and biases).","27c6453a":"<img src=\"http:\/\/ronny.rest\/media\/blog\/2017\/2017_08_16_tanh\/tanh_and_gradient.jpg\" width=\"500\" height=\"500\">","0641eb3d":"# Prediction","5befcc3c":"<img src=\"https:\/\/i.imgur.com\/vB3UAiH.jpg\" width=\"500\" height=\"500\">","81835c1f":"<img src=\"https:\/\/miro.medium.com\/max\/1400\/1*vfglxAeL7CUhfG95vhVdLw.png\" width=\"1024\" height=\"1024\">"}}