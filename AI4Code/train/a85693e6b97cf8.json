{"cell_type":{"264d1c4b":"code","bf1336df":"code","e1986037":"code","ccc830bf":"code","efeb3aeb":"code","9dd9799b":"code","7f7262ad":"code","cca4be13":"code","96360a82":"code","e4ce5601":"code","6fd20705":"code","5349a033":"code","cddc2932":"code","50d5d4b7":"code","9f07ae88":"code","11cfc5a6":"code","55466819":"code","39f6b754":"code","a8903166":"code","d3e015f8":"code","df868e77":"code","2a885e63":"code","65ceacff":"code","8f6fe303":"code","4c224603":"code","9325410d":"code","02efe0fb":"code","4a0d0071":"code","1d75b7f8":"code","d17056e7":"code","9a7f48ff":"markdown","4f071827":"markdown","11b1b74e":"markdown","610ba539":"markdown","0e98513a":"markdown","3a04eb69":"markdown","3e540832":"markdown","6836ca04":"markdown","b2d41ac4":"markdown","29470fac":"markdown","dc813730":"markdown","862c7972":"markdown","7621d25a":"markdown","7074db5b":"markdown","af491986":"markdown","14e26b55":"markdown","524d8a49":"markdown","b5836290":"markdown","7c223ff2":"markdown","bb89519d":"markdown","30d53fac":"markdown","0dcf04d6":"markdown","61419d71":"markdown","7d584242":"markdown","2412ed38":"markdown","6589b00a":"markdown","e7c01b04":"markdown","29e48e91":"markdown","6918d265":"markdown","7cbacfe3":"markdown","aa17e1e3":"markdown","619bbdbe":"markdown","e89dafe0":"markdown","6bc34d04":"markdown"},"source":{"264d1c4b":"import numpy as np\nimport pandas as pd\nimport statsmodels\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import coint, adfuller\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set(style=\"whitegrid\")","bf1336df":"def generate_data(params):\n    mu = params[0]\n    sigma = params[1]\n    return np.random.normal(mu, sigma)","e1986037":"# Set the parameters and the number of datapoints\nparams = (0, 1)\nT = 100\n\nA = pd.Series(index=range(T))\nA.name = 'A'\n\nfor t in range(T):\n    A[t] = generate_data(params)\n\nT = 100\n\nB = pd.Series(index=range(T))\nB.name = 'B'\n\nfor t in range(T):\n    # Now the parameters are dependent on time\n    # Specifically, the mean of the series changes over time\n    params = (t * 0.1, 1)\n    B[t] = generate_data(params)\n    \nfig, (ax1, ax2) = plt.subplots(nrows =1, ncols =2, figsize=(16,6))\n\nax1.plot(A)\nax2.plot(B)\nax1.legend(['Series A'])\nax2.legend(['Series B'])\nax1.set_title('Stationary')\nax2.set_title('Non-Stationary')","ccc830bf":"mean = np.mean(B)\n\nplt.figure(figsize=(12,6))\nplt.plot(B)\nplt.hlines(mean, 0, len(B), linestyles='dashed', colors = 'r')\nplt.xlabel('Time')\nplt.xlim([0, 99])\nplt.ylabel('Value')\nplt.legend(['Series B', 'Mean'])","efeb3aeb":"def stationarity_test(X, cutoff=0.01):\n    # H_0 in adfuller is unit root exists (non-stationary)\n    # We must observe significant p-value to convince ourselves that the series is stationary\n    pvalue = adfuller(X)[1]\n    if pvalue < cutoff:\n        print('p-value = ' + str(pvalue) + ' The series ' + X.name +' is likely stationary.')\n    else:\n        print('p-value = ' + str(pvalue) + ' The series ' + X.name +' is likely non-stationary.')","9dd9799b":"stationarity_test(A)\nstationarity_test(B)","7f7262ad":"# Generate daily returns\n\nXreturns = np.random.normal(0, 1, 100)\n\n# sum up and shift the prices up\n\nX = pd.Series(np.cumsum(\n    Xreturns), name='X') + 50\nX.plot(figsize=(15,7))\n\nnoise = np.random.normal(0, 1, 100)\nY = X + 5 + noise\nY.name = 'Y'\n\npd.concat([X, Y], axis=1).plot(figsize=(15, 7))\n\nplt.show()","cca4be13":"\nplt.figure(figsize=(12,6))\n(Y - X).plot() # Plot the spread\nplt.axhline((Y - X).mean(), color='red', linestyle='--') # Add the mean\nplt.xlabel('Time')\nplt.xlim(0,99)\nplt.legend(['Price Spread', 'Mean']);","96360a82":"score, pvalue, _ = coint(X,Y)\nprint(pvalue)\n\n# Low pvalue means high cointegration!","e4ce5601":"X_returns = np.random.normal(1, 1, 100)\nY_returns = np.random.normal(2, 1, 100)\n\nX_diverging = pd.Series(np.cumsum(X_returns), name='X')\nY_diverging = pd.Series(np.cumsum(Y_returns), name='Y')\n\n\npd.concat([X_diverging, Y_diverging], axis=1).plot(figsize=(12,6));\nplt.xlim(0, 99)\n","6fd20705":"print('Correlation: ' + str(X_diverging.corr(Y_diverging)))\nscore, pvalue, _ = coint(X_diverging,Y_diverging)\nprint('Cointegration test p-value: ' + str(pvalue))","5349a033":"Y2 = pd.Series(np.random.normal(0, 1, 1000), name='Y2') + 20\nY3 = Y2.copy()\n\n# Y2 = Y2 + 10\nY3[0:100] = 30\nY3[100:200] = 10\nY3[200:300] = 30\nY3[300:400] = 10\nY3[400:500] = 30\nY3[500:600] = 10\nY3[600:700] = 30\nY3[700:800] = 10\nY3[800:900] = 30\nY3[900:1000] = 10\n\n\nplt.figure(figsize=(12,6))\nY2.plot()\nY3.plot()\nplt.ylim([0, 40])\nplt.xlim([0, 1000]);\n\n# correlation is nearly zero\nprint( 'Correlation: ' + str(Y2.corr(Y3)))\nscore, pvalue, _ = coint(Y2,Y3)\nprint( 'Cointegration test p-value: ' + str(pvalue))","cddc2932":"!pip install yfinance","50d5d4b7":"pd.core.common.is_list_like = pd.api.types.is_list_like\nfrom pandas_datareader import data as pdr\nimport datetime\nimport yfinance as yf\nyf.pdr_override()","9f07ae88":"def find_cointegrated_pairs(data):\n    n = data.shape[1]\n    score_matrix = np.zeros((n, n))\n    pvalue_matrix = np.ones((n, n))\n    keys = data.keys()\n    pairs = []\n    for i in range(n):\n        for j in range(i+1, n):\n            S1 = data[keys[i]]\n            S2 = data[keys[j]]\n            result = coint(S1, S2)\n            score = result[0]\n            pvalue = result[1]\n            score_matrix[i, j] = score\n            pvalue_matrix[i, j] = pvalue\n            if pvalue < 0.05:\n                pairs.append((keys[i], keys[j]))\n    return score_matrix, pvalue_matrix, pairs","11cfc5a6":"t=pd.read_csv('..\/input\/stocks-listed-on-nifty-500-july-2021\/ind_nifty50list.csv')\nit_t= t[t['Industry']=='IT']\n#display(it_t)\nfin_t= t[t['Industry']=='FINANCIAL SERVICES']\n#display(fin_t)\n#display(t)\n# fin_t.Symbol + '.NS'","55466819":"start = datetime.datetime(2013, 1, 1)\nend = datetime.datetime(2018, 1, 1)\n\ntickers = ['AAPL', 'ADBE', 'ORCL', 'EBAY', 'MSFT', 'QCOM', 'HPQ', 'JNPR', 'AMD', 'IBM', 'SPY']\n\n\ndf = pdr.get_data_yahoo(tickers, start, end)['Close']\ndf.tail()","39f6b754":"# Heatmap to show the p-values of the cointegration test between each pair of\n# stocks. Only show the value in the upper-diagonal of the heatmap\nscores, pvalues, pairs = find_cointegrated_pairs(df)\nimport seaborn\nfig, ax = plt.subplots(figsize=(10,10))\nseaborn.heatmap(pvalues, xticklabels=tickers, yticklabels=tickers, cmap='RdYlGn_r' \n                , mask = (pvalues >= 0.05)\n                )\nprint(pairs)","a8903166":"S1 = df['ADBE']\nS2 = df['MSFT']\n\nscore, pvalue, _ = coint(S1, S2)\npvalue","d3e015f8":"S1 = sm.add_constant(S1)\nresults = sm.OLS(S2, S1).fit()\nS1 = S1['ADBE']\nb = results.params['ADBE']\n\nspread = S2 - b * S1\nspread.plot(figsize=(12,6))\nplt.axhline(spread.mean(), color='black')\nplt.xlim('2013-01-01', '2018-01-01')\nplt.legend(['Spread']);","df868e77":"ratio = S1\/S2\nratio.plot(figsize=(12,6))\nplt.axhline(ratio.mean(), color='black')\nplt.xlim('2013-01-01', '2018-01-01')\nplt.legend(['Price Ratio']);","2a885e63":"def zscore(series):\n    return (series - series.mean()) \/ np.std(series)\n\n\nzscore(ratio).plot(figsize=(12,6))\nplt.axhline(zscore(ratio).mean())\nplt.axhline(1.0, color='red')\nplt.axhline(-1.0, color='green')\nplt.xlim('2013-01-01', '2018-01-01')\nplt.show()","65ceacff":"ratios = df['ADBE'] \/ df['MSFT'] \nprint(len(ratios) * .70 ) ","8f6fe303":"train = ratios[:881]\ntest = ratios[881:]","4c224603":"ratios_mavg5 = train.rolling(window=5, center=False).mean()\nratios_mavg60 = train.rolling(window=60, center=False).mean()\nstd_60 = train.rolling(window=60, center=False).std()\nzscore_60_5 = (ratios_mavg5 - ratios_mavg60)\/std_60\nplt.figure(figsize=(12, 6))\nplt.plot(train.index, train.values)\nplt.plot(ratios_mavg5.index, ratios_mavg5.values)\nplt.plot(ratios_mavg60.index, ratios_mavg60.values)\nplt.legend(['Ratio', '5d Ratio MA', '60d Ratio MA'])\n\nplt.ylabel('Ratio')\nplt.show()","9325410d":"plt.figure(figsize=(12,6))\nzscore_60_5.plot()\nplt.xlim('2013-03-25', '2016-07-01')\nplt.axhline(0, color='black')\nplt.axhline(1.0, color='red', linestyle='--')\nplt.axhline(-1.0, color='green', linestyle='--')\nplt.legend(['Rolling Ratio z-Score', 'Mean', '+1', '-1'])\nplt.show()","02efe0fb":"plt.figure(figsize=(12,6))\n\ntrain[160:].plot()\nbuy = train.copy()\nsell = train.copy()\nbuy[zscore_60_5>-1] = 0\nsell[zscore_60_5<1] = 0\nbuy[160:].plot(color='g', linestyle='None', marker='^')\nsell[160:].plot(color='r', linestyle='None', marker='^')\nx1, x2, y1, y2 = plt.axis()\nplt.axis((x1, x2, ratios.min(), ratios.max()))\nplt.xlim('2013-08-15','2016-07-07')\nplt.legend(['Ratio', 'Buy Signal', 'Sell Signal'])\nplt.show()","4a0d0071":"plt.figure(figsize=(12,7))\nS1 = df['ADBE'].iloc[:881]\nS2 = df['MSFT'].iloc[:881]\n\nS1[60:].plot(color='b')\nS2[60:].plot(color='c')\nbuyR = 0*S1.copy()\nsellR = 0*S1.copy()\n\n# When you buy the ratio, you buy stock S1 and sell S2\nbuyR[buy!=0] = S1[buy!=0]\nsellR[buy!=0] = S2[buy!=0]\n\n# When you sell the ratio, you sell stock S1 and buy S2\nbuyR[sell!=0] = S2[sell!=0]\nsellR[sell!=0] = S1[sell!=0]\n\nbuyR[60:].plot(color='g', linestyle='None', marker='^')\nsellR[60:].plot(color='r', linestyle='None', marker='^')\nx1, x2, y1, y2 = plt.axis()\nplt.axis((x1, x2, min(S1.min(), S2.min()), max(S1.max(), S2.max())))\nplt.ylim(25, 105)\nplt.xlim('2013-03-22', '2016-07-04')\n\nplt.legend(['ADBE', 'MSFT', 'Buy Signal', 'Sell Signal'])\nplt.show()","1d75b7f8":"# Trade using a simple strategy\ndef trade(S1, S2, window1, window2):\n    \n    # If window length is 0, algorithm doesn't make sense, so exit\n    if (window1 == 0) or (window2 == 0):\n        return 0\n    \n    # Compute rolling mean and rolling standard deviation\n    ratios = S1\/S2\n    ma1 = ratios.rolling(window=window1,\n                               center=False).mean()\n    ma2 = ratios.rolling(window=window2,\n                               center=False).mean()\n    std = ratios.rolling(window=window2,\n                        center=False).std()\n    zscore = (ma1 - ma2)\/std\n    \n    # Simulate trading\n    # Start with no money and no positions\n    money = 0\n    countS1 = 0\n    countS2 = 0\n    for i in range(len(ratios)):\n        # Sell short if the z-score is > 1\n        if zscore[i] < -1:\n            money += S1[i] - S2[i] * ratios[i]\n            countS1 -= 1\n            countS2 += ratios[i]\n            #print('Selling Ratio %s %s %s %s'%(money, ratios[i], countS1,countS2))\n        # Buy long if the z-score is < -1\n        elif zscore[i] > 1:\n            money -= S1[i] - S2[i] * ratios[i]\n            countS1 += 1\n            countS2 -= ratios[i]\n            #print('Buying Ratio %s %s %s %s'%(money,ratios[i], countS1,countS2))\n        # Clear positions if the z-score between -.5 and .5\n        elif abs(zscore[i]) < 0.75:\n            money += S1[i] * countS1 + S2[i] * countS2\n            countS1 = 0\n            countS2 = 0\n            #print('Exit pos %s %s %s %s'%(money,ratios[i], countS1,countS2))\n            \n            \n    return money","d17056e7":"trade(df['ADBE'].iloc[881:], df['EBAY'].iloc[881:], 60, 5)","9a7f48ff":"#### Creating a Model\n\nA standard normal distribution has a mean of 0 and a standard deviation 1. Looking at the plot, it's pretty clear that if the time series moves 1 standard deviation beyond the mean, it tends to revert back towards the mean. Using these models, we can create the following trading signals:\n\n* Buy(1) whenever the z-score is below -1, meaning we expect the ratio to increase.\n* Sell(-1) whenever the z-score is above 1, meaning we expect the ratio to decrease.","4f071827":"The computed mean will show that the mean of all data points, but won't be useful for any forecasting of future state. It's meaningless when compared with any specific time, as it's a collection of different states at different times mashed together. This is just a simple and clear example of why non-stationarity can distort the analysis, much more subtle problems can arise in practice.","11b1b74e":"$\\hat{\\phi}$ is the least square estimate and SE($\\hat{\\phi}$) is the usual standard error estimate. The test is a one-sided left tail test. If {$y_{t}$} is stationary, then it can be shown that\n\n$$\\sqrt{T}(\\hat{\\phi}-\\phi)\\xrightarrow[\\text{}]{\\text{d}}N(0,(1-\\phi^{2}))$$\n\nor \n\n$$\\hat{\\phi}\\overset{\\text{A}}{\\sim}N\\bigg(\\phi,\\frac{1}{T}(1-\\phi^{2}) \\bigg)$$\n\nandit follows that $t_{\\phi=1}\\overset{\\text{A}}{\\sim}N(0,1).$ However, under the null hypothesis of non-stationarity, the above result gives\n\n$$\n\\hat{\\phi}\\overset{\\text{A}}{\\sim} N(0,1)\n$$\n\nThe following function will allow us to check for stationarity using the Augmented Dickey Fuller (ADF) test.","610ba539":"Our algorithm listed two pairs that are cointegrated: AAPL\/EBAY, and ABDE\/MSFT. We can analyze their price patterns to make sure there is nothing weird going on.","0e98513a":"Although the correlation is incredibly low, the p-value shows that these time series are cointegrated.","3a04eb69":"By setting two other lines placed at the z-score of 1 and -1, we can clearly see that for the most part, any big divergences from the mean eventually converges back. This is exactly what we want for a pairs trading strategy.","3e540832":"As we can see, based on the test statistic (which correspnds with a specific p-value) for time series A, we can fail to reject the null hypothesis. As such, Series A is likely to be  stationary. On the other hand, Series B is rejected by the hypothesis test, so this time series is likely to be non-stationary.","6836ca04":"# Creating and Back-Testing a Pairs Trading Strategy\nPairs trading is one of the many mean-reversion strategies. It is considered non-directional and relative as it aims to trade on both related stocks with similar statistical and economical properties. It first identifies a historical relationship that has been recently broken and then buys the implied undervalued security while shorting the overvalued security in attempt to revert to the original relationship. We must first understand the difference between correlation and cointegration. Correlation (\u03c1) measures the degree of the linear relationship between variables, it has the following features:\n\n    \u03c1 = -1, means a perfectly negative relationship. They move in the opposite direction.\n    \u03c1 = 0, means there is no linear relationship between the two.\n    \u03c1 = 1, means a perfect linear relationship between the datasets. They move hand in hand.\n    \nCorrelation does not imply causality. It is not used to explain variations in one dataset caused by the other. We use regression analysis for that issue. Negatively correlated assets can be used to hedge one another, and it is the basic diversification rule for portfolio management. If we have two different datasets and their two respective variances, we can calculate a measure called covariance. Covariance has the same principle as correlation except that it is unbounded and therefore not really meaningful. It is however used in the formula of correlation:\n\nIf the correlation coefficient is zero, it does not necessarily mean that there is absolutely no relationship between the two datasets. It simply means that there is not a linear one.\n\nWe must be extremely cautious with correlation. With financial data you will most likely have to calculate correlations of returns and not prices. If you\u2019re interested in short-term relationships, go for returns\u2019 correlations, otherwise, opt for prices if you\u2019re in it for the long-term. Cointegration, on the other hand, is not too dissimilar to correlation. In plain terms, it means that the ratio between the two financial time series will vary around a constant mean. Pairs trading can rely on the constant ratio that is expected to revert to its long-term mean (i.e. converge). Below are two ETFs with the following details:\n\n    EWA: iShares MSCI Australia ETF, seeks to replicate investment results that correspond to the price and yield performance of publicly traded securities in the Australian market.\n    \n    EWC: iShares MSCI Canada ETF seeks to replicate investment results that correspond to the price and yield performance of publicly traded securities in the Canadian market.\n    \nThey do seem very correlated, but are they cointegrated? In other words, is their spread mean-reverting and stationary? That\u2019s what we will be back-testing. If the series are indeed stationary then we will form a pairs trading simple strategy based on their spread (ignoring the hedge ratio). Let\u2019s consider EWA to be Asset_2 variable and EWC to be Asset_1 variable in our code. The calculated correlation between the two seems to be the first unlocked door towards the strategy:","b2d41ac4":"## Pairs Trading\n\nPairs trading is a form of *mean-reversion* that has a distinct advantage of always being hedged against market movements. It is generally a high alpha strategy when backed up by some rigorous statistics. The stratey is based on mathematical analysis.\n\nThe prinicple is as follows. Let's say you have a pair of securities X and Y that have some underlying economic link. An example might be two companies that manufacture the same product, or two companies in one supply chain. If we can model this economic link with a mathematical model, we can make trades on it.\n\nIn order to understand pairs trading, we need to understand three mathematical concepts: **Stationarity, Integration, and Cointegration**.\n\n**Note:** This will assume everyone knows the basics of hypothesis testing.","29470fac":"#### Feature Engineering\n\nWe need to find out what features are actually important in determining the direction of the ratio moves. Knowing that the ratios always eventually revert back to the mean, maybe the moving averages and metrics related to the mean will be important.\n\nLet's try using these features:\n\n* 60 day Moving Average of Ratio\n* 5 day Moving Average of Ratio\n* 60 day Standard Deviation\n* z score","dc813730":"### Why Stationarity is Important\n\nMany statistical test require that the data being tested are stationary. Using certain statistics on a non-stationary data set may lead to garbage results. As an example, let's take an average through our non-stationary $B$.","862c7972":"### Areas of Improvement and Further Steps\n\nThis is by no means a perfect strategy and the implementation of our strategy isn't the best. However, there are several things that can be improved upon.\n\n#### 1. Using more securities and more varied time ranges\n\nFor the pairs trading strategy cointegration test, I only used a handful of stocks. Naturally (and in practice) it would be more effective to use clusters within an industry. I only use the time range of only 5 years, which may not be representative of stock market volatility.\n\n#### 2. Dealing with overfitting\n\nAnything related to data analysis and training models has much to do with the problem of overfitting. There are many different ways to deal with overfitting like validation, such as Kalman filters, and other statistical methods.\n\n#### 3. Adjusting the trading signals\n\nOur trading algorithm fails to account for stock prices that overlap and cross each other. Considering that the code only calls for a buy or sell given its ratio, it doesn't take into account which stock is actually higher or lower.\n\n#### 4. More advanced methods\n\nThis is just the tip of the iceberg of what you can do with algorithmic pairs trading. It's simple because it only deals with moving averages and ratios. If you want to use more complicated statistics, feel free to do so. Other complex examples include subjects such as the Hurst exponent, half-life mean reversion, and Kalman Filters.","7621d25a":"Now we can clearly see when we should buy or sell on the respective stocks.\n\nNow, how much can we expect to make of this strategy?","7074db5b":"# Playing with stocks data","af491986":"Regardless of whether or not we use the spread approach or the ratio approach, we can see that our first plot pair ADBE\/SYMC tends to move around the mean. We now need to standardize this ratio because the absolute ratio might not be the most ideal way of analyzing this trend. For this, we need to use z-scores.\n\nA z-score is the number of standard deviations a datapoint is from the mean. More importantly, the nmber of standard deviations above or below the population mean is from the raw score. The z-score is calculated by the follow:\n\n$$\\mathcal{z}_{i}=\\frac{x_{i}-\\bar{x}}{s} $$","14e26b55":"#### Correlation vs. Cointegration\n\nCorrelation and cointegration, while theoretically similiar, are anything but similiar. To demonstrate this, we can look at examples of two time series that are correlated, but not cointegrated.\n\nA simple example is two series that just diverge.","524d8a49":"### Cointegration\n\nThe correlations between financial quantities are notoriously unstable. Nevertheless, correlations are regularly used in almost all multivariate financial problems. An alternative statistical measure to correlation is cointegration. This is probably a more robust measure of linkage between two financial quantities, but as yet there is little derviaties theory based on this concept.\n\nTwo stocks may be perfectly correlated over short timescales, yet diverge in the long run, with one growing and the other decaying. Conversely, two stocks may follow each other, never being more than a certain distance apart, but with any correlation, positive negaative or varying. If we are delta hedging, then maybe the short timescale orrelation matters, but not if we are holding stocks for a long time in an unhedged portfolio.\n\nWe've constructed an example of two cointegrated series. We'll plot the difference between the two now so we can see how this looks.","b5836290":"Alternatively, we can examine the ration between the two time series","7c223ff2":"From there, we can create two plots that exhibit a stationary and non-stationary time series.","bb89519d":"As we can see, there is a very strong (nearly perfect) correlation between series X and Y. However, our p-value for the cointegration test yields a result of 0.7092, which means there is no cointegration between time series X and Y.\n\nAnother example of this case is a normally distributed series and a sqaure wave.","30d53fac":"As we can see, the p-value is less than 0.05, which means ADBE and MSFT are indeed cointegrated pairs.","0dcf04d6":"#### Calculating the Spread\n\nNow we can plot the spread of the two time series. In order to actually calculate the spread, we use a linear regression to get the coefficient for the linear combination to construct between our two securities, as mentioned with the Engle-Granger method before.","61419d71":"### Trading Signals\n\nWhen conducting any type of trading strategy, it's always important to clearly define and delineate at what point you will actually do a trade. As in, what is the best indicator that I need to buy or sell a particular stock? \n\n#### Setup rules\n\nWe're going to use the ratio time series that we've created to see if it tells us whether to buy or sell a particular moment in time. We'll start off by creating a prediction variable $Y$. If the ratio is positive, it will signal a \"buy,\" otherwise, it will signal a sell. The prediction model is as follows:\n\n$$Y_{t} = sign(Ratio_{t+1}-Ratio_{t}) $$\n\nWhat's great about pair trading signals is that we don't need to know absolutes about where the prices will go, all we need to know is where it's heading: up or down.","7d584242":"### Stationarity\/Non-Stationarity\n\nStationarity is the most commonly untestedassumption in time series analysis. We generally assume that data is stationary when the parameters of the data generating process do not change over time. Else consider two series: A and B. Series A will generate a stationary time series with fixed parameters, while B will change over time.\n\nWe will create a function that creates a z-score for probability density function. The probability density for a Gaussian distribution is:\n\n$$ p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}e^{-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}}$$\n","2412ed38":"#### Testing for Cointegration\n\nThe steps in the cointegration test procdure:\n\n1. Test for a unit root in each component series $y_{t}$ individually, using the univariate unit root tests, say ADF, PP test.\n2. If the unit root cannot be rejected, then the next step is to test cointegration among the components, i.e., to test whether $\\alpha Y_{t}$ is I(0).\n\nIf we find that the time series as a unit root, then we move on to the cointegration process. There are three main methods for testing for cointegration: Johansen, Engle-Granger, and Phillips-Ouliaris. We will primarily use the Engle-Granger test.\n\nLet's consider the regression model for $y_{t}$:\n\n$$y_{1t} = \\delta D_{t} + \\phi_{1t}y_{2t} + \\phi_{m-1} y_{mt} + \\varepsilon_{t} $$\n\n$D_{t}$ is the deterministic term. From there, we can test whether $\\varepsilon_{t}$ is $I(1)$ or $I(0)$. The hypothesis test is as follows:\n\n$$\n\\begin{aligned}\nH_{0} & :  \\varepsilon_{t} \\sim I(1) \\implies y_{t} \\ (no \\ cointegration)  \\\\\nH_{1} & : \\varepsilon_{t} \\sim I(0) \\implies y_{t} \\ (cointegration)  \\\\\n\\end{aligned}\n$$\n\n$y_{t}$ is cointegrated with a *normalized cointegration vector* $\\alpha = (1, \\phi_{1}, \\ldots,\\phi_{m-1}).$\n\nWe also use residuals $\\varepsilon_{t}$ for unit root test.\n\n$$\n\\begin{aligned}\nH_{0} & :  \\lambda = 0 \\ (Unit \\ Root)  \\\\\nH_{1} & : \\lambda < 1 \\ (Stationary)  \\\\\n\\end{aligned}\n$$\n\nThis hypothesis test is for the model:\n\n$$\\Delta\\varepsilon_{t}=\\lambda\\varepsilon_{t-1}+\\sum^{p-1}_{j=1}\\varphi\\Delta\\varepsilon_{t-j}+\\alpha_{t}$$\n\nThe test statistic for the following equation:\n\n$$t_{\\lambda}=\\frac{\\hat{\\lambda}}{s_{\\hat{\\lambda}}} $$\n\nNow that you understand what it means for two time series to be cointegrated, we can test for it and measure it using python:","6589b00a":"Not a bad profit for a strategy that is made from stratch.","e7c01b04":"We are looking through a set of tech companies to see if any of them are cointegrated. We'll start by defining the list of securities we want to look through. Then we'll get the pricing data for each security from the year 2013 - 2018..\n\nAs mentioned before, we have formulated an economic hypothesis that there is some sort of link between a subset of securities within the tech sector and we want to test whether there are any cointegrated pairs. This incurs significantly less multiple comparisons bias than searching through hundreds of securities and slightly more than forming a hypothesis for an individual test.","29e48e91":"#### Augmented Dickey Fuller\n\nIn order to test for stationarity, we need to test for something called a *unit root*. Autoregressive unit root test are based the following hypothesis test:\n\n$$\n\\begin{aligned}\nH_{0} & : \\phi =\\ 1\\ \\implies y_{t} \\sim I(0) \\ | \\ (unit \\ root) \\\\\nH_{1} & : |\\phi| <\\ 1\\ \\implies y_{t} \\sim I(0) \\ | \\ (stationary)  \\\\\n\\end{aligned}\n$$\n\nIt's referred to as a unit root tet because under the null hypothesis, the autoregressive polynominal of $\\scr{z}_{t},\\ \\phi (\\scr{z})=\\ (1-\\phi \\scr{z}) \\ = 0$, has a root equal to unity. \n\n$y_{t}$ is trend stationary under the null hypothesis. If $y_{t}$is then first differenced, it becomes:\n\n$$\n\\begin{aligned}\n\\Delta y_{t} & = \\delta\\ + \\Delta\\scr{z}_{t} \\\\\n\\Delta \\scr_{z} & = \\phi\\Delta\\scr{z}_{t-1}\\ +\\ \\varepsilon_{t}\\ -\\ \\varepsilon_{t-1} \\\\\n\\end{aligned}\n.$$\n\nThe test statistic is\n\n$$ t_{\\phi=1}=\\frac{\\hat{\\phi}-1}{SE(\\hat{\\phi})}$$","6918d265":"#### Train Test Split\n\nWhen training and testing a model, it's common to have splits of 70\/30 or 80\/20. We only used a time series of 252 points (which is the amount of trading days in a year). Before training and splitting the data, we will add more data points in each time series.","7cbacfe3":"### Data Science in Trading\n\nBefore we begin, I\u2019ll first define a function that makes it easy to find cointegrated security pairs using the concepts we\u2019ve already covered.","aa17e1e3":"# Pairs Trading Strategies Using Python\n\nWhen it comes to making money in the stock market, there are a myriad of different ways to make money. And it seems that in the finance community, everywhere you go, people are telling you that you should learn Python. After all, Python is a popular programming language which can be used in all types of fields, including data science. There are a large number of packages that can help you meet your goals, and many companies use Python for development of data-centric applications and scientific computation, which is associated with the financial world.\n\nMost of all Python can help us utilize many different trading strategies that (without it) would by very difficult to analyze by hand or with spreadsheets. One of the trading strategies we will talk about is referred to as **Pairs Trading.**\n\n\n## Goal\n\nOur goal involves the following:\n\n    Part 1: Creating a model that test for stationarity.\n    Part 2: Creating a model that test for cointegration.\n    Part 3: Assigning a portfolio of assests and testing for a cointegrated pair among the dataset.\n    Part 4: Establishing features and labels that will allow us to create trading signals for the strategy","619bbdbe":"$\\mu$ is the mean and $\\sigma$ is the standard deviation. The square of the standard deviation, $\\sigma^{2}$, is the variance. The empircal rule dictates that 66% of the data should be somewhere between $x+\\sigma$ and $x-\\sigma$,which implies that the function `numpy.random.normal` is more likely to return samples lying close to the mean, rather than those far away.","e89dafe0":"Next, we can print the correlation coefficient, $r$, and the cointegration test ","6bc34d04":"#### Training Optimizing\n\nWe can use our model on actual data"}}