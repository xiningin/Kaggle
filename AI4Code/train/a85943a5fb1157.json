{"cell_type":{"58ae9e98":"code","c928755e":"code","214abbac":"code","e331459a":"code","fb8d32f8":"code","3071d4b5":"code","3a9f1c3d":"code","b010fe54":"code","58a17a63":"code","a0f17751":"code","672a149b":"code","c888fe8b":"code","346d7806":"code","7d34829e":"code","41238ff4":"code","76848fac":"code","9be0009a":"code","7133c418":"code","019e4c6b":"code","5f473e01":"code","0e6ed126":"code","3347221b":"code","a7ea09d8":"code","6feab2b4":"code","87c33058":"code","c9ee408f":"code","da1a315f":"code","f9a6a00d":"code","9409ef77":"code","599c8ba3":"code","6e4f1a02":"code","bd9f0b9d":"code","b65a42e9":"code","c68ecea5":"code","783fc8e5":"code","bf55891d":"code","701bf936":"code","f0679a20":"code","0162cf47":"code","699278e2":"code","658958cb":"code","ca1b7e98":"code","d9022ac7":"code","d08d311e":"code","84a03a48":"code","21332364":"code","b24356d0":"code","e5739bb7":"code","4ffced02":"code","38f618cf":"code","fed8cf87":"code","19448326":"code","29cad1aa":"code","0449c3a7":"markdown","c09d34ff":"markdown","a87d7697":"markdown","6f1c2a3f":"markdown","18eb75af":"markdown","1ecfe0a7":"markdown","2d34112e":"markdown","bc2c3f10":"markdown","a8b4a46d":"markdown","7a46095e":"markdown","4fa9fee3":"markdown","55b18c4e":"markdown","a08788b5":"markdown","9c690749":"markdown","3a1fdc74":"markdown","38df8576":"markdown","2b6c3940":"markdown","6c2c11ac":"markdown","4668fb56":"markdown"},"source":{"58ae9e98":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c928755e":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nsample = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")","214abbac":"train.head()","e331459a":"test.head()","fb8d32f8":"sample.head()","3071d4b5":"train_copy = train.copy()\ntest_copy = test.copy()","3a9f1c3d":"train.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\ntest.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)","b010fe54":"train.isnull().sum()","58a17a63":"test.isnull().sum()","a0f17751":"train['Age'] = train['Age'].fillna(train['Age'].mean())\ntest['Age'] = test['Age'].fillna(test['Age'].mean())","672a149b":"from sklearn.impute import SimpleImputer\n\nimpute = SimpleImputer(strategy='most_frequent')","c888fe8b":"train['Embarked'] = impute.fit_transform(train['Embarked'].values.reshape(-1,1))\ntest['Fare'] = test['Fare'].fillna(test['Fare'].mean())","346d7806":"cat_cols = train.select_dtypes(object).columns","7d34829e":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\nfor col in cat_cols:\n    train[col] = le.fit_transform(train[col])\n    test[col] = le.transform(test[col])","41238ff4":"train['Age'] = np.round(train['Age'])\ntest['Age'] = np.round(test['Age'])","76848fac":"from sklearn.preprocessing import KBinsDiscretizer\n\nbinner = KBinsDiscretizer(n_bins=5, encode='ordinal')","9be0009a":"train['Age_group'] = binner.fit_transform(train['Age'].values.reshape(-1,1)).astype('int64')\ntest['Age_group'] = binner.transform(test['Age'].values.reshape(-1,1)).astype('int64')","7133c418":"import matplotlib.pyplot as plt\nimport seaborn as sns","019e4c6b":"corr = train.corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(corr, cmap='coolwarm', annot=True)\nplt.show()","5f473e01":"from sklearn.preprocessing import RobustScaler, StandardScaler","0e6ed126":"x_train, y_train = train.drop('Survived', axis=1), train['Survived']\nx_test = test","3347221b":"robust = RobustScaler()\nstandard = StandardScaler()\n\nfor col in x_train.columns:\n  x_train[col] = robust.fit_transform(x_train[col].values.reshape(-1,1))\n  x_train[col] = standard.fit_transform(x_train[col].values.reshape(-1,1))\n\n  x_test[col] = robust.transform(x_test[col].values.reshape(-1,1))\n  x_test[col] = standard.transform(x_test[col].values.reshape(-1,1))","a7ea09d8":"sns.countplot(y_train)","6feab2b4":"from imblearn.over_sampling import SMOTE","87c33058":"smote = SMOTE()\n\nx_train, y_train = smote.fit_resample(x_train, y_train)","c9ee408f":"sns.countplot(y_train)","da1a315f":"from sklearn.model_selection import train_test_split","f9a6a00d":"x_train_, x_val, y_train_, y_val = train_test_split(x_train, y_train, test_size=0.2)","9409ef77":"from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier","599c8ba3":"from sklearn.metrics import mean_squared_error, classification_report, accuracy_score","6e4f1a02":"def model_selection(x_train_, x_val, y_train_, y_val, model):\n  model = model()\n  model.fit(x_train_, y_train_)\n\n  pred = model.predict(x_val)\n\n  error = np.sqrt(mean_squared_error(y_val, pred))\n  acc = accuracy_score(y_val, pred)\n  report = classification_report(y_val, pred)\n  train_score = model.score(x_train_, y_train_)\n  val_score = model.score(x_val, y_val)\n\n  print('Error:', error)\n  print('\\n')\n  print('ACC :', acc*100)\n  print('\\n')\n  print('Classification Report:', report)\n  print('\\n')\n  print('Train Score:', train_score*100)\n  print('\\n')\n  print('Val Score:', val_score*100)\n  print('\\n')\n  print('Is overfitting:', True if train_score>val_score else False)\n  print('\\n')\n  print('Overfitting by:',train_score*100-val_score*100)","bd9f0b9d":"extratrees = model_selection(x_train_, x_val, y_train_, y_val, ExtraTreesClassifier)\nextratrees","b65a42e9":"gradient = model_selection(x_train_, x_val, y_train_, y_val, GradientBoostingClassifier)\ngradient","c68ecea5":"random = model_selection(x_train_, x_val, y_train_, y_val, RandomForestClassifier)\nrandom","783fc8e5":"xgb = model_selection(x_train_, x_val, y_train_, y_val, XGBClassifier)\nxgb","bf55891d":"lgbm = model_selection(x_train_, x_val, y_train_, y_val, LGBMClassifier)\nlgbm","701bf936":"catboost = model_selection(x_train_, x_val, y_train_, y_val, CatBoostClassifier)\ncatboost","f0679a20":"sgd = model_selection(x_train_, x_val, y_train_, y_val, SGDClassifier)\nsgd","0162cf47":"logistic = model_selection(x_train_, x_val, y_train_, y_val, LogisticRegression)\nlogistic","699278e2":"nb = model_selection(x_train_, x_val, y_train_, y_val, GaussianNB)\nnb","658958cb":"svc = model_selection(x_train_, x_val, y_train_, y_val, SVC)\nsvc","ca1b7e98":"k = model_selection(x_train_, x_val, y_train_, y_val, KNeighborsClassifier)\nk","d9022ac7":"model = SVC()","d08d311e":"from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\nfrom scipy.stats import uniform as sp_randFloat\nfrom scipy.stats import randint as sp_randInt","84a03a48":"params = {\n    \"C\": sp_randFloat(),\n    \"gamma\": ['scale', 'auto'],\n    \"kernel\": ['linear', 'poly', 'rbf', 'sigmoid'],\n    \"degree\": sp_randInt(1, 10),\n    'shrinking': [True, False],\n    'probability':[True, False],\n    }","21332364":"cv = StratifiedKFold(n_splits=10)\n\nsearch = RandomizedSearchCV(estimator=model, param_distributions = params, cv = cv, n_iter = 1000, \n                            n_jobs=-1, scoring='accuracy')","b24356d0":"search.fit(x_train, y_train)","e5739bb7":"print('Best Params:', search.best_params_)\nprint('\\n')\nprint('Best Score:', search.best_score_)\nprint('\\n')\nprint('Best Estimator:', search.best_estimator_)","4ffced02":"model = SVC(C=0.8369134253721343, degree=6, gamma='scale', kernel='rbf', probability=True, shrinking=False)\n\nmodel.fit(x_train, y_train)","38f618cf":"pred = model.predict(x_test)\npred","fed8cf87":"sns.countplot(pred)","19448326":"IDS = test_copy['PassengerId']\n\nPred = pd.DataFrame(pred, columns=['Survived'])\n\nPred = pd.concat([IDS, Pred], axis=1)\nPred","29cad1aa":"Pred.to_csv(\"\/kaggle\/working\/Pred_.csv\", index=False)","0449c3a7":"# Hyper parameter tuning","c09d34ff":"Feature Scaling","a87d7697":"there are some missing cols lets deal with them","6f1c2a3f":"# Data Processing","18eb75af":"# Saving a copy of the datasets","1ecfe0a7":"# Predictions","2d34112e":"i will choose SVC","bc2c3f10":"dropping useless cols","a8b4a46d":"there is some imbalance lets fix this","7a46095e":"Feature Binning","4fa9fee3":"encoding categorical data to numeric data","55b18c4e":"Feature Imbalance","a08788b5":"# Model Building and training","9c690749":"Feature Correlation","3a1fdc74":"# Saving the predictions file","38df8576":"null values?","2b6c3940":"# Splitting training data","6c2c11ac":"# Model Selection","4668fb56":"# Loading the datasets"}}