{"cell_type":{"5293d320":"code","de0dc8db":"code","4533fb2c":"code","2d546e86":"code","43d6eccc":"code","24e626c9":"code","c72e302c":"code","0d09b525":"code","edf34946":"code","ffbcf661":"code","20f98560":"code","f9b07b87":"code","8d4adeda":"code","115caddd":"code","edbb2827":"code","b440bd61":"code","357823ee":"code","f6063a82":"code","ee022b78":"code","ac466d77":"code","57a707a5":"code","84a644b5":"code","413367b6":"code","5b0670fc":"code","b975a3cc":"code","be20b454":"code","e1f763ab":"code","8e47c47e":"code","7df51c26":"code","fc090c36":"code","127e8683":"code","1319c676":"code","97ca8c62":"code","cc0fa833":"code","691b6beb":"code","715f8bf0":"code","a3cf4cc4":"code","bddb5750":"code","be87a49e":"code","450d3dae":"code","ccfec302":"code","7bc29be0":"code","653a1611":"code","f8b4adb8":"code","b9f5e061":"code","b0e16dd5":"markdown","1016fc33":"markdown","a535f5b0":"markdown","477e6528":"markdown","11d8f698":"markdown","56f4991a":"markdown","8d62a1d9":"markdown","7ce65856":"markdown","1f4ef087":"markdown","d85acaba":"markdown","047b5d4d":"markdown","4c72df43":"markdown","63d543e5":"markdown","5ee3a79d":"markdown","3f628863":"markdown","41fe8b2b":"markdown","5c85baa9":"markdown","6363077e":"markdown","c94f458f":"markdown","eedb2368":"markdown"},"source":{"5293d320":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","de0dc8db":"# Imports for Exploratory Data Analysis\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Imports for Machine Learning Models\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\n\n# Validation\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Set the style of the plots' background\nsns.set_style(\"darkgrid\")\n\n# Show the plot in the same window as the notebook\n%matplotlib inline","4533fb2c":"jobs = pd.read_csv(\"\/kaggle\/input\/real-or-fake-fake-jobposting-prediction\/fake_job_postings.csv\")\njobs.head()","2d546e86":"plt.figure(figsize=(12,8))\nsns.heatmap(jobs.isnull(), cmap=\"coolwarm\", yticklabels=False, cbar=False)","43d6eccc":"jobs.drop(columns=[\"department\", \"salary_range\", \"benefits\"], inplace=True)","24e626c9":"jobs.info()","c72e302c":"jobs.describe()","0d09b525":"jobs.isnull().sum()","edf34946":"# List with the columns to check the length of the text\nfeature_lst = [\"company_profile\", \"description\", \"requirements\"]\n\n# For loop to treat the missing values in the columns of the feature_lst.\nfor col in feature_lst:\n    # If the job post is real, change the missing values to \"none\"\n    jobs.loc[(jobs[col].isnull()) & (jobs[\"fraudulent\"] == 0), col] = \"none\"\n    \n    # If the job post is fake, change the missing values to \"missing\"\n    jobs.loc[(jobs[col].isnull()) & (jobs[\"fraudulent\"] == 1), col] = \"missing\"","ffbcf661":"# For loop to create new columns with the lengths of the ones in the feature_lst\nfor num,col in enumerate(feature_lst):\n    jobs[str(num)] = jobs[col].apply(len)","20f98560":"# Rename the new columns created above\njobs = jobs.rename({\"0\": \"profile_length\", \"1\": \"description_length\", \"2\": \"requirements_length\"}, axis=1)","f9b07b87":"jobs.isnull().sum()","8d4adeda":"jobs[\"fraudulent\"].value_counts()","115caddd":"sns.pairplot(data=jobs[[\"fraudulent\", \"profile_length\", \"description_length\", \"requirements_length\"]],\n             hue=\"fraudulent\", height=2, aspect=2);","edbb2827":"profile_grid = sns.FacetGrid(jobs, col=\"fraudulent\", aspect=1.5, height=4, sharey=False)\nprofile_grid = profile_grid.map(plt.hist, \"profile_length\", bins=40)\n\n# Flatten the axes. Create an iterator\naxes = profile_grid.axes.flatten()\n\n# Title\naxes[0].set_title(\"Non-Fraudulent (0)\", fontsize=14)\naxes[1].set_title(\"Fraudulent (1)\", fontsize=14)\n\n# Labels\naxes[0].set_ylabel(\"Count\", fontsize=14)\nfor ax in axes:\n    ax.set_xlabel(\"Profile Text Length\", fontsize=14)","b440bd61":"description_grid = sns.FacetGrid(jobs, col=\"fraudulent\", aspect=1.5, height=4, sharey=False)\ndescription_grid = description_grid.map(plt.hist, \"description_length\", bins=40)\n\n# Flatten the axes. Create an iterator\naxes = description_grid.axes.flatten()\n\n# Title\naxes[0].set_title(\"Non-Fraudulent (0)\", fontsize=14)\naxes[1].set_title(\"Fraudulent (1)\", fontsize=14)\n\n# Labels\naxes[0].set_ylabel(\"Count\", fontsize=14)\nfor ax in axes:\n    ax.set_xlabel(\"Description Text Length\", fontsize=14)","357823ee":"requirements_grid = sns.FacetGrid(jobs, col=\"fraudulent\", aspect=1.5, height=4, sharey=False)\nrequirements_grid = requirements_grid.map(plt.hist, \"requirements_length\", bins=40)\n\n# Another option. Makes less obviuos which axes is to be labelled\n#requirements_grid.set_axis_labels(\"Requirement Length\", \"Count\")\n\n# Flatten the axes. Create an iterator\naxes = requirements_grid.axes.flatten()\n\n# Title\naxes[0].set_title(\"Non Fraudulent (0)\", fontsize=14)\naxes[1].set_title(\"Fraudulent (1)\", fontsize=14)\n\n# Labels\naxes[0].set_ylabel(\"Count\", fontsize=14)\nfor ax in axes:\n    ax.set_xlabel(\"Requirements Text Length\", fontsize=14)","f6063a82":"sns.catplot(x=\"has_company_logo\", hue=\"fraudulent\", data=jobs, kind=\"count\", aspect=2, height=4);\n\nplt.xlabel(\"Company Logo\", fontsize=14)\nplt.xticks([0, 1], (\"Has\", \"Doesn't have\"), fontsize=12)\nplt.ylabel(\"Count\", fontsize=14);","ee022b78":"# Create a 1 by 2 figure and axes\nfig, axes = plt.subplots(1, 2, figsize=(18,8))\n\n# Plot a countplot on the first axes\nemploy = sns.countplot(x=jobs[\"employment_type\"].dropna(), hue=jobs[\"fraudulent\"], palette=\"Set1\", ax=axes[0])\naxes[0].set_xlabel(\"Employment Type\", fontsize=15)\naxes[0].set_ylabel(\"Count\", fontsize=15)\naxes[0].set_title(\"Employment Type Count\", fontsize=15)\naxes[0].legend(\"\")\n\n# Write the height of the bars on top\nfor p in employ.patches:\n    employ.annotate(\"{:.0f}\".format(p.get_height()), \n                        (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                        ha='center', va='center', fontsize=14, color='black', xytext=(0, 12),\n                        textcoords='offset points')\n\n#############################################################\n\n# Plot a countplot on the second axes\nemploy_zoom = sns.countplot(x=jobs[\"employment_type\"].dropna(), hue=jobs[\"fraudulent\"], palette=\"Set1\", ax=axes[1])\naxes[1].set_xlabel(\"Employment Type\", fontsize=15)\naxes[1].set_ylim((0, 1500))\naxes[1].set_ylabel(\"\")\naxes[1].set_title(\"Employment Type Count Zoom\", fontsize=15)\naxes[1].legend(title=\"Fraudulent\", title_fontsize=14, fontsize=12, bbox_to_anchor=(1.2, 0.6));","ac466d77":"jobs.columns","57a707a5":"X1_profile = jobs[\"company_profile\"]\ny1 = jobs[\"fraudulent\"]\nX1_profile_train, X1_profile_test, y1_train, y1_test = train_test_split(X1_profile, y1, test_size=0.2, random_state=42)","84a644b5":"def text_process(text):\n    # Remove the punctuation\n    nopunc = [char for char in text if char not in string.punctuation]\n    \n    # Join the list of characters to form strings\n    nopunc = \"\".join(nopunc)\n    \n    # Remove stopwords\n    return [word for word in nopunc.split() if word.lower() not in stopwords.words(\"english\")]","413367b6":"NB_pipeline = Pipeline([(\"bow no func\", CountVectorizer()),\n                       (\"NB_classifier\", MultinomialNB())])","5b0670fc":"NB_pipeline.fit(X1_profile_train, y1_train)","b975a3cc":"NB_pred = NB_pipeline.predict(X1_profile_test)","be20b454":"print(classification_report(y1_test, NB_pred))","e1f763ab":"print(confusion_matrix(y1_test, NB_pred))","8e47c47e":"NB_func_pipeline = Pipeline([(\"bow with func\", CountVectorizer(analyzer=text_process)),\n                            (\"NB_classifier\", MultinomialNB())])","7df51c26":"X2_profile = jobs[\"company_profile\"]\ny2 = jobs[\"fraudulent\"]\nX2_profile_train, X2_profile_test, y2_train, y2_test = train_test_split(X2_profile, y2, test_size=0.2, random_state=42)","fc090c36":"NB_func_pipeline.fit(X2_profile_train, y2_train)","127e8683":"NB_func_pred = NB_func_pipeline.predict(X2_profile_test)","1319c676":"print(classification_report(y2_test, NB_func_pred))","97ca8c62":"print(confusion_matrix(y2_test, NB_func_pred))","cc0fa833":"NB_tfidf_pipeline = Pipeline([(\"bow no func\", CountVectorizer()),\n                              (\"tfidf\", TfidfTransformer()),\n                              (\"NB_classifier\", MultinomialNB())])","691b6beb":"X3_profile = jobs[\"company_profile\"]\ny3 = jobs[\"fraudulent\"]\nX3_profile_train, X3_profile_test, y3_train, y3_test = train_test_split(X3_profile, y3, test_size=0.2, random_state=42)","715f8bf0":"NB_tfidf_pipeline.fit(X3_profile_train, y3_train)","a3cf4cc4":"NB_tfidf_pred = NB_tfidf_pipeline.predict(X3_profile_test)","bddb5750":"print(classification_report(y3_test, NB_tfidf_pred))","be87a49e":"print(confusion_matrix(y3_test, NB_tfidf_pred))","450d3dae":"NB_func_tfidf_pipeline = Pipeline([(\"bow with func\", CountVectorizer(analyzer=text_process)),\n                              (\"tfidf\", TfidfTransformer()),\n                              (\"NB_classifier\", MultinomialNB())])","ccfec302":"X4_profile = jobs[\"company_profile\"]\ny4 = jobs[\"fraudulent\"]\nX4_profile_train, X4_profile_test, y4_train, y4_test = train_test_split(X4_profile, y4, test_size=0.2, random_state=42)","7bc29be0":"NB_func_tfidf_pipeline.fit(X4_profile_train, y4_train)","653a1611":"NB_func_tfidf_pred = NB_func_tfidf_pipeline.predict(X4_profile_test)","f8b4adb8":"print(classification_report(y4_test, NB_func_tfidf_pred))","b9f5e061":"print(confusion_matrix(y4_test, NB_func_tfidf_pred))","b0e16dd5":" > **The post being either real or fake, the majority of the job posts collected in this dataset do not have a company logo. This suggests that not having a logo is not a determining factor.**","1016fc33":" > **Apparently, the text length of the columns above does not show a different pattern for fraudulent or non-fraudulent posts. Let's take a closer look in each one individually.**","a535f5b0":"## 3.2.3 - No analyser and with TFIDF","477e6528":"# 2. Exploratory Analysis","11d8f698":"## 3.2.4 - With analyser and TFIDF","56f4991a":"## 3.2.1 - No analyser and no TFIDF","8d62a1d9":" > **The graph on the left shows the employment count for each employment type. The graph on the right shows the same information but it is zoomed in (see the difference in the y-axis values). Most of the jobs are full-time and in the first graph we can hardly see some fraudulent posts on the part-time type, not being able to see any fraudulent ads at all on the other employment types. When zoomed in we can see more fraudulent posts in all types. The fake percentage is:**\n> - **Part-time: ~ 9%**\n> - **Other: ~ 7%**\n> - **Full-time: ~ 4%**\n> - **Contract: ~ 3%**\n> - **Temporary: ~ 1%**","7ce65856":"***","1f4ef087":"# Real or Fake Job Posting Analysis and Prediction\n\n \nThe dataset contains 18 columns:\n - **job_id**: _Unique Job ID._\n - **title**: _The title of the job ad entry._\n - **location**: _Geographical location of the job ad._\n - **department**: _Corporate department (e.g. sales)._\n - **salary_range**: _Indicative salary range (e.g. 50.000-60.000)._\n - **company_profile**: _A brief company description._\n - **description**: _The details description of the job ad._\n - **requirements**: _Enlisted requirements for the job opening._\n - **benefits**: _Enlisted offered benefits by the employer._\n - **telecommuting**: _True for telecommuting positions._\n - **has_company_logo**: _True if company logo is present._\n - **has_questions**: _True if screening questions are present._\n - **employment_type**: _Full-time, Part-time, Contract, etc._\n - **required_experience**: _Executive, Entry, Intern, etc._\n - **required_education**: _Doctorate, Master's Degree, Bachelor, etc._\n - **industry**: _Automotive, IT, Health Care, etc._\n - **function**: _Consulting, Engineering, Research, etc._\n - **fraudulent**: _Target. 1 if fake, 0 if real._\n \nThe questions I am going to answer are:\n\n    1. Is there a better feature to predict if a job post is fake?\n    2. Is there a different text length pattern for fraudulent job posts or non-fraudulent ones?\n    3. How important is the company profile in real job posts?\n    \n#### Below is the sequence I will be following:\n    1. Reading and Understanding the Data\n    2. Exploratory analysis\n         -> Missing data\n         -> Data types in the dataframe\n         -> Visualization to check for patters and insights\n    3. Machine Learning Model\n        \n## Important note\n > **This notebook is intended exclusively to practicing and learning purposes. Any corrections, comments and suggestions are more than welcome and I would really appreciate it. Feel free to get in touch if you liked it or if you want to colaborate somehow.**","d85acaba":"# 3. Machine Learning Model","047b5d4d":" > **The best model was the one in which the text was processed and TFIDF was taken into account, presenting a 99% precision for real job posts and 92% precision for fake job posts. The text_process function basically removes any punctuation or stopwords. There are other things to try. Other classification models, e.g., logistic regression, K-Nearest Neighbors, etc can be tested. Overall, the MultinomialNB returned an excellent result.**","4c72df43":"***","63d543e5":"***","5ee3a79d":"# 1. Reading and Understanding the Data","3f628863":"## 3.2.2 - With analyser and no TFIDF","41fe8b2b":"### I will try some different combinations of techniques to process the data and check which one returns a better result. For now I will only use the MultinomialNB.","5c85baa9":" > **The description and requirements columns present a very similar behaviour for the text length. That means that trying to use this features to predict if the job is fake or not probably would not work. In the case of the company_profile column, the two types of job post behave quite differently. The vast majority of the fraudulent job posts do not have a company profile available.**","6363077e":"# 2.1 Visualizations","c94f458f":"---\n## 3.1 Train Test Split","eedb2368":"---\n## 3.2 Pipelines"}}