{"cell_type":{"9f279ec9":"code","4ca04065":"code","bb3ea46b":"code","5390dcd1":"code","5f0a0771":"code","158c688b":"code","c1191f1f":"code","120a0948":"code","5ca0f771":"code","0f9ce04b":"code","90d0a373":"code","01e9128b":"code","8c400814":"code","8ede3ec1":"code","6d604b13":"code","c3b3dfad":"code","3ead985f":"code","69b891ab":"code","28a42c61":"code","52773883":"code","29b64320":"code","04d56a49":"code","50962b58":"code","719e9030":"code","cf303340":"code","c1c5c48e":"code","dd86f2ca":"code","4537dfd1":"markdown","da298f76":"markdown","8f65f9c1":"markdown","9a8aa6aa":"markdown","bd4e7302":"markdown","3450f065":"markdown","cf8089d0":"markdown","fa84bb36":"markdown","597eec32":"markdown","ac47b893":"markdown","9681cf2e":"markdown"},"source":{"9f279ec9":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","4ca04065":"df = pd.read_csv('..\/input\/google-play-store-apps\/googleplaystore_user_reviews.csv')\ndf.head()","bb3ea46b":"df.info()","5390dcd1":"# I decide to drop the missing value since it's way to much\n\ndf.dropna(inplace=True)\ndf.info()","5f0a0771":"# Type of Sentiment\n\ndf.Sentiment.unique()","158c688b":"## Best App\n\nplt.figure(figsize=(10,7))\nbest_app = df.App[df['Sentiment'] == 'Positive'].value_counts().head(10)\nsns.barplot(x=best_app, y=best_app.index, data=df)","c1191f1f":"# Worst App\n\nplt.figure(figsize=(10,7))\nbest_app = df.App[df['Sentiment'] == 'Negative'].value_counts().head(10)\nsns.barplot(x=best_app, y=best_app.index, data=df)","120a0948":"## Handling Categorical\n\ndf['Sentiment'].replace(to_replace=['Positive','Negative','Neutral'], value=['1','0','2'],inplace=True)","5ca0f771":"## Spliting Data Feature & Target\n\nX = df.Translated_Review\ny = df.Sentiment","0f9ce04b":"## PREPROCESSING\n\nimport re\n\ndef clean_text(text):\n    # lowerxase\n    text = text.lower()\n    # Clear punctuation\n    text = re.sub('\\[.*?\\]','',text)\n    return text\n\nclean = lambda x: clean_text(x)\n\nX = X.apply(clean)\nX","90d0a373":"## REMOVE STOPWORDS\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nen_stops = set(stopwords.words('english'))\n\ndef stopwords(text):\n\n    tokens = word_tokenize(text)\n    filtered = []    \n    for w in tokens:\n        if w not in en_stops:\n            filtered.append(w)\n    result = ' '.join(filtered)\n    return result\n\nst = lambda x: stopwords(x)\n\nX = X.apply(st)\nX","01e9128b":"## STEMMING\n\nfrom nltk.stem import PorterStemmer\n\ndef stemming(text):\n    ps = PorterStemmer()\n    text = ps.stem(text)\n    return text\n\nstem = lambda x: stemming(x)\n\nX = X.apply(stem)\nX","8c400814":"# I split the data with train test split\nfrom sklearn.model_selection import train_test_split\n\n# And transform the data with TF-IDF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(use_idf=True, lowercase=True, strip_accents='ascii')\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Transform the feature data to vector before handling imbalance dataset with undersample\n# X Train Vector\nXTV = vectorizer.fit_transform(X_train)\n# X Test Vector\nXTSV = vectorizer.transform(X_test)","8ede3ec1":"# Data Distribution before applying undersampling\n\ny_train.value_counts()","6d604b13":"## Near Miss Undersampling\n\nfrom imblearn.under_sampling import NearMiss","c3b3dfad":"### NearMiss-1:\n###   Majority class examples with minimum average distance to three closest minority class examples.\n\nundersample = NearMiss(version=1, n_neighbors=3)\nXTM1, YTM1 = undersample.fit_resample(XTV, y_train)\n\n# Check value distribution\nYTM1.value_counts()","3ead985f":"### NearMiss-2:\n###    Majority class examples with minimum average distance to three furthest minority class examples.\n\nundersample = NearMiss(version=2, n_neighbors=3)\nXTM2, YTM2 = undersample.fit_resample(XTV, y_train)\n\n# Check value distribution\nYTM2.value_counts()","69b891ab":"### NearMiss-3:\n###    Majority class examples with minimum distance to each minority class example.\n\nundersample = NearMiss(version=3, n_neighbors_ver3=3)\nXTM3, YTM3 = undersample.fit_resample(XTV, y_train)\n\n# Check value distribution\nYTM3.value_counts()","28a42c61":"### Undersample with Tomek Links\nfrom imblearn.under_sampling import TomekLinks\n\nundersample = TomekLinks()\nXTTL, YTTL = undersample.fit_resample(XTV, y_train)\n\n# Check value distribution\nYTTL.value_counts()","52773883":"### Undersample with ENN\n\nfrom imblearn.under_sampling import EditedNearestNeighbours\n\nundersample = EditedNearestNeighbours(n_neighbors=3)\nXTENN, YTENN = undersample.fit_resample(XTV, y_train)\n\n# Check value distribution\nYTENN.value_counts()","29b64320":"### Undersample with OSS\n\nfrom imblearn.under_sampling import OneSidedSelection\n\nundersample = OneSidedSelection(n_neighbors=1, n_seeds_S=200)\nXTOSS, YTOSS = undersample.fit_resample(XTV, y_train)\n\n# Check value distribution\nYTOSS.value_counts()","04d56a49":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nimport xgboost as xgb\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report","50962b58":"## MultinomialNB\nmnb = MultinomialNB()\n\n### Near Miss 1\nmnb.fit(XTM1, YTM1)\npred = mnb.predict(XTSV)\nactual = np.array(y_test)\nMNB_NM1 = accuracy_score(actual, pred)\n\nprint('Near Miss 1')\nprint('Accuracy Score :', MNB_NM1)\nprint('Report : ')\nprint(classification_report(actual, pred))\n\n### Near Miss 2\nmnb.fit(XTM2, YTM2)\npred = mnb.predict(XTSV)\nactual = np.array(y_test)\nMNB_NM2 = accuracy_score(actual, pred)\n\nprint('Near Miss 2')\nprint('Accuracy Score :', MNB_NM2)\nprint('Report : ')\nprint(classification_report(actual, pred))\n\n### Near Miss 3\nmnb.fit(XTM3, YTM3)\npred = mnb.predict(XTSV)\nactual = np.array(y_test)\nMNB_NM3 = accuracy_score(actual, pred)\n\nprint('Near Miss 3')\nprint('Accuracy Score :', MNB_NM3)\nprint('Report : ')\nprint(classification_report(actual, pred))\n\n### Tomek Links\nmnb.fit(XTTL, YTTL)\npred = mnb.predict(XTSV)\nactual = np.array(y_test)\nMNB_TL = accuracy_score(actual, pred)\n\nprint('Tomek Links')\nprint('Accuracy Score :', MNB_TL)\nprint('Report : ')\nprint(classification_report(actual, pred))\n\n### Edited Nearest Neighbour\nmnb.fit(XTENN, YTENN)\npred = mnb.predict(XTSV)\nactual = np.array(y_test)\nMNB_ENN = accuracy_score(actual, pred)\n\nprint('Edited Nearest Neighbour')\nprint('Accuracy Score :', MNB_ENN)\nprint('Report : ')\nprint(classification_report(actual, pred))\n\n### One Side Selection\nmnb.fit(XTOSS, YTOSS)\npred = mnb.predict(XTSV)\nactual = np.array(y_test)\nMNB_OSS = accuracy_score(actual, pred)\n\nprint('One Side Selection')\nprint('Accuracy Score :', MNB_OSS)\nprint('Report : ')\nprint(classification_report(actual, pred))","719e9030":"svc = SVC(kernel='rbf', C=1000, gamma=0.001)\n\n### Near Miss 1\nsvc.fit(XTM1, YTM1)\npred = svc.predict(XTSV)\nactual = np.array(y_test)\nSVC_NM1 = accuracy_score(actual, pred)\n\nprint('Near Miss 1')\nprint('Accuracy Score :', SVC_NM1)\nprint('Report : ')\nprint(classification_report(actual, pred))\n\n### Near Miss 2\nsvc.fit(XTM2, YTM2)\npred = svc.predict(XTSV)\nactual = np.array(y_test)\nSVC_NM2 = accuracy_score(actual, pred)\n\nprint('Near Miss 2')\nprint('Accuracy Score :', SVC_NM2)\nprint('Report : ')\nprint(classification_report(actual, pred))\n\n### Near Miss 3\nsvc.fit(XTM3, YTM3)\npred = svc.predict(XTSV)\nactual = np.array(y_test)\nSVC_NM3 = accuracy_score(actual, pred)\n\nprint('Near Miss 3')\nprint('Accuracy Score :', SVC_NM3)\nprint('Report : ')\nprint(classification_report(actual, pred))\n\n### Tomek Links\nsvc.fit(XTTL, YTTL)\npred = svc.predict(XTSV)\nactual = np.array(y_test)\nSVC_TL = accuracy_score(actual, pred)\n\nprint('Tomek Links')\nprint('Accuracy Score :', SVC_TL)\nprint('Report : ')\nprint(classification_report(actual, pred))\n\n### Edited Nearest Neighbour\nsvc.fit(XTENN, YTENN)\npred = svc.predict(XTSV)\nactual = np.array(y_test)\nSVC_ENN = accuracy_score(actual, pred)\n\nprint('Edited Nearest Neighbour')\nprint('Accuracy Score :', SVC_ENN)\nprint('Report : ')\nprint(classification_report(actual, pred))\n\n### One Side Selection\nsvc.fit(XTOSS, YTOSS)\npred = svc.predict(XTSV)\nactual = np.array(y_test)\nSVC_OSS = accuracy_score(actual, pred)\n\nprint('One Side Selection')\nprint('Accuracy Score :', SVC_OSS)\nprint('Report : ')\nprint(classification_report(actual, pred))","cf303340":"### XGBoost\n\nXGB = xgb.XGBClassifier(objective='multi:softmax', num_class=3, n_estimators=150, seed=123)\n\n### Near Miss 1\nXGB.fit(XTM1, YTM1)\npred = XGB.predict(XTSV)\nactual = np.array(y_test)\nXGB_NM1 = accuracy_score(actual, pred)\n\nprint('Near Miss 1')\nprint('Accuracy Score :', XGB_NM1)\nprint('Report : ')\nprint(classification_report(actual, pred))\n\n### Near Miss 2\nXGB.fit(XTM2, YTM2)\npred = XGB.predict(XTSV)\nactual = np.array(y_test)\nXGB_NM2 = accuracy_score(actual, pred)\n\nprint('Near Miss 2')\nprint('Accuracy Score :', XGB_NM2)\nprint('Report : ')\nprint(classification_report(actual, pred))\n\n### Near Miss 3\nXGB.fit(XTM3, YTM3)\npred = XGB.predict(XTSV)\nactual = np.array(y_test)\nXGB_NM3 = accuracy_score(actual, pred)\n\nprint('Near Miss 3')\nprint('Accuracy Score :', XGB_NM3)\nprint('Report : ')\nprint(classification_report(actual, pred))\n\n### Tomek Links\nXGB.fit(XTTL, YTTL)\npred = XGB.predict(XTSV)\nactual = np.array(y_test)\nXGB_TL = accuracy_score(actual, pred)\n\nprint('Tomek Links')\nprint('Accuracy Score :', XGB_TL)\nprint('Report : ')\nprint(classification_report(actual, pred))\n\n### Edited Nearest Neighbour\nXGB.fit(XTENN, YTENN)\npred = XGB.predict(XTSV)\nactual = np.array(y_test)\nXGB_ENN = accuracy_score(actual, pred)\n\nprint('Edited Nearest Neighbour')\nprint('Accuracy Score :', XGB_ENN)\nprint('Report : ')\nprint(classification_report(actual, pred))\n\n### One Side Selection\nXGB.fit(XTOSS, YTOSS)\npred = XGB.predict(XTSV)\nactual = np.array(y_test)\nXGB_OSS = accuracy_score(actual, pred)\n\nprint('One Side Selection')\nprint('Accuracy Score :', XGB_OSS)\nprint('Report : ')\nprint(classification_report(actual, pred))","c1c5c48e":"x = {'Undersample Method'  : ['Near Miss 1','Near Miss 2','Near Miss 3','Tomek Links',\n                             'Edited Nearest Neighbour','One Side Selection'],     \n     'MultinomialNB'      : [MNB_NM1, MNB_NM2, MNB_NM3, MNB_TL, MNB_ENN, MNB_OSS],\n     'SVM'                : [SVC_NM1, SVC_NM2, SVC_NM3, SVC_TL, SVC_ENN, SVC_OSS],\n     'XGBoost'            : [XGB_NM1, XGB_NM2, XGB_NM3, XGB_TL, XGB_ENN, XGB_OSS]}","dd86f2ca":"result = pd.DataFrame(x)\nresult","4537dfd1":"## Quick Look","da298f76":"## Split And Transform Data","8f65f9c1":"## Text Preprocessing\n\nIn text preproccessing, i just want to apply some standard way of preprocessing in sentiment analysis, in order to make the model understand it better. These step are lowercase, clear punctuation, stopword, and stemming. You can experience with any other step to see what combination might lead to a better result.","9a8aa6aa":"## Modeling\n\nAfter performing undersample to imbalance dataset, time to build the models. Here i'm going to compare each of those undersample method with a few algorithm in text analysis such as Multinomial Naive Bayes, SVM, and XGBoost.","bd4e7302":"### SVM","3450f065":"## Import Modules","cf8089d0":"## Introduction\n\nHi, in this kernel i will cover basic sentiment analysis, handling imbalance dataset with undersample method, and build predictive models. And this time we will be using Google Play Store Apps Dataset.","fa84bb36":"### Multinomial NB","597eec32":"## Undersample\n\nJust as i mentioned before, i'm going to use a few of undersample method. Those are Near Miss, TomekLinks, Edited Nearest Neighbour, and One Sided Selection. I'm only using libraries form imblearn undersample, for more information, check this link.\n\nhttps:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/under_sampling.html","ac47b893":"### XGBoost","9681cf2e":"## Summary\n\nThe best result is SVM with One Side Selection as the undersampling method."}}