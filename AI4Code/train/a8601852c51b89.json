{"cell_type":{"ed9a16f5":"code","e514c66d":"code","36b2a7fc":"code","3a45bff7":"code","7e514289":"code","d2336f93":"code","b117f8a9":"code","550c638e":"code","5ae2c109":"code","13b7c303":"code","c962b6d4":"code","7956c018":"code","360ac1d9":"code","08afd45c":"code","e486a7dd":"markdown","4d8a12bc":"markdown","cca7f348":"markdown","a16c9a61":"markdown","0afe5a67":"markdown","55267367":"markdown","00db3f35":"markdown","5910e43c":"markdown","2653283a":"markdown","cd4c4033":"markdown"},"source":{"ed9a16f5":"%%time\nimport os\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport numpy as np\nimport gc\nimport tensorflow as tf\nimport time\nt0 = time.time()\n\n## this script transports l5kit and dependencies\nos.system('pip uninstall typing -y')\nos.system('pip install --target=\/kaggle\/working pymap3d==2.1.0')\nos.system('pip install --target=\/kaggle\/working protobuf==3.12.2')\nos.system('pip install --target=\/kaggle\/working transforms3d')\nos.system('pip install --target=\/kaggle\/working zarr')\nos.system('pip install --target=\/kaggle\/working ptable')\n\nos.system('pip install --no-dependencies --target=\/kaggle\/working l5kit')\n#!pip install --upgrade pip\n#!pip install pymap3d==2.1.0\n#!pip install -U l5kit","e514c66d":"DIR_INPUT = \"\/kaggle\/input\/lyft-motion-prediction-autonomous-vehicles\"\nimport os\nos.environ[\"L5KIT_DATA_FOLDER\"] = DIR_INPUT\nSINGLE_MODE_SUBMISSION = f\"{DIR_INPUT}\/single_mode_sample_submission.csv\"\nMULTI_MODE_SUBMISSION = f\"{DIR_INPUT}\/multi_mode_sample_submission.csv\"\nfrom l5kit.data import LocalDataManager, ChunkedDataset\nfrom l5kit.dataset import AgentDataset, EgoDataset\nfrom l5kit.evaluation import write_pred_csv\nfrom l5kit.rasterization import build_rasterizer","36b2a7fc":"DEBUG = False  # True just trains for 10 steps instead of the full dataset\ncfg = {\n    'format_version': 4,\n    'model_params': {\n        'model_architecture': 'resnet50',\n        'history_num_frames': 20,\n        'history_step_size': 1,\n        'history_delta_time': 0.1,\n        'future_num_frames': 50,\n        'future_step_size': 1,\n        'future_delta_time': 0.1\n    },\n    \n    'raster_params': {\n        'raster_size': [224, 224],\n        'pixel_size': [0.5, 0.5],\n        'ego_center': [0.25, 0.5],\n        'map_type': 'py_semantic',\n        'satellite_map_key': 'aerial_map\/aerial_map.png',\n        'semantic_map_key': 'semantic_map\/semantic_map.pb',\n        'dataset_meta_key': 'meta.json',\n        'filter_agents_threshold': 0.5\n    },\n    \n    'train_data_loader': {\n        'key': 'scenes\/train.zarr',\n        'batch_size': 12,\n        'shuffle': True,\n        'num_workers': 4\n    },\n    \n    'train_params': {\n        'max_num_steps': 10*16 if DEBUG else 3200,\n        'checkpoint_every_n_steps': 5000,\n        'train_batch' : 32,\n        'num_batch' : 10\n        \n        # 'eval_every_n_steps': -1\n    },\n    \n    'test_data_loader': {\n        'key': 'scenes\/test.zarr',\n        'batch_size': 8,\n        'shuffle': False,\n        'num_workers': 4\n    },\n    \n    \n    \n    'valid_data_loader': {\n        'key': 'scenes\/validate.zarr',\n        'batch_size': 8,\n        'shuffle': False,\n        'num_workers': 4\n    },\n    \n    \n}","3a45bff7":"\ntrain_cfg = cfg[\"train_data_loader\"]\n\n# Rasterizer\ndm = LocalDataManager(None)\nrasterizer = build_rasterizer(cfg, dm)\n\n# Train dataset\/dataloader\n\ntrain_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\ntrain_dataset = AgentDataset(cfg, train_zarr, rasterizer)\nhist_shape = train_dataset[0]['history_positions'].shape\nnum_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\nnum_in_channels = 3 + num_history_channels\nnum_targets = 2 * cfg[\"model_params\"][\"future_num_frames\"]\n\nprint(train_dataset)\n\n\ngc.collect()","7e514289":"dataset_path1 = dm.require(cfg[\"valid_data_loader\"][\"key\"])\nvalid_zarr = ChunkedDataset(dataset_path1).open()\nvalid_dataset = AgentDataset(cfg, valid_zarr, rasterizer)\n\n\n","d2336f93":"valid_itr = iter(valid_dataset)\nn_valid = 1500\n\nval_inputs = np.zeros(shape=(n_valid,224,224, num_in_channels) )\nval_targets = np.zeros(shape=(n_valid,num_targets))\nfor itr in tqdm(range(n_valid)):\n    data = next(valid_itr)\n\n    val_inputs[itr] = data['image'].transpose(1,2,0)    \n    val_targets[itr] = data['target_positions'].reshape(-1,num_targets)\n    gc.collect()\ndel valid_dataset\n    ","b117f8a9":"idx = 100\nplt.scatter(train_dataset[idx]['history_positions'][:,0],train_dataset[idx]['history_positions'][:,1])\nplt.scatter(train_dataset[idx]['target_positions'][:,0],train_dataset[idx]['target_positions'][:,1],c='r')\nplt.show()\nprint(train_dataset[0]['target_positions'].shape) ","550c638e":"from keras.applications.resnet50 import ResNet50\nfrom keras.utils.conv_utils import convert_kernel\nfrom keras.layers import (Input, Conv2D, Flatten,Dense,AveragePooling2D,Dropout,MaxPooling2D,BatchNormalization)\nfrom keras.models import Model, Sequential\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras import optimizers\n\n# detect and init the TPU\n#tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n#tf.config.experimental_connect_to_cluster(tpu)\n#tf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\n#tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# instantiating the model in the strategy scope creates the model on the TPU\n#with tpu_strategy.scope():\n    \nbase_in = Input(shape=(224,224,num_in_channels))\nbase_model=Conv2D(20,kernel_size=1,use_bias=False,padding=\"same\")(base_in)\nbase_model=Conv2D(3,kernel_size=3,use_bias=False,padding=\"same\")(base_model)\n\n\nbase_model = ResNet50(include_top=False,\n                      weights= 'imagenet',\n                      input_tensor= Input(shape = (224,224,3)),\n                      pooling='max'\n                ) (base_model)\n\ndense_model = Dense(1000, activation=\"linear\")(base_model)\ndense_model = Dropout(.25)(dense_model)\ndense_model = Dense(500, activation=\"linear\")(dense_model)\ndense_model = Dropout(.25)(dense_model)\ndense_model = Dense(num_targets, activation=\"linear\")(dense_model)\n\nmodel = Model(inputs=base_in, outputs=dense_model)\nopt = optimizers.Adam(lr=0.002)\nmodel.compile(optimizer=opt, loss='mse')\n\nmodel.summary()","5ae2c109":"import gc\nMC = ModelCheckpoint('.\/model.h5', verbose=False,monitor='val_loss',mode='min',\n        save_weights_only=True,save_best_only=True)\n\nstop = EarlyStopping(monitor = 'val_loss', restore_best_weights=True , patience = 5)\n\ntr_it = iter(train_dataset)\nbatch_size= 16 * tpu_strategy.num_replicas_in_sync\n#batch_size = cfg['train_params']['train_batch']\n#progress_bar = tqdm(range(0,cfg[\"train_params\"][\"max_num_steps\"],batch_size))\nprogress_bar = tqdm(range(cfg[\"train_params\"][\"max_num_steps\"]))\nfour_hours = 60 * 60 * 4\nlosses = []\nhist = []\nfor itr in progress_bar:#range(0,cfg[\"train_params\"][\"max_num_steps\"],batch_size):\n    inputs = np.zeros(shape=(batch_size,224,224,num_in_channels))\n    targets = np.zeros(shape=(batch_size, num_targets))\n    \n    for i in range(batch_size):\n        \n        try:\n            data = next(tr_it)\n        except StopIteration:\n            tr_it = iter(train_dataset)\n            data = next(tr_it)\n            \n        inputs[i] = data['image'].transpose(1,2,0)\n        targets[i] = data['target_positions'].reshape(-1,num_targets)\n   \n    h = model.fit(inputs, targets,\n                  batch_size = batch_size ,\n                  validation_data = (val_inputs, val_targets),\n                  verbose = 0,\n                 callbacks = [MC, stop])\n                  \n    hist.append(h.history)\n    gc.collect()\n    # For training + submission, break if training exceeds 6 hours\n    if (time.time()-t0) > four_hours:\n        print('TimeOut')\n        break\n    \n    \nvl = [hi['val_loss'] for hi in hist]\nl = [hi['loss'] for hi in hist]\nplt.plot(np.log(vl), label = 'val_loss')\nplt.plot(np.log(l), label = 'loss')\nplt.legend(loc=0)\nplt.show()","13b7c303":"model.save('modelv0.h5')\n","c962b6d4":"#Example Prediction:\nimport matplotlib.pyplot as plt\na1 = next(tr_it)\ninp = a1['image'].transpose(1,2,0)\nact = a1['target_positions']\npred = model.predict(inp.reshape(-1,224,224,num_in_channels)).reshape(50,2)\nplt.scatter(act[:,0], act[:,1])\nplt.scatter(pred[:,0],pred[:,1])","7956c018":"test_cfg = cfg[\"test_data_loader\"]\n\n# Rasterizer\nrasterizer = build_rasterizer(cfg, dm)\n\n# Test dataset\/dataloader\ntest_zarr = ChunkedDataset(dm.require(test_cfg[\"key\"])).open()\ntest_mask = np.load(f\"{DIR_INPUT}\/scenes\/mask.npz\")[\"arr_0\"]\ntest_dataset = AgentDataset(cfg, test_zarr, rasterizer, agents_mask=test_mask)","360ac1d9":"t_shape = test_dataset[0][\"target_positions\"].shape\ntimestamps = []\nagent_ids = []\ncoords = []\nfor it in tqdm(test_dataset):\n    \n    dat = it['image'].transpose(1,2,0)\n    coords.append(np.array(model.predict(dat.reshape(1,224,224,num_in_channels)).reshape(t_shape)))\n    timestamps.append(it[\"timestamp\"])\n    agent_ids.append(it[\"track_id\"])\n    \n    ","08afd45c":"from l5kit.evaluation import write_pred_csv\n\n\nwrite_pred_csv('submission.csv',\n                timestamps = np.array(timestamps),\n                track_ids = np.array(agent_ids),\n                coords = np.array(coords) )","e486a7dd":"Now we can use this model to predict from the test set.","4d8a12bc":"Importing some stuff from the l5kit and setting the directories.","cca7f348":"I am going to loop through the train_dataset and use a batch_size variable to train the model in batches.  ","a16c9a61":"v3: Learned How to Attach the proper input to the ResNet model.  So now I can preload imagenet weights\n\nv2: Fixed bug in creating submission where I was using the same timestamp and agent_id for all test shots\n\n\nv1: add predictions.  Try 10000 * 16 training images with a 1\/8 validation.  SGD and mse loss.  Turning GPU on.\n\nv0: ResNet50 with random weights, train on 20,000 samples.  Timed out on gpu.  Got ~65% through training.","0afe5a67":"I will play with some parameters later.  I am finally glad to just have something that functions.","55267367":"**Keras Starter.\n**\nThank you to @pestipeti who did the heavy lifting at https:\/\/www.kaggle.com\/pestipeti\/pytorch-baseline-train \n\nI am newer to deep learning models and have been trying to learn Keras.  Please let me know of any blatent issues!","00db3f35":"Start with some imports and the utility script from the comments","5910e43c":"Config File (dictionary)  from the linked notebook.  This contains parameters for the training.","2653283a":"Load In the train dataset.  I notice the pytorch folks can just import this with DataLoader, but I am not familiar with anything similar in keras.","cd4c4033":"Define the model architecture.\n\nGiven what the examples use, lets start with ResNet50.  I tried to initialize with imagenet weights, but I get dimension mismatches, so that is something to look into.\n\nThe avg pooling layer after ResNet50 will be 2048, so I add in some Dense+ BN + dropout layers, with the last one being 2xnum_targets units.\n\nFirst run will just use sgd optimizer with mse loss."}}