{"cell_type":{"27237650":"code","36799f25":"code","46ea1db0":"code","cdc7d279":"code","1dcd9fcf":"code","a4ba8f3d":"code","01594874":"code","553bd69e":"code","f43c7e8d":"code","0f7f4eed":"code","ac795285":"code","84d5828d":"code","b3c01f99":"code","678843c0":"code","9f9a5592":"code","2cc5f975":"code","11452de4":"code","8166bebd":"code","98db90dd":"code","4a6ce100":"code","11452060":"code","42569d78":"code","fd50725a":"code","c7a01486":"code","a3b1ba57":"code","cafddfa5":"code","27dca3ea":"code","b8056373":"code","9237d097":"code","68590935":"code","663c6ed5":"code","c23c6fed":"code","6bcfa220":"code","51aff985":"code","2fe34a92":"code","a10dc75e":"code","1a285ed6":"code","775f4947":"code","723cb710":"code","26ed6d79":"code","5004eafc":"code","fdcffc59":"code","4f5b27af":"code","95c2ed93":"code","ab304df7":"code","0b15be26":"code","832b6177":"code","9ac9f3aa":"code","8b68b084":"code","841df33b":"code","2c4d8d7c":"code","291f720e":"code","80332deb":"code","2caf95b7":"code","b666e2d1":"code","0eed5e21":"code","65672543":"code","9c962b45":"code","286beae8":"code","127cbae5":"code","a3d4d79e":"code","2f0ca175":"code","4f11bf58":"code","80b44260":"code","30880d4b":"code","faa4678d":"markdown","df7ac812":"markdown","49de8760":"markdown","9a90d38c":"markdown","7b96bc92":"markdown","712a65f3":"markdown","18f0ae6f":"markdown","27b2d052":"markdown","4cfc6364":"markdown","e3fbe6ea":"markdown","64576cd9":"markdown","7a5c5141":"markdown","9acda08e":"markdown","25fdee42":"markdown","8a481944":"markdown","8c17e3d7":"markdown","606514ad":"markdown","62851bd0":"markdown","4bfdd626":"markdown","9fb22f3a":"markdown","cbf0776b":"markdown","317a7c44":"markdown","d630c3d7":"markdown","714b5af6":"markdown","bef2c38f":"markdown","810b7ca8":"markdown","a534b208":"markdown","7a4266fe":"markdown","fb9854ad":"markdown","c7745f00":"markdown","4c173093":"markdown","da3ea7b6":"markdown","2bb0a584":"markdown","910032a4":"markdown","a8ca524f":"markdown","4cd74f49":"markdown","040c9e9a":"markdown","3c97a98a":"markdown","1ed02d8a":"markdown","a3a7d485":"markdown","3fab32ee":"markdown"},"source":{"27237650":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.preprocessing import StandardScaler","36799f25":"data = pd.read_csv(\"..\/input\/adult-census-income\/adult.csv\")\ndata","46ea1db0":"# Represents the number of rows and columns respectively.\ndata.shape","cdc7d279":"[(col, data[col].nunique(), data[col].dtype) for col in data.columns]","1dcd9fcf":"# fnlwgt contains very high amount of unique values. Rest of them seems fine.\ndata['fnlwgt'].nunique()\/data.shape[0]*100","a4ba8f3d":"data.describe()","01594874":"sns.distplot(x=data['age'], axlabel='Age', color='darkblue')","553bd69e":"# We should make groups of age such that each group contains significant amount of \n# information.\n\nprint(len(data[(data['age'] > 16) & (data['age'] <= 25)]))\nprint(len(data[(data['age'] > 25) & (data['age'] <= 32)]))\nprint(len(data[(data['age'] > 32) & (data['age'] <= 40)]))\nprint(len(data[(data['age'] > 40) & (data['age'] <= 50)]))\nprint(len(data[(data['age'] > 50)]))","f43c7e8d":"sns.distplot(x=data['fnlwgt'], axlabel='fnlwgt', color='darkblue');","0f7f4eed":"sns.distplot(x = data['education.num'], axlabel='Education Number', color = 'darkblue')","ac795285":"sns.distplot(x = data['capital.gain'], axlabel='Capital Gain', color = 'darkblue')","84d5828d":"sns.distplot(x = data['capital.loss'], axlabel='Capital Loss', color = 'darkblue')","b3c01f99":"sns.distplot(x = data['hours.per.week'], axlabel='Numbers of Hours per Week', color = 'darkblue')","678843c0":"numerical_var = [col for col in data.columns if data[col].dtype != object]\ncategorical_var = [col for col in data.columns if data[col].dtype == object]\nprint(numerical_var)\nprint(categorical_var)","9f9a5592":"data['income'] = data['income'].apply(lambda x:x.replace(\"<=50K\", \"0\"))\ndata['income'] = data['income'].apply(lambda x:x.replace(\">50K\", \"1\"))\ndata['income'] = data['income'].astype(int)","2cc5f975":"sns.displot(x = data['income'])","11452de4":"categorical_var","8166bebd":"[(i, data[i].unique()) for i in categorical_var]","98db90dd":"sns.barplot(x = 'workclass', y = 'income', data = data)\nplt.xticks(rotation = 90);","4a6ce100":"# For vertical bars categorical variables should be on xaxis.\n# Hue is used to include another categorical variable in the graph.\nsns.barplot(x = data['education'], y = data['income'])\nplt.xticks(rotation = 90);","11452060":"# Not a single person attending only preschool have income <50k.\nlen(data[(data['education'] == 'Preschool') & (data['income'] == 0)])","42569d78":"sns.barplot(x = 'marital.status', y = 'income', data = data)\nplt.xticks(rotation = 90);","fd50725a":"plt.figure(figsize = (14,5))\n\nsns.countplot(x = 'marital.status', data = data);\nplt.xticks(rotation = 90);","c7a01486":"sns.barplot(x = 'relationship', y = 'income', data = data)\nplt.xticks(rotation = 90);","a3b1ba57":"plt.figure(figsize = (14,5))\n\nsns.countplot(x = 'relationship', data = data);\nplt.xticks(rotation = 90);","cafddfa5":"sns.barplot(x = 'occupation', y = 'income', data = data)\nplt.xticks(rotation = 90);","27dca3ea":"plt.figure(figsize = (14,5))\n\nsns.countplot(x = 'occupation', data = data);\nplt.xticks(rotation = 90);","b8056373":"sns.set(style = 'darkgrid', font_scale = 1)\nsns.barplot(x = 'sex', y = 'income', data = data)","9237d097":"sns.set(style = 'darkgrid', font_scale = 1.5)\nsns.catplot(x = 'sex', y = 'income', data = data, kind = 'bar', col = 'race',\n            height = 5, aspect = 1)","68590935":"plt.figure(figsize = (14,5))\n\nsns.countplot(x = 'race', data = data);\nplt.xticks(rotation = 90);","663c6ed5":"data['race'] = data['race'].apply(lambda x: 'Other' if x != 'White' else x)","c23c6fed":"sns.set(style = 'darkgrid', font_scale = 1)\nplt.figure(figsize = (14,5))\nsns.barplot(x = 'native.country', y = 'income', data = data);\nplt.xticks(rotation = 90);\nplt.title('Country-wise Income')","6bcfa220":"plt.figure(figsize = (14,5))\n\nsns.countplot(x = 'native.country', data = data);\nplt.xticks(rotation = 90);","51aff985":"# age\nfor i in data:\n    data.loc[(data['age'] > 16) & (data['age'] <= 25), 'age'] = 1\n    data.loc[(data['age'] > 25) & (data['age'] <= 32), 'age'] = 2\n    data.loc[(data['age'] > 32) & (data['age'] <= 40), 'age'] = 3\n    data.loc[(data['age'] > 40) & (data['age'] <= 50), 'age'] = 4\n    data.loc[data['age'] > 50, 'age'] = 5","2fe34a92":"# education\ndata['education'] = data['education'].apply(lambda x: 'School' if x == '11th' or x == '7th-8th' or x == '10th' \n                                              or x == '5th-6th' or x == '9th' or x == '12th' or x == '1st-4th' \n                                              or x == 'Preschool' else x)\ndata['education'] = data['education'].apply(lambda x: 'Associate' if x == 'Assoc-acdm' or x == 'Assoc-voc' else x)\neducation_map = {'School':1,\n             'HS-grad':2,\n             'Some-college':3,\n             'Bachelors':4,\n             'Prof-school':5,\n             'Associate':6,\n             'Masters':7,\n             'Doctorate':8}\ndata['education'] = data['education'].map(education_map)","a10dc75e":"# creating new feature\ndata['capital_diff'] = abs((data['capital.gain'] - data['capital.loss']))","1a285ed6":"# marital status\ndata['marital.status'] = data['marital.status'].apply(lambda x: 'Prev-Married' if (x == 'Widowed' or x == 'Divorced' or x == 'Separated') else x)\ndata['marital.status'] = data['marital.status'].apply(lambda x: 'Married' if (x == 'Married-civ-spouse' or x == 'Married-spouse-absent' or x == 'Married-AF-spouse') else x)","775f4947":"# workclass\ndata['workclass'] = data['workclass'].apply(lambda x: 'No income' if x == 'Never-worked' or x == 'Without-pay'\n                                            else x)","723cb710":"#Percentage of people with ? in every column.\nfor col in data.columns:\n    print((col, len(data[data[col] == '?'])\/len(data[col])*100))","26ed6d79":"# Converting ? to nan\ndata[data == '?'] = np.nan","5004eafc":"col_with_symbol = ['workclass', 'occupation', 'native.country']","fdcffc59":"# Dropping ?\n# for i in col_with_symbol:\n#     data = data.drop(data.loc[data[i] == '?'].index)","4f5b27af":"# Imputing ? with mode\nfor col in col_with_symbol:\n    data[col].fillna(data[col].mode()[0], inplace=True)","95c2ed93":"data[col_with_symbol].describe()","ab304df7":"data.columns","0b15be26":"# native country\ndata['native.country'] = data['native.country'].apply(lambda x: 'Other' if x != 'United-States' else x)","832b6177":"# handling skewness\n\ndata['capital.gain'] = np.log1p(data['capital.gain'])\ndata['capital.loss'] = np.log1p(data['capital.loss'])\n# data['capital_diff'] = np.log1p(data['capital_diff'])\ndata['fnlwgt'] = 1\/(data['fnlwgt'])\n# data['hours.per.week'] = (data['hours.per.week']).round(decimals = 4))","9ac9f3aa":"data.describe()","8b68b084":"data","841df33b":"plt.figure(figsize = (10, 5))\nsns.heatmap(data.corr(), annot = True)","2c4d8d7c":"y = data['income']\nX = data.drop('income', axis = 1)","291f720e":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 1, test_size = 0.2) ","80332deb":"category_var = [col for col in X.columns if X[col].dtypes == object]\ncategory_var","2caf95b7":"numeric_var = [col for col in X.columns if X[col].dtypes != object]\nnumeric_var","b666e2d1":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndef calc_vif(X):\n\n    # Calculating VIF\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n\n    return(vif)","0eed5e21":"dt = X[numeric_var]\ncalc_vif(dt)","65672543":"#Deleting education num\nX = X.drop('education.num', axis = 1)\n# X = X.drop('capital.gain', axis = 1)\nnumeric_var = [col for col in X.columns if X[col].dtypes != object]\nnumeric_var","9c962b45":"dt = X[numeric_var]\ncalc_vif(dt)","286beae8":"import category_encoders as ce\none_hot = ce.OneHotEncoder(cols = category_var, handle_unknown = 'ignore')\n\n# Creating dataframe for categorical variables which converted to one hot encoded variables.\nX_train_one_hot = pd.DataFrame(one_hot.fit_transform(X_train))\nX_test_one_hot = pd.DataFrame(one_hot.transform(X_test))\n\n\nX_train_one_hot.index = X_train.index\nX_test_one_hot.index = X_test.index\n\nnum_X_train = X_train[numeric_var]\nnum_X_test = X_test[numeric_var]\n\n# Joining numerical and one hot encoded variables to create our final X_train and X_test.\nX_train_new = pd.concat([num_X_train, X_train_one_hot], axis = 1)\nX_test_new = pd.concat([num_X_test, X_test_one_hot], axis = 1)\n\n\n# X_train_new = pd.concat([num_X_train])\n# X_test_new = pd.concat([num_X_test])\n\n# Scaling our records into standard range of 0 and 1.\nscaler = StandardScaler()\n\nX_train_new = scaler.fit_transform(X_train_new)\nX_test_new = scaler.transform(X_test_new)","127cbae5":"# Logistic Regression\nmodel_logr = LogisticRegression(random_state = 1)\nmodel_logr.fit(X_train_new, y_train)\npred_logr = model_logr.predict(X_test_new)\nprint('Logistic Regression accuracy score:{0:0.3f}'. format(accuracy_score(y_test, pred_logr)))","a3d4d79e":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV","2f0ca175":"LogisticRegression().get_params().keys()","4f11bf58":"pipe = Pipeline([('classifier' , LogisticRegression())])\n# pipe = Pipeline([('classifier', RandomForestClassifier())])\n\n# Create param grid.\n\nparam_grid = [\n    {'classifier' : [LogisticRegression()],\n     'classifier__penalty' : ['l1', 'l2'],\n    'classifier__C' : [0.001, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 3.0,\n                       5.0, 10.0, 15.0, 20.0],\n    'classifier__solver' : ['liblinear'],\n    'classifier__max_iter' : [50, 100, 150, 200]}\n]\n\n# Create grid search object\n\nclf = GridSearchCV(pipe, param_grid = param_grid, cv = 5, verbose=True, \n                   n_jobs=-1)\n\n# Fit on data\n\nbest_clf = clf.fit(X_train_new, y_train)","80b44260":"best_clf.best_params_","30880d4b":"model_logr = LogisticRegression(C=3.0, max_iter=50, penalty='l1', solver='liblinear')\nmodel_logr.fit(X_train_new, y_train)\npred_logr = model_logr.predict(X_test_new)\nprint('Logistic Regression accuracy score:{0:0.3f}'. format(accuracy_score(y_test, pred_logr)))","faa4678d":"### Encoding and Feature Scaling","df7ac812":" <center>\n    <h1>Adult Census Income Prediction<\/h2>\n <\/center>","49de8760":"#### Marital Status","9a90d38c":"### Handling Skewness","7b96bc92":"![UCI.png](attachment:0b7658a5-82a9-476b-ac43-4cff98c5493e.png)","712a65f3":"#### Income","18f0ae6f":"<h4>Parameters - <\/h4>\n\n* age: the age of an individual\n* workclass: a general term to represent the employment status of an individual\n* fnlwgt: final weight. This is the number of people the census believes the entry represents..\n* education: the highest level of education achieved by an individual.\n* education\u00adnum: the highest level of education achieved in numerical form.\n* marital\u00adstatus: marital status of an individual.\n* occupation: the general type of occupation of an individual\n* relationship: represents what this individual is relative to others.\n* race: Descriptions of an individual\u2019s race\n* sex: the sex of the individual\n* capital\u00adgain: capital gains for an individual\n* capital\u00adloss: capital loss for an individual\n* hours\u00adper\u00adweek: the hours an individual has reported to work per week\n* native\u00adcountry: country of origin for an individual\n","27b2d052":"#### Relationship","4cfc6364":"#### Education","e3fbe6ea":"### Multi-Collinearity\n\n<h4><span style =\"color:Darkblue;\">Insights<\/span><\/h4>\n\nThere is a presence of multicollinearity if the value of VIF > 10.","64576cd9":"<h4><span style =\"color:Darkblue;\">Insights<\/span><\/h4>\n\n<span style = color:steelblue>Education and Education num are highly correlated with each other. Therefore we should drop one of them in order to significance and uniqueness of the variable. We will see which one to drop in a few mintues.<spam\/>","7a5c5141":"### Model Building","9acda08e":"#### Occupation","25fdee42":"### Feature Engineering","8a481944":"#### FNLWGT (Final Weight)","8c17e3d7":"<span style = color:steelblue>The results before and after the optimization of logistic regression are similar. We didn't improve after optimization<\/span>","606514ad":"#### Sex","62851bd0":"#### Hours per-week","4bfdd626":"### Conclusion\n\nIn this notebook, I have tried to explain the basic flow of building a model with the help of logistic regression. Remember that while building logistic regression it is important that your data should not have multicollinearity and it is generally applicable for binary classfication.","9fb22f3a":"### Correlation","cbf0776b":"#### Capital Loss","317a7c44":"#### Capital Gain","d630c3d7":"<h4><span style =\"color:Darkblue;\">\nIf you find this notebook valuable, please do upvote. It means a lot and will encourage me to make new notebooks like this.\n<\/span><\/h4>","714b5af6":"<h3>Categorical Variables EDA<\/h3>\n\n<h4><span style =\"color:Darkblue;\">Insights<\/span><\/h4>\n\nHere we have used barplot as well as countplot. Barplot which calculates the mean of the values in the particular columns. That's why we are getting values that are between 0 and 1. Countplot shows the number of observation in a category in a column.","bef2c38f":"<h4><span style =\"color:Darkblue;\">Insights<\/span><\/h4>\n\n<span style =\"color:steelblue;\">More than half the values are unique. This probably doesn't give a sigificant information about the data. But, later in feature engineering we will see if this feature holds any importance towards our goal.<\/span>","810b7ca8":"#### Education Num","a534b208":"#### AGE","7a4266fe":"### Splitting of the Data","fb9854ad":"### Numerical Variables EDA","c7745f00":"#### Workclass","4c173093":"#### Native Country","da3ea7b6":"<h4><span style =\"color:Darkblue;\">Insights<\/span><\/h4>\n\n<span style = color:steelblue>The percetage of the entries with (?) is very low as compared to the length of the data of that particular columns. As we can see that the percentage in workclass, occupation, native.country are 6%, 6%, 2% respectively. Therefore, it seems that dropping the na values should be good choice but we can't be sure. So, check the score with and without dropping the columns and go for the one which gives better result.<\/span>","2bb0a584":"<h4><span style =\"color:Darkblue;\">Insights<\/span><\/h4>\n\n<span style = color:steelblue>Earlier we came to a hypothesis in describe() results, that fnlwgt, captial gain and captial loss are right-skewed. Through the graph it validates our hypothesis.<\/span>","910032a4":"#### *Logistic Regression*\n\nLogistic Regression is a classification algorithm and is widely used for binary classfication i.e; when target variable is distributed in only two classes. Though it is used for classification its name is regression because it is derived from Linear Regression.\n\nLimitations of Linear Regression-\n1. Linear Regression gives bad fit if the data is not equally distributed.\n2. The outputs may exceed 0 to 1 range.\n\n#### *Sigmoid Function*\nThe basic difference between between the two algorithms is that Linear regression uses linear function whereas logistic regression uses logistic function or sigmoid function. This S shaped curve takes numeric input and outputs the results in 0 and 1. We only use sigmoid function because it gives one maxima or minima value during estimation and optimization of cost function and it is a part of bigger class of algorithms known as generalized linear models.\n\n![linear-regression-vs-logistic-regression.png](attachment:df6885ba-f839-4dec-90e2-3d4679c1b772.png)\n\n#### *Formula*: \n\n![log reg formula (2).jpg](attachment:2de190a5-8ee3-44fe-8817-368ebcb8c408.jpg)\n\nThe density or probablity of y(i) given x(i) and parameterized by theta is given as:\n\n**P(y|x;theta) = h(x)^y * (1-h(x))^1-y**\n\n#### *Maximum Likelihood Estimation*:\n\nSince, we know that logistic model is a probablistic model and in order to get maximum proablity of y(i) we need to optimize the above given equation. There are two ways in which we can optimize our MLE. But first, we need to understand what is MLE? MLE is known as Maximum Likelihood Estimation, it is nothing but the probablity of data. We always say Likelihood of parameters or Probablity of data. It is a process of choosing right set of paramters in such a way that it maximizes our probablity or likelihood.\n\nThere are basically two algorithms for optimization of MLE:\n1. Gradient Ascent - \n2. Newton's method - (applied in very large datasets in order to get answers quickly)\n\n![Grdient Ascent.jpg](attachment:651615e9-a615-4985-ad53-bea445c4044b.jpg)\n\nGradient Ascent is a concave shaped graph completely opposite of gradient descent, and in this we climb up to reach to the global maxima. At that particular point we get our best result. There is no local maxima because likelihood of logistic regression is always looks like concave graph. This is also the reason why we consider only sigmoid function and not any other function that gives binary result.","a8ca524f":"### Specify the numerical and categorical variables","4cd74f49":"<h4>Data Background - <\/h4>This data was extracted from the **1994 Census bureau** database by Ronny Kohavi and Barry Becker (data Mining and Visualization, Silicon Graphics). *In 2016, Ronny Kohavi was named the 5th most influential scholar in AI and the 26th most influential scholar in Machine Learning.*","040c9e9a":"**Thankyou!**","3c97a98a":"### Optimization","1ed02d8a":"<span style = color:steelblue>\nHere, Married-civ-spouse = when spouse is civilian.\n\nMarried-af-spouse = when spouse is in Armed Forces.<\/span>","a3a7d485":"<h4><span style =\"color:Darkblue;\">Insights<\/span><\/h4>\n\n<span style = 'color:steelblue'> Count tell us if there's a missing data or not. All the numerical columns seems to have no missing data here. The mean and median tell us about the skewness in the data. For capital gain and capital loss, we can see that mean > median which happens when our data is right skewed. Rest of the columns seems fine. We can also say that our capital gain and capital loss are affected by outliers. ![Skewness.jpg](attachment:8c4a678a-1cfe-4e21-96b7-a69e552d0a11.jpg) <\/span>","3fab32ee":"<h4>Task - <\/h4>The prediction task is to determine whether a person makes over $50K a year.\nGiven that the class imbalance is not severe and that both class labels are equally important, it is common to use classification accuracy or classification error to report model performance on this dataset."}}