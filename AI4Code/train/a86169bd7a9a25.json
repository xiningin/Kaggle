{"cell_type":{"97ff2d62":"code","5f9f214e":"code","3aedc3a9":"code","4c2810e8":"code","ef4f0617":"code","bca77f8e":"code","b980850f":"code","67c70a1b":"code","0ceb34ce":"code","73914330":"code","67cacc00":"code","8cd0b1f7":"code","ec77d118":"code","9c1ecb8c":"code","a987827c":"code","21a17748":"code","9cc895af":"code","03053384":"code","ae9237df":"code","75f83cc6":"code","deafe296":"code","18bd7506":"code","3a572e67":"code","b0fcd486":"code","4b9ba15b":"code","ee0b3c94":"code","f3afa285":"code","694ae438":"markdown","03965ea2":"markdown","ca9dd1d9":"markdown","ce8012ce":"markdown","5b107280":"markdown","2cca4d56":"markdown"},"source":{"97ff2d62":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import KFold\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_probability as tfp\nfrom sklearn.preprocessing import RobustScaler\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\nimport gresearch_crypto\nimport gc\n\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\npd.set_option('display.max_columns', None)\n\nDEBUG = True","5f9f214e":"train = pd.read_csv('..\/input\/g-research-crypto-forecasting\/train.csv').set_index(\"timestamp\")\nassets = pd.read_csv('..\/input\/g-research-crypto-forecasting\/asset_details.csv')\n#for assets sorting \nassets_order = pd.read_csv('..\/input\/g-research-crypto-forecasting\/supplemental_train.csv').Asset_ID[:14]\nassets_order = dict((t,i) for i,t in enumerate(assets_order))\n\nif DEBUG:\n    #train = train[1000000:12000000]\n    train = train[10000000:]\n\ntrain[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP','Target']] = \\\ntrain[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP','Target']].astype(np.float32)\n\ntrain['Target'] = train['Target'].fillna(0)","3aedc3a9":"# VWAP column has -inf and inf values. VWAP_max and VWAP_min will be used for replacement\n    \nVWAP_max = np.max(train[np.isfinite(train.VWAP)].VWAP)\nVWAP_min = np.min(train[np.isfinite(train.VWAP)].VWAP)\nprint(VWAP_max, \"\\n\", VWAP_min)\n\ntrain['VWAP'] = np.nan_to_num(train.VWAP, posinf=VWAP_max, neginf=VWAP_min)","4c2810e8":"# Get the series of the 'real' record ids for futher matching\n    \ndf = train[['Asset_ID', 'Target']].copy()\n\ntimes = dict((t,i) for i,t in enumerate(df.index.unique()))\ndf['id'] = df.index.map(times)\ndf['id'] = df['id'].astype(str) + '_' + df['Asset_ID'].astype(str)\nids = df.id.copy()\n\ndel df","ef4f0617":"def add_features(df):\n    df['Upper_Shadow'] = df['High'] - np.maximum(df['Close'], df['Open'])\n    df['Lower_Shadow'] = np.minimum(df['Close'], df['Open']) - df['Low']\n    \n    df['spread'] = df['High'] - df['Low']\n    df['mean_trade'] = df['Volume']\/df['Count']\n    df['log_price_change'] = np.log(df['Close']\/df['Open'])\n    return df\n\ntrain=add_features(train)\ntrain.shape","bca77f8e":"scale_features = train.columns.drop(['Asset_ID','Target'])\nRS = RobustScaler()\ntrain[scale_features] = RS.fit_transform(train[scale_features])","b980850f":"# Filling gaps\n\nind = train.index.unique()\n\ndef reindex(df):\n    df = df.reindex(range(ind[0],ind[-1]+60,60),method='nearest')\n    df = df.fillna(method=\"ffill\").fillna(method=\"bfill\")\n    return df\n\ntrain=train.groupby('Asset_ID').apply(reindex).reset_index(0, drop=True).sort_index()\ntrain.shape","67c70a1b":"# Matching records and marking generated rows as 'non-real'\n\ntrain['group_num'] = train.index.map(times)\ntrain = train.dropna(subset=['group_num'])\ntrain['group_num'] = train['group_num'].astype('int')\n\ntrain['id'] = train['group_num'].astype(str) + '_' + train['Asset_ID'].astype(str)\n\ntrain['is_real'] = train.id.isin(ids)*1\ntrain = train.drop('id', axis=1)","0ceb34ce":"# Features values for 'non-real' rows are set to zeros\n\nfeatures = train.columns.drop(['Asset_ID','group_num','is_real'])\ntrain.loc[train.is_real==0, features]=0.","73914330":"# Sorting assets according to their order in the 'supplemental_train.csv'\n\ntrain['asset_order'] = train.Asset_ID.map(assets_order) \ntrain=train.sort_values(by=['group_num', 'asset_order'])\ntrain.head(20)","67cacc00":"targets = train['Target'].to_numpy().reshape(-1, 14)\n#targets = np.expand_dims(targets, axis=1)\n\nfeatures = train.columns.drop(['Asset_ID', 'Target', 'group_num','is_real'])\ntrain = train[features]\n\ntrain=np.array(train)\ntrain = train.reshape(-1,14,train.shape[-1])\n#train = np.expand_dims(train, axis=1)\ntrain.shape","8cd0b1f7":"# timeseriesgenerator-like class, except it using target from the last timestep insteed of last+1\nclass sample_generator(keras.utils.Sequence):\n    def __init__(self, x_set, y_set, batch_size, length):\n        self.x, self.y = x_set, y_set\n        self.batch_size = batch_size\n        self.length = length\n        self.size = len(x_set)\n\n    def __len__(self):\n        return int(np.ceil(len(self.x) \/ float(self.batch_size)))\n\n    def __getitem__(self, idx):\n        batch_x=[]\n        batch_y=[]\n        for i in range(self.batch_size):\n            start_ind = self.batch_size*idx + i\n            end_ind = start_ind + self.length \n            if end_ind <= self.size:\n                batch_x.append(self.x[start_ind : end_ind])\n                batch_y.append(self.y[end_ind -1])\n\n        return np.array(batch_x), np.array(batch_y)","ec77d118":"#last 10% of the data are used as validation set\nX_train, X_test = train[:-len(train)\/\/10], train[-len(train)\/\/10:]\ny_train, y_test = targets[:-len(train)\/\/10], targets[-len(train)\/\/10:]","9c1ecb8c":"BATCH_SIZE=2**10\ntrain_generator = sample_generator(X_train, y_train, length=15, batch_size=BATCH_SIZE)\nval_generator = sample_generator(X_test, y_test, length=15, batch_size=BATCH_SIZE)\n\nprint(f'Sample shape: {train_generator[0][0].shape}')\nprint(f'Target shape: {train_generator[0][1].shape}')","a987827c":"#https:\/\/github.com\/tensorflow\/tensorflow\/issues\/37495\ndef MaxCorrelation(y_true,y_pred):\n    \"\"\"Goal is to maximize correlation between y_pred, y_true. Same as minimizing the negative.\"\"\"\n    mask = tf.math.not_equal(y_true, 0.)\n    y_true_masked = tf.boolean_mask(y_true, mask)\n    y_pred_masked = tf.boolean_mask(y_pred, mask)\n    return -tf.math.abs(tfp.stats.correlation(y_true_masked,y_pred_masked, sample_axis=None, event_axis=None))\n\ndef Correlation(y_true,y_pred):\n    return tf.math.abs(tfp.stats.correlation(y_pred,y_true, sample_axis=None, event_axis=None))\n\ndef masked_mse(y_true, y_pred):\n    mask = tf.math.not_equal(y_true, 0.)\n    y_true_masked = tf.boolean_mask(y_true, mask)\n    y_pred_masked = tf.boolean_mask(y_pred, mask)\n    return tf.keras.losses.mean_squared_error(y_true = y_true_masked, y_pred = y_pred_masked)\n\ndef masked_mae(y_true, y_pred):\n    mask = tf.math.not_equal(y_true, 0.)\n    y_true_masked = tf.boolean_mask(y_true, mask)\n    y_pred_masked = tf.boolean_mask(y_pred, mask)\n    return tf.keras.losses.mean_absolute_error(y_true = y_true_masked, y_pred = y_pred_masked)\n\ndef masked_cosine(y_true, y_pred):\n    mask = tf.math.not_equal(y_true, 0.)\n    y_true_masked = tf.boolean_mask(y_true, mask)\n    y_pred_masked = tf.boolean_mask(y_pred, mask)\n    return tf.keras.losses.cosine_similarity(y_true_masked, y_pred_masked)\n\ndef get_model(n_assets=14):  \n    x_input = keras.Input(shape=(train_generator[0][0].shape[1], n_assets, train_generator[0][0].shape[-1]))\n\n    branch_outputs = []\n        \n    for i in range(n_assets):\n            # Slicing the ith asset:\n        a = layers.Lambda(lambda x: x[:,:, i])(x_input)\n        a = layers.Masking(mask_value=0.,)(a)\n        #a = layers.BatchNormalization()(a)\n        #a = layers.Bidirectional(layers.LSTM(units=24, return_sequences=True))(a)\n        a = layers.GRU(units=64, return_sequences=True)(a)\n        a = layers.GRU(units=32)(a)\n        branch_outputs.append(a)\n    \n    x = layers.Concatenate()(branch_outputs)\n    x = layers.Dense(units=128)(x)\n    #x = layers.Dropout(0.2)(x)\n    out = layers.Dense(units=n_assets)(x)\n    \n    model = keras.Model(inputs=x_input, outputs=out)\n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3), \n                  #loss = 'mse',\n                  #loss = 'cosine_similarity',\n                  loss = masked_cosine,\n                  metrics=[Correlation]\n                 )\n    \n    return model \n    \nmodel=get_model()\n#model.summary()","21a17748":"gc.collect()","9cc895af":"#example with 3 assets for visibility\ntf.keras.utils.plot_model(get_model(n_assets=3), show_shapes=True)","03053384":"tf.random.set_seed(0)\n\nestop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min',restore_best_weights=True)\nplateau = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.9, patience=3, verbose=1, min_lr=1e-8)\nscheduler = keras.optimizers.schedules.ExponentialDecay(1e-3, (0.5*len(X_train)\/BATCH_SIZE), 1e-3)\n\nlr = keras.callbacks.LearningRateScheduler(scheduler, verbose = 1)\n    \nhistory = model.fit(train_generator, validation_data = (val_generator),\n              epochs = 30, callbacks = [lr])","ae9237df":"gc.collect()","75f83cc6":"fig, ax = plt.subplots(1,2, figsize=(16,8))\n\nhistories = pd.DataFrame(history.history)\n\nepochs = list(range(1,len(histories)+1))\nloss = histories['loss']\nval_loss = histories['val_loss']\nCorrelation = histories['Correlation']\nval_Correlation = histories['val_Correlation']\n\nax[0].plot(epochs, loss, label = 'Train Loss')\nax[0].plot(epochs, val_loss, label = 'Val Loss')\nax[0].set_title('Losses')\nax[0].set_xlabel('Epoch')\nax[0].legend(loc='upper right')\n\nax[1].plot(epochs, Correlation, label = 'Train Correlation')\nax[1].plot(epochs, val_Correlation, label = 'Val Correlation')\nax[1].set_title('Correlations')\nax[1].set_xlabel('Epoch')\nax[1].legend(loc='upper right')\n\nfig.show()","deafe296":"gc.collect()","18bd7506":"predictions = model.predict(val_generator)\npredictions.shape","3a572e67":"print('Asset:    Corr. coef.')\nprint('---------------------')\nfor i in range(14):\n    #drop first 14 values in the y_test, since they are absent in val_generator labels\n    y_true = np.squeeze(y_test[14:,i])\n    y_pred = np.squeeze(predictions[:,i])\n    \n    #get non-zero targets (assuming that zeros indicates 'non-real' rows)\n    real_target_ind = np.argwhere(y_true!=0)\n    asset_id = list(assets_order.keys())[i]\n    asset_name = assets[assets.Asset_ID == asset_id]['Asset_Name'].item()\n    print(f\"{asset_name}: {np.corrcoef(y_pred[real_target_ind].flatten(), y_true[real_target_ind].flatten())[0,1]:.4f}\") \n\ndel predictions","b0fcd486":"#test_df = pd.read_csv('..\/input\/g-research-crypto-forecasting\/example_test.csv')\n#sample_prediction_df = pd.read_csv('..\/input\/g-research-crypto-forecasting\/example_sample_submission.csv')\n\n#test_df = test_df.iloc[2:12]\n#sample_prediction_df=sample_prediction_df[sample_prediction_df.row_id.isin(test_df.row_id)]\n\n#scale_features = test_df.columns.drop(['Asset_ID','group_num','row_id'])#test_df.head(3)\n#test_df.head()","4b9ba15b":"#placeholder for first 15 samples\nsup=pd.read_csv('..\/input\/g-research-crypto-forecasting\/supplemental_train.csv')[:15*14]\nplaceholder=add_features(sup)\nplaceholder[scale_features] = RS.transform(placeholder[scale_features])\nplaceholder['asset_order'] = placeholder.Asset_ID.map(assets_order) \ntest_sample = np.array(placeholder[features])\ntest_sample = test_sample.reshape(-1,14,test_sample.shape[-1])\ntest_sample = np.expand_dims(test_sample, axis=0)\ntest_sample.shape","ee0b3c94":"# for test gap filling\nexample = pd.read_csv('..\/input\/g-research-crypto-forecasting\/example_test.csv')[:14]\nexample['asset_order'] = example.Asset_ID.map(assets_order) \nexample = example[['Asset_ID','asset_order']]\nexample","f3afa285":"env = gresearch_crypto.make_env()\niter_test = env.iter_test()\n\nfor (test_df, sample_prediction_df) in iter_test:\n        \n    test_df['VWAP'] =np.nan_to_num(test_df.VWAP, posinf=VWAP_max, neginf=VWAP_min)\n    test_df=add_features(test_df)\n    test_df[scale_features] = RS.transform(test_df[scale_features])\n    \n    test_data = test_df.merge(example, how='outer', on='Asset_ID').sort_values('asset_order')\n        \n    test = np.array(test_data[features].fillna(0))\n    test = test.reshape(-1,1,14,test.shape[-1])\n        #get test sample, shape (1, 15, 14, test.shape[-1])\n    test_sample = np.hstack([test_sample, test])[:,-15:]\n    y_pred = model.predict(test_sample).squeeze().reshape(-1, 1).squeeze()\n    \n    test_data['Target'] = y_pred\n        \n    #test_df = test_df.merge(test_data[['row_id', 'Target']], how='left', on='row_id')\n    for _, row in test_df.iterrows(): \n        try:\n            sample_prediction_df.loc[sample_prediction_df['row_id'] == row['row_id'], 'Target'] = test_data.loc[test_data['row_id'] == row['row_id'], 'Target'].item()\n        except:\n            sample_prediction_df.loc[sample_prediction_df['row_id'] == row['row_id'], 'Target'] = 0\n       \n    #sample_prediction_df['Target'] = test_df['Target']\n    #display(test_df)\n    #display(test_data)\n    #display(sample_prediction_df)    \n    env.predict(sample_prediction_df)","694ae438":"### Load and prepare the data","03965ea2":"### Intro\n\nHere is an attempt to implement an idea of using the parallel RNN branches for each asset and use learned information to compute the vector of targets at each timestep.\n\nSteps:\n* Fill gaps in the training set like it describer in the competition tutorial notebook;\n* Use `TimeseriesGenerator` like generator class to build the dataset;\n* Build the model. Simplified  structure:\n    - Lambda layer needed for assets separation;\n    - Masking layer. Generated records (filled gaps) has zeros as features values, so they are not used in the computations;\n    - LSTM or GRU layer;\n    - Concatanate layer.\n\n![\u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435.png](attachment:99932047-786c-45c1-afb0-14c19c3b6183.png)\n","ca9dd1d9":"### Submission","ce8012ce":"### Dataset creation\n\nSamples with a duration of 15 records (minutes) will be formed from the `train` array. Each sample has a target vector corresponding to the 15th record. \n\n![data.JPG](attachment:364826a2-fe7e-4ac8-89cb-5d1217ae2d02.JPG)","5b107280":"### Training","2cca4d56":"### The correlation coefficients by asset for the validation data"}}