{"cell_type":{"a9458321":"code","4a50feb8":"code","10fd0b85":"code","4e2d4737":"code","662d168c":"code","70cf2e41":"code","98059bb9":"code","2cd283d4":"code","a2e8dabb":"code","fef8ea49":"code","72ac9f4f":"code","b0a4943d":"code","f8757302":"code","e05272b6":"code","85ff685d":"code","a5062e0d":"code","34257891":"code","7de6b544":"code","28413d01":"code","9dbcd01d":"code","0f758cd8":"code","55195202":"code","644e27bb":"code","2da3755b":"code","a968b4ab":"code","1f66b113":"code","688262b4":"code","a33cc24c":"code","fdf28c2c":"markdown","04d21883":"markdown","c9305611":"markdown","b9e8d567":"markdown","577c76a3":"markdown","d64f0066":"markdown","4e0a3315":"markdown","26f691ea":"markdown","cfdb3fbd":"markdown","e3bdbf18":"markdown","65b93d1f":"markdown","45548196":"markdown","69e8d726":"markdown","a2c79c69":"markdown","b3a0d80e":"markdown","4dcf4a6e":"markdown","4d4f011c":"markdown","854a98df":"markdown","803069bf":"markdown"},"source":{"a9458321":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, RANSACRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\nimport itertools as it\nimport warnings\n\nwarnings.filterwarnings('ignore')","4a50feb8":"# Load the Train & Test Dataset into the Pandas DataFrame\ntrain_df = pd.read_csv('..\/input\/30-days-of-ml\/train.csv')\ntest_df = pd.read_csv('..\/input\/30-days-of-ml\/test.csv')","10fd0b85":"# Print the no. of Rows & Columns for Train, Test DataFrame\nprint(f'Training Dataset Contains, Rows: {train_df.shape[0]}, Columns:{train_df.shape[1]}')\nprint(f'Test Dataset Contains, Rows: {test_df.shape[0]}, Columns:{test_df.shape[1]}')","4e2d4737":"# Check all the columns Name\ntrain_df.columns","662d168c":"# Basic information about the Training Data\ntrain_df.info()","70cf2e41":"# Check no. of missing values in every column for training dataframe\npd.DataFrame(train_df.isnull().sum()).style.background_gradient(cmap='Spectral_r')","98059bb9":"# Check no. of missing values in every column for test dataframe\npd.DataFrame(test_df.isnull().sum()).style.background_gradient(cmap='Wistia_r')","2cd283d4":"plt.figure(figsize=(15, 6))\nsns.heatmap(train_df.corr(), annot=True, cmap='autumn_r');","a2e8dabb":"sns.set(style=\"darkgrid\", palette=\"muted\", color_codes=True)\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# Create a subplot\nf, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\nsns.despine(left=True)\n\n# Plot a simple histogram with binsize determined automatically\nsns.distplot(train_df.target, kde=False, color=\"blueviolet\", ax=axes[0]).set_title('Histogram of target attribute for Training Dataset')\n\n# Plot a kernel density estimate and rug plot\nsns.distplot(train_df.target, hist=False, rug=True, color=\"g\", ax=axes[1]).set_title('Kernel Density Estimate and Rug plot')\n\nplt.setp(axes, yticks=[])\nplt.tight_layout()","fef8ea49":"# Store all the numerical feature names into a list\nnumercal_features = list(train_df.select_dtypes(exclude=['object']).columns)[:-1]\n\n# Store all the categorical feature names into a list\ncategorical_features = train_df.select_dtypes(include=['object']).columns\n\n# Store all the feature names into a list\nall_features = numercal_features + list(categorical_features)","72ac9f4f":"train_df[numercal_features[1:]].describe().T.style.background_gradient(cmap='Pastel2')","b0a4943d":"def distribution_plot(df, color_list, height):\n    \"\"\"\n    This function plots the histogram of the given column of the DataFrame\n    \"\"\"\n    column = df.columns\n    length = len(column)\n    plt.subplots(figsize = (12, height))\n    for i, j in it.zip_longest(column, range(length)):\n        plt.subplot((length\/2), 2, j + 1)\n        plt.subplots_adjust(wspace = 0.2,hspace = 0.3)\n        df[i].hist(bins = 20,  color = color_list[j])\n        plt.title(i)\n    plt.show()","f8757302":"colours = ['firebrick', 'indigo', 'indianred', 'darkviolet', 'tomato', \n          'deeppink', 'sandybrown', 'lime', 'lightsalmon', 'greenyellow', \n          'orange', 'turquoise', 'gold', 'aqua', ]\ndistribution_plot(train_df[numercal_features[1:]], colours, 25)","e05272b6":"colours = [\"lightcoral\",\"darkorange\", \"skyblue\", 'olivedrab', \"springgreen\", \n           \"crimson\", \"teal\", \"goldenrod\", \"orchid\", \"r\", \"royalblue\"]\ndistribution_plot(train_df[categorical_features], colours, 15);","85ff685d":"# Create the input feature dataframe\nX = train_df.drop(['target'], axis=1)\n\n# Create a serise of target attribute\ny = train_df['target']\n\n# Create a dataframe by copying the test dataframe\nX_test = test_df.copy()","a5062e0d":"# Create an OrdinalEncoder object \nordinal_encoder = OrdinalEncoder()\n\n# Fit and Transform the Categorical features of Training Dataset\nX[categorical_features] = ordinal_encoder.fit_transform(X[categorical_features])\n\n# Transform the Categorical features of Test Dataset\nX_test[categorical_features] = ordinal_encoder.transform(X_test[categorical_features])","34257891":"# Check first 3 rows of Categorical features of Training Dataset\nX[categorical_features].head(3)","7de6b544":"# Create Regressor objects of several Algorithm Classes\ndtreg = DecisionTreeRegressor(random_state=1)\nrfreg = RandomForestRegressor(random_state=1)\ngbreg = GradientBoostingRegressor(random_state=1)\nxgbreg = XGBRegressor(random_state=1)\n\n# Create a list of all model objects\nmodels = [dtreg, rfreg, gbreg, xgbreg]","28413d01":"def Modelling(model):\n    \"\"\"\n    This function returns the Root Mean Squared Error(RMSE)\n    value of the given model object\n    \"\"\"\n    X_train, X_valid, y_train, y_valid = train_test_split(X[:200000], y[:200000], test_size=0.25, random_state=1)\n    model.fit(X_train, y_train)\n    model_prediction = model.predict(X_valid)\n    model_rmse = round(np.sqrt(mean_squared_error(model_prediction, y_valid)), 5)\n        \n    return model_rmse","9dbcd01d":"# Create a list of RMSE values of different models\nRMSE = [Modelling(model) for model in models]\n\n# Store the Model Regressors names into a list\nModels = [str(i).split('(')[0] for i in models]\n\n# Create a DataFrame using the name Model names and their corresponding RMSE value\nEval_df = pd.DataFrame(list(zip(Models, RMSE)), columns=['Model', 'RMSE'])","0f758cd8":"# Sort the Eval_df by the RMSE value of the Model\nEval_df.sort_values(by='RMSE').style.background_gradient(cmap='YlGnBu')","55195202":"# Create barplot \nplt.figure(figsize=(8, 6))\nax = sns.barplot(x='RMSE', y='Model', data=Eval_df.sort_values(by='RMSE'))\n\n# Annotate every single Bar with its value, based on it's width           \nfor p in ax.patches:\n    width = p.get_width()\n    plt.text(p.get_width()-0.1, p.get_y()+0.55*p.get_height(),\n             '{}'.format(width),\n             ha='center', va='center')\n    \nplt.title('Model Selection', fontsize=23);","644e27bb":"# Create a dictionary with the best values of XGBRegressor's model tunning parameters\nxgb_params = {\n    'booster': 'gbtree',\n    'n_estimators': 10000,\n    'learning_rate': 0.05,\n    'reg_lambda': 10,\n    'reg_alpha': 26,\n    'subsample': 0.9,\n    'colsample_bytree': 0.12,\n    'max_depth': 3,\n    'random_state': 91 \n}","2da3755b":"N_FOLD = 7\n\n#Setting the kfold parameters\nkf = KFold(n_splits=N_FOLD, shuffle=True, random_state=91)\n\noof_preds = np.zeros((X.shape[0],))\npredictions = 0\nmodel_fi = 0\nmean_rmse = 0\n\nfor num, (train_idx, valid_idx) in enumerate(kf.split(X)):\n    # split the train data into train and validation\n    X_train = X.iloc[train_idx][all_features]\n    X_valid = X.iloc[valid_idx][all_features]\n    y_train = y.iloc[train_idx]\n    y_valid = y.iloc[valid_idx]\n    \n    model = XGBRegressor(**xgb_params)\n    model.fit(X_train, y_train,\n             verbose = False,\n             eval_set = [(X_valid, y_valid)],\n             eval_metric = \"rmse\",\n             early_stopping_rounds = 50)\n    \n    #Mean of the predictions\n    predictions += model.predict(X_test) \/ N_FOLD\n    \n    #Mean of feature importance\n    model_fi += model.feature_importances_ \/ N_FOLD \n    \n    #Out of Fold predictions\n    oof_preds[valid_idx] = model.predict(X_valid)\n    fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_idx]))\n    print(f\"Fold {num} | RMSE: {fold_rmse}\")\n    \n    mean_rmse += fold_rmse \/ N_FOLD\n    \nprint(f\"\\nOverall RMSE: {mean_rmse}\")","a968b4ab":"predictions.shape","1f66b113":"fi_df = pd.DataFrame({\n    'Feature': all_features,\n    'feature_importance_value': list(model_fi)\n})\n\n# Create barplot \nplt.figure(figsize=(12, 6))\nax = sns.barplot(x='feature_importance_value', y='Feature', data=fi_df.sort_values(by='feature_importance_value'))\n\n# Annotate every single Bar with its value, based on it's width           \nfor p in ax.patches:\n    width = p.get_width()\n    plt.text(p.get_width()+0.009, p.get_y()+0.55*p.get_height(),\n             '{:.5}'.format(width),\n             ha='center', va='center')\n    \nplt.title('Feature Importance', fontsize=23);","688262b4":"# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': test_df.index,\n                       'target': predictions})\noutput.to_csv('submission.csv', index=False)","a33cc24c":"output.head()","fdf28c2c":"### Remarks:\n\n> We can observe that the given dataset doesn't contain any null value for Training as well as Test Dataset.\n\n> Hence we don't need to worried about the missing value issue.","04d21883":"# Final Model Building","c9305611":"# 30 Days of Machine Learning - Competition","b9e8d567":"## Missing Value Check","577c76a3":"### Remarks:\n> From the above plot it's clear that as the RMSE value of **XGBRegressor** model is lowest among all the Regressor,\nwe select the XGBRegressor as our final model building","d64f0066":"## Bar plot of the Numerical features","4e0a3315":"# Data Exploration","26f691ea":"## Heat Map of all features","cfdb3fbd":"## Distribution of the target attribute","e3bdbf18":"# Save and submit the output","65b93d1f":"## Statistical Analysis of Numerical Features for Training Dataset","45548196":"## Distribution of different Categorical Features","69e8d726":"# EDA","a2c79c69":"# Model Selection","b3a0d80e":"# Objective\n\n> As the **target** attribute of the given Data of this Competition is **Continuous** in nature, this problem belongs to **Rgression** task.\n\n> The main purose of this notebook is to explore different aspects of the dataset by **Exploratory Data Analysis(EDA)** \n\n> We'll also look into the steps to Create a best **Regression Model** for *predicting the target attribute*\n\n> We will see the step by step process involving **Feature Engineering, Data Processing, Data Validation Techniques and finally Model Selection**\n","4dcf4a6e":"## Import all necessary libraries","4d4f011c":"![image.png](attachment:074f1f5f-6d0c-41cf-928b-0f7400045c8a.png)","854a98df":"# Contents\n\n1. [Data Exploration](#Data-Exploration)\n2. [EDA](#EDA)\n3. [Data Pre-Processing](#Data-Pre-Processing)\n4. [Model Selection](#Model-Selection)\n5. [Final Model Building](#Final-Model-Building)","803069bf":"# Data Pre-Processing"}}