{"cell_type":{"62b0e38d":"code","8b237b2a":"code","9de0a36d":"code","7deeedca":"code","2e68837e":"code","64f3245b":"code","e666694c":"code","a308527e":"code","4a52800b":"code","c05d2de2":"code","a9827a84":"code","97ef5825":"code","371cabad":"code","afd24fbf":"code","8d1219a4":"code","1cec8c9e":"code","fd6af59b":"code","dff423a9":"code","b917c1ba":"code","a9e0224a":"code","8d848ad9":"markdown","e4351ea3":"markdown","c2b85608":"markdown"},"source":{"62b0e38d":"import os\nimport random\n\nfrom glob import glob\nfrom collections import defaultdict\nfrom multiprocessing import Pool, cpu_count, set_start_method\nfrom tqdm.auto import tqdm\nfrom trueskill import Rating, TrueSkill, rate_1vs1\nfrom tensorboardX import SummaryWriter\nfrom scipy.special import comb\nfrom dataclasses import dataclass","8b237b2a":"ROUNDS = 1000\nMATCHES_PER_BOT = 10              # increase it    \nOUTPUT_DIR = 'runs'               # tensorboard logs folder\nBOTS_DIR = 'bots'                 # store all your bots in this folder\nPROCESSES = cpu_count()\nCOPIES = defaultdict(lambda: 1)\nCOPIES['random_.py'] = 3          # create 3 copies of random bot for more fun\n\nRATING_MU = 600\nRATING_SIGMA = 200\nRATING_BETA = RATING_SIGMA \/ 2    # if distance between 2 ratings is beta points\n                                  # then probability of winning is 76%\n                                  # recommended value for TrueSkill rating \n                                  # is sigma \/ 2\n\nRATING_TAU = RATING_SIGMA \/ 1000  # how much most recent results have impact\n                                  # on rating\n                                  # recommended value for TrueSkill rating\n                                  # is 1\/100 of sigma, but I have found that 1\/1000\n                                  # of sigma produces much smoother results\n                \nDRAW_PROB = comb(ROUNDS, ROUNDS \/\/ 2) \/ 2 ** ROUNDS    ","9de0a36d":"set_start_method('fork')\n\n# set TrueSkill defaults\nTrueSkill(RATING_MU, RATING_SIGMA, RATING_BETA, RATING_TAU, DRAW_PROB).make_as_global(); ","7deeedca":"!mkdir bots","2e68837e":"%%writefile bots\/rock.py\ndef rock(observation, configuration):\n    return 0","64f3245b":"%%writefile bots\/paper.py\ndef paper(observation, configuration):\n    return 1","e666694c":"%%writefile bots\/scissors.py\ndef scissors(observation, configuration):\n    return 2","a308527e":"%%writefile bots\/counter_reactionary.py\nimport random\n\ndef copy_opponent(observation, configuration):\n    if observation.step > 0:\n        return observation.lastOpponentAction\n    else:\n        return random.randrange(3)","4a52800b":"%%writefile bots\/copy_opponent.py\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\nlast_counter_action = None\n\ndef counter_reactionary(observation, configuration):\n    global last_counter_action\n    if observation.step == 0:\n        last_counter_action = random.randrange(0, 3)\n    elif get_score(last_counter_action, observation.lastOpponentAction) == 1:\n        last_counter_action = (last_counter_action + 2) % 3\n    else:\n        last_counter_action = (observation.lastOpponentAction + 1) % 3\n\n    return last_counter_action","c05d2de2":"%%writefile bots\/random_.py\nimport random\n\ndef bot(o, c):\n    return random.randrange(3)","a9827a84":"%%writefile bots\/reactionary.py\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\nlast_react_action = None\n\n\ndef reactionary(observation, configuration):\n    global last_react_action\n    if observation.step == 0:\n        last_react_action = random.randrange(3)\n    elif get_score(last_react_action, observation.lastOpponentAction) <= 1:\n        last_react_action = (observation.lastOpponentAction + 1) % 3\n\n    return last_react_action","97ef5825":"%%writefile bots\/statistical.py\naction_histogram = {}\n\ndef statistical(observation, configuration):\n    global action_histogram\n    if observation.step == 0:\n        action_histogram = {}\n        return 0\n    action = observation.lastOpponentAction\n    if action not in action_histogram:\n        action_histogram[action] = 0\n    action_histogram[action] += 1\n    mode_action = None\n    mode_action_count = None\n    for k, v in action_histogram.items():\n        if mode_action_count is None or v > mode_action_count:\n            mode_action = k\n            mode_action_count = v\n            continue\n\n    return (mode_action + 1) % 3","371cabad":"%%writefile bots\/markov_agent.py\nimport random\nimport numpy as np\nfrom collections import defaultdict\n\n\ndef markov_agent(observation, _):\n    k = 2\n    global table, action_seq\n    if observation.step % 250 == 0:  # refresh table every 250 steps\n        action_seq, table = [], defaultdict(lambda: np.ones((3,), 'int32'))\n    if len(action_seq) <= 2 * k + 1:\n        action = random.randrange(3)\n        if observation.step > 0:\n            action_seq.extend([observation.lastOpponentAction, action])\n        else:\n            action_seq.append(action)\n        return action\n    # update table\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    table[key][observation.lastOpponentAction] += 1\n    # update action seq\n    action_seq[:-2] = action_seq[2:]\n    action_seq[-2] = observation.lastOpponentAction\n    # predict opponent next move\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    if observation.step < 500:\n        next_opponent_action_pred = table[key].argmax()\n    else:\n        # add stochasticity for second part of the game\n        next_opponent_action_pred = np.random.choice(3, p=table[key] \/ table[key].sum())\n    # make an action\n    action = (next_opponent_action_pred + 1) % 3\n    # if high probability to lose -> let's surprise our opponent with sudden change of our strategy\n    if observation.step > 900:\n        action = next_opponent_action_pred\n    action_seq[-1] = action\n    return action","afd24fbf":"%%writefile bots\/memory_patters.py\n\nimport random\n\n# how many steps in a row are in the pattern (multiplied by two)\nmemory_length = 6\n# current memory of the agent\ncurrent_memory = []\n# list of memory patterns\nmemory_patterns = []\n\ndef find_pattern(memory):\n    \"\"\" find appropriate pattern in memory \"\"\"\n    for pattern in memory_patterns:\n        actions_matched = 0\n        for i in range(memory_length):\n            if pattern[\"actions\"][i] == memory[i]:\n                actions_matched += 1\n            else:\n                break\n        # if memory fits this pattern\n        if actions_matched == memory_length:\n            return pattern\n    # appropriate pattern not found\n    return None\n\ndef memory_patterns_(obs, conf):\n    global memory_length, memory_patterns, current_memory\n    # if it's not first step, add opponent's last action to agent's current memory\n    if obs.step > 0:\n        current_memory.append(obs.lastOpponentAction)\n    else:\n        memory_length = 6\n        current_memory = []\n        memory_patterns = []\n\n    # if length of current memory is bigger than necessary for a new memory pattern\n    if len(current_memory) > memory_length:\n        # get momory of the previous step\n        previous_step_memory = current_memory[:memory_length]\n        previous_pattern = find_pattern(previous_step_memory)\n        if previous_pattern == None:\n            previous_pattern = {\n                \"actions\": previous_step_memory.copy(),\n                \"opp_next_actions\": [\n                    {\"action\": 0, \"amount\": 0, \"response\": 1},\n                    {\"action\": 1, \"amount\": 0, \"response\": 2},\n                    {\"action\": 2, \"amount\": 0, \"response\": 0}\n                ]\n            }\n            memory_patterns.append(previous_pattern)\n        for action in previous_pattern[\"opp_next_actions\"]:\n            if action[\"action\"] == obs.lastOpponentAction:\n                action[\"amount\"] += 1\n        # delete first two elements in current memory (actions of the oldest step in current memory)\n        del current_memory[:2]\n    my_action = random.randint(0, 2)\n    pattern = find_pattern(current_memory)\n    if pattern != None:\n        my_action_amount = 0\n        for action in pattern[\"opp_next_actions\"]:\n            # if this opponent's action occurred more times than currently chosen action\n            # or, if it occured the same amount of times, choose action randomly among them\n            if (action[\"amount\"] > my_action_amount or\n                    (action[\"amount\"] == my_action_amount and random.random() > 0.5)):\n                my_action_amount = action[\"amount\"]\n                my_action = action[\"response\"]\n    current_memory.append(my_action)\n    return my_action","8d1219a4":"%%writefile bots\/iocaine.py\nimport random\n\ndef recall(age, hist):\n    \"\"\"Looking at the last 'age' points in 'hist', finds the\n    last point with the longest similarity to the current point,\n    returning 0 if none found.\"\"\"\n    end, length = 0, 0\n    for past in range(1, min(age + 1, len(hist) - 1)):\n        if length >= len(hist) - past: break\n        for i in range(-1 - length, 0):\n            if hist[i - past] != hist[i]: break\n        else:\n            for length in range(length + 1, len(hist) - past):\n                if hist[-past - length - 1] != hist[-length - 1]: break\n            else:\n                length += 1\n            end = len(hist) - past\n    return end\n\n\ndef beat(i):\n    return (i + 1) % 3\n\n\ndef loseto(i):\n    return (i - 1) % 3\n\n\nclass Stats:\n    \"\"\"Maintains three running counts and returns the highest count based\n         on any given time horizon and threshold.\"\"\"\n\n    def __init__(self):\n        self.sum = [[0, 0, 0]]\n\n    def add(self, move, score):\n        self.sum[-1][move] += score\n\n    def advance(self):\n        self.sum.append(self.sum[-1])\n\n    def max(self, age, default, score):\n        if age >= len(self.sum):\n            diff = self.sum[-1]\n        else:\n            diff = [self.sum[-1][i] - self.sum[-1 - age][i] for i in range(3)]\n        m = max(diff)\n        if m > score: return diff.index(m), m\n        return default, score\n\n\nclass Predictor:\n    \"\"\"The basic iocaine second- and triple-guesser.    Maintains stats on the\n         past benefits of trusting or second- or triple-guessing a given strategy,\n         and returns the prediction of that strategy (or the second- or triple-\n         guess) if past stats are deviating from zero farther than the supplied\n         \"best\" guess so far.\"\"\"\n\n    def __init__(self):\n        self.stats = Stats()\n        self.lastguess = -1\n\n    def addguess(self, lastmove, guess):\n        if lastmove != -1:\n            diff = (lastmove - self.prediction) % 3\n            self.stats.add(beat(diff), 1)\n            self.stats.add(loseto(diff), -1)\n            self.stats.advance()\n        self.prediction = guess\n\n    def bestguess(self, age, best):\n        bestdiff = self.stats.max(age, (best[0] - self.prediction) % 3, best[1])\n        return (bestdiff[0] + self.prediction) % 3, bestdiff[1]\n\n\nages = [1000, 100, 10, 5, 2, 1]\n\n\nclass Iocaine:\n\n    def __init__(self):\n        \"\"\"Build second-guessers for 50 strategies: 36 history-based strategies,\n             12 simple frequency-based strategies, the constant-move strategy, and\n             the basic random-number-generator strategy.    Also build 6 meta second\n             guessers to evaluate 6 different time horizons on which to score\n             the 50 strategies' second-guesses.\"\"\"\n        self.predictors = []\n        self.predict_history = self.predictor((len(ages), 2, 3))\n        self.predict_frequency = self.predictor((len(ages), 2))\n        self.predict_fixed = self.predictor()\n        self.predict_random = self.predictor()\n        self.predict_meta = [Predictor() for a in range(len(ages))]\n        self.stats = [Stats() for i in range(2)]\n        self.histories = [[], [], []]\n\n    def predictor(self, dims=None):\n        \"\"\"Returns a nested array of predictor objects, of the given dimensions.\"\"\"\n        if dims: return [self.predictor(dims[1:]) for i in range(dims[0])]\n        self.predictors.append(Predictor())\n        return self.predictors[-1]\n\n    def move(self, them):\n        \"\"\"The main iocaine \"move\" function.\"\"\"\n\n        # histories[0] stores our moves (last one already previously decided);\n        # histories[1] stores their moves (last one just now being supplied to us);\n        # histories[2] stores pairs of our and their last moves.\n        # stats[0] and stats[1] are running counters our recent moves and theirs.\n        if them != -1:\n            self.histories[1].append(them)\n            self.histories[2].append((self.histories[0][-1], them))\n            for watch in range(2):\n                self.stats[watch].add(self.histories[watch][-1], 1)\n\n        # Execute the basic RNG strategy and the fixed-move strategy.\n        rand = random.randrange(3)\n        self.predict_random.addguess(them, rand)\n        self.predict_fixed.addguess(them, 0)\n\n        # Execute the history and frequency stratgies.\n        for a, age in enumerate(ages):\n            # For each time window, there are three ways to recall a similar time:\n            # (0) by history of my moves; (1) their moves; or (2) pairs of moves.\n            # Set \"best\" to these three timeframes (zero if no matching time).\n            best = [recall(age, hist) for hist in self.histories]\n            for mimic in range(2):\n                # For each similar historical moment, there are two ways to anticipate\n                # the future: by mimicing what their move was; or mimicing what my\n                # move was.    If there were no similar moments, just move randomly.\n                for watch, when in enumerate(best):\n                    if not when:\n                        move = rand\n                    else:\n                        move = self.histories[mimic][when]\n                    self.predict_history[a][mimic][watch].addguess(them, move)\n                # Also we can anticipate the future by expecting it to be the same\n                # as the most frequent past (either counting their moves or my moves).\n                mostfreq, score = self.stats[mimic].max(age, rand, -1)\n                self.predict_frequency[a][mimic].addguess(them, mostfreq)\n\n        # All the predictors have been updated, but we have not yet scored them\n        # and chosen a winner for this round.    There are several timeframes\n        # on which we can score second-guessing, and we don't know timeframe will\n        # do best.    So score all 50 predictors on all 6 timeframes, and record\n        # the best 6 predictions in meta predictors, one for each timeframe.\n        for meta, age in enumerate(ages):\n            best = (-1, -1)\n            for predictor in self.predictors:\n                best = predictor.bestguess(age, best)\n            self.predict_meta[meta].addguess(them, best[0])\n\n        # Finally choose the best meta prediction from the final six, scoring\n        # these against each other on the whole-game timeframe. \n        best = (-1, -1)\n        for meta in range(len(ages)):\n            best = self.predict_meta[meta].bestguess(len(self.histories[0]), best)\n\n            # We've picked a next move.    Record our move in histories[0] for next time.\n        self.histories[0].append(best[0])\n\n        # And return it.\n        return best[0]\n\n\niocaine = None\n\n\ndef iocaine(observation, configuration):\n    global iocaine\n    if observation.step == 0:\n        iocaine = Iocaine()\n        act = iocaine.move(-1)\n    else:\n        act = iocaine.move(observation.lastOpponentAction)\n\n    return act\n","1cec8c9e":"@dataclass\nclass Observation:\n    lastOpponentAction: int\n    step: int\n\n\n@dataclass\nclass Configuration:\n    episode_steps: int = ROUNDS\n    signs: int = 3\n\nREWARDS = [[0, -1, 1], [1, 0, -1], [-1, 1, 0]]\n\nclass Bot:\n    def __init__(self, path, name):\n        self.path = path\n        self.name = name\n        self.step = 0\n        self._load(path)\n\n    def _load(self, path):\n        with open(path) as f:\n            code = f.read()\n        env = {}\n        code = compile(code, \"<string>\", \"exec\")\n        exec(code, env)\n        self.bot_method = [v for v in env.values() if callable(v)][-1]\n\n    def move(self, prev):\n        c = Configuration()\n        o = Observation(lastOpponentAction=prev, step=self.step)\n        self.step += 1\n        return self.bot_method(o, c)\n\n\nclass Stats:\n    def __init__(self, bots):\n        self.bots = bots\n        self.n_bots = n = len(bots)\n        self.matches = [0] * n\n        self.wins = [0] * n\n        self.ties = [0] * n\n        self.loses = [0] * n\n        self.win_rate = [0] * n\n        self.ratings = [Rating() for _ in range(n)]\n        self.writers = [SummaryWriter(os.path.join(OUTPUT_DIR, b['name']), flush_secs=10) for b in bots]\n\n    def update(self, bot1, bot2, reward):\n        self.matches[bot1] += 1\n        self.matches[bot2] += 1\n        if reward > 0:\n            self.wins[bot1] += 1\n            self.loses[bot2] += 1\n            self.ratings[bot1], self.ratings[bot2] = rate_1vs1(self.ratings[bot1], self.ratings[bot2])\n        elif reward < 0:\n            self.wins[bot2] += 1\n            self.loses[bot1] += 1\n            self.ratings[bot2], self.ratings[bot1] = rate_1vs1(self.ratings[bot2], self.ratings[bot1])\n        else:\n            self.ties[bot1] += 1\n            self.ties[bot2] += 1\n            self.ratings[bot1], self.ratings[bot2] = rate_1vs1(self.ratings[bot1], self.ratings[bot2], drawn=True)\n        self.win_rate[bot1] = self.wins[bot1] \/ self.matches[bot1]\n        self.win_rate[bot2] = self.wins[bot2] \/ self.matches[bot2]\n        self.writers[bot1].add_scalar('win_rate', self.win_rate[bot1], self.matches[bot1])\n        self.writers[bot2].add_scalar('win_rate', self.win_rate[bot2], self.matches[bot2])\n        self.writers[bot1].add_scalar('rating_mu', self.ratings[bot1].mu, self.matches[bot1])\n        self.writers[bot2].add_scalar('rating_mu', self.ratings[bot2].mu, self.matches[bot2])\n        self.writers[bot1].add_scalar('rating_sigma', self.ratings[bot1].sigma, self.matches[bot1])\n        self.writers[bot2].add_scalar('rating_sigma', self.ratings[bot2].sigma, self.matches[bot2])\n        self.writers[bot1].add_scalar('matches_played', self.matches[bot1], self.matches[bot1])\n        self.writers[bot2].add_scalar('matches_played', self.matches[bot2], self.matches[bot2])\n        self.writers[bot1].add_scalar('wins', self.wins[bot1], self.matches[bot1])\n        self.writers[bot2].add_scalar('wins', self.wins[bot2], self.matches[bot2])\n        self.writers[bot1].add_scalar('ties', self.ties[bot1], self.matches[bot1])\n        self.writers[bot2].add_scalar('ties', self.ties[bot2], self.matches[bot2])\n        self.writers[bot1].add_scalar('loses', self.loses[bot1], self.matches[bot1])\n        self.writers[bot2].add_scalar('loses', self.loses[bot2], self.matches[bot2])\n        \ndef run_match(bot1, bot2):\n    reward = 0\n    prev = [None, None]\n    for i in range(ROUNDS):\n        prev = [bot1.move(prev[1]), bot2.move(prev[0])]\n        reward += REWARDS[prev[0]][prev[1]]\n    return reward\n\n\ndef matchmaking(n_bots, n_matches=None):\n    while 1:\n        i1 = random.randrange(n_bots)\n        i2 = random.randrange(n_bots)\n        if i1 != i2:\n            yield i1, i2\n            if n_matches is not None:\n                n_matches -= 1\n                if n_matches <= 0:\n                    break\n\n\ndef worker(bot_indexes):\n    match_bots = [Bot(path=bots[i]['path'], name=bots[i]['name']) for i in bot_indexes]\n    return bot_indexes, run_match(*match_bots)","fd6af59b":"# unfortunately kaggle tensorboard extension is not working see https:\/\/www.kaggle.com\/product-feedback\/89671\n# %load_ext tensorboard\n# %tensorboard --logdir=runs\n\n# that is why we have to use ngrok to proxy requests\n!wget -q https:\/\/bin.equinox.io\/c\/4VmDzA7iaHb\/ngrok-stable-linux-amd64.zip\n!unzip -q ngrok-stable-linux-amd64.zip","dff423a9":"os.system(f'tensorboard --logdir {OUTPUT_DIR} &')\nos.system('.\/ngrok http 6006 &')\n!sleep 2 && curl -s http:\/\/localhost:4040\/api\/tunnels | python3 -c \"import sys, json; print('Tensorboard link', json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n!echo Note that it might take a few minutes for initial tensorboard load because it loads a few MB script bundle, so be patient","b917c1ba":"# Clear logs folder for repeated runs\n!rm -rf runs","a9e0224a":"bots = []\nfor b in glob(BOTS_DIR + '\/*.py'):\n    name = os.path.basename(b)\n    for i in range(COPIES[name]):\n        bots.append(dict(path=b, name=f'{name}_{i:02d}'))\n\nprint('Number of bots', len(bots))\nprint('Processes', PROCESSES)\nprint('Rounds per match', ROUNDS)\nprint('Matches per bot', MATCHES_PER_BOT)\n\nstats = Stats(bots)\n\ntotal_matches = len(bots) * MATCHES_PER_BOT \/\/ 2\n\nwith Pool(PROCESSES) as pool:\n    for res in tqdm(pool.imap_unordered(worker, matchmaking(len(bots), total_matches)), total=total_matches):\n        stats.update(res[0][0], res[0][1], res[1])","8d848ad9":"# Bots definition","e4351ea3":"As we have only limited number of matches played per day, we might want to evaluate our agents faster locally. There were notebooks where agents played one with another, thus number of games was $O(N^2)$. But as number of agents grows, it becomes quite long to wait. \n\nSo here I decided to implement something similar to the official matchmaking and scoring. I don't know exactly how scores are calculated, but I used *TrueSkill* ratings (there is really good introduction to it and python package http:\/\/www.moserware.com\/2010\/03\/computing-your-skill.html), based on this description I believe it might be quite similar to what used in the official competition. \n\nFor comparison I added simple **win_rate** progress. You can see, that while ratings are quite noisy even after 1000 games, **win_rate** converges much beter. I didn't implement all nuances of current matchmaking and scoring, so in reality it might be different, but what we observe so far, it is noisy indeed. \n\nIf you have ideas how to make scoring closer to the official implementaion or how to enhance it, please let me know :)\n\n**UPD:**\nI have found that setting TrueSkill _tau_ parameter to 1\/1000 of _sigma_ instead of recommended 1\/100 produces much smoother results. So after enough games the scores are quite well separated and win_rate and more sophisticated rating calculations look pretty similar. The problem is that our agents play only about 5-6 episodes a day or 150-200 a month and that might be not enough, especially for agents submitted at a later time.\n\n![](https:\/\/i.imgur.com\/guoZA8x.png)","c2b85608":"# Some constants"}}