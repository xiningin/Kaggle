{"cell_type":{"abcc2901":"code","154b06b3":"code","5824cffe":"code","b45b088f":"code","d8afac6f":"code","8b178483":"code","0a457299":"code","d3283c41":"code","d65843e1":"code","d0a4a6cc":"code","3ec36bf3":"code","8ad6b8a5":"code","30cf1707":"code","82554e0c":"code","1ec87bd6":"code","c68798ba":"code","4eb3f7c0":"code","a99d2bca":"code","442c1eb4":"code","c6e0fbb0":"code","a15e234c":"code","38178ca8":"code","c5f8ae5d":"markdown","5ba2202e":"markdown","0c046f18":"markdown","3b8a7541":"markdown","3baf4151":"markdown","39ac235f":"markdown","e9c65aaf":"markdown","25430678":"markdown","b950e123":"markdown"},"source":{"abcc2901":"pip install lifelines","154b06b3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom lifelines import KaplanMeierFitter\n\n# Import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingRegressor\n###\nfrom sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, classification_report,accuracy_score,recall_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split,GridSearchCV\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5824cffe":"df = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\ndf.head()","b45b088f":"df.info()","d8afac6f":"df.describe()","8b178483":"g_H = df.loc[df['ejection_fraction'] > 55]\nb_H = df.loc[df['ejection_fraction'] < 55]\nf_d = df.loc[df['sex'] == 0]\nm_d = df.loc[df['sex'] == 1]\n\ncnd_d = df.loc[df['diabetes'] == 0]\ncd_d = df.loc[df['diabetes'] == 1]\n\nns_d = df.loc[df['smoking'] == 0]\ns_d = df.loc[df['smoking'] == 1]\n\ncnbp_d = df.loc[df['high_blood_pressure'] == 1]\ncbp_d = df.loc[df['high_blood_pressure'] == 0]","0a457299":"g_H['DEATH_EVENT'].value_counts().plot(kind = 'bar', title = 'Good Ejection fraction', xlabel = 'Death', legend = 'Death_event')\nplt.ylabel('Count')\nplt.legend({'0: Alive, 1: Dead'})\nplt.show()\nb_H['DEATH_EVENT'].value_counts().plot(kind = 'bar', title = 'Bad Ejection fraction', xlabel = 'Death')\nplt.ylabel('Count')\nplt.legend({'0: Alive, 1: Dead'})\nplt.show()","d3283c41":"sns.boxplot(x = 'sex', y ='ejection_fraction', data = df)\nplt.legend({'0: Female', '1: Male'})\nplt.ylabel(\"Ejection Fraction\")\nplt.show()","d65843e1":"plt.hist(df.ejection_fraction)\nplt.title('Ejection Fraction of Patients')\nplt.xlabel('Ejection Fraction')\nplt.ylabel('count')\nplt.show()\n\nsns.distplot(m_d.ejection_fraction)\nplt.title('Ejection Fraction of Males')\nplt.show()\nsns.distplot(f_d.ejection_fraction)\nplt.title('Ejection Fraction of Females')\nplt.show()\nsns.distplot(ns_d.ejection_fraction)\nplt.title('Ejection Fraction of Non-Smokers')\nplt.show()\nsns.distplot(s_d.ejection_fraction)\nplt.title('Ejection Fraction of Smokers')\nplt.show()","d0a4a6cc":"import scipy.stats as ss \nt_stat, p_val= ss.ttest_ind(m_d.ejection_fraction,f_d.ejection_fraction)\nprint('P-Value of ejection between Genders - ', p_val)","3ec36bf3":"kmf = KaplanMeierFitter()\nkmf_g = KaplanMeierFitter()\nkmf_ba = KaplanMeierFitter()\nkmf_f = KaplanMeierFitter()\nkmf_m = KaplanMeierFitter()\n\nkmf_nd = KaplanMeierFitter()\nkmf_d = KaplanMeierFitter()\nkmf_nb = KaplanMeierFitter()\nkmf_b = KaplanMeierFitter()","8ad6b8a5":"kmf.fit(df['time'], df['DEATH_EVENT'], label= 'All Patients')\nkmf.plot()\nplt.title(\"The Kaplan-Meier Estimate\")\nplt.xlabel(\"Days Passed\")\nplt.ylabel(\"Survival Probability\")\nplt.show()\n\nkmf_g.fit(g_H['time'], g_H['DEATH_EVENT'], label = 'Good Ejection Fraction')\nkmf_ba.fit(b_H['time'], b_H['DEATH_EVENT'], label = 'Bad Ejection Fraction')\nkmf_g.plot()\nkmf_ba.plot()\nplt.title(\"Ejection Fraction - KMF\")\nplt.xlabel(\"Days Passed\")\nplt.ylabel(\"Survival Probability\")\nplt.show()\n\nkmf_g.fit(f_d['time'], f_d['DEATH_EVENT'], label = 'Female')\nkmf_b.fit(m_d['time'], m_d['DEATH_EVENT'], label = 'Male')\nkmf_g.plot()\nkmf_b.plot()\nplt.title(\"Sex - KMF\")\nplt.xlabel(\"Days Passed\")\nplt.ylabel(\"Survival Probability\")\nplt.show()\n\n\nkmf_nd.fit(cnd_d['time'],cnd_d['DEATH_EVENT'], label = 'No Diabetes')\nkmf_d.fit(cd_d['time'], cd_d['DEATH_EVENT'], label = 'Diabetes')\nkmf_nd.plot()\nkmf_d.plot()\nplt.title(\"Diabetes - KMF\")\nplt.xlabel(\"Days Passed\")\nplt.ylabel(\"Survival Probability\")\nplt.show()\n\nkmf_nb.fit(cnbp_d['time'], cnbp_d['DEATH_EVENT'], label = 'No BP')\nkmf_b.fit(cbp_d['time'], cbp_d['DEATH_EVENT'], label = 'BP')\nkmf_nb.plot()\nkmf_b.plot()\nplt.title(\"BP - KMF\")\nplt.xlabel(\"Days Passed\")\nplt.ylabel(\"Survival Probability\")\nplt.show()","30cf1707":"df2 = df\ny = df2['DEATH_EVENT'].values\nX = df2.drop('DEATH_EVENT', axis= 1).values\n\n\n# Split into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, stratify= y)","82554e0c":"knn = KNeighborsClassifier(n_neighbors=6)\n\n# Fit the classifier to the training data\nknn.fit(X_train, y_train)\n\n# Predict the labels of the test data: y_pred\ny_pred = knn.predict(X_test)\n\n# Generate the confusion matrix and classification report\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n# Evaluate test-set roc_auc_score\nKNN_6_Classifier_roc = roc_auc_score(y_test, y_pred)\n\n# Print roc_auc_score\nprint('ROC AUC score: {:.2f}'.format(KNN_6_Classifier_roc))\n\nfpr, tpr, _ = roc_curve(y_test,  y_pred)\nauc2 = roc_auc_score(y_test, y_pred)\nplt.plot(fpr,tpr,label=\"KNN-6-Classifier, auc=\"+str(auc2))\nplt.legend(loc=4)\nplt.show()","1ec87bd6":"# Create the classifier: logreg\nlogreg = LogisticRegression()\n\n# Fit the classifier to the training data\nlogreg.fit(X_train, y_train)\n\n# Predict the labels of the test set: y_pred\ny_pred = logreg.predict(X_test)\n\n# Compute and print the confusion matrix and classification report\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\nLogReg_ROC = roc_auc_score(y_test, y_pred)\n\n# Print roc_auc_score\nprint('ROC AUC score: {:.2f}'.format(LogReg_ROC))\n\nfpr, tpr, _ = roc_curve(y_test,  y_pred)\nauc2 = roc_auc_score(y_test, y_pred)\nplt.plot(fpr,tpr,label=\"LogisticRegression, auc=\"+str(auc2))\nplt.legend(loc=4)\nplt.show()","c68798ba":"# Create the hyperparameter grid\nc_space = np.logspace(-5, 8, 15)\nparam_grid = {'C': c_space, 'penalty': ['l1', 'l2']}\n\n# Instantiate the logistic regression classifier: logreg\nlogreg = LogisticRegression()\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# Instantiate the GridSearchCV object: logreg_cv\nlogreg_cv = GridSearchCV(logreg, param_grid=param_grid, cv = 5)\n\n# Fit it to the training data\nlogreg_cv.fit(X_train, y_train)","4eb3f7c0":"y_pred = logreg_cv.predict(X_test)\n# Print the optimal parameters and best score\nprint(\"Tuned Logistic Regression Parameter: {}\".format(logreg_cv.best_params_))\nprint(\"Tuned Logistic Regression Accuracy: {}\".format(logreg_cv.best_score_))\n\nLogReg_CV = roc_auc_score(y_test, y_pred)\n\n# Print roc_auc_score\nprint('ROC AUC score: {:.2f}'.format(LogReg_CV))\n\nfpr, tpr, _ = roc_curve(y_test,  y_pred)\nplt.plot(fpr,tpr,label=\"LogisticRegression_CV, auc=\"+str(LogReg_CV))\nplt.legend(loc=4)\nplt.show()","a99d2bca":"# Setup the pipeline steps: steps\nsteps = [('scaler', StandardScaler()),\n        ('logreg', LogisticRegression())]\n\n# Create the pipeline: pipeline\npipeline = Pipeline(steps)\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Fit the pipeline to the training set: knn_scaled\nknn_scaled = pipeline.fit(X_train, y_train)\n\n# Predict the labels of the test set: y_pred\ny_pred = knn_scaled.predict(X_test)\n\n# Instantiate and fit a k-NN classifier to the unscaled data\nknn_unscaled = KNeighborsClassifier().fit(X_train, y_train)\n\n# Compute and print metrics\nprint('Accuracy with Scaling: {}'.format(knn_scaled.score(X_test, y_test)))\nprint('Accuracy without Scaling: {}'.format(knn_unscaled.score(X_test, y_test)))\n\nLogReg_Scaled = roc_auc_score(y_test, y_pred)\n\n# Print roc_auc_score\nprint('ROC AUC score: {:.2f}'.format(LogReg_Scaled))\n\nfpr, tpr, _ = roc_curve(y_test,  y_pred)\nplt.plot(fpr,tpr,label=\"LogisticRegression_Scaled, auc=\"+str(LogReg_Scaled))\nplt.legend(loc=4)\nplt.show()","442c1eb4":"# Instantiate dt\ndt = DecisionTreeClassifier(max_depth=2, random_state=1)\n\n# Instantiate ada\nada_dt = AdaBoostClassifier(base_estimator=dt, n_estimators=180, random_state=1)\n\n# Fit ada to the training set\nada_dt.fit(X_train,y_train)\n\n# Compute the probabilities of obtaining the positive class\ny_pred_prob_dt = ada_dt.predict_proba(X_test)[:,1]\n\n# Evaluate test-set roc_auc_score\nada_dt_roc_auc = roc_auc_score(y_test, y_pred_prob_dt)\n\n# Print roc_auc_score\nprint('ROC AUC score: {:.2f}'.format(ada_dt_roc_auc))\n\nfpr, tpr, _ = roc_curve(y_test,  y_pred_prob_dt)\nauc1 = roc_auc_score(y_test, y_pred_prob_dt)\nplt.plot(fpr,tpr,label=\"Ada_boost_DT, auc=\"+str(auc1))\nplt.legend(loc=4)\nplt.show()","c6e0fbb0":"rfc = RandomForestClassifier()\n# Instantiate ada\nada_rf = AdaBoostClassifier(base_estimator=rfc, n_estimators=180, random_state=1)\n\n# Fit ada to the training set\nada_rf.fit(X_train,y_train)\n\n# Compute the probabilities of obtaining the positive class\ny_pred_prob_rf = ada_rf.predict_proba(X_test)[:,1]\n\n# Evaluate test-set roc_auc_score\nada_rf_roc_auc = roc_auc_score(y_test, y_pred_prob_rf)\n\n# Print roc_auc_score\nprint('ROC AUC score: {:.2f}'.format(ada_rf_roc_auc))\n\nfpr, tpr, _ = roc_curve(y_test,  y_pred_prob_rf)\nauc2 = roc_auc_score(y_test, y_pred_prob_rf)\nplt.plot(fpr,tpr,label=\"Ada_boost_RF, auc=\"+str(auc2))\nplt.legend(loc=4)\nplt.show()","a15e234c":"# Instantiate a DecisionTreeClassifier 'dt' with a maximum depth of 6\ndt = DecisionTreeClassifier(max_depth=3)\n\n# Fit dt to the training set\n\ndt.fit(X_train, y_train)\n\n# Predict test set labels\ny_pred = dt.predict(X_test)\n\n# Compute test set accuracy  \nacc = accuracy_score(y_test, y_pred)\nprint(\"Test set accuracy: {:.2f}\".format(acc))\n\n# Evaluate test-set roc_auc_score\nDT_AUC = roc_auc_score(y_test, y_pred)\n\n# Print roc_auc_score\nprint('ROC AUC score: {:.2f}'.format(DT_AUC))\n\nfpr, tpr, _ = roc_curve(y_test,  y_pred)\nplt.plot(fpr,tpr,label=\"Ada_boost_RF, auc=\"+str(DT_AUC))\nplt.legend(loc=4)\nplt.show()","38178ca8":"from sklearn import tree\ndf2 = df\ny = df2['DEATH_EVENT']\nX = df2.drop('DEATH_EVENT', axis= 1)\nfig = plt.figure(figsize=(25,20))\n\n_ = tree.plot_tree(dt, \n                   feature_names=X.columns,  \n                   class_names='AD',\n                   filled=True)","c5f8ae5d":"**Null Hypothesis** - There is no difference in Ejection Fraction between Genders. \n\n**Alternative Hypothesis** - There is a difference in Ejection Fraction between Genders. ","5ba2202e":"# Models","0c046f18":"# Unsupervised Models","3b8a7541":"# Supervised Models","3baf4151":"# Exploratory Data Analysis","39ac235f":"# Survial Analysis","e9c65aaf":"**As Many know a low EF is a potential factor for a Cardiac Event. Looking at the DT above, we see Serum_Creatinine & platelets levels are another possible factor.**","25430678":"# The Dataset","b950e123":"With a P-Value less than .05 we can **reject** the **Null Hypothesis** and **accept** the **Alternative Hypothesis** that there is a difference in EF between Genders."}}