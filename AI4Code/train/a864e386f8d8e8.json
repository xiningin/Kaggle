{"cell_type":{"4acab3c6":"code","59e9c7c0":"code","bd2d5bdb":"code","60c007d3":"code","78cff18f":"code","51e89958":"code","3a0a09ad":"code","c960d584":"code","36e4efa2":"code","c2c7c1b5":"code","78fd7311":"code","f876417f":"code","dfa25ffe":"code","3b891aaf":"code","47341c4a":"code","bdfd8a76":"code","a32538ff":"code","a46bf0db":"code","13b63284":"code","15415229":"code","b5b7bd6d":"code","0949b7ca":"code","d1ec9925":"code","b25d0f6b":"code","4e311e21":"code","8b46741a":"code","18ccb8dc":"code","0c680f72":"code","6a740368":"code","5701c354":"code","ba159559":"code","4a8a8ece":"code","de57e801":"code","88517852":"code","47c44aa9":"code","f31fbdaf":"code","693b572d":"markdown","7b59a462":"markdown","c287067b":"markdown","e5cb49f0":"markdown","4da14aeb":"markdown","c5a32365":"markdown","e7f309b5":"markdown","74ad59dd":"markdown","45775c4a":"markdown","b84ae04c":"markdown"},"source":{"4acab3c6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","59e9c7c0":"df_train = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')","bd2d5bdb":"print(df_train.info())\nprint('*****')\nprint(df_test.info())","60c007d3":"print(df_train.shape)\nprint(df_test.shape)","78cff18f":"features = [col for col in df_train.columns if 'f' in col]","51e89958":"print(features)","3a0a09ad":"sns.countplot(x = 'claim', data = df_train)","c960d584":"import matplotlib.pyplot as plt\n\nfor idx, feature in enumerate(features):\n    plt.hist(df_train[feature], bins=30, alpha=0.5, label='Train set')\n    plt.hist(df_test[feature], bins=30, alpha=0.5, label='Test set')\n    plt.title(feature + \" Train\/Test\")\n    plt.xlabel(feature)\n    plt.ylabel('Frequency')\n\n    plt.legend()\n    plt.show()","36e4efa2":"df_train['missing'] = df_train[features].isna().sum(axis=1)\ndf_test['missing'] = df_test[features].isna().sum(axis=1)\n\nfeatures.append('missing')","c2c7c1b5":"train_missing = df_train['missing'].unique()\ntest_missing = df_test['missing'].unique()","78fd7311":"X = df_train[features].copy()\ny = df_train['claim'].copy()\n\nx_test = df_test[features].copy()","f876417f":"train_missing.sort()\ntest_missing.sort()","dfa25ffe":"### Plot the missing value data for train set\ntotal = []\nones = []\nzeros = []\nfor val in train_missing:\n    total.append((df_train[df_train['missing']==val]).shape[0])\n    ones.append((df_train[df_train['missing']==val]['claim']==1).sum())\n    zeros.append((df_train[df_train['missing']==val]['claim']==0).sum())\n#print(np.add(ones,zeros))\n#print(total)\n\nplt.bar(train_missing, ones, alpha=0.5, label='Claim = 1')\nplt.bar(train_missing, zeros, alpha=0.5, label='Claim = 0')\nplt.title(\" 0\/1\")\nplt.xlabel('0\/1')\nplt.ylabel('Frequency')\n\nplt.legend()\nplt.show()","3b891aaf":"X = df_train[features].copy()\ny = df_train['claim'].copy()\n\nx_test = df_test[features].copy()","47341c4a":"### Method 1\n\n#from scipy.stats import skew\n\n#skew_feat = X.skew()\n#skew_feat = list([abs(skew_feat.values)>1].index)\n\n#for feat in skew_feat:\n#    median = X[feat].median()\n#    X[feat] = X[feat].fillna(median)\n    \n#skew_feat = x_test.skew()\n#skew_feat = list([abs(skew_feat.values)>1].index)\n\n#for feat in skew_feat:\n#    median = df_test[feat].median()\n#    df_test[feat] = df_test[feat].fillna(median)","bdfd8a76":"### Method 2\n\nfrom scipy.stats import skew\n\nskew_feat = X.skew()\nskew_feat = list(skew_feat[abs(skew_feat.values)>1].index)\n\n\nfor feat in skew_feat:\n    skew_val = abs(X[feat].skew())\n    \n    col_log = pd.Series((np.log(np.abs(X[feat]))) * np.sign(X[feat]))\n    skew_log = abs(col_log.skew())\n    \n    col_sqrt = pd.Series((np.sqrt(np.abs(X[feat])))*np.sign(X[feat]))\n    skew_sqrt = abs(col_sqrt.skew())\n    \n    if skew_log<skew_val and skew_log<skew_sqrt:\n        X[feat] = col_log\n        x_test[feat] = pd.Series((np.log(np.abs(x_test[feat]))) * np.sign(x_test[feat]))\n    if skew_sqrt<skew_val and skew_sqrt<skew_val:\n        X[feat] = col_sqrt\n        x_test[feat] = pd.Series((np.sqrt(np.abs(x_test[feat])))*np.sign(x_test[feat]))","a32538ff":"print(X.isna().sum().sum())\nprint(x_test.isna().sum().sum())\n\nprint(type(X))\nprint(type(x_test))","a46bf0db":"### Filling the remaining NA vakues with mean\nX[features] = X[features].fillna(X[features].mean())\nx_test[features] = x_test[features].fillna(x_test[features].mean())","13b63284":"print(X.isna().sum().sum())\nprint(x_test.isna().sum().sum())","15415229":"from sklearn.preprocessing import RobustScaler\n\nscaler = RobustScaler()\nX[features] = scaler.fit_transform(X[features])\nx_test[features] = scaler.transform(x_test[features])","b5b7bd6d":"### Hyperparameters list\n\nlgb_params = {\n    'metric' : 'auc',\n    'objective' : 'binary',\n    'device_type': 'gpu', \n    'n_estimators': 10000, \n    'learning_rate': 0.12230165751633416, \n    'num_leaves': 1400, \n    'max_depth': 8, \n    'min_child_samples': 3100, \n    'reg_alpha': 10, \n    'reg_lambda': 65, \n    'min_split_gain': 5.157818977461183, \n    'subsample': 0.5, \n    'subsample_freq': 1, \n    'colsample_bytree': 0.2\n}\n\ncatb_params = {\n    'eval_metric' : 'AUC',\n    'iterations': 15585, \n    'objective': 'CrossEntropy',\n    'bootstrap_type': 'Bernoulli', \n    'od_wait': 1144, \n    'learning_rate': 0.023575206684596582, \n    'reg_lambda': 36.30433203563295, \n    'random_strength': 43.75597655616195, \n    'depth': 7, \n    'min_data_in_leaf': 11, \n    'leaf_estimation_iterations': 1, \n    'subsample': 0.8227911142845009,\n    'task_type' : 'GPU',\n    'devices' : '0',\n    'verbose' : 0\n}\n\nxgb_params = {\n    'eval_metric': 'auc', \n    'objective': 'binary:logistic', \n    'tree_method': 'gpu_hist', \n    'gpu_id': 0, \n    'predictor': 'gpu_predictor', \n    'n_estimators': 10000, \n    'learning_rate': 0.01063045229441343, \n    'gamma': 0.24652519525750877, \n    'max_depth': 4, \n    'min_child_weight': 366, \n    'subsample': 0.6423040816299684, \n    'colsample_bytree': 0.7751264493218339, \n    'colsample_bylevel': 0.8675692743597421, \n    'lambda': 0, \n    'alpha': 10\n}\n","0949b7ca":"### LGBM Model\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nimport lightgbm as lgb\n\nlgb_oof = np.zeros(X.shape[0])\nlgb_pred = np.zeros(x_test.shape[0])\nbest_lgb_model = None\nbest_roc_score_lgb = 0\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, (trn_idx, val_idx) in enumerate(cv.split(X, y)):\n    print(f\"===== fold {fold} =====\")\n    X_train = X[features].iloc[trn_idx]\n    y_train = y.iloc[trn_idx]\n    X_valid = X[features].iloc[val_idx]\n    y_valid = y.iloc[val_idx]\n    X_test = x_test[features]\n    \n    model = lgb.LGBMClassifier(**lgb_params)\n    model.fit(\n        X_train, \n        y_train,\n        eval_set=[(X_valid, y_valid)],\n        eval_metric='auc',\n        early_stopping_rounds=200,\n        verbose=0,\n    )\n\n    lgb_oof[val_idx] = model.predict_proba(X_valid)[:, -1]\n    lgb_pred += model.predict_proba(X_test)[:, -1]\n\n    auc = roc_auc_score(y_valid, lgb_oof[val_idx])\n    \n    if auc>best_roc_score_lgb:\n        best_roc_score_lgb = auc\n        best_lgb_model = model\n        \n    print(f\"fold {fold} - lgb auc: {auc:.6f}\\n\")\n\nprint(f\"oof lgb roc = {roc_auc_score(y, lgb_oof)}\")\nlgb_pred = lgb_pred\/5","d1ec9925":"feature_impt=pd.DataFrame()\nfeature_impt['features']=best_lgb_model.feature_name_\nfeature_impt['importance']=best_lgb_model.feature_importances_\n\nfeature_impt.sort_values(by=['importance'],inplace=True,ascending=False)\nplt.figure(figsize = (20,25))\nsns.barplot(x=feature_impt['importance'],y=feature_impt['features'],data=feature_impt);","b25d0f6b":"final_lgb = pd.DataFrame()\nfinal_lgb['id'] = df_test['id']\nfinal_lgb['claim'] = lgb_pred","4e311e21":"#final_lgb.to_csv('final_lgb', index=False)","8b46741a":" ### XGBoost Model\n    \nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nimport xgboost as xgb\n\nxgb_oof = np.zeros(X.shape[0])\nxgb_pred = np.zeros(x_test.shape[0])\nbest_xgb_model = None\nbest_roc_score_xgb = 0\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, (trn_idx, val_idx) in enumerate(cv.split(X, y)):\n    print(f\"===== fold {fold} =====\")\n    X_train = X[features].iloc[trn_idx]\n    y_train = y.iloc[trn_idx]\n    X_valid = X[features].iloc[val_idx]\n    y_valid = y.iloc[val_idx]\n    X_test = x_test[features]\n    \n    model = xgb.XGBClassifier(**xgb_params)\n    model.fit(\n        X_train, \n        y_train,\n        eval_set=[(X_valid, y_valid)],\n        eval_metric='auc',\n        early_stopping_rounds=200,\n        verbose=0,\n    )\n\n    xgb_oof[val_idx] = model.predict_proba(X_valid)[:, -1]\n    xgb_pred += model.predict_proba(X_test)[:, -1]\n\n    auc = roc_auc_score(y_valid, xgb_oof[val_idx])\n    \n    if auc>best_roc_score_xgb:\n        best_roc_score_xgb = auc\n        best_xgb_model = model\n        \n    print(f\"fold {fold} - xgb auc: {auc:.6f}\\n\")\n\nprint(f\"oof xgb roc = {roc_auc_score(y, xgb_oof)}\")\nxgb_pred = xgb_pred\/5","18ccb8dc":"feature_impt=pd.DataFrame(list(best_xgb_model.get_booster().get_fscore().items()),\ncolumns=['feature','importance']).sort_values('importance', ascending=False)\n\nfeature_impt.sort_values(by=['importance'],inplace=True,ascending=False)\nplt.figure(figsize = (20,25))\nsns.barplot(x=feature_impt['importance'],y=feature_impt['feature'],data=feature_impt);","0c680f72":"final_xgb = pd.DataFrame()\nfinal_xgb['id'] = df_test['id']\nfinal_xgb['claim'] = xgb_pred","6a740368":"#final_xgb.to_csv('final_xgb', index=False)","5701c354":"### CATBoost model\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nimport catboost as catb\n\ncatb_oof = np.zeros(X.shape[0])\ncatb_pred = np.zeros(x_test.shape[0])\nbest_catb_model = None\nbest_roc_score_catb = 0\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, (trn_idx, val_idx) in enumerate(cv.split(X, y)):\n    print(f\"===== fold {fold} =====\")\n    X_train = X[features].iloc[trn_idx]\n    y_train = y.iloc[trn_idx]\n    X_valid = X[features].iloc[val_idx]\n    y_valid = y.iloc[val_idx]\n    X_test = x_test[features]\n    \n    model = catb.CatBoostClassifier(**catb_params)\n    model.fit(\n        X_train, \n        y_train,\n        eval_set=[(X_valid, y_valid)],\n        early_stopping_rounds=200,\n        verbose=0,\n    )\n\n    catb_oof[val_idx] = model.predict_proba(X_valid)[:, -1]\n    catb_pred += model.predict_proba(X_test)[:, -1]\n\n    auc = roc_auc_score(y_valid, catb_oof[val_idx])\n    \n    if auc>best_roc_score_catb:\n        best_roc_score_catb = auc\n        best_catb_model = model\n        \n    print(f\"fold {fold} - catb auc: {auc:.6f}\\n\")\n\nprint(f\"oof catb roc = {roc_auc_score(y, catb_oof)}\")\ncatb_pred = catb_pred\/5","ba159559":"feature_impt=pd.DataFrame()\n#feature_impt['features']=best_catb_model.feature_name_\n#feature_impt['importance']=best_catb_model.feature_importances_\n\n#feature_impt.sort_values(by=['importance'],inplace=True,ascending=False)\n#plt.figure(figsize = (20,25))\n#sns.barplot(y=feature_impt['features'],data=feature_impt);","4a8a8ece":"final_catb = pd.DataFrame()\nfinal_catb['id'] = df_test['id']\nfinal_catb['claim'] = catb_pred","de57e801":"#final_catb.to_csv('final_catb', index=False)","88517852":"final_lgb_xgb = (xgb_pred +lgb_pred)\/2","47c44aa9":"final_lgb_xgb_csv = pd.DataFrame()\nfinal_lgb_xgb_csv['id'] = df_test['id']\nfinal_lgb_xgb_csv['claim'] = final_lgb_xgb","f31fbdaf":"final_lgb_xgb_csv.to_csv('submission', index=False)","693b572d":"### It is visible from the feature importance that missing is an important feature","7b59a462":"As it was visible in the plots of training and test data the distribution is skewed. There are two ways I am dealing with the skewness\n1. Fill the missing values of columns with skew values greater than 1 with the median value and the rest with mean value\n2. Replace the column with its log value or sqrt value whichever has less skew value. Fill the missing values with mean\n\n### Uncomment one of the following two cells below to run one of the two methods","c287067b":"### Observing the distribution of data in the train and test set","e5cb49f0":"### Observing the test data","4da14aeb":"Since XGBoost and LGBM models showed the best results I am trying to submit a prediction with a weighted average of 0.6 for LGBM and 0.4 for XGBoost","c5a32365":"### Conclusion\n\n1. The train and test data distribution are similar. This is great!!!\n2. The distribution of the independent features are skewed which may affect the accurarcy of our model. I have put forward some ideas on dealing with skewness in the below cells","e7f309b5":"Let's have an overview of the data","74ad59dd":"### Conclusion\nFrom the bar plot it is visible that the value of claim = 1 is  more than value of claim = 0 is where the missing values are greater than 0. For data where missing rows = 0 the value of claim =0 is significantly higher than claim = 1 ","45775c4a":"## Let us understand why missing is such an important feature","b84ae04c":"### Conclusion\nThe distribution of data with claim made and claim not made are the same. This is one less thing to worry about :)"}}