{"cell_type":{"3467b7a6":"code","c5f6e284":"code","5bc0f9c5":"code","4c26621e":"code","558a4e23":"code","ce866d42":"code","4cbd2973":"code","85d8897d":"code","0251aa34":"code","6825b9bf":"code","e9c9baef":"code","e590f65c":"code","465cad55":"code","fa5072f5":"code","658b2336":"code","940220bc":"code","e2133255":"code","ce991494":"code","bd4c145e":"code","960c07d5":"code","44707f6a":"code","484bb47d":"code","60861255":"code","b3c2fc9a":"code","a46bf414":"code","ca099229":"code","33159e62":"code","27bb9922":"code","ecd8b75e":"code","6d9aa781":"markdown","5ee071fb":"markdown","7d5b4422":"markdown","4cec617f":"markdown","06f1d015":"markdown","33d9914d":"markdown","0da9c57c":"markdown","253066ad":"markdown"},"source":{"3467b7a6":"# All libraries we will need\nimport pandas as pd # To store data as a dataframe\nimport requests # to get the data of an url\nfrom bs4 import BeautifulSoup # to parse the html data and find what we want\nimport re # Regular expressions library, it may be useful \nprint('Setup complete!')","c5f6e284":"# We need the url of the page we are gonna scrape \nurl = 'https:\/\/www.kaggle.com\/rankings?group=datasets&page=1&pageSize=20'\nresponse = requests.get(url) # Get content of page","5bc0f9c5":"# Parse the webpage text as html\npage_html = BeautifulSoup(response.text, 'html.parser') ","4c26621e":"\"\"\"\ndivsContainer = page_html.find('div', attrs={'class':'site-content'}) # Find table with id = pokedex\ndivsContainer.find_all('div')\n\"\"\"","558a4e23":"response.text","ce866d42":"jsonStr = response.text # Get all text from url\njsonStr = jsonStr.split('\"list\":[')[1] # Get everything after \"list\":[\njsonStr = jsonStr.split(');')[0] # Get everything before ); \njsonStr # The json data of the 20 first rows","4cbd2973":"# Using regular expressions we will take all the links to the profiles of the 20 first dataset rankers\nusernames = re.findall('userUrl\":\"\\\/(\\w+)\",\"tier', jsonStr) # Capture group of word with the username between userUrl\":\"\\ and \",\"tier\nlen(usernames)","85d8897d":"usernames","0251aa34":"baseUrl = 'https:\/\/www.kaggle.com\/'\ndatasetsUrl = '\/datasets'\ntop20datasetsProfiles = []\nfor username in usernames:\n    top20datasetsProfiles.append(baseUrl+username) ","6825b9bf":"top20datasetsProfiles","e9c9baef":"resp2 = requests.get(top20datasetsProfiles[0]) # Get content of page\n#resp2.text","e590f65c":"# All information we want is between \"userId\": and ); \njsonStr2 = resp2.text # Get all text from url\njsonStr2 = jsonStr2.split('\"userId\":')[1] # Get everything after \"list\":[\njsonStr2 = jsonStr2.split(');')[0] # Get everything before ); ","465cad55":"jsonStr2","fa5072f5":"# Data to extract \n\n# Bio info\ndisplayName = []\ncountry = []\nregion = []\ncity = []\ngitHubUserName = []\ntwitterUserName = []\nlinkedInUrl = []\nwebsiteUrl = []\noccupation = []\norganization = []\nuserJoinDate = []\n\n# Datasets info\ntier = []\ntotalResults = []\nrankCurrent = []\nrankHighest = []\n\ntotalGoldMedals = []\ntotalSilverMedals = []\ntotalBronzeMedals = []\n","658b2336":"bio = jsonStr2.split('\"perf')[0]\nbio","940220bc":"datasetSummary = jsonStr2.split('datasetsSummary')[1]\ndatasetSummary","e2133255":"all_data = re.findall('(?<=,)[^,]+(?=,)', jsonStr2) # all info between commas\nall_data","ce991494":"all_data[0].split(':\"')[1].split('\"')[0] # Take the second part between \"\"","bd4c145e":"def dataSplits(str1):\n    try:\n        str1 = str1.split(':\"')[1].split('\"')[0]\n    except:\n        str1 = None\n    return str1\n        ","960c07d5":"def getArrayAllContent(str2):\n    str2 = re.findall('(?<=,)[^,]+(?=\",)', str2)\n    return str2","44707f6a":"bioA = getArrayAllContent(bio)\nbioA","484bb47d":"\"\"\"\nfor profile in top20datasetsProfiles[:2]:\n    resp2 = requests.get(profile) # Get content of page\n    # All information we want is between \"userId\": and ); \n    jsonStr2 = resp2.text # Get all text from url\n    jsonStr2 = jsonStr2.split('\"userId\":')[1] # Get everything after \"list\":[\n    jsonStr2 = jsonStr2.split(');')[0] # Get everything before ); \n    \n    bio = jsonStr2.split(',\"perf')[0]\n    datasetSummary = jsonStr2.split('datasetsSummary')[1]\n    \n    bioArr = getArrayAllContent(bio)\n    datasetSummaryArr = getArrayAllContent(datasetSummary)\n    print(len(bioArr))\n    #all_data = re.findall('(?<=,)[^,]+(?=,)', jsonStr2)\n    print(dataSplits(bioArr[0]))\n    displayName.append(dataSplits(bioArr[0]))\n    country.append(dataSplits(bioArr[1]))\n    region.append(dataSplits(bioArr[2]))\n    city.append(dataSplits(bioArr[3]))\n    gitHubUserName.append(dataSplits(bioArr[4]))\n    twitterUserName.append(dataSplits(bioArr[5]))\n    linkedInUrl.append(dataSplits(bioArr[6]))\n    websiteUrl.append(dataSplits(bioArr[7]))\n    occupation.append(dataSplits(bioArr[8]))\n    organization.append(dataSplits(bioArr[9]))\n    userJoinDate.append(dataSplits(bioArr[11]))\n\"\"\"    \n    \n\n    ","60861255":"bio","b3c2fc9a":"for profile in top20datasetsProfiles:\n    resp2 = requests.get(profile) # Get content of page\n    # All information we want is between \"userId\": and ); \n    jsonStr2 = resp2.text # Get all text from url\n    jsonStr2 = jsonStr2.split('\"userId\":')[1] # Get everything after \"list\":[\n    jsonStr2 = jsonStr2.split(');')[0] # Get everything before ); \n    \n    bio = jsonStr2.split('\"perf')[0]\n    datasetSummary = jsonStr2.split('datasetsSummary')[1]\n    \n    print(re.search('displayName\":\"([^,]+)\",', bio).group(1))\n    displayName.append(re.search('displayName\":\"([^,]+)\",', bio).group(1))\n    try:\n        country.append(re.search('country\":\"([^,]+)\",', bio).group(1))\n    except:\n        country.append(None)\n        \n    try:\n        region.append(re.search('region\":\"([^,]+)\",', bio).group(1))\n    except: \n        region.append(None)\n        \n    try:\n        city.append(re.search('city\":\"([^,]+)\",', bio).group(1))\n    except:\n        city.append(None)\n        \n    try:\n        gitHubUserName.append(re.search('gitHubUserName\":\"([^,]+)\",', bio).group(1))\n    except:\n        gitHubUserName.append(None)\n        \n    try:\n        twitterUserName.append(re.search('twitterUserName\":\"([^,]+)\",', bio).group(1))\n    except: \n        twitterUserName.append(None)\n    \n    try:\n        linkedInUrl.append(re.search('linkedInUrl\":\"([^,]+)\",', bio).group(1))\n    except:\n        linkedInUrl.append(None)\n        \n    try:\n        websiteUrl.append(re.search('websiteUrl\":\"([^,]+)\",', bio).group(1))\n    except: \n        websiteUrl.append(None)\n        \n    try:    \n        occupation.append(re.search('occupation\":\"([^,]+)\",', bio).group(1))\n    except:\n        occupation.append(None)\n        \n    try:\n        organization.append(re.search('organization\":\"([^,]+)\",', bio).group(1))\n    except:\n        organization.append(None)\n        \n    userJoinDate.append(re.search('userJoinDate\":\"([^,]+)\",', bio).group(1))\n    \n    tier.append(re.search('tier\":\"([^,]+)\",', datasetSummary).group(1))\n    totalResults.append(re.search('totalResults\":([^,]+),', datasetSummary).group(1))\n    rankCurrent.append(re.search('rankCurrent\":([^,]+),', datasetSummary).group(1))\n    rankHighest.append(re.search('rankHighest\":([^,]+),', datasetSummary).group(1))\n\n    totalGoldMedals.append(re.search('totalGoldMedals\":([^,]+),', datasetSummary).group(1))\n    totalSilverMedals.append(re.search('totalSilverMedals\":([^,]+),', datasetSummary).group(1))\n    totalBronzeMedals.append(re.search('totalBronzeMedals\":([^,]+),', datasetSummary).group(1))","a46bf414":"displayName","ca099229":"datasetSummary","33159e62":"top20KagglersDatasets = pd.DataFrame({\n    'displayName':displayName,\n    'country':country,\n    'region':region,\n    'city':city,\n    'gitHubUserName':gitHubUserName,\n    'twitterUserName':twitterUserName,\n    'linkedInUrl':linkedInUrl,\n    'websiteUrl':websiteUrl,\n    'occupation':occupation,\n    'organization':organization,\n    'userJoinDate':userJoinDate,\n    'tier':tier,\n    'totalResults':totalResults,\n    'rankCurrent':rankCurrent,\n    'rankHighest':rankHighest,\n    'totalGoldMedals':totalGoldMedals,\n    'totalSilverMedals':totalSilverMedals,\n    'totalBronzeMedals':totalBronzeMedals\n\n})","27bb9922":"top20KagglersDatasets","ecd8b75e":"# Build csv\ntop20KagglersDatasets.to_csv('top20KagglersDatasets.csv', index=False)","6d9aa781":"### Use search (in chrome F3) for names of the dataset, in my case \"SRK\", the first one in datasets.\n### We can see it's inside a script tag, and it's format is JSON.\n### In this page the html table (actually divs) it's made dynamically with pushs of json data. To approach this web page we will need to obtain that json first.","5ee071fb":"### The above block return NoneType because it can't find this html data, that we encountered doing inspect element. We have to find why that happens, we'll take a look at response.text to see al information we have\n","7d5b4422":"## After starting this notebook I thought this will be easier (it was very difficult), but whatever, I think later on I will change the volume order of this saga(tutorial series).\n## The resulting dataset can be found here: https:\/\/www.kaggle.com\/ajpass\/top-datasets-kagglers-ranking\n\n\nData Mining - Web Scrapping: The saga:\n1. https:\/\/www.kaggle.com\/ajpass\/data-mining-web-scrapper-vol-1-pokedex # Scrapping a pokedex, all pokemon with all stats\n2. https:\/\/www.kaggle.com\/ajpass\/data-mining-web-scrapping-vol-2-pokedex-pandas # Scrapping a pokedex, using a diferent method than number 1, easier but in case a column has multiple values, in some cases you may need to do some cleaning. Result similar as 1, only changes: Type and ID. \n3. https:\/\/www.kaggle.com\/ajpass\/data-mining-web-scrapper-vol-3-sudoku-to-string # Extract sudokus data and transform it to string\n4. https:\/\/www.kaggle.com\/ajpass\/data-mining-web-scrapping-vol-66-kaggle-datasets # It's about scrapping the top 20 kagglers in datasets ranking\n","4cec617f":"### I was trying to print this with pretty structure but I can't get it to work. I used an online web as a subsitute.\n![formatted.PNG](attachment:formatted.PNG)","06f1d015":"Actually in this json you could find everywork he has done in the platform.","33d9914d":"### We all know that Kaggle datasets don't receive the love they need. So in this notebook we will make a web scrapper of top kagglers in the datasets ranking. \n","0da9c57c":"### As I have seen while doing some research the JSON don't have the same order or shape in some cases, so what I have been making it's no good. \n","253066ad":"### Let's make something a little diferent for this last function:"}}