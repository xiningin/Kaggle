{"cell_type":{"a34c3be2":"code","098865bd":"code","a56d0556":"code","c0d9ff55":"code","b5eb8530":"code","160f9a5c":"code","a55800b8":"code","6c66e7f1":"code","fac198c9":"code","2ad7220c":"code","3d011065":"code","53c3657a":"code","1adf9264":"code","a6c1c3f7":"code","c90d1478":"code","fbe11092":"code","0bd3095a":"code","0555119f":"code","67acf098":"code","15031f03":"code","f6f26ce2":"code","b797ca5f":"code","91ff620b":"code","db5df041":"code","2eeb348c":"code","cda5dfab":"code","7eb275b3":"code","d0fa9c05":"code","9aa3f87a":"code","dcf266aa":"code","37a78ede":"code","d455e5ea":"code","1785029f":"code","06596597":"code","cd577624":"code","ab815e0c":"code","d40d0f40":"code","135f9fe0":"code","a30cde73":"code","4818b62a":"code","7dae3039":"code","e7cb6599":"code","b2c63a38":"code","257e9429":"code","799124b3":"code","7cd98bde":"markdown","5cbef801":"markdown","b715a8ab":"markdown","bc864226":"markdown","229c9e48":"markdown","75eed4fd":"markdown","b5340119":"markdown","73f52aad":"markdown","703d6e3b":"markdown","ce1db261":"markdown","66f534d1":"markdown","8d3cf4b1":"markdown","d2403df5":"markdown","41293cb8":"markdown","021c0f4c":"markdown","166aa644":"markdown","4de8c696":"markdown","42c41d41":"markdown","e357e30b":"markdown","41854d09":"markdown","3bb387d5":"markdown","554ce882":"markdown","47fec7a1":"markdown","693c01ff":"markdown","59235d60":"markdown","f5963a6d":"markdown","4b2d4c77":"markdown","45e031c4":"markdown","0a0208cf":"markdown","806a419d":"markdown","dcf8c596":"markdown","6acdb3eb":"markdown","adebf06f":"markdown","6d6de678":"markdown","941677ae":"markdown","10f3be09":"markdown","e940a6d5":"markdown","b7e09d08":"markdown","c0172fc6":"markdown","3d52f65e":"markdown","8e156a9c":"markdown","9e37eaac":"markdown","7b152949":"markdown","a635621c":"markdown","9797fe57":"markdown","f47bcd62":"markdown","78614948":"markdown","1a1ae048":"markdown","b165e837":"markdown","7b8da348":"markdown","89bed33a":"markdown","899a20c5":"markdown","63313ee2":"markdown","7ac0b654":"markdown"},"source":{"a34c3be2":"import numpy as np\nimport pandas as pd","098865bd":"!pip install ftfy\n!pip install gensim","a56d0556":"!pip install skorch\n!pip install scikit-learn==0.23.0\n\nfrom numpy.ma import MaskedArray\nimport sklearn.utils.fixes\n\nsklearn.utils.fixes.MaskedArray = MaskedArray","c0d9ff55":"train = pd.read_csv(\"..\/input\/sentiment-analysis-pmr3508\/data_train.csv\")","b5eb8530":"train.head()","160f9a5c":"train = train.drop_duplicates(keep='first')","a55800b8":"import seaborn as sns\n\nsns.catplot(x=\"positive\", kind=\"count\", data=train)","6c66e7f1":"X_train = train['review']\ny_train = train['positive']","fac198c9":"import re\nimport string\nfrom ftfy import fix_text\n\ndef clean(text):\n    txt= text.replace(\"<br \/>\",\" \")       # Retirando tags\n    txt= fix_text(txt)                    # Consertando Mojibakes (Ver https:\/\/pypi.org\/project\/ftfy\/)\n    txt= txt.lower()                      # Passando tudo para min\u00fasculo\n    txt= re.findall(r'[a-z0-9]+', txt)    # Selecionando somente letras e n\u00fameros\n    txt= \" \".join(txt)                    # Juntando as letras e n\u00fameros\n    txt= re.sub(\"\\d+\", ' <number> ', txt) # Colocando um token especial para os n\u00fameros\n    txt= re.sub(' +', ' ', txt)           # Deletando espa\u00e7os extras\n    return txt","2ad7220c":"X_train = X_train.apply(clean)","3d011065":"X_train.head()","53c3657a":"X_train = X_train.apply(lambda x: x.split())\nX_train.head()","1adf9264":"from gensim.models.doc2vec import Doc2Vec\n\nd2v = Doc2Vec.load(\"..\/input\/sentiment-analysis-pmr3508\/doc2vec\")","a6c1c3f7":"# Calculando a similaridade entre as duas palavras\nd2v.docvecs.similarity_unseen_docs(d2v, ['one'], ['two'])","c90d1478":"# Descobrindo as palavras mais semelhantes a 'movie'\nd2v.wv.most_similar(positive=['movie'])","fbe11092":"# Descobrindo as palavras mais semelhantes a 'kubrick'\nd2v.wv.most_similar(positive=['kubrick'])","0bd3095a":"d2v.docvecs.similarity_unseen_docs(d2v, ['stanley', 'kubrick'], ['akira', 'kurosawa'])","0555119f":"def emb(txt, model, normalize=False): \n    model.random.seed(42)\n    x=model.infer_vector(txt, steps=25)\n    \n    if normalize: return (x\/np.sqrt(x@x))\n    else: return (x)","67acf098":"X_train = X_train.to_list()\nX_train = [emb(x, d2v) for x in X_train] \nX_train = np.array(X_train)\nX_train.shape","15031f03":"X_train[0]","f6f26ce2":"valid = pd.read_csv(\"..\/input\/sentiment-analysis-pmr3508\/data_test1.csv\")","b797ca5f":"X_valid = valid['review'].tolist()\ny_valid = valid['positive']","91ff620b":"X_valid = [clean(x).split() for x in X_valid]\nX_valid = [emb(x, d2v) for x in X_valid] \nX_valid = np.array(X_valid)","db5df041":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Modelo de Regress\u00e3o Log\u00edstica\nlogreg = LogisticRegression(solver='liblinear',random_state=42)\n\n# Hiperpar\u00e2metros a serem otimizados: 'C' e tipo de regulariza\u00e7\u00e3o\nhyperparams = dict(C=np.linspace(0,10,100), \n                   penalty=['l2', 'l1'])\n\n# Busca de Hiperpar\u00e2metros\nlogreg_clf = RandomizedSearchCV(logreg, hyperparams, scoring='roc_auc', n_iter=50, cv=2, n_jobs=-1, random_state=0, verbose=2)\nsearch_logreg = logreg_clf.fit(X_train, y_train)","2eeb348c":"search_logreg.best_params_, search_logreg.best_score_","cda5dfab":"from sklearn.neural_network import MLPClassifier\nfrom scipy.stats import loguniform as sp_loguniform\n\n# Modelo de Rede Neural de 1 camada escondida\nmlp_1l = MLPClassifier(random_state=42, early_stopping=True)\n\n# Hiperpar\u00e2metros a serem otimizados\nhyperparams = {'hidden_layer_sizes': [(2 ** i) for i in np.arange(6, 10)],\n               'alpha': sp_loguniform(0.000001, 0.1),\n               'learning_rate': ['constant','adaptive']}\n\n# Busca de Hiperpar\u00e2metros\nmlp_clf_1l = RandomizedSearchCV(mlp_1l, hyperparams, scoring='roc_auc', n_iter=25, cv=2, n_jobs=-1, random_state=0, verbose=2)\nsearch_mlp = mlp_clf_1l.fit(X_train, y_train)","7eb275b3":"search_mlp.best_params_, search_mlp.best_score_","d0fa9c05":"from sklearn.neural_network import MLPClassifier\nfrom scipy.stats import loguniform as sp_loguniform\n\n# Modelo de Rede Neural de 2 camadas escondidas\nmlp_2l = MLPClassifier(random_state=42, early_stopping=True)\n\n# Hiperpar\u00e2metros a serem otimizados\nhyperparams = {'hidden_layer_sizes': [(2 ** i, 2 ** j) for j in np.arange(6, 10) for i in np.arange(6, 10)],\n               'alpha': sp_loguniform(0.000001, 0.1),\n               'learning_rate': ['constant','adaptive']}\n\n# Busca de Hiperpar\u00e2metros\nmlp_clf_2l = RandomizedSearchCV(mlp_2l, hyperparams, scoring='roc_auc', n_iter=25, cv=2, n_jobs=-1, random_state=0, verbose=2)\nsearch_mlp = mlp_clf_2l.fit(X_train, y_train)","9aa3f87a":"search_mlp.best_params_, search_mlp.best_score_","dcf266aa":"# Colocando uma seed para garantir reprodutibilidade\nimport torch\ntorch.manual_seed(42)","37a78ede":"import torch.nn as nn\nimport torch.nn.functional as F\n\n# Nossa rede neural\nclass MLPNet(nn.Module):\n    def __init__(self, hidden1_dim=512, hidden2_dim=256, p=0.25):\n        super().__init__()\n        # Primeira camada escondida de tamanho 'hidden1_dim'\n        self.fc1 = nn.Linear(50, hidden1_dim)\n        \n        # Segunda camada escondida de tamanho 'hidden2_dim'\n        self.fc2 = nn.Linear(hidden1_dim, hidden2_dim)\n        \n        # Camada de output de tamanho 2 (para as duas classes de target)\n        self.fc3 = nn.Linear(hidden2_dim, 2)\n\n        # Camada de Dropout para regulariza\u00e7\u00e3o\n        self.dropout = nn.Dropout(p)\n        \n    def forward(self, X, **kwargs):\n        # Aplicando a primeira camada escondida\n        fc_out = F.relu(self.fc1(X))\n        # Aplicando o Dropout\n        fc_out = self.dropout(fc_out)\n        \n        # Aplicando a segunda camada escondida\n        fc_out = F.relu(self.fc2(fc_out))\n        # Aplicando o Dropout\n        fc_out = self.dropout(fc_out)\n        \n        # Aplicando a camada de output\n        fc_out = self.fc3(fc_out)\n        # Obtendo as probabilidades de cada classe com um softmax\n        soft_out = F.softmax(fc_out, dim=-1)\n        \n        return soft_out","d455e5ea":"import torch\n\n# Instanciando a nossa rede\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmlp_net = MLPNet().to(device)","1785029f":"from torch import optim\nfrom skorch import NeuralNetClassifier\n\n# Criando uma Rede Neural do Skorch\nskorch_net = NeuralNetClassifier(mlp_net,\n                                 max_epochs=20,\n                                 lr=1e-4,\n                                 optimizer=optim.Adam,\n                                 optimizer__weight_decay=1e-4,\n                                 train_split=False,\n                                 verbose=0,\n                                 iterator_train__shuffle=True,\n                                 )","06596597":"from skopt import BayesSearchCV\nfrom skopt.space import Real, Integer\n\n# Hiperpar\u00e2metros a serem otimizados\nhyperparams = {'module__hidden1_dim': Integer(256, 2048),\n               'module__hidden2_dim': Integer(128, 512),\n               'module__p': Real(0.1, 0.75, prior='uniform'),\n               'optimizer__weight_decay': Real(1e-10, 1e-2, prior='log-uniform')}\n\n# Busca de Hiperpar\u00e2metros\nskorch_clf = BayesSearchCV(skorch_net, hyperparams, scoring='roc_auc', n_iter=40, cv=3, n_jobs=-1, random_state=42, verbose=0)\nsearch_skorch = skorch_clf.fit(X_train.astype(np.float32), y_train.astype(np.int64))","cd577624":"search_skorch.best_params_, search_skorch.best_score_","ab815e0c":"from sklearn.metrics import roc_auc_score\n\n# Calculando a AUC da regress\u00e3o log\u00edstica\nlogreg_roc_auc = roc_auc_score(y_valid, logreg_clf.predict_proba(X_valid)[:,1])\n\nprint('AUCs --- Log. Reg.: {:.4f}'.format(logreg_roc_auc))","d40d0f40":"from sklearn.metrics import roc_auc_score\n\n# Calculando a AUC da rede neural de 1 camada do sklearn\nmlp_1l_roc_auc = roc_auc_score(y_valid, mlp_clf_1l.predict_proba(X_valid)[:,1])\n\nprint('AUCs --- MLP (1 camada): {:.4f}'.format(mlp_1l_roc_auc))","135f9fe0":"from sklearn.metrics import roc_auc_score\n\n# Calculando a AUC da rede neural de 2 camadas do sklearn\nmlp_2l_roc_auc = roc_auc_score(y_valid, mlp_clf_2l.predict_proba(X_valid)[:,1])\n\nprint('AUCs --- MLP (2 camadas): {:.4f}'.format(mlp_2l_roc_auc))","a30cde73":"from sklearn.metrics import roc_auc_score\n\n# Calculando a AUC da rede neural do Skorch\nskorch_roc_auc = roc_auc_score(y_valid, search_skorch.predict_proba(X_valid)[:,1])\n\nprint('AUCs --- MLP (Skorch): {:.4f}'.format(skorch_roc_auc))","4818b62a":"test = pd.read_csv(\"..\/input\/sentiment-analysis-pmr3508\/data_test2_X.csv\")","7dae3039":"test.head()","e7cb6599":"X_test = test['review'].tolist()\nX_test = [clean(x).split() for x in X_test]\nX_test = [emb(x, d2v) for x in X_test] \nX_test = np.array(X_test)","b2c63a38":"prediction = search_skorch.predict_proba(X_test)[:,1]","257e9429":"submission = {'positive': prediction}\nsubmission = pd.DataFrame(submission)\n\nsubmission.head(10)","799124b3":"submission.to_csv(\"submission.csv\", index = True, index_label = 'Id')","7cd98bde":"# \ud83c\udfad An\u00e1lise de Sentimentos com Redes Neurais\n### Bernardo Coutinho - PMR3508-2020-151\n\n#### \u00cdndice\n- [\ud83d\udcc2 Importa\u00e7\u00e3o](#imports)\n- [\ud83e\uddea Pr\u00e9-Processamento](#preprocessing)\n  - [\ud83d\udcdd Doc2Vec](#doc2vec)\n- [\ud83c\udfaf Classifica\u00e7\u00e3o](#classificacao)\n- [\u2696\ufe0f Compara\u00e7\u00e3o](#comparacao)\n- [\ud83d\udcdd Teste e Submiss\u00e3o](#teste)","5cbef801":"Nossa primeira rede a ser treinada ser\u00e1 uma rede neural de uma camada escondida do **scikit-learn**, usando a classe **MLPClassifier**. Na busca de hiperpar\u00e2metros, otimizaremos a quantidade de neur\u00f4nios na camada escondida, seu coeficiente de regulariza\u00e7\u00e3o e o tipo da taxa de aprendizado.","b715a8ab":"Tamb\u00e9m precisamos instalar as bibliotecas **ftfy**, para corrigir os nossos textos, e **gensim**, para aplicarmos o Doc2Vec:","bc864226":"Processando os dados:","229c9e48":"Em seguida, podemos visualiz\u00e1-los:","75eed4fd":"Conseguimos obter um score maior ainda! Mas conseguimos melhorar mais ainda com mais uma camada.","b5340119":"Para fazer a otimiza\u00e7\u00e3o de hiperpar\u00e2metros, utilizaremos a biblioteca **skorch**, que adapta o **PyTorch** para ser usado em conjunto com o **scikit-learn**.","73f52aad":"Como esperado, nosso melhor resultado nos dados de valida\u00e7\u00e3o foram com a rede neural do **PyTorch**! Agora, podemos utiliz\u00e1-la para fazer a predi\u00e7\u00e3o dos dados de teste.","703d6e3b":"Primeiramente, vamos importar os nossos dados de teste:","ce1db261":"A primeira coisa que faremos \u00e9 definir a classe da nossa rede:","66f534d1":"Salvando o arquivo de submiss\u00e3o:","8d3cf4b1":"Nossos dados est\u00e3o divididos em uma coluna **\"review\"**, que guarda o texto de avalia\u00e7\u00f5es de v\u00e1rios filmes, e uma coluna **\"positive\"**, que indica se a avalia\u00e7\u00e3o foi positiva ou n\u00e3o. Nosso objetivo ser\u00e1 predizer essa vari\u00e1vel com base no texto da avalia\u00e7\u00e3o!","d2403df5":"#### Rede Neural (PyTorch)","41293cb8":"Como esperado, as duas palavras s\u00e3o muito similares! \u00c9 natural pensar que dois n\u00fameros pr\u00f3ximos teriam representa\u00e7\u00f5es parecidas.","021c0f4c":"E aplic\u00e1-la nos nossos dados:","166aa644":"Depois de passada a nossa rede para o **skorch**, podemos otimiz\u00e1-la da mesma forma como far\u00edamos no **scikit-learn**, dessa vez utilizando o **BayesSearchCV** do pacote **skopt**:","4de8c696":"As palavras mais pr\u00f3ximas de **'Kubrick'** foram todos diretores de cinema!","42c41d41":"<a id=\"imports\"><\/a>\n# \ud83d\udcc2 Importa\u00e7\u00e3o","e357e30b":"Como este modelo aprende uma representa\u00e7\u00e3o mais \u00fatil dos textos, podemos fazer alguns experimentos para entend\u00ea-la melhor. Neste caso, estaremos comparando a representa\u00e7\u00e3o da palavra **um** com a palavra **dois** pelo Doc2Vec:","41854d09":"Tamb\u00e9m vamos instalar o **skorch**, uma biblioteca para utilizar o PyTorch em conjunto com o scikit-learn, e alterar a vers\u00e3o do scikit-learn para ser compat\u00edvel com o **skopt**.","3bb387d5":"Nosso resultado foi o melhor at\u00e9 agora! Vamos ver se esse resultado se mant\u00e9m nos nossos dados de valida\u00e7\u00e3o.","554ce882":"Agora nossos textos est\u00e3o bem mais padronizados!","47fec7a1":"Em seguida, podemos instanci\u00e1-la:","693c01ff":"#### Rede Neural (2 camadas)","59235d60":"Com os dados processados, podemos come\u00e7ar efetivamente a nossa classifica\u00e7\u00e3o!","f5963a6d":"## Rede Neural (1 camada)","4b2d4c77":"Cada entrada do nosso X \u00e9 uma representa\u00e7\u00e3o do nosso texto em um vetor de 50 n\u00fameros:","45e031c4":"Como esperado, as palavras mais similares com **filme** foram seus sin\u00f4nimos e outros termos utilizados para se referir a filmes. Podemos utilizar a mesma fun\u00e7\u00e3o, por exemplo, para descobrir a semelhan\u00e7a entre diferentes diretores:","0a0208cf":"Em seguida, podemos transformar nossos dados de treino:","806a419d":"Agora que entendemos melhor nosso modelo, podemos definir nossa fun\u00e7\u00e3o de **embedding**, copiada do notebook exemplo:","dcf8c596":"## Rede Neural (Pytorch)\n\nPor fim, vamos utilizar o **PyTorch** para criar nossa rede neural! O **Pytorch** \u00e9 um framework de Deep Learning muito \u00fatil e vers\u00e1til, caso queira conhecer um pouco mais sobre ele, recomendo o seguinte texto:\n\n - [Construindo uma Rede Neural do zero | Pytorch](https:\/\/medium.com\/turing-talks\/construindo-uma-rede-neural-do-zero-pytorch-671ee06fbbe1)","6acdb3eb":"<a id=\"comparacao\"><\/a>\n# \u2696\ufe0f Compara\u00e7\u00e3o\n\nCom todos os nossos modelos treinados, podemos compar\u00e1-los mais efetivamente a partir de sua **AUC** nos dados de valida\u00e7\u00e3o:","adebf06f":"Realizando a predi\u00e7\u00e3o:","6d6de678":"#### Regress\u00e3o Log\u00edstica","941677ae":"Primeiramente, importamos as bibliotecas **numpy** e **pandas**:","10f3be09":"Dessa forma, primeiro importamos nosso modelo Doc2Vec pr\u00e9-treinado:","e940a6d5":"#### Rede Neural (1 camada)","b7e09d08":"Montando o DataFrame de submiss\u00e3o:","c0172fc6":"Agora, vamos guardar nossos textos como vetores de palavras:","3d52f65e":"Em seguida, importamos os dados de treino:","8e156a9c":"<a id=\"preprocessing\"><\/a>\n# \ud83e\uddea Pr\u00e9-Processamento\n\nAntes de treinar qualquer modelo, precisamos antes limpar e pr\u00e9-processar os nossos textos! Para isto, primeiro vamos dropar as nossas colunas duplicadas:","9e37eaac":"Depois, podemos utilizar a fun\u00e7\u00e3o definida pelo Felipe Maia Polo para limpar os nossos textos:","7b152949":"<a id=\"doc2vec\"><\/a>\n## \ud83d\udcdd Doc2Vec\n\nCom os nossos textos j\u00e1 limpos, podemos process\u00e1-los com nosso modelo **Doc2Vec**. Este modelo de **embedding** converte cada uma das nossas an\u00e1lises em vetores j\u00e1 de um mesmo tamanho, alterando sua representa\u00e7\u00e3o de forma a tornar a predi\u00e7\u00e3o muito mais f\u00e1cil!","a635621c":"Da mesma forma, podemos descobrir as palavras com representa\u00e7\u00e3o mais pr\u00f3xima de uma outra palavra escolhida:","9797fe57":"Em seguida, vamos analisar a distribui\u00e7\u00e3o dos nossos dados:","f47bcd62":"J\u00e1 conseguimos obter uma boa **AUC** mesmo com a Regress\u00e3o Log\u00edstica! Agora podemos testar modelos mais complexos de redes neurais.","78614948":"Agora, vamos aplicar o mesmo processo nos nossos dados de valida\u00e7\u00e3o:","1a1ae048":"## Rede Neural (2 camadas)\n\nAgora, utilizaremos novamente a classe **MLPClassifier** para gerar uma rede neural, dessa vez com 2 camadas escondidas. Na busca de hiperpar\u00e2metros, otimizaremos a quantidade de neur\u00f4nios nas duas camadas escondidas, seu coeficiente de regulariza\u00e7\u00e3o e o tipo da taxa de aprendizado.","b165e837":"<a id=\"teste\"><\/a>\n# \ud83d\udcdd Teste e Submiss\u00e3o","7b8da348":"Temos quase a mesma quantidade de an\u00e1lises positivas e negativas! Agora sabemos que n\u00e3o lidamos com o problema de dados desbalanceados. Agora, vamos dividir os nossos dados em **X** e **y**.","89bed33a":"Como o Doc2Vec transforma um documento inteiro, podemos fazer a compara\u00e7\u00e3o de sequ\u00eancias de palavras ou frases inteiras:","899a20c5":"O objetivo deste notebook \u00e9 realizar uma **An\u00e1lise de Sentimentos** da base de dados de cr\u00edticas do IMDb utilizando o modelo Doc2Vec. Caso voc\u00ea n\u00e3o esteja familiarizado com o **Processamento de Linguagem Natural** (PLN), recomendo a leitura dos seguintes textos!\n\n - [Introdu\u00e7\u00e3o ao Processamento de Linguagem Natural](https:\/\/medium.com\/turing-talks\/introdu%C3%A7%C3%A3o-ao-processamento-de-linguagem-natural-com-baco-exu-do-blues-17cbb7404258)\n - [An\u00e1lise de sentimento usando LSTM no PyTorch](https:\/\/medium.com\/turing-talks\/an%C3%A1lise-de-sentimento-usando-lstm-no-pytorch-d90f001eb9d7)","63313ee2":"<a id=\"classificacao\"><\/a>\n# \ud83c\udfaf Classifica\u00e7\u00e3o\n\n## Regress\u00e3o Log\u00edstica\n\nPrimeiramente, vamos testar a regress\u00e3o log\u00edstica com o resultado do Doc2Vec. Para fazer a busca de hiperpar\u00e2metros, vamos utilizar a classe **RandomizedSearchCV**, otimizando os valores de *C* e do tipo de regulariza\u00e7\u00e3o.","7ac0b654":"Esse foi o nosso melhor resultado por enquanto! Agora, podemos tentar melhorar nosso resultado utilizando um framework de deep learning: o **PyTorch**."}}