{"cell_type":{"3dacc81a":"code","0dbd76f3":"code","cf5154e1":"code","541d2534":"code","0891383a":"code","63b178c7":"code","e556b639":"code","748d761a":"code","402e7638":"code","8b666973":"code","70f410d2":"code","0be01197":"code","29ae3763":"code","0f30f834":"code","e8380a65":"code","356893f0":"code","de03a0ee":"code","6d800bfe":"code","58d9ecdd":"code","ea65923a":"code","ecfe6c8d":"code","3f784ab0":"code","daf7ba49":"code","d50908d9":"code","d0e52a92":"code","4798baba":"code","7c90c242":"code","ffa801a2":"code","a23ce8a4":"code","3dc4f934":"code","b254d76e":"code","77393d8c":"code","b9b025c8":"code","2e7cd1f6":"code","876947c6":"code","90882185":"code","4a2faf83":"code","de0c03c2":"code","66a54f50":"markdown","1f9fd810":"markdown","42b2f38c":"markdown","3babe94f":"markdown","419bc03d":"markdown","dbfa814e":"markdown"},"source":{"3dacc81a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #Plotting\nimport seaborn as sns\n# Scaling preprocessing library\nfrom sklearn.preprocessing import StandardScaler\nimport sklearn.preprocessing\nfrom sklearn.preprocessing import Imputer\n# Math Library\nfrom math import ceil\n# Boosting Libraries\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor\n# Keras importing\nimport keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom keras.optimizers import SGD \nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import np_utils\nimport itertools\nfrom keras.layers import LSTM\nfrom keras.layers import TimeDistributed\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers import Dropout\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","0dbd76f3":"# read the train data \ntrain = pd.read_csv('..\/input\/yds_train2018.csv')\n# print the top 5 row from the dataframe\ntrain.head()","cf5154e1":"# read the test data\ntest = pd.read_csv('..\/input\/yds_test2018.csv')\ntest.head()","541d2534":"# Matching the train and test columns\n[c for c in train.columns if c not in test.columns]","0891383a":"# Looking for NaN values \ndroping_list_all=[]\nfor j in range(0,8):\n    if not train.iloc[:, j].notnull().all():\n        droping_list_all.append(j)        \ndroping_list_all","63b178c7":"# Conditional groupby of train data\n\ndf_grouped = pd.DataFrame(train.groupby(['Year','Month','Product_ID','Country'])['Sales'].sum().reset_index())","e556b639":"# Sorting the grouped dataframes index\ndf_grouped = df_grouped.sort_index()","748d761a":"# Look for top 5 rows\ndf_grouped.head()\n","402e7638":"# Look for last 5 rows\ndf_grouped.tail()","8b666973":"# Importing the expense data for simulation\ndf_expense = pd.read_csv('..\/input\/promotional_expense.csv')\ndf_expense.head()","70f410d2":"# Plotting the flow of Sales according to months with product ID descriptions\n\nfig, axes = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=True, figsize=(16,8))\nnum_graph = 2\nsns.pointplot(x='Month', y='Sales', hue='Product_ID', \n                      data=df_grouped)","0be01197":"# Plotting the flow of Sales according to months with Country descriptions\nfig, axes = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=True, figsize=(16,8))\nnum_graph = 2\nsns.pointplot(x='Month', y='Sales', hue='Country', \n                      data=df_grouped)","29ae3763":"corr = df_grouped.corr()","0f30f834":"corr","e8380a65":"# Renaming the mismatching column indexes\n\ndf_expense.rename(columns={'Product_Type': 'Product_ID', 'Expense_Price': 'Sales'}, inplace=True)","356893f0":"# Joining the training data with expenses and do the differences\ndf_grouped1 = df_grouped.set_index(['Year','Month','Product_ID','Country'])\ndf_expense1= df_expense.set_index(['Year','Month','Product_ID','Country'])\ndf_diff = df_grouped1.join(df_expense1, how='outer', rsuffix='_').fillna(0)\ndf_grouped1['Sales'] = df_diff['Sales']- df_diff['Sales_']\n# Index reset\ndf_grouped1 = df_grouped1.reset_index()\n","de03a0ee":"# top 5 rows\ndf_grouped1.head()","6d800bfe":"# Changing the sales data type\ndf_grouped1['Sales'] = df_grouped1['Sales'].astype(int)\ndf_grouped1.head()","58d9ecdd":"df_grouped1.describe()","ea65923a":"# Taking care of Categorical data in grouped1 country column\npd.get_dummies(df_grouped1, prefix=['Country'])","ecfe6c8d":"# Taking care of Categorical data in test country column\npd.get_dummies(test, prefix=['Country'])","3f784ab0":"# Defining a train function\ndef train_validate_split(df_grouped1, train_percent=.8, validate_percent=.2, seed=None):\n    np.random.seed(seed)\n    perm = np.random.permutation(df_grouped1.index)\n    m = len(df_grouped1.index)\n    train_end = int(train_percent * m)\n    validate_end = int(validate_percent * m) + train_end\n    training = df_grouped1.ix[perm[:train_end]]\n    validate = df_grouped1.ix[perm[train_end:validate_end]]\n    return training, validate","daf7ba49":"training, validate = train_validate_split(df_grouped1)\n\ntraining","d50908d9":"# Taking care of Categorical data in test country column\npd.get_dummies(training, prefix=['Country'])","d0e52a92":"# Taking care of Categorical data in test country column\npd.get_dummies(validate, prefix=['Country'])","4798baba":"# Splitting of Data Columns \nX_test = test.iloc[:, 1:4].values\nY_test = test.iloc[:, 5].values\nY_test","7c90c242":"X_test","ffa801a2":"# Splitting of Data columns\nX_train = df_grouped1.iloc[:, 0:3].values\nY_train = df_grouped1.iloc[:, 4].values\nY_train","a23ce8a4":"X_train","3dc4f934":"# Splitting of Data columns\nX_val = validate.iloc[:, 0:3].values\nY_val = validate.iloc[:, 4].values\nY_val","b254d76e":"# Fitting Random Forest Regression to the dataset\nfrom sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(n_estimators= 500, random_state = 0)\nregressor.fit(X_train, Y_train)\n","77393d8c":"# Predicting the values\ny_pred = regressor.predict(X_test)","b9b025c8":"y_pred","2e7cd1f6":"np.savetxt(\"submission.csv\", y_pred, delimiter=\",\")","876947c6":"my_imputer = Imputer()\nX_train = my_imputer.fit_transform(X_train)\nX_test = my_imputer.transform(X_test)","90882185":"my_model = XGBRegressor()\n# Add silent=True to avoid printing out updates with each cycle\nmy_model.fit(X_train, Y_train, verbose=False)","4a2faf83":"y_pred = my_model.predict(X_test)\ny_pred","de0c03c2":"np.savetxt(\"submission1.csv\", y_pred, delimiter=\",\")","66a54f50":"**In my next version i'll update LSTM RNN method and ARIMA method.**\n\nPlease upvote if you like it as it would motivate me to work more harder.\nThank you!!","1f9fd810":"\n> **Another efficient method with better prediction rate by xgboost method**\n\nXGBoost is a software library that you can download and install on your machine, then access from a variety of interfaces. Specifically, XGBoost supports the following main interfaces:\n\nCommand Line Interface (CLI).\nC++ (the language in which the library is written).\nPython interface as well as a model in scikit-learn.\nR interface as well as a model in the caret package.\nJulia.\nJava and JVM languages like Scala and platforms like Hadoop.\nXGBoost Features\nThe library is laser focused on computational speed and model performance, as such there are few frills. Nevertheless, it does offer a number of advanced features.\n\n**Model Features**\nThe implementation of the model supports the features of the scikit-learn and R implementations, with new additions like regularization. Three main forms of gradient boosting are supported:\n\nGradient Boosting algorithm also called gradient boosting machine including the learning rate.\nStochastic Gradient Boosting with sub-sampling at the row, column and column per split levels.\nRegularized Gradient Boosting with both L1 and L2 regularization.\nSystem Features\nThe library provides a system for use in a range of computing environments, not least:\n\nParallelization of tree construction using all of your CPU cores during training.\nDistributed Computing for training very large models using a cluster of machines.\nOut-of-Core Computing for very large datasets that don\u2019t fit into memory.\nCache Optimization of data structures and algorithm to make best use of hardware.\n\n**Algorithm Features**\nThe implementation of the algorithm was engineered for efficiency of compute time and memory resources. A design goal was to make the best use of available resources to train the model. Some key algorithm implementation features include:\n\nSparse Aware implementation with automatic handling of missing data values.\nBlock Structure to support the parallelization of tree construction.\nContinued Training so that you can further boost an already fitted model on new data.\nXGBoost is free open source software available for use under the permissive Apache-2 license.\n\n**Why Use XGBoost?**\nThe two reasons to use XGBoost are also the two goals of the project:\n\n1. Execution Speed.\n1. Model Performance.","42b2f38c":"**Looking for non matching feature columns**","3babe94f":"**Preprocessing of Libraries**\n\nHere I'll be importing all the needed libraries for Data Prediction and analysis.","419bc03d":"> **ZS Data Science Challenge - 2018**\n\nIt was a unique opportunity for me to solve a real data science problem statement! This have provide me a sneak peek into the actual work done at ZS. The offered problem statement was designed to evaluate:   \n\n1. how i handle data to pre-process and generate relevant features and insights for modeling\n1. my knowledge of machine learning\/statistics  \n1. my proficiency in identifying the right technique to approach the solution  \n1. how i translate the findings from the business context point of view  \nDuring the challenge i was expected to build a model using the provided sample dataset\/s.   ","dbfa814e":"**Importing the Data into pandas dataframes.**"}}