{"cell_type":{"b105cf8b":"code","95f324a0":"code","07fe5c89":"code","02e2b699":"code","5129062b":"code","ae2a0268":"code","07a38285":"code","8df81d10":"code","277cb529":"code","cf5d6ccd":"code","042d52d2":"code","5526f545":"code","f548dd2c":"code","df3aff89":"code","f7debf59":"code","7b1384d0":"code","d66454eb":"code","8b8e84ee":"code","26b9d564":"code","ae220371":"code","e8ec9f9a":"code","0bd45990":"code","88ff06ab":"code","f09d9e3a":"code","6c2db505":"code","d1eb911f":"code","73a40ac9":"code","6ba9fbab":"code","b46bb8f2":"code","13e16322":"code","f4b4b9f2":"code","23455899":"code","18a56d6d":"code","1c387901":"code","0a5078d4":"code","d7b1dddd":"code","ff4e2c32":"markdown","75735227":"markdown","1d55311d":"markdown","fb252e62":"markdown","f1501618":"markdown","c41fb3d5":"markdown","52acc378":"markdown","e54bff16":"markdown","b8730ac2":"markdown","7d2b3f7c":"markdown","b74394a0":"markdown","ec1060b3":"markdown","bddfc427":"markdown","3be04b9b":"markdown","416b96b5":"markdown","13ed1cbd":"markdown","de668212":"markdown","c6dae1aa":"markdown","e3e7cfde":"markdown"},"source":{"b105cf8b":"from pathlib import Path\nfrom typing import List, Tuple, Dict, Union\nfrom functools import partial\n\nimport time\nimport os\nimport sys\nimport random\nimport shutil\n\nfrom sklearn.model_selection import KFold\n\nimport numpy as np \nimport pandas as pd","95f324a0":"import chainer\nfrom chainer import links, functions\nfrom chainer import datasets, iterators, optimizers, serializers\nfrom chainer import training, reporter, cuda\nfrom chainercv.links import PickableSequentialChain","07fe5c89":"class LinearActiv(chainer.Chain):\n    \"\"\"linear -> activation (-> batch norm -> dropout)\"\"\"\n\n    def __init__(\n        self, in_size: int, out_size: int,\n        dropout_rate=None, use_bn=False, activ=functions.relu\n    ) -> None:\n        \"\"\"Initialize.\"\"\"\n        super(LinearActiv, self).__init__()\n        layers = chainer.Sequential(links.Linear(in_size, out_size))\n        if activ is not None:\n            layers.append(activ)\n        if use_bn:\n            layers.append(links.BatchNormalization(out_size))\n        if dropout_rate is not None:\n            layers.append(partial(functions.dropout, ratio=dropout_rate))\n\n        with self.init_scope():\n            self.la = layers\n\n    def __call__(self, x: chainer.Variable) -> chainer.Variable:\n        \"\"\"Forward.\"\"\"\n        return self.la(x)\n    \n\nclass MLP(chainer.Chain):\n    \"\"\"Multi Layer Perceptron.\"\"\"\n\n    def __init__(\n        self, in_dim: int, hidden_dims: List[int],\n        drop_rates: List[float]=None, use_bn=False, use_tail_as_out=True\n    ) -> None:\n        \"\"\"initialize.\"\"\"\n        super(MLP, self).__init__()\n        hidden_dims = [in_dim] + hidden_dims\n        drop_rates = [None] * len(hidden_dims) if drop_rates is None else drop_rates\n        layers = [\n            LinearActiv(\n                hidden_dims[i], hidden_dims[i + 1], drop_rates[i], use_bn)\n            for i in range(len(hidden_dims) - 2)]\n\n        if use_tail_as_out:\n            layers.append(links.Linear(hidden_dims[-2], hidden_dims[-1]))\n        else:\n            layers.append(\n                LinearActiv(\n                    hidden_dims[-2], hidden_dims[-1], drop_rates[-1], use_bn))\n\n        with self.init_scope():\n            self.layers = chainer.Sequential(*layers)\n\n    def __call__(self, x: chainer.Variable) -> chainer.Variable:\n        \"\"\"Forward.\"\"\"\n        return self.layers(x)\n\n\nclass CustomMLP(chainer.Chain):\n    \"\"\"Simple MLP model.\"\"\"\n    \n    def __init__(\n        self, left_mlp: MLP, right_mlp: MLP, tail_mlp: MLP\n    ) -> None:\n        \"\"\"Initialize.\"\"\"\n        super(CustomMLP, self).__init__()\n        with self.init_scope():\n            self.left = left_mlp\n            self.right = right_mlp\n            self.tail = tail_mlp\n        \n    def __call__(self, x_left: chainer.Variable, x_right: chainer.Variable) -> chainer.Variable:\n        \"\"\"Forward.\"\"\"\n        h_left = self.left(x_left)\n        h_right = self.right(x_right)\n        h = functions.concat([h_left, h_right])\n        h = self.tail(h)\n        return h","02e2b699":"class Regressor(links.Classifier):\n    \"\"\"Wrapper for regression model.\"\"\"\n\n    def __init__(\n        self, predictor, lossfun, evalfun_dict\n    ):\n        \"\"\"Initialize\"\"\"\n        super(Regressor, self).__init__(predictor, lossfun)\n        self.compute_accuracy = False\n        self.evalfun_dict = evalfun_dict\n        for name, func in self.evalfun_dict.items():\n            setattr(self, name, None)\n            \n    def evaluate(self, *in_arrs: Tuple[chainer.Variable]) -> None:\n        \"\"\"Calc loss and evaluation metric.\"\"\"\n        for name in self.evalfun_dict.keys():\n            setattr(self, name, None)\n        loss = self(*in_arrs)\n\n        for name, evalfun in self.evalfun_dict.items():\n            setattr(self, name, evalfun(self.y, in_arrs[-1]))\n            reporter.report({name: getattr(self, name)}, self)\n        del loss","5129062b":"def normalized_absolute_error(y_pred: chainer.Variable, t: np.ndarray):\n    \"\"\"\n    \\sum_{i} |y_pred_{i} - t_{i}| \/ \\sum_{i} t_{i}\n    \"\"\"\n    return functions.sum(functions.absolute(y_pred - t)) \/ functions.sum(t)\n\n\nclass WeightedNormalizedAbsoluteError:\n    \"\"\"Metric for this competition\"\"\"\n    \n    def __init__(self, weights: List[float]=[.3, .175, .175, .175, .175]):\n        \"\"\"Initialize.\"\"\"\n        self.weights = weights\n        self.pred_num = len(weights)\n        \n    def __call__(self, y_pred: chainer.Variable, t: np.ndarray) ->  chainer.Variable:\n        \"\"\"Forward.\"\"\"\n        loss = 0\n        for i, weight in enumerate(self.weights):\n            loss += weight * normalized_absolute_error(y_pred[:, i], t[:, i])\n            \n        return loss\n    \n\nclass SelectNormalizedAbsoluteError:\n    \"\"\"For checking each features loss\"\"\"\n    \n    def __init__(self, index: int):\n        \"\"\"Initialize.\"\"\"\n        self.index = index\n        \n    def __call__(self, y_pred: chainer.Variable, t: np.ndarray) ->  chainer.Variable:\n        \"\"\"Forward.\"\"\"\n        return normalized_absolute_error(y_pred[:, self.index], t[:, self.index])","ae2a0268":"def set_random_seed(seed=42):\n    \"\"\"Fix Seeds.\"\"\"\n    # set Python random seed\n    random.seed(seed)\n\n    # set NumPy random seed\n    np.random.seed(seed)\n\n    # set Chainer(CuPy) random seed\n    cuda.cupy.random.seed(seed)","07a38285":"COMPETITION_NAME = \"trends-assessment-prediction\"\nROOT = Path(\".\").resolve().parents[0]\n\nINPUT_ROOT = ROOT \/ \"input\"\nRAW_DATA = INPUT_ROOT \/ COMPETITION_NAME\nTRAIN_IMAGES = RAW_DATA \/ \"fMRI_train\"\nTEST_IMAGES = RAW_DATA \/ \"fMRI_test\"","8df81d10":"fnc = pd.read_csv(RAW_DATA \/ \"fnc.csv\")\nicn_numbers = pd.read_csv(RAW_DATA \/ \"ICN_numbers.csv\")\nloading = pd.read_csv(RAW_DATA \/ \"loading.csv\")\nreveal_ID_site2 = pd.read_csv(RAW_DATA \/ \"reveal_ID_site2.csv\")\n\ntrain_scores = pd.read_csv(RAW_DATA \/ \"train_scores.csv\")\nsample_sub = pd.read_csv(RAW_DATA \/ \"sample_submission.csv\")","277cb529":"sample_sub.shape[0] \/ 5 ","cf5d6ccd":"# # init model\ndef init_model(is_train=True):\n    model = CustomMLP(\n        left_mlp=MLP(\n            in_dim=1378, hidden_dims=[1024, 768], drop_rates=[0.5, 0.5], use_tail_as_out=False),\n        right_mlp=MLP(\n            in_dim=26, hidden_dims=[64, 768], drop_rates=[0.5, 0.5], use_tail_as_out=False),\n        tail_mlp=MLP(\n            in_dim=1536, hidden_dims=[1024, 5], drop_rates=[0.5, 0.0]),\n    )\n    if not is_train:\n        return model\n\n    # # set trainning wrapper\n    train_model = Regressor(\n        predictor=model,\n        lossfun=WeightedNormalizedAbsoluteError(weights=[.3, .175, .175, .175, .175]),\n        evalfun_dict={\n            \"NAE_Age\": SelectNormalizedAbsoluteError(0),\n            \"NAE_Domain1Var1\": SelectNormalizedAbsoluteError(1),\n            \"NAE_Domain1Var2\": SelectNormalizedAbsoluteError(2),\n            \"NAE_Domain2Var1\": SelectNormalizedAbsoluteError(3),\n            \"NAE_Domain2Var2\": SelectNormalizedAbsoluteError(4)}\n    )\n    return train_model","042d52d2":"DEVICE = 0","5526f545":"def create_trainer(train_model, train_dataset, val_dataset, output_dir, device):\n    # # set optimizer\n    optimizer = optimizers.AdamW(alpha=0.001, weight_decay_rate=0.0)\n    optimizer.setup(train_model)\n    \n    # # make iterator\n    train_iter = iterators.MultiprocessIterator(\n        train_dataset, 64, n_processes=2)\n    val_iter = iterators.MultiprocessIterator(\n        val_dataset, 64, repeat=False, shuffle=False, n_processes=2)\n    \n    # # init trainer\n    updater = training.StandardUpdater(train_iter, optimizer, device=device)\n\n    stop_trigger = training.triggers.EarlyStoppingTrigger(\n        check_trigger=(1, 'epoch'), monitor='val\/main\/loss', mode=\"min\",\n        patients=20, max_trigger=(200, 'epoch'), verbose=True)\n\n    trainer = training.trainer.Trainer(\n        updater, stop_trigger=stop_trigger, out=output_dir)\n    \n    # # set extentions\n    lr_attr_name = \"alpha\"\n    log_trigger = (1, \"epoch\")\n    logging_attributes = [\n        \"epoch\", \"elapsed_time\", \"main\/loss\", \"val\/main\/loss\",\n        \"val\/main\/NAE_Age\",\n        \"val\/main\/NAE_Domain1Var1\", \"val\/main\/NAE_Domain1Var2\",\n        \"val\/main\/NAE_Domain2Var1\", \"val\/main\/NAE_Domain2Var2\"]\n\n    # # # evaluator\n    eval_target = trainer.updater.get_optimizer('main').target\n    trainer.extend(\n        training.extensions.Evaluator(\n            val_iter, eval_target, device=device, eval_func=eval_target.evaluate),\n        name='val',trigger=(1, 'epoch'))\n\n    # # # log.\n    trainer.extend(\n        training.extensions.observe_lr(observation_key=lr_attr_name), trigger=log_trigger)\n    trainer.extend(\n        training.extensions.LogReport(logging_attributes, trigger=log_trigger), trigger=log_trigger)\n    trainer.extend(training.extensions.PrintReport(logging_attributes), trigger=log_trigger)\n\n    # # # save snapshot\n    trainer.extend(\n        training.extensions.snapshot_object(\n            trainer.updater.get_optimizer('main').target.predictor,\n            'model_snapshot_{.updater.epoch}.npz'),\n        trigger=training.triggers.MinValueTrigger(\"val\/main\/loss\", (1, \"epoch\")))\n    \n    return trainer","f548dd2c":"val_score_list = []","df3aff89":"features = pd.merge(fnc,loading, on=\"Id\", how=\"inner\") \ntrain_all = train_scores.merge(features, on=\"Id\", how=\"left\")\n\n# #For convenience in trainning, fill NA by mean values. \n\nfor i in range(5):\n    train_all.iloc[:, i + 1] = train_all.iloc[:, i + 1].fillna(train_all.iloc[:, i + 1].mean())\n\nkf = KFold(n_splits=5, shuffle=True, random_state=1086)\ntrain_val_splits = list(kf.split(X=train_scores.Id))","f7debf59":"fold_id = 0\ntrain_index, val_index = train_val_splits[fold_id]\ntrain = train_all.iloc[train_index]\nval = train_all.iloc[val_index]\n\ntrain_dataset = datasets.TupleDataset(\n    train.iloc[:, 6:1384].values.astype(\"f\"),  # fnc\n    train.iloc[:, 1384:].values.astype(\"f\"),  # loading\n    train.iloc[:, 1:6].values.astype(\"f\"),  # label\n)\nval_dataset = datasets.TupleDataset(\n    val.iloc[:, 6:1384].values.astype(\"f\"),  # fnc\n    val.iloc[:, 1384:].values.astype(\"f\"),  # loading\n    val.iloc[:, 1:6].values.astype(\"f\"),  # label\n)\nset_random_seed(1086)\ntrain_model = init_model()\ntrainer = create_trainer(train_model, train_dataset, val_dataset, \"training_result_fold{}\".format(fold_id), DEVICE)\ntrainer.run()","7b1384d0":"best_epoch = trainer.updater.epoch - trainer.stop_trigger.count\nprint(best_epoch, trainer.stop_trigger.best)\nval_score_list.append([fold_id, trainer.stop_trigger.best, best_epoch,])\nshutil.copyfile(\n    \"training_result_fold{}\/model_snapshot_{}.npz\".format(fold_id, best_epoch), \"best_model_fold{},npz\".format(fold_id))","d66454eb":"fold_id = 1\ntrain_index, val_index = train_val_splits[fold_id]\ntrain = train_all.iloc[train_index]\nval = train_all.iloc[val_index]\n\ntrain_dataset = datasets.TupleDataset(\n    train.iloc[:, 6:1384].values.astype(\"f\"),  # fnc\n    train.iloc[:, 1384:].values.astype(\"f\"),  # loading\n    train.iloc[:, 1:6].values.astype(\"f\"),  # label\n)\nval_dataset = datasets.TupleDataset(\n    val.iloc[:, 6:1384].values.astype(\"f\"),  # fnc\n    val.iloc[:, 1384:].values.astype(\"f\"),  # loading\n    val.iloc[:, 1:6].values.astype(\"f\"),  # label\n)\nset_random_seed(1086)\ntrain_model = init_model()\ntrainer = create_trainer(train_model, train_dataset, val_dataset, \"training_result_fold{}\".format(fold_id), DEVICE)\ntrainer.run()","8b8e84ee":"best_epoch = trainer.updater.epoch - trainer.stop_trigger.count\nprint(best_epoch, trainer.stop_trigger.best)\nval_score_list.append([fold_id, trainer.stop_trigger.best, best_epoch,])\nshutil.copyfile(\n    \"training_result_fold{}\/model_snapshot_{}.npz\".format(fold_id, best_epoch), \"best_model_fold{},npz\".format(fold_id))","26b9d564":"fold_id = 2\ntrain_index, val_index = train_val_splits[fold_id]\ntrain = train_all.iloc[train_index]\nval = train_all.iloc[val_index]\n\ntrain_dataset = datasets.TupleDataset(\n    train.iloc[:, 6:1384].values.astype(\"f\"),  # fnc\n    train.iloc[:, 1384:].values.astype(\"f\"),  # loading\n    train.iloc[:, 1:6].values.astype(\"f\"),  # label\n)\nval_dataset = datasets.TupleDataset(\n    val.iloc[:, 6:1384].values.astype(\"f\"),  # fnc\n    val.iloc[:, 1384:].values.astype(\"f\"),  # loading\n    val.iloc[:, 1:6].values.astype(\"f\"),  # label\n)\nset_random_seed(1086)\ntrain_model = init_model()\ntrainer = create_trainer(train_model, train_dataset, val_dataset, \"training_result_fold{}\".format(fold_id), DEVICE)\ntrainer.run()","ae220371":"best_epoch = trainer.updater.epoch - trainer.stop_trigger.count\nprint(best_epoch, trainer.stop_trigger.best)\nval_score_list.append([fold_id, trainer.stop_trigger.best, best_epoch,])\nshutil.copyfile(\n    \"training_result_fold{}\/model_snapshot_{}.npz\".format(fold_id, best_epoch), \"best_model_fold{},npz\".format(fold_id))","e8ec9f9a":"fold_id = 3\ntrain_index, val_index = train_val_splits[fold_id]\ntrain = train_all.iloc[train_index]\nval = train_all.iloc[val_index]\n\ntrain_dataset = datasets.TupleDataset(\n    train.iloc[:, 6:1384].values.astype(\"f\"),  # fnc\n    train.iloc[:, 1384:].values.astype(\"f\"),  # loading\n    train.iloc[:, 1:6].values.astype(\"f\"),  # label\n)\nval_dataset = datasets.TupleDataset(\n    val.iloc[:, 6:1384].values.astype(\"f\"),  # fnc\n    val.iloc[:, 1384:].values.astype(\"f\"),  # loading\n    val.iloc[:, 1:6].values.astype(\"f\"),  # label\n)\nset_random_seed(1086)\ntrain_model = init_model()\ntrainer = create_trainer(train_model, train_dataset, val_dataset, \"training_result_fold{}\".format(fold_id), DEVICE)\ntrainer.run()","0bd45990":"best_epoch = trainer.updater.epoch - trainer.stop_trigger.count\nprint(best_epoch, trainer.stop_trigger.best)\nval_score_list.append([fold_id, trainer.stop_trigger.best, best_epoch,])\nshutil.copyfile(\n    \"training_result_fold{}\/model_snapshot_{}.npz\".format(fold_id, best_epoch), \"best_model_fold{},npz\".format(fold_id))","88ff06ab":"fold_id = 4\ntrain_index, val_index = train_val_splits[fold_id]\ntrain = train_all.iloc[train_index]\nval = train_all.iloc[val_index]\n\ntrain_dataset = datasets.TupleDataset(\n    train.iloc[:, 6:1384].values.astype(\"f\"),  # fnc\n    train.iloc[:, 1384:].values.astype(\"f\"),  # loading\n    train.iloc[:, 1:6].values.astype(\"f\"),  # label\n)\nval_dataset = datasets.TupleDataset(\n    val.iloc[:, 6:1384].values.astype(\"f\"),  # fnc\n    val.iloc[:, 1384:].values.astype(\"f\"),  # loading\n    val.iloc[:, 1:6].values.astype(\"f\"),  # label\n)\nset_random_seed(1086)\ntrain_model = init_model()\ntrainer = create_trainer(train_model, train_dataset, val_dataset, \"training_result_fold{}\".format(fold_id), DEVICE)\ntrainer.run()","f09d9e3a":"best_epoch = trainer.updater.epoch - trainer.stop_trigger.count\nprint(best_epoch, trainer.stop_trigger.best)\nval_score_list.append([fold_id, trainer.stop_trigger.best, best_epoch,])\nshutil.copyfile(\n    \"training_result_fold{}\/model_snapshot_{}.npz\".format(fold_id, best_epoch), \"best_model_fold{},npz\".format(fold_id))","6c2db505":"pd.DataFrame(\n    val_score_list,\n    columns=[\"fold\", \"score\", \"best_epoch\"])","d1eb911f":"def inference_test_data(\n    model: Union[chainer.Chain, PickableSequentialChain],\n    test_iter: chainer.iterators.MultiprocessIterator, gpu_device: int=-1\n) -> Tuple[np.ndarray]:\n    \"\"\"Oridinary Inference.\"\"\"\n    test_pred_list = []\n    test_label_list = []\n    iter_num = 0\n    epoch_test_start = time.time()\n\n    while True:\n        test_batch = test_iter.next()\n        iter_num += 1\n        print(\"\\rtmp_iteration: {:0>5}\".format(iter_num), end=\"\")\n        in_arrays = chainer.dataset.concat_examples(test_batch, gpu_device)\n\n        # Forward the test data\n        with chainer.no_backprop_mode() and chainer.using_config(\"train\", False):\n            prediction_test = model(*in_arrays[:-1])\n            test_pred_list.append(prediction_test)\n            test_label_list.append(in_arrays[-1])\n            prediction_test.unchain_backward()\n\n        if test_iter.is_new_epoch:\n            print(\" => test end: {:.2f} sec\".format(time.time() - epoch_test_start))\n            test_iter.reset()\n            break\n\n    test_pred_all = cuda.to_cpu(functions.concat(test_pred_list, axis=0).data)\n    test_label_all = cuda.to_cpu(functions.concat(test_label_list, axis=0).data)\n    del test_pred_list\n    del test_label_list\n    return test_pred_all, test_label_all","73a40ac9":"sample_sub.head()","6ba9fbab":"test = pd.DataFrame({}, columns=train_scores.columns.tolist())\ntest[\"Id\"] = sample_sub[\"Id\"].apply(lambda x: int(x.split('_')[0])).unique()\ntest = test.fillna(-1)\ntest = test.merge(features, on=\"Id\", how=\"left\")\n\ntest_label = test.iloc[:, 1:6].values.astype(\"f\")\ntest_fnc = test.iloc[:, 6:1384].values.astype(\"f\")\ntest_loading = test.iloc[:, 1384:].values.astype(\"f\")\n\ntest_dataset = datasets.TupleDataset(test_fnc, test_loading, test_label)","b46bb8f2":"test_preds_arr = np.zeros((5, len(test), 5))\nfor fold_id in range(5):\n    test_iter = iterators.MultiprocessIterator(\n        test_dataset, 64, repeat=False, shuffle=False, n_processes=2)\n    model = init_model(is_train=False)\n    model.to_gpu(DEVICE)\n    serializers.load_npz(\"best_model_fold{},npz\".format(fold_id), model)\n    test_pred, _ = inference_test_data(model, test_iter, DEVICE)\n    test_preds_arr[fold_id] =  test_pred\n    del test_iter\n    del test_pred\n    del model","13e16322":"test_pred = test_preds_arr.mean(axis=0)","f4b4b9f2":"test_pred.shape","23455899":"5877 * 5","18a56d6d":"test_sub = test.iloc[:, :6].copy()\ntest_sub.iloc[:, 1:] = test_pred\n\ntest_sub = pd.melt(test_sub, id_vars=\"Id\", value_name=\"Predicted\")\ntest_sub[\"Id\"] = test_sub[\"Id\"].astype(\"str\") + \"_\" +  test_sub[\"variable\"]\n\ntest_sub = pd.merge(sample_sub[\"Id\"], test_sub[[\"Id\", \"Predicted\"]], how=\"left\")\ntest_sub.to_csv('submission.csv', index=False)","1c387901":"test_sub.Predicted.isnull().value_counts()","0a5078d4":"test_sub.head()","d7b1dddd":"test_sub.tail()","ff4e2c32":"#### wrapper","75735227":"## submit","1d55311d":"### definition","fb252e62":"#### loss ","f1501618":"### make trainer","c41fb3d5":"### init model","52acc378":"### run trainer","e54bff16":"## prepare","b8730ac2":"### make dataset","7d2b3f7c":"#### others","b74394a0":"#### model","ec1060b3":"#### fold1","bddfc427":"#### prepare data","3be04b9b":"## inference","416b96b5":"### import","13ed1cbd":"# Simple Baseline (not using 3D spatial map)\n* NN model using fnc and loading\n* optimize weighted normalized absolute errors directly\n* 5-fold averaging","de668212":"## training","c6dae1aa":"### read data","e3e7cfde":"#### fold0"}}