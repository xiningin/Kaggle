{"cell_type":{"1b72f1fa":"code","1a8d38b9":"code","7c83bebc":"code","e48f20cf":"code","e320a5d1":"code","f39b2160":"code","85116d3a":"code","330bf326":"code","afe09a21":"code","615b37dc":"code","c92f7d67":"code","9e500413":"code","e4cef259":"code","99a6e2a4":"code","7a549e32":"code","1be0305d":"code","1a883bd7":"code","57b9c823":"code","3aa51aff":"code","a4623072":"code","96bde436":"code","94cdae42":"markdown","99f264ab":"markdown","788c8a56":"markdown","67d75a16":"markdown","13db4914":"markdown","36f3e263":"markdown","50d1c8a4":"markdown","a89952f0":"markdown","9d9a464a":"markdown"},"source":{"1b72f1fa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1a8d38b9":"from sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt \n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.decomposition import PCA \nfrom scipy.stats import zscore","7c83bebc":"car_data = pd.read_csv(\"\/kaggle\/input\/autompg-dataset\/auto-mpg.csv\")\ncar_data.head(10)","e48f20cf":"car_data = car_data.drop(['car name','origin'],axis=1)\ncar_data.head(2)","e320a5d1":"car_data.info()","f39b2160":"hpIsDigit = pd.DataFrame(car_data.horsepower.str.isdigit())\n\ncar_data[hpIsDigit['horsepower'] == False]","85116d3a":"car_data = car_data.replace('?',np.nan)\ncar_data[hpIsDigit['horsepower'] == False]","330bf326":"# replace the missing values with median value.\n# Note, we do not need to specify the column names below\n# every column's missing value is replaced with that column's median respectively  (axis =0 means columnwise)\n#cData = cData.fillna(cData.median())\n\nmedianFiller = lambda x: x.fillna(x.median())\ncar_data = car_data.apply(medianFiller, axis=0)\n\ncar_data['horsepower'] = car_data['horsepower'].astype('float64')","afe09a21":"X = car_data.drop(['mpg'], axis=1)\ny = car_data[['mpg']]\n\nsns.pairplot(X, diag_kind='kde');","615b37dc":"XScaled = X.apply(zscore)\nXScaled.head()","c92f7d67":"pca = PCA(n_components=6)\npca.fit(XScaled)","9e500413":"print(pca.explained_variance_)","e4cef259":"print(pca.components_)","99a6e2a4":"print(pca.explained_variance_ratio_)","7a549e32":"plt.bar(list(range(1,7)),pca.explained_variance_ratio_,alpha=0.5, align='center')\nplt.ylabel('Variation explained')\nplt.xlabel('eigen Value')\nplt.show()","1be0305d":"plt.step(list(range(1,7)),np.cumsum(pca.explained_variance_ratio_), where='mid')\nplt.ylabel('Cum of variation explained')\nplt.xlabel('eigen Value')\nplt.show()","1a883bd7":"pca3 = PCA(n_components=3)\npca3.fit(XScaled)\nprint(pca3.components_)\nprint(pca3.explained_variance_ratio_)\nXpca3 = pca3.transform(XScaled)","57b9c823":"Xpca3","3aa51aff":"sns.pairplot(pd.DataFrame(Xpca3))","a4623072":"regression_model = LinearRegression()\nregression_model.fit(XScaled, y)\nregression_model.score(XScaled, y)","96bde436":"regression_model = LinearRegression()\nregression_model.fit(Xpca3, y)\nregression_model.score(Xpca3, y)","94cdae42":"# Handling Missing Values: ","99f264ab":"# Dimensionality Reduction\nNow 3 dimensions seems very reasonable. With 3 variables we can explain over 95% of the variation in the original data!","788c8a56":"Principal Component Analysis: \nAn important machine learning method for dimensionality reduction is called Principal Component Analysis. It is a method that uses simple matrix operations from linear algebra and statistics to calculate a projection of the original data into the same number or fewer dimensions.\n\nSTEPS: \n1) STANDARDIZATION: The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis.\n\n2)STEP 2: COVARIANCE MATRIX COMPUTATION: The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or in other words, to see if there is any relationship between them. Because sometimes, variables are highly correlated in such a way that they contain redundant information. So, in order to identify these correlations, we compute the covariance matrix.\n\n3) COMPUTE THE EIGENVECTORS AND EIGENVALUES OF THE COVARIANCE MATRIX TO IDENTIFY THE PRINCIPAL COMPONENTS:\nEigenvectors and eigenvalues are the linear algebra concepts that we need to compute from the covariance matrix in order to determine the principal components of the data.\n\n","67d75a16":"* # BiVariate Plots\nA bivariate analysis among the different independent variables can be done using scatter matrix plot. Seaborn libs create a dashboard reflecting useful information about the dimensions. ","13db4914":"Here, let's replace them with their median values. First replace '?' with NaN and then replace NaN with median.","36f3e263":"# Fit Linear Model\nLets construct two linear models. The first with all the 6 independent variables and the second with only the 3 new variables constructed using PCA.","50d1c8a4":"Looks like by drop reducing dimensionality by 3, we only dropped around 3% in R^2! This is insample (on training data) and hence a drop in R^2 is expected. Still seems easy to justify the dropping of variables. An out of sample (on test data), with the 3 independent variables is likely to do better since that would be less of an over-fit.","a89952f0":"As we are doing the principal component analysis, we are dropping the categorical variables.","9d9a464a":"# Principal Component Analysis"}}