{"cell_type":{"ca78ce82":"code","5b881d04":"code","3eed54fc":"code","e41d330f":"code","2b184263":"code","09bc806a":"code","f0d5b15d":"code","0c4de9ba":"code","bac1e28b":"markdown","c146224f":"markdown","433a4c6a":"markdown","71fab4b2":"markdown","7202d8c5":"markdown","f2330e59":"markdown","d91d4646":"markdown","d8d042bf":"markdown"},"source":{"ca78ce82":"# imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as ses\nimport warnings\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report,plot_confusion_matrix\nwarnings.filterwarnings('ignore')\nfrom sklearn.ensemble import RandomForestClassifier\n","5b881d04":"# Data Fetch\nfile='\/kaggle\/input\/early-diabetes-classification\/diabetes_data.csv'\ndf=pd.read_csv(file, delimiter=';')\ndf.head()\n","3eed54fc":"# Selected Columns\nfeatures=['age', 'polyuria', 'polydipsia', 'sudden_weight_loss', 'weakness', 'polyphagia', 'genital_thrush', 'visual_blurring', 'irritability', 'partial_paresis', 'muscle_stiffness', 'alopecia', 'gender']\ntarget='class'\n# X & Y\nX=df[features]\nY=df[target]\n","e41d330f":"# Handling AlphaNumeric Features\nX=pd.get_dummies(X)\n","2b184263":"f,ax = plt.subplots(figsize=(18, 18))\nmatrix = np.triu(X.corr())\nses.heatmap(X.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax, mask=matrix)\nplt.show()\n","09bc806a":"# Data split for training and testing\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=123)\n","f0d5b15d":"#Model Parameters\nparam={'criterion': 'gini', 'n_estimators': 120, 'max_features': 'auto', 'max_depth': 39, 'n_jobs': -1}\n# Model Initialization\nmodel=RandomForestClassifier(**param)\nmodel.fit(X_train,Y_train)\n","0c4de9ba":"# Confusion Matrix\nplot_confusion_matrix(model,X_test,Y_test,cmap=plt.cm.Blues)\n# Classification Report\nprint(classification_report(Y_test,model.predict(X_test)))\n","bac1e28b":"### Train & Test\n The train-test split is a procedure for evaluating the performance of an algorithm.The procedure involves taking a dataset and dividing it into two subsets.The first subset is utilized to fit\/train the model.The second subset is used for prediction.The main motive is to estimate the performance of the model on new data.","c146224f":"# Early Diabetes Prediction using BlobCity AutoAI\n\n*This code is automatically generated using [BlobCity AutoAI](https:\/\/github.com\/blobcity\/autoai)*\n","433a4c6a":"### Data Fetch\n Pandas is an open-source, BSD-licensed library providing high-performance,easy-to-use data manipulation and data analysis tools.","71fab4b2":"### Accuracy Metrics\n Performance metrics are a part of every machine learning pipeline. They tell you if you're making progress, and put a number on it. All machine learning models,whether it's linear regression, or a SOTA technique like BERT, need a metric to judge performance.","7202d8c5":"### Data Encoding\n Converting the string classes data in the datasets by encoding them to integer either using OneHotEncoding or LabelEncoding","f2330e59":"### Correlation Matrix\n In order to check the correlation between the features, we will plot a correlation matrix. It is effective in summarizing a large amount of data where the goal is to see patterns.","d91d4646":"### Model\n\nA random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the <code>max_samples<\/code> parameter if <code>bootstrap=True<\/code> (default), otherwise the whole dataset is used to build each tree.\n\n#### Model Tuning Parameters\n\n1. n_estimators : The number of trees in the forest.\n\n2. criterion : The function to measure the quality of a split. Supported criteria are 'gini' for the Gini impurity and 'entropy' for the information gain.\n\n3. max_depth : The maximum depth of the tree.\n\n4. max_features : The number of features to consider when looking for the best split:\n\n5. bootstrap : Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.\n\n6. oob_score : Whether to use out-of-bag samples to estimate the generalization accuracy.\n","d8d042bf":"### Feature Selection\n It is the process of reducing the number of input variables when developing a predictive model.Used to reduce the number of input variables to reduce the computational cost of modelling and,in some cases,to improve the performance of the model."}}