{"cell_type":{"0f25fb45":"code","a1837bd2":"code","c7c6f40a":"code","ccf63f51":"code","c9a035d5":"code","fe74517d":"code","5352f131":"code","b10d6ee8":"code","ccebfe13":"code","71cf1be6":"code","0ee3bcd3":"code","3915609f":"code","65d4cdec":"code","f383c7f5":"code","1e827f47":"code","73b6ba5c":"code","88a01692":"code","f1307663":"code","a59cd432":"code","41768bc1":"code","22928c3f":"code","47294549":"code","bff99168":"code","2d765407":"code","4ed2863e":"code","96b86e6e":"code","32d39c07":"code","b2010245":"code","8d76c822":"code","e91f05e6":"code","062263fd":"code","f78ce8c8":"code","912b2a39":"code","ad53f2b5":"code","6b7a390f":"code","6c4dcdfd":"code","752d9f32":"code","07ad433a":"code","f884ff6a":"code","d003b7c0":"code","9c18c5f1":"code","e44f5a2e":"code","d1a185da":"code","a2446231":"code","3ffbd810":"code","e12740de":"code","6e25da49":"code","60200b0b":"code","d131689a":"code","d5b0b788":"code","ae2657ff":"code","2d4493f1":"code","92e65466":"markdown","91a6f96a":"markdown","b9936f5d":"markdown","c8329518":"markdown","9b735adb":"markdown","af3b4fc0":"markdown","b568032e":"markdown","ca9c96f4":"markdown","8be97557":"markdown","9c822559":"markdown","139dd03c":"markdown","e99248d8":"markdown","31dbb949":"markdown","6d58b50e":"markdown","3d4bd6f5":"markdown","426a4ad6":"markdown","e9432b93":"markdown","3313798d":"markdown","512431e3":"markdown","97382032":"markdown","7e7975b7":"markdown","203303a1":"markdown","b40aebb3":"markdown","7337ed5a":"markdown","5df94c10":"markdown","f4358e6f":"markdown","cd26dd6a":"markdown","5ce1bed1":"markdown","53fe51ba":"markdown","c5567cfb":"markdown","efc608e6":"markdown","0e074422":"markdown","08991130":"markdown","c40d753c":"markdown","d0575195":"markdown","f48bb5c6":"markdown","f6e58a08":"markdown","f0e7a749":"markdown","fd0f3960":"markdown","04552093":"markdown","0ae70b5f":"markdown","282b3ee5":"markdown"},"source":{"0f25fb45":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a1837bd2":"df = pd.read_csv('\/kaggle\/input\/online-shoppers-intention\/online_shoppers_intention.csv')\ndf.head()","c7c6f40a":"df.describe(include='all')","ccf63f51":"# Check for null values in data\nnullcount = df.isnull().sum()\nprint('Total number of null values in dataset:', nullcount.sum())","c9a035d5":"df.shape","fe74517d":"# Visualize the data\nsns.countplot(df['Revenue'])\nplt.ylim(0,12000)\nplt.title('Was the transaction completed?', fontsize= 15)\nplt.xlabel('Transaction Completed', fontsize=12)\nplt.ylabel('Count (Entries)', fontsize=12)\nplt.text(x=-.175, y=11000 ,s='10,422', fontsize=15)\nplt.text(x=.875, y=2500, s='1908', fontsize=15)\nplt.show()","5352f131":"df.select_dtypes(include=['int64', 'float64']).hist(figsize=(16,22))","b10d6ee8":"feats = df.drop('Revenue', axis=1)\ntarget = df['Revenue']","ccebfe13":"print(f'Features table has {feats.shape[0]} rows and {feats.shape[1]} columns')\nprint(f'Target table has {target.shape[0]} rows')","71cf1be6":"# Checking for number of unique values for each feature\nuniques = feats.nunique(axis=0)\nprint(uniques)","0ee3bcd3":"feats['Weekend'].value_counts()","3915609f":"feats['Weekend'].value_counts().plot(kind='bar')","65d4cdec":"feats['is_weekend'] = feats['Weekend'].apply(lambda row: 1 if row == True else 0)","f383c7f5":"feats[['Weekend','is_weekend']].tail()","1e827f47":"feats.drop('Weekend', axis=1, inplace=True) ","73b6ba5c":"feats['VisitorType'].value_counts()","88a01692":"feats['VisitorType'].value_counts().plot(kind='bar')","f1307663":"colname = 'VisitorType'\nvisitor_type_dummies = pd.get_dummies(feats[colname], prefix=colname)\npd.concat([feats[colname], visitor_type_dummies], axis=1).tail(n=10)","a59cd432":"visitor_type_dummies.drop('VisitorType_Other', axis=1, inplace=True)\nvisitor_type_dummies.head()","41768bc1":"feats = pd.concat([feats, visitor_type_dummies], axis=1)\nfeats.drop('VisitorType', axis=1, inplace=True) ","22928c3f":"feats['Month'].value_counts()","47294549":"colname = 'Month'\nmonth_dummies = pd.get_dummies(feats[colname], prefix=colname)\nmonth_dummies.drop(colname+'_Feb', axis=1, inplace=True)\nfeats = pd.concat([feats, month_dummies], axis=1)\nfeats.drop('Month', axis=1, inplace=True) ","bff99168":"feats.iloc[0]","2d765407":"feats.dtypes","4ed2863e":"target= target.apply(lambda row: 1 if row==True else 0)\ntarget.head(n=10)","96b86e6e":"feats['OperatingSystems'].value_counts()","32d39c07":"colname = 'OperatingSystems'\noperation_system_dummies = pd.get_dummies(feats[colname], prefix=colname)\noperation_system_dummies.drop(colname+'_5', axis=1, inplace=True)\nfeats = pd.concat([feats, operation_system_dummies], axis=1)","b2010245":"feats['Browser'].value_counts()","8d76c822":"colname = 'Browser'\nbrowser_dummies = pd.get_dummies(feats[colname], prefix=colname)\nbrowser_dummies.drop(colname+'_9', axis=1, inplace=True)\nfeats = pd.concat([feats, browser_dummies], axis=1)","e91f05e6":"feats['TrafficType'].value_counts()","062263fd":"colname = 'TrafficType'\ntraffic_dummies = pd.get_dummies(feats[colname], prefix=colname)\ntraffic_dummies.drop(colname+'_17', axis=1, inplace=True)\nfeats = pd.concat([feats, traffic_dummies], axis=1)","f78ce8c8":"feats['Region'].value_counts()","912b2a39":"colname = 'Region'\nregion_dummies = pd.get_dummies(feats[colname], prefix=colname)\nregion_dummies.drop(colname+'_5', axis=1, inplace=True)\nfeats = pd.concat([feats, region_dummies], axis=1)","ad53f2b5":"drop_cols = ['OperatingSystems', 'Browser', 'TrafficType', 'Region']\nfeats.drop(drop_cols, inplace=True, axis=1)","6b7a390f":"feats.dtypes","6c4dcdfd":"# caluclate the proportion of each target value\ntarget.value_counts()\/target.shape[0]*100","752d9f32":"y_baseline = pd.Series(data=[0]*target.shape[0])","07ad433a":"precision, recall, fscore, _ = metrics.precision_recall_fscore_support(y_pred=y_baseline, y_true=target, average='macro', zero_division=1)\nprint(f'Precision: {precision:.4f}\\nRecall: {recall:.4f}\\nfscore: {fscore:.4f}')","f884ff6a":"X_train, X_test, y_train, y_test = train_test_split(feats, target, test_size=0.2, random_state=42)","d003b7c0":"print(f'Shape of X_train: {X_train.shape}')\nprint(f'Shape of y_train: {y_train.shape}')\nprint(f'Shape of X_test: {X_test.shape}')\nprint(f'Shape of y_test: {y_test.shape}')","9c18c5f1":"model = LogisticRegression(max_iter=10000,random_state=42)\nmodel.fit(X_train, y_train)","e44f5a2e":"y_pred = model.predict(X_test)","d1a185da":"accuracy = metrics.accuracy_score(y_pred=y_pred, y_true=y_test)\nprint(f'Accuracy of the model is {accuracy*100:.4f}%')","a2446231":"precision, recall, fscore, _ = metrics.precision_recall_fscore_support(y_pred=y_pred, y_true=y_test, average='binary')\nprint(f'Precision: {precision:.4f}\\nRecall: {recall:.4f}\\nfscore: {fscore:.4f}')","3ffbd810":"coef_list = [f'{feature}: {coef}' for coef, feature in sorted(zip(model.coef_[0], X_train.columns.values.tolist()))]\nfor item in coef_list:\n    print(item)","e12740de":"from sklearn.linear_model import LogisticRegressionCV\n# help(LogisticRegressionCV)\nCs = np.logspace(-2, 6, 9)\nmodel_l1 = LogisticRegressionCV(Cs=Cs, penalty='l1', cv=10, solver='liblinear', random_state=42, max_iter=10000)\nmodel_l2 = LogisticRegressionCV(Cs=Cs, penalty='l2', cv=10, random_state=42, max_iter=10000)\n\nmodel_l1.fit(X_train, y_train)\nmodel_l2.fit(X_train, y_train)","6e25da49":"# best hyperparameters\nprint(f'Best hyperparameter for l1 regularization model: {model_l1.C_[0]}')\nprint(f'Best hyperparameter for l2 regularization model: {model_l2.C_[0]}')","60200b0b":"y_pred_l1 = model_l1.predict(X_test)\ny_pred_l2 = model_l2.predict(X_test)","d131689a":"accuracy_l1 = metrics.accuracy_score(y_pred=y_pred_l1, y_true=y_test)\naccuracy_l2 = metrics.accuracy_score(y_pred=y_pred_l2, y_true=y_test)\nprint(f'Accuracy of the model with l1 regularization is {accuracy_l1*100:.4f}%')\nprint(f'Accuracy of the model with l2 regularization is {accuracy_l2*100:.4f}%')","d5b0b788":"precision_l1, recall_l1, fscore_l1, _ = metrics.precision_recall_fscore_support(y_pred=y_pred_l1, y_true=y_test, average='binary')\nprecision_l2, recall_l2, fscore_l2, _ = metrics.precision_recall_fscore_support(y_pred=y_pred_l2, y_true=y_test, average='binary')\nprint(f'l1\\nPrecision: {precision_l1:.4f}\\nRecall: {recall_l1:.4f}\\nfscore: {fscore_l1:.4f}\\n\\n')\nprint(f'l2\\nPrecision: {precision_l2:.4f}\\nRecall: {recall_l2:.4f}\\nfscore: {fscore_l2:.4f}')","ae2657ff":"coef_list = [f'{feature}: {coef}' for coef, feature in sorted(zip(model_l1.coef_[0], X_train.columns.values.tolist()))]\nfor item in coef_list:\n    print(item)","2d4493f1":"coef_list = [f'{feature}: {coef}' for coef, feature in sorted(zip(model_l2.coef_[0], X_train.columns.values.tolist()))]\nfor item in coef_list:\n    print(item)","92e65466":"Here we can see that none of the coefficients go right down to zero, which is rare when applying l2 regularization, this is because the feature coefficients get penalized less when they small, and much greater when the coefficients are larger.","91a6f96a":"Overall the model with l2 performs slightly better.","b9936f5d":"Let's look at the distribution of numerical values","c8329518":"## Loading Data","9b735adb":"### Other evaluation metrics\n\nOther common metrics in classification models are precision, recall, and f1-score.\nRecall is defined as the proportion of correct positive predictions relative to total true postive values. Precision is defined as the proportion of correct positive predictions relative to total predicted postive values. F1 score is a combination of precision and recall, defined as 2 times the product of precision and recall, divided by the sum of the two.\n\nIt's useful to use these other evaluation metrics other than accuracy when the distribution of true and false values. We want these values to be as close to 1.0 as possible.","af3b4fc0":"Repeat for the `Browser` column","b568032e":"Finally, drop all the original, unmodified columns","ca9c96f4":"l1 regularization tends to send coefficients all the way down to zero, and is useful for reducing the total number of features in a training dataset. Here we can see some columns are very close to zero. \n\nLet's take a look at the the model coefficients for the model with l2 reglarization.","8be97557":"## EDA","9c822559":"### Feature importances\n   \nExamining the feature importances can show us how the regularization affected the values of the coefficients","139dd03c":"The value of \"9\" appears least, so that is the dummy column we will drop","e99248d8":"Finally repeat for the `Region` column","31dbb949":"## Adding regularization to the model","6d58b50e":"### Categorical columns\nThe other columns are categorical so we will have to deal with them a little differently.","3d4bd6f5":"## Baseline model\nA baseline model should be simple and well understood procedure, and the performance of this model should act as the bedrock for the lowest acceptable performance for any model we build.\n","426a4ad6":"Repeat for the `TrafficType` column","e9432b93":"Currently the operating system, browser, region, traffic type columns are encoded as integer types. This may assume some order to the columns when in fact there is none. For example, a value for the operating system column of \"2\" does not mean it is two times a value of \"1\", since they are just arbitrary labels. Since they are categorical variables we will convert them into dummy variables as this will represent the data more appropriately.\n\nFirst we will look at the `OperatingSystems` column.","3313798d":"Now we will join it back to the original dataset","512431e3":"### Other evaluation metrics\n\nLet's test again with the other evaluation metrics","97382032":"Now we can see that our baseline model would to predict '0', and that this with a 84.525547% accuracy.","7e7975b7":"#### Target variable\n\nWe can do a similar task to the target variable by making all the columns into numerical data types.","203303a1":"We can see that there is a bit of redundant information here, we have three total options, but we know that whenever two of the columns are zero, the other column *has* to be 1, since everyone has to fall into one of the three options. We can then drop one of the columns and assume that a zero in the remaining columns means that the dropped column is equal to one.\n\nHere we will drop the `VisitorType_Other` column since it occurs with the least frequency.","b40aebb3":"We can see that there are three options for the `VisitorType` column, so we will make this a categorical column.\n\nWe can do this using the `get_dummies` function in the pandas library","7337ed5a":"## Column Descriptions:\n\n**Administrative:** This is the number of pages of this type (administrative) that the user visited.\n\n**Administrative_Duration:** This is the amount of time spent in this category of pages.\n\n**Informational:** This is the number of pages of this type (informational) that the user visited.\n\n**Informational_Duration:** This is the amount of time spent in this category of pages.\n\n**ProductRelated:** This is the number of pages of this type (product related) that the user visited.\n\n**ProductRelated_Duration:** This is the amount of time spent in this category of pages.\n\n**BounceRates:** The percentage of visitors who enter the website through that page and exit without triggering any additional tasks.\n\n**ExitRates:** The percentage of pageviews on the website that end at that specific page.\n\n**PageValues:** The average value of the page averaged over the value of the target page and\/or the completion of an eCommerce transaction.\n\n**SpecialDay:** This value represents the closeness of the browsing date to special days or holidays (eg Mother's Day or Valentine's day) in which the transaction is more likely to be finalized. More information about how this value is calculated below.\n\n**Month:** Contains the month the pageview occurred, in string form.\n\n**OperatingSystems:** An integer value representing the operating system that the user was on when viewing the page.\n\n**Browser:** An integer value representing the browser that the user was using to view the page.\n\n**Region:** An integer value representing which region the user is located in.\n\n**TrafficType:** An integer value representing what type of traffic the user is categorized into.\n\n**VisitorType:** A string representing whether a visitor is New Visitor, Returning Visitor, or Other.\n\n**Weekend:** A boolean representing whether the session is on a weekend.\n\n**Revenue:** A boolean representing whether or not the user completed the purchase.\n","5df94c10":"Here we will drop the month of February since it occurs with the lowest frequency.","f4358e6f":"## Dataset Intro\nThe data set is a set of 18 features: 10 numerical and 8 categorical. This dataset has 12330 entries, split into 10,422 entries where the shoppers did not purchase and 1908 entries where the shoppers did purchase. Each entry is based on unique users in a 1-year period to avoid any trends specific to a specific campaign.","cd26dd6a":"Let's do the same for the `Month` column","5ce1bed1":"We fit our model first by instantiating it, then by fitting the model to the training data","53fe51ba":"# Classifying Shoppers Intention\n\n![Interesting-Trends-for-Online-Shopping.jpg](attachment:Interesting-Trends-for-Online-Shopping.jpg)","c5567cfb":"Let's check the data types to confirm that that the are all numerical","efc608e6":"Here we can see that there are no null values in the dataset.","0e074422":"### Converting non-numercal columns to numerical\n\n#### Binary columns\n\nConvert 'Weekend' column to binary, rename to 'is_weekend'","08991130":"The value of \"17\" appears least, so that is the dummy column we will drop","c40d753c":"## Logistic regression model","d0575195":"The value of \"5\" appears least, so that is the dummy column we will drop","f48bb5c6":"Now let's compare against the true values. Let's start by using accuracy, accuracy is defined as the propotion of correct predictions out of the total predictions.","f6e58a08":"## Import Packages","f0e7a749":"The value of \"5\" appears least, so that is the dummy column we will drop","fd0f3960":"86.9830% - that's not bad with for a simple model with little feature engineering!","04552093":"To test the model performance we will predict the outcome on the test features (X_test), and compare those outcomes to real values (y_test)","0ae70b5f":"Let's check the data types to confirm that that the are all numerical","282b3ee5":"### Feature importances\n   \n\nWe can look at which features are important by looking at the magnitude of the coefficients. Those with a larger coefficients will have a greater contribution to the result. Those with a positive value will make the result head toward the true result, that the customer will not subscribe. Features with a negative value for the coefficient will make the result heads towards a false result, that the customer will not subscribe to the product."}}