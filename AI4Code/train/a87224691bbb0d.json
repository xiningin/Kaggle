{"cell_type":{"d83ac62b":"code","eaa7badc":"code","22f30132":"code","b3273a06":"code","cc13a0e2":"code","af6ad406":"code","0af85752":"code","c2609420":"code","e2de931e":"code","e54d07ec":"code","2eba9a9f":"code","a534dd5e":"code","8d28d5ab":"code","f9347bf7":"code","071b6010":"code","34476640":"code","bb3ef906":"code","d6d394c9":"code","0461ef4e":"code","d2b08ac8":"code","01834a8c":"code","669a3f6e":"code","d48a5f92":"code","774bf505":"code","71568b58":"code","c93e95f9":"code","400f4064":"code","10a943ad":"code","c5c22a50":"code","82f8e187":"code","264cfee7":"code","dbced056":"code","dc3b578d":"code","d8bc732a":"code","308459a6":"code","81f9891a":"code","197e2a09":"code","5a5a4fa1":"code","507b7cd1":"code","0ac17072":"code","132fcd40":"markdown","b9692d02":"markdown","35fec3b6":"markdown","61a9ed59":"markdown","91ef028f":"markdown","fbd9b20d":"markdown","feed2a96":"markdown","d2794de4":"markdown","818d98b3":"markdown","9a47bf64":"markdown","bfb28879":"markdown","b61358fd":"markdown","ca053a67":"markdown","e00dc346":"markdown","5ea8ca2e":"markdown","0d9b4cca":"markdown","631115c3":"markdown"},"source":{"d83ac62b":"import pandas as pd\nimport numpy as np\nfrom pprint import pprint\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport holoviews as hv\nhv.extension('bokeh', 'matplotlib', logo=False)\n\n# Avoid warnings to show up (trick for the final notebook on kaggle)\nimport warnings\nwarnings.filterwarnings('ignore')","eaa7badc":"df=pd.read_csv('..\/input\/hmeq.csv', low_memory=False) # No duplicated columns, no highly correlated columns\ndf.drop('DEBTINC', axis=1, inplace=True) # The meaning of this variable is not clear. Better drop it for the moment\ndf.dropna(axis=0, how='any', inplace=True)","22f30132":"df[df['BAD']==0].drop('BAD', axis=1).describe().style.format(\"{:.2f}\")","b3273a06":"df[df['BAD']==1].drop('BAD', axis=1).describe().style.format(\"{:.2f}\")","cc13a0e2":"df.loc[df.BAD == 1, 'STATUS'] = 'DEFAULT'\ndf.loc[df.BAD == 0, 'STATUS'] = 'PAID'","af6ad406":"g = df.groupby('REASON')\ng['STATUS'].value_counts(normalize=True).to_frame().style.format(\"{:.1%}\")","0af85752":"g = df.groupby('JOB')\ng['STATUS'].value_counts(normalize=True).to_frame().style.format(\"{:.1%}\")","c2609420":"%%opts Bars[width=700 height=400 tools=['hover'] xrotation=45]{+axiswise +framewise}\n\n# Categorical\n\ncols = ['REASON', 'JOB']\n\ndd={}\n\nfor col in cols:\n\n    counts=df.groupby(col)['STATUS'].value_counts(normalize=True).to_frame('val').reset_index()\n    dd[col] = hv.Bars(counts, [col, 'STATUS'], 'val') \n    \nvar = [*dd]\nkdims=hv.Dimension(('var', 'Variable'), values=var)    \nhv.HoloMap(dd, kdims=kdims)","e2de931e":"%%opts Histogram[width=700 height=400 tools=['hover'] xrotation=0]{+axiswise +framewise}\n\ng = df.groupby('STATUS')\n\ncols = ['LOAN',\n        'MORTDUE', \n        'VALUE',\n        'YOJ',\n        'DEROG',\n        'DELINQ',\n        'CLAGE',\n        'NINQ',\n        'CLNO']\ndd={}\n\n# Histograms\nfor col in cols:\n    \n    freq, edges = np.histogram(df[col].values)\n    dd[col] = hv.Histogram((edges, freq), label='ALL Loans').redim.label(x=' ')\n    \n    freq, edges = np.histogram(g.get_group('PAID')[col].values, bins=edges)\n    dd[col] *= hv.Histogram((edges, freq), label='PAID Loans').redim.label(x=' ')\n    \n    freq, edges = np.histogram(g.get_group('DEFAULT')[col].values, bins=edges)\n    dd[col] *= hv.Histogram((edges, freq), label='DEFAULT Loans' ).redim.label(x=' ')   \n    \nvar = [*dd]\nkdims=hv.Dimension(('var', 'Variable'), values=var)    \nhv.HoloMap(dd, kdims=kdims)","e54d07ec":"%%opts Scatter[width=500 height=500 tools=['hover'] xrotation=0]{+axiswise +framewise}\n\ng = df.groupby('STATUS')\n\ncols = ['LOAN',\n        'MORTDUE',\n        'VALUE',\n        'YOJ',\n        'DEROG',\n        'DELINQ',\n        'CLAGE',\n        'NINQ',\n        'CLNO']\n\nimport itertools\nprod = list(itertools.combinations(cols,2))\n\ndd = {}\n\nfor p in prod:\n    dd['_'.join(p)] = hv.Scatter(g.get_group('PAID')[list(p)], label='PAID Loans').options(size=5)\n    dd['_'.join(p)] *= hv.Scatter(g.get_group('DEFAULT')[list(p)], label='DEFAULT Loans').options(size=5, marker='x')\n    \nvar = [*dd]\nkdims=hv.Dimension(('var', 'Variable'), values=var)    \nhv.HoloMap(dd, kdims=kdims).collate()","2eba9a9f":"g=sns.PairGrid(df.drop('BAD',axis=1), hue='STATUS', diag_sharey=False, palette={'PAID': 'C0', 'DEFAULT':'C1'})\ng.map_lower(sns.kdeplot)\ng.map_upper(sns.scatterplot)\ng.map_diag(sns.kdeplot, lw=3)\ng.add_legend()\nplt.show()","a534dd5e":"cols=['YOJ', 'CLAGE', 'NINQ']\n\nfor col in cols:\n    \n    plt.figure(figsize=(15,5))\n\n    sns.violinplot(x='JOB', y=col, hue='STATUS',\n                   split=True, inner=\"quart\",  palette={'PAID': 'C0', 'DEFAULT':'C1'},\n                   data=df)\n    \n    sns.despine(left=True)","8d28d5ab":"def compute_corr(df,size=10):\n    '''Function plots a graphical correlation matrix for each pair of columns in the dataframe.\n\n    Input:\n        df: pandas DataFrame\n        size: vertical and horizontal size of the plot'''\n    import scipy\n    import scipy.cluster.hierarchy as sch\n    \n    corr = df.corr()\n    \n    # Clustering\n    d = sch.distance.pdist(corr)   # vector of ('55' choose 2) pairwise distances\n    L = sch.linkage(d, method='complete')\n    ind = sch.fcluster(L, 0.5*d.max(), 'distance')\n    columns = [df.select_dtypes(include=[np.number]).columns.tolist()[i] for i in list((np.argsort(ind)))]\n    \n    # Reordered df upon custering results\n    df = df.reindex(columns, axis=1)\n    \n    # Recompute correlation matrix w\/ clustering\n    corr = df.corr()\n    #corr.dropna(axis=0, how='all', inplace=True)\n    #corr.dropna(axis=1, how='all', inplace=True)\n    #corr.fillna(0, inplace=True)\n    \n    #fig, ax = plt.subplots(figsize=(size, size))\n    #img = ax.matshow(corr)\n    #plt.xticks(range(len(corr.columns)), corr.columns, rotation=45);\n    #plt.yticks(range(len(corr.columns)), corr.columns);\n    #fig.colorbar(img)\n    \n    return corr","f9347bf7":"%%opts HeatMap [tools=['hover'] colorbar=True width=500  height=500 toolbar='above', xrotation=45, yrotation=45]\n\ncorr=compute_corr(df)\ncorr=corr.stack(level=0).to_frame('value').reset_index()\nhv.HeatMap(corr).options(cmap='Viridis')","071b6010":"import pandas as pd\nimport numpy as np\nfrom pprint import pprint\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import classification_report","34476640":"df=pd.read_csv('..\/input\/hmeq.csv', low_memory=False) # No duplicated columns, no highly correlated columns\ndf=pd.get_dummies(df, columns=['REASON','JOB'])\ndf.drop('DEBTINC', axis=1, inplace=True)\ndf.dropna(axis=0, how='any', inplace=True)\ny = df['BAD']\nX = df.drop(['BAD'], axis=1)","bb3ef906":"def cross_validate_model(model, X, y, \n                         scoring=['f1', 'precision', 'recall', 'roc_auc'], \n                         cv=12, n_jobs=-1, verbose=True):\n    \n    scores = cross_validate(pipe, \n                        X, y, \n                        scoring=scoring,\n                        cv=cv, n_jobs=n_jobs, \n                        verbose=verbose,\n                        return_train_score=False)\n\n    #sorted(scores.keys())\n    dd={}\n    \n    for key, val in scores.items():\n        if key in ['fit_time', 'score_time']:\n            continue\n        #print('{:>30}: {:>6.5f} +\/- {:.5f}'.format(key, np.mean(val), np.std(val)) )\n        name = \" \".join(key.split('_')[1:]).capitalize()\n        \n        dd[name] = {'value' : np.mean(val), 'error' : np.std(val)}\n        \n    return  pd.DataFrame(dd)    \n    #print()\n    #pprint(scores)\n    #print()","d6d394c9":"def plot_roc(model, X_test ,y_test, n_classes=0):\n    \n    from sklearn.metrics import roc_curve, auc\n    \n    \"\"\"\n    Target scores, can either be probability estimates \n    of the positive class, confidence values, or \n    non-thresholded measure of decisions (as returned \n    by \u201cdecision_function\u201d on some classifiers).\n    \"\"\"\n    try:\n        y_score = model.decision_function(X_test)\n    except Exception as e:\n        y_score = model.predict_proba(X_test)[:,1]\n    \n    \n    fpr, tpr, _ = roc_curve(y_test.ravel(), y_score.ravel())\n    roc_auc = auc(fpr, tpr)\n\n    # Compute micro-average ROC curve and ROC area\n    #fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n    #roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n    \n    #plt.figure()\n    lw = 2\n    plt.plot(fpr, tpr, color='darkorange',\n             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n\n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    #plt.show()\n    \n# shuffle and split training and test sets\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,\n#                                                    random_state=0)","0461ef4e":"def plot_confusion_matrix(model, X_test ,y_test,\n                          classes=[0,1],\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \n    import itertools\n    from sklearn.metrics import confusion_matrix\n    \n    y_pred = model.predict(X_test)\n    \n    # Compute confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n    np.set_printoptions(precision=2)\n    \n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    #    print(\"Normalized confusion matrix\")\n    #else:\n    #    print('Confusion matrix, without normalization')\n\n    #print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","d2b08ac8":"def feature_importance(coef, names, verbose=False, plot=True):\n    \n    #importances = model.feature_importances_\n\n    \n    \n    #std = np.std([tree.feature_importances_ for tree in model.estimators_],\n    #             axis=0)\n    indices = np.argsort(coef)[::-1]\n    \n    if verbose:\n    \n        # Print the feature ranking\n        print(\"Feature ranking:\")\n    \n        for f in range(len(names)):\n            print(\"{:>2d}. {:>15}: {:.5f}\".format(f + 1, names[indices[f]], coef[indices[f]]))\n        \n    if plot:\n        \n        # Plot the feature importances of the forest\n        #plt.figure(figsize=(5,10))\n        plt.title(\"Feature importances\")\n        plt.barh(range(len(names)), coef[indices][::-1], align=\"center\")\n        #plt.barh(range(X.shape[1]), importances[indices][::-1],\n        #         xerr=std[indices][::-1], align=\"center\")\n        plt.yticks(range(len(names)), names[indices][::-1])\n        #plt.xlim([-0.001, 1.1])\n        #plt.show()","01834a8c":"def plot_proba(model, X, y, bins=40, show_class = 1):\n    \n    from sklearn.calibration import CalibratedClassifierCV\n    \n    model = CalibratedClassifierCV(model)#, cv='prefit')\n    \n    model.fit(X, y)\n    \n    proba=model.predict_proba(X)\n    \n    if show_class == 0:\n        sns.kdeplot(proba[y==0,0], shade=True, color=\"r\", label='True class')\n        sns.kdeplot(proba[y==0,1], shade=True, color=\"b\", label='Wrong class')\n        plt.title('Classification probability: Class 0')\n    elif show_class == 1:\n        sns.kdeplot(proba[y==1,1], shade=True, color=\"r\", label='True class')\n        sns.kdeplot(proba[y==1,0], shade=True, color=\"b\", label='Wrong class')\n        plt.title('Classification probability: Class 1')\n    plt.legend()","669a3f6e":"from sklearn.linear_model import LogisticRegression\n\nsteps = [('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)),\n         ('model', LogisticRegression(random_state=0))]\n\npipe = Pipeline(steps)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)\npipe.fit(X_train, y_train)","d48a5f92":"plt.figure(figsize=(15,10))\n\nplt.subplot(221)\nplot_roc(pipe, X_test ,y_test)\n\nplt.subplot(222)\nplot_confusion_matrix(pipe, X_test ,y_test, normalize=True)\n\nplt.subplot(223)\nplot_proba(pipe, X_test, y_test)\n\nplt.subplot(224)\nfeature_importance(pipe.named_steps['model'].coef_[0], X.columns)\n\nplt.tight_layout()","774bf505":"logit_xval_res = cross_validate_model(pipe, X, y, verbose=False)\nlogit_xval_res.T[['value','error']].style.format(\"{:.2f}\")","71568b58":"from sklearn.linear_model import SGDClassifier\n\nsteps = [('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)),\n         ('model', SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=1000, tol=1e-3, random_state=0))]\n\npipe = Pipeline(steps)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)\npipe.fit(X_train, y_train)","c93e95f9":"plt.figure(figsize=(15,10))\n\nplt.subplot(221)\nplot_roc(pipe, X_test ,y_test)\n\nplt.subplot(222)\nplot_confusion_matrix(pipe, X_test ,y_test, normalize=True)\n\nplt.subplot(223)\nplot_proba(pipe, X_test, y_test)\n\nplt.subplot(224)\nfeature_importance(pipe.named_steps['model'].coef_[0], X.columns)\n\nplt.tight_layout()","400f4064":"sgd_xval_res = cross_validate_model(pipe, X, y, verbose=False)\nsgd_xval_res.T[['value','error']].style.format(\"{:.2f}\")","10a943ad":"from sklearn.svm import SVC\n\nsteps = [('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)),\n         ('model', SVC(random_state=0, kernel='linear', probability=True))]\n\npipe = Pipeline(steps)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)\npipe.fit(X_train, y_train)","c5c22a50":"plt.figure(figsize=(15,10))\n\nplt.subplot(221)\nplot_roc(pipe, X_test ,y_test)\n\nplt.subplot(222)\nplot_confusion_matrix(pipe, X_test ,y_test, normalize=True)\n\nplt.subplot(223)\nplot_proba(pipe, X_test, y_test)\n\nplt.subplot(224)\nfeature_importance(pipe.named_steps['model'].coef_[0], X.columns)\n\nplt.tight_layout()","82f8e187":"svc_xval_res = cross_validate_model(pipe, X, y, verbose=False)\nsvc_xval_res.T[['value','error']].style.format(\"{:.2f}\")","264cfee7":"from sklearn.ensemble import GradientBoostingClassifier\n\nsteps = [('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)),\n         ('model', GradientBoostingClassifier(n_estimators=250, learning_rate=0.05, random_state=0))]\n\npipe = Pipeline(steps)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)\npipe.fit(X_train, y_train)","dbced056":"plt.figure(figsize=(15,10))\n\nplt.subplot(221)\nplot_roc(pipe, X_test ,y_test)\n\nplt.subplot(222)\nplot_confusion_matrix(pipe, X_test ,y_test, normalize=True)\n\nplt.subplot(223)\nplot_proba(pipe, X_test, y_test)\n\nplt.subplot(224)\nfeature_importance(pipe.named_steps['model'].feature_importances_, X.columns)\n\nplt.tight_layout()","dc3b578d":"gbc_xval_res = cross_validate_model(pipe, X, y, verbose=False)\ngbc_xval_res.T[['value','error']].style.format(\"{:.2f}\")","d8bc732a":"from sklearn.ensemble import RandomForestClassifier\n\nsteps = [('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)),\n         ('model', RandomForestClassifier(n_estimators=250, n_jobs=-1, random_state=0))]\n\npipe = Pipeline(steps)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)\npipe.fit(X_train, y_train)","308459a6":"plt.figure(figsize=(15,10))\n\nplt.subplot(221)\nplot_roc(pipe, X_test ,y_test)\n\nplt.subplot(222)\nplot_confusion_matrix(pipe, X_test ,y_test, normalize=True)\n\nplt.subplot(223)\nplot_proba(pipe, X_test, y_test)\n\nplt.subplot(224)\nfeature_importance(pipe.named_steps['model'].feature_importances_, X.columns)\n\nplt.tight_layout()","81f9891a":"rfc_xval_res = cross_validate_model(pipe, X, y, verbose=False)\nrfc_xval_res.T[['value','error']].style.format(\"{:.2f}\")","197e2a09":"from sklearn.ensemble import ExtraTreesClassifier\n\nsteps = [('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)),\n         ('model', ExtraTreesClassifier(n_estimators=250, n_jobs=-1, random_state=0, class_weight='balanced'))]\n\npipe = Pipeline(steps)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)\npipe.fit(X_train, y_train)","5a5a4fa1":"plt.figure(figsize=(15,10))\n\nplt.subplot(221)\nplot_roc(pipe, X_test ,y_test)\n\nplt.subplot(222)\nplot_confusion_matrix(pipe, X_test ,y_test, normalize=True)\n\nplt.subplot(223)\nplot_proba(pipe, X_test, y_test)\n\nplt.subplot(224)\nfeature_importance(pipe.named_steps['model'].feature_importances_, X.columns)\n\nplt.tight_layout()","507b7cd1":"ert_xval_res = cross_validate_model(pipe, X, y, verbose=False)\nert_xval_res.T[['value','error']].style.format(\"{:.2f}\")","0ac17072":"from collections import OrderedDict\n\nres_comp = OrderedDict([\n    ('Logistic regression'              , logit_xval_res[1:]),\n    ('SGD classifier'                   , sgd_xval_res[1:]  ),\n    ('Supporting vector classifier'     , svc_xval_res[1:]  ),\n    ('Random forest classifier'         , rfc_xval_res[1:]  ),\n    ('Extermely random tree classifier' , ert_xval_res[1:]  ),\n    ('Gradient boost classifier'        , gbc_xval_res[1:]  ),\n])\n\nnew_columns = {'level_0' : 'Model'}\n\npd.concat(res_comp).reset_index().drop('level_1', axis=1).rename(columns=new_columns).set_index('Model').sort_values('F1', ascending=False).style.format(\"{:.2f}\")","132fcd40":"## Supporting Vector Classifier\n<a id='svc'><\/a>\nA support vector machine constructs a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier.\n\nThe advantages of support vector machines are:\n* Effective in high dimensional spaces.\n* Still effective in cases where number of dimensions is greater than the number of samples.\n* It uses a subset of the training points in the decision function so it is memory efficient.","b9692d02":"<a id='conclusion'><\/a>\n## Model comparison and conclusions\nThe table below summarizes the performance of the classification models that I considered in this study. Performances are ordered by increasing value of $F_{1}$. The best performances are obtained by the **extremely randomized tree**, followed by the **random forest** and the **logistic regression**. \n\nThe extremely randomized tree allow to identify up to 66% of loans which would cause a DEFAULT while retaining 91% of loans which would be PAID in time. The ROC AUC value is as high as 96%, indicating that the probabilty that the classifier would perform better by random choice is as low as 4%.","35fec3b6":"<a id='sgd'><\/a>\n## SGD Classifier\nThis algorithms implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online\/out-of-core) learning.","61a9ed59":"# Credit risk case study\n*DISCLAIMER: this is my own reference for classification problems. The text documenting the notebook comes from different sources (kaggle data set description, sklearn documentation, matplotlib documentation, wikipedia, etc.).*\n\n## Table of content\n\n* [Dataset overview](#ds)\n* [Exploratory analysis](#explo)\n    * [Descritive statistics for PAID loans](#descp)\n    * [Descritive statistics for DEFAULT loans](#descd)\n    * [DEFAULT as a function of reason for aquiring the loans](#reason)\n    * [DEFAULT as a function of occupation](#occupation)\n    * [Graphical overview](#graph)\n    * [Violin plot](#violin)\n    * [Correlation matrix](#corr)\n* [Test of default classifiers](#classification)\n* [Model evaluation](#eval)\n    * [Precision & recall](#per)\n    * [F1](#f1)\n    * [Receiver operating characteristic](#roc)\n    * [Confusion matrix](#confusion)\n    * [Classification probability](#prob)\n* [Logistic regression](#logit)\n* [SGD classifier](#sgd)\n* [Supporting vector classifier](#svc)\n* [Gradient boosting classifier](#gbrt)\n* [Forest of randomized tree](#frt)\n    * [Randm forest classifier](#rfc)\n    * [Extremely randomized tree](#ert)\n* [Model comparison and conclusion](#conclusion)\n\n## Dataset overview\n<a id='ds'><\/a>\n\nThe dataset contains baseline and loan performance information for 5,960 recent home equity loans. A home equity loan is a loan where the obligor uses the equity of his or her home as the underlying collateral. The target (BAD) is a binary variable indicating whether an applicant eventually defaulted or was seriously delinquent. This adverse outcome occurred in 1,189 cases (20%). \n\nFor each applicant, 11 input variables were recorded:\n\n* BAD: 1 = applicant defaulted on loan or seriously delinquent; 0 = applicant paid loan\n* LOAN: Amount of the loan request\n* MORTDUE: Amount due on existing mortgage\n* VALUE: Value of current property\n* REASON: DebtCon = debt consolidation; HomeImp = home improvement\n* JOB: Occupational categories\n* YOJ: Years at present job\n* DEROG: Number of major derogatory reports\n* DELINQ: Number of delinquent credit lines\n* CLAGE: Age of oldest credit line in months\n* NINQ: Number of recent credit inquiries\n* CLNO: Number of credit lines","91ef028f":"## Exploratory analysis\n<a id='explo'><\/a>\n\nI summarize the main characteristics of the dataset with visual methods and summary statistics. I use the target variable (BAD) to divide the data set into sub-samples and I specifically look for variables, features and correlation which contain classification power.\n\n### Descritive statistics for PAID loans\n<a id='descp'><\/a>","fbd9b20d":"### Descritive statistics for DEFAULT loans\n<a id='descd'><\/a>","feed2a96":"###  DEFAULT as a function of the occupation\n<a id='occupation'><\/a>\nThe fraction of PAID and DEFAULT loans show some dependence on the occupation of the contractor. Office worker and professional executives have the highest probability to pay their loans while sales and self employed have the highest probability to default. The occupation shows a good discriminating power and it will  most likely be an important feature of our classification model.","d2794de4":"### Correlation matrix\n<a id='corr'><\/a>\nFinally I show the correlation matrix among the variables discussed so far. Correlations are useful because they can indicate a predictive relationship that can be exploited in the classification task. \n\nThe plot is color coded: colder colors correspond to low correlation while warmer color correspond to high correlation. The variables are also grouped according to their correlation, i.e. variables with higher correlation are close to each other.\n\nVariables related to the credit history (DELINQ, DEROG, NINQ) are the most correlated with the loan status (BAD), suggesting that these will be the most discriminating variables. These variables are also slightly correlated among them, suggesting that some of the information might be redoundant.\n\nAs already discussed, the amount of the loan or the underlying collateral do not seem related to the loan status. They anyhow form another correlation cluster with other variables such as the age of oldest credit line (CLAGE) and the number of credit lines (CLNO). This is expected since those variables are clearly related.","818d98b3":"### Graphical overview\n<a id='graph'><\/a>\nA coherent graphical overview of the dataset is shown below. For each variable I show an histogram for the whole dataset, for the PAID, and DEFUALT loans, respectively. The correlations among variables are also sumamrized in 2-dimensinal scatter plots.","9a47bf64":"<a id='gbrt'><\/a>\n### Gradient Boosting Classifier\nGradient Tree Boosting or Gradient Boosted Regression Trees (GBRT) is a generalization of boosting to arbitrary differentiable loss functions. GBRT produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function. GBRT is an accurate and effective off-the-shelf procedure that can be used for both regression and classification problems.","bfb28879":"1. From the descriptive statistics above I can draw the following consideration:\n\n* The amount of requested loan, the amount of due mortgage and the value of the underlying collateral are statistically consistent for both loans that been PAID and that resulted in a DEFAULT. This suggests that those variables may not provide significant discrimination power to separate the two classes.\n\n\n* The number of years at the present job (YOJ) seems to discriminate the two classes as DEFAULTs seem more frequent in contractors which have a shorter seniority. This tendency is supported by the correspoding average value quantiles which indicate a distribution skewed toward shorter seniority.\n\n* A similar considerations apply to variables related to the contractor credit history such as: the number of major derogatory reports (DEROG), the number of delinquent credit lines (DELINQ), the age of oldest credit line in months (CLAGE), and the number of recent credit inquiries (NINQ). In the case of DEFAULT the distribution of these variables is skewed toward values that suggest a credit hystory that is worse than the corresponding distribution for PAID loan contractors.\n\n\n* Finally, the number of open credit line (CLNO) seems statistically consistent in both case, suggesting that this variable has no significant discrimination power.","b61358fd":"## Logistic regression\n<a id='logit'><\/a>\n\nLogistic regression is the simplest linear model for classification. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function. The optimization problem is solved minimizing a cost function using an highly optimized coordinate descent algorithm.","ca053a67":"### Violin plot\n<a id='violin'><\/a>\nViolin plot shows the different shapes of the probability density function for some of the variables discussed previously that seem the most promising for the classification task. The plot shows, in different colors, the PAID and the DEFAULT loans. The horizontal dashed lines indecate the position of the mean and the quantiles of the different distributions. Since there is a dependency of the DEFAULT probability on the occupation categories, the \"violins\" are shown for each of them.","e00dc346":"### DEFAULT as a function of the reason for aquiring the loans\n<a id='reason'><\/a>\nThe fraction of PAID and DEFAULT loans do not seem to depend strongly on the reason for acquiring the loan. On average, 80% of the loans have been payed while about the 20% DEFAULT. The 2% discrepancy observed is not statistically significant given the amount of loans in the dataset.","5ea8ca2e":"<a id='frt'><\/a>\n### Forests of randomized trees\nDecision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n\nThe forest of randomized tree technique includes two averaging algorithms based on randomized decision trees: the RandomForest algorithm and the Extra-Trees method. Both algorithms are perturb-and-combine techniques specifically designed for trees. This means a diverse set of classifiers is created by introducing randomness in the classifier construction. The prediction of the ensemble is given as the averaged prediction of the individual classifiers.\n\n<a id='rfc'><\/a>\n#### Random Forest Classifier\nIn random forests, each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set. In addition, when splitting a node during the construction of the tree, the split that is chosen is no longer the best split among all features. Instead, the split that is picked is the best split among a random subset of the features. As a result of this randomness, the bias of the forest usually slightly increases (with respect to the bias of a single non-random tree) but, due to averaging, its variance also decreases, usually more than compensating for the increase in bias, hence yielding an overall better model.","0d9b4cca":"<a id='classification'><\/a>\n# Test of default classifiers\nThe exploratory analysis described above provides good insights on the dataset and higlights the most promising variables with good discrimination power to identify loans resulting in DEFAULT. In this section I develop and investigate supervided machine learning classifiers to predict the outcome of loans. Given the large amount of algorithms available in literature, I begin from the simple methods, such as logistc regression, and gradually increase the model complexity up to randomized trees techniques. Finally I compare the performance of each model and discuss the most appropriate for this loan classification task. In this section, the following models are developed:\n* [Logistic regression](#logit)\n* [SGD classifier](#sgd)\n* [Supporting vector classifier](#svc)\n* [Gradient boosting classifier](#gbrt)\n* [Forest of randomized tree](#frt)\n    * [Randm forest classifier](#rfc)\n    * [Extremely randomized tree](#ert)\n* [Model comparison and conclusion](#conclusion)\n\n<a id='eval'><\/a>\n## Model Evaluation\nThe evaluation of classifiers performance is relatively complex and depenends on many factors, some of which are model dependent. In order to indetify the best model for our classification task, I adopt different evaluation metrics that are briefly summarized in the following.\n\nTo avoid overtraining, the performance of our classification model are evaluated using cross-validation. The training set is randomly splited in $N$ distinct subsets called folds, then the model is trained and evaluated $N$ times by using a different fold for the evaluation of a model that is trained on the other $N-1$ folds. The results of the procedure consist in $N$ evaluation scores for each metric that are then averaged. These averages are fianlly used to compare the different techniques considered in this study.\n\n<a id='per'><\/a>\n### Precision & recall\nPrecision-Recall is a useful performance metric to evaluate a models in those cases when the classes are very imbalanced. In information retrieval, precision is a measure of result relevancy, while recall is a measure of how many truly relevant results are returned. Intuitively, precision is the ability of the classifier not to label as positive a sample that is negative, and recall is the ability of the classifier to find all the positive samples. \n\nA system with high recall but low precision returns many labels that tend to be predicted incorrectly when compared to the training labels. A system with high precision but low recall is just the opposite, returning very few results, but most of its predicted labels are correct when compared to the training labels. An ideal system with high precision and high recall will return many results, with many results labeled correctly.\n\nPrecision ($P$) is defined as the number of true positives ($T_{p}$) over the number of true positives plus the number of false positives ($T_{p}+F_{p}$):\n\n$P = \\frac{T_{p}}{T_{p}+F_{p}}$  \n\nRecall ($R$) is defined as the number of true positives ($T_{p}$) over the number of true positives plus the number of false negatives ($T_{p}+F_{n}$):\n\n$R = \\frac{T_{p}}{T_{p}+F_{n}}$\n\n<a id='f1'><\/a>\n### F1 measure\nIt is often convenient to combine precision and recall into a single metric called the $F_{1}$ score, defined as a weighted harmonic mean of the precision and recall:\n\n$F_{1} = 2\\times \\frac{P \\times R}{P+R}$\n\nWhereas the regular mean treats all values equally, the harmonic mean gives much more weight to low values. As a result, the classifier will only get a high F1 score if both recall and precision are high.\nThe $F_{1}$ score favors classifiers that have similar precision and recall. This is not always what you want: in some contexts you mostly care about precision, and in other contexts you really care about recall.\n\n<a id='roc'><\/a>\n###  Receiver operating characteristic\nA receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied. It is created by plotting the fraction of true positives out of the positives (TPR = true positive rate) vs. the fraction of false positives out of the negatives (FPR = false positive rate), at various threshold settings. TPR is also known as sensitivity, and FPR is one minus the specificity or true negative rate.\nThere is a tradeoff: the higher the recall (TPR), the more false positives (FPR) the classifier produces. The dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner). \n\nThe area under the ROC curve, which is also denoted by AUC, summarise the curve information in one number. The AUC should be interpreted as the probability that a classifier will rank a randomly chosen positive istance higher than a randomly chosen negative one. A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5.\n\n<a id='confusion'><\/a>\n### Confusion matrix\nThe confusion matrix evaluates classification accuracy by computing the confusion matrix with each row corresponding to the true class. By definition, entry $i,j$ in a confusion matrix is the number of observations actually in group $i$, but predicted to be in group $j$. The confusion matrix is not used for model evaluation but it provide a good grasp on the overall model performance.\n\n<a id='prob'><\/a>\n### Classification probability\nThe classification probability provides an estimation of the probability that a given instance of the data belongs to the given class. In a binary classification problem like the one being considered, the histogram of the classification probability for the two class provide a good visual grasp on the model performance. The more the peak of the classification probability are far from each other, the higher the separation power of the model.","631115c3":"<a id='ert'><\/a>\n#### Extremely Randomized Trees\nIn extremely randomized trees, randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias"}}