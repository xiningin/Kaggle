{"cell_type":{"127eb710":"code","fa515442":"code","d54cc887":"code","47991310":"code","27d7e34f":"code","faac1164":"code","b5f966cf":"code","7fb07b6d":"code","3032c8e1":"code","67f30ac2":"code","3ded434b":"code","f8565813":"code","08912a8c":"code","a051942d":"code","36c31385":"code","fda24510":"markdown","48437337":"markdown","1ce05195":"markdown","109344d9":"markdown","dc8e1dca":"markdown","c595857b":"markdown","58a1d3df":"markdown","f027afe3":"markdown","ac12d83c":"markdown"},"source":{"127eb710":"# Data analysis tools\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import pearsonr\nimport random as rnd\n\n# Data visualization tools\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n# Import data\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\n# Display data information\ntrain.info()\nprint('_' * 50)\ntest.info()","fa515442":"train.describe()","d54cc887":"train.describe(include = ['O'])","47991310":"train[[\"Pclass\", \"Survived\"]].groupby([\"Pclass\"], as_index = False).mean().sort_values(by = \"Survived\", ascending = False)","27d7e34f":"train[[\"Sex\", \"Survived\"]].groupby([\"Sex\"], as_index = False).mean().sort_values(by = \"Survived\", ascending = False)","faac1164":"train[[\"SibSp\", \"Survived\"]].groupby([\"SibSp\"], as_index = False).mean().sort_values(by = \"Survived\", ascending = False)","b5f966cf":"train[[\"Fare\", \"Survived\"]].groupby([\"Fare\"], as_index = False).mean().sort_values(by = \"Survived\", ascending = False)","7fb07b6d":"train[[\"Parch\", \"Survived\"]].groupby([\"Parch\"], as_index = False).mean().sort_values(by = \"Survived\", ascending = False)","3032c8e1":"# Dropping features\ntrain = train.drop([\"Ticket\", \"Cabin\", \"PassengerId\", \"Name\", \"Parch\", \"Fare\"], axis = 1)\ntest = test.drop([\"Ticket\", \"Cabin\", \"Name\", \"Parch\", \"Fare\"], axis = 1)\n\n# Dropping missing values\ntrain = train.dropna()\ntest = test.dropna()\nfull_data = [train, test]\n\n# Converting categorical features to numeric\nfor dataset in full_data:\n    dataset[\"Sex\"] = dataset[\"Sex\"].map( {\"female\": 1, \"male\": 0}).astype(int)\n\nfor dataset in full_data:\n    dataset[\"Embarked\"] = dataset[\"Embarked\"].map( {\"S\": 0, \"C\": 1, \"Q\": 2}).astype(int)","67f30ac2":"# machine learning models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Formatting train and test sets\nX_train = train.drop(\"Survived\", axis = 1)\ny_train = train[\"Survived\"]\nX_test = test.drop(\"PassengerId\", axis = 1).copy()","3ded434b":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nprediction = logreg.predict(X_test)\n\nprint(\"Logistic Regression Score:\", round(logreg.score(X_train, y_train) * 100, 2))","f8565813":"# KNN\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\nprediction = knn.predict(X_test)\n\nprint(\"K-Nearest Neighbors Score:\", round(knn.score(X_train, y_train) * 100, 2))","08912a8c":"# Decision Tree\ntree = DecisionTreeClassifier()\ntree.fit(X_train, y_train)\nprediction = tree.predict(X_test)\n\nprint(\"Decision Tree Score:\", round(tree.score(X_train, y_train) * 100, 2))","a051942d":"# Random Forest\nforest = RandomForestClassifier(n_estimators = 100)\nforest.fit(X_train, y_train)\nforest_prediction = forest.predict(X_test)\n\nprint(\"Random Forest Score:\", round(forest.score(X_train, y_train) * 100, 2))","36c31385":"# Submitting output\nsubmission = pd.DataFrame({\"PassengerId\": test[\"PassengerId\"], \"Survived\": forest_prediction})\n\nsubmission.to_csv('submission.csv', index=False)","fda24510":"# Data Inspection\n\nIn this section we will look carefully at the data and derive various statistical measures for each feature. The aim is to understand the data and try to select the features that have the strongest correlation with survival.\n\nFrom the data description on kaggle we know that each row represents a single passenger and the following about the features:\n\n- Survival is whether or not the person survived (0 = NO, 1 = YES).\n- Pclass is the class of the persons ticket (1 = 1st, 2 = 2nd, 3 = 3rd).\n- Sex is the sex of the person.\n- Age is the age in years of the person.\n- Sibsp is the number of siblings or spouses onboard with the person.\n- Parch is the number of parents or children onboard with the person.\n- Ticket is the ticket number of the person.\n- Fare is the price paid by the person.\n- Cabin is the cabin number in which the person stayed.\n- Embarked is the port where they boarded the titanic (C = Cherbourg, Q = Queenstown, S = Southhampton).","48437337":"From the above comaprisons we can see that:\n\n- Pclass correlates strongly with Survived.\n- Sex correlates strongly with Survived.\n- SibSp somewhat correlates with Survived.\n- Fare does not seem to correlate with survived (so we remove it from the feature selection).\n- Parch does not seem to correlate with Survived (so we remove it from the feature selection).\n\nTherefore, we will select the following 5 features for modelling:\n\n- Age\n- Pclass\n- Sex\n- SibSp\n- Embarked","1ce05195":"From the descriptive statistics generated on the numerical features for the train dataset we can see that:\n\n- 38.38% of passengers survived (reperesentative of the actual survival rate of 32%).\n- The average age of passengers was about 30 years old.\n- 75% of passengers onboard were under 38 years of age.\n- Most passengers did not travel with parents or children.\n- Most passengers (75%) payed less than 31 in fares but a small minority payed as much as 512.\n- Most passengers did not travel with siblings more than 1 sibling of spouse.","109344d9":"# Data Wrangling\n\nWe need to drop the features that we won't be using in our models and convert the remaining features to numerical types (this is because most models require that the data be in numerical format).","dc8e1dca":"From the descriptive statistics generated on the categorical features for the train dataset we can see that:\n\n- All names were unique.\n- The passengers were 65% male.\n- Some ticket numbers were repeated (most likely due to families purchasing one ticket for multiple people).\n- Some cabin numbers were repeated (for similiar reasons as ticket numbers).\n- The port of Southampton (S) was most frequently embarked from (72.44%).\n\n## Feature Selection\n\nFrom the above analysis we decide to remove:\n\n- Name as it is unique for each row.\n- Ticket as we cannot be sure that the repeated entries are not an error.\n- Cabin for the same reason as above and also due to a large number of missing values.\n- Passengerid as it is unique for each row.","c595857b":"# Problem Definition\n\nGiven a labelled dataset of features about passengers aboard the titanic and whether or not they survived. Can we train a classification model to accurately predict the survival of passengers from an unlabelled dataset.\n\n## Methodology\n\n1. We will firstly inspect the data and determine which features are important to include in our models.\n2. We will then engineer the selected features such that they are in a form convenient for modelling.\n3. We will then fit and evaluate a number of models.\n4. Finally, we will compare the accuracy of the various models and select the best performing model for submission.","58a1d3df":"From the above printouts we can see that for the test dataset there are:\n\n- 6 numerical and 5 string type features.\n- Age, Cabin and Embarked contain missing values (19.87%, 77.1% and 0.22% respectively).","f027afe3":"# Results\n\nThe decision tree and random forest both have the highest score so we choose the random forest as it tends to generalize better than a single tree.","ac12d83c":"# Modelling\n\nTo determine an accurate model we will evaluate the following classfification models:\n\n- Logistic Regression\n- K-Nearest Neighbors\n- Decision Tree Classification\n- Random Forest Classification"}}