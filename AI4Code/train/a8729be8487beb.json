{"cell_type":{"8250348a":"code","4a70c3af":"code","971ec895":"code","e7faae32":"code","7fa5140c":"code","aae96c2f":"code","b8a7371e":"code","a747693b":"code","85c5342c":"code","dbad60ae":"code","cc1ef014":"code","879ce367":"code","d9b8c7b6":"code","ff780d67":"code","be0d9b42":"code","0a1f6887":"code","8b22ddbc":"code","96a9b974":"code","85e37e55":"markdown","289d3408":"markdown","dfc74515":"markdown","5506e88b":"markdown"},"source":{"8250348a":"import torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","4a70c3af":"import torch\nimport torch.nn as nn\n\n\nclass BNN(nn.Module):\n    def __init__(self, *layers):\n        super(BNN, self).__init__()\n\n        self.layers, self.params = [], nn.ParameterList()\n        for layer in layers:\n            self.layers.append(layer)\n            self.params.extend([*layer.parameters()])   # register module parameters\n\n    def forward(self, x, mode):\n        if mode == 'forward':\n            net_kl = 0\n            for layer in self.layers:\n                x, layer_kl = layer.forward(x, mode)\n                net_kl += layer_kl\n            return x, net_kl\n        else:\n            for layer in self.layers:\n                x = layer.forward(x, mode)\n            return x\n\n    def Forward(self, x, y, n_samples, type):\n\n        assert type in {'Gaussian', 'Softmax'}, 'Likelihood type not found'\n\n        # Sample N samples and average\n        total_kl, total_likelh = 0., 0.\n        for _ in range(n_samples):\n            out, kl = self.forward(x, mode='forward')\n\n            # Gaussian output (with unit var)\n            # lklh = torch.log(torch.exp(-(y - out) ** 2 \/ 2e-2) \/ math.sqrt(2e-2 * math.pi)).sum()\n\n            if type == 'Gaussian':\n                lklh = (-.5 * (y - out) ** 2).sum()\n            else:   # softmax\n                lklh = torch.log(out.gather(1, y)).sum()\n\n            total_kl += kl\n            total_likelh += lklh\n\n        return total_kl \/ n_samples, total_likelh \/ n_samples\n\n    @staticmethod\n    def loss_fn(kl, lklh, n_batch):\n        return (kl \/ n_batch - lklh).mean()","971ec895":"import numpy as np\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\nclass BNNLayer(nn.Module):\n    NegHalfLog2PI = -0.5 * math.log(2.0 * math.pi)\n    softplus = lambda x: math.log(1 + math.exp(x))\n\n    def __init__(self, n_input, n_output, activation, prior_mean, prior_rho):\n        assert activation in {\"relu\", \"softmax\", \"none\"}, \"Activation Type Not Found\"\n\n        super(BNNLayer, self).__init__()\n\n        # Instantiate a large Gaussian block to sample from, much faster than generating random sample every time\n        self._gaussian_block = np.random.randn(10000)\n\n        self.n_input = n_input\n        self.n_output = n_output\n\n        self.W_mean = nn.Parameter(torch.ones((n_input, n_output)).to(device) * prior_mean)\n        self.W_rho = nn.Parameter(torch.ones(n_input, n_output) * prior_rho)\n\n        self.b_mean = nn.Parameter(torch.ones(1, n_output) * prior_mean)\n        self.b_rho = nn.Parameter(torch.ones(1, n_output) * prior_rho)\n\n        self.prior_var = Variable(torch.ones(1, 1) * BNNLayer.softplus(prior_rho) ** 2)\n\n        # Set activation function\n        self.act = None\n        if activation == \"relu\":\n            self.act = F.relu\n        elif activation == \"softmax\":\n            self.act = F.softmax\n\n        self._Var = lambda x: Variable(torch.from_numpy(x).type(torch.FloatTensor))\n\n    def forward(self, X, mode):\n        assert mode in {\"forward\", \"MAP\", \"MC\"}, \"BNNLayer Mode Not Found\"\n\n        _shape = (X.size()[0], self.n_output)\n\n        # Z: pre-activation. Local reparam. trick is used.\n        Z_Mean = torch.mm(X, self.W_mean) + self.b_mean.expand(*_shape)\n\n        if mode == \"MAP\":\n            return self.act(Z_Mean) if self.act is not None else Z_Mean\n\n        Z_Std = torch.sqrt(\n            torch.mm(torch.pow(X, 2), torch.pow(F.softplus(self.W_rho), 2))\n            + torch.pow(F.softplus(self.b_rho.expand(*_shape)), 2)\n        )\n\n        Z_noise = self._random(_shape).to(device)\n        Z = Z_Mean + Z_Std * Z_noise\n\n        if mode == \"MC\":\n            return self.act(Z) if self.act is not None else Z\n\n        # Stddev for the prior\n        Prior_Z_Std = torch.sqrt(\n            torch.mm(\n                torch.pow(X, 2).to(device),\n                self.prior_var.expand(self.n_input, self.n_output).to(device),\n            )\n            + self.prior_var.expand(*_shape).to(device)\n        ).detach()\n\n        # KL[posterior(w|D)||prior(w)]\n        layer_KL = self.sample_KL(Z, Z_Mean, Z_Std, Z_Mean.detach(), Prior_Z_Std)\n\n        out = self.act(Z) if self.act is not None else Z\n        return out, layer_KL\n\n    def _random(self, shape):\n        Z_noise = np.random.choice(self._gaussian_block, size=shape[0] * shape[1])\n        Z_noise = np.expand_dims(Z_noise, axis=1).reshape(*shape)\n        return self._Var(Z_noise)\n\n    @staticmethod\n    def log_gaussian(x, mean, std):\n        return (\n            BNNLayer.NegHalfLog2PI\n            - torch.log(std)\n            - 0.5 * torch.pow(x - mean, 2) \/ torch.pow(std, 2)\n        )\n\n    @staticmethod\n    def sample_KL(x, mean1, std1, mean2, std2):\n        log_prob1 = BNNLayer.log_gaussian(x, mean1, std1)\n        log_prob2 = BNNLayer.log_gaussian(x, mean2, std2)\n        return (log_prob1 - log_prob2).sum()","e7faae32":"def train_bnn(X, Y, epochs=400):\n\n    # Initialize network\n    bnn = BNN(BNNLayer(1, 100, activation='relu', prior_mean=0, prior_rho=0).to(device),\n              BNNLayer(100, 1, activation='none', prior_mean=0, prior_rho=0).to(device)).to(device)\n\n    optim = torch.optim.Adam(bnn.parameters(), lr=1e-1)\n\n    # Main training loop\n    for i_ep in range(epochs):\n        kl, lg_lklh = bnn.Forward(X.to(device), Y.to(device), 1, 'Gaussian')\n        loss = BNN.loss_fn(kl, lg_lklh, 1).to(device)\n        optim.zero_grad()\n        loss.backward()\n        optim.step()\n        \n    return bnn","7fa5140c":"import matplotlib.pyplot as plt\nimport numpy as np\n\nimport torch\nfrom torch.autograd import Variable\n\n\nN = 200 ##  number of points\n\n# train data\nx = np.random.uniform(-2, 2, size=N).reshape((-1, 1))\nnoise = np.random.normal(0, 3, size=N).reshape((-1, 1))\ny = x ** 3 + noise\n\nVar = lambda x, dtype=torch.FloatTensor: Variable(torch.from_numpy(x).type(dtype))\nX = Var(x)\nY = Var(y)\n\n# test data\nx_test = np.linspace(-3, 3)\ny_test = x_test ** 3\nX_test = Var(x_test).unsqueeze(1)\n\n# Train\nbnn = train_bnn(X, Y, epochs=400)\nprint(\"bnn:\\n\", bnn)\n\n# Predict\npred_lst = [bnn.forward(X_test.to(device), mode=\"MC\").cpu().data.numpy().squeeze(1) for _ in range(100)]\npred = np.array(pred_lst).T\npred_mean = pred.mean(axis=1)\npred_std = pred.std(axis=1)\n\n# Plotting\nplt.figure(figsize=(12, 6))\nplt.plot(x, y, ls=\"none\", marker=\"o\", color=\"0.1\", alpha=0.8, label=\"data points\")\nplt.plot(x_test, y_test, ls=\"--\", color=\"r\", label=\"ground truth $y=x^3$\")\nplt.plot(x_test, pred_mean, ls=\"-\", color=\"b\", label=\"mean pred\")\nfor i in range(2):\n    plt.fill_between(\n        x_test,\n        pred_mean - (i+1) * pred_std,\n        pred_mean + (i+1) * pred_std,\n        color=\"b\",\n        alpha=0.1,\n        label=f\"epistemic uncertainty {i+1}$\\sigma$\",\n    )\nplt.legend()\nplt.tight_layout()\nplt.grid()\nplt.show()","aae96c2f":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Normal\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","b8a7371e":"## \u56de\u5e30\u306e\u4f8b\n#def data_set(points=20, xrange=(-4, 4), std=3.):\n#    \"\"\"y=x^3\u306e\u30c7\u30fc\u30bf\u8003\u3048\u308b\"\"\"\n#    xx = torch.tensor([[np.random.uniform(*xrange)] for i in range(points)])\n#    yy = torch.tensor([[x**3 + np.random.normal(0, std)] for x in xx])\n#    return xx, yy\n#\n#x, y = data_set(points=200, xrange=(-2, 2), std=3.) # generate data set of 200 samples\n#x_gt = np.linspace(-3, 3, 100).reshape(100, 1)\n#y_gt = x_gt**3\n#\n#print(x.shape, y.shape)\n#print(x[:5])\n#print(y[:5])\n#\n#plt.figure(figsize=(12,6))\n#plt.plot(x_gt, y_gt, ls=\"--\", color=\"r\", label='ground truth: $y=x^3$')\n#plt.plot(\n#    x.numpy(),\n#    y.numpy(),\n#    \"or\",\n#    marker=\"o\",\n#    color=\"0.1\",\n#    alpha=0.8,\n#    label=\"data points\",\n#)\n#plt.grid()\n#plt.xlabel('x')\n#plt.ylabel('y')\n#plt.legend()\n#plt.show()","a747693b":"# \u56de\u5e30\u306e\u4f8b\ndef toy_function(x):\n    return -x**4 + 3*x**2 -5*np.sin(x) + 1\n    #return x**3\n\n# toy dataset we can start with\nx = torch.tensor([-2, -1.8, -1, 1, 1.8, 2]).reshape(-1,1)\ny = toy_function(x)\nx_gt = np.linspace(-3, 3, 100).reshape(100, 1)\ny_gt = [toy_function(xi) for xi in x_gt]\n\nplt.figure(figsize=(12,6))\nplt.plot(x_gt, y_gt, ls=\"--\", color=\"r\", label='ground truth: $y=-x^4+3x^2-5sin(x)+1$')\nplt.plot(\n    x.numpy(),\n    y.numpy(),\n    \"or\",\n    marker=\"o\",\n    color=\"0.1\",\n    alpha=0.8,\n    label=\"data points\",\n)\nplt.grid()\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()","85c5342c":"class standard_MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = nn.Linear(1,16)\n        self.l2 = nn.Linear(16,16)\n        self.l3 = nn.Linear(16,1)\n    def forward(self, x):\n        x = self.l3(F.sigmoid(self.l2(F.sigmoid(self.l1(x)))))\n        return x","dbad60ae":"# initialization of our standard neural network\nnet1 = standard_MLP().to(device)\n\n# use of a Mean Square Error loss to evaluate the network because we are in a regression problem\ncriterion = nn.MSELoss()\n\n# use of stochastic gradient descent as our optimizer\noptimizer = optim.SGD(net1.parameters(), lr=0.01)\n\n# number of times we are looping over our data to train the network\nepochs = 30000\n\nfor epoch in range(epochs):\n    optimizer.zero_grad()   # zero the gradient buffers\n    output = net1(x.to(device))    # pass the data forward\n    loss = criterion(output, y.to(device))   # evaluate our performance\n    if epoch % 5000 == 0:\n        print(\"epoch {} loss: {}\".format(epoch,loss))\n    loss.backward()  # calculates the gradients\n    optimizer.step()    # updates weigths of the network","cc1ef014":"#x_test = torch.linspace(-2,2,100).reshape(-1,1)\nx_test = torch.Tensor(x_gt)\n\npredictions = net1(x_test.to(device))\n\nplt.figure(figsize=(12,6))\nplt.plot(x_gt, y_gt, ls=\"--\", color=\"r\", label='ground truth')\nplt.plot(x_test.numpy(), predictions.cpu().detach().numpy(), label = 'MLP predictions')\nplt.scatter(x, y, label = 'data points', marker=\"o\", color=\"0.1\", alpha=0.8)\nplt.title('Standard neural networks can overfit...')\nplt.legend()\nplt.grid(True, linestyle='--', linewidth=0.5)\nplt.show()","879ce367":"x_test = (\n    torch.linspace(-100, 100, 100).reshape(-1, 1)\n)  # steps\u5024\u304c\u304b\u3089start\u307e\u3067\u7b49\u9593\u9694\u306b\u914d\u7f6e\u3055\u308c\u305f\u30b5\u30a4\u30ba\u306e1\u6b21\u5143\u30c6\u30f3\u30bd\u30eb\u3092\u4f5c\u6210\n\npredictions = net1(x_test.to(device))\n\nplt.figure(figsize=(12,6))\nplt.plot(x_gt, y_gt, ls=\"--\", color=\"r\", label='ground truth')\nplt.plot(x_test.numpy(), predictions.cpu().detach().numpy(), label = 'MLP predictions')\nplt.scatter(x, y, label = 'data points', marker=\"o\", color=\"0.1\", alpha=0.8)\nplt.title('Standard neural networks can overfit...')\nplt.legend()\nplt.grid(True, linestyle='--', linewidth=0.5)\nplt.show()","d9b8c7b6":"class Linear_BBB(nn.Module):\n    \"\"\"\n        Layer of our BNN.\n    \"\"\"\n    def __init__(self, input_features, output_features, prior_var=1.):\n        \"\"\"\n            Initialization of our layer : our prior is a normal distribution\n            centered in 0 and of variance 20.\n        \"\"\"\n        # initialize layers\n        super().__init__()\n        # set input and output dimensions\n        self.input_features = input_features\n        self.output_features = output_features\n\n        # initialize mu and rho parameters for the weights of the layer\n        self.w_mu = nn.Parameter(torch.zeros(output_features, input_features))\n        self.w_rho = nn.Parameter(torch.zeros(output_features, input_features))\n\n        #initialize mu and rho parameters for the layer's bias\n        self.b_mu =  nn.Parameter(torch.zeros(output_features))\n        self.b_rho = nn.Parameter(torch.zeros(output_features))        \n\n        #initialize weight samples (these will be calculated whenever the layer makes a prediction)\n        self.w = None\n        self.b = None\n\n        # initialize prior distribution for all of the weights and biases\n        self.prior = torch.distributions.Normal(0,prior_var)\n        \n    def forward(self, x):\n        \"\"\"\n          Optimization process\n        \"\"\"\n        # sample weights\n        w_epsilon = Normal(0,1).sample(self.w_mu.shape).to(device)\n        self.w = self.w_mu + torch.log(1+torch.exp(self.w_rho)) * w_epsilon\n\n        # sample bias\n        b_epsilon = Normal(0,1).sample(self.b_mu.shape).to(device)\n        self.b = self.b_mu + torch.log(1+torch.exp(self.b_rho)) * b_epsilon\n\n        # record log prior by evaluating log pdf of prior at sampled weight and bias\n        w_log_prior = self.prior.log_prob(self.w)\n        b_log_prior = self.prior.log_prob(self.b)\n        self.log_prior = torch.sum(w_log_prior) + torch.sum(b_log_prior)\n\n        # record log variational posterior by evaluating log pdf of normal distribution defined by parameters with respect at the sampled values\n        self.w_post = Normal(self.w_mu.data, torch.log(1+torch.exp(self.w_rho)))\n        self.b_post = Normal(self.b_mu.data, torch.log(1+torch.exp(self.b_rho)))\n        self.log_post = self.w_post.log_prob(self.w).sum() + self.b_post.log_prob(self.b).sum()\n        \n        return F.linear(x, self.w, self.b)","ff780d67":"class MLP_BBB(nn.Module):\n    def __init__(self, hidden_units, noise_tol=.1,  prior_var=1.):\n\n        # initialize the network like you would with a standard multilayer perceptron, but using the BBB layer\n        super().__init__()\n        self.hidden1 = Linear_BBB(1,hidden_units, prior_var=prior_var)\n        self.hidden2 = Linear_BBB(hidden_units,hidden_units, prior_var=prior_var)\n        self.out = Linear_BBB(hidden_units, 1, prior_var=prior_var)\n        self.noise_tol = noise_tol # we will use the noise tolerance to calculate our likelihood\n\n    def forward(self, x):\n        # again, this is equivalent to a standard multilayer perceptron\n        x = torch.sigmoid(self.hidden1(x))\n        x = torch.sigmoid(self.hidden2(x))\n        x = self.out(x)\n        return x\n\n    def log_prior(self):\n        # calculate the log prior over all the layers\n        return self.hidden1.log_prior + self.hidden2.log_prior + self.out.log_prior\n\n    def log_post(self):\n        # calculate the log posterior over all the layers\n        return self.hidden1.log_post + self.hidden2.log_post + self.out.log_post\n\n    def sample_elbo(self, input, target, samples):\n        \n        # we calculate the negative elbo, which will be our loss function\n        #initialize tensors\n        outputs = torch.zeros(samples, target.shape[0]).to(device)\n        log_priors = torch.zeros(samples)\n        log_posts = torch.zeros(samples)\n        log_likes = torch.zeros(samples)\n        # make predictions and calculate prior, posterior, and likelihood for a given number of samples\n        for i in range(samples):\n            outputs[i] = self(input).reshape(-1) # make predictions\n            log_priors[i] = self.log_prior() # get log prior\n            log_posts[i] = self.log_post() # get log variational posterior\n            log_likes[i] = Normal(outputs[i], self.noise_tol).log_prob(target.reshape(-1)).sum() # calculate the log likelihood\n        # calculate monte carlo estimate of prior posterior and likelihood\n        log_prior = log_priors.mean()\n        log_post = log_posts.mean()\n        log_like = log_likes.mean()\n        # calculate the negative elbo (which is our loss function)\n        loss = log_post - log_prior - log_like\n        return loss","be0d9b42":"net = MLP_BBB(16, prior_var=1).to(device)  # \u51fa\u529b\u30af\u30e9\u30b9\u306f1\noptimizer = optim.Adam(net.parameters(), lr=.1)\nnet","0a1f6887":"epochs = 2000\nfor epoch in range(epochs):  # loop over the dataset multiple times\n    optimizer.zero_grad()\n    # forward + backward + optimize\n    loss = net.sample_elbo(x.to(device), y.to(device), 1)\n    loss.backward()\n    optimizer.step()\n    if epoch % 100 == 0:\n        print('epoch: {}\/{}'.format(epoch+1,epochs))\n        print('Loss:', loss.item())\nprint('Finished Training')","8b22ddbc":"# samples is the number of \"predictions\" we make for 1 x-value.\nsamples = 100\nx_tmp = torch.Tensor(x_gt).to(device)\ny_samp = np.zeros((samples, 100))\nprint(f\"x_tmp, y_samp: {x_tmp.shape} {y_samp.shape}\")\n\n\n# n\u56de\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3002\u51fa\u529b\u306f\u91cd\u307f\u306e\u30ac\u30a6\u30b9\u5206\u5e03\u304b\u3089\u8a08\u7b97\u3055\u308c\u305f\u300c\u78ba\u7387\u5206\u5e03\u306b\u3064\u3044\u3066n\u70b9plot\u3057\u305f\u30c7\u30fc\u30bf\u300d\u304cn\u56de\u5410\u304d\u51fa\u3055\u308c\u305f\u3068\u3044\u3046\u3053\u3068\nfor s in range(samples):\n    y_tmp = net(x_tmp).cpu().detach().numpy()\n    y_samp[s] = y_tmp.reshape(-1)\nprint(\"y_samp:\", y_samp.shape)  # shape=(samples, 100). \u5404plot\u70b9\u3054\u3068\u306b\u30c7\u30fc\u30bf\u304cn\u500b\u3042\u308b\u3063\u3066\u3053\u3068\n\n\n# x\u8ef8\u306e\u5404\u70b9\u306e100\u70b9\u306e\u3064\u3044\u3066\u306e\u5e73\u5747\u306895%\u4fe1\u983c\u533a\u9593\u3092\u53d6\u308b\nplt.figure(figsize=(12, 6))\n#plt.plot(x_gt, y_gt, ls=\"--\", color=\"r\", label='ground truth: $y=x^3$')\nplt.plot(\n    x_tmp.cpu().numpy(), np.mean(y_samp, axis=0), label=\"Mean Posterior Predictive\"\n)\nplt.fill_between(\n    x_tmp.cpu().numpy().reshape(-1),\n    np.percentile(y_samp, 2.5, axis=0),\n    np.percentile(y_samp, 97.5, axis=0),\n    color=\"b\",\n    alpha=0.2,\n    label=\"95% Confidence(2$\\sigma$)\",\n)\nplt.scatter(x, y, label = 'data points', marker=\"o\", color=\"0.1\", alpha=0.8)\nplt.title(\"Posterior Predictive\")\nplt.grid()\nplt.legend()\nplt.show()","96a9b974":"# samples is the number of \"predictions\" we make for 1 x-value.\nsamples = 100\nx_tmp = (\n    torch.linspace(-100, 100, samples).reshape(-1, 1).to(device)\n)  # steps\u5024\u304c\u304b\u3089start\u307e\u3067\u7b49\u9593\u9694\u306b\u914d\u7f6e\u3055\u308c\u305f\u30b5\u30a4\u30ba\u306e1\u6b21\u5143\u30c6\u30f3\u30bd\u30eb\u3092\u4f5c\u6210\ny_samp = np.zeros((samples, samples))\nprint(f\"x_tmp, y_samp: {x_tmp.shape} {y_samp.shape}\")\n\n\n# n\u56de\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3002\u51fa\u529b\u306f\u91cd\u307f\u306e\u30ac\u30a6\u30b9\u5206\u5e03\u304b\u3089\u8a08\u7b97\u3055\u308c\u305f\u300c\u78ba\u7387\u5206\u5e03\u306b\u3064\u3044\u3066n\u70b9plot\u3057\u305f\u30c7\u30fc\u30bf\u300d\u304cn\u56de\u5410\u304d\u51fa\u3055\u308c\u305f\u3068\u3044\u3046\u3053\u3068\nfor s in range(samples):\n    y_tmp = net(x_tmp).cpu().detach().numpy()\n    y_samp[s] = y_tmp.reshape(-1)\nprint(\"y_samp:\", y_samp.shape)  # shape=(samples, 100). \u5404plot\u70b9\u3054\u3068\u306b\u30c7\u30fc\u30bf\u304cn\u500b\u3042\u308b\u3063\u3066\u3053\u3068\n\n\n# x\u8ef8\u306e\u5404\u70b9\u306e100\u70b9\u306e\u3064\u3044\u3066\u306e\u5e73\u5747\u306895%\u4fe1\u983c\u533a\u9593\u3092\u53d6\u308b\nplt.figure(figsize=(12, 6))\n#plt.plot(x_gt, y_gt, ls=\"--\", color=\"r\", label='ground truth: $y=x^3$')\nplt.plot(\n    x_tmp.cpu().numpy(), np.mean(y_samp, axis=0), label=\"Mean Posterior Predictive\"\n)\nplt.fill_between(\n    x_tmp.cpu().numpy().reshape(-1),\n    np.percentile(y_samp, 2.5, axis=0),\n    np.percentile(y_samp, 97.5, axis=0),\n    color=\"b\",\n    alpha=0.2,\n    label=\"95% Confidence(2$\\sigma$)\",\n)\nplt.scatter(x, y, label = 'data points', marker=\"o\", color=\"0.1\", alpha=0.8)\nplt.title(\"Posterior Predictive\")\nplt.grid()\nplt.legend()\nplt.show()","85e37e55":"https:\/\/github.com\/ThirstyScholar\/bayes-by-backprop","289d3408":"Predict with a Bayesian NN","dfc74515":"NN\u306e\u904e\u5b66\u7fd2\u306e\u4f8b","5506e88b":"https:\/\/github.com\/cpark321\/uncertainty-deep-learning\/blob\/master\/01.%20Bayes-by-Backprop.ipynb"}}