{"cell_type":{"4a1df9c6":"code","7aac3183":"code","449d324c":"code","5ac0a91f":"code","3a485426":"code","01df8cc7":"code","af406ccd":"code","40460ce9":"code","38b30a08":"code","047b68be":"code","04f93687":"code","5aa43c70":"code","96a523ef":"code","5f18eeae":"code","0357d5f2":"code","e9d358f1":"code","ab7493ca":"code","91081221":"code","b9e56f30":"code","0324dedf":"code","1f506f3f":"code","e08edea3":"code","a55a643b":"code","bfe0b69c":"code","8a83abef":"code","33987d5b":"code","b4fea69a":"code","3dbaad49":"code","146639a0":"code","b2e9c8db":"code","c3fe0c9b":"code","adf005f3":"code","c1cce2af":"code","c92d3b0b":"code","7773df3c":"code","11efbc21":"code","311146d2":"code","efa3014b":"code","14ef28cf":"code","7fb70eb2":"code","91d9136d":"code","eb7c797e":"code","4253bbd3":"code","ad4ffa85":"code","b176417a":"code","a68f78c7":"code","5d7e4f56":"markdown"},"source":{"4a1df9c6":"%%capture\n!pip install transformers tokenizers pytorch-lightning","7aac3183":"'''%%capture\n!git clone https:\/\/github.com\/davidtvs\/pytorch-lr-finder.git && cd pytorch-lr-finder && python setup.py install'''","449d324c":"import torch\nfrom torch import nn\nfrom typing import List\nimport torch.nn.functional as F\nfrom transformers import DistilBertTokenizer, AutoTokenizer, AutoModelWithLMHead, DistilBertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\nimport logging\nimport os\nfrom functools import lru_cache\nfrom tokenizers import ByteLevelBPETokenizer\nfrom tokenizers.processors import BertProcessing\nimport pytorch_lightning as pl\nfrom torch.utils.data import DataLoader, Dataset\nimport pandas as pd\nfrom argparse import Namespace\nfrom sklearn.metrics import classification_report\ntorch.__version__","5ac0a91f":"tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')","3a485426":"model = AutoModelWithLMHead.from_pretrained(\"distilroberta-base\")\nbase_model = model.base_model","01df8cc7":"!mkdir -p meow","af406ccd":"model.save_pretrained(\"meow\")","40460ce9":"tmodel =  AutoModelWithLMHead.from_pretrained(\"meow\").base_model","38b30a08":"text = \"Oh! Yeah!\"\nenc = tokenizer.encode_plus(text)\nenc.keys()","047b68be":"out = base_model(torch.tensor(enc[\"input_ids\"]).unsqueeze(0), torch.tensor(enc[\"attention_mask\"]).unsqueeze(0))\nout[0].shape","04f93687":"t = \"Mathew is stoopid\"\nenc = tokenizer.encode_plus(t)\ntoken_representations = base_model(torch.tensor(enc[\"input_ids\"]).unsqueeze(0))[0][0]\nprint(enc[\"input_ids\"])\nprint(tokenizer.decode(enc[\"input_ids\"]))\nprint(f\"Length: {len(enc['input_ids'])}\")\nprint(token_representations.shape)","5aa43c70":"@torch.jit.script\ndef mish(input):\n    return input * torch.tanh(F.softplus(input))\n  \nclass Mish(nn.Module):\n    def forward(self, input):\n        return mish(input)","96a523ef":"class EmoModel(nn.Module):\n    def __init__(self, base_model, n_classes, base_model_output_size=768, dropout=0.05):\n        super().__init__()\n        self.base_model = base_model\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(base_model_output_size, base_model_output_size),\n            Mish(),\n            nn.Dropout(dropout),\n            nn.Linear(base_model_output_size, n_classes)\n        )\n        \n        for layer in self.classifier:\n            if isinstance(layer, nn.Linear):\n                layer.weight.data.normal_(mean=0.0, std=0.02)\n                if layer.bias is not None:\n                    layer.bias.data.zero_()\n\n    def forward(self, input_, *args):\n        X, attention_mask = input_\n        hidden_states = self.base_model(X, attention_mask=attention_mask)\n        \n\n        return self.classifier(hidden_states[0][:, 0, :])","5f18eeae":"classifier = EmoModel(AutoModelWithLMHead.from_pretrained(\"distilroberta-base\").base_model, 3)","0357d5f2":"X = torch.tensor(enc[\"input_ids\"]).unsqueeze(0).to('cpu')\nattn = torch.tensor(enc[\"attention_mask\"]).unsqueeze(0).to('cpu')","e9d358f1":"classifier((X, attn))","ab7493ca":"!mkdir -p distilroberta-tokenizer","91081221":"tokenizer.save_pretrained(\"distilroberta-tokenizer\")","b9e56f30":"!mv distilroberta-tokenizer\/tokenizer_config.json distilroberta-tokenizer\/config.json","0324dedf":"tmp = AutoTokenizer.from_pretrained(\"distilroberta-tokenizer\")","1f506f3f":"!ls distilroberta-tokenizer","e08edea3":"t = \"Mathew is stoopid\"\nenc = tmp.encode_plus(t)\ntoken_representations = tmodel(torch.tensor(enc[\"input_ids\"]).unsqueeze(0))[0][0]\nprint(enc[\"input_ids\"])\nprint(tmp.decode(enc[\"input_ids\"]))\nprint(f\"Length: {len(enc['input_ids'])}\")\nprint(token_representations.shape)","a55a643b":"class TokenizersCollateFn:\n    def __init__(self, max_tokens=512):\n\n        t = ByteLevelBPETokenizer(\n            \"distilroberta-tokenizer\/vocab.json\",\n            \"distilroberta-tokenizer\/merges.txt\"\n        )\n        t._tokenizer.post_processor = BertProcessing(\n            (\"<\/s>\", t.token_to_id(\"<\/s>\")),\n            (\"<s>\", t.token_to_id(\"<s>\")),\n        )\n        t.enable_truncation(max_tokens)\n        t.enable_padding(max_length=max_tokens, pad_id=t.token_to_id(\"<pad>\"))\n        self.tokenizer = t\n\n    def __call__(self, batch):\n        encoded = self.tokenizer.encode_batch([x[0] for x in batch])\n        sequences_padded = torch.tensor([enc.ids for enc in encoded])\n        attention_masks_padded = torch.tensor([enc.attention_mask for enc in encoded])\n        labels = torch.tensor([x[1] for x in batch])\n        \n        return (sequences_padded, attention_masks_padded), labels","bfe0b69c":"!wget https:\/\/www.dropbox.com\/s\/ikkqxfdbdec3fuj\/test.txt\n!wget https:\/\/www.dropbox.com\/s\/1pzkadrvffbqw6o\/train.txt\n!wget https:\/\/www.dropbox.com\/s\/2mzialpsgf9k5l3\/val.txt","8a83abef":"\ntrain_path = \"train.txt\"\ntest_path = \"test.txt\"\nval_path = \"val.txt\"\n\nlabel2int = {\n  \"sadness\": 0,\n  \"joy\": 1,\n  \"love\": 2,\n  \"anger\": 3,\n  \"fear\": 4,\n  \"surprise\": 5\n}","33987d5b":"!wget https:\/\/www.dropbox.com\/s\/607ptdakxuh5i4s\/merged_training.pkl","b4fea69a":"import pickle\n\ndef load_from_pickle(directory):\n    return pickle.load(open(directory,\"rb\"))","3dbaad49":"data = load_from_pickle(directory=\"merged_training.pkl\")\n\nemotions = [ \"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]\ndata= data[data[\"emotions\"].isin(emotions)]\n\n\ndata = data.sample(n=20000);\n\ndata.emotions.value_counts().plot.bar()","146639a0":"data.reset_index(drop=True, inplace=True)","b2e9c8db":"data.emotions.unique()","c3fe0c9b":"\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\ninput_train, input_val, target_train, target_val = train_test_split(data.text.to_numpy(), \n                                                                    data.emotions.to_numpy(), \n                                                                    test_size=0.2)\n\ninput_val, input_test, target_val, target_test = train_test_split(input_val, target_val, test_size=0.5)\n\n\ntrain_dataset = pd.DataFrame(data={\"text\": input_train, \"class\": target_train})\nval_dataset = pd.DataFrame(data={\"text\": input_val, \"class\": target_val})\ntest_dataset = pd.DataFrame(data={\"text\": input_test, \"class\": target_test})\nfinal_dataset = {\"train\": train_dataset, \"val\": val_dataset , \"test\": test_dataset }\n\ntrain_dataset.to_csv(train_path, sep=\";\",header=False, index=False)\nval_dataset.to_csv(test_path, sep=\";\",header=False, index=False)\ntest_dataset.to_csv(val_path, sep=\";\",header=False, index=False)\n","adf005f3":"class EmoDataset(Dataset):\n    def __init__(self, path):\n        super().__init__()\n        self.data_column = \"text\"\n        self.class_column = \"class\"\n        self.data = pd.read_csv(path, sep=\";\", header=None, names=[self.data_column, self.class_column],\n                               engine=\"python\")\n\n    def __getitem__(self, idx):\n        return self.data.loc[idx, self.data_column], label2int[self.data.loc[idx, self.class_column]]\n\n    def __len__(self):\n        return self.data.shape[0]","c1cce2af":"ds = EmoDataset(train_path)\nds[19]","c92d3b0b":"\n\nclass TrainingModule(pl.LightningModule):\n    def __init__(self, hparams):\n        super().__init__()\n        self.model = EmoModel(AutoModelWithLMHead.from_pretrained(\"distilroberta-base\").base_model, len(emotions))\n        self.loss = nn.CrossEntropyLoss() # combine LogSoftmax() and NLLLoss()\n        self.hparams = hparams\n\n    def step(self, batch, step_name=\"train\"):\n        X, y = batch\n        loss = self.loss(self.forward(X), y)\n        loss_key = f\"{step_name}_loss\"\n        tensorboard_logs = {loss_key: loss}\n\n        return { (\"loss\" if step_name == \"train\" else loss_key): loss, 'log': tensorboard_logs,\n               \"progress_bar\": {loss_key: loss}}\n\n    def forward(self, X, *args):\n        return self.model(X, *args)\n\n    def training_step(self, batch, batch_idx):\n        return self.step(batch, \"train\")\n    \n    def validation_step(self, batch, batch_idx):\n        return self.step(batch, \"val\")\n\n    def validation_end(self, outputs: List[dict]):\n        loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n        return {\"val_loss\": loss}\n        \n    def test_step(self, batch, batch_idx):\n        return self.step(batch, \"test\")\n    \n    def train_dataloader(self):\n        return self.create_data_loader(self.hparams.train_path, shuffle=True)\n\n    def val_dataloader(self):\n        return self.create_data_loader(self.hparams.val_path)\n\n    def test_dataloader(self):\n        return self.create_data_loader(self.hparams.test_path)\n                \n    def create_data_loader(self, ds_path: str, shuffle=False):\n        return DataLoader(\n                    EmoDataset(ds_path),\n                    num_workers=4,\n                    batch_size=self.hparams.batch_size,\n                    shuffle=shuffle,\n                    collate_fn=TokenizersCollateFn()\n        )\n        \n    @lru_cache()\n    def total_steps(self):\n        return len(self.train_dataloader()) \/\/ self.hparams.accumulate_grad_batches * self.hparams.epochs\n\n    def configure_optimizers(self):\n        optimizer = AdamW(self.model.parameters(), lr=self.hparams.lr)\n        lr_scheduler = get_linear_schedule_with_warmup(\n                    optimizer,\n                    num_warmup_steps=self.hparams.warmup_steps,\n                    num_training_steps=self.total_steps(),\n        )\n        return [optimizer], [{\"scheduler\": lr_scheduler, \"interval\": \"step\"}]","7773df3c":"#!pip install torch-lr-finder","11efbc21":"'''lr=0.1 ## uper bound LR\nfrom torch_lr_finder import LRFinder\nhparams_tmp = Namespace(\n    train_path=train_path,\n    val_path=val_path,\n    test_path=test_path,\n    batch_size=16,\n    warmup_steps=100,\n    epochs=1,\n    lr=lr,\n    accumulate_grad_batches=1,\n)\nmodule = TrainingModule(hparams_tmp)\ncriterion = nn.CrossEntropyLoss()\noptimizer = AdamW(module.parameters(), lr=5e-7) ## lower bound LR\nlr_finder = LRFinder(module, optimizer, criterion, device=\"cuda\")\nlr_finder.range_test(module.train_dataloader(), end_lr=100, num_iter=100, accumulation_steps=hparams_tmp.accumulate_grad_batches)\nlr_finder.plot()\nlr_finder.reset()'''","311146d2":"lr = 1e-4 \nlr","efa3014b":"#lr_finder.plot(show_lr=lr)","14ef28cf":"hparams = Namespace(\n    train_path=train_path,\n    val_path=val_path,\n    test_path=test_path,\n    batch_size=32,\n    warmup_steps=100,\n    epochs=2,\n    lr=lr,\n    accumulate_grad_batches=1\n)\nmodule = TrainingModule(hparams)","7fb70eb2":"len(emotions)","91d9136d":"import gc; gc.collect()\ntorch.cuda.empty_cache()","eb7c797e":"trainer = pl.Trainer(gpus=1, max_epochs=hparams.epochs, progress_bar_refresh_rate=10,\n                     accumulate_grad_batches=hparams.accumulate_grad_batches)\n\ntrainer.fit(module)","4253bbd3":"with torch.no_grad():\n    progress = [\"\/\", \"-\", \"\\\\\", \"|\", \"\/\", \"-\", \"\\\\\", \"|\"]\n    module.eval()\n    true_y, pred_y = [], []\n    for i, batch_ in enumerate(module.test_dataloader()):\n        (X, attn), y = batch_\n        batch = (X.cuda(), attn.cuda())\n        print(progress[i % len(progress)], end=\"\\r\")\n        y_pred = torch.argmax(module(batch), dim=1)\n        true_y.extend(y.cpu())\n        pred_y.extend(y_pred.cpu())\nprint(\"\\n\" + \"_\" * 80)\nprint(classification_report(true_y, pred_y, target_names=label2int.keys(), digits=len(emotions)))","ad4ffa85":"model = module.model\ntorch.save(model.state_dict(), 'roberta_trained')  # model which is gonna get uploaded in IBM","b176417a":"model = EmoModel(AutoModelWithLMHead.from_pretrained(\"distilroberta-base\").base_model, len(emotions))\nmodel.load_state_dict(torch.load('roberta_trained'))   \nmodel = model.cuda()","a68f78c7":"n = np.random.randint(0,len(ds))        # random index for ds\nprint(ds[n])\nt = ds[n][0]        # custom string can be placed here for prediction\nenc = tokenizer.encode_plus(t)\nX = torch.tensor(enc[\"input_ids\"]).unsqueeze(0)\nattn = torch.tensor(enc[\"attention_mask\"]).unsqueeze(0)\ntorch.argmax( model( (X.cuda(), attn.cuda()) ) ).item()","5d7e4f56":"`torch.Size([1, 768])` represents batch_size, number of tokens in input text (lenght of tokenized text), model's output hidden size."}}