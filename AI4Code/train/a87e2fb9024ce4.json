{"cell_type":{"bebe8710":"code","8d7d8ef3":"code","adc35c70":"code","64a59738":"code","f255bd34":"code","e3d108bc":"code","bb12455b":"code","bef0904f":"code","4675dbbf":"code","3f5d7c82":"code","40f16b06":"code","99093ab0":"code","9f937603":"code","ea554572":"code","88603e72":"code","25fed1bd":"code","79b573a3":"code","0abc6a9e":"code","388e24e2":"code","325fdd0b":"code","05c02604":"code","e1116951":"code","3096e31b":"code","b160df35":"code","67cecbd3":"code","40ae3127":"code","0c1911ee":"code","790525f4":"code","02c4a119":"code","a345020f":"code","b27fa191":"code","b771efa6":"markdown","cf2bcd5a":"markdown","444432ff":"markdown","312eb295":"markdown","a4254f7f":"markdown","2c3ceecb":"markdown","107b0a13":"markdown","c828a1f0":"markdown","d5117e20":"markdown","318ef0c0":"markdown","c124f72e":"markdown","df56820f":"markdown","2721908f":"markdown","428c5d94":"markdown","7fe7c556":"markdown","f79565da":"markdown","06a038cc":"markdown","04625af5":"markdown","b0b2aa98":"markdown","3fcc22b5":"markdown","683b31a6":"markdown","6bb71706":"markdown","6702beeb":"markdown","6a9d6058":"markdown","1c014f60":"markdown","1b043b58":"markdown"},"source":{"bebe8710":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8d7d8ef3":"import tensorflow as tf\n\nimport requests\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import SGD\n","adc35c70":"# Due to memory allocation limit we are using 10 percent of actual dataset\nurl1 = 'https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/kddcup99-mld\/kddcup.data.gz'\nurl2 =  'https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/kddcup99-mld\/kddcup.data_10_percent.gz'\n\nfile_name1 = 'kddcup.data.gz'\nfile_name2 = 'kddcup.data_10_percent.gz'\ndata_path = tf.keras.utils.get_file(file_name2, origin=url2)\n\nprint(data_path)","64a59738":"# read the given data.gz using pandas read csv()\n\ndata = pd.read_csv(data_path, header= None)\ndata","f255bd34":"names_url = 'https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/kddcup99-mld\/kddcup.names'\nf1 = requests.get(names_url)\nprint(f1.text)","e3d108bc":"attack_types = 'https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/kddcup99-mld\/training_attack_types'\ntypes = requests.get(attack_types)\nprint(types.text)","bb12455b":"# Target types data extraction\nattack_dict = {}\ntypes_text_split = types.text.split()\nfor idx in range(0, len(types_text_split)):\n    if idx < len(types_text_split) -1:\n      attack_dict[types_text_split[idx]] = types_text_split[idx+1]\nattack_dict['normal'] = 'normal'\nattack_dict","bef0904f":"# extract column names from .names file\ncol_names = []\nf1_text_split = f1.text.split('\\n')\nfor idx in range(1, len(f1_text_split)):\n  col_name = f1_text_split[idx].split(':')[0]\n  if idx == len(f1_text_split)-1:\n    col_name = 'target'\n  col_names.append(col_name)\ncol_names","4675dbbf":"# assign the actual columns to the dataframe\ndata.columns = col_names\ndata","3f5d7c82":"# Map the class names based on target column. Lets check the unique values in target column\ndata.target.value_counts()","40f16b06":"# map actual type to another column called 'target_type'\ndata['target_type'] = data.target.apply(lambda x : attack_dict[x[0:-1]] )\ndata.target_type.value_counts()","99093ab0":"# check missing values \ndata.info()","9f937603":"# Identifying categorical features\nnumeric_cols = data._get_numeric_data().columns # gets all the numeric column names\n\ncategorical_cols = list(set(data.columns)-set(numeric_cols))\ncategorical_cols","ea554572":"# lets look into deeply to identify if there are any other binary data exists or not\nbinary_cols = []\nfor col in numeric_cols:\n  if len(data[col].unique()) <= 2:\n      result = []\n      s = data[col].value_counts()\n      t = float(len(data[col]))\n      for v in s.index:\n          result.append(\"{}({}%)\".format(v,round(100*(s[v]\/t),1)))\n      print(\"{} - [{}]\".format(col, \" , \".join(result)))\n      binary_cols.append(col)","88603e72":"# combine all categorical column names\nfor col in binary_cols:\n  categorical_cols.append(col)\ncategorical_cols","25fed1bd":"\ndef plot_dist(col, ax):\n    data[col].value_counts().plot(kind='bar', facecolor='y', ax=ax)\n    ax.set_xlabel('{}'.format(col), fontsize=18)\n    ax.set_title(\"{} on KDD Cup\".format(col), fontsize= 18)\n    plt.xticks(rotation=45)\n    return ax\nf, ax = plt.subplots(5,2, figsize = (22,40))\nf.tight_layout(h_pad=15, w_pad=10, rect=[0, 0.08, 1, 0.93])\n\ncategorical_cols_plot = [ col for col in categorical_cols if col!='num_outbound_cmds']\nk = 0\nfor i in range(5):\n    for j in range(2):\n        plot_dist(categorical_cols_plot[k], ax[i][j])\n        k += 1\n__ = plt.suptitle(\"Distributions of Categorical features\", fontsize= 23)","79b573a3":"# identify remaining numeric features by subtracting categorical columns\nnumeric_features = list(set(numeric_cols)-set(categorical_cols))\nnumeric_features","0abc6a9e":"def plot_std_dist(_title):\n  df_std = data[numeric_features].std()\n  plt.figure(figsize=(25,6))\n  plt.plot(list(df_std.index) ,list(df_std.values), 'yo', markersize=25)\n  plt.xticks(rotation=90)\n  plt.title(_title, fontsize= 18)\n  plt.show()\n\nplot_std_dist('Standard Deviation on Nuemeric features')","388e24e2":"def apply_zscore(feature):\n  mean = data[feature].mean()\n  std = data[feature].std()\n  data[feature] = (data[feature] - mean) \/ std\n\nfor feature in numeric_features:\n  apply_zscore(feature)","325fdd0b":"plot_std_dist('Standard Deviation on Nuemeric features after applying zscore')","05c02604":"def apply_dummies(df, feature):\n    get_dummies = pd.get_dummies(df[feature])\n    for x in get_dummies.columns:\n        dummy_name = f\"{feature}-{x}\"\n        df[dummy_name] = get_dummies[x]\n    df.drop(feature, axis=1, inplace=True)\n    return None","e1116951":"data.shape","3096e31b":"# apply one hot encoding for categorical columns except the target and target_types\nfor feature in categorical_cols:\n  if feature not in ['target', 'target_type']:\n    apply_dummies(data, feature)","b160df35":"data","67cecbd3":"# convert to numpy arrays\nx_features = data.columns.drop(['target','target_type'])\nx = data[x_features].values\nprint('Shape of Independent features data : ' + str(x.shape))\ntarget_type_dummies = pd.get_dummies(data['target_type']) # Multi Class Classification (tartget_type is grouped attack types)\ntarget = target_type_dummies.columns\nnum_classes = len(target)\ny = target_type_dummies.values\nprint('Shape of Dependent features data : ' + str(y.shape))","40ae3127":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=101)\nprint('Shape of Independent features Train data : ' + str(X_train.shape))\nprint('Shape of Dependent features Train data : ' + str(y_train.shape))\nprint('Shape of Independent features Test data: ' + str(X_test.shape))\nprint('Shape of Dependent features Test data: ' + str(y_test.shape))","0c1911ee":"# create model with most common parameters \ndef seq_model():\n    model = Sequential()\n    model.add(Dense(x.shape[1],input_dim =x.shape[1],activation = 'relu',kernel_initializer='random_uniform'))\n    model.add(Dense(1,activation='relu',kernel_initializer='random_uniform'))\n    model.add(Dense(y.shape[1],activation='softmax'))   \n    return model","790525f4":"def evaluate_model(model):\n    _, train_acc = model.evaluate(X_train, y_train, verbose=0)\n    _, test_acc = model.evaluate(X_test, y_test, verbose=0)\n    print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n    return None","02c4a119":"def plot_results(history, optimizer, loss_fun):\n    plt.figure(figsize=(25,6))\n    # plot loss during training\n    plt.subplot(121)\n    plt.title('Loss')\n    plt.plot(history.history['loss'], label='train')\n    plt.plot(history.history['val_loss'], label='test')\n    plt.legend()\n    # plot accuracy during training\n    plt.subplot(122)\n    plt.title('Accuracy')\n    plt.plot(history.history['accuracy'], label='train')\n    plt.plot(history.history['val_accuracy'], label='test')\n    plt.legend()\n    plt.suptitle(f\"{optimizer} Optimizer with {loss_fun} as Loss function\", fontsize= 23)\n    plt.show()\n    return None","a345020f":"# model with crossentropy loss and adam optimizer\nmodel = seq_model()\nmodel.compile(loss ='categorical_crossentropy',optimizer = 'adam',metrics = ['accuracy'])\nmonitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto')\nhistory = model.fit(X_train,y_train,validation_data=(X_test,y_test),callbacks=[monitor],verbose=2,epochs=50)\nevaluate_model(model)\nplot_results(history, 'Adam', 'categorical_crossentropy')","b27fa191":"# model with crossentropy loss and SGD optimizer\nmodel = seq_model()\nopt = SGD(lr=0.01, momentum=0.9)\nmodel.compile(loss ='categorical_crossentropy',optimizer = opt,metrics = ['accuracy'])\nmonitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto')\nhistory = model.fit(X_train,y_train,validation_data=(X_test,y_test),callbacks=[monitor],verbose=2,epochs=50)\nevaluate_model(model)\nplot_results(history, 'SGD', 'categorical_crossentropy')","b771efa6":"https:\/\/archive.ics.uci.edu\/ml\/datasets\/KDD+Cup+1999+Data\n![abstract.png](attachment:abstract.png)\n\n![data_info.png](attachment:data_info.png)","cf2bcd5a":"# Split the train & test data","444432ff":"> In the last line of the output, we can see the total number of rows and columns for the given file i.e 494021 rows \u00d7 42 columns","312eb295":"> Except the first line remaining data looks like column names and their data types.\nNow let\u2019s explore the training_attack_types file\n","a4254f7f":"# Numerical Features Exploration","2c3ceecb":"> There is no header provide in the given kddcup.data.gz file. lets look into the attribute information to understand the features names, for this we can see the content inside kddcup.names file","107b0a13":"# Libraries\n","c828a1f0":"# Convert categorical data to numerical values","d5117e20":"# Cross-entropy Loss Function\n* It is the default loss function\n* It calculate a score that summarizes the average difference between the actual and predicted probability distributions for all the classes. The score is minimized and a perfect cross-entropy value is 0.","318ef0c0":"# Sparse Multiclass Cross-Entropy Loss\n* In general, Multi-Class Cross-Entropy Loss works better for small number of discreate classes\n* Where as Sparse Multiclass Cross-Entropy Loss works better for identification of large number of labels (i.e it may have tens or hundreds of thousands of categories, one for each label). Hence not applying on this dataset","c124f72e":"# Dataset","df56820f":"> Note: 'target' column is our prediction and target_type is grouped data","2721908f":"# Get data using tensorflow","428c5d94":"## Feedback\n* Your feedback is much appreciated\n* Comment if you have any doubts or you found any errors in the notebook","7fe7c556":"> there is no missing data otherwise it should display the not-null counts for each feature.","f79565da":"# Dataframe creation","06a038cc":"> Take away:  So the first line represents in the .names file is actual types of attacks and this is our target column. and It is Multi class classification problem, we can see types of attack are separated with space to classify the type. Also type 'normal' is not mentioned. hence add it to dictionary to match the target column","04625af5":"# Model Creation","b0b2aa98":"# Categorical Features Exploration","3fcc22b5":"> Different normal distributions are exists. Hence apply zscore ","683b31a6":"# Conclusion\n* NN with 'Adam' optimizer and 'Cross-Entropy' Loss function gives better accuracy with minimum loss","6bb71706":"**Happy learning!!**","6702beeb":"> According to dataset attribute information\n*   target column 'Normal' represents Good Connection \n*   Bad connection attack types are\n    * DoS(Denial of Service)\n    * User to Root(U2R)\n    * Remote to Local(R2L)\n    * Probe","6a9d6058":"# Goal\n\nIn this nortebook we will explore following tasks & topics: \n* Data Load using tensorflow\n* Data Exploration (Categorical and Nuemrical features)\n* Numerical data distribution (applying zscore)\n* Convert categorical data to numerical values\n* Data split (training set and test set)\n* Creation of Tensorflow Sequential model\n* Popular Optimizers in Tensorflow (tf.keras.optimizers ) ( SGD, Adam)\n* Most used Loss Functions for Multi-Class Classification in Tensorflow\n ","1c014f60":"> There is an extra '.'(dot) at the end of each attack name. Hence lets format to match the actual attack types.","1b043b58":"# Optimizers \n\n### Adam: Adaptive Moment Estimation\n* It is the most popular optimizer\n* which combines RMSProp and Momentum.\n* RMSProp is an exponentially weighted average for gradient accumulation.\n* Momentum is an added term in the objective function, which is a value between 0 and 1 that increases the size of the steps taken towards the minimum by trying to jump from a local minimum. If the momentum term is large then the learning rate should be kept smaller.\n\n### SGD: Stochastic Gradient Descent\n* which consists learning rate and momentum.\n\n"}}