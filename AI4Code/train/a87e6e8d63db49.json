{"cell_type":{"48e89310":"code","69e17297":"code","a20b02b3":"code","0cf292dd":"code","2eef80e2":"code","de80a9be":"code","ff821091":"code","64c8f7de":"code","00625104":"code","515300e3":"code","4fc854ce":"code","20a66241":"code","611d8f11":"code","f6fad79a":"code","c0df7566":"code","ba19a148":"code","2319de85":"code","f05b6037":"markdown","1fc0e48a":"markdown","576c261f":"markdown","6b9ab9a7":"markdown","24b8a494":"markdown","87262f81":"markdown"},"source":{"48e89310":"!pip install tensor-sensor","69e17297":"import tsensor\nimport numpy as np\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, GlobalAveragePooling1D\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","a20b02b3":"# Tokenising sentences\nsentences = [\n    'The quick brown fox jumps over the lazy dog.'\n]\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(sentences)","0cf292dd":"print(\"Word Index       :\", tokenizer.word_index)","2eef80e2":"train_sequence = tokenizer.texts_to_sequences(sentences)\ntrain_sequence = np.array(train_sequence)\nprint(\"Sentence Sequence :\", train_sequence)","de80a9be":"# Create a random embedding layer\nembedding = Embedding(input_dim=len(train_sequence[0]), output_dim=128)","ff821091":"# Get the embeddings of the train sample\ntrain_sample = embedding(train_sequence)","64c8f7de":"print(\"Shape of Input          :\", train_sequence.shape)\nprint(\"Shape of Embedded Input :\", train_sample.shape)","00625104":"with tsensor.explain(fontname='Hack', dimfontname='Hack'):\n    train_sample = embedding(train_sequence)","515300e3":"train_sample[0]","4fc854ce":"GlobalAveragePooling1D()(train_sample)","20a66241":"with tsensor.explain(fontname='Hack', dimfontname='Hack'):\n    z = GlobalAveragePooling1D()(train_sample)","611d8f11":"# More than one sentence\n\ntest_corpus = [\n    'The quick brown fox jumps over the lazy dog.',\n    'The quick brown fox.',\n    'The lazy dog',\n    'The dog',\n    'Dog and the fox',\n    'Hello, world!'\n]\n\nencoded_sentences = tokenizer.texts_to_sequences(test_corpus)\ni = 1\nfor sentence, encoded_sentence in zip(test_corpus, encoded_sentences):\n    print(\"Sentence\",str(i) + \" :\", sentence)\n    print(\"Sequence   :\", encoded_sentence)\n    print(\"---------------------------------------------------------\")\n    i+=1","f6fad79a":"# Length of each sentence in the corpus\nprint(\"Length :\", [len(sentence) for sentence in encoded_sentences])","c0df7566":"# Length of the longest sentence\nprint(\"Max Length :\", max([len(sentence) for sentence in encoded_sentences]))","ba19a148":"MAX_SEQUENCE_LENGTH = 9","2319de85":"# Padding sequence that are shorter than the longest sequence\nX = pad_sequences(encoded_sentences, maxlen=MAX_SEQUENCE_LENGTH)\nX","f05b6037":"## Average Across Tokens","1fc0e48a":"## Tokenization","576c261f":"## Importing Libraries","6b9ab9a7":"## Padding Sequences","24b8a494":"## Create Word Embeddings for More than One Sentence","87262f81":"## Creating Embedding Layer"}}