{"cell_type":{"35c51e16":"code","72c4a369":"code","1fa63753":"code","b5bb2906":"code","beca3b7c":"code","4ab62b18":"code","f6ee09cc":"code","6cae415f":"code","822f7278":"code","2ebf0a2c":"code","8c3b2107":"code","11a62781":"code","315de234":"code","6a6e3d2b":"code","b0f3fe10":"code","6df8af6c":"code","c047e296":"code","cb79a5af":"code","52f5494d":"code","d4521225":"code","5899772c":"code","a4757e2f":"code","16253ba8":"code","4a7869c4":"code","9150c597":"code","b869b0ff":"code","d8c334c2":"code","413fcd13":"code","ceb21c1c":"code","f3e75f8c":"code","ec43f3cc":"code","f9420919":"code","74c5bb71":"code","b2ea94e7":"code","9beecba6":"code","00a9b172":"code","b86113b9":"code","d5daf730":"code","88758bba":"code","b904f8da":"code","74f2d834":"code","7160923f":"code","a14b9858":"code","40f60369":"code","225b8461":"code","cb555c9d":"code","a6a6e9fc":"code","237f4fba":"code","23c9b97f":"code","1b54dea0":"code","92f2aa18":"code","dd635ed2":"code","b6ec0fd0":"markdown","30a8fa83":"markdown","5c781505":"markdown","b772da3f":"markdown","2790196a":"markdown","1e74076e":"markdown","8f81d0be":"markdown","dddfd147":"markdown"},"source":{"35c51e16":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","72c4a369":"! pip install vaderSentiment","1fa63753":"from time import sleep\nimport json\nimport pandas as pd\nimport io\nimport re\nimport numpy as np\nfrom tqdm import tqdm\nimport pandas as pd\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nfrom tqdm import tnrange, tqdm_notebook, tqdm\n\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\n# from plotly.offline import init_notebook_mode, iplot\n# import plotly.graph_objs as go\n# init_notebook_mode(connected=True) ","b5bb2906":"tweets_raw_file   = '\/kaggle\/input\/bitcoin-tweets\/Bitcoin_tweets.csv'\ntweets_clean_file = '.\/Bitcoin_tweets_clean.csv'\nbit_price_file2 = '\/kaggle\/input\/bitcoin-price-history\/BTC-USD.csv'\n# bit_price_file2 = 'data\/BTC-USD.csv'","beca3b7c":"df_raw = pd.read_csv(tweets_raw_file,low_memory=False)\nprint(df_raw.shape)\ndf_raw.head(5)","4ab62b18":"# clean df \ndf_raw = df_raw.sort_values(by = 'date')\ndd = df_raw.sample(frac=0.01, replace=False, random_state=1)\ndd.reset_index(inplace=True)\nfor i,s in enumerate(tqdm(dd['text'],position=0, leave=True)):\n    text = str(dd.loc[i, 'text'])\n    text = text.replace(\"#\", \"\")\n    text = re.sub('https?:\/\/(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+', '', text, flags=re.MULTILINE)\n    text = re.sub('@\\\\w+ *', '', text, flags=re.MULTILINE)\n    dd.loc[i, 'text'] = text\n# f = open(tweets_clean_file, 'a+', encoding='utf-8')\ndd.to_csv(tweets_clean_file, header=True, encoding='utf-8',index=False)","f6ee09cc":"df_clean = pd.read_csv(tweets_clean_file)","6cae415f":"analyzer = SentimentIntensityAnalyzer()\ncompound = []\nfor i,s in enumerate(tqdm(df_clean['text'],position=0, leave=True)):\n    # print(i,s)\n    vs = analyzer.polarity_scores(str(s))\n    compound.append(vs[\"compound\"])\ndf_clean[\"compound\"] = compound\ndf_clean.head(2)","822f7278":"scores = []\nfor i, s in tqdm(df_clean.iterrows(), total=df_clean.shape[0],position=0, leave=True):\n    try:\n        scores.append(s[\"compound\"] * ((int(s[\"user_followers\"]))) * ((int(s[\"user_favourites\"])+1)\/int(s['user_followers']+1)) *((int(s[\"is_retweet\"])+1)))\n    except:\n        scores.append(np.nan)\ndf_clean[\"score\"] = scores\ndf_clean.head(2)","2ebf0a2c":"df_price = pd.read_csv(bit_price_file2)\ndf_price.Date = pd.to_datetime(df_price.Date)\n# df_price.Timestamp = pd.to_datetime(df_price.Timestamp,unit='s')\ndf_price.head(2)","8c3b2107":"# sentiment analysis \ndf_clean = df_clean.drop_duplicates()\ntweets = df_clean.copy()\ntweets['date'] = pd.to_datetime(tweets['date'],utc=True)\ntweets.date = tweets.date.dt.tz_localize(None)\ntweets.index = tweets['date']\n\n# tweets_grouped = tweets.groupby(pd.TimeGrouper('1h'))['score'].sum()\ntweets_grouped = tweets.resample('1h').sum()\n\ncrypto_usd = df_price.copy()\ncrypto_usd['Date'] = pd.to_datetime(crypto_usd['Date'], unit='s')\ncrypto_usd.index = crypto_usd['Date']\n# crypto_usd['Timestamp'] = pd.to_datetime(crypto_usd['Timestamp'], unit='s')\n# crypto_usd.index = crypto_usd['Timestamp']\n\n# crypto_usd_grouped = crypto_usd.groupby(pd.TimeGrouper('1h'))['Weighted_Price'].mean()\ncrypto_usd_grouped = crypto_usd.resample('D')['Close'].mean()","11a62781":"def crosscorr(datax, datay, lag=0, method=\"pearson\"):\n    \"\"\" Lag-N cross correlation. \n    Parameters\n    \u2014------\u2014\n    lag : int, default 0\n    datax, datay : pandas.Series objects of equal length\n\n    Returns\n    \u2014------\u2014\n    crosscorr : float\n    \"\"\"\n    return datax.corrwith(datay.shift(lag), method=method)['score']\n# xcov = [crosscorr(tweets_grouped, crypto_usd_grouped, lag=i, m ='pearson' ) for i in range(-20,20)]\n# tweets_grouped.corrwith(crypto_usd_grouped,method='pearson')","315de234":"beggining = max(tweets_grouped.index.min().replace(tzinfo=None), crypto_usd_grouped.index.min())\nend = min(tweets_grouped.index.max().replace(tzinfo=None), crypto_usd_grouped.index.max())\ntweets_grouped = tweets_grouped[beggining:end]\ncrypto_usd_grouped = crypto_usd_grouped[beggining:end]","6a6e3d2b":"fig, ax1 = plt.subplots(figsize=(20,10))\nax1.set_title(\"Crypto currency evolution compared to twitter sentiment\", fontsize=18)\nax1.tick_params(labelsize=14)\nax2 = ax1.twinx()\nax1.plot_date(tweets_grouped.index, tweets_grouped, 'g-')\nax2.plot_date(crypto_usd_grouped.index, crypto_usd_grouped, 'b-')\n\nax1.set_ylabel(\"Sentiment\", color='g', fontsize=16)\nax2.set_ylabel(\"Bitcoin [$]\", color='b', fontsize=16)\nplt.show()","b0f3fe10":"xcov = [crosscorr(tweets_grouped, crypto_usd_grouped, lag=i, method=\"pearson\") for i in range(-20,20)]\nplt.plot(range(-20,20), xcov)\nplt.title(\"pearson cross-correlation\")\nplt.xlabel(\"lag\")\nplt.ylabel(\"correlation\")\nplt.show()\n\nxcov = [crosscorr(tweets_grouped, crypto_usd_grouped, lag=i, method=\"kendall\") for i in range(-20,20)]\nplt.plot(range(-20,20), xcov)\nplt.title(\"kendall cross-correlation\")\nplt.xlabel(\"lag\")\nplt.ylabel(\"correlation\")\nplt.show()\n\nxcov = [crosscorr(tweets_grouped, crypto_usd_grouped, lag=i, method=\"spearman\") for i in range(-20,20)]\nplt.plot(range(-20,20), xcov)\nplt.title(\"spearman cross-correlation\")\nplt.xlabel(\"lag\")\nplt.ylabel(\"correlation\")\nplt.show()","6df8af6c":"# Normalize time series data\nmin_max_scaler = preprocessing.StandardScaler()\nscore_scaled = min_max_scaler.fit_transform(tweets_grouped['score'].values.reshape(-1,1))\ntweets_grouped['normalized_score'] = score_scaled\n# crypto_used_grouped_scaled = min_max_scaler.fit_transform(crypto_usd_grouped.values.reshape(-1,1))\ncrypto_used_grouped_scaled = crypto_usd_grouped \/ max(crypto_usd_grouped.max(), abs(crypto_usd_grouped.min()))\n# crypto_usd_grouped['normalized_price'] = crypto_used_grouped_scaled\n\nfig, ax1 = plt.subplots(figsize=(20,10))\nax1.set_title(\"Normalized Crypto currency evolution compared to normalized twitter sentiment\", fontsize=18)\nax1.tick_params(labelsize=14)\n\nax2 = ax1.twinx()\nax1.plot_date(tweets_grouped.index, tweets_grouped['normalized_score'], 'g-')\nax2.plot_date(crypto_usd_grouped.index, crypto_used_grouped_scaled, 'b-')\n\nax1.set_ylabel(\"Sentiment\", color='g', fontsize=16)\nax2.set_ylabel(\"Bitcoin normalized\", color='b', fontsize=16)\nplt.show()","c047e296":"#tweets_grouped.T.corr(crypto_usd_grouped, method='pearson')\n#tweets_grouped.T.autocorr(crypto_usd_grouped, lag=20)\nxcov = [crosscorr(tweets_grouped, crypto_usd_grouped, lag=i) for i in range(-20,20)]\nplt.plot(range(-20,20), xcov)\nplt.title(\"lag's impact on correlation (normalized)\")\nplt.xlabel(\"lag\")\nplt.ylabel(\"correlation\")\nplt.show()","cb79a5af":"# Derivative\ntweets_grouped_derivative = pd.Series(np.gradient(tweets_grouped['normalized_score'].values), tweets_grouped.index, name='slope')\ncrypto_usd_grouped_derivative = pd.Series(np.gradient(crypto_usd_grouped.values), crypto_usd_grouped.index, name='slope')","52f5494d":"fig, ax1 = plt.subplots(figsize=(20,10))\nax1.set_title(\"Derivative of crypto currency and sentiment's score\", fontsize=18)\nax1.tick_params(labelsize=14)\n\nax2 = ax1.twinx()\nax1.plot_date(tweets_grouped_derivative.index, tweets_grouped_derivative, 'g-')\nax2.plot_date(crypto_usd_grouped_derivative.index, crypto_usd_grouped_derivative, 'b-')\n\nax1.set_ylabel(\"Sentiment's derivative\", color='g', fontsize=16)\nax2.set_ylabel('Bitcoin price derivative', color='b', fontsize=16)\nplt.show()","d4521225":"xcov = [crosscorr(tweets_grouped, crypto_usd_grouped_derivative, lag=i, method=\"pearson\") for i in range(-20,20)]\nplt.plot(range(-20,20), xcov)\nplt.title(\"pearson cross-corelation (derivative)\")\nplt.xlabel(\"lag\")\nplt.ylabel(\"correlation\")\nplt.show()\n\nxcov = [crosscorr(tweets_grouped, crypto_usd_grouped_derivative, lag=i, method=\"kendall\") for i in range(-20,20)]\nplt.plot(range(-20,20), xcov)\nplt.title(\"kendall cross-corelation (derivative)\")\nplt.xlabel(\"lag\")\nplt.ylabel(\"correlation\")\nplt.show()\n\nxcov = [crosscorr(tweets_grouped, crypto_usd_grouped_derivative, lag=i, method=\"spearman\") for i in range(-20,20)]\nplt.plot(range(-20,20), xcov)\nplt.title(\"spearman cross-corelation (derivative)\")\nplt.xlabel(\"lag\")\nplt.ylabel(\"correlation\")\nplt.show()","5899772c":"! pip install textblob \nfrom textblob import TextBlob\n\ndf = df_clean.copy()\ndf.dropna(subset=['hashtags'], inplace=True)\ndf = df[['text']] \ndf.columns = ['tweets']\ndf.head()","a4757e2f":"import nltk\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nnltk.download('wordnet')\nnltk.download('stopwords')\nnltk.download('punkt')\nstop_words = nltk.corpus.stopwords.words(['english'])\n\nprint(stop_words)","16253ba8":"from nltk.tokenize import TweetTokenizer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nlem = WordNetLemmatizer()\n\ndef cleaning(data):\n    #remove urls\n    tweet_without_url = re.sub(r'http\\S+',' ', data)\n\n    #remove hashtags\n    tweet_without_hashtag = re.sub(r'#\\w+', ' ', tweet_without_url)\n\n    #3. Remove mentions and characters that not in the English alphabets\n    tweet_without_mentions = re.sub(r'@\\w+',' ', tweet_without_hashtag)\n    precleaned_tweet = re.sub('[^A-Za-z]+', ' ', tweet_without_mentions)\n\n    #2. Tokenize\n    tweet_tokens = TweetTokenizer().tokenize(precleaned_tweet)\n\n    #3. Remove Puncs\n    tokens_without_punc = [w for w in tweet_tokens if w.isalpha()]\n\n    #4. Removing Stopwords\n    tokens_without_sw = [t for t in tokens_without_punc if t not in stop_words]\n\n    #5. lemma\n    text_cleaned = [lem.lemmatize(t) for t in tokens_without_sw]\n\n    #6. Joining\n    return \" \".join(text_cleaned)","4a7869c4":"df['cleaned_tweets'] = df['tweets'].apply(cleaning)\ndf['date'] = df_clean['date']\ndf['date_clean'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')\ndf.drop(columns='date',inplace=True)\ndf.head()","9150c597":"def getSubjectivity(tweet):\n    return TextBlob(tweet).sentiment.subjectivity\n\ndef getPolarity(tweet):\n    return TextBlob(tweet).sentiment.polarity","b869b0ff":"def crypto_price_cate(score):\n    if score < 1:\n        return 'negative'\n    elif score == 1:\n        return 'neutral'\n    else:\n        return 'positive'\ndef observe_period(period):\n    res = crypto_usd_grouped.shift(period)\/crypto_usd_grouped\n    res = res.apply(crypto_price_cate)\n    return res \n\ntime_sentiment = observe_period(7) # compare price ratio in 7 days. price_7_days_later\/ price_now \ndf['crypto_sentiment'] = df.date_clean.apply(lambda x: time_sentiment[x] if x in time_sentiment else np.nan)","d8c334c2":"# may takes time \ndf['subjectivity'] = df['cleaned_tweets'].apply(getSubjectivity)\ndf['polarity'] = df['cleaned_tweets'].apply(getPolarity)\ndf.head()","413fcd13":"def getSentiment(score):\n    if score < 0:\n        return 'negative'\n    elif score == 0:\n        return 'neutral'\n    else:\n        return 'positive'\ndf['sentiment'] = df['polarity'].apply(getSentiment)\ndf['target'] = df['sentiment'] == df['crypto_sentiment']\ndf.head()\ndf.to_csv('.\/df_data.csv')","ceb21c1c":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow.keras.layers as Layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Bidirectional, SpatialDropout1D\nfrom tensorflow.keras.models import load_model\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","f3e75f8c":"df.head()","ec43f3cc":"X = df['cleaned_tweets']\ny = pd.get_dummies(df['sentiment']).values\nnum_classes = df['sentiment'].nunique()","f9420919":"seed = 38 # fix random seed for reproducibility\nnp.random.seed(seed)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.2,\n                                                    stratify=y,\n                                                    random_state=seed)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","74c5bb71":"max_features = 20000\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(X_train))\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)","b2ea94e7":"from tensorflow.keras.preprocessing import sequence\nmax_words = 30\nX_train = sequence.pad_sequences(X_train, maxlen=max_words)\nX_test = sequence.pad_sequences(X_test, maxlen=max_words)\nprint(X_train.shape,X_test.shape)","9beecba6":"import tensorflow.keras.backend as K\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Embedding,Conv1D,MaxPooling1D,LSTM\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n\nbatch_size = 128\nepochs = 10\n\nmax_features = 20000\nembed_dim = 100\n\nnp.random.seed(seed)\nK.clear_session()\nmodel = Sequential()\nmodel.add(Embedding(max_features, embed_dim, input_length=X_train.shape[1]))\nmodel.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))    \nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","00a9b172":"tf.keras.utils.plot_model(model, show_shapes=True)","b86113b9":"history = model.fit(X_train, y_train, validation_data=(X_test, y_test), \n                          epochs=epochs, batch_size=batch_size, verbose=2)","d5daf730":"def plot_training_hist(history):\n    '''Function to plot history for accuracy and loss'''\n    \n    fig, ax = plt.subplots(1,2, figsize=(10,4))\n    # first plot\n    ax[0].plot(history.history['accuracy'])\n    ax[0].plot(history.history['val_accuracy'])\n    ax[0].set_title('Model Accuracy')\n    ax[0].set_xlabel('epoch')\n    ax[0].set_ylabel('accuracy')\n    ax[0].legend(['train', 'validation'], loc='best')\n    \n    # second plot\n    ax[1].plot(history.history['loss'])\n    ax[1].plot(history.history['val_loss'])\n    ax[1].set_title('Model Loss')\n    ax[1].set_xlabel('epoch')\n    ax[1].set_ylabel('loss')\n    ax[1].legend(['train', 'validation'], loc='best')\n    \nplot_training_hist(history)","88758bba":"# predict class with test set\ny_pred_test =  np.argmax(model.predict(X_test), axis=1)\nprint('Accuracy:\\t{:0.1f}%'.format(accuracy_score(np.argmax(y_test,axis=1),y_pred_test)*100))\nprint(classification_report(np.argmax(y_test,axis=1), y_pred_test))","b904f8da":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns \ndef plot_confusion_matrix(model, X_test, y_test):\n    '''Function to plot confusion matrix for the passed model and the data'''\n    \n    sentiment_classes = ['Negative','Neutral', 'Positive']\n    # use model to do the prediction\n    y_pred = model.predict(X_test)\n    # compute confusion matrix\n    cm = confusion_matrix(np.argmax(y_pred, axis=1),np.argmax(np.array(y_test),axis=1))\n    \n    print(pd.Series(np.argmax(np.array(y_test),axis=1)).value_counts())\n    print(pd.Series(np.argmax(y_pred, axis=1)).value_counts())\n    \n    # plot confusion matrix\n    plt.figure(figsize=(8,6))\n    sns.heatmap(cm, cmap=plt.cm.Blues, annot=True, fmt='d', \n                xticklabels=sentiment_classes,\n                yticklabels=sentiment_classes)\n    plt.title('Confusion matrix', fontsize=16)\n    plt.xlabel('Actual label', fontsize=12)\n    plt.ylabel('Predicted label', fontsize=12)\n    \nplot_confusion_matrix(model, X_test, y_test)","74f2d834":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nmax_words = 5000\nmax_len=50\n\ndef tokenize_pad_sequences(text):\n    '''\n    This function tokenize the input text into sequnences of intergers and then\n    pad each sequence to the same length\n    '''\n    # Text tokenization\n    tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n    tokenizer.fit_on_texts(text)\n    # Transforms text to a sequence of integers\n    X = tokenizer.texts_to_sequences(text)\n    # Pad sequences to the same length\n    X = pad_sequences(X, padding='post', maxlen=max_len)\n    # return sequences\n    return X, tokenizer\n\nprint('Before Tokenization & Padding \\n', df['cleaned_tweets'][0])\nX, tokenizer = tokenize_pad_sequences(df['cleaned_tweets'])\nprint('After Tokenization & Padding \\n', X[0])","7160923f":"y = pd.get_dummies(df['sentiment'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\nprint('Train Set: ', X_train.shape, y_train.shape)\nprint('Validation Set: ', X_val.shape, y_val.shape)\nprint('Test Set: ', X_test.shape, y_test.shape)","a14b9858":"import tensorflow.keras.backend as K\n\ndef f1_score(precision, recall):\n    ''' Function to calculate f1 score '''\n    \n    f1_val = 2*(precision*recall)\/(precision+recall+K.epsilon())\n    return f1_val","40f60369":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\nfrom tensorflow.keras.metrics import Precision, Recall\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras import datasets\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.callbacks import History\nfrom tensorflow.keras import losses","225b8461":"vocab_size = 5000\nembedding_size = 32\nepochs = 50\nlearning_rate = 0.01\ndecay_rate = learning_rate \/ epochs\nmomentum = 0.8","cb555c9d":"sgd = SGD(learning_rate=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n# Build model\nmodel= Sequential()\nmodel.add(Embedding(vocab_size, embedding_size, input_length=max_len))\nmodel.add(Conv1D(filters=32, kernel_size=1, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Bidirectional(LSTM(32)))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(3, activation='softmax'))","a6a6e9fc":"tf.keras.utils.plot_model(model, show_shapes=True)","237f4fba":"model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy', Precision(), Recall()])","23c9b97f":"history = model.fit(X_train,y_train,validation_data=(X_val, y_val),batch_size=batch_size,epochs=epochs,verbose=1)","1b54dea0":"# Evaluate model on the test set\nloss, accuracy, precision, recall = model.evaluate(X_test, y_test, verbose=0)\n# Print metrics\nprint('')\nprint('Accuracy  : {:.4f}'.format(accuracy))\nprint('Precision : {:.4f}'.format(precision))\nprint('Recall    : {:.4f}'.format(recall))\nprint('F1 Score  : {:.4f}'.format(f1_score(precision, recall)))","92f2aa18":"plot_training_hist(history)","dd635ed2":"plot_confusion_matrix(model, X_test, y_test)","b6ec0fd0":"## Normalization","30a8fa83":"## Derivative of Crypto price ","5c781505":"## Model preparation ","b772da3f":"## Model2 ","2790196a":"## read Bitcoin price ","1e74076e":"## Calculate a score for each tweet\nTo calculate the score for each tweet, we use different variables to which we had a weight based on its importance.\n\nThe compound column represents the sentiment of the tweets and its value is between -1 and 1.\n\nWe also use the number of retweets, the number of likes, and the number of users that follow the tweet's author.","8f81d0be":"## Sentiment anatweets_clean_fileh Vader\nVADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media.\n\nVADER takes into account\n\nnegations and contractions (not good, wasn\u2019t good)\nPunctuation (good!!!), CAPS, emotes :), emojis\nIntensificators (very, kind of), acronyms \u2018lol\u2019\nScores between -1.0 (negative) and 1.0 (positive)\n\nWe will use this sentiment analysis of the tweets to calculate a score that will represent the importance of each tweet.","dddfd147":"# Part 2 NLP Modeling "}}