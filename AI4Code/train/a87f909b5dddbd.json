{"cell_type":{"5680be84":"code","b8f1d574":"code","4acf3052":"code","fb96bf47":"code","cd5aac74":"code","34d6891f":"code","9d5d00bb":"code","1c93c0fd":"code","932dd56b":"code","4509f43f":"code","cdd49923":"code","65b0f84f":"code","5777d69c":"code","892322de":"code","d6d3cf8f":"code","bcebb819":"code","6fc905c1":"code","b02a3442":"code","df22abba":"code","e651067b":"code","638f9138":"code","03a6853e":"code","740bfc9b":"code","a7d206c4":"code","bbf6906b":"code","4995d326":"code","d97dc2ce":"code","526705ff":"code","7b3b939f":"code","ec98bc3a":"code","a5b8339f":"code","481aea2d":"code","3c91b321":"code","bf7b893c":"code","041d8630":"code","a5ac2eef":"code","fc6931ee":"code","5b8580bc":"code","69ac12dd":"code","c665d488":"code","e688aaeb":"code","539845a2":"code","63ac320b":"code","36120d46":"markdown","ad998a46":"markdown","388d785a":"markdown","21d31696":"markdown","7656e57e":"markdown","0a616906":"markdown","0f9c327f":"markdown","4bb0d1f1":"markdown","e6b25fdf":"markdown","47669d3e":"markdown","90905788":"markdown"},"source":{"5680be84":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n#import numpy as np # linear algebra\n#import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b8f1d574":"import numpy as np\nimport pandas as pd \nimport sklearn\nimport scipy.sparse \nimport lightgbm as lgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pickle\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline \n\npd.set_option('display.max_rows', 600)\npd.set_option('display.max_columns', 50)\nsns.set(rc={'figure.figsize':(20, 10)})","4acf3052":"for p in [np, pd, sklearn, scipy, lgb, sns]:\n    print (p.__name__, p.__version__)","fb96bf47":"from tqdm import tqdm_notebook\nfrom itertools import product\nfrom sklearn.metrics import mean_squared_error\nimport gc\n#Funciones necesarias para el an\u00e1lisis\ndef downcast_dtypes(df):\n    \n    '''\n        Changes column types in the dataframe: \n                \n                `float64` type to `float32`\n                `int64`   type to `int32`\n    '''\n    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    \n    return df\n\n\ndef rmse(*args):\n    \n    \"\"\" Funcion that calculates the root mean squared error\"\"\"\n    return np.sqrt(mean_squared_error(*args))\n\ndef get_feature_matrix(sales, test, items, list_lags, date_block_threshold):\n    \n     \n  \n    # Create \"grid\" with columns\n    index_cols = ['shop_id', 'item_id', 'date_block_num']\n\n    # For every month we create a grid from all shops\/items combinations from that month\n    grid = [] \n    cur_items_aux=np.array([])\n    for block_num in sales['date_block_num'].unique():\n        cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n        cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].append(pd.Series(cur_items_aux)).unique()\n        cur_items_aux = cur_items[pd.Series(cur_items).isin(test.item_id)]\n        grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\n    # Turn the grid into a dataframe\n    grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\n    # Add submission shop_id-item_id in order to test predictions\n    test['date_block_num'] = 34\n    grid = grid.append(test[['shop_id', 'item_id', 'date_block_num']])\n\n    # Groupby data to get shop-item-month aggregates\n    gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':[('target','sum')]})\n    # Fix column names\n    gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n    # Join it to the grid\n    all_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n\n    # Same as above but with shop-month aggregates\n    gb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':[('target_shop','sum')]})\n    gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n    all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n\n    # Same as above but with item-month aggregates\n    gb = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':[('target_item','sum')]})\n    gb.columns = [col[0] if col[-1] == '' else col[-1] for col in gb.columns.values]\n    all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n\n    # Downcast dtypes from 64 to 32 bit to save memory\n    all_data = downcast_dtypes(all_data)\n    del grid, gb \n    gc.collect()\n    # List of columns that we will use to create lags\n    cols_to_rename = list(all_data.columns.difference(index_cols)) \n\n    shift_range = list_lags\n\n    for month_shift in tqdm_notebook(shift_range):\n        train_shift = all_data[index_cols + cols_to_rename].copy()\n    \n        train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n    \n        foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n        train_shift = train_shift.rename(columns=foo)\n\n        all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n\n    del train_shift\n\n    # Don't use old data from year 2013\n    all_data = all_data[all_data['date_block_num'] >= date_block_threshold] \n\n    # List of all lagged features\n    fit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n    # We will drop these at fitting stage\n    to_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n\n    # Category for each item\n    item_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n\n    all_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\n    all_data = downcast_dtypes(all_data)\n    gc.collect();\n    \n    return [all_data, to_drop_cols]\n\n\ndef clip20(x):\n    return np.clip(x, 0, 20)\n\ndef clip40(x):\n    return np.clip(x, 0, 20)","cd5aac74":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\nsales = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\nshops = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv')\nitems = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')\nitem_cats = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\ntest = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv')","34d6891f":"sales.head()","9d5d00bb":"sales.describe()","1c93c0fd":"test.head()","932dd56b":"test.describe()","4509f43f":"print(items.shape)","cdd49923":"print(sales.shape,test.shape)","65b0f84f":"\nsns.set_context(\"talk\", font_scale=1.4)\nsales_month = pd.DataFrame(sales.groupby(['date_block_num']).sum().item_cnt_day).reset_index()\nsales_month.columns = ['date_block_num', 'sum_items_sold']\nsns.barplot(x ='date_block_num', y='sum_items_sold', \n            data=sales_month.reset_index());\nplt.plot(sales_month.sum_items_sold)\nplt.title('Distribution of the sum of sales per month')\ndel sales_month","5777d69c":"sales.isnull().sum()","892322de":"comb_shop_item = pd.DataFrame(sales[['date_block_num', 'shop_id', \n                                     'item_id']].drop_duplicates().groupby('date_block_num').size()).reset_index()\ncomb_shop_item.columns = ['date_block_num', 'item-shop_comb']\nsns.barplot(x ='date_block_num', y='item-shop_comb', data=comb_shop_item);\nplt.plot(comb_shop_item['item-shop_comb']);\nplt.title('Number of combinations shop-it with sales per month')\ndel comb_shop_item","d6d3cf8f":"sns.set_context(\"talk\", font_scale=1)\nsales_month_shop_id = pd.DataFrame(sales.groupby(['shop_id']).sum().item_cnt_day).reset_index()\nsales_month_shop_id.columns = ['shop_id', 'sum_sales']\nsns.barplot(x ='shop_id', y='sum_sales', data=sales_month_shop_id, palette='Paired')\nplt.title('Distribution of sales per shop');\ndel sales_month_shop_id","bcebb819":"\nsns.set_context(\"talk\", font_scale=1.4)\nsales_item_id = pd.DataFrame(sales.groupby(['item_id']).sum().item_cnt_day)\n\nplt.xlabel('item id')\nplt.ylabel('sales')\nplt.plot(sales_item_id);","6fc905c1":"anomaly_item = sales_item_id.item_cnt_day.argmax()\nanomaly_item","b02a3442":"sns.set_context(\"talk\", font_scale=0.8)\nsales_item_cat = sales.merge(items, how='left', on='item_id').groupby('item_category_id').item_cnt_day.sum()\nsns.barplot(x ='item_category_id', y='item_cnt_day',\n            data=sales_item_cat.reset_index(), \n            palette='Paired'\n           );\ndel sales_item_cat","df22abba":"tuples_df = pd.Series(list(sales[['item_id', 'shop_id']].itertuples(index=False, name=None)))\ntuples_test = pd.Series(list(test[['item_id', 'shop_id']].itertuples(index=False, name=None)))\nprint(str(round(tuples_df.isin(tuples_test).sum()\/len(tuples_df),2)*100)+'%')","e651067b":"list_lags = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\ndate_block_threshold = 12\nsales_for_modelling = sales[sales.item_id.isin(test.item_id)]\n[all_data, to_drop_cols]  = get_feature_matrix(sales_for_modelling, test, items, list_lags, date_block_threshold)","638f9138":"all_data.head()","03a6853e":"sns.set_context(\"talk\", font_scale=1.4)\nplt.title('Number of different shop-item combinations in data per month')\nall_data.groupby('date_block_num').size().plot();","740bfc9b":"mean_enc_item_cat = pd.DataFrame(all_data.groupby(['shop_id', \n                                                    'item_category_id']).target.agg(['mean', 'var']).reset_index())\nmean_enc_item_cat.columns = ['shop_id', 'item_category_id', 'mean_enc_cat_id', 'var_enc_cat_id']\nall_data = pd.merge(all_data, mean_enc_item_cat, how='left', on=['shop_id', 'item_category_id'])\ndel mean_enc_item_cat\nall_data = downcast_dtypes(all_data)","a7d206c4":"sub_data = all_data[all_data.date_block_num==34].fillna(0)\nall_data = all_data[all_data.date_block_num<34].fillna(0)\nsub_data.head()","bbf6906b":"dates = all_data['date_block_num']\nboolean_test = (dates.isin([22,31,32,33])) # & (boolean)\nboolean_train = ~boolean_test\ndates_train = dates[boolean_train]\ndates_val  = dates[boolean_test]\n\n\nX_train = all_data.loc[boolean_train].drop(to_drop_cols, axis=1)\nX_test =  all_data.loc[boolean_test].drop(to_drop_cols, axis=1)\n\ny_train = all_data.loc[boolean_train, 'target'].values\ny_test =  all_data.loc[boolean_test, 'target'].values\n\n","4995d326":"print('X_train shape is ' + str(X_train.shape))\nprint('X_test shape is ' + str(X_test.shape))\nprint('y_train shape is'+ str(y_train.shape))\nprint('y_test shape is'+ str(y_test.shape))","d97dc2ce":"print(f'Cross-validation is the {round(X_test.shape[0]\/X_train.shape[0],2)*100} %' )","526705ff":"\ntuples_validation_submission = pd.Series(list(X_test[['item_id', 'shop_id']][dates_val==33].itertuples(index=False, name=None)))\nprint(f'The {round(tuples_test.isin(tuples_validation_submission).sum()\/len(tuples_test),2)*100} % of the item_id-shop_id are in the cv set ')\n","7b3b939f":"#lr\nlr = LinearRegression()\nlr.fit(X_train.values, y_train)\npred_lr = lr.predict(X_test.values)\nprint(pred_lr)\nprint('Test R-squared for linreg is %f' % r2_score(y_test, pred_lr))","ec98bc3a":"lgb_params = {\n               'feature_fraction': 0.75,\n               'metric': 'rmse',\n               'nthread':1, \n               'min_data_in_leaf': 2**7, \n               'bagging_fraction': 0.75, \n               'learning_rate': 0.03, \n               'objective': 'mse', \n               'bagging_seed': 2**7, \n               'num_leaves': 2**7,\n               'bagging_freq':1,\n               'verbose':0 \n              }\n\nmodel = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), 100)\npred_lgb = model.predict(X_test)\n\nprint('Test R-squared for LightGBM is %f' % r2_score(y_test, pred_lgb))","a5b8339f":"rfr = RandomForestRegressor(n_estimators=50,min_samples_split=20,max_depth=8,verbose=2)\nrfr.fit(X_train, y_train)\npred_rfr = rfr.predict(X_test).clip(0,20)\nprint('Test R-squared for random forest is %f' % r2_score(y_test, pred_rfr))\nprint('Test rmse for random forest is %f' % mean_squared_error(y_test, pred_rfr,squared=False))\n","481aea2d":"X_test_level2 = np.c_[pred_lr, pred_lgb] \nX_test_level2","3c91b321":"dates_train_level2 = dates_train[dates_train.isin([27, 28, 29, 30, 31])]\n\n# That is how we get target for the 2nd level dataset\ny_train_level2 = y_train[dates_train.isin([27, 28, 29, 30, 31])]\n\nX_train_level2 = np.zeros([y_train_level2.shape[0], 2])\n\nfor cur_block_num in [27, 28, 29, 30, 31]:\n    \n    print(cur_block_num)\n    \n    '''\n        1. Split `X_train` into parts\n           Remember, that corresponding dates are stored in `dates_train` \n        2. Fit linear regression \n        3. Fit LightGBM and put predictions          \n        4. Store predictions from 2. and 3. in the right place of `X_train_level2`. \n           You can use `dates_train_level2` for it\n           Make sure the order of the meta-features is the same as in `X_test_level2`\n    '''      \n    \n    #  YOUR CODE GOES HERE\n    train,train_y = X_train[dates_train < cur_block_num], y_train[dates_train < cur_block_num]\n    lr.fit(train.values, train_y)\n    model = lgb.train(lgb_params, lgb.Dataset(train, label= train_y), 100)\n    \n    test1 = X_train[dates == cur_block_num]\n    \n    pred_lr = lr.predict(test1)\n    pred_gb = model.predict(test1)\n    \n    X_train_level2[dates_train_level2 == cur_block_num, :] = np.c_[pred_lr,pred_gb]\n    ","bf7b893c":"plt.scatter(X_train_level2[:,0],X_train_level2[:,1])","041d8630":"alphas_to_try = np.linspace(0, 1, 1001)\n\n\nbest_alpha = 0 \nr2_train_simple_mix = 0 \nmax_score = 0\nfor alpha in alphas_to_try:\n    mix = alpha * X_train_level2[:,0] + (1-alpha) * X_train_level2[:,1]\n    r2 = r2_score(y_train_level2,mix)\n    if r2 > r2_train_simple_mix:\n        r2_train_simple_mix = r2\n        best_alpha = alpha\n\nprint('Best alpha: %f; Corresponding r2 score on train: %f' % (best_alpha, r2_train_simple_mix))","a5ac2eef":"item_id_target_mean = all_data.groupby('item_id').target.mean()\n\n# In our non-regularized case we just *map* the computed means to the `item_id`'s\nall_data['item_target_enc'] = all_data['item_id'].map(item_id_target_mean)\n\n# Fill NaNs\nall_data['item_target_enc'].fillna(0.3343, inplace=True) \n\nencoded_feature = all_data['item_target_enc'].values\nprint(np.corrcoef(all_data['target'].values, encoded_feature)[0][1])","fc6931ee":"cumsum = all_data.groupby('item_id')['target'].cumsum() - all_data['target']\ncumcnt = all_data.groupby('item_id').cumcount()\n\nall_data['item_target_enc'] = cumsum\/cumcnt\nall_data['item_target_enc'].fillna(0.3343, inplace=True)\nencoded_feature = all_data['item_target_enc'].values\n\ncorr = np.corrcoef(all_data['target'].values, encoded_feature)[0][1]\nprint(corr)\n","5b8580bc":"print(X_train_level2)","69ac12dd":"meta_model = LinearRegression()\nmeta_model.fit(X_train_level2, y_train_level2)","c665d488":"train_preds = meta_model.predict(X_train_level2)\nr2_train_stacking = r2_score(y_train_level2, train_preds) \n\ntest_preds = meta_model.predict(X_test_level2)\ntest_preds2 = clip20(meta_model.predict(X_test_level2))\n\nr2_test_stacking = r2_score(y_test, test_preds) \n\nprint('Train R-squared for stacking is %f' % r2_train_stacking)\nprint('Test  R-squared for stacking is %f' % r2_test_stacking)","e688aaeb":"test_preds2.count()","539845a2":"\npredictions = pd.DataFrame()\n\npredictions['shop_id'] = test.shop_id\npredictions['item_id'] = test.item_id\npredictions['item_cnt_month'] = pd.Series(test_preds2)\nsubmision = test[['ID', 'shop_id', 'item_id']].merge(predictions, on=['shop_id', 'item_id'], how='left').fillna(0)\nsubmision[['ID', 'item_cnt_month']].to_csv('submission.csv',index=False)","63ac320b":"salida = pd.Series(test_preds)\nsalida2 = pd.Series(test_preds2)\nsalida.to_csv('salida.csv')\nsalida2.to_csv('salida2.csv')","36120d46":"# LightGBM","ad998a46":"Sales Item Cat","388d785a":"Mean encodings without regularization","21d31696":"# Linear Regression","7656e57e":"Advanced Feature Engineering","0a616906":"# Random Forest","0f9c327f":"# Stacking","4bb0d1f1":"# Ensembling","e6b25fdf":"# Train\/test split","47669d3e":"# EDA","90905788":"# Expanding mean scheme"}}