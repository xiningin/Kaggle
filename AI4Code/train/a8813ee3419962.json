{"cell_type":{"8981ba90":"code","2f2f84fe":"code","e2fcd734":"code","2bbe426b":"code","b3cdba02":"code","80749a17":"code","8084c8f6":"code","22e0d33a":"code","74a00d0c":"code","8600fa59":"code","e9777781":"code","7a058526":"code","8c0d85b3":"code","2a5c5a6a":"code","0fbde636":"code","e46d23b5":"code","3f2ae051":"code","db0f317e":"code","613dbcfc":"code","9a82514c":"code","14774938":"code","f7d18d83":"code","883cabdb":"markdown","854cd1db":"markdown","3daa6460":"markdown","e1712b81":"markdown","b7d0b4b2":"markdown","8d4a6dde":"markdown","591215d9":"markdown","d076c042":"markdown","d7bd6d28":"markdown","e142e856":"markdown","b20a98e0":"markdown","c3970f6c":"markdown"},"source":{"8981ba90":"import numpy as np\nimport pandas as pd\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import cross_val_score\n\nimport xgboost as xgb\nfrom hyperopt import hp, tpe, fmin\n\n\npd.set_option('display.max_columns', None)","2f2f84fe":"traindf=pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntestdf=pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","e2fcd734":"traindf.info()","2bbe426b":"print(traindf.duplicated().sum())\nprint(testdf.duplicated().sum())","b3cdba02":"list_of_numerics=traindf.select_dtypes(include=['float','int']).columns\ntypes= traindf.dtypes\nmissing= round((traindf.isnull().sum()\/traindf.shape[0]),3)*100\noverview= traindf.apply(lambda x: [round(x.min()), \n                                 round(x.max()), \n                                 round(x.mean()), \n                                 round(x.quantile(0.5))] if x.name in list_of_numerics else x.unique())\n\noutliers= traindf.apply(lambda x: sum(\n                                 (x<(x.quantile(0.25)-1.5*(x.quantile(0.75)-x.quantile(0.25))))|\n                                 (x>(x.quantile(0.75)+1.5*(x.quantile(0.75)-x.quantile(0.25))))\n                                 if x.name in list_of_numerics else ''))\n\n\nexplo = pd.DataFrame({'Types': types,\n                      'Missing%': missing,\n                      'Overview': overview,\n                      'Outliers': outliers}).sort_values(by=['Missing%','Types'],ascending=False)\nexplo.transpose()","80749a17":"for col in ('Alley','Utilities','MasVnrType','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1',\n            'BsmtFinType2','FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond',\n           'PoolQC','Fence','MiscFeature'):\n    traindf[col]=traindf[col].fillna('None')\n    testdf[col]=testdf[col].fillna('None')","8084c8f6":"for col in ('Electrical','MSZoning','Exterior1st','Exterior2nd','KitchenQual','SaleType','Functional'):\n    traindf[col]=traindf[col].fillna(traindf[col].mode()[0])\n    testdf[col]=testdf[col].fillna(traindf[col].mode()[0])","22e0d33a":"for col in ('MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath',\n            'GarageYrBlt','GarageCars','GarageArea'):\n    traindf[col]=traindf[col].fillna(0)\n    testdf[col]=testdf[col].fillna(0)","74a00d0c":"traindf['LotFrontage']=traindf['LotFrontage'].fillna(traindf['LotFrontage'].mean())\ntestdf['LotFrontage']=testdf['LotFrontage'].fillna(traindf['LotFrontage'].mean())","8600fa59":"print(traindf.isnull().sum().sum())","e9777781":"fig, axes = plt.subplots(1,2, figsize=(12,5))\n\nax1= sns.scatterplot(x='GrLivArea', y='SalePrice', data= traindf,ax=axes[0])\nax2= sns.boxplot(x='GrLivArea', data= traindf,ax=axes[1])","7a058526":"#removing outliers recomended by author\ntraindf= traindf[traindf['GrLivArea']<4000]","8c0d85b3":"plt.figure(figsize=[12,14])\nfeatures=['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond',\n       'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n       'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n       'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\n       'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal',\n       'MoSold', 'YrSold', 'SalePrice']\nn=1\nfor f in features:\n    plt.subplot(10,4,n)\n    sns.distplot(traindf[f], kde=False)\n    sns.despine()\n    n=n+1\nplt.tight_layout()\nplt.show()","2a5c5a6a":"len_traindf=traindf.shape[0]\nhouses= pd.concat([traindf, testdf], sort=False)\n\n# turning some ordered categorical variables into ordered numerical\n# maybe this information about order can help on performance\nfor col in [\"ExterQual\", \"ExterCond\", \"BsmtQual\", \"BsmtCond\", \"HeatingQC\", \"KitchenQual\",\n            \"FireplaceQu\",\"GarageQual\",\"GarageCond\",\"PoolQC\"]:\n    houses[col]= houses[col].map({\"Gd\": 4 , \"TA\": 3, \"Ex\": 5, \"Fa\":2, \"Po\":1})\n\n\n# turning categoric into numeric\nhouses= pd.get_dummies(houses)\n\n# separating\ntraindf= houses[:len_traindf]\ntestdf= houses[len_traindf:]","0fbde636":"# x\/y split\nxtrain= traindf.drop('SalePrice', axis=1)\nytrain= traindf['SalePrice']\nxtest= testdf.drop('SalePrice', axis=1)","e46d23b5":"#ytrain = np.log(ytrain)","3f2ae051":"space = {'n_estimators':hp.quniform('n_estimators', 1000, 4000, 100),\n         'gamma':hp.uniform('gamma', 0.01, 0.05),\n         'learning_rate':hp.uniform('learning_rate', 0.00001, 0.025),\n         'max_depth':hp.quniform('max_depth', 3,7,1),\n         'subsample':hp.uniform('subsample', 0.60, 0.95),\n         'colsample_bytree':hp.uniform('colsample_bytree', 0.60, 0.98),\n         'colsample_bylevel':hp.uniform('colsample_bylevel', 0.60, 0.98),\n         'reg_lambda': hp.uniform('reg_lambda', 1, 20)\n        }\n\ndef objective(params):\n    params = {'n_estimators': int(params['n_estimators']),\n             'gamma': params['gamma'],\n             'learning_rate': params['learning_rate'],\n             'max_depth': int(params['max_depth']),\n             'subsample': params['subsample'],\n             'colsample_bytree': params['colsample_bytree'],\n             'colsample_bylevel': params['colsample_bylevel'],\n             'reg_lambda': params['reg_lambda']}\n    \n    xb_a= xgb.XGBRegressor(**params)\n    score = cross_val_score(xb_a, xtrain, ytrain, scoring='neg_mean_squared_error', cv=5, n_jobs=-1).mean()\n    return -score\n","db0f317e":"best = fmin(fn= objective, space= space, max_evals=20, rstate=np.random.RandomState(1), algo=tpe.suggest)","613dbcfc":"print(best)","9a82514c":"xb_b = xgb.XGBRegressor(random_state=0,\n                        n_estimators=int(best['n_estimators']), \n                        colsample_bytree= best['colsample_bytree'],\n                        gamma= best['gamma'],\n                        learning_rate= best['learning_rate'],\n                        max_depth= int(best['max_depth']),\n                        subsample= best['subsample'],\n                        colsample_bylevel= best['colsample_bylevel'],\n                        reg_lambda= best['reg_lambda']\n                       )\n\nxb_b.fit(xtrain, ytrain)","14774938":"#prediction\npreds= xb_b.predict(xtest)\n#preds2= np.exp(preds)","f7d18d83":"#output\noutput = pd.DataFrame({'Id': testdf.Id,'SalePrice': preds})\noutput.to_csv('submission.csv', index=False)","883cabdb":"Depending on the categorical variable, missing value can means \"None\" (which I will fill with \"None\") or \"Not Available\" (which I will fill with the mode).  \nDepending on the numeric variable, missing value can means 0 (which I will fill with 0) or \"Not Available\" (which I will fill with the mean).","854cd1db":"### Feature engineering","3daa6460":"# 2- Preprocessing","e1712b81":"### Missing Values","b7d0b4b2":"# House Prices: on the top with simple model","8d4a6dde":"### Exploring","591215d9":"# 1- Knowing the dataset","d076c042":"# 3- Model","d7bd6d28":"Although IQR method suggest several outliers, for now, I'm going to focus on outliers with remotion recommended by the dataset author.","e142e856":"### Plots","b20a98e0":"### xgboost + optimization","c3970f6c":"### Outliers"}}