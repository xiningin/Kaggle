{"cell_type":{"da7b6903":"code","955b22e1":"code","dbbf6c4f":"code","1fb44671":"code","384c5c77":"code","329ccbe3":"code","1333f694":"code","b72ad56d":"code","0095482a":"code","1c4171af":"code","99255399":"code","761345cd":"code","58878b84":"code","b89b7e10":"code","8a054b08":"code","ef793015":"code","a42d75fc":"code","e5a0ee6f":"code","9beecd16":"code","68d838bf":"markdown","21b7caf0":"markdown","e04b5db8":"markdown","3017058c":"markdown","d0c99a4a":"markdown","bd237b65":"markdown","07100a1c":"markdown","623e41e5":"markdown","55ad3e96":"markdown","376eabdc":"markdown","9617c04b":"markdown","be8f865b":"markdown","862d9a1e":"markdown","d052828e":"markdown"},"source":{"da7b6903":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport tensorflow as tf\nimport tensorflow_datasets as tfds","955b22e1":"de_credit, info = tfds.load('german_credit_numeric', split='train', with_info=True)\nde_credit_df = tfds.as_dataframe(de_credit, info)\n\nde_credit_df.head()","dbbf6c4f":"df = de_credit_df.features.apply(pd.Series)\ndf.columns = [f\"f_{i}\" for i in range(df.shape[1])]\n\ndf_data = pd.concat([df, de_credit_df.label], axis=1)\n\nprint(df_data.shape)\ndf_data.head()","1fb44671":"info","384c5c77":"train_len = int(df_data.shape[0] * 0.75)\n\ndf_train = df_data[:train_len]\ndf_val = df_data[train_len:]\n\nX_train = df_train.drop('label', axis=1)\nY_train = df_train.label\n\n\nX_val = df_val.drop('label', axis=1)\nY_val = df_val.label\n\n\nprint(f\"X Train: \", X_train.shape)\nprint(f\"Y Train: \", Y_train.shape)\nprint(f\"X Val: \", X_val.shape)\nprint(f\"Y Val: \", Y_val.shape)","329ccbe3":"def dnn_model(epochs, callbacks=None):\n    \n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.Dense(1, activation='sigmoid')])\n\n    model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])\n\n    history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=epochs, callbacks=callbacks)\n    \n    return history, model\n\nhist, model = dnn_model(250)","1333f694":"print(f\"Training Set:   {model.evaluate(X_train, Y_train)}\")\nprint(f\"Validation Set: {model.evaluate(X_val, Y_val)}\")","b72ad56d":"def plot(history):\n    fig, axs = plt.subplots(1,2, figsize=(12, 5))\n\n    hist = history.history\n\n    for ax, metric in zip(axs, [\"loss\", \"accuracy\"]):\n        ax.plot(hist[metric])\n        ax.plot(hist[\"val_\"+metric])\n        ax.legend([metric, \"val_\" + metric])\n        ax.set_title(metric)\n\nplot(hist)","0095482a":"#look at the predictions\npreds_classes = model.predict_classes(X_val)\npd.Series(preds_classes.flatten()).value_counts().plot(kind='bar', title=\"Count of Inital Predicted Classes\")","1c4171af":"#look at the original data\nX_train.describe()","99255399":"# balance dataset\nros = RandomOverSampler(random_state=42)\n\nx_balance, y_balance = ros.fit_resample(df_data.drop(\"label\", axis=1), df_data.label.values)\ndf_balance = pd.concat([x_balance, pd.DataFrame(y_balance, columns= ['label'])], axis=1)","761345cd":"mm = MinMaxScaler()\ndf_scaled = pd.DataFrame(mm.fit_transform(df_balance), columns = df_balance.columns)\n\nprint(\"Shape after scaling and balancing: \", df_scaled.shape)","58878b84":"train_len = int(df_scaled.shape[0] * 0.75)\n\ndf_sample = df_scaled.sample(frac=1)\n\ndf_train = df_sample[:train_len]\ndf_val = df_sample[train_len:]\n\nX_train = df_train.drop('label', axis=1)\nY_train = df_train.label\n\n\nX_val = df_val.drop('label', axis=1)\nY_val = df_val.label\n\n\nprint(f\"X Train: \", X_train.shape)\nprint(f\"Y Train: \", Y_train.shape)\nprint(f\"X Val: \", X_val.shape)\nprint(f\"Y Val: \", Y_val.shape)","b89b7e10":"fig, ax = plt.subplots(1,2, figsize=(12,3))\ndf_train['label'].value_counts().plot(kind='bar', title=\"Count target label: Training\", ax=ax[0])\n\ndf_val['label'].value_counts().plot(kind='bar', title=\"Count target label: Validation\", ax=ax[1])","8a054b08":"class EarlyStoppingCallback(tf.keras.callbacks.Callback):\n\n    def on_epoch_end(self,epoch, logs=None):\n        if logs['accuracy'] >0.90:\n            print(\"Accuracy greater than 90%. Stopping Training.\")\n            self.model.stop_training=True","ef793015":"def dnn_model(epochs, callbacks=None):\n    \n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Dense(128, input_shape=(X_train.shape[1],), activation='relu'),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(1, activation='sigmoid')])\n\n    model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])\n\n    history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=epochs, callbacks=callbacks)\n    \n    return history, model\n\nhist_regularized, model_regularized = dnn_model(500, callbacks=[EarlyStoppingCallback()])","a42d75fc":"plot(hist_regularized)","e5a0ee6f":"print(f\"Training Set:   {model_regularized.evaluate(X_train, Y_train)}\")\nprint(f\"Validation Set: {model_regularized.evaluate(X_val, Y_val)}\")","9beecd16":"preds_classes = model_regularized.predict_classes(X_val)\npd.Series(preds_classes.flatten()).value_counts().plot(kind='bar', title=\"Count of Regularized Predicted Classes\")","68d838bf":"<a id=\"settingup-first-model\"><\/a>\n# Setting Up Our First Model\n\nTensorFlow makes it super easy to setup a neural network. In this case we will setup a simple dense nural network classifier using the sequential api.\n\nWhen building a model it is essential to understand the shape of your inputs and outputs. This allows you to build the network with teh right configuration to accept the data, and give the correct output type. In this case we have a classifiation problem with a single predicted class -  i.e. if an attribute is a set of good or bad credit risks. In other words we have a binary classification task. Below is a simple guide to help understand your outputs.\n\n<a id=\"inputs-outputs\"><\/a>\n### Inputs\nInput shapes depend on the type of problem and network architecture. Input shape can be defined in the first layer of the network either calling the `input_shape` parameter or using the `tf.keras.layers.Input` class.\n\n| Data Type | Input Shape |\n| --- | --- |\n| Image | (image height, image width, number of channels) |\n| Sequence | (number of sequence steps, number of features) |\n| Structured |  |\n\n\n\n\n### Outputs\n| Problem Type | Output Neurons | Target Format | Loss Type | Last Neuron Activation |\n| --- | --- | --- | --- | --- |\n| Binary Classification | 1 | Binary | binary_crossentropy |sigmoid|\n| Multi Classification | Number of classes | One-Hot Encoded | categorical_crossentropy |softmax|\n| Multi Classification | Number of classes | Label Encoded | sparse_categorical_crossentropy|softmax|\n| Regression | Number of predictions | Numeric | Any regression metric: MSE\/RMSE\/MSLE\/Huber |None|\n\n\n<a id=\"sequential-api\"><\/a>\n## Sequential API\nThe [sequential model](https:\/\/www.tensorflow.org\/guide\/keras\/sequential_model) is designed to be used with a linear stack of layers with one input and one output. This is good for stright-forward models where each layer is an element in a sequential list. The model stack can be initalized with a list contining the different layers or using the `model.add` method. \n\nFor multiple inputs and outputs [reference the functional api](https:\/\/www.tensorflow.org\/guide\/keras\/functional)\n\n\nThe compile() method: specifying a loss, metrics, and an optimizer\nTo train a model with fit(), you need to specify a loss function, an optimizer, and optionally, some metrics to monitor.\n\nYou pass these to the model as arguments to the compile() method:\n\n\n\n","21b7caf0":"### Loss Plot\nThe loss plot is characteristc overfitting when we see the training loss (blue) continuing to decline while the validation loss is flat then rising. In this case we see that training for fewer epochs will reduce the overfitting.\n\nThe validation plot is also not smooth and this is another indication that overfitting could be occuring. However it can also be an indication that the model is not learning well and so the predictions are noisy.\n\n### Accuracy Plot\nAccuracy plot is also showing classic overfitting bias between train and validation. Notice how the train accruacy rises while validation accuracy is constant. There are some classic data problems that can cause this including:\n- A data imblance resulting in the classifier only predicting one of the two classes. \n- Not enough training and validation data\n- Not training long enoug\n\nBy using the results from both the loss and the accuracy plots we know that we'll need to investigate our initial predictions, the class balances in our training data, and the scalaing of the data. ","e04b5db8":"<a id=\"model-evaluation\"><\/a>\n## Evaluate our Model\n\nWe can evaluate our model by calling the `.evaluate()` method on the fit model object. By default `evaluate()` has verbose output and will return a tuple of the final loss and accuracy of the evaluation set. We can use it to evaluate the model's performance on the training and validation set.","3017058c":"<a id=\"overfitting\"><\/a>\n## Have We overfit?\n\nSo wait what happened? Our training accruacy was 91% after 100 epochs of training but our validation accruacy was only 74%? Is this oferfitting? Or is it something else? The answer: a kind of overfitting on imbalanced data!\n\nOverfitting occurs when the model's predictions become highly variant. That is we see large variations between predictions in an effort to fit closer to the training set. The opposite can also occur, underfitting where the predictions do not generalize effectively. \n\nThere are a few methods to recognize if the model overfit the training data.\n- Training loss declines while validation loss is constant or rises.\n- A large gap between the training accuracy\/ROC AUC\/etc and the validation.\n- Validation score does not change while validation loss declines.\n\n<a id=\"plotting\"><\/a>\n## Plotting\n\nTo get an idea of our model's ability to generalize we plot the loss and accuracy for the training and validation sets. This can be accessed with the history dictionary object returned by `model.fit()`. The history dictionary holds the loss, and metrics values per epoch for each of the training and validation results.","d0c99a4a":"# Getting Started With Tensorflow\n\nThis notebook will help you if you have never used Tensroflow before. You'll learn how to download and use tensorflow datasets, how to setup a simple dense neural network model, and how to evaluate the model's performance. This notebook is part of a series of notebooks in this [github repo](https:\/\/github.com\/nicholasjhana\/tensorflow-certification-study-guide) that help prepare for the Tensorflow Certification. Feel free to check out the other notebooks too!\n\n1. [Tensorflow: Guide to Getting Started](https:\/\/www.kaggle.com\/nicholasjhana\/tensorflow-getting-started\/edit\/run\/46189040)\n2. [TensorFlow: Multi-Class Image Classification With Transfer Learning](https:\/\/www.kaggle.com\/nicholasjhana\/tensorflow-multi-classification-transfer-learning)\n3. Tensorflow: NLP (Coming Soon!)\n4. [Multi-Variate Time Series Forecasting Tensorflow](https:\/\/www.kaggle.com\/nicholasjhana\/multi-variate-time-series-forecasting-tensorflow)\n\n\n## TensorFlow Concepts\nThis notebook covers the following topics from the tensorflow certification handbook:\n- Building, compiling, training, evaluating models for binary classification\n- Identifying and mitigating overfitting\n- Plotting loss and accuracy\n- Matching input and output shapes\n- Using early stopping callbacks\n- Using datasets from tensorflow datasets\n\n## Aknowledgemnts and References\n- [Coursera: TensorFlow In Practice](https:\/\/www.coursera.org\/professional-certificates\/tensorflow-in-practice)\n- [Deep Learning Design Patterns Primer - Andrew Ferlitsch](https:\/\/github.com\/GoogleCloudPlatform\/keras-idiomatic-programmer\/blob\/master\/books\/deep-learning-design-patterns\/Deep%20Learning%20Design%20Patterns%20Primer.pdf)\n- [Weight Decay and L2 Regularization](https:\/\/towardsdatascience.com\/weight-decay-l2-regularization-90a9e17713cd)\n- [Hands On Machine Learning with SciKit-Learn and Tensorflow - Aur\u00e9lien G\u00e9ron](https:\/\/www.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/)\n- [7 Simple Techniques to Prevent Overfitting](https:\/\/www.kaggle.com\/learn-forum\/157623)\n\n## Contents\n1. [TensorFlow Datasets](#tensorflow-datasets)\n    - [Prepare the dataset](#prepare-dataset)\n2. [Setting Up The First Model](#settingup-first-model)\n    - [The Sequential API](#sequential-api)\n        - [Inputs\/Outputs](#inputs-outputs)\n    - [Model Evaluation](#model-evaluation)\n    - [Have We Overfit?](#overfitting)\n    - [Plotting Results](#plotting)\n3. [Standard Preprocessing](#preprocessing)\n4. [Types Of Regularization](#regularization)\n    - [Using Early Stopping](#early-stopping)\n    - [Model With Regularization](#model-regularization)\n        - [Callbacks](#callbacks)\n        - [Evaluation...Again](#evaluation)\n5. [Summary](#summary)","bd237b65":"## Evaluation\n\nIn the above model we only trained until 90% accuracy on the training set. We see that doing this, the validation loss had already started to flatten. Similarly the validation accuracy was also impoving slowly. So we have avoided overfitting, but our model still has some improvement needed. Ideally we'd like to see validation loss to keep dropping with training loss, and for validation accuracy to keep improving with training accruacy.\n\nBelow we see the summary of performance of our model.\n1. Train and validation accuracy were 94% and 81% respectively.\n2. The model is favoring predictions to class 0 over class 1. We know this because above we saw that the true values in the validation set were roughly balanced.","07100a1c":"<a id=\"regularization\"><\/a>\n# Regularization\nNow that we have added some standard ML paractices to the data to make it more standardized for the feed forward network lets also add some regularization methods to the network. regularization helps the model generalize better by making training more \"difficult\". Of course its more complicated than that, but for the purposeds of learning the TensroFlow API we just need to know how it all fits together. Some regularization methods are:\n\n- Dropout between layers: Where not all neurons connect to the next layer. The neuron connections are dropped randomly between epochs. This helps the network generalize because groups of neurons are not able to rely on those neurons around them. This method can be implemented using the `tf.keras.layers.Dropout()` module.\n\n![inbox_4552206_8ed242b547501803019837091669a48c_dropout.png](attachment:inbox_4552206_8ed242b547501803019837091669a48c_dropout.png)\n[Source](https:\/\/www.kaggle.com\/learn-forum\/157623)\n\n- L1\/L2 regularization aka Weight Decay regularization: This is a bias penalty term that is added to the weights that prevents them from becoming too large! We can add eright regularization with tensroflow with the `tf.keras.regularizers.l2()` and to pass it to the `kernel_regularizer` argument of the layer. \n\n![inbox_4552206_a2dcf26f540e6d4c1cb34caec5aeb1c2_l1l2.png](attachment:inbox_4552206_a2dcf26f540e6d4c1cb34caec5aeb1c2_l1l2.png)\n[Source](https:\/\/www.kaggle.com\/learn-forum\/157623)\n\n- Early stopping: This technique relies on training for a large number of epochs and looking to see at which point the validation loss cruve begins to rise. We can then set a criteria manually, or with code on how and when to stop training.\n\n![inbox_4552206_79463f0aed845fa14ab0d3e8325d0226_early.png](attachment:inbox_4552206_79463f0aed845fa14ab0d3e8325d0226_early.png)\n[Source](https:\/\/www.kaggle.com\/learn-forum\/157623)\n\nIf you want to learn more about regularization methods, [this article](https:\/\/towardsdatascience.com\/weight-decay-l2-regularization-90a9e17713cd) is great.","623e41e5":"<a id=\"early-stopping\"><\/a>\n## Early Stopping\nAs mentioned above the premise of Early Stopping is to stop training before overfitting occurs. The benefit of the technique is that you can save yourself from training for much longer than is needed for the model to learn, thereby also saving compute. At the same time it is hard to tell \"when it is good enough to stop\". This is because early stopping relies on looking at the loss or other metrics to determine if training should continue and this is not always so clearly defined.\n\nWe can implement early stopping by inheriting from the `tf.keras.callbacks.Callback` class and defining criteria within the `on_epoch_end` or `on_batch_end`. Alternatively we can also invoke the [`tf.keras.callbacks.EarlyStopping` callback](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\/EarlyStopping).","55ad3e96":"<a id=\"model-regularization\"><\/a>\n## Model with Regularization\nLets apply dropout, L2, and early stopping regularization to the model and train with the standardized (balanced and normalized) datasets. While in practice you might want to determine the best kind of regularization for your specific usecase, below we will demonstrate their effect on our simple classification problem.\n\n<a id=\"callbacks\"><\/a>\n### Callbacks\nWe use callbacks to invoke early stopping, but what's a callback? Callbacks are code that is run at a given point in the training process. Callbacks can be invoked for the start of training, end of training, start of epoch or batch, end of epoch of batch, or any other point in the model pipeline. They can be introduced in the `model.fit` with the `callback` keyword.","376eabdc":"<a id=\"tensorflow-datasets\"><\/a>\n## Tensorflow Datasets\nTensorflow datasets is an API that lets us download and use many prepared ML datasets. Its make to make using the datasets simple, and includes and extendes the datasets from [keras dataset API](https:\/\/keras.io\/api\/datasets\/). The [Tensorflow datasets catalog](https:\/\/www.tensorflow.org\/datasets\/catalog\/overview) has datasets for audio, image (classification and detection), question answering, structured, text, translation and video.  \n\nTensorflow datasets uses the tfrecord format to store data. Datasets can be loaded  with `tfds.load()` function. Data is accessed as a dictionary if no `split` parameter is called or directly as a prefetched tensorflow datasets object if `split` is passed.[See more about using the split function](https:\/\/www.tensorflow.org\/datasets\/splits). Note that for the exam is it assumed you are familiar with using the `tf.Datatsets` format.\n\n\n\nThere are a couple of methods to look at the data. First is to use the `as_dataframe()` method. Second is to iterate over the prefretched data generator object.","9617c04b":"<a id=\"prepare-dataset\"><\/a>\n### Prepare The Dataset\n\nWe will split the data into train and test, and then setup up our first model. \n","be8f865b":"<a id=\"preprocessing\"><\/a>\n## Preprocessing\nThe focus here is on using the TensorFlow API but folloing our first model attempt we see that we still need to apply some common Machine Learning practices to the data. In this case \n\n1. Balance the dataset with random over sampling\n2. Scale the dataset with min max scaling \n3. Apply cross validation","862d9a1e":"<a id=\"summary\"><\/a>\n# Summary\nThis notebook was an introduction to using the TensroFlow API for binary classification with a structured learning problem. It covered how to download data from TensroFlow Datasets, how the Sequential API works, how to apply differnt kinds of regularization, and how to use callbacks.\n\nI hope it helps with your journey to the TensorFlow certification! If oyu have any feedback or comments for improvement please leave them below. And of course if you enjoyed this work please upvote! Thank you!","d052828e":"### Initial predictions\nIt might seem basic, but looking at your predictions can be a simple way to see how the classifer is predicting. Below we notice two tings:\n1. We are predicting for both classes - Good!\n2. The underlying data is probably imbalanced.\n3. Data is not scaled"}}