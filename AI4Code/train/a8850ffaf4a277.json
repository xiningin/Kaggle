{"cell_type":{"65695283":"code","4fe3c90c":"code","dba37838":"code","28e5d5f4":"code","ace2d872":"code","1c927093":"code","33604890":"code","32e2dbb4":"code","441ec00c":"code","0e4695b3":"code","eb953f35":"code","e2896dc1":"code","ddeb3aa6":"markdown","06a5b959":"markdown","984bec8e":"markdown","c21c91d2":"markdown","40af7dd1":"markdown","7c74ca68":"markdown","d7a265ae":"markdown"},"source":{"65695283":"import gc\nimport re\nimport sys\nimport csv\nimport codecs\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport gensim.models.keyedvectors as word2vec\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, Bidirectional, GlobalMaxPool1D,Bidirectional\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\nfrom tqdm.notebook import tqdm\nfrom tqdm import tqdm_notebook\n\nimport warnings\nwarnings.simplefilter('ignore')","4fe3c90c":"# TPU Config\n# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","dba37838":"def get_data():\n    train = pd.read_csv(\"..\/input\/steam-game-reviews\/train.csv\")\n    test = pd.read_csv(\"..\/input\/steam-game-reviews\/test.csv\")\n    sub = pd.read_csv(\"..\/input\/steam-game-reviews\/sample_submission.csv\")\n    \n    print(\"Train Shape : \\t{}\\nTest Shape : \\t{}\\n\".format(train.shape, test.shape))\n\n    return train, test, sub","28e5d5f4":"train, test, sub = get_data()","ace2d872":"list_sentences_train = train[\"user_review\"]\nlist_sentences_test = test[\"user_review\"]","1c927093":"%%time\n\nmax_features = 20000\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(list_sentences_train))\nlist_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)","33604890":"max_len = 500\nX_t = pad_sequences(list_tokenized_train, maxlen=max_len)\nX_te = pad_sequences(list_tokenized_test, maxlen=max_len)","32e2dbb4":"def load_embedding_matrix():\n    word2vec_dict = word2vec.KeyedVectors.load_word2vec_format(\"..\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin\", binary=True)\n    embed_size = 300\n    embedding_index = dict()\n    for word in tqdm_notebook(word2vec_dict.wv.vocab):\n        embedding_index[word] = word2vec_dict.word_vec(word)\n    print(\"Loaded {} word vectors.\".format(len(embedding_index)))\n    gc.collect()\n    \n    # We get the mean, and std of the embedding weights so that we could maintain the same statistics for the rest of our random generated weights.\n    all_embs = np.stack(list(embedding_index.values()))\n    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n    \n    nb_words = len(tokenizer.word_index)\n    \n    # We are going to set the embedding size to the pretrained dimension as we are replicating it.\n    # The size would be Number of Words X Embedding Size\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    gc.collect()\n    # Now we have generated a random matrix lets fill it out with our own dictionary and the one with pretrained embeddings.\n    embedded_count = 0\n    for word, i in tqdm_notebook(tokenizer.word_index.items()):\n        i -= 1\n        # Then we see whether the word is in Word2Vec dictionary, if yes get the pretrained weights.\n        embedding_vector = embedding_index.get(word)\n        # And store that in our embedding_matrix that we will use to train ML model.\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            embedded_count += 1\n    print(\"Total Embedded : {} common wrds\".format(embedded_count))\n        \n    del embedding_index\n    gc.collect()\n    \n    # Finally return the embedding matrix\n    return embedding_matrix","441ec00c":"%%time\n\nembedding_matrix = load_embedding_matrix()","0e4695b3":"embedding_matrix.shape","eb953f35":"embed_size = embedding_matrix.shape[1]\nmax_features = len(tokenizer.word_index)\n\nprint(max_features, embed_size)\n\nwith strategy.scope():    \n    inp = Input(shape=(max_len, ))\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    x = Bidirectional(LSTM(60, return_sequences=True, name='lstm_layer', dropout=0.1, recurrent_dropout=0.1))(x)\n    x = GlobalMaxPool1D()(x)\n    x = Dropout(0.1)(x)\n    x = Dense(50, activation=\"relu\")(x)\n    x = Dropout(0.1)(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=[\"accuracy\"])\nmodel.summary()","e2896dc1":"batch_size = 32\nepochs = 3\n\nhist = model.fit(X_t, train['user_suggestion'], batch_size=batch_size, epochs=epochs, validation_split=0.1)","ddeb3aa6":"### Training","06a5b959":"## Embedding Matrix","984bec8e":"## TPU Config\n\nIn this kernel we are using TPU's, because why not? (it trains the NN very very fast.)","c21c91d2":"### Import Data","40af7dd1":"# Getting Started with NLP : 1.1 | Word2Vec |\n\nCheck my blog for the [explaination](https:\/\/kranthik13.github.io\/blog\/2020\/05\/13\/getting-started-with-nlp-1-1-word2vec.html).","7c74ca68":"# END","d7a265ae":"## Modelling"}}