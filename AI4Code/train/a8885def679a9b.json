{"cell_type":{"d6d7e5f8":"code","864cb60f":"code","32bcc701":"code","07904981":"code","7d4e109c":"code","ff4a20c4":"code","6cb92130":"code","141f78d2":"code","cabc8611":"code","fb45e8f7":"markdown","a898a464":"markdown"},"source":{"d6d7e5f8":"# Import libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras import layers, callbacks, Sequential\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Stop the kaggle spam messages =-)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Stop the TensorFlow spam messages =-D\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n","864cb60f":"# Load and combine data - This dataset comes in three parts\n\ncols = ['Body', 'Label']\n\ndf1 = pd.read_csv('\/kaggle\/input\/email-spam-dataset\/lingSpam.csv', usecols = cols)\ndf2 = pd.read_csv('\/kaggle\/input\/email-spam-dataset\/enronSpamSubset.csv', usecols = cols)\ndf3 = pd.read_csv('\/kaggle\/input\/email-spam-dataset\/completeSpamAssassin.csv', usecols = cols)\n\ndf = df1.append(df2).append(df3)\n\ndf.Body = df.Body.astype(str)\n\ndf.head()","32bcc701":"# Seperate dataset into test and train\n\ny = df.Label\nX = df.Body\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, random_state=123)\n\nprint([x.shape for x in [train_X, test_X, train_y, test_y]])","07904981":"# Split text into tokens and turn these into numbers\n# 'oov' stands for out of vocabulary  and means any word not seen before\n\n# Set Perameters - means you only have to change in one place\nnumWords = 20000\nmaxLength = 200\n\ntokenizer = Tokenizer(num_words = numWords, oov_token = '<OOV>')\ntokenizer.fit_on_texts(train_X)\nword_index = tokenizer.word_index\n\ntrain_sequences = tokenizer.texts_to_sequences(train_X)\ntrain_padded = pad_sequences(train_sequences, padding = 'post', maxlen = maxLength, truncating = 'post')\n\ntest_sequences = tokenizer.texts_to_sequences(test_X)\ntest_padded = pad_sequences(test_sequences, padding = 'post', maxlen = maxLength, truncating = 'post')","7d4e109c":"# Create our model, inputs should equal number of words entered into model\n# output should be 1 as only decision is if Spam\n\nmodel = Sequential([\n    layers.Embedding(numWords, 16, input_length=maxLength),\n    layers.GlobalAveragePooling1D(),\n    layers.Dense(24, activation='relu'),\n    layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n\nmodel.summary()","ff4a20c4":"# run model\n\nnum_epochs = 30\nhistory = model.fit(train_padded, train_y, epochs=num_epochs, \n                    validation_data=(test_padded, test_y), verbose=2)","6cb92130":"# summarise data in graph\n\ndef plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_'+string])\n    plt.show()\n\nplot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","141f78d2":"# Lets stop the model when it is at its best\nearly_stopping = callbacks.EarlyStopping(\n    min_delta=0.001,\n    # Patience means it will carry on checking for this number of epochs before it gives up and goes back to the best result\n    patience=5,\n    restore_best_weights=True,\n)\n\n# We are keeping the model the same\nlarge_num_epochs = 200\nbetter_history = model.fit(train_padded, train_y, epochs=large_num_epochs,\n                           validation_data=(test_padded, test_y), verbose=2,\n                           callbacks=[early_stopping])\n\nprint('\/nBest Model Found')","cabc8611":"# Lets make a prediction - Either enter your own or uncomment one of the examples\n\n#header = input(\"Enter email header: \")\n#body = input(\"Enter email body: \")\n\n# We want the input to match the format the model is used to\n#email = 'Subject: ' + header + '\\n\\n' + body\n\n# Example spam email\nemail = 'Subject: MANAGE YOUR BLOOD PRESSURE + MELT AWAY EXCESS POUNDS FAST\/n\/n*Regulates Blood Pressure\\n*Balances Blood Sugar Levels\\n*Lowers Bad Cholestoral\\n\\n ORDER YOURS NOW!'\n# Example not spam email\n#email = 'Subject: RE: Python lesson \\n\\nKeith  Just a follow up. I would be grateful if you find out how much it would cost to learn to speak python as I have recently brought a pet snake.   Regards  Jake'\n\n\npredict_sequences = tokenizer.texts_to_sequences([email,])\npredict_padded = pad_sequences(predict_sequences, padding = 'post', maxlen = maxLength, truncating = 'post')\n\nprint('SPAM' if model.predict(predict_padded) > 0.5 else 'NOT SPAM')","fb45e8f7":"## This notebook is a practice of how to train a neural network model to predict if an email is spam or not\n\nContent:\n\n* Combining datasets\n* Tokenising text\n* Overfitting in neural networks\n* Early Stopping to prevent overfitting\n* A basic user interface to use model to predict if email is spam\n\nThe main focus is understanding how to use the Tensorflow Keras package to preform natural language processing (NLP)\n\nThese methods could easily be adapted to any sort of sentiment analysis just by training the model on a dataset that is labeled with the sentiment you wish to find.","a898a464":"### We can see from the graphs that we tend to find the best results about 6 epochs in. \n\nAfter that the model starts to overfit\n\n(This will change with different implimentations of the model)"}}