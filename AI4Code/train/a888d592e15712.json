{"cell_type":{"f32e0797":"code","03f3b27b":"code","8e3d60f4":"code","f0713def":"code","cf138dc2":"code","948820b5":"code","a8c079a7":"code","37a0a0db":"code","86ab8c74":"code","a9974b45":"code","e8a853ca":"code","eb400f17":"code","61f89455":"code","f754a59e":"code","50d1e9d6":"code","c47e7f8e":"code","ec52c016":"code","87f17231":"code","34ed576f":"code","bee19cf3":"code","0ddc6c77":"code","9bf95423":"markdown","c7cfe071":"markdown","e4628392":"markdown","1ac1d9bf":"markdown","30b6656d":"markdown","e1f28588":"markdown","70e53339":"markdown","faa11178":"markdown","9883f1f6":"markdown","7b191079":"markdown","b49f4bac":"markdown","d282e8be":"markdown","5fcbaa10":"markdown","cead386a":"markdown","a9db81e8":"markdown"},"source":{"f32e0797":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","03f3b27b":"data = pd.read_csv(\"..\/input\/student-mat.csv\",sep=\";\")\ndata.head()\ndata.info()","8e3d60f4":"from sklearn.preprocessing import LabelEncoder\nlab_enc = LabelEncoder()\n\nfor i in range(data.shape[1]):\n    #print(data.dtypes[i])\n    if data.dtypes[i] == 'object':\n        #print(data.dtypes[i])\n        #print(data.columns[i])\n        data[data.columns[i]] = lab_enc.fit_transform(data[data.columns[i]])","f0713def":"import seaborn as sns\ncorr_matrix = data.corr()\n\ndatac = data.copy()\nlow_corr = []\nfor ind,c_val in enumerate(corr_matrix[\"G3\"]):\n    if abs(c_val) < 0.10:\n        #print(ind)\n        #print(datac.columns[ind])\n        low_corr.append(datac.columns[ind])\n\nfor name in low_corr:\n    #print(name)\n    datac.pop(name)","cf138dc2":"datac.head()","948820b5":"corr_matrix = datac.corr()\nplt.figure(figsize=(20,10))\nsns.heatmap(corr_matrix,annot=True,cmap=\"coolwarm\",fmt=\".2f\",annot_kws={'size':16})\ncorr_matrix[\"G3\"].sort_values(ascending=False)","a8c079a7":"plt.figure(figsize=(10,5))\nax1 = plt.subplot(1, 2, 1)\nsns.distplot(data[\"G3\"], label=\"Whole Group Avg = \" + str(np.round(np.mean(data[\"G3\"]),2)))\nplt.ylabel(\"density\")\nplt.xlabel(\"Final Grade\")\nplt.legend()\nax2 = plt.subplot(1, 2, 2)\nsns.distplot(data[\"G3\"][data[\"sex\"]==0], label=\"Female\")\nsns.distplot(data[\"G3\"][data[\"sex\"]==1], label=\"Male\")\nplt.ylabel(\"density\")\nplt.xlabel(\"Final Grade\")\nplt.legend(('Female Avg = '+str(np.round(np.mean(data[\"G3\"][data[\"sex\"]==0]),2)),\n            'Male Avg = '+str(np.round(np.mean(data[\"G3\"][data[\"sex\"]==1]),2))))\nplt.show()","37a0a0db":"plt.figure(figsize=(10,5))\nax1 = plt.subplot(1, 2, 1)\nplt.scatter(datac[\"G1\"],datac[\"G3\"],color='r')\nplt.ylabel(\"G3\")\nplt.xlabel(\"G1\")\nplt.title(\"corr_val = \" + str(0.8))\nax2 = plt.subplot(1, 2, 2)\nplt.scatter(datac[\"G2\"],datac[\"G3\"],color='k')\nplt.ylabel(\"G3\")\nplt.xlabel(\"G2\")\nplt.title(\"corr_val = \" + str(0.9))\nplt.show()","86ab8c74":"student_behav = np.array([[np.mean(datac[\"G3\"][datac[\"higher\"]==1]),np.mean(datac[\"G3\"][datac[\"higher\"]==0])],\n        [np.mean(datac[\"G3\"][datac[\"romantic\"]==1]),np.mean(datac[\"G3\"][datac[\"romantic\"]==0])],\n         [np.mean(datac[\"G3\"][datac[\"paid\"]==1]),np.mean(datac[\"G3\"][datac[\"paid\"]==0])]])\n\nplt.figure(figsize=(18,6))\nplt.subplot(1,2,1)\nsns.heatmap(student_behav,annot=True,cmap=\"coolwarm\",fmt=\".2f\",annot_kws={'size':16})\nplt.title(\"Average score based classification\")\nplt.xticks([0.5,1.5],(\"Yes\",\"No\"),fontsize = 16)\nplt.yticks([0.5,1.5,2.5],(\"higher\",\"romantic\",\"paid\"),fontsize = 16)\nplt.show()","a9974b45":"ax = sns.boxplot(x=\"failures\", y=\"G3\", data=datac)\nax = sns.swarmplot(x=\"failures\", y=\"G3\", data=datac, color=\".25\")\nplt.xlabel('Failures', fontsize=18)\nplt.ylabel('G3', fontsize=18)\nplt.xticks(fontsize = 16)\nplt.yticks(fontsize = 16)\nplt.show()","e8a853ca":"from sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder()\nfe1_1hot = encoder.fit_transform(datac['Medu'].values.reshape(-1,1))\nfe2_1hot = encoder.fit_transform(datac['Fedu'].values.reshape(-1,1))\nfe3_1hot = encoder.fit_transform(datac['Mjob'].values.reshape(-1,1))\nfe4_1hot = encoder.fit_transform(datac['traveltime'].values.reshape(-1,1))\nfe5_1hot = encoder.fit_transform(datac['goout'].values.reshape(-1,1))\n\n\ndatadrop = datac.drop(columns=[\"Medu\",\"Fedu\",\"Mjob\",\"traveltime\",\"goout\"])\nX_new = datadrop.iloc[:,:-1].values\nY_new = datadrop.iloc[:,-1].values\nX_new = np.concatenate((X_new,fe1_1hot.toarray(),fe2_1hot.toarray(),fe3_1hot.toarray(),fe4_1hot.toarray(),fe5_1hot.toarray()),axis=1)","eb400f17":"for i in range(len(Y_new)):\n    if Y_new[i]>=12:\n        Y_new[i] = 1\n    else:\n        Y_new[i] = 0","61f89455":"from sklearn.preprocessing import StandardScaler\nstand_sca = StandardScaler()\nX_trans = stand_sca.fit_transform(X_new)\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X_trans,Y_new, test_size = 0.2, random_state = 33)","f754a59e":"def Neural_Network_Simple(LR,beta,max_ite,input_nodes,hidden_nodes,output_nodes,X,Y):\n\n    # weight initialization\n    W_1 = np.random.randn(hidden_nodes, input_nodes)\n    W_2 = np.random.randn(output_nodes, hidden_nodes)\n    B_1 = np.zeros((hidden_nodes, 1))\n    B_2 = np.zeros((output_nodes, 1))\n\n    # gradient descent with momentum\n    Vdw_1 = np.random.randn(hidden_nodes, input_nodes)\n    Vdw_2 = np.random.randn(output_nodes, hidden_nodes)\n    Vdb_1 = np.zeros((hidden_nodes, 1))\n    Vdb_2 = np.zeros((output_nodes, 1))\n\n    # cost function\n    N = np.size(Y,0)\n    Cost = np.zeros((max_ite,1))\n\n    for i in range(max_ite):\n        A_1 = W_1.dot(X) + np.tile(B_1, (1, N))\n        # Relu activation\n        Z_1 = A_1\n        Z_1[Z_1 < 0] = 0\n\n        A_2 = W_2.dot(Z_1) + np.tile(B_2, (1, N))\n        Z_2 = 1 \/ (1 + np.exp(-A_2))\n        \n        del_2 = Z_2 - Y\n        #derivative of Relu\n        de_2_acti = A_1\n        de_2_acti[de_2_acti>0] = 1\n        de_2_acti[de_2_acti<=0] = 0\n        del_1 = W_2.T.dot(del_2) * de_2_acti\n        #backprop\n        dw_2 = del_2.dot(Z_1.T)\n        dw_1 = del_1.dot(X.T)\n        db_2 = np.sum(del_2, 1)\n        db_1 = np.sum(del_1, 1).reshape(hidden_nodes,1)\n\n        Vdw_2 = beta * Vdw_2 + (1 - beta) * dw_2\n        Vdw_1 = beta * Vdw_1 + (1 - beta) * dw_1\n        Vdb_2 = beta * Vdb_2 + (1 - beta) * db_2\n        Vdb_1 = beta * Vdb_1 + (1 - beta) * db_1\n        #update weights and bias\n        W_2 = W_2 - LR * Vdw_2\n        W_1 = W_1 - LR * Vdw_1\n        B_2 = B_2 - LR * Vdb_2\n        B_1 = B_1 - LR * Vdb_1\n\n        Cost[i] = 0.5 * np.sum(del_2**2)\/N\n        \n        print(\"iteration #\",i , 'and accuracy is ' + str(Cost[i]))\n\n        \n    return W_1,W_2,B_1,B_2,Cost","50d1e9d6":"def forwardNN_clf(W_1,W_2,B_1,B_2,X):\n    A_1 = W_1.dot(X) + np.tile(B_1, (1, 1))\n    Z_1 = A_1\n    Z_1[Z_1 < 0] = 0\n\n    A_2 = W_2.dot(Z_1) + np.tile(B_2, (1, 1))\n    pred = 1\/(1 + np.exp(-A_2))\n    return pred","c47e7f8e":"W_1,W_2,B_1,B_2,Cost = Neural_Network_Simple(0.01,0.8,300,34,17,1,x_train.T,y_train)","ec52c016":"plt.plot(np.linspace(0,len(Cost)-1,len(Cost)),Cost)\nplt.xlabel(\"Number of itertation\"); plt.ylabel(\"loss\")\nplt.show()","87f17231":"from sklearn.metrics import classification_report, confusion_matrix\n\nann_predict_implem = forwardNN_clf(W_1,W_2,B_1,B_2,x_test.T)\nann_predict_implem = (ann_predict_implem > 0.5)\nprint(classification_report(y_test,ann_predict_implem.T))","34ed576f":"from tensorflow import keras\nfrom tensorflow.keras import layers\nclass_NN = keras.models.Sequential()\n\nclass_NN.add(layers.Dense(units=17,activation='relu',input_dim=34))\n#class_NN.add(layers.Dense(units=17,activation='relu'))\nclass_NN.add(layers.Dense(units=1,activation='sigmoid'))\nclass_NN.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\nclass_NN.fit(x_train,y_train,batch_size=10,epochs=100)","bee19cf3":"ann_predict = class_NN.predict(x_test)\nann_predict = (ann_predict > 0.5) ","0ddc6c77":"print(classification_report(y_test,ann_predict))","9bf95423":"Keras implementation.","c7cfe071":"Prediction function with forward propagation.","e4628392":"**Implementaion of Artificial Neural Network for Classification Problem.**\n\n* Data Analysis.\n* Feature Engineering and Selection.\n* NN classification implementaion from scractch.\n* Keras implementaion.\n\n**Note:** I did not extensively document this kernel since it has lots of similarities to my NN regression implementation kernel. The subtle difference between these two implementaion is what kind output actication is used. You would use linear activation if it is a regression problem, sigmoid for binary classification and softmax for multi class problems.","1ac1d9bf":"Final Grade distribution between male and female. Male performed better.","30b6656d":"Drop Weak Features. Any features correlation value less 0.1 is dropped.","e1f28588":"Data explanation:\n\n* school - student's school (binary: \"GP\" - Gabriel Pereira or \"MS\" - Mousinho da Silveira)\n* sex - student's sex (binary: \"F\" - female or \"M\" - male)\n* age - student's age (numeric: from 15 to 22)\n* address - student's home address type (binary: \"U\" - urban or \"R\" - rural)\n* famsize - family size (binary: \"LE3\" - less or equal to 3 or \"GT3\" - greater than 3)\n* Pstatus - parent's cohabitation status (binary: \"T\" - living together or \"A\" - apart)\n* Medu - mother's education (numeric: 0 - none,  1 - primary education (4th grade), 2 \u2013 5th to 9th grade, 3 \u2013 secondary education or 4 \u2013 higher education)\n* Fedu - father's education (numeric: 0 - none,  1 - primary education (4th grade), 2 \u2013 5th to 9th grade, 3 \u2013 secondary education or 4 \u2013 higher education)\n* Mjob - mother's job (nominal: \"teacher\", \"health\" care related, civil \"services\" (e.g. administrative or police), \"at_home\" or \"other\")\n* Fjob - father's job (nominal: \"teacher\", \"health\" care related, civil \"services\" (e.g. administrative or police), \"at_home\" or \"other\")\n* reason - reason to choose this school (nominal: close to \"home\", school \"reputation\", \"course\" preference or \"other\")\n* guardian - student's guardian (nominal: \"mother\", \"father\" or \"other\")\n* traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)\n* studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)\n* failures - number of past class failures (numeric: n if 1<=n<3, else 4)\n* schoolsup - extra educational support (binary: yes or no)\n* famsup - family educational support (binary: yes or no)\n* paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n* activities - extra-curricular activities (binary: yes or no)\n* nursery - attended nursery school (binary: yes or no)\n* higher - wants to take higher education (binary: yes or no)\n* internet - Internet access at home (binary: yes or no)\n* romantic - with a romantic relationship (binary: yes or no)\n* famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n* freetime - free time after school (numeric: from 1 - very low to 5 - very high)\n* goout - going out with friends (numeric: from 1 - very low to 5 - very high)\n* Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n* Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n* health - current health status (numeric: from 1 - very bad to 5 - very good)\n* absences - number of school absences (numeric: from 0 to 93)\n* G1 - first period grade (numeric: from 0 to 20)\n* G2 - second period grade (numeric: from 0 to 20)\n* G3 - final grade (numeric: from 0 to 20, output target)\n\n","70e53339":"Two highest correlated features are first two mid term grades (G1 and G2), students who performed better in midterms also did well in final exam.","faa11178":"Feature scaling.","9883f1f6":"Monitor training errror. ","7b191079":"Output is probability from sigmoid function, so greater than 0.5 is one class and vise versa.","b49f4bac":"Students who did not fail other courses performed better in their finals.","d282e8be":"Convert Categorical data.","5fcbaa10":"My own NN implementaion.","cead386a":"Here I use 12 pts (out of 20, 60%)  as a cutoff to determine whether students faild finals or not. Since I am using this data as a classification problem demonstration.","a9db81e8":"Students who paid for extra classes, not in romantic relanship, and wants to pursue higher education have better average final scores. "}}