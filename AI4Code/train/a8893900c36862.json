{"cell_type":{"678ac7c0":"code","b73cd8ef":"code","02f8b56c":"code","3182ae4f":"code","1cb938f6":"code","fb177288":"code","2bae0403":"code","be67a480":"markdown","1bb43ae1":"markdown","a7d309c6":"markdown","09d48897":"markdown","098c63ab":"markdown"},"source":{"678ac7c0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\n\npath = \"\/kaggle\/input\/lish-moa\/\"","b73cd8ef":"train_df = pd.read_csv(path + \"train_features.csv\", index_col = \"sig_id\")\ntest_df = pd.read_csv(path + \"test_features.csv\", index_col = \"sig_id\")\nsubm_df = pd.read_csv(path + \"sample_submission.csv\")\ntr_scored = pd.read_csv(path + \"train_targets_scored.csv\", index_col = \"sig_id\")\ntr_nonscored = pd.read_csv(path + \"train_targets_nonscored.csv\", index_col = \"sig_id\")","02f8b56c":"def make_numeric(df):\n    df[\"cp_type\"] = df[\"cp_type\"].replace(\"trt_cp\",0)\n    df[\"cp_type\"] = df[\"cp_type\"].replace(\"ctl_vehicle\",1)\n    df[\"cp_dose\"] = df[\"cp_dose\"].replace(\"D1\",0)\n    df[\"cp_dose\"] = df[\"cp_dose\"].replace(\"D2\",1)\n    return df","3182ae4f":"train_df = make_numeric(train_df)\ntest_df = make_numeric(test_df)","1cb938f6":"num_fold = 5\ndef get_lgbm_model(X_train, y_train, X_val, y_val, fold, columns):\n    \n    train_data = lgb.Dataset(X_train, label=y_train)\n    val_data = lgb.Dataset(X_val, label=y_val)\n\n    params = {\n        \"metric\":\"binary_logloss\",\n        \"learning_rate\":0.01\n    }  \n    \n    model = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\n    model.fit(\n        X_train, \n        y_train, \n        eval_set=[(X_train, y_train), (X_val, y_val)], \n        verbose=10, \n        early_stopping_rounds=10\n    )\n    \n    return model\n\ndef get_lgbm_pred(X, y, test):\n    print(\"get_lgbm_pred \")\n\n    pred = []\n    pred_val = np.zeros((len(X)))\n            \n    #X_train, X_val, y_train, y_val = train_test_split(X_scaled, y.fillna(0).values, test_size=0.2, shuffle=True, random_state=42)\n    kf = KFold(n_splits=num_fold, random_state=None, shuffle=False)\n    fold = 0\n    score = 0\n    for train_index, test_index in kf.split(X, y):\n        fold += 1\n        print(\"fold \", fold)\n    \n        X_train = X.iloc[train_index, :]\n        X_val = X.iloc[test_index, :]\n        y_train = y[train_index]\n        y_val = y[test_index]\n        \n        model = get_lgbm_model(X_train.values, y_train, X_val.values, y_val, fold, X_train.columns)\n\n\n        if fold ==1:\n            pred.append(model.predict(test))\n        else:\n            pred += model.predict(test)\n\n        \n        pred_val[test_index] = model.predict(X_val)            \n        score = score + np.sqrt(mean_squared_error(y_val, pred_val[test_index]))\n        print(\"score \", str(score\/fold))\n                \n    return pred[0]\/num_fold, pred_val","fb177288":"n=0\nfor col in subm_df.columns[1:]:\n    n += 1\n    print(n, col)\n    subm_df[col], pred_tr = get_lgbm_pred(train_df, tr_scored[col], test_df)","2bae0403":"subm_df.to_csv(\"submission.csv\", index=False)","be67a480":"Let's digitize the non-numeric columns: ","1bb43ae1":"Our simple lightgbm model:","a7d309c6":"Finally, our submission file:","09d48897":"Let's run lightgbm for all output columns in the scored file:","098c63ab":"Our input files:"}}