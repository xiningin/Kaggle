{"cell_type":{"a00b4355":"code","7688db5e":"code","eb349867":"code","6baa69d5":"code","de450fdb":"code","5405c92f":"code","64249dd7":"code","08870251":"code","048f785a":"code","1d699b36":"code","f0f26d77":"code","dba7fc8a":"code","24268a2d":"code","ffde97c5":"code","9c74bae8":"code","eacb27a0":"code","d3ee754a":"code","0afa1a41":"code","c6b709c0":"code","43e632f9":"code","68cc2439":"code","28db4a6e":"code","59fc50d6":"code","c909a499":"code","7709f85f":"code","f23b3b25":"code","262e33b9":"code","0017d27c":"code","1b80b0ff":"code","51848081":"code","c0a1b336":"code","54424f48":"code","75c3f1c9":"code","db46cf87":"code","97475041":"code","dd86fcdf":"code","ab36ee33":"code","fed50fb4":"code","0bef19d7":"code","9f0dfe62":"code","3259505a":"code","e060cf48":"code","3cc36c91":"code","c63b09ba":"code","97e8b385":"code","d2b9bff1":"code","bc169fe4":"code","3798c198":"code","efcb5726":"code","cf7997c1":"code","7e4ba4de":"code","c26d81eb":"code","04d4d572":"code","c8bb7cb6":"code","222d7eeb":"code","cc26d334":"markdown","d9b27d4b":"markdown","e88eb7d8":"markdown","6b144681":"markdown","c144f73a":"markdown","86a1fc53":"markdown"},"source":{"a00b4355":"import numpy as np \nimport pandas as pd \npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 100)\npd.set_option('display.width', 1000)\n\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\n\nfrom datetime import datetime\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nfrom mlxtend.regressor import StackingCVRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","7688db5e":"# Drop ID\ntrain.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)","eb349867":"# axList = train[\"SalePrice\"].hist(bins=25, figsize=(4,4))","6baa69d5":"# Log is used to respond to skewness towards large values \ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\ny = train['SalePrice'].reset_index(drop=True)\n# axList = train[\"SalePrice\"].hist(bins=25, figsize=(4,4))","de450fdb":"corr_matrix = train.corr(method='pearson')\ncorr_matrix['SalePrice'].sort_values(kind=\"quicksort\")","5405c92f":"train[\"BsmtFinSF1\"].hist(bins=30, figsize=(15,2))","64249dd7":"train.boxplot(figsize=(15,7), column=[\n'BsmtFinSF1',\n'1stFlrSF',\n'TotalBsmtSF',\n'GrLivArea',])","08870251":"train.boxplot(figsize=(15,7), column=[\n'BsmtUnfSF',\n'2ndFlrSF',\n'MasVnrArea',\n'GarageArea',\n])","048f785a":"train[train['2ndFlrSF'] > 1500]","1d699b36":"train.boxplot(figsize=(15,7), column=[\n'OpenPorchSF',\n'WoodDeckSF',\n'LotFrontage',\n])","f0f26d77":"train.boxplot(figsize=(15,7), column=[\n\n'KitchenAbvGr',\n'BsmtHalfBath',\n'BedroomAbvGr',\n'BsmtFullBath',\n'HalfBath',\n'Fireplaces',\n'TotRmsAbvGrd',\n'FullBath',\n'GarageCars',\n'OverallQual'\n])","dba7fc8a":"train[train['BsmtFullBath'] > 2]\ntrain[train['OverallQual'] < 2]","24268a2d":"train.boxplot(figsize=(15,5), column=[\n'EnclosedPorch',\n'3SsnPorch',\n'ScreenPorch'\n])","ffde97c5":"\ntrain.boxplot(figsize=(15,5), column=[\n#'YrSold',\n#'MoSold',\n'YearRemodAdd',\n'YearBuilt'\n])","9c74bae8":"train[train['YearBuilt'] < 1900]","eacb27a0":"# Put test & train features to one df\ntrain_features = train.drop(['SalePrice'], axis=1)\ntest_features = test\nfeatures = pd.concat([train_features, test_features]).reset_index(drop=True)","d3ee754a":"# Exploring NaN\ndef missing_values_table(df):\n    mis_val = df.isnull().sum()\n    mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n    mis_val_table_ren_columns = mis_val_table.rename(\n    columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n    mis_val_table_ren_columns = mis_val_table_ren_columns[\n        mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n    '% of Total Values', ascending=False).round(1)\n    return mis_val_table_ren_columns\nmissing_values_table(features)","0afa1a41":"# Removing features that are not very useful:\n# - more then 48% of NaN at start : 'PoolQC','MiscFeature','Alley','Fence','FireplaceQu'\nfeatures = features.drop(['PoolQC','MiscFeature','Alley','Fence','FireplaceQu'], axis=1)","c6b709c0":"# Since these column are actually a category , using a numerical number will lead the model to assume\n# that it is numerical , so we convert to string .\nfeatures['MSSubClass'] = features['MSSubClass'].apply(str)\nfeatures['YrSold'] = features['YrSold'].astype(str)\nfeatures['MoSold'] = features['MoSold'].astype(str)","43e632f9":"print(features['Functional'].value_counts(dropna=False))\nprint(features['Electrical'].value_counts(dropna=False))\nprint(features['KitchenQual'].value_counts(dropna=False))","68cc2439":"## Filling these columns With most suitable value for these columns \nfeatures['Functional'] = features['Functional'].fillna('Typ') \nfeatures['Electrical'] = features['Electrical'].fillna(\"SBrkr\") \nfeatures['KitchenQual'] = features['KitchenQual'].fillna(\"TA\") ","28db4a6e":"print(features['Exterior1st'].value_counts(dropna=False))\nprint(features['Exterior2nd'].value_counts(dropna=False))\nprint(features['SaleType'].value_counts(dropna=False))","59fc50d6":"## Filling these with MODE, i.e. , the most frequent value in these columns .\nfeatures['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0]) \nfeatures['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\nfeatures['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])","c909a499":"### Missing data in GarageYrBit most probably means missing Garage, so replace NaN with zero . \nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    features[col] = features[col].fillna(0)\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    features[col] = features[col].fillna('None')\n\n### Same with basement\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    features[col] = features[col].fillna('None')","7709f85f":"\"\"\"\n# Since these column are actually a category , using a numerical number will lead the model to assume\n# that it is numerical , so we convert to string .\nfeatures['MSSubClass'] = features['MSSubClass'].apply(str)\nfeatures['YrSold'] = features['YrSold'].astype(str)\nfeatures['MoSold'] = features['MoSold'].astype(str)\n\n## Filling these columns With most suitable value for these columns \nfeatures['Functional'] = features['Functional'].fillna('Typ') \nfeatures['Electrical'] = features['Electrical'].fillna(\"SBrkr\") \nfeatures['KitchenQual'] = features['KitchenQual'].fillna(\"TA\") \nfeatures[\"PoolQC\"] = features[\"PoolQC\"].fillna(\"None\")\n\n## Filling these with MODE, i.e. , the most frequent value in these columns .\nfeatures['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0]) \nfeatures['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\nfeatures['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\n\n### Missing data in GarageYrBit most probably means missing Garage, so replace NaN with zero . \nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    features[col] = features[col].fillna(0)\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    features[col] = features[col].fillna('None')\n\n### Same with basement\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    features[col] = features[col].fillna('None')\n\"\"\"","f23b3b25":"missing_values_table(features)","262e33b9":"# This code will filll the missing values with the mode \n# (The frequently category appearing) By each MSsubclass:\n# Idea is that similar MSSubClasses will have similar MSZoning\nfeatures['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\n# Fill the remaining columns as None\nobjects = []\nfor i in features.columns:\n    if features[i].dtype == object:\n        objects.append(i)\nfeatures.update(features[objects].fillna('None'))","0017d27c":"missing_values_table(features)","1b80b0ff":"features[\"LotFrontage\"].hist(bins=30, figsize=(15,3))","51848081":"# For missing values in numerical cols , we fillNa with 0\n# We are still filling up missing values \nfeatures['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes:\n        numerics.append(i)\nfeatures.update(features[numerics].fillna(0))\n\n\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics2 = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes:\n        numerics2.append(i)\nskew_features = features[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nfor i in skew_index:\n    features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))","c0a1b336":"missing_values_table(features)","54424f48":"features","75c3f1c9":"print(features['Utilities'].value_counts(dropna=False))\nprint(features['Street'].value_counts(dropna=False))","db46cf87":"# Removing features that are not very useful:\n# - droped because ... : 'Utilities', 'Street'\nfeatures = features.drop(['Utilities', 'Street'], axis=1)\n","97475041":"print(features['MasVnrType'].value_counts(dropna=False))\nprint(features['MasVnrArea'].value_counts(dropna=False))","dd86fcdf":"# - droped because ... : 'MasVnrType'\n# features = features.drop(['MasVnrType'], axis=1)","ab36ee33":"# Adding new features. Sums of categiries.\n\nfeatures['YrBltAndRemod']=features['YearBuilt']+features['YearRemodAdd']\nfeatures['TotalSF']=features['TotalBsmtSF'] + features['1stFlrSF'] + features['2ndFlrSF']\n\nfeatures['Total_sqr_footage'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n                                 features['1stFlrSF'] + features['2ndFlrSF'])\n\nfeatures['Total_Bathrooms'] = (features['FullBath'] + (0.5 * features['HalfBath']) +\n                               features['BsmtFullBath'] + (0.5 * features['BsmtHalfBath']))\n\nfeatures['Total_porch_sf'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                              features['EnclosedPorch'] + features['ScreenPorch'] +\n                              features['WoodDeckSF'])\n\n## If PoolArea = 0 , Then HasPool = 0 too, ...\n\nfeatures['haspool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['has2ndfloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasgarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasbsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasfireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\nfeatures.shape","fed50fb4":"features","0bef19d7":"########################################################\n#  Get_dummies converts Categorical data to numerical, # \n#  as models don't work with Text data                 #\n########################################################\n\nfinal_features = pd.get_dummies(features).reset_index(drop=True)\nfinal_features.shape","9f0dfe62":"final_features","3259505a":"##########################################################\n#  Now, again train and test are spilt back seperately,  #\n#  as now all data processing is done.                   #\n#  Y is taget and its length is used to split            #\n##########################################################\n\nX = final_features.iloc[:len(y), :]\nX_sub = final_features.iloc[len(y):, :]\nX.shape, y.shape, X_sub.shape","e060cf48":"final_features.iloc[[30,88,462, 631, 1322]]","3cc36c91":"# Removing outliers.\n# Can be seen by plotting them in a graph.\n\noutliers = [30, 88, 462, 631, 1322]\n\n# From RandomForest:\n# outliers = [30, 39, 58, 88, 108, 307, 375, 462, 520, 523, 533, 631, 635, 636, 705, 769, 778, 828, 954, 1179, 1218, 1219, 1298, 1322, 1337]\n\nX = X.drop(X.index[outliers])\ny = y.drop(y.index[outliers])","c63b09ba":"overfit = []\nfor i in X.columns:\n    counts = X[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(X) * 100 > 99.94:\n        overfit.append(i)\n\noverfit = list(overfit)\nX = X.drop(overfit, axis=1)\nX_sub = X_sub.drop(overfit, axis=1)\noverfit","97e8b385":"X.shape, y.shape, X_sub.shape","d2b9bff1":"X_sub","bc169fe4":"# Process of modelling K-Folds cross-validator\n# Provides train\/test indices to split data in train\/test sets. \n\n\n# defining error functions for handy use. \n\nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)\n\nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5,16,17,20,25,30,35,40,50,55,60,65,70,80,90]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008, 0.0009]\n\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008, 0.0009, 0.001, 0.002, 0.003, 0.004, 0.005]\ne_l1ratio = [0.08, 0.1, 0.3, 0.5, 0.7, 0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\n# svr_c = [11,13,15,17,19,21,23,25]\n# svr_epsilon = [0.007,0.008,0.009]\n# svr_gamma = [0.0003,0.0004,0.0005,0.0006]\n\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kfolds))\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))                                \nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003,))\ngbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber', random_state =42)\n\nrfr = RandomForestRegressor(n_estimators=100, random_state=42)\n\nlightgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       )\n\nxgboost = XGBRegressor(learning_rate=0.01,n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006)\n\n\n","3798c198":"# Stacking - combine multiple regression models via a meta-regressor. \n\n# In the standard stacking procedure, the first-level regressors are fit to the same \n# training set that is used prepare the inputs for the second-level regressor,\n# which may lead to overfitting. The StackingCVRegressor, however, uses the concept \n# of out-of-fold predictions: the dataset is split into k folds, and in k successive rounds, \n\n# In simple words, Stacking helps avoid fitting on the same data twice , \n# and is effective in reducing overfitting.\n\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, svr, elasticnet, gbr, xgboost, lightgbm, rfr),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)\n\n# Now, we compare the various models that we just created.\n# Using various prediction models that we just created \n\n\n\nscore = cv_rmse(rfr , X)\nprint(\"Random Forest: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(ridge , X)\nprint(\"RIDGE: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(lasso , X)\nprint(\"LASSO: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(elasticnet)\nprint(\"elastic net: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(svr)\nprint(\"SVR: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(lightgbm)\nprint(\"lightgbm: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(gbr)\nprint(\"gbr: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(xgboost)\nprint(\"xgboost: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )","efcb5726":"print('stack_gen')\nstack_gen_model = stack_gen.fit(np.array(X), np.array(y))\n\nprint('Random Forest')\nrandom_forest_model_full_data = rfr.fit(X, y)\n\nprint('elasticnet')\nelastic_model_full_data = elasticnet.fit(X, y)\n\nprint('Lasso')\nlasso_model_full_data = lasso.fit(X, y)\n\nprint('Ridge')\nridge_model_full_data = ridge.fit(X, y)\n\nprint('Svr')\nsvr_model_full_data = svr.fit(X, y)\n\nprint('GradientBoosting')\ngbr_model_full_data = gbr.fit(X, y)\n\nprint('xgboost')\nxgb_model_full_data = xgboost.fit(X, y)\n\nprint('lightgbm')\nlgb_model_full_data = lightgbm.fit(X, y)","cf7997c1":"stack_gen_model.predict(np.array(X)).mean()","7e4ba4de":"# Notice that we are using a few percent from different models to get our final answer, \n# all decimals add up to 1\n\ndef blend_models_predict(X):\n    return ((0.08 * elastic_model_full_data.predict(X)) + \n            (0.08 * lasso_model_full_data.predict(X)) + \n            (0.08 * ridge_model_full_data.predict(X)) + \n            (0.08 * svr_model_full_data.predict(X)) + \n            (0.08 * random_forest_model_full_data.predict(X)) + \n            (0.1 * gbr_model_full_data.predict(X)) + \n            (0.1 * xgb_model_full_data.predict(X)) + \n            (0.1 * lgb_model_full_data.predict(X)) + \n            (0.30 * stack_gen_model.predict(np.array(X))))\n\nprint('RMSLE score on train data:')\nprint(rmsle(y, blend_models_predict(X)))","c26d81eb":"print('Predict submission')\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.iloc[:,1] = (np.expm1(blend_models_predict(X_sub)))","04d4d572":"q1 = submission['SalePrice'].quantile(0.0042)\nq2 = submission['SalePrice'].quantile(0.99)\n# Quantiles helping us get some extreme values for extremely low or high values \nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\nsubmission.to_csv(\"submission.csv\", index=False)","c8bb7cb6":"submission.head()","222d7eeb":"\"\"\"\nsample_submission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\n\nimport matplotlib.pyplot as plt\nfig = plt.figure(figsize=(20,20),linewidth=2)\n\nax1 = fig.add_subplot(211)\nax1.plot(submission[['SalePrice']])\n\nax2 = fig.add_subplot(211)\nax2.plot(sample_submission[['SalePrice']])\n\nplt.legend([\"my\",\"sample\"], fontsize=15)\n           \nplt.xlabel('ID', fontsize=15);\nplt.show()\n\"\"\"","cc26d334":"# Stacking\n\n","d9b27d4b":"# Data processing","e88eb7d8":"# Final Step","6b144681":"# Feature Engineering","c144f73a":"## Submission","86a1fc53":"# Blending Models \/ 'Ensambling'\n\n"}}