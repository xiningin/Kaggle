{"cell_type":{"139a33e1":"code","b689f28c":"code","5df7f394":"code","5f03c00d":"code","090de2c5":"code","9cceb3c5":"code","645759c5":"code","0e470981":"code","4e228e85":"code","5eb3eb30":"code","34e38105":"code","d7b23890":"code","ec7b85f9":"code","c66ad3e6":"markdown","4f0d433c":"markdown","acbeb94a":"markdown"},"source":{"139a33e1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfrom PIL import Image\nimgs = []\ngray_imgs = []\ngray_imgs3chan = []\none_hot_list = []\n\n# Choose the model you want to run\nmodelnum = 2 # 1 for U-Net, 2 for U-Net with dense branch attached, 3 for U-Net with autoencoder attached\n\n# Read in the csv file which contains pokemon types\ndf = pd.read_csv('\/kaggle\/input\/pokemon-images-and-types\/pokemon.csv')\n\n# One hot encode the pokemon's primary type\none_hot = pd.get_dummies(df['Type1'])\n\n# Go through add grab every image and save them as numpy arrays in a list\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/pokemon-images-and-types\/images\/'):\n    for filename in filenames:\n        # split the file name string to find what type of pokemon the image contains\n        pokemonname = filename.split('\/')[-1].split('.')[0]\n        rownum = df.loc[df['Name'] == pokemonname].index[0]\n        encoded =np.array(one_hot.iloc[rownum,:]).astype(np.float32)\n        one_hot_list.append(encoded)\n        \n        # Open the image with PIL and save it as grayscale color and grayscale but in RGB format\n        img = Image.open(os.path.join(dirname, filename)).convert('RGB') # Some are RGBA so we convert them\n        gray_img = img.convert('L') # Save a gray scale version too\n        imgs.append(np.array(img))\n        gray_imgs.append(np.array(gray_img))\n        gray_imgs3chan.append(np.array(gray_img.convert('RGB'))) # This is a 3 channel version of the gray scale image.\n\n# Convert lists to numpy arrays for tensorflow processing\none_hot_list = np.array(one_hot_list)\nimgs = np.array(imgs)\ngray_imgs = np.array(gray_imgs)\ngray_imgs = np.expand_dims(gray_imgs, axis=-1)\ngray_imgs3chan = np.array(gray_imgs3chan)\nprint(imgs.shape, gray_imgs.shape, one_hot_list.shape)\n\n\n# Any results you write to the current directory are saved as output.","b689f28c":"# I just used these commands for data exploration. I haven't used much Pandas before\ndf = pd.read_csv('\/kaggle\/input\/pokemon-images-and-types\/pokemon.csv')\ndf.head()\ndf.Type2.value_counts()\ndf.Type2.isna().sum()\n#len(df.Type1.value_counts()) # there are 18 different pokemon types\ndf.head()\n\n\ndf.loc[df['Name'] == 'charizard'].Type1.iloc[0] == 'Fire'\n#df['Name'].where(df['Name'] == 'charizard')\none_hot = pd.get_dummies(df['Type1'])\nnp.array(one_hot.iloc[2,:]).astype(np.float32)\nprint(\"ignore this\")","5df7f394":"f,ax = plt.subplots(10,2) \nf.subplots_adjust(0,0,3,3)\nfor i in range(0,10,1):\n    ax[i,1].imshow(gray_imgs[i,:,:,0], cmap=plt.get_cmap('gray'))\n    ax[i,0].imshow(imgs[i,:,:])","5f03c00d":"# break the data into a training and testing partition\n\n\ntestdatasize = 30\ntrain_gray = gray_imgs[:-testdatasize]\/255\ntest_gray = gray_imgs[-testdatasize:]\/255\ntrain_color = imgs[:-testdatasize]\/255\ntest_color = imgs[-testdatasize:]\/255\ngray_imgs3chan = gray_imgs3chan[:-testdatasize]\/255 # I can use this for pretraining\ntrain_oh = one_hot_list[:-testdatasize]\ntest_oh = one_hot_list[-testdatasize:]\n\nprint(train_gray.shape, train_color.shape, train_oh.shape)","090de2c5":"# data augmentation by rotation (not currently using this)\ndef D4aug(arr):\n    r1 = np.rot90(arr,k=1,axes=(1,2))\n    r2 = np.rot90(arr,k=2,axes=(1,2))\n    r3 = np.rot90(arr,k=3,axes=(1,2))\n    return np.concatenate((arr,r1,r2,r3),axis=0)\n\n#train_gray = D4aug(train_gray)\n#train_color = D4aug(train_color)\n#gray_imgs3chan = D4aug(gray_imgs3chan)\n#print(train_gray.shape,gray_imgs3chan.shape)","9cceb3c5":"# import tensorflow \nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Concatenate, Conv2DTranspose, SpatialDropout2D, Dense, Add, Flatten","645759c5":"# Define the something like the U-Net Model\ndef unet():\n    X = tf.keras.Input(shape=(120,120,1))\n    l1 = Conv2D(64, (3,3), padding='same', activation='relu')(X)\n    l2 = Conv2D(64, (3,3), padding='same', activation='relu')(l1)\n    \n    MP1 = MaxPooling2D((2,2),strides=(2,2))(l2)\n    MP1 = SpatialDropout2D(.1)(MP1)\n    \n    l3 = Conv2D(128, (3,3), padding='same', activation='relu')(MP1)\n    l4 = Conv2D(128, (3,3), padding='same', activation='relu')(l3)\n    MP2 = MaxPooling2D((2,2),strides=(2,2))(l4)\n    MP2 = SpatialDropout2D(.2)(MP2)\n    \n    l5 = Conv2D(128, (3,3), padding='same', activation='relu')(MP2)\n    l6 = Conv2D(128, (3,3), padding='same', activation='relu')(l5)\n    MP3 = MaxPooling2D((2,2),strides=(2,2))(l6)\n    MP3 = SpatialDropout2D(.2)(MP3)\n    \n    bn1 = Conv2D(256, (3,3), padding='same', activation='relu')(MP3)\n    bn2 = Conv2D(256, (3,3), padding='same', activation='relu')(bn1)\n    bn2 = SpatialDropout2D(.2)(bn2)\n    \n    \n    \n    u1 = Conv2DTranspose(64,(3,3),strides=(2,2), padding='same', activation='relu')(bn2)\n    conc1 = Concatenate()([u1,l6])\n    c1 = Conv2D(3,(3,3),padding='same', activation='relu')(conc1)\n    \n    u2 = Conv2DTranspose(64,(3,3),strides=(2,2), padding='same', activation='relu')(conc1)\n    conc2 = Concatenate()([u2,l4])\n    c2 = Conv2D(3,(3,3),padding='same', activation='relu')(conc2)\n    \n    u3 = Conv2DTranspose(64,(3,3),strides=(2,2), padding='same', activation='relu')(conc2)\n    conc3 = Concatenate()([u3,l2])\n    #conc3 = SpatialDropout2D(.15)(conc3)\n    c3 = Conv2D(3,(3,3),padding='same', activation='sigmoid')(conc3)\n    \n    model = tf.keras.Model(X,c3)\n    return model","0e470981":"# You can use this callback if you want\n# if you don't let the model overfit then the pokemon are not very colorful, which is sad\nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', min_delta=0, patience=15, verbose=0, mode='auto',\n    baseline=None, restore_best_weights=True)\n# Below is code to pretrain. You are simply training the model to output the same grayscale image\n# I did not observe any benefit from pretraining, which is what you would expect when using ReLU activations\n# and skip connection. It is easy to learn the identity map\n# I tried pretraining because the dataset is so small\n# Un-comment the next line to pretrain\n#model.fit(train_gray,gray_imgs3chan,40,20,validation_split=.05,callbacks=[es])","4e228e85":"def unet_with_type():\n    \n    type_in = tf.keras.Input(shape=(18,))\n    d1 = Dense(2**9, activation='relu')(type_in)\n    d2 = Dense(2**8, activation='relu')(d1)\n    \n    d2 = Dense(15*15,activation='relu')(d2)\n    d2 = tf.keras.layers.Reshape((15,15,1))(d2)\n    \n    \n    # Below is the unet\n    X = tf.keras.Input(shape=(120,120,1))\n    l1 = Conv2D(64, (3,3), padding='same', activation='relu')(X)\n    l2 = Conv2D(64, (3,3), padding='same', activation='relu')(l1)\n    \n    MP1 = MaxPooling2D((2,2),strides=(2,2))(l2)\n    MP1 = SpatialDropout2D(.1)(MP1)\n    \n    l3 = Conv2D(128, (3,3), padding='same', activation='relu')(MP1)\n    l4 = Conv2D(128, (3,3), padding='same', activation='relu')(l3)\n    MP2 = MaxPooling2D((2,2),strides=(2,2))(l4)\n    MP2 = SpatialDropout2D(.2)(MP2)\n    \n    l5 = Conv2D(128, (3,3), padding='same', activation='relu')(MP2)\n    l6 = Conv2D(128, (3,3), padding='same', activation='relu')(l5)\n    MP3 = MaxPooling2D((2,2),strides=(2,2))(l6)\n    MP3 = SpatialDropout2D(.2)(MP3)\n    \n    bn1 = Conv2D(256, (3,3), padding='same', activation='relu')(MP3)\n    bn1 = Add()([bn1,d2])\n    bn2 = Conv2D(256, (3,3), padding='same', activation='relu')(bn1)\n    bn2 = SpatialDropout2D(.2)(bn2)\n    \n    \n    \n    u1 = Conv2DTranspose(64,(3,3),strides=(2,2), padding='same', activation='relu')(bn2)\n    conc1 = Concatenate()([u1,l6])\n    c1 = Conv2D(3,(3,3),padding='same', activation='relu')(conc1)\n    \n    u2 = Conv2DTranspose(64,(3,3),strides=(2,2), padding='same', activation='relu')(conc1)\n    conc2 = Concatenate()([u2,l4])\n    c2 = Conv2D(3,(3,3),padding='same', activation='relu')(conc2)\n    \n    u3 = Conv2DTranspose(64,(3,3),strides=(2,2), padding='same', activation='relu')(conc2)\n    conc3 = Concatenate()([u3,l2])\n    #conc3 = SpatialDropout2D(.15)(conc3)\n    c3 = Conv2D(3,(3,3),padding='same', activation='sigmoid')(conc3)\n    \n    model = tf.keras.Model([X,type_in],c3)\n    return model\n\n","5eb3eb30":"def auto_with_unet():\n    type_in = tf.keras.Input(shape=(18,))\n    d1 = Dense(2**9, activation='relu')(type_in)\n    d2 = Dense(2**8, activation='relu')(d1)\n    \n    d2 = Dense(15*15,activation='relu')(d2)\n    d2 = tf.keras.layers.Reshape((15,15,1))(d2)\n    \n    \n    # Below is the unet\n    X = tf.keras.Input(shape=(120,120,1))\n    l1 = Conv2D(64, (3,3), padding='same', activation='relu')(X)\n    l2 = Conv2D(64, (3,3), padding='same', activation='relu')(l1)\n    \n    MP1 = MaxPooling2D((2,2),strides=(2,2))(l2)\n    MP1 = SpatialDropout2D(.1)(MP1)\n    \n    l3 = Conv2D(128, (3,3), padding='same', activation='relu')(MP1)\n    l4 = Conv2D(128, (3,3), padding='same', activation='relu')(l3)\n    MP2 = MaxPooling2D((2,2),strides=(2,2))(l4)\n    MP2 = SpatialDropout2D(.2)(MP2)\n    \n    l5 = Conv2D(128, (3,3), padding='same', activation='relu')(MP2)\n    l6 = Conv2D(128, (3,3), padding='same', activation='relu')(l5)\n    MP3 = MaxPooling2D((2,2),strides=(2,2))(l6)\n    MP3 = SpatialDropout2D(.2)(MP3)\n    \n    bn1 = Conv2D(256, (3,3), padding='same', activation='relu')(MP3)\n    bn1 = Add()([bn1,d2])\n    bn2 = Conv2D(256, (3,3), padding='same', activation='relu')(bn1)\n    bn2 = SpatialDropout2D(.2)(bn2)\n    \n    \n    \n    u1 = Conv2DTranspose(64,(3,3),strides=(2,2), padding='same', activation='relu')(bn2)\n    conc1 = Concatenate()([u1,l6])\n    c1 = Conv2D(3,(3,3),padding='same', activation='relu')(conc1)\n    \n    u2 = Conv2DTranspose(64,(3,3),strides=(2,2), padding='same', activation='relu')(conc1)\n    conc2 = Concatenate()([u2,l4])\n    c2 = Conv2D(3,(3,3),padding='same', activation='relu')(conc2)\n    \n    u3 = Conv2DTranspose(64,(3,3),strides=(2,2), padding='same', activation='relu')(conc2)\n    conc3 = Concatenate()([u3,l2])\n    #conc3 = SpatialDropout2D(.15)(conc3)\n    c3 = Conv2D(3,(3,3),padding='same', activation='sigmoid')(conc3)\n    \n    # output of autoencoder\n    e0 = Flatten()(bn2)\n    e1 = Dense(15*15,activation='relu')(e0)\n    e2 = Dense(2**8,activation='relu')(e1)\n    e3 = Dense(2**9,activation='relu')(e2)\n    eout = Dense(18,activation='sigmoid')(e3)\n    model = tf.keras.Model([X,type_in],[c3,eout])\n    return model\n\n","34e38105":"myadam = tf.keras.optimizers.Adam(learning_rate=0.001\/3.0, beta_1=0.9, beta_2=0.999, amsgrad=False)\nif modelnum == 1:\n    model = unet()\n    model.compile('adam', loss='MSE') # for training unet and auto_with_unet\nelif modelnum == 2:\n    model = unet_with_type()\n    model.compile('adam', loss='MSE') # for training unet and auto_with_unet\nelse:\n    model = auto_with_unet()\n    model.compile(myadam,loss=['MSE','categorical_crossentropy'])\nmodel.summary()","d7b23890":"# train with types\nif modelnum == 2:\n    model.fit([train_gray,train_oh],train_color,10,150,validation_split=.05,callbacks=None)\n# train unet\nelif modelnum == 1:\n    model.fit(train_gray, train_color,10,150,validation_split=.05,callbacks=None)\nelse:\n    model.fit([train_gray,train_oh],[train_color,train_oh],10,150,validation_split=.05,callbacks=None)","ec7b85f9":"# examine results when trained with types\nif modelnum == 2:\n    print('MSE: ',model.evaluate([test_gray,test_oh],test_color))\n    preds = model.predict([test_gray,test_oh])\n\n# examine results when trained w\/o types\nelif modelnum == 1:\n    print(\"MSE loss: \", model.evaluate(test_gray,test_color))\n    preds = model.predict(test_gray)\n\n# examine results of auto_with_unet\nelse:\n    print(model.evaluate([test_gray,test_oh],[test_color,test_oh]))\n    preds, garb = model.predict([test_gray,test_oh])\nnumimgs = 10\nf,ax = plt.subplots(numimgs,2,figsize=[6.4*2,4.8*2]) \nf.subplots_adjust(0,0,2,2)\nfor i in range(0,numimgs,1):\n    ax[i,0].imshow(Image.fromarray( (preds[i]*255).astype(np.uint8)))\n    ax[i,1].imshow(Image.fromarray( (test_color[i]*255).astype(np.uint8)))\n    \nx = [axi.set_axis_off() for axi in ax.ravel()]","c66ad3e6":"Let's visualize some of the data","4f0d433c":"The following table contains the MSE of the runs\n\n|Run Num|U-Net|U-Net with type  | U-Net with AE|\n|---|---|---|---|\n|1|0.0027  |0.002419|0.002484|\n|2|0.002521|0.002371|0.002468|\n|3|0.002801|0.002517|0.002581|\n|4|0.003058|0.002242|0.002399|\n|---|---|---|---|\n|Avg|.00277|.002387|.002483|\n\nSo we got about a 12% improvement by including the type information in prediction over the standard U-Net.\n\nThe model with the lowest MSE in a single run is U-Net with the dense input branch with an MSE of 0.002242 ","acbeb94a":"This is a very small dataset, but I decided to try to do some computer vision anyway.\n\nThe goal of the model is to take in a grayscale image of a pokemon it has never seen before and then correctly color. It will be fun to see how close it gets to the right colors. Since I only have one image of every pokemon, you cannot expect it to be too accurate. \n\nI did a bit of data fusion and used the pokemon type data (fire,grass,etc) to improve the model slightly (about 12% in MSE). This is my first model fusing text and image data and my first Kaggle kernel.\n\nI do not believe these are the best architectures to color images, but I chose it because it is an interesting architecture inspired by some papers I have read and I wanted to explore it.\n\n\nThere are three models. 1) A simple u-net, 2) A u-net that has a dense network feed the pokemon type into the bottle neck layer, and 3) A u-net with an autoencoder on the pokemon type data. I built the third model as a method of forcing the second model to be sure and use the type information.\n\nChoose the model you want to run by changing the variable modelnum in the first code box."}}