{"cell_type":{"40241d2d":"code","c6b70d8b":"code","bef8a220":"code","8fb60d24":"code","8fbe6251":"code","eb5b41a9":"code","84bbb40b":"code","8516ac01":"code","4925065e":"code","4b3b3a59":"code","c7c30f70":"code","16bb3a2d":"code","93afecf1":"code","4286299d":"code","50dbfd6f":"code","5417519a":"code","1edaef66":"code","f99b7e6a":"code","91325c9a":"code","38b04180":"code","2708c45a":"markdown","728e5ac8":"markdown","de858cad":"markdown","718f7249":"markdown","7f055cf3":"markdown","8db3b72e":"markdown","8e949a8c":"markdown","e3ea6437":"markdown","496dddda":"markdown","3d9896ca":"markdown","8c9b89cc":"markdown","70e48f27":"markdown","daa133d1":"markdown"},"source":{"40241d2d":"# Familiar imports\nimport numpy as np\nimport pandas as pd\n\n# For ordinal encoding categorical variables, splitting data\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\n\n# For training models\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\n\n#MAE\nfrom sklearn.metrics import mean_absolute_error\n\n#Preprocessing\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder","c6b70d8b":"# Load the training data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\",index_col=0)\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)\n\n# Preview the data\ntrain.head()","bef8a220":"# Separate target from features\ny = train['target']\nfeatures = train.drop(['target'], axis=1)\n\n# Preview features\nfeatures.head()","8fb60d24":"#first I'll check the count of data uniques for each categorical variable. This is important to define the method of transformation of these variables.\n\n# List of categorical columns\nobject_cols = [col for col in features.columns if 'cat' in col]\n\nfor col in object_cols:\n    print(col,features[col].nunique())","8fbe6251":"# ordinal-encode categorical columns\ncols_OE=[\"cat6\",\"cat7\",\"cat8\",\"cat9\"]\n# One Hot-encode categorical columns\ncols_OHE=object_cols[:6]\n\n#transformation categorical variables\nX = features.copy()\nX_test = test.copy()\n\n#Ordinal encoder\nordinal_encoder = OrdinalEncoder()\nX[cols_OE] = ordinal_encoder.fit_transform(features[cols_OE])\nX_test[cols_OE] = ordinal_encoder.transform(test[cols_OE])\n\n# Apply one-hot encoder\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X[cols_OHE]))\nOH_cols_test = pd.DataFrame(OH_encoder.transform(X_test[cols_OHE]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X.index\nOH_cols_test.index = X_test.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X.drop(object_cols, axis=1)\nnum_X_test = X_test.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nX_train_encode = pd.concat([num_X_train, OH_cols_train], axis=1)\nX_test_encode = pd.concat([num_X_test, OH_cols_test], axis=1)\n\n# Preview the ordinal-encoded features\nX_train_encode.head()","eb5b41a9":"X_train_encode.columns.values","84bbb40b":"X_train, X_valid, y_train, y_valid = train_test_split(X_train_encode, y, random_state=0)","8516ac01":"# Define the model \n#model_RFG = RandomForestRegressor(random_state=1)\n\n# Train the model (will take about 10 minutes to run)\n#model_RFG.fit(X_train, y_train)\n#preds_valid_RFG = model_RFG.predict(X_valid)\n#print(mean_squared_error(y_valid, preds_valid_RFG, squared=False))","4925065e":"#Model with XGBoost\n\n#model_xgb = XGBRegressor(n_estimators=1000, learning_rate=0.05) # Your code here\n\n# Fit the model\n#model_xgb.fit(X_train, y_train, \n             #early_stopping_rounds=5, \n             #eval_set=[(X_valid, y_valid)], \n             #verbose=False)\n#preds_valid_xgb = model_xgb.predict(X_valid)\n#print(mean_squared_error(y_valid, preds_valid_xgb, squared=False))","4b3b3a59":"#LGBM Regressor\n#from lightgbm import LGBMRegressor\n\n#model_lgb = LGBMRegressor(n_estimators=1000, learning_rate=0.05) # Your code here\n\n# Fit the model\n#model_lgb.fit(X_train, y_train, \n             #early_stopping_rounds=5, \n             #eval_set=[(X_valid, y_valid)], \n             #verbose=False)\n#preds_valid_lgb = model_lgb.predict(X_valid)\n#print(mean_squared_error(y_valid, preds_valid_lgb, squared=False))","c7c30f70":"#hyper-optimization LGBM\n\n# Skopt functions\nfrom skopt import BayesSearchCV\nfrom skopt.callbacks import DeadlineStopper, DeltaYStopper\nfrom skopt.space import Real, Categorical, Integer\n# Model selection\nfrom sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgb\n","16bb3a2d":"# Metrics\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import make_scorer\n\n# Reporting util for different optimizers\ndef report_perf(optimizer, X, y, title=\"model\", callbacks=None):\n    \"\"\"\n    A wrapper for measuring time and performances of different optmizers\n    \n    optimizer = a sklearn or a skopt optimizer\n    X = the training set \n    y = our target\n    title = a string label for the experiment\n    \"\"\"\n    start = time()\n    \n    if callbacks is not None:\n        optimizer.fit(X, y, callback=callbacks)\n    else:\n        optimizer.fit(X, y)\n        \n    d=pd.DataFrame(optimizer.cv_results_)\n    best_score = optimizer.best_score_\n    best_score_std = d.iloc[optimizer.best_index_].std_test_score\n    best_params = optimizer.best_params_\n    \n    print((title + \" took %.2f seconds,  candidates checked: %d, best CV score: %.3f \"\n           + u\"\\u00B1\"+\" %.3f\") % (time() - start, \n                                   len(optimizer.cv_results_['params']),\n                                   best_score,\n                                   best_score_std))    \n    print('Best parameters:')\n    pprint.pprint(best_params)\n    print()\n    return best_params","93afecf1":"# Converting average precision score into a scorer suitable for model selection\nroc_auc = make_scorer(roc_auc_score, greater_is_better=True, needs_threshold=True)","4286299d":"# Setting a 5-fold stratified cross-validation (note: shuffle=True)\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)","50dbfd6f":"#We set up a generic LightGBM classifier\n\nclf = lgb.LGBMRegressor(learning_rate=0.2,n_estimators=2000)","5417519a":"search_spaces = {\n    'learning_rate': Real(0.01, 1.0, 'log-uniform'),     # Boosting learning rate\n    'n_estimators': Integer(30, 5000),                   # Number of boosted trees to fit\n    'num_leaves': Integer(2, 512),                       # Maximum tree leaves for base learners\n    'max_depth': Integer(-1, 256),                       # Maximum tree depth for base learners, <=0 means no limit\n    'min_child_samples': Integer(1, 256),                # Minimal number of data in one leaf\n    'max_bin': Integer(100, 1000),                       # Max number of bins that feature values will be bucketed\n    'subsample_freq': Integer(0, 10),                    # Frequency of subsample, <=0 means no enable\n    'min_child_weight': Real(0.01, 10.0, 'uniform'),     # Minimum sum of instance weight (hessian) needed in a child (leaf)\n    'reg_lambda': Real(1e-9, 100.0, 'log-uniform'),      # L2 regularization\n    'reg_alpha': Real(1e-9, 100.0, 'log-uniform'),       # L1 regularization\n        }","1edaef66":"def bayes_search_CV_init(self, estimator, search_spaces, optimizer_kwargs=None,\n                         n_iter=50, scoring=None, fit_params=None, n_jobs=1,\n                         n_points=1, iid=True, refit=True, cv=None, verbose=0,\n                         pre_dispatch='2*n_jobs', random_state=None,\n                         error_score='raise', return_train_score=False):\n\n        self.search_spaces = search_spaces\n        self.n_iter = n_iter\n        self.n_points = n_points\n        self.random_state = random_state\n        self.optimizer_kwargs = optimizer_kwargs\n        self._check_search_space(self.search_spaces)\n        self.fit_params = fit_params\n\n        super(BayesSearchCV, self).__init__(\n             estimator=estimator, scoring=scoring,\n             n_jobs=n_jobs, refit=refit, cv=cv, verbose=verbose,\n             pre_dispatch=pre_dispatch, error_score=error_score,\n             return_train_score=return_train_score)\n        \nBayesSearchCV.__init__ = bayes_search_CV_init","f99b7e6a":"opt = BayesSearchCV(estimator=clf,                                    \n                    search_spaces=search_spaces,                      \n                    scoring=roc_auc,                                  \n                    cv=skf,                                           \n                    n_iter=3000,                                      # max number of trials\n                    n_points=3,                                       # number of hyperparameter sets evaluated at the same time\n                    n_jobs=-1,                                        # number of jobs\n                    iid=False,                                        # if not iid it optimizes on the cv score\n                    return_train_score=False,                         \n                    refit=False,                                      \n                    optimizer_kwargs={'base_estimator': 'GP'},        # optmizer parameters: we use Gaussian Process (GP)\n                    random_state=0)                                   # random state for replicability","91325c9a":"clf.fit(X_train, y_train, \n             early_stopping_rounds=10, \n             eval_set=[(X_valid, y_valid)], \n             verbose=False)","38b04180":"# Use the model to generate predictions\npredictions = clf.predict(X_test_encode)\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'target': predictions})\noutput.to_csv('submission.csv', index=False)","2708c45a":"nex function to fix problem with version of sklearn and BayesSearchCV","728e5ac8":"# Step 6: Keep Learning!\n\nIf you're not sure what to do next, you can begin by trying out more model types!\n1. If you took the **[Intermediate Machine Learning](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning)** course, then you learned about **[XGBoost](https:\/\/www.kaggle.com\/alexisbcook\/xgboost)**.  Try training a model with XGBoost, to improve over the performance you got here.\n\n2. Take the time to learn about **Light GBM (LGBM)**, which is similar to XGBoost, since they both use gradient boosting to iteratively add decision trees to an ensemble.  In case you're not sure how to get started, **[here's a notebook](https:\/\/www.kaggle.com\/svyatoslavsokolov\/tps-feb-2021-lgbm-simple-version)** that trains a model on a similar dataset.","de858cad":"The next code cell separates the target (which we assign to `y`) from the training features (which we assign to `features`).","718f7249":"# Step 4: Train a model\n\nNow that the data is prepared, the next step is to train a model.  \n\nIf you took the **[Intro to Machine Learning](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning)** courses, then you learned about **[Random Forests](https:\/\/www.kaggle.com\/dansbecker\/random-forests)**.  In the code cell below, we fit a random forest model to the data.","7f055cf3":"We define a search space, expliciting the key hyper-parameters to optimize and the range where to look for the best values","8db3b72e":"# Step 3: Prepare the data\n\nNext, we'll need to handle the categorical columns (`cat0`, `cat1`, ... `cat9`).  \n\n","8e949a8c":"# Step 2: Load the data\n\nNext, we'll load the training and test data.  \n\nWe set `index_col=0` in the code cell below to use the `id` column to index the DataFrame.  (*If you're not sure how this works, try temporarily removing `index_col=0` and see how it changes the result.*)","e3ea6437":"Welcome to the **[30 Days of ML competition](https:\/\/www.kaggle.com\/c\/30-days-of-ml\/overview)**!  In this notebook, you'll learn how to make your first submission.\n\nBefore getting started, make your own editable copy of this notebook by clicking on the **Copy and Edit** button.\n\n# Step 1: Import helpful libraries\n\nWe begin by importing the libraries we'll need.  Some of them will be familiar from the **[Intro to Machine Learning](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning)** course and the **[Intermediate Machine Learning](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning)** course.","496dddda":"the line above shows that the categorical variables from cat0 to cat5 are possible to handle with the transformation of ***One Hot encode***, while from cat6 to cat9 could be better used ***Ordinal Enconder***","3d9896ca":"Next, we break off a validation set from the training data.","8c9b89cc":"In the code cell above, we set `squared=False` to get the root mean squared error (RMSE) on the validation data.\n\n# Step 5: Submit to the competition\n\nWe'll begin by using the trained model to generate predictions, which we'll save to a CSV file.","70e48f27":"Once you have run the code cell above, follow the instructions below to submit to the competition:\n1. Begin by clicking on the **Save Version** button in the top right corner of the window.  This will generate a pop-up window.  \n2. Ensure that the **Save and Run All** option is selected, and then click on the **Save** button.\n3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n4. Click on the **Output** tab on the right of the screen.  Then, click on the file you would like to submit, and click on the **Submit** button to submit your results to the leaderboard.\n\nYou have now successfully submitted to the competition!\n\nIf you want to keep working to improve your performance, select the **Edit** button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.","daa133d1":"We then define the Bayesian optimization engine, providing to it our LightGBM, the search spaces, the evaluation metric, the cross-validation. We set a large number of possible experiments and some parallelism in the search operations."}}