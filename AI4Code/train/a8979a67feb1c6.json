{"cell_type":{"29f040d4":"code","c6252cc8":"code","3ac77d4e":"code","2fc20c22":"code","9d602106":"code","48ec6ab5":"code","381ad54f":"code","fca914f4":"code","ba768bc1":"code","810898b2":"code","c7f371fe":"code","958a678c":"code","44df6fa0":"code","3eeae148":"code","477da306":"code","80c45ab1":"code","3e9aa26f":"code","cf3e304b":"code","1d9aa9fd":"code","fc469e38":"code","cbe06260":"code","70bd178c":"code","f0c6e017":"code","99ec3ce7":"code","d52c0a6a":"code","d3e65447":"code","f91c9bb2":"code","0752c0d2":"code","66986733":"code","6b77f33d":"code","9d9bab45":"code","9e6a66e4":"code","c435e21a":"code","b21d391e":"code","5e712f93":"code","601424b4":"code","23d13479":"markdown","b704754c":"markdown","f87bb37b":"markdown","ae43df55":"markdown","eb612b08":"markdown","1563a961":"markdown","9ebcbbec":"markdown","eba5a60d":"markdown","d59c4d41":"markdown","4f8009a9":"markdown","cea6666c":"markdown","021d21b6":"markdown","3b25b85c":"markdown","f161984e":"markdown","6c883c40":"markdown","ac7b3044":"markdown","6d1e46e3":"markdown","28f95849":"markdown"},"source":{"29f040d4":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom glob import glob\nimport cv2\nimport os\nimport seaborn as sns\nimport sklearn\nimport zipfile\nimport random\nimport math\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport keras\nfrom keras import layers\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.optimizers import Adam\nfrom keras.utils.data_utils import Sequence\nimport albumentations","c6252cc8":"keypoint_data = pd.read_csv('..\/input\/facial-keypoints-detection\/IdLookupTable.csv')\nkeypoint_data.head(10).T","3ac77d4e":"destination = '..\/kaggle\/working\/'\n\ntrain_archive = zipfile.ZipFile('..\/input\/facial-keypoints-detection\/training.zip')\ntest_archive = zipfile.ZipFile('..\/input\/facial-keypoints-detection\/test.zip')\n\n\n\ndef extraction(archive, destination):\n    for file in archive.namelist():\n        archive.extract(file, destination)\n        \n        \nextraction(train_archive, destination)\nextraction(test_archive, destination)","2fc20c22":"!unzip -n ..\/input\/facial-keypoints-detection\/test.zip\n!unzip -n ..\/input\/facial-keypoints-detection\/training.zip","9d602106":"TRAIN_DIR = '.\/training.csv'\nTEST_DIR = '.\/test.csv'\nLOOKID_DIR = '..\/input\/facial-keypoints-detection\/IdLookupTable.csv'\n","48ec6ab5":"train_data = pd.read_csv(TRAIN_DIR)\ntest_data = pd.read_csv(TEST_DIR)","381ad54f":"train_data.head(10).T","fca914f4":"train_data.info()","ba768bc1":"train_data.isnull().sum()","810898b2":"test_data.head()","c7f371fe":"lookup_data = pd.read_csv(LOOKID_DIR)\nlookup_data.head()","958a678c":"sns.pairplot(train_data)\nplt.savefig('pairplot.png', dpi=300)","44df6fa0":"fig, ax = plt.subplots(6, 5, figsize=(15, 10))\nax = ax.ravel() \nfor i in range(30):\n    ax[i].hist(train_data[train_data.columns[i]], bins=50, density=True, alpha=0.7, color='red')\n    ax[i].set_title(train_data.columns[i],fontsize=10)\n  # ax[i].axes.get_xaxis().set_visible(False)\nplt.tight_layout()  \nplt.savefig('hist_plot.png', dpi=200)","3eeae148":"train_data.fillna(train_data.describe().T['50%'], inplace= True)\ntrain_data.head()","477da306":"train_data.isnull().sum()","80c45ab1":"def load_images(image):\n    images=[]\n    for idx, sample in image.iterrows():\n        img = np.array(sample['Image'].split(' '), dtype=np.float32)\n        img = np.reshape(img, (96, 96,1))\n        images.append(img)\n        \n    images = np.array(images)\/255.0\n    return images\n\n\ndef format_dataset(data):\n    keypoints=[]\n    features = data.drop('Image', axis=1)\n    for idx, sample in features.iterrows():\n        keypoints.append(sample)\n    keypoints = np.array(keypoints, dtype='float')\n    return keypoints\n\n\n\ndef sample(image, keypoint, axis=None, color='red'):\n    if axis is None:\n        fig, axis = plt.subplots()\n        \n    axis.scatter(keypoint[0::2], keypoint[1::2], s=8, c=color, marker='*')\n    axis.imshow(image.squeeze(), cmap='gray')\n    \n    \n        \n\ndef sample_images(image_data,keypoints, n_rows=6, n_cols=4):\n    fig= plt.figure(figsize=(2*n_cols, 2*n_cols), dpi=200)\n    \n    for i, idx in enumerate(np.random.randint(0, len(keypoints), n_rows*n_cols)):\n        ax = fig.add_subplot(n_rows, n_cols, i+1, xticks=[], yticks=[])\n        sample(image_data[idx], keypoints[idx], axis=ax)\n        ","3e9aa26f":"images = load_images(train_data)\nkeypoints=  format_dataset(train_data)\nprint(images.shape)\nprint(keypoints.shape)","cf3e304b":"test_images = load_images(test_data)\ntest_images.shape","1d9aa9fd":"\nsample_images(images, keypoints)","fc469e38":"# Random angles\nangles =[] \nfor i in range(3):\n    n= random.randint(10,20)\n    angles.append(n)\n    \n    \ndef rotation(images, keypoints):\n    rotated_images=[]\n    rotated_keypoints=[]\n    for i in angles:\n        for angle in [i, -i]:\n            rotate = cv2.getRotationMatrix2D((48, 48), angle, 1.0)\n            radius = - angle*math.pi\/180\n            \n            for image in images:\n                image_rotation=  cv2.warpAffine(image, rotate, (96, 96), flags = cv2.INTER_CUBIC)\n                #image_rotation=  np.reshape(image_rotation, ( 96, 96,1))\n                rotated_images.append(np.reshape(image_rotation, (96, 96,1)))\n                \n                \n            for keypoint in keypoints:\n                keypoint = keypoint - 48\n                \n                for idx in range(0, len(keypoint), 2):\n                    keypoint[idx]= keypoint[idx]*math.cos(radius)- keypoint[idx+1]*math.sin(radius)\n                    \n                    keypoint[idx+1]= keypoint[idx]*math.sin(radius)+ keypoint[idx+1]*math.cos(radius)\n                    \n                    \n                keypoint =keypoint + 48\n                    \n                rotated_keypoints.append(keypoint)\n                    \n                    \n    rotated_images = np.array(rotated_images)\/255.0 \n    rotated_keypoints = np.array(rotated_keypoints, dtype='float')\n    return rotated_images, rotated_keypoints\n                    \n        \n                    \nrotated_images, rotated_keypoints = rotation(images, keypoints)\n\nsample_images(rotated_images, rotated_keypoints)\nprint(rotated_images.shape) \nprint(rotated_keypoints.shape)\n            ","cbe06260":"class data_process(Sequence):\n    def __init__(self, image, keypoint, batch_size, augmentation):\n        self.image = image\n        self.keypoint = keypoint\n        self.batch_size = batch_size\n        self.augmentation = augmentation\n        self.shuffle= True\n        self.on_epoch_end()\n        \n    def __len__(self):\n        return int(np.ceil(len(self.image)\/ float(self.batch_size)))\n    \n    \n    def __getitem__(self, idx):\n        indexes = self.indexes[idx*self.batch_size: (idx+1)*self.batch_size]\n        \n        batch_image = self.image[indexes, ...]\n        batch_key = self.keypoint[indexes,:]\n        \n        if self.augmentation is not None:\n            keypoints = np.array([tuple(zip(key[::2], key[1::2])) for key in batch_key])\n            \n            transformed_image = [self.augmentation(image=image, keypoints=keypoint) for image, keypoint in zip(batch_image, keypoints)]\n            \n            batch_image = np.stack([z['image'] for z in transformed_image], axis=0)\n            \n            batch_key = np.stack([np.array(z['keypoints']).flatten(order='C') for z in transformed_image], axis=0)\n            \n        return batch_image, batch_key\n    \n    def on_epoch_end(self):\n        self.indexes = np.arange(len(self.image))\n        if self.shuffle ==True:\n            np.random.shuffle(self.indexes)\n            \n        ","70bd178c":"xtrain, xval,ytrain, yval = train_test_split(images, keypoints, test_size =0.18, shuffle=True)","f0c6e017":"def conv2d_block(input_tensor, filters, kernel_size, activation=\"relu\"):\n    \n    \n    x = keras.layers.Conv2D(filters, kernel_size=(kernel_size, kernel_size), \n                            padding='same', use_bias=False, strides=(1,1))(input_tensor)\n    x = keras.layers.BatchNormalization(axis=3, scale=False)(x)\n    if (activation==None):\n        return x\n        \n    x = keras.layers.Activation(activation='relu')(x)\n\n    return x\n\n\ndef MultiResBlock(filters, input_layers, alpha = 1.67):\n\n\n    W = alpha * filters\n    shortcut= input_layers\n    shortcut = conv2d_block(shortcut, int(W*0.167) + int(W*0.333) +\n                         int(W*0.5), 1,  activation=None)\n\n    conv3x3 = conv2d_block(input_layers, int(W*0.167), 3, activation=\"relu\" )\n\n    conv5x5 = conv2d_block(conv3x3, int(W*0.333), 3, activation=\"relu\")\n\n    conv7x7 = conv2d_block(conv5x5, int(W*0.5), 3, activation=\"relu\")\n\n    out = keras.layers.concatenate([conv3x3, conv5x5, conv7x7], axis=3)\n    out = keras.layers.BatchNormalization(axis=3)(out)\n\n    out = keras.layers.add([shortcut, out])\n    out = keras.layers.Activation('relu')(out)\n    out = keras.layers.BatchNormalization(axis=3)(out)\n\n    return out","99ec3ce7":"def ResPath(filters, length, input_layers):\n\n\n    shortcut= input_layers\n    shortcut = conv2d_block(shortcut, filters, 1,activation=None)\n\n    out = conv2d_block(input_layers, filters, 3, activation=\"relu\")\n\n    out = keras.layers.add([shortcut, out])\n    out = keras.layers.Activation('relu')(out)\n    out = keras.layers.BatchNormalization(axis=3)(out)\n\n    for i in range(length-1):\n\n        shortcut = out\n        shortcut = conv2d_block(shortcut, filters, 1, activation=None)\n\n        out = conv2d_block(out, filters, 3, activation=\"relu\")\n\n        out = keras.layers.add([shortcut, out])\n        out = keras.layers.Activation('relu')(out)\n        out = keras.layers.BatchNormalization(axis=3)(out)\n\n    return out","d52c0a6a":"def conv2d_block_T(input_tensor, filters, kernel_size):\n    x = keras.layers.Conv2DTranspose(filters, kernel_size=(kernel_size, kernel_size), strides=(2, 2), padding=\"same\")(input_tensor)\n    x = keras.layers.BatchNormalization(axis=3, scale=False)(x)\n    \n    return x\n\n\n\n\n\n","d3e65447":"\n\n\ndef MultiResUnet(input_shape):\n\n\n\n    inputs = keras.Input(input_shape)\n\n    mresblock1 = MultiResBlock(32, inputs)\n    pool1 = keras.layers.MaxPooling2D(pool_size=(2, 2))(mresblock1)\n    mresblock1 = ResPath(32, 4, mresblock1)\n\n    mresblock2 = MultiResBlock(32*2, pool1)\n    pool2 = keras.layers.MaxPooling2D(pool_size=(2, 2))(mresblock2)\n    mresblock2 = ResPath(32*2, 3, mresblock2)\n\n    mresblock3 = MultiResBlock(32*4, pool2)\n    pool3 = keras.layers.MaxPooling2D(pool_size=(2, 2))(mresblock3)\n    mresblock3 = ResPath(32*4, 2, mresblock3)\n\n    mresblock4 = MultiResBlock(32*8, pool3)\n    pool4 = keras.layers.MaxPooling2D(pool_size=(2, 2))(mresblock4)\n    mresblock4 = ResPath(32*8, 1, mresblock4)\n\n    mresblock5 = MultiResBlock(32*16, pool4)\n\n    up6 = keras.layers.concatenate([conv2d_block_T(mresblock5, 32*8, 2), mresblock4], axis=3)\n    mresblock6 = MultiResBlock(32*8, up6)\n\n    up7 = keras.layers.concatenate([conv2d_block_T(mresblock6, 32*4, 2), mresblock3], axis=3)\n    mresblock7 = MultiResBlock(32*4, up7)\n\n    up8 = keras.layers.concatenate([conv2d_block_T(mresblock7, 32*2, 2), mresblock2], axis=3)\n    mresblock8 = MultiResBlock(32*2, up8)\n\n    up9 = keras.layers.concatenate([conv2d_block_T(mresblock8, 32, 2), mresblock1], axis=3)\n    mresblock9 = MultiResBlock(32, up9)\n\n    conv10 = keras.layers.Conv2D(1, kernel_size=(1,1), strides=(1,1), padding=\"same\")(mresblock9)\n    conv10 = keras.layers.BatchNormalization( scale=False)(conv10)\n    conv10= keras.layers.Activation(activation=\"sigmoid\")(conv10)\n    \n    out = keras.layers.Flatten()(conv10)\n    out= keras.layers.Dropout(0.1)(out)\n    out= keras.layers.Dense(30)(out)\n\n    model = keras.Model(inputs=[inputs], outputs=[out])\n\n    return model","f91c9bb2":"input_shape=(96, 96,1)\nmodel =MultiResUnet(input_shape)\nmodel.summary()","0752c0d2":"keras.utils.plot_model(model, to_file=\"MutiResUnet.png\", dpi=200)","66986733":"early_stop = EarlyStopping(monitor='val_loss', patience=20)\ncheckpoint = ModelCheckpoint('model.h5', monitor='val_loss',verbose=1, save_best_only=True, save_weights_only=True)\n\n\nmodel.compile(optimizer= Adam(), loss='mean_squared_error',  metrics=['mae', 'acc', 'mse'])\n\n","6b77f33d":"train=  data_process(xtrain, ytrain, batch_size=128, augmentation=None)\n","9d9bab45":"history= model.fit(train,   steps_per_epoch=len(train),\n                   validation_data=(xval, yval), batch_size=128,\n                   epochs=45, verbose=1, callbacks=[early_stop, checkpoint])","9e6a66e4":"def plot_history(metric):\n    plt.plot(history.history[metric])\n    plt.plot(history.history[\"val_{}\".format(metric)])\n    plt.title('{} vs Epoch'.format(metric))\n    plt.ylabel(metric)\n    plt.xlabel('Epochs')\n    plt.legend(['train', 'validation'], loc='upper right')\n    plt.show()\n","c435e21a":"plot_history(\"acc\")\nplot_history(\"loss\")\nplot_history(\"mae\")\nplot_history(\"mse\")","b21d391e":"model.load_weights('.\/model.h5')\n\nprediction= model.predict(test_images)","5e712f93":"sample_images(test_images, prediction)","601424b4":"print(\"Accuracy: {}\".format(np.mean(history.history['acc'])))\nprint(\"Loss: {}\".format(np.mean(history.history['loss'])))\nprint(\"Mean Absolute error: {}\".format(np.mean(history.history['mae'])))\nprint(\"Mean Squared Error: {}\".format(np.mean(history.history['mse'])))","23d13479":"## **Lets look at the train data**","b704754c":"augment = albumentations.Compose([albumentations.ShiftScaleRotate(rotate_limit=20, p=0.5),\n                           albumentations.RandomBrightnessContrast(p=0.5),\n                           albumentations.GaussianBlur(p=0.3),\n                           albumentations.GaussNoise(var_limit=(1e-5, 1e-3), p=0.5)],\n                          keypoint_params=albumentations.KeypointParams(format='xy', remove_invisible=False))","f87bb37b":"### Histogram Plot","ae43df55":"# Different data augmentation","eb612b08":"# **Handling Null Values with feature median values**","1563a961":"# **Or use !unzip**","9ebcbbec":"## Accuracy: 0.832591313123703\n## Loss: 8.787642423311869\n## Mean Absolute error: 0.9814965281221602\n## Mean Squared Error: 8.787642423311869","eba5a60d":"# **Unzip File**","d59c4d41":"## **Lets see the test data**","4f8009a9":"## ***Show Sample Images***","cea6666c":"### Pairplot","021d21b6":"# ","3b25b85c":"Inspired from this beautiful [Notebook](http:\/\/www.kaggle.com\/balraj98\/data-augmentation-for-facial-keypoint-detection) by Balraj Ashwath","f161984e":"# MultiResUnet architecture","6c883c40":"## **Data Preprocessing**","ac7b3044":"## **LOOK ID Directory**","6d1e46e3":"### Rotation","28f95849":"### The MultiresUnet model was taken from the paper [MultiResUNet : Rethinking the U-Net Architecture for Multimodal Biomedical Image Segmentation](https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/S0893608019302503)"}}