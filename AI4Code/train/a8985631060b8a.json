{"cell_type":{"533a2224":"code","3116365f":"code","338b0474":"code","5382b7c0":"code","3ec5bd89":"code","da40494a":"code","260e9c7d":"code","3f16e89e":"code","8ee8ba43":"code","2b0bdc03":"code","a1912984":"code","9e6ef3e7":"code","1b6ccc66":"code","16576cae":"code","21bafc76":"code","597b149e":"code","2a526752":"code","b74a33ef":"code","5fe4844a":"code","0bc4515e":"code","450e930e":"code","6db878c9":"code","d850f0cd":"code","28dba102":"code","3b38ae75":"code","4ff53c53":"code","e5cd3020":"code","07beadc5":"code","39264a1e":"code","96f99d5b":"code","b2c47bdb":"code","1eaefcce":"code","dad75f50":"code","1ca2e99d":"code","2ec919b9":"code","4b064718":"markdown","00f55723":"markdown","b2c99661":"markdown","2cd6e1f3":"markdown","1de067af":"markdown","700d1681":"markdown","f34d0b50":"markdown","010b60a5":"markdown","133e19a3":"markdown","fef73cfe":"markdown","702ef43c":"markdown","14b06e64":"markdown","bb44a7fb":"markdown","71fe7407":"markdown","2ced91af":"markdown","050d6f90":"markdown","3d256136":"markdown","f5565256":"markdown","81751f0b":"markdown","9aa915ba":"markdown"},"source":{"533a2224":"!pip install jsonlines\n#!pip install cPickle","3116365f":"import json\nimport jsonlines\n\nwith open(\"..\/input\/aarticle\/articles.json\", 'r') as f:\n    json_data = json.load(f)\n    \nwith open(\"..\/working\/articles.jl\", 'w') as outfile:\n    for entry in json_data:\n        json.dump(entry, outfile)\n        outfile.write('\\n')","338b0474":"with open('..\/working\/articles.jl') as myfile:\n    firstNLines=myfile.readlines()[0:1]\n    print(firstNLines)","5382b7c0":"import _pickle as pickle\nimport jsonlines\n\nheads = []\ndescs = []\nkeywords = []\n\nwith jsonlines.open('..\/working\/articles.jl','r') as reader:\n    i = 0\n    for obj in reader:\n        if i < 1000:\n            i += 1\n            head = obj[\"title\"]\n            desc = obj[\"text\"]\n            heads.append(head)\n            descs.append(desc)\n            keywords.append(None)\n        else:\n            break\n        \n# print(heads, descs, keywords)\nwith open('..\/working\/tokens.pkl', 'wb') as f:\n    pickle.dump((heads,descs,keywords),f)\n","3ec5bd89":"FN = 'vocabulary-embedding'","da40494a":"seed=42","260e9c7d":"vocab_size = 40000","3f16e89e":"embedding_dim = 300","8ee8ba43":"lower = False # dont lower case the text","2b0bdc03":"import pickle as pickle\nFN0 = 'tokens' # this is the name of the data file which I assume you already have\nwith open('..\/working\/%s.pkl'%FN0, 'rb') as fp:\n    heads, descs, keywords = pickle.load(fp) # keywords are not used in this project","a1912984":"if lower:\n    heads = [h.lower() for h in heads]","9e6ef3e7":"if lower:\n    descs = [h.lower() for h in descs]","1b6ccc66":"i=0\nheads[i]","16576cae":"descs[i]","21bafc76":"type(desc)\n#type(head)","597b149e":"print(keywords[i])","2a526752":"len(heads),len(set(heads))","b74a33ef":"len(desc),len(set(desc))","5fe4844a":"from collections import Counter\ndef get_vocab(combinedText):\n    #  to get vocab and count in another way, \n    words = combinedText.split()\n    vocab = [word for word, word_count in Counter(words).most_common()]\n    return vocab,words\n\nvocab,words = get_vocab(heads[i]+descs[i])","0bc4515e":"print (vocab[:50])\nprint ('...', len(vocab))","450e930e":"import matplotlib.pyplot as plt\n%matplotlib inline\nplt.plot(words[w] for w in vocab);\nplt.gca().set_xscale(\"log\", nonposx='clip')\nplt.gca().set_yscale(\"log\", nonposy='clip')\nplt.title('word distribution in headlines and discription')\nplt.xlabel('rank')\nplt.ylabel('total appearances');\nwarnings.warn(self.msg_depr % (key, alt_key))","6db878c9":"empty = 0 # RNN mask of no data\neos = 1  # end of sentence\nstart_idx = eos+1 # first real word","d850f0cd":"from itertools import chain\ndef get_idx(vocab):\n    word2idx = dict((word, idx+start_idx) for idx,word in enumerate(vocab))\n    word2idx['<empty>'] = empty\n    word2idx['<eos>'] = eos\n    idx2word = dict((idx,word) for word,idx in word2idx.items())\n\n    return vocab, word2idx, idx2word\nvocab,word2idx, idx2word = get_idx(vocab)\n","28dba102":"word2idx = get_idx(vocab)\nprint(len(vocab))\nprint(len(word2idx))\nprint(len(idx2word))","3b38ae75":"!pip install --upgrade tensorflow","4ff53c53":"!pip install keras","e5cd3020":"! pip install -q tqdm flair\nimport numpy as np\nfrom tqdm import tqdm\nglove_name= ! wget \"http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\" -O glove.6B.zip && unzip glove.6B.zip\n\nword2vec = {}\nwith open('glove.6B.300d.txt','r', encoding='utf-8',errors='ignore') as f:\n  for line in tqdm(f, total =400000):\n    #glove_n_symbols = line.decode().split(b\":\")[1]\n    glove_n_symbols = line.split()\n    word = glove_n_symbols[0]\n    #coefs = np.fromstring(glove_n_symbols, \"f\", sep=\" \")\n    #coefs = np.asarray(glove_n_symbols[1:], dtype='float32')\n    coefs = np.asarray([float(val) for val in glove_n_symbols[1:]])\n    word2vec[word] = coefs\n\nprint('Found %s word vectors.' % len(word2vec))\n#http:\/\/nlp.stanford.edu\/data\/glove.42B.300d.zip","07beadc5":"type(word)\nprint(word)","39264a1e":"type(coefs)","96f99d5b":"print(coefs)","b2c47bdb":"print(glove_n_symbols)","1eaefcce":"#print(word2vec)","dad75f50":"#glove_n_symbols = get_ipython().getoutput(u'wc -l {glove_name}')\n#glove_n_symbols = int(glove_n_symbols[0].split()[0])\n#%debug\nglove_n_symbols = int(float(glove_n_symbols[0].split()[0]))\nglove_n_symbols","1ca2e99d":"#%debug\nimport numpy as np\nglove_index_dict = {}\nglove_embedding_weights = np.empty((glove_n_symbols, embedding_dim),axis=0)\nglobale_scale=.1\nprint(glove_embedding_weights)\n\nwith open(glove_name, 'r',encoding='utf-8',errors='ignore') as fp:\n    i = 0\n    for l in fp:\n        l = l.strip().split()\n        w = l[0]\n        glove_index_dict[w] = i\n        glove_embedding_weights[i,:] = map(float,l[1:])\n        i += 1\nglove_embedding_weights *= globale_scale   \n","2ec919b9":"exit","4b064718":"# Read Glove","00f55723":"1.Use the 'content \/ description \/ text' as the 'desc' and the 'title' as the 'head'.\n2.Once you have the data ready save it in a python pickle file as a tuple: (heads, descs, keywords) were heads is a list of all the head strings, descs is a list of all the article strings in the same order and length as heads. I ignore the keywrods information so you can place None.\n\nCode Link - https:\/\/github.com\/KevinDanikowski\/nlp-tensorflow-practice\/blob\/master\/text_summarizer\/create_pickle_file.py ","b2c99661":"# Tokenize text, return vocab in order of usage\n\nLink - https:\/\/github.com\/KevinDanikowski\/nlp-tensorflow-practice\/blob\/master\/text_summarizer\/text_summarizer.py","2cd6e1f3":"2. Converting JSON file to JSONL","1de067af":"# Word Embeddings","700d1681":"**Link of Original Code - https:\/\/github.com\/llSourcell\/How_to_make_a_text_summarizer\/blob\/master\/vocabulary-embedding.ipynb**","f34d0b50":"# Articles description","010b60a5":"# Word Embedding","133e19a3":"# Creating pickle file","fef73cfe":"# Build Vocabulary :","702ef43c":"# Index words","14b06e64":"# Read tokenized headlines and descriptions","bb44a7fb":"#Generate intial word embedding for headlines and description\n\n#The embedding is limited to a fixed vocabulary size (vocab_size) but a vocabulary of all the words that appeared in the data is built.","71fe7407":"# Load Data","2ced91af":"# Headings tuple","050d6f90":"#  GLOVE code","3d256136":"Dataset link : https:\/\/www.kaggle.com\/hsankesara\/medium-articles \n\nOn kaggle I got this dataset in .csv format then:\n\n1. I converted .csv to .json\n2. Convert .json to .jsonl(.jl)\n3. Convert .jl to .pkl","f5565256":"# Installing packages","81751f0b":"# Importing and opening .JSONL dataset","9aa915ba":"1. I converted the file from  .csv to .json  \n\nHere is gdrive link to download the .jsonfile : https:\/\/drive.google.com\/file\/d\/1Qpq18as3I7qR-tgB2q93FBcMCpd1MRed\/view?usp=sharing"}}