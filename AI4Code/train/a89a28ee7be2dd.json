{"cell_type":{"b1eca304":"code","1c886929":"code","53a42fb9":"code","73ea08c5":"code","e2f4b6fe":"code","1f621ae8":"code","3c9afc29":"code","89108b72":"code","fa0a7bd6":"code","f1793b5f":"code","ab069dad":"code","4aae28dd":"code","42781959":"code","f4de0082":"code","e75b0b17":"code","9197025c":"code","f8e73cd3":"code","ff3682e9":"code","3489426a":"code","6d30718b":"code","aede9a43":"code","7483f837":"code","01e12bc1":"code","cd74522b":"code","5617c9ae":"code","540a6105":"code","3652cfbe":"code","32d7e508":"code","277e158d":"code","183ea9da":"code","b5c4aa06":"code","0b94e4d4":"code","278af2d0":"code","8460a61e":"code","1fd712a6":"code","39f0f7e3":"markdown","ef623c07":"markdown","cce17ac8":"markdown","451836b7":"markdown","28d34001":"markdown","d03f4559":"markdown","cab4865e":"markdown","9830fc46":"markdown","2ec88c83":"markdown"},"source":{"b1eca304":"import pandas as pd\nimport numpy as np\npd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 100)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","1c886929":"houseTrainRaw = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\nhouseTestRaw = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","53a42fb9":"houseTrain = houseTrainRaw.copy()\nhouseTest = houseTestRaw.copy()","73ea08c5":"from sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom datetime import datetime as dt","e2f4b6fe":"# add an attribute year age \nclass YearsToAges(BaseEstimator, TransformerMixin):\n    def __init__(self, yearCols):\n        self.cols = yearCols\n    \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X, y = None):\n        for col in self.cols:\n            X[col + 'Age'] = dt.now().year - X[col]\n            X = X.drop(columns = col).rename(columns = {col + 'Age': col})\n        return X ","1f621ae8":"# track missing columns before imputing if needed\nclass AddMissingIndicator(BaseEstimator, TransformerMixin):\n    def __init__(self, include_missing_cols = False):\n        self.include_missing_cols = include_missing_cols\n    \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X, y = None):\n        if self.include_missing_cols == True:\n            cols = X.columns\n            for col in cols:\n                X[col + '_MissingInd'] = pd.isna(X[col])\n            return X\n        else:\n            return X","3c9afc29":"# select numeric VS categorical attributes\nclass NumCatSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names, include_missing_cols = False):\n        self.attribute_names = attribute_names\n        self.include_missing_cols = include_missing_cols\n        \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X, y = None):\n        if self.include_missing_cols == True:\n            missingCols = [col + '_MissingInd' for col in self.attribute_names]\n            return pd.concat([X[self.attribute_names], X[missingCols]], axis = 1)\n        else:\n            return X[self.attribute_names]","89108b72":"# process numeric attributes\nclass ProcessNumAttr(BaseEstimator, TransformerMixin):\n    def __init__(self, include_missing_cols = False):\n        self.include_missing_cols = include_missing_cols\n    \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X, y = None):\n        imputer = SimpleImputer(strategy = 'median')\n        scaler = StandardScaler()\n        if self.include_missing_cols == True:\n            missingCols = [col for col in X.columns if col.endswith('_MissingInd')]\n            cols = X.drop(columns = missingCols).columns\n            XImp = imputer.fit_transform(X[cols])\n            XScale = scaler.fit_transform(XImp)\n            return np.c_[XScale, X[missingCols]]\n        \n        else:\n            XImp = imputer.fit_transform(X)\n            XScale = scaler.fit_transform(XImp)\n            return XScale","fa0a7bd6":"# process categorical features\nclass ProcessCatAttr(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X, y = None):\n        for col in X.columns:\n            X[col] = X[col].astype('object')\n            X.loc[X[col].isnull(), col] = 'No Feature'\n\n        encoder = OneHotEncoder(handle_unknown = 'ignore')\n        return encoder.fit_transform(X)","f1793b5f":"# put them all together\n# categorize columns\nIdCol = ['Id']\nlabel = ['SalePrice']\nnum = [\n    'YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold', 'LotArea', 'LotFrontage', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', \n    'LowQualFinSF', 'GrLivArea', 'BsmtFullBath','BsmtHalfBath', 'FullBath', 'HalfBath', 'TotRmsAbvGrd', 'BedroomAbvGr', 'KitchenAbvGr', 'Fireplaces', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', \n    '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'OverallQual', 'OverallCond'\n]\nyrCols = ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold']\ncat = houseTrain.drop(columns = IdCol + label + num, axis = 1).columns\n\n# Numeric attributes pipeline\nnum_pipeline = Pipeline([\n    ('years_to_ages', YearsToAges(yrCols)),\n    ('add_missing_ind', AddMissingIndicator(False)),\n    ('select_num_attr', NumCatSelector(num, False)),\n    ('process_num_attr', ProcessNumAttr(False))\n])\n\n# categorical attributes pipeline\ncat_pipeline = Pipeline([\n    ('add_missing_ind', AddMissingIndicator(False)),\n    ('select_cat_attr', NumCatSelector(cat, False)),\n    ('process_cat_attr', ProcessCatAttr())\n])\n\nfull_pipeline = FeatureUnion(\n    transformer_list = [\n        ('num_pipeline', num_pipeline),\n        ('cat_pipeline', cat_pipeline)\n    ]\n)","ab069dad":"houseTrainClean = full_pipeline.fit_transform(houseTrain)","4aae28dd":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet\nfrom sklearn.svm import SVR, LinearSVR\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, StackingRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, r2_score \nimport time as t","42781959":"def batch_fit_models(xT, yT, xV, yV, models):\n\n    # initiate a dictionary to record model results\n    resultCols = [\n        'Model', 'Train Time', \n        'Train RMSE', 'Validation RMSE',\n        'Train MAE', 'Validation MAE',\n        'Train MSLE', 'Validation MSLE',\n        'Train R2', 'Validation R2'\n    ]\n\n    result = dict([(key, []) for key in resultCols])\n    \n    # batch train models\n    for model_name, model in models.items():\n        \n        result['Model'].append(model_name)\n        \n        # train model and record time laps\n        trainStart = t.process_time()\n        fit = model.fit(xT, yT)\n        trainEnd = t.process_time()\n        \n        # back fit the model on train data\n        predTrain = fit.predict(xT)\n        \n        # fit the model on validation data\n        predValid = fit.predict(xV)\n        \n        # create data for result dict\n        result['Train Time'].append(trainEnd - trainStart)\n        result['Train RMSE'].append(np.sqrt(mean_squared_error(yT, predTrain)))\n        result['Validation RMSE'].append(np.sqrt(mean_squared_error(yV, predValid)))\n        result['Train MAE'].append(mean_absolute_error(yT, predTrain))\n        result['Validation MAE'].append(mean_absolute_error(yV, predValid))\n        result['Train MSLE'].append(mean_squared_log_error(yT, predTrain))\n        result['Validation MSLE'].append(mean_squared_log_error(yV, predValid))\n        result['Train R2'].append(r2_score(yT, predTrain))\n        result['Validation R2'].append(r2_score(yV, predValid))\n        \n    # turn result dict into a df\n    dfResult = pd.DataFrame.from_dict(result)\n    \n    return dfResult","f4de0082":"y = houseTrain[label]\nxTrain, xValid, yTrain, yValid = train_test_split(houseTrainClean, y, test_size = 0.2, random_state = 1206)","e75b0b17":"modelsToFit = {\n    'Linear Regression': LinearRegression(),\n    'Ridge': Ridge(alpha = 0.1, random_state = 777),\n    'Lasso': Lasso(alpha = 0.1, random_state = 777),\n    'Elastic Net': ElasticNet(alpha = 0.1, random_state = 777),\n    'Logistic Regression': LogisticRegression(random_state = 777),\n    'SVR (linear kernel)': SVR(kernel = 'linear'),\n    'Linear SVR': LinearSVR(random_state = 777),\n    'Random Forest': RandomForestRegressor(random_state = 777),\n    'AdaBoost': AdaBoostRegressor(random_state = 777),\n    'GBR': GradientBoostingRegressor(random_state = 777),\n    'Stacked Regressors': StackingRegressor(estimators = [('linear_reg', LinearRegression()), ('ridge', Ridge(alpha = 0.1, random_state = 777)), ('lasso', Lasso(alpha = 0.1, random_state = 777)), ('linear_svr', LinearSVR(random_state = 777)), ('linear_kernel_svm', SVR(kernel = 'linear')), ('rf', RandomForestRegressor(random_state = 777)), ('adaboost', AdaBoostRegressor(random_state = 777)), ('gbr', GradientBoostingRegressor(random_state = 777))], final_estimator = ElasticNet(alpha = 0.1, random_state = 777))\n}","9197025c":"baselineModel = batch_fit_models(xTrain, yTrain, xValid, yValid, modelsToFit)\nbaselineModel.sort_values(by = 'Validation RMSE')","f8e73cd3":"# Numeric attributes pipeline\nnum_pipeline = Pipeline([\n    ('years_to_ages', YearsToAges(yrCols)),\n    ('add_missing_ind', AddMissingIndicator(True)),\n    ('select_num_attr', NumCatSelector(num, True)),\n    ('process_num_attr', ProcessNumAttr(True))\n])\n\n# categorical attributes pipeline\ncat_pipeline = Pipeline([\n    ('add_missing_ind', AddMissingIndicator(True)),\n    ('select_cat_attr', NumCatSelector(cat, True)),\n    ('process_cat_attr', ProcessCatAttr())\n])\n\nfull_pipeline = FeatureUnion(\n    transformer_list = [\n        ('num_pipeline', num_pipeline),\n        ('cat_pipeline', cat_pipeline)\n    ]\n)","ff3682e9":"houseTrainMisInd = full_pipeline.fit_transform(houseTrain)\nxTrain2, xValid2, yTrain2, yValid2 = train_test_split(houseTrainMisInd, y, test_size = 0.2, random_state = 1206)","3489426a":"baselineModelMisInd = batch_fit_models(xTrain2, yTrain2, xValid2, yValid2, modelsToFit)\nbaselineModelMisInd.sort_values(by = 'Validation RMSE')","6d30718b":"from sklearn.feature_selection import GenericUnivariateSelect, RFECV, SelectFromModel, f_regression, mutual_info_regression","aede9a43":"def feature_selection_strategy(xT, yT, xV, yV, strats):\n    \n    # initiate a dictionary to record model results\n    resultCols = [\n        'Strategy', 'Train Time', \n        'Train RMSE', 'Validation RMSE',\n        'Train MAE', 'Validation MAE',\n        'Train MSLE', 'Validation MSLE',\n        'Train R2', 'Validation R2'\n    ]\n\n    result = dict([(key, []) for key in resultCols])\n    \n    # fit a stacked regression to data\n    estimators = [\n        ('linear_reg', LinearRegression()), \n        ('ridge', Ridge(alpha = 0.1, random_state = 777)), \n        ('lasso', Lasso(alpha = 0.1, random_state = 777)), \n        ('linear_svr', LinearSVR(random_state = 777)), \n        ('linear_kernel_svm', SVR(kernel = 'linear')), \n        ('rf', RandomForestRegressor(random_state = 777)), \n        ('adaboost', AdaBoostRegressor(random_state = 777)), \n        ('gbr', GradientBoostingRegressor(random_state = 777)) \n    ]\n\n    stackedRegressor = StackingRegressor(estimators = estimators, final_estimator = ElasticNet(alpha = 0.1, random_state = 777))\n    \n    # batch train models\n    for strat_name, strat in strats.items():\n        \n        result['Strategy'].append(strat_name)\n \n        # transform data, train model and record time laps\n    \n        trainStart = t.process_time()\n        selector = strat.fit(xT, yT)\n        xTU = selector.transform(xT)\n        xVU = selector.transform(xV)\n        fit = stackedRegressor.fit(xTU, yT)\n        trainEnd = t.process_time()\n        \n        # back fit the model on train data\n        predTrain = fit.predict(xTU)\n        \n        # fit the model on validation data\n        predValid = fit.predict(xVU)\n        \n        # create data for result dict\n        result['Train Time'].append(trainEnd - trainStart)\n        result['Train RMSE'].append(np.sqrt(mean_squared_error(yT, predTrain)))\n        result['Validation RMSE'].append(np.sqrt(mean_squared_error(yV, predValid)))\n        result['Train MAE'].append(mean_absolute_error(yT, predTrain))\n        result['Validation MAE'].append(mean_absolute_error(yV, predValid))\n        result['Train MSLE'].append(mean_squared_log_error(yT, predTrain))\n        result['Validation MSLE'].append(mean_squared_log_error(yV, predValid))\n        result['Train R2'].append(r2_score(yT, predTrain))\n        result['Validation R2'].append(r2_score(yV, predValid))\n        \n    # turn result dict into a df\n    dfResult = pd.DataFrame.from_dict(result)\n    \n    return dfResult","7483f837":"featureSelectionStrats = {\n    'K Best': GenericUnivariateSelect(mutual_info_regression, 'k_best', 20),\n    'Percentile': GenericUnivariateSelect(mutual_info_regression, 'percentile', 10),\n    'RFECV': RFECV(ElasticNet(alpha = 0.1, random_state = 777), scoring = 'neg_root_mean_squared_error'),\n    'From Model': SelectFromModel(ElasticNet(alpha = 0.1, random_state = 777))\n}","01e12bc1":"featureSelectionResults = feature_selection_strategy(xTrain, yTrain, xValid, yValid, featureSelectionStrats)","cd74522b":"featureSelectionResults.sort_values(by = 'Validation RMSE')","5617c9ae":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV","540a6105":"# Complete data\/model pipeline\n# put them all together\n# categorize columns\nIdCol = ['Id']\nlabel = ['SalePrice']\nnum = [\n    'YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold', 'LotArea', 'LotFrontage', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', \n    'LowQualFinSF', 'GrLivArea', 'BsmtFullBath','BsmtHalfBath', 'FullBath', 'HalfBath', 'TotRmsAbvGrd', 'BedroomAbvGr', 'KitchenAbvGr', 'Fireplaces', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', \n    '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'OverallQual', 'OverallCond'\n]\nyrCols = ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold']\ncat = houseTrain.drop(columns = IdCol + label + num, axis = 1).columns\n\n# Numeric attributes pipeline\nnum_pipeline = Pipeline([\n    ('years_to_ages', YearsToAges(yrCols)),\n    ('add_missing_ind', AddMissingIndicator(False)),\n    ('select_num_attr', NumCatSelector(num, False)),\n    ('process_num_attr', ProcessNumAttr(False))\n])\n\n# categorical attributes pipeline\ncat_pipeline = Pipeline([\n    ('add_missing_ind', AddMissingIndicator(False)),\n    ('select_cat_attr', NumCatSelector(cat, False)),\n    ('process_cat_attr', ProcessCatAttr())\n])\n\ndata_transformation = FeatureUnion(\n    transformer_list = [\n        ('num_pipeline', num_pipeline),\n        ('cat_pipeline', cat_pipeline)\n    ]\n)\n\nfull_pipeline = Pipeline([\n    ('data_transformation', data_transformation),\n    ('feature_selection', SelectFromModel(ElasticNet(alpha = 0.1, random_state = 777)))\n])","3652cfbe":"# stacked regression model\nestimators = [\n        ('linear_reg', LinearRegression()), \n        ('ridge', Ridge(random_state = 777)), \n        ('lasso', Lasso(random_state = 777)), \n        ('linear_svr', LinearSVR(random_state = 777)), \n        ('linear_kernel_svm', SVR(kernel = 'linear')), \n        ('rf', RandomForestRegressor(random_state = 777)), \n        ('adaboost', AdaBoostRegressor(random_state = 777)), \n        ('gbr', GradientBoostingRegressor(random_state = 777)) \n    ]\n\nstackedRegressor = StackingRegressor(estimators = estimators, final_estimator = ElasticNet(random_state = 777))\n\nmodelParaGrid = {\n    'ridge__alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n    'lasso__alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n    'linear_svr__C': [1, 10, 100, 1000],\n    'linear_kernel_svm__C': [1, 10, 100, 1000],\n    'rf__n_estimators': [100, 500, 1000],\n    'rf__max_depth': [3, 5, 10],\n    'adaboost__n_estimators': [50, 100, 500],\n    'adaboost__learning_rate': [0.005, 0.01, 0.1, 1],\n    'gbr__n_estimators': [100, 500, 1000],\n    'gbr__learning_rate': [0.005, 0.01, 0.1, 1],\n    'gbr__min_samples_leaf': [5, 10, 100],\n    'final_estimator__alpha': [0.001, 0.01, 0.1, 1, 5]\n}\n\nrandomSearchStackedReg = RandomizedSearchCV(stackedRegressor, modelParaGrid, cv = 5, scoring = 'neg_mean_squared_error', n_iter = 5, verbose = 3, n_jobs = -1)","32d7e508":"houseTrainFinal = full_pipeline.fit_transform(houseTrain, y)\nrandomSearchStackedReg.fit(houseTrainFinal, y)","277e158d":"modelCVResults = randomSearchStackedReg.cv_results_\nfor mean_score, params in zip(modelCVResults['mean_test_score'], modelCVResults['params']):\n    print(np.sqrt(-mean_score), params)","183ea9da":"randomSearchStackedReg.best_estimator_","b5c4aa06":"# finalize full pipeline with stack regressor\nbestStackedRegressor = randomSearchStackedReg.best_estimator_\n\n# Numeric attributes pipeline\nnum_pipeline = Pipeline([\n    ('years_to_ages', YearsToAges(yrCols)),\n    ('selector', NumCatSelector(num)),\n    ('imputer', SimpleImputer(strategy = 'median')),\n    ('scaler', StandardScaler())\n])\n\n# categorical attributes pipeline\ncat_pipeline = Pipeline([\n    ('selector', NumCatSelector(cat)),\n    ('imputer', SimpleImputer(strategy = 'constant', fill_value = 'No Feature')),\n    ('encoder', OneHotEncoder(handle_unknown = 'ignore'))\n])\n\ndata_transformation = FeatureUnion(\n    transformer_list = [\n        ('num_pipeline', num_pipeline),\n        ('cat_pipeline', cat_pipeline)\n    ]\n)\n\nfull_pipeline_updated = Pipeline([\n    ('data_transformation', data_transformation),\n    ('feature_selection', SelectFromModel(ElasticNet(alpha = 0.1, random_state = 777))),\n    ('stack_regression', bestStackedRegressor)\n])","0b94e4d4":"model = full_pipeline_updated.fit(houseTrain, y)","278af2d0":"testID = houseTest['Id']\ntestPred = model.predict(houseTest)","8460a61e":"submission = pd.concat([testID, pd.DataFrame(testPred)], axis = 1)\nsubmission = submission.rename(columns = {0: 'SalePrice'})\nsubmission.to_csv('house_prices_submission_20200705.csv', index = False)","1fd712a6":"submission","39f0f7e3":"## Data processing\n* Turn built years into ages until current date\/time\n* Add in an indicator to show if a record has missing values for certain attributes (make it optional so that can be added\/dropped in grid search)\n* Categorize numeric and categorical attributes and process them separately","ef623c07":"## Hyperparameter tuning\n* What is the best hyperparameter combination based on no missing indicators and model?\n* Do we want to include missing indicators?\n* Which model should be used for feature selection?","cce17ac8":"## Model Training\n* Split data set into train and test\n* Batch train several models and pick the best performer OR try stacking the regressors\n* Grid search with CV on hyperparameter tuning - full data and model pipeline will be used","451836b7":"In my previous notebook I did a high-level EDA on all the features - checked missing values and explored a list of numeric and categorical attributes that could be useful in training the regression model (you can find my previous notebook here if you are interested: [Predicting House Prices - Data Processing and EDA](https:\/\/www.kaggle.com\/biyuyang\/predicting-house-prices-data-processing-and-eda)). In this notebook, I will explore what's the best model as well as the features that contribute most to the model performances. Below are the topics in this notebook:\n* Write up a data pipeline to execute basic data transformation\n* Batch train some baseline models and pick one for further training\n* Update the data processing pipeline with model tuning and feature selection processes","28d34001":"Looks like the best strategy is to select based on a model of choice. The question is then which model should be the best. Will search for the best combination of everything.","d03f4559":"The results show that stacked regressors can perform better than individual regressors (although more regularization is probably needed - noticed overfitting from differences in train and validation RMSE). What about adding the missing indicators on each column? ","cab4865e":"## Feature selection\n* Univariate feature selection\n* Recursive feature elimination\n* Based on model","9830fc46":"## Submission!\n* Fit the full pipeline on train data\n* Apply it on test data\n* Submit!","2ec88c83":"Hmmm... Looks like we should forget about adding missing indicators?"}}