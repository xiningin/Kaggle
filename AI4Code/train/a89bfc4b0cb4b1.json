{"cell_type":{"6e751eaf":"code","0dbc6cb5":"code","84685f1e":"code","71123d1b":"code","dbbbafbe":"code","b52dce77":"code","4eca9f2c":"code","cf34682d":"code","9a177bdb":"code","7061fa2c":"code","54047524":"code","8afd0df0":"code","f1c86d7a":"code","2b08f9bf":"code","ded3e20a":"code","8fc869d7":"code","27892f46":"markdown","5ced6593":"markdown","fac2fa83":"markdown","b1dcd1ea":"markdown","9ec38f18":"markdown","fe7bce69":"markdown","7b58acf5":"markdown","6a1df873":"markdown","098e3be8":"markdown","b3d41020":"markdown","01f0aa1a":"markdown","abfebba9":"markdown"},"source":{"6e751eaf":"import gc\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom  datetime import datetime, timedelta","0dbc6cb5":"# Correct data types for \"calendar.csv\"\ncalendarDTypes = {\"event_name_1\": \"category\", \n                  \"event_name_2\": \"category\", \n                  \"event_type_1\": \"category\", \n                  \"event_type_2\": \"category\", \n                  \"weekday\": \"category\", \n                  'wm_yr_wk': 'int16', \n                  \"wday\": \"int16\",\n                  \"month\": \"int16\", \n                  \"year\": \"int16\", \n                  \"snap_CA\": \"float32\", \n                  'snap_TX': 'float32', \n                  'snap_WI': 'float32' }\n\n# Read csv file\ncalendar = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/calendar.csv\", \n                       dtype = calendarDTypes)\n\ncalendar[\"date\"] = pd.to_datetime(calendar[\"date\"])\n\n# Transform categorical features into integers\nfor col, colDType in calendarDTypes.items():\n    if colDType == \"category\":\n        calendar[col] = calendar[col].cat.codes.astype(\"int16\")\n        calendar[col] -= calendar[col].min()\n\ncalendar.head()","84685f1e":"# Correct data types for \"sell_prices.csv\"\npriceDTypes = {\"store_id\": \"category\", \n               \"item_id\": \"category\", \n               \"wm_yr_wk\": \"int16\",\n               \"sell_price\":\"float32\"}\n\n# Read csv file\nprices = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/sell_prices.csv\", \n                     dtype = priceDTypes)\n\n# Transform categorical features into integers\nfor col, colDType in priceDTypes.items():\n    if colDType == \"category\":\n        prices[col] = prices[col].cat.codes.astype(\"int16\")\n        prices[col] -= prices[col].min()\n        \nprices.head()","71123d1b":"firstDay = 250\nlastDay = 1913\n\n# Use x sales days (columns) for training\nnumCols = [f\"d_{day}\" for day in range(firstDay, lastDay+1)]\n\n# Define all categorical columns\ncatCols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n\n# Define the correct data types for \"sales_train_validation.csv\"\ndtype = {numCol: \"float32\" for numCol in numCols} \ndtype.update({catCol: \"category\" for catCol in catCols if catCol != \"id\"})\n\n# Read csv file\nds = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/sales_train_validation.csv\", \n                 usecols = catCols + numCols, dtype = dtype)\n\n# Transform categorical features into integers\nfor col in catCols:\n    if col != \"id\":\n        ds[col] = ds[col].cat.codes.astype(\"int16\")\n        ds[col] -= ds[col].min()\n        \nds = pd.melt(ds,\n             id_vars = catCols,\n             value_vars = [col for col in ds.columns if col.startswith(\"d_\")],\n             var_name = \"d\",\n             value_name = \"sales\")\n\n# Merge \"ds\" with \"calendar\" and \"prices\" dataframe\nds = ds.merge(calendar, on = \"d\", copy = False)\nds = ds.merge(prices, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], copy = False)\n\nds.head()","dbbbafbe":"dayLags = [7, 28]\nlagSalesCols = [f\"lag_{dayLag}\" for dayLag in dayLags]\nfor dayLag, lagSalesCol in zip(dayLags, lagSalesCols):\n    ds[lagSalesCol] = ds[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(dayLag)\n    \nwindows = [7, 28]\nfor window in windows:\n    for dayLag, lagSalesCol in zip(dayLags, lagSalesCols):\n        ds[f\"rmean_{dayLag}_{window}\"] = ds[[\"id\", lagSalesCol]].groupby(\"id\")[lagSalesCol].transform(lambda x: x.rolling(window).mean())","b52dce77":"dateFeatures = {\"wday\": \"weekday\",\n                \"week\": \"weekofyear\",\n                \"month\": \"month\",\n                \"quarter\": \"quarter\",\n                \"year\": \"year\",\n                \"mday\": \"day\"}\n\nfor featName, featFunc in dateFeatures.items():\n    if featName in ds.columns:\n        ds[featName] = ds[featName].astype(\"int16\")\n    else:\n        ds[featName] = getattr(ds[\"date\"].dt, featFunc).astype(\"int16\")","4eca9f2c":"ds.head()","cf34682d":"ds.info()","9a177bdb":"# Remove all rows with NaN value\nds.dropna(inplace = True)\n\n# Define columns that need to be removed\nunusedCols = [\"id\", \"date\", \"sales\",\"d\", \"wm_yr_wk\", \"weekday\"]\ntrainCols = ds.columns[~ds.columns.isin(unusedCols)]\nX_train = ds[trainCols]\ny_train = ds[\"sales\"]","7061fa2c":"np.random.seed(777)\n\n# Define categorical features\ncatFeats = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id'] + \\\n           [\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\"]\n\nvalidInds = np.random.choice(X_train.index.values, 2_000_000, replace = False)\ntrainInds = np.setdiff1d(X_train.index.values, validInds)\n\ntrainData = lgb.Dataset(X_train.loc[trainInds], label = y_train.loc[trainInds], \n                        categorical_feature = catFeats, free_raw_data = False)\nvalidData = lgb.Dataset(X_train.loc[validInds], label = y_train.loc[validInds],\n                        categorical_feature = catFeats, free_raw_data = False)","54047524":"del ds, X_train, y_train, validInds, trainInds ; gc.collect()","8afd0df0":"params = {\n          \"objective\" : \"poisson\",\n          \"metric\" :\"rmse\",\n          \"force_row_wise\" : True,\n          \"learning_rate\" : 0.075,\n          \"sub_row\" : 0.75,\n          \"bagging_freq\" : 1,\n          \"lambda_l2\" : 0.1,\n          \"metric\": [\"rmse\"],\n          'verbosity': 1,\n          'num_iterations' : 1200,\n          'num_leaves': 128,\n          \"min_data_in_leaf\": 100,\n         }","f1c86d7a":"# Train LightGBM model\nm_lgb = lgb.train(params, trainData, valid_sets = [validData], verbose_eval = 20) ","2b08f9bf":"# Save the model\nm_lgb.save_model(\"model.lgb\")","ded3e20a":"# Last day used for training\ntrLast = 1913\n# Maximum lag day\nmaxLags = 57\n\n# Create dataset for predictions\ndef create_ds():\n    \n    startDay = trLast - maxLags\n    \n    numCols = [f\"d_{day}\" for day in range(startDay, trLast + 1)]\n    catCols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n    \n    dtype = {numCol:\"float32\" for numCol in numCols} \n    dtype.update({catCol: \"category\" for catCol in catCols if catCol != \"id\"})\n    \n    ds = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/sales_train_validation.csv\", \n                     usecols = catCols + numCols, dtype = dtype)\n    \n    for col in catCols:\n        if col != \"id\":\n            ds[col] = ds[col].cat.codes.astype(\"int16\")\n            ds[col] -= ds[col].min()\n    \n    for day in range(trLast + 1, trLast+ 28 +1):\n        ds[f\"d_{day}\"] = np.nan\n    \n    ds = pd.melt(ds,\n                 id_vars = catCols,\n                 value_vars = [col for col in ds.columns if col.startswith(\"d_\")],\n                 var_name = \"d\",\n                 value_name = \"sales\")\n    \n    ds = ds.merge(calendar, on = \"d\", copy = False)\n    ds = ds.merge(prices, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], copy = False)\n    \n    return ds\n\ndef create_features(ds):          \n    dayLags = [7, 28]\n    lagSalesCols = [f\"lag_{dayLag}\" for dayLag in dayLags]\n    for dayLag, lagSalesCol in zip(dayLags, lagSalesCols):\n        ds[lagSalesCol] = ds[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(dayLag)\n\n    windows = [7, 28]\n    for window in windows:\n        for dayLag, lagSalesCol in zip(dayLags, lagSalesCols):\n            ds[f\"rmean_{dayLag}_{window}\"] = ds[[\"id\", lagSalesCol]].groupby(\"id\")[lagSalesCol].transform(lambda x: x.rolling(window).mean())\n          \n    dateFeatures = {\"wday\": \"weekday\",\n                    \"week\": \"weekofyear\",\n                    \"month\": \"month\",\n                    \"quarter\": \"quarter\",\n                    \"year\": \"year\",\n                    \"mday\": \"day\"}\n\n    for featName, featFunc in dateFeatures.items():\n        if featName in ds.columns:\n            ds[featName] = ds[featName].astype(\"int16\")\n        else:\n            ds[featName] = getattr(ds[\"date\"].dt, featFunc).astype(\"int16\")","8fc869d7":"fday = datetime(2016,4, 25) \nalphas = [1.028, 1.023, 1.018]\nweights = [1\/len(alphas)] * len(alphas)\nsub = 0.\n\nfor icount, (alpha, weight) in enumerate(zip(alphas, weights)):\n\n    te = create_ds()\n    cols = [f\"F{i}\" for i in range(1,29)]\n\n    for tdelta in range(0, 28):\n        day = fday + timedelta(days=tdelta)\n        print(tdelta, day)\n        tst = te[(te['date'] >= day - timedelta(days=maxLags)) & (te['date'] <= day)].copy()\n        create_features(tst)\n        tst = tst.loc[tst['date'] == day , trainCols]\n        te.loc[te['date'] == day, \"sales\"] = alpha * m_lgb.predict(tst) # magic multiplier by kyakovlev\n\n    te_sub = te.loc[te['date'] >= fday, [\"id\", \"sales\"]].copy()\n    te_sub[\"F\"] = [f\"F{rank}\" for rank in te_sub.groupby(\"id\")[\"id\"].cumcount()+1]\n    te_sub = te_sub.set_index([\"id\", \"F\" ]).unstack()[\"sales\"][cols].reset_index()\n    te_sub.fillna(0., inplace = True)\n    te_sub.sort_values(\"id\", inplace = True)\n    te_sub.reset_index(drop=True, inplace = True)\n    te_sub.to_csv(f\"submission_{icount}.csv\",index=False)\n    if icount == 0 :\n        sub = te_sub\n        sub[cols] *= weight\n    else:\n        sub[cols] += te_sub[cols]*weight\n    print(icount, alpha, weight)\n\n\nsub2 = sub.copy()\nsub2[\"id\"] = sub2[\"id\"].str.replace(\"validation$\", \"evaluation\")\nsub = pd.concat([sub, sub2], axis=0, sort=False)\nsub.to_csv(\"submission.csv\",index=False)","27892f46":"### Sales features","5ced6593":"### Date features","fac2fa83":"# Predictions","b1dcd1ea":"### *sales_train_validation.csv*","9ec38f18":"# Model","fe7bce69":"### Split dataset into train and validation set","7b58acf5":"## Create features","6a1df873":"### *calendar.csv*","098e3be8":"### *sell_prices.csv*","b3d41020":"## Define the correct data type for each column in the datasets","01f0aa1a":"# Prepare Datasets for Training","abfebba9":"### Remove unnecessary rows and columns"}}