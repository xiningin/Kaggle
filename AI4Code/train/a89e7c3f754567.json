{"cell_type":{"1af75c37":"code","414bd6fb":"code","6b295f46":"code","27ff22cd":"code","9163f590":"code","6c24c149":"code","22b94c5c":"code","0b2d683f":"code","8e9000ec":"code","52caf466":"code","bc0c319c":"code","6e328124":"code","daf18372":"code","76a52c00":"code","47b3dc33":"code","8d54b7a3":"code","6ba179b9":"code","0f19ac93":"code","2db8aef8":"code","6b421f9a":"code","61d498a6":"code","bea4067b":"code","75aefbe4":"code","79cbdde5":"code","ee64ba1c":"code","6308fb95":"code","d3cafcfd":"code","4e8f9e88":"code","c874af76":"code","f807a0b2":"markdown","346e99e5":"markdown","32a222b1":"markdown","c2c7c97d":"markdown","ac1cfcfd":"markdown","f88d827b":"markdown","b9c45791":"markdown","af4de1f0":"markdown","1cf3aaa5":"markdown","8afc616b":"markdown","24399a70":"markdown","cdb5e77c":"markdown","54f73ce6":"markdown","36c4e95c":"markdown","a9feb656":"markdown","e51b45a1":"markdown","1d631a9d":"markdown","6f6251b0":"markdown","9ec1b734":"markdown","a576e835":"markdown","aa38700f":"markdown","f989991e":"markdown","25221023":"markdown","f6821576":"markdown","1ccdb64f":"markdown"},"source":{"1af75c37":"# Import Desired libraries.\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n# Any results you write to the current directory are saved as output.","414bd6fb":"data=pd.read_csv(\"..\/input\/WA_Fn-UseC_-HR-Employee-Attrition.csv\")\nattrition=data\nprint(data.columns)\nprint(data.shape)","6b295f46":"# Differentiate numerical features (minus the target) and categorical features\ncategorical_features = data.select_dtypes(include=['object']).columns\ncategorical_features\n\nnumerical_features = data.select_dtypes(exclude = [\"object\"]).columns\nprint(categorical_features.shape)\nprint(categorical_features)\nprint(numerical_features)","27ff22cd":"print(data.isnull().values.any())","9163f590":"data.describe() # this creates a kind of summary of the datset withh various statistical features.","6c24c149":"sns.countplot(\"Attrition\",data=data)\nplt.show()","22b94c5c":"corrmat = data.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat,annot=True,fmt= '.1f',vmax=.8, square=True)","0b2d683f":"x, y, hue = \"Attrition\", \"prop\", \"Gender\"\nf, axes = plt.subplots(1,2)\nsns.countplot(x=x, hue=hue, data=data, ax=axes[0])\nprop_df = (data[x]\n           .groupby(data[hue])\n           .value_counts(normalize=True)\n           .rename(y)\n           .reset_index())\nsns.barplot(x=x, y=y, hue=hue, data=prop_df, ax=axes[1])","8e9000ec":"x, y, hue = \"Attrition\", \"prop\", \"Department\"\nf, axes = plt.subplots(1,2,figsize=(10,5))\nsns.countplot(x=x, hue=hue, data=data, ax=axes[0])\nprop_df = (data[x]\n           .groupby(data[hue])\n           .value_counts(normalize=True)\n           .rename(y)\n           .reset_index())\nsns.barplot(x=x, y=y, hue=hue, data=prop_df, ax=axes[1])","52caf466":"sns.pairplot(data=data,x_vars=['MonthlyIncome'], y_vars=['Age'],height=5, hue='Attrition',palette=\"prism\")","bc0c319c":"sns.set()\ncols=['Age','DailyRate','Education','JobLevel','DistanceFromHome','EnvironmentSatisfaction','Attrition']\nsns.pairplot(data[cols],hue='Attrition',height=2.5,palette=\"hls\")","6e328124":"#for c in data.columns:\n    #print(\"---- %s ---\" % c)\n    #print(data[c].value_counts())","daf18372":"data1=data\ndi={\"Yes\": 1, \"No\": 0}\ndata1[\"Attrition\"].replace(di,inplace=True)\n","76a52c00":"attrition=data\ndata1.shape\ntarget=data.iloc[:,1]\nprint(target.head(5))","47b3dc33":"print(target.dtypes)\ntarget=pd.DataFrame(target)\nprint(target.dtypes)","8d54b7a3":"print(data1.columns)","6ba179b9":"data1.head(5)\ndata1.drop([\"Attrition\",\"Over18\",\"StandardHours\",\"EmployeeCount\",\"EmployeeNumber\"],axis=1,inplace=True)\n","0f19ac93":"categorical=data1.select_dtypes(include=['object']).columns\ndata1.shape\nprint(data1.columns)\nprint(categorical)\nPrediction=data1##copy paste","2db8aef8":"print(data1.columns)\ndummie=pd.get_dummies(data=data1, columns=['OverTime','BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole','MaritalStatus'])\ndummie=pd.DataFrame(dummie)\nnew_data=pd.concat([data1, dummie], axis=1)\n# print(new_data.columns)","6b421f9a":"print(target.head(5))","61d498a6":"new_data.drop(['OverTime','BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole','MaritalStatus'],axis=1,inplace=True)\n# Since we have already created dummy variables so we can drop the columns with categorical features.","bea4067b":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(new_data,target,test_size=0.33,random_state=7)\n# print(x_train.shape)\n# print(y_train.shape)\n# print(x_test.shape)\n# print(y_test.shape)","75aefbe4":"#  importing Libraries for our model \n# Importing Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nforest=RandomForestClassifier(n_estimators=1000)\nforest.fit(x_train,y_train.values.ravel())","79cbdde5":"predicted= forest.predict(x_test)\n","ee64ba1c":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nlearning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1]\nfor learning_rate in learning_rates:\n    gb = GradientBoostingClassifier(n_estimators=20, learning_rate = learning_rate, max_features=2, max_depth = 2, random_state = 0)\n    gb.fit(x_train, y_train)\n    print(\"Learning rate: \", learning_rate)\n    print(\"Accuracy score (training): {0:.3f}\".format(gb.score(x_train, y_train)))\n    print(\"Accuracy score (validation): {0:.3f}\".format(gb.score(x_test, y_test)))\n    print()","6308fb95":"# Output confusion matrix and classification report of Gradient Boosting algorithm on validation set\n\ngb = GradientBoostingClassifier(n_estimators=20,learning_rate = 0.5,random_state = 7)\ngb.fit(x_train, y_train)\npredictions = gb.predict(x_test)\n\nprint(\"Confusion Matrix for Gradient boosting:\")\nprint(confusion_matrix(y_test, predictions))\nprint()\nprint(\"Classification Report for Gradient Boosting\")\nprint(classification_report(y_test, predictions))","d3cafcfd":"print(\"Accuracy score (validation): {0:.3f}\".format(forest.score(x_test, y_test)))\nprint(\"Confusion Matrix for Random Forests:\")\nprint(confusion_matrix(y_test, predicted))\nprint()\nprint(\"Classification Report for Random Forests\")\nprint(classification_report(y_test, predicted))","4e8f9e88":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state=7, ratio = 1.0)\nx_train_res, y_train_res = sm.fit_sample(x_train, y_train)\nforest_sm = RandomForestClassifier(n_estimators=500, random_state=7)\nforest_sm.fit(x_train_res, y_train_res.ravel())\nprediction2 = forest_sm.predict(x_test)\nprint(\"Accuracy score (validation): {0:.3f}\".format(forest_sm.score(x_test, y_test)))\nprint(\"Confusion Matrix for Random Forests:\")\nprint(confusion_matrix(y_test, prediction2))\nprint()\nprint(\"Classification Report for Random Forests\")\nprint(classification_report(y_test, prediction2))","c874af76":"gb_sm = GradientBoostingClassifier(n_estimators=20, learning_rate = 0.5, max_features=2, max_depth = 2, random_state = 7)\ngb_sm.fit(x_train_res, y_train_res.ravel())\nprediction3 = gb_sm.predict(x_test)\n\nprint(\"Confusion Matrix for Gradient boosting:\")\nprint(confusion_matrix(y_test, prediction3))\nprint()\nprint(\"Classification Report for Gradient Boosting\")\nprint(classification_report(y_test, prediction3))\nprint(\"Accuracy score (validation): {0:.3f}\".format(gb_sm.score(x_test, y_test)))","f807a0b2":"**MODEL BUILDING :****\n\nI have used two models for this classification problem -Random Forests & Gradient Boosting Algorithm.\n\nFirst , I have applied both the algorithms on the imbalanced dataset and then on the Balanced Dataset. \nFor the purpose of Balancing of our datset, I have used SMOTE(Synthetic MInority Over Sampling Technique).\n\n**RANDOM FORESTS:**-  This is an ensemble method used for building predictive models for  both classification and regression problems.\nFirst I am applying Random Forests Algorithm for this Binary classification problem of Employee Attrition.This is a bagging Ensemble Model.\nBagging is a simple ensembling technique in which we build many independent predictors\/models\/learners and combine them using some model averaging techniques.\n\n**GRADIENT BOOSTING:-** Another way of ensembling to build predictive models is through Boosting. Boosting is a technique in which learners\/predictors are combined sequentially, rather than independently. This means each predictor learns from the mistakes of their previous predictors. ","346e99e5":"Since Attrition is the target variable we do not need it in our predictor variables.\nApart from these from value_counts of each variable we can see that 'Over18', 'StandardHours', 'EmployeeCount' are all same values and can be dropped without loss of information.\n","32a222b1":"Next using Gradient Boosting For our dataset classification.","c2c7c97d":"I have tried to find out the relation between** Department and Attrition**. It can be infered that employees from Sales Department have higher possibility of Attrition whereas Research and Development have higher proportion on the No Attrition side.","ac1cfcfd":"**COUNTPLOT**:-The above plot shows the distribution of our target variable.As can be seen clearly, the graph represents imbalanced dataset.\nSo we' ll need to balance this dataset.\nImbalanced class distributions are an issue when anamoly detection like fraud cases, identification of rare diseases, or cases similiar to the above are present. In such scenarios we are more interested in the minority class and the factors that contibute to the occurrence of them. \n\nVarious techniques are available for handling imbalanced Dataset like Undersampling the Majority class and Oversampling the Minority class.\nIn simple terms, it is decreasing instances of majority classes or increasing instances of minority classes to result in a balanced dataset.\nWe will deal with this issue while building our model.Each has its own set of pros and cons.","f88d827b":"Converting categorical features to dummy variables. I have used get_dummies method of pandas for the same.","b9c45791":"An important feature in our datset is Gender. I ' ll verify this with help of plot to understand who is more likely for Job Attrition if we only consider Gender as the factor.","af4de1f0":"To understand the counts of different values in each feature I have used value_counts method of Pandas. ","1cf3aaa5":"The above plot shows those with lesser age and Lower income groups upto 5000 have higher possibility of Attrition as green dots are more concentrated in that region more.","8afc616b":"Here we applied two methods Gradient Boosting and Random Forests and checked their accuracy as well as confusion matrix.\nBut since this data was not balanced ,next we will see what effect occurs on accuracy after using techniques for imbalanced data.","24399a70":"To see the correlation with our target Variable which is Attrition we will convert it into numerical values.\nI will be using Replace function for this.","cdb5e77c":"Importing the Dataset. After which an important step is to understand our data.","54f73ce6":"So there are 9 categorical features which includes our target  variable \"Attrition\".\nThese will need to be encoded using one of the following ways:\n1. Dummy Encoding\n2.One Hot Encoder\n3.Label Encoder\nRest are numerical features. Before going any furthur let's check for any NULLS in our dataset.","36c4e95c":"Next  fitting our model using Gradient Boosting after using SMOTE for handling imbalanced Data.","a9feb656":"We need to specify our target variable which is Attrition in this case. Also since Attrition is a categorical feature we will map it to numerical values.","e51b45a1":"One important thing to note here is we cannot make direct inferences from first countplot. The second barplot is made in accordance with proportion.\nOne can clearly infer from the plot above that higher proportion of males are likely for Attrition as compared to females.","1d631a9d":"Since there are no nulls we Do not need to worry about this anymore. \nTo get a better grasp of our datset. I will execute the next line of my code.","6f6251b0":"Since attrition is a categorical Variable , one needs to convert it into numerical form. I am replacing Yes with 1 and No with 0.","9ec1b734":"**DATA VISUALIZATION**  \nData Visualization is one of the core step before building any model. Python offers numerous libraries for this purpose.I have used Seaborn library for this porpose. First and foremost I want to see how my Target variable is distributed across the dataset.","a576e835":"Above are the pairplots between various numerical variables.\n1. The first distribution plot for age shows that employees which fall in lower age group are more likely for attrition, However, as the age increases the blue curve goes up.\n2. For the Job Level, there is a sharp peak at lower job levels between 0-2 which shows Atrrition is more likely.\n3. The plot between DistancefromHome and Age shows that again employees with less ages aand living at a distance greater than 15 have higher chances of Attrition.\n4. Employees with lower level of EnvironmentSatisfaction indicates higher chances of Attrition as the Red curve goes above the blue curve for lower levels of EnvironmentSatisfaction.","aa38700f":"Since attrition value to be classified, assigning it to the target variable","f989991e":"Statistical relationship between two variables is referred to as their ** correlation**. The performance of some algorithms can deteriorate if two or more variables are tightly related, called multicollinearity.This is of special importance in Regression.\nFrom the above correlation matrix , we find most of the features are uncorrelated.But, there is a correlation (0.8) between Performance Rating and Performance Salary Hike. We need to look into the white lines shown by EmployeeCount and Standard Hours.TotalWorkingYears with JobLevel also has high correlation(0.8).","25221023":"**EVALUATION OF MODEL:-**\nThere are several metrics for evaluation of any machine learning model. The most common is accuracy.\n**ACCURACY**- This is simply the ratio of TOTAL CORRECT PREDICTIONS to TOTAL NO OF PREDICTIONS.\nHowever, this metric is useful especially if there are equal no of samples belonging to each class.\nThis is certainly not the case in our Dataset. This is because if 90% of data belongs to one particular class say class X and 10% to class Y,then our model will get 90% accuracy even if predicts that entire sample belongs to class X.\n\n In such cases , a classification report is better way to check the quality of classification algorithm predictions. \n This report gives several classifcation metrics like recall , precision, & f1 score.\n \n**** PRECISION:**- it is defined as the ratio of true positives to the sum of true and false positives.  It is the accuracy in our postive predictions.\n\n**RECALL:-** It is defined as the ratio of True Positives to sum of True Positives and False Negatives.,which is how many positives we have identified out of the total positives that are actually present in the dataset.","f6821576":"So there are 35 columns and 1470 rows.\nNext let's check how many categorical variables and numerical variables:","1ccdb64f":"**HR EMPLOYEE ATTRITION DATASET.**\n\nThis is a fictional data set created by IBM data scientists. We need to explore the dataset, understanding the algorithms and techniques which can be applied on it. We' ll try to gain meaningful insights from the dataset, like what are the factors which have an impact on Employee Attrition."}}