{"cell_type":{"03e94f94":"code","e809412c":"code","a697b8f0":"code","aab4ffb6":"code","ad765727":"code","f7e13b54":"code","aeeb8a5f":"code","b71042d9":"code","5b517c81":"code","cc39a8e5":"code","d078135b":"markdown","17bfc327":"markdown","84f37c8e":"markdown","2269bf6f":"markdown","90a65aa1":"markdown","7587c6d4":"markdown","b0e7c524":"markdown","9c9c734e":"markdown","13973e1a":"markdown","128a65b3":"markdown","ad034b84":"markdown","dcbd488c":"markdown","1a7da0ae":"markdown","7ef7e51a":"markdown","6e323227":"markdown","50636720":"markdown","16d1efb6":"markdown","ed93c11b":"markdown","4a873604":"markdown","f2379a39":"markdown","b876f35f":"markdown","a6fa2314":"markdown","63cfd759":"markdown","e67d9e4f":"markdown","b6a339bf":"markdown","f63b3258":"markdown","1dd66d72":"markdown","7800774f":"markdown","a8c7c5f3":"markdown","b6978d70":"markdown","92efb0f1":"markdown","9c487383":"markdown","cfec686f":"markdown","97ed827a":"markdown","fbcd49bb":"markdown","d31c3e31":"markdown"},"source":{"03e94f94":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","e809412c":"from __future__ import print_function\nfrom keras.layers.recurrent import SimpleRNN\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\n","a697b8f0":"INPUT_FILE = \"\/kaggle\/input\/alice-in-wonderland-gutenbergproject\/wonderland.txt\"","aab4ffb6":"# extract the input as a stream of characters\nprint(\"Extracting text from input...\")\nfin = open(INPUT_FILE, 'rb')\nlines = []\nfor line in fin:\n    line = line.strip().lower()\n    line = line.decode(\"ascii\", \"ignore\")\n    if len(line) == 0:\n        continue\n    lines.append(line)\nfin.close()\ntext = \" \".join(lines)","ad765727":"# creating lookup tables\n# Here chars is the number of features in our character \"vocabulary\"\nchars = set([c for c in text])\nnb_chars = len(chars)\nchar2index = dict((c, i) for i, c in enumerate(chars))\nindex2char = dict((i, c) for i, c in enumerate(chars))","f7e13b54":"# create inputs and labels from the text. We do this by stepping\n# through the text ${step} character at a time, and extracting a \n# sequence of size ${seqlen} and the next output char. For example,\n# assuming an input text \"The sky was falling\", we would get the \n# following sequence of input_chars and label_chars (first 5 only)\n#   The sky wa -> s\n#   he sky was ->  \n#   e sky was  -> f\n#    sky was f -> a\n#   sky was fa -> l\nprint(\"Creating input and label text...\")\nSEQLEN = 10\nSTEP = 1","aeeb8a5f":"input_chars = []\nlabel_chars = []\nfor i in range(0, len(text) - SEQLEN, STEP):\n    input_chars.append(text[i:i + SEQLEN])\n    label_chars.append(text[i + SEQLEN])","b71042d9":"# vectorize the input and label chars\n# Each row of the input is represented by seqlen characters, each \n# represented as a 1-hot encoding of size len(char). There are \n# len(input_chars) such rows, so shape(X) is (len(input_chars),\n# seqlen, nb_chars).\n# Each row of output is a single character, also represented as a\n# dense encoding of size len(char). Hence shape(y) is (len(input_chars),\n# nb_chars).\nprint(\"Vectorizing input and label text...\")\nX = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\ny = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\nfor i, input_char in enumerate(input_chars):\n    for j, ch in enumerate(input_char):\n        X[i, j, char2index[ch]] = 1\n    y[i, char2index[label_chars[i]]] = 1","5b517c81":"# Build the model. We use a single RNN with a fully connected layer\n# to compute the most likely predicted output char\nHIDDEN_SIZE = 128\nBATCH_SIZE = 128\nNUM_ITERATIONS = 25\nNUM_EPOCHS_PER_ITERATION = 1\nNUM_PREDS_PER_EPOCH = 100\n\nmodel = Sequential()\nmodel.add(SimpleRNN(HIDDEN_SIZE, return_sequences=False,\n                    input_shape=(SEQLEN, nb_chars),\n                    unroll=True))\nmodel.add(Dense(nb_chars))\nmodel.add(Activation(\"softmax\"))\n\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")","cc39a8e5":"# We train the model in batches and test output generated at each step\nfor iteration in range(NUM_ITERATIONS):\n    print(\"=\" * 50)\n    print(\"Iteration #: %d\" % (iteration))\n    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n    \n    # testing model\n    # randomly choose a row from input_chars, then use it to \n    # generate text from model for next 100 chars\n    test_idx = np.random.randint(len(input_chars))\n    test_chars = input_chars[test_idx]\n    print(\"Generating from seed: %s\" % (test_chars))\n    print(test_chars, end=\"\")\n    for i in range(NUM_PREDS_PER_EPOCH):\n        Xtest = np.zeros((1, SEQLEN, nb_chars))\n        for i, ch in enumerate(test_chars):\n            Xtest[0, i, char2index[ch]] = 1\n        pred = model.predict(Xtest, verbose=0)[0]\n        ypred = index2char[np.argmax(pred)]\n        print(ypred, end=\"\")\n        # move forward with test_chars + ypred\n        test_chars = test_chars[1:] + ypred\n    print()\n","d078135b":"- As the file contains line breaks and non-ASCII characters, so we should do some preliminary cleanup and write out the contents into a variable called text as follows:","17bfc327":"![LSTM Architecture](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-chain.png)","84f37c8e":"<a class=\"anchor\" id=\"0.1\"><\/a>\n# Table of Contents\n\n\n1. [Introduction to Recurrent Neural Network (RNN)](#1)\n1. [RNN Computations](#2)\n1. [SimpleRNN cells](#3)\n1. [Implementation of SimpleRNN with Keras - text generation](#4)\n1. [RNN topologies](#5)\n1. [Vanishing and exploding gradients](#6)\n1. [Long Short Term Memory (LSTM)](#7)\n1. [Gated Recurrent Unit (GRU)](#8)\n1. [Summary and conclusion](#9)\n1. [References](#10)","2269bf6f":"# 8. Gated Recurrent Unit <a class=\"anchor\" id=\"8\"><\/a>\n\n[Back to Table of Contents](#0.1)\n\n\n- The GRU is a variant of the LSTM and was introduced by K. Cho.\n\n- It retains the LSTM's resistance to the vanishing gradient problem, but its internal structure is simpler, and therefore is faster to train, since fewer computations are needed to make updates to its hidden state. \n\n- A Gated Recurrent Unit (GRU), as its name suggests, is a variant of the RNN architecture, and uses gating mechanisms to control and manage the flow of information between cells in the neural network. ","90a65aa1":"- The output of this run is shown above. \n\n- We can see that our model is character-based and has no knowledge of words, yet it learns to spell words that look like they might have come from the original text.","7587c6d4":"# 1. Introduction to Recurrent Neural Network (RNN) <a class=\"anchor\" id=\"1\"><\/a>\n\n\n[Back to Table of Contents](#0.1)\n\n\n- **Recurrent Neural Networks (RNNs)** are a family of networks that are suitable for learning representations of sequential data like text in Natural Language Processing (NLP).\n\n- The idea behind RNNs is to make use of sequential information. \n\n- In a traditional neural network we assume that all inputs (and outputs) are independent of each other. \n\n- But for many tasks that is a very bad idea. If we want to predict the next word in a sentence we better know which words came before it. \n\n- RNNs are called recurrent because they perform the same task for every element of a sequence, with the output being depended on the previous computations. \n\n- Another way to think about RNNs is that they have a \u201cmemory\u201d which captures information about what has been calculated so far. \n\n- In theory RNNs can make use of information in arbitrarily long sequences, but in practice they are limited to looking back only a few steps. \n\n- Here is what a typical RNN looks like:\n\n\n\n\n","b0e7c524":"- Let's check the input folder and files","9c9c734e":"- The repeating module in a standard RNN contains a single layer as shown below:\n\n![Repeating module in a standard RNN](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-SimpleRNN.png)","13973e1a":"![GRU follows the same flow as typical RNN](https:\/\/blog.floydhub.com\/content\/images\/2019\/07\/image15.jpg)","128a65b3":"- Our model consists of generating a character from the model given a random input, then dropping the first character from the input and appending the predicted character from our previous run, and generating another character from the model. \n\n- We continue this 100 times (NUM_PREDS_PER_EPOCH=100) and generate and print the resulting string. The string gives us an indication of the quality of the model.","ad034b84":"# 7. Long Short Term Memory (LSTM) <a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Table of Contents](#0.1)\n\n\n- **Long Short Term Memory** networks \u2013 usually just called **LSTMs** \u2013 are a special kind of RNN, capable of learning long-term dependencies. They work tremendously well on a large variety of problems, and are now widely used.\n\n- LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behaviour.\n\n- All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.","dcbd488c":"# 6. Vanishing and exploding gradients <a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)\n\n\n- Just like traditional neural networks, training the RNN also involves backpropagation. \n\n- The difference in this case is that since the parameters are shared by all time steps, the gradient at each output depends not only on the current time step, but also on the previous ones. \n \n- This process is called **backpropagation through time (BPTT)**.\n\n- Regular RNNs might have a difficulty in learning long range dependencies.\n\n- This kind of dependencies between sequence data is called long-term dependencies because the distance between the relevant information and the point where it is needed to make a prediction is very wide. \n\n- As this distance becomes wider, RNNs have a hard time learning these dependencies because it encounters either a vanishing or exploding gradient problem.\n\n- These problems arise during training of a deep network when the gradients are being propagated back in time all the way to the initial layer.\n\n- The gradients coming from the deeper layers have to go through continuous matrix multiplications because of the the chain rule, and as they approach the earlier layers, if they have small values (<1), they shrink exponentially until they vanish and make it impossible for the model to learn. This is the **vanishing gradient problem**.\n\n- While on the other hand if they have large values (>1) they get larger and eventually blow up and crash the model. This is the **exploding gradient problem.**\n\n","1a7da0ae":"- In the above diagram each rectangle is a vector and arrows represent functions (e.g. matrix multiply). \n\n- Input vectors are in red, output vectors are in blue and green vectors hold the RNN\u2019s state. \n\n- From left to right:\n\n  - 1. Vanilla mode of processing without RNN, from fixed-sized input to fixed-sized output (e.g. image classification).\n  \n  - 2. Sequence output (e.g. image captioning takes an image and outputs a sentence of words).\n  \n  - 3. Sequence input (e.g. sentiment analysis where a given sentence is classified as expressing positive or negative sentiment).\n  \n  - 4. Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French).\n  \n  - 5. Synced sequence input and output (e.g. video classification where we wish to label each frame of the video).","7ef7e51a":"[Go to Top](#0)","6e323227":"- The structure of the GRU allows it to adaptively capture dependencies from large sequences of data without discarding information from earlier parts of the sequence. \n\n- This is achieved through its gating units, similar to the ones in LSTMs, which solve the vanishing\/exploding gradient problem of traditional RNNs. \n\n- These gates are responsible for regulating the information to be kept or discarded at each time step.\n\n- Other than its internal gating mechanisms, the GRU functions just like an RNN, where sequential input data is consumed by the GRU cell at each time step along with the memory, or otherwise known as the hidden state. \n\n- The hidden state is then re-fed into the RNN cell together with the next input data in the sequence. This process continues like a relay system, producing the desired output.\n\n- This process is illustrated in the following diagram.","50636720":"- The overview of GRU is illustrated in the following diagram:\n\n\n![Overview of GRU](https:\/\/blog.floydhub.com\/content\/images\/2019\/07\/image17-1.jpg)","16d1efb6":"- The above diagram shows a RNN being unrolled (or unfolded) into a full network. \n\n- By unrolling we simply mean that we write out the network for the complete sequence. \n\n- For example, if the sequence we care about is a sentence of 5 words, the network would be unrolled into a 5-layer neural network, one layer for each word.\n\n- Please follow the link below for in-depth discussion on RNN.\n\n[Recurrent Neural Network from Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Recurrent_neural_network)\n\n[Introduction to RNNs from AnalyticsVidhya](https:\/\/www.analyticsvidhya.com\/blog\/2017\/12\/introduction-to-recurrent-neural-networks\/)","ed93c11b":"# 10. References <a class=\"anchor\" id=\"10\"><\/a>\n\n[Back to Table of Contents](#0.1)\n\n\nThis kernel is based on the following books and websites:\n\n\n-\tDeep Learning with Python by Francois Chollet\n\n\n-   Deep Learning with Keras by Antonio Gulli and Sujit Pal  \n\n\n-\tAdvanced Deep Learning with Keras by Rowel Atienza\n\n\n-   https:\/\/en.wikipedia.org\/wiki\/Recurrent_neural_network\n\n\n-   https:\/\/www.analyticsvidhya.com\/blog\/2017\/12\/introduction-to-recurrent-neural-networks\/\n\n\n-   https:\/\/medium.com\/explore-artificial-intelligence\/an-introduction-to-recurrent-neural-networks-72c97bf0912\n\n\n-   http:\/\/www.wildml.com\/2015\/09\/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns\/\n\n\n-   https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/\n","4a873604":"**I hope you find this kernel useful and your <font color=\"red\"><b>UPVOTES<\/b><\/font> would be highly appreciated**.","f2379a39":"- Then we import the necessary modules as follows:","b876f35f":"![Sequences of Vectors in RNNs](https:\/\/miro.medium.com\/max\/815\/1*XosBFfduA1cZB340SSL1hg.png)","a6fa2314":"I will not go into Mathematical details behind LSTM. I will do it in a future kernel.","63cfd759":"# 5. RNN topologies <a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)\n\n\n- The APIs for MLP and CNN architectures are limited. Both architectures accept a fixed-size tensor as input and produce a fixed-size tensor as output; and they perform the transformation from input to output in a fixed number of steps given by the number of layers in the model. \n\n- RNNs don't have this limitation\u2014you can have sequences in the input, the output, or both. This means that RNNs can be arranged in many ways to solve specific problems.\n\n- RNNs combine the input vector with the previous state vector to produce a new state vector. This can be thought of as similar to running a program with some inputs and some internal variables. \n\n- The RNNs are more exciting because they allow us to operate over sequences of vectors: Sequences in the input, the output, or in the most general case both. \n\n- This property of being able to work with sequences gives rise to a number of common topologies shown below:","e67d9e4f":"So, now we will come to the end of this kernel.\n\nI hope you find this kernel useful and enjoyable.\n\nYour comments and feedback are most welcome.\n\nThank you","b6a339bf":"- LSTMs also have this chain like structure, but the repeating module has a different structure. \n\n- Instead of having a single neural network layer, there are four, interacting in a very special way as shown below:","f63b3258":"- The next step is to vectorize these input and label texts.","1dd66d72":"- Now, we are building a character-level RNN. In this case, the  vocabulary is the set of characters that occur in the text. \n\n- We will be dealing with the indexes to these characters rather than the characters themselves, the following code snippet creates the necessary lookup tables:","7800774f":"- While there are a few approaches to minimize the problem of vanishing gradients, such as proper initialization of the W matrix, using a ReLU instead of tanh layers, and pre-training the layers using unsupervised methods, the most popular solution is to use the LSTM or GRU architectures (discussed next).\n\n- These architectures have been designed to deal with the vanishing gradient problem and learn long term dependencies more effectively.","a8c7c5f3":"![Typical workflow of RNN](http:\/\/www.wildml.com\/wp-content\/uploads\/2015\/09\/rnn.jpg)","b6978d70":"# 3. SimpleRNN cells <a class=\"anchor\" id=\"3\"><\/a>\n\n\n[Back to Table of Contents](#0.1)\n\n\n- Traditional multilayer perceptron neural networks make the assumption that all inputs are independent of each other. This assumption breaks down in the case of sequence data. \n\n- Time series data, such as stock prices, also exhibit a dependence on past data, called the secular trend.\n\n- RNN cells incorporate this dependence by having a hidden state, or memory, that holds the essence of the past. \n\n- The value of the hidden state at any point in time is a function of the value of the hidden state at the previous time step and the value of the input at the current time step, that is:\n\n\n        ht = \u00d8(ht-1.Xt)\n        \n\n  - ht and ht-1 are the values of the hidden states at the time steps t and t-1 respectively, and xt is the value of the input at time t. \n  \n  - The above equation is recursive, that is, ht-1 can be represented in terms of ht-2 and xt-1, and so on, until the beginning of the sequence. \n  \n- This is how RNNs encode and incorporate information from arbitrarily long sequences.\n","92efb0f1":"# 4.Implementation of SimpleRNN with Keras - text generation <a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)\n\n\n- RNNs have been used extensively by the natural language processing (NLP) community for various applications. One such application is building language models. A language model allows us to predict the probability of a word in a text given the previous words. Language models are important for various higher level tasks such as machine translation, spelling correction, and so on.\n\n- In this example, we will train a character based language model on the text of Alice in Wonderland to predict the next character given 10 previous characters. We have chosen to build a character-based model here because it has a smaller vocabulary and trains quicker. \n\n- The idea is the same as using a word-based language model, except we use characters instead of words. We will then use the trained model to generate some text in the same style.\n\n- We will proceed as follows:","9c487383":"# 2. RNN Computations <a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)\n\n- We can describe the computations within an RNN in terms of equations. \n\n- The internal state of the RNN at a time t is given by the value of the hidden vector ht, which is the sum of the product of the weight matrix W and the hidden state ht-1 at time t-1 and the product of the weight matrix U and the input xt at time t, passed through the tanh nonlinearity. \n\n- The choice of tanh over other nonlinearities has to do with its second derivative decaying very slowly to zero. \n\n- This keeps the gradients in the linear region of the activation function and helps combat the vanishing gradient problem. \n\n- The output vector yt at time t is the product of the weight matrix V and the hidden state ht, with softmax applied to the product so the resulting vector is a set of output probabilities:\n\n\n      ht = tanh(Wht-1 + UXt)\n    \n      yt = softmax(Vht)\n      \n\n- Keras provides the SimpleRNN (for more information refer to: https:\/\/keras.io\/layers\/recurrent\/) recurrent layer that incorporates all the logic we have seen so far, as well as the more advanced variants such as LSTM and GRU that we will see later in this kernel.","cfec686f":"- Finally, we are ready to build our model.\n\n- The RNN is connected to a dense (fully connected) layer. The dense layer has (nb_char) units, which emits scores for each of the characters in the vocabulary. \n\n- The activation on the dense layer is a softmax, which normalizes the scores to probabilities. The character with the highest probability is chosen as the prediction. \n\n- We compile the model with the categorical cross-entropy loss function, a good loss function for categorical outputs, and the RMSprop optimizer.","97ed827a":"- In the next step, we create the input and label texts.","fbcd49bb":"# 9. Summary and conclusion <a class=\"anchor\" id=\"9\"><\/a>\n\n[Back to Table of Contents](#0.1)\n\n\n- In this kernel, I have discussed **Recurrent Neural Networks**.\n\n- We have discussed SimpleRNN cell and its major limitation.\n\n- We have discussed RNN topologies, variants of the SimpleRNN cell - **Long Short Term Memory (LSTM)** and **Gated Recurrent Unit (GRU)**.\n\n- We have also seen basic RNN implementation in Keras in generating text.","d31c3e31":"<a class=\"anchor\" id=\"0\"><\/a>\n# Comprehensive Guide to RNN with Keras\n\n\nHello friends.\n\n\nRNN stands for **Recurrent Neural Networks.**\n\n\nIn this kernel, I discuss **Recurrent Neural Networks**, a class of neural networks that discover the sequential nature of the input data. Inputs could be of text, speech, time series and anything else where sequence matters. We will discuss SimpleRNN cell, its major limitation, RNN topologies, variants of the SimpleRNN cell - **Long Short Term Memory (LSTM)**, **Gated Recurrent Unit (GRU)** and other RNN variants.\n\n\nWe will also see basic RNN implementation in Keras in generating text."}}