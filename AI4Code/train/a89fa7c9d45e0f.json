{"cell_type":{"c7121b59":"code","caca0015":"code","a51311a3":"code","17961817":"code","56a66e44":"code","e5768597":"code","e788a42e":"code","8caff195":"code","73a1b56f":"code","27c3bec6":"code","74ad529a":"code","dce8e490":"code","c87414e8":"code","c2f4e230":"code","1a9e1140":"code","73212020":"code","f13ca44b":"code","d51aac36":"code","40735b76":"code","fff6b45f":"code","8ebf62ab":"markdown","65a465ce":"markdown","458b6576":"markdown","cfb8b996":"markdown","b79651d1":"markdown","4dc0960f":"markdown","75724829":"markdown","b694a0fa":"markdown","99464875":"markdown","0c2c77e1":"markdown"},"source":{"c7121b59":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","caca0015":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","a51311a3":"import warnings\nwarnings.filterwarnings(\"ignore\")","17961817":"dataset = pd.read_csv('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')","56a66e44":"dataset","e5768597":"dataset.info()","e788a42e":"dataset.isnull().sum()","8caff195":"dataset['quality'] = [1 if i > 6.5 else 0 for i in dataset['quality']]","73a1b56f":"dataset","27c3bec6":"dataset['quality'].value_counts()","74ad529a":"x = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values","dce8e490":"x","c87414e8":"y","c2f4e230":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state= 0)","1a9e1140":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","73212020":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier","f13ca44b":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score","d51aac36":"models = []\nmodels.append(['Logistic Regression 1', LogisticRegression(C = 0.1)])\nmodels.append(['Logistic Regression 2', LogisticRegression(C = 0.5)])\nmodels.append(['Logistic Regression 3', LogisticRegression(C = 1.0)])\nmodels.append(['KNeighbours 1', KNeighborsClassifier(n_neighbors = 3, metric = 'minkowski', p = 2)])\nmodels.append(['KNeighbours 2', KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)])\nmodels.append(['SVM 1', SVC(kernel= 'linear')])\nmodels.append(['SVM 2', SVC(kernel= 'rbf')])\nmodels.append(['Naive Bayes', GaussianNB()])\nmodels.append(['Decision Tree 1', DecisionTreeClassifier(criterion= 'gini')])\nmodels.append(['Decision Tree 2', DecisionTreeClassifier(criterion= 'entropy')])\nmodels.append(['Random Forest 1', RandomForestClassifier(n_estimators= 50, criterion= 'gini')])\nmodels.append(['Random Forest 2', RandomForestClassifier(n_estimators= 100, criterion= 'gini')])\nmodels.append(['Random Forest 3', RandomForestClassifier(n_estimators= 200, criterion= 'gini')])\nmodels.append(['Random Forest 4', RandomForestClassifier(n_estimators= 50, criterion= 'entropy')])\nmodels.append(['Random Forest 5', RandomForestClassifier(n_estimators= 100, criterion= 'entropy')])\nmodels.append(['Random Forest 6', RandomForestClassifier(n_estimators= 200, criterion= 'entropy')])\n\nfor m in range(len(models)):\n  model = models[m][1]\n  model.fit(x_train, y_train)\n  y_pred = model.predict(x_test)\n  cm = confusion_matrix(y_test, y_pred)\n  accuracies = cross_val_score(estimator = model, X = x_train, y = y_train, cv = 10)\n  print(models[m][0])\n  print(cm)\n  print('Accuracy Score',accuracy_score(y_test, y_pred))\n  print(\"Mean Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n  print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n  print('-----------------------------------')","40735b76":"from sklearn.model_selection import GridSearchCV","fff6b45f":"parameters = [{'n_estimators': [50, 100, 200, 300, 400, 500, 1000], 'criterion': ['gini']},\n              {'n_estimators': [50, 100, 200, 300, 400, 500, 1000], 'criterion': ['entropy']}]\ngrid_search = GridSearchCV(estimator = model,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search.fit(x_train, y_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nbest_std = grid_search.cv_results_['std_test_score'][grid_search.best_index_]\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint('Best Standard Deviation: {:.2f} %'.format(best_std*100))\nprint(\"Best Parameters:\", best_parameters)","8ebf62ab":"Quality > 6.5 = 'good'\n\nQuality <6.5 = 'bad'","65a465ce":"Above shows the best hyperparameters for RandomForest which can make the model more efficient.","458b6576":"# **Importing Libraries**","cfb8b996":"**Importing different models**","b79651d1":"# **Importing Dataset**","4dc0960f":"As RandomForest has better accuracies than other models, so now GridSearch is applied on RandomForest for a better hyperparameters tuning.","75724829":"# **Feature Scaling**","b694a0fa":"# **Splitting dataset into Train and Test set**","99464875":"Checking if there are any NULL values","0c2c77e1":"# **Models of Selection**"}}