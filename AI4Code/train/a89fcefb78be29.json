{"cell_type":{"fd3979f3":"code","21354bd3":"code","2ebcf3e8":"code","1b3fad04":"code","fdc1192a":"code","bea22cb9":"code","07f01596":"code","70dfc51b":"code","ac24afef":"code","d9a9e101":"code","8011afe2":"code","287ef631":"code","092441ea":"code","c0ae33e9":"code","80812851":"code","103c7f8e":"code","2af875a7":"markdown"},"source":{"fd3979f3":"import pandas as pd\nimport re\nfrom sklearn.model_selection import train_test_split\nimport sklearn\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import HashingVectorizer,CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom tqdm.notebook import tqdm\ntqdm.pandas()","21354bd3":"from nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\n\n\ndef lemmatize_sentence(sentence):\n    return \" \".join([lemmatizer.lemmatize(t) for t in sentence.split()])","2ebcf3e8":"def simple_clean(text):\n    text = text.lower()\n    text = re.sub(r\"[^a-z ]+\", \" \", text)\n    return lemmatize_sentence(text)","1b3fad04":"def encode_sentiment(text):\n    if text == \"positive\":\n        return 1\n    elif text == \"negative\":\n        return 0\n    else:\n        print(\"error\")\ndef decode_sentiment(number):\n    if number == 1:\n        return \"positive\"\n    elif number == 0:\n        return \"negative\"","fdc1192a":"df = pd.read_csv(\"..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\")","bea22cb9":"df[\"sentiment_value\"] = df[\"sentiment\"].apply(encode_sentiment)\ndf[\"review_clean\"] = df[\"review\"].progress_apply(simple_clean)","07f01596":"X_train,X_test,y_train,y_test = train_test_split(df[\"review_clean\"].values,df[\"sentiment_value\"].values, test_size=0.2, random_state=42)","70dfc51b":"pipeline1 = Pipeline(steps=[('featurehash',HashingVectorizer(ngram_range=(1, 2),n_features=8192)),('logisticregression',LogisticRegression(solver='liblinear'))])\npipeline2 = Pipeline(steps=[('countvectorizer',CountVectorizer(ngram_range=(1, 2))),('logisticregression',LogisticRegression(solver='liblinear'))])","ac24afef":"%%time\npipeline1.fit(X_train,y_train)","d9a9e101":"%%time\npipeline2.fit(X_train,y_train)","8011afe2":"%%time\ny_pred_1 = pipeline1.predict(X_test)","287ef631":"%%time\ny_pred_2 = pipeline2.predict(X_test)","092441ea":"print(\"Hashing Vectorizer\")\nprint(classification_report(y_test,y_pred_1))\nprint(\"Count Vectorizer\")\nprint(classification_report(y_test,y_pred_2))","c0ae33e9":"print(\"Total number of features for count vectorizer: \",len(pipeline2['countvectorizer'].get_feature_names()))","80812851":"import joblib\njoblib.dump(pipeline1, 'pipeline1.joblib')\njoblib.dump(pipeline2,'pipeline2.joblib')","103c7f8e":"import os\nprint(f\"Size of hash vectorizer {os.path.getsize('pipeline1.joblib')\/1000000}mb\")\nprint(f\"Size of count vectorizer {os.path.getsize('pipeline2.joblib')\/1000000}mb\")","2af875a7":"# Fast sentiment analysis that uses hashing trick\n\nHashing trick lowers memory requirement as there is no longer a need to store entire vocab in memory.\n\nAlso Logistic Regression will have less weights reducing memory."}}