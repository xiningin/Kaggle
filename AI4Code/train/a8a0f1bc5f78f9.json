{"cell_type":{"66a1238f":"code","2a7faeae":"code","0190f9f5":"code","94a9b2d5":"code","2b3a90ae":"code","f2d2ed48":"code","b48e30cf":"code","a24c94a8":"code","ac5c19e6":"code","ec0b1b09":"code","b2ac8066":"code","63134a0a":"code","28138236":"code","9f95ab4d":"code","db675d3e":"code","e6caa8c6":"code","d021b96e":"code","84035e7c":"code","ad4d4722":"code","ce89cb98":"code","320cab36":"code","bc0720e2":"code","16ee59b3":"code","fc46dfbe":"code","2c09c7b9":"code","6c24c621":"code","cbd6f1e6":"code","d95c61f3":"code","126e7436":"code","8fb56ae6":"code","902147fd":"code","9d6f82a8":"markdown","1108374f":"markdown","c517cfe0":"markdown","82260b37":"markdown","eb6de966":"markdown","22693f8b":"markdown","e49384a0":"markdown","91ecec3d":"markdown","245387d0":"markdown","46ca6c14":"markdown","48b9a34f":"markdown","f541b651":"markdown","b398093a":"markdown","cf75a9aa":"markdown","04e1b6ee":"markdown","d4c82dbb":"markdown","7f619163":"markdown","e41d99cc":"markdown","33da5fc5":"markdown","44300241":"markdown","0d7f7dee":"markdown","8b43b3ff":"markdown","22466492":"markdown","06a881de":"markdown","82c401df":"markdown"},"source":{"66a1238f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display, Markdown, Latex\n\nimport plotly.offline as py \npy.init_notebook_mode(connected=True)                  \nimport plotly.graph_objs as go                         \nimport plotly.tools as tls                             \nfrom collections import Counter                        \nimport plotly.figure_factory as ff\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix, silhouette_score\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score # to split the data\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, fbeta_score #To evaluate our model\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import model_selection\n# Algorithmns models to be compared\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n\n","2a7faeae":"df = pd.read_csv(\"\/kaggle\/input\/german-credit-data-with-risk\/german_credit_data.csv\", index_col=False)\ndf.head()","0190f9f5":"df.info()","94a9b2d5":"df_sm = df.copy()","2b3a90ae":"#cross table for the 'Sex' feature\ncross_sex = pd.crosstab(df['Risk'], df['Sex']).apply(lambda x: x\/x.sum() * 100)\ndecimals = pd.Series([2,2], index=['Male', 'Female'])\ncross_sex = cross_sex.round(decimals)\ncross_sex_transposed = cross_sex.T\ncross_sex_transposed.style.background_gradient(cmap='Blues')","f2d2ed48":"fig, ax = plt.subplots(1,2,figsize=(15,5))\nsns.histplot(df, x='Age', bins=30, hue=\"Sex\", ax=ax[0]).set_title(\"Age\/Sex Distribution\");\nsns.boxplot(data=df, x=\"Sex\", y=\"Age\", ax=ax[1]).set_title(\"Age\/Sex Distribution\");\n\nfig, ax = plt.subplots(1,2,figsize=(15,5))\nsns.boxplot(data=df, x='Risk', y='Age', ax=ax[0]).set_title(\"Age Distribution with Risk\");\nsns.countplot(data=df, x=\"Sex\", hue=\"Risk\", ax=ax[1]).set_title(\"Sex Distribution with Risk\");\n\nfig, ax = plt.subplots(2,1,figsize=(15,7))\nplt.tight_layout(2)\nsns.lineplot(data=df, x='Age', y='Credit amount', hue='Sex', lw=2, ax=ax[0]).set_title(\"Credit Amount Graph Depending on Age and Duration by Sex\", fontsize=15);\nsns.lineplot(data=df, x='Duration', y='Credit amount', hue='Sex', lw=2, ax=ax[1]);","b48e30cf":"df_good = df.loc[df[\"Risk\"] == 'good']['Age'].values.tolist()\ndf_bad = df.loc[df[\"Risk\"] == 'bad']['Age'].values.tolist()\ndf_age = df['Age'].values.tolist()\n\n#First plot\ntrace0 = go.Histogram(\n    x=df_good,\n    histnorm='probability',\n    name=\"Good Credit\",\n    marker=dict(\n        color='#85ead9'\n    )\n)\n#Second plot\ntrace1 = go.Histogram(\n    x=df_bad,\n    histnorm='probability',\n    name=\"Bad Credit\",\n    marker=dict(\n        color='#6f2cea'\n    )\n)\n#Third plot\ntrace2 = go.Histogram(\n    x=df_age,\n    histnorm='probability',\n    name=\"Overall Age\",\n    marker=dict(\n        color='#f392f4'\n    )\n)\n\n#Creating the grid\nfig = tls.make_subplots(rows=2, cols=2, specs=[[{}, {}], [{'colspan': 2}, None]],\n                          subplot_titles=('Good','Bad', 'General Distribuition'))\n\n#setting the figs\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig.append_trace(trace2, 2, 1)\n\nfig['layout'].update(showlegend=True, title='Age Distribuition', bargap=0.05)\npy.iplot(fig, filename='custom-sized-subplot-with-subplot-titles')","a24c94a8":"#Let's look the Credit Amount column\ninterval = (18, 25, 35, 60, 120)\n\ncats = ['Student', 'Young', 'Adult', 'Senior']\ndf_sm[\"Age_cat\"] = pd.cut(df_sm.Age, interval, labels=cats)\n\n\ndf_good = df_sm[df_sm[\"Risk\"] == 'good']\ndf_bad = df_sm[df_sm[\"Risk\"] == 'bad']","ac5c19e6":"trace0 = go.Box(\n    y=df_good[\"Credit amount\"],\n    x=df_good[\"Age_cat\"],\n    name='Good credit',\n    marker=dict(\n        color='#50f466'\n    )\n)\n\ntrace1 = go.Box(\n    y=df_bad['Credit amount'],\n    x=df_bad['Age_cat'],\n    name='Bad credit',\n    marker=dict(\n        color='#b2014e'\n    )\n)\n    \ndata = [trace0, trace1]\n\nlayout = go.Layout(\n    yaxis=dict(\n        title='Credit Amount (US Dollar)',\n        zeroline=False\n    ),\n    xaxis=dict(\n        title='Age Categorical'\n    ),\n    boxmode='group'\n)\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='box-age-cat')","ec0b1b09":"#First plot\ntrace0 = go.Bar(\n                x = df[df[\"Risk\"]== 'good'][\"Housing\"].value_counts().index.values,\n                y = df[df[\"Risk\"]== 'good'][\"Housing\"].value_counts().values,\n                name='Good credit',\n                marker=dict(\n                color='#a4e23f'\n                    )\n                )\n\n#Second plot\ntrace1 = go.Bar(\n                x = df[df[\"Risk\"]== 'bad'][\"Housing\"].value_counts().index.values,\n                y = df[df[\"Risk\"]== 'bad'][\"Housing\"].value_counts().values,\n                name=\"Bad Credit\",\n                marker=dict(\n                color='#b2014e'\n                    )\n                )\n\ndata = [trace0, trace1]\n\nlayout = go.Layout(title='Housing Distribuition')\n\n\nfig = go.Figure(data=data, layout=layout)\nfig.data[0].marker.line.width = 4\nfig.data[0].marker.line.color = \"black\"\nfig.data[1].marker.line.width = 4\nfig.data[1].marker.line.color = \"black\"\n\npy.iplot(fig, filename='Housing-Grouped')\nplt.show()\n","b2ac8066":"#First plot\ntrace0 = go.Bar(\n                x = df_sm[df_sm[\"Risk\"]== 'good'][\"Job\"].value_counts().index.values,\n                y = df_sm[df_sm[\"Risk\"]== 'good'][\"Job\"].value_counts().values,\n                name='Good credit',\n                marker=dict(\n                color='#a4e23f'\n                    )\n                )\n\n#Second plot\ntrace1 = go.Bar(\n                x = df_sm[df_sm[\"Risk\"]== 'bad'][\"Job\"].value_counts().index.values,\n                y = df_sm[df_sm[\"Risk\"]== 'bad'][\"Job\"].value_counts().values,\n                name=\"Bad Credit\",\n                marker=dict(\n                color='#b2014e'\n                    )\n                )\n\ndata = [trace0, trace1]\n\nlayout = go.Layout(title='Job Distribuitio')\n\n\nfig = go.Figure(data=data, layout=layout)\nfig.data[0].marker.line.width = 4\nfig.data[0].marker.line.color = \"black\"\nfig.data[1].marker.line.width = 4\nfig.data[1].marker.line.color = \"black\"\n\npy.iplot(fig, filename='Job-Grouped')\nplt.show()\n","63134a0a":"fig, ax = plt.subplots(1,2,figsize=(15,5))\nsns.histplot(df_sm, x='Credit amount', bins=30, ax=ax[0]).set_title(\"Credit Amount (in Deutsch Mark) Distribution\");\nsns.histplot(df_sm, x='Duration', bins=30, ax=ax[1]).set_title(\"Duration (in month) Distribution\");\n\nfig, ax = plt.subplots(1,2,figsize=(15,5))\nsns.boxplot(data=df_sm, x='Risk', y='Credit amount', ax=ax[0]).set_title(\"Credit Amount (in Deutsch Mark) Distribution with Risk\");\nsns.boxplot(data=df_sm, x='Risk', y='Duration', ax=ax[1]).set_title(\"Duration (in month) Distribution with Risk\");","28138236":"corr = df_sm[['Age', 'Job', 'Housing', 'Saving accounts', 'Checking account', 'Credit amount', 'Duration']].corr()\nsns.set(rc={'figure.figsize':(11,7)})\nsns.heatmap(corr,linewidths=.5, annot=True, cmap=\"YlGnBu\",mask=np.triu(np.ones_like(corr, dtype=np.bool)))\\\n    .set_title(\"Pearson Correlations Heatmap\");","9f95ab4d":"df.isnull().sum()","db675d3e":"display(Markdown(\"#### Explore the Values of Text Columns:\"))\ncols = ['Sex', 'Housing', 'Saving accounts', 'Checking account', 'Purpose', 'Risk']\nfor col in cols:\n    line = \"**\" + col + \":** \"\n    for v in df[col].unique():\n        line = line + str(v) + \", \"\n    display(Markdown(line))","e6caa8c6":"# label encode account quality and fill NaN with 0\ndef SC_LabelEncoder(text):\n    if text == \"little\":\n        return 1\n    elif text == \"moderate\":\n        return 2\n    elif text == \"quite rich\":\n        return 3\n    elif text == \"rich\":\n        return 4\n    else:\n        return 0\n\ndf[\"Saving accounts\"] = df[\"Saving accounts\"].apply(SC_LabelEncoder)\ndf[\"Checking account\"] = df[\"Checking account\"].apply(SC_LabelEncoder)","d021b96e":"# label encode account quality and fill NaN with 0\ndef H_LabelEncoder(text):\n    if text == \"free\":\n        return 0\n    elif text == \"rent\":\n        return 1\n    elif text == \"own\":\n        return 2\n\ndf[\"Housing\"] = df[\"Housing\"].apply(H_LabelEncoder)","84035e7c":"# use LabelEncoder() to encode other categorical columns:\nfrom sklearn.preprocessing import LabelEncoder\nfor col in [\"Sex\", \"Purpose\", \"Risk\", \"Age\"]:\n    le = LabelEncoder()\n    le.fit(df[col])\n    df[col] = le.transform(df[col])\ndf.head()","ad4d4722":"cdf = df.drop(\"Risk\", axis=1)","ce89cb98":"\ninertias = []\n\nfor i in range(2,16):\n    kmeans = KMeans(n_clusters=i, random_state=0).fit(cdf)\n    inertias.append(kmeans.inertia_)\n\nplt.figure(figsize=(10,5))\nplt.title('The Elbow Method showing the optimal k')\nplt.plot(np.arange(2,16),inertias, marker='o', lw=2);","320cab36":"k = range(2,16)\nscore=[]\nfor n_cluster in k:\n    kmeans = KMeans(n_clusters=n_cluster).fit(cdf)\n    score.append(silhouette_score(cdf,kmeans.labels_))\nplt.figure(figsize=(10,5))\nplt.plot(k, score, 'o-')\nplt.xlabel(\"Value for k\")\nplt.ylabel(\"Silhouette score\")\nplt.title('Silhouette Method')\nplt.show()","bc0720e2":"km = KMeans(n_clusters=4, random_state=0)\nclusters = km.fit_predict(cdf)","16ee59b3":"acc = max((sum(clusters == df[\"Risk\"]) \/ len(df)), (sum(clusters != df[\"Risk\"]) \/ len(df)))\ndisplay(Markdown(\"The accuracy rate of 4-Means clustering is \" + str(acc)))","fc46dfbe":"df_clustered = cdf[['Age', 'Job', 'Housing', 'Saving accounts', 'Checking account', 'Credit amount', 'Duration']]\ndf_clustered[\"Cluster\"] = clusters\nsns.pairplot(df_clustered, hue=\"Cluster\",palette='tab10')","2c09c7b9":"# Spliting X and y into train and test version\nX, y = df.drop(\"Risk\", axis=1), df[\"Risk\"]\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.20, random_state=0)","6c24c621":"max_score = 0\nmax_k = 0\nfor k in range(1, 100):\n    neigh = KNeighborsClassifier(n_neighbors=k)\n    neigh.fit(X_train,y_train)\n    score = f1_score(y_test, neigh.predict(X_test))\n    if score > max_score:\n        max_k = k\n        max_score = score\n\ndisplay(Markdown(\"If use K-Nearest Neighbors Classification, the k should be \" + str(max_k) + \" to get best prediction, and then the  mean accuracy is \" + str(max_score)))","cbd6f1e6":"neig = np.arange(1, 25)\ntrain_accuracy = []\ntest_accuracy = []\n# Loop over different values of k\nfor i, k in enumerate(neig):\n    # k from 1 to 25(exclude)\n    knn_model = KNeighborsClassifier(n_neighbors=k)\n    # Fit with knn\n    knn_model.fit(X_train,y_train)\n    #train accuracy\n    train_accuracy.append(knn_model.score(X_train, y_train))\n    # test accuracy\n    test_accuracy.append(knn_model.score(X_test, y_test))\n\n# Plot\nplt.figure(figsize=[12,6])\nplt.plot(neig, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neig, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.title('Value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neig)\nplt.show()\nprint(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))","d95c61f3":"# define models\nModels = {\n    \"SVC\": SVC(),\n    \"DecisionTree\": DecisionTreeClassifier(),\n    \"RandomForest\": RandomForestClassifier(),\n    \"GaussianNaiveBayes\": GaussianNB(),\n   \n}","126e7436":"cv_results = pd.DataFrame(columns=['model', 'train_score', 'test_score'])\nfor key in Models.keys():\n    cv_res = model_selection.cross_validate(Models[key], X_train, y_train, \n                                             return_train_score=True,\n                                             scoring=\"f1\",\n                                             cv=5, n_jobs=-1)\n    res = {\n        'model': key, \n        'train_score': cv_res[\"train_score\"].mean(), \n        'test_score': cv_res[\"test_score\"].mean(),\n        'fit_time': cv_res[\"fit_time\"].mean(),\n        'score_time': cv_res[\"score_time\"].mean()\n        }\n    cv_results = cv_results.append(res, ignore_index=True)\n    print(\"CV for model:\", key, \"done.\")\ncv_results.style.background_gradient(cmap='Blues')","8fb56ae6":"rf = Models[\"RandomForest\"].fit(X_train, y_train)\nprint('f1_score:', f1_score(y_test, rf.predict(X_test)))","902147fd":"from sklearn.svm import SVC\nfeature_importance = pd.DataFrame()\nfeature_importance[\"feature\"] = X_train.columns\nfeature_importance[\"importance\"] = rf.feature_importances_\nfeature_importance = feature_importance.sort_values(\"importance\", ascending=False)\nfeature_importance","9d6f82a8":"**Analysis:** The \"elbow\" in above chart is indicated  at 4. The number of clusters chosen should therefore be 4. \n#### With 4 Clusters:","1108374f":"### Age and Sex Distributons","c517cfe0":"The first eight columns are the feature variables and the last column (Risk) is the target variable, which we want to classify as \u201cgood\u201d or \u201cbad\u201d. The purpose of the Machine Learning model is to capture the relations between the features and the target variables and predict the credit risk for future applicants.","82260b37":"## Conclusion\n1. Machine Learning based models are very pragmatic when it comes to minimizing the risk. Increasing number of banks are choosing such models for their risk analysis and to minimize the loss by preventing defaults. There are a multitude of Machine Learning techniques for the purpose of predicting the real values, classification, clustering etc. We only scratched the surface with our case. However, certain things should considered when applying Machine Learning techniques:\n\n1. The model itself should explain the result. If an algorithm denies a loan of an applicant, it is important for the bank to know the explanation. Otherwise they could even face legal obligations from the applicant. Therefore, it is sometimes reasonable to use relatively simple models (like logistic regression, Gradient boosted trees, Random Forest as in our case) instead of complicated models like Neural Networks.\n1. Recently Apple\u2019s credit card has been accused of discriminating against women. Models can be socially prejudiced as they take insights from the historical data. In our case too, the model predicted that the females are more likely to default. This is because, we have performed a Discrete-Time hazard model where the probability of default is a point-in-time event. A good model however should incorporate the evolution of the impacts of the features on the risk over a period. These kinds of models are called Through-the-cycle (TTC) models, which consider the influence of the macroeconomic situation, social evolution and other factors.","eb6de966":"# Exploratory Data Analysis\n\nAre females more likely to default or is it less risky to lend money to rich people? These kinds of questions can qualitatively be answered by visualizing the data. We create a sub-table for each feature variable in question.","22693f8b":"Now that we have all the features and target variable in the numerical form, we can finally train and fit our model. For that we should first split our data into a train set and a test set. The train set would be used to capture the relations between the features and the target variables while the test set will be used to verify the performance of the model.","e49384a0":"### Importing Libraries","91ecec3d":"Approving loans without proper scientific evaluation increases the risk of default. This can lead to bankruptcy of lending agencies and consequently the destabilization of the banking system. This is what happened in the 2008 financial crisis which affected the world economy adversely. Three components decide the amount of loss that a firm faces as a result of loan default:\n\n* Probability of Default (PD)\n* Exposure at Default (EAD)\n* Loss given Default (LGD)\nThe expected loss (E-Loss) is the simple product of these three quantities:\n\nELoss=PD\u22c5EAD\u22c5LGD\n\nOur focus here is on the Probability of Default (PD). Here, we will look at the example of German Credit data which is taken from the Kaggle database.","245387d0":"The values presented here are in percentage. It seems, the feature \u201cSex\u201d contains valuable information for the classification. In this data set, females are slightly more likely to default (however, this cannot be used as a general conclusion). We can better perceive it from the graph below","46ca6c14":"**If you have any questions, feel free to comment below and if you like this Karnel, please Upvote !**\n### Thank You for Your Review !\n","48b9a34f":"## Handle Missing Values\n\nMost Machine Learning models cannot handle missing values within the feature space, or these can adversely diminish the prediction power of the model. Therefore, we need to check for them","f541b651":"### Clustering","b398093a":"### K-Nearest Neighbors Classification","cf75a9aa":"# Build Model","04e1b6ee":"From above exploration:\n\n* Columns \"Housing\", \"Saving accounts\" and \"Checking accounts\" are Ordinal data.\n* Columns \"Sex\", \"Purpose\" and \"Risk\" are Categorical data.","d4c82dbb":"### Modeling by Other Classifiers\nSince KNN algorithm cost lots of memory and time for prediction, this section want to try some more classifiers.\n#### Model Selection with Cross Validate","7f619163":"Use K-means to cluster people in the records into 4 group and check if the result closed to given two risk groups","e41d99cc":"# Problem Statement","33da5fc5":"* Looking at the chart, the highest credit amount was reached at the age of 60 and around.\n* Looking at the graph, the highest loan amounts between 50-60 duration have been reached.","44300241":"As a first step, we look at the data. Numpy and Pandas libraries in python are excellent tools for data exploration. For the data visualization we mainly use the matplotlib and the seaborn libraries. We import these libraries into our workspace.","0d7f7dee":"Evaluate Model on Testing Set\n\n* Random Forest Classifier gives a good result on both train_score and test_score.\n* SVC and Gaussian Naive Bayes show the less over-fiting.\n* Gaussian Naive Bayes Classifier has least runtime.\n\nRandom Forest Classifier would tell feature importances, while SVC only return coef_ in the case of a linear kernel, which will be too slow.","8b43b3ff":"#### Start with applying Elbow Method.","22466492":"# Credit Risk Analysis Using Machine Learning","06a881de":"# Data Understanding ","82c401df":"# Data Pre-processing For Discrete Categorical Columns\nFrom the raw data we saw that the target variable risk is categorical (\u2018good\u2019 or \u2018bad\u2019). Therefore, this is a classification problem. There are many classification algorithms in the literature with the Random Forest classifier being considered one of the standard classifiers.\n\n"}}