{"cell_type":{"72e17995":"code","f5abf625":"code","3d2b75d6":"code","60d3028c":"code","7f423bba":"code","f1f8fa8d":"code","12cc043e":"code","214578d8":"code","516747b4":"code","dbe8993f":"code","ac7acebf":"code","8906c425":"code","2426dc3b":"code","e70f099f":"code","ef475379":"code","b7d801b1":"code","7f574e81":"code","c562529a":"code","32cdd1d9":"code","65b026c7":"code","abc0630b":"markdown","a50fa1fd":"markdown","55650cfd":"markdown","3b4a11b2":"markdown","5be898cb":"markdown","01144009":"markdown","60294adc":"markdown","29a3c741":"markdown","bb0433f3":"markdown","85d4f491":"markdown","02291036":"markdown","c345deab":"markdown","8cf12159":"markdown","425b9a06":"markdown","91ba015c":"markdown","78a6cea8":"markdown","4de9408b":"markdown","4eebfb7c":"markdown","abb74de4":"markdown","feee52e3":"markdown","5c03744b":"markdown"},"source":{"72e17995":"import sklearn.datasets as datasets","f5abf625":"iris = datasets.load_iris()\nprint(iris.DESCR)","3d2b75d6":"boston = datasets.load_boston()\nprint(boston.DESCR)\n","60d3028c":"from sklearn.feature_extraction import DictVectorizer\n\ndataset = [\n    {\"feature1\": \"value1\", \"feature2\": 1.0, \"feature3\": True},\n    {\"feature1\": \"value2\", \"feature2\": 1.5, \"feature3\": False},\n    {\"feature1\": \"value3\", \"feature2\": 2, \"feature3\": True},\n]\n\ndv = DictVectorizer()\nds = dv.fit_transform(dataset)\nprint(\"Dataset vectorized:\", ds.toarray())\nprint(\"Features:\", dv.get_feature_names())","7f423bba":"from  sklearn.preprocessing import StandardScaler\n\nss = StandardScaler()\naux_data = [\n    [1, 0, 2],\n    [1.5, 12, 3.2],\n    [0.1, 3, 4.2],\n]\n\nss.fit(aux_data)\nz_data = ss.transform(aux_data)\nprint(f'Means={ss.mean_},\\nVariances={ss.var_}')\nprint(f'Standarized data:\\n {z_data}')","f1f8fa8d":"from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n\nsample_data = [\n    [-1, 0 , 10],\n    [2, -3, 4],\n    [5, 6, 5]\n]\n\nmms = MinMaxScaler()\nmas = MaxAbsScaler()\n\nmms_out= mms.fit_transform(sample_data)\nmas_out= mas.fit_transform(sample_data)\nprint(f'MinMaxScaled=\\n{mms_out}\\nMaxAbsScaled=\\n{mas_out}')\n","12cc043e":"from sklearn.feature_extraction.text import CountVectorizer\ncorpus = [\n    \"Sample text\",\n    \"Another sample\",\n    \"sample content\"\n]\n\nvectorizer = CountVectorizer()\nvectorizer.fit_transform(corpus)\nanalyze = vectorizer.build_analyzer()\nnew_text = \"Another sample text content\"\nprint(vectorizer.vocabulary_)\nprint(analyze(new_text))\nprint(vectorizer.transform(analyze(new_text)).toarray())\nprint(vectorizer.transform([new_text]).toarray())\n","214578d8":"from sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\n\nfrom  sklearn.preprocessing import MinMaxScaler\nfrom  sklearn.preprocessing import PolynomialFeatures\n\nimport numpy as np\n\npl1 = Pipeline([('step_1', MinMaxScaler())])\npl2 = make_pipeline(MinMaxScaler(), PolynomialFeatures(2))\n\n#Changing default parameters\npl1.set_params(step_1__feature_range=(0,1))\npl2.set_params(minmaxscaler__feature_range=(0,1))\n\nX = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12]])\nprint(\"Pipeline1\", pl1.fit_transform(X))\nprint(\"Pipeline2\", pl2.fit_transform(X))","516747b4":"from sklearn.model_selection import train_test_split, cross_val_score\n\niris_train_X, iris_test_X, iris_train_y, iris_test_y = train_test_split(iris.data, iris.target)\nboston_train_X, boston_test_X, boston_train_y, boston_test_y = train_test_split(boston.data, boston.target)","dbe8993f":"from sklearn.naive_bayes import MultinomialNB\nmnb = MultinomialNB()\n\n\nmnb.fit(iris_train_X, iris_train_y)\nscore = mnb.score(iris_test_X, iris_test_y)\npredict = mnb.predict(iris_test_X)\n\nprint(score, predict)","ac7acebf":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\n\nknn.fit(iris_train_X, iris_train_y)\nscore = knn.score(iris_test_X, iris_test_y)\npredict = knn.predict(iris_test_X)\n\nprint(score, predict)","8906c425":"from sklearn.svm import SVC\nsvc = SVC()\n\nsvc.fit(iris_train_X, iris_train_y)\nscore = svc.score(iris_test_X, iris_test_y)\npredict = svc.predict(iris_test_X)\n\nprint(score, predict)","2426dc3b":"from sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom matplotlib import pyplot as plt\n\ndtc = DecisionTreeClassifier()\ndtc.fit(iris_train_X, iris_train_y)\n\n\nplt.figure(figsize=(18,18)) #To change default size\ntree.plot_tree(dtc, feature_names=iris.feature_names,\n               class_names=iris.target_names,\n               impurity=False, filled=True)\nplt.show() #only needed here because plt.figure() is used\n\nscore = dtc.score(iris_test_X, iris_test_y)\ntree_txt = tree.export_text(dtc)\nprint(score, tree_txt)","e70f099f":"from sklearn import tree\nfrom sklearn.tree import DecisionTreeRegressor\nfrom matplotlib import pyplot as plt\n\ndtr = DecisionTreeRegressor(max_depth=3)\ndtr.fit(boston_train_X, boston_train_y)\n\nplt.figure(figsize=(18,18)) #To change default size\ntree.plot_tree(dtr, feature_names=boston.feature_names,\n               impurity=False, filled=True)\nplt.show() #only needed here because plt.figure() is used\n\nscore = dtr.score(boston_test_X, boston_test_y)\ntree_txt = tree.export_text(dtr)\nprint(score, tree_txt)","ef475379":"from sklearn.linear_model import SGDRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import datasets\n\nsgdr = SGDRegressor(loss=\"epsilon_insensitive\")\nreg = make_pipeline(StandardScaler(),\n                    sgdr)\nboston = datasets.load_boston()\nreg.fit(boston_train_X, boston_train_y)\nscore = reg.score(boston_test_X, boston_test_y)\nprint(score)","b7d801b1":"from sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\n\nparam_grid = [\n  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n ]\n\nsvc = SVC()\ngs = GridSearchCV(svc, param_grid)\ngs.fit(iris_train_X, iris_train_y)\nprint(gs.cv_results_)\n\nr = gs.cv_results_\naux = zip(r['rank_test_score'], r['mean_test_score'], r['std_test_score'], r['params'])\n\nprint(\"*** Ranking ***\")\nprint(\"'rank_test_score', 'mean_test_score', 'std_test_score', 'params'\")\nfor s in sorted(aux, key=lambda x: x[0]):\n    print(s)","7f574e81":"from sklearn.svm import SVC\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy import stats\n\nparam_grid_random = {'C': stats.expon(scale=100), 'gamma': stats.expon(scale=.1), 'kernel': ['rbf']}\n\nsvc = SVC()\nrs = RandomizedSearchCV(svc, param_grid_random)\nrs.fit(iris_train_X, iris_train_y)\nprint(rs.cv_results_)\n\nr = rs.cv_results_\naux = zip(r['rank_test_score'], r['mean_test_score'], r['std_test_score'], r['params'])\n\nprint(\"*** Ranking ***\")\nprint(\"'rank_test_score', 'mean_test_score', 'std_test_score', 'params'\")\nfor s in sorted(aux, key=lambda x: x[0]):\n    print(s)","c562529a":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.svm import SVC\n\nbc = BaggingClassifier(base_estimator=SVC())\nbc.fit(iris_train_X, iris_train_y)\nbc.score(iris_test_X, iris_test_y)","32cdd1d9":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn import tree\nfrom matplotlib import pyplot as plt\n\nrfc = RandomForestClassifier(n_estimators=4, max_depth=2)\nrfc.fit(iris_train_X, iris_train_y)\n\n\nfig, ((a1,a2),(a3,a4)) = plt.subplots(2,2, figsize=(18,18))\naxs = [a1,a2,a3,a4]\nfor tc,ax in zip(rfc.estimators_, axs):\n    tree.plot_tree(tc,\n                   feature_names=iris.feature_names, class_names=iris.target_names,\n                   impurity=False, filled=True, ax=ax)\nplt.show()\nscore = rfc.score(iris_test_X, iris_test_y)\nimportances = rfc.feature_importances_\n\nprint(iris.feature_names)\nprint(importances)\nprint(score)","65b026c7":"from sklearn.ensemble import AdaBoostRegressor\n\nabr = AdaBoostRegressor(learning_rate=.5)\nabr.fit(boston_train_X, boston_train_y)\nabr.score(boston_test_X, boston_test_y)\n","abc0630b":"From AI-Bootcamp: https:\/\/github.com\/v-fuentec\/AI-bootcamp\n\n# Basic terms\n\nMachine Learning:\n* from [Tom Michell Book]: \u201cA computer  program is  said to learn from experience E with respect to  some class of tasks T and performance  measure P, if  its performance at  tasks in T, as measured by P, improves with experience E.\n* source: http:\/\/www.cs.cmu.edu\/afs\/cs.cmu.edu\/user\/mitchell\/ftp\/mlbook.html\n\nWe use the following nomenclature for predictive modelling:\n* X: \u201cpredictor variable\u201d, \u201cindependent variable\u201d, \u201cattribute\u201d, \u201cfeature\u201d, \u201cpredictor\u201d\n* Y: \u201ctarget variable\u201d, \u201cdependent variable\u201d, \u201cresponse\u201d, \u201coutcome measurement\u201d (sometimes $G$ is used for categorical variables)\n * To refer to our estimation or predicted values we find a couple of terms: $\\hat{y}$, $h_\\theta(x)$, $\\hat{g}$ (used sometimes when the output is qualitative or categorical)...\n\n\nSupervised Learning\n\n* From [Machine Learning Course: Andrew Ng]: In supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output.\nSupervised learning problems are categorized into \"regression\" and \"classification\" problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function.\nIn a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories.\n\nUnsupervised learning\n* From [Machine Learning Course: Andrew Ng]: Unsupervised learning allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don't necessarily know the effect of the variables.\nWe can derive this structure by clustering the data based on relationships among the variables in the data.\nWith unsupervised learning there is no feedback based on the prediction results.\n\nUnderfit or high bias:\n* Not even fitting training data very well. It's like having a very strong preconception or biases regarding the nature of the data.\n\nOverfit or high variance:\n* Hypothesis function can fit almost any function and we don\u2019t have enough data to constraint it to give us a good hypothesis.\n\nThe Curse of Dimensionality:\n* \"The curse of dimensionality, first introduced by Bellman, indicates that the number of samples needed to estimate\nan arbitrary function with a given level of accuracy grows exponentially with respect to the number of input variables (i.e., dimensionality) of the function.\" Source: https:\/\/link.springer.com\/referenceworkentry\/10.1007%2F978-0-387-39940-9_133\n* This is also manifested in that for higher dimensionality the samplying density is lower.\n * \"in high dimensions all feasible training samples sparsely populate the input space\" [Friedman, J., Hastie, T., & Tibshirani, R. (2001). The elements of statistical learning (Vol. 1, No. 10). New York: Springer series in statistics.]\n\nThe \"No Free Lunch\" (NFL) theorem:\n* \"any two optimization algorithms are equivalent when their performance is averaged across all possible problems\". Wolpert, D.H., and Macready, W.G. (2005) \"Coevolutionary free lunches\", IEEE Transactions on Evolutionary Computation, 9(6): 721\u2013735\n* So, no method works best across all problems...","a50fa1fd":"# Supervised Learning\n\nClass RegressorMixin has method:\n* score(X, y, sample_weight=None) Returns the coefficient of determination $R^2$ of the prediction.\n\nClass ClassifierMixin has method:\n* score(X, y, sample_weight=None). Returns the mean accuracy on the given test data and labels.\n\n## Linear Models\n\nBase class LinearModel has methods\n* fit(X, y)\n* predict(X).\nSee: https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.linear_model\n\n### Regression: Linear regression\n* sklearn.linear_model.LinearRegression\n* $h_\\theta(x) = \\theta^T x$ ($x_0$=1)\n * Also written as $\\hat{Y} = \\hat{\\beta_0} + \\sum_j X_j \\hat{\\beta_j}$\n * or if we include a 1 as first component of each vector $x_j$, $\\hat{Y} = X^T\\hat{\\beta}$\n* Minimization objective: $\\min_{w} || X w - y||_2^2$\n\nSome attributes:\n* coef_: array of shape (n_features, ) or (n_targets, n_features). Estimated coefficients for the linear regression problem.\n* intercept_: float or array of shape (n_targets,). Independent term in the linear model.\n\n### Ordinary Least Squares for Linear Regression\n\nLooking for the parameters of vector $b$ to fit the formula $y_i = x_i^T \\beta_i + \\epsilon_i$, where:\n* Usually $x_{i,1} = 1$ and $\\beta_1$ is called the intercept.\n* $\\epsilon_i$ represents unobserved random variables (called residuals or errors).\n\nPrinciple of least squares:\n\"minimizing the sum of the squares of the differences between the observed\ndependent variable (values of the variable being observed) in the given dataset\nand those predicted by the linear function.\"\nsource: https:\/\/en.wikipedia.org\/wiki\/Ordinary_least_squares\n\nThe formula for the Residual Sum of Squares:\n* $RSS(\\beta) = \\sum_i (y_i-x_i^T\\beta)^2$\n\n### Generalized Additive Models\n\n$g(\\mathbb{E}[y|X]) = \\beta_0 + f_1(X_1) + f_2(X_2, X3) + \\ldots + f_M(X_N)$\n\nWith Python's library \"pygam\":\n* \"The feature functions $f_i()$ are built using penalized B splines, which allow us to automatically model non-linear\nrelationships without having to manually try out many different transformations on each variable.\"\nSource: https:\/\/pygam.readthedocs.io\/en\/latest\/notebooks\/tour_of_pygam.html\n\n### Classification: Logistic regression\n* sklearn.linear_model.LogisticRegression(penalty='l2')\n* $h_\\theta(x) = P(y=1 | x; \\theta) = g(\\theta^Tx) = \\frac{1}{1+e^{-\\theta^Tx}}$\n * $g(z)\\ge 0.5\\ when\\ z \\ge 0$\n * $x_0$=1\n* Minimization objective\n * $\\min_{w, c} regularizationTerm + \\sum_{i=1}^n \\log(\\exp(- y_i (X_i^T w + c)) + 1)$\n\nSome attributes:\n* coef_ : coef_ndarray of shape (1, n_features) or (n_classes, n_features). Coefficient of the features in the decision function.\n* intercept_: intercept_ndarray of shape (1,) or (n_classes,). Intercept (a.k.a. bias) added to the decision function.\n\n## Naive Bayes\nBy aplying the Baye's Theorem with the assumption of conditional independence between every pair of features:\n* $P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) P(x_1, \\dots, x_n \\mid y)}{P(x_1, \\dots, x_n)}$\n* assumption: $P(x_i | y, x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n) = P(x_i | y)$\n* We obtain: $P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) \\prod_{i=1}^{n} P(x_i \\mid y)}{P(x_1, \\dots, x_n)}$\n* With a constant input we can clasify with:\n * $P(y \\mid x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y)$\n * $\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i \\mid y) $\n\nThe error rate of the Bayes classifier is called the Bayes error rate or Bayes rate.\n\nFrom sklearn's API:\n* \"The different naive Bayes classifiers differ mainly by the assumptions they make regarding the distribution of $P(x_i \\mid y)$\"\n* sklearn.naive_bayes.BernoulliNB(*[, alpha, \u2026]): Naive Bayes classifier for multivariate Bernoulli models.\n* sklearn.naive_bayes.CategoricalNB(*[, alpha, \u2026]): Naive Bayes classifier for categorical features\n* sklearn.naive_bayes.ComplementNB(*[, alpha, \u2026]): The Complement Naive Bayes classifier described in Rennie et al.\n* sklearn.naive_bayes.GaussianNB(*[, priors, \u2026]): Gaussian Naive Bayes (GaussianNB)\n* sklearn.naive_bayes.MultinomialNB(*[, alpha, \u2026]): Naive Bayes classifier for multinomial models","55650cfd":"## Decision Trees\n\nThe idea of this tree model is:\n* Supervised learning method used for classification or regression.\n* We partition our input data into regions (disjoint data subsets) starting at the root node.\n* Each non-leaf node contains a decision or rule to perform said partitioning.\n * The quality of each split is measured by an impurity metric: entropy, Gini...\n* Leaf nodes contains the model prediction for the input.\n* Many algorithm variants for building the tree: ID3, C4.5, C5, CART...\n\nDecision trees on sklearn:\n\n* Decision Tree Classifier: sklearn.tree.DecisionTreeClassifier(criterion='gini', max_depth=None, min_samples_split=2, ...)\n* Decision Tree Regression: sklearn.tree.DecisionTreeRegressor(criterion='mse', max_depth=None, min_samples_split=2, ...)\n* For both, as usual, methods: fit, predict, ...\n* To plot the tree: sklearn.tree.plot_tree(decision_tree, max_depth=None, feature_names=None, class_names=None, impurity=True, filled=False, ...)\n * For more check methods sklearn.tree.export_graphviz and sklearn.tree.export_text\n * Attribute \"tree_\" provides access the tree itself.\n\n### Example DecisionTreeClassifier","3b4a11b2":"Boston house prices dataset (regression)","5be898cb":"## Using pandas Dataframes\n\nWe can simply use pandas DataFrames e.g.:\n* data = pandas.read_csv('my_dataset.csv')\n* X, y = data.drop(columns=['label'], axis=1), data['label']\n\n## Preprocessing\n\nPackage \"sklearn.preprocessing\"\n\n### Standarization\n\n\"sklearn.preprocessing.StandardScaler\"\n* Standardize features by removing the mean and scaling to unit variance\n* The standard score of a sample x is calculated as: z = (x - u) \/ s\n\n\nSome methods:\n* fit(X[, y, sample_weight]) Compute the mean and std to be used for later scaling.\n* transform(X[, copy])  Perform standardization by centering and scaling\n* fit_transform(X[, y]) Fit to data, then transform it.","01144009":"### Scaling features to a range\n\nWe can use:\n* sklearn.preprocessing.MinMaxScaler([feature_range=(min, max)])\n * Transform features by scaling each feature to a given range.\n* sklearn.preprocessing.MaxAbsScaler()\n * This estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0.\n * It does not shift\/center the data, and thus does not destroy any sparsity.\n\nFor both use .fit() and .transform() or .fit_transform()\n\nCheck:\n* https:\/\/scikit-learn.org\/stable\/auto_examples\/preprocessing\/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py\n\nNOTES from URL:\n* Both StandardScaler and MinMaxScaler are very sensitive to the presence of outliers.\n* MaxAbsScaler also suffers from the presence of large outliers.","60294adc":"# Sklearn datasets","29a3c741":"Inside package \"sklearn.datasets\" we can access a number of well-known datasets. We can use the methods \"sklearn.datasets.load_<dataset_name>()\". The loaded dataset is returned in the form of a dictionary with the following keys:\n* 'data': dataset input features\n* 'target': desired output\n* 'DESCR': String description of the dataset.\n\nIris plants dataset (classification)","bb0433f3":"## Ensembles\n\nEnsembles consist on a combination multiple base models into one with better predictive performance.\n\n## Bagging (Bootstrap AGGregatING)\n\nBagging consists in training $l$ unstable models (slight changes in data generates large difference in fitted model):\n* If training set consists of size $n$, create $l$ sets of size $n$ by sampling with replacement.\n * Each bootstrap sample is expected to have approximately 63% of different instances from the training set.\n* We train each model on its respective new training set.\n* Metamodel prediction is obtained via voting or averaging (on regresion).\n\nsklearns' API:\n* sklearn.ensemble.BaggingClassifier(base_estimator=None, n_estimators=10, max_samples=1.0, max_features=1.0, bootstrap=True, ...)\n * Default base estimator is DecisionTreeClassifier.\n* sklearn.ensemble.BaggingRegressor(base_estimator=None, n_estimators=10, max_samples=1.0, max_features=1.0, bootstrap=True, ...)\n * Default base estimator is DecisionTreeRegressor.","85d4f491":"### Random Forest\n\nRandom forest is an ensemble of decision trees:\n* Each tree has a \"vote\" on the global prediction.\n* To build each tree:\n * We sample our training data with replacement $n$ (size of training set).\n * At each node we select a random subset of features of size $m$ to use for that node.\n\nsklearn's API:\n* sklearn.ensemble.RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=None, min_samples_split=2, ...)\n* sklearn.ensemble.RandomForestRegressor(n_estimators=100, criterion='mse', max_depth=None, min_samples_split=2, ...)\n* As usual, methods: fit(), predict(),...\n\n### Example of RandomForestClassifier showing internals","02291036":"### Example DecisionTreeRegressor","c345deab":"## Support Vector Machines (SVM)\n\n### SVM and kernels (a.k.a. \"the kernel trick\")\nWe can map from a non-linearly separable data into a higher dimension where the linear classifier is able separate them. The idea is:\n* Choose some landmarks $l^{(i)}$ and a similarity function $f_i(x, l^{(i)})$called kernel.\n* Use $f_i(x, l^{(i)})$ as features for our SVM. We have mapped from an n-dimensional space (number of original features of x) to a m-dimensional space (number of landmarks). Usually $m>>n$.\n\n### sklearn SVM API\n\nClassification with SVC: sklearn.svm.SVC\nRegression with SVR: sklearn.svm.SVR\n\n* Some common parameters (kernel='rbf', degree=3, C=1.0)\n * kernel={\u2018linear\u2019, \u2018poly\u2019, \u2018rbf\u2019, \u2018sigmoid\u2019, \u2018precomputed\u2019}\n  * If kernel='poly' constructor parameter degree is used.\n  * For linear kernel is faster to use sklearn.svm.LinearSVC or sklearn.svm.LinearSVR\n * C: Regularization parameter\n* use fit(X,y) and predict(X) as usual","8cf12159":"## Hyper-parameter tuning for an estimator\n\n### Exahustive grid search: GridSearchCV\n\nsklearn.model_selection.GridSearchCV(estimator, param_grid, cv=None)\n* param_grid: dict or list of dictionaries. Dictionary with parameters names (str) as keys and lists of parameter\nsettings to try as values, or a list of such dictionaries, in which case the grids spanned by each dictionary in the\nlist are explored. This enables searching over any sequence of parameter settings.\n\nAttributes:\n* cv_results_: dict of numpy (masked) ndarrays. A dict with keys as column headers and values as columns,\nthat can be imported into a pandas\n * cv_results_['params']\n * cv_results_['mean_test_score']\n * cv_results_['std_test_score']\n * cv_results_['rank_test_score']\n\n\nExample:","425b9a06":"## Boosting\n\nWith Boosting:\n* The metamodel builds incrementally (one model is added with each iteration).\n* Each base-learner (models used internaly by boosting) is trained on the mistake of the previous one.\n* Training set is selectively sampled:\n * All instances star with the same probability of being chosen.\n * Instances wrongly predicted increase their probability of being chosen on the next iteration.\n* Metamodel prediction is calculated by weighted voting (relative to each model accuracy on the training set).\n\nsklearn's API:\n* Classification: sklearn.ensemble.AdaBoostClassifier(base_estimator=None, n_estimators=50, ...)\n * Default base estimator: DecisionTreeClassifier initialized with max_depth=1.\n* Regresion: sklearn.ensemble.AdaBoostRegressor(base_estimator=None, n_estimators=50, ...)\n * Default base estimator: DecisionTreeRegressor initialized with max_depth=3.","91ba015c":"## Using our own data dict with scikit-learn estimators\n\nThe class \"sklearn.feature_extraction.DictVectorizer\" allow us to transform a dictionary of features into vector form.\n* When feature values are strings, this transformer will do a binary one-hot (aka one-of-K) coding\n* If a feature value is a sequence or set of strings, this transformer will iterate over the values and will count the occurrences of each string value.\n* Note that this transformer will only do a binary one-hot encoding when feature values are of type string\n * For categorical features are represented as numeric values use OneHotEncoder after\n\nSome important methods:\n* fit(X[, y]) Learn a list of feature name -> indices mappings.\n* transform(X) Transform feature->value dicts to array or sparse matrix.\n* fit_transform(X[, y]) Learn a list of feature name -> indices mappings and transform X.\n * Like fit(X) followed by transform(X), but does not require materializing X in memory.\n* get_feature_names() Returns a list of feature names, ordered by their indices.","78a6cea8":"## Other transformations\n\n* sklearn.preprocessing.OneHotEncoder\n* sklearn.preprocessing.KBinsDiscretizer: discretizes features into k bins\n* sklearn.preprocessing.Binarizer(threshold=0.0): feature binarization is the process of thresholding numerical features to get boolean values\n* sklearn.feature_extraction.text.CountVectorizer(ngram_rangetuple=(min_n, max_n), min_df=1,...): implements both tokenization and occurrence counting\n * vectorizer.fit_transform(corpus)\n * analyze = vectorizer.build_analyzer()\n* sklearn.preprocessing.LabelBinarizer(...)\n* sklearn.preprocessing.MultiLabelBinarizer(classes=None, ...)\n* sklearn.preprocessing.PolynomialFeatures: generates polynomial and interaction features up to a given degree. E.g. [a,b], degree=2 yields [1, a, b, a\u00b2, ab, b\u00b2]\n* sklearn.compose.ColumnTransformer(transformers...): Transformers is a list of tuples (name, transformer, columns)","4de9408b":"## Pipelines\n\nPipelines can be used to chain multiple estimators into one.\n* sklearn.pipeline.Pipeline or sklearn.pipeline.make_pipeline (fills the names of the pipeline steps)\n* Encapsulate data preprocessing and model\n* Call fit(), fit_transform(), predict()... only once!\n* All estimators in a pipeline, except the last one, must be transformers\n\nYou can access the pipeline paramers for all steps with:\n* pipe.pipe.set_params(<estimator>__<parameter>=value)","4eebfb7c":"## K-Nearest Neighbours: Supervised Learning\n\nK-Nearest Neighbours is a type of instance-based learning that assigns uses closest neighbours (obtained using some distance metric) to $x$, $N_k(x)$, to  calculate $\\hat{y}$.\n* Class: sklearn.neighbors.NearestNeighbors(n_neighbors=5, algorithm='auto',...)\n* For classification: sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, algorithm='auto', ...)\n* For Regression: sklearn.neighbors.KNeighborsRegressor(n_neighbors=5, algorithm='auto', ...)\n\n### related: K-Means algorithm (Unsupervised Learning, Clustering)\n\nGiven a desired number of clusters k:\n* Randomly initialize k cluster centroids $\\mu_i$(centers).\n* Repeat until convergence...\n * Assign clusters to all data points.\n * $x^{(i)}$ is assigned to cluster $c^{(i)}$ with closest centroid $\\mu_c^{(i)}$.\n * Move cluster centroid ($\\mu_k$) to the average of points on the cluster.","abb74de4":"### Randomized search: RandomizedSearchCV\n\nClass sklearn.model_selection.RandomizedSearchCV(estimator, param_distributions, n_iter=10, cv=None)\n* The parameters of the estimator used to apply these methods are optimized by cross-validated search over parameter settings.\n* In contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions.\n* The number of parameter settings that are tried is given by n_iter\n\nTo use as sampling sources for parameters:\n* scipy.stats: contains many useful distributions for sampling parameters, such as expon, gamma, uniform or randint\n\nAttributes:\n* cv_results_ Same as GridSearchCV\n\nRewritting the previous example:","feee52e3":"## Stochastic Gradient Descent (SGD)\n\nGradient Descent:\n* If a multivariate function $F(x)$ is differentiable in the neighbourhood of a point $a$, then $F(x)$ decreases fastest in the direction of the negative gradient at $a$, that is $- \\nabla F(a)$.\n* Analogy: descending a foggy mountain, you only see a certain area close to you with the goal of reaching the lowest ground.\n* Feature Scaling: make sure that all features have similar value ranges, it helps gradient descent to converge faster.\n* We can search the minimum by following an iterative process such as:\n * $a_{n+1}=a_n - \\alpha \\nabla F(a_n) ; n \\ge 0$.\n * $\\alpha$ is the learning rate\n\nGradient Checking:\n* Less efficient\n* Instead of calculating the derivative to obtain the gradient at a given point you can numerically approximate the derivative by calculating:\n * $\\frac{\\delta f}{\\delta \\theta} \\simeq \\frac{f(\\theta+\\epsilon)-f(\\theta-\\epsilon)}{2\\epsilon}$\n\nSGD is sensitive to feature scaling!\n\nsklearn's SGD API:\n* Classification: sklearn.linear_model.SGDClassifier(loss='hinge', *, penalty='l2', alpha=0.0001, ...)\n* Regression: sklearn.linear_model.SGDRegressor(loss='squared_loss', *, penalty='l2', alpha=0.0001, ...)\n\nModel used internally depends on selected loss function L.\n\nSome SGDClassifier Losses:\n* \u2018hinge\u2019: SVC\n* \u2018log\u2019: Logistic Regression\n* \u2018perceptron\u2019: Perceptron\n\nExample of loss for SGDRegressor:\n* \u2018epsilon_insensitive\u2018: SVR","5c03744b":"# Evaluation\n\n* Train & test: sklearn.model_selection.train_test_split\n * Parameters: train_size, test_size, shuffle...\n * X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n* K-Fold evaluation: sklearn.model_selection import cross_val_score\n * scoring: str or callable, default=None (None uses estimator.score)\n * cv: if int number of folds\n * scores = cross_val_score(estimator, X, y, cv=5)\n * To just obtain sample indexes use class:\n sklearn.model_selection.KFold(n_splits=5, shuffle=False)"}}