{"cell_type":{"0c984e8e":"code","88cd82ff":"code","821fbf35":"code","5480c6b1":"code","fbd1ab77":"code","a167a015":"code","ce8ce521":"code","d704d3a9":"code","6c6d5831":"code","a9163305":"code","40f548c9":"code","4a40a9d2":"code","c22e82f1":"code","800dac63":"markdown","2b0aae1c":"markdown","b08a24a2":"markdown","b65b1df7":"markdown","fb168edd":"markdown","d768b703":"markdown","dc9b81a8":"markdown","dfa99f14":"markdown","661d0601":"markdown","9d1ca2cc":"markdown","96778dd0":"markdown","9f17e487":"markdown","7e75aea2":"markdown","8ec0f419":"markdown","ffd99771":"markdown","4666e16a":"markdown","c105444e":"markdown","4d2e2ddd":"markdown","a5c6cde2":"markdown","7933a6c7":"markdown","140fd4e1":"markdown","d4e9adbd":"markdown","90722650":"markdown","68d58e00":"markdown"},"source":{"0c984e8e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans","88cd82ff":"mall_data = pd.read_csv(\"..\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv\")\nmall_data.head()","821fbf35":"mall_data.info()","5480c6b1":"# We can use a heatmap to check correlation between the variables.\ncorr = mall_data.corr()\nplt.figure(figsize=(8,8))\nsns.heatmap(corr,cbar=True,square=True,fmt='.1f',annot=True,cmap='Reds')","fbd1ab77":"# Which gender shops more?\nplt.figure(figsize=(10,10))\nsns.countplot(x=\"Gender\", data=mall_data)","a167a015":"# People of what ages shop more?\nplt.figure(figsize=(16,10))\nsns.countplot(x=\"Age\", data=mall_data)","ce8ce521":"# Is there really no relationship between annual income and spending score?\nplt.figure(figsize=(20,8))\nsns.barplot(x='Annual Income (k$)',y='Spending Score (1-100)',data=mall_data)","d704d3a9":"# For our model, we can choose whatever variables we think are relevant or necessary, we needn't choose all.\n# I'm going to choose age, annual income and spending score columns for my clustering model.\nX = mall_data.iloc[:,[2,3,4]].values\nX","6c6d5831":"wcss = []\nfor i in range(1,11): # It will find wcss value for different number of clusters (for 1 cluster, for 2...until 10 clusters) and put it in our list\n    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=50)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\nwcss","a9163305":"# elbow graph\nsns.set()\nplt.plot(range(1,11),wcss)\nplt.xlabel(\"Number of clusters\")\nplt.ylabel(\"WCSS value\")\nplt.show()","40f548c9":"kmeans = KMeans(n_clusters = 5, init = 'k-means++',random_state = 0)","4a40a9d2":"# we need a label for each datapoint relative to their clusters (will be split into 5 clusters and each will be labelled 0-4)\ny = kmeans.fit_predict(X)","c22e82f1":"# 3d scatterplot using matplotlib\n\nfig = plt.figure(figsize = (10,10))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X[y == 0,0],X[y == 0,1],X[y == 0,2], s = 40 , color = 'red', label = \"cluster 1\")\nax.scatter(X[y == 1,0],X[y == 1,1],X[y == 1,2], s = 40 , color = 'blue', label = \"cluster 2\")\nax.scatter(X[y == 2,0],X[y == 2,1],X[y == 2,2], s = 40 , color = 'green', label = \"cluster 3\")\nax.scatter(X[y == 3,0],X[y == 3,1],X[y == 3,2], s = 40 , color = 'yellow', label = \"cluster 4\")\nax.scatter(X[y == 4,0],X[y == 4,1],X[y == 4,2], s = 40 , color = 'purple', label = \"cluster 5\")\nax.set_xlabel('Age of a customer-->')\nax.set_ylabel('Anual Income-->')\nax.set_zlabel('Spending Score-->')\nax.legend()\nplt.show()","800dac63":"# Building the model","2b0aae1c":"# Unsupervised Machine Learning","b08a24a2":"**Machine learning is divided into supervised and unsupervised learning. In supervised learning, we train our model with data that we have previously acquired (labelled data). But, in unsupervised learning, our data is not labelled so our model must first self-discover any naturally occurring patterns in that training data set.**","b65b1df7":"# Importing libraries and dataset","fb168edd":"**Now, we need to find the optimal number of clusters for this dataset. To do that, we will use WCSS (within-cluster-sum-of-squares) parameter. WCSS is the sum of squares of the distances of each data point in all clusters to their respective centroids. The idea is to minimise the sum.**","d768b703":"**According to the data, females tend to shop more than males.**","dc9b81a8":"# Exploratory Data Analysis","dfa99f14":"**This method aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean.**","661d0601":"**There were two points from where the WCSS value dropped significantly, 3 clusters and 5 clusters. These are the elbow points. After 5, there is no more significant change in value so 5 is the most suitable number of clusters.**","9d1ca2cc":"So basically the workflow is like this: Import libraries and dataset -> check for missing values -> perform necessary imputation -> Exploratory Data Analysis -> choose variables for clusters -> find number of clusters (WCSS) -> build model -> visualize and analyze.\n\n**To get better accuracy, try using more training data, different number of clusters or different number of variables.**","96778dd0":"**This is very interesting evidence, as one must think that people with higher incomes should shop more but as we can see, people of low incomes tend have very similar shopping frequencies as people of higher incomes but people of mediocre incomes tend to shop much less in comparison.**","9f17e487":"**We can see that there's basically no correlation between any of the variables. Just a mild inverse relationship between age and spending score but it's not effective enough for us to consider.**","7e75aea2":"# Clusters","8ec0f419":"# K-means Clustering","ffd99771":"**The higher the spending score of a person, the more they shop in that mall.**","4666e16a":"# Conclusion","c105444e":"**As we can see, 10 clusters have minimum wcss but we need to check elbow points (later explained) so let's still visualize it.**","4d2e2ddd":"# Visualizing the clusters","a5c6cde2":"# Missing Values","7933a6c7":"**This illustration tells us that people tend to shop more in their 30s and least in their 60s.**","140fd4e1":"# Introduction","d4e9adbd":"**This notebook is a guide for beginners into machine learning, k-means clustering to be more specific. There will be comments every step of the way so there is a clear understanding. We will be working on an unlabelled dataset that contains data about customer. We'll be using unsupervised learning - k-means clustering to group the different types of customers that go to the mall.**","90722650":"**Good thing we have no missing values in this dataset so no imputation (replacing missing values with other appropriate ones) necessary.**","68d58e00":"**As we can see, clusters 3 and 5 have higher spending scores.**\n\n**Cluster 3 are people aged less than 30 with very high annual incomes, them having a high spending score makes sense. So, to keep this going on, these people could be given better offers to attract them.**\n\n**Cluster 5 are also people aged less than 30 but with low incomes. As this group of people do like to shop, they can be offered discounts and buy get more free offers to attract them more.**"}}