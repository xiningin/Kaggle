{"cell_type":{"fe01f46d":"code","bc1c47b5":"code","e52df0a8":"code","1ee79f8f":"code","68eb3d1d":"code","722d3c36":"code","6165fa40":"code","0f56addd":"code","b69e8355":"code","2bf6e1e8":"code","80a525ef":"code","3914301c":"code","b2b1db27":"code","86b4651c":"code","37c9f8f7":"code","81e14ec3":"code","52d4dc86":"code","e6a17477":"code","ff8be1b8":"code","44d613c6":"code","818a0a20":"code","c2e1aa1c":"code","1e570b1f":"markdown","27e32c77":"markdown","2941cb43":"markdown","5763bd05":"markdown","a7227acd":"markdown","daba76dd":"markdown","371265a8":"markdown","03a25bad":"markdown"},"source":{"fe01f46d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for detailed visualizations\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","bc1c47b5":"# read the data\n\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","e52df0a8":"# show the data\ntrain.head()","1ee79f8f":"# getting to know more about the data\ntrain.info()","68eb3d1d":"# the sum of the null in columns\ntrain.isnull().sum()","722d3c36":"# dropping the capin column \n\ntrain =  train.drop(columns = ['Cabin'],axis=1)\ntest =  test.drop(columns = ['Cabin'],axis=1)\n\n# dropping examples with nan value for Embarked column which well not really affect the data\ntrain = train.dropna(axis = 0,subset=['Embarked'],how='any')\ntrain.head()","6165fa40":"# heatmap of correlations (which you can generate features and get intiuations)\n\nplt.figure(figsize=(10,6))\nsns.heatmap(train.corr(),annot=True);","0f56addd":"# Nearly 20% of data is missing, can't risk losing all of the data\ntrain.Age.isnull().mean() *100","b69e8355":"# filling the nan with values\n\n# filling the age values in the train and test dataframes\ntrain['Age'].fillna( train['Age'].median() , inplace=True)\ntest['Age'].fillna( train['Age'].median() , inplace=True)\n\n# filling the Fare column with the median in the test dataframe\ntest['Fare'].fillna( train['Fare'].median() , inplace=True)","2bf6e1e8":"# No missing values now\ntrain.info()","80a525ef":"test.info()","3914301c":"# drop those who don't have logical impact eg : names, id's...\n\n# Sex\ntrain[\"Sex\"] = pd.get_dummies(train[\"Sex\"],drop_first=True)\ntest[\"Sex\"]= pd.get_dummies(test[\"Sex\"],drop_first=True)\n\n# Name\ntrain =  train.drop(columns = ['Name'],axis=1)\ntest =  test.drop(columns = ['Name'],axis=1)\n\n# Ticket Number\ntrain =  train.drop(columns = ['Ticket'],axis=1)\ntest =  test.drop(columns = ['Ticket'],axis=1)\n\n# Id\nID = test.PassengerId\ntrain =  train.drop(columns = ['PassengerId'],axis=1)\nids = test.PassengerId\ntest =  test.drop(columns = ['PassengerId'],axis=1)\n\n# Embarked\ntrain[\"Embarked\"]= np.where(train[\"Embarked\"]==\"C\",1,np.where(train[\"Embarked\"]==\"S\",2,3))\ntest[\"Embarked\"]= np.where(test[\"Embarked\"]==\"C\",1,np.where(test[\"Embarked\"]==\"S\",2,3))","b2b1db27":"#show the data after the change\ntrain.head()","86b4651c":"# setting our features and the target column\n\nX_train = train.drop(columns = ['Survived'],axis=1)\ny_train = train.Survived","37c9f8f7":"def sigmoid(input):    \n    output = 1 \/ (1 + np.exp(-input))\n    return output","81e14ec3":"def costFunction(theta, X, y):\n    m = y.size  \n    J=0\n    h = sigmoid(np.dot(X, theta.T))\n    J = 1\/m * np.sum(-y*np.log(h) - (1-y) * np.log(1-h))\n    return J","52d4dc86":"def gradientDecsent(iterations, alpha, x, y):\n    m = x.shape[0]\n    x = np.concatenate([np.ones((m, 1)), x], axis=1)\n    y = y.to_numpy()\n    y = np.reshape(y, (len(y),1))  \n    theta = np.full((1, x.shape[1]), 0)\n\n    cost_history = []\n    for i in range(iterations):\n        theta = theta - (alpha \/ m) * np.transpose(sigmoid(np.dot(x, theta.T)) - y).dot(x)\n        cost_history.append(costFunction(theta, x, y))\n    return theta, cost_history","e6a17477":"#theta, cost_history = gradientDecsent(200000, 0.0042, x, y)\ntheta, cost_history = gradientDecsent(200000, 0.002, X_train, y_train)\nprint(theta)","ff8be1b8":"plt.plot(np.arange(len(cost_history)), cost_history, lw=2)\nplt.xlabel('Number of iterations')\nplt.ylabel('Cost J');","44d613c6":"#preparing the test data for modeling\nx_ones = np.concatenate([np.ones((test.shape[0], 1)),test], axis=1)\ny_pred = sigmoid(np.dot(x_ones, theta.T))","818a0a20":"y_pred_final = []\nfor i in y_pred:\n    if i >= 0.5:\n        y_pred_final.append(1)\n    else:\n        y_pred_final.append(0)\nprint(y_pred_final)","c2e1aa1c":"#submission\nmy_submission = pd.DataFrame({'PassengerId': ids, 'Survived': y_pred_final})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","1e570b1f":"## Gradient descent optimizer\nminimize the cost function by changing the weights (theta) after each iteration.\n\n* $\\hat{y}$ : The predicted value\n* $Y$ : True Value of the Target\n* $X$ : Independent columns\n* $m$ : Number of training examples\n* $\\alpha$ : step of the point \n\n### $$ \\theta = \\theta - \\frac{\\alpha}{m} \\sum \\limits _{i=1}^{m}(\\hat{y}-Y^{i}).X^i $$\n\noptimization function minimize the cost function by changing the weights (theta) after each iteration(epoch) and gradient descent is the easiest one of them , dont worry it may at first for some people seem overwealming but you will gain intiuation after you see this gif and it will be clear for you.\n\n![0cef7a_216e740d402b4a1f9c8f4ccaac893622_mv2](https:\/\/user-images.githubusercontent.com\/59618586\/114191120-5a828700-994c-11eb-90cb-f198e2b6171f.jpg) \n![1_PQ8tdohapfm-YHlrRIRuOA](https:\/\/user-images.githubusercontent.com\/59618586\/116012986-f5a67c80-a62d-11eb-8666-bc90e8e1bf02.gif)\n","27e32c77":"## Importing the tools ","2941cb43":"zero values consider not survived and ones values consider survived","5763bd05":"#### Please don't forget to upvote if you found this notebook helpful and follow me to see more content like this.","a7227acd":"## Cost function of Logistic Regression:\n\n* $\\hat{y}$ : The predicted value\n* $Y$ : True Value of the Target\n* $X$ : Independent columns\n* $m$ : Number of training examples\n\n### $$Cost = \\frac {-\\sum(Y\\log(\\hat{y}) + (1-Y)\\log(1-\\hat{y}))}{m}$$","daba76dd":"## Simple EDA","371265a8":"we see a decrease in the cost by each iteration getting to the global minimum ","03a25bad":"## Sigmoid function:\nsegmoid function is an equation to give probability between 0 and 1 to classify between two classes\n\n<\/br>\n\nin segmoid we combines the linear equation to the exponential power but in negative to give small numbers range between 0 and 1\n\n<\/br>\n\n* $\\hat{y}$ : The predicted value\n* $X$ : Independent columns\n* $m$ : Number of training examples\n\n### $$\\hat{y}=\\frac{1}{1+e^{-(\\sum \\limits _{i=1} ^ {m} \\theta_0 + \\theta_1X_1 + ... + \\theta_{i} X_i)}}$$"}}