{"cell_type":{"8df382ac":"code","39acaaf0":"code","ad73659a":"code","fe53e17b":"code","cffbd006":"code","dce6eb4a":"code","fa8f519f":"code","9b7a0225":"code","ad1995c1":"code","178668f4":"code","0d09aadd":"code","c830b9b7":"code","4f159ebc":"code","c423891c":"code","ac552718":"code","9b242bb1":"code","087ce9a6":"code","65cce30b":"code","73adcbae":"code","02762834":"code","962bb85f":"code","1e44bf2e":"code","4eb786a4":"code","e2a1b0c6":"code","a2b7c6eb":"code","8cc34a6c":"code","8fe8b09e":"code","0d2003de":"code","96ad5be9":"markdown","5a58984d":"markdown","31c9fc46":"markdown","a14d81d4":"markdown","d8e36d82":"markdown","b3494a27":"markdown","dcbe4fb9":"markdown","877d2694":"markdown","552f072c":"markdown","6dc8792b":"markdown","4a766a98":"markdown","f635f1c0":"markdown","ab5c0053":"markdown","108c9b11":"markdown","e19d8e42":"markdown","d80fb8db":"markdown","8ca43b62":"markdown","aa76fbdd":"markdown","9b68f79c":"markdown","7b0714e1":"markdown","51f36bb6":"markdown","d04f9688":"markdown","303920fc":"markdown","7cd196e0":"markdown","b45b4177":"markdown","1cfb637b":"markdown","27dba2a3":"markdown","83f68b9d":"markdown","097fd6d0":"markdown","675225ac":"markdown","b15cdcd3":"markdown","7bcea719":"markdown","5a68fcd9":"markdown","2a4e368f":"markdown","9343e5f9":"markdown","8497233d":"markdown","0fb3be69":"markdown"},"source":{"8df382ac":"# First, look at everything.\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\/\"]).decode(\"utf8\"))","39acaaf0":"import gzip\nimport tensorflow as tf\nimport numpy as np\nfrom collections import namedtuple\nimport matplotlib.pyplot as plt","ad73659a":"def read32(bytestream):\n    return np.frombuffer(bytestream.read(4), \n                         dtype=np.dtype(np.uint32).newbyteorder('>'))[0]","fe53e17b":"def one_hot(labels_dense, num_classes):\n    \"\"\"Convert class labels from scalars to one-hot vectors.\n    Args:\n        labels_dense: A numpy array containing image labels\n        num_classes: Number of classes in the image\n    Returns:\n        labels_one_hot: A numpy array containing one-hot encoded data.\n    \"\"\"\n    \n    num_labels = labels_dense.shape[0]\n    index_offset = np.arange(num_labels) * num_classes\n    labels_one_hot = np.zeros((num_labels, num_classes))\n    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n    \n    return labels_one_hot","cffbd006":"def extract_images(f):\n    \"\"\"Extract the images into a 4D uint8 numpy array [index, y, x, depth].\n    Args:\n        f: A file object that can be passed into a gzip reader.\n    Returns:\n        data: A 4D uint8 numpy array [index, y, x, depth].\n    Raises:\n        ValueError: If the bytestream does not start with 2051.\n    \"\"\"\n    \n    print('Extracting', f)\n    \n    with open(file=f, mode='rb') as bytestream:\n        magic = read32(bytestream)\n        \n        if magic != 2051:\n            raise ValueError('Invalid magic number %d in MNIST image file: %s' % (magic, f.name))\n        \n        num_images = read32(bytestream)\n        rows = read32(bytestream)\n        cols = read32(bytestream)\n        buf = bytestream.read(rows * cols * num_images)\n        data = np.frombuffer(buf, dtype=np.uint8)\n        data = data.reshape(num_images, rows, cols, 1)\n        \n        return data","dce6eb4a":"def extract_labels(f, one_hot_encode=False, num_classes=10):\n    \"\"\"Extract the labels into a 1D uint8 numpy array [index].\n    Args:\n        f: A file object that can be passed into a gzip reader.\n        one_hot_encode: Does one hot encoding for the result. Default False.\n        num_classes: Number of classes for the one hot encoding.\n    Returns:\n        labels: a 1D uint8 numpy array.\n    Raises:\n        ValueError: If the bystream doesn't start with 2049.\n    \"\"\"\n    \n    print('Extracting', f)\n    with open(file=f, mode='rb') as bytestream:\n        magic = read32(bytestream)\n        \n        if magic != 2049:\n            raise ValueError('Invalid magic number %d in MNIST label file: %s' % (magic, f.name))\n            \n        num_items = read32(bytestream)\n        buf = bytestream.read(num_items)\n        labels = np.frombuffer(buf, dtype=np.uint8)\n        \n        if one_hot_encode:\n            return one_hot(labels, num_classes)\n        \n        return labels","fa8f519f":"class Dataset(object):\n\n    def __init__(self, images, labels, one_hot=False, dtype=tf.float32, reshape=True, seed=None):\n        \"\"\"Construct a dataset given a image and its labels.   \n\n        Args:\n            images: Numpy image data.\n            labels: Image labels\n            one_hot: Ind\u0131cates one hot encoding for the data. Default is false\n            dtype: Data type of our image.It can be either `uint8` to leave \n                   the input as `[0, 255]`, or `float32` to rescale into `[0, 1]`.\n            reshape: Convert shape from [num examples, rows, columns, depth]\n                     to [num examples, rows*columns] (assuming depth == 1)\n            seed: Provides for convenient deterministic testing\n\n        Returns:\n\n        \"\"\"\n\n        seed1, seed2 = tf.random.get_seed(seed)\n\n        # If op level seed is not set, use whatever graph level seed is returned\n        np.random.seed(seed1 if seed is None else seed2)\n\n        if dtype not in (tf.uint8, tf.float32):\n            raise TypeError('Invalid image dtype %r, expected uint8 or float32' % dtype)\n\n        assert images.shape[0] == labels.shape[0], (\n          'images.shape: %s labels.shape: %s' % (images.shape, labels.shape))\n        \n        self.num_examples = images.shape[0]\n        \n        # Convert shape from [num examples, rows, columns, depth]\n        # to [num examples, rows*columns] (assuming depth == 1)\n        if reshape:\n            assert images.shape[3] == 1\n            images = images.reshape(images.shape[0],\n                                    images.shape[1] * images.shape[2])\n        if dtype == tf.float32:\n            # Convert from [0, 255] -> [0.0, 1.0].\n            images = images.astype(np.float32)\n            images = np.multiply(images, 1.0 \/ 255.0)\n            \n        self.features = images\n        self.targets = labels\n        self.epochs_completed = 0\n        self.index_in_epoch = 0\n        \n    def next_batch(self, batch_size, shuffle=True):\n        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n    \n        start = self.index_in_epoch\n        \n        # Shuffle for the first epoch\n        if self.epochs_completed == 0 and start == 0 and shuffle:\n            perm0 = np.arange(self.num_examples)\n            np.random.shuffle(perm0)\n            self.features = self.features[perm0]\n            self.targets = self.targets[perm0]\n\n        # Go to the next epoch\n        if start + batch_size > self.num_examples:\n            # Finished epoch\n            self.epochs_completed += 1\n            # Get the rest examples in this epoch\n            rest_num_examples = self.num_examples - start\n            images_rest_part = self.features[start:self.num_examples]\n            labels_rest_part = self.targets[start:self.num_examples]\n            \n            # Shuffle the data\n            if shuffle:\n                perm = np.arange(self.num_examples)\n                np.random.shuffle(perm)\n                self.features = self.features[perm]\n                self.targets = self.targets[perm]\n                \n            # Start next epoch\n            start = 0\n            self.index_in_epoch = batch_size - rest_num_examples\n            end = self.index_in_epoch\n            images_new_part = self.features[start:end]\n            labels_new_part = self.targets[start:end]\n            \n            return np.concatenate((images_rest_part, images_new_part), \n                                  axis=0), np.concatenate((labels_rest_part, \n                                                           labels_new_part), axis=0)\n        \n        else:\n            self.index_in_epoch += batch_size\n            end = self.index_in_epoch\n            \n            return self.features[start:end], self.targets[start:end]","9b7a0225":"def read_raw_mnist(img_names, one_hot=False, dtype=tf.float32, val_size=5000, \n                   reshape=True, seed=None):\n    \"\"\" Reads the original mnist data as in zipped format.\n    \n    Args:\n        img_names: A dictionary containing names of the training and test images.\n                   Dictionary keys should be 'train_images', 'train_labels', \n                   'test_images', 'test_labels'\n        one_hot: Indicates whether labels should be converted to one-hot encoding.Default False\n        dtype: Data type of images. Default to tf.float32\n        reshape: Convert shape from [num examples, rows, columns, depth]\n                 to [num examples, rows*columns] (assuming depth == 1). Default True\n        val_size: Validation data size apart from test data\n        \n    Returns:\n        Datasets as a namedtuple contains train, validation, test data.\n    \"\"\"\n    TRAIN_IMAGES = img_names['train_images']\n    TRAIN_LABELS = img_names['train_labels']\n    TEST_IMAGES = img_names['test_images']\n    TEST_LABELS = img_names['test_labels']\n\n    \n    train_images = extract_images(TRAIN_IMAGES)\n    train_labels = extract_labels(TRAIN_LABELS, one_hot_encode=one_hot)\n    test_images = extract_images(TEST_IMAGES)\n    test_labels = extract_labels(TEST_LABELS, one_hot_encode=one_hot)\n\n\n    val_images = train_images[:val_size]\n    val_labels = train_labels[:val_size]\n    train_images = train_images[val_size:]\n    train_labels = train_labels[val_size:]\n\n    kwargs = dict(dtype=dtype, reshape=reshape, seed=seed)\n    \n    train = Dataset(train_images, train_labels, **kwargs)\n    val = Dataset(val_images, val_labels, **kwargs)\n    test = Dataset(test_images, test_labels, **kwargs)\n    \n    Datasets = namedtuple('Datasets', ['train', 'val', 'test'])\n    \n    return Datasets(train=train, val=val, test=test)","ad1995c1":"TRAIN_IMAGES = '..\/input\/train-images-idx3-ubyte'\nTRAIN_LABELS = '..\/input\/train-labels-idx1-ubyte'\nTEST_IMAGES = '..\/input\/t10k-images-idx3-ubyte'\nTEST_LABELS = '..\/input\/t10k-labels-idx1-ubyte'\n\nimg_data = {'train_images': TRAIN_IMAGES, 'train_labels': TRAIN_LABELS, \n            'test_images':TEST_IMAGES, 'test_labels': TEST_LABELS }\n\nmnist = read_raw_mnist(img_data, one_hot=True)","178668f4":"width = 28 # width of the image in pixels \nheight = 28 # height of the image in pixels\nchannels = 1  # gray scale image\nkernel_size = 5  # kernel size for convolutions\nnum_classes = 10\nnum_pixels = 28*28\nkernel_size = 5\nbatch_size = 300","0d09aadd":"with tf.device('\/device:GPU:0'):\n    with tf.name_scope('Inputs'):\n        x = tf.placeholder(tf.float32, shape=[None, num_pixels], name='features')\n        y = tf.placeholder(tf.float32, shape=[None, num_classes], name='labels')\n\n    with tf.name_scope('Reshape'):    \n        X = tf.reshape(x, [-1, width, height, channels], name='features')  ","c830b9b7":"with tf.device('\/device:GPU:0'):\n    with tf.name_scope('Conv1'):\n        with tf.name_scope('Weights'):\n            w_conv1 = tf.Variable(tf.truncated_normal([kernel_size, kernel_size, channels,\n                                                       32], stddev=0.1), name='weights')\n            b_conv1 = tf.Variable(tf.constant(0.1, shape=[32]), name='biases') \n\n        with tf.name_scope('Convolution'):\n            conv1 = tf.nn.conv2d(X, w_conv1, strides=[1, 1, 1, 1], padding='SAME', \n                                 name='conv2D') + b_conv1\n\n        with tf.name_scope('ReLU'):\n            relu_conv1 = tf.nn.relu(conv1, name='relu')","4f159ebc":"with tf.device('\/device:GPU:0'):\n    with tf.name_scope('DropOut'):\n        drop1 = tf.nn.dropout(relu_conv1, 0.5, name='drop')\n        \n    with tf.name_scope('Max_Pool_1'):\n        max_conv1 = tf.nn.max_pool(drop1, ksize=[1, 2, 2, 1] , \n                                   strides=[1, 2, 2, 1], padding='SAME', name='maxpool') ","c423891c":"with tf.device('\/device:GPU:0'):\n    with tf.name_scope('Conv2'):\n        with tf.name_scope('Weights'):\n            w_conv2 = tf.Variable(tf.truncated_normal([kernel_size, kernel_size, 32, 64], \n                                                      stddev=0.1), name='weights')\n            b_conv2 = tf.Variable(tf.constant(0.1, shape=[64]), name='biases') \n\n        with tf.name_scope('Convolution'):\n            conv2 = tf.nn.conv2d(max_conv1, w_conv2, strides=[1, 1, 1, 1], padding='SAME', \n                                 name='conv2D') + b_conv2\n\n        with tf.name_scope('ReLU'):\n            relu_conv2 = tf.nn.relu(conv2, name='relu')","ac552718":" with tf.device('\/device:GPU:0'):\n    with tf.name_scope('Max_Pool_2'):\n        max_conv2 = tf.nn.max_pool(relu_conv2, ksize=[1, 2, 2, 1], \n                                   strides=[1, 2, 2, 1], padding='SAME', name='maxpool') ","9b242bb1":"with tf.device('\/device:GPU:0'):\n    with tf.name_scope('Flatten'):\n        layer2_mat = tf.reshape(max_conv2, shape=[-1, 7 * 7 * 64], name='flatten')","087ce9a6":"with tf.device('\/device:GPU:0'):\n    with tf.name_scope('Dense_1'):\n        with tf.name_scope('Weights'):\n            w_fc1 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 1024], stddev=0.1), \n                                name='weights')\n            b_fc1 = tf.Variable(tf.constant(0.1, shape=[1024]), name='biases') \n\n        with tf.name_scope('MatMul'):\n            fcl = tf.matmul(layer2_mat, w_fc1, name='matmul') + b_fc1\n\n        with tf.name_scope('ReLU'):\n            relu_fc1 = tf.nn.relu(fcl, name='relu')","65cce30b":"with tf.device('\/device:GPU:0'):\n    with tf.name_scope('DropOut'):\n        keep = tf.placeholder(tf.float32, name='keep')\n        drop = tf.nn.dropout(relu_fc1, keep, name='drop')","73adcbae":"with tf.device('\/device:GPU:0'):\n    with tf.name_scope('Dense_2'):\n        with tf.name_scope('Weights'):\n            w_fc2 = tf.Variable(tf.truncated_normal([1024, num_classes], stddev=0.1), \n                                name='weights') \n            b_fc2 = tf.Variable(tf.constant(0.1, shape=[num_classes]), name='biases') \n\n        with tf.name_scope('MatMul'):\n            fc = tf.matmul(drop, w_fc2, name='matmul') + b_fc2\n\n        with tf.name_scope('Softmax'):\n            Y_CNN= tf.nn.softmax(fc, name='softmax')","02762834":"learning_rate = 1e-4\nwith tf.device('\/device:GPU:0'):\n    with tf.name_scope('Loss'):\n        loss = tf.losses.softmax_cross_entropy(y, Y_CNN)\n\n    with tf.name_scope('Train'):\n        train = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n        # train = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n\n    with tf.name_scope('Accuracy'):\n        is_true = tf.equal(tf.argmax(Y_CNN, 1), tf.argmax(y, 1), name='output')\n        acc = tf.reduce_mean(tf.cast(is_true, tf.float32), name='accuracy')","962bb85f":"epochs = 30000\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n\nsess.run(tf.global_variables_initializer())\n\n# Graph data\n# Uncomment this if you wanna check and analyze the graph on your own\n# writer = tf.summary.FileWriter('logs')\n# writer.add_graph(sess.graph)\n\nfor i in range(epochs):\n    batch = mnist.train.next_batch(batch_size)\n    if i%1000 == 0:\n        train_acc = sess.run(acc, feed_dict={x:batch[0], y: batch[1], keep: 1.0})\n        print(\"Step %d, training accuracy %g\" %(i, float(train_acc)))\n    sess.run(train, feed_dict={x: batch[0], y: batch[1], keep: 0.5})","1e44bf2e":"# evaluate in batches to avoid out-of-memory issues\nn_batches = mnist.test.features.shape[0] \/\/ 50  # Floor division\ncumulative_accuracy = 0.0\n# Evaluate the Model\nfor index in range(n_batches):\n    batch = mnist.test.next_batch(50)\n    cumulative_accuracy += sess.run(acc, feed_dict={x: batch[0], y: batch[1], keep: 1.0})\nprint(\"test accuracy {}\".format(cumulative_accuracy \/ n_batches))","4eb786a4":"def get_activations(layer, stimuli):\n    units = sess.run(layer, feed_dict={x:np.reshape(stimuli, [1,784], order='F'), keep:1.0})\n    plot_nn_filter(units)","e2a1b0c6":"def plot_nn_filter(units):\n    import math\n    filters = units.shape[3]\n    plt.figure(1, figsize=(20,20))\n    n_columns = 6\n    n_rows = math.ceil(filters \/ n_columns) + 1\n    for i in range(filters):\n        plt.subplot(n_rows, n_columns, i+1)\n        plt.title('Filter ' + str(i))\n        plt.imshow(units[0,:,:,i], interpolation=\"nearest\", cmap=\"gray\")\n    plt.tight_layout()","a2b7c6eb":"image = mnist.test.features[0]\nplt.imshow(np.reshape(image, [height,width]), interpolation=\"nearest\", cmap=\"gray\")","8cc34a6c":"get_activations(conv1, image)","8fe8b09e":"get_activations(conv2, image)","0d2003de":"sess.close()","96ad5be9":"The <span style=\"background-color:#dcdcdc\"> one-hot = True<\/span> argument only means that, in contrast to Binary representation, the labels will be presented in a way that to represent a number N, the $N^{th}$ bit is 1 while the the other bits are 0. For example, five and zero in a binary code would be:\n\n<pre>\nNumber representation:    0\nBinary encoding:        [2^5]  [2^4]   [2^3]   [2^2]   [2^1]   [2^0]  \nArray\/vector:             0      0       0       0       0       0 \n\nNumber representation:    5\nBinary encoding:        [2^5]  [2^4]   [2^3]   [2^2]   [2^1]   [2^0]  \nArray\/vector:             0      0       0       1       0       1  \n<\/pre>\n\nUsing a different notation, the same digits using one-hot vector representation can be show as: \n\n<pre>\nNumber representation:    0\nOne-hot encoding:        [5]   [4]    [3]    [2]    [1]   [0]  \nArray\/vector:             0     0      0      0      0     1   \n\nNumber representation:    5\nOne-hot encoding:        [5]   [4]    [3]    [2]    [1]    [0]  \nArray\/vector:             1     0      0      0      0      0   \n<\/pre>","5a58984d":"### Data Processing\n\nIn this part, we are going to see how we can read the fmnist data from the original data source. Almost %80 of the AI work is usually processing the data as can be seen from this case also. We will read our data using numpy, convert labels to one-hot encoding, scale our data between 0-1, shuffle the data and retrieve batches repeatedly.  ","31c9fc46":"<h4>Defining kernel weight and bias<\/h4>\n\nWe define a kernel here. The size of the filter\/kernel is 5x5;  Input channels is 1 (grayscale);  and we need 32 different feature maps (here, 32 feature maps means 32 different filters are applied on each image. So, the output of convolution layer would be 28x28x32). In this step, we create a filter \/ kernel tensor of shape <code>[filter_height, filter_width, in_channels, out_channels]<\/code>","a14d81d4":"**A CONTAINER FOR OUR DATA INCLUDING BATCH FUNCTION**","d8e36d82":"The imported data can be divided as follow:\n\n- Training (mnist.train) >>  Use the given dataset with inputs and related outputs for training of NN. In our case, if you give an image that you know that represents a \"nine\", this set will tell the neural network that we expect a \"nine\" as the output.  \n        - 55,000 data points\n        - mnist.train.features for inputs\n        - mnist.train.targets for outputs\n  \n   \n- Validation (mnist.val) >> The same as training, but now the data is used to generate model properties (classification error, for example) and from this, tune parameters like the optimal number of hidden units or determine a stopping point for the back-propagation algorithm  \n        - 5,000 data points\n        - mnist.val.features for inputs\n        - mnist.val.targets for outputs\n  \n  \n- Test (mnist.test) >> the model does not have access to this informations prior to the testing phase. It is used to evaluate the performance and accuracy of the model against \"real life situations\". No further optimization beyond this point.  \n        - 10,000 data points\n        - mnist.test.features for inputs\n        - mnist.test.targets for outputs\n  ","b3494a27":"history = model.fit(mnist.train.features.reshape((55000, width, height, channels)), \n                    mnist.train.targets,batch_size=256, epochs=10, verbose=1,\n                    validation_data=(mnist.val.features.reshape((5000, width, height, channels)), \n                           mnist.val.targets))","dcbe4fb9":"<h4>Weights and Biases of kernels<\/h4>\n\nWe apply the convolution again in this layer. Lets look at the second layer kernel:  \n- Filter\/kernel: 5x5 (25 pixels) \n- Input channels: 32 (from the 1st Conv layer, we had 32 feature maps) \n- 64 output feature maps  \n\n<b>Notice:<\/b> here, the input image is [14x14x32], the filter is [5x5x32], we use 64 filters of size [5x5x32], and the output of the convolutional layer would be 64 convolved image, [14x14x64].\n\n<b>Notice:<\/b> the convolution result of applying a filter of size [5x5x32] on image of size [14x14x32] is an image of size [14x14x1], that is, the convolution is functioning on volume.","877d2694":"**ONE-HOT ENCODING**","552f072c":"## Visualization\n\nLet's take a look at all the filters(weights and biases) to see what our model does.","6dc8792b":"score = model.evaluate(mnist.test.features.reshape((5000, width, height, channels), \n                                                   mnist.test.targets, verbose=0)","4a766a98":"**EXTRACT IMAGE DATA**","f635f1c0":"## Introduction\n\nIn this section, we will use the famous [Fashion MNIST Dataset](https:\/\/github.com\/zalandoresearch\/fashion-mnist\/) to build two Neural Networks capable to perform classification. ","ab5c0053":"## Deep Learning on FMNIST\n\nArchitecture of our network is:\n    \n- (Input) -> [batch_size, 28, 28, 1]  >> Apply 32 filter of [5x5]\n- (Convolutional layer 1)  -> [batch_size, 28, 28, 32]\n- (ReLU 1)  -> [?, 28, 28, 32]\n- [Drop out 1] -> [?, 28, 28, 1]\n- (Max pooling 1) -> [?, 14, 14, 32]\n- (Convolutional layer 2)  -> [?, 14, 14, 64] \n- (ReLU 2)  -> [?, 14, 14, 64] \n- (Max pooling 2)  -> [?, 7, 7, 64] \n- [fully connected layer 3] -> [1x1024]\n- [ReLU 3]  -> [1x1024]\n- [Drop out 2]  -> [1x1024]\n- [fully connected layer 4] -> [1x10]\n\n<img src=\"https:\/\/www.katacoda.com\/basiafusinska\/courses\/tensorflow-getting-started\/tensorflow-mnist-expert\/assets\/convolution.png\" alt=\"HTML5 Icon\" style=\"width:800px;height:350px;\"> \n<div style=\"text-align:center\">Our Model without DropOut Layers<\/div>\n\n\n\nThe next cells will explore this new architecture. In this part, we are going to build our graph and visualize the whole architecture of the system so that we understand it very well. ","108c9b11":"<h4>Readout Layer (Softmax Layer)<\/h4>\n\nType: Softmax, Fully Connected Layer.","e19d8e42":"Third layer completed","d80fb8db":"<h3>Convolutional Layer 1<\/h3>","8ca43b62":"## Conclusion\n\nI have changed batch size and optimizers to find the maximum accuracy. If you train more, perhaps you can reach to 94-95 percent. Also check out my original MNIST [kernel](https:\/\/www.kaggle.com\/fazilbtopal\/a-detailed-cnn-with-tensorflow\/). Feel free to comment about your thoughts on my code!","aa76fbdd":"Second layer completed. So, what is the output of the second layer, layer2?\n- it is 64 matrix of [7x7]","9b68f79c":"<a id=\"ref1\"><\/a>\n<h2>What is Deep Learning?<\/h2>\n\n<b>Brief Theory:<\/b> Deep learning (also known as deep structured learning, hierarchical learning or deep machine learning) is a branch of machine learning based on a set of algorithms that attempt to model high-level abstractions in data by using multiple processing layers, with complex structures or otherwise, composed of multiple non-linear transformations.\n\n<img src=\"https:\/\/ibm.box.com\/shared\/static\/gcbbrh440604cj2nksu3f44be87b8ank.png\" alt=\"HTML5 Icon\" style=\"width: 600px; height: 450px;\">\n<div style=\"text-align: center\">It's time for deep learning. Our brain doesn't work with only one or three layers. Why it would be different with machines?. <\/div>","7b0714e1":"## A Look to Our Graph\n\nNote: This was captured before adding the second DropOut layer right after our first Convolution. You can uncomment the section in session where we write our graph data so that we can see it.\n\n<a href=\"https:\/\/ibb.co\/py0w6gP\"><img src=\"https:\/\/i.ibb.co\/M62Sybh\/graph.png\" alt=\"graph\" border=\"0\"><\/a><br \/>\n","51f36bb6":"<img src=\"https:\/\/ibm.box.com\/shared\/static\/iizf4ui4b2hh9wn86pplqxu27ykpqci9.png\" style=\"width: 800px; height: 400px;\" alt=\"HTML5 Icon\" >\n\n\n<h4>Apply the ReLU activation Function<\/h4>\n\nIn this step, we just go through all outputs convolution layer, <b>conv1<\/b>, and wherever a negative number occurs, we swap it out for a 0. It is called ReLU activation Function.<br> Let f(x) is a ReLU activation function $f(x) = max(0,x)$.","d04f9688":"Our First layer is now completed. Let's start building the second second layer.","303920fc":"**READ FMNIST DATA**","7cd196e0":"# CONVOLUTIONAL NEURAL NETWORK APPLICATION","b45b4177":"<b>In Practice, defining the term \"Deep\":<\/b> in this context, deep means that we are studying a Neural Network which has several hidden layers (more than one), no matter what type (convolutional, pooling, normalization, fully-connected etc). The most interesting part is that some papers noticed that Deep Neural Networks with the right architectures\/hyper-parameters achieve better results than shallow Neural Networks with the same computational power (e.g. number of neurons or connections). \n\n<b>In Practice, defining \"Learning\":<\/b> In the context of supervised learning, the learning part consists of a target\/feature which is to be predicted using a given set of observations with the already known final prediction (label). After some training, it is possible to generate a \"function\" that map inputs to desired outputs. The only problem is how well this map operation occurs. While trying to generate this \"function\", the training process continues until the model achieves a desired level of accuracy on the training data.","1cfb637b":"## Keras Version\nI have added the keras code here for those who want to try and compare it.\n\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(5, 5), activation='relu',\n                                 padding='SAME', input_shape=(28, 28, 1)))\nmodel.add(tf.keras.layers.Dropout(rate=0.5))\nmodel.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(5, 5), activation='relu',\n                                 padding='SAME'))\nmodel.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(units=1024, activation='relu'))\nmodel.add(tf.keras.layers.Dropout(rate=0.5))\nmodel.add(tf.keras.layers.Dense(units=num_classes, activation='softmax'))\n\nmodel.compile(optimizer='rmsprop', loss=tf.losses.softmax_cross_entropy, metrics=['accuracy'])\nmodel.summary()","27dba2a3":"It is a best practice to create placeholders before variable assignments when using TensorFlow. We'll create placeholders for inputs (\"Xs\") and outputs (\"Ys\").   \n\n<b>Placeholder 'X':<\/b> represents the \"space\" allocated input or the images. \n<ul>\n    <li>Each input has 784 pixels distributed by a 28 width x 28 height matrix<\/li>   \n    <li>The 'shape' argument defines the tensor size by its dimensions.<\/li>     \n    <li>1st dimension = None. Indicates that the batch size, can be of any size.<\/li>     \n    <li>2nd dimension = 784. Indicates the number of pixels on a single flattened MNIST image.<\/li>    \n<\/ul>\n    \n<b>Placeholder 'Y':<\/b> represents the final output or the labels.\n<ul>\n    <li>10 possible classes (0, 1, 2, 3, 4, 5, 6, 7, 8, 9)<\/li>  \n    <li>The 'shape' argument defines the tensor size by its dimensions.<\/li>    \n    <li>1st dimension = None. Indicates that the batch size, can be of any size.<\/li>     \n    <li>2nd dimension = 10. Indicates the number of targets\/outcomes<\/li>   \n<\/ul>\n<b>dtype for both placeholders:<\/b> if you not sure, use tf.float32. The limitation here is that the later presented softmax function only accepts float32 or float64 dtypes.","83f68b9d":"<h4>Dropout Layer, Optional phase for reducing overfitting<\/h4>\n\nIt is a phase where the network \"forget\" some features. At each training step in a mini-batch, some units get switched off randomly so that it will not interact with the network. That is, it weights cannot be updated, nor affect the learning of the other network nodes.  This can be very useful for very large neural networks to prevent overfitting.","097fd6d0":"! tensorboard --logdir='logs\/'","675225ac":"**EXTRACT LABELS**","b15cdcd3":"<h4>Converting images of the data set to tensors<\/h4>\n\nThe input image is 28 pixels by 28 pixels, 1 channel (grayscale). In this case, the first dimension is the <b>batch number<\/b> of the image, and can be of any size (so we set it to -1). The second and third dimensions are width and height, and the last one is the image channels.","7bcea719":"<h3>Fully Connected Layer<\/h3>\n\nYou need a fully connected layer to use the Softmax and create the probabilities in the end. Fully connected layers take the high-level filtered images from previous layer, that is all 64 matrices, and convert them to a flat array.\n\nSo, each matrix [7x7] will be converted to a matrix of [49x1], and then all of the 64 matrix will be connected, which make an array of size [3136x1]. We will connect it into another layer of size [1024x1]. So, the weight between these 2 layers will be [3136x1024]\n\n\n<img src=\"https:\/\/ibm.box.com\/shared\/static\/pr9mnirmlrzm2bitf1d4jj389hyvv7ey.png\" alt=\"HTML5 Icon\" style=\"width: 800px; height: 400px;\"> \n","5a68fcd9":"## Train the Neural Network\n\n<h4>Define the loss function<\/h4>\n\nWe need to compare our output, layer4 tensor, with ground truth for all mini_batch. we can use <b>cross entropy<\/b> to see how bad our CNN is working - to measure the error at a softmax layer.\n\nThe following code shows an toy sample of cross-entropy for a mini-batch of size 2 which its items have been classified.","2a4e368f":"<h3>Convolutional Layer 2<\/h3>","9343e5f9":"<a id=\"ref7\"><\/a>\n<h2>Summary of the Deep Convolutional Neural Network<\/h2>\n\nNow is time to remember the structure of  our network\n\n#### 0) Input - MNIST dataset\n#### 1) Convolutional \n#### 2) Processing - Dropout\n#### 3) Max-Pooling\n#### 4) Convolutional and Max-Pooling\n#### 5) Fully Connected Layer\n#### 6) Processing - Dropout\n#### 7) Readout layer - Fully Connected\n#### 8) Outputs - Classification","8497233d":"<h4>Apply the max pooling<\/h4>\n\n<b>max pooling<\/b> is a form of non-linear down-sampling. It partitions the input image into a set of rectangles and, and then find the maximum value for that region. \n\nLets use <b>tf.nn.max_pool<\/b> function to perform max pooling. \n<b>Kernel size:<\/b> 2x2 (if the window is a 2x2 matrix, it would result in one output pixel)  \n<b>Strides:<\/b> dictates the sliding behaviour of the kernel. In this case it will move 2 pixels everytime, thus not overlapping. The input is a matrix of size 28x28x32, and the output would be a matrix of size 14x14x32.\n\n<img src=\"https:\/\/ibm.box.com\/shared\/static\/kmaja90mn3aud9mro9cn8pbbg1h5pejy.png\" alt=\"HTML5 Icon\" style=\"width: 900px; height: 500px;\"> \n\n","0fb3be69":"<img src=\"https:\/\/ibm.box.com\/shared\/static\/vn26neef1nnv2oxn5cb3uueowcawhkgb.png\" style=\"width: 800px; height: 400px;\" alt=\"HTML5 Icon\" >\n\n<\/h4>Convolve with weight tensor and add biases.<\/h4>\n\nTo create convolutional layer, we use <b>tf.nn.conv2d<\/b>. It computes a 2-D convolution given 4-D input and filter tensors.\n\nInputs:\n- tensor of shape [batch, in_height, in_width, in_channels]. x of shape [batch_size, 28 ,28, 1]\n- a filter \/ kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]. W is of size [5, 5, 1, 32]\n- stride which is  [1, 1, 1, 1]. The convolutional layer, slides the \"kernel window\" across the input tensor. As the input tensor has 4 dimensions:  [batch, height, width, channels], then the convolution operates on a 2D window on the height and width dimensions. __strides__ determines how much the window shifts by in each of the dimensions. As the first and last dimensions are related to batch and channels, we set the stride to 1. But for second and third dimension, we could set other values, e.g. [1, 2, 2, 1]\n    \n    \nProcess:\n- Change the filter to a 2-D matrix with shape [5\\*5\\*1,32]\n- Extracts image patches from the input tensor to form a *virtual* tensor of shape `[batch, 28, 28, 5*5*1]`.\n- For each batch, right-multiplies the filter matrix and the image vector.\n\nOutput:\n- A `Tensor` (a 2-D convolution) of size tf.Tensor 'add_7:0' shape=(?, 28, 28, 32)- Notice: the output of the first convolution layer is 32 [28x28] images. Here 32 is considered as volume\/depth of the output image."}}