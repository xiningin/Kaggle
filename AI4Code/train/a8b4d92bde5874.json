{"cell_type":{"560c390e":"code","95166413":"code","4323d913":"code","c7debee4":"code","44fcaf51":"code","c5bc4c48":"code","876ededb":"code","2f6962ea":"code","f81753ce":"code","e3294e33":"code","1267de60":"code","523e225d":"code","c19f766e":"code","34471c57":"code","b881d35d":"code","446cf945":"code","0c18da8e":"code","bcc76bcc":"code","c3f5b235":"code","dcff3287":"code","ea5ccffa":"code","85c516ff":"code","90cda808":"code","204f9da8":"markdown","a4717e94":"markdown","4a8d0d65":"markdown","bc3cf554":"markdown","d8d5b873":"markdown","74e887e1":"markdown","b4b0e4a7":"markdown","7ae4fdd3":"markdown","1d0adc28":"markdown"},"source":{"560c390e":"# https:\/\/www.tensorflow.org\/official_models\/fine_tuning_bert#setup","95166413":"# import sys\n# import subprocess\n# subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'bert-for-tf2'])\n\n# from pip._internal import main as pipmain\n# pipmain(['install', 'bert-for-tf2'])\n\nimport pip\npip.main(['install', 'bert-for-tf2'])","4323d913":"import os\nimport numpy as np\nimport pandas as pd\nimport datetime\nimport sys\n# import zipfile\n# import modeling\n# import optimization\n# import run_classifier\n# import tokenization\n\n# from tokenization import FullTokenizer\nfrom sklearn.preprocessing import LabelEncoder\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow_hub as hub\nfrom tqdm import tqdm_notebook\nfrom tqdm import notebook # tqdm.notebook.tqdm\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model\n\nprint(f'Python {sys.version}')\nprint(f'Tensorflow {tf.__version__}')","c7debee4":"# https:\/\/www.kaggle.com\/c\/amazon-pet-product-reviews-classification\n\ntrain_df = pd.read_csv('..\/input\/amazon-pet-product-reviews-classification\/train.csv', index_col='id')\nval_df = pd.read_csv('..\/input\/amazon-pet-product-reviews-classification\/valid.csv', index_col='id')\ntest_df = pd.read_csv('..\/input\/amazon-pet-product-reviews-classification\/test.csv', index_col='id')\n\nlabel_encoder = LabelEncoder().fit(pd.concat([train_df['label'], val_df['label']]))\n\n# Train and Validation Features (text)\nX_train_val, X_test = pd.concat([train_df['text'], val_df['text']]).values, test_df['text'].values\n\n# Train and Validation Labels\ny_train_val = label_encoder.fit_transform(pd.concat([train_df['label'], val_df['label']]))\n\n# Split into new train and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1, random_state=0, stratify=y_train_val)","44fcaf51":"max_seq_length = 128  # Your choice here.\n\ntrain_text = X_train\ntrain_text = [' '.join(t.split()[0:max_seq_length]) for t in train_text]\ntrain_text = np.array(train_text, dtype=object)[:, np.newaxis]\ntrain_label = y_train\n\nval_text = X_val\nval_text = [' '.join(t.split()[0:max_seq_length]) for t in val_text]\nval_text = np.array(val_text, dtype=object)[:, np.newaxis]\nval_label = y_val\n\ntest_text = X_test\ntest_text = [' '.join(t.split()[0:max_seq_length]) for t in test_text]\ntest_text = np.array(test_text, dtype=object)[:, np.newaxis]","c5bc4c48":"# https:\/\/stackabuse.com\/text-classification-with-bert-tokenizer-and-tf-2-0-in-python\/\n\nimport bert\n\nBertTokenizer = bert.bert_tokenization.FullTokenizer\nbert_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/3\",\n                            trainable=False)\nvocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\nto_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = BertTokenizer(vocabulary_file, to_lower_case)\n\n# Test tokenizer\ntokenizer.tokenize(\"don't be so judgmental\")","876ededb":"class PaddingInputExample(object):\n    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n  When running eval\/predict on the TPU, we need to pad the number of examples\n  to be a multiple of the batch size, because the TPU requires a fixed batch\n  size. The alternative is to drop the last batch, which is bad because it means\n  the entire output data won't be generated.\n  We use this class instead of `None` because treating `None` as padding\n  battches could cause silent errors.\n  \"\"\"\n\nclass InputExample(object):\n    \"\"\"A single training\/test example for simple sequence classification.\"\"\"\n\n    def __init__(self, guid, text_a, text_b=None, label=None):\n        \"\"\"Constructs a InputExample.\n    Args:\n      guid: Unique id for the example.\n      text_a: string. The untokenized text of the first sequence. For single\n        sequence tasks, only this sequence must be specified.\n      text_b: (Optional) string. The untokenized text of the second sequence.\n        Only must be specified for sequence pair tasks.\n      label: (Optional) string. The label of the example. This should be\n        specified for train and dev examples, but not for test examples.\n    \"\"\"\n        self.guid = guid\n        self.text_a = text_a\n        self.text_b = text_b\n        self.label = label\n\ndef create_tokenizer_from_hub_module(tf_hub):\n    \"\"\"\n    Bert tokenizer for Tensorflow 2\n    https:\/\/stackabuse.com\/text-classification-with-bert-tokenizer-and-tf-2-0-in-python\/\n\n    \n    Get the vocab file and casing info from the Hub module.\n    \"\"\"\n    BertTokenizer = bert.bert_tokenization.FullTokenizer\n    bert_layer = hub.KerasLayer(tf_hub,\n                                trainable=False)\n    vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n    to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n    tokenizer = BertTokenizer(vocabulary_file, to_lower_case)\n    \n    return tokenizer\n\ndef convert_single_example(tokenizer, example, max_seq_length=256):\n    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n\n    if isinstance(example, PaddingInputExample):\n        input_ids = [0] * max_seq_length\n        input_mask = [0] * max_seq_length\n        segment_ids = [0] * max_seq_length\n        label = 0\n        return input_ids, input_mask, segment_ids, label\n\n    tokens_a = tokenizer.tokenize(example.text_a)\n    if len(tokens_a) > max_seq_length - 2: # Take into account that we will be prepending [CLS] and appending [SEP] to the sequence\n        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n\n    tokens = []\n    segment_ids = []\n    tokens.append(\"[CLS]\")\n    segment_ids.append(0)\n    for token in tokens_a:\n        tokens.append(token)\n        segment_ids.append(0)\n    tokens.append(\"[SEP]\")\n    segment_ids.append(0)\n    \n    #print(tokens)\n    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n    # tokens are attended to.\n    input_mask = [1] * len(input_ids)\n\n    # Zero-pad up to the sequence length.\n    while len(input_ids) < max_seq_length:\n        input_ids.append(0)\n        input_mask.append(0)\n        segment_ids.append(0)\n\n    assert len(input_ids) == max_seq_length\n    assert len(input_mask) == max_seq_length\n    assert len(segment_ids) == max_seq_length\n\n    return input_ids, input_mask, segment_ids, example.label\n\ndef convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n\n    input_ids, input_masks, segment_ids, labels = [], [], [], []\n    for example in notebook.tqdm(examples, desc=\"Converting examples to features\"):\n        input_id, input_mask, segment_id, label = convert_single_example(\n            tokenizer, example, max_seq_length\n        )\n        input_ids.append(input_id)\n        input_masks.append(input_mask)\n        segment_ids.append(segment_id)\n        labels.append(label)\n    return (\n        np.array(input_ids),\n        np.array(input_masks),\n        np.array(segment_ids),\n        np.array(labels).reshape(-1, 1),\n    )\n\ndef convert_text_to_examples(texts, labels):\n    \"\"\"Create InputExamples\"\"\"\n    InputExamples = []\n    for text, label in zip(texts, labels):\n        InputExamples.append(\n            InputExample(guid=None, text_a=\" \".join(text), text_b=None, label=label)\n        )\n    return InputExamples","2f6962ea":"# Instantiate tokenizer\nbert_path = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/3\"\ntokenizer = create_tokenizer_from_hub_module(bert_path)\n\n# Convert data to InputExample format\ntrain_examples = convert_text_to_examples(train_text, train_label)\nval_examples = convert_text_to_examples(val_text, val_label)\n\n# Convert to features\n(train_input_ids, train_input_masks, train_segment_ids, train_labels) = convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_seq_length)\n(val_input_ids, val_input_masks, val_segment_ids, val_labels) = convert_examples_to_features(tokenizer, val_examples, max_seq_length=max_seq_length)","f81753ce":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport os\nimport re\nimport numpy as np\nfrom tqdm import tqdm_notebook\n#from tensorflow.keras import backend as K\nfrom keras import backend as K\nfrom tensorflow.keras.layers import Layer\nimport keras\n\n\n'''\nResources explaining how keras.layers.Layer is implemented\nYou can override the build() and call() methods in a subclass of Layer because Layer.__call__() calls build() and call()\n\nhttps:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/Layer#__call__\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/v2.3.0\/tensorflow\/python\/keras\/engine\/base_layer.py#L875-L994\n'''\n\nclass BertLayer(Layer):\n    \n    '''BertLayer which support next output_representation param:\n    \n    pooled_output: the first CLS token after adding projection layer () with shape [batch_size, 768]. \n    sequence_output: all tokens output with shape [batch_size, max_length, 768].\n    mean_pooling: mean pooling of all tokens output [batch_size, max_length, 768].\n    \n    For view trainable parameters call model.trainable_weights after creating model.\n    \n    '''\n    \n    def __init__(self, output_representation='pooled_output', is_trainable=False, **kwargs):\n        \n        self.is_trainble = is_trainable\n        self.output_size = 768\n        # version 3 includes functionality to get intermediate activations of all L=12 Transformer blocks (hidden layers) are returned as a Python list: outputs[\"encoder_outputs\"]\n        self.tf_hub = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/3\"\n        self.output_representation = output_representation\n        self.supports_masking = True\n        \n        super(BertLayer, self).__init__(**kwargs)\n\n\n    '''\n    build() is called by the Layer method __call__()\n    defining build() here will make the inherited Layer.__call__() call the build() method defined in the BertLayer subclass\n    build() is called in Layer.__call__() before call()\n    '''\n    def build(self, input_shape):\n#         print('BertLayer.build() is called')\n\n        # Get pre-trained BERT module\n        self.bert = hub.KerasLayer(self.tf_hub, trainable=True)\n        \n        # Set trainable variables\n        variables = self.bert.variables\n        self.bert._trainable_weights = [var for var in variables if '\/layer_10\/' in var.name]\n        self.bert._trainable_weights.extend([var for var in variables if '\/layer_11\/' in var.name])\n         \n        # Set non-trainable variables - these weights are \"frozen\"\n        trainable_vars_name = [var.name for var in self._trainable_weights]\n        for var in variables:\n            if var.name not in trainable_vars_name:\n                self.bert._non_trainable_weights.append(var)\n        \n        super(BertLayer, self).build(input_shape)\n    \n    '''\n    call() is called by the Layer method __call__()\n    defining call() here will make the inherited Layer.__call__() call the call() method defined in the BertLayer subclass\n    build() is called in Layer.__call__() before call()\n    '''\n    def call(self, inputs):\n#         print('BertLayer.call() is called')\n        \n        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n        input_word_ids, input_mask, input_type_ids = inputs\n        bert_inputs = dict(input_word_ids=input_word_ids, input_mask=input_mask, input_type_ids=input_type_ids)\n        result = self.bert(inputs=bert_inputs) # result = [pooled_output, sequence_output]; Version 3 includes ['encoder_outputs']\n        \n        if self.output_representation == \"pooled_output\":\n            pooled = result[\"pooled_output\"] # result[0]\n            \n        elif self.output_representation == \"mean_pooling\":\n            result_tmp = result[\"sequence_output\"] # result[1]\n        \n            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) \/ (\n                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n            input_mask = tf.cast(input_mask, tf.float32)\n            pooled = masked_reduce_mean(result_tmp, input_mask)\n        \n        elif self.output_representation == \"sequence_output\":\n            pooled = result[\"sequence_output\"] # result[1]\n        \n        # does well overall; does ok on the free text prediction test\n        elif self.output_representation == \"sum_mean_pooling\":\n            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) \/ (\n                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n            input_mask = tf.cast(input_mask, tf.float32)\n            \n            pooled = masked_reduce_mean(result['encoder_outputs'][-4], input_mask)\n            for result_tmp in result['encoder_outputs'][-3:]:\n                pooled += masked_reduce_mean(result_tmp, input_mask)\n        \n        # does decently well overall\n        elif self.output_representation == \"concat_mean_pooling\":\n            # https:\/\/towardsdatascience.com\/nlp-extract-contextualized-word-embeddings-from-bert-keras-tf-67ef29f60a7b\n            # concatenate the outputs from the last 4 encoder layers\n            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) \/ (\n                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n            input_mask = tf.cast(input_mask, tf.float32)\n            \n            pooled1 = masked_reduce_mean(result['encoder_outputs'][-1], input_mask)\n            pooled2 = masked_reduce_mean(result['encoder_outputs'][-2], input_mask)\n            pooled3 = masked_reduce_mean(result['encoder_outputs'][-3], input_mask)\n            pooled4 = masked_reduce_mean(result['encoder_outputs'][-4], input_mask)\n            pooled = tf.concat([pooled1, pooled2, pooled3, pooled4], axis=1)\n        \n        # this doesn't work well overall\n        elif self.output_representation == \"mean_pooling_concat\":\n            result_tmp = tf.concat([result['encoder_outputs'][-1], result['encoder_outputs'][-2], result['encoder_outputs'][-3], result['encoder_outputs'][-4]], axis=1)\n            pooled = tf.reduce_sum(result_tmp, axis=1)\n        \n        elif self.output_representation == \"concat_sequence\":\n            pooled = tf.concat([result['encoder_outputs'][-1], result['encoder_outputs'][-2], result['encoder_outputs'][-3], result['encoder_outputs'][-4]], axis=1)\n        \n        # decent - low data category \"small animals\" doesn't do well\n        elif self.output_representation == \"sum_sequence\":\n            pooled = result['encoder_outputs'][-4]\n            for result_tmp in result['encoder_outputs'][-3:]:\n                pooled += result_tmp\n#             pooled = result['encoder_outputs'][-1] + result['encoder_outputs'][-2] + result['encoder_outputs'][-3] + result['encoder_outputs'][-4]\n        \n        return pooled\n    \n    def compute_mask(self, inputs, mask=None):\n        \n        if self.output_representation == 'sequence_output':\n            inputs = [K.cast(x, dtype=\"bool\") for x in inputs]\n            mask = inputs[1]\n            \n            return mask\n        else:\n            return None\n        \n        \n    def compute_output_shape(self, input_shape):\n        if self.output_representation == \"sequence_output\":\n            return (input_shape[0][0], input_shape[0][1], self.output_size)\n        else:\n            return (input_shape[0][0], self.output_size)","e3294e33":"import keras\n\ndef build_model(max_seq_length, n_classes):\n    \n    max_seq_length = 128  # Your choice here.\n    input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"segment_ids\")\n    bert_inputs = [input_word_ids, input_mask, segment_ids]\n\n    bert_layer = BertLayer(output_representation='sum_mean_pooling', is_trainable=True) # mean_pooling\n#     bert_layer.trainable = False # Set entire BERT layer to non-trainable\n    bert_output = bert_layer(inputs=bert_inputs)\n    \n    drop = keras.layers.Dropout(0.3)(bert_output)\n    dense = keras.layers.Dense(256, activation='relu')(drop)\n    drop = keras.layers.Dropout(0.3)(dense)\n    logits = keras.layers.Dense(128, activation='relu')(drop)\n    if len(np.shape(logits)) > 2:\n        logits = keras.layers.Flatten()(logits)\n    pred = keras.layers.Dense(n_classes, activation='softmax')(logits)\n    \n    model = keras.models.Model(inputs=bert_inputs, outputs=pred)\n    Adam = tf.keras.optimizers.Adam(lr=1e-3)\n    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam, metrics=['sparse_categorical_accuracy'])\n    model.summary()\n\n    return model\n\ndef initialize_vars(sess):\n    sess.run(tf.local_variables_initializer())\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf.tables_initializer())\n    K.set_session(sess)","1267de60":"tf.keras.backend.clear_session()\n\nn_classes = len(label_encoder.classes_)\nmodel = build_model(max_seq_length, n_classes)","523e225d":"from keras.callbacks import EarlyStopping\n\n    \nEPOCHS = 5 # SAMPLES_PER_EPOCH = 62469, BATCH_SIZE = 256\nBATCH_SIZE = 256\nprint(f'BATCH_SIZE is {BATCH_SIZE}')\ne_stopping = EarlyStopping(monitor='val_sparse_categorical_accuracy', patience=3, verbose=1, mode='max', restore_best_weights=True)\ncallbacks = [e_stopping]\n\nlr = 1e-3 # initial learning rate\nfor epoch in range(1, EPOCHS+1):\n    history = model.fit([train_input_ids, train_input_masks, train_segment_ids],\n                    train_labels,\n                    validation_data = ([val_input_ids, val_input_masks, val_segment_ids], val_labels),\n                    epochs=1,\n                    verbose=1,\n                    batch_size=BATCH_SIZE,\n                    callbacks=callbacks)\n    \n    if (epoch%2) == 0:\n        # Manually decay learning rate\n        lr = lr\/2\n        Adam = tf.keras.optimizers.Adam(lr=lr)\n        model.optimizer = Adam\n        model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam, metrics=['sparse_categorical_accuracy'])","c19f766e":"'''\n# Choose an optimizer and loss function for training:\n# EPOCHS = 10, SAMPLES_PER_EPOCH = 62469, BATCH_SIZE = 256\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-3, decay_steps=int(2*62469\/256), decay_rate=0.5, staircase=True) \noptimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n\n\n# Select metrics to measure the loss and the accuracy of the model. These metrics accumulate the values over epochs and then print the overall result.\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\ntest_loss = tf.keras.metrics.Mean(name='test_loss')\ntest_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n\n\n# Use tf.GradientTape to train the model:\n@tf.function\ndef train_step(model, images, labels):\n    with tf.GradientTape() as tape:\n        # training=True is only needed if there are layers with different\n        # behavior during training versus inference (e.g. Dropout).\n        predictions = model(images, training=True)\n        loss = loss_object(labels, predictions) \n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n    train_loss(loss)\n    train_accuracy(labels, predictions)\n    \n    \n# Test the model:\n@tf.function\ndef test_step(model, images, labels):\n    # training=False is only needed if there are layers with different\n    # behavior during training versus inference (e.g. Dropout).\n    predictions = model(images, training=False)\n    t_loss = loss_object(labels, predictions)\n\n    test_loss(t_loss)\n    test_accuracy(labels, predictions)\n    \n    \n# Use tf.data to batch and shuffle the dataset:\ntrain_ds = tf.data.Dataset.from_tensor_slices((train_input_ids, train_input_masks, train_segment_ids, train_labels)).shuffle(62469).batch(256)\ntest_ds = tf.data.Dataset.from_tensor_slices((val_input_ids, val_input_masks, val_segment_ids, val_labels)).batch(256)\n\n\n# Start training loop\nEPOCHS = 10\nfor epoch in range(EPOCHS):\n    # Reset the metrics at the start of the next epoch\n    train_loss.reset_states()\n    train_accuracy.reset_states()\n    test_loss.reset_states()\n    test_accuracy.reset_states()\n\n    for input_ids, input_masks, segment_ids, labels in train_ds:\n        train_step(model, [input_ids, input_masks, segment_ids], labels)\n\n    for test_input_ids, test_input_masks, test_segment_ids, test_labels in test_ds:\n        test_step(model, [test_input_ids, test_input_masks, test_segment_ids], test_labels)\n\n    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n    print(template.format(epoch + 1,\n                        train_loss.result(),\n                        train_accuracy.result() * 100,\n                        test_loss.result(),\n                        test_accuracy.result() * 100))\n'''","34471c57":"# save model\nmodel.save(\"BERT_Keras_TF2\")\n\n# save weights\n# model.save_weights(\"BERT_Keras_TF2.h5\")","b881d35d":"### PREDICTIONS ON VALIDATION SET ### \nprediction = model.predict([val_input_ids, val_input_masks, val_segment_ids], verbose=1)\npreds = label_encoder.classes_[np.argmax(prediction, axis=1)]\n\npd.set_option('display.max_colwidth', None)\nresults = pd.DataFrame({'Text':val_text[:,0], 'Label':label_encoder.classes_[val_labels[:,0]], 'Pred':preds})\nresults.head()","446cf945":"results[(results.Label != results.Pred)]","0c18da8e":"results[(results.Label == results.Pred)]","bcc76bcc":"total_count = len(results)\nerror_count = len(results[(results.Label != results.Pred)])\ncorrect_count = total_count - error_count\n\nprint(f'Validation Results: {correct_count}\/{total_count} - {100*correct_count\/total_count:.2f}%')","c3f5b235":"for label in label_encoder.classes_:\n    print(f'{label} - {len(results[(results.Label==results.Pred) & (results.Label==label)])}')","dcff3287":"# https:\/\/scikit-learn.org\/0.18\/auto_examples\/model_selection\/plot_confusion_matrix.html\n\nimport itertools\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = np.round(cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis], 2)\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n        \n    print(cm)\n\n    thresh = cm.max() \/ 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n    \n\n# Compute confusion matrix\n# labels = label_encoder.classes_\ncnf_matrix = confusion_matrix(y_true=results.Label, y_pred=results.Pred)\nclass_names = label_encoder.classes_ # list(set(results.Label))\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure(figsize=(10,10))\nplot_confusion_matrix(cnf_matrix, classes=class_names,\n                      title='Confusion matrix, without normalization')\n\n# Plot normalized confusion matrix\nplt.figure(figsize=(10,10))\nplot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n                      title='Normalized confusion matrix')\n\nplt.show()","ea5ccffa":"test_text = [['''\nThis fake dog stuffed animal is great because my cat loves it!\n''']]\ntest_label = [[0]] # input arrays need to be 2-D: [batch, seq_length]\ntest_examples = convert_text_to_examples(test_text, test_label)\ninput_ids, input_masks, segment_ids, labels = convert_examples_to_features(tokenizer, test_examples, max_seq_length=max_seq_length)\n\nprob_a = model.predict([input_ids, input_masks, segment_ids], verbose=1)\npreds = label_encoder.classes_[np.argmax(prob_a, axis=1)]\n\nprint()\nprint(f'Raw text: {test_text[0][0]}')\nprint(tokenizer.convert_ids_to_tokens(input_ids[0]))\n# print(tokenizer.tokenize(test_text[0][0]))\n\nprint()\nfor prob, class_ in zip(prob_a[0], label_encoder.classes_):\n    print(f'{class_:20} - {prob:.3f}')\n    \nprint()\nprint(f'Pred: {preds[0]}')","85c516ff":"'''\nThere's currently no good way to get to the intermediate layers of models imported via hub.KerasLayer\nhttps:\/\/github.com\/tensorflow\/hub\/issues\/453\nhttps:\/\/stackoverflow.com\/questions\/57410282\/tensorflow-2-hub-how-can-i-obtain-the-output-of-an-intermediate-layer\nhttps:\/\/stackoverflow.com\/questions\/55333558\/how-to-access-bert-intermediate-layer-outputs-in-tf-hub-module\n\nhttps:\/\/keras.io\/getting_started\/faq\/#how-can-i-obtain-the-output-of-an-intermediate-layer-feature-extraction\n\n'''","90cda808":"model.layers[3].result_concat","204f9da8":"# Create Model","a4717e94":"# Get BERT Intermediate Layers","4a8d0d65":"# Save Model","bc3cf554":"# Load Data","d8d5b873":"# BERT Layer","74e887e1":"# Prediction on Validation Set","b4b0e4a7":"# Train Model","7ae4fdd3":"> # Tokenization","1d0adc28":"# Prediction on Free Text"}}