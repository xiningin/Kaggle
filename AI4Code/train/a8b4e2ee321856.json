{"cell_type":{"af9eaed0":"code","c4b0f23d":"code","5ead7c32":"code","137f9bd3":"code","279ae4d7":"code","9c441756":"code","78bb0488":"code","1006b189":"code","818ed43e":"code","ebbcf171":"code","e1602116":"code","2604eabd":"code","17a6f726":"code","06c640b6":"code","17f45975":"code","c82fb879":"code","7c14e156":"code","ae7c6daf":"code","06073e5d":"code","c3267d5e":"code","8a572922":"code","9b816424":"code","be2a0af1":"code","c31fca4f":"code","bc37cba7":"code","d3cd6a74":"code","a4da4381":"code","f8e7c027":"code","f77e054a":"code","7a302865":"markdown","a7e70f83":"markdown","a63ac01c":"markdown","cb94b95b":"markdown"},"source":{"af9eaed0":"# import all libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport re\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import scale\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\n\nimport warnings # supress warnings\nwarnings.filterwarnings('ignore')","c4b0f23d":"# import Housing.csv\nhousing = pd.read_csv(\"..\/input\/housing-simple-regression\/Housing.csv\",delimiter=',', encoding=\"utf-8-sig\")\nhousing.head()","5ead7c32":"# number of observations \nlen(housing.index)","137f9bd3":"# filter only area and price\ndf = housing.loc[:, ['area', 'price']]\ndf.head()","279ae4d7":"# recaling the variables (both)\ndf_columns = df.columns\nscaler = MinMaxScaler()\ndf = scaler.fit_transform(df)\n\n# rename columns (since now its an np array)\ndf = pd.DataFrame(df)\ndf.columns = df_columns\n\ndf.head()","9c441756":"# visualise area-price relationship\nsns.regplot(x=\"area\", y=\"price\", data=df, fit_reg=False)","78bb0488":"# split into train and test\ndf_train, df_test = train_test_split(df, \n                                     train_size = 0.7, \n                                     test_size = 0.3, \n                                     random_state = 10)\nprint(len(df_train))\nprint(len(df_test))","1006b189":"# split into X and y for both train and test sets\n# reshaping is required since sklearn requires the data to be in shape\n# (n, 1), not as a series of shape (n, )\nX_train = df_train['area']\nX_train = X_train.values.reshape(-1, 1)\ny_train = df_train['price']\n\nX_test = df_test['area']\nX_test = X_test.values.reshape(-1, 1)\ny_test = df_test['price']","818ed43e":"len(X_train)","ebbcf171":"degrees = [1,2,3,6,10,20]\n\ny_train_pred = np.zeros((len(y_train),len(degrees)))\n\ny_test_pred = np.zeros((len(y_test),len(degrees)))\n\nfor i,degree in enumerate(degrees):\n    \n    model = make_pipeline(PolynomialFeatures(degree),LinearRegression())\n    model = model.fit(X_train,y_train)\n    \n    y_train_pred[:, i] = model.predict(X_train)\n    y_test_pred[:, i] = model.predict(X_test)","e1602116":"# visualise train and test predictions\n# note that the y axis is on a log scale\n\nplt.figure(figsize=(16, 8))\n\n# train data\nplt.subplot(121)\nplt.scatter(X_train, y_train)\nplt.yscale('log')\nplt.title(\"Train data\")\nfor i, degree in enumerate(degrees):    \n    plt.scatter(X_train, y_train_pred[:, i], s=15, label=str(degree))\n    plt.legend(loc='upper left')\n    \n# test data\nplt.subplot(122)\nplt.scatter(X_test, y_test)\nplt.yscale('log')\nplt.title(\"Test data\")\nfor i, degree in enumerate(degrees):    \n    plt.scatter(X_test, y_test_pred[:, i], label=str(degree))\n    plt.legend(loc='upper left')","2604eabd":"# compare r2 for train and test sets (for all polynomial fits)\nprint(\"R-squared values: \\n\")\n\nfor i, degree in enumerate(degrees):\n    train_r2 = round(sklearn.metrics.r2_score(y_train, y_train_pred[:, i]), 2)\n    test_r2 = round(sklearn.metrics.r2_score(y_test, y_test_pred[:, i]), 2)\n    print(\"Polynomial degree {0}: train score={1}, test score={2}\".format(degree, \n                                                                         train_r2, \n                                                                         test_r2))","17a6f726":"# data preparation\n\n# list of all the \"yes-no\" binary categorical variables\n# we'll map yes to 1 and no to 0\nbinary_vars_list =  ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']\n\n# defining the map function\ndef binary_map(x):\n    return x.map({'yes': 1, \"no\": 0})\n\n# applying the function to the housing variables list\nhousing[binary_vars_list] = housing[binary_vars_list].apply(binary_map)\nhousing.head()","06c640b6":"status = pd.get_dummies(housing['furnishingstatus'], drop_first = True)\nstatus.head()","17f45975":"housing = pd.concat([housing, status], axis = 1)\nhousing.head()","c82fb879":"housing.drop(['furnishingstatus'], axis = 1, inplace = True)\nhousing.head()","7c14e156":"# train-test 70-30 split\ndf_train, df_test = train_test_split(housing, \n                                     train_size = 0.7, \n                                     test_size = 0.3, \n                                     random_state = 100)\n\n# rescale the features\nscaler = MinMaxScaler()\n\n# apply scaler() to all the numeric columns \nnumeric_vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking','price']\ndf_train[numeric_vars] = scaler.fit_transform(df_train[numeric_vars])\ndf_train.head()","ae7c6daf":"df_test[numeric_vars] = scaler.fit_transform(df_test[numeric_vars])\ndf_test.head()","06073e5d":"y_train = df_train.pop('price')\nX_train = df_train\n\ny_test = df_test.pop('price')\nX_test = df_test","c3267d5e":"# num of max features\nlen(X_train.columns)","8a572922":"lm = LinearRegression()\n\nlm = lm.fit(X_train,y_train)\n\nrfe = RFE(lm,10)             \nrfe = rfe.fit(X_train, y_train)\n\nlist(zip(X_train.columns,rfe.support_,rfe.ranking_))","9b816424":"# predict prices of X_test\ny_pred = rfe.predict(X_test)\n\n# evaluate the model on test set\nr2 = sklearn.metrics.r2_score(y_test, y_pred)\nprint(r2)","be2a0af1":"lm = LinearRegression()\n\nlm.fit(X_train,y_train)\n\nrfe = RFE(lm,6)\n\nrfe = rfe.fit(X_train,y_train)\n\ny_pred = rfe.predict(X_test)\n\nr2 = sklearn.metrics.r2_score(y_test,y_pred)\nprint(r2)","c31fca4f":"lm = LinearRegression()\n\nscore = cross_val_score(lm,X_train,y_train,scoring = 'r2',cv=5)\nscore","bc37cba7":"scores = cross_val_score(lm,X_train,y_train,scoring='neg_mean_squared_error',cv=5)\nscores","d3cd6a74":"folds = KFold(n_splits=5,shuffle=True,random_state=100)\n\nhyper_params = [{'n_features_to_select': list(range(1, 14))}]\n\nlm = LinearRegression()\nlm.fit(X_train, y_train)\nrfe = RFE(lm) \n\nmodel_cv = GridSearchCV(estimator = rfe, \n                        param_grid = hyper_params, \n                        scoring= 'r2', \n                        cv = folds, \n                        verbose = 1,\n                        return_train_score=True)      \n\n# fit the model\nmodel_cv.fit(X_train, y_train)                  \n","a4da4381":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results","f8e7c027":"# plotting cv results\nplt.figure(figsize=(16,6))\n\nplt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_test_score\"])\nplt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_train_score\"])\nplt.xlabel('number of features')\nplt.ylabel('r-squared')\nplt.title(\"Optimal Number of Features\")\nplt.legend(['test score', 'train score'], loc='upper left')\nplt.show()","f77e054a":"\n# final model\nn_features_optimal = 10\n\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, n_features_to_select=n_features_optimal)             \nrfe = rfe.fit(X_train, y_train)\n\n# predict prices of X_test\ny_pred = lm.predict(X_test)\nr2 = sklearn.metrics.r2_score(y_test, y_pred)\nprint(r2)","7a302865":"<table style=\"width:100%\">\n  <tr>\n    <th>   <\/th>\n    <th>degree-1<\/th>\n    <th>degree-2<\/th> \n    <th>degree-3<\/th>\n    <th>...<\/th>\n    <th>degree-n<\/th>\n  <\/tr>\n  <tr>\n    <th>x1<\/th>\n  <\/tr>\n  <tr>\n    <th>x2<\/th>\n  <\/tr>\n   <tr>\n    <th>x3<\/th>\n    <\/tr>\n    <tr>\n    <th>...<\/th>\n    <\/tr>\n    <tr>\n    <th>xn<\/th>\n    <\/tr>\n<\/table>","a7e70f83":"Let's now predict the y labels (for both train and test sets) and store the predictions in a table. Each row of the table is one data point, each column is a value of $n$ (degree).","a63ac01c":"### Polynomial Regression\n\nYou already know simple linear regression:\n\n$y = \\beta_0 + \\beta_1 x_1$\n\nIn polynomial regression of degree $n$, we fit a curve of the form:\n\n$y = \\beta_0 + \\beta_1 x_1 + \\beta_2x_1^2 + \\beta_3x_1^3 ... + \\beta_nx_1^n$\n\nIn the experiment below, we have fitted polynomials of various degrees on the housing data and compared their performance on train and test sets.\n\nIn sklearn, polynomial features can be generated using the `PolynomialFeatures` class. Also, to perform `LinearRegression` and `PolynomialFeatures` in tandem, we will use the module `sklearn_pipeline` - it basically creates the features and feeds the output to the model (in that sequence).","cb94b95b":"For the first experiment, we'll do regression with only one feature. Let's filter the data so it only contains `area` and `price`."}}