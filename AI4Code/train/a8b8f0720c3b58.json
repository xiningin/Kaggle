{"cell_type":{"4ef890e4":"code","d9622441":"code","119ac4b9":"code","52bf824d":"code","40839dfb":"code","9213f369":"code","9a99982f":"code","5d0bace0":"code","9dcdbe0a":"code","b0155b3a":"code","61b57001":"code","ce97b394":"code","562eb986":"code","70cfb3a2":"code","0792f386":"code","f1f5b5d3":"code","65502775":"code","f0004874":"code","e3bf2dac":"code","b89086c0":"code","ade4dc56":"code","582d6851":"code","8ffcc625":"code","758dc6da":"code","9df0315f":"markdown","81601818":"markdown","decbd56f":"markdown","3cde2a5d":"markdown","5ef04f1a":"markdown","6f166405":"markdown","7503dd2a":"markdown","5bd43bd0":"markdown","963fc114":"markdown","25b3a4c0":"markdown","3248f401":"markdown","2d683fa6":"markdown","9c709401":"markdown","3c30322e":"markdown","d3a2139d":"markdown","da69533b":"markdown","8cef3e13":"markdown","8952e2d4":"markdown","7580f028":"markdown","c61a0974":"markdown","c6d93bb6":"markdown","1955a68d":"markdown","b4b5dba5":"markdown","28085e92":"markdown"},"source":{"4ef890e4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d9622441":"data = pd.read_csv('\/kaggle\/input\/million-headlines\/abcnews-date-text.csv')\ndata.head()","119ac4b9":"data['publish_year'] = data['publish_date'].apply(lambda x:int(x\/10000))\ndata['publish_month'] = data['publish_date'].apply(lambda x:int(((x)%10000)\/100))\ndata['publish_day'] = data['publish_date'].apply(lambda x:((x)%10000)%100)","52bf824d":"import matplotlib.pyplot as plt\nplt.hist(data['publish_year'], facecolor='blue', alpha=0.8, rwidth = 0.5)\nplt.xlabel('Year')\nplt.ylabel('#News Headlines')\nplt.title('#News Headlines in each year')\nplt.show()","40839dfb":"plt.hist(data['publish_month'],12, facecolor='blue', alpha=0.8, rwidth = 0.5)\nplt.xlabel('Month')\nplt.ylabel('#News Headlines')\nplt.title('#News Headlines in each month')\nplt.show()","9213f369":"plt.hist(data['publish_day'],31, facecolor='blue', alpha=0.8, rwidth = 0.5)\nplt.xlabel('Day')\nplt.ylabel('#News Headlines')\nplt.title('#News Headlines on each day of month')\nplt.show()","9a99982f":"corp = str()\nfor i in range(len(data['headline_text'])):\n    corp += (' ')+data['headline_text'][i]","5d0bace0":"import nltk\nwords = nltk.word_tokenize(corp)\n#data['headline_text'][1] + (' ') + data['headline_text'][2] + data['headline_text'][3]","9dcdbe0a":"from nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nf_words = [w for w in words if not w in stop_words] \n\npunctuations = '''!()-[]{};:'\"\\,<>.\/?@#$%^&*_~'''\nfp_words = [w for w in f_words if not w in punctuations] ","b0155b3a":"fd = nltk.FreqDist(fp_words)\n\ndf_fdist = pd.DataFrame.from_dict(fd, orient='index')\ndf_fdist.columns = ['Frequency']\ndf_fdist.index.name = 'Term'\n\nfreq_df = df_fdist[df_fdist['Frequency']>500]\nd = freq_df.to_dict()['Frequency']\n\n#plt.figure(figsize=(20, 8))\nfreq_df1 = df_fdist[df_fdist['Frequency']>7500]\nfreq_df1.sort_values('Frequency',ascending=False).plot(kind='bar')\n#freq_df1.columns","61b57001":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(32,32))\nwordcloud = WordCloud()\nwordcloud.generate_from_frequencies(frequencies=d)\nplt.figure()\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","ce97b394":"from nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\nimport nltk\ndef text_cleaner(text):\n    stop_words = set(stopwords.words('english'))\n    f_words = [w for w in nltk.word_tokenize(text) if not w in stop_words] \n    punctuations = '''!()-[]{};:'\"\\,<>.\/?@#$%^&*_~'''\n    fp_words = [w for w in f_words if not w in punctuations] \n    fp_words_stem = [ps.stem(words) for words in fp_words]\n    fp_sent = ' '.join(word for word in fp_words_stem)\n    return fp_sent\n\n#text_cleaner(data['headline_text'][1])","562eb986":"data['headline_text_clean'] = data['headline_text'].apply(text_cleaner)\ndata['headline_text_clean'][:10]","70cfb3a2":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(max_features=10000)\nvectorizer.fit(data['headline_text_clean'].values)\ndata_tfidf = vectorizer.transform(data['headline_text_clean'])\n\ntfidf_to_word = np.array(vectorizer.get_feature_names())\n#tfidf_to_word","0792f386":"from sklearn.decomposition import NMF\nnmf = NMF(n_components=50, solver=\"mu\")\nW = nmf.fit_transform(data_tfidf)\nH = nmf.components_\n#W.shape\n#H.shape","f1f5b5d3":"for i, topic in enumerate(H):\n    print(\"Topic {}: {}\".format(i + 1, \",\".join([str(x) for x in tfidf_to_word[topic.argsort()[-10:]]])))","65502775":"topic_list = np.array([(\"Topic\" + \" \"+ str(i)) for i in range(1,51)])\nfor i,topic in enumerate(W[:10000,]):\n    print(\"Headline {}: {}\".format(i+1,\",\".join([str(x) for x in topic_list[topic.argsort()[-10:]]])))","f0004874":"from sklearn.manifold import TSNE\ntsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\ntsne_results = tsne.fit_transform(W[:10000,])\n\ndf_subset = pd.DataFrame()\ndf_subset.head()\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndf_subset = pd.DataFrame()\ndf_subset['tsne-2d-one'] = tsne_results[:,0]\ndf_subset['tsne-2d-two'] = tsne_results[:,1]","e3bf2dac":"plt.figure(figsize=(16,10))\nplt.scatter(x=tsne_results[:,0],y=tsne_results[:,1],alpha=0.8, c=\"y\")\n\nplt.figure(figsize=(16,10))\nx = x=tsne_results[:,0]\ny = y=tsne_results[:,1]\nplt.scatter(x,y,alpha=0.8, c=y)","b89086c0":"from sklearn.cluster import KMeans\n\n# Number of clusters\nkmeans = KMeans(n_clusters=10)\n# Fitting the input data\nkmeans = kmeans.fit(W)\n# Getting the cluster labels\nlabels = kmeans.predict(W)\n# Centroid values\ncentroids = kmeans.cluster_centers_","ade4dc56":"centroids\ndata['cluster'] = labels\ndata['cluster'].value_counts()\ndata[['headline_text_clean','cluster']].sample(n=1000)","582d6851":"from sklearn.cluster import KMeans\nimport numpy as np\nsse = {}\nfor k in np.arange(1, 10, 1):\n    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(W)\n    #data[\"clusters\"] = kmeans.labels_\n    #print(data[\"clusters\"])\n    sse[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center\nplt.figure()\nplt.plot(list(sse.keys()), list(sse.values()))\nplt.xlabel(\"Number of cluster\")\nplt.ylabel(\"SSE\")\nplt.show()","8ffcc625":"clust = data['cluster'][2]\nW_filter = data[data['cluster'] == clust].index\nW_filter\n\nW[W_filter].shape","758dc6da":"from sklearn.metrics.pairwise import cosine_similarity\n\ndef cos_pair(X):\n    sim_x = []\n    for i in range(len(W[W_filter])):\n        sim_x.append(cosine_similarity(X.reshape(1,W.shape[1]),W[W_filter][i].reshape(1,W.shape[1])))\n    return sim_x\n        \nsim = cos_pair(W[0])\ntop10recos = sorted(range(len(sim)), key=lambda i: sim[i])[-10:]\ndata['headline_text'][top10recos]","9df0315f":"**6. Plotting Number of Headlines published by every day of month**","81601818":"**23. Finding the cluster membership of the input query for recommendation engine. ANd filtering all the headlines belonging to that clusters. The search for recommendations would happen over the members of only this cluster and not globally.**","decbd56f":"**3. Creating the Publish Year, Publish Month and Publish Day. **","3cde2a5d":"**18. Implementing t-SNE to reduce the W (50 dim vectors) to 2 dims for plotting**","5ef04f1a":"**7. Creating text corpus for word cloud and frequency plots**","6f166405":"**17. Each document can be represented as a collection of topics. Matrix W represents each document (10000 rows\/documents subsetted here) and topics making it (50 columns\/topics). Here we are printing the top 10 topics making the document.**","7503dd2a":"**20. Implement k-Means. 10 clusters is fast but 100 clusters gives better results. ** ","5bd43bd0":"**2. Reading the 'A million headline' dataset**","963fc114":"**22. Plotting Inertial vs Number of Clusters. If k belongs to np.arange(1,102,0), it runs k-Means with k=1,11,21,....,101. Larger k might take 15 mins+ to run. **","25b3a4c0":"**5. Plotting Number of Headlines published every month**","3248f401":"**13. Applying cleaning function to 'headline_text'**","2d683fa6":"**9. Cleaning the corpus - removing stopwords & punctuations**","9c709401":"**12. Cleaner function to apply on the 'headline_text' column for modeling preparation**","3c30322e":"**11. Creating Word Cloud**","d3a2139d":"**15. Decomposing the TF-IDF matrix to smaller matrices W (11,03,663 X 50) and H (50 X 10000) using Non-Negative Matrix Factorization**","da69533b":"**4. Plotting Number of Headlines every year**","8cef3e13":"**24. A function to find cosine similarity for a given input query with all the other elements. The top 10 similar elements would be shown as recommendations for the input element. ** ","8952e2d4":"**14. Implementing TF-IDF on the data with capping the number of features \/ token to 10000. The dimension of TF-IDF is 11,03,663 X 10000**","7580f028":"**16. Each topic is a made of collection of words. Matrix H represents each topic (50 rows\/topics) and words making it (10000 columns\/words). Here we are printing the top 10 words making the topic.**","c61a0974":"**19. t-SNE option 2 for plotting **","c6d93bb6":"**1.Importing the basic libraries and walking through the source directory containing the files.**","1955a68d":"**21. Getting results from clustering**","b4b5dba5":"**10. Creating the Frequency dictionary and frequency plots**","28085e92":"**8. Tokenize the corpus**"}}