{"cell_type":{"bb51f654":"code","62f05efe":"code","15c92f84":"code","a24c7f70":"code","13730e91":"code","8b41d91a":"code","4cbabe7b":"code","ae2974e2":"code","a21e350a":"code","b1212c93":"code","871c67f1":"code","ff02ffd6":"code","755c84c0":"code","0ae1fdca":"code","b7940c26":"code","2075b38d":"code","bd4151c7":"code","82d752f2":"code","82d219e5":"code","1e3964be":"code","cfcd4813":"code","be483780":"code","efe41a98":"code","0bc14f0c":"code","5db10e1c":"code","9f626870":"code","251d55ca":"code","960d83c7":"code","262245df":"code","3220aa06":"code","e7210f09":"code","48f29c11":"code","19af62ed":"code","8d6500c9":"code","4bb12431":"code","901cd817":"code","86b2d174":"code","1b574d94":"code","5591ef14":"code","a5cdf48a":"code","cec57c81":"code","8011e7ca":"code","1ca1e0b0":"code","7130fac7":"code","a69d3a89":"code","8fed1547":"code","216ab89e":"code","0fb510dc":"markdown","0e8f974f":"markdown","2a2772a5":"markdown","f6e7fa13":"markdown","6ce1d247":"markdown","c3764a83":"markdown","4f77d2e5":"markdown","cbeca054":"markdown","ab2a27cb":"markdown","bfa64682":"markdown","1bdd9a7e":"markdown","53013d2d":"markdown","19ade9c2":"markdown","2ea15739":"markdown","cf3cb169":"markdown","fb4adca7":"markdown","79eb1551":"markdown","af50c9c4":"markdown","62963f0e":"markdown","82576685":"markdown","73a215bf":"markdown","79b2d057":"markdown","8b65e3d5":"markdown","a5feeb3e":"markdown","f18f9496":"markdown","19ea3914":"markdown","b80b3a6b":"markdown","40510a95":"markdown","299128fd":"markdown","c283d2b4":"markdown","f5cb53f1":"markdown","ad6a5b2d":"markdown"},"source":{"bb51f654":"!pip install textstat\n!pip install rich\n\nimport os\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport re\nimport nltk\nimport textstat\nimport time\nimport wandb\nimport rich\nimport spacy\n\nfrom pandas import DataFrame\nfrom matplotlib.lines import Line2D\nfrom rich.console import Console\nfrom rich import print\nfrom rich.theme import Theme\nfrom nltk.corpus import stopwords\nfrom nltk import pos_tag\nfrom collections import Counter\nfrom wordcloud import WordCloud,STOPWORDS\nfrom spacy import displacy\nfrom nltk.tokenize import sent_tokenize, word_tokenize \nfrom sklearn.feature_extraction.text import CountVectorizer as CV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error as mse\n\nnltk.download('stopwords')","62f05efe":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"api_key\")\n\nos.environ[\"WANDB_SILENT\"] = \"true\"","15c92f84":"! wandb login $api_key","a24c7f70":"sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n\ndef custom_palette(custom_colors):\n    customPalette = sns.set_palette(sns.color_palette(custom_colors))\n    sns.palplot(sns.color_palette(custom_colors),size=0.8)\n    plt.tick_params(axis='both', labelsize=0, length = 0)\n\npalette = [\"#7209B7\",\"#3F88C5\",\"#136F63\",\"#F72585\",\"#FFBA08\"]\npalette2 = sns.diverging_palette(120, 220, n=20)\ncustom_palette(palette)\n\ncustom_theme = Theme({\n    \"info\" : \"italic bold cyan\",\n    \"warning\": \"italic bold magenta\",\n    \"danger\": \"bold blue\"\n})\n\nconsole = Console(theme=custom_theme)","13730e91":"train_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")","8b41d91a":"train_df.head()","4cbabe7b":"test_df.head()","ae2974e2":"train_df.nunique()","a21e350a":"msno.bar(train_df,color=palette[2], sort=\"ascending\", figsize=(10,5), fontsize=12)\nplt.show()","b1212c93":"excerpt1 = train_df['excerpt'].min()\nconsole.print(\"Before preprocessing: \",style=\"info\")\nconsole.print(excerpt1,style='warning')\n\ne = re.sub(\"[^a-zA-Z]\", \" \", excerpt1)\ne = e.lower()\n        \ne = nltk.word_tokenize(e)\n        \ne = [word for word in e if not word in set(stopwords.words(\"english\"))]\n        \nlemma = nltk.WordNetLemmatizer()\ne = [lemma.lemmatize(word) for word in e]\ne=\" \".join(e)\nconsole.print(\"After preprocessing: \",style=\"info\")\nconsole.print(e,style='warning')","871c67f1":"#====== Preprocessing function ======\ndef preprocess(data):\n    excerpt_processed=[]\n    for e in data['excerpt']:\n        \n        # find alphabets\n        e = re.sub(\"[^a-zA-Z]\", \" \", e)\n        \n        # convert to lower case\n        e = e.lower()\n        \n        # tokenize words\n        e = nltk.word_tokenize(e)\n        \n        # remove stopwords\n        e = [word for word in e if not word in set(stopwords.words(\"english\"))]\n        \n        # lemmatization\n        lemma = nltk.WordNetLemmatizer()\n        e = [lemma.lemmatize(word) for word in e]\n        e=\" \".join(e)\n        \n        excerpt_processed.append(e)\n        \n    return excerpt_processed ","ff02ffd6":"# train_df[\"excerpt_preprocessed\"] = preprocess(train_df)\n# test_df[\"excerpt_preprocessed\"] = preprocess(test_df)\n\n# #====== Saving to csv files and creating artifacts ======\n# train_df.to_csv(\"train_excerpt_preprocessed.csv\")\n\n# run = wandb.init(project='commonlit', name='excerpt_preprocessed')\n\n# artifact = wandb.Artifact('train_excerpt_preprocessed', type='dataset')\n\n# #====== Add a file to the artifact's contents ======\n# artifact.add_file(\"train_excerpt_preprocessed.csv\")\n\n# #====== Save the artifact version to W&B and mark it as the output of this run ====== \n# run.log_artifact(artifact)\n\n# run.finish()\n\n# #====== Saving to csv files and creating artifacts ======\n# test_df.to_csv(\"test_excerpt_preprocessed.csv\")\n\n# run = wandb.init(project='commonlit', name='excerpt_preprocessed')\n\n# artifact = wandb.Artifact('test_excerpt_preprocessed', type='dataset')\n\n# #====== Add a file to the artifact's contents ======\n# artifact.add_file(\"test_excerpt_preprocessed.csv\")\n\n# #====== Save the artifact version to W&B and mark it as the output of this run ====== \n# run.log_artifact(artifact)\n\n# run.finish()","755c84c0":"run = wandb.init(project='commonlit')\nartifact = run.use_artifact('ruchi798\/commonlit\/train_excerpt_preprocessed:v0', type='dataset')\nartifact_dir = artifact.download()\nrun.finish()\n\npath = os.path.join(artifact_dir,\"train_excerpt_preprocessed.csv\")\ntrain_df = pd.read_csv(path)\ntrain_df = train_df.drop(columns=[\"Unnamed: 0\"])\n\nrun = wandb.init(project='commonlit')\nartifact = run.use_artifact('ruchi798\/commonlit\/test_excerpt_preprocessed:v0', type='dataset')\nartifact_dir = artifact.download()\nrun.finish()\n\npath = os.path.join(artifact_dir,\"test_excerpt_preprocessed.csv\")\ntest_df = pd.read_csv(path)\ntest_df = test_df.drop(columns=[\"Unnamed: 0\"])","0ae1fdca":"#====== Function to plot wandb bar chart ======\ndef plot_wb_bar(df,col1,col2,name,title): \n    run = wandb.init(project='commonlit', job_type='image-visualization',name=name)\n    \n    dt = [[label, val] for (label, val) in zip(df[col1], df[col2])]\n    table = wandb.Table(data=dt, columns = [col1,col2])\n    wandb.log({name : wandb.plot.bar(table, col1,col2,title=title)})\n\n    run.finish()\n    \n#====== Function to plot wandb histogram ======\ndef plot_wb_hist(df,name,title):\n    run = wandb.init(project='commonlit', job_type='image-visualization',name=name)\n\n    dt = [[x] for x in df[name]]\n    table = wandb.Table(data=dt, columns=[name])\n    wandb.log({name : wandb.plot.histogram(table, name, title=title)})\n\n    run.finish()","b7940c26":"fig, ax = plt.subplots(1,2,figsize=(20,10))\nsns.kdeplot(train_df['target'], color=palette[0], shade=True,ax=ax[0])\nsns.kdeplot(train_df['standard_error'], color=palette[1], shade=True,ax=ax[1])\nax[0].axvline(train_df['target'].mean(), color=palette[0],linestyle=':', linewidth=2)\nax[1].axvline(train_df['standard_error'].mean(), color=palette[1],linestyle=':', linewidth=2)\nax[0].set_title(\"Target Distribution\",font=\"Serif\")\nax[1].set_title(\"Standard Error Distribution\",font=\"Serif\")\nax[0].annotate('mean', xy=(-0.3* np.pi, 0.2), xytext=(1, 0.2), font='Serif',\n            arrowprops=dict(arrowstyle=\"->\",\n                            connectionstyle=\"angle3,angleA=0,angleB=-90\"));\nax[1].annotate('mean', xy=(0.49, 6), xytext=(0.57, 6), font='Serif',\n            arrowprops=dict(arrowstyle=\"->\",\n                            connectionstyle=\"angle3,angleA=0,angleB=-90\"));\nplt.show()\n\nsns.jointplot(x=train_df['target'], y=train_df['standard_error'], kind='hex',height=10,edgecolor=palette[4])\nplt.suptitle(\"Target vs Standard error \",font=\"Serif\")\nplt.subplots_adjust(top=0.95)\nplt.show()","2075b38d":"plot_wb_hist(train_df,\"target\",\"Target Distribution\")\nplot_wb_hist(train_df,\"standard_error\",\"Standard Error Distribution\")","bd4151c7":"run = wandb.init(project='commonlit', name='count')\n\n# maximum target\nm_t = train_df[\"target\"].max() \n\n# minimum target\nl_t = train_df[\"target\"].min() \n\n# maximum standard error\nm_se = train_df[\"standard_error\"].max()\n\n# minimum standard error\nl_se = train_df[\"standard_error\"].min() \n\nwandb.log({'Target (highest value)': m_t, \n           'Target (lowest value)': l_t,\n           'Standard error (highest value)': m_se, \n           'Standard error (lowest value)': l_se\n          })\n\nrun.finish()","82d752f2":"plt.figure(figsize=(16, 8))\nsns.countplot(y=\"license\",data=train_df,palette=\"BrBG\",linewidth=3)\nplt.title(\"License Distribution\",font=\"Serif\")\nplt.show()","82d219e5":"license_data = pd.DataFrame(train_df.license.value_counts().reset_index().values,columns=[\"license\", \"counts\"])\nplot_wb_bar(license_data,'license', 'counts',\"license\",\"License Distribution\")","1e3964be":"def get_top_n_words(corpus, n=None):\n    vec = CV().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef get_top_n_bigram(corpus, n=None):\n    vec = CV(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\n\ndef get_top_n_trigram(corpus, n=None):\n    vec = CV(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","cfcd4813":"def plot_bt(x,w,p):\n    common_words = x(train_df['excerpt_preprocessed'], 20)\n    common_words_df = DataFrame (common_words,columns=['word','freq'])\n\n    plt.figure(figsize=(16,8))\n    sns.barplot(x='freq', y='word', data=common_words_df,facecolor=(0, 0, 0, 0),linewidth=3,edgecolor=sns.color_palette(p,20))\n    plt.title(\"Top 20 \"+ w,font='Serif')\n    plt.xlabel(\"Frequency\", fontsize=14)\n    plt.yticks(fontsize=13)\n    plt.xticks(rotation=45, fontsize=13)\n    plt.ylabel(\"\");\n    return common_words_df","be483780":"common_words = get_top_n_words(train_df['excerpt_preprocessed'], 20)\ncommon_words_df1 = DataFrame(common_words,columns=['word','freq'])\nplt.figure(figsize=(16, 8))\nax = sns.barplot(x='freq', y='word', data=common_words_df1,facecolor=(0, 0, 0, 0),linewidth=3,edgecolor=sns.color_palette(\"ch:start=3, rot=.1\",20))\n\nplt.title(\"Top 20 unigrams\",font='Serif')\nplt.xlabel(\"Frequency\", fontsize=14)\nplt.yticks(fontsize=13)\nplt.xticks(rotation=45, fontsize=13)\nplt.ylabel(\"\");\n\ncommon_words_df2 = plot_bt(get_top_n_bigram,\"bigrams\",\"ch:rot=-.5\")\ncommon_words_df3 = plot_bt(get_top_n_trigram,\"trigrams\",\"ch:start=-1, rot=-.6\")","efe41a98":"plot_wb_bar(common_words_df1,'word','freq',\"unigrams\",\"Top 20 unigrams\")\nplot_wb_bar(common_words_df2,'word','freq',\"bigrams\",\"Top 20 bigrams\")\nplot_wb_bar(common_words_df3,'word','freq',\"trigrams\",\"Top 20 trigrams\")","0bc14f0c":"# color function for the wordcloud\ndef color_wc(word=None,font_size=None,position=None, orientation=None,font_path=None, random_state=None):\n    h = int(360.0 * 150.0 \/ 255.0)\n    s = int(100.0 * 255.0 \/ 255.0)\n    l = int(100.0 * float(random_state.randint(80, 120)) \/ 255.0)\n    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n\nplt.subplots(figsize=(16,16))\nwc = WordCloud(stopwords=STOPWORDS,background_color=\"white\", contour_width=2, contour_color='blue',width=1500, height=750,color_func=color_wc,max_words=150, max_font_size=256,random_state=42)\nwc.generate(' '.join(train_df['excerpt_preprocessed']))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","5db10e1c":"text_props = train_df.copy()\n\ndef avg_word_len(df):\n    df = df.str.split().apply(lambda x : [len(i) for i in x]).map(lambda x: np.mean(x))\n    return df\n\ntext_len = train_df['excerpt'].str.len()\ntext_len_pre = train_df['excerpt_preprocessed'].str.len()\navg_text = avg_word_len(train_df['excerpt'])\navg_text_pre = avg_word_len(train_df['excerpt_preprocessed'])\nlexicon_count = []\nlexicon_count_pre = []\nsentence_count = []\nfor i in range(len(train_df)):\n    lc = textstat.lexicon_count(train_df['excerpt'][i])\n    lcp = textstat.lexicon_count(train_df['excerpt_preprocessed'][i])\n    sc = textstat.sentence_count(train_df['excerpt'][i])\n    lexicon_count.append(lc)\n    lexicon_count_pre.append(lcp)\n    sentence_count.append(sc)\n    \ntext_props['text_len'] = text_len\ntext_props['text_len_pre'] = text_len_pre\ntext_props['lexicon_count'] = lexicon_count\ntext_props['lexicon_count_pre'] = lexicon_count_pre\ntext_props['avg_text'] = avg_text\ntext_props['avg_text_pre'] = avg_text_pre\ntext_props['sentence_count'] = sentence_count\n\ndef plot_distribution(col1,col2,title1,title2):\n    fig, ax = plt.subplots(1,2,figsize=(20,10))\n    sns.kdeplot(data=text_props, x=col1,color=palette[3],label=\"Excerpt\",ax=ax[0])\n    sns.kdeplot(data=text_props, x=col2,color=palette[4],label=\"Excerpt preprocessed\",ax=ax[0])\n    ax[0].set_title(title1,font=\"Serif\")\n\n    sns.scatterplot(data=text_props,x=col1,y='target',color= palette[3],ax=ax[1],markers='.')\n    sns.scatterplot(data=text_props,x=col2,y='target',color= palette[4],ax=ax[1],markers='.')\n    ax[1].set_title(title2,font=\"Serif\")\n\n    plt.show()\n\ncustom_lines = [Line2D([0], [0], color=palette[3], lw=4),\n                Line2D([0], [0], color=palette[4], lw=4)]\n\nplt.figure(figsize=(20, 1))\nlegend = plt.legend(custom_lines, ['Excerpt', 'Excerpt preprocessed'],loc=\"center\")\nplt.setp(legend.texts, family='Serif')\nplt.axis('off')\nplt.show()\n\nplot_distribution(\"text_len\",\"text_len_pre\",\"Character count distribution\",\"Character count vs Target\")\nplot_distribution(\"lexicon_count\",\"lexicon_count_pre\",\"Word count distribution\",\"Word count vs Target\")\nplot_distribution(\"avg_text\",\"avg_text_pre\", \"Average word length distribution\",\"Average word length vs Target\")\n\nfig, ax = plt.subplots(1,2,figsize=(20,10))\nsns.kdeplot(data=text_props, x=sentence_count,color=palette[3],label=\"Excerpt\",ax=ax[0])\nax[0].set_title(\"Sentence count distribution\",font=\"Serif\")\nax[0].set_xlabel(\"sentence_count\")\nsns.scatterplot(data=text_props,x='sentence_count',y='target',color= palette[3],ax=ax[1],markers='.')\nax[1].set_title(\"Sentence count vs Target\",font=\"Serif\")\nplt.show()\n\nnum_cols = ['text_len','text_len_pre','lexicon_count','lexicon_count_pre','avg_text','avg_text_pre','sentence_count','target']\ncorr = text_props[num_cols].corr()\n\nfig = plt.figure(figsize=(12,12),dpi=80)\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask=mask, cmap='BuPu', robust=True, center=0,\n            square=True, linewidths=.5)\nplt.title('Correlation of text properties', fontsize=15,font=\"Serif\")\nplt.show()","9f626870":"plot_wb_hist(text_props,\"text_len\",\"Character Count Distribution\")\nplot_wb_hist(text_props,\"lexicon_count\",\"Word Count Distribution\")\nplot_wb_hist(text_props,\"avg_text\",\"Average Word Length Distribution\")\nplot_wb_hist(text_props,\"sentence_count\",\"Sentence Count Distribution\")","251d55ca":"text_props['pos_tags'] = text_props['excerpt_preprocessed'].str.split().map(pos_tag)\n\ndef count_tags(pos_tags):\n    tag_count = {}\n    for word,tag in pos_tags:\n        if tag in tag_count:\n            tag_count[tag] += 1\n        else:\n            tag_count[tag] = 1\n    return tag_count\n\ntext_props['tag_counts'] = text_props['pos_tags'].map(count_tags)","960d83c7":"set_pos = set([tag for tags in text_props['tag_counts'] for tag in tags])\ntag_cols = list(set_pos)\n\nfor tag in tag_cols:\n    text_props[tag] = text_props['tag_counts'].map(lambda x: x.get(tag, 0))","262245df":"pos = text_props[tag_cols].sum().sort_values(ascending = False)\nplt.figure(figsize=(16,10))\nax = sns.barplot(x=pos.index, y=pos.values,palette=\"Wistia\")\nplt.xticks(rotation = 50)\nax.set_yscale('log')\nplt.title('POS tags frequency',fontsize=15,font=\"Serif\")\nplt.show()","3220aa06":"pos_data = pd.DataFrame({'part_of_speech':pos.index, 'freq':pos.values})\nplot_wb_bar(pos_data,'part_of_speech', 'freq',\"POS\",\"POS tags frequency\")","e7210f09":"plt.figure(figsize=(10,8))\nsns.scatterplot(data=text_props,x='NN',y='target',color= palette[3],markers='.',label=\"noun, singular\")\nsns.scatterplot(data=text_props,x='JJ',y='target',color= palette[4],markers='.',label=\"adjective\",)\nsns.scatterplot(data=text_props,x='VBD',y='target',color= palette[0],markers='.',label=\"verb past tense\")\nsns.scatterplot(data=text_props,x='RB',y='target',color= palette[1],markers='.',label=\"adverb\")\nplt.legend(title=\"POS tag\",bbox_to_anchor=(1.4, 1))\nplt.xlabel(\"POS tags count\")\nplt.title(\"POS vs Target\")\nplt.show()","48f29c11":"toughest_excerpt = text_props[text_props[\"target\"] == text_props[\"target\"].min()].excerpt.values[0]\nlowest_target = text_props[text_props[\"target\"] == text_props[\"target\"].min()].target.values[0]\nnlp = spacy.load(\"en_core_web_sm\")\nsentences = sent_tokenize(toughest_excerpt)\nword_count = lambda sentence: len(word_tokenize(sentence))\npos_text = max(sentences, key=word_count)  \n\nconsole.print(\"Target of the toughest excerpt: \",style=\"info\")\nconsole.print(lowest_target,style='warning')\n\nconsole.print(\"Longest sentence of the toughest excerpt: \",style=\"info\")","19af62ed":"doc = nlp(pos_text)\ndisplacy.render(doc, style=\"dep\")","8d6500c9":"# flesch_re, flesch_kg, fog_scale, automated_r,coleman, linsear, text_standard  = ([] for i in range(7))\n# for i in range(len(text_props)):\n#     flr = textstat.flesch_reading_ease(train_df['excerpt'][i])\n#     flkg = textstat.flesch_kincaid_grade(train_df['excerpt'][i])\n#     fs = textstat.gunning_fog(train_df['excerpt'][i])\n#     ar = textstat.automated_readability_index(train_df['excerpt'][i])\n#     cole = textstat.coleman_liau_index(train_df['excerpt'][i])\n#     lins = textstat.linsear_write_formula(train_df['excerpt'][i])\n#     ts = textstat.text_standard(train_df['excerpt'][i])\n    \n#     flesch_re.append(flr)\n#     flesch_kg.append(flkg)\n#     fog_scale.append(fs)\n#     automated_r.append(ar)\n#     coleman.append(cole)\n#     linsear.append(lins)\n#     text_standard.append(ts)\n    \n# text_props['flesch_re'] = flesch_re\n# text_props['flesch_kg'] = flesch_kg\n# text_props['fog_scale'] = fog_scale\n# text_props['automated_r'] = automated_r\n# text_props['coleman'] = coleman\n# text_props['linsear'] = linsear\n# text_props['text_standard'] = text_standard","4bb12431":"# #====== Saving to csv files and creating artifacts ======\n# text_props.to_csv(\"text_props_readability.csv\")\n\n# run = wandb.init(project='commonlit', name='text_props_readability')\n\n# artifact = wandb.Artifact('text_props_readability', type='dataset')\n\n# #====== Add a file to the artifact's contents ======\n# artifact.add_file(\"text_props_readability.csv\")\n\n# #====== Save the artifact version to W&B and mark it as the output of this run ====== \n# run.log_artifact(artifact)\n\n# run.finish()","901cd817":"run = wandb.init(project='commonlit')\nartifact = run.use_artifact('ruchi798\/commonlit\/text_props_readability:v0', type='dataset')\nartifact_dir = artifact.download()\nrun.finish()\n\npath = os.path.join(artifact_dir,\"text_props_readability.csv\")\ntext_props = pd.read_csv(path)\ntext_props = text_props.drop(columns=[\"Unnamed: 0\"])","86b2d174":"readability_cols = ['flesch_re','flesch_kg','fog_scale','automated_r','coleman','linsear','text_standard','target']\n\ncorr = text_props[readability_cols].corr()\nfig = plt.figure(figsize=(12,12),dpi=80)\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask=mask, cmap='PuBuGn', robust=True, center=0,\n            square=True, linewidths=.5,annot=True)\nplt.title('Correlation of readability tests', fontsize=15,font=\"Serif\")\nplt.show()","1b574d94":"plt.figure(figsize=(10,8))\nsns.kdeplot(text_props[\"flesch_re\"],color=palette[4],shade=True)\nplt.title(\"Distribution of Flesch Reading Ease test\")\nplt.show()","5591ef14":"plot_wb_hist(text_props,\"flesch_re\",\"Flesch Reading Ease Distribution\")","a5cdf48a":"text_props.loc[text_props['flesch_re'] > 60]['flesch_re'].count() \/ len(text_props) *100","cec57c81":"pd.set_option('display.max_colwidth', None)\nmax_text = text_props[text_props[\"target\"] == text_props[\"target\"].max()]['excerpt']\nmin_text = text_props[text_props[\"target\"] == text_props[\"target\"].min()]['excerpt']\n\nmax_text_f = text_props[text_props[\"flesch_re\"] == text_props[\"flesch_re\"].max()]['excerpt']\nmin_text_f = text_props[text_props[\"flesch_re\"] == text_props[\"flesch_re\"].min()]['excerpt']\n\nconsole.print(\"Highest Target\", style=\"danger\")\nconsole.print(max_text, style=\"info\")\ntext_props[text_props[\"target\"] == text_props[\"target\"].max()][['flesch_re','target','text_standard']]","8011e7ca":"console.print(\"Highest Flesch Reading Ease Score\", style=\"danger\")\nconsole.print(max_text_f, style=\"info\")\ntext_props[text_props[\"flesch_re\"] == text_props[\"flesch_re\"].max()][['flesch_re','target','text_standard']]","1ca1e0b0":"console.print(\"Lowest Target\", style=\"danger\")\nconsole.print(min_text, style=\"warning\")\ntext_props[text_props[\"target\"] == text_props[\"target\"].min()][['flesch_re','target','text_standard']]","7130fac7":"console.print(\"Lowest Flesch Reading Ease Score\", style=\"danger\")\nconsole.print(min_text_f, style=\"warning\")\ntext_props[text_props[\"flesch_re\"] == text_props[\"flesch_re\"].min()][['flesch_re','target','text_standard']]","a69d3a89":"def training(model, X_train, y_train, X_test, y_test, model_name):\n    t1 = time.time()\n    \n    model = make_pipeline(\n        TfidfVectorizer(binary=True, ngram_range=(1,1)),\n        model,\n    )\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    MSE = mse(y_test, y_pred)\n    \n    t2 = time.time()\n    training_time = t2-t1 \n    \n    console.print(\"--- Model:\", model_name,\"---\",style='warning')\n    console.print(\"MSE: \",MSE,style='danger')\n    console.print(\"Training time:\",training_time,style='danger')\n\nridge = Ridge(fit_intercept = True, normalize = False)\nlr = LinearRegression()\nm = [ridge,lr]\nmn = [\"Ridge Regression\",\"Linear Regression\"]\n\nX = train_df[\"excerpt_preprocessed\"]\ny = train_df['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nfor i in range(0,len(m)):\n    training(model=m[i], X_train=X_train, y_train=y_train, X_test=X_test,y_test=y_test, model_name=mn[i])","8fed1547":"def training_all(model,X,y):\n    \n    model = make_pipeline(\n        TfidfVectorizer(binary=True, ngram_range=(1,1)),\n        model,\n    )\n    model.fit(X, y)\n    y_pred = model.predict(test_df[\"excerpt_preprocessed\"])\n    \n    return y_pred","216ab89e":"test_pred = training_all(lr,X,y)\npredictions = pd.DataFrame()\npredictions['id'] = test_df['id']\npredictions['target'] = test_pred\npredictions.to_csv(\"\/kaggle\/working\/submission.csv\", index=False)\npredictions","0fb510dc":"Logging **custom histograms** for target and standard error distribution \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f","0e8f974f":"<img src=\"https:\/\/camo.githubusercontent.com\/dd842f7b0be57140e68b2ab9cb007992acd131c48284eaf6b1aca758bfea358b\/68747470733a2f2f692e696d6775722e636f6d2f52557469567a482e706e67\">\n\n> I will be integrating W&B for visualizations and logging artifacts!\n> \n> [CommonLit Project on W&B Dashboard](https:\/\/wandb.ai\/ruchi798\/commonlit?workspace=user-ruchi798) \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f\n> \n> - To get the API key, an account is to be created on the [website](https:\/\/wandb.ai\/site) first.\n> - Next, use secrets to use API Keys more securely \ud83e\udd2b","2a2772a5":"# Missing values\ud83d\udd2e","f6e7fa13":"It's important to preprocess the excerpt before we proceed further!","6ce1d247":"\ud83d\udccc More than **70%** of excerpts can be easily understood by **13-15 year olds**.","c3764a83":"Here I have used a viz module called [missingno](https:\/\/pypi.org\/project\/missingno\/) to visualize missing values in the training set.\n\nWe don't have any missing values in the columns of our interest, i.e., ```excerpt```, ```target``` and ```standard_error```!","4f77d2e5":"Using the **saved artifact** \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f","cbeca054":"| Abbreviation    | Meaning                                              |\n|-----------------|------------------------------------------------------|\n| CC              | coordinating conjunction                             |\n| CD              | cardinal digit                                       |\n| DT              | determiner                                           |\n| EX              | existential there                                    |\n| FW              | foreign word                                         |\n| IN              | preposition\/subordinating conjunction                |\n| JJ              | adjective (large)                                    |\n| JJR             | adjective, comparative (larger)                      |\n| JJS             | adjective, superlative (largest)                     |\n| LS              | list item marker                                     |\n| MD              | modal (could, will)                                  |\n| NN              | noun, singular                                       |\n| NNS             | noun plural                                          |\n| NNP             | proper noun, singular                                |\n| NNPS            | proper noun, plural                                  |\n| PDT             | predeterminer                                        |\n| POS             | possessive ending (parent\\ 's)                       |\n| PRP             | personal pronoun (hers, herself, him,himself)        |\n| PRP dollar-sign | possessive pronoun (her, his, mine, my, our )        |\n| RB              | adverb (occasionally, swiftly)                       |\n| RBR             | adverb, comparative (greater)                        |\n| RBS             | adverb, superlative (biggest)                        |\n| RP              | particle (about)                                     |\n| SYM             | symbol                                               |\n| TO              | infinite marker (to)                                 |\n| UH              | interjection (goodbye)                               |\n| VB              | verb (ask)                                           |\n| VBG             | verb gerund (judging)                                |\n| VBD             | verb past tense (pleaded)                            |\n| VBN             | verb past participle (reunified)                     |\n| VBP             | verb, present tense not 3rd person singular(wrap)    |\n| VBZ             | verb, present tense with 3rd person singular (bases) |\n| WDT             | wh-determiner (that, what)                           |\n| WP              | wh- pronoun (who)                                    |\n| WP dollar-sign  | possessive wh-pronoun                                |\n| WRB             | wh- adverb (how)                                     |\n\n\ud83d\udccc Higher the grade, more the complexity of grammar(?)\n\n[Penn Part of Speech Tags](https:\/\/cs.nyu.edu\/~grishman\/jet\/guide\/PennPOS.html)","ab2a27cb":"Logging a **custom bar chart** for POS distribution \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f","bfa64682":"Let's see which excerpts have the highest and lowest ```Target``` and ```Flesch Reading Ease Score``` \u2b07\ufe0f","1bdd9a7e":"Logging a **dictionary of custom objects** \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f","53013d2d":"\ud83d\udccc ```sentence_count``` and ```target``` are very highly correlated since extremely long sentences can be complex to read and understand.","19ade9c2":"# Import libraries \ud83d\udcda","2ea15739":"# Submission file \ud83d\udcdd","cf3cb169":"Illustrations tools \u26a1\n\n- [Canva](https:\/\/www.canva.com\/) \ud83d\udd8c\ufe0f\n\n<img src=\"https:\/\/i.imgur.com\/pl3FhXV.png\">","fb4adca7":"Logging a **custom bar chart** for license distribution \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f","79eb1551":"# EDA \ud83d\udcca","af50c9c4":"# Pre-processing excerpt \u2702\ufe0f","62963f0e":"A snapshot of the newly created artifacts \u2b07\ufe0f\n\n<img src=\"https:\/\/i.imgur.com\/WFeQRt7.png\">","82576685":"Since I have already logged the artifact, I can directly use it in this manner \u2b07\ufe0f","73a215bf":"Logging a **custom histogram** for the distribution of Flesch Reading Ease scores \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f","79b2d057":"Here's a snapshot of my [project](https:\/\/wandb.ai\/ruchi798\/commonlit?workspace=user-ruchi798) \u2b07\ufe0f\n\n<img src=\"https:\/\/i.imgur.com\/vmxri2T.png\">","8b65e3d5":"# Part-of-Speech tagging \ud83c\udff7\ufe0f","a5feeb3e":"Logging **custom histograms** for the distribution of character count, word count, average word length and sentence count \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f","f18f9496":"Logging the preprocessed dataset as an **artifact** \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f","19ea3914":"# Baseline model \u2699\ufe0f","b80b3a6b":"# Readability tests \ud83e\uddea\n\n[textstat](https:\/\/pypi.org\/project\/textstat\/) is a library used to calculate statistics from text.\nIt came in super handy for calculating scores of various readability tests!\n\n- ```flesch_re:``` [The Flesch Reading Ease formula](https:\/\/en.wikipedia.org\/wiki\/Flesch%E2%80%93Kincaid_readability_tests#Flesch_reading_ease)\n- ```flesch_kg:``` [The Flesch-Kincaid Grade Level ](https:\/\/en.wikipedia.org\/wiki\/Flesch%E2%80%93Kincaid_readability_tests#Flesch_reading_ease)\n- ```fog_scale:``` [The Fog Scale (Gunning FOG Formula)](https:\/\/en.wikipedia.org\/wiki\/Gunning_fog_index)\n- ```automated_r:``` [Automated Readability Index](https:\/\/en.wikipedia.org\/wiki\/Automated_readability_index)\n- ```coleman:``` [The Coleman-Liau Index](https:\/\/en.wikipedia.org\/wiki\/Coleman%E2%80%93Liau_index)\n- ```linsear:``` [Linsear Write Formula](https:\/\/en.wikipedia.org\/wiki\/Linsear_Write)\n- ```text_standard:``` Readability Consensus based upon all the above tests","40510a95":"Logging **custom bar charts** for unigrams, bigrams and trigrams \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f","299128fd":"# Introduction \ud83d\udcdd\n\ud83c\udfaf **Goal:** To build algorithms to rate the complexity of reading passages for grade 3-12 classroom use. \n\n\ud83d\udcd6 **Data:** \n> **train.csv \/ test.csv** - the training and testing set\n> - ```id``` - unique ID for excerpt\n> - ```url_legal``` - URL of source \n> - ```license``` - license of source material \n> - ```excerpt``` - text to predict reading ease of\n> - ```target``` - reading ease\n> - ```standard_error``` - measure of spread of scores among multiple raters for each excerpt\n\n\ud83d\udccc **Note:** ```url_legal```, ```license``` and ```standard error``` are blank in the test set.\n\n\ud83e\uddea **Evaluation metric:** Root Mean Squared Error (RMSE)\n> $$RMSE = \\sqrt{\\frac{1}{n}\\Sigma_{i=1}^{n}{\\Big(\\frac{y_i - \\hat{y_i}}{\\sigma_i}\\Big)^2}}$$\n> where \n> * $y_i$ : original value\n> * $\\hat{y_i}$ : predicted value\n> * $n$ : number of rows in the test data","c283d2b4":"Logging the text properties dataset as an **artifact** \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f\n\nThis helps me to save on time since I can directly use the saved artifact for my workflow \ud83e\udd73","f5cb53f1":"<img src=\"https:\/\/i.imgur.com\/mi9U6o5.png\">","ad6a5b2d":"There is a strong correlation between our ```target``` variable and the ```Flesch Readability Ease``` test values.\n\nLet's explore the distribution of test values for ```Flesch Readability Ease```.\n\n| Score          | Notes                                                                  |\n|----------------|------------------------------------------------------------------------|\n| 90-100         | very easy to read, easily understood by an average 11-year-old student |\n| 80-90          | easy to read                                                           |\n| 70-80          | fairly easy to read                                                    |\n| 60-70          | easily understood by 13- to 15-year-old students                       |\n| 50-60          | fairly difficult to read                                               |\n| 30-50          | difficult to read, best understood by college graduates                |\n| 0-30           | very difficult to read, best understood by university graduates        |"}}