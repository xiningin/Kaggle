{"cell_type":{"b3bc034f":"code","58a88903":"code","b1945976":"code","5c85650a":"code","90d405d9":"code","3e1828f6":"code","10609a56":"code","ba51e6bc":"code","da004afc":"code","54a123ba":"code","e4b139ca":"code","83128251":"code","179e3adb":"code","377e9f18":"code","835504be":"code","ef882bcd":"markdown","d6f7eba2":"markdown","25f6a5e4":"markdown","2c353644":"markdown","a494a458":"markdown","78ab422b":"markdown","37fcd118":"markdown","7a4aa705":"markdown","04c7b9fd":"markdown"},"source":{"b3bc034f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","58a88903":"import pandas as pd\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.tokenize import wordpunct_tokenize\nfrom nltk.corpus import stopword\nimport unicodedata\nfrom collections import Counter\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix","b1945976":"_MOST_COMMON_COUNT = 1200","5c85650a":"data = pd.read_csv('..\/input\/sms-spam-collection-dataset\/spam.csv',encoding='Windows-1252')\n","90d405d9":"data = data.loc[:,['v1','v2']]\ndata = data.dropna()\ndata.head()","3e1828f6":"stop_words = set(stopwords.words('english'))\nstemmer = SnowballStemmer(\"english\")\ndef normalize_text(text):\n    tokens = wordpunct_tokenize(text)\n    tokens = [\n        unicodedata.normalize('NFD', stemmer.stem(t) )\n        for t in tokens \n        if t not in stop_words\n    ]\n    sentence = ' '.join(tokens)\n    return sentence","10609a56":"data.loc[:, 'v2'] = data.loc[:, 'v2'].apply(normalize_text)\n","ba51e6bc":"counter = Counter()\nfor line in data.loc[:, 'v2']:\n    counter.update(wordpunct_tokenize(line))","da004afc":"most_common = [w[0] for w in counter.most_common(_MOST_COMMON_COUNT)]\nword_map = {str(word): index for (word, index) in zip(most_common, range(len(most_common)))}","54a123ba":"def transform_to_freq(text):\n    features = np.zeros(_MOST_COMMON_COUNT)\n    for word in wordpunct_tokenize(text):\n        if str(word) in word_map:\n            word_index = word_map[str(word)]\n            features[word_index] = features[word_index] + 1\n    return features.tolist()","e4b139ca":"X = np.array([transform_to_freq(line) for line in data.loc[:, 'v2']])\ny = (data.loc[:, 'v1'].values == 'spam').reshape(-1, 1)","83128251":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","179e3adb":"regressor = LogisticRegression(solver='lbfgs').fit(X_train, y_train.ravel())\npredictions = regressor.predict(X_test)","377e9f18":"cm = confusion_matrix(y_test, predictions)\ncm","835504be":" tn = cm[0][0] # True Negatives\nfn = cm[1][0] # False Negatives\ntp = cm[1][1] # True Positives\nfp = cm[0][1] # False Positives\n\nrecall = tp \/ (tp + fn)\nprecision = tp \/ (tp + fp)\naccuracy = (tp + tn) \/ (tn + fn + tp + fp)\nf1_score = 2 * ((precision * recall) \/ (precision + recall))*100\nf1_score","ef882bcd":"**In order to check which words are more frequent**\n\nNow all messages are normalized, reducing the total amount of distinct words that we have in our messages and now we are ready to identify which words are more frequent","d6f7eba2":"Here I have build an spam classifier they are so many ways but I have made this one and since I am a beginner so suggestions are most welcome","25f6a5e4":"Before we start with coding stuff, let us first analyse the data set.In our first column we have found our data as spam and ham(not spam). It can be easily seen from the below table that text is in the form of string and we need to first vectorize the string in the numerical list of list, so basically this is used for learning vector representations of words.","2c353644":"# Precision\nof all the data set where we predicted what fraction y = 1 have actually spam mails\n\n# Recall\nof all the data set having actually spam mails what eaction did we correctly detect have spam mails","a494a458":"**VECTORIZATION**\n\nwe define a fixed length vector where each entry corresponds to a word in our pre-defined dictionary of words. The size of the vector equals the size of the dictionary. Then, for representing a text using this vector, we count how many times each word of our dictionary appears in the text and we put this number in the corresponding vector entry.\n\nFor example, if our dictionary contains the words {ABDULKALAM, is, the, not, great}, and we want to vectorize the text \u201cABDULKALAM is great\u201d, we would have the following vector: (1, 1, 0, 0, 1)\n\nTo improve this representation, you can use some more advanced techniques like removing stopwords, lemmatizing words, using n-grams or using tf-idf instead of counts.","78ab422b":"![https:\/\/www.google.com\/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-confusion-matrix-a9ad42dcfd62&psig=AOvVaw2wmH8em8FLKpB8zAcUp1k1&ust=1598809478631000&source=images&cd=vfe&ved=0CAIQjRxqFwoTCPimi8v7wOsCFQAAAAAdAAAAABAD](http:\/\/)\n\nyou can go though this to get a desired picture of confusion matrix\nWell I'tell you a short about this wht is **CONFUSION MATRIX**\n\nA confusion matrix is a table used to describe the performance of a classifier or a data set for that truw values should be known\n\n","37fcd118":"**STOPWARDS**\n\n stop word is a commonly used word (such as \u201cthe\u201d, \u201ca\u201d, \u201can\u201d, \u201cin\u201d) that a search engine wants to ignore.We do not want these words to take space in our database, or taking up valuable processing time. For this, we can remove them easily, by storing a list of words that you consider to stop words. NLTK(Natural Language Toolkit) in python has a list of stopwords stored in 16 different languages. You can find them in the nltk_data directory.","7a4aa705":"# Spam classification with NLTK and Logistic Regression\n","04c7b9fd":"* If we choose supervised learning then we should take X = features of mail and Y= spam or not spam\n* feature of X will be list of words included for spam or not spam.\n* firstly I have taken NLTK implementation ,which have so many features like tokenizers,stemmers,stopward\n* numpy is used for multiplication of matix\n* pandas for reading the csv and a little of pre-processing  and sklearn for train-test splitting, confusion matrix and losgistic regression.\n\n\n\n"}}