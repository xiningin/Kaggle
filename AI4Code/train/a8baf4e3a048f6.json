{"cell_type":{"d7b6e20b":"code","2b35f371":"code","60289ceb":"code","b462a81a":"code","7961bd8e":"code","5bab27dd":"code","a27b9e6a":"code","779be21d":"code","5b2e0f56":"code","d8f0a067":"code","70e23a89":"code","af3db74a":"code","0f11a8dd":"code","a4458e8c":"code","7224bc72":"code","c1713eba":"code","d77591fa":"markdown","2aa51349":"markdown","2710cb94":"markdown","2f052104":"markdown"},"source":{"d7b6e20b":"!pip install git+git:\/\/github.com\/llefebure\/dynamic_bernoulli_embeddings.git","2b35f371":"import pickle\nimport re\n\nimport numpy as np\nimport pandas as pd\nfrom dynamic_bernoulli_embeddings.analysis import DynamicEmbeddingAnalysis\nfrom dynamic_bernoulli_embeddings.training import train_model\nfrom nltk import word_tokenize as nltk_word_tokenize\nfrom gensim.corpora import Dictionary\nfrom tqdm.notebook import tqdm\ntqdm.pandas()","60289ceb":"def _bad_word(word):\n    if len(word) < 2:\n        return True\n    if any(c.isdigit() for c in word):\n        return True\n    if \"\/\" in word:\n        return True\n    return False\n\ndef word_tokenize(text):\n    text = re.sub(r\"co-operation\", \"cooperation\", text)\n    text = re.sub(r\"-\", \" \", text)\n    words = [w.lower().strip(\"'.\") for w in nltk_word_tokenize(text)]\n    words = [w for w in words if not _bad_word(w)]\n    return words","b462a81a":"dataset = pd.read_csv(\"..\/input\/un-general-debates\/un-general-debates.csv\")\ndataset[\"bow\"] = dataset.text.progress_apply(word_tokenize)\ndataset[\"time\"] = dataset.year - dataset.year.min()","7961bd8e":"dictionary = Dictionary(dataset.bow)\ndictionary.filter_extremes(no_below=10, no_above=1.)\ndictionary.compactify()\nprint(len(dictionary))","5bab27dd":"model, loss_history = train_model(\n    dataset, dictionary.token2id, validation=.1, num_epochs=6, k=100)","a27b9e6a":"loss_history.loss.plot(title=\"Training Loss\")","779be21d":"loss_history.l_pos.plot(title=\"Positive\")","5b2e0f56":"loss_history.l_neg.plot(title=\"Negative\")","d8f0a067":"loss_history.l_prior.plot(title=\"Prior\")","70e23a89":"np.save(\"embeddings\", model.get_embeddings())\nloss_history.to_csv(\"loss_history.csv\", index=False)\npickle.dump(dictionary.token2id, open(\"dictionary.pkl\", \"wb\"))","af3db74a":"emb = DynamicEmbeddingAnalysis(model.get_embeddings(), dictionary.token2id)","0f11a8dd":"emb.absolute_drift()","a4458e8c":"over_time = {}\nfor i in range(0, dataset.time.max() + 1, 5):\n    col = str(dataset.year.min() + i)\n    over_time[col] = emb.neighborhood(\"climate\", i, 10)\npd.DataFrame(over_time)","7224bc72":"over_time = {}\nfor i in range(0, dataset.time.max() + 1, 5):\n    col = str(dataset.year.min() + i)\n    over_time[col] = emb.neighborhood(\"afghanistan\", i, 10)\npd.DataFrame(over_time)","c1713eba":"pd.DataFrame([(dataset.year.min() + i, term) for i, term in emb.change_points(20)], columns=[\"Year\", \"Term\"])","d77591fa":"# Dynamic Bernoulli Embeddings\n\nDynamic Bernoulli Embeddings, discussed [here](http:\/\/www.cs.columbia.edu\/~blei\/papers\/RudolphBlei2018.pdf), are a way to train word embeddings that smoothly change with time. Documents are grouped into time buckets, and every word in the vocabulary gets an embedding for each timestep. These embeddings smoothly drift over time, so embeddings across different timesteps can be meaningfully compared with cosine or euclidean distance.\n\nAfter finding the paper authors' code a bit challenging to use, I created my own PyTorch based implementation. This notebook shows how to apply it to the UN General Debate corpus. You can find my code [here](https:\/\/github.com\/llefebure\/dynamic_bernoulli_embeddings).","2aa51349":"## Preprocessing\n\nThe `train_model` function expects a pandas data frame with at least two columns, `bow` and `time`. `bow` is just a list of words in the document, and `time` is expected to be an integer in `[0, T)` where `T` is the total number of timesteps. It also expects a dictionary mapping tokens to their index in `[0, V)` where `V` is the size of the vocabulary. Any token found in the dataset but not in the vocabulary is ignored.\n\nHere, I do some light preprocessing of the text and use `gensim` to build the dictionary. Documents are bucketed by year.","2710cb94":"## Training\n\nWith the inputs prepared, we can now train the model. We'll set aside 10% of the dataset for validation, run for 6 epochs, and use an embedding dimension of 100.","2f052104":"## Analysis\n\nWe can use the trained embeddings to do some cool analysis such as:\n* finding which embeddings changed the most over time (absolute drift)\n* looking up the \"neighborhood\" of an embedding and seeing how this has changed over time\n* looking for change points where an embedding significantly changed from one timestep to the next indicating some significant event"}}