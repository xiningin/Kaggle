{"cell_type":{"92574697":"code","a077017a":"code","967b9210":"code","b4e3611b":"code","086e8609":"code","0b41d30e":"code","95980dca":"code","53c636b5":"code","6926891b":"code","26e315b2":"code","6b3d9208":"code","2b83b00a":"code","88922bd5":"code","f54b872f":"code","c1ed57c5":"code","7704f661":"code","5781a0b2":"code","35bc7fe1":"markdown","202f9e8d":"markdown","2817811c":"markdown","aaab4a82":"markdown","1daffb57":"markdown","b91e7b25":"markdown","62da1da9":"markdown","496cd799":"markdown","e5d236d1":"markdown","ac24a18e":"markdown","d1e2d3f7":"markdown","e789cd05":"markdown","da681cdd":"markdown","d58c900d":"markdown","34af4566":"markdown","84c32a5c":"markdown"},"source":{"92574697":"pip install plotly==4.14.3","a077017a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.pipeline import make_pipeline\nimport plotly.graph_objects as go\nfrom plotly import tools\nimport plotly.offline as py\nimport matplotlib.pyplot as plt\nimport string\nimport seaborn as sns","967b9210":"train_df = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","b4e3611b":"train_df.shape","086e8609":"train_df.head()","0b41d30e":"print(\"samples with disaster tweet:\", sum(train_df['target']==1))\nprint(\"samples without disaster tweet:\", sum(train_df['target']==0))","95980dca":"test_df.head()","53c636b5":"## target count ##\ncnt_srs = train_df['target'].value_counts()\ntrace = go.Bar(\n    x=cnt_srs.index,\n    y=cnt_srs.values,\n    marker=dict(\n        color=cnt_srs.values,\n        colorscale = 'Picnic',\n        reversescale = True\n    ),\n)\n\nlayout = go.Layout(\n    title='Target Count',\n    font=dict(size=18)\n)\n\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"TargetCount\")\n\n## target distribution ##\nlabels = (np.array(cnt_srs.index))\nsizes = (np.array((cnt_srs \/ cnt_srs.sum())*100))\n\ntrace = go.Pie(labels=labels, values=sizes)\nlayout = go.Layout(\n    title='Target distribution',\n    font=dict(size=18),\n    width=600,\n    height=600,\n)\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"usertype\")\n","6926891b":"from wordcloud import WordCloud, STOPWORDS\n\n# Thanks : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(train_df[\"text\"], title=\"Word Cloud of Questions\")","26e315b2":"from collections import defaultdict\nstopwords = set(STOPWORDS)\nmore_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown', '-', '...', '|', '&', '?', '??', 'via', '2'}\nstopwords = stopwords.union(more_stopwords)\ntrain1_df = train_df[train_df[\"target\"]==1]\ntrain0_df = train_df[train_df[\"target\"]==0]\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in stopwords]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n## Get the bar chart from non disaster tweet ##\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n## Get the bar chart from disaster tweets ##\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of non disaster tweets\", \n                                          \"Frequent words of disaster tweets\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\npy.iplot(fig, filename='word-plots')\n","6b3d9208":"freq_dict = defaultdict(int)\nstopwords = set(STOPWORDS)\nmore_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown', '-', '...', '|', '&', '?', '??', 'via', '2'}\nstopwords = stopwords.union(more_stopwords)\nfor sent in train0_df[\"text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of non disaster tweets\", \n                                          \"Frequent words of disaster tweets\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Count Plots\")\npy.iplot(fig, filename='word-plots')","2b83b00a":"## Number of words in the text ##\ntrain_df[\"num_words\"] = train_df[\"text\"].apply(lambda x: len(str(x).split()))\ntest_df[\"num_words\"] = test_df[\"text\"].apply(lambda x: len(str(x).split()))\n\n## Number of unique words in the text ##\ntrain_df[\"num_unique_words\"] = train_df[\"text\"].apply(lambda x: len(set(str(x).split())))\ntest_df[\"num_unique_words\"] = test_df[\"text\"].apply(lambda x: len(set(str(x).split())))\n\n## Number of characters in the text ##\ntrain_df[\"num_chars\"] = train_df[\"text\"].apply(lambda x: len(str(x)))\ntest_df[\"num_chars\"] = test_df[\"text\"].apply(lambda x: len(str(x)))\n\n## Number of stopwords in the text ##\ntrain_df[\"num_stopwords\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\ntest_df[\"num_stopwords\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n## Number of punctuations in the text ##\ntrain_df[\"num_punctuations\"] =train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntest_df[\"num_punctuations\"] =test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n## Number of title case words in the text ##\ntrain_df[\"num_words_upper\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest_df[\"num_words_upper\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n## Number of title case words in the text ##\ntrain_df[\"num_words_title\"] = train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntest_df[\"num_words_title\"] = test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n## Average length of the words in the text ##\ntrain_df[\"mean_word_len\"] = train_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest_df[\"mean_word_len\"] = test_df[\"text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","88922bd5":"## Truncate some extreme values for better visuals ##\ntrain_df['num_words'].loc[train_df['num_words']>60] = 60 #truncation for better visuals\ntrain_df['num_punctuations'].loc[train_df['num_punctuations']>10] = 10 #truncation for better visuals\ntrain_df['num_chars'].loc[train_df['num_chars']>350] = 350 #truncation for better visuals\n\nf, axes = plt.subplots(3, 1, figsize=(10,20))\nsns.boxplot(x='target', y='num_words', data=train_df, ax=axes[0])\naxes[0].set_xlabel('Target', fontsize=12)\naxes[0].set_title(\"Number of words in each class\", fontsize=15)\n\nsns.boxplot(x='target', y='num_chars', data=train_df, ax=axes[1])\naxes[1].set_xlabel('Target', fontsize=12)\naxes[1].set_title(\"Number of characters in each class\", fontsize=15)\n\nsns.boxplot(x='target', y='num_punctuations', data=train_df, ax=axes[2])\naxes[2].set_xlabel('Target', fontsize=12)\n#plt.ylabel('Number of punctuations in text', fontsize=12)\naxes[2].set_title(\"Number of punctuations in each class\", fontsize=15)\nplt.show()","f54b872f":"features = ['num_words', 'num_unique_words', 'num_chars', 'num_stopwords', 'num_punctuations', 'num_words_upper', 'num_words_title', 'mean_word_len']\ntrain_vectors = train_df[features].values\ntest_vectors = train_df[features].values","c1ed57c5":"from sklearn import metrics\ntrain_y = train_df[\"target\"].values\n\ndef runModel(train_X, train_y, test_X, test_y, test_X2):\n    model = linear_model.LogisticRegression(C=5., solver='sag')\n    model.fit(train_X, train_y)\n    pred_test_y = model.predict_proba(test_X)[:,1]\n    pred_test_y2 = model.predict_proba(test_X2)[:,1]\n    return pred_test_y, pred_test_y2, model\n\nprint(\"Building model.\")\ncv_scores = []\npred_full_test = 0\npred_train = np.zeros([train_df.shape[0]])\nkf = model_selection.KFold(n_splits=3, shuffle=True, random_state=2017)\nfor dev_index, val_index in kf.split(train_df):\n    dev_X, val_X = train_vectors[dev_index], train_vectors[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runModel(dev_X, dev_y, val_X, val_y, test_vectors)\n    pred_full_test = pred_full_test + pred_test_y\n    pred_train[val_index] = pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))","7704f661":"cv_scores","5781a0b2":"for thresh in np.arange(0.3, 0.5, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_val_y>thresh).astype(int))))","35bc7fe1":"> words like earthquakes, forest, fire are most frequent and yes are disaster tweets","202f9e8d":"> Again disaster tweets are visibly making sense from bigrams as well","2817811c":"> Classes are almost balanced and good to go.","aaab4a82":"# Check for unbalanced classes","1daffb57":"Bigram Frequency","b91e7b25":"# Analyze Data","62da1da9":"# Look at frequency of words","496cd799":"Install Plotly","e5d236d1":"> Looks like number of punctuations and charactors in disaster tweets are more than that of not. may be more information about the disaster to spread to all.","ac24a18e":"* fire, suicide, police are all relevant for disaster\n* will, new, now, body are having high values for non disaster which is suprising ","d1e2d3f7":"Here we have used only meta informations to build the model ignoring word or text importance as such. ","e789cd05":"> Building Base Model making use of meta informations","da681cdd":"# Conclusion\n\n* We found that meta informations like punctuations and charactor length are more in case of disaster tweets\n* Including meta informations also along with tf-idf vectors may help to improve the model\n* We have tried a combination of tf-idf vectors along with meta information in another notebook [NLP Disaster Predict - shallow ml approaches](https:\/\/www.kaggle.com\/vivekam101\/nlp-disaster-predict-shallow-ml-approaches)\n\n* We will try to get contextual information using transformer models in another notebook. Please upvote if you find it useful.\n","d58c900d":"# Word Frequency plot in each classes","34af4566":"# EDA on meta informations in tweets\n\n![nlp1-cover.jpg](attachment:nlp1-cover.jpg)\n\nYou are predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.\n\nIntroduction and References\nEDA and approach on meta information in tweets. We have tried a combination of tf-idf vectors along with meta information in another notebook [NLP Disaster Predict - shallow ml approaches](https:\/\/www.kaggle.com\/vivekam101\/nlp-disaster-predict-shallow-ml-approaches)\n\nThis kernel includes codes and ideas from kernels below. If this kernel helps you, please upvote their work as well.\n\nReferences: \n[Simple Exploration Notebook - QIQC](https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-qiqc) by [@sudalairajkumar](https:\/\/www.kaggle.com\/sudalairajkumar)\n\n\n# Load libraries","84c32a5c":"# Meta Feature Analysis\n\nNow let us create some meta features and then look at how they are distributed between the classes. The ones that we will create are\n```\nNumber of words in the text\nNumber of unique words in the text\nNumber of characters in the text\nNumber of stopwords\nNumber of punctuations\nNumber of upper case words\nNumber of title case words\nAverage length of the words\n```"}}