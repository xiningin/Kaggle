{"cell_type":{"220a651e":"code","5c4fbbd4":"code","a9671c8d":"code","b7295e4b":"code","ea571056":"code","fe32eea0":"code","2432bfc3":"code","0334bc36":"code","2728c80a":"code","1dc5837b":"code","9fa223fd":"code","bcf9a75b":"code","dd7b018c":"code","1e7c8ee9":"code","63be3045":"code","d0a7520e":"code","4a64d527":"code","8f912453":"code","881f22d5":"code","d2e99c7e":"code","72b623ed":"code","2f2bf179":"code","774b3188":"code","396e12a7":"code","835f2d39":"code","02262b0c":"code","37538acd":"code","b19b7444":"code","48bb940d":"code","e76ffa55":"code","c1f04d2e":"code","0ff53ce7":"code","35ae882a":"code","922c5479":"code","5b580667":"code","e0c803f5":"code","7e68d531":"code","cd7e21dc":"code","f39cab86":"code","5532a68a":"code","8665f9aa":"code","109973c9":"code","3f8bdadb":"code","ca512c1b":"code","074c07c4":"code","853e3ccb":"code","347b2838":"code","9dfbef60":"code","e960b04f":"code","1328fdd6":"code","74dee948":"code","f068cf02":"code","311204bc":"code","4def4b58":"code","5a314c25":"code","1b7ba0e4":"code","3452c3a8":"code","eb0c23e6":"code","7e1cad5e":"code","b52def53":"code","b9bbb632":"code","1348e4a2":"code","fc5f2226":"code","f96958b8":"code","d0c63783":"code","87d96c43":"code","bb03565c":"code","d09e3134":"code","e58021f6":"code","4af6c1db":"code","87c831cd":"code","a3b84ac7":"code","0fb14af9":"code","ec563083":"code","23dbd3c1":"code","fd1d3fcd":"code","e339f873":"code","4bf177ed":"code","04c0b1a3":"code","3a8681e9":"code","a9ba579a":"code","b261c3d7":"code","b74e40e9":"code","5900113e":"code","a5b5bd52":"code","5dc7abd7":"code","fe0639e8":"code","f9842da2":"code","b722f0aa":"code","b6a34cf1":"code","41928adb":"code","39bbd429":"code","7db7b3f4":"code","099266f9":"code","eb0fd479":"code","142e13af":"code","eb219101":"code","3f07e1a2":"code","32650e8e":"code","12d67790":"code","81ed833e":"code","79d0eb9e":"code","8ef31c01":"code","de518980":"code","0a046135":"code","7db27868":"code","41e8fd42":"code","b9513608":"code","b7c882e5":"code","0bc35682":"code","e992805c":"code","50c82e37":"code","551f223b":"code","e64b40c9":"code","3e5e01db":"markdown","0404b658":"markdown","509fb01f":"markdown","59be83de":"markdown","8ac5a11f":"markdown","da171e45":"markdown","5d5141bc":"markdown","e2843408":"markdown","1758e10d":"markdown","a071f2c8":"markdown","bfe99673":"markdown","f5dc8bfe":"markdown","a805dea2":"markdown","7bde9004":"markdown","c434dbd5":"markdown","a66df3bb":"markdown","c2ec5192":"markdown","d5748c24":"markdown","392c3977":"markdown","631c3502":"markdown","31b1e151":"markdown","28f9438d":"markdown","9710e767":"markdown","34a393d3":"markdown","5591e4ea":"markdown","05fca864":"markdown","4f8db4d4":"markdown","7e48ae66":"markdown","3e3ad50e":"markdown","aea9ecd4":"markdown","e814b4e2":"markdown","35c9226e":"markdown","74170e52":"markdown","5eef4985":"markdown","72773d0b":"markdown","55db32a8":"markdown","20a7d6f0":"markdown","075e1089":"markdown","f24c8a3b":"markdown","2687a399":"markdown","3d878156":"markdown","7b31b01c":"markdown","269074af":"markdown","b85107aa":"markdown","9de898fe":"markdown","554154b4":"markdown","2978edad":"markdown","731c283e":"markdown","c62bc7c4":"markdown","be1d79c6":"markdown","0dc3440b":"markdown","9f02f863":"markdown","f01c0793":"markdown","e093db81":"markdown","913b0306":"markdown","b5a87ff6":"markdown","a1d08ac7":"markdown","dcd70073":"markdown","456f48f6":"markdown","a5b5ad87":"markdown","6a5e2980":"markdown","f1b57c36":"markdown","01956a5d":"markdown","d754d503":"markdown","c703faf9":"markdown","b2234852":"markdown","f295fc60":"markdown","b9e1b729":"markdown","93940eab":"markdown","8c2b32fd":"markdown","7dda2d3e":"markdown"},"source":{"220a651e":"# linear algebra \nimport numpy as np\n\n# data processing \nimport pandas as pd\n\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style \n\n# machine learning algorithms \nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB","5c4fbbd4":"train_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","a9671c8d":"train_df.info()","b7295e4b":"train_df.describe()","ea571056":"train_df.columns","fe32eea0":"test_df.columns # same columns, but does not include \"Survived\"","2432bfc3":"train_df.head(8)","0334bc36":"# shows us which columns are missing data and how much\nmissing = train_df.isnull().sum().sort_values(ascending=False) # ascending=False (Top > Bottom)\nmissing","2728c80a":"women = train_df[train_df[\"Sex\"] == \"female\"]\nmen = train_df[train_df[\"Sex\"] == \"male\"]\n\na = len(women[women[\"Survived\"] == 1])\nb = len(men[men[\"Survived\"] == 1])\na, b # twice as many women survived as men - \"Sex\" is clearly important","1dc5837b":"women.describe() # 74% of women survived","9fa223fd":"men.describe() # only 18% of men survived","bcf9a75b":"g = sns.relplot(x=\"Age\", y=\"Survived\", kind=\"line\", data=men)\ng.fig.autofmt_xdate()\n\"\"\"\n- :func:`scatterplot` (with ``kind=\"scatter\"``; the default)\n- :func:`lineplot` (with ``kind=\"line\"``)\n\"\"\"","dd7b018c":"h = sns.relplot(x=\"Age\", y=\"Survived\", kind=\"line\", data=women)\nh.fig.autofmt_xdate()","1e7c8ee9":"sns.barplot(x=\"Sex\", y=\"Survived\", data=train_df)","63be3045":"train_df.columns","d0a7520e":"train_df.Embarked.unique() # Oh right, the 2 NaN values","4a64d527":"S_total = train_df[train_df[\"Embarked\"] == \"S\"]\nC_total = train_df[train_df[\"Embarked\"] == \"C\"]\nQ_total = train_df[train_df[\"Embarked\"] == \"Q\"]\nlen(S_total), len(C_total), len(Q_total) # most people embarked from S, so most will die from S. Let's check the ratios, tho","8f912453":"S_live = len(S_total[S_total[\"Survived\"] == 1])\nC_live = len(C_total[C_total[\"Survived\"] == 1])\nQ_live = len(Q_total[Q_total[\"Survived\"] == 1])\n\nS_live, C_live, Q_live","881f22d5":"round((S_live \/ len(S_total)), (3)), round((C_live \/ len(C_total)), (3)), round((Q_live \/ len(Q_total)), (3))","d2e99c7e":"sns.barplot(x=\"Embarked\", y=\"Survived\", data=train_df)","72b623ed":"train_df.columns","2f2bf179":"sns.barplot(x=\"Pclass\", y=\"Survived\", data=train_df)","774b3188":"train_df.Pclass.unique()","396e12a7":"w_survived = women[women[\"Survived\"] == 1]\nm_survived = men[men[\"Survived\"] == 1]","835f2d39":"visualito =  sns.factorplot(x=\"Pclass\", y=\"Survived\", data=train_df, aspect=2)","02262b0c":"women_Pclass_3 = len(women[women[\"Pclass\"] == 3])\nmen_Pclass_3 = len(men[men[\"Pclass\"] == 3])\n\nws_Pclass_3 = len(w_survived[w_survived[\"Pclass\"] == 3])\nms_Pclass_3 = len(m_survived[m_survived[\"Pclass\"] == 3])\n\nprint(\"CLASS 3 TOTAL:\", women_Pclass_3, men_Pclass_3, \"women:men\")\nprint(\"SURVIVED:\", ws_Pclass_3, ms_Pclass_3, \"women:men\")","37538acd":"women_Pclass_2 = len(women[women[\"Pclass\"] == 2])\nmen_Pclass_2 = len(men[men[\"Pclass\"] == 2])\n\nws_Pclass_2 = len(w_survived[w_survived[\"Pclass\"] == 2])\nms_Pclass_2 = len(m_survived[m_survived[\"Pclass\"] == 2])\n\nprint(\"CLASS 2 TOTAL:\", women_Pclass_2, men_Pclass_2, \"women:men\")\nprint(\"SURVIVED:\", ws_Pclass_2, ms_Pclass_2, \"women:men\")","b19b7444":"women_Pclass_1 = len(women[women[\"Pclass\"] == 1])\nmen_Pclass_1 = len(men[men[\"Pclass\"] == 1])\n\nws_Pclass_1 = len(w_survived[w_survived[\"Pclass\"] == 1])\nms_Pclass_1 = len(m_survived[m_survived[\"Pclass\"] == 1])\n\nprint(\"CLASS 1 TOTAL:\", women_Pclass_1, men_Pclass_1, \"women:men\")\nprint(\"SURVIVED:\", ws_Pclass_1, ms_Pclass_1, \"women:men\")","48bb940d":"train_df.columns","e76ffa55":"train_df.SibSp.unique() # of siblings \/ spouses aboard","c1f04d2e":"train_df.Parch.unique() # of parents \/ children aboard","0ff53ce7":"# creating \"Relatives\" column\ntrain_df[\"Relatives\"] = train_df[\"SibSp\"] + train_df[\"Parch\"]\ntrain_df.columns","35ae882a":"train_df = train_df.drop([\"SibSp\", \"Parch\"], axis = 1)","922c5479":"test_df[\"Relatives\"] = test_df[\"SibSp\"] + test_df[\"Parch\"]\ntest_df.columns","5b580667":"test_df = test_df.drop([\"SibSp\", \"Parch\"], axis = 1)","e0c803f5":"train_df.info() # perfect, 891 values for \"Relatives\"","7e68d531":"visual =  sns.factorplot(x=\"Relatives\",y=\"Survived\", data=train_df)","cd7e21dc":"visual =  sns.factorplot(x=\"Relatives\",y=\"Survived\", data=train_df, aspect=2)","f39cab86":"print(train_df.PassengerId.unique()[0:20], \"etc...\")","5532a68a":"train_df.columns","8665f9aa":"train_df = train_df.drop(['PassengerId'], axis=1) # removing 'PassengerId' from dataset","109973c9":"train_df.columns","3f8bdadb":"train_df.describe()","ca512c1b":"missing","074c07c4":"train_df.Cabin","853e3ccb":"train_df = train_df.drop(['Cabin'], axis=1) # removing 'Cabin' from dataset","347b2838":"test_df = test_df.drop(['Cabin'], axis=1) # removing 'Cabin' from dataset","9dfbef60":"train_df.columns","e960b04f":"test_df.columns","1328fdd6":"missing_updated = train_df.isnull().sum().sort_values(ascending=False) # ascending=False (Top > Bottom)\nmissing_updated","74dee948":"train_df.Age.describe()","f068cf02":"mean = train_df[\"Age\"].mean()\nstd = train_df[\"Age\"].std() # how spread out our data is (measure of variation)\nnansum = train_df[\"Age\"].isnull().sum()\n\nmean, std, nansum","311204bc":"np_test = np.random.randint(90, 100, size=10) # computes an array of 10 random integers between 90 and 100\nnp_test # this was a test","4def4b58":"# mean = train_df[\"Age\"].mean()\n# std = train_df[\"Age\"].std() # how spread out our data is (measure of variation)\n# nansum = train_df[\"Age\"].isnull().sum()\n\nrand_age = np.random.randint(mean - std, mean + std, size = nansum)\nrand_age","5a314c25":"# code used to substitute the rand_age values in for the NaN values\ntrain_df.loc[train_df.Age.isnull(), \"Age\"] = rand_age ","1b7ba0e4":"train_df.Age.describe()","3452c3a8":"train_df.Age.isnull().sum() # should equal 0","eb0c23e6":"train_df.describe()","7e1cad5e":"test_df.describe()","b52def53":"mean = test_df[\"Age\"].mean()\nstd = test_df[\"Age\"].std() # how spread out our data is (measure of variation)\nnansum = test_df[\"Age\"].isnull().sum()\n\nrand_age = np.random.randint(mean - std, mean + std, size = nansum)\nrand_age","b9bbb632":"test_df.loc[test_df.Age.isnull(), \"Age\"] = rand_age ","1348e4a2":"test_df.Age.isnull().sum()","fc5f2226":"test_df.describe()","f96958b8":"test_df.Fare.isnull().sum()","d0c63783":"train_df[\"Age\"] = train_df[\"Age\"].astype(int) # converting \"Age\"column to integer values\ntest_df[\"Age\"] = test_df[\"Age\"].astype(int)","87d96c43":"train_df[\"Embarked\"].describe()","bb03565c":"train_df.loc[train_df.Embarked.isnull(), \"Embarked\"] = \"S\"","d09e3134":"train_df[\"Embarked\"].isnull().sum()","e58021f6":"test_df.Embarked.describe()","4af6c1db":"test_df.loc[test_df.Embarked.isnull(), \"Embarked\"] = \"S\"","87c831cd":"test_df[\"Embarked\"].isnull().sum()","a3b84ac7":"test_df.isnull().sum()","0fb14af9":"test_df[\"Fare\"].describe()","ec563083":"test_df.loc[test_df.Fare.isnull(), \"Fare\"] = 35.627188","23dbd3c1":"test_df.isnull().sum()","fd1d3fcd":"train_df.info()","e339f873":"train_df[\"Fare\"] = train_df[\"Fare\"].astype(int)\ntest_df[\"Fare\"] = test_df[\"Fare\"].astype(int)\ntest_df.info()","4bf177ed":"train_df[\"Name\"].head(10)","04c0b1a3":"# extract titles\ntrain_df[\"Title\"] = train_df.Name.str.extract(\" ([A-Za-z]+)\\.\", expand=False)\ntrain_df[\"Title\"].describe()","3a8681e9":"# TRAINING DATA #\n\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\n# extract titles\ntrain_df['Title'] = train_df.Name.str.extract(\" ([A-Za-z]+)\\.\", expand=False)\n\n# replace titles with a more common title or as Rare\ntrain_df['Title'] = train_df['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\n                                              'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntrain_df['Title'] = train_df['Title'].replace('Mlle', 'Miss')\ntrain_df['Title'] = train_df['Title'].replace('Ms', 'Miss')\ntrain_df['Title'] = train_df['Title'].replace('Mme', 'Mrs')\n\n# convert titles into numbers\n# .map() used for substituting each value in a Series with another value from a dict \ntrain_df['Title'] = train_df['Title'].map(titles)\n\n# filling NaN with 0 (to be safe)\ntrain_df['Title'] = train_df['Title'].fillna(0)\ntrain_df.columns","a9ba579a":"train_df[\"Title\"] = train_df[\"Title\"].astype(int)\ntrain_df[\"Title\"]","b261c3d7":"# TESTING DATA #\n\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\n# extract titles\ntest_df['Title'] = test_df.Name.str.extract(\" ([A-Za-z]+)\\.\", expand=False)\n\n# replace titles with a more common title or as Rare\ntest_df['Title'] = test_df['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\n                                              'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntest_df['Title'] = test_df['Title'].replace('Mlle', 'Miss')\ntest_df['Title'] = test_df['Title'].replace('Ms', 'Miss')\ntest_df['Title'] = test_df['Title'].replace('Mme', 'Mrs')\n\n# convert titles into numbers\n# .map() used for substituting each value in a Series with another value from a dict \ntest_df['Title'] = test_df['Title'].map(titles)\n\n# filling NaN with 0 (to be safe)\ntest_df['Title'] = test_df['Title'].fillna(0)\ntest_df.columns","b74e40e9":"test_df[\"Title\"] = test_df[\"Title\"].astype(int)\ntest_df[\"Title\"]","5900113e":"genders = {\"male\": 0, \"female\": 1}\ntrain_df[\"Sex\"] = train_df[\"Sex\"].map(genders)\ntest_df[\"Sex\"] = test_df[\"Sex\"].map(genders)","a5b5bd52":"train_df.Ticket.describe()","5dc7abd7":"train_df = train_df.drop([\"Ticket\"], axis=1)\ntest_df = test_df.drop([\"Ticket\"], axis=1)","fe0639e8":"train_df.columns","f9842da2":"test_df.columns","b722f0aa":"num_ports = {\"S\": 0, \"C\": 1, \"Q\": 2}\ntrain_df[\"Embarked\"] = train_df[\"Embarked\"].map(num_ports)\ntest_df[\"Embarked\"] = test_df[\"Embarked\"].map(num_ports)","b6a34cf1":"train_df[\"Embarked\"].unique()","41928adb":"test_df[\"Embarked\"].unique()","39bbd429":"train_df.info()","7db7b3f4":"train_df = train_df.drop([\"Name\"], axis=1)\ntest_df = test_df.drop([\"Name\"], axis=1)","099266f9":"train_df.info()","eb0fd479":"test_df.info()","142e13af":"# train_df minus the \"Survived\" feature\nX_train = train_df.drop(\"Survived\", axis=1) # axis=1 tells us that we want to drop a column\n\n# only \"Survived\" (the target variable) from train_df\nY_train = train_df[\"Survived\"] \n\n# copy of test_df without \"PassengerId -> BECAUSE we removed \"PassengerId\" from train_df\n# because, as we said before, we don't need to be testing with \"PassengerId\"\nX_test = test_df.drop(\"PassengerId\", axis=1).copy()\n\n# X_train without \"Survived\" and X_test without \"PassengerId\" are practically the same content","eb219101":"# max_iter = The maximum number of passes over the training data (aka epochs)'\n# tol = the stopping criterion\n\nsgd = linear_model.SGDClassifier(max_iter=5, tol=None)\nsgd.fit(X_train, Y_train)\nY_prediction = sgd.predict(X_test) # what we would submit... printing Y_pred gives us all the 0s and 1s\n\nprint(sgd.score(X_train, Y_train))\n\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nprint(acc_sgd)\n\nprint(Y_prediction[0:10])","3f07e1a2":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\n\nY_prediction = random_forest.predict(X_test)\n\nprint(random_forest.score(X_train, Y_train))\n\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(acc_random_forest)\n\nprint(Y_prediction[0:10])","32650e8e":"logreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\n\nY_prediction = logreg.predict(X_test)\n\nprint(logreg.score(X_train, Y_train))\n\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nprint(acc_log)\n\nprint(Y_prediction[0:10]) # just like random forest model, little flexibility ","12d67790":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\n\nY_prediction = knn.predict(X_test)\n\nprint(knn.score(X_train, Y_train))\n\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nprint(acc_knn)\n\nprint(Y_prediction[0:10]) # consistently hits 84.4","81ed833e":"gaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\n\nY_prediction = gaussian.predict(X_test)\n\nprint(gaussian.score(X_train, Y_train))\n\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nprint(acc_gaussian)\n\nprint(Y_prediction[0:10]) # stable, not jumping around","79d0eb9e":"perceptron = Perceptron(max_iter=4)\nperceptron.fit(X_train, Y_train)\n\nY_prediction = perceptron.predict(X_test)\n\nprint(perceptron.score(X_train, Y_train))\n\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nprint(acc_perceptron)\n\nprint(Y_prediction[0:10]) # stable response","8ef31c01":"linear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\n\nY_prediction = linear_svc.predict(X_test)\n\nprint(linear_svc.score(X_train, Y_train))\n\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nprint(acc_linear_svc)\n\nprint(Y_prediction[0:10])","de518980":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\n\nY_prediction = decision_tree.predict(X_test)\n\nprint(decision_tree.score(X_train, Y_train))\n\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nprint(acc_decision_tree)\n\nprint(Y_prediction[0:10])","0a046135":"# let's make a dataframe and find out\nresults = pd.DataFrame({\n    \"Model\": [\"Stochastic Gradient Descent\", \"Random Forest\", \"Logistic Regression\", \n              \"K Nearest Neighbor\", \"Gaussian Naive Bayes\", \"Perceptron\",\n              \"Linear Support Vector Machine\", \"Decision Tree\"], \n    \"Score\": [acc_sgd, acc_random_forest, acc_log, acc_knn, acc_gaussian, \n              acc_perceptron, acc_linear_svc, acc_decision_tree]})\nresults","7db27868":"results_df = results.sort_values(by=\"Score\", ascending=False)\nresults_df","41e8fd42":"results_df.plot.bar(x=\"Model\", y=\"Score\")","b9513608":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\n\nY_prediction_rf = random_forest.predict(X_test)\n\nprint(random_forest.score(X_train, Y_train))\n\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(acc_random_forest)\n\nprint(Y_prediction_rf[0:10])","b7c882e5":"import os\nos.remove(\"\/kaggle\/working\/wowsubmission.csv\") # removed incorrect submission","0bc35682":"output = pd.DataFrame({\"PassengerId\": test_df.PassengerId, \"Survived\": Y_prediction_rf})\noutput.to_csv(\"goodsubmission.csv\", index=False)\nprint(\"Your submission was successfully saved!\")","e992805c":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_prediction_rf})","50c82e37":"submission","551f223b":"submission.to_csv(\"wowsubmission.csv\", index=False)","e64b40c9":"predictions = random_forest.predict(X_test)\npred_list = [int(x) for x in predictions]\n\ntest2 = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\noutput = pd.DataFrame({'PassengerId': test2['PassengerId'], 'Survived': pred_list})\noutput.to_csv('Titanic_with_logistic.csv', index=False)","3e5e01db":"# File Submission","0404b658":"Simple: let's convert this to numeric.","509fb01f":"Wow, 681 unique values. That's tough, and with that much unique data, it's easier just to drop.\n\nLet's remove \"Ticket\" from both datasets.","59be83de":"A big part of machine learning is **classification** \u2014 we want to know what class (a.k.a. group) an observation belongs to. The ability to precisely classify observations is extremely valuable for various **business applications** like **predicting** whether a particular user will buy a product or forecasting whether a given loan will default or not.\n\nData science provides a plethora of classification algorithms such as logistic regression, support vector machine, naive Bayes classifier, and decision trees. But near the top of the classifier hierarchy is the **random forest classifier**. \n\nDecision trees are the building blocks of the random forest model. Note: **they are very sensitive to changes in the dataset**. Branching: \n\n*What feature will allow me to split the observations at hand in a way that the resulting groups are as different from each other as possible (and the members of each resulting subgroup are as similar to each other as possible)?*\n\nRandom forest, like its name implies, consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model\u2019s prediction.\n\nThe logic: *A large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models.*\n\nThe low correlation between models is the key - The reason for this wonderful effect is that the trees protect each other from their individual errors. While some trees may be wrong, many other trees will be right, so as a group the trees are able to move in the correct direction. So, it is important that the **predictions (and therefore the errors) made by the individual trees need to have low correlations with each other**.\n\nWhy **random**? Each tree in a random forest can pick only from a random subset of features. This forces even more variation amongst the trees in the model and ultimately results in lower correlation across trees and more diversification.\n\nTL;DR:\n\n**The random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree**.\n\nTo make accurate predictions we need: \n- features that have at least some predictive power\n- trees of the forest and more importantly their predictions need to be uncorrelated (or at least have low correlations with each other)","8ac5a11f":"Converting \u201cFare\u201d from float to int64, using the \u201castype()\u201d function pandas provides:","da171e45":"Again, only 1 missing value (in only the test dataset), so let's just fill it in with the mean","5d5141bc":"The k-nearest neighbors (KNN) algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to **solve both classification and regression problems**.\n\nA **supervised machine learning algorithm** is one that relies on labeled input data to learn a function that produces an appropriate output when given new unlabeled data.\n\n*Imagine a computer is a child, we are its supervisor (e.g. parent, guardian, or teacher), and we want the child (computer) to learn what a pig looks like. We will show the child several different pictures, some of which are pigs and the rest could be pictures of anything (cats, dogs, etc). When we see a pig, we shout \u201cpig!\u201d When it\u2019s not a pig, we shout \u201cno, not pig!\u201d After doing this several times with the child, we show them a picture and ask \u201cpig?\u201d and they will correctly (most of the time) say \u201cpig!\u201d or \u201cno, not pig!\u201d depending on what the picture is. That is supervised machine learning.*\n\nA **classification problem** has a discrete value as its output. For example, \u201clikes pineapple on pizza\u201d and \u201cdoes not like pineapple on pizza\u201d are discrete. There is no middle ground. The analogy above of teaching a child to identify a pig is another example of a classification problem.\n\nA **regression problem** has a real number (a number with a decimal point) as its output. For example, we could estimate someone\u2019s weight given their height.\n\n**The KNN algorithm assumes that similar things exist in close proximity; calculating the distance between points on a graph**.\n\n**It is important to choose the right value for K:** To select the K that\u2019s right for your data, we run the KNN algorithm several times with different values of K and choose the K that reduces the number of errors we encounter while maintaining the algorithm\u2019s ability to accurately make predictions when it\u2019s given data it hasn\u2019t seen before.\n\n**Selecting K**:\n- As we decrease the value of K to 1, our predictions become less stable\n- Inversely, as we increase the value of K, our predictions become more stable due to majority voting \/ averaging, and thus, more likely to make more accurate predictions (up to a certain point)\n- make K an odd number (tiebreaker)\n\n**Advantages of KNN**:\n- The algorithm is simple and easy to implement\n- There\u2019s no need to build a model, tune several parameters, or make additional assumptions\n- The algorithm is versatile. It can be used for classification and regression\n\n**Disadvantages of KNN**:\n- model can get slow with addition of more independent variables","e2843408":"Clearly, people with 1-3 relatives had a high chance of survival... and sometimes 6.","1758e10d":"### Pclass","a071f2c8":"Now, we want to compute random numbers between the mean and std, for the amount of NaN values. ","bfe99673":"## K Nearest Neighbor:","f5dc8bfe":"We have lots of weird, pesky names. \n\nWe are going to group them up into categories...\n\nThen replace them with a corresponding value!","a805dea2":"# Data Exploration\/Analysis","7bde9004":"Gradient Descent is a very popular optimization technique in Machine Learning and Deep Learning and it can be used with most, if not all, of the learning algorithms. \n\nA gradient is basically the slope of a function; the more the gradient, the steeper the slope. So gradient descent literally means descending a slope to reach the lowest point on that surface.\n\n\u201cGradient descent is an iterative algorithm, that starts from a random point on a function and travels down its slope in steps until it reaches the lowest point of that function.\u201d\n \nThe word *stochastic* means a system or a process that is linked with a random probability. Hence, in **Stochastic Gradient Descent**, a few samples are selected randomly instead of the whole data set for each iteration.\n\n**Advantages of SGD:**\n\n- Efficiency (it uses only a single example (a batch size of 1) per iteration)\n\n- Ease of implementation (lots of opportunities for code tuning)\n\n**Disadvantages of SGD:** \n\n- SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations\n\n- sensitive to feature scaling","c434dbd5":"## Analyzing Columns","a66df3bb":"Embarked feature only has 2 missing values, we will just fill these with the mean.","c2ec5192":"### Embarked","d5748c24":"Support vector machine is highly preferred by many as it **produces significant accuracy with less computation power**. Support Vector Machine, abbreviated as SVM can be **used for both regression and classification tasks**. But, it is widely used in classification objectives.\n\nThe objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N \u2014 the number of features) that distinctly classifies the data points.\n\nIn the SVM algorithm, we are looking to maximize the margin between the data points and the hyperplane. The loss function that helps maximize the margin is hinge loss.\n\nThe cost is 0 if the predicted value and the actual value are of the same sign. If they are not, we then calculate the loss value.","392c3977":"OH right. Time to remove name.","631c3502":"## Decision Tree","31b1e151":"**IMPORTANT: always look at the example submission file first. While I originally thought to remove \"PassengerId\" from test_df, I realized that it is a necessary part of the submission. SO, do not!**","28f9438d":"In our **Data Exploration\/Analysis**, we saw that many columns play a role in survival. \n\nPassengerId, on the other hand, seems pointless. Let's take a look:","9710e767":"Now let's do the same for the test_df","34a393d3":"## Stochastic Gradient Descent (SGD):","5591e4ea":"### Fare","05fca864":"### Embarked","4f8db4d4":"## Random Forest:","7e48ae66":"### Embarked","3e3ad50e":"As you can see:\n\n34% survived from S (total = 644)\n\n55% survived from C (total = 168)\n\n39% survived from Q (total = 177)\n","aea9ecd4":"We need to make all values numeric so our machine learning models can process them \n\nWe also need to take care of missing data (NaN values)\n\n**Let's take a look at what exactly is missing**","e814b4e2":"We can see that the passengers embarked from 3 ports\n\nLet's see if these ports have any correlation with survival","35c9226e":"We can see that our Random Forest Model worked the best: \nLet's run it again to get our best Y_prediction and submit our results!","74170e52":"**train_df has 11 features + the target variable (\"Survived\")**\n\nPassengerId: Unique Id of a passenger\n\nSurvived: Survival \n\nPclass: Ticket class\n\nName: Name\n\nSex: Sex\n\nAge: Age in years\n\nSibSp: # of siblings \/ spouses aboard\n\nParch: # of parents \/ children aboard\n\nTicket: Ticket number\n\nFare: Passenger fare\n\nCabin: Cabin number\n\nEmbarked: Port of embarkation","5eef4985":"\"Age\" still has 177 missing values. I don't want to remove \"Age\" because as we observed, has a strong correlation to survival.","72773d0b":"### SibSp\/Parch","55db32a8":"**From these simple functions, we can tell that age and sex are important factors**","20a7d6f0":"**Logistic Regression is used when the dependent variable(target) is categorical** (which applies here). *Linear regression*, on the other hand, is not suitable for classification problems. \n\nFor example: \n- to predict whether an email is spam (1) or (0) \n- to predict whether a tumor is malignant (1) or not (0)\n\nData is fit into linear regression model, which then be acted upon by a logistic function predicting the target categorical dependent variable, hence the name.\n\nCan be used for traditional statistics and machine learning. \n\nLogistic regression is like predicting whether something is true or false, not for something continuous like size. \n\nInstead of fitting a line to the data, logistic regression fits an \"S\" shape logistic function. The curve goes from 0 to 1. \n\nFor example, if a mouse's weight is > 50%, then it will classify it as *obese*. If not, the it will classify it as *not obese*. (example in which obesity is predicted by weight)","075e1089":"Some features contain missing values such as \"Age\"","f24c8a3b":"**Now our features are ready for the machine learning models as ALL values have been converted to integer type.**","2687a399":"**Clearly, Pclass is a relevant feature.**","3d878156":"As a reminder, we have to deal with Cabin (687), Embarked (2) and Age (177). ","7b31b01c":"### Fare","269074af":"The **top of the tree** (or bottom depending on how you look at it) is called the **root node**. **Intermediate nodes** have arrows pointing to and away from them. Finally, the **nodes at the bottom** of the tree without any edges pointing away from them are **called leaves**. Leaves **tell you what class each sample belongs to**.\n\nThe scikit-learn implementation of the DecisionTreeClassifer uses the minimum impurity decrease to determine whether a node should be split.\n\n**Decision Trees are easy to interpret, don\u2019t require any normalization, and can be applied to both regression and classification problems**. Unfortunately, Decision Trees are seldom used in practice because they don\u2019t generalize well.\n\nAs we discussed above, **Random Forest** combines multiple Decision Trees to achieve better accuracy.","b85107aa":"# Getting Data","9de898fe":"### Name","554154b4":"### Ticket","2978edad":"## Perceptron:","731c283e":"### Age","c62bc7c4":"### Sex","be1d79c6":"# Importing Libraries","0dc3440b":"## Gaussian Naive Bayes: ","9f02f863":"**Okay, I believe X_train and X_test need to have the exact same features.**","f01c0793":"# Now, which ML model worked the best?","e093db81":"### Cabin","913b0306":"# Conclusion","b5a87ff6":"Unlike what we see with Stochastic Gradient Descent, this Random Forest Model gives us the same high result each time!","a1d08ac7":"**Perceptron is a single layer neural network**. \n\nThe Perceptron Model takes an input, aggregates it (weighted sum) and returns 1 only if the aggregated sum is more than some threshold else returns 0. Our goal is to find the w vector that can perfectly classify positive inputs and negative inputs in our data.\n\n**The perceptron consists of 4 parts**.\n- Input values or One input layer\n- Weights and Bias\n- Net sum\n- Activation Function\n\n**Perceptron is usually used to classify the data into two parts**. Therefore, it is also known as a Linear Binary Classifier.","dcd70073":"Almost all women from Pclass 1 & 2 survived, however half died from Pclass 3. \n\nIn Pclasses 1, 2, and 3, the vast majority of men died. \n\nPclass 1 saw the highest survivor ratio.","456f48f6":"## We still have to deal with missing data","a5b5ad87":"Naive Bayes is the most straightforward and fast classification algorithm, which is **suitable for large datasets**. Naive Bayes classifier is successfully used in various applications such as **spam filtering, text classification, sentiment analysis, and recommender systems**. It uses Bayes theorem of probability for prediction of unknown class.\n\n**Naive Bayes classifier assumes that the effect of a particular feature in a class is independent of other features**. For example, a loan applicant is desirable or not depending on his\/her income, previous loan and transaction history, age, and location. Even if these features are interdependent, these features are still considered independently. This assumption simplifies computation, and that's why it is considered as naive. \n\n**Overall, a very simple, easy, and effective model**. ","6a5e2980":"### Sex\/Age","f1b57c36":"# Converting Features","01956a5d":"**REMINDER: ADD PICTURES TO THE MACHINE LEARNING MODEL DESCRIPTIONS!**","d754d503":"Now we will train several Machine Learning models and compare their results. \n\nNote that because the dataset does not provide labels for their testing-set, we need to use the predictions on the training set to compare the algorithms with each other. \n\nLater on, we will use cross validation.","c703faf9":"Convert \"Sex\" feature to numeric values.","b2234852":"# Data Processing ","f295fc60":"Cabin seems pointless too, and so much is missing. Let's drop it from both our datasets.","b9e1b729":"# Building Machine Learning Models","93940eab":"These would make more sense as a combined feature, which shows the total number of relatives a person has on the Titanic.\n\nLet's combine them and add said feature to both our train_df and test_df","8c2b32fd":"## Logistic Regression: ","7dda2d3e":"## Linear Support Vector Machine:"}}