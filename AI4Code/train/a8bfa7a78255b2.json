{"cell_type":{"432c0c28":"code","d5d88b59":"code","40e0c79d":"code","9501c5e5":"code","4be91dc9":"code","a4e18b35":"code","2ead0c01":"code","d30ad7db":"code","281f14b6":"code","79d4c573":"code","f86f24da":"code","f4668451":"code","dfd908b1":"code","b1a66ec7":"code","aaf2875b":"code","d9dcf828":"code","eac3eaa6":"code","9337f21e":"code","c5fbc57f":"code","0a700c31":"code","2278214e":"code","97096c29":"code","62ffa693":"markdown","90c41fe5":"markdown","430b3749":"markdown","90b2fe78":"markdown","9b815770":"markdown"},"source":{"432c0c28":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.svm import SVR\nfrom sklearn.multioutput import MultiOutputRegressor\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d5d88b59":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jul-2021\/train.csv',parse_dates=['date_time'])\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jul-2021\/test.csv')\nsample = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jul-2021\/sample_submission.csv')\n","40e0c79d":"train.describe()","9501c5e5":"targets = sample.columns[sample.columns.str.contains('target')].tolist()\ninputs = test.columns.tolist()\n","4be91dc9":"train['day_name'] = train['date_time'].apply(lambda x: x.dayofweek)\ntrain['hour'] = train['date_time'].apply(lambda x: x.hour)\ntrain['day_hour'] = train['day_name']*24 + train['hour']","a4e18b35":"sns.pairplot(train[['hour'] + targets])","2ead0c01":"fig, ax = plt.subplots(3,1,figsize=(20,20))\nax[0].hist(train['target_carbon_monoxide'],bins=20)\nax[1].hist(train['target_benzene'],bins=20)\nax[2].hist(train['target_nitrogen_oxides'],bins=20)","d30ad7db":"sns.pairplot(train)","281f14b6":"features = ['sensor_1', 'sensor_2', 'sensor_3', 'sensor_4']\nX = train[features]\ny = train[targets]","79d4c573":"from skmultilearn.model_selection import iterative_train_test_split\n\nX_train, y_train, X_test, y_test = iterative_train_test_split(X.to_numpy(), y.to_numpy(), test_size = 0.8)","f86f24da":"# Scale everything\nxScaler = MinMaxScaler(feature_range=(1,2))\nyScaler = MinMaxScaler(feature_range=(1,2))\n\nnewX = xScaler.fit_transform(X_train)\nnewY = yScaler.fit_transform(y_train)","f4668451":"from sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import fbeta_score, make_scorer\n\nmsle_scorer = make_scorer(mean_squared_log_error,greater_is_better=False)\n\nparameters = [{'kernel': ['rbf'], 'C': [1, 10, 100, 1000]},\n              {'kernel': ['sigmoid'], 'C': [1, 10, 100, 1000]}]\n\ngrid_parameters = [{'estimator__kernel': ['rbf'], 'estimator__C': [1, 10, 100, 1000]},\n              {'estimator__kernel': ['sigmoid'], 'estimator__C': [1, 10, 100, 1000]}]\n\n","dfd908b1":"# Grid Search Training\nmySVR = SVR()\n\nclf = GridSearchCV(MultiOutputRegressor(mySVR), grid_parameters, scoring=msle_scorer)\n#clf = MultiOutputRegressor(GridSearchCV(SVR(), parameters, scoring=msle_scorer))\n\nclf.fit(X=newX, y=newY)\n\nprint(clf.best_params_)\n\nmeans = clf.cv_results_['mean_test_score']\nstds = clf.cv_results_['std_test_score']\n\nfor mean, std, params in zip(means, stds, clf.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\" % (mean, std * 2, params))","b1a66ec7":"y_pred = yScaler.inverse_transform(clf.predict(xScaler.transform(X_test)))\n\nmean_squared_log_error(y_test,y_pred)","aaf2875b":"# Custom HyperParamater Tuning\n# Scale everything\nxScaler = StandardScaler()\nyScaler = StandardScaler()\n\nnewX = xScaler.fit_transform(X_train)\nnewY = yScaler.fit_transform(y_train)","d9dcf828":"SVR_dict = dict()\nfor param in parameters:\n    for Cparam in param['C']:\n        kernel = param['kernel'][0]\n        newSVR = SVR(kernel=kernel, C=Cparam)\n        custom_reg = MultiOutputRegressor(newSVR)\n        SVR_dict[kernel+'_'+str(Cparam)] = custom_reg\n        custom_reg.fit(newX,newY)\n        \n        y_orig_pred = yScaler.inverse_transform(custom_reg.predict(xScaler.transform(X_test)))\n        y_pred = np.where(y_orig_pred < 0, 0, y_orig_pred)\n        if ((y_test < 0).any() or (y_pred < 0).any()):\n            print(\"Something weird happened:\")\n            print((y_test < 0).any())\n            print((y_pred < 0).any())\n        else:\n            msle = mean_squared_log_error(y_test,y_pred)\n        print(f\"{param['kernel']} with C:{Cparam} MSLE: {msle}\")\n\nprint(\"No tweaks\")\nbase = MultiOutputRegressor(SVR())\nbase.fit(newX,newY)\ny_orig_pred = yScaler.inverse_transform(custom_reg.predict(xScaler.transform(X_test)))\nif ((y_test < 0).any() or (y_pred < 0).any()):\n    print(\"Something weird happened:\")\n    print((y_test < 0).any())\n    print((y_pred < 0).any())\nelse:\n    msle = mean_squared_log_error(y_test,y_pred)\nprint(f\"MSLE: {msle}\")\n","eac3eaa6":"# Debugging\nnewSVR = SVR(kernel='rbf', C=10)\ncustom_reg = MultiOutputRegressor(newSVR)\ncustom_reg.fit(newX,newY)\n        \ny_pred = yScaler.inverse_transform(custom_reg.predict(xScaler.transform(X_test)))\n\n(y_pred < 0).any()\n","9337f21e":"# Best one was RBF with C = 10\nSVR_dict['rbf_10']\n\nclf = SVR_dict['rbf_10']","c5fbc57f":"xScaler = StandardScaler()\nyScaler = StandardScaler()\n\nnewX = xScaler.fit_transform(X)\nnewY = yScaler.fit_transform(y)\n\nnewSVR = SVR(kernel='rbf', C=10)\nclf = MultiOutputRegressor(newSVR)\nclf.fit(newX,newY)\n\n","0a700c31":"\ntestX = xScaler.transform(test[features])\ntestY_orig = yScaler.inverse_transform(clf.predict(testX))\ntestY = np.where(testY_orig < 0, 0, testY_orig)","2278214e":"submission_df = pd.DataFrame(columns = ['date_time'] + targets)\nsubmission_df['date_time'] = test['date_time']\nsubmission_df[targets] = testY","97096c29":"submission_df.to_csv('\/kaggle\/working\/submission.csv',index=False)","62ffa693":"# Ideas\n* Relative Humidity vs Absolute Humidity. Partial pressure of h20, or amount of h20 in the air might be an interesting feature. \n* Fourier Transform some of the data and see if that provides any interesting features\n* Correlation plots vs each of the targets.\n","90c41fe5":"# Predictions","430b3749":"# Training\nStarting with sensors first, since it's a smaller feature set.","90b2fe78":"# Exploratory Data Analysis\n\nUnits for CO and benzen appear to in ppm based on ranges of values given in various cities via the EPA in the 2011-2013 time range. The Nitrogen oxides are typically measured in the 10s of ppb, not ppm. So the numbers may be a bit small to be ppm for nitrogen oxides. There also appears to be a difference in amount of NO in the EPA data between areas with high humidity. Areas with higher humidity appear to have lower amounts of NO.","9b815770":"Ok, this is terrible and I shouldn't do it, but I'm curious to see if it makes my final predictions different. For some reason tuning the hyperparameters made for a *worse* outcome than my initial first pass. Which is super confusing to me. So I'm going to use the tuned hyperparameters with *all* of the original data."}}