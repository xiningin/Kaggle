{"cell_type":{"48841fd9":"code","35c4e7e7":"code","7840d5c1":"code","8dbd895d":"code","b86f6bfd":"code","b98bd0aa":"code","e4c965a2":"code","b474af0a":"code","b4732cc5":"code","7263adab":"code","7652a075":"code","19590531":"code","c1725b43":"code","893397b8":"code","ef54800b":"code","6f892826":"code","5ca7886e":"code","b0b1d209":"code","bbc002b3":"code","3ccd3c6e":"markdown","7da08bd8":"markdown","59eca953":"markdown","c64f6cab":"markdown","0562b35e":"markdown","70e58484":"markdown","84582b04":"markdown","74a07b0b":"markdown","0dce3124":"markdown","4c8a6563":"markdown","4b625b2b":"markdown","8affeb10":"markdown","29309b4d":"markdown","619e2152":"markdown","ef93b701":"markdown"},"source":{"48841fd9":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.optimizers import SGD","35c4e7e7":"# Generate dummy data\nimport numpy as np\ny_train = np.random.randint(2, size=(1000, 1))\nX_train = y_train + (np.random.normal(size=(1000, 1)) \/ 5)\ny_test = np.random.randint(2, size=(100, 1))\nX_test = y_test + np.random.random(size=(100, 1))","7840d5c1":"import matplotlib.pyplot as plt\nplt.scatter(X_train[:, 0], y_train, marker='x', c=y_train)\nplt.axis('off')","8dbd895d":"clf = Sequential()\nclf.add(Dense(2, activation='linear', input_shape=(1,), name='hidden'))\nclf.add(Dense(1, activation='linear', name='out'))\nclf.compile(loss='binary_crossentropy', optimizer=SGD(), metrics=['accuracy'])\n\nclf.fit(X_train, y_train, epochs=10, batch_size=128)","b86f6bfd":"from keras.utils import plot_model\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\n\nSVG(model_to_dot(clf).create(prog='dot', format='svg'))","b98bd0aa":"y_pred = clf.predict(X_test)\ny_pred[:5]","e4c965a2":"y_pred = clf.predict(X_test)\ny_pred = (y_pred > 0.5).astype(int)","b474af0a":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","b4732cc5":"def sample_threeclass(n, ratio=0.8):\n    np.random.seed(42)\n    y_0 = np.random.randint(2, size=(n, 1))\n    switch = (np.random.random(size=(n, 1)) <= ratio)\n    y_1 = ~y_0 & switch\n    y_2 = ~y_0 & ~switch\n    y = np.concatenate([y_0, y_1, y_2], axis=1)\n    \n    X = y_0 + (np.random.normal(size=n) \/ 5)[np.newaxis].T\n    return (X, y)\n\n\nX_train, y_train = sample_threeclass(1000)\nX_test, y_test = sample_threeclass(100)","7263adab":"clf = Sequential()\nclf.add(Dense(3, activation='linear', input_shape=(1,), name='hidden'))\nclf.add(Dense(3, activation='linear', name='out'))\nclf.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])\n\nclf.fit(X_train, y_train, epochs=10, batch_size=128)","7652a075":"clf.predict(X_test)[:5]","19590531":"y_test_pred = np.zeros(shape=y_test.shape)\n\nfor x, y in enumerate(clf.predict(X_test).argmax(axis=1)):\n    y_test_pred[x][y] = 1","c1725b43":"f'Achieved accuracy score of {(y_test_pred == y_test).sum().sum() \/ (y_test.shape[0] * y_test.shape[1])}'","893397b8":"clf = Sequential()\nclf.add(Dense(3, activation='linear', input_shape=(1,), name='hidden'))\nclf.add(Dense(3, activation='softmax', name='out'))\nclf.compile(loss='categorical_crossentropy', optimizer=SGD(), metrics=['accuracy'])\n\nclf.fit(X_train, y_train, epochs=20, batch_size=128)","ef54800b":"y_test_pred = clf.predict(X_test)","6f892826":"y_test_pred[:5]","5ca7886e":"y_test_pred.argmax(axis=1)","b0b1d209":"y_test_pred = np.zeros(shape=y_test.shape)\n\nfor x, y in enumerate(clf.predict(X_test).argmax(axis=1)):\n    y_test_pred[x][y] = 1","bbc002b3":"f'Achieved accuracy score of {(y_test_pred == y_test).sum().sum() \/ (y_test.shape[0] * y_test.shape[1])}'","3ccd3c6e":"## Single-class model\n\nWe'll start with a demonstration of basic `keras` in action. For more on Keras, see the [\"Keras sequential and functional modes\"](https:\/\/www.kaggle.com\/residentmario\/keras-sequential-and-functional-modes) notebook.\n\nIn this section we'll build a simpe single-layer feedforward linear neural network.","7da08bd8":"# Linear and non-linear activation, and softmax\n\n## Introduction\n\nThis notebook is continuation of \"[Implementing a linear neural network](https:\/\/www.kaggle.com\/residentmario\/implementing-a-feedforward-neural-network)\".\n\nEvery node in a neural network (save the output layer and input layers) both emit and recieve **tensors**. The inputs given to the node by the input tensors must be reshaped so that may be shipped off to the output tensor. The reshaping operation that does this is known as the **activation function**.\n\nThe simplest activation function is `linear`. A linear activation is simply a linear transform on the data. The input signals from the tensors that sink into the node are simply added up, and the result of that addition is packaged and sent downstream through the output tensor. A linear activator allows a node to learn linear relationships within the data. However, linear relationships are limited in power, as they can only learn on linear transformations of the dataset. A neural network consisting of linear layers is comparable to a simple linear regression model in power, as it can only discover and interpret signals in the data which are linearly shaped.  But, they are brutally simple, and trivially easy to train, as their derivative when performing backpropagation is a constant value of 1. In other words:\n\n$$f(x) = x \\implies f'(x) = 1$$\n\nTechnically speaking, there *is* one other activation function that is even simpler than linear activation: **binary activation**. We saw this kind of activation function in \"[Implementing a perceptron](https:\/\/www.kaggle.com\/residentmario\/implementing-a-perceptron\/)\". Binary activation is so simple, however, that it isn't useful for anything except for trivial examples, and Keras doesn't even implement it.\n\nIn this notebook I'm going to look at an example application of both linear activation and a non-linear competitor: softmax activation. The example will be based on an artificial dataset.","59eca953":"Here are the results we get:","c64f6cab":"Notice that the resulting values are not `0` and `1` class labels, but numbers somewhere in between. They can even be negative values!\n\nLinear activation doesn't map values directly to classes. In fact, very few activation functions ever do. Most activation functions are functions with long tails that are asymptotically bounded by 0 or 1. So high-confidence predictions will be *close* to those values, but they will never be 0 or 1 exactly. Binary activation is an exception to this rule, because it explicitly codifies `0` and `1` as the only legal target values. And linear activation is an exception to this rule, as demonstrated here, because `f(x) = x` is an unbounded function!\n\nTo generate true class labels using linear activation, we can assign a `0` when `y_pred <= 0.5` and a `1` otherwise.","0562b35e":"As you can see, the values we get now are very easily interpretable. We can see in the second case for example that the network is highly confident that the given value is a class 0 observation. In the first case it is less confident that that the second.\n\nNote that the output of a softmax layer will always sum to 1, and can thus be *interpreted* as a probability. However, doing so is highly suspect when the loss metric we are using doesn't use these probabilities. For example, loss metric I used for this example, cross-entropy, only cares about getting things right or wrong, and not the probabilities assigned to those values by the model. Thus the model has no incentive to gauge realistic probabilities; it only needs to get close enough approximations to pick a winner. We can reliably order these values: e.g. look at `[0.29880288, 0.41953945, 0.2816577 ]` and say \"Class 2 is more likely than Class 1, which is more likely than Class 3\". But we would get into hot water if we tried to say \"Class 2 and Class 3 are almost equally likely\".\n\nThe Naive Bayes algorithm is a non-neural machine learning technique has a similar asterisk in this regard. See my notebook on \"[Probability calibration](https:\/\/www.kaggle.com\/residentmario\/notes-on-classification-probability-calibration\/)\" for some thoughts on techniques you can use to transform an ordered result into a probability that you can rely on.","70e58484":"The hidden layer consists of two nodes using linear activation. The output consists of a single node combining output from the previous two nodes using yet another linear activation function. The end predictions looks something like this:","84582b04":"It looks like this model has no false positives for the 0 class, and no false negatives for the 1 class. An interesting result!","74a07b0b":"## A weakness of  linear activation\n\nLinear activation functions have many weaknesses.\n\nOne is that they are not a great fit for categorical tasks like this one. In the same vein as linear regression models, they do not map very naturally onto the data.\n\nIn this following few cells I build and run a linear model on a slightly more complicated multiclassification problem. In this case `y` consists of three mutually exclusive classes.","0dce3124":"Here is the data we are working with. `y_test` and `y_train` are classes of two clusters of points, one meaned near 1 and the other near 0. `X_test` and `X_train` are the same 0 and 1 values, plus some Gaussian noise for displacement. There is a very small amount of overlap in this artificial dataset.","4c8a6563":"Here's a classification report telling us how well we did:","4b625b2b":"We build a simple `keras` model. Notice the use of `binary_crossentropy` for the loss. Cross-entropy is a recommended loss metric for categorical classification tasks, and the subject of the next notebook.","8affeb10":"## Introducing softmax\n\nSoftmax is a non-linear activation function, and is arguably the simplest of the set.\n\nRecall that the linear activation function is:\n\n$$f(x) = x$$\n\nOk, here's the softmax activation function:\n\n$$y_i = f(x) = \\frac{\\exp{z_i}}{\\sum_{j=0}^n \\exp{z_j}}$$\n\nIn this expression, $z_i$ is the current value. The denominator in the expression is the sum across every value passed to a node in the layer. In other words, the softmax function is dependent not just on the contribution of the current value, but on that of every other value passed to this layer in the network. This is nice because it normalizes the values. The values outputted by nodes in a softmax layer will always sum to 1. When we are performing classification, these values are directly interpretable as probabilities!\n\nThe `exp` terms ($e$ to an exponent) give the resulting values an exponential character. This formula has a few interesting characteristics. Maximum values and values very near maximum are distorted to be even larger after the transformation, at the cost of smaller values, which are made even smaller.\n\nThis effect is further enhanced if you start with large values, which will increase the skew even further. In other words, unlike the linear function the softmax function is *not* scale-invariant. It can help (a lot) to normalize the values in the dataset prior to training.\n\nLet's train a neural network which differs from the previous neural network solely in its use of the softmax function for the output layer.","29309b4d":"As you can see, the values that we get are not particularly meaningful, and they range wildly, roughly through $x \\in [-1, 1]$.\n\nNevertheless, we still get good accuracy on this artificial dataset if we take the class score with the highest value for each observation as the class.","619e2152":"We can use `model_to_dot` to generate and view a `pydot` visualization of our network:","ef93b701":"## Conclusion\n\nTo review:\n\n**Linear activation**\n* Simple and easy to reason about.\n* Scale-invariant.\n* Generates results that are not very fitted, w.r.t classification tasks.\n* Can only discover linear relationships in the data.\n\n**Non-linear activation**\n* Can discover non-linear relationships in the data.\n* Usually not scale-invariant.\n* More complex and harder to reason about.\n\n**Sofmax activation** (a type of non-linear activation)\n* Not scale-invariant.\n* Provides scaled results which may be interpretable as probabilities, making it very appropriate for classification tasks."}}