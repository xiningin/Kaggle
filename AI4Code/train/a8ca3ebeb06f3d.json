{"cell_type":{"d19c9272":"code","51c7006c":"code","d26aefe1":"code","91537fc7":"code","af80ee4b":"code","2c1c2221":"code","976edc47":"code","c2f7d4fa":"code","133f4f72":"code","743bcb24":"code","c5a4b997":"markdown","d91fcecd":"markdown","a9829fdf":"markdown"},"source":{"d19c9272":"import pyspark\nimport pyspark.sql\nimport pyspark.streaming\nimport pyspark.mllib\nimport pyspark.ml\nfrom pyspark.sql import SparkSession","51c7006c":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)","d26aefe1":"spark = SparkSession.builder \\\n    .appName('spark_for_conservative_credit_score') \\\n    .master('local[*]') \\\n    .config('spark.driver.memory','8G') \\\n    .config('spark.driver.maxResultSize', '2G') \\\n    .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer') \\\n    .config('spark.kryoserializer.buffer.max', '800M')\\\n    .getOrCreate()","91537fc7":"from pyspark.sql.types import IntegerType\nfrom pyspark.sql.types import DoubleType","af80ee4b":"train = spark.read.csv('\/data\/paper_train1.csv', header=True)","2c1c2221":"# loading train data set\nfile_location = \"\/data\/paper_train1.csv\"\nfile_type = \"csv\"\n# CSV options\ninfer_schema = \"false\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ntrain = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location).cache()\n\n# loading test data set\nfile_location = \"\/data\/paper_valid1.csv\"\nfile_type = \"csv\"\n# CSV options\ninfer_schema = \"false\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\nvalid = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location).cache()","976edc47":"from pyspark.sql.functions import *\ntrain = train.withColumn(\"loan_amnt\", train.loan_amnt.cast(\"float\"))\\\n             .withColumn(\"emp_length\", train.emp_length.cast(\"float\"))\\\n             .withColumn(\"annual_inc\", train.annual_inc.cast(\"float\"))\\\n             .withColumn(\"dti\", train.dti.cast(\"float\"))\\\n             .withColumn(\"delinq_2yrs\", train.delinq_2yrs.cast(\"float\"))\\\n             .withColumn(\"revol_util\", regexp_replace(\"revol_util\", \"%\", \"\").cast(\"float\"))\\\n             .withColumn(\"total_acc\", train.total_acc.cast(\"float\"))\\\n             .withColumn(\"credit_length_in_years\", train.credit_length_in_years.cast(\"float\"))\\\n             .withColumn(\"int_rate\", regexp_replace(\"int_rate\", \"%\", \"\").cast(\"float\"))\\\n             .withColumn(\"remain\", train.remain.cast(\"float\"))\\\n             .withColumn(\"issue_year\", train.issue_year.cast(\"float\"))\\\n             .withColumn(\"phi_loan_amnt\", train.phi_loan_amnt.cast(\"float\"))\\\n             .withColumn(\"phi_emp_length\", train.phi_emp_length.cast(\"float\"))\\\n             .withColumn(\"phi_annual_inc\", train.phi_annual_inc.cast(\"float\"))\\\n             .withColumn(\"phi_dti\", train.phi_dti.cast(\"float\"))\\\n             .withColumn(\"phi_delinq_2yrs\", train.phi_delinq_2yrs.cast(\"float\"))\\\n             .withColumn(\"phi_revol_util\", regexp_replace(\"phi_revol_util\", \"%\", \"\").cast(\"float\"))\\\n             .withColumn(\"phi_total_acc\", train.phi_total_acc.cast(\"float\"))\\\n             .withColumn(\"phi_credit_length_in_years\", train.phi_credit_length_in_years.cast(\"float\"))\\\n             .withColumn(\"phi_int_rate\", regexp_replace(\"phi_int_rate\", \"%\", \"\").cast(\"float\"))\\\n             .withColumn(\"CRI\", train.CRI.cast(\"float\"))\\\n             .withColumn(\"train_flag\", train.train_flag.cast(\"float\"))\n\nvalid = valid.withColumn(\"loan_amnt\", valid.loan_amnt.cast(\"float\"))\\\n             .withColumn(\"emp_length\", valid.emp_length.cast(\"float\"))\\\n             .withColumn(\"annual_inc\", valid.annual_inc.cast(\"float\"))\\\n             .withColumn(\"dti\", valid.dti.cast(\"float\"))\\\n             .withColumn(\"delinq_2yrs\", valid.delinq_2yrs.cast(\"float\"))\\\n             .withColumn(\"revol_util\", regexp_replace(\"revol_util\", \"%\", \"\").cast(\"float\"))\\\n             .withColumn(\"total_acc\", valid.total_acc.cast(\"float\"))\\\n             .withColumn(\"credit_length_in_years\", valid.credit_length_in_years.cast(\"float\"))\\\n             .withColumn(\"int_rate\", regexp_replace(\"int_rate\", \"%\", \"\").cast(\"float\"))\\\n             .withColumn(\"remain\", valid.remain.cast(\"float\"))\\\n             .withColumn(\"issue_year\", valid.issue_year.cast(\"float\"))\\\n             .withColumn(\"phi_loan_amnt\", valid.phi_loan_amnt.cast(\"float\"))\\\n             .withColumn(\"phi_emp_length\", valid.phi_emp_length.cast(\"float\"))\\\n             .withColumn(\"phi_annual_inc\", valid.phi_annual_inc.cast(\"float\"))\\\n             .withColumn(\"phi_dti\", valid.phi_dti.cast(\"float\"))\\\n             .withColumn(\"phi_delinq_2yrs\", valid.phi_delinq_2yrs.cast(\"float\"))\\\n             .withColumn(\"phi_revol_util\", regexp_replace(\"phi_revol_util\", \"%\", \"\").cast(\"float\"))\\\n             .withColumn(\"phi_total_acc\", valid.phi_total_acc.cast(\"float\"))\\\n             .withColumn(\"phi_credit_length_in_years\", valid.phi_credit_length_in_years.cast(\"float\"))\\\n             .withColumn(\"phi_int_rate\", regexp_replace(\"phi_int_rate\", \"%\", \"\").cast(\"float\"))\\\n             .withColumn(\"CRI\", valid.CRI.cast(\"float\"))\\\n             .withColumn(\"train_flag\", valid.train_flag.cast(\"float\"))","c2f7d4fa":"train.registerTempTable(\"train\")\ntrain.write.parquet('AA_DFW_ALL.parquet', mode='overwrite')\nvalid.registerTempTable(\"valid\")\nvalid.write.parquet('AA_DFW_ALL.parquet', mode='overwrite')\n\nprint(\" >>>>>>> \" + str(train.count())+ \" loans opened by TRAIN data_set for model training!\")\nprint(\" >>>>>>> \" + str(valid.count())+ \" loans opened by VALID data_set for model validation!\")","133f4f72":"print(\" == imbalancement of the loan train and valid datasets  ==\")\nprint(\" >>>>>>> Train dataset: \" + str(train.groupby('default_loan').count().collect()))\nprint(\" >>>>>>> Test dataset: \" + str(valid.groupby('default_loan').count().collect()))","743bcb24":"# Set the response and predictor variables and set up regression models with train and test datasets.\nY = \"default_loan\"\n\ncategoricals = [\"phi_term_month\", \"home_ownership\", \"purpose\", \"addr_state\", \"verification_status\", \"application_type\"]\nnumerics = [\"CRI\", \"phi_loan_amnt\", \"phi_emp_length\", \"phi_annual_inc\", \"phi_dti\", \"phi_delinq_2yrs\", \"phi_revol_util\", \"phi_total_acc\", \"phi_credit_length_in_years\", \"phi_int_rate\"]\nX = categoricals + numerics\n\n# now we can save the valid to use it as an imput data for our final model","c5a4b997":"# Feature Engineering","d91fcecd":"# Machine Learning Codes for Credit Scoring - Feature Engineering!\n\nFor citation: https:\/\/doi.org\/10.1016\/j.eswa.2021.114835 (Journal - Expert Systems with Applications.)\n\nAsk for full-text in [ResearchGate](https:\/\/www.researchgate.net\/profile\/Afshin-Ashofteh-2)\n\nAfshin Ashofteh [email](aashofteh@novaims.unl.pt)\n\nSubject: Credit Risk and Credit Scoring.\n\nDatasource: loan.csv - Each loan includes applicant information provided by the applicant as well as current loan status (Current, Late, Fully Paid, etc.) and latest payment information.\n","a9829fdf":"Links: \n*[ResearchGate](https:\/\/www.researchgate.net\/profile\/Afshin-Ashofteh-2)\n*[Kaggle](https:\/\/www.kaggle.com\/aashofteh)\n*[Google Scholar](https:\/\/scholar.google.com\/citations?user=oIa1W0gAAAAJ&hl=en)\n*[Data Science Discussion Group](https:\/\/www.linkedin.com\/groups\/12420006)\n*[email](aashofteh@novaims.unl.pt)"}}