{"cell_type":{"26e36a0f":"code","4159b4d6":"code","5c18b97b":"code","5c91ddf6":"code","ea4d6159":"code","07a55041":"code","10cc6b7a":"code","62df83af":"code","3e3d5532":"code","9300c6e6":"code","4c1ba120":"code","c6707922":"code","5430de49":"code","eda8d166":"code","d800770c":"code","fa5625a4":"code","193e27ea":"code","166b7940":"code","d51ecbc1":"code","2d204ce9":"code","a0b9913a":"code","5d4567f9":"code","c5319a85":"code","392f5dfb":"code","8e1a262f":"code","267a3fdb":"code","699d1de5":"code","962ad4e8":"code","bde23295":"code","0ebf398d":"code","66086a73":"code","71769c8a":"code","e7753a2c":"code","6adbf8da":"markdown","88b963de":"markdown","db894322":"markdown","e431b877":"markdown","265ff863":"markdown","cd3aef73":"markdown","4fb5048c":"markdown"},"source":{"26e36a0f":"import nltk\nimport numpy as np\nimport pandas as pd\nimport os \nimport re\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot')\n\nN_MC = 400\nCUTOFF_PROB = 0.5","4159b4d6":"df = pd.read_csv(\"\/kaggle\/input\/phish-reviews\/Reviews.csv\")\ndf.head()","5c18b97b":"# Regex to remove punctuation\ndef remove_punctuation(review):\n    symbols=\"[\u2019'`!)(&@#.,\/'`~:;|\\?]\"\n    return re.sub('\\r',' ', re.sub('\\n', ' ', re.sub(symbols,' ',review)))\n","5c91ddf6":"# Remove newlines and convert all to lowercase\ndf.Reviews = df.Reviews.apply(remove_punctuation)\ndf.Reviews = df.Reviews.apply(lambda n: n.lower())","ea4d6159":"df.Reviews","07a55041":"# Tokenize Reviews\ndf.Reviews = df.Reviews.apply(nltk.word_tokenize)","10cc6b7a":"# Filter Stop Words\nstops = nltk.corpus.stopwords.words('english')\ndef filter_stop_words(tk_review):\n    return [word for word in tk_review if word not in stops]","62df83af":"df.Reviews=df.Reviews.apply(filter_stop_words)","3e3d5532":"# POS-Tagging\ndf['Reviews2'] = df.Reviews.apply(nltk.pos_tag)","9300c6e6":"# Lemmatization\nLemming = nltk.stem.WordNetLemmatizer()\n\ndef penn2morphy(penntag):\n    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n    morphy_tag = {'NN':'n', 'JJ':'a',\n                  'VB':'v', 'RB':'r'}\n    try:\n        return morphy_tag[penntag[:2]]\n    except:\n        return 'n'\n\ndef lemmatize_words(tag_review):\n    output = []\n    for w in tag_review:\n        try:\n            lm = Lemming.lemmatize(w[0], pos=penn2morphy(w[1]))\n        except Exception as e:\n            lm = w\n        output.append(lm)\n        \n    return output\n","4c1ba120":"df['Reviews2'] = df.Reviews2.apply(lemmatize_words)","c6707922":"df.head()","5430de49":"# Bin Scores and view Histogrm\ndef bin_score(row):\n    return 1 if row['Score'] >= 4 else 0\ndf['BinnedScore'] = df.apply(bin_score, axis=1)\ndf.groupby('BinnedScore').size().plot(kind='bar')","eda8d166":"# Extract N most common words\ndef n_most_common(reviews, n=100):\n    words = []\n    sentences = reviews.values\n    \n    # Iterate through sentences to extract all unique words\n    for s in sentences:\n        for w in s:\n            if w not in words:\n                words.append(w)\n    \n    # Generate a counter for each word\n    counts = {w: 0 for w in words}\n    \n    # Iterate through all words, incrementing count\n    for sent in sentences:\n        for word in sent:\n            counts[word] += 1\n    \n    # Generate list of \n    words = sorted([(w, counts[w]) for w in words], key= lambda x: x[1], reverse=True)\n    return words[0:n]\n    ","d800770c":"MC = n_most_common(df.Reviews2,N_MC)","fa5625a4":"# Create Categorical Variables\nMC = [w[0] for w in MC]\ndef create_categorical_column(dframe, word):\n    def hasword(row):\n        if word in row['Reviews2']:\n            return 1\n        else:\n            return 0\n    dframe[word] = dframe.apply(lambda row: hasword(row), axis=1)","193e27ea":"DF = df.drop(columns=['Unnamed: 0', 'Reviews', 'Score'])\nDF.head()\nDF.to_csv('CleanedDF1.csv')","166b7940":"DF.head()","d51ecbc1":"#One-Hot Encoding\ndef encode_word(row, word):\n    return 1 if word in row['Reviews2'] else 0\nlenmc = len(MC)\nfor i, word in enumerate(MC):\n    DF[str(word)] = DF.apply(lambda row: encode_word(row, word), axis=1)\n    print(f'{i}\/{lenmc}')\n    ","2d204ce9":"from sklearn.model_selection import train_test_split\n\n\ntrain, test = train_test_split(DF, test_size=0.2)\n\ntrain_y = train.pop('BinnedScore')\ntest_y = test.pop('BinnedScore')","a0b9913a":"train.drop('Reviews2', inplace=True, axis=1)\ntest.drop('Reviews2', inplace=True, axis=1)","5d4567f9":"from sklearn.naive_bayes import ComplementNB, GaussianNB\nmodel = ComplementNB(alpha=1)\nmodel.fit(train, train_y)","c5319a85":"import random\ndef pred_cat(prob, CUTOFF_PROB=CUTOFF_PROB):\n    if prob > CUTOFF_PROB:\n        return '1'\n    elif prob < CUTOFF_PROB:\n        return '0'\n    else:\n        return random.choice([0, 1])","392f5dfb":"# Make Predictions!\nprobs = [round(p[1], 4) for p in model.predict_proba(test)]\npredictions = [pred_cat(p) for p in probs]\nobserved = [v for v in test_y.values]\nbenchmark_predictions = [1 for i in test_y.values]\n\nresults = pd.DataFrame([predictions, observed, benchmark_predictions, probs]).transpose()\nresults.rename({0:'Predicted', 1:'Observed', 2:'Benchmark_Predictions', 3:'Probabilities'}, axis=1, inplace=True)\n\nresults = results.apply(lambda x: x.apply(str))\nresults","8e1a262f":"def model_right(row):\n    return 1 if row['Predicted'] == row['Observed'] else 0\n\ndef benchmark_right(row):\n    return 1 if row['Benchmark_Predictions'] == row['Observed'] else 0\n","267a3fdb":"results['model_right'] = results.apply(model_right, axis=1)\nresults['benchmark_right'] = results.apply(benchmark_right, axis=1)","699d1de5":"results","962ad4e8":"from sklearn.metrics import roc_auc_score\nAUCstat = roc_auc_score(results['Observed'].apply(float).values.flatten(), results['Probabilities'].apply(float).values.flatten())","bde23295":"print(f'model error: {1 - results[\"model_right\"].sum() \/ len(results)}')\nprint(f'AUC: {AUCstat}')\nprint(f'benchmark error: {1 - results[\"benchmark_right\"].sum() \/ len(results)}')","0ebf398d":"Positives = results[results['Observed'] == '1']\nNegatives = results[results['Observed'] == '0']\n\nsensitivity = len(Positives[Positives['Predicted']=='1']) \/ len(Positives)\nspecificity = len(Negatives[Negatives['Predicted']=='0']) \/ len(Negatives)\n\nprint(f'Sensitivty: {round(sensitivity*100, 2)}%')\nprint(f'Specificity: {round(specificity*100, 2)}%')","66086a73":"results","71769c8a":"def ROC(rdf):\n    \n    df1 = pd.concat([rdf['Probabilities'], rdf['Observed']], axis=1)\n\n    ss = []\n    sc = []    \n    \n    def populate_ss(ctf):\n        \n        df1['TempPred'] = df1['Probabilities'].apply(lambda p: pred_cat(float(p), CUTOFF_PROB=float(ctf)))\n        \n        Positives = df1[df1['Observed'] == '1']\n        Negatives = df1[df1['Observed'] == '0']\n        \n        sensi = len(Positives[Positives['TempPred'] == '1']) \/ len(Positives)\n        speci = len(Negatives[Negatives['TempPred'] == '0']) \/ len(Negatives)\n        \n        ss.append(sensi)\n        sc.append(1-speci)\n\n    cutoffs = np.arange(0, 1, 0.005)\n    for i in cutoffs:\n        populate_ss(i)\n\n    plt.title(\"ROC Chart\")\n    plt.xlabel('1-Specificity')\n    plt.ylabel('Sensitivity')\n    \n    plt.plot([0, 0, 1], [0, 1, 1], c='g', label='Ideal')\n    plt.plot(sc, ss, c='b', label='Observed')\n    plt.plot([0, 1], [0, 1], c='r', label='Benchmark')\n    \n    plt.legend()","e7753a2c":"ROC(results)","6adbf8da":"ROC Chart","88b963de":"Naive Bayes Classification for Phish Show Reviews V1\n\nTODO:\n* Optimize HyperParameter N_MC\n* N_MC Heuristic instead of frequency?\n* Construct ROC chart from probabilities\n* Scrape Summer 2019 Data to compare\n* Ensemble of reviews per show?","db894322":"Unfinished! Need to add error\/benchmark type dealios and vastly improve my preprocessing... but the pipeline is taking shape!\n\nCurrent Baselines: \n\n100 words, 40.75% error, 59.45% Sensitivity, 58.96% Specificity\n\n200 words, 38.25% error, 62.46% Sensitivity, 60.77% Specificity\n\n400 words, 37.02% error, 65.82% Sensitivity, 58.94% Specificity","e431b877":"Next, we extract Reviews2 and BinnedScore, and do final preprocessing for model-building","265ff863":"With our data prepared, we can start building a model!\n\nNOTE: Have to revise data cleaning to get rid of single-charactar tokens it looks like :\/","cd3aef73":"Now, with a decent baseline amount of text preprocessing done, time to bin the scores.\n","4fb5048c":"Phase 1: Load Data and Process text\n"}}