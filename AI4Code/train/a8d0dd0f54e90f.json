{"cell_type":{"82502b06":"code","f6856ab8":"code","c166a19f":"code","ec6feb86":"code","87ec55d5":"code","68dcdaf2":"code","8d2b7a2f":"code","31ee16b0":"code","6b437ffa":"code","fefc11fd":"code","d1da4e50":"code","5db43134":"code","77a775ce":"code","275912d8":"code","ada35852":"code","80382201":"code","996e4ebb":"code","04106fe8":"code","83c65449":"code","c3e76381":"code","8e8760a7":"code","e8fc2a69":"code","349764d8":"code","7ee262dc":"code","da96415b":"code","d0613b64":"code","3783a0ea":"code","dd954a6e":"code","c5131cc0":"code","30a0a896":"code","909c19cc":"code","25b71338":"code","d504ef31":"code","1adcc2f1":"code","fc02f25f":"code","543ba1d3":"code","7b6d8148":"code","d1cd2372":"code","f94e4fc9":"code","1f61eb3b":"code","b7d45536":"code","4f29d8dc":"code","87c6dbc9":"code","7f17c106":"code","acfb9af4":"code","93f74701":"code","96460404":"code","8bdb2cd0":"code","f37e2d81":"markdown","2872c00a":"markdown","3678b4c3":"markdown","c1441167":"markdown"},"source":{"82502b06":"import torch\nimport random\nimport torch.nn as nn\nimport math\nfrom torch.autograd import Variable\nfrom math import log10\nimport os","f6856ab8":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","c166a19f":"def load_img(filepath):\n    img = Image.open(filepath).convert(\"YCbCr\")\n\n    y, _, _ = img.split()\n    return y","ec6feb86":"def is_image_file(filename):\n    return any(filename.endswith(extension) for extension in [\".png\", \".jpg\", \".tif\"])","87ec55d5":"import cv2\nif hasattr(cv2.imread(\"..\/input\/the-rvlcdip-dataset-test\/test\/scientific_publication\/2500126531_2500126536.tif\"), \"shape\"):\n    print(\"ok\")","68dcdaf2":"if hasattr(cv2.imread(\"..\/input\/the-rvlcdip-dataset-test\/test\/scientific_publication\/0000022799.tif\"), \"shape\"):\n    print(\"ok\")","8d2b7a2f":"import torch.utils.data as data\nimport torchvision\nfrom os import listdir\nclass DatasetFromFolder(data.Dataset):\n    def __init__(self, image_dir, LR_transform=None, HR_2_transform=None, HR_4_transform=None, HR_8_transform=None):\n        super(DatasetFromFolder, self).__init__()\n        self.image_filenames = []\n        for dirname, _, filenames in os.walk(image_dir):\n            for filename in filenames:\n                if os.path.join(dirname, filename)!=\"..\/input\/the-rvlcdip-dataset-test\/test\/scientific_publication\/2500126531_2500126536.tif\":\n                    self.image_filenames.append(os.path.join(dirname, filename))\n        self.LR_transform = LR_transform\n        self.HR_2_transform = HR_2_transform\n        self.HR_4_transform = HR_4_transform\n        self.HR_8_transform = HR_8_transform\n\n    def __getitem__(self, index):\n        input = load_img(self.image_filenames[index])\n        HR_8 = self.HR_8_transform(input)\n        HR_4 = self.HR_4_transform(HR_8)\n        HR_2 = self.HR_2_transform(HR_8)\n        LR = self.LR_transform(HR_8)\n        to_tensor = torchvision.transforms.ToTensor()\n        HR_8 = to_tensor(HR_8)\n        return LR, HR_2, HR_4, HR_8\n\n    def __len__(self):\n        return len(self.image_filenames)","31ee16b0":"from PIL import Image\nimport matplotlib.pyplot as plt\nim = Image.open(\"..\/input\/the-rvlcdip-dataset-test\/test\/letter\/0000000700.tif\")\nplt.imshow(im, cmap=\"gray\")","6b437ffa":"# from IPython.display import Image, display\n# display(Image(\"..\/input\/the-rvlcdip-dataset-test\/test\/letter\/0000000700.tif\"))","fefc11fd":"from torchvision.transforms import ToTensor\nt = ToTensor()\nimt = t(im)\nimt.shape","d1da4e50":"def get_training_set():\n    root_dir = \"..\/input\/the-rvlcdip-dataset-test\"\n    train_dir = join(root_dir, \"test\")\n\n    return DatasetFromFolder(train_dir,\n                             LR_transform=LR_transform(crop_size),\n                             HR_2_transform=HR_2_transform(crop_size),\n                             HR_4_transform = HR_4_transform(crop_size),\n                             HR_8_transform = HR_8_transform(crop_size)\n                            )","5db43134":"def LR_transform(crop_size):\n    return Compose([\n        Scale(crop_size\/\/8),\n        ToTensor(),\n    ])\ndef HR_2_transform(crop_size):\n    return Compose([\n        Scale(crop_size\/\/4),\n        ToTensor(),\n    ])\ndef HR_4_transform(crop_size):\n    return Compose([\n        Scale(crop_size\/\/2),\n        ToTensor(),\n    ])\n\ndef HR_8_transform(crop_size):\n    return Compose([\n        CenterCrop((crop_size, crop_size))\n    ])\n","77a775ce":"from os.path import exists, join, basename\nfrom os import makedirs, remove\nfrom six.moves import urllib\nimport tarfile\nfrom torchvision.transforms import Compose, CenterCrop, ToTensor, Scale, RandomCrop\ncrop_size =256","275912d8":"train_set = get_training_set()","ada35852":"len(train_set)","80382201":"from torch.utils.data.dataset import random_split\ntrain_len = 30000\nvalid_len = 9996\nTrainData1, ValidationData1 = random_split(train_set, [train_len, valid_len])","996e4ebb":"from torch.utils.data import DataLoader\ntraining_data_loader= DataLoader(dataset = TrainData1, num_workers=1, batch_size=64, shuffle=True)\ntesting_data_loader = DataLoader(dataset=ValidationData1, num_workers=1, batch_size=32, shuffle=False)","04106fe8":"import numpy as np\nfrom PIL import Image\nimport random\na = training_data_loader.dataset[0]","83c65449":"import torchvision.transforms as transforms\nplt.imshow(transforms.ToPILImage()(a[0]), interpolation=\"bicubic\", cmap=\"gray\")","c3e76381":"class L1_Charbonnier_loss(nn.Module):\n    \"\"\"L1 Charbonnierloss.\"\"\"\n    def __init__(self):\n        super(L1_Charbonnier_loss, self).__init__()\n        self.eps = 1e-3\n\n    def forward(self, X, Y):\n        diff = torch.add(X, -Y)\n        error = torch.sqrt( diff * diff + self.eps * self.eps )\n        loss = torch.sum(error) \n        return loss","8e8760a7":"def get_upsample_filter(size):\n    \"\"\"Make a 2D bilinear kernel suitable for upsampling\"\"\"\n    factor = (size + 1) \/\/ 2\n    if size % 2 == 1:\n        center = factor - 1\n    else:\n        center = factor - 0.5\n    og = np.ogrid[:size, :size]\n    filter = (1 - abs(og[0] - center) \/ factor) * \\\n             (1 - abs(og[1] - center) \/ factor)\n    return torch.from_numpy(filter).float()","e8fc2a69":"class _Conv_Block(nn.Module):    \n    def __init__(self):\n        super(_Conv_Block, self).__init__()\n        \n        self.cov_block = nn.Sequential(\n#             nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n#             nn.LeakyReLU(0.2, inplace=True),\n#             nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n#             nn.LeakyReLU(0.2, inplace=True),\n#             nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n#             nn.LeakyReLU(0.2, inplace=True),\n#             nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n#             nn.LeakyReLU(0.2, inplace=True),\n#             nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n#             nn.LeakyReLU(0.2, inplace=True),\n#             nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n#             nn.LeakyReLU(0.2, inplace=True),\n#             nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n#             nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.ConvTranspose2d(in_channels=64, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n        \n    def forward(self, x):  \n        output = self.cov_block(x)\n        return output ","349764d8":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        \n        self.conv_input = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.relu = nn.LeakyReLU(0.2, inplace=True)\n        \n        self.convt_I1 = nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=4, stride=2, padding=1, bias=False)\n        self.convt_R1 = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=3, stride=1, padding=1, bias=False)\n        self.convt_F1 = self.make_layer(_Conv_Block)\n  \n        self.convt_I2 = nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=4, stride=2, padding=1, bias=False)\n        self.convt_R2 = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=3, stride=1, padding=1, bias=False)\n        self.convt_F2 = self.make_layer(_Conv_Block)        \n        \n        self.convt_I3 = nn.ConvTranspose2d(in_channels=1, out_channels=1, kernel_size=4, stride=2, padding=1, bias=False)\n        self.convt_R3 = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=3, stride=1, padding=1, bias=False)\n        self.convt_F3 = self.make_layer(_Conv_Block)\n        \n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. \/ n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            if isinstance(m, nn.ConvTranspose2d):\n                c1, c2, h, w = m.weight.data.size()\n                weight = get_upsample_filter(h)\n                m.weight.data = weight.view(1, 1, h, w).repeat(c1, c2, 1, 1)\n                if m.bias is not None:\n                    m.bias.data.zero_()\n                    \n    def make_layer(self, block):\n        layers = []\n        layers.append(block())\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.relu(self.conv_input(x))\n        \n        convt_F1 = self.convt_F1(out)\n        convt_I1 = self.convt_I1(x)\n        convt_R1 = self.convt_R1(convt_F1)\n        HR_2x = convt_I1 + convt_R1\n        \n        convt_F2 = self.convt_F2(convt_F1)\n        convt_I2 = self.convt_I2(HR_2x)\n        convt_R2 = self.convt_R2(convt_F2)\n        HR_4x = convt_I2 + convt_R2\n        \n        convt_F3 = self.convt_F3(convt_F2)\n        convt_I3 = self.convt_I3(HR_4x)\n        convt_R3 = self.convt_R3(convt_F3)\n        HR_8x = convt_I3 + convt_R3\n#         HR_4x = HR_4x.to(device)\n#         HR_2x = HR_2x.to(device)\n        return HR_2x, HR_4x, HR_8x","7ee262dc":"okay = iter(training_data_loader)\nfirst = next(okay)\nfirst[0].shape","da96415b":"# plt.imshow(first[2][0][0].cpu(), cmap = 'gray')\nplt.imshow(transforms.ToPILImage()(first[1][0]), interpolation=\"bicubic\")","d0613b64":"class lapinit():\n    def __init__(self, batchSize = 4, nEpochs = 100, lr = 3e-4, step = 100,\n            cuda = {'action' : True}, resume = '', start_epoch = 1, threads = 1,\n            momentum = 0.9, weight_decay = 1e-4, pretrained = ''):\n        self.batchSize = batchSize\n        self.nEpochs = nEpochs\n        self.lr = lr\n        self.step = step\n        self.cuda = cuda\n        self.resume = resume\n        self.start_epoch = start_epoch\n        self.threads = threads\n        self.momentum = momentum\n        self.weight_decay = weight_decay\n        self.pretrained = pretrained\n\nopt = lapinit()","3783a0ea":"model = Net()\nmodel = model.to(device)\ncriterion = L1_Charbonnier_loss()","dd954a6e":"temp = model((torch.randn(1,1,128,128)).to(device))","c5131cc0":"first[0].shape","30a0a896":"first[0] = first[0].to(device)\ntmp1 = model(first[0])\ntmp1[0].shape#, tmp2.shape","909c19cc":"import torch.optim as optim\noptimizer = optim.Adam(model.parameters(), lr=opt.lr)","25b71338":"okay = iter(testing_data_loader)\ntest1 = next(okay)\ntest1[0].shape","d504ef31":"test1[0] = test1[0].to(device)","1adcc2f1":"def train(training_data_loader, optimizer, model, criterion, epoch):\n\n    lr = adjust_learning_rate(optimizer, epoch-1)\n\n    for param_group in optimizer.param_groups:\n        param_group[\"lr\"] = lr\n\n    print(\"Epoch={}, lr={}\".format(epoch, optimizer.param_groups[0][\"lr\"]))\n\n    model.train()\n\n    for iteration, batch in enumerate(training_data_loader, 1):\n\n        input, label_x2, label_x4, label_x8 = Variable(batch[0]), Variable(batch[1], requires_grad=False), Variable(batch[2], requires_grad=False), Variable(batch[3], requires_grad=False)\n        input, label_x2, label_x4, label_x8 = input.to(device), label_x2.to(device), label_x4.to(device), label_x8.to(device)\n\n        HR_2x, HR_4x, HR_8x = model(input)\n        loss_x2 = criterion(HR_2x, label_x2)\n        loss_x4 = criterion(HR_4x, label_x4)\n        loss_x8 = criterion(HR_8x, label_x8)\n        loss = loss_x2 + loss_x4 + loss_x8 \n\n        optimizer.zero_grad()\n\n        loss_x2.backward(retain_graph=True)\n        loss_x4.backward(retain_graph=True)\n        loss_x8.backward(retain_graph=True)\n\n        optimizer.step()\n\n        if iteration%100 == 0:\n            print(\"===> Epoch[{}]({}\/{}): Loss: {:.10f}\".format(epoch, iteration, len(training_data_loader), loss.item()))\n","fc02f25f":"def save_checkpoint(model, epoch):\n    model_folder = \"checkpoint\/\"\n    model_out_path = model_folder + \"lapsrn_model_epoch_{}.pth\".format(epoch)\n    state = {\"epoch\": epoch ,\"model\": model, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n    if not os.path.exists(model_folder):\n        os.makedirs(model_folder)\n\n    torch.save(state, model_out_path)\n\n    print(\"Checkpoint saved to {}\".format(model_out_path))\n","543ba1d3":"def adjust_learning_rate(optimizer, epoch):\n    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 1 epochs\"\"\"\n    lr = opt.lr * (0.1 ** (epoch \/\/ (opt.step\/10)))\n    return lr","7b6d8148":"plt.imshow(transforms.ToPILImage()(test1[1][0].cpu()), interpolation=\"bicubic\", cmap=\"gray\")","d1cd2372":"plt.imshow(transforms.ToPILImage()(test1[0][0].cpu()), interpolation=\"bicubic\", cmap=\"gray\")\nplt.show()","f94e4fc9":"def test():\n    avg_psnr1 = 0\n    avg_psnr2 = 0\n    avg_psnr3 = 0\n    for batch in testing_data_loader:\n        LR, HR_2_target, HR_4_target, HR_8_target = Variable(batch[0]), Variable(batch[1]), Variable(batch[2]), Variable(batch[3])\n        \n        if torch.cuda.is_available():\n            LR = LR.cuda()\n            HR_2_target = HR_2_target.cuda()\n            HR_4_target = HR_4_target.cuda()\n            HR_8_target = HR_8_target.cuda()\n\n        HR_2, HR_4, HR_8= model(LR)\n        mseloss = nn.MSELoss()\n        mse1 = mseloss(HR_2, HR_2_target)\n        mse2 = mseloss(HR_4, HR_4_target)\n        mse3 = mseloss(HR_8, HR_8_target)\n        psnr1 = 10 * log10(1 \/ torch.Tensor.tolist(mse1))\n        psnr2 = 10 * log10(1 \/ torch.Tensor.tolist(mse2))\n        psnr3 = 10 * log10(1 \/ torch.Tensor.tolist(mse3))\n        avg_psnr1 += psnr1\n        avg_psnr2 += psnr2\n        avg_psnr3 += psnr3\n    print(\"Avg. PSNR for 2x: {:.4f} dB\".format(avg_psnr1 \/ len(testing_data_loader)))\n    print(\"Avg. PSNR for 4x: {:.4f} dB\".format(avg_psnr2 \/ len(testing_data_loader)))\n    print(\"Avg. PSNR for 8x: {:.4f} dB\".format(avg_psnr3 \/ len(testing_data_loader)))\n    return avg_psnr1\/len(testing_data_loader), avg_psnr2\/len(testing_data_loader), avg_psnr3\/len(testing_data_loader)\n","1f61eb3b":"from datetime import datetime\nstart=datetime.now()\ny = []\nx = [i for i in range(opt.nEpochs)]\nfor epoch in range(opt.start_epoch, opt.nEpochs + 1):\n    \n    train(training_data_loader, optimizer, model, criterion, epoch)\n    # save_checkpoint(model, epoch)\n    if epoch%1==0:\n        save_checkpoint(model, epoch)\n    y.append(test())\n    print(datetime.now() - start)","b7d45536":"import os\nsave_checkpoint(model, 6)","4f29d8dc":"x = range(len(y))\nplt.plot(x, y)","87c6dbc9":"okay = iter(testing_data_loader)\ntest1 = next(okay)\ntest1[0].shape","7f17c106":"test1[0] = test1[0].cuda()\ntmp1 = model(test1[0])","acfb9af4":"test1[0][0].shape","93f74701":"plt.imshow(test1[0][0][0].cpu(), cmap = 'gray')","96460404":"plt.imshow(tmp1[2][0][0].detach().cpu(), cmap = 'gray')","8bdb2cd0":"plt.imshow(test1[1][0][0].cpu(), cmap = 'gray')","f37e2d81":"Low resolution image - ","2872c00a":"Prediction -","3678b4c3":"Original label -","c1441167":"Checking if the model works on a random image of dimension 1x1x128x128"}}