{"cell_type":{"5941910c":"code","d7cfe690":"code","9045d710":"code","078285b1":"code","c35c348c":"code","acf16b3a":"code","ac676a14":"code","f6604adf":"code","d176721c":"code","8c9c5680":"code","81e470c5":"code","28de7da6":"code","b2b72f04":"code","f6f29a22":"code","7c6303b7":"code","18d1151d":"code","2e9c0855":"code","2a403ba2":"markdown","b5f586fb":"markdown","ce046e7a":"markdown","ffade41d":"markdown","7f451e5d":"markdown","2a9fdb04":"markdown","ea4649ff":"markdown","6f30ef94":"markdown","223ee2aa":"markdown","cac804bf":"markdown","af7537b7":"markdown"},"source":{"5941910c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d7cfe690":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import DistanceMetric","9045d710":"iris_data = pd.read_csv('\/kaggle\/input\/iris\/Iris.csv')\niris_data.head()","078285b1":"data = iris_data[['SepalLengthCm', 'SepalWidthCm']].to_numpy()\ndata = np.append(data, [[9, 5]], axis = 0)\ndata = np.append(data, [[4.5,6]], axis = 0)\ndata = np.append(data, [[4.6,6.1]], axis = 0)","c35c348c":"def plot_result(cleaned_data, outliers):\n    plt.figure(figsize = (10, 7))\n    plt.scatter(x = cleaned_data[:,0], y = cleaned_data[:,1], c = 'b', label = 'Cleaned data')\n    plt.scatter(x = outliers[:,0], y = outliers[:,1], c = 'r', label = 'Detected outliers')\n    plt.legend()\n    plt.show()","acf16b3a":"plt.figure(figsize = (10, 7))\nplt.scatter(x = data[:,0], y = data[:,1])\nplt.show()","ac676a14":"def get_r_neighborhood(data, p, r):\n    neighborhood = []\n    for point in data:\n        if np.linalg.norm(p - point) <= r:\n            neighborhood.append(point)\n    return neighborhood","f6604adf":"def DB(data, r_frac = 3, frac = 0.98):\n    # it is conveniently to specify r parameter as a number of standard deviations of the distances of points from the data to its mean point \n    r = r_frac * np.linalg.norm(data - data.mean(axis = 0), axis = 1).var()\n    cleaned_data = []\n    outliers = []\n    n = len(data)\n    for p in data:\n        neighborhood = get_r_neighborhood(data, p, r)\n        if len(neighborhood) <= (1 - frac) * n:\n            outliers.append(p)\n        else:\n            cleaned_data.append(p)\n    return np.array(cleaned_data), np.array(outliers)","d176721c":"cleaned_data, outliers = DB(data)\nplot_result(cleaned_data, outliers)","8c9c5680":"def n_hat(data, p, r, a):\n    N_p_r = get_r_neighborhood(data, p, r)\n    mul = 0\n    for neighbor in N_p_r:\n        mul += len(get_r_neighborhood(data, neighbor, r * a))\n    return mul \/ len(N_p_r)    ","81e470c5":"def MDEF(data, p, r, a):\n    return 1 - len(get_r_neighborhood(data, p, r * a)) \/ n_hat(data, p, r, a)","28de7da6":"def sigma_MDEF(data, p, r, a):\n    return np.array([len(get_r_neighborhood(data, neighbor, r * a)) for neighbor in get_r_neighborhood(data, p, r * a)]).std() \/ n_hat(data, p, r, a)","b2b72f04":"def LOCI(data, r, a):\n    cleaned_data = []\n    outliers = []\n    for p in data:\n        if MDEF(data, p, r, a) > 3 * sigma_MDEF(data, p, r, a):\n            outliers.append(p)\n        else:\n            cleaned_data.append(p)\n    return np.array(cleaned_data), np.array(outliers)        ","f6f29a22":"cleaned_data, outliers = LOCI(data, 3, 0.5)\nplot_result(cleaned_data, outliers)","7c6303b7":"from sklearn.neighbors import NearestNeighbors","18d1151d":"def KNN(data, k, n):\n    NN = NearestNeighbors(n_neighbors=k).fit(data)\n    distances, indices = NN.kneighbors(data)\n    k_th_distances = distances[:,k - 1]\n    sorted_indexes = np.argsort(k_th_distances)\n    cleaned_data_inds = sorted_indexes[:len(data) - n]\n    outlier_data_inds = sorted_indexes[len(data) - n:]\n    return data[cleaned_data_inds], data[outlier_data_inds]","2e9c0855":"cleaned_data, outliers = KNN(data, 5, 3)\nplot_result(cleaned_data, outliers)","2a403ba2":"In this kernel I tell about approaches based on the distance between the objects. It is intuitively clear, that very far points are more likely to be an outlier the the other. So, here you'll see some ideas belonging to such approach. ","b5f586fb":"Thus, we consider a point as outlier if \n$$MDEF(p, r, a) > 3 \\cdot \\sigma_{MDEF}(p, r, a)$$","ce046e7a":"In this algorithm we consider a point to be outlier in the data $D$ if at least a fraction $frac * D$ of objects in $D$ are at a distance greater than $r$ from\nthis point. $r$ and $frac$ are hyperparameters and should be chosen according to the data.\n<br>\nSo, we define a neighborhood of the point $p$ as: <br>\n$N_p(r) = \\{q: q \\in D$ and $distance(p, q) \\leqslant r\\}$\n<br>\nThen $p$ is an outlier if:\n$$|N_p(r)| \\leqslant (1 - frac) * |D|$$","ffade41d":"<h2>Distance Based-Outlier Approach<\/h2>","7f451e5d":"<h1>Algorithms<\/h1>","2a9fdb04":"I'm starting a serie of post about the anomaly detection algorithms. I'm based mostly on the book [Anomaly Detection Principles and Algorithms](https:\/\/www.amazon.com\/Detection-Principles-Algorithms-Terrorism-Computation-ebook\/dp\/B077LKSY5B). So, it is the first post from the serie. <br> **Please, upload this kernel if you want to see the next post sooner, thanks**","ea4649ff":"<h2>k-NN outlier<\/h2>","6f30ef94":"<h1>Introduction<\/h1>","223ee2aa":"This algorithm is more complicated. So, lets define:\n$$\\hat{n}(p, r, a) = \\frac{1}{N_p(r)}\\sum_{q \\in N_p(r)}|N_q(a \\cdot r)|$$ - the mean count of $a\\cdot r$ - distance neighbors of the $r$ - distance neighbors of the point $p$ \n$$MDEF(p, r, a) = 1 - \\frac{|N_p(a \\cdot r)|}{\\hat{n}(p, r, a)}$$ -  the Multi-granularity Deviation Factor (MDEF) of point $p$. Is negative for non-anomalies points and has hight value for anomaly one (when a point has fewer neighbors relative to its neighbors).\n$$\\sigma_{MDEF}(p, r, a) = \\frac{\\sigma_\\hat{n}(p, r, a)}{\\hat{n}(p, r, a)}$$\nwhere $\\sigma_\\hat{n}(p, r, a)$ - the standard deviation of $|N_q(a \\cdot r)|$ values for all $q$ - neighbors of $p$ ","cac804bf":"This approach is based on the distance from a poit to its k-th neighbor. So, algorithm is simple:\n* For each point we calculate the distance to its k-th neighbor\n* Consider n points with the biggest distances as outliers\n<br>\n<br>\nThus, n and k are hyperparameters of the model","af7537b7":"<h2>Local Correlation Integral (LOCI) Algorithm<\/h2>"}}