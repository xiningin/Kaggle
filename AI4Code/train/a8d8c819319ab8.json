{"cell_type":{"ad1ea7b4":"code","33827137":"code","97e9097b":"code","496fdda0":"code","1566b294":"code","00fab832":"code","7ee9700f":"code","d92535cf":"code","808460e7":"code","4b9d304b":"code","dff2b1ec":"code","878a9281":"code","081c4e4f":"code","8bdbbc3c":"code","d7901496":"code","6302998b":"code","982f5538":"code","a26ace45":"code","8237bab7":"code","35ae7cdf":"code","9ce4d94d":"code","ded680f4":"code","d3105664":"code","0c8bf58a":"code","6cba1a90":"code","b8b8aece":"code","984a15fa":"code","c85ff7ba":"code","cd95830b":"code","87e95755":"code","0205d764":"code","866f5a61":"code","d2c7be6e":"code","1d91a79b":"code","f57c73cc":"code","df70e5d0":"code","0dc9eea4":"code","dd54db7a":"code","d603f5a3":"code","81eeebf2":"code","8e2fc652":"code","b856f97b":"code","c9eb6637":"code","a8a67403":"code","2e7fb0d8":"code","637be411":"code","6141b730":"markdown","dc55f65f":"markdown","bcec1e68":"markdown","bb49ec45":"markdown","3ff78d25":"markdown","a09846d8":"markdown","ee7e65de":"markdown","e0321cea":"markdown","336d1508":"markdown","6d7d73e6":"markdown","6de51560":"markdown","44498d4f":"markdown","c925c6f4":"markdown","9b0a70ef":"markdown","dc66486a":"markdown"},"source":{"ad1ea7b4":"# Importing Packages\nimport numpy as np\nimport pandas as pd\nimport statistics\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nsns.set() # By default seaborn theme, scaling, and color palette\n\nimport os\nos.getcwd()","33827137":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","97e9097b":"# Import dataset\na = pd.read_csv('\/kaggle\/input\/bank-marketing\/bank-marketing.csv')\ndf = pd.DataFrame(a)\n\n# Understanding the data\ndf.head()","496fdda0":"# Checking data_types\ndf.info()","1566b294":"# Checking shape : Total number of rows and columns\ndf.shape","00fab832":"# Checking Unique values \ndf.nunique()","7ee9700f":"# Checking duplicates\ndf[df.duplicated()]","d92535cf":"# Checking missing values\ndf.isnull().sum()","808460e7":"df.describe()","4b9d304b":"print('Mean is',df['pdays'].mean())\nprint('Median is',df['pdays'].median())\nprint('Mode is',df['pdays'].mode()[0])","dff2b1ec":"# Since for relevant pdays, need to drop -1\ndf1 = df[df['pdays'] != -1]\ndf1.info()","878a9281":"print('Mean is',df1['pdays'].mean())\nprint('Median is',df1['pdays'].median())\nprint('Mode is',df1['pdays'].mode()[0])","081c4e4f":"plt.title('Education Graph without -1 pdays')\nplt.xlabel('balance')\ndf2 = df1.groupby(['education']).median()['balance']\ndf2.plot(kind='barh',color='cyan')\nplt.show()\n\nprint('Unknown has highest median',df2.max())","8bdbbc3c":"print('Boxplot without -1 pdays')\nsns.boxplot(df1['pdays'])\nplt.show()","d7901496":"# Handling the Outliers\ndf2 = df1[df1['pdays'] >= 600]\ndf1 = df1.drop(df2.index,axis=0)\n\nprint('Boxplot with no Outliers')\nsns.boxplot(df1['pdays'])\nplt.show()","6302998b":"# Resultant New Data after filtering out ouliers & -1 pdays\ndf1.info()","982f5538":"df1['response'] = np.where(df1['response']=='no',0,1)","a26ace45":"df1.head()","8237bab7":"cols = df1.columns.to_list()\nnum_cols= df1._get_numeric_data().columns.to_list()\ncat_cols = list(set(cols) - set(num_cols))\n\nprint('Numerical Columns')\nprint(num_cols)\nprint('\\nCategorical Columns')\nprint(cat_cols)","35ae7cdf":"for i in df1[num_cols]:\n    sns.barplot(df1['response'],df1[i])\n    plt.show()","9ce4d94d":"for i in df1[cat_cols]:\n    sns.barplot(df1['response'],df1[i],hue=df1[i])\n    plt.legend(bbox_to_anchor=(1.01,.99),borderaxespad=0,loc=2)\n    plt.show()","ded680f4":"# Yes previous campaign data is useful & pdays, poutcome is associated with the target variable 'response'","d3105664":"# The -1 pdays or missing values or outliers are filtered out as we would not be using that in Modelling.","0c8bf58a":"# Correlation Matrix on Resultant Data\nplt.figure(figsize=(12,7))\ncorr = df1.corr()\nsns.heatmap(corr,annot=True,cmap= 'twilight_shifted')\nplt.show()","6cba1a90":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import recall_score, precision_score, f1_score, confusion_matrix, accuracy_score","b8b8aece":"# Describing the resultant data \ndf1.reset_index(inplace = True)\ndf1.drop(['index'],axis=1,inplace=True)\ndf1.describe()","984a15fa":"# Understanding the resultant data \ndf1.head()","c85ff7ba":"# Since contact,marital,education,month,poutcome,loan,job,default,housing,targeted column is categorical so firstly need to handle it\ndf1[['education', 'contact', 'default', 'loan', 'housing', 'targeted', 'month', 'poutcome', 'job', 'marital']].nunique()","cd95830b":"# One-Hot Encoding\nE1 = pd.get_dummies(df1['education'],drop_first=True,prefix='education')\nE2 = pd.get_dummies(df1['contact'],drop_first=True,prefix='contact')\nE3 = pd.get_dummies(df1['default'],drop_first=True,prefix='default')\nE4 = pd.get_dummies(df1['loan'],drop_first=True,prefix='loan')\nE5 = pd.get_dummies(df1['housing'],drop_first=True,prefix='housing')\nE6 = pd.get_dummies(df1['targeted'],drop_first=True,prefix='targeted')\nE7 = pd.get_dummies(df1['month'],drop_first=True,prefix='month')\nE8 = pd.get_dummies(df1['poutcome'],drop_first=True,prefix='poutcome')\nE9 = pd.get_dummies(df1['job'],drop_first=True,prefix='job')\nE10 = pd.get_dummies(df1['marital'],drop_first=True,prefix='marital')\n\ndfd = pd.concat([E1,E2,E3,E4,E5,E6,E7,E8],axis=1)\ndfd.head()","87e95755":"# Actual Dummies\ndf1.drop(['poutcome', 'targeted', 'month', 'loan', 'education', 'housing', 'contact', 'marital', 'job', 'default'],axis=1,inplace=True)\ndf_dummy = pd.concat([df1,dfd],axis=1)\ndf_dummy.head()","0205d764":"## Generally X - Independent variable, y - Dependent variable\n\nX = df_dummy.drop(['response'],axis=1)\ny = df_dummy['response']\n\nprint(X.head())\nprint(y.head())","866f5a61":"# Train-Test split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.3,random_state=11)","d2c7be6e":"# Checking data split\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)\n","1d91a79b":"# Scaling\n\n# Applying Standard Scaling to make both of them comparable\nfrom sklearn.preprocessing import StandardScaler\nmm = StandardScaler()\n\nX_train = mm.fit_transform(X_train)\nX_test = mm.fit_transform(X_test)","f57c73cc":"# Logistic Regression code\nlg = LogisticRegression()\nlg.fit(X_train,y_train)\n\ny_train_pred = lg.predict(X_train)\ny_test_pred = lg.predict(X_test)\n\n\nprint(y_train_pred)\nprint(y_test_pred)","df70e5d0":"# Validating on Training\nprint(recall_score(y_train,y_train_pred))\nprint(precision_score(y_train,y_train_pred))\nprint(f1_score(y_train,y_train_pred))\nprint(accuracy_score(y_train,y_train_pred))\nprint('\\n')\n\n# Validating on Testing\nprint(recall_score(y_test,y_test_pred))\nprint(precision_score(y_test,y_test_pred))\nprint(f1_score(y_test,y_test_pred))\nprint(accuracy_score(y_test,y_test_pred))\n","0dc9eea4":"# Confusion Matrix\nprint('Confusion Matrix for training data is:')\nprint(confusion_matrix(y_train, y_train_pred))\nprint('\\n')\nprint('Confusion Matrix for testing data is:')\nprint(confusion_matrix(y_test,y_test_pred))","dd54db7a":"# Recursive feature Elimination","d603f5a3":"# P Values & Vif","81eeebf2":"# K fold Cross Validation","8e2fc652":"# Feature Selection","b856f97b":"from sklearn.ensemble import RandomForestClassifier\n\nrf_model = RandomForestClassifier(max_depth=14,min_samples_split=7)\nrf_model.fit(X_train,y_train)\n\ny_train_pred = rf_model.predict(X_train)\ny_test_pred = rf_model.predict(X_test)\n\nprint(y_train_pred)\nprint(y_test_pred)\n","c9eb6637":"# Validating on Training\nprint(recall_score(y_train,y_train_pred))\nprint(precision_score(y_train,y_train_pred))\nprint(f1_score(y_train,y_train_pred))\nprint(accuracy_score(y_train,y_train_pred))\nprint('\\n')\n\n# Validating on Testing\nprint(recall_score(y_test,y_test_pred))\nprint(precision_score(y_test,y_test_pred))\nprint(f1_score(y_test,y_test_pred))\nprint(accuracy_score(y_test,y_test_pred))\n","a8a67403":"# Confusion Matrix\nprint('Confusion Matrix for training data is:')\nprint(confusion_matrix(y_train, y_train_pred))\nprint('\\n')\nprint('Confusion Matrix for testing data is:')\nprint(confusion_matrix(y_test,y_test_pred))","2e7fb0d8":"# K fold Cross Validation","637be411":"# Feature Selection","6141b730":"##### The necessary transformations for the categorical variables and the numeric variables","dc55f65f":"#### Make a box plot for pdays. Do you see any outliers?","bcec1e68":"#### Bi-Variate Analysis | EDA","bb49ec45":"##### Visualizing Categorical Features with Response","3ff78d25":"### Logistic Regression\n","a09846d8":"Tasks\n\n\nRead in the file and get basic information about the data, including numerical summaries.\n- Describe the pdays column, make note of the mean, median and minimum values. Anything\nfishy in the values?\n- Describe the pdays column again, this time limiting yourself to the relevant values of pdays. How\ndifferent are the mean and the median values?\n- Plot a horizontal bar graph with the median values of balance for each education level value.\nWhich group has the highest median?\n- Make a box plot for pdays. Do you see any outliers?\nThe final goal is to make a predictive model to predict if the customer will respond positively to the\ncampaign or not. The target variable is \u201cresponse\u201d.\nFirst, perform bi-variate analysis to identify the features that are directly associated with the target\nvariable. You can refer to the notebook we used for the EDA discussion.\n- Convert the response variable to a convenient form\n- Make suitable plots for associations with numerical features and categorical features\u2019\nAre the features about the previous campaign data useful?\nAre pdays and poutcome associated with the target?\nIf yes, and if you plan to use them \u2013 how do you handle the pdays column with a value of -1 where the\nprevious campaign data is missing? Explain your approach and your decision.\nBefore the predictive modeling part, make sure to perform \u2013\n- The necessary transformations for the categorical variables and the numeric variables\n- Handle variables corresponding to the previous campaign\n- Train test split\n\nPredictive model 1: Logistic regression\n- Make a predictive model using logistic regression\n- Use RFE to select top n features in an automated fashion (choose n as you see fit)\n- Using p values and VIF, get rid of the redundant features\n- Estimate the model performance using k fold cross validation\n- What is the precision, recall, accuracy of your model?\n- Which features are the most important from your model?\n\nPredictive model 2: Random Forest\n- Make a predictive model using random forest technique\n- Use not more than 50 trees, and control the depth of the trees to prevent overfitting\n- Estimate the model performance using k fold cross validation\n- What is the precision, recall, accuracy of your model?\n- Using the feature importance values from the Random Forest module, identify the most\nimportant features for the model\nCompare the performance of the Random Forest and the logistic model \u2013\n- Evaluate both models on the test set\n- Which metric did you choose and why?\n- Which model has better performance on the test set?\n- Compare the feature importance from the different models \u2013 do they agree? Are the top\nfeatures similar in both models?","ee7e65de":"#### Describe the pdays column again, this time limiting yourself to the relevant values of pdays. How different are the mean and the median values?\n","e0321cea":"### Random Forest","336d1508":"##### Identifying categorical and numerical columns\n\n","6d7d73e6":"#### Plot a horizontal bar graph with the median values of balance for each education level value. Which group has the highest median?","6de51560":"## Bank Marketing Prediction\n","44498d4f":"##### Visualizing Numerical Features with Response","c925c6f4":"#### Make suitable plots for associations with numerical features and categorical features","9b0a70ef":"#### Convert the response variable to a convenient form","dc66486a":"#### Describe the pdays column, make note of the mean, median and minimum values. Anything fishy in the values?"}}