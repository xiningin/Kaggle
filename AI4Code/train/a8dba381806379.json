{"cell_type":{"5cae0914":"code","23cdc9b3":"code","e74cb84c":"code","16decaf2":"code","5d903b5c":"code","e1ac061f":"code","7ed8f74a":"code","d3c9cb93":"code","4fc78136":"code","4012e18c":"code","d8e8cb81":"code","ab7dd3e6":"code","d26791d1":"code","357ae657":"code","98984f79":"code","1b7593b4":"code","18156289":"code","e66faeda":"code","022d9416":"code","c2098b71":"code","89838c92":"code","ca817181":"code","99b744f0":"code","fab727fc":"code","7b4eaae6":"code","26ad21de":"code","bc96f9c0":"code","ddb9f7fd":"code","5ec0b21d":"code","205aded4":"code","c6401109":"code","6f293bd3":"code","275a5874":"code","e66a1cf2":"code","8df9bd32":"code","b6113316":"code","833f31a7":"code","f48c24ca":"code","bea6fda0":"code","24cbe413":"code","32a7815f":"code","9eddf141":"code","473ced80":"code","008aaedf":"code","1312eaee":"code","72a62f59":"code","1907b30e":"code","a0a7f0a0":"code","b626fcfd":"code","1011ba30":"code","08b19803":"code","1cd1bfe2":"code","01eb105b":"code","385455aa":"code","8770dc0d":"code","40d18432":"code","61bce3e8":"code","123d84ba":"code","76518180":"code","b543e9c3":"code","789d4598":"code","de8b1035":"markdown","91667ebc":"markdown","8ec9c71b":"markdown","e0d19858":"markdown","640b7841":"markdown","c5185da8":"markdown","962fca43":"markdown","48cff37f":"markdown","d37e0eff":"markdown","f587208e":"markdown","70c7e1ee":"markdown","ce3ad3b0":"markdown","93ad78cc":"markdown","eabaff76":"markdown","7c19a2ce":"markdown","0d056f91":"markdown","8216d4b7":"markdown","2b4c8cab":"markdown","fe137ada":"markdown","77624e72":"markdown"},"source":{"5cae0914":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n","23cdc9b3":"df = pd.read_csv('..\/input\/abalone.csv')","e74cb84c":"df.head()","16decaf2":"df.describe()","5d903b5c":"df['age'] = df['Rings']+1.5\ndf = df.drop('Rings', axis = 1)","e1ac061f":"sns.heatmap(df.isnull())","7ed8f74a":"sns.pairplot(df)","d3c9cb93":"df.info()","4fc78136":"numerical_features = df.select_dtypes(include = [np.number]).columns\ncategorical_features = df.select_dtypes(include = [np.object]).columns","4012e18c":"numerical_features","d8e8cb81":"categorical_features","ab7dd3e6":"plt.figure(figsize = (20,7))\nsns.heatmap(df[numerical_features].corr(),annot = True)","d26791d1":"sns.countplot(x = 'Sex', data = df, palette = 'Set3')","357ae657":"plt.figure(figsize = (20,7))\nsns.swarmplot(x = 'Sex', y = 'age', data = df, hue = 'Sex')\nsns.violinplot(x = 'Sex', y = 'age',data = df)","98984f79":"# outlier handling\ndf = pd.get_dummies(df)\ndummy_df = df","1b7593b4":"var = 'Viscera weight'\nplt.scatter(x = df[var], y = df['age'])\nplt.grid(True)","18156289":"df.drop(df[(df['Viscera weight'] > 0.5) &\n          (df['age'] < 20)].index, inplace = True)\ndf.drop(df[(df['Viscera weight']<0.5) & (\ndf['age'] > 25)].index, inplace = True)","e66faeda":"var = 'Shell weight'\nplt.scatter(x = df[var], y = df['age'])\nplt.grid(True)","022d9416":"df.drop(df[(df['Shell weight'] > 0.6) &\n          (df['age'] < 25)].index, inplace = True)\ndf.drop(df[(df['Shell weight']<0.8) & (\ndf['age'] > 25)].index, inplace = True)","c2098b71":"var = 'Shucked weight'\nplt.scatter(x = df[var], y = df['age'])\nplt.grid(True)","89838c92":"df.drop(df[(df['Shucked weight'] >= 1) &\n          (df['age'] < 20)].index, inplace = True)\ndf.drop(df[(df['Viscera weight']<1) & (\ndf['age'] > 20)].index, inplace = True)","ca817181":"var = 'Whole weight'\nplt.scatter(x = df[var], y = df['age'])\nplt.grid(True)","99b744f0":"df.drop(df[(df['Whole weight'] >= 2.5) &\n          (df['age'] < 25)].index, inplace = True)\ndf.drop(df[(df['Whole weight']<2.5) & (\ndf['age'] > 25)].index, inplace = True)","fab727fc":"var = 'Diameter'\nplt.scatter(x = df[var], y = df['age'])\nplt.grid(True)","7b4eaae6":"df.drop(df[(df['Diameter'] <0.1) &\n          (df['age'] < 5)].index, inplace = True)\ndf.drop(df[(df['Diameter']<0.6) & (\ndf['age'] > 25)].index, inplace = True)\ndf.drop(df[(df['Diameter']>=0.6) & (\ndf['age'] < 25)].index, inplace = True)","26ad21de":"var = 'Height'\nplt.scatter(x = df[var], y = df['age'])\nplt.grid(True)","bc96f9c0":"df.drop(df[(df['Height'] > 0.4) &\n          (df['age'] < 15)].index, inplace = True)\ndf.drop(df[(df['Height']<0.4) & (\ndf['age'] > 25)].index, inplace = True)","ddb9f7fd":"var = 'Length'\nplt.scatter(x = df[var], y = df['age'])\nplt.grid(True)","5ec0b21d":"df.drop(df[(df['Length'] <0.1) &\n          (df['age'] < 5)].index, inplace = True)\ndf.drop(df[(df['Length']<0.8) & (\ndf['age'] > 25)].index, inplace = True)\ndf.drop(df[(df['Length']>=0.8) & (\ndf['age'] < 25)].index, inplace = True)","205aded4":"X = df.drop('age', axis = 1)\ny = df['age']","c6401109":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.feature_selection import SelectKBest","6f293bd3":"standardScale = StandardScaler()\nstandardScale.fit_transform(X)\n\nselectkBest = SelectKBest()\nX_new = selectkBest.fit_transform(X, y)\n\nX_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size = 0.25)\n\n\n\n\n\n\n\n\n\n","275a5874":"from sklearn.linear_model import LinearRegression","e66a1cf2":"lm = LinearRegression()\nlm.fit(X_train, y_train)","8df9bd32":"y_train_pred = lm.predict(X_train)\ny_test_pred = lm.predict(X_test)\n","b6113316":" from sklearn.metrics import mean_absolute_error, mean_squared_error\ns = mean_squared_error(y_train, y_train_pred)\nprint('Mean Squared error of training set :%2f'%s)\n\np = mean_squared_error(y_test, y_test_pred)\nprint('Mean Squared error of testing set :%2f'%p)","833f31a7":"from sklearn.metrics import r2_score\ns = r2_score(y_train, y_train_pred)\nprint('R2 Score of training set:%.2f'%s)\n\np = r2_score(y_test, y_test_pred)\nprint('R2 Score of testing set:%.2f'%p)\n","f48c24ca":"from sklearn.linear_model import Ridge\n","bea6fda0":"ridge_mod = Ridge(alpha=0.01, normalize=True)\nridge_mod.fit(X_train, y_train)\nridge_mod.fit(X_test, y_test)\nridge_model_pred = ridge_mod.predict(X_test)\nridge_mod.score(X_train, y_train)","24cbe413":"ridge_mod.score(X_test, y_test)","32a7815f":"plt.scatter(y_test, ridge_model_pred)\nplt.xlabel('True Values')\nplt.ylabel('Predictions')\n","9eddf141":"from sklearn.svm import SVR","473ced80":"# LINEAR KERNEL\n\nsvr = SVR(kernel = 'linear')\nsvr.fit(X_train, y_train)\nsvr.fit(X_test, y_test)","008aaedf":"y_train_pred = svr.predict(X_train)\ny_test_pred = svr.predict(X_test)\n\nsvr.score(X_train, y_train)\n","1312eaee":"\nsvr.score(X_test, y_test)","72a62f59":" from sklearn.ensemble import RandomForestRegressor\n","1907b30e":" regr = RandomForestRegressor(max_depth=2, random_state=0,\n                              n_estimators=100)\n","a0a7f0a0":"regr.fit(X_train, y_train)\nregr.fit(X_test, y_test)","b626fcfd":"y_train_pred = regr.predict(X_train)\ny_test_pred = regr.predict(X_test)\n\nregr.score(X_train, y_train)","1011ba30":"regr.score(X_test, y_test)","08b19803":"from sklearn.ensemble import GradientBoostingRegressor","1cd1bfe2":"gbr = GradientBoostingRegressor()\ngbr.fit(X_train, y_train)\ngbr.fit(X_test, y_test)","01eb105b":"y_train_pred = regr.predict(X_train)\ny_test_pred = regr.predict(X_test)\n\n\nregr.score(X_train, y_train)","385455aa":"regr.score(X_test, y_test)","8770dc0d":"from sklearn.neighbors import KNeighborsRegressor","40d18432":"knn = KNeighborsRegressor(n_neighbors =4 )\nknn.fit(X_train, y_train)\nknn.fit(X_test, y_test)","61bce3e8":"y_train_pred = knn.predict(X_train)\ny_test_pred = knn.predict(X_test)\n\n\nknn.score(X_train, y_train)","123d84ba":"knn.score(X_test, y_test)","76518180":"# Hyperparameter Tuning using GridSearchCV\n\nfrom sklearn.model_selection import  GridSearchCV\nparam  = {'alpha':[0.01, 0.1, 1,10,100],\n         'solver' : ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']}\nglrm0 = GridSearchCV(estimator = Ridge(random_state=10,),\nparam_grid = param,scoring= 'r2' ,cv = 5,  n_jobs = -1)\nglrm0.fit(X_train, y_train)\nglrm0.best_params_, glrm0.best_score_\n","b543e9c3":"ridge_mod = Ridge(alpha=0.001,solver = 'sag', random_state = 10, normalize=True)\nridge_mod.fit(X_train, y_train)\nridge_mod.fit(X_test, y_test)\nridge_model_pred = ridge_mod.predict(X_test)\nridge_mod.score(X_train, y_train)","789d4598":"ridge_mod.score(X_test, y_test)","de8b1035":"# 1)Linear regression","91667ebc":"# Feature Selection and Standardization","8ec9c71b":"# Attribute Information:\n\nGiven is the attribute name, attribute type, the measurement unit and a brief description. The number of rings is the value to predict: either as a continuous value or as a classification problem. \n\nName \/ Data Type \/ Measurement Unit \/ Description \n----------------------------- \n\nSex \/ nominal \/ -- \/ M, F, and I (infant) \n\nLength \/ continuous \/ mm \/ Longest shell measurement \n\nDiameter\t\/ continuous \/ mm \/ perpendicular to length \n\nHeight \/ continuous \/ mm \/ with meat in shell \n\nWhole weight \/ continuous \/ grams \/ whole abalone \n\nShucked weight \/ continuous\t\/ grams \/ weight of meat \n\nViscera weight \/ continuous \/ grams \/ gut weight (after bleeding) \n\nShell weight \/ continuous \/ grams \/ after being dried \n\nRings \/ integer \/ -- \/ +1.5 gives the age in years \n\n","e0d19858":"# Data Preprocessing","640b7841":"# import libraries","c5185da8":"# 2)Ridge","962fca43":"# Data Description\n\n\nPredicting the age of abalone from physical measurements. The age of abalone is determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope -- a boring and time-consuming task. Other measurements, which are easier to obtain, are used to predict the age. Further information, such as weather patterns and location (hence food availability) may be required to solve the problem. \n\nFrom the original data examples with missing values were removed (the majority having the predicted value missing), and the ranges of the continuous values have been scaled for use with an ANN (by dividing by 200).\n\n","48cff37f":"https:\/\/www.kaggle.com\/ragnisah\/eda-abalone-age-prediction","d37e0eff":"# 4) RandomForestRegression","f587208e":"After hyperparameter tuning, CV score has improve slightly while, R2_Score has decreased showing base model was overfit. \n\nwe use tuning on different different model. ","70c7e1ee":"you have seen the performance of each one of above model.\n\nso according to you which model should we start or choose? \n\n\"Suppose there exist two explanations for an occurrence. In this case the simpler one is usually better. Another way of saying it is that the more assumptions you have to make, the more unlikely an explanation.\" Hence, starting with the simplest model Ridge, for various reasons:\n\n        - Feature Dimension is less\n        - No misisng values\n        - Few categorical features","ce3ad3b0":"# EDA","93ad78cc":"# Model Selection","eabaff76":"    Whole Weight is almost linearly varying with all other features except age\n    Heigh has least linearity with remaining features\n    Age is most linearly proprtional with Shell Weight followed by Diameter and length\n    Age is least correlated with Shucked Weight\n","7c19a2ce":"# 3)Support vector Regression","0d056f91":"# 6)KNeighborsRegressor","8216d4b7":"# Hyperparameter Tunning Using GridSearchCV","2b4c8cab":"**Key insight:**\n\n- All numerical features but 'sex'\n        - Though features are not normaly distributed, are close to normality\n        - None of the features have minimum = 0 except Height (requires re-check)\n        - Each feature has difference scale range\n","fe137ada":"    Male : age majority lies in between 7.5 years to 19 years\n    Female: age majority lies in between 8 years to 19 years\n    Immature: age majority lies in between 6 years to < 10 years\n","77624e72":"# 5)Gradient Boosting Regressor"}}