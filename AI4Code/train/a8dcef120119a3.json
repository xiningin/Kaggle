{"cell_type":{"36736563":"code","6d2e855f":"code","1d35e2bc":"code","d45eb473":"code","424dba30":"code","134501ca":"code","cd2f1b8c":"code","8cf39dab":"code","5beec022":"code","34ead528":"code","e8788905":"code","0f5c3b29":"code","daa1e94f":"code","92337f95":"code","e5b7a530":"code","1c226882":"code","efb6fb2c":"code","86d4d05e":"code","2067e2ed":"code","c6092674":"code","d999d91c":"code","32b7e440":"code","306b8f84":"code","4c1aaccc":"code","b68d5140":"code","198d5bf8":"code","ab982a0a":"code","4f5e32f5":"code","a72b3032":"code","40fd2ee5":"code","7fea14a3":"code","b78502c4":"code","d8be53f4":"code","13a1d4ff":"code","a6483c3b":"code","9d45eeac":"code","c846c399":"code","e519c53e":"markdown","d3b99cb7":"markdown","87d2c035":"markdown","538047e8":"markdown","5ffc4de8":"markdown","6577ba19":"markdown","8fe07729":"markdown","b31935f8":"markdown","64bf736d":"markdown","3215003e":"markdown","3ce3993f":"markdown","808c30ac":"markdown","a4945a9d":"markdown","369170af":"markdown","039ea9c5":"markdown","b40e70fd":"markdown","293b05be":"markdown","44d5866f":"markdown","54abfe9d":"markdown","0ee1c9d0":"markdown","e84fd078":"markdown","9c0a7be6":"markdown","2e8197f4":"markdown","9c50a413":"markdown"},"source":{"36736563":"import numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nimport sklearn\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\n\n\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\n\n\n\nfrom skopt import BayesSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import RepeatedKFold\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sqlalchemy import Table, Column, Float, Integer, BigInteger\nimport missingno as msno\n\nfrom sklearn.feature_selection import SelectFromModel\n","6d2e855f":"df = pd.read_csv('..\/input\/company-bankruptcy-prediction\/data.csv')\ndf.head()","1d35e2bc":"df.shape","d45eb473":"plt.figure(figsize=(5,5))\nmsno.matrix(df)\nplt.show()","424dba30":"plt.figure(figsize=(4,4))\nmsno.heatmap(df)\nplt.show()","134501ca":"df.info()","cd2f1b8c":"plt.figure(figsize=(12,6))\nsns.countplot(df['Bankrupt?'])\nplt.show()","8cf39dab":"plt.figure(figsize=(17,17))\nsns.heatmap(df.corr(), annot=False, cmap='coolwarm')\nplt.show()","5beec022":"y = df['Bankrupt?']\nX = df.drop('Bankrupt?', axis=1)","34ead528":"from imblearn.over_sampling import SMOTE\nover = SMOTE()\nX, y = over.fit_resample(X, y)","e8788905":"X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=97, test_size=0.2)","0f5c3b29":"print(X_train.shape,y_train.shape)\nprint(X_test.shape,y_test.shape)","daa1e94f":"sc = StandardScaler()\nX_train_sc = sc.fit_transform(X_train)\nX_train_sc = pd.DataFrame(X_train_sc, columns=X_train.columns, index=X_train.index)","92337f95":"sel_ = SelectFromModel(LogisticRegression(C=1, penalty='l1',solver='liblinear'))\nsel_.fit(X_train_sc, y_train)","e5b7a530":"sel_.get_support()","1c226882":"sel_.estimator_.coef_","efb6fb2c":"selected_feat = X_train.columns[(sel_.get_support())]\nprint('total features: {}'.format((X_train.shape[1])))\nprint('selected features: {}'.format(len(selected_feat)))\nprint('features with coefficients shrank to zero: {}'.format(\n      np.sum(sel_.estimator_.coef_ == 0)))","86d4d05e":"np.sum(sel_.estimator_.coef_ == 0)","2067e2ed":"removed_feats = X_train.columns[(sel_.estimator_.coef_ == 0).ravel().tolist()]\nremoved_feats = removed_feats.to_list()","c6092674":"X_train_sc.drop(removed_feats, axis=1, inplace=True)","d999d91c":"X_train_sc.shape, y_train.shape","32b7e440":"X_test_sc = sc.transform(X_test)\nX_test_sc = pd.DataFrame(X_test_sc, columns=X_test.columns, index=X_test.index)\nX_test_sc.drop(removed_feats, axis=1, inplace=True)","306b8f84":"X_test_sc.shape, y_test.shape","4c1aaccc":"pc = PCA(n_components=len(X_train_sc.columns))\nX_train_pc=pc.fit_transform(X_train_sc)\nPC_df_train=pd.DataFrame(X_train_pc,columns=['PC_' +str(i) for i in range(1,pc.n_components_+1)])","b68d5140":"PC_df_train","198d5bf8":"plt.figure(figsize=(12,6))\nplt.plot(PC_df_train.std())\nplt.title('Scree Plot - PCA components')\nplt.xlabel('Principal Component')\nplt.xticks(rotation=90)\nplt.ylabel('Standard deviation')\nplt.show()","ab982a0a":"pc = PCA(n_components=15)\nX_train_pc=pc.fit_transform(X_train_sc)\nPC_df_train=pd.DataFrame(X_train_pc,columns=['PC_' +str(i) for i in range(1,pc.n_components_+1)])","4f5e32f5":"X_test_pc = pc.transform(X_test_sc)\nPC_df_test=pd.DataFrame(X_test_pc,columns=['PC_' +str(i) for i in range(1,pc.n_components_+1)])","a72b3032":"print(PC_df_train.shape)\ny_train.shape","40fd2ee5":"classifier = LogisticRegression()\nclassifier.fit(PC_df_train,y_train)\ny_lr=classifier.predict(X_test_pc)","7fea14a3":"print('Confusion Matrix \\n',confusion_matrix(y_lr,y_test))\nprint()\nprint('Accuracy Score \\n', accuracy_score(y_lr,y_test))\nprint()\nprint('Classification Report \\n',classification_report(y_lr,y_test))","b78502c4":"classifier = SVC()\nclassifier.fit(pc.fit_transform(X_train_sc),y_train)\ny_svc=classifier.predict(X_test_pc)\n\nprint('Confusion Matrix \\n',confusion_matrix(y_svc,y_test))\nprint()\nprint('Accuracy Score \\n', accuracy_score(y_svc,y_test))\nprint()\nprint('Classification Report \\n',classification_report(y_svc,y_test))","d8be53f4":"classifier = RandomForestClassifier()\nclassifier.fit(X_train_pc,y_train)\ny_rfc=classifier.predict(X_test_pc)\n\nprint('Confusion Matrix \\n',confusion_matrix(y_rfc,y_test))\nprint()\nprint('Accuracy Score \\n', accuracy_score(y_rfc,y_test))\nprint()\nprint('Classification Report \\n',classification_report(y_rfc,y_test))","13a1d4ff":"classifier = GradientBoostingClassifier()\nclassifier.fit(X_train_pc,y_train)\ny_gbc=classifier.predict(X_test_pc)\n\nprint('Confusion Matrix \\n',confusion_matrix(y_gbc,y_test))\nprint()\nprint('Accuracy Score \\n', accuracy_score(y_gbc,y_test))\nprint()\nprint('Classification Report \\n',classification_report(y_gbc,y_test))","a6483c3b":"classifier = GaussianNB()\nclassifier.fit(X_train_pc,y_train)\ny_gb=classifier.predict(X_test_pc)\n\nprint('Confusion Matrix \\n',confusion_matrix(y_gb,y_test))\nprint()\nprint('Accuracy Score \\n', accuracy_score(y_gb,y_test))\nprint()\nprint('Classification Report \\n',classification_report(y_gb,y_test))","9d45eeac":"classifier = XGBClassifier()\nclassifier.fit(X_train_pc, y_train)\ny_xg=classifier.predict(X_test_pc)\n\nprint('Confusion Matrix \\n',confusion_matrix(y_xg,y_test))\nprint()\nprint('Accuracy Score \\n', accuracy_score(y_xg,y_test))\nprint()\nprint('Classification Report \\n',classification_report(y_xg,y_test))","c846c399":"lr_df = pd.DataFrame(data=[f1_score(y_test,y_lr),accuracy_score(y_test, y_lr), recall_score(y_test, y_lr), precision_score(y_test, y_lr), roc_auc_score(y_test, y_lr)], \n             columns=['Logistic Regression'], index=[\"F1\",\"Accuracy\", \"Recall\", \"Precision\", \"ROC AUC Score\"])\nrf_df = pd.DataFrame(data=[f1_score(y_test,y_rfc),accuracy_score(y_test, y_rfc), recall_score(y_test, y_rfc),precision_score(y_test, y_rfc), roc_auc_score(y_test, y_rfc)], \n             columns=['Random Forest Score'],index=[\"F1\",\"Accuracy\", \"Recall\", \"Precision\", \"ROC AUC Score\"])\nnb_df = pd.DataFrame(data=[f1_score(y_test,y_gb),accuracy_score(y_test, y_gb), recall_score(y_test, y_gb), precision_score(y_test, y_gb), roc_auc_score(y_test, y_gb)], \n             columns=['Naive Bayes'], index=[\"F1\",\"Accuracy\", \"Recall\", \"Precision\", \"ROC AUC Score\"])\n\nxg_df = pd.DataFrame(data=[f1_score(y_test,y_xg),accuracy_score(y_test, y_xg), recall_score(y_test, y_xg), precision_score(y_test, y_xg), roc_auc_score(y_test, y_xg)], \n             columns=['XG Boost'], index=[\"F1\",\"Accuracy\", \"Recall\", \"Precision\", \"ROC AUC Score\"])\ngbc_df = pd.DataFrame(data=[f1_score(y_test,y_gbc),accuracy_score(y_test, y_gbc), recall_score(y_test, y_gbc), precision_score(y_test, y_gbc), roc_auc_score(y_test,y_gbc)], \n             columns=['Gradient Boosting'], index=[\"F1\",\"Accuracy\", \"Recall\", \"Precision\", \"ROC AUC Score\"])\nsvc_df = pd.DataFrame(data=[f1_score(y_test,y_svc),accuracy_score(y_test, y_svc), recall_score(y_test, y_svc), precision_score(y_test, y_svc), roc_auc_score(y_test,y_svc)], \n             columns=['Gradient Boosting'], index=[\"F1\",\"Accuracy\", \"Recall\", \"Precision\", \"ROC AUC Score\"])\n\n\ndf_models = round(pd.concat([lr_df,rf_df,nb_df,gbc_df,xg_df,svc_df], axis=1),3)\ncolors = [\"bisque\",\"ivory\",\"sandybrown\",\"steelblue\",\"lightsalmon\"]\ncolormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\nbackground_color = \"white\"\n\nfig = plt.figure(figsize=(18,26)) # create figure\ngs = fig.add_gridspec(4, 2)\ngs.update(wspace=0.1, hspace=0.5)\nax0 = fig.add_subplot(gs[0, :])\n\nsns.heatmap(df_models.T, cmap=colormap,annot=True,fmt=\".1%\",vmin=0,vmax=0.95, linewidths=2.5,cbar=False,ax=ax0,annot_kws={\"fontsize\":16})\nfig.patch.set_facecolor(background_color) # figure background color\nax0.set_facecolor(background_color) \n\nax0.text(0,-0.5,'Model Comparison',fontsize=20,fontweight='bold',fontfamily='serif')\nplt.show()\n","e519c53e":"## Model Building","d3b99cb7":"### Seperating dependent and independent features","87d2c035":"#### missingno.heatmap visualizes the correlation matrix about the locations of missing values in columns.","538047e8":"from the above analysis we can clearly see the only int and float value is these\n","5ffc4de8":"### Corelation Matrix","6577ba19":"# Conclusion\n### From the above model analysis we can clearly see Random Forest and XG Boost is giving accuracy of 96%","8fe07729":"## Logistic Regression","b31935f8":"## Feature elemination - using L1 Regularization","64bf736d":"### Standarization","3215003e":"## Scree Plot - PCA Analysis\nIn multivariate statistics, a scree plot is a line plot of the eigenvalues of factors or principal components in an analysis. The scree plot is used to determine the number of factors to retain in an exploratory factor analysis (FA) or principal components to keep in a principal component analysis (PCA)\n\nTo select number of principal components elbow method is used\nWe can clearly, proper elbow is not formed in the below graph, so we can select all the components","3ce3993f":"## Info about the type of data type for feature present in the dataset ","808c30ac":"## SVC Classifier","a4945a9d":"## PCA Visualization","369170af":"### Spliting of data into train and test ","039ea9c5":"from the above graph we can clearly see that no null value is present in the data","b40e70fd":"## check data is balanced or imbalanced?","293b05be":"Gradient Boosting classifier","44d5866f":"### Data is imbalanced , so to balance it we will use balancing technique ..Here we are using SMOT ","54abfe9d":"From the above plot we can clearly see that data in imbalanced. To balance the data we have to do upsampling or downsampling technique","0ee1c9d0":"## Random forest Classifier","e84fd078":"## XGB Classifier","9c0a7be6":"We can see that it is forming an elbow at PC_15, So we can take 10 principal components for further analysis","2e8197f4":"## Null value check\n#### missingno.matrix-there is a bar on the right side of this diagram. This is a line plot for each row's data completeness.\n","9c50a413":"We can clearly see that the some feature are highly corelated to each other. we will eleminate it with feature elemination technique"}}