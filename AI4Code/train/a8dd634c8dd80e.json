{"cell_type":{"f1dfcc64":"code","8be27046":"code","f32aa6ad":"code","a8c6d161":"code","3f6a9a2d":"code","88e09885":"code","d9bafcf8":"code","2ee78fe9":"code","d4ea5af7":"code","31f54e8b":"code","c521b0fa":"code","3f74a532":"code","f4112044":"code","e7fdcda1":"code","d6a73047":"code","dfec0eb3":"code","ade3b3cd":"code","9379ae02":"code","5a83e510":"code","48c93889":"code","2f9538e2":"code","0311dae6":"code","4824e456":"code","8ccc758f":"code","52f9a7c0":"code","17c770ff":"code","8d66bfdd":"code","56002d33":"code","b9fff1bc":"code","6b8ee81f":"code","643a46cc":"code","b164d1c0":"code","d979c35a":"code","5d92bea1":"code","a36ec711":"code","1d994408":"code","eb7ad123":"code","08276c8b":"code","63dc159f":"code","a8c54dd8":"code","e84b9e38":"code","3be889dc":"code","a529943b":"code","7c55f3ad":"code","d30fb669":"code","7b1bf2ee":"code","798688b0":"code","2913222c":"code","dfedfead":"code","6f4f76f5":"code","3279664d":"markdown","69faacca":"markdown","c55be9e1":"markdown","80e84882":"markdown","f05cddb9":"markdown","b51483d4":"markdown","f0e89e18":"markdown","74b52004":"markdown","7e73d2d2":"markdown","2441b7e9":"markdown","f1229378":"markdown","b164c62d":"markdown","97fc0f2f":"markdown","5af7048d":"markdown","25a9752a":"markdown","e5304ea9":"markdown","4dce8bc7":"markdown","05fc904c":"markdown","753bd76c":"markdown","5915b293":"markdown","e716f013":"markdown","10b6758a":"markdown","706e29a6":"markdown","84a940c9":"markdown","5d002c9a":"markdown","09b45b2f":"markdown","01881658":"markdown","67b26cd9":"markdown","466ff01c":"markdown","70559bf8":"markdown","d5b003d9":"markdown","3925c1d0":"markdown","f5afd525":"markdown","4ca6fb84":"markdown","d09d0ecd":"markdown","a7e1012e":"markdown","2a474504":"markdown","320b8112":"markdown","928e1b5f":"markdown","62549494":"markdown","47b2986c":"markdown","033c1dcd":"markdown","5851b05e":"markdown","a681e9ed":"markdown","79a1102e":"markdown","5973d5b2":"markdown","d5e29f0b":"markdown"},"source":{"f1dfcc64":"# package details\n!pip show datasets","8be27046":"from datasets import load_dataset\n\ndataset = load_dataset('csv', data_files='..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv')\ndataset # DatasetDict","f32aa6ad":"dataset = load_dataset(\"csv\", data_files={\"train\": \"..\/input\/mlqa-hindi-processed\/mlqa_hindi.csv\",  \"validation\": \"..\/input\/mlqa-hindi-processed\/xquad.csv\"})\ndataset","a8c6d161":"dataset = load_dataset('csv', data_files='..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv', split=\"train\")\ndataset # This will be a Dataset and not DatasetDict","3f6a9a2d":"dataset_20pct = load_dataset('csv', data_files='..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv', split=\"train[:20%]\")\ndataset_80pct = load_dataset('csv', data_files='..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv', split=\"train[:80%]\")\ndataset_20pct, dataset_80pct","88e09885":"dataset_first100 = load_dataset('csv', data_files='..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv', split=\"train[:100]\")\ndataset_last50 = load_dataset('csv', data_files='..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv', split=\"train[-50:]\")\ndataset_first100, dataset_last50","d9bafcf8":"# 10-fold cross-validation (see also next section on rounding behavior):\n# The validation datasets are each going to be 10%:\n# [0%:10%], [10%:20%], ..., [90%:100%].\n# And the training datasets are each going to be the complementary 90%:\n# [10%:100%] (for a corresponding validation set of [0%:10%]),\n# [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,\n# [0%:90%] (for a validation set of [90%:100%]).\n\n# For fold0, use val_ds_folds[0] and train_ds_folds[0]\nval_ds_folds = load_dataset('csv', data_files='..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv', split=[f'train[{k}%:{k+10}%]' for k in range(0, 100, 10)])\ntrain_ds_folds = load_dataset('csv', data_files='..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv', split=[f'train[:{k}%]+train[{k+10}%:]' for k in range(0, 100, 10)])\nval_ds_folds, train_ds_folds","2ee78fe9":"dataset = load_dataset('csv', data_files='..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv', split=\"train[:20%]+train[-20%:]\")\ndataset","d4ea5af7":"dataset = load_dataset('csv', data_files='..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv', split=\"train\")\n\nsplits = dataset.train_test_split(test_size=0.2, seed=2021) # sklearn syntax\nsplits","31f54e8b":"selected  = dataset.select(range(100))\nprint(selected.shape)\nselected = dataset.select(range(1,1000, 100))\nprint(selected.shape)","c521b0fa":"dataset[42][\"question\"], dataset[\"question\"][42]","3f74a532":"dataset[42]","f4112044":"dataset[\"question\"][:5]","e7fdcda1":"try:\n    dataset[[\"question\", \"answer_text\"]]\nexcept ValueError as e:\n    print(\"Value Error!\", e)","d6a73047":"dataset[\"question\"][42] = \"Why does this not work?\"\ndataset[\"question\"][42]","dfec0eb3":"try:\n    dataset[\"new column\"] = \"single value\"\nexcept TypeError as e:\n    print(\"TypeError!\", e)\ntry:\n    dataset[\"new column2\"] = [x[0] for x in dataset[\"question\"]]\nexcept TypeError as e:\n    print(\"TypeError!\", e)","ade3b3cd":"print(splits[\"train\"])\nprint(splits[\"test\"])","9379ae02":"%%time\nshuffled = dataset.shuffle(seed=2021)","5a83e510":"%%time\nshuffled = dataset.shuffle(seed=2021)","48c93889":"%time load_dataset('squad', split=\"validation\")","2f9538e2":"%time load_dataset('squad', split=\"validation\")","0311dae6":"dataset = load_dataset('squad', split=\"validation\")\ndataset.info","4824e456":"def map_fn(example):\n    example[\"banana\"] = \"monkey\"\n    example[\"id\"] = example[\"id\"].upper()\n    \n    # since nothing is returned, this map_fn does nothing\n    \nx = dataset.select(range(1)).map(map_fn)\nx[0]","8ccc758f":"mlqa = load_dataset(\"csv\", data_files=\"..\/input\/mlqa-hindi-processed\/mlqa_hindi.csv\", split=\"train\")\n\ndef form_answers(example):\n    example[\"answers\"] = {\n        \"text\": [example[\"answer_text\"]],\n        \"answer_start\": [example[\"answer_start\"]]\n    }\n    return example\n\nnew_mlqa = mlqa.map(form_answers, remove_columns=[\"answer_text\", \"answer_start\"])\nnew_mlqa[0]","52f9a7c0":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n\ndef tokenize_function(example):\n    return tokenizer(example[\"context\"])\n\n%time tokenized = dataset.map(tokenize_function)\n[tokenized[0][x][:10] for x in [\"input_ids\", \"attention_mask\"]]","17c770ff":"%time tokenized = dataset.map(tokenize_function, batched=True, batch_size=1000)","8d66bfdd":"length_before = len(dataset)\n\ndef qa_tokenize_function(examples):\n    return tokenizer(\n            examples[\"question\"],\n            examples[\"context\"],\n            truncation=\"only_second\",\n            max_length=128,\n            stride=64,\n            return_overflowing_tokens=True,\n            return_offsets_mapping=True,\n            padding=\"max_length\",\n        )\n\n# this will error because it doesn't remove the other columns like id\n# tokenized = dataset.map(qa_tokenize_function, batched=True, batch_size=1000) \n\n%time tokenized = dataset.map(qa_tokenize_function, batched=True, batch_size=1000, remove_columns=dataset.column_names) \nlength_before, len(tokenized)","56002d33":"capitalized = dataset.map(lambda x: {\"context\": x[\"context\"].upper()}, desc=\"Capitalizing\", remove_columns=dataset.column_names)\ncapitalized[0] # should be capitalized","b9fff1bc":"# stop annoying tf warnings\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' ","6b8ee81f":"print(tokenized[0], \"\\n\")\n\ntorch_tensors = tokenized.with_format(\"torch\", columns=['attention_mask', \"input_ids\"],)\nprint(torch_tensors[0], \"\\n\")\n# print(torch_tensors[0][\"input_ids\"][:10], torch_tensors[0][\"attention_mask\"][:10])\n\nnp_tensors = tokenized.with_format(\"numpy\", columns=['attention_mask', \"input_ids\"])\nprint(np_tensors[0], \"\\n\")\n\ntf_tensors = tokenized.with_format(\"tf\", columns=['attention_mask', \"input_ids\"])\nprint(tf_tensors[0], \"\\n\")","643a46cc":"xquad = load_dataset(\"csv\", data_files=\"..\/input\/mlqa-hindi-processed\/xquad.csv\", split=\"train\")\nprint(xquad.features)\n\nfrom datasets import Value\nnew_features = xquad.features.copy()\nnew_features[\"answer_start\"] = Value('int32')\nxquad = xquad.cast(new_features)\nxquad.features","b164d1c0":"# Let's keep only the examples where the context is less than 200 words when splitting at whitespace\ndef filter_by_num_words(example):\n    return len(example[\"context\"].split()) < 200\n\n\nlength_before = len(dataset)\nfiltered = dataset.filter(filter_by_num_words)\nlength_after = len(filtered)\nlength_before, length_after, filtered[-1][\"context\"]","d979c35a":"length_before = len(dataset)\nfiltered = dataset.filter(lambda x: \"football\" in x[\"context\"].lower()) # keep examples that have the word football in the context\nlength_after = len(filtered)\nlength_before, length_after, filtered[-1][\"context\"]","5d92bea1":"original_columns = dataset.column_names\nnew_ds = dataset.remove_columns([x for x in original_columns if x != \"id\"])\nprint(original_columns)\nprint(new_ds.column_names)","a36ec711":"stream_dataset = load_dataset('oscar', \"unshuffled_deduplicated_en\", split='train', streaming=True)\nprint(next(iter(stream_dataset)))","1d994408":"shuffled_dataset = stream_dataset.shuffle(buffer_size=10_000, seed=42)","eb7ad123":"mapped_stream = stream_dataset.map(lambda x: {\"text\": x[\"text\"].upper()})\nprint(next(iter(mapped_stream)))","08276c8b":"from datasets import interleave_datasets\nfrom itertools import islice\nen_dataset = load_dataset('oscar', \"unshuffled_deduplicated_en\", split='train', streaming=True)\nfr_dataset = load_dataset('oscar', \"unshuffled_deduplicated_fr\", split='train', streaming=True)\n\nmultilingual_dataset = interleave_datasets([en_dataset, fr_dataset])\nprint(list(islice(multilingual_dataset, 2)))","63dc159f":"dataset.to_json(\"dataset.json\")\ndataset.to_csv(\"dataset.csv\", index=False) #  pandas syntax","a8c54dd8":"tokenized.save_to_disk(\"tokenized_dataset\")\n%ls tokenized_dataset","e84b9e38":"from datasets import Dataset\n\nloaded = Dataset.load_from_disk(\"tokenized_dataset\")\n{key:loaded[0][key][:10] for key in [\"input_ids\", \"attention_mask\", \"offset_mapping\"]}","3be889dc":"from datasets import concatenate_datasets\n\nmlqa = load_dataset(\"csv\", data_files=\"..\/input\/mlqa-hindi-processed\/mlqa_hindi.csv\", split=\"train\")\nxquad = load_dataset(\"csv\", data_files=\"..\/input\/mlqa-hindi-processed\/xquad.csv\", split=\"train\")\n\nconcat = concatenate_datasets([mlqa, xquad])\n\nlen(mlqa), len(xquad), len(concat)","a529943b":"from datasets import Dataset\nimport pandas as pd\n\ndf = pd.read_csv(\"..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv\")\ndataset = Dataset.from_pandas(df)\n\nmy_dict = {\"col_a\": [\"apple\", \"orange\", \"banana\"], \"col_b\": list(range(3)), \"col_c\": [True, False, True], \"col_d\": [[\"x\", \"y\", \"z\"]]*3}\ndataset = Dataset.from_dict(my_dict)","7c55f3ad":"from datasets import DatasetDict\nfrom functools import partial\n\ntokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n\nPAD_ON_RIGHT = tokenizer.padding_side == \"right\"\nFOLD = 0\nMAX_LEN = 384\nDOC_STRIDE = 128\n\nchaii_data = load_dataset(\"csv\", data_files=\"..\/input\/chaii-extra\/train_folds.csv\", split=\"train\")\nexternal_data = load_dataset(\"csv\", data_files=[\"..\/input\/chaii-extra\/mlqa_hindi.csv\", \"..\/input\/chaii-extra\/xquad.csv\"], split=\"train\")\n\ntrain_data = chaii_data.filter(lambda x: x[\"kfold\"]!=FOLD)\nvalid_data = chaii_data.filter(lambda x: x[\"kfold\"]==FOLD)\n\ndef form_answers(example):\n    example[\"answers\"] = {\n        \"text\": [example[\"answer_text\"]],\n        \"answer_start\": [example[\"answer_start\"]]\n    }\n    return example\n\ndef add_id(example, idx):\n    # validation features need a unique id\n    example[\"id\"] = \"id\" + str(idx)\n    return example\n\n\ncols = [\"context\", \"question\", \"answer_text\", \"answer_start\"]\n\ntrain_data = train_data.remove_columns([x for x in train_data.column_names if x not in cols])\nvalid_data = valid_data.remove_columns([x for x in valid_data.column_names if x not in cols])\nexternal_data = external_data.remove_columns([x for x in external_data.column_names if x not in cols])\n\nraw_dataset = DatasetDict()\nraw_dataset[\"train\"] = concatenate_datasets([train_data, external_data], axis=0)\nraw_dataset[\"validation\"] = valid_data\n\n# When using map on a DatasetDict, all splits will get mapped\nraw_dataset = raw_dataset.map(form_answers, desc=\"Formatting answers\")\nraw_dataset = raw_dataset.map(add_id, desc=\"Adding id column\", with_indices=True)","d30fb669":"def prepare_train_features(examples, tokenizer, pad_on_right, max_length, doc_stride):\n    # ref: https:\/\/github.com\/huggingface\/notebooks\/blob\/master\/examples\/question_answering.ipynb\n    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n    # left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n    # in one example possible giving several features when a context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n    tokenized_examples = tokenizer(\n        examples[\"question\" if pad_on_right else \"context\"],\n        examples[\"context\" if pad_on_right else \"question\"],\n        truncation=\"only_second\" if pad_on_right else \"only_first\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    # The offset mappings will give us a map from token to character position in the original context. This will\n    # help us compute the start_positions and end_positions.\n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n\n    # Let's label those examples!\n    tokenized_examples[\"start_positions\"] = []\n    tokenized_examples[\"end_positions\"] = []\n\n    for i, offsets in enumerate(offset_mapping):\n        # We will label impossible answers with the index of the CLS token.\n        input_ids = tokenized_examples[\"input_ids\"][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n\n        # One example can give several spans, this is the index of the example containing this span of text.\n        sample_index = sample_mapping[i]\n        answers = examples[\"answers\"][sample_index]\n        # If no answers are given, set the cls_index as answer.\n        if len(answers[\"answer_start\"]) == 0:\n            tokenized_examples[\"start_positions\"].append(cls_index)\n            tokenized_examples[\"end_positions\"].append(cls_index)\n        else:\n            # Start\/end character index of the answer in the text.\n            start_char = answers[\"answer_start\"][0]\n            end_char = start_char + len(answers[\"text\"][0])\n\n            # Start token index of the current span in the text.\n            token_start_index = 0\n            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n                token_start_index += 1\n\n            # End token index of the current span in the text.\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                token_end_index -= 1\n\n            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                tokenized_examples[\"start_positions\"].append(cls_index)\n                tokenized_examples[\"end_positions\"].append(cls_index)\n            else:\n                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n                # Note: we could go after the last offset if the answer is the last word (edge case).\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n\n    return tokenized_examples\n\n\ndef prepare_validation_features(examples, tokenizer, pad_on_right, max_length, doc_stride):\n    # ref: https:\/\/github.com\/huggingface\/notebooks\/blob\/master\/examples\/question_answering.ipynb\n    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n    # left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n    # in one example possible giving several features when a context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n    tokenized_examples = tokenizer(\n        examples[\"question\" if pad_on_right else \"context\"],\n        examples[\"context\" if pad_on_right else \"question\"],\n        truncation=\"only_second\" if pad_on_right else \"only_first\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n\n    # We keep the example_id that gave us this feature and we will store the offset mappings.\n    tokenized_examples[\"example_id\"] = []\n\n    for i in range(len(tokenized_examples[\"input_ids\"])):\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1 if pad_on_right else 0\n\n        # One example can give several spans, this is the index of the example containing this span of text.\n        sample_index = sample_mapping[i]\n        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n\n        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n        # position is part of the context or not.\n        tokenized_examples[\"offset_mapping\"][i] = [\n            (o if sequence_ids[k] == context_index else None)\n            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n        ]\n\n    return tokenized_examples","7b1bf2ee":"train_features = raw_dataset[\"train\"].map(\n    partial(\n        prepare_train_features,\n        tokenizer=tokenizer,\n        pad_on_right=PAD_ON_RIGHT,\n        max_length=MAX_LEN,\n        doc_stride=DOC_STRIDE,\n    ),\n    batched=True,\n    remove_columns=raw_dataset[\"train\"].column_names,\n    desc=\"Creating train features\"\n)\n\nvalid_features = raw_dataset[\"validation\"].map(\n    partial(\n        prepare_validation_features,\n        tokenizer=tokenizer,\n        pad_on_right=PAD_ON_RIGHT,\n        max_length=MAX_LEN,\n        doc_stride=DOC_STRIDE,\n    ),\n    batched=True,\n    remove_columns=raw_dataset[\"validation\"].column_names,\n    desc=\"Creating validation features\"\n)","798688b0":"train_features, valid_features","2913222c":"from torch.utils.data import DataLoader\n\ntrain_dataloader = DataLoader(train_features.with_format(\"torch\"), batch_size=16, num_workers=4, shuffle=True)\nvalid_dataloader = DataLoader(valid_features.with_format(\"torch\"), batch_size=32, num_workers=4, shuffle=False)","dfedfead":"# this just prevents annoying warnings from popping up\n%env TOKENIZERS_PARALLELISM=true\n\nfor x in train_dataloader:\n    break\nx","6f4f76f5":"import tensorflow as tf\n\n\ntf_train_features = train_features.with_format('tensorflow')\n\ntf_train_x = {x: tf_train_features[x].to_tensor(default_value=0, shape=[None, MAX_LEN]) for x in ['input_ids', 'attention_mask']}\ntf_train_y = {x: tf_train_features[x] for x in ['start_positions', 'end_positions']}\n\ntf_train_dataset = tf.data.Dataset.from_tensor_slices((tf_train_x, tf_train_y)).batch(32)\nnext(iter(tf_train_dataset))","3279664d":"# Concatenate Datasets\n\n### All datasets need to have the exact same features.","69faacca":"# Use with Tensorflow","c55be9e1":"# Remove columns\n\n#### Since there isn't an easy way to select a ","80e84882":"# Save dataset","f05cddb9":"# Split dataset when loading\n\n### Splitting by percentage","b51483d4":"# \ud83d\udd25 The BIGGEST advantage is\n####  the fact that the datasets are memory-mapped using Apache Arrow and cached locally. This means that only the necessary data will be loaded into memory, allowing the possibility to work with a dataset that is larger than the system memory (e.g. c4 is hundreds of GB, mc4 is several TB).  `datasets` can work on local files, data in memory (e.g. a pandas dataframe or a dict), or easily pull from over 1,500 datasets from the Hugging Face Hub. For the large datasets, a streaming mode is also possible (details later), so that you don't have to download a ton of data. Many functions mirror their analogs in sklearn or pandas, and you can even stick the dataset straight into a PyTorch DataLoader!","f0e89e18":"# Shuffling\n","74b52004":"# Filter the dataset\n\n### Use a function that returns `True` if the example should be kept, `False` if ignored.","7e73d2d2":"# Interleave streamed datasets\n### Mix multiple streams","2441b7e9":"# 10-fold split","f1229378":"## The code above loads a DatasetDict, which is basically a dict object with split names(train, validation, test) as keys and datasets as values.  \n\n## If you specify the split when loading, you can load in multiple files, as seen below.\n\n#### All of the files need to have the same column names and column types.","b164c62d":"# Most datasets on the hub have info about them","97fc0f2f":"# If the dataset has splits, access each split like a dict\n\n### `splits` is a `DatasetDict`","5af7048d":"# Loading local files\n\n### I'm putting this first because chaii gives the training and test data in csv format. Here is how you could read it.","25a9752a":"# Funkier splits\n\n### This takes the first 20% and the last 20%","e5304ea9":"# Example for chaii-qa\n\n### This is slightly modified from Abhishek's notebook here: https:\/\/www.kaggle.com\/abhishek\/hello-friends-tez-se-chaii-train-kar-lo\n\n### My version doesn't use pandas at all \ud83d\ude09","4dce8bc7":"# pandas behavior that does NOT work: selecting multiple columns at once.\n\n####  To get the same effect, remove all the columns that are not needed (if you have columns A,B,C,D and want A&B, you would have to remove C&D)","05fc904c":"# Dataset behaves both like a dict and a list\n### Can select by index first or by column name","753bd76c":"# This will tokenize the examples, adding padding, allowing for overflow, and using a stride\n\nThe number of rows and columns before mapping != rows and columns afterwards","5915b293":"<h1 style=\"text-align:center;\"> Hugging Face Datasets<\/h1>\n\n<div style=\"width:100%;text-align: center;\"> <img align=middle src=\"https:\/\/huggingface.co\/docs\/datasets\/_images\/datasets_logo.png\" alt=\"hf dataset logo\" style=\"height:300px;margin:3rem auto;\"> <\/div>\n\n\n#### Recently, `datasets` version 1.12.1 became a default package in Kaggle notebooks. This is a big relief because there were previously issues with `pyarrow` and `fsspec` (depending on which version of `datasets` you used) that made it annoying to use.\n\n#### I thought I would do a quick tour of `datasets` by showing what is possible and how to use it for this competition (full example at the very end).\n\n#### A lot of this is adapted from the [official documentation](https:\/\/huggingface.co\/docs\/datasets\/), with some tailored for the chaii-qa competition. I can't possibly cover every feature, so please explore the documentation to see the full extent of what is possible or [check out their course](https:\/\/huggingface.co\/course\/chapter1). I'm sure the developers will continue to add great new features in the future. \n\n#### Also, please explore the publicly available datasets (more than 1,500 as of Sep 30, 2021) at [huggingface.co\/datasets](https:\/\/huggingface.co\/datasets) (or [hf.co\/datasets](https:\/\/hf.co\/datasets) if don't want to type as much \ud83d\ude09). Feel free to add one!\n\n#### Lastly, the team at Hugging Face also has a [paper in EMNLP 2021](https:\/\/arxiv.org\/abs\/2109.02846) that gives a formal overview of the package.","e716f013":"# Streaming\n\nFor when the datasets are many GBs and you don't want to save it to your disk first.","10b6758a":"# Load dataset from memory (pandas or dict)","706e29a6":"# Splitting by slice","84a940c9":"# nor does it allow a column to be added like pandas","5d002c9a":"# Using batches usually results in a speed up\n\n### default batch size is 1000\n## Time taken to tokenize the dataset\nwithout batching: ~12s  \nwith batching: ~4s","09b45b2f":"# If you just want to load a file into a Dataset without it turning into a DatasetDict, use the `split` argument\n","01881658":"# Split after loading","67b26cd9":"### `prepare_train_features` and `prepare_validation_features` functions hidden in next cell","466ff01c":"# Format the dataset\n\n### Can turn lists into numpy, torch, or tensorflow tensors","70559bf8":"# Save processed dataset\n\nIf the dataset has special types (e.g. tensors) that can't save to csv or json, use `save_to_disk`","d5b003d9":"# Cast data in the dataset\n\n### This is a trivial example where int64 gets cast to int32","3925c1d0":"# Processing\n\n### `map` is your go-to function to make changes to the data.\n\n\n### `map` gets passed a single example as a `dict` if not using batches, and when using batches, it gets multiple examples as a `dict` where the keys are the column names and the values are the examples (of length batch_size).  \n\n\n### `map` must return a dict, so modifying the values in place will not change them\n\n```python\ndef map_fn(example):\n    \"\"\"\n    example looks like this when not batched:\n    {\"col a\": 1, \"col b\": \"string_val1\", \"col c\": [1,2,3]}\n    \n    example likes like this when batched (batch_size=4):\n    {\"col a\": [1,2,3,4], \"col b\": [\"string_val1\", \"val2\", \"val3\", \"val4\"], \"col c\": [[1,2,3], [2,4,6], [-1,-2,-3], [0,0,0]]}\n    \"\"\"\n    \n    # this is where you would modify the data\n    example[\"x\"] = example[\"y\"]*10\n    \n    return example\n```","f5afd525":"# Other `map` features: lambda functions and task descriptions\n\n### `remove_columns` does not remove the column name that gets returned from the function. In the example below, \"context\" is included in `dataset.column_names` but the output still has the \"context\" column because it gets returned from the function\n\n### Use `desc` to add a description next to the progress bar. Useful when running a script with multiple mapping steps","4ca6fb84":"# A common step with these QA datasets\n\n#### Getting it in the right format","d09d0ecd":"# Loading is also cached\n\n### This is downloading the dataset from the HF Hub. They have many benchmarks (squad, glue) and community-added datasets ","a7e1012e":"# That's it for now. Please check out the documentation at https:\/\/huggingface.co\/docs\/datasets\/ or the course at https:\/\/huggingface.co\/course\/chapter1 for more details.","2a474504":"# `map` is good for tokenizing","320b8112":"# lambda functions also possible for `filter`","928e1b5f":"# Selecting data\n\n### For when you don't want all of it.","62549494":"# Map a streamed dataset","47b2986c":"# Use with PyTorch DataLoader","033c1dcd":"# Load processed dataset","5851b05e":"# Shuffle a streamed dataset\n\nIt won't shuffle the entire dataset (doing so would require looping through the entire dataset), but it will shuffle one buffer (say, 10,000 examples) at a time","a681e9ed":"# Number of rows and columns before `map` does not have to equal number of rows and columns after `map`\n\n### Especially useful when tokenizing.   \n### NOTE: The number of rows for each column must be the same, so columns that are not being \"wrapped\" need to be removed.  For instance when tokenizing, you could map one example (a long string and its `id`) and get out 3 `input_ids` and 3 `attention_mask` if you return the overflowing tokens. You do not, however, get 3 `id` so `id` either must be duplicated 3 times or dropped.","79a1102e":"# Thanks for reading!\n\nI hope it was useful \ud83d\ude0a\n\nIf you spot any errors, please let me know! I'm a human, after all. Feedback welcome!\n\n<div style=\"width:100%;text-align: center;\"> \n    <iframe src=\"https:\/\/giphy.com\/embed\/xULW8v7LtZrgcaGvC0\" width=\"480\" height=\"480\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen><\/iframe>\n<\/div>","5973d5b2":"# Shuffling is cached so if you run it again, it doesn't have to do all the work over again.\n\n### This is much more important on big datasets than this one.","d5e29f0b":"# `datasets` does not allow values to be assigned"}}