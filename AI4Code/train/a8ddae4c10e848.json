{"cell_type":{"1a6a8cc9":"code","f5a337fd":"code","9bcb405b":"code","0868ce20":"code","34cecb35":"code","05da9af0":"code","61b098f7":"code","f7b8c48f":"code","2b87addc":"code","1e524712":"code","462c1195":"code","bebefbee":"code","76697c5a":"code","bad1160d":"code","af259e19":"code","9736989f":"code","e384383d":"code","319f377a":"code","c6762498":"code","154417d5":"code","c37968e6":"code","1beb68fb":"code","f11e03af":"code","c5cd8e2e":"code","292d2d60":"code","0a3beb30":"code","dfa3ba1a":"code","158891ed":"code","158e234c":"code","de9edfdc":"code","840f6441":"code","b8882eca":"markdown","5fef89f9":"markdown","c6256d98":"markdown","cafff5e0":"markdown"},"source":{"1a6a8cc9":"# import libraries\n#!pip install turicreate\nimport numpy as np\nimport pandas as pd\n#import turicreate\nfrom sklearn.preprocessing import OneHotEncoder","f5a337fd":"# libraries used\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n\n# Encoders\nfrom category_encoders import TargetEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\n# Strategic imports\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import GridSearchCV, train_test_split, RepeatedStratifiedKFold, cross_val_score\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score\n\n# Machine learning Models\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import StackingClassifier, VotingClassifier\n\nfrom tensorflow import keras\nimport tensorflow as tf\n\n# imports to mute warnings\npd.options.display.max_rows=200\npd.set_option('mode.chained_assignment', None)\n\nfrom warnings import simplefilter\nfrom sklearn.exceptions import ConvergenceWarning\nsimplefilter(\"ignore\", category=ConvergenceWarning)\nsimplefilter(\"ignore\", category=RuntimeWarning)\n\nprint(\"sns.__version__\", sns.__version__)\n#print(\"sklearn.__version__\", sklearn.__version__)","9bcb405b":"train = pd.read_csv('..\/input\/alc-datathon-2021\/covid_mental_health_train.csv')\ntest = pd.read_csv('..\/input\/alc-datathon-2021\/covid_mental_health_test.csv')","0868ce20":"# summary stats\ntrain.describe()","34cecb35":"train.columns.tolist()","05da9af0":"plt.figure(figsize=(60, 8))\nsns.set(font_scale=1.5)\nsns.heatmap(train.corr(), cmap='coolwarm', annot=True, annot_kws={'size':15})\nplt.show()","61b098f7":"important_features = ['id', 'current_mental', 'past_mental', 'past_physical', 'current_physical', 'optimism', 'deterioration_interact', 'frustration', 'difficulty_work', 'difficulty_living', 'deterioration_economy', 'healthy_sleep']","f7b8c48f":"new_train = pd.DataFrame()\nfor feature in important_features:\n    new_train[feature] = train[feature]\nnew_train[\"depression\"] = train[\"depression\"]\nnew_train.head()","2b87addc":"one_hot_encoded_training_predictors = pd.get_dummies(new_train)\none_hot_encoded_training_predictors.head()","1e524712":"new_test = pd.DataFrame()\nfor feature in important_features:\n    new_test[feature] = test[feature]\nnew_test.head()","462c1195":"one_hot_encoded_test = pd.get_dummies(new_test)\none_hot_encoded_test.head()","bebefbee":"print(len(one_hot_encoded_training_predictors))","76697c5a":"X_trainTest = one_hot_encoded_training_predictors.drop(columns = \"depression\")\nY_trainTest = one_hot_encoded_training_predictors[\"depression\"]","bad1160d":"from sklearn.model_selection import KFold\nkf = KFold(n_splits=2)\nkf.get_n_splits(X_trainTest)\n\n\nKFold(n_splits=5, random_state=None, shuffle=False)\nfor train_index, test_index in kf.split(X_trainTest):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_val = X_trainTest.iloc[train_index], X_trainTest.iloc[test_index]\n    y_train, y_val = Y_trainTest.iloc[train_index], Y_trainTest.iloc[test_index]","af259e19":"print(len(X_train))","9736989f":"X_testTest = one_hot_encoded_test\nX_testTest.head()","e384383d":"# Let's load the packages\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.inspection import permutation_importance\nimport shap\nfrom matplotlib import pyplot as plt\n\nplt.rcParams.update({'figure.figsize': (12.0, 8.0)})\nplt.rcParams.update({'font.size': 14})","319f377a":"rf = RandomForestClassifier(n_estimators=100)\nrf = rf.fit(X_train, y_train)\nrf.feature_importances_\nplt.barh(sorted(X_train), rf.feature_importances_)","c6762498":"from sklearn import tree\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import plot_roc_curve","154417d5":"Y_val_pred = rf.predict_proba(X_val)[:, 1]\nprint(roc_auc_score(y_val, Y_val_pred))","c37968e6":"viz = plot_roc_curve(rf, X_val, y_val, name='ROC fold', alpha=0.3, lw=1)","1beb68fb":"Y_test_pred = rf.predict_proba(X_testTest)[:, 1]","f11e03af":"from sklearn.tree import DecisionTreeClassifier","c5cd8e2e":"pipeline = Pipeline([\n    ('normalizer', StandardScaler()), #Step1 - normalize data\n    ('clf', LogisticRegression()) #step2 - classifier\n])\npipeline.steps","292d2d60":"from sklearn.model_selection import cross_validate\n\n\nscores = cross_validate(pipeline, X_train, y_train)\nscores","0a3beb30":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_predict\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nclfs = []\nclfs.append(SVC(probability = True))\nclfs.append(LogisticRegression())\nclfs.append(KNeighborsClassifier(n_neighbors=3))\nclfs.append(DecisionTreeClassifier())\nclfs.append(RandomForestClassifier())\n#clfs.append(GradientBoostingClassifier())\nclfs.append(AdaBoostClassifier(n_estimators=100, random_state=0))\nclfs.append(GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0))\nclfs.append(ExtraTreesClassifier(n_estimators=10000, max_depth=8, min_samples_split=10, random_state=0)\n)\nfor i, classifier in enumerate(clfs):\n    pipeline.set_params(clf = classifier)\n    #scores = cross_validate(pipeline, X_train, y_train)\n    y_pred = cross_val_predict(pipeline, X_val, y_val, cv=3, method = \"predict_proba\")[:, 1]\n    \n    if i==1:\n        pipeline.fit(X_train, y_train)\n        y_test_pred = pipeline.predict_proba(X_testTest)[:, 1]\n        \n    print(str(classifier))\n    print(\"roc_score\", roc_auc_score(y_val, y_pred))\n    print(\"--------------------------\")\n    ","dfa3ba1a":"ensemble_hard = VotingClassifier(estimators=[('lr', clfs[1]), ('extra', clfs[-1])], voting='soft', weights=[0.15, 2])\nensemble_hard = ensemble_hard.fit(X_train, y_train)\ny_ensemble_test = ensemble_hard.predict_proba(X_testTest)[:, 1]\nprint(y_ensemble_test)","158891ed":"y_ensemble_val = ensemble_hard.predict_proba(X_val)[:, 1]\nprint(roc_auc_score(y_val, y_ensemble_val))","158e234c":"submission = pd.DataFrame({'id':one_hot_encoded_test.id,'depression':y_test_pred})","de9edfdc":"submission.head()","840f6441":"submission.to_csv(\"26_Newbies_attempt_logisticExtraVotingkfold_pipeline.csv\", index = False)","b8882eca":"# using random forest","5fef89f9":"We have participated in the competition Ada Lovelace Datathon 2021, we managed to score 11 th position in the leaderboard, we are publishing our attempts in the series of notebooks, rest of the notebook links are-\n\n* [EDA and Data Visualization](https:\/\/www.kaggle.com\/nawshadbintanizam\/eda-allfeatures)\n* [Handling Data Imbalance](https:\/\/www.kaggle.com\/erabaka\/classifier-experiments-with-resample)\n* [Hardcoding Logistic Regression and Decision Tree](https:\/\/www.kaggle.com\/nabilatasnim\/hardcode-logisticregression-decisiontree)","c6256d98":"## using decision tree","cafff5e0":"# KFOLD"}}