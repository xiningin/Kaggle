{"cell_type":{"1ed34261":"code","364010bd":"code","528e00c3":"code","94749cb1":"code","974c8b6a":"code","93a8b70f":"code","b9a3c832":"code","bb022ba7":"code","03aa70b0":"code","b9a8d2f8":"code","3c54c969":"code","3bf1e116":"code","e52fcf54":"code","48bb7751":"markdown","15029203":"markdown","dec8d138":"markdown","06ea3126":"markdown","95d56be4":"markdown","1c278c05":"markdown","62c7742b":"markdown"},"source":{"1ed34261":"# import mods.\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns \nimport tensorflow as tf\nimport keras\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.layers import Dense, LSTM, LeakyReLU, Dropout\nfrom matplotlib import pyplot as plt \nfrom sklearn.preprocessing import MinMaxScaler","364010bd":"# Getting Data \nurl   = 'https:\/\/raw.githubusercontent.com\/Milisha-Gupta\/Status-Neo\/main\/BTC_USD_2013-10-01_2021-04-17-CoinDesk.csv'\ndf    = pd.read_csv(url)\n\n# Show a smaple of the whole data set\ndf.head()","528e00c3":"# Visulize historical data of btc\nprice = df[['Closing Price (USD)']]\nplt.figure(figsize = (20,6))\nplt.plot(price)\nplt.title('BTC Prices', font='Times New Roman', fontsize=20, fontweight='bold')\nplt.show()","94749cb1":"#  heatmap of correlations \nplt.figure(figsize=(10,10))\nsns.heatmap(df.corr(),annot=True);","974c8b6a":"# This mehtod returns a value between 0 ~ 10\nMMScaler = MinMaxScaler()\nNormdf   = MMScaler.fit_transform(price.values)\n\n# A quick comparation between df and Normdf\nprint(f'Acutal:{price.values[0]} || NormDf{Normdf[0]}')\nprint(f'Acutal:{price.values[10]} || NormDf{Normdf[10]}')\nprint(f'Acutal:{price.values[100]} || NormDf{Normdf[100]}')","93a8b70f":"def univariate_date(df, start_index, end_index, history_size, target_size):\n    data   = []\n    labels = []\n\n    # Setting start_index, end_index\n    start_index   = start_index + history_size\n    if end_index is None:\n        end_index = len(df)\n\n    for i in range(start_index, end_index):\n        indices   = range(i - history_size, i)\n        # reshape data (history_size) >> (history_size, 1)\n        data.append(np.reshape(df[indices], (history_size,1)))\n        labels.append(df[i+target_size])\n    return np.array(data), np.array(labels)","b9a3c832":"# define how much days of data we're going to learn to predict the next point\npast_history  = 5\nfuture_target = 0\n\n# last indext of the 80% data used for training \nTrain_Split   = int(len(Normdf) * 0.8)\n\n# Start Spliting \nx_train, y_train = univariate_date(Normdf, 0, Train_Split, past_history, future_target)\n\nx_test, y_test = univariate_date(Normdf, Train_Split, None, past_history, future_target)","bb022ba7":"# Model Parameters \nneurons         = 64\nlearning_rate   = 0.0001\nactfcn          = 'swish'\nadam            = Adam(lr=learning_rate)\nlossfcn         = 'mse'\nbatch_s         = 5            # batch size\nepochs          = 250\n\n# Model initialization\nmodel = Sequential()\n# layers\nmodel.add(LSTM(units= neurons, activation=actfcn, input_shape= (None, 1)))\nmodel.add(LeakyReLU(alpha=0.5))\nmodel.add(Dropout(0.05))\nmodel.add(Dense(units=1))\n# Compile \nmodel.compile(optimizer=adam, loss=lossfcn)","03aa70b0":"# Qucik glance at model summary\nmodel.summary()","b9a8d2f8":"# use training df to train the model\nhistory = model.fit(x_train, y_train, validation_split=0.1, batch_size=batch_s, epochs= epochs, shuffle=False)","3c54c969":"# Training Loss vs Val loss\nloss        = history.history['loss']\nval_loss    = history.history['val_loss']\nepoch       = range(len(loss))\n\n# plot\nplt.figure(figsize=(20,6))\nplt.plot(epoch, loss, 'b', label='Training Loss')\nplt.plot(epoch, val_loss, 'r', label='Validation Loss')\nplt.title('Training & Validation Loss', fontsize=20, fontweight='bold')\nplt.legend()\nplt.show()","3bf1e116":"# Parameters\noriginal        = pd.DataFrame(MMScaler.inverse_transform(y_test))\npred            = pd.DataFrame(MMScaler.inverse_transform(model.predict(x_test)))\noriginal_index  = range(original.index.start,original.index.stop)\nperd_index      = range(pred.index.start,pred.index.stop)\n\n# plot\nplt.figure(figsize=(20,6))\nplt.plot(original_index, original[0], 'b', label='Test Data')\nplt.plot(perd_index, pred[0], 'r', label='Predication Data')\nplt.title('Test vs Predication Data', fontsize=20, fontweight='bold')\nplt.xlabel('Days')\nplt.ylabel('Price (USD)')\nplt.legend()\nplt.show()","e52fcf54":"# print(original.sum(),pred.sum())\naccuracy = round(((pred.sum() \/ original.sum())*100).iloc[0], 2)\nprint(f'Model Accuracy is {accuracy}%')","48bb7751":"## Bitcoin Price Predication \nThe goal of this code is to ascertain with what accuracy the direction of Bitcoin price in USD can be predicted. The price data is sourced from the Bitcoin Price Index.","15029203":"## Step 5 : Perdictions","dec8d138":"#### Data Spliting \nWe need to split our dataset into two parts (TrainingDf , TestDf) which is training dataset and validation dataset. Training Dataset will be used to teach our model while validation dataset will be use in case of testing the perdicted values. This step is very important as we want to make sure our model perdictions make sence.","06ea3126":"## Step 2 : Data Perparation \n#### Normalization \nThe goal of normalizing data is to change the values of numeric columns in the dataset to a commen scale, without distorting differences in the range of values.","95d56be4":"## Step 3 : Bulid Model\nThe next step is to bulid our model architecture. Finding the right model is an art, it may takes several tries to find the right model.\nin this code, we have used LSTM model with one LSTM layer and 64 neurons by using the relu activation function.","1c278c05":"## Step 4 : Modeling\nNow, we can start training our model with TensorFlow","62c7742b":"## Step 1 : Downloading Data\nBTC Prices Dataset is downloaded from `finance.yahoo.com`"}}