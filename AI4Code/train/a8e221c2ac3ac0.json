{"cell_type":{"247b9dc8":"code","409e76b7":"code","5555b33f":"code","6bb542b1":"code","5f08f478":"code","5c486cc5":"code","f6817f78":"code","059c2dd0":"code","fb5f9a70":"code","e177ec63":"code","654ea5de":"code","d7202299":"code","0bc57063":"code","f042857e":"code","52a1ff36":"code","42a1f7e0":"code","f36415d9":"code","8f17ac9f":"code","85a864ff":"code","4f675d83":"code","4c8a5d4f":"code","91a3a0c6":"code","c58ff490":"code","f1a6e02a":"code","a0da7d8e":"code","a69ae05e":"code","f2c7ccf2":"code","175e2123":"code","9e394971":"code","c3458504":"code","2cf9a583":"code","4e94bcc8":"code","4c26e46c":"code","592586a9":"code","cf8da6a2":"code","73dccd87":"code","4544e627":"code","8e2e6447":"code","83033061":"code","7d092007":"code","427e151b":"code","047b42d7":"code","c9b6247f":"code","f527278f":"markdown","4a361785":"markdown","18519e43":"markdown","e7166f94":"markdown","05461fb5":"markdown","f9331d6c":"markdown","3e2dc106":"markdown","107bd277":"markdown","e125d135":"markdown","4678b095":"markdown","92c6e16d":"markdown","55a2d181":"markdown","3a9a0877":"markdown","0eea5219":"markdown","e348012f":"markdown","77caa21e":"markdown","da6c4167":"markdown","530360b1":"markdown","99655f68":"markdown","2e964e2c":"markdown","2c7267c0":"markdown","4c74fe1e":"markdown","40b44a2b":"markdown","2cd46daf":"markdown","80e548aa":"markdown","04fc4d32":"markdown","905b22da":"markdown","332c039b":"markdown"},"source":{"247b9dc8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","409e76b7":"PATH = '\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv'","5555b33f":"spam_data = pd.read_csv(f'{PATH}')","6bb542b1":"import chardet\nwith open(f'{PATH}', 'rb') as rawdata:\n    result = chardet.detect(rawdata.read(100000))\nprint(result)","5f08f478":"spam_data = pd.read_csv(f'{PATH}', encoding='Windows-1252')","5c486cc5":"spam_data","f6817f78":"spam_data.head()","059c2dd0":"spam_data.isnull()","fb5f9a70":"spam_data.isnull().sum()","e177ec63":"spam_data.drop(spam_data[[\"Unnamed: 2\"\t, \"Unnamed: 3\"\t,\"Unnamed: 4\"]], axis=1, inplace=True)","654ea5de":"spam_data.rename(columns={\"v1\" : 'label', \"v2\" : 'message'}, inplace=True)","d7202299":"spam_data.isnull().sum().sort_index()\/len(spam_data)","0bc57063":"spam_data","f042857e":"labels  = pd.get_dummies(spam_data['label'], drop_first=True)","52a1ff36":"labels","42a1f7e0":"spam_data = pd.concat([spam_data, labels], axis=1)\nspam_data","f36415d9":"spam_data.drop(\"label\", axis=1, inplace=True)","8f17ac9f":"spam_data","85a864ff":"messages = spam_data['message'].to_numpy().tolist()","4f675d83":"messages[0:3]","4c8a5d4f":"import re\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer","91a3a0c6":"stop_words = set(stopwords.words('english')) # for Stopword Removal\nstemmer = PorterStemmer()  # for stemming","c58ff490":"# Replacing Emial id's with the single string using regular expression\n\nmessages = [ re.sub(r\"[a-zA-Z0-9.!#$%&'*+\/=?^_`{|}~-]+@[a-zA-Z]+[.]+[a-zA-Z]+[.]?[a-zA-Z]*\", 'EMAILID', word) for word in messages ]\n\n# Replacing web address with the single string using regular expression\n\nmessages = [ re.sub(r\"https?:\\\/\\\/w{0,3}\\w*?\\.(\\w*?\\.)?\\w{2,3}\\S*|www\\.(\\w*?\\.)?\\w*?\\.\\w{2,3}\\S*|(\\w*?\\.)?\\w*?\\.\\w{2,3}[\\\/\\?]\\S*\", 'WEBADDRESS', word) for word in messages ]\n\n# Replacing Phone number with the single string using regular expression\n\nmessages = [ re.sub(r\"\\d{10}\\d{0,9}\", 'PHONENUMBER', word) for word in messages ]","f1a6e02a":"# tokenizing the each sentence\n\ntokenized_by_word = [ word_tokenize(message) for message in messages] ","a0da7d8e":"# punctuations which are needed to be removed \n\npunctuations = '''!()-[]{};:'\"\\,<>.\/?@#$%^&*_~'''","a69ae05e":"def removePunctuations(word):\n    text = \"\"\n    for i in word:\n        if i not in punctuations: # Noise Removal\n            text += i.lower()     # Lowercasing the words\n    \n    return stemmer.stem(text)     # stemming the word","f2c7ccf2":"def wordFilter(sentence):\n    message = []\n    for word in sentence:\n        cleaned_word =  removePunctuations(word)\n        \n        # checking the string wheather it is Stopword or not and also checking the spaces\n        \n        if cleaned_word.isspace() or cleaned_word == \"\" or cleaned_word in stop_words: \n            continue\n        else:\n            message.append(removePunctuations(word))\n    return message","175e2123":"processed_message =  [ wordFilter(message) for message in tokenized_by_word]","9e394971":"processed_message[0:1]","c3458504":"def flatten(my_list):\n  result = []\n  for el in my_list:\n    if isinstance(el, list):\n      flat_list = flatten(el)\n      result += flat_list\n    else:\n      result.append(el)\n  return result","2cf9a583":"words_token = flatten(processed_message)","4e94bcc8":"words_token[0:10]","4c26e46c":"# When building BoW vectors, we generally create a features dictionary\n\ndef create_features_dictionary(document_tokens):\n  features_dictionary = {}\n  index = 0\n  for token in document_tokens:\n    if token not in features_dictionary:\n      features_dictionary[token] = index\n      index += 1\n  return features_dictionary","592586a9":"\n# Turning text into a BoW vector is known as feature extraction or vectorization. \n\ndef tokens_to_bow_vector(document_tokens, features_dictionary):\n  bow_vector = [0] * len(features_dictionary)\n  for token in document_tokens:\n    if token in features_dictionary:\n      feature_index = features_dictionary[token]\n      bow_vector[feature_index] += 1\n  return bow_vector","cf8da6a2":"message_dictionary = create_features_dictionary(words_token)","73dccd87":"message_vector = [tokens_to_bow_vector(message, message_dictionary) for message in processed_message]","4544e627":"messages_label = spam_data['spam'].to_numpy()","8e2e6447":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split","83033061":"spam_classifier = MultinomialNB()","7d092007":"X_train, X_test, y_train, y_test = train_test_split(message_vector, messages_label, test_size=0.20, random_state=42)","427e151b":"spam_classifier.fit(X_train, y_train)","047b42d7":"predictions = spam_classifier.score(X_test, y_test)","c9b6247f":"predictions * 100","f527278f":"## **One Hot Encode the target variable**","4a361785":"Text cleaning is a technique that developers use in a variety of domains. Depending on the goal of your project and where you get your data from, you may want to remove unwanted information, such as:\n\n   * Punctuation and accents\n   * Special characters\n   * Numeric digits\n   * Leading, ending, and vertical whitespace\n   * HTML formatting\n","18519e43":"A few common operations that require tokenization include:\n\n   * Finding how many words or sentences appear in text\n   * Determining how many times a specific word or phrase exists\n   * Accounting for which terms are likely to co-occur\n","e7166f94":"Since there are maximum number of values are null in three column. so, we drop all the three columns","05461fb5":"### Renaming the Column names","f9331d6c":"### Noise Removal","3e2dc106":"Till now we have fininshed our initial processing","107bd277":"As, we can see that there are two columns of target value one in encoded value and one in string format, So we drop the string column of target value.","e125d135":"Text preprocessing is an approach for cleaning and preparing text data for use in a specific context. Developers use it in almost all natural language processing (NLP) pipelines, including voice recognition software, search engine lookup, and machine learning model training. It is an essential step because text data can vary. From its format (website, text message, voice recognition) to the people who create the text (language, dialect), there are plenty of things that can introduce noise into your data.","4678b095":"### Stopword Removal\n\nStopwords are words that we remove during preprocessing when we don\u2019t care about sentence structure. They are usually the most common words in a language and don\u2019t provide any information about the tone of a statement. They include words such as \u201ca\u201d, \u201can\u201d, and \u201cthe\u201d.","92c6e16d":"### **Remove the columns**","55a2d181":"### Now we have to check the null values count in each columns","3a9a0877":" We will use few common approaches for cleaning and processing text data. They include:\n\n   * Using Regex & NLTK libraries\n   * Removing unnecessary characters and formatting\n   * Tokenization \u2013 break multi-word strings into smaller components\n   * Normalization \u2013 a catch-all term for processing data; this includes stemming and lemmatization\n","0eea5219":"### Normalization\n\nTokenization and noise removal are staples of almost all text pre-processing pipelines. However, some data may require further processing through text normalization. Text normalization is a catch-all term for various text pre-processing tasks.A few of them:\n\n  *  Upper or lowercasing\n  *  Stopword removal\n  *  Stemming \u2013 bluntly removing prefixes and suffixes from a word","e348012f":"### We got an **UnicodeDecodeError**","77caa21e":"# **Text Preprocessing**","da6c4167":"specifying path for the file","530360b1":"### Tokenization","99655f68":"### For resolving this error we have to know value encoding ","2e964e2c":"# Building Models","2c7267c0":"dividing data into training and test set","4c74fe1e":"### Now wite the value of encoding inside the `read_csv()` ","40b44a2b":"Now again we check the null values after deleting the column","2cd46daf":"\n# **Loading and Understanding the Data**","80e548aa":"### Stemming\n\nIn natural language processing, stemming is the text preprocessing normalization task concerned with bluntly removing word affixes (prefixes and suffixes). For example, stemming would cast the word \u201cgoing\u201d to \u201cgo\u201d. This is a common method used by search engines to improve matching between user input and website hits.","04fc4d32":"Now, we convert the message column into list ","905b22da":"Now we have two DataFrame one labels with ecoded value and our initial DataFrame.So we have to combine both to make a single DataFrame","332c039b":"Accuracy of test set"}}