{"cell_type":{"d8167958":"code","1f03aec0":"code","fd88c047":"code","588bf9e1":"code","564b7d56":"code","8fe3259f":"code","8ae138e5":"code","88eb46fb":"markdown","69a2d85e":"markdown","e4f8e599":"markdown","71d84972":"markdown","2ee3ef3b":"markdown","edfaa6fb":"markdown","38a0b66b":"markdown","57a09ee5":"markdown"},"source":{"d8167958":"!pip install git+https:\/\/github.com\/keras-team\/keras-tuner.git -q","1f03aec0":"import pandas as pd\n\ndef load_data(path):\n    data = pd.read_csv(path)\n    y = data[\"label\"]\n    x = data.drop(labels=[\"label\"], axis=1).values.reshape(-1, 28, 28, 1)\n    return x, y\n\nx_train, y_train = load_data(\"..\/input\/digit-recognizer\/train.csv\")\nx_test, _ = load_data(\"..\/input\/digit-recognizer\/train.csv\")","fd88c047":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\ndef augment_images(x, hp):\n    use_rotation = hp.Boolean('use_rotation')\n    if use_rotation:\n        x = layers.experimental.preprocessing.RandomRotation(\n            hp.Float('rotation_factor', min_value=0.05, max_value=0.2)\n        )(x)\n    use_zoom = hp.Boolean('use_zoom')\n    if use_zoom:\n        x = layers.experimental.preprocessing.RandomZoom(\n            hp.Float('use_zoom', min_value=0.05, max_value=0.2)\n        )(x)\n    return x\n\ndef make_model(hp):\n    inputs = keras.Input(shape=(28, 28, 1))\n    x = layers.experimental.preprocessing.Rescaling(1. \/ 255)(inputs)\n    x = layers.experimental.preprocessing.Resizing(64, 64)(x)\n    x = augment_images(x, hp)\n    \n    num_block = hp.Int('num_block', min_value=2, max_value=5, step=1)\n    num_filters = hp.Int('num_filters', min_value=32, max_value=128, step=32)\n    for i in range(num_block):\n        x = layers.Conv2D(\n            num_filters,\n            kernel_size=3,\n            activation='relu',\n            padding='same'\n        )(x)\n        x = layers.Conv2D(\n            num_filters,\n            kernel_size=3,\n            activation='relu',\n            padding='same'\n        )(x)\n        x = layers.MaxPooling2D(2)(x)\n    \n    reduction_type = hp.Choice('reduction_type', ['flatten', 'avg'])\n    if reduction_type == 'flatten':\n        x = layers.Flatten()(x)\n    else:\n        x = layers.GlobalAveragePooling2D()(x)\n\n    x = layers.Dense(\n        units=hp.Int('num_dense_units', min_value=32, max_value=512, step=32),\n        activation='relu'\n    )(x)\n    x = layers.Dropout(\n        hp.Float('dense_dropout', min_value=0., max_value=0.7)\n    )(x)\n    outputs = layers.Dense(10)(x)\n    model = keras.Model(inputs, outputs)\n    \n    learning_rate = hp.Float('learning_rate', min_value=3e-4, max_value=3e-3)\n    optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n    model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                  optimizer=optimizer,\n                  metrics=[keras.metrics.SparseCategoricalAccuracy(name='acc')])\n    model.summary()\n    return model","588bf9e1":"import kerastuner as kt\n\ntuner = kt.tuners.RandomSearch(\n    make_model,\n    objective='val_acc',\n    max_trials=100,\n    overwrite=True)\n\ncallbacks=[keras.callbacks.EarlyStopping(monitor='val_acc', mode='max', patience=3, baseline=0.9)]\ntuner.search(x_train, y_train, validation_split=0.2, callbacks=callbacks, verbose=1, epochs=100)","564b7d56":"best_hp = tuner.get_best_hyperparameters()[0]\nmodel = make_model(best_hp)\nhistory = model.fit(x_train, y_train, validation_split=0.2, epochs=50)","8fe3259f":"val_acc_per_epoch = history.history['val_acc']\nbest_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\nmodel = make_model(best_hp)\nmodel.fit(x_train, y_train, epochs=best_epoch)","8ae138e5":"import numpy as np\n\npredictions = model.predict(x_test)\nsubmission = pd.DataFrame({\"ImageId\": list(range(1, len(predictions) + 1)),\n                           \"Label\": np.argmax(predictions, axis=-1)})\nsubmission.to_csv(\"submission.csv\", index=False)","88eb46fb":"## Make a submission","69a2d85e":"## Load the data\n\nWe use Pandas to load the data into NumPy arrays.\n\nOur inputs are uint8 arrays of shape `(num_samples, 28, 28, 1)` and our targets are integer arrays of shape `(num_samples,)`.","e4f8e599":"On the free Kaggle GPU, trying out 100 models takes 4 hours.\nAt the end of the search, our best validation accuracy is 99.33%.","71d84972":"# Keras + KerasTuner best practices\n\nThis notebook presents how to use KerasTuner to find a high-performing model in just a few lines of code.\n\nFirst, let's start by installing the latest Kerastuner version:","2ee3ef3b":"## Find the best epoch value\n\nNow, we can retrieve the best hyperparameters, use them to build the best model,\nand train the model for 50 epochs to find at which epoch training should stop.","edfaa6fb":"## Define a tunable model\n\nWe define a function `def make_model(hp):` which builds a compiled Keras model,\nparameterized by hyperparameters obtained from the `hp` argument.\n\nOur model includes a stage that does random image data augmentation, via the `augment_images`\nfunction. Our image augmentation is itself tunable: we'll find the best augmentation\nconfiguration during the hyperparameter search.","38a0b66b":"## Run hyperparameter search\n\nNow, we launch the search. For simplicity, we'll use `RandomSearch`. We'll limit the search to 100 different model configurations.\n\nNote that we configure the calls to `model.fit()` to use the `EarlyStopping` callbacks.\nIndeed, we train for 100 epochs, but the model is likely to start overfitting much earlier than that --\nin general, always use a large number of epochs + the `EarlyStopping` callback.\n\nOur search is guided by validation accuracy, which is computed on a fixed 20% hold-out set of the training data.","57a09ee5":"## Train the production model\n\nFinally, we can train the best model configuration from scratch for the optimal number of epochs.\n\nThis time, we train on the entirety of the training data -- no validation split. Our model parameters are already validated."}}