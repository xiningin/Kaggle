{"cell_type":{"b820f34c":"code","6b45183e":"code","37b0afe0":"code","b3134f85":"code","742c8725":"code","5ea52aee":"code","e43c3ee5":"code","b8de371f":"code","ca421afa":"code","30820bdd":"code","a0326789":"code","11571e50":"code","9b8a35e2":"code","453e0568":"code","34c36621":"code","92613dc1":"code","b497b1f9":"code","e54c8cc6":"code","79f5a3a2":"code","447adcef":"code","12da1e14":"code","aa1bb25f":"code","bbbe61d7":"code","24accd38":"code","028b9495":"code","132b9cfe":"code","8bac9def":"code","2da1686a":"code","e176914f":"code","460b4826":"code","45d6a3e4":"code","d16e8878":"code","32231777":"code","6ceb39b4":"code","ad1dc4f0":"code","8eee0e9d":"code","5fa3ddac":"code","fa1d3c5c":"code","b399768d":"code","380fdeec":"code","fbf460ea":"code","c7c58751":"code","e0906767":"code","85cbf799":"code","c211e35b":"code","07f982ef":"code","7d45ef82":"code","51331813":"code","b9cbe5c5":"code","c5a99d99":"code","6e4fedb6":"code","43b40f3c":"code","212d0e4c":"code","c5c8e64f":"code","5f69a99c":"code","d49ded76":"code","f9b2d34c":"code","ddf95103":"code","cabfe698":"code","1c4f3666":"markdown","4ed5e31e":"markdown","c76d9413":"markdown","66fa6226":"markdown"},"source":{"b820f34c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6b45183e":"!pip install py7zr","37b0afe0":"import pandas as pd\nfrom py7zr import unpack_7zarchive\nimport shutil\nfrom PIL import Image as PImage\nfrom matplotlib import pyplot as plt\nimport keras\nimport tensorflow as tf","b3134f85":"check = pd.read_csv('\/kaggle\/input\/cifar-10\/trainLabels.csv')","742c8725":"'''shutil.register_unpack_format('7zip', ['.7z'], unpack_7zarchive)\nshutil.unpack_archive('\/kaggle\/input\/cifar-10\/train.7z', '\/kaggle\/working\/train)'''","5ea52aee":"'''shutil.rmtree('\/kaggle\/working\/train')'''","e43c3ee5":"'''os.listdir('\/kaggle\/working\/train\/train')'''","b8de371f":"'''def loadImages(path):\n    # return array of images\n\n    imagesList = os.listdir(path)\n    loadedImages = []\n    for image in imagesList:\n        img = PImage.open(path + image)\n        loadedImages.append(img)\n\n    return loadedImages\n'''","ca421afa":"'''path = '\/kaggle\/working\/train\/train\/'\ntrain_images = loadImages(path)'''","30820bdd":"#train_images[0]\n#plt.imshow(train_images[0])\n#plt.show()","a0326789":"#train_images[0].size","11571e50":"from keras.datasets import cifar10\n","9b8a35e2":"data = tf.keras.datasets.cifar10","453e0568":"(x_train, y_train), (x_test, y_test) = data.load_data()","34c36621":"x_train.shape","92613dc1":"x_train[0]","b497b1f9":"plt.imshow(x_train[0])\nplt.show()","e54c8cc6":"from collections import Counter","79f5a3a2":"b=[]\nfor i in range(len(y_train)):\n    y_train[i][0]\n    b.extend([y_train[i][0]])","447adcef":"a = Counter(b)","12da1e14":"a","aa1bb25f":"new = sorted(a.items())","bbbe61d7":"new = dict(new)","24accd38":"labels = new.keys()\nlabels","028b9495":"values = new.values()\nvalues","132b9cfe":"index = np.arange(len(labels))","8bac9def":"plt.bar(index,values)\nplt.xlabel('class')\nplt.ylabel('occurance')\nplt.xticks(index,labels)\nplt.title('occurance of different classes')\nplt.show()","2da1686a":"b=[]\nfor i in range(len(y_test)):\n    y_test[i][0]\n    b.extend([y_test[i][0]])","e176914f":"a = Counter(b)\na","460b4826":"new = sorted(a.items())\nnew = dict(new)\nnew","45d6a3e4":"labels = new.keys()\nlabels","d16e8878":"values = new.values()\nvalues","32231777":"plt.bar(index,values)\nplt.xlabel('class')\nplt.ylabel('occurance')\nplt.xticks(index,labels)\nplt.title('occurance of different classes')\nplt.show()","6ceb39b4":"a =x_train[0]\nnp.amax(a)","ad1dc4f0":"x_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train \/= 255\nx_test \/= 255\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')","8eee0e9d":"a =x_train[0]\nnp.amax(a)","5fa3ddac":"num_classes = 10","fa1d3c5c":"# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)","b399768d":"print(y_train.shape)\nprint(y_test.shape)","380fdeec":"y_train[0]","fbf460ea":"for_tsne = []","c7c58751":"for i in range(1000):\n    a = x_train[i]\n    for_tsne.append(a.flatten())","e0906767":"for_tsne = np.array(for_tsne)","85cbf799":"for_tsne.shape","c211e35b":"from sklearn.manifold import TSNE\n\n# Picking the top 1000 points as TSNE takes a lot of time for 15K points\ndata_1000 = for_tsne\nlabels_1000 = y_train[0:1000]\n\nmodel = TSNE(n_components=2, random_state=0)\n# configuring the parameteres\n# the number of components = 2\n# default perplexity = 30\n# default learning rate = 200\n# default Maximum number of iterations for the optimization = 1000\n\ntsne_data = model.fit_transform(data_1000)\n","07f982ef":"# creating a new data frame which help us in ploting the result data\ntsne_data = np.hstack((tsne_data, labels_1000))\ntsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n","7d45ef82":"import seaborn as sn\n# Ploting the result of tsne\nsn.FacetGrid(tsne_df, hue=\"label\", size=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.show()","51331813":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D","b9cbe5c5":"x_train[0].shape","c5a99d99":"input_shape = (32,32,3)\nbatch_size =100\nepochs =10","6e4fedb6":"model  = Sequential()\n\nmodel.add(Conv2D(128,(3,3),activation = 'relu',input_shape = input_shape))\nmodel.add((MaxPooling2D(pool_size = (2,2))))\n\nmodel.add(Conv2D(256,(3,3),activation = 'relu'))\nmodel.add((MaxPooling2D(pool_size = (2,2))))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(num_classes, activation='softmax'))\n\n","43b40f3c":"model.summary()","212d0e4c":"model.compile(loss= 'categorical_crossentropy',optimizer = 'adam' , metrics = ['accuracy'])\nhistory=model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test, y_test))","c5c8e64f":"%matplotlib notebook\n%matplotlib inline\n\nimport time\n# https:\/\/gist.github.com\/greydanus\/f6eee59eaf1d90fcb3b534a25362cea4\n# https:\/\/stackoverflow.com\/a\/14434334\n# this function is used to update the plots for each epoch and error\ndef plt_dynamic(x, vy, ty, ax, colors=['b']):\n    ax.plot(x, vy, 'b', label=\"Validation Loss\")\n    ax.plot(x, ty, 'r', label=\"Train Loss\")\n    #plt.legend()\n    plt.grid()\n    plt.show()\n    fig.canvas.draw()","5f69a99c":"fig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,epochs+1))\n\n# print(history.history.keys())\n# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n\n# we will get val_loss and val_acc only when you pass the paramter validation_data\n# val_loss : validation loss\n# val_acc : validation accuracy\n\n# loss : training loss\n# acc : train accuracy\n# for each key in histrory.histrory we will have a list of length equal to number of epochs\n\nvy = history.history['val_accuracy']\nty = history.history['accuracy']\nplt_dynamic(x, vy, ty, ax)","d49ded76":"model  = Sequential()\n\nmodel.add(Conv2D(6,(5,5),activation = 'relu',input_shape = input_shape))\nmodel.add((MaxPooling2D(pool_size = (2,2))))\n\nmodel.add(Conv2D(16,(5,5),activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size= (2,2)))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(120,activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(84,activation = 'relu'))\n\nmodel.add(Dense(10,activation = 'softmax'))","f9b2d34c":"model.summary()","ddf95103":"model.compile(loss= 'categorical_crossentropy',optimizer = 'adam' , metrics = ['accuracy'])\nhistory=model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test, y_test))","cabfe698":"fig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,epochs+1))\n\n# print(history.history.keys())\n# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n\n# we will get val_loss and val_acc only when you pass the paramter validation_data\n# val_loss : validation loss\n# val_acc : validation accuracy\n\n# loss : training loss\n# acc : train accuracy\n# for each key in histrory.histrory we will have a list of length equal to number of epochs\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","1c4f3666":"using lenet","4ed5e31e":"using tsne to check for visualization","c76d9413":"Using cnn models to predict the label of the images","66fa6226":"eda to check no of labels of image in test and train data"}}