{"cell_type":{"a816322b":"code","12e12895":"code","d63fae5e":"code","cdce29cd":"code","bd8e79f3":"code","85c91353":"code","39af6721":"code","deaa7f7d":"code","3066af0d":"code","b5f3a1b5":"code","5be9e003":"code","ad4c1b5b":"code","4ec45f5a":"code","fe2105af":"code","77a9a6fb":"code","36c93448":"code","c4be4e8b":"code","8ba5bc33":"code","1275b2ef":"code","9b32b36d":"code","eaefee0b":"code","8ffeb8ad":"code","043a882c":"code","8f4c4809":"code","108190b0":"code","3fea6337":"code","cf7b9bbb":"code","75d2ab82":"code","72169066":"code","86b3ef01":"code","fd008474":"code","f5095bea":"code","8876efe0":"code","4e059631":"code","1f32cfee":"code","a6611ba3":"code","1d0940c4":"code","f1e724b2":"code","44a30d48":"code","e221a415":"code","1a607809":"code","014669f3":"code","69a049ae":"code","10020499":"code","533b43cd":"code","db624652":"code","b4a77e4d":"code","02169356":"code","3fa4b02a":"code","d6107e0d":"code","e9878734":"code","1c257ef0":"code","0d9c5f5e":"code","50b6c587":"code","e4c49b06":"code","4a73c031":"code","e803800e":"markdown","eb833be6":"markdown","f027a44f":"markdown","1cf54853":"markdown","fd60d4e0":"markdown","0ed0a77f":"markdown","31b43f8d":"markdown","556277c6":"markdown","17d0cf89":"markdown","98b87b45":"markdown","4535376a":"markdown","56a8cfad":"markdown","2db9acd5":"markdown","d977e4d9":"markdown","09e53d63":"markdown","0e6d0875":"markdown","035ebbf1":"markdown","eba2aa6d":"markdown","c6da321a":"markdown","b32dae25":"markdown","30b565d8":"markdown","a10bb482":"markdown","bc6369c4":"markdown","9709a220":"markdown","2c476fce":"markdown","3199bdf3":"markdown","0ea3f70d":"markdown","3f6c1e00":"markdown","a7409f6c":"markdown","dd8c570e":"markdown","69d80bd1":"markdown","34eea8ab":"markdown","6001cbe8":"markdown","e0864aad":"markdown","7fc75746":"markdown","ccbd7628":"markdown","dea80923":"markdown","ac73f24b":"markdown","b75ec152":"markdown","46e10f7d":"markdown","f78f05d9":"markdown","6e24559b":"markdown"},"source":{"a816322b":"!ls \/kaggle\/input\/results","12e12895":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d63fae5e":"#get the dataset\nimport os\nimport tarfile\nfrom six.moves import urllib\nDOWNLOAD_ROOT=\"https:\/\/raw.githubusercontent.com\/ageron\/handson-ml\/master\/\"\nHOUSING_PATH=\"datasets\/housing\"\nHOUSING_URL=DOWNLOAD_ROOT+HOUSING_PATH+\"\/housing.tgz\"\n\n\n#download the data\ndef fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n    \"\"\"\n    create datasets\/housing directory\n    download housing.tgz file\n    extract housing.csv\n    \n    \"\"\"\n    if not os.path.isdir(housing_path):\n        os.makedirs(housing_path)\n    \n    tgz_path=os.path.join(housing_path,\"housing.tgz\")\n    urllib.request.urlretrieve(housing_url,tgz_path)\n    housing_tgz=tarfile.open(tgz_path)\n    housing_tgz.extractall(path=housing_path)\n    housing_tgz.close()\n\n#load the data from csv into pandas df\ndef load_housing_data(housing_path=HOUSING_PATH):\n    csv_path=os.path.join(housing_path,\"housing.csv\")\n    return pd.read_csv(csv_path)","cdce29cd":"fetch_housing_data()\nhousing=load_housing_data()\nhousing.head()","bd8e79f3":"housing.info()","85c91353":"housing.describe()","39af6721":"%matplotlib inline\nimport matplotlib.pyplot as plt\nhousing.hist(bins=50,figsize=(20,15))\nplt.show()","deaa7f7d":"housing['income_cat']=np.ceil(housing['median_income']\/1.5)\nhousing['income_cat'].where(housing['income_cat']<5,5,inplace=True)  #everything that belongs to cat 6 and beyond is put into cat 5\n","3066af0d":"housing.hist(['income_cat'],figsize=(5,5))","b5f3a1b5":"from sklearn.model_selection import StratifiedShuffleSplit\n\nstrat_split=StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=13)\nfor train_ind,test_ind in strat_split.split(housing,housing['income_cat']):\n    strat_train_set=housing.loc[train_ind]\n    strat_test_set=housing.loc[test_ind]\n\n    \n","5be9e003":"f,ax=plt.subplots(2,2,sharey=True,sharex=True)\ns=housing['income_cat'].value_counts()\/len(housing)\nax[0,0].hist(s)\nax[0,0].set_title('total data')\n\nss=strat_test_set['income_cat'].value_counts()\/len(strat_test_set)\nax[1,1].hist(ss)\nax[1,1].set_title('test set')\nss=strat_train_set['income_cat'].value_counts()\/len(strat_train_set)\nax[1,0].hist(ss)\nax[1,0].set_title('train set')\n\n#delete the axes[0,1] as it holds no plots\nf.delaxes(ax[0,1])\nplt.show()","ad4c1b5b":"for s in (strat_train_set,strat_test_set):\n    s.drop('income_cat',axis=1,inplace=True)\n","4ec45f5a":"housing=strat_train_set.copy()  #make a copy of the training data for EDA","fe2105af":"housing.plot(kind='scatter',x='longitude',y='latitude',alpha=0.4, \n             s=housing['population']\/100, #circles of radius s indicates population size\n             label='population',\n             c=housing['median_house_value'],  #color indicating median_income\n             cmap=plt.get_cmap('jet'),\n             colorbar=True,\n             figsize=(20,10)\n            )\nplt.legend()","77a9a6fb":"from pandas.plotting import scatter_matrix\nhousing.columns\nattributes=['median_house_value','median_income','total_rooms','housing_median_age']\nscatter_matrix(housing[attributes],figsize=(20,10))\nplt.show()","36c93448":"housing.plot(kind='scatter',x='median_house_value',y='median_income',alpha=0.1)","c4be4e8b":"housing=strat_train_set.drop('median_house_value',axis=1)  # housing is now just predictors. (X)\nhousing_labels=strat_train_set['median_house_value'].copy() #labels (Y)","8ba5bc33":"housing.isna().sum()","1275b2ef":"from sklearn.impute import SimpleImputer\nimputer=SimpleImputer(missing_values=np.nan,strategy='median')\n#drop the ocean_proximity feature which is categorical\nhousing_numer=housing.drop('ocean_proximity',axis=1)\n#impute the numerical data. Remember that this is all been done on the training set\nimputer.fit(housing_numer)\nprint(imputer.statistics_)\nXX=imputer.transform(housing_numer)  #XX is a numpy array\nhousing_tr=pd.DataFrame(XX,columns=housing_numer.columns) # convert XX to a pandas df\nhousing_tr.head()","9b32b36d":"\"\"\" Commented out. since this process can now directly be done using the OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nencoder=LabelEncoder()\nhousing_cat=housing['ocean_proximity']\nhousing_cat_encoded=encoder.fit_transform(housing_cat)\nprint(housing_cat_encoded)\nprint(encoder.classes_)    #<1H mapped to 0, INLAND mapped to 1,..\n\"\"\"","eaefee0b":"#housing_cat_encoded.reshape(-1,1).shape","8ffeb8ad":"from sklearn.preprocessing import OneHotEncoder\nohe=OneHotEncoder()\nhousing_cat=housing['ocean_proximity']\n#housing_cat_ohe=ohe.fit_transform(housing_cat.values.reshape(-1,1)) \n# the reshape is because fit_transform expects a 2D array. Our housing_cat_encoded array is of shape \n# (n,) i.e a 1D array. We reshape it to (n,1) . We specify the '1' in the (-1,1) the '-1' causes numpy \n# to automagically infer 'n'\nhousing_cat_ohe=ohe.fit_transform(housing_cat.values.reshape(-1,1)) \nhousing_cat_ohe\n","043a882c":"housing_cat_ohe.toarray()","8f4c4809":"#Custom Transformers\nfrom sklearn.base import BaseEstimator,TransformerMixin\n\n#Including the BaseEstimator provides the class with get_params() and set_params()\n#Including the TransformerMixin as a base provides the fit_transform() method\n\nroom_ix,bedrooms_ix,population_ix,household_ix= 3,4,5,6  #indices in the df\n\nclass CombinedAttributesAdder(BaseEstimator,TransformerMixin): \n    def __init__(self,add_bedrooms_per_room=True):\n        self.add_bedrooms_per_room=add_bedrooms_per_room\n    \n    def fit(self,X,y=None):\n        return self   #nothing to do except return self in this method\n    \n    def transform(self,X,y=None):\n        rooms_per_household=X[:,room_ix]\/X[:,household_ix]\n        population_per_household=X[:,population_ix]\/X[:,household_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room=X[:,bedrooms_ix]\/X[:,room_ix]\n            return np.c_[X,rooms_per_household,population_per_household,bedrooms_per_room]  #include new features         \n        else:\n            return np.c_[X,rooms_per_household,population_per_household]\n\n\n        \n","108190b0":"attribute_adder=CombinedAttributesAdder(add_bedrooms_per_room=True)\nhousing_new_attribs= attribute_adder.transform(housing.values)\nprint(housing_new_attribs.shape)\nprint(housing.shape)","3fea6337":"class DataFrameSelector(BaseEstimator,TransformerMixin):\n    def __init__(self,attribute_names):\n        self.attribute_names=attribute_names\n    def fit(self,X,y=None):\n        return self\n    def transform(self,X,y=None):\n        return X[self.attribute_names].values    #return the np values from the df for the selected attributes","cf7b9bbb":"#the entire data cleaning steps in one go\nhousing=strat_train_set.drop('median_house_value',axis=1)   #this is our X\nhousing_labels=strat_train_set['median_house_value'].copy() # our Y\nhousing_num=housing.drop('ocean_proximity',axis=1)  #X=X_num+X_cat. This is our X_num\nnumerical_attribs=list(housing_num)\ncategorical_attrib=['ocean_proximity']  #X_cat\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n#pipeline 1  load_df->impute_nans->add_new_attribs->perform standardization\npipe_num=Pipeline([\n    ('selector',DataFrameSelector(numerical_attribs)),                    #load data\n    ('imputer',SimpleImputer(missing_values=np.nan,strategy='median')),   #impute \n    ('attribs_adder',CombinedAttributesAdder()),                          #add_new_attribs\n    ('std_scaler',StandardScaler())                                       #perform standardization\n])\n\n#pipeline 2 load_df->ohe\npipe_cat=Pipeline([\n    ('selector',DataFrameSelector(categorical_attrib)),                   #load_df\n    ('ohe',OneHotEncoder())                                               #perform ohe\n])\n","75d2ab82":"#test pipelines\nnum_res=pipe_num.fit_transform(housing)\nprint(num_res.shape)\n\nohe_res=pipe_cat.fit_transform(housing)\nprint(ohe_res.toarray())","72169066":"from sklearn.pipeline import FeatureUnion\nfull_pipe=FeatureUnion(\n    [\n        ('pipe_num',pipe_num),   \n        ('pipe_cat',pipe_cat)\n    ]\n)\n","86b3ef01":"#load the dataframe into the full pipe.\nhousing_prep=full_pipe.fit_transform(housing)  #the pipe_num will produce its output (num_op) and the pipe_cat will produce it's output (cat_op)\n#housing_prep=concat(num_op,cat_op)\nprint(housing_prep)  # a sparse matrix (ohe introduces zeros)\nprint(housing_prep.shape)","fd008474":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\n\nforest_reg=RandomForestRegressor()\n\n#cross validation expects a utility function (greater is better) rather than a cost function (lower is better)\nscores=cross_val_score(forest_reg,housing_prep,housing_labels,scoring='neg_mean_squared_error',cv=2)\nscores\n\n","f5095bea":"\nrmse=np.sqrt(-scores)","8876efe0":"def display_scores(scores):\n    print(\"scores: \",scores)\n    print(\"mean: \",scores.mean())\n    print('std_dev:',scores.std())\n","4e059631":"display_scores(rmse)","1f32cfee":"#forest_reg.feature_importances_","a6611ba3":"RESULTS='\/kaggle\/input\/results'\nif not os.path.isdir(RESULTS):\n    os.makedirs(RESULTS)","1d0940c4":"#using Grid Search\n\n\nfrom sklearn.externals import joblib\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid=[\n    {'n_estimators':[20,30,40],'max_features':[6,8,9,10]},             #3X4 combinations of params\n    {'bootstrap':[False],'n_estimators':[20,30],'max_features':[9,10]} #1X2X2 combinations of params\n    \n]\nreg=RandomForestRegressor()\n\nGRID_SEARCH_RESULT_LOC=\"\/kaggle\/input\/results\/gridsearch_randforestreg.pkl\"\n\n\ntry :\n    gs_cv=joblib.load(GRID_SEARCH_RESULT_LOC)\n    print('loaded previous instance of gridsearch with best_params: ',gs_cv.best_params_)\nexcept(FileNotFoundError):\n    print('no saved file found. continuing with gridsearch')\n    gs_cv=GridSearchCV(reg,param_grid,cv=5,scoring='neg_mean_squared_error',n_jobs=-1)  #n_jobs=-1 ==>parallel execution\n    gs_cv.fit(housing_prep,housing_labels)\n#save gridsearch\n    joblib.dump(gs_cv,GRID_SEARCH_RESULT_LOC)\n    \n    \n","f1e724b2":"#best parameters and the best estimator that grid search found\ncvres=gs_cv.cv_results_\ncvres\n#cdvresdf=pd.DataFrame(cvres.values,columns=cvres.keys())","44a30d48":"print(gs_cv.best_estimator_.feature_importances_)","e221a415":"\n\"\"\" \nJust to recap. these were the pipelines we had so far\n\n#pipeline 1  load_df->impute_nans->add_new_attribs->perform standardization\npipe_num=Pipeline([\n    ('selector',DataFrameSelector(numerical_attribs)),                    #load data\n    ('imputer',SimpleImputer(missing_values=np.nan,strategy='median')),   #impute \n    ('attribs_adder',CombinedAttributesAdder()),                          #add_new_attribs\n    ('std_scaler',StandardScaler())                                       #perform standardization\n])\n\n#pipeline 2 load_df->ohe\npipe_cat=Pipeline([\n    ('selector',DataFrameSelector(categorical_attrib)),                   #load_df\n    ('ohe',OneHotEncoder())                                               #perform ohe\n])\n\nfull_pipe=FeatureUnion(\n    [\n        ('pipe_num',pipe_num),\n        ('pipe_cat',pipe_cat)\n    ]\n)\n\n\"\"\"\n\npipe_reg=Pipeline([\n    ('full_pipe',full_pipe),\n    ('reg',RandomForestRegressor())\n])\n#the parameters are referenced using reg__  where reg refers to the RandomForestRegressor.\n#we can further nest these calls. full_pipe->pipe_num-->imputer-->strategy can take either 'mean' or median\n\nparameter_grid=[\n    {\n     'reg__n_estimators':[20,30,40],'reg__max_features':[6,8,9,10],\n     'full_pipe__pipe_num__imputer__strategy':['mean','median']\n    },#3X4X2 combinations of params\n    #{'reg__bootstrap':[False],'reg__n_estimators':[20,30],'reg__max_features':[9,10]}  #1X2X2 combinations of params\n]\n\nGRID_SEARCH_PIPL_LOC=\"\/kaggle\/input\/results\/gridsearch_over_pipe_randforestreg\"\ntry:\n    gs_p_cv=joblib.load(GRID_SEARCH_PIPL_LOC)\n    print('loaded previous instance of gridsearch with best_params: ',gs_p_cv.best_params_)\nexcept(FileNotFoundError):\n    print('File not found. continuing with GridSearch')\n    gs_p_cv=GridSearchCV(pipe_reg,cv=5,param_grid=parameter_grid,scoring='neg_mean_squared_error',n_jobs=-1)\n    gs_p_cv.fit(housing,housing_labels)\n    joblib.dump(gs_p_cv,GRID_SEARCH_PIPL_LOC)\n","1a607809":"#best params\nprint(gs_p_cv.best_params_)\n#best scores\nbs=gs_p_cv.best_score_\nrmsbs=np.sqrt(-bs)\nprint(rmsbs)","014669f3":"#all params , all scores\ncvsres=gs_p_cv.cv_results_\ncvsresdf=pd.DataFrame.from_dict(cvsres)\ncvsresdf.sort_values(by=['rank_test_score'])","69a049ae":"#gs_p_cv.best_estimator_.named_steps.full_pipe.get_params()  This is how you would access the params under full_pipe","10020499":"feature_importances=gs_p_cv.best_estimator_.named_steps.reg.feature_importances_\nextra_attribs=['rooms_per_house','pop_per_house','bedrooms_per_house']\nohe_attrs=list(ohe.get_feature_names())\nattributes=numerical_attribs+extra_attribs+ohe_attrs        #if you look up the code, this is the order in which we split the data columnwise\nsorted_list_of_important_features=sorted(zip(feature_importances,attributes),reverse=True)\nplt.figure()\nplt.barh(attributes,feature_importances)\nplt.show()\n","533b43cd":"from sklearn.metrics import mean_squared_error\nfinal_model=gs_p_cv.best_estimator_\nX_test=strat_test_set.drop('median_house_value',axis=1)\ny_test=strat_test_set['median_house_value'].copy()\n\nfinal_predictions=final_model.predict(X_test)  #the pipeline exposes the methods of its final estimator which in our case is the RandomForestRegressor\n                                                #If we were to open up the pipe, this is what will be seen\n                                                #X_test_tr=full_pipe.transform(X_test)\n                                                #final_predictions=res.predict(X_test_tr)\n#final error\nfinal_mse=mean_squared_error(final_predictions,y_test)\nfinal_rmse=np.sqrt(final_mse)\nfinal_rmse\n\n","db624652":"from sklearn.svm import SVR\npipe_svr=Pipeline(\n    [\n        ('data_pipe',full_pipe),\n        ('svr',SVR())\n    ]\n)\n\npara_grid=[\n    {\n        'data_pipe__pipe_num__imputer__strategy':['mean','median'],\n        'svr__kernel':['linear','rbf'],\n        'svr__C':[0.3,0.5,0.7,1.0,1.2,1.4,1.6,1.8,2.0],\n        'svr__gamma':['auto','scale']\n    }\n]\n\nGRID_SEARCH_PIPL_SVR_LOC=\"\/kaggle\/input\/results\/gridsearch_over_pipe_svr\"\ntry:\n    gs_p_svr_cv=joblib.load(GRID_SEARCH_PIPL_SVR_LOC)\n    print('loaded previous instance of gridsearch with best_params: ',gs_p_svr_cv.best_params_)\nexcept(FileNotFoundError):\n    print('File not found. continuing with GridSearch')\n    gs_p_svr_cv=GridSearchCV(pipe_svr,cv=5,param_grid=para_grid,scoring='neg_mean_squared_error',n_jobs=-1)\n    gs_p_svr_cv.fit(housing,housing_labels)\n    joblib.dump(gs_p_svr_cv,GRID_SEARCH_PIPL_SVR_LOC)\n\nsvr_model=gs_p_svr_cv.best_estimator_\nprint(gs_p_svr_cv.best_params_)\nsvr_preds=svr_model.predict(X_test)\nsvr_mse=mean_squared_error(svr_preds,y_test)\nsvr_rmse=np.sqrt(svr_mse)\nsvr_rmse","b4a77e4d":"from sklearn.model_selection import RandomizedSearchCV\nimport scipy\n\n#the parameter grid for RandomizedSearchCV is a dictionary\npara_grid={\n     'svr__gamma': ['auto','scale'],\n     'data_pipe__pipe_num__imputer__strategy':['mean','median'],\n      'svr__kernel':['linear','rbf'],\n      'svr__C':[0.5,0.7,0.8,1.0,1.2,1.4]\n        \n    \n}\n\nRND_SEARCH_PIPL_SVR_LOC=\"\/kaggle\/input\/results\/rndsearch_over_pipe_svr\"\n#RND_SEARCH_PIPL_SVR_LOC=\"rndsearch_over_pipe_svr\"\ntry:\n    rnd_best_model=joblib.load(RND_SEARCH_PIPL_SVR_LOC)\n    print('loaded previous instance of random search with best_params: ',rnd_best_model.best_params_)\nexcept(FileNotFoundError):\n    print('File not found. continuing with RandomizedSearchCV')\n    rnd_p_svr_cv=RandomizedSearchCV(pipe_svr,cv=5,\n                                    param_distributions=para_grid,\n                                    n_iter=5,\n                                    scoring='neg_mean_squared_error',\n                                    n_jobs=-1)\n    rnd_best_model=rnd_p_svr_cv.fit(housing,housing_labels)  #https:\/\/chrisalbon.com\/machine_learning\/model_selection\/hyperparameter_tuning_using_random_search\/\n    joblib.dump(rnd_best_model,RND_SEARCH_PIPL_SVR_LOC)\n\n    \nprint(rnd_best_model.best_params_)\nsvr_rnd_preds=rnd_best_model.predict(X_test)\nsvr_rnd_mse=mean_squared_error(svr_rnd_preds,y_test)\nsvr_rnd_rmse=np.sqrt(svr_rnd_mse)\nsvr_rnd_rmse","02169356":"def indices_of_first_k_features(ar,k):\n    print(k)\n    return np.sort(np.argpartition(np.array(ar),-k)[-k:])   #get the indices of the k-highest values of the features\n     \n\n\n\nclass ImportantFeatures(BaseEstimator,TransformerMixin):\n    def __init__(self,feature_importances=None,kk=None):\n        self.feature_importances=feature_importances\n        self.kk=kk\n        \n    \n    def fit(self,X,y=None):\n        self.feature_indices=indices_of_first_k_features(self.feature_importances,self.kk)\n        return self\n    \n    def transform(self,X):\n        return X[:,self.feature_indices]\n    \n        \n        ","3fa4b02a":"k=5\n\npipe_pick_features=Pipeline(\n    [\n        ('data_pipe',full_pipe),\n        ('pick_k_important_features',ImportantFeatures(feature_importances,k))\n    ]\n)\n\nhousing_imp_feat=pipe_pick_features.fit_transform(housing)\n\n\n\n","d6107e0d":"housing_imp_feat","e9878734":"print(housing_imp_feat[0:3].toarray())","1c257ef0":"cols=[]\nfor i,j in sorted_list_of_important_features[0:k]:\n    cols.append(j)\n\n#print(housing_prep[0:3].toarray())\nimp_df=pd.DataFrame(housing_prep.toarray(),columns=attributes)\nprint(imp_df[0:3][cols])\n","0d9c5f5e":"\ndata_prep_predict=Pipeline(\n    [\n        ('pipe_pick_features',pipe_pick_features),    #here k=5 . In the next exercise we will run a grid search over k\n        ('svr',SVR())                                 #Will run a grid search for the best parameters in exercise 5\n    \n    ]\n)\n\ndata_prep_predict.fit(housing,housing_labels)\n\n\n\n\n","50b6c587":"#get some data.\nda=housing.iloc[0:3]\nda_lab=housing_labels.iloc[0:3]\n\n\n#run predictions\nda_preds=data_prep_predict.predict(da)\n\nprint('rmse : ',np.sqrt(mean_squared_error(da_lab,da_preds)))\n","e4c49b06":"\nfrom scipy.stats import reciprocal,expon\n\npipe_pick_features=Pipeline(\n    [\n        ('data_pipe',full_pipe),\n        ('pick_important_features',ImportantFeatures()),\n        ('svr',SVR())\n    ]\n)\n\n\nparam_grid=[{\n            'data_pipe__pipe_num__imputer__strategy':['mean','median','most_frequent'],  #nesting to reach the imputer \n            'pick_important_features__feature_importances':[feature_importances],    #fixed feature_importances from our RandomForestRegressor()\n            'pick_important_features__kk':list(range(1,len(feature_importances)+1)),\n            'svr__kernel': ['linear', 'rbf'],\n            'svr__C': np.linspace(1,20,num=5),\n            'svr__gamma': np.linspace(0.1,1,num=5)\n           }]\n\nGRID_SEARCH_EX_5_LOC=\"\/kaggle\/input\/results\/gridsearch_over_full_pipe\"\ntry:\n    gs_5=joblib.load(GRID_SEARCH_EX_5_LOC)\n    print('loaded previous instance of gridsearch with best_params: ',gs_5.best_params_)\nexcept(FileNotFoundError):\n    print('File not found. continuing with GridSearch')\n    gs_5=GridSearchCV(pipe_pick_features,\n                                     param_grid,\n                                     cv=2,\n                                     scoring='neg_mean_squared_error',    \n                                     verbose=2,\n                                     n_jobs=4\n                                    )\n    gs_5.fit(housing,housing_labels)\n    joblib.dump(gs_5,GRID_SEARCH_EX_5_LOC)\n    \n\n\n","4a73c031":"\n#predict\nbest_model=gs_5.best_estimator_\npreds=best_model.predict(X_test)\nmse=mean_squared_error(preds,y_test)\nrmse=np.sqrt(mse)\nprint(rmse)\n","e803800e":"Combine the two pipelines in one big pipeline using sklearns [FeatureUnion](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.FeatureUnion.html).  \nThis estimator applies a list of transformer objects **in parallel** to the input data, **then concatenates the results**. This is useful to combine several feature extraction mechanisms into a single transformer. ","eb833be6":"**Data Cleaning**  \nHandling missing values using sklearns [Imputer](https:\/\/scikit-learn.org\/stable\/modules\/impute.html)","f027a44f":"Now a break from pipelines.  \nI will return to this once again after the section on Training below.   \n\nThe output from our pipeline is the input to the regression algorithm.   \nThe first regression test  is the default implementation with default parameters and we will arrive at a score.  \nThe next attempt will be fine tuning the parameters to get better scores.  \nFinally I will put everything into one giant pipeline.   \ni.e our final pipeline will be like this.\n![](https:\/\/drive.google.com\/uc?export=download&id=1cZsFAxJjMQ5_bWAwfsp4PyyfUd0VkIz6)\n\n","1cf54853":"**2.** Replace GridSearchCV with RandomizedSearchCV","fd60d4e0":"**Fine tuning the model**  \nI will be using [GridSearch](https:\/\/scikit-learn.org\/stable\/modules\/grid_search.htm) to fine tune the hyperparameters and possibly arrive at a better scoring than 52105.\n\n","0ed0a77f":"The results from the pipeline match the one using our previous method.\n","31b43f8d":"There are 169 nans in the total_bedroom entries","556277c6":"The above table results from the total runs. (3X4X2=24)","17d0cf89":"**Handling Text and categorical data**  \n**The process of text->LabelEncoder->OneHotEncoder can now be replaced directly by the OneHotEncoder i.e text->OneHotEncoder**\nEncoding ocean_proximity entries ['<1H OCEAN','INLAND',etc..] as numbers.  \nThis is done using the [LabelEncoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html)","98b87b45":"**Training and Evaluation on Training Set with Default settings**  \nI will be using the [RandomForestRegressor](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html) and [CrossValidation](https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html)","4535376a":"there is a correlation between median_income and median_house_value.  \nNot too many low income group of people buying expensive houses.","56a8cfad":"**Pipelines**  \nWouldn't it be great if everything we did upto now can be made simpler? \ni.e something that does this.\n![](https:\/\/drive.google.com\/uc?export=download&id=17y3SJan5oAQd9tFccd2K50wp7V2CDGL6)\nThis is achieved via the use of [sklearn's Pipeline](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html).  \nPipelines can have flexibility built into them . We can write our own transformers to do custom tasks.   ","2db9acd5":"Further EDA on the training set  \nFirst make a copy to create the exploration set.   \n","d977e4d9":"I now add it to the data cleaning pipeline","09e53d63":"I will be using the *sorted_list_of_important_features* later on while solving one the chapters exercises.","0e6d0875":"**4.** Creat a pipeline that incorporates the full data preparation and the final prediction.\n","035ebbf1":"This image depicts what I have done so far  \n![](https:\/\/drive.google.com\/uc?export=download&id=1X1Sezi4cu9c_gsZ1Nn8uFyEjpM9AV-6m)\n\n","eba2aa6d":"**Transformation Pipelines**   \nNow we start with the building of a pipeline whose task will be to do all our data processing steps till now but in few lines of code at the end.  \nSo we feed input to our pipeline and it outputs our processed data.  \n\nThe pipeline includes a list of tuples of the form (name,Transformer_instance).  \nWe can refer to the variables used in a particular transformer referenced by its name using '__'  . \n \nConsider this   \n\nIf the pipeline is pp= [('svcname',SVC(kernel='linear')]  \nthen the 'C' parameter of the SVC can be referred to using svcname__C i.e \n\npp.set_params(svcname__C=0.2)  \n\n\nFor ease of reading I am replicating some variables here .   \nThere will be 2 pipelines. 1 for the numerical attributes and the other for the categorical variables.  \n","c6da321a":"**5.** Run GridSearch across the final pipeline.","b32dae25":"**Observations**  \nAttributes have different scales. (TD: scale)  \nThe 'total bedrooms' attribute has missing values.  (TD: Impute)  \nThe attributes are mostly tail-heavy, extending further right of the median than the left (TD: Normalize the data)\n","30b565d8":"Example of use of the Combined AttributesAdder","a10bb482":"**1.** Use an SVR and parameterize the model.  I will borrow the data preprocessing pipeline from the above code and replace the Regression algorithm with an SVR.","bc6369c4":"Creating a new attribute called income_category to see how the income classes are divided.  \n","9709a220":"**Custom Transformers**  \nAs in the book , I would like to create a class that takes in data and does some custom transformations which can then be placed inside the pipeline.\n\nInorder to write a transformer, the class needs to implement 3 methods.   \n1. fit()   \n2. transform()  \n3. fit_transform()  \n\nOur first transformer (CombinedAttributesAdder) adds new features to our data.  \n","2c476fce":"**3.** Add a transformer in the preparation pipeline to select only the most important attributes\n","3199bdf3":"**EDA**\n","0ea3f70d":"**Back to Pipelines**  \nNow I will incorporate a gridsearch and the regression algorithm and make the final pipeline.\n[GridSearch over a pipeline](https:\/\/scikit-learn.org\/stable\/modules\/compose.html#pipeline)\n","3f6c1e00":"** Exercises at the end of the chapter**","a7409f6c":"once we have encoded from strings to numbers, the next step is to perform one-hot-encoding (OHE). This is done because the results from the label encoder are not representative of actual distance between the classes .e.g. categories 0 and 4 are closer than categores 0 and 1. An ML algorithm might view this as the opposite. i.e. the algorithm might consider *<1H OCEAN* and *INLAND* as closer than *<1H OCEAN* and *NEAR OCEAN*","dd8c570e":"I will now confirm if these values are correct using the list of important features I built earlier ","69d80bd1":"**Feature Importance**  \nWhat features are important?  \nThis can be got from the RandomForestRegressor. I will access this from the pipeline as follows","34eea8ab":"I have been reading [Aurelien Gerons book](https:\/\/www.amazon.in\/Hands-Machine-Learning-Scikit-Learn-Tensor\/dp\/9352135210\/ref=sr_1_1?qid=1569817836&refinements=p_27%3AAurelien+Geron&s=books&sr=1-1). These are my notes and codes based on it.\nSome of the pipelines take a long while to run. So I have saved the output from their first runs and load the results for successive runs.","6001cbe8":"Now we are ready to clean the data.  \nThis is where the separation between the predictors (X) and labels (Y) needs to happen\n","e0864aad":"Ok. so this nearly matches the one in the chapter. (49694). ","7fc75746":"The second custom transformer loads in a pandas dataframe and transforms the dataframe into numpy values.  \nThis is done because the input is a dataframe and I would like to directly input the df into the transfomer pipeline which will then execute the data cleaning steps in the correct order","ccbd7628":"**Split into training and test datasets**   \nOur training data is not exactly balanced across the 5 classes. So we use [Stratified splitting](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedShuffleSplit.html) based on the income_cat attribute\n","dea80923":"As is visible , there are 3 added columns to our data.  ","ac73f24b":"Compare the distribution of the income_cat attribute over the entire dataset to the that in the startified train and test samples","b75ec152":"![](http:\/\/)Comparison shows similar distribution in the train and test data as in the original dataset.  \nNow that the splitting was done using the income_cat attribute, we can now remove it from the split datasets","46e10f7d":"That's California with clusters formed around Bay Area,LA,San Diego.  \nNow, is there any correlation between the features?","f78f05d9":"The resulting rmses are high and can be bought to lower values by testing a wider range of params. ( ${C}$ can be made higher and ${gamma}$ lower). However this is something I will leave for a later date. I feel I have achieved what I set out to learn from this chapter which was to develop an end-to-end ML pipeline.","6e24559b":"**Evaluation on the Test Set**"}}