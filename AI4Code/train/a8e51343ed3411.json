{"cell_type":{"819a3cde":"code","35e198bd":"code","53c34fd2":"code","b51d91fb":"code","8743ebf1":"code","936d11bb":"code","667cc358":"code","491fe766":"code","7744b2d7":"markdown","0c336f8a":"markdown","83ed35eb":"markdown","220b4ede":"markdown","ffd2ad0c":"markdown","711085c9":"markdown","831f0205":"markdown","aa52e2a9":"markdown","5f9b14f5":"markdown"},"source":{"819a3cde":"#Quick load dataset and check\nimport pandas as pd","35e198bd":"from google.colab import drive\ndrive.mount('\/content\/drive')","53c34fd2":"TRAIN_SET_PATH = \"\/content\/drive\/My Drive\/colab\/train_set.csv\"\nTEST_SET_PATH = \"\/content\/drive\/My Drive\/colab\/test_set.csv\"\ndata_train = pd.read_csv(TRAIN_SET_PATH)\ndata_test = pd.read_csv(TEST_SET_PATH)","b51d91fb":"n = 20\n\n#Helper Functions\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.feature_selection import chi2\nfrom sklearn.impute import SimpleImputer\n\n\ndef getTrainingValues(properties=[], impute=True):\n  df = pd.read_csv(TRAIN_SET_PATH)\n  \n  if(impute):\n    simple_imp = SimpleImputer(missing_values=-1, strategy='mean')\n    new_df = pd.DataFrame(simple_imp.fit_transform(df))\n    new_df.columns = df.columns\n    new_df.index = df.index\n    df = new_df\n\n  if len(properties) == 0:\n    # separate target & id from values \n    properties = list(df.columns.values)\n    properties.remove('target')\n    properties.remove('id')\n\n  X = df[properties]\n  y = df['target'].astype(int)\n  return X, y\n\ndef getTestValues(properties=[]):\n  df = pd.read_csv(TEST_SET_PATH)\n  orig = df\n\n  # do we need to impute here??\n  simple_imp = SimpleImputer(missing_values=-1, strategy='most_frequent')\n  new_df = pd.DataFrame(simple_imp.fit_transform(df))\n  new_df.columns = df.columns\n  new_df.index = df.index\n  df = new_df\n\n  if len(properties) == 0:\n    properties = list(df.columns.values)\n\n  X = df[properties]\n  return orig, X\n\ndef getNBestFeatures(n):\n  X, y = getTrainingValues(impute = True)\n  bestfeatures = SelectKBest(score_func=f_classif, k=n)\n  fit = bestfeatures.fit(X,y)\n  dfscores = pd.DataFrame(fit.scores_)\n  dfcolumns = pd.DataFrame(X.columns)\n  featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n  featureScores.columns = ['Specs','Score']\n  # print(featureScores.nlargest(n,'Score'))\n  return featureScores.nlargest(n,'Score')['Specs']\n\ndef extrac_one_label(x_val, y_val, label):\n    X_pos = x_val[y_val == label]\n    y_pos = y_val[y_val == label]\n    return X_pos, y_pos\n\ndef calc_class_weights(target):\n  neg, pos = np.bincount(target)\n  total = neg + pos\n  print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(total, pos, 100 * pos \/ total))\n  weight_for_0 = (1 \/ neg)*(total)\/2.0 \n  weight_for_1 = (1 \/ pos)*(total)\/2.0\n  class_weight = {0: weight_for_0, 1: weight_for_1}\n  print('Weight for class 0: {:.2f}'.format(weight_for_0))\n  print('Weight for class 1: {:.2f}'.format(weight_for_1))\n  return class_weight\n\ngetNBestFeatures(n)","8743ebf1":"import pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport imblearn\nfrom imblearn.over_sampling import SMOTE\n\nX, y = getTrainingValues(getNBestFeatures(n), impute=False)\n\noversample = SMOTE()\nX, y = oversample.fit_resample(X, y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\nmodel = keras.Sequential([\n    keras.layers.Flatten(input_shape=(n,)),\n    keras.layers.Dense(4, activation=tf.nn.relu),\n    keras.layers.Dense(4, activation=tf.nn.relu),\n    keras.layers.Dense(1, activation=tf.nn.sigmoid),\n])\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n#variate between epochs and batch size\nmodel.fit(X_train, y_train, epochs=10, batch_size=64)\n\ntest_loss, test_acc = model.evaluate(X_test, y_test)\nprint('Test accuracy:', test_acc)","936d11bb":"from sklearn import metrics\n\ndef get_score(y_test,y_pred):\n  score = metrics.f1_score(y_test, y_pred, labels=None, pos_label=1, average='binary', zero_division='warn')\n  print(\"binary f1 score is: \",score)\n  score = metrics.f1_score(y_test, y_pred, labels=None, pos_label=1, average='weighted', zero_division='warn')\n  print(\"weighted f1 score is: \",score)\n  score = metrics.f1_score(y_test, y_pred, labels=None, pos_label=1, average='macro', zero_division='warn')\n  print(\"macro f1 score is: \",score)\n  score = metrics.accuracy_score(y_test,y_pred)\n  print(\"total acc is: \",score)\n  return score","667cc358":"# test our results on ones and zeros \n\n# treshold for the probability to predict 0\/1\nTRESH = 0.5\n\ny_pred = model.predict(X_test)\n\ny_pred[y_pred < TRESH]  = 0\ny_pred[y_pred >= TRESH] = 1\nget_score(y_test, y_pred)\n\n\nX_pos, y_pos = extrac_one_label(X_test, y_test, 1)\nX_neg, y_neg = extrac_one_label(X_test, y_test, 0)\n\ny_negpred = model.predict(X_neg)\ny_negpred[y_negpred < TRESH]  = 0\ny_negpred[y_negpred >= TRESH] = 1\nprint(\"Accuracy of predicting 0:\", sum(y_negpred==0)\/len(y_negpred))\nprint(\"sum 0:\", sum(y_negpred==0))\n\n\ny_pospred = model.predict(X_pos)\ny_pospred[y_pospred < TRESH]  = 0\ny_pospred[y_pospred >= TRESH] = 1\nprint(\"Accuracy of predicting 1:\", sum(y_pospred==1)\/len(y_pospred))\nprint(\"sum 1:\", sum(y_pospred==1))\n","491fe766":"data_real, data_test = getTestValues(getNBestFeatures(n))\ny_target = model.predict(data_test)\n\nprint((y_target))\n\n\ny_target[y_target < TRESH]  = 0\ny_target[y_target >= TRESH] = 1\n\n\nprint(sum(y_target))\n\ndata_out = pd.DataFrame(data_real['id'].copy())\ndata_out.insert(1, \"target\", y_target.astype(int), True)\ndata_out.to_csv('submission.csv',index=False)\ndata_out","7744b2d7":"### Feature Selection","0c336f8a":"## Our Code\n","83ed35eb":"Change TRAIN_SET_PATH and TEST_SET_PATH if your datasets are located different.","220b4ede":"### Submission\n\nPlease only submit the csv files with predicted outcome with its id and target [here](https:\/\/www.kaggle.com\/t\/b3dc81e90d32436d93d2b509c98d0d71). Your column should only contain `0` and `1`.","ffd2ad0c":"### Model\n\n34        ps_car_13\n33        ps_car_12\n16    ps_ind_17_bin\n27    ps_car_07_cat\n19        ps_reg_02\n5     ps_ind_06_bin\n6     ps_ind_07_bin\n24    ps_car_04_cat\n23    ps_car_03_cat\n20        ps_reg_03\n22    ps_car_02_cat\n4     ps_ind_05_cat\n15    ps_ind_16_bin\n36        ps_car_15\n18        ps_reg_01\n14        ps_ind_15\n25    ps_car_05_cat\n28    ps_car_08_cat\n0         ps_ind_01\n21    ps_car_01_cat","711085c9":"# Machine Learning 2020 Course Projects\n\n## Project Schedule\n\nIn this project, you will solve a real-life problem with a dataset. The project will be separated into two phases:\n\n27th May - 10th June: We will give you a training set with target values and a testing set without target. You predict the target of the testing set by trying different machine learning models and submit your best result to us and we will evaluate your results first time at the end of phase 1.\n\n9th June - 24th June: Students stand high in the leader board will briefly explain  their submission in a proseminar. We will also release some general advice to improve the result. You try to improve your prediction and submit final results in the end. We will again ask random group to present and show their implementation.\nThe project shall be finished by a team of two people. Please find your teammate and REGISTER via [here](https:\/\/docs.google.com\/forms\/d\/e\/1FAIpQLSf4uAQwBkTbN12E0akQdxfXLgUQLObAVDRjqJHcNAUFwvRTsg\/alreadyresponded).\n\nThe submission and evaluation is processed by [Kaggle](https:\/\/www.kaggle.com\/t\/b3dc81e90d32436d93d2b509c98d0d71).  In order to submit, you need to create an account, please use your team name in the `team tag` on the [kaggle page](https:\/\/www.kaggle.com\/t\/b3dc81e90d32436d93d2b509c98d0d71). Two people can submit as a team in Kaggle.\n\nYou can submit and test your result on the test set 2 times a day, you will be able to upload your predicted value in a CSV file and your result will be shown on a leaderboard. We collect data for grading at 22:00 on the **last day of each phase**. Please secure your best results before this time.\n\n","831f0205":"If you are not on google colab or you dont want to link your drive, skip this part.","aa52e2a9":"## Project Description\n\nCar insurance companies are always trying to come up with a fair insurance plan for customers. They would like to offer a lower price to the careful and safe driver while the careless drivers who file claims in the past will pay more. In addition, more safe drivers mean that the company will spend less in operation. However, for new customers, it is difficult for the company to know who the safe driver is. As a result, if a company offers a low price, it bears a high risk of cost. If not, the company loses competitiveness and encourage new customers to choose its competitors.\n\n\nYour task is to create a machine learning model to mitigate this problem by identifying the safe drivers in new customers based on their profiles. The company then offers them a low price to boost safe customer acquirement and reduce risks of costs. We provide you with a dataset (train_set.csv) regarding the profile (columns starting with ps_*) of customers. You will be asked to predict whether a customer will file a claim (`target`) in the next year with the test_set.csv \n\n~~You can find the dataset in the `project` folders in the jupyter hub.~~ We also upload dataset to Kaggle and will test your result and offer you a leaderboard in Kaggle. Please find them under the Data tag on the following page:\nhttps:\/\/www.kaggle.com\/t\/b3dc81e90d32436d93d2b509c98d0d71","5f9b14f5":"## Phase 1: 26th May - 9th June\n\n### Data Description\n\nIn order to take a look at the data, you can use the `describe()` method. As you can see in the result, each row has a unique `id`. `Target` $\\in \\{0, 1\\}$ is whether a user will file a claim in his insurance period. The rest of the 57 columns are features regarding customers' profiles. You might also notice that some of the features have minimum values of `-1`. This indicates that the actual value is missing or inaccessible.\n"}}