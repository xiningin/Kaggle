{"cell_type":{"97a2c9f2":"code","fc919f4d":"code","5a591428":"code","33de4654":"code","f22548e6":"code","0d4aac6b":"code","b4b1de22":"code","7beb82f6":"code","33e61a44":"code","fd26ee6d":"code","25092c4e":"code","de4ec95f":"code","d28f7278":"code","8e73c9bd":"code","6f179a82":"code","81f61a9b":"code","618370b0":"code","5dfe37c2":"code","c4cab1c5":"code","5ed4d0fe":"code","1e499b9d":"code","352b266c":"code","162ce2e8":"code","f4582fab":"code","51f38e19":"code","b532adcf":"code","38fcbb95":"code","1ffa0a06":"code","f2eef7a2":"code","4c39c7f3":"code","40d8f020":"code","011bcdc9":"code","06fd8607":"code","e42cfd7a":"code","501eee09":"code","197a678e":"code","0c908c06":"code","42b3698f":"code","d7286bd7":"code","833b6da2":"code","27c892d8":"markdown","5a854740":"markdown","caa54f8b":"markdown","1f6a8a8e":"markdown","f1ca3946":"markdown","a716ec8d":"markdown","c4397bb1":"markdown","3402c61e":"markdown","5602b647":"markdown","7c4dfa64":"markdown","81476054":"markdown","56231125":"markdown","39ea8d18":"markdown","53e2e9e6":"markdown","3e353b9b":"markdown","e31657be":"markdown","c40fefdf":"markdown","b9a75225":"markdown","b64ef4dc":"markdown","8c5ae2b5":"markdown","99939b98":"markdown","f811cc9d":"markdown","979212b7":"markdown","e689a0d5":"markdown","bf586997":"markdown","7e229240":"markdown","77f6d05e":"markdown","f56dbe2b":"markdown","3689b11d":"markdown","edc96ee6":"markdown","e60c3868":"markdown","9ef468bd":"markdown","7f054ab6":"markdown","597ee46c":"markdown","6e92fc7b":"markdown"},"source":{"97a2c9f2":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\n#from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom IPython.core.display import display, HTML, Javascript\nfrom string import Template\nimport json\nimport IPython.display\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","fc919f4d":"water_df = pd.read_csv('\/kaggle\/input\/water-potability\/water_potability.csv')\nwater_df.head(4)","5a591428":"water_df.describe()","33de4654":"water_df.hist(column='ph', by='Potability')","f22548e6":"#first replace the Nan values with the mean of the classification\npH_0_1 = water_df.query('Potability == 1')['ph'][water_df['ph'] == 0].index\nwater_df.loc[pH_0_1,'ph'] = water_df.query('Potability == 1')['ph'][water_df['ph'] == 0 ].mean()\npH_nan_1 = water_df.query('Potability == 1')['ph'][water_df['ph'].isna()].index\nwater_df.loc[pH_nan_1,'ph'] = water_df.query('Potability == 1')['ph'][water_df['ph'].notna()].mean()\npH_0_0 = water_df.query('Potability == 0')['ph'][water_df['ph'] == 0].index\nwater_df.loc[pH_0_0,'ph'] = water_df.query('Potability == 0')['ph'][water_df['ph'] == 0 ].mean()\npH_nan_0 = water_df.query('Potability == 0')['ph'][water_df['ph'].isna()].index\nwater_df.loc[pH_nan_0,'ph'] = water_df.query('Potability == 0')['ph'][water_df['ph'].notna()].mean()\n#Set any value that fails the guideline for pH not to be potable\nwater_df.loc[~water_df.ph.between(6.5, 8.5), 'Potability'] = 0\n","0d4aac6b":"water_df.hist(column='ph', by='Potability')","b4b1de22":"water_df.hist(column='Hardness', by='Potability')","7beb82f6":"water_df.hist(column='Solids', by='Potability')","33e61a44":"sns.set_theme(style=\"ticks\")\n\nax = sns.regplot(x=\"Conductivity\", y=\"Solids\", data=water_df)","fd26ee6d":"water_df.hist(column='Chloramines', by='Potability')","25092c4e":"water_df.hist(column='Sulfate', by='Potability')","de4ec95f":"#first replace the Nan values with the mean of the classification\nSulfate_nan_1 = water_df.query('Potability == 1')['Sulfate'][water_df['Sulfate'].isna()].index\nwater_df.loc[Sulfate_nan_1,'Sulfate'] = water_df.query('Potability == 1')['Sulfate'][water_df['Sulfate'].notna()].mean()\nSulfate_nan_0 = water_df.query('Potability == 0')['Sulfate'][water_df['Sulfate'].isna()].index\nwater_df.loc[Sulfate_nan_0,'Sulfate'] = water_df.query('Potability == 0')['Sulfate'][water_df['Sulfate'].notna()].mean()\n","d28f7278":"water_df.hist(column='Conductivity', by='Potability')","8e73c9bd":"water_df.hist(column='Organic_carbon', by='Potability')","6f179a82":"water_df.hist(column='Trihalomethanes', by='Potability')","81f61a9b":"#first replace the Nan values with the mean of the classification\nTHM_nan_1 = water_df.query('Potability == 1')['Trihalomethanes'][water_df['Trihalomethanes'].isna()].index\nwater_df.loc[THM_nan_1,'Trihalomethanes'] = water_df.query('Potability == 1')['Trihalomethanes'][water_df['Trihalomethanes'].notna()].mean()\nTHM_nan_0 = water_df.query('Potability == 0')['Trihalomethanes'][water_df['Trihalomethanes'].isna()].index\nwater_df.loc[THM_nan_0,'Trihalomethanes'] = water_df.query('Potability == 0')['Trihalomethanes'][water_df['Trihalomethanes'].notna()].mean()\n#Set any value that fails the guideline for Trihalomethanes not to be potable\nwater_df.loc[water_df.Trihalomethanes > 80, 'Potability'] = 0","618370b0":"water_df.hist(column='Trihalomethanes', by='Potability')","5dfe37c2":"water_df.hist(column='Turbidity', by='Potability')","c4cab1c5":"sns.set_theme(style=\"ticks\")\nsns.pairplot(water_df, hue=\"Potability\")","5ed4d0fe":"quality_water_df = water_df","1e499b9d":"#Conductivity 400\nquality_water_df.loc[quality_water_df.Conductivity > 400, 'Potability'] = 0\n\n#If we applied all these criteria... we would have no potable samples...\n#Hardness 500 - taste\n#quality_water_df.loc[quality_water_df.Hardness > 500, 'Potability'] = 0\n#Solids 1000 - palability\n#quality_water_df.loc[quality_water_df.Solids > 1000, 'Potability'] = 0\n#Chloramines 4\n#quality_water_df.loc[quality_water_df.Chloramines > 4, 'Potability'] = 0\n#sulfate - no health impact\n#Organic_carbon 2\n#quality_water_df.loc[quality_water_df.Organic_carbon > 2, 'Potability'] = 0\n#Turbidity 5\n#quality_water_df.loc[quality_water_df.Turbidity > 5, 'Potability'] = 0","352b266c":"quality_water_df.describe()","162ce2e8":"sns.pairplot(quality_water_df, hue=\"Potability\")","f4582fab":"X = quality_water_df.drop([\"Potability\",\"Solids\"], axis=1) #dropping Solids as the results don't appear reliable\nY = quality_water_df[\"Potability\"]\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.10, random_state = 0)","51f38e19":"# Logistic Regression\n\nlogreg = LogisticRegression(solver='liblinear')\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","b532adcf":"coeff_df = pd.DataFrame(quality_water_df.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)","38fcbb95":"# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","1ffa0a06":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","f2eef7a2":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","4c39c7f3":"# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron","40d8f020":"# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc","011bcdc9":"# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd","06fd8607":"# Decision Tree\n\ndecision_tree = tree.DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","e42cfd7a":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","501eee09":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","197a678e":"# rules defined in the tree object clf\ndef rules(clf, features, labels, node_index=0):\n    \"\"\"Structure of rules in a fit decision tree classifier\n\n    Parameters\n    ----------\n    clf : DecisionTreeClassifier\n        A tree that has already been fit.\n\n    features, labels : lists of str\n        The names of the features and labels, respectively.\n\n    \"\"\"\n    node = {}\n    if clf.tree_.children_left[node_index] == -1:  # indicates leaf\n        #count_labels = zip(clf.tree_.value[node_index, 0], labels)\n        #node['name'] = ', '.join(('{} of {}'.format(int(count), label)\n        #                          for count, label in count_labels))\n        node['type']='leaf'\n        node['value'] = clf.tree_.value[node_index, 0].tolist()\n        node['error'] = np.float64(clf.tree_.impurity[node_index]).item()\n        node['samples'] = clf.tree_.n_node_samples[node_index]\n    else:\n        feature = features[clf.tree_.feature[node_index]]\n        threshold = clf.tree_.threshold[node_index]\n        node['type']='split'\n        node['label'] = '{} > {}'.format(feature, threshold)\n        node['error'] = np.float64(clf.tree_.impurity[node_index]).item()\n        node['samples'] = clf.tree_.n_node_samples[node_index]\n        node['value'] = clf.tree_.value[node_index, 0].tolist()\n        left_index = clf.tree_.children_left[node_index]\n        right_index = clf.tree_.children_right[node_index]\n        node['children'] = [rules(clf, features, labels, right_index),\n                            rules(clf, features, labels, left_index)]\n        \n    return node\n\nclass MyEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.integer):\n            return int(obj)\n        elif isinstance(obj, np.floating):\n            return float(obj)\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        else:\n            return super(MyEncoder, self).default(obj)","0c908c06":"cols = X_train.columns\nd = rules(decision_tree, cols, None)\nwith open('output.json', 'w') as outfile:  \n    json.dump(d, outfile,cls=MyEncoder)\n\nj = json.dumps(d, cls=MyEncoder)","42b3698f":"html_string = \"\"\"\n<!DOCTYPE html>\n<html>\n  <head>\n    <meta http-equiv=\"Content-Type\" content=\"text\/html;charset=utf-8\"\/>\n    <script type=\"text\/javascript\" src=\"https:\/\/d3js.org\/d3.v3.min.js\"><\/script>\n    <style type=\"text\/css\">\nbody {\n  font-family: \"Helvetica Neue\", Helvetica;\n}\n.hint {\n  font-size: 12px;\n  color: #999;\n}\n.node rect {\n  cursor: pointer;\n  fill: #fff;\n  stroke-width: 1.5px;\n}\n.node text {\n  font-size: 11px;\n}\npath.link {\n  fill: none;\n  stroke: #ccc;\n}\n    <\/style>\n  <\/head>\n  <body>\n    <div id=\"body\">\n      <div id=\"footer\">\n        Decision Tree viewer\n        <div class=\"hint\">click to expand or collapse<\/div>\n        <div id=\"menu\">\n          <select id=\"datasets\"><\/select>\n        <\/div>\n\n      <\/div>\n    <\/div>    \n\"\"\"","d7286bd7":"js_string=\"\"\"\n var m = [20, 120, 20, 120],\n    w = 1280 - m[1] - m[3],\n    h = 800 - m[0] - m[2],\n    i = 0,\n    rect_width = 80,\n    rect_height = 20,\n    max_link_width = 20,\n    min_link_width = 1.5,\n    char_to_pxl = 6,\n    root;\n\/\/ Add datasets dropdown\nd3.select(\"#datasets\")\n    .on(\"change\", function() {\n      if (this.value !== '-') {\n        d3.json(this.value + \".json\", load_dataset);\n      }\n    })\n  .selectAll(\"option\")\n    .data([\n      \"-\",\n      \"output\"\n    ])\n  .enter().append(\"option\")\n    .attr(\"value\", String)\n    .text(String);\nvar tree = d3.layout.tree()\n    .size([h, w]);\nvar diagonal = d3.svg.diagonal()\n    .projection(function(d) { return [d.x, d.y]; });\nvar vis = d3.select(\"#body\").append(\"svg:svg\")\n    .attr(\"width\", w + m[1] + m[3])\n    .attr(\"height\", h + m[0] + m[2] + 1000)\n  .append(\"svg:g\")\n    .attr(\"transform\", \"translate(\" + m[3] + \",\" + m[0] + \")\");\n\/\/ global scale for link width\nvar link_stoke_scale = d3.scale.linear();\nvar color_map = d3.scale.category10();\n\/\/ stroke style of link - either color or function\nvar stroke_callback = \"#ccc\";\nfunction load_dataset(json) {\n  root = json;\n  root.x0 = 0;\n  root.y0 = 0;\n  var n_samples = root.samples;\n  var n_labels = root.value.length;\n  if (n_labels >= 2) {\n    stroke_callback = mix_colors;\n  } else if (n_labels === 1) {\n    stroke_callback = mean_interpolation(root);\n  }\n  link_stoke_scale = d3.scale.linear()\n                             .domain([0, n_samples])\n                             .range([min_link_width, max_link_width]);\n  function toggleAll(d) {\n    if (d && d.children) {\n      d.children.forEach(toggleAll);\n      toggle(d);\n    }\n  }\n  \/\/ Initialize the display to show a few nodes.\n  root.children.forEach(toggleAll);\n  update(root);\n}\nfunction update(source) {\n  var duration = d3.event && d3.event.altKey ? 5000 : 500;\n  \/\/ Compute the new tree layout.\n  var nodes = tree.nodes(root).reverse();\n  \/\/ Normalize for fixed-depth.\n  nodes.forEach(function(d) { d.y = d.depth * 180; });\n  \/\/ Update the nodes\u2026\n  var node = vis.selectAll(\"g.node\")\n      .data(nodes, function(d) { return d.id || (d.id = ++i); });\n  \/\/ Enter any new nodes at the parent's previous position.\n  var nodeEnter = node.enter().append(\"svg:g\")\n      .attr(\"class\", \"node\")\n      .attr(\"transform\", function(d) { return \"translate(\" + source.x0 + \",\" + source.y0 + \")\"; })\n      .on(\"click\", function(d) { toggle(d); update(d); });\n  nodeEnter.append(\"svg:rect\")\n      .attr(\"x\", function(d) {\n        var label = node_label(d);\n        var text_len = label.length * char_to_pxl;\n        var width = d3.max([rect_width, text_len])\n        return -width \/ 2;\n      })\n      .attr(\"width\", 1e-6)\n      .attr(\"height\", 1e-6)\n      .attr(\"rx\", function(d) { return d.type === \"split\" ? 2 : 0;})\n      .attr(\"ry\", function(d) { return d.type === \"split\" ? 2 : 0;})\n      .style(\"stroke\", function(d) { return d.type === \"split\" ? \"steelblue\" : \"olivedrab\";})\n      .style(\"fill\", function(d) { return d._children ? \"lightsteelblue\" : \"#fff\"; });\n  nodeEnter.append(\"svg:text\")\n      .attr(\"dy\", \"12px\")\n      .attr(\"text-anchor\", \"middle\")\n      .text(node_label)\n      .style(\"fill-opacity\", 1e-6);\n  \/\/ Transition nodes to their new position.\n  var nodeUpdate = node.transition()\n      .duration(duration)\n      .attr(\"transform\", function(d) { return \"translate(\" + d.x + \",\" + d.y + \")\"; });\n  nodeUpdate.select(\"rect\")\n      .attr(\"width\", function(d) {\n        var label = node_label(d);\n        var text_len = label.length * char_to_pxl;\n        var width = d3.max([rect_width, text_len])\n        return width;\n      })\n      .attr(\"height\", rect_height)\n      .style(\"fill\", function(d) { return d._children ? \"lightsteelblue\" : \"#fff\"; });\n  nodeUpdate.select(\"text\")\n      .style(\"fill-opacity\", 1);\n  \/\/ Transition exiting nodes to the parent's new position.\n  var nodeExit = node.exit().transition()\n      .duration(duration)\n      .attr(\"transform\", function(d) { return \"translate(\" + source.x + \",\" + source.y + \")\"; })\n      .remove();\n  nodeExit.select(\"rect\")\n      .attr(\"width\", 1e-6)\n      .attr(\"height\", 1e-6);\n  nodeExit.select(\"text\")\n      .style(\"fill-opacity\", 1e-6);\n  \/\/ Update the links\n  var link = vis.selectAll(\"path.link\")\n      .data(tree.links(nodes), function(d) { return d.target.id; });\n  \/\/ Enter any new links at the parent's previous position.\n  link.enter().insert(\"svg:path\", \"g\")\n      .attr(\"class\", \"link\")\n      .attr(\"d\", function(d) {\n        var o = {x: source.x0, y: source.y0};\n        return diagonal({source: o, target: o});\n      })\n      .transition()\n      .duration(duration)\n      .attr(\"d\", diagonal)\n      .style(\"stroke-width\", function(d) {return link_stoke_scale(d.target.samples);})\n      .style(\"stroke\", stroke_callback);\n  \/\/ Transition links to their new position.\n  link.transition()\n      .duration(duration)\n      .attr(\"d\", diagonal)\n      .style(\"stroke-width\", function(d) {return link_stoke_scale(d.target.samples);})\n      .style(\"stroke\", stroke_callback);\n  \/\/ Transition exiting nodes to the parent's new position.\n  link.exit().transition()\n      .duration(duration)\n      .attr(\"d\", function(d) {\n        var o = {x: source.x, y: source.y};\n        return diagonal({source: o, target: o});\n      })\n      .remove();\n  \/\/ Stash the old positions for transition.\n  nodes.forEach(function(d) {\n    d.x0 = d.x;\n    d.y0 = d.y;\n  });\n}\n\/\/ Toggle children.\nfunction toggle(d) {\n  if (d.children) {\n    d._children = d.children;\n    d.children = null;\n  } else {\n    d.children = d._children;\n    d._children = null;\n  }\n}\n\/\/ Node labels\nfunction node_label(d) {\n  if (d.type === \"leaf\") {\n    \/\/ leaf\n    var formatter = d3.format(\".2f\");\n    var vals = [];\n    d.value.forEach(function(v) {\n        vals.push(formatter(v));\n    });\n    return \"[\" + vals.join(\", \") + \"]\";\n  } else {\n    \/\/ split node\n    return d.label;\n  }\n}\n\/**\n * Mixes colors according to the relative frequency of classes.\n *\/\nfunction mix_colors(d) {\n  var value = d.target.value;\n  var sum = d3.sum(value);\n  var col = d3.rgb(0, 0, 0);\n  value.forEach(function(val, i) {\n    var label_color = d3.rgb(color_map(i));\n    var mix_coef = val \/ sum;\n    col.r += mix_coef * label_color.r;\n    col.g += mix_coef * label_color.g;\n    col.b += mix_coef * label_color.b;\n  });\n  return col;\n}\n\/**\n * A linear interpolator for value[0].\n *\n * Useful for link coloring in regression trees.\n *\/\nfunction mean_interpolation(root) {\n  var max = 1e-9,\n      min = 1e9;\n  function recurse(node) {\n    if (node.value[0] > max) {\n      max = node.value[0];\n    }\n    if (node.value[0] < min) {\n      min = node.value[0];\n    }\n    if (node.children) {\n      node.children.forEach(recurse);\n    }\n  }\n  recurse(root);\n  var scale = d3.scale.linear().domain([min, max])\n                               .range([\"#2166AC\",\"#B2182B\"]);\n  function interpolator(d) {\n    return scale(d.target.value[0]);\n  }\n  return interpolator;\n}\n \"\"\"","833b6da2":"h = display(HTML(html_string))\nj = IPython.display.Javascript(js_string)\nIPython.display.display_javascript(j)","27c892d8":"There are a few values below what is considered good for drinking water.\nWe can aslo replace the values as we did with pH.","5a854740":"This model uses a decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. \n\nThe model confidence score is the highest among models evaluated so far.","caa54f8b":"## Trihalomethanes\nTHM levels up to 80 ppm is considered safe in drinking water.","1f6a8a8e":"## Solids (TDS)\nDesirable limit for TDS is 500 mg\/L and maximum limit is 1000 mg\/l which prescribed for drinking purpose.  However these guidelines are based on taste.  Over 1000 mg\/L is considered unacceptable.","f1ca3946":"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.","a716ec8d":"# Data cleaning\nWe have incomplete data for pH, Sulphate & Trihalomethanes.\nIdeally we want to replace or remove the missing values. The question is what to replace the missing values with?  For now it will be assumed to be the mean of the classification unless a relationship can be observed.","c4397bb1":"## Sulfate\nIt is generally considered that taste impairment is minimal at levels below 250 mg\/l. No health-based guideline value has been derived for sulfate.","3402c61e":"Many samples classified potable would not pass the US EPA","5602b647":"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time.","7c4dfa64":"# Exploring models\n\n","81476054":"Reclassifying all smaples that don't pass a quality standard.","56231125":"Again some of the samples classified as potable are above the limits","39ea8d18":"As with the ph, we will replace some of the missing values and reclassify the values above the safe limits","53e2e9e6":"# Getting to know the data\n\n## pH\npH is a measurement of electrically charged particles in a substance. It indicates how acidic or alkaline (basic) that substance is. The pH scale ranges from 0 to 14\n\nThe WHO guidelines for Drinking water is between 6.5 and 8.5\nOutside these limits many halmful metals may become soluble.\n\n## Hardness\nHardness is the amount of dissolved calcium and magnesium in the water.\nNot of health concern at levels found in\ndrinking-water, however can remove other metals that maybe be harmful.\n\nThe taste threshold for the calcium ion is in the range of 100\u2013300 mg\/l. the taste threshold for magnesium is probably lower than that for calcium. In\nsome instances, consumers tolerate water hardness in excess of 500 mg\/l\n\n## Solids\nTotal dissolved solids (TDS) comprise inorganic salts (principally calcium, magnesium, potassium, sodium, bicarbonates, chlorides and sulfates) and small amounts of organic matter that are dissolved in water\n\nThe palatability of water with a total dissolved solids (TDS) level of less than about 600 mg\/l is generally considered to be good; drinking-water becomes significantly and increasingly unpalatable at TDS levels greater than about 1000 mg\/l. \n\nDesirable limit for TDS is 500 mg\/l and maximum limit is 1000 mg\/l which prescribed for drinking purpose.\n\n## Chloramines\nMonochloramine, dichloramines and trichloramines are considered by-products of drinking-water chlorination. Chloramines, such as monochloramine, dichloramine and trichloramine (nitrogen trichloride), are generated from the reaction of chlorine with ammonia. Among chloramines, monochloramine is the only useful chlorine disinfectant, and chloramination systems are operated to minimize the formation of dichloramine and trichloramine. Higher chloramines, particularly trichloramine, are likely to give rise to taste and odour complaints, except at very low concentrations.\n\nFor monochloramine, no odour or taste was detected at concentrations between\n0.5 and 1.5 mg\/l. For dichloramine, the organoleptic effects between 0.1 and 0.5 mg\/l were found to be \u201cslight\u201d and \u201cacceptable\u201d. \nMost individuals are able to taste chloramines at concentrations below 5\nmg\/l, and some at levels as low as 0.3 mg\/l.\n\nChlorine levels up to 4 milligrams per liter (mg\/L or 4 parts per million (ppm)) are considered safe in drinking water.\n\n## Sulfate\nSulfates occur naturally in numerous minerals and are used commercially, principally in the chemical industry. They are discharged into water in industrial wastes and through atmospheric deposition; however, the highest levels usually occur in groundwater and are from natural sources.\nThe presence of sulfate in drinking-water can cause noticeable taste, and very high levels might cause a laxative effect in unaccustomed consumers. Taste impairment varies with the nature of the associated cation; taste thresholds have been found to range from 250 mg\/l for sodium sulfate to 1000 mg\/l for calcium sulfate. It is generally considered that taste impairment is minimal at levels below 250 mg\/l. No health-based guideline value has been derived for sulfate.\nNot of health concern at levels found in drinking-water.\nThe ratio of the chloride and sulfate concentrations to the bicarbonate concentration (Larson ratio).\n\n## Conductivity\nPure water is not a good conductor of electric current rather\u2019s a good insulator. Increase in ions concentration enhances the electrical conductivity of water. Generally, the amount of dissolved solids in water determines the electrical conductivity. Electrical conductivity (EC) actually measures the ionic process of a solution that enables it to transmit current. According to WHO standards, EC value should not exceeded 400 \u03bcS\/cm.\n\n## Total Organic Carbon (TOC)\nHigh colour from natural organic carbon (e.g. humics) could also indicate a high propensity to produce by-products from disinfection processes. No health-based guideline value is proposed for colour in drinking-water. According to US EPA < 2 mg\/L as TOC in treated \/ drinking water, and < 4 mg\/Lit in source water which is use for treatment.\n\n## Trihalomethanes\nTHMs are formed in drinking-water primarily as a result of chlorination of organic matter present naturally in raw water supplies. The rate and degree of THM formation increase as a function of the chlorine and humic acid concentration, temperature, pH and bromide ion concentration.\nBromide can be involved in the reaction between chlorine and naturally occurring organic matter in drinking-water, forming brominated and mixed chloro-bromo by-products, such as trihalomethanes (THMs) and halogenated acetic acids (HAAs), or it can react with ozone to form bromate. Trihalomethanes and haloacetic acids are the most common DBPs and occur at among the highest concentrations in drinking-water.\nTHM levels up to 80 ppm is considered safe in drinking water.\n\n## Turbidity\nThe turbidity of water depends on the quantity of solid matter present in the suspended state. It is a measure of light emitting properties of water and the test is used to indicate the quality of waste discharge with respect to colloidal matter. \nHigh levels of turbidity can protect microorganisms from the effects of disinfection, stimulate the growth of bacteria and give rise to a\nsignificant chlorine demand.\nThe mean turbidity value obtained for Wondo Genet Campus (0.98 NTU) is lower than the WHO recommended value of 5.00 NTU.\n\n## Potability\nIndicates if water is safe for human consumption where 1 means Potable and 0 means Not potable.","3e353b9b":"Logistic Regression is a useful model to run early in the workflow. Logistic regression measures the relationship between the categorical dependent variable (feature) and one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution. \n\nNote the confidence score generated by the model based on our training dataset.","e31657be":"We can use Logistic Regression to validate our assumptions and decisions for feature creating and completing goals. This can be done by calculating the coefficient of the features in the decision function.\n\nPositive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability).","c40fefdf":"Again there is a large number of samples that are above the acceptable drinking limits","b9a75225":"Again some of the conductivity values of the samples calssified potable exceed the stated limits. Note again that we have no values that are close to seawater which contracts the Solids (TDS) values.\n<img src=\"https:\/\/www.fondriest.com\/environmental-measurements\/wp-content\/uploads\/2014\/02\/conductivity_averages.jpg\" width=\"250\"> <br \/> https:\/\/www.fondriest.com\/environmental-measurements\/parameters\/water-quality\/conductivity-salinity-tds\/","b64ef4dc":"All values are within the acceptable range.","8c5ae2b5":"## Turbidity\nWHO recommended value below 5.00 NTU, ideally below 1 NTU","99939b98":"D3 Tree visualisation - from https:\/\/www.kaggle.com\/bhavesh09\/titanic-decision-tree-visual-with-d3-js","f811cc9d":"## Hardness\nIn some instances, consumers tolerate water hardness in excess of 500 mg\/l. ","979212b7":"## Chloramines\n\nChlorine levels up to 4 milligrams per liter (mg\/L or 4 parts per million (ppm)) are considered safe in drinking water. Chloramines are lower based on taste and smell.","e689a0d5":"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem.","bf586997":"From: https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\n\nNow we are ready to train a model and predict the required solution. There are 60+ predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification and regression problem. We want to identify relationship between output (Potable or not) with other variables or features (water chemistry \/ measurements). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. These include:\n\nLogistic Regression\nKNN or k-Nearest Neighbors\nSupport Vector Machines\nNaive Bayes classifier\nDecision Tree\nRandom Forrest\nPerceptron\nArtificial neural network\nRVM or Relevance Vector Machine\n\nSetting up the training and test sets","7e229240":"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. \n","77f6d05e":"## Model evaluation\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees' habit of overfitting to their training set.","f56dbe2b":"There are values outside of the WHO guidelines that are classified as Potable.  We will reclassify these values.","3689b11d":"## Conductivity\nEC value should not exceeded 400 \u03bcS\/cm.","edc96ee6":"Next we model using Support Vector Machines which are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training samples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new test samples to one category or the other, making it a non-probabilistic binary linear classifier.\n","e60c3868":"## Total Organic Carbon (TOC)\nAccording to US EPA < 2 mg\/L as TOC in treated \/ drinking water","9ef468bd":"## pH\nThe WHO guidelines for Drinking water is between 6.5 and 8.5 Outside these limits many halmful metals may become soluble.","7f054ab6":"There is a large number of water samples that are above the acceptable 1,000 mg\/l TDS limit.  However this would leave most of our water samples unaceptable, it is only for this reason we will not reclassify the water samples.  There is expected to be a strong correclation between TDS and Conductivity.\n<img src=\"https:\/\/www.researchgate.net\/profile\/Azm-Al-Homoud\/publication\/227328358\/figure\/fig10\/AS:397293128830980@1471733471117\/Relationship-between-electrical-conductivity-EC-and-total-dissolved-solids-TDS.png\" width=\"400\"> https:\/\/www.researchgate.net\/figure\/Relationship-between-electrical-conductivity-EC-and-total-dissolved-solids-TDS_fig10_227328358","597ee46c":"# Starting with Exploring the Water Samples","6e92fc7b":"The solids values are questionable.  EC will be a more reliable measure of the quality of water."}}