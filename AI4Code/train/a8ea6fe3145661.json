{"cell_type":{"2d423ffd":"code","8b8e8ab6":"code","16b2fc51":"code","678cd66a":"code","47f16f58":"code","19a2fa15":"code","7aab7b73":"code","4fc34835":"code","a1e8ac70":"code","e99626a0":"code","24ca27b5":"code","73632d3b":"code","090e9f43":"code","3a4fcdc8":"code","8907275f":"code","618a8b67":"code","04aac792":"code","f15192f5":"code","cdca7b21":"markdown"},"source":{"2d423ffd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8b8e8ab6":"df = pd.read_csv(\"..\/input\/student-marks-dataset\/Student.csv\")\nprint(df.head())\nprint(f\"shape of the dataset is : {df.shape}\")","16b2fc51":"df.isnull().sum()","678cd66a":"df.Result.value_counts()","47f16f58":"#imbalanced data so we need to balance it\n\nResult_0 = df[df[\"Result\"] == 0]\nResult_1 = df[df[\"Result\"] == 1]\nfrom sklearn.utils import resample\ndf_minority_upsampled = resample(Result_1, replace = True, n_samples = 755)\nnew_df = pd.concat([Result_0, df_minority_upsampled])\n\nfrom sklearn.utils import shuffle\nnew_df = shuffle(new_df)","19a2fa15":"new_df.Result.value_counts()","7aab7b73":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense\n","4fc34835":"X = new_df.drop(['Result'], axis = 1)\ny = new_df['Result']","a1e8ac70":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","e99626a0":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","24ca27b5":"from tensorflow import keras\n\nmodel = keras.Sequential(\n    [ keras.layers.Dense(\n            10, activation=\"relu\", input_shape=[3]\n        ),\n        keras.layers.Dense(25, activation=\"relu\"),\n        keras.layers.Dropout(0.3),\n        keras.layers.Dense(30, activation=\"relu\"),\n        keras.layers.Dropout(0.3),\n        keras.layers.Dense(1, activation=\"sigmoid\"),\n    ]\n)\nmodel.summary()","73632d3b":"model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['binary_accuracy'])","090e9f43":"early_stopping = keras.callbacks.EarlyStopping(patience = 10, min_delta = 0.001, \n                                               restore_best_weights =True )\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_test, y_test),\n    batch_size=40,\n    epochs=500,\n    callbacks=[early_stopping],\n    verbose=1, \n)","3a4fcdc8":"predictions =(model.predict(X_test)>0.5).astype(\"int32\")\n              \npredictions[:10]","8907275f":"model.evaluate(X_test, y_test)","618a8b67":"model.evaluate(X_train, y_train)","04aac792":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\naccuracy_score(y_test, predictions)","f15192f5":"print(classification_report(y_test, predictions))","cdca7b21":"94% accuracy"}}