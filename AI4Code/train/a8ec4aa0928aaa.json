{"cell_type":{"fb620829":"code","f409b1bc":"code","5b1d286d":"code","5f7d31e0":"code","7b5e3935":"code","9b3583aa":"code","93cef01a":"code","f5545272":"code","2dc4dc18":"code","8d4b6907":"code","50058186":"code","d5f65655":"code","8942eea0":"code","a308ae16":"code","a6220c3c":"code","3b0fb097":"code","85896043":"code","9327c605":"code","c71a2c20":"code","29a369e3":"code","b93cb892":"code","c8fc9bb7":"code","804d9b7e":"markdown","b6b9cd72":"markdown","79a6c4f3":"markdown","29777b7b":"markdown","750ba9c0":"markdown","7b252d07":"markdown"},"source":{"fb620829":"import spacy\nfrom gensim.parsing.preprocessing import remove_stopwords\nfrom nltk.corpus  import stopwords\nimport re\nfrom gensim.utils import lemmatize\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tqdm import tqdm\nfrom tensorflow import keras\nimport tensorflow as tf\nimport datetime\nimport numpy as np\nimport pandas as pd\nfrom transformers import *\nfrom keras.utils.np_utils import to_categorical","f409b1bc":"#nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\",'ner'])\ndf_train=pd.read_csv('\/kaggle\/input\/dbpedia-classes\/DBPEDIA_train.csv',encoding='utf-8-sig')\ndf_val=pd.read_csv('\/kaggle\/input\/dbpedia-classes\/DBPEDIA_val.csv',encoding='utf-8-sig')\ndf_test=pd.read_csv('\/kaggle\/input\/dbpedia-classes\/DBPEDIA_test.csv',encoding='utf-8-sig')","5b1d286d":"df_train=df_train.dropna(axis=0)\ndf_val=df_val.dropna(axis=0)\ndf_test=df_test.dropna(axis=0)","5f7d31e0":"def cleaning(df):\n    #df.loc[:,'SentimentText']=pd.DataFrame(df.loc[:,'SentimentText'].str.lower())\n    df.loc[:,'text'] = [re.sub(r'\\d+','', i) for i in df.loc[:,'text']]\n    df.loc[:,'text'] = [re.sub(r'[^a-zA-Z]',' ', i) for i in df.loc[:,'text']]\n    df.loc[:,'text'] = [re.sub(r\"\\b[a-zA-Z]\\b\", ' ', i) for i in df.loc[:,'text']]\n    \n    #df.loc[:,'text'] = [re.sub(r\"[#|\\.|_|\\^|\\$|\\&|=|;|,|\u2010|-|\u2013|(|)|\/\/|\\\\+|\\|*|\\']+\",'', i) for i in df.loc[:,'text']]\n    df.loc[:,'text'] = [re.sub(' +',' ', i) for i in df.loc[:,'text']]\n    return(df)","7b5e3935":"df_train=cleaning(df_train)\ndf_val=cleaning(df_val)\ndf_test=cleaning(df_test)","9b3583aa":"\nfrom sklearn.preprocessing import LabelEncoder\n# encode class values as integers\nencoder = LabelEncoder()\nencoder.fit(df_train['l3'])\nencoded_Y = encoder.transform(df_train['l3'])","93cef01a":"import pandas as pd\nimport torch\nimport transformers\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import DistilBertModel, DistilBertTokenizer,DistilBertForMaskedLM","f5545272":"from torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'","2dc4dc18":"# Defining some key variables that will be used later on in the training\nMAX_LEN = 512\nTRAIN_BATCH_SIZE = 4\nVALID_BATCH_SIZE = 2\nEPOCHS = 1\nLEARNING_RATE = 1e-05\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')","8d4b6907":"df_train['encoded_l3']=encoded_Y\ndf_val['encoded_l3']=encoder.transform(df_val['l3'])\ndf_test['encoded_l3']=encoder.transform(df_test['l3'])","50058186":"df_data=df_train.iloc[:,[0,4]]","d5f65655":"class Triage(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.len = len(dataframe)\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __getitem__(self, index):\n        text = str(self.data.text[index])\n        text = \" \".join(text.split())\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'targets': torch.tensor(self.data.encoded_l3[index], dtype=torch.long)\n        } \n    \n    def __len__(self):\n        return self.len","8942eea0":"'''\ntrain_size = 0.8\ntrain_dataset=df_data.sample(frac=train_size,random_state=200).reset_index(drop=True)\ntest_dataset=df_data.drop(train_dataset.index).reset_index(drop=True)\n\n\nprint(\"FULL Dataset: {}\".format(df_data.shape))\nprint(\"TRAIN Dataset: {}\".format(train_dataset.shape))\nprint(\"TEST Dataset: {}\".format(test_dataset.shape)) \n'''\ntraining_set = Triage(df_train, tokenizer, MAX_LEN)\ntesting_set = Triage(df_test, tokenizer, MAX_LEN)","a308ae16":"train_params = {'batch_size': TRAIN_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntest_params = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntraining_loader = DataLoader(training_set, **train_params)\ntesting_loader = DataLoader(testing_set, **test_params)","a6220c3c":"\n# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n\nclass DistillBERTClass(torch.nn.Module):\n    def __init__(self):\n        super(DistillBERTClass, self).__init__()\n        self.l1 = transformers.DistilBertModel.from_pretrained('distilbert-base-uncased')\n        self.l2 = torch.nn.Dropout(0.2)\n        self.l3 = torch.nn.Linear(768, 1)\n    \n    def forward(self, ids, mask):\n        output_1= self.l1(ids, mask)\n        output_2 = self.l2(output_1[0])\n        output = self.l3(output_2)\n        return output","3b0fb097":"\nmodel = DistillBERTClass()\nmodel.to(device)","85896043":"# Creating the loss function and optimizer\nloss_function = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)","9327c605":"def train(epoch):\n    model.train()\n    for _,data in enumerate(training_loader, 0):\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        targets = data['targets'].to(device, dtype = torch.long)\n\n        outputs = model(ids, mask).squeeze()\n\n        optimizer.zero_grad()\n        loss = loss_function(outputs, targets)\n        if _%10000==0:\n            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()","c71a2c20":"for epoch in range(EPOCHS):\n    train(epoch)","29a369e3":"def valid(model, testing_loader):\n    model.eval()\n    n_correct = 0; n_wrong = 0; total = 0\n    with torch.no_grad():\n        for _, data in enumerate(testing_loader, 0):\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n            targets = data['targets'].to(device, dtype = torch.long)\n            outputs = model(ids, mask).squeeze()\n            big_val, big_idx = torch.max(outputs.data, dim=1)\n            total+=targets.size(0)\n            n_correct+=(big_idx==targets).sum().item()\n    return (n_correct*100.0)\/total","b93cb892":"print('This is the validation section to print the accuracy and see how it performs')\nprint('Here we are leveraging on the dataloader crearted for the validation dataset, the approcah is using more of pytorch')\n\nacc = valid(model, testing_loader)\nprint(\"Accuracy on test data = %0.2f%%\" % acc)","c8fc9bb7":"torch.save(model,'model_dbpedia.pt')","804d9b7e":"The data has three Class variables L1(9 classes), L2(70 Classes), L3(200+ classes). We are focussing on L3 i.e 200+ classes","b6b9cd72":"Let's do some basic pre-processing of text to make Bert's job easier.","79a6c4f3":"Load pytorch and Hugging Face related packages ","29777b7b":"This kernel is prepared to demonstrate the utilisation and efficiancy of pre-trained Bert in Sentence classification with classes being as high as 200+\nThe Data used is kaggle available Wikipdia data with over 200k+ training articles","750ba9c0":"It's recommended to Label emcode output class variable ","7b252d07":"Enable GPU"}}