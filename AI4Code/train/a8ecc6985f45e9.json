{"cell_type":{"de5a4847":"code","b443dbf7":"code","630e6956":"code","882fb671":"code","7fe21200":"code","e1034cf5":"code","b75446c8":"code","7459a7d7":"code","8436ac1a":"code","b59ee1e9":"code","c35ca6cc":"code","e9955b79":"code","e5b271e4":"code","ecc529d2":"code","a22ea2da":"code","cd53a02f":"code","94e40542":"code","82389e62":"code","fddf030a":"code","b806de26":"code","69621706":"code","c4d09ced":"code","7307752c":"code","f12cb8d5":"code","25125285":"code","595bf2b4":"code","21f67c6e":"code","6b1def76":"code","57816327":"code","1286e9cb":"code","1d509ce6":"code","84f120c3":"code","6245b32f":"code","2f22e442":"code","1cf75aed":"code","7083206a":"markdown","cdc4cbe1":"markdown","35a12d55":"markdown","447c02a5":"markdown","e21f87a2":"markdown","91cebb90":"markdown","a2146f93":"markdown","60eb2fe3":"markdown","5f3796da":"markdown","696a6b9b":"markdown","45aaea7f":"markdown","ee77a953":"markdown","5cdb335d":"markdown","b481f9c4":"markdown","a79e7279":"markdown","d607e478":"markdown","167a832c":"markdown","d543ebe3":"markdown","473377ad":"markdown","6ed18fb4":"markdown","7878c7c3":"markdown","3a132811":"markdown","b8de6ce2":"markdown","e3bef111":"markdown","78b0dac1":"markdown","6d6dad9f":"markdown","a241dd40":"markdown","bb79199f":"markdown","416ff638":"markdown","b6c5984e":"markdown","066b9907":"markdown","22f2fddc":"markdown","5ca7aaae":"markdown","d598db8f":"markdown","8a343f3d":"markdown","581c3a7d":"markdown","29e65760":"markdown","f442c080":"markdown","15620f0d":"markdown","ca111a88":"markdown","441e649a":"markdown","d43d969c":"markdown","7097b0d8":"markdown","e996bf22":"markdown","90fcee9c":"markdown","b97cd68c":"markdown","e3e9a173":"markdown"},"source":{"de5a4847":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, f1_score\nimport pickle\nimport os.path\nimport plotly.offline as pyo\nimport plotly.graph_objs as go\nimport spacy\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer","b443dbf7":"#","630e6956":"# Load one of the availables trained pipelines for English\n# English pipeline optimized for CPU. Components: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer.\nnlp = spacy.load('en_core_web_sm')\n\n\n# stop words built in spacy (english)\nprint(nlp.Defaults.stop_words)","882fb671":"print(f\"Number of default stop words : {len(nlp.Defaults.stop_words)}\")","7fe21200":"# Checking if a word is a stop word\nnlp.vocab['is'].is_stop","e1034cf5":"nlp.vocab['below'].is_stop","b75446c8":"nlp.vocab['btw'].is_stop","7459a7d7":"s_stemmer = SnowballStemmer(language='english')","8436ac1a":"words = ['run', 'runner', 'ran', 'runs', 'easily', 'fairly', 'fairness','boats','boating']","b59ee1e9":"for word in words:\n    print(word+ ' ------> ' + s_stemmer.stem(word))","c35ca6cc":"# Function to display lemmas\ndef show_lemmas(text):\n    for token in text:\n        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}')","e9955b79":"doc = nlp(u\"I saw eighteen mice today!\")\nshow_lemmas(doc)","e5b271e4":"doc = nlp(u\"I am meeting him tomorrow at the meeting.\")\nshow_lemmas(doc)","ecc529d2":"count_vect = CountVectorizer()","a22ea2da":"phrase = [\"I'd like to have a glass of water please\"]","cd53a02f":"# Fit Vectorizer to the Data (build a vocab, count the number of words...)\n# Learn a vocabulary dictionary of all tokens in the raw documents\ncount_vect.fit(phrase)","94e40542":"# Show features\ncount_vect.get_feature_names()","82389e62":"# Learn the vocabulary dictionary and return document-term matrix\ncount_vect.fit_transform(phrase)","fddf030a":"# shows a mapping of terms to feature indices.\ncount_vect.vocabulary_","b806de26":"df = pd.read_csv('\/kaggle\/input\/mbti-personality-types-500-dataset\/MBTI 500.csv')","69621706":"df.head()","c4d09ced":"df['posts'][0]","7307752c":"df['type'][0]","f12cb8d5":"df['type'].unique()","25125285":"print(f\"Total of {len(df['type'].unique())} types of classified MBTI posts\")","595bf2b4":"df.isnull().sum()","21f67c6e":"df_bar_chart=df.groupby('type').count()\n\n\ntrace1 = go.Bar(x=df_bar_chart.index, y=df_bar_chart['posts'])\n\ndata = [trace1]\nlayout = go.Layout(title='MBTI # Classified Posts per Type')\n\nfig = go.Figure(data=data, layout=layout)\n\nfig.show()","6b1def76":"# Flag to re-create or not the machine learning model\nrecreate_model=False","57816327":"# We'll save the model into a file:\nfilename = 'mbti_svm_v2.sav'","1286e9cb":"# If the model file doesn't exists\nif not os.path.isfile(filename):\n    recreate_model=True","1d509ce6":"X = df['posts'] # features\ny = df['type']  # labels\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","84f120c3":"# Check if need to recreate the model\nif recreate_model:    \n    \n    # Creating an instance to vectorizer:\n    vectorizer = TfidfVectorizer()\n    \n    # Training the vectorizer:\n    X_train_tfidf = vectorizer.fit_transform(X_train)\n    \n    # Training the classifier:\n    clf = LinearSVC()\n    clf.fit(X_train_tfidf, y_train)\n    \n    # Pipelining the vectorizer and the classifier\n    text_clf = Pipeline([('tfidf',TfidfVectorizer()),('clf',LinearSVC())])\n    text_clf.fit(X_train, y_train)\n    \n    # saving the model to disk\n    pickle.dump(text_clf, open(filename, 'wb'))\n\n# If there is no need to recreate the model, just open the file from the disk    \nelse:\n    # loading the model from disk\n    text_clf = pickle.load(open(filename, 'rb'))","6245b32f":"predictions = text_clf.predict(X_test)","2f22e442":"print(classification_report(y_test, predictions))","1cf75aed":"print(f\"Overall accuracy of the model: {round(metrics.accuracy_score(y_test, predictions),2)}\")","7083206a":"# Training the model, save it to disk and open to make predictions","cdc4cbe1":"MBTI (Myers-Briggs Type Indicator) is an introspective self-report questionnaire indicating differing psychological preferences (cognitive functions) in how people perceive the world and make decisions","35a12d55":"### Stemming","447c02a5":"References:\n\n - https:\/\/en.wikipedia.org\/wiki\/Support-vector_machine\n\n- https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC","e21f87a2":"# Context","91cebb90":"This machine learning model takes it's time to train data\n\nTo avoid waiting every time, We're going to use the feature dump\/load from pickle","a2146f93":"It converts a collection of raw documents to a matrix of TF-IDF features.","60eb2fe3":"Stemming is used to return similarities words on the search process. \n \n - Example: search=boat, also returns \"boats\" and \"boating\"","5f3796da":"\"Stop words\" are words that appears so frequently that don't require tagging as thoroughly as nouns, verbs and modifiers","696a6b9b":"The machine learning supervised model that we'll use here is a Classification kind, named Support Vector Machine","45aaea7f":"# Using the test data to make predictions and analyze the accurace of the model","ee77a953":"We're going to need a pipelined model ir order to facilitate the entire process of CountVectorizer (TfidfVectorizer) and svm.LinearSVC model","5cdb335d":"# Split the data into train and test","b481f9c4":"Now, let's look about Lemmatization","a79e7279":"# About the dataset","d607e478":"# Lemmatization","167a832c":"### Count Vectorization","d543ebe3":"# Text Feature Extraction","473377ad":"As said, this dataset doesn't has any stop words","6ed18fb4":"An alternative to CountVectorizer is the TfidVectorizer","7878c7c3":"TfidVectorizer will be used to create the machine learning model for this study","3a132811":"# Checking the number of posts per type","b8de6ce2":"This study was made based on the kaggle dataset https:\/\/www.kaggle.com\/zeyadkhalid\/mbti-personality-types-500-dataset","e3bef111":"# Stop words","78b0dac1":"# Read the dataset into a pandas dataframe","6d6dad9f":"Now it's time to read the dataset and make a simple exploratory analysis","a241dd40":"# Recreate the model?","bb79199f":"# Importing libraries","416ff638":"Posts are preprocessed texts:\n\n    - No punctuations, stopwords, URLs\n    \n    - Lemmatization\n    \n    - Reconstruct samples to be equal-sized chunks (500 words per sample)\n    \nPersonality types are 16 unique values","b6c5984e":"In order to understand lemmatization, first we'll look at the concept of Stemming","066b9907":"TfidVectorizer calculates an inverse frequency for each word","22f2fddc":"### Lemmatization example","5ca7aaae":"To do that, we're going to use the Pipeline feature from sklearn.pipeline\n\nReferences: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html","d598db8f":"### Model Pipeline","8a343f3d":"# Checking null values","581c3a7d":"In constrast with stemming, Lemmatization looks beyond word reduction, and considers a language's full vocabulary to apply a morphological analysis to words.","29e65760":"## TfidVectorizer","f442c080":"    ~106K records of preprocessed posts and their authors' personality types.\n    \n    Posts are equal-sized: 500 words per sample   ","15620f0d":"After preprocess data it's time to extract features from the text in order to prepare the machine learning model","ca111a88":"Now it's time to talk about Text Feature Extraction","441e649a":"# Model","d43d969c":"# End","7097b0d8":"As said before, this dataset also has Lemmatization preprocess feature","e996bf22":"Let's use the library Spacy to see examples of english stop words\n\nSpacy is the Industrial Strength Natural Language Processing: https:\/\/spacy.io\/","90fcee9c":"Let's use a sophisticated stemmer, the SnowballStemmer from NLTK (natural language toolkit)","b97cd68c":"1. Treats each word of a text individually as a feature\n\n2. After that, counts each occurrence of each word in the document\n\n3. Than, makes a matrix DTM (Document Term Matrix)","e3e9a173":"# Content"}}