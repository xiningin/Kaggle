{"cell_type":{"8f04dafe":"code","d92c6bab":"code","8466587e":"code","de97620e":"code","bc809dcc":"code","bbc369ed":"code","29a64b7d":"code","766c3f5b":"code","9a58a569":"code","c9ca452a":"code","c755c2ed":"code","1cfe8baa":"code","36c33a97":"code","8ae074dc":"code","bf9edb5a":"code","67aa3e54":"code","a4c14c39":"code","07b3d877":"code","717806a1":"code","c894223a":"code","e81b0826":"code","f10bee22":"code","c40aaf67":"code","87842ab5":"code","95356957":"code","73b6b6c7":"code","55739cd8":"code","5a2fb441":"code","510e9d64":"code","48011283":"code","7a7a5640":"code","6828516a":"code","6cf52b5a":"code","58185cbd":"code","bd6018e2":"code","50163ea4":"code","ea53b1b9":"code","5ac97009":"code","06cf6141":"code","b25a7834":"code","f3a8a78c":"code","14247224":"code","6976889b":"code","20239efa":"code","f4feb0df":"code","8e5f6565":"code","52beeb20":"code","1ba9687f":"code","60929379":"code","42d9cb2d":"code","144d26bd":"code","b89673c2":"code","ec47d538":"code","25ea9d92":"code","c7416127":"code","19eb50bd":"code","904002e5":"code","60773e3d":"code","70973d98":"code","a17bfeb1":"code","d0fdc9c1":"code","6bd27161":"code","726e5d89":"code","475f3e18":"code","d0d2713b":"code","bdea2cbd":"code","0f27a780":"code","58af5155":"code","f68e8cdc":"code","eeb410d8":"code","60f75fec":"code","99b417be":"code","df52c8bc":"code","7bebbd0e":"code","53c04c22":"code","4b968491":"code","03bf93af":"code","1834bcb7":"code","2469372a":"code","6ef56c49":"code","27f9473b":"code","8bc3bdc6":"code","9bfc2a23":"code","cae2a5f1":"code","6afb3483":"code","227fc1db":"code","540f8d54":"code","fed9cf0d":"code","91a67af2":"code","69272501":"code","a2e0212e":"code","2b8d054a":"code","890ea3b5":"code","63ad2d88":"markdown","01feb471":"markdown","31deed23":"markdown","bebe3fd8":"markdown","1bcf3b41":"markdown","25519149":"markdown","d1cdfc4f":"markdown","f2a2cc06":"markdown","fbea29ee":"markdown","bd71e9c4":"markdown","86c4f4a1":"markdown","3085e2e7":"markdown","6391399b":"markdown","9f3cfde7":"markdown","df09ece5":"markdown","b1c80b47":"markdown","8f046227":"markdown","7bd5e196":"markdown","c3a5d8e7":"markdown","582a3555":"markdown","9d48505f":"markdown","471ca0bc":"markdown","146e69af":"markdown","d3932939":"markdown","c11281d6":"markdown","2bf8a665":"markdown","0fa3e9d9":"markdown","b246dcb4":"markdown","2f630e4f":"markdown","e0b3cec2":"markdown","9f3e0267":"markdown","8b1be070":"markdown","84293cac":"markdown","aa74d717":"markdown","6a0defa0":"markdown","af5d7534":"markdown","7e67776c":"markdown","91a3419d":"markdown","9ef623d6":"markdown","a2640ad2":"markdown","2ac3b656":"markdown","a1a5775c":"markdown","e6da0f5d":"markdown","9f97b1d7":"markdown","611d4bd8":"markdown","08fa65a4":"markdown","176393ee":"markdown","2a1b7fbd":"markdown","b8c59798":"markdown","35033076":"markdown","4c76b4b8":"markdown","6bebb2a2":"markdown","a3df208d":"markdown"},"source":{"8f04dafe":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n% matplotlib inline\n\nfrom sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score,StratifiedKFold,cross_val_predict\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.svm import SVC\n\n# Suppress Warnings:-\nimport warnings\nwarnings.filterwarnings('ignore')","d92c6bab":"# Getting train and test dataset\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ncombine = [train,test]  # we will use this dataset while creating new features or during imputing missing values","8466587e":"print(train.shape)\nprint(test.shape)","de97620e":"train.head()","bc809dcc":"# There are 5 integer variable, 2 float and 5 string variales\nprint(train.dtypes.value_counts())\nprint('***'*20)\ntrain.info() \n","bbc369ed":"# Find out features with missing values\ntrain.isnull().sum()","29a64b7d":"# On the given train dataset we will see how many survived\ntrain['Survived'].value_counts()","766c3f5b":"# Let's plot the above data\nsns.countplot('Survived',data = train)","9a58a569":"train['Sex'].value_counts()","c9ca452a":"sns.countplot('Sex', data = train)","c755c2ed":"# Explore Sex Vs Survived\ntrain[['Sex','Survived']].groupby('Sex').mean()","1cfe8baa":"# Visualize the survival probability on Sex \nsns.catplot(x = 'Sex', y = 'Survived', data = train, kind = 'bar')\nplt.ylabel('Survival Probability')","36c33a97":"sns.countplot(x= 'Sex', hue = 'Survived', data = train)","8ae074dc":"# Get the break up of 891 across 3 classes\ntrain['Pclass'].value_counts()","bf9edb5a":"# plot the data\nsns.countplot('Pclass',data = train)","67aa3e54":"# survival probability \ntrain[['Pclass','Survived']].groupby('Pclass').mean()","a4c14c39":"# plot the data\nsns.catplot('Pclass', y = 'Survived', data = train, kind = 'bar')\nplt.ylabel('Survival possibility')","07b3d877":"sns.countplot(x='Pclass', hue = 'Survived',data = train)","717806a1":"sns.catplot(x='Pclass', y = 'Survived',hue = 'Sex',data = train, kind = 'point')","c894223a":"# Get the count of embarked feature\ntrain['Embarked'].value_counts()","e81b0826":"sns.countplot('Embarked', data = train)","f10bee22":"# Explore Embarked vs Survived\ntrain[['Embarked','Survived']].groupby('Embarked').mean()","c40aaf67":"sns.catplot('Embarked','Survived',data = train, hue = 'Sex', kind = 'point')","87842ab5":"# Age distribution\nprint(\"Oldest Passenger's age:\", train['Age'].max())\nprint(\"Youngest Passenger's age:\", train['Age'].min())\nprint (\"Average Age:\",round(train['Age'].mean(),2))","95356957":"# Visualize Age distribution\ntrain['Age'].plot(kind = 'hist',)\nplt.xlabel('Age')","73b6b6c7":"# Explore 'Age' vs 'Survived'\ng = sns.FacetGrid(train, col = 'Survived')\ng.map(plt.hist, 'Age')","55739cd8":"sns.kdeplot(train[train['Survived']==0]['Age'], shade = True, color = 'red')\nsns.kdeplot(train[train['Survived']==1]['Age'],shade = True, color = 'blue')\nplt.legend(['Not Survived','Survived'])\nplt.title('Age Vs Survived')\nplt.xlabel('Age')","5a2fb441":"# continue exploring the distribution of age with other features.\ng = sns.FacetGrid(train,col = 'Survived', row = 'Sex')\ng.map(plt.hist, 'Age')","510e9d64":"g = sns.FacetGrid(train,col = 'Survived', row = 'Pclass')\ng.map(plt.hist, 'Age')","48011283":"g = sns.FacetGrid(train,col = 'Survived', row = 'Embarked')\ng.map(plt.hist, 'Age')","7a7a5640":"train[['Name','Age']][train['Age']== 80]","6828516a":"train['Fare'].plot(kind = 'hist')","6cf52b5a":"train['Fare'].max()","58185cbd":"train[train['Fare']>500]","bd6018e2":"g = sns.FacetGrid(train,col = 'Survived')\ng.map(plt.hist,'Fare')","50163ea4":"train['Parch'].value_counts()","ea53b1b9":"sns.countplot(x = 'Parch', data = train)","5ac97009":"# Explore Parch Vs Survived\ntrain[['Parch','Survived']].groupby('Parch').mean().sort_values(by = 'Survived',ascending = False)","06cf6141":"sns.catplot('Parch','Survived',data = train, kind = 'point')\nplt.ylabel('Survival Probability')","b25a7834":"train['SibSp'].value_counts()","f3a8a78c":"# Plot the data\nsns.countplot('SibSp',data = train)","14247224":"train[['SibSp','Survived']].groupby('SibSp').mean().sort_values(by = 'Survived',ascending = False)","6976889b":"# plot\nsns.catplot('SibSp','Survived',data = train, kind = 'point')","20239efa":"sns.heatmap(train.corr(),annot = True)","f4feb0df":"# Let's view our training dataset\ntrain.head()","8e5f6565":"# we need to create new feature for both train and test data:\nfor dataset in combine:\n    dataset['Title'] = dataset['Name'].str.extract('([A-Za-z]+)\\.')","52beeb20":"# we will replace some of the titles to the most common titles\nfor dataset in combine:\n    dataset['Title'].replace(['Capt','Col','Don','Jonkheer','Major','Rev','Sir','Countess','Dr'], 'Others',inplace=True)\n    dataset['Title'].replace(['Lady','Miss','Mlle','Mme','Mrs','Ms'],'Miss\/Mrs', inplace = True)","1ba9687f":"# Let's review the titles created.\ntrain['Title'].unique()","60929379":"pd.crosstab(train['Title'],train['Sex'])","42d9cb2d":"# Calculate the average age for the designated title\ntrain[['Title','Age']].groupby('Title').mean()","144d26bd":"# Create a new feature 'Family' which will give us the total number of family members onboard\nfor dataset in combine:\n    dataset['Family']= dataset['SibSp'] + dataset['Parch']+1","b89673c2":"train['Family'].value_counts()","ec47d538":"# Creating another variable which will tell us if the person was travelling alone or not.\nfor dataset in combine:\n    dataset['Alone'] = dataset['Family'].map(lambda x: 1 if x == 1 else 0)","25ea9d92":"# Creating a function to fill in the missing age based on the average age as per title,\n# which we have calculated earlier\ndef impute_age(cols):\n    age = cols[0]\n    title = cols[1]\n    if pd.isnull(age):\n        if title == 'Mr':\n            return 32\n        elif title == 'Master':\n            return 5\n        elif title == 'Miss\/Mrs':\n            return 28\n        else:\n            return 45\n    else:\n        return age\n        ","c7416127":"# Calling the above function to fill up missing age for both train and test set\nfor dataset in combine:\n    dataset['Age']= dataset[['Age','Title']].apply(impute_age, axis = 1)","19eb50bd":"# Validating if the missing age has been updated.\nprint(train.isnull().sum())\nprint('*'*40)\nprint(test.isnull().sum())","904002e5":"#  Embarked\n# fill up the missing data with S port, as the most of the passenger boarded from S port. \ntrain['Embarked'].fillna('S', inplace = True)","60773e3d":"# Fare\ntest[test['Fare'].isnull()]","70973d98":"test['Fare'].fillna(test['Fare'].mode()[0], inplace = True)","a17bfeb1":"test.isnull().sum()","d0fdc9c1":"# Explore title vs Survived\ntrain[['Title', 'Survived']].groupby('Title').mean().sort_values(by = 'Survived',ascending = False)","6bd27161":"sns.catplot('Title',y = 'Survived',data = train, kind = 'bar')","726e5d89":"train[['Family','Survived']].groupby('Family').mean().sort_values(by = 'Survived', ascending = False)","475f3e18":"sns.catplot(x= 'Family',y='Survived', data = train,kind = 'bar')","d0d2713b":"# Lets look at the dataset with added features\ntrain.head()","bdea2cbd":"train.drop(['PassengerId','Name','Ticket','Cabin'],axis = 1, inplace = True)\n","0f27a780":"PassengerId = test['PassengerId']\ntest.drop(['PassengerId','Name','Ticket','Cabin'],axis = 1, inplace = True)","58af5155":"print(train.head())\nprint(test.head())","f68e8cdc":"cols = ['Pclass','Sex','Embarked','Title']\ntrain = pd.get_dummies(train,columns=cols,drop_first= True)\ntest = pd.get_dummies(test,columns=cols,drop_first= True)","eeb410d8":"# Age and Fare features are continous variable so, let's create bins for these two variables.\n# For train set\ntrain['Age_bin'] = pd.cut(train['Age'].astype(int),5)\ntrain['Fare_bin'] = pd.qcut(train['Fare'],4)\n\n# For test set:\ntest['Age_bin'] = pd.cut(test['Age'].astype(int),5)\ntest['Fare_bin'] = pd.qcut(test['Fare'],4)\n   ","60f75fec":"train['Age_bin'].value_counts()","99b417be":"train['Fare_bin'].value_counts()","df52c8bc":"# Create 'Age_band' feature based on the bins\ndef age_band(dataset):\n    dataset['Age_band']=0\n    dataset.loc[dataset['Age']<=16,'Age_band']=0\n    dataset.loc[(dataset['Age']>16) & (dataset['Age']<=32),'Age_band']=1\n    dataset.loc[(dataset['Age']>32) & (dataset['Age']<=48),'Age_band']=2\n    dataset.loc[(dataset['Age']>48) & (dataset['Age']<64),'Age_band'] = 3\n    dataset.loc[dataset['Age']>64,'Age_band']=4","7bebbd0e":"# Calling the function on both train and test set\nage_band(train)\nage_band(test)","53c04c22":"# Create 'Fare_cat' feature based on the bins\ndef fare_cat(dataset):\n    dataset['Fare_cat']=0\n    dataset.loc[dataset['Fare']<= 7.91, 'Fare_cat']=0\n    dataset.loc[(dataset['Fare']>7.91) & (dataset['Fare']<=14.45),'Fare_cat']=1\n    dataset.loc[(dataset['Fare']>14.45) & (dataset['Fare']<=31),'Fare_cat']=2\n    dataset.loc[(dataset['Fare']>31) & (dataset['Fare']<=513),'Fare_cat'] = 3\n   ","4b968491":"# Calling the function on both train and test set\nfare_cat(train)\nfare_cat(test)","03bf93af":"train.head()","1834bcb7":"train.columns","2469372a":"# Dropping features\ndrop_features = ['Age','Fare','Age_bin','Fare_bin']\ntrain.drop(columns = drop_features,axis = 1, inplace = True)\ntest.drop(columns = drop_features,axis = 1, inplace = True)","6ef56c49":"train.head()","27f9473b":"# Create our feature matrix X and response vector \u00fd\nX = train.drop('Survived',axis = 1)\ny = train['Survived']","8bc3bdc6":"kfold = StratifiedKFold(n_splits= 10)","9bfc2a23":"classifiers = [LogisticRegression(),KNeighborsClassifier()\n               ,DecisionTreeClassifier(),RandomForestClassifier(n_estimators=100)\n               ,GradientBoostingClassifier(),SVC()]","cae2a5f1":"clf_scores=[]\nfor classifier in classifiers:\n    scores = cross_val_score(classifier,X,y,cv = kfold,scoring = 'accuracy')\n    clf_scores.append(scores)\n\ncv_mean =[]\ncv_std = []\nfor score in clf_scores:\n    cv_mean.append(score.mean())\n    cv_std.append(score.std())","6afb3483":"model_names = ['Logistic Regression','KNN','Decision Tree','Random Forest','Gradient Boosting','SVC']\nModel_evaluation_scores = pd.DataFrame({'Algorithms': model_names,'Mean Score':cv_mean,'Std':cv_std})\nModel_evaluation_scores","227fc1db":"# Visualize the above scores:-\nsns.barplot(x = 'Mean Score',y= 'Algorithms',data = Model_evaluation_scores,palette= 'Set1',)\nplt.title('Cross validation scores')","540f8d54":"rfc = RandomForestClassifier()\nn_estimators = range(100,500,100)\nparam_grid = {'n_estimators':n_estimators}\ngrid = GridSearchCV(rfc,param_grid,scoring = 'accuracy',cv =10,verbose=1)\ngrid.fit(X,y)\nprint(grid.best_score_)\nprint(grid.best_params_)","fed9cf0d":"gbc = GradientBoostingClassifier()\nparam_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\ngrid = GridSearchCV(gbc,param_grid,cv = 10,scoring = 'accuracy',verbose = 1)\ngrid.fit(X,y)\nprint(grid.best_score_)\nprint(grid.best_params_)\n","91a67af2":"svc = SVC(probability= True)\nparam_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\ngrid = GridSearchCV(svc,param_grid,cv=10,scoring = 'accuracy',verbose = 1)\ngrid.fit(X,y)\nprint(grid.best_score_)\nprint(grid.best_params_)","69272501":"# SVC classification report\nsvc = SVC(kernel='rbf',C= 50,gamma=0.01)\npred_response = cross_val_predict(svc,X,y,cv=10)\nprint(confusion_matrix(y,pred_response))","a2e0212e":"# for some reason the 'Title_Master' still shows up under test set, which should have been dropped during dummy variable creation.\ntest = test.drop('Title_Master',axis = 1)\ntest.head()","2b8d054a":"# Let's predict the response for our test data set\nsvc = SVC(kernel='rbf',C= 50,gamma=0.01)\nsvc.fit(X,y)\npredict_survival = svc.predict(test)\npred = pd.Series(predict_survival,name = 'Survived')","890ea3b5":"result = pd.concat([PassengerId,pred],axis = 1)\n# submission.head()\nresult.to_csv('Titanic_survival_prediction.csv',index = False)","63ad2d88":" *** Note : - Maximum number of the passengers boarded from Southhampton port and it had the lowest number of survived people. C Port passengers had higher survival rate and in that female passengers had a higher chance.","01feb471":" **Creating new features for both train and test set** ","31deed23":"*Based on the above cross validated scores, I will perform hyper parameter tuning on Random Forest, Gradient Boosting and SVC****","bebe3fd8":"Note : - Travelling with Sibling or Spouse had good chance for survival. ","1bcf3b41":"## 3. Analyze and Visualize the data","25519149":"**4. Embarked**","d1cdfc4f":"**Confusion Matrix for the best model **\n  - SVC is giving highest score of 83.16% among other algorithms. We will have a look at the confusion matrix, to see what was the underlying predicted values. \n        ","f2a2cc06":"## 5. Dropping Features","fbea29ee":"Note : - Passenger accompanied by 3 other family members stand a higher chance in survival rate... but as the family size increases the survival probability gets worse.\nAlso, a person travelling alone doesn't seem to do any good.","bd71e9c4":"**SVC**","86c4f4a1":"## 1. Importing the required libraries**","3085e2e7":"Note: There are more Male passengers onboard than Female","6391399b":"** Vusualize our new features  \"Family\" and \"Title\" **","9f3cfde7":"**a. Exploratory Data Analysis on Categorical Features**","df09ece5":"**1. Survived Feature**","b1c80b47":"**1.  Age**","8f046227":"** Note : 342 people survived out of 891 which is 38% on the training dataset","7bd5e196":"## 7. Modeling - Evaluate Model Performance and Finalize it\n   We will select the best model in Scikit learn using kfold crossvalidation evaluation process\n   \n   ***  We  will use GridSearchCV for hyperparameter optimization.\n\n","c3a5d8e7":"** Note:- Most of the passenger boarded from S - Southampton port","582a3555":"**We will finalize our features before we go to modeling**","9d48505f":"**Dropping Features**\n            We are dropping some of the features from our training dataset, which doesn't contribute to our response variable (\"Survival\") or highly incomplete.\n   Features to exclude are :-\n        * PassengerId\n        * Name\n        * Ticket\n        * Cabin\n        ","471ca0bc":"** Finalize our Model**\n\n**So, overall the SVC seems to be doing a better job with a score of 83.16 and it's optimal parameters are -  C:50, gamma: 0.01, kernel:rbf\n\n**For Gradient Boosting the best score is 82.15 and it's optimal parameters are - learning_rate': 0.1, 'loss': 'deviance', 'max_depth': 4, 'max_features': 0.3, 'min_samples_leaf': 100, 'n_estimators': 300\n\n**Last but not the least, Random forest, the best score is 82.04 and it optimal parameter, n_estimators = 100","146e69af":"This is just the initial approach... I understand there are mutiple ways to get the predictions better. I will continue to work on it.  \nIn the meantime, Please do upvote, if you like the notebook and I look forward to some feedback to perform better. \nThank you :-)","d3932939":"**4. SibSp**","c11281d6":"** Note :- Most of the  passengers belongs to Pclass -3","2bf8a665":" **We will use GridSearchCV to perform parameter tuning**","0fa3e9d9":"**c. Let's visualize correlation between the features**\n","b246dcb4":"**3. Parch ( Parents and Children)**","2f630e4f":"** Note :- \n                - Most of the children below the age of 5 were saved\n                - Oldest passenger (80 yrs old) survived. \n                - Large number of 15 - 25 years old didn't survive.\n                - Most of the Female between 15 to 35 survived.  ","e0b3cec2":"## 6. Converting Features\n\nWe need to transform our categorical features into dummy variables.\nBelow are the features...\n1. Pclass\n2. Sex\n3. Title\n4. Embarked","9f3e0267":"## 2. Load and Check Data","8b1be070":"Note:- Most of the passengers are from Pclass 3 and their survival possibility is the lowest.\n   The upper class passengers (Pclass = 1) were most likely to have survived.\n","84293cac":"** Dataframe to evaluate the model performance.**","aa74d717":"## 8. Prediction on test data set\n**We will predict on the test set using SVC algorithm with optimal parameters, since it gave the highest score during model evaluation**","6a0defa0":"**Overall observations on numerical features:- **\n1. Age:-\n                -Most passengers are in 15 to 35 age distribution\n                -Most of the children below 5 years were saved\n                - Large number of 15-25 years didn't survive\n                - Oldest passenger was survived\n            \n 2. Fare - Passengers who have paid higher fare has survived.\n 3. Parch & SipSp - Most of the passengers with family size (>3) had very low survival rate or almost null.","af5d7534":"*** This is my first step into data science with Machine Learning. I will try my best to give a clear and detailed information in this notebook. I am here to learn and build my knowledge as I go along. Creating this Kernel came with lots of efforts.\n\nI would like to thank the other kagglers (experts) who have shared their work in this platfrom and through them I have learnt new methods and concepts. \n\nI have collected some inputs from other sources to help me in putting up this simple workflow which we will see it below.\n*\n\n**Contents of my notebook:**\n1. Importing required libraries\n2. Load and Check data\n3. Analyze and Visualize the data\n4. Creating new features & fill out missing values\n5. Dropping features\n6.  Converting features\n7. Modeling - Evaluate Model Performance and Finalize\n8. Prediction on test data\n9. Submission","7e67776c":"** 2. Sex Feature **","91a3419d":"**b. Exploratory Data Analysis on Numerical features**","9ef623d6":"## 4. Creating new features & fill out Missing values","a2640ad2":" ** Note : -  Most of the Male passengers didn't survive however most of the Female passengers survived the disaster.\n                    \n  Female have got 74% survival chances as compare to 18% survival for Men.","2ac3b656":"**2. Fare**","a1a5775c":"**Overall Observations from Categorical features :-**\n1.  Sex\n                -There were more male passenger on the ship\n                -Female had a higher surival possibility\n2. Pclass\n                     - Most of the passengers onboard were from Pclass = 3\n                     - The upper class passengers (Pclass = 1)  were most likely to have survived with higher survival rate.\n                     - Female had a higher chance of survival\n3. Embarked - Most of the people were from S port and survival was lowest in this port\n                 -  Port C passenger had high survival rate\n                 - Female stands a higher chance","e6da0f5d":"Note:- We can see that there is not much correlation among features. The highest correlation can be seen between Parch and SibSp of 0.41\nSo, we will retain all the features as of now. ","9f97b1d7":"**Observations :- We see that that are 3 features with missing values...those are :- **\n1.  Age\n2. Cabin\n3. Embarked\n\n*** Based on the analysis which we will do below, we will decide if we need to keep the features and complete it or drop it.","611d4bd8":"## 9 . Submission","08fa65a4":"**3. Pclass** ","176393ee":"**** Segregate Categorical and Numerical Features****\n*             Categorical features are :- \n            1. Survived\n            2. Sex\n            3. Pclass\n            4. Embarked\n*           Numerical features are :- \n            1. Age\n            2. Fare\n            3. Parch\n            4. SibSp","2a1b7fbd":"       -Fares <= 50 dollars, had the highest frequency\n            -Fares varied significantly with few passengers paying as high as $512\n             - Passengers who had paid higher fare have survived.","b8c59798":"**Gradient Boosting**","35033076":"**Random Forest**","4c76b4b8":"**We know that , there are missing values for Age, Cabin and Embarked**\n\nAlso, we know from the previous analysis that 'Age' is a good predictor.\nand so is the Embarked feature. Hence, we will complete these features.\n\nCabin has lots of missing values - 77% of the data are missing.Hence we will drop the feature.","6bebb2a2":"**Filling up missing values for both train and test set**","a3df208d":"**Few algorithms to train our model**\n        * LogisticRegression\n        * KNeighborClassifier\n        * DecisionTreeClassifier\n        * RandomForestClassifier\n        * Gradient Boosting\n        * SVC\n        *** We will evaluate the model performance using classification accuracy across all models, using kfold cross validation"}}