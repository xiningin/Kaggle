{"cell_type":{"46a447fc":"code","e078d0f3":"code","380f4d20":"code","a67b32e2":"code","7e6a3d2c":"code","551de2eb":"code","384b37d2":"code","387093c3":"code","35abf283":"code","6e6e150e":"code","7ea44326":"code","80e0ae59":"code","35c3d233":"code","2fae96fd":"code","a56c83d1":"code","04db3bfe":"code","b84d44a3":"code","e6a5fc03":"code","a47f6b0b":"code","ffabe51e":"code","ae6db2c6":"code","a7e8e3e8":"code","2cf385b1":"code","36a2ec75":"code","97f098d7":"code","8ad0ec4f":"code","d24322fb":"code","3f89638a":"code","6fa36dfb":"code","d45febba":"code","26ec26e3":"code","2502b0aa":"code","263c7a47":"code","1f3bf145":"code","246c13c6":"code","603dfc0d":"code","4479bec7":"code","4c3029fd":"code","2891b920":"code","f683743b":"code","fe4e3926":"code","23288bec":"code","be5910c8":"code","88eb896c":"code","02ee78eb":"code","26dca0dd":"code","69e965ac":"code","731b8fab":"code","e105ac22":"code","656ada4e":"code","3acf1ea0":"code","c262a950":"code","e87a98e6":"code","da8e204e":"code","3e5def46":"code","d0d44f23":"code","a0cd0013":"code","fb17e43c":"code","bd59fe86":"code","67fea37c":"code","99047d09":"code","fc642049":"code","0abd1f28":"code","94ef9884":"code","c5abb8aa":"code","b2fd20d0":"code","722deba9":"code","b5bf14c5":"code","4faa23c3":"code","92dce760":"markdown","f613827b":"markdown","e795fb99":"markdown","517e9064":"markdown","c99c9fa0":"markdown","2a8bf49a":"markdown","a1542f13":"markdown","aa1e56cb":"markdown","2afcef73":"markdown","06cbbce7":"markdown","ba9ecc4e":"markdown","f000e92e":"markdown","35569d52":"markdown","81f9c132":"markdown","a8ad95ba":"markdown","eba2fddc":"markdown","4e469bc1":"markdown","eb4c6eff":"markdown","42185a4e":"markdown","f93744ae":"markdown","8c1492bb":"markdown","60b03c27":"markdown","3b6fc610":"markdown","9a06557d":"markdown"},"source":{"46a447fc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e078d0f3":"import random\nimport warnings\nimport gc\nimport tensorflow as tf\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\n\nfrom xgboost import XGBClassifier\n\nwarnings.filterwarnings(\"ignore\")","380f4d20":"seed = 47","a67b32e2":"def evaluate_model(model, x, y):\n    y_pred_prob = model.predict(x)\n    acc = accuracy_score(y, y_pred_prob)\n    return {'accuracy' : acc}","7e6a3d2c":"def get_xgboost_model(params=None):\n    if params is None:\n        params = {'colsample_bytree': 0.1,\n                  'eta': 0.12,\n                  'gamma': 5, \n                  'max_depth': 2,\n                  'min_child_weight': 9,\n                  'n_estimators': 1000, \n                  'subsample': 0.9}          \n\n    return XGBClassifier(**params,\n                         objective='multi:softmax',\n                         random_state=seed, \n                         tree_method='gpu_hist', \n                         predictor='gpu_predictor',\n                         early_stopping_rounds=200,\n                         verbosity=0)","551de2eb":"def get_nn_model(n_layers=None, n_units=32, activation='swish'):\n    model = tf.keras.Sequential()\n    \n    if n_layers is not None and n_layers > 0:\n        for _ in range(n_layers):\n            model.add(tf.keras.layers.Dense(units=n_units, activation=activation))\n    model.add(tf.keras.layers.Dense(units=7, activation='softmax'))\n    model.compile(optimizer=tf.keras.optimizers.Adam(),\n                  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n                  metrics=[tf.metrics.SparseCategoricalAccuracy()])\n\n    return model","384b37d2":"def get_pipelines(model):\n    pipelines = list()\n    # normalize\n    p = Pipeline([('s',MinMaxScaler()), ('m',model)])\n    pipelines.append(('norm', p))\n    # standardize\n    p = Pipeline([('s',StandardScaler()), ('m',model)])\n    pipelines.append(('std', p))\n    # quantile\n    p = Pipeline([('s',QuantileTransformer(n_quantiles=100, output_distribution='normal')), ('m',model)])\n    pipelines.append(('quan', p))\n    # pca\n    p = Pipeline([('s',PCA()), ('m',model)])\n    pipelines.append(('pca', p))\n    # svd\n    p = Pipeline([('s',TruncatedSVD()), ('m',model)])\n    pipelines.append(('svd', p))\n    \n    p = Pipeline([('s',StandardScaler()), ('p', PowerTransformer()), ('m',model)])\n    pipelines.append(('std-power', p))\n    # scale and power\n    p = Pipeline([('s',MinMaxScaler()), ('p', PowerTransformer()), ('m',model)])\n    pipelines.append(('min-max-power', p))\n    \n    p = Pipeline([('p', PowerTransformer()), ('m',model)])\n    pipelines.append(('power', p))\n    \n    return pipelines","387093c3":"def score_model(x, y, model):\n    # define the cross-validation procedure\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=seed)\n    # evaluate model\n    scores = cross_val_score(model, x, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    return scores","35abf283":"geomean = lambda x, axis : np.exp(np.mean(np.log(x), axis=axis))\nharmonic_mean = lambda x, axis : len(x) \/ np.sum(1.0\/x, axis=axis) \n\nfuncs = {'mean' : np.mean, \n         'std' : np.std, \n         'var' : np.var, \n         'geo_mean' : geomean, \n         'harmonic_mean' : harmonic_mean, \n         'median' : np.median,\n         'None_feature_engineering' : None}","6e6e150e":"# train_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/train.csv', sep=',')\nrandom.seed(seed)\nn = 4000000\ns = 400000\nskip = sorted(random.sample(range(1, n),n-s))\n\ntrain_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/train.csv', sep=',', skiprows=skip)","7ea44326":"x_train = train_df.drop(['Id', 'Soil_Type7','Soil_Type15', 'Cover_Type'], axis=1).values\ny_train = train_df['Cover_Type'].values \nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=seed, shuffle=True)","80e0ae59":"params = {'colsample_bytree': 0.1,\n          'eta': 0.12,\n          'gamma': 5, \n          'max_depth': 2,\n          'min_child_weight': 9,\n          'n_estimators': 1000, \n          'subsample': 0.9}          \n\nmodel = XGBClassifier(**params, \n                      objective='multi:softmax',\n                      random_state=seed, \n                      tree_method='gpu_hist', \n                      predictor='gpu_predictor',\n                      early_stopping_rounds=200,\n                      verbosity=0)\n\nmodel.fit(x_train, y_train)\nresults = evaluate_model(model, x_test, y_test)\nprint(results)","35c3d233":"results, names = list(), list()\n\nfor key in funcs.keys():\n    x_train = train_df.drop(['Id', 'Soil_Type7','Soil_Type15', 'Cover_Type'], axis=1)\n    if funcs[key] is not None:\n        x_train[key] = funcs[key](x_train, axis=1)\n    y_train = train_df['Cover_Type']\n    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size = 0.2, random_state = seed, shuffle=True)\n    model = get_xgboost_model()\n    model.fit(x_train, y_train)\n    result = evaluate_model(model, x_test, y_test)\n    names.append(key)\n    results.append(result['accuracy'])\n    \nfor name, score in zip(names, results):\n    print('>%s: %f' % (name, score))\n\nindex = np.argmax(results)\nprint(\"Best Result: \", names[index], results[index])","2fae96fd":"x_train = train_df.drop(['Id', 'Soil_Type7','Soil_Type15', 'Cover_Type'], axis=1)\ny_train = train_df['Cover_Type']\nx_train['mean'] = np.mean(x_train, axis=1)\nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=seed, shuffle=True)","a56c83d1":"def get_models_n_estimators():\n    models = dict()\n    trees = [10, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n    for n in trees:\n        params = {'n_estimators' : n}\n        models[str(n)] = get_xgboost_model(params)\n    return models","04db3bfe":"models = get_models_n_estimators()\nresults, names = list(), list()\n\nfor i, (name, model) in enumerate(models.items()):\n    model.fit(x_train, y_train, verbose=True)\n    score = evaluate_model(model, x_test, y_test)\n    results.append(score['accuracy'])\n    names.append(name)\n    print(name, 'accuracy: %.3f' % (results[i]))\n\nindex = np.argmax(results)\nn_estimators = int(names[index])\nprint(\"Best number of estimators\", n_estimators)","b84d44a3":"def get_models_n_depths():\n    models = dict()\n    for depth in range(1,20):\n        params = {'n_estimators' : n_estimators, 'max_depth' : depth}\n        models[str(depth)] = get_xgboost_model(params)\n    return models","e6a5fc03":"models = get_models_n_depths()\nresults, names = list(), list()\n\nfor i, (name, model) in enumerate(models.items()):\n    model.fit(x_train, y_train, verbose=True)\n    score = evaluate_model(model, x_test, y_test)\n    results.append(score['accuracy'])\n    names.append(name)\n    print(name, 'accuracy: %.3f' % (results[i]))\n\nindex = np.argmax(results)\nmax_depth = int(names[index])\nprint(\"Best max depth\", max_depth)","a47f6b0b":"def get_models_subsamples():\n    models = dict()\n    for subsample in np.arange(0.1, 1.1, 0.1):\n        params = {'n_estimators' : n_estimators, 'max_depth' : max_depth, 'subsample' : subsample}\n        key = '%.1f' % subsample\n        models[key] = get_xgboost_model(params)\n    return models","ffabe51e":"models = get_models_subsamples()\nresults, names = list(), list()\n\nfor i, (name, model) in enumerate(models.items()):\n    model.fit(x_train, y_train, verbose=True)\n    score = evaluate_model(model, x_test, y_test)\n    results.append(score['accuracy'])\n    names.append(name)\n    print(name, 'accuracy: %.3f' % (results[i]))\n\nindex = np.argmax(results)\nsubsample = float(names[index])\nprint(\"Best subsample\", subsample)","ae6db2c6":"def get_models_lr():\n    models = dict()\n    rates = [0.0001, 0.001, 0.003, 0.005, 0.01, 0.03, 0.05, 0.1, 0.12, 0.13, 0.3, 0.5, 1.0]\n    for r in rates:\n        params = {'n_estimators' : n_estimators, 'max_depth' : max_depth, 'subsample' : subsample, 'eta' : r}\n        key = '%.4f' % r\n        models[key] = get_xgboost_model(params)\n    return models","a7e8e3e8":"models = get_models_lr()\nresults, names = list(), list()\n\nfor i, (name, model) in enumerate(models.items()):\n    model.fit(x_train, y_train, verbose=True)\n    score = evaluate_model(model, x_test, y_test)\n    results.append(score['accuracy'])\n    names.append(name)\n    print(name, 'accuracy: %.3f' % (results[i]))\n\nindex = np.argmax(results)\neta = float(names[index])\nprint(\"Best learning rate\", eta)","2cf385b1":"def get_models_nfeatures():\n    models = dict()\n    for i in np.arange(0.1, 1.1, 0.1):\n        params = {'n_estimators' : n_estimators, 'max_depth' : max_depth, 'subsample' : subsample, 'eta' : eta, 'colsample_bytree' : i}\n        key = '%.1f' % i\n        models[key] = get_xgboost_model(params)\n    return models","36a2ec75":"models = get_models_nfeatures()\nresults, names = list(), list()\n\nfor i, (name, model) in enumerate(models.items()):\n    model.fit(x_train, y_train, verbose=True)\n    score = evaluate_model(model, x_test, y_test)\n    results.append(score['accuracy'])\n    names.append(name)\n    print(name, 'accuracy: %.3f' % (results[i]))\n\nindex = np.argmax(results)\ncolsample_bytree = float(names[index])\nprint(\"Best colsample_bytree\", colsample_bytree)","97f098d7":"def get_models_n_gamma():\n    models = dict()\n    # for gamma in range(1,20):\n    for gamma in np.arange(0.0, 1.1, 0.1):\n        params = {'n_estimators' : n_estimators, \n                  'max_depth' : max_depth,\n                  'subsample' : subsample,\n                  'eta' : eta, \n                  'colsample_bytree' : colsample_bytree,\n                  'gamma' : gamma}\n        models[str(gamma)] = get_xgboost_model(params)\n    return models","8ad0ec4f":"models = get_models_n_gamma()\nresults, names = list(), list()\n\nfor i, (name, model) in enumerate(models.items()):\n    model.fit(x_train, y_train, verbose=True)\n    score = evaluate_model(model, x_test, y_test)\n    results.append(score['accuracy'])\n    names.append(name)\n    print(name, 'accuracy: %.3f' % (results[i]))\n\nindex = np.argmax(results)\ngamma = float(names[index])\nprint(\"Best gamma\", gamma)","d24322fb":"def get_models_n_min_child_weight():\n    models = dict()\n    for min_child_weight in range(1,20):\n        params = {'n_estimators' : n_estimators, \n                  'max_depth' : max_depth,\n                  'subsample' : subsample,\n                  'eta' : eta, \n                  'colsample_bytree' : colsample_bytree,\n                  'gamma' : gamma,\n                  'min_child_weight' : min_child_weight}\n        models[str(min_child_weight)] = get_xgboost_model(params)\n    return models","3f89638a":"models = get_models_n_min_child_weight()\nresults, names = list(), list()\n\nfor i, (name, model) in enumerate(models.items()):\n    model.fit(x_train, y_train, verbose=True)\n    score = evaluate_model(model, x_test, y_test)\n    results.append(score['accuracy'])\n    names.append(name)\n    print(name, 'accuracy: %.3f' % (results[i]))\n\nindex = np.argmax(results)\nmin_child_weight = int(names[index])\nprint(\"Best min_child_weight\", min_child_weight)","6fa36dfb":"def get_models_n_reg_alpha():\n    models = dict()\n    for reg_alpha in [0, 1e-5, 1e-2, 0.1, 0.01, 0.001, 0.003, 1, 10, 100]:\n        params = {'n_estimators' : n_estimators, \n                  'max_depth' : max_depth,\n                  'subsample' : subsample,\n                  'eta' : eta, \n                  'colsample_bytree' : colsample_bytree,\n                  'gamma' : gamma,\n                  'min_child_weight' : min_child_weight,\n                  'reg_alpha': reg_alpha}\n        models[str(reg_alpha)] = get_xgboost_model(params)\n    return models","d45febba":"models = get_models_n_min_child_weight()\nresults, names = list(), list()\n\nfor i, (name, model) in enumerate(models.items()):\n    model.fit(x_train, y_train, verbose=True)\n    score = evaluate_model(model, x_test, y_test)\n    results.append(score['accuracy'])\n    names.append(name)\n    print(name, 'accuracy: %.3f' % (results[i]))\n\nindex = np.argmax(results)\nreg_alpha = int(names[index])\nprint(\"reg_alpha\", reg_alpha)","26ec26e3":"params = {'n_estimators' : n_estimators,\n          'max_depth' : max_depth,\n          'subsample' : subsample,\n          'eta' : eta, \n          'colsample_bytree' : colsample_bytree,\n          'gamma' : gamma,\n          'min_child_weight' : min_child_weight,\n          'reg_alpha' : reg_alpha}\n         \nprint('Best Params: ', params)","2502b0aa":"model = get_xgboost_model(params)\nmodel.fit(x_train, y_train, verbose=True)\nscore = evaluate_model(model, x_test, y_test)\nprint(score)","263c7a47":"params = {'n_estimators' : n_estimators,\n          'max_depth' : max_depth,\n          'subsample' : subsample,\n          'eta' : eta, \n          'colsample_bytree' : colsample_bytree,\n          'gamma' : gamma,\n          'min_child_weight' : min_child_weight,\n          'reg_alpha' : reg_alpha}\nmodel = get_xgboost_model(params)\nmodel.fit(x_train, y_train, verbose=True)\nscore = evaluate_model(model, x_test, y_test)\nprint(score)","1f3bf145":"train_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/train.csv', sep=',')\nx_train = train_df.drop(['Id', 'Soil_Type7','Soil_Type15', 'Cover_Type'], axis=1)\ny_train = train_df['Cover_Type']\nx_train['mean'] = np.mean(x_train, axis=1)\nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=seed, shuffle=True)","246c13c6":"params = {'n_estimators' : n_estimators,\n          'max_depth' : max_depth,\n          'subsample' : subsample,\n          'eta' : eta, \n          'colsample_bytree' : colsample_bytree,\n          'gamma' : gamma,\n          'min_child_weight' : min_child_weight,\n          'reg_alpha' : reg_alpha}\n        \nmodel = get_xgboost_model(params)\nmodel.fit(x_train, y_train, verbose=True)\nscore = evaluate_model(model, x_test, y_test)\nprint(score)","603dfc0d":"del train_df, x_train, y_train, x_test, y_test\ngc.collect()","4479bec7":"test_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/test.csv', sep=',')\nx_test = test_df.drop(['Id', 'Soil_Type7','Soil_Type15'], axis=1)\nx_test['mean'] = np.mean(x_test, axis=1)","4c3029fd":"target = model.predict(x_test).squeeze()\nids = test_df['Id'].values\nsubmission_xgboost = pd.DataFrame({'Id' : ids, 'Cover_Type' : target})","2891b920":"submission_xgboost.head()","f683743b":"submission_xgboost.to_csv('submission_xgboost.csv', index=False) # score 0.95378","fe4e3926":"del test_df, x_test\ngc.collect()","23288bec":"train_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/train.csv', sep=',', skiprows=skip)","be5910c8":"scaler = StandardScaler()\nle = LabelEncoder()\n\nx_train = train_df.drop(['Id', 'Soil_Type7','Soil_Type15', 'Cover_Type'], axis=1)\n\ny_train = train_df['Cover_Type'].values \ny_train = le.fit_transform(y_train)\nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=seed, shuffle=True)\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)","88eb896c":"model = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Dense(units=7, activation='softmax'))\nmodel.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(), \n               loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n                metrics=[tf.metrics.SparseCategoricalAccuracy()])\n\nmodel.fit(x_train, y_train, batch_size=32, epochs=20)\nmodel.evaluate(x_test, y_test)","02ee78eb":"scaler = StandardScaler()\nle = LabelEncoder()\n\nresults, names = list(), list()\n\nfor key in funcs.keys():\n    x_train = train_df.drop(['Id', 'Soil_Type7','Soil_Type15', 'Cover_Type'], axis=1)\n    if funcs[key] is not None:\n        x_train[key] = funcs[key](x_train, axis=1)\n    y_train = train_df['Cover_Type'].values\n    y_train = le.fit_transform(y_train)\n    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size = 0.2, random_state = seed, shuffle=True)\n    x_train = scaler.fit_transform(x_train.values)\n    x_test = scaler.transform(x_test)    \n    model = get_nn_model()\n    model.fit(x_train, y_train, batch_size=32, epochs=15, verbose=0)\n    result = model.evaluate(x_test, y_test, verbose=0)[1]\n    names.append(key)\n    results.append(result)\n    \nfor name, score in zip(names, results):\n    print('>%s: %f' % (name, score))\n\nindex = np.argmax(results)\nprint(\"Best Result: \", names[index], results[index])","26dca0dd":"transformers = {'Min-Max-Scaler': MinMaxScaler(), \n                'Standard-Scaler': StandardScaler(),\n                'QuantileTransformer': QuantileTransformer(n_quantiles=100, output_distribution='normal'),\n                'PCA': PCA(),\n                'TruncatedSVD': TruncatedSVD(),\n                'PowerTransformer': PowerTransformer(),\n                'No-transformer': None}","69e965ac":"results, names = list(), list()\n\nfor key in transformers.keys():\n    x_train = train_df.drop(['Id', 'Soil_Type7','Soil_Type15', 'Cover_Type'], axis=1)\n    y_train = train_df['Cover_Type'].values\n    y_train = le.fit_transform(y_train)\n    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=seed, shuffle=True)\n    \n    if transformers[key] is not None:\n        x_train = transformers[key].fit_transform(x_train.values)\n        x_test = transformers[key].transform(x_test)    \n    \n    model = get_nn_model()\n    model.fit(x_train, y_train, batch_size=32, epochs=15, verbose=0)\n    result = model.evaluate(x_test, y_test, verbose=0)[1]\n    names.append(key)\n    results.append(result)\n    \nfor name, score in zip(names, results):\n    print('>%s: %f' % (name, score))\n\nindex = np.argmax(results)\nprint(\"Best Result: \", names[index], results[index])","731b8fab":"scaler = StandardScaler()\nle = LabelEncoder()\nx_train = train_df.drop(['Id', 'Soil_Type7','Soil_Type15', 'Cover_Type'], axis=1)\ny_train = train_df['Cover_Type'].values \nx_train['std'] = np.std(x_train, axis=1)\ny_train = le.fit_transform(y_train)\nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=seed, shuffle=True)\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)","e105ac22":"def get_models_n_layers():\n    models = dict()\n    for n_layers in [0, 1, 2, 3, 4, 5, 6, 7, 8 ,9, 10]:\n        models[n_layers] = get_nn_model(n_layers=n_layers)\n    return models","656ada4e":"models =  get_models_n_layers()\nresults, names = list(), list()\n\nfor i, (name, model) in enumerate(models.items()):\n    model.fit(x_train, y_train, batch_size=32, epochs=15, verbose=0)\n    result = model.evaluate(x_test, y_test, verbose=0)[1]\n    results.append(result)\n    names.append(name)\n    print(name, 'accuracy: %.3f' % (results[i]))\n\nindex = np.argmax(results)\nn_layers = int(names[index])\nprint(\"Best number of layers\", n_layers)","3acf1ea0":"def get_models_n_units():\n    models = dict()\n    for n_units in [8, 16, 32, 64, 128, 256, 512, 1024, 2048]:\n        models[n_units] = get_nn_model(n_layers=n_layers, n_units=n_units)\n    return models","c262a950":"models =  get_models_n_units()\nresults, names = list(), list()\n\nfor i, (name, model) in enumerate(models.items()):\n    model.fit(x_train, y_train, batch_size=32, epochs=15, verbose=0)\n    result = model.evaluate(x_test, y_test, verbose=0)[1]\n    results.append(result)\n    names.append(name)\n    print(name, 'accuracy: %.3f' % (results[i]))\n\nindex = np.argmax(results)\nn_units = int(names[index])\nprint(\"Best number of units\", n_units)","e87a98e6":"def get_models_n_activations():\n    models = dict()\n    for activation in [\"swish\", \"relu\", \"selu\", \"softplus\", \"elu\"]:\n        models[activation] = get_nn_model(n_layers=n_layers, n_units=n_units, activation=activation)\n    return models","da8e204e":"models = get_models_n_activations()\nresults, names = list(), list()\n\nfor i, (name, model) in enumerate(models.items()):\n    model.fit(x_train, y_train, batch_size=32, epochs=15, verbose=0)\n    result = model.evaluate(x_test, y_test, verbose=0)[1]\n    results.append(result)\n    names.append(name)\n    print(name, 'accuracy: %.3f' % (results[i]))\n\nindex = np.argmax(results)\nactivation = names[index]\nprint(\"Best activation function\", activation)","3e5def46":"results, names = list(), list()\nbatches = [8, 16, 32, 64, 128, 256, 512]\n\nfor i, (name, batch_size) in enumerate(zip(batches, batches)):\n    get_nn_model(n_layers=n_layers, n_units=n_units, activation=activation)\n    model.fit(x_train, y_train, batch_size=batch_size, epochs=15, verbose=0)\n    result = model.evaluate(x_test, y_test, verbose=0)[1]\n    results.append(result)\n    names.append(name)\n    print(name, 'accuracy: %.3f' % (results[i]))\n\nindex = np.argmax(results)\nbatch_size = int(names[index])\nprint(\"Best batch_size\", batch_size)","d0d44f23":"print(\"Best parameters\")\nprint(\"n_layers:\", n_layers)\nprint(\"n_units:\", n_units)\nprint(\"activation:\", activation)\nprint(\"batch_size:\", batch_size)","a0cd0013":"train_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/train.csv', sep=',', skiprows=skip)\nscaler = StandardScaler()\nle = LabelEncoder()\nx_train = train_df.drop(['Id', 'Soil_Type7','Soil_Type15', 'Cover_Type'], axis=1)\ny_train = train_df['Cover_Type'].values \nx_train['std'] = np.std(x_train, axis=1)\ny_train = le.fit_transform(y_train)\nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=seed, shuffle=True)\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)","fb17e43c":"model = get_nn_model(n_layers=n_layers, n_units=n_units, activation=activation)\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=15)\nscore = model.evaluate(x_test, y_test, verbose=0)[1]\nprint(score)","bd59fe86":"del train_df, x_train, y_train, x_test, y_test\ngc.collect()","67fea37c":"test_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/test.csv', sep=',')\nx_test = test_df.drop(['Id', 'Soil_Type7','Soil_Type15'], axis=1)\nx_test['std'] = np.std(x_test, axis=1)\nx_test = scaler.transform(x_test)","99047d09":"preds = model.predict(x_test)\ntarget = np.argmax(preds, axis=-1)\nids = test_df['Id'].values\nsubmission_nn = pd.DataFrame({'Id' : ids, 'Cover_Type' : target + 1})","fc642049":"submission_nn = pd.DataFrame({'Id' : ids, 'Cover_Type' : target + 1})","0abd1f28":"submission_nn.head()","94ef9884":"submission_nn.to_csv('submission_nn.csv', index=False) # score 0.93079","c5abb8aa":"del test_df, x_test\ngc.collect()","b2fd20d0":"df_submission_xgboost = pd.read_csv('submission_xgboost.csv')\ndf_submission_nn = pd.read_csv('submission_nn.csv')\nids = df_submission_xgboost['Id'].values\nsubmission_ensemble = pd.DataFrame({'Id' : ids,\n                           'Cover_Type' : np.array(df_submission_xgboost['Cover_Type'].values + df_submission_nn['Cover_Type'].values)\/\/2})\n","722deba9":"submission_ensemble .head()","b5bf14c5":"submission_ensemble.to_csv('submission_ensemble.csv', index=False) # 0.93155","4faa23c3":"submission_xgboost.to_csv('submission.csv', index=False)","92dce760":"# 4 - Testing different learning rates","f613827b":"# 4 testing different number of batch","e795fb99":"# 5 - Testing different number of features","517e9064":"# 3 - Testing different subsamples\n","c99c9fa0":"# Submission Best Model - XGboost","2a8bf49a":"# 8 - Testing different number of reg_alpha","a1542f13":"# Ensemble XGBoost and Neural Network","aa1e56cb":"# 1 - Testing different number of estimators","2afcef73":"# 2 - Testing different number of units","06cbbce7":"<h3>Feature Engineering XGBoost<\/h3>\n","ba9ecc4e":"# Neural Network - Testing different configurations","f000e92e":"# Neural Network Baseline","35569d52":"# 1 - Testing different number o layers","81f9c132":"# Submission","a8ad95ba":"# XGBoost Baseline","eba2fddc":"<h3>Neural Network Pipelines<\/h3>","4e469bc1":"<h3>Feature Engineering Neural Network Model<\/h3>","eb4c6eff":"# 3 - Testing different activation functions","42185a4e":"# 6 - Testing different number of gamma","f93744ae":"# 7 - Testing different number of min_child_weight","8c1492bb":"# 2 - Testing different max_depth","60b03c27":"# Feature Engineering\n\nHere wee will experiment creating synthetic features using central tendency statistics.","3b6fc610":"# XGBoost - Testing different configurations","9a06557d":"# Submission"}}