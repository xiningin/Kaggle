{"cell_type":{"64989869":"code","a6005e24":"code","a087dead":"code","80208609":"code","76fdcedd":"code","0054d820":"code","e39a3c20":"code","e9323bc9":"code","bcabf951":"code","871a4a2f":"code","6963756e":"code","7b876805":"code","6155505f":"code","c0f72fb6":"code","d0a456f2":"code","3bd9cdff":"code","8ecffde2":"code","f4f7872b":"code","dd43d96d":"code","7fa8a7f6":"code","77472ed5":"code","1877bc80":"code","9ebecb17":"code","4f4c8aec":"code","ec43adbe":"code","fa7505c9":"code","ae2c1c6d":"code","fd700677":"code","81f2bae2":"code","aaa63659":"code","b280b15a":"code","483e917c":"code","e007e32c":"code","8306833f":"code","3af30ee8":"code","044f297e":"code","8a7f93c7":"code","55f50dee":"code","02094aaf":"code","9d831f89":"code","b03ff48f":"code","3a5f0fcc":"code","049d53d9":"code","06f28525":"code","85d06dac":"code","c141cded":"code","a11fb96f":"code","934f85c3":"code","2865db4d":"code","cbade724":"code","478a6d1e":"markdown","43f46b99":"markdown","49efbe87":"markdown","17a92c84":"markdown","ca2f356f":"markdown","f3710253":"markdown","9b815938":"markdown","472f06cd":"markdown","2cc67106":"markdown","fcdbb130":"markdown","19ec895c":"markdown","73c1c79b":"markdown","b1d846c8":"markdown","a84da5f5":"markdown","f2074272":"markdown"},"source":{"64989869":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a6005e24":"X_train_y = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv',header=0)\nX_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv',header=0)\nX_train_y.set_index('Id', inplace=True)\nX_test.set_index('Id', inplace=True)\ny_train = X_train_y['SalePrice']\nX_train= X_train_y.drop('SalePrice', axis=1)","a087dead":"X_train.info()\nprint('DIM X_Train: ', X_train.shape, ' DIM X_test: ', X_test.shape )","80208609":"X = pd.concat([X_train, X_test])\n#X.set_index('Id', inplace=True)","76fdcedd":"#NULL Contain columns:\nX[X.columns[X.isna().any()].tolist()].info()","0054d820":"\nX['MSZoning']=X['MSZoning'].fillna('RL')\nX['LotFrontage']=X['LotFrontage'].fillna(X['LotFrontage'].median())\nX.drop('GarageYrBlt',axis=1, inplace=True) # drop because Null value may be described by GarageFinish or NA, there is no option to write if there is no garaga at all\nX.drop('PoolQC',axis=1, inplace=True) \nX.drop('MiscFeature',axis=1, inplace=True) ","e39a3c20":"\nX['Alley'].fillna('NA', inplace=True) # Fill NaN as No alley access\nX['Utilities'].fillna('AllPub', inplace=True)\nX['Exterior1st'].fillna('VinylSd', inplace=True)  # 1st most common\nX['Exterior2nd'].fillna('MetalSd', inplace=True)  # 2nd most common\nX['MasVnrType'].fillna('None', inplace=True)\nX['MasVnrArea'].fillna(0, inplace=True) # Every missing value got MasVnrType \"None\" so filling with 0\nX['BsmtQual'].fillna('TA', inplace=True)\nX['BsmtCond'].fillna('TA', inplace=True)\nX['BsmtExposure'].fillna('No', inplace=True)\nX['BsmtFinType1'].fillna('Unf', inplace=True)\nX['BsmtFinType2'].fillna('Unf', inplace=True)\nX['BsmtFinSF1'].fillna(0, inplace=True)\nX['BsmtFinSF2'].fillna(0, inplace=True)\nX['BsmtUnfSF'].fillna(0, inplace=True)\nX['TotalBsmtSF'].fillna(0, inplace=True)\nX['Electrical'].fillna('SBrkr', inplace=True)\nX['BsmtFullBath'].fillna(0, inplace=True)\nX['BsmtHalfBath'].fillna(0, inplace=True)\nX['KitchenQual'].fillna('TA', inplace=True)\nX['Functional'].fillna('Typ', inplace=True)\nX['FireplaceQu'].fillna('NA', inplace=True)\nX['GarageType'].fillna('NA', inplace=True)\nX['GarageFinish'].fillna('NA', inplace=True)\nX['GarageCars'].fillna(2, inplace=True)\nX['GarageArea'].fillna(X[X['GarageCars']==2]['GarageArea'].mean(), inplace=True) # fill with the mean of garages with 2 cars\nX['GarageQual'].fillna('NA', inplace=True)\nX['GarageCond'].fillna('NA', inplace=True)\nX['Fence'].fillna('NA', inplace=True)\nX['SaleType'].fillna('WD', inplace=True)","e9323bc9":"col_to_drop=[]\nfor col in X.select_dtypes(include='object').columns:\n    if X[col].value_counts()[0]\/X[col].shape[0] > 0.85:\n        print('Feature: ',col,' has no valuable information because there is one value dominated over dataset with', 100*X[col].value_counts()[0]\/X[col].shape[0],'% of all')\n        col_to_drop.append(col)\nX.drop(col_to_drop,axis=1,inplace=True)     ","bcabf951":"X['MSSubClass']=X['MSSubClass'].astype('category')\nX['OverallQual']=X['OverallQual'].astype('category')\nX['OverallCond']=X['OverallCond'].astype('category')\nX['YearBuilt']=X['YearBuilt'].astype('category')\nX['YearRemodAdd']=X['YearRemodAdd'].astype('category')\nX['BsmtFullBath']=X['BsmtFullBath'].astype('category')\nX['BsmtHalfBath']=X['BsmtHalfBath'].astype('category')\nX['FullBath']=X['FullBath'].astype('category')\nX['HalfBath']=X['HalfBath'].astype('category')\nX['BedroomAbvGr']=X['BedroomAbvGr'].astype('category')\nX['KitchenAbvGr']=X['KitchenAbvGr'].astype('category')\nX['TotRmsAbvGrd']=X['TotRmsAbvGrd'].astype('category')\nX['Fireplaces']=X['Fireplaces'].astype('category')\nX['GarageCars']=X['GarageCars'].astype('category')\nX['MoSold']=X['MoSold'].astype('category')\nX['YrSold']=X['YrSold'].astype('category')","871a4a2f":"col_to_drop=[]\nfor col in X.select_dtypes(include='category').columns:\n    if X[col].value_counts().iloc[0]\/(X[col].shape[0]) > 0.85:\n        print('Feature: ',col,' has no valuable information because there is one value dominated over dataset with', 100*X[col].value_counts().iloc[0]\/(X[col].shape[0]),'% of all')\n        col_to_drop.append(col)\nX.drop(col_to_drop,axis=1,inplace=True)","6963756e":"X[(X.select_dtypes(include='object').columns)]=X[(X.select_dtypes(include='object').columns)].astype('category')","7b876805":"X.info()","6155505f":"plt.figure(figsize=(30,30))\nfor i,col in enumerate(X.select_dtypes(exclude=['category']).columns):\n    plt.subplot(4,5,i+1)\n    if i in [4,9,14,15,16,17,18]:\n        ax=sns.distplot(X[col],kde=False , hist=True, color='red')\n    else:\n        ax=sns.distplot(X[col],kde=False , hist=True)\n","c0f72fb6":"X[['BsmtFinSF2','LowQualFinSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal']].describe(percentiles=[.25, .5, .75, .9])","d0a456f2":"X.drop(['BsmtFinSF2','LowQualFinSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal'],axis=1, inplace=True)","3bd9cdff":"X[['MasVnrArea', 'WoodDeckSF', 'OpenPorchSF','2ndFlrSF', 'BsmtFinSF1']].describe(percentiles=[0.1,.25, .5, .75, .9])","8ecffde2":"X.drop(['MasVnrArea', 'WoodDeckSF', 'OpenPorchSF','2ndFlrSF', 'BsmtFinSF1'],axis=1, inplace=True)","f4f7872b":"X.info()","dd43d96d":"X['SalePrice']= y_train","7fa8a7f6":"test = pd.DataFrame()\ntest['SalePrice']=X['SalePrice']\ntest=test.sort_values(by='SalePrice').reset_index()\n\n","77472ed5":"plt.figure(figsize=(10,10))\nplt.subplot(2,2,1)\nax=sns.distplot(X['SalePrice'],kde=False , hist=True)\nax.set_title('Dist plot of SalePrice')\nplt.subplot(2,2,2)\nax=sns.distplot(np.log(X['SalePrice']),kde=False , hist=True)\nax.set_title('Dist plot of log SalePrice')\n\n#test=sorted(test)\nplt.subplot(2,2,3)\nsns.scatterplot(x=test.index,y=test['SalePrice'], data=test)\nplt.subplot(2,2,4)\nsns.scatterplot(x=test.index,y=np.log(test['SalePrice']), data=test)\n#ax=sns.distplot(X[col],kde=False , hist=True)\n","1877bc80":"for col in X.select_dtypes(include='category').columns:\n    mean_encode=(np.log(X.groupby(col)['SalePrice'].mean())-np.log(X.groupby(col)['SalePrice'].mean().min()))\/(np.log(X.groupby(col)['SalePrice'].mean().max())-np.log(X.groupby(col)['SalePrice'].mean().min()))\n    X[col+'_enc']= X[col].map(mean_encode).astype(float)","9ebecb17":"X.drop(X.select_dtypes(include='category').columns, axis=1, inplace=True)","4f4c8aec":"X_train=X.iloc[:1460,:]","ec43adbe":"X_pred=X.iloc[1460:,:]","fa7505c9":"\n#for col in  X_train.columns:\nplt.figure(figsize=(20,1))\n#data=pd.concat([X_train[col], X_train['SalePrice']], axis=1)\n#sns.boxplot( data = X[(X.select_dtypes(exclude='object').columns)],width=0.8)\nsns.boxplot( x=\"SalePrice\", data=X_train)\n#plt.xscale('log')\nplt.show()","ae2c1c6d":"pd.set_option('display.max_columns', 500)\nX_train[X_train.SalePrice>500000].sort_values(by='SalePrice')","fd700677":"#correlation matrix\ncorrmat = X_train.corr()\nf, ax = plt.subplots(figsize=(35, 35))\nsns.heatmap(corrmat, vmax=1,vmin=0.75, square=True,annot=True,cmap=\"YlGnBu\");","81f2bae2":"X_train.drop(['1stFlrSF', 'Exterior2nd_enc', 'Fireplaces_enc', 'GarageCars_enc', 'TotRmsAbvGrd_enc'], axis=1, inplace=True)\nX_pred.drop(['1stFlrSF', 'Exterior2nd_enc', 'Fireplaces_enc', 'GarageCars_enc', 'TotRmsAbvGrd_enc'], axis=1, inplace=True)","aaa63659":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor","b280b15a":"#steps = [ ('rf', RandomForestRegressor())]#('scaler', StandardScaler()),\nsteps = [('scaler', StandardScaler()), ('sgb', GradientBoostingRegressor())]\npipeline = Pipeline(steps)","483e917c":"#parameters = {'rf__n_estimators': [100, 200, 300, 400, 500],\n#'rf__max_depth': [2,4, 6, 8],\n#'rf__min_samples_leaf': [0.01,0.025, 0.05, 0.1, 0.2],\n#'rf__max_features': ['log2','sqrt']}\nparameters = {'sgb__n_estimators': [1800],\n'sgb__min_samples_split' :[30],\n'sgb__max_depth': [6],\n'sgb__subsample' : [0.8],\n'sgb__learning_rate' : [0.005],\n'sgb__min_samples_leaf': [0.01],\n'sgb__warm_start' : [True],\n'sgb__max_features': [10]}#range(2,30,1)}#['sqrt','log2',None]}","e007e32c":"grid_rf = GridSearchCV(estimator=pipeline,\nparam_grid=parameters,\ncv=5,\nscoring='neg_mean_squared_error',\nverbose=1,\nn_jobs=-1)","8306833f":"X=X_train.drop('SalePrice',axis=1)","3af30ee8":"y=np.log(X_train['SalePrice'])\n","044f297e":"X_train, X_test, y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)","8a7f93c7":"from keras import layers\nfrom keras.layers import Input, Dense, Activation, BatchNormalization\n\nfrom keras.models import Model","55f50dee":"X_train.shape","02094aaf":"def RegrModel(input_shape):\n\n    \n    X_input = Input(input_shape)\n\n    X = Dense(120, name='lay1')(X_input)\n    X = BatchNormalization(axis = -1, name = 'bn1')(X)\n    X = Activation('relu')(X)\n    X = Dense(200, name='lay2')(X)\n    X = BatchNormalization(axis = -1, name = 'bn2')(X)\n    X = Activation('relu')(X)\n    X = Dense(50, name='lay3')(X)\n    X = BatchNormalization(axis = -1, name = 'bn3')(X)\n    X = Activation('relu')(X)\n\n    X = Dense(1, activation='relu', name='output')(X)\n\n\n    model = Model(inputs = X_input, outputs = X, name='RegrModel')\n\n    \n    return model","9d831f89":"regrModel = RegrModel(X_train.shape[1:])","b03ff48f":"regrModel.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=[\"mean_squared_error\"])","3a5f0fcc":"regrModel.fit(x=X_train, y = y_train, epochs = 100, batch_size=64)","049d53d9":"grid_rf.fit(X_train, y_train)\ny_pred = grid_rf.predict(X_test)\nprint(grid_rf.best_params_)\nprint(grid_rf.best_score_)\nrmse_test = MSE(y_test, y_pred)**(1\/2)\nprint('RMSE=',rmse_test)","06f28525":"X_pred.drop('SalePrice',axis=1, inplace=True)","85d06dac":"X_pred=X_pred.fillna(0)","c141cded":"y_pred=regrModel.predict(X_pred)","a11fb96f":"result=pd.DataFrame()\nresult['Id']=X_pred.index","934f85c3":"result['SalePrice']= np.exp(y_pred).reshape(-1,1)","2865db4d":"result.set_index('Id', inplace=True)","cbade724":"result.to_csv('result_rf_keras.csv')","478a6d1e":"All features to drop because of outliers, rest of numerical features with outliers are accepted due its true data and may be significant","43f46b99":"# 1.3 Feature Changing - Numerical to Category\n\nChanging types of features which are numerical to category (Months, Years etc.)****","49efbe87":"Alternative drop - also because of many outliers","17a92c84":"# 1.4 Feature Changing - Object to Category\n\nChanging types of features which are object to category","ca2f356f":"**Tuning with** https:\/\/www.analyticsvidhya.com\/blog\/2016\/02\/complete-guide-parameter-tuning-gradient-boosting-gbm-python\/","f3710253":"# 1.2 Feature Selection - Category\n\nSelecting Features which has variability inside dataset, i am removing all the columns which got one of the values distributed more than 88%","9b815938":"# 1.6 Category Mean Encoding\n\nafter that I will try to get rid or accept the outliers","472f06cd":"# 2.0 Building GradientBoostingRegressor\n\nTutorial for Hyperparameter tuning on parameters section","2cc67106":"# **1.1 Finding NaN values, dealing with NaN's investigating possible Imputing**\n\nInvestigating by value_counts() on object types (which later i will switch to category). Reading Column description to find a clue. For numerical if there are most of data filled i will use either mean or median","fcdbb130":"Alley: Type of alley access to property\n\n       Grvl\tGravel\n       Pave\tPaved\n       NA \tNo alley access\n       \nFilling NaN's as NA because no access also may determine value of the house","19ec895c":"# Logarithm Mean Encoding of Categorical features","73c1c79b":"# Correlation Matrix to figure out which sibling features to drop","b1d846c8":"# 1.5 Feature Selection - Numerical features\n\nFinding Features to drop and reduce dimensions, RED Axes for further investigation","a84da5f5":"# **1.0 Import Train and Test Data**","f2074272":"# **DeSkewing**"}}