{"cell_type":{"34158511":"code","e12c5ce3":"code","fb50b09b":"code","621dc362":"code","6592188a":"code","0febbc38":"code","d6a118ae":"code","7ab0e19f":"code","a042df11":"code","1ecfc93b":"code","5d3d598d":"code","2369963c":"code","8d76560a":"code","8501bb52":"code","b5bda1a8":"code","d34c6471":"code","d8aea7b9":"code","bfa1da78":"code","0c821611":"code","be69ff6b":"code","81b488d6":"code","aa899d76":"code","95e1d142":"code","d3b5181d":"code","15e025e8":"code","0a755f9d":"code","d7276f59":"code","44bd91d0":"code","75a386cf":"code","f9cf6c49":"code","de5e4a2e":"code","39241f63":"code","e3e93bc9":"code","9e2ced2a":"code","78f355d4":"code","f52f64ed":"code","95977da5":"code","c921b53b":"code","d69b9d09":"code","f8b1bdfd":"code","197c1791":"code","81c43955":"code","0cdbf1ae":"code","843bdb56":"code","d5207fb5":"code","0308c187":"code","2865680f":"code","30a43904":"code","7677b5ae":"code","d322d48b":"code","663c10f6":"code","85364879":"code","becd5371":"code","9b3c1045":"code","fa33abb5":"code","8a448291":"code","80c4a0f1":"code","7f1471aa":"code","64cff0d8":"code","a44fb07a":"code","cc0cd16b":"code","d8d4b0c8":"code","f5e2df24":"code","f116d531":"code","12f33f5c":"code","0108cb17":"code","ad6ca9b5":"code","84de21ee":"code","8221a5f3":"code","847d98a9":"code","7a1143b4":"code","7fef99b6":"code","9da48ef5":"code","bb79964e":"code","aa7111a9":"code","92f9318c":"code","5a3c33b4":"code","e0e7bda4":"code","16600ac3":"code","f7db90e1":"code","bec2ef2c":"code","bd35e1fd":"code","2aa3365e":"code","010b7217":"code","545ac823":"code","480551a9":"code","8d763ff5":"code","693654d3":"code","a62ff53d":"code","11daff89":"code","868147f9":"code","57fec3fb":"code","b3837e7e":"code","7ca8dde5":"code","3e5ab8c1":"code","73ed65cd":"code","10505de5":"code","72f6fbab":"code","0b105eae":"code","7df793c5":"code","16c4e79e":"code","d0063656":"code","af873b95":"code","30b0c042":"code","558c1d36":"code","38b565d9":"code","87438b31":"code","fcf59e6d":"code","fd2baa17":"code","f4757bef":"code","19e40cc8":"code","1f40d246":"code","596e22ef":"code","7bddbc0a":"code","6e552863":"code","ec75b084":"code","e253d73d":"code","7be75bbf":"code","f286be34":"code","66c5580a":"code","0c0763c5":"code","3bc72c6a":"code","9cf9fca3":"code","7150dd25":"markdown","f08133c5":"markdown","d33d50c3":"markdown","da656646":"markdown","b8806a21":"markdown","b707bf60":"markdown","06548877":"markdown","568e3a05":"markdown","c350af3d":"markdown","499fcf75":"markdown","5c5d61ad":"markdown","dfa45638":"markdown","63465f25":"markdown","156aebde":"markdown","17e95f4c":"markdown","358d8377":"markdown","c4c1b936":"markdown","dcfe8109":"markdown","ed59e4f0":"markdown","56c7e7b4":"markdown","17dc5aee":"markdown","4687f26e":"markdown","c2bfd5bd":"markdown","c0e075f5":"markdown","301f2566":"markdown","0479ab7a":"markdown","fb37ac5e":"markdown","f143bef1":"markdown","6b1e6523":"markdown","22f18d10":"markdown","d6d6c41d":"markdown","98795662":"markdown","8ffff9e6":"markdown","7cea1187":"markdown","415627d5":"markdown","72d69d22":"markdown","ce0db59c":"markdown","1e6b3e92":"markdown","d778166f":"markdown","1002297c":"markdown","c24efa74":"markdown","6432c881":"markdown","492d715b":"markdown","15abefd2":"markdown","85277b8b":"markdown","935d6014":"markdown","d917744a":"markdown","1aeee795":"markdown","004f718a":"markdown","73109e32":"markdown","a41f08a6":"markdown","39e35062":"markdown","01256ff2":"markdown","74240d63":"markdown","f55ef71b":"markdown","eefab494":"markdown","d0ae293e":"markdown","311dd97a":"markdown","f4527bf8":"markdown","3632641e":"markdown","3b445cd5":"markdown","667c5049":"markdown"},"source":{"34158511":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random as rnd\n\nfrom collections import Counter\n\n# From Sklearn, sub-library model_selection, train_test_split so I can, well, split to training and test sets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.model_selection import learning_curve, validation_curve\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n%matplotlib inline\nplt.style.use(\"seaborn-whitegrid\")\n","e12c5ce3":"dataset = pd.read_csv(\"..\/input\/airlines-customer-satisfaction\/Invistico_Airline.csv\")\ndata = dataset.sample(frac=.05)\ndata.sample(10)","fb50b09b":"data.info()","621dc362":"category = [\"satisfaction\", \"Gender\", \"Customer Type\", \"Type of Travel\", \"Class\"]\nfor c in category:\n    print (\"{} \\n\".format(data[c].value_counts()))","6592188a":"#Mapping satisfied and dissatisfied in number \nsatisfaction_mapping = {\"satisfied\": 1,\"dissatisfied\": 0 }\ndata['satisfaction']  = data['satisfaction'].map(satisfaction_mapping)\n\n#Mapping Male and Female in number \nGender_mapping = {\"Male\": 1,\"Female\": 2 }\ndata['Gender']  = data['Gender'].map(Gender_mapping)\n\n#Mapping Loyal and disloyal in number \nCustomer_Type_mapping = {\"Loyal Customer\": 1,\"disloyal Customer\": 0 }\ndata['Customer Type']  = data['Customer Type'].map(Customer_Type_mapping)\n\n#Mapping Business travel and Business travel in number \nType_of_Travel_mapping = {\"Business travel\": 1,\"Personal Travel\": 2 }\ndata['Type of Travel']  = data['Type of Travel'].map(Type_of_Travel_mapping)\n\n#Mapping Business and Eco and Eco plus in number \nClass_mapping = {\"Business\": 1,\"Eco\": 3, \"Eco Plus\": 2 }\ndata['Class']  = data['Class'].map(Class_mapping)\n","0febbc38":"plt.figure(figsize=(15, 5))\nplt.subplot(121)\nsns.countplot(x='Class',data=data)\nplt.subplot(122)\nsns.countplot(x='Class',hue='satisfaction',data=data)","d6a118ae":"numericVar = [\"Age\", \"Flight Distance\", \"Seat comfort\", \"Departure\/Arrival time convenient\", \"Food and drink\", \"Gate location\", \"Inflight wifi service\", \"Inflight entertainment\", \"Online support\", \"Ease of Online booking\", \"On-board service\", \"Leg room service\", \"Baggage handling\", \"Checkin service\", \"Cleanliness\", \"Online boarding\", \"Departure Delay in Minutes\", \"Arrival Delay in Minutes\"]\n\nfig, axs = plt.subplots(nrows=9, ncols=2,figsize=(20,25))\n\nrow = 0\ncol = 0\nfor n in numericVar:\n    if(col==2):\n        row+=1\n        col=0\n    axs[row,col].hist(data[n], bins = 50)\n    axs[row,col].set_xlabel(n)\n    axs[row,col].set_ylabel('Frequency')\n    axs[row,col].set_title(\"{} distribution with hist\".format(n))\n    \n    col+=1\n    \nfig.tight_layout()","7ab0e19f":"plt.figure(figsize=(80,30))\nsns.countplot(x='Age',hue='satisfaction',data=data)","a042df11":"# import missingno\n# # Plot graphic of missing values\n# missingno.matrix(data, figsize = (30,20))\n\n\nsns.heatmap(data.isnull(),cmap='Blues')","1ecfc93b":"## Fill Missing Values\ndata['Arrival Delay in Minutes']=data['Arrival Delay in Minutes'].fillna(data['Arrival Delay in Minutes'].mean()) ","5d3d598d":"sns.heatmap(data.isnull(),cmap='Blues')\n# sns.heatmap(dataset.isnull(),yticklabels=False,cbar=False,cmap='YlGnBu')","2369963c":"data.isnull().sum()","8d76560a":"plt.figure(figsize=(15,10))\nsns.boxplot(x='Gender',y='Age',data=data)","8501bb52":"list1 =[\"satisfaction\", \"Gender\", \"Customer Type\", \"Age\" , \"Type of Travel\", \"Class\" , \"Flight Distance\" , \"Seat comfort\" ,\"Departure\/Arrival time convenient\" ,\"Food and drink\"\n, \"Gate location\" ,\"Inflight wifi service\",\"Inflight entertainment\",\"Online support\",\"Ease of Online booking\"\n,\"On-board service\",\"Leg room service\",\"Baggage handling\",\"Checkin service\",\"Cleanliness\",\"Online boarding\"\n,\"Departure Delay in Minutes\", \"Arrival Delay in Minutes\"]\n\nplt.subplots(figsize=(15,15)) \nsns.heatmap(data[list1].corr(), annot = True, fmt = \".2f\")\nplt.show()","b5bda1a8":"#define a function, so that we can make bar chart for every feature. \ndef barchart(feature):\n    g = sns.barplot(x=feature,y=\"satisfaction\",data=data)\n    g = g.set_ylabel(\"Satisfaction Probability\")","d34c6471":"# For Gender feature.\nbarchart('Gender')","d8aea7b9":"# For Customer Type feature.\nbarchart('Customer Type')","bfa1da78":"# For Class feature.\nbarchart('Class')","0c821611":"# For Type of Travel feature.\nbarchart('Type of Travel')","be69ff6b":"# For Age feature.\ng = sns.FacetGrid(data, col = \"satisfaction\")\ng.map(sns.distplot, \"Age\", bins = 25)\nplt.show()","81b488d6":"# For Flight distance feature.\ng = sns.FacetGrid(data, col = \"satisfaction\")\ng.map(sns.distplot, \"Flight Distance\", bins = 25)\nplt.show()","aa899d76":"sns.barplot(x=\"Customer Type\", y=\"satisfaction\", hue=\"Gender\", data=data);","95e1d142":"sns.barplot(x=\"Class\", y=\"satisfaction\", hue=\"Gender\", data=data);","d3b5181d":"sns.barplot(x=\"Type of Travel\", y=\"satisfaction\", hue=\"Gender\", data=data);","15e025e8":"data = data.drop(['Arrival Delay in Minutes'], axis=1)\ndata.head(10)","0a755f9d":"data = data.drop(['Departure Delay in Minutes'], axis=1)\ndata.head(10)","d7276f59":"X = data.drop('satisfaction',axis=1).values \ny = data['satisfaction'].values\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2, random_state = 100)","44bd91d0":"# y = data.satisfaction\n# data = data.drop('satisfaction',axis=1)\n\n# # split train and test data\n# x_train,x_test,y_train,y_test = train_test_split(data,y,test_size=0.2)\n\nprint (X_train.shape, y_train.shape)\nprint (X_test.shape, y_test.shape)","75a386cf":"def plotLearningCurves(X_train, y_train, classifier, title):\n    train_sizes, train_scores, test_scores = learning_curve(\n            classifier, X_train, y_train, cv=5, scoring=\"accuracy\")\n    \n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    plt.plot(train_sizes, train_scores_mean, label=\"Training Error\")\n    plt.plot(train_sizes, test_scores_mean, label=\"Cross Validation Error\")\n    \n    plt.legend()\n    plt.grid()\n    plt.title(title, fontsize = 18, y = 1.03)\n    plt.xlabel('Training Error', fontsize = 14)\n    plt.ylabel('Cross Validation Error', fontsize = 14)\n    plt.tight_layout()","f9cf6c49":"def plotValidationCurves(X_train, y_train, classifier, param_name, param_range, title):\n    train_scores, test_scores = validation_curve(\n        classifier, X_train, y_train, param_name = param_name, param_range = param_range,\n        cv=5, scoring=\"accuracy\")\n\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    plt.plot(param_range, train_scores_mean, label=\"Training Error\")\n    plt.plot(param_range, test_scores_mean, label=\"Cross Validation Error\")\n\n    plt.legend()\n    plt.grid()\n    plt.title(title, fontsize = 18, y = 1.03)\n    plt.xlabel('Training Error', fontsize = 14)\n    plt.ylabel('Cross Validation Error', fontsize = 14)\n    plt.tight_layout()","de5e4a2e":"# # LogisticRegression\n# from sklearn.linear_model import LogisticRegression\n# LR = LogisticRegression()\n# LR.fit(X_train,y_train)\n\n# # making predictions on the testing set \n# y_perdict_test = LR.predict(X_test)\n\n# # from sklearn.metrics import confusion_matrix,classification_report\n# # cm = confusion_matrix(y_test,y_perdict_test)\n# # sns.heatmap(cm,annot=True)\n\n  \n# # comparing actual response values (y_test) with predicted response values (y_pred) \n# from sklearn.metrics import accuracy_score\n\n# LR_accuracy = accuracy_score(y_test,y_perdict_test)\n# print(\"Logistic Regression model accuracy:\", LR_accuracy) ","39241f63":"# Choose some initial parameters combinations to try\nrondomForestClf = RandomForestClassifier(n_estimators=9, # no of sample trees\n                             # max_features=['log2', 'sqrt','auto'], \n                             # criterion=['entropy', 'gini'],\n                             max_depth=2, # max depth of each ensamble tree\n                             min_samples_split=2, # min no of samples in each node\n                             min_samples_leaf=1 # min no of samples in each leaf\n                            )\n\n# Fit the best algorithm to the data. \nrondomForestClf.fit(X_train, y_train)\nrondomForestPredictions1 = rondomForestClf.predict(X_test)","e3e93bc9":"# call general function to fit the classifier and draw the learning curve\nplt.figure(figsize = (16,5))\ntitle = 'Random Forest Learning Curve'\nplotLearningCurves(X_train, y_train, rondomForestClf, title)","9e2ced2a":"# call general function to fit the classifier and draw the validation curve\ntitle = 'Random Forest Validation Curve'\nparam_name = 'n_estimators'\nparam_range = [4, 6, 9]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, rondomForestClf, param_name, param_range, title)","78f355d4":"print(accuracy_score(y_test, rondomForestPredictions1))\nprint(confusion_matrix(y_test, rondomForestPredictions1))\nprint(classification_report(y_test, rondomForestPredictions1))","f52f64ed":"rondomForestClf_disp = plot_roc_curve(rondomForestClf, X_test, y_test)\nplt.show()","95977da5":"# change some parameters combinations to increase the accuracy\n# increase the number of samples split \/ leaf numbers\nrondomForestClf = RandomForestClassifier(n_estimators=9, # no of sample trees\n                             # max_features=['log2', 'sqrt','auto'], \n                             # criterion=['entropy', 'gini'],\n                             max_depth=2, # max depth of each ensamble tree\n                             min_samples_split=5, # min no of samples in each node\n                             min_samples_leaf=3 # min no of samples in each leaf\n                            )\n\n# Fit the best algorithm to the data. \nrondomForestClf.fit(X_train, y_train)\nrondomForestPredictions2 = rondomForestClf.predict(X_test)","c921b53b":"# call general function to fit the classifier and draw the learning curve\nplt.figure(figsize = (16,5))\ntitle = 'Random Forest Learning Curve'\nplotLearningCurves(X_train, y_train, rondomForestClf, title)","d69b9d09":"# call general function to fit the classifier and draw the validation curve\ntitle = 'Random Forest Validation Curve'\nparam_name = 'n_estimators'\nparam_range = [4, 6, 9]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, rondomForestClf, param_name, param_range, title)","f8b1bdfd":"print(accuracy_score(y_test, rondomForestPredictions2))\nprint(confusion_matrix(y_test, rondomForestPredictions2))\nprint(classification_report(y_test, rondomForestPredictions2))","197c1791":"# change some parameters combinations to increase the accuracy\n# change the tree depth with preserving the other parameters\nrondomForestClf = RandomForestClassifier(n_estimators=9, # no of sample trees\n                             # max_features=['log2', 'sqrt','auto'], \n                             # criterion=['entropy', 'gini'],\n                             max_depth=3, # max depth of each ensamble tree\n                             min_samples_split=5, # min no of samples in each node\n                             min_samples_leaf=1 # min no of samples in each leaf\n                            )\n\n# Fit the best algorithm to the data. \nrondomForestClf.fit(X_train, y_train)\n\nrondomForestPredictions3 = rondomForestClf.predict(X_test)","81c43955":"# call general function to fit the classifier and draw the learning curve\nplt.figure(figsize = (16,5))\ntitle = 'Random Forest Learning Curve'\nplotLearningCurves(X_train, y_train, rondomForestClf, title)","0cdbf1ae":"# call general function to fit the classifier and draw the validation curve\ntitle = 'Random Forest Validation Curve'\nparam_name = 'n_estimators'\nparam_range = [4, 6, 9]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, rondomForestClf, param_name, param_range, title)","843bdb56":"print(accuracy_score(y_test, rondomForestPredictions3))\nprint(confusion_matrix(y_test, rondomForestPredictions3))\nprint(classification_report(y_test, rondomForestPredictions3))","d5207fb5":"rondomForestClf_disp = plot_roc_curve(rondomForestClf, X_test, y_test)\nplt.show()","0308c187":"# change some parameters combinations to increase the accuracy\n# change the samples split \/ leaf numbers with preserving the other parameters\nrondomForestClf = RandomForestClassifier(n_estimators=9, # no of sample trees\n                             # max_features=['log2', 'sqrt','auto'], \n                             # criterion=['entropy', 'gini'],\n                             max_depth=3, # max depth of each ensamble tree\n                             min_samples_split=8, # min no of samples in each node\n                             min_samples_leaf=3 # min no of samples in each leaf\n                            )\n\n# Fit the best algorithm to the data. \nrondomForestClf.fit(X_train, y_train)\n\nrondomForestPredictions4 = rondomForestClf.predict(X_test)","2865680f":"# call general function to fit the classifier and draw the learning curve\nplt.figure(figsize = (16,5))\ntitle = 'Random Forest Learning Curve'\nplotLearningCurves(X_train, y_train, rondomForestClf, title)","30a43904":"# call general function to fit the classifier and draw the validation curve\ntitle = 'Random Forest Validation Curve'\nparam_name = 'n_estimators'\nparam_range = [4, 6, 9]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, rondomForestClf, param_name, param_range, title)","7677b5ae":"print(accuracy_score(y_test, rondomForestPredictions4))\nprint(confusion_matrix(y_test, rondomForestPredictions4))\nprint(classification_report(y_test, rondomForestPredictions4))","d322d48b":"# change some parameters combinations to increase the accuracy\n# change the tree depth with preserving the other parameters\nrondomForestClf = RandomForestClassifier(n_estimators=9, # no of sample trees\n                             # max_features=['log2', 'sqrt','auto'], \n                             # criterion=['entropy', 'gini'],\n                             max_depth=5, # max depth of each ensamble tree\n                             min_samples_split=8, # min no of samples in each node\n                             min_samples_leaf=5 # min no of samples in each leaf\n                            )\n\n# Fit the best algorithm to the data. \nrondomForestClf.fit(X_train, y_train)\n\nrondomForestPredictions5 = rondomForestClf.predict(X_test)","663c10f6":"# call general function to fit the classifier and draw the learning curve\nplt.figure(figsize = (16,5))\ntitle = 'Random Forest Learning Curve'\nplotLearningCurves(X_train, y_train, rondomForestClf, title)","85364879":"# call general function to fit the classifier and draw the validation curve\ntitle = 'Random Forest Validation Curve'\nparam_name = 'n_estimators'\nparam_range = [4, 6, 9]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, rondomForestClf, param_name, param_range, title)","becd5371":"print(accuracy_score(y_test, rondomForestPredictions5))\nprint(confusion_matrix(y_test, rondomForestPredictions5))\nprint(classification_report(y_test, rondomForestPredictions5))","9b3c1045":"# change some parameters combinations to increase the accuracy\n# change the tree depth with preserving the other parameters\nrondomForestClf = RandomForestClassifier(n_estimators=9, # no of sample trees\n                             # max_features=['log2', 'sqrt','auto'], \n                             # criterion=['entropy', 'gini'],\n                             max_depth=8, # max depth of each ensamble tree\n                             min_samples_split=8, # min no of samples in each node\n                             min_samples_leaf=5 # min no of samples in each leaf\n                            )\n\n# Fit the best algorithm to the data. \nrondomForestClf.fit(X_train, y_train)\n\nrondomForestPredictions6 = rondomForestClf.predict(X_test)","fa33abb5":"# call general function to fit the classifier and draw the learning curve\nplt.figure(figsize = (16,5))\ntitle = 'Random Forest Learning Curve'\nplotLearningCurves(X_train, y_train, rondomForestClf, title)","8a448291":"# call general function to fit the classifier and draw the validation curve\ntitle = 'Random Forest Validation Curve'\nparam_name = 'n_estimators'\nparam_range = [4, 6, 9]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, rondomForestClf, param_name, param_range, title)","80c4a0f1":"print(accuracy_score(y_test, rondomForestPredictions6))\nprint(confusion_matrix(y_test, rondomForestPredictions6))\nprint(classification_report(y_test, rondomForestPredictions6))","7f1471aa":"rondomForestClf = RandomForestClassifier()\n\nparameters = {'n_estimators': [4, 6, 9], # no of sample trees\n              'max_features': ['log2', 'sqrt','auto'], \n              'criterion': ['entropy', 'gini'],\n              'max_depth': [2, 3, 5, 8], # max depth of each ensamble tree\n              'min_samples_split': [2, 5, 8], # min no of samples in each node\n              'min_samples_leaf': [1, 3, 5] # min no of samples in each leaf\n             }\n\n# Type of scoring used to compare parameter combinations\nacc_scorer = make_scorer(accuracy_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(rondomForestClf, parameters, scoring=acc_scorer)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nrondomForestClf = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nrondomForestClf.fit(X_train, y_train)\n\npredictions = rondomForestClf.predict(X_test)\n\nprint(grid_obj.best_estimator_)","64cff0d8":"# call general function to fit the classifier and draw the learning curve\nplt.figure(figsize = (16,5))\ntitle = 'Random Forest Learning Curve'\nplotLearningCurves(X_train, y_train, rondomForestClf, title)","a44fb07a":"# call general function to fit the classifier and draw the validation curve\ntitle = 'Random Forest Validation Curve'\nparam_name = 'n_estimators'\nparam_range = [4, 6, 9]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, rondomForestClf, param_name, param_range, title)","cc0cd16b":"knn = KNeighborsClassifier(1)\nknn.fit(X_train, y_train)\nknnPredictions1 = knn.predict(X_test)\nprint(accuracy_score(y_test, knnPredictions1))\nprint(confusion_matrix(y_test, knnPredictions1))\nprint(classification_report(y_test, knnPredictions1))","d8d4b0c8":"knn_disp = plot_roc_curve(knn, X_test, y_test)\nplt.show()","f5e2df24":"plt.figure(figsize = (16,5))\ntitle = 'KNN k=1 Learning Curve'\nplotLearningCurves(X_train, y_train, knn, title)","f116d531":"knn = KNeighborsClassifier(3)\nknn.fit(X_train, y_train)\nknnPredictions3 = knn.predict(X_test)\nprint(accuracy_score(y_test, knnPredictions3))\nprint(confusion_matrix(y_test, knnPredictions3))\nprint(classification_report(y_test, knnPredictions3))","12f33f5c":"knn_disp = plot_roc_curve(knn, X_test, y_test)\nplt.show()","0108cb17":"plt.figure(figsize = (16,5))\ntitle = 'KNN k=3 Learning Curve'\nplotLearningCurves(X_train, y_train, knn, title)","ad6ca9b5":"knn = KNeighborsClassifier(5)\nknn.fit(X_train, y_train)\nknnPredictions5 = knn.predict(X_test)\nprint(accuracy_score(y_test, knnPredictions5))\nprint(confusion_matrix(y_test, knnPredictions5))\nprint(classification_report(y_test, knnPredictions5))","84de21ee":"knn_disp = plot_roc_curve(knn, X_test, y_test)\nplt.show()","8221a5f3":"plt.figure(figsize = (16,5))\ntitle = 'KNN k=5 Learning Curve'\nplotLearningCurves(X_train, y_train, knn, title)","847d98a9":"knn = KNeighborsClassifier(7)\nknn.fit(X_train, y_train)\nknnPredictions7 = knn.predict(X_test)\nprint(accuracy_score(y_test, knnPredictions7))\nprint(confusion_matrix(y_test, knnPredictions7))\nprint(classification_report(y_test, knnPredictions7))","7a1143b4":"knn_disp = plot_roc_curve(knn, X_test, y_test)\nplt.show()","7fef99b6":"plt.figure(figsize = (16,5))\ntitle = 'KNN k=7 Learning Curve'\nplotLearningCurves(X_train, y_train, knn, title)","9da48ef5":"# call general function to fit the classifier and draw the validation curve\nknn = KNeighborsClassifier()\ntitle = 'KNN Validation Curve'\nparam_name = 'n_neighbors'\nparam_range = np.arange(1,9,2)\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, knn, param_name, param_range, title)","bb79964e":"# Train a SVC model using different kernal\nsvclassifier = SVC(C = 0.1 , gamma =1 ,kernel='sigmoid')\nsvclassifier.fit(X_train, y_train)\n# Make prediction\nSvcPredictions1 = svclassifier.predict(X_test)\n# Evaluate our model\nprint(classification_report(y_test,SvcPredictions1))","aa7111a9":"# call general function to fit the classifier and draw the learning curve\nplt.figure(figsize = (16,5))\ntitle = 'Support Vector Machine Learning Curve'\nplotLearningCurves(X_train, y_train, svclassifier, title)","92f9318c":"# call general function to fit the classifier and draw the validation curve\ntitle = 'Support Vector Machine Validation Curve'\nparam_name = 'C'\nparam_range = [0.1,1, 10]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, svclassifier, param_name, param_range, title)","5a3c33b4":"# Train a SVC model using different kernal\nsvclassifier = SVC(C = 1 , gamma =1 ,kernel='sigmoid')\nsvclassifier.fit(X_train, y_train)\n# Make prediction\nSvcPredictions2 = svclassifier.predict(X_test)\n# Evaluate our model\nprint(classification_report(y_test,SvcPredictions2))","e0e7bda4":"# call general function to fit the classifier and draw the learning curve\nplt.figure(figsize = (16,5))\ntitle = 'Support Vector Machine Learning Curve'\nplotLearningCurves(X_train, y_train, svclassifier, title)","16600ac3":"# call general function to fit the classifier and draw the validation curve\ntitle = 'Support Vector Machine Validation Curve'\nparam_name = 'C'\nparam_range = [0.1,1, 10]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, svclassifier, param_name, param_range, title)","f7db90e1":"# Train a SVC model using different kernal\nsvclassifier = SVC(C = 10 , gamma =1 ,kernel='sigmoid')\nsvclassifier.fit(X_train, y_train)\n# Make prediction\nSvcPredictions3 = svclassifier.predict(X_test)\n# Evaluate our model\nprint(classification_report(y_test,SvcPredictions3))","bec2ef2c":"# call general function to fit the classifier and draw the learning curve\nplt.figure(figsize = (16,5))\ntitle = 'Support Vector Machine Learning Curve'\nplotLearningCurves(X_train, y_train, svclassifier, title)","bd35e1fd":"# call general function to fit the classifier and draw the validation curve\ntitle = 'Support Vector Machine Validation Curve'\nparam_name = 'C'\nparam_range = [0.1,1, 10]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, svclassifier, param_name, param_range, title)","2aa3365e":"# Train a SVC model using different kernal\nsvclassifier = SVC(C = 10 , gamma =0.01 ,kernel='sigmoid')\nsvclassifier.fit(X_train, y_train)\n# Make prediction\nSvcPredictions4 = svclassifier.predict(X_test)\n# Evaluate our model\nprint(classification_report(y_test,SvcPredictions4))","010b7217":"# call general function to fit the classifier and draw the learning curve\nplt.figure(figsize = (16,5))\ntitle = 'Support Vector Machine Learning Curve'\nplotLearningCurves(X_train, y_train, svclassifier, title)","545ac823":"# call general function to fit the classifier and draw the validation curve\ntitle = 'Support Vector Machine Validation Curve'\nparam_name = 'C'\nparam_range = [0.1,1, 10]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, svclassifier, param_name, param_range, title)","480551a9":"# Train a SVC model using different kernal\nsvclassifier = SVC(C = 10 , gamma =0.01 ,kernel='rbf')\nsvclassifier.fit(X_train, y_train)\n# Make prediction\nSvcPredictions5 = svclassifier.predict(X_test)\n# Evaluate our model\nprint(classification_report(y_test,SvcPredictions5))","8d763ff5":"# call general function to fit the classifier and draw the learning curve\nplt.figure(figsize = (16,5))\ntitle = 'Support Vector Machine Learning Curve'\nplotLearningCurves(X_train, y_train, svclassifier, title)","693654d3":"# call general function to fit the classifier and draw the validation curve\ntitle = 'Support Vector Machine Validation Curve'\nparam_name = 'C'\nparam_range = [0.1,1, 10]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, svclassifier, param_name, param_range, title)","a62ff53d":"# Train a SVC model using different kernal\nsvclassifier = SVC(C = 10 , gamma =1 ,kernel='rbf')\nsvclassifier.fit(X_train, y_train)\n# Make prediction\nSvcPredictions6 = svclassifier.predict(X_test)\n# Evaluate our model\nprint(classification_report(y_test,SvcPredictions6))","11daff89":"# call general function to fit the classifier and draw the learning curve\nplt.figure(figsize = (16,5))\ntitle = 'Support Vector Machine Learning Curve'\nplotLearningCurves(X_train, y_train, svclassifier, title)","868147f9":"# call general function to fit the classifier and draw the validation curve\ntitle = 'Support Vector Machine Validation Curve'\nparam_name = 'C'\nparam_range = [0.1,1, 10]\nplt.figure(figsize = (16,5))\nplotValidationCurves(X_train, y_train, svclassifier, param_name, param_range, title)","57fec3fb":"param_grid = {'C': [0.1,1, 10], 'gamma': [1,0.1,0.01],'kernel': ['sigmoid']}\ngrid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2)\nsvclassifier7 = grid.fit(X_train,y_train)\nSvcPredictions7 = svclassifier7.predict(X_test)\nprint(grid.best_estimator_)","b3837e7e":"print(accuracy_score(y_test, SvcPredictions7))\nprint(confusion_matrix(y_test, SvcPredictions7))\nprint(classification_report(y_test, SvcPredictions7))","7ca8dde5":"svm_disp = plot_roc_curve(svclassifier, X_test, y_test)\nplt.show()","3e5ab8c1":"def neural_network_learning_curve(classifier):\n    # call general function to fit the classifier and draw the learning curve\n    title = 'Neural Network Learning Curve'\n    plt.figure(figsize = (16,5))\n    plotLearningCurves(X_train, y_train, classifier, title)","73ed65cd":" def neural_network_validation_curve(classifier):   \n    # call general function to fit the classifier and draw the validation curve\n    title = 'Neural Network Validation Curve'\n    param_name=\"alpha\"\n    param_range = np.logspace(-6, -1, 5)\n    plt.figure(figsize = (16,5))\n    plotValidationCurves(X_train, y_train, classifier, param_name, param_range, title)","10505de5":"nnclf1 = MLPClassifier(hidden_layer_sizes=(2, 1), activation='logistic', solver='adam', max_iter=500, \n                    alpha=1e-5, random_state=1)\n\nnnclf1.fit(X_train, y_train)","72f6fbab":"NeuralNetworkPredictions1 = nnclf1.predict(X_test)\nprint(confusion_matrix(y_test,NeuralNetworkPredictions1))\nprint(classification_report(y_test,NeuralNetworkPredictions1))","0b105eae":"neural_network_learning_curve(nnclf1)","7df793c5":"neural_network_validation_curve(nnclf1)","16c4e79e":"nnclf2 = MLPClassifier(hidden_layer_sizes=(2, 2), activation='logistic', solver='adam', max_iter=500, \n                    alpha=1e-5, random_state=1)\n\nnnclf2.fit(X_train, y_train)","d0063656":"NeuralNetworkPredictions2 = nnclf1.predict(X_test)\nprint(confusion_matrix(y_test,NeuralNetworkPredictions2))\nprint(classification_report(y_test,NeuralNetworkPredictions2))","af873b95":"neural_network_learning_curve(nnclf2)","30b0c042":"neural_network_validation_curve(nnclf2)","558c1d36":"nnclf3 = MLPClassifier(hidden_layer_sizes=(2, 3), activation='logistic', solver='adam', max_iter=500, \n                    alpha=1e-5, random_state=1)\n\nnnclf3.fit(X_train, y_train)","38b565d9":"NeuralNetworkPredictions3 = nnclf1.predict(X_test)\nprint(confusion_matrix(y_test,NeuralNetworkPredictions3))\nprint(classification_report(y_test,NeuralNetworkPredictions3))","87438b31":"neural_network_learning_curve(nnclf3)","fcf59e6d":"neural_network_validation_curve(nnclf3)","fd2baa17":"nnclf4 = MLPClassifier(hidden_layer_sizes=(3, 2), activation='logistic', solver='adam', max_iter=500, \n                    alpha=1e-5, random_state=1)\n\nnnclf4.fit(X_train, y_train)","f4757bef":"NeuralNetworkPredictions4 = nnclf1.predict(X_test)\nprint(confusion_matrix(y_test,NeuralNetworkPredictions4))\nprint(classification_report(y_test,NeuralNetworkPredictions4))","19e40cc8":"neural_network_learning_curve(nnclf4)","1f40d246":"neural_network_validation_curve(nnclf4)","596e22ef":"nnclf5 = MLPClassifier(hidden_layer_sizes=(3, 3), activation='logistic', solver='adam', max_iter=500, \n                    alpha=1e-5, random_state=1)\n\nnnclf5.fit(X_train, y_train)","7bddbc0a":"NeuralNetworkPredictions5 = nnclf1.predict(X_test)\nprint(confusion_matrix(y_test,NeuralNetworkPredictions5))\nprint(classification_report(y_test,NeuralNetworkPredictions5))","6e552863":"neural_network_learning_curve(nnclf5)","ec75b084":"neural_network_validation_curve(nnclf5)","e253d73d":"nnclf6 = MLPClassifier(hidden_layer_sizes=(5, 3), activation='logistic', solver='adam', max_iter=500, \n                    alpha=1e-5, random_state=1)\n\nnnclf6.fit(X_train, y_train)","7be75bbf":"NeuralNetworkPredictions6 = nnclf1.predict(X_test)\nprint(confusion_matrix(y_test,NeuralNetworkPredictions6))\nprint(classification_report(y_test,NeuralNetworkPredictions6))","f286be34":"neural_network_learning_curve(nnclf6)","66c5580a":"neural_network_validation_curve(nnclf6)","0c0763c5":"# Import the classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\n \n\n# Instantiate the classfiers and make a list\nclassifiers = [LogisticRegression(random_state=1234), \n               SVC(),\n               KNeighborsClassifier(), \n               RandomForestClassifier(random_state=1234),\n               MLPClassifier()]\n\nresult_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\n \n# print('auc =', auc)\nlr_fpr1, lr_tpr1, _ = roc_curve(y_test, rondomForestPredictions3)\nlr_fpr2, lr_tpr2, _ = roc_curve(y_test, knnPredictions5)\nlr_fpr3, lr_tpr3, _ = roc_curve(y_test, SvcPredictions4)\nlr_fpr4, lr_tpr4, _ = roc_curve(y_test, NeuralNetworkPredictions2)\n# fpr , tpr, _= roc_curve(X_test, predict6_test)\nauc1 = roc_auc_score(y_test, rondomForestPredictions3)\nauc2 = roc_auc_score(y_test, knnPredictions5)\nauc3 = roc_auc_score(y_test, SvcPredictions5)\nauc4 = roc_auc_score(y_test, NeuralNetworkPredictions2)\n\n\n\n\nresult_table = result_table.append({'classifiers':RandomForestClassifier.__class__.__name__,\n                                     'fpr':lr_fpr1, \n                                     'tpr':lr_tpr1, \n                                     'auc':auc1}, ignore_index=True)\n\nresult_table = result_table.append({'classifiers':KNeighborsClassifier.__class__.__name__,\n                                     'fpr':lr_fpr2, \n                                     'tpr':lr_tpr2, \n                                     'auc':auc2}, ignore_index=True)\n\nresult_table = result_table.append({'classifiers':SVC.__class__.__name__,\n                                     'fpr':lr_fpr3, \n                                     'tpr':lr_tpr3, \n                                     'auc':auc3}, ignore_index=True)\n\nresult_table = result_table.append({'classifiers':MLPClassifier.__class__.__name__,\n                                     'fpr':lr_fpr4, \n                                     'tpr':lr_tpr4, \n                                     'auc':auc4}, ignore_index=True)\n\n \nfig = plt.figure(figsize=(8,6))\n\n# for i in result_table.index:\n#     plt.plot(result_table.loc[i]['fpr'], \n#              result_table.loc[i]['tpr'], \n#              label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n\n\nplt.plot(result_table.loc[0]['fpr'], \n         result_table.loc[0]['tpr'], \n         label=\"RandomForestClassifier, AUC={:.3f}\".format( result_table.loc[0]['auc']))\n\nplt.plot(result_table.loc[1]['fpr'], \n         result_table.loc[1]['tpr'], \n         label=\"KNeighborsClassifier, AUC={:.3f}\".format( result_table.loc[1]['auc']))\n\nplt.plot(result_table.loc[2]['fpr'], \n         result_table.loc[2]['tpr'], \n         label=\"SVM, AUC={:.3f}\".format( result_table.loc[2]['auc']))\n\nplt.plot(result_table.loc[3]['fpr'], \n         result_table.loc[3]['tpr'], \n         label=\"MLPClassifier, AUC={:.3f}\".format( result_table.loc[3]['auc']))\n    \n# plt.plot([0,1], [0,1], color='orange', linestyle='--')\n\nplt.xticks(np.arange(0.0, 1.1, step=0.1))\nplt.xlabel(\"Flase Positive Rate\", fontsize=15)\n\nplt.yticks(np.arange(0.0, 1.1, step=0.1))\nplt.ylabel(\"True Positive Rate\", fontsize=15)\n\nplt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\nplt.legend(prop={'size':13}, loc='lower right')\n\nplt.show()","3bc72c6a":"# Regression AutoML\n\nimport sklearn.model_selection\nimport sklearn.datasets\nimport sklearn.metrics\nimport os  \nimport autosklearn.regression\n\ntmp_folder='\/tmp\/autosklearn_regression_example_tmp'\noutput_folder='\/tmp\/autosklearn_regression_example_out'\n    \n\nautoml = autosklearn.regression.AutoSklearnRegressor(\n    time_left_for_this_task=120,\n    per_run_time_limit=30,\n    tmp_folder = tmp_folder,\n    output_folder = output_folder,\n)\nautoml.fit(X_train, y_train, dataset_name='Airlines' )\n\nprint(automl.show_models())\npredictions = automl.predict(X_test)\nprint(\"R2 score:\", sklearn.metrics.r2_score(y_test, predictions))","9cf9fca3":"# Cross-Validation AutoML\n\nimport sklearn.model_selection\nimport sklearn.datasets\nimport sklearn.metrics\n\nimport autosklearn.classification\n\n\nautoml = autosklearn.classification.AutoSklearnClassifier(\n    time_left_for_this_task=120,\n    per_run_time_limit=30,\n    tmp_folder='\/tmp\/autosklearn_cv_example_tmp1',\n    output_folder='\/tmp\/autosklearn_cv_example_out1',\n    delete_tmp_folder_after_terminate=False,\n    resampling_strategy='cv',\n    resampling_strategy_arguments={'folds': 5},\n)\n\n# fit() changes the data in place, but refit needs the original data. We\n# therefore copy the data. In practice, one should reload the data\nautoml.fit(X_train.copy(), y_train.copy(), dataset_name='AirLines')\n# During fit(), models are fit on individual cross-validation folds. To use\n# all available data, we call refit() which trains all models in the\n# final ensemble on the whole dataset.\nautoml.refit(X_train.copy(), y_train.copy())\n\nprint(automl.show_models())\n\npredictions = automl.predict(X_test)\nprint(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, predictions))\n","7150dd25":"### 7.1- Correlation Between numeric values (Satisfaction, Gender, Customer Type, Age, Type of Travel, Class, Flight Distance, Seat comfort, Departure\/Arrival time convenient, Food and drink, Gate location, Inflight wifi service, Inflight entertainment, Online support, Ease of Online booking, On-board service, Leg room service, Baggage handling, Checkin service, Cleanliness, Online boarding, Departure Delay in Minutes, Arrival Delay in Minutes)","f08133c5":"## STEP #3.8: Check features with satisfaction","d33d50c3":"# Exploratory Data Analysis (EDA)","da656646":"rbf generates a high variance model as the gab increased between the training error and the cross validation error, now change the gamma parameter to 1 and check","b8806a21":"As shown in the above curve, there are variations between the four curves that shows the randomforest classifier has the biggest area under curve, and SVM covers he lowest area under curve, while neural network has better performance than K nearest neighbors.","b707bf60":"Initialy tried with C=0.1, and gamma=1, and kernal = segmoid, accuracy = 0.9, then change the C parameter to 1","06548877":"### finally we have the best accuracy of .90 when we tryed to make 2 hidin layers with 8 nodes for prediction: NeuralNetworkPredictions6","568e3a05":"# STEP #6: Comparison between all classifier performance(AUC curve) ","c350af3d":"### STEP #5.3.2: Results for neighbors = 3","499fcf75":"## STEP #3.6: Prepare the Data for Training \/ Data Cleaning\nfind the best way to fill the missing values in each feature\n### Find the missing values in train and test data files\nCount the null values in each column to decide to drop these rows or replace the values\n","5c5d61ad":"now C=1, and gamma=1, and kernal = segmoid, accuracy = 0.9 (no change), then change the C parameter to 10","dfa45638":"### Install autosklearn on Kaggle\n!apt-get remove swig\n!apt-get install swig3.0 build-essential -y\n!ln -s \/usr\/bin\/swig3.0 \/usr\/bin\/swig\n!apt-get install build-essential\n!pip install --upgrade setuptools\n!pip install auto-sklearn","63465f25":"## STEP #3.5: Numerical Analysis","156aebde":"### So changing the sample split \/ leaf numbers only without changing the tree depth decrease the accuracy","17e95f4c":"### STEP #5.3.1: Results for neighbors = 1","358d8377":"### as shown above, the accuracy increased significantly when the neighbors increased to 5\n### Validation Curve\n","c4c1b936":"### STEP #5.3.4: Results for neighbors = 7","dcfe8109":"## STEP #5.3: K Nearest Neighbors Calssifier","ed59e4f0":"## STEP #5.5: Neural Network Calssifier","56c7e7b4":"### STEP #5.3.3: Results for neighbors = 5","17dc5aee":"### 8.1- Arrival Delay in Minutes Feature","4687f26e":"## Define a general function for showing the Learning curve for any classifier","c2bfd5bd":"For Ages between 20 and 40 have high dissatisfaction, while the Ages more than 40 have high satisfaction.","c0e075f5":"# STEP #2: IMPORT DATASET","301f2566":"# STEP #5: Fitting and Tuning an Algorithm","0479ab7a":"### then we added another node in the second hidin layer and we recieved .89 accuracy","fb37ac5e":"# STEP #1: Import Libraries","f143bef1":"# STEP #7: Apply AutoML (e.g. auto sklearn )and compare its performance to your best model","6b1e6523":"# STEP #4: Split dataset to train \/ test data","22f18d10":"Feature Departure Delay in Minutes has no correlation with any other feature, so this feature is going to be dropped","d6d6c41d":"for C and gamma parameters, the effect is slightly noticable, now change the kernal parameter to rbf","98795662":"### accuracy increased after increasing the number of samples split \/ leaf, so next expirement will preserve the samples split \/ leaf number and try to increase the tree depth","8ffff9e6":"## STEP #3.1: Check the count \/ data type for each feature in train and test data files","7cea1187":"# STEP #9: # Reference\n\nThis kernel would have been imposible to make if not this amazing tutorials:\nMost basic matplotlib: https:\/\/towardsdatascience.com\/plt-xxx-or-ax-xxx-that-is-the-question-in-matplotlib-8580acf42f44\n\nUnderstand the difference between all the methods (add_subplot, add_subplots, add_axes ...): https:\/\/towardsdatascience.com\/the-many-ways-to-call-axes-in-matplotlib-2667a7b06e06\n\nMatplotlib grid documentation: https:\/\/matplotlib.org\/tutorials\/intermediate\/gridspec.html\n\nGreat tutorial fore beginners: https:\/\/github.com\/rougier\/matplotlib-tutorial\n\n50 beautiful plots using matplotlib: https:\/\/www.machinelearningplus.com\/plots\/top-50-matplotlib-visualizations-the-master-plots-python\/\n\nPandas plotting capabilities: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/visualization.html","415627d5":"### here we added another node to the second hidin layer so we have .85 accuracy","72d69d22":"now C=10, and gamma=1, and kernal = segmoid, accuracy = 0.9 (no change), then change the gamma parameter to 0.01","ce0db59c":"## STEP #5.2: Random Forest Calssifier","1e6b3e92":"> ### as shown we built an neural classifier with 2 hiden layer wich contint of 3 nodes and we got .84 accuracy","d778166f":"### here we tryed to swap the number of nodes in our 2 hidin layer instead of (2, 3) we made it (3, 2), so we recieved .88 accuracy","1002297c":"By checking the features, there are two types of features:\n   * Categorical Features: satisfaction, Gender, Customer Type, Type of Travel, Class.\n   * Numerical Features: Age, Flight Distance, Seat comfort, Departure\/Arrival time convenient, Food and drink, Gate location, Inflight wifi service, Inflight entertainment, Online support, Ease of Online booking, On-board service, Leg room service, Baggage handling, Checkin service, Cleanliness, Online boarding, Departure Delay in Minutes, Arrival Delay in Minutes.","c24efa74":"## STEP #3.2: Features analysis","6432c881":"# STEP #8: Conclusion\n\nWith this notebook we learned the basics of EDA with Pandas and Matplotlib as well as the foundations\nfor applying the classification models of the scikit learn library.\nBy EDA we found a strong impact of features like Gender and Age on Satisfaction.\n\nWe then built a simple baseline model with Pandas, using only these features.\nAgain, using Pandas, we also created a dataset that can be used by the scikit learn classifiers for prediction.\n\nWe applied Random Forest, k-nearest neighbors, Support vector machine (SVM) and Multilayer Perceptron (MLP).\n\n\nDeciding by Auto ML, the best ML models for this task and set of features was: Random Forest with accuracy 85.7%\n","492d715b":"# Intro\nThis data shows whether a customer is satisfied with the airlines or not after travelling with them. There are several other measurement or to say feedback taken from the customers as well as their demographic data is also recorded.\n\nData set URL : https:\/\/www.kaggle.com\/teejmahal20\/airline-passenger-satisfaction\n","15abefd2":"* ### keeping the tree depth at 2 and add more points in split \/ leaf","85277b8b":"Feature Arrival Delay in Minutes has no correlation with any other feature, so this feature is going to be dropped","935d6014":"# STEP #3: Explore \/Visualze Data set","d917744a":"so the best parameters will be: max_depth=3, min_samples_leaf=1, min_samples_split=5 for perdiction rondomForestPredictions3","1aeee795":"### as shown above, increasing the depth with the same number of samples for split \/ leaf increase the accuracy slightly, so next we will preserve the depth and try to increase the samples split \/ leaf numbers","004f718a":"===========================================================================================\n\n## STEP #3.9: Feature Engineering","73109e32":"from shown above in dataset, missing values for Arrival Delay in Minutes column are not that much (didnot get more than 90%), so we are going to keep them","a41f08a6":"### 8.2- Departure Delay in Minutes Feature","39e35062":"## STEP #5.1: Logistic Regression Calssifier","01256ff2":"## STEP #3.4: Mapping","74240d63":"## STEP #3.3: Categorical Analysis:\n\ncheck the specific values for each of categorical features","f55ef71b":"### then we added another node to the second hidin layer and we still recieved the same accuracy .85","eefab494":"==================================================================================================================\n\n## STEP #3.7: Visualization","d0ae293e":"* Departure delay in Minutes and Arrival Delay in Minutes features have the weakest correlation with all other features, so these twi feature are going to be dropped.\n* Food and drink feature has the strongest correlation with seat comfort feature (0.72).\n* Ease of Online booking, Online boarding, Online support, Cleanless, Baggage handling, and inflight wifi service features are considered to be have the secong strong correclation between all of them (~0.6)\n* Food drink, Online suppor, Cleanless, Baggage handling, Gate location, Onboard service, inflight wifi service features are considered to be have the secong strong correclation between all of them (~0.5)\n* satisfaction feature has a strong correlation with inflight entertainment and Ease of Online booking\n* Now, visualize each of these features with the satisfaction feature","311dd97a":"as shown above, there are missing values in Arrival Delay in Minutes feature in dataset","f4527bf8":"## STEP #5.4: Support Vector Machine Calssifier","3632641e":"changing the gamma parameter to 1,increased the gab (higher variance),so the best parameters are: C= 10, gamma = 0.01, kernal = segmoid for prediction: SvcPredictions4","3b445cd5":"so, we conclude that all the categorical features have specific values","667c5049":"### as shown above, the accuracy increased significantly when the tree depth increased to 5, so the tree depth parameter has a great impact on the accuracy, let's increase it to see if the accuracy will increase or not"}}