{"cell_type":{"615c6231":"code","74421220":"code","0ea054e8":"code","561e118b":"code","73b86f0c":"code","698959c5":"code","1be07677":"code","c10e450e":"code","1084afcc":"code","445bd1c5":"code","9d3382fd":"code","f79e0387":"code","e80dde07":"code","68cf9d73":"code","64a8ab1a":"code","4e844347":"code","662f7321":"code","d17c5273":"code","2f2f256d":"code","a5664e61":"code","de597eb9":"code","14b307b1":"code","29382f9e":"code","7ac3398e":"code","2fac5c8a":"code","10a27116":"code","6e1255e2":"code","a65f4bd6":"code","68eb920b":"code","5efb7abb":"code","83fe2286":"code","d6575052":"code","0fbd4f81":"code","58ab3725":"code","0250a2fd":"code","9b25d99d":"code","6a668e7e":"code","16b1d0d0":"code","a84b70a5":"code","26344718":"markdown","3a4ccbf1":"markdown","b500ce75":"markdown","318128ae":"markdown","06d85622":"markdown","5f5b8cde":"markdown","868fae25":"markdown","cc2e57f2":"markdown","dde5fa91":"markdown","679c8b60":"markdown","8a92c66b":"markdown","33548525":"markdown"},"source":{"615c6231":"#import libraries\nimport pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport warnings \nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n%matplotlib inline","74421220":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0ea054e8":"#load files:-\ntrain  = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsample_sub=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","561e118b":"train.shape, test.shape","73b86f0c":"train.isnull().sum(), test.isnull().sum()","698959c5":"# substitue NaN value here with mode\n\ntrain['location'] = train['location'].fillna(train['location'].mode()[0])\ntrain['keyword'] = train['keyword'].fillna(train['keyword'].mode()[0])\n\ntest['location'] = test['location'].fillna(test['location'].mode()[0])\ntest['keyword'] = test['keyword'].fillna(test['keyword'].mode()[0])","1be07677":"train.isnull().sum().sum(), test.isnull().sum().sum()","c10e450e":"train.shape, test.shape","1084afcc":"#compare tweets in train and test file\nlength_train=train['text'].str.len()\nlength_test=test['text'].str.len()\nplt.hist(length_train, bins=20, label=\"train_tweets\")\nplt.hist(length_test, bins=20, label=\"test_tweets\")\nplt.legend()\nplt.show()","445bd1c5":"train['target'].value_counts()","9d3382fd":"percentage_disaster=(train.target.value_counts() \/ len(train.target)) * 100\npercentage_disaster","f79e0387":"label = train.groupby('target')['target'].sum()\nlabel\/len(train)*100","e80dde07":"train.groupby('target').text.count().plot.bar(ylim=0)\nplt.show()","68cf9d73":"# Importing HTMLParser\nfrom html.parser import HTMLParser\nhtml_parser = HTMLParser()","64a8ab1a":"# Created a new columns i.e. clean_tweet contains the same tweets but cleaned version\ntrain['processed_text'] = train['text'].apply(lambda x: html_parser.unescape(x))\ntest['processed_text'] = test['text'].apply(lambda x: html_parser.unescape(x))","4e844347":"# Apostrophe Dictionary\napostrophe_dict = {\n\"ain't\": \"am not \/ are not\",\n\"aren't\": \"are not \/ am not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had \/ he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he shall \/ he will\",\n\"he'll've\": \"he shall have \/ he will have\",\n\"he's\": \"he has \/ he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how has \/ how is\",\n\"i'd\": \"I had \/ I would\",\n\"i'd've\": \"I would have\",\n\"i'll\": \"I shall \/ I will\",\n\"i'll've\": \"I shall have \/ I will have\",\n\"i'm\": \"I am\",\n\"i've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had \/ it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it shall \/ it will\",\n\"it'll've\": \"it shall have \/ it will have\",\n\"it's\": \"it has \/ it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had \/ she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she shall \/ she will\",\n\"she'll've\": \"she shall have \/ she will have\",\n\"she's\": \"she has \/ she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as \/ so is\",\n\"that'd\": \"that would \/ that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that has \/ that is\",\n\"there'd\": \"there had \/ there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there has \/ there is\",\n\"they'd\": \"they had \/ they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they shall \/ they will\",\n\"they'll've\": \"they shall have \/ they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had \/ we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what shall \/ what will\",\n\"what'll've\": \"what shall have \/ what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what has \/ what is\",\n\"what've\": \"what have\",\n\"when's\": \"when has \/ when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where has \/ where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who shall \/ who will\",\n\"who'll've\": \"who shall have \/ who will have\",\n\"who's\": \"who has \/ who is\",\n\"who've\": \"who have\",\n\"why's\": \"why has \/ why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you had \/ you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you shall \/ you will\",\n\"you'll've\": \"you shall have \/ you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}\napostrophe_dict","662f7321":"def lookup_dict(text, dictionary):\n    for word in text.split():\n        if word.lower() in dictionary:\n            if word.lower() in text.split():\n                text = text.replace(word, dictionary[word.lower()])\n    return text","d17c5273":"train['processed_text'] = train['processed_text'].apply(lambda x: lookup_dict(x,apostrophe_dict))\ntest['processed_text'] = test['processed_text'].apply(lambda x: lookup_dict(x,apostrophe_dict))","2f2f256d":"short_word_dict = {\n\"121\": \"one to one\",\n\"a\/s\/l\": \"age, sex, location\",\n\"adn\": \"any day now\",\n\"afaik\": \"as far as I know\",\n\"afk\": \"away from keyboard\",\n\"aight\": \"alright\",\n\"alol\": \"actually laughing out loud\",\n\"b4\": \"before\",\n\"b4n\": \"bye for now\",\n\"bak\": \"back at the keyboard\",\n\"bf\": \"boyfriend\",\n\"bff\": \"best friends forever\",\n\"bfn\": \"bye for now\",\n\"bg\": \"big grin\",\n\"bta\": \"but then again\",\n\"btw\": \"by the way\",\n\"cid\": \"crying in disgrace\",\n\"cnp\": \"continued in my next post\",\n\"cp\": \"chat post\",\n\"cu\": \"see you\",\n\"cul\": \"see you later\",\n\"cul8r\": \"see you later\",\n\"cya\": \"bye\",\n\"cyo\": \"see you online\",\n\"dbau\": \"doing business as usual\",\n\"fud\": \"fear, uncertainty, and doubt\",\n\"fwiw\": \"for what it's worth\",\n\"fyi\": \"for your information\",\n\"g\": \"grin\",\n\"g2g\": \"got to go\",\n\"ga\": \"go ahead\",\n\"gal\": \"get a life\",\n\"gf\": \"girlfriend\",\n\"gfn\": \"gone for now\",\n\"gmbo\": \"giggling my butt off\",\n\"gmta\": \"great minds think alike\",\n\"h8\": \"hate\",\n\"hagn\": \"have a good night\",\n\"hdop\": \"help delete online predators\",\n\"hhis\": \"hanging head in shame\",\n\"iac\": \"in any case\",\n\"ianal\": \"I am not a lawyer\",\n\"ic\": \"I see\",\n\"idk\": \"I don't know\",\n\"imao\": \"in my arrogant opinion\",\n\"imnsho\": \"in my not so humble opinion\",\n\"imo\": \"in my opinion\",\n\"iow\": \"in other words\",\n\"ipn\": \"I\u2019m posting naked\",\n\"irl\": \"in real life\",\n\"jk\": \"just kidding\",\n\"l8r\": \"later\",\n\"ld\": \"later, dude\",\n\"ldr\": \"long distance relationship\",\n\"llta\": \"lots and lots of thunderous applause\",\n\"lmao\": \"laugh my ass off\",\n\"lmirl\": \"let's meet in real life\",\n\"lol\": \"laugh out loud\",\n\"ltr\": \"longterm relationship\",\n\"lulab\": \"love you like a brother\",\n\"lulas\": \"love you like a sister\",\n\"luv\": \"love\",\n\"m\/f\": \"male or female\",\n\"m8\": \"mate\",\n\"milf\": \"mother I would like to fuck\",\n\"oll\": \"online love\",\n\"omg\": \"oh my god\",\n\"otoh\": \"on the other hand\",\n\"pir\": \"parent in room\",\n\"ppl\": \"people\",\n\"r\": \"are\",\n\"rofl\": \"roll on the floor laughing\",\n\"rpg\": \"role playing games\",\n\"ru\": \"are you\",\n\"shid\": \"slaps head in disgust\",\n\"somy\": \"sick of me yet\",\n\"sot\": \"short of time\",\n\"thanx\": \"thanks\",\n\"thx\": \"thanks\",\n\"ttyl\": \"talk to you later\",\n\"u\": \"you\",\n\"ur\": \"you are\",\n\"uw\": \"you\u2019re welcome\",\n\"wb\": \"welcome back\",\n\"wfm\": \"works for me\",\n\"wibni\": \"wouldn't it be nice if\",\n\"wtf\": \"what the fuck\",\n\"wtg\": \"way to go\",\n\"wtgp\": \"want to go private\",\n\"ym\": \"young man\",\n\"gr8\": \"great\"\n}","a5664e61":"train['processed_text'] = train['processed_text'].apply(lambda x: lookup_dict(x,short_word_dict))\ntest['processed_text'] = test['processed_text'].apply(lambda x: lookup_dict(x,short_word_dict))","de597eb9":"emoticon_dict = {\n\":)\": \"happy\",\n\":\u2011)\": \"happy\",\n\":-]\": \"happy\",\n\":-3\": \"happy\",\n\":->\": \"happy\",\n\"8-)\": \"happy\",\n\":-}\": \"happy\",\n\":o)\": \"happy\",\n\":c)\": \"happy\",\n\":^)\": \"happy\",\n\"=]\": \"happy\",\n\"=)\": \"happy\",\n\"<3\": \"happy\",\n\":-(\": \"sad\",\n\":(\": \"sad\",\n\":c\": \"sad\",\n\":<\": \"sad\",\n\":[\": \"sad\",\n\">:[\": \"sad\",\n\":{\": \"sad\",\n\">:(\": \"sad\",\n\":-c\": \"sad\",\n\":-< \": \"sad\",\n\":-[\": \"sad\",\n\":-||\": \"sad\"\n}\nemoticon_dict","14b307b1":"train['processed_text'] = train['processed_text'].apply(lambda x: lookup_dict(x,emoticon_dict))\ntest['processed_text'] = test['processed_text'].apply(lambda x: lookup_dict(x,emoticon_dict))","29382f9e":"import re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nwords = stopwords.words(\"english\")\n\ntrain['processed_text'] = train['text'].apply(lambda x: \" \".join([stemmer.stem(i) \nfor i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())\n\ntest['processed_text'] = test['text'].apply(lambda x: \" \".join([stemmer.stem(i) \nfor i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())","7ac3398e":"import string\n\n#make all words lower case\ntrain['processed_text'] = train['processed_text'].str.lower()\ntest['processed_text'] = test['processed_text'].str.lower()\n\n#Remove punctuation\ntable = str.maketrans('', '', string.punctuation)\ntrain['processed_text'] = [train['processed_text'][row].translate(table) for row in range(len(train['processed_text']))]\ntest['processed_text'] = [test['processed_text'][row].translate(table) for row in range(len(test['processed_text']))]\n\n# remove hash tags\ntrain['processed_text'] = train['processed_text'].str.replace(\"#\", \" \")\ntest['processed_text'] = test['processed_text'].str.replace(\"#\", \" \")\n\n#remove words less than 1 character\ntrain['processed_text'] = train['processed_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\ntest['processed_text'] = test['processed_text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))","2fac5c8a":"#put frequent words in a mosiac\nfreq_words = ' '.join([text for text in train['processed_text']])\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110, max_words=50).generate(freq_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","10a27116":"from collections import Counter\nfrom itertools import chain\n\n# split words into lists\nv = train['processed_text'].str.split().tolist() \n# compute global word frequency\nc = Counter(chain.from_iterable(v))\n# filter, join, and re-assign\ntrain['processed_text'] = [' '.join([j for j in i if c[j] > 1]) for i in v]\n\n# split words into lists\nv = test['processed_text'].str.split().tolist() \n# compute global word frequency\nc = Counter(chain.from_iterable(v))\n# filter, join, and re-assign\ntest['processed_text'] = [' '.join([j for j in i if c[j] > 1]) for i in v]","6e1255e2":"train","a65f4bd6":"test","68eb920b":"#define x, y and t_test\ny=train.target\nX=train['processed_text']\nX_test=test['processed_text']","5efb7abb":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, test_size=0.10, random_state=42, shuffle=True)","83fe2286":"X_train.shape, X_val.shape, y_train.shape,y_val.shape","d6575052":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer_tfidf = TfidfVectorizer(stop_words='english', max_df=0.7)\ntrain_tfIdf = vectorizer_tfidf.fit_transform(X_train.values.astype('U'))\nval_tfIdf = vectorizer_tfidf.transform(X_val.values.astype('U'))\nX_test_tfIdf = vectorizer_tfidf.transform(X_test.values.astype('U'))\nprint(vectorizer_tfidf.get_feature_names()[:5])","0fbd4f81":"train_tfIdf.shape,  val_tfIdf.shape, X_test_tfIdf.shape","58ab3725":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\n\nmodel=XGBClassifier(max_depth=6, n_estimators=2000)              #MLPClassifier(early_stopping=True)\nmodel.fit(train_tfIdf, y_train)\n#create hyper parameters\npenalty=['l1', 'l2']\nC=np.logspace(0,4,10)\nhyperparameters=dict()   #penalty=penalty, C=C)\n#create grid search\nclf=GridSearchCV(model, hyperparameters, cv=5, verbose=0)\n#fit the model\nbest_model=clf.fit(train_tfIdf, y_train) # training the model\nprint(clf.score(train_tfIdf, y_train))","0250a2fd":"best_model.predict(val_tfIdf)\nprint(clf.score(val_tfIdf, y_val))","9b25d99d":"y_pred=best_model\ny_pred = model.predict_proba(val_tfIdf)\ny_pred = y_pred >= 0.3\ny_pred=y_pred.astype(int)","6a668e7e":"prediction = model.predict_proba(val_tfIdf) # predicting on the validation set\nprediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 then 1 else 0\nprediction_int = prediction_int.astype(np.int)","16b1d0d0":"df=pd.DataFrame({'Actual': y_val, 'Predicted':prediction_int})\ndf","a84b70a5":"test_pred = clf.predict_proba(X_test_tfIdf)\ntest_pred_int = test_pred[:,1] >= 0.3\ntest_pred_int = test_pred_int.astype(np.int)\ntest['target'] = test_pred_int\nsubmission = test[['id','target']]\nsubmission.to_csv('submission.csv', index=False) # writing data to a CSV file\nsubmission","26344718":"Split train set for training and testing","3a4ccbf1":"Preprocessing raw text and getting it ready for machine learning","b500ce75":"Converting Text to Word Frequency Vectors with TfidfVectorizer","318128ae":"Compare tweets in train and test file","06d85622":"Remove rare words","5f5b8cde":"Identify percentage of disaster tweets","868fae25":"Impute any null values","cc2e57f2":"Define X, y and X_test","dde5fa91":"Loads train, test, and sample files","679c8b60":"Import libraries","8a92c66b":"Competition description\n\nTwitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\nBut, it\u2019s not always clear whether a person\u2019s words are actually announcing a disaster.\n\nAcknowledgments\n\nThis dataset was created by the company figure-eight and originally shared on their \u2018Data For Everyone\u2019 website here.","33548525":"Define and train the model"}}