{"cell_type":{"bfb24633":"code","79709407":"code","ee2b630c":"code","99579f0a":"code","7a42446e":"code","d6f952e6":"code","343b1af4":"code","2f0dda9e":"code","345f3b8b":"code","6a530d66":"code","af639350":"code","53289df8":"code","3544d1a8":"code","c69ed922":"code","114ed2e5":"code","a80a6423":"code","94542b09":"code","de5214f0":"code","82b81c5a":"code","6cd24c68":"code","d84e5db6":"code","ba28d1e2":"code","d83b496f":"code","d90989e3":"code","c184b4ad":"code","21f62f8e":"code","5464e8be":"code","ed376c08":"markdown","db4fe2c1":"markdown","87b3ae63":"markdown","5d52b200":"markdown","05981013":"markdown","9bd513aa":"markdown","9857f719":"markdown","3fb5a06b":"markdown","c583d7fb":"markdown","ac811fb4":"markdown","619952ff":"markdown","4887e87e":"markdown","83a9791f":"markdown","b66b20f9":"markdown","4a13d90a":"markdown","a6ac0c28":"markdown","f4d6fd5b":"markdown","811cc918":"markdown","8f6a9f78":"markdown","30c9c9d1":"markdown","790f0963":"markdown","34e27862":"markdown","f1b7bb84":"markdown","1a8e1097":"markdown","73fa1d53":"markdown","8180acb0":"markdown","3e5d603a":"markdown","c9e7617d":"markdown","f5f7713b":"markdown"},"source":{"bfb24633":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","79709407":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib\n\n\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n","ee2b630c":"df = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndf.head()","99579f0a":"df['bmi'].isnull().value_counts()\n","7a42446e":"s = (df.dtypes == 'object')\nobject_cols = list(s[s].index)\n\ns = (df.dtypes != 'object')\nnum_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)\n\nprint(\"Numerical variables:\")\nprint(num_cols)","d6f952e6":"fig=plt.figure(figsize=(20,10),facecolor='white')\ngs=fig.add_gridspec(2,2)\nax=[None for i in range(3)]\nax[0]=fig.add_subplot(gs[0,0])\nax[1]=fig.add_subplot(gs[0,1])\nax[2]=fig.add_subplot(gs[1,0])\n\n\nsns.kdeplot(data=df[df.stroke==1],x='age',ax=ax[0],shade=True,color='lightcoral',alpha=1)\nsns.kdeplot(data=df[df.stroke==0],x='age',ax=ax[0],shade=True,color='palegreen',alpha=0.5)\n\nsns.kdeplot(data=df[df.stroke==1],x='avg_glucose_level',ax=ax[1],shade=True,color='lightcoral',alpha=1)\nsns.kdeplot(data=df[df.stroke==0],x='avg_glucose_level',ax=ax[1],shade=True,color='palegreen',alpha=0.5)\n\nsns.kdeplot(data=df[df.stroke==1],x='bmi',ax=ax[2],shade=True,color='lightcoral',alpha=1)\nsns.kdeplot(data=df[df.stroke==0],x='bmi',ax=ax[2],shade=True,color='palegreen',alpha=0.5)\n\nfor i in range(3):\n    ax[i].set_ylabel('')\n    #ax[i].grid(which='both', axis='y', zorder=0, color='black', linestyle=':', dashes=(2,7))\n    \n    for direction in ['top','right','left']:\n        ax[i].spines[direction].set_visible(False)\n","343b1af4":"df = df.drop('bmi', axis=1)","2f0dda9e":"palettes = ['palegreen','lightcoral']\nfig=plt.figure(figsize=(15,15),facecolor='white')\n\ngs=fig.add_gridspec(4,2)\n\nobject_cols.append('hypertension')\nobject_cols.append('heart_disease')\n\nax=[None for _ in range(len(object_cols))]\n\nax[0]=fig.add_subplot(gs[0,0])\nax[1]=fig.add_subplot(gs[0,1])\nax[2]=fig.add_subplot(gs[1,0])\nax[3]=fig.add_subplot(gs[1,1])\nax[4]=fig.add_subplot(gs[2,0])\nax[5]=fig.add_subplot(gs[2,1])\nax[6]=fig.add_subplot(gs[3,0])\n\n#palette1=[\"wheat\" for _ in range(10)]\n#palette1[0] = \"gold\"\n\nsns.countplot(data=df, hue='stroke', ax=ax[0],x=object_cols[0], palette=palettes, orient='v')\nsns.countplot(data=df, hue='stroke', ax=ax[1],x=object_cols[1], palette=palettes, orient='v')\nsns.countplot(data=df, hue='stroke', ax=ax[2],x=object_cols[2], palette=palettes, orient='v')\nsns.countplot(data=df, hue='stroke', ax=ax[3],x=object_cols[3], palette=palettes, orient='v')\nsns.countplot(data=df, hue='stroke', ax=ax[4],x=object_cols[4], palette=palettes, orient='v')\nsns.countplot(data=df, hue='stroke', ax=ax[5],x=object_cols[5], palette=palettes, orient='v')\nsns.countplot(data=df, hue='stroke', ax=ax[6],x=object_cols[6], palette=palettes, orient='v')\n\nfor i in range(7):\n    ax[i].grid(color='black', linestyle=':', axis='y', zorder=0,  dashes=(5,10))\n    ax[i].set_ylabel('')\n    ax[i].set_xlabel(object_cols[i])\n    \n    for direction in ['top','right','left']:\n        ax[i].spines[direction].set_visible(False)\n\n\n#plt.tight_layout()","345f3b8b":"df = df[df['gender'] != 'Other']","6a530d66":"g = sns.FacetGrid(data=df, row='smoking_status', col='Residence_type', hue='stroke',\n                  size=2.5, aspect=2, palette=palettes)\ng.map(plt.scatter, 'age', 'avg_glucose_level', edgecolor='#EAE0D5', lw=0.2)","af639350":"g = sns.FacetGrid(data=df, row='work_type', col='gender', hue='stroke',\n                  size=2.5, aspect=2, palette=palettes)\ng.map(plt.scatter, 'age', 'avg_glucose_level', edgecolor='#EAE0D5', lw=0.2)","53289df8":"g = sns.FacetGrid(data=df, row='work_type', col='smoking_status', hue='stroke',\n                  size=2.5, aspect=2, palette=palettes)\ng.map(plt.scatter, 'age', 'avg_glucose_level', edgecolor='#EAE0D5', lw=0.2)","3544d1a8":"sns.heatmap(df.corr(), annot=True, fmt='.2f')","c69ed922":"obj_feat = df.dtypes[df.dtypes == 'O'].index.values\nle = LabelEncoder()\n\nfor i in obj_feat:\n    df[i] = le.fit_transform(df[i])\ndf","114ed2e5":"X = df.drop('stroke', axis=1)\ny = df['stroke']\n\nscaler = StandardScaler()\n\nscaler.fit(X)\nX_scaled = scaler.transform(X)\nX = pd.DataFrame(X_scaled, columns=X.columns)\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11)","a80a6423":"X_train.shape, X_test.shape,y_train.shape, y_test.shape","94542b09":"#testing = X_test['id']\n\nX_train = X_train.drop('id', axis=1)\nX_test = X_test.drop('id', axis=1)\n\nX_train.shape, X_test.shape","de5214f0":"all_model = [LogisticRegression(), KNeighborsClassifier(), DecisionTreeClassifier(),\n            RandomForestClassifier(), BernoulliNB(), SVC()]\n\nrecall = []\nprecision = []\nf1=[]\nfor model in all_model:\n    cv = cross_val_score(model, X_train, y_train, scoring='recall', cv=10).mean()\n    recall.append(cv)\n    \n    cv = cross_val_score(model, X_train, y_train, scoring='precision', cv=10).mean()\n    precision.append(cv)\n    \n    cv = cross_val_score(model, X_train, y_train, scoring='f1', cv=10).mean()\n    f1.append(cv)\n\nmodel = ['LogisticRegression', 'KNeighborsClassifier', 'DecisionTreeClassifier',\n         'RandomForestClassifier', 'BernoulliNB', 'SVC']\n\nscore = pd.DataFrame({'Model': model, 'Precision': precision, 'Recall': recall, 'F1':f1})\nscore.style.background_gradient(high=1,axis=0)","82b81c5a":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE()\nX_bal, Y_bal = smote.fit_resample(X, y)\nX_train, X_test, y_train, y_test = train_test_split(X_bal, Y_bal, test_size=0.2, random_state=11)","6cd24c68":"testing = X_test['id']\n\nX_train = X_train.drop('id', axis=1)\nX_test = X_test.drop('id', axis=1)\n\nX_train.shape, X_test.shape","d84e5db6":"all_model = [LogisticRegression(), KNeighborsClassifier(), DecisionTreeClassifier(),\n            RandomForestClassifier(), BernoulliNB(), SVC()]\n\nrecall = []\nprecision = []\nf1=[]\nbalanced_accuracy=[]\n\n\nfor model in all_model:\n    cv = cross_val_score(model, X_train, y_train, scoring='recall', cv=10).mean()\n    recall.append(cv)\n    \n    cv = cross_val_score(model, X_train, y_train, scoring='precision', cv=10).mean()\n    precision.append(cv)\n    \n    cv = cross_val_score(model, X_train, y_train, scoring='f1', cv=10).mean()\n    f1.append(cv)\n    \n    cv = cross_val_score(model, X_train, y_train, scoring='balanced_accuracy', cv=10).mean()\n    balanced_accuracy.append(cv)\n\nmodel = ['LogisticRegression', 'KNeighborsClassifier', 'DecisionTreeClassifier',\n         'RandomForestClassifier', 'BernoulliNB', 'SVC']\n\nscore = pd.DataFrame({'Model': model, 'Precision': precision, 'Recall': recall, 'F1':f1, 'balanced_accuracy':balanced_accuracy})\nscore.style.background_gradient(high=1,axis=0)","ba28d1e2":"model_1 = RandomForestClassifier(random_state=1)\n\n# Train the model (will take about 10 minutes to run)\nmodel_1.fit(X_train, y_train)\npred = model_1.predict(X_test)\n\n#print(confusion_matrix(y_test, pred, labels=(1,0)))\nprint(classification_report(y_test, pred))","d83b496f":"from xgboost import XGBClassifier\nmodel_2 = XGBClassifier(n_estimators=1000,learning_rate=0.05) # Your code here\n\n# Fit the model\nmodel_2.fit(X_train, y_train, \n             early_stopping_rounds=20, \n             eval_set=[(X_test, y_test)], \n             verbose=False) # Your code here\n\n# Get predictions\npredictions_2 = model_2.predict(X_test) # Your code here\n\nprint(classification_report(y_test, predictions_2))","d90989e3":"from lightgbm import LGBMClassifier\n\nlgbm_parameters = {\n    'metric': 'f1', \n    'n_jobs': -1,\n    'n_estimators': 50000,\n    'reg_alpha': 10.924491968127692,\n    'reg_lambda': 17.396730654687218,\n    'colsample_bytree': 0.21497646795452627,\n    'subsample': 0.7582562557431147,\n    'learning_rate': 0.009985133666265425,\n    'max_depth': 20,\n    'num_leaves': 65,\n    'min_child_samples': 27,\n    'max_bin': 523,\n    'cat_l2': 0.025083670064082797\n}\n\nlgbm_model = LGBMClassifier(**lgbm_parameters)\n# Fit the model\nlgbm_model.fit(X_train, y_train, \n             eval_set=[(X_test, y_test)], \n             verbose=False) # Your code here\n\n# Get predictions\npredictions_3 = lgbm_model.predict(X_test) \n\nprint(classification_report(y_test, predictions_3))","c184b4ad":"fig=plt.figure(figsize=(17,5),facecolor='white')\ngs=fig.add_gridspec(1,3)\nax=[None for i in range(3)]\nax[0]=fig.add_subplot(gs[0,0])\nax[1]=fig.add_subplot(gs[0,1])\nax[2]=fig.add_subplot(gs[0,2])\n\ncm_XGBoost = confusion_matrix(y_test,predictions_2)\nsns.heatmap(cm_XGBoost, annot=True, ax = ax[0], fmt = 'g' ,cmap=plt.cm.Reds)\ncm_RF = confusion_matrix(y_test,pred)\nsns.heatmap(cm_RF, annot=True, ax = ax[1], fmt = 'g' ,cmap=plt.cm.Blues)\ncm_LGBM = confusion_matrix(y_test,predictions_3)\nsns.heatmap(cm_LGBM, annot=True, ax = ax[2], fmt = 'g' ,cmap=plt.cm.Greens)\ntitle=['XG_Boost','RF','LGBM']\n\nfor i in range(3):\n    #ax[i].grid(color='black', linestyle=':', axis='y', zorder=0,  dashes=(5,10))\n    ax[i].set_xlabel('Predicted label')\n    ax[i].set_ylabel('Actual label')\n    #ax[i].text(title[i])\n    for direction in ['top','right','left']:\n        ax[i].spines[direction].set_visible(False)\n","21f62f8e":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import roc_curve\n\npred_prob1 = model_1.predict_proba(X_test)\npred_prob2 = model_2.predict_proba(X_test)\n\nfpr1, tpr1, thresh1 = roc_curve(y_test, pred_prob1[:,1], pos_label=1)\nfpr2, tpr2, thresh2 = roc_curve(y_test, pred_prob2[:,1], pos_label=1)\n\nrandom_probs = [0 for i in range(len(y_test))]\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)\n\n\nauc_score1 = roc_auc_score(y_test, pred_prob1[:,1])\nauc_score2 = roc_auc_score(y_test, pred_prob2[:,1])\n\nprint('AUC score for RF and XGBoost',auc_score1, auc_score2)\n\nplt.figure(figsize=(10,7)) #to set the size of the figure generated\n\nplt.plot(fpr1, tpr1,color='yellow', label='RandomForest')\nplt.plot(fpr2, tpr2,color='green', label='XGBoost')\n\nplt.xlabel('False Positive Rate',fontsize=18,labelpad =10) #Label for x axis\nplt.ylabel('True Positive Rate',fontsize=18) #Label for y axis\n\nplt.title('Receiver Operating Characteristic',fontsize=22).set_position([.5, 1.02]) #Plot title\nplt.legend(loc=\"lower right\",fontsize=13)\nplt.show()","5464e8be":"print(\"Accuracy of final XGBoost model :\",accuracy_score(y_test,predictions_2)*100)","ed376c08":"Obervation: \n\nWe have both numerical type as well as categorical type variables.\nSince model takes numbers as input, we will have to encode the categorical features later on in thr pre-processing step before feeding it to the model.","db4fe2c1":"**2. Standardizing and splitting**","87b3ae63":"Hi!\n\nIn this notebook, you'll learn how to do a complete ML Project from scratch.\nIf you are a beginner, this notebook will take you through all the steps that are required to execute a successful ML project.\n\nThis notebook will take you through:\n1. Understanding the Task of stroke prediction\n2. How to Import Relevant libraries\n3. Exploring dataset with beautiful vizualizations\n4. Identifying challenges in data - missing features, imbalanced dataset etc.\n5. Pre-processing of data \n6. Handling data Imbalance with SMOTE Technique\n7. Training multiple classifiers - Logistic Regression, KNN, Decision Tree, RF, Bernoulli Naive Bayes, Support Vectors, XG Boost, LGBM.\n8. K-Fold cross validation to observe model performance\n9. Evaluate models at different metrices - Precision, Recall, F1_score, balances_accuracy, AUC_ROC etc.\n10. Confusion matrix of different models\n11. Finally, select the best model and predict on Test set.\n\nLet's Start!!\n","5d52b200":"XGBoost performed best amongst all the baselines with 96.5% accuracy.","05981013":"**We can see the difference in learning with proper training. Random Forest outperform with clear win! Let's train RF model and evaluate it's performance on test data**","9bd513aa":"# Plotting Confusion Matrix of RF, XGB, LGBM","9857f719":"Light GBM classifier didn't outperform XGBoost.","3fb5a06b":"BMI has 4% missing values","c583d7fb":"Observation:\n\n1. Majority of people who had stroke were working in 'Private' sector","ac811fb4":"Observations:\n\n1. People with age 65-85 have high chances of getting stroke. \n2. bmi can't distinguish stroke patterns and also have 4% missing values, hence I will drop this feature.","619952ff":"Okay! So it's F1-score is 0.95 on test set.\nLet's try to improve it with boosting models.","4887e87e":"# Imbalanced data handling - SMOTE","83a9791f":"**Trying Boosting Ensemble method**","b66b20f9":"Interesting Observations: \n\n1. Only 2 children have strokes and both are female\n2. Older females in govt_jobs have higher risk of stroke.","4a13d90a":"# Data Preprocessing","a6ac0c28":"Observation:\n\n1. Type of Residence either Urban or Rural has no effect on having stroke. This feature can also be dropped.","f4d6fd5b":"# Please upvote if you find this useful :)","811cc918":"**3. Some pairwise Analysis**","8f6a9f78":"Observation: \n\n1. We see that in every feature, there are higher samples of no stroke (stroke=0) as compared to the other class. Hence it is a Highly Imbalanced dataset\n2. Others category in 'gender' can be ignored","30c9c9d1":"**2. Checking the distribution of Categorical variables**","790f0963":"**1. Encoding categorical variables**","34e27862":"# Training multiple models without correcting data Imbalance issue","f1b7bb84":"**1. Check for Numerical and categorical features**","1a8e1097":"**AIM - Given set of features for a patient, predict chances of stroke.**","73fa1d53":"We can see how poor the model performs without training it with enough samples from both classes.","8180acb0":"# 2. Import Relevant libraries","3e5d603a":"Performance improved by XGBoost!!!","c9e7617d":"**b. Bivariate Analysis of numerical features**\n","f5f7713b":"# 3. Exploring dataset with beautiful vizualizations"}}