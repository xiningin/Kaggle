{"cell_type":{"55669869":"code","20624a1e":"code","3ff564f1":"code","62171a38":"code","d4a2a38b":"code","b11674d9":"code","a4a1e97c":"code","9a40da6e":"code","69031d0d":"code","762e31fc":"code","934108bd":"code","4ba55d73":"code","6531c5eb":"code","572cb404":"markdown","f79ba44d":"markdown","55125072":"markdown","47a827a0":"markdown","a43ca3e7":"markdown","3ee99bad":"markdown","0cff3570":"markdown","b81313ed":"markdown","082d8e22":"markdown","aedaefe8":"markdown","07e6808e":"markdown","8963139c":"markdown","8208ec32":"markdown","eb0dfdde":"markdown","6327f0fe":"markdown"},"source":{"55669869":"#Librer\u00edas \nimport numpy as np \nimport pandas as pd \nimport torch\nfrom torchvision import datasets\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nfrom torch import optim\n\n# Utilizada para la divisi\u00f3n de la data del training set\nfrom sklearn.model_selection import train_test_split\n\n# Librer\u00edas para Gr\u00e1ficos\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Random Seed para resultados predecibles\nnp.random.seed(1)","20624a1e":"# Import de Data\ntraining_data = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\n\nprint('\u2192 Train Shape:', training_data.shape)\nprint('\u2192 Test Shape:', test_data.shape)        ","3ff564f1":"# Separaci\u00f3n de im\u00e1genes y sus respectivos labels del Training Set, conversi\u00f3n en Array\ndif_x = np.array(training_data.drop('label',axis=1), dtype=np.float)\ny = np.array(training_data['label'])\nx = np.reshape(dif_x, newshape = (dif_x.shape[0],1,28,28))\n\n# Rearreglo del Test Set en un Array\ndif_x_test = np.array(test_data, dtype=np.float)\nx_test = np.reshape(dif_x_test, newshape = (dif_x_test.shape[0],1,28,28))\n\nprint('\u2192 Train Array Shape:', x.shape)\nprint('\u2192 Test Array Shape:',x_test.shape)         ","62171a38":"# Conversi\u00f3n de las im\u00e1genes y las etiquetas en Tensors de Pytorch del Training Set\n\n# Normalizaci\u00f3n de Datos divididos entre 255\nX = torch.from_numpy(x).type(torch.FloatTensor)\/255\nY = torch.from_numpy(y)\n\n# Conversi\u00f3n del Test Set a Tensor\nX_test = torch.from_numpy(x_test).type(torch.FloatTensor)\/255\ntemp = np.zeros(x_test.shape)\nY_test = torch.from_numpy(temp).type(torch.FloatTensor)\n\n# Separaci\u00f3n de los datos en Train y Validation Sets (80% - 20%)\nX_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.20)\n\n# Creaci\u00f3n del Dataset de Train, Valudation y Test\ntrain = torch.utils.data.TensorDataset(X_train, Y_train)\nvalidation = torch.utils.data.TensorDataset(X_val, Y_val)\ntest = torch.utils.data.TensorDataset(X_test, Y_test)\n\n# Creaci\u00f3n de los DataLoaders\ntrain_loader = torch.utils.data.DataLoader(train, batch_size = 32, shuffle = False)\nval_loader = torch.utils.data.DataLoader(validation, batch_size = 32, shuffle = False)\ntest_loader = torch.utils.data.DataLoader(test, batch_size= 32, shuffle = False)\n\nprint('\u2192 Train Length:', len(train))\nprint('\u2192 Validation Length:', len(validation))\nprint('\u2192 Test Length:', len(test))     ","d4a2a38b":"# Obtenci\u00f3n de 1 Batch para graficar ejemplos del Training Set\ndataiter = iter(train_loader)\nimages_train, labels_train = dataiter.next()\nimg = images_train.numpy()\n\n# Impresi\u00f3n del primer ejemplo\n\nfig = plt.figure(figsize = (25,5)) \nax = fig.add_subplot(111)\nax.imshow(np.squeeze(img[0]), cmap='gray')","b11674d9":"class coolNet(nn.Module):\n    def __init__(self):\n        super(coolNet, self).__init__()\n        \n        # Convolutional Layers\n        \n        # Convolutional Layer 1 (1 a 6 channels) con Xavier Normal Initialization y Batch Normalization (Capa 1 de la red)\n        self.conv1 = nn.Conv2d(1,6, kernel_size=5, padding = 2) \n        torch.nn.init.xavier_normal_(self.conv1.weight)\n        torch.nn.init.zeros_(self.conv1.bias)\n        self.conv1_bn = nn.BatchNorm2d(6)\n        \n        # Convolutional Layer 2 (6 a 16 channels) con Batch Normalization (Capa 2 de la red)\n        self.conv2 = nn.Conv2d(6, 16, kernel_size=5) # layer2\n        self.conv2_bn = nn.BatchNorm2d(16)\n        \n        #Linear Layers\n        \n        # Linear Layer 1 (400 a 120) con Batch Normalization (Capa 3 de la red)\n        self.fc1 = nn.Linear(16*5*5,120)\n        self.fc1_bn = nn.BatchNorm1d(120)\n        \n        # Linear Layer 2 (120 a 84) con Batch Normalization (Capa 4 de la red)\n        self.fc2 = nn.Linear(120, 84) \n        self.fc2_bn = nn.BatchNorm1d(84)\n        \n        # Output Layer 3 (84 a 10) con Batch Normalization (Capa 5 de la red)\n        self.fc3 = nn.Linear(84, 10) \n        self.fc3_bn = nn.BatchNorm1d(10)\n        \n        # MaxPool Layer 2x2\n        self.maxPool = nn.MaxPool2d((2,2)) \n        \n        # Dropout con 0.2 \n        self.dropout = nn.Dropout(p=0.2)\n    \n    # Forward Propagation\n    def forward(self, x):\n    \n        x = self.maxPool(F.relu(self.conv1_bn(self.conv1(x))))\n        x = self.dropout(x)\n        \n        x = self.maxPool(F.relu(self.conv2_bn(self.conv2(x)))) \n        x = self.dropout(x)\n        \n        # Flatten Tensor\n        x = x.view(-1, 16*5*5)\n        \n        x = F.relu(self.fc1_bn(self.fc1(x)))\n        x = self.dropout(x)\n        \n        x = F.relu(self.fc2_bn(self.fc2(x)))\n        x = self.dropout(x)\n        \n        # Output y Clasificaci\u00f3n\n        x = F.log_softmax(self.fc3_bn(self.fc3(x)), dim=1)\n        \n        return x\n        ","a4a1e97c":"# Definici\u00f3n del Modelo\nmodel = coolNet()\nprint(model)","9a40da6e":"# Definici\u00f3n de la Funci\u00f3n de P\u00e9rdida\ncriterion = nn.CrossEntropyLoss()\n\n# Definici\u00f3n del Optimizador con learning rate = 0.003\noptimizer = optim.Adam(model.parameters(), lr=0.003)\n\n# N\u00famero de Iteraciones\nepochs = 50\n\ntraining_loss, validation_loss = [],[]\ntraining_acc, validation_acc = [],[]\n\nfor epoch in range(epochs):\n    \n    # Inicializaci\u00f3n de M\u00e9tricas\n    tloss = 0.0\n    vloss = 0.0\n    total_val = 0\n    correct_val = 0\n    total_train = 0\n    correct_train = 0\n    train_accuracy = 0.0\n    validation_accuracy =0.0\n    \n    # ---- Entrenamiento -----\n    model.train()\n    for images, labels in train_loader:\n    \n        # Inicializar Gradientes en 0\n        optimizer.zero_grad()\n\n        # Obtenci\u00f3n de Outputs\n        outputs = model(images)\n      \n        # C\u00e1lculo de P\u00e9rdida\n        loss = criterion(outputs, labels)\n\n        # Backward Pass\n        loss.backward()\n\n        # Actualizaci\u00f3n de Par\u00e1metros\n        optimizer.step()\n\n        # Actualizaci\u00f3n de funci\u00f3n de p\u00e9rdida\n        tloss += loss.item()*images.size(0)\n        \n        # C\u00e1lculo de Accuracy\n        _, predicted_train = torch.max(outputs,1)\n        total_train += labels.size(0)\n        correct_train += (predicted_train == labels).sum()\n    \n    # ---- Validaci\u00f3n ----\n    model.eval()\n    for images, labels in val_loader:\n    \n        # Obtenci\u00f3n de Outputs\n        outputs = model(images)\n\n        # C\u00e1lculo de P\u00e9rdida\n        loss = criterion(outputs, labels)\n\n        # Backward Pass\n        loss.backward()\n\n        # Actualizaci\u00f3n de funci\u00f3n de p\u00e9rdida\n        vloss += loss.item()*images.size(0)\n        \n        # C\u00e1lculo de Accuracy del Validation Set\n        _, predicted_val = torch.max(outputs,1)\n        total_val += labels.size(0)\n        correct_val += (predicted_val == labels).sum()\n    \n    # M\u00e9tricas del Modelo\n    \n    # P\u00e9rdidas de ambos sets\n    tloss = tloss\/len(train_loader)\n    vloss = vloss\/len(val_loader)\n    training_loss.append(tloss)\n    validation_loss.append(vloss)\n    \n    #C\u00e1lculo del Accuracy del Set\n    train_accuracy = correct_train.item() \/ total_train\n    validation_accuracy = correct_val.item() \/ total_val\n    \n    training_acc.append(train_accuracy)\n    validation_acc.append(validation_accuracy)\n    \n    \n    print('\u2192Epoch: {}\/{} \\t \u2192Training Loss: {:.6f} \\t \u2192Validation Loss: {:.6f} \\t \u2192Train Accuracy: {:.6f} \\t \u2192Validation Accuracy: {:.6f}'.format(\n        epoch+1,epochs,tloss,vloss,train_accuracy,validation_accuracy))\n    ","69031d0d":"plt.plot(training_loss, label='Training Loss')\nplt.plot(validation_loss, label='Validation Loss')\nplt.legend()\nplt.ylabel('P\u00e9rdidas')\nplt.xlabel('Epoch')\nplt.title('Comparaci\u00f3n de P\u00e9rdidas')\nplt.show()","762e31fc":"\nplt.plot(training_acc, label='Training Accuracy')\nplt.plot(validation_acc, label='Validation Accuracy')\nplt.legend()\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.title('Comparaci\u00f3n de Accuracy')\nplt.show()","934108bd":"# Obtenci\u00f3n de 1 Batch para graficar ejemplos del Test Set\n\ndataitertest = iter(test_loader)\nimages_test,labels_test = dataitertest.next()\nimgtest = images_test.numpy()\n\n# Impresi\u00f3n del primer ejemplo\n\nfig = plt.figure(figsize = (25,5)) \nax = fig.add_subplot(111)\nax.imshow(np.squeeze(imgtest[0]), cmap='gray')","4ba55d73":"# Objetos para almacenar las predicciones\nImageId, Label = [],[]\n\n# ---- Predicciones ----\nfor images,_ in test_loader:\n    \n    outputs = model(images)\n    _, predicted_test = torch.max(outputs,1)\n    \n    for i in range(len(predicted_test)):        \n        ImageId.append(len(ImageId)+1)\n        Label.append(predicted_test[i].numpy())\n    \nsub = pd.DataFrame(data={'ImageId':ImageId, 'Label':Label})\nsub.describe","6531c5eb":"# Escritura del archivo CSV\nsub.to_csv(\"submission.csv\", index=False)\nsub.head()","572cb404":"Inicialmente, se cargan los set de data predeterminados de MINST (Train y Test) para luego realizar la separaci\u00f3n de los primeros en im\u00e1genes (x) y labels (y). Posteriormente se convierten en Tensors acorde a Pytorch y se crea el respectivo database para trabajar con ellos m\u00e1s adelante.","f79ba44d":"### **Submission al MNIST Competition**","55125072":"### **Import de Training y Test Set**","47a827a0":"## **Pre-procesamiento de Datos**","a43ca3e7":"### **Impresi\u00f3n de Estructura de la Red**","3ee99bad":"# Proyecto 1 \nEste notebook forma parte del Proyecto I de la materia Computaci\u00f3n Emergente de la Universidad Metropolitana. Participa en la competici\u00f3n MINST de Kaggle. Realizado por Gina Cuadrado y Sabrina Garc\u00eda.\n","0cff3570":"### **Gr\u00e1fico de las P\u00e9rdidas**","b81313ed":"## **Arquitectura del modelo**\n\nSe emplear\u00e1 una arquitectura basada en LeNet5 en cuanto al n\u00famero de capas y n\u00famero de neuronas, ya que se considera que dicha arquitectura es la m\u00e1s utilizada en la identificaci\u00f3n y reconocimiento de n\u00fameros. LeNet5 utiliza 2 capas convolucionales con MaxPool y 3 capas lineales que finalizan con un output de clasificaci\u00f3n entre los 10 posibles n\u00fameros que debe identificar la red. Se utilizaron tambi\u00e9n los siguientes elementos:\n* **Xavier Normal Initialization**: para la inicializaci\u00f3n de los pesos y los biases. Se utiliza para que los valores se mantengan estables y no se eleven a 1 o se reduzcan hasta 0.\n* **Batch Normalization**: utilizado en todas las capas de la red (convolucionales y lineales) para ayudar en la convergencia de los gradientes.\n* **Regularizaci\u00f3n Dropout**: de acuerdo a una probabilidad de descarte (0.2), son omitidas algunas de las neuronas de cada una de las capas con el fin de que la red no memorice los datos que all\u00ed se contienen y obtener una mejor regularizaci\u00f3n.\n","082d8e22":"Se observar\u00e1 uno de los ejemplos contenidos en el Training Set para verificar que las dimensiones utilizadas anteriormente son correctas.","aedaefe8":"## **Entrenamiento del Modelo**","07e6808e":"## **Predicciones Finales**","8963139c":"### **Conversi\u00f3n en Arrays**","8208ec32":"### **Gr\u00e1fico de los Accuracy**","eb0dfdde":"## **Visualizaci\u00f3n de Datos**","6327f0fe":"### **Conversi\u00f3n en Tensors y Data Augmentation**\nSe convirtieron todos los datos ingresados en tensores para poder trabajar en Pytorch y se realizaron algunas t\u00e9cnicas de Data Augmentation para mejorar los resultados considerando que el dataset no es tan grande."}}