{"cell_type":{"c01afc8c":"code","ce84cae1":"code","9cd2def5":"code","e3887840":"code","f1429041":"code","79ec296b":"code","7ea5c874":"code","cb82d3af":"code","3cd2c8e9":"code","8cb4792c":"code","5e54a72f":"code","a01f783e":"code","16b49260":"code","20478404":"code","5c29d21d":"code","3a85a003":"code","146fb3dd":"code","dbc35c18":"code","02cebbef":"code","ceb75ebe":"code","4a569f58":"code","f5d330aa":"code","15128a10":"code","a631e1e6":"code","0890ba5b":"code","1b2d0898":"code","572dd54f":"code","303f860f":"code","3e2ce0bd":"code","e55a6fdd":"code","a54fbe13":"code","0893b356":"code","afe1bc1a":"code","131ef324":"code","5c4be527":"code","9fa7fb03":"code","da8fca4a":"code","a805ec08":"code","e210396e":"code","cf50a6b6":"code","df7deff9":"code","835f1453":"code","6a67f185":"code","3b12ce5d":"code","45fcf764":"code","a20eefbe":"code","577e1058":"code","e775985d":"code","486e1f87":"code","99266fcc":"markdown","5191eb19":"markdown","5fd0c648":"markdown","919db055":"markdown","ce3c224c":"markdown","09d6d403":"markdown","f0acc412":"markdown","81e1e4e9":"markdown","1e47d1f0":"markdown","9cd0ac7e":"markdown","ef305529":"markdown","b1c28762":"markdown","35216300":"markdown","e55d352d":"markdown","9854c74a":"markdown","2c66e124":"markdown","d6be54c2":"markdown","3a75fd39":"markdown","d84c08f4":"markdown","1ff40066":"markdown","b14f0886":"markdown","cfbc1e94":"markdown","26caf8e9":"markdown","9e7322dc":"markdown","0cd2f1a9":"markdown","d0f079e1":"markdown","88cc9a88":"markdown","faf615d1":"markdown","241d8d03":"markdown","bc7f52fe":"markdown","2fb69df9":"markdown","d9fb28a8":"markdown","33b0b49c":"markdown","ac8ffaeb":"markdown","fbd0fc0e":"markdown","8c581c56":"markdown","7c56eb8d":"markdown","12921afd":"markdown","548dd380":"markdown","c34cf074":"markdown","7bdfab09":"markdown","b93c0ab9":"markdown","2bc6696a":"markdown","0520472f":"markdown","f5874056":"markdown","6c4fc599":"markdown","2210580f":"markdown","33b8de12":"markdown","dd57d52a":"markdown","c8eed3f2":"markdown","47587110":"markdown","93d65deb":"markdown","2734100d":"markdown","8cf60e41":"markdown","0207eb90":"markdown","b6a0d331":"markdown","8aba5b47":"markdown","035db608":"markdown","91cb56e6":"markdown","60580c72":"markdown","9be60106":"markdown","4ff2c486":"markdown","68880199":"markdown","59c88fb2":"markdown","e38ee0b8":"markdown","38c5fc24":"markdown","cb49b48b":"markdown","fd8c058d":"markdown","2cea75ed":"markdown","e7ec0e74":"markdown"},"source":{"c01afc8c":"import pandas as pd \nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n\n\nimport gc\nfrom datetime import datetime \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn import svm\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nimport xgboost as xgb\n\npd.set_option('display.max_columns', 100)\n\n\nRFC_METRIC = 'gini'  #metric used for RandomForrestClassifier\nNUM_ESTIMATORS = 100 #number of estimators used for RandomForrestClassifier\nNO_JOBS = 4 #number of parallel jobs used for RandomForrestClassifier\n\n\n#TRAIN\/VALIDATION\/TEST SPLIT\n#VALIDATION\nVALID_SIZE = 0.20 # simple validation using train_test_split\nTEST_SIZE = 0.20 # test size using_train_test_split\n\n#CROSS-VALIDATION\nNUMBER_KFOLDS = 5 #number of KFolds for cross-validation\n\n\n\nRANDOM_STATE = 2018\n\nMAX_ROUNDS = 1000 #lgb iterations\nEARLY_STOP = 50 #lgb early stop \nOPT_ROUNDS = 1000  #To be adjusted based on best validation rounds\nVERBOSE_EVAL = 50 #Print out metric result\n\nIS_LOCAL = False\n\nimport os\n\nif(IS_LOCAL):\n    PATH=\"..\/input\/credit-card-fraud-detection\"\nelse:\n    PATH=\"..\/input\"\nprint(os.listdir(PATH))","ce84cae1":"data_df = pd.read_csv(PATH+\"\/creditcard.csv\")","9cd2def5":"print(\"Credit Card Fraud Detection data -  rows:\",data_df.shape[0],\" columns:\", data_df.shape[1])","e3887840":"data_df.head()","f1429041":"data_df.describe()","79ec296b":"total = data_df.isnull().sum().sort_values(ascending = False)\npercent = (data_df.isnull().sum()\/data_df.isnull().count()*100).sort_values(ascending = False)\npd.concat([total, percent], axis=1, keys=['Total', 'Percent']).transpose()","7ea5c874":"temp = data_df[\"Class\"].value_counts()\ndf = pd.DataFrame({'Class': temp.index,'values': temp.values})\n\ntrace = go.Bar(\n    x = df['Class'],y = df['values'],\n    name=\"Credit Card Fraud Class - data unbalance (Not fraud = 0, Fraud = 1)\",\n    marker=dict(color=\"Red\"),\n    text=df['values']\n)\ndata = [trace]\nlayout = dict(title = 'Credit Card Fraud Class - data unbalance (Not fraud = 0, Fraud = 1)',\n          xaxis = dict(title = 'Class', showticklabels=True), \n          yaxis = dict(title = 'Number of transactions'),\n          hovermode = 'closest',width=600\n         )\nfig = dict(data=data, layout=layout)\niplot(fig, filename='class')","cb82d3af":"class_0 = data_df.loc[data_df['Class'] == 0][\"Time\"]\nclass_1 = data_df.loc[data_df['Class'] == 1][\"Time\"]\n#plt.figure(figsize = (14,4))\n#plt.title('Credit Card Transactions Time Density Plot')\n#sns.set_color_codes(\"pastel\")\n#sns.distplot(class_0,kde=True,bins=480)\n#sns.distplot(class_1,kde=True,bins=480)\n#plt.show()\nhist_data = [class_0, class_1]\ngroup_labels = ['Not Fraud', 'Fraud']\n\nfig = ff.create_distplot(hist_data, group_labels, show_hist=False, show_rug=False)\nfig['layout'].update(title='Credit Card Transactions Time Density Plot', xaxis=dict(title='Time [s]'))\niplot(fig, filename='dist_only')","3cd2c8e9":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))\ns = sns.boxplot(ax = ax1, x=\"Class\", y=\"Amount\", hue=\"Class\",data=data_df, palette=\"PRGn\",showfliers=True)\ns = sns.boxplot(ax = ax2, x=\"Class\", y=\"Amount\", hue=\"Class\",data=data_df, palette=\"PRGn\",showfliers=False)\nplt.show();","8cb4792c":"tmp = data_df[['Amount','Class']].copy()\nclass_0 = tmp.loc[tmp['Class'] == 0]['Amount']\nclass_1 = tmp.loc[tmp['Class'] == 1]['Amount']\nclass_0.describe()","5e54a72f":"class_1.describe()","a01f783e":"fraud = data_df.loc[data_df['Class'] == 1]\n\ntrace = go.Scatter(\n    x = fraud['Time'],y = fraud['Amount'],\n    name=\"Amount\",\n     marker=dict(\n                color='rgb(238,23,11)',\n                line=dict(\n                    color='red',\n                    width=1),\n                opacity=0.5,\n            ),\n    text= fraud['Amount'],\n    mode = \"markers\"\n)\ndata = [trace]\nlayout = dict(title = 'Amount of fraudulent transactions',\n          xaxis = dict(title = 'Time [s]', showticklabels=True), \n          yaxis = dict(title = 'Amount'),\n          hovermode='closest'\n         )\nfig = dict(data=data, layout=layout)\niplot(fig, filename='fraud-amount')","16b49260":"plt.figure(figsize = (14,14))\nplt.title('Credit Card Transactions features correlation plot (Pearson)')\ncorr = data_df.corr()\nsns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns,linewidths=.1,cmap=\"Reds\")\nplt.show()","20478404":"s = sns.lmplot(x='V20', y='Amount',data=data_df, hue='Class', fit_reg=True,scatter_kws={'s':2})\ns = sns.lmplot(x='V7', y='Amount',data=data_df, hue='Class', fit_reg=True,scatter_kws={'s':2})\nplt.show()","5c29d21d":"s = sns.lmplot(x='V2', y='Amount',data=data_df, hue='Class', fit_reg=True,scatter_kws={'s':2})\ns = sns.lmplot(x='V5', y='Amount',data=data_df, hue='Class', fit_reg=True,scatter_kws={'s':2})\nplt.show()","3a85a003":"var = data_df.columns.values\n\ni = 0\nt0 = data_df.loc[data_df['Class'] == 0]\nt1 = data_df.loc[data_df['Class'] == 1]\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(8,4,figsize=(16,28))\n\nfor feature in var:\n    i += 1\n    plt.subplot(8,4,i)\n    sns.kdeplot(t0[feature], bw=0.5,label=\"Class = 0\")\n    sns.kdeplot(t1[feature], bw=0.5,label=\"Class = 1\")\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","146fb3dd":"target = 'Class'\npredictors = ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\\\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19',\\\n       'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28',\\\n       'Amount']","dbc35c18":"train_df, test_df = train_test_split(data_df, test_size=TEST_SIZE, random_state=RANDOM_STATE, shuffle=True )\ntrain_df, valid_df = train_test_split(train_df, test_size=VALID_SIZE, random_state=RANDOM_STATE, shuffle=True )","02cebbef":"clf = RandomForestClassifier(n_jobs=NO_JOBS, \n                             random_state=RANDOM_STATE,\n                             criterion=RFC_METRIC,\n                             n_estimators=NUM_ESTIMATORS,\n                             verbose=False)","ceb75ebe":"clf.fit(train_df[predictors], train_df[target].values)","4a569f58":"preds = clf.predict(valid_df[predictors])","f5d330aa":"tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': clf.feature_importances_})\ntmp = tmp.sort_values(by='Feature importance',ascending=False)\nplt.figure(figsize = (7,4))\nplt.title('Features importance',fontsize=14)\ns = sns.barplot(x='Feature',y='Feature importance',data=tmp)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)\nplt.show()   \n","15128a10":"cm = pd.crosstab(valid_df[target].values, preds, rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\nsns.heatmap(cm, \n            xticklabels=['Not Fraud', 'Fraud'],\n            yticklabels=['Not Fraud', 'Fraud'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\nplt.title('Confusion Matrix', fontsize=14)\nplt.show()","a631e1e6":"roc_auc_score(valid_df[target].values, preds)","0890ba5b":"clf = AdaBoostClassifier(random_state=RANDOM_STATE,\n                         algorithm='SAMME.R',\n                         learning_rate=0.8,\n                             n_estimators=NUM_ESTIMATORS)","1b2d0898":"clf.fit(train_df[predictors], train_df[target].values)","572dd54f":"preds = clf.predict(valid_df[predictors])","303f860f":"tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': clf.feature_importances_})\ntmp = tmp.sort_values(by='Feature importance',ascending=False)\nplt.figure(figsize = (7,4))\nplt.title('Features importance',fontsize=14)\ns = sns.barplot(x='Feature',y='Feature importance',data=tmp)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)\nplt.show()   ","3e2ce0bd":"cm = pd.crosstab(valid_df[target].values, preds, rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\nsns.heatmap(cm, \n            xticklabels=['Not Fraud', 'Fraud'],\n            yticklabels=['Not Fraud', 'Fraud'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\nplt.title('Confusion Matrix', fontsize=14)\nplt.show()","e55a6fdd":"roc_auc_score(valid_df[target].values, preds)","a54fbe13":"clf = CatBoostClassifier(iterations=500,\n                             learning_rate=0.02,\n                             depth=12,\n                             eval_metric='AUC',\n                             random_seed = RANDOM_STATE,\n                             bagging_temperature = 0.2,\n                             od_type='Iter',\n                             metric_period = VERBOSE_EVAL,\n                             od_wait=100)","0893b356":"clf.fit(train_df[predictors], train_df[target].values,verbose=True)","afe1bc1a":"preds = clf.predict(valid_df[predictors])","131ef324":"tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': clf.feature_importances_})\ntmp = tmp.sort_values(by='Feature importance',ascending=False)\nplt.figure(figsize = (7,4))\nplt.title('Features importance',fontsize=14)\ns = sns.barplot(x='Feature',y='Feature importance',data=tmp)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)\nplt.show()   ","5c4be527":"cm = pd.crosstab(valid_df[target].values, preds, rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\nsns.heatmap(cm, \n            xticklabels=['Not Fraud', 'Fraud'],\n            yticklabels=['Not Fraud', 'Fraud'],\n            annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\nplt.title('Confusion Matrix', fontsize=14)\nplt.show()","9fa7fb03":"roc_auc_score(valid_df[target].values, preds)","da8fca4a":"# Prepare the train and valid datasets\ndtrain = xgb.DMatrix(train_df[predictors], train_df[target].values)\ndvalid = xgb.DMatrix(valid_df[predictors], valid_df[target].values)\ndtest = xgb.DMatrix(test_df[predictors], test_df[target].values)\n\n#What to monitor (in this case, **train** and **valid**)\nwatchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n\n# Set xgboost parameters\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eta'] = 0.039\nparams['silent'] = True\nparams['max_depth'] = 2\nparams['subsample'] = 0.8\nparams['colsample_bytree'] = 0.9\nparams['eval_metric'] = 'auc'\nparams['random_state'] = RANDOM_STATE","a805ec08":"model = xgb.train(params, \n                dtrain, \n                MAX_ROUNDS, \n                watchlist, \n                early_stopping_rounds=EARLY_STOP, \n                maximize=True, \n                verbose_eval=VERBOSE_EVAL)","e210396e":"fig, (ax) = plt.subplots(ncols=1, figsize=(8,5))\nxgb.plot_importance(model, height=0.8, title=\"Features importance (XGBoost)\", ax=ax, color=\"green\") \nplt.show()","cf50a6b6":"preds = model.predict(dtest)","df7deff9":"roc_auc_score(test_df[target].values, preds)","835f1453":"params = {\n          'boosting_type': 'gbdt',\n          'objective': 'binary',\n          'metric':'auc',\n          'learning_rate': 0.05,\n          'num_leaves': 7,  # we should let it be smaller than 2^(max_depth)\n          'max_depth': 4,  # -1 means no limit\n          'min_child_samples': 100,  # Minimum number of data need in a child(min_data_in_leaf)\n          'max_bin': 100,  # Number of bucketed bin for feature values\n          'subsample': 0.9,  # Subsample ratio of the training instance.\n          'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n          'colsample_bytree': 0.7,  # Subsample ratio of columns when constructing each tree.\n          'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n          'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n          'nthread': 8,\n          'verbose': 0,\n          'scale_pos_weight':150, # because training data is extremely unbalanced \n         }","6a67f185":"dtrain = lgb.Dataset(train_df[predictors].values, \n                     label=train_df[target].values,\n                     feature_name=predictors)\n\ndvalid = lgb.Dataset(valid_df[predictors].values,\n                     label=valid_df[target].values,\n                     feature_name=predictors)","3b12ce5d":"evals_results = {}\n\nmodel = lgb.train(params, \n                  dtrain, \n                  valid_sets=[dtrain, dvalid], \n                  valid_names=['train','valid'], \n                  evals_result=evals_results, \n                  num_boost_round=MAX_ROUNDS,\n                  early_stopping_rounds=2*EARLY_STOP,\n                  verbose_eval=VERBOSE_EVAL, \n                  feval=None)\n","45fcf764":"fig, (ax) = plt.subplots(ncols=1, figsize=(8,5))\nlgb.plot_importance(model, height=0.8, title=\"Features importance (LightGBM)\", ax=ax,color=\"red\") \nplt.show()","a20eefbe":"preds = model.predict(test_df[predictors])","577e1058":"roc_auc_score(test_df[target].values, preds)","e775985d":"kf = KFold(n_splits = NUMBER_KFOLDS, random_state = RANDOM_STATE, shuffle = True)\n\n# Create arrays and dataframes to store results\noof_preds = np.zeros(train_df.shape[0])\ntest_preds = np.zeros(test_df.shape[0])\nfeature_importance_df = pd.DataFrame()\nn_fold = 0\nfor train_idx, valid_idx in kf.split(train_df):\n    train_x, train_y = train_df[predictors].iloc[train_idx],train_df[target].iloc[train_idx]\n    valid_x, valid_y = train_df[predictors].iloc[valid_idx],train_df[target].iloc[valid_idx]\n    \n    evals_results = {}\n    model =  LGBMClassifier(\n                  nthread=-1,\n                  n_estimators=2000,\n                  learning_rate=0.01,\n                  num_leaves=80,\n                  colsample_bytree=0.98,\n                  subsample=0.78,\n                  reg_alpha=0.04,\n                  reg_lambda=0.073,\n                  subsample_for_bin=50,\n                  boosting_type='gbdt',\n                  is_unbalance=False,\n                  min_split_gain=0.025,\n                  min_child_weight=40,\n                  min_child_samples=510,\n                  objective='binary',\n                  metric='auc',\n                  silent=-1,\n                  verbose=-1,\n                  feval=None)\n    model.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n                eval_metric= 'auc', verbose= VERBOSE_EVAL, early_stopping_rounds= EARLY_STOP)\n    \n    oof_preds[valid_idx] = model.predict_proba(valid_x, num_iteration=model.best_iteration_)[:, 1]\n    test_preds += model.predict_proba(test_df[predictors], num_iteration=model.best_iteration_)[:, 1] \/ kf.n_splits\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = predictors\n    fold_importance_df[\"importance\"] = clf.feature_importances_\n    fold_importance_df[\"fold\"] = n_fold + 1\n    \n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n    del model, train_x, train_y, valid_x, valid_y\n    gc.collect()\n    n_fold = n_fold + 1\ntrain_auc_score = roc_auc_score(train_df[target], oof_preds)\nprint('Full AUC score %.6f' % train_auc_score)                                    ","486e1f87":"pred = test_preds","99266fcc":"## <a id=\"42\">Check missing data<\/a>  \n\nLet's check if there is any missing data.","5191eb19":"The ROC-AUC score obtained for the test set is **0.946**.","5fd0c648":"Let's look into more details to the data.","919db055":"For some of the features we can observe a good selectivity in terms of distribution for the two values of **Class**: **V4**, **V11** have clearly separated distributions for **Class** values 0 and 1, **V12**, **V14**, **V18** are partially separated, **V1**, **V2**, **V3**, **V10** have a quite distinct profile, whilst **V25**, **V26**, **V28** have similar profiles for the two values of **Class**.  \n\nIn general, with just few exceptions (**Time** and **Amount**), the features distribution for legitimate transactions (values of **Class = 0**)  is centered around 0, sometime with a long queue at one of the extremities. In the same time, the fraudulent transactions (values of **Class = 1**) have a skewed (asymmetric) distribution.","ce3c224c":"## <a id=\"61\">RandomForestClassifier<\/a>\n\n\n### Define model parameters\n\nLet's set the parameters for the model.","09d6d403":"### Features importance\n\nLet's see also the features importance.","f0acc412":"### Prepare the model\n\nLet's prepare the model, creating the **Dataset**s data structures from the train and validation sets.","81e1e4e9":"# <a id=\"2\">Load packages<\/a>","1e47d1f0":"The most important features are **V17**, **V12**, **V14**, **V10**, **V11**, **V16**.\n\n\n### Confusion matrix\n\nLet's show a confusion matrix for the results we obtained. ","9cd0ac7e":"## Features density plot","ef305529":"Let's also visualize the features importance.\n\n### Features importance","b1c28762":"### Area under curve\n\nLet's calculate ROC-AUC.","35216300":"## Transactions in time","e55d352d":"### Type I error and Type II error\n\nWe need to clarify that confussion matrix are not a very good tool to represent the results in the case of largely unbalanced data, because we will actually need a different metrics that accounts in the same time for the **selectivity** and **specificity** of the method we are using, so that we minimize in the same time both **Type I errors** and **Type II errors**.\n\n\n**Null Hypothesis** (**H0**) - The transaction is not a fraud.  \n**Alternative Hypothesis** (**H1**) - The transaction is a fraud.  \n\n**Type I error** - You reject the null hypothesis when the null hypothesis is actually true.  \n**Type II error** - You fail to reject the null hypothesis when the the alternative hypothesis is true.  \n\n**Cost of Type I error** - You erroneously presume that the the transaction is a fraud, and a true transaction is rejected.  \n**Cost of Type II error** - You erroneously presume that the transaction is not a fraud and a ffraudulent transaction is accepted.  \n\nThe following image explains what **Type I error** and **Type II error** are:    \n\n\n<img src=\"https:\/\/i.stack.imgur.com\/x1GQ1.png\" width=\"600\"\/>\n\nAnd this alternative image explains even better:  \n\n<img src=\"https:\/\/i2.wp.com\/flowingdata.com\/wp-content\/uploads\/2014\/05\/Type-I-and-II-errors1.jpg\" width=\"600\"\/>\n\n\n\nLet's calculate the ROC-AUC score <a href='#8'>[4]<\/a>.\n\n### Area under curve","9854c74a":"The real transaction have a larger mean value, larger Q1, smaller Q3 and Q4 and larger outliers; fraudulent transactions have a smaller Q1 and mean, larger Q4 and smaller outliers.\n\nLet's plot the fraudulent transactions (amount) against time. The time is shown is seconds from the start of the time period (totaly 48h, over 2 days).","2c66e124":"### Features importance\n\nLet's see also the features importance.","d6be54c2":"There is no missing data in the entire dataset.","3a75fd39":"### Predict the target values\n\nLet's now predict the **target** values for the **val_df** data, using predict function.","d84c08f4":"Let's start with a RandomForrestClassifier <a href='#8'>[3]<\/a>   model.","1ff40066":"# <a id=\"8\">References<\/a>\n\n[1] Credit Card Fraud Detection Database, Anonymized credit card transactions labeled as fraudulent or genuine, https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud  \n[2] Principal Component Analysis, Wikipedia Page, https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis  \n[3] RandomForrestClassifier, http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html  \n[4] ROC-AUC characteristic, https:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic#Area_under_the_curve   \n[5] AdaBoostClassifier, http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostClassifier.html  \n[6] CatBoostClassifier, https:\/\/tech.yandex.com\/catboost\/doc\/dg\/concepts\/python-reference_catboostclassifier-docpage\/  \n[7] XGBoost Python API Reference, http:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html  \n[8] LightGBM Python implementation, https:\/\/github.com\/Microsoft\/LightGBM\/tree\/master\/python-package  \n[9] LightGBM algorithm, https:\/\/www.microsoft.com\/en-us\/research\/wp-content\/uploads\/2017\/11\/lightgbm.pdf   \n\n","b14f0886":"## <a id=\"63\">CatBoostClassifier<\/a>\n\n\nCatBoostClassifier is a gradient boosting for decision trees algorithm with support for handling categorical data <a href='#8'>[6]<\/a>.\n\n### Prepare the model\n\nLet's set the parameters for the model and initialize the model.","cfbc1e94":"Let's now predict the **target** values for the **valid_df** data, using **predict** function.","26caf8e9":"# <a id=\"3\">Read the data<\/a>","9e7322dc":"### Predict test set\n\n\nWe used the train and validation sets for training and validation. We will use the trained model now to predict the target value for the test set.","0cd2f1a9":"## <a id=\"43\">Data unbalance<\/a>","d0f079e1":"Only **492** (or **0.172%**) of transaction are fraudulent. That means the data is highly unbalanced with respect with target variable **Class**.","88cc9a88":"As expected, there is no notable correlation between features **V1**-**V28**. There are certain correlations between some of these features and **Time** (inverse correlation with **V3**) and **Amount** (direct correlation with **V7** and **V20**, inverse correlation with **V1** and **V5**).\n\n\nLet's plot the correlated and inverse correlated values on the same graph.\n\nLet's start with the direct correlated values: {V20;Amount} and {V7;Amount}.","faf615d1":"### Confusion matrix\n\nLet's visualize the confusion matrix.","241d8d03":"### Prepare the model\n\nWe initialize the DMatrix objects for training and validation, starting from the datasets. We also set some of the parameters used for the model tuning.","bc7f52fe":"### Train the model\n\nLet's train the model. ","2fb69df9":"# <a id=\"4\">Check the data<\/a>","d9fb28a8":"We can confirm that the two couples of features are inverse correlated (the regression lines for **Class = 0** have a negative slope while the regression lines for **Class = 1** have a very small negative slope).\n","33b0b49c":"We investigated the data, checking for data unbalancing, visualizing the features and understanding the relationship between different features. \nWe then investigated two predictive models. The data was split in 3 parts, a train set, a validation set and a test set. For the first three models, we only used the train and test set.  \n\nWe started with **RandomForrestClassifier**, for which we obtained an AUC scode of **0.85** when predicting the target for the test set.  \n\nWe followed with an **AdaBoostClassifier** model, with lower AUC score (**0.83**) for prediction of the test set target values.    \n\nWe then followed with an **CatBoostClassifier**, with the AUC score after training 500 iterations **0.86**.    \n\nWe then experimented with a **XGBoost** model. In this case, se used the validation set for validation of the training model.  The best validation score obtained was   **0.984**. Then we used the model with the best training step, to predict target value from the test data; the AUC score obtained was **0.974**.\n\nWe then presented the data to a **LightGBM** model. We used both train-validation split and cross-validation to evaluate the model effectiveness to predict 'Class' value, i.e. detecting if a transaction was fraudulent. With the first method we obtained values of AUC for the validation set around **0.974**. For the test set, the score obtained was **0.946**.   \nWith the cross-validation, we obtained an AUC score for the test prediction of  **0.93**.","ac8ffaeb":"### Plot variable importance","fbd0fc0e":"The ROC-AUC score obtained with AdaBoostClassifier is **0.83**.","8c581c56":"XGBoost is a gradient boosting algorithm <a href='#8'>[7]<\/a>.\n\nLet's prepare the model.","7c56eb8d":"## <a id=\"64\">LightGBM<\/a>\n\n\nLet's continue with another gradient boosting algorithm, LightGBM <a href='#8'>[8]<\/a> <a href='#8'>[9]<\/a>.\n\n\n### Define model parameters\n\nLet's set the parameters for the model. We will use these parameters only for the first lgb model.","12921afd":"## Transactions amount","548dd380":"# <a id=\"6\">Predictive models<\/a>  \n\n","c34cf074":"Fraudulent transactions have a distribution more even than valid transactions - are equaly distributed in time, including the low real transaction times, during night in Europe timezone.","7bdfab09":"### Fit the model\n\nLet's fit the model.","b93c0ab9":"### Predict the target values\n\nLet's now predict the **target** values for the **valid_df** data, using predict function.","2bc6696a":"The AUC score for the prediction from the test data was 0.93.\n\nWe prepare the test prediction, from the averaged predictions for test over the 5 folds.","0520472f":"The AUC score for the prediction of fresh data (test set) is **0.974**.","f5874056":"### Training and validation using cross-validation\n\nLet's use now cross-validation. We will use cross-validation (KFolds) with 5 folds. Data is divided in 5 folds and, by rotation, we are training using 4 folds (n-1) and validate using the 5th (nth) fold.\n\nTest set is calculated as an average of the predictions ","6c4fc599":"Let's calculate also the ROC-AUC.\n\n\n### Area under curve","2210580f":"The **ROC-AUC** score obtained with **RandomForrestClassifier** is **0.85**.\n\n\n\n","33b8de12":"We can confirm that the two couples of features are correlated (the regression lines for **Class = 0** have a positive slope, whilst the regression line for **Class = 1** have a smaller positive slope).\n\nLet's plot now the inverse correlated values.","dd57d52a":"Let's calculate also the ROC-AUC.\n\n\n### Area under curve","c8eed3f2":"The ROC-AUC score obtained with CatBoostClassifier is **0.86**.","47587110":"Let's predict now the target for the test data.\n\n### Predict test data","93d65deb":"## <a id=\"63\">XGBoost<\/a>","2734100d":"The best validation score (ROC-AUC) was **0.984**, for round **241**.","8cf60e41":"Let's train the **RandonForestClassifier** using the **train_df** data and **fit** function.","0207eb90":"<h1><center><font size=\"6\">Credit Card Fraud Detection Predictive Models<\/font><\/center><\/h1>\n\n\n<img src=\"https:\/\/kaggle2.blob.core.windows.net\/datasets-images\/310\/684\/3503c6c827ca269cc00ffa66f2a9c207\/dataset-card.jpg\" width=\"400\"><\/img>\n\n\n# <a id='0'>Content<\/a>\n\n- <a href='#1'>Introduction<\/a>  \n- <a href='#2'>Load packages<\/a>  \n- <a href='#3'>Read the data<\/a>  \n- <a href='#4'>Check the data<\/a>  \n    - <a href='#41'>Glimpse the data<\/a>  \n    - <a href='#42'>Check missing data<\/a>\n    - <a href='#43'>Check data unbalance<\/a>\n- <a href='#5'>Data exploration<\/a>\n- <a href='#6'>Predictive models<\/a>  \n    - <a href='#61'>RandomForrestClassifier<\/a> \n    - <a href='#62'>AdaBoostClassifier<\/a>     \n    - <a href='#63'>CatBoostClassifier<\/a> \n    - <a href='#64'>XGBoost<\/a> \n    - <a href='#65'>LightGBM<\/a> \n- <a href='#7'>Conclusions<\/a>\n- <a href='#8'>References<\/a>\n","b6a0d331":"### Confusion matrix\n\nLet's visualize the confusion matrix.","8aba5b47":"Looking to the **Time** feature, we can confirm that the data contains **284,807** transactions, during 2 consecutive days (or **172792** seconds).","035db608":"# <a id=\"5\">Data exploration<\/a>","91cb56e6":"# <a id=\"7\">Conclusions<\/a>","60580c72":"## Features correlation","9be60106":"Best validation score  was obtained for round **85**, for which **AUC ~= 0.974**.\n\nLet's plot variable importance.","4ff2c486":"# <a id=\"1\">Introduction<\/a>  \n\nThe datasets contains transactions made by credit cards in **September 2013** by european cardholders. This dataset presents transactions that occurred in two days, where we have **492 frauds** out of **284,807 transactions**. The dataset is **highly unbalanced**, the **positive class (frauds)** account for **0.172%** of all transactions.  \n\nIt contains only numerical input variables which are the result of a **PCA transformation**.   \n\nDue to confidentiality issues, there are not provided the original features and more background information about the data.  \n\n* Features **V1**, **V2**, ... **V28** are the **principal components** obtained with **PCA**;  \n* The only features which have not been transformed with PCA are **Time** and **Amount**. Feature **Time** contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature **Amount** is the transaction Amount, this feature can be used for example-dependant cost-senstive learning.   \n* Feature **Class** is the response variable and it takes value **1** in case of fraud and **0** otherwise.  \n\n","68880199":"Let's check data unbalance with respect with *target* value, i.e. **Class**.","59c88fb2":"### Run the model\n\nLet's run the model, using the **train** function.","e38ee0b8":"Let's run a model using the training set for training. Then, we will use the validation set for validation. \n\nWe will use as validation criterion **GINI**, which formula is **GINI = 2 * (AUC) - 1**, where **AUC** is the **Receiver Operating Characteristic - Area Under Curve (ROC-AUC)** <a href='#8'>[4]<\/a>.  Number of estimators is set to **100** and number of parallel jobs is set to **4**.\n\nWe start by initializing the RandomForestClassifier.","38c5fc24":"## <a id=\"62\">AdaBoostClassifier<\/a>\n\n\nAdaBoostClassifier stands for Adaptive Boosting Classifier <a href='#8'>[5]<\/a>.\n\n### Prepare the model\n\nLet's set the parameters for the model and initialize the model.","cb49b48b":"### Define predictors and target values\n\nLet's define the predictor features and the target features. Categorical features, if any, are also defined. In our case, there are no categorical feature.","fd8c058d":"## <a id=\"41\">Glimpse the data<\/a>\n\nWe start by looking to the data features (first 5 rows).","2cea75ed":"### Split data in train, test and validation set\n\nLet's define train, validation and test sets.","e7ec0e74":"### Area under curve\n\nLet's calculate the ROC-AUC score for the prediction."}}