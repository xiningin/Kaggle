{"cell_type":{"eb037d52":"code","32df1b57":"code","ea4c46ff":"code","183e5846":"code","435f1d0e":"code","b14193bb":"code","86676e75":"code","c8461627":"code","f3fe9668":"code","08cd8420":"code","e246f170":"code","ece83aee":"code","3510839e":"code","9679d37c":"code","8661dcfc":"code","41e16ee6":"code","2d0fd8e7":"code","9a4a958f":"code","700a6c32":"code","39043375":"code","ba53c8c4":"code","01488b3d":"code","feaa4100":"code","f0725167":"code","bab0bdec":"code","6bfda00c":"code","5247a394":"code","247a6d5c":"code","8f48aff8":"code","41dfd87a":"code","0b3bdea0":"code","d94cec56":"markdown","23898190":"markdown","950b75d3":"markdown","a80b040e":"markdown","7d91dcf4":"markdown","1fcce01e":"markdown","ed08f0ff":"markdown","60310779":"markdown","d077caf5":"markdown","5d5ba96c":"markdown","d3e7385b":"markdown","d22f0c1e":"markdown","5f76addb":"markdown","376f641a":"markdown","5e7a7feb":"markdown","7e2e528f":"markdown","422f5fb7":"markdown","00dbb2a4":"markdown","34c1fe58":"markdown","b8a4c8b0":"markdown","bc75c05f":"markdown","746f93e3":"markdown","0e04f3f7":"markdown","f354e588":"markdown","eddcc48a":"markdown","0e5d312c":"markdown","a0df81b9":"markdown"},"source":{"eb037d52":"## Data manipulation\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy import stats\n\n## Visualization\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n%matplotlib inline\nplt.style.use('seaborn')\n\n## Modeling\nfrom sklearn.preprocessing import StandardScaler\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import mean_squared_error, roc_auc_score\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD, PCA, LatentDirichletAllocation, NMF\nfrom sklearn.linear_model import Ridge,ElasticNet, SGDRegressor, LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom scipy import sparse\nfrom scipy.stats import norm, skew\n\nfrom sklearn.manifold import TSNE\n\n## others\nimport copy\nimport os\nimport time\nimport warnings\nimport gc\nimport os\nimport pickle\nfrom six.moves import urllib\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Any results you write to the current directory are saved as output.","32df1b57":"train_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')\nfull_data = pd.concat([train_data, test_data])\n\nprint(\"train_data:\", train_data.info())\nprint(\"test_data:\", test_data.info())\n","ea4c46ff":"train_data.head(10)","183e5846":"# Numerical features\nnum_vars = []\n# Categorical features\ncat_vars = []\nfor var, dtype in full_data.dtypes.items():\n    if \"float\" in str(dtype) or \"int\" in str(dtype):\n        num_vars.append(var)\n    if \"object\" in str(dtype):\n        cat_vars.append(var)\n\nid_var = \"ID_code\" # this is just the order of data\ncat_vars.remove(id_var)\ntarget_var = \"target\"\nnum_vars.remove(target_var)\nprint(\"There are %d numerical features: %s\" %(len(num_vars), num_vars))\nprint(\"There are %d numerical features: %s\" %(len(cat_vars), cat_vars))\n","435f1d0e":"train_data.describe()","b14193bb":"test_data.describe()","86676e75":"sns.countplot(train_data[\"target\"])","c8461627":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis= 1, keys = ['total', 'percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt[\"Type\"] = types\n    return np.transpose(tt)","f3fe9668":"missing_data(train_data)","08cd8420":"missing_data(test_data)","e246f170":"train_unique_1= train_data[num_vars].nunique().reset_index()\ntrain_unique_2 = train_unique_1.rename(columns = {\"index\":'feature', 0:'unique'}).sort_values('unique')\nsns.barplot(x = 'feature', y = 'unique', color = 'b', data = train_unique_2)","ece83aee":"test_unique_1= test_data[num_vars].nunique().reset_index()\ntest_unique_2 = test_unique_1.rename(columns = {\"index\":'feature', 0:'unique'}).sort_values('unique')\nsns.barplot(x = 'feature', y = 'unique', color = 'b', data = test_unique_2)","3510839e":"std_scaler = StandardScaler()\n# Notice we are using full datasets in order to capture more information\nstd_scaler.fit(full_data[num_vars].values) \ntrain_std_df = pd.DataFrame(std_scaler.transform(train_data[num_vars].values), columns=num_vars)\ntest_std_df = pd.DataFrame(std_scaler.transform(test_data[num_vars].values) , columns=num_vars)\n\ntrain_std_df['target'] = train_data['target'].values\ntrain_std_df[num_vars].describe()","9679d37c":"corr_data = full_data[num_vars].corr()\n\ncmap = sns.diverging_palette(220, 10, as_cmap=True) # Use different colors by palette\n# Draw the heatmap with the mask and correct aspect ratio\n# sns.heatmap(corr_data)    , cbar_kws={\"shrink\": .5}\nsns.heatmap(corr_data, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5)","8661dcfc":"train_norm_data = train_data[num_vars].apply(lambda x: stats.normaltest(x)[1])\nprint(\"There are %d features normally distributed.\" % ((train_norm_data<0.05).sum()))\n\nprint(\"Top 10 features with highest P value:\")\ntrain_norm_data.sort_values(ascending=False).head(10)","41e16ee6":"test_norm_data = test_data[num_vars].apply(lambda x: stats.normaltest(x)[1])\nprint(\"There are %d features normally distributed.\" % ((test_norm_data<0.05).sum()))\n\nprint(\"Top 10 features with highest P value:\")\ntest_norm_data.sort_values(ascending=False).head(10)","2d0fd8e7":"## Plot var_146\nsns.distplot(train_data['var_146'])","9a4a958f":"sns.pairplot(train_data[num_vars[:20] + ['target']][:10000], hue='target')\n# 1: green; 0: blue","700a6c32":"%%time\npca = PCA(n_components = 2)\npca2d = pca.fit_transform(train_data[num_vars][:10000].values)\npca2d_df = pd.DataFrame({'pca_0':pca2d[:,0], 'pca_1':pca2d[:,1], 'target': train_data['target'][:10000].values})\nsns.lmplot(x='pca_0', y='pca_1', data=pca2d_df, hue='target', fit_reg=False)","39043375":"%%time\n## 2D\nsvd = TruncatedSVD(n_components=2)\nsvd2d = svd.fit_transform(train_data[num_vars][:10000].values)\nsvd2d_df = pd.DataFrame({'svd_0':svd2d[:,0],'svd_1':svd2d[:,1],'target':train_data['target'][:10000].values})\nsns.lmplot(x='svd_0', y='svd_1', data=svd2d_df, hue='target', fit_reg=False)","ba53c8c4":"%%time\n# 1D\ntsne = TSNE(n_components=1)\ntsne1d = tsne.fit_transform(train_data[num_vars][:10000].values)\ntsne1d_df = pd.DataFrame({'tsne_0':tsne1d.reshape(-1), 'target':train_data['target'][:10000].values})\nsns.distplot(tsne1d_df.query('target==0')['tsne_0'], label='target:0')\nsns.distplot(tsne1d_df.query('target==1')['tsne_0'], label='target:1')\nplt.legend()","01488b3d":"%%time\n## 2D\ntsne = TSNE(n_components=2, perplexity = 50, n_iter = 2000)\ntsne2d = tsne.fit_transform(train_data[num_vars][:10000].values)\ntsne2d_df = pd.DataFrame({'tsne_0':tsne2d[:,0],'tsne_1':tsne2d[:,1],'target':train_data['target'][:10000].values})\nsns.lmplot(x='tsne_0', y='tsne_1', data=tsne2d_df, hue='target', fit_reg=False)\nplt.legend()","feaa4100":"%%time\ntrain_x = train_std_df[num_vars].values\ntrain_y = train_std_df['target'].values\ntest_x = test_std_df[num_vars].values\n\nlr = LogisticRegression()\nlr.fit(train_x, train_y)","f0725167":"lr_feature_importance = pd.DataFrame({'feature':num_vars, 'lr_importance':lr.coef_.reshape(-1), \n                                      'abs_lr_importance': abs(lr.coef_.reshape(-1))})\n                            \nlr_feature_importance.sort_values('abs_lr_importance', ascending=False).head(10)","bab0bdec":"%%time\nlgb_clf = lgb.LGBMClassifier(n_jobs=-1)\nlgb_clf.fit(train_x, train_y)","6bfda00c":"lgb_feature_importance = pd.DataFrame({'feature':num_vars, \n                                       'lgb_importance':lgb_clf.feature_importances_.reshape(-1)})\n                                        \nlgb_feature_importance.sort_values('lgb_importance', ascending=False).head()","5247a394":"feature_importance = pd.merge(lr_feature_importance, lgb_feature_importance, on='feature')\nfeature_importance.head(20)","247a6d5c":"from bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\nfrom bokeh.models import ColumnDataSource\n\noutput_notebook()\n\nTOOLTIPS = [\n    (\"Feature\", \"@feature\"),\n    (\"LR importance\", \"@abs_lr_importance\"),\n    (\"LGB importance\", \"@lgb_importance\")\n]\np = figure(plot_width=400, plot_height=400, tooltips=TOOLTIPS)\np.circle('abs_lr_importance', \n         'lgb_importance', source=ColumnDataSource(feature_importance), size=8)\nshow(p)","8f48aff8":"sns.distplot(train_std_df['var_53'])","41dfd87a":"# var_53 against target\nsns.distplot(train_std_df.query('target==0')['var_53'], label='target:0')\nsns.distplot(train_std_df.query('target==1')['var_53'], label='target:1')\nplt.legend()","0b3bdea0":"# var_81 against target\nsns.distplot(train_std_df.query('target==0')['var_81'], label='target:0')\nsns.distplot(train_std_df.query('target==1')['var_81'], label='target:1')\nplt.legend()","d94cec56":"### Visualization with Dimension Reduction","23898190":"# EDA (Exploratory Data Analysis)\n* Basic statistics: count, std, min, max, mean, median, quartiles\n* Distributions\n* Missing values\n* Unique values\n* Feature correlations\n* Feature importance[](http:\/\/)","950b75d3":"#### PCA","a80b040e":"## Feature importance from logistic regression","7d91dcf4":"## Missing values","1fcce01e":"## Feature correlation","ed08f0ff":"### Feature Standardization","60310779":"## Target distribution","d077caf5":"#### TruncatedSVD","5d5ba96c":"## Normality tests\nUsing scipy.stats.normaltest(). If p<0.05, the data is normally distributed.","d3e7385b":"Dimension reduction is used to reducing the number of random variables under consideration by obtaining a set of principal variables. \n\nThe most common approaches are \n* PCA\n* TruncatedSVD\n* TSNE\n \n TSNE is considered as the go-to algorithms for visualizing higher dimensional data.\n     https:\/\/distill.pub\/2016\/misread-tsne\/\n ","d22f0c1e":"## Combined feature importance","5f76addb":"# Load Data","376f641a":"    - Removing the mean and scaling to unit variance: x_new = (x - u)\/sigma\n    - Required by SVM\/ K-means.\n    - Good for linear models, such as Linear regression, Logistic Regression, LASSO\/ Ridge and NN to converge faster\n    - No need for tree models****","5e7a7feb":"Similarly, we can look at another important variant","7e2e528f":"## Feature importance from LightGBM","422f5fb7":"# Feature importance","00dbb2a4":"#### TSNE","34c1fe58":"## Basic statistics","b8a4c8b0":"### Plot the features with highest P value","bc75c05f":"### Examine Features that are important in Both analysis","746f93e3":"### Pair plotting","0e04f3f7":"Define Variables that are useful for later use","f354e588":"# Additional readings\n* [Applied Predictive Modeling - Chapter 3 Data Pre-Processing](http:\/\/appliedpredictivemodeling.com\/toc\/)\n* [\u673a\u5668\u5b66\u4e60\u7279\u5f81\u5de5\u7a0b\u5b9e\u7528\u6280\u5de7\u5927\u5168](https:\/\/zhuanlan.zhihu.com\/p\/26444240)\n* [Discover Feature Engineering](http:\/\/machinelearningmastery.com\/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it\/)\n* [Selecting good features \u2013 Part IV: stability selection, RFE and everything side by side](http:\/\/blog.datadive.net\/selecting-good-features-part-iv-stability-selection-rfe-and-everything-side-by-side\/)","eddcc48a":"Preview the data","0e5d312c":"## Unique values","a0df81b9":"# Import the commonly used packages"}}