{"cell_type":{"44aee42c":"code","8c6012c2":"code","42fba3dd":"code","ed815736":"code","9d1ef047":"code","999509f4":"code","637f82b3":"code","ed15137a":"code","a5321a8a":"code","fc6fdbc2":"code","7334622d":"code","7c6d2036":"code","beab2c76":"code","634fcb43":"code","897ec7df":"code","5369f4b6":"code","d479c121":"code","86f6be8e":"code","83023052":"code","a20b83ce":"code","bd3dd1c7":"markdown","c069df03":"markdown","a8e556f7":"markdown","ef96209b":"markdown","1ddd52e7":"markdown","89ab6034":"markdown","72b595eb":"markdown","f8982cff":"markdown"},"source":{"44aee42c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8c6012c2":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly_express as px","42fba3dd":"train = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/test.csv')\ntrain","ed815736":"train.target.unique()","9d1ef047":"train.target.value_counts()","999509f4":"fig = px.histogram(train,'target', histnorm='percent')\nfig.show()\n#histnorm gives the format of y. By default it gives the count of value occurences of x variable","637f82b3":"fig = px.pie(train,values='feature_48',names='target',title='Class Distribution')\nfig.update_traces(textposition='inside', textinfo='percent+label')\nfig.show()\n#In px.pie, data visualized by the sectors of the pie is set in values. The sector labels are set in names.","ed15137a":"fig = px.scatter(train, x = 'feature_0', y= 'feature_7', \n                 hover_name='target', color='target')\nfig.show()\n#hovername gives the value of the point as we move our cursor over the plot","a5321a8a":"corr_mat = train.corr()\nmask = np.zeros_like(corr_mat)\nmask[np.triu_indices_from(mask)] = True\n\nplt.figure(figsize=(20,20))\nsns.heatmap(corr_mat, annot=False, mask=mask,\n           linewidths=.5)","fc6fdbc2":"some_columns = train.columns[:8]\nsns.pairplot(data=train[some_columns], kind='scatter')","7334622d":"train.drop(columns='id',inplace=True)  #not required","7c6d2036":"train.describe()","beab2c76":"px.scatter(train.describe().T, x='mean', y= 'max', size='std',\n          hover_name='std', title='Describe Plot')","634fcb43":"feat_unique_counts= np.zeros(len(train.columns[:-1]))\nc=0\nfor i in train.columns[:-1]:\n    feat_unique_counts[c]= train[i].nunique()\n    c+=1\n\nplt.figure(figsize=(20,8))\nplt.grid()\nplt.xticks(rotation=90)\nplt.stem(train.columns[:-1],feat_unique_counts)","897ec7df":"features = train.columns[:-1]\nzero_counts, nonzero_counts= np.zeros(len(features)),np.zeros(len(features))\nc=0\n\nfor i in features:\n    zero_counts[c] = train[i].value_counts()[0]\/len(train)\n    nonzero_counts[c] = 1-zero_counts[c]\n    c+=1\n    \nvaluecounts_df = pd.DataFrame(data=zero_counts,\n                             columns=['zeros'],\n                             index=features)\n\nvaluecounts_df['non-zeros'] = nonzero_counts\nvaluecounts_df","5369f4b6":"labels = ['0','!0']\nexplode = [0.1,0]\n\nfig, ax = plt.subplots(15,5, figsize=(20,20))\nfor  i ,feature  in enumerate(features , 1):\n    plt.subplot(15, 5, i)\n    plt.pie(valuecounts_df.T[feature], labels=labels,explode=explode)\n    plt.xlabel(feature, fontsize=9)\n    \nfig.tight_layout()\nplt.show()","d479c121":"mean_diff = np.zeros(len(features))\nc=0\nfor i in features:\n    mean_diff[c] = train[i].mean()-test[i].mean()\n    c+=1\n    \npx.bar(mean_diff, hover_name=mean_diff,\n      title='Mean difference between train & test sets')","86f6be8e":"px.histogram(train, y='feature_12', x='target')","83023052":"enc = {\n    'Class_1':0.0,\n    'Class_2':1.0,\n    'Class_3':2.0,\n    'Class_4':3.0,\n    'Class_5':4.0,\n    'Class_6':5.0,\n    'Class_7':6.0,\n    'Class_8':7.0,\n    'Class_9':8.0    \n}\n\ntrain.target.replace(to_replace=enc,inplace=True)\ntrain.target","a20b83ce":"X = train.drop(columns='target').to_numpy()\ny = train.target.to_numpy()\ntest = test.drop(columns='id').to_numpy()\n\nX.shape,test.shape","bd3dd1c7":"We see that mean and max are mostly positively correlated, except for a few features. Also, in general std dev is high for higher mean and max values.","c069df03":"# Unique Counts of each Feature","a8e556f7":"# Encoding the Classes","ef96209b":"Difference in mean is quite small (<0.06 in almost all features). However train means > test means in almost all the features.","1ddd52e7":"# Visualising Zero & Non-Zero proportion in Features","89ab6034":"# Correlation Plot","72b595eb":"It's hard to distinguish numerical and categorical values as there are no binary\/trinary features","f8982cff":"Class_6 and Class 8 alone rakes up >50% of the samples. While Class_5 and Class_4 have very less representation"}}