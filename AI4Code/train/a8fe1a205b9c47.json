{"cell_type":{"8869ef75":"code","c1997a77":"code","b6b97790":"code","792b58f2":"code","c15cf004":"code","fc46f74a":"code","fa423e51":"code","cfe87325":"code","03dc97ac":"code","ed3a9679":"code","1f6c3fd8":"code","378bbc43":"code","73052a6b":"code","4ccafe35":"code","4ff378ef":"code","ac015eb4":"code","27ac8249":"code","9e724771":"code","1c6f9d53":"code","bbe1ad00":"code","e0f22c2a":"code","458ee6d7":"code","9f57788f":"markdown","e8e7c770":"markdown","3bb06e4c":"markdown","7ca32622":"markdown","968d4e91":"markdown","9b9f569e":"markdown","691f4836":"markdown","6ce4e9e5":"markdown","6c709ca0":"markdown","f1bd511d":"markdown","d66ca78c":"markdown","4bdd473b":"markdown","360c4be8":"markdown","428c4a67":"markdown","b40c071c":"markdown","79d8fefe":"markdown","428cd462":"markdown","93da927c":"markdown","809f49a8":"markdown","a86e3967":"markdown","b5b1ebfd":"markdown","3676fcba":"markdown","7a3a4fb0":"markdown","01fbe4de":"markdown","11762cbd":"markdown","a21c162e":"markdown"},"source":{"8869ef75":"!pip install --upgrade pip\n!pip install pymap3d==2.1.0\n!pip install -U l5kit","c1997a77":"import matplotlib.pyplot as plt\n\nimport numpy as np\nimport pandas as pd\n\nfrom l5kit.data import ChunkedDataset, LocalDataManager\nfrom l5kit.dataset import EgoDataset, AgentDataset\n\nfrom l5kit.rasterization import build_rasterizer\nfrom l5kit.configs import load_config_data\nfrom l5kit.visualization import draw_trajectory, TARGET_POINTS_COLOR\nfrom l5kit.geometry import transform_points\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom l5kit.data import PERCEPTION_LABELS\nfrom prettytable import PrettyTable\n\nimport os\n\nfrom matplotlib import animation, rc\nfrom IPython.display import HTML\n\nrc('animation', html='jshtml')","b6b97790":"def animate_solution(images):\n\n    def animate(i):\n        im.set_data(images[i])\n \n    fig, ax = plt.subplots()\n    im = ax.imshow(images[0])\n    \n    return animation.FuncAnimation(fig, animate, frames=len(images), interval=60)","792b58f2":"# set env variable for data\nos.environ[\"L5KIT_DATA_FOLDER\"] = \"..\/input\/lyft-motion-prediction-autonomous-vehicles\"\n# get config\ncfg = load_config_data(\"..\/input\/lyft-config-files\/visualisation_config.yaml\")\nprint(cfg)","c15cf004":"dm = LocalDataManager()\ndataset_path = dm.require(cfg[\"val_data_loader\"][\"key\"])\nzarr_dataset = ChunkedDataset(dataset_path)\nzarr_dataset.open()\nprint(zarr_dataset)","fc46f74a":"import zarr","fa423e51":"train_zarr = zarr.open(\"..\/input\/lyft-motion-prediction-autonomous-vehicles\/scenes\/train.zarr\")\n\nprint(type(train_zarr))\n","cfe87325":"train_zarr.info","03dc97ac":"train_zarr.agents[:5]","ed3a9679":"agents = zarr_dataset.agents\nprobabilities = agents[\"label_probabilities\"]\nlabels_indexes = np.argmax(probabilities, axis=1)\ncounts = []\nfor idx_label, label in enumerate(PERCEPTION_LABELS):\n    counts.append(np.sum(labels_indexes == idx_label))\n    \ntable = PrettyTable(field_names=[\"label\", \"counts\"])\nfor count, label in zip(counts, PERCEPTION_LABELS):\n    table.add_row([label, count])\nprint(table)","1f6c3fd8":"rast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, zarr_dataset, rast)","378bbc43":"data = dataset[80]\n\nim = data[\"image\"].transpose(1, 2, 0)\nim = dataset.rasterizer.to_rgb(im)\ntarget_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\ndraw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\nfigsize = plt.subplots(figsize = (10,10))\nplt.title('Autonomous Vehicle with Trajectory',fontsize=20)\n\nplt.imshow(im[::-1])\nplt.show()","73052a6b":"cfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, zarr_dataset, rast)\ndata = dataset[80]\n\nim = data[\"image\"].transpose(1, 2, 0)\nim = dataset.rasterizer.to_rgb(im)\ntarget_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\ndraw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\nfigsize = plt.subplots(figsize = (10,10))\nplt.title('Satellite View',fontsize=20)\nplt.imshow(im[::-1])\nplt.show()","4ccafe35":"dataset = AgentDataset(cfg, zarr_dataset, rast)\ndata = dataset[80]\n\nim = data[\"image\"].transpose(1, 2, 0)\nim = dataset.rasterizer.to_rgb(im)\ntarget_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\ndraw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n\nfig = plt.subplots(figsize=(10,10))\nplt.imshow(im[::-1])\nplt.show()","4ff378ef":"from IPython.display import display, clear_output\nimport PIL\n \ncfg[\"raster_params\"][\"map_type\"] = \"py_semantic\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, zarr_dataset, rast)\nscene_idx = 34\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    images.append(PIL.Image.fromarray(im[::-1]))","ac015eb4":"anim = animate_solution(images)\nHTML(anim.to_jshtml())","27ac8249":"from IPython.display import display, clear_output\nimport PIL\n \ncfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, zarr_dataset, rast)\nscene_idx = 34\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\n\nfor i, idx in enumerate(indexes):\n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    images.append(PIL.Image.fromarray(im[::-1]))","9e724771":"anim = animate_solution(images)\nHTML(anim.to_jshtml())","1c6f9d53":"DIR_INPUT = \"\/kaggle\/input\/lyft-motion-prediction-autonomous-vehicles\"\n\nSINGLE_MODE_SUBMISSION = f\"{DIR_INPUT}\/single_mode_sample_submission.csv\"\nMULTI_MODE_SUBMISSION = f\"{DIR_INPUT}\/multi_mode_sample_submission.csv\"","bbe1ad00":"# Single mode\nsample_submission = pd.read_csv(SINGLE_MODE_SUBMISSION)\n\n# Multi mode\n# sample_submission = pd.read_csv(MULTI_MODE_SUBMISSION)","e0f22c2a":"sample_submission.head()\n","458ee6d7":"sample_submission.to_csv(\"submission.csv\", index=False)","9f57788f":"Code is taken from: \n\nhttps:\/\/github.com\/lyft\/l5kit\n\nhttps:\/\/github.com\/lyft\/l5kit\/blob\/master\/examples\/visualisation\/visualise_data.ipynb\n","e8e7c770":"## Satellite","3bb06e4c":"### The 2020 Lyft competition dataset is stored in four structured arrays: scenes, frames, agents and tl_faces.\n *Note: in the following all _interval fields assume that information is stored consecutively in the arrays. This means that if frame_index_interval for scene_0 is (0, 100), frames from scene_1 will start from index 100 in the frames array.*","7ca32622":"# DATA FORMAT\n","968d4e91":"# Example prediction solution","9b9f569e":"### Experiment with the largest-ever self-driving Prediction Dataset to build motion prediction model.","691f4836":"# Prediction Dataset sample","6ce4e9e5":" Green represents the autonomous vehicle (AV), and blue is primarily all the other cars\/vehicles\/exogenous factors we need to predict for.\n If we are able to accurately predict the path the vehicles go through, it will make it easier for an AV to compute its trajectory on the fly.","6c709ca0":"# EXAMPLE","f1bd511d":" The dataset is provided in zarr format. The zarr files are flat, compact, and highly performant for loading. To read the dataset please use our new Python software kit.\n The dataset consists of frames and agent states. A frame is a snapshot in time which consists of ego pose, time, and multiple agent states. Each agent state describes the position, orientation, bounds, and type.","d66ca78c":"![](http:\/\/https:\/\/self-driving.lyft.com\/wp-content\/uploads\/2020\/06\/motion_dataset_2-1.png)","4bdd473b":" Reading from a zarr array is as easy as slicing from it like you would any numpy array. The return value is an ordinary numpy array. Zarr takes care of determining which chunks to read from.","360c4be8":"## Semantic","428c4a67":"![](https:\/\/self-driving.lyft.com\/wp-content\/uploads\/2020\/06\/motion_dataset_2-1.png)","b40c071c":"# Scene Visualisation","79d8fefe":"# Autonomous Vehicle with Trajectory","428cd462":"# Satellite View","93da927c":"![](https:\/\/self-driving.lyft.com\/wp-content\/uploads\/2020\/06\/diagram-prediction-1.jpg)","809f49a8":"# Load the data","a86e3967":"# Imports","b5b1ebfd":"# Sample Submission","3676fcba":"# About the competitions","7a3a4fb0":"# Data Overview","01fbe4de":"# What is autonomous vehicle ?\nA self-driving car, also known as an autonomous vehicle (AV), connected and autonomous vehicle (CAV), driverless car, robo-car, or robotic car,(automated vehicles and fully automated vehicles in the European Union) is a vehicle that is capable of sensing its environment and moving safely with little or no human input.\n\nSelf-driving cars combine a variety of sensors to perceive their surroundings, such as radar, lidar, sonar, GPS, odometry and inertial measurement units. Advanced control systems interpret sensory information to identify appropriate navigation paths, as well as obstacles and relevant signage\n","11762cbd":"What is semantic map?\n\n","a21c162e":"#  Entire Scene"}}