{"cell_type":{"b7ce644d":"code","7a6c38f3":"code","6dda764e":"code","a4fc3485":"code","d96ff57f":"code","5d668413":"code","61657a81":"code","9a727f74":"code","6caa622b":"code","6abb332f":"code","ddfd3668":"code","0899cb69":"code","d63e1452":"code","0323b91d":"code","0abb59be":"code","154c8e24":"code","a67fbf44":"code","c9353ebd":"code","a5d2fa76":"code","483ebb74":"code","cdbf1091":"code","0e15965d":"code","d3b33e94":"code","b3dd3d6b":"code","be4b95c3":"code","23937608":"code","817da601":"markdown","5d7f86d9":"markdown","9df35540":"markdown","35a94165":"markdown","4a73034f":"markdown"},"source":{"b7ce644d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7a6c38f3":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LinearRegression , Ridge , LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder , PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score , mean_squared_error\nfrom scipy import stats\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom graphviz import Source\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix , classification_report\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nimport warnings\nwarnings.filterwarnings('ignore')\nsns.set(style=\"darkgrid\")\nplt.style.use('fivethirtyeight')\n\n","6dda764e":"def dataset_overview(data):\n    \n    print(\"------------\")\n    print(data.head())\n    print(\"-----------\")\n    print(data.columns)\n    print(\"------------\")\n    print(\"Shape of the dataset\")\n    print(data.shape)\n    print(\"-------------\")\n    print(\"Null Value counts\")\n    print(data.isnull().sum())\n    print(\"-------------\")\n    print(\"dataset informaation\")\n    print(data.info())\n    print(\"---------------\")\n    plt.figure(figsize = (10,10))\n    print(sns.heatmap(data.corr(), annot = True, cmap = \"RdYlGn\"))\n    \n    \n","a4fc3485":"df=pd.read_csv('..\/input\/fetal-health-classification\/fetal_health.csv')\ndf.head()","d96ff57f":"df['fetal_health'].value_counts()","5d668413":"dataset_overview(df)","61657a81":"def correlation_matrix(data):\n    corr = data.corr().round(2)\n\n    # Mask for the upper triangle\n    mask = np.zeros_like(corr, dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True\n\n    # Set figure size\n    f, ax = plt.subplots(figsize=(20, 20))\n\n    # Define custom colormap\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n    # Draw the heatmap\n    d=sns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\n    plt.tight_layout()\n    return d","9a727f74":"correlation_matrix(df)","6caa622b":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n","6abb332f":"def vif_values(data, col):\n    cols=col\n    X=data.drop(cols, axis=1)\n    vif_data = pd.DataFrame()\n    vif_data[\"feature\"] = X.columns\n    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n                          for i in range(len(X.columns))]\n    return vif_data\n","ddfd3668":"df.columns","0899cb69":"vif_values(df,'fetal_health')","d63e1452":"df=df[df['fetal_health']!=3]","0323b91d":"x = df.drop(\"fetal_health\", axis=1)\ny = df.fetal_health\nX_train , X_test , y_train , y_test = train_test_split(x , y, test_size=0.25, random_state=42)","0abb59be":"def train_test_function(data, col):\n    x = data.drop(col, axis=1)\n    y = data[col]\n    X_train , X_test , y_train , y_test = train_test_split(x , y, test_size=0.25, random_state=42)\n    return X_train, X_test, y_train, y_test","154c8e24":"train_test_function(df, 'fetal_health')","a67fbf44":"from sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix,ConfusionMatrixDisplay\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import confusion_matrix,classification_report,precision_score, plot_roc_curve, plot_precision_recall_curve, balanced_accuracy_score\n\ndef clf_scores(clf, y_predicted):\n    # Accuracy\n    acc_train = clf.score(X_train, y_train)*100\n    acc_test = clf.score(X_test, y_test)*100\n    \n    roc = roc_auc_score(y_test, y_predicted)*100 \n    tn, fp, fn, tp = confusion_matrix(y_test, y_predicted).ravel()\n    cm = confusion_matrix(y_test, y_predicted)\n    correct = tp + tn\n    incorrect = fp + fn\n    d=[acc_train, acc_test,  roc, correct, incorrect,  cm]\n    index=[\"acc_train\",'Test Accuracy',\"Roc Score\",\"COrrect\",\"Incorrect\",\"Confusion\"  ]\n    output=pd.DataFrame(data=d, index=index)\n    \n    d=sns.heatmap(cm, annot=True)\n    dd=plot_roc_curve(clf, X_train, y_train)\n    ddd=plot_precision_recall_curve(clf, X_train, y_train)\n\n    return output,d, dd, ddd","c9353ebd":"#1. Logistic regression\n\nfrom sklearn.linear_model import LogisticRegression\nclf_lr = LogisticRegression(solver='liblinear')\nclf_lr.fit(X_train, y_train)\n\nY_pred_lr = clf_lr.predict(X_test)\nprint(clf_scores(clf_lr, Y_pred_lr))","a5d2fa76":"# 2 Random Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\nclf_rf = RandomForestClassifier()\nclf_rf.fit(X_train, y_train)\n\nY_pred_rf = clf_rf.predict(X_test)\nprint(clf_scores(clf_rf, Y_pred_rf))","483ebb74":"# 3 XGboost\nfrom sklearn.ensemble import GradientBoostingClassifier\nclf_xg = GradientBoostingClassifier()\nclf_xg.fit(X_train, y_train)\n\nY_pred_xg = clf_xg.predict(X_test)\nprint(clf_scores(clf_xg, Y_pred_xg))","cdbf1091":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score","0e15965d":"# k-fold CV \nscores = cross_val_score(clf_lr, X_train, y_train, scoring='r2', cv=5).mean()\nscores","d3b33e94":"# k-fold CV \nscores = cross_val_score(clf_lr, X_train, y_train, scoring='r2', cv=10).mean()\nscores","b3dd3d6b":"# k-fold CV \nscores = cross_val_score(clf_lr, X_test, y_test, scoring='r2', cv=5).mean()\nscores","be4b95c3":"# k-fold CV \nscores = cross_val_score(clf_rf, X_train, y_train, scoring='r2', cv=5).mean()\nscores","23937608":"# k-fold CV \nscores = cross_val_score(clf_xg, X_train, y_train, scoring='r2', cv=5).mean()\nscores","817da601":"# Function for Correlation Matrix","5d7f86d9":"# Function for VIF","9df35540":"# Cross Validation","35a94165":"# Models Building steps","4a73034f":"# Function for Train Test"}}