{"cell_type":{"bc6e52ea":"code","ce5b36fa":"code","a115046b":"code","24964f4e":"code","31988209":"code","81677905":"code","74a07e63":"code","efda5a76":"code","5ad318d5":"code","7d4abfd8":"code","e16dd76b":"code","dff9f9c2":"code","17e6b0d1":"code","d8989953":"code","e191529f":"code","597bd975":"code","0ee7816c":"code","96150e14":"code","dd126fb0":"code","ffc897d7":"code","c0a6f20d":"code","dccf26bf":"code","9c460742":"code","82e1bc92":"code","e60eab14":"code","81f1dc90":"code","47246e6d":"code","03bf408b":"code","c97fac37":"code","5cf56843":"code","4287b829":"code","fb4939d2":"code","14294a10":"code","429dd19d":"code","dfe42864":"code","aa538051":"code","f71b3015":"code","1661c158":"code","04a384cc":"code","6a876a31":"code","e7f25c57":"code","8baf6386":"code","992fa86d":"code","a8eddfa2":"code","3acc6372":"code","6009ae15":"code","a4d6f0d7":"code","15a91d06":"code","f25022dd":"markdown","1ab807a5":"markdown","a8c55e56":"markdown","fbfad6f0":"markdown","62101bc2":"markdown","f238a7a5":"markdown","163aa042":"markdown","f645fbb8":"markdown","6463d32c":"markdown","e115c5b7":"markdown","53437a15":"markdown","e92abf1e":"markdown","dd4ff85c":"markdown","d54c2f38":"markdown"},"source":{"bc6e52ea":"%matplotlib  inline\n\nimport numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nfrom fbprophet import Prophet\n\nimport os\nprint(os.listdir(\"..\/input\"))","ce5b36fa":"df = pd.read_csv(\"..\/input\/avocado.csv\", parse_dates=[\"Date\"])\n\ndel df['Unnamed: 0']\ncols = ['Date', 'AveragePrice', 'type', 'region']\ndf = df[cols]\ndf = df[(df.region =='TotalUS') & (df.type == 'conventional') ] #& (df.Date >= '2016-01-01')\n\ndel df['region']\ndel df['type']\n\ndf = df.sort_values(\"Date\")\n\ndf.columns = ['ds', 'y']\ndf.set_index('ds', inplace=True)\n\n# Train test split \ntrain = df[:-12]\ntest = df[-12:]\n\ntrain.info()","a115046b":"train.head()","24964f4e":"fig, ax = plt.subplots()\nfig.set_size_inches(12, 8)\n\nax = sns.scatterplot(x=train.index, y=train.y)\nax = sns.scatterplot(x=test.index, y=test.y)\n\nax.axes.set_xlim(train.index.min(), test.index.max());","31988209":"from pandas import DataFrame\nfrom datetime import datetime, timedelta\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom itertools import product\nfrom math import sqrt\nfrom sklearn.metrics import mean_squared_error \n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ncolors = [\"windows blue\", \"amber\", \"faded green\", \"dusty purple\"]\nsns.set(rc={\"figure.figsize\": (20,10), \"axes.titlesize\" : 18, \"axes.labelsize\" : 12, \n            \"xtick.labelsize\" : 14, \"ytick.labelsize\" : 14 })","81677905":"seasonal_decompose(train.y, model='additive').plot()\nprint(\"Dickey\u2013Fuller test: p=%f\" % adfuller(train.y)[1])","74a07e63":"train['y_box'], lmbda = stats.boxcox(train.y)\n\nseasonal_decompose(train.y_box, model='additive').plot()\nprint(\"Dickey\u2013Fuller test: p=%f\" % adfuller(train.y_box)[1])","efda5a76":"# Initial approximation of parameters using Autocorrelation and Partial Autocorrelation Plots\nax = plt.subplot(211)\n\n# Plot the autocorrelation function\nplot_acf(train.y_box[0:].values.squeeze(), lags=16, ax=ax)\nax = plt.subplot(212)\nplot_pacf(train.y_box[0:].values.squeeze(), lags=16, ax=ax)\nplt.tight_layout()","5ad318d5":"train['y_box_1d'] = train['y_box'].diff(periods=1)\ntrain.head()","7d4abfd8":"fig, ax_arr = plt.subplots(2,1)\n\nax_arr[0].plot(train.y_box)\nax_arr[1].plot(train.y_box_1d)\nplt.tight_layout();","e16dd76b":"# STL-decomposition\nseasonal_decompose(train.y_box_1d[1:]).plot()   \nprint(\"Dickey\u2013Fuller test: p=%f\" % adfuller(train.y_box_1d[1:])[1])","dff9f9c2":"# Initial approximation of parameters using Autocorrelation and Partial Autocorrelation Plots\nax = plt.subplot(211)\n# Plot the autocorrelation function\nplot_acf(train.y_box_1d[1:].values.squeeze(), lags=16, ax=ax)\nax = plt.subplot(212)\nplot_pacf(train.y_box_1d[1:].values.squeeze(), lags=16, ax=ax)\nplt.tight_layout()","17e6b0d1":"# Initial approximation of parameters\nps = range(0, 2)\nd = 1\nqs = range(0, 2)\n\nparameters = product(ps, qs)\nparameters_list = list(parameters)\nlen(parameters_list)","d8989953":"%%time \n\n# Model Selection\nresults = []\nbest_aic = float(\"inf\")\nwarnings.filterwarnings('ignore')\nfor param in parameters_list:\n    try:\n        model = SARIMAX(train.y_box, order=(param[0], d, param[1])).fit(disp=-1)\n    except ValueError:\n        print('bad parameter combination:', param)\n        continue\n    aic = model.aic\n    if aic < best_aic:\n        best_model = model\n        best_aic = aic\n        best_param = param\n    results.append([param, model.aic])","e191529f":"# Best Models\nresult_table = pd.DataFrame(results)\nresult_table.columns = ['parameters', 'aic']\nprint(result_table.sort_values(by='aic', ascending=True).head())","597bd975":"print(best_model.summary())","0ee7816c":"print(\"Dickey\u2013Fuller test:: p=%f\" % adfuller(best_model.resid[13:])[1])","96150e14":"best_model.plot_diagnostics(figsize=(15, 12))\nplt.show()","dd126fb0":"# Inverse Box-Cox Transformation Function\ndef invboxcox(y,lmbda):\n   if lmbda == 0:\n      return(np.exp(y))\n   else:\n      return(np.exp(np.log(lmbda*y+1)\/lmbda))","ffc897d7":"test['yhat_ARIMA'] = invboxcox(best_model.forecast(12), lmbda)\ntest['yhat_ARIMA'] = np.round(test.yhat_ARIMA, 2)\n\ntest.tail()","c0a6f20d":"test.y.plot(linewidth=3)\ntest.yhat_ARIMA.plot(color='r', ls='--', label='Predicted Units', linewidth=3)\n\nplt.legend()\nplt.grid()\nplt.title('Price - weekly forecast')\nplt.ylabel('$');","dccf26bf":"test['e'] = test.y - test.yhat_ARIMA\n\nrmse = np.sqrt(np.mean(test.e**2)).round(2)\nmape = np.round(np.mean(np.abs(100*test.e\/test.y)), 0)\n\nprint('RMSE = $', rmse)\nprint('MAPE =', mape, '%')","9c460742":"%%time \n\n# Model Selection\nresults = []\nbest_aic = float(\"inf\")\nwarnings.filterwarnings('ignore')\nfor param in parameters_list:\n    try:\n        model = SARIMAX(train.y_box, order=(param[0], d, param[1]), trend='ct').fit(disp=-1)\n    except ValueError:\n        print('bad parameter combination:', param)\n        continue\n    aic = model.aic\n    if aic < best_aic:\n        best_model = model\n        best_aic = aic\n        best_param = param\n    results.append([param, model.aic])","82e1bc92":"# Best Models\nresult_table = pd.DataFrame(results)\nresult_table.columns = ['parameters', 'aic']\nprint(result_table.sort_values(by = 'aic', ascending=True).head())","e60eab14":"print(best_model.summary())","81f1dc90":"print(\"Dickey\u2013Fuller test:: p=%f\" % adfuller(best_model.resid[13:])[1])","47246e6d":"best_model.plot_diagnostics(figsize=(15, 12))\nplt.show()","03bf408b":"test['yhat_ARIMAct'] = invboxcox(best_model.forecast(12), lmbda)\ntest['yhat_ARIMAct'] = np.round(test.yhat_ARIMAct, 2)\n\ntest.tail()","c97fac37":"test.y.plot(linewidth=3)\ntest.yhat_ARIMAct.plot(color='r', ls='--', label='Predicted Units', linewidth=3)\n\nplt.legend()\nplt.grid()\nplt.title('Price - weekly forecast')\nplt.ylabel('$');","5cf56843":"test['e'] = test.y - test.yhat_ARIMAct\n\nrmse = np.sqrt(np.mean(test.e**2)).round(2)\nmape = np.round(np.mean(np.abs(100*test.e\/test.y)), 0)\n\nprint('RMSE = $', rmse)\nprint('MAPE =', mape, '%')","4287b829":"%%time \n\n# Initial approximation of parameters\nQs = range(0, 2)\nqs = range(0, 3)\nPs = range(0, 3)\nps = range(0, 3)\nD=1\nd=1\nparameters = product(ps, qs, Ps, Qs)\nparameters_list = list(parameters)\nlen(parameters_list)\n\ni = 52 # weekly seasonality \n\n# Model Selection\nresults = []\nbest_aic = float(\"inf\")\nwarnings.filterwarnings('ignore')\nfor param in parameters_list:\n    try:\n        model = SARIMAX(train.y_box, order=(param[0], d, param[1]), seasonal_order=(param[2], D, param[3], i)).fit(disp=-1)\n    except ValueError:\n        print('bad parameter combination:', param)\n        continue\n    aic = model.aic\n    if aic < best_aic:\n        best_model = model\n        best_aic = aic\n        best_param = param\n    results.append([param, model.aic])","fb4939d2":"# Best Models\nresult_table = pd.DataFrame(results)\nresult_table.columns = ['parameters', 'aic']\nprint(result_table.sort_values(by = 'aic', ascending=True).head())\nprint(best_model.summary())","14294a10":"print(\"Dickey\u2013Fuller test:: p=%f\" % adfuller(best_model.resid[i+1:])[1])","429dd19d":"best_model.plot_diagnostics(figsize=(15, 12))\nplt.show()","dfe42864":"# STL-decomposition\nplt.subplot(211)\nbest_model.resid[i+1:].plot()\nplt.ylabel(u'Residuals')\nax = plt.subplot(212)\n\nplot_acf(best_model.resid[i+1:].values.squeeze(), lags=i, ax=ax)\n\nprint(\"Dickey\u2013Fuller test:: p=%f\" % adfuller(best_model.resid[i+1:])[1])\n\nplt.tight_layout()","aa538051":"test['yhat_SARIMA'] = invboxcox(best_model.forecast(12), lmbda)\ntest['yhat_SARIMA'] = np.round(test.yhat_SARIMA, 2)\n\ntest.tail()","f71b3015":"test.y.plot(linewidth=3)\ntest.yhat_SARIMA.plot(color='r', ls='--', label='Predicted Units', linewidth=3)\n\nplt.legend()\nplt.grid()\nplt.title('Price - weekly forecast')\nplt.ylabel('$');","1661c158":"test['e'] = test.y - test.yhat_SARIMA\n\nrmse = np.sqrt(np.mean(test.e**2)).round(2)\nmape = np.round(np.mean(np.abs(100*test.e\/test.y)), 0)\n\nprint('RMSE = $', rmse)\nprint('MAPE =', mape, '%')","04a384cc":"%%time \n\n# Initial approximation of parameters\nQs = range(0, 2)\nqs = range(0, 3)\nPs = range(0, 3)\nps = range(0, 3)\nD=1\nd=1\nparameters = product(ps, qs, Ps, Qs)\nparameters_list = list(parameters)\nlen(parameters_list)\n\ni = 52 # weekly seasonality \n\n# Model Selection\nresults = []\nbest_aic = float(\"inf\")\nwarnings.filterwarnings('ignore')\nfor param in parameters_list:\n    try:\n        model = SARIMAX(train.y_box, order=(param[0], d, param[1]), seasonal_order=(param[2], D, param[3], i), trend='ct').fit(disp=-1)\n    except ValueError:\n        print('bad parameter combination:', param)\n        continue\n    aic = model.aic\n    if aic < best_aic:\n        best_model = model\n        best_aic = aic\n        best_param = param\n    results.append([param, model.aic])","6a876a31":"# Best Models\nresult_table = pd.DataFrame(results)\nresult_table.columns = ['parameters', 'aic']\nprint(result_table.sort_values(by = 'aic', ascending=True).head())\nprint(best_model.summary())","e7f25c57":"print(\"Dickey\u2013Fuller test:: p=%f\" % adfuller(best_model.resid[i+1:])[1])","8baf6386":"best_model.plot_diagnostics(figsize=(15, 12))\nplt.show()","992fa86d":"# STL-decomposition\nplt.subplot(211)\nbest_model.resid[i+1:].plot()\nplt.ylabel(u'Residuals')\nax = plt.subplot(212)\n\nplot_acf(best_model.resid[i+1:].values.squeeze(), lags=i, ax=ax)\n\nprint(\"Dickey\u2013Fuller test:: p=%f\" % adfuller(best_model.resid[i+1:])[1])\n\nplt.tight_layout()","a8eddfa2":"test['yhat_SARIMAct'] = invboxcox(best_model.forecast(12), lmbda)\ntest['yhat_SARIMAct'] = np.round(test.yhat_SARIMAct, 2)\n\ntest.tail()","3acc6372":"test.y.plot(linewidth=3)\ntest.yhat_SARIMAct.plot(color='r', ls='--', label='Predicted Units', linewidth=3)\n\nplt.legend()\nplt.grid()\nplt.title('Price - weekly forecast')\nplt.ylabel('$');","6009ae15":"test['e'] = test.y - test.yhat_SARIMAct\n\nrmse = np.sqrt(np.mean(test.e**2)).round(2)\nmape = np.round(np.mean(np.abs(100*test.e\/test.y)), 0)\n\nprint('RMSE = $', rmse)\nprint('MAPE =', mape, '%')\n\ndel test['e']","a4d6f0d7":"test","15a91d06":"test.y.plot(linewidth=3)\n\ntest.yhat_ARIMA.plot(color='r', ls='--', label='ARIMA forecast', linewidth=3)\ntest.yhat_ARIMAct.plot(color='r', ls=':', label='ARIMA constant trend', linewidth=3)\ntest.yhat_SARIMA.plot(color='grey', ls='--', label='SARIMA', linewidth=3)\ntest.yhat_SARIMAct.plot(color='grey', ls=':', label='SARIMA constant trend', linewidth=3)\n\nplt.legend()\nplt.grid()\nplt.title('Price - weekly forecast')\nplt.ylabel('$');","f25022dd":"# Results so far","1ab807a5":"Note the AICs are negative but this is not a problem.\n\nUsually, AIC is positive; however, it can be shifted by any additive constant, and some shifts can result in negative values of AIC. It is not the absolute size of the AIC value, it is the relative values over the set of models considered, and particularly the differences between AIC values, that are important.","a8c55e56":"The p-value indicates that series is stationary.","fbfad6f0":"**Analysis of Results**\n\nThe coef column shows the weight (i.e. importance) of each feature and how each one impacts the time series. The P>|z| column informs us of the significance of each feature weight. Here, each weight has a p-value lower or close to 0.05, so it is reasonable to retain all of them in our model.\n\nWhen fitting seasonal ARIMA models (and any other models for that matter), it is important to run model diagnostics to ensure that none of the assumptions made by the model have been violated. The plot_diagnostics object allows us to quickly generate model diagnostics and investigate for any unusual behavior.","62101bc2":"# ARIMA with Constant trend","f238a7a5":"The p-value indicates that series is not stationary.\n\n# Box-Cox transformation","163aa042":"# Differencing \n## d=1","f645fbb8":"# SARIMA with constant trend","6463d32c":"# Upnext...\n## Do avocados and bread go together? Let's find out!","e115c5b7":"# Stationarity check and Seasonal decomposition","53437a15":"Our primary concern is to ensure that the residuals of our model are uncorrelated and normally distributed with zero-mean. If the seasonal ARIMA model does not satisfy these properties, it is a good indication that it can be further improved.\n\nIn the histogram (top right), the $KDE$ line should follow the $N(0,1)$ line (normal distribution with mean 0, standard deviation 1) closely. This is an indication whether the residuals are normally distributed or not.\n\nIn the Q-Q-plot the ordered distribution of residuals (blue dots) should follow the linear trend of the samples taken from a standard normal distribution with $N(0, 1)$. Again, this is an indication whether the residuals are normally distributed.\n\nThe standardized residual plot doesn't display any obvious seasonality.\n\nThis is confirmed by the autocorrelation plot, which shows that the time series residuals have low correlation with lagged versions of itself.\n\n## Prediction","e92abf1e":"# Only selecting `TotalUS` prices for `conventional` avocados","dd4ff85c":"# SARIMAX\nSeasonal AutoRegressive Integrated Moving Average with eXogenous regressors model\n\nWhen dealing with seasonal effects, we make use of the seasonal $ARIMA$, which is denoted as $ARIMA(p,d,q)(P,D,Q)s$. Here, $(p, d, q)$ are the non-seasonal parameters described above, while $(P, D, Q)$ follow the same definition but are applied to the seasonal component of the time series. The term $s$ is the periodicity of the time series (4 for quarterly periods, 12 for yearly periods, 52 for weekly periods etc.).\n\n### Parameter Selection\nThe same process of parameter selection for seasonal ARIMA. For each combination we fit a new seasonal ARIMA model with $SARIMAX()$ and assess its overall quality.","d54c2f38":"The p-value indicates that series is stationary as the computed p-value is lower than the significance level $\\alpha=0.01$.\n\nThere are not many spikes in the plots outside the insignificant zone (shaded) so there may not be enough information available in the residuals to be extracted by AR and MA models.\n\nThere may be a seasonal component available in the residuals at the lags of 4 weeks represented by spikes at these intervals. But probably not significant.\n\n# ARIMA Model\nAutoRegressive Integrated Moving Average\n\nARIMA models are denoted with the notation $ARIMA(p, d, q)$. These parameters account for seasonality, trend, and noise in datasets:\n\n* $p$ - the number of lag observations to include in the model, or lag order. ($AR$)\n* $d$ - the number of times that the raw observations are differenced, or the degree of differencing. ($I$)\n* $q$ - the size of the moving average window, also called the order of moving average.($MA$)\n\nA linear regression model is constructed including the specified number and type of terms, and the data is prepared by a degree of differencing in order to make it stationary, i.e. to remove trend and seasonal structures that negatively affect the regression model. A value of 0 for a parameter indicates to not use that element of the model.\n\n### Parameter Selection\nWe will iteratively explore different combinations of parameters. For each combination we fit a new ARIMA model with $SARIMAX()$ and assess its overall quality.\n\nWe will use the **AIC** (Akaike Information Criterion) value, returned with ARIMA models fitted using `statsmodels`.\n\nThe AIC measures how well a model fits the data while taking into account the overall complexity of the model. A model that fits the data very well while using lots of features will be assigned a larger AIC score than a model that uses fewer features to achieve the same goodness-of-fit. Therefore, we are interested in finding the model that yields the **lowest AIC value**.\n\n"}}