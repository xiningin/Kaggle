{"cell_type":{"1b81b535":"code","307806ce":"code","f94efd65":"code","7d2a0d91":"code","773414a9":"code","9bd35033":"code","1c6f521c":"code","ca13725c":"code","654e743b":"code","d7d8b14c":"code","89bbce0c":"code","bc2a440c":"code","2e0a53e8":"code","576c0079":"markdown","4edb7aeb":"markdown","e8336a86":"markdown","9ed9beb2":"markdown"},"source":{"1b81b535":"import torch\nfrom torchvision import datasets, transforms, models\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torch import nn, optim\nimport torch.nn.functional as F\nimport sys\nimport os\nfrom torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler\nimport albumentations as A\nfrom torch.optim import lr_scheduler\nimport time\nimport copy","307806ce":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","f94efd65":"np.random.seed(0)\ntorch.manual_seed(0)","7d2a0d91":"root_dir = \"..\/input\/cell-images-for-detecting-malaria\/cell_images\/cell_images\/\"\ncell_images = datasets.ImageFolder(root=root_dir)\ncell_images.class_to_idx","773414a9":"dataset_size = len(cell_images)\ndataset_indices = list(range(dataset_size))\nnp.random.shuffle(dataset_indices)\nval_split_index = int(np.floor(0.2 * dataset_size))\ntest_split_index = int(np.floor(0.1 * dataset_size))\ntrain_idx, val_idx, test_idx = dataset_indices[val_split_index+test_split_index:], \\\n                               dataset_indices[:val_split_index], \\\n                               dataset_indices[val_split_index:val_split_index+test_split_index]","9bd35033":"train_sampler = SubsetRandomSampler(train_idx)\nval_sampler = SubsetRandomSampler(val_idx)\ntest_sampler = SubsetRandomSampler(test_idx)","1c6f521c":"transform = {\n    \"train\": transforms.Compose([\n         transforms.Resize((224, 224)),\n         transforms.RandomHorizontalFlip(p=0.5),\n         transforms.RandomRotation(degrees=(-90, 90)),\n         transforms.RandomVerticalFlip(p=0.5),\n         transforms.ToTensor(),\n         transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n         ]),\n    \"validation\": transforms.Compose([\n         transforms.Resize((224, 224)),\n         transforms.ToTensor(),\n         transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n         ]),\n    \"test\": transforms.Compose([\n         transforms.Resize((224, 224)),\n         transforms.ToTensor(),\n         transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n         ])\n}","ca13725c":"dataloaders = {\n    \"train\": torch.utils.data.DataLoader(\n        datasets.ImageFolder(root=root_dir, transform=transform[\"train\"]), \n        sampler=SubsetRandomSampler(train_idx), batch_size=128\n    ),\n    \"validation\": torch.utils.data.DataLoader(\n        datasets.ImageFolder(root=root_dir, transform=transform[\"validation\"]), \n        sampler=SubsetRandomSampler(val_idx), batch_size=128\n    ),\n    \"test\": torch.utils.data.DataLoader(\n        datasets.ImageFolder(root=root_dir, transform=transform[\"test\"]), \n        sampler=SubsetRandomSampler(test_idx), batch_size=128\n    ),\n}","654e743b":"model = models.resnet34(pretrained=True)\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 2)","d7d8b14c":"model","89bbce0c":"model.to(device)\nloss_criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.1)","bc2a440c":"epochs = 10\n\nfor e in range(epochs):\n    print(\"In epoch\", e)\n    running_loss = 0\n    model.train()\n    for images, labels in dataloaders['train']: # processing one batch at a time\n        images, labels = images.to(device), labels.to(device)\n        predictions = model(images).to(device) # predict labels\n        loss = loss_criterion(predictions, labels) # calculate the loss\n        \n        # BACK PROPAGATION OF LOSS to generate updated weights\n        optimizer.zero_grad() # pytorch accumulates gradients from previous backwards\n                              # passes by default -- we want to zero them out;\n                              # you can read online why they have this implementation choice\n        loss.backward() # compute gradients by using the predictions' grad_fn\n                        # that was passed to loss_criterion() above -- this is how it\n                        # knows what model parameters need updating eventually\n                        # (this is confusing IMO and not obvious to those used to OOP)\n        optimizer.step() # using gradients just calculated for model parameters, \n                         # update the weights via the optimizer (which was init with those \n                         # model parameters)\n        \n        running_loss += loss.item()\n    valid_loss = 0\n    model.eval()\n    for images, labels in dataloaders['validation']: # processing one batch at a time\n        images, labels = images.to(device), labels.to(device)\n        with torch.no_grad():\n            predictions = model(images).to(device)\n            loss = loss_criterion(predictions, labels)\n        valid_loss += loss.item()\n    print(f\"\\tTraining loss: {running_loss\/len(dataloaders['train'])}\")\n    print(f\"\\tValidation loss: {valid_loss\/len(dataloaders['validation'])}\")    ","2e0a53e8":"r_loss = 0\ncorrects = 0\nfor images, labels in dataloaders['test']:\n    images, labels = images.to(device), labels.to(device)\n    with torch.no_grad():\n        predictions = model(images).to(device)\n        loss = loss_criterion(predictions, labels)\n        _, preds = torch.max(predictions, 1)\n        corrects += torch.sum(preds == labels.data)\/128\n        r_loss += loss.item()\ntest_acc = corrects.double() \/ len(dataloaders['test'])\ntest_loss = r_loss \/ (len(dataloaders['test']))\nprint('Holdout accuracy: ', test_acc.item())\nprint('Holdout loss: ', test_loss)","576c0079":"### Model","4edb7aeb":"### Read Dataset","e8336a86":"### Holdout\nThe model generalization is pretty good, since holdout accuracy is around 94% and loss is close to training and validation losses, which means the performance is very similar.","9ed9beb2":"### DataLoader"}}