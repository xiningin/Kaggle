{"cell_type":{"b8682c11":"code","76587b3c":"code","2a6371f3":"code","f5ef1def":"code","2f5c9757":"code","2be11793":"code","38b68523":"code","ccf5225a":"code","ecb34913":"code","54179f30":"code","39e1ffea":"code","6b12e0fe":"code","6d000522":"code","5f8c7561":"code","7878ad28":"code","769b9e41":"code","d1e918f1":"code","2c25c209":"code","fa648a66":"code","12570929":"code","f51f0cf6":"code","80b62b79":"code","06e7872e":"code","df9efd74":"code","3a896f44":"code","2d40f813":"code","b335e0d6":"code","30e2e92e":"code","42520b70":"code","12cebcf3":"code","b7538f14":"code","6efdcb95":"code","3f1effb0":"code","d80eebba":"code","033e6f96":"code","17968b94":"code","660ea783":"code","8bbfc4a3":"code","f4c6f6bc":"code","4d6142fe":"code","748a00da":"code","fd68acef":"code","d76c05d2":"code","4ee29113":"code","e61e8f0b":"code","e6229dd1":"code","e47e9d05":"code","f073d526":"code","c87f9f5b":"code","590b8e59":"code","ad7480e6":"code","a4ade920":"code","9248588d":"code","527f4c43":"code","773750a6":"code","27641d85":"code","73c69885":"code","ae3ae613":"code","b8dfb423":"code","6341ff2f":"code","a2c19f8b":"code","584ca5e3":"code","c81de3d4":"code","260a40ad":"code","9e9d5b2c":"code","951fefd5":"code","1133f3e3":"code","eaae589a":"code","e74cf787":"code","aeedd2a6":"code","f8764e59":"code","02039723":"code","b30da6db":"code","b57b1da3":"code","05e3b8f9":"code","ff163f1b":"code","e306ba86":"markdown","47801fdc":"markdown","b5838324":"markdown","a6a383ed":"markdown","3752274e":"markdown","ac897a0a":"markdown","21db3bfe":"markdown","840f465a":"markdown","adeb8710":"markdown","53f8ceb4":"markdown","7b6b8c17":"markdown","953056f0":"markdown","e0487d36":"markdown","b92da5dd":"markdown","7237f541":"markdown","47efe270":"markdown","fdad25f0":"markdown","9d52546c":"markdown","00b929d0":"markdown","af4eedbb":"markdown","a68c0931":"markdown","973de722":"markdown","d357aa98":"markdown","68d4ad31":"markdown","cb0c3e85":"markdown","40e0f4ce":"markdown","2e3f28a2":"markdown","c0440f0a":"markdown","ec3675b8":"markdown","8b5db703":"markdown"},"source":{"b8682c11":"# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","76587b3c":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2a6371f3":"# Importing Pandas and NumPy\nimport pandas as pd, numpy as np, seaborn as sns,matplotlib.pyplot as plt","f5ef1def":"pd.set_option('display.max_columns', None)","2f5c9757":"# Importing all datasets\nchurn_data = pd.read_csv(\"\/kaggle\/input\/telecom-churn-data-sets\/churn_data.csv\")\nchurn_data.head()","2be11793":"customer_data = pd.read_csv(\"\/kaggle\/input\/telecom-churn-data-sets\/customer_data.csv\")\ncustomer_data.head()","38b68523":"internet_data = pd.read_csv(\"\/kaggle\/input\/telecom-churn-data-sets\/internet_data.csv\")\ninternet_data.head()","ccf5225a":"# Merging on 'customerID'\ndf_1 = pd.merge(churn_data, customer_data, how='inner', on='customerID')","ecb34913":"# Final dataframe with all predictor variables\ntelecom = pd.merge(df_1, internet_data, how='inner', on='customerID')","54179f30":"# Let's see the head of our master dataset\ntelecom.head()","39e1ffea":"# Let's check the dimensions of the dataframe\ntelecom.shape","6b12e0fe":"# let's look at the statistical aspects of the dataframe\ntelecom.describe()","6d000522":"# Let's see the type of each column\ntelecom.info()","5f8c7561":"#The varaible was imported as a string we need to convert it to float\n# telecom['TotalCharges'] = telecom['TotalCharges'].astype(float) \ntelecom.TotalCharges = pd.to_numeric(telecom.TotalCharges, errors='coerce')","7878ad28":"telecom.info()","769b9e41":"\nplt.figure(figsize=(20,40))\nplt.subplot(10,2,1)\nax = sns.distplot(telecom['tenure'], hist=True, kde=False, \n             bins=int(180\/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\nax.set_ylabel('# of Customers')\nax.set_xlabel('Tenure (months)')\nplt.subplot(10,2,2)\nax = sns.countplot(x='PhoneService', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,3)\nax =sns.countplot(x='Contract', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,3)\nax =sns.countplot(x='Contract', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,4)\nax =sns.countplot(x='PaperlessBilling', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,5)\nax =sns.countplot(x='PaymentMethod', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,6)\nax =sns.countplot(x='Churn', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,7)\nax =sns.countplot(x='gender', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,8)\nax =sns.countplot(x='SeniorCitizen', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,9)\nax =sns.countplot(x='Partner', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,10)\nax =sns.countplot(x='Dependents', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,11)\nax =sns.countplot(x='MultipleLines', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,12)\nax =sns.countplot(x='InternetService', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,13)\nax =sns.countplot(x='OnlineSecurity', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,14)\nax =sns.countplot(x='OnlineBackup', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,15)\nax =sns.countplot(x='DeviceProtection', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,16)\nax =sns.countplot(x='TechSupport', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,17)\nax =sns.countplot(x='StreamingTV', data=telecom)\nax.set_ylabel('# of Customers')\n\nplt.subplot(10,2,18)\nax =sns.countplot(x='StreamingMovies', data=telecom)\nax.set_ylabel('# of Customers')\nplt.subplot(10,2,19)\nax = sns.distplot(telecom['MonthlyCharges'], hist=True, kde=False, \n             bins=int(180\/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\nax.set_ylabel('# of Customers')\nax.set_xlabel('MonthlyCharges')\nplt.subplot(10,2,20)\nax = sns.distplot(telecom['TotalCharges'], hist=True, kde=False, \n             bins=int(180\/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\nax.set_ylabel('# of Customers')\nax.set_xlabel('TotalCharges');","d1e918f1":"sns.pairplot(telecom)\nplt.show()","2c25c209":"plt.figure(figsize=(25, 10))\nplt.subplot(1,3,1)\nsns.boxplot(x = 'tenure', y = 'Churn', data=telecom)\nplt.subplot(1,3,2)\nsns.boxplot(x = 'MonthlyCharges', y = 'Churn', data=telecom)\nplt.subplot(1,3,3)\nsns.boxplot(x = 'TotalCharges', y = 'Churn', data=telecom)\nplt.show()","fa648a66":"# List of variables to map\n\nvarlist =  ['PhoneService', 'PaperlessBilling', 'Churn', 'Partner', 'Dependents']\n\n# Defining the map function\ndef binary_map(x):\n    return x.map({'Yes': 1, \"No\": 0})\n\n# Applying the function to the housing list\ntelecom[varlist] = telecom[varlist].apply(binary_map)","12570929":"telecom.head()","f51f0cf6":"# Creating a dummy variable for some of the categorical variables and dropping the first one.\ndummy1 = pd.get_dummies(telecom[['Contract', 'PaymentMethod', 'gender', 'InternetService']], drop_first=True)\n\n# Adding the results to the master dataframe\ntelecom = pd.concat([telecom, dummy1], axis=1)","80b62b79":"telecom.head()","06e7872e":"# Creating dummy variables for the remaining categorical variables and dropping the level with big names.\n\n# Creating dummy variables for the variable 'MultipleLines'\nml = pd.get_dummies(telecom['MultipleLines'], prefix='MultipleLines')\n# Dropping MultipleLines_No phone service column\nml1 = ml.drop(['MultipleLines_No phone service'], 1)\n#Adding the results to the master dataframe\ntelecom = pd.concat([telecom,ml1], axis=1)\n\n# Creating dummy variables for the variable 'OnlineSecurity'.\nos = pd.get_dummies(telecom['OnlineSecurity'], prefix='OnlineSecurity')\nos1 = os.drop(['OnlineSecurity_No internet service'], 1)\n# Adding the results to the master dataframe\ntelecom = pd.concat([telecom,os1], axis=1)\n\n# Creating dummy variables for the variable 'OnlineBackup'.\nob = pd.get_dummies(telecom['OnlineBackup'], prefix='OnlineBackup')\nob1 = ob.drop(['OnlineBackup_No internet service'], 1)\n# Adding the results to the master dataframe\ntelecom = pd.concat([telecom,ob1], axis=1)\n\n# Creating dummy variables for the variable 'DeviceProtection'. \ndp = pd.get_dummies(telecom['DeviceProtection'], prefix='DeviceProtection')\ndp1 = dp.drop(['DeviceProtection_No internet service'], 1)\n# Adding the results to the master dataframe\ntelecom = pd.concat([telecom,dp1], axis=1)\n\n# Creating dummy variables for the variable 'TechSupport'. \nts = pd.get_dummies(telecom['TechSupport'], prefix='TechSupport')\nts1 = ts.drop(['TechSupport_No internet service'], 1)\n# Adding the results to the master dataframe\ntelecom = pd.concat([telecom,ts1], axis=1)\n\n# Creating dummy variables for the variable 'StreamingTV'.\nst =pd.get_dummies(telecom['StreamingTV'], prefix='StreamingTV')\nst1 = st.drop(['StreamingTV_No internet service'], 1)\n# Adding the results to the master dataframe\ntelecom = pd.concat([telecom,st1], axis=1)\n\n# Creating dummy variables for the variable 'StreamingMovies'. \nsm = pd.get_dummies(telecom['StreamingMovies'], prefix='StreamingMovies')\nsm1 = sm.drop(['StreamingMovies_No internet service'], 1)\n# Adding the results to the master dataframe\ntelecom = pd.concat([telecom,sm1], axis=1)","df9efd74":"telecom.head()","3a896f44":"# We have created dummies for the below variables, so we can drop them\ntelecom = telecom.drop(['Contract','PaymentMethod','gender','MultipleLines','InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n       'TechSupport', 'StreamingTV', 'StreamingMovies'], 1)","2d40f813":"# Checking for outliers in the continuous variables\nnum_telecom = telecom[['tenure','MonthlyCharges','SeniorCitizen','TotalCharges']]","b335e0d6":"# Checking outliers at 25%, 50%, 75%, 90%, 95% and 99%\nnum_telecom.describe(percentiles=[.25, .5, .75, .90, .95, .99])","30e2e92e":"# Adding up the missing values (column-wise)\ntelecom.isnull().sum()","42520b70":"print('No. of Null Records for TotalCharges:',telecom.TotalCharges.isnull().sum())","12cebcf3":"print('No. of Records for TotalCharges:',len(telecom))","b7538f14":"print('No. of non Records for TotalCharges:',len(telecom)-telecom.TotalCharges.isnull().sum())","6efdcb95":"# Checking the percentage of missing values\nround(100*(telecom.isnull().sum()\/len(telecom.index)), 2)","3f1effb0":"telecom = telecom.dropna()\ntelecom = telecom.reset_index(drop=True)\n\n","d80eebba":"# Checking percentage of missing values after removing the missing values\nround(100*(telecom.isnull().sum()\/len(telecom.index)), 2)","033e6f96":"from sklearn.model_selection import train_test_split","17968b94":"# Putting feature variable to X\nX = telecom.drop(['Churn','customerID'], axis=1)\n\nX.head()","660ea783":"# Putting response variable to y\ny = telecom['Churn']\n\ny.head()","8bbfc4a3":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","f4c6f6bc":"from sklearn.preprocessing import StandardScaler","4d6142fe":"scaler = StandardScaler()\n\nX_train[['tenure','MonthlyCharges','TotalCharges']] = scaler.fit_transform(X_train[['tenure','MonthlyCharges','TotalCharges']])\n\nX_train.head()","748a00da":"X_test[['tenure','MonthlyCharges','TotalCharges']] = scaler.transform(X_test[['tenure','MonthlyCharges','TotalCharges']])\n\nX_test.head()","fd68acef":"### Checking the Churn Rate\nchurn = (sum(telecom['Churn'])\/len(telecom['Churn'].index))*100\nchurn","d76c05d2":"# Importing matplotlib and seaborn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","4ee29113":"# Let's see the correlation matrix \nplt.figure(figsize = (25,25))        # Size of the figure\nsns.heatmap(telecom.corr(),annot = True,cmap=\"tab20c\")\nplt.show()","e61e8f0b":"plt.figure(figsize=(10,8))\ntelecom.corr()['Churn'].sort_values(ascending = False).plot(kind='bar');","e6229dd1":"X_test = X_test.drop(['MultipleLines_No','OnlineSecurity_No','OnlineBackup_No','DeviceProtection_No','TechSupport_No',\n                       'StreamingTV_No','StreamingMovies_No'], 1)\nX_train = X_train.drop(['MultipleLines_No','OnlineSecurity_No','OnlineBackup_No','DeviceProtection_No','TechSupport_No',\n                         'StreamingTV_No','StreamingMovies_No'], 1)","e47e9d05":"plt.figure(figsize = (25,25))\nsns.heatmap(X_train.corr(),annot = True,cmap=\"tab20c\")\nplt.show()","f073d526":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nmodel = DecisionTreeClassifier()","c87f9f5b":"# fit the model with the training data\nmodel.fit(X_train,y_train)","590b8e59":"# predict the target on the train dataset\npredict_train = model.predict(X_train)\npredict_train","ad7480e6":"trainaccuracy = accuracy_score(y_train,predict_train)\nprint('accuracy_score on train dataset : ', trainaccuracy)","a4ade920":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif.tail()","9248588d":"features_to_remove = vif.loc[vif['VIF'] >= 4.99,'Features'].values\nfeatures_to_remove = list(features_to_remove)\nprint(features_to_remove)","527f4c43":"X_train = X_train.drop(columns=features_to_remove, axis = 1)\nX_train.head()","773750a6":"X_test = X_test.drop(columns=features_to_remove, axis = 1)\nX_test.head()","27641d85":"# fit the model with the training data\nmodel.fit(X_train,y_train)","73c69885":"# predict the target on the train dataset\npredict_train = model.predict(X_train)\npredict_train","ae3ae613":"trainaccuracy = accuracy_score(y_train,predict_train)\nprint('accuracy_score on train dataset : ', trainaccuracy)","b8dfb423":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","6341ff2f":"from sklearn import metrics\n# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train, predict_train )\nprint(confusion)\n","a2c19f8b":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","584ca5e3":"# Let's see the sensitivity of our model\ntrainsensitivity= TP \/ float(TP+FN)\ntrainsensitivity","c81de3d4":"# Let us calculate specificity\ntrainspecificity= TN \/ float(TN+FP)\ntrainspecificity","260a40ad":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","9e9d5b2c":"# Positive predictive value \nprint (TP \/ float(TP+FP))","951fefd5":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","1133f3e3":"draw_roc(y_train,predict_train)","eaae589a":"#Looking at the confusion matrix again","e74cf787":"from sklearn.metrics import precision_score, recall_score\nprecision_score(y_train,predict_train)","aeedd2a6":"recall_score(y_train,predict_train)","f8764e59":"# predict the target on the test dataset\npredict_test = model.predict(X_test)\nprint('Target on test data\\n\\n',predict_test)","02039723":"confusion2 = metrics.confusion_matrix(y_test, predict_test )\nprint(confusion2)","b30da6db":"# Let's check the overall accuracy.\ntestaccuracy= accuracy_score(y_test,predict_test)\ntestaccuracy","b57b1da3":"# Let's see the sensitivity of our lmodel\ntestsensitivity=TP \/ float(TP+FN)\ntestsensitivity","05e3b8f9":"# Let us calculate specificity\ntestspecificity= TN \/ float(TN+FP)\ntestspecificity","ff163f1b":"# Let us compare the values obtained for Train & Test:\nprint(\"Train Data Accuracy    :{} %\".format(round((trainaccuracy*100),2)))\nprint(\"Train Data Sensitivity :{} %\".format(round((trainsensitivity*100),2)))\nprint(\"Train Data Specificity :{} %\".format(round((trainspecificity*100),2)))\nprint(\"Test Data Accuracy     :{} %\".format(round((testaccuracy*100),2)))\nprint(\"Test Data Sensitivity  :{} %\".format(round((testsensitivity*100),2)))\nprint(\"Test Data Specificity  :{} %\".format(round((testspecificity*100),2)))","e306ba86":"We have almost 27% churn rate","47801fdc":"### Step 7: Model Building\nLet's start by splitting our data into a training set and a test set.","b5838324":"# EDA","a6a383ed":"## Telecom Churn Case Study\nWith 21 predictor variables we need to predict whether a particular customer will switch to another telecom provider or not. In telecom terminology, this is referred to as churning and not churning, respectively.","3752274e":"### Step 6: Looking at Correlations","ac897a0a":"#### Converting some binary variables (Yes\/No) to 0\/1","21db3bfe":"It means that 11 * 100\/7043 = 0.1561834%, best is to remove these observations from the analysis","840f465a":"#### Checking for Missing Values and Inputing Them","adeb8710":"#### Checking for Outliers","53f8ceb4":"#### Dropping highly correlated dummy variables","7b6b8c17":"#### Dropping the repeated variables","953056f0":"After dropping highly correlated variables now let's check the correlation matrix again.","e0487d36":"#### Combining all data files into one consolidated dataframe","b92da5dd":"From the distribution shown above, you can see that there no outliers in your data. The numbers are gradually increasing.","7237f541":"# Plotting the ROC Curve","47efe270":"### Step 5: Feature Scaling","fdad25f0":"### Step 11: Making predictions on the test set","9d52546c":"### Step 4: Test-Train Split","00b929d0":"# Decision Tree","af4eedbb":"#### Checking the Correlation Matrix","a68c0931":"# VIF","973de722":"### Step 2: Inspecting the Dataframe","d357aa98":"# VIF","68d4ad31":"### Step 1: Importing and Merging Data","cb0c3e85":"Now you can see that you have all variables as numeric.","40e0f4ce":"# Final Observation:","2e3f28a2":"Now we don't have any missing values","c0440f0a":"## Precision and Recall","ec3675b8":"#### For categorical variables with multiple levels, create dummy features (one-hot encoded)","8b5db703":"It is a type of supervised learning algorithm that is mostly used for classification problems. Surprisingly, it works for both categorical and continuous dependent variables. In this algorithm, we split the population into two or more homogeneous sets. This is done based on most significant attributes\/ independent variables to make as distinct groups as possible."}}