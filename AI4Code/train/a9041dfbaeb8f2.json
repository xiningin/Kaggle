{"cell_type":{"33427b08":"code","ff95efe2":"code","a89332f1":"code","4213ce5b":"code","ed82d6b9":"code","20f6aaa9":"code","215ac397":"code","d4bfc919":"code","47af0fb3":"code","dbae1374":"code","622eb08d":"code","0604fd8c":"code","d25292b0":"code","42e5ce6c":"code","085dced3":"code","35f2b96d":"code","7f544d0b":"code","ac6fa18d":"code","7f2ce610":"code","ec8202d7":"code","c99719f5":"code","7159b4ae":"code","331653fb":"code","47869e34":"markdown","493d7a9a":"markdown","b096a91b":"markdown","bc721fdc":"markdown","6c9ce755":"markdown","7d38535f":"markdown","e83ca3e8":"markdown","393392c7":"markdown","a880a986":"markdown","c211d1f0":"markdown","92bc5e3d":"markdown","3bd127bb":"markdown","c8477128":"markdown","70428f58":"markdown","d790944d":"markdown","c5797c12":"markdown"},"source":{"33427b08":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ff95efe2":"#importing the libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport string, os\nimport nltk\nimport re\nimport keras\nimport random\nimport io\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Dropout\nfrom keras.optimizers import Adamax\nimport sys\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom PIL import Image, ImageDraw, ImageFont\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","a89332f1":"data = pd.read_csv(\"..\/input\/lyrics\/Songs.csv\")\ndata.head()","4213ce5b":"#Printing the names of artists in the lyrics data\nprint(\"Artists in the data:\\n\",data.Artist.value_counts()) ","ed82d6b9":"#Printing the size of dataset\nprint(\"Size of Dataset:\",data.shape)","20f6aaa9":"#Adding a column of numbers of Characters,words and sentences in each msg\ndata[\"No_of_Characters\"] = data[\"Lyrics\"].apply(len)\ndata[\"No_of_Words\"]=data.apply(lambda row: nltk.word_tokenize(row[\"Lyrics\"]), axis=1).apply(len)\ndata[\"No_of_Lines\"] = data[\"Lyrics\"].str.split('\\n').apply(len)\ndata.describe()","215ac397":"#Plotting the comparative song lengths for various artists\nplt.figure(figsize=(15,15))\nax = sns.pairplot(data, hue=\"Artist\", palette=\"plasma\")","d4bfc919":"# Generate a word cloud image\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(stopwords=stopwords, background_color=\"#444160\",colormap=\"Purples\", max_words=800).generate(\" \".join(data[\"Lyrics\"]))\nplt.figure(figsize=(12,12))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.show()","47af0fb3":"#A function to disply the song in an asthetically pleasing way! lol\ndef My_song(song):\n    img = Image.open(\"..\/input\/image-for-notebook\/Pink and White Geometric Marketing Presentation (1).png\")\n    Text_on_image = ImageDraw.Draw(img)\n    myFont = ImageFont.truetype(\"..\/input\/font-style\/DancingScript-VariableFont_wght.ttf\", 45)\n    Text_on_image.text((620,90), song, font=myFont, fill =(255, 255, 255))\n    return img    \n#Having a look at the first 500 charachters of a random song lyrics\nMy_song(data.Lyrics[42][:500])","dbae1374":"#Lining up all the lyrics to create corpus\nCorpus =''\nfor listitem in data.Lyrics:\n    Corpus += listitem\n    \nCorpus = Corpus.lower() #converting all alphabets to lowecase \nprint(\"Number of unique characters:\", len(set(Corpus)))","622eb08d":"#To See all the unique characters present in the Corpus\nprint(\"The unique characters:\",sorted(set(Corpus)))","0604fd8c":"#Keeping only a limited set of characters. \nto_remove = ['{', '}', '~', '\u00a9', '\u00e0', '\u00e1', '\u00e3', '\u00e4', '\u00e7', '\u00e8', '\u00e9', '\u00ea', '\u00eb', '\u00ed', '\u00f1', '\u00f3', '\u00f6', '\u00fc', '\u014f',\n             '\u0435', '\u0627', '\u0633', '\u0644', '\u0645', '\u0648', '\\u2005', '\\u200a', '\\u200b', '\u2013', '\u2014', '\u2018', '\u2019', '\u201a', '\u201c', '\u201d', \n             '\u2026', '\\u205f', '\\ufeff', '!', '&', '(', ')', '*', '-',  '\/', ]\nfor symbol in to_remove:\n    Corpus = Corpus.replace(symbol,\" \")\n\n#Corpus = re.sub(\"[^A-Za-z0-9'\\.\\n]\",\"\",Corpus) Alterneativly could be used but I want to pick and chose (:","d25292b0":"#To See all the unique characters present in the Corpus\nprint(\"The unique characters:\",sorted(set(Corpus)))","42e5ce6c":"# Storing all the unique characters present in my corpus to bult a mapping dic. \nsymb = sorted(list(set(Corpus)))\n\nL_corpus = len(Corpus) #length of corpus\nL_symb = len(symb) #length of total unique characters\n\n#Building dictionary to access the vocabulary from indices and vice versa\nmapping = dict((c, i) for i, c in enumerate(symb))\nreverse_mapping = dict((i, c) for i, c in enumerate(symb))\n\nprint(\"Total number of characters:\", L_corpus)\nprint(\"Number of unique characters:\", L_symb)","085dced3":"#Splitting the Corpus in equal length of strings and output target\nlength = 40\nfeatures = []\ntargets = []\nfor i in range(0, L_corpus - length, 1):\n    feature = Corpus[i:i + length]\n    target = Corpus[i + length]\n    features.append([mapping[j] for j in feature])\n    targets.append(mapping[target])\n    \n    \nL_datapoints = len(targets)\nprint(\"Total number of sequences in the Corpus:\", L_datapoints)","35f2b96d":"# reshape X and normalize\nX = (np.reshape(features, (L_datapoints, length, 1)))\/ float(L_symb)\n\n# one hot encode the output variable\ny = np_utils.to_categorical(targets)","7f544d0b":"#Initialising the Model\nmodel = Sequential()\n#Adding layers\nmodel.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\nmodel.add(Dense(y.shape[1], activation='softmax'))\n#Compiling the model for training  \nopt = Adamax(learning_rate=0.01)\nmodel.compile(loss='categorical_crossentropy', optimizer=opt)\n\n#Model's Summary               \nmodel.summary()\n\n","ac6fa18d":"#Training the Model\nhistory = model.fit(X, y, batch_size=128, epochs=100)","7f2ce610":"from keras.models import load_model\n#To be used later; I am saving the model \nmodel.save(\"Lyrics_Generator.h5\")","ec8202d7":"history_df = pd.DataFrame(history.history)\n#Plotting the learnings \n\nfig = plt.figure(figsize=(15,4), facecolor=\"#B291B6\")\nfig.suptitle(\"Learning Plot of Model for Loss\")\npl=sns.lineplot(data=history_df[\"loss\"],color=\"#444160\")\npl.set(ylabel =\"Training Loss\")\npl.set(xlabel =\"Epochs\")","c99719f5":"# The function to generate text from model\ndef Lyrics_Generator(starter,Ch_count): #,temperature=1.0):\n    generated= \"\"\n    starter = starter \n    seed=[mapping[char] for char in starter]\n    generated += starter \n    # Generating new text of given length\n    for i in range(Ch_count):\n        seed=[mapping[char] for char in starter]\n        x_pred = np.reshape(seed, (1, len(seed), 1))\n        x_pred = x_pred\/ float(L_symb)\n        prediction = model.predict(x_pred, verbose=0)[0]  \n        # Getting the index of the next most probable index\n        prediction = np.asarray(prediction).astype('float64')\n        prediction = np.log(prediction) \/ 1.0 \n        exp_preds = np.exp(prediction)\n        prediction = exp_preds \/ np.sum(exp_preds)\n        probas = np.random.multinomial(1, prediction, 1)\n        index = np.argmax(prediction)\n        next_char = reverse_mapping[index]  \n        # Generating new text\n        generated += next_char\n        starter = starter[1:] + next_char\n       \n    return generated","7159b4ae":"#Generating a song from the model\nsong_1 = Lyrics_Generator(\"the shoe shrunk, and the school belt got ridiculously petit\", 400)\n#Let's have a look at the song\nMy_song(song_1)","331653fb":"#Generating a song from the model using a song out of the corpus\nsong_2 = Lyrics_Generator(\"i'm a sunflower, a little funny\", 400)\n#Let's have a look at the song\nMy_song(song_2)","47869e34":"So I have a total of 745 songs\n\n**I will do a little feature engineering to extract more information on the songs such as:**\n* Number of characters\n* Number of words\n* Number of lines","493d7a9a":"<a id=\"7\"><\/a>\n# <p style=\"background-color:#B291B6;font-family:newtimeroman;font-size:150%;color:#444160;text-align:center;border-radius:20px 60px;\">CONCLUSION<\/p>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160\">On observing the output of the Lyrics Generator, it is clear that while some of the sentences might be correct, but most of the lyrics do not make sense. It does look like a song tho. The model didn't learn the meaning of the songs. However, the character-based approach is producing some legitimate words. \nTo get to a song that makes better sense I may consider a transformer-based text generator, but that's for some other time.<\/p>\n\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\"> Neural networks amaze me all the time. There is something surreal about them that makes working with it exciting. Of course, we can peel off the layers and see the maths behind them. Get to the matrices and tensor to understand how these neurons are working. Even get the values of weights and biases and assure ourselves that this is no sorcery.  Still, when I see the result play out it is astonishing.<\/p>  \n\n**Some useful resources on Text generation:**\n\n[The Unreasonable Effectiveness of Recurrent Neural Networks](http:\/\/karpathy.github.io\/2015\/05\/21\/rnn-effectiveness\/)\n\n[RNN ](https:\/\/towardsdatascience.com\/character-level-language-model-1439f5dd87fe)\n\n[GTP-2 ](https:\/\/openai.com\/blog\/better-language-models\/) \n\n\n**<span style=\"color:#444160;\"> If you liked this Notebook, please do upvote.<\/span>**\n\n**<span style=\"color:#444160;\"> Best Wishes!<\/span>**\n\n<a id=\"8\"><\/a>\n# <p style=\"background-color:#B291B6;font-family:newtimeroman;font-size:150%;color:#444160;text-align:center;border-radius:20px 60px;\">END<\/p>","b096a91b":"**Generating the songs**","bc721fdc":"# <p style=\"background-color:#B291B6;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">Lyrics Generator<\/p>\n\n<img src=\"https:\/\/github.com\/KarnikaKapoor\/Files\/blob\/main\/Pink%20and%20White%20Geometric%20Marketing%20Presentation.gif?raw=true\">\n\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">In this project, I will be building a model to generate text. My goal is to build a song lyrics generator to explore the \"creative\" side of the Recurrent Neural Networks(RNN). RNN Text generator is one of my most desired to-do projects. I am finally checking this one off my to-do list. So yeyyy! <\/p> \n\n<a id='top'><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<p style=\"background-color:#B291B6;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">TABLE OF CONTENTS<\/p>   \n    \n* [1. IMPORTING LIBRARIES](#1)\n    \n* [2. LOADING DATA](#2)\n    \n* [3. DATA EXPLORATION](#3)  \n    \n* [4. DATA PREPREPROCESSING](#4)  \n    \n* [5. MODEL BUILDING](#5) \n      \n* [6. EVALUATING MODELS](#6)\n    \n* [7. CONCLUSION](#7)\n    \n* [8. END](#8)\n\n\n<a id=\"1\"><\/a>\n# <p style=\"background-color:#B291B6;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">IMPORTING LIBRARIES<\/p>\n","6c9ce755":"Let us finally see the outcome by putting a seed in from one of my old blogposts [blogpost](https:\/\/karnikakapoor.blogspot.com\/2017\/04\/killers-confession.html)","7d38535f":"<a id=\"5\"><\/a>\n# <p style=\"background-color:#B291B6;font-family:newtimeroman;font-size:150%;color:#444160;text-align:center;border-radius:20px 60px;\">MODEL BUILDING<\/p>\n\n\nRecurrent Neural Networks are pretty popular with generating text. In this project, I will be using a LSTM Model, an improved version of a standard recurrent neural network\n\n**Following steps are involved in the model building**\n\n* Initialising the Model\n* Defining by adding layers\n* Compiling the Model\n* Training the Model\n\n**Building the Model**","e83ca3e8":"<a id=\"2\"><\/a>\n# <p style=\"background-color:#B291B6;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">LOADING DATA<\/p>\nFor this project, I have prepared a dataset of song lyrics. Let's load it and have a look.","393392c7":"The generation of text with RNN involves the following workflow.  \n\n<p style=\"background-color:#B3C5E3;font-family:newtimeroman;color:#444160;text-align:center;font-size:120%;\">Loading Data \u27a1\ufe0f Preprocessing \u27a1\ufe0f Building Mapping Dictionary \u27a1\ufe0f Building Model \u27a1\ufe0f Generating Text<\/p>\n\nAs I have loaded and explored the data,  I will proceed further by pre-processing the text.  \n\n\n<a id=\"4\"><\/a>\n# <p style=\"background-color:#B291B6;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">DATA PREPROCESSING<\/p>\n\n**In this section, I will be performing the following:**\n\n**Creating a Corpus of Lyrics text:** For the model, we need a sequence of the text string. I am creating a Corpus out of Lyrics column. \n\n**Removing the unrequired characters that may have sneaked in my text corpus:** The data cleaning process for NLP is crucial preprocessing. To do that, I look into the Corpus to check for what this Corpus is comprised of. That is, all the unique symbols present. After examining the Corpus, I will be eliminating any foreign language or irrelevant symbols from the Corpus. \n\n**Creating a dictionary to map characters and their indices:** The computer doesn\u2019t understand the text. For the computer, the text is just a cluster of symbols. It works with numbers. So we create a dictionary to map each unique character in our Corpus to a number and vice versa. This will be used to encode and decode the information going in and getting out of the RNN\n\n**Splitting the corpus into smaller sentences of equal length:** Encoding and splitting the corpus into smaller sequences of equal length: At this point, Corpus contain only intended characters (i.e, lower cap English alphabets, Numbers and a few punctuations). We will encode this corpus and create small sequences of equal lengths of features and the corresponding targets. Each feature and target will contain the mapped index in the dictionary of the unique characters they signify. \n\nThe labels are then resized and normalized. Whereas the targets are one-hot encoded. Ready to be sent to the RNN for the training, but before that let us built the RNN model. \n\n**Creating a Corpus**","a880a986":"To be noted: On average our songs have 1400-ish characters","c211d1f0":"The total number of unique characters present in the Corpus clearly shows, that some of the foreign language scripts have sneaked in. I will take a look at all the characters present. I will then remove the unrequired characters. ","92bc5e3d":"**Creating a list of sorted unique characters**","3bd127bb":"Another song generated by a seed of the lyrics of a song that's stuck in my head today. (Sunflower by Shannon Purser)","c8477128":"<a id=\"6\"><\/a>\n# <p style=\"background-color:#B291B6;font-family:newtimeroman;font-size:150%;color:#444160;text-align:center;border-radius:20px 60px;\">EVALUATING MODELS<\/p>\n\nNow that I have my model trained on the songs lyrics let us see how it performs. I hope it creates some sensible song.\n\n**To evaluate my model, I shall be having a look at:**\n* The performance of the model via Learning Curves\n* The outcome text it generates\n\n**Plotting the learning curve for the loss function**","70428f58":"**Encoding the Labels and Targets**","d790944d":"*Unsurprisingly one of the most frequent words in songs is \"Love\".*\n\nNext let us look at the lyrics of song No 42 in the lyrics. Why 42 you may ask! Well! because it is the answer to the Ultimate Question of Life, the Universe, and Everything. \ud83d\udc2c ","c5797c12":"<a id=\"3\"><\/a>\n# <p style=\"background-color:#B291B6;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">DATA EXPLORATION<\/p>\n\n**In this section, I will be:**\n* Exploring the various artists in data\n* Explore the number of songs and their corresponding information\n* Explore the various words in lyrics via wordcloud "}}