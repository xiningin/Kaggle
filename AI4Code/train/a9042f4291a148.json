{"cell_type":{"fb7ec89e":"code","e547d861":"code","cfd7e258":"code","624f01cf":"code","df1d83f9":"code","6e58023e":"code","05f09e9f":"code","8f8a4521":"code","5fc71536":"code","33d064d3":"code","4090948b":"code","6aee688b":"code","4494f349":"code","2d48a80c":"code","6d4eff6d":"code","1765f337":"code","5956f2cb":"code","c77d13f0":"code","b2726326":"code","58df2927":"code","2ff18689":"code","b994a24f":"code","0a8dc16f":"code","c385dbd2":"code","a49061b7":"code","90799028":"code","bb2507df":"code","0dcd8975":"code","48a64b49":"code","6218230f":"code","35d967aa":"code","527f6234":"code","ef21aadd":"code","3cc7f12f":"code","7dd20bf9":"code","5ab499ed":"code","4ad1337e":"code","7f7cf6e1":"code","a6d7b3ce":"code","ddc3ecfb":"code","00861766":"code","6d1d75bd":"markdown","0dc3dcd6":"markdown","2379f744":"markdown","721b4a0e":"markdown","73edf0e8":"markdown","f54d8bde":"markdown","03fa9e93":"markdown","6588fc0e":"markdown","8aa74df4":"markdown","a4a1f266":"markdown","b1083dd5":"markdown","f7061853":"markdown","746b35c5":"markdown","6af144c9":"markdown","aecdfee8":"markdown","9d3036f3":"markdown","6a7f9d69":"markdown","b42f8a6f":"markdown","33b9a68d":"markdown"},"source":{"fb7ec89e":"# Python Libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n\n# Libraries for Visualization\nimport matplotlib.pyplot as plt\n\n\n# Library for splitting the data in Train and Test\nfrom sklearn.model_selection import train_test_split\n\n\n# Library required for the Support Vector Machine Algorithm\nfrom sklearn.svm import SVC\n\n# Library required for the K \u2013 Nearest Neighbour (KNN) Algorithm\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Library required for the Random Forest Classifier  Algorithm\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Library required for the Logistic Regression Algorithm\nfrom sklearn.linear_model import LogisticRegression\n\n# Library required for the Gaussian Naive Bayes Algorithm\nfrom sklearn.naive_bayes import GaussianNB\n\n\n\n# Library for the metric required to evaluate the model\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import accuracy_score\n","e547d861":"### Step 2: Reading the Data","cfd7e258":"\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","624f01cf":"iris_d = pd.read_csv('\/kaggle\/input\/iris-dataset\/Iris.csv')","df1d83f9":"# Fetching first 5 rows of the dataset\n\niris_d.head()","6e58023e":"# Fetching 10 records at random from the dataset\n\niris_d.sample(10)","05f09e9f":"# Data type of columns\n\niris_d.info()","8f8a4521":"# Shape of Data\n\niris_d.shape","5fc71536":"# Identifying Null values\n\niris_d.isnull().sum()","33d064d3":"# understanding the mathematics of the datasets\n\niris_d.describe()","4090948b":"iris_d['Species'].value_counts()","6aee688b":"# Box plot of price\nplt.figure(figsize= (10,10))\n# sns.boxplot(data= iris_d)\nsns.boxplot(data= iris_d,width = 0.8, fliersize = 5)\nplt.show()\n#plt.figure(figsize=(10,10))\n#sns.set(rc={'figure.figsize':(5,20)})","4494f349":"# To plot the Species data vs SepalLength using a box plot:\n\nsns.boxplot(x=\"Species\", y=\"SepalLengthCm\", data=iris_d)\nplt.show()","2d48a80c":"# To plot the Species data vs SepalWidth using a box plot:\n\nsns.boxplot(x=\"Species\", y=\"SepalWidthCm\", data=iris_d)\nplt.show()","6d4eff6d":"# To plot the Species data vs PetalLength using a box plot:\n\nsns.boxplot(x=\"Species\", y=\"PetalLengthCm\", data=iris_d)\nplt.show()","1765f337":"# To plot the Species data vs PetalWidth using a box plot:\n\nsns.boxplot(x=\"Species\", y=\"PetalWidthCm\", data=iris_d)\nplt.show()","5956f2cb":"# Scatter Plot representing Species w.r.t PetalLength and PetalWidth\n\nsns.FacetGrid(iris_d, hue= \"Species\", height = 5).map(plt.scatter, \"PetalLengthCm\", \"PetalWidthCm\").add_legend()\nplt.show()","c77d13f0":"# Scatter Plot representing species w.r.t SepalLength and SepalWidth\n\nsns.FacetGrid(iris_d, hue= \"Species\", height = 5).map(plt.scatter, \n                                                      \"SepalLengthCm\", \n                                                      \"SepalWidthCm\").add_legend()\nplt.show()","b2726326":"# Pairplot representing the relationship between species w.r.t. all parameters. \n# An perceptible pattern shows a close relationship between the two. \n\nsns.pairplot(iris_d.drop(\"Id\", axis=1), hue=\"Species\", height=4)\nplt.show()","58df2927":"# Correlation\n\niris_d.corr()","2ff18689":"# Vizually appealing corr matrix\ncor_iris = iris_d.corr()\n\n#cor_iris.style.background_gradient(cmap = 'coolwarm')\ncor_iris.style.background_gradient(cmap = 'viridis')","b994a24f":"# Heatmap\n\nsns.heatmap(cor_iris, \n            xticklabels=cor_iris.columns.values,\n            yticklabels=cor_iris.columns.values)\nplt.show()","0a8dc16f":"# In order to run Naive_Bayes classifier, replace the \"Species\" values.\n\niris_d['Species'].replace(\"Iris-setosa\",1,inplace= True)\niris_d['Species'].replace(\"Iris-virginica\",2,inplace = True)\niris_d['Species'].replace(\"Iris-versicolor\",3,inplace=True)","c385dbd2":"# Checking the Species values \n\niris_d['Species'].unique()","a49061b7":"feature = iris_d.iloc[:, 0:4]\ntarget = iris_d['Species']\n\n# X = dataframe.iloc[:, 0:4]  \n# Y = dataframe['Species']","90799028":"# Splitting the data into Train and Test dataset\n\nX_train, X_test, y_train, y_test = train_test_split(feature, target, test_size = 0.4, \n                                                    random_state = 40)\nprint(\"X-Train :\",X_train.shape)\nprint(\"X-Test :\",X_test.shape)\nprint(\"Y-Train :\",y_test.shape)\nprint(\"Y-Test :\",y_test.shape)","bb2507df":"KN_model = KNeighborsClassifier()\nKN_model.fit(X_train, y_train)\npred_vals_KN = KN_model.predict(X_test)\nprint(accuracy_score(y_test, pred_vals_KN))\npred_vals_KN","0dcd8975":"# This dataframe gives the Actual vs Predicted values\n\ndfIris_d = pd.DataFrame(pred_vals_KN, index = range(60,), columns = ['Predicted'])\ndfIris_d['Actual'] = y_test\ndfIris_d   ","48a64b49":"print('Mean Absolute Error =', mean_absolute_error(y_test, pred_vals_KN))\nprint(\"Mean Squared Error= \", mean_squared_error(y_test, pred_vals_KN))\nprint(\"Root Mean Squared Error= \", np.sqrt(mean_squared_error(y_test, pred_vals_KN)))","6218230f":"Support_vm = SVC()\nSupport_vm.fit(X_train, y_train)\npred_vals_SVC = Support_vm.predict(X_test)\nprint(accuracy_score(y_test, pred_vals_SVC))\npred_vals_SVC","35d967aa":"# This dataframe gives the Actual vs Predicted values\n\ndfIris_d = pd.DataFrame(pred_vals_SVC, index = range(60,), columns = ['Predicted'])\ndfIris_d['Actual'] = y_test\ndfIris_d   ","527f6234":"print('Mean Absolute Error =', mean_absolute_error(y_test, pred_vals_SVC))\nprint(\"Mean Squared Error= \", mean_squared_error(y_test, pred_vals_SVC))\nprint(\"Root Mean Squared Error= \", np.sqrt(mean_squared_error(y_test, pred_vals_SVC)))","ef21aadd":"RF_model = RandomForestClassifier(n_estimators=5)\nRF_model.fit(X_train, y_train)\npred_vals_RF = RF_model.predict(X_test)\nprint(accuracy_score(y_test, pred_vals_RF))\npred_vals_RF","3cc7f12f":"# This dataframe gives the Actual vs Predicted values\n\ndfIris_d = pd.DataFrame(pred_vals_RF, index = range(60,), columns = ['Predicted'])\ndfIris_d['Actual'] = y_test\ndfIris_d  ","7dd20bf9":"print('Mean Absolute Error =', mean_absolute_error(y_test, pred_vals_RF))\nprint(\"Mean Squared Error= \", mean_squared_error(y_test, pred_vals_RF))\nprint(\"Root Mean Squared Error= \", np.sqrt(mean_squared_error(y_test, pred_vals_RF)))","5ab499ed":"LR_model = LogisticRegression()\nLR_model.fit(X_train, y_train)\npred_vals_LR = LR_model.predict(X_test)\nprint(accuracy_score(y_test, pred_vals_LR))\npred_vals_LR","4ad1337e":"# This dataframe gives the Actual vs Predicted values\n\ndfIris_d = pd.DataFrame(pred_vals_LR, index = range(60,), columns = ['Predicted'])\ndfIris_d['Actual'] = y_test\ndfIris_d  ","7f7cf6e1":"print('Mean Absolute Error =', mean_absolute_error(y_test, pred_vals_LR))\nprint(\"Mean Squared Error= \", mean_squared_error(y_test, pred_vals_LR))\nprint(\"Root Mean Squared Error= \", np.sqrt(mean_squared_error(y_test, pred_vals_LR)))","a6d7b3ce":"#Train and test model\nGNB_model = GaussianNB()\nGNB_model.fit(X_train ,y_train)\npred_vals_GNB = GNB_model.predict(X_test)\nprint(accuracy_score(y_test, pred_vals_GNB))\npred_vals_GNB","ddc3ecfb":"# This dataframe gives the Actual vs Predicted values\n\ndfIris_d = pd.DataFrame(pred_vals_GNB, index = range(60,), columns = ['Predicted'])\ndfIris_d['Actual'] = y_test\ndfIris_d    ","00861766":"print('Mean Absolute Error =', mean_absolute_error(y_test, pred_vals_GNB))\nprint(\"Mean Squared Error= \", mean_squared_error(y_test, pred_vals_GNB))\nprint(\"Root Mean Squared Error= \", np.sqrt(mean_squared_error(y_test, pred_vals_GNB)))","6d1d75bd":"### Evaluating the Performance of the Model","0dc3dcd6":"### Evaluating the Performance of the Model","2379f744":"### 3. RandomForest","721b4a0e":"### 5. Gaussian Naive Bayes","73edf0e8":"#### Conclusion:: According to the above correlation matrix, PetalLengthCm and PetalWidthCm have strongest positive correlation.","f54d8bde":"### Step 5: Splitting the data into Train Data and Test Data\n#### For this step, firstly separate the 'Feature' and 'Target'variable from the data.","03fa9e93":"### 2. Support Vector Machine (SVM)","6588fc0e":"### 4. Logistic Regression","8aa74df4":"### Evaluating the Performance of the Model","a4a1f266":"# Problem Statement\n## This dataset comprises  of three types of flowers framework i.e. Versicolor, Setosa and Virginica. The numeric variables which the dataset contains are Sepal width, Sepal length, Petal width and Petal length. In this dataset,  the researcher is anticipating the categorization of the flowers dependent on these parameters. The information comprises of continuous numeric variables which portray the dimensions of the particular feature. \n\n## The objective is to build the model by applying different algorithm and see which model gives the best fit depending upon these parameters.","b1083dd5":"### Step 3: Understanding the Data\n#### The dataset comprises of small set of data with 150 sample size. It has four feature variables (IVs) belonging to either of the three target variables (DVs).","f7061853":"### The Correlation matrix","746b35c5":"## Solution\n### Step 1 : Importing Libraries","6af144c9":"#### The correlation matrix represents the parameters and flower frameworks which are best correlated with each other. Here the researcher tried to find out strongest and weakest correlated pair (except the main diagonal).","aecdfee8":"### Evaluating the Performance of the Model","9d3036f3":"### Evaluating the Performance of the Model","6a7f9d69":"### Step 4: Exploring the data through Visualization\n#### Here, Boxplot, scatter Plot and Pair Plot is used for visual representation of the data which helps the to understand various statistical measures such as mean, median, deviation, etc. In this, the researcher has tried to explore every possible relationship between the feature variables and the target variables.","b42f8a6f":"### Conclusion:: Among all models, Support Vector Machine gives 98.33% accuracy and the mean absolute error is 1.6% whereas other models gives perfect model fit. Though in Machine Learning, there is no specific model which gives 100% result accuracy of the dataset. This dataset apart from Support Vector Machine, gives 100% accuracy, which is nearly impossible. \n### What do you think regarding the same? Kindly provide your valuable feedback in the comment section.","33b9a68d":"### Step 6: Fitting the Model\/ Applying the Algorithm\n\n#### Here, the researcher has used 6 different model\/algorithm to find out the best fit.\n\n#### 1. K \u2013 Nearest Neighbour (KNN)"}}