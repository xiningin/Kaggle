{"cell_type":{"ebffd590":"code","78a360c0":"code","b501d9e3":"code","4cb5a5d4":"code","f7b50cc6":"code","1d59b888":"code","02365952":"code","fb81881c":"code","ab0076ae":"code","f451b8fd":"code","9602370d":"code","9c3b67ac":"code","5d619bf7":"code","7a10e6be":"code","93876759":"code","18146ecf":"code","bf19f028":"code","90af1969":"code","454ef39d":"code","b0e28899":"code","10372fec":"code","f8d690bf":"code","c307889f":"code","2341f7b1":"code","ad4b3bb6":"code","5141cc5b":"code","439626a5":"code","d0dd4076":"code","53379592":"code","d5b2e319":"code","77cb056a":"code","8287cbad":"code","aa758d2c":"code","bd336fb5":"code","8677f9f2":"code","819e568f":"code","e7efa1f3":"code","779306d2":"code","272987b7":"code","4d7703b1":"code","b81dd0ba":"code","a302ab83":"code","00a774b8":"code","0762ed93":"code","63df24c5":"code","57e79828":"code","3df15a9a":"code","da5c1b74":"code","57ac9579":"code","86244dd4":"code","73a05bae":"code","c5733dc5":"code","fddcd6c6":"code","f5af11ae":"code","bc456a32":"code","f469d3d9":"code","636b1e43":"code","c5ea229d":"code","c59bfdd1":"code","3b6b4214":"code","7b0d53dc":"code","819ec21d":"code","2c2cb8bd":"code","eb120f61":"code","eca6e80f":"code","9aa20ed5":"code","14dfd50b":"code","b1776407":"code","0bef5db1":"markdown","00607464":"markdown"},"source":{"ebffd590":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","78a360c0":"import matplotlib.pyplot as plt\nimport seaborn as sns","b501d9e3":"df_train=pd.read_csv(\"\/kaggle\/input\/intercampusai2019\/train.csv\")\ndf_test=pd.read_csv(\"\/kaggle\/input\/intercampusai2019\/test.csv\")\ndf_train.head(5)","4cb5a5d4":"df_train.describe()","f7b50cc6":"df_train.shape","1d59b888":"df_train.columns","02365952":"df_train.describe(include=[np.object, pd.Categorical]).T","fb81881c":"import missingno as msno","ab0076ae":"missingData = df_train.columns[df_train.isnull().any()].tolist()\nmsno.matrix(df_train[missingData])","f451b8fd":"msno.bar(df_train[missingData], color=\"blue\", log=True, figsize=(30,18))","9602370d":"msno.heatmap(df_train[missingData], figsize=(20,20))","9c3b67ac":"cat_cols=['Division', 'Qualification', 'Gender', 'Channel_of_Recruitment', 'Previous_Award', 'State_Of_Origin',\n         'Foreign_schooled', 'Marital_Status',  'Past_Disciplinary_Action', 'Previous_IntraDepartmental_Movement']\n\ndef count_unique(df, cols):\n    for col in cols:\n        print('\\n' + 'For column ' + col)\n        print(df[col].value_counts())\n        \ncount_unique(df_train, cat_cols)","5d619bf7":"def plot_bars(df, cols):\n    for col in cols:\n        fig = plt.figure(figsize=(6,6)) # define plot area\n        ax = fig.gca() # define axis\n        counts = df[col].value_counts() # find the counts for each unique categor\n        counts.plot.bar(ax = ax, color = 'blue') # Use the plot.bar method on the\n        ax.set_title(col) # Give the plot a main title\n        ax.set_xlabel('Count by' + col) # Set text for the x axis\n        ax.set_ylabel('Count')# Set text for y axis\n        plt.show()\n        \nplot_bars(df_train, cat_cols)","7a10e6be":"dataset=[df_train, df_test]\nfor data in dataset:\n    data['Age']= 2019-data['Year_of_birth']\n    \ndf_train.head(5)","93876759":"dataset=[df_train, df_test]\nfor data in dataset:\n    data['No_of_years_at_work']= 2019-data['Year_of_recruitment']\n    \ndf_train.head(5)","18146ecf":"dataset = [df_train, df_test]\nzones = {'LAGOS':\"SW\",\"FCT\":\"NC\", \"OGUN\":\"SW\",\"RIVERS\":\"SS\", \"ANAMBRA\":\"SE\",\"KANO\":\"NW\", \"DELTA\":\"SS\", \"OYO\":\"SW\", \n         \"KADUNA\":\"NW\",\"IMO\":\"SE\", \"EDO\":\"SS\", \"ENUGU\":\"SE\", \"ABIA\":\"SE\", \"OSUN\":\"SW\",\"ONDO\":\"SW\", \"NIGER\":\"NC\", \n         \"KWARA\":\"NC\", \"PLATEAU\":\"NC\", \"AKWA IBOM\":\"SS\", \"NASSARAWA\":\"NC\", \"KATSINA\":\"NW\", \"ADAMAWA\":\"NE\",\"BENUE\":\"NC\",\n         \"BAUCHI\":\"NE\", \"KOGI\":\"NC\", \"SOKOTO\":\"NW\", \"CROSS RIVER\":\"SS\", \"EKITI\":\"SW\", \"BORNO\":\"NE\", \"TARABA\":\"NE\",\n         \"KEBBI\":\"NW\", \"BAYELSA\":\"SS\", \"EBONYI\":\"SE\", \"GOMBE\":\"NE\", \"ZAMFARA\":\"NW\",\"JIGAWA\":\"NW\", \"YOBE\":\"NE\"}\nfor data in dataset:\n    data['Zones']=data['State_Of_Origin'].replace(zones)\n    \ndf_train.head(5)","bf19f028":"df_train['Qualification'] = df_train['Qualification'].fillna(method=\"bfill\")\ndf_test['Qualification'] = df_test['Qualification'].fillna(method=\"bfill\")","90af1969":"print(df_train['Last_performance_score'].mean())\nprint(df_train['Last_performance_score'].max())\nprint(df_train['Last_performance_score'].median())\nprint(df_train['Last_performance_score'].min())","454ef39d":"print(df_train.Age.mean())\nprint(df_train.Age.min())\nprint(df_train.Age.max())\nprint(df_train.Age.median())","b0e28899":"print(df_train.Trainings_Attended.mean())\nprint(df_train.Trainings_Attended.min())\nprint(df_train.Trainings_Attended.max())\nprint(df_train.Trainings_Attended.median())","10372fec":"\"\"\"dataset = [df_train, df_test]\nfor data in dataset:\n    data[\"Last_performance_score\"] = pd.cut(data[\"Last_performance_score\"], 3, labels=[\"low\", \"medium\", \"high\"])\n    data[\"Training_score_average\"] = pd.cut(data[\"Training_score_average\"], 3, labels=[\"low\", \"medium\", \"high\"])  \n    data[\"Trainings_Attended\"] = pd.cut(data[\"Trainings_Attended\"], 3, labels=[\"low\", \"medium\", \"high\"])\"\"\"","f8d690bf":"df_train['Zones'].value_counts()","c307889f":"df_train.head(5)","2341f7b1":"df_train['No_of_previous_employers'].value_counts()","ad4b3bb6":"del df_train['Year_of_birth']\ndel df_test['Year_of_birth']\ndel df_train['Year_of_recruitment']\ndel df_test['Year_of_recruitment']\ndel df_test['State_Of_Origin']\ndel df_train['State_Of_Origin']","5141cc5b":"cat_cols=['Gender', 'Previous_Award','Foreign_schooled', 'Marital_Status',  'Qualification', 'Division','Channel_of_Recruitment', 'Zones',\n          'Past_Disciplinary_Action', 'Previous_IntraDepartmental_Movement', 'No_of_previous_employers']\n\ntrain=pd.get_dummies(df_train, prefix=cat_cols,columns=cat_cols)\ntest=pd.get_dummies(df_test, prefix=cat_cols, columns=cat_cols)","439626a5":"print(\"This is the shape of the training set \",train.shape)\nprint(\"This is the shape of the test set \", test.shape)","d0dd4076":"del train['EmployeeNo']\ndel test['EmployeeNo']","53379592":"#Heat map\ncorrmat= train.corr()\nf, ax =plt.subplots(figsize=(40,40))\nsns.heatmap(corrmat, annot= True,square=True)","d5b2e319":"del train['Promoted_or_Not']\nprint(\"This is the shape of the training set \",train.shape)\nprint(\"This is the shape of the test set \", test.shape)","77cb056a":"df_train['Promoted_or_Not'].value_counts()","8287cbad":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier\nimport sklearn.metrics as sklm\nfrom math import sqrt","aa758d2c":"feature=train.columns\ntarget=['Promoted_or_Not']\nX=train[feature].values\ny=df_train[target].values \nsplit_test_size=0.30\nX_train, X_test, y_train, y_test= train_test_split(X,y, test_size=split_test_size, random_state=42)","bd336fb5":"print(\"{0:0.2f}% in training set\".format((len(X_train)\/len(train.index)) * 100))\nprint(\"{0:0.2f}% in test set\".format((len(X_test)\/len(train.index)) * 100))","8677f9f2":"from sklearn.preprocessing import RobustScaler\nss=RobustScaler()\nX_train=ss.fit_transform(X_train)\nX_test=ss.transform(X_test)\ntest1= ss.transform(test)","819e568f":"'''from fancyimpute import KNN\nknn=KNN(k=3)\nX_train = knn.fit_transform(X_train)\nX_test=  knn.transfrom(X_test)\ntest1=knn.transform(test1)\n'''","e7efa1f3":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(ratio='minority')\nX_train, y_train = smote.fit_sample(X_train, y_train)","779306d2":"#Logistic Regression without parameter tuning\nlr = LogisticRegression()\nlr.fit(X_train, y_train)","272987b7":"def plot_auc(labels, probs):\n    ## Compute the false positive rate, true positive rate\n    ## and threshold along with the AUC\n    fpr, tpr, threshold = sklm.roc_curve(labels, probs[:,1])\n    auc = sklm.auc(fpr, tpr)\n    \n    ## Plot the result\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, color = 'blue', label = 'AUC = %0.2f' % auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","4d7703b1":"probabilities = lr.predict_proba(X_test)\n\ndef score_model(probs, threshold):\n    return np.array([1 if x > threshold else 0 for x in probs[:,1]])\n\ndef print_metrics(labels, probs, threshold):\n    scores = score_model(probs, threshold)\n    metrics = sklm.precision_recall_fscore_support(labels, scores)\n    conf = sklm.confusion_matrix(labels, scores)\n    print('                 Confusion matrix')\n    print('                 Score positive    Score negative')\n    print('Actual positive    %6d' % conf[0,0] + '             %5d' % conf[0,1])\n    print('Actual negative    %6d' % conf[1,0] + '             %5d' % conf[1,1])\n    print('')\n    print('Accuracy        %0.2f' % sklm.accuracy_score(labels, scores))\n    print('AUC             %0.2f' % sklm.roc_auc_score(labels, probs[:,1]))\n    print('Macro precision %0.2f' % float((float(metrics[0][0]) + float(metrics[0][1]))\/2.0))\n    print('Macro recall    %0.2f' % float((float(metrics[1][0]) + float(metrics[1][1]))\/2.0))\n    print(' ')\n    print('           Positive      Negative')\n    print('Num case   %6d' % metrics[3][0] + '        %6d' % metrics[3][1])\n    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])\n    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])\n    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])\n    \n\nprint_metrics(y_test, probabilities, 0.5)","b81dd0ba":"probabilities = lr.predict_proba(X_train)\nprint_metrics(y_train, probabilities, 0.5)","a302ab83":"plot_auc(y_train, probabilities)","00a774b8":"gb = GradientBoostingClassifier()\ngb.fit(X_train, y_train)","0762ed93":"plt.bar(range(len(gb.feature_importances_)), gb.feature_importances_)\nplt.show()","63df24c5":"probabilities = gb.predict_proba(X_train)\nprint_metrics(y_train, probabilities, 0.5)","57e79828":"probabilities = gb.predict_proba(X_test)\nprint_metrics(y_test, probabilities, 0.5)","3df15a9a":"plot_auc(y_test, probabilities)","da5c1b74":"gb = GradientBoostingClassifier(max_depth=2, n_estimators=500, max_features='auto', loss='exponential' )\ngb.fit(X_train, y_train)","57ac9579":"probabilities = gb.predict_proba(X_train)\nprint_metrics(y_train, probabilities, 0.5)","86244dd4":"probabilities = gb.predict_proba(X_test)\nprint_metrics(y_test, probabilities, 0.5)","73a05bae":"plot_auc(y_test, probabilities)","c5733dc5":"import xgboost as xgb","fddcd6c6":"xgb=xgb.XGBClassifier(max_depth=3, n_estimators=500, n_jobs=-1, scale_pos_weight=4)\nxgb.fit(X_train, y_train)","f5af11ae":"plt.bar(range(len(xgb.feature_importances_)), xgb.feature_importances_)\nplt.show()","bc456a32":"probabilities = xgb.predict_proba(X_train)\nprint_metrics(y_train, probabilities, 0.5)","f469d3d9":"probabilities = xgb.predict_proba(X_test)\nprint_metrics(y_test, probabilities, 0.5)","636b1e43":"from sklearn.metrics import f1_score\ny_pred=xgb.predict(X_test)\nf1_score(y_test,y_pred, average='weighted')","c5ea229d":"plot_auc(y_test, probabilities)","c59bfdd1":"print(xgb.predict(test1))","3b6b4214":"solution=xgb.predict(test1)\nmy_submission=pd.DataFrame({'EmployeeNo':df_test['EmployeeNo'],'Promoted_or_Not': solution})\nmy_submission.to_csv('xgboostFirstSubmission.csv', index=False)","7b0d53dc":"from catboost import CatBoostClassifier","819ec21d":"model = CatBoostClassifier (iterations= 1700, learning_rate=0.2, depth=5, verbose=True, scale_pos_weight=2)\nmodel.fit(X_train, y_train)","2c2cb8bd":"probabilities = model.predict_proba(X_train)\nprint_metrics(y_train, probabilities, 0.5)","eb120f61":"probabilities = model.predict_proba(X_test)\nprint_metrics(y_test, probabilities, 0.5)","eca6e80f":"from sklearn.metrics import f1_score\ny_pred=model.predict(X_test)\nf1_score(y_test,y_pred, average='weighted')","9aa20ed5":"plot_auc(y_test, probabilities)","14dfd50b":"print(model.predict(test1))","b1776407":"solution=model.predict(test1)\nmy_submission=pd.DataFrame({'EmployeeNo':df_test['EmployeeNo'],'Promoted_or_Not': solution})\nmy_submission.to_csv('CatBoostSubmission.csv', index=False)","0bef5db1":"## Feature Engineering","00607464":"## Training and Applying algorithms"}}