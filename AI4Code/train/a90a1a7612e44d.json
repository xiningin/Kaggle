{"cell_type":{"be3d8923":"code","75bae3f2":"code","f4bd50d9":"code","c5a89393":"code","e8bfa19c":"code","4a40154e":"code","8871fc52":"code","5a3629d9":"code","99c752a4":"code","d2e2a0e5":"code","d027fdd8":"code","ea502a34":"code","a595eeb9":"code","800d64a9":"code","34680402":"code","48504d57":"code","d54dc190":"markdown","4b93a108":"markdown","57120934":"markdown","60e154af":"markdown","b1594bd7":"markdown","2b9b732d":"markdown","f182e0f5":"markdown","1a0a93b4":"markdown","48037ed3":"markdown","e0a7ebbf":"markdown","294bcd8c":"markdown","995339af":"markdown","42a933d7":"markdown","10e5e80f":"markdown","16653878":"markdown","6e91f335":"markdown","4993ee76":"markdown","e0016ab3":"markdown","02b171d1":"markdown"},"source":{"be3d8923":"# Importing libraries\nimport numpy as np\nimport random\nimport math\nfrom collections import deque\nimport collections\nimport pickle\nimport time\nimport math\nimport random\nfrom itertools import permutations\n\n# for building DQN model\nfrom keras import layers\nfrom keras import Sequential\nfrom keras.layers import Dense, Activation, Flatten\nfrom keras.optimizers import Adam\n\n# for plotting graphs\nimport matplotlib.pyplot as plt\n","75bae3f2":"import os\nprint(os.listdir(\"..\/input\/tm-file\"))","f4bd50d9":"# Loading the time matrix (part of the sample code)\nTime_matrix = np.load(\"..\/input\/tm-file\/TM.npy\")","c5a89393":"print(Time_matrix.max())\nprint(Time_matrix.min())\nprint(Time_matrix.mean())\nprint(Time_matrix.var())","e8bfa19c":"class DQNAgent:\n    def __init__(self, state_size, action_size):\n        \n        # Define size of state and action\n        self.state_size = state_size\n        self.action_size = action_size\n\n        # Write here: Specify the hyper parameters for the DQN\n        self.discount_factor = 0.95\n        self.learning_rate = 0.01\n        self.epsilon = 1\n        self.epsilon_max = 1\n        self.epsilon_decay = -0.0005 #for 15k episodes\n        self.epsilon_min = 0.00001\n        \n        self.batch_size = 32\n\n        # create replay memory using deque\n        self.memory = deque(maxlen=2000)\n\n        # Initialize the value of the states tracked\n        self.states_tracked = []\n        \n        # We are going to track state [0,0,0] and action (0,2) at index 2 in the action space.\n        self.track_state = np.array(env.state_encod_arch1([0,0,0])).reshape(1, 36)\n\n        # create main model and target model\n        self.model = self.build_model()\n\n    # approximate Q function using Neural Network\n    def build_model(self):\n        \"\"\"\n        Function that takes in the agent and constructs the network\n        to train it\n        @return model\n        @params agent\n        \"\"\"\n        input_shape = self.state_size\n        model = Sequential()\n        # Write your code here: Add layers to your neural nets       \n        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n        # the output layer: output is of size num_actions\n        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\n        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n        model.summary\n        return model\n\n    def get_action(self, state, possible_actions_index, actions):\n        \"\"\"\n        get action in a state according to an epsilon-greedy approach\n        possible_actions_index, actions are the 'ride requests' that teh driver got.\n        \"\"\"        \n        # get action from model using epsilon-greedy policy\n        # Decay in \u03b5 after each episode       \n        if np.random.rand() <= self.epsilon:\n            # explore: choose a random action from the ride requests\n            return random.choice(possible_actions_index)\n        else:\n            # choose the action with the highest q(s, a)\n            # the first index corresponds to the batch size, so\n            # reshape state to (1, state_size) so that the first index corresponds to the batch size\n            state = np.array(env.state_encod_arch1(state)).reshape(1, 36)\n\n            # Use the model to predict the Q_values.\n            q_value = self.model.predict(state)\n\n            # truncate the array to only those actions that are part of the ride  requests.\n            q_vals_possible = [q_value[0][i] for i in possible_actions_index]\n\n            return possible_actions_index[np.argmax(q_vals_possible)]\n\n    def append_sample(self, state, action_index, reward, next_state, done):\n        \"\"\"appends the new agent run output to replay buffer\"\"\"\n        self.memory.append((state, action_index, reward, next_state, done))\n        \n    # pick samples randomly from replay memory (with batch_size) and train the network\n    def train_model(self):\n        \"\"\" \n        Function to train the model on eacg step run.\n        Picks the random memory events according to batch size and \n        runs it through the network to train it.\n        \"\"\"\n        if len(self.memory) > self.batch_size:\n            # Sample batch from the memory\n            mini_batch = random.sample(self.memory, self.batch_size)\n            # initialise two matrices - update_input and update_output\n            update_input = np.zeros((self.batch_size, self.state_size))\n            update_output = np.zeros((self.batch_size, self.state_size))\n            actions, rewards, done = [], [], []\n\n            # populate update_input and update_output and the lists rewards, actions, done\n            for i in range(self.batch_size):\n                state, action, reward, next_state, done_boolean = mini_batch[i]\n                update_input[i] = env.state_encod_arch1(state)     \n                actions.append(action)\n                rewards.append(reward)\n                update_output[i] = env.state_encod_arch1(next_state)\n                done.append(done_boolean)\n\n            # predict the target q-values from states s\n            target = self.model.predict(update_input)\n            # target for q-network\n            target_qval = self.model.predict(update_output)\n\n\n            # update the target values\n            for i in range(self.batch_size):\n                if done[i]:\n                    target[i][actions[i]] = rewards[i]\n                else: # non-terminal state\n                    target[i][actions[i]] = rewards[i] + self.discount_factor * np.max(target_qval[i])\n            # model fit\n            self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n            \n    def save_tracking_states(self):\n        # Use the model to predict the q_value of the state \n        q_value = self.model.predict(self.track_state)\n        \n        print(\"States_tracked value {0}.\".format(q_value[0][2]))\n        \n        # Grab the q_value of the action index \n        self.states_tracked.append(q_value[0][2])\n        \n    def save(self, name):\n        self.model.save(name)\n        \n# Defining hyperparameters\nm = 5  # number of cities, ranges from 0 ..... m-1\nt = 24  # number of hours, ranges from 0 .... t-1\nd = 7  # number of days, ranges from 0 ... d-1\nC = 5  # Per hour fuel and other costs\nR = 9  # per hour revenue from a passenger\n\n\nclass CabDriver():\n\n    def __init__(self):\n        \"\"\"initialise your state and define your action space and state space\"\"\"\n        self.action_space = [(0, 0)] + \\\n            list(permutations([i for i in range(m)], 2))\n        self.state_space = [[x, y, z]\n                            for x in range(m) for y in range(t) for z in range(d)]\n        self.state_init = random.choice(self.state_space)\n        #self.state_init = [0,0,0]\n        # Start the first round\n        self.reset()\n\n    ## Encoding state (or state-action) for NN input\n\n    def state_encod_arch1(self, state):\n        \"\"\"convert the state into a vector so that it can be fed to the NN. \n        This method converts a given state into a vector format. \n        Hint: The vector is of size m + t + d.\"\"\"\n\n        state_encod = [0 for _ in range(m+t+d)]\n        state_encod[self.state_get_loc(state)] = 1\n        state_encod[m+self.state_get_time(state)] = 1\n        state_encod[m+t+self.state_get_day(state)] = 1\n\n        return state_encod\n\n    # Use this function if you are using architecture-2\n\n    def state_encod_arch2(self, state, action):\n        \"\"\"convert the (state-action) into a vector so that it can be fed to the NN. \n        This method converts a given state-action pair into a vector format. \n        Hint: The vector is of size m + t + d + m + m.\"\"\"\n        state_encod = [0 for _ in range(m+t+d+m+m)]\n        state_encod[self.state_get_loc(state)] = 1\n        state_encod[m+self.state_get_time(state)] = 1\n        state_encod[m+t+self.state_get_day(state)] = 1\n        if (action[0] != 0):\n            state_encod[m+t+d+self.action_get_pickup(action)] = 1\n        if (action[1] != 0):\n            state_encod[m+t+d+m+self.action_get_drop(action)] = 1\n\n        return state_encod\n\n    ## Getting number of requests\n\n    def requests(self, state):\n        \"\"\"Determining the number of requests basis the location. \n        Use the table specified in the MDP and complete for rest of the locations\"\"\"\n        location = state[0]\n        if location == 0:\n            requests = np.random.poisson(2)\n        if location == 1:\n            requests = np.random.poisson(12)\n        if location == 2:\n            requests = np.random.poisson(4)\n        if location == 3:\n            requests = np.random.poisson(7)\n        if location == 4:\n            requests = np.random.poisson(8)\n\n        if requests > 15:\n            requests = 15\n        # (0,0) is not considered as customer request, however the driver is free to refuse all\n        # customer requests. Hence, add the index of action (0,0).\n        possible_actions_index = random.sample(range(1, (m-1)*m + 1), requests) + [0]\n        actions = [self.action_space[i] for i in possible_actions_index]\n\n        return possible_actions_index, actions\n\n    def update_time_day(self, time, day, ride_duration):\n        \"\"\"\n        Takes in the current state and time taken for driver's journey to return\n        the state post that journey.\n        \"\"\"\n        ride_duration = int(ride_duration)\n\n        if (time + ride_duration) < 24:\n            time = time + ride_duration\n            # day is unchanged\n        else:\n            # duration taken spreads over to subsequent days\n            # convert the time to 0-23 range\n            time = (time + ride_duration) % 24 \n            \n            # Get the number of days\n            num_days = (time + ride_duration) \/\/ 24\n            \n            # Convert the day to 0-6 range\n            day = (day + num_days ) % 7\n\n        return time, day\n    \n    def next_state_func(self, state, action, Time_matrix):\n        \"\"\"Takes state and action as input and returns next state\"\"\"\n        next_state = []\n        \n        # Initialize various times\n        total_time   = 0\n        transit_time = 0    # to go from current  location to pickup location\n        wait_time    = 0    # in case driver chooses to refuse all requests\n        ride_time    = 0    # from Pick-up to drop\n        \n        # Derive the current location, time, day and request locations\n        curr_loc = self.state_get_loc(state)\n        pickup_loc = self.action_get_pickup(action)\n        drop_loc = self.action_get_drop(action)\n        curr_time = self.state_get_time(state)\n        curr_day = self.state_get_day(state)\n        \"\"\"\n         3 Scenarios: \n           a) Refuse all requests\n           b) Driver is already at pick up point\n           c) Driver is not at the pickup point.\n        \"\"\"    \n        if ((pickup_loc== 0) and (drop_loc == 0)):\n            # Refuse all requests, so wait time is 1 unit, next location is current location\n            wait_time = 1\n            next_loc = curr_loc\n        elif (curr_loc == pickup_loc):\n            # means driver is already at pickup point, wait and transit are both 0 then.\n            ride_time = Time_matrix[curr_loc][drop_loc][curr_time][curr_day]\n            \n            # next location is the drop location\n            next_loc = drop_loc\n        else:\n            # Driver is not at the pickup point, he needs to travel to pickup point first\n            # time take to reach pickup point\n            transit_time      = Time_matrix[curr_loc][pickup_loc][curr_time][curr_day]\n            new_time, new_day = self.update_time_day(curr_time, curr_day, transit_time)\n            \n            # The driver is now at the pickup point\n            # Time taken to drop the passenger\n            ride_time = Time_matrix[pickup_loc][drop_loc][new_time][new_day]\n            next_loc  = drop_loc\n\n        # Calculate total time as sum of all durations\n        total_time = (wait_time + transit_time + ride_time)\n        next_time, next_day = self.update_time_day(curr_time, curr_day, total_time)\n        \n        # Construct next_state using the next_loc and the new time states.\n        next_state = [next_loc, next_time, next_day]\n        \n        return next_state, wait_time, transit_time, ride_time\n    \n\n    def reset(self):\n        \"\"\"Return the current state and action space\"\"\"\n        return self.action_space, self.state_space, self.state_init\n\n    def reward_func(self, wait_time, transit_time, ride_time):\n        \"\"\"Takes in state, action and Time-matrix and returns the reward\"\"\"\n        # transit and wait time yield no revenue, only battery costs, so they are idle times.\n        passenger_time = ride_time\n        idle_time      = wait_time + transit_time\n        \n        reward = (R * passenger_time) - (C * (passenger_time + idle_time))\n\n        return reward\n\n    def step(self, state, action, Time_matrix):\n        \"\"\"\n        Take a trip as cabby to get rewards next step and total time spent\n        \"\"\"\n        # Get the next state and the various time durations\n        next_state, wait_time, transit_time, ride_time = self.next_state_func(\n            state, action, Time_matrix)\n\n        # Calculate the reward based on the different time durations\n        rewards = self.reward_func(wait_time, transit_time, ride_time)\n        total_time = wait_time + transit_time + ride_time\n        \n        return rewards, next_state, total_time\n\n    def state_get_loc(self, state):\n        return state[0]\n\n    def state_get_time(self, state):\n        return state[1]\n\n    def state_get_day(self, state):\n        return state[2]\n\n    def action_get_pickup(self, action):\n        return action[0]\n\n    def action_get_drop(self, action):\n        return action[1]\n\n    def state_set_loc(self, state, loc):\n        state[0] = loc\n\n    def state_set_time(self, state, time):\n        state[1] = time\n\n    def state_set_day(self, state, day):\n        state[2] = day\n\n    def action_set_pickup(self, action, pickup):\n        action[0] = pickup\n\n    def action_set_drop(self, action, drop):\n        action[1] = drop\n","4a40154e":"episode_time = 24*30 #30 days before which car has to be recharged\nn_episodes = 100\nm = 5\nt = 24\nd = 7\n\n# Invoke Env class\nenv = CabDriver()\naction_space, state_space, state = env.reset()\n\n# Set up state and action sizes.\nstate_size = m+t+d\naction_size = len(action_space)\n\n# Invoke agent class\nagent = DQNAgent(action_size=action_size, state_size=state_size)\n\n# to store rewards in each episode\nrewards_per_episode, episodes = [], []\n# Rewards for state [0,0,0] being tracked.\nrewards_init_state = []","8871fc52":"start_time = time.time()\nscore_tracked = []\n\nfor episode in range(n_episodes):\n\n    done = False\n    score = 0\n    track_reward = False\n\n    # reset at the start of each episode\n    env = CabDriver()\n    action_space, state_space, state = env.reset()\n    # Save the initial state so that reward can be tracked if initial state is [0,0,0]\n    initial_state = env.state_init\n\n    # Total time driver rode in this episode\n    total_time = 0  \n    while not done:\n        # 1. Get a list of the ride requests driver got.\n        possible_actions_indices, actions = env.requests(state)\n        # 2. Pick epsilon-greedy action from possible actions for the current state.\n        action = agent.get_action(state, possible_actions_indices, actions)\n\n        # 3. Evaluate your reward and next state\n        reward, next_state, step_time = env.step(state, env.action_space[action], Time_matrix)\n        # 4. Total time driver rode in this episode\n        total_time += step_time\n        if (total_time > episode_time):\n            # if ride does not complete in stipulated time skip\n            # it and move to next episode.\n            done = True\n        else:\n            # 5. Append the experience to the memory\n            agent.append_sample(state, action, reward, next_state, done)\n            # 6. Train the model by calling function agent.train_model\n            agent.train_model()\n            # 7. Keep a track of rewards, Q-values, loss\n            score += reward\n            state = next_state\n\n    # store total reward obtained in this episode\n    rewards_per_episode.append(score)\n    episodes.append(episode)\n    \n\n    # epsilon decay\n    agent.epsilon = (1 - 0.00001) * np.exp(agent.epsilon_decay * episode)\n\n    # every 10 episodes:\n    if ((episode + 1) % 10 == 0):\n        print(\"episode {0}, reward {1}, memory_length {2}, epsilon {3} total_time {4}\".format(episode,\n                                                                         score,\n                                                                         len(agent.memory),\n                                                                         agent.epsilon, total_time))\n    # Save the Q_value of the state, action pair we are tracking\n    if ((episode + 1) % 5 == 0):\n        agent.save_tracking_states()\n\n    # Total rewards per episode\n    score_tracked.append(score)\n\n    if(episode % 1000 == 0):\n        print(\"Saving Model {}\".format(episode))\n        agent.save(name=\"SuperCabs_car_model.h5\")\n\n    \nelapsed_time = time.time() - start_time\nprint(elapsed_time)\n","5a3629d9":"def save_obj(obj, name ):\n    with open(name + '.pkl', 'wb') as f:\n        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)","99c752a4":"save_obj(agent.states_tracked,\"SuperCabs_model_weights\")","d2e2a0e5":"agent.states_tracked","d027fdd8":"state_tracked_sample = [agent.states_tracked[i] for i in range(len(agent.states_tracked)) if agent.states_tracked[i] < 1000]","ea502a34":"plt.figure(0, figsize=(16,7))\nplt.title('Q_value for state [0,0,0]  action (0,2)')\nxaxis = np.asarray(range(0, len(agent.states_tracked)))\nplt.semilogy(xaxis,np.asarray(agent.states_tracked))\nplt.show()","a595eeb9":"score_tracked_sample = [score_tracked[i] for i in range(len(score_tracked)) if (i % 4 == 0)]","800d64a9":"plt.figure(0, figsize=(16,7))\nplt.title('Rewards per episode')\nxaxis = np.asarray(range(0, len(score_tracked_sample)))\nplt.plot(xaxis,np.asarray(score_tracked_sample))\nplt.show()","34680402":"import numpy as np\ntime = np.arange(0,15000)\nepsilon = []\nfor i in range(0,15000):\n    epsilon.append((1 - 0.00001) * np.exp(-0.0005 * i))","48504d57":"import matplotlib.pyplot as plt\nplt.plot(time, epsilon)\nplt.show()","d54dc190":"for episode in range(Episodes):\n\n    # Write code here\n    # Call the environment\n    # Call all the initialised variables of the environment\n    \n\n    #Call the DQN agent\n    \n    \n    while !terminal_state:\n        \n        # Write your code here\n        # 1. Pick epsilon-greedy action from possible actions for the current state\n        # 2. Evaluate your reward and next state\n        # 3. Append the experience to the memory\n        # 4. Train the model by calling function agent.train_model\n        # 5. Keep a track of rewards, Q-values, loss\n        ","4b93a108":"#### Check the max, min and mean time values to help us in defining the 'next_step' function in the Environment","57120934":"### Cab-Driver Agent","60e154af":"### 5. Plot the Q-Value convergence for state action pairs","b1594bd7":"### 1. Load the Time Matrix","2b9b732d":"### 3. DQN block","f182e0f5":"### The 'total_time' above includes the 'last ride' time also in each episode. Although it exceeds 24*30 = 720, our code drops the last ride from the replay buffer. So the total ride time per episode is limited to < 720","1a0a93b4":"### Tracking Convergence","48037ed3":"<div class=\"alert alert-block alert-info\">\nTry building a similar epsilon-decay function for your model.\n<\/div>","e0a7ebbf":"#### Since the max time is 11 hours between any 2 points, the next state of the cab driver can only change by  1 day","294bcd8c":"## Goals\n\n### Create the environment: \n\nYou are given the \u2018Env.py\u2019 file with the basic code structure. This is the \"environment class\" - each method (function) of the class has a specific purpose. Please read the comments around each method carefully to understand what it is designed to do. Using this framework is not compulsory, you can create your own framework and functions as well.\n\n### Build an agent that learns to pick the best request using DQN:\n\nYou can choose the hyperparameters (epsilon (decay rate), learning-rate, discount factor etc.) of your choice.\n\nTraining depends purely on the epsilon-function you choose. If the \n\u03f5\n decays fast, it won\u2019t let your model explore much and the Q-values will converge early but to suboptimal values. If \n\u03f5\n decays slowly, your model will converge slowly. We recommend that you try converging the Q-values in 4-6 hrs.  We\u2019ve created a sample \n\u03f5\n-decay function at the end of the Agent file (Jupyter notebook) that will converge your Q-values in ~5 hrs. Try building a similar one for your Q-network.\n\nIn the Agent file, we\u2019ve provided the code skeleton. Using this structure is not necessary though.\n\n           You have to submit your final DQN model as a pickle file as well.\n\n### Convergence:\n\nYou need to converge your results. The Q-values may be suboptimal since the agent won't be able to explore much in 5-6 hours of simulation. But it is important that your Q-values converge. There are two ways to check the convergence of the DQN model:\n\nSample a few state-action pairs and plot their Q-values along episodes\n\nCheck whether the total rewards earned per episode are showing stability\n\n          Showing one of these convergence plots will suffice.","995339af":"### We can see from the above plot that the rewards converge at around 1500. Since the initial state is picked to be random for each episode, some initial states may be less rewarding than others inherently regardless of the model quality.","42a933d7":"#### Using log scale as the initial q_values are way too high compared to the steady state value (around 600)","10e5e80f":"### 4. Run the episodes, build up replay buffer and train the model.\n### Note:\n#### The moment total episode time exceeds 720 (30 days), we ignore the most recent ride and do NOT save that experience in the replay memory\n#### The init state is randomly picked from the state space for each episode","16653878":"> ### 2. Agent & Cab Driver Class\n\nIf you are using this framework, you need to fill the following to complete the following code block:\n1. State and Action Size\n2. Hyperparameters\n3. Create a neural-network model in function 'build_model()'\n4. Define epsilon-greedy strategy in function 'get_action()'\n5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n6. Complete the 'train_model()' function with following logic:\n   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n      - Initialise your input and output batch for training the model\n      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n      - Get Q(s', a) values from the last trained model\n      - Update the input batch as your encoded state and output batch as your Q-values\n      - Then fit your DQN model using the updated input and output batch.","6e91f335":"## Problem Statement\nYou are hired as a Sr. Machine Learning Er. at SuperCabs, a leading app-based cab provider in a large Indian metro city. In this highly competitive industry, retention of good cab drivers is a crucial business driver, and you believe that a sound RL-based system for assisting cab drivers can potentially retain and attract new cab drivers. \n \n\nCab drivers, like most people, are incentivised by a healthy growth in income. The goal of your project is to build an RL-based algorithm which can help cab drivers maximise their profits by improving their decision-making process on the field.","4993ee76":"#### Epsilon-decay sample function","e0016ab3":"### Save the tracked states ","02b171d1":"### 6. Track rewards per episode."}}