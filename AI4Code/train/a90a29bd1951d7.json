{"cell_type":{"4e111f64":"code","7e59b1d7":"code","af4ddd77":"code","cd2b53ae":"code","ee3cf0ae":"code","d05af3da":"code","59d15419":"code","92829960":"code","fa66b439":"code","56d805ad":"code","84dc479b":"code","25d9a91c":"code","0e242d5a":"code","ad963e6b":"code","9393ca90":"code","faefc96d":"code","f57bf593":"code","0752d357":"markdown","7a15916f":"markdown","a0fc17c6":"markdown","0318743f":"markdown","bce55855":"markdown","9697a768":"markdown","dd01bd78":"markdown","e3df7c49":"markdown"},"source":{"4e111f64":"import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport nltk\nfrom nltk.corpus import stopwords","7e59b1d7":"df_train = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ndf_test = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')","af4ddd77":"df_train.head()","cd2b53ae":"df_test.head()","ee3cf0ae":"df_train.info()","d05af3da":"df_test.info()","59d15419":"# Plot the distribution of count of words\nwords = df_train['excerpt'].str.split().apply(len)\nplt.figure(figsize=(10,5))\nplt.hist(words, alpha=0.8, bins=15)\nplt.legend(loc='best')\nplt.xlabel('Count of words')\nplt.ylabel('Count')\nplt.title('Count of words in excerpt')\nplt.show()","92829960":"import re","fa66b439":"def clean_text(txt):\n    txt = re.sub(\"[^a-zA-Z]\", \" \", txt)\n    txt = txt.lower()\n\n    txt = nltk.word_tokenize(txt)\n    txt = [word for word in txt if not word in set(stopwords.words(\"english\"))]\n\n    lemma = nltk.WordNetLemmatizer()\n    txt = [lemma.lemmatize(word) for word in txt]\n    txt = \" \".join(txt)\n    return txt\n\n# df_train['excerpt'] = df_train['excerpt'].apply(lambda x: clean_text(x))\n# df_test['excerpt'] = df_test['excerpt'].apply(lambda x: clean_text(x))","56d805ad":"df_train.head()","84dc479b":"from sklearn.model_selection import KFold\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification, RobertaTokenizer, TFRobertaForSequenceClassification","25d9a91c":"MAXLEN = 512\nBATCH_SIZE = 3\nEPOCHS = 20\nLR = 1e-5\nN_SPLITS = 5\n\ndef get_model(bert_model):\n    input_ids = tf.keras.layers.Input(shape=(MAXLEN, ), dtype='int32', name='input_ids')\n    attention_mask = tf.keras.layers.Input(shape=(MAXLEN, ), dtype='int32', name='attention_mask')\n#     token_type_ids = tf.keras.layers.Input(shape=(MAXLEN, ), dtype='int32', name='token_type_ids')\n\n    X = bert_model(input_ids=input_ids, attention_mask=attention_mask)[0]\n    outputs = tf.keras.layers.Dense(1, use_bias=True, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(X)\n    \n    model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=outputs)\n    model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adam(lr=LR), metrics=[tf.keras.metrics.RootMeanSquaredError()])\n    return model","0e242d5a":"fold = 0\ntrain = df_train['excerpt']\ntargets = df_train['target']\n\nfor train_idx, val_idx in KFold(N_SPLITS, shuffle=True, random_state=2021).split(train):\n    # get data\n    if fold != 0:\n        fold += 1\n        continue\n    \n    X_train = train[train_idx]\n    X_val = train[val_idx]\n    y_train = targets[train_idx]\n    y_val = targets[val_idx]\n    \n    y_train = tf.constant(y_train, dtype=tf.float32)\n    y_val = tf.constant(y_val, dtype=tf.float32)\n    \n    # process data\n    X_train = [clean_text(x) for x in X_train]\n    X_val = [clean_text(x) for x in X_val]\n    \n    # get model\n    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n    bert_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n\n    model = get_model(bert_model)\n    # model.load_weights('..\/input\/commonlit-readability-model\/checkpoint\/variables\/variables')\n    model.summary()\n    \n    # tokenize data\n    X_train = tokenizer(X_train, padding=\"max_length\", max_length=MAXLEN, return_tensors='tf', truncation=True)\n    X_val = tokenizer(X_val, padding=\"max_length\", max_length=MAXLEN, return_tensors='tf', truncation=True)\n    X_train = {\"input_ids\": X_train['input_ids'], \"attention_mask\": X_train['attention_mask']}\n    X_val = {\"input_ids\": X_val['input_ids'], \"attention_mask\": X_val['attention_mask']}\n    \n    # train model\n    checkpoint = [tf.keras.callbacks.ModelCheckpoint(f'Fold{fold}\/checkpoint', save_weights_only=False, save_best_only=True)]\n    model.fit(X_train, y_train, batch_size=BATCH_SIZE, validation_data=(X_val, y_val), epochs=EPOCHS, callbacks=[checkpoint])\n    \n    # save model\n    tokenizer.save_pretrained(f'Fold{fold}\/tokenizer\/')\n    bert_model.save_pretrained(f'Fold{fold}\/distil_bert\/')\n    \n    fold += 1","ad963e6b":"# test = df_test['excerpt']\n# X_test = [clean_text(x) for x in test]\n# X_test[0]","9393ca90":"# result = 0\n# for i in range(N_SPLITS):\n#     # get model\n#     tokenizer = RobertaTokenizer.from_pretrained(f'..\/input\/commonlit-readability-model\/robert-large\/Fold{i}\/tokenizer\/')\n#     bert_model = TFRobertaForSequenceClassification.from_pretrained(f'..\/input\/commonlit-readability-model\/robert-large\/Fold{i}\/distil_bert')\n\n#     model = get_model(bert_model)\n#     if i != 1: model.load_weights(f'..\/input\/commonlit-readability-model\/robert-large\/Fold{i}\/checkpoint\/variables\/variables')\n#     else: model.load_weights(f'..\/input\/commonlit-readability-model\/robert-large\/Fold{i}\/checkpoint\/variables')\n#     # tokenize data\n#     X_test_token = tokenizer(X_test, padding=\"max_length\", max_length=MAXLEN, return_tensors='tf', truncation=True)\n#     X_test_token = {\"input_ids\": X_test_token['input_ids'], \"attention_mask\": X_test_token['attention_mask']}\n    \n#     # predict\n#     result += model.predict(X_test_token)\n\n# # result \/= N_SPLITS","faefc96d":"# # result = 0\n# for i in range(N_SPLITS):\n#     # get model\n#     tokenizer = DistilBertTokenizer.from_pretrained(f'..\/input\/commonlit-readability-model\/distil\/Fold{i}\/tokenizer\/')\n#     bert_model = TFDistilBertForSequenceClassification.from_pretrained(f'..\/input\/commonlit-readability-model\/distil\/Fold{i}\/distil_bert')\n\n#     model = get_model(bert_model)\n#     model.load_weights(f'..\/input\/commonlit-readability-model\/distil\/Fold{i}\/checkpoint\/variables\/variables')\n    \n#     # tokenize data\n#     X_test_token = tokenizer(X_test, padding=\"max_length\", max_length=MAXLEN, return_tensors='tf', truncation=True)\n#     X_test_token = {\"input_ids\": X_test_token['input_ids'], \"attention_mask\": X_test_token['attention_mask']}\n    \n#     # predict\n#     result += model.predict(X_test_token)\n\n# result \/= (N_SPLITS*2)","f57bf593":"# submission_df = pd.DataFrame({'id': df_test.id, 'target': 0})\n# submission_df.target = result\n\n# submission_file = 'submission.csv'\n# submission_df.to_csv(submission_file, index=False)\n\n# submission_df","0752d357":"# 1. load data","7a15916f":"# 6. Submission","a0fc17c6":"# 3. Tokenization","0318743f":"# 2. preprocess data","bce55855":"    well, this notebook has a really low rank -_-! but I hope its structure could help you a little bit more.\n    The only thing you should change is the get_model function\n    Then you could train your own model with preprocess, K-FOLD and inferences.\n    Thank you!","9697a768":"# 5. Save model","dd01bd78":"# overview\n\nFor Competition [CommonLit Readability Prize](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize)\n\n    release the train section for train\n    release the submit section for test","e3df7c49":"# 4. Train model"}}