{"cell_type":{"30b2d483":"code","9b30dd46":"code","411cba8d":"code","5b659d4b":"code","796cd52e":"code","e08544a0":"code","29450cdc":"code","b65d0deb":"code","f907a23e":"code","78b652e2":"code","68714194":"code","d7c1fded":"code","2594209e":"code","8db1246b":"code","3bf0ded4":"code","6a84002a":"code","e9839536":"code","d13f5825":"code","040e7597":"code","86c02957":"code","c8b4bd1b":"code","6a4198eb":"code","3d9b2e9e":"code","2efe5a83":"code","08411d53":"code","0891c16f":"code","c0af4845":"code","40e88706":"code","6458ceb0":"code","a42c46bd":"code","5f31d9f5":"code","432dcf97":"code","e45df38c":"code","19c3d973":"code","69ea4f5a":"code","8090d75a":"code","6c8fccd8":"code","56b6f7af":"code","baac852b":"code","0f7517ae":"code","af193895":"code","74575357":"code","f1c53023":"code","8e716890":"code","0b57d87c":"code","6806c555":"code","2c8979cc":"markdown","9c7b33d1":"markdown","0997c3d5":"markdown","12ca5f1d":"markdown","9faffa61":"markdown","19fdd4c8":"markdown","3617c03c":"markdown","1873be66":"markdown","a25afaac":"markdown","21689685":"markdown","ed62287d":"markdown","436f56fc":"markdown","3fef2d29":"markdown","29562953":"markdown","4923ac78":"markdown","50c51ce3":"markdown","55d39083":"markdown"},"source":{"30b2d483":"import os\nimport random\nimport joblib\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nimport optuna\nfrom optuna.samplers import TPESampler\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","9b30dd46":"import warnings\nwarnings.filterwarnings('ignore')","411cba8d":"PATH = '..\/input\/creditcardfraud\/creditcard.csv'","5b659d4b":"SEED = 0\nN_FOLDS = 3","796cd52e":"def set_seed(seed=42):\n    \"\"\"Utility function to use for reproducibility.\n    :param seed: Random seed\n    :return: None\n    \"\"\"\n    np.random.seed(seed)\n    random.seed(seed)\n    # tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n\n\ndef set_display():\n    \"\"\"Function sets display options for charts and pd.DataFrames.\n    \"\"\"\n    # Plots display settings\n    plt.style.use('fivethirtyeight')\n    plt.rcParams['figure.figsize'] = 12, 8\n    plt.rcParams.update({'font.size': 14})\n    # DataFrame display settings\n    pd.set_option('display.max_columns', None)\n    pd.set_option('display.max_rows', None)\n    pd.options.display.float_format = '{:.4f}'.format\n\n\ndef cv_score(model, x: pd.DataFrame, y: pd.Series, metric: str) -> float:\n    \"\"\"Function computed cross-validation score\n    for classification model based on a specified metric.\n    :param model: sklearn model instance\n    :param x: Original DataFrame with features and target values\n    :param y: Target labels\n    :param metric: Scoring metric\n    :return: Mean CV score\n    \"\"\"\n    kf = StratifiedKFold(N_FOLDS, shuffle=True, random_state=SEED)\n    score = cross_val_score(model, x, y, scoring=metric, cv=kf, n_jobs=-1)\n    return score.mean()","e08544a0":"set_seed()\nset_display()","29450cdc":"data = pd.read_csv(PATH)\nprint(f'Data shape: {data.shape}')\ndata.head()","b65d0deb":"data.info()","f907a23e":"data.describe()","78b652e2":"# Target distribution\nclasses = data['Class'].value_counts(normalize=True)\nplt.pie(classes.values, labels=classes.index, autopct='%1.1f%%')\nplt.title('Class Imbalance')\nplt.show()","68714194":"# Amount distribution\nplt.hist(data['Amount'], bins=30, log=True)\nplt.xlabel('Transaction amount')\nplt.title('Amount Distribution')\nplt.show()","d7c1fded":"# Time distribution of transactions\ndata['Time'].describe()","2594209e":"plt.hist(data['Time'], bins=30)\nplt.xlabel('Seconds from transaction #1')\nplt.title('Transactions Time')\nplt.show()","8db1246b":"# Feature engineering: temporal features\n# Obtain hour from \"Time\" column, which represents\n# seconds passed from the 1st transaction.\n# Data set contains transactions for 2 days.\ndata['hour'] = data['Time'] \/\/ 3600\ndata['hour'] = data['hour'].apply(lambda x: x if x < 24 else x - 24)","3bf0ded4":"hours = data['hour'].value_counts()\nplt.bar(hours.index, hours.values)\nplt.xlabel('Hour')\nplt.ylabel('N transactions')\nplt.title('Transactions by Hour')\nplt.show()","6a84002a":"# Distribution of numerical features\nnum_features = [col for col in data.columns if col.find('V') > -1]\n\nn_cols = 2\nn_rows = 14\n\nfig = plt.gcf()\nfig.set_size_inches(n_cols * 6, n_rows * 4)\n\nfor pos, feature in enumerate(num_features):\n    sp = plt.subplot(n_rows, n_cols, pos + 1)\n    plt.hist(data[feature], bins=30)\n    plt.title(feature)\n\nplt.show()","e9839536":"correlation = data.corr()\nax = sns.heatmap(correlation, center=0, cmap='RdBu_r')\nl, r = ax.get_ylim()\nax.set_ylim(l + 0.5, r - 0.5)\nplt.yticks(rotation=0)\nplt.title('Correlation Matrix')\nplt.show()","d13f5825":"idx = [col for col in data.columns if col != 'Class']\nplt.barh(correlation.loc[idx, 'Class'].index, correlation.loc[idx, 'Class'].values)\nplt.title('Feature Correlation with Target')\nplt.show()","040e7597":"# sklearn classification models\nmodel_gausnb = make_pipeline(RobustScaler(), GaussianNB())\nmodel_aboost = AdaBoostClassifier()\nmodel_svc = make_pipeline(RobustScaler(), SVC())","86c02957":"models = [\n    ('Gaussian Naive Bayes', model_gausnb),\n    ('Ada boost', model_aboost),\n    ('SVC', model_svc),\n]","c8b4bd1b":"# Input features and target labels\nx = data.drop('Class', axis=1)\ny = data['Class']","6a4198eb":"for name, model in models:\n    score = cv_score(model, x, y, metric='roc_auc')\n    print(f'{name} model AUC score: {score}')","3d9b2e9e":"# Create a dictionary of parameters and values to search.\n# Since we created a pipeline including scaler and classifier,\n# we need to use parameter name with double underscore after model name.\nparams = {\n    'gaussiannb__var_smoothing': [0.001, 0.1, 0.5, 0.9, 0.95, 0.96, 0.97, 0.98, 0.99]\n}\n\n# We will use Stratified K-Fold algorithm to train the models\n# on several subsets of data and check the AUC score of validation subsets.\nkf = StratifiedKFold(N_FOLDS, shuffle=True, random_state=SEED)\n\n# Define Grid-Search\ngrid = GridSearchCV(model_gausnb, params, cv=kf, scoring='roc_auc',\n                    n_jobs=-1, refit=True)\n\n# Display the results\ngrid.fit(x, y)\nprint('Best AUC score:', grid.best_score_)\nprint(grid.best_params_)\n\n# Save the best model.\njoblib.dump(grid, 'gaussiannb.pkl')","2efe5a83":"# In this case we use model without preprocessing step.\n# Parameters in the dictionary are named the same as in model definition.\n# Parameter space is limited to reduce grid-search time.\nparams = {\n    'n_estimators': [50, 60],\n    'learning_rate': [0.7, 0.8]\n}\n\ngrid = GridSearchCV(model_aboost, params, cv=kf, scoring='roc_auc',\n                    n_jobs=-1, refit=True)\n\n# Warning: This grid-search takes a long time to complete.\ngrid.fit(data.drop('Class', axis=1), data['Class'])\nprint('Best AUC score:', grid.best_score_)\nprint(grid.best_params_)\n\njoblib.dump(grid, 'adaboost.pkl')","08411d53":"# We will use Stratified K-Fold algorithm to train the models\n# on several subsets of data and check the AUC score of validation subsets.\nkf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)","0891c16f":"scores = []\n\nfor train_index, test_index in kf.split(x, y):\n\n    x_train, x_test = x.iloc[train_index, :], x.iloc[test_index, :]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    model_xgb = XGBClassifier(objective='binary:logistic')\n\n    model_xgb.fit(x_train, y_train, eval_set=[(x_test, y_test)],\n                  eval_metric='auc', early_stopping_rounds=50,\n                  verbose=0)\n\n    scores.append(model_xgb.best_score)\n    model_xgb.save_model(f'xgboost{len(scores)}.bin')\n\n    print(f'Completed training model {len(scores)}.')","c0af4845":"print('XGBoost average AUC score:', np.mean(scores))\nfor i, score in enumerate(scores):\n    print(f'Model {i} AUC score: {score}')","40e88706":"# Display feature importance.\nimportance = pd.DataFrame({\n    'features': x.columns,\n    'importance': model_xgb.feature_importances_\n})\nimportance.sort_values(by='importance', inplace=True)\n\nplt.figure(figsize=(12, 16))\nplt.barh(importance['features'], importance['importance'])\nplt.title('XGBoost Feature Importance')\nplt.show()","6458ceb0":"# Default parameters of the last trained model\nmodel_xgb.get_params","a42c46bd":"scores = []\n\nfor train_index, test_index in kf.split(x, y):\n\n    x_train, x_test = x.iloc[train_index, :], x.iloc[test_index, :]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    model_lgb = LGBMClassifier(objective='binary', metrics='auc')\n\n    model_lgb.fit(x_train, y_train, eval_set=(x_test, y_test),\n                  eval_metric='auc', early_stopping_rounds=50,\n                  verbose=0)\n\n    scores.append(model_lgb.best_score_['valid_0']['auc'])\n    model_lgb.booster_.save_model(f'lgbm{len(scores)}.txt',\n                                  num_iteration=model_lgb.best_iteration_)\n\n    print(f'Completed training model {len(scores)}.')","5f31d9f5":"print('LGBM average AUC score:', np.mean(scores))\nfor i, score in enumerate(scores):\n    print(f'Model {i} AUC score: {score}')","432dcf97":"# Display feature importance.\nimportance = pd.DataFrame({\n    'features': x.columns,\n    'importance': model_lgb.feature_importances_\n})\nimportance.sort_values(by='importance', inplace=True)\n\nplt.figure(figsize=(12, 16))\nplt.barh(importance['features'], importance['importance'])\nplt.title('LGBM Feature Importance')\nplt.show()","e45df38c":"# Default parameters of the last trained model\nmodel_lgb.get_params()","19c3d973":"scores = []\n\nfor train_index, test_index in kf.split(x, y):\n\n    x_train, x_test = x.iloc[train_index, :], x.iloc[test_index, :]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    model_cb = CatBoostClassifier(eval_metric='AUC')\n    model_cb.fit(x_train, y_train, eval_set=[(x_test, y_test)],\n                 early_stopping_rounds=20, use_best_model=True,\n                 verbose=0)\n\n    scores.append(model_cb.best_score_['validation']['AUC'])\n    model_cb.save_model(f'catboost{len(scores)}.cbm')\n\n    print(f'Completed training model {len(scores)}.')","69ea4f5a":"print('Average AUC score:', np.mean(scores))\nfor i, score in enumerate(scores):\n    print(f'Model {i} AUC score: {score}')","8090d75a":"# Display feature importance.\nimportance = pd.DataFrame({\n    'features': x.columns,\n    'importance': model_cb.feature_importances_\n})\nimportance.sort_values(by='importance', inplace=True)\n\nplt.figure(figsize=(12, 16))\nplt.barh(importance['features'], importance['importance'])\nplt.title('CatBoost Feature Importance')\nplt.show()","6c8fccd8":"# Parameters of the last trained model\nmodel_cb.get_all_params()","56b6f7af":"def objective(trial, x, y):\n    \"\"\"Function performs grid-search for optimal parameters.\n    :param trial: optuna trial object\n    :param x: Input features\n    :param y: Target values\n    :return: AUC score\n    \"\"\"\n    X_train, X_valid, y_train, y_valid = train_test_split(\n        x, y, stratify=y, test_size=.2, random_state=SEED)\n\n    # Parameter space for all tunable arguments.\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 500, 3000, 100),\n        'booster': 'gbtree',\n        'reg_lambda': trial.suggest_int('reg_lambda', 1, 100),\n        'reg_alpha': trial.suggest_int('reg_alpha', 1, 100),\n        'subsample': trial.suggest_float('subsample', 0.2, 1.0, step=0.1),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0, step=0.1),\n        'max_depth': trial.suggest_int('max_depth', 3, 16),\n        'min_child_weight': trial.suggest_int('min_child_weight', 2, 10),\n        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.97),\n        'gamma': trial.suggest_float('gamma', 0, 20)\n    }\n\n    xgb_clf = XGBClassifier(objective='binary:logistic', **params)\n\n    xgb_clf.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],\n                eval_metric='auc', early_stopping_rounds=50,\n                verbose=False)\n\n    return xgb_clf.best_score","baac852b":"# Initialize an object to perform search for optimal parameters.\nstudy = optuna.create_study(\n    sampler=TPESampler(seed=SEED),  # Type of sampling\n    direction='maximize',  # Whether the metric should be minimized or maximized.\n    study_name='xgb')\n\n# Create lambda function to call grid-search function\n# and pass the complete set of input features and target values.\nfunc = lambda trial: objective(trial, x, y)\n\n# Here we call lambda function, which in turn calls optimization function.\nstudy.optimize(func, n_trials=100)","0f7517ae":"print('Best XGBoost AUC score:', study.best_value)\nbest_params = study.best_params\nprint(best_params)","af193895":"# Save the study.\njoblib.dump(study, 'xgb_study.pkl')","74575357":"# Repeat cross-validation with optimized parameters and save trained models.\nscores = []\n\nfor train_index, test_index in kf.split(x, y):\n\n    x_train, x_test = x.iloc[train_index, :], x.iloc[test_index, :]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    model_xgb = XGBClassifier(objective='binary:logistic', **best_params)\n\n    model_xgb.fit(x_train, y_train, eval_set=[(x_test, y_test)],\n                  eval_metric='auc', early_stopping_rounds=50,\n                  verbose=0)\n\n    scores.append(model_xgb.best_score)\n    model_xgb.save_model(f'xgboost_opt{len(scores)}.bin')\n\n    print(f'Completed training model {len(scores)}.')","f1c53023":"print('XGBoost average AUC score:', np.mean(scores))\nfor i, score in enumerate(scores):\n    print(f'Model {i} AUC score: {score}')","8e716890":"model_cb = CatBoostClassifier(eval_metric='AUC')\n\nparams = {'learning_rate': [0.1, 0.2, 0.3, 0.4],\n          'depth': [4, 5, 6, 7]}\n\ngrid = model_cb.grid_search(params, X=x, y=y, stratified=True, refit=True)","0b57d87c":"# This grid-search method fits the model, which was passed to it\n# and returns a dictionary of grid-search.\n# Check that the model is actually fitted.\nmodel_cb.is_fitted()","6806c555":"# Save the optimized and trained model and display the parameters.\nmodel_cb.save_model('catboost_opt.cbm')\nprint('CatBoost optimal parameters:', grid['params'])","2c8979cc":"Tree-based models do not require scaling of input features. Other classification models benefit from data preprocessing:\nscaling makes training faster and ensures better convergence. Possible scalers include StandardScaler, MinMaxScaler, PowerTransformer and some others. For this particular problem we will use RobustScaler, where needed. This scaler is recommended for data, which has outliers.","9c7b33d1":"## Gradient Boosting with default parameters","0997c3d5":"### Example 2: Improve AdaBoost classifier","12ca5f1d":"## Exploratory Data Analysis","9faffa61":"## Conclusions\n- The fraud detection algorithm could be further improved by adding more train samples. At present the data is limited to transactions made during 2 days. For more reliable results the data should cover at least one month of transactions or more.\n- Synthetic resampling could be applied to address class imbalance issue. However, due to limited number of examples belonging to the positive class (fraud) and the nature of the problem it could be risky to apply such techniques.\n- To ensure the prediction reliability several models of various architectures could be combined and process the data independently. As a final step either all identified possible frauds could be unified and passed for investigation or predicted probabilities could be averaged to reduce the false positives.","19fdd4c8":"### Example 1: XGBoost model","3617c03c":"## Grid-Search for optimal parameters","1873be66":"### Example 1: Improve XGBoost with optuna library","a25afaac":"### Example 2: Optimizing CatBoost model with built-in grid-search","21689685":"Optuna library requires us to define optimization function, which takes a \"trial\" object (special class used for searching parameter space), input features and target values. This function is called multiple times\nand returns validation score for various parameter combinations.","ed62287d":"### Example 1: Improve Naive Bayes classifier","436f56fc":"## Functions","3fef2d29":"### Example 2: LightGBM model","29562953":"## Hyper-Parameter Tuning","4923ac78":"### Example 3: CatBoost model","50c51ce3":"# Searching for the Best Model\nIn this notebook I develop a **credit card fraud detection model** testing several **sklearn classifiers** and **gradient boosting models** with default parameters and then optimizing hyper-parameters to get higher **AUC score**.\n\nThe [dataset](https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud) contains credit card transactions made by European cardholders in two days in September 2013: a total of **492 frauds out of 284,807 transactions**. Frauds account for **0.172%** of all transactions, which makes the data set highly imbalanced.\n\n**Features:**\n- V1, V2 \u2026 V28 - numerical features, described in the data set annotation as the output of PCA\n- Time - seconds elapsed between each transaction and the first transaction in the data set\n- Amount - the transaction amount\n\n**Target value:**\n- Class - takes value 1 in case of fraud and 0 otherwise","55d39083":"## Cross-Validating sklearn models with default parameters"}}