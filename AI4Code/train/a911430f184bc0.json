{"cell_type":{"d28b1db2":"code","9e58b1cb":"code","7a119c7e":"code","515b4a07":"code","fd0f146a":"code","0ded6142":"code","4dfca9cf":"code","2ddf23ec":"code","ea4211fa":"code","86020491":"code","1d29af55":"code","7ede5586":"code","7e9b1ea6":"code","612f186b":"code","29fa181c":"code","c10a7496":"code","7c888302":"code","a47105d7":"code","226627db":"code","3e69af40":"code","10da4f78":"code","0d675a33":"code","38cecfe9":"code","cb1a7fd5":"code","3b1e7630":"code","719c9f83":"code","15eef0a6":"code","1a2c5646":"code","70398859":"code","34541c0f":"code","f2b1d51a":"code","88640cd8":"code","ed24ef6a":"code","ca23a477":"code","2c6141f7":"code","fd713c86":"code","a18f5414":"code","568bf688":"code","737ce048":"code","4a786b46":"code","1bce84fa":"code","e7b6a2b0":"code","3fbc4db5":"code","12cabe82":"code","cab9e838":"code","434e7790":"code","b8b21d56":"code","88342aab":"code","db14aa50":"code","05b8f45f":"code","3fb477e7":"code","bf387630":"code","456f2d4e":"markdown","1c8dc3b2":"markdown","cc4779a7":"markdown","d049a89e":"markdown","bed3a363":"markdown","b4a57eef":"markdown","5eb3db79":"markdown","3cb0ea6d":"markdown","c455a608":"markdown","cd41d801":"markdown","6437edc9":"markdown","82e78ec6":"markdown","f6a48360":"markdown","82812d62":"markdown","a521b9c5":"markdown","9f5fe27e":"markdown","ceddd15f":"markdown","fba29d25":"markdown","5a3c205f":"markdown","36b70b24":"markdown","9a1c1a0f":"markdown","0433d8e0":"markdown","2057a851":"markdown","af2e628d":"markdown","b5f4e773":"markdown","6177228f":"markdown","19763bd1":"markdown","277fe706":"markdown","9f094c13":"markdown","731761bd":"markdown","41d5ddeb":"markdown","05a97038":"markdown","d52a56e7":"markdown","adf35c21":"markdown","1cb26bf2":"markdown","98fa20a1":"markdown","5ac45fa9":"markdown","3e0b6384":"markdown","30d8783d":"markdown","eb003390":"markdown","2cd0c5c0":"markdown","a4746fba":"markdown","d0919878":"markdown","68b0a963":"markdown","7bee1142":"markdown","8af4baa7":"markdown","82f82d94":"markdown","55b977f8":"markdown"},"source":{"d28b1db2":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\n\nimport warnings\nwarnings.simplefilter(\"ignore\")","9e58b1cb":"# read from a regular csv file\ndata = pd.read_csv(\"..\/input\/winequality-red.csv\")","7a119c7e":"'''\nThis is translation for me :)\nInput variables (based on physicochemical tests):\n1 - fixed acidity        - \u0444\u0438\u043a\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u0430\u044f \u043a\u0438\u0441\u043b\u043e\u0442\u043d\u043e\u0441\u0442\u044c\n2 - volatile acidity     - \u043b\u0435\u0442\u0443\u0447\u0430\u044f \u043a\u0438\u0441\u043b\u043e\u0442\u043d\u043e\u0441\u0442\u044c\n3 - citric acid          - \u043b\u0438\u043c\u043e\u043d\u043d\u0430\u044f \u043a\u0438\u0441\u043b\u043e\u0442\u0430\n4 - residual sugar       - \u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u044b\u0439 \u0441\u0430\u0445\u0430\u0440\n5 - chlorides            - \u0445\u043b\u043e\u0440\u0438\u0434\u044b\n6 - free sulfur dioxide  - \u0441\u0432\u043e\u0431\u043e\u0434\u043d\u044b\u0439 \u0434\u0438\u043e\u043a\u0441\u0438\u0434 \u0441\u0435\u0440\u044b\n7 - total sulfur dioxide - \u043e\u0431\u0449\u0438\u0439 \u0434\u0438\u043e\u043a\u0441\u0438\u0434 \u0441\u0435\u0440\u044b\n8 - density              - \u043f\u043b\u043e\u0442\u043d\u043e\u0441\u0442\u044c\n9 - pH                   - \u0432\u043e\u0434\u043e\u0440\u043e\u0434\u043d\u044b\u0439 \u043f\u043e\u043a\u0430\u0437\u0430\u0442\u0435\u043b\u044c(\u043a\u0438\u0441\u043b\u043e\u0442\u043d\u043e\u0441\u0442\u044c \u0441\u0440\u0435\u0434\u044b pH)\n10 - sulphates           - \u0441\u0443\u043b\u044c\u0444\u0430\u0442\u044b\n11 - alcohol             - \u0430\u043b\u043a\u043e\u0433\u043e\u043b\u044c\nOutput variable:\n12 - quality (score between 0 and 10) - \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u043e (0 - 10)\n'''\ndata.head(10)","515b4a07":"data.tail()","fd0f146a":"data.info()","0ded6142":"data.describe()","4dfca9cf":"print(\"Number of unique values in each column:\\n\")\nfor i in data.columns:\n    print(i, len(data[i].unique()))","2ddf23ec":"data['bin_quality'] = pd.cut(data['quality'], bins=[0, 6.5, 10], labels=[\"bad\", \"good\"])","ea4211fa":"data.head(10)","86020491":"fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(14, 10))\n\ndata_length = len(data)\nquality_percentage = [100 * i \/ data_length for i in data[\"quality\"].value_counts()]\nbin_quality_percentage = [100 * i \/ data_length for i in data[\"bin_quality\"].value_counts()]\n\nsns.countplot(\"quality\", data=data, ax=ax[0, 0])\nsns.countplot(\"bin_quality\", data=data, ax=ax[0, 1]);\n\nsns.barplot(x=data[\"quality\"].unique(), y=quality_percentage, ax=ax[1, 0])\nax[1, 0].set_xlabel(\"quality\")\n\nsns.barplot(x=data[\"bin_quality\"].unique(), y=bin_quality_percentage, ax=ax[1, 1])\nax[1, 1].set_xlabel(\"bin_quality\")\n\nfor i in range(2):\n    ax[1, i].set_ylabel(\"The percentage of the total number\")\n    ax[1, i].set_yticks(range(0, 101, 10))\n    ax[1, i].set_yticklabels([str(i) + \"%\" for i in range(0, 101, 10)])\n    for j in range(2):\n        ax[i, j].yaxis.grid()\n        ax[i, j].set_axisbelow(True)","1d29af55":"plt.figure(figsize=[9, 9])\nsns.heatmap(data.corr(), xticklabels=data.columns[:-1], yticklabels=data.columns[:-1], \n\n            square=True, cmap=\"Spectral_r\", center=0);","7ede5586":"#  The function takes on the input column name and restrictions on the y axis. \n#  Next, the function builds a histogram of the distribution of the values \n#  of this column, a histogram of the dependence of the two types \n# of quality to the column passed as a parameter.\n\ndef drawing_two_barplots(column, ylims):\n    fig = plt.figure(figsize=(14, 12))\n    gs = gridspec.GridSpec(2, 2)\n    ax0 = fig.add_subplot(gs[0, :])\n    ax1 = fig.add_subplot(gs[1, 0])\n    ax2 = fig.add_subplot(gs[1, 1])\n    \n    sns.distplot(data[data.columns[column]], kde=False, ax=ax0)\n    sns.barplot(\"quality\", data.columns[column], data=data, ax=ax1)\n    sns.barplot(\"bin_quality\", data.columns[column], data=data, ax=ax2)\n    ax1.set_ylim(ylims[0], ylims[1])\n    ax2.set_ylim(ylims[0], ylims[1])\n    ax1.set_yticks(np.linspace(ylims[0], ylims[1], 11))\n    ax2.set_yticks(np.linspace(ylims[0], ylims[1], 11))\n    ax1.yaxis.grid()\n    ax2.yaxis.grid()\n    ax1.set_axisbelow(True)\n    ax2.set_axisbelow(True)","7e9b1ea6":"drawing_two_barplots(0, [0, 10])","612f186b":"drawing_two_barplots(1, [0, 1.2])","29fa181c":"drawing_two_barplots(2, [0, 0.5])","c10a7496":"drawing_two_barplots(3, [0, 3.6])","7c888302":"drawing_two_barplots(4, [0, 0.18])","a47105d7":"drawing_two_barplots(5, [0, 20])","226627db":"drawing_two_barplots(6, [0, 60])","3e69af40":"drawing_two_barplots(7, [0.994, 0.999])","10da4f78":"drawing_two_barplots(8, [3.1, 3.5])","0d675a33":"drawing_two_barplots(9, [0, 0.9])","38cecfe9":"drawing_two_barplots(10, [0, 13])","cb1a7fd5":"from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler, normalize\n\nfrom sklearn.naive_bayes           import GaussianNB\nfrom sklearn.linear_model          import LogisticRegression\nfrom sklearn.neighbors             import KNeighborsClassifier\nfrom sklearn.svm                   import SVC\nfrom sklearn.tree                  import DecisionTreeClassifier\nfrom sklearn.neural_network        import MLPClassifier\nfrom sklearn.ensemble              import ExtraTreesClassifier\nfrom sklearn.ensemble              import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nfrom lightgbm import LGBMClassifier\n\nFEATURES = slice(0,-2, 1)","3b1e7630":"model_names = ['LogisticRegression',\n               'KNeighborsClassifier',\n               'SVC',\n               'MLPClassifier',\n               'ExtraTreesClassifier',\n               'RandomForestClassifier',\n               'LinearDiscriminantAnalysis',\n               'LGBMClassifier']\n\nclassifiers = [LogisticRegression, # \u041b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f\n               KNeighborsClassifier, # K-\u0431\u043b\u0438\u0436\u0430\u0439\u0448\u0438\u0445 \u0441\u043e\u0441\u0435\u0434\u0435\u0439\n               SVC, # \u041c\u0435\u0442\u043e\u0434 \u043e\u043f\u043e\u0440\u043d\u044b\u0445 \u0432\u0435\u043a\u0442\u043e\u0440\u043e\u0432\n               MLPClassifier, # \u0422\u0440\u0451\u0445\u0441\u043b\u043e\u0439\u043d\u044b\u0439 \u043f\u0435\u0440\u0446\u0435\u043f\u0442\u0440\u043e\u043d\n               ExtraTreesClassifier, # \u042d\u043a\u0441\u0442\u0440\u0430 (randomized) \u0434\u0435\u0440\u0435\u0432\u044c\u044f \n               RandomForestClassifier, # \u0421\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u043b\u0435\u0441\n               LinearDiscriminantAnalysis, # \u041b\u0438\u043d\u0435\u0439\u043d\u044b\u0439 \u0434\u0438\u0441\u043a\u0440\u0438\u043c\u0438\u043d\u0430\u043d\u0442\u043d\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0437\n               LGBMClassifier] # \u0413\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0431\u0443\u0441\u0442\u0438\u043d\u0433","719c9f83":"#  This function takes an instance of the model, data and labels as input, \n#  and there is an optional parameter that indicates the number of splits (rounds) to validate. \n#  The function returns the average value of cross-validation, as well as the standard deviation.\n\ndef cross_val_mean_std(clsf, data, labels, cv=5):\n    cross_val = cross_val_score(clsf, data, labels, cv=cv)\n    cross_val_mean = cross_val.mean() * 100\n    cross_val_std = cross_val.std() * 100\n    return round(cross_val_mean, 3), round(cross_val_std, 3)","15eef0a6":"#  This function takes the type of training model, training and test data, \n#  and parameters for that model, if any, as input. \n#  The function returns the already trained model, which we can use \n#  if necessary, as well as a dictionary with the \n#  results of cross-validation of training and test data.\n\ndef train_and_validate_model(model, train, train_labels, test, test_labels, parameters=None):\n    \n    if parameters is not None:\n        model = model(**parameters)\n    else:\n        model = model()\n        \n    model.fit(train, train_labels)\n    train_valid = cross_val_mean_std(model, train, train_labels)\n    test_valid = cross_val_mean_std(model, test, test_labels)\n        \n    res_of_valid = {\"train_mean\": train_valid[0], \"train_std\": train_valid[1],\n                    \"test_mean\":  test_valid[0],  \"test_std\":  test_valid[1]}\n    \n    return res_of_valid, model","1a2c5646":"#  This function takes a dictionary derived from the work of a past function \n#  that contains a cross-validation result for one or more models and \n#  creates a Pandas table (which returns), optionally adding postfix to the column names.\n\ndef create_table_with_scores(res_of_valid, postfix=\"\"):\n    if not hasattr(res_of_valid[\"test_std\"], \"len\"):\n        index = [0]\n    else:\n        index = list(res_of_valid[\"test_std\"])\n\n    table = pd.DataFrame({\"Test mean score\" + postfix:  res_of_valid[\"test_mean\"],\n                          \"Test std score\" + postfix:   res_of_valid[\"test_std\"],\n                          \"Train mean score\" + postfix: res_of_valid[\"train_mean\"],\n                          \"Train std score\" + postfix:  res_of_valid[\"train_std\"]}, \n                          index=index)\n    return table","70398859":"#  This function takes a list of Pandas tables that are created by the function above, \n#  then it takes a list of names in text format (the length of the lists must match), \n#  there is an optional argument - the number of the column to sort, if necessary.\n#  Returns one large table that consists of a list of tables that the function has accepted, \n#  as well as a new column with model names from the second argument. \n#  If the third parameter was specified, the function returns the table with sorting.\n\ndef table_of_results(model_results, model_names=None, col_sort_by=None):\n    res = model_results[0]\n    for i in model_results[1:]:\n        res = res.append(i)\n    if model_names is not None:\n        names = []\n        for i, j in enumerate(model_names):\n            names += [j] * len(model_results[i])\n        res[\"Model name\"] = names\n    if col_sort_by is not None:\n        sort_by = res.columns[col_sort_by]\n        res = res.sort_values(by=sort_by, ascending=False)\n    res = res.reset_index(drop=True)\n    return res","34541c0f":"#  This function takes in a large table from the previous function as well \n#  as column numbers to draw a scatter chart. \n#  This function is used to draw cross-validation results from training and test data\n#  and compare different models or the same models with different parameters.\n\ndef graph_for_the_results_table(table, col_x, col_y, col_style):\n    x = table.columns[col_x]\n    y = table.columns[col_y]\n    style = table.columns[col_style]\n    plt.figure(figsize=[8, 8])\n    min_lim = min(min(table[x]), min(table[y]))\n    max_lim = max(max(table[x]), max(table[y]))\n    ax = sns.scatterplot(x, y, style, style=style, data=table, s=100)\n    ax.set_xlim(min_lim - 0.01 * max_lim, max_lim + 0.01 * max_lim)\n    ax.set_ylim(min_lim - 0.01 * max_lim, max_lim + 0.01 * max_lim)\n    ax.grid()\n    ax.set_axisbelow(True)","f2b1d51a":"train, test, train_labels, test_labels = train_test_split(data[data.columns[FEATURES]], \n                                                          data[data.columns[-2:]], \n                                                          test_size=0.25, random_state=3)\n\nb_train_labels = np.array(train_labels)[:, 1]\nb_test_labels = np.array(test_labels)[:, 1]\n\ntrain_labels = np.array(train_labels)[:, 0].astype(int)\ntest_labels = np.array(test_labels)[:, 0].astype(int)\n\nsc = StandardScaler()\ntrain = sc.fit_transform(train)\ntest = sc.fit_transform(test)","88640cd8":"classifiers_scores = []\nb_classifiers_scores = []\n\nclassifiers_importance = []\n\nfor i, clsf in enumerate(classifiers):\n    t = [0, 0]\n    \n    res_of_valid, t[0] = train_and_validate_model(clsf, train, train_labels, test, test_labels)\n    b_res_of_valid, t[1] = train_and_validate_model(clsf, train, b_train_labels, test, b_test_labels)\n    \n    classifiers_importance.append(t)\n    \n    classifiers_scores.append(create_table_with_scores(res_of_valid, \" ('quality')\"))\n    b_classifiers_scores.append(create_table_with_scores(b_res_of_valid, \" ('bin_quality')\"))\n    \nclassifiers_scores = table_of_results(classifiers_scores, model_names, 0)\nb_classifiers_scores = table_of_results(b_classifiers_scores, model_names, 0)","ed24ef6a":"classifiers_scores","ca23a477":"graph_for_the_results_table(classifiers_scores, 0, 2, 4)","2c6141f7":"graph_for_the_results_table(classifiers_scores, 1, 3, 4)","fd713c86":"b_classifiers_scores","a18f5414":"graph_for_the_results_table(b_classifiers_scores, 0, 2, 4)","568bf688":"graph_for_the_results_table(b_classifiers_scores, 1, 3, 4)","737ce048":"importances = []\nb_importances = []\n\nfor clsf, b_clsf in classifiers_importance:\n    if hasattr(clsf, \"feature_importances_\"):\n        importances.append(clsf.feature_importances_)\n        b_importances.append(b_clsf.feature_importances_)\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 6))\nfig.suptitle(\"The average importance of features\")\nsns.barplot(list(range(1, 12)), np.mean(importances, axis=0), ax=ax1)\nsns.barplot(list(range(1, 12)), np.mean(b_importances, axis=0), ax=ax2)\nax1.set_title(\"quality\")\nax2.set_title(\"bin_quality\")\nax1.get_yaxis().set_visible(False)\nax2.get_yaxis().set_visible(False)\nax1.set_xticklabels(data.columns, rotation=90)\nax2.set_xticklabels(data.columns, rotation=90);","4a786b46":"#  This function takes model and parameters to find optimal, training and test data, \n#  postfix for column names, number of iterations and partitions to cross-validate \n#  for RandomizedSearchCV.\n#  Returns only the table with the results.\n\ndef tuning_models(model, params, train, train_labels, \n                                 test, test_labels, postfix=\"\", iterations=50, cv=5):\n    \n    model_1 = model()\n    random_search = RandomizedSearchCV(model_1, params, iterations, scoring='accuracy', cv=cv)\n    random_search.fit(train, train_labels)\n    \n    parameter_set = []\n    mean_test_scores = list(random_search.cv_results_['mean_test_score'])\n    for i in sorted(mean_test_scores, reverse=True):\n        if i > np.mean(mean_test_scores):\n            parameter_set.append(random_search.cv_results_[\"params\"][mean_test_scores.index(i)])\n        \n    params_set_updated = []\n    for i in parameter_set:\n        if i not in params_set_updated:\n            params_set_updated.append(i)\n    \n    results = []\n    for i in params_set_updated:\n        res_of_valid, _ = train_and_validate_model(model, train, train_labels, test, test_labels, parameters=i)\n        results.append(create_table_with_scores(res_of_valid, postfix))\n    \n    results_table = table_of_results(results)\n    return results_table","1bce84fa":"params = {\"kernel\": [\"rbf\", \"poly\", \"linear\", \"sigmoid\"],\n          \"C\": np.arange(0.1, 1.5, 0.1), \n          \"gamma\": list(np.arange(0.1, 1.5, 0.1)) + [\"auto\"],\n          \"probability\": [True, False],\n          \"shrinking\": [True, False]}","e7b6a2b0":"svc_res = tuning_models(SVC, params, train, train_labels, \n                        test, test_labels, \" ('quality')\", 100)\n\nb_svc_res = tuning_models(SVC, params, train, b_train_labels, \n                          test, b_test_labels, \" ('bin_quality')\", 100)","3fbc4db5":"params = {\"n_estimators\": np.arange(1, 500, 2),\n          \"max_depth\": list(np.arange(2, 100, 2)) + [None],\n          \"min_samples_leaf\": np.arange(1, 20, 1),\n          \"min_samples_split\": np.arange(2, 20, 2),\n          \"max_features\": [\"auto\", \"log2\", None]}","12cabe82":"extra_res = tuning_models(ExtraTreesClassifier, params, train, train_labels, \n                          test, test_labels, \" ('quality')\", 100)\n\nb_extra_res = tuning_models(ExtraTreesClassifier, params, train, b_train_labels, \n                            test, b_test_labels, \" ('bin_quality')\", 100)\n\nforest_res = tuning_models(RandomForestClassifier, params, train, train_labels, \n                           test, test_labels, \" ('quality')\", 100)\n\nb_forest_res = tuning_models(RandomForestClassifier, params, train, b_train_labels, \n                             test, b_test_labels, \" ('bin_quality')\", 100)","cab9e838":"params = {\"boosting_type\": [\"gbdt\"],\n          \"num_leaves\": np.arange(2, 100, 2),\n          \"max_depth\": list(np.arange(2, 100, 2)) + [-1],\n          \"learning_rate\": [0.001, 0.003, 0.006, 0.01, 0.02, 0.04, 0.06, 0.08, 0.1, 0.12, 0.14, 0.17, 0.2, 0.3, 0.4],\n          \"n_estimators\": np.arange(2, 300, 5),\n          \"reg_alpha\": np.arange(0, 1, 0.1),\n          \"reg_lambda\": np.arange(0, 1, 0.1)}","434e7790":"lgb_res = tuning_models(LGBMClassifier, params, train, train_labels, \n                        test, test_labels, \" ('quality')\", 100)\n\nb_lgb_res = tuning_models(LGBMClassifier, params, train, b_train_labels, \n                          test, b_test_labels, \" ('bin_quality')\", 100);","b8b21d56":"all_results = table_of_results([svc_res, extra_res, forest_res, lgb_res], \n                               [\"SVC\", \"ExtraTrees\", \"RandomForest\", \"LightGBM\"], 0)\nall_results.head(10)","88342aab":"graph_for_the_results_table(all_results, 0, 2, 4)","db14aa50":"graph_for_the_results_table(all_results, 1, 3, 4)","05b8f45f":"b_all_results = table_of_results([b_svc_res, b_extra_res, b_forest_res, b_lgb_res], \n                                 [\"SVC\", \"ExtraTrees\", \"RandomForest\", \"LightGBM\"], 0)\nb_all_results.head(10)","3fb477e7":"graph_for_the_results_table(b_all_results, 0, 2, 4)","bf387630":"graph_for_the_results_table(b_all_results, 1, 3, 4)","456f2d4e":"<img src=\"https:\/\/adpearance.com\/images\/blog\/Spam_Trees.png\" width=600>","1c8dc3b2":"### LGBMClassifier: parameters and tuning <a id=\"26\"><\/a>","cc4779a7":"## Training and testing standard models <a id=\"21\"><\/a>\nVariables prefixed with \"b_\" are the variables associated with a binary classification","d049a89e":"### 7. Gradient Boosting (in our case LGBMClassifier) <a id=\"19\"><\/a>","bed3a363":"### A set of classifiers <a id=\"11\"><\/a>","b4a57eef":"In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.\n\nIn addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.\n\nWhen data is unlabelled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The support vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data, and is one of the most widely used clustering algorithms in industrial applications.","5eb3db79":"## Dependence of quality on different parameters <a id=\"8\"><\/a>","3cb0ea6d":"## Configure the models <a id=\"23\"><\/a>\nWe will configure 4 models: SVC, ExtraTreesClassifier, RandomForestClassifier, LGBMClassifier, as they give results above average according to my estimates.","c455a608":"# Working with data <a id=\"p1\"><\/a>\n## Import Python libraries <a id=\"1\"><\/a>","cd41d801":"### 5. ExtraTreesClassifier & RandomForestClassifier <a id=\"17\"><\/a>\nThe principle of operation is similar, if not too go into the essence of the algorithms, so they are in one point.","6437edc9":"<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/7\/72\/SVM_margin.png\/440px-SVM_margin.png\" width=500>","82e78ec6":"## Visualize the results of tuned models <a id=\"27\"><\/a>\n#### For classification","f6a48360":"### SVC: parameters and tuning <a id=\"24\"><\/a>","82812d62":"## Loading the dataset <a id=\"2\"><\/a>","a521b9c5":"Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.\n\nLDA is closely related to analysis of variance (ANOVA) and regression analysis, which also attempt to express one dependent variable as a linear combination of other features or measurements. However, ANOVA uses categorical independent variables and a continuous dependent variable, whereas discriminant analysis has continuous independent variables and a categorical dependent variable (i.e. the class label). Logistic regression and probit regression are more similar to LDA than ANOVA is, as they also explain a categorical variable by the values of continuous independent variables. These other methods are preferable in applications where it is not reasonable to assume that the independent variables are normally distributed, which is a fundamental assumption of the LDA method.\n\nLDA is also closely related to principal component analysis (PCA) and factor analysis in that they both look for linear combinations of variables which best explain the data. LDA explicitly attempts to model the difference between the classes of data. PCA on the other hand does not take into account any difference in class, and factor analysis builds the feature combinations based on differences rather than similarities. Discriminant analysis is also different from factor analysis in that it is not an interdependence technique: a distinction between independent variables and dependent variables (also called criterion variables) must be made.\n\nLDA works when the measurements made on independent variables for each observation are continuous quantities. When dealing with categorical independent variables, the equivalent technique is discriminant correspondence analysis.\n\nDiscriminant analysis is used when groups are known a priori (unlike in cluster analysis). Each case must have a score on one or more quantitative predictor measures, and a score on a group measure. In simple terms, discriminant function analysis is classification - the act of distributing things into groups, classes or categories of the same type. ","9f5fe27e":"#### For binary classification","ceddd15f":"Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.\n\nThe idea of gradient boosting originated in the observation by Leo Breiman that boosting can be interpreted as an optimization algorithm on a suitable cost function. Explicit regression gradient boosting algorithms were subsequently developed by Jerome H. Friedman, simultaneously with the more general functional gradient boosting perspective of Llew Mason, Jonathan Baxter, Peter Bartlett and Marcus Frean. The latter two papers introduced the view of boosting algorithms as iterative functional gradient descent algorithms. That is, algorithms that optimize a cost function over function space by iteratively choosing a function (weak hypothesis) that points in the negative gradient direction. This functional gradient view of boosting has led to the development of boosting algorithms in many areas of machine learning and statistics beyond regression and classification. ","fba29d25":"## Life-simplifying functions <a id=\"20\"><\/a>","5a3c205f":"Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.\n\nThe first algorithm for random decision forests was created by Tin Kam Ho using the random subspace method, which, in Ho's formulation, is a way to implement the \"stochastic discrimination\" approach to classification proposed by Eugene Kleinberg.\n\nAn extension of the algorithm was developed by Leo Breiman and Adele Cutler, and \"Random Forests\" is their trademark. The extension combines Breiman's \"bagging\" idea and random selection of features, introduced first by Ho and later independently by Amit and Geman in order to construct a collection of decision trees with controlled variance. ","36b70b24":"### Thank you for opening my notebook.\n\nThis notebook predicted the quality of red wine by using multiple classifiers of the most popular and least useful library scikit-learn and library for gradient boosting, which worked well lately, lightgbm.  Before that, the entire dataset is analyzed using a graphical representation of the data, parameters and their dependencies among themselves.\n\nIf you find this notebook entertaining, interesting or useful (well, suddenly), be sure to write a comment that could be improved in this and in the following notebooks. <b>While there, write a comment with your thoughts about this notebook definitely!<\/b>\n\n\u0421ontent:\n- [Part One: Working with data](#p1)\n  - [Import Python libraries](#1)\n  - [Loading the dataset](#2)\n  - [Quick view of raw data](#3)\n  - [Create a binary quality column](#4)\n  - [Visualization of dataset](#5)\n    - [The distribution of unique quality values](#6)\n    - [Correlation heatmap](#7)\n  - [Dependence of quality on different parameters](#8)\n\n- [Part Two: The use of machine learning for classification](#p2)\n  - [Import libraries: sklearn and lightgbm](#10)\n    - [A set of classifiers](#11)\n  - [Short description each algorithm](#12)\n    - [1. Logistic Regression](#13)\n    - [2. k-Nearest Neighbors (kNN)](#14)\n    - [3. Support Vector Machine (SVM)](#15)\n    - [4. Multilayer Perceptron classifier (MLPClassifier)](#16)\n    - [5. ExtraTreesClassifier and RandomForestClassifier](#17)\n    - [6. Fisher's linear discriminant or LinearDiscriminantAnalysis (LDA)](#18)\n    - [7. Gradient Boosting (in our case LGBMClassifier)](#19)\n  - [Life-simplifying functions](#20)\n  - [Training and testing standard models](#21)\n  - [Visualization of the first results](#22)\n  - [Configure the models](#23)\n    - [SVC: parameters and tuning](#24)\n    - [ExtraTreesClassifier and RandomForestClassifier: parameters and tuning](#25)\n    - [LGBMClassifier: parameters and tuning](#26)\n  - [Visualize the results of tuned models](#27)\n  \nThe author's English is not his native language, so the author uses a translator.\nThe native language of the author of the Russian.","9a1c1a0f":"#### Importance of parameters for each type of classification","0433d8e0":"A multilayer perceptron (MLP) is a class of feedforward artificial neural network. An MLP consists of, at least, three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that uses a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training. Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. It can distinguish data that is not linearly separable.\n\nMultilayer perceptrons are sometimes colloquially referred to as \"vanilla\" neural networks, especially when they have a single hidden layer.","2057a851":"### 4. Multilayer Perceptron classifier (MLPClassifier) <a id=\"16\"><\/a>","af2e628d":"### ExtraTreesClassifier and RandomForestClassifier: parameters and tuning <a id=\"25\"><\/a>","b5f4e773":"<img src=\"https:\/\/impossible.works\/thumbs\/impossible-news\/international-news\/ti-tha-ginei-me-ta-makedonika-emporika-simata-meta-ti-simfwnia\/simata-1024x576.jpg\" width=\"500\">","6177228f":"### Correlation heatmap <a id=\"7\"><\/a>","19763bd1":"In pattern recognition, the k-nearest neighbors algorithm (k-NN) is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:\n\n        In k-NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n\n        In k-NN regression, the output is the property value for the object. This value is the average of the values of its k nearest neighbors.\n\nk-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until classification. The k-NN algorithm is among the simplest of all machine learning algorithms.\n\nBoth for classification and regression, a useful technique can be used to assign weight to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1\/d, where d is the distance to the neighbor.\n\nThe neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.\n\nA peculiarity of the k-NN algorithm is that it is sensitive to the local structure of the data.","277fe706":"<img src=\"https:\/\/i.imgur.com\/3KqBd3C.png\" width=600>","9f094c13":"<img src=\"https:\/\/blog.knowledgent.com\/wp-content\/uploads\/2016\/01\/PartII-1024x765.jpg\" width=600>","731761bd":"1. <img src=\"https:\/\/pbs.twimg.com\/media\/DSTrDtVUIAA3CZb.jpg\">","41d5ddeb":"<img src=\"http:\/\/coxdocs.org\/lib\/exe\/fetch.php?media=perseus:user:activities:matrixprocessing:learning:lda2.png\">","05a97038":"<img src=\"https:\/\/plot.ly\/~florianh\/140.png\">","d52a56e7":"## Create a binary quality column <a id=\"4\"><\/a>\nbad: quality < 6.5<br>\ngood: quality > 6.5","adf35c21":"### 2. k-Nearest Neighbors (kNN) <a id=\"14\"><\/a>","1cb26bf2":"## Short description each algorithm <a id=\"12\"><\/a>\nDescription of algorithms without explanation from Wikipedia","98fa20a1":"### 1. Logistic Regression <a id=\"13\"><\/a>","5ac45fa9":"<h1 align=\"center\">Red Wine Quality Analysis<\/h1>","3e0b6384":"Divide the data into training and test samples. <br>Labels are also divisible for classifications and for binary classifications.<br>Normalize data for better learning outcomes.","30d8783d":"## Quick view of raw data <a id=\"3\"><\/a>","eb003390":"## Visualization of the first results <a id=\"22\"><\/a>\n#### For classification","2cd0c5c0":"Training and testing of models with standard parameters.","a4746fba":"### 6. Fisher's linear discriminant or LinearDiscriminantAnalysis (LDA) <a id=\"18\"><\/a>","d0919878":"# The use of machine learning for classification <a id=\"p2\"><\/a>\n## Import libraries: sklearn and lightgbm <a id=\"10\"><\/a>","68b0a963":"In statistics, the logistic model (or logit model) is a widely used statistical model that, in its basic form, uses a logistic function to model a binary dependent variable; many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model; it is a form of binomial regression. Mathematically, a binary logistic model has a dependent variable with two possible values, such as pass\/fail, win\/lose, alive\/dead or healthy\/sick; these are represented by an indicator variable, where the two values are labeled \"0\" and \"1\". In the logistic model, the log-odds (the logarithm of the odds) for the value labeled \"1\" is a linear combination of one or more independent variables (\"predictors\"); the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled \"1\" can vary between 0 (certainly the value \"0\") and 1 (certainly the value \"1\"), hence the labeling; the function that converts log-odds to probability is the logistic function, hence the name. The unit of measurement for the log-odds scale is called a logit, from logistic unit, hence the alternative names. Analogous models with a different sigmoid function instead of the logistic function can also be used, such as the probit model; the defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at a constant rate, with each dependent variable having its own parameter; for a binary independent variable this generalizes the odds ratio. <br><br>Logistic regression was developed by statistician David Cox in 1958. The binary logistic regression model has extensions to more than two levels of the dependent variable: categorical outputs with more than two values are modelled by multinomial logistic regression, and if the multiple categories are ordered, by ordinal logistic regression, for example the proportional odds ordinal logistic model. The model itself simply models probability of output in terms of input, and does not perform statistical classification (it is not a classifier), though it can be used to make a classifier, for instance by choosing a cutoff value and classifying inputs with probability greater than the cutoff as one class, below the cutoff as the other; this is a common way to make a binary classifier. The coefficients are generally not computed by a closed-form expression, unlike linear least squares. ","7bee1142":"A positive correlation means that as the value of the first parameter increases, so will the second one. For example, with a higher content of acid in the red wine, the wine has a higher density, since the acids themselves are denser than water.\n\nA negative correlation means that as the value of the first parameter increases, the value of the second parameter decreases. For example, with a higher acid content in red wine, the pH value will be lower, indicating an acidic environment.\n\nThe closer the correlation value is to zero, the less the parameters affect each other.","8af4baa7":"#### For binary classification","82f82d94":"### 3. Support Vector Machine (SVM) <a id=\"15\"><\/a>\nIn our case, the classifier, so Support Vector Classifier (SVC)","55b977f8":"## Visualization of dataset <a id=\"5\"><\/a>\n### The distribution of unique quality values <a id=\"6\"><\/a>"}}