{"cell_type":{"5b208038":"code","72c1e52f":"code","dd6ae732":"code","fcc5993e":"code","ec6e427c":"code","1f8fd02d":"code","1364b1db":"code","6622f459":"code","0596553c":"code","a13f73f5":"code","99e7cbd3":"code","d8744931":"code","e8ea2a72":"code","262a3560":"code","36679d19":"code","e954eefd":"code","306e4ca4":"code","925cd96d":"code","31eb88b5":"code","69ff6d11":"code","3abe4482":"code","66ef20b3":"code","e2905805":"code","e64f6303":"code","488ef67a":"markdown","f32982f6":"markdown","da957b79":"markdown","a88ec6e3":"markdown","e638208d":"markdown","d3f8315a":"markdown","4fe76053":"markdown","abf5e7bc":"markdown","02dcbb7b":"markdown","5f3dde30":"markdown","e33a747e":"markdown","e1590703":"markdown","0f988879":"markdown","f9d48549":"markdown","00b55d4f":"markdown"},"source":{"5b208038":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow.keras import models, layers, regularizers\nfrom sklearn.model_selection import KFold\nimport datatable\nimport gc\nimport warnings\nsns.set_style('darkgrid')\nwarnings.filterwarnings('ignore')\nSEED = 2222\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)","72c1e52f":"train_path = '..\/input\/jane-street-market-prediction\/train.csv'\n\n# use datatable to load big data file\ntrain_file = datatable.fread(train_path).to_pandas()\ntrain_file.info()","dd6ae732":"# It is found from info() that there are only two datatypes - float64 and int32\nfor c in train_file.columns:\n    min_val, max_val = train_file[c].min(), train_file[c].max()\n    if train_file[c].dtype == 'float64':\n        if min_val>np.finfo(np.float16).min and max_val<np.finfo(np.float16).max:\n            train_file[c] = train_file[c].astype(np.float16)\n        elif min_val>np.finfo(np.float32).min and max_val<np.finfo(np.float32).max:\n            train_file[c] = train_file[c].astype(np.float32)\n    elif train_file[c].dtype == 'int32':\n        if min_val>np.iinfo(np.int8).min and max_val<np.iinfo(np.int8).max:\n            train_file[c] = train_file[c].astype(np.int8)\n        elif min_val>np.iinfo(np.int16).min and max_val<np.iinfo(np.int16).max:\n            train_file[c] = train_file[c].astype(np.int16)\ntrain_file.info()","fcc5993e":"print('There are %s NAN values in the train data'%train_file.isnull().sum().sum())","ec6e427c":"features = train_file.columns[train_file.columns.str.contains('feature')]\n\nval_range = train_file[features].max()-train_file[features].min()\nfiller = pd.Series(train_file[features].min()-0.01*val_range, index=features)\n# This filler value will be used as a constant replacement of missing values \n\n\n\"\"\"\nA function to fill all missing values with negative outliers as discussed in the referred notebook\nhttps:\/\/www.kaggle.com\/rajkumarl\/jane-tf-keras-lstm\n\"\"\"\ndef fill_missing(df):\n    df[features] = df[features].fillna(filler)\n    return df  \n\ntrain = fill_missing(train_file)\ntrain = train.loc[train.weight > 0]\ntrain.info()","1f8fd02d":"print(\"Now we have %d missing values in our data\" %train.isnull().sum().sum())","1364b1db":"\"\"\"\nfrom notebook\nhttps:\/\/www.kaggle.com\/rajkumarl\/jane-day-242-feature-generation-and-selection\/comments\nbased on selected features\n\"\"\"\ndef feature_transforms(df):\n    # Generate Features using Linear shifting, Natural Logarithm and Square Root\n    for f in [f'feature_{i}' for i in [1,2,6,7,9,10,20,25,35,37,38,39,40,42,50,51,52,53,54,56,69,70,71,83,97,109,112,122,123,124,126,128,129]]: \n        # linear shifting to value above 1.0\n        df['pos_'+str(f)] = (df[f]+abs(train[f].min())+1).astype(np.float16)\n    for f in [f'feature_{i}' for i in [1,2,6,7,20,25,35,37,38,39,40,42,50,51,52,53,54,69,70,71,97,109,112,122,123,126,128,129]]: \n        # Natural log of all the values\n        df['log_'+str(f)] = np.log(df['pos_'+str(f)]).astype(np.float16)\n    for f in [f'feature_{i}' for i in [1,2,6,9,10,37,38,39,40,50,51,52,53,54,56,69,70,71,83,109,112,122,123,124,126,128,129]]: \n        # Square root of all the values\n        df['sqrt_'+str(f)] = np.sqrt(df['pos_'+str(f)]).astype(np.float16)\n    \n    # Linearly shifted values are used for log and sqrt transformations\n    # However they are useless since we have our original values which are 100% correlated\n    # Let's drop them from our data\n    df.drop([f'pos_feature_{i}' for i in [1,2,6,7,9,10,20,25,35,37,38,39,40,42,50,51,52,53,54,56,69,70,71,83,97,109,112,122,123,124,126,128,129]], inplace=True, axis=1)\n    \n    # From the Shap Dependence plots, the following features seem to have cubic relationship with target\n    cubic = [37, 39]\n    for i in cubic:\n        f = f'feature_{i}'\n        threes = np.full((len(df[f])), 3)\n        df['cub_'+f] =np.power(df[f], threes) \n        \n    # From the Shap Dependence plots, the following features seem to have quadratic relationship with target\n    quad = [53, 64, 67, 68]\n    for i in quad:\n        f = f'feature_{i}'\n        df['quad_'+f] =np.square(df[f]) \n    \n    return df","6622f459":"\"\"\"\nfrom notebook\nhttps:\/\/www.kaggle.com\/rajkumarl\/jane-day-242-feature-generation-and-selection\/comments\nbased on selected features\n\"\"\"\ndef manipulate_pairs(df):\n    # features that can be added together or subtracted\n    sub_pairs = [(3,6),(30,37)]\n    for i,j in sub_pairs:\n        df[f'sub_{i}_{j}'] = df[f'feature_{i}']-df[f'feature_{j}']\n\n    add_pairs = [(35,39)]\n    for i,j in add_pairs:\n        df[f'add_{i}_{j}'] = df[f'feature_{i}']+df[f'feature_{j}']\n\n    sub_log_pairs = [(9,20), (29,25), (109,7),(112,97)]\n    for i,j in sub_log_pairs:\n        df[f'sub_{i}_log{j}'] = df[f'feature_{i}']-df[f'log_feature_{j}']\n    \n    add_log_pairs = [(9,20), (29,25), (109,7), (112,97)]\n    for i,j in add_log_pairs:\n        df[f'add_{i}_log{j}'] = df[f'feature_{i}']+df[f'log_feature_{j}']\n        \n    # features that can be multiplied together\n    mul_pairs = [(39,95), (122,35)]\n    for i,j in mul_pairs:\n        df[f'mul_{i}_{j}'] = df[f'feature_{i}']*df[f'feature_{j}']\n\n    mul_log_pairs = [(6,42),(122,35)]\n    for i,j in mul_log_pairs:\n        df[f'mul_{i}_log{j}'] = df[f'feature_{i}']*df[f'log_feature_{j}']\n    return df","0596553c":"# Perform feature generation\ntrain = feature_transforms(train)\ntrain = manipulate_pairs(train)","a13f73f5":"\"\"\"\nfrom notebook\nhttps:\/\/www.kaggle.com\/rajkumarl\/jane-day-242-feature-generation-and-selection\/comments\n\"\"\"\nselected_features = ['weight', 'feature_1', 'feature_2', 'feature_6', 'feature_9',\n       'feature_10', 'feature_16', 'feature_20', 'feature_29', 'feature_37',\n       'feature_38', 'feature_39', 'feature_40', 'feature_51', 'feature_52',\n       'feature_53', 'feature_54', 'feature_69', 'feature_70', 'feature_71',\n       'feature_83', 'feature_100', 'feature_109', 'feature_112',\n       'feature_122', 'feature_123', 'feature_124', 'feature_126',\n       'feature_128', 'feature_129', 'log_feature_1', 'log_feature_2',\n       'log_feature_6', 'log_feature_37', 'log_feature_38', 'log_feature_39',\n       'log_feature_40', 'log_feature_50', 'log_feature_51', 'log_feature_52',\n       'log_feature_53', 'log_feature_54', 'log_feature_69', 'log_feature_70',\n       'log_feature_71', 'log_feature_109', 'log_feature_112',\n       'log_feature_122', 'log_feature_123', 'log_feature_126',\n       'log_feature_128', 'log_feature_129', 'sqrt_feature_1',\n       'sqrt_feature_2', 'sqrt_feature_6', 'sqrt_feature_9', \n       'sqrt_feature_10', 'sqrt_feature_37', 'sqrt_feature_38', 'sqrt_feature_39',\n       'sqrt_feature_40', 'sqrt_feature_50', 'sqrt_feature_51',\n       'sqrt_feature_52', 'sqrt_feature_53', 'sqrt_feature_54',\n       'sqrt_feature_56', 'sqrt_feature_69', 'sqrt_feature_70',\n       'sqrt_feature_71', 'sqrt_feature_83', 'sqrt_feature_109',\n       'sqrt_feature_112', 'sqrt_feature_122', 'sqrt_feature_123',\n       'sqrt_feature_124', 'sqrt_feature_126', 'sqrt_feature_128',\n       'sqrt_feature_129', 'cub_feature_37', 'cub_feature_39',\n       'quad_feature_53', 'quad_feature_64', 'quad_feature_67',\n       'quad_feature_68', 'sub_3_6', 'sub_30_37', 'add_35_39', 'add_9_log20',\n       'sub_9_log20', 'add_29_log25', 'sub_29_log25', 'add_109_log7',\n       'sub_109_log7', 'add_112_log97', 'sub_112_log97', 'mul_39_95',\n       'mul_122_35', 'mul_6_log42', 'mul_122_log35']","99e7cbd3":"X = train[selected_features]\ny = train[[c for c in train.columns if 'resp' in c]]\ny = (y>0)*1\ny = (y.mean(axis=1)>0.5).astype(np.int64)\nX.shape, y.shape","d8744931":"# Expand dimension of dataset to adopt in the neural network\nX = np.expand_dims(X,-2)\ny = np.expand_dims(y,-1)\nX.shape, y.shape","e8ea2a72":"# save some memory\ndel train_file, train\ngc.collect()","262a3560":"cv_data = []\nfor train, val in KFold(3).split(X,y):\n    train_data = tf.data.Dataset.from_tensor_slices((X[train],y[train]))\n    val_data = tf.data.Dataset.from_tensor_slices((X[val],y[val]))\n    cv_data.append((train_data, val_data))","36679d19":"# save some memory\ndel X, y\ngc.collect()","e954eefd":"cv_data","306e4ca4":"class Residual(tf.keras.Model):  \n    \"\"\"The Residual layer of ResNet\"\"\"\n    def __init__(self, units):\n        super().__init__()\n        self.d1 = layers.Dense(units, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))\n        self.d2 = layers.Dense(units, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))\n        self.d3 = layers.Dense(units, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))\n        self.bn1 = layers.BatchNormalization()\n        self.bn2 = layers.BatchNormalization()\n\n    def call(self, X):\n        Y = tf.keras.activations.relu(self.bn1(self.d1(X)))\n        Y = layers.Dropout(0.3)(self.bn2(self.d2(Y)))\n        X = self.d3(X)\n        Y += X\n        return layers.Dropout(0.3)(tf.keras.activations.relu(Y))","925cd96d":"class ResnetBlock(layers.Layer):\n    def __init__(self, num_units, num_residuals):\n        super(ResnetBlock, self).__init__()\n        self.residual_layers = []\n        for i in range(num_residuals):\n            self.residual_layers.append(Residual(num_units))\n\n    def call(self, X):\n        for layer in self.residual_layers.layers:\n            X = layer(X)\n        return X","31eb88b5":"def create_model():\n    model= tf.keras.Sequential([\n        layers.Input(shape=(100,)),\n        layers.GaussianNoise(0.2),\n        layers.Dense(64, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)),\n        layers.BatchNormalization(),\n        layers.Activation('relu'),\n        layers.Dropout(0.5),\n        ResnetBlock(64, 2),\n        ResnetBlock(128, 2),\n        ResnetBlock(256, 2),\n        ResnetBlock(512, 2),\n        layers.Dense(64, activation='relu'),\n        layers.Dense(1, activation='sigmoid')])\n    \n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n                  loss=tf.keras.losses.BinaryCrossentropy(), \n                  metrics=['accuracy'])\n    return model\n","69ff6d11":"filler","3abe4482":"train_accu = []\ntrain_loss = []\nval_accu = []\nval_loss = []\nmodels = []\nfold = 1\nfor train, val in cv_data:\n    model = create_model()\n    hist = model.fit(train_data.repeat(), \n                 epochs=40, \n                 steps_per_epoch=200, \n                 batch_size=5120, \n                 validation_data=val_data, \n                 validation_steps=100, \n                 verbose=0)\n    models.append(model)\n    train_accu.append(hist.history['accuracy'])\n    train_loss.append(hist.history['loss'])\n    val_accu.append(hist.history['val_accuracy'])\n    val_loss.append(hist.history['val_loss'])\n    print('\\n Fold completed: ',fold, '\\n')\n    fold += 1    ","66ef20b3":"# plt.figure(figsize=(12,4))\nfor i in range(3):\n    plt.figure(figsize=(12,4))\n    plt.plot(train_loss[i], label=f'training_{i+1}')\n    plt.plot(val_loss[i], label=f'validation_{i+1}')\n    plt.ylabel('Binary CrossEntropy Loss', size=16)\n    plt.xlabel('Number of Epochs', size=16)\n    plt.title('Training vs Validation Loss', color='r', size=20)\n    plt.ylim([0.68,0.71])\n    plt.legend()\n    plt.show()","e2905805":"# plt.figure(figsize=(12,4))\nfor i in range(3):\n    plt.figure(figsize=(12,4))\n    plt.plot(train_accu[i], label=f'training_{i+1}')\n    plt.plot(val_accu[i], label=f'validation_{i+1}')\n    plt.ylabel('Accuracy', size=16)\n    plt.xlabel('Number of Epochs', size=16)\n    plt.title('Training vs Validation Loss', color='r', size=20)\n    plt.legend()\n    plt.show()","e64f6303":"for m in range(3):\n    model = models[m]\n    model.save_weights(f'resnet_select_feature_{m+1}.h5', save_format='h5')\n    print(f'model_{m+1} weights saved successfully.')","488ef67a":"### Split data into examples (X) with `selected_features` and target (y) ","f32982f6":"### Thank you for your time!","da957b79":"# 4. FEATURE GENERATION AND SELECTION","a88ec6e3":"# 6. RESIDUAL NETWORK MODELING","e638208d":"# 3. HANDLING MISSING VALUES","d3f8315a":"### That's a great reduction in memory usage (around 74% reduction)! It will help us go further efficiently!","4fe76053":"### Let's incorporate 5-fold Cross Validation","abf5e7bc":"## Save model for inference","02dcbb7b":"# 2. LOAD DATA AND OPTIMIZE MEMORY","5f3dde30":"# 5. PREPARE DATA FOR RESIDUAL NEURAL NET","e33a747e":"# 1. IMPORT LIBRARIES","e1590703":"### Model performance seems quite same for all three folds of data","0f988879":"### This notebook has the following Notebooks as reference on detailed analysis and reasonable way of handling the missing values, and feature generation and selection. Apart from that there are few other good notebooks from where this notebook got some value addition pieces of ideas! I wish to thank my fellow kagglers who compel me to learn and grow!\n\n### [Kaggle Notebook] [Jane TF Keras LSTM](https:\/\/www.kaggle.com\/rajkumarl\/jane-tf-keras-lstm) (to fill missing values)\n### [Kaggle Notebook] [Jane Day 242 Feature Generation and Selection](https:\/\/www.kaggle.com\/rajkumarl\/jane-day-242-feature-generation-and-selection) (to generate and select features)\n","f9d48549":"### Let's visualize the model performance","00b55d4f":"# 7. TRAINING"}}