{"cell_type":{"b690ad8f":"code","a10a8a70":"code","0b8f452f":"code","74392a04":"code","fd96a7ef":"code","8021992d":"code","1a92df67":"code","58aa7a69":"code","f718e6da":"code","3900cb94":"code","0dfc6f55":"code","62629024":"code","67c70430":"code","fa692d51":"code","cea1b3d2":"code","c69965c3":"code","c3c01050":"code","4bff3017":"code","a7cca5b6":"code","f2dea94b":"code","0ebc587a":"code","b93a2756":"code","7023665a":"code","dec9ea5a":"code","d5327687":"code","1000e379":"code","98e58859":"code","7fed3a1e":"code","1be3a4aa":"code","b84b5304":"code","ee7adaf2":"code","7ad16454":"markdown","ae7b5552":"markdown","e96e15c1":"markdown","5acdedcc":"markdown","8e764f5a":"markdown","cdd1c786":"markdown"},"source":{"b690ad8f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a10a8a70":"tweet= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsubmission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")","0b8f452f":"print('There are {} rows and {} columns in train'.format(tweet.shape[0],tweet.shape[1]))\nprint('There are {} rows and {} columns in train'.format(test.shape[0],test.shape[1]))","74392a04":"import re\nimport string","fd96a7ef":"df=([tweet,test])\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\nfor dat in df:\n    dat['text']=dat['text'].apply(lambda x : remove_punct(x))\n    dat['text']=dat['text'].apply(lambda x : remove_emoji(x))\n    dat['text']=dat['text'].apply(lambda x : remove_html(x))\n    dat['text']=dat['text'].apply(lambda x : remove_URL(x))","8021992d":"import torch\nfrom transformers import DistilBertModel\nbert_distill= DistilBertModel.from_pretrained(\"distilbert-base-uncased\")","1a92df67":"from transformers import DistilBertTokenizer\ntokenizer= DistilBertTokenizer.from_pretrained('distilbert-base-uncased',do_lower_case=True)","58aa7a69":"def tokenize(data,tokenizer):\n    return data[\"text\"].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))","f718e6da":"def pad(tokenized):\n    max_len = 0\n    for i in tokenized.values:\n        if len(i) > max_len:\n            max_len = len(i)\n\n    return np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])","3900cb94":"def mask(padded):\n    return np.where(padded != 0, 1, 0)","0dfc6f55":"def preprocess(data,tokenizer):\n    tokenized = tokenize(data,tokenizer)\n    padded = pad(tokenized)\n    attention_mask = mask(padded)\n    return padded,attention_mask","62629024":"tokenized = tokenize(tweet,tokenizer)\npadded = pad(tokenized)\ntarget=np.asarray(tweet['target'])","67c70430":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(padded,target , test_size=0.15, random_state=42)","fa692d51":"print(\"X_train.shape = {} X_test.shape = {} y_train.shape = {} y_test.shape = {} \".format(X_train.shape,X_test.shape,y_train.shape,y_test.shape))","cea1b3d2":" \n    train_attention_mask = np.where(X_train != 0, 1, 0)\n    test_attention_mask = np.where(X_test != 0, 1, 0)\n    print(\"train {} test {} \".format(train_attention_mask.shape,test_attention_mask.shape))","c69965c3":"input_ids = torch.tensor(X_train,device=\"cuda\")  \nattention_mask = torch.tensor(train_attention_mask,device=\"cuda\")\ninput_ids_test = torch.tensor(X_test,device=\"cuda\")  \nattention_mask_test = torch.tensor(test_attention_mask,device=\"cuda\")","c3c01050":"y_train=torch.tensor(y_train,device=\"cuda\")\ny_test= torch.tensor(y_test,device=\"cuda\")","4bff3017":"import torch                                        # root package\nfrom torch.utils.data import TensorDataset, DataLoader    ","a7cca5b6":"bs=32\ntrain_ds = TensorDataset(input_ids,attention_mask,y_train)\ntrain_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n\nvalid_ds = TensorDataset(input_ids_test,attention_mask_test,y_test)\nvalid_dl = DataLoader(valid_ds, batch_size=bs * 2)","f2dea94b":"from torch import nn\nimport torch.nn.functional as F\nclass tweet_model(nn.Module):\n  \n    def __init__(self,bert_model):\n        super(tweet_model,self).__init__()\n        self.bert_model = bert_model\n        self.lin1 = nn.Linear(768,512)\n        self.lin2 = nn.Linear(512,64)\n        self.lin3 = nn.Linear(64,1)\n        #self.sigmoid = nn.Sigmoid()\n        self.dropout1=nn.Dropout(0.4)\n        self.dropout2=nn.Dropout(0.2)\n\n    def forward(self, input_ids,attention_mask):\n        # Return only the logits from the transfomer\n        output = self.bert_model(input_ids,attention_mask=attention_mask)\n        #print(\"a={} b={} c={} \".format(output[0].shape,input_ids.shape,attention_mask.shape))\n        output = output[0][:,0,:]\n        output = self.dropout1(output)\n        output= F.relu(self.lin1(output))\n        output = self.dropout2(output)\n        output=F.relu(self.lin2(output))\n        output=F.sigmoid(self.lin3(output))\n        \n        return output              ","0ebc587a":"model=tweet_model(bert_distill)\nmodel.cuda()","b93a2756":"for param in model.bert_model.parameters():\n    param.requires_grad = False","7023665a":"from torch import optim\nlr=0.001\nopt = optim.Adam(model.parameters(),lr=lr)","dec9ea5a":"def loss_batch(model,loss_func,input_ids,attention_mask,y_train, opt=None):\n#    loss_func = nn.BCELoss()\n    y_tensor = torch.tensor(y_train, dtype=torch.float,device=\"cuda\",requires_grad=False)\n    loss = loss_func(model(input_ids,attention_mask=attention_mask), y_tensor)\n\n    if opt is not None:\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\n    return loss.item()\n\n\ndef fit(epochs, model,loss_func, opt, train_dl, valid_dl):\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        for input_ids,attention_mask,y_train in train_dl:\n            loss = loss_batch(model,loss_func,input_ids,attention_mask,y_train, opt) \n            total_loss+=loss\n        print(\"trainLOSS = \" + str(total_loss\/len(train_dl)))\n        model.eval()\n        with torch.no_grad():\n            total_loss1 = 0\n            for input_ids,attention_mask,y_train in valid_dl:\n                loss = loss_batch(model,loss_func,input_ids,attention_mask,y_train,None) \n                total_loss1+=loss\n        print(\"validLOSS = \" + str(total_loss1\/len(valid_dl))+ \" epoch = \" + str(epoch) )\n\n            \n        ","d5327687":"import torch.nn.functional as F\nloss_func = F.binary_cross_entropy\nfit(45,model,loss_func,opt,train_dl,valid_dl)","1000e379":"def predict(input_ids,attention_mask,model):\n    with torch.no_grad():\n        predicted = model(input_ids,attention_mask=attention_mask)\n        predicted = predicted.cpu().numpy()\n       #print(predicted)\n\n        return predicted.round()","98e58859":"def accuracy(valid_ds,model):\n    input_ids1,attention_mask1,ans = valid_ds[:]\n    predicted = predict(input_ids1,attention_mask1,model)\n   # print(predicted)\n    ans=ans.cpu().numpy()\n    predicted=predicted.reshape(predicted.shape[0],)\n    return (np.sum(ans==predicted)\/len(ans))","7fed3a1e":"print(accuracy(valid_ds[:],model))","1be3a4aa":"pdd,att = preprocess(test,tokenizer)\npdd=torch.tensor(pdd,device=\"cuda\")\natt=torch.tensor(att,device=\"cuda\")","b84b5304":"prediction=predict(pdd,att,model)\nprediction=prediction.reshape(prediction.shape[0],)","ee7adaf2":"model_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nmodel_submission['target'] = np.round(prediction).astype('int')\nmodel_submission.to_csv('model_submission.csv', index=False)\nmodel_submission.describe()","7ad16454":"**Load the data**","ae7b5552":"*import tokenizer*","e96e15c1":"**Distill-bert**","5acdedcc":"**Data Cleaning**","8e764f5a":"**Freeze bert layer**","cdd1c786":"**Model**"}}