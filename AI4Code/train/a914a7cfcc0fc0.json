{"cell_type":{"d378ec2a":"code","9fe24882":"code","f5132434":"code","ce7b4519":"code","1add3c00":"code","7a262693":"code","223b1c85":"code","fd2970e3":"code","54edeea6":"code","b34ac8a5":"code","acd20713":"code","19c7c16d":"code","f13136fc":"code","29c310f0":"code","420fc5d9":"code","0bd1cc55":"code","35fa4008":"code","471fcffa":"code","98bcb469":"code","01c72263":"code","471d07bc":"code","da37675f":"code","557a8d9d":"code","db29d091":"code","f3ec27e8":"code","46fd98c7":"code","b4fa75a6":"code","bf1509ac":"code","97bfa88d":"code","ec43eff0":"code","2f5d1425":"code","8ca2a4d1":"code","aeb84aec":"code","66890513":"code","f379d643":"code","b86f9166":"code","4261c823":"code","40054dfa":"code","8b128b69":"code","81786193":"code","e4d5b28f":"code","dd92a256":"code","b3debac1":"code","9bfc7040":"code","2829d3bf":"code","8880e15f":"code","b8744494":"code","6c719ed1":"code","72d17fc9":"code","b6e61a7a":"code","0de83790":"code","37388096":"code","5dc8108b":"code","397246a0":"code","c39d5753":"code","7b3b3c1e":"code","eb1ce21c":"code","1dad6c3e":"code","524572b6":"code","7ea3838a":"code","cb594b3f":"code","3da3041c":"code","409391a4":"code","c200942a":"code","6756ce37":"code","05eeb68b":"code","9c85e1df":"code","85b406e6":"code","4db77605":"code","b89ab9ec":"code","de844440":"code","f9b5be0e":"code","a5e0a012":"code","7b0402ba":"code","438e3203":"code","d0b3f9ec":"code","3062864d":"code","cb37d30d":"code","5b9c5090":"code","50b09f77":"code","882ec4d6":"code","796ad14c":"code","06e68ca8":"code","5085d1f0":"code","ecd00ab4":"code","6b4776ae":"code","1b4ebd37":"code","f8f5be73":"code","8786ff82":"code","7cc5546f":"code","c88e4cea":"code","17987201":"code","4188d617":"code","a14530c6":"code","c7eb7a65":"code","f2fb5782":"code","a7747042":"code","46d193f5":"code","c9ba0f35":"code","2905074e":"code","1bc5c4f7":"code","1686bd2a":"code","0ff84b99":"code","59064a9f":"code","d3c91fc0":"code","ef63b585":"code","10e1a130":"code","5bfa22f3":"code","6d0a11af":"code","86f95eb0":"code","a7360220":"code","79a94717":"code","3da121a8":"code","48a82df4":"code","29b6aab7":"code","06a50c9e":"code","58f0388e":"code","40e214e5":"code","fc938852":"code","30307a57":"code","e4c071b9":"code","878b023b":"code","3d5cbfb2":"code","0d7a78c1":"code","f6b464d3":"markdown","fc774909":"markdown","e99d2e9d":"markdown","209da7e6":"markdown","02a8f1b4":"markdown","5e17d20f":"markdown","4e9927c9":"markdown","29c255da":"markdown","fa12d50f":"markdown","2217c03d":"markdown","90760816":"markdown","0391fd2c":"markdown","241a8b4b":"markdown","7501f007":"markdown","c324ab56":"markdown","c0519c94":"markdown","6563da21":"markdown","97dcde75":"markdown","6ec58d45":"markdown","47836a55":"markdown","81a883f2":"markdown","f657d716":"markdown","f6211cc8":"markdown","ab692edb":"markdown","bf8ace3e":"markdown","fb36184a":"markdown","c08456de":"markdown","38dd6f43":"markdown","89d35327":"markdown","51073100":"markdown","ff4fb907":"markdown","de6b78de":"markdown","fbe6b5a3":"markdown","19190000":"markdown","219b3828":"markdown","7cb319a0":"markdown","95ed336a":"markdown","86f51b0f":"markdown","c6616f02":"markdown","275b83a3":"markdown","418d0ec3":"markdown","e3fcdf4c":"markdown","b1e50552":"markdown","33a90a5d":"markdown","dcbee6af":"markdown","ee1f623a":"markdown","df0c290a":"markdown","5c512519":"markdown","b7e52552":"markdown","ea80d435":"markdown","0b2c6612":"markdown","b0f86b88":"markdown","4b991498":"markdown","e4d13181":"markdown","f94da85d":"markdown","27b731cb":"markdown","55b6d3f7":"markdown","97e9553c":"markdown","8ad354a0":"markdown","c96f314c":"markdown","0b492ffa":"markdown","07be4ef7":"markdown","a0bcb659":"markdown","ad81a9be":"markdown","242cf5cb":"markdown"},"source":{"d378ec2a":"#import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n# from IPython.display import display\nimport geopandas\nimport json\nfrom datetime import date\nimport holidays\nimport matplotlib.ticker as ticker\nfrom IPython.core.display import HTML\nimport plotly.offline as offline\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom ipywidgets import widgets\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import preprocessing\nfrom sklearn.impute import KNNImputer\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom scipy.stats import uniform, randint\nfrom sklearn.model_selection import RandomizedSearchCV \nfrom sklearn.model_selection import KFold \n#using to plot partial dependence is pdpbox\nimport pdpbox.pdp as pdp\n\nimport warnings   # remove all warnings from the output\n\nwarnings.filterwarnings(\"ignore\")\n\n# # Set notebook mode to work in offline\n# offline.init_notebook_mode()\n\n%matplotlib inline\n\n\npd.set_option('display.max_colwidth',-1)\n#avoid scientific enumeration output\nnp.set_printoptions(suppress=True)","9fe24882":"# read house price data file\ndf_house=pd.read_csv('..\/input\/mlebourne-20162017\/melb_data.csv')\ndf_house.head()","f5132434":"df_house.describe(include = \"all\")","ce7b4519":"df_house.shape","1add3c00":"df_house.info()","7a262693":"df_house.isnull().sum(axis=0)","223b1c85":"df_house.drop(['Method','Distance','Propertycount'],axis=1,inplace=True)","fd2970e3":"df_house.shape","54edeea6":"#calculate null percentage\ndef null(df, feature, plot=False):\n    t = df[feature].isna().mean() * 100\n    print(f'% of null: {t}%')\n    if plot:\n        sns.countplot(df[feature], palette='Set3')","b34ac8a5":"null(df_house,'BuildingArea')\ndf_house.drop('BuildingArea',axis=1,inplace=True)","acd20713":"null(df_house,'CouncilArea')\ndf_house.drop('CouncilArea',axis=1,inplace=True)","19c7c16d":"df_house.shape","f13136fc":"#pick out the num of rooms is smaller than that of bedrooms.\nerror_rooms=df_house[df_house['Rooms']<df_house['Bedroom2']]\nerror_room_percentage = (error_rooms.shape[0]\/df_house.shape[0])*100\nerror_room_percentage","29c310f0":"# As the percentage about data where the num of rooms is smaller than that of bedrooms is only 1.5%, to prevent this error affect the following analysis, I will choose to delete it.\nhouse=df_house.copy()\nhouse.drop(error_rooms.index,inplace=True)","420fc5d9":"house.shape","0bd1cc55":"#As the price number is too long, I will shorten it\uff0cbased on price column describe\nhouse['Price'] = house['Price']\/1000000","35fa4008":"house.rename(columns={'Price':'Price(Million)'},inplace=True)","471fcffa":"ax= sns.boxplot(palette=[\"m\"],\n            data=house['Price(Million)'])\nax.set(xlabel='Housing Price',ylabel='Price-Milliom',xticklabels=[])","98bcb469":"def funnel_plot(df,col,top):\n    # pick out the top ten features of one column with the biggest amount of real estates, and other features will be integrated into one feature named 'others'\n    data =df[col].value_counts().head(top)\n    others=df[~df[col].isin(data.index)][col].shape[0]\n    data_toframe = data.to_frame(name='Count') #convert picked-out series transferred into dataframe \n    \n    # build a new series, convert index into one new column, adding to last dataframe, easy for drawing funnel plotly\n    new_row=pd.Series(others,index=['Count'],name='Others') #index is new column name set,name is related index\n    new_frame = data_toframe.append(new_row,ignore_index=False).sort_values(by='Count',ascending=False).reset_index()\n    new_frame.rename(columns={'index':col},inplace=True)\n    \n    # graph_objects trace go.Funnel \n    fig = go.Figure(go.Funnel(\n             y=new_frame[col].values,\n             x=new_frame['Count'].values,\n             textposition = \"inside\",\n             textinfo = \"value+percent total\"\n            )           \n        )\n    fig.update_layout(showlegend=False,height=900, width=900)\n\n    fig.show()","01c72263":"funnel_plot(house,'Suburb',10)","471d07bc":"sns.set_theme(style=\"darkgrid\")\nsns.countplot(house['Type'], palette='Set2')","da37675f":"# We firstly use pie chart to show the distribution, mainly focus on top 10\n# data_sell=df_house['SellerG'].value_counts().head(10)\n# data_sell.to_frame(name='SellerG')\n# data_sell.plot.pie(autopct=\"%.1f%%\",figsize=(10, 10))\nfunnel_plot(house,'SellerG',10)","557a8d9d":"# To tell the format of date column is consistent\nm1 = house['Date'].eq('') | house['Date'].isna()\nm2 = pd.to_datetime(house['Date'],format='%d\/%m\/%Y',errors='coerce').isna()\n\nm1.eq(m2).all()","db29d091":"house['Postcode'].dtypes","f3ec27e8":"house['Postcode'] = house['Postcode'].astype('int')","46fd98c7":"null(house,'Car',True)","b4fa75a6":"#find car and price relationship to decide how to deal with null value\nsns.set_theme(style=\"white\")\n\n# Plot miles per gallon against horsepower with other semantics\nsns.relplot(x=\"Price(Million)\", y=\"Car\", size=\"Car\",\n            sizes=(40, 400), alpha=.5, palette=\"muted\",\n            height=6, data=house)","bf1509ac":"house['Car'].fillna(house['Car'].value_counts().index[0], inplace=True)\nhouse['Car'].isnull().sum()","97bfa88d":"null(house,'YearBuilt')","ec43eff0":"house.info()","2f5d1425":"process_df= house[['YearBuilt','Price(Million)','Rooms','Bathroom','Car','Suburb','Type']]","8ca2a4d1":"#transfer non-numeric data columns into numeric type with labelencoder\nprocess_df[['Suburb','Type']]=process_df[['Suburb','Type']].apply(LabelEncoder().fit_transform)","aeb84aec":"# KNN used to fill null value\nimputer = KNNImputer(n_neighbors=6,weights='uniform', metric='nan_euclidean')\n\nimputer.fit(process_df.values)\n\n# transform the dataset\nXtrans = imputer.transform(process_df.values)\n\nhouse['YearBuilt'] = Xtrans[:,:1].flatten().round(0).astype(int)","66890513":"house.shape","f379d643":"fig_dis = make_subplots(rows=3, cols=2,subplot_titles=('Rooms','Price(Million)','Bathroom','Car','Landsize'))\n\nfig_dis.add_trace(go.Violin(y=house['Rooms'],\n                            box_visible=True,\n                            meanline_visible=True,\n                            line_color='black',\n#                             fillcolor='lightseagreen',\n                            opacity=0.5,\n                            x0='Rooms number'),\n              row=1, col=1\n             )\nfig_dis.add_trace(go.Violin(y=house['Price(Million)'],\n                            box_visible=True,\n                            meanline_visible=True,\n                            line_color='red',\n#                             fillcolor='lightseagreen',\n                            opacity=0.5,\n                            x0='Price'),\n              row=1, col=2\n             )\nfig_dis.add_trace(go.Violin(y=house['Bathroom'],\n                            box_visible=True,\n                            meanline_visible=True,\n                            line_color='blue',\n#                             fillcolor='lightseagreen',\n                            opacity=0.5,\n                            x0='Bathroom number'),\n              row=2, col=1\n             )\nfig_dis.add_trace(go.Violin(y=house['Car'],\n                            box_visible=True,\n                            meanline_visible=True,\n                            line_color='purple',\n#                             fillcolor='lightseagreen',\n                            opacity=0.5,\n                            x0='Parking spot number'),\n              row=2, col=2\n             )\nfig_dis.add_trace(go.Violin(y=house['Landsize'],\n                            box_visible=True,\n                            meanline_visible=True,\n                            line_color='green',\n#                             fillcolor='lightseagreen',\n                            opacity=0.5,\n                            x0='Land size'),\n              row=3, col=1\n             )\n\n\nfig_dis.update_layout(showlegend=False,height=1000, width=1000)\nfig_dis.show()","b86f9166":"# restore a dataframe including Suburb','Price(Million)','Date','SellerG','Type' so that draw graphs to analyse relationship bewteen time and selling price and selling number\nhouse_rate = house[['Suburb','Price(Million)','Date','SellerG','Type']]\nhouse_rate['Date'] = pd.to_datetime(house_rate['Date'])\nhouse_rate['Num'] = 1","4261c823":"time_price = house_rate[['Date','Price(Million)','Num']].set_index('Date')","40054dfa":"# accroding to the same days, calculate everyday's average selling price of real estates and total number of the sold\ndf_dp = time_price.resample('D').mean().reset_index()\ndf_numd = time_price.resample('D').sum().reset_index()","8b128b69":"fig_area = px.area(df_dp, x='Date', y='Price(Million)', title='Area Visualize - Time Series with Selling House Price - Avg')\n\nfig_area.update_xaxes(\n    rangeslider_visible=True,\n    rangeselector=dict(\n        buttons=list([\n            dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n            dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n#             dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n            dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n            dict(step=\"all\")\n        ])\n    )\n    )\n                   \n\nfig_scatter = px.scatter(df_dp, x='Date', y='Price(Million)', title='Scatter Visualize - Time Series with Selling House Price - Avg',color_discrete_sequence=['#ca8687'])\n\nfig_scatter.update_xaxes(\n    rangeslider_visible=True,\n    rangeselector=dict(bgcolor='#ca8687',bordercolor='#ca8687',\n        buttons=list([\n            dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n            dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n#             dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n            dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n            dict(step=\"all\")\n        ])\n    ),\n#      title_font_color='#ca8687',\n    color='#ca8687'    \n    )\n                   \n                   \nfig_area.show()\nfig_scatter.show()\n\n\n#number of houses selling\nfig_num = px.line(df_numd, x='Date', y='Num', title='Total Selling House Number- Time Series  ',color_discrete_sequence=['#45b97c'])\n\nfig_num.update_xaxes(\n    rangeslider_visible=True,\n    rangeselector=dict(bgcolor='#45b97c',bordercolor='#45b97c',\n        buttons=list([\n            dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n            dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n#             dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n            dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n            dict(step=\"all\")\n        ])\n    ),\n#      title_font_color='#ca8687',\n    color='#45b97c'    \n    )\n\n\nfig_num.show()","81786193":"# choose the period of time for statistics\nperiod=['10D','3W','m','Q','y']","e4d5b28f":"def draw_avgprice_time(data,period):\n    fig, axis = plt.subplots(5, 3, figsize=(30,40))\n    color = ['#5c7a29','#5f3c23','#faa755','#bd6758','#d71345']\n   \n    for i,p in enumerate (period):\n        df = data.resample(p).mean().reset_index()\n        df_sum = data.resample(p).sum().reset_index()\n        df_num = time_price.resample(p).sum().reset_index()\n\n        fig_new = 'fig'+str(i)\n        fig_new_sum = 'fig'+str(i)\n        \n#         axis='ax'+str(i)\n        fig_new_sum = sns.lineplot(data=df_sum, x='Date', y='Price(Million)',ax=axis[i,0],color=color[i])\n        fig_num = sns.lineplot(data=df_num, x='Date', y='Num',ax=axis[i,1],color=color[i])\n        fig_new = sns.lineplot(data=df, x='Date', y='Price(Million)',ax=axis[i,2],color=color[i])\n\n\n        fig_new.set_title('Average Selling Price ('+ p+')',fontsize=15)\n        fig_new_sum.set_title('Sum Selling Price ('+ p+')',fontsize=15)\n        fig_num.set_title('Total Num of selling ('+ p+')',fontsize=15)\n","dd92a256":"sns.set_theme(style='darkgrid')\ndraw_avgprice_time(time_price,period)","b3debac1":"df_year = time_price.resample('Y').sum().reset_index()","9bfc7040":"trace_bar=go.Bar(x=df_year['Date'].dt.year.tolist(),y=df_year['Price(Million)'].tolist(),name='Total selling price'\n                )\n\ntrace_line=go.Line(x=df_year['Date'].dt.year.tolist(),y=df_year['Num'].tolist(),name='Total selling number')\n\ndata=[trace_bar,trace_line]\n\nlayout= go.Layout(title='Trend of price and selling number in one year',xaxis=dict(title='Year',tickmode='array',tickvals=[2016,2017])\n                 )\nfig = go.Figure(data,layout)\n\nfig.show()","2829d3bf":"trendency_price = ((df_year['Price(Million)'][1]-df_year['Price(Million)'][0])\/df_year['Price(Million)'][0]) * 100\ntrendency_num = ((df_year['Num'][1]-df_year['Num'][0])\/df_year['Num'][0])*100\n\nprint('From 2016 to 2017, the total sum price of selling real estates increases %d' %trendency_price + '%')\nprint('From 2016 to 2017, the total number of real estates sold increases %d'%trendency_num +'%')\n","8880e15f":"# get the maximum of the boxplot\ndef Upper_Fence(df,col):\n    Q1 = df[col].quantile(0.25)\n    Q3= df[col].quantile(0.75)\n\n    IQR = Q3 - Q1\n\n    Upper_Fence = Q3 + (1.5 * IQR)\n    return Upper_Fence","b8744494":"upper_price = Upper_Fence(time_price,'Price(Million)')","6c719ed1":"df_Upper_Fence = time_price[time_price['Price(Million)']>=upper_price].reset_index().sort_values(by='Date')","72d17fc9":"# Select country\nAU_holidays = holidays.AU(prov='VIC')\n\n\nfig,ax=plt.subplots(figsize=(20,10))\n\nx= df_Upper_Fence['Date']\ny=df_Upper_Fence['Price(Million)']\n\nax.scatter(x,y,c='r',lw=2,marker='o')\n\n\nax.xaxis.set_major_locator(ticker.MultipleLocator(base=20)) #set aixs intervals,let xticks value not too filled\nax.grid(linestyle = '-',linewidth =1, color= 'gray',alpha = 0.4) \n# ax.patch.set_facecolor(\"\")  \nax.set_title('Time with high price selling',fontsize=20)\nax.set_ylabel('Price(Million)',fontsize=15)\nax.set_xlabel('Date',fontsize=15)\nax.set_xticklabels(x,rotation=90)\nax.tick_params(labelsize=12)\nstyle = dict(size=15, color='black')\n\n# tell whether there is some day is holiday that cause selling price is high\nn=0\nfor i in x.dt.date:\n    if i not in AU_holidays:\n        continue\n    elif i in AU_holidays:\n        ax.annotate(AU_holidays.get(i)+' ('+i+')',xy=(i,list(y)[x.dt.date.index(i)]),xycoords='data',xytext=(50,200),rotation=15,size=13,\n                    textcoords='offset points', arrowprops=dict(arrowstyle=\"->\",connectionstyle=\"arc3,rad=-0.2\",color='black'),\n                    bbox=dict(pad=0, facecolor=\"none\", edgecolor=\"none\"))\n        n+=1\n\nif n == 0:\n    print('These dates are not holidays which may affect selling prices.' )","b6e61a7a":"house_type=house[['Type','Rooms','SellerG','Date','Bedroom2','Bathroom','Car','Landsize','YearBuilt','Price(Million)']]\n\nhouse_type['Date'] = pd.to_datetime(house_type['Date'])\nhouse_type['Age(Years)']=(house_type['Date'].dt.year)-house_type['YearBuilt']","0de83790":"## relationships between the age of real estates and selling prices\nfig_age = px.scatter(house_type, x='Age(Years)', y='Price(Million)', color='Type',\n                     size='Price(Million)', hover_data=['Price(Million)','Age(Years)','Rooms','Bedroom2','Bathroom','Car','Landsize','YearBuilt'],\n#                      trendline=\"rolling\", trendline_options=dict(function=\"median\", window=5),\n                     range_x=[-10,850], range_y=[0,10],\n                   )\n\n# fig_age['layout'].pop('updatemenus')\nfig_age.update_layout(xaxis=dict(\n                        rangeslider=dict(visible=True)\n                    ),\n                     dragmode=\"zoom\"\n                     )\nfig_age.show()","37388096":"house_type[house_type['Age(Years)']<0]","5dc8108b":"ax = sns.countplot(house_type['Type'], palette='Set2')\n\nsums = house_type.groupby(['Type'])['Type'].sum().values\nnobs = house_type['Type'].value_counts().values\nnob_num = [str(x) for x in nobs.tolist()]\nnob = [\"n: \" + i for i in nob_num]\n\npos = range(len(nob))\nfor tick,label in zip(pos,ax.get_xticklabels()):\n    ax.text(pos[tick], nobs[tick] + 0.06, nob[tick], horizontalalignment='center', size='small', color='black', weight='semibold')","397246a0":"fig_type, ax_type = plt.subplots(figsize=(10,10))\nax_type=sns.violinplot(x=house_type['Type'],y=house_type['Price(Million)'],hue=house_type['Type'],palette=\"muted\")\n","c39d5753":"df_type_price = house_type[['Type','Date','Price(Million)']]","7b3b3c1e":"df_type_price['Date']=df_type_price['Date'].dt.year","eb1ce21c":"df_type_sum = df_type_price.groupby(['Type','Date']).agg({'Price(Million)':'sum'})","1dad6c3e":"df_type_sum = df_type_sum.reset_index()","524572b6":"df_type_avg = df_type_price.groupby(['Type','Date']).agg({'Price(Million)':'median'})\ndf_type_avg = df_type_avg.reset_index()","7ea3838a":"# trace_sum=go.Bar(x=df_type_sum['Date'],y=df_type_sum['Price(Million)'].tolist(),name='Total selling price of different types', marker=df_type_sum['Type'], barmode=\"group\"\n#                 )\n\n# trace_median=go.Line(x=df_type_avg['Date'],y=df_type_avg['Price(Million)'].tolist(),name='Median selling price of different types')\n\n\n# data=[trace_sum,trace_median]\n\n# layout= go.Layout(title='Trend of price and selling number in one year',xaxis=dict(title='Year',tickmode='array',tickvals=[2016,2017])\n#                  )\n# fig = go.Figure(data,layout)\n\n# fig.show()\n\nfig_sum = px.area(df_type_sum, x='Date', y='Price(Million)',\n             color='Type',range_x=[2016,2017],\n             height=400)\nfig_sum.show()","cb594b3f":"fig_median = px.area(df_type_avg, x='Date', y='Price(Million)',\n             color='Type',range_x=[2016,2017],\n             height=400)\nfig_median.show()","3da3041c":"fig_hr = px.scatter(house_type[house_type['Type']=='h'], x='Age(Years)', y='Rooms', color='Car',\n                     size='Price(Million)', hover_data=['Price(Million)','Age(Years)','Rooms','Bedroom2','Bathroom','Car','Landsize','YearBuilt'],\n                     range_x=[-10,250]\n                   )\n\nfig_hr.update_layout(xaxis=dict(\n                        rangeslider=dict(visible=True)\n                    ),\n                     dragmode=\"zoom\"\n                     )\nfig_hr.show()\n","409391a4":"upper_land = Upper_Fence(house_type,'Landsize')\nupper_age = Upper_Fence(house_type,'Age(Years)')","c200942a":"fig_scat_matr = px.scatter_matrix(house_type[(house_type['Landsize']<upper_land) & (house_type['Age(Years)']<upper_age)][['Age(Years)','Rooms','Bathroom','Price(Million)','Landsize','Type','Car']],\n                                  dimensions=['Age(Years)','Rooms','Bathroom','Price(Million)','Landsize','Car'],\n#                                   symbol='Type',\n                                  color=\"Type\",\n                                  )\n\nfig_scat_matr.update_layout(\n                  width=900,\n                  height=900,\n                  )\nfig_scat_matr.update_traces(diagonal_visible=False)\n\nfig_scat_matr.show()","6756ce37":"## SellerG, build top agencies who sell most \ntop_agency = house_type['SellerG'].value_counts().head(5)\ntop_agency_df = house_type [house_type['SellerG'].isin(top_agency.index)]","05eeb68b":"fig_seller = px.violin(top_agency_df, y=\"Price(Million)\", x=\"SellerG\", color=\"Type\", box=False, \n                      hover_data=top_agency_df.columns, category_orders={\"SellerG\": (top_agency_df['SellerG'].value_counts().index)}\n                      )\nfig_seller.update_traces(meanline_visible=True)\nfig_seller.show()","9c85e1df":"house_map = house.drop(['Address','SellerG','Postcode','Regionname'],axis=1)\nhouse_map['Date'] = pd.to_datetime(house_map['Date'])\nhouse_map['Age(Years)']=(house_map['Date'].dt.year)-house_map['YearBuilt']","85b406e6":"vic_region = geopandas.read_file('..\/input\/mlebourne-20162017\/suburb-10-vic.geojson')","4db77605":"house_map_summary = house_map[['Suburb','Price(Million)','Type','Rooms','Landsize','Age(Years)']].groupby(['Suburb','Type']).agg({'Price(Million)':'mean','Rooms':'mean','Landsize':'mean','Age(Years)':'mean'})","b89ab9ec":"# set suburb uppercase to match geojson data format\nhouse_map_summary.index = house_map_summary.index.set_levels(house_map_summary.index.levels[0].str.upper(), level=0)\n","de844440":"#choose only house data\nh_df = house_map_summary.loc[(slice(None),'h'),:].reset_index()\nh_df.drop('Type',axis=1,inplace=True)","f9b5be0e":"#choose only unit data\nu_df = house_map_summary.loc[(slice(None),'u'),:].reset_index()\nu_df.drop('Type',axis=1,inplace=True)","a5e0a012":"#choose only townhouse data\nt_df = house_map_summary.loc[(slice(None),'t'),:].reset_index()\nt_df.drop('Type',axis=1,inplace=True)","7b0402ba":"def draw_plotmap_num(df,t):\n\n    map= px.choropleth_mapbox(\n                    data_frame=df,\n                    geojson=vic_region,\n                    color='Price(Million)',\n                    locations='Suburb', \n                    featureidkey='properties.vic_loca_2',\n                    mapbox_style=\"carto-positron\",\n                    color_continuous_scale='viridis',\n                    center={\"lat\": -37.81110, \"lon\": 144.97150},\n                    hover_data={'Suburb','Rooms','Landsize','Age(Years)'},\n                    opacity=0.7,\n                    zoom=8,\n                    title=t + ' -Average selling price in Melbourne',#map title\n    )\n   \n    map.update_layout(margin=dict(\n        l=0,\n        r=0,\n        b=50,\n        t=30,\n    ),height=500,width=600)\n    \n    map.show()","438e3203":"draw_plotmap_num(h_df,'House')\ndraw_plotmap_num(t_df,'Townhouse')\ndraw_plotmap_num(u_df,'Unit')","d0b3f9ec":"df_house_sub = house_map.copy()","3062864d":"df_house_sub[(df_house_sub['Type']=='h')& (df_house_sub['Suburb']=='Bentleigh East')].shape","cb37d30d":"df_house_sub[df_house_sub['Type']=='h']['Suburb'].value_counts().head(10)","5b9c5090":"df_house_sub[df_house_sub['Type']=='u']['Suburb'].value_counts().head(10)","50b09f77":"df_house_sub[df_house_sub['Type']=='t']['Suburb'].value_counts().head(10)","882ec4d6":"sns.pairplot(df_house_sub[df_house_sub['Suburb']=='Bentleigh East'][['Rooms','Price(Million)','Landsize','Type']],hue='Type')","796ad14c":"sns.pairplot(df_house_sub[df_house_sub['Suburb']=='Reservoir'][['Rooms','Price(Million)','Landsize','Type']],hue='Type')","06e68ca8":"house_ml_df = house.drop(['Address','Lattitude','Longtitude','Regionname'],axis=1)\nhouse_ml_df['Date']= pd.to_datetime(house_ml_df['Date'])\nhouse_ml_df['Age(Years)']=(house_ml_df['Date'].dt.year)-house_ml_df['YearBuilt']\n\nhouse_ml_df['Sell_Month'] = house_ml_df['Date'].dt.month\nhouse_ml_df['Sell_Year'] = house_ml_df['Date'].dt.year\nhouse_ml_df.drop(['Date'],axis=1,inplace = True)","5085d1f0":"# Get each feature's outlier data\nupper_price = Upper_Fence(house_ml_df,'Price(Million)')\nupper_Rooms = Upper_Fence(house_ml_df,'Rooms')\nupper_Bedrooms = Upper_Fence(house_ml_df,'Bedroom2')\nupper_Bathroom = Upper_Fence(house_ml_df,'Bathroom')\nupper_Car = Upper_Fence(house_ml_df,'Car')\nupper_Landsize = Upper_Fence(house_ml_df,'Landsize')\nupper_Age = Upper_Fence(house_ml_df,'Age(Years)')","ecd00ab4":"# remove outliers \nhouse_ml_df = house_ml_df[(house_ml_df['Price(Million)']<=upper_price) & (house_ml_df['Rooms'] <= upper_Rooms) &\n                         (house_ml_df['Bedroom2']<=upper_Bedrooms) & (house_ml_df['Bathroom']<=upper_Bathroom) &\n                         (house_ml_df['Car']<=upper_Car) & (house_ml_df['Landsize']<=upper_Landsize) &\n                         (house_ml_df['Age(Years)']<= upper_Age)]","6b4776ae":"house_ml_df.shape","1b4ebd37":"house_ml_df.info()","f8f5be73":"# 'type' only has 3 categories, and they donot have ordinal features, so I will use onehot encoder\nonehot = pd.get_dummies(house_ml_df['Type'])\nhouse_ml_df = house_ml_df.drop('Type',axis=1)\nhouse_ml_df = house_ml_df.join(onehot)\n\nml_df = house_ml_df.copy()","8786ff82":"X_set = house_ml_df.drop(['Price(Million)'],axis=1)\ny_set = house_ml_df['Price(Million)']\nx_train, x_test,y_train, y_test = train_test_split(X_set,y_set,test_size=0.3,random_state=0)","7cc5546f":"# function of returning evaluating dataset\ndef evaluate (true,predicted):\n    mae=metrics.mean_absolute_error(true,predicted)\n    mse=metrics.mean_squared_error(true,predicted)\n    rmse=np.sqrt(metrics.mean_squared_error(true,predicted))\n    r2_square=metrics.r2_score(true,predicted)\n    return mae, mse, rmse, r2_square  \n\n\ndef print_evaluate (true,predicted):\n    mae=metrics.mean_absolute_error(true,predicted)\n    mse=metrics.mean_squared_error(true,predicted)\n    rmse=np.sqrt(metrics.mean_squared_error(true,predicted))\n    r2_square=metrics.r2_score(true,predicted)\n    print('MAE:',mae)\n    print('MSE:',mse)\n    print('RMSE:',rmse)\n    print('R Square:',r2_square)","c88e4cea":"# Next will use cross-validation to mean encode the last features\nkf = KFold(n_splits=5,shuffle=True, random_state=10) \n\n\ndf_train =  pd.concat([x_train,y_train],axis=1).sort_index() # build a new whole train set\ndf_train['kfold'] = -1 \nmean_of_target = df_train['Price(Million)'].mean()\n\n\n# The data of training set is divided into K fold.\nfor foldid,(train_index, test_index) in enumerate(kf.split(X=df_train.drop(['Price(Million)'],axis=1))):\n#     print(\"Train:\", train_index, \"Validation:\",test_index)\n    df_train.iloc[test_index,df_train.columns.get_loc('kfold')] = foldid\n\nencoded_dfs = []\n\n# For each folded data divided: use the remaining K-1 folded data to calculate the target coding of the folded data\nfor fold in df_train[\"kfold\"].unique():\n    df_train_cv = df_train[df_train['kfold'] != fold].copy()\n    df_val_cv = df_train[df_train['kfold'] == fold].copy()\n    \n    means_sub = df_train_cv.groupby('Suburb')['Price(Million)'].mean().to_dict()\n    means_seller = df_train_cv.groupby('SellerG')['Price(Million)'].mean().to_dict()\n    df_val_cv['Suburb'] = df_val_cv['Suburb'].map(means_sub)\n    df_val_cv['SellerG'] = df_val_cv['SellerG'].map(means_seller)\n    \n    encoded_dfs.append(df_val_cv)\n\n# The above folded data and the calculated target code are combined to form a new training set data.\nencoded_dfs = pd.concat(encoded_dfs, axis=0).sort_index()\nencoded_dfs[['Suburb','SellerG']] = encoded_dfs[['Suburb','SellerG']].fillna(mean_of_target)\nencoded_dfs.drop('kfold', axis=1, inplace=True)","17987201":"# As there is no single mapping deduced from train set I think we should use the whole train set to fit the encodings and then use it on test set\nmeans_test_suburb = df_train.groupby('Suburb')['Price(Million)'].mean()\nmeans_test_seller = df_train.groupby('SellerG')['Price(Million)'].mean()\n\nx_test['Suburb'] = x_test['Suburb'].map(means_test_suburb)\nx_test['SellerG'] = x_test['SellerG'].map(means_test_seller)\nx_test[['Suburb','SellerG']] = x_test[['Suburb','SellerG']].fillna(mean_of_target)\n","4188d617":"x_train_cv = encoded_dfs.drop(['Price(Million)'],axis=1)\ny_train_cv = encoded_dfs['Price(Million)']","a14530c6":"rf = RandomForestRegressor(n_estimators = 200)\n\n#function to train a ranom forest regressor model and evaluation\ndef rf_evaluation(x_train,x_test,y_train,y_test):\n\n    rf.fit(x_train,y_train)\n\n    pred_train_rf = rf.predict(x_train)\n    pred_test_rf = rf.predict(x_test)\n\n    rf_test = pd.DataFrame(columns=['MAE','MSE','RMSE','R Square'])\n    rf_train = pd.DataFrame(columns=['MAE','MSE','RMSE','R Square'])\n\n    rf_train.loc[0] = [*evaluate(y_train,pred_train_rf)] \n    rf_test.loc[0] = [*evaluate(y_test,pred_test_rf)]\n\n    print('Random Forest Regressor Model')\n    print('train set evaluation:')\n    print (rf_train)\n    \n    print('===============')\n    print('test set evaluation:')\n    print (rf_test)\n    return rf_train,rf_test\n\nrf_train_cross,rf_test_cross= rf_evaluation(x_train_cv, x_test,y_train_cv, y_test)","c7eb7a65":"# function to evaluate xgboost model\ndef xgb_evaluation(x_train,x_test,y_train,y_test):\n    params = {'colsample_bytree': uniform(0.7, 0.3),\n          'gamma': uniform(0, 0.5),\n          'learning_rate': uniform(0.003, 0.3), # default 0.1 \n          'max_depth': randint(2, 6), # default 3\n          'n_estimators': randint(100, 250), # default 100\uff0c It is related to the complexity of our XGBoost model, because it represents the number of weak learners in our decision tree.\n          'subsample': uniform(0.6, 0.4)}\n\n    xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\",random_state=42)\n    kf = KFold(shuffle=True, n_splits=5)\n\n\n    #use hyperparamter tuning to randomly find the best params for xgboost \n    xgb_search = RandomizedSearchCV(xgb_model,param_distributions=params, random_state=42, n_iter=4, cv=kf, verbose=1, n_jobs=1, return_train_score=True)\n    xgb_search.fit(x_train,y_train)\n    \n    pred_train = xgb_search.predict(x_train)\n    pred_test = xgb_search.predict(x_test)\n\n    df_test = pd.DataFrame(columns=['MAE','MSE','RMSE','R Square'])\n    df_train = pd.DataFrame(columns=['MAE','MSE','RMSE','R Square'])\n\n    df_train.loc[0] = [*evaluate(y_train,pred_train)] \n    df_test.loc[0] = [*evaluate(y_test,pred_test)]\n    \n    print('XGBoost Model')\n    print('train set evaluation:')\n    print (df_train)\n    print('===============')\n    print('test set evaluation:')\n    print (df_test)\n    return xgb_search","f2fb5782":"xgb_search_cv = xgb_evaluation(x_train_cv, x_test,y_train_cv, y_test)","a7747042":"#function to train gradient boosting regressor and evaluate \ndef gbr_evaluation(x_train,x_test,y_train,y_test):\n    # set gradient boost parameters\n    params = {'loss':'huber',\n            'learning_rate':0.03,\n            'n_estimators': 100,\n            'subsample': 0.9,\n            'min_samples_split':10,  \n            'max_depth': 4,\n    }\n\n    gb_model= GradientBoostingRegressor(**params)\n    gb_model.fit(x_train,y_train)\n\n    train_pred_gb = gb_model.predict(x_train)\n    test_pred_gb = gb_model.predict(x_test)\n\n    print('Gradient Boost Regressor Model')\n    print('train set evaluation:\\n________________')\n    print_evaluate(y_train,train_pred_gb)\n    print('===============')\n    print('test set evaluation:\\n_________________')\n    print_evaluate(y_test,test_pred_gb)","46d193f5":"gbr_evaluation(x_train_cv, x_test,y_train_cv, y_test)","c9ba0f35":"import category_encoders as ce\n\ndf_house_binary = house_ml_df.copy()\nb_encoder = ce.BinaryEncoder(cols=['Suburb','SellerG'])\ndf_binary = b_encoder.fit_transform(df_house_binary)","2905074e":"X_binary = df_binary.drop(['Price(Million)'],axis=1)\ny_binary = df_binary['Price(Million)']\nx_train_bi, x_test_bi,y_train_bi, y_test_bi = train_test_split(X_binary,y_binary,test_size=0.3,random_state=0)","1bc5c4f7":"#evaluate xgboost with binary encoding\nxgb_evaluation(x_train_bi, x_test_bi,y_train_bi, y_test_bi)","1686bd2a":"#evaluate random forest regressor with binary encoding\nrf_evaluation(x_train_bi, x_test_bi,y_train_bi, y_test_bi)","0ff84b99":"#evaluate gradient boosting regressor with binary encoding\ngbr_evaluation(x_train_bi, x_test_bi,y_train_bi, y_test_bi)","59064a9f":"importances = xgb_search_cv.best_estimator_.feature_importances_","d3c91fc0":"from xgboost import plot_importance\nplot_importance(xgb_search_cv.best_estimator_,importance_type = 'total_gain')","ef63b585":"# Create the partial dependency plot\npdp_age = pdp.pdp_isolate(xgb_search_cv, x_train_cv, X_set.columns, feature='Age(Years)')\npdp.pdp_plot(pdp_age, 'age')\nplt.ylabel(\"Change in prediction\")\nplt.show()","10e1a130":"interaction = ['Age(Years)', 'Suburb']\n\npdp_as = pdp.pdp_interact(xgb_search_cv, x_train_cv, X_set.columns, interaction)\npdp.pdp_interact_plot(pdp_as, interaction, plot_type='contour')\nplt.show()","5bfa22f3":"# from sklearn.inspection import partial_dependence\n# from sklearn.inspection import plot_partial_dependence\n# from mpl_toolkits.mplot3d import Axes3D","6d0a11af":"# fig = plt.figure(figsize=(12,12))\n# pdp,axes = partial_dependence(xgb_search, x_train_cv,interaction)\n\n# XX, YY = np.meshgrid(axes[0],axes[1])\n# Z = pdp[0].T\n# ax = Axes3D(fig)\n# surf = ax.plot_surface(XX , YY, Z, rstride=1,cstride=1,\n#                       cmap=plt.cm.BuPu, edgecolor='k')\n# ax.set_xlabel(interaction[0])\n# ax.set_ylabel(interaction[1])\n# ax.set_zlabel('Partial dependence')\n\n# ax.view_init(elev=22,azim=122)\n# plt.colorbar(surf)\n\n# plt.subplots_adjust(top=0.9)\n\n# plt.show()","86f95eb0":"#house_map['SellerG'] = house['SellerG']\n# #define the interactive modules'content\n# textbox_seller = widgets.Dropdown(\n#     description='Agency: ',\n#     value='Jellis',\n#     options=list(np.sort(house_map['SellerG'].unique()))\n# )\n\n# textbox_type = widgets.Dropdown(\n#     description='Type: ',\n#     value='h',\n#     options=house_map['Type'].unique().tolist()\n# )\n\n","a7360220":"# trace_seller = go.Figure(go.Scattermapbox(lon=house_map['Longtitude'],lat=house_map['Lattitude'],\n# #                            color='Type',\n# #                                    size='Price(Million)',\n#                             mode='markers',\n#                             marker=go.scattermapbox.Marker(\n#                                 color=house_map['Price(Million)'],\n#                                 sizemin=1,\n#                                 showscale=True\n# #                                 sizemode=house_map['Price(Million)']\n#                                 ),\n#                             text=house_map[['Suburb','Price(Million)','Type','Age(Years)']],\n#                             hoverinfo='text',\n#                                    ))\n# trace_seller.update_layout(mapbox_style='stamen-terrain',\n#                          mapbox=dict(center=dict(\n#                                     lat=-37.81110,\n#                                     lon=144.97150),\n#                                     zoom=8\n#                          ),\n#                      margin=dict(\n#                             l=0,\n#                             r=0,\n#                             b=0,\n#                             t=0),\n#                           height=1000,\n#                           width=1000)\n\n# g_seller = go.FigureWidget(trace_seller)\n\n# # g_seller.show()","79a94717":"# def validate():\n#     if (textbox_seller.value in house_map['SellerG'].unique()) and (textbox_type.value in house_map['Type'].unique()):\n#         return True\n#     else:\n#         return False\n    \n    \n# def response(change):\n#     if validate():\n#         filter_list = [i and j for i, j in\n#                         zip(house_map['Type']==textbox_type.value, house_map['SellerG'] == textbox_seller.value\n#                             )]\n#         temp_df = house_map[filter_list]\n\n \n           \n#         x_lon = temp_df['Longtitude']\n#         x_lat = temp_df['Lattitude']\n#         x = [temp_df['Longtitude'],temp_df['Lattitude']]\n\n#         #         x2 = temp_df['dep_delay']\n\n#         #read the chosen data and map into the interatcive grapgh` \n#         with g_seller.batch_update():\n#             g_seller.data[0].lon = x[0]\n#             g_seller.data[0].lat = x[1]  \n\n#             g_seller.layout.barmode = 'overlay'\n#             g_seller.layout.title = 'Melbourne Map'\n            \n# # connect function with textbox for interaction\n# textbox_type.observe(response, names=\"value\")\n# textbox_seller.observe(response, names=\"value\")\n","3da121a8":"# # container = widgets.HBox(children=[use_age,age])\n# container_type = widgets.HBox([textbox_type])\n# container_sell = widgets.HBox([textbox_seller])\n# container_all = widgets.VBox([\n#               container_type,\n#               container_sell,\n#               g_seller\n# ])\n\n","48a82df4":"%%HTML\n<div class='tableauPlaceholder' id='viz1631786138926' style='position: relative'><noscript><a href='#'><img alt='Dashboard 1 ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Bo&#47;Book1_16317858380400&#47;Dashboard1&#47;1_rss.png' style='border: none' \/><\/a><\/noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' \/> <param name='embed_code_version' value='3' \/> <param name='path' value='views&#47;Book1_16317858380400&#47;Dashboard1?:language=en-US&amp;:embed=true&amp;publish=yes' \/> <param name='toolbar' value='yes' \/><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Bo&#47;Book1_16317858380400&#47;Dashboard1&#47;1.png' \/> <param name='animate_transition' value='yes' \/><param name='display_static_image' value='yes' \/><param name='display_spinner' value='yes' \/><param name='display_overlay' value='yes' \/><param name='display_count' value='yes' \/><param name='language' value='en-US' \/><param name='filter' value='publish=yes' \/><\/object><\/div>                <script type='text\/javascript'>                    var divElement = document.getElementById('viz1631786138926');                    var vizElement = divElement.getElementsByTagName('object')[0];                    if ( divElement.offsetWidth > 800 ) { vizElement.style.width='1000px';vizElement.style.height='827px';} else if ( divElement.offsetWidth > 500 ) { vizElement.style.width='1000px';vizElement.style.height='827px';} else { vizElement.style.width='100%';vizElement.style.height='727px';}                     var scriptElement = document.createElement('script');                    scriptElement.src = 'https:\/\/public.tableau.com\/javascripts\/api\/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                <\/script>","29b6aab7":"house_suburb_avg = house.copy()","06a50c9e":"house_suburb_avg['Date']= pd.to_datetime(house_suburb_avg['Date'])\nhouse_suburb_avg['Age(Years)']=(house_suburb_avg['Date'].dt.year)-house_suburb_avg['YearBuilt']","58f0388e":"# use dataset without outliers \nhouse_suburb_avg = house_suburb_avg[(house_suburb_avg['Price(Million)']<=upper_price) & (house_suburb_avg['Rooms'] <= upper_Rooms) &\n                         (house_suburb_avg['Bedroom2']<=upper_Bedrooms) & (house_suburb_avg['Bathroom']<=upper_Bathroom) &\n                         (house_suburb_avg['Car']<=upper_Car) & (house_suburb_avg['Landsize']<=upper_Landsize) & (house_suburb_avg['Age(Years)']<= upper_Age)]","40e214e5":"suburb_avg = house_suburb_avg.groupby(['Suburb','Type']).agg({'Price(Million)':'mean'})","fc938852":"suburb_avg = suburb_avg.reset_index()","30307a57":"seller_sub = house_suburb_avg.groupby(['SellerG','Suburb','Type']).agg({'Type':'count'}).rename(columns={'Type':'Number'})","e4c071b9":"# function used to find the top 5 agnecies' most popular selling suburbs\ndef sort_seller_num(data,agency):\n    # find all targeted agency dtata\n    df = data.loc[(agency,slice(None)),:]\n\n    #reset index and sorted descending according to each type house's total selling number\n    df = df.reset_index().sort_values(['Number','Type'],ascending = False)\n\n    #according to different types, rank them and pick out the first suburb with top selling number\n    df['sort'] = df.groupby('Type',axis=0)['Number'].rank(ascending=False)\n    df = df[df['sort'].isin([1,2,3])].sort_values(['Type','Number'],ascending = False).drop(['sort'],axis=1)\n    \n    df_merged = pd.merge(df,suburb_avg,how='inner', left_on=['Suburb', 'Type'],\n         right_on=['Suburb', 'Type']\n         )\n    return df_merged\n    ","878b023b":"df_Jellis = sort_seller_num(seller_sub,'Jellis')\ndf_hockingstuart = sort_seller_num(seller_sub,'hockingstuart')\ndf_nelson = sort_seller_num(seller_sub,'Nelson') \ndf_barry = sort_seller_num(seller_sub,'Barry') \ndf_ray = sort_seller_num(seller_sub,'Ray') ","3d5cbfb2":"# concat the top 5 agnecies' most popular selling suburbs\ndf_3d = pd.concat([df_Jellis,df_hockingstuart,df_nelson,df_barry,df_ray],axis=0)","0d7a78c1":"fig_seller = px.scatter_3d(df_3d, x='Number', y='Suburb', z='Price(Million)',\n              color='SellerG', symbol='Type')\n\nfig_seller.update_layout(showlegend=True,height=900, width=900)\n\nfig_seller.show()","f6b464d3":"#### I chose KNN method to fill null value of YearBuilt","fc774909":"Choose 'Bentleigh East' suburb area, then see each type's price distribution","e99d2e9d":"#### From price distribution, we can find the difference between prices of different houses are big, so we need to find which features will affect prices, why there will be outliers.","209da7e6":"From last plot, staring form 0 to 70 age(years), the y-axis data is negative, it means that the age in this range has negtive impact on targeted value(price). Higher age(during 0-70 years old) will lead to lower price. While after 70 years, y-axis starts positive and probably the more old, the more expensive real estates are. \n\n(Note: pdp has cos - it assumes no correlations among variables)","02a8f1b4":"#### Relationship between sellerG and price\nDuring last analysis on sellerG, we found prices sold by Jellis are almost higher than others. We will step further to find out the reason","5e17d20f":"**Between 2016 and 2017, in one year, whatever in number of house selling or in the price of selling, there is obvious increase trendency**","4e9927c9":"### Firstly,Look through all the dataset and check the features I will analyse.","29c255da":"#### 10. Car ","fa12d50f":"From the last two graphs, Look at one year from 2016 to 2017, different colorful areas stand for different types of real estates\u2019 total prices and average prices. **The first graph, we can see that all types\u2018 real estates\u2019 overall selling prices increases among one year. But in the second graph, units and tolls\u2019 average prices increased, only houses\u2019 average selling prices decreased by 5%.**","2217c03d":"#### Building area is releated to Landsize based on the type of home(house),\n#### For example, \n1. Example 1:\n   > LCR of a standard home.<br>\n   Building = 200 m2<br>\n   Land = 400 m2<br>\n   LCR = 2:1 or 200%.<br>\nThis is good; the land is double the building size.\n2. Example 2:\n    > LCR of a standard apartment.<br>\n    Apartment building = 10,000 m2 <br>\n    Land = 1,000 m2 <br>\n    LCR = 1:10 or 10%.<br>\nThis is bad; the land is only a small fraction of the building size.\n\n\n**_'Land appreciates, buildings depreciate'_**  [1]\n\n[1]: https:\/\/www.yourinvestmentpropertymag.com.au\/expert-advice\/cam-mclellan\/land-vs-building-200528.aspx\n\nHowever, the proportion between land size and building area is flexible and cannot be inferred. On this dataset analysis, I will choose to drop buildingarea column and study with landsize and type of home. ","90760816":"we pick out the top 5 agencies, Jellis,hockingstuart,Nelson, Bary,Ray\nFrom last map, most houses sold by Jellis are located in expensive area, next we use numbers to anlyse further","0391fd2c":"## Data preprocessing\n","241a8b4b":"#### 4. Rooms\n\nRooms should be related to bedroom2 and bathroom,while the num of rooms should be more than that of bedrooms.","7501f007":"At the first bubble graph the size of the bubble stands for how much the real estates were sold at, and the color means the car space. Looking deeper, for example at the 65 years of the real estate list. It seems that rooms are more , the real estates are more expensive. But this is not the common phenomenon, so there must be some other features affecting selling price not only the rooms.","c324ab56":"From the graph above, if in the particular area (Bentleigh East,etc.), more landsize of townhouse and units will have more possibilities to be sold more. ","c0519c94":"#### 7. Type","6563da21":"#### Relationship with Time series and Selling prices ","97dcde75":"From last graph, it verifies that there could be a correlation between some suburbs and real estates'age, which affect prices.","6ec58d45":"#### As the null percentage is quite low, and only from the scatterplot above, the relationship between price and car sapce is not directly correlated, so I just use the most frequent number to fill null","47836a55":"#### Have a try, change encoding from target encoding into binary encoding to see any changes","81a883f2":"From last bar chart, it is obviously that **houses are sold the most**, they are in the majority of the market of Melbourne selling real estates. ","f657d716":"I divided all data features into 4 categories:\n- house location info : suburb,address,postcode,councilarea, lattitude,longtitude,regionname\n- house quality:rooms,type,bedroom2,bathroom,car,landsize,building area,yearbuilt\n- sell and buy:price, method, sellerg,date\n- unknown: distance, propertycount","f6211cc8":"From the 1st and the 2nd graphs, when I clicked  one month or six months or one year to see the tendency of the prices of real estates. Through the time passes, from the beginning of the 2016 to the end of 2017, the prices of the real estate fluctuating. While the total selling number of real estates also fluctuates in the one year.\n\nIn order to see the tendency of the selling price according to time, I draw the following 15 graphs show.\n","ab692edb":"From analysis about the selling prices of different types of real estates, we know houses are usually more expensive than other two types.\n\nOn the first map, east melbourne,toorak and cantebury have the highest average selling price of houses. On the second map, Toorak, south yarra,sandringham have the top 3 highest average selling price of townhouses. On the third map, Glen waverley, mckinnon and brighton have the top 3 highest average selling price of units.\n\nIn additon, through last 3 map, they look like more near city, more expensive. The southeast area has the most expensive real estates.\n\nTo analyse relationships between rooms, landsize and suburbs, I will choose the same suburb area.","bf8ace3e":"####  1. Method & Distance & Propertycount\nMethod means the way to sell house, this analysis will focus on the features affecting prices and tendency of prices. Technically, method won't the most significant cause to price tendency. Distance is one column that is not related to the price.Thus I will choose to drop these two coloumns.","fb36184a":"The last combination of bar chart and line chart shows the tendency of total Melbourne real estates selling price and the total selling numbers, in 2016 the total selling prices is about 6000 million Australian dollars, and **in 2017**, the total selling prices is more than 7,000 million dollars, so **total increased 18% in one year.** And **in 2016**, the total selling number is around 6000, increased to more than 7000 in 2017. Up to **17 percent increase** in Melbourne total number of buying real estates.","c08456de":"> - h = House\n> - u = Unit\n> - t = Townhouse","38dd6f43":"Firstly, I will skip outlier data, which exist in price,landsize and other columns.","89d35327":"## - Identifying other columns with full value whether having logical,data type .etc. errors    and handling -","51073100":"#### Relationship between the agencies and real estate selling prices.\nFrom the last SellerG funnel plot, I pick out the top 5 agencies' selling situation.","ff4fb907":"#### Different types of real estates ","de6b78de":"#### To predict selling prices in the future, I will use machine learning to train a model for prediction. In addition, model can help us know which features are important for predicting selling price.","fbe6b5a3":"#### 8. SellerG\nSellerG columns will stand for names of different real estate agencies","19190000":"## - Identifying columns not essential for analysing and deleting -","219b3828":"From last 3D graph, it shows top 5 agencies' average area prices of top popular suburbs where they sold most real estates. I find the reason why Jellis the analysis mentioned before, sold almost most expensive, this is because most houses Jellis sold are located in more expensive suburbs than other agencies sold. Thus suburb feature causes agencies(SellerG) to affect their selling prices.","7cb319a0":"From this multiple graphs, this is one obvious relationship between the rooms and the bathrooms\uff0c there is a linear trend\uff0c meaning **that bathrooms are more and real estates are more likely to have more rooms.** And there's no other obvious relationships between each feature, can get from this graph. ","95ed336a":"\n#### 3. CouncilArea  \nCouncilArea is dependent on suburb and postcode, we have suburb and postcode,so I will delete this column","86f51b0f":"Analysing from different types of real estates, to see price distribution in a map.","c6616f02":"***There is a problem, interactive widgets combined with plotly figurewidget cannot display on Kaggle.* So I use Tableau to embed a map**","275b83a3":"#### From the graph above, it lists important features which affect selling prices: suburb,type,postcode,age,sellerG are the most 5 important features.","418d0ec3":"All analysis above are about the general increase tendency of real estates in Melbourne between time and prices. \n\nI will focus on the different types of real estate,  different agencies and different suburbs and real estates\u2019 ages to analyze the relationships between prices or their effects on the tendency of selling prices. ","e3fcdf4c":"#### 11. YearBuilt\n\nOriginal construction date","b1e50552":"#### Relationship between location and prices ","33a90a5d":"Look at the graph above, **there's not obviously shown that real estates are older, the prices will be higher.** So the selling price will depend on many other features probably like the real estate suburb and how big it is, but not mainly dependent on how old the real estate is.**However, through feature importance, I found relations and I will talk at machine learning part.**","dcbee6af":"#### 6. Suburb","ee1f623a":"## In todays' case, we use analyse this data from a few perspectives:\n\n\n#### 1.The relationship between time and price in melbourne.\n\nIn one year, the total number of real estates sold increases and average price also increases.\n#### 2.Different types of real estates' selling trendency\nHouses are most popular, but in one year , houses' average selling price decreases, other types increases.\n#### 3.What features will mostly affect Melbourne's housing rate?\nSuburb, type, age of houses, seller agency\n#### (Whether agencies will affect the selling price.)\nTypically, suburb increased the effect of agencies on prices. \n\n","df0c290a":"This violin plot shows each types selling price distribution, we can see that **the average price of houses is higher than tolls and higher than units.**","5c512519":"# Melbourne real estates' pricing rate analysis and prediction","b7e52552":"#### Gradient Boosting Regressor","ea80d435":"#### From last three regression models wvgboost ith different encoding methods, I found XGBoost with mean encoding and cross validation performs best, accuracy reaches at 80.4%.","0b2c6612":"Starting analyzing data I firstly pick out numeric features and the check there are some outliers among that data, needing to pay attention to these value which will have effect on the prediction accuracy\u3002","b0f86b88":"Through the last graph I want to see whether some high selling prices of real estates happened on some particular holidays, but I found there was no correlation. so **date may not be the most important feature to affect selling price but we can see the price tendency.**","4b991498":"#### 5. Price","e4d13181":"## - Identifying missing value and handling -","f94da85d":"#### Random Forest Regressor","27b731cb":"#### 9. Postcode\npostcode data type should be int,not float","55b6d3f7":"#### XGBoost","97e9553c":"#### 2. BuildingArea","8ad354a0":"#### The relationship between suburb and price we have already seen from last maps,postcode feature is similar with suburb.\n\n#### Next, I will see the relationships with age(years) of real estates.","c96f314c":"Choose 'Bentleigh East' suburb area, then see each type's price distribution","0b492ffa":"From the violin which shows top 5 agencies and their general selling prices' distribution. From the average selling price level,the house prices that Jellis sold at are mostly higher than other four agencies. Unit prices of these 5 agencies sold at have no obious difference. Townhouse prices of Jellis sold at are higher than that of hockingstuart, who sold townhouses at higher prices than other 3 agencies left.\n\nHowever, agency feature has no direct relationships with prices, I will study further later.","07be4ef7":"#### Next I will use XGBoost to find important features and verify my analysis via data","a0bcb659":"## Data Analysis","ad81a9be":"From the graph above, there is a problem, where 3 realestates were sold before they were built, we named it as preselling products. As the number of this problem is small, we can ignore it. (Note: there is on house whose age is 821, outlier, I should notice this when using machine learning to predict price.)","242cf5cb":"### Date\n#### Date means the date of selling the house"}}