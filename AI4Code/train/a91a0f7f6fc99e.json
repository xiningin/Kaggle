{"cell_type":{"ac6616b9":"code","ece7fe20":"code","89bae157":"code","a27407e0":"code","1df89d05":"code","17d742c3":"code","fe737304":"code","b7b21549":"code","5376147e":"code","25891c3f":"code","0565f700":"code","75a0db9f":"code","09c0b4e1":"code","912a3ab8":"code","df67192e":"code","d33827cf":"code","4f41ec35":"code","cac81f13":"code","fa59fde5":"code","db412c5e":"code","96d02cf2":"code","cecc818a":"code","ab9e8629":"code","20c4bbed":"code","2af2caae":"code","0fed7fbc":"code","367c3b8f":"code","bd65a185":"code","8e26eda4":"code","704950f4":"code","0e159c71":"code","b1463643":"code","e3ba0452":"code","55a3b390":"code","b7925bbc":"code","0ef06fd4":"code","8b187811":"code","5d99e9e5":"code","d2b7947d":"code","9410446f":"code","cba584f0":"code","a659c848":"code","ca7203d5":"code","f8e96d53":"code","1849e0b4":"code","c5113aa4":"code","d0d22680":"code","dd0ce6ef":"code","d4237764":"code","d88ddc41":"code","84fe5eb4":"code","446d5627":"markdown","f70ac49d":"markdown","c50a80f1":"markdown","0eff65e5":"markdown","fb7b5c99":"markdown","a7b54335":"markdown","27afb388":"markdown","2b4f53b4":"markdown","ee624315":"markdown","e4b55141":"markdown","b753fb81":"markdown","13510017":"markdown","fab4db4e":"markdown","2a8d62ef":"markdown","c11f3cd3":"markdown","e20cd851":"markdown","e01959dc":"markdown","367d14ea":"markdown","364f2c5d":"markdown"},"source":{"ac6616b9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import QuantileTransformer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.pipeline import Pipeline\n\nfrom mlxtend.classifier import StackingCVClassifier\n\nfrom xgboost import XGBClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ece7fe20":"train_df=pd.read_csv('..\/input\/learn-together\/train.csv')\ntest_df=pd.read_csv('..\/input\/learn-together\/test.csv')","89bae157":"train_df.head()","a27407e0":"test_df.head()","1df89d05":"print(\"shape training csv: %s\" % str(train_df.shape)) \nprint(\"shape test csv: %s\" % str(test_df.shape)) ","17d742c3":"train_df.columns","fe737304":"print(train_df.dtypes.value_counts())\nprint(test_df.dtypes.value_counts())","b7b21549":"train_df.describe()","5376147e":"print(f\"Missing Values in train: {train_df.isna().any().any()}\")\nprint(f\"Missing Values in test: {test_df.isna().any().any()}\")","25891c3f":"train_df = train_df.drop([\"Id\"], axis = 1)\n\ntest_ids = test_df[\"Id\"]\ntest_df = test_df.drop([\"Id\"], axis = 1)","0565f700":"plt.figure(figsize=(10,5))\nplt.title(\"Distribution of forest categories (Target Variable)\")\nsns.distplot(train_df[\"Cover_Type\"])\nplt.show()","75a0db9f":"sns.FacetGrid(train_df, hue=\"Cover_Type\", size=10).map(plt.scatter, \"Horizontal_Distance_To_Hydrology\", \"Vertical_Distance_To_Hydrology\").add_legend()","09c0b4e1":"temp = train_df[['Hillshade_9am','Hillshade_Noon','Hillshade_3pm','Cover_Type']]\nplt.figure(figsize=(15,12))\npd.plotting.parallel_coordinates(temp,'Cover_Type', colormap=plt.get_cmap(\"Set1\"))\nplt.title(\"parallel plots of Hillshade with forest categories\")\nplt.xlabel(\"Hillshade\")\nplt.show()","912a3ab8":"print(\"percent of negative values (training): \" + '%.3f' % ((train_df.loc[train_df['Vertical_Distance_To_Hydrology'] < 0].shape[0] \/ train_df.shape[0])*100))\nprint(\"percent of negative values (testing): \" + '%.3f' % ((test_df.loc[test_df['Vertical_Distance_To_Hydrology'] < 0].shape[0]\/ test_df.shape[0])*100))","df67192e":"plt.figure(figsize=(12,8))\nsns.boxplot(train_df['Vertical_Distance_To_Hydrology'])\nplt.show()","d33827cf":"plt.figure(figsize=(12,8))\nsns.boxplot(test_df['Vertical_Distance_To_Hydrology'])\nplt.show()","4f41ec35":"cols=list(train_df.columns)","cac81f13":"#list of columns other than Soil_Type and Wilderness_Area\nother_columns=[]\nfor i in range(10):\n    other_columns.append(cols[i])\nprint(other_columns)\n","fa59fde5":"print(f\"Train Column Types: {set(train_df.dtypes)}\")\nprint(f\"Test Column Types: {set(test_df.dtypes)}\")\n","db412c5e":"for column in train_df.columns:\n    print(column, train_df[column].nunique())","96d02cf2":"for column in test_df.columns:\n    print(column, test_df[column].nunique())","cecc818a":"print(\"- - - Train - - -\")\nprint(train_df[\"Soil_Type7\"].value_counts())\nprint(train_df[\"Soil_Type15\"].value_counts())\nprint(\"\\n\")\nprint(\"- - - Test - - -\")\nprint(test_df[\"Soil_Type7\"].value_counts())\nprint(test_df[\"Soil_Type15\"].value_counts())","ab9e8629":"train_df.drop([\"Soil_Type7\", \"Soil_Type15\"], axis = 1,inplace=True)\ntest_df.drop([\"Soil_Type7\", \"Soil_Type15\"], axis = 1,inplace=True)","20c4bbed":"train_minmax=train_df[other_columns]\ntest_minmax=test_df[other_columns]\nmm_scaler = MinMaxScaler()\n# my_train_minmax = mm_scaler.fit_transform(train_df[other_columns])\nmm_scaler.fit(train_minmax)\ntrain_trans=mm_scaler.transform(train_minmax)\ntest_trans=mm_scaler.transform(test_minmax)\n\ntemp_train=pd.DataFrame(train_trans)\ntemp_test=pd.DataFrame(test_trans)\ntemp_test.head()\n","2af2caae":"for i in range(10):\n    temp_train.rename(columns={i:other_columns[i]},inplace=True)\n    temp_test.rename(columns={i:other_columns[i]},inplace=True)\ntemp_train.head()\n","0fed7fbc":"train_df[other_columns]=temp_train[other_columns]\ntest_df[other_columns]=temp_test[other_columns]\ntrain_df.head()","367c3b8f":"# def model_training(model,X_train,y_train):\n#     scores =  cross_val_score(model, X_train, y_train,\n#                               cv=5)\n#     return scores.mean()","bd65a185":"X=train_df.drop(['Cover_Type'], axis=1)\ny=train_df['Cover_Type']\nX_train, X_val, y_train, y_val = train_test_split(X,y, test_size=0.25,random_state=42)","8e26eda4":"X_train.shape, X_val.shape, y_train.shape, y_val.shape","704950f4":"# importing K-Nearest Neighbors Classifier function\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_model=KNeighborsClassifier(n_jobs=-1)\n# classifier learning the model\nknn_model = knn_model.fit(X_train, y_train)","0e159c71":"# Using 10 K-Fold CV on data, gives peroformance measures\naccuracy  = cross_val_score(knn_model, X_train, y_train, cv = 10, scoring = 'accuracy')\nf1_score = cross_val_score(knn_model, X_train, y_train, cv = 10, scoring = 'f1_macro')\n# calculating mean of all 10 observation's accuracy and f1, \n# taking percent and rounding to two decimal places\nacc_mean = np.round(accuracy.mean() * 100, 2)\nf1_mean = np.round(f1_score.mean() * 100, 2)\nprint(\"The accuracy score of training set:\", acc_mean,\"%\")\nprint(\"f1 score is\", f1_mean)","b1463643":"# importing model for feature importance\n# from sklearn.ensemble import ExtraTreesClassifier\n# passing the model\n# etc_model = ExtraTreesClassifier(n_jobs=-1,random_state = 42)\n# training the model\n# etc_model=etc_model.fit(X_train, y_train)\n# etc_model=etc_model.fit(X, y)\n\n# extracting feature importance from model and making a dataframe of it in descending order\n# etc_feature_importances = pd.DataFrame(etc_model.feature_importances_, index = X.columns, columns=['ETC']).sort_values('ETC', ascending=False)\n# show top 10 features\n# etc_feature_importances.head(10)","e3ba0452":"sample_train = train_df[['Elevation','Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Fire_Points','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology','Aspect','Wilderness_Area4',\n            'Hillshade_Noon','Hillshade_3pm','Hillshade_9am','Slope','Soil_Type22','Soil_Type10','Soil_Type4','Soil_Type34','Soil_Type34','Wilderness_Area3','Soil_Type12',\n            'Soil_Type2','Wilderness_Area1', 'Cover_Type']]","55a3b390":"# feeding sample features to var 'X'\nX = sample_train.iloc[:,:-1]\n# feeding our target variable to var 'y'\ny = sample_train['Cover_Type']","b7925bbc":"# etc_model = ExtraTreesClassifier(n_jobs=-1,random_state = 42)\n# etc_model = etc_model.fit(X, y)\n# accuracy  = cross_val_score(etc_model, X, y, cv = 10, scoring = 'accuracy')\n# f1_score = cross_val_score(etc_model, X, y, cv = 10, scoring = 'f1_macro')\n\n# calculating mean of all 10 observation's accuracy and f1, taking percent and rounding to two decimal places\n# acc_mean = np.round(accuracy.mean() * 100, 2)\n# f1_mean = np.round(f1_score.mean() * 100, 2)\n\n# returns performance measure and time of the classifier \n# print(\"The accuracy score of this classifier on our training set is\", acc_mean,\"%\")\n# print(\"f1 score is\", f1_mean,\"%\")","0ef06fd4":"# Number of trees in random forest\n# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 8)]\n# Number of features to consider at every split\n# max_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\n# max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n# max_depth.append(None)\n# Minimum number of samples required to split a node\n# min_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\n# min_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\n# bootstrap = [True, False]\n# Create the random grid\n# random_grid = {'n_estimators': n_estimators,\n#                'max_features': max_features,\n#                'max_depth': max_depth,\n#                'min_samples_split': min_samples_split,\n#                'min_samples_leaf': min_samples_leaf,\n#                'bootstrap': bootstrap}\n# print(random_grid)","8b187811":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\n# rf_model = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\n# rf_random = RandomizedSearchCV(estimator = rf_model, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\n# rf_random.fit(X_train, y_train)","5d99e9e5":"# rf_random.best_params_","d2b7947d":"# model = RandomForestClassifier(n_estimators=100,random_state = 42)\n# model=RandomForestClassifier(n_estimators=885,min_samples_split=2,min_samples_leaf=1,max_features='sqrt',max_depth=110,bootstrap=False,random_state = 42)\n# Train the model on training data\n# model.fit(X_train, y_train)","9410446f":"# predictions = model.predict(X_val)\n# accuracy_score(y_val, predictions)","cba584f0":"model_rf=RandomForestClassifier(n_estimators=885,\n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   max_features='sqrt',\n                                   max_depth=110,\n                                   bootstrap=False,\n                                  random_state=42)\n# model_rf.fit(X,y)","a659c848":"model_xgb = OneVsRestClassifier(XGBClassifier(random_state=42))\n# model_xgb.fit(X,y)","ca7203d5":"model_et = ExtraTreesClassifier(random_state=42)","f8e96d53":"# sclf = StackingCVClassifier(classifiers=[model_rf,model_xgb,model_et],\n#                             use_probas=True,\n#                             meta_classifier=model_rf,random_state=42)\n# labels = ['Random Forest', 'XGBoost', 'ExtraTrees', 'MetaClassifier']","1849e0b4":"# for clf, label in zip([model_rf, model_xgb, model_et, model_rf], labels):\n#     scores = cross_val_score(clf, X_train.values, y_train.values.ravel(),\n#                              cv=5,scoring='accuracy')\n#     print(\"Accuracy: %0.2f (+\/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n","c5113aa4":"# sclf.fit(X_train.values, y_train.values.ravel())","d0d22680":"# val_pred = sclf.predict(X_val.values)\n# acc = accuracy_score(y_val, val_pred)\n# print(acc)","dd0ce6ef":"# model_sclf = StackingCVClassifier(classifiers=[model_rf,model_xgb,model_et],\n#                                use_probas=True,\n#                                meta_classifier=model_rf,random_state=42)\n# model_sclf.fit(X.values, y.values.ravel())","d4237764":"sample_test = test_df[['Elevation','Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Fire_Points','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology','Aspect','Wilderness_Area4',\n            'Hillshade_Noon','Hillshade_3pm','Hillshade_9am','Slope','Soil_Type22','Soil_Type10','Soil_Type4','Soil_Type34','Soil_Type34','Wilderness_Area3','Soil_Type12',\n            'Soil_Type2','Wilderness_Area1']]","d88ddc41":"# test_pred = model_rf.predict(test_df)\n# test_pred = model_sclf.predict(sample_test.values)\n# test_pred = etc_model.predict(sample_test.values)","84fe5eb4":"# Save test predictions to file\n# output = pd.DataFrame({'Id': test_ids,\n#                        'Cover_Type': test_pred})\n# output.to_csv('submission_sclf2.csv', index=False)","446d5627":"## NaN\nLet's see how many NaN we have in our datasets\n","f70ac49d":"## The challenge:\n\nIn this competition you\u2019ll predict what types of trees there are in an area based on various geographic features.\n\nThe competition datasets comes from a study conducted in four wilderness areas within the beautiful Roosevelt National Forest of northern Colorado. These areas represent forests with very little human disturbances \u2013 the existing forest cover types there are more a result of ecological processes rather than forest management practices.\n\nThe data is in raw form and contains categorical data such as wilderness areas and soil type.","c50a80f1":"## Import Packages","0eff65e5":"## K-Nearest Neighbors","fb7b5c99":"## Extra-Trees Classifier","a7b54335":"Let's drop `Soil_Type7` and `Soil_Type15` columns from train and test dataset","27afb388":"## Make Predictions on the Test Set","2b4f53b4":"In the train set, coulmns `Soil_Type7` and `Soil_Type15` have only a single value.\n\n**Let's check the number of unique values for these columns in the train and test set.**","ee624315":"**Let's delete the Id column in the training set but store it for the test set before deleting**","e4b55141":"## Load Dataset","b753fb81":"## Model Training\n\nLet's use 80% of the Data for training, and 20% for validation. We'll then train a simple Random Forest Classifier with 100 trees\n","13510017":"### Percentage of Negative value in `Vertical_Distance_To_Hydrology`","fab4db4e":"![](https:\/\/lh3.googleusercontent.com\/hnKsVpLaBNd736a9utrKVNzzUzKFqR2Bs-3ObYT45pAz3O6sdM1Akzg_lR2dqway1D3DRcJ4NHfVkrlHSy6B_Dy9qNo0BhW4aVo9BAq5SOKXT42Jp18AtUjxGkbA6SEJ4J-vlwQYwzBKSEaSTbqCfRBnbn-YWUq-bJtRXxpCUpjJRaDswUp2dTd8mJAxe13XLWrmuuvFoeT9lLm8cHaR0dQ6egZeSyqzHPepppK38nLeODy3h6qxWrnHG_blLOzZpulRhs5H5CCIUEAvQWa1-CNY9-GI5BXEv9CbkFP5f4nud75OuJhGEBAioO45dLXwujCTEqxCkR_mMbklkbb17SL1d5P9VLyKORiTjxCqiHB3zetyUt-e6pr8rK4FUXk1MRn_lyoJhCYTd0USy1yzvgRITFwNY2zH3O8MJXR89u6IoL6RHmcwvawt6chrl5d0s0nHgcIr9myYGgattmE1Kds8vacjldnT2fXIgTbOAKzxk62X6nT-elfByuz58egaLS4pS4WRed-wpR0tH7PrxomD5JPQ8mZgO_dEVr_l6sgqmANmWY449aLuODB0NNIa4Hd0Hh0LNKmgDmNeW7PexEMkaemNmp6A1ghsMgqqRHZrKK0w3-w_KM-2458w3DE87jKq6A-fdJyPMqc_J7cYKfiR9G3v7tc=w1000-h600-no)","2a8d62ef":"## Data Types","c11f3cd3":"### Feature types","e20cd851":"## Random Forest","e01959dc":"So, there are no missing values in train\/test datasets.","367d14ea":"### Parallel plots\n\nA hillshade is a grayscale 3D representation of the surface, with the sun's relative position taken into account for shading the image. This function uses the altitude and azimuth properties to specify the sun's position.\n\n1. **Hillshade_9am** (0 to 255 index - Hillshade index at 9am, summer solstice\n\n2. **Hillshade_Noon** (0 to 255 index) - Hillshade index at noon, summer solstice\n\n3. **Hillshade_3pm** (0 to 255 index) - Hillshade index at 3pm, summer solstice\n","364f2c5d":"## Stacking\nEnsemble stacking can be referred to as blending, because all the numbers are blended to produce a prediction or classification."}}