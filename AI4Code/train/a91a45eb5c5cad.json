{"cell_type":{"29ee0d20":"code","3f693001":"code","9b27a651":"code","db3a2034":"code","35cb150c":"code","30aef01a":"code","6f088944":"code","5f3afe55":"code","ad89f132":"code","494bdee2":"code","e1a03551":"code","3296d1a3":"code","a2d594f6":"code","11e8e158":"code","f17800c4":"code","2f2aacbf":"code","2752526e":"code","3e808304":"code","e537361d":"code","f8ad9d8e":"code","5218ca81":"code","6645bd28":"code","d458f4a0":"code","8c9d49fa":"code","0e55906a":"code","89d1d89a":"code","ee86e18b":"code","e35a1c40":"code","df9d197f":"code","887716f5":"code","c8f599f6":"code","9f009f0d":"code","0caf2207":"code","2281c13c":"code","2f9062e4":"code","a7883bb2":"code","9acb8ab5":"code","c354a241":"code","f296c272":"code","4871fd49":"markdown","b58fca98":"markdown","7d778f28":"markdown","cfa31dd1":"markdown","f191d1b9":"markdown","df8f68c7":"markdown","bd97a330":"markdown","c9cbf598":"markdown","d0909f71":"markdown","962d3745":"markdown","0c776576":"markdown","7b776d99":"markdown","ecbe542f":"markdown","cc62e355":"markdown","16ab804a":"markdown","473ce5f5":"markdown","a0025495":"markdown","3af6c5a9":"markdown","b0f730c3":"markdown","af8ec7b9":"markdown","f6dc0b36":"markdown","37f22799":"markdown","0ba566c5":"markdown","0fcc9944":"markdown","6d41ed42":"markdown","4c9d32fb":"markdown","ec20a2d8":"markdown","921e7514":"markdown","9b666a51":"markdown","28be8c65":"markdown","4afec3e9":"markdown","1ba461d0":"markdown"},"source":{"29ee0d20":"# Import important libraries \n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cufflinks as cf\ncf.go_offline()","3f693001":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\npid = test[\"PassengerId\"]","9b27a651":"train.info()\ntrain.describe()","db3a2034":"sns.heatmap(train.isnull(), yticklabels = False, cbar = False, cmap = 'viridis')","35cb150c":"sns.set_style('whitegrid')\nsns.countplot(x = 'Survived', data = train)","30aef01a":"sns.set_style('whitegrid')\nsns.countplot(x = 'Survived', hue = 'Sex', data = train)","6f088944":"sns.set_style('whitegrid')\nsns.countplot(x = 'Survived', hue = 'Pclass', data = train)","5f3afe55":"sns.barplot(\"Pclass\", \"Survived\", data = train)\n","ad89f132":"sns.barplot(\"Embarked\", \"Survived\", data = train)\n","494bdee2":"sns.distplot(train['Age'].dropna(), kde = False, bins = 30)","e1a03551":"sns.countplot(x = 'SibSp', data = train)","3296d1a3":"sns.barplot(\"SibSp\", \"Survived\", data = train)\n","a2d594f6":"sns.barplot(\"Parch\", \"Survived\", data = train)","11e8e158":"train['Fare'].iplot(kind = 'hist', bins = 30, color = 'blue')","f17800c4":"sns.boxplot(x = 'Pclass', y = 'Age', data = train)","2f2aacbf":"# We create a function to get the avg. age based on Pclass\n\ndef pred_age(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    \n    if pd.isnull(Age):\n\n        if Pclass == 1:\n            return 38\n\n        elif Pclass == 2:\n            return 29\n\n        else:\n            return 25\n\n    else:\n        return Age\n    \ntrain['Age'] = train[['Age','Pclass']].apply(pred_age, axis = 1)","2752526e":"# We create a function to get the avg. age based on Pclass\n\ndef pred_age(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    \n    if pd.isnull(Age):\n\n        if Pclass == 1:\n            return 40\n\n        elif Pclass == 2:\n            return 28\n\n        else:\n            return 24\n\n    else:\n        return Age\n    \ntest['Age'] = test[['Age','Pclass']].apply(pred_age, axis = 1)","3e808304":"sns.heatmap(train.isnull(), yticklabels = False, cbar = False, cmap = 'viridis')","e537361d":"sns.heatmap(test.isnull(), yticklabels = False, cbar = False, cmap = 'viridis')","f8ad9d8e":"\nfamily = train[\"SibSp\"] + train[\"Parch\"]      # combining the siblings and family data\ntrain[\"Alone\"] = np.where(family >  0, 0, 1)  # If a passenger is traveling alone or with a family member\n\nfam = test[\"SibSp\"] + test[\"Parch\"]\ntest[\"Alone\"] = np.where(fam >  0, 0, 1)","5218ca81":"train.drop(['PassengerId', 'Ticket', 'Cabin', 'Name', 'SibSp', 'Parch'], axis = 1, inplace = True)\ntest.drop(['PassengerId', 'Ticket', 'Cabin', 'Name', 'SibSp', 'Parch'], axis = 1, inplace = True)","6645bd28":"train.info()","d458f4a0":"sns.heatmap(train.isnull(), yticklabels = False, cbar = False, cmap = 'viridis')","8c9d49fa":"sns.countplot(x = 'Embarked', data = train)","0e55906a":"train.fillna({\"Embarked\": \"S\"}, inplace = True)","89d1d89a":"sns.heatmap(test.isnull(), yticklabels = False, cbar = False, cmap = 'viridis')","ee86e18b":"test[\"Fare\"].fillna(test[\"Fare\"].median(), inplace = True)","e35a1c40":"train = pd.get_dummies(train, columns = ['Embarked', 'Sex', 'Pclass'], drop_first = True)\ntest = pd.get_dummies(test, columns = ['Embarked', 'Sex', 'Pclass'], drop_first = True)","df9d197f":"train.head()","887716f5":"test.head()","c8f599f6":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n","9f009f0d":"X = train.drop(\"Survived\",axis = 1)\nY = train[\"Survived\"]\n#X_test = test","0caf2207":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)","2281c13c":"# Logistic Regression\n\nlogmod = LogisticRegression(max_iter = 200)\n\nlogmod.fit(X_train,Y_train)\n\npred = logmod.predict(X_test)\n\nacc_logmod = round(accuracy_score(Y_test, pred) * 100, 2)\nprint(acc_logmod)","2f9062e4":"# K Nearest Neighbors\n\nknn = KNeighborsClassifier(n_neighbors = 5)\n\nknn.fit(X_train, Y_train)\n\nknn_pred = knn.predict(X_test)\n\nacc_knn = round(accuracy_score(Y_test, knn_pred) * 100, 2)\n\nprint(acc_knn)\n#knn.score(X_train, Y_train)","a7883bb2":"# Support Vector Machines\n\n\nparam_grid = {\"C\": [0.1, 1, 10, 100, 1000], \"gamma\": [1, 0.1, 0.01, 0.001, 0.0001]}\n\ngrid = GridSearchCV(SVC(), param_grid, verbose = 3)\n\ngrid.fit(X_train, Y_train)\n\ngrid_pred = grid.predict(X_test)\n\nacc_svc = round(accuracy_score(Y_test, grid_pred) * 100, 2)\nprint(acc_svc)\n#print(grid.score(X_train, Y_train))","9acb8ab5":"# Random Forest\n\n\nrandom_forest = RandomForestClassifier(n_estimators = 150, random_state = 5)\n\nrandom_forest.fit(X_train, Y_train)\n\nRF_pred = random_forest.predict(X_test)\n\nacc_RF = round(accuracy_score(Y_test, RF_pred) * 100, 2)\nprint(acc_RF)\n#random_forest.score(X_train, Y_train)","c354a241":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'KNN', 'Support Vector Machines', 'Random Forest'],\n    'Score': [acc_logmod, acc_knn, acc_svc, acc_RF]})\n\nmodels.sort_values(by='Score', ascending = False)","f296c272":"predict = random_forest.predict(test)\n\nsubmit = pd.DataFrame({\n        \"PassengerId\": pid,\n        \"Survived\": predict})\n\nsubmit.to_csv('submit.csv', index=False)","4871fd49":"# ** 2. Load data **\n\nLet's start by reading the training and test dataset.","b58fca98":"### 2.2 The Data \n\nExplore the structure and summary of our data set to try and understand the data and how to proceed.","7d778f28":"# ** 5. Prediction **","cfa31dd1":"We can see clearly that there are two columns with missing data, the 'Age' column has about 20% missing data which is small for some reasonable replacement through imputation. The 'Cabin' column has a lot of missing data and we might have to drop the column beacause it would be difficult to do something useful with it, \nWe continue with more exploratory analysis.","f191d1b9":"The missing values in 'Age' has been filled as shown in the plot. ","df8f68c7":"This plot shows that most passengers in the dataset do not have children or spouse on board.","bd97a330":"### 4.2 Building the Model","c9cbf598":"The plot above shows that passengers are more likely to embark in Southampton (S). We use S to fill the missing data in the Embarked column","d0909f71":"Looks like we have more passengers that didn't survive than passengers that did survive in our dataset.","962d3745":"# ** 4. Model **\n\nFinally, we are ready to train our model and do some predictions. We will use four different classification techniques to approach this - \n\n* Logistic Regression\n* K Nearest Neighbors\n* Support Vector Machines\n* Random Forest","0c776576":"We can see a kind of trend here. Looks like passengers that did not survive are much more likely to be male and passengers that survived are likely to be female.","7b776d99":"# ** Introduction **\nI am a newbie to machine learning and this is my first kaggle kernel. I chose the Titanic dataset to show some basic concept in data science and machine learning. First, we will visually expore the Titanic dataset and then some data cleaning with missing values imputation. Finally, we will train a model to predict survival on the Titanic.\n\n\nThere are three parts to my notebook as follows:\n\n* **Exploratory data analysis**\n* **Data cleaning and missing value imputation**\n* **Models and prediction**","ecbe542f":"We will use the Random Forest model for our predictions.","cc62e355":"### 4.3 Model evaluation\n\nWe rank our four classifiers and choose the classifier with the better accuracy. For our prediction, we use the random forest model.","16ab804a":"### 3.1 Missing data \n\nUsing seaborn to create a heatmap to see where we are missing data.\n","473ce5f5":"# ** Conclusion **\n\nThank you for reading. I look forward to doing more with more details. Please feel free to comment and give suggestions.\n","a0025495":"# A Study for Survival. Titanic\n### ** Ifeanyi N. **\n30.04.2020\n\n1. ** 1. Introduction **\n2. ** Load data **   \n2.1 Load the training and test dataset.        \n2.2 The data\n\n3. ** Exploratory data analysis **   \n3.1 Missing data     \n3.2 Data cleaning       \n3.3 Categorical features        \n4. ** Model **   \n4.1 Split into training and test set   \n4.2 Building the model\n5. ** Prediction **\n6. ** Conclusion **   \n#### ** Reference **\n","3af6c5a9":"Looks like the 'Age' is skewed toward younger passengers of average of about 20 - 30.","b0f730c3":"Looking at the passengers that survived based on passenger class, it looks like passengers that did not survive tends to be from the third class or the cheapest class.\n\n","af8ec7b9":"# ** Reference **\n\n[Exploring Survival on the Titanic](http:\/\/www.kaggle.com\/mrisdal\/exploring-survival-on-the-titanic)   \n[A Journey Through Titanic](https:\/\/www.kaggle.com\/omarelgabry\/a-journey-through-titanic)    \n[Titanic Data Science Solutions](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions)   ","f6dc0b36":"The 'Embarked' column has two missing values which is easy to fill in.\n","37f22799":"We create dummie variables for our categorical variables, however, we drop the first columns of our dummie variables to avoid the problem of multicollinearity.","0ba566c5":"### 3.2 Data Cleaning \n\nThrough our exploration, we found out we had some missing data. Cleaning the dataset and cleaning the data would help us for our machine learning algorithms.\n","0fcc9944":"This plot shows when 'Age' is seperated by class ('Pclass'), the first class and second class tends to be older than the third class. We use this simple information to predict the missing values of 'Age' based on 'Pclass'.\n","6d41ed42":"### 3.3 Categorical Features \nUsing pandas, we can convert categorical features into dummy variables.","4c9d32fb":"The dataset is split into training dataset (80%) and test dataset (20%).\n","ec20a2d8":"### 4.1 Split into training and test set","921e7514":"The family column can be represented by one column instead of having two columns Parch & SibSp, we can have only one column to represent if the passenger was alone or had any family member aboard or not. Meaning, if having any family member will increase chances of Survival or not.","9b666a51":"# ** 3. Exploratory data analysis **\n\nExploratory analysis to help visualise the relationship or patterns in the data. Let's begin with missing values.","28be8c65":"We drop some columns we do not need for our model.","4afec3e9":"### 2.1 Load trainig and test dataset","1ba461d0":"We look also at the 'Fare' column. This plot indicates that most of the purchase range from 0 - 80. This makes sense because we have already seen that most passengers are in the third class of the 'Pclass' column. "}}