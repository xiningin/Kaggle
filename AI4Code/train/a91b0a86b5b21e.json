{"cell_type":{"2d1d0067":"code","303fac7e":"code","b70f85fc":"code","6bfdfce9":"code","290098af":"code","34f64ce1":"code","b91731dd":"code","b057a3ac":"code","c1455ca7":"code","df55e77e":"code","c461a67b":"code","8c0cf108":"code","fc38c7a4":"code","b1845560":"code","c550f9e3":"code","5e99885d":"code","dfbc4c43":"code","1c77a297":"code","8ee085e3":"code","3e3b9fb3":"code","88e01a50":"code","326902b9":"markdown","c5781e90":"markdown","8ea97386":"markdown","a8ed86c8":"markdown","9799275f":"markdown","5a48e636":"markdown","ddbac6cf":"markdown","e4ad4351":"markdown","03ae94f1":"markdown","6dfb6c85":"markdown","be4ec9be":"markdown","7a6cd9bf":"markdown","5d69c02a":"markdown","e737a00a":"markdown","dad8efd2":"markdown","6c55b3bd":"markdown","d94e2438":"markdown","b5722df7":"markdown","ab3b3d0a":"markdown","edd82c05":"markdown"},"source":{"2d1d0067":"!pip install tf-nightly-gpu-2.0-preview tfp-nightly\n!pip install -q pydot\n!apt-get install graphviz","303fac7e":"# plotting inline\n%matplotlib inline\n\n# importing necessary modules\nimport keras\nimport random\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as sp\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Activation, concatenate, Input, Embedding\nfrom tensorflow.keras.layers import Reshape, Concatenate, BatchNormalization, Dropout, Add, Lambda\nfrom tensorflow.keras.layers import add\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom copy import deepcopy\n\n# turning off automatic plot showing, and setting style\nplt.ioff()\nplt.style.use('bmh')","b70f85fc":"# setting seed \nnp.random.seed(42)\n\n# generating big and small datasets\nX = np.random.uniform(0.0, 0.5, 100).reshape(-1,1)\n\n# let us generate a grid to check how models fit the data\nx_grid = np.linspace(-5, 5, 1000).reshape(-1,1)\n\n# defining the function\nnoise = sp.norm(0.00, 0.02)\ntarget_toy = lambda x: (x + 0.3*np.sin(2*np.pi*(x + noise.rvs(1)[0])) + \n                        0.3*np.sin(4*np.pi*(x + noise.rvs(1)[0])) + \n                        noise.rvs(1)[0] - 0.5)\n\n# runnning the target\ny = np.array([target_toy(e) for e in X])","6bfdfce9":"# let us check the toy data\nplt.figure(figsize=[12,6], dpi=200)\n\n# first plot\nplt.plot(X, y, 'kx', label='Toy data', alpha=0.8)\nplt.title('Simple 1D example with toy data by Blundell et. al (2015)')\nplt.xlabel('$x$'); plt.ylabel('$y$')\nplt.xlim(-0.5,1.0); plt.ylim(-1.0,1.0)\nplt.legend();\nplt.show()","290098af":"# prior network output #\n\n# shared input of the network\nnet_input = Input(shape=(1,),name='input')\n\n# let us build the prior network with five layers\nprior_net = Sequential([Dense(16,'elu',kernel_initializer='glorot_normal',trainable=False),\n                        Dense(16,'elu',kernel_initializer='glorot_normal',trainable=False)],\n                       name='prior_net')(net_input)\n\n# prior network output\nprior_output = Dense(1,'linear',kernel_initializer='glorot_normal',\n                     trainable=False, name='prior_out')(prior_net)\n\n# compiling a model for this network\nprior_model = Model(inputs=net_input, outputs=prior_output)\n\n# let us score the network and plot the results\nprior_preds = 3 * prior_model.predict(x_grid)","34f64ce1":"# let us check the toy data\nplt.figure(figsize=[12,6], dpi=200)\n\n# first plot\nplt.plot(X, y, 'kx', label='Toy data', alpha=0.8)\nplt.plot(x_grid, prior_preds, label='prior net (p)')\nplt.title('Predictions of the prior network: random function')\nplt.xlabel('$x$'); plt.ylabel('$y$')\nplt.xlim(-0.5,1.0); plt.ylim(-1.0,1.0)\nplt.legend();\nplt.show()","b91731dd":"# adding trainable network #\n\n# trainable network body\ntrainable_net = Sequential([Dense(16,'elu'),\n                            Dense(16,'elu')],\n                           name='trainable_net')(net_input)\n\n# trainable network output\ntrainable_output = Dense(1,'linear',name='trainable_out')(trainable_net)","b057a3ac":"# using a lambda layer so we can control the weight (beta) of the prior network\nprior_scale = Lambda(lambda x: x * 3.0, name='prior_scale')(prior_output)\n\n# lastly, we use a add layer to add both networks together and get Q\nadd_output = add([trainable_output, prior_scale], name='add')\n\n# defining the model and compiling it\nmodel = Model(inputs=net_input, outputs=add_output)\nmodel.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])","c1455ca7":"# checking final architecture\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\n\nSVG(model_to_dot(model).create(prog='dot', format='svg'))","df55e77e":"# let us fit the model\nmodel.fit(X, y, epochs=2000, batch_size=100, verbose=0)\n\n# let us get the individual output of the trainable net\ntrainable_model = Model(inputs=model.input, outputs=model.get_layer('trainable_out').output)","c461a67b":"# let us check the toy data\nplt.figure(figsize=[12,6], dpi=200)\n\n# first plot\nplt.plot(X, y, 'kx', label='Toy data', alpha=0.8)\nplt.plot(x_grid, 3 * prior_model.predict(x_grid), label='prior net (p)')\nplt.plot(x_grid, trainable_model.predict(x_grid), label='trainable net (f)')\nplt.plot(x_grid, model.predict(x_grid), label='resultant (Q)')\nplt.title('Adding the trainable net, and testing our full Keras randomized prior functions model')\nplt.xlabel('$x$'); plt.ylabel('$y$')\nplt.xlim(-0.5,1.0); plt.ylim(-1.0,1.0)\nplt.legend();\nplt.show()","8c0cf108":"# function to get a randomized prior functions model\ndef get_randomized_prior_nn():\n\n    # shared input of the network\n    net_input = Input(shape=(1,), name='input')\n\n    # trainable network body\n    trainable_net = Sequential([Dense(16,'elu'),\n                                Dense(16,'elu')], \n                               name='trainable_net')(net_input)\n    \n    # trainable network output\n    trainable_output = Dense(1, 'linear', name='trainable_out')(trainable_net)\n\n    # prior network body - we use trainable=False to keep the network output random \n    prior_net = Sequential([Dense(16,'elu',kernel_initializer='glorot_normal',trainable=False),\n                            Dense(16,'elu',kernel_initializer='glorot_normal',trainable=False)], \n                           name='prior_net')(net_input)\n    \n    # prior network output\n    prior_output = Dense(1, 'linear', kernel_initializer='glorot_normal', trainable=False, name='prior_out')(prior_net)\n    \n    # using a lambda layer so we can control the weight (beta) of the prior network\n    prior_output = Lambda(lambda x: x * 3.0, name='prior_scale')(prior_output)\n\n    # lastly, we use a add layer to add both networks together and get Q\n    add_output = add([trainable_output, prior_output], name='add')\n\n    # defining the model and compiling it\n    model = Model(inputs=net_input, outputs=add_output)\n    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n    \n    # returning the model \n    return model","fc38c7a4":"# wrapping our base model around a sklearn estimator\nbase_model = KerasRegressor(build_fn=get_randomized_prior_nn, \n                            epochs=3000, batch_size=100, verbose=0)\n\n# create a bagged ensemble of 10 base models\nbag = BaggingRegressor(base_estimator=base_model, n_estimators=9, verbose=2)","b1845560":"# fitting the ensemble\nbag.fit(X, y.ravel())","c550f9e3":"# individual predictions on the grid of values\ny_grid = np.array([e.predict(x_grid.reshape(-1,1)) for e in bag.estimators_]).T\ntrainable_grid = np.array([Model(inputs=e.model.input,outputs=e.model.get_layer('trainable_out').output).predict(x_grid.reshape(-1,1)) for e in bag.estimators_]).T\nprior_grid = np.array([Model(inputs=e.model.input,outputs=e.model.get_layer('prior_scale').output).predict(x_grid.reshape(-1,1)) for e in bag.estimators_]).T","5e99885d":"# let us check the toy data\nplt.figure(figsize=[16,12], dpi=200)\n\n# loop for plotting predictions of each head \nfor i in range(9):\n    plt.subplot(3,3,i+1)\n    plt.plot(X, y, 'kx', label='Toy data', alpha=0.8)\n    plt.plot(x_grid, prior_grid[0,:,i], label='prior net (p)')\n    plt.plot(x_grid, trainable_grid[0,:,i], label='trainable net (f)')\n    plt.plot(x_grid, y_grid[:,i], label='resultant (Q)')\n    plt.title('Ensemble: Model #{}'.format(i+1), fontsize=14)\n    plt.xlabel('$x$'); plt.ylabel('$y$')\n    plt.xlim(-0.5,1.0); plt.ylim(-1.0,1.0)\nplt.tight_layout()\nplt.show();","dfbc4c43":"# computing mean and stddev\nmean = np.array(y_grid).mean(axis=1)\nstd = np.array(y_grid).std(axis=1)\n\n# opening figure\nfig = plt.figure(figsize=[12,5], dpi=150)\n\n# title of the plot\nfig.suptitle('Uncertainty estimates given by bootstrapped ensemble of neural networks with randomized priors', verticalalignment='center')\n\n# first subplot, \nplt.subplot(1, 2, 1)\n\n# let us plot the training data\nplt.plot(X, y, 'kx',  label='Toy data')\nplt.title('Mean and Deviation', fontsize=12)\nplt.xlim(-0.5, 1.0); plt.ylim(-1.5, 1.5)\nplt.legend()\n\n# plotting predictive mean and deviation\nplt.plot(x_grid, mean, 'r--', linewidth=1.5)\nplt.fill_between(x_grid.reshape(1,-1)[0], mean - std, mean + std, alpha=0.5, color='red')\nplt.fill_between(x_grid.reshape(1,-1)[0], mean + 2*std, mean - 2*std, alpha=0.2, color='red')\n\n# second subplot\nplt.subplot(1, 2, 2)\n\n# let us plot the training data\nplt.plot(X, y, 'kx', label='Toy data')\nplt.title('Samples', fontsize=12)\nplt.xlim(-0.5, 1.0); plt.ylim(-1.5, 1.5)\nplt.legend()\n\n# loop for plotting predictions of each head \nfor i in range(9):\n    plt.plot(x_grid, y_grid[:,i], linestyle='--', linewidth=1.5)\n    \n# showing\nplt.show();","1c77a297":"# function to get a randomized prior functions model\ndef get_regular_nn():\n\n    # shared input of the network\n    net_input = Input(shape=(1,), name='input')\n\n    # trainable network body\n    trainable_net = Sequential([Dense(16, 'elu'),\n                                Dense(16, 'elu')], \n                               name='trainable_net')(net_input)\n    \n    # trainable network output\n    trainable_output = Dense(1, activation='linear', name='trainable_out')(trainable_net)\n\n    # defining the model and compiling it\n    model = Model(inputs=net_input, outputs=trainable_output)\n    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n    \n    # returning the model \n    return model","8ee085e3":"# wrapping our base models around a sklearn estimator\nbase_rpf = KerasRegressor(build_fn=get_randomized_prior_nn, \n                          epochs=3000, batch_size=100, verbose=0)\nbase_reg = KerasRegressor(build_fn=get_regular_nn, \n                          epochs=3000, batch_size=100, verbose=0)\n\n# our models\nprior_but_no_boostrapping = BaggingRegressor(base_rpf, n_estimators=9, bootstrap=False)\nbootstrapping_but_no_prior = BaggingRegressor(base_reg, n_estimators=9)\nno_prior_and_no_boostrapping = BaggingRegressor(base_reg, n_estimators=9, bootstrap=False)\n\n# fitting the models\nprior_but_no_boostrapping.fit(X, y.ravel())\nbootstrapping_but_no_prior.fit(X, y.ravel())\nno_prior_and_no_boostrapping.fit(X, y.ravel())","3e3b9fb3":"# individual predictions on the grid of values\ny_grid = np.array([e.predict(x_grid.reshape(-1,1)) for e in bag.estimators_]).T\ny_grid_1 = np.array([e.predict(x_grid.reshape(-1,1)) for e in prior_but_no_boostrapping.estimators_]).T\ny_grid_2 = np.array([e.predict(x_grid.reshape(-1,1)) for e in bootstrapping_but_no_prior.estimators_]).T\ny_grid_3 = np.array([e.predict(x_grid.reshape(-1,1)) for e in no_prior_and_no_boostrapping.estimators_]).T","88e01a50":"# opening figure\nfig = plt.figure(figsize=[12,9], dpi=150)\n\n# title of the plot\nfig.suptitle('Bootstrapping and priors: impact of model components on result', verticalalignment='center')\n\n# second subplot\nplt.subplot(2, 2, 1)\n\n# let us plot the training data\nplt.plot(X, y, 'kx', label='Toy data')\nplt.title('Full model with priors and bootstrap', fontsize=12)\nplt.xlim(-0.5, 1.0); plt.ylim(-1.5, 1.5)\nplt.legend()\n\n# loop for plotting predictions of each head \nfor i in range(9):\n    plt.plot(x_grid, y_grid[:,i], linestyle='--', linewidth=1.5)\n\n# second subplot\nplt.subplot(2, 2, 2)\n\n# let us plot the training data\nplt.plot(X, y, 'kx', label='Toy data')\nplt.title('No bootrapping, but use of priors', fontsize=12)\nplt.xlim(-0.5, 1.0); plt.ylim(-1.5, 1.5)\nplt.legend()\n\n# loop for plotting predictions of each head \nfor i in range(9):\n    plt.plot(x_grid, y_grid_1[:,i], linestyle='--', linewidth=1.5)\n\n# second subplot\nplt.subplot(2, 2, 3)\n\n# let us plot the training data\nplt.plot(X, y, 'kx', label='Toy data')\nplt.title('No priors, but use of bootstrapping', fontsize=12)\nplt.xlim(-0.5, 1.0); plt.ylim(-1.5, 1.5)\nplt.legend()\n\n# loop for plotting predictions of each head \nfor i in range(9):\n    plt.plot(x_grid, y_grid_2[:,i], linestyle='--', linewidth=1.5)\n\n# second subplot\nplt.subplot(2, 2, 4)\n\n# let us plot the training data\nplt.plot(X, y, 'kx', label='Toy data')\nplt.title('Both bootstrapping and priors turned off', fontsize=12)\nplt.xlim(-0.5, 1.0); plt.ylim(-1.5, 1.5)\nplt.legend()\n\n# loop for plotting predictions of each head \nfor i in range(9):\n    plt.plot(x_grid, y_grid_3[:,i], linestyle='--', linewidth=1.5)\n    \n# showing\nplt.show();","326902b9":"The second part of the model is the trainable network,  with the same architecture as the prior, but with no fixed weights. It receives the same `net_input` as the prior.","c5781e90":"We can see how the prior network affects the final predictions, pushing model diversity into the ensemble. How this model diversity influences the final predictions?","8ea97386":"The plot shows the impact of priors and bootstrapping as drivers of uncertainty. At the upper left corner we see the full model, which shows a wide disagreement between ensemble members (uncertainty) on regions with no data. Generally, turning bootstrapping off will reduce uncertainty the  most (upper right corner), as opposed to turning priors off (lower left corner), but it can vary a bit across different seeds. With both bootstrapping and priors off, there's still a little disagreement between ensemble members due to random initialization of weights, but is a lot less compared to the other models.","a8ed86c8":"# 1. Starting with a simple example\n\nTo motivate this tutorial with a simple yet effective example, I'll borrow the following data generating process, shown in this [bayesian neural network paper](https:\/\/arxiv.org\/pdf\/1505.05424.pdf) by Blundell et. al (2015):\n\n$$ y = x + 0.3 \\cdot{} sin(2\u03c0(x + \\epsilon)) + 0.3 \\cdot{} sin(4 \\cdot{} \\pi \\cdot{}(x + \\epsilon)) + \\epsilon$$\n\nwhere $\\epsilon \\sim \\mathcal{N}(0, 0.02)$ and $x \\sim U(0.0,0.5)$:","9799275f":"In the example, we have sufficient data only in the interval **$[0.0, 0.5]$**, but we're interested in how the model would generalize to other regions of the space. As we move away from the data-rich areas, we expect that the uncertainty will increase: that's where our randomized prior functions model will help us. Let us build the model step-by-step using this simple dataset as our support.","5a48e636":"Trainable and prior interact via an `add` layer, so the trainable network can optimize its weights conditioned on the prior. We use a `Lambda` layer to scale the prior output, so we can implement **$\\beta$** in the `add` layer.","ddbac6cf":"The predictions of this net show that it implements a random function, like we wanted:","e4ad4351":"Then, we build the models. For models with no bootstrap, we set the option `bootstrap` to `False` in `BaggingRegressor`. For models with no prior, we use `get_regular_nn` instead of `get_randomized_prior_nn` for the `build_fn` option.","03ae94f1":"# 4. Effect of prior and bootstrapping\n\nThis model is really cool, but one might ask: what is **actually** driving uncertainty? The bootstrap? The prior? The neural network optimization process, with many local optima? \n\nIn order to draft some answers to these questions, let us conclude this post running three more models, comparing them to the full randomized prior functions model:\n\n1. Ensemble of networks with prior, but with bootstrapping turned off\n2. Ensemble of regular networks (no prior), but with bootstrapping turned on\n3. Ensemble of regular networks, and boostrapping turned off (no prior, and no bootstrap)\n\nWe start by defining a function `get_regular_nn` to implement a regular NN to use in models (2) and (3). I'll reuse the implementation of the trainable network:","6dfb6c85":"We fit the ensemble just like any other `sklearn` model. ","be4ec9be":"And finally, let us plot prior, trainable and final output together:","7a6cd9bf":"After fitting our ensemble, we can check what the trainable, prior and resultant networks output looks like, just as we did before:","5d69c02a":"The architecture is actually very simple. It's like traning two nets in parallel, but in this case, one of them is not trained! For me, this model is the simplest way to be bayesian: you just use vanilla networks, but combined in a smart way. \n\nLet us fit the model, and get the trainable output:","e737a00a":"Let us check the final architecture:","dad8efd2":"We can see that in data rich regions, we have strong agreement across members of the ensemble. However, the further we are from the data, disagreement and larger uncertainties arise. The uncertainty estimate is principled; in the paper, the authors show (for the linear case) that each member of the ensemble is actually a sample from the real posterior. To use the model to make decisions, a simple and good policy is to randomly select one member of the ensemble and let it take the wheel for the next round or episode (which is equivalent to Thompson Sampling).","6c55b3bd":"As we can see, the trainable network (dark red) compensated the prior network (blue), reasonably fitting our data (purple). However, in regions away from our data, the prior network will dominate, fulfilling its uncertainty-driver role. \n\nLet us wrap everything up in the function `get_randomized_prior_nn` below, so that we're ready to move to the next step: Bootstrapped Ensembles!","d94e2438":"# 2. Randomized Prior Functions\n\nTo understand what problem randomized prior functions solve, let us recap [\"Deep Exploration via Bootstrapped DQN\"](https:\/\/arxiv.org\/abs\/1602.04621). This approach trains an ensemble of models, each on a bootstrap sample of the data (or a single model with many bootstrapped heads), and approximates uncertainty with the prediction variance across the ensemble. The bootstrap acts as an (actually very good) approximation to the posterior distribution, such that each member of the ensemble can be seen as a *sample* of the true posterior. Having a good approximation to the posterior gives great benefits for exploration in reinforcement learning: as the agent now knows uncertainty, it can prioritize decisions to maximize both learning and rewards. However, even if the approach works well in practice, there was still something missing: the uncertainty comes only from the data, unlike other bayesian approaches where it also comes from a *prior* distribution, which would help the agent make decisions in contexts where there is no training data.  \n\nTo address this shortcoming, [Osband et. al](https:\/\/papers.nips.cc\/paper\/8080-randomized-prior-functions-for-deep-reinforcement-learning.pdf) proposed a simple yet effective model. It consists of two networks, used in parallel: the *trainable* network **$f$** and the *prior* network **$p$**, which are combined to form the final output **$Q$**, through a scaling factor **$\\beta$**:\n\n**$$\\large Q = f + \\beta\\cdot p$$**\n\nLet us start with the first part of the model, the prior network. First, we initialize the network with an `Input`, and a `prior_net`, for which we enforce fixed weights by setting the parameter `trainable` to `False`. The architecture of the network, and the `glorot_normal` initiatilization have a deep tie to the family of functions that this prior will implement. You're welcome to try different settings!","b5722df7":"# Introduction to Randomized Prior Functions\n\nBayesian deep learning has been receiving a lot of attention in the ML community, with many attempts to quantify uncertainty in neural networks. [Variational Inference](https:\/\/arxiv.org\/abs\/1505.05424), [Monte Carlo Dropout](https:\/\/arxiv.org\/abs\/1506.02142) and [Bootstrapped Ensembles](https:\/\/arxiv.org\/abs\/1602.04621) are some examples of research in this area. \n\nRecently, the paper [\"Randomized Prior Functions for Deep Reinforcement Learning\"](https:\/\/papers.nips.cc\/paper\/8080-randomized-prior-functions-for-deep-reinforcement-learning.pdf), presented at NeurIPS 2018, proposes a simple yet effective model for capturing uncertainty, by building an ensemble of bootsrapped neural networks coupled with **randomized prior functions**: randomly initialized networks that aim to dictate the model's behavior in regions of the space where there is no training data. \n\nIn this Notebook, I'll build a simple implementation of this model using `keras` and show some cool visuals to get an intuitive feel of how it works. This is the first of a series of posts where I'll explore this model and apply it in some cool use cases. \n\n# Why is this relevant?\n\nMany industry problems require methods that actively **experiment and make decisions** instead of just mapping a function from explanatory variables to target. For instance, a dynamic pricing model does not know optimal prices in advance; it needs to experiment and discover these prices through good decision making, with a healthy dose of learning user behavior (exploration) and using this knowledge to maximize end results, for instance, by finding the best promotions for each user (exploitaion). Given this scenario, reinforcement learning and contextual bandit methods present themselves as a strong alternative to supervised learning, given that they show this kind of active behavior we need.\n\nDespite the fact that the majority of deep learning research has evolved outside of Bayesian (or even statistical) analysis, decision theory shows that the only [admissible decision rules are Bayesian](https:\/\/en.wikipedia.org\/wiki\/Decision_theory) (any decision rule that is not Bayesian can be improved by some Bayesian alternative). In this post, we make use of one of the most advanced bayesian techniques for reinforcement learning, and check how it builds uncertainty estimates to incentivize healthy exploration.","ab3b3d0a":"# 3. Bootstrapped Ensembles\n\nAlthough already very cool, our model is not complete yet, as with only one network we just have one sample of the posterior distribution. We need to generate more samples if we want to have a good approximation of the true posterior. This is where a bootstrapped ensemble comes in, and it's actually very simple to build it.\n\nAs we have the generator function `get_randomized_prior_nn` for our model, we can just generate many versions of it and train them using the BaggingRegressor from `sklearn`. This effectively performs the bootstrapping we need, in just a few lines of code:\n","edd82c05":"# 5. Conclusion\n\nIn this tutorial, we built one of the state-of-the-art models for deep bayesian learning: a neural network with a randomized prior function, in a simple `keras` implementation. The model is pretty cool and does a good job in estimating uncertainty. To further test the model and apply it to different areas, I'll try to apply it in a bandit problem next. Stay tuned!\n\nHope you liked the tutorial!!!! Any feedback is appreciated!\n\nSee you soon!"}}