{"cell_type":{"47f4a80b":"code","2019d650":"code","f98d2c08":"code","775f533f":"code","c85338ed":"code","28ab6f85":"code","5c4af7b5":"code","fb3c19ad":"code","d2ba08a5":"code","ebe9ffb8":"code","620a3d38":"code","ed215a52":"code","449fee18":"code","dabb2ae8":"code","e0c03992":"code","f08cff05":"code","dadc6726":"code","9b72edb4":"code","5af08e91":"code","c891fa56":"code","dd2468ed":"code","fd169e47":"code","a32720c8":"code","964b33c1":"code","d5981af2":"code","c4e1d0f9":"code","4d80f320":"code","8c15f17c":"code","f7c071f7":"code","e7bb23f0":"code","6d2c4a6c":"code","4d197440":"code","9ec8ed5b":"code","8a71d412":"code","b0d691d7":"code","42b6d3ef":"code","8758af8b":"code","fb6f0014":"code","521d1cc4":"code","b8657257":"markdown","6f267aa0":"markdown","31b3f5ac":"markdown","ebab4c72":"markdown","673cb4c3":"markdown","8961f86c":"markdown","c73867b4":"markdown","520e470b":"markdown","82adcd31":"markdown","de67e231":"markdown","141abbbf":"markdown","581f69e6":"markdown","aee55357":"markdown","8026c85b":"markdown","5dc613cd":"markdown","8a786364":"markdown","2ac2b41c":"markdown","9cbd068f":"markdown","079a5727":"markdown","57f6141c":"markdown","083afd07":"markdown","847de6cc":"markdown","31f6c37c":"markdown","ed47ab2f":"markdown","d7096436":"markdown","b992ad4f":"markdown","7a3b6deb":"markdown","f6a6bdac":"markdown","ea3a5148":"markdown","f55ad047":"markdown"},"source":{"47f4a80b":"# Import the packages required to perform data analysis\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n# Affects the appearence of matplotlib plots\nsns.set_palette(\"husl\")\nsns.set_style(\"whitegrid\")\n\npd.options.display.max_rows=None\npd.options.display.max_columns=None\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2019d650":"# Import the train dataset\ndf_train = pd.read_csv('..\/input\/train.csv')\n# Import the test dataset\ndf_test = pd.read_csv('..\/input\/test.csv')\n# Save PassengerId for final submission\npassengerId = df_test.PassengerId\n# Join the train and test datasets for preprocessing\ndf_full= pd.concat([df_train, df_test], axis=0, ignore_index=True, sort=False)\n# Create indices to separate data later on\ntrain_idx = len(df_train)\ntest_idx = len(df_full) - len(df_test)\n# Make a copy of df_full\ndf = df_full.copy()\n# Eyeball the data\ndf.tail()","f98d2c08":"# Study the features of the dataframe\ndf.info()","775f533f":"# Find missing values in each feature\ndf.isnull().sum()","c85338ed":"# Get a list of the feature names\ndf.columns.values","28ab6f85":"# Building the logic to write the get_title function\nprint(df['Name'][0])\nprint(df['Name'][0].split(',')[1].split('.')[0].strip())","5c4af7b5":"# Function to extract titles from names\ndef get_title(name):\n    if '.' in name:\n        return name.split(',')[1].split('.')[0].strip()\n    else:\n        return 'unknown'","fb3c19ad":"# Applying the function get_title() on the feature 'Name' using a list comprehension\ntitles = pd.Series([x for x in df.Name.apply(lambda x : get_title(x))])\ntitle_count = titles.value_counts().sort_index()\ntitle_count","d2ba08a5":"# Group the titles that have a low frequency of occurence into an array\nrare_titles = title_count[title_count <= 10]\nrare_titles.index.values","ebe9ffb8":"# Next, group the titles to be engineered\nrare_titles_Royalty = ['Lady', 'the Countess', 'Don', 'Dona', 'Jonkheer', 'Sir']\nrare_titles_Officer = ['Capt', 'Col', 'Major']\nrare_titles_Mr = ['Rev']\nrare_titles_Mrs = ['Mme']\nrare_titles_Miss = ['Mlle', 'Ms']\nrare_titles_Doctor = ['Dr']\n\n# Create a function to Normalize the titles\ndef normalize_titles(df):\n    title = df['Title']\n    \n    if title in rare_titles_Royalty:\n        return 'Royalty'\n    if title in rare_titles_Officer:\n        return 'Officer'\n    if title in rare_titles_Mr:\n        return 'Mr'\n    elif title in rare_titles_Mrs:\n        return 'Mrs'\n    elif title in rare_titles_Miss:\n        return 'Miss'\n    elif title in rare_titles_Doctor:\n        if df['Sex'] == 'male':\n            return 'Mr'\n        else: \n            return 'Mrs'\n    else: \n        return title","620a3d38":"# Create a feature named Title in the train dataframe\ndf['Title'] = titles\ndf['Title'].describe()","ed215a52":"# Apply the engineered titles to the Title column\ndf['Title'] = df.apply(normalize_titles, axis = 1)\ndf['Title'].describe()","449fee18":"# Eyeball the dataframe\ndf.head()","dabb2ae8":"# Analysing the Age column\nprint(df['Age'].describe(), \"\\n\")\nprint(\"Number NaN values:\", df['Age'].isnull().sum())","e0c03992":"# Group by Sex, Pclass, and Title \ngrouped = df.groupby(['Sex', 'Pclass', 'Title'])\ngrouped.Age.median()","f08cff05":"# Apply the grouped median value on the Age NaN\ndf.Age = grouped.Age.transform(lambda x: x.fillna(x.median(), inplace=False))","dadc6726":"# Analysing the feature named 'Cabin'\nprint(df.Cabin.describe(), \"\\n\")\nprint('Number of NaN values:', df.Cabin.isnull().sum())","9b72edb4":"# Drop feature Cabin since it has few values and is unlikely to impact our analysis\ndf.drop(['Cabin'], axis=1, inplace=True)\n# Fill Embarked NaN values with the most frequently embarked location\ndf.Embarked = df.Embarked.fillna(df.Embarked.mode().values[0])\n# Fill Fare NaN values with median fare\ndf.Fare = round(df.Fare.fillna(df.Fare.median()))","5af08e91":"# View changes\ndf.info()","c891fa56":"# Create feature FamilySize\ndf['FamilySize'] = df['SibSp'] + df['Parch'] + 1","dd2468ed":"# Checkpoint-1\ndf_before_dummies = df.copy()\n# Eyeball the dataframe\ndf.head()","fd169e47":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,4))\n\nsurvived = 'Survived'\ndied = 'Died'\n\nwomen = df[df.Sex=='female']\nmen = df[df.Sex=='male']\n\nax = sns.distplot(women[women.Survived==1].Age.dropna(), ax=axes[0], bins = 20, label=survived, kde=False, color='c')\nax = sns.distplot(women[women.Survived==0].Age.dropna(), ax=axes[0], bins = 40, label=died, kde=False, color='r')\nax.legend()\nax.set_title('Female')\n\nax = sns.distplot(men[men.Survived==1].Age.dropna(), ax=axes[1], bins = 20, label=survived, kde=False, color='b')\nax = sns.distplot(men[men.Survived==0].Age.dropna(), ax=axes[1], bins = 40, label=died, kde=False, color='r')\nax.legend()\nax.set_title('Male')\n","a32720c8":"# Create a catplot\nax = sns.catplot(x='Pclass', hue='Survived', col='Title', data=df, palette='inferno',  \n                 col_wrap=3, height=4, aspect=1, kind='count')","964b33c1":"df.Sex = df.Sex.map({'male':0, 'female':1})","d5981af2":"# Create dummy variables for categorical features\nPclass_dummies = pd.get_dummies(df.Pclass, prefix='Pclass')\nTitle_dummies = pd.get_dummies(df.Title, prefix='Title')\nEmbarked_dummies = pd.get_dummies(df.Embarked, prefix='Embarked')","c4e1d0f9":"# Concatenate dummy columns with main dataset\ndf_dummies = pd.concat([df, Pclass_dummies, Title_dummies, Embarked_dummies], axis=1)\n# Drop categorical features\ndf_dummies.drop(columns={'PassengerId', 'Name', 'Ticket', 'Title', 'Embarked', 'Pclass', 'SibSp', 'Parch'}, inplace=True)\ndf_dummies.columns.values","4d80f320":"# Re-order the columns in the dataframe\ncols_reordered = ['Fare', 'Sex', 'Age', 'FamilySize', \n                  'Pclass_1', 'Pclass_2', 'Pclass_3', 'Title_Master',\n                  'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Officer',\n                  'Title_Royalty', 'Embarked_C', 'Embarked_Q', 'Embarked_S',\n                  'Survived']\n\n# Re-ordered columns and checkpoint-2\ndf_preprocessed = df_dummies[cols_reordered]\n\n# Convert features 'Fare' and 'Age' to datatype int\ndf_preprocessed.Age = df_preprocessed.Age.astype(int)\ndf_preprocessed.Fare = df_preprocessed.Fare.astype(int)\ndf_preprocessed.tail()","8c15f17c":"# Import the packages required for machine learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import accuracy_score","f7c071f7":"# Create the train dataset\ntrain_df = df_preprocessed[:train_idx].copy()\nprint(\"Number of records:\",len(train_df))\n# Convert the feature Survived to int\ntrain_df.Survived = train_df.Survived.astype(int)\ntrain_df.tail()","e7bb23f0":"# Create the test dataset\ntest_df = df_preprocessed[test_idx:].copy()\ntest_df.drop(['Survived'], axis=1, inplace=True)\nprint(\"Number of records:\",len(test_df))\ntest_df.tail()","6d2c4a6c":"# Create the targets\ntargets = train_df.Survived.values\nprint(\"Length of the target array:\",len(targets))\nprint(targets.shape)\n\n# Create the inputs\ninputs = train_df.iloc[:, :-1]\nprint(\"Length of the input dataframe:\",len(inputs))\nprint(inputs.shape)","4d197440":"X_train, X_test, y_train, y_test = train_test_split(inputs, targets, test_size=418, shuffle=True, \n                                                   random_state=20, stratify=targets)","9ec8ed5b":"# Instantiate the model\nxgb = XGBClassifier()\n# Fit the data\nxgb.fit(X_train, y_train)\n# Predict the values\nxgb_predictions = xgb.predict(X_train)\n# Display the accuracy score of the train dataset\nprint(\"Accuracy score: %.2f%%\" % (round(xgb.score(X_train, y_train) * 100)))","8a71d412":"kfold = StratifiedKFold(n_splits=10, random_state=20, shuffle=True)\nresults = cross_val_score(xgb, X_train, y_train, cv=kfold)\nprint('Accuracy score \\nmean: %.2f%% \\nsd: %.2f%%' % (results.mean()*100, results.std()*100))","b0d691d7":"# Plot feature importance\nfig, axes = plt.subplots(figsize=(10, 8))\nplot_importance(xgb, ax=axes)\nplt.show()","42b6d3ef":"# fit model on all training data\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\n\n# make predictions for test data and evaluate\ny_pred = model.predict(X_test)\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n\n# Fit model using each importance as a threshold\nthresholds = sorted(model.feature_importances_)\nfor thresh in thresholds:\n    # select features using threshold\n    selection = SelectFromModel(model, threshold=thresh, prefit=True)\n    select_X_train = selection.transform(X_train)\n    # train model\n    selection_model = XGBClassifier()\n    selection_model.fit(select_X_train, y_train)\n    # eval model\n    select_X_test = selection.transform(X_test)\n    y_pred = selection_model.predict(select_X_test)\n    predictions = [round(value) for value in y_pred]\n    accuracy = accuracy_score(y_test, predictions)\n    print(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], accuracy*100.0))","8758af8b":"# instantiate the model\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\n# select features using threshold\nselection = SelectFromModel(model, threshold=0.002, prefit=True)\nselection_model = XGBClassifier()\nselect_X_train = selection.transform(X_train)\n# train model\nselection_model.fit(select_X_train, y_train)\n# test model\nselect_X_test = selection.transform(test_df)\ny_pred = selection_model.predict(select_X_test)\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_test.shape[1], accuracy*100.0))","fb6f0014":"test_df['PassengerId'] = passengerId\ntest_df.reset_index(inplace=True)\ntest_df.drop(['index'], axis=1, inplace=True)\nsubmission_df = pd.concat([test_df.PassengerId, pd.DataFrame(y_pred)], axis=1)\nsubmission_df.columns = ['PassengerId', 'Survived']\nsubmission_df['PassengerId'] = passengerId\nsubmission_df.tail()","521d1cc4":"submission_df.to_csv(\"..\/working\/submission_titanic.csv\", index = False)","b8657257":"**1. Looking at Sex, Age and Survival**","6f267aa0":"## Implementing the model on test data","31b3f5ac":"### Analysing features 'Embarked' and 'Fare'","ebab4c72":"## Create the inputs and targets","673cb4c3":"Cabin is a categorical feature and the total number of missing values in this feature is too high to be imputed successfully because the dataset does not provide sufficient information to impute this data. Therefore, we shall drop this column from our final preprocessed dataset.","8961f86c":"### Create feature 'FamilySize'","c73867b4":"By grouping 'Age' with ['Sex', 'Pclass', 'Title'] and then taking the median value, we are able to get values that better reflect the original values missing rather than simply taking the median of the of the feature 'Age'. Then use the transform with groupby to apply the median values to the feature 'Age' as shown below.","520e470b":"The above histogram shows that a higher number of women survived when compared to men. Women between 15 - 35 and men between 25 - 35 have a higher chance of survival.","82adcd31":"In this section, we take a look at the relationship between the features in our dataset and their impact on survival.","de67e231":"## Feature Selection with XGBoost Feature Importance Scores","141abbbf":"The 'Age' feature has 263 missing values. These values have to be imputed before using this feature in any analyses. The best way to imputed these missing values is to do the following:","581f69e6":"**2. Analysing Pclass and Survival**","aee55357":"### Create a checkpoint-1","8026c85b":"This kernel has been heavily influenced by other kernels, articles and discussions that I came across while figuring out how to proceed analysing this dataset.","5dc613cd":"## Create a feature named 'Title' from the categorical feature 'Name'","8a786364":"The features Age, Cabin and Embarked have missing values. Ignore the missing values in the feature Survived because it shows the missing values from the test dataset.","2ac2b41c":"### pd .get_dummies()","9cbd068f":"## Exploratory Data Analysis","079a5727":"## Split the data","57f6141c":"### Moving on, we analyse the feature 'Cabin'","083afd07":"### Next, we analyse the feature 'Age'","847de6cc":"## Model - XGBoost","31f6c37c":"# Predicting Survival Rate from the Titanic Dataset - Preprocessing","ed47ab2f":"## Feature Engineering","d7096436":"The plot above shows that Pclass 1 and Pclass 2 passengers are more likely to survive than Pclass 3 passengers. Also, women survived more than men.","b992ad4f":"# Predicting Survival Rate from the Titanic Dataset - Machine Learning","7a3b6deb":"Next, check the accuracy score of the training model using cross-validation.","f6a6bdac":"### Convert the feature 'Sex' from categorical data to numeric data","ea3a5148":"## Create a checkpoint-2","f55ad047":"#### df_preprocessed is a cleaned dataframe which can be used to fit  machine learning models to predict the survival of passengers."}}