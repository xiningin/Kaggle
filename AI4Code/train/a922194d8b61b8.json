{"cell_type":{"c5c750c7":"code","332c4df3":"code","bfdf0592":"code","b347fa03":"code","1d08ef98":"code","c16bd38f":"code","0f730e93":"code","da768b2c":"code","807a7fb8":"code","a055536d":"code","a6155585":"code","89b4ea0f":"code","39598de5":"code","48a5cb85":"code","79c9e9db":"code","d6de5aa6":"code","b2428d56":"code","bd9ca69e":"code","5e8d8400":"code","95171066":"code","60ac564d":"code","ca534cc1":"code","81ab9332":"code","151548e1":"code","aa6113aa":"code","be5a2243":"code","138933a3":"code","1f5faa16":"code","bd62dcaa":"code","3ad11e5e":"code","8f462d0c":"code","fcd756c7":"code","5b476101":"code","7c36e2aa":"code","d923fb53":"code","3d5f9070":"code","109d8b33":"code","a25e92a7":"code","a2f45216":"code","075b0cf2":"markdown"},"source":{"c5c750c7":"!pip install ..\/input\/sacremoses\/sacremoses-master\/ > \/dev\/null\n!pip install ..\/input\/transformers\/transformers-master\/ > \/dev\/null","332c4df3":"import os\nimport sys\nimport glob\nimport torch\n\nimport transformers\nimport numpy as np\nimport pandas as pd\nimport math\n\nimport torch\nfrom transformers import BertModel\nimport torch, random, os, multiprocessing, glob, numpy as np, pandas as pd\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.utils import data\nfrom transformers import (\n    BertTokenizer, BertModel, BertForSequenceClassification, \n    WEIGHTS_NAME, CONFIG_NAME\n)\nfrom tqdm import tqdm_notebook as tqdm\nfrom transformers import AdamW\nfrom transformers.optimization import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n\ntransformers.__version__","bfdf0592":"train = pd.read_csv(\"..\/input\/google-quest-challenge\/train.csv\",)\ntest = pd.read_csv(\"..\/input\/google-quest-challenge\/test.csv\",)\n\ntarget_cols = ['question_asker_intent_understanding', 'question_body_critical', \n               'question_conversational', 'question_expect_short_answer', \n               'question_fact_seeking', 'question_has_commonly_accepted_answer', \n               'question_interestingness_others', 'question_interestingness_self', \n               'question_multi_intent', 'question_not_really_a_question', \n               'question_opinion_seeking', 'question_type_choice',\n               'question_type_compare', 'question_type_consequence',\n               'question_type_definition', 'question_type_entity', \n               'question_type_instructions', 'question_type_procedure', \n               'question_type_reason_explanation', 'question_type_spelling', \n               'question_well_written', 'answer_helpful',\n               'answer_level_of_information', 'answer_plausible', \n               'answer_relevance', 'answer_satisfaction', \n               'answer_type_instructions', 'answer_type_procedure', \n               'answer_type_reason_explanation', 'answer_well_written']","b347fa03":"# From the Ref Kernel's\nfrom math import floor, ceil\n\ndef _get_masks(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\ndef _get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    \n    if len(tokens) > max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n        \n    segments = []\n    first_sep = True\n    current_segment_id = 0\n    \n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            if first_sep:\n                first_sep = False \n            else:\n                current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\ndef _get_ids(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    \n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids\n\ndef _trim_input(title, question, answer, max_sequence_length=512, t_max_len=30, q_max_len=239, a_max_len=239):\n    \n    #293+239+30 = 508 + 4 = 512\n    t = tokenizer.tokenize(title)\n    q = tokenizer.tokenize(question)\n    a = tokenizer.tokenize(answer)\n    \n    t_len = len(t)\n    q_len = len(q)\n    a_len = len(a)\n\n    if (t_len+q_len+a_len+4) > max_sequence_length:\n        \n        if t_max_len > t_len:\n            t_new_len = t_len\n            a_max_len = a_max_len + floor((t_max_len - t_len)\/2)\n            q_max_len = q_max_len + ceil((t_max_len - t_len)\/2)\n        else:\n            t_new_len = t_max_len\n      \n        if a_max_len > a_len:\n            a_new_len = a_len \n            q_new_len = q_max_len + (a_max_len - a_len)\n        elif q_max_len > q_len:\n            a_new_len = a_max_len + (q_max_len - q_len)\n            q_new_len = q_len\n        else:\n            a_new_len = a_max_len\n            q_new_len = q_max_len\n            \n            \n        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\"%(max_sequence_length, (t_new_len + a_new_len + q_new_len + 4)))\n        \n        t = t[:t_new_len]\n        q = q[:q_new_len]\n        a = a[:a_new_len]\n    \n    return t, q, a\n\ndef _convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    \n    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n\n    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n    input_masks = _get_masks(stoken, max_sequence_length)\n    input_segments = _get_segments(stoken, max_sequence_length)\n\n    return [input_ids, input_masks, input_segments]\n\ndef compute_input_arays(df, columns, tokenizer, max_sequence_length):\n    \n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n        t, q, a = _trim_input(t, q, a, max_sequence_length)\n        ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n    return [\n        torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n        torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n        torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),\n    ]\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","1d08ef98":"tokenizer = BertTokenizer.from_pretrained(\"..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased-vocab.txt\")\ninput_categories = list(train.columns[[1,2,5]]); input_categories","c16bd38f":"%%time\noutputs = compute_output_arrays(train, columns = target_cols)\ninputs = compute_input_arays(train, input_categories, tokenizer, max_sequence_length=512)\ntest_inputs = compute_input_arays(test, input_categories, tokenizer, max_sequence_length=512)","0f730e93":"inputs[0]","da768b2c":"outputs[0]","807a7fb8":"test_inputs[0]","a055536d":"%%time\nlengths = np.argmax(inputs[0] == 0, axis=1)\nlengths[lengths == 0] = inputs[0].shape[1]\ny_train_torch = torch.tensor(train[target_cols].values, dtype=torch.float32)","a6155585":"lengths","89b4ea0f":"min(lengths), max(lengths)","39598de5":"dataset = data.TensorDataset(inputs[0], #input_ids\n                             inputs[1], #input_masks\n                             inputs[2], #input_segments\n                             y_train_torch, #targets,\n                             lengths, #lengths of each seq\n                            )","48a5cb85":"next(iter(dataset)) # #input_ids, #input_masks, #input_segments, #targets, #lengths of each seq","79c9e9db":"BATCH_SIZE = 8\ntrain_loader = data.DataLoader(dataset,\n                               batch_size=BATCH_SIZE,\n                               shuffle=True,\n                               drop_last=True,\n                              )","d6de5aa6":"next(iter(train_loader)) #input_ids, input_masks, input_segments, targets, lengths","b2428d56":"y_train_torch[0], len(y_train_torch[0])","bd9ca69e":"bert_model = 'bert-base-uncased'\ndo_lower_case = 'uncased' in bert_model\ndevice = 'cpu'\n\noutput_model_file = 'bert_pytorch.bin'","5e8d8400":"from transformers import BertConfig\nbert_model_config = '..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased\/bert_config.json'\nbert_config = BertConfig.from_json_file(bert_model_config)\nbert_config.num_labels = 30","95171066":"model = BertForSequenceClassification.from_pretrained('..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased\/', config=bert_config);\nmodel.zero_grad();\nmodel.to(device);\n'''\nx = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\nx = tf.keras.layers.Dropout(0.2)(x)\nout = tf.keras.layers.Dense(30, activation=\"sigmoid\", name=\"dense_output\")(x)\n''';\n","60ac564d":"model.classifier","ca534cc1":"model","81ab9332":"epochs = 1 # had run for 3 epochs\nACCUM_STEPS = 1\noptimizer = AdamW(model.parameters(), lr=3e-5, eps=4e-5)\nscheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=0.05, num_training_steps= epochs*len(train_loader)\/\/ACCUM_STEPS)\nmodel = model.train();","151548e1":"for name, param in model.named_parameters():\n    if param.requires_grad:\n        print(name)","aa6113aa":"from torch.nn import CrossEntropyLoss, MSELoss, BCELoss\ntr_loss = 0\nimport gc\ncriterion = nn.BCEWithLogitsLoss()","be5a2243":"%%time\ntemp = False\nif temp:\n    for _ in tqdm(range(epochs)):\n        model.train();\n        tk0 = tqdm(enumerate(train_loader))\n        for j, batch in tk0:\n\n            input_ids, input_masks, input_segments, y, _ = batch\n            input_ids, input_masks, input_segments, y = input_ids.to(device), input_masks.to(device), input_segments.to(device), y.to(device)\n\n            outputs = model(input_ids = input_ids.long(), \n                            labels = None, \n                            attention_mask = input_masks, \n                            token_type_ids = input_segments\n                           )\n            logits = outputs[0] #output preds\n            loss = criterion(logits, y) #sigmoid is taken\n            del input_ids, input_masks, input_segments, y\n            gc.collect()\n\n            if ACCUM_STEPS > 1:\n                loss = loss \/ ACCUM_STEPS\n\n            loss.backward()\n            tr_loss += loss.item()\n\n            optimizer.step()\n            scheduler.step() # Update learning rate schedule # See latest docs for the order\n            optimizer.zero_grad()\n            global_step += 1\n            gc.collect()\n        torch.cuda.empty_cache()\nelse:\n    print(\"Not Running As of Now..\")\ntorch.cuda.empty_cache()\ngc.collect()","138933a3":"output_dir= \".\"\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n\ntorch.save(optimizer.state_dict(), os.path.join(output_dir, 'optimizer.pt'))\ntorch.save(scheduler.state_dict(), os.path.join(output_dir, 'scheduler.pt'))","1f5faa16":"del dataset, train_loader, lengths, optimizer, scheduler\nimport gc\ngc.collect(), torch.cuda.empty_cache()","bd62dcaa":"test_inputs","3ad11e5e":"%%time\nsequences = np.array(test_inputs[0])\nlengths = np.argmax(sequences == 0, axis=1)\nlengths[lengths == 0] = sequences.shape[1]","8f462d0c":"lengths","fcd756c7":"dataset = data.TensorDataset(test_inputs[0],\n                             test_inputs[1],\n                             test_inputs[2],\n                             torch.from_numpy(lengths)\n                            )\n\ntest_loader = data.DataLoader(dataset, \n                               batch_size=BATCH_SIZE,  \n                               shuffle=False, \n                               drop_last=False\n                             )","5b476101":"next(iter(test_loader))","7c36e2aa":"model.eval();","d923fb53":"test_preds = np.zeros((len(test), 30)); test_preds.shape","3d5f9070":"if temp:\n    tk0 = tqdm(test_loader)\n    for i, x_batch in enumerate(tk0):\n        with torch.no_grad():\n            outputs = model(input_ids = x_batch[0].to(device), \n                            labels = None, \n                            attention_mask = x_batch[1].to(device),\n                            token_type_ids = x_batch[2].to(device),\n                           )\n            logits = outputs[0]\n            test_preds[i*BATCH_SIZE : (i+1)*BATCH_SIZE] = logits.detach().cpu().squeeze().numpy()\ntest_pred = torch.sigmoid(torch.tensor(test_preds)).numpy()","109d8b33":"test_pred","a25e92a7":"submission = pd.DataFrame.from_dict({\n    'qa_id': test['qa_id']\n})\n\nfor i in range(30):\n    submission[target_cols[i]] = test_pred[:, i]\n    \nsubmission.to_csv(os.path.join(output_dir, 'submission.csv'), index=False)\nsubmission.head()","a2f45216":"ls","075b0cf2":"# have attempted converting this kernel to PyTorch\n- https:\/\/www.kaggle.com\/adityaecdrid\/bert-base-tf2-0-minimalistic\n\n- **But the issue is i am not able to get that score, rather my kernel score is .219. I have tried my best to figure out that's going wrong but i am not able to do-so. \n- So Asking for help from the community as to share what did i do wrong?**\n\nThanks a lot for your time,\nAditya."}}