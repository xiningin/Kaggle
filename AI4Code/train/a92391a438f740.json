{"cell_type":{"85c2a75b":"code","8ef4e969":"code","d0381231":"code","9d361881":"code","f88adbce":"code","ea8f45ce":"code","7aa39e29":"code","5fb67696":"code","9850e56f":"code","7e882151":"code","d0860d10":"code","cb36ffd1":"code","e01e03e8":"code","dd1b9693":"code","ed87e609":"code","cac37bac":"code","70e20dfb":"code","2ef1f9f1":"code","202138d6":"code","dd71cf14":"code","bea116d9":"code","255069f9":"code","9409685c":"code","838226f3":"code","dbe8ff9d":"markdown"},"source":{"85c2a75b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8ef4e969":"#Read the data and remain the columns there\ntitanic = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntitanic.head()","d0381231":"#Check the proportion of dead people\nprint('Pclass: ')\nA = titanic.groupby(['Pclass'])['Survived'].value_counts(normalize=True)\nfor i in range(1,4):\n    print((i, A[i][0]))\nprint('Parch: ')\nB = titanic.groupby(['Parch'])['Survived'].value_counts(normalize=True)\nfor i in range(0,7):\n    print((i, B[i][0]))\nprint('SibSp: ')\nC = titanic.groupby(['SibSp'])['Survived'].value_counts(normalize=True)\nfor i in range(0,6):\n    print((i, C[i][0]))\nprint('Embarked: ')\nD = titanic.groupby(['Embarked'])['Survived'].value_counts(normalize=True)\nfor character in ['C','Q','S']:\n    print((character, D[character][0]))\nprint('Age: ')\nE = titanic.groupby(['Sex'])['Survived'].value_counts(normalize=True)\nfor name in ['male','female']:\n    print((name, E[name][0]))","9d361881":"# Import all of the libraries\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\nimport itertools\nimport scipy\nfrom scipy.cluster import hierarchy \nfrom sklearn.tree import DecisionTreeClassifier\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import DBSCAN \nfrom sklearn.datasets.samples_generator import make_blobs \nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom mlxtend.classifier import StackingClassifier\nfrom sklearn import metrics\nfrom mlxtend.plotting import plot_learning_curves\nfrom mlxtend.plotting import plot_decision_regions\nfrom xgboost import XGBClassifier","f88adbce":"titanic['Title'] = titanic['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\nfor i in range(0,len(titanic)):\n    if titanic['Title'][i] != 'Mr' and titanic['Title'][i] != 'Miss' and titanic['Title'][i] != 'Mrs' and titanic['Title'][i] != 'Master':\n        titanic['Title'].replace(titanic['Title'][i], 'Others', inplace=True)\n    else: continue\ntitanic['Title'].value_counts()","ea8f45ce":"bins_age = np.linspace(min(titanic[\"Age\"]), max(titanic[\"Age\"]), 6)\ngroup_names_age = ['Age1','Age2','Age3','Age4','Age5']\nbins_fare = np.linspace(min(titanic[\"Fare\"]), max(titanic[\"Fare\"]), 6)\ngroup_names_fare = ['Fare1','Fare2','Fare3','Fare4','Fare5']\n\ntitanic['Sex'].replace(to_replace=['female','male'],value=[0,1],inplace=True)\ntitanic['Embarked'].replace(to_replace=['C','Q','S'],value=[0,1,2],inplace=True)\ntitanic['Age-binned'] = pd.cut(titanic['Age'], bins_age, labels=group_names_age, include_lowest=True )\ntitanic['Fare-binned'] = pd.cut(titanic['Fare'], bins_fare, labels=group_names_fare, include_lowest=True)\n\ntitanic.head()","7aa39e29":"TA = titanic.groupby(['Title'])['Age'].mean()\nfor name in ['Mr','Miss','Mrs','Master','Others']:\n    for i in range(0,len(titanic)):\n        if titanic['Title'][i] == name:\n            if titanic['Age'][i] == np.nan:\n                titanic['Age'].replace(titanic['Age'][i], TA[name])\n            else: continue\n        else: continue\ntitanic['Age'].isna().value_counts()","5fb67696":"dummy_age = pd.get_dummies(titanic['Age-binned'])\ndummy_fare = pd.get_dummies(titanic['Fare-binned'])\ndummy_embarked = pd.get_dummies(titanic['Embarked'])\ntitanic = pd.concat([titanic, dummy_title, dummy_age, dummy_fare, dummy_embarked], axis=1)\ntitanic.head()","9850e56f":"x_data2 = titanic[['Pclass','Parch','SibSp','Sex','Embarked','Age1','Age2','Age3','Age4','Age5','Fare1','Fare2','Fare3','Fare4','Fare5']]\ny_data = titanic['Survived']\nx_data = x_data2.fillna(2)","7e882151":"x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.4, random_state=0)\nKs = 10\nmean_acc = np.zeros((Ks-1))\nstd_acc = np.zeros((Ks-1))\nConfustionMx = [];\nfor n in range(1,Ks):\n    \n    #Train Model and Predict \n    neigh = KNeighborsClassifier(n_neighbors = n).fit(x_train,y_train)\n    yhat=neigh.predict(x_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n\n    \n    std_acc[n-1]=np.std(yhat==y_test)\/np.sqrt(yhat.shape[0])\nprint( \"The best accuracy was with\", mean_acc.max(), \"with k=\", mean_acc.argmax()+1)\nprint(x_data.shape)","d0860d10":"test_titanic = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","cb36ffd1":"#create the title part\ntest_titanic['Title'] = test_titanic['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\nfor i in range(0,len(test_titanic)):\n    if test_titanic['Title'][i] != 'Mr' and test_titanic['Title'][i] != 'Miss' and test_titanic['Title'][i] != 'Mrs' and test_titanic['Title'][i] != 'Master':\n        test_titanic['Title'].replace(test_titanic['Title'][i], 'Others', inplace=True)\n    else: continue\ntest_titanic['Title'].value_counts()\n\n#make the binning\nbins_age_test = np.linspace(min(test_titanic[\"Age\"]), max(test_titanic[\"Age\"]), 6)\ngroup_names_age = ['Age1','Age2','Age3','Age4','Age5']\nbins_fare_test = np.linspace(min(test_titanic[\"Fare\"]), max(test_titanic[\"Fare\"]), 6)\ngroup_names_fare = ['Fare1','Fare2','Fare3','Fare4','Fare5']\n\n#Settings in the numbers\ntest_titanic['Sex'].replace(to_replace=['female','male'],value=[0,1],inplace=True)\ntest_titanic['Embarked'].replace(to_replace=['C','Q','S'],value=[0,1,2],inplace=True)\ntest_titanic['Age-binned'] = pd.cut(test_titanic['Age'], bins_age_test, labels=group_names_age, include_lowest=True )\ntest_titanic['Fare-binned'] = pd.cut(test_titanic['Fare'], bins_fare_test, labels=group_names_fare, include_lowest=True)\n\n#Dummies\ndummy_title_test = pd.get_dummies(test_titanic['Title'])\ndummy_age_test = pd.get_dummies(test_titanic['Age-binned'])\ndummy_fare_test = pd.get_dummies(test_titanic['Fare-binned'])\ntest_titanic = pd.concat([test_titanic, dummy_title_test, dummy_age_test, dummy_fare_test], axis=1)\n\ntest_titanic.head()","e01e03e8":"x_datatest2 = titanic[['Pclass','Parch','SibSp','Sex','Embarked','Age1','Age2','Age3','Age4','Age5','Fare1','Fare2','Fare3','Fare4','Fare5']]\nx_datatest = x_datatest2.fillna(2)\nx_datatest.isna().value_counts()","dd1b9693":"SvM = svm.SVC(kernel='rbf')\nyhat_svm = cross_val_score(SvM, x_data, y_data, cv=10).mean()\nSvM.fit(x_data, y_data)\nyhat_svm2 = SvM.predict(x_datatest)\nyhat_svm3 = SvM.predict(x_test)\nyhat_svm","ed87e609":"from sklearn.metrics import classification_report, confusion_matrix\nimport itertools\nclassification_report(y_test, yhat_svm3)","cac37bac":"from sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier\ndrugTree = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4) # it shows the default parameters\ndrugTree.fit(x_train,y_train)\npredTree = drugTree.predict(x_test)\nprint(\"DecisionTrees's Accuracy: \", metrics.accuracy_score(y_test, predTree))\npredtree2 = drugTree.predict(x_datatest)\npredtree2[0]","70e20dfb":"from sklearn import linear_model\nregr = linear_model.LinearRegression()\nregr.fit(x_train,y_train)\ny_hat_linear= regr.predict(x_datatest)\ny_hat_linear2 = []\nfor i in range(0,len(x_datatest)):\n    if y_hat_linear[i] < 0.62:\n        y_hat_linear2.append(0)\n    else: y_hat_linear2.append(1)\ny_hat_linear2[0:5]","2ef1f9f1":"from sklearn import metrics\nfrom sklearn.ensemble import VotingClassifier \na1 = LogisticRegression(C=0.01, solver='liblinear')\na2 = svm.SVC(kernel='rbf', probability = True) \na3 = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4) \na4 = KNeighborsClassifier(n_neighbors = 7)\na5 = XGBClassifier()\na6 = RandomForestClassifier()\na7 = GradientBoostingClassifier()\na9 = ensemble.AdaBoostClassifier()\nvote_est = [('LR',a1),('SVM',a2),('DecisionTree',a3),('KNN',a4),('XGB',a5),('Forest',a6),('GBC',a7),('ada',a9)]\nvot_soft = ensemble.VotingClassifier(estimators = vote_est, voting ='soft') \nvot_soft.fit(x_train,y_train)\ny_pred = vot_soft.predict(x_test) \n  \n# using accuracy_score \nscore = metrics.accuracy_score(y_test, y_pred) \nprint(\"Soft Voting Score % d\" % score) \nprint(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred))","202138d6":"y_predtest = vot_soft.predict(x_datatest)\ny_predtest[0]","dd71cf14":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nnumerical_cols = ['Pclass','Parch','SibSp','female','male','C','Q','S','Kids','Adolescent','Working-Age','Middle-Age','Elderly','Lowest','Low','Medium','High','Highest']\n\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer()),\n    ('scaler', StandardScaler())\n])\n\npreprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, numerical_cols)])\n    \na1 = LogisticRegression(C=0.01, solver='liblinear')\na2 = SVC(kernel='rbf', probability = True) \na3 = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4) \na4 = KNeighborsClassifier(n_neighbors = 4)\npipe=Pipeline(steps=[('preprocessor', preprocessor),('voting',VotingClassifier([\n        ('a1', a1), ('a2', a2), ('a3', a3), ('a4',a4)]))\n])\nRcross = cross_val_score(pipe, x_data, y_data, cv=15).mean()\nRcross","bea116d9":"pipe.fit(x_train,y_train)\nyhat_cross = pipe.predict(x_datatest)\nyhat_cross[0:5]","255069f9":"# example DataFrame to write to CSV\ndf_list_cross = []\nfor i in range(0,418):\n    df_list_cross.append([test_titanic['PassengerId'][i],y_predtest[i]])\ndf_cross = pd.DataFrame(df_list_cross,columns=['PassengerId', 'Survived'])\n\n# set an index to avoid a 'blank' row number column and write to a file named 'submission.csv'\ndf_cross.set_index('PassengerId').to_csv('submission_crossvote_votingY.csv')","9409685c":"import matplotlib.gridspec as gridspec\na1 = LogisticRegression(C=0.01, solver='liblinear')\na2 = SVC(kernel='rbf', probability = True) \na3 = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4) \na4 = KNeighborsClassifier(n_neighbors = 4)\na5 = XGBClassifier()\na6 = RandomForestClassifier()\na7 = GradientBoostingClassifier()\na8 = NuSVC(probability=True)\na9 = ensemble.AdaBoostClassifier()\nsclf = StackingClassifier(classifiers=[a2, a3, a4, a5, a6, a7, a8, a9], \n                          meta_classifier=a1)\n\nlabel = ['SVC', 'DecisionTree', 'KNN','XGB','Forest','GBC','ExTree','Stacking Classifier']\nclf_list = [a2, a3, a4, a5 ,a6, a7,a8,sclf]\n    \nclf_cv_mean = []\nclf_cv_std = []\nfor clf in clf_list:\n        \n    scores = cross_val_score(clf, x_data, y_data, cv=4, scoring='accuracy')\n    print (\"Accuracy: \" ,scores.mean())\n    clf_cv_mean.append(scores.mean())\n    clf_cv_std.append(scores.std())\n        \n    clf.fit(x_data, y_data)","838226f3":"y_Stack = clf.predict(x_datatest)\ny_Stack[0:5]","dbe8ff9d":"The Libraries included in the project:\n\n1.) The classifier : Classification is the process of predicting the class of given data points. Classes are sometimes called as targets\/ labels or categories. Classification predictive modeling is the task of approximating a mapping function (f) from input variables (X) to discrete output variables (y) (From: https:\/\/towardsdatascience.com\/machine-learning-classifiers-a5cc4e1b0623). Since the data\n\n2.) Cross-Validation : \nCross-Validation relates to the operation on the training set. While the normal regression or classification train only on one data and test on the other, Cross-Validation makes the training data divided into n parts, having n-1 parts to be trained and the last part to be tested before going to the test dataset. The test set will change every folding.\n\n![image.png](attachment:image.png)(From https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html)\n"}}