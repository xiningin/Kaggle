{"cell_type":{"ad0b8446":"code","b95f70a0":"code","bc9459be":"code","b1cb1563":"code","139eaeb9":"code","1ef8fbc2":"code","0183023d":"code","ee8ac56e":"code","dbd68ed4":"code","7c585867":"code","a1191ebf":"code","1c1e6db0":"code","530a68b5":"code","77e75618":"code","005f054f":"code","dd5eaef9":"code","c8e2fb9c":"code","bf5da777":"code","6c3ebb53":"code","06f4b330":"code","2a877f74":"code","e59b4e6c":"code","884710d7":"code","9d449454":"code","600fca44":"code","6f63e52a":"code","55815389":"code","1e2387ed":"markdown","ca9b7eda":"markdown","5af04ccd":"markdown","400dd9c4":"markdown","f3a36a71":"markdown","9eb5654f":"markdown","db3d6b11":"markdown","8837053b":"markdown","9ec88f14":"markdown","f80af933":"markdown","c9a99291":"markdown","48e85167":"markdown","0e4e1a2e":"markdown","46012aa2":"markdown","0193a73d":"markdown","914b41cf":"markdown","f00f8c21":"markdown","37104363":"markdown","1716a17c":"markdown","57d8bdf8":"markdown","a0c43b63":"markdown"},"source":{"ad0b8446":"import pandas as pd\nimport numpy as np\n\n# for reproducibility\nimport os\nos.environ['PYTHONHASHSEED'] = str(0)\nnp.random.seed(5)\n\n# options\npd.set_option('display.max_columns', 28)\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ntrain.shape, test.shape","b95f70a0":"train.isnull().sum().sum(), test.isnull().sum().sum()","bc9459be":"train.values.min(), train.values.max(), test.values.min(), test.values.max()","b1cb1563":"#heatmap\nto_heatmap = []\nfor i in train.sample(n = 10, random_state = 5).index:\n    new_map = pd.DataFrame(np.array(train.drop(columns = 'label').iloc[i])\n              .reshape(28, 28)[::-1])\n    to_heatmap.append(new_map)","139eaeb9":"import plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfig = make_subplots(rows=2, cols=5)\n\nfor i in range(len(to_heatmap)):\n    if i < 5:\n        fig.add_trace(go.Heatmap(z = to_heatmap[i],\n                                 colorscale = 'Greys',\n                                 showscale = False),\n                             row = 1,\n                             col = i+1)\n    else:\n        fig.add_trace(go.Heatmap(z = to_heatmap[i],\n                                 colorscale = 'Greys',\n                                 showscale = False),\n                             row = 2,\n                             col = i-4)\nfig.update_layout(title_text = 'Figure 1: Random Training Numbers')\nfig.show()","1ef8fbc2":"import plotly.express as px\n\nplot_df = pd.concat([train['label'],train.drop(columns = 'label')],\n                     axis = 1)\nplot_df = plot_df.sort_values('label', axis = 0)\nplot_df['label'] = plot_df['label'].transform(lambda x: x.astype(object))\nfig = px.scatter_3d(plot_df, \n                 x = 'pixel387', \n                 y = 'pixel397',\n                 z = 'pixel402',\n                 color = 'label',\n                 title = 'Figure 2: Plot of Class Groupings by Pixels')\nfig.show()","0183023d":"# LDA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nresponse = train['label']\n\ntrain_predictors, test_predictors, train_response, test_response = train_test_split(train.drop(columns = 'label'),\n                                                                response,\n                                                                train_size = 0.7,\n                                                                shuffle = True)","ee8ac56e":"# reduce\nlda = LinearDiscriminantAnalysis()\ntrain_lda = lda.fit_transform(train_predictors, train_response)\ntest_lda = lda.transform(test_predictors)","dbd68ed4":"np.cumsum(pd.Series(lda.explained_variance_ratio_))[np.cumsum(lda.explained_variance_ratio_) > 0.95]","7c585867":"lda_df = pd.concat([train_response.reset_index(), \n                    pd.DataFrame(train_lda)], \n                    axis = 1).drop(columns = 'index')\nlda_df = lda_df.sort_values('label', axis = 0)\nlabels = ['label']\nfor i in range(len(lda_df.columns)-1):\n    labels.append(f'Linear Discriminant {i+1}')\nlda_df.columns = labels\nlda_df['label'] = lda_df['label'].transform(lambda x: x.astype(object))\nlda_df.describe()","a1191ebf":"fig = px.scatter_3d(lda_df, \n                 x = 'Linear Discriminant 1', \n                 y = 'Linear Discriminant 2', \n                 z = 'Linear Discriminant 3',\n                 color = 'label',\n                 title = 'Figure 3: Training Data Projected Onto LD1, LD2, and LD3')\nfig.show()","1c1e6db0":"def normalize(data):\n    if np.std(data) == 0:\n        return data\n    else:\n        return (data - np.mean(data))\/np.std(data)\n\ntrain_predictors = train_predictors.transform(lambda x: normalize(x))","530a68b5":"from sklearn.decomposition import PCA\npca = PCA(random_state = 5)\ntrain_pca = pca.fit_transform(train_predictors)","77e75618":"# number of entries for > 0.999% variance is explained\nnp.cumsum(pd.Series(pca.explained_variance_ratio_))[np.cumsum(pd.Series(pca.explained_variance_ratio_)) > 0.95][:1]","005f054f":"# number of entries for > 0.999% variance is explained\nnp.cumsum(pd.Series(pca.explained_variance_ratio_))[np.cumsum(pd.Series(pca.explained_variance_ratio_)) > 0.99][:1]","dd5eaef9":"pca = PCA(n_components = 525,\n          random_state = 5)\ntrain_pca = pca.fit_transform(train_predictors)","c8e2fb9c":"pca_df = pd.concat([train_response.reset_index(), \n                    pd.DataFrame(train_pca)], \n                    axis = 1).drop(columns = 'index')\npca_df = pca_df.sort_values('label', axis = 0)","bf5da777":"labels = ['label']\nfor i in range(len(pca_df.columns)-1):\n    labels.append(f'Principal Component {i+1}')\npca_df.columns = labels\npca_df['label'] = pca_df['label'].transform(lambda x: x.astype(object))\n\nfig = px.scatter_3d(pca_df, \n                 x = 'Principal Component 1', \n                 y = 'Principal Component 2', \n                 z = 'Principal Component 3',\n                 color = 'label',\n                 title = 'Figure 4: Training Data Projected Onto PC1, PC2, and PC3')\nfig.show()","6c3ebb53":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters = 10,\n                random_state = 5)\ntrain_kmeans = kmeans.fit_predict(train_pca)","06f4b330":"kmeans_df = pd.concat([pd.Series(train_kmeans).rename('cluster'),\n                       pd.DataFrame(train_pca)], \n                       axis = 1)\nkmeans_df = kmeans_df.sort_values('cluster', axis = 0)","2a877f74":"labels = ['cluster']\nfor i in range(len(kmeans_df.columns)-1):\n    labels.append(f'Principal Component {i+1}')\nkmeans_df.columns = labels\nkmeans_df['cluster'] = kmeans_df['cluster'].transform(lambda x: x.astype(object))\n\nfig = px.scatter_3d(kmeans_df, \n                 x = 'Principal Component 1', \n                 y = 'Principal Component 2',\n                 z = 'Principal Component 3',\n                 color = 'cluster',\n                 title = 'Figure 5: Training Data Projected Onto PC1, PC2, and PC3 with K-Means Clustering')\nfig.show()","e59b4e6c":"train_lda = lda.fit_transform(train, response)","884710d7":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.constraints import MaxNorm\nfrom tensorflow.keras.optimizers import RMSprop\nimport time","9d449454":"callback = [EarlyStopping(monitor = 'loss',\n                          min_delta = 0.001,\n                          patience = 5)]\noptim = RMSprop(lr = 0.05)\n\nmodel = Sequential()\nmodel.add(Dense(2048, input_dim = 9, kernel_constraint = MaxNorm(4)))\nmodel.add(Activation('softmax'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2048))\nmodel.add(Activation('softmax'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10))\nmodel.add(Activation('softmax'))\n\n\n\nmodel.compile(optimizer = optim,\n              loss = 'sparse_categorical_crossentropy',\n              metrics = ['accuracy'])","600fca44":"start_time = time.time()\n\nhistory = model.fit(train_lda, \n                    response, \n                    batch_size = 200, \n                    epochs = 20,\n                    verbose = 0,\n                    callbacks = callback,\n                    shuffle = True,\n                    validation_split = 0.1)\n\nprint(f'Runtime {time.time() - start_time} seconds')","6f63e52a":"history_plot = history.history\nhistory_plot.update({'epoch' : history.epoch})","55815389":"fig = make_subplots(rows = 1,\n                    cols = 1,\n                    x_title = 'Epoch')\n\ntrace1 = go.Scatter(x = history_plot['epoch'], \n                    y = history_plot['acc'],\n                    name = 'Training Accuracy')\n\ntrace2 = go.Scatter(x = history_plot['epoch'], \n                    y = history_plot['loss'],\n                    name = 'Training Loss')\n\ntrace3 = go.Scatter(x = history_plot['epoch'], \n                    y = history_plot['val_acc'],\n                    name = 'Validation Accuracy')\n\ntrace4 = go.Scatter(x = history_plot['epoch'], \n                    y = history_plot['val_loss'],\n                    name = 'Validation Loss')\n\nfig.add_traces([trace1, trace2, trace3, trace4])\nfig.update_layout(title = 'Figure 6: Model Accuracy and Loss per Epoch')\nfig.show()","1e2387ed":"### PCA with K-Means Clustering <a name = 'kmeans'><\/a>\nK-Means Clustering generates cluster centers based on the grouping of the data and then iteratively assigns each data point as belonging to one of these clusters based on distance. As with PCA, it is unsupervised, so the cluster centers are necessarily unlabeled according to the true classes. This means there is a degree of inaccuracy in the clustering that must be taken into account. There are also some other [drawbacks of k-means clustering](https:\/\/stats.stackexchange.com\/questions\/133656\/how-to-understand-the-drawbacks-of-k-means), which are worth looking in to.","ca9b7eda":"### Linear Discriminant Analysis <a name = 'lda'><\/a>\n\nLinear Discriminant Analysis is a supervised learning method, meaning it is provided with class labels in order to execute its covariance based algorithms. This enables it to perform quite well in dimensionality reduction. Essentially, LDA projects the data onto vectors (linear discriminants) which maximize the separability between the resulting scalars for each class ([here](http:\/\/courses.cs.tamu.edu\/rgutier\/cs790_w02\/l6.pdf) are some excellent slides on the derivation of the formula). The result is a matrix of size (*n* rows, *p-1* columns), where *p* is the number of classes. LDA is also a method of classification.","5af04ccd":"The clustering does look ok, but perhaps not quite as tight as with LDA.","400dd9c4":"# Summary <a name = 'summary'><\/a>","f3a36a71":"# Call Me By Your Number: An Exercise in Machine Learning with Python\n\n## Contents\n1. [Overview](#overview)\n2. [A Look at the Data](#look)\n3. [Visualizing the Data](#heatmap)\n4. [Dimensionality Reduction](#dim)\n    - [LDA](#lda)\n    - [PCA](#pca)\n    - [PCA with K-Means](#kmeans)\n5. [Classification with a Neural Network](#keras)\n6. [Modeling](#model)\n7. [Results](#results)\n8. [Summary](#summary)\n\n# Overview <a name = 'overview'><\/a>\n\nThe primary purpose of this notebook is to explore dimensionality reduction techniques and neural networks for classifying images of handwritten letters from the MNIST dataset. Some helpful notebooks I found were:\n\n* [Interactive Intro to Dimensionality Reduction](https:\/\/www.kaggle.com\/arthurtok\/interactive-intro-to-dimensionality-reduction) by **Anisotropic** (code sometimes doesn't run, but still a good resource)\n* [Introduction to CNN Keras - 0.997 (top 6%)](https:\/\/www.kaggle.com\/yassineghouzam\/introduction-to-cnn-keras-0-997-top-6#2.-Data-preparation) by **Yassine Ghouzam, PhD**\n\nNote: the scatter plots in this notebook are interactive, so feel free to move things around!\n\n# A Look at the Data <a name = 'look'><\/a>\nMNIST is a collection of 28x28 pixel images of handwritten numbers. Numerically, they are represented by 28x28 matrices with each entry each falling on a scale from 0-255&mdash;rating their intensity&mdash;with 0 being nothing (or total black), and 255 being the maximum (or pure white).","9eb5654f":"### Principal Component Analysis <a name = 'pca'><\/a>\nPrincipal Component Analysis works very differently. It projects the data onto vectors that are orthogonal to each other and thus linearly independent. [Principal Component Analysis](http:\/\/www.ccs.neu.edu\/home\/vip\/teach\/MLcourse\/5_features_dimensions\/lecture_notes\/PCA\/PCA.pdf) (Li, Wang) gives excellent detail on the methodology and theory. A important difference between PCA and LDA is that PCA is unsupervised, meaning it does not take given classes into account when deriving the new set of vectors, which can significantly impact the groupings of the data.\n\nAccording to Li and Wang, the data must be normalized first. We have some totally empty columns, so we have to define a new function to normalize our data to avoid NaNs.","db3d6b11":"We see that at the 7th index (8th discriminant) more than 97% of the variance is explained, and a fully 100% at the 9th and final discrimiant. This means we will only be working with ***nine*** features. That is a huge improvement in computational time from the original 784.","8837053b":"# Dimensionality Reduction <a name = 'dim'><\/a>","9ec88f14":"Our neural network seems to be fairly well generalized and accurate with both an in- and out-of-sample accuracy of ~91-92% (note: this varies from run to run, even without changing any parameters and setting random seeds). This won't win any competitions, but it's fast and it performs well. For images, convolutional networks appear to be the way to go. In a pinch, this one can do. I think it's important to note that while experimenting with the execution of this report, I found that this model performed about as well as a k-nearest neighbors classifier with n = 7 neighbors, but that KNN was significantly faster both in setup and execution. KNN is also a lot easier to understand and explain.\n\nThe big takeaway for me is that there are few hard and fast rules about neural networks and how to make them perform better. Mostly, they require a lot of tinkering. After adjusting the batch size, number of epochs, number of layers, number of neurons in each layer, layer activation algorithms, and whatever else I could think to based on a variety of resources, there were few discernible patterns in the results. Accuracy across these tests ranged from 9-96%. A configuration that seemed quite sensible would perform horrible, and another that was a shot in the dark would be fantastic. Even running the exact same model didn't usually garner the same results. Reproducibility is a common problem with neural networks, so saving past models is crucial.\n\nUltimately, neural networks are a useful predictive tool, but a great deal of care must go into their utilization.","f80af933":"We see that the labels are fairly mixed up at least as far as one pixel relates to another, but there appears to be a semblance of clustering in the 0s and 4s in this particular visualization. Of course, we can't get a full view of the 784 dimensional space.","c9a99291":"There is pretty obvious clustering of each of the values, so this looks like a very good option.","48e85167":"There is nothing missing, which is very nice. We also see there are no entries outside of the established 0-255 range, so we don't need to fix anything at all.","0e4e1a2e":"In contrast to LDA, PCA requires 314 features to explain at least 95% of the variance, and 525 to get above 99%. It is a step down from 784, but it's not too big of one in comparison.","46012aa2":"# Classification with a Neural Network <a name = 'keras'><\/a>\n\nI decided to use a multilayer perceptron-style model as I want to employ dimensionality reduction, save some time, and, if possible, present a method which is perhaps more scalable at the expense of accuracy. A convolutional network generally performs better for image data, but requires the original, unreduced dataset. Ghouzam mentions in his \"Introduction to CNN Keras\" that his high-performance convolutional model took 2.5 hours to run over 2 epochs (probably on a system similar to mine). The Keras documentation includes a [model which performed at 99.25% test accuracy](https:\/\/keras.io\/examples\/mnist_cnn\/) that took just over 3 minutes to run on a modern GPU, which I don't have.\n\nOne of the common quandaries in designing a neural net is how many layers and nodes to choose. There seem to be few real answers. Of the only definite ones, the use of linear activation functions in an MLP model is [equivalent to a 2-layer, input-output model](https:\/\/en.wikipedia.org\/wiki\/Multilayer_perceptron#Activation_function), so we want to use non-linear activators for a multilayer model to be sensible. As far as the number of neurons in each layer, there seems to be a lot of tinkering involved. Here are a few resources that discuss this:\n\n* [How to choose the number of hidden layers and nodes in a feedforward neural network? (StackExchange)](https:\/\/stats.stackexchange.com\/questions\/181\/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw)\n\n* [How many hidden units should I use?](ftp:\/\/ftp.sas.com\/pub\/neural\/FAQ3.html#A_hu)\n\n* [Review on Methods to Fix Number of Hidden Neurons in Neural Networks (Hindawi)](https:\/\/www.hindawi.com\/journals\/mpe\/2013\/425740\/)\n\nFor this model, I found it was very nearly a situation of the more, the merrier. As I'm not trying to absolutely maximize my results, but instead learn, I decided at some point that \"that's pretty good\" and went with it. That said, I tried many, many different configurations before stopping. [Dropout:  A Simple Way to Prevent Neural Networks from Overfitting](http:\/\/www.jmlr.org\/papers\/volume15\/srivastava14a\/srivastava14a.pdf) (Srivastava et al., 2014) talks at length about some optimization techniques, of which I applied dropout and max-norm which are good methods to prevent overfitting of the data. There have been good results in using pretrained models, as well, if you can find them for your application.","0193a73d":"We see that LDA also drastically reduces the range of each column, so further normalization will probably not be necessarily as we aren't concerned with linearity.","914b41cf":"# Visualizing the Data <a name = 'heatmap'><\/a>\nSeeing (hah!) as these rows represent images, we should be able to look at them. Let's plot 10 random entries from the training set.","f00f8c21":"Yep, those are numbers.","37104363":"# Results <a name = 'results'><\/a>","1716a17c":"We see the sets are fairly large at 42,000 and 28,000 in training and test, respectively.","57d8bdf8":"We see a fairly decent grouping here. As we actually have the class labels for our data, we will be sticking with LDA for reducing the dimensions.","a0c43b63":"# Modeling <a name = 'model'><\/a>\nI chose to set aside 10% from the training data for validation in each epoch of the keras model. Normally, I would run a 10-fold cross-validation loop, but I'm exploring the capabilities of the model. With the data being shuffled before each epoch, this should give a good indicator of performance."}}