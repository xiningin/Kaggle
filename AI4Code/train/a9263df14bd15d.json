{"cell_type":{"086558fe":"code","b615fd0f":"code","307c02a4":"code","e8950003":"code","51187b96":"code","1ab71351":"code","1538b2b7":"code","2f758fcb":"code","35a79002":"code","f88fece6":"code","25b807b0":"code","0f390a6a":"code","b4b9f5e9":"code","bb6ed657":"code","c1d7e09e":"code","eb5b5c23":"code","49593216":"code","9659fa4b":"code","96037c26":"code","f763bca4":"code","ef3fd48b":"code","2d517dac":"code","afaf380c":"code","ac216266":"code","3fe50dcd":"code","accaf9a6":"code","dc8bff97":"code","13cb7646":"code","225c2828":"code","03f84cab":"code","f8c08fea":"code","aa62f7c5":"code","c510e9ec":"code","84c27c6e":"code","9b1175b5":"code","c95013d9":"code","ee346d4e":"code","68cc2ce7":"code","29c4e709":"code","76ad600a":"code","5d7ae9f6":"code","876d243b":"code","61d274d5":"code","cd9dbf04":"code","12a8a7cb":"code","4e5f2e3f":"code","cd78ecff":"code","022a688a":"code","3b807b83":"code","796091ec":"code","9497d4c0":"code","70663db5":"code","ac1b3c74":"code","5e09b8f2":"code","c714a739":"code","82b717bf":"code","5f947cf4":"code","b1460fdc":"code","a0201ff5":"code","83bd791f":"code","9e178718":"code","441b2f18":"code","0a2f3ef4":"code","8efd9118":"code","e173a52c":"code","a3b37104":"code","699b7aaa":"code","c8b52425":"code","c1519593":"code","ee556b1d":"code","22f2ad8c":"code","0aed66ec":"code","6c491c3e":"code","c7287008":"code","a0c31220":"code","13f9bdb5":"code","9e31e843":"code","bc93f694":"code","46571ff5":"code","5295196e":"code","20b0b869":"code","f7e12793":"code","b4bf80fa":"code","2b6a08ed":"code","8f6bb7c0":"code","89960de3":"code","851af403":"markdown","f2899b04":"markdown","ea5efd3a":"markdown","8aaf1b9d":"markdown","e3a4f763":"markdown","dd768e74":"markdown","7171916a":"markdown","434e5317":"markdown","74262aee":"markdown","e59e5987":"markdown","858f81fb":"markdown","33928946":"markdown","b896366d":"markdown","f6e7d16e":"markdown","b600f697":"markdown","786bd368":"markdown","15396064":"markdown","fdfa5cbe":"markdown","fc833d1a":"markdown","ed1fb039":"markdown","766b112f":"markdown","9bd1ae62":"markdown","e6c04e6f":"markdown","7de4e694":"markdown","b0c77194":"markdown","f0ca960a":"markdown","c44e1f4d":"markdown","df33dc9e":"markdown","4fd797ef":"markdown","9afbbf0b":"markdown","3771891c":"markdown","3a9c69e5":"markdown","dc811356":"markdown","c07901d0":"markdown","def0a164":"markdown","f7b1f2a5":"markdown","6b8cf4a1":"markdown","038b6561":"markdown"},"source":{"086558fe":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datatable as dt\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n%matplotlib inline\n\n# Tensorflow\nimport tensorflow as tf\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\n\n# Scaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Split\nfrom sklearn.model_selection import train_test_split\n\n# Scoring\nfrom sklearn.metrics import confusion_matrix, accuracy_score, plot_confusion_matrix\nfrom sklearn.metrics import classification_report\n\n# SMOTE\nfrom imblearn.over_sampling import SMOTE\n\n# Removes warning\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b615fd0f":"# Using datatable for faster loading\n\ntrain_df = dt.fread(r'\/kaggle\/input\/titanic\/train.csv').to_pandas()\ntest_df = dt.fread(r'\/kaggle\/input\/titanic\/test.csv').to_pandas()\n\nprint(\"Data is loaded\")","307c02a4":"print (\"Train: \",train_df.shape[0],\"passenger, and \",train_df.shape[1],\"features\")\nprint (\"Test: \",test_df.shape[0],\"passenger, and \",test_df.shape[1],\"features\")","e8950003":"train_df.head()","51187b96":"train_df.describe()","1ab71351":"train_df","1538b2b7":"train_df.info()","2f758fcb":"test_df.info()","35a79002":"sns.countplot(x='Survived',data=train_df)","f88fece6":"sns.countplot(x='Sex',data=train_df)","25b807b0":"g = sns.FacetGrid(train_df, col='Survived')\ng.map(plt.hist, 'Age')","0f390a6a":"g = sns.FacetGrid(data=train_df,col='Sex')\ng.map(plt.hist,'Age')","b4b9f5e9":"plt.figure(1, figsize=(15, 8))\nfor i, x in enumerate(['Pclass', 'Age','SibSp','Parch','Fare','Embarked']):\n    plt.subplot(2, 3, i+1)\n    plt.tight_layout()\n    sns.histplot(train_df[x])\n    plt.title('{}'.format(x))\nplt.show()","bb6ed657":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\ngrid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","c1d7e09e":"# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})\ngrid = sns.FacetGrid(train_df, row='Embarked', col='Survived', size=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()","eb5b5c23":"corr = train_df.corr()\nplt.subplots(figsize=(8,7))\nsns.heatmap(corr, vmax=0.9, cmap='coolwarm', square=True)","49593216":"train_df.corr()['Survived'].sort_values()","9659fa4b":"train_df.isnull().sum()","96037c26":"test_df.isnull().sum()","f763bca4":"sns.boxplot(train_df[\"Fare\"])\nplt.show()","ef3fd48b":"Q1 = train_df[\"Fare\"].quantile(0.25)\nQ3 = train_df[\"Fare\"].quantile(0.75)\nIQR = Q3-Q1\nlower_range = Q1 -(1.5 * IQR)\nupper_range = Q3 +(1.5 * IQR)\n\nprint(\"Score for lower range:\", lower_range)\nprint(\"Score for upper range:\", upper_range)","2d517dac":"train_df.loc[(train_df[\"Fare\"]>upper_range),:]","afaf380c":"sns.boxplot(train_df[\"Age\"])\nplt.show()","ac216266":"Q1 = train_df[\"Age\"].quantile(0.25)\nQ3 = train_df[\"Age\"].quantile(0.75)\nIQR = Q3-Q1\nlower_range = Q1 -(1.5 * IQR)\nupper_range = Q3 +(1.5 * IQR)\n\nprint(\"Score for lower range:\", lower_range)\nprint(\"Score for upper range:\", upper_range)","3fe50dcd":"train_df.loc[(train_df[\"Age\"]>upper_range),:]","accaf9a6":"train_df.shape","dc8bff97":"train_df = train_df.drop_duplicates()","13cb7646":"train_df.shape","225c2828":"print('Before deletion for train data: ' + str(train_df.shape))\nprint('Before deletion for test data: ' + str(test_df.shape))\n\ntrain_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\ntest_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\ncombine = [train_df, test_df]\n\nprint('\\nAfter deletion for train data: ' + str(train_df.shape))\nprint('After deletion for test data: ' + str(test_df.shape))\n","03f84cab":"train_df['AgeBands'] = pd.qcut(train_df['Age'], 5)\ntrain_df['AgeBands'].unique()","f8c08fea":"for dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 19, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 19) & (dataset['Age'] <= 25), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 25) & (dataset['Age'] <= 31.8), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 31.8) & (dataset['Age'] <= 41), 'Age'] = 3\n    dataset.loc[dataset['Age'] > 41, 'Age'] = 4\n    \ntrain_df.head()","aa62f7c5":"train_df['Age'].value_counts()","c510e9ec":"# Check missing value in Age feature from train data\ntrain_df['Age'].isnull().sum()","84c27c6e":"# check the most frequent value in Age feature\nfreq_age_train = train_df['Age'].dropna().mode()[0]\nfreq_age_test = test_df['Age'].dropna().mode()[0]","9b1175b5":"train_df['Age'] = train_df['Age'].fillna(freq_age_train)\ntest_df['Age'] = test_df['Age'].fillna(freq_age_test)","c95013d9":"train_df = train_df.drop(['AgeBands'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.head()","ee346d4e":"# Impute missing value in test data for column 'Fare' with its median\ntest_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\ntest_df.head()","68cc2ce7":"train_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","29c4e709":"for dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain_df = train_df.drop(['FareBand'], axis=1)\ncombine = [train_df, test_df]\n    \ntrain_df.head(10)","76ad600a":"for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_df['Title'], train_df['Sex'])","5d7ae9f6":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\n                                                 'Don', 'Dr', 'Major', 'Rev', 'Sir',\n                                                 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","876d243b":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_df.head()","61d274d5":"train_df = train_df.drop(['Name', 'PassengerId'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.shape, test_df.shape","cd9dbf04":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","12a8a7cb":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","4e5f2e3f":"train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntest_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ncombine = [train_df, test_df]\n\ntrain_df.head()","cd78ecff":"for dataset in combine:\n    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n\ntrain_df.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)","022a688a":"for dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ntrain_df.head()","3b807b83":"train_df['Embarked'].value_counts()","796091ec":"# Fill missing value with the most frequent value\nfreq_embarked = train_df['Embarked'].dropna().mode()[0]\nfreq_embarked","9497d4c0":"train_df['Embarked'].replace({'':freq_embarked},inplace=True)","70663db5":"train_df['Embarked'].value_counts()","ac1b3c74":"train_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","5e09b8f2":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    \ntrain_df.head()","c714a739":"train_df[\"Survived\"] = train_df[\"Survived\"].astype(int)\ntrain_df.head()","82b717bf":"X = train_df.drop(['Survived'], axis=1)\ny = train_df['Survived']","5f947cf4":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","b1460fdc":"#y_train.value_counts()","a0201ff5":"# Handle imbalance class using oversampling minority class with SMOTE method\n#os = SMOTE(sampling_strategy='minority',random_state = 1,k_neighbors=5)\n#train_smote_X,train_smote_Y = os.fit_resample(X_train,y_train)\n#X_train = pd.DataFrame(data = train_smote_X, columns = X_train.columns)\n#y_train = pd.DataFrame(data = train_smote_Y)","83bd791f":"#y_train.value_counts()","9e178718":"#scaling the data\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","441b2f18":"model = Sequential()","0a2f3ef4":"# First hidden layer\nmodel.add(Dense(100, activation='swish'))\nmodel.add(Dropout(0.5))","8efd9118":"# Second hidden layer\nmodel.add(Dense(100, activation='swish'))\nmodel.add(Dropout(0.5))","e173a52c":"# Output layer\nmodel.add(Dense(1, activation='sigmoid'))","a3b37104":"model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['Accuracy'])","699b7aaa":"earlystopping = callbacks.EarlyStopping(monitor='val_loss',\n                                        mode='min',\n                                        verbose=1,\n                                        patience=70)","c8b52425":"history = model.fit(X_train, y_train,validation_data=(X_test,y_test), batch_size = 32, epochs = 200,callbacks =[earlystopping])","c1519593":"# summarize history for acc\nplt.plot(history.history['Accuracy'])\nplt.plot(history.history['val_Accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='lower right')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()","ee556b1d":"print('Max val_acc achieved: %.2f' %(max(history.history['val_Accuracy'])*100), '%')\nprint('Max acc achieved: %.2f' %(max(history.history['Accuracy'])*100), '%')","22f2ad8c":"print('Final val_acc achieved: %.2f' %(history.history['val_Accuracy'][-1]*100), '%')\nprint('Final acc achieved: %.2f' %(history.history['Accuracy'][-1]*100), '%')","0aed66ec":"val_accuracy = np.mean(history.history['val_Accuracy'])\nprint(\"\\n%s: %.2f%%\" % ('Mean of validation accuracy', val_accuracy*100))","6c491c3e":"y_pred = model.predict(X_test)","c7287008":"y_pred = (y_pred > 0.5)","a0c31220":"ann_acc = round(accuracy_score(y_pred,y_test) * 100, 2)\nprint('Model Accuracy:',ann_acc,'%')","13f9bdb5":"ann_cm = confusion_matrix(y_test, y_pred)\ncmap1 = sns.diverging_palette(275,150,  s=40, l=65, n=6)\nplt.subplots(figsize=(10,6))\ncf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(ann_cm\/np.sum(ann_cm), cmap = cmap1, annot = True, annot_kws = {'size':15})","9e31e843":"print(classification_report(y_pred,y_test))","bc93f694":"test_model = test_df.drop(['PassengerId'], axis=1)","46571ff5":"test_model = scaler.transform(test_model)","5295196e":"pred = model.predict(test_model)","20b0b869":"pred = (pred > 0.5)","f7e12793":"pred = pred.astype(int)","b4bf80fa":"sub = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")","2b6a08ed":"sub['Survived'] = pred","8f6bb7c0":"sub.head(20)","89960de3":"sub.to_csv(\"submission.csv\",index=False)","851af403":"<a id=\"introduction\"><\/a>\n## 1. Introduction","f2899b04":"#### Fare (There are Outliers)","ea5efd3a":"<a id=\"drop\"><\/a>\n### 4.2 Drop Unwanted Data","8aaf1b9d":"<a id=\"correlation\"><\/a>\n### 3.2 Correlation","e3a4f763":"<a id=\"data_acquisition\"><\/a>\n## 2. Data Acquisition","dd768e74":"<a id=\"new_feature\"><\/a>\n### 4.4 Creating New Features","7171916a":"#### **Source and special thanks to:**\n* [<div align='left'><font size=\"3\" color=\"#000000\"> https:\/\/www.kaggle.com\/mostafaalaa123\/simple-solution-for-titanic\/notebook#ML-Models\n<\/font><\/div>](https:\/\/www.kaggle.com\/mostafaalaa123\/simple-solution-for-titanic\/notebook#ML-Models)\n* [<div align='left'><font size=\"3\" color=\"#000000\"> https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\n<\/font><\/div>](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions)","434e5317":"<a id=\"convert_feature\"><\/a>\n### 4.5 Convert Categorical Features","74262aee":"# **Titanic - Machine Learning from Disaster**","e59e5987":"### Variable Notes\n##### **Pclass:**\n<div align='left'><font size=\"3\" color=\"#000000\">A proxy for socio-economic status (SES).\n<\/font><\/div>\n\n* <div align='left'><font size=\"3\" color=\"#000000\"> 1st = Upper\n<\/font><\/div>\n* <div align='left'><font size=\"3\" color=\"#000000\"> 2nd = Middle\n<\/font><\/div>\n* <div align='left'><font size=\"3\" color=\"#000000\"> 3rd = Lower\n<\/font><\/div>\n\n\n##### **SibSp:**\n<div align='left'><font size=\"3\" color=\"#000000\"> The dataset defines family relations in this way\n<\/font><\/div>\n\n* <div align='left'><font size=\"3\" color=\"#000000\"> Sibling = brother, sister, stepbrother, stepsister\n<\/font><\/div>\n* <div align='left'><font size=\"3\" color=\"#000000\"> Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n<\/font><\/div>\n\n##### **Parch:**\n<div align='left'><font size=\"3\" color=\"#000000\"> The dataset defines family relations in this way\n<\/font><\/div>\n\n* <div align='left'><font size=\"3\" color=\"#000000\"> Parent = mother, father\n<\/font><\/div>\n* <div align='left'><font size=\"3\" color=\"#000000\"> Child = daughter, son, stepdaughter, stepson\n<\/font><\/div>\n* <div align='left'><font size=\"3\" color=\"#000000\"> Some children travelled only with a nanny, therefore parch=0 for them.\n<\/font><\/div>","858f81fb":"<a id=\"add_layers\"><\/a>\n### 5.1 Add Layers","33928946":"<a id=\"outlier\"><\/a>\n### 3.4 Outlier Detection","b896366d":"<a id=\"cleaning\"><\/a>\n## 4. Data Cleaning and Preprocessing","f6e7d16e":"#### 4.3.3 Create 'Age*Class' Feature","b600f697":"#### 4.3.1 Create AgeBands and Replace Value of Age Feature","786bd368":"<a id=\"ann\"><\/a>\n## 5. Building the ANN","15396064":"#### 4.3.1 Creating 'Title' Feature","fdfa5cbe":"<a id=\"evaluation\"><\/a>\n### 5.3 Evaluation","fc833d1a":"#### 4.4.1 Encode Sex Feature","ed1fb039":"### Load Data","766b112f":"#### Age (There are Outliers)","9bd1ae62":"#### 4.4.1 Encode Embarked Feature","e6c04e6f":"> <center><img src=\"https:\/\/elogeel.files.wordpress.com\/2010\/05\/050510_1627_multilayerp1.png\" width=\"500px\"><\/center>","7de4e694":"### Import Library","b0c77194":"#### 4.4.2 Encode Survived Feature","f0ca960a":"<a id=\"train_model\"><\/a>\n### 5.2 Train the Model","c44e1f4d":"#### 4.3.2 Create FareBands and Replace Value of Fare Feature","df33dc9e":"<a id=\"submission\"><\/a>\n## 7. Submission","4fd797ef":"<a id=\"reference\"><\/a>\n## 8. Reference","9afbbf0b":"<a id=\"missing_value\"><\/a>\n### 3.3 Check Missing Value","3771891c":"* The Artificial Neural Network consists of an input layer, a hidden layer, and an output layer.","3a9c69e5":"<div align='left'><font size=\"3\" color=\"#000000\"> The sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew. While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n<\/font><\/div>\n\n* **Goal:**\n<div align='left'><font size=\"3\" color=\"#000000\"> Build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).\n<\/font><\/div>","dc811356":"#### 4.3.2 Creating 'IsAlone' Feature","c07901d0":"<a id=\"duplicate\"><\/a>\n### 4.1 Check Duplicate Data","def0a164":"<h1 style='color:white;background-color:black' > Table of Contents <\/h1>\n\n* [Introduction](#introduction)\n* [Data Acquisition](#data_acquisition)\n* [Exploratory Data Analysis (EDA)](#eda)\n    - [Distribution](#distribution)\n    - [Correlation](#correlation)\n    - [Check Missing Value](#missing_value)\n    - [Outlier Detection](#outlier)\n* [Data Cleaning and Preprocessing](#cleaning)\n    - [Check Duplicate Data](#duplicate)\n    - [Drop Unwanted Data](#drop)\n    - [Completing a Numerical Continuous Feature](#continuous_feature)\n    - [Creating New Features](#new_feature)\n    - [Convert Categorical Features](#convert_feature)\n* [Building the ANN](#ann)\n    - [Add Layers](#add_layers)\n    - [Train the Model](#train_model)\n    - [Evaluation](#evaluation)\n* [Submission](#submission)","f7b1f2a5":"> <center><img src=\"https:\/\/awsimages.detik.net.id\/community\/media\/visual\/2019\/08\/29\/b82a0315-099c-4252-9af8-bd3831756de5.jpeg?w=700&q=80\" width=\"1400px\"><\/center>","6b8cf4a1":"<a id=\"eda\"><\/a>\n## 3. Exploratory Data Analysis (EDA)","038b6561":"<a id=\"continuous_feature\"><\/a>\n### 4.3 Completing a Numerical Continuous Feature"}}