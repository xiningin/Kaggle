{"cell_type":{"76367b6b":"code","d039ec98":"code","23c6c596":"code","959b4db2":"code","296965ea":"code","fe86f07e":"code","b5a2f844":"code","32252864":"code","433184df":"code","138adc3a":"code","23d53bd2":"code","17305a0f":"code","29f3f6fb":"code","88348259":"code","caaefedd":"code","01823d81":"code","13a343c8":"code","e03e3f2b":"code","4a529bfa":"code","9658f987":"code","3dc17dbd":"code","4f3e781d":"code","a0183a9c":"code","3e176878":"code","83892f92":"code","f6437986":"code","b93e81a6":"code","c16efc99":"markdown","41e6d05a":"markdown","db28f479":"markdown","e1194130":"markdown","a0e0f885":"markdown","f626de5a":"markdown","6a65ca50":"markdown","c1bb816c":"markdown","28100146":"markdown","1d531693":"markdown","8d638383":"markdown","94f21c41":"markdown"},"source":{"76367b6b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","d039ec98":"df = pd.read_csv(\"..\/input\/predicting-a-pulsar-star\/pulsar_stars.csv\")\ndf.head()","23c6c596":"plt.figure(figsize=(12,6))\nplt.subplot(121)\nax = sns.countplot(y = df[\"target_class\"],\n                   palette=[\"r\",\"g\"],\n                   linewidth=1,\n                   edgecolor=\"k\"*2)\nfor i,j in enumerate(df[\"target_class\"].value_counts().values):\n    ax.text(.7,i,j,weight = \"bold\",fontsize = 27)\nplt.title(\"Count for target variable in dataset\")\n\n\nplt.subplot(122)\nplt.pie(df[\"target_class\"].value_counts().values,\n        labels=[\"not pulsar stars\",\"pulsar stars\"],\n        autopct=\"%1.0f%%\",wedgeprops={\"linewidth\":2,\"edgecolor\":\"white\"})\nmy_circ = plt.Circle((0,0),.7,color = \"white\")\nplt.gca().add_artist(my_circ)\nplt.subplots_adjust(wspace = .2)\nplt.title(\"Proportion of target variable in dataset\")\nplt.show()","959b4db2":"df.info()","296965ea":"df.isnull().sum()","fe86f07e":"#Renaming columns\ndf = df.rename(columns={' Mean of the integrated profile':\"mean_profile\",\n       ' Standard deviation of the integrated profile':\"std_profile\",\n       ' Excess kurtosis of the integrated profile':\"excess_kurtosis_profile\",\n       ' Skewness of the integrated profile':\"skewness_profile\", \n        ' Mean of the DM-SNR curve':\"mean_dmsnr_curve\",\n       ' Standard deviation of the DM-SNR curve':\"std_dmsnr_curve\",\n       ' Excess kurtosis of the DM-SNR curve':\"excess_kurtosis_dmsnr_curve\",\n       ' Skewness of the DM-SNR curve':\"skewness_dmsnr_curve\",\n       })","b5a2f844":"df.describe()","32252864":"plt.figure(figsize=(16,12))\nsns.heatmap(data=df.corr(), annot=True, cmap=\"magma\", linewidth=1, fmt=\".2f\")\nplt.title(\"Correlation Map\",fontsize=20)\nplt.tight_layout()\nplt.show()","433184df":"sns.pairplot(df ,hue = \"target_class\")","138adc3a":"import itertools\n\ncolumns = [x for x in df.columns if x not in [\"target_class\"]]\nlength  = len(columns)\n\nplt.figure(figsize=(13,25))\n\nfor i,j in itertools.zip_longest(columns, range(length)):\n    plt.subplot(length\/2 ,length\/4, j+1)\n    sns.violinplot(x=df[\"target_class\"], y=df[i],\n                   palette=[\"Orangered\",\"lime\"], alpha=.5)\n    plt.title(i)","23d53bd2":"X = df.drop('target_class', axis = 1)\ny = df.target_class","17305a0f":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)","29f3f6fb":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\n\ndef model(algorithm, dtrain_x, dtrain_y, dtest_x, dtest_y, of_type):\n    \n    print (\"*****************************************************************************************\")\n    print (\"MODEL - OUTPUT\")\n    print (\"*****************************************************************************************\")\n    algorithm.fit(dtrain_x,dtrain_y)\n    predictions = algorithm.predict(dtest_x)\n    \n    print (algorithm)\n    print (\"\\naccuracy_score :\",accuracy_score(dtest_y,predictions))\n    \n    print (\"\\nclassification report :\\n\",(classification_report(dtest_y,predictions)))\n        \n    plt.figure(figsize=(13,10))\n    plt.subplot(221)\n    sns.heatmap(confusion_matrix(dtest_y,predictions),annot=True,fmt = \"d\",linecolor=\"k\",linewidths=3)\n    plt.title(\"CONFUSION MATRIX\",fontsize=20)\n    \n    predicting_probabilites = algorithm.predict_proba(dtest_x)[:,1]\n    fpr,tpr,thresholds = roc_curve(dtest_y,predicting_probabilites)\n    plt.subplot(222)\n    plt.plot(fpr,tpr,label = (\"Area_under the curve :\",auc(fpr,tpr)),color = \"r\")\n    plt.plot([1,0],[1,0],linestyle = \"dashed\",color =\"k\")\n    plt.legend(loc = \"best\")\n    plt.title(\"ROC - CURVE & AREA UNDER CURVE\",fontsize=20)\n    \n    if  of_type == \"feat\":\n        \n        dataframe = pd.DataFrame(algorithm.best_estimator_.feature_importances_,dtrain_x.columns).reset_index()\n        dataframe = dataframe.rename(columns={\"index\":\"features\",0:\"coefficients\"})\n        dataframe = dataframe.sort_values(by=\"coefficients\",ascending = False)\n        plt.subplot(223)\n        ax = sns.barplot(x = \"coefficients\" ,y =\"features\",data=dataframe,palette=\"husl\")\n        plt.title(\"FEATURE IMPORTANCES\",fontsize =20)\n        for i,j in enumerate(dataframe[\"coefficients\"]):\n            ax.text(.011,i,j,weight = \"bold\")\n    \n    elif of_type == \"coef\" :\n        \n        dataframe = pd.DataFrame(algorithm.coef_.ravel(),dtrain_x.columns).reset_index()\n        dataframe = dataframe.rename(columns={\"index\":\"features\",0:\"coefficients\"})\n        dataframe = dataframe.sort_values(by=\"coefficients\",ascending = False)\n        plt.subplot(223)\n        ax = sns.barplot(x = \"coefficients\" ,y =\"features\",data=dataframe,palette=\"husl\")\n        plt.title(\"FEATURE IMPORTANCES\",fontsize =20)\n        for i,j in enumerate(dataframe[\"coefficients\"]):\n            ax.text(.011,i,j,weight = \"bold\")\n            \n    elif of_type == \"none\" :\n        return (algorithm)","88348259":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV","caaefedd":"dt = DecisionTreeClassifier(random_state = 42)","01823d81":"param_grid = {'criterion': ['gini', 'entropy'],\n    'max_depth': range(1, 9),\n    'min_samples_split': range(25, 31),\n    'min_samples_leaf': range(2, 6)}","13a343c8":"dt_search = GridSearchCV(dt, param_grid, cv = 5, n_jobs = -1)","e03e3f2b":"model(dt_search, X_train, y_train, X_test, y_test, \"feat\")","4a529bfa":"dt_search.best_params_","9658f987":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV","3dc17dbd":"rt = RandomForestClassifier(n_estimators = 100, random_state = 42)","4f3e781d":"rt_search = RandomizedSearchCV(rt, param_grid, cv = 5, n_jobs = -1)","a0183a9c":"model(rt_search, X_train, y_train, X_test, y_test, \"feat\")","3e176878":"rt_search.best_params_","83892f92":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(random_state=42, n_jobs=-1, C=1.6)","f6437986":"model(lr, X_train, y_train, X_test, y_test, \"coef\")","b93e81a6":"from sklearn.ensemble import GradientBoostingClassifier\ngbc = GradientBoostingClassifier()\nmodel(gbc, X_train, y_train, X_test, y_test, \"feat\")","c16efc99":"## RandomForestClassifier","41e6d05a":"## Model","db28f479":"## LogisticRegression","e1194130":"## Statistical Information","a0e0f885":"## Data","f626de5a":"## Correlation HeatMap","6a65ca50":"## DecisionTreeClassifier","c1bb816c":"## Gradient Bosting Classifier","28100146":"## Pair Plot","1d531693":"## Missing values","8d638383":"## Proportion of target class in train & test data","94f21c41":"## Data Information"}}