{"cell_type":{"01c2d5b8":"code","185add09":"code","41ddbcba":"code","e1e52767":"code","63a75003":"code","adbb7795":"code","1183719b":"code","0d3b29e6":"code","49ea3105":"code","bfbd2c10":"code","c1bc4821":"code","cf071b96":"code","a27d8e37":"code","f999fa65":"code","8ba0d08c":"code","deab4d1f":"code","13eeac05":"code","27331b2c":"code","017a4c07":"code","21427ee4":"code","d2801c1f":"code","e1a9a524":"code","ab9ecae6":"code","d701d9bf":"code","adaab668":"code","e8ef9257":"code","8c2b863c":"code","1554cfb3":"code","c853f367":"code","1ceb2607":"code","7222f6d8":"code","119386e0":"code","aa51fa4d":"code","60de58c2":"code","981b0346":"code","bf30d221":"code","1fc21957":"code","a7a2060d":"code","f7325291":"code","0391b5ba":"code","6a6bd431":"code","f73be13a":"code","dc6d602b":"code","c6ee41f8":"code","91afe205":"code","975838fb":"code","9bb9ebd2":"code","8f36de33":"code","9433d3fa":"code","37bd38ea":"code","ca39db80":"code","0d9e28e8":"code","6ecdcfd6":"code","86013922":"code","dc3e26d6":"code","30ee3d01":"code","d7735255":"code","05ba9534":"code","22dc2fea":"code","9c76b6c3":"code","74bcf6e2":"code","8ed24e9f":"code","487e99eb":"code","cf3e9833":"code","e5f90cc6":"code","2e5542b3":"code","6d8d120d":"code","9123aa4d":"code","4c18ef0b":"code","5aa09f6c":"code","5b20162e":"code","903e9a16":"code","4118fa21":"code","77c09b5a":"code","10e7a3d1":"code","fea83d3f":"code","aaaed741":"code","e166f907":"code","eb399a23":"code","0e4b62c9":"code","4025f4c1":"code","9fa3cdc1":"code","b4792703":"code","780c896e":"code","0284f0e8":"code","23094758":"code","b2cddcea":"markdown","934f0fa3":"markdown","43dbcbb5":"markdown","cbb34a7e":"markdown","c7f25a8d":"markdown","1dfb5b35":"markdown","2f5a1d29":"markdown","31790a5f":"markdown"},"source":{"01c2d5b8":"# Kaggle Final Homework\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\n","185add09":"musical_preferences_train = pd.read_csv('..\/input\/mymusicalprefrences\/train.csv')\nmusical_preferences_train.head(10000)","41ddbcba":"pd.options.display.max_columns = None\npd.options.display.max_rows = 20\n","e1e52767":"musical_preferences_test = pd.read_csv('..\/input\/mymusicalprefrences\/test.csv')\nmusical_preferences_test","63a75003":"musical_preferences_train.shape # it starts from 0 to 664","adbb7795":"musical_preferences_test.shape # iit starts from 665 to 964","1183719b":"type(musical_preferences_train)","0d3b29e6":"musical_preferences_train.isnull().sum()","49ea3105":"musical_preferences_test.isnull().sum()","bfbd2c10":"df = pd.concat([musical_preferences_train, musical_preferences_test]).reset_index(drop=True)","c1bc4821":"type(df[\"BPM\"][0])","cf071b96":"df[\"Category\"] = df[\"Category\"].replace({0:\"dislike\",1:\"like\"})\ndf","a27d8e37":"df[\"isMajor\"] = df[\"Key\"].apply(lambda x: x.split(\" \")[1])\ndf[\"Key\"] = df[\"Key\"].apply(lambda x: x.split(\" \")[0])","f999fa65":"df.loc[:,\"isMajor\"] = (df[\"isMajor\"]==\"Major\").astype(int)","8ba0d08c":"df","deab4d1f":"df.groupby(\"Key\").Category.value_counts()","13eeac05":"# we encoder the Key value\ndf[\"Key\"]=df[\"Key\"].fillna(\"NaN\")\ndf.loc[:,\"Key\"] = df[\"Key\"].replace({\"A\": 1,\"A\u266d\": 2, \"B\": 3,\"B\u266d\":4,\"C\": 5,\"D\": 6,\"D\u266d\": 7,\"E\":8,\"F#\":9,\"G\":10,\"F\":11, \"C#\":12, \"E\u266d\":13, \"NaN\":0})","27331b2c":"df[df[\"Key\"]==0]","017a4c07":"df","21427ee4":"df['Country']","d2801c1f":"df.groupby(\"Country\").Category.value_counts()","e1a9a524":"df[\"Country\"]=df[\"Country\"].fillna(\"NaN\")\ndf[df[\"Country\"]==\"NaN\"]","ab9ecae6":"encoder_x=LabelEncoder()\ndf[\"Country\"]=encoder_x.fit_transform(df[\"Country\"])","d701d9bf":"print(df[\"Country\"][889])\nprint(df[\"Country\"][751])\nprint(df[\"Country\"][947])\n\n# here the NaN is transfered to 33","adaab668":"df","e8ef9257":"pd.options.display.max_rows = 20","8c2b863c":"df.groupby(\"Release_year\").Category.value_counts()","1554cfb3":"df[\"Release_year\"].unique()","c853f367":"# i choose to keep all the Release_year, because the information is useful","1ceb2607":"df = df.drop(['Album_type', 'Version'], axis = 1)\ndf","7222f6d8":"mask = ~df.Category.isna() #mask for train dataset","119386e0":"mask","aa51fa4d":"genres=[] #The Next step is find out the most popular artists,albums and genres \n\nunicgenres=[]\nraregenres=[]\ncommongenres=[]\n\ndf.Artists_Genres = df.Artists_Genres.fillna(\"NA\")\n\nfor i in df.index:\n    genres.extend(df.loc[i, \"Artists_Genres\"].split('|'))\n    \nunicgenres=np.unique(genres)\nfor i in unicgenres:\n    if(genres.count(i)<3):\n        raregenres.append(i)\n    else:\n        commongenres.append(i)\n        \nprint(\"In dataset \",len(unicgenres),\" unique artists genres\")\nprint(\"In dataset \",len(raregenres),\" rare artists\")\nprint(\"In dataset \",len(commongenres),\" common artists genres\")","60de58c2":"unicgenres","981b0346":"generesInTrain=[] # we find out \u0441ommon value in test and train dataset.\n\nfor i in df[mask].index:\n    generesInTrain.extend(df.loc[mask].Artists_Genres[i].split('|'))\n    \ngeneresInTest=[]\n\nfor i in df[~mask].index:\n    generesInTest.extend(df.loc[~mask].Artists_Genres[i].split('|'))\n    \ngeneresOnlyInTrain=[]\ngeneresOnlyInTest=[]\n\nfor i in commongenres:    \n    if(genres.count(i)==generesInTrain.count(i)):      \n        generesOnlyInTrain.append(i)\n    if(genres.count(i)==generesInTest.count(i)):\n        generesOnlyInTest.append(i)\n        \nprint(\"common generes only in train set \",generesOnlyInTrain)\nprint(\"common generes only in test set \", generesOnlyInTest)\n\nfor i in generesOnlyInTrain:\n    commongenres.remove(i)\n    raregenres.append(i)\nlen(commongenres)","bf30d221":"df.Artists_Genres = df.Artists_Genres.fillna(\"NA\")\n\ndf[\"Other generes\"]=0\n\n# \u5148\u5c06\u6240\u6709\u7684\u5171\u6709\u57fa\u56e0\u5efa\u4e00\u4e2a\u5217\nfor i in commongenres:\n    df[i]=0\n\n# \u5982\u679c\u5728\u5171\u6709\u57fa\u56e0\u91cc\uff0c\u90a3\u4e48\u76f8\u5e94\u7684\u5171\u6709\u57fa\u56e0\u4e3a1\uff0c\u5982\u679c\u4e0d\u5728\uff0c\u5219\u5728other generes\nfor j in df.index:\n    for i in df.loc[j,\"Artists_Genres\"].split('|'):\n        if  i in commongenres:\n            df.loc[j,i]=1\n        else:\n            df.loc[j,\"Other generes\"]=1\ndf","1fc21957":"df = df.drop(\"Artists_Genres\", axis=1)\ndf","a7a2060d":"mask = ~df.Category.isna() #mask for train dataset\n\nalbum=[] #find count of the most popular and rare value of genres in dataset \nunicalbum=[]\nrarealbum=[]\ncommonalbum=[]\n\ndf.Album = df.Album.fillna(\"NA\")\n\nfor i in df.index:\n    album.extend(df.loc[i, \"Album\"].split('|'))\nunicalbum=np.unique(album)\n\nfor i in unicalbum:\n    if(album.count(i)<4):\n        rarealbum.append(i)\n    else:\n        commonalbum.append(i)\n\nprint(\"In dataset \",len(unicalbum),\" unique album\")\nprint(\"In dataset \",len(rarealbum),\" rare album\")\nprint(\"In dataset \",len(commonalbum),\" common album\")\n\n","f7325291":"InTrain=[] # we find out common value in test and train dataset. \n\nfor i in df[mask].index:\n    InTrain.extend(df.loc[mask].Album[i].split('|'))\n\nInTest=[]\n\nfor i in df[~mask].index:\n    InTest.extend(df.loc[~mask].Album[i].split('|'))\n    \nOnlyInTrain=[]\nOnlyInTest=[]\n\nfor i in commonalbum:    \n    if(album.count(i)==InTrain.count(i)):      \n        OnlyInTrain.append(i)\n    if(album.count(i)==InTest.count(i)):\n        OnlyInTest.append(i)\nprint(\"common album only in train set \",OnlyInTrain)\nprint(\"common album only in test set \", OnlyInTest)\nfor i in OnlyInTrain:\n    commonalbum.remove(i)\n    rarealbum.append(i)\nlen(commonalbum)","0391b5ba":"df.Album = df.Album.fillna(\"NA\")\n\ndf[\"Other_album\"]=0\n\nfor i in commonalbum:\n    df[i]=0\nfor j in df.index:\n    for i in df.loc[j,\"Album\"].split('|'):\n        if  i in commonalbum:\n            df.loc[j,i]=1\n        else:\n            df.loc[j,\"Other_album\"]=1\n            \ndf","6a6bd431":"df = df.drop(\"Album\", axis=1)\ndf","f73be13a":"artist=[] #find out the most popular artist\nunicartist=[]\nrareartist=[]\ncommonartist=[]\n\ndf.Artists = df.Artists.fillna(\"NA\")\n\nfor i in df.index:\n    artist.extend(df.loc[i, \"Artists\"].split('|'))\nunicartist=np.unique(artist)\nfor i in unicartist:\n    if(artist.count(i)<4):\n        rareartist.append(i)\n    else:\n        commonartist.append(i)\n\nprint(\"In dataset \",len(unicartist),\" unique artists\")\nprint(\"In dataset \",len(rareartist),\" rare artists\")\nprint(\"In dataset \",len(commonartist),\" common artists\")","dc6d602b":"artistInTrain=[]\n\nfor i in df[mask].index:\n    artistInTrain.extend(df.loc[mask].Artists[i].split('|'))\n\nartistInTest=[]\n\nfor i in df[~mask].index:\n    artistInTest.extend(df.loc[~mask].Artists[i].split('|'))\n    \nartistOnlyInTrain=[]\nartistOnlyInTest=[]\n\nfor i in commonartist:    \n    if(artist.count(i)==artistInTrain.count(i)):      \n        artistOnlyInTrain.append(i)\n    if(artist.count(i)==artistInTest.count(i)):\n        artistOnlyInTest.append(i)\n        \nprint(\"common artist only in train set \",artistOnlyInTrain)\nprint(\"common artist only in test set \", artistOnlyInTest)\n\nfor i in artistOnlyInTrain:\n    commonartist.remove(i)\n    rareartist.append(i)\n    \nlen(commonartist)","c6ee41f8":"df.Artists = df.Artists.fillna(\"NA\")\n\ndf[\"Other artist\"]=0\n\nfor i in commonartist:\n    df[i]=0\nfor j in df.index:\n    for i in df.loc[j,\"Artists\"].split('|'):\n        if  i in commonartist:\n            df.loc[j,i]=1\n        else:\n            df.loc[j,\"Other artist\"]=1\n\ndf","91afe205":"df = df.drop(\"Artists\", axis=1)\ndf","975838fb":"df.rename(columns = {'Vocal ' : 'Vocal'}, inplace = True)\ndf[\"Vocal\"] = df[\"Vocal\"].fillna(\"NaN\")","9bb9ebd2":"df.loc[:,\"Vocal\"] = df[\"Vocal\"].replace({'F': 0, 'M': 1,'F|M': 2, 'N': 3,'NaN': 3})\ndf","8f36de33":"df = df.drop(\"Labels\",axis=1)\ndf = df.drop(\"Track\",axis=1)\ndf\n","9433d3fa":"df = df.drop(index=661)","37bd38ea":"clean_data = df.copy()\nclean_data","ca39db80":"\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nnew = pd.get_dummies(clean_data['Key'])\n\nnew","0d9e28e8":"new.columns = ['Key_1', 'Key_2', 'Key_3','Key_4', 'Key_5', 'Key_6','Key_7', 'Key_8', 'Key_9',\n              'Key_10', 'Key_11', 'Key_12', 'Key_13']\nnew","6ecdcfd6":"clean_data = clean_data.join(new)\n","86013922":"clean_data = clean_data.drop(['Key'], axis = 1)\nclean_data","dc3e26d6":"new_country = pd.get_dummies(clean_data['Country'])\n\nnew_country","30ee3d01":"clean_data = clean_data.join(new_country)\nclean_data = clean_data.drop(['Country'], axis = 1)\nclean_data","d7735255":"tr_mask = ~clean_data.Category.isna()\ntr_mask","05ba9534":"X_train0 = clean_data.loc[tr_mask].iloc[:, 2:]\ny_train0 = clean_data.loc[tr_mask,\"Category\"]\nX_test0 = clean_data.loc[~tr_mask].iloc[:, 2:]\n","22dc2fea":"X_train0","9c76b6c3":"\n# df.loc[:,\"Vocal\"] = df[\"Vocal\"].replace({'F': 0, 'M': 1,'F|M': 2, 'N': 3,'NaN': 3})\ny_train0 = y_train0.replace({'like': 1, 'dislike': 0})\ny_train0","74bcf6e2":"X_test0","8ed24e9f":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_train0, y_train0, test_size = 0.3, stratify = y_train0)\n","487e99eb":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","cf3e9833":"X_train","e5f90cc6":"y_train","2e5542b3":"y_test","6d8d120d":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n","9123aa4d":"scaler.fit(X_train)","4c18ef0b":"X_train = pd.DataFrame(data=scaler.transform(X_train),columns = X_train.columns,index=X_train.index)","5aa09f6c":"X_test = pd.DataFrame(data=scaler.transform(X_test),columns = X_test.columns,index=X_test.index)","5b20162e":"X_train.head(10)","903e9a16":"# random forest\n\nfrom sklearn.ensemble import RandomForestClassifier","4118fa21":"pd.options.display.max_columns = None\npd.options.display.max_rows = 20","77c09b5a":"print(np.isnan(X_test).any())","10e7a3d1":"# first we use random forest\n\nrf = RandomForestClassifier(n_estimators=2000)\nrf.fit(X_train, y_train)\nrf_pred = rf.predict(X_test)\n\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nprint(confusion_matrix(y_test, rf_pred))\nprint(classification_report(y_test, rf_pred))\n","fea83d3f":"# ..\/input\/mymusicalprefrences\/test.csv\n# X_train0, y_train0\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\n\n\n\n\n\n\ngbc = RandomForestClassifier(n_estimators=100000)\ngbc.fit(X_train0, y_train0)\n","aaaed741":"model = gbc\nsample = pd.read_csv(\"..\/input\/mymusicalprefrences\/sample_submition.csv\")\nsample[\"Category\"] = model.predict(X_test0)\n\n\n\n\n","e166f907":"sample","eb399a23":"# .\/\nsample.to_csv(\"submission.csv\", index=False)\nsample.head(10)\n\n# here is the project end, but next i have tried many other methods","0e4b62c9":"# second we use XGboost (i choose this because it is very popular hahaha)\n\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\n\n\nxgb = XGBClassifier(n_estimators=100)\nxgb.fit(X_train, y_train)\nxgb_pred = xgb.predict(X_test)\n\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nprint(confusion_matrix(y_test, xgb_pred))\nprint(classification_report(y_test, xgb_pred))\n\n","4025f4c1":"from sklearn.ensemble import GradientBoostingClassifier\n# ..\/input\/mymusicalprefrences\/test.csv\n\ngbc = GradientBoostingClassifier(n_estimators=150)\ngbc.fit(X_train, y_train)\ngbc_pred = gbc.predict(X_test)\n\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nprint(confusion_matrix(y_test, gbc_pred))\nprint(classification_report(y_test, gbc_pred))\n","9fa3cdc1":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\n\n\nGNB = GaussianNB()\nGNB.fit(X_train, y_train)\nGNB_pred = GNB.predict(X_test)\n\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nprint(confusion_matrix(y_test, GNB_pred))\nprint(classification_report(y_test, GNB_pred))\n","b4792703":"\nMNB = MultinomialNB()\nMNB.fit(X_train, y_train)\nMNB_pred = MNB.predict(X_test)\n\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nprint(confusion_matrix(y_test, MNB_pred))\nprint(classification_report(y_test, MNB_pred))\n","780c896e":"BNB = BernoulliNB()\nBNB.fit(X_train, y_train)\nBNB_pred = BNB.predict(X_test)\n\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nprint(confusion_matrix(y_test, BNB_pred))\nprint(classification_report(y_test, BNB_pred))","0284f0e8":"from sklearn.svm import SVC\n\nSV = SVC()\nSV.fit(X_train, y_train)\nSV_pred = SV.predict(X_test)\n\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nprint(confusion_matrix(y_test, SV_pred))\nprint(classification_report(y_test, SV_pred))","23094758":"\n# that is the end, welcome to contact me at 3190101962@zju.edu.cn\n","b2cddcea":"## Next we do the data prediction, and make a check_point","934f0fa3":"### step6: Album","43dbcbb5":"### step4: delete the Version and Album_type\t","cbb34a7e":"### step5: Artists_Genres ","c7f25a8d":"## \u8fd9\u91cc\u4ea7\u51fa\u9884\u6d4b\u77e9\u9635","1dfb5b35":"### step2: Country","2f5a1d29":"### step1: Key and Major\/Minor","31790a5f":"### step3: Realease_year"}}