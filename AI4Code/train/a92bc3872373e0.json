{"cell_type":{"333a6250":"code","4b8324d8":"code","639d2e14":"code","b817cb12":"code","be0d6299":"code","3d92c218":"code","ffa01578":"code","ebb230fd":"code","8a36968d":"code","d04d9fed":"code","802a93fa":"code","4b856b07":"code","3be4bf05":"code","0be5ea58":"code","b0489426":"code","334c2640":"code","8ba9396c":"code","7cb4990f":"code","e6656654":"code","d4c46686":"code","9eba26fe":"code","55703e4c":"code","029ec03c":"code","dbf4fb22":"code","058a1e61":"code","c8cfc4f3":"code","79517cf0":"code","ffc40bdf":"code","c86a01b7":"code","fc6c8f19":"code","13b067fa":"code","2a54c321":"code","7f0755e2":"code","58cede09":"code","2853ba12":"code","6f9bdbb0":"code","3bc4484c":"code","f5913d4e":"code","9f8c165a":"code","06ae3cff":"code","e241030a":"code","0e4080c7":"code","f373627a":"code","0e543392":"code","f5f5dc36":"code","7f0099a2":"code","d547d352":"code","b45599a9":"code","d480c2fe":"code","4bda9cc2":"code","99429a67":"code","f2861253":"code","d2005dc2":"code","536939b3":"code","50c57b64":"code","870fb686":"code","be847b75":"code","7c53bdac":"code","af631e04":"code","d844e366":"code","ad29ba71":"code","af42a946":"code","b22d69c4":"code","6fb5862c":"code","c964a273":"code","d704796d":"code","2005d89f":"code","a3e2a35f":"code","1d3338be":"code","69d369ec":"code","d267a1db":"code","067a77ee":"code","34c570a5":"code","396d973e":"code","5f27a5d7":"code","80754d37":"code","161f899a":"code","288615e8":"code","51ed4af5":"code","0f384d22":"code","6c5fa393":"code","63df9f28":"code","319382f0":"code","12a229b1":"code","4e25aac2":"code","73dfe530":"code","fad33a7b":"code","9a9b3018":"code","87cd0517":"code","2d9a5376":"markdown","67991e09":"markdown","2bc71683":"markdown","6d0ad183":"markdown","7beb945c":"markdown","78055129":"markdown","9b7e23bd":"markdown","7a815e86":"markdown","207d3137":"markdown","d35ce7f8":"markdown","1b751ade":"markdown","5fc98430":"markdown","17da2dc2":"markdown","044aa0dd":"markdown","f562e365":"markdown","8e800e39":"markdown","56868230":"markdown","660339fb":"markdown","6328b506":"markdown","052f46ca":"markdown","5dc1af94":"markdown","c5baab8e":"markdown","769df8e0":"markdown","c11e7ae4":"markdown","367ce216":"markdown","079407c7":"markdown","bc3f67b4":"markdown","aae49351":"markdown","7cec5646":"markdown","ec61a37f":"markdown","e612ae80":"markdown","63d223f5":"markdown","1a839740":"markdown","670a31d6":"markdown","45f22b61":"markdown","94bdbcec":"markdown","00feb040":"markdown","dab99361":"markdown","ca75fb33":"markdown","46529649":"markdown","2b230a97":"markdown","b8675e39":"markdown","284fc6e4":"markdown","5eb3e4ea":"markdown","461045ea":"markdown","0db84725":"markdown","d87db2f5":"markdown","4301c510":"markdown","79e97bd8":"markdown","c5bdb7d3":"markdown","87e9d6c1":"markdown","13735edb":"markdown","1e8c57bb":"markdown","3850c963":"markdown","c2eb50be":"markdown","1710a63e":"markdown","26992524":"markdown","c8875edc":"markdown","b48ade60":"markdown","57f7ce51":"markdown","51af5974":"markdown","220cc25d":"markdown","46e134d8":"markdown","82c0aa75":"markdown","8e8fc6b5":"markdown","0f467656":"markdown","59ec4cbd":"markdown","280852f6":"markdown","a17bdab0":"markdown","3e8ddc3d":"markdown","474e4a6c":"markdown","43e1e568":"markdown","cb127a6e":"markdown","2c953bdb":"markdown","fd4c1dac":"markdown","1695b865":"markdown","cbf74a3d":"markdown","f91bff29":"markdown","3c6d2782":"markdown","51e7f2ad":"markdown","965a1bdb":"markdown","9e110170":"markdown","cc867c3c":"markdown","5beb5078":"markdown","a80204c1":"markdown","116f18e9":"markdown","905d5ef1":"markdown","e370265c":"markdown","1f282eac":"markdown","59f23111":"markdown","0fd6dba2":"markdown","26620194":"markdown","04f2726b":"markdown","9b6ad7d0":"markdown","0e7cabdd":"markdown","2b0beeb5":"markdown","93b74c91":"markdown","9f967c5d":"markdown","b4754193":"markdown","f2e57443":"markdown","60e80c6c":"markdown","e54f69bc":"markdown","05589823":"markdown","f582005f":"markdown","af6fd23d":"markdown","8d4fc84a":"markdown","98d90167":"markdown","fb4876f7":"markdown","a0daad69":"markdown","fb705f16":"markdown","6e8aebb1":"markdown","e5eeaa83":"markdown","9b7c4688":"markdown","34ae5410":"markdown","411acea6":"markdown","e8d791c5":"markdown","6d20e75b":"markdown","cfb7eb7f":"markdown"},"source":{"333a6250":"import os\nimport copy\nimport datetime\nimport warnings\n\nimport random\nfrom datetime import datetime\nimport re\n\nimport numpy as np\nfrom scipy.stats import t\nimport pandas as pd\nimport keras\n\nfrom matplotlib import pyplot as plt\nimport matplotlib as mpl\n\nfrom wordcloud import WordCloud\n\nimport seaborn as sns\n","4b8324d8":"np.random.seed(42)\n\npd.set_option('display.max_columns', 100, 'display.width', 1024)\npd.options.mode.chained_assignment = None\nwarnings.filterwarnings('ignore')\n\nmpl.rcParams['axes.facecolor'] = '#12162f'\nplt.grid(False)\nplt.close()\n\nDATA_PATH = '..\/input\/'\nSPLIT_DATE = '2019-01-01'","639d2e14":"# Read CSV casitng dates\nanswers = pd.read_csv(os.path.join(DATA_PATH, 'answers.csv'))\nanswer_scores = pd.read_csv(os.path.join(DATA_PATH, 'answer_scores.csv'))\ncomments = pd.read_csv(os.path.join(DATA_PATH, 'comments.csv'))\nemails = pd.read_csv(os.path.join(DATA_PATH, 'emails.csv'))\ngroups = pd.read_csv(os.path.join(DATA_PATH, 'groups.csv'))\ngroup_memberships = pd.read_csv(os.path.join(DATA_PATH, 'group_memberships.csv'))\nmatches = pd.read_csv(os.path.join(DATA_PATH, 'matches.csv'))\nprofessionals = pd.read_csv(os.path.join(DATA_PATH, 'professionals.csv'))\nquestions = pd.read_csv(os.path.join(DATA_PATH, 'questions.csv'))\nquestion_scores = pd.read_csv(os.path.join(DATA_PATH, 'question_scores.csv')) \nschool_memberships = pd.read_csv(os.path.join(DATA_PATH, 'school_memberships.csv'))\nstudents = pd.read_csv(os.path.join(DATA_PATH, 'students.csv'))\ntags = pd.read_csv(os.path.join(DATA_PATH, 'tags.csv'))\ntag_questions = pd.read_csv(os.path.join(DATA_PATH, 'tag_questions.csv'))\ntag_users = pd.read_csv(os.path.join(DATA_PATH, 'tag_users.csv'))","b817cb12":"answers['answers_date_added'] = pd.to_datetime(answers['answers_date_added'], infer_datetime_format=True)\ncomments['comments_date_added'] = pd.to_datetime(comments['comments_date_added'], infer_datetime_format=True)\nemails['emails_date_sent'] = pd.to_datetime(emails['emails_date_sent'], infer_datetime_format=True)\nprofessionals['professionals_date_joined'] = pd.to_datetime(professionals['professionals_date_joined'], infer_datetime_format=True)\nquestions['questions_date_added'] = pd.to_datetime(questions['questions_date_added'], infer_datetime_format=True)\nstudents['students_date_joined'] = pd.to_datetime(students['students_date_joined'], infer_datetime_format=True)","be0d6299":"answers[['answers_date_added']].head(1)","3d92c218":"# Last Answer\ntemp = answers.groupby('answers_author_id')['answers_date_added'].max()\nprofessionals['date_last_answer'] = pd.merge(professionals, pd.DataFrame(temp.rename('last_answer')), left_on='professionals_id', right_index=True, how='left')['last_answer']\n# First Answer\ntemp = answers.groupby('answers_author_id')['answers_date_added'].min()\nprofessionals['date_first_answer'] = pd.merge(professionals, pd.DataFrame(temp.rename('first_answer')), left_on='professionals_id', right_index=True, how='left')['first_answer']\n# Last Comment\ntemp = comments.groupby('comments_author_id')['comments_date_added'].max()\nprofessionals['date_last_comment'] = pd.merge(professionals, pd.DataFrame(temp.rename('last_comment')), left_on='professionals_id', right_index=True, how='left')['last_comment']\n# First Comment\ntemp = comments.groupby('comments_author_id')['comments_date_added'].min()\nprofessionals['date_first_comment'] = pd.merge(professionals, pd.DataFrame(temp.rename('first_comment')), left_on='professionals_id', right_index=True, how='left')['first_comment']\n# Last Activity\nprofessionals['date_last_activity'] = professionals[['date_last_answer', 'date_last_comment']].max(axis=1)\n# First Activity\nprofessionals['date_first_activity'] = professionals[['date_first_answer', 'date_first_comment']].min(axis=1)\n\n# Last activity (Question)\ntemp = questions.groupby('questions_author_id')['questions_date_added'].max()\nstudents['date_last_question'] = pd.merge(students, pd.DataFrame(temp.rename('last_question')), left_on='students_id', right_index=True, how='left')['last_question']\n# First activity (Question)\ntemp = questions.groupby('questions_author_id')['questions_date_added'].min()\nstudents['date_first_question'] = pd.merge(students, pd.DataFrame(temp.rename('first_question')), left_on='students_id', right_index=True, how='left')['first_question']\n# Last activity (Comment)\ntemp = comments.groupby('comments_author_id')['comments_date_added'].max()\nstudents['date_last_comment'] = pd.merge(students, pd.DataFrame(temp.rename('last_comment')), left_on='students_id', right_index=True, how='left')['last_comment']\n# First activity (Comment)\ntemp = comments.groupby('comments_author_id')['comments_date_added'].min()\nstudents['date_first_comment'] = pd.merge(students, pd.DataFrame(temp.rename('first_comment')), left_on='students_id', right_index=True, how='left')['first_comment']\n# Last activity (Total)\nstudents['date_last_activity'] = students[['date_last_question', 'date_last_comment']].max(axis=1)\n# First activity (Total)\nstudents['date_first_activity'] = students[['date_first_question', 'date_first_comment']].min(axis=1)\n","ffa01578":"pro_emails = pd.merge(professionals, emails, how='inner', left_on='professionals_id', right_on='emails_recipient_id')\npro_emails = pro_emails[pro_emails['emails_frequency_level'] == 'email_notification_immediate']\npro_emails = pro_emails[['professionals_id', 'emails_id', 'emails_date_sent']]\n\npro_email_ques = pro_emails.merge(matches, left_on='emails_id', right_on='matches_email_id')\npro_email_ques = pro_email_ques.drop(columns=['emails_id', 'matches_email_id']) \\\n                 .set_index('professionals_id').rename(columns={'matches_question_id': 'questions_id'})","ebb230fd":"\nexists = 'Exists'\nmiss = 'Missing'\n\nfields = {\n    'students': {\n        'df': students,\n        'features': ['location'],\n        'count': {\n            'questions': questions,\n            'comments': comments\n        },\n        'memberships': {\n            'groups': group_memberships,\n            'schools': school_memberships\n        },\n        'plts': {}\n    },\n    'professionals': {\n        'df': professionals,\n        'features': ['location', 'industry', 'headline'],\n        'count': {\n            'answers': answers,\n            'comments': comments\n        },\n        'memberships': {\n            'groups': group_memberships,\n            'schools': school_memberships\n        },\n        'plts': {} \n    }\n}\n\nfor user in fields.keys():\n    user_spec = fields[user]\n    \n    df = user_spec['df']\n    \n    tmp = df[['{}_{}'.format(user, feature) for feature in user_spec['features']]]\n    tmp = tmp.fillna(miss)\n    tmp[tmp != miss] = exists \n\n    for feature in user_spec['features']:\n        user_spec['plts'][feature] = tmp.groupby('{}_{}'.format(user, feature)).size()\/len(tmp)\n    \n    # Counts\n    for key, cdf in user_spec['count'].items():\n        tf = cdf.groupby('{}_author_id'.format(key)).size()\n        tf['counts'] = pd.merge(df, pd.DataFrame(tf.rename('count')), left_on='{}_id'.format(user), right_index=True, how='left')['count'].fillna(0).astype(int)\n        \n        tf = tf['counts']\n        tf[tf > 0] = exists\n        tf[tf != exists] = miss\n        \n        user_spec['plts'][key] = tf.groupby(tf.values).size()\/len(tf)\n\n    # Counts\n    for key, mdf in user_spec['memberships'].items():\n        unique_userid_with_membership = mdf['{}_memberships_user_id'.format(key[:-1])].unique()\n        tf = pd.DataFrame()\n        tf['val'] = df['{}_id'.format(user)]\n        \n        tf[tf['val'].isin(unique_userid_with_membership)] = exists\n        tf[tf['val'] != exists] = miss\n\n        user_spec['plts'][key] = tf.groupby('val').size()\/len(tf)\n        \n    # Tags\n    unique_user_ids_with_tags = tag_users['tag_users_user_id'].unique()\n    tf = pd.DataFrame()\n    tf['val'] = df['{}_id'.format(user)]\n    \n    tf[tf['val'].isin(unique_user_ids_with_tags)] = exists\n    tf[tf['val'] != exists] = miss\n    \n    user_spec['plts']['tags'] = tf.groupby('val').size()\/len(tf)\n    plt_data = pd.DataFrame(user_spec['plts'])\n    \n    plt_data.T.plot(kind='bar', stacked=True, figsize=(14, 5), colors=('#852ab2', '#21b7f2'))\n    plt.ylabel('Ratio', fontsize=14)\n    plt.title('Data for {}'.format(user), fontsize=20)\n    plt.yticks(np.arange(0, 1.05, 0.1))\n    leg = plt.legend(bbox_to_anchor=(1, 0.5), fontsize=15)\n    for text in leg.get_texts():\n        plt.setp(text, color = 'w')\n    plt.show()","8a36968d":"n_locations = 20\n\nusers = [\n    ('students', students),\n    ('professionals', professionals)\n]\n\nfor user, df in users:\n    locations = df['{}_location'.format(user)].value_counts().sort_values(ascending=True).tail(n_locations)\n    \n    ax = locations.plot(kind='barh',figsize=(14, 10),width=0.8, fontsize=14) \n    ax.set_title('Top %s {} locations'.format(user) % n_locations, fontsize=20)\n    ax.set_xlabel('Number of {}'.format(user), fontsize=14)\n    for p in ax.patches:\n        ax.annotate(str(p.get_width()), (p.get_width(), p.get_y()), color='w', fontsize=14)\n    plt.show()\n    ","d04d9fed":"users = [\n    ('students', students),\n    ('professionals', professionals)\n]\n\ncolors = {'students' : 'cyan', 'professionals' : 'mediumvioletred'}\n\nfor user, df in users:\n    \n    years = df['{}_date_joined'.format(user)].dt.year.unique()\n    years.sort()\n    \n    min_date = df['{}_date_joined'.format(user)].min()\n    min_date = min_date.strftime(\"%B %Y\")\n    \n    max_date = df['{}_date_joined'.format(user)].max()\n    max_date = max_date.strftime(\"%B %Y\")\n    \n    \n    amounts = [len(df[df['{}_date_joined'.format(user)].dt.year == y]) for y in years]\n    \n    for i in range(len(amounts)):\n        if i > 0:\n            amounts[i] += amounts[i - 1]\n    to_plot = pd.DataFrame({'years': years, 'users': amounts})\n    plt.figure(figsize=(14, 5))\n    \n    plt.plot('years', 'users', data=to_plot, marker='o', color=colors[user])\n    x = to_plot['years']\n    y = to_plot['users']\n    plt.fill_between(x, y, color=colors[user], alpha = 0.4)\n    \n    plt.ylabel('Users', fontsize=14)\n    plt.title('Growth of {}'.format(user), fontsize=20)\n    plt.show()","802a93fa":"entities = [\n    ('questions', questions),\n    ('answers', answers)\n]\n\ncolors = {'questions' : 'cyan', 'answers' : 'mediumvioletred'}\n\nfor entity, df in entities:\n    min_date = df['{}_date_added'.format(entity)].min().strftime(\"%B %Y\")\n    max_date = df['{}_date_added'.format(entity)].max().strftime(\"%B %Y\")\n\n    df['year'] = df['{}_date_added'.format(entity)].dt.year\n    plt_data = df.groupby('year').size()\n    plt_data.plot(figsize=(14, 5), color=colors[entity],  marker='o')\n\n    x = plt_data.reset_index()['year']\n    y = plt_data.reset_index()[0]\n    plt.fill_between(x, y, color=colors[entity], alpha = 0.4)\n\n    plt.xlabel('Year', fontsize=15)\n    plt.ylabel('{} Count'.format(entity.capitalize()), fontsize=15)\n    plt.title('Number of {} asked per year ({}-{})'.format(entity.capitalize(), min_date, max_date), fontsize=20)\n    plt.show()","4b856b07":"from collections import Counter \n\nll = [\n    ('students', 'questions', students, questions),\n    ('professionals', 'answers', professionals, answers)\n]\n\ncolors = {'students' : 'cyan', 'professionals' : 'mediumvioletred'}\n\nfor user, entity, user_df, entity_df in ll:\n    tm = dict(sorted(Counter(pd.merge(user_df, entity_df, left_on='{}_id'.format(user), right_on='{}_author_id'.format(entity), how='inner').groupby('{}_id'.format(user)).size().values).items())) \n    t_d = {}\n    t_d['{}_amount'.format(entity)] = list(tm.keys())\n    t_d['{}_amount'.format(user)] = list(tm.values())\n\n    plt_data = pd.DataFrame(t_d)\n\n    plt_data.plot(x='{}_amount'.format(entity), y='{}_amount'.format(user), kind='bar', figsize=(14, 5), color=colors[user])\n    plt.xlim(-1, 30)\n    plt.xlabel('{} Count'.format(entity.capitalize()), fontsize=15)\n    plt.ylabel('{} Count'.format(user.capitalize()), fontsize=15)\n    plt.title('{} {} Diagram'.format(user.capitalize(), entity.capitalize()), fontsize=20)\n    plt.show()\n","3be4bf05":"n = 10\n\nll = [\n    ('students', 'questions', students, questions),\n    ('professionals', 'answers', professionals, answers)\n]\n\ncolors = {'students' : '#549da8', 'professionals' : '#852ab2'} \n\nfor user, entity, user_df, entity_df in ll:\n    top_n = pd.DataFrame(pd.merge(user_df, entity_df, left_on='{}_id'.format(user), right_on='{}_author_id'.format(entity), how='inner').groupby('{}_id'.format(user)).size().reset_index())\n    plt_data = top_n.rename(index=str, columns={0: '{}_amount'.format(entity)}).sort_values(by=['{}_amount'.format(entity)], ascending=False)[:n]\n\n    plt_data.plot(kind='bar', figsize=(14, 5), color=colors[user])\n    plt.xticks(np.arange(len(plt_data)), tuple(plt_data['{}_id'.format(user)]), rotation=90)\n    plt.xlabel('{} Ids'.format(user.capitalize()), fontsize=15)\n    plt.ylabel('{} Count'.format(entity.capitalize()), fontsize=15)\n    plt.title('Top {} {} with most {}'.format(n, user.capitalize(), entity), fontsize=20)\n    leg = plt.legend(loc='best', fontsize=15)\n    for text in leg.get_texts():\n        plt.setp(text, color = 'w')\n    plt.show()","0be5ea58":"users = [\n    ('students', students),\n    ('professionals', professionals)\n]\n\nmin_rel_date = '01-01-2016'\nmax_rel_date = '01-01-2019'\n\nplt_data = {}\n\nfor user, df in users:\n    df = df[(df['{}_date_joined'.format(user)] >= min_rel_date) & (df['{}_date_joined'.format(user)] <= max_rel_date)]\n    df = (df['date_first_activity'] - df['{}_date_joined'.format(user)]).dt.days.fillna(10000).astype(int)\n    df = df.groupby(df).size()\/len(df)\n    df = df.rename(lambda x: 0 if x < 0 else x)\n    df = df.rename(lambda x: x if x <= 1 or x == 10000 else '> 1')\n    df = df.rename({10000: 'NaN'})\n    df = df.groupby(level=0).sum()\n\n    plt_data[user] = df\n\nplt_data = pd.DataFrame(plt_data)\n\nplt_data.plot(kind='bar', figsize=(14, 5), colors=('#852ab2', '#21b7f2'))\nplt.xlabel('Days', fontsize=15)\nplt.ylabel('Ration', fontsize=15)\nplt.title('Days before first activity after registration', fontsize=20)\nleg = plt.legend(bbox_to_anchor=(1, 0.5), fontsize=15)\nfor text in leg.get_texts():\n    plt.setp(text, color = 'w')\nplt.show()","b0489426":"# Date of export\ncurrent_date = datetime(2019, 2 ,1)\n\nusers = [\n    ('students', students),\n    ('professionals', professionals)\n]\n\nplt_data = {}\n\nfor user, df in users:\n    df = ((current_date - df['date_last_activity']).dt.days\/30).dropna().astype(int)\n    df = df.groupby(df).size()\/len(df)\n    df = df.rename(lambda x: 0 if x < 0 else x).rename(lambda x: x if x <= 30 or x == 10000 else '> 30').rename({10000:'NaN'})\n    df = df.groupby(level=0).sum()\n\n    plt_data[user] = df\n\nplt_data = pd.DataFrame(plt_data)\n\nplt_data.plot(kind='bar', figsize=(14, 5), colors=('#852ab2', '#21b7f2'))\nplt.xlabel('Months', fontsize=15)\nplt.ylabel('Ratio', fontsize=15)\nplt.title('Last activity by Month', fontsize=20)\nleg = plt.legend(loc='best', fontsize=15)\nfor text in leg.get_texts():\n    plt.setp(text, color = 'w')\nplt.show()","334c2640":"df = questions\n\nmin_rel_date = '01-01-2016'\nmax_rel_date = '01-01-2019'\n\ntmp = answers[['answers_question_id', 'answers_date_added']].groupby('answers_question_id').min()\ndf['questions_first_answers'] = pd.merge(questions, pd.DataFrame(tmp), left_on='questions_id', right_index=True, how='left')['answers_date_added']\n\ndf = df[(df['questions_date_added'] >= min_rel_date) & (df['questions_date_added'] <= max_rel_date)]\ndf = ((df['questions_first_answers'] - df['questions_date_added']).dt.days).fillna(10000).astype(int)\ndf = df.groupby(df).size()\/len(df.index)\ndf = df.rename(lambda x: 0 if x < 0 else x)\ndf = df.rename(lambda x: x if x <= 10 or x == 10000 else '> 10')\ndf = df.rename({10000:'NaN'})\ndf = df.groupby(level=0).sum()\n\nplt_data = pd.DataFrame({'Questions': df})\n\nplt_data.plot(kind='bar', figsize=(14, 5), color='#8d56ad')\nplt.xlabel('Days', fontsize=15)\nplt.ylabel('Ratio', fontsize=15)\nplt.title('Days for first answer', fontsize=20)\nleg = plt.legend(loc='best', fontsize=15)\nfor text in leg.get_texts():\n    plt.setp(text, color = 'w')\nplt.show()","8ba9396c":"a_q = pd.merge(answers, questions, how='right', left_on='answers_question_id', right_on='questions_id')\na_q['days_diff'] = (a_q['answers_date_added'] - a_q['questions_date_added']).dt.days\nplt_data = a_q[['year_y', 'days_diff']].groupby('year_y').mean()\nplt_data.plot(figsize=(14, 5), color='#42f4ad',  marker='o')\n\nx = plt_data.reset_index()['year_y']\ny = plt_data.reset_index()['days_diff']\nplt.fill_between(x, y, color='#42f4ad', alpha = 0.4)\n\nplt.xlabel('Year', fontsize=15)\nplt.ylabel('Mean Response Timein Days', fontsize=15)\nplt.title('Evolution of Mean Response Time per year ({0}-{1})'.format(min_date, max_date), fontsize=20)\nleg = plt.legend(loc='best', fontsize=15)\nfor text in leg.get_texts():\n    plt.setp(text, color = 'w')\nplt.show()","7cb4990f":"min_date = emails['emails_date_sent'].min().strftime(\"%B %Y\")\nmax_date = emails['emails_date_sent'].max().strftime(\"%B %Y\")\n\nemails['year'] = emails['emails_date_sent'].dt.year\nplt_data = emails.groupby('year').size()\n\nplt_data.plot(figsize=(14, 5), color='#f4d641',  marker='o')\n\nx = plt_data.reset_index()['year']\ny = plt_data.reset_index()[0]\nplt.fill_between(x, y, color='#f4d641', alpha = 0.4)\n\nplt.xlabel('Year', fontsize=20)\nplt.ylabel('Emails Amount', fontsize=20)\nplt.title('Number of Emails sent per year ({0}, {1})'.format(min_date, max_date), fontsize=20)\nleg = plt.legend(loc='best', fontsize=15)\nfor text in leg.get_texts():\n    plt.setp(text, color = 'w')\nplt.show()","e6656654":"e_m = pd.DataFrame(pd.merge(emails, matches, how='inner', left_on='emails_id', right_on='matches_email_id').groupby('emails_id').size().reset_index()).rename(index=str, columns={0: \"questions_amount\"}).sort_values(by=['questions_amount'], ascending=False)\nplt_data = e_m.groupby('questions_amount').size().reset_index().rename(index=str, columns={0: \"emails_amount\"})\n\nmapping = {\n    1: '1',\n    2: '2',\n    3: '3',\n    4: '4 - 7',\n    8: '8 - 10',\n}\n\ndef get_key(x):\n    for i in range(x, 0, -1):\n        if i in mapping:\n            return mapping[i]\n\n\nplt_data['groups'] = plt_data['questions_amount'].apply(lambda x: '>10' if x >= 11 else get_key(x))\nplt_data = pd.DataFrame({'groups' :['0'], 'emails_amount' : [len(emails) - len(e_m)]}).append(plt_data.groupby('groups').sum().reset_index()[['groups', 'emails_amount']])\n\nplt_data.plot(kind='bar', figsize=(14, 5), color='#57c6b1')\n\nplt.xticks(np.arange(len(plt_data)), tuple(plt_data['groups']))\nplt.xlabel('Questions Count', fontsize=15)\nplt.ylabel('Emails Count', fontsize=15)\nplt.title('Questions contained in each email', fontsize=20)\nleg = plt.legend(loc='best', fontsize=15)\nfor text in leg.get_texts():\n    plt.setp(text, color = 'w')\nplt.show()","d4c46686":"entities = [\n    ('students', students),\n    ('professionals', professionals),\n    ('questions', questions)\n]\n\ndfs = []\n\nfor entity, df in entities:\n    if entity == 'questions':\n        df = tag_questions\n        df = pd.merge(df, tags, left_on='tag_questions_tag_id', right_on='tags_tag_id')\n    else:\n        df = tag_users[tag_users['tag_users_user_id'].isin(df['{}_id'.format(entity)])]\n        df = pd.merge(df, tags, left_on='tag_users_tag_id', right_on='tags_tag_id')\n\n    df['entity_type'] = entity\n\n    dfs.append(df)\n\n\nplt_data = pd.concat(dfs)\n\n\nplt_data = plt_data[['tags_tag_name', 'entity_type']].pivot_table(index='tags_tag_name', columns='entity_type', aggfunc=len, fill_value=0)\n\nfor entity, df in entities:\n    plt_data[entity] = plt_data[entity] \/ len(df)\n\nplt_data['sum'] = (plt_data['professionals'] + plt_data['students'] + plt_data['questions'])\nplt_data = plt_data.sort_values(by='sum', ascending=False).drop(['sum'], axis=1).head(100)\n\n\n# Wordcloud\nplt.figure(figsize=(20, 20))\nwordloud_values = ['students', 'professionals', 'questions']\naxisNum = 1\nfor wordcloud_value in wordloud_values:\n    wordcloud = WordCloud(margin=0, max_words=20, random_state=42).generate_from_frequencies(plt_data[wordcloud_value])\n    ax = plt.subplot(1, 3, axisNum)\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.title(wordcloud_value)\n    plt.axis(\"off\")\n    axisNum += 1\nplt.show()    ","9eba26fe":"from nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\n\nclass TextProcessor:\n    \"\"\"\n    Class for carrying all the text pre-processing stuff throughout the project\n    \"\"\"\n\n    def __init__(self):\n        self.stopwords = stopwords.words('english')\n        self.ps = PorterStemmer()\n\n        # stemmer will be used for each unique word once\n        self.stemmed = dict()\n\n    def process(self, text: str, allow_stopwords: bool = False) -> str:\n        \"\"\"\n        Process the specified text,\n        splitting by non-alphabetic symbols, casting to lower case,\n        removing stopwords, HTML tags and stemming each word\n\n        :param text: text to precess\n        :param allow_stopwords: whether to remove stopwords\n        :return: processed text\n        \"\"\"\n        ret = []\n\n        # split and cast to lower case\n        text = re.sub(r'<[^>]+>', ' ', str(text))\n        for word in re.split('[^a-zA-Z]', str(text).lower()):\n            # remove non-alphabetic and stop words\n            if (word.isalpha() and word not in self.stopwords) or allow_stopwords:\n                if word not in self.stemmed:\n                    self.stemmed[word] = self.ps.stem(word)\n                # use stemmed version of word\n                ret.append(self.stemmed[word])\n        return ' '.join(ret)","55703e4c":"\ntp = TextProcessor()","029ec03c":"answers['answers_body'] = answers['answers_body'].apply(tp.process)\nans_train = answers[answers['answers_date_added'] < SPLIT_DATE]\n\nquestions['questions_title'] = questions['questions_title'].apply(tp.process)\nquestions['questions_body'] = questions['questions_body'].apply(tp.process)\nquestions['questions_whole'] = questions['questions_title'] + ' ' + questions['questions_body']\nque_train = questions[questions['questions_date_added'] < SPLIT_DATE]\n\nprofessionals['professionals_headline'] = professionals['professionals_headline'].apply(tp.process)\nprofessionals['professionals_industry'] = professionals['professionals_industry'].apply(tp.process)\npro_train = professionals[professionals['professionals_date_joined'] < SPLIT_DATE]\n\nstu_train = students[students['students_date_joined'] < SPLIT_DATE]\n\ntags['tags_tag_name'] = tags['tags_tag_name'].apply(lambda x: tp.process(x, allow_stopwords=True))\n\ntag_que = tag_questions.merge(tags, left_on='tag_questions_tag_id', right_on='tags_tag_id')\ntag_pro = tag_users.merge(tags, left_on='tag_users_tag_id', right_on='tags_tag_id')","dbf4fb22":"from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n\ndef train_d2v(df: pd.DataFrame, target: str, features: list, dim: int) -> (Doc2Vec, dict):\n    \"\"\"\n    Train Doc2Vec object on provided data\n    :param df: data to work with\n    :param target: column name of target entity in df to train embeddings for\n    :param features: list of feature names to be used for training\n    :param dim: dimension of embedding vectors to train\n    :return: trained Doc2Vec object\n    \"\"\"\n    prepared = []\n    for feature in features:\n        if feature != target:\n            prepared += [TaggedDocument(row[feature].split(), [row[target]])\n                         for i, row in df[[feature, target]].drop_duplicates().iterrows()]\n        else:\n            prepared += [TaggedDocument(s.split(), [s]) for s in df[target].drop_duplicates()]\n    # shuffle prepared data, just in case\n    prepared = random.sample(prepared, len(prepared))\n    d2v = Doc2Vec(prepared, vector_size=dim, workers=4, epochs=10, dm=0)\n    docvecs = {d2v.docvecs.index2entity[i]: d2v.docvecs.vectors_docs[i]\n               for i in range(len(d2v.docvecs.index2entity))}\n    return d2v, docvecs\n\n\ndef pipeline_d2v(que: pd.DataFrame, ans: pd.DataFrame, pro: pd.DataFrame, tag_que: pd.DataFrame, tag_pro: pd.DataFrame,\n                 dim: int) -> (dict, dict, Doc2Vec):\n    \"\"\"\n    Pipeline for training embeddings for\n    professional's industries and question's tags via doc2vec algorithm\n    on question titles, bodies, answer bodies, names of tags, professional industries and headlines\n\n    :param que: raw questions.csv dataset\n    :param ans: raw answers.csv dataset\n    :param pro: raw professionals.csv dataset\n    :param tag_que: tags.csv merged with tag_questions.csv\n    :param tag_pro: tags.csv merged with tag_users.csv\n    :param dim: dimension of doc2vec embeddings to train\n    :return: trained tags, industries embeddings and question's Doc2Vec model\n    \"\"\"\n    # aggregate all the tags in one string for same professionals\n    pro_tags = tag_pro[['tag_users_user_id', 'tags_tag_name']].groupby(by='tag_users_user_id', as_index=False) \\\n        .aggregate(lambda x: ' '.join(x)).rename(columns={'tags_tag_name': 'tags_pro_name'})\n    pro_tags = pro.merge(pro_tags, left_on='professionals_id', right_on='tag_users_user_id')\n\n    # merge questions, tags, answers and professionals\n    que_tags = que.merge(tag_que, left_on='questions_id', right_on='tag_questions_question_id')\n    ans_que_tags = ans.merge(que_tags, left_on=\"answers_question_id\", right_on=\"questions_id\")\n    df = ans_que_tags.merge(pro_tags, left_on='answers_author_id', right_on='professionals_id')\n\n    text_features = ['questions_title', 'questions_body', 'answers_body', 'tags_tag_name', 'tags_pro_name',\n                     'professionals_industry', 'professionals_headline']\n\n    # train and save question's tags embeddings\n    _, tags_embs = train_d2v(df, 'tags_tag_name', text_features, dim)\n\n    # aggregate all the tags in one string for same questions\n    que_tags = que_tags[['questions_id', 'tags_tag_name']].groupby(by='questions_id', as_index=False) \\\n        .aggregate(lambda x: ' '.join(x))\n\n    # merge questions, aggregated tags, answers and professionals\n    que_tags = que.merge(que_tags, on='questions_id')\n    ans_que_tags = ans.merge(que_tags, left_on=\"answers_question_id\", right_on=\"questions_id\")\n    df = ans_que_tags.merge(pro_tags, left_on='answers_author_id', right_on='professionals_id')\n\n    # train and save professional's industries embeddings\n    _, inds_embs = train_d2v(df, 'professionals_industry', text_features, dim)\n\n    head_d2v, _ = train_d2v(df, 'professionals_headline', text_features, 5)\n\n    ques_d2v, _ = train_d2v(que_tags, 'questions_id', ['questions_whole'], dim)\n\n    return tags_embs, inds_embs, head_d2v, ques_d2v","058a1e61":"n_emb = 10\ntag_embs, ind_embs, head_d2v, ques_d2v = pipeline_d2v(que_train, ans_train, pro_train, tag_que, tag_pro, n_emb)","c8cfc4f3":"from sklearn.manifold import TSNE\n\ndef vis_emb(id_to_vec: dict, occurrences: pd.Series, title: str):\n    \"\"\"\n    Visualize embeddings via T-SNE\n    \"\"\"\n    top = set(occurrences.value_counts().iloc[:100].index)\n    filtered = {key: value for key, value in id_to_vec.items() if key in top}\n    \n    proj = TSNE(n_components=2).fit_transform(np.vstack(filtered.values()))\n    \n    _, ax = plt.subplots(figsize=(12, 12))\n    plt.scatter(proj[:, 0], proj[:, 1], alpha=0.7, s=90, c='#69f0ff')\n    for i, name in enumerate(filtered.keys()):\n        ax.annotate(name, (proj[i, 0], proj[i, 1]), color='w')\n        \n    ax.set_title(title, fontsize=20)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    \n    plt.show()","79517cf0":"vis_emb(tag_embs, tag_que['tags_tag_name'], 'Doc2Vec tags embeddings')","ffc40bdf":"vis_emb(ind_embs, pro_train['professionals_industry'], 'Doc2Vec industries embeddings')","c86a01b7":"from gensim.corpora import Dictionary\nfrom gensim.models import TfidfModel\nfrom gensim.models.ldamulticore import LdaMulticore\n\ndef pipeline_lda(que: pd.DataFrame, dim: int) -> (Dictionary, TfidfModel, LdaMulticore):\n    \"\"\"\n    Pipeline for training embeddings for questions via LDA algorithm\n    on question titles and bodies\n\n    :param que: raw questions.csv dataset\n    :param dim: dimension of doc2vec embeddings to train\n    :return: trained tags, industries embeddings and question's Doc2Vec model\n    \"\"\"\n    lda_tokens = que['questions_whole'].apply(lambda x: x.split())\n\n    # create Dictionary and train it on text corpus\n    lda_dic = Dictionary(lda_tokens)\n    lda_dic.filter_extremes(no_below=10, no_above=0.6, keep_n=8000)\n    lda_corpus = [lda_dic.doc2bow(doc) for doc in lda_tokens]\n\n    # create TfidfModel and train it on text corpus \n    lda_tfidf = TfidfModel(lda_corpus)\n    lda_corpus = lda_tfidf[lda_corpus]\n\n    # create LDA Model and train it on text corpus\n    lda_model = LdaMulticore(\n        lda_corpus, num_topics=dim, id2word=lda_dic, workers=4,\n        passes=20, chunksize=1000, random_state=0\n    )\n\n    return lda_dic, lda_tfidf, lda_model","fc6c8f19":"lda_dic, lda_tfidf, lda_model = pipeline_lda(que_train, n_emb)","13b067fa":"def plot_lda_wordcloud(lda_model, rows, columns):\n    \"\"\"\n    Word Cloud for each topic of the LDA model\n    \"\"\"  \n    n_ax = 0\n    plt.figure(figsize=(5*columns, 2*rows))\n    for t_id in range(lda_model.state.get_lambda().shape[0]):\n        #gather most relevant terms for the given topic\n        topics_terms = lda_model.state.get_lambda()\n        tmp = {}\n        for i in range(1, len(topics_terms[0])):\n            tmp[lda_model.id2word[i]]=topics_terms[t_id,i]\n\n        # wordcloud drawing\n        wordcloud = WordCloud(margin=0, max_words=15).generate_from_frequencies(tmp)\n        n_ax += 1\n        ax = plt.subplot(rows, columns, n_ax)\n\n        plt.imshow(wordcloud, interpolation='bilinear')\n        title = t_id\n        plt.title(title)\n        plt.axis(\"off\")\n        plt.margins(x=0, y=0)\n\n    plt.show()","2a54c321":"_, _, lda_for_vis_model = pipeline_lda(que_train, 30)\nplot_lda_wordcloud(lda_for_vis_model, 6, 5)","7f0755e2":"from abc import ABC\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\nclass BaseProc(ABC):\n    \"\"\"\n    Class with implementation of basic preprocessors logic\n    \"\"\"\n\n    def __init__(self):\n        self.pp = {}\n        self.features = {\n            'categorical': [],\n            'numerical': {'zero': [], 'mean': []},\n            'date': []\n        }\n\n    def _unroll_features(self):\n        \"\"\"\n        Called once after self.features specification in constructor of child class,\n        unrolls all the features in single separate list self.features['all']\n        \"\"\"\n        self.features['all'] = ([name for name, deg in self.features['categorical']]\n                                if 'categorical' in self.features else []) + \\\n                               (self.features['numerical']['zero'] + self.features['numerical']['mean']\n                                if 'numerical' in self.features else []) + \\\n                               ([f + p for f in self.features['date']\n                                 for p in ['_time', '_doy_sin', '_doy_cos']]\n                                if 'date' in self.features else [])\n\n    def datetime(self, df: pd.DataFrame, feature: str):\n        \"\"\"\n        Generates a bunch of new datetime features and drops the original feature inplace\n\n        :param df: data to work with\n        :param feature: name of a column in df that contains date\n        \"\"\"\n        # iterate over suffix of generated features and function to calculate it\n        for suf, fun in [('_time', lambda d: d.year + (d.dayofyear + d.hour \/ 24) \/ 365),\n                         ('_doy_sin', lambda d: np.sin(2 * np.pi * d.dayofyear \/ 365)),\n                         ('_doy_cos', lambda d: np.cos(2 * np.pi * d.dayofyear \/ 365))]:\n            df[feature + suf] = df[feature].apply(fun)\n            # add created feature to the list of generated features\n            self.features['gen'].append(feature + suf)\n\n        df.drop(columns=feature, inplace=True)\n\n    def __get_preprocessor(self, fit_data: np.array, feature: str, base):\n        \"\"\"\n        Creates new preprocessor object of class base and fits it\n        or uses existing one in self.pp and returns it\n\n        :param fit_data: NumPy array of data to fit new preprocessor\n        :param feature: feature name to search for in self.pp\n        :param base: new preprocessor's class\n        :returns: preprocessor object\n        \"\"\"\n        if feature in self.pp:\n            preproc = self.pp[feature]\n        else:\n            preproc = base()\n            preproc.fit(fit_data)\n            self.pp[feature] = preproc\n        return preproc\n\n    def numerical(self, df: pd.DataFrame, feature: str, fillmode: str):\n        \"\"\"\n        Transforms via StandardScaler, fills NaNs according to fillmode\n\n        :param df: data to work with\n        :param feature: name of a column in df that contains numerical data\n        :param fillmode: method to fill NaNs, either 'mean' or 'zero'\n        \"\"\"\n        # calculate default value and fill NaNs with it\n        if fillmode == 'mean':\n            if feature in self.pp:\n                na = self.pp[feature].mean_[0]\n            else:\n                na = df[feature].mean()\n        else:\n            na = 0\n\n        df[feature].fillna(na, inplace=True)\n\n        # standardize feature values\n        fit_data = df[feature].values.reshape(-1, 1).astype('float64')\n        sc = self.__get_preprocessor(fit_data, feature, StandardScaler)\n        df[feature] = sc.transform(fit_data)\n\n    def categorical(self, df: pd.DataFrame, feature: str, n: int):\n        \"\"\"\n        Encodes top n most popular values with different labels from 0 to n-1,\n        remaining values with n and NaNs with n+1\n\n        :param df: data to work with\n        :param feature: name of a column in df that contains categorical data\n        :param n: number of top by popularity values to move in separate categories.\n                  0 to encode everything with different labels\n        \"\"\"\n        vc = df[feature].value_counts()\n        # number of unique values to leave\n        n = len(vc) if n == 0 else n\n        # unique values to leave\n        top = set(vc[:n].index)\n        isin_top = df[feature].isin(top)\n\n        fit_data = df.loc[isin_top, feature]\n        le = self.__get_preprocessor(fit_data, feature, LabelEncoder)\n\n        # isin_le differs from isin_top if new preprocessor object was fitted\n        isin_le = df[feature].isin(set(le.classes_))\n        df.loc[isin_le, feature] = le.transform(df.loc[isin_le, feature])\n\n        # unique values to throw away - encode with single label n\n        bottom = set(vc.index) - set(le.classes_)\n        isin_bottom = df[feature].isin(bottom)\n        df.loc[isin_bottom, feature] = n\n\n        df[feature].fillna(n + 1, inplace=True)\n\n    def preprocess(self, df: pd.DataFrame):\n        \"\"\"\n        Full preprocessing pipeline\n\n        :param df: data to work with\n        \"\"\"\n        # preprocess all date features\n        self.features['gen'] = []\n        if 'date' in self.features:\n            for feature in self.features['date']:\n                self.datetime(df, feature)\n\n        # preprocess all numerical features, including generated features from dates\n        if 'numerical' in self.features:\n            for fillmode in self.features['numerical']:\n                for feature in self.features['numerical'][fillmode] + \\\n                               (self.features['gen'] if fillmode == 'mean' else []):\n                    if feature in df.columns:\n                        self.numerical(df, feature, fillmode)\n\n        # preprocess all categorical features\n        if 'categorical' in self.features:\n            for feature, n in self.features['categorical']:\n                self.categorical(df, feature, n)","58cede09":"class Averager:\n    \"\"\"\n    Small class useful for computing averaged features values\n    \"\"\"\n    \n    def __init__(self):\n        self.sum = 0\n        self.cnt = 0\n\n    def upd(self, val):\n        self.sum += val\n        self.cnt += 1\n\n    def get(self):\n        if self.cnt == 0:\n            return None\n        return self.sum \/ self.cnt","2853ba12":"class QueProc(BaseProc):\n    \"\"\"\n    Questions data preprocessor\n    \"\"\"\n\n    def __init__(self, tag_embs, ques_d2v, lda_dic, lda_tfidf, lda_model):\n        super().__init__()\n\n        self.tag_embs = tag_embs\n        self.ques_d2v = ques_d2v\n\n        self.lda_dic = lda_dic\n        self.lda_tfidf = lda_tfidf\n        self.lda_model = lda_model\n\n        self.features = {\n            'numerical': {\n                'zero': ['questions_body_length', 'questions_tag_count'],\n                'mean': []\n            }\n        }\n\n        self._unroll_features()\n\n    def transform(self, que, tags):\n        \"\"\"\n        Main method to calculate, preprocess question's features and append textual embeddings\n\n        :param que: questions dataframe with preprocessed textual columns\n        :param tags: merged tags and tag_questions dataframes with preprocessed textual columns\n        :return: dataframe of question's id, question's date added and model-friendly question's features\n        \"\"\"\n        que['questions_time'] = que['questions_date_added']\n        que['questions_body_length'] = que['questions_body'].apply(lambda s: len(str(s)))\n\n        # append aggregated tags to each question\n        tags_grouped = tags.groupby('tag_questions_question_id', as_index=False)[['tags_tag_name']] \\\n            .agg(lambda x: ' '.join(set(x)))\n        tags_grouped['questions_tag_count'] = tags_grouped['tags_tag_name'].apply(lambda x: len(x.split()))\n        df = que.merge(tags_grouped, how='left', left_on='questions_id', right_on='tag_questions_question_id')\n\n        # launch feature pre-processing\n        self.preprocess(df)\n\n        # prepare tag embeddings\n\n        tag_emb_len = list(self.tag_embs.values())[0].shape[0]\n\n        def __convert(s):\n            embs = []\n            for tag in str(s).split():\n                if tag in self.tag_embs:\n                    embs.append(self.tag_embs[tag])\n            if len(embs) == 0:\n                embs.append(np.zeros(tag_emb_len))\n            return np.vstack(embs).mean(axis=0)\n\n        mean_embs = df['tags_tag_name'].apply(__convert)\n\n        lda_emb_len = len(self.lda_model[[]])\n        lda_corpus = [self.lda_dic.doc2bow(doc) for doc in df['questions_whole'].apply(lambda x: x.split())]\n        lda_corpus = self.lda_tfidf[lda_corpus]\n        lda_que_embs = self.lda_model.inference(lda_corpus)[0]\n\n        d2v_emb_len = len(self.ques_d2v.infer_vector([]))\n\n        def __infer_d2v(s):\n            self.ques_d2v.random.seed(0)\n            return self.ques_d2v.infer_vector(s.split(), steps=100)\n\n        d2v_que_embs = df['questions_whole'].apply(__infer_d2v)\n\n        # re-order the columns\n        df = df[['questions_id', 'questions_time'] + self.features['all']]\n\n        # append lda question embeddings\n        for i in range(lda_emb_len):\n            df[f'que_lda_emb_{i}'] = lda_que_embs[:, i]\n\n        # append d2v question embeddings\n        for i in range(d2v_emb_len):\n            df[f'que_d2v_emb_{i}'] = d2v_que_embs.apply(lambda x: x[i])\n\n        # append tag embeddings\n        for i in range(tag_emb_len):\n            df[f'que_tag_emb_{i}'] = mean_embs.apply(lambda x: x[i])\n\n        return df","6f9bdbb0":"que_proc = QueProc(tag_embs, ques_d2v, lda_dic, lda_tfidf, lda_model)\nque_data = que_proc.transform(que_train, tag_que)","3bc4484c":"class StuProc(BaseProc):\n    \"\"\"\n    Students data preprocessor\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n        self.features = {\n            'categorical': [('students_location', 100), ('students_state', 40)],\n            'numerical': {\n                'zero': ['students_questions_asked'],\n                'mean': ['students_average_question_body_length', 'students_average_answer_body_length',\n                         'students_average_answer_amount']\n            },\n            'date': []\n        }\n\n        self._unroll_features()\n\n    def transform(self, stu, que, ans) -> pd.DataFrame:\n        \"\"\"\n        Main method to calculate, preprocess students's features and append textual embeddings\n\n        :param stu: students dataframe with preprocessed textual columns\n        :param que: questions dataframe with preprocessed textual columns\n        :param ans: answers dataframe with preprocessed textual columns\n        :return: dataframe of students's id, timestamp and model-friendly students's features after that timestamp\n        \"\"\"\n        stu['students_state'] = stu['students_location'].apply(lambda s: str(s).split(', ')[-1])\n\n        que['questions_body_length'] = que['questions_body'].apply(lambda s: len(str(s)))\n        ans['answers_body_length'] = ans['answers_body'].apply(lambda s: len(str(s)))\n\n        # prepare all the dataframes needed for iteration\n        que_change = stu.merge(que, left_on='students_id', right_on='questions_author_id')\n        ans_change = que_change.merge(ans, left_on='questions_id', right_on='answers_question_id') \\\n            .rename(columns={'answers_date_added': 'students_time'})\n\n        # add new columns which will be used to determine to which change corressponds stacked DataFrame row\n        ans_change['change_type'] = 'answer'\n        que_change['change_type'] = 'question'\n        que_change = que_change.rename(columns={'questions_date_added': 'students_time'})\n\n        # stack two DataFrame to form resulting one for iteration\n        df = pd.concat([que_change, ans_change], ignore_index=True, sort=True).sort_values('students_time')\n\n        # data is a dist with mapping from student's id to his list of features\n        # each list contains dicts with mapping from feature name to its value on a particular moment\n        data = {}\n        avgs = {}\n\n        for i, row in stu.iterrows():\n            cur_stu = row['students_id']\n            \n            # DEFAULT CASE\n            # student's feature values before he left any questions\n            if cur_stu not in data:\n                new = {'students_questions_asked': 0,\n                       'students_previous_question_time': row['students_date_joined']}\n                for feature in ['students_time'] + self.features['numerical']['mean']:\n                    new[feature] = None\n                data[cur_stu] = [new]\n                avgs[cur_stu] = {feature: Averager() for feature in self.features['numerical']['mean']}\n        \n        for i, row in df.iterrows():\n            cur_stu = row['students_id']\n\n            # features on previous timestamp\n            prv = data[cur_stu][-1]\n            new = prv.copy()\n\n            new['students_time'] = row['students_time']\n\n            # UPDATE RULES\n            # if current change is new question, update question-depended features\n            if row['change_type'] == 'question':\n                new['students_questions_asked'] += 1\n                new['students_previous_question_time'] = row['questions_date_added']\n                new['students_average_question_body_length'] = row['questions_body_length']\n            # if new answer is added, update answer-depended features\n            else:  \n                new['students_average_answer_body_length'] = row['answers_body_length']\n                new['students_average_answer_amount'] = new['students_average_answer_amount'] + 1 \\\n                    if new['students_average_answer_amount'] is not None else 1\n\n            # NORMALIZE AVERAGE FEATURES\n            for feature in ['students_average_question_body_length'] if row['change_type'] == 'question' else \\\n                    ['students_average_answer_body_length', 'students_average_answer_amount']:\n                avgs[cur_stu][feature].upd(new[feature])\n                new[feature] = avgs[cur_stu][feature].get()\n\n            data[cur_stu].append(new)\n\n        # construct a DataFrame out of dict of list of feature dicts\n        df = pd.DataFrame([{**f, **{'students_id': id}} for (id, fs) in data.items() for f in fs])\n\n        df = df.merge(stu, on='students_id')\n        # launch feature pre-processing\n        self.preprocess(df)\n\n        # re-order the columns\n        df = df[['students_id', 'students_time'] + self.features['all']]\n\n        return df","f5913d4e":"stu_proc = StuProc()\nstu_data = stu_proc.transform(stu_train, que_train, ans_train)","9f8c165a":"class ProProc(BaseProc):\n    \"\"\"\n    Professionals data preprocessor\n    \"\"\"\n\n    def __init__(self, tag_embs, ind_embs, head_d2v, ques_d2v):\n        super().__init__()\n\n        self.tag_embs = tag_embs\n        self.ind_embs = ind_embs\n\n        self.head_d2v = head_d2v\n        self.ques_d2v = ques_d2v\n\n        self.features = {\n            'categorical': [('professionals_industry', 100), ('professionals_location', 100),\n                            ('professionals_state', 40)],\n            'numerical': {\n                'zero': [],  # ['professionals_questions_answered'],\n                'mean': ['professionals_average_question_body_length',\n                         'professionals_average_answer_body_length']\n            }\n        }\n\n        self._unroll_features()\n\n\n    def transform(self, pro, que, ans, tags) -> pd.DataFrame:\n        \"\"\"\n        Main method to calculate, preprocess students's features and append textual embeddings\n\n        :param pro: professionals dataframe with preprocessed textual columns\n        :param que: questions dataframe with preprocessed textual columns\n        :param ans: answers dataframe with preprocessed textual columns\n        :param tags: merged tags and tag_users dataframes with preprocessed textual columns\n        :return: dataframe of professional's id, timestamp and model-friendly professional's features after that timestamp\n        \"\"\"\n        # aggregate tags for each professional\n        tags_grouped = tags.groupby('tag_users_user_id', as_index=False)[['tags_tag_name']] \\\n            .aggregate(lambda x: ' '.join(set(x)))\n\n        pro['professionals_industry_raw'] = pro['professionals_industry']\n        pro['professionals_state'] = pro['professionals_location'].apply(lambda loc: str(loc).split(', ')[-1])\n        que['questions_body_length'] = que['questions_body'].apply(lambda s: len(str(s)))\n        ans['answers_body_length'] = ans['answers_body'].apply(lambda s: len(str(s)))\n\n        # prepare all the dataframes needed for iteration\n        df = pro.merge(ans, left_on='professionals_id', right_on='answers_author_id') \\\n            .merge(que, left_on='answers_question_id', right_on='questions_id') \\\n            .sort_values('answers_date_added')\n\n        # data is a dist with mapping from professional's id to his list of features\n        # each list contains dicts with mapping from feature name to its value on a particular moment\n        data = {}\n        que_emb_len = len(self.ques_d2v.infer_vector([]))\n\n        for i, row in pro.iterrows():\n            cur_pro = row['professionals_id']\n\n            # DEFAULT CASE\n            # professional's feature values before he left any questions\n            if cur_pro not in data:\n                new = {'professionals_questions_answered': 0,\n                       'professionals_previous_answer_date': row['professionals_date_joined']}\n                for feature in ['professionals_time', 'professionals_average_question_age',\n                                'professionals_average_question_body_length',\n                                'professionals_average_answer_body_length']:\n                    new[feature] = None\n                new['pro_que_emb'] = np.zeros(que_emb_len)\n                data[cur_pro] = [new]\n\n        def __infer_d2v(s):\n            self.ques_d2v.random.seed(0)\n            return self.ques_d2v.infer_vector(s.split(), steps=100)\n\n        for i, row in df.iterrows():\n            cur_pro = row['professionals_id']\n\n            prv = data[cur_pro][-1]\n            # UPDATE RULES\n            new = {'professionals_time': row['answers_date_added'],\n                   'professionals_questions_answered': prv['professionals_questions_answered'] + 1,\n                   'professionals_previous_answer_date': row['answers_date_added'],\n                   'professionals_average_question_age':\n                       (row['answers_date_added'] - row['questions_date_added']) \/ np.timedelta64(1, 's'),\n                   'professionals_average_question_body_length': row['questions_body_length'],\n                   'professionals_average_answer_body_length': row['answers_body_length'],\n                   'pro_que_emb': __infer_d2v(row['questions_whole'])}\n            length = len(data[cur_pro])\n            if length != 1:\n                # NORMALIZE AVERAGE FEATURES\n                for feature in ['professionals_average_question_age', 'professionals_average_question_body_length',\n                                'professionals_average_answer_body_length', 'pro_que_emb']:\n                    new[feature] = (prv[feature] * (length - 1) + new[feature]) \/ length\n            data[cur_pro].append(new)\n\n        # construct a dataframe out of dict of list of feature dicts\n        df = pd.DataFrame([{**f, **{'professionals_id': id}} for (id, fs) in data.items() for f in fs])\n\n        df = df.merge(pro, on='professionals_id').merge(tags_grouped, how='left', left_on='professionals_id',\n                                                        right_on='tag_users_user_id')\n        # launch feature pre-processing\n        self.preprocess(df)\n\n        # prepare subscribed tag embeddings\n\n        tag_emb_len = list(self.tag_embs.values())[0].shape[0]\n\n        def __convert_tag(s):\n            embs = []\n            for tag in str(s).split():\n                if tag in self.tag_embs:\n                    embs.append(self.tag_embs[tag])\n            if len(embs) == 0:\n                embs.append(np.zeros(tag_emb_len))\n            return np.vstack(embs).mean(axis=0)\n\n        mean_tag_embs = df['tags_tag_name'].apply(__convert_tag)\n\n        # prepare industry embeddings\n        industry_emb_len = list(self.ind_embs.values())[0].shape[0]\n        ind_embs = df['professionals_industry_raw'] \\\n            .apply(lambda x: self.ind_embs.get(x, np.zeros(industry_emb_len)))\n\n        head_emb_len = len(self.head_d2v.infer_vector([]))\n\n        def __convert_headline(s):\n            self.head_d2v.random.seed(0)\n            return self.head_d2v.infer_vector(s.split(), steps=100)\n\n        head_embs = df['professionals_headline'].apply(__convert_headline)\n\n        que_embs = df['pro_que_emb']\n\n        # re-order the columns\n        df = df[['professionals_id', 'professionals_time'] + self.features['all']]\n\n        # append subscribed tag embeddings\n        for i in range(tag_emb_len):\n            df[f'pro_tag_emb_{i}'] = mean_tag_embs.apply(lambda x: x[i])\n\n        for i in range(industry_emb_len):\n            df[f'pro_ind_emb_{i}'] = ind_embs.apply(lambda x: x[i])\n\n        for i in range(head_emb_len):\n            df[f'pro_head_emb_{i}'] = head_embs.apply(lambda x: x[i])\n\n        for i in range(que_emb_len):\n            df[f'pro_que_emb_{i}'] = que_embs.apply(lambda x: x[i])\n\n        return df","06ae3cff":"pro_proc = ProProc(tag_embs, ind_embs, head_d2v, ques_d2v)\npro_data = pro_proc.transform(pro_train, que_train, ans_train, tag_pro)","e241030a":"# construct dataframe used to extract positive pairs\npairs_df = questions.merge(answers, left_on='questions_id', right_on='answers_question_id') \\\n    .merge(professionals, left_on='answers_author_id', right_on='professionals_id') \\\n    .merge(students, left_on='questions_author_id', right_on='students_id')\n\npairs_df = pairs_df[['questions_id', 'students_id', 'professionals_id', 'answers_date_added']]\n\n# extract positive pairs\npos_pairs = list(pairs_df.loc[pairs_df['answers_date_added'] < SPLIT_DATE].itertuples(index=False, name=None))\n\n# extract professional answers for activity filters\npro_answers = pairs_df.drop(columns=['students_id']).set_index('professionals_id')","0e4080c7":"pairs_df.head(1)","f373627a":"qa_data = questions.merge(answers, left_on='questions_id', right_on='answers_question_id')\n\nque_age = pd.to_datetime(qa_data['answers_date_added']) - pd.to_datetime(qa_data['questions_date_added'])\nque_age = que_age.apply(lambda x: x.days + np.round(x.seconds \/ 3600) \/ 24)\n\n\n#plt.figure(figsize=(20,10))\nfig, ax = plt.subplots(figsize=(12,6))\n\n# Select only ages in range [3, 100] and plot their historgram\ncut_que_age = que_age[(que_age < 100) & (que_age > 3)]\ncut_que_age.hist(bins=100, density=True)\n\n# Set parameter lambda of exponential distribution\nmean = 30\nlambd = 1 \/ mean\n\n# Plot exponential distribution\nx = np.linspace(0, 100, 1000)\ny = lambd * np.exp(-lambd * x)\n\nax.plot(x, y)\nax.set_xlim(3, 100)\n\nplt.show()","0e543392":"class BatchGenerator(keras.utils.Sequence):\n    \"\"\"\n    Class to ingest data from pre-processed DataFrames to model\n    in form of batches of NumPy arrays\n    \"\"\"\n    \n    exp_mean = 30\n    \n    def __init__(self, que: pd.DataFrame, stu: pd.DataFrame, pro: pd.DataFrame,\n                 batch_size: int, pos_pairs: list, nonneg_pairs: list, pro_dates: dict):\n        \"\"\"\n        :param que: pre-processed questions data\n        :param stu: pre-processed students data\n        :param pro: pre-processed professionals data\n        :param batch_size: actually, half of the real batch size\n        Number of both positive and negative pairs present in generated batch\n        :param pos_pairs: tuples of question, student and professional, which form positive pair\n        (professional answered on the given question from corresponding student)\n        :param nonneg_pairs: tuples of question, student and professional, which are known to form a positive pair.\n        Superset of pos_pairs, used in sampling of negative pairs\n        :param pro_dates: mappings from professional's id to his registration date\n        \"\"\"\n        self.batch_size = batch_size\n\n        # extract mappings from question's id to question's date and features\n        que_ar = que.values\n        self.que_feat = {que_ar[i, 0]: que_ar[i, 2:] for i in range(que_ar.shape[0])}\n        self.que_time = {que_ar[i, 0]: pd.Timestamp(que_ar[i, 1]) for i in range(que_ar.shape[0])}\n\n        self.pos_pairs = pos_pairs\n        self.on_epoch_end()  # shuffle pos_pairs\n        self.nonneg_pairs = {(que, stu, pro) for que, stu, pro, time in nonneg_pairs}\n\n        # these lists are used in sampling of negative pairs\n        self.ques_stus_times = [(que, stu, self.que_time[que]) for que, stu, pro, time in pos_pairs]\n\n        self.pros = np.array([pro for que, stu, pro, time in nonneg_pairs])\n        self.pros_times = np.array([pro_dates[pro] for que, stu, pro, time in nonneg_pairs])\n\n        # simultaneously sort two arrays containing professional features\n        sorted_args = np.argsort(self.pros_times)\n        self.pros = self.pros[sorted_args]\n        self.pros_times = self.pros_times[sorted_args]\n\n        # extract mappings from student's id to student's date and features\n        self.stu_feat = {}\n        self.stu_time = {}\n        for stu_id, group in stu.groupby('students_id'):\n            group_ar = group.values[:, 1:]\n            self.stu_feat[stu_id] = np.array([group_ar[i, 1:] for i in range(group_ar.shape[0])])\n            self.stu_time[stu_id] = np.array([group_ar[i, 0] for i in range(group_ar.shape[0])])\n\n        # extract mappings from professional's id to professional's date and features\n        self.pro_feat = {}\n        self.pro_time = {}\n        for pro_id, group in pro.groupby('professionals_id'):\n            group_ar = group.values[:, 1:]\n            self.pro_feat[pro_id] = np.array([group_ar[i, 1:] for i in range(group_ar.shape[0])])\n            self.pro_time[pro_id] = np.array([group_ar[i, 0] for i in range(group_ar.shape[0])])\n\n    def __len__(self):\n        return len(self.pos_pairs) \/\/ self.batch_size\n\n    @staticmethod\n    def __find(feat_ar: np.ndarray, time_ar: np.ndarray, search_time):\n        pos = np.searchsorted(time_ar[1:], search_time)\n        assert time_ar[pos] is pd.NaT or time_ar[pos] < search_time\n        return feat_ar[pos]\n\n    def __convert(self, pairs: list) -> (np.ndarray, np.ndarray):\n        \"\"\"\n        Convert list of pairs of ids to NumPy arrays\n        of question and professionals features\n        \"\"\"\n        x_que, x_pro, current_times = [], [], []\n        for que, stu, pro, current_time in pairs:\n            que_data = self.que_feat[que]\n\n            # find student's and professional's feature at current time\n            stu_data = BatchGenerator.__find(self.stu_feat[stu], self.stu_time[stu], current_time)\n            pro_data = BatchGenerator.__find(self.pro_feat[pro], self.pro_time[pro], current_time)\n\n            # prepare current time as feature itself\n            current_time = current_time.year + (current_time.dayofyear + current_time.hour \/ 24) \/ 365\n            current_times.append(current_time)\n\n            x_que.append(np.hstack([stu_data, que_data]))\n            x_pro.append(pro_data)\n\n        # and append them to both questions and professionals\n        return np.vstack(x_que), np.vstack(x_pro)\n\n    def __getitem__(self, index):\n        \"\"\"\n        Generate the batch\n        \"\"\"\n        pos_pairs = self.pos_pairs[self.batch_size * index: self.batch_size * (index + 1)]\n        neg_pairs = []\n\n        for i in range(len(pos_pairs)):\n            while True:\n                # sample question, its student and time\n                que, stu, zero = random.choice(self.ques_stus_times)\n                # calculate shift between question's and current time\n                shift = np.random.exponential(BatchGenerator.exp_mean)\n                current_time = zero + pd.Timedelta(int(shift * 24 * 60), 'm')\n                # find number of professionals with registration date before current time\n                i = np.searchsorted(self.pros_times, current_time)\n                if i != 0:\n                    break\n\n            while True:\n                # sample professional for negative pair\n                pro = random.choice(self.pros[:i])\n                # check if he doesn't form a positive pair\n                if (que, stu, pro) not in self.nonneg_pairs:\n                    neg_pairs.append((que, stu, pro, current_time))\n                    break\n\n        # convert lists of pairs to NumPy arrays of features\n        x_pos_que, x_pos_pro = self.__convert(pos_pairs)\n        x_neg_que, x_neg_pro = self.__convert(neg_pairs)\n\n        # return the data in its final form\n        return [np.vstack([x_pos_que, x_neg_que]), np.vstack([x_pos_pro, x_neg_pro])], \\\n               np.vstack([np.ones((len(x_pos_que), 1)), np.zeros((len(x_neg_que), 1))])\n\n    def on_epoch_end(self):\n        # shuffle positive pairs\n        self.pos_pairs = random.sample(self.pos_pairs, len(self.pos_pairs))","f5f5dc36":"# mappings from professional's id to his registration date. Used in batch generator\npro_to_date = {row['professionals_id']: row['professionals_date_joined'] for i, row in professionals.iterrows()}","7f0099a2":"bg = BatchGenerator(que_data, stu_data, pro_data, 64, pos_pairs, pos_pairs, pro_to_date)","d547d352":"import tensorflow as tf\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Lambda, Embedding, Concatenate, Layer\nfrom keras.optimizers import Adam\n\n\ndef l2_reg_last_n(alpha: float, n: int):\n    \"\"\"\n    Adds L2 regularization on weights connected with the last n features with multiplier alpha\n    \"\"\"\n    return lambda w: alpha * tf.reduce_mean(tf.square(w[-n:, :]))\n\n\ndef categorize(inputs: tf.Tensor, emb_input_dims: list, emb_output_dims: list):\n    \"\"\"\n    Replaces categorical features with trainable embeddings\n\n    :param inputs: tensor with encoded categorical features in first columns\n    :param emb_input_dims: number of unique classes in categorical features\n    :param emb_output_dims: embedding dimensions of categorical features\n    :return: transformed tensor\n    \"\"\"\n    n_embs = len(emb_input_dims)\n\n    if n_embs > 0:\n        embs = []\n\n        # iterate over categorical features\n        for i, nunique, dim in zip(range(n_embs), emb_input_dims, emb_output_dims):\n            # separate their values with Lambda layer\n            tmp = Lambda(lambda x: x[:, i])(inputs)\n            # pass them through Embedding layer\n            embs.append(Embedding(nunique, dim)(tmp))\n\n        # pass all the numerical features directly\n        embs.append(Lambda(lambda x: x[:, n_embs:])(inputs))\n        # and concatenate them\n        outputs = Concatenate()(embs)\n    else:\n        outputs = inputs\n\n    return outputs\n\n\nclass Encoder(Model):\n    \"\"\"\n    Model for extraction of high-level feature vector from question or professional\n    \"\"\"\n\n    def __init__(self, input_dim: int, inter_dim: int, output_dim: int, emb_input_dims: list, emb_output_dims: list,\n                 reg: float = 0.0):\n        \"\"\"\n        :param input_dim: dimension of raw feature vector\n        :param inter_dim: dimension of intermediate layer\n        :param output_dim: dimension of computed high-level feature vector\n        :param emb_input_dims: number of unique classes in categorical features\n        :param emb_output_dims: embedding dimensions of categorical features\n        :param reg:\n        \"\"\"\n        self.inputs = Input((input_dim,))\n        self.categorized = categorize(self.inputs, emb_input_dims, emb_output_dims)\n\n        # here goes main dense layers\n        self.inter = Dense(inter_dim, activation='tanh',\n                           kernel_regularizer=l2_reg_last_n(reg, 10))(self.categorized)\n\n        self.outputs = Dense(output_dim)(self.inter)\n\n        super().__init__(self.inputs, self.outputs)\n\n\nclass DistanceModel(Model):\n    \"\"\"\n    Main model which combines two encoders (for questions and professionals),\n    calculates distance between high-level feature vectors and applies activation\n    \"\"\"\n\n    def __init__(self, que_dim: int, que_input_embs: list, que_output_embs: list,\n                 pro_dim: int, pro_input_embs: list, pro_output_embs: list,\n                 inter_dim: int, output_dim: int):\n        \"\"\"\n        :param que_dim: dimension of question's raw feature vector\n        :param que_input_embs: number of unique classes in question's categorical features\n        :param que_output_embs: embedding dimensions of question's categorical features\n        :param pro_dim: dimension of professional's raw feature vector\n        :param pro_input_embs: number of unique classes in professional's categorical features\n        :param pro_output_embs: embedding dimensions of professional's categorical features\n        :param inter_dim: dimension of Encoder's intermediate layer\n        :param output_dim: dimension of high-level feature vectors\n        \"\"\"\n        super().__init__()\n\n        # build an Encoder model for questions\n        self.que_model = Encoder(que_dim, inter_dim, output_dim, que_input_embs, que_output_embs, reg=2.0)\n        # same for professionals\n        self.pro_model = Encoder(pro_dim, inter_dim, output_dim, pro_input_embs, pro_output_embs, reg=0.2)\n\n        # calculate distance between high-level feature vectors\n        self.merged = Lambda(lambda x: tf.reduce_sum(tf.square(x[0] - x[1]), axis=-1))(\n            [self.que_model.outputs[0], self.pro_model.outputs[0]])\n        # and apply activation - e^-x here, actually\n        self.outputs = Lambda(lambda x: tf.reshape(tf.exp(-self.merged), (-1, 1)))(self.merged)\n\n        super().__init__([self.que_model.inputs[0], self.pro_model.inputs[0]], self.outputs)","b45599a9":"model = DistanceModel(que_dim=len(que_data.columns) - 2 + len(stu_data.columns) - 2,\n                      que_input_embs=[102, 42], que_output_embs=[2, 2],\n                      pro_dim=len(pro_data.columns) - 2,\n                      pro_input_embs=[102, 102, 42], pro_output_embs=[2, 2, 2],\n                      inter_dim=20, output_dim=10)\nmodel.summary()","d480c2fe":"for lr, epochs in zip([0.01, 0.001, 0.0001, 0.00001], [5, 10, 10, 5]):\n    model.compile(Adam(lr=lr), loss='binary_crossentropy', metrics=['accuracy'])\n    model.fit_generator(bg, epochs=epochs, verbose=2)","4bda9cc2":"from sklearn.utils import shuffle\n\ndef permutation_importance(model: keras.models.Model, x_que: np.ndarray, x_pro: np.ndarray, y: np.ndarray,\n                           fn: dict, n_trials: int) -> pd.DataFrame:\n    \"\"\"\n    Calculate model feature importance via random permutations of feature values\n\n    :param model: model to evaluate\n    :param x_que: pre-processed questions data\n    :param x_pro: pre-processed professionals data\n    :param y: target labels\n    :param fn: dict with feature names of both questions and professionals\n    :param n_trials: number of shuffles for each feature\n    :return: Pandas DataFrame with importance of each feature\n    \"\"\"\n    # model performance on normal, non-shuffled data\n    base_loss, base_acc = model.evaluate([x_que, x_pro], y)\n    losses = []\n    for i, name in enumerate(fn['que'] + fn['pro']):\n        loss = 0\n        for j in range(n_trials):\n            x_que_i, x_pro_i = copy.deepcopy(x_que), copy.deepcopy(x_pro)\n\n            if name in fn['que']:\n                x_que_i[:, i] = shuffle(x_que_i[:, i])\n            else:\n                x_pro_i[:, i - len(fn['que'])] = shuffle(x_pro_i[:, i - len(fn['que'])])\n            loss += model.evaluate([x_que_i, x_pro_i], y, verbose=0)[0]\n\n        losses.append(loss \/ n_trials)\n\n    fi = pd.DataFrame({'importance': losses}, index=fn['que'] + fn['pro'])\n    fi.sort_values(by='importance', inplace=True, ascending=True)\n    fi['importance'] -= base_loss\n\n    return fi\n\n\ndef plot_fi(fi, title='Feature importances via shuffle', xlabel='Change in loss after shuffling feature\\'s values'):\n    \"\"\"\n    Nicely plot Pandas DataFrame with feature importance\n    \"\"\"\n    def get_color(feature: str):\n        if feature.startswith('que'):\n            if '_emb_' in feature:\n                return 'royalblue'\n            else:\n                return 'cornflowerblue'\n        elif feature.startswith('pro'):\n            if '_emb_' in feature:\n                return 'firebrick'\n            else:\n                return 'indianred'\n        else:\n            return 'seagreen'\n\n    fi['color'] = fi.index.map(get_color)\n    fig, ax = plt.subplots(figsize=(8, 20))\n    plt.barh(fi.index, fi.importance, color=fi.color)\n    plt.title(title, fontsize=20)\n    plt.xlabel(xlabel, fontsize=15)\n    ax.yaxis.tick_right()\n    plt.show()","99429a67":"# dummy batch generator used to extract single big batch of data to calculate feature importance\nbg = BatchGenerator(que_data, stu_data, pro_data, 1024, pos_pairs, pos_pairs, pro_to_date)\n\n# dict with descriptions of feature names, used for visualization of feature importance\nfn = {\"que\": list(stu_data.columns[2:]) + list(que_data.columns[2:]),\n      \"pro\": list(pro_data.columns[2:])}\n\n# calculate and plot feature importance\nfi = permutation_importance(model, bg[0][0][0], bg[0][0][1], bg[0][1], fn, n_trials=3)\nplot_fi(fi)","f2861253":"def vis_model_emb(layer, names, title):\n    \"\"\"\n    Visualize embeddings of a single feature\n    \"\"\"\n    emb = layer.get_weights()[0]\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax.scatter(emb[:, 0], emb[:, 1], s=90, c='#69f0ff')\n    for i, name in enumerate(names):\n        ax.annotate(name, (emb[i, 0], emb[i, 1]), color='w')\n    plt.title(title)","d2005dc2":"vis_model_emb(model.get_layer('embedding_2'), stu_proc.pp['students_state'].classes_, \"Student's states\")","536939b3":"vis_model_emb(model.get_layer('embedding_5'), pro_proc.pp['professionals_state'].classes_, \"Professional's states\")","50c57b64":"# non-negative pairs are all known positive pairs to the moment\nnonneg_pairs = pos_pairs\n\n# extract positive pairs\npos_pairs = list(pairs_df.loc[pairs_df['answers_date_added'] >= SPLIT_DATE].itertuples(index=False, name=None))\nnonneg_pairs += pos_pairs\n\n# extract and preprocess feature for all three main entities\n\nque_proc = QueProc(tag_embs, ques_d2v, lda_dic, lda_tfidf, lda_model)\nque_data = que_proc.transform(questions, tag_que)\n\nstu_proc = StuProc()\nstu_data = stu_proc.transform(students, questions, answers)\n\npro_proc = ProProc(tag_embs, ind_embs, head_d2v, ques_d2v)\npro_data = pro_proc.transform(professionals, questions, answers, tag_pro)\n\n# initialize batch generator\nbg = BatchGenerator(que_data, stu_data, pro_data, 64, pos_pairs, nonneg_pairs, pro_to_date)","870fb686":"loss, acc = model.evaluate_generator(bg)\nprint(f'Loss: {loss}, accuracy: {acc}')","be847b75":"from sklearn.neighbors import KDTree\n\nclass Predictor:\n    \"\"\"\n    Class for handling closest professionals or questions queries\n    \"\"\"\n\n    def __init__(self, model: keras.Model, que_data: pd.DataFrame, stu_data: pd.DataFrame, pro_data: pd.DataFrame,\n                 que_proc: QueProc, pro_proc: ProProc, que_to_stu: dict, pos_pairs: list):\n        \"\"\"\n        :param model: compiled Keras model\n        :param que_data: processed questions's data\n        :param stu_data: processed student's data\n        :param pro_data: processed professional's data\n        :param que_proc: question's data processor\n        :param pro_proc: professional's data processor\n        :param que_to_stu: mappings from question's id to its author id\n        :param pos_pairs: list of positive question-student-professional-time pairs\n        \"\"\"\n        self.model = model\n\n        # construct mappings from entity id to features\n        self.que_dict = {row.values[0]: row.values[2:] for i, row in que_data.iterrows()}\n        self.stu_dict = {stu: group.values[-1, 2:] for stu, group in stu_data.groupby('students_id')}\n        self.pro_dict = {pro: group.values[-1, 2:] for pro, group in pro_data.groupby('professionals_id')}\n        \n        self.entity_to_paired = dict()\n        \n        # construct mappings from entity to other entities it was in positive pair\n        for que, stu, pro, time in pos_pairs:\n            if que not in self.entity_to_paired:\n                self.entity_to_paired[que] = {pro}\n            else:\n                self.entity_to_paired[que].add(pro)\n                \n            if pro not in self.entity_to_paired:\n                self.entity_to_paired[pro] = {que}\n            else:\n                self.entity_to_paired[pro].add(que)\n\n        # form final features for 1all known questions and professionals\n\n        que_feat, que_ids, pro_feat, pro_ids = [], [], [], []\n\n        for que in self.que_dict.keys():\n            cur_stu = que_to_stu[que]\n            if cur_stu in self.stu_dict:\n                # actual question's features are both question and student's features\n                que_feat.append(np.hstack([self.stu_dict[cur_stu], self.que_dict[que]]))\n                que_ids.append(que)\n\n        for pro in self.pro_dict.keys():\n            pro_feat.append(self.pro_dict[pro])\n            pro_ids.append(pro)\n\n        self.pro_feat = np.vstack(pro_feat)\n        self.pro_ids = np.vstack(pro_ids)\n        self.que_feat = np.vstack(que_feat)\n        self.que_ids = np.vstack(que_ids)\n\n        # create two encoders\n        self.que_model = model.que_model\n        self.pro_model = model.pro_model\n\n        # compute latent vectors for questions and professionals\n        self.que_lat_vecs = self.que_model.predict(self.que_feat)\n        self.pro_lat_vecs = self.pro_model.predict(self.pro_feat)\n\n        # create KDTree trees from question and professional latent vectors\n        self.que_tree = KDTree(self.que_lat_vecs)\n        self.pro_tree = KDTree(self.pro_lat_vecs)\n\n        # initialize preprocessors\n        self.que_proc = que_proc\n        self.pro_proc = pro_proc\n\n    def __get_que_latent(self, que_df: pd.DataFrame, que_tags: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Get latent vectors for questions in raw format\n        \"\"\"\n        que_df['questions_date_added'] = pd.to_datetime(que_df['questions_date_added'])\n\n        # extract and preprocess question's features\n        que_feat = self.que_proc.transform(que_df, que_tags).values[:, 2:]\n\n        # actual question's features are both question and student's features\n        stu_feat = np.vstack([self.stu_dict[stu] for stu in que_df['questions_author_id']])\n        que_feat = np.hstack([stu_feat, que_feat])\n\n        # encode question's data to get latent representation\n        lat_vecs = self.que_model.predict(que_feat)\n\n        return lat_vecs\n\n    def __get_pro_latent(self, pro_df: pd.DataFrame, que_df: pd.DataFrame, ans_df: pd.DataFrame,\n                         pro_tags: pd.DataFrame) -> np.ndarray:\n        \"\"\"\n        Get latent vectors for professionals in raw format\n        \"\"\"\n        pro_df['professionals_date_joined'] = pd.to_datetime(pro_df['professionals_date_joined'])\n        que_df['questions_date_added'] = pd.to_datetime(que_df['questions_date_added'])\n        ans_df['answers_date_added'] = pd.to_datetime(ans_df['answers_date_added'])\n\n        # extract and preprocess professional's features\n        pro_feat = self.pro_proc.transform(pro_df, que_df, ans_df, pro_tags)\n\n        # select the last available version of professional's features\n        pro_feat = pro_feat.groupby('professionals_id').last().values[:, 1:]\n        \n        # encode professional's data to get latent representation\n        lat_vecs = self.pro_model.predict(pro_feat)\n\n        return lat_vecs\n\n    def __construct_df(self, ids, sims, scores):\n        scores = np.around(scores, 4)\n        tuples = []\n        for i, cur_id in enumerate(ids):\n            for j, sim in enumerate(sims[i]):\n                if sim[0] not in self.entity_to_paired.get(cur_id, {}):\n                    tuples.append((cur_id, sim[0], scores[i, j]))\n        score_df = pd.DataFrame(tuples, columns=['id', 'match_id', 'match_score'])\n        return score_df\n\n    def __get_ques_by_latent(self, ids: np.ndarray, lat_vecs: np.ndarray, top: int) -> pd.DataFrame:\n        \"\"\"\n        Get top questions with most similar latent representations to given vectors\n        \"\"\"\n        dists, ques = self.que_tree.query(lat_vecs, k=top)\n        ques = self.que_ids[ques]\n        scores = np.exp(-dists)\n        return self.__construct_df(ids, ques, scores)\n\n    def __get_pros_by_latent(self, ids: np.ndarray, lat_vecs: np.ndarray, top: int) -> pd.DataFrame:\n        \"\"\"\n        Get top professionals with most similar latent representations to given vectors\n        \"\"\"\n        dists, pros = self.pro_tree.query(lat_vecs, k=top)\n        pros = self.pro_ids[pros]\n        scores = np.exp(-dists)\n        return self.__construct_df(ids, pros, scores)\n\n    def find_pros_by_que(self, que_df: pd.DataFrame, que_tags: pd.DataFrame, top: int = 10) -> pd.DataFrame:\n        \"\"\"\n        Get top professionals with most similar internal representation to given questions\n\n        :param que_df: question's data in raw format\n        :param que_tags: questions's tags in raw format\n        :param top: number of professionals for each question to return\n        :return: dataframe of question's ids, matched professional's ids and similarity scores\n        \"\"\"\n        lat_vecs = self.__get_que_latent(que_df, que_tags)\n        return self.__get_pros_by_latent(que_df['questions_id'].values, lat_vecs, top)\n\n    def find_ques_by_que(self, que_df: pd.DataFrame, que_tags: pd.DataFrame, top: int = 10) -> pd.DataFrame:\n        \"\"\"\n        Get top questions with most similar internal representation to given questions\n\n        :param que_df: question's data in raw format\n        :param que_tags: questions's tags in raw format\n        :param top: number of questions for each question to return\n        :return: dataframe of question's ids, matched question's ids and similarity scores\n        \"\"\"\n        lat_vecs = self.__get_que_latent(que_df, que_tags)\n        return self.__get_ques_by_latent(que_df['questions_id'].values, lat_vecs, top)\n\n    def find_ques_by_pro(self, pro_df: pd.DataFrame, que_df: pd.DataFrame, ans_df: pd.DataFrame,\n                         pro_tags: pd.DataFrame, top: int = 10) -> pd.DataFrame:\n        \"\"\"\n        Get top questions with most similar internal representation to given professional\n\n        :param pro_df: professional's data in raw format\n        :param que_df: question's data in raw format\n        :param ans_df: answer's data in raw format\n        :param pro_tags: professional's tags data in raw format\n        :param top: number of questions for each professional to return\n        :return: dataframe of professional's ids, matched question's ids and similarity scores\n        \"\"\"\n        lat_vecs = self.__get_pro_latent(pro_df, que_df, ans_df, pro_tags)\n        return self.__get_ques_by_latent(pro_df['professionals_id'].values, lat_vecs, top)\n\n    def find_pros_by_pro(self, pro_df: pd.DataFrame, que_df: pd.DataFrame, ans_df: pd.DataFrame,\n                         pro_tags: pd.DataFrame, top: int = 10) -> pd.DataFrame:\n        \"\"\"\n        Get top professionals with most similar internal representation to given professional\n\n        :param pro_df: professional's data in raw format\n        :param que_df: question's data in raw format\n        :param ans_df: answer's data in raw format\n        :param pro_tags: professional's tags data in raw format\n        :param top: number of questions for each professional to return\n        :return: dataframe of professional's ids, matched professional's ids and similarity scores\n        \"\"\"\n        lat_vecs = self.__get_pro_latent(pro_df, que_df, ans_df, pro_tags)\n        return self.__get_pros_by_latent(pro_df['professionals_id'].values, lat_vecs, top)","7c53bdac":"class Formatter:\n    \"\"\"\n    Class with useful for Predictor input\/output functionality\n    \"\"\"\n\n    def __init__(self, data_path: str):\n        pro = pd.read_csv(data_path + 'professionals.csv')\n        que = pd.read_csv(data_path + 'questions.csv')\n\n        tags = pd.read_csv(data_path + 'tags.csv')\n        tag_users = pd.read_csv(data_path + 'tag_users.csv')\n        tag_que = pd.read_csv(data_path + 'tag_questions.csv')\n\n        tag_merged = tags.merge(tag_users, left_on='tags_tag_id', right_on='tag_users_tag_id')\n        tags_grouped = tag_merged.groupby('tag_users_user_id').agg(lambda x: ' '.join(x))[['tags_tag_name']]\n        self.pro = pro.merge(tags_grouped, left_on='professionals_id', right_index=True, how='left')\n\n        tag_merged = tags.merge(tag_que, left_on='tags_tag_id', right_on='tag_questions_tag_id')\n        tags_grouped = tag_merged.groupby('tag_questions_question_id').agg(lambda x: ' '.join(x))[['tags_tag_name']]\n        self.que = que.merge(tags_grouped, left_on='questions_id', right_index=True, how='left')\n\n    def get_que(self, scores: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Append all the question's data to question's scoring dataframe from Predictor\n\n        :param scores: result of similar questions query on Predictor object\n        :return: extended dataframe\n        \"\"\"\n        return self.que.merge(scores, left_on='questions_id', right_on='match_id').sort_values('match_score',\n                                                                                               ascending=False)\n\n    def get_pro(self, scores: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Append all the professional's data to professional's scoring dataframe from Predictor\n\n        :param scores: result of similar professionals query on Predictor object\n        :return: extended dataframe\n        \"\"\"\n        return self.pro.merge(scores, left_on='professionals_id', right_on='match_id').sort_values('match_score',\n                                                                                                   ascending=False)\n\n    @staticmethod\n    def __convert_tuples(ids, tags):\n        tuples = []\n        for i, tgs in enumerate(tags):\n            que = ids[i]\n            for tag in tgs.split(' '):\n                tuples.append((que, tag))\n        return tuples\n\n    @staticmethod\n    def convert_que_dict(que_dict: dict) -> (pd.DataFrame, pd.DataFrame):\n        \"\"\"\n        Converts dictionary of question data into desired form\n        :param que_dict: dictionary of question data\n        \"\"\"\n        # get DataFrame from dict\n        que_df = pd.DataFrame.from_dict(que_dict)\n\n        # create question-tag tuples\n        tuples = Formatter.__convert_tuples(que_df['questions_id'].values, que_df['questions_tags'].values)\n\n        # create DataFrame from tuples\n        que_tags = pd.DataFrame(tuples, columns=['tag_questions_question_id', 'tags_tag_name'])\n        que_df.drop(columns='questions_tags', inplace=True)\n\n        que_tags['tags_tag_name'] = que_tags['tags_tag_name'].apply(lambda x: tp.process(x, allow_stopwords=True))\n        que_df['questions_title'] = que_df['questions_title'].apply(tp.process)\n        que_df['questions_body'] = que_df['questions_body'].apply(tp.process)\n        que_df['questions_whole'] = que_df['questions_title'] + ' ' + que_df['questions_body']\n        \n        return que_df, que_tags\n\n    @staticmethod\n    def convert_pro_dict(pro_dict: dict) -> (pd.DataFrame, pd.DataFrame):\n        \"\"\"\n        Converts dictionary of professional data into desired form\n        :param pro_dict: dictionary of professional data\n        \"\"\"\n        # get DataFrame from dict\n        pro_df = pd.DataFrame.from_dict(pro_dict)\n        pros = pro_df['professionals_id'].values\n\n        # create professional-tag tuples\n        tuples = Formatter.__convert_tuples(pro_df['professionals_id'].values,\n                                            pro_df['professionals_subscribed_tags'].values)\n\n        # create DataFrame from tuples\n        pro_tags = pd.DataFrame(tuples, columns=['tag_users_user_id', 'tags_tag_name'])\n        pro_df.drop(columns='professionals_subscribed_tags', inplace=True)\n        \n        pro_tags['tags_tag_name'] = pro_tags['tags_tag_name'].apply(lambda x: tp.process(x, allow_stopwords=True))\n        pro_df['professionals_headline'] = pro_df['professionals_headline'].apply(tp.process)\n        pro_df['professionals_industry'] = pro_df['professionals_industry'].apply(tp.process)\n\n        return pro_df, pro_tags","af631e04":"# mappings from question's id to its author id. Used in Predictor\nque_to_stu = {row['questions_id']: row['questions_author_id'] for i, row in questions.iterrows()}","d844e366":"predictor = Predictor(model, que_data, stu_data, pro_data, que_proc, pro_proc, que_to_stu, nonneg_pairs)","ad29ba71":"formatter = Formatter(DATA_PATH)","af42a946":"que_sample_dict = {\n    'questions_id': ['0'],\n    'questions_author_id': ['02946e467bab4fd794e42f9670cb4279'],\n    'questions_date_added': ['2017-07-29 13:30:50 UTC+0000'],\n    'questions_title': ['I want to study law but not sure what subjects need to be taken,so need some advice on that.'],\n    'questions_body': ['#law-practice #lawyer #career-details'],\n    'questions_tags': ['lawyer law-practice career-details']\n}","b22d69c4":"que_sample_df, que_sample_tags = formatter.convert_que_dict(que_sample_dict)","6fb5862c":"tmp = predictor.find_ques_by_que(que_sample_df, que_sample_tags)\nformatter.get_que(tmp)","c964a273":"tmp = predictor.find_pros_by_que(que_sample_df, que_sample_tags)\nformatter.get_pro(tmp)","d704796d":"pro_sample_dict = {\n    'professionals_id': ['eae09bbc30e34f008e10d5aa70d521b2'],\n    'professionals_location': ['Narberth, Pennsylvania'],\n    'professionals_industry': ['Veterinary'],\n    'professionals_headline': ['Veterinarian'],\n    'professionals_date_joined': ['2017-09-26 18:10:23 UTC+0000'],\n    'professionals_subscribed_tags': ['veterinary #animal']\n}","2005d89f":"pro_sample_df, pro_sample_tags = formatter.convert_pro_dict(pro_sample_dict)","a3e2a35f":"tmp = predictor.find_ques_by_pro(pro_sample_df, questions, answers, pro_sample_tags)\nformatter.get_que(tmp)","1d3338be":"tmp = predictor.find_pros_by_pro(pro_sample_df, questions, answers, pro_sample_tags)\nformatter.get_pro(tmp)","69d369ec":"def activity_filter(ans_dates: np.ndarray, cur_date: np.datetime64, tail_prob: np.double=0.05,\n                    min_days: np.double=0.5, max_days: np.double=15):\n    \"\"\"\n    Check whether a professional is currently active or not\n    by estimating how likely distance from current date to\n    previous answer date is to come from t-distribution with\n    parameters estimated from professional's answer dates.\n    \n    :param ans_dates: ndarray of professional's answer dates\n    :param cur_date: np.datetime64 object containing current date\n    :param tail_prob: tail probability of t-distribution\n    :param min_days: minimum number of days after previous answer date\n    :param max_days: maximum number of days after previous answer date\n    \"\"\"\n    # get realization of some random variable,\n    # which is distance between current date and previos answer date\n    realiz = (cur_date - ans_dates[-1]) \/ np.timedelta64(1, 'D')\n    \n    # compute intervals between answers if there is at least two answers so far\n    # (the first date in ans_dates is registration date)\n    if ans_dates.size >= 3:\n        int_lens = np.diff(ans_dates) \/ np.timedelta64(1, 'D')\n    else:\n        # check if registration or previous answer was\n        # more than min_days and less than max_days ago\n        return True if (realiz > min_days and realiz < max_days) else False\n    \n    # clip interval lengths to be in range [min_days, max_days]\n    int_lens = np.clip(int_lens, min_days, max_days)\n    \n    # estimate parameters of t-distribution\n    df = int_lens.size - 1\n    loc = int_lens.mean()\n    scale = max(int_lens.std(ddof=1), min_days)\n    \n    # estimate probability of this random variable coming from t-distibution\n    prob = t.cdf(realiz, df=df, loc=loc, scale=scale)    \n    if prob > 0.5:\n        prob = 1 - prob\n    \n    # if this probability is greater than probability of coming from one\n    # tail of t-distribution, or previous answer was less than max_days ago,\n    # and also it's been at least min_days since previous answer,\n    # than conclude that professional is active\n    if (prob > tail_prob \/ 2 or realiz < max_days) and realiz > min_days:\n        return True\n    return False","d267a1db":"pro_answer_dates = pro_data.set_index('professionals_id')['professionals_time']\npro_answer_dates.fillna(pro_to_date, inplace=True)","067a77ee":"que_dict = {\n    'questions_id': ['6351c23f2e144b359c3301d40b3d81ef'],\n    'questions_author_id': ['4627ecfaf1814d2e8935ed3e3cd4b88b'],\n    'questions_date_added': ['2018-08-14 20:10:09 UTC+0000'],\n    'questions_title': ['What major is best for an aspiring Occupational Therapist?'],\n    'questions_body': ['#majors #occupationaltherapist'],\n    'questions_tags': ['majors occupationaltherapist']\n}","34c570a5":"que_df, que_tags = formatter.convert_que_dict(que_dict)","396d973e":"top_n = 28000\ncurrent_date = np.datetime64('2018-08-16 00:00:00')","5f27a5d7":"pred_pros = predictor.find_pros_by_que(que_df, que_tags, top=top_n)","80754d37":"active_pros = []\n\nfor pro in pred_pros['match_id'].iloc[:top_n]:\n    try:\n        answer_dates = pro_answer_dates.loc[pro].values\n    except:\n        continue\n    \n    answer_dates = answer_dates[answer_dates < current_date]\n    \n    if answer_dates.size > 0 and activity_filter(answer_dates, current_date):\n        active_pros.append(pro)\n\nactive_pros = np.array(active_pros)","161f899a":"print('Amount of active professionals among top {} is {}.'.format(top_n, len(active_pros)))","288615e8":"pred_pros.head(10)","51ed4af5":"active_pros = pred_pros.merge(pd.DataFrame({'match_id': active_pros}), on='match_id')\nactive_pros.head(10)","0f384d22":"def spam_filter(question_id: str, email_ques: np.ndarray, previous_email_date: np.datetime64,\n                   cur_date: np.datetime64, min_days: np.double=0.5):\n    \"\"\"\n    Do not send an email if previous email was sent less than min_days ago\n    or if professional was already email-notified about the question.\n    \n    :param que: question about which email notification is going to be sent\n    :param email_ques: ndarray of email-notified questions\n    :param previous_email_date: np.datetime64 object containing previous email date\n    :param cur_date: np.datetime64 object containing current date\n    :param min_days: minimum number of days after previous email notification\n    \"\"\"\n    if (cur_date - previous_email_date) \/ np.timedelta64(1, 'D') < min_days or question_id in email_ques:\n        return False\n    return True","6c5fa393":"def email_filter(que: str, email_ques: np.ndarray, email_dates: np.ndarray,\n                 ans_ques: np.ndarray, ans_dates: np.ndarray,\n                 cur_date: np.datetime64, offset_days: np.double,\n                 min_days: np.double=0.5, max_days: np.double=7, thresh: np.double=0.1):\n    \"\"\"\n    Makes a decision about whether to send email to active professional or not\n    by computing the coefficient based on fraction of email-notified questions\n    that were answered and on distribution of intervals between\n    email notification and answer date.\n    \n    :param que: question about which email notification is going to be sent\n    :param email_ques: ndarray of email-notified questions\n    :param email_dates: ndarray of dates email notifications where sent\n    :param ans_ques: ndarray of answered questions\n    :param ans_dates: ndarray of answer dates\n    :param cur_date: np.datetime64 object containing current date\n    :param offset_days: period (in days) during which we want que to be answered\n    :param min_days: minimum number of days after previous email notification\n    :param max_days: number of days on which to clip interval lengths\n    :param thresh: threshold for F1 score\n    \"\"\"\n    # do not send email if previous email was sent less than min_days ago\n    # or if professional was already email-notified about this question\n    if not spam_filter(que, email_ques, email_dates.max(), cur_date, min_days):\n        return False\n    \n    # select answered email-notified questions, and also email and answer dates assosiated to them\n    ques, email_idx, ans_idx = np.intersect1d(email_ques, ans_ques, assume_unique=False, return_indices=True)\n    email_dates = email_dates[email_idx]\n    ans_dates = ans_dates[ans_idx]\n    \n    # do not send email if less than 2 email-notified questions were answered\n    if ques.size < 2:\n        return False\n    \n    # compute fraction of email-notified questions that were answered\n    email_frac = ques.size \/ email_ques.size\n    \n    # compute intervals between email notification and answer date\n    # and clip them to be in range [min_days, max_days]\n    int_lens = (ans_dates - email_dates) \/ np.timedelta64(1, 'D') \n    int_lens = np.clip(int_lens, min_days, max_days)\n    \n    # estimate parameters of t-distribution\n    df = int_lens.size - 1\n    loc = int_lens.mean()\n    scale = max(int_lens.std(ddof=1), min_days)\n    \n    # estimate probability of answering an email-notified question within offset_days period\n    ans_prob = t.cdf(offset_days, df=df, loc=loc, scale=scale)\n    \n    # compute F1 score based on email_frac and anwer_prob\n    score = 2 * email_frac * ans_prob \/ (email_frac + ans_prob)\n    \n    # send email if F1 score is larger than threshold\n    return score > thresh","63df9f28":"test_question_id = '6351c23f2e144b359c3301d40b3d81ef'","319382f0":"emailed_pros = []\n\n# iterate over active professionals\nfor pro in active_pros['match_id']:\n    # get email-notified questions and answered questions for a particular professional\n    try:\n        email_ques = pro_email_ques['questions_id'].loc[pro].values\n        email_dates = pro_email_ques['emails_date_sent'].loc[pro].values\n        ans_ques = pro_answers['questions_id'].loc[pro].values\n        ans_dates = pro_answers['answers_date_added'].loc[pro].values\n    except:\n        continue\n\n    # Exclude dates that are greater than current date\n    email_ques, email_dates = email_ques[email_dates < current_date], email_dates[email_dates < current_date]\n    ans_ques, ans_dates = ans_ques[ans_dates < current_date], ans_dates[ans_dates < current_date]\n\n    if email_ques.size == 0 or ans_ques.size == 0:\n        continue\n\n    # if email_filter concludes that professional is likely to answer a given question\n    # within a day after receving an email notification, append him to emailed_pros\n    if email_filter(test_question_id, email_ques, email_dates, ans_ques, ans_dates, current_date, 1):\n        emailed_pros.append(pro)\n\nemailed_pros = np.array(emailed_pros)","12a229b1":"emailed_pros = active_pros.merge(pd.DataFrame({'match_id': emailed_pros}), on='match_id')\nemailed_pros","4e25aac2":"def send_quesionts_to_professional(pro_sample_dict, pro_answer_dates, questions,\n                                   answers, pro_email_ques, current_date=np.datetime64('now'), \n                                   top_content=20, min_days=7):\n    eps_1 = 0.01\n    eps_2 = 0.5\n    eps_3 = 0.3\n    \n    # Get top n suggested questions\n    pro_sample_df, pro_sample_tags = Formatter.convert_pro_dict(pro_sample_dict)\n    tmp = predictor.find_ques_by_pro(pro_sample_df, questions, answers, pro_sample_tags, top=top_content)\n    content_result = formatter.get_que(tmp)\n    que_ids = content_result['questions_id'].values\n    \n    # Get answer dates for professional\n    pro_id = pro_sample_df['professionals_id'].iloc[0]\n    answer_dates = pro_answer_dates.loc[pro_id].values\n    \n    # Check if professional is active\n    is_active = activity_filter(answer_dates, current_date)\n    \n    # Exclude dates that are greater than current date\n    try:\n        email_ques = pro_email_ques['questions_id'].loc[pro_id].values\n        email_dates = pro_email_ques['emails_date_sent'].loc[pro_id].values\n\n        email_ques, email_dates = email_ques[email_dates < current_date], email_dates[email_dates < current_date]\n    except:\n        email_ques = []\n        email_dates = []\n\n    if not email_ques:\n        mask = [True] * len(que_ids)\n    else:\n        mask = []\n        for que_id in que_ids:\n            mask.append(spam_filter(que_id, email_ques, email_dates.max(), cur_date, min_days=min_days))\n            \n    mask = np.array(mask)\n    passed_questions = que_ids[mask]\n    spam_questions = que_ids[~mask]\n    \n    explore_questions = []\n    \n    if is_active:\n        if passed_questions.size > 0:\n            for sq in spam_questions:\n                if np.random.rand() < eps_1:\n                    explore_questions.append(qs)\n    else:\n        if passed_questions.size > 0:\n            e_cond = eps_2\n        else:\n            e_cond = eps_3\n    \n        for sq in spam_questions:\n            if np.random.rand() < e_cond:\n                explore_questions.append(qs)\n            \n    final_q_ids = np.append(passed_questions, explore_questions)       \n    final_df = content_result[content_result['questions_id'].isin(final_q_ids)]\n    \n    return final_df","73dfe530":"pro_sample_dict = {\n    'professionals_id': ['eae09bbc30e34f008e10d5aa70d521b2'],\n    'professionals_location': ['Narberth, Pennsylvania'],\n    'professionals_industry': ['Veterinary'],\n    'professionals_headline': ['Veterinarian'],\n    'professionals_date_joined': ['2017-09-26 18:10:23 UTC+0000'],\n    'professionals_subscribed_tags': ['veterinary #animal']\n}","fad33a7b":"send_quesionts_to_professional(\n    pro_sample_dict,\n    pro_answer_dates,\n    questions,\n    answers,\n    pro_email_ques,\n    current_date=np.datetime64('2019-01-05')\n)","9a9b3018":"pro_sample_dict = {\n    'professionals_id': ['6b659c8487eb495cac6c55104c05cfc8'],\n    'professionals_location': ['Greater Seattle Area'],\n    'professionals_industry': ['Airlines\/Aviation'],\n    'professionals_headline': ['Finance Specialist - Business Skills Rotation Program at Boeing'],\n    'professionals_date_joined': ['2019-01-02 05:10:24 UTC+0000'],\n    'professionals_subscribed_tags': \n    'college business career finance management career-counseling internships entrepreneurship '\n    'career-path math leadership communications interviews resume english networking career-development '\n    'administration mentoring coaching employment organization skills strategy customer-service '\n    'development bussiness start-ups culture community-service growth diversity innovation mental-health-care '\n    'airlines\/aviation authenicleadership change civic-and-social-organization first-generation-college-students '\n    'goals health,-wellness-and-fitness'\n}","87cd0517":"send_quesionts_to_professional(\n    pro_sample_dict,\n    pro_answer_dates,\n    questions,\n    answers,\n    pro_email_ques,\n    current_date=np.datetime64('2019-01-05')\n)","2d9a5376":"##### Relevant questions\nHere we find relevant questions for professional.","67991e09":"### Project Source Code <a id=\"de_psc\"><\/a>","2bc71683":"Create a question and convert it into required form","6d0ad183":"#### Visualization\nFor visualization purposes it's better to have more topics to get the better filling of what's going on.\n\nTextPreprocessor stems tags, so it cuts words a bit.","7beb945c":"At the testing moment of 2018-08-16 there is a really small amount of active users, with a high probability of answering within 1 day.","78055129":"## 7. Data ingestion <a id=\"di\"><\/a>","9b7e23bd":"### User locations\nFrom the previous chart, we saw that both types of users highly use *location*.\nAs we will see later on, there is some *location* based correlation between students and professionals in terms of answered questions. Also, it's reasonable, because lot's of career questions are location dependent. \n\nIt means that it would be beneficial to have some knowledge based on such features.\n\nIt's easy to define a matrix of similarities between questions based on its content: *text* and *tags*. It's even possible to define such matrix including *industry*. However, more useful would be also take into account *location* feature as well as a *headline*. So we did it in our model.","7a815e86":"There are three main entities in our dataset for which we compute features separately: questions, students and professionals. Later in pipeline, when passing data to model, student's features are combined with question's to obtain complete question's feature vector.  \nThe reason for splitting question's features into two parts is the design principle in which question's features need to be time-independent, while student's feature vector, just like the professional's one, inevitably change in time.  \nThus, for proper forming of train data, we need to compute for each moment features of student and professional change. These moments will corresponds to appearances of new answers.","207d3137":"#### *spam_filter* function","d35ce7f8":"When designing model architecture, we had several requirements for it:\n- Ability to be trained on solving binary classification task of prediction whether the given question and professional form a positive pair\n- Ability to be generalized well for finding the most similar entities for building a recommendation engine\n- Ability to simultaneously consider features of any nature\n- Low variance not to overfit on a small amount of training data  \n\nApart from that, there were some properties our model would nice to have:\n- Ability to process question's and professional's data separately, for possible optimizations\n- Assurance of the possibility of a fast and straightforward finding of most similar entities","1b751ade":"### General Idea <a id=\"nna_general_idea\"><\/a>","5fc98430":"![Explore vs Exploit](https:\/\/drive.google.com\/uc?id=12ufAEGwMt5DM5JZxQ_73EJoZqyRScboQ)\n\n*Activity_filter* and *email_filter* will be really useful once some amount of active users is reached. For current system it's important to explore and try to activate incative professionals but not that hard, because they would immedetly drop off and that's what happens with current system.\n\nIn order to solve this issue, we can apply epsilon-greedy algorithm to try to activate inactive and \"cold\" professionals by breaking *activity_filter* and *spam_filter* with some probability. This is needed for making an attempt to activate the user or remind him about relevant questions which were already sent to him.\n\nCan set to be daily, but based on the current state it seems to be really spamming. In the ideal case, the system should define this period automatically.\n\nWe need to set some epsilon which is the probability of exploration.\nFor the current system, it should be reasonable, because professionals are pretty much inactive. \nOnce the activity of professionals grow epsilon should decrease and ideally calculated automatically.\n\nWe have here four cases:\n- Active professionals for which at least a few questions passed spam filter: eps=0.01\n- Active professionals for which there are no questions passed spam filter: *Don't spam active professional*\n- Inactive professionals with all the questions passed spam filter: eps=0.5\n- Inactive professionals without the questions passed the spam filter: eps=0.3","17da2dc2":"As we can see, there is a bad with active professionals, and this holds for different periods of time. It means that professionals get bored fast and the current system doesn't help them, but rather spams, that's why we have such low email answer and overall activity rate among the professionals.","044aa0dd":"You can test our recommendation engine in real time: http:\/\/careervillage-kaggle.datarootlabs.com","f562e365":"Recommender engine should take into account the metrics derived from the discussion:\n\nA good recommender system for CareerVillage.org is effective at helping reach a high 95% percent of questions which get a 10-quality-point answer within 24 hours, without churning out Pros, and within the bounds of fairness. \n\n*Percent of questions:* It's important that students feel that they can trust the CV community to be there for them, no matter what their career question is.\n\n*Receiving a:* Literally \"a\" meaning \"at least one\". CV wants questions to be answered multiple times, but it's much more important to get at least one answer than it is to go from the second answer to the third answer.\n\n*Within 24 hours:* Speed matters because it matters to the students. In addition, there is an insight that when a student asks a question and does not get an answer within 3 days, his likelihood of ever asking a question again drops a lot.\n\n*Without churning out Pros:* Must ensure that the emails sent are not pestering people so much that they can't remain with CV.\n\n*Within the bounds of fairness:* It's critically important. Recommendation system must work well for questions from wealthy, well-connected, generally-well-resourced students the as for everyone else.\n\nScoring of system features by importance:\n1. No question left behind\n2. Matches should feel \u201crelevant\u201d to the professionals\n3. Don\u2019t churn the professionals (too much email \u2192 feels like spam \u2192 people leave)\n4. Speed to first answer\n5. Works for newbies (solution for the cold start problem for new Professionals)\n6. Something in common (Place, School, Group)\n7. \\# of answers per question\n8. Interacted before?\n9. Bonus: Quality.","8e800e39":"Email is spam if the previous email was sent less than *min_days* ago\nor if professional was already email-notified about the question.","56868230":"Professionals emails for spam emails filtering","660339fb":"- Model environment\n- Train VAE\n- Engineer reward function in order to solve:\n    - No question left behind\n    - Don\u2019t churn the professionals (too much email \u2192 feels like spam \u2192 people leave) (partially solved with our currenlt approach, but there are still custom parameters which can be learned and adjusted automatically)\n    - Speed to first answer\n- Train the agent","6328b506":"#### Training","052f46ca":"The logic is pretty much the same as it was with question.","5dc1af94":"## 1. Introduction <a id=\"introduction\"><\/a>\nWith this notebook we would like to show our approach for a new recommendation engine at www.careervillage.org.\nThe main goal here is to develop a method to recommend relevant questions to the professionals who are most likely to answer them.\n\nDescription of solution:\n- Method for taking into account all the possible content features using custom Neural Network Architecture. Allows automatically find a probability mapping between question and professional entities in all of the combinations (que-que, que-pro, pro-que, pro-pro). It is trained in a way that even without any information about professional and his activity, it still recommends questions, and breaks cold start problem having its own internal scoring.\n- Method *activity_filter* for filtering out all the inactive professionals in order to send immediate emails to professionals who are most likely answer the question, and answer it fast\n- Methods *spam_filter* and *email_filter* for sending emails which allows to handle spam problem and controls frequency and amount of emails for each professional based on his personal emails reaction type and speed (here reaction==answer)\n- The method which tries to activate inactive and \"cold\" professionals by breaking *activity_filter* and *email_filter* with some probability. This is needed for making an attempt to activate the user or remind him about relevant questions which were already sent to him.\n- Suggestions and ideas for RL approach, environment modeling, and reward engineering to handle complex metrics\n- Code available for deployment\n- Live Demo with NN based fast similarity search\n\nContent Table Description:\n- **Data overview** section shows us the data diagram and highlights the main and secondary data.\n- **Set Up** section is for loading all necessary libraries, setting globals and loading CSV files.\n- **EDA** section is for the insights, but we did it as simple and short as possible - as there are already a lot of existing notebooks sharing different insights\n- **NLP** section is for useful extraction data from text\n- **Feature Engineering** section is dedicated to the creation of new feature-columns for further usage in EDA and modeling.\n- **Data Ingestion** mainly for batch generator\n- **Neural Network Architecture** to combine so-called content feature for solving the problem, only based on static data provided by professional and student as well as the content of the question itself\n- **The Recommendation Engine** - problem solution\n- **Deployment** - test it fast and easy\n- **Future Plans** are for dreams :)","c5baab8e":"#### Base feature preprocessing class","769df8e0":"## 5. NLP <a id=\"nlp\"><\/a>","c11e7ae4":"### Top n Professionals with most Answers & Students with most Questions\nThe distribution of students with the highest amount of questions is quite linear. 6 of these curious students asked more than 70 questions, and 4 students asked more than 40 questions.\n\nThe distribution of professionals with the highest amount of answers is fascinating. Some ultimate professionals answered more than a thousand questions - the platform contains 51123 answers, and those people added almost 12.5% of them (6344)! \n\nHowever, our task is to increase the number of answers to improve the system, so we need to focus on the professionals that drop off quickly.","367ce216":"### Simple explanation <a id=\"nna_simple_explanation\"><\/a>","079407c7":"### Number of Answers & Questions added per year\nHere we can see the dynamics of the number of answers and questions per year. Questions amount correlates with yearly registration of students, so answers amount have the same pattern and grows too. However, it interesting what caused such a decrease in amounts during 2017.","bc3f67b4":"### Suggestion on Reinforcement Learning Approach <a id=\"re_rla\"><\/a>","aae49351":"### Doc2Vec <a id=\"nlp_doc2vec\"><\/a>","7cec5646":"Our Neural Network trained on \"content\" features is already a generalized way to solve the main recommender tasks:\n1. Once a student writes a question, the engine should simultaneously find similar questions which could help him to find already provided answers or help with the formulation of his own question.\n2. New questions should be answered quickly. For this reason, it should be forwarded to professionals who can answer the questions best.\n3. Find the list of most relevant questions for professional to answer. They can be sent to professional by email","ec61a37f":"We distinguish three main types of features: categorical, numerical and datetime-like\n- For categorical features, we consider only top N of its most popular categories. We encode them via LabelEncoder with labels from 0 to N-1; all the remaining categories encoded with N and NaNs with N+1 label. Later, in the model we train embeddings for every label of each categorical feature\n- In a numerical feature, we fill its NaNs with either zero or mean and then standardize it via StandardScaler\n- From the datetime-like feature we extract three new features: absolute time, sine and cosine of the scaled day of the year. Then, we work with three new features just like with numerical ones\n\nAll this basic feature preprocessing logic is implemented in abstract base class BaseProc. Classes with the implementation of feature engineering unique for each of three entities (question, student, professional) are inherited from BaseProc.","e612ae80":"So, for sampling negative pair, we choose a random question, sample random current time and sample random professional among those who registered at the current time and who were not forming positive pair with selected question.\nFor given tuple of the question, student and professional, we use their features at a current time. The final batch consists of equal amounts of positive and negative pairs.","63d223f5":"Number of topics is selected manually. Empirically we derived, that the dimensionality should be set to 10. Higher dimensionality doesn't give us an increase in accuracy. It's also good to have the same amount of features from each NLP model to compare their importance in general.","1a839740":"The general solution to the problem is to build the classifier trained on binary classification problem and then partially transferred for recommendation engine purposes  \nThe problem we choose is to determine whether professional will answer given question based on features of question and professional. \nIn the binary classification problem, we need both positive and negative samples  \nFirst ones are easy to obtain: they are formed from those questions and professionals, where professional answered that question. Thus, we can compute positive pairs directly from data","670a31d6":"### Mean response time per year\nHere we can see the evolution of mean response time, which is a significant mark that shows the mean speed of answering questions. The dynamics show decreasing average time to response which changed from ~790 days to ~5.\n\nThe answer to the question indicates that the question is appropriate, but we should not rely on very long-time answers. So we need to pay more attention to the early stages of the relationship with the professional.","45f22b61":"Let's compare scores for all professionals and only for active professionals","94bdbcec":"All the text processing stuff is implemented in TextProcessor class","00feb040":"#### Question's features processing class\n- **Numerical**\n    - Question's body length in symbols\n    - Question's number of tags\n- Averaged question's tags embeddings pre-trained via doc2vec\n- Unique question's embedding inferred via doc2vec \n- Unique question's topic distributions inferred via LDA","dab99361":"## 9. Recommendation engine <a id=\"re\"><\/a>","ca75fb33":"#### Activity features used in EDA\nUser activities divided into 3 main categories:\n- Answer\n- Question\n- Comment\n\nTime of these activities helps us to derive essential features such us *date_last_activity* which is used later on in activity based recommendation decisions","46529649":"permutation_importance() function implements calculating of feature importances, plot_fi() displays them","2b230a97":"This function is all about controlling the number of emails sent by the system in order not to spam professionals.\nIt makes a decision about whether or not to send an immediate email about a certain question to a professional.  \nThere are several cases that are processed by this function:  \n   1. If email is considered spam by *spam_filter* function, do not send it.  \n   2. If professional answered less than two email-notified questions, then we cannot estimate how long it will take him to answer the new one. As we don't have any reason to believe that a professional would \"immediately\" react to an email, we do not send it.  \n   3. Otherwise, we compute F1 score based on (a) and (b), and send an email if it is greater than a certain threshold.  \n   Here,  \n   (a) is the fraction of email-notified questions that were answered by this professional;  \n   (b) is the estimated probability of answering a question within a period of *offset_days* after receiving an email notification. We first construct Student's t-distribution based on intervals between email notification dates and answer dates (to learn more about how t-distribution is constructed, refer to the description of *active_filter* function). These intervals are clipped to be in range $[min\\_days, max\\_days]$ to limit the size of possible fluctuations. The desired probability is equal to the area under t-distribution pdf which lies below *offset_days*.","b8675e39":"### CSV Files","284fc6e4":"Check which professionals are likely to answer a given question within a day after receving an email notification","5eb3e4ea":"Negative samples are a bit more tricky. The logic of sampling negative question-professional pairs is implemented in a batch generator, which is used for transforming processed dataframe into arrays ready to be fed into the model.  \nTo determine the exact feature vectors of both students and professionals, we need the concept of the current time:\n- For positive professional-question pairs, the current time is the time of an answer that connects the given question and professional\n- In the case of negative pairs, we sample current time as a random shift from question's added date  ","461045ea":"## 8. Neural Network Architecture <a id=\"nna\"><\/a>","0db84725":"In order to use our code easily in a good project structure, you\u2019re free to go to https:\/\/github.com\/dataroot\/kaggle-cv and follow the README.md","d87db2f5":"### LDA <a id=\"nlp_lda\"><\/a> ","4301c510":"### Globals & Settings","79e97bd8":"Select only professionals who are estimated to be active on current date","c5bdb7d3":"### Users Missing Data\nProfessionals highly use the *Location*, *Industry*, *Headline* and *Tags*. These features are included in our Content Model for recommendations as well as a *Location* of the students.\n\n*Groups*, *Schools* and *Comments* rarely used by both types of users.\n\nStudents rarely use *Tags*.\n\nNear 0.6 both from students and professionals are not posting *Questions* and *Answers* correspondingly.","87e9d6c1":"The main reason to use Latent Dirichlet Allocation is uncovering the themes lurking in the text data. By using LDA on pizza orders, it is possible to infer pizza topping themes like spicy, salty, savory, and sweet. It can be used to extract the topics distribution across the questions and words characterize corresponding topics.\n\nAs a result, each question is represented as a vector of fixed length, which is easy to use for similarity measures. \n\nIn order to perform LDA on question's text data, we need to:\n- Filter the text data and anomalies\n- Calculate the TF-IDF\n- Train LDA Model","13735edb":"Textual data is the main form of information in the provided dataset. To utilize it numerically, first, we apply some generic text preprocessing. We remove stopwords and HTML tags, cast everything to lower case and stem each word. NLTK and regular expressions come to rescue here.","1e8c57bb":"### How many questions are contained in each email\nMost emails contain 1-3 questions, so an average amount of questions per email is 2.33.\n\nAccessing questions is also possible directly from the Career Village website, so professionals are not restricted to answering emails, so contact method should not be assumed. However, we need inferred links between questions and professionals to build a recommender.","3850c963":"If you wish to add a new feature, there are two possible options:\n- New feature is time-independent. In that case, first you can either:\n    - Add new feature as a column of target entity's dataframe you passing to transform()\n    - Add a calculation of new feature on the top of transform() method of target entity's processor class\n        - For example, in case of target entity being student, we have added student's state as a feature with the following line, which is the first in StuPruc's transform() method:  \n        `stu['students_state'] = stu['students_location'].apply(lambda s: str(s).split(', ')[-1])`  \n        \n    Then, you have to add a new feature to self.features dict in target entity's class constructor and to specify its type:\n    - If the feature's type is categorical, you have to append tuple of feature's name and number of its most popular categories to consider\n    - If the feature is numerical, append its name to either 'mean' or 'zero,' depending on value to fill its NaNs\n- New feature is time-dependent. Then, to properly train model, you need to:\n    - Update self.features just like in the previous case\n    - Specify its value default value below # DEFAULT CASE, which is used in case professional or student do not have answers or questions respectively at a time\n        - For example, default value of student's number of asked questions is zero and it is specified in line:  \n        `new = {'students_questions_asked': 0, ...`\n    - Specify formulas to update it on current timestamp below # UPDATE RULES\n        - For example, the value of the number of student's asked questions is updated in line  \n        `new['students_questions_asked'] += 1`\n    - If a feature needs to be averaged, add its name to list below # NORMALIZE AVERAGE FEATURES","c2eb50be":"As was mentioned in [EDA](#eda), it would be handy to take into account all the existing features and find some way to balance between them in our recommendation decision automatically.\n\nExisting data gives us beneficial knowledge of what worked and what didn't in terms of question answering between the students and the professionals. So we decided to use that knowledge and sort of build our latent space based on that. Such a classification task puts positive students, questions and professionals features closer to each other. Moreover, this is useful not only in terms of static features such as *locations*, *headlines*, etc. but also in previous interactions. Even students\/questions **generalized** tags mapped to professionals **specialised** tags automatically just solving such optimization task on existing data.\n\nSo, that's pretty much it. That's how you can think of such a transfer learning approach.","1710a63e":"Get top n professionals based on Content Model Predictor","26992524":"The first tool we use to extract features from text is doc2vec algorithm.  \nDoc2Vec is a development of well-known Word2Vec algorithm used for unsupervised training of word embeddings on large text corpora. The difference is that Doc2Vec adds simultaneous training not only for words but also for tags associated with documents. If the tags are unique than we achieve descriptive high-dimensional vectors of each document. If the tag appears in pair with many documents, then its embedding contains information about all the words associated documents consist of.  \nWe use Doc2Vec in both of named modes. Via the first mode, we obtain individual question's embeddings based on its title and body, and thanks to the properties of the algorithm, we are able to compute these vectors for new data.  \nVia the second mode, we obtain embeddings for question's tags and professional's industries. We will use all of the textual data available for that purpose:\n- Question's title\n- Question's body\n- Answer's body\n- Question's tags names\n- Professional's tags names\n- Professional's industries\n- Professional's headlines \n\nWe use Doc2Vec implementation from Gensim, an excellent toolkit for topic modeling","c8875edc":"## 3. Set Up <a id=\"set_up\"><\/a>","b48ade60":"##### Similar professionals\nAs a bonus, we are also able to find similar professionals by given professional.","57f7ce51":"#### Testing with real emails","51af5974":"In order to use our Neural Network for recommendations, we defined a Predictor class with all the necessary functions for each of the three initial recommender problems.\n\nWe have three main methods for each problem:\n- find_ques_by_que\n- find_pros_by_que\n- find_ques_by_pro\n\nAlso, we can even find similar professional given the professional.  \n\nAll the methods run in a few milliseconds. That speed is achieved by precomputing latent vectors of existing questions and professionals and storing them in special data structure named K-d tree, which is high-dimensional binary search tree. Thus, for each new entity (professional or student) we only compute its latent vector and find entities with similar latent vector in logarithmic time on average.\n\nThis gives us a generalized content-based recommender which combines and scores all the content features automatically and puts them in the space which is created while solving optimization problem on existing success\/failure in answered \/ unanswered questions by professionals.\n\nThe model will be improved over time with each new question\/answer.","220cc25d":"### Batch generator  <a id=\"di_batch_generator\"><\/a>","46e134d8":"So, its final value is $ \\frac{1}{30} $.  ","82c0aa75":"# Recommender Engine for CareerVillage","8e8fc6b5":"#### *email_filter* description:","0f467656":"#### Training\nLDA training pipeline is implemented in pipeline_lda() function","59ec4cbd":"To find relevant questions\/professionals given question we also take into account student's features, as it is evident that the same questions regarding a career in different for example locations lead to different answers from different professionals. So with each question, we also take into account all possible features to find the most relevant similar questions. Also, our network helps us with this in a few lines of code.","280852f6":"We will split the definition of recommender functionality into two parts:\n- Content-based recommendations \n- Activity-based recommendations","a17bdab0":"To objectively measure the performance of our recommendation engine, we split data into train and test subsets right here. Train subset of data will consist of each dataframe split by datetime-like column with values before SPLIT_DATE. Thus, we are sure no data after SPLIT_DATE will make it to train ","3e8ddc3d":"### Libraries","474e4a6c":"It's important to understand that we don't want our model to reach 0.9+ accuracy because we use this optimization task for automatic balancing between the features for the decision. We derived, that 0.75 accuracy is optimal for our task because it would find the similarities and differences between the entities in a way that even **cold start** problem for question and professional can be solved.\n\nFor example, we have professional with no answers, so no statistical features, but he has tags, these tags help us to find enough questions for him, based on \"what worked previously\" knowledge for similar question and professional. So even if there is no similar question and professional, we are able to map their features and find at least something which maximizes the probability of answering the question.   \n\nIn case professional has no tags, recommendations are based on the **probabilistic** approach, and **activity** features explained later.","43e1e568":"Model architecture is implemented in DistanceModel class, which aggregates two Encoder objects and is inherited from keras.Model","cb127a6e":"### Probabilistic Approach <a id=\"re_probabilistic_approach\"><\/a>","2c953bdb":"#### Datetimes features preprocessing","fd4c1dac":"### Evaluation <a id=\"nna_evaluation\"><\/a>","1695b865":"Formatter class is needed for visual representation and ability to compare the results","cbf74a3d":"<img style=\"float: left; margin-right: 20px; width: 150px; height: 150px;\" src=\"https:\/\/drive.google.com\/uc?id=16U6-OZznqKrz-I4cQtBqF5znzxR-osdT\"> \ndata root labs ([DRL](https:\/\/datarootlabs.com)) provides full-service data science & artificial intelligence solutions for startups and enterprises. Our core products include AI Solution Development for startups, AI Transformation for Enterprises and Startup Venture Services. We work with a large number of startups and startup accelerators by fully or partially closing clients AI development needs.\n\nWe \u2764 to contribute to the development of the ecosystem by running data root university, the largest data science and data engineering school in Ukraine.","f91bff29":"## Future Plans <a id=\"fp\"><\/a>","3c6d2782":"#### *activity_filter* function\nThis function is based on professional's activity, currently, it includes only answer activity, but it is easy to integrate comments as well.\n\nIt filters out inactive professionals and is used while making a decision to send an immediate email by the next function.\n\nSo, here is more detailed description of *is_active* function:\n- Let $x_i$ be the amount of time (in days) between $(i-1)$<sup>th<\/sup> and $i$<sup>th<\/sup> answer of a particular professional, so that $x_1$, ..., $x_n$ are lengths of intervals between answers $0$ (registration date) through $n$. To avoid too big fluctuations, interval lengths are clipped on range $[min\\_days, max\\_days]$.\n- Our goal is to estimate the distribution of answer intervals, even if there is a small number of them. *Student's t-distribution* is particularly useful for working in \"small-data\" regime, so we choose to estimate the distribution of answer intervals with t-distribution, parameters for which are inferred from the data (lengths of intervals).\n- Student's t-distribution is usually used to estimate the true mean $\\mu$ of i.i.d. Gaussian random variables when the true variance $\\sigma^2$ is unknown, but we are going to use it to estimate whether a particular observation is likely to come from t-distribution with $n-1$ degrees of freedom, sample mean $\\overline{x}$ and sample variance $s^2$, where ${\\overline{x}} = {x_1 + ... + x_n \\over n}$ and ${s^2} = {1 \\over n-1}\\sum\\limits_{i=1}^n(x_i - \\overline{x})^2$\n- Let's call $x$ the realization of some random variable, which in our case is a distance between the current date and previous answer date. We then infer how likely was it that $x$ was generated by our estimated t-distribution. If this probability is greater than the probability of coming from one tail of t-distribution, then $x$ was probably generated by this t-distribution.\n\nThere are 2 cases that we process separately:\n\n1) if professional gave 0 or 1 answers, then there is no distribution to be computed, and so professional is estimated to be active if $x$ lies in the range $[min\\_days, max\\_days]$\n\n2) if professional gave more than 1 answer, then it is inferred whether $x$ was generated by t-distribution as described above;  \n- if this is the case and $x$ is greater than min_days\n- if this is not the case, but $x$ lies in the range $[min\\_days, max\\_days]$\n\nthen conclude that professional is active.","51e7f2ad":"## 4. EDA <a id=\"eda\"><\/a> ","965a1bdb":"#### Students Features Preprocessing Class\n- **Categorical**\n    - Location\n    - State - extracted from location\n- **Numerical**\n    - Number of asked questions\n    - Average asked question body length\n    - Average body length of answer on student's questions\n    - Average number of answers on student's questions","9e110170":"#### By professional","cc867c3c":"### Last activity  <a id=\"eda_plots_la\"><\/a>  \nDepending on the last comment, question or answer of a user, we have extracted the last activity date. On the previous plot we have seen, that many users haven't done any activity yet. For the 'last activity' plot we take a look only on users with already have one activity (*dropna*).  ","5beb5078":"### Training <a id=\"nna_training\"><\/a>","a80204c1":"#### Visualization","116f18e9":"Predictor class handles all the main computational stuff of finding most similar entities given entity. Also it checks that entity we found has not formed positive pair with given yet.","905d5ef1":"Another nice information to see is categorical features embeddings trained by model","e370265c":"#### Professionals Features Preprocessing Class\n- **Categorical**\n    - Industry\n    - Location\n    - State - extracted from location\n- **Numerical**\n    - Average answered question's body length\n    - Average answer's body length\n- Averaged subscribed tag embedding pre-trained via doc2vec\n- Industry embedding pre-trained via doc2vec\n- Headline embedding infered via doc2vec\n- Averaged question embedding infered via doc2vec","1f282eac":"#### Testing *activity_filter* with Predictor","59f23111":"Here we display how they look like in case of student's and professional's states","0fd6dba2":"##### Similar questions\nHere we find similar questions given question.","26620194":"### Number of Emails sent per year\nThe number of emails sent yearly tends to grow each year.","04f2726b":"Doc2Vec training pipeline is implemented in train_d2v() and pipeline_d2v() functions","9b6ad7d0":"#### By question","0e7cabdd":"## 6. Feature Engineering <a id=\"feature_engineering\"><\/a>","2b0beeb5":"To evaluate model performance on test set, we compute each entity features on full data. This adds new feature vectors that correspond to answers after SPLIT_DATE, and they are computed using data after SPLIT_DATE.  \nPositive pairs are formed from questions and professionals where professional gave an answer to that question after SPLIT_DATE","93b74c91":"### Users Growth Dynamics\nThis shows us that growth is stable which is suitable for improvement recommendations.\n\nCurrent data is beneficial not only in terms of content for similarity matrix but also in terms of activity and interaction between users.","9f967c5d":"### Additional data computation <a id=\"di_add_data_comp\"><\/a>","b4754193":"### Days for first answer <a id=\"eda_plots_fa\"><\/a>\nAs we can see most of the questions receive answers on the same day, which is good, but obviously, we want to make the ration higher for this first day.   \nAlso, it seems that interest in answering questions drops off quickly. Over 30% of answers are provided on the first day of involvement in CV.","f2e57443":"### Activity Based <a id=\"re_activity_based\"><\/a>","60e80c6c":"### Plots & Insights <a id=\"eda_plots\"><\/a> ","e54f69bc":"### Tags Wordclouds <a id=\"eda_plots_tm\"><\/a> \nIn most of the cases, students are not using tags. Student tags are similar to questions tags. The current system is recommending questions tags, and they are not that similar to those which professionals are following. \n\nTags of questions and students and more generalized comparing to professionals tags. It means that even if we apply some processing and modeling techniques and deriving similarities out of it, there still be unmatched student and professionals using tags due to **generalized** vs. **specialized** tags problem.\n\nOur model also solves this issue.","05589823":"### Professionals Answers & Students Questions amounts\nStudents are usually asking one or two questions.\nProfessionals are usually answering only one question. It can be caused by not sufficient feedback from students. Comment system and scores are used rarely.","f582005f":"### Demo <a id=\"de_demo\"><\/a>","af6fd23d":"##### Relevant professionals\nHere we find relevant professionals by question.","8d4fc84a":"There is interesting idea arrived in the process of working on project. It actually was used in different project by our team and did it's job surpisingly well. \n\n\nHere we'll describe the possibility to integrate a reinforcement learning approach in recommendation engine and why it can solve a lot of problems in a more general way.\n\nReinforcement learning is all about finding an optimal \"strategy\" for agent inside of the given environment.\n\n![RL Diagram](https:\/\/drive.google.com\/uc?id=1kl2eUCTBR7R169Kv_ykt7sGCeaCbmyds)\n\nIn our case an agent is the recommendation engine itself which is able to interact with the environment (users) by sending emails, it's the only action our agent currently able to do.\n\nIn order to model an environment, we need to emulate users behavior: answers, reactions on emails, comments, hearts, etc. This can be done using VAE (variational autoencoder). To achieve this, the model needs to learn the probability distribution of the training data. VAE is one of the most popular approaches to learn complicated data distribution in an unsupervised fashion. The model aims to learn the underlying probability distribution of the training data so that it could easily sample new data from that learned distribution. The idea is to learn a low-dimensional latent representation of the training data called latent variables (variables which are not directly observed but are rather inferred through a mathematical model) which we assume to have generated our actual training data. These latent variables can store useful information about the type of output the model needs to generate. Once we trained our VAE on data, we are good to go with lots of artificial professionals.\n\nBut, even though, we need to think of some evolution in users behavior. So VAE needs to be updated with new data once the recommendation engine is used in production and shown some influence on users behavior. But that's a story about production.\n\nOnce VAE is trained and we have our artificial environment, reward engineering will come to action. For example, it's really easy to develop a reward function which will lead to an increase in answers amount. Other metrics are harder, but not impossible.\n\nSo that's what we are planning to try on our free time.","98d90167":"### Intro <a id=\"re_intro\"><\/a>","fb4876f7":"## Table of Content\n1. [Introduction](#introduction)\n2. [Data Overview](#data_overview)\n3. [Set Up](#set_up)\n4. [EDA](#eda)\n5. [NLP](#nlp)\n    1. [Doc2Vec](#nlp_doc2vec)\n    2. [LDA](#nlp_lda)\n6. [Feature Engineering](#feature_engineering)\n7. [Data Ingestion](#di)\n    1. [Additional Data Computation](#di_add_data_comp)\n    1. [Batch Generator](#di_batch_generator)\n8. [Neural Netwrok Architecture](#nna)\n    1. [General Idea](#nna_general_idea)\n    2. [Simple explanation](#nna_simple_explanation)\n    3. [Training](#nna_training)\n    4. [Evaluation](#nna_evaluation)\n    4. [Test](#nna_test)\n9. [Recommendation Engine](#re)\n    1. [Intro](#re_intro)\n    2. [Content Based](#re_content_based)\n    3. [Activity Based](#re_activity_based)\n    4. [Probabilistic Approach](#re_probabilistic_approach)\n    5. [Suggestion on Reinforcement Learning Approach](#re_rla)\n10. [Deployment](#de)\n    1. [Project Source Code](#de_psc)\n    2. [Demo](#de_demo)\n11. [Future Plans](#fp)","a0daad69":"## Deployment <a id=\"de\"><\/a>","fb705f16":"## 2. Data Overview <a id=\"data_overview\"><\/a>\nBelow you can find the diagram with data and relations.\nEssential primary entities are \"Professional\", \"Student\", \"Question\" and \"Answer\".\nThe most useful secondary entity is \"Tag.\"\n\nPrimary and secondary entities are used to create so-called \"content\" features.\nOther data is divided into two main parts \"activity\" and \"time\" features.\n\n![data overview](https:\/\/drive.google.com\/uc?id=1QdxvAXep1_kJX_CzpGnwkGz3qE6_pgkc)","6e8aebb1":"For describing the model, feature importance is really nice information to have. One of the approaches for calculating feature importance in neural networks is permutation importance. We shuffle feature values randomly one by one and measure how the loss function is affected.","e5eeaa83":"#### Let's test it on a few professionals","9b7c4688":"Such a long list of requirements was the reason why the only viable option for us from the beginning was neural networks. The final architecture meets all of them, and here it is:\n\n![model](https:\/\/drive.google.com\/uc?id=1vol5ixKhuJ9bwaEzqUh8tVohutYE8CL9)\n\nLet's take a closer look at it.\n- The main parts of the model are two separate encoders, for both questions and professionals. They consist of two dense layers, which transform preprocessed features into latent space of fixed dimension.\n- Our objective is to train model in a way that makes encoders produce close latent representations for similar questions, professionals and positive question-professional pairs. To ensure that, first we compute the euclidian distance between latent vectors. Then, we pass it through an activation function, which in our case is $ e^{-x} $. That way, ideally we will have similar latent vectors for positive question-professional pairs, with zero distance and activation value of 1. On negative pairs, respectively, we wish to maximize the distance between their latent vectors, and output of our network to have a value of 0.\n\nTo sum it up, our model can be though of as such function of inputs q (questions) and p (professionals):\n$$ nn(q, p) = e^{-\\sqrt{(E_{p}(p)^2 - E_{q}(q)^2}} $$\nwhere $ E_{p} $ and $ E_{q} $ are two encoders which can be though of as linear operators, which they are in case of single layer.  \nSome other notes and properties of our models: \n- Numerical, textual and datetime-like features are easy to make suitable for the neural network. That's not the case with categorical features. However, there is a helpful tool, which is embedding layers. We pass each categorical feature values separately through the embedding layer, that gives us a dense numerical representation for each category. Then, we concatenate them with the rest numerical features. That helps us to consider all the data simultaneously with a small amount of parameters.\n- Notice, that with that architecture we have all the costly operations on question's and professional's features separate. That allows us to precompute latent vectors for known questions and professional to save on computational time when finding similar entities.\n- Also notice, that the most similar questions and professionals have the lowest euclidian distance between them. Euclidian distance is a ubiquitous metric and there are quite a few data structures that help us to optimize finding of the closest latent vectors and to achieve the fastest possible recommendations eventually.\n- We have also added L2 regularization on weights connected with question's tags embeddings, and professional's averaged answered question's embedding. That way we merely lost on model performance but gained higher feature importance on other features - question's text and professional's tags, industry and headline. With that, our model is much more robust to missing parts of data (because it pays equal attention to all the features). Also, with that, we solved a cold start problem for professionals - otherwise, predictions for professionals with no answer history (and with zeroed average question embedding) were significantly less meaningful compared to other. \n- Siamese networks inspired this architecture.","34ae5410":"### Content Based<a id=\"re_content_based\"><\/a>","411acea6":"### First activity after registration  <a id=\"eda_plots_fa\"><\/a>\nThere are two general types of users:\n- Activity right after registration\n- No activity at all","e8d791c5":"This architecture we named DistanceModel, and there were others as well. Two of them are:\n- ConcatModel. Instead of calculating euclidian distance and passing it through unusual activation, we concatenate latent vectors and pass them through some dense layers.\n- SimpleModel. Instead of concatenating latent vectors, we concatenate preprocessed feature vectors directly and pass that through dense layers.\n\n![models](https:\/\/drive.google.com\/uc?id=1wmx-cWuJvzY3eN_gXd8b5qXVadZkv71d)\n\nAll of the models gave approximately similar results, but because only DistanceModel has a bunch of nice mentioned properties, we decided to stick with it. Summarization of all three model performances on same set of features presented in the table below: \n\n![model_summary](https:\/\/drive.google.com\/uc?id=11BzYsRLyieEZzapOP22Y4gB8FZcxGOos)","6d20e75b":"To sample random shift of current time in negative pairs, we selected exponential distribution. It's general PDF formula is:  \n$$ f(x)=\\lambda e^{-\\lambda x} $$\nThe parameter $ \\lambda $ was selected to obtain the best approximation of the real distribution of time between the appearance of question and its answer:","cfb7eb7f":"### Test <a id=\"nna_test\"><\/a>"}}