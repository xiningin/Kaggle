{"cell_type":{"a31354c7":"code","5d8d8c6d":"code","4d0418d2":"code","ff1b18cf":"code","700773d4":"code","0583d544":"code","ef63c1d6":"code","ede8aacd":"code","940e32e9":"code","06a722b8":"code","31add87e":"code","f539df65":"code","66d19584":"code","45ad3a62":"code","b7dd898a":"code","82fceee0":"code","f7905d27":"code","2a03841f":"code","91d71952":"code","c462b026":"code","c1ecf082":"code","2b7badeb":"code","9cf283be":"code","08ced017":"code","6ae25369":"code","5c125957":"code","2f20a7e1":"code","de92d41c":"code","2e16c93d":"code","cf2b4262":"code","90805c3c":"code","c5756379":"code","eef86a93":"code","f7002715":"code","cab0de61":"code","cc68ff42":"code","5e4a5221":"code","647b7d29":"code","5133cf20":"code","98dbcbde":"code","93af8da1":"code","4cb5c820":"markdown","47d7b1c1":"markdown","97e67723":"markdown","39d8acc4":"markdown","6691e2c5":"markdown","84d2a223":"markdown","10e7e5e9":"markdown","a4718532":"markdown","4befda41":"markdown","033e1930":"markdown","6437be8d":"markdown","ec3279c2":"markdown","a3b4601e":"markdown","57d6f76e":"markdown","091cf159":"markdown","e305af82":"markdown","302f2e19":"markdown","257081c5":"markdown","f6781b18":"markdown","4a29990f":"markdown","65470ec2":"markdown","57c52dcf":"markdown","9d091393":"markdown","6b62b6df":"markdown","74812a51":"markdown","3ebce228":"markdown","9ea0894e":"markdown","9079392c":"markdown","6ea758cf":"markdown","d35cb1eb":"markdown","216aa7a0":"markdown","6c0ac6f7":"markdown","dcac14c9":"markdown","26277ab2":"markdown","a442fbd6":"markdown","49bbf8b1":"markdown","59324f8c":"markdown","c01a0459":"markdown"},"source":{"a31354c7":"# There are a lot of warnings about CV not having enough data for each fold.\n# TODO: Find a better way to deal with the warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#\u00a0Same old imports\nimport numpy as np\nimport pandas as pd\nimport os\nimport pandas_profiling as pdp\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import f1_score, confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pylab as plt\nfrom hyperopt import hp, tpe, Trials\nfrom hyperopt.fmin import fmin\nfrom tqdm import tqdm\nimport itertools\n\n#\u00a0Models\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom tpot import TPOTClassifier\n\n","5d8d8c6d":"#\u00a0Some constants\nDATA_PATH = \"..\/input\/Pokemon.csv\"\nTARGET_COL = \"Type 1\"\nENCODED_TARGET_COL = \"encoded_type_1\"\nTO_DROP_COLS = [\"#\", \"Name\"]\n# The dataset is small\nTEST_RATIO = 0.1\n# For reproducibility\nSEED = 31415\nRUN_HP_OPTIMIZATION = False\n# Reduce this if needed! (resources are scarce here!)\nMAX_EVALS = 200\nHP_SPACE = {\n    #\u00a0Trying to reduce class imbalance\n    'max_delta_step': 2, \n    # To avoid overfitting\n    'reg_alpha': hp.loguniform('reg_alpha', np.log(0.01), np.log(1)), \n    'reg_lambda': hp.loguniform('reg_lambda', np.log(0.01), np.log(1)), \n    'n_estimators': hp.quniform('n_estimators', 100, 1000, 1),\n    'max_depth': hp.quniform('max_depth', 2, 8, 1),\n    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(1)),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0),\n    'gamma': hp.loguniform('gamma', np.log(0.01), np.log(1)),\n}\n#\u00a0Optimal hp from previous run\nOPTIMAL_HP = {'colsample_bytree': 0.7316836664311229, 'gamma': 0.04744535212276833, \n              'learning_rate': 0.02478735341127185, 'max_depth': 5.0, 'n_estimators': 349.0, \n              'reg_alpha': 0.03216806358838591, 'reg_lambda': 0.019055394071559602}\n# Tpot conf values: increase these for more runs (and hopefully better results)\nTPOT_GENERATION = 20\nTPOT_POPULATION_SIZE = 100","4d0418d2":"#\u00a0Some useful functions \n\n\n# Inspired from here: http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/\n# plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    \n    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.set_title(title)\n    fig.colorbar(im)\n    tick_marks = np.arange(len(classes))\n    ax.set_xticks(tick_marks)\n    ax.set_xticklabels(classes, rotation=45)\n    ax.set_yticks(tick_marks)\n    ax.set_yticklabels(classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        ax.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    fig.tight_layout()\n    ax.set_ylabel('True Type 1')\n    ax.set_xlabel('Predicted Type 1')\n    ax.grid(False)","ff1b18cf":"pokemon_df = pd.read_csv(DATA_PATH)\npokemon_df.sample(5)","700773d4":"pokemon_df.dtypes","0583d544":"pdp.ProfileReport(pokemon_df)","ef63c1d6":"target_s = pokemon_df['Type 1']\n\"There are {} unique major types\".format(target_s.nunique())\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\ntarget_s.value_counts().plot(kind='bar', ax=ax)\nax.set_ylabel('Number')\nax.set_xlabel(\"Pokemons' Type 1\")","ede8aacd":"fig, ax = plt.subplots(1, 1, figsize=(12, 8))\ntarget_s.value_counts(normalize=True).mul(100).plot(kind='bar', ax=ax)\nax.set_ylabel('%')\nax.set_xlabel(\"Pokemons' Type 1\")","940e32e9":"le = LabelEncoder()\nencoded_target_s = pd.Series(le.fit_transform(target_s), name=ENCODED_TARGET_COL)\ndummified_target_s = pd.get_dummies(target_s)\ndummified_features_df = (pokemon_df.drop(TO_DROP_COLS + [TARGET_COL], axis=1)\n                                   .assign(Generation=lambda df: df.Generation.astype(str))\n                                   .assign(**{\"Legendary\": lambda df: df[\"Legendary\"].astype(int), \n                                              \"Type 2\": lambda df: df[\"Type 2\"].fillna(\"missing\")})\n                                   .pipe(pd.get_dummies))\nfeatures_and_targets_df = pd.concat([encoded_target_s, dummified_features_df], axis=1)","06a722b8":"encoded_target_s.sample(5)","31add87e":"le.inverse_transform(encoded_target_s.sample(5))","f539df65":"dummified_target_s.sample(5)","66d19584":"dummified_features_df.sample(5)","45ad3a62":"#\u00a0Inspired from this: https:\/\/seaborn.pydata.org\/examples\/many_pairwise_correlations.html\n\ncorr_df = pd.concat([dummified_features_df, dummified_target_s], axis=1).corr()\n\n\nmask = np.zeros_like(corr_df, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(20, 12))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr_df, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\n","b7dd898a":"train_df, test_df = train_test_split(features_and_targets_df, \n                                     stratify=encoded_target_s, \n                                     test_size=TEST_RATIO, random_state=SEED)","82fceee0":"train_df.head(1).T","f7905d27":"#\u00a0The three variations of the F1 score for unbalanced classes are different for unblanaced classes\n\ntrue_classes = [\"a\", \"b\", \"c\", \"a\", \"c\", \"c\"]\npredicted_classes = [\"a\", \"b\", \"c\", \"c\", \"c\", \"c\"]\n\nprint(\"Unbalanced: \")\nprint(\"Weighted F1 score:\", f1_score(true_classes, predicted_classes, average=\"weighted\"))\nprint(\"Micro F1 score:\", f1_score(true_classes, predicted_classes, average=\"micro\"))\nprint(\"Macro F1 score:\", f1_score(true_classes, predicted_classes, average=\"macro\"))\n\n# The three variations of the F1 score for balanced classes are the same\n\ntrue_classes = [\"a\", \"b\", \"c\", \"a\", \"b\", \"c\"]\npredicted_classes = [\"a\", \"b\", \"c\", \"a\", \"b\", \"c\"]\nprint(32 * \"-\")\n\nprint(\"Balanaced: \")\nprint(\"Weighted F1 score:\", f1_score(true_classes, predicted_classes, average=\"weighted\"))\nprint(\"Micro F1 score:\", f1_score(true_classes, predicted_classes, average=\"micro\"))\nprint(\"Macro F1 score:\", f1_score(true_classes, predicted_classes, average=\"macro\"))","2a03841f":"train_features_df = train_df.drop(ENCODED_TARGET_COL, axis=1)\ntrain_target_s = train_df[ENCODED_TARGET_COL]","91d71952":"def improvement_in_percent(model_score, baseline_score):\n    return (100 * (model_score - baseline_score)  \/ baseline_score).round(3)","c462b026":"lr = LogisticRegression(random_state=SEED)\nlr_scores = cross_val_score(lr, X=train_features_df, y=train_target_s, cv=5, scoring=\"f1_weighted\")\nprint(\"Logistic regression mean and std scores are: ({}, {})\".format(lr_scores.mean(), lr_scores.std()))\nlr.fit(train_features_df, train_target_s)","c1ecf082":"xgb_clf = XGBClassifier(random_state=SEED)\nxgb_clf_scores = cross_val_score(xgb_clf, X=train_features_df, y=train_target_s, cv=5, scoring=\"f1_weighted\")\n\"Simple XGBoost classification mean and std scores are: ({}, {})\".format(xgb_clf_scores.mean(), xgb_clf_scores.std())","2b7badeb":"\"This is a {} % improvement\".format(improvement_in_percent(xgb_clf_scores.mean(), lr_scores.mean()))","9cf283be":"#\u00a0More trees\nclf = XGBClassifier(random_state=SEED, n_estimators=1000)\nclf_scores = cross_val_score(clf, X=train_features_df, y=train_target_s, cv=5, scoring=\"f1_weighted\")\nprint(\"Alternative XGBoost classification mean and std scores are: ({}, {})\".format(clf_scores.mean(), clf_scores.std()))\nprint(\"This is a {} % improvement\".format(improvement_in_percent(clf_scores.mean(), lr_scores.mean())))","08ced017":"# Smaller learning rate\nclf = XGBClassifier(random_state=SEED, learning_rate=0.01)\nclf_scores = cross_val_score(clf, X=train_features_df, y=train_target_s, cv=5, scoring=\"f1_weighted\")\nprint(\"Alternative XGBoost classification mean and std scores are: ({}, {})\".format(clf_scores.mean(), clf_scores.std()))\nprint(\"This is a {} % improvement\".format(improvement_in_percent(clf_scores.mean(), lr_scores.mean())))","6ae25369":"class HPOptimizer(object):\n\n    def __init__(self):\n        #\u00a0A progress bar to monitor the hyperopt optimization process\n        self.pbar = tqdm(total=MAX_EVALS, desc=\"Hyperopt\")\n        self.trials = Trials()\n\n    def objective(self, hyperparameters):\n        hyperparameters = {\n            \"max_delta_step\": hyperparameters[\"max_delta_step\"],\n            \"reg_alpha\": '{:.3f}'.format(hyperparameters[\"reg_alpha\"]), \n            \"reg_lambda\": '{:.3f}'.format(hyperparameters[\"reg_lambda\"]), \n            \"n_estimators\": int(hyperparameters[\"n_estimators\"]), \n            \"max_depth\": int(hyperparameters[\"max_depth\"]),\n            \"learning_rate\": '{:.3f}'.format(hyperparameters[\"learning_rate\"]), \n            \"colsample_bytree\": '{:.3f}'.format(hyperparameters['colsample_bytree']),\n            \"gamma\": \"{:.3f}\".format(hyperparameters['gamma']),\n        }\n        print(\"The current hyperparamters are: {}\".format(hyperparameters))\n\n        clf = XGBClassifier(\n            n_jobs=4,\n            **hyperparameters\n        )\n\n        scores = cross_val_score(clf, X=train_features_df, y=train_target_s, cv=5, \n                                 scoring=\"f1_weighted\")\n        print(\"Mean and std CV scores are: ({}, {})\".format(scores.mean(), scores.std()))\n        # Update the progress bar after each iteration\n        self.pbar.update()\n        #\u00a0Since we are minimizing the objective => return -1 * mean(scores) (this is a loss)\n        return -scores.mean()\n\n    def run(self):\n        if RUN_HP_OPTIMIZATION:\n            optimal_hp = fmin(fn=objective,\n                              space=HP_SPACE,\n                              algo=tpe.suggest,\n                              trials= trials,\n                              max_evals=MAX_EVALS)\n        else:\n            optimal_hp = OPTIMAL_HP\n        self.optimal_hp = optimal_hp","5c125957":"hp_optimizer = HPOptimizer()\nhp_optimizer.run()\noptimal_hp = hp_optimizer.optimal_hp\nprint(\"The optimal hyperparamters are: {}\".format(optimal_hp))","2f20a7e1":"if RUN_HP_OPTIMIZATION:\n    hyperaramters_df = pd.DataFrame(trials.idxs_vals[1])\n    losses_df = pd.DataFrame(trials.results)\n    hyperopt_trials_df = pd.concat([losses_df, hyperaramters_df], axis=1)","de92d41c":"if RUN_HP_OPTIMIZATION:\n    #\u00a0Check that the argmin of the hyperopt_trials_df DataFrame is the same as the optimal_hp \n    min_loss_index = losses_df['loss'].argmin()\n    assert (hyperaramters_df.loc[min_loss_index, :].to_dict() == optimal_hp)","2e16c93d":"def hp_vs_loss_scatterplot(hyperparameter):\n\n    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n    hyperopt_trials_df.plot(x=hyperparameter, y='loss', kind='scatter', ax=ax)\n    best_coordinates = hyperopt_trials_df.loc[min_loss_index, [hyperparameter, \"loss\"]].values\n    ax.annotate(\"Best {}: {}\".format(hyperparameter, round(best_coordinates[0], 3)), \n                xy=best_coordinates, \n                color=\"red\")","cf2b4262":"if RUN_HP_OPTIMIZATION:\n    #\u00a0Remove the \"max_delta_step\" since it is fixed for now\n    HP_SPACE.pop(\"max_delta_step\")\n    for hyperparmeter in HP_SPACE.keys():\n        hp_vs_loss_scatterplot(hyperparmeter)","90805c3c":"parsed_optimal_hp = {\n    \"n_estimators\": int(optimal_hp[\"n_estimators\"]), \n    \"max_depth\": int(optimal_hp[\"max_depth\"]),\n    \"learning_rate\": optimal_hp[\"learning_rate\"], \n    \"colsample_bytree\": '{:.3f}'.format(optimal_hp['colsample_bytree']),\n    \"gamma\": \"{:.3f}\".format(optimal_hp['gamma']),\n}\n\nbest_xgb_clf =  XGBClassifier(random_state=SEED, **parsed_optimal_hp)\nbest_xgb_clf.fit(train_features_df, train_target_s)","c5756379":"rf_clf = RandomForestClassifier(random_state=SEED)\nrf_clf_scores = cross_val_score(rf_clf, X=train_features_df, y=train_target_s, cv=5, scoring=\"f1_weighted\")\nprint(\"Simple random forests classification mean and std scores are: ({}, {})\".format(rf_clf_scores.mean(), rf_clf_scores.std()))\nprint(\"This is a {} % improvement\".format(improvement_in_percent(rf_clf_scores.mean(), lr_scores.mean())))","eef86a93":"nn_clf = MLPClassifier(random_state=SEED)\nnn_clf_scores = cross_val_score(nn_clf, X=train_features_df, y=train_target_s, cv=5, scoring=\"f1_weighted\")\nprint(\"Simple classification neural network mean and std scores are: ({}, {})\".format(nn_clf_scores.mean(), nn_clf_scores.std()))\nprint(\"This is a {} % improvement\".format(improvement_in_percent(nn_clf_scores.mean(), lr_scores.mean())))","f7002715":"That's a better start. Let's see if one can improve things. ","cab0de61":"TPOTClassifier?","cc68ff42":"# Previous values: TPOT_GENERATION=15 and TPOT_POPULATION_SIZE=80\nTPOT_GENERATION = 20\nTPOT_POPULATION_SIZE = 100\n#\u00a0TPOT will have TPOT_POPULATION_SIZE + offspring_size * TPOT_GENERATION runs in total. \n#\u00a0The offspring_size is set to 100 by default.\n\n\ntpot_clf = TPOTClassifier(generations=TPOT_GENERATION, \n                          population_size=TPOT_POPULATION_SIZE,\n                          random_state=SEED, cv=5, \n                          n_jobs=-1, memory='auto', \n                          early_stop = 10,\n                          verbosity=2, scoring=\"f1_weighted\")\ntpot_clf.fit(train_features_df, train_target_s)","5e4a5221":"test_features_df = test_df.drop(ENCODED_TARGET_COL, axis=1)\nencoded_test_targets_s = test_df[ENCODED_TARGET_COL]","647b7d29":"def test_evaluation(clf):\n    \"\"\"\n    Evaluate a classifier on the test dataset. Returns a confusion matrix and F1 score. \n    \"\"\"\n    encoded_test_predictions_s = clf.predict(test_features_df)\n    test_predictions_s = pd.Series(le.inverse_transform(encoded_test_predictions_s), \n                                   name=\"predicted_type_1\")\n    test_targets_s = pd.Series(le.inverse_transform(encoded_test_targets_s), \n                               name=\"true_type_1\")\n    test_cm = confusion_matrix(test_targets_s, test_predictions_s)\n    test_f1_score = f1_score(test_targets_s, test_predictions_s, average='weighted').round(3)\n    return test_cm, test_f1_score","5133cf20":"test_cm_tpot, test_f1_score_tpot = test_evaluation(tpot_clf)\ntest_cm_best_xgb, test_f1_score_best_xgb = test_evaluation(best_xgb_clf)\ntest_cm_lr, test_f1_score_lr = test_evaluation(lr)","98dbcbde":"print(\"Tpot test F1 weighted score is {}\".format(test_f1_score_tpot))\nprint(\"Best XGBoost test F1 weighted score is {}\".format(test_f1_score_best_xgb))\nprint(\"Logistic regression test F1 weighted score is {}\".format(test_f1_score_lr))","93af8da1":"#\u00a0Confusion matrix for Tpot\nplot_confusion_matrix(test_cm_tpot, classes=target_s.unique())\nplot_confusion_matrix(test_cm_tpot, classes=target_s.unique(), normalize=True)","4cb5c820":"In the following notebook, I will try to predict the **major type** (`Type 1` column in this dataset) \nof Pokemons given various features (more about this in what follows).  \n\nBefore you start reading this notebook, I highly recommend checking a previous [EDA notebook](https:\/\/www.kaggle.com\/yassinealouini\/pokemon-eda) where I explore more in details the dataset. \n\nEnjoy!","47d7b1c1":"Let's try to vary the hyperparamters for the XGBoost classifier and see what we get.","97e67723":"#\u00a0Baseline model","39d8acc4":"# Evaluation metric","6691e2c5":"##\u00a0Tuning the XGBoost classifier","84d2a223":"As with any ML problem, one usually starts by establishing a baseline, i.e. a score\/error that one aims at improving. \nWhy is that important? Well, without a baseline, it is hard to tell if one is making progress or not. Moreover, some problems are much easier than others: a very high accuracy might look impressive\nbut is less impressive one compared to a high accuracy obtained with a very simple model. \n\nAs a baseline, let's use a linear regression model.\n","10e7e5e9":"To end this preparation phase, let's see if there are any **correlations**","a4718532":"#\u00a0TPOT","4befda41":"Alright, now that the features have been prepared and split, it is time to pick an evaluation metric. \n\nSince this an **nbalanced multi-class classification** problem, I will be using the [**F1 score**](https:\/\/en.wikipedia.org\/wiki\/F1_score) with **weighted** average: the F1 score is computed for each class then we take the weighted average using the true classes count.\n\nCheck the sklearn documentation for more details [here](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.f1_score.html). In what follows, I have provied two examples of usage of the `f1_score`(unblanaced and balanced classes). ","033e1930":"Let's try other models starting with a random forests classifier","6437be8d":"Some correlations: \n\n* **Steel** major types tend to be positivelt correlated with **Defense** and **Psychic** with ** Special Attack**. \n* **Ghost** major types tend to be positvely correlated with a Type 2 of **Grass** and **Grass** with **Poison**. \n* **Fairy** major types tend to be positvely correlated with the **Generation** 6. \n* **Dragon** major types tend to be positvely correlated with **Attack** and **Total**. \n\nThese observations aren't surprising to any true Pokemon connoisseur but are, nonethless, reassuring to find using the data. ","ec3279c2":"#\u00a0Stacking ","a3b4601e":"Let's train our best XGBoost classifier (using the optimal hyperparamters) on the train dataet then evaluate it on the test dataset.","57d6f76e":"#\u00a0Train tuned XGBoost classifier model on train data and evaluate on test","091cf159":"Could we do better?","e305af82":"A \"simple\" (no hyperparameters tuning) XGBoost classifier does better than the baseline. ","302f2e19":"As you can see, trying different hyperparamters values manually would be tedious. Is there a better way?\nFortunately, there is (at least) one method: using an automatic hyperparameter optimizaton tool. \n    \nOne of these is [**hyperopt**](http:\/\/https:\/\/github.com\/hyperopt\/hyperopt).","257081c5":"Based on the target's historgrams: \n\n1. This is a **multi-class** (**18** major types) **classification** (categorical target) problem\n2. This is an **unblanaced** problem. Indeed, some types (fairy and flying) are much less common than the other ones.","f6781b18":"That's impressive. Tpot is by far the winner!","4a29990f":"# Exploring the hyperopt trials","65470ec2":"Now that a baseline score has been found, let's try to improve it. ","57c52dcf":"#\u00a0Neural network","9d091393":"Some ideas to test: \n\n* More hyperopt iterations and other hypreparamters to optimize. This had the effect of improving the test F1 weighted score. Add more regularization?\n* Use 3 folds CV instead of 5 folds CV. \n* Change the objective to optimize (try something that accounts for the classes' imbalance).\n* Try a neural network.\n* Try random forests.\n* Try stacking.\n* Try TPOT => done\n* Try TPOT with more generations and bigger poupulation size (for now: generations=10, population_size=40)\n\nI hope you have enjoyed this notebook. Stay tuned for updates!","6b62b6df":"#\u00a0Load the data and quick exploration","74812a51":"#\u00a0Test evaluation","3ebce228":"Notice that some major types (check the EDA notebook) aren't present for all the generations: flying, dark, and steel types aren't available for the six generations. \n\nThus some **features engineering** based on the `Generation` column might be useful. ","9ea0894e":"Notice that the `#` and `Name` columns aren't useful for predicting the major type so will be dropped (these are the `TO_DROP_COLS`).  ","9079392c":"As mentionned in the beginning, I will predict the major type (this is the `Type 1` column). \nLet's explore the target to start. ","6ea758cf":"#\u00a0Random Forests","d35cb1eb":"#\u00a0Hyperopt + XGboost","216aa7a0":"Let's explore the saved trials (these are handy to store hyperopt runs).","6c0ac6f7":"I will split the features and targets into train and test datasets. \n\nThe test dataset will only be used at the end to evaluate the various trained models (you should do this as well whenever you train an ML model). Next, I will use cross validation to train and evaluate the model using the train dataset. ","dcac14c9":"Alright. Let's stack our best models and use an XGBoost as a second-level model. \nTo be continued...","26277ab2":"Let's **dummify** (i.e. transform categorical columns into boolean ones) the target, the `Type 2` and `Generation` columns. \n\nFor that I use pandas [`get_dummies`](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.get_dummies.html) function. Also, since not every Pokemon has a `Type 2`, I have filled the missing values with the \"missing\" type before dummifying. Notice also that I have used a `LabelEncoder` for the target col (since the target contains strings). \n\nFinally, I drop the `TARGET_COL` and `TO_DROP_COLS` (i.e. `Name` and `#`) columns from the features. ","a442fbd6":"##\u00a0Train and test split","49bbf8b1":"#\u00a0Simple XGBoost","59324f8c":"#\u00a0To wrap up","c01a0459":"This isn't very promising for a start. Probably will neeed some hp tuning..."}}