{"cell_type":{"f6aeda93":"code","8143ceee":"code","221f1a11":"code","78e91451":"code","e34cdb6b":"code","3dfa3c0c":"code","b85cdfda":"code","a06e29a7":"code","1775891b":"code","74bb7e82":"code","2b1642a8":"code","586d7599":"code","9261bacc":"code","d57bab42":"code","ef70143b":"code","94fe73f9":"code","c2ef8e19":"code","980db016":"code","71be52ca":"code","c42e5cb7":"code","3724b063":"code","24118ef2":"code","6618faf8":"code","738a67f6":"code","704775b7":"code","0fdf9046":"code","dffff7a7":"code","9d4ca2f9":"code","547b93ce":"code","9ad076bd":"code","88811ea8":"code","f20d8b29":"code","2c66cac1":"code","70300684":"code","ebf85c4a":"code","eca9b519":"code","e3a0ab35":"code","aa4417e9":"code","6e0db708":"code","00fda11e":"code","560bca42":"code","d83bcdab":"code","ab8e64d9":"code","c4053d03":"code","1dc05a99":"code","94e54888":"code","98791237":"code","4766795b":"code","bfd4adb2":"code","9607ae1b":"code","439d4a71":"code","726097b4":"code","cd7a1b9b":"code","f70124cf":"code","02a82f80":"code","92edf8ba":"code","a3aa2d20":"code","3e20a295":"code","5bc19925":"code","2eb3f1b1":"code","dbaea08c":"code","546de4db":"code","0906a1c0":"code","e3bbc4c9":"code","d4c2a4d0":"code","29cb5ae0":"code","af18c980":"code","fa4040ad":"code","e2e5d1ad":"code","3b0888f2":"code","e4b95d9b":"code","44f4e9f1":"code","06836eab":"code","0ed6dbb4":"code","0a04dece":"code","ccc63a5b":"code","5ffd5a8b":"code","910f0fda":"code","9870cb94":"code","9f69a6b5":"code","fbb773a6":"code","23cefa51":"code","e9989ec5":"code","3c65d9ac":"code","513031f8":"code","b1d6551f":"code","69d1a51b":"code","c0a960bf":"code","94a4701d":"code","ecac2b21":"code","77f9843c":"code","0908521c":"code","eef3ed0f":"code","3d7fd206":"code","2eb97499":"code","1eb70c61":"code","1a0d0777":"code","dbbf00c9":"code","fdd894dd":"code","26544dd5":"code","22c92bd3":"code","1f056eb1":"code","2a527454":"code","f7150ce9":"code","e9394e6e":"code","024bc478":"code","da3724f6":"code","112837c8":"code","2951696f":"code","caf2358d":"code","4cdfb0dc":"code","9729adf3":"code","99470406":"code","7f578dee":"code","9475d593":"markdown","3f079140":"markdown","6b18571b":"markdown","030fbace":"markdown","e90a9dba":"markdown","adbe42de":"markdown","7fc88723":"markdown","8eed2064":"markdown","58af7572":"markdown","ca36b818":"markdown","e7bdf96c":"markdown","9b25e529":"markdown","10d93ebb":"markdown","0c860391":"markdown","94e0aaca":"markdown"},"source":{"f6aeda93":"!pip install autoviml","8143ceee":"import tensorflow as tf\nprint(tf.__version__)","221f1a11":"import numpy as np\nimport pandas as pd\nimport pickle\n\n# Data Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport wordcloud\nfrom wordcloud import WordCloud, STOPWORDS\n\n# Text Processing\nimport re\nimport itertools\nimport string\nimport collections\nfrom collections import Counter, namedtuple\n\nfrom sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\n\nfrom nltk.stem.porter import PorterStemmer\n\nfrom tensorflow.keras.models import load_model\nfrom keras.models import Sequential\nfrom keras import layers\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import LSTM, Dense,Dropout,Flatten,GlobalMaxPool1D\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom keras.optimizers import Adam, RMSprop\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\n# Ignore noise warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")","78e91451":"data = pd.read_csv('..\/input\/project\/essays (2).csv',encoding='cp1252')\ndata.drop(['#AUTHID'], axis=1, inplace = True)\ndata.head(10)","e34cdb6b":"data.describe().T","3dfa3c0c":"data.shape","b85cdfda":"data.info()","a06e29a7":"data['TEXT'][100]","1775891b":"def convert(post):\n    posts = re.sub(\"[^a-zA-Z]\", \" \", post)\n    post = re.sub('\\s+', ' ', post)\n    post = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", post)\n    post = re.sub(r\"\\'s\", \" \\'s \", post)\n    post = re.sub(r\"\\'ve\", \" have \", post)\n    post = re.sub(r\"n\\'t\", \" not \", post)\n    post = re.sub(r\"\\'re\", \" are \", post)\n    post = re.sub(r\"\\'d\" , \" would \", post)\n    post = re.sub(r\"\\'ll\", \" will \", post)\n    post = re.sub(r\",\", \" , \", post)\n    post = re.sub(r\"!\", \" ! \", post)\n    post = re.sub(r\"\\(\", \" ( \", post)\n    post = re.sub(r\"\\)\", \" ) \", post)\n    post = re.sub(r\"\\?\", \" \", post)\n    post = re.sub(r'[^\\w\\s]','',post)\n    post = re.sub(' +', ' ', post).lower()\n    \n\n    post = post.strip()\n\n    return post","74bb7e82":"convert(data['TEXT'][100])","2b1642a8":"corpus = data[\"TEXT\"].apply(convert)","586d7599":"corpus","9261bacc":"df_new = corpus.to_frame()","d57bab42":"categorical_column = ['cEXT', 'cNEU', 'cAGR', 'cCON', 'cOPN']\nfor i in categorical_column:\n    df_new[i] = data[i]\n\ndf_new","ef70143b":"df_new.to_csv('cleaned.csv', index = False) ","94fe73f9":"df = pd.read_csv(r'..\/input\/project\/cleanedStopped (1).csv')\nlabel_map = {'n':0, 'y':1}\ndf['cEXT'] = df['cEXT'].map(label_map)\ndf['cNEU'] = df['cNEU'].map(label_map)\ndf['cAGR'] = df['cAGR'].map(label_map)\ndf['cCON'] = df['cCON'].map(label_map)\ndf['cOPN'] = df['cOPN'].map(label_map)\ndf.head()","c2ef8e19":"fig, ax = plt.subplots(1, 5, figsize=(20,15))\ngenre_col = ['navy','crimson']\ncol=['cEXT', 'cNEU', 'cAGR', 'cCON', 'cOPN']\nfor i in range(len(col)):\n      count = df[col[i]].value_counts()\n      ax[i].pie(x = count.values, labels = None, autopct='%1.1f%%', startangle=90, textprops={'size': 12, 'color': 'white'},\n          pctdistance=0.5, radius=1.3, colors=genre_col)\n      ax[i].set_title(col[i], color='darkred', fontdict={'fontsize': 15})\n\nfig.legend([\"No\",\"Yes\"], loc='center right', fontsize=12)\nfig.show()","980db016":"df[\"length_posts\"] = df[\"TEXT\"].apply(len)\nsns.distplot(df[\"length_posts\"]).set_title(\"Distribution of Lengths of all 50 Posts\")","71be52ca":"#Finding the most common words in all posts.\nwords = list(df[\"TEXT\"].apply(lambda x: x.split()))\nwords = [x for y in words for x in y]\nCounter(words).most_common(40)","c42e5cb7":"#Plotting the most common words with WordCloud.\nwc = wordcloud.WordCloud(width=1200, height=500, \n                         collocations=False, background_color=\"white\", \n                         colormap=\"tab20b\").generate(\" \".join(words))\n\n# collocations to False  is set to ensure that the word cloud doesn't appear as if it contains any duplicate words\nplt.figure(figsize=(25,10))\n# generate word cloud, interpolation \nplt.imshow(wc, interpolation='bilinear')\n_ = plt.axis(\"off\")","3724b063":"fig, ax = plt.subplots(10, sharex=True, figsize=(20,15))\n\nk = 0\nj=0\nfor i in ['cEXT', 'cNEU', 'cAGR', 'cCON', 'cOPN']:\n  for j in [0,1]:\n    df_4 = df[df[i] == j]\n    wordcloud = WordCloud(max_words=1628,relative_scaling=1,normalize_plurals=False).generate(df_4['TEXT'].to_string())\n    plt.subplot(5,2,k+1)\n    plt.xticks([])  \n    plt.yticks([])  \n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.title(i+\" for : \"+str(j))\n    ax[k].axis(\"off\")\n    k+=1","24118ef2":"def plot_history(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training acc')\n    plt.plot(x, val_acc, 'r', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()","6618faf8":"df = pd.read_csv(r'..\/input\/project\/cleanedStopped (1).csv')\ndf","738a67f6":"# text_clf = Pipeline([('countvet', CountVectorizer()),('vect', TfidfVectorizer()), \n#                       ('clf', MultinomialNB()) ])","704775b7":"# convet = CountVectorizer(max_features=10000)\nconvet = CountVectorizer()\ntfidf = TfidfTransformer()\n\nconvet.fit(df['TEXT'])\npickle.dump(convet, open(\"countvet.pickel\", \"wb\"))\nconvect = pickle.load(open(\"countvet.pickel\", \"rb\"))\nvect = convet.transform(df['TEXT'])\n\ntfidf.fit(vect)\npickle.dump(tfidf, open(\"tfidfvet.pickel\", \"wb\"))\ntfidf = pickle.load(open(\"tfidfvet.pickel\", \"rb\"))\nvect = tfidf.transform(vect)","0fdf9046":"print(vect.shape)","dffff7a7":"label_map = {'n':0, 'y':1}\ndf['cEXT'] = df['cEXT'].map(label_map)\ndf['cNEU'] = df['cNEU'].map(label_map)\ndf['cAGR'] = df['cAGR'].map(label_map)\ndf['cCON'] = df['cCON'].map(label_map)\ndf['cOPN'] = df['cOPN'].map(label_map)","9d4ca2f9":"df","547b93ce":"from sklearn.model_selection import train_test_split\nfeatures_train, features_test, ext_train, ext_test, neu_train, neu_test, agr_train, agr_test, con_train, con_test, opn_train, opn_test = train_test_split(vect, df['cEXT'],df['cNEU'],df['cAGR'],df['cCON'], df['cOPN'], test_size = 0.20, random_state = 0)","9ad076bd":"features_train.shape, ext_train.shape, features_test.shape, ext_test.shape","88811ea8":"from xgboost import XGBClassifier\n\n# fit model on training data\nEXT = XGBClassifier()\nEXT.fit(features_train, ext_train)\npickle.dump(EXT, open(\"extmodel.sav\", \"wb\"))\nEXT = pickle.load(open(\"extmodel.sav\", \"rb\"))\next_score_train=EXT.score(features_train,ext_train)\next_score_test=EXT.score(features_test, ext_test)\n\nNEU = XGBClassifier()\nNEU.fit(features_train, neu_train)\npickle.dump(NEU, open(\"neumodel.sav\", \"wb\"))\nNEU = pickle.load(open(\"neumodel.sav\", \"rb\"))\nneu_score_train=NEU.score(features_train,neu_train)\nneu_score_test=NEU.score(features_test,neu_test)\n\n\nAGR = XGBClassifier()\nAGR.fit(features_train, agr_train)\npickle.dump(AGR, open(\"agrmodel.sav\", \"wb\"))\nAGR = pickle.load(open(\"agrmodel.sav\", \"rb\"))\nagr_score_train=AGR.score(features_train,agr_train)\nagr_score_test=AGR.score(features_test,agr_test)\n\nCON = XGBClassifier()\nCON.fit(features_train, con_train)\npickle.dump(CON, open(\"conmodel.sav\", \"wb\"))\nCON = pickle.load(open(\"conmodel.sav\", \"rb\"))\ncon_score_train = CON.score(features_train,con_train)\ncon_score_test = CON.score(features_test,con_test)\n\nOPN = XGBClassifier()\nOPN.fit(features_train, opn_train)\npickle.dump(OPN, open(\"opnmodel.sav\", \"wb\"))\nOPN = pickle.load(open(\"opnmodel.sav\", \"rb\"))\nopn_score_train = OPN.score(features_train,opn_train)\nopn_score_test = OPN.score(features_test,opn_test)","f20d8b29":"print('EXT train score is :',ext_score_train)\nprint('EXT test score is :',ext_score_test)\n\nprint('NEU train score is :',neu_score_train)\nprint('NEU test score is :',neu_score_test)\n\nprint('AGR train score is :',agr_score_train)\nprint('AGR test score is :',agr_score_test)\n\nprint('CON train score is :',con_score_train)\nprint('CON test score is :',con_score_test)\n\nprint('OPN train score is :',opn_score_train)\nprint('OPN test score is :',opn_score_test)","2c66cac1":"from sklearn.naive_bayes import MultinomialNB\n\nfrom xgboost import XGBClassifier\n\n# fit model on training data\nEXT = MultinomialNB()\nEXT.fit(features_train, ext_train)\npickle.dump(EXT, open(\"extmodel.sav\", \"wb\"))\nEXT = pickle.load(open(\"extmodel.sav\", \"rb\"))\next_score_train=EXT.score(features_train,ext_train)\next_score_test=EXT.score(features_test, ext_test)\n\nNEU = MultinomialNB()\nNEU.fit(features_train, neu_train)\npickle.dump(NEU, open(\"neumodel.sav\", \"wb\"))\nNEU = pickle.load(open(\"neumodel.sav\", \"rb\"))\nneu_score_train=NEU.score(features_train,neu_train)\nneu_score_test=NEU.score(features_test,neu_test)\n\n\nAGR = MultinomialNB()\nAGR.fit(features_train, agr_train)\npickle.dump(AGR, open(\"agrmodel.sav\", \"wb\"))\nAGR = pickle.load(open(\"agrmodel.sav\", \"rb\"))\nagr_score_train=AGR.score(features_train,agr_train)\nagr_score_test=AGR.score(features_test,agr_test)\n\nCON = MultinomialNB()\nCON.fit(features_train, con_train)\npickle.dump(CON, open(\"conmodel.sav\", \"wb\"))\nCON = pickle.load(open(\"conmodel.sav\", \"rb\"))\ncon_score_train = CON.score(features_train,con_train)\ncon_score_test = CON.score(features_test,con_test)\n\nOPN = MultinomialNB()\nOPN.fit(features_train, opn_train)\npickle.dump(OPN, open(\"opnmodel.sav\", \"wb\"))\nOPN = pickle.load(open(\"opnmodel.sav\", \"rb\"))\nopn_score_train = OPN.score(features_train,opn_train)\nopn_score_test = OPN.score(features_test,opn_test)","70300684":"print('EXT train score is :',ext_score_train)\nprint('EXT test score is :',ext_score_test)\n\nprint('NEU train score is :',neu_score_train)\nprint('NEU test score is :',neu_score_test)\n\nprint('AGR train score is :',agr_score_train)\nprint('AGR test score is :',agr_score_test)\n\nprint('CON train score is :',con_score_train)\nprint('CON test score is :',con_score_test)\n\nprint('OPN train score is :',opn_score_train)\nprint('OPN test score is :',opn_score_test)","ebf85c4a":"df = pd.read_csv(r'..\/input\/project\/cleanedStopped (1).csv')\nlabel_map = {'n':0, 'y':1}\ndf['cEXT'] = df['cEXT'].map(label_map)\ndf['cNEU'] = df['cNEU'].map(label_map)\ndf['cAGR'] = df['cAGR'].map(label_map)\ndf['cCON'] = df['cCON'].map(label_map)\ndf['cOPN'] = df['cOPN'].map(label_map)\ndf.head()","eca9b519":"#Count Vectorizer + TFIDF Vectorizer\n# convet = CountVectorizer(max_features=10000)\nconvet = CountVectorizer()\ndf = df.sample(frac = 1)\n\nconvet.fit(df['TEXT'])\npickle.dump(convet, open(\"countvet.pickel\", \"wb\"))\nconvect = pickle.load(open(\"countvet.pickel\", \"rb\"))\nvect = convet.transform(df['TEXT'])","e3a0ab35":"vect.shape","aa4417e9":"y = y= df[['cEXT', 'cNEU', 'cAGR', 'cCON', 'cOPN']].values\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(vect,y,train_size=0.8,random_state=42)","6e0db708":"early_stopping = EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=10, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)\n\ncallbacks_save = ModelCheckpoint('count_dense_personality.hdf5', \n                                 monitor='val_loss', \n                                 mode='min', \n                                 save_best_only=True)\n\noptimizer=Adam(lr=1e-3,beta_1=0.9,beta_2=0.999)","00fda11e":"X_train.shape[1]","560bca42":"input_dim = X_train.shape[1]  # Number of features\n\nmodel = Sequential()\nmodel.add(Dense(10, input_dim=input_dim, activation='relu'))\nmodel.add(Dense(5, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","d83bcdab":"history_dense = model.fit(X_train,y_train,epochs=50,validation_data=(X_test,y_test),batch_size=16, callbacks=[early_stopping, callbacks_save])","ab8e64d9":"history_dense.history","c4053d03":"plot_history(history_dense)","1dc05a99":"df.head()","94e54888":"text = [df['TEXT'][713]]","98791237":"text_vect = convet.transform(text)","4766795b":"text_vect","bfd4adb2":"text_vect.shape","9607ae1b":"y_hat = model.predict(text_vect)","439d4a71":"y_hat[0]","726097b4":"mo = load_model(r'.\/count_dense_personality.hdf5')","cd7a1b9b":"mo.summary()","f70124cf":"y_hat2 = mo.predict(text_vect)","02a82f80":"y_hat2[0]","92edf8ba":"df = df.sample(frac = 1)\n\nx = df['TEXT'].values\ny= df[['cEXT', 'cNEU', 'cAGR', 'cCON', 'cOPN']].values\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(x)\nword_to_index = tokenizer.word_index\n\npickle.dump(tokenizer, open(\"token.pickel\", \"wb\"))\ntokenizer = pickle.load(open(\"token.pickel\", \"rb\"))\n\nx = tokenizer.texts_to_sequences(x)\nprint(x[0])\n\nvocab_size = len(word_to_index)+1\nprint(vocab_size)\nmax_length = 10000\nembedding_dim = 16\nx = pad_sequences(x, maxlen=max_length)","a3aa2d20":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(x,y,train_size=0.8,random_state=42)","3e20a295":"early_stopping = EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=10, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)\n\ncallbacks_save = ModelCheckpoint('embedding_dense_personality.hdf5', \n                                 monitor='val_loss', \n                                 mode='min', \n                                 save_best_only=True)\n\noptimizer=Adam(lr=1e-3,beta_1=0.9,beta_2=0.999)","5bc19925":"embedding_dim = 50\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\nmodel.add(Flatten())\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dense(5, activation='sigmoid'))\n\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\nmodel.summary()","2eb3f1b1":"history_dense_embedding = model.fit(X_train,y_train,epochs=50,validation_data=(X_test,y_test),batch_size=32, callbacks=[early_stopping, callbacks_save])","dbaea08c":"print(history_dense_embedding.history)","546de4db":"plot_history(history_dense_embedding)","0906a1c0":"mo = load_model(r'.\/embedding_dense_personality.hdf5')\nmo.summary()","e3bbc4c9":"text","d4c2a4d0":"text_vect = tokenizer.texts_to_sequences([text])\ntext_vect = pad_sequences(text_vect, maxlen=max_length)","29cb5ae0":"y_hat3 = mo.predict(text_vect)\ny_hat3[0]","af18c980":"early_stopping = EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=10, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)\n\ncallbacks_save = ModelCheckpoint('embedding_dense_maxpool_personality.hdf5', \n                                 monitor='val_loss', \n                                 mode='min', \n                                 save_best_only=True)\n\noptimizer=Adam(lr=1e-3,beta_1=0.9,beta_2=0.999)","fa4040ad":"embedding_dim = 50\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dense(5, activation='sigmoid'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()","e2e5d1ad":"history_dense_maxpool_embedding = model.fit(X_train,y_train,epochs=50,validation_data=(X_test,y_test),batch_size=32, callbacks=[early_stopping, callbacks_save])","3b0888f2":"history_dense_maxpool_embedding.history","e4b95d9b":"plot_history(history_dense_maxpool_embedding)","44f4e9f1":"df = pd.read_csv(r'.\/cleaned.csv')\nlabel_map = {'n':0, 'y':1}\ndf['cEXT'] = df['cEXT'].map(label_map)\ndf['cNEU'] = df['cNEU'].map(label_map)\ndf['cAGR'] = df['cAGR'].map(label_map)\ndf['cCON'] = df['cCON'].map(label_map)\ndf['cOPN'] = df['cOPN'].map(label_map)\ndf.head()","06836eab":"#if needed for WORDCLOUD\n\n# for i in ['cEXT', 'cNEU', 'cAGR', 'cCON', 'cOPN']:\n#   for j in [0,1]:\n#     fig , ax1 = plt.subplots(1 , 1 , figsize = [26,8])\n#     dx = df[df[i] == j][\"TEXT\"]\n#     wordcloud1 = WordCloud(background_color = 'black' , width = 600 , height = 400).generate(\" \".join(dx))\n#     ax1.imshow(wordcloud1)\n#     ax1.axis('off')\n#     ax1.set_title('Wordcloud for posts '+ i + \" : \"+str(j) , fontsize = 20)","0ed6dbb4":"# vocab_size = 20000\n# sent_length = 7000\n# embedding_vocab_features = 50","0a04dece":"# from tensorflow.keras.preprocessing.text import Tokenizer\n# max_len=200   # maximum words in a sentence\n# VAL_SPLIT = 0.2\n\n# tokenizer = Tokenizer()\n# text=df['TEXT']\n# tokenizer.fit_on_texts(text)\n# max_features = len(tokenizer.word_index) + 1 # maximum number of unique words\n# print(max_features)\n\n# input_sequences = []\n# for line in (df):\n#     print(line)\n#     token_list = tokenizer.texts_to_sequences([line])[0]\n#     print(\"Token list : \",end=\"\")\n#     print(token_list)\n#     for i in range(1, len(token_list)):\n#         n_gram_sequence = token_list[:i+1]\n#         input_sequences.append(n_gram_sequence)\n#     print(\"Input sequences : \",end=\"\")\n#     print(input_sequences)\n#     break","ccc63a5b":"\n\ndf = df.sample(frac = 1)\n\nx = df['TEXT'].values\ny= df[['cEXT', 'cNEU', 'cAGR', 'cCON', 'cOPN']].values\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(x)\nword_to_index = tokenizer.word_index\nx = tokenizer.texts_to_sequences(x)\nprint(x[0])\n\nvocab_size = len(word_to_index)+1\nprint(vocab_size)\nmax_length = 10000\nembedding_dim = 16\nx = pad_sequences(x, maxlen=max_length)","5ffd5a8b":"y","910f0fda":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(x,y,train_size=0.8,random_state=42)","9870cb94":"early_stopping = EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=10, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)\n\ncallbacks_save = ModelCheckpoint('cnn_best_personality.hdf5', \n                                 monitor='val_loss', \n                                 mode='min', \n                                 save_best_only=True)\n\noptimizer=Adam(lr=1e-3,beta_1=0.9,beta_2=0.999)\n# optimizer=Adam(lr=1e-3)","9f69a6b5":"model=Sequential()\nmodel.add(layers.Embedding(vocab_size, embedding_dim, input_length=max_length))\nmodel.add(layers.Conv1D(32, 7, activation='relu'))\nmodel.add(layers.MaxPooling1D(5))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Conv1D(32, 7, activation='relu'))\nmodel.add(layers.GlobalMaxPooling1D())\nmodel.add(layers.Dense(5,activation='sigmoid'))\nmodel.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n# model.summary()","fbb773a6":"model.summary","23cefa51":"y_train.shape","e9989ec5":"history = model.fit(X_train,y_train,epochs=50,validation_data=(X_test,y_test),batch_size=32, callbacks=[early_stopping, callbacks_save])","3c65d9ac":"history.history","513031f8":"plot_history(history)","b1d6551f":"# df = pd.read_csv(r'cleaned.csv')\ndf = pd.read_csv(r'.\/cleaned.csv')\nlabel_map = {'n':0, 'y':1}\ndf['cEXT'] = df['cEXT'].map(label_map)\ndf['cNEU'] = df['cNEU'].map(label_map)\ndf['cAGR'] = df['cAGR'].map(label_map)\ndf['cCON'] = df['cCON'].map(label_map)\ndf['cOPN'] = df['cOPN'].map(label_map)\ndf","69d1a51b":"df = df.sample(frac = 1)\ncorpus = df['TEXT'].copy()\ncorpus = corpus.to_list()","c0a960bf":"def word_counter(text):  \n    \n    count = Counter()\n    for i in text.values:\n        for word in i.split():\n            count[word] += 1\n    return count    \n\ntext = df['TEXT']\ncounter = word_counter(text)\n\nvocab_size = len(counter)\nprint(\"Vocabulary size is \"+str(vocab_size))","94a4701d":"vocab_size = 29903\nmax_length = 10000      #sentence length for padding the maximum length of the sentence present\nembedding_dim = 16","ecac2b21":"one_hot_repr = [one_hot(words, vocab_size) for words in corpus]\nembedded_docs = pad_sequences(one_hot_repr, padding='pre', maxlen=max_length)","77f9843c":"X = np.array(embedded_docs)\ny= df[['cEXT', 'cNEU', 'cAGR', 'cCON', 'cOPN']].values\nfrom sklearn.model_selection import train_test_split\n# X_train, X_test, ext_train, ext_test, neu_train, neu_test, agr_train, agr_test, con_train, con_test, opn_train, opn_test = train_test_split(X, df['cEXT'],df['cNEU'],df['cAGR'],df['cCON'], df['cOPN'], test_size = 0.20, random_state = 0)\nX_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.8,random_state=42)","0908521c":"df = pd.read_csv(r'.\/cleaned.csv')\nlabel_map = {'n':0, 'y':1}\ndf['cEXT'] = df['cEXT'].map(label_map)\ndf['cNEU'] = df['cNEU'].map(label_map)\ndf['cAGR'] = df['cAGR'].map(label_map)\ndf['cCON'] = df['cCON'].map(label_map)\ndf['cOPN'] = df['cOPN'].map(label_map)\ndf.head()","eef3ed0f":"df = df.sample(frac = 1)\n\nx = df['TEXT'].values\ny= df[['cEXT', 'cNEU', 'cAGR', 'cCON', 'cOPN']].values\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(x)\nword_to_index = tokenizer.word_index\nx = tokenizer.texts_to_sequences(x)\nprint(x[0])\n\nvocab_size = len(word_to_index)+1\nprint(vocab_size)\nmax_length = 10000\nembedding_dim = 16\nx = pad_sequences(x, maxlen=max_length)","3d7fd206":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(x,y,train_size=0.8,random_state=42)","2eb97499":"\nearly_stopping = EarlyStopping(\n    monitor='val_loss',\n    min_delta=0.0001, # minimium amount of change to count as an improvement\n    patience=10, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)\n\ncallbacks_save = ModelCheckpoint('lstm_best_personality.hdf5', \n                                 monitor='val_loss', \n                                 mode='min', \n                                 save_best_only=True)\n\noptimizer=Adam(learning_rate=1e-8)\n# optimizer=Adam(lr=1e-3)","1eb70c61":"#model making\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embedding_dim, input_length = max_length))\nmodel.add(LSTM(64))\nmodel.add(Dense(256, activation=\"relu\"))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(5, activation='sigmoid'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nprint(model.summary())","1a0d0777":"#training the model\nhistory_lstm = model.fit(X_train,y_train,epochs=50,validation_data=(X_test,y_test),batch_size=32, callbacks=[early_stopping, callbacks_save])","dbbf00c9":"history_lstm.history","fdd894dd":"plot_history(history_lstm)","26544dd5":"import pandas as pd\nimport numpy as np\nimport nltk \nimport re\nfrom pandas_profiling import ProfileReport\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport sklearn.metrics\nimport sklearn\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","22c92bd3":"df = pd.read_csv(r'.\/cleaned.csv')\n","1f056eb1":"df = pd.read_csv(r'.\/cleaned.csv')\nlabel_map = {'n':0, 'y':1}\ndf['cEXT'] = df['cEXT'].map(label_map)\ndf['cNEU'] = df['cNEU'].map(label_map)\ndf['cAGR'] = df['cAGR'].map(label_map)\ndf['cCON'] = df['cCON'].map(label_map)\ndf['cOPN'] = df['cOPN'].map(label_map)\ndf","2a527454":"df = df.sample(frac = 1)\n\nx = df['TEXT'].values\ny= df[['cEXT', 'cNEU', 'cAGR', 'cCON', 'cOPN']].values\ndf","f7150ce9":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(x,y,train_size=0.8,random_state=42)","e9394e6e":"X_train","024bc478":"df","da3724f6":"from autoviml.Auto_NLP import Auto_NLP","112837c8":"input_feature, target1 = \"TEXT\", 'cOPN'\ntarget2 = 'cEXT'\ntarget3 =  'cNEU'\ntarget4 = 'cAGR'\ntarget5 = 'cCON'","2951696f":"train, test = train_test_split(df, test_size=0.2)","caf2358d":"train_x1, test_x1, final1, predicted1 = Auto_NLP(input_feature, train, test,target1,\n                                            score_type=\"balanced_accuracy\",\n                                            top_num_features=100,\n                                            modeltype=\"classification\",\n                                            verbose=2,\n                                            build_model=True)\n                                            \n","4cdfb0dc":"\ntrain_x2, test_x2, final2, predicted2= Auto_NLP(input_feature, train, test,target2,\n                                            score_type=\"balanced_accuracy\",\n                                            top_num_features=100,\n                                            modeltype=\"classification\",\n                                            verbose=2,\n                                            build_model=True)","9729adf3":"train_x3, test_x3, final3, predicted3= Auto_NLP(input_feature, train, test,target3,\n                                            score_type=\"balanced_accuracy\",\n                                            top_num_features=100,\n                                            modeltype=\"classification\",\n                                            verbose=2,\n                                            build_model=True)","99470406":"train_x4, test_x4, final4, predicted4= Auto_NLP(input_feature, train, test,target4,\n                                            score_type=\"balanced_accuracy\",\n                                            top_num_features=100,\n                                            modeltype=\"classification\",\n                                            verbose=2,\n                                            build_model=True)","7f578dee":"train_x5, test_x5, final5, predicted5= Auto_NLP(input_feature, train, test,target5,\n                                            score_type=\"balanced_accuracy\",\n                                            top_num_features=100,\n                                            modeltype=\"classification\",\n                                            verbose=2,\n                                            build_model=True)","9475d593":"#Data Loading","3f079140":"# AUTO NLP","6b18571b":"#ML Model Making","030fbace":"#Count Vectorizer + Dense Layer","e90a9dba":"## LSTM","adbe42de":"# CNN","7fc88723":"#Data Visualization","8eed2064":"We can see that most no of lengthly posts have between 1500-2500 words.\n\nThe line that you see represents the kernel density estimation","58af7572":"#Cycle 2","ca36b818":"#Preprocessing","e7bdf96c":"#Embedding + Dense Layer","9b25e529":"#Cycle","10d93ebb":"#Embedding+Dense+Max Pooling","0c860391":"Some of the most common occurences of the words in our document or data is - like, im, really, think, know, etc.","94e0aaca":"#Importing\/Installing necessary libraries"}}