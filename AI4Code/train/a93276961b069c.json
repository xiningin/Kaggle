{"cell_type":{"623ce082":"code","ef69fde3":"code","23acf272":"code","d3abf279":"code","f74bf2f7":"code","f2690c9d":"code","d2234d1b":"code","800f96d4":"code","8785fdde":"code","f26c3a58":"code","f1bebddc":"code","a67e633d":"markdown","73bc47f5":"markdown","2b90bb7a":"markdown","5ad600d6":"markdown","da67e00f":"markdown","bd73c0e2":"markdown"},"source":{"623ce082":"import numpy as np\nimport pandas as pd\npd.set_option('max_columns', None)\nimport re\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import confusion_matrix, classification_report","ef69fde3":"data = pd.read_csv('..\/input\/bakery-sales\/Bakery Sales.csv')","23acf272":"data","d3abf279":"data.info()","f74bf2f7":"def preprocess_inputs(df):\n    df = df.copy()\n    \n    # Drop place column\n    df = df.drop('place', axis=1)\n    \n    # Drop single-valued columns\n    df = df.drop(['croque monsieur', 'mad garlic'], axis=1)\n    \n    # Drop rows without sales or date information\n    missing_rows = df[df[['datetime', 'day of week', 'total']].isna().any(axis=1)].index\n    df = df.drop(missing_rows, axis=0).reset_index(drop=True)\n    \n    # Fill remaining missing values with 0\n    df = df.fillna(0)\n    \n    # Remove time information from datetime column\n    df['datetime'] = df['datetime'].apply(lambda x: re.sub(r' \\d+:\\d+$', '', x))\n    \n    # Save a copy of the datetime and day of week columns\n    day_mapping = df[['datetime', 'day of week']].copy()\n    \n    # Group by date\n    df = df.groupby(by='datetime', as_index=False).sum()\n    \n    # Recreate day of week values from day_mapping\n    df['day'] = df['datetime'].apply(lambda x: day_mapping[day_mapping['datetime'] == x].values[0][1])\n    \n    # Drop datetime column\n    df = df.drop('datetime', axis=1)\n    \n    # Change day column to be is_weekend\n    df['is_weekend'] = df['day'].apply(lambda x: 'Weekend' if x == 'Sat' or x == 'Sun' else 'Workday')\n    df = df.drop('day', axis=1)\n    \n    # Split df into X and y\n    y = df['is_weekend']\n    X = df.drop('is_weekend', axis=1)\n    \n    # Train-test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)\n    \n    # Scale X\n    scaler = StandardScaler()\n    scaler.fit(X_train)\n    X_train = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n    X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n    \n    return X_train, X_test, y_train, y_test","f2690c9d":"X_train, X_test, y_train, y_test = preprocess_inputs(data)","d2234d1b":"X_train","800f96d4":"y_train.value_counts()","8785fdde":"model = LogisticRegression()\nmodel.fit(X_train, y_train)","f26c3a58":"print(\"Test Accuracy: {:.2f}%\".format(model.score(X_test, y_test) * 100))","f1bebddc":"y_pred = model.predict(X_test)\n\ncm = confusion_matrix(y_test, y_pred, labels=['Workday', 'Weekend'])\nclr = classification_report(y_test, y_pred, labels=['Workday', 'Weekend'])\n\nplt.figure(figsize=(6, 6))\nsns.heatmap(cm, annot=True, fmt='g', cmap='Blues', cbar=False)\nplt.xticks(ticks=[0.5, 1.5], labels=['Workday', 'Weekend'])\nplt.yticks(ticks=[0.5, 1.5], labels=['Workday', 'Weekend'])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\nprint(\"Classification Report:\\n----------------------\\n\", clr)","a67e633d":"# Results","73bc47f5":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps:\/\/youtu.be\/fWSxa0sJfu4","2b90bb7a":"# Preprocessing","5ad600d6":"# Getting Started","da67e00f":"# Task for Today  \n\n***\n\n## Bakery Day of Sale Prediction  \n\nGiven *data about bakery sales*, let's try to predict whether a given day of sale is a **weekend** or not.\n\nWe will use a logistic regression model to make our predictions.","bd73c0e2":"# Training"}}