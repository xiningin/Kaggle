{"cell_type":{"aa7520a1":"code","cb4a22ba":"code","7a7911e8":"code","c0f777b5":"code","b15db931":"code","07720dc0":"code","eb22fede":"code","032e09bd":"code","1970228c":"code","f3307872":"code","4ae1b25a":"code","3b3efec9":"code","652c1fbe":"code","8a2464ed":"code","46770077":"code","8278d317":"code","fc348318":"code","9d9c681c":"code","fbab8a7e":"code","d5f910a5":"code","2aa39ef8":"code","8bd0839a":"code","efb3bdd2":"code","d25ade25":"code","2f60b951":"code","f7a1f2ef":"code","87dcb530":"code","88d5aceb":"code","19ffce9e":"code","fb1b026b":"code","d8304664":"code","a8aa5aa2":"code","e87cf27f":"code","d2d45711":"code","3b91ab94":"code","e11b1973":"code","baa20dec":"code","41ff407d":"code","e05f88e4":"code","f09505a8":"code","ead6bb22":"code","b77d791d":"code","cb0cccc8":"code","e50c05ff":"code","82543632":"code","e0d90c00":"code","f09da125":"code","31785df7":"code","66e83d5e":"code","db2d3141":"code","c4b750c9":"code","50cbe100":"markdown","caab586f":"markdown","55b859d1":"markdown","6a324fec":"markdown","813e4b8c":"markdown","1f94ae60":"markdown","28177201":"markdown","5d53504d":"markdown","3e778810":"markdown","040cb283":"markdown","88acd1c0":"markdown","945ddc77":"markdown","8236261d":"markdown","52f15ee3":"markdown","3fb99cb3":"markdown","ae29cdca":"markdown","a2a9eaa1":"markdown","13feb1c3":"markdown","2152fe88":"markdown","8ea5d591":"markdown","ec5d5fa1":"markdown","9c83b62e":"markdown"},"source":{"aa7520a1":"!nvidia-smi","cb4a22ba":"!pip install efficientnet -q","7a7911e8":"import os\nimport cv2\nimport glob\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm.notebook import tqdm\n\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nimport efficientnet.tfkeras as efn\n\nfrom sklearn.model_selection import GroupKFold, train_test_split","c0f777b5":"class Config:\n    IMAGES = '..\/input\/vinbigdata-chest-xray-resized-png-1024x1024\/train'\n    DATA = '..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train.csv'\n    \n    AUTOTUNE = tf.data.experimental.AUTOTUNE\n    \n    PRETRAINING_IMAGE_SIZE = 256 # I am using smaller image size for faster training\n    PRETRAINING_BATCH_SIZE = 8\n    PRETRAINING_NUM_CLASSES = 15\n    PRETRAINING_LR = 0.001","b15db931":"config = Config()","07720dc0":"df = pd.read_csv(config.DATA)\ndf.head(2)","eb22fede":"# SELECTING ONLY REQUIRED COLUMNS\ndf['image_path'] = df['image_id'].map(lambda x: f'{config.IMAGES}\/{x}.png')\ndf = df[['image_path', 'class_id']]\ndf.head(2)","032e09bd":"df.class_id.nunique()","1970228c":"df_train, df_valid = train_test_split(df, test_size=0.1, random_state=1234)","f3307872":"def aug_func(image_path, label):\n    file_bytes = tf.io.read_file(image_path)\n    img = tf.image.decode_png(file_bytes, channels=3)\n    img = tf.image.resize(img, [config.PRETRAINING_IMAGE_SIZE, config.PRETRAINING_IMAGE_SIZE])\n    img = img\/255.\n    return img, label","4ae1b25a":"train_dataset = tf.data.Dataset.from_tensor_slices((df_train['image_path'].values, df_train['class_id'].values))\ntrain_dataset = train_dataset.map(aug_func, num_parallel_calls=config.AUTOTUNE)\ntrain_dataset = train_dataset.repeat()\ntrain_dataset = train_dataset.batch(config.PRETRAINING_BATCH_SIZE)\ntrain_dataset = train_dataset.prefetch(config.AUTOTUNE)","3b3efec9":"valid_dataset = tf.data.Dataset.from_tensor_slices((df_valid['image_path'].values, df_valid['class_id'].values))\nvalid_dataset = valid_dataset.map(aug_func, num_parallel_calls=config.AUTOTUNE)\nvalid_dataset = valid_dataset.batch(config.PRETRAINING_BATCH_SIZE)\nvalid_dataset = valid_dataset.prefetch(config.AUTOTUNE)","652c1fbe":"for i, j in zip(train_dataset, valid_dataset):\n    print(i[0].shape, i[1].shape, j[0].shape, j[1].shape)\n    plt.figure(figsize=(20,10))\n    plt.subplot(1,2,1)\n    plt.imshow(i[0][0])\n    plt.subplot(1,2,2)\n    plt.imshow(j[0][0])\n    break","8a2464ed":"model = tf.keras.Sequential([\n    efn.EfficientNetB2(\n        input_shape=(config.PRETRAINING_IMAGE_SIZE, config.PRETRAINING_IMAGE_SIZE, 3),\n        weights='imagenet',\n        include_top=False),\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dense(config.PRETRAINING_NUM_CLASSES, activation='softmax')\n])","46770077":"# Training only the classifier layer for 1 epoch\nmodel.layers[0].trainable = False\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=config.PRETRAINING_LR),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n    metrics=tf.keras.metrics.SparseCategoricalAccuracy())\n\nmodel.summary()","8278d317":"STEPS = len(df_train) \/\/ config.PRETRAINING_BATCH_SIZE\n\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint('.\/efficientNet_Pretraining', save_best_only=True)\nlr_schedular = tf.keras.callbacks.ReduceLROnPlateau(patience=1, min_delta=0.01)\nearly_stopping = tf.keras.callbacks.EarlyStopping(min_delta=0.001, patience=1)","fc348318":"# Only last layer\n\nmodel.fit(x = train_dataset,\n         epochs = 1,\n         steps_per_epoch = STEPS,\n         validation_data = valid_dataset,\n         callbacks = [model_checkpoint, lr_schedular, early_stopping])","9d9c681c":"# Training whole model with a small learning rate for 1 epoch\n\nmodel.layers[0].trainable = True\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0001),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n    metrics=tf.keras.metrics.SparseCategoricalAccuracy())\n\nmodel.summary()","fbab8a7e":"# Whole model with hlaf steps so that weights don't distort much\n\nmodel.fit(x = train_dataset,\n         epochs = 3,\n         steps_per_epoch = STEPS,\n         validation_data = valid_dataset,\n         callbacks = [model_checkpoint, lr_schedular, early_stopping])","d5f910a5":"# Reloading the saved model and also saving the model in the output dir\nmodel = tf.keras.models.load_model('.\/efficientNet_Pretraining')","2aa39ef8":"# Cross checking the model performance\nmodel.evaluate(valid_dataset)","8bd0839a":"# Training whole model \n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0001),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n    metrics=tf.keras.metrics.SparseCategoricalAccuracy())","efb3bdd2":"# Whole model\n\nmodel.fit(x = train_dataset,\n         epochs = 3,\n         steps_per_epoch = STEPS,\n         validation_data = valid_dataset,\n         callbacks = [model_checkpoint, lr_schedular, early_stopping])","d25ade25":"class CurrentConfig:\n    DATA = '..\/input\/siim-covid19-detection\/train_study_level.csv'\n    IMAGE_FOLDER = '..\/input\/siimfisabiorsna-covid19-image-size-1024\/study'\n    \n    AUTOTUNE = tf.data.experimental.AUTOTUNE\n    \n    IMAGE_SIZE = 256 # I am using smaller image size for faster training\n    BATCH_SIZE = 8\n    NUM_CLASSES = 15\n    LR = 0.001","2f60b951":"current_config = CurrentConfig()","f7a1f2ef":"df = pd.read_csv(current_config.DATA)\n\nimage_files = glob.glob(current_config.IMAGE_FOLDER + '\/*')\ntemp_df = pd.DataFrame()\ntemp_df['paths'] = image_files\ntemp_df['id'] = temp_df['paths'].map(lambda x: x.split('\/')[-1].split('.')[0])","87dcb530":"df = pd.merge(df, temp_df, on='id')\ndf.head(2)","88d5aceb":"np.sum(df.isna())","19ffce9e":"for i in tqdm(df.paths):\n    if not os.path.exists(i):\n        print('image not found')\n        break","fb1b026b":"def mapper(row):\n    if row[0] == 1: return 1\n    if row[1] == 1: return 2\n    if row[2] == 1: return 3\n    if row[3] == 1: return 4\n\nlabels = []    \n\nfor row in df[df.columns[1:-1]].values:\n    labels.append(mapper(row))\n    \ndf['class'] = labels\ndf.columns = ['id', 'N', 'T', 'I', 'A', 'paths', 'class']\n\ndf.head(2)","d8304664":"df_train, df_temp = train_test_split(df, test_size=0.4, random_state=1234, stratify=df['class'])\n\ndf_valid, df_test = train_test_split(df_temp, test_size=0.5, random_state=1234, stratify=df_temp['class'])","a8aa5aa2":"# Clearing GPU memory\nfrom numba import cuda\ncuda.get_current_device().reset()","e87cf27f":"def aug_func(image_path, label):\n    file_bytes = tf.io.read_file(image_path)\n    img = tf.image.decode_png(file_bytes, channels=3)\n    img = tf.image.resize(img, [current_config.IMAGE_SIZE, current_config.IMAGE_SIZE])\n    img = img\/255.\n    return img, label","d2d45711":"train_dataset = tf.data.Dataset.from_tensor_slices((df_train['paths'].values, (df_train['N'].values, \n                                                                               df_train['T'].values,\n                                                                               df_train['I'].values,\n                                                                               df_train['A'].values)))\n\ntrain_dataset = train_dataset.map(aug_func, num_parallel_calls=current_config.AUTOTUNE)\ntrain_dataset = train_dataset.repeat()\ntrain_dataset = train_dataset.batch(current_config.BATCH_SIZE)\ntrain_dataset = train_dataset.prefetch(current_config.AUTOTUNE)","3b91ab94":"valid_dataset = tf.data.Dataset.from_tensor_slices((df_valid['paths'].values, (df_valid['N'].values, \n                                                                               df_valid['T'].values,\n                                                                               df_valid['I'].values,\n                                                                               df_valid['A'].values)))\n\nvalid_dataset = valid_dataset.map(aug_func, num_parallel_calls=current_config.AUTOTUNE)\nvalid_dataset = valid_dataset.batch(current_config.BATCH_SIZE)\nvalid_dataset = valid_dataset.prefetch(current_config.AUTOTUNE)","e11b1973":"test_dataset = tf.data.Dataset.from_tensor_slices((df_test['paths'].values, (df_test['N'].values, \n                                                                             df_test['T'].values,\n                                                                             df_test['I'].values,\n                                                                             df_test['A'].values)))\n\ntest_dataset = test_dataset.map(aug_func, num_parallel_calls=current_config.AUTOTUNE)\ntest_dataset = test_dataset.batch(current_config.BATCH_SIZE)\ntest_dataset = test_dataset.prefetch(current_config.AUTOTUNE)","baa20dec":"for i, j, k in zip(train_dataset, valid_dataset, test_dataset):\n    print(i[0].shape, i[1][0].shape, i[1][1].shape, i[1][2].shape, i[1][3].shape)\n    plt.figure(figsize=(20,10))\n    plt.subplot(1,3,1)\n    plt.imshow(i[0][0])\n    plt.subplot(1,3,2)\n    plt.imshow(j[0][0])\n    plt.subplot(1,3,3)\n    plt.imshow(k[0][0])\n    break","41ff407d":"base_model = model = tf.keras.Sequential([\n    efn.EfficientNetB2(\n        input_shape=(current_config.IMAGE_SIZE, current_config.IMAGE_SIZE, 3),\n        weights='imagenet',\n        include_top=False),\n    tf.keras.layers.GlobalAveragePooling2D()\n])","e05f88e4":"def get_model(base_model):\n    \n    inputs = tf.keras.layers.Input(shape=(current_config.IMAGE_SIZE, current_config.IMAGE_SIZE, 3))\n    \n    classifier_one   = tf.keras.layers.Dense(1, activation='softmax', name='out1')\n    classifier_two   = tf.keras.layers.Dense(1, activation='softmax', name='out2')\n    classifier_three = tf.keras.layers.Dense(1, activation='softmax', name='out3')    \n    classifier_four  = tf.keras.layers.Dense(1, activation='softmax', name='out4')    \n    \n    x = base_model(inputs)\n    \n    out1 = classifier_one(x)\n    out2 = classifier_two(x)\n    out3 = classifier_three(x)\n    out4 = classifier_four(x)\n    \n    return tf.keras.models.Model(inputs=[inputs], outputs=[out1, out2, out3, out4])","f09505a8":"model = get_model(base_model)\nmodel.layers[0].trainable = False\nmodel.summary()","ead6bb22":"tf.keras.utils.plot_model(model)","b77d791d":"model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=current_config.LR),\n              \n              loss=[tf.keras.losses.BinaryCrossentropy(), tf.keras.losses.BinaryCrossentropy(),\n                   tf.keras.losses.BinaryCrossentropy(), tf.keras.losses.BinaryCrossentropy()],\n             \n              metrics={'out1': tf.keras.metrics.AUC(name='auc_precision_recall', curve='PR'), \n                      'out2': tf.keras.metrics.AUC(name='auc_precision_recall', curve='PR'),\n                      'out3': tf.keras.metrics.AUC(name='auc_precision_recall', curve='PR'),\n                      'out4': tf.keras.metrics.AUC(name='auc_precision_recall', curve='PR')})","cb0cccc8":"model_checkpoint = tf.keras.callbacks.ModelCheckpoint('.\/efficientNet_before_pretraining', save_best_only=True)\nlr_schedular = tf.keras.callbacks.ReduceLROnPlateau(patience=1, min_delta=0.01)\nearly_stopping = tf.keras.callbacks.EarlyStopping(min_delta=0.001, patience=1)","e50c05ff":"# Training the last layers\n\nSTEPS = len(df_train)\/\/current_config.BATCH_SIZE\n\n# Before using Pretrained weights\nBP_history = model.fit(x = train_dataset, \n                  validation_data = valid_dataset,\n                  steps_per_epoch = STEPS,\n                  epochs = 2,\n                  callbacks=[model_checkpoint, lr_schedular, early_stopping]\n                 )","82543632":"base_model = tf.keras.models.load_model('.\/efficientNet_Pretraining')\nbase_model.summary()","e0d90c00":"# Removing the top layer\nbase_model.pop()\nbase_model.summary()","f09da125":"model = get_model(base_model)\nmodel.summary()","31785df7":"tf.keras.utils.plot_model(model)","66e83d5e":"model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=current_config.LR),\n              \n              loss=[tf.keras.losses.BinaryCrossentropy(), tf.keras.losses.BinaryCrossentropy(),\n                   tf.keras.losses.BinaryCrossentropy(), tf.keras.losses.BinaryCrossentropy()],\n             \n              metrics={'out1': tf.keras.metrics.AUC(name='auc_precision_recall', curve='PR'), \n                      'out2': tf.keras.metrics.AUC(name='auc_precision_recall', curve='PR'),\n                      'out3': tf.keras.metrics.AUC(name='auc_precision_recall', curve='PR'),\n                      'out4': tf.keras.metrics.AUC(name='auc_precision_recall', curve='PR')})","db2d3141":"model_checkpoint = tf.keras.callbacks.ModelCheckpoint('.\/efficientNet_after_pretraining', save_best_only=True)\nlr_schedular = tf.keras.callbacks.ReduceLROnPlateau(patience=1, min_delta=0.01)\nearly_stopping = tf.keras.callbacks.EarlyStopping(min_delta=0.001, patience=1)","c4b750c9":"# Training the last layers\n\nSTEPS = len(df_train)\/\/current_config.BATCH_SIZE\n\nAP_history = model.fit(x = train_dataset, \n                  validation_data = valid_dataset,\n                  steps_per_epoch = STEPS,\n                  epochs = 2,\n                  callbacks=[model_checkpoint, lr_schedular, early_stopping]\n                 )","50cbe100":"# Conclusion (Work in Progress)\nFor now as you can see I am getting this error (if anyone knows why this error is coming then pls do tell in the comments) but one more thing to note is that even in the first epochs there is considerable difference. Now you have to decide weather you want to pretrain your models on similar dataset or not. In my opinion the difference will only increase with bigger models with larger Image size.","caab586f":"# Training the model","55b859d1":"# Kindly Upvote \ud83d\ude0a","6a324fec":"# Loading the dataset","813e4b8c":"# Loading the pretraining data","1f94ae60":"# Creating the model","28177201":"# Creating the model with the pretrained base_model","5d53504d":"___________________________________________________","3e778810":"This much pre-training is enough so let's use this model as feature extractor for our current competition data","040cb283":"-----------------------------------------------","88acd1c0":"# Training the Model\nI will not finetune the models and their hyperparameter as that will be very tidious and the notebook is already long enough. My main objective to see which pretrained weights models performs better.","945ddc77":"# Analysing Effects of Pretraining On Similar Dataset\n\nIn this notebook I will try to do the following:\n* Pretrain the model on a previous competiton similar dataset\n* Finetune the pretrained model on the current competition dataset\n* Generate pseudo labels for the previous dataset from fine tuned dataset\n* Retrain the model on the generated pseudo labels and observe the model performance on the current competition dataset.","8236261d":"# Creating the dataset\n\nFor some reason my session crashed so I have restarted from this point.","52f15ee3":"# Clearing GPU Memory","3fb99cb3":"# Starting work on current Dataset","ae29cdca":"# Configuration Current","a2a9eaa1":"# Creating the pretraining dataset","13feb1c3":"# Pretraining the model","2152fe88":"# Coniguration\n","8ea5d591":"# How would a not-so pretrained model perform","ec5d5fa1":"I tried with a little larger learning rate but it didn't seem to be working well so I moved back to the smaller one.","9c83b62e":"# Spliting the dataset\nFor our purpose we will split the dataset in 3 parts. train, valid and test. I will not be doing k-fold cross validation as it takes too much time."}}