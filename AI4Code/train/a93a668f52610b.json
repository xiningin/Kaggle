{"cell_type":{"80bff735":"code","78fdaf62":"code","e7fb93cf":"code","3e72ea9b":"code","722ec604":"code","b9bd6550":"code","f0a7cf94":"code","cc34507e":"code","755cd3ce":"code","eaa86114":"code","37ab9849":"code","b577794c":"code","014da671":"code","6d3da571":"code","445ba4b2":"code","d727f68c":"code","1433533e":"code","1bc6ff9d":"code","42c1d06c":"code","588197d3":"code","166a4ce7":"code","4c66c8f4":"code","1bc6ec61":"code","4c9321ad":"code","bd0db739":"code","4a43fdac":"code","17ee2d76":"code","08fa8589":"code","07179cdc":"markdown","511e893f":"markdown","9f428c7c":"markdown","85cc57ee":"markdown","565f86cc":"markdown","d3100134":"markdown","c816c7ea":"markdown","bcfde97f":"markdown","0c75ccc0":"markdown","8f4083ff":"markdown","c1cfd22b":"markdown","6fe4061b":"markdown","e2757a63":"markdown","98c738ae":"markdown","e26becc4":"markdown","e976ae99":"markdown","3050885a":"markdown","35f90eb1":"markdown","c9ffa632":"markdown","9abd7355":"markdown","e78eb97d":"markdown","e655366d":"markdown"},"source":{"80bff735":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","78fdaf62":"import pandas as pd\nimport keras\nfrom keras.datasets import cifar10\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nimport os\n\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport itertools\n\n%matplotlib inline","e7fb93cf":"sample = pd.read_csv('\/kaggle\/input\/cifar-10\/sampleSubmission.csv')\nsample.head()","3e72ea9b":"trainLabel = pd.read_csv('\/kaggle\/input\/cifar-10\/trainLabels.csv')\ntrainLabel","722ec604":"!pip install pylzma\n!pip install py7zlib","b9bd6550":"import py7zlib\nimport time\n\nfp = open(\"\/kaggle\/input\/cifar-10\/train.7z\",'rb')\n#\u751f\u6210\u4e00\u4e2aarchive\u5bf9\u8c61\narchive = py7zlib.Archive7z(fp)\n\n#\u8bfb\u53d6\u6587\u4ef6\u4e2d\u6240\u6709\u7684\u6587\u4ef6\u540d\nnames = archive.getnames()\n#search\nstartTime = time.time()\n\n#\u6839\u636e\u6587\u4ef6\u540d\u8fd4\u56de\u6587\u4ef6\u7684archiveFile\u7c7b\nmember = archive.getmember(names[0])\nend_1_time = time.time()\nprint(\"search time is {}\".format(end_1_time-startTime))\n\n#read data\n#\u8bfb\u53d6\u6587\u4ef6\u7684\u6240\u6709\u6570\u636e\ndata = member.read()\nend_2_time = time.time()\nprint(\"read time is {}\".format(end_2_time-end_1_time))","f0a7cf94":"names","cc34507e":"!pip install py7zr\n!python -m py7zr x ..\/input\/cifar-10\/train.7z \/kaggle\/working\/\n!python -m py7zr x ..\/input\/cifar-10\/test.7z \/kaggle\/working\/\n","755cd3ce":"(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nprint('x_train shape:', x_train.shape)\nprint('y_train shape:', y_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')","eaa86114":"y_train","37ab9849":"fig, axs = plt.subplots(1,2,figsize=(15,5))\nsns.countplot(y_train.ravel(),ax=axs[0])\naxs[0].set_title('Training data')\naxs[0].set_xlabel('Classes')\n\nsns.countplot(y_test.ravel(),ax=axs[1])\naxs[1].set_title('Testing data')\naxs[1].set_xlabel('Classes')\nplt.show()","b577794c":"x_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train \/= 255\nx_test \/= 255","014da671":"x_train","6d3da571":"num_classes = 10\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)","445ba4b2":"x_train.shape[:]","d727f68c":"model =Sequential()\nmodel.add(Conv2D(32,(3,3),padding='same',input_shape=x_train.shape[1:]))\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(32,(3,3)))\nmodel.add(MaxPooling2D(3,strides=2))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64,(3,3),padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(64,(3,3)))\nmodel.add(MaxPooling2D(3,strides=2))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(128,(3,3),padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(128,(3,3)))\nmodel.add(MaxPooling2D(3,strides=2))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes))\n\nmodel.add(Activation('softmax'))\n\nmodel.summary()\n\n","1433533e":"opt = keras.optimizers.Adam(learning_rate=0.001,decay=1e-6,epsilon=1e-08,beta_1=0.9,beta_2=0.999)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy'])","1bc6ff9d":"history = None\nprint('Not using data argumentation.')\nhistory = model.fit(x_train,y_train,\n                   batch_size=128,\n                   epochs=5,\n                   validation_data=(x_test,y_test),\n                   shuffle=True)","42c1d06c":"def eva(history):\n    fig, axs = plt.subplots(1,2,figsize=(15,5))\n    axs[0].plot(history.history['accuracy'])\n    axs[0].plot(history.history['val_accuracy'])\n    axs[0].set_title('Model Accuracy')\n    axs[0].set_ylabel('Accuracy')\n    axs[0].set_xlabel('Epoch')\n    axs[0].legend(['train','validate'], loc='upper left')\n    \n    axs[1].plot(history.history['loss'])\n    axs[1].plot(history.history['val_loss'])\n    axs[1].set_title('Model Loss')\n    axs[1].set_ylabel('Loss')\n    axs[1].set_xlabel('Epoch')\n    axs[1].legend(['train','validate'], loc='upper left')\n    plt.show()\n    \nprint(history.history.keys())\neva(history)","588197d3":"scores = model.evaluate(x_test,y_test)\nprint('Test loss:',scores[0])\nprint('Test accuracy:',scores[1])\n\npred = model.predict(x_test)","166a4ce7":"labels = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']","4c66c8f4":"def show_test(number):\n    fig = plt.figure(figsize = (3,3))\n    test_image = np.expand_dims(x_test[number], axis=0)\n    test_result = model.predict_classes(test_image)\n    plt.imshow(x_test[number])\n    dict_key = test_result[0]\n    plt.title(\"Predicted: {} \".format(labels[dict_key]))","1bc6ec61":"show_test(10)","4c9321ad":"path = os.path.join(os.getcwd(),'save_models')\nmodel_name = 'keras_cifar10_trained_model.h5'\nif not os.path.isdir(path):\n    os.mkdir(path)\n\nmodel_path = os.path.join(path,model_name)\nmodel.save(model_path)\nprint('Saved trained model at %s ' % model_path)\n\nscores = model.evaluate(x_test,y_test,verbose=1)\nprint('Test loss:',scores[0])\nprint('Test accuracy:', scores[1])","bd0db739":"!ls ..\/working","4a43fdac":"train_images_path = \"\/kaggle\/working\/train\"\ntest_images_path = \"\/kaggle\/working\/test\"","17ee2d76":"!ls \/kaggle\/working","08fa8589":"test_images_path","07179cdc":"The CIFAR-10 dataset contains 60,000 color images of 32 x 32 pixels in 3 channels divided into 10 classes. Each class contains 6,000 images. The training set contains 50,000 images, while the test sets provides 10,000 images. This image taken from the CIFAR repository ( https:\/\/www.cs.toronto.edu\/~kriz\/cifar.html ). This is a classification problem with 10 classes(muti-label classification). We can take a view on this image for more comprehension of the dataset.","511e893f":"# **9\u3001Check the predictions**","9f428c7c":"The above method is not very easy to control","85cc57ee":"As we can see, each classe contain exacly 6000 examples( 5000 for training and 1000 for test).\n\nThe graph above is very important for the training, for example if we had just 1000 samples of label 1 that will be a problem , the model will find difficulties to detect label 1\"less accuracy \", so that's not going to happend everything look fine. It's important to know the distribution of dataset behind different classes because the goodness of our model depend on it.\n\nNow let's doing some preprocessing.\n\nThe output variable have 10 posible values. This is a multiclass classification problem. We need to encode these lables to one hot vectors (ex : \"bird\" -> [0,0,1,0,0,0,0,0,0,0]).","565f86cc":"# **2\u3001Replacement method**","d3100134":"# **1\u3001 7z file**","c816c7ea":"In the next stage in the deep pipeline, Our net will learn 64 convolutional filters, each of which with a 3 x 3 size. The output dimension is the same one of the input shape and activation is relu; folowed by another 64 convolutional filters, each of which with a 3 x 3 size and activation is also relu. After that we have a max-pooling operation with pool size 2 x 2 and a dropout at 25%.\n","bcfde97f":"# **7\u3001Score trained model and prediction**","0c75ccc0":"# **10\u3001Save**","8f4083ff":"# **Just a simple example**","c1cfd22b":"Confusion matrix can be very helpfull to see your model drawbacks. We plot the confusion matrix of the validation results. For good vizualization of our confusion matrix, we have to define to fonction","6fe4061b":"# **8\u3001Confusion matrix**","e2757a63":"And the Final stage in the deep pipeline is a dense network with 512 units and relu activation followed by a dropout at 50% and by a softmax layer with 10 classes as output, one for each category.","98c738ae":"**2.1 Introduction**","e26becc4":"# **5\u3001Model training**","e976ae99":"# **6\u3001Evaluate the model**","3050885a":"The challenge is to recognize previously unseen images and assign them to one of the 10 classes.","35f90eb1":"Before making network ready for training we have to make sure to add below things:\n\nA loss function: to measure how good the network is\nAn optimizer: to update network as it sees more data and reduce loss value\nMetrics: to monitor performance of network\nAlso note that for data augmentation:\n\nOne of the most commun tehnique to avoid overfitting is data augmentation. And We know that overfitting is generaly occur when we don't have enought data for training the model. To avoid this overfitting problem, we need to expand artificially our dataset. The idea is to alter the training data with small transformations to reproduce the variations occuring when someone is writing a digit.\n\nDifferent data aumentation techniques are as follows: Cropping, Rotating, Scaling, Translating, Flipping, Adding Gaussian noise to input images, etc...","c9ffa632":"6.1 Training and validation curves.\u00b6\nLet's see the training and validation process by the visualization of history of fitting. This allow us to quickly know if how our model fit our data (overfitting, underfitting, model convergence, etc...)","9abd7355":"# **3\u3001Normalize**","e78eb97d":"In the first stage, Our net will learn 32 convolutional filters, each of which with a 3 x 3 size. The output dimension is the same one of the input shape, so it will be 32 x 32 and activation is relu, which is a simple way of introducing non-linearity; folowed by another 32 convolutional filters, each of which with a 3 x 3 size and activation is also relu. After that we have a max-pooling operation with pool size 2 x 2 and a dropout at 25%.\n","e655366d":"# **4\u3001Defining the model architecture Using ConVnets**"}}