{"cell_type":{"721e71f5":"code","92e88f02":"code","fdb055ad":"code","0771d7a3":"code","fbc7bd70":"code","6c24cb92":"code","e2282f02":"code","420bc469":"code","d6d0d8ae":"code","57233a7b":"code","28234981":"code","c57f075b":"code","6a0f1948":"code","2f74b8dd":"code","e608080a":"code","9d072741":"code","ad055184":"code","f53a3928":"code","b933d707":"code","c8002be7":"code","d23080f4":"code","fa1978d8":"code","4729b84b":"code","a48c1ff1":"code","aefbd6b0":"markdown","b8939ca0":"markdown"},"source":{"721e71f5":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version 20210331 --apt-packages libomp5 libopenblas-dev\n!rm -rf \/kaggle\/working\/*.whl\n!rm -rf \/kaggle\/working\/*.py","92e88f02":"import os\n\nos.environ['XLA_USE_BF16'] = \"1\"\nos.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '100000000'","fdb055ad":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport torch\nfrom torch import nn, optim\nfrom  torch.utils.data import Dataset, DataLoader\n\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\n\nfrom matplotlib import pyplot as plt\nimport os, random, gc\nimport re, time, json, pickle\n\nfrom sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom tqdm.notebook import tqdm\nfrom joblib import Parallel, delayed","0771d7a3":"from transformers import AutoTokenizer, AutoConfig, AutoModelForTokenClassification, AutoModelForSequenceClassification","fbc7bd70":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything()","6c24cb92":"MAX_LENGTH = 300\nNUM_TARGETS = 1\n\nSEED = 321\n\nMODEL_NAME = \"roberta-base\"\n\nMODEL_ROOT = Path(\".\")","e2282f02":"TRAIN_BATCH_SIZE = 16\nTRAIN_NUM_WORKERS = 2\n\nVAL_BATCH_SIZE = 20\nVAL_NUM_WORKERS = 2","420bc469":"seed_everything(SEED)\n\n\ndf = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\n\ndf[\"fold\"] = -1\nN_SPLITS = 5\nkf = KFold(n_splits=N_SPLITS, random_state=SEED, shuffle=True)\n\nfor fold, (_, val_set) in enumerate(kf.split(np.arange(len(df)))):\n    df.loc[val_set, \"fold\"] = fold\n\nprint(df.shape)\ndf.head()","d6d0d8ae":"df.fold.value_counts()","57233a7b":"def gen_data(model_name=MODEL_NAME):\n    X_input_ids = []\n    X_masks = []\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    with open(MODEL_ROOT\/f\"{MODEL_NAME}-tokenizer.pkl\", \"wb\") as f:\n        pickle.dump(tokenizer, f)\n\n    for excerpt in tqdm(df.excerpt):\n        inp = tokenizer(excerpt, add_special_tokens=True, return_tensors=\"pt\",\n                        max_length=MAX_LENGTH, padding=\"max_length\", truncation=True)\n        X_input_ids.append(inp[\"input_ids\"])\n        X_masks.append(inp[\"attention_mask\"])\n\n    X_input_ids = torch.cat(X_input_ids)\n    X_masks = torch.cat(X_masks)\n    Y = torch.tensor(df.target.values, dtype=torch.float32)\n    \n    print(X_input_ids.shape, X_masks.shape, Y.shape)\n    \n    return X_input_ids, X_masks, Y","28234981":"X_input_ids, X_masks, Y = gen_data()","c57f075b":"class CRPDataset(Dataset):\n    def __init__(self, X_input_ids, X_masks, Y, is_train=True):\n        assert X_input_ids.shape == X_masks.shape\n        \n        self.X_input_ids = X_input_ids\n        self.X_masks = X_masks\n        self.Y = Y\n        \n    def __len__(self):\n        return len(self.X_input_ids)\n    \n    def __getitem__(self, idx):\n        return (self.X_input_ids[idx], self.X_masks[idx]), self.Y[[idx]] ","6a0f1948":"ds = CRPDataset(X_input_ids, X_masks, Y)\nlen(ds)","2f74b8dd":"(x, x_mask), y = ds[0]\nx.shape, x_mask.shape, y","e608080a":"def get_model(model_name, task=\"token_classification\", num_targets=NUM_TARGETS):\n    task = task.lower()\n        \n    if \"token\" in task:\n        model_instance = AutoModelForTokenClassification\n    elif \"sequence\" in task:\n        model_instance = AutoModelForSequenceClassification\n        \n    model = model_instance.from_pretrained(model_name)\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    config = AutoConfig.from_pretrained(model_name)\n    \n    if hasattr(model, \"classifier\"):\n        model.classifier = nn.Linear(model.classifier.in_features, NUM_TARGETS)\n        \n    return config,tokenizer, model","9d072741":"class AttentionBlock(nn.Module):\n    def __init__(self, in_features, middle_features, out_features):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = middle_features\n        self.out_features = out_features\n        self.W = nn.Linear(in_features, middle_features)\n        self.V = nn.Linear(middle_features, out_features)\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n        return context_vector","ad055184":"class CRPTokenModel(nn.Module):\n    def __init__(self, model_name=MODEL_NAME, num_targets=NUM_TARGETS, alpha=0.5, p=0.5):\n        super().__init__()\n        self.model_name = model_name\n        self.num_targets = num_targets\n        self.alpha = alpha\n        self.p = p\n        \n        config,tokenizer, model = get_model(model_name, task=\"token_classification\", num_targets=1)\n        \n        self.in_features =  model.classifier.in_features\n        model.classifier = nn.Identity()\n        \n        self.config = config\n        self.tokenizer = tokenizer\n        self.model = model\n        \n        self.att = AttentionBlock(self.in_features, self.in_features, 1)\n        self.fc = nn.Linear(self.in_features, self.num_targets)\n        \n    def forward(self, *args, **kwargs):\n        x = self.model(*args, **kwargs)[\"logits\"]\n        x = self.att(x)\n        x = self.fc(x)\n        return x","f53a3928":"def one_step(xb, yb, net, criterion, optimizer, device, scheduler=None):\n    xb, yb = (xb[0].to(device), xb[1].to(device)), yb.to(device)\n        \n    net.zero_grad()\n    o = net(input_ids=xb[0], attention_mask=xb[1])\n    loss = criterion(o, yb)\n    loss.backward()\n    xm.optimizer_step(optimizer, barrier=True)\n  \n    with torch.no_grad():\n        l = loss.item()\n        r2 = r2_score(yb.cpu().numpy(), o.cpu().numpy())\n\n        rmse = torch.sqrt(torch.mean(torch.square(o - yb))).item()\n        mad = torch.mean(torch.abs(o - yb)).item()\n\n    if scheduler is not None:\n        scheduler.step()\n\n    return l, rmse, mad, r2","b933d707":"@torch.no_grad()\ndef evaluate(net, criterion, val_laoder, device):\n    net.eval()\n\n    os, y = [], []\n    val_laoder = tqdm(val_laoder, leave = False, total=len(val_laoder))\n\n    for icount, (xb, yb) in  enumerate(val_laoder):\n\n        y.append(yb.to(device))\n\n        xb = (xb[0].to(device), xb[1].to(device))\n        o = net(input_ids=xb[0], attention_mask=xb[1])\n\n        os.append(o)\n\n    y = torch.cat(y)\n    o = torch.cat(os)\n\n    l = criterion(o, y).item()\n    \n    r2 = r2_score(y.cpu().numpy(), o.cpu().numpy())\n\n    rmse = torch.sqrt(torch.mean(torch.square(o - y))).item()\n    mad = torch.mean(torch.abs(o - y)).item()\n\n    return l, rmse, mad, r2","c8002be7":"def one_epoch(net, criterion, optimizer, scheduler, train_laoder, val_laoder, device):\n    net.train()\n    l, rmse, mad, r2, icount = 0.,0.,0.,0., 0\n    train_laoder = tqdm(train_laoder, leave = False)\n    epoch_bar = train_laoder\n  \n    for (xb, yb) in  epoch_bar:\n        _l, _rmse, _mad, _r2 = one_step(xb, yb, net, criterion, optimizer, device)\n        l += _l\n        rmse += _rmse\n        mad += _mad\n        r2 += _r2\n\n        icount += 1\n        \n        if hasattr(epoch_bar, \"set_postfix\") and not icount%10:\n            epoch_bar.set_postfix(\n                loss=\"{:.3f}\".format(l\/icount),\n                rmse=\"{:.3f}\".format(rmse\/icount),\n                mad=\"{:.3f}\".format(mad\/icount),\n                r2=\"{:.3f}\".format(r2\/icount),\n            )\n  \n    scheduler.step()\n\n    l \/= icount\n    rmse \/= icount\n    mad \/= icount\n    r2 \/= icount\n\n    l_val, rmse_val, mad_val, r2_val = evaluate(net, criterion, val_laoder, device)\n\n    return (l, l_val), (rmse, rmse_val), (mad, mad_val), (r2, r2_val)","d23080f4":"class AutoSave:\n    def __init__(self, top_k=2, metric=\"f1\", mode=\"min\", root=None, name=\"ckpt\"):\n        self.top_k = top_k\n        self.logs = []\n        self.metric = metric\n        self.mode = mode\n        self.root = Path(root or MODEL_ROOT)\n        assert self.root.exists()\n        self.name = name\n\n        self.top_models = []\n        self.top_metrics = []\n\n    def log(self, model, metrics):\n        metric = metrics[self.metric]\n        rank = self.rank(metric)\n\n        self.top_metrics.insert(rank+1, metric)\n        if len(self.top_metrics) > self.top_k:\n            self.top_metrics.pop(0)\n\n        self.logs.append(metrics)\n        self.save(model, metric, rank, metrics[\"epoch\"])\n\n    def save(self, model, metric, rank, epoch):\n        t = time.strftime(\"%Y%m%d%H%M%S\")\n        name = \"{}_epoch_{:02d}_{}_{:.04f}_{}\".format(self.name, epoch, self.metric, metric, t)\n        name = re.sub(r\"[^\\w_\\-\\.]\", \"\", name) + \".pth\"\n        path = self.root.joinpath(name)\n\n        old_model = None\n        self.top_models.insert(rank+1, name)\n        if len(self.top_models) > self.top_k:\n            old_model = self.root.joinpath(self.top_models[0])\n            self.top_models.pop(0)      \n        \n        xm.save(model.state_dict(), path.as_posix())\n\n        if old_model is not None:\n            old_model.unlink()\n\n        self.to_json()\n\n    def rank(self, val):\n        r = -1\n        for top_val in self.top_metrics:\n            if val <= top_val:\n                return r\n            r += 1\n        return r\n  \n    def to_json(self):\n        # t = time.strftime(\"%Y%m%d%H%M%S\")\n        name = \"{}_logs\".format(self.name)\n        name = re.sub(r\"[^\\w_\\-\\.]\", \"\", name) + \".json\"\n        path = self.root.joinpath(name)\n\n        with path.open(\"w\") as f:\n            json.dump(self.logs, f, indent=2)","fa1978d8":"def one_fold(model_name, fold, train_set, val_set, epochs=20, save=True, save_root=None):\n    device = xm.xla_device(fold + 1)\n    save_root = Path(save_root) or MODEL_ROOT\n\n    saver = AutoSave(root=save_root, name=f\"crp_{model_name}_fold{fold}\", metric=\"rmse_val\")\n   \n    net = CRPTokenModel(model_name)\n    net = net.to(device)\n    \n    with open(MODEL_ROOT\/f\"{model_name}-config.pkl\", \"wb\") as f:\n        pickle.dump(net.config, f)\n    \n    with open(MODEL_ROOT\/f\"{model_name}-tokenizer.pkl\", \"wb\") as f:\n        pickle.dump(net.tokenizer, f)\n\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(net.parameters(), lr=5e-5)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=1e-5, T_max=epochs)\n\n    train_data = CRPDataset(X_input_ids=X_input_ids[train_set] , X_masks=X_masks[train_set], Y=Y[train_set], is_train=True)\n    train_laoder = DataLoader(train_data, batch_size=TRAIN_BATCH_SIZE, num_workers=TRAIN_NUM_WORKERS, shuffle=True, pin_memory=True)\n\n    val_data = CRPDataset(X_input_ids=X_input_ids[val_set] , X_masks=X_masks[val_set], Y=Y[val_set], is_train=False)\n    val_laoder = DataLoader(val_data, batch_size=VAL_BATCH_SIZE, num_workers=VAL_NUM_WORKERS, shuffle=False)\n\n    epochs_bar = tqdm(list(range(epochs)), leave=False)\n\n    for epoch  in epochs_bar:\n        epochs_bar.set_description(f\"--> [EPOCH {epoch:02d}]\")\n        net.train()\n\n        (l, l_val), (rmse, rmse_val), (mad, mad_val), (r2, r2_val) = one_epoch(\n            net=net,\n            criterion=criterion,\n            optimizer=optimizer,\n            scheduler=scheduler,\n            train_laoder=train_laoder,\n            val_laoder=val_laoder, \n            device=device\n          )\n\n        epochs_bar.set_postfix(\n        loss=\"({:.3f}, {:.3f})\".format(l, l_val),\n        rmse=\"({:.3f}, {:.3f})\".format(rmse, rmse_val),\n        mad=\"({:.3f}, {:.3f})\".format(mad, mad_val),\n        r2=\"({:.3f}, {:.3f})\".format(r2, r2_val),\n        )\n\n        print(\n            \"[{epoch:02d}] loss: {loss} rmse: {rmse} mad: {mad} r2: {r2}\".format(\n                epoch=epoch,\n                loss=\"({:.3f}, {:.3f})\".format(l, l_val),\n                rmse=\"({:.3f}, {:.3f})\".format(rmse, rmse_val),\n                mad=\"({:.3f}, {:.3f})\".format(mad, mad_val),\n                r2=\"({:.3f}, {:.3f})\".format(r2, r2_val),\n            )\n        )\n\n        if save:\n            metrics = {\n                \"epoch\": epoch,\n                \"loss\": l, \"rmse\": -rmse, \"mad\": mad, \"r2\": r2,\n                \"loss_val\": l_val, \"rmse_val\": -rmse_val, \"mad_val\": mad_val, \"r2_val\": r2_val,\n            }\n\n            saver.log(net, metrics)","4729b84b":"def train(model_name, epochs=20, save=True, n_splits=5, seed=SEED, save_root=None, suffix=\"\", folds=None):\n    gc.collect()\n    torch.cuda.empty_cache()\n    save_root = save_root or MODEL_ROOT\/f\"{model_name}{suffix}\"\n    save_root.mkdir(exist_ok=True, parents=True)\n    seed_everything(seed)\n    fold_bar = tqdm(df.reset_index(drop=True).reset_index().groupby(\"fold\").index.apply(list).items(), total=df.fold.max()+1)\n    def run(fold, val_set, model_name, epochs, save, save_root):\n        print(f\"\\n############################### [FOLD {fold}  SEED {seed}]\")\n        fold_bar.set_description(f\"[FOLD {fold}  SEED {seed}]\")\n        train_set = np.setdiff1d(df.index, val_set)\n        one_fold(model_name, fold=fold, train_set=train_set , val_set=val_set , epochs=epochs, save=save, save_root=save_root)\n        gc.collect()\n        torch.cuda.empty_cache()\n        \n    Parallel(n_jobs=N_SPLITS, backend=\"threading\")(delayed(run)(fold, val_set, model_name, epochs, save, save_root) for fold, val_set in fold_bar)","a48c1ff1":"for seed in [666]:\n    train(MODEL_NAME, epochs=2, suffix=f\"_maxlen{MAX_LENGTH}_seed{seed}\", folds=None, seed=seed)","aefbd6b0":"# Training the model","b8939ca0":"# Roberta Model Fold Parallel Training on TPU\n\nIn this notebook, I add some trivil modifications to show how to use TPU for parallel fold training of Roberta Model, inspired by the following notebooks:\n\n[CLR Prize Roberta Model [Training]][1]\n\n[Super-duper fast pytorch tpu kernel... \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25][2]\n\n[1]: https:\/\/www.kaggle.com\/kneroma\/clr-prize-roberta-model-training\n[2]: https:\/\/www.kaggle.com\/abhishek\/super-duper-fast-pytorch-tpu-kernel"}}