{"cell_type":{"737a9bcb":"code","55c618d0":"code","97324b03":"code","d95b47e7":"code","5f0fa1c8":"code","2527be02":"code","100e61ea":"code","e79c02d9":"code","560eec19":"code","b7168498":"code","1a216de3":"code","a863699e":"code","26d03d7c":"code","c2a7061a":"code","3e2a642b":"markdown","45bfeb8a":"markdown","4133a025":"markdown","a3e9c2aa":"markdown","5bbcdbd6":"markdown","57287f7f":"markdown","7fd680df":"markdown","5bff65a9":"markdown","3eeaf4a7":"markdown","89815589":"markdown","ccd1748a":"markdown","ef2bed30":"markdown"},"source":{"737a9bcb":"import os\nimport re\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport keras.backend as K\nfrom keras.preprocessing import text, sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints\nfrom keras.layers import *\nfrom keras.models import Model\n\n\n# For reproducibility.\nseed = 999\nnp.random.seed(seed)\ntf.set_random_seed(seed)\nsession_conf = tf.ConfigProto(\n    intra_op_parallelism_threads=1,\n    inter_op_parallelism_threads=1\n)\nsess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\nK.set_session(sess)","55c618d0":"DATA_DIR = '..\/input\/movie-review-sentiment-analysis-kernels-only'\ntrain_file = os.path.join(DATA_DIR, 'train.tsv')\ntest_file  = os.path.join(DATA_DIR, 'test.tsv')\ndf_train = pd.read_table(train_file)\ndf_test  = pd.read_table(test_file)","97324b03":"EMBEDDING_FILE =  '..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec'\n\ndef load_embeddings(filename):\n    embeddings = {}\n    with open(filename) as f:\n        for line in f:\n            values = line.rstrip().split(' ')\n            word = values[0]\n            vector = np.asarray(values[1:], dtype='float32')\n            embeddings[word] = vector\n    return embeddings\n\nembeddings = load_embeddings(EMBEDDING_FILE)","d95b47e7":"df_train.Phrase = df_train.Phrase.str.replace(\"n't\", 'not')\ndf_test.Phrase = df_test.Phrase.str.replace(\"n't\", 'not')","5f0fa1c8":"df_train[['Phrase']]","2527be02":"df_train.Phrase = df_train.Phrase.apply(lambda x: re.sub(r'[0-9]+', '0', x))\ndf_test.Phrase = df_test.Phrase.apply(lambda x: re.sub(r'[0-9]+', '0', x))\n\nx_train = df_train['Phrase'].values\nx_test  = df_test['Phrase'].values\ny_train = df_train['Sentiment'].values\nx = np.r_[x_train, x_test]","100e61ea":"tokenizer = Tokenizer(lower=True, filters='\\n\\t')\ntokenizer.fit_on_texts(x)\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test  = tokenizer.texts_to_sequences(x_test)\nvocab_size = len(tokenizer.word_index) + 1  # +1 is for zero padding.\nprint('vocabulary size: {}'.format(vocab_size))","e79c02d9":"maxlen = len(max((s for s in np.r_[x_train, x_test]), key=len))\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen, padding='post')\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen, padding='post')\nprint('maxlen: {}'.format(maxlen))\nprint(x_train.shape)\nprint(x_test.shape)","560eec19":"def filter_embeddings(embeddings, word_index, vocab_size, dim=300):\n    embedding_matrix = np.zeros([vocab_size, dim])\n    for word, i in word_index.items():\n        if i >= vocab_size:\n            continue\n        vector = embeddings.get(word)\n        if vector is not None:\n            embedding_matrix[i] = vector\n    return embedding_matrix\n\nembedding_size = 300\nembedding_matrix = filter_embeddings(embeddings, tokenizer.word_index,\n                                     vocab_size, embedding_size)\nprint('OOV: {}'.format(len(set(tokenizer.word_index) - set(embeddings))))","b7168498":"class Attention(Layer):\n    \"\"\"\n    Keras Layer that implements an Attention mechanism for temporal data.\n    Supports Masking.\n    Follows the work of Raffel et al. [https:\/\/arxiv.org\/abs\/1512.08756]\n    # Input shape\n        3D tensor with shape: `(samples, steps, features)`.\n    # Output shape\n        2D tensor with shape: `(samples, features)`.\n    :param kwargs:\n    Just put it on top of an RNN Layer (GRU\/LSTM\/SimpleRNN) with return_sequences=True.\n    The dimensions are inferred based on the output shape of the RNN.\n    Example:\n        model.add(LSTM(64, return_sequences=True))\n        model.add(Attention())\n    \"\"\"\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n        if self.bias:\n            eij += self.b\n        eij = K.tanh(eij)\n        a = K.exp(eij)\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","1a216de3":"def build_model(maxlen, vocab_size, embedding_size, embedding_matrix):\n    input_words = Input((maxlen, ))\n    x_words = Embedding(vocab_size,\n                        embedding_size,\n                        weights=[embedding_matrix],\n                        mask_zero=True,\n                        trainable=False)(input_words)\n    x_words = SpatialDropout1D(0.3)(x_words)\n    x_words = Bidirectional(LSTM(50, return_sequences=True))(x_words)\n    x = Attention(maxlen)(x_words)\n    x = Dropout(0.2)(x)\n    x = Dense(50, activation='relu')(x)\n    x = Dropout(0.2)(x)\n    pred = Dense(5, activation='softmax')(x)\n\n    model = Model(inputs=input_words, outputs=pred)\n    return model\n\nmodel = build_model(maxlen, vocab_size, embedding_size, embedding_matrix)\nmodel.compile(optimizer='nadam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()\n","a863699e":"save_file = 'model_by_tyk.h5'\nhistory = model.fit(x_train, y_train,\n                    epochs=10, verbose=1,\n                    batch_size=512, shuffle=True)","26d03d7c":"y_pred = model.predict(x_test, batch_size=512)\ny_pred = y_pred.argmax(axis=1).astype(int)\ny_pred.shape","c2a7061a":"mapping = {phrase: sentiment for _, _, phrase, sentiment in df_train.values}\n\n# Overlapping\nfor i, phrase in enumerate(df_test.Phrase.values):\n    if phrase in mapping:\n        y_pred[i] = mapping[phrase]\n\ndf_test['Sentiment'] = y_pred\ndf_test[['PhraseId', 'Sentiment']].to_csv('submission2.csv', index=False)","3e2a642b":"\uc5b4\ud150\uc158 \ub808\uc774\uc5b4 \uc815\uc758\uc644\ub8cc \ud6c4  the entire model \uc815\uc758","45bfeb8a":"# Building a model\n\nLSTM \ubaa8\ub378\ub9c1","4133a025":"# Preprocessings\n\nAfter loading the datasets, we will preprocess the datasets. In this time, we will apply following techniques:\n\n* Nigation handling ##\ubd80\uc815\ubb38\n* Replacing numbers ##\uc22b\uc790\ub300\uccb4\n* Tokenization ##\ud1a0\ud070\ud654->\uac1c\ubcc4 \ub2e8\uc5b4\ubcc4\ub85c \uc790\ub974\uae30..\n* Zero padding ##\ubaa8\ub4e0 \ubb38\uc7a5 \uae38\uc774 \uac19\uac8c 0\uc73c\ub85c \ud328\ub529\n\n\uc218\ud589\ud568.","a3e9c2aa":"# Data Loading\n\nwe load the dataset and apply some transformations to use it in a deep learning model.","5bbcdbd6":"In addition to the datasets, we load a pretrained word embeddings.","57287f7f":"## Replacing numbers","7fd680df":"# Training the model","5bff65a9":"## Tokenization","3eeaf4a7":"## Negation handling","89815589":"# Making a submission file\n\n<\uc81c\ucd9c\ud574\ubcf4\uae30>","ccd1748a":"## \uc644\ub8cc","ef2bed30":"## Zero padding"}}