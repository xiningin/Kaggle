{"cell_type":{"4c00df95":"code","9528b08e":"code","964ad72c":"code","cbe744b7":"code","5a23828d":"code","deb88300":"code","d2ed233a":"code","86cc29af":"code","7ac31a37":"code","6b87772b":"code","aae358aa":"code","77561212":"code","a503afbc":"code","6654c161":"code","508e6ed1":"code","89886645":"code","dd4ace9a":"code","c9e9aacf":"code","28bd70d2":"code","519a67f4":"code","f985677b":"code","4d1cd84d":"code","9c4a7edf":"code","370c9ba1":"code","9f6dcfc8":"code","b49a9632":"code","5d9b8384":"markdown","5e3497b0":"markdown","0f3a544d":"markdown","0b5a2d87":"markdown","1c9aa8c1":"markdown","c3d39056":"markdown","f28d42eb":"markdown","fbfd1a08":"markdown","6536f420":"markdown","33af371b":"markdown","7e2feffa":"markdown","18b354bc":"markdown","e908e7ad":"markdown","5bb01ad9":"markdown","5f6b84a2":"markdown","d5aa6f2c":"markdown"},"source":{"4c00df95":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9528b08e":"ha=pd.read_csv('\/kaggle\/input\/health-care-data-set-on-heart-attack-possibility\/heart.csv')","964ad72c":"ha","cbe744b7":"ha.info()","5a23828d":"for i in ha:\n    print(f'{i}={ha[i].unique()}')","deb88300":"ha.loc[ha.ca==4]","d2ed233a":"ha=ha.drop(index=ha.loc[ha.ca==4].index)","86cc29af":"ha=ha.reset_index(drop=True)","7ac31a37":"ha","6b87772b":"hacopy=ha.copy()","aae358aa":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","77561212":"from pandas_profiling import ProfileReport\nx=ProfileReport(ha,explorative=True)\nx.to_notebook_iframe()","a503afbc":"for i in ha:\n    print(f'{i}={ha[i].unique()}')","6654c161":"ha=pd.get_dummies(data=ha,columns=['cp','restecg','slope','ca','thal'])","508e6ed1":"from sklearn.preprocessing import MinMaxScaler\na=MinMaxScaler()\nha[['age','trestbps','thalach','chol','oldpeak']]=a.fit_transform(ha[['age','trestbps','thalach','chol','oldpeak']])\nha","89886645":"from sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras ","dd4ace9a":"# li=[]\n# for i in range(30):\n#     X_train,X_test,y_train,y_test=train_test_split(ha.drop(columns='target'),ha['target'],test_size=0.3,random_state=)\n#     model=keras.Sequential([\n#         keras.layers.Dense(20,input_shape=(len(X_train.columns),),activation='relu'),\n#         keras.layers.Dense(10,activation='relu'),\n#         keras.layers.Dense(1,activation='sigmoid')\n#     ])\n#     model.compile(optimizer='adam',\n#                  loss='binary_crossentropy',\n#                  metrics=['accuracy'])\n#     model.fit(X_train,y_train,epochs=5)\n#     li.append(model.evaluate(X_test,y_test))\n# print(li)","c9e9aacf":"# o=[li[i][1] for i in range(len(li))]\n# o.index(max(o))","28bd70d2":"# max(o)","519a67f4":"X_train,X_test,y_train,y_test=train_test_split(ha.drop(columns='target'),ha['target'],test_size=0.3,random_state=15)\nmodel=keras.Sequential([\n    keras.layers.Dense(20,input_shape=(len(X_train.columns),),activation='relu'),\n    keras.layers.Dense(10,activation='relu'),\n    keras.layers.Dense(1,activation='sigmoid')\n])\nmodel.compile(optimizer='adam',\n             loss='binary_crossentropy',\n             metrics=['accuracy'])\nmodel.fit(X_train,y_train,epochs=50)\nmodel.evaluate(X_test,y_test)","f985677b":"l=model.predict(X_test)","4d1cd84d":"k=[]\nfor i in l:\n    if i<=0.5:\n        k.append(0)\n    else:\n        k.append(1)","9c4a7edf":"k[:10]","370c9ba1":"y_test[0:10]","9f6dcfc8":"from sklearn.metrics import confusion_matrix, classification_report\nprint(classification_report(y_test,k))","b49a9632":"cm=tf.math.confusion_matrix(labels=y_test,predictions=k)\nsns.heatmap(cm,annot=True,fmt='d')\nplt.xlabel('predicted')\nplt.ylabel('Truth')","5d9b8384":"Now we have a fully cleaned data","5e3497b0":"Now we have all our values in the between of 0 to 1\nwhich is perfect for classification of data","0f3a544d":"# Prediction","0b5a2d87":"Now we have to scale the of some columns that is:\n* age\n* trestbps\n* thalach\n* chol\n* oldpeak\n'I am using a min max scalar for this'","1c9aa8c1":"# **READING THE DATA**","c3d39056":"Columns information\n\n1) age\n2) sex\n3) chest pain type (4 values)\n4) resting blood pressure\n5) serum cholestoral in mg\/dl\n6) fasting blood sugar > 120 mg\/dl\n7) resting electrocardiographic results (values 0,1,2)\n8) maximum heart rate achieved\n9) exercise induced angina\n10) oldpeak = ST depression induced by exercise relative to rest\n11) the slope of the peak exercise ST segment\n12) number of major vessels (0-3) colored by flourosopy\n13) thal: 0 = normal; 1 = fixed defect; 2 = reversable defect\n14) target: 0= less chance of heart attack 1= more chance of heart attack","f28d42eb":"# Accuracy and Confusion matrix","fbfd1a08":"# **DATA CLEANING**","6536f420":"There is some classification columns has more than 2 option so we have to change it under one hot encoding. \n* cp\n* restecg\n* slope\n* ca\n* thal","33af371b":"No missing value in dataset","7e2feffa":"12) number of major vessels (0-3) colored by flourosopy\nThere are only 0,1,2,3 vesses colored by flourosopy.. but in dataset ca=[0,2,1,3,4] so 4 must be removed from there...","18b354bc":"# Data preprocessing","e908e7ad":"# ** Exploratory DATA Analysis **","5bb01ad9":"***Working in progress, finding the way for more accuracy in model***","5f6b84a2":"now we have to analyze the ","d5aa6f2c":"From the above data profiling, the correlation matrix give a rough a idea about the dataset\n1. As our target row in correlation has very less correlation with other features.\n2. even there is not a very high correlation with each other. \nso we have to use nueral network for this datasets."}}