{"cell_type":{"4cf92e08":"code","ca0123c4":"code","6515105f":"code","def164cc":"code","220866a4":"code","d237f213":"code","d4c58c6a":"code","1c1629c0":"code","11550974":"code","07fc8596":"code","1ee8f949":"code","a752d786":"code","2dfdb4c6":"code","131c6b03":"code","cc82d1a2":"code","e135257f":"code","19e34d46":"code","a4ee6315":"code","353ddbf9":"code","0e37e585":"code","7441d006":"code","263e3213":"code","628ceb67":"code","25d0b967":"code","be74406f":"code","1f533b35":"code","ece86a16":"code","8bb64a51":"code","49838af6":"code","824b69b4":"code","d7c70a14":"code","541944ce":"code","6efdcb3d":"code","4b4c4164":"code","a78be0ac":"code","c8e0b0e9":"code","633180ac":"code","1f4ae9e0":"code","8c793f06":"markdown","a9178e8e":"markdown","060dd456":"markdown","62cec79c":"markdown","19636e31":"markdown","e0b52c82":"markdown","fd198f89":"markdown","043892e7":"markdown","4ee8269c":"markdown","7b120ad0":"markdown","19afcd7b":"markdown","2280f77f":"markdown","975c253c":"markdown","cce08a94":"markdown"},"source":{"4cf92e08":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler","ca0123c4":"train_df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv\", index_col=\"id\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv\", index_col='id')\nsub_sample_df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/sample_submission.csv\")","6515105f":"train_df","def164cc":"train_df.describe().T.style.background_gradient(subset=['mean'], cmap='coolwarm').background_gradient(subset=['std'], cmap='inferno')","220866a4":"plt.figure(figsize=(14,5))\ntarget_values_sr = train_df['loss'].value_counts()\nsns.barplot(x=target_values_sr.index, y=target_values_sr.values, palette='coolwarm')\nplt.title(\"Target unique values\", fontdict={'fontsize':20});","d237f213":"corr_mat = train_df.corr()\nplt.figure(figsize=(25,6))\ncorr_mat['loss'][:-1].plot(kind='bar', grid=True)\nplt.title(\"Features correlation to target label\", fontdict={'fontsize':20});","d4c58c6a":"import tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.layers import ReLU\nfrom tensorflow.keras.layers import BatchNormalization, LayerNormalization\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.layers import Conv1D, Flatten, MaxPooling1D, Embedding","1c1629c0":"X = train_df.iloc[:,:-1]\ny = train_df.iloc[:,-1]","11550974":"# Split our data\nXtrain, Xvalid, ytrain, yvalid = train_test_split(X, y, test_size=0.2, random_state=45)\n\n# Scale it\nscaler = StandardScaler()\nXtrain_scaled = scaler.fit_transform(Xtrain)\nXvalid_scaled = scaler.transform(Xvalid)","07fc8596":"N_INPUTS = X.shape[1]","1ee8f949":"def create_autoencoder():\n    # Define an encoder\n    visible = Input(shape=(N_INPUTS,))\n    e = Dense(N_INPUTS*2)(visible)\n    e = BatchNormalization()(e)\n    e = ReLU()(e)\n    # Define a bottleneck\n    n_bottleneck = N_INPUTS\n    bottleneck = Dense(n_bottleneck)(e)\n    # Define decoder\n    d = Dense(N_INPUTS*2)(bottleneck)\n    d = BatchNormalization()(d)\n    d = ReLU()(d)\n    # Output layer\n    output = Dense(N_INPUTS, activation='linear')(d)\n\n    # Define autoencoder model\n    autoenc = Model(inputs=visible, outputs=output)\n    # Compile model\n    autoenc.compile(optimizer='adam',loss='mse')\n    \n    return autoenc, visible, bottleneck","a752d786":"# instantiate autoencoder\nautoenc_m, visible, bottleneck = create_autoencoder()\n# Plot model\nplot_model(autoenc_m, show_shapes=True)\n# Fit the autoencoder model to reconstruct inputs\nhistory = autoenc_m.fit(Xtrain_scaled, \n                        Xtrain_scaled, \n                        epochs=50, \n                        verbose=2, \n                        validation_data=(Xvalid_scaled, Xvalid_scaled))","2dfdb4c6":"history_df = pd.DataFrame(history.history)\n\nhistory_df.plot(figsize=(10,5))","131c6b03":"# Define an encoder model (without the decoder)\nencoder = Model(inputs=visible, outputs=bottleneck)\n# Save the model\nencoder.save('encoder.h5')","cc82d1a2":"# Plot the model\nplot_model(encoder, 'encoder.png', show_shapes=True)","e135257f":"scaler_out = StandardScaler()\n# Reshape our target series\nytrain = np.array(ytrain).reshape((len(ytrain),1))\nyvalid = np.array(yvalid).reshape((len(yvalid),1))\n\nytrain_scaled = scaler_out.fit_transform(ytrain)\nyvalid_scaled = scaler_out.transform(yvalid)","19e34d46":"# Define a model\ndef create_ANN():\n    \n    input_lyr = Input(shape=(N_INPUTS,))\n    x = Dense(100, activation='relu')(input_lyr)\n    x = Dense(100, activation='relu')(x)\n    x = Dense(50, activation='relu')(x)\n    output_lyr = Dense(1)(x)\n    \n    model = Model(inputs=input_lyr, outputs=output_lyr, name='baseline_model')\n    # model.summary()\n    \n    return model\n    \nann_model = create_ANN()\nplot_model(ann_model, show_shapes=True)","a4ee6315":"# Compile and Fit base model\nann_model.compile(optimizer='adam', loss='mse')\nhistory_base = ann_model.fit(Xtrain_scaled, \n                             ytrain_scaled,\n                             epochs=20,\n                             validation_data=(Xvalid_scaled,yvalid_scaled))","353ddbf9":"history_base_df = pd.DataFrame(history_base.history)\n\nhistory_base_df.plot(figsize=(10,5))","0e37e585":"# Make prediction\ny_pred_base = ann_model.predict(Xvalid_scaled)\ny_pred_base = scaler_out.inverse_transform(y_pred_base)\nyval = scaler_out.inverse_transform(yvalid_scaled)\n\n# Calculate RMSE\nrmse = np.sqrt(mean_squared_error(yval, y_pred_base))\nprint(f\"Base Model RMSE= {rmse}\")","7441d006":"# Load the encoder from file\nencoder = load_model(\"encoder.h5\")\n# Encode training dataset\nXtrain_enc = encoder.predict(Xtrain_scaled)\n# \nXvalid_enc = encoder.predict(Xvalid_scaled)\n\nann_enc_model = create_ANN()\nann_enc_model.compile(optimizer=\"adam\", loss='mse')\nhist_ann_enc = ann_enc_model.fit(Xtrain_enc,\n                                 ytrain_scaled,\n                                 epochs=20,\n                                 validation_data=(Xvalid_enc, yvalid_scaled))","263e3213":"hist_enc_df = pd.DataFrame(hist_ann_enc.history)\n\nhist_enc_df.plot(figsize=(10,5))","628ceb67":"# Make prediction\ny_pred_enc = ann_enc_model.predict(Xvalid_enc)\ny_pred_enc = scaler_out.inverse_transform(y_pred_enc)\nyval = scaler_out.inverse_transform(yvalid_scaled)\n\n# Calculate RMSE\nrmse = np.sqrt(mean_squared_error(yval, y_pred_enc))\nprint(f\"Base Model RMSE= {rmse}\")","25d0b967":"es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6,mode='min',restore_best_weights=True, min_delta=0.0001)\nplateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3, mode='min',verbose=1)\nweight_initializer = tf.keras.initializers.glorot_uniform()\nbias_init = tf.keras.initializers.Zeros()","be74406f":"def best_ann():\n    \n    input_lyr = Input(shape=(N_INPUTS,))\n    layer_1 = Dense(100, activation='relu',kernel_initializer=weight_initializer, bias_initializer=bias_init)(input_lyr)\n    #dropout_1 = Dropout(0.2)(layer_1)\n    layer_norm_1 = LayerNormalization()(layer_1)\n    layer_2 = Dense(100, activation='relu')(layer_norm_1)\n    #dropout_2 = Dropout(0.2)(layer_2)\n    layer_norm_2 = LayerNormalization()(layer_2)\n    layer_3 = Dense(50, activation='relu')(layer_norm_2)\n    output = Dense(1)(layer_3)\n    \n    model = Model(inputs=input_lyr, outputs=output)\n    \n    return model","1f533b35":"# Instantiate best model\nbest_model = best_ann()\nplot_model(best_model,show_shapes=True)","ece86a16":"tf.random.set_seed(45)\n\n# Compile the model\nbest_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),\n                   loss=tf.keras.losses.mean_squared_error)\n\n# Fit the model\nhist_best_model = best_model.fit(Xtrain_enc,\n                                 ytrain_scaled,\n                                 batch_size=128,\n                                 epochs=50,\n                                 validation_data=(Xvalid_enc, yvalid_scaled),\n                                 callbacks=[es,plateau])","8bb64a51":"hist_best_df = pd.DataFrame(hist_best_model.history)\n\nhist_best_df.drop('lr',axis=1).plot(figsize=(10,5))","49838af6":"# Make prediction\ny_pred_enc_b = best_model.predict(Xvalid_enc)\ny_pred_enc_b = scaler_out.inverse_transform(y_pred_enc_b)\nyval = scaler_out.inverse_transform(yvalid_scaled)\n\n# Calculate RMSE\nrmse = np.sqrt(mean_squared_error(yval, y_pred_enc_b))\nprint(f\"Base Model RMSE= {rmse}\")","824b69b4":"def plot_history(d):\n    df = pd.DataFrame(d)\n    df.drop('lr', axis=1).plot(figsize=(10,5))\n    plt.show()","d7c70a14":"N_FOLDS = 10\nSEED = 45","541944ce":"rmse_folds = []\ntest_sub = np.zeros((len(test_df),1))\n\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\ntrain_df['kfold'] = -1\n\nfor fold, (tr_idx, val_idx) in enumerate(skf.split(X=train_df, y=train_df['loss'])):\n    train_df.loc[val_idx,'kfold'] = fold\n\n\nfor fold in range(N_FOLDS):\n    \n    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5,mode='min',restore_best_weights=True, min_delta=0.0001)\n    plateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3, mode='min',verbose=1)\n    weight_initializer = tf.keras.initializers.glorot_uniform()\n    bias_init = tf.keras.initializers.Zeros()\n\n    \n    print('==================')\n    print(f\"TRAINING FOLD={fold+1}\")\n    print('==================')\n    train = train_df[train_df['kfold'] != fold].reset_index(drop=True)\n    valid = train_df[train_df['kfold'] == fold].reset_index(drop=True)\n    xtrain = train.drop(['kfold', 'loss'], axis=1)\n    xvalid = valid.drop(['kfold','loss'], axis=1)\n    \n    ytrain = np.array(train['loss']).reshape((len(xtrain),1))\n    yvalid = np.array(valid['loss']).reshape((len(xvalid), 1))\n    \n    # Instantiate our scaler for input data and target\n    scaler_in = StandardScaler()\n    scaler_out = StandardScaler()\n    # Scale input dataset\n    xtrain_scaled = scaler_in.fit_transform(xtrain)\n    xvalid_scaled = scaler_in.transform(xvalid)\n    test_scaled = scaler_in.transform(test_df)\n    # Scale output target\n    ytrain_scaled = scaler_out.fit_transform(ytrain)\n    yvalid_scaled = scaler_out.transform(yvalid)\n    \n    # Encode our datasets\n    xtrain_enc = encoder.predict(xtrain_scaled)\n    xvalid_enc = encoder.predict(xvalid_scaled)\n    test_enc = encoder.predict(test_scaled)\n    \n    # Instantiate and fit ANN model\n    best_model_cv = best_ann()\n    best_model_cv.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),\n                          loss=tf.keras.losses.mean_squared_error)\n    \n    history = best_model_cv.fit(xtrain_enc,\n                                ytrain_scaled,\n                                epochs=30,\n                                batch_size=128,\n                                validation_data=(xvalid_enc, yvalid_scaled),\n                                callbacks=[es, plateau])\n    \n    \n    # Evaluate the model\n    y_pred = best_model_cv.predict(xvalid_enc)\n    y_pred_sub = best_model_cv.predict(test_enc)\n    # Calculate rmse\n    y_pred = scaler_out.inverse_transform(y_pred)\n    y_true = scaler_out.inverse_transform(yvalid_scaled)\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    rmse_folds.append(rmse)\n    print('==============================')\n    print(f\"FOLD={fold+1}, RMSE={rmse}\")\n    print('==============================')\n    \n    # Plot history\n    plot_history(history.history)\n    \n    # Add predictions on test dataset for submission\n    test_sub += y_pred_sub\n    \nprint('===============================================')\nprint(f'AVERAGE RMSE AFTER {N_FOLDS} FOLDS = {np.average(rmse_folds)}')\nprint('===============================================')","6efdcb3d":"test_sub = test_sub \/ N_FOLDS\ntest_sub = scaler_out.inverse_transform(test_sub)\nsub_df = sub_sample_df.copy()\nsub_df['loss'] = test_sub\nsub_df.to_csv('first_encoder_sub_10_folds.csv', index=False)","4b4c4164":"tf.keras.backend.clear_session()","a78be0ac":"def conv1D_model():\n    \n    # Create a  model\n    input_L = Input(shape=((N_INPUTS,1)))\n    conv1 = Conv1D(64, 2, activation='relu')(input_L) \n    conv2 = Conv1D(128, 2, activation='relu')(conv1)\n    maxpool1 = MaxPooling1D(pool_size=3, strides=1, padding='same')(conv2)\n    flatten = Flatten()(maxpool1)\n    x = Dense(50, activation='relu')(flatten)\n    x = Dense(25, activation='relu')(x)\n    output = Dense(1)(x)\n    \n    model = Model(inputs=input_L, outputs=output)\n    \n    return model","c8e0b0e9":"cnn_rmse_folds = []\nann_rmse_folds = []\ntest_sub_ann = np.zeros((len(test_df),1))\ntest_sub_cnn = np.zeros((len(test_df),1))\n\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\ntrain_df['kfold'] = -1\n\nfor fold, (tr_idx, val_idx) in enumerate(skf.split(X=train_df, y=train_df['loss'])):\n    train_df.loc[val_idx,'kfold'] = fold\n\n\nfor fold in range(N_FOLDS):\n    \n    # Split the data\n    train = train_df[train_df['kfold'] != fold].reset_index(drop=True)\n    valid = train_df[train_df['kfold'] == fold].reset_index(drop=True)\n    xtrain = train.drop(['kfold', 'loss'], axis=1)\n    xvalid = valid.drop(['kfold','loss'], axis=1)\n    \n    ytrain = np.array(train['loss']).reshape((len(xtrain),1))\n    yvalid = np.array(valid['loss']).reshape((len(xvalid), 1))\n    \n    # Instantiate our scaler for input data and target\n    scaler_in = StandardScaler()\n    scaler_out = StandardScaler()\n    # Scale input dataset\n    xtrain_scaled = scaler_in.fit_transform(xtrain)\n    xvalid_scaled = scaler_in.transform(xvalid)\n    test_scaled = scaler_in.transform(test_df)\n    # Scale output target\n    ytrain_scaled = scaler_out.fit_transform(ytrain)\n    yvalid_scaled = scaler_out.transform(yvalid)\n    \n    print('======================================')\n    print(f'TRAINING AUTOENCODER IN FOLD={fold+1}')\n    print('======================================')\n    # instantiate autoencoder\n    autoenc_m, visible, bottleneck = create_autoencoder()\n    # Fit the autoencoder model to reconstruct inputs\n    autoenc_m.fit(xtrain_scaled, \n                  xtrain_scaled, \n                  epochs=15, \n                  verbose=2, \n                  validation_data=(xvalid_scaled, xvalid_scaled),\n                  callbacks=[plateau])\n    \n    # Define an encoder model (without the decoder)\n    encoder = Model(inputs=visible, outputs=bottleneck)\n    \n    # Encode our datasets\n    xtrain_enc = encoder.predict(xtrain_scaled)\n    xvalid_enc = encoder.predict(xvalid_scaled)\n    test_enc = encoder.predict(test_scaled)\n    \n    print('===========================')\n    print(f\"TRAINING ANN MODEL FOLD={fold+1}\")\n    print('===========================')\n    \n    #===================== ANN Model ============================#\n    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6,mode='min',restore_best_weights=True, min_delta=0.0001)\n    plateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3, mode='min',verbose=1)\n    weight_initializer = tf.keras.initializers.glorot_uniform()\n    bias_init = tf.keras.initializers.Zeros()\n    \n    # Instantiate and fit ANN model\n    best_model_cv = best_ann()\n    best_model_cv.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),\n                          loss=tf.keras.losses.mean_squared_error)\n    \n    history_ann = best_model_cv.fit(xtrain_enc,\n                                    ytrain_scaled,\n                                    epochs=30,\n                                    batch_size=128,\n                                    validation_data=(xvalid_enc, yvalid_scaled),\n                                    callbacks=[es, plateau])\n\n    # Evaluate the ANN model\n    y_pred_ann = best_model_cv.predict(xvalid_enc)\n    y_pred_sub_ann = best_model_cv.predict(test_enc)\n    # Calculate rmse\n    y_pred_ann = scaler_out.inverse_transform(y_pred_ann)\n    y_true = scaler_out.inverse_transform(yvalid_scaled)\n    ann_rmse = np.sqrt(mean_squared_error(y_true, y_pred_ann))\n    ann_rmse_folds.append(ann_rmse)\n    \n    print('===========================')\n    print(f\"FOLD={fold+1}, RMSE={ann_rmse}\")\n    print('===========================')\n    \n    # Plot history\n    plot_history(history_ann.history)\n    \n    test_sub_ann += y_pred_sub_ann\n    \n    #====================== CNN Model ===========================#\n    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4,mode='min',restore_best_weights=True, min_delta=0.0001)\n    plateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=2, mode='min',verbose=1)\n\n    print('=============================')\n    print(f\"TRAINING CNN MODEL FOLD={fold+1}\")\n    print('=============================')\n    # Prepare shape of input data for CNN model\n    xtr_conv = xtrain_scaled.reshape((xtrain_scaled.shape[0],xtrain_scaled.shape[1], 1))\n    xval_conv = xvalid_scaled.reshape((xvalid_scaled.shape[0],xvalid_scaled.shape[1], 1))\n    test_conv = test_scaled.reshape((test_scaled.shape[0],test_scaled.shape[1], 1))\n    \n    # Instantiate and fit Conv1D model\n    conv_model = conv1D_model()\n    conv_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),\n                       loss=tf.keras.losses.mean_squared_error)\n    \n    \n    history_cnn = conv_model.fit(xtr_conv,\n                                 ytrain_scaled,\n                                 epochs=30,\n                                 batch_size=128,\n                                 validation_data=(xval_conv, yvalid_scaled),\n                                 callbacks=[es, plateau])\n    \n    # Evaluate the CNN model\n    y_pred_cnn = conv_model.predict(xval_conv)\n    y_pred_sub_cnn = conv_model.predict(test_conv)\n    # Calculate rmse\n    y_pred_cnn = scaler_out.inverse_transform(y_pred_cnn)\n    y_true = scaler_out.inverse_transform(yvalid_scaled)\n    cnn_rmse = np.sqrt(mean_squared_error(y_true, y_pred_cnn))\n    cnn_rmse_folds.append(cnn_rmse)\n    \n    \n    print('==============================')\n    print(f\"FOLD={fold+1}, RMSE={cnn_rmse}\")\n    print('==============================')\n    \n    # Plot henc\n    plot_history(history_cnn.history)\n    \n    # Add predictions on test dataset for submission\n    test_sub_cnn += y_pred_sub_cnn\n    \nprint('===============================================')\nprint(f'AVERAGE RMSE FOR ANN MODEL AFTER {N_FOLDS} FOLDS = {np.average(ann_rmse_folds)}')\nprint('===============================================')\nprint('===============================================')\nprint(f'AVERAGE RMSE FOR CNN MODEL AFTER {N_FOLDS} FOLDS = {np.average(cnn_rmse_folds)}')\nprint('===============================================')","633180ac":"test_sub = (test_sub_ann \/ N_FOLDS) + (test_sub_cnn \/ N_FOLDS) \/ 2\ntest_sub = scaler_out.inverse_transform(test_sub)\nsub_df = sub_sample_df.copy()\nsub_df['loss'] = test_sub\nsub_df.to_csv('encoder_ann_cnn_sub_10_folds_sc_best.csv', index=False)","1f4ae9e0":"tf.keras.backend.clear_session()","8c793f06":"## Create an autoencoder with Keras Functional API - autoencoder as data preperation","a9178e8e":"The correlation between features and target is very weak almost doesn't exists. Correlation between features very weak too.","060dd456":"Now, we can use saved encoder from autoencoder model to compress input data and train a different predictive model. First, let's establish a baseline in performance on this problem. As a good practice, I will scale both the input variables and target variable prior to fitting and evaluating the model. I've already scaled train and test variables, now I only have to scale target variable.","62cec79c":"In the last competitions I've spent more time in EDA than on modelling, therefore in this notebook my EDA will stay simple for now. I hope you forgive me this shortcut but my goal is to learn Keras Functional API and I want to see if using aoutoencoder can help to improve a model with artificially created dataset with noise.","19636e31":"Now we have nice model I will wrap it up with one of the cross validation method and take an average of it as it usually helps to improve predictions(with different split we can achieve better results).","e0b52c82":"## K-fold Cross Validation","fd198f89":"## Encoder and CNN ensemble for tabular data","043892e7":"## Simply EDA","4ee8269c":"## Time for first submission ","7b120ad0":"Hi Kagglers!!\n\nIn this notebook I'd like to learn keras functional API to help me build a model and I hope you guys will learn that with me. I also will use encoder from autoencoder as data preperation, than use simple ANN as a baseline. After that I will try to make my model more robust and wrap it all in cross validation.","19afcd7b":"## Base model","2280f77f":"As we can see our model is not a good model, so we need to address that. Let's see if data prepared via autoencoder can help us.","975c253c":"Our model behaviour is much better with data transformed by encoder from autoencoder. Now it would be a good time to improve our model by adding e.g. Dropout, weight_inittializer or early stopping.","cce08a94":"# **Don't forget to upvote \ud83d\udc4d if you like it. Big Thanks \ud83d\udc97**"}}