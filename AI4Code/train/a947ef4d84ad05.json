{"cell_type":{"4bc4f9a5":"code","017dab1c":"code","e9dfb58c":"code","8a7ced35":"code","7df07293":"code","9e77c207":"code","71a34710":"code","33a66967":"code","886097c2":"code","3bd9f51f":"code","4d2aa6ad":"code","ad819c6b":"code","a841eea6":"code","065d9776":"code","0ac040e1":"code","c3e55fda":"code","124dd7b5":"code","1cf1b68b":"code","caa4b9b8":"code","8bcf03cd":"code","b8dec04b":"code","1c621ca1":"code","5814366b":"code","c792ff8b":"code","f319d719":"code","dc3b7d71":"code","80163c53":"code","c9559044":"code","2fc1ca2a":"code","1607d473":"code","408e7fcd":"code","da92f44c":"code","b811c562":"code","47bb3fbf":"code","c694728c":"code","2977ead1":"code","b8fd8b64":"code","5d12ddbe":"code","7f1d7845":"code","c4a1cb4c":"code","b459d7f5":"code","a4947e04":"code","9e2b6008":"code","4f940e38":"code","6683e56c":"code","017f9ffc":"markdown","60aa8c96":"markdown","47b92c1a":"markdown","69a8e6da":"markdown","e9bb237b":"markdown","33f70b9a":"markdown","fbdfdcc0":"markdown"},"source":{"4bc4f9a5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nimport warnings\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","017dab1c":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n\nprint(\"train shape:\" + str(train.shape))\nprint(\"test shape:\" + str(test.shape))","e9dfb58c":"train","8a7ced35":"train.isnull().sum()","7df07293":"#Remove Outliers\ntrain.drop([523, 1298], axis = 0, inplace = True)","9e77c207":"from sklearn.model_selection import train_test_split\n\nX = train.drop(\"SalePrice\", axis = 1, inplace = False)\ny = train.SalePrice\n\nX_train_full, X_test_full, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state = 0)","71a34710":"X_train_full","33a66967":"#DO I STILL NEED THESE?????\ncategorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and X_train_full[cname].dtype == \"object\"]","886097c2":"# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and X_train_full[cname].dtype == \"object\"]\n\n\n\"\"\"#NOOOOOOOTTTT NEEEDEDDD             Drop Features with more than 50% of NA values \ndrop_na_cols = list(X_train_full.columns[(X_train_full.isnull().sum() > 700)])\n\nfor col in drop_na_cols:\n    #Remove Drop cols from categorical_cols & numerical_cols \n    if col in categorical_cols: \n        categorical_cols.remove(col)\n    if col in numerical_cols:\n        numerical_cols.remove(col)\n\"\"\"\n\ncat_NA_mode = ['Exterior1st', 'Exterior2nd', 'SaleType']\n\nnum_NA_mean = ['KitchenQual','FireplaceQu']\n\n#Impute numericals\n\"\"\"num_zero = ['MasVnrArea', 'LotFrontage',\n            'BsmtFullBath', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF',  'BsmtHalfBath',\n            'GarageYrBlt', 'GarageArea', 'GarageCars']\"\"\"","3bd9f51f":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder, StandardScaler, RobustScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin","4d2aa6ad":"class general_cleaner(BaseEstimator, TransformerMixin):\n    '''\n    This class applies what we know from the documetation.\n    It cleans some known missing values\n    If flags the missing values\n\n    This process is supposed to happen as first step of any pipeline\n    '''\n    \n    def __init__(self, drop_NA = False, fraction = None):\n        self.drop_NA = drop_NA\n        self.fraction = fraction\n        self.drop_columns = []\n\n    \n    def fit(self, X, y=None):\n        self.drop_columns = [] # for safety, when we refit we want new columns\n        return self\n    \n    def transform(self, X, y = None):\n        \n        #Check if we need to drop any columns & save as list to remove later\n        if (self.drop_NA):\n            if (len(self.drop_columns) == 0):\n                self.drop_columns = list(X.columns[((X.isnull().sum())\/(X.shape[0]) > self.fraction)])\n        \n        #Impute missing values \n        \n        X['MSSubClass'] = X ['MSSubClass'].apply(str)\n        X['YrSold'] = X['YrSold'].astype(str)\n        X['MoSold'] = X['MoSold'].astype(str)\n\n        # Replace \"NA\" values with \"None\"\n        for col in ('PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'MasVnrType', 'Utilities'):\n            X[col] = X[col].fillna('None')\n\n        # Replace values with 0\n        X['MasVnrArea'] = X['MasVnrArea'].fillna(0)\n\n        # Replace the missing with their mode\n        for col in ('Exterior1st', 'Exterior2nd', 'SaleType'):\n            X[col] = X[col].fillna(X[col].mode()[0])\n\n        X['MSZoning'] = X.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\n        #Replace missing values with Median\n        #X['LotFrontage'] = X.groupby(\"Neighborhood\")['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n        #Replace NA with default values\n        X['Electrical'] = X['Electrical'].fillna(\"SBrkr\")\n        X['Functional'] = X['Functional'].fillna('Typ')\n        X['KitchenQual'] = X['KitchenQual'].fillna('TA')\n\n        # Replacing Garage NA with None and missing values with 0, no Garage = no Garage features\n        for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n            X[col] = X[col].fillna('None')\n\n        for col in ('GarageArea', 'GarageCars'): #'GarageYrBlt' impute mean\n            X[col] = X[col].fillna(0)\n\n        # Replacing Basement NA with None and missing values with 0\n        for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n            X[col] = X[col].fillna('None')\n\n        for col in ('BsmtFullBath', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF',  'BsmtHalfBath'):\n            X[col] = X[col].fillna(0)\n        \n        #Remove specified columns \n        if (self.drop_NA):\n            for col in self.drop_columns:\n                del X[col]\n        \n        return X","ad819c6b":"tmp = X_train_full.copy()\ngt = general_cleaner(drop_NA= True, fraction = 0.6)\ntmp = gt.fit_transform(tmp)\ntmp.head()","a841eea6":"tmp.isnull().sum().sort_values(ascending = False)","065d9776":"tmp2 = X_test_full.copy()\ntmp2 = gt.transform(tmp2)\ntmp2.head()","0ac040e1":"class feat_sel(BaseEstimator, TransformerMixin):\n    '''\n    This transformer selects either numerical or categorical features.\n    In this way we can build separate pipelines for separate data types.\n    '''\n    def __init__(self, dtype='numeric'):\n        self.dtype = dtype\n\n    def fit( self, X, y=None ):\n        return self \n\n    def transform(self, X, y=None):\n        if self.dtype == 'numeric':\n            num_cols = X.columns[X.dtypes != object].tolist()\n            return X[num_cols]\n        elif self.dtype == 'category':\n            cat_cols = X.columns[X.dtypes == object].tolist()\n            return X[cat_cols]","c3e55fda":"tmp = X_train_full.copy()\nselector = feat_sel()  # it is numeric by defaultw\ntmp = selector.transform(tmp)  # no reason to fit again\ntmp.head()","124dd7b5":"tmp = X_train_full.copy()\nselector = feat_sel(dtype = 'category')  # it is numeric by defaultw\ntmp = selector.transform(tmp)  # no reason to fit again\ntmp.head()","1cf1b68b":"class df_imputer(BaseEstimator, TransformerMixin):\n    '''\n    Just a wrapper for the SimpleImputer that keeps the dataframe structure\n    '''\n    def __init__(self, strategy='mean'):\n        self.strategy = strategy\n        self.imp = None\n        self.statistics_ = None\n\n    def fit(self, X, y=None):\n        self.imp = SimpleImputer(strategy=self.strategy)\n        self.imp.fit(X)\n        self.statistics_ = pd.Series(self.imp.statistics_, index=X.columns)\n        return self\n\n    def transform(self, X):\n        # X is supposed to be a DataFrame\n        Ximp = self.imp.transform(X)\n        Xfilled = pd.DataFrame(Ximp, index=X.index, columns=X.columns)\n        return Xfilled\n\n\nclass df_scaler(BaseEstimator, TransformerMixin):\n    '''\n    Wrapper of StandardScaler or RobustScaler\n    '''\n    \n    def __init__(self, method='standard'):\n        self.scl = None\n        self.scale_ = None\n        self.method = method\n        if self.method == 'standard':\n            self.mean_ = None\n        elif method == 'robust':\n            self.center_ = None\n        self.columns = None  # this is useful when it is the last step of a pipeline before the model\n\n    def fit(self, X, y=None):\n        if self.method == 'standard':\n            self.scl = StandardScaler()\n            self.scl.fit(X)\n            self.mean_ = pd.Series(self.scl.mean_, index=X.columns)\n        elif self.method == 'robust':\n            self.scl = RobustScaler()\n            self.scl.fit(X)\n            self.center_ = pd.Series(self.scl.center_, index=X.columns)\n        self.scale_ = pd.Series(self.scl.scale_, index=X.columns)\n        return self\n\n    def transform(self, X):\n        # assumes X is a DataFrame\n        Xscl = self.scl.transform(X)\n        Xscaled = pd.DataFrame(Xscl, index=X.index, columns=X.columns)\n        self.columns = X.columns\n        return Xscaled\n\n    def get_feature_names(self):\n        return list(self.columns)  # this is going to be useful when coupled with FeatureUnion","caa4b9b8":"scale = df_scaler()\ntmp = X_train_full.copy()\nnumerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype != \"object\"]\nnew_data= scale.fit_transform(tmp[numerical_cols])\nnew_data","8bcf03cd":"class dummify(BaseEstimator, TransformerMixin):\n    '''\n    Wrapper for get dummies\n    '''\n    def __init__(self, drop_first = True, match_cols = True):\n        self.drop_first = drop_first\n        self.columns = []  # useful to well behave with FeatureUnion\n        self.match_cols = match_cols\n\n    def fit(self, X, y = None):\n        self.columns = []  # for safety, when we refit we want new columns\n        return self\n    \n    def match_columns(self, X):\n        miss_train = list(set(X.columns) - set(self.columns))\n        miss_test = list(set(self.columns) - set(X.columns))\n        \n        err = 0\n        \n        if len(miss_test) > 0:\n            for col in miss_test:\n                X[col] = 0  # insert a column for the missing dummy\n                err += 1\n        if len(miss_train) > 0:\n            for col in miss_train:\n                del X[col]  # delete the column of the extra dummy\n                err += 1\n                \n        if err > 0:\n            warnings.warn('The dummies in this set do not match the ones in the train set, we corrected the issue.',\n                         UserWarning)\n            \n        return X\n        \n    def transform(self, X):\n        X = pd.get_dummies(X, drop_first = self.drop_first)\n        if (len(self.columns) > 0): \n            if self.match_cols:\n                X = self.match_columns(X)\n            self.columns = X.columns\n        else:\n            self.columns = X.columns\n        return X\n    \n    def get_features_name(self):\n        return self.columns","b8dec04b":"tmp = X_train_full[['HouseStyle']].copy()\ndummifier = dummify()\ntmp = dummifier.transform(tmp)  # no reason to call the fit method here\ntmp.sample(5)","1c621ca1":"tmp = X_train_full[['RoofMatl']].copy()\ndummifier = dummify()\ndummifier.fit_transform(tmp).sum()  # to get how many dummies are present","5814366b":"# However, in the test set we don't have all those values\n\nX_test_full.RoofMatl.value_counts()","c792ff8b":"tmp = X_test_full[['RoofMatl']].copy()\n\ndummifier.transform(tmp).sum()  # the same instance as before","f319d719":"from scipy.stats import skew, norm\nimport scipy.stats as stats\n#from scipy.special import boxcox1p\n#from scipy.stats import boxcox_normmax\n\nclass tr_numeric(BaseEstimator, TransformerMixin):\n    def __init__(self, eng_feat = True, years_old = True, fix_skews = True):\n        self.columns = []  # useful to well behave with FeatureUnion\n        self.eng_feat = eng_feat\n        self.years_old = years_old\n        self.fix_skews = fix_skews\n        self.skew_columns = []\n        \n\n    def fit(self, X, y=None):\n        self.skew_columns = []  # for safety, when we refit we want new columns\n        return self\n    \n\n    def remove_skew(self, X, column):\n        X[column] = np.log1p(X[column])\n        #X[column]= boxcox1p(X[column], boxcox_normmax(X[column] + 1))\n        return X\n\n\n    def engineer_features(self, X):\n        if self.eng_feat:\n            X['sf_per_room'] = X.loc[:,'GrLivArea'] \/ X.loc[:,'TotRmsAbvGrd']\n            X['TotalSF'] = X.loc[:,'TotalBsmtSF'] + X.loc[:,'1stFlrSF'] + X.loc[:,'2ndFlrSF']\n        return X\n    \n    def subtract_years_old(self, X):\n        if self.years_old:\n            X['YearsSinceBuilt'] = 2010 - X.loc[:,'YearBuilt']\n            X['YearsSinceRemod'] = 2010 - X.loc[:,'YearRemodAdd']\n            X.drop(['YearBuilt','YearRemodAdd'], axis = 1, inplace = True)\n        return X\n\n    def transform(self, X, y=None):\n        \n        #We must perform any feature engineering before fixing skews\n        if self.eng_feat:\n            X = self.engineer_features(X)\n            \n        if self.years_old:\n            X = self.subtract_years_old(X)\n        \n        if self.fix_skews:\n            if (len(self.skew_columns) == 0):\n                skew_mask = (X.apply(lambda x: skew(x)).abs() > 0.5)\n                skew_columns = list(skew_mask.loc[skew_mask.values].index)\n                self.skew_columns = skew_columns\n            \n            for col in self.skew_columns:\n                X = self.remove_skew(X, col)\n                \n        \n        self.columns = X.columns \n        return X\n    \n\n    def get_features_name(self):  # again, it will be useful later\n        return self.columns","dc3b7d71":"#Lets test the numeric transformer\nnumeric_pipe = Pipeline([('fs', feat_sel(dtype='numeric')),\n                         ('transf', tr_numeric())])\nfull_pipe = Pipeline([('gen_cl', general_cleaner()), ('num_pipe', numeric_pipe)])\n\ntmp = X_train_full.copy()\ntmp = full_pipe.fit_transform(tmp)\n\ntmp.head()","80163c53":"numeric_pipe = Pipeline([('fs', feat_sel(dtype='numeric')),  # select only the numeri features\n                         ('imputer', df_imputer(strategy='median')),  # impute the missing values with the median of each column\n                         ('transf', tr_numeric()),  # remove skew and create a new feature\n                         ('scl', df_scaler(method='standard'))])  # scale the data\n\nfull_pipe = Pipeline([('gen_cl', general_cleaner()), ('num_pipe', numeric_pipe)])  # put the cleaner on top because we like it clean","c9559044":"tmp = X_train_full.copy()\n\ntmp = full_pipe.fit_transform(tmp)\n\ntmp.head()","2fc1ca2a":"full_pipe.get_params()","1607d473":"h_car_categorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() > 10 and X_train_full[cname].dtype == \"object\"]\nh_car_categorical_cols","408e7fcd":"h_car_categorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() > 10 and X_train_full[cname].dtype == \"object\"]\n\ntmp = X_train_full.copy()\ngt = general_cleaner(drop_NA= True, fraction = 0.6)\ntmp = gt.fit_transform(tmp)\ntmp['Neighborhood'].value_counts()","da92f44c":"# Ordinal encoding\nordinal_cols = ['ExterQual', 'LotShape', 'BsmtQual', 'BsmtCond', 'BsmtExposure', \n 'BsmtFinType1', 'BsmtFinType2', 'MSSubClass', 'HeatingQC', 'Functional', 'FireplaceQu', 'KitchenQual',\n 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive']\n\nordinal_categories = [\n    #ExterQual\n    ['Po','Fa','TA','Gd','Ex'],\n    #LotShape\n    ['Reg','IR1' ,'IR2','IR3'],\n    #BsmtQual\n    ['None','Fa','TA','Gd','Ex'],\n    #BsmtCond\n    ['None','Po','Fa','TA','Gd','Ex'],\n    #BsmtExposure\n    ['None','No','Mn','Av','Gd'],\n    #BsmtFinType1\n    ['None','Unf','LwQ', 'Rec','BLQ','ALQ' , 'GLQ' ],\n    #BsmtFinType2\n    ['None','Unf','LwQ', 'Rec','BLQ','ALQ' , 'GLQ' ],\n    #msclass\n    ['20','30','40','45','50','60','70','75','80','85', '90','120','150','160','180','190'],\n    #HeatingQC\n    ['Po','Fa','TA','Gd','Ex'],\n    #Functional\n    ['Sev','Maj2','Maj1','Mod','Min2','Min1','Typ'],\n    #FireplaceQu\n    ['None','Po','Fa','TA','Gd','Ex'],\n    #KitchenQual\n    ['Fa','TA','Gd','Ex'],\n    #GarageFinish\n    ['None','Unf','RFn','Fin'],\n    #GarageQual\n    ['None','Po','Fa','TA','Gd','Ex'],\n    #GarageCond\n    ['None','Po','Fa','TA','Gd','Ex'],\n    #PavedDrive\n    ['N', 'P', 'Y']\n]\n\n#ordinal_dict = dict(zip(ordinal_cols, ordinal_categories))\n","b811c562":"class make_ordinal(BaseEstimator, TransformerMixin):\n    '''\n    Transforms ordinal features in order to have them as numeric (preserving the order)\n    \n    '''\n    def __init__(self, ordinal_cols, categories):\n        self.ordinal_cols = ordinal_cols\n        self.categories = categories\n        self.impute = None\n    \n\n    def fit(self, X, y=None):\n        self.impute = OrdinalEncoder(categories = self.categories)\n        self.impute.fit(X[self.ordinal_cols])\n        return self\n    \n\n    def transform(self, X, y=None):\n        Ximp = self.impute.transform(X[self.ordinal_cols])\n        Ximp_pd = pd.DataFrame(Ximp, index = X[self.ordinal_cols].index, columns = X[self.ordinal_cols].columns)\n        \n        X[self.ordinal_cols] = Ximp_pd\n        \n        return X","47bb3fbf":"#make sure it works\n\ncat_pipe = Pipeline([('fs', feat_sel(dtype='category')),\n                     ('imputer', df_imputer(strategy='most_frequent')), \n                     ('ord', make_ordinal(ordinal_cols = ordinal_cols, categories = ordinal_categories))\n                    ])\n\nfull_pipe = Pipeline([('gen_cl', general_cleaner()), ('cat_pipe', cat_pipe)])\n\ntmp = X_train_full.copy()\n\ntmp = full_pipe.fit_transform(tmp)\n\ntmp2 = X_test_full\n\ngh = full_pipe.transform(tmp2)\n\ngh.head()","c694728c":"cat_pipe = Pipeline([('fs', feat_sel(dtype='category')),\n                     ('imputer', df_imputer(strategy='most_frequent')), \n                     ('ord', make_ordinal(ordinal_cols = ordinal_cols, categories = ordinal_categories)),\n                     ('dummies', dummify())\n                    ])\n\nfull_pipe = Pipeline([('gen_cl', general_cleaner()), ('cat_pipe', cat_pipe)])\n\ntmp = X_train_full.copy()\n\ntmp = full_pipe.fit_transform(tmp)\n\ntmp.head()","2977ead1":"tmp = X_test_full.copy()\n\ntmp = full_pipe.transform(tmp)\n\ntmp.head()","b8fd8b64":"full_pipe.get_params()","5d12ddbe":"class FeatureUnion_df(TransformerMixin, BaseEstimator):\n    '''\n    Wrapper of FeatureUnion but returning a Dataframe, \n    the column order follows the concatenation done by FeatureUnion\n\n    transformer_list: list of Pipelines\n\n    '''\n    def __init__(self, transformer_list, n_jobs=None, transformer_weights=None, verbose=False):\n        self.transformer_list = transformer_list\n        self.n_jobs = n_jobs\n        self.transformer_weights = transformer_weights\n        self.verbose = verbose  # these are necessary to work inside of GridSearch or similar\n        self.feat_un = FeatureUnion(self.transformer_list, \n                                    self.n_jobs, \n                                    self.transformer_weights, \n                                    self.verbose)\n        \n    def fit(self, X, y=None):\n        self.feat_un.fit(X)\n        return self\n\n    def transform(self, X, y=None):\n        X_tr = self.feat_un.transform(X)\n        columns = []\n        \n        for trsnf in self.transformer_list:\n            cols = trsnf[1].steps[-1][1].get_features_name()  # getting the features name from the last step of each pipeline\n            columns += list(cols)\n\n        X_tr = pd.DataFrame(X_tr, index=X.index, columns=columns)\n        \n        return X_tr\n\n    def get_params(self, deep=True):  # necessary to well behave in GridSearch\n        return self.feat_un.get_params(deep=deep)","7f1d7845":"numeric_pipe = Pipeline([('fs', feat_sel('numeric')),\n                         ('imputer', df_imputer(strategy='median')),\n                         ('transf', tr_numeric())])\n\n\ncat_pipe = Pipeline([('fs', feat_sel('category')),\n                     ('imputer', df_imputer(strategy='most_frequent')), \n                     ('ord', make_ordinal(ordinal_cols = ordinal_cols, categories = ordinal_categories)), \n                     ('dummies', dummify())])\n\n\nprocessing_pipe = FeatureUnion_df(transformer_list=[('cat_pipe', cat_pipe),\n                                                 ('num_pipe', numeric_pipe)])\n\n\nfull_pipe = Pipeline([('gen_cl', general_cleaner()), \n                      ('processing', processing_pipe), \n                      ('scaler', df_scaler())])  # the scaler is here to have also the ordinal features scaled\n\ntmp = X_train_full.copy()\ntmp = full_pipe.fit_transform(tmp)\ntmp.head()","c4a1cb4c":"from sklearn.linear_model import LinearRegression, BayesianRidge\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import KFold, cross_val_score, GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor \nfrom sklearn.svm import SVR\n\n\nfolds = KFold(5, shuffle=True, random_state=541)\n","b459d7f5":"def grid_search(data, target, estimator, param_grid, scoring, cv):\n    \n    grid = GridSearchCV(estimator=estimator, param_grid=param_grid, \n                        cv=cv, scoring=scoring, n_jobs=-1, return_train_score=False)\n    \n    pd.options.mode.chained_assignment = None  # this is because the gridsearch throws a lot of pointless warnings\n    tmp = data.copy()\n    grid = grid.fit(tmp, target)\n    pd.options.mode.chained_assignment = 'warn'\n    \n    result = pd.DataFrame(grid.cv_results_).sort_values(by='mean_test_score', \n                                                        ascending=False).reset_index()\n    \n    del result['params']\n    times = [col for col in result.columns if col.endswith('_time')]\n    params = [col for col in result.columns if col.startswith('param_')]\n    \n    result = result[params + ['mean_test_score', 'std_test_score'] + times]\n    \n    return result, grid.best_params_","a4947e04":"SVR_full_pipe = Pipeline([('gen_cl', general_cleaner()), \n                      ('processing', processing_pipe), \n                      ('scaler', df_scaler()),\n                      ('SVR', SVR())]) \n\n\nParams_SVR= {'processing__num_pipe__imputer__strategy': ['mean', 'median'],\n         'processing__num_pipe__transf__eng_feat' : [True, False],\n         'processing__num_pipe__transf__fix_skews' : [True, False],\n         'processing__num_pipe__transf__years_old' : [True, False],\n         'processing__cat_pipe__dummies__drop_first': [True, False],\n         'SVR__gamma': ['auto', 'scale'],\n         'SVR__C': [0.1, 0.5, 1, 50, 100, 1000],\n         'SVR__epsilon': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10]\n              }\n\n\n\"\"\"res, bp = grid_search(X_train_full, y_train, SVR_full_pipe, \n            param_grid=Params_SVR,\n            cv=folds, scoring='neg_mean_squared_error')\"\"\"\n\nres","9e2b6008":"bp","4f940e38":"full_pipe.get_params()","6683e56c":"RFR_full_pipe = Pipeline([('gen_cl', general_cleaner()), \n                      ('processing', processing_pipe), \n                      ('scaler', df_scaler()),\n                      ('RFR', RandomForestRegressor())]) \n\nParams_RFR = {'processing__num_pipe__imputer__strategy': ['mean', 'median'],\n         'processing__num_pipe__transf__eng_feat' : [True, False],\n         'processing__num_pipe__transf__fix_skews' : [True, False],\n         'processing__num_pipe__transf__years_old' : [True, False],\n         'processing__cat_pipe__dummies__drop_first': [True, False],\n         'RFR__n_estimators': [100,200,300],\n         'RFR__criterion':['gini'],\n         'RFR__bootstrap': [True,False],\n         'RFR__max_depth': [3,5,10,20,50,75,100,None],\n         'RFR__max_features': ['auto','sqrt'],\n         'RFR__min_samples_leaf': [1,2,4,10],\n         'RFR__min_samples_split': [2,5,10]}\n\n\"\"\"res_1, bp_1 = grid_search(X_train_full, y_train, RFR_full_pipe, \n            param_grid=Params_RFR,\n            cv=folds, scoring='neg_mean_squared_error')\"\"\"","017f9ffc":"# Numeric Feature Pipeline\nWe can finally explicitly build our first pipeline. Ideally, we want it as follows\n\n1. Clean the data following the documentation\n2. Impute the missing values with the mean or the median (nothing stops us from using other types of imputations\n3. Apply some transformations on some features\n4. Create new features\n5. Scale the data\nWe already have every element but one, let's make a custom transformer","60aa8c96":"# Feature Engineering","47b92c1a":"The normal OneHotEncoding or the standard get_dummies would create a dataset with only 3 columns and, when a model is called, an error caused by the mismatch in shape. This custom dummifier takes care of it as follows","69a8e6da":"# Exploratory Data Analysis","e9bb237b":"**Short note: the custom dummifier**\nOne common issue one encounter when working with categorical features is that some categories may be very rare, resulting into a mismatch between the columns in the train and test (or validation set).\n\nFor this reason, the implementation here proposed takes care of the issue by creating or deleting any column that is not present in both sets. In other words, we assing the attribute columns when we first transform the training set and, when the transform method is called again, we check for missing or extra columns. If a column is missing, we add it with all 0's, otherwise we drop it.","33f70b9a":"I hope it is now evident why I kept implementing a get_features_name method in the previous classes. It was all for this moment.\n\nThe complete pipeline will then be","fbdfdcc0":"# Categorical Pipeline"}}