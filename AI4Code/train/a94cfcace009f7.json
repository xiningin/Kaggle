{"cell_type":{"e04437e6":"code","ae7106f9":"code","7ef6084f":"code","1f83cf7d":"code","0acaf364":"code","968ad650":"code","5ca39eb2":"code","54f6d088":"code","5b4a5493":"code","419ba69a":"code","ee10abc2":"code","050ccb5b":"code","a42e8bb4":"code","e8b91fd0":"code","1b2ce943":"code","f5acab7c":"code","628597bd":"code","bd931c28":"code","62635279":"code","744080e9":"code","8aa50daf":"code","d33c2d74":"code","96bb9740":"code","e047c92a":"code","6c45a9f3":"code","5c4d4e6a":"code","386db097":"code","63b69275":"code","08bdb240":"code","4c43033e":"code","9d70f17f":"code","5f77fe50":"code","4a1f5205":"code","2725b73a":"code","5fbcf61f":"code","1bc4c036":"code","f5c3d015":"markdown","4e817d35":"markdown","08d31e87":"markdown","11e836f3":"markdown","7fdcc002":"markdown","a29bed5e":"markdown","7c189467":"markdown","29eb5f77":"markdown","f3e74539":"markdown","03d92eb1":"markdown","9da21e8e":"markdown","941d5b07":"markdown","d7f7a353":"markdown","260ba686":"markdown","65993a80":"markdown","d10cbe50":"markdown","5ddaecc0":"markdown","ddefef79":"markdown","52a44c86":"markdown","051be89b":"markdown","9c7890a3":"markdown","1b6fb199":"markdown","e6a81add":"markdown","d6da7464":"markdown","825a64b4":"markdown","6b91fd89":"markdown","93cf7450":"markdown","b71f03da":"markdown","7b923ea3":"markdown","4ca39693":"markdown","01d84648":"markdown","7cd939ee":"markdown","49d79853":"markdown","348624a3":"markdown","c6174d38":"markdown","7bb9c87f":"markdown","322d48e9":"markdown","52887097":"markdown","96cf0d27":"markdown","d2197a5a":"markdown","6d8c1700":"markdown"},"source":{"e04437e6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.metrics import accuracy_score,f1_score,roc_auc_score,roc_curve,confusion_matrix,log_loss,precision_score,recall_score,auc\nfrom sklearn.utils import resample\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nimport plotly.figure_factory as ff\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ae7106f9":"data=pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\ndata.head()","7ef6084f":"print(data.shape)\ndata.columns","1f83cf7d":"data.describe()","0acaf364":"data.isnull().sum().sum()","968ad650":"plt.figure(figsize=(18,9))\nsns.heatmap(data.corr(),vmax = .8, square = True)\nplt.title(\"Correlation between features\")\nplt.show()","5ca39eb2":"print(data[\"Class\"].value_counts())\nplt.style.use('dark_background')\nsns.countplot(\"Class\",data=data,color=\"cyan\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Count\")\nplt.show()","54f6d088":"# Let's look at the Amount and Time distributions\nfig, ax = plt.subplots(1, 2, figsize=(20,10))\nsns.distplot(data[\"Amount\"],ax=ax[0],color=\"orange\")\nsns.distplot(data[\"Time\"],ax=ax[1],color=\"cyan\")\nax[0].set_title(\"Distribution of Transaction Amount\")\nax[1].set_title(\"Distribution of Transaction Time\")","5b4a5493":"fraud = data.loc[data['Class'] == 1]\nno_fraud = data.loc[data['Class'] == 0]\nfig, ax = plt.subplots(2, 1, figsize=(16,14))\nsns.scatterplot(fraud[\"Amount\"],fraud[\"Time\"],ax=ax[0],color=\"yellow\")\nax[0].set_title(\"Amount-Time Distrubition for Fraud\",fontsize=16)\nsns.scatterplot(no_fraud[\"Amount\"],no_fraud[\"Time\"],ax=ax[1],color=\"green\")\nax[1].set_title(\"Amount-Time Distrubition for No-Fraud\", fontsize=16)\nplt.show()","419ba69a":"#confusion matrix\ndef conf_matrix(actual, predicted):\n    plt.figure(figsize=(14,8))\n    cm = confusion_matrix(actual, predicted)\n    sns.heatmap(cm, xticklabels=['predicted_negative', 'predicted_positive'], \n                yticklabels=['true_negative', 'true_positive'], annot=True,\n                fmt='d', annot_kws={'fontsize':26}, cmap='Blues');\n    accuracy = accuracy_score(actual,predicted)\n    precision = precision_score(actual,predicted)\n    recall = recall_score(actual,predicted)\n    f1 = f1_score(actual,predicted)\n\n    cm_results = {\"accuracy\":accuracy, \"precision\":precision, \"recall\":recall, \"f1\":f1}\n    return cm_results\n\n\n#ROC Curve\ndef plot_roc_curve(model,X,y):\n    plt.figure(figsize=(14,8))\n    y_pred_prob = model.predict_proba(X)[:,1]\n    fpr, tpr, thresholds = roc_curve(y, y_pred_prob)\n    plt.plot([0, 1], [0, 1], 'k--',color=\"red\")\n    plt.plot(fpr, tpr,label='ROC curve (area = %0.2f)' % roc_auc_score(y,y_pred_prob))\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()  \n    \n    \n#Precision-Recall Curve\nfrom sklearn.metrics import precision_recall_curve\ndef plot_pre_recall(model,X,y):\n    probs=model.predict_proba(X)[:,1]\n    precision, recall, thresholds = precision_recall_curve(y, probs)\n    auc_score = auc(recall, precision) \n    plt.figure(figsize=(16,8))\n    plt.plot([0, 1], [0.5, 0.5], linestyle='--')\n    plt.plot(recall, precision, marker='.')\n    plt.title('Precision-Recall Curve')\n    plt.show()\n    print('AUC: %.3f' % auc_score)\n    ","ee10abc2":"eps=0.001\ndata['Amount'] = np.log(data.pop('Amount')+eps)\nX=data.drop(\"Class\",1)\ny=data[\"Class\"]\nX_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.3,random_state=42)\nfrom sklearn.preprocessing import RobustScaler\nrbst=RobustScaler()\nX_train=rbst.fit_transform(X_train)\nX_test=rbst.transform(X_test)","050ccb5b":"logreg = LogisticRegression().fit(X_train, y_train)\nlog_pred = logreg.predict(X_test)\nprint(\"Accuracy score of Logistic Regression on Imbalanced Data:\",accuracy_score(y_test,log_pred))\nprint(classification_report(y_test,log_pred))\nconf_matrix(y_test,log_pred)","a42e8bb4":"plot_roc_curve(logreg,X_test,y_test)","e8b91fd0":"plot_pre_recall(logreg,X_test,y_test)","1b2ce943":"dectree=DecisionTreeClassifier().fit(X_train,y_train)\ndec_pred=dectree.predict(X_test)\nprint(\"Accuracy score of Decision Tree Classifier on Imbalanced Data:\",accuracy_score(y_test,dec_pred))\nprint(classification_report(y_test,dec_pred))\nconf_matrix(y_test,dec_pred)\n","f5acab7c":"plot_roc_curve(dectree,X=X_test,y=y_test)","628597bd":"plot_pre_recall(dectree,X_test,y_test)","bd931c28":"data_majority = data[data.Class==0]\ndata_minority = data[data.Class==1]\n \n# Upsample minority class\ndata_major_downsampled = resample(data_majority, \n                                 replace=True, \n                                 n_samples=data_minority.shape[0], \n                                 random_state=123)\n\ndata_downsampled = pd.concat([data_major_downsampled, data_minority])\n \n# Display new class counts\ndata_downsampled.Class.value_counts()","62635279":"y_down = data_downsampled.Class\nX_down = data_downsampled.drop('Class', axis=1)\nX_train_down,X_test_down,y_train_down,y_test_down=train_test_split(X_down,y_down,test_size=0.3,random_state=0)\nX_train_down=rbst.fit_transform(X_train_down)\nX_test_down=rbst.transform(X_test_down)\nlogreg_down = LogisticRegression().fit(X_train_down, y_train_down)\nlog_pred_down = logreg_down.predict(X_test_down)\n\nprint(\"Accuracy score of Logistic Regression on Down-Sampling Data:\", accuracy_score(y_test_down, log_pred_down) )\nprint(classification_report(y_test_down,log_pred_down))\nconf_matrix(y_test_down, log_pred_down)","744080e9":"plot_roc_curve(logreg_down,X_test_down,y_test_down)","8aa50daf":"plot_pre_recall(logreg_down,X_test_down,y_test_down)","d33c2d74":"dectree_down=DecisionTreeClassifier()\ndectree_down.fit(X_train_down, y_train_down)\ndec_pred_down=dectree_down.predict(X_test_down)\nprint(\"Accuracy score of Decision Tree Classifier on DOWN-SAMPLING Data:\",accuracy_score(y_test_down,dec_pred_down))\nprint(classification_report(y_test_down,dec_pred_down))\nprint(\"roc auc score of Decision Tree Classifier:\",roc_auc_score(y_test_down,dec_pred_down))\nconf_matrix(y_test_down,dec_pred_down)\n","96bb9740":"plot_roc_curve(dectree_down,X_test_down,y_test_down)","e047c92a":"plot_pre_recall(dectree_down, X_test_down,y_test_down)","6c45a9f3":"from imblearn.over_sampling import SMOTE\noversample = SMOTE(sampling_strategy='auto', k_neighbors=6, random_state=42)\nX_smt, y_smt = oversample.fit_resample(X, y)\n\nX_train_smt,X_test_smt,y_train_smt,y_test_smt=train_test_split(X_smt,y_smt,test_size=0.3,random_state=42)\nX_train_smt=rbst.fit_transform(X_train_smt)\nX_test_smt=rbst.transform(X_test_smt)\n\nlogreg_smote=LogisticRegression()\nlogreg_smote.fit(X_train_smt,y_train_smt)\nlogreg_smt_pred=logreg_smote.predict(X_test_smt)\nprint(\"Accuracy score of Logistic Regression on SMOTE technique:\",accuracy_score(y_test_smt,logreg_smt_pred))\nprint(classification_report(y_test_smt,logreg_smt_pred))\nconf_matrix(y_test_smt,logreg_smt_pred)","5c4d4e6a":"plot_roc_curve(logreg_smote,X_test_smt,y_test_smt)","386db097":"plot_pre_recall(logreg_smote, X_test_smt,y_test_smt)","63b69275":"dectree_smote=DecisionTreeClassifier()\ndectree_smote.fit(X_train_smt, y_train_smt)\ndec_pred_sm=dectree_smote.predict(X_test_smt)\nprint(\"Accuracy score of Decision Tree Classifier on SMOTE technique:\",accuracy_score(y_test_smt,dec_pred_sm))\nprint(classification_report(y_test_smt,dec_pred_sm))\nprint(\"roc auc score of Decision Tree Classifier:\",roc_auc_score(y_test_smt,dec_pred_sm))\nconf_matrix(y_test_smt,dec_pred_sm)\n","08bdb240":"plot_roc_curve(dectree_smote,X_test_smt,y_test_smt)","4c43033e":"plot_pre_recall(dectree_smote, X_test_smt,y_test_smt)","9d70f17f":"#Since we will use the existing unprocessed data set, we will use the first X_train_im, y_train_im, X_test_im, y_test_im sets we created.\nrf=RandomForestClassifier()\nrf.fit(X_train,y_train)\nrf_pred=rf.predict(X_test)\nprint(\"Accuracy score of Random Forest Classifier on Imbalanced Data:\",accuracy_score(y_test,rf_pred))\nprint(classification_report(y_test,rf_pred))\nconf_matrix(y_test,rf_pred)\n","5f77fe50":"plot_roc_curve(rf,X_test,y_test)","4a1f5205":"plot_pre_recall(rf,X_test,y_test)","2725b73a":"from xgboost import XGBClassifier\nxgb = XGBClassifier(scale_pos_weight=578) # SumofMajority\/SumofMinority\nxgb.fit(X_train, y_train)\nxgb_pred=xgb.predict(X_test)\nprint(\"Accuracy score of XGB on Imbalanced Data:\",accuracy_score(y_test,xgb_pred))\nprint(classification_report(y_test,xgb_pred))\nconf_matrix(y_test,xgb_pred)","5fbcf61f":"plot_roc_curve(xgb,X_test,y_test)","1bc4c036":"plot_pre_recall(xgb,X_test,y_test)","f5c3d015":"#### In order to observe much better, before applying some of these techniques, let's create a model in its unbalanced form and look at its results.\n#### Let's continue our way by separating our data as train-test. Thus, we can make more accurate decisions.\n","4e817d35":"Let's look at the number of rows and columns of the data set and column names","08d31e87":"## <span style=\"color:green\">Application of DOWN-SAMPLING technique with Decision Tree Classifier<\/span>","11e836f3":"### The reason I show the Roc-curve graph is its relation to the precision-recall curve. Note that the precision-recall curve shows better results in unbalanced data sets. Don't let the Roc-curve mislead you.","7fdcc002":"### New scores after applying the SMOTE technique seem satisfactory. Let's look at the roc_curve and precision_recall chart","a29bed5e":"# <center> <span style=\"color:green\"> CONCLUSION <\/span> <\/center>","7c189467":"# <center><span style=\"color:green\">INTRODUCTION TO IMBALANCED DATASET AND ANALYSIS<\/span><\/center>","29eb5f77":"### We got much better results after downsampling. Along with accuracy, recall, precision, f1-score and auc score performed much better than the model's unstable data analysis.","f3e74539":"## <span style=\"color:green\">DOWN-SAMPLING TECHNIQUE <\/span>","03d92eb1":"## <span style=\"color:green\">SMOTE TECHNIQUE <\/span>\nLet's remember the SMOTE technique again:\n#### SMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line.Specifically, a random example from the minority class is first chosen. Then k of the nearest neighbors for that example are found (typically k=5). A randomly selected neighbor is chosen and a synthetic example is created at a randomly selected point between the two examples in feature space.\n\n> ### SMOTE first selects a minority class instance a at random and finds its k nearest minority class neighbors. The synthetic instance is then created by choosing one of the k nearest neighbors b at random and connecting a and b to form a line segment in the feature space. The synthetic instances are generated as a convex combination of the two chosen instances a and b. \n\nAdvantages:\n* Mitigates the problem of overfitting caused by random oversampling as synthetic examples are generated rather than replication of instances\n* No loss of useful information\n\nDisadvantages:\n* While generating synthetic examples SMOTE does not take into consideration neighboring examples from other classes. This can result in increase in overlapping of classes and can introduce additional noise\n* SMOTE is not very effective for high dimensional data\n","9da21e8e":"### The Auc score shows us the success of the technique and model. Never forget this:\n> ### <i>The Precision-Recall Plot Is More Informative than the ROC Plot When Evaluating Binary Classifiers on Imbalanced Datasets<\/i>","941d5b07":"## Additional analysis: <span style=\"color:green\">Analyze the imbalanced dataset using the Random Forests Classifier<\/span>","d7f7a353":"### <span style=\"color:red\">==> Loading Dataset<\/span>","260ba686":"### <span style=\"color:green\">Logistic Regression on Imbalanced Dataset<\/span>","65993a80":"### <span style=\"color:green\">Decision Tree Classifier on Imbalanced Dataset<\/span>","d10cbe50":"## <span style=\"color:green\">XGBoost for Imbalanced Classification<\/span>\n#### The stochastic gradient boosting algorithm, also called gradient boosting machines or tree boosting, is a powerful machine learning technique that performs well or even best on a wide range of challenging machine learning problems.XGBoost provides a highly efficient implementation of the stochastic gradient boosting algorithm and access to a suite of model hyperparameters designed to provide control over the model training process. XGBoost is an effective machine learning model, even on datasets where the class distribution is skewed.Before any modification or tuning is made to the XGBoost algorithm for imbalanced classification, it is important to test the default XGBoost model and establish a baseline in performance.\n> ### The implementation provides a hyperparameter designed to tune the behavior of the algorithm for imbalanced classification problems; this is the <span style=\"color:blue\">scale_pos_weight<\/span> hyperparameter.A sensible default value to set for the scale_pos_weight hyperparameter is the inverse of the class distribution. For example, for a dataset with a 1 to 100 ratio for examples in the minority to majority classes, the scale_pos_weight can be set to 100. This will give classification errors made by the model on the minority class (positive class) 100 times more impact, and in turn, 100 times more correction than errors made on the majority class.The XGBoost documentation suggests a fast way to estimate this value using the training dataset as the total number of examples in the majority class divided by the total number of examples in the minority class.\nhttps:\/\/machinelearningmastery.com\/xgboost-for-imbalanced-classification\/","5ddaecc0":"### Current tactic use tree-based algorithms. Decision trees often perform well on imbalanced datasets because their hierarchical structure allows them to learn signals from both classes.\n> ### I'll show you random forests last. You will find that we get very effective results for the unbalanced data set. Stay curious ..","ddefef79":"### <span style=\"color:green\">Application of SMOTE technique with Logistic Regression<\/span>","52a44c86":"### <span style=\"color:green\">Application of DOWN-SAMPLING technique with Logistic Regression<\/span>","051be89b":"## <span style=\"color:green\">Penalize Algorithms (Cost-Sensitive Training)<\/span>\n#### The next tactic is to use penalized learning algorithms that increase the cost of classification mistakes on the minority class.A popular algorithm for this technique is Penalized-XGB","9c7890a3":"#### Looking at the results, we can see the success of this technique. With the Decision Tree Classifier, we got much better results than Logistic Regression. We see that there are very close scores. Let's look at the roc-curve and precision_recall_curve chart and interpret the result","1b6fb199":"#### Do you have NaN values?","e6a81add":"### <span style=\"color:green\">ROC Curve vs. Precision-Recall Curve with imbalenced data:<\/span>","d6da7464":"#### As it is seen, while the accuracy is 99%, our other important metrics f1-score, precision, recall and roc-auc score did not give a consistent result for this accuracy. This is the \"ACCURACY PARADOX\" that is meant to be explained in full. Along with accuracy, these metrics should have been consistent.Decision tree classifiers gave better results than logistic regression, but attention should be paid to the inconsistency of other metrics with accuracy.\nLet's look at two techniques that are methods of dealing with this. SMOTE, Down-Sampling and penalized model\n\n###  <span style=\"color:red\">Technical 1:  Random Down-Sampling:<\/span>\n#### Down-sampling involves randomly removing observations from the majority class to prevent its signal from dominating the learning algorithm.The most common heuristic for doing so is resampling without replacement.\n\nHere are the steps:\n* First, we'll separate observations from each class into different DataFrames.\n* Next, we'll resample the majority class without replacement, setting the number of samples to match that of the minority class.\n* Finally, we'll combine the down-sampled majority class DataFrame with the original minority class DataFrame.\n\n### <span style=\"color:red\">Technical 2: Synthetic Minority Over-sampling Technique (SMOTE):<\/span>\n#### SMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line.Specifically, a random example from the minority class is first chosen. Then k of the nearest neighbors for that example are found (typically k=5). A randomly selected neighbor is chosen and a synthetic example is created at a randomly selected point between the two examples in feature space.\n\n> ### SMOTE first selects a minority class instance a at random and finds its k nearest minority class neighbors. The synthetic instance is then created by choosing one of the k nearest neighbors b at random and connecting a and b to form a line segment in the feature space. The synthetic instances are generated as a convex combination of the two chosen instances a and b.","825a64b4":"> ### The results of the RF algorithm are not bad without applying any technique. As we mentioned above, tree-based models can give successful results in such datasets. Never forget this alternative.","6b91fd89":"### <span style=\"color:green\">Application of SMOTE technique with Decision Tree Classifier<\/span>","93cf7450":"## <span style=\"color:red\">Techniques for Imbalanced Classes:<\/span>\n- Selecting performance metrics, such as those that focus on the minority class.\n- Selecting data preparation methods, such as those that attempt to re-balance the classes (Upsampling & Downsampling)\n  \n- Selecting classification algorithms, such as those that penalize misclassification errors differently.\n- Synthetic Minority Oversampling Technique (SMOTE).\n\nWe will prefer using SMOTE and Random Down-Sampling\n\n#### https:\/\/machinelearningmastery.com\/imbalanced-classification-with-python\/\nFirst let's explain these terms\n#### Wow, incredible accuracy. Okay, you shouldn't say it's enough for me. Never say that you will use this model, this accuracy is very misleading. I heard why he said misleading. Accuracy is never enough to evaluate a model. So when we look at the Precision, Recall and F1 scores of the fraud situation from the Confusion Matrix table, we will see that the model output is actually insufficient.\n\n#### First, let's explain the following terms:\n* True Positive (TP) \u2013 You predicted positive and it\u2019s true.You predicted that a woman is pregnant and she actually is.\n* True Negative (TN) \u2013 You predicted negative and it\u2019s true.You predicted that a man is not pregnant and he actually is not.\n* False Positive (FP)(Type 1 Error) \u2013 You predicted positive and it\u2019s false.You predicted that a man is pregnant but he actually is not.\n* False Negative (FN)(Type 2 Error) \u2013 You predicted negative and it\u2019s false.You predicted that a woman is not pregnant but she actually is.\n\n#### <span style=\"color:green\">What is Confusion Matrix <\/span>Well, it is a performance measurement for machine learning classification problem where output can be two or more classes. It is a table with 4 different combinations of predicted and actual values.\n![](https:\/\/miro.medium.com\/max\/924\/1*7EYylA6XlXSGBCF77j_rOA.png)\nhttps:\/\/towardsdatascience.com\/understanding-confusion-matrix-a9ad42dcfd62\n#### <span style=\"color:green\">Precision:<\/span> Precision is the number of True Positives divided by the number of True Positives and False Positives. Put another way; it is the number of positive predictions divided by the total number of positive class values predicted. It is also called the Positive Predictive Value (PPV). Precision can be thought of as a measure of a classifier's exactness. A low precision can also indicate a large number of False Positives.\n\n#### <span style=\"color:green\">Recall:<\/span> Recall is the number of True Positives divided by the number of True Positives and the number of False Negatives. Put another way it is the number of positive predictions divided by the number of positive class values in the test data. It is also called Sensitivity or the True Positive Rate.Recall can be thought of as a measure of a classifier's completeness. A low recall indicates many False Negatives.It is extremely useful for measuring Recall, Precision, Specificity, Accuracy and most importantly AUC-ROC Curve.\nLet\u2019s understand TP, FP, FN, TN in terms of pregnancy analogy.\n\n\n#### <span style=\"color:green\">F1-Score<\/span> is the 2*((precision x recall)\/(precision+recall)).Put another way, the F1 score conveys the balance between the precision and the recall.\n#### <span style=\"color:green\">ROC Curve:<\/span>AUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease. The ROC curve is plotted with TPR against the FPR where TPR is on y-axis and FPR is on the x-axis.An excellent model has AUC near to the 1 which means it has good measure of separability. A poor model has AUC near to the 0 which means it has worst measure of separability. In fact it means it is reciprocating the result. It is predicting 0s as 1s and 1s as 0s. And when AUC is 0.5, it means model has no class separation capacity whatsoever.\n\n#### <span style=\"color:green\">Precision-Recall Curve:<\/span>As shown before when one has imbalanced classes, precision and recall are better metrics than accuracy, in the same way, for imbalanced classes a Precision-Recall curve is more suitable than a ROC curve.A Precision-Recall curve is a plot of the Precision (y-axis) and the Recall (x-axis) for different thresholds, much like the ROC curve. Note that in computing precision and recall there is never a use of the true negatives, these measures only consider correct predictions. Reviewing both precision and recall is useful in cases where there is an imbalance in the observations between the two classes. Specifically, there are many examples of no event (class 0) and only a few examples of an event (class 1).\n","b71f03da":"### This time, the new DataFrame has fewer observations than the original, and the ratio of the two classes is now 1:1.","7b923ea3":"#### I hear what you're saying. This is <span style=\"color:blue\">IMBALANCED CLASS.<\/span> What is Imbalanced Classification? Let's explain\n#### Classification modeling involves assigning a class tag to a sample. For example, binary classification. They take the values \u200b\u200b0 or 1. There are also classification labels such as Yes \/ No. We always set them to 0-1. Imbalanced classification problems are uneven distribution of samples.Class distribution is so unstable that for every value in the minority class there may be hundreds or even thousands of sample differences in the majority class. Considering this data set, there are 284315 for 0 and only 492 for 1.\n#### Imbalanced classification problem occurs everywhere. For example:\n- Fraud Detection.\n- Claim Prediction\n- Churn Prediction.\n- Spam Detection.\n- Anomaly Detection.\n- Outlier Detection.\n- Intrusion Detection\n- Conversion Prediction.\n#### https:\/\/machinelearningmastery.com\/imbalanced-classification-with-python\/\n#### For such distributions, the machine learning algorithms used for classification pose a challenge for predictive modeling, as they are designed around an equal number of sample assumptions for each class.\n#### Since the majority class is more important in these models, the minority class is more sensitive to error. This results in models with low predictive performance, especially for the minority class.When we examine the models in more depth and find 90% accuracy, we realize that the truth is not so. \n### So what are we gonna do? Do you have a solution? Let me relax you. Come on down.","4ca39693":"#### Good news. We don't have null values. At least we won't deal with it.\n#### Okeey. Now let's look at the correlations first. We will do this with heatmap.","01d84648":"### Logistic regression gave a better result than decision tree classifier, but still not an adequate score compared to the auc score.We need to deal with this data set until auc makes the problem acceptable.","7cd939ee":"### You are working on your dataset. You create a classification model and immediately get 90% accuracy. Isn't this result really great? We go a little further and discover that almost all of the data belongs to a class. How can we say this? Imbalanced data can hurt our heads in this case.\n### I can predict the facial expression that occurs when you discover the imbalance.\n### This dataset is an example of the situation of an imbalanced dataset and the annoying results it can cause.\n### In this notebook, we will explore techniques that we can use to achieve excellent but truly excellent results in our dataset containing imbalanced data. In particular we will consider:\n-  What does imbalanced data mean?\n-  What is the paradox of truth?\n-  What are the various approaches to the processing of imbalanced data?","49d79853":"##  <span style=\"color:red\">==> Import libraries<\/span>","348624a3":"#### After that, we have to look at the distribution of our class and evaluate. We need to see if there is an imbalanced distribution or not. If there is an imbalanced distribution, we need to put more emphasis on this situation. Which is very important. Let's check and draw our way. So just accuracy is very dangerously misleading.","c6174d38":"> ### <span style=\"color:red\">Important Note:<\/span> If you have an imbalanced dataset accuracy can give you false assumptions regarding the classifier\u2019s performance, it\u2019s better to rely on precision and recall, in the same way a Precision-Recall curve is better to calibrate the probability threshold in an imbalanced class scenario as a ROC curve.\n> ### ROC Curves: summarise the trade-off between the true positive rate and false positive rate for a predictive model using different probability thresholds.\n> ### Precision-Recall curves: summarise the trade-off between the true positive rate and the positive predictive value for a predictive model using different probability thresholds.\n> ### ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets. In both cases the area under the curve (AUC) can be used as a summary of the model performance.\n> ### ROC curves present an optimistic picture of the model on datasets with a class imbalance.\nhttps:\/\/machinelearningmastery.com\/roc-curves-and-precision-recall-curves-for-classification-in-python\/\n\n> ### <i>As we have seen here, the auc score is quite small compared to accuracy (78.9%). The roc-auc score seems to be more successful, but it would not be correct to use the roc-auc score in unstable classes. We will deal with the precision-recall curve and comment on the auc score.<\/i>\n","7bb9c87f":"### We achieved the most successful scores ever with SMOTE technique. There are several options of the SMOTE technique. We explained the most commonly used technique here. If you want to see the others, I leave a link below.\nhttps:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/generated\/imblearn.over_sampling.SMOTE.html","322d48e9":"# <span style=\"color:orange\"> I tried to show how we can work with unbalanced datasets using the 4 most popular methods. If you found it successful, please do not withhold your support. If there is an error or missing place, please feel free to appear. Thanks for coming here ...<\/span>","52887097":"####  This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile). Where the goal is to scale properties using statistics that are resistant to outliers. However, outliers can often influence the sample mean \/ variance in a negative way. In such cases, the median and the interquartile range often give better results. So we will use Robustscaler","96cf0d27":"#### <span style=\"color:red\">Important Note:<\/span> Our last tactic will be to use tree-based algorithms.I have already shown it above through the Decision Tree Classifier so that you can better understand this section above, but it is necessary to better understand its importance. Actually, I wanted to show you the performance of the Decision Tree Classifier step by step above. This is because modern applied machine learning algorithms (Random Forests, Gradient Boosted Tree etc ..) always outperform individual decision trees.\n","d2197a5a":"#### What do summary statistics look like?","6d8c1700":"#### First of all, I want to look at the distribution of Amount and Time variables and scale them. By scaling, we take the data into a smaller range and evaluate it."}}