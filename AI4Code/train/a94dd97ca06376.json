{"cell_type":{"d0d2ed4d":"code","36345366":"code","2204d357":"code","f12c0d99":"code","2ef65342":"code","af2eed0c":"code","257c2ee5":"code","94ed321d":"code","c7a15fdb":"code","efe0954e":"code","78567fcf":"code","8d91495d":"markdown","28927d7c":"markdown","43d0a85d":"markdown","d286f2d6":"markdown","06530175":"markdown"},"source":{"d0d2ed4d":"import numpy as np\nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","36345366":"import torch\nimport torchvision\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split","2204d357":"tran = torchvision.transforms.Compose(\n    [ torchvision.transforms.Resize((32,32)),\n      torchvision.transforms.ToTensor(),\n      transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) ])\ndata_train = torchvision.datasets.ImageFolder(\nroot='\/kaggle\/input\/cell-images-for-detecting-malaria\/cell_images\/cell_images',\ntransform=tran\n)","f12c0d99":"len(data_train)","2ef65342":"batch_size = 256\nsubset_train, subset_val, subset_test = torch.utils.data.random_split(data_train, [19000, 5000, 3558])\niter_train = torch.utils.data.DataLoader(subset_train, batch_size, shuffle=True, num_workers=2)\niter_val = torch.utils.data.DataLoader(subset_val, batch_size, shuffle=True, num_workers=2)\niter_test = torch.utils.data.DataLoader(subset_test, batch_size, shuffle=True, num_workers=2)","af2eed0c":"import torch.nn as nn\nimport torch.nn.functional as F\nclass Net(torch.nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 72)\n        self.fc2 = nn.Linear(72, 12 )\n        self.fc3 = nn.Linear(12, 2)\n    \n    def forward(self, x):\n        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x","257c2ee5":"net = Net()\nnet\nparams = list(net.parameters())\nprint(len(params))\nprint(params[0].size()) ","94ed321d":"all_acc = []\nhist1 = []\nhist2 = []\nloss = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\nfor epoch in range(18):\n    train_loss = 0\n    net.train()\n    for X, y in iter_train:\n        optimizer.zero_grad()\n        l = loss(net(X), y)\n        l.backward()\n        optimizer.step()\n        train_loss += l.item()\n    train_loss \/= len(iter_train)\n    \n    val_loss = 0\n    val_acc = 0\n    net.eval()\n    for X, y in iter_val:\n        with torch.no_grad():\n            y_hat = net(X)\n        val_loss += loss(y_hat, y).item()\n        val_acc += ((y_hat.argmax(axis=1) == y).sum() \/ y.shape[0]).item()\n    val_loss \/= len(iter_val)\n    val_acc \/= len(iter_val)\n    all_acc.append(np.mean(val_acc))\n    hist1.append(np.mean(val_loss))\n    hist2.append(np.mean(train_loss))\n    print(f'Epoch #{epoch + 1:02d}: [train_loss={train_loss:.05f}][ val_loss={val_loss:.05f}][ val_acc={val_acc:.05f}]\\n')","c7a15fdb":"plt.plot(hist1, 'b', label ='val loss')\nplt.plot(hist2, 'y', label ='train loss')\nplt.legend\nplt.xlabel('Epoch', fontsize=10, color='black')\nplt.grid(True)\nplt.legend(loc='upper right')","efe0954e":"plt.plot(all_acc)\nplt.xlabel('Epoch', fontsize=10, color='black')\nplt.ylabel('accuracy', fontsize=10, color='black')\nplt.grid(True)","78567fcf":"from sklearn.metrics import accuracy_score\n\nnet.eval()\n\ny_pred, y_true = [], []\nfor X, y in iter_test:\n    with torch.no_grad():\n        y_hat = net(X)\n    preds = y_hat.max(dim=1)[1]\n    y_pred.extend(preds.numpy())\n    y_true.extend(y.numpy())\n\nprint('Test accuracy:', accuracy_score(y_true, y_pred))","8d91495d":"*\u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435*","28927d7c":"*\u0418\u0442\u043e\u0433\u043e\u0432\u0430\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u0430\u044f \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c*","43d0a85d":"*\u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430\n\u041e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439\n\u0422\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f \u0432 \u0422\u0435\u043d\u0437\u043e\u0440\u044b*","d286f2d6":"*\u041c\u043e\u0434\u0435\u043b\u044c*","06530175":"*\u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0432\u044b\u0431\u043e\u0440\u043e\u043a*"}}