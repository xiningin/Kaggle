{"cell_type":{"a26a758a":"code","3d1f4c1a":"code","48571cec":"code","eb9407a3":"code","24acf5d6":"code","d023a648":"code","cc272bae":"code","63bb74ca":"code","a39bc1f1":"code","c0c24e1f":"code","91b2ac89":"code","b6551b96":"code","a3391622":"code","8db32bd3":"code","0ad43fee":"code","ec07d282":"code","3e54b3f1":"code","62a14001":"code","ed43a11b":"markdown","e8f0d24c":"markdown","501482cc":"markdown","dbb25a22":"markdown","eea213d3":"markdown","33ce23e6":"markdown","45acce08":"markdown","563cbffb":"markdown","26d4b43e":"markdown","cde2f5d0":"markdown","543da9fe":"markdown","39f3b2ff":"markdown","4f071c2d":"markdown","f3ac6e08":"markdown","ae60a801":"markdown","cfb0228f":"markdown","b1e4ae29":"markdown"},"source":{"a26a758a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport spacy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\nseed = 100","3d1f4c1a":"#path = 'dataset\/'\npath = '..\/input\/'\nnews = pd.read_json(path+ 'Sarcasm_Headlines_Dataset.json',lines=True)\nnews.head()","48571cec":"news['num_words'] = news['headline'].apply(lambda x: len(str(x).split()))\nprint('Maximum number of word',news['num_words'].max())\n\nprint('\\nSentence:\\n',news[news['num_words'] == 39]['headline'].values)\ntext = news[news['num_words'] == 39]['headline'].values","eb9407a3":"# Word tokenize\nnlp = spacy.load('en')\ndoc = nlp(text[0])\n\n# List compresion method to get tokens\ntoken = [w.text for w in doc ]\nprint(token)","24acf5d6":"# Data preprocessing\n# Remove punctuation\nprint('Quotes:',spacy.lang.punctuation.LIST_QUOTES)\nprint('\\nPunctuations:',spacy.lang.punctuation.LIST_PUNCT)\n#print('\\n Currency:',spacy.lang.punctuation.LIST_CURRENCY)\n\n# list of punctuation contains most of punctuation, we will use only that for our analysis\npunc = [w.text for w in doc  if  w.is_punct ]\nprint('\\nPunctuation:',punc)","d023a648":"stopwords = list(spacy.lang.en.stop_words.STOP_WORDS)\nprint('Number of stopwords is','-'*20,len(stopwords))\nprint('Ten stop words',list(stopwords)[:10])\nstop = [w.text for w in doc if w.is_stop]\nprint('*'*100,'\\n\\nStop word in sentence: ',stop)","cc272bae":"digit = [w.text for w in doc if w.is_digit]\nprint('Digit in sentence: ',digit)","63bb74ca":"lemma = [w.lemma_ for w in doc]\nprint(lemma)","a39bc1f1":"spacy.displacy.render(doc, style='ent', jupyter=True)","c0c24e1f":"df = pd.DataFrame(\n{\n    'token': [w.text for w in doc],\n    'lemma':[w.lemma_ for w in doc],\n    'POS': [w.pos_ for w in doc],\n    'TAG': [w.tag_ for w in doc],\n    'DEP': [w.dep_ for w in doc],\n    'is_stopword': [w.is_stop for w in doc],\n    'is_punctuation': [w.is_punct for w in doc],\n    'is_digit': [w.is_digit for w in doc],\n})\n\ndef highlight_True(s):\n    \"\"\"\n    Highlight True and False\n    \"\"\"\n    return ['background-color: yellow' if v else '' for v in s]\ndf.style.apply(highlight_True,subset=['is_stopword', 'is_punctuation', 'is_digit'])","91b2ac89":"def clean_text(df):\n    \"\"\"\n    Text preprocessing:\n    tokenize, make lower case,\n    Remove Stop word, punctuation, digit\n    lemmatize\n    \"\"\"\n    nlp = spacy.load('en')\n    for i in range(df.shape[0]):\n        doc = nlp(df['headline'][i])\n        # Word Tokenize\n        #token = [w.text for w in doc]\n        \n        # Make Lower case\n        # Remove Stop word, punctuation, digit and lemmatize\n        text = [w.lemma_.lower().strip() for w in doc \n               if not (w.is_stop |\n                    w.is_punct |\n                    w.is_digit)\n               ]\n        text = \" \".join(text)\n        \n        if i <5: print('Sentence:',i,text)\n        df['headline'][i] = text\n    return df","b6551b96":"news_df = clean_text(news)","a3391622":"sns.countplot(news['is_sarcastic'])","8db32bd3":"tf = TfidfVectorizer(analyzer='word',ngram_range=(1,3),max_features=5000)\nX = tf.fit_transform(news_df['headline'])","0ad43fee":"y = news_df['is_sarcastic']\nX_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size=0.3, random_state=seed)","ec07d282":"nb = BernoulliNB()\nnb.fit(X_train,y_train)","3e54b3f1":"pred = nb.predict(X_valid)\nprint('Confusion matrix\\n',confusion_matrix(y_valid,pred))\nprint('Classification_report\\n',classification_report(y_valid,pred))","62a14001":"proba = nb.predict_proba(X_valid)[:,1]\nfpr,tpr, threshold = roc_curve(y_valid,proba)\nauc_val = auc(fpr,tpr)\n\nplt.figure(figsize=(14,8))\nplt.title('Reciever Operating Charactaristics')\nplt.plot(fpr,tpr,'b',label = 'AUC = %0.2f' % auc_val)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')","ed43a11b":"### Punctuation\nSpacy library contains different punctuations, such as **Quotes, currency, punctuation** ect,\nIn above sentence we have seen inveted comma punctuation in the sentence and it will be considered as new word tocken, which is not usefull for our analysis. So we will remove that punctuation from sentence.","e8f0d24c":"### Tfidf vectorizer","501482cc":"## Text preprocessing","dbb25a22":"### Data analysis","eea213d3":"### Stopword\nIn this step we will remove stop words in dataset","33ce23e6":"### Target valriable distribution","45acce08":"### Model Selection","563cbffb":"## Import dataset","26d4b43e":"## Model Evaluvation","cde2f5d0":"### Named Entities\nA named entity is a \"real-world object\" that's assigned a name \u2013 for example, a person, a country, a product or a book title.","543da9fe":"### Word tokenize\nA sentence or data split into words is called word tokenize","39f3b2ff":"### Reciever Operating Charactaristics","4f071c2d":"### Digit","f3ac6e08":"## News headline","ae60a801":"### Thank you","cfb0228f":"### Lemmatizing\nLemmetiztion is the process of retrieving the root word of the current word. Lemmatization is an essential process in NLP to bring different variants of a single word to one root word.","b1e4ae29":"### Model\nNaive bayes model"}}