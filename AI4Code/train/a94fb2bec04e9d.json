{"cell_type":{"cbf40bd0":"code","f6dadfee":"code","fa0e17d5":"code","6c103b37":"code","988fcb05":"code","0766dca0":"code","1efba9b3":"code","6a6c3f62":"code","7f9b37e7":"code","21c2c7b5":"code","3eb1a433":"code","496b8a69":"code","e13f5903":"markdown","9576d5b2":"markdown","f8e93865":"markdown","cb80043b":"markdown"},"source":{"cbf40bd0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f6dadfee":"from fastai.vision.all import *","fa0e17d5":"import torch\nimport fastai\nfrom fastai.tabular.all import *\nfrom fastai.text.all import *\nfrom fastai.vision.all import *\nfrom fastai.medical.imaging import *\nfrom fastai import *\n\nimport time\nfrom datetime import datetime\n\nprint(f'Notebook last run on {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\nprint('Using fastai version ',fastai.__version__)\nprint('And torch version ',torch.__version__)","6c103b37":"def plot_fastai_results(learn):\n    '''\n    Plots sensitivity, speficificty, prevalence, accuracy, and confusion matrix for a fastai model named \"learn\".\n    Some portions are adapted from https:\/\/github.com\/fastai\/fastai\/blob\/master\/nbs\/61_tutorial.medical_imaging.ipynb\n    '''\n    interp = Interpretation.from_learner(learn)\n    interp = ClassificationInterpretation.from_learner(learn)\n    interp.plot_confusion_matrix(figsize=(7,7))\n    losses,idxs = interp.top_losses()\n    len(dls.valid_ds)==len(losses)==len(idxs)\n    upp, low = interp.confusion_matrix()\n    tn, fp = upp[0], upp[1]\n    fn, tp = low[0], low[1]\n    sensitivity = tp\/(tp + fn)\n    print('Sensitivity: ',sensitivity)\n    specificity = tn\/(fp + tn)\n    print('Specificity: ',specificity)\n    #val = dls.valid_ds.cat\n    prevalance = 15\/50\n    print('Prevalance: ',prevalance)\n    accuracy = (sensitivity * prevalance) + (specificity * (1 - prevalance))\n    print('Accuracy: ',accuracy)","988fcb05":"tfms = aug_transforms(max_rotate=25)","0766dca0":"len(tfms)","1efba9b3":"from PIL import Image\n\nimage = Image.open(\"..\/input\/chelslovo\/dataset\/lev110.jpg\")\nimage","6a6c3f62":"# importing all the required libraries\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport skimage.io as io\nfrom skimage.transform import rotate, AffineTransform, warp\nfrom skimage.util import random_noise\nfrom skimage.filters import gaussian\nimport matplotlib.pyplot as plt\nimport PIL.Image\nimport matplotlib.pyplot as plt\nimport torch\nfrom torchvision import transforms","7f9b37e7":"#Code by Naim Mhedhbi https:\/\/www.kaggle.com\/naim99\/data-augmentation-techniques\n\ndef imshow(img, transform):\n    \"\"\"helper function to show data augmentation\n    :param img: path of the image\n    :param transform: data augmentation technique to apply\"\"\"\n    \n    img = PIL.Image.open(img)\n    fig, ax = plt.subplots(1, 2, figsize=(15, 4))\n    ax[0].set_title(f'original image {img.size}')\n    ax[0].imshow(img)\n    img = transform(img)\n    ax[1].set_title(f'transformed image {img.size}')\n    ax[1].imshow(img)","21c2c7b5":"#Code by Naim Mhedhbi https:\/\/www.kaggle.com\/naim99\/data-augmentation-techniques\n\nloader_transform = transforms.Resize((140, 140))\n\nimshow('..\/input\/chelslovo\/dataset\/lev110.jpg', loader_transform)","3eb1a433":"#Code by Naim Mhedhbi https:\/\/www.kaggle.com\/naim99\/data-augmentation-techniques\n\n#flip image up-to-down\nflipUD = np.flipud(image)\n\nplt.imshow(flipUD)\nplt.title('Up Down Flipped');","496b8a69":"#Code by Naim Mhedhbi https:\/\/www.kaggle.com\/naim99\/data-augmentation-techniques\n\n#Hue can be described of as the shade of the colors in an image\n\nimg = PIL.Image.open('..\/input\/chelslovo\/dataset\/lev110.jpg')\nfig, ax = plt.subplots(2, 2, figsize=(16, 10))\n\n# brightness\nloader_transform1 = transforms.ColorJitter(brightness=2)\nimg1 = loader_transform1(img)\nax[0, 0].set_title(f'brightness')\nax[0, 0].imshow(img1)\n\n# contrast\nloader_transform2 = transforms.ColorJitter(contrast=2)\nimg2 = loader_transform2(img)\nax[0, 1].set_title(f'contrast')\nax[0, 1].imshow(img2)\n\n# saturation\nloader_transform3 = transforms.ColorJitter(saturation=2)\nimg3 = loader_transform3(img)\nax[1, 0].set_title(f'saturation')\nax[1, 0].imshow(img3)\nfig.savefig('color augmentation', bbox_inches='tight')\n\n# hue\nloader_transform4 = transforms.ColorJitter(hue=0.2)\nimg4 = loader_transform4(img)\nax[1, 1].set_title(f'hue')\nax[1, 1].imshow(img4)\n\nfig.savefig('color augmentation', bbox_inches='tight')","e13f5903":"#That's all for now!","9576d5b2":"![](https:\/\/static.wixstatic.com\/media\/1309a5_d8e3c18b49dc4e1b83a2e32e5f8dda31.jpg\/v1\/fill\/w_257,h_154,al_c,lg_1,q_80\/1309a5_d8e3c18b49dc4e1b83a2e32e5f8dda31.webp)","f8e93865":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcRqH9Fb6TeSJk8RGb98y8xFggHbN97CEMYEjg&usqp=CAU)amazon.com.br","cb80043b":"#Sl\u00f3vo - \u0441\u043b\u043e\u0432\u043e\n\n\"A sound or a combination of sounds, or its representation in writing or printing, that symbolizes and communicates a meaning and may consist of a single morpheme or of a combination of morphemes.\"\n\nhttps:\/\/www.thefreedictionary.com\/Slovo#:~:text=1.,of%20a%20combination%20of%20morphemes."}}