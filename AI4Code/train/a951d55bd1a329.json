{"cell_type":{"13d8bf93":"code","4a07e6c5":"code","8aa905fb":"code","6a5e4d4e":"code","64e5c359":"code","0427e6b1":"code","e091881a":"code","3e22deb2":"code","b38727ab":"code","e993cc5f":"code","04342640":"code","3abe6a77":"code","2bfa489d":"code","ef38138a":"code","7af69665":"code","1f9030a2":"code","e16a88c4":"code","71c7973f":"code","e0613fa2":"markdown","fda34d3f":"markdown","1ac887f0":"markdown","77c4f147":"markdown","f7ab87ac":"markdown","57f18d0b":"markdown","4d6072aa":"markdown","f92d3131":"markdown","8bd8e904":"markdown","7410d634":"markdown","39a64cef":"markdown","440a7f01":"markdown","e2da3d5a":"markdown","d627ce50":"markdown","442d652e":"markdown"},"source":{"13d8bf93":"import numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nfrom matplotlib.image import imread\nfrom PIL import Image\nimport IPython.display\n\nimport tensorflow as tf\nimport keras.preprocessing.image as process_im\nfrom keras.models import Model\nfrom tensorflow.python.keras import models \nfrom tensorflow.python.keras import losses\nfrom tensorflow.python.keras import layers\nfrom tensorflow.python.keras import backend as K\n\nimport functools\n","4a07e6c5":"# Set paths to content and style pictures\n\ncontent_paths = {'rainbow': '\/kaggle\/input\/pictures\/DSC04907_IG.jpg',\n                'rainbow2': '\/kaggle\/input\/pictures\/DSC05029_IG.jpg',\n                'tower': '\/kaggle\/input\/pictures\/DSC06205-Edit_IG.jpg',\n                'mp': '\/kaggle\/input\/pictures\/LRM_EXPORT_20190925_190945.jpg'}\nstyle_paths = {'vg': '\/kaggle\/input\/pictures\/vangogh.jpeg',\n              'umbrella': '\/kaggle\/input\/pictures\/rainprincess.jpg',\n              'face': '\/kaggle\/input\/pictures\/face.png',\n              'scream': '\/kaggle\/input\/pictures\/the-scream.jpg'}","8aa905fb":"# Plot sample images\n\ndef plot_samples(path1, path2):  \n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n    image = imread(path1)\n    ax[0].imshow(image)\n    ax[0].axis('off')\n    ax[0].set_title(\"Content Image\", fontsize=20)\n    image = imread(path2)\n    ax[1].imshow(image)\n    ax[1].axis('off')\n    ax[1].set_title(\"Style Image\", fontsize=20)\n    \n%matplotlib inline \nplot_samples(content_paths['rainbow'], style_paths['face'])","6a5e4d4e":"# load image as array\ndef load_file(image_path):\n    image =  Image.open(image_path)\n    max_dim = 512\n    factor = max_dim\/max(image.size)\n    image = image.resize((round(image.size[0]*factor),round(image.size[1]*factor)),\n                         Image.ANTIALIAS)\n    im_array = process_im.img_to_array(image)\n    #adding extra axis to the array as to generate a batch of single image \n    im_array = np.expand_dims(im_array,axis=0) \n    return im_array\n\n# preprocess image to vgg19 input requirements\ndef img_preprocess(img_path):\n    image = load_file(img_path)\n    img = tf.keras.applications.vgg19.preprocess_input(image)\n    return img\n\n# inverse preprocess step\ndef deprocess_img(processed_img):\n  x = processed_img.copy()\n  if len(x.shape) == 4:\n    x = np.squeeze(x, 0)\n  # Input dimension must be [1, height, width, channel] or [height, width, channel]\n  assert len(x.shape) == 3 \n  \n  \n  # perform the inverse of the preprocessing step\n  x[:, :, 0] += 103.939\n  x[:, :, 1] += 116.779\n  x[:, :, 2] += 123.68\n  x = x[:, :, ::-1] # converting BGR to RGB channel\n\n  x = np.clip(x, 0, 255).astype('uint8')\n  return x","64e5c359":"# Test image loading functions\n\nimg = img_preprocess(content_paths['rainbow'])\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\nax[0].imshow(img[0])\nax[0].axis('off')\nax[0].set_title(\"Processed Image\", fontsize=20)\n\nimg_deprocess = deprocess_img(img)\nax[1].imshow(img_deprocess)\nax[1].axis('off')\nax[1].set_title(\"Deprocessed Image\", fontsize=20)\n\nplt.show()","0427e6b1":"# set the layers to get content and style \ncontent_layers = ['block5_conv2']\nstyle_layers = ['block1_conv1',\n                'block2_conv1',\n                'block3_conv1', \n                'block4_conv1', \n                'block5_conv1']\nnumber_content=len(content_layers)\nnumber_style =len(style_layers)","e091881a":"def get_model():\n    # get vgg model\n    vgg=tf.keras.applications.vgg19.VGG19(include_top=False,weights='imagenet')\n    vgg.trainable=False\n    # get the content and style layers\n    content_output=[vgg.get_layer(layer).output for layer in content_layers]\n    style_output=[vgg.get_layer(layer).output for layer in style_layers]\n    model_output= style_output+content_output\n    return models.Model(vgg.input,model_output) # set model input and output\n\nmodel = get_model()\nmodel.summary()","3e22deb2":"model.output","b38727ab":"def get_content_loss(generated_content_feature,content_features):\n    # equation 2\n    loss = tf.reduce_mean(tf.square(generated_content_feature-content_features))\n    return loss","e993cc5f":"def gram_matrix(tensor):\n    # refer to picture above for better visualization\n    channels=int(tensor.shape[-1]) # get number of channels\n    vector=tf.reshape(tensor,[-1,channels]) # unroll into 2d matrix (h*w,channels)\n    n=tf.shape(vector)[0]\n    gram_matrix=tf.matmul(vector,vector,transpose_a=True) # compute gram matrix\n    return gram_matrix\/tf.cast(n,tf.float32)\n\ndef get_style_loss(generated_style_features,style_gram_matrix):\n    # get gram matrix of generated activations\n    generated_gram_matrix = gram_matrix(generated_style_features) \n    # frobenius norm of gram matrices -- equation 3\n    loss = tf.reduce_mean(tf.square(style_gram_matrix-generated_gram_matrix)) \n    return loss","04342640":"# extract out features\/activations of content and style images \n# from model output\ndef get_features(model,content_path,style_path):\n    # preprocess images\n    content_img=img_preprocess(content_path)\n    style_image=img_preprocess(style_path)\n    \n    # output of model for each image\n    content_output=model(content_img)\n    style_output=model(style_image)\n    \n    # extract out the features\/activations, content is last layer\n    content_feature = [layer[0] for layer in content_output[number_style:]]\n    # style has few layers\n    style_feature = [layer[0] for layer in style_output[:number_style]]\n    return content_feature,style_feature","3abe6a77":"def compute_loss(model, loss_weights,image, style_gram_matrix, content_features):\n    #style weight and content weight are user given parameters\n    #that define what percentage of content and\/or style will be preserved \n    # in the generated image\n    style_weight,content_weight = loss_weights \n    \n    output=model(image)\n    content_loss=0\n    style_loss=0\n    \n    # extract content,style activations of generated image\n    generated_style_features = output[:number_style]\n    generated_content_feature = output[number_style:]\n    \n    # compute style loss\n    weight_per_layer = 1.0\/float(number_style) # our \u03bb weighting is equal here\n    for a,b in zip(style_gram_matrix,generated_style_features):\n        style_loss+=weight_per_layer*get_style_loss(b[0],a) # equation 4\n        \n    # compute content loss\n    weight_per_layer =1.0\/ float(number_content)\n    for a,b in zip(generated_content_feature,content_features):\n        content_loss+=weight_per_layer*get_content_loss(a[0],b)\n    \n    # apply \u03b1 and \u03b2 in equation 1\n    style_loss *= style_weight\n    content_loss *= content_weight\n    # get total loss -- equation 1\n    total_loss = content_loss + style_loss\n    \n    \n    return total_loss,style_loss,content_loss","2bfa489d":"def compute_grads(dictionary):\n    with tf.GradientTape() as tape:\n        all_loss=compute_loss(**dictionary)\n        \n    total_loss=all_loss[0]\n    # return gradients and loss\n    return tape.gradient(total_loss,dictionary['image']),all_loss","ef38138a":"def run_style_transfer(content_path,style_path,epochs=500, content_weight=1e3, style_weight=1e-3, show_img=False):\n    \n    model=get_model()\n    \n    for layer in model.layers:\n        layer.trainable = False\n    \n    # get activations and style matrix\n    content_feature,style_feature = get_features(model,content_path,style_path)\n    # get gram matrix of style image\n    style_gram_matrix=[gram_matrix(feature) for feature in style_feature]\n    \n    # initialize generated image\n    generated = img_preprocess(content_path)\n    generated = tf.Variable(generated,dtype=tf.float32)\n    \n    optimizer = tf.keras.optimizers.Adam(learning_rate=5, beta_1=0.99, epsilon=1e-1)\n    \n    best_loss,best_img=float('inf'),None\n    \n    # relative weightings of content and style image\n    loss_weights = (style_weight, content_weight)\n    dictionary={'model':model,\n              'loss_weights':loss_weights,\n              'image':generated,\n              'style_gram_matrix':style_gram_matrix,\n              'content_features':content_feature}\n    \n    # for clipping image\n    norm_means = np.array([103.939, 116.779, 123.68])\n    min_vals = -norm_means\n    max_vals = 255 - norm_means   \n  \n    imgs = []\n    for i in range(epochs):\n        # run gradient descent and update generated image\n        grad,all_loss = compute_grads(dictionary)\n        total_loss,style_loss,content_loss = all_loss\n        optimizer.apply_gradients([(grad,generated)])\n        clipped=tf.clip_by_value(generated,min_vals,max_vals)\n        generated.assign(clipped)\n        \n        if total_loss<best_loss:\n            best_loss = total_loss\n            best_img = deprocess_img(generated.numpy())\n            \n        # for visualization \n        # print out every 5 epochs\n        \n        if show_img:\n            if i%5==0:\n                plot_img = generated.numpy()\n                plot_img = deprocess_img(plot_img)\n                imgs.append(plot_img)\n                IPython.display.clear_output(wait=True)\n                IPython.display.display_png(Image.fromarray(plot_img))\n                print('Epoch: {}'.format(i))        \n                print('Total loss: {:.4e}, ' \n                  'style loss: {:.4e}, '\n                  'content loss: {:.4e}, '.format(total_loss, style_loss, content_loss))\n    \n    IPython.display.clear_output(wait=True)\n    \n    \n    return best_img,best_loss,imgs","7af69665":"best, best_loss,image = run_style_transfer(content_paths['rainbow'], \n                                           style_paths['face'], epochs=1000,\n                                           style_weight=1e-4,\n                                           show_img=True)","1f9030a2":"def plot_images(generated, content_path, style_path):\n    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\n    ax[0].imshow(generated)\n    ax[0].axis('off')\n    ax[0].set_title(\"Generated Image\", fontsize=20)\n    \n    image = imread(content_path)\n    ax[1].imshow(image)\n    ax[1].axis('off')\n    ax[1].set_title(\"Content Image\", fontsize=20)\n    \n    image = imread(style_path)\n    ax[2].imshow(image)\n    ax[2].axis('off')\n    ax[2].set_title(\"Style Image\", fontsize=20)","e16a88c4":"plot_images(best, content_paths['rainbow'], style_paths['face'])","71c7973f":"best, best_loss,image = run_style_transfer(content_paths['mp'], \n                                           style_paths['vg'], epochs=1000,\n                                           style_weight=1e-3)\nplot_images(best, content_paths['mp'], style_paths['vg'])","e0613fa2":"# **Neural Style Transfer for Artistic Style Pictures** <br>\nAuthor: TeYang, Lau<br>\nCreated: 23\/7\/2020<br>\nLast update: 8\/8\/2020<br>\n\n<img src = 'https:\/\/media.giphy.com\/media\/UqY2VYZY3u88vikIo6\/giphy.gif' width=\"900\">\n<br><br>\n\nThis notebook was created after my course in the [Deep Learning Specialization](https:\/\/www.coursera.org\/learn\/convolutional-neural-networks) from [Deeplearning.ai](https:\/\/www.deeplearning.ai\/). Some of the notes here are from the course assignment notebooks and some of the code are from this [notebook](https:\/\/www.kaggle.com\/sayakdasgupta\/neural-style-transfer-using-vgg19) by [sayak]( https:\/\/www.kaggle.com\/sayakdasgupta).\n\n**Neural Style Transfer (NST)** is a machine learning optimization technique that uses 2 images - a content image and a style image, and blends them together so the output looks like the content image, but 'painted' in the style of the style image. This algorithm was first introduced by Gatys et al. in the paper called [A Neural Algorithm of Artistic Style](https:\/\/arxiv.org\/pdf\/1508.06576.pdf). The core innovation of NST is the use of deep learning to minimize the generated image's content and style so that it matches that of the content and style images. By doing so, it allows us to compose our own pictures in the style of any other paintings\/pictures ranging from google images to famous paintings.\n\nThis notebook will go through the implementation of NST and the process is as follows:\n\n1. [Preparing Data](#DataPrep)\n2. [Image Preprocessing](#Preprocess)\n3. [Model Setup](#Model)\n4. [Cost Function](#Cost)<br>\n    4.1. [Content Cost Function](#ContentCost)<br>\n    4.2. [Style Cost Function](#StyleCost)    \n5. [Train Model](#Train)   \n6. [Conclusion](#Conclusion)\n<br><br>\n","fda34d3f":"<a id='ContentCost'><\/a>\n# 4.1. Content Cost Function\n \n\nThe idea of the content cost is that you want the content of the generated image to be similar to the content image. Therefore, we want the activations of the generated image and content image at the selected layer to be similar. To compute this disimilarity, we can use the **euclidean distance** of the 2 activations and minimize this disimilarity\/cost:\n\n$$\\Large J_{content}(C,G) = \\sum _{ \\text{all entries}} (a^{(C)} - a^{(G)})^2\\tag{2} $$\n\n* a is the activation of a layer in the neural network, C is content and G is generated\n\n***Note:*** the original paper applies a normalization constant of 1\/2 to the cost\/euclidean distance. However this can be accounted for in the weighting hyperparameter applied to the cost, so it can be omitted here.\n\nThis picture gives an idea of the activations of a convolutional layer:\n\n<img src = 'https:\/\/09qjnq.bn.files.1drv.com\/y4mKB-XobWfxcitfXiDUtp_f_niv15mEwC2YNhUlnQS41FfewmrEFL5PQksReg895h4YulwOeHeg8bgjvPSb0ZwkTukkyWVNf4cYLDIu_yvRp3vrU08w-cwpkCTTl2SlKTglJTmm7xl1_mk9OmsUAiY-U1bYg2uo3sMmrbzNk8h4-tYMl9K5f8HZXEbE6c5EjVGImuGjr7KIqJPUaaxm-IPlw?width=1600&height=745&cropmode=none'>","1ac887f0":"<a id='Preprocess'><\/a>\n# 2. Image Preprocessing\n\nHere we write some functions to prepare the image for inputting into the convolutional neural network.","77c4f147":"## Animated Neural Style Transfer\n<img src='https:\/\/media.giphy.com\/media\/fVi9gXYJ2RQMS0ROUL\/giphy.gif' align='left'>","f7ab87ac":"It is always good to test out our functions to make sure they work.","57f18d0b":"Since we are using many layers for the style of the image, we have to combine them. This is done through applying weightings `\u03bb` to the layers. `\u03bb` can be tuned to give more weightings to specific layers of the network. For example, if we want high level styles, we will give more weightings to the higher layers of the network. \n\n$$\\Large J_{style}(S,G) = \\sum_{l} \\lambda^{[l]} J^{[l]}_{style}(S,G)\\tag{4}$$","4d6072aa":"<a id='DataPrep'><\/a>\n# 1. Preparing Data\n\nWe start by loading the dependencies, set the filepaths of our content and style iamges, and plot out some of them.","f92d3131":"<a id='Model'><\/a>\n# 3. Model Setup\n\nNeural style transfer works by passing the content and style image through the neural network and extracting out the features\/activations in certain layers. Here we select the layers that we want to extract the content and features of the images. For the content, we usually want to select a layer from the middle of the CNN as the earlier layers usually contain lower-level features and vice versa for the later layers. However, there is no 'right' way and instead, we can treat this as a hyperparameter that can be tuned to improve the final generated image we achieve. For the style layers, we usually select multiple layers which contain low-level to high-level features.\n\nFor the CNN, we will be using VGG19 as it has been found to perform the best.","8bd8e904":"<a id='Train'><\/a>\n# 5. Train Model\n\nFinally, let's put all the functions together to perform neural style transfer! Remember to turn on GPU for faster training. With GPU, it takes about 10 times faster to compose one image for me!","7410d634":"<a id='Conclusion'><\/a>\n# 6. Conclusion\n\nNeural style transfer is a fun activity for learning deep learning and neural networks and can be a good way to take a break from doing\/learning too much deep learning while still learning and practicing it. Fun aside, NST also has real life applications. For example, it can be used as a data augmentation tool for creating new medical images, especially for positive cases, which are usually more scarce compared to negative cases. It has also been used to improve [3D Cardiovascular MR Image Segmentation](https:\/\/arxiv.org\/abs\/1909.09716#:~:text=Recent%20years%2C%20deep%20neural%20networks,in%20medical%20image%20segmentation%20problem.&text=Specifically%2C%20neural%20style%20transfer%20algorithm,%2C%20contrast%2C%20texture%2C%20etc.). This is the era of deep learning and AI and I look forward to seeing what they can do and contribute for the medical and health industry.\n\n**If you like this notebook, please give me an upvote and\/or check out my other work! Happy composing artistic style pictures!**","39a64cef":"## Animated Neural Style Transfer\n<img src='https:\/\/media.giphy.com\/media\/XF3R9rQHDumCP6z1xI\/giphy.gif' align='left'>","440a7f01":"<font size=\"+3\" color=\"steelblue\"><b>My other works<\/b><\/font><br>\n\n<div class=\"row\">\n    \n  <div class=\"col-sm-4\">\n    <div class=\"card\">\n      <div class=\"card-body\" style=\"width: 20rem;\">\n         <h5 class=\"card-title\"><u>Pneumonia Detection with PyTorch<\/u><\/h5>\n         <img style='height:170' src=\"https:\/\/raw.githubusercontent.com\/teyang-lau\/Pneumonia_Detection\/master\/Pictures\/train_grid.png\" class=\"card-img-top\" alt=\"...\"><br>\n         <p class=\"card-text\">Pneumonia Detection using Transfer Learning via ResNet in PyTorch<\/p>\n         <a href=\"https:\/\/www.kaggle.com\/teyang\/pneumonia-detection-resnets-pytorch\" class=\"btn btn-primary\" style=\"color:white;\">Go to Post<\/a>\n      <\/div>\n    <\/div>\n  <\/div>   \n    \n  <div class=\"col-sm-4\">\n    <div class=\"card\">\n      <div class=\"card-body\" style=\"width: 20rem;\" style='background:red'>\n        <h5 class=\"card-title\"><u>120 Dog Breeds Classification<\/u><\/h5>\n        <img style='width:250px' src=\"https:\/\/raw.githubusercontent.com\/teyang-lau\/Dog_Breeds_Classification_CNN\/master\/Pictures\/dogbreeds.jpg\" class=\"card-img-top\" alt=\"...\"><br>\n        <p class=\"card-text\">Dog Breeds Classification using Transfer Learning via Inception V3.<\/p>\n        <a href=\"https:\/\/www.kaggle.com\/teyang\/dog-breeds-classification-using-transfer-learning?scriptVersionId=35621180\" class=\"btn btn-primary\" style=\"color:white;\">Go to Post<\/a>\n      <\/div>\n    <\/div>\n  <\/div>\n    \n  <div class=\"col-sm-4\">\n    <div class=\"card\">\n      <div class=\"card-body\" style=\"width: 20rem;\">\n         <h5 class=\"card-title\"><u>Covid-19 & Google Trends<\/u><\/h5>\n         <img style='height:135px' src=\"https:\/\/miro.medium.com\/max\/821\/1*Fi6masemXJT3Q8YWekQCDQ.png\" class=\"card-img-top\" alt=\"...\"><br><br>\n         <p class=\"card-text\">Covid-19-Google Trend Analysis and Data Vizualization<\/p>\n         <a href=\"https:\/\/www.kaggle.com\/teyang\/covid-19-google-trends-auto-arima-forecasting\" class=\"btn btn-primary\" style=\"color:white;\">Go to Post<\/a>\n      <\/div>\n    <\/div>\n  <\/div>  ","e2da3d5a":"Feel free to experiment with some of the hyperparameters, such as `epochs`, `content_weight`(\u03b1) and `style_weight`(\u03b2) to compose the best artistic images! What I usually do is to try out a few `content_weights`, settle on one and then keep it constant while tuning `epochs` and `style_weights` to get a desired balanced effect. Here are some examples of performing NST on my pictures in Peru:","d627ce50":"<a id='StyleCost'><\/a>\n# 4.2. Style Cost Function\n\nThe style of an image is harder to understand. However, it is a simple idea that involves a gram matrix or style matrix, which is simply a correlation matrix of a layer's channels\/features. To be correct, it is actually cross-correlation that is used, which is a unnormalized correlation.\n\n### Gram\/style matrix\n\n* The style matrix is also called a gram matrix. \n* In linear algebra, the Gram matrix $G_{gram}$ of a set of vectors $(v_{1},\\dots ,v_{n})$ is the matrix of dot products, whose entries are ${\\displaystyle G_{(gram)i,j} = v_{i}^T v_{j} = np.dot(v_{i}, v_{j})  }$. \n* In other words, $G_{(gram),i,j}$ compares how similar $v_i$ is to $v_j$: If they are highly similar, you would expect them to have a large dot product, and thus for $G_{(gram)i,j}$ to be large. \n\n\n\n<img src = 'https:\/\/09qknq.bn.files.1drv.com\/y4mvXzoDHegi5-iQkd3QFshk1DVx2YahQqazDxM8DpMzoMLubZV0ao02lu0mj-YuPSFbgz_6rbwyWffJtwKfxhULwj_Iu8eAOryRxYhP9w83VBjvUJS3FkEFrGNTcZLRUxvbTwiX5UI950Nx40JpBygVHvumDnk-lcPTU21jH8_3T_bVAhcIA-VzKgiDqBb-UxUljb-TsdiFPB31ILUGvjcfA?width=1600&height=529&cropmode=none'>\n\n\n#### $G_{(gram)i,j}$: correlation\nThe result is a matrix of dimension $(n_C,n_C)$ where $n_C$ is the number of filters (channels). The value $G_{(gram)i,j}$ measures how similar the activations of filter $i$ are to the activations of filter $j$. \n\n*Note:*\n\n#### $G_{(gram),i,i}$: prevalence of patterns or textures\n* The diagonal elements $G_{(gram)ii}$ measure how \"active\" a filter $i$ is. \n* For example, suppose filter $i$ is detecting vertical textures in the image. Then $G_{(gram)ii}$ measures how common  vertical textures are in the image as a whole.\n* If $G_{(gram)ii}$ is large, this means that the image has a lot of vertical texture. \n\nBy capturing the prevalence of different types of features $G_{(gram)ii}$, as well as how much different features occur together $G_{(gram)ij}$, the Style matrix $G_{gram}$ measures the style of an image. \n\n### Example\nHere is an example of activations from a layer of a convolutional neural network. Each 9x9 corresponds to one neuron\/channel. The channel with the red border is detecting vertical features while the channel with the green border is looking for orange colored patches in an image. If these two channels are highly correlated, it means that whenever a part of the image has these kind of vertical texture, then it will also probably have an orange hue. Therefore, the correlation tells us which of these features tend to occur or not occur together in different parts of an image.\n\n<img src='https:\/\/2tqhnq.bn.files.1drv.com\/y4mZgMdq4GFWQXoCuR6y4JG_Dmitye0FwOvV_HMqBzl9nWdLQdmF6yNEaOSaABKP0HN1RaAemRc3dnHqQUFM6QN2TW4cHfeiEOn_pOox1LJ_pSRKy-r0Jz2e34MU0Q_j2n-cwQzSlNyuEi_S2R7L9ei8l4zLmBGcrC3FIP0xD82YhK5vI2mZNNSGKcMtJw5boFnJLobkfl2BVT4_SmTcJMRGA?width=351&height=336&cropmode=none'>\n\n### Style Cost\nThe idea of the style cost is that we want the style of the generated image to be similar to the style image. Therefore, we want the style\/gram matrix of the generated image and style image at the selected layer to be similar. To compute this disimilarity, we can use the euclidean distance of the 2 gram\/style matrix **(Frobenius norm)** and minimize this disimilarity\/cost: \n\n$$\\Large J_{style}^{[l]}(S,G) = \\sum _{i=1}^{n_C}\\sum_{j=1}^{n_C}(G^{(S)}_{(gram)i,j} - G^{(G)}_{(gram)i,j})^2\\tag{3} $$\n\n* $G_{gram}^{(S)}$ Gram matrix of the \"style\" image.\n* $G_{gram}^{(G)}$ Gram matrix of the \"generated\" image.\n* Remember, this cost is computed using the hidden layer activations for a particular hidden layer in the network $a^{[l]}$","442d652e":"<a id='Cost'><\/a>\n# 4. Cost Function\n\nThe cost function is the most important idea in neural style transfer. Here, I am using cost and loss interchangeably. The overall idea is simple. We want the generated image to have the same content as the content image while having the same style as the style image. Therefore, we are looking to reduce the cost\/loss of the generated image, which we define as the sum of its content cost and style cost:\n\n$$\\Large J_{generated}(G) = \\alpha J_{content}(C,G) + \\beta J_{style}(S,G)\\tag{1}$$\n\n* J is the cost\/loss\n* \u03b1 and \u03b2 are relative weightings applied to the content and style cost respectively\n* G is \"Generated\", C is \"Content\", S is \"Style\"\n\nThe idea is to use gradient descent to minimize `Jgenerated`, and use it to update the generated Image by the following steps:\n\n1. Initialize `G` (generated image pixels) randomly, for e.g., 100x100x3 <br>\n\n<img src = 'https:\/\/zdo5qg.bn.files.1drv.com\/y4mOAvrAJpQC-cJqGGmEYe8KzkwUZ1-QYPxUPslckWrgDnAoYnLqhSRIqVW6MEms_HPMuXxD8SvfFjKmNIwewzJNTxBBqvltJQuQcumQS-go2mGUKvplFIhvnWiboew4kxvvZ_bN17k8HeFTD3U96nVqjg7RSt7Ih0nJsUO7KRvScD39Vy136uU7CyqfGJCra58zYqe0IgAGUOx-zBl0RK2Vg?width=139&height=97&cropmode=none' align='left'><br><br>\n\n<br><br>\n2. Use gradient descent to minimize `Jgenerated` and update `G`\n\n<img src = 'https:\/\/zdo2qg.bn.files.1drv.com\/y4mVd3fQ4G1k-k06JrI8hHZg9QwC_DjrQuET1lwkSQNNB235nO5uQeK4Qppc8wvCu4iTHIqNc0wKaJytzzqSWmiEHntVtXFpV_gs074R8-mrPBdA0IgoEXV2bAcEwuoq203kgwfrGuuyTYnIMj6W7uo7WnFvCH5SsgTuoi6vEP0lTnz_mnBr83VkvNgS3xDGK1brUD-5FHMJo4cDLmGUJrUQA?width=657&height=310&cropmode=none' align='left'>\n"}}