{"cell_type":{"74097c26":"code","76cf1673":"code","0cf00b51":"code","eb29bf81":"code","367bb0dc":"code","2ca8fbc0":"code","436316a1":"code","5a242fab":"code","48404df0":"code","95766dbe":"code","61b45039":"code","c4043799":"code","fcbd4b71":"code","9aa150c3":"code","1c586f50":"code","ccc66c9a":"code","97e24746":"code","bd307623":"code","9e494b7f":"code","ef8b94f1":"code","ce84dbeb":"code","41b849ed":"code","195936ed":"code","bfab8c98":"code","7086947d":"code","795c0186":"code","df2945ef":"code","87338716":"code","296b4b45":"code","d038de19":"code","58ec1bfb":"code","9dd0e649":"code","ca8a1930":"code","1202aea5":"code","f8cbd71f":"code","399f1545":"code","97e02a97":"code","9f1495ea":"code","fc36c385":"code","3f2d8666":"code","e7325d50":"code","dc7d2e66":"code","e38108ae":"code","019fe630":"code","a55a7981":"code","61206ffe":"markdown","52ecc5f1":"markdown","9630888d":"markdown","2f9684a6":"markdown","f057b183":"markdown","9866d91c":"markdown","18bfe8ca":"markdown","3ee62407":"markdown","a952792f":"markdown","a005eb31":"markdown","2cc2d98c":"markdown","944371bb":"markdown","493629d4":"markdown","2ff90508":"markdown","dec78c5c":"markdown","5f08e7ec":"markdown","4b64f2cd":"markdown","777a7ef2":"markdown","382072ed":"markdown","4c7ffbf9":"markdown","5870d594":"markdown","1633737c":"markdown","22e2dc1a":"markdown","2caa4116":"markdown","d2d7c8a4":"markdown","49f672f1":"markdown","8e12fe64":"markdown","c077b6d1":"markdown","a9f2e485":"markdown","d5eaca3a":"markdown","de0e25c4":"markdown","1a6131f4":"markdown","17029b11":"markdown","d925b106":"markdown","e549279b":"markdown","f4a6ef4c":"markdown","624657fb":"markdown","98a822b1":"markdown","b39ecf25":"markdown","c642f787":"markdown","8e17d05c":"markdown","9f5e35bd":"markdown","a422bb16":"markdown","fc8c399e":"markdown","6139e616":"markdown","925f0a9e":"markdown","4a6c3661":"markdown","3000acb5":"markdown","bd199dab":"markdown","3fe4bac8":"markdown","3f5a4d0c":"markdown","d044fe0c":"markdown","3999c597":"markdown","4f8de226":"markdown","ea668806":"markdown","f8fa82e7":"markdown","543e81e7":"markdown","e4288c50":"markdown","92060826":"markdown"},"source":{"74097c26":"%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline","76cf1673":"!pip install xgboost==0.90\n\n","0cf00b51":" !pip install fastai==0.7.0","eb29bf81":"from fastai.imports import *\nfrom fastai.structured import *\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pandas_summary import DataFrameSummary\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom IPython.display import display\nimport xgboost as xgb\nfrom sklearn import metrics\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV","367bb0dc":"df_raw = pd.read_csv(os.path.join(\"..\/input\",\"train.csv\"))","2ca8fbc0":"df_raw['Age'] = df_raw['YrSold'] - df_raw['YearBuilt']","436316a1":"df_raw['Remodel Age'] = df_raw['YrSold'] - df_raw['YearRemodAdd']","5a242fab":"df_raw.drop(['YrSold'], axis=1,inplace=True)\ndf_raw.drop(['YearBuilt'], axis=1,inplace=True)\ndf_raw.drop(['YearRemodAdd'], axis=1,inplace=True)\ndf_raw.drop(['GarageArea'], axis=1,inplace=True)\n","48404df0":"df_raw['Quality'] = df_raw['OverallCond']+df_raw['OverallQual']\n","95766dbe":"df_raw.drop(['OverallCond'], axis=1,inplace=True)\ndf_raw.drop(['OverallQual'], axis=1,inplace=True)\n","61b45039":"df_raw['Avg area of room'] = df_raw['GrLivArea']\/df_raw['TotRmsAbvGrd']","c4043799":"train_cats(df_raw)","fcbd4b71":"df_raw.SalePrice = np.log(df_raw.SalePrice)","9aa150c3":"df_trn2, y_trn, nas = proc_df(df_raw, 'SalePrice', max_n_cat=30)\n","1c586f50":"df_trn2.drop(['Id'], axis=1,inplace=True)\n","ccc66c9a":"def split_vals(a,n): return a[:n].copy(), a[n:].copy()\n\nn_valid = 350  \nn_trn = len(df_trn2)-n_valid\nraw_train, raw_valid = split_vals(df_raw, n_trn)\nX_train, X_valid = split_vals(df_trn2, n_trn)\ny_train, y_valid = split_vals(y_trn, n_trn)\nX_train.shape, y_train.shape, X_valid.shape\n\nn_test = 100\n","97e24746":"def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n\ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train),\n           rmse(m.predict(X_valid), y_valid),\n           m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)","bd307623":"\nm = RandomForestRegressor(n_estimators=120, min_samples_leaf=1, \n                      max_features=0.6, n_jobs=-1, oob_score=True)\nm.fit(X_train, y_train) \nprint_score(m)","9e494b7f":"#Understanding how number of estimators will effect the perfomance\npreds = np.stack([t.predict(X_valid) for t in m.estimators_])\npreds[:,0], np.mean(preds[:,0]), y_valid[0]\nplt.plot([metrics.r2_score(y_valid, np.mean(preds[:i+1], axis=0)) for i in range(100)]);","ef8b94f1":"import lightgbm as lgb\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.055, n_estimators=850,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 6, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\nmodel_lgb.fit(X_train, y_train) \nprint_score(model_lgb)","ce84dbeb":"from xgboost import XGBRegressor\nclf3= XGBRegressor(max_depth=9,learning_rate=0.07,subsample=.8,min_child_weight=3,colsample_bytree=.6,scale_pos_weight=1,\ngamma=10,reg_alpha=6,reg_lambda=1.1)\n# n_estimators = 100 (default)\n# max_depth = 3 (default)\nclf3.fit(X_train,y_train)\nprint_score(clf3)","41b849ed":"dtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_valid, label=y_valid)\n\nparams = {\n    'booster': 'gbtree', \n    'objective': 'reg:squarederror', # regression task\n    'subsample': 0.8, # 80% of data to grow trees and prevent overfitting\n    'colsample_bytree': 0.89, # 89% of features used\n    'eta': 0.1,\n    'gamma':0,\n    'max_depth': 10,\n    'seed': 42} # for reproducible results\nparams['eval_metric'] = \"rmse\"\nnum_boost_round = 999\n","195936ed":"model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dtest, \"Test\")],\n    early_stopping_rounds=10\n)","bfab8c98":"cv_results = xgb.cv(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    seed=42,\n    nfold=5,\n    metrics={'rmse'},\n    early_stopping_rounds=10\n)\ncv_results","7086947d":"params['eval_metric'] = \"rmse\"\ngridsearch_params = [\n    (max_depth, min_child_weight)\n    for max_depth in range(5,12)\n    for min_child_weight in range(1,8)\n]\n\nmin_mae = float(\"Inf\")\nbest_params = None\n\nmin_rmse = float(\"Inf\")\nfor max_depth, min_child_weight in gridsearch_params:\n    print(\"CV with max_depth={}, min_child_weight={}\".format(\n                             max_depth,\n                             min_child_weight))\n\n    # Update our parameters\n    params['max_depth'] = max_depth\n    params['min_child_weight'] = min_child_weight\n\n    # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=42,\n        nfold=5,\n        metrics={'rmse'},\n        early_stopping_rounds=10\n    )\n    \n      # Update best RMSLE\n    mean_rmse = cv_results['test-rmse-mean'].min()\n    boost_rounds = cv_results['test-rmse-mean'].argmin()\n    print(\"\\tRMSE {} for {} rounds\".format(mean_rmse, boost_rounds))\n    if mean_rmse < min_rmse:\n        min_rmse = mean_rmse\n        best_params = (max_depth,min_child_weight)\n\nprint(\"Best params: {}, {}, RMSE: {}\".format(best_params[0], best_params[1], min_rmse))  ","795c0186":"gridsearch_params = [\n    (subsample, colsample)\n    for subsample in [i\/10. for i in range(5,10)]\n    for colsample in [i\/10. for i in range(5,10)]\n]\n\nmin_rmse = float(\"Inf\")\nbest_params = None\n\n# We start by the largest values and go down to the smallest\nfor subsample, colsample in reversed(gridsearch_params):\n    print(\"CV with subsample={}, colsample={}\".format(\n                             subsample,\n                             colsample))\n\n    # We update our parameters\n    params['subsample'] = subsample\n    params['colsample_bytree'] = colsample\n\n    # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=num_boost_round,\n        seed=42,\n        nfold=5,\n        metrics={'rmse'},\n        early_stopping_rounds=10\n    )\n\n    # Update best score\n    mean_rmse = cv_results['test-rmse-mean'].min()\n    boost_rounds = cv_results['test-rmse-mean'].argmin()\n    print(\"\\tRMSE {} for {} rounds\".format(mean_rmse, boost_rounds))\n    if mean_rmse < min_rmse:\n        min_rmse = mean_rmse\n        best_params = (subsample,colsample)\n\nprint(\"Best params: {}, {}, RMSE: {}\".format(best_params[0], best_params[1], min_rmse))","df2945ef":"min_rmse = float(\"Inf\")\nbest_params = None\n\nfor eta in [.022, .025, .02, .027, .028, .021]:\n    print(\"CV with eta={}\".format(eta))\n\n    # We update our parameters\n    params['eta'] = eta\n\n    # Run and time CV\n    cv_results = xgb.cv(params,\n            dtrain,\n            num_boost_round=num_boost_round,\n            seed=42,\n            nfold=5,\n            metrics=['rmse'],\n            early_stopping_rounds=10\n          )\n\n    # Update best score\n    mean_rmse = cv_results['test-rmse-mean'].min()\n    boost_rounds = cv_results['test-rmse-mean'].argmin()\n    print(\"\\tRMSE {} for {} rounds\\n\".format(mean_rmse, boost_rounds))\n    if mean_rmse < min_rmse:\n        min_rmse = mean_rmse\n        best_params = eta\n\nprint(\"Best params: {}, RMSE: {}\".format(best_params, min_rmse))","87338716":"min_rmse = float(\"Inf\")\nbest_params = None\n\nfor reg_alpha in [0, 0.001, 0.005, 0.01, 0.05]:\n    print(\"CV with reg_alpha ={}\".format(reg_alpha ))\n\n    # We update our parameters\n    params['reg_alpha '] = reg_alpha \n\n    # Run and time CV\n    cv_results = xgb.cv(params,\n            dtrain,\n            num_boost_round=num_boost_round,\n            seed=42,\n            nfold=5,\n            metrics=['rmse'],\n            early_stopping_rounds=10\n          )\n\n    # Update best score\n    mean_rmse = cv_results['test-rmse-mean'].min()\n    boost_rounds = cv_results['test-rmse-mean'].argmin()\n    print(\"\\tRMSE {} for {} rounds\\n\".format(mean_rmse, boost_rounds))\n    if mean_rmse < min_rmse:\n        min_rmse = mean_rmse\n        best_params = reg_alpha \n\nprint(\"Best params: {}, RMSE: {}\".format(best_params, min_rmse))","296b4b45":"params = {\n    'booster': 'gbtree', \n    'objective': 'reg:squarederror', # regression task\n    'subsample': 0.8, # 80% of data to grow trees and prevent overfitting\n    'colsample' : 0.8, # 80% of features used\n    'eta': 0.02,\n    'max_depth': 6,\n    'min_child_weight' : 5,\n    'seed': 42} # for reproducible results\nparams['eval_metric'] = \"rmse\"\nnum_boost_round = 999\n","d038de19":"model = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=num_boost_round,\n    evals=[(dtest, \"Test\")],\n    early_stopping_rounds=10\n)","58ec1bfb":"#Plotting a graph between the scores obtaibned from the validation sets of our models vs the score obtained\n#from kaggle\nx = [\n0.12694616827013327,0.13292792672788983,\n0.23365237301751454]\ny = [\n0.13052,0.14462,\n0.26203]\nimport matplotlib.pyplot as plt\nplt.plot(x, y, linewidth=3)\n\nplt.show()","9dd0e649":"t=m.estimators_[0].tree_\nfi = rf_feat_importance(m, df_trn2) \nfi.plot('cols', 'imp', figsize=(10,6), legend=False);","ca8a1930":"#Calculating feature importance by bar chart\ndef plot_fi(fi): return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\nplot_fi(fi[:30]);","1202aea5":"from scipy.cluster import hierarchy as hc\nto_keep = fi[fi.imp>0.001].cols; len(to_keep)\ndf_keep = df_trn2[to_keep].copy()\ncorr = np.round(scipy.stats.spearmanr(df_keep).correlation, 4)\ncorr_condensed = hc.distance.squareform(1-corr)\nz = hc.linkage(corr_condensed, method='average')\nfig = plt.figure(figsize=(16,10))\ndendrogram = hc.dendrogram(z, labels=df_keep.columns, \n      orientation='left', leaf_font_size=16)\nplt.show()","f8cbd71f":"from pdpbox import pdp # \nfrom plotnine import *\nfrom sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\ndef plot_pdp(feat, clusters=None, feat_name=None):\n    feat_name = feat_name or feat\n    p = pdp.pdp_isolate(m, df_trn2,feature=feat_name,model_features=df_trn2.columns)\n    return pdp.pdp_plot(p, feat_name, plot_lines=True, \n                        cluster=clusters is not None, \n                        n_cluster_centers=clusters)\n\nplot_pdp('Age')","399f1545":"from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\ndef plot_pdp(feat, clusters=None, feat_name=None):\n    feat_name = feat_name or feat\n    p = pdp.pdp_isolate(m, df_trn2,feature=feat_name,model_features=df_trn2.columns)\n    return pdp.pdp_plot(p, feat_name, plot_lines=True, \n                        cluster=clusters is not None, \n                        n_cluster_centers=clusters)\n\nplot_pdp('GrLivArea')","97e02a97":"from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\ndef plot_pdp(feat, clusters=None, feat_name=None):\n    feat_name = feat_name or feat\n    p = pdp.pdp_isolate(m, df_trn2,feature=feat_name,model_features=df_trn2.columns)\n    return pdp.pdp_plot(p, feat_name, plot_lines=True, \n                        cluster=clusters is not None, \n                        n_cluster_centers=clusters)\n\nplot_pdp('Quality')","9f1495ea":"df_raw['SalePrice'] = np.expm1(df_raw['SalePrice'])\nf, ax = plt.subplots(figsize=(16, 8))\ncorr = df_raw.corr()\nfig = sns.heatmap(corr)","fc36c385":"fig = sns.distplot(df_raw['SalePrice'],color='darkcyan')","3f2d8666":"fig = sns.jointplot(x=df_raw[\"SalePrice\"], y=df_raw[\"GrLivArea\"], kind='scatter',s=200, color='pink', edgecolor=\"white\", linewidth=2)","e7325d50":"sns.jointplot(x=df_raw[\"SalePrice\"], y=df_raw[\"Age\"], kind='hex', color='skyblue',gridsize=13)","dc7d2e66":"fig = sns.jointplot(x=df_raw[\"SalePrice\"], y=df_raw[\"GarageCars\"], kind='reg',color = 'limegreen')","e38108ae":"f, ax = plt.subplots(figsize=(10, 8))\nfig = sns.boxplot(x=\"Quality\", y=\"SalePrice\", data=df_raw)\nfig.axis(ymin=0, ymax=800000)","019fe630":"f, ax = plt.subplots(figsize=(10, 8))\nsns.swarmplot(x='TotRmsAbvGrd', y='SalePrice', hue='GrLivArea',\n              data =df_raw,color = 'red',alpha=0.8)","a55a7981":"f, ax = plt.subplots(figsize=(10, 8))\nsns.lineplot(x='Avg area of room', y='SalePrice', data=df_raw, hue='BedroomAbvGr')","61206ffe":"### Now incorporating the best parameters into the final model","52ecc5f1":"#### Boxplot\n*A boxplot is a standardized way of displaying the distribution of data based on a five number summary (\u201cminimum\u201d, first quartile (Q1), median, third quartile (Q3), and \u201cmaximum\u201d). It can tell you about your outliers and what their values are. It can also tell you if your data is symmetrical, how tightly your data is grouped, and if and how your data is skewed.*","9630888d":"RMSLE in XgBoost with Cross Validation : 0.137010","2f9684a6":"RMSLE in Random Forest : 0.13227153479706344","f057b183":"#### Metrics Used : RMSLE","9866d91c":"#### The most important features in the dataset:\n1. GrLivArea: Above grade (ground) living area square feet\n2. Age: How many years old the house was when it was bought.","18bfe8ca":"#### *Using grid search for reg_alpha*","3ee62407":"**Creating validation set**","a952792f":"#### *Using grid search for subsample and colsample*","a005eb31":"##### Insight : Saleprice of a house increases with GrLivArea","2cc2d98c":"This graph shows the distribution of SalePrice and Age and the relationship between Saleprice and Age through a hexplot. Saleprice shows a decrease with Age. ","944371bb":"After performing Grid Search on eta, best parameters are:\n\neta : 0.02","493629d4":"#### Swarmplot\n*The swarmplot plot is a one-dimensional scatter plot like \"stripchart\", but with closely-packed, non-overlapping points.*","2ff90508":"This graph shows the distribution of SalePrice and GrLivArea and the relationship between Saleprice and GrLivArea. Saleprice increases with GrLivArea. ","dec78c5c":"# 4.3 XgBoost with Cross Validation and Grid Search ","5f08e7ec":"This plot shows the distribution of saleprice in the dataset.","4b64f2cd":"After performing Grid Search on max_depth and min_child_weight, best parameters are:\n    max_depth : 6\n    min_child_weight : 5    ","777a7ef2":"RMSLE in Simple XgBoost : 0.123834","382072ed":"#### Heatmap\n*Heat Map of correlation coefficients.*","4c7ffbf9":"*A distribution plot shows the distribution of a particular feature*","5870d594":"As the line obtained is similar to graph of (y=x), it can be said that the validation set is a decent\nrepresentation of test set on kaggle","1633737c":"# Creating and comparing different Models","22e2dc1a":"This graph shows relationship between Saleprice, Avg area of room and BedroomAbvGr by a lineplot. The saleprice generally increases with Avg area of room and BedroomAbvGr.","2caa4116":"#### Distribution Plot","d2d7c8a4":"**Creating new features**","49f672f1":"# Data Visualization","8e12fe64":"RMSLE in Light GBM : 0.12468412714185853","c077b6d1":"#### Problem Statement : **Predict house sale prices**","a9f2e485":"# 4.2 XgBoost with cross validation","d5eaca3a":"# 4.1 Simple XgBoost","de0e25c4":"#### Line chart\n*A line chart or line plot or line graph is a type of chart which displays information as a series of data points called 'markers' connected by straight line segments.*","1a6131f4":"RMSLE in XgBoost : 0.23183511849800476\n\n\n","17029b11":"# House Sale Price Prediction [Kaggle Problem](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/overview)","d925b106":"#### *Using grid search for eta*","e549279b":"## Important outcomes:\n1. LightGBM and XgBoost with grid search performed better than Sklearn's XgBoost and Random Forest.\n2. LightGBM and XgBoost with grid search gave similar results.\n3. Using XgBoost with grid search improved the RMSLE from 0.123636 to 0.121844.\n4. Feature Engineering helped to improve score on Kaggle. \n5. Before feature Engineering: Rank-40%\n6. After feature Engineering: Rank-32%","f4a6ef4c":"##### Insight : Saleprice of a house increases with Quality","624657fb":"#### Marginal Plots","98a822b1":"# 3. Sklearn's XgBoost","b39ecf25":"After performing Grid Search on subsample and colsample, best parameters are:\n    subsample : 0.8\n    colsample : 0.8  ","c642f787":"This graph shows the distribution of SalePrice and GarageCars and the relationship between Saleprice and GarageCars through a regplot. Saleprice shows increase with GarageCars. ","8e17d05c":"**Taking the log of the SalePrice as we need to calculate rmsle**","9f5e35bd":"# Feature Engineering","a422bb16":"# 1. Random Forest","fc8c399e":"# Understanding the data","6139e616":"This graph shows  relationship between Saleprice and Quality by a boxplot. The saleprice generally increases with Quality.","925f0a9e":"### Finding molticollinearity using Dendogram","4a6c3661":"# 4. Native XgBoost","3000acb5":"# 2. LightGBM","bd199dab":"##### Insight : Saleprice of a house decreases with the age of house","3fe4bac8":"#### *Using grid search for max_depth and min_child_weight*","3f5a4d0c":"#### Importing Libraries and reading the data","d044fe0c":"### Partial dependency plots","3999c597":"**Handling with missing values and performing one hot encoding using proc_df technique of fastai**","4f8de226":"This graph shows relationship between Saleprice, TotRmsAbvGrd and GrLivArea by a swarmplot.The darker the color the smaller is the GrLivArea. The saleprice generally increases with TotRmsAbvGrd and GrLivArea.","ea668806":"**A function to calculate Root Mean Squared Logarithmic Error (RMSLE)**","f8fa82e7":"**Converting into categorical values using train_cats and apply_cats techniques of fastai**","543e81e7":"### Calculating feature importance","e4288c50":"*A marginal plot allows to study the relationship between 2 numeric variables. The central chart display their correlation. It is usually a scatterplot, a hexbin plot, a 2D histogram or a 2D density plot. The marginal charts, usually at the top and at the right, show the distribution of the 2 variables using histogram or density plot.*","92060826":"There is no multicollinearity among the features."}}