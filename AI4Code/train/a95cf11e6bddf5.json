{"cell_type":{"6b654eb9":"code","0702db8f":"code","a6c4786c":"code","3bac230b":"code","040a9acb":"code","2d1117b9":"code","d96c36ce":"code","fbe9121d":"code","7164be08":"code","b8f1d864":"code","94b0de24":"code","3128eb36":"code","b29befac":"code","e042cef8":"code","ec673cb0":"code","b7200896":"code","35394023":"code","7ea14b82":"code","771cc91a":"code","e15d8c40":"code","ffef70fe":"code","5a63f6b0":"code","692d8589":"code","e2191bb4":"code","dfdec3f6":"code","cf57c627":"code","fda22c21":"code","7da3a650":"code","94137426":"code","d32d443d":"code","c6c50bc1":"code","98b1ba8f":"code","4e38a01d":"code","12de4410":"code","d5a766c9":"code","5d109950":"code","cae07867":"code","e016a398":"code","2bda54ce":"code","b71ce725":"code","dd9bac00":"code","2756f9fa":"markdown","f39d69ca":"markdown","65d71edd":"markdown","5d31dc89":"markdown","826d28d8":"markdown","4a7b1c98":"markdown","3ea5d1ef":"markdown","072d7c21":"markdown","ee0450f9":"markdown","add561ad":"markdown","b368c64d":"markdown","43c3a9d2":"markdown","87062952":"markdown","f07d63fc":"markdown","380519ac":"markdown","6e03fcae":"markdown","b2a3c222":"markdown","18d3a8ab":"markdown","9ec1ccc3":"markdown","3b5516bc":"markdown"},"source":{"6b654eb9":"# scikeras library requires tensorflow >= 2.7.0\n!pip install -U tensorflow==2.7.0\n!pip install -U scikeras","0702db8f":"# load packages\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport numpy as np\nimport pandas as pd\npd.set_option('mode.chained_assignment',  None)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve\nfrom tensorflow import keras\nfrom scikeras.wrappers import KerasClassifier\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance","a6c4786c":"file_path = \"..\/input\/credit-risk-dataset\/loan\/loan.csv\"\ndf = pd.read_csv(file_path, index_col=[0])","3bac230b":"df.keys()","040a9acb":"attribs_to_use = [\n    'funded_amnt',          # actual amount funded to loans\n    'term',                 # number of payments (month)\n    'int_rate',             # interest rate\n    'installment',          # monthly payment \n    'sub_grade',            # credit rating\n    'emp_length',           # number of years of employment\n    'home_ownership',       # home ownership status of each borrower\n    'issue_d',              # the month which the loan was funded\n    'loan_status',          # current status of the loan (=target variable)\n    'purpose',              # purpose of the loan\n    'dti',                  # debt to monthly income ratio\n    'delinq_2yrs',          # number of 30+ days past-due incidences for last two years\n    'earliest_cr_line',     # date each borrower's earliest reported credit line was opened\n    'inq_last_6mths',       # number of inquiries in past 6 months\n    'total_acc',            # total number of credit lines \n    'pub_rec',              # number of derogatory public records\n    'revol_bal',            # total credit revolving balance\n    'revol_util',           # total revolving utilization rate \n    'total_rec_late_fee',   # late fees received to date \n    'recoveries',           # post charge off gross recovery\n    'last_pymnt_d',         # last month payment was received\n    'last_pymnt_amnt',      # last month payment amount\n    'last_credit_pull_d',   # the most recent month LC pulled credit for this loan\n    'acc_now_delinq',       # number of delinquent accounts\n    'total_pymnt',          # total payments received to date\n    'annual_inc'\n]\n\ndf_filter = df[attribs_to_use]","2d1117b9":"df_filter.shape","d96c36ce":"# select date columns\ndate_columns = [\n    'issue_d', \n    'earliest_cr_line', \n    'last_pymnt_d', \n    'last_credit_pull_d'\n]\n\n# drop rows whose date is NaN\ndf_filter = df_filter.dropna(subset=date_columns)\ndf_filter = df_filter.reset_index(drop=True)\n\n# format dates\nfor col in date_columns:\n    df_filter.loc[:, col] = pd.to_datetime(df_filter[col])\n\ndf_filter[date_columns].head()","fbe9121d":"plt.figure(figsize=(10,8))\n\nax = df_filter['loan_status'].value_counts().sort_values().plot.barh(edgecolor='black', grid=False, fontsize=12)\n\nfor p in ax.patches:\n    width = p.get_width()\n    ax.text(width+5000, p.get_y(), '{:.2%}'.format(width\/len(df_filter)), ha='left', va='bottom', fontsize=14)\n\nsns.despine()    ","7164be08":"# removed the 'Issued' class since these loans just got approved and have not begun any repayment processes yet\nissue = df_filter['loan_status'] == 'Issued'\ndf_filter_issue = df_filter[~issue]\n\n# binary classification\nlabel_categories = [\n    (0, ['Fully Paid', 'Does not meet the credit policy. Status:Fully Paid', 'Current']),\n    (1, ['Late (31-120 days)', 'Late (16-30 days)', 'In Grace Period', \n         'Charged Off', 'Default', 'Does not meet the credit policy. Status:Charged Off'])\n]\n\n# function to apply the transformation\ndef classify_label(text):\n    for category, matches in label_categories:\n        if any(match in text for match in matches):\n            return category\n    return None\n\ndf_filter_issue.loc[:, 'label'] = df_filter_issue['loan_status'].apply(classify_label)\ndf_filter_issue = df_filter_issue.drop('loan_status', axis=1)\n\n# check label classes imbalance\nneg, pos = np.bincount(df_filter_issue['label'])\ntotal = neg + pos\nprint('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n    total, pos, 100 * pos \/ total))","b8f1d864":"plt.figure(figsize=(8,6))\n\nax = sns.countplot(x='home_ownership', data=df_filter_issue, edgecolor='black')\n\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+.4, height, '{:.2%}'.format(height\/len(df_filter)), ha='center', va='bottom', fontsize=14)\n\nsns.despine()","94b0de24":"df_filter_issue['home_ownership'] = df_filter_issue['home_ownership'].replace(['NONE', 'ANY'], 'OTHER')","3128eb36":"df_filter_issue['emp_length'].unique()","b29befac":"# copy the feature\nemp_length = df_filter_issue['emp_length'].copy()\n\n# select not null values\nemp_length_nonull = emp_length.dropna()\n\n# manually replace '< 1 year' to '0'\nemp_length_nonull = emp_length_nonull.replace('< 1 year', '0')\n\n# format the rest by removing all strings except for numbers\nemp_length_encode = [re.sub('[<years+ ]', '', txt) for txt in emp_length_nonull]\n\n# squeeze back to the original dataset\nemp_length[emp_length.notnull()] = np.squeeze(emp_length_encode)\ndf_filter_issue.loc[:, 'emp_length'] = emp_length\n\ndf_filter_issue['emp_length']","e042cef8":"mis_vals = df_filter_issue.isnull().sum()\nmis_vals[mis_vals > 0]","ec673cb0":"# impute missing values with SimpleImputer\ndf_filter_issue['emp_length'] = SimpleImputer(strategy='most_frequent').fit_transform(df_filter_issue[['emp_length']]).astype(int)\ndf_filter_issue['revol_util'] = SimpleImputer(strategy='median').fit_transform(df_filter_issue[['revol_util']])","b7200896":"df_filter_issue['term'] = pd.factorize(df_filter_issue['term'])[0]\ndf_filter_issue['sub_grade'] = pd.factorize(df_filter_issue['sub_grade'])[0]","35394023":"# one-hot encoding other categorical variables and drop date columns\ndf_modeling = pd.get_dummies(df_filter_issue, columns=['home_ownership', 'purpose'], drop_first=True)\ndf_modeling = df_modeling.drop(date_columns, axis=1)","7ea14b82":"# split train test set\ntrain, test = train_test_split(df_modeling, test_size=0.2, stratify=df_modeling['label'], random_state=1)\n\ny_train = train.pop('label')\ny_test = test.pop('label')\nX_train = train\nX_test = test\n\n(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)","771cc91a":"# normalize data\nscaler = MinMaxScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","e15d8c40":"# set metrics to track\nMETRICS = [\n      keras.metrics.BinaryAccuracy(name='accuracy'),\n      keras.metrics.Precision(name='precision'),\n      keras.metrics.Recall(name='recall'),\n      keras.metrics.AUC(name='auc'),\n      keras.metrics.AUC(name='prc', curve='PR'), \n]\n\n# function to build simple dnn\ndef make_model(metrics, size):\n    model = keras.Sequential([\n        keras.layers.Dense(\n            16, \n            activation='relu', \n            input_shape=(size,)),\n        keras.layers.Dropout(0.5),\n        keras.layers.Dense(\n            8, \n            activation='relu'),\n        keras.layers.Dropout(0.5),        \n        keras.layers.Dense(\n            1, \n            activation='sigmoid'),\n  ])\n\n    model.compile(\n      optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n      loss=keras.losses.BinaryCrossentropy(),\n      metrics=metrics)\n    return model","ffef70fe":"# find the class weight to handle unbalanced label classes\nweight_for_0 = (1 \/ neg) * (total \/ 2.0)\nweight_for_1 = (1 \/ pos) * (total \/ 2.0)\n\nclass_weight = {0: weight_for_0, 1: weight_for_1}\n\nprint('Weight for class 0: {:.2f}'.format(weight_for_0))\nprint('Weight for class 1: {:.2f}'.format(weight_for_1))","5a63f6b0":"EPOCHS = 100\nBATCH_SIZE = 2048\n\nearly_stopping = keras.callbacks.EarlyStopping(\n    monitor='val_prc', \n    verbose=1,\n    patience=10,\n    mode='max',\n    restore_best_weights=True)","692d8589":"model_base = make_model(metrics=METRICS, size=X_train_scaled.shape[-1])\n\nclassifier_base = KerasClassifier(model=model_base,\n                                  epochs=EPOCHS,\n                                  batch_size=BATCH_SIZE,\n                                  callbacks=[early_stopping],\n                                  validation_split=0.1,\n                                  verbose=0)\n\nclassifier_base.fit(X_train_scaled, y_train, class_weight=class_weight)","e2191bb4":"# results output\ndef plot_cm(labels, predictions, p=0.5):\n    plt.figure(figsize=(8,6))\n    \n    cm = confusion_matrix(labels, predictions > p)\n    sns.heatmap(cm, annot=True, fmt=\"d\")\n    plt.title('Confusion matrix @{:.2f}'.format(p))\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n\nprediction_base = model_base.predict(X_test_scaled, batch_size=BATCH_SIZE)\nevaluation_base = model_base.evaluate(X_test_scaled, y_test, batch_size=BATCH_SIZE, verbose=0)\n\nfor name, value in zip(model_base.metrics_names, evaluation_base):\n    print(name, ': ', value)\nprint()\n\nplot_cm(y_test, prediction_base)","dfdec3f6":"# check feature importance\nperm_base = PermutationImportance(classifier_base, random_state=1).fit(X_train_scaled, y_train)\neli5.show_weights(perm_base, feature_names=X_train.columns.tolist())","cf57c627":"cols_selected = ['installment', 'int_rate', 'recoveries', 'last_pymnt_amnt', 'total_rec_late_fee', 'term', \n                 'funded_amnt', 'sub_grade', 'pub_rec', 'total_acc', 'dti', 'total_pymnt', 'purpose', 'label'] \\\n                + date_columns\n\ndf_filter_new = df_filter_issue[cols_selected]\ndf_filter_new.shape","fda22c21":"# checking correlations\ncorr = df_filter_new.corr().round(1)\n\nplt.figure(figsize=(25,20))\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr, annot=True, mask=mask, cbar=True)","7da3a650":"col = 'funded_amnt'\nx1 = df_filter_new[col].min()\nx2 = df_filter_new[col].max()\n\nplt.figure(figsize=(8,6))\nsns.scatterplot(x='funded_amnt', y='last_pymnt_amnt', data=df_filter_issue)\nplt.plot([x1,x2], [x1,x2], color='red', lw=10, alpha=.5)\nsns.despine()","94137426":"df_filter_new['overpay'] = df_filter_new['last_pymnt_amnt'] - df_filter_new['installment']","d32d443d":"# manually calculate a loan duration from issue date until last payment date in a monthly basis\ndf_filter_new['duration'] = np.round((df_filter_new['last_pymnt_d'] - df_filter_new['issue_d']).dt.days \/ 31)\n\n# calculate total sceduled payment \ntotal_scheduled_pay = df_filter_new['installment'] * df_filter_new['duration']\n\n# calculate the payment gap between actual and scheduled \ndf_filter_new['pay_gap'] = df_filter_new['total_pymnt'] - total_scheduled_pay","c6c50bc1":"plt.figure(figsize=(12,6))\nsns.boxplot(x='purpose', y='funded_amnt', data=df_filter_new)\nplt.xticks(rotation=30)\nplt.title('Loan amounts grouped by purpose')","98b1ba8f":"# calculate median loan amounts for each group in purpose category\npurpose_median = dict(df_filter_new.groupby('purpose')['funded_amnt'].median())\ndf_filter_new['overfunding'] = df_filter_new['funded_amnt'] - df_filter_new['purpose'].map(purpose_median)","4e38a01d":"# check if a borrower's credit was pulled by LC after one's last payment date\ndf_filter_new['credit_pull_after_last_pay'] = (df_filter_new['last_credit_pull_d'] - df_filter_new['last_pymnt_d']).dt.days\ndf_filter_new['credit_pull_after_last_pay'] = np.where(df_filter_new['credit_pull_after_last_pay'] > 0, 1, 0)","12de4410":"df_modeling = pd.get_dummies(df_filter_new, columns=['purpose'], drop_first=True)\ndf_modeling = df_modeling.drop(date_columns, axis=1)","d5a766c9":"train, test = train_test_split(df_modeling, test_size=0.2, stratify=df_modeling['label'], random_state=1)\n\ny_train = train.pop('label')\ny_test = test.pop('label')\nX_train = train\nX_test = test\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","5d109950":"model_ext = make_model(metrics=METRICS, size=X_train_scaled.shape[-1])\n\nclassifier_ext = KerasClassifier(model=model_ext,\n                                 epochs=EPOCHS,\n                                 batch_size=BATCH_SIZE,\n                                 callbacks=[early_stopping],\n                                 validation_split=0.1,\n                                 verbose=0)\n\nclassifier_ext.fit(X_train_scaled, y_train, class_weight=class_weight)","cae07867":"prediction_ext = model_ext.predict(X_test_scaled, batch_size=BATCH_SIZE)\nevaluation_ext = model_ext.evaluate(X_test_scaled, y_test, batch_size=BATCH_SIZE, verbose=0)\n\nfor name, value in zip(model_ext.metrics_names, evaluation_ext):\n    print(name, ': ', \"{:.4f}\".format(value))\nprint()\n\nplot_cm(y_test, prediction_ext)","e016a398":"def plot_roc(name, labels, predictions, **kwargs):\n    fp, tp, _ = roc_curve(labels, predictions)\n    \n    plt.plot(fp, tp, label=name, linewidth=2, **kwargs)\n    plt.xlabel('FP')\n    plt.ylabel('TP')\n    plt.grid(True)\n    plt.legend()\n","2bda54ce":"plt.figure(figsize = (8, 6))\nplot_roc('Base', y_test, prediction_base)\nplot_roc('Extended', y_test, prediction_ext, ls='--', color='orange')","b71ce725":"def plot_prc(name, labels, predictions, **kwargs):\n    precision, recall, _ = precision_recall_curve(labels, predictions)\n    \n    plt.plot(precision, recall, label=name, linewidth=2, **kwargs)\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.grid(True)\n    plt.legend()\n    ","dd9bac00":"plt.figure(figsize = (8, 6))\nplot_prc('Base', y_test, prediction_base)\nplot_prc('Extended', y_test, prediction_ext, ls='--', color='orange')","2756f9fa":"## Data Cleaning and Formatting\n\nThere are 74 features in this dataset, and many of them show minor usability (mostly filled with NaNs, no variance, or duplicated to another). So I manually filtered the features that I found the most useful.","f39d69ca":"## Feature selection and engineering","65d71edd":"## Compare performances under ROC and PR curve","5d31dc89":"### Ordinal Encoding ","826d28d8":"### Model setup\n\nCodes reference for model setup and evaluation : [Tensorflow Core](https:\/\/www.tensorflow.org\/tutorials\/structured_data\/imbalanced_data).","4a7b1c98":"### Format dates","3ea5d1ef":"## Setup","072d7c21":"### Credit events\n\nThe last thing to check is if there are borrowers whose official credit report was pulled after their last payment date. If so, it might indicate that they have applied for another loans or had some types of credit events\/reports, all of which might affect their existing loan repayment.","ee0450f9":"> 1.4.2 `emp_length`\n\nConvert it to a numeric form while keeping nan values.","add561ad":"This is a scatterplot between `funded_amnt` and `last_pymnt_amnt` and the red straight line is `funded_amnt` = `funded_amnt`, meaning that some borrowers pay in lumpsum to repay their outstanding loans possibly for early repayment. Suppose good borrowers(who actively repay their loans) might be willing to repay amounts more than what they are scheduled to because they want to save interest costs. Comparing `last_pymnt_amnt` and `installment`(pre-calculated monthly payment schedule) might capture this overpayment trait of good borrowers.","b368c64d":"Another way to capture the overpayment.","43c3a9d2":"### Imputations","87062952":"### Initial feature filtering","f07d63fc":"### Overpayment","380519ac":"Both ROC and PR curves clearly show that the classifier with the new dataset overperforms the previous one. The size of the new dataset is smaller than the original one, consisting of 14 features from top permutation importance scores and five newly added features. This tells how critical it is to choose the right features for model performance and how important it is to understand the industry or business from which the data is generated.\n\nThere is still plenty of room to improve the model performance by\n* finding and adding other features\n* applying different scaler (some data are highly skewed)\n* tuning model hyperparameters (grid search) ","6e03fcae":"### Overfunding\n\nThe next thing I wanted to see is how far each borrower's loan amount is from a median loan amount of each `purpose` category they belong to, hoping this might capture overfunding.","b2a3c222":"## Build another classifier with the new dataset","18d3a8ab":"## Train a simple feedforward neural network and evaluate the performance","9ec1ccc3":"### Customized formatting for other columns\n\n> 1.4.1 `home_ownership`\n\nSome of categories in this data only have a few observations (`OTHER`, `NONE`, `ANY`). Simply combine them into one.","3b5516bc":"### Format target variable\n\nThe `loan_status` is the target variable here containing ten different classes with an imbalanced number of observations. Since this project aims to build a classifier to find defaultable accounts in advance, I prefered to divide them into two groups."}}