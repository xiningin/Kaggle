{"cell_type":{"a9d90938":"code","a8aaf431":"code","b282d73f":"code","99ec13aa":"code","ad5c9ed0":"code","484ac1a1":"code","5f517088":"code","85890e1a":"code","710ddf89":"code","36abd582":"code","c13f67b3":"code","fa2bae15":"code","c31d7690":"code","b0aa3f74":"code","fe68aedb":"code","5a2d4440":"code","28113917":"code","8939343a":"code","fa3654c1":"code","f5057d60":"code","edefb5f7":"code","06b96b8c":"code","0bd631c1":"code","fcf77436":"code","e4c1f54e":"code","57f99b20":"code","5e360ba7":"code","7a5a83a1":"code","f4ea6439":"code","85f2b615":"code","f5dc7790":"code","a0daa20e":"code","81c22348":"code","14d4b014":"code","13d746c0":"code","4a30ec8e":"code","bcc0dcc2":"code","d0d6dbfa":"code","44e2cbf8":"markdown","24e7317c":"markdown","8226003c":"markdown","0b5fe5a0":"markdown","af507491":"markdown","99c04920":"markdown","62f963d1":"markdown","7dc0a102":"markdown","bd93bdc1":"markdown","4a282b49":"markdown","e6e53493":"markdown","6ddb54ef":"markdown","2beb1802":"markdown","e0622cc0":"markdown","18b865c6":"markdown","1ba9807d":"markdown","4edc14e3":"markdown","97d82392":"markdown","7dfc7c80":"markdown","f4dc9dc9":"markdown","23896e13":"markdown","d0abd054":"markdown","f7e237ff":"markdown","062e8341":"markdown","b91a72a7":"markdown","68ea5316":"markdown","62f2abd5":"markdown","ca86f7e5":"markdown","2b292ec3":"markdown"},"source":{"a9d90938":"import pandas as pd\nfrom pandas.api.types import CategoricalDtype\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\nimport scipy.stats as stats\nfrom scipy.stats import norm\nfrom scipy.special import boxcox1p\n\nimport statsmodels\nimport statsmodels.api as sm\n#print(statsmodels.__version__)\n\nimport collections\nimport math\n\nfrom sklearn.preprocessing import scale, StandardScaler, RobustScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold, StratifiedKFold, RandomizedSearchCV\nfrom sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV, LinearRegression, ElasticNet,  HuberRegressor\nfrom sklearn.metrics import mean_squared_error, balanced_accuracy_score\nfrom xgboost import XGBRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.utils import resample\n\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","a8aaf431":"train_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')","b282d73f":"train_data.head()","99ec13aa":"fig, axes = plt.subplots(1,2, figsize=(18, 6))\nsns.distplot(train_data['SalePrice'], ax=axes[0]);\nsm.qqplot(train_data['SalePrice'], stats.norm, fit=True, line='45', ax=axes[1]);","ad5c9ed0":"print('This distribution is far from normal. In particular, it has a high skew ({:.4f}).'.format(train_data['SalePrice'].skew()))\nprint('A log-transformation of Sale Price generates a distribution that is much closer to a Gaussian.')","484ac1a1":"train_data['log1pSalePrice'] = np.log1p(train_data['SalePrice'])\nfig, axes = plt.subplots(1,2, figsize=(18, 6))\nsns.distplot(train_data['log1pSalePrice'], ax=axes[0]);\naxes[0].set_xlabel('log(1+SalePrice)')\nsm.qqplot(train_data['log1pSalePrice'], stats.norm, fit=True, line='45', ax=axes[1]);","5f517088":"plt.scatter(train_data['GrLivArea'], train_data['SalePrice'], c='blue', marker='s')\nplt.title('In search of outliers...')\nplt.xlabel('GrLivArea')\nplt.ylabel('SalePrice')","85890e1a":"train_data = train_data[train_data['GrLivArea'] < 4500]\ntrain_data = train_data[train_data['SaleCondition'] == 'Normal']","710ddf89":"plt.scatter(train_data['GrLivArea'], train_data['SalePrice'], c='blue', marker='s')\nplt.title('In search of outliers...')\nplt.xlabel('GrLivArea')\nplt.ylabel('SalePrice')","36abd582":"corrmatrix = train_data.corr()\nf, ax = plt.subplots(figsize=(15,12))\nsns.heatmap(corrmatrix, vmax=0.8, square=True)","c13f67b3":"k = 10\ncols = corrmatrix.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train_data[cols].values.T)\nfig = plt.gcf()\nfig.set_size_inches(18.5, 10.5)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size':10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","fa2bae15":"Neighborhood = train_data.groupby('Neighborhood')\nNeighborhood['SalePrice'].median()","c31d7690":"ID_train = train_data['Id']\nID_test = test_data['Id']","b0aa3f74":"train_data.drop(\"Id\", axis=1, inplace=True)\ntest_data.drop(\"Id\", axis=1, inplace=True)\ntrain_data.drop(\"TotRmsAbvGrd\", axis=1, inplace=True)\ntest_data.drop(\"TotRmsAbvGrd\", axis=1, inplace=True)\ntrain_data.drop(\"GarageYrBlt\", axis=1, inplace=True)\ntest_data.drop(\"GarageYrBlt\", axis=1, inplace=True)\ntrain_data.drop(\"GarageArea\", axis=1, inplace=True)\ntest_data.drop(\"GarageArea\", axis=1, inplace=True)\ntrain_data.drop(\"1stFlrSF\", axis=1, inplace=True)\ntest_data.drop(\"1stFlrSF\", axis=1, inplace=True)","fe68aedb":"y = train_data['log1pSalePrice']\ntrain_data.drop(['SalePrice','log1pSalePrice'], axis=1, inplace=True)","5a2d4440":"#print(train_data.shape)\n#print(test_data.shape)\nntrain = train_data.shape[0]\nntest = test_data.shape[0]\nprint(\"Number of training (testing) examples: {} ({})\".format(ntrain, ntest))","28113917":"Combined_data = pd.concat([train_data, test_data]).reset_index(drop=True)\nprint(\"Thecombined dataset has size: {}\".format(Combined_data.shape))","8939343a":"total = Combined_data.isnull().sum().sort_values(ascending=False)\npercent = (Combined_data.isnull().sum())\/Combined_data.isnull().count().sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total','Percent']).sort_values('Total', ascending=False)\nmissing_data.head(40)","fa3654c1":"Combined_data.drop('PoolQC', axis=1, inplace=True)\nCombined_data.drop('MiscFeature', axis=1, inplace=True)\nCombined_data.drop(\"Alley\", axis=1, inplace=True)","f5057d60":"# Lot Frontage: NA most likely means no lot frontage\nCombined_data['LotFrontage'].fillna(0, inplace=True)\n\n# Fence: NA most likely means no fence\nCombined_data['Fence'].fillna('None', inplace=True)\n\n# FireplaceQu: NA means no fireplace\nCombined_data['FireplaceQu'].fillna('None', inplace=True)\n\n# GarageCond: NA means no garage\nCombined_data['GarageCond'].fillna('None', inplace=True)\n# GarageFinish: NA means no garage\nCombined_data['GarageFinish'].fillna('None', inplace=True)\n# GarageQual: NA means no garage\nCombined_data['GarageQual'].fillna('None', inplace=True)\n# GarageType: NA means no garage\nCombined_data['GarageType'].fillna('None', inplace=True)\n# BsmtFinType1: NA means no basement\nCombined_data[\"BsmtFinType1\"].fillna('None', inplace=True)\n# BsmtFinType2: NA means no basement\nCombined_data[\"BsmtFinType2\"].fillna('None', inplace=True)\n#BsmtExposure: NA means no basement\nCombined_data['BsmtExposure'].fillna(\"None\", inplace=True)\n#BsmtQual: NA means no basement\nCombined_data['BsmtQual'].fillna('None', inplace=True)\n#BsmtCond: NA means no basement\nCombined_data['BsmtCond'].fillna('None', inplace=True)\n# MasVnrType: NA means none\nCombined_data['MasVnrType'].fillna('None', inplace=True)\n# MasVnrAreaL NA most likely means 0\nCombined_data['MasVnrArea'].fillna(0, inplace=True)\n# MasVnrAreaL NA most likely means 0\nCombined_data['Electrical'].fillna('SBrkr', inplace=True)\n# BsmtHalfBath: NA most likely means 0\nCombined_data['BsmtHalfBath'].fillna(0, inplace=True)\n# BsmtFullBath: NA most likely means 0\nCombined_data['BsmtFullBath'].fillna(0, inplace=True)\n# BsmtFinSF1: NA most likely means 0\nCombined_data['BsmtFinSF1'].fillna(0, inplace=True)\n# BsmtFinSF2: NA most likely means 0\nCombined_data['BsmtFinSF2'].fillna(0, inplace=True)\n# BsmtUnfinSF: NA most likely means 0\nCombined_data['BsmtUnfSF'].fillna(0, inplace=True)\n# TotalBsmtSF: NA most likely means 0\nCombined_data['TotalBsmtSF'].fillna(0, inplace=True)\n# GarageCars: NA most likely means 0\nCombined_data['GarageCars'].fillna(0, inplace=True)\n# GarageArea: NA likely means 0\n# Combined_data['GarageArea'].fillna(0, inplace=True)\n# Basement Condition: NA likely means no basement\nCombined_data['Utilities'].fillna('ELO', inplace=True)\n# Basement Condition: NA likely means no basement\nCombined_data['Functional'].fillna('No', inplace=True)\n# Basement Condition: NA likely means no basement\nCombined_data['KitchenQual'].fillna('Po', inplace=True)\n# MSZoning (general Zoning Classification): RL is the most common value\nCombined_data['MSZoning'].fillna('RL', inplace=True)\n# Sale Type: fill in with the most frequent value ('WD')\nCombined_data['SaleType'].fillna('WD', inplace=True)\n\n#Exterior 1 and Exterior2: fill in with most frequent value\nCombined_data['Exterior1st'] = Combined_data['Exterior1st'].fillna(Combined_data['Exterior1st'].mode()[0])\nCombined_data['Exterior2nd'] = Combined_data['Exterior2nd'].fillna(Combined_data['Exterior2nd'].mode()[0])","edefb5f7":"Combined_data = Combined_data.replace(\n    {\n        \"MSSubClass\": {20: \"SC20\", 30: \"SC30\" , 40: \"SC40\", 50: \"SC50\", 60: \"SC60\", 70: \"SC70\", 80: \"SC80\",\n                        90: \"SC90\", 100: \"SC100\", 110: \"SC110\", 120: \"SC120\", 130: \"SC130\", 140: \"SC140\", 150: \"SC150\", 160: \"SC160\",\n                         170: \"SC170\", 180: \"SC180\", 190: \"SC190\"},\n        \"MoSold\":{1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun', 7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'}\n    }\n)","06b96b8c":"Combined_data.BsmtCond = pd.factorize(Combined_data.BsmtCond.astype(CategoricalDtype(categories=[\"None\", \"Po\", \"Fa\", \"TA\", 'Gd', \"Ex\"], ordered=True)))[0]\nCombined_data.BsmtExposure = pd.factorize(Combined_data.BsmtExposure.astype(CategoricalDtype(categories=['None', 'No', \"Mn\", \"Av\", \"Gd\"], ordered=True)))[0]\nCombined_data.BsmtFinType1 = pd.factorize(Combined_data.BsmtFinType1.astype(CategoricalDtype(categories=['None', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'], ordered=True)))[0]\nCombined_data.BsmtFinType2 = pd.factorize(Combined_data.BsmtFinType2.astype(CategoricalDtype(categories=['None', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'], ordered=True)))[0]\nCombined_data.BsmtQual = pd.factorize(Combined_data.BsmtQual.astype(CategoricalDtype(categories=[\"None\", \"Po\", \"Fa\", \"TA\", 'Gd', \"Ex\"], ordered=True)))[0]\nCombined_data.ExterCond = pd.factorize(Combined_data.ExterCond.astype(CategoricalDtype(categories=[\"None\", \"Po\", \"Fa\", \"TA\", 'Gd', \"Ex\"], ordered=True)))[0]\nCombined_data.ExterQual = pd.factorize(Combined_data.ExterQual.astype(CategoricalDtype(categories=[\"None\", \"Po\", \"Fa\", \"TA\", 'Gd', \"Ex\"], ordered=True)))[0]\nCombined_data.FireplaceQu = pd.factorize(Combined_data.FireplaceQu.astype(CategoricalDtype(categories=[\"None\", \"Po\", \"Fa\", \"TA\", 'Gd', \"Ex\"], ordered=True)))[0]\nCombined_data.Functional = pd.factorize(Combined_data.Functional.astype(CategoricalDtype(categories=['No','Sal', 'Sev', 'Maj2', 'Maj1', 'Mod', 'Min2', 'Min1', 'Typ'], ordered=True)))[0]\nCombined_data.GarageCond = pd.factorize(Combined_data.GarageCond.astype(CategoricalDtype(categories=['None', 'Po', 'Fa', 'TA', \"Gd\", \"Ex\"], ordered=True)))[0]\nCombined_data.GarageFinish = pd.factorize(Combined_data.GarageFinish.astype(CategoricalDtype(categories=['None', 'Unf', 'RFn', 'Fin'], ordered=True)))[0]\nCombined_data.GarageQual = pd.factorize(Combined_data.GarageQual.astype(CategoricalDtype(categories=[\"None\", \"Po\", \"Fa\", \"TA\", 'Gd', \"Ex\"], ordered=True)))[0]\nCombined_data.HeatingQC = pd.factorize(Combined_data.HeatingQC.astype(CategoricalDtype(categories=[\"Po\", \"Fa\", \"TA\", 'Gd', \"Ex\"], ordered=True)))[0]\nCombined_data.KitchenQual = pd.factorize(Combined_data.KitchenQual.astype(CategoricalDtype(categories=[\"Po\", \"Fa\", \"TA\", 'Gd', \"Ex\"], ordered=True)))[0]\nCombined_data.LandSlope = pd.factorize(Combined_data.LandSlope.astype(CategoricalDtype(categories=['Sev', \"Mod\", \"Gtl\"], ordered=True)))[0]\nCombined_data.LotShape = pd.factorize(Combined_data.LotShape.astype(CategoricalDtype(categories=['IR3', \"IR2\", \"IR1\", \"Reg\"], ordered=True)))[0]\nCombined_data.PavedDrive = pd.factorize(Combined_data.PavedDrive.astype(CategoricalDtype(categories=['N', \"P\", \"Y\"], ordered=True)))[0]\nCombined_data.Street = pd.factorize(Combined_data.Street.astype(CategoricalDtype(categories=['Grvl', \"Pave\"], ordered=True)))[0]\nCombined_data.Utilities = pd.factorize(Combined_data.Utilities.astype(CategoricalDtype(categories=['ELO', \"NoSeWa\", \"NoSewr\", \"AllPub\"], ordered=True)))[0]","0bd631c1":"Combined_data['Total_Home_Quality'] = Combined_data['OverallQual'] + Combined_data['OverallCond']\nCombined_data['Total_Basement_Quality'] = Combined_data['BsmtQual'] + Combined_data['BsmtCond']\nCombined_data['Total_Basement_Finished_SqFt'] = Combined_data['BsmtFinSF1'] + Combined_data['BsmtFinSF2']\nCombined_data['Total_Exterior_Quality'] = Combined_data['ExterQual'] + Combined_data['ExterCond']\nCombined_data['Total_Garage_Quality'] = Combined_data['GarageCond'] + Combined_data['GarageQual'] + Combined_data['GarageFinish']\nCombined_data['Total_Basement_Finish_Type'] = Combined_data['BsmtFinType1'] + Combined_data['BsmtFinType2'] \nCombined_data['Total_Bathrooms'] = Combined_data['BsmtFullBath'] + (0.5 * Combined_data['BsmtHalfBath']) + Combined_data['FullBath'] + (0.5 * Combined_data['HalfBath'])\nCombined_data['Total_Land_Quality'] = Combined_data['LandSlope'] + Combined_data['LotShape']\n\nCombined_data.drop(['OverallQual','OverallCond','BsmtQual', 'BsmtCond', 'BsmtFinSF1',\n                    'BsmtFinSF2', 'ExterQual', 'ExterCond', 'GarageCond', 'GarageQual', 'GarageFinish', 'BsmtFinType1', \n                    'BsmtFinType2', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'LandSlope', 'LotShape'], \n                   axis=1, inplace=True)","fcf77436":"Combined_data = Combined_data.replace({'Neighborhood':{\n        \"MeadowV\" : 0,\"IDOTRR\" : 0, \"BrDale\" : 0, \"OldTown\" : 0, \"Edwards\" : 0, \"BrkSide\" : 0, \"Sawyer\" : 0, \n        \"Blueste\" : 1, \"SWISU\" : 1, \"NAmes\" : 1, \"NPkVill\" : 1, \"Mitchel\" : 1, \"SawyerW\" : 1,\n        \"Gilbert\" : 2, \"NWAmes\" : 2, \"Blmngtn\" : 2, \"CollgCr\" : 2, \"ClearCr\" : 2, \"Crawfor\" : 2, \n        \"Veenker\" : 3, \"Somerst\" : 3, \"Timber\" : 3, \"StoneBr\" : 3, \"NoRidge\" : 3, \"NridgHt\" : 3} })","e4c1f54e":"total = Combined_data.isnull().sum().sort_values(ascending=False)\npercent = (Combined_data.isnull().sum())\/Combined_data.isnull().count().sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total','Percent']).sort_values('Total', ascending=False)\nmissing_data.head(10)","57f99b20":"Skewed_Feature_Check = ['LotArea', 'MasVnrArea', 'BsmtUnfSF', 'TotalBsmtSF', 'LowQualFinSF', 'GrLivArea', 'WoodDeckSF',\n                       'OpenPorchSF', 'PoolArea','MiscVal', 'Total_Basement_Finished_SqFt']\nn = len(Skewed_Feature_Check)\n#fig, ax = plt.subplots(3, 4, figsize=(36, 16))\n\nfor i  in range(n):\n    feature = Skewed_Feature_Check[i]\n    #sns.distplot(Combined_data[feature], kde=False, fit=norm, ax = ax[i][0]) \n    #sm.qqplot(Combined_data[feature], stats.norm, fit=True, line='45', ax=ax[i%3][i\/\/3], label = feature);\n    #ax[i%3][i\/\/3].legend(fontsize=12)\n    print('{:>15}: {:.6g}        {}'.format(feature, stats.skew(Combined_data[feature]), stats.skewtest(Combined_data[feature])))\n    #from scipy.special import boxcox1p\n    #lam = 0.15\n    #Combined_data[feature] = boxcox1p(Combined_data[feature], lam)\n    Combined_data[feature] = np.log1p(Combined_data[feature])\n    #sns.distplot(Combined_data[feature], kde=False, fit=stats.norm, ax = ax[i][1]) \n    #sm.qqplot(Combined_data[feature], stats.norm, fit=True, line='45', ax=ax[i][1]);","5e360ba7":"categorical_features = Combined_data.select_dtypes(include=[\"object\"]).columns\nnumerical_features = Combined_data.select_dtypes(exclude=[\"object\"]).columns\nprint('Numerical features: {}'.format(numerical_features.shape[0]))\nprint('Categorical features: {}'.format(categorical_features.shape[0]))\n\nCombined_data_numerical = Combined_data[numerical_features]\nCombined_data_categorical = Combined_data[categorical_features]","7a5a83a1":"corrmatrix_combined = Combined_data_numerical.corr()\nf, ax = plt.subplots(figsize=(12,9))\nsns.heatmap(corrmatrix_combined, vmax=.8, square=True)","f4ea6439":"Combined_data_categorical = pd.get_dummies(Combined_data_categorical, drop_first=True)\nCombined_data = pd.concat([Combined_data_categorical, Combined_data_numerical], axis=1)","85f2b615":"train_data = Combined_data[:ntrain]\ntest_data = Combined_data[ntrain:]\ntest_data = test_data.reset_index(drop=True)\nprint(\"Number of training (testing) examples: {} ({})\".format(train_data.shape[0], test_data.shape[0]))\nprint(\"Size of combined dataset is  {}\".format(Combined_data.shape))","f5dc7790":"X_train, X_val, y_train, y_val = train_test_split(train_data, y, test_size=0.2, random_state=1)\nprint('X_train : {}'.format(X_train.shape))\nprint('X_val : {}'.format(X_val.shape))\nprint('y_train : {}'.format(y_train.shape))\nprint('X_val : {}'.format(y_val.shape))","a0daa20e":"scaler = RobustScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\ntest_data = scaler.transform(test_data)","81c22348":"n_folds = 5\n\ndef rmse_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state = 91).get_n_splits(X_train)\n    return cross_val_score(model, X_train, y_train, scoring='neg_mean_squared_error', cv=kf)","14d4b014":"# run Cross-Validation Score on a basic model with no parameter Tuning\n\nfor Model in [LinearRegression, Ridge, Lasso, XGBRegressor, ElasticNet, RandomForestRegressor,  HuberRegressor, GaussianProcessRegressor, SVR, KernelRidge]:\n    if Model == XGBRegressor: cv_res = rmse_cv(XGBRegressor(objective='reg:squarederror'))\n    else: cv_res = rmse_cv(Model())\n    print('{}: {:.5f} +\/- {:5f}'.format(Model.__name__, -cv_res.mean(), cv_res.std()))","13d746c0":"alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\ncv_ridge = [-rmse_cv(Ridge(alpha = alpha)).mean() \n            for alpha in alphas]","4a30ec8e":"cv_ridge = pd.Series(cv_ridge, index = alphas)\ncv_ridge.plot(title = \"Ridge Regression Cross-Validation\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")\n\nRR_best = Ridge(alpha = np.argmin(cv_ridge))\nRR_best.fit(X_train, y_train)\npredicted_prices = RR_best.predict(test_data)\n\nmy_submission = pd.DataFrame({'Id': ID_test, 'SalePrice': predicted_prices})\nmy_submission.to_csv('submission.csv', index=False)","bcc0dcc2":"alphas = [0.0001,0.0005,0.001, 0.005, 0.01,0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\ncv_lasso = [-rmse_cv(Lasso(alpha = alpha)).mean() \n            for alpha in alphas]","d0d6dbfa":"cv_lasso = pd.Series(cv_lasso, index = alphas)\ncv_lasso.plot(title = \"Lasso Regression Cross-Validation\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")\nplt.xscale('log')\n\nLASSO_best = Lasso(alpha = np.argmin(cv_ridge))\nLASSO_best.fit(X_train, y_train)\npredicted_prices = LASSO_best.predict(test_data)\n\nmy_submission = pd.DataFrame({'Id': ID_test, 'SalePrice': predicted_prices})\nmy_submission.to_csv('submission_LASSO.csv', index=False)","44e2cbf8":"There are three columns where most of the data is missing and it's irrelevant to the response (Sale Price of a home). We drop these.","24e7317c":"We will now deal with missing values. We first look at the numbers of missing values for each feature.","8226003c":"We are looking to remove multicollinearity at this stage. There are four sets of variables which contain roughly the same information:\n1. TotalBsmntSF and 1stFlrSF. High correlation because basement sits below 1st floor.\n2. GarageCars and GarageArea. Very high correlation because the bigger the area the more cars can fit in.\n3. GarageYrBlt and YearBuilt. Very highly correlated - usually the garage is built at the same time as the house.\n4. TotRmsAbvGrd and GrLivArea. Very highly correlated. The more area the more rooms there are.\n\n We drop one variable from each pairing, the one whose correlation with the response is smaller.","0b5fe5a0":"# EDA (Part I)","af507491":"We now look at the correlation matrix of numerical predictors.","99c04920":"Dataset webpage: https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques","62f963d1":"### Cross-validation scoring","7dc0a102":"We start by looking at how the response (Sale Price) is distributed.","bd93bdc1":"We see that there are a few outliers: houses with large living area that sold at a low price. We remove these points. We also focus on Normal Sales only. With these modifications, we replot the relationship between Sale Price and Living Area.","4a282b49":"We drop the ID column since we don't need it. We save it, however, since it will be needed later.","e6e53493":"We will bin the Neighborhood variable by median SalePrice.","6ddb54ef":"We now split into categorical and numerical features.","2beb1802":"Conversely, there are some featurees encoded as nominal categories even though their ordering carries information. We switch them back to an ordinal category representation.","e0622cc0":"We check for skew in the distributions of continous numerical features. If the distribution is skewed we normalize the variable with a log transformation.","18b865c6":"Finally we will bin the 'Neighborhood' feature into quartiles based on 'SalePrice'.","1ba9807d":"In a number of categories, there is an obvious way to impute a missing value. For example, NA in the 'GarageFinish' column likely means that the propery doesn't have a garage.","4edc14e3":"### Distribution of the response","97d82392":"### Import data","7dfc7c80":"### Ridge Regression","f4dc9dc9":"### LASSO","23896e13":"# MODELLING","d0abd054":"### SalePrice vs. Living Area","f7e237ff":"At this point we separately save the log-transformed SalePrice column and drop it from the features dataframe.","062e8341":"Now we create new features. For example, we define a 'Total_Home_Quality' feature which is the sum of 'OverallQual'  and 'OverallCond'. To avoid multicollinearity, we drop the individual components of the combination from the dataframe.","b91a72a7":"We are going to combine the training and test data in order to do cleaning. For this reason we need to get the number of records in each set and set a variable that identifies the split point of the combined dataset.","68ea5316":"### Import libraries","62f2abd5":"We check that we have successfully dealt with all missing values. This is the case.","ca86f7e5":"Next we look at the relationship between Sale Price and Living Area. Typically a more expensive house is sold for more.","2b292ec3":"We notice that some features are encoded as ordinal categories even though there is no meaning to their ordering. We switch them back to a nominal category representation."}}