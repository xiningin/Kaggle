{"cell_type":{"64baecbd":"code","121836f8":"code","d7df35b1":"code","221c6874":"code","fee19168":"code","818dc7b3":"code","baae07b8":"code","8fe406c1":"code","7bb90b27":"code","2312daaf":"code","8c284c9b":"code","2d0f2ff0":"code","6586deba":"code","6a582735":"code","d9d58310":"code","df28b844":"code","d0394712":"code","508a90b5":"code","a15d8b39":"code","bc241fd8":"code","80fbf097":"code","889fb1ae":"code","9c569146":"code","64d96e43":"code","8971fbf7":"code","ce71e4c5":"code","9a7fb94a":"code","7dd414bf":"code","4773dac2":"code","194eb03e":"code","691c6c73":"code","fc5657a6":"code","08590527":"code","6fa372e8":"code","1e9572b8":"code","9ec0de8d":"code","72948795":"code","7cbcaebf":"code","4cae7a1f":"code","a44c1cb3":"code","b263cc7a":"code","30425741":"code","d54afb4e":"code","42aa3ae1":"markdown","98ee516b":"markdown","ba125bbb":"markdown","1a41cba7":"markdown","cee807d7":"markdown","8cbc385c":"markdown","01ab37a9":"markdown","e6fa5bfe":"markdown","51fadeee":"markdown","7bc9fcd9":"markdown","e64532de":"markdown","e09e5fb0":"markdown","9053b6fd":"markdown","63fbbf82":"markdown","9a5f5214":"markdown","9b2e5766":"markdown","089a03cc":"markdown","696d4e51":"markdown","d3531153":"markdown","a2735683":"markdown","c681198f":"markdown","17782387":"markdown","1b592663":"markdown","f427bc86":"markdown"},"source":{"64baecbd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","121836f8":"df = pd.read_csv('..\/input\/heart.csv')","d7df35b1":"df.head()","221c6874":"df.describe()","fee19168":"df.isna().any()","818dc7b3":"df.info()","baae07b8":"# Check the count of with disease and without disease\nfig, ax = plt.subplots(figsize=(6.,5.))\nsns.countplot(x= 'target', data=df, palette='Accent')","8fe406c1":"# use the pair plot try to find the inner relationship\nsns.pairplot(data = df)","7bb90b27":"# The disease rate difference between genders\nplt.figure(figsize=(12, 8))\nsns.countplot(x = 'target', hue='sex', data = df,palette='bwr')\nplt.xlabel(\"Sex (0 = female, 1= male)\")\nplt.title('Heart Frequency for Sex')\nplt.legend(['No Disease', 'Disease'])\nplt.ylabel('Frequency')","2312daaf":"# what about the age\nplt.figure(figsize=(25,8), dpi=100)\nsns.countplot(x = 'age', hue='target', data=df)\nplt.title('Heart Disease Frequency for Ages')\nplt.xticks(rotation=0)\nplt.xlabel('Age')\nplt.ylabel('Frequency')","8c284c9b":"# chest pain type\nprint(\"There are {} types of chest pain\".format(len(df[\"cp\"].unique())))","2d0f2ff0":"plt.figure(figsize=(12, 8))\nsns.countplot(x =\"cp\", hue= \"target\", data=df)\nplt.title(\"Different chest type and thier disease count\")\nplt.legend(['No disease', 'Disease'])\nplt.xlabel(\"Chest pain type\")","6586deba":"df.groupby('target')['trestbps'].mean()","6a582735":"print(\"With disease, the average blood pressure is {}\".format(df.groupby('target')['trestbps'].mean()[1]))\nprint(\"Normal, the average blood pressure is {}\".format(df.groupby('target')['trestbps'].mean()[0]))","d9d58310":"# The blood pressuer distribution\nplt.figure(figsize=(8, 8))\nsns.violinplot(x = 'target', y ='trestbps' ,data = df)\nplt.title(\"Blood pressure difference\")\nplt.ylabel(\"Resting blood pressure\")\nplt.xlabel(\"Target (0 = No disease, 1= Disease)\")","df28b844":"print(\"With disease, the average blood pressure is {}\".format(df.groupby('target')['thalach'].mean()[1]))\nprint(\"Normal, the average blood pressure is {}\".format(df.groupby('target')['thalach'].mean()[0]))","d0394712":"# The blood pressuer distribution\nplt.figure(figsize=(6,7))\nsns.violinplot(x = 'target', y ='thalach' ,data = df)\nplt.title(\"Maximun heart rate difference\")\nplt.ylabel(\"Maximum heart rate\")\nplt.xlabel(\"Target (0 = No disease, 1= Disease)\")","508a90b5":"sns.scatterplot(x = 'age', y = 'thalach',hue='target',data = df, palette='bwr')\nplt.xlabel(\"Age\")\nplt.ylabel(\"Maximum Heart Rate\")\nplt.show()","a15d8b39":"# handle the dummy data, there are three dummy datas: cp, slope, thal\na = pd.get_dummies(df['cp'], prefix='cp')\nb = pd.get_dummies(df['slope'], prefix='slope')\nc = pd.get_dummies(df['thal'], prefix='thal')\n\n# new frame\nframes = [df, a, b, c]\ndf_dummyed = pd.concat(frames, axis=1)\ndf_dummyed.drop(['cp', 'slope', 'thal'], axis=1, inplace= True)","bc241fd8":"from sklearn.preprocessing import StandardScaler","80fbf097":"scaler = StandardScaler()\nscaler.fit(df_dummyed.drop(['target','cp_0', 'cp_1', 'cp_2', 'cp_3', 'thal_0',\n       'thal_1', 'thal_2', 'thal_3', 'slope_0', 'slope_1', 'slope_2'], axis=1))\n","889fb1ae":"scaled_features = scaler.transform(df_dummyed.drop(['target','cp_0', 'cp_1', 'cp_2', 'cp_3', 'thal_0',\n       'thal_1', 'thal_2', 'thal_3', 'slope_0', 'slope_1', 'slope_2'], axis=1))","9c569146":"df_feat = pd.DataFrame(scaled_features, columns=df_dummyed.columns[:-12])\ndf_feat = df_feat.join(df_dummyed[['cp_0', 'cp_1', 'cp_2', 'cp_3', 'thal_0',\n       'thal_1', 'thal_2', 'thal_3', 'slope_0', 'slope_1', 'slope_2']])\ndf_feat.head()","64d96e43":"from sklearn.model_selection import train_test_split","8971fbf7":"X_train, X_test, y_train, y_test = train_test_split(df_feat, df['target'], test_size= 0.20, random_state=0)","ce71e4c5":"from sklearn.metrics import confusion_matrix, classification_report","9a7fb94a":"precisions = [] \nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train,y_train)\nprint(\"Test Accuracy {:.2f}%\".format(lr.score(X_test,y_test)*100))\nprecisions.append(lr.score(X_test,y_test)*100)","7dd414bf":"pred_y = lr.predict(X_test)\nprint(\"Classification report:\\n\")\nprint(classification_report(y_test, pred_y))\n\nprint(\"Confusion matrix:\\n\")\nprint(confusion_matrix(y_test, pred_y))","4773dac2":"# KNN Model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 2)  # n_neighbors means k\nknn.fit(X_train, y_train)\nprediction = knn.predict(X_test)\n\nprint(\"{} NN Score: {:.2f}%\".format(2, knn.score(X_test, y_test)*100))","194eb03e":"# try ro find best k value\nscoreList = []\nfor i in range(1,20):\n    knn2 = KNeighborsClassifier(n_neighbors = i)  # n_neighbors means k\n    knn2.fit(X_train, y_train)\n    scoreList.append(knn2.score(X_test, y_test))\n    \nplt.plot(range(1,20), scoreList)\nplt.xticks(np.arange(1,20,1))\nplt.xlabel(\"K value\")\nplt.ylabel(\"Score\")\nplt.show()\n\n\nprint(\"Maximum KNN Score is {:.2f}%\".format((max(scoreList))*100))","691c6c73":"# KNN Model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 10)  # n_neighbors means k\nknn.fit(X_train, y_train)\nprediction = knn.predict(X_test)\n\nprint(\"{} NN Score: {:.2f}%\".format(2, knn.score(X_test, y_test)*100))\nprecisions.append(knn.score(X_test,y_test)*100)","fc5657a6":"pred_y = knn.predict(X_test)\nprint(\"Classification report:\\n\")\nprint(classification_report(y_test, pred_y))\n\nprint(\"Confusion matrix:\\n\")\nprint(confusion_matrix(y_test, pred_y))","08590527":"from sklearn.svm import SVC\nsvm = SVC(random_state = 1)\nsvm.fit(X_train, y_train)\nprint(\"Test Accuracy of SVM Algorithm: {:.2f}%\".format(svm.score(X_test,y_test)*100))\nprecisions.append(svm.score(X_test,y_test)*100)","6fa372e8":"pred_y = svm.predict(X_test)\nprint(\"Classification report:\\n\")\nprint(classification_report(y_test, pred_y))\n\nprint(\"Confusion matrix:\\n\")\nprint(confusion_matrix(y_test, pred_y))","1e9572b8":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train, y_train)\nprint(\"Accuracy of Naive Bayes: {:.2f}%\".format(nb.score(X_test,y_test)*100))\nprecisions.append(nb.score(X_test,y_test)*100)","9ec0de8d":"pred_y = nb.predict(X_test)\nprint(\"Classification report:\\n\")\nprint(classification_report(y_test, pred_y))\n\nprint(\"Confusion matrix:\\n\")\nprint(confusion_matrix(y_test, pred_y))","72948795":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train, y_train)\nprint(\"Decision Tree Test Accuracy {:.2f}%\".format(dtc.score(X_test, y_test)*100))\nprecisions.append(dtc.score(X_test, y_test)*100)","7cbcaebf":"pred_y = dtc.predict(X_test)\nprint(\"Classification report:\\n\")\nprint(classification_report(y_test, pred_y))\n\nprint(\"Confusion matrix:\\n\")\nprint(confusion_matrix(y_test, pred_y))","4cae7a1f":"# Random Forest Classification\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 1000, random_state = 1)\nrf.fit(X_train, y_train)\nprint(\"Random Forest Algorithm Accuracy Score : {:.2f}%\".format(rf.score(X_test,y_test)*100))\nprecisions.append(rf.score(X_test,y_test)*100)","a44c1cb3":"pred_y = rf.predict(X_test)\nprint(\"Classification report:\\n\")\nprint(classification_report(y_test, pred_y))\n\nprint(\"Confusion matrix:\\n\")\nprint(confusion_matrix(y_test, pred_y))","b263cc7a":"from sklearn.ensemble import AdaBoostClassifier\nabc = AdaBoostClassifier(n_estimators=100)\nabc.fit(X_train, y_train)\nprint(\"AdaBoost Accuracy Score : {:.2f}%\".format(abc.score(X_test,y_test)*100))\nprecisions.append(abc.score(X_test,y_test)*100)","30425741":"pred_y = rf.predict(X_test)\nprint(\"Classification report:\\n\")\nprint(classification_report(y_test, pred_y))\n\nprint(\"Confusion matrix:\\n\")\nprint(confusion_matrix(y_test, pred_y))","d54afb4e":"methods = [\"Logistic Regression\", \"KNN\", \"SVM\", \"Naive Bayes\", \"Decision Tree\", \"Random Forest\", \"Adaboost\"]\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,5))\nplt.yticks(np.arange(0,100,10))\nplt.ylabel(\"Accuracy %\")\nplt.xlabel(\"Algorithms\")\nsns.barplot(x=methods, y=precisions, palette=\"gnuplot\")\nplt.show()","42aa3ae1":"### Support Vector Machine (SVM) Algorithm\n* \u652f\u6301\u5411\u91cf\u673a\u4ec5\u652f\u6301\u6570\u503c\u7c7b\u6570\u636e\n* \u4ec5\u652f\u6301\u4e8c\u5143\u5206\u7c7b","98ee516b":"[](http:\/\/) \n# Introduction\n\nMachine learning is good at find some latent regular in the data. In this data, there're many features, and a target now.  So a lot of machine learning tools are good at do the prediction. \nHowever, we need to do the data exploration, and fit the data to machine learning tools.","ba125bbb":"# What causes Heart Diseaze?","1a41cba7":"> # Conclusion","cee807d7":"### BOOM\uff01 Why not try some boosting method","8cbc385c":"### Chest pain type influence","01ab37a9":"Resting blood pressure is not very important","e6fa5bfe":"> # Machine Learning","51fadeee":"It seems there is no obvious distribution between two variables. So we need to do some analysis more specificly.","7bc9fcd9":"### Maxium heart rate","e64532de":"> ### Column introduction\n* age:    age in years\n* sex:  (1 = male; 0 = female)\n* cp:   chest pain type\n* trestbps:   resting blood pressure (in mm Hg on admission to the hospital)\n* chol:   serum cholestoral in mg\/dl\n* fbs:    (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n* restecg:     resting electrocardiographic results\n* thalach:   maximum heart rate achieved\n* exang:   exercise induced angina (1 = yes; 0 = no)\n* oldpeak:    ST depression induced by exercise relative to rest\n* slope:  the slope of the peak exercise ST segment\n* ca:       number of major vessels (0-3) colored by flourosopy\n* thal:     3 = normal; 6 = fixed defect; 7 = reversable defect\n* target: 1 or 0","e09e5fb0":"# Contents\n\n1. [Introduction ](#section1)\n2. [Load Data ](#section2)\n3. [Data Exploration](#section3)\n4. [The Explanation](#section4)\n5. [Conclusion](#section5)","9053b6fd":"> # handle the dummy variables and standarlize the data\nWe need to standarlize the feature for the purpose of training","63fbbf82":"### resting blood pressure","9a5f5214":"### Naive Bayes Algorithm","9b2e5766":"* The data has 14 columns, 13 columns are the features, and 1 column is the target.","089a03cc":"Age between 40 and 60 have the most possibility to have heart disease.","696d4e51":"### K-Nearest Neighbour (KNN) Classification","d3531153":"### Decision Tree","a2735683":"### Random forest","c681198f":"### Logistic Regression\n\u6ce8\u610f\uff1a\u56e0\u53d8\u91cf\u548c\u6b8b\u5dee\u90fd\u8981\u7b26\u5408\u4e8c\u9879\u5206\u5e03\uff0c\u624d\u80fd\u4f7f\u7528logistic regression","17782387":"[](http:\/\/) # Load Data\n* Load the data from the csv file","1b592663":"> [](http:\/\/) #Data Exploration","f427bc86":"Female seems to have more possibility to suffer from heart disease."}}