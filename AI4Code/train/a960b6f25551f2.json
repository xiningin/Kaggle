{"cell_type":{"c4d888df":"code","7da46ae7":"code","0128864b":"code","384fd397":"code","1929c90c":"code","f2932c31":"code","6ba4315b":"code","0f1b428e":"code","5a19fee4":"code","32362a82":"code","d141c31f":"code","3b6d62e7":"code","8d0e3abe":"code","4f4122a7":"code","3a559c9d":"code","2d166287":"code","4efb8a0b":"code","e2534161":"code","4fbd55cd":"code","e88891bc":"code","665a7a57":"code","e2831566":"code","92311932":"code","e0c55b7c":"code","f892cf9d":"code","b8e6392c":"code","4af7af04":"code","756b9f70":"code","c9f0a240":"code","08c781d9":"code","94b129b8":"code","d7f319b2":"code","c277202f":"code","8189d880":"code","3a09965c":"code","63900091":"code","33b26879":"code","dd1bb0eb":"code","378ed79f":"code","6df8038e":"code","b05fb542":"code","06760d2b":"code","ac2f60a8":"code","bb6ceb37":"code","178a7dce":"code","fc1a9baa":"code","c0e4809e":"code","47b2f5bd":"code","81a31eb7":"code","bb845867":"code","12dd132f":"code","a45e144b":"code","10c48689":"code","5f57ac45":"code","27a775ca":"code","b5afbac7":"code","92b8c58b":"markdown","34836852":"markdown","d68699d0":"markdown","ea5e6baf":"markdown","6659b9f7":"markdown","d158f0e8":"markdown","a6afc8f2":"markdown","ece41a72":"markdown","8f7d7664":"markdown","474ecd1c":"markdown","647a326c":"markdown","9f0d4c29":"markdown","0a8cb9db":"markdown","e9ef0384":"markdown","a0a2500b":"markdown","48fedf79":"markdown","2fb7ec48":"markdown","aa5fbda0":"markdown","d998eec6":"markdown","26c4cd33":"markdown","4f8b50e6":"markdown","125fb8ef":"markdown","101dcb7c":"markdown","7692a240":"markdown","46a41ef8":"markdown","dec0148b":"markdown","9f728fc1":"markdown","64ac8edc":"markdown","acde2f80":"markdown","3d5b1115":"markdown","97609b1d":"markdown","f2936fe1":"markdown"},"source":{"c4d888df":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7da46ae7":"import numpy as np\nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n\n# Ignoring annoying warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# some stats stuff\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n# Models\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\n\n# Misc\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\n\n# Setting pandas display property\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n# limiting floats output to 3 decimal points\npd.set_option('display.float_format', lambda x:'{:.3f}'.format(x)) ","0128864b":"# Importing train and test dataset\nTRAIN_PATH = '\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv'\nTEST_PATH = '\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv'\n\ntrain = pd.read_csv(TRAIN_PATH)\ntest = pd.read_csv(TEST_PATH)","384fd397":"# display the first five rows of the train dataset\ntrain.head(5)","1929c90c":"# display the first five rows of the test dataset\ntest.head(5)","f2932c31":"print(\"The train data size is: {}\".format(train.shape))\nprint(\"The test data size is: {}\".format(test.shape))","6ba4315b":"# saving the Id column\ntrain_ID = train['Id']\ntest_ID = train['Id']\n\n# dropping the Id column since it's unnecessary for prediction purpose\ntrain.drop(\"Id\", axis=1, inplace=True)\ntest.drop(\"Id\", axis=1, inplace=True)","0f1b428e":"sns.scatterplot(x='GrLivArea', y='SalePrice', data=train)","5a19fee4":"#Removing outliers\ntrain.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index, inplace=True)\ntrain.drop(train[(train['OverallQual']<5) & (train['SalePrice']>200000)].index, inplace=True)\n\n# checking the plot again\nsns.scatterplot(x='GrLivArea', y='SalePrice', data=train)","32362a82":"sns.distplot(train['SalePrice'], fit=norm)\n\n#Get the fitted parameters\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint(f\"mu = {mu} and sigma = {sigma}\")\n\n#Plotting the Q-Q plot\nfig = plt.figure()\nstats.probplot(train['SalePrice'], plot=plt)\nplt.show()","d141c31f":"train['SalePrice'] = np.log1p(train['SalePrice'])\n\n# checking the new distribution\nsns.distplot(train['SalePrice'], fit=norm)\n\n# getting the fitted parameters\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint(f\"mu = {mu} and sigma = {sigma}\")\n\n# plotting the Q-Q plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","3b6d62e7":"# saving the labels of training dataset\ny_train = train['SalePrice'].values\n\n# cocantenating train and test data\ndata = pd.concat((train,test)).reset_index(drop=True)\ndata.drop(['SalePrice'], axis=1, inplace=True)","8d0e3abe":"# size of the combined data\ndata.shape","4f4122a7":"# determine the threshold for missing values\ndef percent_missing(df):\n    data = pd.DataFrame(df)\n    df_cols = list(pd.DataFrame(data))\n    dict_x = {}\n    for i in range(0, len(df_cols)):\n        dict_x.update({df_cols[i]: round(data[df_cols[i]].isnull().mean()*100,2)})\n    \n    return dict_x\n\nmissing = percent_missing(data)\ndf_miss = sorted(missing.items(), key=lambda x: x[1], reverse=True)\nprint('Percent of missing data')\ndf_miss[0:10]","3a559c9d":"import missingno as msno\n\nmsno.matrix(data)\nmsno.heatmap(data)","2d166287":"# barplot of missing values\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nsns.set_color_codes(palette='deep')\nmissing = round(train.isnull().mean()*100,2)\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar(color=\"b\")\n# Tweak the visual presentation\nax.xaxis.grid(False)\nax.set(ylabel=\"Percent of missing values\")\nax.set(xlabel=\"Features\")\nax.set(title=\"Percent missing data by feature\")\nsns.despine(trim=True, left=True)","4efb8a0b":"# checking the datatype\ndata.dtypes","e2534161":"# some of the non-numeric predictors are stored as numbers, convert them into strings\ndata['MSSubClass'] = data['MSSubClass'].apply(str)\ndata['YrSold'] = data['YrSold'].apply(str)\ndata['MoSold'] = data['MoSold'].apply(str)","4fbd55cd":"def handle_missing(features):\n    # the data description states that NA refers to typical ('Typ') values\n    features['Functional'] = features['Functional'].fillna('Typ')\n    # Replace the missing values in each of the columns below with their mode\n    features['Electrical'] = features['Electrical'].fillna(\"SBrkr\")\n    features['KitchenQual'] = features['KitchenQual'].fillna(\"TA\")\n    features['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])\n    features['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\n    features['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\n    features['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n    \n    # the data description stats that NA refers to \"No Pool\"\n    features[\"PoolQC\"] = features[\"PoolQC\"].fillna(\"None\")\n    # Replacing the missing values with 0, since no garage = no cars in garage\n    for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n        features[col] = features[col].fillna(0)\n    # Replacing the missing values with None\n    for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n        features[col] = features[col].fillna('None')\n    # NaN values for these categorical basement features, means there's no basement\n    for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n        features[col] = features[col].fillna('None')\n        \n    # Group the by neighborhoods, and fill in missing value by the median LotFrontage of the neighborhood\n    features['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n    # We have no particular intuition around how to fill in the rest of the categorical features\n    # So we replace their missing values with None\n    objects = []\n    for i in features.columns:\n        if features[i].dtype == object:\n            objects.append(i)\n    features.update(features[objects].fillna('None'))\n        \n    # And we do the same thing for numerical features, but this time with 0s\n    numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    numeric = []\n    for i in features.columns:\n        if features[i].dtype in numeric_dtypes:\n            numeric.append(i)\n    features.update(features[numeric].fillna(0))    \n    return features\n\ndata = handle_missing(data)","e88891bc":"# Let's make sure we handled all the missing values\nmissing = percent_missing(data)\ndf_miss = sorted(missing.items(), key=lambda x: x[1], reverse=True)\nprint('Percent of missing data')\ndf_miss[0:10]","665a7a57":"# Fetch all numeric features\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric = []\nfor i in data.columns:\n    if data[i].dtype in numeric_dtypes:\n        numeric.append(i)","e2831566":"# Create box plots for all numeric features\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=data[numeric] , orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)","92311932":"# Find skewed numerical features\nskew_features = data[numeric].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nprint(\"There are {} numerical features with Skew > 0.5 :\".format(high_skew.shape[0]))\nskewness = pd.DataFrame({'Skew' :high_skew})\nskew_features.head(10)","e0c55b7c":"# Normalize skewed features\nfor i in skew_index:\n    data[i] = boxcox1p(data[i], boxcox_normmax(data[i] + 1))","f892cf9d":"# Let's make sure we handled all the skewed values\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=data[skew_index] , orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)","b8e6392c":"data['BsmtFinType1_Unf'] = 1*(data['BsmtFinType1'] == 'Unf')\ndata['HasWoodDeck'] = (data['WoodDeckSF'] == 0) * 1\ndata['HasOpenPorch'] = (data['OpenPorchSF'] == 0) * 1\ndata['HasEnclosedPorch'] = (data['EnclosedPorch'] == 0) * 1\ndata['Has3SsnPorch'] = (data['3SsnPorch'] == 0) * 1\ndata['HasScreenPorch'] = (data['ScreenPorch'] == 0) * 1\ndata['YearsSinceRemodel'] = data['YrSold'].astype(int) - data['YearRemodAdd'].astype(int)\ndata['Total_Home_Quality'] = data['OverallQual'] + data['OverallCond']\ndata = data.drop(['Utilities', 'Street', 'PoolQC',], axis=1)\ndata['TotalSF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']\ndata['YrBltAndRemod'] = data['YearBuilt'] + data['YearRemodAdd']\n\ndata['Total_sqr_footage'] = (data['BsmtFinSF1'] + data['BsmtFinSF2'] +\n                                 data['1stFlrSF'] + data['2ndFlrSF'])\ndata['Total_Bathrooms'] = (data['FullBath'] + (0.5 * data['HalfBath']) +\n                               data['BsmtFullBath'] + (0.5 * data['BsmtHalfBath']))\ndata['Total_porch_sf'] = (data['OpenPorchSF'] + data['3SsnPorch'] +\n                              data['EnclosedPorch'] + data['ScreenPorch'] +\n                              data['WoodDeckSF'])\ndata['TotalBsmtSF'] = data['TotalBsmtSF'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\ndata['2ndFlrSF'] = data['2ndFlrSF'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\ndata['GarageArea'] = data['GarageArea'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\ndata['GarageCars'] = data['GarageCars'].apply(lambda x: 0 if x <= 0.0 else x)\ndata['LotFrontage'] = data['LotFrontage'].apply(lambda x: np.exp(4.2) if x <= 0.0 else x)\ndata['MasVnrArea'] = data['MasVnrArea'].apply(lambda x: np.exp(4) if x <= 0.0 else x)\ndata['BsmtFinSF1'] = data['BsmtFinSF1'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\n\ndata['haspool'] = data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ndata['has2ndfloor'] = data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ndata['hasgarage'] = data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ndata['hasbsmt'] = data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ndata['hasfireplace'] = data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","4af7af04":"def logs(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(np.log(1.01+res[l])).values)   \n        res.columns.values[m] = l + '_log'\n        m += 1\n    return res\n\nlog_features = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n                 'TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n                 'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n                 'TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF',\n                 'EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','YearRemodAdd','TotalSF']\n\ndata = logs(data, log_features)","756b9f70":"def squares(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(res[l]*res[l]).values)   \n        res.columns.values[m] = l + '_sq'\n        m += 1\n    return res \n\nsquared_features = ['YearRemodAdd', 'LotFrontage_log', \n              'TotalBsmtSF_log', '1stFlrSF_log', '2ndFlrSF_log', 'GrLivArea_log',\n              'GarageCars_log', 'GarageArea_log']\ndata = squares(data, squared_features)","c9f0a240":"data = pd.get_dummies(data).reset_index(drop=True)\ndata.shape","08c781d9":"data.head()","94b129b8":"# Remove any duplicated column names\ndata = data.loc[:,~data.columns.duplicated()]","d7f319b2":"X = data.iloc[:len(y_train), :]\nX_test = data.iloc[len(y_train):, :]\nX.shape, y_train.shape, X_test.shape","c277202f":"numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric = []\nfor i in X.columns:\n    if X[i].dtype in numeric_dtypes:\n        if i in ['TotalSF', 'Total_Bathrooms','Total_porch_sf','haspool','hasgarage','hasbsmt','hasfireplace']:\n            pass\n        else:\n            numeric.append(i)     \n# visualising some more outliers in the data values\nfig, axs = plt.subplots(ncols=2, nrows=0, figsize=(12, 150))\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\nsns.color_palette(\"husl\", 8)\nfor i, feature in enumerate(list(X[numeric]), 1):\n    if(feature=='MiscVal'):\n        break\n    plt.subplot(len(list(numeric)), 3, i)\n    sns.scatterplot(x=feature, y='SalePrice', hue='SalePrice', palette='Blues', data=train)\n        \n    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n    plt.ylabel('SalePrice', size=15, labelpad=12.5)\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(loc='best', prop={'size': 10})\n        \nplt.show()","8189d880":"# Setup cross validation folds\nkf = KFold(n_splits=12, random_state=42, shuffle=True)","3a09965c":"# define error metrics\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y,y_pred))\n\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y_train, scoring=\"neg_mean_squared_error\", cv=kf))\n    return rmse","63900091":"# Light Gradient Boosting Regressor\nlightgbm = LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.01, \n                       n_estimators=7000,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1,\n                       random_state=42)\n\n# XGBoost Regressor\nxgboost = XGBRegressor(learning_rate=0.01,\n                       n_estimators=6000,\n                       max_depth=4,\n                       min_child_weight=0,\n                       gamma=0.6,\n                       subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='reg:linear',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=27,\n                       reg_alpha=0.00006,\n                       random_state=42)\n\n# Ridge Regressor\nridge_alphas = [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=kf))\n\n# Support Vector Regressor\nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))\n\n# Gradient Boosting Regressor\ngbr = GradientBoostingRegressor(n_estimators=6000,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42)  \n\n# Random Forest Regressor\nrf = RandomForestRegressor(n_estimators=1200,\n                          max_depth=15,\n                          min_samples_split=5,\n                          min_samples_leaf=5,\n                          max_features=None,\n                          oob_score=True,\n                          random_state=42)\n\n# Stack up all the models above, optimized using xgboost\nstack_gen = StackingCVRegressor(regressors=(xgboost, lightgbm, svr, ridge, gbr, rf),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","33b26879":"scores = {}\n\nscore = cv_rmse(lightgbm)\nprint(\"lightgbm: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['lgb'] = (score.mean(), score.std())","dd1bb0eb":"score = cv_rmse(xgboost)\nprint(\"xgboost: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['xgb'] = (score.mean(), score.std())","378ed79f":"score = cv_rmse(svr)\nprint(\"SVR: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['svr'] = (score.mean(), score.std())","6df8038e":"score = cv_rmse(ridge)\nprint(\"ridge: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['ridge'] = (score.mean(), score.std())","b05fb542":"score = cv_rmse(rf)\nprint(\"rf: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['rf'] = (score.mean(), score.std())","06760d2b":"score = cv_rmse(gbr)\nprint(\"gbr: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['gbr'] = (score.mean(), score.std())","ac2f60a8":"print('stack_gen')\nstack_gen_model = stack_gen.fit(np.array(X), np.array(y_train))","bb6ceb37":"print('lightgbm')\nlgb_model_full_data = lightgbm.fit(X, y_train)","178a7dce":"print('xgboost')\nxgb_model_full_data = xgboost.fit(X, y_train)","fc1a9baa":"print('Svr')\nsvr_model_full_data = svr.fit(X, y_train)","c0e4809e":"print('Ridge')\nridge_model_full_data = ridge.fit(X, y_train)","47b2f5bd":"print('RandomForest')\nrf_model_full_data = rf.fit(X, y_train)","81a31eb7":"gbr_model_full_data = gbr.fit(X, y_train)","bb845867":"# Blend models in order to make the final predictions more robust to overfitting\ndef blended_predictions(X):\n    return ((0.1 * ridge_model_full_data.predict(X)) + \\\n            (0.2 * svr_model_full_data.predict(X)) + \\\n            (0.1 * gbr_model_full_data.predict(X)) + \\\n            (0.1 * xgb_model_full_data.predict(X)) + \\\n            (0.1 * lgb_model_full_data.predict(X)) + \\\n            (0.05 * rf_model_full_data.predict(X)) + \\\n            (0.35 * stack_gen_model.predict(np.array(X))))","12dd132f":"# Get final precitions from the blended model\nblended_score = rmsle(y_train, blended_predictions(X))\nscores['blended'] = (blended_score, 0)\nprint('RMSLE score on train data:')\nprint(blended_score)","a45e144b":"sns.set_style(\"white\")\nfig = plt.figure(figsize=(24, 12))\n\nax = sns.pointplot(x=list(scores.keys()), y=[score for score, _ in scores.values()], markers=['o'], linestyles=['-'])\nfor i, score in enumerate(scores.values()):\n    ax.text(i, score[0] + 0.002, '{:.6f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')\n\nplt.ylabel('Score (RMSE)', size=20, labelpad=12.5)\nplt.xlabel('Model', size=20, labelpad=12.5)\nplt.tick_params(axis='x', labelsize=13.5)\nplt.tick_params(axis='y', labelsize=12.5)\n\nplt.title('Scores of Models', size=20)\n\nplt.show()","10c48689":"# Read in sample_submission dataframe\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.shape","5f57ac45":"# Append predictions from blended models\nsubmission.iloc[:,1] = np.floor(np.expm1(blended_predictions(X_test)))","27a775ca":"# Fix outlier predictions\nq1 = submission['SalePrice'].quantile(0.0045)\nq2 = submission['SalePrice'].quantile(0.99)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\nsubmission.to_csv(\"submission_regression1.csv\", index=False)","b5afbac7":"# Scale predictions\nsubmission['SalePrice'] *= 1.001619\nsubmission.to_csv(\"submission_regression2.csv\", index=False)","92b8c58b":"## Blending the models and getting predictions","34836852":"### Log-Transformation of target variable","d68699d0":"SalePrice is our target variable","ea5e6baf":"Key features of the model training process:\n- Cross Validation: Using 12-fold cross-validation\n- Models: On each run of cross-validation I fit 7 models (ridge, svr, gradient boosting, random forest, xgboost, lightgbm regressors)\n- Stacking: In addition, I trained a meta StackingCVRegressor optimized using xgboost\n- Blending: All models trained will overfit the training data to varying degrees. Therefore, to make final predictions, I blended their predictions together to get more robust predictions.","6659b9f7":"## Feature Tranformations","d158f0e8":"We will use box-cox tranformation to normalize the data","a6afc8f2":"The target variable is rightly skewed. Since linear models have an assumption of normally distributed data, we need to transform this variable and make it normally distributed. ","ece41a72":"## Best performing model","8f7d7664":"## Fitting the models","474ecd1c":"### Importing necessary libraries","647a326c":"## Submitting predictions","9f0d4c29":"Concatinating the train and test data in the same dataframe","0a8cb9db":"## Training the models","e9ef0384":"### Get cross validation score for each model","a0a2500b":"## Exploring target variable","48fedf79":"## Training a model","2fb7ec48":"## DATA PROCESSING","aa5fbda0":"Removing the outliers","d998eec6":"## Feature Engineering","26c4cd33":"## Creating interesting features\n","4f8b50e6":"## Missing data","125fb8ef":"## Recreating training and test set","101dcb7c":"The data seem to be now normally distributed","7692a240":"There are two outliers at the bottom right. These have large GrLivArea that are of a low price. These are huge  outliers and can have an significant impact on modelling. So, we can safely delete them","46a41ef8":"## Setting up models","dec0148b":"ML models have trouble recognizing more complex patterns (and we're staying away from neural nets for this competition), so let's help our models out by creating a few features based on our intuition about the dataset, e.g. total area of floors, bathrooms and porch area of each house","9f728fc1":"## Fixing skewed features","64ac8edc":"### Outliers","acde2f80":"## Encoding categorical features","3d5b1115":"Visualizeing some of the features we're going to train our models on","97609b1d":"### Visualizing missing values","f2936fe1":"### Imputing the missing values "}}