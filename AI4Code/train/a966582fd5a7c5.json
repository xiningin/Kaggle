{"cell_type":{"e84a093c":"code","2e0a8182":"code","d1a9e791":"code","71c67839":"code","29970d57":"code","34033e94":"code","2a84774b":"code","11d7cc35":"code","d1af4258":"code","1506e2de":"code","300e4b51":"code","822e4cb4":"code","834a7ff3":"code","fc8955f1":"code","38f28798":"code","abd822b7":"code","8af9fd74":"code","3f200f19":"code","4d8c5ac3":"code","a02abfa8":"code","df9d2662":"code","4e56c3f8":"code","01a2ec92":"code","9e730804":"code","d95b66df":"code","629e4652":"code","cf583bda":"code","8c7e2f93":"code","41d7dab5":"code","d8947dca":"code","9d51a5e3":"code","bb18f9ca":"code","c25c000b":"code","43083799":"code","c2472e9f":"code","70300270":"code","118fcc18":"code","e953dd71":"code","e0004807":"code","729746f4":"code","c0b85ea7":"code","6504e728":"code","e6b1a547":"code","00dae114":"code","09506953":"code","8024fbd4":"code","402855ba":"code","baa8cbbf":"code","ca450076":"code","3632a2c9":"code","b212e200":"code","3b75f7c9":"code","5320029f":"code","78816bcb":"code","4f0bd065":"code","86f5f7e4":"code","ffc96b7c":"code","9a086c97":"code","3f8fee81":"code","c5db5671":"code","437a8eb2":"code","a8a1c0a4":"code","d1c2e2c0":"code","ed531b0f":"code","524880c1":"code","079d1de7":"code","9f5bf95c":"code","d2f95aa1":"code","dcc88653":"code","d3a31964":"code","0f9ad565":"code","13eff4b5":"code","c1920c81":"code","c46ba590":"code","8540bb5a":"code","1a34ddd2":"code","0442ce13":"code","e3ec645e":"code","6fc6c400":"code","3fe5af4d":"code","b2d08d5a":"code","33e486f6":"code","da0e9ad9":"markdown","7d9e53ef":"markdown","44ebb900":"markdown","31f13f19":"markdown","2ef858ac":"markdown","41196771":"markdown","1042a96a":"markdown","3ee6d130":"markdown","50971bc5":"markdown","ef472862":"markdown","0375cb3e":"markdown","2b775afe":"markdown","6261c706":"markdown","03174a96":"markdown","9e838b21":"markdown","081234f6":"markdown","022cdf41":"markdown","d9198a4c":"markdown","18e0029b":"markdown","17a692b6":"markdown","1c578668":"markdown","a077d8da":"markdown","77c3efb8":"markdown","2371c233":"markdown","48fdf3f9":"markdown","5af15ba5":"markdown","56ba99b4":"markdown","5233efcf":"markdown","ff06b23e":"markdown","98366fd1":"markdown","c37744ba":"markdown","5d51bbcd":"markdown","93a01e56":"markdown","f19c0026":"markdown","9bd72c51":"markdown","5462b593":"markdown","3df06493":"markdown","73257d7a":"markdown","1bf67d8f":"markdown","e6b7f51e":"markdown","a4442689":"markdown","1ff1280f":"markdown","ececab43":"markdown","1b64e17c":"markdown","fbd67c7e":"markdown","c7c3ee67":"markdown","3a1dc9ca":"markdown","0eafbd8c":"markdown","e5dee3b2":"markdown","9a5852c5":"markdown","4f49b059":"markdown","f188c16d":"markdown","c7331bb4":"markdown","83df4ecc":"markdown","72eb6ae4":"markdown","5a9dfe59":"markdown","09eb9bb3":"markdown","e81f3033":"markdown","a91ef43b":"markdown","d35a549e":"markdown","2903ccfd":"markdown","654e9277":"markdown","c294361d":"markdown","3fa4cb32":"markdown","b2453d48":"markdown"},"source":{"e84a093c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport re\nimport string\nimport numpy as np \nimport random\nimport pandas as pd \n\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom collections import Counter\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport nltk as nlp\n\nfrom tqdm import tqdm\nimport os\n\n\n\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nimport matplotlib.pyplot as plt\nplt.style.use(\"seaborn-whitegrid\")\n\nimport seaborn as sns\n\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2e0a8182":"train_df = pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/test.csv\")\n\ntrain_df.columns","d1a9e791":"train_df.head()","71c67839":"train_df.describe()","29970d57":"print(train_df.shape)\nprint(test_df.shape)","34033e94":"train_df.info()","2a84774b":"train_df.dropna(inplace=True)","11d7cc35":"temp = train_df.groupby('sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\ntemp.style.background_gradient(cmap='Reds')","d1af4258":"def bar_plot(variable):\n   \n    # get feature\n    var = train_df[variable]\n    # count number of categorical variable(value\/sample)\n    varValue = var.value_counts()\n    \n    # visualize\n    plt.figure(figsize = (9,3))\n    plt.bar(varValue.index, varValue)\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    plt.show()\n    print(\"{}: \\n {}\".format(variable,varValue))","1506e2de":"plt.figure(figsize=(10,8))\nsns.countplot(x='sentiment',data=train_df)","300e4b51":"fig = go.Figure(go.Funnelarea(\n    text =temp.sentiment,\n    values = temp.text,\n    title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of Sentiment Distribution\"}\n    ))\nfig.show()","822e4cb4":"lens = [len(x) for x in train_df.text]\nplt.figure(figsize=(12, 5));\n\nprint (\"Max length:\", max(lens))\nprint (\"Min length:\", min(lens))\nprint (\"Mean length:\", np.mean(lens))\n\nsns.distplot(lens);\nplt.title('Text length distribution')","834a7ff3":"lens = [len(x) for x in train_df.selected_text]\nplt.figure(figsize=(12, 5));\nprint (\"Max length:\", max(lens))\nprint (\"Min length:\", min(lens))\nprint (\"Mean length:\", np.mean(lens))\nsns.distplot(lens);\nplt.title('Text length distribution')","fc8955f1":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","38f28798":"results_jaccard=[]\n\nfor ind,row in train_df.iterrows():\n    sentence1 = row.text\n    sentence2 = row.selected_text\n\n    jaccard_score = jaccard(sentence1,sentence2)\n    results_jaccard.append([sentence1,sentence2,jaccard_score])","abd822b7":"jaccard = pd.DataFrame(results_jaccard,columns=[\"text\",\"selected_text\",\"jaccard_score\"])\ntrain_df = train_df.merge(jaccard,how='outer')","8af9fd74":"train_df['Num_words_ST'] = train_df['selected_text'].apply(lambda x:len(str(x).split())) #Number Of words in Selected Text\ntrain_df['Num_word_text'] = train_df['text'].apply(lambda x:len(str(x).split())) #Number Of words in main text\ntrain_df['difference_in_words'] = train_df['Num_word_text'] - train_df['Num_words_ST'] #Difference in Number of words text and Selected Text","3f200f19":"train_df.head() ","4d8c5ac3":"#Duygulara gore Jaccard scrore ortalama degerleri","a02abfa8":"train_df.groupby('sentiment').mean()['jaccard_score']","df9d2662":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","4e56c3f8":"train_df['text'] = train_df['text'].apply(lambda x:clean_text(x))\ntrain_df['selected_text'] = train_df['selected_text'].apply(lambda x:clean_text(x))","01a2ec92":"train_df['sentiment'] = train_df['sentiment'].map({'positive': 1, 'negative': 2, 'neutral':0})\n","9e730804":"train_df.head()","d95b66df":"def remove_stopword(x):\n    return [y for y in x if y not in stopwords.words('english')]\n","629e4652":"#remove stopwords - selected text\n\ntrain_df['selected_text_clear'] = train_df['selected_text'].apply(lambda x:str(x).split())\n\ntrain_df['selected_text_clear'] = train_df['selected_text_clear'].apply(lambda x:remove_stopword(x))","cf583bda":"#remove stopwords - text\n\ntrain_df['text_clear'] = train_df['text'].apply(lambda x:str(x).split())\n\ntrain_df['text_clear'] = train_df['text_clear'].apply(lambda x:remove_stopword(x))","8c7e2f93":"lemma = nlp.WordNetLemmatizer()\n\ndef lemmatizate_word(x):\n    return [lemma.lemmatize(word) for word in x]\n\ntrain_df['selected_text_clear'] = train_df['selected_text_clear'].apply(lambda x:lemmatizate_word(x)) #selected text\ntrain_df['text_clear'] = train_df['text_clear'].apply(lambda x:lemmatizate_word(x)) #text","41d7dab5":"def ngram(text):    \n    return [(text[i],text[i+1]) for i in range(0,len(text)-1)]\n\ntrain_df['ngram_text'] = train_df['text_clear'].apply(lambda x:str(x).split())\nngram_list = []\n\n    \ntrain_df['ngram_text'] = train_df['ngram_text'].apply(lambda ngram_list:ngram(ngram_list))\n","d8947dca":"train_df.ngram_text","9d51a5e3":"train_df.head()","bb18f9ca":"top = Counter([item for sublist in train_df['selected_text_clear'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(25))\ntemp = temp.iloc[1:,:]\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')\n","c25c000b":"fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Selected Text', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","43083799":"top = Counter([item for sublist in train_df['text_clear'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(25))\ntemp = temp.iloc[1:,:]\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","c2472e9f":"fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Text', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","70300270":"Positive_sent = train_df[(train_df['sentiment']== 1) ]\n\ntop = Counter([item for sublist in Positive_sent['text_clear'] for item in sublist])\ntemp_positive = pd.DataFrame(top.most_common(25))\ntemp_positive.columns = ['Common_words','count']\ntemp_positive.style.background_gradient(cmap='Greens')","118fcc18":"Negative_sent = train_df[(train_df['sentiment']== 2) ]\n\ntop = Counter([item for sublist in Negative_sent['text_clear'] for item in sublist])\ntemp_negative = pd.DataFrame(top.most_common(25))\ntemp_negative = temp_negative.iloc[1:,:] #except 'im'\ntemp_negative.columns = ['Common_words','count']\ntemp_negative.style.background_gradient(cmap='Reds')","e953dd71":"Neutral_sent = train_df[(train_df['sentiment']== 0) ]\n\ntop = Counter([item for sublist in Neutral_sent['text_clear'] for item in sublist])\ntemp_neutral = pd.DataFrame(top.most_common(25))\ntemp_neutral = temp_neutral.loc[1:,:] #except 'im'\ntemp_neutral.columns = ['Common_words','count']\ntemp_neutral.style.background_gradient(cmap='Greys')","e0004807":"raw_text = [word for word_list in train_df['selected_text_clear'] for word in word_list]\n","729746f4":"def words_unique(sentiment,numwords,raw_words):\n    '''\n    Input:\n        segment - Segment category (ex. 'Neutral');\n        numwords - how many specific words do you want to see in the final result; \n        raw_words - list  for item in train_data[train_data.segments == segments]['temp_list1']:\n    Output: \n        dataframe giving information about the name of the specific ingredient and how many times it occurs in the chosen cuisine (in descending order based on their counts)..\n    '''\n    allother = []\n    for item in train_df[(train_df.sentiment != sentiment)]['selected_text_clear']:\n        for word in item:\n            allother.append(word)\n    allother = list(set(allother ))\n    \n    specificnonly = [x for x in raw_text if x not in allother]\n    \n    mycounter = Counter()\n    \n    for item in train_df[(train_df.sentiment == sentiment) ]['selected_text_clear']:\n        for word in item:\n            mycounter[word] += 1\n    keep = list(specificnonly)\n    \n    for word in list(mycounter):\n        if word not in keep:\n            del mycounter[word]\n    \n    Unique_words = pd.DataFrame(mycounter.most_common(numwords), columns = ['words','count'])\n    \n    return Unique_words","c0b85ea7":"Unique_Positive= words_unique(1, 10, raw_text)\nprint(\"The top 10 unique words in Positive Tweets are:\")\nUnique_Positive.style.background_gradient(cmap='Greens')","6504e728":"Unique_Negative= words_unique(2, 10, raw_text)\nprint(\"The top 10 unique words in Negative Tweets are:\")\nUnique_Negative.style.background_gradient(cmap='Reds')","e6b1a547":"Unique_Neutral= words_unique(0, 10, raw_text)\nprint(\"The top 10 unique words in Neutral Tweets are:\")\nUnique_Neutral.style.background_gradient(cmap='Greys')\n","00dae114":"train_df2 = train_df[train_df['jaccard_score'] > 0.2]\ntrain_df2.head()","09506953":"temp = train_df2.groupby('sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\ntemp.style.background_gradient(cmap='Reds')","8024fbd4":"selected_text_listt = []\nfor i in train_df2['selected_text_clear']:\n    i = ' '.join(i)\n    selected_text_listt.append(i)\n    \nfrom sklearn.feature_extraction.text import CountVectorizer \nmax_features =500\n\ncount_vectorizer = CountVectorizer(max_features=max_features,stop_words = \"english\")\n\nsparce_matrix = count_vectorizer.fit_transform(selected_text_listt).toarray()  \n","402855ba":"y = train_df2.iloc[:,3:4].values     # sentiment\nx = sparce_matrix\n# train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 42)","baa8cbbf":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","ca450076":"# %% naive bayes\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\n\nfrom sklearn.metrics import *\n# Predicting the Test set results\ny_pred = nb.predict(x_test)\n\n\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))\n\nnb_02_accuracy = accuracy_score(y_test, y_pred)","3632a2c9":"from sklearn.metrics import confusion_matrix\nmat = confusion_matrix(y_test, y_pred)\nsns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False,\n            xticklabels='', yticklabels='')\nplt.xlabel('true label')\nplt.ylabel('predicted label');","b212e200":"# from sklearn.naive_bayes import MultinomialNB\n# nb = MultinomialNB()\n# nb.fit(x_train,y_train)\n\n# from sklearn.metrics import *\n# # Predicting the Test set results\n# y_pred = nb.predict(x_test)\n\n\n# print(classification_report(y_test, y_pred))\n# print(confusion_matrix(y_test, y_pred))\n# print(accuracy_score(y_test, y_pred))","3b75f7c9":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 42)\nclassifier.fit(x_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(x_test)\n\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))\n\nlr_02_accuracy = accuracy_score(y_test, y_pred)","5320029f":"from sklearn.metrics import confusion_matrix\nmat = confusion_matrix(y_test, y_pred)\nsns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False,\n            xticklabels='', yticklabels='')\nplt.xlabel('true label')\nplt.ylabel('predicted label');","78816bcb":"# Fitting Decision Tree Classification to the Training set\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclassifier.fit(x_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(x_test)\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))\n\ndt_02_accuracy = accuracy_score(y_test, y_pred)","4f0bd065":"from sklearn.metrics import confusion_matrix\nmat = confusion_matrix(y_test, y_pred)\nsns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False,\n            xticklabels='', yticklabels='')\nplt.xlabel('true label')\nplt.ylabel('predicted label');","86f5f7e4":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators=100, criterion = 'entropy', random_state = 0)\nclassifier.fit(x_train, y_train)\n\ny_pred = classifier.predict(x_test)\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))\n\nrf_02_accuracy = accuracy_score(y_test, y_pred)","ffc96b7c":"from sklearn.metrics import confusion_matrix\nmat = confusion_matrix(y_test, y_pred)\nsns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False,\n            xticklabels='', yticklabels='')\nplt.xlabel('true label')\nplt.ylabel('predicted label');","9a086c97":"from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 3, metric = 'minkowski', p = 2)\nclassifier.fit(x_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(x_test)\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))\n\nknn_02_accuracy = accuracy_score(y_test, y_pred)","3f8fee81":"from sklearn.metrics import confusion_matrix\nmat = confusion_matrix(y_test, y_pred)\nsns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False,\n            xticklabels='', yticklabels='')\nplt.xlabel('true label')\nplt.ylabel('predicted label');","c5db5671":"# LOAD LIBRARIES\nfrom sklearn.svm import SVC\nclf = SVC(probability=True,kernel='poly',degree=4,gamma='auto')\nclf.fit(x_train, y_train)\n\ny_pred = clf.predict(x_test)\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))\n\nsvm_02_accuracy = accuracy_score(y_test, y_pred)","437a8eb2":"from sklearn.metrics import confusion_matrix\nmat = confusion_matrix(y_test, y_pred)\nsns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False,\n            xticklabels='', yticklabels='')\nplt.xlabel('true label')\nplt.ylabel('predicted label');","a8a1c0a4":"from lightgbm import LGBMClassifier\nlgbm_model = LGBMClassifier().fit(x_train, y_train)\n\n# Predicting the Test set results\ny_pred = lgbm_model.predict(x_test)\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))\n\nlgbm_02_accuracy = accuracy_score(y_test, y_pred)","d1c2e2c0":"train_df8 = train_df[train_df['jaccard_score'] > 0.8]","ed531b0f":"selected_text_listt = []\nfor i in train_df8['selected_text_clear']:\n    i = ' '.join(i)\n    selected_text_listt.append(i)\n    \nfrom sklearn.feature_extraction.text import CountVectorizer \nmax_features = 500\n\ncount_vectorizer = CountVectorizer(max_features=max_features,stop_words = \"english\")\n\nsparce_matrix = count_vectorizer.fit_transform(selected_text_listt).toarray()  \n","524880c1":"y = train_df8.iloc[:,3:4].values     # sentiment\nx = sparce_matrix\n# train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 42)\n\n\n# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","079d1de7":"# %% naive bayes\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\n\nfrom sklearn.metrics import *\n# Predicting the Test set results\ny_pred = nb.predict(x_test)\n\n\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))\n\nnb_08_accuracy = accuracy_score(y_test, y_pred)","9f5bf95c":"from sklearn.metrics import confusion_matrix\nmat = confusion_matrix(y_test, y_pred)\nsns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False,\n            xticklabels='', yticklabels='')\nplt.xlabel('true label')\nplt.ylabel('predicted label');","d2f95aa1":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 42)\nclassifier.fit(x_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(x_test)\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))\n\nlr_08_accuracy = accuracy_score(y_test, y_pred)","dcc88653":"from sklearn.metrics import confusion_matrix\nmat = confusion_matrix(y_test, y_pred)\nsns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False,\n            xticklabels='', yticklabels='')\nplt.xlabel('true label')\nplt.ylabel('predicted label');","d3a31964":"# Fitting Decision Tree Classification to the Training set\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclassifier.fit(x_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(x_test)\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))\n\ndt_08_accuracy = accuracy_score(y_test, y_pred)","0f9ad565":"from sklearn.metrics import confusion_matrix\nmat = confusion_matrix(y_test, y_pred)\nsns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False,\n            xticklabels='', yticklabels='')\nplt.xlabel('true label')\nplt.ylabel('predicted label');","13eff4b5":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators=100, criterion = 'entropy', random_state = 0)\nclassifier.fit(x_train, y_train)\n\ny_pred = classifier.predict(x_test)\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))\n\nrf_08_accuracy = accuracy_score(y_test, y_pred)","c1920c81":"from sklearn.metrics import confusion_matrix\nmat = confusion_matrix(y_test, y_pred)\nsns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False,\n            xticklabels='', yticklabels='')\nplt.xlabel('true label')\nplt.ylabel('predicted label');","c46ba590":"from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 3, metric = 'minkowski', p = 2)\nclassifier.fit(x_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(x_test)\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))\n\nknn_08_accuracy = accuracy_score(y_test, y_pred)","8540bb5a":"from sklearn.metrics import confusion_matrix\nmat = confusion_matrix(y_test, y_pred)\nsns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False,\n            xticklabels='', yticklabels='')\nplt.xlabel('true label')\nplt.ylabel('predicted label');","1a34ddd2":"# LOAD LIBRARIES\nfrom sklearn.svm import SVC\nclf = SVC(probability=True,kernel='poly',degree=4,gamma='auto')\nclf.fit(x_train, y_train)\n\ny_pred = clf.predict(x_test)\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))\n\nsvm_08_accuracy = accuracy_score(y_test, y_pred)","0442ce13":"from sklearn.metrics import confusion_matrix\nmat = confusion_matrix(y_test, y_pred)\nsns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False,\n            xticklabels='', yticklabels='')\nplt.xlabel('true label')\nplt.ylabel('predicted label');","e3ec645e":"from lightgbm import LGBMClassifier\nlgbm_model = LGBMClassifier().fit(x_train, y_train)\n\n# Predicting the Test set results\ny_pred = lgbm_model.predict(x_test)\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint(accuracy_score(y_test, y_pred))\n\nlgbm_08_accuracy = accuracy_score(y_test, y_pred)","6fc6c400":"df_accuracies = [lr_02_accuracy,nb_02_accuracy,dt_02_accuracy,rf_02_accuracy,knn_02_accuracy,svm_02_accuracy,lgbm_02_accuracy,lr_08_accuracy,nb_08_accuracy,dt_08_accuracy,rf_08_accuracy,knn_08_accuracy,svm_08_accuracy,lgbm_08_accuracy]\n","3fe5af4d":"df_accuracies = pd.DataFrame(data = df_accuracies, index=range(len(df_accuracies)),columns=['accuracy'])\ndf_accuracies['model_name'] = ['logistic regression 02','naive bayes 02','desicion tree 02','random forest 02','knn 02','svm 02','lightgbm 02','logistic regression 08','naive bayes 08','desicion tree 08','random forest 08','knn 08','svm 08','lightgbm 08']","b2d08d5a":"df_accuracies.head(12)\n","33e486f6":"df_accuracies.plot(kind='bar',x='model_name',y='accuracy',figsize=(15,10))\n","da0e9ad9":"# Variable Description\n1. textID: unique ID for each piece of text\n2. text: the text of the tweet\n3. selected_text: the general sentiment of the tweet\n4. sentiment: [train only] the text that supports the tweet's sentiment; Positive, Negative, Neutral\n","7d9e53ef":"# Cleaning the Data\nNow Before We Dive into extracting information out of words in text and selected text,let's first clean the data","44ebb900":"> SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible.\nIn addition to performing linear classification, SVMs can efficiently perform a non-linear classification, implicitly mapping their inputs into high-dimensional feature spaces.","31f13f19":"Confusion Matrix ","2ef858ac":"Sentiment analysis is the interpretation and classification of emotions (positive, negative and neutral) within text data using text analysis techniques. Sentiment analysis allows businesses to identify customer sentiment toward products, brands or services in online conversations and feedback.","41196771":"# Naive Bayes","1042a96a":"# Text Length Distribution","3ee6d130":"#  Most Common words in \"Text\"","50971bc5":"Most 25 common ***neutral*** words in Selected Texts","ef472862":"# RandomForest","0375cb3e":"Most 25 common ***positive*** words in Selected Texts","2b775afe":"# Unique Words in each Segment\nWe will look at unique words in each segment in the Following Order:","6261c706":"StandardScaler standardizes a feature by subtracting the mean and then scaling to unit variance. Unit variance means dividing all the values by the standard deviation.","03174a96":"# Support Vector Machine","9e838b21":"Bag of Words","081234f6":"The top 100 unique words in Negative Tweets are:","022cdf41":"> Naive Bayes is a classification algorithm for binary (two-class) and multiclass classification problems. It is called Naive Bayes or idiot Bayes because the calculations of the probabilities for each class are simplified to make their calculations tractable.\n> \n> Rather than attempting to calculate the probabilities of each attribute value, they are assumed to be conditionally independent given the class value.\n> \n> This is a very strong assumption that is most unlikely in real data, i.e. that the attributes do not interact. Nevertheless, the approach performs surprisingly well on data where this assumption does not hold.","d9198a4c":"\"My ridiculous dog is amazing.\" [sentiment: positive]\n\nWith all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment.","18e0029b":"# Basic Data Analysis\n","17a692b6":"# **Removing the stopwords**","1c578668":"# Support Vector Machine","a077d8da":"Classification report is used to measure the quality of predictions from a classification algorithm. How many predictions are True and how many are False. More specifically, True Positives, False Positives, True negatives and False Negatives are used to predict the metrics of a classification report. \n\nThe report shows the main classification metrics precision, recall and f1-score on a per-class basis.\n\nThe metrics are calculated by using true and false positives, true and false negatives. Positive and negative in this case are generic names for the predicted classes. There are four ways to check if the predictions are right or wrong:\n\n    TN \/ True Negative: when a case was negative and predicted negative\n    TP \/ True Positive: when a case was positive and predicted positive\n    FN \/ False Negative: when a case was positive but predicted negative\n    FP \/ False Positive: when a case was negative but predicted positive\n\n**Precision** \u2013 What percent of your predictions were correct?\n\nPrecision is the ability of a classifier not to label an instance positive that is actually negative. For each class it is defined as the ratio of true positives to the sum of true and false positives.\n\nTP \u2013 True Positives\nFP \u2013 False Positives\n\nPrecision \u2013 Accuracy of positive predictions.\nPrecision = TP\/(TP + FP)\n\n\n**Recall** \u2013 What percent of the positive cases did you catch? \n\nRecall is the ability of a classifier to find all positive instances. For each class it is defined as the ratio of true positives to the sum of true positives and false negatives.\n\nFN \u2013 False Negatives\n\nRecall: Fraction of positives that were correctly identified.\nRecall = TP\/(TP+FN)\n\n\n**F1 score** \u2013 What percent of positive predictions were correct? \n\nThe F1 score is a weighted harmonic mean of precision and recall such that the best score is 1.0 and the worst is 0.0. Generally speaking, F1 scores are lower than accuracy measures as they embed precision and recall into their computation. As a rule of thumb, the weighted average of F1 should be used to compare classifier models, not global accuracy.\n\nF1 Score = 2*(Recall * Precision) \/ (Recall + Precision)","77c3efb8":"> Random forests is a supervised learning algorithm. It can be used both for classification and regression. It is also the most flexible and easy to use algorithm. A forest is comprised of trees. It is said that the more trees it has, the more robust a forest is. Random forests creates decision trees on randomly selected data samples, gets prediction from each tree and selects the best solution by means of voting. It also provides a pretty good indicator of the feature importance.","2371c233":"# LightGBM ","48fdf3f9":"# Decision Tree","5af15ba5":"# Most Common words \"Selected Text\"","56ba99b4":"# Modeling with jaccard scores over 0.2","5233efcf":"Confusion Matrix","ff06b23e":"Most 25 common ***negative*** words in Selected Texts","98366fd1":"Accuracy score: In multilabel classification, this function computes subset accuracy: the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true.","c37744ba":"The top 100 unique words in Neutral Tweets are:","5d51bbcd":"# Load and Check Data\n","93a01e56":"# K-Nearest Neighbour","f19c0026":"# Most common words Sentiments Wise\nLet's look at the most common words in different sentiments","9bd72c51":"> Decision tree is the most powerful and popular tool for classification and prediction. A Decision tree is a flowchart like tree structure, where each internal node denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (terminal node) holds a class label.","5462b593":"# Content:\n\n1. Load and Check Data\n2. Variable Description\n3. Univariate Variable Analysis\n4. Text Length Distribution\n5. Selected Text Length Distribution\n6. Basic Data Analysis\n7. Cleaning the Data\n    *     Removing square brackets, links, punctuations etc.\n    *     Removing the stopwords\n    *     Lemmatization\n8. N-Gram Modelling\n9. Most Common Words Analysis\n    *     In \"Selected Text\"\n    *     In \"Text\"\n10. Most common words Sentiments Wise\n    *     Most 25 common positive words\n    *     Most 25 common negative words\n    *     Most 25 common neutral words\n11. Unique Words in each Segment\n    *     Unique 10 Positive words\n    *     Unique 10 Negative words\n    *     Unique 10 Neutral words\n12. Modeling With Jaccard Scores Over 0.2\n    *     Naive Bayes\n    *     Logistic Regression\n    *     Decision Tree\n    *     Random Forest\n    *     K-Nearest Neighbour\n    *     Support Vector Machine\n    *     LightGBM\n13. Modeling With Jaccard Scores Over 0.8\n    *     Naive Bayes\n    *     Logistic Regression\n    *     Decision Tree\n    *     Random Forest\n    *     K-Nearest Neighbour\n    *     Support Vector Machine\n    *     LightGBM","3df06493":"Confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix). Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa). The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabeling one as another). ","73257d7a":"**The top 10 unique words in Positive Tweets are:**","1bf67d8f":"# Univariate Variable Analysis\nCategorical Variable: textID, text, selected_text  , sentiment","e6b7f51e":"# Modeling with jaccard scores over 0.8","a4442689":"# Lemmatization","1ff1280f":"# Logistic Regression","ececab43":"We have one null Value in the train , as the test field for value is NAN we will just remove it\n\n","1b64e17c":"Bag of Words\n\nA bag-of-words model, or BoW for short, is a way of extracting features from text for use in modeling, such as with machine learning algorithms. The approach is very simple and flexible. In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity.","fbd67c7e":"> K Nearest Neighbor(KNN) is a very simple, easy to understand, versatile and one of the topmost machine learning algorithms. KNN used in the variety of applications such as finance, healthcare, political science, handwriting detection, image recognition and video recognition. In Credit ratings, financial institutes will predict the credit rating of customers. In loan disbursement, banking institutes will predict whether the loan is safe or risky. In political science, classifying potential voters in two classes will vote or won\u2019t vote. KNN algorithm used for both classification and regression problems. ","c7c3ee67":"# Up to now, we have analyzed the dataset.\n\n# From now on we will focus on modeling and handle 7 different machine learning methods:\n#     * Naive Bayes\n#     * Logistic Regression\n#     * Decision Tree\n#     * Random Forest\n#     * K-Nearest Neighbour\n#     * Support Vector Machine\n#     * LightGBM\n# These methods are preferred taking into account the opinions and tips of leading professionals.\n\n# In order to achieve better results we have focused on Jaccard score and we have implemented 7 different machine learning methods taking into account 2 different jaccard scores: 0.2 and 0.8  ","3a1dc9ca":"# K-Nearest Neighbour","0eafbd8c":"> Sentiment names converted to numeric","e5dee3b2":"# There are three important terms that need to be known well in order to understand machine learning models. These terms are:\n#     * Classification report\n#     * Accuracy score\n#     * Confusion matrix\n# A general overview of these terms can be found below.","9a5852c5":"**Categorical Variable**\n\nLets look at the distribution of tweets in the train set","4f49b059":"It\u2019s estimated that 80% of the world\u2019s data is unstructured, in other words it\u2019s unorganized. Huge volumes of text data (emails, support tickets, chats, social media conversations, surveys, articles, documents, etc), is created every day but it\u2019s hard to analyze, understand, and sort through, not to mention time-consuming and expensive.\n\nSentiment analysis, however, helps businesses make sense of all this unstructured text by automatically tagging it.","f188c16d":"# N-Gram Modelling","c7331bb4":"# Selected Text Length Distribution","83df4ecc":"Scikit-learn\u2019s CountVectorizer is used to convert a collection of text documents to a vector of term\/token counts. It also enables the \u200bpre-processing of text data prior to generating the vector representation. This functionality makes it a highly flexible feature representation module for text.","72eb6ae4":"> LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n> \n> * Faster training speed and higher efficiency.\n>  \n> * Lower memory usage.\n>  \n> * Better accuracy.\n>  \n> * Support of parallel and GPU learning.\n>  \n> * Capable of handling large-scale data.","5a9dfe59":"# Why Perform Sentiment Analysis?","09eb9bb3":"# Random Forest","e81f3033":"# Logistic Regression","a91ef43b":"* positive sonucunu veren tweetler ortalama olarak %31 oraninda selected text olarak kaydedilmis. yani textlerin ortalama %69u elenmis.\n* negative sonucunu veren tweetler ortalama olarak %33u oraninda selected text olarak kaydedilmis. yani textlerin ortalama %67si elenmis.\n* neutral sonucunu veren tweetler ortalama olarak %97si oraninda selected text olarak kaydedilmis. yani textlerin ortalama %3u elenmis.\n","d35a549e":"# Tweet Sentiment Analysis","2903ccfd":"> Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. Unlike linear regression which outputs continuous number values, logistic regression transforms its output using the logistic sigmoid function to return a probability value which can then be mapped to two or more discrete classes.","654e9277":"![](https:\/\/www.kdnuggets.com\/images\/sentiment-fig-1-689.jpg)","c294361d":"# Decission Tree","3fa4cb32":"# LightGBM","b2453d48":"# Naive Bayes"}}