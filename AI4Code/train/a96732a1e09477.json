{"cell_type":{"a2b34628":"code","2305eada":"code","a56b5d4f":"code","ad98fba3":"code","f38f4d23":"code","2602e78c":"code","449e0497":"code","4758a5b9":"code","16d4b77d":"code","48eb7806":"code","61a040cd":"code","f7fe3a2b":"code","3ac86a52":"code","0febcc77":"code","6df05f79":"code","89fa6e53":"code","95f75f6d":"code","6463b4e4":"code","fad5fb1d":"code","5d5da6c2":"code","0b787390":"code","5afb890a":"code","23a861aa":"code","0c16e045":"code","7acf3d7c":"code","a9374c5e":"code","1a360c24":"code","3fe338b7":"code","f44e1791":"code","f9723d99":"code","2df51fe2":"code","c3ababad":"code","1f60e10f":"code","b13cce84":"code","51ca024f":"code","6dc7cdde":"code","2659b972":"code","595ce4db":"code","3f2e9412":"code","002badc7":"code","4a1dc182":"code","9503c003":"code","5bdb3a6f":"code","d44b6840":"code","222ecb45":"code","d64bdaf0":"code","67a5748c":"code","d962a335":"code","472b2651":"code","e3832336":"code","0f06de46":"code","614bf9c3":"markdown","2ffb318a":"markdown","9446a39f":"markdown","67845ded":"markdown","ba41079b":"markdown","6c668243":"markdown","2077939e":"markdown","332c3a9e":"markdown","7e69bf49":"markdown","74d67248":"markdown","af4dd079":"markdown","67877088":"markdown","98540ef0":"markdown","885b2589":"markdown","a81d34bd":"markdown","a0c9a503":"markdown","0032ce0c":"markdown","82292d63":"markdown","2931dc75":"markdown","415cc09a":"markdown","cad574f3":"markdown","3fc8d293":"markdown","6f54df29":"markdown","92eec0f9":"markdown","f3b69c38":"markdown","8cbe7c5a":"markdown","d0e92662":"markdown","2846b8c1":"markdown"},"source":{"a2b34628":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2305eada":"#!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","a56b5d4f":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam","ad98fba3":"data = pd.read_csv('\/kaggle\/input\/onion-or-not\/OnionOrNot.csv')\nprint('Dataset Shape = {}'.format(data.shape))\nprint('Dataset Memory Usage = {:.2f} MB'.format(data.memory_usage().sum() \/ 1024**2))","f38f4d23":"data.head()","2602e78c":"data.isna().sum()","449e0497":"x=data.label.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('samples')","4758a5b9":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntext_len=data[data['label']==1]['text'].str.len()\nax1.hist(text_len,color='red')\nax1.set_title('Onions')\ntext_len=data[data['label']==0]['text'].str.len()\nax2.hist(text_len,color='green')\nax2.set_title('Not Onion')\nfig.suptitle('Characters in text')\nplt.show()","16d4b77d":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntext_len=data[data['label']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(text_len,color='red')\nax1.set_title('Onions')\ntext_len=data[data['label']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(text_len,color='green')\nax2.set_title('Not Onions')\nfig.suptitle('Words in a text')\nplt.show()","48eb7806":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword=data[data['label']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\nax1.set_title('Onions')\nword=data[data['label']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\nax2.set_title('Not Onions')\nfig.suptitle('Average word length in each text')","61a040cd":"def create_corpus(target):\n    corpus=[]\n    \n    for x in data[data['label']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","f7fe3a2b":"corpus=create_corpus(0)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] ","3ac86a52":"x,y=zip(*top)\nplt.bar(x,y)","0febcc77":"corpus=create_corpus(1)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n    \n\n\nx,y=zip(*top)\nplt.bar(x,y)","6df05f79":"plt.figure(figsize=(10,5))\ncorpus=create_corpus(1)\n\ndic=defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y)","89fa6e53":"plt.figure(figsize=(10,5))\ncorpus=create_corpus(0)\n\ndic=defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y,color='green')","95f75f6d":"counter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)","6463b4e4":"sns.barplot(x=y,y=x)","fad5fb1d":"def get_top_text_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","5d5da6c2":"plt.figure(figsize=(10,5))\ntop_text_bigrams=get_top_text_bigrams(data['text'])[:10]\nx,y=map(list,zip(*top_text_bigrams))\nsns.barplot(x=y,y=x)","0b787390":"example=\"New competition launched :https:\/\/www.kaggle.com\/c\/nlp-getting-started\"","5afb890a":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\nremove_URL(example)","23a861aa":"data['text']=data['text'].apply(lambda x : remove_URL(x))","0c16e045":"example = \"\"\"<div>\n<h1>Onion or Not Onion<\/h1>\n<p>Kaggle <\/p>\n<a href=\"https:\/\/www.kaggle.com\/c\/nlp-getting-started\">getting started<\/a>\n<\/div>\"\"\"","7acf3d7c":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\nprint(remove_html(example))","a9374c5e":"data['text']=data['text'].apply(lambda x : remove_html(x))","1a360c24":"# Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nremove_emoji(\"Omg another sad day \ud83d\ude14\ud83d\ude14\")","3fe338b7":"data['text']=data['text'].apply(lambda x: remove_emoji(x))","f44e1791":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\nexample=\"I am a #king\"\nprint(remove_punct(example))","f9723d99":"data['text']=data['text'].apply(lambda x : remove_punct(x))","2df51fe2":"!pip install pyspellchecker","c3ababad":"from spellchecker import SpellChecker\nspell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n        \ntext = \"corect me plese\"\ncorrect_spellings(text)","1f60e10f":"#data['text']=data['text'].apply(lambda x : correct_spellings(x)#)","b13cce84":"def create_corpus(df):\n    corpus=[]\n    for tweet in tqdm(data['text']):\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus","51ca024f":"corpus=create_corpus(data)","6dc7cdde":"#\/kaggle\/input\/glove-vectors\/glove.6B.100d.txt","2659b972":"embedding_dict={}\nwith open('\/kaggle\/input\/glove-vectors\/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","595ce4db":"MAX_LEN=100\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntext_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","3f2e9412":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","002badc7":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec","4a1dc182":"model=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=1e-5)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])","9503c003":"model.summary()","5bdb3a6f":"train=text_pad[:data.shape[0]]\n#test=text_pad[data.shape[0]:]","d44b6840":"X_train,X_test,y_train,y_test=train_test_split(train,data['label'].values,test_size=0.15)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","222ecb45":"model.fit(X_train,y_train,batch_size=16,epochs=15,validation_data=(X_test,y_test),verbose=2)","d64bdaf0":"model_loss = pd.DataFrame(model.history.history)\nmodel_loss.plot()","67a5748c":"y_pred = model.predict(X_test)","d962a335":"y_pred = y_pred > 0.5","472b2651":"from sklearn.metrics import accuracy_score, confusion_matrix","e3832336":"accuracy_score(y_pred, y_test)","0f06de46":"confusion_matrix(y_pred, y_test)","614bf9c3":"'to' dominats in both of them","2ffb318a":"## Baseline Model","9446a39f":"## EDA","67845ded":"Removing Punctuations","ba41079b":"#### Number of words in a text","6c668243":"## GloVe for Vectorization","2077939e":"Onions","332c3a9e":"Spelling corrections","7e69bf49":"Not onions","74d67248":"ohh,as expected ! There is a class distribution.There are more text with class 0  than class 1 ","af4dd079":"Onions","67877088":"The distribution of both seems to be almost same.50 to 100 characters in a text are the most common among both.","98540ef0":"Not onions","885b2589":"## Stopwords","a81d34bd":"we will do a bigram (n=2) analysis over the texts.Let's check the most common bigrams in texts.","a0c9a503":"Removing URLs","0032ce0c":"#### Punctuations","82292d63":"Here we will use GloVe pretrained corpus model to represent our words.It is available in 3 varieties :50D ,100D and 200 Dimentional.We will try 100 D here.","2931dc75":"#### Common words","415cc09a":"Number of characters in text","cad574f3":"## Target Distribution","3fc8d293":"#### Average number word length in a tweet","6f54df29":"## Ngram analysis","92eec0f9":"## Please upvote if you like it","f3b69c38":"Removing HTML Tags","8cbe7c5a":"## Missing Values","d0e92662":"Removing Emojis","2846b8c1":"## Data Cleaning"}}