{"cell_type":{"4063c039":"code","3968ccff":"code","760153d2":"code","3dcafff5":"code","6ceb2713":"code","c4c7fcd1":"code","905b4c73":"code","227af07d":"code","fffefbd0":"code","6c7ed4e8":"code","c6bc78c5":"code","8661b145":"code","b35304a3":"code","3374a111":"code","64a76b83":"code","3295501f":"code","f3dbb850":"code","0e116e65":"code","68d4006c":"code","4ad93db9":"code","1e590e55":"code","3610bff1":"code","9904019f":"code","eea39cdc":"code","822a8f9d":"code","624aab0e":"code","512ca074":"code","3969bd94":"markdown","510b8e05":"markdown","7246ecdb":"markdown","078bb984":"markdown","f4bfad37":"markdown","743bcf33":"markdown","95c5efb8":"markdown","ab198e8a":"markdown","88cb14ae":"markdown","a9a41816":"markdown","c09953f9":"markdown","ce04486c":"markdown","ec36eda8":"markdown","748c7769":"markdown","46ebabea":"markdown","f72ff555":"markdown","d227d87f":"markdown","985375b0":"markdown","7187f9b1":"markdown","e1c90087":"markdown","0a2ee138":"markdown","f3167120":"markdown","7ca47565":"markdown","bb35057a":"markdown","eed27cca":"markdown","aebc4768":"markdown"},"source":{"4063c039":"import pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 10000)\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,GRU,Dense\nfrom keras.initializers import Constant\nimport os","3968ccff":"from sklearn.feature_extraction.text import TfidfVectorizer\nimport warnings\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV","760153d2":"rawtrain = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\nrawtest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nrawtrain.info()","3dcafff5":"#lets convert target to object type.\nrawtrain['target'] = rawtrain['target'].astype(object)","6ceb2713":"print('There are {} rows and {} columns in train'.format(rawtrain.shape[0],rawtrain.shape[1]))\nprint('There are {} rows and {} columns in train'.format(rawtest.shape[0],rawtest.shape[1]))","c4c7fcd1":"#NaN values\nrawtrain.isnull().sum()  ","905b4c73":"# Lets make some world Cloud\ntext = rawtrain.text.values\nwordcloud = WordCloud(width = 5000, height = 2500,background_color = 'white',\n                      stopwords = STOPWORDS).generate(str(text))\n\nfig = plt.figure(figsize = (16, 10), facecolor = 'k', edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","227af07d":"#Keyword countvalues\nrawtrain.keyword.value_counts()[:10].plot.bar(color='green',);","fffefbd0":"rawtrain.groupby('target').target.value_counts().plot.barh()","6c7ed4e8":"fig,(ax1,ax2) = plt.subplots(1,2,figsize = (18,6))\n\ntweet_len = rawtrain[rawtrain['target'] == 1]['text'].str.len()\nax1.hist(tweet_len,color = 'black')\nax1.set_title('disaster tweets')\n\n\ntweet_len = rawtrain[rawtrain['target'] == 0]['text'].str.len()\nax2.hist(tweet_len,color = 'green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()\n","c6bc78c5":"fig,(ax1,ax2) = plt.subplots(1,2,figsize = (18,5))\ntweet_len = rawtrain[rawtrain['target'] == 1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color = 'orange')\nax1.set_title('disaster tweets')\n\ntweet_len = rawtrain[rawtrain['target'] ==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color = 'purple')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Words in a tweet')\nplt.show()","8661b145":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(18,6))\nword1 = rawtrain[rawtrain['target'] == 1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word1.map(lambda x: np.mean(x)),ax=ax1,color='red')\nax1.set_title('disaster')\n\nword0 = rawtrain[rawtrain['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word0.map(lambda x: np.mean(x)),ax=ax2,color='green')\nax2.set_title('Not disaster')\nfig.suptitle('Average word length in each tweet')","b35304a3":"data = pd.concat([rawtrain,  rawtest])\ndata","3374a111":"data['body_len'] = data['text'].apply(lambda x: len(x) - x.count(\" \"))\ndata.head()","64a76b83":"import string\ndef count_punct(text):\n    count = sum([1 for char in text if char in string.punctuation])\n    return round(count\/(len(text) - text.count(\" \")), 3)*100\n\ndata['punct%'] = data['text'].apply(lambda x: count_punct(x))\n\ndata.head()","3295501f":"plt.subplots(1,figsize = (18,8))\nbins = np.linspace(0, 200, 40)\nplt.hist(data[data['target'] == 1]['body_len'], bins, alpha=0.5, density=True, label='1')\nplt.hist(data[data['target'] == 0]['body_len'], bins, alpha=0.5, density=True, label='0')\nplt.legend(loc='upper left')\nplt.xlim(0,150)\nplt.show()","f3dbb850":"bins = np.linspace(0, 50, 40)\nplt.subplots(1,figsize = (18,8))\nplt.hist(data[data['target']==1]['punct%'], bins, alpha=0.5, density=True, label='1')\nplt.hist(data[data['target']==0]['punct%'], bins, alpha=0.5, density=True, label='0')\nplt.legend(loc='upper right')\nplt.xlim(0,45)\nplt.show()","0e116e65":"data[['punct%', 'body_len']].kurt()","68d4006c":"for i in range(1,7):\n    fig = plt.subplots(figsize=(10,4))\n    plt.hist((data['punct%'])**(1\/i), bins=35)\n    plt.hist((data['body_len'])**(1\/i), bins=35)\n    plt.title(i)\n    plt.show()","4ad93db9":"data['punct%tr'] = data['punct%']**(1\/3)","1e590e55":"def clean_text(text):\n    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    links = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    text = links.sub(r'',text)\n    tags = re.compile(r'<.*?>')\n    text = tags.sub(r'',text)\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text =  emoji_pattern.sub(r'', text)\n    tokens = re.split('\\W+', text)\n    #text = [ps.stem(word) for word in tokens if word not in stopwords]\n    return text\n","3610bff1":"data['cleaned_text'] = data['text'].apply(lambda x: clean_text(x.lower()))\ndata.head(20)","9904019f":"import nltk\nps = nltk.PorterStemmer()\nstopword = nltk.corpus.stopwords.words('english')\ndef stemming(text):\n    tokens = re.split('\\W+', text)\n    text = [ps.stem(word) for word in tokens if word not in stopword]\n    return text\ndata['stem_text'] = data['cleaned_text'].apply(lambda x: stemming(x))","eea39cdc":"\ntfidf_vect = TfidfVectorizer(analyzer=stemming)\nX_tfidf = tfidf_vect.fit_transform(data['cleaned_text'])\ntfidframe = pd.DataFrame(X_tfidf.toarray())\n\n# For concate\ndata = data.reset_index(drop=True)\nX_features = pd.concat([data['body_len'], data['punct%'], tfidframe], axis=1)\nX_features.head()","822a8f9d":"### Re organize data for algoritms\ntrain = X_features[:rawtrain.shape[0]]\ntest = X_features[rawtrain.shape[0]:]\ny_train = rawtrain['target'].astype('int')","624aab0e":"type(y_train.values)","512ca074":"\nrf = RandomForestClassifier()\nparam = {'n_estimators': [10, 50, 100],\n        'max_depth': [10, 20,  None]}\n\ngs = GridSearchCV(rf, param, cv=5, n_jobs=-1)\ngs_fit = gs.fit(train, y_train)\npd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]","3969bd94":"\n\n\n# Models","510b8e05":"<h1 style=\"text-align:center;font-size:200%;;\">Real or Not? NLP with Disaster Tweets<\/h1>\n<img src=\"https:\/\/www.kdnuggets.com\/wp-content\/uploads\/slideshare-data-mining-wordle.jpg\">","7246ecdb":"### Feature creation","078bb984":"## Work path in this notebook, (inspired by [@shahules](https:\/\/www.kaggle.com\/shahules\/tweets-complete-eda-and-basic-modeling) and [@marcovasquez](https:\/\/www.kaggle.com\/marcovasquez\/basic-nlp-with-tensorflow-and-wordcloud\/notebook)).\n- Basic EDA\n- Data Cleaning\n- Feature engineering \n- Machine learning models","f4bfad37":"Target equal to 0 est quite bite longuer than 1. Maybe in urgent situation people do not have time to write lot, or are hurry to post first! But when the text len is more than 60 we have target 1 as longuer!! Intresting.","743bcf33":"### Number of characters in tweets","95c5efb8":"There is a class distribution.There are more tweets with class 0 ( No disaster) than class 1 ( disaster tweets), And sample is not unballanced","ab198e8a":"### Let's stem tweets","88cb14ae":"## Evaluate created features","a9a41816":"### Length of text in tweets, ","c09953f9":"### Box-Cox Power Transformation\n\n**Base Form**: $$ y^x $$\n\n| X    | Base Form           |           Transformation               |\n|------|--------------------------|--------------------------|\n| -2   | $$ y ^ {-2} $$           | $$ \\frac{1}{y^2} $$      |\n| -1   | $$ y ^ {-1} $$           | $$ \\frac{1}{y} $$        |\n| -0.5 | $$ y ^ {\\frac{-1}{2}} $$ | $$ \\frac{1}{\\sqrt{y}} $$ |\n| 0    | $$ y^{0} $$              | $$ log(y) $$             |\n| 0.5  | $$ y ^ {\\frac{1}{2}}  $$ | $$ \\sqrt{y} $$           |\n| 1    | $$ y^{1} $$              | $$ y $$                  |\n| 2    | $$ y^{2} $$              | $$ y^2 $$                |\n\n\n**Process**\n1. Determine what range of exponents to test\n2. Apply each transformation to each value of your chosen feature\n3. Use some criteria to determine which of the transformations yield the best distribution","ce04486c":"###  Average word length in a tweet","ec36eda8":"# Loading the data and print head and get in touch","748c7769":"# Data exploration\nClass distribution","46ebabea":"As we noticed our data are tailled Kurtosis important [ref.](https:\/\/www.itl.nist.gov\/div898\/handbook\/eda\/section3\/eda35b.htm), we will deal with this in the next step","f72ff555":"# Data cleaning","d227d87f":"- As we go transformation become better and better, I'll chose the third one for punct%. Body_len seem to not respond.\n- We notice also stacked bar in left, that just mean 0, for tweets without punctuations.","985375b0":"# Feature engineering \nBased on what we've in the above we can make some interesting feature.","7187f9b1":"The distribution of both seems to be almost same.120 t0 140 characters in a tweet are the most common among both.","e1c90087":"Before we begin with anything else,let's check the class distribution.There are only two classes 0 and 1.","0a2ee138":"## Vectorizing data","f3167120":"We have also more punct, target 0, but notice the peak for 0.","7ca47565":"### To be continued... pleas like if it help, and correct me if I'm wrong or doing un-necessary things.\n- keeplearning","bb35057a":"Sounds we've good distribtion here.","eed27cca":"* # util Libraries.","aebc4768":"### Data transformation"}}