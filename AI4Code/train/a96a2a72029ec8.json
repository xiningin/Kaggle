{"cell_type":{"772a8189":"code","956ade27":"code","75a07d9a":"code","a3ca1785":"code","a141dd64":"code","8a755020":"code","4088944e":"code","e150b874":"code","f874b6a6":"code","9e0f3aea":"code","02e83c87":"code","5ac05c33":"code","21b10b17":"code","9f634393":"code","adae8ec9":"code","66925cd2":"code","d4b63846":"code","806fac64":"code","0fa5a266":"code","d9396f31":"code","649d8953":"code","0f68c851":"code","889cd6e6":"code","56f83f81":"code","cd6848a7":"code","b71e550c":"code","1d848e9f":"code","be232d5d":"code","37338467":"code","3f624ce1":"code","7f37283d":"code","94563a56":"code","27c97164":"code","fde1b137":"code","b4baa401":"code","4d180dfb":"code","0c5f4c4c":"code","6f9932c7":"code","36e52e82":"code","ddb30b7a":"code","311b6a03":"code","1c7f4430":"code","cfc5fbcb":"code","5e5a3588":"code","65870444":"code","e474b8a8":"code","86655ce9":"code","2b93fd72":"code","499adbbe":"code","73ebc691":"code","eddc52db":"code","e7ff1582":"code","a2d5b8a2":"code","f9fbf810":"code","5226c065":"code","486292b4":"code","4ea1da8d":"code","32c4c4a9":"code","0a0672c7":"code","9c80aa0c":"code","3871a813":"code","89db0b31":"code","036f7b9e":"code","f80af5b4":"code","1e3376cf":"markdown","5026c10b":"markdown","67aa15e9":"markdown","5bd9d390":"markdown","eab3c3ce":"markdown","d89026ca":"markdown","b6af9bab":"markdown","6689593a":"markdown","42dc5926":"markdown","e72042cd":"markdown","b8cc8088":"markdown","250e2846":"markdown"},"source":{"772a8189":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","956ade27":"import matplotlib.pyplot as plt","75a07d9a":"train= pd.read_csv('..\/input\/loan-prediction-problem-dataset\/train_u6lujuX_CVtuZ9i.csv')\ntest= pd.read_csv('..\/input\/loan-prediction-problem-dataset\/test_Y3wMUE5_7gLdaTN.csv')\ntrain.head()","a3ca1785":"train.shape","a141dd64":"train.info()","8a755020":"#print the unique values in the object variables\nprint('Married: ' + str(train['Married'].unique()))\nprint('Dependents: '+ str(train['Dependents'].unique()))\nprint('Education: '+ str(train['Education'].unique()))\nprint('Self_Employed: '+ str(train['Self_Employed'].unique()))\nprint('Property_Area: '+ str(train['Property_Area'].unique()))","4088944e":"print('Loan status',train['Loan_Status'].value_counts())\n\n","e150b874":"train.describe()","f874b6a6":"# will look for the missing vlaues in the data\ntrain.isnull().sum().sort_values(ascending=False)","9e0f3aea":"#checking the relatioship between the variables and target \nimport seaborn as sns\n\nsns.countplot(train['Loan_Status'],hue=train['Credit_History'])","02e83c87":"print(pd.crosstab(train['Credit_History'],train['Loan_Status']))","5ac05c33":"train['Credit_History'].value_counts()","21b10b17":"#will impute the missing values by 1 since 1 have the most frequesnt values in the dataset\ntrain['Credit_History']= train['Credit_History'].fillna(1.0)","9f634393":"sns.countplot(train['Loan_Status'],hue=train['Credit_History'])","adae8ec9":"#next we have self employed column to look \nsns.countplot(train['Loan_Status'],hue=train['Self_Employed'])","66925cd2":"train.Self_Employed.value_counts()","d4b63846":"\n#fill the missing values by No in data set\ntrain['Self_Employed']= train['Self_Employed'].fillna('No')","806fac64":"sns.countplot(train['Loan_Status'],hue=train['Self_Employed'])","0fa5a266":"plt.scatter(train['Loan_Status'],train['LoanAmount'])","d9396f31":"#will fill the missing values with the mean value\ntrain['LoanAmount']=train['LoanAmount'].fillna(train.LoanAmount.mean())","649d8953":"train.isnull().sum().sort_values(ascending=False)","0f68c851":"train['Dependents'].isnull().sum()\/len(train['Dependents'])","889cd6e6":"#drop the other missing values from the data\ntrain.dropna(inplace=True)","56f83f81":"train.describe()","cd6848a7":"plt.boxplot(train['ApplicantIncome'])","b71e550c":"plt.boxplot(train['CoapplicantIncome'])","1d848e9f":"#creating a new variable called total income by adding applicant income + coapplicant income\n\ntrain['TotalIncome']= train['ApplicantIncome']+train['CoapplicantIncome']","be232d5d":"sns.boxplot(x='Education',y='ApplicantIncome',data=train)","37338467":"sns.distplot(train.TotalIncome,kde=False)","3f624ce1":"\ntrain = train.drop(columns=['Loan_ID'],axis=1)","7f37283d":"#Next, make all other columns numerical as well. \ntrain['Married'] = np.where((train['Married'] == 'Yes'), 1, 0)\ntrain['Gender'] = np.where((train['Gender'] == 'Female'), 1, 0)\ntrain['Education'] = np.where((train['Education'] == 'Graduate'), 1, 0)\ntrain['Self_Employed'] = np.where((train['Self_Employed'] == 'Yes'), 1, 0)\ntrain['Dependents'] = np.where((train['Dependents'] == '0'), 0, 1)\ntrain['Loan_Status'] = np.where((train['Loan_Status'] == 'Y'), 1, 0)","94563a56":"train['Property_Area'].value_counts()","27c97164":"#Doing label encoding the categorical features\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ntrain['Property_Area']=le.fit_transform(train['Property_Area'])","fde1b137":"train.head()","b4baa401":"y= train['Loan_Status']\nX= train.drop(columns=['Loan_Status'])","4d180dfb":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n#apply SelectKBest class to extract top 10 best features\nbestfeatures = SelectKBest(score_func=chi2, k='all')\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Name of the column','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(10,'Score'))","0c5f4c4c":"corrmat = train.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(10,10))\n\n#plot heat map\ng=sns.heatmap(train[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","6f9932c7":"X= X[['Married','Credit_History','TotalIncome','CoapplicantIncome','LoanAmount','ApplicantIncome']]","36e52e82":"X.head()","ddb30b7a":"X.shape,y.shape","311b6a03":"y.value_counts()","1c7f4430":"from imblearn.over_sampling import SMOTE \nsm = SMOTE(random_state = 2) \nX, y = sm.fit_sample(X, y)","cfc5fbcb":"y.value_counts()","5e5a3588":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,classification_report","65870444":"#spliiitng the test train\nx_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=6)","e474b8a8":"x_train.shape,x_test.shape","86655ce9":"#fitting Logistic regression\nfrom sklearn.linear_model import LogisticRegression\nlog=LogisticRegression()","2b93fd72":"log.fit(x_train,y_train)","499adbbe":"import pickle\n","73ebc691":"#saving my model to pickle file\npickle.dump(log,open('model_pkl','wb'))","eddc52db":"model = pickle.load(open('model_pkl','rb'))","e7ff1582":"x_test","a2d5b8a2":"q= model.predict([[1,1.0,459.000000,0.000000,25.000000,3459]])","f9fbf810":"q[0]","5226c065":"log.score(x_train,y_train)","486292b4":"#Predicting test dataset\npred=log.predict(x_test)","4ea1da8d":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test,pred)","32c4c4a9":"from sklearn import metrics\nmetrics.confusion_matrix(y_test,pred)","0a0672c7":"print(classification_report(y_test,pred))","9c80aa0c":"#decision tree\nfrom sklearn.tree import DecisionTreeClassifier\nclf=DecisionTreeClassifier()\nclf.fit(x_train,y_train)","3871a813":"clf.score(x_train,y_train)","89db0b31":"pred1=clf.predict(x_test)","036f7b9e":"metrics.confusion_matrix(y_test,pred1)","f80af5b4":"print(classification_report(y_test,pred1))","1e3376cf":"Next we have Loan amount variable to look and since its a numerical variable we will look for scatter plot","5026c10b":"Will try with Decision tree","67aa15e9":"Logistic regression performed well out of two models","5bd9d390":"Married,credit history,coapplicantincome,loan amount,total income having high importance in variables list","eab3c3ce":"Will look for the data description ","d89026ca":"#  Handling the outliers from the dataset ","b6af9bab":"We can see that graduate people also having same outliers since they are graduate their income must be higher we will chekck the same with coapplicant income","6689593a":"The data set is not balanced in outputs","42dc5926":"Read the train and test data set first","e72042cd":"# Checking the missing values in the dataset","b8cc8088":"Selecting Important features ","250e2846":"dataset is not balanced so we use smote algorithm to balance the output"}}