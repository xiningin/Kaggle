{"cell_type":{"c0a21e1c":"code","041071ba":"code","cf60bfb0":"code","fd525050":"code","8ddd8cd9":"code","875733f1":"code","2d220140":"code","f6a70097":"code","af033218":"code","6486d63f":"code","770d8fb3":"code","c9b9f1c9":"code","7749f6a0":"code","87d19272":"code","b3500d28":"code","269812d9":"code","16576030":"code","d43baa5f":"code","690198bd":"code","6e0df226":"code","23467843":"code","67cd7a5b":"code","c15dfd2e":"code","0a7549c3":"code","6ab6e164":"code","86cd8e39":"code","167cfeb4":"code","94d171cc":"code","88ddf518":"code","4c6584df":"code","5a330688":"code","6f9d31a8":"code","63b6b5f9":"code","96d3036a":"code","82f20923":"code","5b0b866d":"code","5aa2706b":"code","da443fb1":"code","b93fa337":"code","c3583662":"code","df67c6ee":"code","9d263ba7":"code","a6cabf8c":"code","9a2c9932":"code","f126cd45":"code","05fb0593":"code","5443c14b":"code","2037fef4":"code","05430ac7":"code","11cea937":"code","af3b8ba2":"code","8b952505":"code","f8a5996c":"code","acb48504":"code","e4501d96":"code","db1f2a8f":"code","7542b7b5":"code","836d7d3e":"code","c7842c74":"code","6453f75d":"code","e7f06e87":"code","2e652338":"code","ad2517f4":"code","85cb8ce7":"code","cbd8d9ca":"code","7d205d30":"code","43198776":"code","e00b2914":"code","6168cfc1":"code","e5ba4191":"code","5f86952a":"code","689a4f48":"code","ffa81b48":"code","a7b619bb":"code","44d591d2":"code","12f4b3f6":"code","23a0e27e":"code","12fbe927":"code","5238caf7":"markdown","a3cade48":"markdown","2548efae":"markdown","c3345a09":"markdown","68f250c3":"markdown","f9046d41":"markdown","e0dcc8e8":"markdown","e2c8c6be":"markdown","7c9b3afc":"markdown","f85264c6":"markdown","f3f5dd80":"markdown","0858a73d":"markdown","b8dd727f":"markdown","5ba797e5":"markdown","0df90e1e":"markdown","38fec04c":"markdown","6414b86b":"markdown","6e6173a3":"markdown","82f0f6b1":"markdown","e77fa9dd":"markdown","9eca5f7a":"markdown","b919ae01":"markdown","c296b99f":"markdown","5fb1fa72":"markdown","0c13f835":"markdown","e84f172b":"markdown","0b8bd607":"markdown","54df702e":"markdown","9000f0c8":"markdown","45d3e5cb":"markdown","e4be7092":"markdown","9b6e5d08":"markdown","2f05e96f":"markdown","afd646cc":"markdown","766e42a4":"markdown","def90bc8":"markdown","d90f8d03":"markdown","671dd87c":"markdown"},"source":{"c0a21e1c":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt","041071ba":"X = 2*np.random.rand(100,1)\ny = 4+3*X+np.random.randn(100,1)","cf60bfb0":"plt.plot(X,y,'b.')\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.axis([0,2,0,15])\nplt.show()","fd525050":"X_b = np.c_[np.ones((100, 1)), X]  # add x0 = 1 to each instance\ntheta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)","8ddd8cd9":"theta_best","875733f1":"X_new = np.array([[0], [2]])\nX_new_b = np.c_[np.ones((2, 1)), X_new]  # add x0 = 1 to each instance\ny_predict = X_new_b.dot(theta_best)\ny_predict","2d220140":"plt.plot(X_new, y_predict, \"r-\", linewidth=2, label=\"Predictions\")\nplt.plot(X, y, \"b.\")\nplt.xlabel(\"$x_1$\", fontsize=18)\nplt.ylabel(\"$y$\", rotation=0, fontsize=18)\nplt.legend(loc=\"upper left\", fontsize=14)\nplt.axis([0, 2, 0, 15])\nplt.show()","f6a70097":"from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(X, y)\nlin_reg.intercept_, lin_reg.coef_","af033218":"eta = 0.1\nn_iterations = 1000\nm = 100\ntheta = np.random.randn(2,1)\n\nfor iteration in range(n_iterations):\n    gradients = 2\/m * X_b.T.dot(X_b.dot(theta) - y)\n    theta = theta - eta * gradients","6486d63f":"theta","770d8fb3":"X_new_b.dot(theta)","c9b9f1c9":"import numpy as np\nimport numpy.random as rnd\n\nnp.random.seed(42)","7749f6a0":"m = 100\nX = 6 * np.random.rand(m, 1) - 3\ny = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)","87d19272":"plt.plot(X, y, \"b.\")\nplt.xlabel(\"$x_1$\", fontsize=18)\nplt.ylabel(\"$y$\", rotation=0, fontsize=18)\nplt.axis([-3, 3, 0, 10])\nplt.show()","b3500d28":"from sklearn.preprocessing import PolynomialFeatures\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly_features.fit_transform(X)\nX[0]","269812d9":"X_poly[0]","16576030":"lin_reg = LinearRegression()\nlin_reg.fit(X_poly, y)\nlin_reg.intercept_, lin_reg.coef_","d43baa5f":"X_new=np.linspace(-3, 3, 100).reshape(100, 1)\nX_new_poly = poly_features.transform(X_new)\ny_new = lin_reg.predict(X_new_poly)\nplt.plot(X, y, \"b.\")\nplt.plot(X_new, y_new, \"r-\", linewidth=2, label=\"Predictions\")\nplt.xlabel(\"$x_1$\", fontsize=18)\nplt.ylabel(\"$y$\", rotation=0, fontsize=18)\nplt.legend(loc=\"upper left\", fontsize=14)\nplt.axis([-3, 3, 0, 10])\nplt.show()","690198bd":"t = np.linspace(-10, 10, 100)\nsig = 1 \/ (1 + np.exp(-t))\nplt.figure(figsize=(9, 3))\nplt.plot([-10, 10], [0, 0], \"k-\")\nplt.plot([-10, 10], [0.5, 0.5], \"k:\")\nplt.plot([-10, 10], [1, 1], \"k:\")\nplt.plot([0, 0], [-1.1, 1.1], \"k-\")\nplt.plot(t, sig, \"b-\", linewidth=2, label=r\"$\\sigma(t) = \\frac{1}{1 + e^{-t}}$\")\nplt.xlabel(\"t\")\nplt.legend(loc=\"upper left\", fontsize=20)\nplt.axis([-10, 10, -0.1, 1.1])\nplt.show()","6e0df226":"#lets do this using iris dataset \nfrom sklearn import datasets\niris = datasets.load_iris()\nlist(iris.keys())\n#to know more about the dataset \n#print(iris.DESCR)","23467843":"X = iris[\"data\"][:, 3:]  # petal width\ny = (iris[\"target\"] == 2).astype(np.int) \n\nfrom sklearn import datasets\niris = datasets.load_iris()\nlist(iris.keys()) # 1 if Iris-Virginica, else 0","67cd7a5b":"from sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression(random_state=42)\nlog_reg.fit(X, y)","c15dfd2e":"X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\ny_proba = log_reg.predict_proba(X_new)\n\nplt.plot(X_new, y_proba[:, 1], \"g-\", linewidth=2, label=\"Iris-Virginica\")\nplt.plot(X_new, y_proba[:, 0], \"b--\", linewidth=2, label=\"Not Iris-Virginica\")","0a7549c3":"import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\ndata = sm.datasets.engel.load_pandas().data\ndata.head()","6ab6e164":"mod = smf.quantreg('foodexp ~ income', data)\nres = mod.fit(q=.5)\nprint(res.summary())","86cd8e39":"quantiles = np.arange(.05, .96, .1)\ndef fit_model(q):\n    res = mod.fit(q=q)\n    return [q, res.params['Intercept'], res.params['income']] + \\\n            res.conf_int().loc['income'].tolist()\n\nmodels = [fit_model(x) for x in quantiles]\nmodels = pd.DataFrame(models, columns=['q', 'a', 'b', 'lb', 'ub'])\n\nols = smf.ols('foodexp ~ income', data).fit()\nols_ci = ols.conf_int().loc['income'].tolist()\nols = dict(a = ols.params['Intercept'],\n           b = ols.params['income'],\n           lb = ols_ci[0],\n           ub = ols_ci[1])\n\nprint(models)\nprint(ols)","167cfeb4":"x = np.arange(data.income.min(), data.income.max(), 50)\nget_y = lambda a, b: a + b * x\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nfor i in range(models.shape[0]):\n    y = get_y(models.a[i], models.b[i])\n    ax.plot(x, y, linestyle='dotted', color='grey')\n\ny = get_y(ols['a'], ols['b'])\n\nax.plot(x, y, color='red', label='OLS')\nax.scatter(data.income, data.foodexp, alpha=.2)\nax.set_xlim((240, 3000))\nax.set_ylim((240, 2000))\nlegend = ax.legend()\nax.set_xlabel('Income', fontsize=16)\nax.set_ylabel('Food expenditure', fontsize=16);","94d171cc":"n = models.shape[0]\np1 = plt.plot(models.q, models.b, color='black', label='Quantile Reg.')\np2 = plt.plot(models.q, models.ub, linestyle='dotted', color='black')\np3 = plt.plot(models.q, models.lb, linestyle='dotted', color='black')\np4 = plt.plot(models.q, [ols['b']] * n, color='red', label='OLS')\np5 = plt.plot(models.q, [ols['lb']] * n, linestyle='dotted', color='red')\np6 = plt.plot(models.q, [ols['ub']] * n, linestyle='dotted', color='red')\nplt.ylabel(r'$\\beta_{income}$')\nplt.xlabel('Quantiles of the conditional food expenditure distribution')\nplt.legend()\nplt.show()","88ddf518":"from sklearn import datasets\niris = datasets.load_iris()\nlist(iris.keys())","4c6584df":"X = iris[\"data\"][:, 3:]  # petal width\ny = (iris[\"target\"] == 2).astype(np.int)  # 1 if Iris-Virginica, else 0","5a330688":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\n\nreg = LinearRegression()\n\nmes = cross_val_score(reg,X,y,scoring='neg_mean_squared_error',cv=5)\nmean_mes = np.mean(mes)\nprint(mean_mes)","6f9d31a8":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Ridge \n\nridge = Ridge()\n\nparameters = {'alpha':[1e-15,1e-10,1e-8,1e-4,1e-3,1e-2,1,5,10,20]}\nridge_reg = GridSearchCV(ridge,parameters,scoring='neg_mean_squared_error',cv=5)\nridge_reg.fit(X,y)","63b6b5f9":"print(ridge_reg.best_params_)\nprint(ridge_reg.best_score_)","96d3036a":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Lasso\n\nlasso = Lasso()\n\nparameters = {'alpha':[1e-15,1e-10,1e-8,1e-4,1e-3,1e-2,1,5,10,20]}\nridge_reg = GridSearchCV(lasso,parameters,scoring='neg_mean_squared_error',cv=5)\nridge_reg.fit(X,y)","82f20923":"print(ridge_reg.best_params_)\nprint(ridge_reg.best_score_)","5b0b866d":"from sklearn import datasets\nbostan = datasets.load_boston()\n#Tto know more about the dataset \n#print(bostan.DESCR)","5aa2706b":"bostan_df = pd.DataFrame(bostan.data,columns=bostan.feature_names)\nbostan_df.head()","da443fb1":"bostan_df['house_price'] = bostan.target\nbostan_df.head()\nbostan_df.describe()","b93fa337":"X = bostan_df.drop('house_price',axis=1)\ny = bostan_df['house_price']","c3583662":"from sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import train_test_split\nels_reg = ElasticNet()\nels_reg","df67c6ee":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=1)","9d263ba7":"els_reg.fit(X_train,y_train)","a6cabf8c":"print('RSquare value of elastic net :')\nnp.round(els_reg.score(X_test,y_test)*100,2)","9a2c9932":"predict_els = els_reg.predict(X_test)","f126cd45":"from sklearn import metrics\nprint('mean square error for ElasticNet for Test Data:')\nnp.round(metrics.mean_squared_error(y_test,predict_els),2)","05fb0593":"import pandas as pd\nHitters = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")","5443c14b":"from sklearn.preprocessing import scale\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.cross_decomposition import PLSRegression, PLSSVD\nfrom sklearn.metrics import mean_squared_error","2037fef4":"dummies = pd.get_dummies(Hitters[['League', 'Division', 'NewLeague']])","05430ac7":"Hitters['Salary'].fillna('0',inplace=True)","11cea937":"y = Hitters.Salary\nX_ = Hitters.drop(['Salary', 'League', 'Division', 'NewLeague'], axis=1).astype('float64')","af3b8ba2":"X = pd.concat([X_, dummies[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)","8b952505":"pca = PCA()\nX_reduced = pca.fit_transform(scale(X))","f8a5996c":"pd.DataFrame(pca.components_.T).loc[:4,:5]","acb48504":"from sklearn import model_selection\n# 10-fold CV, with shuffle\nn = len(X_reduced)\nkf_10 = model_selection.KFold( n_splits=10, shuffle=True, random_state=1)\n\nregr = LinearRegression()\nmse = []\n\n# Calculate MSE with only the intercept (no principal components in regression)\nscore = -1*model_selection.cross_val_score(regr, np.ones((n,1)), y.ravel(), cv=kf_10, scoring='neg_mean_squared_error').mean()    \nmse.append(score)\n\n# Calculate MSE using CV for the 19 principle components, adding one component at the time.\nfor i in np.arange(1, 20):\n    score = -1*model_selection.cross_val_score(regr, X_reduced[:,:i], y.ravel(), cv=kf_10, scoring='neg_mean_squared_error').mean()\n    mse.append(score)\n    \n# Plot results    \nplt.plot(mse, '-v')\nplt.xlabel('Number of principal components in regression')\nplt.ylabel('MSE')\nplt.title('Salary')\nplt.xlim(xmin=-1);","e4501d96":"np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)","db1f2a8f":"import pandas as pd\ndf = pd.read_csv(\"..\/input\/hitters\/Hitters.csv\")","7542b7b5":"dummies = pd.get_dummies(Hitters[['League', 'Division', 'NewLeague']])\nHitters['Salary'].fillna('0',inplace=True)","836d7d3e":"y = Hitters.Salary\nX = Hitters.drop(['Salary', 'League', 'Division', 'NewLeague'], axis=1).astype('float64')","c7842c74":"from sklearn.svm import SVR\nregressor=SVR(kernel='linear',degree=1)","6453f75d":"from sklearn.model_selection import train_test_split\nxtrain,xtest,ytrain,ytest=train_test_split(X,y)\nregressor.fit(xtrain,ytrain)\npred=regressor.predict(xtest)\nprint(regressor.score(xtest,ytest))","e7f06e87":"from sklearn.metrics import r2_score\nprint(r2_score(ytest,pred))","2e652338":"from sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import MinMaxScaler","ad2517f4":"def rfr_model(X, y):\n# Perform Grid-Search\n    gsc = GridSearchCV(\n        estimator=RandomForestRegressor(),\n        param_grid={\n            'max_depth': range(3,7),\n            'n_estimators': (10, 50, 100, 1000),\n        },\n        cv=5, scoring='neg_mean_squared_error', verbose=0,n_jobs=-1)\n    \n    grid_result = gsc.fit(X, y)\n    best_params = grid_result.best_params_\n    \n    rfr = RandomForestRegressor(max_depth=best_params[\"max_depth\"], n_estimators=best_params[\"n_estimators\"],random_state=False, verbose=False)\n    scores = cross_val_score(rfr, X, y, cv=10, scoring='neg_mean_absolute_error')\n\n    return scores","85cb8ce7":"rfr_model(X,y)","cbd8d9ca":"from sklearn.datasets import load_boston\nboston = load_boston()","7d205d30":"print(boston.keys())","43198776":"data = pd.DataFrame(boston.data)\ndata.columns = boston.feature_names","e00b2914":"data.head()","6168cfc1":"data['PRICE'] = boston.target","e5ba4191":"data.info()","5f86952a":"import xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd\nimport numpy as np","689a4f48":"X, y = data.iloc[:,:-1],data.iloc[:,-1]","ffa81b48":"data_dmatrix = xgb.DMatrix(data=X,label=y)","a7b619bb":"data_dmatrix = xgb.DMatrix(data=X,label=y)","44d591d2":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)","12f4b3f6":"xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 10)","23a0e27e":"xg_reg.fit(X_train,y_train)\n\npreds = xg_reg.predict(X_test)","12fbe927":"rmse = np.sqrt(mean_squared_error(y_test, preds))\nprint(\"RMSE: %f\" % (rmse))","5238caf7":"**9. Support Vector Regression**\n\nThose who are in Machine Learning or Data Science are quite familiar with the term SVM or Support Vector Machine. But SVR is a bit different from SVM. As the name suggest the SVR is an regression algorithm , so we can use SVR for working with continuous Values instead of Classification which is SVM.\n\nSo the first thing we have to understand is what is this boundary line ?(yes! that red line). Think of it as to lines which are at a distance of \u2018e\u2019 (though not e its basically epsilon) but for simplicity lets say its \u2018e\u2019.\n\n![image.png](attachment:image.png)\n\nAssuming our hyper plane is a straight line going through the Y axis\n\nWX+b=0 \n\nSo we can state that the two the equation of the boundary lines are\n\nWx+b=+e\nWx+b=-e\n\nrespectively\nThus coming in terms with the fact that for any linear hyper plane the equation that satisfy our SVR is:\n\ne\u2264y-Wx-b\u2264+e\n\nstating the fact that y=Wx+b\n\ny-Wx-b=0\n\nThis applies for all other type of regression (non-linear,polynomial)\n","a3cade48":"as python made our lifes easy ... al this is prebuit in         **sklearn.linear_model **","2548efae":"you can observe change ","c3345a09":"Now you will convert the dataset into an optimized data structure called **Dmatrix** that XGBoost supports and gives it acclaimed performance and efficiency gains. You will use this later in the tutorial.","68f250c3":"get dumies of columns ...","f9046d41":"#NOTE: kernel=\u2019linear\u2019 \u2192 we are setting the kernel to a linear kernel\n#DEFAULT: kernel=\u2019rbf\u2019\n\n>SVR(kernel=\u2019rbf\u2019,epsilon=1.0,degree=3)\n\n#here we set the kernel to \u2018rbf\u2019 of degree 3 and a epsilon value of 1.0\n\n#by default the kernel is \u2018rbf\u2019 degree is 3 and epsilon is 0.1\n\n#other kernels are \u2192 \u2018linear\u2019,\u2019poly\u2019(for polynomial),\u2019 rbf\u2019","e0dcc8e8":"We see that the smallest cross-validation error occurs when  M=18  components are used. This is barely fewer than  M=19 , which amounts to simply performing least squares, because when all of the components are used in PCR no dimension reduction occurs. However, from the plot we also see that the cross-validation error is roughly the same when only one component is included in the model. This suggests that a model that uses just a small number of components might suffice.","e2c8c6be":"**11.XGBoost**\n\nXGBoost is one of the most popular machine learning algorithm these days. Regardless of the type of prediction task at hand; regression or classification\n\n**Boosting**\n\nBoosting is a sequential technique which works on the principle of an ensemble. It combines a set of weak learners and delivers improved prediction accuracy. At any instant t, the model outcomes are weighed based on the outcomes of previous instant t-1. The outcomes predicted correctly are given a lower weight and the ones miss-classified are weighted higher. Note that a weak learner is one which is slightly better than random guessing. For example, a decision tree whose predictions are slightly better than 50%.","7c9b3afc":"**6. Lasso Regression**\n\nLasso stands for Least Absolute Shrinkage and Selection Operator. It makes use of L1 regularization technique in the objective function. Thus the objective function in LASSO regression becomes:\n\n![image.png](attachment:image.png)\n\n\u03bb is the regularization parameter and the intercept term is not regularized. We do not assume that the error terms are normally distributed.\nFor the estimates we don't have any specific mathematical formula but we can obtain the estimates using some statistical software.\nNote that lasso regression also needs standardization.","f85264c6":"the straight line is unable to capture the patterns in the data. This is an example of under-fitting.To overcome under-fitting, we need to increase the complexity of the model.To generate a higher order equation we can add powers of the original features as new features. The linear model,\n\n                       Y(pred) = b0 + b1*x\n\nshould be transformed to\n![image.png](attachment:image.png)\nThis is still considered to be linear model as the coefficients\/weights associated with the features are still linear. x\u00b2 is only a feature. However the curve that we are fitting is quadratic in nature.","f3f5dd80":"**10.Random Forest Regression**\n\nA Random Forest is an ensemble technique capable of performing both regression and classification tasks with the use of multiple decision trees and a technique called Bootstrap Aggregation, commonly known as bagging. What is bagging you may ask? Bagging, in the Random Forest method, involves training each decision tree on a different data sample where sampling is done with replacement.\n\n![image.png](attachment:image.png)\n\n\nThe basic idea behind this is to combine multiple decision trees in determining the final output rather than relying on individual decision trees. If you want to read more on Random Forests, I have included some reference links which provide in depth explanations on this topic.","0858a73d":"It is quite clear from the plot that the quadratic curve is able to fit the data better than the linear line...\n\nso depends on the data we have to choose the algorithms :)","b8dd727f":"randomly genrate data usingf numpy ..!","5ba797e5":"**4.Quantile regression**\n\nThere are at least two motivations for quantile regression: Suppose our dependent variable is bimodal or multimodal that is, it has multiple humps. If we knew what caused the multimodality, we could separate on that variable and do stratified analysis, but if we don\u2019t know that, quantile regression might be good. (ordinary least squares)OLS regression will, here, be as misleading as relying on the mean as a measure of centrality for a bimodal distribution.\n","0df90e1e":"and use any of Optimizing algorithms and get gobal minimum ","38fec04c":"Principal components regression (PCR) can be performed using the PCA() function, which is part of the sklearn library. In this lab, we'll apply PCR to the Hitters data, in order to predict Salary...","6414b86b":"You should see that the optimal value of alpha is 5, with a negative MSE of -0.15373851579535563 . This is a slight improvement upon the basic multiple linear regression","6e6173a3":"same as in linear regression we need to nake our cost function and minimize in order to get the gobal minimum \n\n![image.png](attachment:image.png)","82f0f6b1":"for doing this we need to use Optimizing tech to find global minimize. normally we have so many Optimizing algorithms,now we will use gradient descent method for getting the gobal minimum \n![image.png](attachment:image.png)","e77fa9dd":"**3.Logistic regression**\n\nIn logistic regression, the dependent variable is binary in nature (having two categories). Independent variables can be continuous or binary. In multinomial logistic regression, you can have more than two categories in your dependent variable.\n\n![image.png](attachment:image.png)","9eca5f7a":"lets move on to next regression algorithm\n\n**2. Polynomial Regression**\n\nLinear regression requires the relation between the dependent variable and the independent variable to be linear. What if the distribution of the data was more complex as shown in the below figure?thats where the polynomial regression come into picture ","b919ae01":"**Visualizing the results**\n\nWe estimate the quantile regression model for many quantiles between .05 and .95, and compare best fit line from each of these models to Ordinary Least Squares results.\n\nFor convenience, we place the quantile regression results in a Pandas DataFrame, and the OLS results in a dictionary.","c296b99f":"*my main perpose of making of this kernel is to give a better understanding of math behind algorithms *\n\n![image.png](attachment:image.png)\n\nhere my discussion about the importance of math behind machine learing ....\n\nhttps:\/\/www.kaggle.com\/getting-started\/111585\n\nand this kernel is totally only for regression algorithms.\n\n**what is regression algorithms? .**\n\nIn machine learning, regression algorithms attempt to estimate the mapping function (f) from the input variables (x) to numerical or continuous output variables (y).\nIn this case, y is a real value, which can be an integer or a floating point value. Therefore, regression prediction problems are usually quantities or sizes.\nFor example, when provided with a dataset about houses, and you are asked to predict their prices, that is a regression task because price will be a continuous output.","5fb1fa72":"first lets do with linear regression and then compare with ridge ","0c13f835":"firstly lets start with very well know regression algorithms \n\n**1. Linear Regression**\n\nIt is the simplest form of regression. It is a technique in which the dependent variable is continuous in nature. The relationship between the dependent variable and independent variables is assumed to be linear in nature.Goal is to design a model that can predict marks if given the number of hours studied. Using the training data, a regression line is obtained which will give minimum error. This linear equation is then used for any new data. That is, if we give number of hours studied by a student as an input, our model should predict their mark with minimum error.\n\nY(pred) = b0 + b1*x","e84f172b":"**5.Ridge regression**\n\nIt's important to understand the concept of regularization before jumping to ridge regression.\n\n**Regularization**\n\nRegularization helps to solve over fitting problem which implies model performing well on training data but performing poorly on validation (test) data. Regularization solves this problem by adding a penalty term to the objective function and control the model complexity using that penalty term.\n\n**L1 Loss function or L1 Regularization**\n\nIn L1 regularization we try to minimize the objective function by adding a penalty term to the **sum of the absolute values of coefficients**. This is also known as least absolute deviations method. Lasso Regression makes use of L1 regularization.\n\n**L2 Loss function or L2 Regularization**\n\nIn L2 regularization we try to minimize the objective function by adding a penalty term to the **sum of the squares of coefficients**.RidgeRegression or shrinkage regression makes use of L2 regularization.\n\nTraditional linear fitting involves minimizing the RSS (residual sum of squares). In ridge regression, a new parameter is added, and now the parameters will minimize:\n\n![image.png](attachment:image.png)","0b8bd607":"**8. Principal Components Regression (PCR)**\n\nPCR is a regression technique which is widely used when you have many independent variables OR multicollinearity exist in your data. It is divided into 2 steps:\n\n    1.Getting the Principal components\n    2.Run regression analysis on principal components\n\nThe most common features of PCR are:\n\n    1.Dimensionality Reduction\n    2.Removal of multicollinearity\n    \n**Getting the Principal components**\nPrincipal components analysis is a statistical method to extract new features when the original features are highly correlated. We create new features with the help of original features such that the new features are uncorrelated.\n\nlets do this using Hitters data ...","54df702e":"Well, you can see that your RMSE for the price prediction came out to be around 10.8 per 1000$.","9000f0c8":"we introduce GridSearchCV. This will allow us to automatically perform 5-fold cross-validation with a range of different regularization parameters in order to find the optimal value of alpha.","45d3e5cb":"as i said if we pass all the our training data to our hypothesis equation we get our y predition \n\nY(pred) = b0 + b1*x\n\nbut how choose b0 and b1 ?\n\nThe values b0 and b1 must be chosen so that they minimize the error. If sum of squared error is taken as a metric to evaluate the model, then goal to obtain a line that best reduces the error.\n![image.png](attachment:image.png)","e4be7092":"lets add the dependent variable house_price ","9b6e5d08":"In quantile regression we try to estimate the quantile of the dependent variable given the values of X's.Note that the dependent variable should be continuous.\n\nQuantile regression model:\nFor qth quantile we have the following regression model,\nThis seems similar to linear regression model but here the objective function we consider to minimize is:\n![image.png](attachment:image.png)\nwhere q is the qth quantile.\n\nIf q = 0.5 i.e. if we are interested in the median then it becomes median regression (or least absolute deviation regression) and substituting the value of q = 0.5 in above equation we get the objective function as:\n","2f05e96f":"**that's all for this kernal ....**\n\n**thank you for reading if u like this please upvote that keeps me motivation....\n**\n**thank you ..... :)**","afd646cc":"This plot compares best fit lines for 10 quantile regression models to the least squares fit. As Koenker and Hallock (2001) point out, we see that:\n\n1.Food expenditure increases with income\n\n2.The dispersion of food expenditure increases with income\n\n3.The least squares estimates fit low income observations quite poorly (i.e. the OLS line passes over most low income households)","766e42a4":"Now we'll perform 10-fold cross-validation to see how it influences the MSE:","def90bc8":"**7. Elastic Net Regression**\n\nElastic Net regression is preferred over both ridge and lasso regression when one is dealing with highly correlated independent variables.\nIt is a combination of both L1 and L2 regularization.\n\nThe objective function in case of Elastic Net Regression is:\n\n![image.png](attachment:image.png)\n\nLike ridge and lasso regression, it does not assume normality.","d90f8d03":"this is how it looks ..!\n\nLogistic regression models the probability of the default classFor example, if we are modeling people\u2019s sex as male or female from their height, then the first class could be male and the logistic regression model could be written as the probability of male given a person\u2019s height, or more formally:\n\nP(sex=male|height)\n\nWritten another way, we are modeling the probability that an input (X) belongs to the default class (Y=1), we can write this formally as:\n\nP(X) = P(Y=1|X)\n\nWe\u2019re predicting probabilities ! I thought logistic regression was a classification algorithm? :)\n\nNote that the probability prediction must be transformed into a binary values (0 or 1) in order to actually make a probability prediction. More on this later when we talk about making predictions.","671dd87c":"**Least Absolute Deviation**\n\nThe LAD model is a special case of quantile regression where q=0.5"}}