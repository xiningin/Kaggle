{"cell_type":{"6d9c1bfd":"code","51f47963":"code","d05c618c":"code","6f277ba9":"code","ca649c3a":"code","b443506f":"code","3e720cd8":"code","7fe5bc1b":"code","b7b8ea5e":"code","e87f8ee1":"code","c78723a8":"code","21ec13fd":"code","06574e91":"code","5cb76e0b":"code","149598c5":"code","ba641f27":"code","0d681359":"code","706fecd6":"code","7f82629b":"code","a455468f":"code","d06662cf":"code","770c510d":"code","ef5dfb9e":"code","e716ef79":"code","3403b9e7":"code","3efeb12c":"code","73392109":"code","8f7906a1":"code","3e6249b0":"code","5a63e5b3":"code","3de3fd0d":"code","75dc97b9":"code","ea2b5d04":"code","af7e7543":"code","b5ab8a3f":"code","9c9cb7f0":"code","52de93df":"code","f0e615d7":"code","fb319257":"code","24fbf1cc":"code","ed852295":"code","ce3bdbcf":"code","a79a0e38":"code","96c6dd50":"code","47831b76":"code","a462778d":"code","5974b530":"code","eafe822f":"code","9c810f8e":"code","0e43867d":"code","1aa237d0":"code","b88bb921":"code","23047c45":"code","ae92e255":"code","e58acadd":"code","09f6353a":"code","d262db09":"code","89ea748a":"code","8649b644":"code","704a2eb5":"code","efd03e0c":"code","f07f8563":"code","3173279b":"code","efc48e8e":"code","ae6db583":"code","627fd1bc":"code","cdd9a00b":"code","5249bf33":"code","a444de34":"code","a681b5a1":"code","f2833b68":"code","516b6796":"markdown","ccb93e83":"markdown","c01bdcb3":"markdown","bb26c151":"markdown"},"source":{"6d9c1bfd":"#Importing important library\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","51f47963":"#Importing dataset\ndf_test=pd.read_csv('..\/input\/titanic\/test.csv')\ndf=pd.read_csv('..\/input\/titanic\/train.csv')\ndf_gender=pd.read_csv('..\/input\/titanic\/gender_submission.csv')","d05c618c":"#heading of Dataframe","6f277ba9":"df_test.head()","ca649c3a":"df.head()","b443506f":"df_gender.head()","3e720cd8":"\"\"\"\n\nExploratory Data Analysis\n\n\n\"\"\"","7fe5bc1b":"df.describe()","b7b8ea5e":"df.info()","e87f8ee1":"#Heatmap of NaN values\nplt.figure(figsize=(8,8))\nsns.heatmap(df.isnull(),cmap='BuPu_r',yticklabels=False,cbar=False)","c78723a8":"#This heatmap shows the in Cabin there was lot of NaN_values. So we drop Cabin column ","21ec13fd":"df=df.drop('Cabin',axis=1)","06574e91":"df.head()","5cb76e0b":"plt.figure(figsize=(8,8))\nsns.heatmap(df.isnull(),cmap='BuPu_r',yticklabels=False,cbar=False)","149598c5":"#Now we see how many people survived \nsns.countplot(x='Survived',data=df)","ba641f27":"sns.countplot(x='Survived',data=df,hue='Pclass')","0d681359":"#This graph shows that the death rate among third class is more than first and second class\n\n\n\n#Now check which Sex survive most","706fecd6":"sns.countplot(x='Survived',data=df,hue='Sex')","7f82629b":"#This shows that death rate is more in male and Surving rate is more in female\n#Now see Age of passenger","a455468f":"sns.distplot(df['Age'].dropna(),bins=50,kde=False)","d06662cf":"#This shows many passenger age is between 20 to 40  years","770c510d":"#Now we see passengers SibSp ","ef5dfb9e":"sns.barplot(x='Pclass',y='SibSp',data=df)","e716ef79":"#This shows Third class have more Sibling than 2nd and 1st class","3403b9e7":"#Now we See the Age by class using boxplot","3efeb12c":"plt.figure(figsize=(8,8))\nsns.boxplot(x='Pclass',y='Age',data=df)","73392109":"#It shows the average age of First class people is 37\n#It shows the average age of Second class people is 29\n#It shows the average age of Third class people is 23","8f7906a1":"\"\"\"\n\n\nFilling the missing data\n\n\"\"\"","3e6249b0":"#Function to fill the missing data\ndef fill_age(cols):\n  Age=cols[0]\n  Pclass=cols[1]\n\n  if pd.isnull(Age):\n    \n    if Pclass==1:\n      return 37\n\n    if Pclass==2:\n      return 29\n\n    if Pclass==3:\n      return 23\n  \n\n  else:\n    return Age","5a63e5b3":"#So we make the function that fill the missing Age of the people. Now time to apply it on our data","3de3fd0d":"df['Age1']=df[['Pclass','Age']].apply(fill_age,axis=1)","75dc97b9":"df=df.drop('Age',axis=1)","ea2b5d04":"#Finally Now check the null values by visualizing on heatmap","af7e7543":"plt.figure(figsize=(8,8))\nsns.heatmap(df.isnull(),cmap='BuPu_r',yticklabels=False,cbar=False)","b5ab8a3f":"#Now make the dummy of the few columns like Sex and Embarked for fitting on our machine learning model","9c9cb7f0":"sex = pd.get_dummies(df['Sex'],drop_first=True)\nembark = pd.get_dummies(df['Embarked'],drop_first=True)","52de93df":"#Now dropping the old(Sex,Embarked) columns and useless columns( Name and ticket)","f0e615d7":"df.drop(['Sex','Embarked','Name','Ticket'],axis=1,inplace=True)","fb319257":"df= pd.concat([df,sex,embark],axis=1)","24fbf1cc":"df.head()","ed852295":"\"\"\"\n\nMachine learning part\n\n\n\"\"\"","ce3bdbcf":"#Importing machine learning modules\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score","a79a0e38":"svc=SVC()\nlg=LogisticRegression()","96c6dd50":"#Dividing our data into X_set and y_set\n\nX=df[['PassengerId','Pclass', 'SibSp', 'Parch', 'Fare', 'Age1','male', 'Q', 'S']]\ny=df['Survived']","47831b76":"#Now using train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","a462778d":"#So fiting to the data set on our model\n#First we try with Logistic Regression","5974b530":"lg.fit(X_train,y_train)","eafe822f":"pred_lg=lg.predict(X_test)","9c810f8e":"#classification report\nprint(classification_report(y_test,pred_lg))\nprint('_______________________________________________________________')\nprint('_______________________________________________________________')\nprint(                                                                 )\n#Accuracy score\nprint('Accuracy of the logistic regression model ==',accuracy_score(y_test,pred_lg)*100)","0e43867d":"#Now lets try with SVC","1aa237d0":"svc.fit(X_train,y_train)","b88bb921":"pred_svc=svc.predict(X_test)","23047c45":"#classification report\nprint(classification_report(y_test,pred_svc))\nprint('_______________________________________________________________')\nprint('_______________________________________________________________')\nprint(                                                                 )\n#Accuracy score\nprint('Accuracy of the Support vector machine model ==',accuracy_score(y_test,pred_svc)*100)","ae92e255":"#Now with grid search","e58acadd":"param_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']} ","09f6353a":"grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=3)","d262db09":"grid.fit(X_train,y_train)","89ea748a":"print('Best parameter of grid',grid.best_params_)\n\nprint('______________________________')\nprint('______________________________')\n\nprint('Best Score of grid',grid.best_score_)\n\nprint('______________________________')\nprint('______________________________')\n\n\nprint('Best Estimator',grid.best_estimator_)","8649b644":"#Now our model make prediction\ngrid_predictions = grid.predict(X_test)","704a2eb5":"#classification report\nprint(classification_report(y_test,grid_predictions))\nprint('_______________________________________________________________')\nprint('_______________________________________________________________')\nprint(                                                                 )\n#Accuracy score\nprint('Accuracy of the Support vector machine model ==',accuracy_score(y_test,grid_predictions)*100)","efd03e0c":"#Now tring with KNearestneigbors","f07f8563":"error_rate = []\n\n# Will take some time\nfor i in range(1,40):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))","3173279b":"plt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","efc48e8e":"#So we choose value of K=25","ae6db583":"# First we check with K=1\nknn = KNeighborsClassifier(n_neighbors=1)\n\nknn.fit(X_train,y_train)\npred = knn.predict(X_test)\n\nprint('WITH K=1')\nprint('\\n')\nprint(confusion_matrix(y_test,pred))\nprint('\\n')\nprint(classification_report(y_test,pred))","627fd1bc":"# Now with K=25\nknn = KNeighborsClassifier(n_neighbors=25)\n\nknn.fit(X_train,y_train)\npred = knn.predict(X_test)\n\nprint(classification_report(y_test,pred))\n\nprint('_______________________________________')\nprint('_______________________________________')\nprint()\nprint('Accuracy Score of K=25 ==',accuracy_score(y_test,pred)*100)","cdd9a00b":"#As we see that KNearest classifer not do great job","5249bf33":"#So In my opinion Logistic regression do the best work","a444de34":"print(classification_report(y_test,pred_lg))","a681b5a1":"print(accuracy_score(y_test,pred_lg)*100)","f2833b68":"#Hope you Enjoy","516b6796":"# This graph shows that the death rate is more than the survived rate. Now check which class survived most","ccb93e83":"# END","c01bdcb3":"# #Now we check NaN values by visualising","bb26c151":"This info fuction shows that there was many NaN values. So we plot in our heatmap"}}