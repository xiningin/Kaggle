{"cell_type":{"cd377e58":"code","e74aaf72":"code","3b8597aa":"code","13a690fd":"code","ed29c5b0":"code","832fcb43":"code","fdaf350e":"code","9da62ed7":"code","7b211e29":"code","e5154b90":"code","2d31e4f5":"code","dfa2d4f3":"code","b7b79094":"code","21df1039":"code","11bd5d4c":"code","2ddfc28c":"code","5becb07c":"code","0a8f811e":"code","3b601e76":"code","221be97b":"code","e518d8f3":"code","089e32aa":"code","60cdeba9":"code","57873f1a":"markdown","c4ea1f43":"markdown","dfc7f49c":"markdown","53a6686e":"markdown","8e73f494":"markdown","999e40fd":"markdown","d3faa804":"markdown","2848b57e":"markdown","2c070e56":"markdown","692d0026":"markdown","e6665b6a":"markdown","946da1c8":"markdown","ef2a6d2d":"markdown","d654fe32":"markdown","ad651ea6":"markdown","d8cb1f21":"markdown","ccbf9638":"markdown","1e0c6b3b":"markdown","10e1c126":"markdown","efc8e35b":"markdown"},"source":{"cd377e58":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom plotly.offline import plot, iplot, init_notebook_mode as py\nimport seaborn as sns\nimport plotly.graph_objs as go\n\n#for standardising data\nfrom sklearn.preprocessing import StandardScaler \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","e74aaf72":"df=pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","3b8597aa":"print(\"-\"*50)\nprint('Shape of the dataframe:',df.shape)\nprint(\"Number of records in train data set:\",df.shape[0])\nprint(\"Information of the dataset:\")\ndf.info()\nprint(\"-\"*50)\nprint(\"First 5 records of the dataset:\")\ndf.head()\nprint(\"-\"*50)","13a690fd":"df.head()","ed29c5b0":"# Define missing plot to detect all missing values in dataset\ndef missing_plot(dataset, key) :\n    null_feat = pd.DataFrame(len(dataset[key]) - dataset.isnull().sum(), columns = ['Count'])\n    percentage_null = pd.DataFrame((len(dataset[key]) - (len(dataset[key]) - dataset.isnull().sum()))\/len(dataset[key])*100, columns = ['Count'])\n    percentage_null = percentage_null.round(2)\n\n    trace = go.Bar(x = null_feat.index, y = null_feat['Count'] ,opacity = 0.8, text = percentage_null['Count'],  textposition = 'auto',marker=dict(color = '#7EC0EE',\n            line=dict(color='#000000',width=1.5)))\n\n    layout = dict(title =  \"Missing Values (count & %)\")\n\n    fig = dict(data = [trace], layout=layout)\n    iplot(fig)\n    ","832fcb43":"# Plotting \nmissing_plot(df, 'Class')\n","fdaf350e":"\nfraud_count = len(df[df.Class == 1])\nnone_fraud_count = len(df[df.Class == 0])\n\nprint('CASE COUNT')\nprint('--------------------------------------------')\nprint('Total number of cases are {}'.format(len(df)))\nprint('Number of Non-fraud cases are {}'.format(none_fraud_count))\nprint('Number of fraud cases are {}'.format(fraud_count))\n\nprint('Percentage of Non-fraud cases is {} % of total'.format((none_fraud_count\/len(df['Class'])*100)))\n\nprint('Percentage of fraud cases is {} % of total'.format((fraud_count\/len(df['Class'])*100)))\n\n\nx=df.Class.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('samples')","9da62ed7":"nonfraud_cases = df[df.Class == 0]\nfraud_cases = df[df.Class == 1]","7b211e29":"nonfraud_cases.describe()","e5154b90":"fraud_cases.describe()","2d31e4f5":"#Create visualization of the distribution of the amount in comparision to target feature\nf, (ax1, ax2) = plt.subplots(1, 2, sharex=True,figsize=(10,4))\nsns.distplot(df[(df['Class'] == 1)]['Amount'], ax=ax1, kde=False, color='blue',label='Fraud')\nsns.distplot(df[(df['Class'] == 0)]['Amount'],ax=ax2, kde=False, color='red',label='Non-fraud');\nf.suptitle('Amount distribution')\nf.legend(loc='upper right')\nax1.grid()\nax2.grid()\nplt.show()","dfa2d4f3":"scaler=StandardScaler()\n\namount = df['Amount'].values\n\ndf['Amount'] = scaler.fit_transform(amount.reshape(-1, 1))\n\nprint(df['Amount'].head(10))","b7b79094":"X = df.drop('Class', axis = 1).values\ny = df['Class'].values","21df1039":"# import library\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\n\nsmote = SMOTE()\n# fit predictor and target variable\nx_smote, y_smote = smote.fit_resample(X, y)\n\n\nprint('Original dataset shape', Counter(y))\nprint('Resample dataset shape', Counter(y_smote))","11bd5d4c":"from numpy import where\nfrom matplotlib import pyplot\n\n# scatter plot of examples by class label\nfor label, _ in Counter(y_smote).items():\n\trow_ix = where(y == label)[0]\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label))\npyplot.legend()\npyplot.show()","2ddfc28c":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics as metrics\n\nfrom sklearn.metrics import accuracy_score,roc_auc_score,precision_score, recall_score,f1_score,classification_report","5becb07c":"# split data into train and test sets\nseed = 7\ntest_size = 0.33\nX_train, X_test, y_train, y_test = train_test_split(x_smote, y_smote ,test_size=test_size, random_state=seed)","0a8f811e":"\n# fit model no training data\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\n","3b601e76":"\n# make predictions for test data\ny_pred = model.predict(X_test)\npredictions = [round(value) for value in y_pred]\n","221be97b":"accuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n","e518d8f3":"precision = precision_score(y_test, predictions)\nprint(\"Precision : %.2f%%\" % (precision * 100.0))","089e32aa":"c_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\n\nsns.heatmap(pd.DataFrame(c_matrix), annot=True, cmap=\"Reds\" ,fmt='g',\n            xticklabels=['Not Fraud', 'Fraud'],\n            yticklabels=['Not Fraud', 'Fraud'],)\nax1.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion Matrix', y=1.1,fontsize=14)\nplt.show()","60cdeba9":"recall = recall_score(y_test, y_pred)\nprint(\"Recall : \",recall )\n\n\n#F1 SCORE\n\nf1score = f1_score(y_test,y_pred, average='macro')\nprint(\"F1 Score : \",f1score )","57873f1a":"By this, the entire data set scales with a zero mean and unit variance, altogether.\n\nLet us now try to implement the concept of Standardization","c4ea1f43":"# Model \n","dfc7f49c":"Data description for none fraud cases ","53a6686e":"Let's read our data using pandas library ","8e73f494":"So, the data obtained contains features of various dimensions and scales altogether. Different scales of the data features affect the modeling of a dataset adversely.\n\nIt leads to a biased outcome of predictions in terms of misclassification error and accuracy rates. Thus, it is necessary to Scale the data prior to modeling.\n\nThis is when standardization comes into picture.","999e40fd":"Our ultimate intent is to tackle this situation by building classification models to classify and distinguish fraud transactions.","d3faa804":"let's see the fraudulent cases ","2848b57e":"Amount between fraud and Non-f","2c070e56":"We can see that ou of  284807 cas , there are only 492 fraud cases which is 0.17%.\nSo we're dealing with a highly imbalanced data","692d0026":"### Evaluate predictions","e6665b6a":"# Credit Card Fraud Detection With Machine Learning in Python Using XGBoost","946da1c8":"Importing needed packagers","ef2a6d2d":"In the next process, we are going to do some data processing and Exploratory Data Analysis (EDA).\n\n# Data Processing and EDA\n\n\n## Class distribution\n\nLet\u2019s have a look at how many fraud cases and non-fraud cases are there in our dataset.","d654fe32":"What we need to do right now is to balance our data! \nbecause in such cases, you get a pretty high accuracy just by predicting the majority class, but you **fail** to capture the minority class, which is most often the point of creating the model in the first place.","ad651ea6":"We are now ready to train our model.\n","d8cb1f21":"After seeing the description of our data it is clear that it needs to be standardized \n\nso let's use 'StandardScaler' method from sklearn package ","ccbf9638":"Hopefully No missing data ","1e0c6b3b":" ## Synthetic Minority Oversampling Technique (SMOTE)\n \n This technique generates synthetic data for the minority class.\n\nSMOTE (Synthetic Minority Oversampling Technique) works by randomly picking a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.\n    ![image.png](attachment:211a4710-deb0-4f08-890d-93e84fd72a2a.png)\n    \n**SMOTE algorithm works in 4 simple steps:**\n\n1. Choose a minority class as the input vector\n2. Find its k nearest neighbors (k_neighbors is specified as an argument in the SMOTE() function)\n3. Choose one of these neighbors and place a synthetic point anywhere on the line joining the point under consideration and its chosen neighbor\n4. Repeat the steps until data is balanced","10e1c126":"This is when standardization comes into picture.\n\nStandardization is a scaling technique wherein it makes the data scale-free by converting the statistical distribution of the data into the below format:\n\n* mean \u2013 0 (zero)\n* standard deviation \u2013 1\n\n\n![![image.png](attachment:a301536d-1fd7-4de8-8324-badab95ed59f.png)]       ","efc8e35b":"Let's check Visually if there's any missing data\n\n"}}