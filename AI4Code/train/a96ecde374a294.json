{"cell_type":{"a9ee7f7d":"code","3dc2c010":"code","344713ba":"code","4545d770":"code","82711723":"code","01d7adb2":"code","8a68cae3":"code","6d51a69d":"code","22ad9fd5":"code","4a5e1017":"code","0242611d":"code","0695a418":"code","e563a82c":"code","99fbd6e7":"code","694a55e7":"code","33f3ba51":"code","df7ab3f5":"code","7c434685":"code","e9457213":"code","a719fa24":"code","cc9e0a47":"code","9a031cbb":"code","dee936f0":"code","e42e0583":"code","307d05ce":"code","af8b8425":"code","e7c7ab76":"code","3208d9fe":"code","10be7b25":"code","5113c769":"code","bd574aea":"code","6088f7ca":"code","5bb8a6e5":"markdown","74363d45":"markdown","225d3f5e":"markdown","329f6c78":"markdown","b9ca7fb5":"markdown","822bd94f":"markdown","7407c942":"markdown","33063b1a":"markdown","efb805ec":"markdown","cc75b01f":"markdown","b603396f":"markdown","c236b88e":"markdown","584ce46a":"markdown","434380bc":"markdown","e4fe031e":"markdown","45425c42":"markdown","de3550ec":"markdown","191f9662":"markdown","65ae259d":"markdown","0166830c":"markdown","219037d7":"markdown","40b52000":"markdown","3250ea32":"markdown","31f55b49":"markdown","3c56c5aa":"markdown","ad292ae3":"markdown"},"source":{"a9ee7f7d":"import os\nimport optuna\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom scipy import stats\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom xgboost import XGBRegressor\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom keras import backend as K\nfrom sklearn import preprocessing\n\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing as pt\n\nprint(tf.__version__)\n\npd.options.display.max_rows = 999\npd.options.display.max_columns = 999\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ndata = pd.read_csv('..\/input\/raisadataset\/ml_task_data.csv')","3dc2c010":"data.head(10)","344713ba":"data.info()","4545d770":"NUM_COLS = list(data.dtypes[data.dtypes != 'object'].index)\nNON_BINARY_COLS = NUM_COLS.copy()\nCAT_COLS = list(data.dtypes[data.dtypes == 'object'].index)","82711723":"for col in CAT_COLS:\n    print(data[col].value_counts(), \"\\n\\n\")","01d7adb2":"data.isna().sum()\/data.shape[0] * 100","8a68cae3":"data.columns","6d51a69d":"data.proppantPerFoot.mean()","22ad9fd5":"data.describe()","4a5e1017":"data[['OperatorAlias', 'proppantPerFoot']].groupby('OperatorAlias').mean().sort_values('proppantPerFoot', ascending = False);","0242611d":"data[['OperatorAlias', 'proppantPerFoot']].groupby('OperatorAlias').mean().plot(kind = 'bar', figsize = (50,10))","0695a418":"for i in range(len(NUM_COLS)):\n    fig = plt.figure(figsize = (10,5))\n    ax = fig.gca()\n    sns.boxplot(data= data[NUM_COLS[i]], orient=\"h\", palette=\"Set1\", ax = ax)\n    fig.suptitle(NUM_COLS[i])","e563a82c":"rows = np.any(stats.zscore(data[NUM_COLS].values) > 2.5, axis=1)\noutliers = data[NUM_COLS].loc[rows]\noutliers.shape","99fbd6e7":"# removing outliers\n\nnon_out_rows = np.any(stats.zscore(data[NUM_COLS].values) < 2.5, axis=1)\ndata[NUM_COLS] = data[NUM_COLS].loc[non_out_rows]\nprint(data.shape)","694a55e7":"sns.pairplot(data[NUM_COLS], diag_kind='kde', plot_kws={'alpha':0.5})","33f3ba51":"data = pd.concat([data, pd.get_dummies(data.Basin)], axis=1)\ndata = pd.concat([data, pd.get_dummies(data.State)], axis=1)\ndata.head()","df7ab3f5":"data['SpudDate'] = pd.to_datetime(data['SpudDate'])\ndata['CompletionDate'] = pd.to_datetime(data['CompletionDate'])\ndata['completion_period'] = (data.CompletionDate - data.SpudDate).astype('timedelta64[D]')\ndata['SpudDate_year'] = pd.DatetimeIndex(data['SpudDate']).year\ndata['SpudDate_month'] = pd.DatetimeIndex(data['SpudDate']).month\ndata['CompletionDate_month'] = pd.DatetimeIndex(data['CompletionDate']).month\ndata['CompletionDate_year'] = pd.DatetimeIndex(data['CompletionDate']).year\ndata.drop(['SpudDate', 'CompletionDate'], axis = 1, inplace = True)\nle = preprocessing.LabelEncoder()\ndata['OperatorAlias'] = le.fit_transform(data.OperatorAlias.values)","7c434685":"# Deleting the BVHH feature as it contains about 46% missing values.\n# train = data.drop(['BVHH'], axis = 1)\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\ndf_tmp = data[data['proppantPerFoot'].notna()]\ntrain = df_tmp.copy()\nlabels = train.proppantPerFoot\ntrain = train.drop(['proppantPerFoot'], axis = 1)\n\n#Calculating the numerical and categorical features again \n# sonce a lot of features were added to the numerical columns\nNUM_COLS = list(train.dtypes[train.dtypes != 'object'].index)\nCAT_COLS = list(train.dtypes[train.dtypes == 'object'].index)\n\n# Filling missing values using most frequent class for categorical columns and using mean otherwise\ntrain[NUM_COLS] = train[NUM_COLS].fillna(train[NUM_COLS].mean())\n\ntrain = train[NUM_COLS]\n# imputer = IterativeImputer(random_state=0, verbose = 2)\n# imputed = imputer.fit_transform(train.values)\n# train = pd.DataFrame(imputed, columns = train.columns)\n\n# train['OperatorAlias'] = data['OperatorAlias'].fillna(data['OperatorAlias'].mode().iloc[0])\n# train = pd.concat([train, pd.get_dummies(train.OperatorAlias)], axis=1)\n# train = train.drop(['OperatorAlias'], axis = 1)\n\ntrain.isna().sum().sum()","e9457213":"train[['OperatorAlias', 'ANADARKO BASIN', 'DENVER BASIN', 'PERMIAN BASIN',\n       'WILLISTON BASIN', 'CO', 'ND', 'NM', 'OK', 'TX', 'WY',\n       'completion_period', 'SpudDate_month',\n       'CompletionDate_month', 'CompletionDate_year']] = train[['OperatorAlias', 'ANADARKO BASIN', 'DENVER BASIN', 'PERMIAN BASIN',\n       'WILLISTON BASIN', 'CO', 'ND', 'NM', 'OK', 'TX', 'WY',\n       'completion_period', 'SpudDate_month',\n       'CompletionDate_month', 'CompletionDate_year']].astype(int)","a719fa24":"train.isna().sum()","cc9e0a47":"corr = data[['Latitude', 'Longitude', 'LateralLengthInMiles', 'BVHH',\n       'proppantPerFoot', 'completion_period', 'SpudDate_year', 'SpudDate_month',\n       'CompletionDate_month', 'CompletionDate_year', 'OperatorAlias']].corr()\nf, ax = plt.subplots(figsize=(20, 10))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, cmap=cmap, vmax = 1, vmin = -1, center=0, square=True, linewidths=.5)","9a031cbb":"k_best = SelectKBest(f_classif, k = 10);\nk_best.fit_transform( train.values, labels.values);\n\nk_best.pvalues_;\np_values = pd.DataFrame({'column': train.columns, 'p_value': k_best.pvalues_});\np_values.sort_values('p_value', ascending=True, inplace = True);\nseg_col = list(p_values.query('p_value < 0.05').column)\nseg_col","dee936f0":"clf = ExtraTreesRegressor()\nclf.fit(train.values,labels.values)\n\nz = clf.feature_importances_\n\ndf = pd.DataFrame()\ndf[\"values\"] = z\ndf['column'] = list(train.columns.values)\n\ndf.sort_values(by='values', ascending=False, inplace = True)\nlist(df.head(10).column)","e42e0583":"normalizer = pt.Normalization()\nnormalizer.adapt(np.array(train))","307d05ce":"# Normalizing the dataset\ntrain_exp = normalizer(train.values).numpy()\nx_train, x_test, y_train, y_test = train_test_split( train_exp, labels.values, test_size = 0.2, random_state=42, shuffle = True )","af8b8425":"# def objective(trial,data=x_train,target=y_train):\n    \n#     train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.15,random_state=42, shuffle = False)\n    \n#     param = {\n#         'tree_method':'gpu_hist',  # this parameter means using the GPU when training our model to speedup the training process\n#         'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n#         'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n#         'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n#         'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n#         'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]),\n#         'n_estimators': trial.suggest_categorical('n_estimators', [10, 20, 100, 200, 500, 1000]),\n#         'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20]),\n#         'random_state': trial.suggest_categorical('random_state', [48]),\n#         'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n#     }\n    \n#     model = XGBRegressor(**param)  \n#     model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=100,verbose=False)\n#     preds = model.predict(test_x)\n#     acc = mean_squared_error(test_y, preds, squared=False)\n#     return acc\n\n\n# study = optuna.create_study(direction='minimize')\n# study.optimize(objective, n_trials=50)\n# print('Number of finished trials:', len(study.trials))\n# print('Best trial:', study.best_trial.params)","e7c7ab76":"param = {'tree_method': 'gpu_hist', 'n_estimators': 100, 'random_state': 2020}\n\n# param = {'lambda': 1.5649318011837972, 'alpha': 0.0044128173545724125, \n#          'colsample_bytree': 0.5, 'subsample': 0.8, 'learning_rate': 0.009, \n#          'n_estimators': 500, 'max_depth': 11, 'random_state': 48, 'min_child_weight': 3}\n\nreg1 = XGBRegressor(**param)\nreg1.fit(x_train, y_train)\n\ncv = KFold(n_splits=10, random_state=1, shuffle=True)\nreg1_scores = cross_val_score(reg1, x_train, y_train, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n\nprint('MSE CV score on training set: {:.2f} +\/- {:.2f}'.format(np.abs(np.mean(reg1_scores)), np.std(reg1_scores)))\nprint('MSE on test set: {:.2f}'.format(mean_squared_error(y_test, reg1.predict(x_test), squared=False)))\n\nprint('r2 score on training set: {:.2f}'.format(r2_score(y_train, reg1.predict(x_train))))\nprint('r2 on test set: {:.2f}'.format(r2_score(y_test, reg1.predict(x_test))))","3208d9fe":"params = {\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'verbose': 0\n}\n\nreg2 = LGBMRegressor(**params)\nreg2.fit(x_train, y_train)\n\ncv = KFold(n_splits=10, random_state=1, shuffle=True)\nreg1_scores = cross_val_score(reg2, x_train, y_train, scoring = 'neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n\nprint('MSE CV score on training set: {:.2f} +\/- {:.2f}'.format(np.abs(np.mean(reg1_scores)), np.std(reg1_scores)))\nprint('MSE on test set: {:.2f}'.format(mean_squared_error(y_test, reg2.predict(x_test), squared=False)))\n\nprint('r2 score on training set: {:.2f}'.format(r2_score(y_train, reg2.predict(x_train))))\nprint('r2 on test set: {:.2f}'.format(r2_score(y_test, reg2.predict(x_test))))","10be7b25":"# Combining the results of the two algorithms\nprint('MSE on test set: {:.2f}'.format(r2_score(y_test, (reg1.predict(x_test) * 2 + reg2.predict(x_test))\/3)))","5113c769":"def root_mean_squared_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n\nmodel = keras.Sequential([\n    layers.Dense(64, activation='relu'),\n    layers.Dense(125, activation='relu'),\n    layers.Dense(500, activation='relu'),\n    layers.Dense(1)\n])\n\nmodel.compile(loss = root_mean_squared_error, metrics= [tf.keras.metrics.RootMeanSquaredError()], optimizer=tf.keras.optimizers.Adam(0.001))\nhistory = model.fit( x_train, y_train, validation_split=0.2, verbose = 0, epochs = 200)","bd574aea":"print('MSE on test set: {:.2f}'.format(mean_squared_error(y_train, model.predict(x_train), squared=False)))\nprint('MSE on test set: {:.2f}'.format(mean_squared_error(y_test, model.predict(x_test), squared=False)))\n\nprint('r2 score on training set: {:.2f}'.format(r2_score(y_train, model.predict(x_train))))\nprint('r2 on test set: {:.2f}'.format(r2_score(y_test, model.predict(x_test))))","6088f7ca":"def plot_loss(history):\n    \n    plt.plot(history.history['loss'], label='loss')\n    plt.plot(history.history['val_loss'], label='val_loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Error [Proppant Per Foot]')\n    plt.legend()\n    plt.grid(True)\n    \nplot_loss(history)","5bb8a6e5":"In the first column we can see that the mean has a very close value to the midean so we can have a preliminary assumption that it will be normal distribution without skewness to any of the sides. Same for the second column.\nLooking at the values of the max and min values in the Longitude and Latitude features we found them between the boundries of the max and min latitude in the united states so we can assume that -as expected- the conpany's business lies only inside the US without no unrealistic values in both features.\n- Latitude of USA ranges from 24.9493 to 49.5904\n- Longtitude in USA ranges from -125.0011 to -66.9326 [[1]](https:\/\/gist.github.com\/graydon\/11198540#gistcomment-2702247)","74363d45":"#### Featuer Selection \nthere are many ways to do this step, I usually combine all of them to find the best solution.\n- Statistical-based selection\n    - Correlation: usually we use pearson correlation coefficient which gives us the linear correlation between features\n    - Hypothesis testing: this p-value can be calculated from tests like ANOVA to calculate the segnificance of the feature based on the alternative and NULL hypothesis.\n- Model based Selection: uing Machine Learning models that already uses this like tree based models, tree based models performs the plits based on metrics like the information gain for categorical variable where this can be used as an importance measure for features.\n\nI will start with the statistical based tests and will start with the correlation matrix's heatmap, I didn't use the original matrix as the numbers are not visually appealing.","225d3f5e":"Correlation method calcuates the correlation between the target variable and all the other non-binary numerical features.","329f6c78":"*Outliers Detection*\n\n\nThere are many methods that can be used to detect outliers in a dataset. In this project I will discuss the following:\n* Box Plot method\n* Standarization (Z-sore) method\n\n##### Box Plot :: Consists of five main components:\n* Q1, first quartile (Midean of the first half of the data)\n* Q2, Midean of the data\n* Q3, midean of the second half of the data\n* Max value\n* Min value\n\n##### Main equations in box plots:\n$$ IQR = Q3 - Q1 $$\n$$ Outliers = Q3 + 1.5 * IQR$$\n$$ Q1 - 1.5 * IQR $$\n\n##### Z-score method\nZ-score represents the number of standard deviations removed from the mean for each data point. In a simpler way, it is the distance for a point from the mean in standard deviations.\n$$ z-score = {x - mean \\over std} $$","b9ca7fb5":"Now we will check the data types in each column and also the number of non-null values in each feature using the df.info() function.","822bd94f":"### Model Choice and Optimization\n\nI checked multiple models and tree-baed models gave me the best results on this dataset, I will use XGBoost algorithm which is a ensemble boosting algorithm based on the decision trees concept and will also use the LightGBM algorithm which is based on simillar idea. I will use Optuna optimizer to optimize the model as follows [[2]](https:\/\/www.kaggle.com\/kst6690\/make-your-xgboost-model-awesome-with-optuna):","7407c942":"According to the the z-scores methods only 24 instances (rows) are considered as outliers which is not a significant number and can be dropped.","33063b1a":"# Data Science task for Oil & Gas data.\n\nI will explain the logic of behind my code and will go through this project as a tutorial. This project contains the following steps in the Data Ssience pipeline:\n\n- Exploratory Data Analysis\n- Feature Engineering:\n    - Feature Selection\n    - Feature Construction\n    - Data Cleaning\n- Model Selection\n- Model Optimization\n- Model Evaluation\n\nFirst I will start with importing all the needed data science libaries that will be used through out this project, it is always a good practice to do so in the begning of a jupyter notebook to make it make it easier to be converted into a .py file.","efb805ec":"# Feature Engineering\n\nNow I am going to start with feature construction and one-hot encode [Basin, State and Operator Alias] as they are all categrical nominal variables with no obvious order. For the Operator Alias, I would usually drop or merge levels of nominal features with more than 20 levels if the data is too large but here I will try the results with one hot encoding this feature as I believe it should have a good effect on the target. Also the levels of this features can't be merged without a strong domain knowledge of the Oil & Gas field in the USA.","cc75b01f":"Looks like there is some weak relationship between the numerical features and the target variable, I will need to investigate in more details in the upcomming sections.","b603396f":"We can now check the relation between some features and the targett variable, starting with the relation between the Operator and the Propant Per Foot target variable. Since the OperatorAlias is a categorical varialbe, we can't measure the correlation but we can plot the relationshio between the mean Proppant per foot values for each operaor as follows.","c236b88e":"So according to all methods, Latitude and the completion_period have the highest importance besides some other features.","584ce46a":"Now we will explore the Categorical features and check the number of levels in each feature, if a nominal variable has too many levels, we might choose whether to exclude it to minimize running time, memory and avoid the Curse of Dimentionality problem. We can also decrease the number of levels by merging multiple levels together for instance if a review in Amazon can have 4 stars, 3 stars, 2 stars or 1 star then we can merge 4 and 3 stars in one category and 2 and 1 in another to have 2 levels instead of 4 levels.","434380bc":"Now I will try a Multi-Layer Perceptron to see if it might give better results on this kind of data.","e4fe031e":"### Exploratory Data Analysis (EDA) & Data Wrangling\n\nFirst thing in EDA is usually exploring the first few rows of the data to see how the data is represented and formulate an idea about the suitable data type for each column, and whether there are obvious missing values in the small sample.","45425c42":"It seems from the Box plots that the BVHH and the Latitude feature has most of the outliers, I will look into the percentage of outliers using the z-score which is more accurate and more appealing and can be directly related to standard deviation fro each feature.","de3550ec":"It is obvious from the above that we will need to change the type of the string features containig dates and time to datetime data type to make it easier to extract info but this will be handled later in the data wrangling phase.\n\nNow I will explore the percentage of missing values in each column. There are no silver bullets when it comes to dropping columns if the missing values exceeds a certain limit but by common sense, many developpers tend to drop the whole column if the number of missing values in this column exceeds 20-30% of the data, otherwise they impute the missing values. I used this in many of my competitions and it gave me better results than imputation if the percentage of missing values is too high.\n\nImputing missing values in columns with less than 30% missing values can be done using many approaches:\n- Numerical variables: Imputation can be performed using mean value, Midean value or more advance statistical\/ML methods that predicts the missing value based on the relationship between the feature with missing value and all other features.\n- Categorical variables: The most frequent class is usually used for categorical variables imputation.","191f9662":"# Conclusion\n\nXGBoost algorithm performed the best on the given dataset but in general the performance of all the models didn't go far apart.\nFor the performance metrics, I used Root Mean Squared Error but since I don't have a benchmarking model to compare the RMSE values, I used the R2 metric which gives a value bounded by 1 to show the performance of the model, the nearer to 1 is the value the better the model perforamnce is.\n\nThe first model was the best and it gave r2 of 81 n the test set and 87 on the training set which suggests a small variance but it also scored 87 on the training set which suggests that there might be potential Bias (underfitting), this might be caused by the model itself or by the dataset, based on the fine-tuning and the multiple models I used I would suggest that the problem might be in the dataset.","65ae259d":"Finally I will try the model based feature selection using extratrees.","0166830c":"Checking the parameters of the population or statistics of sample are also very important to get a grasp of what the data distribution looks like and whether the distribution is shifted right or left, whether it has outliers and also in some cases whether it has some unrealistic values that can be intrepreted as missing values.\n\nIn many dataset, having a 0 value in a column like age means that this value is missing and hence a non-participation bias can be spotted.","219037d7":"Hypothesis testing, I will use ANOVA test for thatm if the variable's p-val is less than 0.05 then this variable is considred segnificant to the model.","40b52000":"Finally let's explore the relationship between the features and the target again after removing the outliers.","3250ea32":"I will also change the types of the string date features to datetime datatype to make it easier to handle them later. I will also split the datetime features into year and month which are the important part of the feature. Then I will calculate the total time to complete the project which is the difference between the CompletionDate feature and SpudDate feature which is apparently the difference between the time of completion and the time they started digging.","31f55b49":"Visualizing the results from the Deep Neural Network using the following function [[3]](https:\/\/www.tensorflow.org\/tutorials\/keras\/regression) the taining and the validation loss shows us the the model starts overfitting at about 50 epochs.","3c56c5aa":"Now I will divide the columns into Numerical and Categorical columns, any non-object type column col is considered a numerical one as it was obvious that the data doesn't contain boolean features.\n\nCategorical variables:\nIn these variables a certain descrete quality is assigned for each experimental unit[1]. In other words it is a variable that can have a value from a discrete finite set of vaues, it can be subdivided in two different categories:\n- Categorical Ordinal: Where the values have a natural order, this can change from one context to the other so domain knowledge is very important. For example: GPA feature can have values of A, B+, B and etc.. those values have a natural order whhere A is the highest possible passing grade and D is the lowest possible passing grade.\n- Categorical Nominal: Values don't have a specific order, for instance, Gender feature in most of the contexts won't have an order between Male and Female unless you are concerned with one of them in a cerain context and might want to favour it in the prediction.\n\nDealing with Categorical variables can be through Label Encoding for Ordinal features where A+ can be given a numerical value of 4, B can be given 3 and so on. Or we can use one-hot encoding in case of Nominal Variables as the values don't have a natural order.\n\nOne hot encoding a feature adds new features for each unique category, so if you have only two catogries \"Y\" and \"N\" in a feature F, you will have two new columns Y and N where Y feature will have 1s in the places F = \"Y\" and N feature will have 1's in the places F = \"N\"\n\n#### Example\n\nOne-hot encoding:\n\nF &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Y | N <br>\nY &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;         1 | 0 <br>\nN &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;         0 | 1 <br>\nN &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;         0 | 1 <br>\nY &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;         1 | 0 <br>\nY &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;         1 | 0 <br>\n\nLabel encoding: if Y is ranked lower than N: <br>\nF  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    F_new <br>\nY  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;            1 <br>\nN  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;            2 <br>\nN  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;            2 <br>\nY  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;            1 <br>\nY  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;            1 <br>\n\n[1] Mathematical Sttistics with Applications 7th edition","ad292ae3":"Although I was thinking I might see EXXONMOBIL and CHEVRON at the top producing companies but it seems that back in 2010 the situationwas very different from the market now. In general, There is definitely a relation between the operator and the target variable, this relation might be caused because of other confonding variables such as the value of the company in the market, the number if wells it owns and etc. The operator's Alias feature contains more than 200 unique values, one hot encoding them migth be inefficient so I will try label encoding them."}}