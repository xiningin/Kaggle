{"cell_type":{"73b93572":"code","1642d6c0":"code","c64afa55":"code","9041fc5f":"code","fae7a513":"code","ef76f7a5":"code","5d9aa261":"code","f9b74c16":"code","a30c7f03":"code","e26b10b6":"code","bd99587a":"code","4f54056e":"code","a6c3b1a5":"code","441df228":"code","02f4723d":"code","21d26d90":"code","451cfab5":"code","8c40e38d":"code","2b18cb53":"code","b293b6b0":"code","b2aba704":"code","85d4625c":"code","f4008591":"code","dfabddcc":"code","e10603f9":"code","75171453":"code","6b161d54":"markdown","3783af8f":"markdown","a3afb951":"markdown","9442a3a3":"markdown","bbd7965e":"markdown","b5943cd3":"markdown","856b207f":"markdown","5b0dbe68":"markdown","abcef384":"markdown","e46870e1":"markdown"},"source":{"73b93572":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1642d6c0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, LassoCV, LogisticRegression\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.preprocessing import StandardScaler\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm \nfrom scipy.stats import shapiro","c64afa55":"df = pd.read_csv(\"..\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv\")\ndisplay(df.head())\ndf.tail()","9041fc5f":"tab_info=pd.DataFrame(df.dtypes).T.rename(index={0:'column type'})\ntab_info=tab_info.append(pd.DataFrame(df.isnull().sum()).T.rename(index={0:'null values (nb)'}))\ntab_info=tab_info.append(pd.DataFrame(df.isnull().sum()\/df.shape[0]*100).T.\n                         rename(index={0:'null values (%)'}))\ndisplay(tab_info)","fae7a513":"if 'Serial No.' in df.columns:\n    del df['Serial No.']","ef76f7a5":"df.describe()","5d9aa261":"newdf = df.copy()\n# newdf = newdf.rename(columns={'GRE Score':'GRE','TOEFL Score':'TOEFL','University Rating':'Rating'})\nnewdf.columns = ['GRE','TOEFL','Rating','SOP','LOR','CGPA','Research','Percent']\ny = newdf['Percent']\nif 'Percent' in newdf.columns:\n    del newdf['Percent']","f9b74c16":"fig, axes = plt.subplots(2, 3, figsize=(18, 10))\nsns.boxplot(ax=axes[0, 0], data=newdf['GRE']).set_title('GRE Score')\nsns.boxplot(ax=axes[0, 1], data=newdf['TOEFL']).set_title('TOEFL Score')\nsns.boxplot(ax=axes[0, 2], data=newdf['Rating']).set_title('University Rating')\nsns.boxplot(ax=axes[1, 0], data=newdf['SOP']).set_title('Statement of Purpose')\nsns.boxplot(ax=axes[1, 1], data=newdf['LOR']).set_title( 'Letter of Recommendation')\nsns.boxplot(ax=axes[1, 2], data=newdf['CGPA']).set_title('Undergrad GPA')","a30c7f03":"x = newdf['LOR'].idxmin()\nnewdf.iloc[x]","e26b10b6":"fig, axes = plt.subplots(2, 3, figsize=(18, 10))\nsns.distplot(newdf['GRE'],ax=axes[0, 0],kde=False).set_title('GRE Score')\nsns.distplot(newdf['TOEFL'],ax=axes[0, 1],kde=False).set_title('TOEFL Score')\nsns.distplot(newdf['Rating'],ax=axes[0, 2],bins=5,kde=False).set_title('University Rating')\nsns.distplot(newdf['SOP'],ax=axes[1, 0],bins=9,kde=False).set_title('Statement of Purpose')\nsns.distplot(newdf['LOR'],ax=axes[1, 1],bins=9,kde=False).set_title( 'Letter of Recommendation')\nsns.distplot(newdf['CGPA'],ax=axes[1, 2],kde=False).set_title('Undergrad GPA')\n","bd99587a":"\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\nsm.qqplot(newdf['GRE'],ax=axes[0, 0])\nsm.qqplot(newdf['TOEFL'],ax=axes[0, 1])\nsm.qqplot(newdf['Rating'],ax=axes[0, 2])\nsm.qqplot(newdf['SOP'],ax=axes[1, 0])\nsm.qqplot(newdf['LOR'],ax=axes[1, 1])\nsm.qqplot(newdf['CGPA'],ax=axes[1, 2])\nplt.show()\n","4f54056e":"X_train, X_test, y_train, y_test = train_test_split(newdf,y,test_size=0.2,random_state = 123)\n\nprint (X_train.shape, y_train.shape)\nprint (X_test.shape, y_test.shape)","a6c3b1a5":"scaler = StandardScaler()\nscaler.fit(X_train.fillna(0))","441df228":"lm = LinearRegression()\nlin_m1 = lm.fit(X_train, y_train)\npredictions1 = lm.predict(X_test)","02f4723d":"plt1=plt.scatter(y_test, predictions1)\nplt.plot([0.4,1],[0.4,1],'r')\nplt.title('Full Feature Regression')\nplt.xlabel('Original Value')\nplt.ylabel('Predicted Value')\nlin_m1.score(X_test, y_test)","21d26d90":"clf = LassoCV().fit(X_train, y_train)  # applying lasso\nimportance = np.abs(clf.coef_)   # weight of each column\nprint(importance)","451cfab5":"feature_names=X_train.columns\nidx_third = importance.argsort()[-3]\nthreshold = importance[idx_third] + 0.01\n\nidx_features = (-importance).argsort()[:4]\nname_features = np.array(feature_names)[idx_features]\nprint('Selected features: {}'.format(name_features))\n\nsfm = SelectFromModel(clf, threshold=threshold)\nsfm.fit(X_train, y_train)\nX_transform = sfm.transform(X_train)\n\nn_features = sfm.transform(X_train).shape[1]","8c40e38d":"sfm.fit(X_train, y_train)\n","2b18cb53":"var_sel_X = X_train[name_features]\nlin_m2 = lm.fit(var_sel_X, y_train)\npredictions2 = lm.predict(X_test[name_features])","b293b6b0":"plt.scatter(y_test, predictions2)\nplt.plot([0.4,1],[0.4,1],'r')\nplt.title('Feature Selected Regression')\nplt.xlabel('Original Value')\nplt.ylabel('Predicted Value')\nlin_m2.score(X_test[name_features], y_test)","b2aba704":"plt.scatter(y_test, predictions1, c='b', marker='+', label='full feature')\nplt.scatter(y_test, predictions2, c='r', marker='.', label='select feature')\nplt.plot([0.4,1],[0.4,1],'m')\nplt.legend(loc='upper left')\nplt.show()","85d4625c":"def full_feature(df,i):\n    X_train, X_test, y_train, y_test = train_test_split(newdf,y,test_size=0.2,random_state = i)\n    scaler = StandardScaler()\n    scaler.fit(X_train.fillna(0))\n    lm = LinearRegression()\n    linear_model = lm.fit(X_train, y_train)\n    predictions = lm.predict(X_test)\n    return linear_model.score(X_test, y_test)","f4008591":"def feature_select(df,i):\n    X_train, X_test, y_train, y_test = train_test_split(newdf,y,test_size=0.2,random_state = i)\n    scaler = StandardScaler()\n    scaler.fit(X_train.fillna(0))\n    clf = LassoCV().fit(X_train, y_train)\n    importance = np.abs(clf.coef_)\n    feature_names=X_train.columns\n    idx_third = importance.argsort()[-3]\n    threshold = importance[idx_third] + 0.01\n\n    idx_features = (-importance).argsort()[:4]\n    name_features = np.array(feature_names)[idx_features]\n\n    linear_model = lm.fit(X_train[name_features], y_train)\n    predictions = lm.predict(X_test[name_features])\n    return linear_model.score(X_test[name_features], y_test)","dfabddcc":"# Apply 500 random states and add each accuracy to seperate list\naccuracy1 = []\naccuracy2 = []\nfor i in range(500):\n    accuracy1.append(full_feature(newdf,i))\n    accuracy2.append(feature_select(newdf,i))","e10603f9":"# Create distribution plot for each model result\nsns.distplot(accuracy1,label='Full Feature')\nsns.distplot(accuracy2,label='Select Feature')\nplt.legend()\nplt.show()","75171453":"idx_features = (-importance).argsort()\nfeature = np.array(feature_names)[idx_features]\nweight = sorted(importance,reverse=True)\nfeat_w = pd.DataFrame({'weight':weight},index=feature)\nfeat_w.plot.bar(rot=0,color='red')","6b161d54":"## Split and Scale Data\nNext we will split the data into train and test sets. Also, since the range for each feature differs, scaling is also needed.","3783af8f":"## Look for Outliers in each column\nUsing a boxplot, visually check if there are any outliers within each feature. The research column is excluded as it is a categorical feature with only two options.","a3afb951":"Using the boxplot  notice, how none of the features have an outlier except the Letter of Recommendation column. Pulling out this candidate's data, all of his\/her scores are below the 25th percentile except for the undergraduate GPA. However, it is also beneath the 50th percentile. Hence we can assume a low probability of acceptance.  \nOn the other hand, this data is also valuable and noteworthy, so despite being a outlier will not be removed.","9442a3a3":"## Variable Selection \n* LASSO  \n    Lasso is a method for feature selection. It weighs each feature giving each a coefficient. The coefficient with the highest absolute value becomes the most important feature and so on. Then we can set a threshold for the coefficients such that only values that pass the threshold can be selected. ","bbd7965e":"Import data and check if data is correctly imported. See what data type each column is and check if there are any missing data.","b5943cd3":"Notice how both models have similar results. Simply comparing one random state we can not be sure whether using all features or selected is better. In order to get a better understanding on which is a better solution we will apply the previous two model over several random states.","856b207f":"## Check Distribution\nExamine each feature and see if it is normally distributed. The research columns is again excluded for reasons same as above. Drawing a distribution graph each shape of the distribution for features can be examined. Couple of the features resemble a bell curve representing a normal distribution. In order to ensure the graphs are normally distributed, a qq-plot for each feature. Although the data is not ideally normal it tends to have a bell curve shape. Examining the qq-plot, notice the university rating, statement of purpose, and letter of recommendation graph is more of a stairways shape. This is due to the data not being entirely continuous but rather discrete.","5b0dbe68":"## Understand Each Column\n1. Serial Number: unique id for each applicant\n2. GRE Score\n3. TOEFL Score\n4. University Rating\n5. Statement of Purpose\n6. Letter of Recommendation\n7. Undergraduate GPA\n8. Research Experience\n9. Chance of Admission (Response column)   \n\nWe can drop the Serial Number as it does not affect the chance of admission.","abcef384":"## Create a Linear Regression with All Features","e46870e1":"Full feature is slightly more accurate skewed towards the right meaning it is more accurate generally in predicting. It seems wise to select all features for this regression and so we will now analyze on which factor is most heavily weighted. This is acutally performed while using LASSO regression. When selecting which features to use, each one was weighted.  \nAs the graph below shows, undergraduate GPA has the most significance when applying to grad school followed by research experience and letter of recommendation. From this people can understand it is important to take care of one's grades during college in order to apply grad school."}}