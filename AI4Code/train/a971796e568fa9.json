{"cell_type":{"e50542ea":"code","1ca89974":"code","0ce29861":"code","dfda9c9f":"code","6a1b19c9":"code","97ffc52b":"code","35ffabc3":"code","74525c08":"code","c915882e":"code","0273a830":"code","3c80aea8":"code","70743771":"code","66f7630b":"code","96bdf51a":"code","e8531b9f":"code","4b719efa":"code","38006b2a":"code","cf50182b":"code","28766534":"code","38525c72":"code","dd455083":"code","3990c4d2":"code","bb8e23cf":"code","e107ca00":"code","44c70a9f":"code","6b09eb49":"code","efb1e3f7":"code","83504cbe":"code","da88e4f8":"markdown","e8fb39f1":"markdown","4728e2ef":"markdown","fa3e3dea":"markdown","d6e91d6b":"markdown","61d9851e":"markdown","8157ec78":"markdown","03b41d0a":"markdown","cc9f7366":"markdown","efdc7c16":"markdown","4de409d3":"markdown","c4b2ad0a":"markdown","d267df46":"markdown","b293c542":"markdown","3f7821fb":"markdown","eb8ea8a8":"markdown","5a469933":"markdown","11ec0483":"markdown","0ba79df8":"markdown","3412a404":"markdown","d615143a":"markdown","789b3512":"markdown"},"source":{"e50542ea":"classes = ['blank','non_blank']","1ca89974":"def random_seed(seed_value, use_cuda=True):\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars\n    random.seed(seed_value) # Python\n    if use_cuda: \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False","0ce29861":"from fastai.vision import *\nimport cv2\nimport numpy as np\nfrom PIL import Image, ImageFile\nfrom fastprogress import master_bar, progress_bar\nimport torch\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.nn import Sequential, Linear, CrossEntropyLoss\nimport torchvision\nfrom torchvision import datasets, models, transforms\nfrom torchvision.transforms.functional import pad as TorchPad, to_tensor\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, log_loss","dfda9c9f":"class Resize2(object):\n    '''\n    Resize object along LONGER border (tarnsforms.Resize works along smaller border)\n    '''\n    def __init__(self, new_max_size, interpolation=Image.NEAREST):\n        self.new_max_size = new_max_size\n        self.interpolation = interpolation\n\n    def __call__(self, img):\n        old_size = img.size[:2]\n        ratio = float(self.new_max_size)\/max(old_size)\n        new_size = tuple([int(x * ratio) for x in old_size])\n        return img.resize(new_size, resample=self.interpolation)\n    \n\nclass SquarePad(object):\n    '''\n    Square img by extending smaller border to longer border size and filling empty space  \n    '''\n    def __init__(self, sqr_size, padding_mode=\"reflect\"):\n        self.sqr_size = sqr_size\n        self.padding_mode = padding_mode\n        \n    def __call__(self, img):\n        old_size = img.size[:2]\n        pad_size = [0,0]\n        pad_size.extend([self.sqr_size - x for x in old_size])\n        return TorchPad(img, tuple(pad_size), padding_mode=self.padding_mode)","6a1b19c9":"MAX_SIZE = 500\ncolor_mean =[0.485, 0.456, 0.406]\ncolor_std=[0.229, 0.224, 0.225]\n\ntransforms = transforms.Compose([\n        Resize2(MAX_SIZE),\n        SquarePad(MAX_SIZE),\n        transforms.ColorJitter(brightness=(0.9,1.2), contrast=(0.8, 1.2),saturation=(0.8, 1.1)), \n        transforms.ToTensor(),\n        transforms.Normalize(color_mean, color_std)\n    ])","97ffc52b":"train_set = datasets.ImageFolder(\"..\/input\/serengeti2\/train\", transforms)\nvalid_set = datasets.ImageFolder(\"..\/input\/serengeti2\/test\", transforms)","35ffabc3":"BATCH_SIZE = 32\n\nrandom_seed(123)\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE,shuffle=True, num_workers=4)\nrandom_seed(123)\nval_loader = torch.utils.data.DataLoader(valid_set, batch_size=BATCH_SIZE,shuffle=True, num_workers=4)","74525c08":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#torch.cuda.device_count() # print how many GPU is available","c915882e":"def show_image(dataset, image_nr):\n    message = \"True label: %s, %s\" % (str(dataset[image_nr][1]), classes[dataset[image_nr][1]])\n    \n    inp = np.array(dataset[image_nr][0]).transpose(1,2,0)\n    plt.imshow(np.clip(color_std * inp + color_mean, 0, 1))\n    plt.title(message)\n    plt.axis('off')\n    plt.show()","0273a830":"show_image(train_set, image_nr=2031)","3c80aea8":"!pip install torch-lr-finder","70743771":"from torch_lr_finder import LRFinder","66f7630b":"def define_model():\n    random_seed(123)\n    model = models.resnet34(pretrained=True)\n    for param in model.parameters():\n        param.requires_grad = False\n\n    num_ftrs = model.fc.in_features\n    # you can stack multiple linear layers here\n    model.fc = Sequential(Linear(num_ftrs, len(classes)))\n\n    model = model.to(device) # przerzucamy model na GPU\n    return model","96bdf51a":"model = define_model()\ncriterion = CrossEntropyLoss()","e8531b9f":"# search for best learning rate\noptimizer = optim.Adam(model.fc.parameters(), lr=0.00001) # only for range_test\nlr_finder = LRFinder(model, optimizer, criterion, device=device)\nrandom_seed(123)\nlr_finder.range_test(train_loader, end_lr=10, num_iter=100)","4b719efa":"lr_finder.plot()","38006b2a":"model = define_model()\ncriterion = CrossEntropyLoss()\noptimizer = optim.Adam(model.fc.parameters(), lr=0.02)","cf50182b":"def return_y_output(data, optimizer=optimizer):\n    X, y = data\n    X, y = X.to(device), y.to(device)\n\n    optimizer.zero_grad()\n    model.eval()\n    outputs = model(X)\n    return y, outputs\n\n\ndef model_predict(model, data_loader, labels):\n    all_targets = np.array([])\n    all_pred = np.array([])\n    all_proba = []\n    losses = np.array([])\n    valid_loss = 0.0\n    \n    with torch.no_grad():\n        for data in data_loader:\n            X, y = data\n            Xes = np.vstack([Xes, X]) if 'Xes' in locals() else X\n            X, y = X.to(device), y.to(device)\n            out = model(X)\n            loss = criterion(out, y)\n            valid_loss += loss.item()\n            # about softmax - https:\/\/discuss.pytorch.org\/t\/vgg-output-layer-no-softmax\/9273\/6?u=mielnicka\n            out = F.softmax(out, dim=1)\n            for p in out:\n                all_proba.append(list(np.array(p)))\n            _, pred = torch.max(out, 1)\n            # append values to arrays\n            all_targets = np.append(all_targets, y.squeeze())\n            all_pred = np.append(all_pred, pred)\n            batch_losses = [log_loss([int(y[n])], [np.array(out[n])], labels=labels)\n                            for n in range(len(y))]  \n            losses = np.append(losses, batch_losses)\n    return Xes, all_targets, all_pred, all_proba, losses, valid_loss\n\n\ndef train_one_epoch(n_epochs = 1, optimizer=optimizer, is_cyclic= False):\n    mb = master_bar(range(n_epochs))\n    for epoch in mb:\n        train_loss = 0.0\n        for data in progress_bar(train_loader, parent=mb):\n            y, outputs = return_y_output(data, optimizer=optimizer)\n            # return logits, jak chc\u0119 probabilities, owi\u0144 to w F.Softmax(x, dim=1) \n            loss = criterion(outputs, y)  \n            loss.backward()\n            if is_cyclic:\n                scheduler.step()\n            optimizer.step()\n            train_loss += loss.item()\n        _, valid_targets, valid_predicted, _, _, valid_loss = model_predict(model, val_loader, labels=range(len(classes)))\n        val_prec = precision_score(valid_targets.astype(int), valid_predicted.astype(int))\n        val_recall = recall_score(valid_targets.astype(int), valid_predicted.astype(int))\n        mb.write(f'EPOCH: {epoch}, train_loss: {train_loss:03.6f}, val_loss: {valid_loss:03.6f}, val_prec: {val_prec:02.4f}, val_recall: {val_recall:02.4f}')\n    mb.write(f'FINISHED TRAINING.')","28766534":"step_size = len(train_loader)\n# adjust learning rate based on lr_finder.plot() values \nscheduler = torch.optim.lr_scheduler.CyclicLR(optimizer,step_size_up=step_size,mode='exp_range',\n                                 base_lr = 0.003, max_lr=0.02, cycle_momentum=False)","38525c72":"random_seed(123)\ntrain_one_epoch(3)","dd455083":"torch.save(model, \"\/kaggle\/working\/model_stage1.pkl\")","3990c4d2":"valid_X, valid_targets, valid_predicted, val_probability, valid_losses, valid_loss = model_predict(model, val_loader, labels=range(len(classes)))","bb8e23cf":"conf_matrix = confusion_matrix(valid_targets, valid_predicted)\nsns.heatmap(conf_matrix, annot=True, cmap=\"Greens\",cbar=False,\n            xticklabels = sorted(classes), yticklabels = sorted(classes));","e107ca00":"from torch import topk\nfrom torch.autograd import Variable\nimport skimage.transform","44c70a9f":"# get the index of mispredicted and well predicted images in `valid_predicted` list\nwrong_labeled_valid_idx = []\nfor n in range(len(valid_predicted)):\n    if (int(valid_predicted[n]) != int(valid_targets[n])):\n        wrong_labeled_valid_idx.append(n)\nwell_labeled_valid_idx = list(set(range(len(valid_targets))) - set(wrong_labeled_valid_idx))","6b09eb49":"class SaveFeatures():\n    features=None\n    def __init__(self, last_layer): self.hook = last_layer.register_forward_hook(self.hook_fn)\n    def hook_fn(self, module, input, output): self.features = ((output.cpu()).data).numpy()\n        \n\ndef getCAM(feature_conv, selected_weights):\n    _, n_connections, conv_height, conv_weight = feature_conv.shape\n    # multiple activated connections with conv features\n    cam = selected_weights.dot(feature_conv.reshape((n_connections, conv_height*conv_weight)))\n    # return 16 x 16 = (1, 256) array\n    # now reshape it to (16,16) array\n    cam = cam.reshape(conv_height, conv_weight)\n    # normalize values to zero-one\n    cam_img = (cam - np.min(cam)) \/ (np.max(cam) - np.min(cam))\n    return cam_img\n\n\ndef getHeatmap(image, mode=model):\n    X_tensor = (image.unsqueeze(0)).cuda()\n    X_tensor.requires_grad=True\n\n    # get model features which goes from the last Resnet layer to `fc` layer (the last layer indicating which label was activated)\n    activated_features = SaveFeatures(model._modules.get('layer4'))\n\n    pred = model(X_tensor)\n    pred_proba = F.softmax(pred, dim=1).data.squeeze()\n\n    # take weights activated by the image during the prediction\n    # weight_softmax has length equal to the number of classes (here 2)\n    weight_softmax_params = [x for x in model._modules.get('fc').parameters()]\n    weight_softmax = np.squeeze(weight_softmax_params[0].cpu().data.numpy())\n\n    # choose predicted class\n    class_idx = topk(pred_proba,1)[1].int()\n\n    # knowing which class was predicted, let's take weights leading to that class\n    # along with activated_features they'll be used to compute the heatmap\n    heatmap = getCAM(activated_features.features, weight_softmax[class_idx])\n    return heatmap\n\n\ndef print_heatmaps(images_number_list, columns=3, w=21, max_n_images = 30):\n    '''\n    display validation images returned by model_predict\n    '''\n    # to prevent printing hundrets of images\n    n_images_to_display = min(len(wrong_labeled_valid_idx), max_n_images)\n    rows = np.ceil(n_images_to_display \/ columns).astype(int)\n    h = rows * 7\n    fig = plt.figure(figsize=(w, h))\n    for i, image_nr in enumerate(images_number_list[:n_images_to_display], start=1):\n        fig.add_subplot(rows, columns, i)\n        plt.title('True label: %s\\nPredicted:%s, Probability: %s' % (classes[int(valid_targets[image_nr])],\n                                                   classes[int(valid_predicted[image_nr])], str(val_probability[image_nr])))\n        plt.axis('off')\n        image = to_tensor(valid_X[image_nr].transpose(1,2,0))#valid_set[488][0]\n        heatmap = getHeatmap(image)\n        img = np.array(image).transpose(1,2,0)\n        plt.imshow(np.clip(color_std * img + color_mean, 0, 1))\n        plt.imshow(skimage.transform.resize(heatmap, img.shape[0:2]), alpha=0.2, cmap='jet')\n    plt.show()","efb1e3f7":"print_heatmaps(wrong_labeled_valid_idx, max_n_images = 15)","83504cbe":"print_heatmaps(well_labeled_valid_idx, max_n_images = 15)","da88e4f8":"## Pytorch\nThe model is written in **Pytorch 1.1.0**. To create **deterministic model**, Pytorch needs custom seed, which will be called before any training. I've added it also before loading the data, to make the shuffling process deterministic.","e8fb39f1":"## Validate\nPredict on validation set and plot confusion matrix.","4728e2ef":"I've tested several optimizers (Adagrad, Adamax, Rmsprop, SGD with Nesterov) and **Adam** seemed to perform the best.\n\nBest learning rate has been found using [LRFinder](https:\/\/github.com\/davidtvs\/pytorch-lr-finder). The rule of thumb is to choose (for cyclical leraning rates) the points where the steepest decreesing slope starts and ends. In this case it is `0.003` and `0.02`.","fa3e3dea":"## Training\n\nWe will evaluate the model using precision and recall, both calculated for `non-blank`.Ideally, all the `non-blank` images would be predicted well even in the stake of more incorrectly predicted `blank`s. It means we prefer **recall to be close to 1**, with as high precision as possible.","d6e91d6b":"# What next?\nWhat are the final thoughts, what would i change?\n1. If possible, include subset of all locations to the training set. It will prevent from misrecognizing trunks or stones as animals.\n2. Test more data augmentation.\n3. include more images. Although the model works good, we expect the recall to be close to 1. \n4. Investigate if probability cutoff can be shifted (ex. probability of `non_blank` == 0.45 will be labeled as `non_blank`)","61d9851e":"As we can see below, the model mislabel in specific cases:\n- when the **sunlight** cover most of the picture\n- the **fallen trunk** to the right in the background is recognized as animal\n- recognizing **over\/underexposet rabbits**","8157ec78":"## Wrong labeled images\nThe validation images come from location unseen for the model. Thus \n- there might be **species not well identified** as animal\n- there might be **objects recognized as animals**","03b41d0a":"# Initial notes\n## Problem description\n**Camera traps** are wildely used in **wildlife research**. To avoid recording when noting happens, the camera is usually triggered when the move is detected. Nonetheless **wind** can also move the plants and **trigger recording**, causing **vast amount of snapshots to be blank** (without animals recorded). In the research process they will be manually excluded by researchers, which is mundane and time-consumig job.\n\nIt encouraged me to test if it's hard to use **transfer learning** to train the model **recognizing blank images**.","cc9f7366":"Each image is **normalized** to adjust to the distribution of images used to train the Resnet34 model. A little data augmentation is added (`ColorJitter`).","efdc7c16":"The model uses **Cyclical Learning Rates** (CLR). Instead of using constant learning rate, in CLR the learning rate (*lr*) changes between `max_lr` to `base_lr`.\n\nIt this case it oscilates, increasing and decreasing the *lr* lineary. The `step_size_up` determines the number of iterations for one amplitude (single increasing). In this case its is the number of batches in train set, which means that within one epoch, the cycle of *lr* will change once. \n\nYou can find more details about **Cyclical Learning Rates** in [this paper](https:\/\/arxiv.org\/abs\/1506.01186).","4de409d3":"## Well labeled images\nIt seems that animals are well recognized. Even though the trunk in the background is recognized as animal (on blank images visible as blue  circle), the model has learned to ignore it.","c4b2ad0a":"# Heatmap\nWe'll print the heatmap to display **which pixels are recognized as given class** representation. It will help to determine:\n- which images are missclasified? Do they have something in common?\n- do well classified files sufer overfitting?\n\nThe heatmap is based on the code from [snappishproductions](http:\/\/snappishproductions.com\/blog\/2018\/01\/03\/class-activation-mapping-in-pytorch.html) blog. I've added comments to the code below to briefly explain how does it work.","d267df46":"# Model","b293c542":"The batch size is choosed arbitrary.","3f7821fb":"Create the model once again (it seems that `lr_finder` modified the weights. Recreating the model is the easiest solution).","eb8ea8a8":"## End","5a469933":"# Load the data","11ec0483":"## About the Dataset\nI've used the data form [Serengeti Dataset](http:\/\/lila.science\/datasets\/snapshot-serengeti) to train and test the model. The data consists of camera trap images from Serengeti National Park in Tanzania. About **75% percent of images are blank**. There are >1TB of images labeled by numerous volunteers in [Zooniverse](https:\/\/www.zooniverse.org\/projects\/zooniverse\/snapshot-serengeti). For th emodel in this notebook, about **3400 images** were used. It has two purposes:\n- to represent real live data of money\/time limited research team (which has few time to label more images or a few money to buy camera traps)\n- to fit in Kaggle GPU limits\n\nThe dataset consists of images from **4 locations**: **3** of them are in the **train set** and **1** is in the **validation set**. Each location is represented by images from one roll (images taken to the next battery change). The images were labeled with species names, which I turned into **non_blank** class. **Blank** class (no animals present) was remained. I've chosen the data so that the dataset will be **balanced**. I revised the dataset to ensure the labels are valid. About 40 problematic images were excluded (animals are very far or hardly visible). They are indeed very valuable for the model, but they could be misleading for small datasets. Besides that I aimed to train **proof of concept model**, which can be a subject of further experiments.","0ba79df8":"Display sample image:","3412a404":"Load the `resnet34` model, add the linear function on the top of the model (`model.fc`). The training pipeline is inspired by the notebook from [FastAI course Lesson 2](https:\/\/github.com\/fastai\/course-v3\/blob\/master\/nbs\/dl1\/lesson2-download.ipynb).","d615143a":"Even though the recall is high, there are several mislabeled `non-blank` images. We'll investigate them in the next section.","789b3512":"The model will be created using **transfer learnig**. It will be build with pretrained **Resnet34** and **single Linear layer** on the top. The images need to be reshaped to adjust them to the square resnet shape:\n1. Rescale all images to have the same size of the longer border. \n2. Keeping aspect ratio, add padding so that th eshorter border will be the same length as the longer one. Fill the padding space with reflection of the image. \n\nTo do this, we'll use two custom functions: `Resize2` and `SquarePad`. As the result, the images are **squared and the same size**."}}