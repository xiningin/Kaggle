{"cell_type":{"5f857709":"code","99830ff7":"code","4dba407d":"code","886553e5":"code","f1f99e7b":"code","5821d822":"code","aa9cb123":"code","60360bc5":"code","21e89787":"code","77b358d2":"code","ba65b4a9":"code","f5a0de58":"code","abc50761":"code","95bd5112":"code","aaf6abd6":"code","f034b5e8":"code","6b8254a6":"code","af7eb611":"code","2740e9e4":"code","b528f4b2":"code","7aae33ea":"code","2b63b8db":"code","459b20a5":"code","8de212d0":"code","5d1ae762":"code","ab4d0619":"code","72ddda8c":"markdown","562ed135":"markdown","9239c9ba":"markdown","31841830":"markdown","511d07d3":"markdown","197cc795":"markdown","0fc32666":"markdown"},"source":{"5f857709":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os, gc, warnings\nimport seaborn as sns\n\nimport category_encoders as ce\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, StratifiedKFold,TimeSeriesSplit\nimport lightgbm as lgb\nfrom sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)\n#display notebook in full width\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:100% !important; }<\/style>\"))\n\nwarnings.filterwarnings(\"ignore\", message=\"categorical_feature in Dataset is overridden\")\nwarnings.filterwarnings(\"ignore\", message=\"categorical_feature in param dict is overridden\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", message=\"F-score is ill-defined and being set to 0.0 due to no predicted samples\")\n\nrandom_state = 42\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","99830ff7":"def get_class_from_prob(y_prob):\n        return  [0  if x < THR else 1 for x in y_prob]\n    \ndef write_predictions(sub, y_prob, filename ):          \n#     sub_prob = sub.copy()\n    sub[TARGET] = y_prob    \n    sub.to_csv( filename, index = False)\n    \n    y_pred =   get_class_from_prob(y_prob) \n    print('Test prediction:Postive class count {}, Percent {:0.2f}'.format(sum(y_pred), sum(y_pred) * 100 \/ len(y_pred)))\n    return sub\n\n\ndef set_ordinal_encoding(data, cat_cols):       \n    for col in [x for x in cat_cols if data[x].dtype == 'object']:\n        data[col], uniques = pd.factorize(data[col])\n        #the factorize sets null values to -1, so convert them back to null, as we want LGB to handle null values\n        data[col] = data[col].replace(-1, np.nan)\n    print('Finished: Ordinal Encoding')\n    return data\n\ndef get_train_test(df, features, ID):\n    X_train =  df[df[TARGET].notnull()]\n    X_test  =  df[df[TARGET].isnull()]\n    y_train = X_train[TARGET]\n    sub = pd.DataFrame()\n    sub[ID] = X_test[ID]\n    X_train = X_train[features]\n    X_test = X_test[features]\n    return X_train, X_test, y_train, sub\n\ndef plot_feature_imp(feature_imp, top_n = 30):\n    feature_imp = feature_imp.sort_values(['importance'], ascending = False)\n    feature_imp_disp = feature_imp.head(top_n)\n    plt.figure(figsize=(10, 12))\n    sns.barplot(x=\"importance\", y=\"feature\", data=feature_imp_disp)\n    plt.title('LightGBM Features')\n    plt.show() \n    \n\n\ndef cv_results(y_valid, y_prob, verbose = True):   \n    scores = {}                      \n    y_pred_class =  [0  if x < 0.5 else 1 for x in y_prob]\n    scores['cv_accuracy']  = accuracy_score(y_valid, y_pred_class)\n    scores['cv_auc']       = roc_auc_score(y_valid, y_prob)\n    scores['cv_f1']      =   f1_score(y_valid, y_pred_class, average = 'binary')\n    if verbose:\n        print('CV accuracy {:0.6f}'.format( scores['cv_accuracy'] ))\n        print('CV AUC  {:0.6f}'.format( scores['cv_auc']   ))\n        print('CV F1 %0.6f' %scores['cv_f1'] )\n    return scores  \n\n\ndef run_lgb_with_cv(params, X_train, y_train, X_test,cat_cols, shuffle_split = True, \n                    test_size =0.2, verbose_eval = 100, esr = 300 ):\n    \n    if shuffle_split: \n        X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = test_size , \n                                                  random_state = random_state, stratify = y_train)\n    else:\n        #since data is sorted according to time, the split will be according to time with shuffle = False\n        X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = test_size , \n                                                      random_state = random_state, shuffle = False)\n  \n    print('Train shape{} Valid Shape{}, Test Shape {}'.format(X_train.shape, X_valid.shape, X_test.shape))\n    print('Number of Category Columns {}:'.format(len(cat_cols)))\n\n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_valid  = lgb.Dataset(X_valid, y_valid)\n   \n    lgb_results = {}    \n   \n    \n    model = lgb.train(params,\n                      lgb_train,\n                      num_boost_round = 10000,\n                      valid_sets =  [lgb_train,lgb_valid],  #Including train set will do early stopiing for train instead of validation\n                      early_stopping_rounds = esr,                      \n                      categorical_feature = cat_cols,\n                      evals_result = lgb_results,\n                      verbose_eval = verbose_eval\n                       )\n    y_prob_valid = model.predict(X_valid)    \n    cv_results(y_valid, y_prob_valid, verbose = True)\n  \n    feature_imp = pd.DataFrame()\n    feature_imp['feature'] = model.feature_name()\n    feature_imp['importance']  = model.feature_importance()\n    feature_imp = feature_imp.sort_values(by = 'importance', ascending= False )\n    return model, feature_imp, lgb_results\n\ndef run_lgb_no_cv(params, X_train, y_train, X_test,cat_cols,num_rounds = 100, verbose_eval = 100, ):    \n      \n    print('Train shape{}  Test Shape {}'.format(X_train.shape, X_test.shape))\n    print('Number of Category Columns {}, Number of Rounds {}:'.format(len(cat_cols), num_rounds))\n   \n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_results = {}    \n\n    warnings.filterwarnings(\"ignore\", message=\"categorical_feature in Dataset is overridden\")\n    model = lgb.train(params,\n                      lgb_train,\n                      num_boost_round = num_rounds,                 \n                      categorical_feature = cat_cols,\n                      evals_result = lgb_results,\n                      valid_sets =  [lgb_train],\n                      verbose_eval = verbose_eval\n                       )\n  \n    feature_imp = pd.DataFrame()\n    feature_imp['feature'] = model.feature_name()\n    feature_imp['importance']  = model.feature_importance()\n    feature_imp = feature_imp.sort_values(by = 'importance', ascending= False )\n    return model, feature_imp\n\ndef plot_results(history):\n    tr_auc =history['training']['auc']\n    val_auc = history['valid_1']['auc']\n   \n    n_iter = list(range(1, len(history['training']['auc']) + 1))\n\n    plt.figure(figsize = (12, 6))\n#     plt.subplot(1,2,1)\n    plt.plot(n_iter , tr_auc, 'b', label = 'Training AUC')\n    plt.plot(n_iter , val_auc, 'r', label = 'Validation AUC')\n    plt.grid(True)\n    plt.legend()\n    plt.xlabel('iteration')  \n\n    plt.show()","4dba407d":"%%time\n# data_path = 'data'\ndata_path = '\/kaggle\/input\/av-amex-coupon\/data\/'\nTARGET = 'redemption_status'\nTHR = 0.5\n\ntrain =  pd.read_csv(os.path.join(data_path, 'train.csv'))\ntest =  pd.read_csv(os.path.join(data_path, 'test.csv'))\ncampaign = pd.read_csv(os.path.join(data_path, 'campaign_data.csv'))\ncoupon  = pd.read_csv(os.path.join(data_path, 'coupon_item_mapping.csv'))\ncust_demo = pd.read_csv(os.path.join(data_path, 'customer_demographics.csv'))\ncust_trans = pd.read_csv(os.path.join(data_path, 'customer_transaction_data.csv'))\nitem = pd.read_csv(os.path.join(data_path, 'item_data.csv'))\ndata = train.append(test, ignore_index = True, sort = False)\nprint('Train  - rows:', train.shape[0], 'columns:', train.shape[1])\nprint('Test  - rows:',  test.shape[0], 'columns:', test.shape[1])\nprint('Campaign  - rows:',  campaign.shape[0], 'columns:', campaign.shape[1])\nprint('Coupon  - rows:',  coupon.shape[0], 'columns:', coupon.shape[1])\nprint('Cust_demo   - rows:',  cust_demo.shape[0], 'columns:', cust_demo .shape[1])\nprint('Cust_trans   - rows:',  cust_trans.shape[0], 'columns:', cust_trans.shape[1])\nprint('item  - rows:',  item.shape[0], 'columns:', item.shape[1])\nprint('combined data - rows:',  data.shape[0], 'columns:', data.shape[1])","886553e5":"campaign['start_date'] = pd.to_datetime(campaign['start_date'], format = '%d\/%m\/%y')\ncampaign['end_date'] = pd.to_datetime(campaign['end_date'],format = '%d\/%m\/%y')\ncust_trans['date'] = pd.to_datetime(cust_trans['date'])\n","f1f99e7b":"data = pd.merge(data, campaign, how = 'left', on =['campaign_id'])\ndata = pd.merge(data, cust_demo, how = 'left', on =['customer_id'])\nprint(data.shape)\n","5821d822":"cust_trans = pd.merge(cust_trans, item, how = 'left', on =['item_id'])\ncust_trans = cust_trans.drop_duplicates(subset= cust_trans.columns.tolist(), keep= 'first')\n\n# trans_merge = pd.merge(train_merge, cust_trans, how = 'left', on =['customer_id'])\nprint(cust_trans.shape)\n","aa9cb123":"# Number of redemmptions by customer: Overftiing do not use due to target leak\n# col ='red_count'\n# df = data.groupby(['customer_id']).agg({'redemption_status':'sum'}).reset_index()\n# df.columns = ['customer_id', col]\n# data = data.merge(df, how = 'left', on = 'customer_id')\n# data[data[TARGET] == 0]","60360bc5":"# # Number of redemptions before start date of event for customer\n# col ='cust_red_before'\n# df = data.groupby(['customer_id', 'start_date']).agg({'redemption_status':'sum'})\n# df.columns = ['red_count']\n# df = df.reset_index()\n# df['red_sum'] = df.groupby('customer_id')['red_count'].apply(lambda x : x.cumsum())\n# df[col] = df['red_sum'] - df['red_count']\n\n\n# data = data.merge(df[['customer_id', 'start_date', 'cust_red_before']], on = ['customer_id', 'start_date'], how = 'left')\n\n# # Number of redemptions before start date of event for a coupon\n# col ='coup_red_before'\n# df = data.groupby(['coupon_id', 'start_date']).agg({'redemption_status':'sum'})\n# df.columns = ['red_count']\n# df = df.reset_index()\n# df['red_sum'] = df.groupby('coupon_id')['red_count'].apply(lambda x : x.cumsum())\n# df[col] = df['red_sum'] - df['red_count']\n# data = data.merge(df[['coupon_id', 'start_date', col]], on = ['coupon_id', 'start_date'], how = 'left')\n\n# # Redemption Count and or Features\n# col = 'red_count_and'\n# data[col] =  data['cust_red_before'] *  data['coup_red_before']\n# data[col] = data[col].apply(lambda x: 0 if x ==0 else 1)\n\n\n\n# col = 'red_count_or'\n# data[col] =  data['cust_red_before'] +  data['coup_red_before']\n# data[col] =  data[col].apply(lambda x: 0 if x ==0 else 1)\n\n\n","21e89787":"\ncol = 'coup_purc_count'\ndf = cust_trans.merge(coupon, on = 'item_id', how = 'inner')\ndf = data.merge(df, on = ['customer_id', 'coupon_id'], how = 'inner')\ndf = df[df.date < df.start_date]\n\n# agg_map = { 'redemption_status':['mean', 'sum']}\ndf = df.groupby(['customer_id', 'coupon_id']).agg({'id':'count'})\ndf.columns = [col]\ndf = df.reset_index()\n\ndata = data.merge(df, on = ['customer_id', 'coupon_id'], how = 'left' )\ndata[col] = data[col].fillna(0)\n\n\n# data[data[TARGET] ==1].head()","77b358d2":"# Number of redemptions for a coupon by all customers\ncol = 'coup_disc_count'\ndf = cust_trans[cust_trans.coupon_discount != 0]\ndf = df.merge(coupon, on = 'item_id', how = 'inner')\n\ndf_train = data.groupby(['coupon_id', 'start_date', 'end_date']).agg({'id':'count'})\ndf_train = df_train.reset_index()\n\ndf = df_train.merge(df, how = 'inner', on = ['coupon_id'])\ndf = df[df.date < df.start_date]\n\n\ndf = df.groupby('coupon_id').agg({'coupon_id':'count'})\ndf.columns = [col]\ndf = df.reset_index()\n\ndata = data.merge(df, on = [ 'coupon_id'], how = 'left' )\ndata[col] = data[col].fillna(0)\n\ndata[data[TARGET] ==1].head()\n","ba65b4a9":"## Magic Feature\ncol = 'red_magic'\ndata[col] = data['coup_purc_count'] * data['coup_disc_count']\ndata[col] = data[col].apply(lambda x: 0 if x ==0 else 1)","f5a0de58":"# Nunber of trasnactions where coupon redemetion was done before event start date\ndef find_coupon_counts(df_data):\n    col = 'coup_count_b'  \n\n    df = cust_trans[cust_trans.coupon_discount !=0]\n    df = df_data.merge(df, how = 'left', on = 'customer_id')\n    df  = df[(df.date < df.start_date )]\n\n    #Only keep tranasaction where coupon and item match with scheme\n    df = df.merge(coupon, on = ['coupon_id','item_id'], how = 'inner')\n\n\n    #for a cutomer and coopn find number of transations were coupons was redeemed\n    df =  df.groupby(['coupon_id', 'customer_id']).agg({'coupon_id':'count'})\n    df.columns = [col]\n    df =df.reset_index()\n\n    df_data = pd.merge(df_data, df, how = 'left', on = ['coupon_id', 'customer_id'])\n    df_data[col] = df_data[col].fillna(0)\n    return df_data\n\ndata = find_coupon_counts(data)\n# data[data[TARGET] ==1].head()","abc50761":"## Aggregates for customer transaction before event start date\nagg_map = {'customer_id': 'count', 'selling_price':'sum' ,'quantity':'sum', 'coupon_discount':'sum', 'other_discount':'sum'}\ndf = cust_trans.groupby(['date', 'customer_id']).agg(agg_map)\n\ndf.columns = ['c_trans_count', 'selling_price_mean', 'quantity_mean', 'coupon_discount_mean', 'other_discount_mean']\ndf  = df.reset_index()\ndf_train = data.groupby(['customer_id', 'start_date', 'end_date']).agg({'coupon_id':'count'})\ndf_train = df_train.reset_index()\n\ndf = data.merge(df, how = 'inner', on = ['customer_id'])\ndf = df[df.date < df.start_date]\ndf = df.drop_duplicates(subset = ['customer_id', 'date'], keep = 'first')\n\nagg_map = {'c_trans_count':'sum', 'selling_price_mean':'sum' ,'quantity_mean':'sum', 'coupon_discount_mean':'sum', 'other_discount_mean':'sum'}\ndf = df.groupby(['customer_id']).agg(agg_map)\ndf = df.reset_index()\n\nfor col in ['selling_price_mean' ,'quantity_mean', 'coupon_discount_mean',  'other_discount_mean']:\n    df[col] = df[col] \/ df['c_trans_count']\n\n# df['cd_sp_ratio'] = df['coupon_discount_mean'] \/ df['selling_price_mean']\n# df['cd_od_ratio'] = df['coupon_discount_mean'] \/ df['other_discount_mean']\n# df['od_sp_ratio'] = df['other_discount_mean'] \/ df['selling_price_mean']\n# df['tot_disc_mean'] =  df['coupon_discount_mean']  + df['other_discount_mean']\n\n\ndata = data.merge(df, on = ['customer_id'], how = 'left' )\n\ndata[data[TARGET] ==1].head()\n\n","95bd5112":"## Aggregates for customer transaction with coupon discounts Before start date\n# agg_map = {'customer_id': 'count', 'selling_price':'sum' ,'quantity':'sum', 'coupon_discount':'sum', 'other_discount':'sum'}\n\n# df = cust_trans[cust_trans.coupon_discount !=0 ]\n# df = df.groupby(['date', 'customer_id']).agg(agg_map)\n\n# df.columns = ['trans_count_d', 'selling_price_mean_d', 'quantity_mean_d', 'coupon_discount_mean_d', 'other_discount_mean_d']\n# df  = df.reset_index()\n# df_train = data.groupby(['customer_id', 'start_date', 'end_date']).agg({'coupon_id':'count'})\n# df_train = df_train.reset_index()\n\n# df = df_train.merge(df, how = 'inner', on = ['customer_id'])\n# df = df[df.date < df.start_date]\n# df = df.drop_duplicates(subset = ['customer_id', 'date'], keep = 'first')\n\n# agg_map = {'trans_count_d':'sum', 'selling_price_mean_d':'sum' ,'quantity_mean_d':'sum', 'coupon_discount_mean_d':'sum', 'other_discount_mean_d':'sum'}\n# df = df.groupby(['customer_id']).agg(agg_map)\n# df = df.reset_index()    \n\n# data = data.merge(df, on = ['customer_id'], how = 'left' )\n\n\n# for col in ['trans_count_d', 'selling_price_mean_d' ,'quantity_mean_d', 'coupon_discount_mean_d',  'other_discount_mean_d']:   \n#     data[col] = data[col].fillna(0)     \n    \n#     if col != 'trans_count_d':\n#         data[col] = data[col] \/ data['trans_count_d']        \n#         data[col] = data[col].fillna(0)            \n\n\n# data['trans_ratio'] = data['trans_count_d'] \/ data['c_trans_count'] \n# data['disc_ratio'] =  data['coupon_discount_mean_d'] \/ data['other_discount_mean_d'] \n# data['disc_ratio']  = data['disc_ratio'].fillna( 0)\n# data['disc_ratio']  = data['disc_ratio'].replace(-np.inf, 0)\n# # data[data[TARGET] ==0].head()\n\n","aaf6abd6":"# Nunmber of items per coupon\ncol = 'coup_item_count'\ndf = coupon.groupby('coupon_id').agg({'item_id':'count'}).reset_index()\ndf.columns = ['coupon_id', col]\ndata = pd.merge(data, df, how = 'left', on = 'coupon_id')\n\n#Frquency Counts\nfreq_cols = ['campaign_id']\n# freq_cols = ['campaign_id','coupon_id', 'customer_id']\nfor col in freq_cols:\n    data[col + '_count'] = data[col].map(data[col].value_counts(dropna=False)) \n    \n","f034b5e8":"# Aggregates based on transaction\n\n#Number of coupons per item\ncol = 'item_count_coup'\ndf = coupon.groupby('item_id').agg({'coupon_id':'count'}).reset_index()\ndf.columns = ['item_id', col]\ncust_trans  = pd.merge(cust_trans , df, how = 'left', on = 'item_id')\ncust_trans[col] = cust_trans[col].fillna(0)\n\n## Aggregate Values\nagg_map = {'customer_id': 'count', 'selling_price':'mean' ,'quantity':'mean', 'coupon_discount':'mean', 'other_discount':'mean', 'item_count_coup':'mean'}\n\ndf = cust_trans.groupby('customer_id').agg(agg_map)\n\ndf.columns = ['trans_count','selling_price','quantity','coupon_discount','other_discount', 'item_count_coup']\ndf = df.reset_index()\ndata = pd.merge(data, df, how = 'left', on = 'customer_id')\n# data.head()","6b8254a6":"\n# Aggregate transaction values where coipon code discount is not zero\n\nagg_map = {'customer_id': 'count', 'selling_price':'mean' ,'quantity':'mean', 'coupon_discount':'mean', 'other_discount':'mean', 'item_count_coup':'mean'}\ndf = cust_trans[cust_trans.coupon_discount !=0]\ndf = df.groupby('customer_id').agg(agg_map)\ndf.columns = ['trans_count_c','selling_price_c','quantity_c','coupon_discount_c','other_discount_c', 'item_count_coup_c']\ndf = df.reset_index()\n\ndata = pd.merge(data, df, how = 'left', on = 'customer_id')\n\n# Ratio of transaction with coupons and total transaction by a customer\ncol ='trans_ratio'\ndata[col] =  data['trans_count_c'] \/ data['trans_count'] \n\ncol ='disc_ratio'\ndata[col] =  data['coupon_discount_c'] \/ data['other_discount_c'] \n\n\n# data[data[TARGET] == 1].head()","af7eb611":"## Aggregates on coupon\ncol = 'coup_count_all'\ndf = cust_trans.merge(coupon, on = 'item_id', how = 'inner')\ndf_grp = df.groupby('coupon_id').agg({'coupon_id':'count'})\ndf_grp.columns = [col]\ndf_grp = df_grp.reset_index()\ndf_grp.head()\n# print(df_grp.shape)\n\ncol = 'coup_count_disc'\ndf_disc = df[df.coupon_discount !=0]\ndf_disc_grp = df_disc.groupby('coupon_id').agg({'coupon_id':'count'})\ndf_disc_grp.columns = [col]\ndf_disc_grp = df_disc_grp.reset_index()\ndf_disc_grp.head()\ndf_disc_grp\n\ndf = df_grp.merge(df_disc_grp, how = 'left', on = 'coupon_id' )\ndf[col] = df[col].fillna(0)\n\ncol = 'coup_count_ratio'\ndf[col] = df.coup_count_disc \/ df.coup_count_all\ndata =  data.merge(df, how = 'left', on = 'coupon_id')\n\n# data[data[TARGET] == 1].head()","2740e9e4":"all_cols = data.columns.tolist()\n\n\ncat_cols = ['campaign_type',  'marital_status', 'rented']\ndelete_col = ['campaign_id','id','start_date','end_date', TARGET,]  \ndelete_col += ['trans_count'] + ['selling_price','quantity','coupon_discount','other_discount', 'item_count_coup'] + [ 'coup_count_b','coup_item_count','coup_count_all', 'coup_count_disc']\ndelete_col += ['trans_count_c','selling_price_c','quantity_c','coupon_discount_c','other_discount_c', 'item_count_coup_c']\n\nfeatures = [x for x in all_cols if x not in delete_col]\ncat_cols = [x for x in cat_cols  if x in features]\n\nage_map = {'18-25': 1, '26-35': 2, '36-45': 3, '46-55': 4, '56-70' : 5, '70+':6}\ndata['age_range'] = data['age_range'].map(age_map)\ndata['family_size'] = data['family_size'].replace(to_replace=['5+'],value= 5).astype(float)\ndata['no_of_children'] = data['no_of_children'].replace(to_replace=['3+'],value= 3).astype(float)\n\n    \ndata = data.sort_values(by = 'start_date')\ndata = set_ordinal_encoding(data, cat_cols)\n\nX_train, X_test, y_train, sub = get_train_test(data, features, 'id')\nprint('Train Shape {}, Test Shape {}'.format(X_train.shape, X_test.shape))\n","b528f4b2":"# ### Category Encoding\n# col = 'customer_id_cat'\n# X_train[col] = X_train['customer_id']\n# X_test[col]  = X_test['customer_id']\n\n\n# # en = ce.TargetEncoder(cols = [col], smoothing= 50, min_samples_leaf= 2)\n# en =  ce.CatBoostEncoder(cols = [col])\n# X_train = en.fit_transform(X_train, y_train)\n# X_test =  en.transform(X_test)\n","7aae33ea":"X_train.describe()","2b63b8db":"X_test.describe()","459b20a5":"%%time\nPREDICT_TEST = True\nverbose_eval = 100\n# cat_cols = []\nparams = {}\n\nparams['bagging_fraction'] = 0.8\nparams['bagging_freq'] = 1\nparams['feature_fraction'] = 0.7\n# params['min_gain_to_split'] = 2\n# params['min_sum_hessian_in_leaf'] = 2\nparams['num_leaves'] =  6\nparams['lambda_l1'] = 0.8\nparams['lambda_l2'] = 4\n# params['min_data_in_leaf'] = 80\n\nparams['learning_rate'] = 0.02\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'binary'\nparams['seed'] =  random_state\nparams['metric'] = 'auc'\n# params['scale_pos_weight'] = 5\n\n\n#test size will create validation set based on start_date  = 2013-05-19\nmodel, feature_imp, history =  run_lgb_with_cv(params, X_train, y_train, X_test, cat_cols ,\n                                      shuffle_split = False, test_size= 0.28845589, verbose_eval = verbose_eval, esr = 500)\n\nif PREDICT_TEST:\n    #Run for whole training set to predict on test data\n    print('*' * 100)\n    # num_rounds = int(model.best_iteration + 0.20 * model.best_iteration)\n    num_rounds = int(model.best_iteration + 0.15 * model.best_iteration)\n    model, feature_imp = run_lgb_no_cv(params, X_train, y_train, X_test,cat_cols,\n                                       num_rounds = num_rounds, verbose_eval = 100, )\nplot_results(history)","8de212d0":"print(params)\nif PREDICT_TEST:\n    y_prob_test = model.predict(X_test)  \n    sub = write_predictions(sub, y_prob_test, 'lgb_sub.csv' )\nplot_feature_imp(feature_imp, top_n = 30)\n","5d1ae762":"print(features)","ab4d0619":"# ['coup_count_b', 'coup_item_count','coup_count_all', 'coup_count_disc]","72ddda8c":"## New Features","562ed135":"## Data Preprocessing","9239c9ba":"## Train with Holdout","31841830":"## Merge Data","511d07d3":"## Load data","197cc795":"#### Number of times a customer purchased an item for given coupon before start date","0fc32666":"## Prepare Data for Trianing"}}