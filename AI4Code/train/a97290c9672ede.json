{"cell_type":{"f01cf451":"code","e7cec73f":"code","ab20f674":"code","2486adbc":"code","8693ecdc":"code","5dadd084":"code","1f2b344b":"code","e5f9f67c":"code","d7b7c11a":"code","2bbac3b8":"code","f74524eb":"code","2c880de8":"code","6b36fc8f":"code","11cac01c":"code","76fd9a70":"code","e9617be7":"code","eece0658":"code","3a05bdde":"code","31c190b9":"code","29bef926":"code","e1d843fa":"code","984a2a1d":"code","7cabdbf4":"code","13601e76":"code","e37cf5a2":"code","42961fda":"code","2c05a2bf":"code","a2a08acf":"code","f410562e":"code","21f5e72c":"code","fd7daeac":"code","ce386a5c":"code","7d355dce":"code","a890ff41":"code","aea1307a":"code","ecd0b624":"code","a1ca9e8c":"code","9d96587f":"code","d61182fc":"code","42f51cb6":"code","1156abbe":"code","76142bb4":"code","5a203e68":"code","9ea868b7":"code","8924a8d2":"code","a46ce5e7":"code","7ce46f6c":"code","2f2dd2e7":"code","fce3c9c5":"code","f715a929":"code","e66212b4":"code","ac3106c7":"code","5ab35115":"code","250cb885":"code","e24084a0":"code","4af0a136":"code","ab71a2b4":"code","d8f670cc":"code","02393ade":"code","8f9bcf0f":"code","6ea0218e":"code","83ab1e13":"code","23132d13":"code","749ea699":"code","933bf4fa":"code","894c03a5":"code","c7838b30":"code","13cb6c1d":"code","ba3ad927":"code","c8545325":"code","f4604016":"code","9de609ef":"code","acab272e":"code","6a8872ea":"code","23dec231":"code","3cf9483b":"code","eb530018":"code","ede62295":"code","b338c1b6":"code","20f2a688":"code","ddeb1a91":"code","c235fa83":"markdown","8b1c07ff":"markdown","5b1979f3":"markdown","979f2ef9":"markdown","b11506c9":"markdown","b7eadbda":"markdown","37687b17":"markdown","2b6500f3":"markdown","98235827":"markdown","061977fd":"markdown","45a63169":"markdown","bb922453":"markdown","89a0a591":"markdown","03276a54":"markdown","11e6e363":"markdown","f51c8e2b":"markdown","fe3b8792":"markdown","05a3b233":"markdown","3f85d04c":"markdown","2ee4f0c0":"markdown","079afe44":"markdown","05b9f0bb":"markdown","07c047c1":"markdown","480cbd06":"markdown","f4366c8e":"markdown","d3d35d2f":"markdown","872a1633":"markdown","353fecfb":"markdown","0695ba73":"markdown","33dc3e5d":"markdown","7935c00a":"markdown"},"source":{"f01cf451":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error,r2_score","e7cec73f":"train_data = pd.read_csv(\"..\/input\/flight-price-prediction-2206\/Data_Train.csv\")","ab20f674":"train_data = train_data.drop(\"Unnamed: 0\", axis = 1)","2486adbc":"train_data.head()","8693ecdc":"pd.set_option('display.max_columns', None)","5dadd084":"train_data.info()","1f2b344b":"train_data.isnull().sum()\n","e5f9f67c":"train_data[\"Duration\"].value_counts()","d7b7c11a":"train_data[\"Journey_day\"] = pd.to_datetime(train_data.Date_of_Journey, format=\"%d\/%m\/%Y\").dt.day","2bbac3b8":"train_data[\"Journey_month\"] = pd.to_datetime(train_data[\"Date_of_Journey\"], format = \"%d\/%m\/%Y\").dt.month","f74524eb":"train_data[\"Journey_Year\"] = pd.to_datetime(train_data[\"Date_of_Journey\"], format = \"%d\/%m\/%Y\").dt.year","2c880de8":"# Checking if there are multiple Journey years.\ntrain_data[\"Journey_Year\"].value_counts()","6b36fc8f":"# Since its only one year, i.e. 2019, we can drop this column and also we can now drop the parent column, i.e. **Date_of_Journey**.\ntrain_data.drop(['Date_of_Journey','Journey_Year' ], axis = 1, inplace = True)","11cac01c":"#For Dep_time column.\n# Extracting Hours.\ntrain_data[\"Dep_hour\"] = pd.to_datetime(train_data[\"Dep_Time\"]).dt.hour\n\n# Extracting Minutes.\ntrain_data[\"Dep_min\"] = pd.to_datetime(train_data[\"Dep_Time\"]).dt.minute\n\n# Now we can drop Dep_Time as it is of no use to our model.\ntrain_data.drop([\"Dep_Time\"], axis = 1, inplace = True)","76fd9a70":"#For Arrival time column.\n# Extracting Hours\ntrain_data[\"Arrival_hour\"] = pd.to_datetime(train_data.Arrival_Time).dt.hour\n\n# Extracting Minutes\ntrain_data[\"Arrival_min\"] = pd.to_datetime(train_data.Arrival_Time).dt.minute\n\n# Now we can drop Arrival_Time as it is of no use to our model\ntrain_data.drop([\"Arrival_Time\"], axis = 1, inplace = True)","e9617be7":"duration = list(train_data[\"Duration\"])\n\nfor i in range(len(duration)):\n    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n        if \"h\" in duration[i]:\n            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n        else:\n            duration[i] = \"0h \" + duration[i]           # Adds 0 hour\n\nduration_hours = []\nduration_mins = []\nfor i in range(len(duration)):\n    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))","eece0658":"# Adding duration_hours and duration_mins list to train_data dataframe\n\ntrain_data[\"Duration_hours\"] = duration_hours\ntrain_data[\"Duration_mins\"] = duration_mins","3a05bdde":"# Now we can remove the Duration column.\ntrain_data.drop([\"Duration\"], axis = 1, inplace = True)","31c190b9":"train_data[\"Airline\"].value_counts()","29bef926":"Airline = train_data[[\"Airline\"]]\nAirline = pd.get_dummies(Airline, drop_first= True)\nAirline.head()","e1d843fa":"train_data[\"Source\"].value_counts()","984a2a1d":"Source = train_data[[\"Source\"]]\nSource = pd.get_dummies(Source, drop_first= True)\nSource.head()","7cabdbf4":"train_data[\"Destination\"].value_counts()","13601e76":"Destination = train_data[[\"Destination\"]]\nDestination = pd.get_dummies(Destination, drop_first = True)\nDestination.head()","e37cf5a2":"train_data[\"Route\"]","42961fda":"train_data['Additional_Info'].value_counts()","2c05a2bf":"train_data.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)","a2a08acf":"train_data.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)","f410562e":"# Concatenate dataframe --> train_data + Airline + Source + Destination\nfinal_data = pd.concat([train_data, Airline, Source, Destination], axis = 1)","21f5e72c":"final_data.head()","fd7daeac":"# Now we just need to drop the parent categorical columns and our preprocessing will be complete!\nfinal_data.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)","ce386a5c":"final_data.shape","7d355dce":"final_data.info()","a890ff41":"final_data[final_data['Total_Stops'].isna()]","aea1307a":"final_data = final_data.drop(9039, axis = 0)","ecd0b624":"final_data['Airline_Trujet'].value_counts()","a1ca9e8c":"final_data = final_data.drop('Airline_Trujet', axis = 1)","9d96587f":"final_data.head()","d61182fc":"final_data.info()","42f51cb6":"final_data.shape","1156abbe":"print(final_data.columns)","76142bb4":"final_data.head()","5a203e68":"X = final_data.drop('Price', axis = 1)\ny = final_data['Price']","9ea868b7":"X.head()","8924a8d2":"y.head()","a46ce5e7":"final_data.corr()['Price'].sort_values()","7ce46f6c":"plt.figure(figsize = (20,20))\nsns.heatmap(train_data.corr(), annot = True)\n\nplt.show()","2f2dd2e7":"selection = ExtraTreesRegressor()\nselection.fit(X, y)","fce3c9c5":"print(selection.feature_importances_)","f715a929":"plt.figure(figsize = (25,18))\nfeat_importances = pd.Series(selection.feature_importances_, index=X.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.show()","e66212b4":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n","ac3106c7":"scaler = StandardScaler()\nscaled_X_train = scaler.fit_transform(X_train)\nscaled_X_test = scaler.transform(X_test)","5ab35115":"reg_rf = RandomForestRegressor()\nreg_rf.fit(scaled_X_train, y_train)","250cb885":"y_pred = reg_rf.predict(scaled_X_test)","e24084a0":"reg_rf.score(X_train, y_train)\n","4af0a136":"sns.displot(y_test-y_pred, kde = True)\nplt.show()","ab71a2b4":"\nplt.scatter(y_test, y_pred, alpha = 0.5)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()","d8f670cc":"print('MAE:', mean_absolute_error(y_test, y_pred))\nprint('MSE:', mean_squared_error(y_test, y_pred))\nprint('RMSE:', np.sqrt(mean_squared_error(y_test, y_pred)))","02393ade":"r2_score(y_test, y_pred)","8f9bcf0f":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]","6ea0218e":"# Creating the parameter grid\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}","83ab1e13":"# Grid search of parameters, using 5 fold cross validation, search across 50 different combinations\nfinal_model = RandomizedSearchCV(estimator = reg_rf, \n                               param_distributions = random_grid,\n                               scoring='neg_mean_squared_error', \n                               n_iter = 10, cv = 5, verbose=2,\n                               random_state=42, n_jobs = 1)","23132d13":"final_model.fit(scaled_X_train,y_train)\n","749ea699":"final_model.best_params_\n","933bf4fa":"predictions = final_model.predict(scaled_X_test)","894c03a5":"plt.figure(figsize = (20,18))\nsns.displot(y_test-predictions, kde = True)\nplt.show()","c7838b30":"plt.figure(figsize = (8,8))\nplt.scatter(y_test, predictions, alpha = 0.5)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()","13cb6c1d":"print('MAE:', mean_absolute_error(y_test, predictions))\nprint('MSE:', mean_squared_error(y_test, predictions))\nprint('RMSE:', np.sqrt(mean_squared_error(y_test, predictions)))","ba3ad927":"r2_score(y_test, predictions)","c8545325":"test_data = pd.read_csv('..\/input\/flight-price-prediction-2206\/Test_set.csv')","f4604016":"test_data.drop('Unnamed: 0', axis = 1, inplace = True)","9de609ef":"test_data.head()","acab272e":"# Preprocessing\n\nprint(\"Test data Info\")\nprint(\"-\"*75)\nprint(test_data.info())\n\nprint('\\n\\n')\n\nprint(\"Null values :\")\nprint(\"-\"*75)\ntest_data.dropna(inplace = True)\nprint(test_data.isnull().sum())\n\n# EDA\n\n# Date_of_Journey\ntest_data[\"Journey_day\"] = pd.to_datetime(test_data.Date_of_Journey, format=\"%d\/%m\/%Y\").dt.day\ntest_data[\"Journey_month\"] = pd.to_datetime(test_data[\"Date_of_Journey\"], format = \"%d\/%m\/%Y\").dt.month\ntest_data.drop([\"Date_of_Journey\"], axis = 1, inplace = True)\n\n# Dep_Time\ntest_data[\"Dep_hour\"] = pd.to_datetime(test_data[\"Dep_Time\"]).dt.hour\ntest_data[\"Dep_min\"] = pd.to_datetime(test_data[\"Dep_Time\"]).dt.minute\ntest_data.drop([\"Dep_Time\"], axis = 1, inplace = True)\n\n# Arrival_Time\ntest_data[\"Arrival_hour\"] = pd.to_datetime(test_data.Arrival_Time).dt.hour\ntest_data[\"Arrival_min\"] = pd.to_datetime(test_data.Arrival_Time).dt.minute\ntest_data.drop([\"Arrival_Time\"], axis = 1, inplace = True)\n\n# Duration\nduration = list(test_data[\"Duration\"])\n\nfor i in range(len(duration)):\n    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n        if \"h\" in duration[i]:\n            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n        else:\n            duration[i] = \"0h \" + duration[i]           # Adds 0 hour\n\nduration_hours = []\nduration_mins = []\nfor i in range(len(duration)):\n    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))   # Extracts only minutes from duration\n\n# Adding Duration column to test set\ntest_data[\"Duration_hours\"] = duration_hours\ntest_data[\"Duration_mins\"] = duration_mins\ntest_data.drop([\"Duration\"], axis = 1, inplace = True)\n\n\n# Categorical data\n\nprint(\"Airline\")\nprint(\"-\"*75)\nprint(test_data[\"Airline\"].value_counts())\nAirline = pd.get_dummies(test_data[[\"Airline\"]], drop_first= True)\n\nprint()\n\nprint(\"Source\")\nprint(\"-\"*75)\nprint(test_data[\"Source\"].value_counts())\nSource = pd.get_dummies(test_data[[\"Source\"]], drop_first= True)\n\nprint()\n\nprint(\"Destination\")\nprint(\"-\"*75)\nprint(test_data[\"Destination\"].value_counts())\nDestination = pd.get_dummies(test_data[[\"Destination\"]], drop_first = True)\n\n# Additional_Info contains almost 80% no_info\n# Route and Total_Stops are related to each other\ntest_data.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)\n\n# Replacing Total_Stops\ntest_data.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)\n\n# Concatenate dataframe --> test_data + Airline + Source + Destination\nfinal_test_data = pd.concat([test_data, Airline, Source, Destination], axis = 1)\n\nfinal_test_data.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)\n\nprint()\nprint()\n\nprint(\"Shape of test data : \", final_test_data.shape)","6a8872ea":"final_test_data.head()","23dec231":"final_test_data.info()","3cf9483b":"scaled_test_data = scaler.transform(final_test_data)","eb530018":"price = final_model.predict(scaled_test_data)","ede62295":"price","b338c1b6":"price = pd.DataFrame(price, columns=['Predicted Price'])","20f2a688":"price","ddeb1a91":"price.to_csv('Predicted_Prices.csv')","c235fa83":"# **Feature Selection**\n\nWe need to find the features which affects our target feature the most","8b1c07ff":"# **PROJECT 2: Flight Price Prediction Using Supervised ML**\n\n**SUBMITTED BY: KANAV JAIN**\n\n**Dataset Used: [*Dataset*](https:\/\/drive.google.com\/drive\/folders\/1lPoRgB7CmvT6lPs--0YTVy2tPLTANmly?usp=sharing)**","5b1979f3":"**4. Saving this DataFrame as a csv file!**","979f2ef9":"**Making Predictions**","b11506c9":"# **EXPLORATORY DATA ANALYSIS**","b7eadbda":"**Test train split**","37687b17":"**Scaling the data**","2b6500f3":"Now, we need to perform the same steps for **Dep_Time**, **Arrival_Time** to extract values we need for our model.","98235827":"A plot of feature importance in prediction of the flight price.","061977fd":"As we can see ***Total_Stops*** affects the price mostly. \nSo ***Total_Stops*** is the most important feature here.\nNow we can visualize this correlation with the help of a *heatmap*.","45a63169":"For the **Duration_time** column,  we can create two lists, *Hours* and *Minutes* and then add them to our dataframe!","bb922453":"**Fitting the model on the Randomized Search CV instance.**","89a0a591":"# **TEST SET**\n\nNow we need to import our test dataset and perform all the preprocessing steps again on it!\nBasically this dataset is used for making predictions on a certain amount of data and it will be saved in the output","03276a54":"We can see that ***Date_of_Journey*** is a object data type and in order to use this in our model for prediction we need to convert it into ***timestamp***!\n\nWe can do this easily with the help of ***pandas.to_datetime*** method.\n\nWe will add two columns here:\n1. Journey_day -> Day of Journey\n2. Journey_month -> Month of Journey","11e6e363":"**Clearly, This model is not performing that good, so we need to perform hyperparameter tuning on this**\n\n**We will do this with the help of Randomized Search CV**","f51c8e2b":"**r2_score has also improved to 81.2% value!**","fe3b8792":"**1. Scale this new data with already fitted scaler**","05a3b233":"**Clearly this is a much better model than the previous one!**","3f85d04c":"Now we can see that **Route** and **Total_Stops** are related to each other,  we can drop route.\nAlso,  **Additional_Info** contains mostly *'no info'*, so we can drop it too.","2ee4f0c0":"# **Importing Required Libraries**","079afe44":"**So I think we can use this model for our prediction puposes and hence our Flight Price Prediction model is completed**","05b9f0bb":"# **Using Random Forests to fit an initial vague model**","07c047c1":"# **Handling Categorical Data**\nNow that we have handled the time and data data, we can shift our focus to preprocessing the categorical data in our dataframe in order to make it fit for training our model\n\nWe can do this easily with the help of ***OneHotEncoder*** and ***LabelEncoder***\n\nWe will perform our operations on various columns by creating new dataframes and in the end we will concatenate all the dataframes!","480cbd06":"# **THANK YOU!**","f4366c8e":"Since this flight has just been used one time we can drop this column.","d3d35d2f":"# **Importing Train Data**","872a1633":"# **SHAPE AI DATA ANALYST 25TH MAY BATCH**","353fecfb":"**3. Converting it to a pandas DataFrame**","0695ba73":"**Declaring the model and fitting the train data on it**","33dc3e5d":"CLEARLY, NO NULL DATA IS THERE!\n\nLETS CONFIRM THIS BELOW","7935c00a":"**2. Make predictions using already existing model**"}}