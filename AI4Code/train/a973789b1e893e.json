{"cell_type":{"489a745b":"code","2154bc8e":"code","0a3b6a4e":"code","b1a651d9":"code","beb27402":"code","84d5f3d2":"code","c7f0cf18":"code","4e6f5162":"code","97ed2377":"code","fde164c1":"code","b36896c7":"code","23c04f3e":"code","4ca45bf0":"code","d9fdb80a":"code","6e44ef1a":"code","a4726794":"code","d36435f6":"code","77bd173d":"code","19fb1c87":"code","1c177fce":"code","d79c3044":"code","38063f0a":"code","07392a19":"code","d2aa7042":"markdown","59eb3f9b":"markdown","ebb1964c":"markdown","d8f4c7de":"markdown","bdab3a12":"markdown","130b1a4f":"markdown","b520b2d6":"markdown","58a21bec":"markdown","666799b5":"markdown","30847c7f":"markdown","6df4c995":"markdown","e8c6ef95":"markdown","b7146a53":"markdown","4d54fb74":"markdown","06f70763":"markdown","b31879ff":"markdown","6b3aedee":"markdown","62911866":"markdown","6b943576":"markdown","38a84e47":"markdown","25830d02":"markdown","60e97076":"markdown","110e489e":"markdown","c50618d9":"markdown","7c14ad5c":"markdown","d1a17022":"markdown","a1711068":"markdown"},"source":{"489a745b":"# Import libraries\nfrom subprocess import call\n\nimport patsy\nimport folium as folium\nfrom folium import plugins\nfrom folium.plugins import HeatMap\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom IPython.core.display import Image\nfrom matplotlib import pyplot\nfrom numpy import isnan\nfrom patsy.highlevel import dmatrices, dmatrix\nfrom sklearn import model_selection\nfrom sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, chi2\nfrom sklearn.impute import KNNImputer\nfrom sklearn.impute._iterative import IterativeImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz, DecisionTreeRegressor, plot_tree\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nfrom scipy.stats import chi2_contingency\nfrom scipy.stats import chi2\nfrom mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\nfrom sklearn.model_selection import GridSearchCV\nfrom pandas.plotting import scatter_matrix\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_columns', None)\npd.set_option('display.expand_frame_repr', False)\npd.set_option('max_colwidth', -1)","2154bc8e":"# Import data\ndf_prices = pd.read_csv('..\/input\/california-housing-prices\/housing.csv', delimiter = \",\")\ndf_orig = df_prices.copy()\ndf = df_prices.copy()\n","0a3b6a4e":"df_prices.head(20)\nprint(df_prices.shape)","b1a651d9":"# Check for null values\nmissing_values_train = df_prices.isnull().sum()\nmissing_values_train = missing_values_train.to_frame(name='num_missing')\nmissing_values_train['perc_missing'] = (missing_values_train['num_missing']\/df_prices.shape[0])*100\nfor index, row in missing_values_train.iterrows():\n    if (row['num_missing'] > 0):\n        print (\"For \\\"%s\\\" the number of missing values are: %d (%.0f%%)\" %  (index,\n                                                                     row['num_missing'],\n                                                                    row['perc_missing']))","beb27402":"df_prices[df_prices.isnull().any(axis=1)]","84d5f3d2":"#%matplotlib inline\n_ = plt.figure(figsize=(20, 10))\n\n# cubehelix palette is a part of seaborn that produces a colormap\ncmap = sns.cubehelix_palette(light=1, as_cmap=True, reverse=True)\n_ = sns.heatmap(df_prices.isnull(), cmap=cmap)","c7f0cf18":"# Continuous density plot\nfig_missing, axes = plt.subplots(1, 1, figsize=(10, 8))\n\n# Plot frequency plot\/ histogram\n_ = sns.histplot(x=\"median_house_value\", kde=True, data=df_prices, ax=axes, bins=40);\n_ = axes.set(xlabel=\"House Value\", ylabel='Density');\naxes.xaxis.label.set_size(18)\naxes.yaxis.label.set_size(18)\naxes.tick_params('y', labelsize = 14);\naxes.tick_params('x', labelsize = 14);","4e6f5162":"print(\"House value summary statistics:\\n\")\nprint(df_prices['median_house_value'].describe())","97ed2377":"# Replace missing values\ndf_train = df_prices.copy()\nmedian = df_train['total_bedrooms'].median()\ndf_train['total_bedrooms'].fillna(median, inplace=True)\nprint(\"Number of null values in total bedrooms column: {}\".format(df_train['total_bedrooms'].isnull().sum()))","fde164c1":"print(df_train.nunique())","b36896c7":"all_vars = list(df_prices.columns.values)\nprint(all_vars)","23c04f3e":"# Separate continuous and categorical variables\nnames_con = ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population',\n             'households', 'median_income', 'median_house_value']\nnames_cat = ['ocean_proximity']\n\ndf_train_con = df_train.loc[:, names_con]\ndf_train_cat = df_train.loc[:, names_cat]","4ca45bf0":"_ = df_prices.hist(bins=50, figsize=(20,15))\n_ = plt.savefig(\"attribute_histogram_plots\")\nplt.show()","d9fdb80a":"proximity_counts = df_train_cat['ocean_proximity'].value_counts(normalize = True)\nlegend_labels = ['<1H Ocean', 'Inland', 'Near Ocean', 'Near Bay', 'Island']\n\n# change the background bar colors to be light grey\nbars = plt.bar(proximity_counts.index, proximity_counts.values, align='center', linewidth=0,\n               color='lightslategrey')\n# make one bar, the <1H Ocean bar, a contrasting color\nbars[0].set_color('#1F77B4')\n\n# soften all labels by turning grey\n_ = plt.xticks(proximity_counts.index, legend_labels, fontsize=11, alpha=0.8)\n_ = plt.title('Ocean Proximity', fontsize=14, pad=30, alpha=0.8)\n\n# remove all the ticks (both axes), and tick labels on the Y axis\nplt.tick_params(top=False, bottom=False, left=False, right=False, labelleft=False,\n                labelbottom=True)\n\n# Remove the frame - my method\nax = plt.gca()\nax.set_frame_on(False)","6e44ef1a":"# Class level counts for categorical variables.\nfor variable in names_cat:\n    print(df_prices[variable].value_counts())\n    print(\"\\n\")","a4726794":"fig = plt.figure();\n_ = scatter_matrix(df, figsize = (25,25), alpha=0.9, diagonal=\"kde\", marker=\"o\");","d36435f6":"california_map = folium.Map(location=[36.7783,-119.4179], zoom_start = 6, min_zoom=5)\ndf_map = df_prices[['latitude', 'longitude']]\ndata = [[row['latitude'],row['longitude']] for index, row in df_map.iterrows()]\n_ = HeatMap(data, radius=10).add_to(california_map)\ncalifornia_map","77bd173d":"X = df_prices.loc[:,[\"longitude\",\"latitude\"]]\ny = df_prices.loc[:,\"median_house_value\"]","19fb1c87":"regressor = DecisionTreeRegressor(random_state=0)\nregressor = regressor.fit(X,y)","1c177fce":"# Visualising the decision tree regression results\n_ = plt.figure(figsize=(10,10), dpi=150)\n_ = plot_tree(regressor,max_depth=2,feature_names=X.columns, impurity=False, filled=True)\n_ = plt.show()","d79c3044":"# A scatter plot of latitude vs longitude\n_ = df.plot.scatter(x='longitude',y='latitude',c='DarkBlue',s=1.5)","38063f0a":"_ = plt.figure(figsize=[6,4], dpi=120)\n_ = plt.ylim(32,42)\n_ = plt.xlim(-125,-114)\n_ = plt.ylabel(\"Latitude\")\n_ = plt.xlabel(\"Longitude\")\n_ = plt.title(\"Median House Value\")\n# A scatter plot of latitude vs longitude\n_ = plt.scatter(x=X[\"longitude\"],y=X[\"latitude\"],c=df[\"median_house_value\"], s=1.5)\n# Show first two decision points\nsplits = regressor.tree_.threshold[:2]\n_ = plt.plot([-125,-114],[splits[0],splits[0]])\n_ = plt.plot([splits[1],splits[1]],[32,splits[0]])\n_ = plt.colorbar()\n_ = plt.show()","07392a19":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nregressor_acc = regressor.fit(X_train, y_train)\ny_pred = regressor_acc.predict(X_test)\nkfold = model_selection.KFold(n_splits=10, random_state=7, shuffle=True)\n\n# MSE - take absolute value first\nscoring = 'neg_mean_squared_error'\nresults = model_selection.cross_val_score(regressor_acc, X, y, cv=kfold, scoring=scoring)\nprint(\"MSE: %.3f (%.3f)\" % (results.mean(), results.std()))\n\n# R-Squared\nscoring = 'r2'\nresults = model_selection.cross_val_score(regressor_acc, X, y, cv=kfold, scoring=scoring)\nprint(\"R^2: %.3f (%.3f)\" % (results.mean(), results.std()))\n\n# Mean Absolute Error\nscoring = 'neg_mean_absolute_error'\nresults = model_selection.cross_val_score(regressor_acc, X, y, cv=kfold, scoring=scoring)\nprint(\"MAE: %.3f (%.3f)\" % (results.mean(), results.std()))","d2aa7042":"### High level overview","59eb3f9b":"For this analysis we use the <b>California Housing<\/b> dataset found on the Kaggle Data\nRepository at the following location:\n\n<a href=https:\/\/www.kaggle.com\/camnugent\/california-housing-prices>California Housing Prices<\/a>\n\nThe objective of the analysis is to predict median house prices in different regions of California as per the 1990\ncensus data.\n\nWe hope to gain valuable insights by following this process. The various steps in the\nprocess can be elaborated on as follows:\n\n- Load data\n- Prepare data\n    - Clean data\n        - Missing values\n        - Outliers\n        - Erroneous values\n    - Explore data\n        - Exploratory descriptive analysis (EDA)\n        - Correlation analysis\n        - Variable cluster analysis\n    - Transform Data\n        - Engineer features\n        - Encode data\n        - Scale & normalise data\n        - Impute data (if not done in previous steps)\n        - Feature selection\/ importance analysis\n- Build model\n    - Model selection\n    - Data sampling (validation strategy, imbalanced classification)\n    - Hyperparameter optimisation\n- Validate model\n    - Accuracy testing\n- Analysis of results\n    - Response curves\n    - Accuracy analysis\n    - Commentary\n\nThis dataset is used in the second chapter of Aur\u00e9lien G\u00e9ron's recent book 'Hands-On Machine learning with\nScikit-Learn and TensorFlow' - highly recommended!\n\nThe data dictionary for this dataset is as follows:\n\n| Variable | Definition | Type |\n|----------|------------|-----|\n| longitude| Longitude\t| Continuous |\n| latitude | Latitude   | Continuous |\n| housingmedianage | Median age of houses in area | Continuous |\n| total_rooms | Avg No. of rooms of houses in area | Continuous |\n| total_bedrooms | Avg No. of bedrooms of houses in area | Continuous |\n| population | No. of people in area | Continuous |\n| households | No. of households in area | Continuous |\n| median_income | Median income of household | Continuous |\n| medianhousevalue | Median value of houses in area | Continuous |\n| ocean_proximity | Proximity to ocean\t| Categorical |\n\nLet us start the exploration!","ebb1964c":"Now let us try to find the areas where prices are highest by using a Decision Tree.","d8f4c7de":"We will start by analysing the distributions for median house value first to get a feel for the data.","bdab3a12":"To conclude this analysis, let's see just how predictive \"location\" is in terms of house price.\n\nWe will build a simple Decision Tree model based on location only and do an accuracy test.","130b1a4f":"Consider a sample of missing values from the data set:","b520b2d6":"#### Continuous variable overview\nMost of the variables are continuous","58a21bec":"There is a skew to the right with a huge spike at high end of prices - this is likely to be a data issue. We will\nhave to investigate.\nMaximum price was 500 001 and minimum age was around 14 999 dollars.","666799b5":"Let's have a look at a heatmap of prices.","30847c7f":"We observed 207 (1%) null values for <i>total_bedrooms<\/i>.\n\nThe visualised missing values look as follows for the training set:","6df4c995":"We will peform our analysis as follows:\n\n- Missing data checks.\n- Model accuracy check.\n- EDA (without missing data replacement).\n- Variable adjustments & missing value imputation.\n- Model accuracy check","e8c6ef95":"<div class=\"alert alert-block alert-info\">\n<b>Missing values<\/b>\n<\/div>","b7146a53":"#### Entire set features","4d54fb74":"Let us do a simple scatter plot of longitude vs latitude to see shape of price distribution.","06f70763":"<div class=\"alert alert-block alert-info\">\n<b>Exploration of data<\/b>\n<\/div>","b31879ff":"We start by looking at the number of unique records per variable.","6b3aedee":"There are no columns with only one value. We therefore retain all columns for ML purposes as there is\nenough variability to warrant using the data.\n\nWe start by separating continuous and categorical variables for further high level analysis.","62911866":"Let's continue with the high level analysis.","6b943576":"We observe that the null values have been removed. We now have a dataset ready for further analysis -\nalbeit a bit of a black box hack :) We will now do some very limited EDA just to get a feel for the data\nas previously discussed.","38a84e47":"<div class=\"alert alert-block alert-info\">\n<b>Load data<\/b>\n<\/div>","25830d02":"Based on this simple EDA and model accuracy results we can confirm that location is a very strong predictor of house\nprices indeed. An R-squared of 73% is a very strong indication of strong correlation between location and house price\n. This is however not a surprise as everyone know that it is all about: \"location, location, location\"!\n\nIn our next notebook we will do some more EDA and investigate correlation patterns with other variables. We will then\n build some predictive models using Pipelines. Cannot wait!\n\n","60e97076":"# I. California Housing Exploratory Data Analysis\n### Prediction of median house prices for California districts derived from the 1990 census.","110e489e":"Let us now annotate this plot with the first two splits in the Decision Tree.\n","c50618d9":"We now have a look at correlation in the dataset. We use Pearson's Correlation as most of the variables are continuous.","7c14ad5c":"Class percentages:","d1a17022":"We quantify the number of missing values in the data set:","a1711068":"#### Categorical variable overview"}}