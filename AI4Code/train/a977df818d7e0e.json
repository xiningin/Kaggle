{"cell_type":{"19b6ca06":"code","a066232a":"code","d9d93424":"code","c67fc894":"code","cba059f1":"code","0ff81194":"code","6b35503a":"code","70f867cf":"code","fc6990d5":"code","71fd919c":"code","de60d486":"code","f234ada1":"code","e6ae27ed":"code","f2087dab":"code","1107623e":"code","465144a2":"code","c4c4408e":"code","41c11dd0":"code","d916ecf8":"code","c8e137ce":"code","146a5fff":"code","7da50560":"code","ac5012d9":"code","96a5403a":"code","63c6d55c":"code","ac7a7460":"markdown","df479ddd":"markdown","7ae2a4f7":"markdown","30aa15a8":"markdown","35e8b81b":"markdown","3c9446eb":"markdown","1cf87dd3":"markdown","1f9eb36f":"markdown","14f8dc16":"markdown","a210d85f":"markdown","5c22a10c":"markdown"},"source":{"19b6ca06":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport matplotlib.pyplot as plt\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import tree\nfrom sklearn import ensemble\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import linear_model\nimport re\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n#Read all the csv's\ndf_train=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf_example=pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\ndf_test=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","a066232a":"df_train.head()","d9d93424":"##Check if we got any nulls up in here.\n##Cabin and Age have some nulls - we'll need to impute them.\ndf_train.isnull().sum()","c67fc894":"print(df_train['Survived'].value_counts())\nprint(df_train['Survived'].value_counts()\/df_train['Survived'].count())","cba059f1":"#Ticket Class\nprint(df_train['Pclass'].value_counts())\nprint(df_train[['Pclass','Survived']].value_counts().sort_index())","0ff81194":"#Age - Significant bunching around 20-30\ndf_train['Age'].plot.hist()","6b35503a":"# Sex - More Male than female. Male have 81.1% death rate. Female have 25.7% death rate.\nprint(df_train[['Sex']].value_counts())\nprint(df_train[['Sex','Survived']].value_counts())\ndf_train[['Sex','Survived']].value_counts()","70f867cf":"# Sibsp - Number of Siblings or Spouses aboard - Large Proportion didn't have any.\nprint(df_train['SibSp'].value_counts())\ndf_train['SibSp'].plot.hist()","fc6990d5":"#Parch - Number of Parents and children onboard. Alot of people didn't have any\nprint(df_train['Parch'].value_counts())\ndf_train['Parch'].plot.hist()","71fd919c":"#Ticket - Ticket Number. Looks like it's all Unique. However, there seems to be prefixes to some of the Ticket Numbers. \n# We can probably introduce two new features - one if the Ticket Number has a letter and another based on the length of the ticket number.\ndf_train['Ticket'].unique()\n# df_train['Ticket'].str.len()\n# df_train['Ticket'].str.isnumeric().astype(int)","de60d486":"#Fare. Fare price passenger paid. Big proportion of those below 20 bucks.\nprint(df_train['Fare'].value_counts().sort_index())\ndf_train['Fare'].plot.hist(bins=20)","f234ada1":"#Cabin. Cabin Number. This one had heaps of NaN's - i'm assuming those without cabins. Also alot of unique values. \n#There are also multi cabins. There's a bunch of way's we can split this: by alphabet, NaN vs. not NaN, by number, multi-cabins...\ndf_train['Cabin'].unique()\n# df_train['Cabin'].isna().astype(int)","e6ae27ed":"# Embarked - Point of Embarkation for the passenger. Heavily weighted towards Southampth. C = Cherbourg, Q = Queenstown, S = Southampton\ndf_train['Embarked'].value_counts()","f2087dab":"#returns the Title within the name columns\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"","1107623e":"def splitage(age):\n    if age<15:\n        result='<15'\n    elif age>=15:\n        result='>=15'\n    else:\n        result='NULL'\n    \n    return result","465144a2":"def leftcabin(s):\n    if s!=s:\n        return 'NULL'\n    return s[:1]\n    ","c4c4408e":"#This Function accepts the dataframe in the form of test\/train.\ndef preprocess_X(df,trainortest):\n    #Grab all the string categorical values and one hot encode them.\n    #Gotta One-hot encode before putting into the Tree Algo.\n    from sklearn.preprocessing import OrdinalEncoder\n    \n    #Ticket Feature Creation\n    df['Ticket_str_length']=df['Ticket'].str.len()\n    df['Ticket_is_numeric']=df['Ticket'].str.isnumeric().astype(int)\n    #Cabin Number Feature Creation\n    df['Cabin_isna']=df['Cabin'].isna().astype(int)\n    df['Cabin_letter']=df['Cabin'].apply(leftcabin)   \n    #Age Feature Creation\n    df['agebin']=df['Age'].apply(splitage)\n\n    df['Title']=df['Name'].apply(get_title)\n    df['Title'] = df['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    df['Title'] = df['Title'].replace('Mlle', 'Miss')\n    df['Title'] = df['Title'].replace('Ms', 'Miss')\n    df['Title'] = df['Title'].replace('Mme', 'Mrs')\n    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n    df['IsAlone'] = 0\n    df.loc[df['FamilySize'] == 1, 'IsAlone'] = 1\n    \n\n    print(df['Title'].unique())\n    col_string_list=['Sex', 'Embarked','Title','agebin','Cabin_letter']\n    X2=df[col_string_list]\n    X2=X2.where(pd.notnull(X2), 'None')\n    encoder = OrdinalEncoder()\n    encoder.fit(X2)\n    X_encoded = encoder.transform(X2)\n    df_encoded=pd.DataFrame(X_encoded,columns=col_string_list)\n    \n    #All the numeric stuff\n    col_numeric_list=['Pclass', 'Age', 'SibSp',\n        'Parch','FamilySize','IsAlone','Fare','Ticket_str_length','Ticket_is_numeric','Cabin_isna']\n    if trainortest=='Train':\n        col_numeric_list.append('Survived')      \n    df_numeric=df[col_numeric_list]\n    #Concatenate in the columns\n#     print(df_numeric.head())\n#     print(df_encoded.head())\n    df_result=pd.concat([df_numeric,df_encoded],axis=1)\n    df_result = df_result.fillna(-1)\n    \n\n    return df_result","41c11dd0":"# We should get cleaned data ready to place into our tree models\ndf_train_post=preprocess_X(df_train,'Train')\ndf_test_post=preprocess_X(df_test,'Test')\ndf_train_post","d916ecf8":"#Let's plot a Correlation Matrix.\ndef plot_corr(df,size=10):\n    import seaborn as sns\n    '''Function plots a graphical correlation matrix for each pair of columns in the dataframe.\n\n    Input:\n        df: pandas DataFrame\n        size: vertical and horizontal size of the plot'''\n\n    colormap = plt.cm.viridis\n    plt.figure(figsize=(12,12))\n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n    sns.heatmap(df.corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)","c8e137ce":"plot_corr(df_train_post)","146a5fff":"#Let's remove highly correlated metrics and leave one's with high magnitude\n#This will help with model training faster and model interpretability.\nmain_features=['Pclass','IsAlone','Fare','Cabin_isna','Sex','Embarked','agebin','Title']\nX=df_train_post.drop(['Survived'], axis=1) #[main_features]\nXtest=df_test_post #[main_features]\nY=df_train_post['Survived']","7da50560":"#Cross Validating our Decision Tree\nfrom sklearn.model_selection import cross_val_score\ndepth = []\nfor i in range(1,20):\n    clf = tree.DecisionTreeClassifier(max_depth=i)\n    # Perform 7-fold cross validation \n    scores = cross_val_score(estimator=clf, X=X, y=Y, cv=10, n_jobs=4)\n    depth.append((i,scores.mean()))\nprint(depth)","ac5012d9":"#Let's train our Decision Tree\nclf = tree.DecisionTreeClassifier(max_depth=4)\n#Seems to work with a dataframe\nclf=clf.fit(X,Y)\nplt.figure(figsize=(25,15))\ntree.plot_tree(clf, feature_names=X.columns,class_names=True,filled=True,fontsize=12)","96a5403a":"#Let's Predict \nsubmission_arr=clf.predict(df_test_post)\ndf_survived_output=pd.DataFrame(submission_arr, columns=['Survived'])\ndf_submission=pd.concat([df_test['PassengerId'],df_survived_output['Survived']], axis=1)\ndf_submission.to_csv('submissionDecisionTree.csv',index=False)\ndf_submission","63c6d55c":"from sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import BaggingClassifier\n\n#Using Grid Search and Cross Validation in our Random Forest to get best hyperparameters\n\n#The step in the pipeline doesn't matter because we'll replace them in the param_grid\npipe = Pipeline([ ('classifier', ensemble.ExtraTreesClassifier())])\n\n\nparam_grid = [\n    {'classifier': [ensemble.RandomForestClassifier()],\"classifier__criterion\" : [\"gini\"], \"classifier__min_samples_leaf\" : [1, 2, 3], \n               \"classifier__min_samples_split\" : [2,3], \n               \"classifier__n_estimators\": [30,50,60,80], 'classifier__max_depth': [1,2,3,4,5,6]},\n    {'classifier':[ensemble.BaggingClassifier()],\"classifier__n_estimators\": [20,50,100],\n               'classifier__bootstrap' : [True, False],'classifier__bootstrap_features' : [True, False]},\n    {'classifier': [linear_model.LogisticRegression()], 'classifier__penalty' : ['l1', 'l2'], 'classifier__C' : np.logspace(-4, 4, 20), 'classifier__solver' : ['liblinear']}\n              ]\n\ngs = GridSearchCV(estimator=pipe, param_grid=param_grid, cv=7)\n\ngs = gs.fit(X, Y)\n\n\nprint(gs.best_score_)\nprint(gs.best_estimator_.get_params()['classifier'])\n\n#Let's train our random forest using the best performing hyperparameters function\nclf = gs.best_estimator_.get_params()['classifier']\n#Seems to work with a dataframe\nclf=clf.fit(X,Y)\n\n#Let's Predict using our random forest\nsubmission_arr=clf.predict(Xtest)\ndf_survived_output=pd.DataFrame(submission_arr, columns=['Survived'])\ndf_submission=pd.concat([df_test['PassengerId'],df_survived_output['Survived']], axis=1)\ndf_submission.to_csv('submissionOutput.csv',index=False)\ndf_submission","ac7a7460":"## Preprocessing time\nWe'll need to turn all string categories into numeric values to make them inputtable into our Models for training and predicting.","df479ddd":"### Predictor - Survived (What we're trying to predict in the test set)\nAround 61.6% of all people in this dataset died in the Titanic. This means if you just did guessed all 0's - you might have a chance of getting a 60-ish% prediction rate - meaning that the dataset is biased towards those who died.","7ae2a4f7":"### Feature - Sibsp, Parch, Ticket, Fare, Cabin, Embarked","30aa15a8":"### Feature - Sex: \nMore Male than female onboard Titanic. Male have 81.1% death rate. Female have 25.7% death rate. Like - PClass - likely to be a strong predictor","35e8b81b":"## Decision Tree Algorithm\n[See more](https:\/\/scikit-learn.org\/stable\/modules\/tree.html#tree).\nIf you take a look at the plot. You will see the strong predictors of Survival (Sex,PClass,Cabin_isna) are splitting the tree near the top (Gini Indexes of around 0.5).","3c9446eb":"### Feature - Age:\nSignificant bunching around 20-30. We can find out more about how it correlates with Survival in a correlation matrix we will do after preprocessing.\nCheckout the [Tableau](https:\/\/public.tableau.com\/views\/TitanicEDA-Kaggle\/Age-2?:language=en-GB&:display_count=y&publish=yes&:origin=viz_share_link) for better EDA.\nIt suggests we should bucket into three groups: Pre-15,Post-15,and NULLS'","1cf87dd3":"## Correlation of Features\nLet's check out the Correlation of Features and see if there's any strong correlations between Survival and it's Features.\nPClass @ -0.34 , IsAlone @ -0.2 , Fare @ 0.26, Cabin_isna @ -0.32, Sex @ -0.54 \n","1f9eb36f":"### Feature - Ticket Class\nHow many people had 1st, 2nd or 3rd Class tickets? How does that influence survival rate? \n* 1st Class: 136\/(80+136) = 63%\n* 2nd Class 87\/(97+87)= 47%\n* 3rd Class 119\/(119+372) = 24%\n\nJudging from these stats - looks like it does have a big impact - the better the class the better the survival rate.","14f8dc16":"# Titanic Modelling Tutorial:\n**Workflow:**\nLoad -> EDA (Exploratory Data Analysis) -> Preprocessing\/Feature Engineering -> Model Training -> Model Fitting -> Output\n\n**Models Used:** Logistic Regression, Decision Trees, Random Forest, Gradient Boosted Trees\n\nNote: [Tableau](https:\/\/public.tableau.com\/views\/TitanicEDA-Kaggle\/Age-2?:language=en-GB&:display_count=y&publish=yes&:origin=viz_share_link) is such a good tool for EDA! Please use it for small datasets - super quick to whip up charts.","a210d85f":"## Let's Search the entire space of all models and combinations of hyperparameters\nWe'll use GridSearchCV to search the entire space of a couple of models and their hyperparameters to come up with the best model.\nIn my case I found the Random Forest Classifier to do the best - however not by much.\nDecision Trees got near my max score of 0.78229. \nIn the real world I'd use Decision Trees purely because they are much more easily explainable.","5c22a10c":"## Let's do some Exploratory Data Analysis (EDA)\nMost of the work is understanding the data to get features that can be used as indicators for prediction,"}}