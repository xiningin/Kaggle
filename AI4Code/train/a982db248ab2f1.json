{"cell_type":{"a95617e8":"code","0e432b22":"code","a60ebf3c":"code","175c8490":"code","550e545a":"code","0978b996":"code","ecc3291d":"code","506c85f6":"code","1fa2363a":"code","a76ffde6":"code","3c5fc67f":"code","bc5990dd":"code","3fd423c3":"code","a41935f4":"code","df16273f":"code","46b451f3":"code","5d25dfb1":"code","16460a98":"code","11acd8df":"code","254184ad":"code","a6721e23":"code","fb61898c":"code","100741bf":"code","7686d42b":"code","aefb7f17":"code","a1785e04":"code","5daa4307":"code","3d4b6d71":"code","ef8696ec":"code","2ae20205":"markdown","c65aa231":"markdown","ea3fdf59":"markdown","e3f0bfec":"markdown","13aeea99":"markdown","6a5e0634":"markdown","fade8890":"markdown","ebff2658":"markdown","606fd790":"markdown","27114ebb":"markdown","0f546657":"markdown","e029d6f9":"markdown","1ce63068":"markdown","8f2d8705":"markdown","25477133":"markdown","9ec0544a":"markdown","101f3ab7":"markdown","b813ecc8":"markdown","8217812c":"markdown","5b9c87b7":"markdown","cc6c4de4":"markdown","7f228cf0":"markdown","a3cc49ef":"markdown","20651144":"markdown","d9612fe8":"markdown","c32ca7f8":"markdown","8e0f27ad":"markdown","85ed087c":"markdown","28f78dcd":"markdown","9988aa61":"markdown"},"source":{"a95617e8":"#importing libraries \nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt","0e432b22":"# Importing Data\ndata = pd.read_csv( '..\/input\/dp-lr-model-train-cleaned-datset\/train_cleaned.csv')\ndata.head()","a60ebf3c":"#seperating independent and dependent variables\nx = data.drop(['Item_Outlet_Sales'], axis=1)\ny = data['Item_Outlet_Sales']\nx.shape, y.shape","175c8490":"# Importing the train test split function\nfrom sklearn.model_selection import train_test_split\ntrain_x,test_x,train_y,test_y = train_test_split(x,y, random_state = 56)","550e545a":"#importing Linear Regression and metric mean square error\nfrom sklearn.linear_model import LinearRegression as LR\nfrom sklearn.metrics import mean_absolute_error as mae","0978b996":"# Creating instance of Linear Regresssion\nlr = LR()\n\n# Fitting the model\nlr.fit(train_x, train_y)","ecc3291d":"# Predicting over the Train Set and calculating error\ntrain_predict = lr.predict(train_x)\nk = mae(train_predict, train_y)\nprint('Training Mean Absolute Error', k )","506c85f6":"# Predicting over the Test Set and calculating error\ntest_predict = lr.predict(test_x)\nk = mae(test_predict, test_y)\nprint('Test Mean Absolute Error    ', k )","1fa2363a":"lr.coef_","a76ffde6":"plt.figure(figsize=(8, 6), dpi=120, facecolor='w', edgecolor='b')\nx = range(len(train_x.columns))\ny = lr.coef_\nplt.bar( x, y )\nplt.xlabel( \"Variables\")\nplt.ylabel('Coefficients')\nplt.title('Coefficient plot')","3c5fc67f":"# Arranging and calculating the Residuals\nresiduals = pd.DataFrame({\n    'fitted values' : test_y,\n    'predicted values' : test_predict,\n})\n\nresiduals['residuals'] = residuals['fitted values'] - residuals['predicted values']\nresiduals.head()","bc5990dd":"plt.figure(figsize=(10, 6), dpi=120, facecolor='w', edgecolor='b')\nf = range(0,2131)\nk = [0 for i in range(0,2131)]\nplt.scatter( f, residuals.residuals[:], label = 'residuals')\nplt.plot( f, k , color = 'red', label = 'regression line' )\nplt.xlabel('fitted points ')\nplt.ylabel('residuals')\nplt.title('Residual plot')\nplt.ylim(-4000, 4000)\nplt.legend()","3fd423c3":"# Histogram for distribution\nplt.figure(figsize=(10, 6), dpi=120, facecolor='w', edgecolor='b')\nplt.hist(residuals.residuals, bins = 150)\nplt.xlabel('Error')\nplt.ylabel('Frequency')\nplt.title('Distribution of Error Terms')\nplt.show()","a41935f4":"# importing the QQ-plot from the from the statsmodels\nfrom statsmodels.graphics.gofplots import qqplot\n\n## Plotting the QQ plot\nfig, ax = plt.subplots(figsize=(5,5) , dpi = 120)\nqqplot(residuals.residuals, line = 's' , ax = ax)\nplt.ylabel('Residual Quantiles')\nplt.xlabel('Ideal Scaled Quantiles')\nplt.title('Checking distribution of Residual Errors')\nplt.show()","df16273f":"# Importing Variance_inflation_Factor funtion from the Statsmodels\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\n\n# Calculating VIF for every column (only works for the not Catagorical)\nVIF = pd.Series([variance_inflation_factor(data.values, i) for i in range(data.shape[1])], index =data.columns)\nVIF","46b451f3":"# Creating instance of Linear Regresssion\nlr = LR(normalize = True)\n\n# Fitting the model\nlr.fit(train_x, train_y)","5d25dfb1":"# Predicting over the Train Set and calculating error\ntrain_predict = lr.predict(train_x)\nk = mae(train_predict, train_y)\nprint('Training Mean Absolute Error', k )","16460a98":"# Predicting over the Test Set and calculating error\ntest_predict = lr.predict(test_x)\nk = mae(test_predict, test_y)\nprint('Test Mean Absolute Error    ', k )","11acd8df":"plt.figure(figsize=(8, 6), dpi=120, facecolor='w', edgecolor='b')\nx = range(len(train_x.columns))\ny = lr.coef_\nplt.bar( x, y )\nplt.xlabel( \"Variables\")\nplt.ylabel('Coefficients')\nplt.title('Normalized Coefficient plot')","254184ad":"#seperating independent and dependent variables\nx = data.drop(['Item_Outlet_Sales'], axis=1)\ny = data['Item_Outlet_Sales']\nx.shape, y.shape","a6721e23":"Coefficients = pd.DataFrame({\n    'Variable'    : x.columns,\n    'coefficient' : lr.coef_\n})\nCoefficients.head()","fb61898c":"sig_var = Coefficients[Coefficients.coefficient > 0.5]","100741bf":"subset = data[sig_var['Variable'].values]\nsubset.head()","7686d42b":"# Importing the train test split function\nfrom sklearn.model_selection import train_test_split\ntrain_x,test_x,train_y,test_y = train_test_split(subset, y , random_state = 56)","aefb7f17":"#importing Linear Regression and metric mean square error\nfrom sklearn.linear_model import LinearRegression as LR\nfrom sklearn.metrics import mean_absolute_error as mae","a1785e04":"# Creating instance of Linear Regresssion with Normalised Data\nlr = LR(normalize = True)\n\n# Fitting the model\nlr.fit(train_x, train_y)","5daa4307":"# Predicting over the Train Set and calculating error\ntrain_predict = lr.predict(train_x)\nk = mae(train_predict, train_y)\nprint('Training Mean Absolute Error', k )","3d4b6d71":"# Predicting over the Test Set and calculating error\ntest_predict = lr.predict(test_x)\nk = mae(test_predict, test_y)\nprint('Test Mean Absolute Error    ', k )","ef8696ec":"plt.figure(figsize=(8, 6), dpi=120, facecolor='w', edgecolor='b')\nx = range(len(train_x.columns))\ny = lr.coef_\nplt.bar( x, y )\nplt.xlabel( \"Variables\")\nplt.ylabel('Coefficients')\nplt.title('Normalized Coefficient plot')","2ae20205":"The QQ-plot clearly verifies our findings from the the histogram of the residuals, the data is mostly normal in nature, but there sre some outliers on the higher end of the Residues.","c65aa231":"Here we can see that the model depends upon some Independent variables toos much, But these coefficients are not suitable for interpretation because these are not scaled, therefore we will perform the interpretation in this note book later.","ea3fdf59":"### Plotting residual curve (Is there constant Variance OR Homoscedastic?)","e3f0bfec":"#### Predicting over the train set","13aeea99":"#### Arranging coefficients with features","6a5e0634":"## Model Interpretability\n\nSo far we have simply been predicting the values using the linear regression, But in order to Interpret the model, the normalising of the data is essential.","fade8890":"### Parameters of Linear Regression","ebff2658":"### Implementing Linear Regression","606fd790":"The Residual plot clearly Looks Homoscedastic, i.e. the the variance of the error across the dataset is nearly constant.","27114ebb":"#### Training Model","0f546657":"From the ACF plot, we can clearly see that there is almost negligible correlation between the error terms. Hence there is no autocorrelation present in the data.","e029d6f9":"Now the coefficients we see are normalised and we can easily make final inferences out of it.\n\nHere we can see that there are a lot of Coefficients which are near to zero and not Significant.\nSo let us try removing them and build the model again.","1ce63068":"#### Creating new subsets of data","8f2d8705":"### Splitting the data into train set and the test set","25477133":"#### Plotting the coefficients","9ec0544a":"#### Splitting the data into train set and the test set","101f3ab7":"### QQ-Plot (Is the data Normally Distributed?)","b813ecc8":"### Importing the data","8217812c":"According to the Histogram, the distribution of error is nearly normal, But there are some outliers on the Higher end of the errors.","5b9c87b7":"### Checking Distribution of Residuals","cc6c4de4":"#### Chossing variables with sigificance greater than 0.5 ( Filtering Significant Features)","7f228cf0":"### Importing Libraries","a3cc49ef":"### Variance Inflation Factor (VIF) (Checking for multi collinearity)","20651144":"### Plotting the coefficients","d9612fe8":"#### Predicting over the test set","c32ca7f8":"### Segregating variables: Independent and Dependent Variables","8e0f27ad":"#### Extracting the significant subset do independent Variables","85ed087c":"From this list, we clearly see that there happens to be no Independent Variable over the value of 5, which means that there are no features that exhibit the Multicollinearity in the dataset. Note that VIF only works for the Continuous Variables. ","28f78dcd":"## Checking assumptions of Linear Model","9988aa61":"#### Implementing Linear Regression"}}