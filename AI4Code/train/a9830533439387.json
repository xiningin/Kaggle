{"cell_type":{"5f62c0ef":"code","384d0966":"code","95d38f9c":"code","6c4bc389":"code","44da978a":"code","7191a2c3":"code","5b73524a":"code","7414eec2":"code","5c4539dd":"code","d200a9ac":"code","f46264f8":"code","8cb7ce8d":"code","9fa85520":"code","88c85758":"code","cd5cb479":"code","371855c0":"code","067489d1":"code","4cb0cb17":"code","717078e3":"code","c5b82450":"code","f6d41e35":"code","ed002ca9":"code","4a15cff4":"code","5f366bd6":"code","ddbf80a0":"code","163d5e79":"code","55c1e150":"code","4c2e7d9a":"code","e3c413a5":"code","006cf92d":"code","2eb287fb":"code","dae11575":"code","987c3865":"markdown","30ceb461":"markdown","4ec96da9":"markdown","cff5f9b0":"markdown","2065964f":"markdown","90a5bc1b":"markdown","a238f2bf":"markdown","da303ee8":"markdown","9e27805a":"markdown","1774a6fe":"markdown","570d16bf":"markdown","b7173adb":"markdown","51c181c5":"markdown"},"source":{"5f62c0ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","384d0966":"data = pd.read_csv(\"\/kaggle\/input\/ibm-watson-marketing-customer-value-data\/WA_Fn-UseC_-Marketing-Customer-Value-Analysis.csv\")\ndata.info()","95d38f9c":"data.head(5)","6c4bc389":"# Check if each customer is unique in this dataset\nlen(data[\"Customer\"]) == len(data[\"Customer\"].unique())","44da978a":"data.describe(include=\"all\")","7191a2c3":"import plotly.express as px","5b73524a":"# Income distribution\nfig = px.histogram(data, x=\"Income\", title=\"Distribution of income\")\nfig.show()","7414eec2":"(data[\"Income\"]>0).value_counts()\/len(data)\n# ~25% customers report zero income.","5c4539dd":"data[data[\"Income\"]>0].describe()[\"Income\"]","d200a9ac":"data[\"IncomeGroup\"] = pd.cut(data[\"Income\"], [10000, 30000, 50000, 70000, data[\"Income\"].max()], \n       labels=[\"10K - 30K\", \"30K - 50K\", \"50K - 70K\", \"Above 70K\"], ordered=False)\ndata[\"IncomeGroup\"] = data[\"IncomeGroup\"].astype(str)\ndata[\"IncomeGroup\"].fillna(\"Zero Income\", inplace=True)","f46264f8":"features = [\"State\", \"Education\", \"EmploymentStatus\", \"Gender\", \"Marital Status\", \"Location Code\", \"IncomeGroup\", \"Sales Channel\", \"Vehicle Class\", \"Vehicle Size\"]\nfor col in features:\n    temp = pd.DataFrame(data[col].value_counts())\n    temp.reset_index(inplace=True)\n    temp.columns = [col, \"Count\"]\n    fig = px.pie(temp, values=\"Count\", names=col, title=\"Number of customers grouped by \"+col)\n    fig.show()","8cb7ce8d":"data[\"zero_income\"] = [1 if x==0 else 0 for x in data[\"Income\"]]\nfor col in features:\n    temp = pd.DataFrame(data.groupby(col).mean()[\"zero_income\"])\n    temp.reset_index(inplace=True)\n    temp.columns = [col, \"% reported zero income\"]\n    fig = px.bar(temp, x=col, y=\"% reported zero income\", title=\"% with zero income grouped by \"+col)\n    fig.show()\n# We find out all of them are unemployed.","9fa85520":"data.describe()[\"Customer Lifetime Value\"]\n# We see there is significant variance in CLV. The huge gap between the 75% percentile and the max suggests potential outliers.","88c85758":"np.percentile(data[\"Customer Lifetime Value\"], 99)\n# Check the 99% percentile","cd5cb479":"np.percentile(data[\"Customer Lifetime Value\"], 98)\n# Check the 98% percentile","371855c0":"for col in features:\n    fig = px.box(data, x=col, y=\"Customer Lifetime Value\", title=\"Boxplot of CLV grouped by \"+col)\n    fig.show()","067489d1":"# One-hot encoding of categorical variables\nX = data[\"Income\"]\nfor col in features:\n    dummies = pd.get_dummies(data[col])\n    data = pd.concat([data, dummies], axis=1)\n    X = pd.concat([X, dummies.iloc[:, :-1]], axis=1)","4cb0cb17":"# Run Linear Regresion\nimport statsmodels.api as sm\nfrom scipy import stats\ny = data[\"Customer Lifetime Value\"]\nX = sm.add_constant(X.drop(columns=\"Income\", axis=1))\nmodel = sm.OLS(y,X)\nresults = model.fit()\nresults.summary()","717078e3":"from sklearn.cluster import KMeans as km\nfrom sklearn import metrics\nfrom scipy.spatial.distance import cdist\nimport matplotlib.pyplot as plt","c5b82450":"# Try different number of clusters and select the optimal k using the elbow method\ninertias = []\nfor k in range(1,10):\n    kmeans = km(n_clusters=k).fit(X)\n    inertias.append(kmeans.inertia_)","f6d41e35":"plt.plot(range(1,10), inertias, \"bo-\")\nplt.title(\"The Elbow Method using Inertia\")\nplt.show()\n# We decide to move forward with 4 clusters.","ed002ca9":"kmeans = km(3, random_state=1).fit(X)\ndata[\"KMeansLabel\"] = kmeans.labels_\ndata[\"KMeansLabel\"].value_counts()","4a15cff4":"for col in [\"Customer Lifetime Value\", \"Number of Policies\", \"Total Claim Amount\"]:\n    temp = pd.DataFrame(data.groupby(\"KMeansLabel\").mean()[col])\n    temp.reset_index(inplace=True)\n    fig = px.bar(temp, x=\"KMeansLabel\", y=col, title=\"Avg. \"+col+\" grouped by KMeansLabel\")\n    fig.show()\n# We check customer quality across clusters.","5f366bd6":"for col in features:\n    temp = data.pivot_table(values=\"Customer\", index=[\"KMeansLabel\", col], aggfunc=\"count\")\n    temp.reset_index(inplace=True)\n    fig = px.bar(temp, x=\"KMeansLabel\", y=\"Customer\", color=col, title=\"Distribution of \"+col+\" across Clusters\")\n    fig.show()\n# We check the distribution of features across clusters.","ddbf80a0":"# Generate filters based on the findings above\ndata[\"KMeansLabel_manual\"] = \"Others\"\ndata.loc[(data[\"Location Code\"]==\"Suburban\") & (data[\"EmploymentStatus\"]==\"Employed\") & (data[\"Marital Status\"]!=\"Single\"), \"KMeansLabel_manual\"] = \"Suburb family\"\ndata.loc[(data[\"EmploymentStatus\"]==\"Unemployed\"), \"KMeansLabel_manual\"] = \"Unemployed\"\ndata.loc[(data[\"Location Code\"]==\"Rural\") & (data[\"EmploymentStatus\"]==\"Employed\") & (data[\"Income\"]>=50000), \"KMeansLabel_manual\"] = \"Rural high-income employee\"\ndata.loc[(data[\"Location Code\"]==\"Urban\") & (data[\"EmploymentStatus\"]==\"Employed\"), \"KMeansLabel_manual\"] = \"Urban employee\"\ndata[\"KMeansLabel_manual\"].value_counts()","163d5e79":"for col in [\"Customer Lifetime Value\", \"Number of Policies\", \"Total Claim Amount\"]:\n    temp = pd.DataFrame(data.groupby(\"KMeansLabel_manual\").mean()[col])\n    temp.reset_index(inplace=True)\n    fig = px.bar(temp, x=\"KMeansLabel_manual\", y=col, title=\"Avg. \"+col+\" grouped by KMeansLabel (Manual)\")\n    fig.show()","55c1e150":"for col in features:\n    temp = data.pivot_table(values=\"Customer\", index=[\"KMeansLabel_manual\", col], aggfunc=\"count\")\n    temp.reset_index(inplace=True)\n    fig = px.bar(temp, x=\"KMeansLabel_manual\", y=\"Customer\", color=col, title=\"Distribution of \"+col+\" across KMean Clusters (Manual)\", )\n    fig.show()","4c2e7d9a":"from statsmodels.stats.multicomp import pairwise_tukeyhsd","e3c413a5":"tukey_results = pairwise_tukeyhsd(endog=data[\"Customer Lifetime Value\"], groups=data[\"KMeansLabel_manual\"], alpha=0.15)\nprint(tukey_results)","006cf92d":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)","2eb287fb":"# Transform all features used for KMeans clustering into 2 dimensions for a 2D scatter plot\nreduced_dimensions = pd.DataFrame(pca.fit_transform(X))\nreduced_dimensions.columns = [\"PCA1\", \"PCA2\"]\ndata = pd.concat([data, reduced_dimensions], axis=1)","dae11575":"fig = px.scatter(data, x=\"PCA1\", y=\"PCA2\", color=\"KMeansLabel_manual\", title=\"Plot of Manually Defined Clusters based on PCA\")\nfig.show()","987c3865":"# Examine statistical significance using pairwise Tukey test\n\nAt a significance level of 0.15, the group Rural high-income employee, Suburb family and Urban employee have significantly higher CLV than the remaining groups, although the difference between each group is not statistically significant.\n\nThis means, we are able to identify high-value customer groups with clear criteria for targeting.","30ceb461":"# Kmeans clustering","4ec96da9":"All zero-income customers are unemployed, so we can keep these observations.","cff5f9b0":"# Regression analysis on CLV\n\nThe model has a low adjusted R-squared score (0.129) and thus a low explaining power.\n\nNevertheless, the model suggests some variables with significant impacts on CLV:\n\n* Female customers have higher CLV than male customers assuming all other things equal.\n* Divorced and married customers have higher CLV than single customers.\n* Customers with average income (i.e., 30K - 50K) have higher CLV than customers in other income groups.\n* Being employeed is associated with an uplift of 448 comapred to the umemployeed group.\n* If the insured vehicles are luxury cars, SUV or sports cars, then the customer contributes to a higher CLV.\n* Customers with small and medium-size cars also tend to have higher CLV.\n\nOverall, marital status, income and employment status have the strongest explaining power, and we should pay more attention to these factors when examining results from Kmeans clustering.","2065964f":"## Conclusions drawn from KMean clusters\n\n### Customer quality\n* The three clusters show in terms of CLV: Cluster 0 > 2 > 1 (8,300 > 8,000 > 7,700).\n* Cluster 2 have the lowest claim amount.\n\n### Customer profile\n* Cluster 0: Live in suburban areas, employed, married\n* Cluster 1: Live in suburban areas, unemployed, single\n* Cluster 2: Live in rural or urban areas (not suburban), employed, income above 50K\n","90a5bc1b":"# Visualising clusters using PCA","a238f2bf":"# Generate manual clusters based on our learnings so far","da303ee8":"# Summary statistics\n\nDistrbution across gender, education, employment, policy type, sales channel","9e27805a":"# Correlation between different variables and CLV","1774a6fe":"## Revisit manually-generated clusters and their characteristics\n\n* The four identifiable clusters account for 77% of the whole sample.\n* Rural high-income employee and suburb family groups have the highest CLV (8,300+), whereas the unemployed contribute the least (7,600). Urban employees stands somewhere in between (8,100).\n* One reason behind high value of rural high-income employee is that they have the lowest claim amount on average.","570d16bf":"We have a clean dataset with no missing values.","b7173adb":"The plot shows that the four groups we defined stay rather distinct from each other, with the group \"Others\" scattering across the plot.","51c181c5":"# Load and clean the dataset"}}