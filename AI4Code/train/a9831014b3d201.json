{"cell_type":{"227e9ba2":"code","fdba4eed":"code","a65d8f5b":"code","aa64c3c7":"code","7997bb34":"code","64269dca":"code","852ceafd":"code","3f878816":"code","dcd110a7":"code","12e24693":"code","8726a9d2":"code","27e9d01e":"code","4ef76092":"code","d817cde8":"code","c95321d0":"markdown","569ecdbd":"markdown"},"source":{"227e9ba2":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nimport lightgbm as lgb\nimport optuna\nimport joblib","fdba4eed":"train_df = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/train.csv')\ntest_df = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/test.csv')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/sample_submission.csv')","a65d8f5b":"def label_encoder(c):\n    lc = LabelEncoder()\n    return lc.fit_transform(c)","aa64c3c7":"def preprocess(df):\n    label_cols = ['Name', 'Ticket']\n    onehot_cols = ['Pclass', 'Sex', 'Cabin', 'Embarked', 'BucketAge']\n    numerical_cols = ['SibSp', 'Parch', 'SibSpParch', 'BucketFare', 'Survived']\n    age_map = df[['Age', 'Pclass']].dropna().groupby('Pclass').mean().to_dict()\n    df.Age = df.Age.fillna(df.Pclass.map(age_map['Age']))\n    df['BucketAge'] = df.Age\/\/35\n    df['BucketFare'] = train_df.Fare\/\/2\n    df['SibSpParch'] = df.SibSp + df.Parch\n    df.Cabin = df.Cabin.fillna('X').map(lambda x: x[0].strip())\n    df.Ticket = df.Ticket.fillna('X').map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else 'X')\n    df.Fare = df.Fare.fillna(df.Fare.mean())\n    df.Embarked = df.Embarked.fillna('X')\n    df.Name = df.Name.map(lambda x: x.split(',')[0])\n    onehot_encoded_df = pd.get_dummies(df[onehot_cols])\n    label_encoded_df = df[label_cols].apply(label_encoder)\n    numerical_df = df[numerical_cols]\n    return pd.concat([numerical_df, label_encoded_df, onehot_encoded_df], axis=1)","7997bb34":"all_df = preprocess(df = pd.concat([train_df, test_df]))","64269dca":"# Re-split all data\nX = all_df[:train_df.shape[0]]\ny = X.pop('Survived')\nX_ = all_df[train_df.shape[0]:].drop(columns=['Survived'])","852ceafd":"def objective(trial):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n    dtrain = lgb.Dataset(X_train, label=y_train)\n    dval = lgb.Dataset(X_test, label=y_test)\n \n    param = {\n        'objective': 'binary',\n        'boosting': 'gbdt',\n        'metric': 'auc',\n        'verbose': -1,\n        'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 1),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 100, 2000, 50),\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 512),\n        'max_depth': trial.suggest_int('max_depth', 2, 10),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n#         'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'max_bin': trial.suggest_int('max_bin', 10, 300, 10),\n    }\n    \n    folds = KFold(n_splits=5)\n    accuracies = []\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n        print(\"Fold {}\".format(fold_))\n        X_train = X.iloc[trn_idx]\n        y_train = y[trn_idx]\n        X_test = X.iloc[val_idx]\n        y_test = y[val_idx]\n        \n        dtrain = lgb.Dataset(X_train, label=y_train)\n        dval = lgb.Dataset(X_test, label=y_test)\n    \n        gbm = lgb.train(param, dtrain, valid_sets=[dval], num_boost_round=10000, early_stopping_rounds=100, verbose_eval=-1)\n        preds = gbm.predict(X_test)\n        pred_labels = np.rint(preds)\n        accuracy = accuracy_score(y_test, pred_labels)\n        accuracies.append(accuracy)\n    return np.mean(accuracies)","3f878816":"def main(n_trials=10):\n    try:  # try to load an already saved trials object, and increase the max\n        study = joblib.load(\"tabular_apr.optuna\")\n        print(\"Found saved Study! Loading...\")\n    except:  # create a new trials object and start searching\n        study = optuna.create_study(direction='maximize')\n\n    study.optimize(objective, n_trials)\n    \n    joblib.dump(study, \"tabular_apr.optuna\")","dcd110a7":"loops = 4\ntrials_x_loop = 50\n\nfor i in range(loops):\n    main(trials_x_loop)","12e24693":"study = joblib.load(\"tabular_apr.optuna\")","8726a9d2":"study.best_value, study.best_params ","27e9d01e":"optuna.visualization.plot_param_importances(study)","4ef76092":"optuna.visualization.plot_optimization_history(study)","d817cde8":"optuna.visualization.plot_parallel_coordinate(study)","c95321d0":"<h4>Study state is saved in each loop.<\/h4>\n<h4>Set <b>loops<\/b> and <b>trials_x_loop<\/b> for save study frequency and total number of trials.<\/h4>","569ecdbd":"<h3>LightGBM parameter optimization with OPTUNA<\/h3>\n\nHyperparameter search notebook for: https:\/\/www.kaggle.com\/jmargni\/tps-apr-2021-lightgbm-cv\n"}}