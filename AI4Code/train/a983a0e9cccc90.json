{"cell_type":{"1509c39e":"code","6204e086":"code","6580c23f":"code","63cc0411":"code","3e3d93b9":"code","7094b46e":"code","fa58b298":"code","e97302bb":"code","c2739e2a":"code","deb48252":"code","87ad7bcb":"code","a43a0c0e":"code","ac2b78f0":"code","9e66b031":"markdown","6c6220a4":"markdown","ff4a4770":"markdown","47cf7561":"markdown","818fed08":"markdown","e4ec3eec":"markdown","6c53c5e2":"markdown","1524d0cf":"markdown","a3e19089":"markdown","56e23380":"markdown","dd2f7009":"markdown","defba434":"markdown","5cb1d9a9":"markdown","c399503d":"markdown","10564f67":"markdown","474e6455":"markdown","7cbfd2ff":"markdown","ae7c8921":"markdown","83547a06":"markdown","de7c7931":"markdown","c4c9169b":"markdown","a21c8faa":"markdown","50a0307e":"markdown","9332fee6":"markdown","5b9375c8":"markdown","8e7d1da2":"markdown","a6b52e3d":"markdown","a53a708c":"markdown","600e4e53":"markdown","0f6c75dd":"markdown","dc92783c":"markdown","ec0a13ab":"markdown","19b95a9b":"markdown","2b3b50a8":"markdown","0ac422d5":"markdown","26eb0b8e":"markdown","834847cc":"markdown","728b984d":"markdown","caef93b4":"markdown","a4dbc0ae":"markdown","bf5917a1":"markdown","02079ec4":"markdown","6df6c5b7":"markdown","60a839d5":"markdown","6a62772f":"markdown","95ad4ec9":"markdown","4efe8ead":"markdown","54b4f79e":"markdown","35f246d6":"markdown","5b887406":"markdown","275a2018":"markdown","41e2dd88":"markdown","dc2d2d80":"markdown","a0eeb6fb":"markdown","a0257716":"markdown","3a176c70":"markdown","cfba6ebd":"markdown","dc96317a":"markdown","18007887":"markdown","b98c118f":"markdown","bdbba0a4":"markdown","2a3f0a19":"markdown","544fb4e5":"markdown","1e71c864":"markdown","4548744d":"markdown","eff34455":"markdown","e451d77b":"markdown","9fb26771":"markdown","1bd3c962":"markdown","1bb75d06":"markdown","d7bd722b":"markdown","8e03881e":"markdown","aefd6200":"markdown","a99bf0c0":"markdown","505a2216":"markdown","8390a6bd":"markdown","f6f741aa":"markdown","1fde56e6":"markdown","325b32fb":"markdown","6a65744c":"markdown","034ccb56":"markdown","d5c3731e":"markdown","adffdab4":"markdown","43aaba22":"markdown","fab69640":"markdown","15607520":"markdown"},"source":{"1509c39e":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \n\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler,PowerTransformer\nfrom sklearn.model_selection import train_test_split, RepeatedStratifiedKFold,cross_val_score,GridSearchCV\nfrom sklearn.linear_model import LinearRegression,LogisticRegression\nfrom sklearn.pipeline import Pipeline,make_pipeline\nfrom sklearn.metrics import mean_squared_error,classification_report,make_scorer,accuracy_score,plot_roc_curve,auc,roc_curve\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import KFold\n\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","6204e086":"df_credit = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf_credit.head()","6580c23f":"df_credit['Class'].value_counts(normalize=True)","63cc0411":"\ndf_credit = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\n\nX = df_credit.drop('Class', axis=1)\ny = df_credit['Class']\n\nmodel =DummyClassifier(strategy='most_frequent')\n\n#pipeline = Pipeline(steps=[('imp', SimpleImputer(strategy='median')),('s',MinMaxScaler()),('m', model)]) \n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n\nresult = cross_val_score(model, X, y,  scoring='accuracy',cv=cv, n_jobs=-1)\n\nprint(f'{round(np.mean(result),6)}')","3e3d93b9":"\ndef classification_report_with_validation(y_true, y_pred):\n    real_values.extend(y_true)\n    predicted_values.extend(y_pred)\n    return accuracy_score(y_true, y_pred)\n\n\ndf_credit = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\n\nX = df_credit.drop('Class', axis=1)\ny = df_credit['Class']\n\nreal_values = []\npredicted_values = []\n\nmodel =LogisticRegression(solver='liblinear')\n\npipeline = Pipeline(steps=[('s',MinMaxScaler()),('m', model)]) \n\ncv = KFold(n_splits=10, random_state=42)\n\nresult = cross_val_score(pipeline, X, y,  scoring=make_scorer(classification_report_with_validation),cv=cv)\n\nprint(classification_report(real_values, predicted_values)) ","7094b46e":"df_credit = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\n\nX = df_credit.drop('Class', axis=1)\ny = df_credit['Class']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1, stratify=y)\n\npipeline = make_pipeline(MinMaxScaler(), LogisticRegression(solver='liblinear'))\n\npipeline.fit(X_train,y_train)\nprobs = pipeline.predict_proba(X_test)\nfpr1, tpr1, thresholds = roc_curve(y_test, probs[:, 1], pos_label=1)\nroc_auc1 = auc(fpr1, tpr1)\n\n\nfig, ax = plt.subplots(figsize=(7.5, 7.5))\n \nplt.plot(fpr1, tpr1, label='ROC Curve 1 (AUC = %0.2f)' % (roc_auc1))\nplt.plot([0, 1], [0, 1], linestyle='--', color='red', label='Random Classifier')   \nplt.plot([0, 0, 1], [0, 1, 1], linestyle=':', color='green', label='Perfect Classifier')\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.legend(loc=\"lower right\")\nplt.show()","fa58b298":"# log loss for naive probability predictions.\nfrom sklearn.metrics import log_loss\n# generate 2 class dataset\ndf_credit = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\n\nX = df_credit.drop('Class', axis=1)\ny = df_credit['Class']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, stratify=y)\n\n\n# no skill prediction 0\nprobabilities = [[1, 0] for _ in range(len(y_test))]\navg_logloss = log_loss(y_test, probabilities)\nprint('P(class0=1): Log Loss=%.3f' % (avg_logloss))\n# no skill prediction 1\nprobabilities = [[0, 1] for _ in range(len(y_test))]\navg_logloss = log_loss(y_test, probabilities)\nprint('P(class1=1): Log Loss=%.3f' % (avg_logloss))\n# baseline probabilities\nprobabilities = [[0.99, 0.01] for _ in range(len(y_test))]\navg_logloss = log_loss(y_test, probabilities)\nprint('Baseline: Log Loss=%.3f' % (avg_logloss))\n# perfect probabilities\navg_logloss = log_loss(y_test, y_test)\nprint('Perfect: Log Loss=%.3f' % (avg_logloss))","e97302bb":"column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\ndf_boston= pd.read_csv('..\/input\/boston-house-prices\/housing.csv',header=None, delimiter=r\"\\s+\", names=column_names)\ndf_boston = df_boston.drop('CHAS', axis=1)\ndf_boston.head()","c2739e2a":"df_boston['MEDV'].describe()","deb48252":"X = df_boston.drop('MEDV',axis=1)\ny = df_boston['MEDV']\npipeline = make_pipeline(PowerTransformer(method='yeo-johnson'), LinearRegression())\ncv = KFold(n_splits=10, random_state=42)\nresults = cross_val_score(pipeline, X, y, cv=cv, scoring='neg_mean_absolute_error')\nprint(f'MAE: {round(results.mean()*-1,3)}, ({round(results.std(),3)})')","87ad7bcb":"X = df_boston.drop('MEDV',axis=1)\ny = df_boston['MEDV']\npipeline = make_pipeline(PowerTransformer(method='yeo-johnson'), LinearRegression())\ncv = KFold(n_splits=10, random_state=42)\nresults = cross_val_score(pipeline, X, y, cv=cv, scoring='neg_mean_squared_error')\nprint(f'MSE: {round(results.mean()*-1,3)}, ({round(results.std(),3)})')","a43a0c0e":"X = df_boston.drop('MEDV',axis=1)\ny = df_boston['MEDV']\npipeline = make_pipeline(PowerTransformer(method='yeo-johnson'), LinearRegression())\ncv = KFold(n_splits=10, random_state=42)\nresults = cross_val_score(pipeline, X, y, cv=cv, scoring='neg_mean_squared_error')\nprint(f'RMSE: {round(np.sqrt(results.mean()*-1),3)}, ({round(results.std(),3)})')","ac2b78f0":"X = df_boston.drop('MEDV',axis=1)\ny = df_boston['MEDV']\npipeline = make_pipeline(PowerTransformer(method='yeo-johnson'), LinearRegression())\ncv = KFold(n_splits=10, random_state=42)\nresults = cross_val_score(pipeline, X, y, cv=cv, scoring='r2')\nprint(f'R Squared: {round(results.mean(),3)}, ({round(results.std(),3)})')","9e66b031":"- Logarithmic loss or log loss metric is based on probabilities.\n- The log loss function calculates the negative log likelihood for probability predictions made by the binary classification model.\n\n","6c6220a4":"image credit: https:\/\/giphy.com","ff4a4770":"- When we look at the formula above, if we put 100 cases as non-fraud, based on the equation we can get 99.8% accuracy\n-  99.8% accuracy !!!!\n- It is great isn't it ?\n- Let's see all of this by using Dummy Classifier.","47cf7561":"- Higher value of True Positive Rate (TPR) means that false negative is very low. Model correctly predicted positive class.\n\n- Lower value of False Positive Rate means that false positive is very low. Model correctly predicted negative class.","818fed08":"<div class=\"alert alert-block alert-info\">\n<b>Precision & Recall --> Which one to use and When ?<\/b> \n     <ul style=\"list-style-type:none\">\n         <li><b>Precision:<\/b>  When our aim is to minimize false positive (Only fraud case, not include non-fraud transaction as a fraud transaction)<\/li>\n         <li><b>Recall : <\/b> When our aim is to minimize false negative (Every cancer patient should be classified as  a cancer patient, not classified as a healthy one)<\/li>\n      <\/ul>\n    \n   \n<\/div>\n","e4ec3eec":"image credit: https:\/\/www.negotiations.com","6c53c5e2":"- In real life we want to get perfect prediction on the positive class\n- Which means that we are looking high recall and high precision.\n- We have to balance them to get what we want.\n- F score provides us a score which combines precision and recall into a single measure without losing  their properties.","1524d0cf":"<a id=\"3\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Accuracy<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","a3e19089":"- In this study we will divide our evaluation metrics into two categories.\n    - Classification Evaluation Metrics\n    - Regression Evaluation Metrics\n \n - Ok let's start.","56e23380":"#### Hi all.  \ud83d\ude4b\u2022\u2642\ufe0f \n\n#### We continue our **Beginner-Intermediate Friendly Machine Learning series**, which would help anyone who wants to learn or refresh the basics of ML.\n\n#### What we have covered: \n\n#### [Beginner Friendly Detailed Explained EDAs \u2013 For anyone at the beginnings of DS\/ML journey](https:\/\/www.kaggle.com\/general\/253911#1393015) \u2714\ufe0f\n\n#### [BIAS & VARIANCE TRADEOFF](https:\/\/www.kaggle.com\/kaanboke\/ml-basics-bias-variance-tradeoff) \u2714\ufe0f\n\n#### [LINEAR ALGORITHMS](https:\/\/www.kaggle.com\/kaanboke\/ml-basics-linear-algorithms)  \u2714\ufe0f\n\n#### [NONLINEAR ALGORITHMS](https:\/\/www.kaggle.com\/kaanboke\/nonlinear-algorithms)  \u2714\ufe0f\n\n#### [The Most Used Methods to Deal with MISSING VALUES](https:\/\/www.kaggle.com\/kaanboke\/the-most-used-methods-to-deal-with-missing-values)  \u2714\ufe0f\n\n#### [Beginner Friendly End to End ML Project- Classification with Imbalanced Data](https:\/\/www.kaggle.com\/kaanboke\/beginner-friendly-end-to-end-ml-project-enjoy)  \u2714\ufe0f\n\n#### [How to Prevent the Data Leakage ?](https:\/\/www.kaggle.com\/kaanboke\/how-to-prevent-the-data-leakage) \u2714\ufe0f\n\n#### In this notebook we will  cover one of the important concepts of the **Machine Learning Evaluation Metrics**\n#### Enjoy \ud83e\udd18","dd2f7009":"Image Credit: https:\/\/miro.medium.com\/","defba434":"image credit: https:\/\/programmerah.com","5cb1d9a9":"![](https:\/\/cdn-images-1.medium.com\/max\/959\/1*WDKhO-z7rti70ZTv59yJ9A.jpeg)","c399503d":"![](https:\/\/i.stack.imgur.com\/UN1Pk.png)","10564f67":"<a id=\"0\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>What is Evaluation Metrics? & Why We Need Them?<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n","474e6455":"#### **Precision**\n- Precision gives us accuracy of the positive classs (fraud case, cancer case, malign case, etc).\n- Main aim of the precision is the minimize the false positive (Type 1 error)","7cbfd2ff":"- **False Negative**: Predicted value is negative, but actual value is positive. Our prediction is false.\n   - We predict non-fraud, benign, but actual value is fraud or malign.\n   - Which is also called **Type 2 error**.","ae7c8921":"<a id=\"10\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Mean Squared Error (MSE)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","83547a06":" <a id=\"9\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Mean Absolute Error (MAE)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","de7c7931":"![](https:\/\/i.ytimg.com\/vi\/fcO9820wCXE\/hqdefault.jpg)","c4c9169b":"![](https:\/\/www.magazine.etnfocus.com\/wp-content\/uploads\/2017\/08\/metrics.jpg)","a21c8faa":"- What we are looking for  is the lowest level loss. The best possible log loss is 0.0\n- Any model with lower log-loss value brings us better predictions.","50a0307e":"- **True Positive**: Predicted vale is positive and we predicted correctly. \n- Real Transaction --> Fraud  and our model correctly predict as fraud.","9332fee6":"image credit: https:\/\/www.mydatamodels.com","5b9375c8":"<a id=\"7\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Log Loss<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","8e7d1da2":"<a id=\"8\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>Regression Evaluation Metrics<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","a6b52e3d":"- MSE is the squarred average squared differences between predicted value and the real value.\n- Lower the MSE, better the prediction.","a53a708c":"<a id=\"6\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>ROC Curve (AUC)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","600e4e53":"- **True Negative** : We predict as negative and our prediction is correct. Actual value is negative ( non-fraud, benign, etc.)","0f6c75dd":"<a id=\"4\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Precision & Recall<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","dc92783c":"#### **By the way, when you like the topic, you can show it by supporting** \ud83d\udc4d\n\n####  **Feel free to leave a comment in the notebook**. \n\n#### All the best \ud83e\udd18","ec0a13ab":"- As shown below, MAE is average absolute differences between  our predicitions and the real value.\n- MAE is easily interpretable\n- Lower the MAE, better the prediction.","19b95a9b":"image credit: https:\/\/slidetodoc.com\/class-5-thurs-sep-23-example-of-using","2b3b50a8":"#### **Recall**\n- Recall gives us the score of the number of correct positive predictions made out of all correct positive predictions.\n- Main aim ofthe recall is the minimize the false negative (Type 2 error).","0ac422d5":"<img src=\"https:\/\/els-jbs-prod-cdn.jbs.elsevierhealth.com\/cms\/attachment\/36cdb4ec-0c7d-48cb-9a4d-7cb463f8b7c3\/gr1.jpg\" width=\"600\">","26eb0b8e":"![](https:\/\/www.negotiations.com\/wp-content\/uploads\/2017\/05\/negotiation-success.jpg)","834847cc":"- Ok we have 99.8% accuracy on the credit card fraud, without learning anything, why we are bothering ourselves to build a model?\n\n- We can easily select every case as a non-fraud and 99.8 out of 100 times, we are right.\n\n- Why aren't we celebrating it?","728b984d":"- It is clearly seen in the formula, why accuracy is not a good measure for the imbalanced data.\n- Imagine we have  a data...\n- Just kidding, you don't need to imagine we have real data to see.\n- We will use credit card fraud data to estimate fraud cases.\n- This is imbalanced data. Be careful !!!","caef93b4":"<a id=\"11\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Root Mean Squared Error (RMSE)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","a4dbc0ae":"- **False Positive**: Our prediction is positive but actual value is negative. Our prediction is false.\n   - We predict as fraud or malign but actual value is non-fraud or benign.\n   - Which is also called as **Type 1 error**.","bf5917a1":"- Classification problems are the most common problems in the Machine Learning.\n- It would be a good idea to refresh our knowledge on the classification evaluation metrics.\n- In the classification part of the study, we will use Credit Card Fraud dataset.","02079ec4":"<a id=\"toc\"><\/a>\n\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Table of Contents<\/h3>\n    \n* [What is Evaluation Metrics?](#0)\n* [Classification Evalution Metrics](#1)\n    * [Confusion Matrix](#2)\n    * [Accuracy](#3)\n    * [Precision & Recall](#4)\n    * [F Score (F Measure)](#5)\n    * [ROC Curve (AUC)](#6)\n    * [Log Loss](#7)\n  \n  \n* [Regression Evaluation Metrics](#8)    \n    * [Mean Absolute Error(MAE)](#9)\n    * [Mean Squared Error (MSE)](#10)    \n    * [Root Mean Squared Error (RMSE)](#11)\n    * [R Squared (R2)](#12)\n\n\n* [Conclusion](#13)\n* [References & Further Reading](#14)","6df6c5b7":"<a id=\"14\"><\/a>\n<font color=\"darkblue\" size=+1.5><b>References & Further Reading<\/b><\/font>\n\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n\n\n[Machine Learning - Beginner &Intermediate-Friendly BOOKS](https:\/\/www.kaggle.com\/general\/255972)","60a839d5":" #### **What is the problem with the accuracy metric for the imbalanced data?**\n\n- Accuracy metric with imbalanced data gives us the accuracy on the majority class (non-fraud)\n- We can reach to 99.8% accuracy without building a machine leraning model, by always predicting the non-fraud.\n- The problem here is that accuracy is an inadequate measure for quantifying predictive performance in this imbalanced setting.\n- Accuracy does not report the correct score for the imbalanced data.\n- As we have seen overwhelming number of non-fraud instances (99.8%) surprass the fraud instances.\n- Even Dummy Classifier can get the 99.8% accuracy score.","6a62772f":"![](https:\/\/miro.medium.com\/max\/1400\/1*FUZS9K4JPqzfXDcC83BQTw.png)","95ad4ec9":"<div class=\"alert alert-block alert-info\">\n<b>Rule of Thumb:<\/b> Do not use accuracy score metric with the imbalanced data.\n<\/div>","4efe8ead":"- Most of the applications in default uses R squared as a metric for the regression problems.\n- R squared gives us the proportion of the target variable is explained by the feature(s).\n- R squared provides an indication of the goodness of fit of a set of predictions to the actual values.","54b4f79e":"- We have used log loss() function of the scikit-learn.\n- It took the predicted probability for each class as input and returned the average log loss.\n- Predicting certainty for fraud and non fraud label is punished with large log loss scores.\n- Since dataset has .5% minority class instances,  being certain for the minority class in all cases results in a much larger log loss score.\n- Baseline did better job by using target distribution.\n- Any model brings us lower than baseline log loss score would make prediction with skill.\n- Perfect log loss score: 0.0 means that there is no difference between prediction and the real values.","35f246d6":"<a id=\"2\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Confusion Matrix<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","5b887406":"image credit: https:\/\/www.magazine.etnfocus.com","275a2018":"image credit: https:\/\/en.wikipedia.org","41e2dd88":"- **Enjoy** \ud83e\udd18","dc2d2d80":"- We have covered one of the most important concepts of the Machine Learning.\n- We have looked at both classification metrics and regression metrics.\n- We have talked about the misusages of the metrics and the correct ones.\n\n- Evaluation metrics are crucial. Based on the model performance we are giving decisions.\n- We should remember that we are not just only looking for a better model, also looking for our end goal.\n- Before deciding evaluation metrics, it would be a good idea to talk your customer, stakeholders and relevant people to clarify their goals and what they realy want.\n- And please remember that most of the classification problems in the real life have imbalanced data.\n","a0eeb6fb":"![](https:\/\/media.giphy.com\/media\/l2JJsJQY6yj9HLaZW\/giphy.gif)","a0257716":"- It would be good idea to refresh our knowledge on the regression evaluation metrics.\n- We will look at \n    - Mean Absolute Error, \n    - Mean Squared Error\n    - Root Mean Squared Error\n    - R2\n- First we will see their definitions and formulas and then see them in the action.\n- In this study, we will use Boston House  prices dataset.","3a176c70":"- In machine learning, evaluation metrics are used to measure the performance of machine learning models\/algorithms.\n- Evaluation metrics are crucial. Based on the model performance we are giving decisions.\n- We should remember that we are not just only looking for a better model, also looking for our end goal.\n- Let's imagine our end goal is to make an application to detect fraud.\n- We develop our model based on the data in hand, which contains 99.5% non-fraud cases and %.5 fraud cases.\n- Without using the correct evaluation metric on this imbalanced data we will deploy the model with poor performance and prediction on the real data.","cfba6ebd":"image credit: https:\/\/www.youtube.com\/channel\/UCeoF_5Kw0YyWOqhAbQGrxJQ","dc96317a":"- RMSE is basically square root of the MSE\n- By taking the square root of the MSE, units are converted  back to the original units of the target variable.","18007887":"image credit: https:\/\/www.superheuristics.com","b98c118f":"![](https:\/\/slidetodoc.com\/presentation_image\/7d85c6a301ba5b97b7d3b73273b073d0\/image-13.jpg)","bdbba0a4":"- Before starting the evaluation metrics, we should be on the same page.\n- Let's refresh the basics.","2a3f0a19":"- We didn't make any extensive exploratory analysis with the data. \n- We have just used it for showing the usage of the classification metrics on the imbalanced data.\n- For having said that precision: .93 , recall : 77 and f1 score: .83\n- And accuracy is 1.00 !!!","544fb4e5":"- One of the most common evaluation metrics, we can see in the real world and also in the Kaggle.\n- Accuracy is better to use with balanced classification problem and when all predictions and prediction errors are equally important (for example using iris dataset. Every class has equal instances).\n\n- Balanced Data: Target has equal or almost equal number of instances.\n- Prediction Errors are  equally important: Predicting  Class A, Class B or Probability of detecting fraud or detecting non fraud\n- Is it really possible in the real life?\n- I can't say, it is impossible, but fair to say it is rare.\n- Most of the classification problem, we handle in ML, has imbalanced data and consequences of the prediction errors are rarely same.\n- When we have the imbalanced data, accuracy is not a good evaluation metric to use.\n","1e71c864":"<a id=\"5\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>F Score (F Measure)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","4548744d":"image credit: https:\/\/www.jtcvs.org\/article\/S0022-5223(18)32875-7","eff34455":"![](https:\/\/i.pinimg.com\/originals\/aa\/91\/7a\/aa917a42422eaedb18224224519e48f0.jpg)","e451d77b":"<div class=\"alert alert-block alert-info\">\n<b>Note:<\/b>  ROC Curve (AUC) is often more meaningful than using accuracy metric for classification problems with imbalanced data.\n<\/div>","9fb26771":"![](https:\/\/www.mydatamodels.com\/wp-content\/uploads\/2020\/10\/2.-Accuracy-formula-machine-learning-algorithms.png)","1bd3c962":"- Formula for the accuracy is easy one: Total number of correct predictions divided by the total number of predictions.","1bb75d06":"- As we have mentioned before, deciding which metric to use very crucial step on the Machine Learning projects.\n- Stakeholders \/ customers concerns should be taken into consideration before deciding which metric to use.\n\n- In fraud detection case, if our customer aims to reduce false negative:\n    - Which means every fraud case should be defined as a fraud case\n    - Missing the prediction of the fraud case should be minimum\n    - We have to focus on how to reduce wrongly classified non-fraud cases.\n    - In that case we are looking for minimizin type 2 error and increasing the positive rate.\n    - We are looking for higher score recall for fraud case.\n    \n    \n- In fraud detection case, if our customer aims to reduce false positive:\n    - Which means we want to be sure that positive case should be positive case, not the others\n    - We do not want to classify our loyal customer's transaction as a fraud transaction and block his\/her account.\n    - We have to focus on wrongly classified positive case.\n    - In that case we are looking for minimizin type 1 error and decreasing the false positive rate.\n    - We are looking for higher score precision for fraud case.\n \n \n- If we want to reduce the risk of the fraud without losing our customer:\n    - We want to make a balance between precision and recall\n    - It would be good idea to focus on F score\n","d7bd722b":"image credit: https:\/\/www.publichealthnotes.com","8e03881e":"#### **By the way, when you like the topic, you can show it by supporting** \ud83d\udc4d\n\n####  **Feel free to leave a comment in the notebook**. \n\n#### All the best \ud83e\udd18","aefd6200":"![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/26\/Precisionrecall.svg\/600px-Precisionrecall.svg.png)","a99bf0c0":"<img src=\"https:\/\/programmerah.com\/wp-content\/uploads\/2020\/11\/20190714113817886.png\" width=\"600\">","505a2216":"![](https:\/\/www.superheuristics.com\/wp-content\/uploads\/2021\/03\/Blog_image_confusion-matrix.png)","8390a6bd":"![](https:\/\/i.imgur.com\/19LNbyQ.jpg)","f6f741aa":"image credit : https:\/\/stackoverflow.com\/questions\/56401346\/mean-absolute-error-in-tensorflow-without-built-in-functions\/56401550","1fde56e6":"<img src=\"https:\/\/www.publichealthnotes.com\/wp-content\/uploads\/2020\/04\/slide_9.jpg\" width=\"600\">\n","325b32fb":"<a id=\"1\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>Classification Evaluation Metrics<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","6a65744c":"image credit: https:\/\/cdn-images-1.medium.com","034ccb56":"image credit: https:\/\/www.pinterest.com","d5c3731e":"<a id=\"13\"><\/a>\n<font color=\"darkblue\" size=+1.5><b>Conclusion<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","adffdab4":"- Below code-snippet is generated by using code recipe in the [Imbalanced Classification with Python](https:\/\/machinelearningmastery.com\/imbalanced-classification-with-python\/). I have made changes and modified it to adjust to the problem at hand.","43aaba22":"image credit: https:\/\/stackoverflow.com","fab69640":"- ROC Curve (receiver operating characteristic curve- AUC) measures model's ability to make distinction between two classes (positive & negative).\n- ROC Curve score close to 1, represents better model.\n- ROC Curve shows false positive rate against the true positive rate (recall)\n- What we are looking for : **High recall and low false positive rate**\n- ROC Curve should be as close as possible to the top left corner.\n- No matter how imbalanced  data we have,predicting randomly always produces an AUC of 0.5.","15607520":"<a id=\"12\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>R Squared (R2)<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>"}}