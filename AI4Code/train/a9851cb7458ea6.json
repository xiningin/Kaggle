{"cell_type":{"dbe2b89b":"code","916fa242":"code","f7d2a268":"code","837aa3bc":"code","cedc3aba":"code","5974f070":"code","5587a34e":"code","a1593aab":"code","165ff336":"code","2882c350":"code","8081e655":"code","55f56112":"code","c8d02dc1":"code","432869aa":"code","ff9a2377":"code","b5ebdebd":"code","f961aa11":"code","599c2485":"code","a60cff56":"code","59c14455":"code","53038936":"code","23a041ad":"code","738ccd9a":"code","922ee323":"code","a04ad94c":"code","b23ed408":"code","c20e9d8a":"code","db7b6ca1":"code","e3aff34d":"code","f4a47853":"code","094f6030":"code","bce78aa1":"code","db0941d2":"code","4a89acb4":"code","382c72ee":"code","7bd84c13":"code","2ce93cbb":"code","dff9161e":"code","80a413bd":"code","f7cb3d68":"code","2225d788":"code","bad83c8a":"code","976cdd92":"code","be849699":"code","c4fcced5":"code","b1a476c4":"code","2e39a12a":"code","8ff45635":"code","836ea592":"code","4f2d15b7":"code","6f86e145":"code","6ee03c83":"code","bf1dbd62":"code","8cab0c0d":"code","94bcaacd":"code","a6f63705":"code","3e3b0bb0":"code","4c7c9fd4":"code","9709ec9d":"code","7a15835b":"code","1737aef1":"code","37679071":"code","93cb3269":"code","a946c1b4":"code","9b9feb26":"code","b7469934":"code","6938b989":"code","fa662fcd":"code","566cff3b":"code","9f84126d":"code","2f2538d6":"code","a79fe29a":"code","1e076c53":"code","3ef66785":"code","16da080e":"code","edd938a2":"code","ea757ceb":"code","c4a1d050":"code","79f73c5d":"code","ecc2d57f":"code","0d2b6c07":"code","d36b2a3a":"code","bba7d388":"code","db575b5d":"code","73363bb1":"code","6a3ee86f":"code","0d04b6f8":"code","a6da7878":"code","ea93cab4":"code","de70a8ce":"markdown","27365e6a":"markdown","067d7062":"markdown","21ac3ba0":"markdown","dc295916":"markdown","ec3fa2ba":"markdown","8210527f":"markdown","400f85e4":"markdown","365c3577":"markdown","05b1e021":"markdown","2fd1a7fc":"markdown","c630faaa":"markdown"},"source":{"dbe2b89b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","916fa242":"data_law = pd.read_csv(\"\/kaggle\/input\/heart-failure-prediction\/heart.csv\")\ndata = data_law.copy()","f7d2a268":"data.info()","837aa3bc":"#\uacb0\uce21\uce58 \nprint(data.isna().sum(), \"\\n\")\nprint(data.isnull().sum())\n","cedc3aba":"data['FastingBS'] = data['FastingBS'].astype('object')\ndata['HeartDisease'] = data['HeartDisease'].astype('object')","5974f070":"obj_cols = []\nnum_cols = []\nfor col in data.columns :\n    if data[col].dtype == 'object' : \n        obj_cols.append(col)\n    else :\n        num_cols.append(col)","5587a34e":"data[num_cols].describe()","a1593aab":"data[obj_cols].describe()","165ff336":"data.boxplot(num_cols)","2882c350":"for col in num_cols :\n    print(col, \" 0\uc778 \uac1c\uc218 : \" ,(data[col] == 0 ).sum())","8081e655":"# Resting BP\uc774 0\uc77c \uc218 \uc5c6\uc73c\ubbc0\ub85c \uc81c\uac70 ( BP never be 0)\ndata1  = data[data['RestingBP']!=0].reset_index(drop = True)\n\n# Cholesterol\uc774 0\uc778 \uc790\ub8cc\uc5d0 \ub300\ud574\uc11c \uacb0\uce21\uce58\ub85c \ud310\ub2e8\ud55c \uacbd\uc6b0\uc640 \uadf8\ub807\uc9c0 \uc54a\uc740\uacbd\uc6b0\ub85c \uad6c\ubd84\ud558\uc5ec \ubd84\uc11d\uc744 \uc9c4\ud589\n# multi datasets for Cholesterol is 0 is missing data or not\ndata2 = data1.copy()\ndata2 = data2.drop('Cholesterol',axis = 1) #cholesterol \ubcc0\uc218 \uc81c\uac70\ndata3 = data1.copy()\ndata3 = data3[data3['Cholesterol']!=0].reset_index(drop = True) #cholesterol\uc774 0\uc778 \uc790\ub8cc\ub97c \uc81c\uac70\n\n#data2['Cholesterol_a']=(data2['Cholesterol'] == 0).astype('int') #cholesterol\uc774 0\uc778\uc9c0 \uc5ec\ubd80\ub97c \ub098\ud0c0\ub0b4\ub294 \ubc94\uc8fc\n# data3 = data1.copy()\n# data3.loc[data3['Cholesterol'] ==0,'Cholesterol']  = data3['Cholesterol'].median() #cholesterol\uc774 0\uc778\uacbd\uc6b0 \uc911\uc559\uac12\uc73c\ub85c \ub300\uccb4\n# data4 = data3.copy()\n#data4['Cholesterol_a'] = data2['Cholesterol_a'] #\ub450 \ucc98\ub9ac \ubaa8\ub450 \uc801\uc6a9\n\ndatas = [data1, data2, data3]","55f56112":"print(data.shape)\nprint(data1.shape) \nprint(data2.shape) #cholesterol \ubcc0\uc218 \uc81c\uac70 \nprint(data3.shape) #cholesterol \uc774\uc0c1\uce58 \uc81c\uac70","c8d02dc1":"data = data3.copy()","432869aa":"\nobj_cols = []\nnum_cols = []\nfor col in data.columns :\n    if data[col].dtype == 'object' : \n        obj_cols.append(col)\n    else :\n        num_cols.append(col)","ff9a2377":"data[num_cols].boxplot()","b5ebdebd":"def outlier_find(data,num_cols_list):\n    out_lier_indexs = []\n    \n    for num_col in num_cols_list :\n        out_lier_index = []\n        Q1 = np.percentile(data[num_col], 25)\n        Q3 = np.percentile(data[num_col], 75)\n        \n        IQR = Q3-1\n        \n        lower_bound = Q1 - IQR*1.5\n        upper_bound = Q3 + IQR*1.5\n        \n        out_lier_index_list = data[(data[num_col]<lower_bound )| (data[num_col]>upper_bound)].index\n        out_lier_index.extend(out_lier_index_list)\n        out_lier_indexs.append(out_lier_index)\n        \n    o_data = pd.DataFrame(\n        {\n            'num_cols' : num_cols_list,\n            'out_lier_index' : out_lier_indexs\n        }\n    )\n    return(o_data)","f961aa11":"len(data)","599c2485":"o_list = outlier_find(data, num_cols)\nprint(o_list)\nprint(len(o_list.loc[3,'out_lier_index']))","a60cff56":"data.boxplot('Oldpeak')","59c14455":"num_data = data[num_cols]\nfrom sklearn.preprocessing import MinMaxScaler\nminmax = MinMaxScaler()\nnum_data = minmax.fit_transform(data[num_cols])\nnum_data = pd.DataFrame(num_data, columns= num_cols)","53038936":"import seaborn as sns\nsns.heatmap(num_data.corr())","23a041ad":"obj_data = data[obj_cols]\nobj_data.describe()","738ccd9a":"obj_data = pd.get_dummies(data = obj_data, columns=['Sex',\n 'ChestPainType',\n 'RestingECG',\n 'ExerciseAngina',\n 'ST_Slope'])\n    \n","922ee323":"obj_data.columns","a04ad94c":"obj_data = obj_data.drop([\"Sex_F\", \"ChestPainType_NAP\", \"RestingECG_Normal\", \"ExerciseAngina_N\", \"ST_Slope_Flat\"], axis = 1)\nscaled_data = pd.concat([num_data,obj_data],axis = 1)","b23ed408":"from sklearn.preprocessing import LabelEncoder\nobj_data2 =  data[obj_cols]\n\nfor cat_col in obj_cols[:-1] :\n    le = LabelEncoder()\n    obj_data2.loc[:,cat_col] = le.fit_transform(obj_data2[cat_col])\n\n    \nscaled_data2 = pd.concat([num_data,obj_data2],axis = 1)","c20e9d8a":"scaled_data2","db7b6ca1":"scaled_data","e3aff34d":"import seaborn as sns\nsns.heatmap(scaled_data.corr())","f4a47853":"scaled_data.columns","094f6030":"scaled_data = scaled_data.astype('float64')","bce78aa1":"from sklearn.model_selection import train_test_split\n\nx = scaled_data.drop('HeartDisease', axis = 1).copy()\ny = scaled_data['HeartDisease'].copy()\n\ntrain_x, test_x, train_y, test_y = train_test_split(x, y, test_size = 0.2, random_state = 10)","db0941d2":"print(train_x.shape)\nprint(train_y.shape)\nprint(test_x.shape)\nprint(test_y.shape)\n","4a89acb4":"train_x.info()","382c72ee":"from sklearn.linear_model import LogisticRegression\n\nlogit_lasso = LogisticRegression()   \n#logit_lasso = LogisticRegression(penalty='l1', solver='liblinear', C=0.1)   ","7bd84c13":"logit_lasso.fit(train_x, train_y)\nlogit_lasso.score(train_x, train_y)","2ce93cbb":"train_x.columns[(logit_lasso.coef_ != 0)[0]].values","dff9161e":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, roc_auc_score\nimport matplotlib.pyplot as plt\n\ndef plot_roc_curve(fper, tper):\n    plt.plot(fper, tper, color='red', label='ROC')\n    plt.plot([0, 1], [0, 1], color='green', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic Curve')\n    plt.legend()\n    plt.show()\n\ntrain_scores = []\ntest_scores = []\nselected_columns = []\nauc_scores = []\ncoefs = []\nfor data in datas:\n    obj_cols = []\n    num_cols = []\n    for col in data.columns :\n        if data[col].dtype == 'object' : \n            obj_cols.append(col)\n        else :\n            num_cols.append(col)\n\n    num_data = data[num_cols]\n    obj_data = data[obj_cols]\n\n    # \uc815\uaddc\ud654\n    from sklearn.preprocessing import MinMaxScaler\n    minmax = MinMaxScaler()\n    num_data = minmax.fit_transform(data[num_cols])\n    num_data = pd.DataFrame(num_data, columns= num_cols)\n\n    obj_data = pd.get_dummies(data = obj_data, columns=['Sex',\n     'ChestPainType',\n     'RestingECG',\n     'ExerciseAngina',\n     'ST_Slope'])\n\n    obj_data = obj_data.drop([\"Sex_F\", \"ChestPainType_NAP\", \"RestingECG_Normal\", \"ExerciseAngina_N\", \"ST_Slope_Flat\"], axis = 1)\n    scaled_data = pd.concat([num_data,obj_data],axis = 1)\n    scaled_data = scaled_data.astype('float64')\n\n    x = scaled_data.drop('HeartDisease', axis = 1).copy()\n    y = scaled_data['HeartDisease'].copy()\n    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = 0.2, random_state = 10)\n\n    logit_lasso = LogisticRegression(penalty='l1', solver='liblinear', C=0.1)   \n    #logit_lasso = LogisticRegression()\n    \n    logit_lasso.fit(train_x, train_y)\n    train_score = logit_lasso.score(train_x, train_y)\n    test_score = logit_lasso.score(test_x, test_y)\n    pred = logit_lasso.predict_proba(test_x)[:,1]\n    train_scores.append(train_score)\n    test_scores.append(test_score)\n    coefs.append(logit_lasso.coef_)\n    selected_columns.append(train_x.columns[(logit_lasso.coef_ != 0)[0]].values)\n    fper, tper, thresholds = roc_curve(test_y, pred)\n    plot_roc_curve(fper, tper)\n    auc_scores.append(roc_auc_score(test_y, pred))\n\n","80a413bd":"selected_columns","f7cb3d68":"pd.DataFrame({\n    \"datas\":['data1', 'data2', 'data3'],\n    \"train_scores\":train_scores,\n    \"test_scores\":test_scores,\n    \"auc_scores\":auc_scores,\n    \"selected_columns\":selected_columns,\n    \"coef\" : coefs\n})","2225d788":"len(coefs[2][0])","bad83c8a":"#lasso\ub85c \ub3c4\ucd9c\ub41c \uacc4\uc218\npd.DataFrame({\n    \"columns\": selected_columns[2],\n    \"coef\":coefs[2][0][coefs[2][0] != 0]\n}\n)","976cdd92":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\n","be849699":"random_state = 17\nclassifier = [DecisionTreeClassifier(random_state = random_state),\n             SVC(random_state = random_state),\n             RandomForestClassifier(random_state = random_state),\n             LogisticRegression(random_state = random_state),\n             KNeighborsClassifier()]\n\ndt_param_grid = {\"min_samples_split\" : range(10,500,20),\n                \"max_depth\": range(1,20,2)}\n\nsvc_param_grid = {\"kernel\" : [\"rbf\"],\n                 \"gamma\": [0.001, 0.01, 0.1, 1],\n                 \"C\": [1,10,50,100,200,300,1000],\n                  \"probability\":[True]}\n\nrf_param_grid = {\"max_features\": [1,3,10],\n                \"min_samples_split\":[2,3,10],\n                \"min_samples_leaf\":[1,3,10],\n                \"bootstrap\":[False],\n                \"n_estimators\":[100,300],\n                \"criterion\":[\"gini\"]}\n\nlogreg_param_grid = {\"C\":np.logspace(-3,3,7),\n                    \"penalty\": [\"l1\"],\n                     \"solver\" : [\"liblinear\"]\n                    }\n\nknn_param_grid = {\"n_neighbors\": np.linspace(1,19,10, dtype = int).tolist(),\n                 \"weights\": [\"uniform\",\"distance\"],\n                 \"metric\":[\"euclidean\",\"manhattan\"]}\nclassifier_param = [dt_param_grid,\n                   svc_param_grid,\n                   rf_param_grid,\n                   logreg_param_grid,\n                   knn_param_grid]","c4fcced5":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom tqdm import tqdm\ncv_result = []\nbest_params = []\nbest_estimators = []\nfor i in tqdm(range(len(classifier))):\n    clf = GridSearchCV(classifier[i], param_grid=classifier_param[i], cv = StratifiedKFold(n_splits = 10), scoring = \"accuracy\", n_jobs = -1,verbose = 1)\n    clf.fit(train_x,train_y)\n    cv_result.append(clf.best_score_)\n    best_params.append(clf.best_params_)\n    best_estimators.append(clf.best_estimator_)\n    print((classifier[i]), cv_result[i])","b1a476c4":"best_params","2e39a12a":"cv_result = pd.DataFrame(\n    {\n        'cv_result' : cv_result,\n        'best_params' : best_params,\n        'best_estimators': best_estimators\n    }\n    \n)","8ff45635":"cv_result","836ea592":"classifier","4f2d15b7":"dt_model = best_estimators[0]\nsvc_model = best_estimators[1]\nrf_model = best_estimators[2]\nLogit_model = best_estimators[3]\nknn_model = best_estimators[4]","6f86e145":"dt_model.fit(train_x, train_y)\nsvc_model.fit(train_x, train_y)\nrf_model.fit(train_x, train_y)\nLogit_model.fit(train_x, train_y)\nknn_model.fit(train_x, train_y)","6ee03c83":"predicts = [\n    dt_model.predict(test_x),\n    svc_model.predict(test_x),\n    rf_model.predict(test_x),\n    Logit_model.predict(test_x),\n    knn_model.predict(test_x),\n]\npred_probas = [\n    pd.DataFrame(dt_model.predict_proba(test_x)).iloc[:,1],\n    pd.DataFrame(svc_model.predict_proba(test_x)).iloc[:,1],\n    pd.DataFrame(rf_model.predict_proba(test_x)).iloc[:,1],\n    pd.DataFrame(Logit_model.predict_proba(test_x)).iloc[:,1],\n    pd.DataFrame(knn_model.predict_proba(test_x)).iloc[:,1],\n]","bf1dbd62":"train_scores = [\n    dt_model.score(train_x, train_y),\n    svc_model.score(train_x, train_y),\n    rf_model.score(train_x, train_y),\n    Logit_model.score(train_x, train_y),\n    knn_model.score(train_x, train_y),\n]","8cab0c0d":"test_scores = [\n    dt_model.score(test_x, test_y),\n    svc_model.score(test_x, test_y),\n    rf_model.score(test_x, test_y),\n    Logit_model.score(test_x, test_y),\n    knn_model.score(test_x, test_y),\n]","94bcaacd":"from sklearn.metrics import roc_auc_score\n\nauc_scores = [roc_auc_score(test_y, pred) for pred in pred_probas]\n\nmodel_result = pd.DataFrame(\n    {\n        'best_estimators' : best_estimators,\n        'train_scores' : train_scores,\n        'test_scores' : test_scores,\n        'auc_scores' : auc_scores\n    }\n)","a6f63705":"model_result","3e3b0bb0":"!pip install lofo-importance","4c7c9fd4":"from sklearn.model_selection import KFold\nfrom lofo import LOFOImportance, Dataset, plot_importance\n\ndef get_lofo_importance(data_x, data_y):\n    cv = KFold(n_splits=5, shuffle=True, random_state=0)\n    dataset =  Dataset(pd.concat([data_x, data_y],axis = 1), target='HeartDisease',features=data_x.columns )\n    \n\n    \n    lofo_impos = []\n    for model in best_estimators:\n        mod = model\n        \n        \n        \n        lofo_imp = LOFOImportance(dataset, cv=cv, scoring=\"neg_mean_absolute_error\", model=mod)\n    \n        lofo_impos.append(lofo_imp.get_importance())\n    return lofo_impos\nscaled_data.columns\nlofo = get_lofo_importance(train_x, train_y)\npd.DataFrame({\n    'model' :best_estimators,\n    'lofo_impos': lofo\n})","9709ec9d":"print(lofo[1])\nprint(best_estimators[1])\n\nplot_importance(lofo[1], figsize=(16, 8))","7a15835b":"best_estimators[1]","1737aef1":"\nSVC_features= lofo[1]['feature'][lofo[1]['importance_mean']>0]\n\nsvc_model = SVC(C=200, gamma=0.01, probability=True, random_state=17)\nsvc_train = train_x[SVC_features]\nsvc_model.fit(svc_train, train_y)\n\ntrain_score = svc_model.score(svc_train, train_y)\ntest_score = svc_model.score(test_x[SVC_features], test_y)\npred = svc_model.predict_proba(test_x[SVC_features])[:,1]\n\nfper, tper, thresholds = roc_curve(test_y, pred)\nplot_roc_curve(fper, tper)\n\nauc_score = roc_auc_score(test_y, pred)\n","37679071":"pd.DataFrame({\n    \"train_score\":[train_score],\n    \"test_score\":[test_score],\n    \"auc_score\":[auc_score]\n})","93cb3269":"scaled_data2=scaled_data2.astype('float64')","a946c1b4":"from sklearn.model_selection import train_test_split\n\nx = scaled_data2.drop('HeartDisease', axis = 1).copy()\ny = scaled_data2['HeartDisease'].copy()\n\ntrain_x, test_x, train_y, test_y = train_test_split(x, y, test_size = 0.2, random_state = 10)","9b9feb26":"x_train, x_val, y_train, y_val = train_test_split(train_x, train_y, test_size = 0.2, random_state = 10, stratify = train_y)","b7469934":"import lightgbm as lgb","6938b989":"cate_feature = [str(col) for col in obj_cols[:-1] ]","fa662fcd":"cate_feature","566cff3b":"import optuna\nfrom sklearn.metrics import log_loss","9f84126d":"def objective_function(trial):\n    params = {\n        'objective' : 'binary',\n        'max_bin' : trial.suggest_int('max_bin', 255,300),\n        'learning-rate' : 0.05,\n        'num_leaves' : trial.suggest_int('num_leaves', 32,356),\n        'verbose' : -1\n    }\n    \n    lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cate_feature)\n    lgb_val = lgb.Dataset(x_val, y_val, reference = lgb_train, categorical_feature = cate_feature)\n    \n    model = lgb.train(params, lgb_train, valid_sets = [lgb_train, lgb_val],\n                      verbose_eval = 0,\n                      num_boost_round = 1000,\n                      early_stopping_rounds = 20\n\n                     )\n\n    test_prediction_prob = model.predict(test_x, numiteration = model.best_iteration)\n    score = log_loss(test_y, test_prediction_prob)\n    \n    return(score)","2f2538d6":"study = optuna.create_study(sampler = optuna.samplers.RandomSampler(seed = 10))\nstudy.optimize(objective_function, n_trials = 50)\nstudy.best_params\n\n","a79fe29a":"lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cate_feature)\nlgb_val = lgb.Dataset(x_val, y_val, reference = lgb_train, categorical_feature = cate_feature)","1e076c53":"params = {\n        'objective' : 'binary',\n        'max_bin' : study.best_params['max_bin'],\n        'learning-rate' : 0.05,\n        'num_leaves' : study.best_params['num_leaves'],\n        'verbose' : -1\n}\n\nmodel = lgb.train(params, lgb_train, valid_sets = [lgb_train, lgb_val],\n                  verbose_eval = 10,\n                  num_boost_round = 500,\n                  early_stopping_rounds = 10\n                 \n                 )\ntrain_prediction_prob = model.predict(train_x, numiteration = model.best_iteration)\ntest_prediction_prob = model.predict(test_x, numiteration = model.best_iteration)","3ef66785":"import matplotlib.pyplot as plt\nplt.plot(model.feature_importance())","16da080e":"test_prediction = (test_prediction_prob > 0.5).astype('int')\ntrain_prediction = (train_prediction_prob > 0.5).astype('int')","edd938a2":"test_prediction","ea757ceb":"from sklearn.metrics import classification_report\nprint(classification_report(train_prediction, train_y))\nprint(classification_report(test_prediction, test_y))","c4a1d050":"from sklearn.metrics import accuracy_score\n\nprint(accuracy_score(train_y,train_prediction))\nprint(accuracy_score(test_y,test_prediction))","79f73c5d":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(test_prediction, test_y)","ecc2d57f":"from sklearn.metrics import roc_curve, roc_auc_score\nimport matplotlib.pyplot as plt\n\ndef plot_roc_curve(fper, tper):\n    plt.plot(fper, tper, color='red', label='ROC')\n    plt.plot([0, 1], [0, 1], color='green', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic Curve')\n    plt.legend()\n    plt.show()","0d2b6c07":"fper, tper, thresholds = roc_curve(test_y, test_prediction_prob)\nplot_roc_curve(fper, tper)","d36b2a3a":"roc_auc_score(test_y, test_prediction_prob)","bba7d388":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom tqdm import tqdm\nfrom sklearn.metrics import roc_curve, roc_auc_score\nimport lightgbm as lgb\nimport optuna\nfrom sklearn.metrics import log_loss\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import accuracy_score\nimport os\n\ndef find_best_model(data):\n    \n    obj_cols = []\n    num_cols = []\n    for col in data.columns :\n        if data[col].dtype == 'object' : \n            obj_cols.append(col)\n        else :\n            num_cols.append(col)\n    # min-max scaling\n#     minmax = MinMaxScaler()\n#     scaled_num_data = minmax.fit_transform(data[num_cols])\n#     scaled_num_data = pd.DataFrame(scaled_num_data, columns= num_cols)\n\n\n    # categorical cols incoding (dummy)\n    \n    num_data = data[num_cols]\n    obj_data = data[obj_cols]\n    \n    # \uc815\uaddc\ud654\n    from sklearn.preprocessing import MinMaxScaler\n    minmax = MinMaxScaler()\n    num_data = minmax.fit_transform(data[num_cols])\n    num_data = pd.DataFrame(num_data, columns= num_cols)\n    \n    obj_data = pd.get_dummies(data = obj_data, columns=['Sex',\n     'ChestPainType',\n     'RestingECG',\n     'ExerciseAngina',\n     'ST_Slope'])\n    obj_data = obj_data.drop([\"Sex_F\", \"ChestPainType_NAP\", \"RestingECG_Normal\", \"ExerciseAngina_N\", \"ST_Slope_Flat\"], axis = 1)\n    scaled_data = pd.concat([num_data,obj_data],axis = 1)\n    \n\n    # categorical cols incoding (label)\n    obj_data2 =  data[obj_cols]\n\n    for cat_col in obj_cols[:-1] :\n        le = LabelEncoder()\n        obj_data2.loc[:,cat_col] = le.fit_transform(obj_data2[cat_col])\n\n    scaled_data2 = pd.concat([num_data,obj_data2],axis = 1)\n\n    scaled_data = scaled_data.astype('float64')\n    scaled_data2 = scaled_data2.astype('float64')\n    \n    \n    # decision tree, svc, randomforest, logistic, knn\n    \n    # train, test split\n    x = scaled_data.drop('HeartDisease', axis = 1).copy()\n    y = scaled_data['HeartDisease'].copy()\n    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = 0.2, random_state = 10)\n\n\n    cv_result = []\n    best_params = []\n    best_estimators = []\n    for i in tqdm(range(len(classifier))):\n        clf = GridSearchCV(classifier[i], param_grid=classifier_param[i], cv = StratifiedKFold(n_splits = 10), scoring = \"accuracy\", n_jobs = -1,verbose = 1)\n        clf.fit(train_x,train_y)\n        cv_result.append(clf.best_score_)\n        best_params.append(clf.best_params_)\n        best_estimators.append(clf.best_estimator_)\n        print((classifier[i]), cv_result[i])\n\n\n#    cv_results = pd.DataFrame(\n#        {\n#            'cv_result' : cv_result,\n#            'best_params' : best_params,\n#            'best_estimators': best_estimators\n#        }    \n#    )\n\n    dt_model = best_estimators[0]\n    svc_model = best_estimators[1]\n    rf_model = best_estimators[2]\n    Logit_model = best_estimators[3]\n    knn_model = best_estimators[4]\n\n    dt_model.fit(train_x, train_y)\n    svc_model.fit(train_x, train_y)\n    rf_model.fit(train_x, train_y)\n    Logit_model.fit(train_x, train_y)\n    knn_model.fit(train_x, train_y)\n\n    predicts = [\n        dt_model.predict(test_x),\n        svc_model.predict(test_x),\n        rf_model.predict(test_x),\n        Logit_model.predict(test_x),\n        knn_model.predict(test_x),\n    ]\n\n    train_scores = [\n        dt_model.score(train_x, train_y),\n        svc_model.score(train_x, train_y),\n        rf_model.score(train_x, train_y),\n        Logit_model.score(train_x, train_y),\n        knn_model.score(train_x, train_y),\n    ]\n\n    test_scores = [\n        dt_model.score(test_x, test_y),\n        svc_model.score(test_x, test_y),\n        rf_model.score(test_x, test_y),\n        Logit_model.score(test_x, test_y),\n        knn_model.score(test_x, test_y),\n    ]\n\n    pred_probas = [\n        pd.DataFrame(dt_model.predict_proba(test_x)).iloc[:,1],\n        pd.DataFrame(svc_model.predict_proba(test_x)).iloc[:,1],\n        pd.DataFrame(rf_model.predict_proba(test_x)).iloc[:,1],\n        pd.DataFrame(Logit_model.predict_proba(test_x)).iloc[:,1],\n        pd.DataFrame(knn_model.predict_proba(test_x)).iloc[:,1],\n    ]\n    \n    \n    auc_scores = [roc_auc_score(test_y, pred) for pred in pred_probas]\n\n\n\n    \n    # LGBM\n    \n    x = scaled_data2.drop('HeartDisease', axis = 1).copy()\n    y = scaled_data2['HeartDisease'].copy()\n\n    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = 0.2, random_state = 10)\n    x_train, x_val, y_train, y_val = train_test_split(train_x, train_y, test_size = 0.2, random_state = 10, stratify = train_y)\n    \n    cate_feature = [str(col) for col in obj_cols[:-1] ]\n    study = optuna.create_study(sampler = optuna.samplers.RandomSampler(seed = 10))\n    study.optimize(objective_function, n_trials = 50)\n    os.system('cls')\n    lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cate_feature)\n    lgb_val = lgb.Dataset(x_val, y_val, reference = lgb_train, categorical_feature = cate_feature)\n    params = {\n            'objective' : 'binary',\n            'max_bin' : study.best_params['max_bin'],\n            'learning-rate' : 0.05,\n            'num_leaves' : study.best_params['num_leaves'],\n            'verbose' : -1\n    }\n\n    model = lgb.train(params, lgb_train, valid_sets = [lgb_train, lgb_val],\n                      verbose_eval = 10,\n                      num_boost_round = 500,\n                      early_stopping_rounds = 10\n\n                     )\n\n    test_prediction_prob = model.predict(test_x, numiteration = model.best_iteration)\n    test_prediction = (test_prediction_prob > 0.5).astype('int')\n    train_prediction_prob = model.predict(train_x, numiteration = model.best_iteration)\n    train_prediction = (train_prediction_prob > 0.5).astype('int')\n\n\n\n    fper, tper, thresholds = roc_curve(test_y, test_prediction_prob)\n    plot_roc_curve(fper, tper)\n    \n    \n    best_estimators.append(\"LGBM\")\n    train_scores.append(accuracy_score(train_y,train_prediction))\n    test_scores.append(accuracy_score(test_y,test_prediction))\n    auc_scores.append(roc_auc_score(test_y, test_prediction_prob))\n\n    \n    \n\n    model_result = pd.DataFrame(\n        {\n            'best_estimators' : best_estimators,\n            'train_scores' : train_scores,\n            'test_scores' : test_scores,\n            'auc_scores' : auc_scores\n        }\n    )\n        \n    print(model_result)\n    print(confusion_matrix(test_prediction, test_y))\n    print(classification_report(test_prediction, test_y))\n        \n    \n\n    return(model_result)","db575b5d":"#data1_result = find_best_model(data1)","73363bb1":"#data2_result = find_best_model(data2)","6a3ee86f":"data3_result = find_best_model(data3)","0d04b6f8":"#data1_result","a6da7878":"#data2_result #cholesterol \ubcc0\uc218 \uc81c\uac70","ea93cab4":"data3_result #cholesterol \uacb0\uce21\uce58 \uc81c\uac70","de70a8ce":"4. \ub85c\uc9c0\uc2a4\ud2f1 GLM","27365e6a":"data 1, 2, 3 \uc5d0 \ub300\ud574 \ubc18\ubcf5","067d7062":"4. \uae30\ud0c0 \uc5ec\ub7ec \ubaa8\ud615\ub4e4 \ucd5c\uc801\ud569 \uacb0\uacfc \ube44\uad50","21ac3ba0":"\ucf5c\ub808\uc2a4\ud14c\ub864 \ubcc0\ud658 \ud6c4 \uacb0\uacfc \ube44\uad50","dc295916":"Light GBM","ec3fa2ba":"\uadf8\ub798\ud504\ub294 tebleau","8210527f":"1.Age: age of the patient [years]\n\n2.Sex: sex of the patient [M: Male, F: Female]\n\n3.ChestPainType: chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\n\n4.RestingBP: resting blood pressure [mm Hg]\n    (\uc548\uc815 \ud608\uc555)\n\n5.Cholesterol: serum cholesterol [mm\/dl]\n    (0 is impossible)\n\n6.FastingBS: fasting blood sugar [1: if FastingBS > 120 mg\/dl, 0: otherwise]\n    (\uacf5\ubcf5 \ud608\ub2f9)\n\n7.RestingECG: resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite     left ventricular hypertrophy by Estes' criteria]\n    (\uc2ec\uc804\ub3c4)\n\n8.MaxHR: maximum heart rate achieved [Numeric value between 60 and 202]\n    (\ucd5c\ub300 \ud608\uc555)\n\n9.ExerciseAngina: exercise-induced angina [Y: Yes, N: No]\n    (\uc6b4\ub3d9\uc73c\ub85c \uc720\ubc1c\ub41c \ud611\uc2ec\uc99d \uc720\ubb34)\n\n10.Oldpeak: oldpeak = ST [Numeric value measured in depression]\n    ()\n\n11.ST_Slope: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n\n12.HeartDisease: output class [1: heart disease, 0: Normal]","400f85e4":"\n2. \uc790\ub8cc \uc124\uba85","365c3577":"3. \uc804\ucc98\ub9ac \uacfc\uc815","05b1e021":"1. \uc8fc\uc81c\uc18c\uac1c\n","2fd1a7fc":"min-max \uc815\uaddc\ud654","c630faaa":"LOFO (Leave One Feature Out Importance)"}}