{"cell_type":{"7525667a":"code","d0db502c":"code","92019f66":"code","f303862c":"code","84f47a29":"code","1f9107fd":"code","25a5eba7":"code","ea72c548":"code","2388755e":"code","da2f8536":"code","bef65ff9":"code","071706cf":"code","59c94643":"code","e0e16377":"code","9d23b661":"code","ca17923f":"code","83bd6807":"code","ec91b5f4":"code","9bb496b8":"code","ba347c46":"code","c6c84285":"code","474e0b4f":"code","bc0bfba8":"code","6afdd79a":"code","da2a2a60":"code","2714506c":"code","44a46f97":"code","1ef4f515":"code","620861c7":"code","599639f3":"code","535d6045":"code","1242b1d9":"code","31bf4bfe":"code","fb3d3ead":"code","98b4472e":"code","049a4bfd":"code","8200e3f7":"code","df4c8566":"code","3d0a6a79":"code","dd977666":"code","27d48b11":"code","f918e24d":"code","98943523":"code","d380c116":"code","06680d52":"code","d20cae72":"code","92f43217":"code","40ca69a9":"code","758ea057":"code","b23f4c4a":"code","35819f81":"code","8c3bb025":"code","6ca91808":"code","d110efb8":"code","f5efe2d0":"code","52a081a4":"code","6fa2b187":"code","7c6108a1":"code","e070da0f":"code","dc75112c":"code","f1abb456":"code","10463e2b":"code","8b8802ef":"code","bc4ec823":"code","f91afa67":"code","12b33762":"code","de67ee82":"code","b9a9a481":"code","0079a4f5":"code","d0c46f8a":"code","6f995c07":"code","60975c9e":"code","74bb5663":"code","eecf1254":"code","55410b7a":"code","e41ad9f9":"code","9b252a82":"code","b1a286a0":"code","6d7d1368":"code","3eabff23":"code","82955455":"code","08fa6143":"code","fb70975a":"code","15dba12c":"code","d99fc905":"markdown","a74fb3ca":"markdown","6b60caa9":"markdown","33ed3226":"markdown","7db03a70":"markdown","7bf59ab3":"markdown","835b2e45":"markdown","3107d14a":"markdown","8055cf21":"markdown","487c85ba":"markdown","9d7d1b0f":"markdown","fee60c1f":"markdown","86f71897":"markdown","2b6475d6":"markdown","be43d84d":"markdown","8d4dd690":"markdown","83c4b7fb":"markdown","80a1fe8b":"markdown","a62c07df":"markdown"},"source":{"7525667a":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom tpot import TPOTClassifier","d0db502c":"train_df = pd.read_csv('..\/input\/titanic\/train.csv') \ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\n\nprint('train_df.shape:', train_df.shape)\nprint('test_df.shape:', test_df.shape)","92019f66":"# combine train, test data\ntrain_test_data = [train_df, test_df]","f303862c":"print(train_df.columns.to_list())\nprint(test_df.columns.to_list())","84f47a29":"train_df.info()","1f9107fd":"train_df.describe(include='object')","25a5eba7":"test_df.info()","ea72c548":"test_df.describe(include='object')","2388755e":"# null field\ntrain_null_s = train_df.isnull().sum()\nprint(train_null_s[train_null_s != 0])\nprint('-'*80)\ntest_null_s = test_df.isnull().sum()\nprint(test_null_s[test_null_s != 0])","da2f8536":"# detect target outlier index\noutlier_detection_field = ['Age', 'Fare']\nweight = 2\n\noutlier_indices = []\n\nfor col in outlier_detection_field:\n    q1 = np.nanpercentile(train_df[col], 25)\n    q3 = np.nanpercentile(train_df[col], 75)\n    iqr = q3-q1\n    iqr_weight = iqr * weight\n\n    lowest_val = q1 - iqr_weight\n    highest_val = q3 + iqr_weight\n\n    outlier_index = train_df[(train_df[col]<lowest_val) | (highest_val<train_df[col])].index\n    outlier_indices.extend(outlier_index)\n    \n    print('{}: {} \/ {} (record size:{})'.format(col, lowest_val, highest_val, outlier_index.shape[0]))","bef65ff9":"# drop outlier index\ntrain_df.drop(outlier_indices, axis=0, inplace=True)","071706cf":"train_df['PassengerId']","59c94643":"test_df['PassengerId']","e0e16377":"# drop 'passengerId' field\n# for test data, save the 'PassengerId' field for submission\n\ntrain_df.drop('PassengerId', axis=1, inplace=True)\n\ntest_df_PId = test_df['PassengerId']\ntest_df.drop('PassengerId', axis=1, inplace=True)","9d23b661":"print(train_df.columns.to_list())\nprint(test_df.columns.to_list())","ca17923f":"print(train_df['Pclass'].value_counts())\n\nsns.barplot(data=train_df, x='Pclass', y='Survived')","83bd6807":"# scale\nminMaxScaler = MinMaxScaler()\n\nfor data in train_test_data:\n    data['Pclass'] = minMaxScaler.fit_transform(data[['Pclass']])","ec91b5f4":"train_df['Pclass'].value_counts()","9bb496b8":"train_df['Name'].head(10)","ba347c46":"# get 'title' field from 'name' field\n\n# train_df['Name'].str.extract(' ([a-zA-Z]+)\\. ', expand=False).value_counts()\nfor data in train_test_data:\n    data['Title'] = data['Name'].str.extract(' ([a-zA-Z]+)\\. ', expand=False)","c6c84285":"# drop 'name' field\n\nfor data in train_test_data:\n    data.drop('Name', axis=1, inplace=True)","474e0b4f":"print(train_df['Title'].value_counts())\nprint('-'*50)\nprint(test_df['Title'].value_counts())","bc0bfba8":"# encode\n\ntitle_mapping = {\n    'Mr':0,\n    'Miss':1,\n    'Mrs':2,\n    'Master':3,\n    'Dr':4, 'Rev':4, 'Major':4, 'Mlle':4, 'Col':4, 'Ms':4, 'Countess':4, 'Mme':4, 'Lady':4, 'Sir':4, 'Don':4, 'Jonkheer':4, 'Capt':4, 'Dona':4\n}\n\nfor data in train_test_data:\n    data['Title'] = data['Title'].map(title_mapping)","6afdd79a":"train_df['Title'].value_counts()","da2a2a60":"# scale\n\nminMaxScaler = MinMaxScaler()\n\nfor data in train_test_data:\n    data['Title'] = minMaxScaler.fit_transform(data[['Title']])","2714506c":"train_df['Title'].value_counts()","44a46f97":"sns.barplot(data=train_df, x='Title', y='Survived')","1ef4f515":"print(train_df['Sex'].value_counts())\n\nsns.barplot(data=train_df, x='Sex', y='Survived')","620861c7":"# encode\n\nfor data in train_test_data:\n    data['Sex'] = data['Sex'].astype('category').cat.codes","599639f3":"train_df['Sex'].value_counts()","535d6045":"train_df['Age'].isnull().sum()","1242b1d9":"# fill null with the middle value of the title\n# train_df.groupby('Title')['Age'].transform('median')\n\nfor data in train_test_data:\n    data['Age'].fillna(train_df.groupby('Title')['Age'].transform('median'), inplace=True)","31bf4bfe":"train_df['Age'].isnull().sum()","fb3d3ead":"sns.distplot(train_df['Age'])","98b4472e":"# binning\n# pd.qcut(train_df['Age'], 5).cat.codes\n\nfor data in train_test_data:\n    data['Age'] = pd.qcut(data['Age'], 9).cat.codes","049a4bfd":"# scale\n\nminMaxScaler = MinMaxScaler()\n\nfor data in train_test_data:\n    data['Age'] = minMaxScaler.fit_transform(data[['Age']])","8200e3f7":"train_df['Age'].describe()","df4c8566":"print(train_df['SibSp'].value_counts())\nsns.barplot(data=train_df, x='SibSp', y='Survived')","3d0a6a79":"print(train_df['Parch'].value_counts())\nsns.barplot(data=train_df, x='Parch', y='Survived')","dd977666":"for data in train_test_data:\n    data['FamilySize'] = data['Parch'] + data['SibSp']","27d48b11":"sns.barplot(data=train_df, x='FamilySize', y='Survived')","f918e24d":"# drop\nfor data in train_test_data:\n    data.drop(['SibSp', 'Parch'], axis=1, inplace=True)","98943523":"# binning\n# train_df.loc[(1<=train_df['FamilySize']) & (train_df['FamilySize']<4), 'FamilySize'].value_counts()\n\n# for data in train_test_data:\n#     data.loc[data['FamilySize']==0, 'FamilySize'] = 0\n#     data.loc[(1<=data['FamilySize']) & (data['FamilySize']<4), 'FamilySize'] = 1\n#     data.loc[(4<=data['FamilySize']) & (data['FamilySize']<7), 'FamilySize'] = 2\n#     data.loc[(7<=data['FamilySize']), 'FamilySize'] = 3","d380c116":"# scale\nminMaxScaler = MinMaxScaler()\n\nfor data in train_test_data:\n    data['FamilySize'] = minMaxScaler.fit_transform(data[['FamilySize']])","06680d52":"sns.barplot(data=train_df, x='FamilySize', y='Survived')","d20cae72":"print(train_df['Ticket'].value_counts())\nprint('-'*80)\nprint(train_df['Ticket'].unique().shape)","92f43217":"# drop 'Ticket' field\nfor data in train_test_data:\n    data.drop('Ticket', axis=1, inplace=True)","40ca69a9":"print(train_df['Fare'].isnull().sum())\nprint(test_df['Fare'].isnull().sum())","758ea057":"train_df.groupby(['Embarked', 'Pclass'])['Fare'].median()","b23f4c4a":"# fill null with the middle value of the 'Embarked', 'Pclass'\n\nfor data in train_test_data:\n    data['Fare'].fillna(train_df.groupby(['Embarked', 'Pclass'])['Fare'].transform('median'), inplace=True)","35819f81":"test_df['Fare'].isnull().sum()","8c3bb025":"sns.distplot(train_df['Fare'])","6ca91808":"# log transformation to import skewed data\nfig = plt.figure(figsize=(14, 7))\nax1 = fig.add_subplot(2, 1, 1)\nax2 = fig.add_subplot(2, 1, 2)\n\nsns.distplot(train_df['Fare'], ax=ax1)\nsns.distplot(np.log1p(train_df['Fare']), ax=ax2)\n\nfor data in train_test_data:\n    data['Fare'] = np.log1p(data[['Fare']])","d110efb8":"# binning\n# pd.qcut(train_df['Fare'], 5).astype('category').cat.codes.value_counts()\n\nfor data in train_test_data:\n    data['Fare'] = pd.qcut(data['Fare'], 10).astype('category').cat.codes","f5efe2d0":"train_df['Fare'].value_counts()","52a081a4":"# scale\nminMaxScaler = MinMaxScaler()\n\nfor data in train_test_data:\n    data['Fare'] = minMaxScaler.fit_transform(data[['Fare']])","6fa2b187":"train_df['Fare'].value_counts()","7c6108a1":"print(train_df['Cabin'].value_counts())\nprint('-'*80)\nprint(train_df['Cabin'].unique().shape)\nprint('-'*80)\nprint(train_df['Cabin'].str[:1].value_counts())","e070da0f":"print(test_df['Cabin'].str[:1].value_counts())","dc75112c":"# replace 'Cabin' field to the first character of the field\nfor data in train_test_data:\n    data['Cabin'] = data['Cabin'].str[:1]","f1abb456":"sns.barplot(data=train_df, x='Cabin', y='Survived')","10463e2b":"# encode\ncabin_mapping={\"A\":1, \"B\":2, \"C\":3, \"D\":4, \"E\":5, \"F\":6, \"G\":7, \"T\":8}\n\nfor data in train_test_data:\n    data['Cabin'] = data['Cabin'].map(cabin_mapping)","8b8802ef":"print(train_df['Cabin'].value_counts())\nprint('-'*80)\n# print(train_df.groupby(['Pclass', 'Embarked'])['Cabin'].median())\nprint(train_df.groupby(['Pclass'])['Cabin'].median())","bc4ec823":"# fill null with the middle value of the 'Pclass'\nfor data in train_test_data:\n    data['Cabin'].fillna(data.groupby(['Pclass'])['Cabin'].transform('median'), inplace=True)","f91afa67":"print(train_df['Cabin'].isnull().sum())\nprint('-'*80)\nprint(train_df['Cabin'].value_counts())","12b33762":"# scale\nminMaxScaler = MinMaxScaler()\n\nfor data in train_test_data:\n    data['Cabin'] = minMaxScaler.fit_transform(data[['Cabin']])","de67ee82":"print(train_df['Embarked'].isnull().sum())\nprint(test_df['Embarked'].isnull().sum())","b9a9a481":"print(train_df['Embarked'].value_counts())\nsns.barplot(data=train_df, x='Embarked', y='Survived')","0079a4f5":"# there aren't many missing values(just 2 records in train data), so fill null to most value\nfor data in train_test_data:\n    data['Embarked'] = data['Embarked'].fillna('S')","d0c46f8a":"# encode\n\nfor data in train_test_data:\n    data['Embarked'] = data['Embarked'].astype('category').cat.codes","6f995c07":"# scale\nminMaxScaler = MinMaxScaler()\n\nfor data in train_test_data:\n    data['Embarked'] = minMaxScaler.fit_transform(data[['Embarked']])","60975c9e":"train_df.head()","74bb5663":"y_train_s = train_df['Survived']\nx_train_df = train_df.drop('Survived', axis=1)","eecf1254":"x_train, x_test, y_train, y_test = train_test_split(x_train_df, y_train_s, test_size=0.2, random_state=10)","55410b7a":"def cross_val_score_result(estimator, x, y, scoring, cv):\n    clf_scores = cross_val_score(estimator, x, y, scoring=scoring, cv=cv)\n    clf_scores_mean = np.round(np.mean(clf_scores), 4)\n    \n    return clf_scores_mean","e41ad9f9":"classifiers = [\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    GradientBoostingClassifier(),\n    KNeighborsClassifier(),\n    SVC(),\n    LGBMClassifier(),\n    XGBClassifier(), \n    AdaBoostClassifier()\n#     TPOTClassifier()\n]","9b252a82":"best_clf_score = 0\nbest_clf = None\n\nclf_name = []\nclf_mean_score = []\n\nfor clf in classifiers:\n    current_clf_score = cross_val_score_result(clf, x_train, y_train, 'accuracy', 10)\n    clf_name.append(clf.__class__.__name__)\n    clf_mean_score.append(current_clf_score)\n    \n    if current_clf_score > best_clf_score:\n        best_clf_score = current_clf_score\n        best_clf = clf","b1a286a0":"clf_df = pd.DataFrame({\"clf_name\":clf_name, \"clf_mean_score\":clf_mean_score})\nplt.figure(figsize=(8, 6))\nsns.barplot(data=clf_df, x=\"clf_mean_score\", y=\"clf_name\")\n\nprint('best classifier: {}({})'.format(best_clf.__class__.__name__, best_clf_score))","6d7d1368":"# train the classifier get the highest score\nlgbm_clf = LGBMClassifier()\n\ngrid_param = {\n    'learning_rate':[0.005, 0.01, 0.015, 0.02],\n    'n_estimators':[100, 150, 200],\n    'bossting_type':['rf', 'gbdt', 'dart', 'goss'],\n    'max_depth':[10, 15, 20]\n}\n\nlgbm_grid = GridSearchCV(lgbm_clf, grid_param, cv=10)\nlgbm_grid.fit(x_train, y_train)","3eabff23":"print('best_param:', lgbm_grid.best_params_)\nprint('best_score:{:.4f}'.format(lgbm_grid.best_score_))","82955455":"test_df.head()","08fa6143":"test_pred = lgbm_grid.best_estimator_.predict(test_df)\n\nsubmission = pd.DataFrame({\n    'PassengerId': test_df_PId,\n    'Survived': test_pred\n})","fb70975a":"submission.to_csv('submission_test.csv', index=False)","15dba12c":"check_submission = pd.read_csv('submission_test.csv')\ncheck_submission","d99fc905":"# index\n---\n1. [load libraries and datasets](#load)\n2. [glimpse dataset](#glimpse)\n3. [outlier detection](#outlier)\n4. [preprocess data](#preprocess)\n5. [train](#train)\n6. [predict](#predict)","a74fb3ca":"### <a id='Age'>`Age` field<\/a>\n  - missing value exists\n  - binning\n  - MinMaxScale","6b60caa9":"### <a id='Ticket'>`Ticket` field<\/a>\n  - drop","33ed3226":"### <a id='Name'>`Name` field<\/a>\n  - get 'Title' field from 'Name' field\n  - encode\n  - MinMaxScale","7db03a70":"# <a id=\"train\">5. train<\/a>\n  - [classifier cross_val_score](#cross_val_score)\n  - [hyperparameter tuning](#tuning)","7bf59ab3":"# <a id=\"preprocess\">4. preprocess data<\/a>\n  - [`PassengerId`](#PassengerId)\n  - [`Pclass`](#Pclass)\n  - [`Name`](#Name)\n  - [`Sex`](#Sex)\n  - [`Age`(missing value exists)](#Age)\n  - [`SibSp`, `Parch`](#SibSp)\n  - [`Ticket`](#Ticket)\n  - [`Fare`(missing value exists)](#Fare)\n  - [`Cabin`(missing value exists)](#Cabin)\n  - [`Embarked`(missing value exists)](#Embarked)","835b2e45":"# <a id=\"outlier\">3. outlier detection<\/a>\n  - **target field**: `Age`, `Fare`\n  - exclude detection field\n    - object type: `Name`, `Sex`, `Ticket`, `Cabin`, `Embarked`\n    - simple index field: `PassengerId`\n    - well-refined field: `Pclass`, `SibSp`, `Parch`\n  - drop outlier record","3107d14a":"### <a id='Pclass'>`Pclass` field<\/a>\n  - MinMaxScale","8055cf21":"### <a id='Sex'>`Sex` field<\/a>\n  - encode","487c85ba":"### <a id=\"PassengerId\">`PassengerId` field<\/a>\n  - drop","9d7d1b0f":"### <a id='SibSp'>`SibSp`, `Parch` field<\/a>\n  - get 'FamilySize' from 'SibSp', 'Parch' field\n  - drop 'SibSp', 'Parch' field\n  - ~~binning~~\n  - MinMaxScale","fee60c1f":"### <a id='cross_val_score'>classifier cross_val_score<\/a>","86f71897":"### <a id='Fare'>`Fare` field<\/a>\n  - missing value exists\n  - log transformation to improve skewed data\n  - binning\n  - MinMaxScale","2b6475d6":"### <a id=\"tuning\">classifier hyperparameter tuning<\/a>","be43d84d":"### <a id='Embarked'>`Embarked` field<\/a>\n  - missing value exists\n  - encode\n  - MinMaxScale","8d4dd690":"# <a id='load'>1. load libraries and datasets<\/a>","83c4b7fb":"# <a id=\"predict\">6. predict<\/a>","80a1fe8b":"# <a id='glimpse'>2. glimpse dataset<\/a>","a62c07df":"### <a id='Cabin'>`Cabin` field<\/a>\n  - missing value exists\n  - replace field to the first character of the field\n  - encode\n  - MinMaxScale"}}