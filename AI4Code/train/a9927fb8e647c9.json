{"cell_type":{"5cb93fdd":"code","0cd96e52":"code","1a04d1cf":"code","02d93abf":"code","fa2aa7dd":"code","00d401bd":"code","66912153":"code","0b13e310":"code","4ed50a16":"code","31f5817d":"code","99dfc5fa":"code","5df3f6d8":"code","a15e7e20":"code","5008571d":"code","9cde9b0f":"code","830d3d2a":"code","32e47d69":"code","4f339d6f":"code","2e2155fd":"code","8d1d46e7":"code","3ddd5eb2":"code","b8213632":"code","b0d2f98a":"code","f1643b57":"markdown"},"source":{"5cb93fdd":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","0cd96e52":"#Load all data in\nall_data = pd.read_csv(\"..\/input\/Churn_Modelling.csv\")","1a04d1cf":"#One-encoding Gender, Geography and NumOfProducts\ndummy_gender = pd.get_dummies(all_data['Gender'], prefix='Gender')\ndummy_geo = pd.get_dummies(all_data['Geography'],prefix = 'Geo')\ndummy_NoOfProducts=pd.get_dummies(all_data['NumOfProducts'],prefix='NOP')","02d93abf":"#Dealing with Age,catogorized it into 7 sections\nbins = [18,22,34,40,60,80,100]\nlabels = ['18-22','23-34','35-40','41-60','61-80','81-100']\ndummy_age_labels=pd.cut(all_data['Age'],bins,labels=labels,right=False)","fa2aa7dd":"all_data['Age_labeled']= dummy_age_labels","00d401bd":"dummy_age=pd.get_dummies(all_data['Age_labeled'],prefix='Age')","66912153":"# Dealing with creditscore, catogorizing it into 5 catogories\nbins =[300,579,669,739,799,850]\nlabels =['Very Poor','Fair','Good','Very Good','Exceptional']\ndummy_crdscore_labels=pd.cut(all_data['CreditScore'],bins,labels=labels)\nall_data['CreditScore_labled']= dummy_crdscore_labels\ndummy_creditscore = pd.get_dummies(all_data['CreditScore_labled'], prefix = 'CreditLevel')","0b13e310":"#Tenure catogorized into 4 sections\nbins = [0,1,5,8,11]\nlabels = ['0-1','1-5','5-8','8-11']\ndummy_tenure_labels=pd.cut(all_data['Tenure'],bins,labels=labels,right=False)\nall_data['Tenure_labeled']= dummy_tenure_labels\ndummy_tenure = pd.get_dummies(all_data['Tenure_labeled'],prefix = 'Tenure')","4ed50a16":"# Get Balance and EstimatedSalary standard\nfrom sklearn.preprocessing import StandardScaler\nall_data['Balance'] = StandardScaler().fit_transform(all_data.filter(['Balance']))\nall_data['EstimatedSalary'] = StandardScaler().fit_transform(all_data.filter(['EstimatedSalary']))","31f5817d":"data_combined = pd.concat([all_data,dummy_age,dummy_tenure,dummy_creditscore,dummy_geo,dummy_gender,dummy_NoOfProducts],axis =1)","99dfc5fa":"data_combined.drop(['Gender', 'Age', 'CreditScore','Geography','NumOfProducts','Tenure'], axis=1, inplace=True)","5df3f6d8":"data_combined.drop(['Surname','CustomerId','Age_labeled','CreditScore_labled','Tenure_labeled'],\n                   axis=1,inplace = True)","a15e7e20":"y_data=data_combined['Exited']\ndata_combined.drop(['Exited'],axis=1, inplace = True)\nX_data = data_combined\n\nX_data.set_index('RowNumber')\nX_data.reset_index(drop = True, inplace = True)\nX_data.drop('RowNumber',axis =1, inplace = True)","5008571d":"#split data into train and test dataset\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X_data,y_data,test_size=0.2,random_state=2)","9cde9b0f":"import tensorflow as tf\n\nmodel = tf.keras.models.Sequential()","830d3d2a":"model.add(tf.keras.layers.Dense(units = 128,\n                                input_dim = 28, #totally 28 features as input\n                                use_bias = True,\n                                kernel_initializer ='uniform',\n                                activation ='relu',\n                                bias_initializer ='zeros'\n                                ))\n#add 2 hidden layers\nfor i in range(0,2):\n    model.add(tf.keras.layers.Dense(units=128, kernel_initializer='normal',\n                     bias_initializer='zeros',activation='relu'))\n    model.add(tf.keras.layers.Dropout(.40)) # dropout some data to avoid overfitting\n\n\n#output layer\nmodel.add(tf.keras.layers.Dense(units =2,\n                               activation ='softmax'))","32e47d69":"optimizer = tf.keras.optimizers.Adam(0.00001) #use Adam as optimizer and the learning rate is 0.00001\nloss_function = \"sparse_categorical_crossentropy\"\n\nmodel.compile(optimizer = optimizer, loss = loss_function, metrics=['accuracy'])","4f339d6f":"model.summary()","2e2155fd":"train_history = model.fit(x = x_train,\n                          y = y_train,\n                          validation_split = 0.2, #use 20% of train data as validation data\n                          epochs = 200,\n                          batch_size = 50,\n                          verbose =1\n                         )    ","8d1d46e7":"def v_train_history(trainhist, train_metrics, valid_metrics):\n    plt.plot(trainhist.history[train_metrics])\n    plt.plot(trainhist.history[valid_metrics])\n    plt.title('Training metrics')\n    plt.ylabel(train_metrics)\n    plt.xlabel('Epochs')\n    plt.legend(['train','validation'],loc='upper left')\n    plt.show()","3ddd5eb2":"v_train_history(train_history,'acc','val_acc')","b8213632":"v_train_history(train_history,'loss','val_loss')","b0d2f98a":"evaluate_result = model.evaluate(x=x_test,\n                               y=y_test)","f1643b57":"This is the version using Keras to learn and predict the result, for data analysis, pls. refer to another version:\n\n[https:\/\/www.kaggle.com\/sunixliu\/bank-customer-churn-model](http:\/\/)"}}