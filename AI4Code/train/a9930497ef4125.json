{"cell_type":{"90c8ad06":"code","844a1cd9":"code","9d3a04e1":"code","4db42717":"code","6780c549":"code","8be5b5f1":"code","69b3e461":"code","1ca4a6fb":"code","2ce8d643":"code","c0ecfba7":"code","52edc2ce":"code","306925ef":"code","4907442b":"code","c85f4e71":"code","c107b2da":"code","27ef42f9":"code","5c104980":"code","67bf5b32":"code","337456b4":"code","45af2083":"code","68556cb3":"code","fa5cbd06":"code","98f5e92c":"code","7d73d1c6":"code","fae9aa6b":"code","62271d13":"code","ef23fe34":"code","5e36eff1":"code","2b56ce91":"code","d13ca19b":"code","bb0335b0":"code","7345ef4b":"code","15d7d8e3":"code","81c023f0":"code","3ee68c2d":"code","e437dddf":"code","5e83aafb":"markdown","0ec8ebcf":"markdown","bbf5a0e0":"markdown","16f5dd25":"markdown","5201e721":"markdown","1febb20b":"markdown","b90698b0":"markdown","84454114":"markdown","9ac9ca08":"markdown","f0754292":"markdown","38211e0c":"markdown","373db4cc":"markdown","6a7df82e":"markdown","1e0605f5":"markdown","04e3e908":"markdown","fc3815c1":"markdown","d795eaea":"markdown","e5f176d0":"markdown","ae1362b6":"markdown","968ca8d3":"markdown","b92814ff":"markdown","6dacb907":"markdown"},"source":{"90c8ad06":"# PLEASE DON'T RUN THIS CELL NOW.\nrun = wandb.init(project='shopee', group='SwAV', job_type='swav_vs_supervised')\n\ndata = [['supervised', df['resnet-f1'].mean()], ['swav', df['swav-f1'].mean()]]\ntable = wandb.Table(data=data, columns = [\"label\", \"value\"])\nwandb.log({\"swav_vs_supervised\" : wandb.plot.bar(table, \"label\", \"value\",\n                               title=\"SwAV pre- vs Supervised trained ResNet-50\")})\n           \nrun.finish()\nrun","844a1cd9":"!git clone https:\/\/github.com\/ayulockin\/SwAV-TF\n    \nimport sys\nsys.path.append('SwAV-TF\/utils')","9d3a04e1":"import multicrop_dataset\nimport architecture","4db42717":"import tensorflow as tf\nimport tensorflow_datasets as tfds\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport random\nimport time\nimport os\nos.environ['WANDB_SILENT'] = \"true\"\nimport gc\n\nfrom itertools import groupby\nfrom tqdm import tqdm\n\ntf.random.set_seed(666)\nnp.random.seed(666)","6780c549":"gpus = tf.config.list_physical_devices('GPU')\nif gpus:\n  try:\n    # Currently, memory growth needs to be the same across GPUs\n    for gpu in gpus:\n      tf.config.experimental.set_memory_growth(gpu, True)\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Memory growth must be set before GPUs have been initialized\n    print(e)","8be5b5f1":"import wandb\n\nwandb.login()","69b3e461":"TRAIN_PATH = '..\/input\/shopee-product-matching\/train_images\/'\nTRAIN_CSV = '..\/input\/shopee-product-matching\/train.csv'","1ca4a6fb":"df = pd.read_csv(TRAIN_CSV)\nimages_df = df[['posting_id', 'image']]\nimages_df.head()","2ce8d643":"AUTO = tf.data.experimental.AUTOTUNE\n\n@tf.function\ndef parse_data(df_dict):\n    img = tf.io.read_file(TRAIN_PATH+df_dict['image'])\n    img = tf.image.decode_jpeg(img, channels=3)\n    image = {'image': img}\n    return image\n\nvizloader = tf.data.Dataset.from_tensor_slices(dict(images_df))\n\nvizloader = (\n    vizloader\n    .map(parse_data, num_parallel_calls=AUTO)\n    .prefetch(AUTO)\n)","c0ecfba7":"def show_batch(image_batch):\n  plt.figure(figsize=(20,20))\n  for n in range(25):\n      ax = plt.subplot(5,5,n+1)\n      plt.imshow(image_batch[n])\n      plt.axis('off')\n    \nimage_batch = []\nfor img in vizloader.take(25):\n    image_batch.append(img['image'])\n\nshow_batch(image_batch)","52edc2ce":"# Configs\nBS = 16\n# Image sizes used to train the model. Use larger image size [224,96] better features.\nSIZE_CROPS = [128, 96]\n# Number of different augmentations of the same image\nNUM_CROPS = [2, 2]\n# Parameters for Random Resize Crop.\nMIN_SCALE = [0.5, 0.14] \nMAX_SCALE = [1., 0.5]\n\n# Experimental options\noptions = tf.data.Options()\noptions.experimental_optimization.noop_elimination = True\noptions.experimental_optimization.map_vectorization.enabled = True\noptions.experimental_optimization.apply_default_optimizations = True\noptions.experimental_deterministic = False\noptions.experimental_threading.max_intra_op_parallelism = 1","306925ef":"# Get multiple data loaders\ntrainloaders = multicrop_dataset.get_multires_dataset(vizloader,\n    size_crops=SIZE_CROPS,\n    num_crops=NUM_CROPS,\n    min_scale=MIN_SCALE,\n    max_scale=MAX_SCALE,\n    options=options)","4907442b":"# Prepare the final data loader\n\n# Zipping \ntrainloaders_zipped = tf.data.Dataset.zip(trainloaders)\n\n# Final trainloader\ntrainloaders_zipped = (\n    trainloaders_zipped\n    .batch(BS)\n    .prefetch(AUTO)\n)\n\nim1, im2, im3, im4 = next(iter(trainloaders_zipped))\nprint(im1.shape, im2.shape, im3.shape, im4.shape)","c85f4e71":"tf.keras.backend.clear_session()\nfeature_backbone = architecture.get_resnet_backbone()\nfeature_backbone.summary()\n\ndel feature_backbone\n_ = gc.collect()","c107b2da":"tf.keras.backend.clear_session()\nprojection_prototype = architecture.get_projection_prototype(2048, 128, 256)\nprojection_prototype.summary()\n\ndel projection_prototype\n_ = gc.collect()","27ef42f9":"def sinkhorn(sample_prototype_batch):\n    Q = tf.transpose(tf.exp(sample_prototype_batch\/0.01))\n    Q \/= tf.keras.backend.sum(Q)\n    K, B = Q.shape\n\n    u = tf.zeros_like(K, dtype=tf.float32)\n    r = tf.ones_like(K, dtype=tf.float32) \/ K\n    c = tf.ones_like(B, dtype=tf.float32) \/ B\n\n    for _ in range(3):\n        u = tf.keras.backend.sum(Q, axis=1)\n        Q *= tf.expand_dims((r \/ u), axis=1)\n        Q *= tf.expand_dims(c \/ tf.keras.backend.sum(Q, axis=0), 0)\n\n    final_quantity = Q \/ tf.keras.backend.sum(Q, axis=0, keepdims=True)\n    final_quantity = tf.transpose(final_quantity)\n\n    return final_quantity","5c104980":"# @tf.function\n# Reference: https:\/\/github.com\/facebookresearch\/swav\/blob\/master\/main_swav.py\ndef train_step(input_views, feature_backbone, projection_prototype, \n               optimizer, crops_for_assign, temperature):\n    # ============ retrieve input data ... ============\n    im1, im2, im3, im4 = input_views \n    inputs = [im1, im2, im3, im4]\n    batch_size = inputs[0].shape[0]\n\n    # ============ create crop entries with same shape ... ============\n    crop_sizes = [inp.shape[1] for inp in inputs] # list of crop size of views\n    unique_consecutive_count = [len([elem for elem in g]) for _, g in groupby(crop_sizes)] # equivalent to torch.unique_consecutive\n    idx_crops = tf.cumsum(unique_consecutive_count)\n    \n    # ============ multi-res forward passes ... ============\n    start_idx = 0\n    with tf.GradientTape() as tape:\n        for end_idx in idx_crops:\n            concat_input = tf.stop_gradient(tf.concat(inputs[start_idx:end_idx], axis=0))\n            _embedding = feature_backbone(concat_input) # get embedding of same dim views together\n            if start_idx == 0:\n                embeddings = _embedding # for first iter\n            else:\n                embeddings = tf.concat((embeddings, _embedding), axis=0) # concat all the embeddings from all the views\n            start_idx = end_idx\n        \n        projection, prototype = projection_prototype(embeddings) # get normalized projection and prototype\n        projection = tf.stop_gradient(projection)\n\n        # ============ swav loss ... ============\n        # https:\/\/github.com\/facebookresearch\/swav\/issues\/19\n        loss = 0\n        for i, crop_id in enumerate(crops_for_assign): # crops_for_assign = [0,1]\n            with tape.stop_recording():\n                out = prototype[batch_size * crop_id: batch_size * (crop_id + 1)]\n                \n                # get assignments\n                q = sinkhorn(out) # sinkhorn is used for cluster assignment\n            \n            # cluster assignment prediction\n            subloss = 0\n            for v in np.delete(np.arange(np.sum(NUM_CROPS)), crop_id): # (for rest of the portions compute p and take cross entropy with q)\n                p = tf.nn.softmax(prototype[batch_size * v: batch_size * (v + 1)] \/ temperature) \n                subloss -= tf.math.reduce_mean(tf.math.reduce_sum(q * tf.math.log(p), axis=1))\n            loss += subloss \/ tf.cast((tf.reduce_sum(NUM_CROPS) - 1), tf.float32)\n        \n        loss \/= len(crops_for_assign)\n\n    # ============ backprop ... ============\n    variables = feature_backbone.trainable_variables + projection_prototype.trainable_variables\n    gradients = tape.gradient(loss, variables)\n    optimizer.apply_gradients(zip(gradients, variables))\n\n    return loss","67bf5b32":"def train_swav(feature_backbone, \n               projection_prototype, \n               dataloader, \n               optimizer, \n               crops_for_assign,\n               temperature, \n               epochs=50):\n  \n    step_wise_loss = []\n    epoch_wise_loss = []\n    \n    for epoch in range(epochs):\n        w = projection_prototype.get_layer('prototype').get_weights()\n        w = tf.transpose(w)\n        w = tf.math.l2_normalize(w, axis=1)\n        projection_prototype.get_layer('prototype').set_weights(tf.transpose(w))\n\n        for step, inputs in enumerate(dataloader):\n            loss = train_step(inputs, feature_backbone, projection_prototype, \n                              optimizer, crops_for_assign, temperature)\n\n            if step%500==0:\n                print(f'[{step}|{len(trainloaders_zipped)}] loss: {loss}')\n            \n            step_wise_loss.append(loss)\n        epoch_wise_loss.append(np.mean(step_wise_loss))\n\n        print(\"epoch: {} epoch-wise loss: {:.3f}\".format(epoch + 1, np.mean(step_wise_loss)))\n        wandb.log({'epoch': epoch, 'loss':np.mean(step_wise_loss)})\n\n    return epoch_wise_loss, [feature_backbone, projection_prototype]","337456b4":"# ============ re-initialize the networks and the optimizer ... ============\ntf.keras.backend.clear_session()\nfeature_backbone = architecture.get_resnet_backbone()\nprojection_prototype = architecture.get_projection_prototype(dense_2=128, prototype_dimension=256)\n\ndecay_steps = 1000\nlr_decayed_fn = tf.keras.experimental.CosineDecay(\n    initial_learning_rate=0.1, decay_steps=decay_steps)\nopt = tf.keras.optimizers.SGD(learning_rate=lr_decayed_fn)\n\n# ================= initialize wandb ======================\nrun = wandb.init(project='shopee', group='SwAV', job_type='train')\n\n# ======================= train  ===========================\nepoch_wise_loss, models = train_swav(feature_backbone, \n    projection_prototype, \n    trainloaders_zipped, \n    opt,\n    crops_for_assign=[0, 1],\n    temperature=0.1, \n    epochs=10\n)\n\nrun.finish()\nrun","45af2083":"# Serialize the models\nfeature_backbone, projection_prototype = models\nfeature_backbone.save_weights('feature_backbone_10_epochs.h5')\nprojection_prototype.save_weights('projection_prototype_10_epochs.h5')","68556cb3":"run = wandb.init(project='shopee', group='SwAV', job_type='artifacts')\nartifact = wandb.Artifact('swav-model', type='model')\nartifact.add_file('feature_backbone_10_epochs.h5')\nartifact.add_file('projection_prototype_10_epochs.h5')\nrun.log_artifact(artifact)\nrun.finish()","fa5cbd06":"import cv2\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F \nimport torchvision.models as models\nfrom torch.utils.data import Dataset \n\nimport albumentations as A \nfrom albumentations.pytorch.transforms import ToTensorV2\n\n\nimport gc\nimport cudf\nimport cuml\nimport cupy\nfrom cuml.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\n\ndef seed_torch(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_torch()","98f5e92c":"class CFG:\n    img_size = 512\n    batch_size = 12\n    seed = 2020\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    classes = 11014\n    \n    scale = 30 \n    margin = 0.5","7d73d1c6":"def read_dataset():\n    # Read CSV file.\n    df = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\n    df_cu = cudf.DataFrame(df)\n    image_paths = '..\/input\/shopee-product-matching\/train_images\/' + df['image']\n    return df, df_cu, image_paths","fae9aa6b":"def get_test_transforms():\n    # Transformations.\n    return A.Compose(\n        [\n            A.Resize(CFG.img_size,CFG.img_size,always_apply=True),\n            A.Normalize(),\n        ToTensorV2(p=1.0)\n        ]\n    )\n\nclass ShopeeDataset(Dataset):\n    '''\n    Dataloader\n    '''\n    def __init__(self, image_paths, transforms=None):\n\n        self.image_paths = image_paths\n        self.augmentations = transforms\n\n    def __len__(self):\n        return self.image_paths.shape[0]\n\n    def __getitem__(self, index):\n        image_path = self.image_paths[index]\n        \n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.augmentations:\n            augmented = self.augmentations(image=image)\n            image = augmented['image']       \n    \n        return image","62271d13":"def get_image_predictions(df, embeddings,threshold = 3.4):\n    # Get predictions using KNN.\n    if len(df) > 3:\n        KNN = 50\n    else : \n        KNN = 3\n    \n    model = NearestNeighbors(n_neighbors = KNN)\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    \n    predictions = []\n    for k in tqdm(range(embeddings.shape[0])):\n        idx = np.where(distances[k,] < threshold)[0]\n        ids = indices[k,idx]\n        posting_ids = df['posting_id'].iloc[ids].values\n        predictions.append(posting_ids)\n        \n    del model, distances, indices\n    gc.collect()\n    return predictions","ef23fe34":"def get_image_embeddings(image_paths, model):\n    embeds = []\n    \n    model.eval()\n    model = model.to(CFG.device)\n\n    image_dataset = ShopeeDataset(image_paths=image_paths,transforms=get_test_transforms())\n    image_loader = torch.utils.data.DataLoader(\n        image_dataset,\n        batch_size=CFG.batch_size,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=4\n    )\n    \n    with torch.no_grad():\n        for img in tqdm(image_loader): \n            img = img.cuda()\n            feat = model(img)\n            image_embeddings = feat.detach().cpu().numpy()\n            embeds.append(image_embeddings)\n    \n    del model\n    image_embeddings = np.concatenate(embeds)\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return image_embeddings","5e36eff1":"class ResNet50Embedding(nn.Module):\n    def __init__(self, original_model):\n        super(ResNet50Embedding, self).__init__()\n        self.features = nn.Sequential(*list(original_model.children())[:-1])\n\n    def forward(self, x):\n        x = self.features(x)\n        return x\n\n# Load model\nmodel_resnet_swav = torch.hub.load('facebookresearch\/swav', 'resnet50')\nmodel_resnet = models.resnet50(pretrained=True)\n    \n# Get output at Global Average Pooling.\nmodel_resnet_swav = ResNet50Embedding(model_resnet_swav).to(device)\nmodel_resnet = ResNet50Embedding(model_resnet).to(device)","2b56ce91":"df, df_cu, image_paths = read_dataset()\ndf.head()","d13ca19b":"# Get embeddings using SwAV-ResNet\nimage_embeddings_swav = get_image_embeddings(image_paths.values, model_resnet_swav)\nimage_embeddings_swav = image_embeddings_swav.reshape((34250, 2048))\n\n# Get embeddings uing Supervised-ResNet\nimage_embeddings_sup = get_image_embeddings(image_paths.values, model_resnet)\nimage_embeddings_sup = image_embeddings_sup.reshape((34250, 2048))","bb0335b0":"# Get image similarity predictions using SwAV embeddings \nimage_predictions_swav = get_image_predictions(df, image_embeddings_swav, threshold = 1.7)\n\n# Get image similarity predictions using Supervised embeddings\nimage_predictions_sup = get_image_predictions(df, image_embeddings_sup, threshold = 1.7)","7345ef4b":"tmp = df.groupby('label_group').posting_id.agg('unique').to_dict()\ndf['target'] = df.label_group.map(tmp)\nprint('train shape is', df.shape )\ndf.head()","15d7d8e3":"# Ref: https:\/\/www.kaggle.com\/cdeotte\/part-2-rapids-tfidfvectorizer-cv-0-700\ndef getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.target,row[col]) )\n        return 2*n \/ (len(row.target)+len(row[col]))\n    return f1score","81c023f0":"df['image_predictions_swav'] = image_predictions_swav\ndf['image_predictions_sup'] = image_predictions_sup\n\ndf.head()","3ee68c2d":"df['swav-f1'] = df.apply(getMetric('image_predictions_swav'),axis=1)\ndf['resnet-f1'] = df.apply(getMetric('image_predictions_sup'),axis=1)","e437dddf":"run = wandb.init(project='shopee', group='SwAV', job_type='swav_vs_supervised')\n\ndata = [['supervised', df['resnet-f1'].mean()], ['swav', df['swav-f1'].mean()]]\ntable = wandb.Table(data=data, columns = [\"label\", \"value\"])\nwandb.log({\"swav_vs_supervised\" : wandb.plot.bar(table, \"label\", \"value\",\n                               title=\"SwAV pre- vs Supervised trained ResNet-50\")})\n           \nrun.finish()\nrun","5e83aafb":"### Data Loader","0ec8ebcf":"# Train Step","bbf5a0e0":"# Training Loop","16f5dd25":"# Visualize Train Images","5201e721":"# Multi Crop Resize Data Augmentation\n\nProduces multiple views of the same image instead of just a pair of views without quadratically increasing the memory and computational requirement. More details [here](https:\/\/wandb.ai\/authors\/swav-tf\/reports\/Unsupervised-Visual-Representation-Learning-with-SwAV--VmlldzoyMjg3Mzg).","1febb20b":"# Imports and Setups","b90698b0":"# Model Architecture","84454114":"* **v1**: Introduction to SwAV pretraining.\n* **v2**: I wanted to see the effect on CV with only image embedding by using ResNet pretrained with SwAV vs good-old ResNet. \n\n## TL;DR\n\nEven though the top-1 accuracy of ResNet-50 is better than SwAV pre-trained ResNet-50, the CV score is ~5% better. This suggests that this method might help get a jump on the leader board.\n\n**If you have the resources, pre-training an EfficientNet model might work better than using ArcFace loss.**\n\nCheck out the **Effect on CV Score: A Comparative Study** section below to see the details. To learn more about SwAV continue reading. ","9ac9ca08":"### Read Data, Get Image Embeddings, And Get Predictions","f0754292":"### Configurations","38211e0c":"# Save the trained weights as W&B Artifacts","373db4cc":"# Sinkhorn Knopp for Cluster Assignment\n\nOnline cluster assignment and set up the swapped prediction problem. More details [here](https:\/\/wandb.ai\/authors\/swav-tf\/reports\/Unsupervised-Visual-Representation-Learning-with-SwAV--VmlldzoyMjg3Mzg).","6a7df82e":"## Disclaimer\/Notes\n\n* It might happen that you run out of RAM while running this Kernel. You might want to lower the resolution of the cropped images by changing this parameter `SIZE_CROPS`, you can also change the number of views generated by changing this `NUM_CROPS`. Note that GPU is not going to be an issue in the default configuration.\n* I have trained this model on a GCP instance and I have saved the weights of the same as W&B artifacts. To download the weigths use the code snippet below:\n\n```\nimport wandb\nrun = wandb.init()\nartifact = run.use_artifact('ayush-thakur\/shopee\/swav-model:v0', type='model')\nartifact_dir = artifact.download()\n```\n\n* This is a minimal implementation of the technique. Fortunately the authors of the SwAV paper have provided us with the pretrained weights. To use it,\n\n```\nimport torch\nmodel = torch.hub.load('facebookresearch\/swav', 'resnet50')\n```\n\nFinally if you like the work consider upvoting the kernel and would appreciate a star for our [GitHub repo](https:\/\/github.com\/ayulockin\/SwAV-TF).","1e0605f5":"### Compute CV Score","04e3e908":"### Get Predictions","fc3815c1":"That's amazing! Even though the top-1 accuracy of ResNet-50 is better than SwAV pre-trained ResNet-50, the CV score is ~5% better. This suggests that this method might help get a jump on the leader board.\n\nIf you have the resources, pre-training an EfficientNet model might work better than using ArcFace loss. \n\nHope you find it useful.","d795eaea":"Visit https:\/\/wandb.ai\/authorize to get your API token.","e5f176d0":"### Models\n\nThe comparative study is between ResNet-50 trained using conventional image-classification method vs ResNet-50 pre-trained using SwAV. \n\n**Note**: The supervised counterpart has better Top-1 accuracy metric. It would be interesting to see if SwAV training methodology of cluster assignment is improving the CV score or not. \n\n![swav-vs-resnet.png](attachment:10d08793-f959-4822-a159-99e1d1d9fb86.png)","ae1362b6":"# Unsupervised Learning of Visual Features by Contrasting Cluster Assignments\n\n[Paper](https:\/\/arxiv.org\/abs\/2006.09882) | [W&B Report](https:\/\/wandb.ai\/authors\/swav-tf\/reports\/Unsupervised-Visual-Representation-Learning-with-SwAV--VmlldzoyMjg3Mzg) | [Official PyTorch Implementation](https:\/\/github.com\/facebookresearch\/swav) | [Minimal TensorFlow Implementation](https:\/\/github.com\/ayulockin\/SwAV-TF)\n\n## Context\n\nUnsupervised visual representation learning is progressing at an exceptionally fast pace. Most of the modern training frameworks (SimCLR, BYOL, MoCo (V2)) in this area make use of a self-supervised model pre-trained with some contrastive learning objective. Saying these frameworks perform great w.r.t supervised model pre-training would be an understatement, as evident from the figure below - \n\n![img](https:\/\/i.imgur.com\/TzzuGE5.png)\n\nFigure 1: Top-1 accuracy of linear classifiers trained with the frozen features of different self-supervised methods w.r.t the fully supervised methods. ([Source](https:\/\/arxiv.org\/abs\/2006.09882))\n\nMoreover, when the features learned using these different self-supervised methods are fine-tuned with as little as 1% and 10% of labeled training data show tremendous performance - \n\n![image.png](attachment:image.png)\n\nFigure 2: Performance of different semi-supervised and self-supervised frameworks on fine-tuning with very little labeled data. ([Source](https:\/\/arxiv.org\/abs\/2006.09882))\n\n## How can SwAV be helpful in this competition?\n\nMany fellow Kagglers are using pretrained image classifier with ARC Face loss function. However these image classifiers are not optimized to bring together semantically similar images in the embedding space. SwAV in my opinion can be useful and might give some gain in the LB (subject to experiments).\n\nI along with [Sayak Paul](https:\/\/twitter.com\/RisingSayak) minimally implemented this paper in TensorFlow. **This kernel is providing the same training regime for Shoppe dataset.** \n\n## What's SwAV?\n\n![image.png](attachment:image.png)\n\nFigure 3: High-level overview of SwAV. ([Source](https:\/\/arxiv.org\/abs\/2006.09882))\n\nThe authors of this paper investigated a question:\n\n> **Can we learn a meaningful metric that reflects apparent similarity among instances via pure discriminative learning?**\n\nTo answer this, they devised a novel unsupervised feature learning algorithm called instance-level discrimination. Here each image and its transformations\/views are treated as two separate instances. Each image instance is treated as a separate class. The aim is to learn an embedding, mapping $x$ (image) to $v$ (feature) such that semantically similar instances(images) are closer in the embedding space.\n\n![img](https:\/\/i.imgur.com\/oRvT9wZ.gif)\n\n## Using Weights and Biases\n\nI have used W&B to keep track of the experiments and to save the trained models as artifacts. You will [require an account](https:\/\/wandb.ai\/site) to run this kernel with W&B instrumentation. ","968ca8d3":"# Effect on CV Score: A Comparative Study","b92814ff":"### Generate Image Embeddings","6dacb907":"### Imports"}}