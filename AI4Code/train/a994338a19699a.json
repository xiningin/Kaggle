{"cell_type":{"c846355b":"code","461c1226":"code","02229905":"code","5d0468b8":"code","49e22434":"code","11801db0":"code","f52e37a3":"code","63f02d81":"code","3ac727b9":"code","4bf06a44":"code","52fdfc58":"code","bdf07065":"code","7f1dd15a":"code","f8c3cf6e":"code","e15937b4":"code","4b26b397":"code","1bb4494c":"code","d4e5fc4a":"code","58ebeda8":"code","a395df16":"code","6df1754b":"code","1c613f6c":"code","a23a06b4":"code","1666085f":"code","7b5adbaf":"code","19dd063c":"code","f55cef2f":"code","3190b172":"code","a40db93a":"code","7377e497":"code","1ffed59c":"code","f2c9f552":"code","a76f8611":"code","fb493ecf":"markdown","022a95de":"markdown","ecd20d7d":"markdown","6e2c3f63":"markdown","17fe80bd":"markdown","7f4c6a13":"markdown","62dbbaf4":"markdown","083b5a46":"markdown","bfad9dd6":"markdown","b7f6c1a0":"markdown","a90ae4f1":"markdown","afccb6c5":"markdown","fc6d7cd6":"markdown","7871c59d":"markdown","30899648":"markdown","2495b4ce":"markdown","f93a77e7":"markdown","372db6e5":"markdown","c646d2cb":"markdown","3cfbb626":"markdown","db7380ac":"markdown"},"source":{"c846355b":"import pandas as pd\nimport time\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Import Model Libraries\nimport xgboost as xgb\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# Train a base decision tree regressor model on the data\nfrom sklearn.tree import DecisionTreeRegressor\n\nimport warnings\nwarnings.filterwarnings('ignore')\n","461c1226":"from sklearn.datasets import load_boston\nboston = load_boston()","02229905":"bos = pd.DataFrame(boston.data)\nbos.columns = boston.feature_names\nbos['PRICE'] = boston.target\nbos.head()","5d0468b8":"# Split Train\/Test Set\nX_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(bos.drop([\"PRICE\"], axis=1), bos[\"PRICE\"],random_state=10, test_size=0.25)","49e22434":"# Examine our dataset\nX_train_2.head()","11801db0":"# Examine shape of the dataset\nX_train_2.shape","f52e37a3":"# Check for any missing values\nX_train_2.isnull().any()","63f02d81":"# Train a base decision tree regressor model on the data\nfrom sklearn.tree import DecisionTreeRegressor\n# Fit model\ntree_reg1 = DecisionTreeRegressor(max_depth=2)\ntree_reg1.fit(X_train_2,y_train_2)","3ac727b9":"# Compute errors\/residuals on first tree\nr1 = y_train_2 - tree_reg1.predict(X_train_2)","4bf06a44":"# Fit second model\ntree_reg2 = DecisionTreeRegressor(max_depth=2)\ntree_reg2.fit(X_train_2,r1)","52fdfc58":"# Compute errors\/residuals on second tree\nr2 = r1 - tree_reg2.predict(X_train_2)","bdf07065":"# Fit third model\ntree_reg3 = DecisionTreeRegressor(max_depth=2)\ntree_reg3.fit(X_train_2,r2)","7f1dd15a":"# Add up the predictions of each tree model, which is our ensemble of three trees\ny_pred = sum(tree.predict(X_train_2) for tree in (tree_reg1, tree_reg2, tree_reg3))\n","f8c3cf6e":"y_pred[:10]","e15937b4":"#actual values\ny_train_2[:10]","4b26b397":"tree_reg1.predict(X_train_2)[:10]","1bb4494c":"# Create dataframe of all predictions\npredictions = pd.DataFrame(tree_reg1.predict(X_train_2)[:10], columns=['Model_1'])\npredictions['Model_2'] = pd.DataFrame(tree_reg2.predict(X_train_2)[:10])\npredictions['Model_3'] = pd.DataFrame(tree_reg3.predict(X_train_2)[:10])\npredictions['Ensemble'] = pd.DataFrame(y_pred[:10])\npredictions['Actual'] = y_train_2.head(10).reset_index()['PRICE']\n\n# Display predictions\npredictions","d4e5fc4a":"errors = []\nfor n_estimators in [1,2,3,4,5,6,7,8,9,10]:\n    clf = xgb.XGBRegressor(max_depth=2, n_estimators=n_estimators)\n    clf.fit(X_train_2, y_train_2, verbose=False)\n    errors.append(\n        {\n            'Tree Count': n_estimators,\n            'Average Error': np.average(y_train_2 - clf.predict(X_train_2)),\n        })\n    \nn_estimators_lr = pd.DataFrame(errors).set_index('Tree Count').sort_index()\nn_estimators_lr","58ebeda8":"# Use the Sklearn GradientBoostingRegressor ensemble method to perform the same thing as the previous code above\nfrom sklearn.ensemble import GradientBoostingRegressor\n\ngbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3)\ngbrt.fit(X_train_2,y_train_2)","a395df16":"import xgboost as xgb\nGradientBoostingRegressor()","6df1754b":"# Examine the default parameters\nxgb.XGBRegressor()","1c613f6c":"# Create empty array to store results\nresults = []\n# Create watchlist to keep track of train\/validation performance\neval_set = [(X_train_2, y_train_2), (X_test_2, y_test_2)]","a23a06b4":"# Enumerate through different max_depth values and store results\nfor max_depth in [2,3,4,5,10,12,15]:\n    clf = xgb.XGBRegressor(max_depth=max_depth)\n    clf.fit(X_train_2, y_train_2, eval_set=eval_set, verbose=False)\n    results.append(\n        {\n            'max_depth': max_depth,\n            'train_error': metrics.mean_squared_error(y_train_2, clf.predict(X_train_2)),\n            'test_error': metrics.mean_squared_error(y_test_2, clf.predict(X_test_2))\n        })\n    \n# Display Results\nmax_depth_lr = pd.DataFrame(results).set_index('max_depth').sort_index()\nmax_depth_lr","1666085f":"# Plot Max_Depth Learning Curve\nmax_depth_lr.plot(title=\"Max_Depth Learning Curve\")","7b5adbaf":"# Reset results array\nresults = []\n\nfor learning_rate in [0.05,0.1,0.2,0.4,0.6,0.8,1]:\n    clf = xgb.XGBRegressor(max_depth=2,learning_rate=learning_rate, n_estimators=200)\n    clf.fit(X_train_2, y_train_2, eval_set=eval_set, verbose=False)\n    results.append(\n        {\n            'learning_rate': learning_rate,\n            'train_error': metrics.mean_squared_error(y_train_2, clf.predict(X_train_2)),\n            'test_error': metrics.mean_squared_error(y_test_2, clf.predict(X_test_2))\n        })\n    \nlearning_rate_lr = pd.DataFrame(results).set_index('learning_rate').sort_index()\nlearning_rate_lr","19dd063c":"# Plot Learning Rate\nlearning_rate_lr.plot(title=\"Learning Rate Learning Curve\")","f55cef2f":"# Reset results array\nresults = []\n\nfor n_estimators in [50,60,100,150,200,500,750,1000, 1500]:\n    clf = xgb.XGBRegressor(max_depth=2,learning_rate=0.10, n_estimators=n_estimators)\n    clf.fit(X_train_2, y_train_2, eval_set=eval_set, verbose=False)\n    results.append(\n        {\n            'n_estimators': n_estimators,\n            'train_error': metrics.mean_squared_error(y_train_2, clf.predict(X_train_2)),\n            'test_error': metrics.mean_squared_error(y_test_2, clf.predict(X_test_2))\n        })\n    \nn_estimators_lr = pd.DataFrame(results).set_index('n_estimators').sort_index()\nn_estimators_lr","3190b172":"n_estimators_lr.plot(title=\"N_Estimators Learning Curve\")","a40db93a":"model = xgb.XGBRegressor()\n# Define Parameters\nparam_grid = {\"max_depth\": [2,3,10],\n              \"max_features\" : [1.0,0.3,0.1],\n              \"min_samples_leaf\" : [3,5,9],\n              \"n_estimators\": [50,100,300],\n              \"learning_rate\": [0.05,0.1,0.02,0.2]}\n# Perform Grid Search CV\ngs_cv = GridSearchCV(model, param_grid=param_grid, cv = 3, verbose=10, n_jobs=-1 ).fit(X_train_2, y_train_2)","7377e497":"# Best hyperparmeter setting\ngs_cv.best_estimator_","1ffed59c":"# Use our best model parameters found by GridSearchCV\nbest_model = xgb.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=3, max_features=1.0, min_child_weight=1,\n       min_samples_leaf=3, missing=None, n_estimators=300, n_jobs=1,\n       nthread=None, objective='reg:linear', random_state=0, reg_alpha=0,\n       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n       subsample=1)\n# Create eval_set\neval_set = [(X_train_2, y_train_2), (X_test_2, y_test_2)]\n\n# Fit our model to the training set\nbest_model.fit(X_train_2, y_train_2, eval_set=eval_set, verbose=False)\n\n# Make predictions with test data\ny_pred = best_model.predict(X_test_2)\npredictions = [round(value) for value in y_pred]\n\n# Retrieve performance metrics\nresults = best_model.evals_result()\nepochs = len(results['validation_0']['rmse'])\nx_axis = range(0, epochs)\n\n# Plot log loss curve\nfig, ax = plt.subplots()\nax.plot(x_axis, results['validation_0']['rmse'], label='Train')\nax.plot(x_axis, results['validation_1']['rmse'], label='Test')\nax.legend()\nplt.ylabel('RMSE')\nplt.title('XGBoost RMSE')","f2c9f552":"# Fit the training set and apply early stopping \nbest_model.fit(X_train_2, y_train_2, early_stopping_rounds=10, eval_set=eval_set, verbose=True)","a76f8611":"# Plot basic feature importance chart\nfig, ax = plt.subplots(figsize=(12,12))\nxgb.plot_importance(best_model, height=0.5, ax=ax)\nplt.show()","fb493ecf":"**N_Estimators**","022a95de":"# Load Boston Dataset","ecd20d7d":"**Train third tree**","6e2c3f63":"**Learning_Rate**","17fe80bd":"# Feature Importance","7f4c6a13":"# Tree Ensemble (Boosting) from Scratch","62dbbaf4":"The best learning rate is 0.05","083b5a46":"# ENSEMBLE: Combine all three tree predictions","bfad9dd6":"**max_depth**","b7f6c1a0":"**Learning Curve**","a90ae4f1":"# model prediction","afccb6c5":"**Train first tree**","fc6d7cd6":"Best N_Estimators is 50","7871c59d":"Looks like the best max_depth is 3","30899648":"# GridSearchCV","2495b4ce":"# Using Sklearn Gradient Boosting Regressor","f93a77e7":"# XGBoost Model","372db6e5":"**Train second tree**","c646d2cb":"**First 10 ENSEMBLE Predictions**\n","3cfbb626":"# Check for any missing values","db7380ac":"# Train Test Split"}}