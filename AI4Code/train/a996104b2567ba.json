{"cell_type":{"a47f3734":"code","661cb703":"code","eccf7c2a":"code","db8529d2":"code","ddaf8738":"code","52e7b3ea":"code","66b23c6a":"code","c4ee1e40":"code","cd119e59":"code","6218551e":"code","77adf4cc":"code","b726fec5":"code","374e2e13":"code","b1bd7e3e":"code","eed05444":"code","b4cdc244":"code","afada2d9":"code","b46671a9":"code","8268e261":"code","5df64747":"code","745fcf83":"code","97e8af00":"code","d1381d9d":"code","f70c22e7":"code","db0a289e":"code","46cba256":"code","17630ff9":"code","67de279e":"code","b947221f":"code","56fae729":"code","1a1f82fc":"code","7d0ccaf1":"code","99c87c12":"code","c3d2a67d":"code","c4e84422":"code","a9d3417d":"code","1f8df612":"code","d759db06":"code","418ad1d7":"code","4a22b2ae":"code","595f291a":"code","12499d77":"code","b8c27167":"code","2d8baf62":"code","c97744f0":"code","8ae09117":"code","8b8f3123":"code","b93a006b":"code","3a0d2c16":"code","004417ed":"code","497d69bb":"code","cdc1e6bf":"code","ecf5a8a8":"code","8ecef12a":"code","3a71383c":"markdown","7008ba1d":"markdown","90079c8a":"markdown"},"source":{"a47f3734":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","661cb703":"#This is an example to learn charting and plotting\n#Credits :\n    #1. https:\/\/www.kaggle.com\/kashishrastogi\/store-sales-forecasting\n    #2. https:\/\/www.kaggle.com\/shivamb\/store-sales-forecasting-exploration\n    ","eccf7c2a":"\nimport pandas as pd\nimport numpy as np\nimport calendar\n\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nimport plotly.offline as offline\nimport plotly.graph_objs as go\noffline.init_notebook_mode(connected = True)","db8529d2":"!python -m pip install tslearn\n!python -m pip install neuralprophet\n!python -m pip install prophet\n!python -m pip install kats\n!python -m pip install galearn","ddaf8738":"from tslearn.utils import to_time_series\nfrom neuralprophet import NeuralProphet\nfrom prophet import Prophet\nfrom kats.consts import TimeSeriesData\nfrom kats.models.prophet import ProphetModel, ProphetParams\n","52e7b3ea":"df_train = pd.read_csv(\"\/kaggle\/input\/store-sales-time-series-forecasting\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/store-sales-time-series-forecasting\/test.csv\")\ndf_oil = pd.read_csv(\"\/kaggle\/input\/store-sales-time-series-forecasting\/oil.csv\")\ndf_holidays = pd.read_csv(\"\/kaggle\/input\/store-sales-time-series-forecasting\/holidays_events.csv\")\ndf_stores = pd.read_csv(\"\/kaggle\/input\/store-sales-time-series-forecasting\/stores.csv\")\ndf_taxation = pd.read_csv(\"\/kaggle\/input\/store-sales-time-series-forecasting\/transactions.csv\")","66b23c6a":"print('Training Data Shape', df_train.shape)\nprint('Test Data Shape', df_test.shape)\nprint('Oil Data Shape', df_oil.shape)\nprint('Holiday Data Shape', df_holidays.shape)\nprint('Stores Data Shape', df_stores.shape)\nprint('Taxation Data Shape', df_taxation.shape)\n","c4ee1e40":"df_train.head()","cd119e59":"df_oil.columns","6218551e":"df_stores.columns","77adf4cc":"df_taxation.columns","b726fec5":"## combine datasets\ndf_train = df_train.merge(df_oil, on = 'date', how='left')\ndf_train = df_train.merge(df_holidays, on = 'date', how='left')\ndf_train = df_train.merge(df_stores, on = 'store_nbr', how='left')\ndf_train = df_train.merge(df_taxation, on = ['date', 'store_nbr'], how='left')\ndf_train = df_train.rename(columns = {\"type_x\" : \"holiday_type\", \"type_y\" : \"store_type\"})\n\ndf_test = df_test.merge(df_oil, on = 'date', how='left')\ndf_test = df_test.merge(df_holidays, on = 'date', how='left')\ndf_test = df_test.merge(df_stores, on = 'store_nbr', how='left')\ndf_test = df_test.merge(df_taxation, on = ['date', 'store_nbr'], how='left')\ndf_test = df_test.rename(columns = {\"type_x\" : \"holiday_type\", \"type_y\" : \"store_type\"})\n\n","374e2e13":"df_train.head()","b1bd7e3e":"df_test.head()","eed05444":"#Sales by date\n\ndf_train.groupby(\"date\").agg({'sales':\"mean\"}).plot()","b4cdc244":"df_train.groupby(\"date\").agg({'sales':\"mean\"}).plot()","afada2d9":"def create_aggregate_plot(df, x_axis, y_axis, basis_of_aggregation, metric, title ):\n    agg = df.groupby([x_axis, y_axis]).agg({basis_of_aggregation : metric}).reset_index()\n    fig = px.line(agg, x= x_axis, y= basis_of_aggregation , color= y_axis)\n    fig.update_layout(title = title)\n    fig.show()","b46671a9":"create_aggregate_plot(df_train, \"date\", \"store_type\", \"sales\", \"mean\", \"Average Sales by Date and Store \" )\n\n\n# agg = df_train.groupby(['date', 'store_type']).agg({\"sales\" : \"mean\"}).reset_index()\n# fig = px.line(agg, x='date', y=\"sales\", color='store_type')\n# fig.update_layout(title = \"Average Sales by Date and Store \")\n# fig.show()","8268e261":"create_aggregate_plot(df_train, \"date\", \"locale_name\", \"sales\", \"mean\", \"Average Sales by Date and Locale name\" )\n\n# agg = df_train.groupby(['date', \"locale_name\"]).agg({\"sales\" : \"mean\"}).reset_index()\n# fig = px.line(agg, x='date', y=\"sales\", color='locale_name')\n# fig.update_layout(title = \"Average Sales by Date and Locale name \")\n# fig.show()","5df64747":"def vbar(col):\n    temp = df_train.groupby(col).agg({\"sales\" : \"mean\"}).reset_index()\n    temp = temp.sort_values('sales', ascending = False)\n    c = {\n        'x' : list(temp['sales'])[:15][::-1], \n        'y' : list(temp[col])[:15][::-1],\n        'title' : \"Average sales by \"+col\n    }\n    trace = go.Bar(y=[str(_) + \"    \" for _ in c['y']], x=c['x'], orientation=\"h\", marker=dict(color=\"#f77e90\"))\n    return trace \n\n    layout = go.Layout(title=c['title'], \n                           paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)',\n                           xaxis_title=\"\", yaxis_title=\"\", width=650)\n    fig = go.Figure([trace], layout=layout)\n    fig.update_xaxes(tickangle=45, tickfont=dict(color='crimson'))\n    fig.update_yaxes(tickangle=0, tickfont=dict(color='crimson'))\n    fig.show()\n    \ntrace1 = vbar('family') \ntrace2 = vbar('store_type') \ntrace3 = vbar('state') \ntrace4 = vbar('city') \n\ntitles = ['Store Family', 'Store Type', 'State', 'City']\ntitles = ['Top ' + _ + \" by Average Sales\" for _ in titles]\nfig = make_subplots(rows=2, cols=2, subplot_titles = titles)\n\nfig.add_trace(trace1, row=1, col=1)\nfig.add_trace(trace2, row=1, col=2)\nfig.add_trace(trace3, row=2, col=1)\nfig.add_trace(trace4, row=2, col=2)\n\nfig.update_layout(height=800, paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)', showlegend = False)\nfig.show()","745fcf83":"\ntrace1 = vbar('cluster') \ntrace2 = vbar('store_nbr') \n\ntitles = ['Cluster Number', 'Store Number']\ntitles = ['Top ' + _ + \" by Average Sales\" for _ in titles]\nfig = make_subplots(rows=1, cols=2, subplot_titles = titles)\n\nfig.add_trace(trace1, row=1, col=1)\nfig.add_trace(trace2, row=1, col=2)\n\nfig.update_layout(height=500, paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)', showlegend = False)\nfig.show()","97e8af00":"def create_ts_features(df):\n    df['date'] = pd.to_datetime(df['date'])\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['quarter'] = df['date'].dt.quarter\n    df['month'] = df['date'].dt.month\n    df['year'] = df['date'].dt.year\n    df['dayofyear'] = df['date'].dt.dayofyear\n    df['dayofmonth'] = df['date'].dt.day\n    return df\n    \ndf_train = create_ts_features(df_train)\ndf_test = create_ts_features(df_test)\ndf_train.head()","d1381d9d":"def hbar(col):\n    temp = df_train.groupby(col).agg({\"sales\" : \"mean\"}).reset_index()\n    temp = temp.sort_values(col, ascending = False)\n    c = {\n        'y' : list(temp['sales']), \n        'x' : list(temp[col]),\n        'title' : \"Average sales by \"+col\n    }\n    trace = go.Bar(y=c['y'], x=c['x'], orientation=\"v\", marker=dict(color=\"#bbe070\"))\n    return trace \n\n    layout = go.Layout(title=c['title'], \n                           paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)',\n                           xaxis_title=\"\", yaxis_title=\"\", width=650)\n    fig = go.Figure([trace], layout=layout)\n    fig.update_xaxes(tickangle=45, tickfont=dict(color='crimson'))\n    fig.update_yaxes(tickangle=0, tickfont=dict(color='crimson'))\n    fig.show()\n    \ntrace1 = hbar('dayofweek') \ntrace2 = hbar('dayofmonth') \ntrace3 = hbar('dayofyear') \ntrace4 = hbar('month') \ntrace5 = hbar('quarter') \ntrace6 = hbar('year') \n\ntitles = ['Day of Week', 'Day of Month', 'Day of Year', 'Month', 'Quarter', 'Year']\ntitles = ['Avg Sales by ' + _ for _ in titles]\nfig = make_subplots(rows=3, cols=2, subplot_titles = titles)\n\nfig.add_trace(trace1, row=1, col=1)\nfig.add_trace(trace2, row=1, col=2)\nfig.add_trace(trace3, row=2, col=1)\nfig.add_trace(trace4, row=2, col=2)\nfig.add_trace(trace5, row=3, col=1)\nfig.add_trace(trace6, row=3, col=2)\n\nfig.update_layout(height=1200, paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)', showlegend = False)\nfig.show()","f70c22e7":"\nagg = df_train.groupby([\"year\", \"month\"]).agg({\"sales\"  :\"mean\", \"transactions\" : \"mean\"}).reset_index()\nfig = px.box(agg, y=\"sales\", facet_col=\"month\", color=\"month\",\n             boxmode=\"overlay\", points='all')\nfig.update_layout(title = \"Average Sales Distribution by Store Type\")\nfig.show()","db0a289e":"\nagg = df_train.groupby([\"year\", \"store_type\"]).agg({\"sales\"  :\"mean\", \"transactions\" : \"mean\"}).reset_index()\nfig = px.box(agg, y=\"sales\", facet_col=\"store_type\", color=\"store_type\",\n             boxmode=\"overlay\", points='all')\nfig.update_layout(title = \"Average Sales Distribution by Store Type\")\nfig.show()","46cba256":"df_train['holiday_type'] = df_train['holiday_type'].fillna(\"No Holiday\/Event\")\ndf_train['holiday_type'].value_counts()\n\ndef convert_to_size(x):\n    if x < 50:\n        return 6\n    elif x < 100:\n        return 10\n    elif x < 150:\n        return 15\n    elif x < 250:\n        return 18 \n    elif x < 300:\n        return 24 \n    elif x < 500:\n        return 30 \n    else:\n        return 40\n\ndef bubble(col1, col2):\n    vc = df_train.groupby([col1, col2]).agg({\"sales\" : \"mean\"}).reset_index()\n    vc = vc.sort_values(col2)    \n    fig = px.scatter(vc, x=col1, y=col2, \n                     size='sales', color='sales', size_max=40)\n    fig.update_layout(title = \"Average Sales by \"+col1+\" and \" + col2)\n    fig.show()\n    \nbubble('month', 'holiday_type')\nbubble('month', 'store_type')","17630ff9":"df_st_sa = df_train.groupby('store_type').agg({\"sales\" : \"mean\"}).reset_index().sort_values(by='sales', ascending=False)\ndf_fa_sa = df_train.groupby('family').agg({\"sales\" : \"mean\"}).reset_index().sort_values(by='sales', ascending=False)[:10]\ndf_cl_sa = df_train.groupby('cluster').agg({\"sales\" : \"mean\"}).reset_index() \n# chart color\ndf_fa_sa['color'] = '#496595'\ndf_fa_sa['color'][2:] = '#c6ccd8'\ndf_cl_sa['color'] = '#c6ccd8'\n\n# chart\nfig = make_subplots(rows=2, cols=2, \n                    specs=[[{\"type\": \"bar\"}, {\"type\": \"pie\"}],\n                           [{\"colspan\": 2}, None]],\n                    column_widths=[0.7, 0.3], vertical_spacing=0, horizontal_spacing=0.02,\n                    subplot_titles=(\"Top 10 Highest Product Sales\", \"Highest Sales in Stores\", \"Clusters Vs Sales\"))\n\nfig.add_trace(go.Bar(x=df_fa_sa['sales'], y=df_fa_sa['family'], marker=dict(color= df_fa_sa['color']),\n                     name='Family', orientation='h'), \n                     row=1, col=1)\nfig.add_trace(go.Pie(values=df_st_sa['sales'], labels=df_st_sa['store_type'], name='Store type',\n                     marker=dict(colors=['#334668','#496595','#6D83AA','#91A2BF','#C8D0DF']), hole=0.7,\n                     hoverinfo='label+percent+value', textinfo='label'), \n                    row=1, col=2)\nfig.add_trace(go.Bar(x=df_cl_sa['cluster'], y=df_cl_sa['sales'], \n                     marker=dict(color= df_cl_sa['color']), name='Cluster'), \n                     row=2, col=1)\n\n# styling\nfig.update_yaxes(showgrid=False, ticksuffix=' ', categoryorder='total ascending', row=1, col=1)\nfig.update_xaxes(visible=False, row=1, col=1)\nfig.update_xaxes(tickmode = 'array', tickvals=df_cl_sa.cluster, ticktext=[i for i in range(1,17)], row=2, col=1)\nfig.update_yaxes(visible=False, row=2, col=1)\nfig.update_layout(height=500, bargap=0.2,\n                  margin=dict(b=0,r=20,l=20), xaxis=dict(tickmode='linear'),\n                  title_text=\"Average Sales Analysis\",\n                  template=\"plotly_white\",\n                  title_font=dict(size=25, color='#8a8d93', family=\"Lato, sans-serif\"),\n                  font=dict(color='#8a8d93'),\n                  hoverlabel=dict(bgcolor=\"#f2f2f2\", font_size=13, font_family=\"Lato, sans-serif\"),\n                  showlegend=False)\nfig.show()","67de279e":"# data \ndf_2013 = df_train[df_train['year']==2013][['month','sales']]\ndf_2013 = df_2013.groupby('month').agg({\"sales\" : \"mean\"}).reset_index().rename(columns={'sales':'s13'})\ndf_2014 = df_train[df_train['year']==2014][['month','sales']]\ndf_2014 = df_2014.groupby('month').agg({\"sales\" : \"mean\"}).reset_index().rename(columns={'sales':'s14'})\ndf_2015 = df_train[df_train['year']==2015][['month','sales']]\ndf_2015 = df_2015.groupby('month').agg({\"sales\" : \"mean\"}).reset_index().rename(columns={'sales':'s15'})\ndf_2016 = df_train[df_train['year']==2016][['month','sales']]\ndf_2016 = df_2016.groupby('month').agg({\"sales\" : \"mean\"}).reset_index().rename(columns={'sales':'s16'})\ndf_2017 = df_train[df_train['year']==2017][['month','sales']]\ndf_2017 = df_2017.groupby('month').agg({\"sales\" : \"mean\"}).reset_index()\ndf_2017_no = pd.DataFrame({'month': [9,10,11,12], 'sales':[0,0,0,0]})\ndf_2017 = df_2017.append(df_2017_no).rename(columns={'sales':'s17'})\ndf_year = df_2013.merge(df_2014,on='month').merge(df_2015,on='month').merge(df_2016,on='month').merge(df_2017,on='month')\n\n# top levels\ntop_labels = ['2013', '2014', '2015', '2016', '2017']\n\ncolors = ['rgba(38, 24, 74, 0.8)', 'rgba(71, 58, 131, 0.8)',\n          'rgba(122, 120, 168, 0.8)', 'rgba(164, 163, 204, 0.85)',\n          'rgba(190, 192, 213, 1)']\n\n# X axis value \ndf_year = df_year[['s13','s14','s15','s16','s17']].replace(np.nan,0)\nx_data = df_year.values\n\n# y axis value (Month)\ndf_2013['month'] =['1M','2M','3M','4M','5M','6M','7M','8M','9M','10M','11M','12M']\ny_data = df_2013['month'].tolist()\n\nfig = go.Figure()\nfor i in range(0, len(x_data[0])):\n    for xd, yd in zip(x_data, y_data):\n        fig.add_trace(go.Bar(\n            x=[xd[i]], y=[yd],\n            orientation='h',\n            marker=dict(\n                color=colors[i],\n                line=dict(color='rgb(248, 248, 249)', width=1)\n            )\n        ))\n\nfig.update_layout(title='Avg Sales for each Year',\n    xaxis=dict(showgrid=False, \n               zeroline=False, domain=[0.15, 1]),\n    yaxis=dict(showgrid=False, showline=False,\n               showticklabels=False, zeroline=False),\n    barmode='stack', #barnorm='percent',\n    plot_bgcolor='#fff', paper_bgcolor='#fff',\n    margin=dict(l=0, r=50, t=100, b=10),\n    showlegend=False, \n)\n\nannotations = []\nfor yd, xd in zip(y_data, x_data):\n    # labeling the y-axis\n    annotations.append(dict(xref='paper', yref='y',\n                            x=0.14, y=yd,\n                            xanchor='right',\n                            text=str(yd),\n                            font=dict(family='Arial', size=14,\n                                      color='rgb(67, 67, 67)'),\n                            showarrow=False, align='right'))\n    # labeling the first Likert scale (on the top)\n    if yd == y_data[-1]:\n        annotations.append(dict(xref='x', yref='paper',\n                                x=xd[0] \/ 2, y=1.1,\n                                text=top_labels[0],\n                                font=dict(family='Arial', size=14,\n                                          color='rgb(67, 67, 67)'),\n                          showarrow=False))\n    space = xd[0]\n    for i in range(1, len(xd)):\n            # labeling the Likert scale\n            if yd == y_data[-1]:\n                annotations.append(dict(xref='x', yref='paper',\n                                        x=space + (xd[i]\/2), y=1.1,\n                                        text=top_labels[i],\n                                        font=dict(family='Arial', size=14,\n                                                  color='rgb(67, 67, 67)'),\n                                        showarrow=False))\n            space += xd[i]\nfig.update_layout(\n    annotations=annotations)\nfig.show()","b947221f":"# data\ndf_train['week'] = df_train['date'].dt.isocalendar().week\ndf_m_sa = df_train.groupby('month').agg({\"sales\" : \"mean\"}).reset_index()\ndf_m_sa['sales'] = round(df_m_sa['sales'],2)\ndf_m_sa['month_text'] = df_m_sa['month'].apply(lambda x: calendar.month_abbr[x])\ndf_m_sa['text'] = df_m_sa['month_text'] + ' - ' + df_m_sa['sales'].astype(str) \n\ndf_w_sa = df_train.groupby('week').agg({\"sales\" : \"mean\"}).reset_index() \ndf_q_sa = df_train.groupby('quarter').agg({\"sales\" : \"mean\"}).reset_index() \n# chart color\ndf_m_sa['color'] = '#496595'\ndf_m_sa['color'][:-1] = '#c6ccd8'\ndf_w_sa['color'] = '#c6ccd8'\n\n# chart\nfig = make_subplots(rows=2, cols=2, vertical_spacing=0.08,\n                    row_heights=[0.7, 0.3], \n                    specs=[[{\"type\": \"bar\"}, {\"type\": \"pie\"}],\n                           [{\"colspan\": 2}, None]],\n                    column_widths=[0.7, 0.3],\n                    subplot_titles=(\"Month wise Avg Sales Analysis\", \"Quarter wise Avg Sales Analysis\", \n                                    \"Week wise Avg Sales Analysis\"))\n\nfig.add_trace(go.Bar(x=df_m_sa['sales'], y=df_m_sa['month'], marker=dict(color= df_m_sa['color']),\n                     text=df_m_sa['text'],textposition='auto',\n                     name='Month', orientation='h'), \n                     row=1, col=1)\nfig.add_trace(go.Pie(values=df_q_sa['sales'], labels=df_q_sa['quarter'], name='Quarter',\n                     marker=dict(colors=['#334668','#496595','#6D83AA','#91A2BF','#C8D0DF']), hole=0.7,\n                     hoverinfo='label+percent+value', textinfo='label+percent'), \n                     row=1, col=2)\nfig.add_trace(go.Scatter(x=df_w_sa['week'], y=df_w_sa['sales'], mode='lines+markers', fill='tozeroy', fillcolor='#c6ccd8',\n                     marker=dict(color= '#496595'), name='Week'), \n                     row=2, col=1)\n\n# styling\nfig.update_yaxes(visible=False, row=1, col=1)\nfig.update_xaxes(visible=False, row=1, col=1)\nfig.update_xaxes(tickmode = 'array', tickvals=df_w_sa.week, ticktext=[i for i in range(1,53)], \n                 row=2, col=1)\nfig.update_yaxes(visible=False, row=2, col=1)\nfig.update_layout(height=750, bargap=0.15,\n                  margin=dict(b=0,r=20,l=20), \n                  title_text=\"Average Sales Analysis\",\n                  template=\"plotly_white\",\n                  title_font=dict(size=25, color='#8a8d93', family=\"Lato, sans-serif\"),\n                  font=dict(color='#8a8d93'),\n                  hoverlabel=dict(bgcolor=\"#f2f2f2\", font_size=13, font_family=\"Lato, sans-serif\"),\n                  showlegend=False)\nfig.show()","56fae729":"# data\ndf_train['year'] = df_train['date'].dt.year\ndf_train['month'] = df_train['date'].dt.month\ndf_train['week'] = df_train['date'].dt.isocalendar().week\ndf_train['quarter'] = df_train['date'].dt.quarter\ndf_train['day_of_week'] = df_train['date'].dt.day_name()\ndf_dw_sa = df_train.groupby('day_of_week').agg({\"sales\" : \"mean\"}).reset_index()\ndf_dw_sa.sales = round(df_dw_sa.sales, 2)\n\n# chart\nfig = px.bar(df_dw_sa, y='day_of_week', x='sales', title='Avg Sales vs Day of Week',\n             color_discrete_sequence=['#c6ccd8'], text='sales',\n             category_orders=dict(day_of_week=[\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\", \"Friday\",\"Saturday\",\"Sunday\"]))\nfig.update_yaxes(showgrid=False, ticksuffix=' ', showline=False)\nfig.update_xaxes(visible=False)\nfig.update_layout(margin=dict(t=60, b=0, l=0, r=0), height=350,\n                  hovermode=\"y unified\", \n                  yaxis_title=\" \", template='plotly_white',\n                  title_font=dict(size=25, color='#8a8d93', family=\"Lato, sans-serif\"),\n                  font=dict(color='#8a8d93'),\n                  hoverlabel=dict(bgcolor=\"#c6ccd8\", font_size=13, font_family=\"Lato, sans-serif\"))","1a1f82fc":"ax = df_oil.set_index('date').plot(figsize = (16, 8))\nax.set_xlabel('Date', fontsize = 'large')\nax.set_ylabel(\"Crude Oil\", fontsize = 'large')","7d0ccaf1":"avg_sales = df_train.groupby('date').agg({'sales': 'mean'}).reset_index()\n#daily_avg_sales['weekly_avg_sales'] = daily_avg_sales['sales'].rolling(window=7).mean()\navg_sales['weekly_avg_sales'] = avg_sales['sales'].ewm(span=7, adjust=False).mean()\n#ax = daily_avg_sales.set_index('date').plot(figsize = (16, 8))\nax1 = avg_sales.plot(x= 'date', y= ['sales', 'weekly_avg_sales'], figsize=(18,6))\n\navg_transactions = df_taxation.groupby('date').agg({'transactions': 'mean'}).reset_index()\n#avg_transaction['weekly_avg_sales'] = avg_transaction['transactions'].rolling(window=7).mean()\navg_transactions['weekly_avg_transactions'] = avg_transactions['transactions'].ewm(span=7, adjust=False).mean()\n\nax2 = avg_transactions.plot(x= 'date', y= ['transactions', 'weekly_avg_transactions'], figsize=(18,6))","99c87c12":"df_train.columns","c3d2a67d":"df_test.columns","c4e84422":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","a9d3417d":"a = df_train[[\"store_nbr\", \"sales\"]]\na[\"ind\"] = 1\na[\"ind\"] = a.groupby(\"store_nbr\").ind.cumsum().values\na = pd.pivot(a, index = \"ind\", columns = \"store_nbr\", values = \"sales\").corr()\nmask = np.triu(a.corr())\nplt.figure(figsize=(20, 20))\nsns.heatmap(a,\n        annot=True,\n        fmt='.1f',\n        cmap='coolwarm',\n        square=True,\n        mask=mask,\n        linewidths=1,\n        cbar=False)\nplt.title(\"Correlations among stores\",fontsize = 20)\nplt.show()","1f8df612":"a = df_train.set_index(\"date\").groupby(\"store_nbr\").resample(\"D\").sales.sum().reset_index()\npx.line(a, x = \"date\", y= \"sales\", color = \"store_nbr\", title = \"Daily total sales of the stores\")\n","d759db06":"#check sales zero\na = df_train.set_index(\"date\").groupby(\"store_nbr\").resample(\"D\").sales.sum().reset_index()\npx.line(a, x = \"date\", y= \"sales\", color = \"store_nbr\", title = \"Daily total sales of the stores\")\n","418ad1d7":"# PACF - ACF\n# ------------------------------------------------------\nimport statsmodels.api as sm\n\n# DATA VISUALIZATION\n# ------------------------------------------------------\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n","4a22b2ae":"df_taxation.date = pd.to_datetime(df_taxation.date)","595f291a":"temp = pd.merge(df_train.groupby([\"date\", \"store_nbr\"]).sales.sum().reset_index(), df_taxation, how = \"left\")\nprint(\"Spearman Correlation between Total Sales and Transactions: {:,.4f}\".format(temp.corr(\"spearman\").sales.loc[\"transactions\"]))\n#px.line(transactions.sort_values([\"store_nbr\", \"date\"]), x='date', y='transactions', color='store_nbr',title = \"Transactions\" )","12499d77":"a = df_taxation.copy()\na[\"year\"] = a.date.dt.year\na[\"month\"] = a.date.dt.month\npx.box(a, x=\"year\", y=\"transactions\" , color = \"month\", title = \"Transactions\")","b8c27167":"\na = df_taxation.set_index(\"date\").resample(\"M\").transactions.mean().reset_index()\na[\"year\"] = a.date.dt.year\npx.line(a, x='date', y='transactions', color='year',title = \"Monthly Average Transactions\" )","2d8baf62":"px.scatter(temp, x = \"transactions\", y = \"sales\", trendline = \"ols\", trendline_color_override = \"red\")","c97744f0":"\na = df_taxation.copy()\na[\"year\"] = a.date.dt.year\na[\"dayofweek\"] = a.date.dt.dayofweek+1\na = a.groupby([\"year\", \"dayofweek\"]).transactions.mean().reset_index()\npx.line(a, x=\"dayofweek\", y=\"transactions\" , color = \"year\", title = \"Transactions\")","8ae09117":"df_oil[\"date\"] = pd.to_datetime(df_oil.date)\n# Resample\ndf_oil = df_oil.set_index(\"date\").dcoilwtico.resample(\"D\").sum().reset_index()\n# Interpolate\ndf_oil[\"dcoilwtico\"] = np.where(df_oil[\"dcoilwtico\"] == 0, np.nan, df_oil[\"dcoilwtico\"])\ndf_oil[\"dcoilwtico_interpolated\"] = df_oil.dcoilwtico.interpolate()\n# Plot\np = df_oil.melt(id_vars=['date']+list(df_oil.keys()[5:]), var_name='Legend')\npx.line(p.sort_values([\"Legend\", \"date\"], ascending = [False, True]), x='date', y='value', color='Legend',title = \"Daily Oil Price\" )","8b8f3123":"temp.head()","b93a006b":"df_oil.head()","3a0d2c16":"df_stores.head()","004417ed":"df_taxation.head()","497d69bb":"#correlation of sales and taxation\ntemp = pd.merge(df_taxation, df_oil, how = \"left\")\nprint(\"Correlation with Daily Oil Prices\")\nprint(temp.drop([\"store_nbr\", \"dcoilwtico\"], axis = 1).corr(\"spearman\").dcoilwtico_interpolated.loc[[\"transactions\"]], \"\\n\")\n\n\nfig, axes = plt.subplots(1, 2, figsize = (15,5))\ntemp.plot.scatter(x = \"dcoilwtico_interpolated\", y = \"transactions\", ax=axes[0])\n#temp.plot.scatter(x = \"dcoilwtico_interpolated\", y = \"sales\", ax=axes[1], color = \"r\")\naxes[0].set_title('Daily oil price & Transactions', fontsize = 15)\n#axes[1].set_title('Daily Oil Price & Sales', fontsize = 15);","cdc1e6bf":"# Let's apply an AB test to Events and Holidays features. Are they statistically significant? Also it can be a good way for first feature selection.\n\n# H0: The sales are equal (M1 = M2)\n# H1: The sales are not equal (M1 != M2)","ecf5a8a8":"#holidays = pd.read_csv(\"..\/input\/store-sales-time-series-forecasting\/holidays_events.csv\")\ndf_holidays[\"date\"] = pd.to_datetime(df_holidays.date)\n\nholidays = df_holidays.copy()\n# holidays[holidays.type == \"Holiday\"]\n# holidays[(holidays.type == \"Holiday\") & (holidays.transferred == True)]\n\n# Transferred Holidays\ntr1 = holidays[(holidays.type == \"Holiday\") & (holidays.transferred == True)].drop(\"transferred\", axis = 1).reset_index(drop = True)\ntr2 = holidays[(holidays.type == \"Transfer\")].drop(\"transferred\", axis = 1).reset_index(drop = True)\ntr = pd.concat([tr1,tr2], axis = 1)\ntr = tr.iloc[:, [5,1,2,3,4]]\n\nholidays = holidays[(holidays.transferred == False) & (holidays.type != \"Transfer\")].drop(\"transferred\", axis = 1)\nholidays = holidays.append(tr).reset_index(drop = True)\n\n\n# Additional Holidays\nholidays[\"description\"] = holidays[\"description\"].str.replace(\"-\", \"\").str.replace(\"+\", \"\").str.replace('\\d+', '')\nholidays[\"type\"] = np.where(holidays[\"type\"] == \"Additional\", \"Holiday\", holidays[\"type\"])\n\n# Bridge Holidays\nholidays[\"description\"] = holidays[\"description\"].str.replace(\"Puente \", \"\")\nholidays[\"type\"] = np.where(holidays[\"type\"] == \"Bridge\", \"Holiday\", holidays[\"type\"])\n\n \n# Work Day Holidays, that is meant to payback the Bridge.\nwork_day = holidays[holidays.type == \"Work Day\"]  \nholidays = holidays[holidays.type != \"Work Day\"]  \n\n\n# Split\n\n# Events are national\nevents = holidays[holidays.type == \"Event\"].drop([\"type\", \"locale\", \"locale_name\"], axis = 1).rename({\"description\":\"events\"}, axis = 1)\n\nholidays = holidays[holidays.type != \"Event\"].drop(\"type\", axis = 1)\nregional = holidays[holidays.locale == \"Regional\"].rename({\"locale_name\":\"state\", \"description\":\"holiday_regional\"}, axis = 1).drop(\"locale\", axis = 1).drop_duplicates()\nnational = holidays[holidays.locale == \"National\"].rename({\"description\":\"holiday_national\"}, axis = 1).drop([\"locale\", \"locale_name\"], axis = 1).drop_duplicates()\nlocal = holidays[holidays.locale == \"Local\"].rename({\"description\":\"holiday_local\", \"locale_name\":\"city\"}, axis = 1).drop(\"locale\", axis = 1).drop_duplicates()\n\n\n\nd = pd.merge(df_train.append(df_test), df_stores)\nd[\"store_nbr\"] = d[\"store_nbr\"].astype(\"int8\")\n\n\n# National Holidays & Events\n#d = pd.merge(d, events, how = \"left\")\nd = pd.merge(d, national, how = \"left\")\n# Regional\nd = pd.merge(d, regional, how = \"left\", on = [\"date\", \"state\"])\n# Local\nd = pd.merge(d, local, how = \"left\", on = [\"date\", \"city\"])\n\n# Work Day: It will be removed when real work day colum created\nd = pd.merge(d,  work_day[[\"date\", \"type\"]].rename({\"type\":\"IsWorkDay\"}, axis = 1),how = \"left\")\n\n# EVENTS\nevents[\"events\"] =np.where(events.events.str.contains(\"futbol\"), \"Futbol\", events.events)\n\ndef one_hot_encoder(df, nan_as_category=True):\n    original_columns = list(df.columns)\n    categorical_columns = df.select_dtypes([\"category\", \"object\"]).columns.tolist()\n    # categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n    new_columns = [c for c in df.columns if c not in original_columns]\n    df.columns = df.columns.str.replace(\" \", \"_\")\n    return df, df.columns.tolist()\n\nevents, events_cat = one_hot_encoder(events, nan_as_category=False)\nevents[\"events_Dia_de_la_Madre\"] = np.where(events.date == \"2016-05-08\", 1,events[\"events_Dia_de_la_Madre\"])\nevents = events.drop(239)\n\nd = pd.merge(d, events, how = \"left\")\nd[events_cat] = d[events_cat].fillna(0)\n\n# New features\nd[\"holiday_national_binary\"] = np.where(d.holiday_national.notnull(), 1, 0)\nd[\"holiday_local_binary\"] = np.where(d.holiday_local.notnull(), 1, 0)\nd[\"holiday_regional_binary\"] = np.where(d.holiday_regional.notnull(), 1, 0)\n\n# \nd[\"national_independence\"] = np.where(d.holiday_national.isin(['Batalla de Pichincha',  'Independencia de Cuenca', 'Independencia de Guayaquil', 'Independencia de Guayaquil', 'Primer Grito de Independencia']), 1, 0)\nd[\"local_cantonizacio\"] = np.where(d.holiday_local.str.contains(\"Cantonizacio\"), 1, 0)\nd[\"local_fundacion\"] = np.where(d.holiday_local.str.contains(\"Fundacion\"), 1, 0)\nd[\"local_independencia\"] = np.where(d.holiday_local.str.contains(\"Independencia\"), 1, 0)\n\n\nholidays, holidays_cat = one_hot_encoder(d[[\"holiday_national\",\"holiday_regional\",\"holiday_local\"]], nan_as_category=False)\nd = pd.concat([d.drop([\"holiday_national\",\"holiday_regional\",\"holiday_local\"], axis = 1),holidays], axis = 1)\n\nhe_cols = d.columns[d.columns.str.startswith(\"events\")].tolist() + d.columns[d.columns.str.startswith(\"holiday\")].tolist() + d.columns[d.columns.str.startswith(\"national\")].tolist()+ d.columns[d.columns.str.startswith(\"local\")].tolist()\nd[he_cols] = d[he_cols].astype(\"int8\")\n\nd[[\"family\", \"city\", \"state\", \"type\"]] = d[[\"family\", \"city\", \"state\", \"type\"]].astype(\"category\")\n\ndel holidays, holidays_cat, work_day, local, regional, national, events, events_cat, tr, tr1, tr2, he_cols\ngc.collect()\n\nd.head(10)","8ecef12a":"def AB_Test(dataframe, group, target):\n    \n    # Packages\n    from scipy.stats import shapiro\n    import scipy.stats as stats\n    \n    # Split A\/B\n    groupA = dataframe[dataframe[group] == 1][target]\n    groupB = dataframe[dataframe[group] == 0][target]\n    \n    # Assumption: Normality\n    ntA = shapiro(groupA)[1] < 0.05\n    ntB = shapiro(groupB)[1] < 0.05\n    # H0: Distribution is Normal! - False\n    # H1: Distribution is not Normal! - True\n    \n    if (ntA == False) & (ntB == False): # \"H0: Normal Distribution\"\n        # Parametric Test\n        # Assumption: Homogeneity of variances\n        leveneTest = stats.levene(groupA, groupB)[1] < 0.05\n        # H0: Homogeneity: False\n        # H1: Heterogeneous: True\n        \n        if leveneTest == False:\n            # Homogeneity\n            ttest = stats.ttest_ind(groupA, groupB, equal_var=True)[1]\n            # H0: M1 == M2 - False\n            # H1: M1 != M2 - True\n        else:\n            # Heterogeneous\n            ttest = stats.ttest_ind(groupA, groupB, equal_var=False)[1]\n            # H0: M1 == M2 - False\n            # H1: M1 != M2 - True\n    else:\n        # Non-Parametric Test\n        ttest = stats.mannwhitneyu(groupA, groupB)[1] \n        # H0: M1 == M2 - False\n        # H1: M1 != M2 - True\n        \n    # Result\n    temp = pd.DataFrame({\n        \"AB Hypothesis\":[ttest < 0.05], \n        \"p-value\":[ttest]\n    })\n    temp[\"Test Type\"] = np.where((ntA == False) & (ntB == False), \"Parametric\", \"Non-Parametric\")\n    temp[\"AB Hypothesis\"] = np.where(temp[\"AB Hypothesis\"] == False, \"Fail to Reject H0\", \"Reject H0\")\n    temp[\"Comment\"] = np.where(temp[\"AB Hypothesis\"] == \"Fail to Reject H0\", \"A\/B groups are similar!\", \"A\/B groups are not similar!\")\n    temp[\"Feature\"] = group\n    temp[\"GroupA_mean\"] = groupA.mean()\n    temp[\"GroupB_mean\"] = groupB.mean()\n    temp[\"GroupA_median\"] = groupA.median()\n    temp[\"GroupB_median\"] = groupB.median()\n    \n    # Columns\n    if (ntA == False) & (ntB == False):\n        temp[\"Homogeneity\"] = np.where(leveneTest == False, \"Yes\", \"No\")\n        temp = temp[[\"Feature\",\"Test Type\", \"Homogeneity\",\"AB Hypothesis\", \"p-value\", \"Comment\", \"GroupA_mean\", \"GroupB_mean\", \"GroupA_median\", \"GroupB_median\"]]\n    else:\n        temp = temp[[\"Feature\",\"Test Type\",\"AB Hypothesis\", \"p-value\", \"Comment\", \"GroupA_mean\", \"GroupB_mean\", \"GroupA_median\", \"GroupB_median\"]]\n    \n    # Print Hypothesis\n    # print(\"# A\/B Testing Hypothesis\")\n    # print(\"H0: A == B\")\n    # print(\"H1: A != B\", \"\\n\")\n    \n    return temp\n    \n# Apply A\/B Testing\nhe_cols = d.columns[d.columns.str.startswith(\"events\")].tolist() + d.columns[d.columns.str.startswith(\"holiday\")].tolist() + d.columns[d.columns.str.startswith(\"national\")].tolist()+ d.columns[d.columns.str.startswith(\"local\")].tolist()\nab = []\nfor i in he_cols:\n    ab.append(AB_Test(dataframe=d[d.sales.notnull()], group = i, target = \"sales\"))\nab = pd.concat(ab)\nab","3a71383c":"Combine the oil , holiday, stores, taxation etc to train and test to make a combined dataset","7008ba1d":"EDA","90079c8a":"Importing "}}