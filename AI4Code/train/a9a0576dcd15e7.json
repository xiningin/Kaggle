{"cell_type":{"58df33f2":"code","eaff3604":"code","0e781eb7":"code","06ecf0a8":"code","dc9d19c3":"code","e5d57f20":"code","48965298":"code","089f2148":"code","77aadf8d":"code","d3f9ae11":"code","7fdb17c9":"code","db8e4b94":"code","1b237f83":"code","ef19cfe0":"code","7f6994f3":"code","4c1452ee":"code","a1f73cbe":"code","443a7f26":"code","0874d975":"code","7f52e1dc":"code","6bb98287":"code","576ec40f":"code","e8badd30":"code","260e262d":"code","a75d55e5":"code","a65d15f8":"code","1b07d7bd":"code","ca072049":"code","2f86b5ad":"code","12c08df4":"code","d4bc6335":"code","b8e02b72":"code","c7ccb1e1":"code","dbded1fc":"code","05071964":"code","998a89ee":"code","140b8bae":"code","9e5afa51":"markdown","39b564fd":"markdown","a67c1b89":"markdown","a8f85166":"markdown","c01f913b":"markdown","dd1e3f29":"markdown"},"source":{"58df33f2":"from pandas import Series\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.feature_selection import f_classif, mutual_info_classif\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder, OneHotEncoder, StandardScaler\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import auc, roc_auc_score, roc_curve, recall_score, f1_score, log_loss, average_precision_score\nfrom sklearn.metrics import precision_score, accuracy_score, brier_score_loss, confusion_matrix\nimport pandas_profiling\nfrom pandas_profiling import ProfileReport","eaff3604":"DATA_DIR = '\/kaggle\/input\/sf-dst-scoring\/'\ntrain = pd.read_csv(DATA_DIR+'\/train.csv')\ntest = pd.read_csv(DATA_DIR+'\/test.csv')\nsample_submission = pd.read_csv(DATA_DIR+'\/sample_submission.csv')","0e781eb7":"#A function to show scores\ndef scores(y_test, y_pred, prob):\n    fpr, tpr, threshold = roc_curve(y_test, prob)\n    roc_auc = roc_auc_score(y_test, prob)\n    \n    plt.figure()\n    plt.plot([0, 1], label='Baseline', linestyle='--')\n    plt.plot(fpr, tpr, label = 'Regression')\n    plt.title('Logistic Regression ROC AUC = %0.3f' % roc_auc)\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.legend(loc = 'lower right')\n    plt.show()\n    print (\"f1_score:\",round(f1_score(y_test,y_pred), 3))\n    print (\"accuracy_score:\",round(accuracy_score(y_test,y_pred), 3))\n    print (\"precision_score:\",round(precision_score(y_test,y_pred), 3))\n    print (\"recall_score:\",round(recall_score(y_test,y_pred), 3))\n    print (\"log_loss:\",round(log_loss(y_test,y_pred), 3))\n    print (\"roc_auc_score:\",round(roc_auc, 3))\n    print(\"average_precision_score:\", round(average_precision_score(y_test,y_pred), 3))\n    print(\"brier_score_loss:\", round(brier_score_loss(y_test,y_pred), 3))\n    print('Confusion matrix:\\n{}' .format(confusion_matrix(y_test,y_pred)))","06ecf0a8":"#Let's have a quick look at the data\ntrain.info()","dc9d19c3":"test.info()","e5d57f20":"#Let's prepare train data for naive model\ntrain_naive = train.copy()\ntrain_naive.dropna(inplace = True)\nord_enc = OrdinalEncoder()\ntrain_naive.drop(['client_id','app_date'],axis=1,inplace=True)\nobj_columns = train_naive.select_dtypes(include=['object']).columns\ntrain_naive[obj_columns] = ord_enc.fit_transform(train_naive[obj_columns]).astype(int)\ntrain_naive.head(5)","48965298":"#The naive model itself\nmodel = LogisticRegression()\nX = train_naive.drop(['default'], axis = 1).values\ny = train_naive['default'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nproba = model.predict_proba(X_test)[:, 1]","089f2148":"#And it's metrics\nscores(y_test, y_pred, proba)","77aadf8d":"#It is time to unite datasets. The new feature \"test\" will be used later in order to divide them. \n#And the \"default\" feature will now be added to test dataset.\ntest['test'] = 1 \ntrain['test'] = 0 \ntest['default'] = 0 \ndata = train.append(test, sort = False).reset_index(drop=True) \ndata.info()","d3f9ae11":"#Let's have a little more careful look\nprofile = ProfileReport(data)\nprofile","7fdb17c9":"sns.countplot(data['education'])","db8e4b94":"#The most frequent value is \"SCH\". Let's get rid of the missing values.\ndata[data['education'].isnull()] = data[data['education'].isnull()].fillna('SCH')\ndata.info()","1b237f83":"#\"app_date\" now must be converted to a proper format and encoded.\ndata['app_date'] = data['app_date'].apply(lambda x: datetime.datetime.strptime(x,'%d%b%Y'))\ndata['app_date'] = data['app_date'].apply(lambda x: (data['app_date'].max() - x).days)\ndata.head(5)","ef19cfe0":"#Now let's encode all the object columns  \nord_enc = OrdinalEncoder()\nobj_columns = data.select_dtypes(include=['object']).columns\ndata[obj_columns] = ord_enc.fit_transform(data[obj_columns]).astype(int)\ndata.head(5)","7f6994f3":"# Let's look at feature's distribution\nplt.figure()\nsns.distplot(data['age'])","4c1452ee":"# Does not look good enough. let's log it.\nplt.figure()\nsns.distplot(data['age'].apply(lambda x: np.log(x+1)))","a1f73cbe":"#Looks better. Let's keep it this way.\ndata['age'] = data['age'].apply(lambda x: np.log(x+1))","443a7f26":"plt.figure()\nsns.distplot(data['score_bki'])","0874d975":"#Good enough. So be it.","7f52e1dc":"plt.figure()\nsns.distplot(data['income'])","6bb98287":"plt.figure()\nsns.distplot(data['income'].apply(lambda x: np.log(x+1)))","576ec40f":"#That's better\ndata['income'] = data['income'].apply(lambda x: np.log(x+1))","e8badd30":"plt.figure()\nsns.distplot(data['decline_app_cnt'])","260e262d":"#All the same.\nplt.figure()\nsns.distplot(data['decline_app_cnt'].apply(lambda x: np.log(x+1)))\ndata['decline_app_cnt'] = data['decline_app_cnt'].apply(lambda x: np.log(x+1))","a75d55e5":"plt.figure()\nsns.distplot(data['bki_request_cnt'])","a65d15f8":"plt.figure()\nsns.distplot(data['bki_request_cnt'].apply(lambda x: np.log(x+1)))\ndata['bki_request_cnt'] = data['bki_request_cnt'].apply(lambda x: np.log(x+1))","1b07d7bd":"plt.figure()\nsns.distplot(data['app_date'])","ca072049":"#Let's see the numerical feature's significance\nnum_cols = ['age','decline_app_cnt','score_bki','income','bki_request_cnt','app_date']\nimp_num = pd.Series(f_classif(data[data['test'] == 0][num_cols], data[data['test'] == 0]['default'])[0], index = num_cols)\nimp_num.sort_values(inplace = True)\nimp_num.plot(kind = 'barh')","2f86b5ad":"#And the cat\/bin ones\ncat_cols = ['education','sex','car','car_type','good_work','region_rating','home_address','work_address','sna','first_time',\n'foreign_passport']\nimp_cat = Series(mutual_info_classif(data[data['test'] == 0][cat_cols], data[data['test'] == 0]['default'],\n                                     discrete_features =True), index = cat_cols)\nimp_cat.sort_values(inplace = True)\nimp_cat.plot(kind = 'barh')","12c08df4":"#With them\nmodel = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42, class_weight='balanced')\nX = data[data['test']==0].drop(['default','test','client_id'], axis = 1).values\ny = data[data['test']==0]['default'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nproba = model.predict_proba(X_test)[:, 1]\nscores(y_test, y_pred, proba)","d4bc6335":"#And without them\nmodel = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42, class_weight='balanced')\nX = data[data['test']==0].drop(['default','age','sex','test','client_id'], axis = 1).values\ny = data[data['test']==0]['default'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nproba = model.predict_proba(X_test)[:, 1]\nscores(y_test, y_pred, proba)","b8e02b72":"#Both look almost the same. I'll get rid of the two freatures.\ndata.drop(['age','sex'], axis = 1, inplace = True)","c7ccb1e1":"#Now I'll try making dummies of the cat columns\ncat_cols = ['education','home_address','work_address','sna','first_time']\ndata_w_dummies = pd.get_dummies(data, columns=cat_cols)\nmodel = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42, class_weight='balanced')\nX = data_w_dummies[data_w_dummies['test']==0].drop(['default','test','client_id'], axis = 1).values\ny = data_w_dummies[data_w_dummies['test']==0]['default'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nproba = model.predict_proba(X_test)[:, 1]\nscores(y_test, y_pred, proba)","dbded1fc":"#Trying hyperparameters\npenalty = ['l1', 'l2']\nC = np.logspace(0, 4, 10)\nhyperparameters = dict(C=C, penalty=penalty)\nmodel = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42, class_weight='balanced')\nmodel.fit(X_train, y_train)\nclf = GridSearchCV(model, hyperparameters, cv=5, verbose=0)\nbest_model = clf.fit(X_train, y_train)\nprint('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\nprint('Best C:', best_model.best_estimator_.get_params()['C'])","05071964":"model = LogisticRegression(penalty='l1',C=1.0,solver='liblinear', max_iter=1000, random_state=42, class_weight='balanced').fit(X_train, y_train)\nX = data_w_dummies[data_w_dummies['test']==0].drop(['default','test','client_id'], axis = 1).values\ny = data_w_dummies[data_w_dummies['test']==0]['default'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nproba = model.predict_proba(X_test)[:, 1]\nscores(y_test, y_pred, proba)","998a89ee":"#So let's split the whole data and make the submission\ndata_train = data[data['test'] == 0].drop(['test'],axis=1)\ndata_test = data[data['test'] == 1].drop(['test'],axis=1)\nX_train = data_train.drop(['default'], axis = 1)\ny_train = data_train['default']\nX_test = data_test.drop(['default'], axis=1)","140b8bae":"#Submission\nmodel = LogisticRegression(solver='liblinear', max_iter=1000, random_state=42, class_weight='balanced')\nmodel.fit(X_train, y_train)\ny_pred = model.predict_proba(X_test)[:,1]\nsubmission = pd.DataFrame(data={'client_id':data_test['client_id'], 'default':y_pred})\nsubmission.to_csv('submission.csv', index=False)\nsubmission","9e5afa51":"This one is ok","39b564fd":"Now I have to:  \n1.Replace the missing values in \"education\" with the most frequent one.  \n2.Convert \"app_date\" to a proper datetime format.  \n3.Encode all the object columns.  \n(These are \"app_date\",\"education\",\"sex\",\"car\".\"car_type\",\"foreign_passport\" )  ","a67c1b89":"'age' and 'sex' seem to be of lowest significance. I'll try the model with an without them","a8f85166":"Strange, but it looks better without dummies.","c01f913b":"There are missing values in \"education' and some columns containing object type values.","dd1e3f29":"Again it looks the same"}}