{"cell_type":{"9f1cbb98":"code","87d5b796":"code","8294c03e":"code","fa369d27":"code","84cd11d1":"code","062aa9a4":"code","3afb92d6":"code","f5a97931":"code","4cdf088c":"code","4f63582c":"code","a0ff5f40":"code","9e533619":"code","dbd26aef":"code","d18a3c14":"code","f5ceaf7a":"code","73aae1dc":"code","be705387":"code","8c121651":"code","1e01f198":"code","2f6dc8d5":"code","b7f77320":"code","c38e4a09":"code","6d77058b":"code","ceee0967":"code","c74cdb4d":"code","a4412daa":"code","f96a3bcb":"code","9f73afd8":"markdown","9774ab0b":"markdown","6e63f6c4":"markdown","ae4a8706":"markdown","611ac54d":"markdown","9b1c4500":"markdown","1424e466":"markdown","bd2d55a1":"markdown","f8a620a2":"markdown","8fbf8629":"markdown"},"source":{"9f1cbb98":"from scipy.stats import pearsonr\n\nx=[2,1,1,1,1,1]\ny=[2,1,3,4,5,6]\nz,_=pearsonr(x,y)\nz","87d5b796":"debug = 1","8294c03e":"import matplotlib.pyplot as plt","fa369d27":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","84cd11d1":"#%%timeit\nss=pd.read_csv('..\/input\/kkb-repl4nlp-assignment0\/sts-kaggle-sample_submission.csv')\ntrain=pd.read_csv('..\/input\/kkb-repl4nlp-assignment0\/sts-kaggle-train.csv')\ntest=pd.read_csv('..\/input\/kkb-repl4nlp-assignment0\/sts-kaggle-test.csv')\ntrain.head(5)","062aa9a4":"test.head(5)","3afb92d6":"print(train.info())","f5a97931":"train['len_sa']=train['sentence_a'].apply(lambda x : len(x))\ntrain['len_sb']=train['sentence_b'].apply(lambda x : len(x))\nprint(train[['len_sa','len_sb']].describe())","4cdf088c":"ix=train['len_sa'].idxmax()\nprint(train.loc[ix,'sentence_a'])\nprint(train.loc[ix,'sentence_b'])","4f63582c":"ix=train['len_sb'].idxmax()\nprint(train.loc[ix,'sentence_a'])\nprint(train.loc[ix,'sentence_b'])","a0ff5f40":"print('Class of Label = ')\nprint(len(train['similarity'].unique()))\nprint('Info of Label = ')\nprint(train['similarity'].describe())","9e533619":"train['counts']=1\ndtbt=train.drop('id',axis=1).groupby('similarity').sum()\ndtbt=dtbt.reset_index()\ndtbt.sort_values('similarity',inplace=True)\ndtbt","dbd26aef":"plt.figure(figsize=(10,10))\nplt.bar(list(dtbt.index),dtbt['counts'])\nplt.title('Distribution of Labels')\nplt.show()","d18a3c14":"# !pip install -U sentence_transformers # https:\/\/github.com\/UKPLab\/sentence-transformers","f5ceaf7a":"import nltk\nfrom scipy.spatial import distance\n\nimport fasttext\n# from sentence_transformers import SentenceTransformer\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\nfrom tqdm import tqdm","73aae1dc":"# from gensim.models import KeyedVectors\n# FASTTEXTFILE='..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec'\n# ft=KeyedVectors.load_word2vec_format(FASTTEXTFILE,limit=500000)","be705387":"test.drop('id',axis=1,inplace=True)\ntest.head(5)","8c121651":"#net = SentenceTransformer('bert-large-nli-stsb-mean-tokens') # 0.77\n#net = SentenceTransformer('paraphrase-distilroberta-base-v1')\u3000# 0.72\n#net = SentenceTransformer('stsb-roberta-large') # 0.76","1e01f198":"#Load AutoModel from huggingface model repository\n# tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers\/stsb-bert-large\")\n# model = AutoModel.from_pretrained(\"sentence-transformers\/stsb-bert-large\") # 0.77\n\n# tokenizer = AutoTokenizer.from_pretrained(\"binwang\/bert-large-nli-stsb\")\n# model = AutoModel.from_pretrained(\"binwang\/bert-large-nli-stsb\")  # 0.77\n\n# tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers\/roberta-large-nli-stsb-mean-tokens\")\n# model = AutoModel.from_pretrained(\"sentence-transformers\/roberta-large-nli-stsb-mean-tokens\")  # 0.76\n\ntokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers\/ce-roberta-large-stsb\")\nmodel = AutoModel.from_pretrained(\"sentence-transformers\/ce-roberta-large-stsb\")  # 0.077\n\n# #sentence-transformers\/stsb-roberta-large\n# cross-encoder\/stsb-roberta-large\n# sentence-transformers\/ce-roberta-large-stsb","2f6dc8d5":"def vec_similarity(vec1,vec2):\n    return ( 1 - distance.cosine(vec1,vec2))\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n    return sum_embeddings \/ sum_mask","b7f77320":"if debug:\n    \n    sentence1 = test.iloc[0,0]\n    sentence2 = test.iloc[0,1]\n    encoded_input1 = tokenizer(sentence1, padding=True, truncation=True, max_length=128, return_tensors='pt')\n    encoded_input2 = tokenizer(sentence2, padding=True, truncation=True, max_length=128, return_tensors='pt')\n    print(encoded_input1)\n    #Compute token embeddings\n    with torch.no_grad():\n        model_output1 = model(**encoded_input1)\n        model_output2 = model(**encoded_input2)\n     \n    print(model_output1)\n    print(len(model_output1))\n    print(model_output1[0].shape)\n    print(model_output1[1].shape)\n    #Perform pooling. In this case, mean pooling\n    sentence_embedding1 = mean_pooling(model_output1, encoded_input1['attention_mask'])\n    sentence_embedding2 = mean_pooling(model_output2, encoded_input2['attention_mask'])    \n    score = vec_similarity(sentence_embedding1,sentence_embedding2)\n    \n    print(type(sentence_embedding1 ))\n    print('\u53e5\u5411\u91cf\u7ef4\u5ea6 = ',sentence_embedding1.shape)\n    print('vec1 = ',sentence_embedding1)\n    print('vec2 = ', sentence_embedding2)\n    print('score = ',score)","c38e4a09":"w1_vecs = []\nw2_vecs = []\ns1_vecs = []\ns2_vecs = []","6d77058b":"res = []\nfor row in tqdm(range(len(test) )):\n    sentence1 = test.iloc[row,0]\n    sentence2 = test.iloc[row,1]\n\n    #Tokenize sentences\n    encoded_input1 = tokenizer(sentence1, padding=True, truncation=True, max_length=128, return_tensors='pt')\n    encoded_input2 = tokenizer(sentence2, padding=True, truncation=True, max_length=128, return_tensors='pt')\n    \n    #Compute token embeddings\n    with torch.no_grad():\n        model_output1 = model(**encoded_input1)\n        model_output2 = model(**encoded_input2)\n#     w1_vecs.append(model_output1[0] )\n#     w2_vecs.append(model_output1[1] )    \n    #Perform pooling. In this case, mean pooling\n    sentence_embedding1 = mean_pooling(model_output1, encoded_input1['attention_mask'])\n    sentence_embedding2 = mean_pooling(model_output2, encoded_input2['attention_mask'])\n#     s1_vecs.append(sentence_embedding1 )\n#     s2_vecs.append(sentence_embedding2 )\n#     vec1 = net.encode(test.iloc[row,0], show_progress_bar=False)\n#     vec2 = net.encode(test.iloc[row,1], show_progress_bar=False)     \n    score = vec_similarity(sentence_embedding1,sentence_embedding2)\n    res.append(score)\n    \nres","ceee0967":"# word_sentence_vecs = pd.DataFrame({'w1':w1_vecs,'w2':w2_vecs,'s1':s1_vecs,'s2':s2_vecs})\n# word_sentence_vecs.to_csv('word_sentence_vecs.csv')","c74cdb4d":"ss","a4412daa":"ss['similarity'] = [5 * i for i in res]\n#ss['similarity'] = res\nss.set_index('id',inplace=True)\nss.to_csv('submission.csv')","f96a3bcb":"ss","9f73afd8":"### sentence\u4fe1\u606f","9774ab0b":"# Pre-trained Model + Inference","6e63f6c4":"## \u7785\u4e00\u773c\u6570\u636e\u5427","ae4a8706":"### label\u4fe1\u606f","611ac54d":"test\u7684\u6807\u7b7e\u53ea\u67090~5\u8fd96\u4e2a\u6574\u6570\uff0c\u6240\u4ee5\u8fd9\u5e94\u8be5\u662f\u4e2a\u591a\u5206\u7c7b\u95ee\u9898\uff0c\u5e76\u975e\u8fde\u7eed\u503c\u7684\u56de\u5f52\u3002\n\u4e0d\u8981\u88abtrain\u7684\u6807\u7b7e\u9a97\u4e86","9b1c4500":"\u770b\u4e86\u4e00\u4e0b\u6700\u957f\u7684\u53e5\u5b50\uff0c\u4e0d\u77e5\u9053\u5404\u79cd\u6a21\u578b\u7684seqence length\u6700\u5927\u5230\u591a\u5c11\uff0c\u4e2a\u4eba\u611f\u89c9\u4ee5\u540e\u5fae\u8c03\u5e94\u8be5\u4f1a\u6709\u7528\u5427","1424e466":"# EDA","bd2d55a1":"sentencebert\u8bb2\u89e3\uff1a\nhttps:\/\/www.cnblogs.com\/shona\/p\/12026349.html\n\n\u8001\u5e08\u7ed9\u7684\u94fe\u63a5\uff1a\nhttps:\/\/github.com\/muralikrishnasn\/semantic_similarity\/blob\/master\/sts_similarities\/sbert.ipynb","f8a620a2":"# **\u76f4\u63a5\u7528SentenceBERT\u505a\u63a8\u7406**","8fbf8629":"similarity\u53d6\u503c\u4e3a 0~5\uff0c \u4f46\u662f\u6709140\u4e2a\u503c\uff0c\u4e0d\u77e5\u9053\u4eba\u5de5\u600e\u4e48\u6807\u7684\u70b9???\u83ab\u975e\u662f\u597d\u591a\u4eba\u4e00\u8d77\u6807\u6ce8\u7136\u540e\u53d6\u5747\u503c???"}}