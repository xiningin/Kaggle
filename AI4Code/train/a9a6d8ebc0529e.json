{"cell_type":{"5f4a6e3c":"code","df35b1d9":"code","7619d120":"code","c61deaca":"code","fa7c386d":"code","0a96ef52":"code","82cfa65f":"code","6b1951de":"code","77608f6f":"code","23364b66":"code","ae340e56":"code","069d922b":"code","a89b2a56":"code","b154c378":"code","a5d5f25f":"code","fba611f5":"code","0db8d41b":"code","da2c3f1c":"code","bc04ee56":"code","c6db8607":"code","8258b259":"code","6a6a5baa":"code","69e8a22e":"code","1b9fb34c":"code","b2111697":"code","3d44bba5":"code","5ed25723":"code","6af4f3da":"code","2be09105":"code","4266f40e":"code","b90c58dd":"code","57083f59":"code","d48e7fc0":"code","f97e5de4":"code","38b9ac4a":"code","027fc40a":"code","fe84a0c2":"code","b7166cc9":"code","f55ac510":"code","e6fc5e5b":"code","5ffb1fa3":"code","a3c17c12":"code","19177f68":"code","6032a3ba":"code","6f285c40":"code","591efaaf":"code","2d1a4c86":"code","6fb5afc9":"code","874a8522":"code","dcb12305":"code","9362ae1f":"code","a81227ee":"code","0ef9305e":"code","ec7af24f":"code","985dcda3":"code","802b04d7":"code","76dd36cd":"code","531ec7dc":"code","ca992a78":"code","f1f8b60d":"code","12efab11":"code","2f6be038":"code","e936de2b":"code","d0b9b5a1":"code","e9b4f616":"code","d94cad67":"code","dd57f1c1":"code","c63ac6f5":"code","30c58383":"code","660e432e":"code","d3e74c3c":"code","f2136f62":"code","a23c8bbd":"code","b88daf1e":"code","daf9fbf1":"markdown","1683caa6":"markdown","e2d6076c":"markdown","2fc638e5":"markdown","825e5441":"markdown","9c7c2350":"markdown","09e7a350":"markdown","d099fe91":"markdown","394a9ac5":"markdown","acb72d53":"markdown","a595e8b8":"markdown","253dc3a3":"markdown","52c4170b":"markdown","7b98bc93":"markdown","265dda3a":"markdown","5a306320":"markdown","72d3def1":"markdown","2fb5535c":"markdown","dd396a9d":"markdown","f6acc7a7":"markdown","f7080060":"markdown","debfa07d":"markdown","5ff3193e":"markdown","7de7bae4":"markdown","f7e1c952":"markdown","5cce5acd":"markdown","e78082ed":"markdown"},"source":{"5f4a6e3c":"import pandas as pd","df35b1d9":"path = \"..\/input\/bus1234.csv\/\"","7619d120":"df = pd.read_csv(path + \"bus1234.csv\")","c61deaca":"df.shape","fa7c386d":"df.head()","0a96ef52":"df.drop(df.iloc[:, 0:2], inplace = True, axis = 1)","82cfa65f":"df.head()","6b1951de":"df_train = df[:7260]\ndf_test = df[7260:]","77608f6f":"df_train.shape","23364b66":"df_test.shape","ae340e56":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\n%matplotlib inline","069d922b":"df_train.hist(bins = 50, figsize = (10,10));\nplt.show()","a89b2a56":"df_train['replace'].value_counts()","b154c378":"### Let's shuffle the data before creating the subsample\n\ndf_train = df_train.sample(frac = 1)\n\n## Not replacing : 7206\n## Replacing : 54\n\nnot_df = df_train.loc[df_train['replace'] == 0][:54]\nyes_df = df_train.loc[df_train['replace'] == 1]\n\nequal_df = pd.concat([not_df, yes_df])\n\n## Shuffle again\n\nxdf_train = equal_df.sample(frac = 1, random_state = 42)","a5d5f25f":"xdf_train['replace'].value_counts()","fba611f5":"df.isnull().sum()","0db8d41b":"plt.figure(figsize = (10,8))\nsns.boxplot(x = 'miles', data = df_train);","da2c3f1c":"sns.displot( x= 'miles', kde = True, data = df_train, height = 7, aspect =1 );","bc04ee56":"from scipy.stats import skew","c6db8607":"print(skew(df_train['miles']))\n","8258b259":"import numpy as np","6a6a5baa":"xdf = df_train.copy()","69e8a22e":"xdf['miles'] = np.sqrt(xdf['miles'])","1b9fb34c":"print(skew(xdf['miles']))","b2111697":"sns.displot( x= 'miles', kde = True, data = xdf, height = 7, aspect =1 );","3d44bba5":"xdf.head()","5ed25723":"xdf['replace'].value_counts()","6af4f3da":"##\n\nxdf['group'].value_counts()","2be09105":"plt.figure(figsize = (10,8))\nsns.countplot(x = 'group', data = xdf_train);","4266f40e":"plt.figure(figsize = (10,8))\nsns.countplot(x = 'group', hue='replace',data = xdf_train, palette = \"husl\" );","b90c58dd":"plt.figure(figsize = (10,8))\nsns.violinplot(x = 'group', y = 'miles', hue='replace', data = xdf_train, palette = \"husl\");","57083f59":"xdf_train['month'].value_counts()","d48e7fc0":"plt.figure(figsize = (10,8))\nsns.countplot(x = 'month', hue = 'replace', data = xdf_train)\nplt.xticks(\n    ticks = np.arange(12),\n    labels = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'])\nplt.title(\"Replacement by Month\")\nplt.show()","f97e5de4":"#### Splitting the original dataset\nfrom sklearn.model_selection import train_test_split","38b9ac4a":"X = df_train.drop(['replace'], axis = 1)\ny = df_train['replace']","027fc40a":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 1)","fe84a0c2":"#### Splitting corrected dataset","b7166cc9":"sX = xdf.drop(['replace'], axis = 1)\nsy = xdf['replace']","f55ac510":"sX_train, sX_test, sy_train, sy_test = train_test_split(sX, sy, test_size = 0.1, random_state = 27)","e6fc5e5b":"X_train.columns","5ffb1fa3":"## Copying all the title from dataset\n\ncolumns = [x for x in X_train.columns]","a3c17c12":"columns","19177f68":"from sklearn.preprocessing import MinMaxScaler","6032a3ba":"## Normalizing the Original dataset","6f285c40":"norm = MinMaxScaler().fit(X_train)\n\n## transform training data\ntrain_norm = norm.transform(X_train)\n\n## transform testing dataset\ntest_norm = norm.transform(X_test)","591efaaf":"X_train_norm = pd.DataFrame(train_norm, columns = columns)\nX_test_norm = pd.DataFrame(test_norm, columns = columns)","2d1a4c86":"X_train_norm","6fb5afc9":"## Standardization (on Corrected Data)","874a8522":"from sklearn.preprocessing import StandardScaler","dcb12305":"scaler = StandardScaler()","9362ae1f":"train_scaler = scaler.fit_transform(sX_train)\ntest_scaler = scaler.fit_transform(sX_test)","a81227ee":"sX_train_scaler = pd.DataFrame(train_scaler, columns = columns)\nsX_test_scaler = pd.DataFrame(test_scaler, columns = columns)","0ef9305e":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score","ec7af24f":"y_train","985dcda3":"lrmodel1 = LogisticRegression(max_iter = 1000)\nlrmodel1.fit(X_train_norm, y_train)\nscore1 = lrmodel1.score(X_test_norm, y_test)","802b04d7":"lr_pred1 = lrmodel1.predict(X_test_norm)","76dd36cd":"print(\"Score:\", score1)","531ec7dc":"print(classification_report(y_test, lr_pred1))","ca992a78":"lrmodel2 = LogisticRegression(max_iter = 1000)\nlrmodel2.fit(sX_train_scaler, sy_train)\nscore2 = lrmodel2.score(sX_test_scaler, sy_test)\nlr_ypred2 = lrmodel2.predict(sX_test)","f1f8b60d":"print(\"Score:\", score2)","12efab11":"print(classification_report(y_test, lr_pred1))","2f6be038":"df_train['replace'].value_counts()","e936de2b":"os_X = xdf.drop(['replace'], axis = 1)\nos_y = xdf['replace']","d0b9b5a1":"os_Xtrain, os_Xtest, os_ytrain, os_ytest = train_test_split(os_X, os_y, test_size = 0.1, random_state = 1)","e9b4f616":"from imblearn.over_sampling import SMOTE\nimport collections","d94cad67":"counter = collections.Counter(os_ytrain)\nprint(\"Before\", counter)\n\n# oversampling the train dataset using SMOTE\n\nsmt = SMOTE()\n\nX_train_sm, y_train_sm = smt.fit_resample(os_Xtrain, os_ytrain)\n\ncounter = collections.Counter(y_train_sm)\nprint(\"After:\", counter)","dd57f1c1":"lrmodel_sm = LogisticRegression(max_iter = 1000)\nlrmodel_sm.fit(X_train_sm, y_train_sm)","c63ac6f5":"score_sm = lrmodel_sm.score(os_Xtest, os_ytest)\nlrpred_sm = lrmodel_sm.predict(os_Xtest)","30c58383":"print(\"Score:\", score_sm)","660e432e":"lrcm_sm = confusion_matrix(y_test, lr_pred1, labels = (1,0))\nlrcm_sm","d3e74c3c":"from xgboost import XGBClassifier","f2136f62":"xgb1 = XGBClassifier(n_estimators = 100)\nxgb1.fit(X_train_sm, y_train_sm)","a23c8bbd":"xgb_score = xgb1.score(os_Xtest, os_ytest)\nprint(xgb_score)","b88daf1e":"xgb_pred1 = xgb1.predict(os_Xtest)\nxgb_cm1 = confusion_matrix(os_ytest, xgb_pred1, labels = [1,0])\nprint(\"Confusion Matrix: \\n\", xgb_cm1)","daf9fbf1":"Note: We are balancing the dataset to visualize the dataset. We will train the model on unbalanced dataset.","1683caa6":"### Model Perparation","e2d6076c":"## Logistic Regression\n","2fc638e5":"Since the dataset will be tiny to train our model. We will not use this dataset to train. It's just for Data Visualization","825e5441":"#### Feature Scaling\n\nOur data has various range of numbers so it's better to perform feature scaling. \n\n- Machine Learning algorithms like <b> Linear Regression <\/b>, <b>logistic regression <\/b> , <b> neural network <\/b> etc that use <b> gradient descent <\/b> as an optimization technique require data to be scaled.\n- Distance algorithms like <b>KNN <\/b>, <b> K-Means <\/b> and <b>SVM  <\/b> are most affected by the range of features. This is because behind the scenese they are using distances between data points to determine their similarity.\n- Tree Based algorithms, on the other hand are fairly insensitive to the scale of the features. Think about it, a <b> decision tree <\/b> is only splitting a node based on a single feature. The decision tree splits a node on a feature that increases the homogeneity of the node. \n\n\nWe will use <b> Normalization <\/b> \n\n- Normalization is good to use when you know that the distribution of your data does not follow a Gaussian distribution. This can be useful in algorithms that do not assume any distribution of the data like K-Nearest Neighbors and Neural Networks.\n- Standardization, on the other hand, can be helpful in cases where the data follows a Gaussian distribution. However, this does not have to be necessarily true. Also, unlike normalization, standardization does not have a bounding range. So, even if you have outliers in your data, they will not be affected by standardization.\n\nIn short, <b> Normalization <\/b> for <b> original data <\/b> and <b>Standardization <\/b> for <b> corrected data <\/b>.\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/04\/feature-scaling-machine-learning-normalization-standardization\/","9c7c2350":"### Equally Distributing the dataset","09e7a350":"# Bus Engine Replacement\n\n\nAs we can see this is a Classification problem, we have to predict whether the bus engine should be <b> Repalce (1) <\/b> or <b> Not Replace (0) <\/b> based on the given features <b>group<\/b>, <b> year <\/b> and <b> Miles <\/b>.\n\nWe will use all the classification algorithms, based on the accuracy we get we will check the <b> classification report <\/b>, <b> confusion matrix <\/b> and <b> tune hyperparameters<\/b> only on the higest accurate model. \n\n\n### Outline\n\n- <b>1. Data Observation <\/b>\n    - We have already done this part, we will do further observation on EDA\n\n\n\n- <b>2. Exploratory Data Analysis <\/b>\n    - Dataset Preprocessing\n        - Null Values\n        - Outliers\n    - Data Visualization\n    - Preparation for Models (Splitting Dataset)\n    \n    \n    \n- <b> Models, Hyperparamter Tuning, Cross validation and Model Evaluation <\/b>\n    - Logistic Regression\n    - Naive Bayes\n    - K-Nearest Neighbor\n    - Decision Trees\n    - Random Forest\n    - XGBoost","d099fe91":"It doesn't have a pefect curve (bell curve), the miles seemed to be more populated or positively skewed on <b> ( 0 - 50000 ) <\/b> \n\nWe will treat this feature <b> numpy square root <\/b>\n\nAlso, we will check the prediction for both the dataset, and compare the model accuracy.","394a9ac5":"## Observations","acb72d53":"## XGBoost","a595e8b8":"### Seperating Testing and Training dataset\n\nSince, it is only one dataset, so we will seperate, 1000 data for testing.","253dc3a3":"We see our dataset is highly imbalanced. Our model will be biased towards <b> Negative Output ( 0 ) <\/b>. Since it is a few dataset, we will focus on <b>Recall <\/b>","52c4170b":"### Is there any missing values?","7b98bc93":"## Logistic Regression","265dda3a":"### Any Outliers ?\n\n<b> Miles <\/b> has some weird form of distribution. Let's check it our further.","5a306320":"We observe two columns that are not usefull at all, dropping this columns <b> Unnamed <\/b> and <b> id <\/b>","72d3def1":"### Exploratory Data Analysis","2fb5535c":"#### Starting with original dataset\n\nWe haven't performed scaling on prediction feature (i.e <b> y_train <\/b> or <b> y_test <\/b>. As it is prediction feature, so we don't have to scale it down.","dd396a9d":"### We will balance the dataset","f6acc7a7":"<b> xdf <\/b> is maintained data whereas <b> df_train <\/b> is unchanged data.","f7080060":"#### Corrected dataset","debfa07d":"# OverSampling Technique (SMOTE)\n\nSMOTE is an oversampling technique where the synthetic samples are generated for the minority class. This algorithm helps to overcome the overfitting problem posed by random oversampling. It focuses on the feature space to generate new instances with the help of interpolation between the positive instances that lie together.\n\nWe will oversample <b> xdf <\/b>","5ff3193e":"<b> Observations: <\/b>\n- April & December, October & January are the popular month to get the engine replaced.\n- Also, December, November & September and March are the month not to get engine replaced.","7de7bae4":"## Training and Testing\n\nWe will not use equally distributed dataset, since it is very less to train.\n- We will use <b> X_train_norm <\/b> derived from <b> df_train <\/b> original data with scaling\n- and <b> sXtrain_scaler <\/b> derived from <b> xdf <\/b> which is corrected data.\n\nWe will train the dataset on the following algorithms:\n- LogisticRegression","f7e1c952":"## Data Visualization\n\nWe will try to visualize the dataset with respect to our target feature <b> replace <\/b> and try to find out how it is correalted with target feature.","5cce5acd":"#### Seperating dataset\n\nAs we know, we have two dataset. To compare the result, we will see, how effective is the model after feature engineering.\n\n- <b> df_train <\/b> unchanged dataset\n- <b> xdf <\/b> corrected dataset\n\n\n\n- We will use split both the dataset, <b> df_train <\/b> will be indicated by normal <b> X_train <\/b>, <b> X_test <\/b>, <b> y_train <\/b> and <b> y_test <\/b>\n- And, <b> xdf <\/b> will be <b> sX_train <\/b>, <b> sX_test <\/b>,  <b> sy_train <\/b> and <b> sy_test <\/b>\n\n","e78082ed":"Since 0's has more number of datas, so our prediction is heavily biased towards 0's."}}