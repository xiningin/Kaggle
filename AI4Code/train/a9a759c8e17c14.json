{"cell_type":{"040529f6":"code","390f9b43":"code","979aab72":"code","4041a934":"code","8ecf4059":"code","4461d1e4":"code","3b4fee58":"code","3f8b3732":"code","d6587564":"code","7ed12b13":"code","bb3ca431":"code","b4273b78":"code","9288824a":"code","4f7a4ebd":"code","50208540":"code","8216726b":"code","1cdddee0":"code","62c7b0f4":"code","391e5b86":"code","d422e68e":"code","856999c3":"markdown","4832db3a":"markdown","4d1f5076":"markdown","f3e23d68":"markdown","275f59d2":"markdown"},"source":{"040529f6":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import TensorDataset\nfrom torch.utils.data import random_split\n#from math import ceil\nfrom sklearn import metrics\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F","390f9b43":"train_data = pd.read_csv(\"..\/input\/santander-customer-transaction-prediction\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/santander-customer-transaction-prediction\/test.csv\")","979aab72":"def get_data(train,test):\n    #train\/validation set\n    train_data = train\n    #train_data = pd.read_csv(\"..\/input\/santander-customer-transaction-prediction\/train.csv\")\n    y = train_data[\"target\"]\n    X = train_data.drop([\"ID_code\",\"target\"],axis=1)\n    y_tensor = torch.tensor(y.values, dtype=torch.float32)\n    X_tensor = torch.tensor(X.values, dtype=torch.float32)\n    ds = TensorDataset(X_tensor, y_tensor)\n    train_ds, val_ds = random_split(ds,[int(0.8*len(ds)), len(ds)-(int(0.8*len(ds)))])\n    \n    #test set\n    #test_data = pd.read_csv(\"..\/input\/santander-customer-transaction-prediction\/test.csv\")\n    test_data = test\n    test_ids = test_data[\"ID_code\"]\n    X_test = test_data.drop([\"ID_code\"],axis=1)\n    #y_tensor = torch.tensor(y.values,dtype=torch.float32)\n    X_tensor = torch.tensor(X_test.values,dtype=torch.float32)\n    y_tensor = torch.tensor(y.values,dtype=torch.float32)\n    test_ds = TensorDataset(X_tensor,y_tensor)\n    \n    return train_ds, val_ds, test_ds, test_ids\n    ","4041a934":"class NN(nn.Module):\n    def __init__(self, input_size):\n        super(NN, self).__init__()\n        self.net = nn.Sequential(\n            nn.BatchNorm1d(input_size),\n            nn.Linear(input_size, 50),\n            nn.ReLU(inplace=True),\n            nn.Linear(50,1),\n        )\n    def forward(self,x):\n        return torch.sigmoid(self.net(x)).view(-1)","8ecf4059":"def get_predicitons(loader, model, device):\n    model.eval()\n    saved_preds=[]\n    true_labels=[]\n    \n    with torch.no_grad():\n        for x,y in loader:\n            x=x.to(device)\n            y=y.to(device)\n            scores = model(x)\n            saved_preds += scores.tolist()\n            true_labels += y.tolist()\n            \n    model.train()\n    return saved_preds, true_labels","4461d1e4":"# Check if GPU is available\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = NN(input_size=200).to(DEVICE)\noptimizer = optim.Adam(model.parameters(), lr=2e-3, weight_decay=1e-4)\nloss_fn = nn.BCELoss() #if nn.BCEWithLogitsLoss is used then the sigmoid is not required in the model because is\n#included in the loss function\n\ntrain_ds, val_ds, test_ds, test_ids = get_data(train_data,test_data)\n\ntrain_loader = DataLoader(train_ds, batch_size = 1024, shuffle = True)\nval_loader = DataLoader(val_ds, batch_size = 1024)\ntest_loader = DataLoader(test_ds, batch_size = 1024)","3b4fee58":"EPOCHS = 20\n\n#model.train()\nfor epoch in range(EPOCHS):\n    probabilities, true = get_predicitons(val_loader, model, device=DEVICE)\n    print(f\"VALID ROC:{metrics.roc_auc_score(true, probabilities)}\")\n    #data, targets = next(iter(train_loader))\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        data = data.to(DEVICE)\n        targets = targets.to(DEVICE)\n\n        #forward\n        scores = model(data)\n        #print(scores.shape)\n        loss = loss_fn(scores, targets)\n        #print(loss)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()","3f8b3732":"# Check if data is correlated\n\ntrain_data.corr().abs()","d6587564":"col_names = [f\"var_{i}\" for i in range(200)]\nfor col in tqdm(col_names):\n             count = test_data[col].value_counts()\n             uniques = count.index[count == 1]\n             #print(uniques)\n             test_data[col + \"_u\"] = test_data[col].isin(uniques)\n\ntest_data[\"has_unique\"] = test_data[[col + \"_u\" for col in col_names]].any(axis=1)","7ed12b13":"real_test = test_data.loc[test_data[\"has_unique\"], [\"ID_code\"]+col_names]\nfake_test = test_data.loc[~test_data[\"has_unique\"], [\"ID_code\"]+col_names]","bb3ca431":"train_and_test = pd.concat([train_data, real_test],axis=0)","b4273b78":"for col in tqdm(col_names):\n    count = train_and_test[col].value_counts().to_dict()\n    #print(count)\n    train_and_test[col+\"_unique\"] = train_and_test[col].apply(\n        lambda x: 1 if count[x]==1 else 0).values\n    fake_test[col+\"_unique\"] = fake_test[col].apply(\n        lambda x: 1 if count[x]==1 else 0).values","9288824a":"real_test = train_and_test[train_and_test[\"ID_code\"].str.contains(\"test\")].copy()\nreal_test.drop([\"target\"], axis=1, inplace=True)\ntrain_data_2 = train_and_test[train_and_test[\"ID_code\"].str.contains(\"train\")].copy()\n","4f7a4ebd":"test_data_2 = pd.concat([real_test, fake_test], axis=0)","50208540":"# new nn\n\nclass NN_new(nn.Module):\n    def __init__(self, input_size, hidden_dim):\n        super(NN_new, self).__init__()\n        self.bn = nn.BatchNorm1d(input_size)\n        self.fc1 = nn.Linear(2, hidden_dim)\n        self.fc2 = nn.Linear(input_size\/\/2*hidden_dim, 1)\n        #self.net = nn.Sequential(\n        #    nn.BatchNorm1d(input_size),\n        #    nn.Linear(input_size, 50),\n        #    nn.ReLU(inplace=True),\n        #    nn.Linear(50,1),\n        #)\n    def forward(self,x):\n        BATCH_SIZE = x.shape[0]\n        x = self.bn(x)\n        orig_features = x[:,:200].unsqueeze(2) #(BATCH_SIZE, 200, 1)\n        new_features = x[:,200:].unsqueeze(2) #(BATCH_SIZE, 200, 1)\n        x = torch.cat([orig_features,new_features],dim=2) #(BATCH_SIZE, 200, 2)\n        #x = x.view(-1,1)\n        x=self.fc1(x) #(BATCH_SIZE, 200*hidden_dim)\n        x= F.relu(x).reshape(BATCH_SIZE,-1)\n        x=self.fc2(x)\n        return torch.sigmoid(x).view(-1)","8216726b":"# Check if GPU is available\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = NN_new(input_size=400, hidden_dim=16).to(DEVICE)\noptimizer = optim.Adam(model.parameters(), lr=2e-3, weight_decay=1e-4)\nloss_fn = nn.BCELoss() #if nn.BCEWithLogitsLoss is used then the sigmoid is not required in the model because is\n#included in the loss function\n\ntrain_ds, val_ds, test_ds, test_ids = get_data(train_data_2,test_data_2)\n\ntrain_loader = DataLoader(train_ds, batch_size = 1024, shuffle = True)\nval_loader = DataLoader(val_ds, batch_size = 1024)\ntest_loader = DataLoader(test_ds, batch_size = 1024)","1cdddee0":"data, targets = next(iter(test_loader))\ndata.size()","62c7b0f4":"EPOCHS = 30\n\n#model.train()\nfor epoch in range(EPOCHS):\n    probabilities, true = get_predicitons(val_loader, model, device=DEVICE)\n    print(f\"VALID ROC:{metrics.roc_auc_score(true, probabilities):.4f}\")\n    #data, targets = next(iter(train_loader))\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        data = data.to(DEVICE)\n        targets = targets.to(DEVICE)\n\n        #forward\n        scores = model(data)\n        #print(scores.shape)\n        loss = loss_fn(scores, targets)\n        #print(loss)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()","391e5b86":"def get_submission(model, loader, test_ids, device):\n    all_preds=[]\n    model.eval()\n    with torch.no_grad():\n        for x,y in loader:\n            x = x.to(device)\n            y = y.to(device)\n            score =model(x)\n            prediction=score.float()\n            all_preds += prediction.tolist()\n            \n    model.train()\n    df= pd.DataFrame({\n        \"ID_code\" : test_ids.values,\n        \"target\" : np.array(all_preds)\n    })\n    \n    df.to_csv(\"submission.csv\",index=False)","d422e68e":"get_submission(model, test_loader, test_ids, DEVICE)","856999c3":"# create a simple NN as Baseline","4832db3a":"## Baseline","4d1f5076":"# Model improvment","f3e23d68":"features seems to be uncorrelated","275f59d2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \nimport pandas as pd# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session"}}