{"cell_type":{"08cac92c":"code","a9d16df4":"code","a8a30d2e":"code","6a9a01e6":"code","a4cd4a51":"code","fc8781c2":"code","b8f6c410":"code","bce2efd5":"code","4901c0fd":"code","8734bdfc":"code","3aab011a":"code","b7e48e6f":"markdown","43ab87bf":"markdown","5d6df245":"markdown","b8ff13a6":"markdown","3a9b4f57":"markdown","2bea84ed":"markdown","2c53bd29":"markdown","44448612":"markdown","0ce2e789":"markdown"},"source":{"08cac92c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a9d16df4":"import matplotlib.pyplot as plt, matplotlib.image as mpimg\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\n%matplotlib inline\n\ntrain_df = pd.read_csv('..\/input\/train.csv')\nlabels = train_df['label']\nimages = train_df.drop(['label'], axis = 1)\ntrain_x, test_x, train_y, test_y = train_test_split(images, labels, train_size = 0.8, random_state = 42)","a8a30d2e":"import random\n\ndef get_image(x):\n    random.seed(42)\n    i = random.randint(0,len(x))\n    img = x.iloc[i].values\n    img = img.reshape(28,28)\n    plt.imshow(img,cmap='gray')\n    plt.title(train_y.iloc[i])\n    \nget_image(train_x)","6a9a01e6":"def get_graph(x):\n    i = random.randint(0,len(x))\n    plt.hist(train_x.iloc[i])\n    plt.title(str(train_y.iloc[i]))\n    \nget_graph(train_x)","a4cd4a51":"\"\"\"\"\n\n# We load dependencies\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Establish parameter ranges over which to search\n\nc_range = np.logspace(-2,10,13)\ngamma_range = np.logspace(-9,3,13)\n\n#Set up a dictionary of key value pairs for each parameter and value array\n\nparam_grid = dict(gamma = gamma_range, C = c_range)\n\n#Set up cross validation to test our parameters over smaller sets of data to find an average score \n#for each parameter value\n\ncv = StratifiedShuffleSplit(n_splits = 5, test_size = 0.2, random_state = 42)\nrandsearch = RandomizedSearchCV(SVC(), n_iter = 10, param_distributions = param_grid, cv = cv)\nrandsearch.fit(train_x, train_y)\n\nprint(\"The best parameters are %s with a score of %0.2f\"\n      % (randsearch.best_params_, randsearch.best_score_))","fc8781c2":"clf_kernel = SVC(kernel = 'poly', degree = 2)\nclf_kernel.fit(train_x, train_y)\nclf_kernel.score(test_x, test_y)","b8f6c410":"bw_train_x = train_x\nbw_test_x = test_x\n\nbw_train_x[bw_train_x > 0] = 1\nbw_test_x[bw_test_x > 0] = 1\n","bce2efd5":"\nclf_bw = SVC()\nclf_bw.fit(train_x, train_y.values.ravel())\nclf_bw.score(test_x, test_y)\n","4901c0fd":"test_df = pd.read_csv('..\/input\/test.csv')\npredictions = clf_kernel.predict(test_df)\ntest_df.head(10)","8734bdfc":"test_df.index += 1\nmy_submission = pd.DataFrame({'ImageID': test_df.index, 'Label': predictions})\nmy_submission.head()","3aab011a":"my_submission.to_csv('my_submission.csv', index = False)","b7e48e6f":"**MNIST Dataset**\n\nHosted by [Yann LeCun](http:\/\/http:\/\/yann.lecun.com\/exdb\/mnist\/), the MNIST database is a series of handwritten digits, with a training set of 60,000 examples and a test set of 10,000 examples.  It consists of a series of examples that have been normalized by size and centered.\n\nIt's a classic entry level dataset for those seeking to cut their teeth on image and pattern recognition techniques.  Here, we'll run a series of algorithms on the data to compare performance","43ab87bf":"**Stage 1: Load in Data and Dependencies**","5d6df245":"So a polynomial kernel with a degree of 2 achieves a 0.976 accuracy score on the training set.\n\n**Stage 3b: Feature Engineering to Reduce Dimensionality**\n\nWe can do a simple conditional replacement of any pixel value > 1 with 1 and leave all existing 0s stet.  This will reduce all the numbers to black and white.","b8ff13a6":"**Stage 4: Submit Predictions**\n\nNow we use our model built using the training data to make predictions on the actual test data (not to be confused with the test set we created earlier using train_test_split).  We need to have a csv file with two columns, one being the imageid, conforming to the number of the image (and it can't be zero indexed!) and the label our model has ascribed to the digit in question.  Let's go.","3a9b4f57":"so the black and white transformation leads to a lower score on the test set than the polynomial kernel.  Yet it may be that we have overfitted to the data with the polynomial kernel.","2bea84ed":"**Stage 2: Explore our Data and Establish What We Have**\n\nSo what do we have here? The dataset consists of a series of images of handwriting that are 28 pixels by 28 pixels.  Here, the data has been pre-prepared by being turned into a one-hot encoded array of 0s and 1s to denote whether a pixel is black or white. From this our model can learn and classify our images.  So we are also looking here at a classification problem, which should give us some hints in terms of model selection.\n\nWe can even reconstruct the image by taking an example, and loading it into a numpy array and reshaping it so that it is two dimensional (28 x 28 pixels).  Then we can plot it with matplotlib.","2c53bd29":"**Stage 3a: Building a Polynomial Kernel**\n\nLet's start first with the kernel approach.  First we need to try and find the best parameters through RandomizedSearchCV which will seek out the best parameters through randomised search through a range of parameters we supply.  It's quicker and less computationally intensive than GridSearchCV, but the trade-off is that we might not find the absolute optimum parameters given that a randomised search may miss these compared to the exhaustive search offered by GridSearchCV.","44448612":"**Stage 3: Building a Classifier**\n\nOur first attempt at classification will include creating a support vector classifier.  A support vector classifier works by identifying a hyperplane that can divide two classes.  Support vectors are the data points positioned as such that if they were moved, the hyperplane itself would move.\n\nThe further from these datapoints and the hyperplane other datapoints lie, the more confident we can be about the class to which those belong.\n\nSo ideally we want our datapoints to be as far away from the hyperplane as possible.  \n\nBut what if our true labels are intermingled? SV classifiers permit some margin of error. Cleverly called 'margins'. The classifier will select a hyperplane with the maximum margin between the hyperplane and any points in the training set, giving the greatest chance possible that on new data, the correct class will be selected.\n\nWe can manipulate a parameter 'c' which is a penalty term that applies a cost for each misclassified training example.  So for large values of C, the  model will choose a smaller margin hyperplane if that does a good job of getting all the training points classified correctly.  A small value of C will cause the optimizer to look for a larger margin, and thus it will tolerate more data points falling on the 'wrong' side of the hyperplane even if that means misclassifying some examples.\n\nSVM commonly performs well on image recognition challenges, and particularly on colour-based recognition.\n\nOur data is currently grayscale, as the image reproduction above showed.  This means that it is unlikely that our data will be linearly separable.  We have two possible tactics to address this. We can reduce the dimensionality of our data by changing our images to black and white by handily simply replacing any pixel value > 0 with 1 and leaving all our existing 0s intact.\n\nAlternatively, we can manipulate the kernel parameter.  This is a means by which we can separate data that is not otherwise separable in 2 dimensional space.   If we go up to a higher dimensionality space via kernelling, we can use a plane rather than a line to separate our data. \n\nWe've chosen to go down the polynomial kernel route, but we've included the b\/w approach as an alternative route for comparison.  The end submission uses the polynomial kernel.","0ce2e789":"Further, we can examine the histogram of train images"}}