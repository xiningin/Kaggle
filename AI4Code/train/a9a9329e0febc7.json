{"cell_type":{"4e79ef15":"code","5a344fb8":"code","76f2031b":"code","dd0c795e":"code","d131f75a":"code","6d372901":"code","a9da72e6":"code","209ff98e":"code","2c3670ff":"code","0fce8932":"code","8886e379":"code","f1101156":"code","e081b6bb":"code","12694389":"code","4ccda0d1":"code","8b44d8a7":"code","9997065b":"code","098e0914":"code","f67873ed":"code","f6205657":"code","cc388e95":"code","fc38c861":"code","4363d389":"code","d48e8e7e":"code","d53a81f1":"code","2e5feaf1":"code","1f28a038":"code","64c9bd39":"code","3acebd7b":"code","72cc8c09":"code","883b40b9":"code","5730b3f6":"code","9952a7a6":"code","44f8aca9":"code","f0d66e47":"code","911ac171":"code","767bca4a":"code","46c75724":"code","5990f047":"code","b4a53195":"code","04341d21":"code","ee13e3d1":"code","2dc1388b":"code","0ca801cb":"code","f6d5c6ca":"code","6cd5247a":"code","79509bd7":"code","7919f34c":"code","40a6a5b7":"code","428fc38d":"code","5aa7c15f":"code","c260628a":"code","11ff0732":"code","d76ef3f4":"code","d84eda82":"code","73f6555e":"code","32c63799":"code","d3903eb4":"code","521574da":"code","b0b1fbc8":"code","d6c7a9fc":"code","e9220aea":"code","c00d3e71":"code","5f64c81e":"code","6d15248d":"code","5f14dcc3":"code","e239f026":"code","8ad303aa":"code","d78ff3cf":"code","8396037a":"code","df6072d0":"code","c730d1bc":"code","faefaa1d":"code","3a845ed9":"code","c3aec795":"code","d840bc4b":"code","9fd06de0":"code","63d0057d":"code","39ab7a47":"code","ba0ff938":"code","988ecee3":"code","64330df4":"code","cfc54540":"code","e7305d3a":"code","54e48fe9":"code","47be2e92":"code","b952e81a":"code","0e8c699f":"markdown","13dad483":"markdown","3bbfc186":"markdown","8c31c00b":"markdown","f96a1f59":"markdown","2b0fda55":"markdown","a93d082d":"markdown","6298ed58":"markdown","18bf36d9":"markdown","ca8e1a8d":"markdown","248fbc76":"markdown","32fc2375":"markdown","d551d04e":"markdown","ac5be122":"markdown","508542d6":"markdown","3c3f2cbe":"markdown","a945d683":"markdown","3224df5f":"markdown","86914276":"markdown","e6f13366":"markdown","dae43e03":"markdown","516877a7":"markdown","5b98d810":"markdown","3cfe4cf8":"markdown","4a355bcb":"markdown","f594eddd":"markdown","9a57476b":"markdown","994d17bb":"markdown","d348c586":"markdown","61ad28c9":"markdown","d724087e":"markdown","adab167c":"markdown","99e27886":"markdown","9c0f5863":"markdown","7323a8b6":"markdown","eb60fad5":"markdown","b31e2549":"markdown","132b720b":"markdown","ca008a7f":"markdown","3a0035c6":"markdown","704da236":"markdown","a19b31d0":"markdown","3a605e19":"markdown","caf6a37b":"markdown","0a47f930":"markdown","d403c714":"markdown","994f310e":"markdown","3ea8f5e7":"markdown","cc5afb8b":"markdown","59adb858":"markdown","7204135a":"markdown","2f65c54c":"markdown","77420df5":"markdown","bc2c0e17":"markdown"},"source":{"4e79ef15":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm_notebook as tqdm","5a344fb8":"df_train = pd.read_csv('\/kaggle\/input\/weather-postprocessing\/pp_train.csv', index_col=0)\ndf_test = pd.read_csv('\/kaggle\/input\/weather-postprocessing\/pp_test.csv', index_col=0)\n\nX_train = pd.read_csv('\/kaggle\/input\/nb1-linear-regression\/X_train.csv', index_col=0)\ny_train = pd.read_csv('\/kaggle\/input\/nb1-linear-regression\/y_train.csv', index_col=0, squeeze=True)\nX_valid = pd.read_csv('\/kaggle\/input\/nb1-linear-regression\/X_valid.csv', index_col=0)\ny_valid = pd.read_csv('\/kaggle\/input\/nb1-linear-regression\/y_valid.csv', index_col=0, squeeze=True)\nX_test = pd.read_csv('\/kaggle\/input\/nb1-linear-regression\/X_test.csv', index_col=0)","76f2031b":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.metrics import mean_squared_error","dd0c795e":"X_train1 = X_train[['t2m_fc_mean']].values","d131f75a":"X_train1.shape","6d372901":"model = Sequential([Dense(1, input_shape=(1,))])","a9da72e6":"model.summary()","209ff98e":"model.layers[0].weights","2c3670ff":"a, b = [p.numpy().squeeze() for p in model.layers[0].weights]\na, b","0fce8932":"plt.scatter(X_train1[::1000], y_train[::1000], alpha=0.2)\nx = np.linspace(-15, 20, 2)\nplt.plot(x, a*x+b, c='r', lw=2)","8886e379":"preds = model(X_train1)  # Same as model.predict(X_train1)","f1101156":"y_train.shape, preds.shape","e081b6bb":"mse = mean_squared_error(y_train, preds[:, 0])\nmse","12694389":"e = preds[:, 0] - y_train\ndl_da = np.mean(2 * X_train1[:, 0] * e)\ndl_db = np.mean(2 * e)\ndl_da, dl_db","4ccda0d1":"with tf.GradientTape() as g:\n    preds = model(X_train1)\n    mse = mean_squared_error(y_train, preds[:, 0])","8b44d8a7":"dloss_dparam = g.gradient(mse, model.trainable_weights)","9997065b":"dl_da, dl_db = dloss_dparam\ndl_da, dl_db","098e0914":"lr = 1e-3","f67873ed":"a, b = model.get_weights()","f6205657":"a -= lr * dl_da\nb -= lr * dl_db","cc388e95":"model.set_weights([a, b])","fc38c861":"a, b = [p.numpy().squeeze() for p in model.layers[0].weights]\nplt.scatter(X_train1[::1000], y_train[::1000], alpha=0.2)\nx = np.linspace(-15, 20, 2)\nplt.plot(x, a*x+b, c='r', lw=2)","4363d389":"preds = model(X_train1)\nmse = mean_squared_error(y_train, preds[:, 0])\nmse","d48e8e7e":"def gradient_descent_step(model, lr):\n    with tf.GradientTape() as g:\n        preds = model(X_train1)\n        mse = mean_squared_error(y_train, preds[:, 0])\n    dl_da, dl_db = g.gradient(mse, model.trainable_weights)\n    a, b = model.get_weights()\n    a -= lr * dl_da\n    b -= lr * dl_db\n    model.set_weights([a, b])\n    return (a, b), mse.numpy()","d53a81f1":"model = Sequential([Dense(1, input_shape=(1,))])","2e5feaf1":"from IPython.display import clear_output\nfrom time import sleep","1f28a038":"def plot_line():\n    a, b = [p.numpy().squeeze() for p in model.layers[0].weights]\n    plt.scatter(X_train1[::1000], y_train[::1000], alpha=0.2)\n    x = np.linspace(-15, 20, 2)\n    plt.plot(x, a*x+b, c='r', lw=2)\n    plt.show()","64c9bd39":"params = []\nloss = []\nfor _ in range(20):\n    p, l = gradient_descent_step(model, 1e-3)\n    params.append(p)\n    loss.append(l)\n    plot_line()\n    sleep(0.2)\n    clear_output(True)","3acebd7b":"loss[-1]","72cc8c09":"plt.plot(loss);","883b40b9":"model = Sequential([Dense(1, input_shape=(1,))])","5730b3f6":"model.compile(tf.keras.optimizers.SGD(1e-3), 'mse')","9952a7a6":"model.fit(\n    X_train1, \n    y_train.values, \n    epochs=20,\n    batch_size=len(X_train1)\n)","44f8aca9":"model = Sequential([Dense(1, input_shape=(1,))])\nmodel.compile(tf.keras.optimizers.SGD(1e-3), 'mse')\nmodel.fit(\n    X_train1, \n    y_train.values, \n    batch_size=128,\n    epochs=2,\n)","f0d66e47":"X_train.shape, y_train.shape","911ac171":"model = Sequential([Dense(1, input_shape=(22,))])","767bca4a":"model.compile(tf.keras.optimizers.SGD(1e-4), 'mse')","46c75724":"model.summary()","5990f047":"model.fit(\n    X_train.values,\n    y_train.values,\n    validation_data=(X_valid.values, y_valid.values)\n)","b4a53195":"Oh crap, something went wrong... What could it be?","04341d21":"Oh crap, something went wrong... What could it be","ee13e3d1":"X_train.std().plot.bar()\nplt.yscale('log')","2dc1388b":"mean = X_train.mean()\nstd = X_train.std()\nX_train_norm = ((X_train - mean) \/ std).values\nX_valid_norm = ((X_valid - mean) \/ std).values\nX_test_norm = ((X_test - mean) \/ std).values\ny_train = y_train.values\ny_valid = y_valid.values","0ca801cb":"X_train_norm.std(0)","f6d5c6ca":"model = Sequential([Dense(1, input_shape=(22,))])","6cd5247a":"model.compile(tf.keras.optimizers.Adam(1e-3), 'mse')","79509bd7":"model.fit(\n    X_train_norm,\n    y_train,\n    batch_size=128,\n    epochs=4,\n    validation_data=(X_valid_norm, y_valid)\n)","7919f34c":"from sklearn.metrics import r2_score, mean_squared_error\ndef print_scores(model, X_train=X_train_norm, X_valid=X_valid_norm):\n    preds_train = model.predict(X_train, batch_size=10_000)\n    preds_valid = model.predict(X_valid, batch_size=10_000)\n    r2_train = r2_score(y_train, preds_train)\n    r2_valid = r2_score(y_valid, preds_valid)\n    mse_train = mean_squared_error(y_train, preds_train)\n    mse_valid = mean_squared_error(y_valid, preds_valid)\n    print(f'Train R2 = {r2_train}\\nValid R2 = {r2_valid}\\nTrain MSE = {mse_train}\\nValid MSE = {mse_valid}')","40a6a5b7":"print_scores(model)","428fc38d":"model = Sequential([\n    Dense(32, input_shape=(22,), activation='relu'),\n    Dense(1, activation='linear')\n])","5aa7c15f":"model.compile(tf.keras.optimizers.Adam(1e-3), 'mse')","c260628a":"model.fit(X_train_norm, y_train, 1024, epochs=12, validation_data=(X_valid_norm, y_valid))","11ff0732":"print_scores(model)","d76ef3f4":"model = Sequential([\n    Dense(256, input_shape=(22,), activation='relu'),\n    Dense(256, activation='relu'),\n    Dense(256, activation='relu'),\n    Dense(1, activation='linear')\n])","d84eda82":"model.summary()","73f6555e":"model.compile(tf.keras.optimizers.Adam(1e-4), 'mse')","32c63799":"model.summary()","d3903eb4":"h = model.fit(X_train_norm, y_train, 1024, epochs=30, validation_data=(X_valid_norm, y_valid))","521574da":"plt.plot(h.history['loss'][1:])\nplt.plot(h.history['val_loss'][1:])","b0b1fbc8":"early_stopping = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)","d6c7a9fc":"model = Sequential([\n    Dense(256, input_shape=(22,), activation='relu'),\n    Dense(256, activation='relu'),\n    Dense(256, activation='relu'),\n    Dense(1, activation='linear')\n])","e9220aea":"model.compile(tf.keras.optimizers.Adam(1e-4), 'mse')","c00d3e71":"model.fit(X_train_norm, y_train, 1024, epochs=30, validation_data=(X_valid_norm, y_valid), callbacks=[early_stopping])","5f64c81e":"print_scores(model)","6d15248d":"preds = model.predict(X_test, 10_000).squeeze()\nsub =  pd.DataFrame({'id': range(len(preds)), 'Prediction': preds})\nsub.to_csv('submission1.csv', index=False)","5f14dcc3":"split_date = '2015-01-01'\ndf_train = df_train.dropna(subset=['t2m_obs'])\nstations = df_train.station\nstations_train = df_train.station[df_train.time < split_date]\nstations_valid = df_train.station[df_train.time >= split_date]\nstations_test = df_test.station","e239f026":"stations_train.head()","8ad303aa":"unique_stations = pd.concat([df_train.station, df_test.station]).unique()","d78ff3cf":"len(unique_stations)","8396037a":"stat2id = {s: i for i, s in enumerate(unique_stations)}","df6072d0":"ids = stations.apply(lambda x: stat2id[x])","c730d1bc":"ids_train = ids[df_train.time < split_date]\nids_valid = ids[df_train.time >= split_date]\nids_test = stations_test.apply(lambda x: stat2id[x])","faefaa1d":"ids_train.head()","3a845ed9":"features_in = Input(shape=(22,))\nid_in = Input(shape=(1,))","c3aec795":"emb_layer = Embedding(len(unique_stations), 2)\nemb = emb_layer(id_in)","d840bc4b":"emb_layer.get_weights()[0].shape","9fd06de0":"id_in, emb","63d0057d":"emb = Flatten()(emb)\nemb","39ab7a47":"x = Concatenate()([features_in, emb])\nx","ba0ff938":"x = Dense(100, activation='relu')(x)\nout = Dense(1, activation='linear')(x)","988ecee3":"model = tf.keras.models.Model(inputs=[features_in, id_in], outputs=out)","64330df4":"model.summary()","cfc54540":"tf.keras.utils.plot_model(model)","e7305d3a":"model.compile(tf.keras.optimizers.Adam(1e-3), 'mse')","54e48fe9":"model.fit([X_train_norm, ids_train], y_train, 1024, 20, \n          validation_data=([X_valid_norm, ids_valid], y_valid))","47be2e92":"print_scores(model, X_train=[X_train_norm, ids_train], X_valid=[X_valid_norm, ids_valid])","b952e81a":"preds = model.predict([X_test, ids_test], 10_000).squeeze()\nsub =  pd.DataFrame({'id': range(len(preds)), 'Prediction': preds})\nsub.to_csv('submission2.csv', index=False)","0e8c699f":"! Question time","13dad483":"Next we build an embedding layer for the station ID input. At its core an embedding is a lookup table. For each station ID the embedding table stores a certain number (in our case 2) parameters.","3bbfc186":"We can also check if the error has gone down.","8c31c00b":"## Early stopping\n\nThe easiest way to deal with overfitting is to simply say. I train until the validation score of my model stops decreasing. This is called early stopping.","f96a1f59":"### Gradients!\n\nFor our linear regression we can easily compute the gradients of the MSE with respect to a and b by hand. Let's compute those.","2b0fda55":"Let's compute the mean squared error. For this we will use the function defined in Keras.","a93d082d":"Technically early stopping is a callback. A callback is a function to be called during training, e.g. at the end of each epoch. This needs to be passed to the `.fit` method.","6298ed58":"That was easy. Before we move on, there are a few more minor changes we want to make to our algorithm that will allow os to train much larger models later on. \n\nSpecifically, in deep learning we use a version of gradient descent called stochastic gradient descent. What does that mean? So far, we have always considered every single training sample to compute the gradients. \n\nIn stochastic gradient descent, on the other hand, only a batch (a small sample) of the entire training dataset is used for each gradient computation. Since each batch should be a representative sample of the entire dataset, we randomly draw from the entire dataset without replacement. When all sample from the dataset have been sampled we completed one epoch.\n\nSGD is used mostly for practical purposes. Computing gradients for all samples for a large network would blow up RAM but the random element can also be helpful to over come local minima.","18bf36d9":"Finally, let's train the model.","ca8e1a8d":"Create a dictionary that maps from the actual DWD station ID to our continuous range of IDs.","248fbc76":"## Keras API - Stochastic Gradient Descent\n\nCoding this from scratch is a good learning experience but pretty annoying if we want to just build a model. So now let's do the same using the Keras API in 3 lines of code.\n\nWe already know how to define the model.","32fc2375":"## Early stopping and overfitting\/regularization\n\nNow we can add more hidden layers very easily. We can also increase the number of hidden neurons.","d551d04e":"Finally, we fit the model just like we did with the sklearn models in the previous lessons. ","ac5be122":"This will look different everytime but generally won't match the data very well. We can now also make predictions with our untrained model.","508542d6":"## Linear regression with gradient descent\n\nBefore we go to neural networks, let's actually go back to linear regression. As we will see soon, they are closely related. But now we will put more focus on how to train such an algorithm. Note that this is not actually how linear regressions are usually fitted. Because LR is linear we can just do some simple linear algebra to find the optimal parameters (see [normal equation](https:\/\/en.wikipedia.org\/wiki\/Linear_least_squares#Derivation_of_the_normal_equations)).\n\nHere we will use a much more general optimization algorithm, gradient descent, that will allow us to fit more complicated functions later.\n\nWe will use Tensorflow and Keras to build this algorithm.","3c3f2cbe":"We will now think of a linear regression as a network. To define a network using Keras, we use the Sequential model class. This class takes a list of layers as input. In the case of a linear regression we only have one layer with one input and one output. A layer in which all inputs and outputs are connected is called a Dense layer.","a945d683":"Now that we have done one gradient descent step, let's check if our line fits the data better.","3224df5f":"Let's print out the results as we did before.","86914276":"Finally, to create a model with the functional API, we need to connect the inputs and outputs.","e6f13366":"Let's not plot the line defined by these parameters on top of the training dataset.","dae43e03":"So our error is pretty large as expected. Our job is now to build an algorithm that lowers the error. For this we will do some calculus! These will be pretty much the only equations in the workshop ;)","516877a7":"So now we have a pretty good model. To create a better model, you can try fiddling with the hyper-parameters, i.e. number of layers, number of neurons in each layer, type of activation function, learning rate, etc.\n\nBut actually, there is one feature that we have not used so far and that is the station ID. Station ID is a categorical feature rather than a continuous feature, so we have to treat it differently.\n\nIn neural networks, categorical features can be dealt with using something called embeddings.","5b98d810":"Wow, they are really different. This is a problem for gradient descent algorithms!","3cfe4cf8":"# Your turn\n\nNow it's your turn. We learned lots of thing just now, so try to recreate the basic steps from this notebook:\n\n1. Import the features from the previous notebooks and normalize them.\n2. Train a linear regression using Keras and achieve a similar score to our sklearn linear regression.\n3. Train a neural network by adding a hidden layer.\n4. Add more layers and make them bigger. Prevent overfitting by using early stopping.\n5. Add an embedding for the station IDs.\n\nFurther challenges: If you are donw with all of that, it's time to train the best possible model. This will be the last session with this dataset. Here are some ideas:\n\n1. Play around with the hyper-parameters of the NNs and see what works best.\n2. Create a super-ensemble of NNs, RFs and LRs. You can also train several models of each and average the predictions.\n3. If you want to look under the hood of the embedding layer, you could try to visualize the two embedding features on a map. Did the network learn anything meteorologically reasonable?","4a355bcb":"We can also plot the model.","f594eddd":"Now we can train the model. Because we now have two inputs we need to pass a list of the two inputs to the fit function.","9a57476b":"All features have a standard deviatio of one now. Now we can train our multiple linear regression with SGD.","994d17bb":"Next we want to concatenate the embedding features and the regular features together.","d348c586":"### Keras Functional API\n\nSo far we have built Keras models using the Sequential API which is great for building models that go straight from a single input to a single output. Now we have two inputs however. For more complex models, there is another API, the functional API.\n\nThe functional API works by defining the layers separately with inputs and outputs.\n\nFirst we start with two input layers, one for the regular features and one for the station ID.","61ad28c9":"## Embeddings\n\nBefore we start we need to do some pre-processing of the features. In addition to the X's we had before, we now need another feature vector that contains the station IDs.","d724087e":"# Neural networks\n\nNow we will finally get to neural networks, which have been at the heart of the artificial intelligence revolution. In this notebook we will see that neural networks are actually really simple and very similar to the linear regression we already used, just non-linear.\n\nIn fact, we will build a linear regression algorithm from scratch and then extend it to more complex networks. Let's get started!","adab167c":"## Neural Networks\n\nWe basically already have everything in place to go from linear regression to neural networks. All we need to do is add some hidden layers.","99e27886":"Gradient descent is an iterative algorithm. We have to take many steps until we reach the minimum. So let's write a little function and loop over it while checking what happens to the loss and the line.","9c0f5863":"Next we need to compile the model. This means telling the Model which optimizer to use (we will talk about that more later) and which loss to minimize, in our case the MSE.","7323a8b6":"You will also notice that the parameters are not Numpy arrays but instead Tensorflow Variables. In many ways they bahave the same but have some important additional properties as we will see soon. In order to get a Numpy array from a TF Variable, use `.numpy()`","eb60fad5":"As expected, we have 23 parameters: 22 weights and 1 bias.","b31e2549":"Wow. This model is pretty darn good. Turns out the station ID is a really important predictor. Let's submit this now.","132b720b":"## Multiple linear regression with Keras - Normalization\n\nSo now that we have a universially usable optimization algorithm, we can start building larger networks. For now, let's stay linear and train a multiple linear regression.","ca008a7f":"Now we can build a standard neural network.","3a0035c6":"Now we will change one more thing. Instead of Vanilla SGD we will use something called Adam as our optimizer. It's basically SGD with two additional tricks: momentum and adaptive gradients.","704da236":"What we see here is a classic case of overfitting. The training error continues to go down but the validation erro increases again. This happens when we overparameterize the problem, i.e. we have a model that is more complex than the problem we are trying to solve.\n\nOne way to deal with this is to go back to using a smaller neural network. But if we look at the validation scores the \"too big\" model actually gives a better validation score after a few epochs.\n\nThis is one of the key insights in deep learning. Overparameterizing a problem actually yields better scores if you figure out how to deal with the overfitting problem.","a19b31d0":"Early stopping has a patience parameter which indicates how long to wait for a decrease in score before stopping. Setting this to >1 makes sense because the validation score can fluctuate quite a bit sometimes. \n\n`restore_best_weights` means that the parameters of the network with the lowest validation score are restored.","3a605e19":"## Normalize the data\n\nFor SGD to work well, all input features should be approximately the same scale. More specifically they should be scale one with mean zero. Why? Because the way the parameters are initialized assume this is the case.\n\nThe easiest way to normalize the data is to subtract the mean and divide by the standard deviation. One very important thing is to: \n\n**USE THE SAME NORMALIZATION ON TRAINING, VALIDATION AND TESTING SET!!!**","caf6a37b":"As we would expect, we end up with a model with 2 parameters: One weight and one bias. We can check what these parameters are. At the moment they are random, since we did not do any training yet.","0a47f930":"However, we want to build much more complex networks later and coding up the gradients by hand would be very cumbersome. Thankfully, Tensorflow has an easy way to automaticaly compute gradients. In fact, autodifferentiation is the key feature of Tensorflow, Pytorch and all other deep learning libraries.","d403c714":"The embedding layer returns a tensor with shape [batch_size, input_len, emb_dim]. Since we specify our input length to always be one, we can remove this dimension by squeezing the layer.","994f310e":"Let's start with a simple univariate regression as we did in the linear regression notebook.","3ea8f5e7":"Hurra! We have computed gradient descent from scratch and trained a linear regression. Finally we can check how the loss went down.","cc5afb8b":"Oh crap, something went wrong... What could it be. Let's have a look at the typical scales of each of the features.","59adb858":"By just adding one line we already have better results than we had with the random forest. So wha actually happened in this one line?","7204135a":"Now let's use the gradients to update our weights. First we need to set a learning rate. There are a few consideration when doing so. Too small learning rates will take forever but too large learning rates will blow up the algorithm. Unfortunately there are no great rules in finding learning rates, so we just have to try it out.","2f65c54c":"The `stations_*` vectors contain the station ID for each sample. However the station IDs are kind of random with gaps between the individual stations. We need a continuous range of IDs, so let's create this.","77420df5":"This comes very close to the sklearn linear regression we did in lesson 1.","bc2c0e17":"The datasets were still Pandas Dataframes. With `.values()` we can convert them to numpy arrays."}}