{"cell_type":{"0a41b530":"code","80e6328a":"code","152314e9":"code","d815e0a3":"code","da0b6b76":"code","44ae6751":"code","3c8b8426":"code","0edbf273":"code","cd186aad":"code","debf2b04":"code","367a2e34":"code","eedb888a":"code","5b38f705":"code","06b309a2":"code","8ec03a56":"code","a760f5ae":"code","ea66ccb6":"code","9ab0c815":"code","a7aa2dab":"code","aff891d8":"code","83bde2fd":"code","32435b60":"code","446ae6f0":"code","924a35d1":"code","57a626b3":"code","173bb257":"code","b6bdea89":"code","bcdbe38d":"code","e8607ee7":"code","d59d94f5":"code","c30d8fb5":"code","3f7a83c4":"code","1ab73d29":"code","393f2eaf":"code","e802cf64":"code","b77a93fb":"code","5a6f3f69":"code","1291f23a":"code","e3be72ec":"code","095b5085":"markdown","0512fc96":"markdown","e90cf891":"markdown","71b9a1ef":"markdown","47e21f5f":"markdown","13a98e6d":"markdown","cbb20139":"markdown","36a481c3":"markdown","b8d4d603":"markdown","c8c52a88":"markdown","0531fd0d":"markdown","15cbaef4":"markdown","0873ea7e":"markdown","3d7a8cb1":"markdown","ce507d1c":"markdown","eb42848e":"markdown","621ce068":"markdown","bb1cf340":"markdown","906f26b1":"markdown","9cbe7487":"markdown","92d4ef5a":"markdown","5f8d9b60":"markdown","00c33651":"markdown","6b4c0c93":"markdown","e3704310":"markdown"},"source":{"0a41b530":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","80e6328a":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\nprint(\"train shape : \",train.shape,\"  test shape : \",test.shape)","152314e9":"train1 = train[train.columns[train.isnull().sum()!=0]]\ntest1 = test[test.columns[test.isnull().sum()!=0]]\nna_prop1 = (train1.isnull().sum() \/ train1.shape[0]).sort_values()\nna_prop2 = (test1.isnull().sum() \/ test1.shape[0]).sort_values()\nplt.figure(figsize=(10,8))\nsns.set_style('whitegrid')\nplt.subplot(211)\nna_prop1.plot.barh(color='blue')\nplt.title('Missing values(train set)', weight='bold')\nplt.subplot(212)\nna_prop2.plot.barh(color='blue')\nplt.title('Missing values(test set)', weight='bold' )","d815e0a3":"Lot_na = na_prop1.sort_values(ascending=False).index[:6]\nprint(train.drop(Lot_na,axis=1).dropna(axis=0).shape)\nprint(test.drop(Lot_na,axis=1).dropna(axis=0).shape)","da0b6b76":"plt.figure(figsize = (8,3))\nplt.subplot(121)\nsns.distplot(train['SalePrice'])\nplt.subplot(122)\nstats.probplot(train['SalePrice'],plot=plt)","44ae6751":"plt.figure(figsize = (8,3))\nplt.subplot(121)\nsns.distplot(np.log(train['SalePrice']))\nplt.subplot(122)\nstats.probplot(np.log(train['SalePrice']),plot=plt)","3c8b8426":"plt.figure(figsize = (8,3))\nplt.subplot(121)\nsns.residplot(x = train.Id, y = train.SalePrice)\nplt.subplot(122)\nsns.residplot(x = train.Id, y = np.log(train.SalePrice))","0edbf273":"plt.figure(figsize=(15,5))\nsns.scatterplot(x='YearBuilt', y=\"SalePrice\", data=train)","cd186aad":"train1 = train.drop('SalePrice',axis=1).select_dtypes(exclude='object')\ntrain2 = train.select_dtypes(include='object')\ntest1 = test.select_dtypes(exclude='object')\nprint(\"Numeric column number : \", train1.shape[1])\nprint(\"Categorical column number : \", train2.shape[1])","debf2b04":"plt.rc('axes',labelsize=9)\nplt.rc('xtick',labelsize=10)\nplt.rc('ytick',labelsize=10)\nplt.figure(figsize = (15,18))\nplt.subplots_adjust(hspace = 0.5, wspace = 0.3)\nfor i,col in enumerate(train1.columns[1:]):\n    plt.subplot(6,6,i+1)\n    sns.distplot(train1[col],kde=False)","367a2e34":"plt.figure(figsize = (15,18))\nplt.subplots_adjust(hspace = 0.5, wspace = 0.3)\nfor i,col in enumerate(test1.columns[1:]):\n    plt.subplot(6,6,i+1)\n    sns.distplot(test1[col],kde=False)","eedb888a":"pd.concat([train1.describe().T[[\"min\",\"max\"]] ,test1.describe().T[[\"min\",\"max\"]]],axis=1, sort=False)","5b38f705":"plt.figure(figsize = (20,20))\nplt.subplots_adjust(hspace = 0.5, wspace = 0.5)\nfor i,col in enumerate(train1.columns[1:]):\n    plt.subplot(6,6,i+1)\n    sns.scatterplot(x=col, y=\"SalePrice\", data=train)","06b309a2":"skew_list = []\nfor col in train1.columns:\n    median = train1[col].median()\n    skew = stats.skew(train1[col].fillna(median))\n    skew_list.append(skew)\nskew_table = pd.DataFrame(skew_list, index = train1.columns, columns=[\"skew\"])\n\nplt.figure(figsize=(13,4))\nsns.barplot(skew_table.index, skew_table[\"skew\"])\nplt.xticks(rotation=90)","8ec03a56":"plt.style.use('seaborn-darkgrid')\nplt.figure(figsize = (18,25))\nplt.subplots_adjust(hspace = 0.7, wspace = 0.5)\nfor i,col in enumerate(train2.columns[1:]):\n    plt.subplot(9,5,i+1)\n    sns.boxplot(train2[col],train['SalePrice'],linewidth=2)\n    plt.xticks(rotation=90)","a760f5ae":"for col in train1.columns:\n    print('{} : {}'.format(col,round(train[col].var()\/train[col].mean(),3),2))","ea66ccb6":"plt.figure(figsize=(4,13))\nsns.heatmap(train.corr(method=\"spearman\")[[\"SalePrice\"]].sort_values(by = \"SalePrice\", ascending = False)[1:],annot=True,cmap = \"Blues\")","9ab0c815":"plt.figure(figsize = (15,15))\nsns.heatmap(train.corr(method=\"spearman\"),cmap = 'Blues',annot=True,fmt='.1f')","a7aa2dab":"low_cor = train.corr(method=\"spearman\")['SalePrice']\nlow_cor = low_cor[abs(low_cor)<0.1]\nplt.rc('axes',labelsize=9)\nplt.rc('xtick',labelsize=10)\nplt.rc('ytick',labelsize=10)\nplt.figure(figsize = (20,8))\nplt.subplots_adjust(hspace = 0.5, wspace = 0.3)\nfor i,col in enumerate(low_cor.index[1:]):\n    plt.subplot(2,5,i+1)\n    sns.scatterplot(train[col],train['SalePrice'])","aff891d8":"drop_col = [\"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\", \"Street\", \"Utilities\"]\n\ntest.loc[test['GarageYrBlt'] == 2207,'GarageYrBlt'] = 2007\n\nfor df in [train, test]:\n    df.drop(drop_col, axis=1, inplace=True)\n    for col in [\"FireplaceQu\",\"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\",\n                \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\",'MasVnrType']:\n        df[col] = df[col].fillna(\"None\")\n    \n    for col in ['GarageYrBlt', 'GarageArea', 'GarageCars','BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea']:\n        df[col] = df[col].fillna(0)","83bde2fd":"for df in [train, test]:\n    df['MSZoning'] = df['MSZoning'].fillna(df['MSZoning'].mode()[0])\n    df[\"Functional\"] = df[\"Functional\"].fillna(df['SaleType'].mode()[0])\n    df['Electrical'] = df['Electrical'].fillna(df['Electrical'].mode()[0])\n    df['KitchenQual'] = df['KitchenQual'].fillna(df['KitchenQual'].mode()[0])\n    df['Exterior1st'] = df['Exterior1st'].fillna(df['Exterior1st'].mode()[0])\n    df['Exterior2nd'] = df['Exterior2nd'].fillna(df['Exterior2nd'].mode()[0])\n    df['SaleType'] = df['SaleType'].fillna(df['SaleType'].mode()[0])\n    df[\"LotFrontage\"].interpolate(method='linear',inplace=True)\n    df['MoSold'] = df['MoSold'].astype(str)\n    df['YrSold'] = df['YrSold'].astype(str)\n    df['GarageYrBlt'] = df['GarageYrBlt'].astype(str)\n    df['BsmtUnfSF'] = np.log1p(df['BsmtUnfSF'])","32435b60":"train1 = train[train.columns[train.isnull().sum()!=0]]\ntest1 = test[test.columns[test.isnull().sum()!=0]]\nna_prop1 = (train1.isnull().sum() \/ train1.shape[0]).sort_values()\nna_prop2 = (test1.isnull().sum() \/ test1.shape[0]).sort_values()\nprint(na_prop1)\nprint(na_prop2)","446ae6f0":"train = train.drop(train[train.LotArea > 100000].index)\ntrain = train.drop(train[(train.OverallQual==10) & (train.SalePrice<200000)].index)\ntrain = train.drop(train[(train.OverallCond==6) & (train.SalePrice > 700000)].index)\ntrain = train.drop(train[(train.GrLivArea>4000) & (train.SalePrice<200000)].index)\ntrain = train.drop(train[(train.YearBuilt<1900) & (train.SalePrice>400000)].index)\ntrain = train.drop(train[(train.YearBuilt>2000) & (train.SalePrice<100000)].index)\ntrain = train.drop(train[(train.MasVnrArea==0) & (train.SalePrice>650000)].index)\ntrain = train.drop(train[(train.OpenPorchSF>=500) & (train.SalePrice<50000)].index)\ntrain = train.drop(train[(train.BsmtHalfBath==1) & (train.SalePrice>700000)].index)\ntrain = train.drop(train[(train.YrSold==\"2007\") & (train.SalePrice>700000)].index)\ntrain = train.drop(train[(train.BedroomAbvGr==8)].index)\ntrain = train.drop(train[(train.LotFrontage>=300)].index)\ntrain.reset_index(drop=True,inplace=True)\ntrain[\"SalePrice\"] = np.log(train[\"SalePrice\"])\nprint(train.shape)","924a35d1":"test.loc[(test['YearBuilt'] > test['YearRemodAdd']), 'YearRemodAdd'] = test.loc[(test['YearBuilt'] > test['YearRemodAdd']), 'YearBuilt']\nfor df in [train, test]:\n    df['Remod-Bulit'] = df['YearRemodAdd'] - df['YearBuilt']\n    df['Total_Bathrooms'] = df['FullBath'] + df['HalfBath'] + df['BsmtFullBath'] + df['BsmtHalfBath']\n    df['Total_SF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n    df['Total_Square_Feet'] = df['BsmtFinSF1'] + df['BsmtFinSF2'] + df['1stFlrSF'] + df['2ndFlrSF']\n    df['Total_Porch'] = df['OpenPorchSF'] + df['3SsnPorch'] + df['EnclosedPorch'] + df['ScreenPorch'] + df['WoodDeckSF']\n    df['ispool'] = df['PoolArea'].apply(lambda x: \"1\" if x > 0 else \"0\")\n    df['isgarage'] = df['GarageArea'].apply(lambda x: \"1\" if x > 0 else \"0\")\n    df.drop('PoolArea',axis=1, inplace = True)","57a626b3":"train1 = train.drop('SalePrice',axis=1).select_dtypes(exclude='object')\ntrain2 = train.select_dtypes(include='object')\nprint(\"Numeric column number : \", train1.shape[1])\nprint(\"Categorical column number : \", train2.shape[1])","173bb257":"from sklearn.preprocessing import LabelEncoder\ntrain_y = train[\"SalePrice\"]\nall_data = pd.concat([train.drop('SalePrice',axis=1),test],axis=0,sort=False)\ntrain2 = train.select_dtypes(include='object')\nfor c in train2.columns:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\ntrain = all_data.iloc[:train.shape[0], :]\ntest = all_data.iloc[train.shape[0]:, :]","b6bdea89":"train.drop('Id', axis=1,inplace=True)\ntest.drop('Id', axis=1,inplace=True)","bcdbe38d":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(train, train_y, test_size=0.2, random_state=10)","e8607ee7":"import lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nlgb_regressor=lgb.LGBMRegressor(objective='regression', num_leaves=5, learning_rate=0.02, n_estimators=2310, max_bin=50, bagging_fraction=0.9,bagging_freq=5, bagging_seed=7, \n                                feature_fraction=0.9, feature_fraction_seed=123,n_jobs=-1)\nlgb_regressor.fit(x_train,y_train)\ny_head=lgb_regressor.predict(x_test)\nmean_squared_error(y_test, y_head)","d59d94f5":"tmp = pd.DataFrame({'Feature': x_train.columns, 'Feature importance': lgb_regressor.feature_importances_})\ntmp = tmp.sort_values(by='Feature importance',ascending=False)\nplt.figure(figsize = (15,15))\nplt.title('Features importance',fontsize=14)\ns = sns.barplot(x='Feature',y='Feature importance',data=tmp)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)","c30d8fb5":"from sklearn.model_selection import KFold \nfrom sklearn.metrics import mean_squared_error\ndef lgb_cv(n_estimators, reg_alpha, reg_lambda, min_split_gain, min_child_weight,min_child_samples, colsample_bytree, x_data=None, y_data=None, n_splits=5, output='score'):\n    score = 0\n    kf = KFold(n_splits=n_splits)\n    models = []\n    for train_index, valid_index in kf.split(x_data):\n        x_train, y_train = x_data.iloc[train_index], y_data[train_index]\n        x_valid, y_valid = x_data.iloc[valid_index], y_data[valid_index]\n        \n        model = lgb.LGBMRegressor(\n            num_leaves = 4, \n            learning_rate = 0.01, \n            n_estimators = int(n_estimators), \n            reg_alpha = reg_alpha, \n            reg_lambda = reg_lambda,\n            min_split_gain= min_split_gain,\n            min_child_weight = min_child_weight,\n            min_child_samples = int(min_child_samples),\n            colsample_bytree = np.clip(colsample_bytree, 0, 1), \n        )\n        \n        model.fit(x_train, y_train)\n        models.append(model)\n        \n        pred = model.predict(x_valid)\n        true = y_valid\n        score -= mean_squared_error(true, pred)\/n_splits\n    \n    if output == 'score':\n        return score\n    if output == 'model':\n        return models","3f7a83c4":"from functools import partial \nfrom bayes_opt import BayesianOptimization\nfunc_fixed = partial(lgb_cv, x_data=train, y_data=train_y, n_splits=5, output='score')\nlgbBO = BayesianOptimization(\n    func_fixed, \n    {     \n        'n_estimators': (1000, 3000),                        \n        'reg_alpha': (0.0001, 1),       \n        'reg_lambda': (0.0001, 1), \n        'min_split_gain' : (0.001, 0.1),\n        'min_child_weight' : (0.001, 0.1),\n        'min_child_samples' : (10,25),\n        'colsample_bytree': (0.85, 1.0),\n    }, \n    random_state=4321            \n)\nlgbBO.maximize(init_points=5, n_iter=20)","1ab73d29":"params = lgbBO.max['params']\nlgb_models = lgb_cv(\n    params['n_estimators'], \n    params['reg_alpha'], \n    params['reg_lambda'], \n    params['min_split_gain'], \n    params['min_child_weight'],\n    params['min_child_samples'],\n    params['colsample_bytree'],\n    x_data=train, y_data=train_y, n_splits=5, output='model')","393f2eaf":"from sklearn.ensemble import GradientBoostingRegressor\ngb_regressor = GradientBoostingRegressor(n_estimators=1992, learning_rate=0.03, max_depth=3, max_features='sqrt', min_samples_leaf=15, min_samples_split=8, loss='huber', random_state =42)\ngb_regressor.fit(x_train,y_train)\ny_head=gb_regressor.predict(x_test)\nmean_squared_error(y_test, y_head)","e802cf64":"from xgboost import XGBRegressor\nxgb_regressor = XGBRegressor(learning_rate=0.02,n_estimators = 2400,max_depth=3, min_child_weight=0.01,gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,objective='reg:linear', nthread=-1, scale_pos_weight=1, seed=27,reg_alpha=0.00006)\nxgb_regressor.fit(x_train,y_train)\ny_head=xgb_regressor.predict(x_test)\nmean_squared_error(y_test, y_head)","b77a93fb":"from mlxtend.regressor import StackingCVRegressor\nstack_regressor = StackingCVRegressor(regressors=(gb_regressor, lgb_regressor, xgb_regressor),meta_regressor=gb_regressor, use_features_in_secondary=True)\nstack_regressor.fit(np.array(x_train),np.array(y_train))\ny_head=stack_regressor.predict(np.array(x_test))\nmean_squared_error(y_test, y_head)","5a6f3f69":"final_pred1 = lgb_regressor.predict(test)\npreds = []\nfor model in lgb_models:\n    pred = model.predict(test)\n    preds.append(pred)\nfinal_pred2 = np.mean(preds, axis=0)\nfinal_pred3 = gb_regressor.predict(test)\nfinal_pred4 = xgb_regressor.predict(test)\nfinal_pred5 = stack_regressor.predict(np.array(test))","1291f23a":"y_pred = np.floor(np.exp(0.2*final_pred1+0.2*final_pred2 + 0.2 * final_pred3 + 0.2 * final_pred4  + 0.2 * final_pred3))","e3be72ec":"submit = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsubmit[\"SalePrice\"] = y_pred\nsubmit.to_csv('submit.csv', index=False)\nsubmit.head()","095b5085":"## coefficient of variation","0512fc96":"## Processing outliers\n\n\nBased on above graphs, remove outliers. Mostly, variables with a high correlation were seen","e90cf891":"It seems that the price of the house gradually increases a little.\n\n\n## Observation of independent variable\n\nDivide train data into numeric and category","71b9a1ef":"We can see strong correlation in (TotalBsmtSF and 1stFlrSF), (GarageCars , GarageArea), (GarageYrBlit, YearBuilt), (GrLivArea, TotRmsAbcGrd).","47e21f5f":"Check data's shape don't have NA without variables has a lot of NA ratio.","13a98e6d":"# 5. Predict","cbb20139":"Check there is the rest missing values.","36a481c3":"# Introduction\n\nIn house prices datasets, there are many variable. So i refer to data script file and make pandas profiling documents.\n\n# 1. Preparations and data Exploration\n\n\n## Import basic library and datasets","b8d4d603":"It seems to satisfy the normality. Let's compare these with resident plot.","c8c52a88":"## GradientBoostingRegressor","0531fd0d":"# 4. Modeling\n\nI use some model and ensemble.\n\n## Lightgbm","15cbaef4":"## Check missing value","0873ea7e":"Fill the rest with mode.\nLotFrontage is filled using interploate.\nAnd change type","3d7a8cb1":"SalePrice is not normally distributed. target is right-skewed. Let's see same plot to get log transform.","ce507d1c":"- In some variables, as if there are skewness and imbalance.\n- MoSold and YrSold need to change category\n- In test, there is invalid value. (GarageYrBlt == 2207) I decided to think that 2007 wasn't entered as a 2207.","eb42848e":"## Lightgbm + BayesianOptimization","621ce068":"\n# 3. Data processing\n\n## Dealing with missing values\n\n\n- Defining drop variables over 80% NA values.\n- Some variables have small categorical variety and unbalance(Street, Utilities)\n- Seeing categorical data description file, there are description of NA values. There are NA values mean that there are no values. And MasVnrType has 'None' value and it occupy a large proportion.\n- Likewise, numeric data too. So replace 0.\n- Based on above results, process train and test data.","bb1cf340":"## XGBRegressor","906f26b1":"# 2. Exploratory Data Analysis\n\n## Observation of target variable","9cbe7487":"In left plot, we can see some splashing values. But right plot is appropriate than left. It seems to log transform make normal distributes.\n\n\nSee plot house prices by Yearbulit","92d4ef5a":"## StackingCVRegressor","5f8d9b60":"Labeling data and dividing train set.","00c33651":"Result is 0.12085","6b4c0c93":"## Create new features\n\n- Remod-Bulit : year of remodeling - year of yearbulit\n- Total_Bathrooms : fullbath + halfbath + bstmfullbath + bstmhalfbath\n- Total_SF : TotalBsmtSF + 1stFlrSF + 2ndFlrSF\n- Total_square_feet : BsmtFinSF1 + BsmtFinSF2 + 1stFlrSF + 2ndFlrSF\n- Total_Porch_Area = OpenPorchSF + 3SsnPorch + EnclosedPorch + ScreenPorch + WoodDeckSF\n- ispool : zero value is 0, rest value is 1\n- isgarage : have garage\n\nIn the process of creating a variable, I find data that YearRemodAdd < YearBuilt in testset, so I change this to same","e3704310":"## correlation anaylsis"}}