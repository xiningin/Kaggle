{"cell_type":{"8ab31000":"code","dc1ec659":"code","92232d18":"code","bc9fc9f9":"code","6855d017":"code","7dbad0ee":"code","49475a49":"code","a132c35a":"code","155b6b7c":"code","b34d8e71":"code","b676f1bf":"code","acad28cd":"code","c9e45a59":"code","c93b565a":"code","51f2c560":"code","c91b4ee5":"code","c8df0e34":"code","ebf14f71":"code","57091523":"code","fbc67646":"code","b2ac17bc":"code","65f76471":"code","12bbff77":"code","07130760":"code","1970e00c":"code","10684b07":"code","4c218aa4":"code","2e89bc9c":"code","7ca583a2":"code","81d90255":"code","59ce0a7e":"code","9e21a092":"code","87d010b6":"code","a96082c6":"code","5fe2269f":"code","b9665937":"code","a69c9c61":"code","c48ee5a4":"code","8455d507":"code","50ab5084":"code","b37eb226":"code","08a50b5d":"code","f30adad6":"code","0b44f60b":"code","7ecca1f9":"code","6278cb66":"code","9ea69a05":"code","5cb88700":"code","a26a5116":"code","b9bc712c":"code","e27b404a":"code","1bba27ab":"code","2b1e033f":"code","2ca62982":"code","9b68623b":"code","26679d10":"code","596ad8a0":"code","82d5070a":"code","044a6b62":"code","d2f32ee4":"code","3ea70acf":"code","76503715":"code","b268b557":"code","cbebf864":"code","adb92284":"code","47b137c3":"code","96297add":"code","03d56b77":"code","a832210a":"code","ccc359c1":"code","d67bb540":"code","89963607":"code","01c88a3b":"code","e298b85d":"code","6357116f":"code","a116707a":"code","2b9ac725":"code","fbe2c9c9":"code","946c3143":"code","5869dd83":"code","2f579b52":"code","aa64793e":"code","7eead4b0":"code","8213f69c":"code","0c4b9647":"code","190437f8":"code","690afea7":"code","925fe4a5":"code","f594d5c4":"code","3e50c4bd":"code","c92f79dd":"code","b7a90278":"code","373091ba":"code","b3cf8cb3":"code","9fcd3227":"code","af6baa92":"code","e53927c6":"code","ada74b07":"code","dcf6b8b2":"code","545fbc77":"code","676622df":"code","c9d45310":"code","6dec0ca3":"code","b81c91a4":"code","502b22ef":"code","33fec557":"code","d481adef":"code","a68e1c12":"code","0c0bfabb":"code","9c345275":"code","f1c7f737":"code","b3824473":"code","3d57810d":"code","85a34c96":"code","be7b7d5e":"code","1e7c7dc9":"code","14c6ce2c":"code","61bb9973":"code","c62eb23b":"code","5ebe6ba9":"code","7b68f28f":"code","a7784177":"code","33863f9e":"code","d9252384":"code","e587e2ef":"code","7b86ac66":"code","3992c5d7":"code","a9742579":"code","50e5028b":"code","777f6bf3":"code","ee75a7ba":"code","9f0ee863":"code","e3ee5234":"code","3ef8d3c4":"code","05bc6229":"code","dde6b08c":"code","9f2dfd25":"code","27cabffe":"code","9abebb0e":"code","6fe75f40":"code","44b55113":"code","a9508d20":"code","629b8633":"code","4af1fee8":"code","ee7d234e":"code","61bd8f99":"code","896dcca4":"code","9b28e4fe":"code","d6968d0f":"code","8696327d":"code","0b0b5f3f":"markdown","512b6e1b":"markdown","18e61fdc":"markdown","25a6332e":"markdown","a474b4d1":"markdown","e05b0584":"markdown","6fbd40ba":"markdown","ba9ecd9c":"markdown","0a6bdcba":"markdown","69ca44f1":"markdown","26521966":"markdown","5b847762":"markdown","e55f0ab2":"markdown","c48b4d23":"markdown","314fbdde":"markdown","8eeaa45e":"markdown","12608a20":"markdown","e49fa68c":"markdown","2e1e610b":"markdown","c4da7178":"markdown","aa347586":"markdown","a26b864f":"markdown","4bf1c30a":"markdown","d27c6211":"markdown","74ee9235":"markdown","f62ef50c":"markdown","28092864":"markdown","09469496":"markdown","2f196991":"markdown","5b17e78d":"markdown","b7b6be6f":"markdown","78640488":"markdown","654a5683":"markdown","d436d04a":"markdown","93f6d52a":"markdown","492bcf2b":"markdown","2a0f5ce5":"markdown","5331e5bb":"markdown","2eb0c5a1":"markdown","7311e557":"markdown","047d26e5":"markdown","50bfebd1":"markdown","037cbfd8":"markdown","32fdc606":"markdown","e85cbb11":"markdown","8daae6de":"markdown","3dd5b9ba":"markdown","87d35493":"markdown","35ee1509":"markdown","f8ee67e3":"markdown","7f48a998":"markdown","e15d5bdf":"markdown","5803618a":"markdown","95469bb9":"markdown","b7d6f49d":"markdown","404b431e":"markdown","78f71682":"markdown","4491700b":"markdown","00d0c1be":"markdown","7f1864ba":"markdown","d63b29ee":"markdown","e8551712":"markdown","7cef28fc":"markdown","e1938964":"markdown","efccb443":"markdown","88d5b7f0":"markdown","dd2fe6d8":"markdown","7013c9a9":"markdown","bff8ac68":"markdown","4f95dafb":"markdown","1d7e0204":"markdown","089a3003":"markdown","ea2d68dc":"markdown","3f1e6315":"markdown","e3b5f2da":"markdown","1bf31ae9":"markdown","5765d5a3":"markdown","10dc369b":"markdown","c517f5bc":"markdown","47da0f81":"markdown","13ce17e6":"markdown","2188a59e":"markdown","ab4e07ca":"markdown","7f8e5f47":"markdown","471e91be":"markdown","32eb58f8":"markdown","dbaabf4c":"markdown","b9a648eb":"markdown","22d5ae2c":"markdown"},"source":{"8ab31000":"#general\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sys\nimport time, datetime\nimport pickle\nimport math","dc1ec659":"# EDA\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","92232d18":"# ML libraries\n## Preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import decomposition\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\n\n## Pipelines\nfrom imblearn.pipeline import Pipeline # Pipeline for imbalanced dataset, oversampling ONLY in train, not in test\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline # another way of doing it, good for oversampling\n\n## ML Algorithms\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree, export_graphviz\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import LinearSVC \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import IsolationForest\nfrom xgboost import XGBClassifier\n\n# Neuronal networks\nimport tensorflow as tf\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Input\nfrom keras.models import Model, load_model\nfrom keras.callbacks import ModelCheckpoint, TensorBoard\nfrom keras import regularizers\n\n# Metrics and visualizers\nfrom sklearn.metrics import make_scorer, auc, accuracy_score, recall_score, precision_score, roc_auc_score, roc_curve, f1_score, classification_report, confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix, plot_roc_curve\nfrom sklearn.model_selection import cross_val_score, validation_curve","bc9fc9f9":"# ML libraries\n## FAISS Library, to speed knn --> Scikitlearn is too slow\n# https:\/\/towardsdatascience.com\/make-knn-300-times-faster-than-scikit-learns-in-20-lines-5e29d74e76bb\n# https:\/\/github.com\/facebookresearch\/faiss\n# https:\/\/github.com\/facebookresearch\/faiss\/issues\/890\n!apt -qq install -y libomp-dev\n!pip install faiss\nimport faiss","6855d017":"# LOCAL MODULES\ndata_path = \"..\/input\/creditcardfraud\/\"\noutput_path = \".\/\"","7dbad0ee":"# GENERAL PURPOSE\nRANDOM_STATE = 42\n\n# DATA\ndata_origin = \"creditcard.csv\"","49475a49":"# REPORTING Class\n# This class is used to both compare train and test metrics of a model, and compare the different metrics\n# of all the created models.\nclass ClassificationModelReport:\n\n    model = None\n    data = [\n        # 0 - training\n        {\n            'X': None, \n            'y': None,\n            'y_pred': None, \n        },\n        # 1- validation\n        {\n            'X': None, \n            'y': None,\n            'y_pred': None, \n        },\n    ]\n    metrics = [] # list storing dictionaries of metrics for train [0] and val [1]\n    summary = {}\n\n    def init_report(self, model, X_train, y_train, X_val, Y_val):  \n        \"\"\"Stores data and calculates the different metrics, stores it in the local vars and show them on screen\"\"\"\n\n        # train\n        self.model = model\n        self.data[0][\"X\"] = X_train\n        self.data[0][\"y\"] = y_train\n        if \"FaissKNeighbors\" in str(type(model)):\n            self.data[0][\"y_pred\"] = pd.Series(model.predict(np.ascontiguousarray(X_train.values)), index=X_train.index, name=\"Class_predicted\")\n            self.data[0][\"y_pred_proba\"] = None\n        elif \"tensorflow.python.keras\" in str(type(model)): # prediction returns an array of arrays. We have to convert it. Also, predict returns predict_proba, and predict_classes returns predict\n            self.data[0][\"y_pred\"] = pd.Series([item[0] for item in model.predict_classes(X_train)], index=X_train.index, name=\"Class_predicted\")\n            self.data[0][\"y_pred_proba\"] = None # pd.Series([item[0] for item in model.predict(X_train)], index=X_train.index, name=\"Class_predicted\")\n        else:\n            self.data[0][\"y_pred\"] = pd.Series(model.predict(X_train), index=X_train.index, name=\"Class_predicted\")\n            if (model.predict_proba(X_train) is not None):\n                self.data[0][\"y_pred_proba\"] = pd.Series(pd.DataFrame(model.predict_proba(X_train), index=X_train.index).iloc[:,-1], name=\"Class_predicted\")\n            else:\n                self.data[0][\"y_pred_proba\"] = None\n\n        #test\n        self.data[1][\"X\"] = X_val\n        self.data[1][\"y\"] = y_val\n        if \"FaissKNeighbors\" in str(type(model)):\n            self.data[1][\"y_pred\"] = pd.Series(model.predict(np.ascontiguousarray(X_val.values)), index=X_val.index, name=\"Class_predicted\")\n            self.data[1][\"y_pred_proba\"] = None\n        elif \"tensorflow.python.keras\" in str(type(model)): # prediction returns an array of arrays. We have to convert it. Also, predict returns predict_proba, and predict_classes returns predict\n            self.data[1][\"y_pred\"] = pd.Series([item[0] for item in model.predict_classes(X_val)], index=X_val.index, name=\"Class_predicted\")\n            self.data[1][\"y_pred_proba\"] = None # pd.Series([item[0] for item in model.predict(X_val)], index=X_val.index, name=\"Class_predicted\")\n        else:\n            self.data[1][\"y_pred\"] = pd.Series(model.predict(X_val), index=X_val.index, name=\"Class_predicted\")\n            if (model.predict_proba(X_test) is not None):\n                self.data[1][\"y_pred_proba\"] = pd.Series(pd.DataFrame(model.predict_proba(X_val), index=X_val.index).iloc[:,-1], name=\"Class_predicted\")\n            else:\n                self.data[1][\"y_pred_proba\"] = None\n\n        # Calculate the metrics\n        self.metrics = [self.empty_metrics(), self.empty_metrics()] # train and validation\n        for i in range(0, len(self.metrics)):\n            self.metrics[i][\"accuracy\"] = round(accuracy_score(self.data[i][\"y\"], self.data[i][\"y_pred\"]),4)\n            self.metrics[i][\"precision\"] = round(precision_score(self.data[i][\"y\"], self.data[i][\"y_pred\"]),4)\n            self.metrics[i][\"recall\"] = round(recall_score(self.data[i][\"y\"], self.data[i][\"y_pred\"]),4)\n            self.metrics[i][\"f1\"] = round(f1_score(self.data[i][\"y\"], self.data[i][\"y_pred\"]),4)\n            self.metrics[i][\"cm\"] = confusion_matrix(self.data[i][\"y\"], self.data[i][\"y_pred\"])\n            if self.data[i][\"y_pred_proba\"] is not None:\n                self.metrics[i][\"auc\"] = round(roc_auc_score(self.data[i][\"y\"], self.data[i][\"y_pred_proba\"]),4)\n                self.metrics[i][\"tpr\"], self.metrics[i][\"fpr\"], thresholds = roc_curve(self.data[i][\"y\"], self.data[i][\"y_pred_proba\"])\n  \n    def show_report(self):\n        '''Shows the report for this model'''\n\n        display(pd.DataFrame(self.metrics, index=[\"train\",\"validation\"])[[\"accuracy\",\"precision\",\"recall\",\"f1\",\"auc\"]])\n\n        fig, axs = plt.subplots(1, 2, figsize=(14, 5), sharex=True)\n        axs[0].title.set_text(\"Train Confusion Matrix\")\n        sns.heatmap(self.metrics[0][\"cm\"], annot=True, annot_kws={\"size\":9}, fmt='g', ax=axs[0]) # fmt = format of annot, in that case, plain notation (g)\n        axs[1].title.set_text(\"Validation Confusion Matrix\")\n        sns.heatmap(self.metrics[1][\"cm\"], annot=True, annot_kws={\"size\":9}, fmt='g', ax=axs[1]) # fmt = format of annot, in that case, plain notation (g)\n        plt.show()\n\n        if (self.data[0][\"y_pred_proba\"] is not None) & (self.data[1][\"y_pred_proba\"] is not None):\n            fig, axs = plt.subplots(1, 2, figsize=(14, 5), sharex=True)\n            axs[0].title.set_text(\"Train ROC Curve\")\n            plot_roc_curve(self.model, self.data[0][\"X\"], self.data[0][\"y\"], ax=axs[0])\n            axs[1].title.set_text(\"Validation ROC Curve\")\n            plot_roc_curve(self.model, self.data[1][\"X\"], self.data[1][\"y\"], ax=axs[1])\n\n    def plot_roc_curve_custom(self, y = None, y_pred_prob = None): # deprecated\n        \"\"\"Plots the roc curve of the trained model\"\"\"\n        if y is None: y, y_pred_prob = self.y, self.y_pred_prob\n\n        fpr, tpr, threshold = roc_curve(y, y_pred_prob)\n        roc_auc = auc(fpr, tpr)\n        plt.title('Receiver Operating Characteristic')\n        plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n        plt.legend(loc = 'lower right')\n        plt.plot([0, 1], [0, 1],'r--')\n        plt.xlim([0, 1])\n        plt.ylim([0, 1])\n        plt.ylabel('True Positive Rate')\n        plt.xlabel('False Positive Rate')\n        plt.margins(0.1)\n        plt.show()\n  \n    def show_cv_results(self, cv_results, metric):\n        '''\n        Compares the results of a GridSearchCV\n\n        parameters:\n        - cv_results -> the cv_results_ object stored in the GridSearchCV instance (GridSearchCV.cv_results_)\n        - metric -> the name of the metric to compare\n        '''\n        # The detailed results of the Cross-Validation process are stored the follwing way\n        # cv_results_ is a dictionary\n        # Metric results are stored for each Kfold split in the following keys: \n        # - split*number_of_fold*_test_*metric*\n        # - split*number_of_fold*_train_*metric*\n        # For each kfold split there is an array with length j. The length j corresponds to all combinations of the parameters fed to GridSearchCV\n        # The different combinations of hypermparameters are stored in this array, from 0 to j-1\n\n        print (\"********************************\")\n        print (\"METRIC: {}\".format(metric.upper()))\n        print (\"********************************\")\n\n        fig, axes = plt.subplots(1, len(cv_results[\"params\"]), sharey = True, figsize=(5*len(cv_results[\"params\"]), 5))\n        fig.subplots_adjust(hspace=0.3)\n        # plt.xticks(np.arange(gscv.n_splits_), np.arange(1,gscv.n_splits_+1))\n\n        # We gather the best model that GridSearchCV has calculated\n        # e.g. 'rank_test_accuracy': array([2, 1, 3], dtype=int32)\n        iModel_best = cv_results[\"rank_test_\" + metric].tolist().index(1) # 1 is the first position in ranking, the best model in test dataset\n\n        # We loop through the hyperparameters combination\n        models_scores = pd.DataFrame(columns=[\"model\",\"split\",\"train\",\"test\"])\n        for iModel, model in enumerate(cv_results[\"params\"]): # params is an array of dictionaries\n            split_metric_scores = None # this list will store the train and test metric scores for each of the splits\n            has_metrics = True\n            for iSplit in range(0, gscv.n_splits_):\n                # if we find that there are no values, we must inform the result as nan\n                if np.isnan(cv_results[\"split\" + str(iSplit) + \"_train_\" + metric][iModel]): has_metrics = False\n                if has_metrics:\n                    # store the metric\n                    split_metric_scores = pd.DataFrame([[\n                        iModel, \n                        int(iSplit),\n                        cv_results[\"split\" + str(iSplit) + \"_train_\" + metric][iModel], \n                        cv_results[\"split\" + str(iSplit) + \"_test_\" + metric][iModel]\n                    ]], columns=models_scores.columns)\n                else:\n                    # store nan\n                    split_metric_scores = pd.DataFrame([[\n                        iModel, \n                        int(iSplit),\n                        np.nan, \n                        np.nan\n                    ]], columns=models_scores.columns)\n                models_scores = models_scores.append(split_metric_scores, ignore_index=True)\n            # and we plot the values for train and test for the different splits in the model\n            axes[iModel].title.set_text(\"MODEL {}\".format(iModel))\n            if iModel == iModel_best: axes[iModel].set_facecolor('wheat') #highlight the selected model by GridSearchCV\n            if has_metrics: sns.lineplot(data=models_scores[models_scores[\"model\"]==iModel].iloc[:,1:].set_index(\"split\"), ax=axes[iModel])\n        plt.show()\n\n        # And one last plot to compare the models\n        fig, axes = plt.subplots(1, 2, sharey = True, sharex = True, figsize=(10,5))\n        fig.subplots_adjust(wspace=0.3)\n        axes[0].title.set_text(\"MODEL COMPARISON - Train\")\n        sns.boxplot(data=models_scores, x=\"model\", y=\"train\", ax=axes[0], showmeans=True)\n        axes[1].title.set_text(\"MODEL COMPARISON - Test\")\n        sns.boxplot(data=models_scores, x=\"model\", y=\"test\", ax=axes[1], showmeans=True)\n        plt.show()\n\n    def show_nnhistory_results(self, nnhistory, metrics):\n        '''\n        Plots the results returned by the history object of keras tensor flow model.\n\n        parameters:\n        nnhistory -> keras.fit output\n        metrics -> string of the metrics used for validation\n\n        returns:\n        plots entropy loss per epoch evolution and the corresponding metric, both for train and test datasets\n        '''\n        \n        fig = plt.figure(figsize=(6,5))\n        plt.title('Binary Cross Entropy Loss')\n        plt.plot(nnhistory.history['loss'], color='blue', label='train')\n        plt.plot(nnhistory.history['val_loss'], color='orange', label='test')\n        legend = plt.legend(loc='upper left', shadow=True, fontsize='large')\n        plt.show()\n        \n        if 1==2: # currently disabled. TODO: Metrics do not behave correctly. I need to check why.\n            if \"list\" in str(type(metrics)):\n                fig, axes = plt.subplots(1, len(metrics), figsize=(len(metrics)*6, 5), sharex = True)\n                for i, m in enumerate(metrics):\n                    axes[i].set_title('Classification ' + m.upper())\n                    axes[i].plot(nnhistory.history[m], color='blue', label='train')\n                    axes[i].plot(nnhistory.history['val_' + m], color='orange', label='test')\n                    axes[i].legend(loc='upper left', shadow=True, fontsize='large')\n                plt.show()\n            else:\n                plt.title('Classification ' + metrics.upper())\n                plt.plot(nnhistory.history[metrics], color='blue', label='train')\n                plt.plot(nnhistory.history['val_' + metrics], color='orange', label='test')\n                legend = plt.legend(loc='upper left', shadow=True, fontsize='large')\n                plt.show()\n            \n  \n    def get_model_params(self, cv_results, model_key):\n        '''\n        Returns the params of the model using the key in the GridSearchCV cv_results_ object.\n\n        parameters:\n        cv_results -> GridSearchCV.cv_results_\n        model_key -> the key of the selected model on cv_results[\"params\"]\n\n        returns:\n        dictionary with the model parameters\n        '''\n        return {model_key:cv_results[\"params\"][model_key]}\n\n    def store_report(self, key = \"\", name = \"\"):\n        \"\"\"Add the VALIDATION metrics of the current model to the dictionary of compared models\"\"\"\n\n        self.summary[key] = {\n            \"name\" : name,\n            \"model\" : self.model, \n            \"y\": self.data[1][\"y\"], \n            \"y_pred\": self.data[1][\"y_pred\"], \n            \"y_pred_proba\": self.data[1][\"y_pred_proba\"], \n            \"accuracy\" : self.metrics[1][\"accuracy\"], \n            \"precision\": self.metrics[1][\"precision\"], \n            \"recall\": self.metrics[1][\"recall\"], \n            \"f1\": self.metrics[1][\"f1\"], \n            \"auc\" : self.metrics[1][\"auc\"], \n            \"tpr\": self.metrics[1][\"tpr\"], \n            \"fpr\": self.metrics[1][\"fpr\"], \n            \"cm\": self.metrics[1][\"cm\"], \n        }\n\n    def compare_models(self, order_by=\"accuracy\"):\n        \"\"\"Plots the metrics of the different models that have been stored\"\"\"\n        df_summary = pd.DataFrame(self.summary).T.reset_index()\n        return df_summary[[\"name\",\"accuracy\",\"recall\",\"precision\",\"f1\",\"auc\"]].sort_values(by=order_by, ascending=False)\n\n    def empty_metrics(self):\n        return {\n          'accuracy': None, \n          'precision': None, \n          'recall': None, \n          'f1': None, \n          'cm': None, \n          'auc': None, \n          'fpr': None, \n          'tpr': None, \n        }\n\nreport = ClassificationModelReport()","a132c35a":"# ISOLATION FOREST wrapper\n# We override the predict method to return fraud\/non-fraud instead of outlier\/non-outlier\n# https:\/\/scikit-learn.org\/stable\/developers\/develop.html\nclass IsolationForestWrapper():    \n    '''\n    This class is a wrapper for IsolationForest. We override the predict method to return fraud\/non-fraud instead of outlier\/non-outlier\n    '''\n    \n    model = None\n    n_estimators = None\n    max_samples = None\n    max_features = None\n    contamination = None\n    bootstrap = None\n    verbose = None\n    random_state = None\n    \n    def __init__(self, n_estimators = 100, max_samples = 1.0, max_features = 1.0, contamination = 'auto', bootstrap = False, verbose = 0, random_state = RANDOM_STATE):\n        \n        self.n_estimators = n_estimators\n        self.max_samples = max_samples\n        self.max_features = max_features\n        self.contamination = contamination\n        self.bootstrap = bootstrap\n        self.verbose = verbose\n        self.random_state = random_state\n        \n        self.model = IsolationForest(\n            n_estimators = n_estimators, \n            max_samples = max_samples, \n            max_features = max_features, \n            contamination = contamination, \n            bootstrap = bootstrap, \n            verbose = verbose, \n            random_state = random_state, \n        )\n    \n    def fit(self, X, y=None):\n        self.model.fit(X)\n    \n    def predict(self, X):\n        # IsolationForest predict method returns whether a value is an outlier (-1) or not (1)\n        # We have to \"transform\" our prediction based on this. If -1, we mean Fraud, and by 1 we mean NoFraud\n        y_pred = pd.Series(self.model.predict(X)).apply(lambda x: 1 if x==-1 else 0).to_numpy()\n        return y_pred # the output is an array of values\n    \n    def predict_proba(self, X):\n        return None\n    \n    def get_params(self, deep=True):\n        # suppose this estimator has parameters \"alpha\" and \"recursive\"\n        return {\n            \"n_estimators\" : self.n_estimators, \n            \"max_samples\" : self.max_samples, \n            \"max_features\" : self.max_features, \n            \"contamination\" : self.contamination, \n            \"bootstrap\" : self.bootstrap, \n            \"verbose\" : self.verbose, \n            \"random_state\" : self.random_state, \n        }\n\n    def set_params(self, **parameters):\n        for parameter, value in parameters.items():\n            setattr(self, parameter, value)\n        return self","155b6b7c":"# FAISS Class to execute Knn algorithms\n# https:\/\/github.com\/j-adamczyk\/Towards_Data_Science\/issues\/1 --> issues with series, dataframes and numpy arrays solved\nclass FaissKNeighbors:\n    def __init__(self, k=5):\n        self.index = None\n        self.y = None\n        self._y_np = None\n        self.k = k\n\n    def fit(self, X, y):\n        self.index = faiss.IndexFlatL2(X.shape[1])\n        self.index.add(X.astype(np.float32))\n        self.y = y\n        self._y_np = np.array(y, dtype=np.int)\n\n    def predict(self, X):\n        distances, indices = self.index.search(X.astype(np.float32), k=self.k)\n        # votes = self.y[indices]\n        votes = self._y_np[indices]\n        predictions = np.array([np.argmax(np.bincount(x)) for x in votes])\n        return predictions","b34d8e71":"data = pd.read_csv(data_path + data_origin)","b676f1bf":"\n# to visualize pairplots and correlations, we will reduce the data and balance the class\ndata_ = data[data[\"Class\"]==1]\ndata_ = data_.append(data[data[\"Class\"]==0].sample(n=len(data_), random_state=RANDOM_STATE))\n\n# Correlation matrix\ncorr_ = data_.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr_, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(25, 25))\n\n# Generate a custom diverging colormap\n#sns.set_theme(style=\"white\")\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr_, annot=True, mask=mask, cmap=cmap, vmax=1, center=0,\n          square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\n# We finally print boxplots for all variables\nfig, axes = plt.subplots(6, 5, figsize=(15,25), sharex = True)\nfig.tight_layout(pad=2.0)\ncols = data.columns.drop(\"Class\")\nfor i in range(0, len(cols)):\n    col = (i % 5)\n    if col == 0: row = int(i\/5) # change row\n    sns.boxplot(data=data_, x=\"Class\", y=cols[i], ax=axes[row, col], whis=4) # instead of 1.5IQR we loog for extreme outliers, over 3IQR\nplt.show()\n\n# Calculating VIF\nvif = pd.DataFrame()\nvif[\"variables\"] = data.columns[:-1]\nvif[\"VIF\"] = [variance_inflation_factor(data.iloc[:,:-1].values, i) for i in range(data.shape[1]-1)]\ndisplay(vif)","acad28cd":"# The variables come from PCA, except from Time and Amount. We will Scale these two variables in order\n# to have everything at the same level of information\nscaler = StandardScaler()\nscaler.fit(data[[\"Time\",\"Amount\"]])\nscaled_cols = scaler.transform(data[[\"Time\",\"Amount\"]])\nscaled_cols","c9e45a59":"df_ = pd.DataFrame(scaled_cols)\ndata[\"time_scaled\"] = df_[0]\ndata[\"amount_scaled\"] = df_[1]\ndata.drop([\"Time\",\"Amount\"], axis=1, inplace=True)\ndata","c93b565a":"del scaled_cols","51f2c560":"# Create X and y\nX_data = data.drop(\"Class\", axis=1)\ny_data = data[\"Class\"]","c91b4ee5":"# get dev and val\n# val is our FINAL VALIDATION DATASET, AND IT WILL BE THE SAME TO COMPARE THE DIFFERENT MODELS\nX_dev, X_val, y_dev, y_val = train_test_split(X_data, y_data, \n                                                    train_size=396\/492, \n                                                    random_state=RANDOM_STATE,\n                                                    stratify=y_data)","c8df0e34":"# Let's check the target distribution\ndisplay('dev dataset: {}'.format(y_dev.sum()\/len(y_dev)))\ndisplay('val dataset: {}'.format(y_val.sum()\/len(y_val)))","ebf14f71":"X_train, X_test, y_train, y_test = train_test_split(X_dev, y_dev, \n                                                    train_size=300\/396, \n                                                    random_state=RANDOM_STATE,\n                                                    stratify=y_dev)","57091523":"# Let's check the target distribution\ndisplay('train dataset: {}'.format(y_train.sum()\/len(y_train)))\ndisplay('test dataset: {}'.format(y_test.sum()\/len(y_test)))","fbc67646":"print(\"Data length: {}\".format(len(data)))\nprint(\"Dev length: {}\".format(len(X_dev)))\nprint(\"Train length: {}\".format(len(X_train)))\nprint(\"Test length: {}\".format(len(X_test)))\nprint(\"Validation length: {}\".format(len(X_val)))","b2ac17bc":"# Check for number of positive cases\nprint(\"Number of positive classes = {}\".format(y_data.sum()))","65f76471":"# Instantiate and fit nearmiss object\nnm = NearMiss()\n\n# We will undersample the dev dataset, as validation must be kept as originally generated for comparison purposes\nX_dev_us, y_dev_us = nm.fit_resample(X_dev, y_dev)\nX_dev_us = pd.DataFrame(X_dev_us, columns=X_dev.columns)\n\nprint(\"New dataset length = {}\".format(len(y_dev_us)))\nprint(\"New dataset class balance = {}\".format(y_dev_us.sum()\/len(y_dev_us)))","12bbff77":"X_train_us, X_test_us, y_train_us, y_test_us = train_test_split(X_dev_us, y_dev_us, \n                                                    train_size=300\/396, \n                                                    random_state=RANDOM_STATE,\n                                                    stratify=y_dev_us)","07130760":"# Let's check the target distribution\ndisplay('train dataset: {}'.format(y_train_us.sum()\/len(y_train_us)))\ndisplay('test dataset: {}'.format(y_test_us.sum()\/len(y_test_us)))","1970e00c":"print(\"Dev length: {}\".format(len(X_dev_us)))\nprint(\"Train length: {}\".format(len(X_train_us)))\nprint(\"Test length: {}\".format(len(X_test_us)))","10684b07":"# Let's check the number of positive class entries that exist and the corresponding length of the oversampling\nprint(\"Number of positive classes = {}\".format(len(y_dev[y_dev==0])))","4c218aa4":"smoter = SMOTE(random_state=RANDOM_STATE)\n\n# We oversample the dev dataset \nX_dev_os, y_dev_os = smoter.fit_resample(X_dev, y_dev)\nX_dev_os = pd.DataFrame(X_dev_os, columns=X_dev.columns)\nX_dev_os_fcols = pd.DataFrame(X_dev_os, columns=[\"f\" + str(x) for x in range(0, len(X_dev.columns))])\n\n# We oversample the train dataset\nX_train_os, y_train_os = smoter.fit_resample(X_train, y_train)\nX_train_os = pd.DataFrame(X_train_os, columns=X_train.columns)\nX_train_os_fcols = pd.DataFrame(X_train_os, columns=[\"f\" + str(x) for x in range(0, len(X_train.columns))])\n\n# We change the column names to dev dataset\nX_dev_fcols = X_dev.copy()\nX_dev_fcols.columns = [\"f\" + str(x) for x in range(0, len(X_dev.columns))]\n\n# We change the column names to test dataset\nX_test_fcols = X_test.copy()\nX_test_fcols.columns = [\"f\" + str(x) for x in range(0, len(X_test.columns))]\n\n# We change the column names to dev dataset\nX_val_fcols = X_val.copy()\nX_val_fcols.columns = [\"f\" + str(x) for x in range(0, len(X_val.columns))]","2e89bc9c":"print(\"Length of oversampled training set = {}\".format(len(X_train_os)))\nprint(\"Balance of oversampled training set Class = {}\".format(y_train_os.sum()\/len(y_train_os)))","7ca583a2":"print(\"Dev length: {}\".format(len(X_dev_os)))\nprint(\"Train length: {}\".format(len(X_train_os)))","81d90255":"# We will loop on different max_depths to get the optimal result\nfor i in range(3, 7):\n    dt = DecisionTreeClassifier(max_depth=i, random_state=RANDOM_STATE)\n    dt.fit(X_train, y_train)\n    \n    y_train_prediction = dt.predict(X_train)\n    y_test_prediction = dt.predict(X_test)\n    \n    train_accuracy = accuracy_score(y_train, y_train_prediction)\n    test_accuracy = accuracy_score(y_test, y_test_prediction)\n    print(\"Tree depth: {}. AUC --> Train: {} - Test: {}\".format(i, round(train_accuracy,5), round(test_accuracy,5)))\n    train_f1 = f1_score(y_train, y_train_prediction)\n    test_f1 = f1_score(y_test, y_test_prediction)\n    print(\"               F1  --> Train: {} - Test: {}\".format(round(train_f1,5), round(test_f1,5)))\n    print(\"\")","59ce0a7e":"dt = DecisionTreeClassifier(max_depth=3, random_state=RANDOM_STATE)\ndt.fit(X_dev, y_dev)","9e21a092":"model = dt","87d010b6":"# REPORT\nreport.init_report(model, X_dev, y_dev, X_val, y_val)\nreport.show_report()","a96082c6":"report.store_report(\n  key=\"dt_imbalanced\", \n  name=\"Decision Tree - Imbalanced Baseline\"\n)\nreport.compare_models()","5fe2269f":"# Set the kfold strategy\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE) # we will keep 5 splits in order to keep a significant number of frauds in the positive class\n\n# Instanciate the model\nxgb = XGBClassifier()\n\n# Set parameters for GridSearchCV. We will only provide different options for the max_depth\nparameters = {\n  'objective':['binary:logistic'],\n  'learning_rate': [0.05],\n  'max_depth': list(range(3, 7)),\n  #'min_child_weight': [11],\n  #'silent': [1],\n  #'subsample': [0.7],\n  #'colsample_bytree': [0.7],\n  'n_estimators': [10], #number of trees\n  #'missing':[-999], # we have no missings in the db\n  'seed': [RANDOM_STATE]\n}\n\n# Instantiate Grid Search\ngscv = GridSearchCV(\n    xgb, \n    parameters, \n    cv = kfold, \n    scoring = ['accuracy','precision','recall','roc_auc','f1'], \n    return_train_score = True, # we want to see the difference between train and test \n    verbose = 1, \n    refit = 'accuracy' \n)\n\ngscv.fit(X_dev, y_dev)","b9665937":"report.show_cv_results(gscv.cv_results_, \"accuracy\")","a69c9c61":"report.get_model_params(gscv.cv_results_, 1)","c48ee5a4":"# Let's check all the scores before proceeding with the best estimator\nprint(\"mean_train_accuracy {}\".format(gscv.cv_results_[\"mean_train_accuracy\"]))\nprint(\"mean_test_accuracy {}\".format(gscv.cv_results_[\"mean_test_accuracy\"]))\nprint(\"mean_train_roc_auc {}\".format(gscv.cv_results_[\"mean_train_roc_auc\"]))\nprint(\"mean_test_roc_auc {}\".format(gscv.cv_results_[\"mean_test_roc_auc\"]))\nprint('Accuracy best score: {}'.format(gscv.best_score_))\n# Let's check the score for the validation partition\nprint(\"Score {}\".format(gscv.score(X_val, y_val)))\n# Let's paint the ROC curve\nprint(plot_roc_curve(gscv.best_estimator_, X_dev, y_dev))","8455d507":"# SAVE best model\nmodel = gscv.best_estimator_","50ab5084":"# REPORT\nreport.init_report(model, X_dev, y_dev, X_val, y_val)\nreport.show_report()","b37eb226":"report.store_report(\n  key=\"xgb_imbalanced_gridsearchcv\", \n  name=\"XGBClassifier - Imbalanced with GridSearchCV\"\n)\nreport.compare_models()","08a50b5d":"# instantiating and training the base\nisolfor = IsolationForestWrapper(\n    n_estimators = 100, \n    max_samples = 0.5, # as float, indicates the % of the training dataset to use when training each particular tree (n_estimators)\n    max_features = 0.7, # as float, indicates the % of the training dataset features to use when training each particular tree (n_estimators)\n    contamination = 0.0017, # the weight of the positive class in our dataset\n    bootstrap = False, # no replacement in datasets\n    verbose = 2, \n    random_state = RANDOM_STATE\n)\nisolfor.fit(X_dev)","f30adad6":"# let's see the results of the base model, from there, we will make some variations\nreport.init_report(isolfor, X_dev, y_dev, X_val, y_val)\nreport.show_report()","0b44f60b":"# We will use validation_curve to modify test some of the parameters\nparam_name = \"n_estimators\"\nparam_range = [50, 100, 250]\ntrain_scores, test_scores = validation_curve(\n    IsolationForestWrapper(\n        contamination = 0.0017, \n    ), \n    X_dev, \n    y_dev, \n    param_name = param_name, \n    param_range = param_range,\n    scoring=\"recall\", \n    cv = 5, \n    n_jobs=1, \n    verbose = 2, \n)","7ecca1f9":"# We finally print boxplots for all the models\n# First, prepare the data\ndf_results = pd.DataFrame(columns=[\"model_variation\",\"type\",\"score\"])\nfor i in range(0, len(train_scores)):\n    s = pd.Series(train_scores[i], name=\"score\")\n    df_ = pd.DataFrame(s)\n    df_[\"model_variation\"] = i\n    df_[\"type\"] = \"train\"\n    df_results = df_results.append(df_, ignore_index=True)\n    \n    s = pd.Series(test_scores[i], name=\"score\")\n    df_ = pd.DataFrame(s)\n    df_[\"model_variation\"] = i\n    df_[\"type\"] = \"test\"\n    df_results = df_results.append(df_, ignore_index=True)\n\n# plot\nfig, axes = plt.subplots(1, 1, figsize=(7*len(train_scores),8), sharex = True, sharey = True)\nfig.tight_layout(pad=2.0)\nsns.boxplot(\n    data = df_results, \n    x = \"model_variation\", \n    y = \"score\",\n    hue = \"type\", \n    ax = axes\n)\nplt.show()\nprint(train_scores)\nprint(test_scores)","6278cb66":"# We will use validation_curve to modify test some of the parameters\nparam_name = \"max_samples\"\nparam_range = [0.6, 0.8, 1.0] # 50% of the dataset and above\ntrain_scores, test_scores = validation_curve(\n    IsolationForestWrapper(\n        contamination = 0.0017, \n    ), \n    X_dev, \n    y_dev, \n    param_name = param_name, \n    param_range = param_range,\n    scoring=\"recall\", \n    cv = 5, \n    n_jobs=1, \n    verbose = 2, \n)","9ea69a05":"# We finally print boxplots for all the models\n# First, prepare the data\ndf_results = pd.DataFrame(columns=[\"model_variation\",\"type\",\"score\"])\nfor i in range(0, len(train_scores)):\n    s = pd.Series(train_scores[i], name=\"score\")\n    df_ = pd.DataFrame(s)\n    df_[\"model_variation\"] = i\n    df_[\"type\"] = \"train\"\n    df_results = df_results.append(df_, ignore_index=True)\n    \n    s = pd.Series(test_scores[i], name=\"score\")\n    df_ = pd.DataFrame(s)\n    df_[\"model_variation\"] = i\n    df_[\"type\"] = \"test\"\n    df_results = df_results.append(df_, ignore_index=True)\n\n# plot\nfig, axes = plt.subplots(1, 1, figsize=(7*len(train_scores),8), sharex = True, sharey = True)\nfig.tight_layout(pad=2.0)\nsns.boxplot(\n    data = df_results, \n    x = \"model_variation\", \n    y = \"score\",\n    hue = \"type\", \n    ax = axes\n)\nplt.show()\nprint(train_scores)\nprint(test_scores)","5cb88700":"# We now work on the max_features variable\nparam_name = \"max_features\"\nparam_range = [0.6, 0.8, 1.0] # 50% of the features and above\ntrain_scores, test_scores = validation_curve(\n    IsolationForestWrapper(\n        contamination = 0.0017, \n    ),\n    X_dev, \n    y_dev, \n    param_name = param_name, \n    param_range = param_range,\n    scoring=\"recall\", \n    cv = 5, \n    n_jobs=1, \n    verbose = 2, \n)","a26a5116":"# We finally print boxplots for all the models\n# First, prepare the data\ndf_results = pd.DataFrame(columns=[\"model_variation\",\"type\",\"score\"])\nfor i in range(0, len(train_scores)):\n    s = pd.Series(train_scores[i], name=\"score\")\n    df_ = pd.DataFrame(s)\n    df_[\"model_variation\"] = i\n    df_[\"type\"] = \"train\"\n    df_results = df_results.append(df_, ignore_index=True)\n    \n    s = pd.Series(test_scores[i], name=\"score\")\n    df_ = pd.DataFrame(s)\n    df_[\"model_variation\"] = i\n    df_[\"type\"] = \"test\"\n    df_results = df_results.append(df_, ignore_index=True)\n\n# plot\nfig, axes = plt.subplots(1, 1, figsize=(7*len(train_scores),8), sharex = True, sharey = True)\nfig.tight_layout(pad=2.0)\nsns.boxplot(\n    data = df_results, \n    x = \"model_variation\", \n    y = \"score\",\n    hue = \"type\", \n    ax = axes\n)\nplt.show()\nprint(train_scores)\nprint(test_scores)","b9bc712c":"report.store_report(\n  key=\"isolation_imbalanced_validationcurve\", \n  name=\"IsolationForest - Imbalanced with ValidationCurve\"\n)\nreport.compare_models()","e27b404a":"# We have to work with the dev dataset. When oversampling, we should be carefull, \n# as we ONLY want to oversample the TRAINING data, not the TEST.\n# We will use a specific pipeline (imblearn) to handle this situation\n\nlogreg = LogisticRegression()\n\nparameters = {\n    'penalty' : ['l1','l2'], # l1 = Lasso is effective when having non relevant independent variables. l2 = Ridge is effective when there is correlation between the independent variables.\n    'max_iter' : [100, 500, 1000], \n    'solver' : ['liblinear'], # better for small datasets, but it can be used for both l1 and l2 penalty\n  }\n\n# Instantiate Grid Search\ngscv = GridSearchCV(\n    logreg, \n    param_grid = parameters, \n    scoring = ['accuracy','roc_auc'], \n    cv = 5, # default value\n    return_train_score = True, # we want to see the difference between train and test\n    verbose = 2, \n    refit = 'accuracy' # yes, using accuracy as the best estimator metric\n)\n\ngscv.fit(X_dev, y_dev) # we include dev, as the GridSearchCV will perform k-fold strategy","1bba27ab":"report.show_cv_results(gscv.cv_results_, \"accuracy\")","2b1e033f":"# Let's check all the scores before proceeding with the best estimator\nprint(\"mean_train_accuracy {}\".format(gscv.cv_results_[\"mean_train_accuracy\"]))\nprint(\"mean_test_accuracy {}\".format(gscv.cv_results_[\"mean_test_accuracy\"]))\nprint(\"mean_train_roc_auc {}\".format(gscv.cv_results_[\"mean_train_roc_auc\"]))\nprint(\"mean_test_roc_auc {}\".format(gscv.cv_results_[\"mean_test_roc_auc\"]))\nprint('Accuracy best score: {}'.format(gscv.best_score_))\n# Let's paint the ROC curve\nprint(plot_roc_curve(gscv.best_estimator_, X_dev, y_dev))","2ca62982":"gscv.best_params_","9b68623b":"# SAVE best model\nmodel = gscv.best_estimator_","26679d10":"# REPORT\nreport.init_report(model, X_dev, y_dev, X_val, y_val)\nreport.show_report()  ","596ad8a0":"report.store_report(\n  key=\"logreg_imbalanced_gridsearchcv\", \n  name=\"Logistic Regression - Imbalanced with GridSearchCV\"\n)\nreport.compare_models()","82d5070a":"knn_faiss = FaissKNeighbors()\nknn_faiss.fit(np.ascontiguousarray(X_dev.values), y_dev)","044a6b62":"model = knn_faiss","d2f32ee4":"report.init_report(model, X_dev, y_dev, X_val, y_val)\nreport.show_report()","3ea70acf":"report.store_report(\n  key=\"knnfaiss_imbalanced_k5\", \n  name=\"Knn Faiss - Imbalanced\"\n)\nreport.compare_models()","76503715":"svc = LinearSVC()\n\nparameters = {\n    'penalty' : ['l2'], \n    'C' : [0.1, 1], # [0.1, 1, 2, 5, 10, 25, 50, 100], \n    'random_state' : [RANDOM_STATE], \n    'max_iter' : [100, 1000]\n  }\n\n# Instantiate Grid Search\ngscv = GridSearchCV(\n    svc, \n    param_grid = parameters, \n    scoring = ['accuracy','roc_auc'], \n    cv = 5, # default value, c\n    return_train_score = True, # we want to see the difference between train and test\n    verbose = 2, \n    refit = 'accuracy' # yes, using accuracy as the best estimator metric\n)\n\ngscv.fit(X_dev, y_dev) # we include dev, as the GridSearchCV will perform k-fold strategy","b268b557":"report.show_cv_results(gscv.cv_results_, \"accuracy\")","cbebf864":"# Let's check all the scores before proceeding with the best estimator\nprint(\"mean_train_accuracy {}\".format(gscv.cv_results_[\"mean_train_accuracy\"]))\nprint(\"mean_test_accuracy {}\".format(gscv.cv_results_[\"mean_test_accuracy\"]))\nprint(\"mean_train_roc_auc {}\".format(gscv.cv_results_[\"mean_train_roc_auc\"]))\nprint(\"mean_test_roc_auc {}\".format(gscv.cv_results_[\"mean_test_roc_auc\"]))\nprint('Accuracy best score: {}'.format(gscv.best_score_))\n# Let's paint the ROC curve\nprint(plot_roc_curve(gscv.best_estimator_, X_dev, y_dev))\nplt.show()","adb92284":"gscv.best_params_","47b137c3":"# RETRAIN THE MODEL and get \nsvc = LinearSVC(\n    penalty = \"l2\", \n    C = 1,\n    max_iter = 100, \n    random_state = RANDOM_STATE, \n)\nsvc.fit(X_dev, y_dev)","96297add":"# We use CalibratedClassifier to get prediction probabilites, as LinearSVC does not natively include them\nclf = CalibratedClassifierCV(\n  svc, \n  cv = \"prefit\",  # the model is already fit, no need to do CV\n)\nclf.fit(X_val, y_val)","03d56b77":"model = clf","a832210a":"report.init_report(clf, X_dev, y_dev, X_val, y_val)\nreport.show_report()","ccc359c1":"report.store_report(\n  key=\"svc_imbalanced_gridsearchcv\", \n  name=\"SVC - Imbalanced with GridSearchCV\"\n)\nreport.compare_models()","d67bb540":"# We have to work with the dev dataset. When oversampling, we should be carefull, \n# as we ONLY want to oversample the DEV\/TRAINING data, not the TEST neither VALIDATION.\n# We will use a specific pipeline (imblearn) to handle this situation\n\nxgb = Pipeline([\n        ('sampling', SMOTE()),\n        ('classification', XGBClassifier())\n])\n\nparameters = {\n    'classification__objective':['binary:logistic'],\n    'classification__learning_rate': [0.05],\n    'classification__max_depth': list(range(3, 7)),\n    #'min_child_weight': [11],\n    #'silent': [1],\n    #'subsample': [0.7],\n    #'colsample_bytree': [0.7],\n    'classification__n_estimators': [10], #number of trees\n    #'missing':[-999], # we have no missings in the db\n    'classification__seed': [RANDOM_STATE]\n  }\n# Instantiate Grid Search\ngscv = GridSearchCV(\n    xgb, \n    param_grid = parameters, \n    cv = 5, # default is 5\n    scoring = ['accuracy','precision','recall','roc_auc','f1'], \n    return_train_score = True, # we want to see the difference between train and test\n    verbose = 2, \n    refit = 'accuracy' # although we will refit manually\n)\n\ngscv.fit(X_dev_fcols, y_dev)","89963607":"report.show_cv_results(gscv.cv_results_, \"accuracy\")","01c88a3b":"max_depth = gscv.best_params_[\"classification__max_depth\"]\nprint(\"Optimal max_depth: {}\".format(max_depth))","e298b85d":"# Let's check all the scores before proceeding with the best estimator\nprint(\"mean_train_accuracy {}\".format(gscv.cv_results_[\"mean_train_accuracy\"]))\nprint(\"mean_test_accuracy {}\".format(gscv.cv_results_[\"mean_test_accuracy\"]))\nprint(\"mean_train_roc_auc {}\".format(gscv.cv_results_[\"mean_train_roc_auc\"]))\nprint(\"mean_test_roc_auc {}\".format(gscv.cv_results_[\"mean_test_roc_auc\"]))\nprint('AUC best score: {}'.format(gscv.best_score_))","6357116f":"# SAVE best model\nmodel = XGBClassifier(\n    objective = \"binary:logistic\",\n    learning_rate = 0.05,\n    max_depth = max_depth,\n    n_estimators = 10, # number of trees\n    seed = RANDOM_STATE, \n)","a116707a":"# We fit the model with the best parameters. We do it on the dev data (oversampled)\nmodel.fit(X_dev_os, y_dev_os)","2b9ac725":"report.init_report(model, X_dev, y_dev, X_val, y_val)\nreport.show_report()","fbe2c9c9":"report.store_report(\n  key=\"xgb_oversamplig_gridsearchcv\",\n  name=\"XGBClassifier - Oversampling with GridSearchCV\"\n)\nreport.compare_models()","946c3143":"# METRICS - We will store all the metrics in a dataframe\ndf_results_ = pd.DataFrame(columns=[\"depth\",\"iter\",\"error_train\",\"error_test\",\"accuracy_train\",\"accuracy_test\"]) # when iter = Nan, that means that we are averaging all of them\n\n# DEPTH LOOP - As we will not be using GridSearchCV, we will iterate different depths to select the best model\nmin_depth = 4\nmax_depth = 7\nfor idepth in range(min_depth, max_depth+1):\n\n  # Initialize xgbclassifier\n  xgboost = XGBClassifier(max_depth=idepth, n_estimators=10, seed=RANDOM_STATE)\n\n  # We create the kfold splitter. We will always shuffle the results. We use Stratified to ensure the ratio of positive class between splits\n  kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE) # we will keep 5 splits in order to keep a significant number of frauds in the positive class\n\n  # We now loop through the kfolds\n  i = 0\n  for train_indexes_, test_indexes_ in kfold.split(X_dev, y_dev): # split generates two arrays of indexes, one for train and the other for test\n    print(\"Depth: {} - KFold: {}\".format(idepth, i+1))\n    # We now generate the datasets from the indexes\n    X_train_, y_train_ = X_dev.iloc[train_indexes_], y_dev.iloc[train_indexes_]\n    X_test_, y_test_ = X_dev.iloc[test_indexes_], y_dev.iloc[test_indexes_]\n    \n    # We need to oversample ONLY the training set using SMOTE\n    smoter = SMOTE(random_state=RANDOM_STATE)\n    X_train_, y_train_ = smoter.fit_resample(X_train_, y_train_)\n    X_train_, y_train_ = pd.DataFrame(X_train_, columns=X_dev.columns), pd.Series(y_train_)\n    \n    # We train the model and test at the same time\n    xgboost.fit(X_train_, y_train_, eval_set=[(X_train_, y_train_), (X_test_, y_test_)], eval_metric=\"error\", verbose=False)\n\n    # And finally we store the results for this loop and calculate the average\n    evals_result = xgboost.evals_result()\n    result_ = [idepth, i+1, np.mean(evals_result[\"validation_0\"][\"error\"]), np.mean(evals_result[\"validation_1\"][\"error\"]), 1-np.mean(evals_result[\"validation_0\"][\"error\"]), 1-np.mean(evals_result[\"validation_1\"][\"error\"])]\n    df_results_ = df_results_.append(pd.Series(result_, index = df_results_.columns), ignore_index=True)\n    i += 1\n\n  depth_average_ = df_results_[df_results_[\"depth\"]==idepth].groupby(by=[\"depth\"], as_index=False).mean()\n  depth_average_.loc[0,\"iter\"] = np.nan\n  df_results_ = df_results_.append(depth_average_, ignore_index=True)\n  \ndisplay(df_results_)\n\n","5869dd83":"fig, axs = plt.subplots(max_depth - min_depth + 1, sharey=True, sharex=True, figsize=(15,25))\naxs[0].set(ylim=(0.94, 1.0))\nplt.xticks(ticks=range(1,6))\ni = 0\nfor idepth in range(min_depth, max_depth+1):\n  axs[i].set_title(\"Depth: \" + str(idepth))\n  sns.lineplot(\n      data=df_results_[(df_results_[\"depth\"]==idepth) & (~df_results_[\"iter\"].isnull())], \n      x=\"iter\", \n      y=\"accuracy_train\",\n      color=\"b\",\n      ax = axs[i]\n  )\n\n  sns.lineplot(\n      data=df_results_[(df_results_[\"depth\"]==idepth) & (~df_results_[\"iter\"].isnull())], \n      x=\"iter\", \n      y=\"accuracy_test\",\n      color=\"g\",\n      ax = axs[i]\n  )\n\n\n  # Average for train\n  acc_ = df_results_[(df_results_[\"depth\"]==idepth) & (df_results_[\"iter\"].isnull())][\"accuracy_train\"].iloc[0]\n  axs[i].axhline(acc_, ls='--', color='b')\n  # Average for test\n  acc_ = df_results_[(df_results_[\"depth\"]==idepth) & (df_results_[\"iter\"].isnull())][\"accuracy_test\"].iloc[0]\n  axs[i].axhline(acc_, ls='-.', color='g')\n\n  i+=1","2f579b52":"# Initialize xgbclassifier\nxgb = XGBClassifier(\n  max_depth=7, \n  n_estimators=10, \n  seed=RANDOM_STATE\n)\n  \n# We train the model and validate at the same time\n# eval_metric = error = #(wrong cases)\/#(all cases) (1-accuracy)\n# The training dataset is the oversampled one.\nxgb.fit(X_dev_os, y_dev_os, eval_set=[(X_dev_os, y_dev_os), (X_val, y_val)], eval_metric=\"error\", verbose=False)\n\n# And finally we store the results for this loop and calculate the average\nevals_result = xgb.evals_result()\nevals_result","aa64793e":"# SAVE best model\nmodel = xgb","7eead4b0":"report.init_report(model, X_dev_os, y_dev_os, X_val, y_val)\nreport.show_report()","8213f69c":"report.store_report(\n  key=\"xgb_oversamplig_kfold\",\n  name=\"XGBClassifier - Oversampling with Kfold\"\n)\nreport.compare_models()","0c4b9647":"# instantiating and training the base\nisolfor = IsolationForestWrapper(\n    n_estimators = 100, \n    max_samples = 0.5, # as float, indicates the % of the training dataset to use when training each particular tree (n_estimators)\n    max_features = 0.7, # as float, indicates the % of the training dataset features to use when training each particular tree (n_estimators)\n    contamination = 0.5, # the weight of the positive class in our dataset\n    bootstrap = False, # no replacement in datasets\n    verbose = 2, \n    random_state = RANDOM_STATE\n)\nisolfor.fit(X_dev_os)","190437f8":"report.init_report(isolfor, X_dev_os, y_dev_os, X_val, y_val)\nreport.show_report()","690afea7":"report.store_report(\n  key=\"isolation_oversampling\", \n  name=\"IsolationForest - Oversampling\"\n)\nreport.compare_models()","925fe4a5":"# We have to work with the dev dataset. When oversampling, we should be carefull, \n# as we ONLY want to oversample the TRAINING data, not the TEST.\n# We will use a specific pipeline (imblearn) to handle this situation\n\nlogreg = Pipeline([\n        ('sampling', SMOTE()),\n        ('classification', LogisticRegression())\n])\n\nparameters = {\n    'classification__penalty' : ['l1','l2'],  # l1 = Lasso is effective when having non relevant independent variables. l2 = Ridge is effective when there is correlation between the independent variables.\n    'classification__max_iter' : [100, 500, 1000], \n    'classification__solver' : ['liblinear'], \n    'classification__random_state': [RANDOM_STATE],\n}\n# Instantiate Grid Search\ngscv = GridSearchCV(\n    logreg, \n    param_grid = parameters, \n    scoring = ['accuracy','roc_auc'], \n    return_train_score = True, # we want to see the difference between train and test\n    verbose = 2, \n    refit = 'accuracy' \n)\n\ngscv.fit(X_dev_fcols, y_dev)","f594d5c4":"report.show_cv_results(gscv.cv_results_, \"accuracy\")","3e50c4bd":"# SAVE best model\nparams = gscv.best_params_\nparams","c92f79dd":"# Instantiate model\nlogreg = LogisticRegression(\n    penalty = params[\"classification__penalty\"],\n    random_state = RANDOM_STATE, \n    max_iter = params[\"classification__max_iter\"],  \n    solver = 'liblinear', \n)\nlogreg.fit(X_dev_os, y_dev_os)","b7a90278":"model = logreg","373091ba":"report.init_report(model, X_dev_os, y_dev_os, X_val, y_val)\nreport.show_report()","b3cf8cb3":"report.store_report(\n  key=\"logreg_oversamplig_gridsearchcv\",\n  name=\"Logistic Regression - Oversampling with GridSearch\"\n)\nreport.compare_models()","9fcd3227":"# We have to work with the dev dataset. When oversampling, we should be carefull, \n# as we ONLY want to oversample the DEV\/TRAINING data, not the TEST neither VALIDATION.\n# We will use a specific pipeline (imblearn) to handle this situation\n\nxgb = XGBClassifier(\n)\n\nparameters = {\n    'objective':['binary:logistic'],\n    'learning_rate': [0.05],\n    'max_depth': list(range(4, 7)),\n    'n_estimators': [10], #number of trees\n    'seed': [RANDOM_STATE]\n  }\n# Instantiate Grid Search\ngscv = GridSearchCV(\n    xgb, \n    param_grid = parameters, \n    cv = 5, # default is 5\n    scoring = ['accuracy','precision','recall','roc_auc','f1'], \n    return_train_score = True, # we want to see the difference between train and test\n    verbose = 2, \n    refit = 'accuracy' # although we will refit manually\n)\n\ngscv.fit(X_dev_us, y_dev_us)","af6baa92":"report.show_cv_results(gscv.cv_results_, \"accuracy\")","e53927c6":"max_depth = gscv.best_params_[\"max_depth\"]\nprint(\"Optimal max_depth: {}\".format(max_depth))","ada74b07":"# Let's check all the scores before proceeding with the best estimator\nprint(\"mean_train_accuracy {}\".format(gscv.cv_results_[\"mean_train_accuracy\"]))\nprint(\"mean_test_accuracy {}\".format(gscv.cv_results_[\"mean_test_accuracy\"]))\nprint(\"mean_train_roc_auc {}\".format(gscv.cv_results_[\"mean_train_roc_auc\"]))\nprint(\"mean_test_roc_auc {}\".format(gscv.cv_results_[\"mean_test_roc_auc\"]))\nprint('AUC best score: {}'.format(gscv.best_score_))","dcf6b8b2":"# SAVE best model\nmodel = XGBClassifier(\n    objective = \"binary:logistic\",\n    learning_rate = 0.05,\n    max_depth = max_depth,\n    n_estimators = 10, # number of trees\n    seed = RANDOM_STATE, \n)","545fbc77":"# We fit the model with the best parameters. We do it on the dev data (oversampled)\nmodel.fit(X_dev_us, y_dev_us)","676622df":"report.init_report(model, X_dev_us, y_dev_us, X_val, y_val)\nreport.show_report()","c9d45310":"report.store_report(\n  key=\"xgb_undersamplig_gridsearchcv\",\n  name=\"XGBClassifier - Undersampling with GridSearchCV\"\n)\nreport.compare_models()","6dec0ca3":"# instantiating and training the base\nisolfor = IsolationForestWrapper(\n    n_estimators = 100, \n    max_samples = 0.5, # as float, indicates the % of the training dataset to use when training each particular tree (n_estimators)\n    max_features = 0.7, # as float, indicates the % of the training dataset features to use when training each particular tree (n_estimators)\n    contamination = 0.5, # the weight of the positive class in our dataset\n    bootstrap = False, # no replacement in datasets\n    verbose = 2, \n    random_state = RANDOM_STATE\n)\nisolfor.fit(X_dev_os)","b81c91a4":"report.init_report(isolfor, X_dev_us, y_dev_us, X_val, y_val)\nreport.show_report()","502b22ef":"report.store_report(\n  key=\"isolation_undersampling\", \n  name=\"IsolationForest - Undersampling\"\n)\nreport.compare_models()","33fec557":"# We have to work with the dev dataset. When oversampling, we should be carefull, \n# as we ONLY want to oversample the TRAINING data, not the TEST.\n# We will use a specific pipeline (imblearn) to handle this situation\n\nlogreg = LogisticRegression()\n\nparameters = {\n    'penalty' : ['l1','l2'],  # l1 = Lasso is effective when having non relevant independent variables. l2 = Ridge is effective when there is correlation between the independent variables.\n    'max_iter' : [100, 500, 1000], \n    'solver' : ['liblinear'], \n    'random_state': [RANDOM_STATE],\n}\n# Instantiate Grid Search\ngscv = GridSearchCV(\n    logreg, \n    param_grid = parameters, \n    scoring = ['accuracy','roc_auc'], \n    return_train_score = True, # we want to see the difference between train and test\n    verbose = 2, \n    refit = 'accuracy' \n)\n\ngscv.fit(X_dev_us, y_dev_us)","d481adef":"report.show_cv_results(gscv.cv_results_, \"accuracy\")","a68e1c12":"# SAVE best model\nparams = gscv.best_params_\nparams","0c0bfabb":"# Instantiate model\nlogreg = LogisticRegression(\n    penalty = params[\"penalty\"],\n    random_state = RANDOM_STATE, \n    max_iter = params[\"max_iter\"],  \n    solver = 'liblinear', \n)\nlogreg.fit(X_dev_us, y_dev_us)","9c345275":"model = logreg","f1c7f737":"report.init_report(model, X_dev_us, y_dev_us, X_val, y_val)\nreport.show_report()","b3824473":"report.store_report(\n  key=\"logreg_undersamplig_gridsearchcv\",\n  name=\"Logistic Regression - Undersampling with GridSearch\"\n)\nreport.compare_models()","3d57810d":"# The metrics will always be the same (except for autoencoder)\nmetrics = [\n    keras.metrics.Accuracy(name=\"accuracy\"),\n    keras.metrics.Precision(name=\"precision\"),\n    keras.metrics.Recall(name=\"recall\"),\n    keras.metrics.AUC(name=\"auc\"),\n    # if needed, we will calculate f1 from recall and precision\n]","85a34c96":"# Function to easily repeat the training and evaluation process for each potential model\ndef compile_train_test_nn(NNKeras_model, X_train, y_train, validation_data=None, epochs=30, batch_size=256, learning_rate=0.01, class_weight={0:1,1:1}, metrics=[\"accuracy\"], loss=\"binary_crossentropy\", verbose=0, callbacks=None):\n    # Compile the model\n    NNKeras_model.compile(\n        optimizer = keras.optimizers.Adam(learning_rate), # Adam tends to be faster for training and more flexible in terms of learning rate\n        loss = loss, \n        metrics = metrics \n    )\n\n    # Train the model\n    t = time.perf_counter()\n\n    his = NNKeras_model.fit(\n        X_train, \n        y_train,\n        epochs = epochs,\n        batch_size = batch_size, \n        validation_data = validation_data, \n        class_weight = class_weight, \n        callbacks = callbacks, \n        verbose = verbose,\n    )\n\n    elapsed_time = datetime.timedelta(seconds=(time.perf_counter() - t))\n    print('Training time:', elapsed_time)\n    \n    # show results\n    report.show_nnhistory_results(his, [\"accuracy\", \"recall\", \"precision\"])\n    \n    return NNKeras_model","be7b7d5e":"# Corrected class weight for the original dataset\nclass_weight_IMBL = {0: 1\/(len(y_data[y_data==0])\/len(y_data)), 1: 1\/(len(y_data[y_data==1])\/len(y_data))} # we give more weight to the positive class, as this dataset is clearly imbalanced. We use the inverse of natural weight.\n\n# Balanced class weight for over and undersampling\nclass_weight_BL = {0: 1, 1: 1} # same weight","1e7c7dc9":"## POTENTIAL MODEL 1.1\n# instantiate and create model\nNNKeras_1_1 = keras.Sequential(\n    [\n        keras.layers.Dense(\n            256, activation=\"relu\", input_shape=(len(X_train.columns),)\n        ),\n        keras.layers.Dense(256, activation=\"relu\"),\n        keras.layers.Dropout(0.3),\n        keras.layers.Dense(256, activation=\"relu\"),\n        keras.layers.Dropout(0.3),\n        keras.layers.Dense(1, activation=\"sigmoid\"), # binary output, we only need one neuron\n    ]\n)\n\nprint(\"Model 1.1 - original - corrected weight\")\nNNKeras_1_1 = compile_train_test_nn(\n    NNKeras_1_1, \n    X_train, y_train, validation_data = (X_test, y_test), \n    epochs = 40, \n    batch_size = pow(2, 16), # we do not want our entropy loss results to be waggy\n    metrics = metrics, \n    learning_rate = 0.001, \n    class_weight = class_weight_IMBL, \n)","14c6ce2c":"report.init_report(NNKeras_1_1, X_train, y_train, X_val, y_val)\nreport.show_report()","61bb9973":"## POTENTIAL MODEL 1.2 - same weight\n# instantiate and create model\nNNKeras_1_2 = keras.Sequential(\n    [\n        keras.layers.Dense(\n            256, activation=\"relu\", input_shape=(len(X_train.columns),)\n        ),\n        keras.layers.Dense(256, activation=\"relu\"),\n        keras.layers.Dropout(0.3),\n        keras.layers.Dense(256, activation=\"relu\"),\n        keras.layers.Dropout(0.3),\n        keras.layers.Dense(1, activation=\"sigmoid\"), # binary output, we only need one neuron\n    ]\n)\n\nprint(\"Model 1.2 - original - same weight\")\nNNKeras_1_2 = compile_train_test_nn(\n    NNKeras_1_2, \n    X_train, y_train, validation_data = (X_test, y_test), \n    epochs = 50, \n    batch_size = pow(2, 16), \n    learning_rate = 0.001, \n    class_weight = class_weight_BL, \n)","c62eb23b":"report.init_report(NNKeras_1_2, X_train, y_train, X_val, y_val)\nreport.show_report()","5ebe6ba9":"report.store_report(\n  key=\"kr_1_2_imbalanced_sameweight\", \n  name=\"Keras 1_2 - Imbalanced - Same Weight\"\n)\nreport.compare_models()","7b68f28f":"## POTENTIAL MODEL 2_1\n# Light architecture\n# instantiate and create model\nNNKeras_2_1 = Sequential([\n    keras.layers.Dense(X_train.shape[1], input_shape=(X_train.shape[1], ), activation='relu'),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dropout(0.3), \n    keras.layers.Dense(1, activation='sigmoid')\n])\nprint(\"Model 2.1 - original\")\nNNKeras_2_1 = compile_train_test_nn(\n    NNKeras_2_1, \n    X_train, y_train, validation_data = (X_test, y_test), \n    epochs = 80, \n    batch_size = pow(2, 16), \n    learning_rate = 0.001, \n    class_weight = class_weight_IMBL, \n)","a7784177":"report.init_report(NNKeras_2_1, X_train, y_train, X_val, y_val)\nreport.show_report()","33863f9e":"## POTENTIAL MODEL 2_2\n# Light architecture\n# instantiate and create model\nNNKeras_2_2 = Sequential([\n    keras.layers.Dense(X_train.shape[1], input_shape=(X_train.shape[1], ), activation='relu'),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dropout(0.3), \n    keras.layers.Dense(1, activation='sigmoid')\n])\nprint(\"Model 2.2 - original\")\nNNKeras_2_2 = compile_train_test_nn(\n    NNKeras_2_2, \n    X_train, y_train, validation_data = (X_test, y_test), \n    epochs = 80, \n    batch_size = pow(2, 16), \n    learning_rate = 0.001, \n    class_weight = class_weight_BL, \n)","d9252384":"report.init_report(NNKeras_2_2, X_train, y_train, X_val, y_val)\nreport.show_report()","e587e2ef":"report.store_report(\n  key=\"kr_2_2_imbalanced_sameweight\", \n  name=\"Keras 2_2 - Imbalanced - Same Weight\"\n)\nreport.compare_models()","7b86ac66":"## POTENTIAL MODEL 3\n# instantiate and create model\n# https:\/\/medium.com\/datadriveninvestor\/building-neural-network-using-keras-for-classification-3a3656c726c1\nNNKeras_3 = keras.Sequential(\n    [\n        keras.layers.Dense(4, activation=\"relu\", kernel_initializer = \"random_normal\", input_shape=(len(X_train_os.columns),)),\n        keras.layers.Dropout(0.5), \n        keras.layers.Dense(4, activation=\"relu\", kernel_initializer = \"random_normal\"),\n        keras.layers.Dropout(0.5), \n        keras.layers.Dense(1, activation=\"sigmoid\", kernel_initializer = \"random_normal\"), # binary output, we only need one neuron\n    ]\n)\n\nprint(\"Model 3 - oversampling\")\nNNKeras_3 = compile_train_test_nn(\n    NNKeras_3, \n    X_train_os, y_train_os, validation_data = (X_test, y_test), \n    epochs = 80, \n    batch_size = pow(2, 16), \n    learning_rate = 0.001, \n    class_weight = class_weight_BL, \n)","3992c5d7":"report.init_report(NNKeras_3, X_train_os, y_train_os, X_val, y_val)\nreport.show_report()","a9742579":"report.store_report(\n  key=\"kr_3_oversampling\", \n  name=\"Keras 3 - Oversampling\"\n)\nreport.compare_models()","50e5028b":"## POTENTIAL MODEL 4\n# instantiate and create model\nNNKeras_4 = Sequential([\n    keras.layers.Dense(X_train_os.shape[1], input_shape=(X_train_os.shape[1], ), activation='relu'),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dropout(0.3), \n    keras.layers.Dense(1, activation='sigmoid')\n])\nprint(\"Model 4 - oversampling\")\nNNKeras_4 = compile_train_test_nn(\n    NNKeras_4, \n    X_train_os, y_train_os, validation_data = (X_test, y_test), \n    epochs = 40, \n    batch_size = pow(2, 16), \n    learning_rate = 0.001, \n    class_weight = class_weight_BL, \n)","777f6bf3":"report.init_report(NNKeras_4, X_train_os, y_train_os, X_val, y_val)\nreport.show_report()","ee75a7ba":"report.store_report(\n  key=\"kr_4_oversampling\", \n  name=\"Keras 4 - Oversampling\"\n)\nreport.compare_models()","9f0ee863":"## POTENTIAL MODEL 5\n# instantiate and create model\n# https:\/\/medium.com\/datadriveninvestor\/building-neural-network-using-keras-for-classification-3a3656c726c1\nNNKeras_5 = keras.Sequential(\n    [\n        keras.layers.Dense(4, activation=\"relu\", kernel_initializer = \"random_normal\", input_shape=(len(X_train_us.columns),)),\n        keras.layers.Dropout(0.5), \n        keras.layers.Dense(4, activation=\"relu\", kernel_initializer = \"random_normal\"),\n        keras.layers.Dropout(0.5), \n        keras.layers.Dense(1, activation=\"sigmoid\", kernel_initializer = \"random_normal\"), # binary output, we only need one neuron\n    ]\n)\n\nprint(\"Model 5 - undersampling\")\nNNKeras_5 = compile_train_test_nn(\n    NNKeras_5, \n    X_train_us, y_train_us, validation_data = (X_test, y_test), \n    epochs = 30, \n    batch_size = pow(2, 16), \n    learning_rate = 0.001, \n    class_weight = class_weight_BL, \n)","e3ee5234":"report.init_report(NNKeras_5, X_train_us, y_train_us, X_val, y_val)\nreport.show_report()","3ef8d3c4":"report.store_report(\n  key=\"kr_5_undersampling\", \n  name=\"Keras 5 - Undersampling\"\n)\nreport.compare_models()","05bc6229":"## POTENTIAL MODEL 6\n# instantiate and create model\nNNKeras_6 = Sequential([\n    keras.layers.Dense(X_train_us.shape[1], input_shape=(X_train_us.shape[1], ), activation='relu'),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dropout(0.3), \n    keras.layers.Dense(1, activation='sigmoid')\n])\nprint(\"Model 6 - undersampling\")\nNNKeras_6 = compile_train_test_nn(\n    NNKeras_6, \n    X_train_us, y_train_us, validation_data = (X_test, y_test), \n    epochs = 40, \n    batch_size = pow(2, 16), \n    learning_rate = 0.001, \n    class_weight = class_weight_BL, \n)","dde6b08c":"report.init_report(NNKeras_6, X_train_us, y_train_us, X_val, y_val)\nreport.show_report()","9f2dfd25":"report.store_report(\n  key=\"kr_6_undersampling\", \n  name=\"Keras 6 - Undersampling\"\n)\nreport.compare_models()","27cabffe":"print(\"Model 1 - Autoencoding- Imbalanced\")\nNNKeras_Autoencoder_1 = Sequential([\n    # encoder\n    keras.layers.Dense(X_dev.shape[1], activation=\"tanh\", input_shape=(X_dev.shape[1], ), activity_regularizer=regularizers.l1(0.00001)), \n    keras.layers.Dense(int(X_dev.shape[1]\/2), activation=\"relu\"), \n    #decoder\n    keras.layers.Dense(int(X_dev.shape[1]\/2), activation=\"tanh\"), \n    keras.layers.Dense(X_dev.shape[1], activation=\"relu\")\n])\n\nNNKeras_Autoencoder_1 = compile_train_test_nn(\n    NNKeras_Autoencoder_1, \n    X_dev, y_dev, validation_data=(X_val, y_val), \n    epochs = 20, \n    batch_size = pow(2, 16), \n    learning_rate = 0.001, \n    loss = \"mean_squared_error\", \n    metrics = [\"accuracy\"], \n    class_weight = class_weight_BL, \n)","9abebb0e":"# With the fitted model, we now predict train and test registries\nX_dev_predict_ = pd.DataFrame([x for x in NNKeras_Autoencoder_1.predict(X_dev)], columns=X_dev.columns, index=X_dev.index)\nX_val_predict_ = pd.DataFrame([x for x in NNKeras_Autoencoder_1.predict(X_val)], columns=X_val.columns, index=X_val.index)\n\nX_dev_rmse = np.sqrt(pow(X_dev_predict_ - X_dev, 2).mean(axis=1))\nX_val_rmse = np.sqrt(pow(X_val_predict_ - X_val, 2).mean(axis=1))\ndel X_dev_predict_, X_val_predict_\n\n# Now, we can plot the errors against the classes to see if there is a difference. If we cannot \"reconstruct\" frauds, we will have a hyer rmse\ndf_dev = pd.DataFrame(X_dev_rmse, columns=[\"rmse\"])\ndf_val = pd.DataFrame(X_val_rmse, columns=[\"rmse\"])\ndf_dev[\"Class\"] = y_dev\ndf_val[\"Class\"] = y_val\n\nfig, axes = plt.subplots(1, 2, figsize=(2*7, 7), sharex=True, sharey=True)\nsns.boxenplot(data=df_dev, x=\"Class\", y=\"rmse\", ax=axes[0])\nsns.boxenplot(data=df_val, x=\"Class\", y=\"rmse\", ax=axes[1])\nplt.show()","6fe75f40":"rmse_thresholds = [2, 3, 4, 5]\nfor rmse_threshold in rmse_thresholds:\n    y_dev_pred = df_dev[\"rmse\"].apply(lambda x: 0 if x<rmse_threshold else 1)\n    y_val_pred = df_val[\"rmse\"].apply(lambda x: 0 if x<rmse_threshold else 1)\n    \n    cm_dev = confusion_matrix(y_dev.values, y_dev_pred.values)\n    cm_val = confusion_matrix(y_val.values, y_val_pred.values)\n\n    print(\"Threshold {}\".format(rmse_threshold))\n    print(\"------------------------------\")\n    print(\"DEV Accuracy: {}\".format((cm_dev[0,0]+cm_dev[1,1])\/cm_dev.sum()))\n    print(\"DEV Precision: {}\".format(cm_dev[1,1]\/(cm_dev[0,1]+cm_dev[1,1])))\n    print(\"DEV Recall: {}\".format(cm_dev[1,1]\/(cm_dev[1,0]+cm_dev[1,1])))\n    print(\"VAL Accuracy: {}\".format((cm_val[0,0]+cm_val[1,1])\/cm_val.sum()))\n    print(\"VAL Precision: {}\".format(cm_val[1,1]\/(cm_val[0,1]+cm_val[1,1])))\n    print(\"VAL Recall: {}\".format(cm_val[1,1]\/(cm_val[1,0]+cm_val[1,1])))\n    print(\"\")\n    ","44b55113":"print(\"Model 2 - Autoencoding- Oversampling\")\nNNKeras_Autoencoder_2 = Sequential([\n    # encoder\n    keras.layers.Dense(X_dev_os.shape[1], activation=\"tanh\", input_shape=(X_dev_os.shape[1], ), activity_regularizer=regularizers.l1(0.00001)), \n    keras.layers.Dense(int(X_dev_os.shape[1]\/2), activation=\"relu\"), \n    #decoder\n    keras.layers.Dense(int(X_dev_os.shape[1]\/2), activation=\"tanh\"), \n    keras.layers.Dense(X_dev_os.shape[1], activation=\"relu\")\n])\n\nNNKeras_Autoencoder_2 = compile_train_test_nn(\n    NNKeras_Autoencoder_2, \n    X_dev_os, y_dev_os, validation_data=(X_val, y_val), \n    epochs = 60, \n    batch_size = pow(2, 16), \n    learning_rate = 0.001, \n    loss = \"mean_squared_error\", \n    metrics = [\"accuracy\"], \n    class_weight = class_weight_BL, \n)","a9508d20":"# With the fitted model, we now predict train and test registries\nX_dev_os_predict_ = pd.DataFrame([x for x in NNKeras_Autoencoder_1.predict(X_dev_os)], columns=X_dev_os.columns, index=X_dev_os.index)\nX_val_predict_ = pd.DataFrame([x for x in NNKeras_Autoencoder_1.predict(X_val)], columns=X_val.columns, index=X_val.index)\n\nX_dev_os_rmse = np.sqrt(pow(X_dev_os_predict_ - X_dev_os, 2).mean(axis=1))\nX_val_rmse = np.sqrt(pow(X_val_predict_ - X_val, 2).mean(axis=1))\ndel X_dev_os_predict_, X_val_predict_\n\n# Now, we can plot the errors against the classes to see if there is a difference. If we cannot \"reconstruct\" frauds, we will have a hyer rmse\ndf_dev_os = pd.DataFrame(X_dev_os_rmse, columns=[\"rmse\"])\ndf_val = pd.DataFrame(X_val_rmse, columns=[\"rmse\"])\ndf_dev_os[\"Class\"] = y_dev_os\ndf_val[\"Class\"] = y_val\n\nfig, axes = plt.subplots(1, 2, figsize=(2*7, 7), sharex=True, sharey=True)\nsns.boxenplot(data=df_dev_os, x=\"Class\", y=\"rmse\", ax=axes[0])\nsns.boxenplot(data=df_val, x=\"Class\", y=\"rmse\", ax=axes[1])\nplt.show()","629b8633":"rmse_thresholds = [2, 3, 4, 5]\nfor rmse_threshold in rmse_thresholds:\n    y_dev_os_pred = df_dev_os[\"rmse\"].apply(lambda x: 0 if x<rmse_threshold else 1)\n    y_val_pred = df_val[\"rmse\"].apply(lambda x: 0 if x<rmse_threshold else 1)\n    \n    cm_dev_os = confusion_matrix(y_dev_os.values, y_dev_os_pred.values)\n    cm_val = confusion_matrix(y_val.values, y_val_pred.values)\n\n    print(\"Threshold {}\".format(rmse_threshold))\n    print(\"------------------------------\")\n    print(\"DEV Accuracy: {}\".format((cm_dev_os[0,0]+cm_dev_os[1,1])\/cm_dev_os.sum()))\n    print(\"DEV Precision: {}\".format(cm_dev_os[1,1]\/(cm_dev_os[0,1]+cm_dev_os[1,1])))\n    print(\"DEV Recall: {}\".format(cm_dev_os[1,1]\/(cm_dev_os[1,0]+cm_dev_os[1,1])))\n    print(\"VAL Accuracy: {}\".format((cm_val[0,0]+cm_val[1,1])\/cm_val.sum()))\n    print(\"VAL Precision: {}\".format(cm_val[1,1]\/(cm_val[0,1]+cm_val[1,1])))\n    print(\"VAL Recall: {}\".format(cm_val[1,1]\/(cm_val[1,0]+cm_val[1,1])))\n    print(\"\")","4af1fee8":"print(\"Model 3 - Autoencoding- Undersampling\")\nNNKeras_Autoencoder_3 = Sequential([\n    # encoder\n    keras.layers.Dense(X_dev_us.shape[1], activation=\"tanh\", input_shape=(X_dev_us.shape[1], ), activity_regularizer=regularizers.l1(0.00001)), \n    keras.layers.Dense(int(X_dev_us.shape[1]\/2), activation=\"relu\"), \n    #decoder\n    keras.layers.Dense(int(X_dev_us.shape[1]\/2), activation=\"tanh\"), \n    keras.layers.Dense(X_dev_us.shape[1], activation=\"relu\")\n])\n\nNNKeras_Autoencoder_3 = compile_train_test_nn(\n    NNKeras_Autoencoder_3, \n    X_dev_us, y_dev_us, validation_data=(X_val, y_val), \n    epochs = 60, \n    batch_size = pow(2, 16), \n    learning_rate = 0.001, \n    loss = \"mean_squared_error\", \n    metrics = [\"accuracy\"], \n    class_weight = class_weight_BL, \n)","ee7d234e":"# With the fitted model, we now predict train and test registries\nX_dev_us_predict_ = pd.DataFrame([x for x in NNKeras_Autoencoder_1.predict(X_dev_us)], columns=X_dev_us.columns, index=X_dev_us.index)\nX_val_predict_ = pd.DataFrame([x for x in NNKeras_Autoencoder_1.predict(X_val)], columns=X_val.columns, index=X_val.index)\n\nX_dev_us_rmse = np.sqrt(pow(X_dev_us_predict_ - X_dev_us, 2).mean(axis=1))\nX_val_rmse = np.sqrt(pow(X_val_predict_ - X_val, 2).mean(axis=1))\ndel X_dev_us_predict_, X_val_predict_\n\n# Now, we can plot the errors against the classes to see if there is a difference. If we cannot \"reconstruct\" frauds, we will have a hyer rmse\ndf_dev_us = pd.DataFrame(X_dev_us_rmse, columns=[\"rmse\"])\ndf_val = pd.DataFrame(X_val_rmse, columns=[\"rmse\"])\ndf_dev_us[\"Class\"] = y_dev_us\ndf_val[\"Class\"] = y_val\n\nfig, axes = plt.subplots(1, 2, figsize=(2*7, 7), sharex=True, sharey=True)\nsns.boxenplot(data=df_dev_us, x=\"Class\", y=\"rmse\", ax=axes[0])\nsns.boxenplot(data=df_val, x=\"Class\", y=\"rmse\", ax=axes[1])\nplt.show()","61bd8f99":"rmse_thresholds = [2, 3, 4, 5]\nfor rmse_threshold in rmse_thresholds:\n    y_dev_us_pred = df_dev_us[\"rmse\"].apply(lambda x: 0 if x<rmse_threshold else 1)\n    y_val_pred = df_val[\"rmse\"].apply(lambda x: 0 if x<rmse_threshold else 1)\n    \n    cm_dev_us = confusion_matrix(y_dev_us.values, y_dev_us_pred.values)\n    cm_val = confusion_matrix(y_val.values, y_val_pred.values)\n\n    print(\"Threshold {}\".format(rmse_threshold))\n    print(\"------------------------------\")\n    print(\"DEV Accuracy: {}\".format((cm_dev_us[0,0]+cm_dev_us[1,1])\/cm_dev_us.sum()))\n    print(\"DEV Precision: {}\".format(cm_dev_us[1,1]\/(cm_dev_us[0,1]+cm_dev_us[1,1])))\n    print(\"DEV Recall: {}\".format(cm_dev_us[1,1]\/(cm_dev_us[1,0]+cm_dev_us[1,1])))\n    print(\"VAL Accuracy: {}\".format((cm_val[0,0]+cm_val[1,1])\/cm_val.sum()))\n    print(\"VAL Precision: {}\".format(cm_val[1,1]\/(cm_val[0,1]+cm_val[1,1])))\n    print(\"VAL Recall: {}\".format(cm_val[1,1]\/(cm_val[1,0]+cm_val[1,1])))\n    print(\"\")","896dcca4":"report.compare_models()","9b28e4fe":"# And finally, we generate the report and save the Report instance to have all the info with us\nversion = str(datetime.datetime.now().year) + str(datetime.datetime.now().month) + str(datetime.datetime.now().day) + \"_\" + str(datetime.datetime.now().hour) + \"_\" + str(datetime.datetime.now().minute)","d6968d0f":"# CSV file with the results\nreport.compare_models().to_csv(output_path + version + \" - Credit Card Fraud Detection - report.csv\")","8696327d":"# Pickle with the report\n# pickle.dump(report, open( output_path + version + \"Credit Card Fraud Detection - report pickle.pkl\", \"wb\" ))","0b0b5f3f":"The model is absolutely agnostic to the **n_estimators** parameter. We can also see that there is an extreme varaibility in the test results. Our model is no really solid.\n\nLet's see the max_samples param. ","512b6e1b":"#### **Results**\n\n1. Precisions and recalls vary between datasets. It is not a solid model. It behaves differently depending on the prediction inputs.\n2. The trained model offers low precision (85%) and recall (76%). Those values are **even lower when predicting on the validation set**.\n3. Accuracy and AUC values are pretty similar","18e61fdc":"#### Undersampling","25a6332e":"# Light EDA & Preprocessing","a474b4d1":"**Model 1.1 and 1.2 summary**\n- The balance between precision and recall is much better in model 1.2. Model 1.1 clearly benefits recall, but has a very poor performance in terms of precision, which would not be admissible when dealing with fraud and transactions denial (bad customer experience).\n- We will stick to the original weight of the class and check for other NN architectures.","e05b0584":"# Neuronal Networks","6fbd40ba":"### Conclusions","ba9ecd9c":"### Undersampled (us)\nWe will use the NearMiss object to reduce the number of negative classes keeping the same richness of data.","0a6bdcba":"## Decision Tree XGBoost - GridSearchCV\n\nWe will follow the next steps:\n\n1. We will see which depth is the best for the xgboost using GridSearchCV with SMOTE (imbalance pipeline)\n2. We will select the GridSearchCV best parameters\n3. We will refit the model with all the dev dataset\n4. We will execute the report","69ca44f1":"## Isolation Forest","26521966":"#### Oversampling\n\nLet's see what happens when using the oversampled datasets. We will use the balanced class weight","5b847762":"## Variables, constants, classes and functions","e55f0ab2":"### Parameters Selection","c48b4d23":"## Quick understanding of data","314fbdde":"**KERAS SUMMARY**\n- Working with the original datasets offers better results in terms of recall + precision, taking into account that it is not admissible to have low precision rates due to potentially negative customer experience (transaction denials).\n- Denser model, offers better f1.","8eeaa45e":"## Autoencoder","12608a20":"## Decision Tree XGBoost - GridSearchCV","e49fa68c":"## Isolation Forest","2e1e610b":"We will now compare the different scores for training and test. We want to see that the values are homogeneus, and as similar as possible.","c4da7178":"**Model 3 summary**\n- Using oversampling dataset for training seems to generate overfitting in terms of precision. When using new data, precision falls almost to 0, while keeping a good value of recall.","aa347586":"### Result","a26b864f":"### **Results**\n\n- Small overfitting in terms of precision.\n- Very good balance in terms of precision and recall.\n- Pretty similar to XGBoost","4bf1c30a":"**Autoencoder Summary**\n- Although the models shows a difference between non-fraud and fraud rmse, the overlapping of the different values does not allow the model to be competitive compared to the others. The balance between precision and recall is not good enough.\n- We will not store these results","d27c6211":"# OVERSAMPLING Models","74ee9235":"First, we will generate DecisionTree, Logistic Regression and SVC models with the imbalanced data.\n\nWe only do it in order to set a baseline. Having such an imbalanced dataset, we are training these models for the negative case (Class=0)","f62ef50c":"# **CREDIT CARD FRAUD - Step by step process**\nhttps:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud\n\n---","28092864":"### Selected model","09469496":"### Depth selection","2f196991":"492 positive cases. Therefore, the new dataset will have 984 entries in total (50% positive and 50% negative).","5b17e78d":"### Result\n\n- Clearly overfited model, the training dataset offers too good results, that decay with the validation dataset, specially in terms of precision.","b7b6be6f":"Isolation Forests can detect outlieres in multidimensional datasets. When predicting values, the results can be:\n\n1. -1 for outliers and 1 for normal\n2. values around 0.5 in all dataset, indicating there are no outliers\n\nWe will build using the initial coding indicated in this article of towardsdatascience, https:\/\/towardsdatascience.com\/outlier-detection-with-isolation-forest-3d190448d45e.\n\nThe process will be:\n\n1. Create a class that will handle IsolationForest so that it can feed our report class. IsolationForest is a non supervised algorithm that returns whether a registry is outlier or not. We need to convert outlier to fraud and non-outlier to non-fraud.\n2. fit the model with the training data\n3. predict \"outliers\"-->fraud\n4. From there, we will compare different parameters","78640488":"## Keras","654a5683":"- Our recall is not very good for the positive class. We are missing 19% of the fraud transactions.\n- Our precision is very good for the positive class, although we will find an 8% of FALSE POSITIVES. ","d436d04a":"### Imbalanced","93f6d52a":"### Selected model","492bcf2b":"- Population: fraudulent cases (492)\n- Statisticaly significance VALIDATION sample\/dataset (96 | 20% - 95% confidence + 9% error)\n\n- 396\/96 registries development and validation (development will be used when applying cross-validation strategy)\n- 300\/96 registries training and testing (development)\n- Split independent X from dependant y\n- All splits will have the same weight of the positive class\n\n- training = model fit\n- testing = testing the model fit\n- dev = fiting and cross-validating the model\n- validation = final validation of the final model\n\n**IMPORTANT NOTE** When spliting the dataset, we have to be careful in terms of statistical significance when validating the results and comparing the different models. \n\nWe will probably find variability between training, test and validation due to this factor, specially in the undersampling case.","2a0f5ce5":"### Result","5331e5bb":"## DecisionTree XGBoost - GridSearchCV\n\nLet's start with a simple option. We will use XGBoost with k-fold strategy.","2eb0c5a1":"None of the parameters affect the metrics of our model. Therefore, we will keep the original one and store in it in our report. Remember that this model is not solid at all.","7311e557":"## DecisionTree (Baseline)","047d26e5":"**COMMENTS**\n- The best models are for the original dataset. When undersampling or oversampling, the models tend to be bad in terms of precision.\n- **XGBClassifier** is the best model, with an 81% of recall, and 92% precision. That would allow us to reduce the impact of FALSE POSITIVES, which can be very negative in terms of Customer Experience. \n- We may be able to improve the XGBClassifier estimator by playing a little bit more with the hyperparameters, but I will leave it as it is right now.\n- Although KNN Faiss offers very good results, it si quite slow in terms of prediction, as it has to calculate all the distances. KNN Faiss is a much faster algorithm compared to standard KNN SKlearn, but still slow.","50bfebd1":"## Splitting dataset (Original \/ Undersampled \/ Oversampled)","037cbfd8":"### Oversampled (os - fcols)\n\nThis dataset will only be used in baseline and GridSearchCV strategies with SMOTE. In terms of GridSearchCV and when creating a pipeline, SMOTE changes the name of the dataset columns following this nomenclature f0, f1, f3... fn. If we want to predict the y_values, the trained model expect those names.\n\nWe will only oversample the training dataset. The rest, will only have their column names changed.","32fdc606":"Generally, there is no overfitting in the different models. Same accuracy values. \nThe last Kfold set has a higher accuracy, but again consistent between train and test.","e85cbb11":"### Results\n\n- Training with undersampled dataset generates very bad results in terms of accuracy and precision.\n\nVERY BAD MODEL","8daae6de":"Bellow, you will find my notebook to deal with the Credit Card Fraud challenge. The intention of this notebook is not only to get the best model taking into account my recently acquired machine learning skills, but to understand the different algorithms and specifics, such as imbalanced datasets, collinearity, data processing, ... and even the use of different EDA modules.\n\n**I hope you like it. I am sure there are some mistakes. Therefore, please feel free to make the necessary comments, critics or whatever you wish.**\n\nThe notebook has been structured the following way:\n\n**Initialization**\n\nAll the necessary modules for this notebook, including custom made, and also global variables and constants.\n\n**Loading data**\n\nSelf explanatory\n\n**Light EDA & Preprocessing**\n\nLight EDA as the data has been preprocessed. I also include the necessary transformation of Time and AMount and the creation of under and oversampled dataframes to be used in the following sections.\n\n**IMBALANCED - Models**\n\nI have worked with the original dataset, highly imbalanced, and applied different ML algorithms.\n- Decision Tree (as a baseline)\n- Decision Tree XGBoost\n- Isolation Forest\n- Logistic Regression\n- knn\n\n**OVERSAMPLING - Models**\n\nSame as before, but using the SMOTE incremented dataset.\n\n**UNDERSAMPLING - Models**\n\nSame as before, but using the undersampled dataset, with 984 registries (50% fraud).\n\n**NEURONAL NETWORKS**\nAlthough will include imbalanced, oversampling and undersampling datasets, I have separated this section as it is a completely different approach.\nI have worked on different architectures and even used Autoenconders.\n\n**SUMMARY**\n\nComparison of all the metrics for the different models.\n\n**PENDING WORK**\n- Redo all models with a dataset that does not include the independent variables that haven't got a high correlation with the Class.\n- Redo the models eliminating outliers (specially those that affect the positive Class).","3dd5b9ba":"#### **Results**\n- Dev and val accuracies are quite similiar, AUC is better in validation but precision and recall are pretty bad in validation (85% and 78%).\n- Compared to baseline, there is not really a difference at all. \n- Therefore, we do not proceed whit hypermparameter optiimzation.","87d35493":" # Initialization <a id=\"initialization\" name=\"initialization\"><\/a>","35ee1509":"## kNN Faiss\n\nOur data is already scaled. Therefore, we do not need any extra work. ","f8ee67e3":"## Logistic Regression - GridSearchCV","7f48a998":"Let's split de database. Although there is a time variable, this is not a time series. Therefore, we will randomly split the dataset. We have to make sure that we have the same distribution of positive class.","e15d5bdf":"## Transforming data","5803618a":"The process is as follows:\n\n1. define and fit the autoencoder\n2. predict the vectors\n3. calculate MSE for each of them, compared to the right one\n4. plot MSEs depending on the class value to see relationship\n5. apply quartiles or any other differentiator to predict fraud\/no-fraud\n\nhttps:\/\/www.pyimagesearch.com\/2020\/03\/02\/anomaly-detection-with-keras-tensorflow-and-deep-learning\/","95469bb9":"From what we see, this is a model that easily tends to overfitting, as the accuracy is higher in train from depth=3 onwards. We have to remember that accuracy is not a great metric when working with imbalanced datasets. F1 behaves more or less the same. \n\nWe will work with a depth of 3.","b7d6f49d":"Generally, there is no overfitting in the different models. Same accuracy values. \nThe last Kfold set has a higher accuracy, but again consistent between train and test.","404b431e":"## LogisticRegression - GridSearchCV","78f71682":"## Logistic Regression - GridSearchCV","4491700b":"We will build on top of  Keras documentation's \"Credit Card Fraud Detection\" example. We will play with the different training datasets, and also hyperparameters and layers. https:\/\/keras.io\/examples\/structured_data\/imbalanced_classification\/\n\nAlso building from here https:\/\/medium.com\/datadriveninvestor\/building-neural-network-using-keras-for-classification-3a3656c726c1\n\nIn order to find the best model we could use GridSearchCV, but we would need to wrap the keras model with KerasClassifier and, therefore, we would need to downgrade the sklearn package due to retro-compatibility issues. By doing this, we would ruin previous code (KerasClassifier example: https:\/\/machinelearningmastery.com\/use-keras-deep-learning-models-scikit-learn-python\/). \n\nWe will train the models with the different datasets (imbalanced, oversampled and undersampled).\n\nA good tutorial on how to deal with neuronal networks https:\/\/cs231n.github.io\/neural-networks-3\/\n","00d0c1be":"- This dataset is the result of previous PCA over the original variables, probably because of privacy issues. \n- The class indicates whether a transaction is fraud (1 - ground truth) or not (0). The database is clearly imbalanced (0.0017% of frauds among the whole db)\n- The original field format of Amount has been kept and indicates the purchase amount. Positive figures.\n- There are no Null values, therefore we will not have to add, interpolate or extrapolate any data.\n\n**Variables**\n- V20 and above: do not seem to have influence in the dependant Class. \u00bfShould we not include them in some models?\n- V4, V10, V11, V12, V14, V16, V17, V18 are slightly correlated with the Class, but all abs values are lower than 0.75.\n- V3->V1-V2 \/ V10->V11 \/ V5->V1-V3 \/ V7->V1-V3-V5 \/ V7->V1-V2-V3-V5 \/ V10->V3-V7-V9 \/ V11->V10 \/ V12->V4-V10 \/ V14->V11-V12 \/ V16->V11-V12 \/ V17->V11-V12-V16 \/ V18->V10-V12-V16-V17 are highly correlated (over 0.8)\n- Amount is potentially affected by collinearity (see below)\n\n**Multicollinearity?**\n\nInteresting reading: \n- https:\/\/www.geeksforgeeks.org\/detecting-multicollinearity-with-vif-python\/\n- https:\/\/statisticsbyjim.com\/regression\/multicollinearity-in-regression-analysis\/\n\n\"Multicollinearity affects the coefficients and p-values, but it does not influence the predictions, precision of the predictions, and the goodness-of-fit statistics (including the metrics such as r, r_squared, standard_error - not for logistic regression, of course). If your primary goal is to make predictions, and you don\u2019t need to understand the role of each independent variable, you don\u2019t need to reduce severe multicollinearity.\"\n\n--> As our objective is predicting fraud and not specifically understanding the impact of each variable, we could temporaly reject dealing with it. Attending to VIF, only Amonunt is a multicollinear variable.\n\n**Outliers and relevant variables**\n\nAs seen, there are many variables that, analyzed separately, seem to have no effect on the Class. Once we have found the best model, we will retrain it **without the influence of these variables** and even **potential outliers**.","7f1864ba":"The model is absolutely agnostic to the **max_samples** parameter. Let's see the max_features param.","d63b29ee":"### Selected model","e8551712":"#### **Results**\n- The prediction over the validation set is worse than the dev\/trained one. \n- Again, this model may be unstable depending on the data to predict.","7cef28fc":"Very poor results for both precision and recall (and f1). Let's see what happens if we play with the parameters. We will use validation_curve to do so, via ceteris-paribus.\n\nWe start with n_estimators.","e1938964":"# And the winner is.... XGBClassifier with Imbalanced dataset","efccb443":"#### **Results**","88d5b7f0":"## Isolation Forest","dd2fe6d8":"### Selected model","7013c9a9":"### Selected model","bff8ac68":"### A comment about the sample\nAs we already know, our dataset is clearly imbalanced. The number of positive cases is 492. When addressing Cross-validation for training purposes, we must consider that if we want a confidence level of 95% we should have training sets of 396 positive cases.","4f95dafb":"We will start with a dense model that includes dropout. We will try two combinations:\n- One that tries to balanced the class (Model 1.1)\n- A second one that keeps the same weight for the existing class (Model 1.2)","1d7e0204":"The final size of the training dataset will be 228.838*2 = 454.902","089a3003":"Analyzing the metrics we can see:\n- Again, training accuracy results tend to be better than the test dataset. That indicates potential overfitting. \n- Max_depth 3 (4, 5 and 6) behave more or less the same for both accuracy and AUC metrics.\n- We will choose the best_estimator, that has been refit using the whole DEV database. Nevertheless, is affected by overfitting.","ea2d68dc":"**Model 2.1 and 2.2 summary**\n- The balance between precision and recall is much better in model 2.2. Model 2.1 clearly benefits recall, but has a very poor performance in terms of precision, which would not be admissible when dealing with fraud and transactions denial (bad customer experience).\n- Compared to previous models, recall is not good enough, quite low.\n- We will stick to the original weight of the class","3f1e6315":"### Results\n\n- This model clearly favours recall. \n- In return, precision is very low both in training and validation.\n- AUC value is better and accuracy less than the presence of class in dataset.\n\nBAD MODEL","e3b5f2da":"## SVM","1bf31ae9":"#### Imbalanced","5765d5a3":"### Undersampling","10dc369b":"## Decision Tree XGBoost - KFold\n\nWe will follow the next steps:\n\n1. We will try different depth for the XGBClassifier\n2. Each depth loop will have a 5-kfold strategy.\n3. Each training set will be oversampled through SMOTE\n4. At each loop we will calculate the avg accuracy for both training and test\n5. We will finally select the depth xgboost that does NOT overfit and offers the best accuracy\n6. Once done, we will fit the depth xgboost again to get the final model and validate\n7. We will then generate and store the report.","c517f5bc":"### Depth selection","47da0f81":"## Library Imports","13ce17e6":"It is evident that the model does not encode the frauds as good as the non-frauds, but we can see that there are overlappings in terms of rmse value. Therefore, our model will have a lower precision. It will all depend on the rmse threshold.\n\nThe good thing is that there seems not to be overfitting, as the rmse distributions are pretty similar.","2188a59e":"Blue is for train and green is for test\n\nDepth 7 is the best model possible, as the accuracies of train and test are very close and quite homogeneous no matter what the kfold they use. The blue line (train) is still under the green (test) line, indicating there is no overfitting.\n\nTherefore, we have to set max_depth to 7, and retrain the model with oversampled data.","ab4e07ca":"# IMBALANCED Models","7f8e5f47":"### Parameters selection","471e91be":"# UNDERSAMPLING Models","32eb58f8":"## Loading data","dbaabf4c":"### Parameters selection","b9a648eb":"### Original (imbalanced)","22d5ae2c":"### Oversampling"}}