{"cell_type":{"2879e3f4":"code","74e96dac":"code","5b2ab98e":"code","f04c538b":"code","537428d1":"code","6b1d9146":"code","a874eb44":"code","9243a734":"code","f32beec7":"code","2f0f0bf9":"code","1f66e27a":"code","03ae0fe5":"code","74da85d4":"code","829e014a":"code","aaab3266":"code","a96a72e4":"code","c2f757af":"code","6da775d0":"code","95da7fc1":"code","53c626b5":"code","07b17e95":"code","8bc7520d":"code","e43a41b8":"code","7c22f726":"code","3873333a":"code","ceda8964":"code","9cfd269f":"code","2e5123dc":"markdown","9816556b":"markdown","0da5bf1e":"markdown","01cca34c":"markdown","73f4dbaf":"markdown","6c28922e":"markdown","9bcc3988":"markdown","defd89ce":"markdown","151a76db":"markdown","2c3270dd":"markdown","84ae5949":"markdown","2a8f0d1f":"markdown","35f02e83":"markdown","7d187b5a":"markdown","55ddf2ed":"markdown","4f2554f5":"markdown","1db539b9":"markdown","f1952a20":"markdown","6ebc3d7e":"markdown","f3e7dcb4":"markdown","6b6831be":"markdown","4fe9cb4c":"markdown","d95a19c6":"markdown","271450b6":"markdown","da67ae66":"markdown","1b5a1c94":"markdown","0e8f2f54":"markdown","abd65226":"markdown","dca201f4":"markdown"},"source":{"2879e3f4":"#Importing Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.base import BaseEstimator,TransformerMixin\nfrom sklearn.pipeline import Pipeline\n\n","74e96dac":"#Reading the .data file using pandas\n\ncols = ['MPG','Cylinders','Displacement','Horsepower','Weight',\n                'Acceleration', 'Model Year', 'Origin']\n\nauto_df=pd.read_csv(\"..\/input\/autompg\/auto-mpg.data\",names=cols,na_values='?',comment='\\t',sep=\" \",skipinitialspace=True)\ndata=auto_df.copy()\n\nsplit=StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\nfor train_index,test_index in split.split(data,data['Cylinders']):\n    strat_train_set=data.loc[train_index]\n    strat_test_set=data.loc[test_index]","5b2ab98e":"data=strat_train_set.drop('MPG',axis=1)\ndata_label=strat_train_set[\"MPG\"].copy()\ndata.head()","f04c538b":"def preprocess_origin_col(df):\n    df[\"Origin\"] = df[\"Origin\"].map({1: \"India\", 2: \"USA\", 3: \"Germany\"})\n    return df","537428d1":"index_acc,index_hp,index_cyl=4,2,0\n\nclass FeatureAdder(BaseEstimator,TransformerMixin):\n    def __init__(self,acc_power=True):\n        self.acc_power=acc_power\n    def fit(self,X,y=None):\n        return self\n    def transform(self,X):\n        acc_cylinder=X[:,index_acc]\/X[:,index_cyl]\n        if self.acc_power:\n            acc_power=X[:,index_acc]\/X[:,index_hp]\n            return np.c_[X,acc_power,acc_cylinder]\n        return np.c_[X,acc_cylinder]\n    ","6b1d9146":"def numeric_pipeline(data):\n    numerics = ['float64', 'int64']\n    num_attrs = data.select_dtypes(include=numerics)\n\n    num_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('attrs_adder', FeatureAdder()),\n        ('std_scaler', StandardScaler()),\n        ])\n    return num_attrs, num_pipeline\n\n\ndef data_pipeline(data):\n    cat_attrs = [\"Origin\"]\n    num_attrs, num_pipeline = numeric_pipeline(data) #Calling numeric_pipeline function\n    full_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, list(num_attrs)),\n        (\"cat\", OneHotEncoder(), cat_attrs),\n        ])\n    prepared_data = full_pipeline.fit_transform(data)\n    return prepared_data","a874eb44":"preprocess_df=preprocess_origin_col(data)\nprepared_data=data_pipeline(preprocess_df)\nprepared_data[0]\n","9243a734":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()              #Calling Linear Regression\nlin_reg.fit(prepared_data, data_label)    #Fitting our data to the model","f32beec7":"# Testing the predictions with sample\nsample_data = data.iloc[:5]\nsample_labels = data_label.iloc[:5]\n\nsample_data_prepared = data_pipeline(sample_data)\n\nprint(\"Prediction of samples: \", lin_reg.predict(sample_data_prepared))\n\nprint(\"Actual Labels of samples: \", list(sample_labels))","2f0f0bf9":"from sklearn.metrics import mean_squared_error\n\nlg_prediction=lin_reg.predict(prepared_data)\nlin_mse= mean_squared_error(data_label,lg_prediction)\nlin_rmse= np.sqrt(lin_mse)\nlin_rmse","1f66e27a":"#Random Forest\nfrom sklearn.ensemble import RandomForestRegressor\n\ntree_reg= RandomForestRegressor()\ntree_reg.fit(prepared_data,data_label)\ntree_prediction = tree_reg.predict(prepared_data)\ntree_mse=mean_squared_error(data_label,tree_prediction)\ntree_rmse=np.sqrt(tree_mse)\ntree_rmse","03ae0fe5":"from sklearn.tree import DecisionTreeRegressor\n\ndec_tree= DecisionTreeRegressor()\ndec_tree.fit(prepared_data,data_label)","74da85d4":"dec_prediction = dec_tree.predict(prepared_data)\ndec_mse=mean_squared_error(data_label,dec_prediction)\ndec_rmse=np.sqrt(dec_mse)\ndec_rmse","829e014a":"from sklearn.model_selection import cross_val_score\n\n# Function for cross_validation\ndef cross_validation(estimator,independent,dependent,cv=None):\n    scores=cross_val_score(estimator,independent,dependent,scoring=\"neg_mean_squared_error\",cv=cv)\n    return scores\n\n# Function to calculate Root mean square error\ndef root_mean_square(scores):\n    rmse_array=np.sqrt(-scores)\n    rmse=np.mean(rmse_array)\n    return rmse\n","aaab3266":"# DecisionTreeRegressor\ndec_tree_scores=cross_val_score(dec_tree,prepared_data,data_label,scoring=\"neg_mean_squared_error\",cv=10) \ndec_cross_rmse=root_mean_square(dec_tree_scores)\nprint(\"Mean Root mean Square of DecisionTreeRegressor:\",dec_cross_rmse)\n\n# LinearRegression\nlinear_reg_scores=cross_val_score(lin_reg,prepared_data,data_label,scoring=\"neg_mean_squared_error\",cv=10) \nlinear_reg_rmse=root_mean_square(linear_reg_scores)\nprint(\"-\"*70)\nprint(\"Mean Root mean Square of LinearRegression:\",linear_reg_rmse)\n\n#RandomForestRegressor\ntree_cv=cross_validation(tree_reg,prepared_data,data_label,cv=10)\ntree_rmse=root_mean_square(tree_cv)\nprint(\"-\"*70)\nprint(\"Mean Root mean Square of LinearRegression:\",tree_rmse)\nprint(\"-\"*70)","a96a72e4":"root_mean_values=np.array([tree_rmse,dec_cross_rmse,linear_reg_rmse])\nroot_mean_data=pd.DataFrame([root_mean_values],columns=[\"RandomForestRegressor\",\"DecisionTreeRegressor\",\"LinearRegression\"])\nroot_mean_data","c2f757af":"tree_reg.get_params() # Output every parameter for RandomForestRegressor","6da775d0":"from sklearn.model_selection import GridSearchCV\n\n# We create 2 grids to be explored\nparam_grid=[\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n  ]\n\nforest_reg=RandomForestRegressor()\ngrid_search= GridSearchCV(forest_reg,\n                          param_grid,\n                          scoring='neg_mean_squared_error',\n                          return_train_score=True,\n                          cv=10)\ngrid_search.fit(prepared_data,data_label)","95da7fc1":"# Storing the results\ncv_scores=grid_search.cv_results_","53c626b5":"for mean_square,param in zip(cv_scores['mean_test_score'],cv_scores['params']):\n    print(np.sqrt(-mean_square),param)\n\nprint(\"-\"*60)\n# TO check for the parameters selected by our estimator.\nprint(grid_search.best_estimator_)","07b17e95":"feature_importance=grid_search.best_estimator_.feature_importances_\nfeature_importance","8bc7520d":"extra=[\"acc_power\",\"acc_cyl\"]\nnum=[\"float64\",\"int64\"]\n\nnum_aatr= list(data.select_dtypes(include=num))\n\nattr=num_aatr+extra\n\nsorted(zip(attr,feature_importance),reverse=True)","e43a41b8":"final=grid_search.best_estimator_\n\nauto_test_data=strat_test_set.drop(\"MPG\",axis=1)\nauto_test_label=strat_test_set[\"MPG\"].copy()\n\nauto_test_process=preprocess_origin_col(auto_test_data)\nauto_test_prepared=data_pipeline(auto_test_process)\n\nfinal_predict=final.predict(auto_test_prepared)\nfinal_mse=mean_squared_error(final_predict,auto_test_label)\nfinal_rmse=np.sqrt(final_mse)\nfinal_rmse\n","7c22f726":"def predict_func(config,model):\n    if type(config)==dict:                # Checking whether incoming data is a DataFrame or not, if not convert it into one\n        df=pd.DataFrame(config)\n    else:\n        df=config\n    preproc_df=preprocess_origin_col(df)  # Encoding\n    prep_data=data_pipeline(preproc_df)   # Data and Numeric Pipeline\n    pred=model.predict(prep_data)         # Prdiction\n    return pred","3873333a":"#Creating our own example\nvehicle_config = {\n    'Cylinders': [4, 6, 8],\n    'Displacement': [155.0, 160.0, 165.5],\n    'Horsepower': [93.0, 130.0, 98.0],\n    'Weight': [2500.0, 3150.0, 2600.0],\n    'Acceleration': [15.0, 14.0, 16.0],\n    'Model Year': [81, 80, 78],\n    'Origin': [3, 2, 1]\n}\n\nvalues=predict_func(vehicle_config, final)\nprint(\"Predictions:\")\nprint(\"-\"*60)\nfor i in range(len(values)):\n    print(i,\"->\" ,values[i])","ceda8964":"import pickle\nwith open(\"model.bin\",\"wb\") as f_out:\n    pickle.dump(final,f_out)\n    f_out.close()","9cfd269f":"with open(\"model.bin\",\"rb\") as f_in:\n    model=pickle.load(f_in)\n\npredict_func(vehicle_config,model)","2e5123dc":"Through linear regression we get mean squared error of 2.95 which is good but still we make decision after comparing it with other models.","9816556b":"K-cross valiation technique divides the training data into K distinct subsets called folds, then it trains on individual fold and evaluate the model K times, picking a different fold for evaluation every time and training on other K-1 folds.\n\nResult is an array containing the K evaluation scores:","0da5bf1e":"### Mean Squared Error","01cca34c":"### Segregate independent and dependent variables.","73f4dbaf":"#### Viewing the Feature Importance ","6c28922e":"# Predicting Fuel Efficiency of Vehicles.\n\n   Selecting and training models\n   1. Select and train a few algorithms.\n   2. Evaluation using mean squared error.\n   3. Model Evaluation using Cross Validation.\n   4. Hyperparameter Tuning\n   5. Check Feature Importance \n   6. Evaluate final model\n   7. Save the model","9bcc3988":"### From Raw Data to process data in 2 steps ","defd89ce":"### RandomForestRegressor","151a76db":"### DecisionTreeRegressor","2c3270dd":"Although the error has reduced but still we will go with one more model and compare the performance","84ae5949":"### Model Validation using  Cross Validation","2a8f0d1f":"We can see that features [Weight, Model Year, Horsepower, Displacement, Cylinders] have got the larger number.","35f02e83":"## Function:\n    1) numeric_pipeline:Function to process numerical transformations\n                        Argument: data->original dataframe.\n                        Returns: num_attrs-> numerical dataframe\n                                 num_pipeline->numerical pipeline object\n                         \n    2) data_pipeline: Complete transformation pipeline for both numerical and categorical data.\n                      Argument: data-> original dataframe \n                      Returns: prepared_data-> transformed data, ready to use\n","7d187b5a":"As we can see that RandomForestRegressor provides us with the minimum error so we will continue with the same.","55ddf2ed":"### Creating Custom Attribute Adder class\nCreating new features, information about same is present in EDA.ipynb, refer to same to understand it more","4f2554f5":"Hyperparameters are parameters that are not directly learnt with the model. It is possible and recommended to search the \nhyper-parameter space or best cross validation score.\nThe grid search provided by GridSearchCV exhaustively generates candidates from a grid of parameter values specified with the param_grid parameter. For instance, the following param_grid\n\nTo find the names and current values for all parameters for a given estimator use:","1db539b9":"### Encoding the categorical \"Origin\" column.","f1952a20":"### Comparing the results of all three model","6ebc3d7e":"# Linear Model","f3e7dcb4":"### Evaluating model on the entire Test Data ","6b6831be":"Here error which we received is 0 but no model can be perfect. This means overfitting has occured.\nBecause of similar scenario, we don't touch our test data until we are sure of the efficiency of our model.","4fe9cb4c":"#### EDA has been done on same data in different file. Please see the same for more information.","d95a19c6":"#### Checking Feature Importance ","271450b6":"### Hyperparameter Tuning using GridSearchCV ","da67ae66":"#### Printing all the parameters along with their scores","1b5a1c94":"### Creating a function to cover entire workflow ","0e8f2f54":"# Traning Models:\n    1.Linear Regression\n    2.Decision Tree\n    3.Random Forest","abd65226":"### Validating all 3 models","dca201f4":"### Save the Model "}}