{"cell_type":{"1b00912b":"code","c88153bf":"code","387f8ebb":"code","f4e6b110":"code","152c8fc7":"code","c46cff60":"code","3cae8ea9":"code","7c9d3bc6":"code","b1f09a63":"code","cb3ddc14":"code","ead0dd0a":"code","17685ae5":"code","92040b6c":"code","526b66c5":"code","085156f6":"code","4b55d735":"code","2ae90f4a":"code","80036994":"markdown","21745b07":"markdown"},"source":{"1b00912b":"# Import the necessary libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport time\nimport warnings\nimport gc\nfrom six.moves import urllib\nimport matplotlib\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings('ignore')","c88153bf":"#Add All the Models Libraries\n\n# Scalers\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import shuffle\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.preprocessing import LabelEncoder\n\n# Models\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn.svm import SVC # Support Vector Classifier\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.ensemble import ExtraTreesClassifier \nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.ensemble import BaggingClassifier\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\nfrom scipy.stats import reciprocal, uniform\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\n\n# Cross-validation\nfrom sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nfrom sklearn.model_selection import cross_validate\n\n# GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n#Common data processors\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import check_array\nfrom scipy import sparse\n\n#Accuracy Score\nfrom sklearn.metrics import accuracy_score","387f8ebb":"# to make this notebook's output stable across runs\nnp.random.seed(123)\n\n# To plot pretty figures\n%matplotlib inline\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12","f4e6b110":"train = pd.read_csv(\"..\/input\/train_users_2.csv\")\ntest = pd.read_csv(\"..\/input\/test_users.csv\")\nid_test = test['id']\nlabels = train['country_destination'].values\ndf_train = train.drop(['country_destination'], axis=1)\ntrain_flag = df_train.shape[0]","152c8fc7":"#We now concat Training and Test set\ndf_total = pd.concat((df_train, test),axis=0, ignore_index = True)","c46cff60":"df_total = df_total.drop(['id','date_first_booking'], axis=1)","3cae8ea9":"#Date Account created - Capture Date, month and year seperately.\n\ndate_ac = np.vstack(df_total.date_account_created.astype(str).apply(lambda x:list(map(int,x.split('-')))).values)\ndf_total['Day'] = date_ac[:,0]\ndf_total['Month']= date_ac[:,1]\ndf_total['year'] = date_ac[:,2]\n\ndf_total = df_total.drop(['date_account_created'],axis=1)","7c9d3bc6":"#Time Stamp first active\n\ntime_stp = np.vstack(df_total.timestamp_first_active.astype(str)\n                     .apply(lambda x: list(map(int,[x[:4],x[4:6],x[6:8],x[8:]]))).values)\n\ndf_total['tfa_day'] = time_stp[:,0]\ndf_total['tfa_Month'] = time_stp[:,1]\ndf_total['tfa_year'] = time_stp[:,2]\n\ndf_total = df_total.drop(['timestamp_first_active'],axis=1)","b1f09a63":"#impute the missing age\nval = df_total.age.values\ndf_total['age'] = np.where(np.logical_or(val<14,val>100),-1,val)","cb3ddc14":"# The CategoricalEncoder class will allow us to convert categorical attributes to one-hot vectors.\n\nclass CategoricalEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n                 handle_unknown='error'):\n        self.encoding = encoding\n        self.categories = categories\n        self.dtype = dtype\n        self.handle_unknown = handle_unknown\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the CategoricalEncoder to X.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_feature]\n            The data to determine the categories of each feature.\n        Returns\n        -------\n        self\n        \"\"\"\n\n        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n                        \"or 'ordinal', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.handle_unknown not in ['error', 'ignore']:\n            template = (\"handle_unknown should be either 'error' or \"\n                        \"'ignore', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n                             \" encoding='ordinal'\")\n\n        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n        n_samples, n_features = X.shape\n\n        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n\n        for i in range(n_features):\n            le = self._label_encoders_[i]\n            Xi = X[:, i]\n            if self.categories == 'auto':\n                le.fit(Xi)\n            else:\n                valid_mask = np.in1d(Xi, self.categories[i])\n                if not np.all(valid_mask):\n                    if self.handle_unknown == 'error':\n                        diff = np.unique(Xi[~valid_mask])\n                        msg = (\"Found unknown categories {0} in column {1}\"\n                               \" during fit\".format(diff, i))\n                        raise ValueError(msg)\n                le.classes_ = np.array(np.sort(self.categories[i]))\n\n        self.categories_ = [le.classes_ for le in self._label_encoders_]\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform X using one-hot encoding.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to encode.\n        Returns\n        -------\n        X_out : sparse matrix or a 2-d array\n            Transformed input.\n        \"\"\"\n        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n        n_samples, n_features = X.shape\n        X_int = np.zeros_like(X, dtype=np.int)\n        X_mask = np.ones_like(X, dtype=np.bool)\n\n        for i in range(n_features):\n            valid_mask = np.in1d(X[:, i], self.categories_[i])\n\n            if not np.all(valid_mask):\n                if self.handle_unknown == 'error':\n                    diff = np.unique(X[~valid_mask, i])\n                    msg = (\"Found unknown categories {0} in column {1}\"\n                           \" during transform\".format(diff, i))\n                    raise ValueError(msg)\n                else:\n                    # Set the problematic rows to an acceptable value and\n                    # continue `The rows are marked `X_mask` and will be\n                    # removed later.\n                    X_mask[:, i] = valid_mask\n                    X[:, i][~valid_mask] = self.categories_[i][0]\n            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n\n        if self.encoding == 'ordinal':\n            return X_int.astype(self.dtype, copy=False)\n\n        mask = X_mask.ravel()\n        n_values = [cats.shape[0] for cats in self.categories_]\n        n_values = np.array([0] + n_values)\n        indices = np.cumsum(n_values)\n\n        column_indices = (X_int + indices[:-1]).ravel()[mask]\n        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n                                n_features)[mask]\n        data = np.ones(n_samples * n_features)[mask]\n\n        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n                                shape=(n_samples, indices[-1]),\n                                dtype=self.dtype).tocsr()\n        if self.encoding == 'onehot-dense':\n            return out.toarray()\n        else:\n            return out","ead0dd0a":"class DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names]","17685ae5":"cat_pipeline = Pipeline([\n        (\"selector\", DataFrameSelector(['affiliate_channel',\n                                        'affiliate_provider','first_device_type'])),\n        (\"cat_encoder\", CategoricalEncoder(encoding='onehot-dense')),\n    ])\n\nnum_pipeline = Pipeline([\n        (\"selector\", DataFrameSelector(['signup_flow','Day','Month','year','tfa_day','tfa_Month','tfa_year'])),\n        ('std_scaler', StandardScaler()),\n      ])\n\nfull_pipeline = FeatureUnion(transformer_list=[\n    (\"cat_pipeline\", cat_pipeline),\n    (\"num_pipeline\", num_pipeline)\n \n    ])","92040b6c":"#create the dummies for the other categorical variables to apply transformation.\n\nfeatures = ['gender','language','signup_method','first_affiliate_tracked', 'signup_app','first_browser']\n\nfor f in features:\n    df_total_dummy = pd.get_dummies(df_total[f], prefix=f)\n    df_total = df_total.drop([f], axis=1)\n    df_total = pd.concat((df_total, df_total_dummy), axis=1)\n\n# Now splitting train and test\nx = df_total[:train_flag]\nx_test = df_total[train_flag:]","526b66c5":"final_train_X = full_pipeline.fit_transform(x)\nfinal_test_X = full_pipeline.transform(x_test)\nle = LabelEncoder()\ntrain_set_y = le.fit_transform(labels)","085156f6":"forest_class = RandomForestClassifier(random_state = 42)\n\nn_estimators = [100, 500]\nmin_samples_split = [10, 20]\n\nparam_grid_forest = {'n_estimators' : n_estimators, 'min_samples_split' : min_samples_split}\n\n\nrand_search_forest = GridSearchCV(forest_class, param_grid_forest, cv = 4, refit = True,\n                                 n_jobs = -1, verbose=2)\n\nrand_search_forest.fit(final_train_X, train_set_y)","4b55d735":"random_estimator = rand_search_forest.best_estimator_\n\ny_pred_random_estimator = random_estimator.predict_proba(final_train_X)","2ae90f4a":"y_pred = random_estimator.predict_proba(final_test_X) \n\n# We take the 5 highest probabilities for each person\nids = []  #list of ids\ncts = []  #list of countries\nfor i in range(len(id_test)):\n    idx = id_test[i]\n    ids += [idx] * 5\n    cts += le.inverse_transform(np.argsort(y_pred[i])[::-1])[:5].tolist()\n\n# Generating a csv file with the predictions \nsub = pd.DataFrame(np.column_stack((ids, cts)), columns=['id', 'country'])\nsub.to_csv('output_randomForest.csv',index=False)","80036994":"One Hot Encoding for Characters","21745b07":"Model Development"}}