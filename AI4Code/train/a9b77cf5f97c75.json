{"cell_type":{"fc2ffbc0":"code","e17d8c49":"code","7cba796e":"code","91e28d56":"code","670d4321":"code","c924ec9d":"code","277cff7e":"code","ca05419f":"code","0298be7e":"code","4444edba":"code","103d08a7":"code","3178360e":"code","6f430a59":"code","36b7a539":"code","bd2d0ed6":"code","14041f4c":"code","569e483a":"code","2d90482a":"code","1cb26241":"code","c5670040":"code","678f2607":"code","457a389c":"code","c0dafd6c":"code","245e6c61":"code","47b54c63":"code","1c3347b3":"code","5c6d0dcf":"code","d55ba47a":"code","c9024e2e":"code","d67f9e03":"code","43300749":"code","4e62145f":"code","a446c583":"code","d10fb143":"code","e0b86858":"code","849968ae":"code","bfd945db":"code","71286d10":"code","9baae8b1":"code","02720586":"code","120d89f1":"code","f68c5bb9":"code","aa3302fd":"code","a801c896":"code","2651f038":"markdown","b731ddd9":"markdown","eb830206":"markdown","5a09e285":"markdown","be79251d":"markdown","a615b277":"markdown","55f39091":"markdown","076b00d9":"markdown","fae436a1":"markdown","052f94ec":"markdown","9a5a8a11":"markdown","eb14f997":"markdown","163a8355":"markdown","59b82166":"markdown","29bb33b5":"markdown","14fff19b":"markdown","96b60a8d":"markdown","e74766e1":"markdown"},"source":{"fc2ffbc0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e17d8c49":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","7cba796e":"dirname = \"..\/input\/kakr-4th-competition\"","91e28d56":"raw_train = pd.read_csv(os.path.join(dirname, 'train.csv'), index_col='id')\nraw_test = pd.read_csv(os.path.join(dirname, 'test.csv'), index_col='id')","670d4321":"# \ub370\uc774\ud130 \uc0b4\ud3b4\ubcf4\uae30\nraw_train.head()","c924ec9d":"# \ub370\uc774\ud130 \uad6c\uc870\nprint('Rows: {} Columns: {}'.format(raw_train.shape[0], raw_train.shape[1]))","277cff7e":"# \ub370\uc774\ud130 \ud0c0\uc785\nraw_train.info()","ca05419f":"# \ud1b5\uacc4\uc801 \uc694\uc57d\nraw_train.describe().T","0298be7e":"# null \uac12\uc774 \uc874\uc7ac\ud558\ub294\uc9c0 \ud655\uc778\nround((raw_train.isnull().sum() \/ raw_train.shape[0]) * 100, 2).astype(str) + ' %'","4444edba":"# '?'\uac12\uc774 \uc5bc\ub9c8\ub098 \ub098\ud0c0\ub098\ub294\uc9c0\nround((raw_train.isin(['?']).sum() \/ raw_train.shape[0])\n      * 100, 2).astype(str) + ' %'","103d08a7":"# \uc18c\ub4dd \ube44\uc728\nincome = raw_train['income'].value_counts(normalize=True)\nround(income * 100, 2).astype('str') + ' %'","3178360e":"# Creating a barplot for 'Income'\nincome = raw_train['income'].value_counts()\n\nplt.style.use('seaborn-whitegrid')\nplt.figure(figsize=(7, 5))\nsns.barplot(income.index, income.values, palette='bright')\nplt.title('Distribution of Income', fontdict={\n          'fontname': 'Monospace', 'fontsize': 20, 'fontweight': 'bold'})\nplt.xlabel('Income', fontdict={'fontname': 'Monospace', 'fontsize': 15})\nplt.ylabel('Number of people', fontdict={\n           'fontname': 'Monospace', 'fontsize': 15})\nplt.tick_params(labelsize=10)\nplt.show()","6f430a59":"# Creating a distribution plot for 'Age'\nage = raw_train['age'].value_counts()\n\nplt.figure(figsize=(10, 5))\nplt.style.use('fivethirtyeight')\nsns.distplot(raw_train['age'], bins=20)\nplt.title('Distribution of Age', fontdict={\n          'fontname': 'Monospace', 'fontsize': 20, 'fontweight': 'bold'})\nplt.xlabel('Age', fontdict={'fontname': 'Monospace', 'fontsize': 15})\nplt.ylabel('Number of people', fontdict={\n           'fontname': 'Monospace', 'fontsize': 15})\nplt.tick_params(labelsize=10)\nplt.show()","36b7a539":"# Creating a barplot for 'Education'\nedu = raw_train['education'].value_counts()\n\nplt.style.use('seaborn')\nplt.figure(figsize=(10, 5))\nsns.barplot(edu.values, edu.index, palette='Paired')\nplt.title('Distribution of Education', fontdict={\n          'fontname': 'Monospace', 'fontsize': 20, 'fontweight': 'bold'})\nplt.xlabel('Number of people', fontdict={\n           'fontname': 'Monospace', 'fontsize': 15})\nplt.ylabel('Education', fontdict={'fontname': 'Monospace', 'fontsize': 15})\nplt.tick_params(labelsize=12)\nplt.show()","bd2d0ed6":"# Creating a barplot for 'Years of Education'\nedu_num = raw_train['education_num'].value_counts()\n\nplt.style.use('ggplot')\nplt.figure(figsize=(10, 5))\nsns.barplot(edu_num.index, edu_num.values, palette='colorblind')\nplt.title('Distribution of Years of Education', fontdict={\n          'fontname': 'Monospace', 'fontsize': 20, 'fontweight': 'bold'})\nplt.xlabel('Years of Education', fontdict={\n           'fontname': 'Monospace', 'fontsize': 15})\nplt.ylabel('Number of people', fontdict={\n           'fontname': 'Monospace', 'fontsize': 15})\nplt.tick_params(labelsize=12)\nplt.show()","14041f4c":"# Creating a pie chart for 'Marital status'\nmarital = raw_train['marital_status'].value_counts()\n\nplt.style.use('default')\nplt.figure(figsize=(10, 7))\nplt.pie(marital.values, labels=marital.index, startangle=10, explode=(\n    0, 0.20, 0, 0, 0, 0, 0), shadow=True, autopct='%1.1f%%')\nplt.title('Marital distribution', fontdict={\n          'fontname': 'Monospace', 'fontsize': 20, 'fontweight': 'bold'})\nplt.legend()\nplt.legend(prop={'size': 7})\nplt.axis('equal')\nplt.show()","569e483a":"# Creating a donut chart for 'Age'\nrelation = raw_train['relationship'].value_counts()\n\nplt.style.use('bmh')\nplt.figure(figsize=(20, 10))\nplt.pie(relation.values, labels=relation.index,\n        startangle=50, autopct='%1.1f%%')\ncentre_circle = plt.Circle((0, 0), 0.7, fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\nplt.title('Relationship distribution', fontdict={\n          'fontname': 'Monospace', 'fontsize': 30, 'fontweight': 'bold'})\nplt.axis('equal')\nplt.legend(prop={'size': 15})\nplt.show()","2d90482a":"# Creating a barplot for 'Sex'\nsex = raw_train['sex'].value_counts()\n\nplt.style.use('default')\nplt.figure(figsize=(7, 5))\nsns.barplot(sex.index, sex.values)\nplt.title('Distribution of Sex', fontdict={\n          'fontname': 'Monospace', 'fontsize': 20, 'fontweight': 'bold'})\nplt.xlabel('Sex', fontdict={'fontname': 'Monospace', 'fontsize': 15})\nplt.ylabel('Number of people', fontdict={\n           'fontname': 'Monospace', 'fontsize': 15})\nplt.tick_params(labelsize=10)\nplt.grid()\nplt.show()","1cb26241":"# Creating a Treemap for 'Race'\nimport squarify\nrace = raw_train['race'].value_counts()\n\nplt.style.use('default')\nplt.figure(figsize=(7, 5))\nsquarify.plot(sizes=race.values, label=race.index, value=race.values)\nplt.title('Race distribution', fontdict={\n          'fontname': 'Monospace', 'fontsize': 20, 'fontweight': 'bold'})\nplt.show()","c5670040":"# Creating a barplot for 'Hours per week'\nhours = raw_train['hours_per_week'].value_counts().head(10)\n\nplt.style.use('bmh')\nplt.figure(figsize=(15, 7))\nsns.barplot(hours.index, hours.values, palette='colorblind')\nplt.title('Distribution of Hours of work per week', fontdict={\n          'fontname': 'Monospace', 'fontsize': 20, 'fontweight': 'bold'})\nplt.xlabel('Hours of work', fontdict={'fontname': 'Monospace', 'fontsize': 15})\nplt.ylabel('Number of people', fontdict={\n           'fontname': 'Monospace', 'fontsize': 15})\nplt.tick_params(labelsize=12)\nplt.show()","678f2607":"# Creating a countplot of income across age\nplt.style.use('default')\nplt.figure(figsize=(20, 7))\nsns.countplot(raw_train['age'], hue=raw_train['income'])\nplt.title('Distribution of Income across Age', fontdict={\n          'fontname': 'Monospace', 'fontsize': 20, 'fontweight': 'bold'})\nplt.xlabel('Age', fontdict={'fontname': 'Monospace', 'fontsize': 15})\nplt.ylabel('Number of people', fontdict={\n           'fontname': 'Monospace', 'fontsize': 15})\nplt.tick_params(labelsize=12)\nplt.legend(loc=1, prop={'size': 15})\nplt.show()","457a389c":"# Creating a countplot of income across education\nplt.style.use('seaborn')\nplt.figure(figsize=(20, 7))\nsns.countplot(raw_train['education'],\n              hue=raw_train['income'], palette='colorblind')\nplt.title('Distribution of Income across Education', fontdict={\n          'fontname': 'Monospace', 'fontsize': 20, 'fontweight': 'bold'})\nplt.xlabel('Education', fontdict={'fontname': 'Monospace', 'fontsize': 15})\nplt.ylabel('Number of people', fontdict={\n           'fontname': 'Monospace', 'fontsize': 15})\nplt.tick_params(labelsize=12)\nplt.legend(loc=1, prop={'size': 15})\nplt.show()","c0dafd6c":"# Creating a countplot of income across years of education\nplt.style.use('bmh')\nplt.figure(figsize=(20, 7))\nsns.countplot(raw_train['education_num'],\n              hue=raw_train['income'])\nplt.title('Income across Years of Education', fontdict={\n          'fontname': 'Monospace', 'fontsize': 20, 'fontweight': 'bold'})\nplt.xlabel('Years of Education', fontdict={\n           'fontname': 'Monospace', 'fontsize': 15})\nplt.ylabel('Number of people', fontdict={\n           'fontname': 'Monospace', 'fontsize': 15})\nplt.tick_params(labelsize=12)\nplt.legend(loc=1, prop={'size': 15})\nplt.savefig('bi2.png')\nplt.show()","245e6c61":"# Creating a countplot of income across Marital Status\nplt.style.use('seaborn')\nplt.figure(figsize=(20, 7))\nsns.countplot(raw_train['marital_status'], hue=raw_train['income'])\nplt.title('Income across Marital Status', fontdict={\n          'fontname': 'Monospace', 'fontsize': 20, 'fontweight': 'bold'})\nplt.xlabel('Marital Status', fontdict={\n           'fontname': 'Monospace', 'fontsize': 15})\nplt.ylabel('Number of people', fontdict={\n           'fontname': 'Monospace', 'fontsize': 15})\nplt.tick_params(labelsize=12)\nplt.legend(loc=1, prop={'size': 15})\nplt.show()","47b54c63":"# Creating a countplot of income across race\nplt.style.use('fivethirtyeight')\nplt.figure(figsize=(20, 7))\nsns.countplot(raw_train['race'], hue=raw_train['income'])\nplt.title('Distribution of income across race', fontdict={\n          'fontname': 'Monospace', 'fontsize': 20, 'fontweight': 'bold'})\nplt.xlabel('Race', fontdict={\n           'fontname': 'Monospace', 'fontsize': 15})\nplt.ylabel('Number of people', fontdict={\n           'fontname': 'Monospace', 'fontsize': 15})\nplt.tick_params(labelsize=12)\nplt.legend(loc=1, prop={'size': 15})\nplt.show()","1c3347b3":"# Creating a countplot of income across sex\nplt.style.use('fivethirtyeight')\nplt.figure(figsize=(7, 3))\nsns.countplot(raw_train['sex'], hue=raw_train['income'])\nplt.title('Distribution of income across sex', fontdict={\n          'fontname': 'Monospace', 'fontsize': 20, 'fontweight': 'bold'})\nplt.xlabel('Sex', fontdict={\n           'fontname': 'Monospace', 'fontsize': 15})\nplt.ylabel('Number of people', fontdict={\n           'fontname': 'Monospace', 'fontsize': 15})\nplt.tick_params(labelsize=12)\nplt.legend(loc=1, prop={'size': 10})\nplt.savefig('bi3.png')\nplt.show()","5c6d0dcf":"# Creating a pairplot of dataset\nsns.pairplot(raw_train)\nplt.savefig('multi1.png')\nplt.show()","d55ba47a":"corr = raw_train.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(7, 5))\n    ax = sns.heatmap(corr, mask=mask, vmax=.3, square=True,\n                     annot=True, cmap='RdYlGn')\nplt.savefig('multi2.png')\nplt.show()","c9024e2e":"pip install pycaret","d67f9e03":"# pycaret \ucd08\uae30\ud654\nfrom pycaret.classification import *\nclf = setup(data=raw_train, session_id=999, high_cardinality_features=['native_country'], target='income',\n            use_gpu=False, silent=True, fix_imbalance=False, normalize=True, feature_selection=False)","43300749":"# \uc9c0\uc815\ub41c \ubaa8\ub378 \ubaa9\ub85d\uc5d0\uc11c \uac00\uc7a5 \uc131\ub2a5\uc774 \uc88b\uc740 5\uac1c\uc758 \ubaa8\ub378\uc744 \ucd94\ub824\ub0c4\nbest_5 = compare_models(n_select=5, include=['lightgbm', 'xgboost', 'gbc', 'rf', 'ada', 'et'])","4e62145f":"# 5\uac1c\uc758 \ubaa8\ub378\uc744 \uc559\uc0c1\ube14\ud574\uc11c \uc131\ub2a5\uc744 \ub354 \uac1c\uc120\ud558\uace0\uc790\ud568\nblended = blend_models(estimator_list=best_5, fold=5, method='auto')","a446c583":"# \uc55e\uc11c kfold\ub85c \ud6c8\ub828\ud588\uc744\ub54c \uac00\uc7a5 best\uc600\ub358 parameter\ub97c \uae30\uc900\uc73c\ub85c train data\ub97c \uc804\uccb4 \ub2e4\uc0ac\uc6a9\ud574\uc11c \ucd5c\uc885 \ud559\uc2b5\nfinal = finalize_model(blended)","d10fb143":"# test set\uc5d0 \ub300\ud574\uc11c \ubaa8\ub378 \ud3c9\uac00\nprediction_test = predict_model(final, data=raw_test)","e0b86858":"\ndef preprocess(df):\n    # null\uac12\uc774 count \ucd9c\ub825\n    print(df.apply(lambda x: sum(x.isnull()), axis=0))\n    print(' ')\n    \n    # income column\uc744 string\uc5d0\uc11c integer\ub85c \ubcc0\uacbd\n    df['income_level'] = np.where(df.income == '<=50K', 0, 1)\n\n    # \uc131\ubcc4\n    df['gender'] = df['sex'].map({'Male': 0, 'Female': 1}).astype(int)\n\n    \n    # \uc778\uc885\n    ethnicity_key = {'White': 0, 'Black': 1, 'Asian-Pac-Islander': 2,\n                     'Amer-Indian-Eskimo': 3, 'Other': 4}\n\n    df['ethnicity'] = df['race'].map(ethnicity_key).astype(int)\n\n    # \uad6d\uac00\n    origin_key = {'?': 0, 'United-States': 1, 'Mexico': 2, 'Philippines': 3,\n                  'Germany': 4, 'Canada': 5, 'Puerto-Rico': 6, 'El-Salvador': 7,\n                  'India': 8, 'Cuba': 9, 'England': 10, 'Jamaica': 11, 'South': 12,\n                  'China': 13, 'Italy': 14, 'Dominican-Republic': 15, 'Vietnam': 16,\n                  'Guatemala': 17, 'Japan': 18, 'Poland': 19, 'Columbia': 20, 'Taiwan': 21,\n                  'Haiti': 22, 'Iran': 23, 'Portugal': 24, 'Nicaragua': 25, 'Peru': 26,\n                  'France': 27, 'Greece': 28, 'Ecuador': 29, 'Ireland': 30, 'Hong': 31,\n                  'Trinadad&Tobago': 32, 'Cambodia': 33, 'Laos': 34, 'Thailand': 35,\n                  'Yugoslavia': 36, 'Outlying-US(Guam-USVI-etc)': 37, 'Hungary': 38,\n                  'Honduras': 39, 'Scotland': 40, 'Holand-Netherlands': 41}\n\n    df['native_country'] = df['native_country'].map(origin_key).astype(int)\n\n    # \uace0\uc6a9\ud615\ud0dc\n    work_key = {'Private': 0, 'Self-emp-not-inc': 1, 'Local-gov': 2, '?': 3,\n                'State-gov': 4, 'Self-emp-inc': 5, 'Federal-gov': 6,\n                'Without-pay': 7, 'Never-worked': 8}\n\n    df['work'] = df['workclass'].map(work_key).astype(int)\n\n    # \uacb0\ud63c\uc0c1\ud0dc\n    marital_status_key = {'Married-civ-spouse': 0, 'Never-married': 1, 'Divorced': 2,\n                          'Separated': 3, 'Widowed': 4, 'Married-spouse-absent': 5,\n                          'Married-AF-spouse': 6}\n\n    df['marital_status'] = df['marital_status'].map(marital_status_key).astype(int)\n\n    # \uc5c5\uc885\n    occupation_key = {'Prof-specialty': 0, 'Craft-repair': 1, 'Exec-managerial': 2,\n                      'Adm-clerical': 3, 'Sales': 4, 'Other-service': 5,\n                      'Machine-op-inspct': 6, '?': 7, 'Transport-moving': 8,\n                      'Handlers-cleaners': 9, 'Farming-fishing': 10, 'Tech-support': 11,\n                      'Protective-serv': 12, 'Priv-house-serv': 13, 'Armed-Forces': 14}\n\n    df['occupation'] = df['occupation'].map(occupation_key).astype(int)\n\n    # \uac00\uc871\uad00\uacc4\n    relationship_key = {'Husband': 0, 'Not-in-family': 1, 'Own-child': 2, 'Unmarried': 3,\n                        'Wife': 4, 'Other-relative': 5}\n\n    df['relationship'] = df['relationship'].map(relationship_key).astype(int)\n\n    # raw column \uc0ad\uc81c\n    df = df.drop(['income'], axis=1)\n    df = df.drop(['sex'], axis=1)\n    df = df.drop(['race'], axis=1)\n    # df = df.drop(['native.country'], axis=1)\n    df = df.drop(['workclass'], axis=1)\n    # df = df.drop(['marital.status'], axis=1)\n    df = df.drop(['education'], axis=1)\n    # dummy = pd.get_dummies(df['education'], prefix='education')\n    # del df['education']\n    # df = pd.concat([df, dummy], axis=1)\n    # df = df.drop(['education_num'], axis=1)\n\n    # \uc8fc\ub2f9 \uadfc\ubb34 \uc2dc\uac04\n    df['hours_per_week'] = df['hours_per_week'].astype(int)\n    # df.loc[df['hours_per_week'] < 40, 'hours_per_week'] = 0\n    # df.loc[df['hours_per_week'] == 40, 'hours_per_week'] = 1\n    # df.loc[df['hours_per_week'] > 40, 'hours_per_week'] = 2\n\n    # \uc591\ub3c4\uc18c\ub4dd\ucc28\n    df['capital_diff'] = df['capital_gain'] - df['capital_loss']\n    #df['fnlwgt_log'] = np.log(df['fnlwgt'])\n    #df['education_num'] \/= 10\n    # df['age_log'] = np.log(df['age'])\n    #del df['fnlwgt']\n    # del df['native_country']\n\n    return df","849968ae":"# \ub370\uc774\ud130 \uc804\ucc98\ub9ac\nall_data = pd.concat([raw_train, raw_test])\nall_data = preprocess(all_data)\ntrain = all_data.iloc[:len(raw_train)]\ntest = all_data.iloc[len(raw_train):]\n\ntrain_x = train.drop(['income_level'], axis=1)\ntrain_y = train['income_level']\n\ntest_x = test.drop(['income_level'], axis=1)","bfd945db":"from catboost import CatBoostClassifier","71286d10":"prediction = np.zeros(len(test_x))\nlearning_params = [ \n    {\n    \"learning_rate\": 0.2, \n    \"iterations\": 212, \n    \"depth\": 4, \n    \"l2_leaf_reg\": 3, \n    \"random_seed\": 62,\n    \"random_strength\": 1, \n    \"eval_metric\": 'Accuracy'\n    },\n    {\n    \"learning_rate\": 0.2, \n    \"iterations\": 273, \n    \"depth\": 4, \n    \"l2_leaf_reg\": 3, \n    \"random_seed\": 8,\n    \"random_strength\": 1, \n    \"eval_metric\": 'Accuracy'\n    },\n    {\n    \"learning_rate\": 0.2, \n    \"iterations\": 277, \n    \"depth\": 4, \n    \"l2_leaf_reg\": 3, \n    \"random_seed\": 145,\n    \"random_strength\": 1, \n    \"eval_metric\": 'Accuracy'\n    }]\nfor param in learning_params:\n    model = CatBoostClassifier(**param)\n    model.fit(train_x, train_y, verbose=False)\n    prediction += model.predict(test_x)\n\nprediction = prediction \/ 3\nprediction[prediction < 1] = 0\nprediction = prediction.astype(np.int64)","9baae8b1":"from sklearn.metrics import accuracy_score","02720586":"# \ud559\uc2b5\uc14b\uc758 \uc131\ub2a5\ntrain_prediction = model.predict(train_x)\naccuracy_score(train_y, train_prediction)","120d89f1":"# \uc608\uce21 \uc810\uc218\uac00 \ub0ae\uc740 \uacb0\uacfc\ub4e4\uc758 \uc815\ud655\ub3c4\ntrain_score_list = model.predict_proba(train_x)\nlow_score_indexes = np.where(np.logical_and(train_score_list[:,0] < 0.55, train_score_list[:,0] > 0.45))[0]\naccuracy_score(train_y[low_score_indexes], train_prediction[low_score_indexes])","f68c5bb9":"score_list = model.predict_proba(test_x)","aa3302fd":"score_ranges = [\n    (0.55, 0.50, 999, 30), # label 1 min\n    (0.50, 0.45, 2000000, 9)  # label 0\n]\nfor up, down, seed, random_range in score_ranges:\n    indexes = np.where(np.logical_and(score_list[:,0] < up, score_list[:,0] > down))[0]\n    np.random.seed(seed)\n    rand_val = np.random.randint(0,random_range,size=len(indexes))\n    rand_threshold = np.random.randint(1, random_range -1)\n    rand_val[rand_val<rand_threshold] = 0;\n    rand_val[rand_val>=rand_threshold] = 1\n    prediction[indexes] = rand_val","a801c896":"submission = pd.read_csv(os.path.join(dirname, 'sample_submission.csv'))\nsubmission['prediction'] = prediction\nsubmission.to_csv('submission.csv', index=False)\n","2651f038":"## Step 4: \uc804\ucc98\ub9ac","b731ddd9":"* \ud55c\uad6d\uacfc \ub9c8\ucc2c\uac00\uc9c0\ub85c \ubbf8\uad6d\ub3c4 \uc8fc\uae30\uc801\uc73c\ub85c \uc131\uc778\uc744 \ub300\uc0c1\uc73c\ub85c \ud55c \uc5ec\ub7ec \uc778\uad6c\uc870\uc0ac\ub97c \uc2dc\ud589\ud569\ub2c8\ub2e4.\n* \uc774 \ub300\ud68c\ub294 1994\ub144 \ubbf8\uad6d \uc131\uc778\uc744 \ub300\uc0c1\uc73c\ub85c \uc870\uc0ac\ud55c \ub370\uc774\ud130\ub97c \ubc14\ud0d5\uc73c\ub85c \uc9c4\ud589\ub429\ub2c8\ub2e4.\n\n* \uc5ec\ub7ec\ubd84\uc740 \uc774 \ub370\uc774\ud130\uc5d0\uc11c \uac01 \uc0ac\ub78c\uc758 \uc18c\ub4dd\uc744 \uc608\uce21\ud558\uba74 \ub429\ub2c8\ub2e4.\n\n* \ub098\uc774, \uacb0\ud63c \uc5ec\ubd80, \uc9c1\uc885 \ub4f1 \ucd1d 14\uac1c\uc758 feature\ub97c \ud1b5\ud574 \uc608\uce21\uc744 \ud558\uba74 \ub429\ub2c8\ub2e4.\n* \uc608\uce21\ud574\uc57c \ud558\ub294 \uac12\uc740 \uac04\ub2e8\ud569\ub2c8\ub2e4.\n\n* \uc5f0\uc18c\ub4dd\uc774 $50,000 \uc774 \ub118\ub294\ub2e4\uba74 1\n\n* \uc5f0\uc18c\ub4dd\uc774 $50,000 \uc774 \ub118\uc9c0 \uc54a\ub294\ub2e4\uba74 0\n\n* \uc9c0\uae08\uacfc \uae08\uc561\uc758 \uac00\uce58\uac00 \ub2e4\ub974\uaca0\uc9c0\ub9cc \ucd5c\ub300\ud55c \uc5ec\ub7ec\ubd84\uc758 \uc778\uc0ac\uc774\ud2b8\ub97c \ubc14\ud0d5\uc73c\ub85c \uc815\ud655\ud558\uac8c \uc608\uce21\ud558\ub294 \ubaa8\ub378\uc744 \ub9cc\ub4e4\uc5b4\ubd05\uc2dc\ub2e4.\n\n","eb830206":"> ## Step 3: Pycaret \ubaa8\ub378\ub9c1","5a09e285":"![image.png](attachment:image.png)","be79251d":"![image.png](attachment:image.png)","a615b277":"* model\uc758 \uc608\uce21\uce58\ub97c 0.55 ~ 0.50, 0.45 ~ 0.50 2\ubd84\ub958\ub85c \ub098\ub220\uc11c \uc784\uc758\ub85c \uc9c0\uc815\ud558\uace0 submission \uc81c\ucd9c\uc744 \ud1b5\ud574\uc11c best random\uac12\uc744 \ucc3e\ub294\ub2e4.","55f39091":"## Step 1: \uae30\ucd08 \ub370\uc774\ud130 \ud30c\uc545","076b00d9":"## Step 6: \ubb34\uc791\uc704","fae436a1":"* pycaret\uc740 \uc785\ub825 \ub370\uc774\ud130\uc758 \uc804\ucc98\ub9ac\ub97c \ub0b4\ubd80\uc5d0\uc11c \uc790\ub3d9\uc73c\ub85c \ucc98\ub9ac\ud574\uc8fc\ub294 \ubd80\ubd84\uc774 \uc788\ub2e4.\n* \uc5ec\uae30\uc11c\ubd80\ud130\ub294 pycaret\uc744 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uace0 \uc804\ucc98\ub9ac\ubd80\ud130 \ubaa8\ub378 \ud30c\ub77c\uba54\ud130 \uc870\uc815\uae4c\uc9c0 \uc9c1\uc811 handling\ud558\uace0\uc790 \ud55c\ub2e4.","052f94ec":"## Step 7: \uc81c\ucd9c","9a5a8a11":"## PyCaret\uc740 \ubb34\uc5c7\uc785\ub2c8\uae4c?\n\n* PyCaret\uc740 Python\uc758 \uc624\ud508 \uc18c\uc2a4 \ub85c\uc6b0 \ucf54\ub4dc \uba38\uc2e0\ub7ec\ub2dd \ub77c\uc774\ube0c\ub7ec\ub9ac\ub85c, ML \uc2e4\ud5d8\uc5d0\uc11c \uac00\uc124\uc744 \uc778\uc0ac\uc774\ud2b8\uc8fc\uae30 \uc2dc\uac04\uc73c\ub85c \uc904\uc774\ub294 \uac83\uc744 \ubaa9\ud45c\ub85c\ud569\ub2c8\ub2e4.\n* \uc774\ub97c \ud1b5\ud574 \ub370\uc774\ud130 \uacfc\ud559\uc790\ub294 \uc885\ub2e8 \uac04 \uc2e4\ud5d8\uc744 \ube60\ub974\uace0 \ud6a8\uc728\uc801\uc73c\ub85c \uc218\ud589 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n* \ub2e4\ub978 \uc624\ud508 \uc18c\uc2a4 \uae30\uacc4 \ud559\uc2b5 \ub77c\uc774\ube0c\ub7ec\ub9ac\uc640 \ube44\uad50\ud558\uc5ec PyCaret\uc740 \ucf54\ub4dc \uba87 \uc904\ub9cc\uc73c\ub85c \ubcf5\uc7a1\ud55c \uae30\uacc4 \ud559\uc2b5 \uc791\uc5c5\uc744 \uc218\ud589\ud558\ub294 \ub370 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \ub300\uccb4 \ub85c\uc6b0 \ucf54\ub4dc \ub77c\uc774\ube0c\ub7ec\ub9ac\uc785\ub2c8\ub2e4.\n* PyCaret\uc740 \uac04\ub2e8\ud558\uace0 \uc0ac\uc6a9\ud558\uae30 \uc27d\uc2b5\ub2c8\ub2e4.\n* PyCaret\uc5d0\uc11c \uc218\ud589\ub418\ub294 \ubaa8\ub4e0 \uc791\uc5c5 \uc740 \ubc30\ud3ec\ub97c \uc704\ud574 \uc644\uc804\ud788 \uc870\uc815 \ub41c \uc0ac\uc6a9\uc790 \uc9c0\uc815 \ud30c\uc774\ud504 \ub77c\uc778 \uc5d0 \uc790\ub3d9\uc73c\ub85c \uc800\uc7a5\ub429\ub2c8\ub2e4.\n* PyCaret\uc740 \ubcf8\uc9c8\uc801\uc73c\ub85c scikit-learn, XGBoost, LightGBM, spaCy \ub4f1\uacfc \uac19\uc740 \uc5ec\ub7ec \uae30\uacc4 \ud559\uc2b5 \ub77c\uc774\ube0c\ub7ec\ub9ac \ubc0f \ud504\ub808\uc784 \uc6cc\ud06c\ub97c \ub458\ub7ec\uc2fc Python \ub798\ud37c\uc785\ub2c8\ub2e4.\n\n* https:\/\/pycaret.org\/","eb14f997":"## Step 5: CatBoost \ubaa8\ub378\ub9c1","163a8355":"## Step 2: \ub370\uc774\ud130 \ubd84\uc11d \uc2dc\uc791","59b82166":"EDA\ub294 \ub9ce\uc740\ubd84\ub4e4\uc774 \uc798 \uc791\uc131 \ud574 \uc8fc\uc2e0 \ub0b4\uc6a9\uc774 \uc788\uc5b4\uc11c \uc5ec\ub7ec\uac00\uc9c0 \ub370\uc774\ud130\ub97c \ucc38\uace0 \ud588\uc9c0\ub9cc\n\ucd5c\uc885\uc801\uc73c\ub85c \ub2e4\uc74c \uc800\uc790\uc758 EDA\uac00 \ud070 \ub3c4\uc6c0\uc774 \ub418\uc5c8\uc2b5\ub2c8\ub2e4.\n\nhttps:\/\/github.com\/Aditya-Mankar\/Census-Income-Prediction","29bb33b5":"* \uc55e\uc11c \uc0ac\uc6a9\ud588\ub358 pycaret\uc774 \uc88b\uc740 \ubaa8\ub378\ub9c1 \ub3c4\uad6c\uc774\uae34\ud558\ub098 \ubbf8\uc138\ud55c \ud30c\ub77c\uba54\ud130 \uc870\uc815\uc774 \uc5b4\ub835\ub2e4.\n* pycaret\uc5d0\uc11c \uac00\uc7a5 \uc88b\uc740 \uc218\uce58\ub97c \ubcf4\uc600\ub358 catboost\ub97c \uc9c1\uc811 \ud638\ucd9c\ud574\uc11c \ud30c\ub77c\uba54\ud130 \uc870\uc815\uc744 \uac70\uce5c\ub2e4.(submission \uc81c\ucd9c\uc744 \ud1b5\ud574\uc11c)","14fff19b":"* \uc544\ubb34\ub9ac \uc798 \ubaa8\ub378\ub9c1\uc744 \ud558\ub354\ub77c\ub3c4 \uacbd\uacc4\uc120 \uadfc\ucc98\uc5d0 \uc788\ub294 \uc788\ub294 sample\uc740 \ubd84\ub958 \uc131\ub2a5\uc774 \ub5a8\uc5b4\uc9c8\uc218\ubc16\uc5d0 \uc5c6\ub2e4.\n* Label-1: 1.0 ~ 0.5\n* Label-0: 0.0 ~ 0.5\n* \uc608\uce21 \uc810\uc218\uac00 0.55 ~ 0.45 \uc778 sample\uc758 \uacbd\uc6b0, \uc989 \uacbd\uacc4\uc120 \uadfc\ucc98\uc5d0 \uc788\ub294 \uacbd\uc6b0 \uc815\ud655\ub3c4\uac00 \ub9e4\uc6b0 \ub0ae\ub2e4. \ubb34\uc791\uc704\ub85c \uc8fc\uc0ac\uc704\ub97c \ub358\uc9c0\ub294 \uc218\uc900\uc758 \uc815\ud655\ub3c4","96b60a8d":"## \ub370\uc774\ud130 \uc815\ubcf4\n* age : \ub098\uc774\n* workclass : \uace0\uc6a9 \ud615\ud0dc\n* fnlwgt : \uc0ac\ub78c \ub300\ud45c\uc131\uc744 \ub098\ud0c0\ub0b4\ub294 \uac00\uc911\uce58 (final weight\uc758 \uc57d\uc790)\n* education : \uad50\uc721 \uc218\uc900 (\ucd5c\uc885 \ud559\ub825)\n* education_num : \uad50\uc721 \uc218\uc900 \uc218\uce58\n* marital_status: \uacb0\ud63c \uc0c1\ud0dc\n* occupation : \uc5c5\uc885\n* relationship : \uac00\uc871 \uad00\uacc4\n* race : \uc778\uc885\n* sex : \uc131\ubcc4\n* capital_gain : \uc591\ub3c4 \uc18c\ub4dd\n* capital_loss : \uc591\ub3c4 \uc190\uc2e4\n* hours_per_week : \uc8fc\ub2f9 \uadfc\ubb34 \uc2dc\uac04\n* native_country : \uad6d\uc801\n* income : \uc218\uc775 (\uc608\uce21\ud574\uc57c \ud558\ub294 \uac12, target variable)","e74766e1":"* \ub300\ud68c \ubaa9\uc801\uc774 \uc544\ub2c8\ub77c\uba74 \uc5ec\uae30\uc11c \ubaa8\ub378\ub9c1\uc740 \uc885\ub8cc\ud588\uc744\uac83\uc774\ub2e4.\n* \uadf8\ub7ec\ub098 \uc2e4\uc81c \ubaa8\ub378\uacfc \ub2e4\ub974\uac8c \ub300\ud68c\uc758 \uacbd\uc6b0 \ud3c9\uac00 \ub370\uc774\ud130\uac00 \uacf5\uac1c \ub418\uc5b4 \uc788\uace0(Data Leakage), \uc77c\ubd80\uc774\uae34 \ud558\uc9c0\ub9cc \uadf8\uc5d0 \ub300\ud55c \ud3c9\uac00 \ub610\ud55c \uac00\ub2a5\ud558\ub2e4\n* \uc5ec\uae30\uc11c\ubd80\ud130\ub294 \ubaa8\ub378\ub9c1\uc2dc \uc911\uc694\ud55c \uc9c0\ud45c\uc778 \uc77c\ubc18\ud654\ub97c \ud3ec\uae30\ud558\uace0, test \ub370\uc774\ud130\uc5d0 overfitting\ud558\ub294 \uacfc\uc815\uc744 \ub2e4\ub8ec\ub2e4."}}