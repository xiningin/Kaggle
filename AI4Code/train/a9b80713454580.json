{"cell_type":{"3387fa8f":"code","d41eba09":"code","fb0de48f":"code","f22abac3":"code","084f6b25":"code","ce25a98a":"code","049e4c38":"code","6b2e9ac6":"code","3088e83e":"code","9f696e4e":"code","a01e7c82":"code","2d7784fd":"code","16127758":"code","540c1a82":"code","554e2c97":"code","ee4a4449":"code","0253658f":"code","f3b963ac":"markdown","23799d12":"markdown","d55f81a2":"markdown","85af0454":"markdown","c8a2c969":"markdown","a3d8ae25":"markdown","ed52e749":"markdown","f40f4eee":"markdown","3cb40dbb":"markdown","fad4c4da":"markdown","a18cfa95":"markdown","7105ce89":"markdown","f3d963ee":"markdown","a3154b1c":"markdown","ec223b02":"markdown","995488b6":"markdown","28b96b92":"markdown","6d23e0f8":"markdown","5a3b5798":"markdown","26b26877":"markdown","d40e36b8":"markdown","4840f7d2":"markdown","e0f8a557":"markdown","8a8e19b5":"markdown","4e501ec5":"markdown","dda538b8":"markdown","7cd50bf5":"markdown","bf345c29":"markdown"},"source":{"3387fa8f":"#import Python libraries used in this lecture\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score, learning_curve, validation_curve\nimport sklearn.model_selection as model_selection\nfrom sklearn import tree\nfrom sklearn import metrics","d41eba09":"df2 = pd.read_csv(\"..\/input\/diabetescsv\/diabetes.csv\") # read data\nX = df2.iloc[:,0:8] #predictors. stop is excluded\ny = df2.iloc[:,8] #target\nX_train,X_test,y_train,y_test = model_selection.train_test_split(X,y,test_size=0.2,random_state=4) # 20% for test data\n\nclf_11 = DecisionTreeClassifier(criterion='entropy',max_depth=4)\nclf_11 = clf_11.fit(X_train,y_train)\ny_pred = clf_11.predict(X_train) #accuracy on training data\nprint(\"Accuracy on training data:\",metrics.accuracy_score(y_train, y_pred))\ny_pred = clf_11.predict(X_test)  #accuracy on  test data\nprint(\"Accuracy on test data:\",metrics.accuracy_score(y_test, y_pred))\n\nfig = plt.figure(figsize=(24,25)) # plot tree\n_ = tree.plot_tree(clf_11, feature_names=df2.columns, class_names =['0','1'], filled=True)","fb0de48f":"clf_12 = DecisionTreeClassifier(criterion='entropy',max_depth=15)\nclf_12 = clf_12.fit(X_train,y_train)\ny_pred = clf_12.predict(X_train) #accuracy on training data\nprint(\"Accuracy on training data:\",metrics.accuracy_score(y_train, y_pred))\ny_pred = clf_12.predict(X_test)  #accuracy on  test data\nprint(\"Accuracy on test data:\",metrics.accuracy_score(y_test, y_pred))\n\nfig = plt.figure(figsize=(24,25))\n_ = tree.plot_tree(clf_12, feature_names=df2.columns, class_names =['0','1'], filled=True)","f22abac3":"#split data into training and test data (80% versus 20%)\nX_train,X_test,y_train,y_test = model_selection.train_test_split(X,y,test_size=0.2,random_state=4) \n#further split traning data into training and validation data (90% versus 10%)\nX_train_new, X_val, y_train_new, y_val = model_selection.train_test_split(X_train, y_train, test_size=0.1, random_state=4)\n#set range of max_depth\nmax_depth_range = range(1, 16) ","084f6b25":"#creat two lists which record results\nval_results = []\ntrain_results = []","ce25a98a":"#calculate the accuracy of decision tree model with max_depth in [1,15] \nfor k in max_depth_range:\n    clf_2 = DecisionTreeClassifier(max_depth=k)\n    clf_2 = clf_2.fit(X_train_new, y_train_new)\n    #accuracy on training\n    pred_train_new = clf_2.predict(X_train_new)\n    train_score = metrics.accuracy_score(y_train_new, pred_train_new) \n    train_results.append(train_score)\n    #accuracy on validation data\n    pred_val = clf_2.predict(X_val) \n    val_score = metrics.accuracy_score(y_val, pred_val) \n    val_results.append(val_score)","049e4c38":"#plot the score curves on both validation and test datasets\nplt.plot(max_depth_range, val_results, 'g-', label='Val score')\nplt.plot(max_depth_range, train_results, 'r-', label='Train score')\nplt.ylabel('Score')\nplt.xlabel('Model complexity: tree depth')\nplt.legend()\nplt.grid(True)\nplt.show()","6b2e9ac6":"clf_best1 = DecisionTreeClassifier(max_depth=2)\nclf_best1 = clf_best1.fit(X_train, y_train)\ny_pred = clf_best1.predict(X_test)\nprint(metrics.accuracy_score(y_test, y_pred))","3088e83e":"#split data into training and test data (80% versus 20%)\nX_train,X_test,y_train,y_test = model_selection.train_test_split(X,y,test_size=0.2,random_state=4) \n# create a decision tree\nclf_22 = DecisionTreeClassifier(criterion='entropy')","9f696e4e":"scores = cross_val_score(clf_22, X_train, y_train, cv=10,scoring=\"accuracy\")\nprint(\"accuracy on validation data\", scores)\nprint(\"mean\", np.mean(scores))","a01e7c82":"def plot_validation_curve(clf, X, y, param_name, param_range):\n    train_scores, test_scores = validation_curve(clf, X, y,\n                                                 cv=10, \n                                                 scoring=\"accuracy\",  \n                                                 param_name=param_name,\n                                                 param_range=param_range,\n                                                 n_jobs=-1)\n    plt.figure(figsize=(6,4))\n    x_range = param_range \n    train_scores_mean =  np.mean(train_scores, axis=1) \n    test_scores_mean = np.mean(test_scores, axis=1) \n    plt.plot(x_range, train_scores_mean, 'g-', label='Training score')\n    plt.plot(x_range, test_scores_mean, 'b-', label='Validation score')\n    plt.ylabel('Score')\n    plt.xlabel('Model complexity: tree depth')\n    plt.text(6, 0.8, 'Overfitting',fontsize=12,color=\"r\")\n    plt.text(1, 0.8, 'Underfitting',fontsize=12,color=\"r\")\n    plt.axvline(4, ls='--') \n    plt.legend()\n    plt.grid(True)\n    plt.show()","2d7784fd":"plot_validation_curve(clf_22, X_train, y_train, param_name='max_depth', param_range=range(1,16))","16127758":"clf_best2 = DecisionTreeClassifier(max_depth=4)\nclf_best2 = clf_best2.fit(X_train, y_train)\ny_pred = clf_best2.predict(X_test)\nprint(metrics.accuracy_score(y_test, y_pred))","540c1a82":"#split data into training and test data (80% versus 20%)\nX_train,X_test,y_train,y_test = model_selection.train_test_split(X,y,test_size=0.2,random_state=4) ","554e2c97":"from sklearn.model_selection import GridSearchCV\n# create Decision Tree classifer object\nclf_31 = DecisionTreeClassifier() \nparam_grid = [{'criterion':['gini'], 'max_depth':list(range(1,16))},\n               {'criterion':['entropy'], 'max_depth':list(range(1,16))}]\n# create a grid search object \ngs = GridSearchCV(clf_31, param_grid, scoring='accuracy', cv=10)                          \n# fit model using grid search\ngs = gs.fit(X_train,y_train)\n \n#set the clf to the best combination of parameters\nclf_best3 = gs.best_estimator_\nprint(\"best model:\", clf_best3.get_params())\n# Fit the best model to the data. \nclf_best3 = clf_best3.fit(X_train, y_train)\n\ny_pred = clf_best3.predict(X_test)\nprint(metrics.accuracy_score(y_test, y_pred))\n","ee4a4449":"#split data into training and test data (80% versus 20%)\nX_train,X_test,y_train,y_test = model_selection.train_test_split(X,y,test_size=0.2,random_state=4) ","0253658f":"from sklearn.model_selection import RandomizedSearchCV\n# create Decision Tree classifer object\nclf_32 = DecisionTreeClassifier() \nparam_grid = [{'criterion':['gini'], 'max_depth':list(range(1,16))},\n               {'criterion':['entropy'], 'max_depth':list(range(1,16))}]\n\ngs = RandomizedSearchCV(clf_32, param_grid, scoring='accuracy', cv=10)                          \n# create a grid search objective\ngs = gs.fit(X_train,y_train)\n\n#set the clf to the best combination of parameters\nclf_best4 = gs.best_estimator_\nprint(\"best model:\", clf_best4.get_params())\n# Fit the best model to the data. \nclf_best4 = clf_best4.fit(X_train, y_train)\n\ny_pred = clf_best4.predict(X_test)\nprint(metrics.accuracy_score(y_test, y_pred))","f3b963ac":"### Overfitting  \n* In overfitting, a model fits too closely or exactly to a particular set of data\n\n|<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/1\/19\/Overfitting.svg\/1200px-Overfitting.svg.png\" width =200 >|\n|:--:|\n|[Overfitting](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/1\/19\/Overfitting.svg\/1200px-Overfitting.svg.png)|\n\n* In the example\n    * green line perfectly fitting training data and classify the data\n    * but too complex and too dependent on this particular data\n    * The green line likely fails to fit new data compared with the black line\n    \n* A model learns too many details and even noise in training data\n* This is not good to generalization on new data ","23799d12":"### Example 3: Plot Validation Curves\n* Dataset: Diabetes dataset (see previous lecture and lab)\n* Classifier: decision tree\n* Split data into two parts: a training set  and a test set.\n* Traning data uses 5-fold cross-validation\n* The range of tree depth is [1,15]","d55f81a2":"### Example 1: Overfitting (Simple Tree)\n* Dataset: Diabetes dataset, available at https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database.\n* Classifier: decision tree\n* We compare the accuracy of the decision tree with different tree depth on both training and test data\n* First, we look at max_depth = 4\n    1. create a tree with max_depth = 4\n    2. calculate the accuracy of the model on both training and test data\n        * accuracy = 0.80 on training data\n        * accuracy = 0.79 on test data\n    3. plot the tree","85af0454":"### Model Selection\n* Since the tree with max_depth = 2 has the highest score on validation data, it will be selected as the final model\n* For big data, validation data may cover many records similar to test data. Holdout method may work well\n* For small data, validation data may not cover few records similar to test data. Holdout method may not work well","c8a2c969":"### validation_curve\n* `sklearn.model_selection.validation_curve`:  determine training and test scores for varying parameter values.  \n* Define a function which plot score curves on both training and test data\n* Test data actually is validation data because training data is split into 10-folds for cross-validation\n* For each tree depth k, we generate 10 models. Thus we use the mean accuracy to evaluate the performance of the decision tree at tree depth = k","a3d8ae25":"### Rerefrence \n1. Sklearn 3.1. Cross-validation: evaluating estimator performance: https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html\n2. Sklearn 3.2. Tuning the hyper-parameters of an estimator https:\/\/scikit-learn.org\/stable\/modules\/grid_search.html   ","ed52e749":"## Part 1 Model Selection and Overfitting\n### What is Model Selection?\n*  Model selection: how to select the final model from potential candidate models based on their performance on the training dataset\n    * select a model from different types of models, e.g. from three classifiers: logistic regression, decision tree and KNN\n    * select a model from the same model with different model hyperparameters, e.g., from the decision tree model with different tree depths\n* Model evaluation: how to evaluate the performance of a candidate models using certain metrics, e.g., accuracy  \n\n![image.png](attachment:image.png)","f40f4eee":"### Dataset for Tuning Hyperparameter\n* On the test dataset\n    * Test dataset is regarded as new data to a model\n    * A model cannot see class labels of test data. In Kaggle competitions, class labels of test data are set unseen to participants \n    * Thus, it is impossible to tune parameters on the test dataset\n    * In other words, hyperparameter tuning in our previous labs is wrong\n* On the training dataset\n    * A hyperparameter must be tuned on training data set\n    * However, this is bad practice because it leads to overfitting","3cb40dbb":"## Part 2 Manually Tuning Hyperparameter\n### Holdout method: training, validation and test datasets\n* Separate the data into three parts\n    * Training dataset: used to find the best internal parameter of a model \n    * Validation dataset: used for model selection and hyperparameter tuning \n    * Test dataset: used to evaluate the performance of a model\n* Selection might be different depending on the way you split dataset \n\n![image.png](attachment:image.png)","fad4c4da":"### Example 4: GridSearchCV (brute-force exhaustive search )\n* Dataset: Diabetes dataset (see previous lecture and lab)\n* Classifier: decision tree\n* Hyperparameters\n    * criterion:  this parameter is to measure the quality of a split. There are two options\n        * `gini` for the Gini impurity\n        * `entropy` for the information gain\n    * `max_depth`:  set its range  [1,15]\n    * Total $2 \\times 15 = 30$ combinations\n","a18cfa95":"### Example 1: Overfitting (Complex Tree)\n* Secondly, we look at max_depth =15 \n    1. create a tree with max_depth = 15\n    2.  Check the accuracy of the model on training and test data\n        * accuracy = 1 on training data\n        * accuracy = 0.71 on test data, worse than the tree with max_depth =4\n    3.  Plot the tree\n        * Very complex tree","7105ce89":"### Sklearn implementation\n* In Sklearn, no direct method to split data into three parts\n    1. a training set\n    2. a validation set\n    3. a test set. \n* A solution is to apply `train_test_split` twice  \n    1. split data into two parts: a training data set and a test data set \n    2. split training data further into two parts:  training and validation data sets","f3d963ee":"### Parameter and Hyperparameter \n* A machine learning model always has a few parameters. Some deep learning models even have over hundred parameters\n* Parameters of a model can be classified into two types:\n    1. **parameter**: internal coefficients or weights for a model \n        * e.g., Logistic regression: coefficients ($\\beta_0, \\cdots, \\beta_n$) \n        $$ \\pi(x) = \\frac{e^{\\beta_0 +\\beta_1 x_1+ \\cdots + \\beta_n x_n}}{1+e^{\\beta_0 +\\beta_1 x+\\cdots+\\beta_n x_n}}$$\n        * learned in model fitting\n        * fitting is to find optimal internal model parameters  \n\n    2. **hyperparameter**: specified by practitioner when configuring the model\n        * e.g. decision tree: depth of a tree \n        * tuned by practitioner \n* Tuning hyper-parameters may significantly improve the performance of a machine learning model \n* Manual tuning:  use the accuracy curve or other curves to select a best hyper-parameter\n* Automatic tuning: a more powerful hyper-parameter tuning technique through search","a3154b1c":"### Model Evaluation\n* Evaluate the model on test data","ec223b02":"### Overfitting and Underfitting\n![image.png](attachment:image.png)\n\n* Overfitting \n    * a model performs very well on the training data\n    * but not on the test data\n    * e.g. overfit when the tree depth = 10\n* Underfitting  \n    * a model fits the training data not well\n    * and does not the fit test data very well \n    * e.g. underfit when the tree depth = 1","995488b6":"### Disadvantages in Manual Tuning\n* Tune one hyperparameter at each time but a model may have many hyperparameters\n* Find the optimal value manually through observing a plotting","28b96b92":"# Model Selection and Hyperparameter Tuning\n[COMP20121 Machine Learning for Data Analytics](https:\/\/sites.google.com\/site\/hejunhomepage\/Teaching\/machine-learning-for-data-analytics)\n\nAuthor: Jun He\n\nA machine learning model usually contains parameters. Some deep learning models even have over hundred parameters. Model selection is to select the best model with the best hyperparameters. This can be done manually or automatically. This lecture covers model evaluation and hyperparameter tuning.\n\n* Explain concepts of model selection, overfitting, hyperparameter tuning\n* Introduce two methods used in model selection:  holdout method and k-fold cross-validation\n* Present several examples of manual and automatic tuning of hyperparameters","6d23e0f8":"### k-fold cross-validation\n* In k-fold cross-validation, randomly split the training dataset into k folds without replacement\n    * k \u22121 folds are used for training\n    * 1 fold is used for validation\n* This procedure is repeated k times so that we obtain k models and performance estimates.\n* Advantage:\n    * Each sample point will be part of a training and test dataset exactly once\n\n    \n![image.png](attachment:image.png)","5a3b5798":"### Example 2: Holdout Method for Tuning Hyperparameter \n* Dataset: Diabetes dataset (see previous lecture and lab)\n* Classifier: decision tree\n* Model selection\n    * uses one criterion: accuracy of a classifier\n    * the same model (decision tree) with different values of maximal depth of tree (hyperparameter)\n* Split data into three parts:   training, validation, and test datasets\n* The range of tree depth is [1,15]","26b26877":"### How Many Folds?\n* The more folders, the more computation time \n* Very large data sets: 3-fold cross validation is good choice\n* Very small data sets: use leave-one-out is good choice\n* Common choice\n    * Many researchers use k=10 as this seems to be good choice for many data\n","d40e36b8":"### Considerations in Model Selection \n* In a business project, model selection is not only a technical problem\n* There are many considerations such as \n    * business requirements \n    * cost \n    * performance\n    * state-of-the-art\n    * maintenance\n* It is difficult or impossible to find the best model meeting all criteria. Usually, turn to find a satisfactory model\n* The best model in terms of technical performance may not be the best in business\n    * [Netflix prize](https:\/\/www.kaggle.com\/netflix-inc\/netflix-prize-data) In 2009, 'Netflix held the Netflix Prize open competition for the best algorithm to predict user ratings for films. The grand prize was \\$1,000,000 and was won by BellKor's Pragmatic Chaos team. This is the dataset that was used in that competition'. But this algorithm was never used by Netflix","4840f7d2":"### Model Evaluation\n* Evaluate the model on test data","e0f8a557":"## Summary\n* Model selection is to select the final model from potential candidate models based on their performance on the training dataset\n* Overfitting means that a model fits training data very well but does not fit test data very well. Overfitting often happens in a complex model\n* Tuning hyper-parameters may significantly improve the performance of a machine learning model and avoid overfitting\n* Manual tuning of  hyperparameters can be implemented through the holdout method or k-fold cross-evaluation\n* Automatic Tuning of hyperparameters can be implemented through grid search","8a8e19b5":"### Model Selection\n* Plot both training score curve and validation curve\n* The model with max_depth =4 will be selected","4e501ec5":"### cross_val_score\n* The simplest way to use cross-validation is to call the cross_val_score function which returns the score of the classifier on each fold\n* The following example demonstrates how to estimate the accuracy of decisin tree by splitting the training data, fitting a model and computing the score on folds ","dda538b8":"## Part 3 Automatic Tuning of Hyperparameters\n### Automatic Tuning of hyperparameters: Search\n* Finding an optimal hyperparameter is a search problem\n    * For each different combination of hyperparameters, evalaute the performance of the model and then choose the best one\n* Sklearn provides two methods, called grid search, to automaically tune hyperparamters\n    1. `GridSearchCV` exhaustively generates candidates from a grid of parameter values specified with the `param_grid` parameter\n        * apply a brute-force exhaustive search to different combinations of hyper-parameters   \n        * evaluates the model performance for each combination \n    2. `RandomizedSearchCV` implements a randomized search over parameters\n        * apply randomized search to different combinations of hyper-parameters  \n        * each setting is sampled from a distribution over possible combinations (but not exhaustively) \n* Grid search is based on k-fold cross-evaluation","7cd50bf5":"## Learning objectives\n* What is model selection?\n* What is holdout method? \n* What is k-fold cross-validation?\n* How to tune hyperparameter manually or automatically?","bf345c29":"### Example 5: RandomizedSearchCV (Randomized Search) \n* Dataset: Diabetes dataset (see previous lecture and lab)\n* Classifier: decision tree\n* Hyperparameters\n    * criterion:  this parameter is to measure the quality of a split. There are two options\n        * gini for the Gini impurity\n        * entropy for the information gain\n    * max_depth:  set its range  [1,15]\n    * Total $2 \\times 15 = 30$ combinations"}}