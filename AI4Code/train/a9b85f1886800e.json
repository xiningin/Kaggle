{"cell_type":{"34b22b74":"code","aac44394":"code","924846f5":"code","df22c897":"code","d39d8397":"code","509f6ed4":"markdown","3212decd":"markdown","eb69bc0a":"markdown","490e9cba":"markdown","ccca37b3":"markdown","27524390":"markdown","24266aec":"markdown","295fe03b":"markdown"},"source":{"34b22b74":"# Importing libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport warnings\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, QuantileTransformer\nfrom sklearn.utils import shuffle\nfrom sklearn.ensemble import IsolationForest, RandomForestClassifier\nfrom sklearn.svm import OneClassSVM, SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\n\nfrom collections import Counter\n\npd.set_option('display.max_columns', None)\nwarnings.filterwarnings('ignore')","aac44394":"# Loading dataset\ndf = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\nprint(df.shape)\nprint(df['Class'].value_counts())\ndf.head()","924846f5":"def make_data(dataset, scale = None, ballance = None):\n    \n    '''\n    Creates train and validation datasets using differend scaling and undersampling methods\n    \n    scale - scaling method:\n     - 'standard' for StandardScaler \n     - 'quantile' for QuantileTransformer\n     - None - do not use scaling\n     \n    ballance - method of dataset undersampling:\n     - 'random_smaples' - creates subset of random non fraud_trasactions with number of samples equal to fraud transactions amount\n     - 'outlier elimination' - removing outliers from majority class with IF, then extracting support vectors using OCSVM \n        and then sub-sampling this subset using random samples\n     - None - do not use ballancing\n    '''\n    \n    data = dataset.copy()\n        \n    # Split data to train and validation datasets\n    Y = data['Class']\n    X = data.drop('Class', axis = 1)\n    x, val_x, y, val_y = train_test_split(X, Y, test_size = 0.15, stratify = Y, random_state = 1)\n        \n    # Ballancing data if method is 'random_samples'\n    if ballance == 'random_samples':        \n        x['Class'] = y\n        fraud = x[x['Class'] == 1]        \n        non_fraud = x[x['Class'] == 0]        \n        non_fraud = non_fraud.sample(fraud.shape[0], random_state = 1)        \n        x = pd.concat([fraud, non_fraud])\n        y = x['Class']\n        x = x.drop('Class', axis = 1)\n    \n    # Scaling data\n    if scale:        \n        if scale == 'standard':            \n            scaler = StandardScaler()        \n            x = pd.DataFrame(scaler.fit_transform(x), index = x.index, columns = x.columns)\n            val_x = pd.DataFrame(scaler.transform(val_x), index = val_x.index, columns = val_x.columns)            \n        elif scale == 'quantile':            \n            scaler = QuantileTransformer(output_distribution = 'normal')\n            x = pd.DataFrame(scaler.fit_transform(x), index = x.index, columns = x.columns)\n            val_x = pd.DataFrame(scaler.transform(val_x), index = val_x.index, columns = val_x.columns)            \n        else:\n            raise Exception(f'{scale} is not supported scaling method.\\nUse \"standard\" or \"quantile\" methods.')        \n    \n    # Ballancing data if method is 'outlier_elimination'\n    if ballance == 'outlier_elimination':        \n        x['Class'] = y\n        fraud = x[x['Class'] == 1]        \n        non_fraud = x[x['Class'] == 0]\n        non_fraud.drop('Class', axis = 1, inplace = True)        \n        \n        # Detecting outliers with IF\n        IF = IsolationForest(n_jobs = -1)\n        outliers = IF.fit_predict(non_fraud)\n                \n        # Removing outliers\n        non_fraud['Outliers'] = outliers\n        non_fraud = non_fraud[non_fraud['Outliers'] == 1]\n        non_fraud.drop('Outliers', axis = 1, inplace = True)        \n        \n        # Taking no more than 25000 samples for OCSVM train\n        if non_fraud.shape[0] > 25000:\n            non_fraud = non_fraud.sample(25000, random_state = 1)\n        \n        # Training OCSVM        \n        ocsvm = OneClassSVM(gamma = 'auto', kernel = 'rbf', max_iter = 10)\n        ocsvm.fit(non_fraud)        \n        # Extracting support vectors\n        vectors = pd.DataFrame(ocsvm.support_vectors_, columns = non_fraud.columns)\n        \n        # Ballancing dataset taking random samples from support vectors        \n        non_fraud = vectors.sample(fraud.shape[0], random_state = 1)        \n        non_fraud['Class'] = 0\n        x = pd.concat([fraud, non_fraud])\n        y = x['Class']\n        x = x.drop('Class', axis = 1)\n        \n    elif ballance not in ['random_samples', 'outlier_elimination'] and ballance is not None:\n        raise Exception(f'{ballance} is not supported balancing method.\\nUse \"random_samples\" or \"outlier_elimination\" methods.')\n       \n    x, y = shuffle(x, y)    \n       \n    return (x, val_x, y, val_y)","df22c897":"# Creating datasets for experiments\ndatasets = {\n    'Raw_standard': make_data(df, scale = 'standard'),\n    'Raw_quantile': make_data(df, scale = 'quantile'),\n    'Random_samples_standard': make_data(df, scale = 'standard', ballance = 'random_samples'),\n    'Random_samples_quantile': make_data(df, scale = 'quantile', ballance = 'random_samples'),\n    'Out_elim_standard': make_data(df, scale = 'standard', ballance = 'outlier_elimination'),\n    'Out_elim_quantile': make_data(df, scale = 'quantile', ballance = 'outlier_elimination'),\n           }\n\n# Defining classifiers\nclassifiers = {\n    'SVM': SVC(random_state = 1),\n    'RandomForest': RandomForestClassifier(n_jobs = -1, random_state = 1),\n    'LogisticRegression': LogisticRegression(n_jobs = -1, random_state = 1),\n              }","d39d8397":"# Training classifiers and plotting results\nfor classifier in classifiers:        \n    fig = plt.figure(figsize = (13, 10))\n    \n    for i, key in enumerate(datasets.keys()):\n        print(f'Training {classifier}: {key}')\n        \n        # Train classifier\n        clf = classifiers[classifier]\n        clf.fit(datasets[key][0], datasets[key][2])\n        preds = clf.predict(datasets[key][1])\n        \n        # Create confusin matrix plot\n        plt.subplot(f'23{i+1}')\n        cm = confusion_matrix(datasets[key][-1], preds)\n        sns.heatmap(cm, fmt = 'd', annot = True, square = True, cbar = False, cmap = 'Blues', \n                    xticklabels =  ['P_Non_fraud', 'P_Fraud'], yticklabels = ['Non_fraud', 'Fraud']).set_title(key)\n        \n    print('\\n', classifier)\n    \n    # Show plots\n    plt.show()","509f6ed4":"Final part - training classifiers and plotting results.","3212decd":"Now it's time to create datasets and define our classifiers. I'm using default settings for all classifiers, because I'm only interested in impact of different techniques on results and do not chasing 100% accuracy.","eb69bc0a":"## Goal\n\nI decided to run some experiments to test this techniqe and compare it with other techniques. Also, recently I found [this](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.QuantileTransformer.html) interesting scaler, which makes distribution of our features normal, so I decided to test it here and compare results with StandardScaler.\n\nSo, to be more specific, I want to make next things:\n1. I want to create 6 datasets, on which I will train different models:\n\n * Unballanced dataset, sclaed with StandardScaler\n * Unballanced dataset, scaled with QuantileTransformer \n * Ballanced with random sampling dataset, scaled with StandardScaler\n * Ballanced with random sampling dataset, scaled with QuantileTransformer\n * Ballanced with technique, described in paper dataset, scaled with StandardScaler\n * Ballanced with technique, described in paper dataset, scaled with QuantileTransformer\n \n \n2. Train 3 classifiers on these datasets: SVC, RandomForest, LogisticRegression.\n\n3. Compare results by plotting confusion matrices for each classifier.","490e9cba":"## Conclusions:\n\nSo, in validation dataset we have 42648 non-fraudulent and 74 fraudulent transactions. By training 3 classifiers on 6 datasets we got next results:\n\nDespite of high imballance of Raw datasets, we got relatively good results - about 67-82% fraud transactions were classified correctly, also classifiers, trained on imballanced datasets, predict non_fraud transactions with accuracy very close to 100%.\n\nTested undersampling techniques gave us approximately equal results in predicting fraudulent transactions (about 93%), but outlier_elimination technique gave us more false negative.","ccca37b3":"Let's start.","27524390":"We have only about 0.17% fraudulent transactions in our dataset.","24266aec":"To ease datasets creation I'll make function that will take dataset, scale method and ballance method as input and returns train and test datasets.\n\nThe function supports 2 scaling methods:\n\n* 'standard' - scaling, using StandardScaler\n* 'quantile' - scaling, using QuantileTransformer\n\nAlso the function supports 2 ballancing techniques:\n\n* 'random_samples' - one of the easiest and most common dataset ballancing techniques, it's just takes random samples from majority class in amount, equal to minority class.\n* 'outlier_elimination' - this is my implementation of technique, described in paper, but I made some changes here: in paper, to eliminate outliers rKNN (reversal KNN) algorithm have been used, but here I'm using IsolationForest algorithm instead. Also authors of paper used all support vectors (SV) for training, but in my case if I'll use all SV, then my dataset still will be very imballanced, so I'm taking random samples from SV to create ballanced dataset.","295fe03b":"## Introduction\n\nThis kernel was inspired by [this](https:\/\/www.researchgate.net\/profile\/G_Sundarkumar\/publication\/267628579_A_novel_hybrid_undersampling_method_for_mining_unbalanced_datasets_in_banking_and_insurance\/links\/571f482308aefa64889a1166\/A-novel-hybrid-undersampling-method-for-mining-unbalanced-datasets-in-banking-and-insurance.pdf) paper. Authors of this paper described a very interesting undersampling method for imballanced datasets where first you remove outliers from majority class and then extract support vectors using OCSVM, these support vectors serves as training subset for majority class.\n\nAlso you can check my another kernels on this dataset:\n* [PCA vs TSNE visualisation](https:\/\/www.kaggle.com\/trolukovich\/pca-vs-tsne-visualisation)\n* [IsolatedForest and LOF fraud detection](https:\/\/www.kaggle.com\/trolukovich\/isolatedforest-and-lof-fraud-detection)"}}