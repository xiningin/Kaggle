{"cell_type":{"c924308e":"code","cc7c2f87":"code","79433e4c":"code","0cea341c":"code","0b697562":"code","ef3a9983":"code","e0addbac":"code","f0aa9015":"code","0d43eaab":"code","19df9251":"code","21913b2e":"code","403c869c":"code","6e22ca2d":"code","a5fc6ea3":"code","0b70634d":"code","737f53ae":"code","14fe67dc":"code","99ac7085":"code","29763968":"code","8c8f0499":"markdown"},"source":{"c924308e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom sklearn.model_selection import StratifiedKFold\n\nimport transformers\nimport torch\nfrom tqdm.notebook import tqdm\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","cc7c2f87":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndevice","79433e4c":"train_df = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/train.csv')\n# train_df = train_df.head(100)\ntest_df = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/test.csv')","0cea341c":"train_df.head()","0b697562":"train_df.shape","ef3a9983":"num_bins = int(np.floor(1 + np.log2(len(train_df))))\ntrain_df.loc[:,'bins'] = pd.cut(train_df['target'],bins=num_bins,labels=False)\n\ntarget = train_df['target'].to_numpy()\nbins = train_df.bins.to_numpy()\n\ndef rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))","e0addbac":"train_sent_len = [len(i.split()) for i in train_df['excerpt']]\ntest_sent_len = [len(i.split()) for i in test_df['excerpt']]\n\nplt.hist(train_sent_len, bins=range(min(train_sent_len), max(train_sent_len) + 1, 1), \n              alpha=0.4, color=\"red\")\n\nplt.hist(test_sent_len, bins=range(min(test_sent_len), max(test_sent_len) + 1, 1), \n              alpha=0.4, color=\"blue\")\n\n\nlabels = ['Train','Test']\nplt.legend(labels)\nplt.xlabel(\"length of sentence\")\nplt.ylabel(\"proportion\")\nplt.title(\"comparing number of words per sentence distribution in Train and Test\")\nplt.show()","f0aa9015":"from transformers import AutoTokenizer,AutoModel\n\ntokenizer = AutoTokenizer.from_pretrained('\/kaggle\/input\/robertalarge')\nmodel = AutoModel.from_pretrained('\/kaggle\/input\/robertalarge')","0d43eaab":"model = model.to(device)","19df9251":"def data_encode(data, maximum_length):\n    \n    encoded = tokenizer(\n        data.values.tolist(),\n        add_special_tokens=True,\n        max_length=maximum_length,\n        pad_to_max_length=True,\n        return_attention_mask=True,\n        return_tensors='pt'\n    )\n#     my_list.append(encoded)\n    return encoded#,input_ids,attention_masks\n\ndef get_embeddings(encoded):\n    encoded.to(device)\n    return model(**encoded)['last_hidden_state'][:,0].cpu().detach().numpy()","21913b2e":"# t= data_encode(train_texts[0:50],200)","403c869c":"# get_embeddings(t)","6e22ca2d":"max_len = 200 # 200 for actual training\nbatch_size = 100\n\ntrain_texts = train_df['excerpt']\ntrain_target = train_df['target']\n\ntrain_embeddings = []\nincr = 20\nfor i in tqdm(range(0,len(train_texts),incr)):\n#     print(f'from {i} to {incr+i}')\n    train_input = data_encode(train_texts[i:i+incr],max_len)\n#     print(train_input.shape)\n    embeddings = get_embeddings(train_input)\n    train_embeddings.extend(embeddings)\n\n    \n\ntest_texts = test_df['excerpt']\n# test_target = [0 for i in range(test_df.shape[0])] #fake\n\ntest_embeddings = []\nfor i in tqdm(range(0,len(test_texts),incr)):\n#     print(f'from {i} to {incr+i}')\n    test_input = data_encode(test_texts[i:i+incr],max_len)\n    embeddings = get_embeddings(test_input)\n    test_embeddings.extend(embeddings)","a5fc6ea3":"# train_embeddings = []\n# train_embeddings = model(**train_input)['last_hidden_state'][:,0].detach().numpy()\n    \n# train_embeddings = np.array(train_embeddings)\n\n# test_embeddings = model(**test_input)['last_hidden_state'][:,0].detach().numpy()\n\n# test_embeddings = np.array(test_embeddings)","0b70634d":"#for kfold  \nnum_bins = int(np.floor(1 + np.log2(len(train_df))))\ntrain_df.loc[:,'bins'] = pd.cut(train_df['target'],bins=num_bins,labels=False)\nbins = train_df.bins.to_numpy()\n\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\n\ndef get_preds_svm(X,y,X_test,bins=bins,nfolds=5,C=10,kernel='rbf'):\n    kfold = StratifiedKFold(n_splits=nfolds)\n    scores = list()\n    preds = np.zeros((X_test.shape[0]))\n    for k, (train_idx,valid_idx) in enumerate(kfold.split(X,bins)):\n        svd_model = SVR(C=C,kernel=kernel,gamma='auto')\n        \n        \n        X_train,y_train = X[train_idx], y[train_idx]\n        X_valid,y_valid = X[valid_idx], y[valid_idx]\n        \n        svd_model.fit(X_train,y_train)\n        prediction = svd_model.predict(X_valid)\n#         score = rmse_score(prediction,y_valid)\n        score = np.sqrt(mean_squared_error(y_valid,prediction)) ## RMSE SCORE\n        print(f'Fold {k} , rmse score: {score}')\n        scores.append(score)\n        preds += svd_model.predict(X_test)\n        \n    print(\"mean rmse\",np.mean(scores))\n    return np.array(preds)\/nfolds","737f53ae":"train_embeddings = np.array([i.tolist() for i in train_embeddings])\ntest_embeddings = np.array([i.tolist() for i in test_embeddings])","14fe67dc":"svm_preds1 = get_preds_svm(train_embeddings,train_target,test_embeddings)\nsvm_preds2 = get_preds_svm(train_embeddings,train_target,test_embeddings)\nsvm_preds3 = get_preds_svm(train_embeddings,train_target,test_embeddings)\nsvm_preds4 = get_preds_svm(train_embeddings,train_target,test_embeddings)\nsvm_preds5 = get_preds_svm(train_embeddings,train_target,test_embeddings)","99ac7085":"svm_preds = (svm_preds1 + svm_preds2 + svm_preds3 + svm_preds4 + svm_preds5)\/5","29763968":"pd.DataFrame({\n    'id':test_df.id,\n    'target':svm_preds\n}).to_csv('submission.csv',index=False)","8c8f0499":"# Hugging Face models"}}