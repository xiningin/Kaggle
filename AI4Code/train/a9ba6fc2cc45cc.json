{"cell_type":{"eb4f0957":"code","2a0afc2b":"code","4aa8305d":"code","2c84035f":"code","f95a8a1c":"code","8832b95a":"code","fc048bf1":"code","12151404":"code","0c62e819":"code","1f6f619a":"code","12b5e395":"code","e1a40afd":"code","734a2356":"code","46eea1d3":"code","722e6560":"code","a3a776db":"code","a30cc62a":"code","ba494013":"code","fda2f471":"code","b3ea4043":"code","10b2dcb4":"code","4c5aad00":"code","4d391a83":"code","a090f719":"code","971dd01d":"code","8a4f3190":"code","a06fa8f6":"code","28415739":"code","6c74d0e1":"markdown","46df4871":"markdown","d3c6c47e":"markdown","cb314707":"markdown","24451bd1":"markdown","f0412913":"markdown","301805e9":"markdown","d1374bd7":"markdown","521e3334":"markdown","0c952821":"markdown"},"source":{"eb4f0957":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# from sklearn.preprocessing import RobustScaler\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2a0afc2b":"train_csv = pd.read_csv('..\/input\/older-dataset-for-dont-overfit-ii-challenge\/train.csv')\n\ny = train_csv['target'] \nx = train_csv.drop([\"id\" ,'target'],axis=1)\n","4aa8305d":"x.head(2)\n","2c84035f":"x.info()","f95a8a1c":"y = pd.DataFrame(y)\ny.info()","8832b95a":"x.var()","fc048bf1":"test_csv  = pd.read_csv('..\/input\/older-dataset-for-dont-overfit-ii-challenge\/test.csv')\ntest = test_csv.drop([\"id\"],axis=1)\ntest.info()","12151404":"test.info()","0c62e819":"print(x.isnull().values.any() ,'\\n',test.isnull().values.any())","1f6f619a":"test.info()","12b5e395":"x.shape\n","e1a40afd":"y.shape\n","734a2356":"test.shape","46eea1d3":"import matplotlib.pyplot as plt\ny.hist()\n","722e6560":"plt.bar(range(2), (x.shape[0], test.shape[0]))\nplt.xticks(range(2), ('train','test'))\nplt.ylabel('Number of data') \nplt.title('Can we avoid overfitting')\nplt.show()","a3a776db":"print('Distributions of the first 28 columns')\nplt.figure(figsize=(12,12))\nfor i, col in enumerate(list(x.columns)[2:30]):\n    plt.subplot(7,4,i+1)\n    plt.hist(x[col])\n    plt.title(col)","a30cc62a":"# from sklearn.preprocessing import StandardScaler\n# x_final = x\n# ss = StandardScaler().fit(x_final)\n# ss.transform(x_final)\n# x_final = pd.DataFrame(x_final)\n#x_final","ba494013":"from sklearn.preprocessing import RobustScaler\nrbst = RobustScaler()\nx_final = rbst.fit_transform(x)\ntest_final = rbst.fit_transform(test)\n\n\nx_final = pd.DataFrame(x_final)\ntest_final = pd.DataFrame(test_final)","fda2f471":"#Split data\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_final , y , train_size=0.85, test_size=0.15, random_state = 42 ,stratify = y)","b3ea4043":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\n\n\nrandom_state = 0\nlogreg_clf = LogisticRegression(solver='liblinear',random_state = random_state)\nparam_grid = {'class_weight' : ['balanced', None], \n                'penalty' : ['l2','l1'],  \n                'C' : [0.001, 0.01, 0.1, 1, 10]}\n\ngrid = GridSearchCV(estimator = logreg_clf, param_grid = param_grid , scoring = 'accuracy', verbose = 1, n_jobs = -1, cv = 35 )\n\ngrid.fit(x_train,y_train)\n\nprint(\"Best Score:\" + str(grid.best_score_))\nprint(\"Best Parameters: \" + str(grid.best_params_))","10b2dcb4":"#Based on the grid search i came out with best hyperparameters\n\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg_clf = LogisticRegression(solver='liblinear' , C =0.2 , class_weight='balanced' , \n                                penalty='l1' , random_state = 45)\n\nlogreg_clf.fit(x_train, y_train) \ny_pred_lr = logreg_clf.predict(x_val)\n\nfrom sklearn.metrics import accuracy_score\n\naccuracy_lg = accuracy_score(y_val,  y_pred_lr)\n\nprint('prediction accuracyy',accuracy_lg*100,'%')\n\nprint('model score' , logreg_clf.score(x_val, y_val)*100 ,'%')","4c5aad00":"from sklearn.ensemble import RandomForestClassifier \nfrom sklearn.model_selection import GridSearchCV\n\nrandom_state = 42\nRFC_model = RandomForestClassifier(random_state = random_state,n_estimators = 10 ,criterion = 'gini')\n\nparam_grid_RFC = {'max_depth' : [5,7,9,11], \n                'min_samples_leaf' : [1, 2,5,7,9] ,\n                'class_weight' : ['balanced', 'balanced_subsample' ]}\n\ngrid_rf = GridSearchCV(estimator = RFC_model, param_grid = param_grid_RFC , scoring = 'roc_auc', verbose = 1, n_jobs = -1, cv = 35 )\n\ngrid_rf.fit(x_train,y_train)\n\nprint(\"Best Score:\" + str(grid_rf.best_score_))\nprint(\"Best Parameters: \" + str(grid_rf.best_params_))","4d391a83":"from sklearn.ensemble import RandomForestClassifier \nRFC_model = RandomForestClassifier(random_state = 42 ,n_estimators = 100 ,criterion = 'gini' , \n                                   class_weight='balanced', max_depth = 9, min_samples_leaf= 7)\nRFC_model.fit(x_train, y_train)\n\nfrom sklearn.metrics import accuracy_score\ny_pred_rf = RFC_model.predict(x_val)\naccuracy_rf = accuracy_score(y_val,  y_pred_rf)\nprint('Validation accuracyy',accuracy_rf*100,'%')\nprint('model score' , RFC_model.score(x_train, y_train))\n","a090f719":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nrandom_state = 42\nknn_model = KNeighborsClassifier(metric ='minkowski' ,metric_params = None ,  leaf_size=30)\n\nparam_grid_knn = {'n_neighbors' : [3,4,5,7],\n                  'weights':['uniform', 'distance'] , \n                  'p' :[2,3,4,5] }\n\ngrid_knn = GridSearchCV(estimator = knn_model, param_grid = param_grid_knn , verbose = 1, n_jobs = -1, cv = 20 )\n\ngrid_knn.fit(x_train,y_train)\n\nprint(\"Best Score:\" + str(grid_knn.best_score_))\nprint(\"Best Parameters: \" + str(grid_knn.best_params_))","971dd01d":"from sklearn.neighbors import KNeighborsClassifier\n\nkn_model = KNeighborsClassifier(metric ='minkowski' ,metric_params = None ,  leaf_size=2 , \n                                n_neighbors= 3, p=2,weights='uniform')\nkn_model.fit(x_train, y_train)\n\nfrom sklearn.metrics import accuracy_score\n\ny_pred_kn = kn_model.predict(x_val)\n\naccuracy_kn = accuracy_score(y_val,  y_pred_kn)\nprint('Prediction accuracyy',accuracy_kn*100,'%')\nprint('Train score' , kn_model.score(x_train, y_train))","8a4f3190":"\ny_pred_lr_test = logreg_clf.predict(test_final) #to be saved\ny_pred_lr_test = pd.DataFrame({\"ID\": test_csv[\"id\"],\"Target\": y_pred_lr_test})\n\n\ny_pred_lr_test.to_csv('submission.csv', index=False)  \ny_pred_lr_test","a06fa8f6":"y_pred_lr_test['Target'].hist()","28415739":"#10","6c74d0e1":"**Model 3 : KNeighborsClassifier**","46df4871":"**Model :1 - logistic regression**","d3c6c47e":"# **Scaling data**","cb314707":"I will use first use grid search with models to choose best hyper parameters \n","24451bd1":"**Grid search fot his model to get best hyperparameters**","f0412913":"# Read CSVs","301805e9":"# **Submission**","d1374bd7":"**First use Grissearch to get best parameters for Logistic regression**","521e3334":"**Try grid search first**","0c952821":"**Model 2 : RandomForestClassifi**"}}