{"cell_type":{"86d40dd7":"code","aa321195":"code","daff3f88":"code","05945a98":"code","5c32e68b":"code","f058ea14":"code","e6123f84":"code","75d4bcc6":"code","975c3bdd":"code","69f85291":"code","71552813":"code","92abf0d4":"code","7abca4a1":"code","64aa58f7":"code","8916d943":"code","15901724":"code","3ecf6a34":"code","a35ef94e":"code","d0eed223":"code","d4d1ca84":"code","a6723edb":"code","1c4da8b5":"code","2c7aff99":"code","d1bc5522":"code","1b27391e":"code","b25350d8":"code","effad9a6":"code","8c26944e":"code","bee5198c":"code","65040ebc":"code","e0652f97":"code","69662ae2":"code","c8758bf8":"markdown","33051ec3":"markdown","98f5ee4b":"markdown","454ff1bd":"markdown","64778584":"markdown","dc5b9d6c":"markdown","9625b394":"markdown","760194a2":"markdown","45381b02":"markdown","7bf540b3":"markdown","38ff5565":"markdown","73f877af":"markdown","a67e0484":"markdown","aa4d63e1":"markdown","5e032c90":"markdown","c929338c":"markdown","8de2aef6":"markdown","a7f361d4":"markdown","c6cd8203":"markdown","66cee54c":"markdown","d022ec1b":"markdown","aa53aa7a":"markdown","b589d04c":"markdown","09a08b4e":"markdown","cb624a0e":"markdown","802ad120":"markdown","2a97df9f":"markdown","d0003f79":"markdown","54bd7eec":"markdown","29d13a7e":"markdown","7e8a3b1d":"markdown","7631ae7c":"markdown"},"source":{"86d40dd7":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"ticks\")\n%matplotlib inline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten ,Conv2D, MaxPooling2D\nfrom keras.callbacks import ReduceLROnPlateau\n\n","aa321195":"train_data = pd.read_csv(\"..\/input\/fashionmnist\/fashion-mnist_test.csv\")\ntest_data = pd.read_csv(\"..\/input\/fashionmnist\/fashion-mnist_test.csv\")\n\nprint(f'Training Data size is : {train_data.shape}')\nprint(f'Test Data size is : {test_data.shape}')","daff3f88":"train_data.head()","05945a98":"test_data.head()","5c32e68b":"X = train_data.drop(['label'], axis=1, inplace=False)\ny = train_data['label']\n\nprint('X shape is ' , X.shape)\nprint('y shape is ' , y.shape)","f058ea14":"X_test = test_data.drop(['label'], axis=1, inplace=False)\ny_test = test_data['label']\n\nprint('X shape is ' , X_test.shape)\nprint('y shape is ' , y_test.shape)","e6123f84":"plt.figure(figsize=(12,10))\nplt.style.use('ggplot')\nfor i in  range(20)  :\n    plt.subplot(4,5,i+1)\n    plt.imshow(X.values[ np.random.randint(1,X.shape[0])].reshape(28,28) , cmap='gray')\n    ","75d4bcc6":"y.value_counts()","975c3bdd":"y_test.value_counts()","69f85291":"plt.figure(figsize=(12,12))\nplt.pie(y.value_counts(),labels=list(y.value_counts().index),autopct ='%1.2f%%' ,\n        labeldistance = 1.1,explode = [0.05 for i in range(len(y.value_counts()))] )\nplt.show()\n","71552813":"X = X \/ 255.0\nX_test = X_test \/ 255.0","92abf0d4":"X.shape","7abca4a1":"X_test.shape","64aa58f7":"X = X.values.reshape(-1,28,28,1)\nX_test = X_test.values.reshape(-1,28,28,1)","8916d943":"X.shape","15901724":"X_test.shape","3ecf6a34":"ohe  = OneHotEncoder()\ny = np.array(y)\ny = y.reshape(len(y), 1)\nohe.fit(y)\ny = ohe.transform(y).toarray()","a35ef94e":"y.shape","d0eed223":"ohe  = OneHotEncoder()\ny_test = np.array(y_test)\ny_test = y_test.reshape(len(y_test), 1)\nohe.fit(y_test)\ny_test = ohe.transform(y_test).toarray()","d4d1ca84":"y_test.shape","a6723edb":"X_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size=0.15, random_state=44, shuffle =True)\n\nprint('X_train shape is ' , X_train.shape)\nprint('X_test shape is ' , X_cv.shape)\nprint('y_train shape is ' , y_train.shape)\nprint('y_test shape is ' , y_cv.shape)","1c4da8b5":"KerasModel = keras.models.Sequential([\n        keras.layers.Conv2D(filters = 32, kernel_size = (3,3),  activation = tf.nn.relu , padding = 'same'),\n        keras.layers.MaxPool2D(pool_size=(2,2), strides=None, padding='valid'),\n        keras.layers.BatchNormalization(),\n        keras.layers.Conv2D(filters=32, kernel_size=(2,2),activation = tf.nn.relu , padding='same'),\n        keras.layers.MaxPool2D(),\n        keras.layers.BatchNormalization(),\n        keras.layers.Dropout(0.5),        \n        keras.layers.Flatten(),    \n        keras.layers.Dropout(0.5),        \n        keras.layers.Dense(64),    \n        keras.layers.Dropout(0.3),            \n        keras.layers.Dense(units= 10,activation = tf.nn.softmax ),                \n\n    ])","2c7aff99":"KerasModel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])","d1bc5522":"epochs_number = 200\nhist = KerasModel.fit(X_train,y_train,validation_data=(X_cv, y_cv),epochs=epochs_number,batch_size=64,verbose=1)","1b27391e":"score = KerasModel.evaluate(X_test, y_test, verbose=0)\nscore","b25350d8":"KerasModel.summary()","effad9a6":"ModelLoss, ModelAccuracy = KerasModel.evaluate(X_test, y_test)\n\nprint('Test Loss is {}'.format(ModelLoss))\nprint('Test Accuracy is {}'.format(ModelAccuracy ))","8c26944e":"ModelAcc = hist.history['acc']\nValAcc = hist.history['val_acc']\nLossValue = hist.history['loss']\nValLoss = hist.history['val_loss']\nepochs = range(len(ModelAcc))","bee5198c":"plt.plot(range(1,epochs_number+1),ModelAcc, 'ro', label='Accuracy of Training ')\nplt.plot(range(1,epochs_number+1), ValAcc, 'r', label='Accuracy of Validation')\nplt.title('Training Vs Validation Accuracy')\nplt.legend()\nplt.figure()\n","65040ebc":"plt.plot(range(1,epochs_number+1), LossValue, 'ro', label='Loss of Training ')\nplt.plot(range(1,epochs_number+1), ValLoss, 'r', label='Loss of Validation')\nplt.title('Training Vs Validation loss')\nplt.legend()\nplt.show()","e0652f97":"y_pred = KerasModel.predict(X_test)\n\nprint('Prediction Shape is {}'.format(y_pred.shape))","69662ae2":"for i in list(np.random.randint(0,len(X_test) ,size= 20)) : \n    print(f'for sample  {i}  the predicted value is   {np.argmax(y_pred[i])}   , while the actual letter is {np.argmax(y_test[i])}')\n    if np.argmax(y_pred[i]) != np.argmax(y_test[i]) : \n        print('==============================')\n        print('Found mismatch . . ')\n        plt.figure(figsize=(5,5))\n        plt.style.use('ggplot')\n        plt.imshow(X_test[i].reshape(28,28))\n        plt.show()\n        print('==============================')","c8758bf8":"then we need to reshape them , to be 4 dimensions , so first dimension will be open for all sample size , then 28 x 28 as image size , then 1","33051ec3":"let's check random 20 samples , & we need to have a look to any mismatch images , to see why it confused","98f5ee4b":"Ok great , let's make a pie chart for training output","454ff1bd":"____\n\nok no mismatch found . \n\n","64778584":"and for testing","dc5b9d6c":"ok , looks like more epochs might be needed for the model , which will increase its accuracy \n\nhow about loss value","9625b394":"how it looks like ? ","760194a2":"then read the data ","45381b02":"we also need to be sure that output numbers are kinda equally distributed for training data","7bf540b3":"and check its dimension","38ff5565":"and test data","73f877af":"a good accuracy , and might increase if we make more epochs\n\n_____\n\nnow let's have a look to chart of epochs-accuracy , to know if we should do more epochs or we shpuld stop earlier\n\nfirst to calculate history accuracy values","a67e0484":"____\n\n# Dimension Adjusting\n\nit;s very important to adjust dimensions for data before building the CNN , let's first normalize both traing & test data . \n\nofcourse y will not be normalized or it will mislead the training","aa4d63e1":"____\n\n# Data Splitting .\n\nwe have to split our train data ,  to get cross-validation data , and train data\n","5e032c90":"how is accuracy for test data , which never seen by the model yet","c929338c":"_____\n\n# Data Processing\n\nthen we can define X & y data for training","8de2aef6":"greart . how it looks like now ? ","a7f361d4":"now how y looks like ? ","c6cd8203":"also we have to categorize y , to convert single numbers like (7) into One Hot Matrix like [0 0 0 0 0 0 1 0 0 0]","66cee54c":"let's have a look to a random 20 numbers ","d022ec1b":"____\n\n# Conv2D Model\n\nnow we can build our model , which will contain Conv layer then Maxpooling then normalize it \n\nthen second layer contain Conv then Max then normalize\n\nthen drop it out with 50 %\n\nthen Flatten it \n\nthen drop it out , then a FC with 64 units , then drop out , then last FC output layer ","aa53aa7a":"then we'll do it again to test output data","b589d04c":"now we can start training for 200 epochs . ","09a08b4e":"complie the model using adam optimizer & loss function : categorical crossentropy , since it's multilassifier","cb624a0e":"and test data ? ","802ad120":"now how X dimension looks like","2a97df9f":"then draw Training accuracy with epocs","d0003f79":"# Classifying Mnist Fashion Using CNN\nBy : Hesham Asem\n\n______\n\n\nlet's build Conv2D Neural Network , to classify tens of thousands of mnist Fashion images . . \n\nData File  :https:\/\/www.kaggle.com\/zalando-research\/fashionmnist\n\nfirst to import needed libraries\n","54bd7eec":"and for testing data","29d13a7e":"again , more epochs here will decrease the loss in the model \n\n_____\n\nnow to predict X Test","7e8a3b1d":"how is the final loss & accuracy","7631ae7c":"now how train data dimension looks like"}}