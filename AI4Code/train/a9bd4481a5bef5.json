{"cell_type":{"973e635f":"code","2b8ca127":"code","1d6706f1":"code","c75c3bff":"code","c8da70e4":"code","219127c0":"code","0048b1a8":"code","cfea000d":"code","6ddcd59f":"code","1f962d7c":"code","c3e5a9cd":"code","98cdc64a":"markdown","ba5eca74":"markdown","7502cd68":"markdown","4fd12ffa":"markdown","71540499":"markdown","62e25aca":"markdown","aad2bd9e":"markdown","41cc3dda":"markdown","c335b577":"markdown","b5e18dbb":"markdown","2c087e52":"markdown"},"source":{"973e635f":"import numpy as np\nimport seaborn as sns\nx_sig = np.arange(-10,10,0.5)\nx_sig = np.c_[np.ones((len(x_sig), 1)), x_sig]\ny_sig = np.array([(1\/(1+np.exp(-i))) for i in x_sig[:,1]]).reshape(-1,1)\nsns.lineplot(x_sig[:,1],y_sig[:,0])","2b8ca127":"import numpy as np\nimport seaborn as sns\n\nx = np.arange(-10,10,0.5)\ny = np.array([0 if i < np.random.randint(-10,10) else 1 for i in x_sig[:,1]]).reshape(-1,1)\nsns.scatterplot(x,y[:,0])\nsns.lineplot(x_sig[:,1],y_sig[:,0],color='red')","1d6706f1":"def sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))","c75c3bff":"m = len(y)\nlearning_rate = 0.1\ntheta = np.zeros(shape = (2,1))\nfor i in range(10000):\n    theta = theta - ((learning_rate\/m) * x_sig.T.dot(sigmoid(x_sig.dot(theta)) - y))","c8da70e4":"pred = sigmoid(x_sig.dot(theta))","219127c0":"sns.scatterplot(x,y[:,0])\nsns.lineplot(x,pred[:,0],color='red')","0048b1a8":"import tensorflow as tf\nx_sig_tf = tf.cast(tf.constant(x_sig),'double')\ny_sig_tf = tf.cast(tf.constant(y_sig),'double')","cfea000d":"def sigmoid(x):\n    return 1 \/ (1 + tf.math.exp(-x))","6ddcd59f":"m = len(y)\nlearning_rate = 0.1\ntheta = tf.cast(tf.zeros(shape = (2,1)),'double')\nfor i in range(10000):\n     theta = theta - ((learning_rate\/m) * tf.linalg.matmul(tf.transpose(x_sig),sigmoid(tf.linalg.matmul(x_sig,theta)) - y))","1f962d7c":"pred = sigmoid(tf.linalg.matmul(x_sig,theta))","c3e5a9cd":"sns.scatterplot(x,y[:,0])\nsns.lineplot(x,pred[:,0],color='red')","98cdc64a":"This line of code predicts the output and draws the sigmoid curve using the dot product of theta with the input and passing it through the sigmoid function","ba5eca74":"This curve essentially represents the likelihood of an external data point. The likelihood is a quantity which defines what chance a data point will be 1 or a 0. Based on the curve points closer to -10 will have more chance to be 0 whereas points closer to 10 will have more chance to be 1. \n\nThus the loss function or the maximum likelihood function is maximised by changing the values of $\\theta(x)$ so that we can get the maximum value. The dummy dataset that we will use for this demonstartion is given below","7502cd68":"## The Sigmoid Function \n___\n\nThe sigmoid function for logistic regression is defined as \n\n## $Probability = \\frac {1} {1+e^{-\\theta(x)}}$\n\nwhere\n\n## $\\theta(x) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_nx_n$\n\nBased on the above equation a sigmoid function will look as defined below","4fd12ffa":"## Numpy implementation of gradient descent\n___\n\nThe below code will explain the numpy implementation of gradient descent algorithm","71540499":"## Concept of Log Odds\n___\n\nWe know that the model is defined as \n\n## $p = \\frac {1} {1+e^{-\\theta(x)}}$\n\nThis can be simplified down as \n\n## $log(\\frac{p}{1-p}) = \\theta(x)$\n\nThe term \n\n## **$\\frac{p}{1-p}$**\n\nis called the odds and log of the term is log odds.\n\nIts the ratio between the favourable outcomes and the not favourable outcomes","62e25aca":"This code defines the sigmoid function","aad2bd9e":"## Tensorflow implementation of gradient descent\u00b6\n___\n\nIn this section we will rewrite the above code using tensorflow.","41cc3dda":"## Cost function and gradient descent\n___\n\nMathematically the Maximum Liklihood function can be defined as \n\n![](https:\/\/miro.medium.com\/max\/4800\/1*2g14OVjyJqio2zXwJxgj2w.png)\n\nThis equation needs to be maximised in order to reduce our losses.\n\nAnother cost function that we use is called the **binary cross entropy function**. It is mathematically defined as \n\n## $\\lambda(\\theta(x),y) = -\\frac{1}{m}\\displaystyle\\sum\\limits_{i=0}^m \\theta(x)log(y) + (1-y)(log(1-\\theta(x)) $\n\nwhere m = batch size\n\nhence \n\n## $\\frac{\\partial}{\\partial\\theta} \\lambda(\\theta(x),y)) = \\vec{X^T}*(\\vec{\\theta(X)}-\\vec{Y})$\n\nSo \n\n## $\\theta_1 = \\theta_0 - \\frac{\\alpha}{m}*\\vec{X^T}*(\\vec{\\theta_0(X)}-\\vec{Y})$\n\nThus in this way the batch gradient descent algorithm will proceed.\n\na detailed derivation of the gradient descent is described in [here](https:\/\/math.stackexchange.com\/a\/3220477)","c335b577":"In the above code gradient descent is carried out in order to find the necessary value of theta","b5e18dbb":"# Logistic Regression\n\nLogistic regression is another highly popular algorithm which is used for binary classification problems. Binary classification problems are problems where we need to classify a given dataset into two different classes. But if it is a classification algorithm then why do we call it logistic regression. \n\nThe reason is that using the logistic regression algorithm , we find a decision boundary based on the sigmoid curve. The points that lie above the boundary belong to one class and the points that lie below the boundary belong to another class. Thus we essentially fit a curve so as to reduce the cost function. This is very similar to ordianry linear regression and hence the algorithm is called logistic regression.","2c087e52":"## Conclusion\n___\n\nThe above code examples clearly show us how logistic regression works internally and how we can implement them in tensorflow or numpy. \n\nThis notebook and the preeding one [linear regression from scratch](https:\/\/www.kaggle.com\/ezzzio\/linear-regression-from-scratch) Give us a clear intuition of how regression algorithm works, how gradient descent plays an important role and how we can implement these algorithms in numpy and tensorflow. These form the basis of perceptrons and neural networks."}}