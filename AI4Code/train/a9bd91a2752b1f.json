{"cell_type":{"425338b0":"code","cddd38dc":"code","25a49c8c":"code","85d4b982":"code","09e1b3bf":"code","f78e5fea":"code","6940edf5":"code","ef1d6e68":"code","b37d8f88":"code","953c1e90":"code","143fd05f":"code","823c6743":"code","fc648859":"code","420ed17d":"code","ab01e56f":"code","e68f93d2":"code","2ebd2dc9":"code","cef3759c":"code","7f5abdfe":"code","8a5b4434":"code","5a198d0a":"code","749022d4":"code","e466d69b":"code","a57840c2":"code","613a158a":"code","3995917f":"code","54774bc7":"code","76cca6a3":"code","2ac6333c":"code","b5b62d39":"code","47200991":"code","15bbb191":"code","1bf31243":"code","adcf4cfb":"code","86bf5f97":"code","707d9ae7":"code","a0fadd5a":"code","9e0109f6":"code","f784d75c":"code","b9cf1e26":"code","af910fc3":"code","81c19758":"code","b4c9bef3":"code","5c129dad":"code","0e5a605c":"code","9f8dcf3f":"code","7e1a4111":"code","bcab6ccc":"code","a225756e":"code","6423785a":"code","f51bfdbb":"code","3cbacbc9":"code","cc9bd8e9":"code","027ce03c":"code","c3056d4c":"code","2b3a0f89":"markdown","1670fa12":"markdown","6d2df61e":"markdown","fdd2d9fd":"markdown","1fa0fe26":"markdown","067397b5":"markdown","cffb3ab6":"markdown","772bd0d9":"markdown","2f64b3eb":"markdown","0c271587":"markdown","f6efc003":"markdown","eee5a9cb":"markdown","2a9e0769":"markdown","b47a11a5":"markdown","79a28a2f":"markdown","64da5fc2":"markdown","cbfebd8e":"markdown","4eaf406b":"markdown","c59bf8ba":"markdown","6ff9fe3d":"markdown","593170fe":"markdown","842e2143":"markdown","a81b993b":"markdown","b3a43b92":"markdown","cca99153":"markdown","0e390ed1":"markdown","60060a45":"markdown","d3c83a7f":"markdown","11a38736":"markdown","e178263c":"markdown"},"source":{"425338b0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cddd38dc":"import pandas as pd\nimport numpy as np\nimport random\nimport time\nimport datetime\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\n\nimport re\nimport json\nfrom tqdm.autonotebook import tqdm\nimport string\nimport collections\nfrom textblob import TextBlob\n\nimport spacy\n\nimport nltk\nfrom nltk.probability import FreqDist\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n\nfrom keras.preprocessing import sequence, text\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import utils\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.callbacks import *\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers import Dense, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils.vis_utils import plot_model\n\nimport warnings\nwarnings.filterwarnings('ignore')","25a49c8c":"#define paths\nos.listdir('\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/')\ntrain_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\ntest_path = '..\/input\/coleridgeinitiative-show-us-the-data\/test'","85d4b982":"#read train data\ntrain_df = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/train.csv')\ntrain_df.head()","09e1b3bf":"#create a function to get the text from the JSON file and append it to the new column in table\ndef read_json_pub(filename, train_path = train_path, output = 'text'):\n    json_path = os.path.join(train_path, (filename + '.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data","f78e5fea":"#apply the function to train data\ntqdm.pandas()\ntrain_df['text'] = train_df['Id'].progress_apply(read_json_pub)","6940edf5":"#recheck\ntrain_df.head()","ef1d6e68":"#read submission data\nsubmission_df = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv')\n\n#apply the function to submission data\ntqdm.pandas()\nsubmission_df['text'] = submission_df['Id'].progress_apply(read_json_pub)\n\nsubmission_df.head()","b37d8f88":"#save\nsubmission_df.to_csv('submission_df.csv')","953c1e90":"#let's read the first sample\n\nimport json\nwith open('..\/input\/coleridgeinitiative-show-us-the-data\/train\/d0fa7568-7d8e-4db9-870f-f9c6f668c17b.json') as f:\n    sample = json.load(f)\n    \nsample[:2]","143fd05f":"#get all 'section_title'\nfor s in sample:\n    print(s['section_title'])","823c6743":"#define stopwords\nfrom nltk.corpus import stopwords\nstopwords_list = stopwords.words('english') + list(string.punctuation)\nstopwords_list += [\"''\", '\"\"', '...', '``']","fc648859":"#https:\/\/towardsdatascience.com\/text-analysis-feature-engineering-with-nlp-502d6ea9225d\n\ndef text_cleaning(text, flg_stemm = False, flg_lemm = True, lst_stopwords = None):\n    '''\n    Converts all text to lower case, tokenize, remove multiple spaces, stopwords, stemming, lemmatize, \n    then convert all back to string\n    \n    text: string - name of column containing text\n    lst_stopwords: list - list of stopwords to remove\n    flg_stemm: bool - whether stemming is to be applied\n    flg_lemm: bool - whether lemmitisation is to be applied\n    '''\n    \n    #clean (convert to lowercase and remove punctuations and characters and then strip)\n    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n            \n    #tokenize (convert from string to list)\n    lst_text = text.split()\n    \n    #remove Stopwords\n    if lst_stopwords is not None:\n        lst_text = [word for word in lst_text if word not in \n                    stopwords_list]\n                \n    #stemming (remove -ing, -ly, ...)\n    if flg_stemm == True:\n        ps = nltk.stem.porter.PorterStemmer()\n        lst_text = [ps.stem(word) for word in lst_text]\n                \n    #lemmatisation (convert the word into root word)\n    if flg_lemm == True:\n        lem = nltk.stem.wordnet.WordNetLemmatizer()\n        lst_text = [lem.lemmatize(word) for word in lst_text]\n            \n    #back to string from list\n    text = \" \".join(lst_text)\n    return text","420ed17d":"#clean pub_title text\ntqdm.pandas()\ntrain_df['pub_title'] = train_df['pub_title'].progress_apply(text_cleaning)","ab01e56f":"#clean dataset_title text\ntqdm.pandas()\ntrain_df['dataset_title'] = train_df['dataset_title'].progress_apply(text_cleaning)","e68f93d2":"#clean train text\ntqdm.pandas()\ntrain_df['text'] = train_df['text'].progress_apply(text_cleaning)","2ebd2dc9":"#review\ntrain_df.head()","cef3759c":"#save\ntrain_df.to_csv('train_df.csv')","7f5abdfe":"#get info\ntrain_df.info()","8a5b4434":"#check null values\ntrain_df.isnull().sum()","5a198d0a":"#get summary\ntrain_df.describe()","749022d4":"print('Number of duplicates in Id:', train_df['Id'].duplicated().sum())\nprint('Number of duplicates in pub_title:', train_df['pub_title'].duplicated().sum())\nprint('Number of duplicates in dataset_title:', train_df['dataset_title'].duplicated().sum())\nprint('Number of duplicates in dataset_label:', train_df['dataset_label'].duplicated().sum())\nprint('Number of duplicates in cleaned_label:', train_df['cleaned_label'].duplicated().sum())","e466d69b":"#check out duplicates\nid_duplicates = train_df['Id'] == '170113f9-399c-489e-ab53-2faf5c64c5bc'\ntrain_df.loc[id_duplicates][:10]","a57840c2":"#check out duplicates\npub_title_duplicates = train_df['pub_title'] == 'science and engineering indicator 2014'\ntrain_df.loc[pub_title_duplicates][:10]","613a158a":"#check out duplicates\ndataset_title_duplicates = train_df['dataset_title'] == 'alzheimers disease neuroimaging initiative adni'\ntrain_df.loc[dataset_title_duplicates][:10]","3995917f":"def get_num_words_per_sample(sample_texts):\n    \"\"\"Returns the median number of words per sample given corpus.\n\n    # Arguments\n        sample_texts: list, sample texts.\n\n    # Returns\n        int, median number of words per sample.\n    \"\"\"\n    num_words = [len(s.split()) for s in sample_texts]\n    return np.median(num_words)\n\nprint('dataset_title median word count:', get_num_words_per_sample(train_df['dataset_title']))\nprint('cleaned_label median word count:', get_num_words_per_sample(train_df['cleaned_label']))\nprint('text median word count:', get_num_words_per_sample(train_df['text']))","54774bc7":"#calculate the number of samples\/number of words per sample ratio\nlen(train_df['dataset_title']) \/ get_num_words_per_sample(train_df['dataset_title'])","76cca6a3":"plt.figure(figsize = (30, 20)),\n\nsns.countplot(y = train_df['Id'], \n              order = train_df['Id'].value_counts(ascending = False)[:20].index, \n              palette = 'Spectral')\nplt.ylabel('Id',fontsize = 20)\nplt.title('Id')\nplt.show()\n\n#save\nplt.savefig('Id.png')","2ac6333c":"train_df['pub_title'].unique()","b5b62d39":"train_df['pub_title'].value_counts().head(10).to_frame()","47200991":"#create a frequency distribution to see which words are used the most\nwords = list( train_df['pub_title'].values)\nstopwords = stopwords_list\nsplit_words = []\n\nfor word in words:\n    lo_w = []\n    list_of_words = str(word).split()\n    for w in list_of_words:\n        if w not in stopwords:\n            lo_w.append(w)\n    split_words.append(lo_w)\nallwords = []\n\nfor wordlist in split_words:\n    allwords += wordlist\n    \n#get 100 most common words\nmostcommon = FreqDist(allwords).most_common(100)\nmostcommon","15bbb191":"#plot frequency distributions\nwordcloud = WordCloud(width = 1600, height = 800, \n                      background_color = 'black', \n                      colormap = 'Spectral', \n                      stopwords = stopwords_list).generate(str(mostcommon))\n\nfig = plt.figure(figsize = (20, 10), facecolor = 'white')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.title('Top 100 Most Common Words in dataset_title', fontsize = 30)\nplt.tight_layout()\n\n#save\nplt.savefig('pub_title_wordcloud.png')","1bf31243":"train_df['dataset_title'].unique()","adcf4cfb":"train_df['dataset_title'].value_counts().head(20).to_frame()","86bf5f97":"#create a frequency distribution to see which words are used the most\nwords = list( train_df['dataset_title'].values)\nstopwords = stopwords_list\nsplit_words = []\n\nfor word in words:\n    lo_w = []\n    list_of_words = str(word).split()\n    for w in list_of_words:\n        if w not in stopwords:\n            lo_w.append(w)\n    split_words.append(lo_w)\nallwords = []\n\nfor wordlist in split_words:\n    allwords += wordlist\n    \n#get 100 most common words\nmostcommon = FreqDist(allwords).most_common(100)\nmostcommon","707d9ae7":"#plot frequency distributions\nwordcloud = WordCloud(width = 1600, height = 800, \n                      background_color = 'black', \n                      colormap = 'Spectral', \n                      stopwords = stopwords_list).generate(str(mostcommon))\n\nfig = plt.figure(figsize = (20, 10), facecolor = 'white')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.title('Top 100 Most Common Words in dataset_title', fontsize = 30)\nplt.tight_layout()\n\n#save\nplt.savefig('dataset_title_wordcloud.png')","a0fadd5a":"plt.figure(figsize = (30, 30)),\n\nsns.countplot(y = train_df['dataset_title'], \n              order = train_df['dataset_title'].value_counts().index, \n              palette = 'Spectral')\nplt.ylabel('dataset_title',fontsize = 30)\nplt.xticks(fontsize = 30)\nplt.show()\n\n#save\nplt.savefig('dataset_title.png')","9e0109f6":"from sklearn.feature_extraction.text import CountVectorizer\n\n#get bigrams \nvectorizer = CountVectorizer(ngram_range = (2, 2))\n\n#matrix of ngrams\nngrams = vectorizer.fit_transform(train_df['dataset_title']) \nfeatures = (vectorizer.get_feature_names())\nprint('\\n\\nFeatures : \\n', features)\n\n#count frequency of ngrams\nprint('\\n\\nX1 : \\n', ngrams.toarray())\n  \n#apply TFIDF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(ngram_range = (2, 2))\nngrams = vectorizer.fit_transform(train_df['dataset_title'])\nscores = (ngrams.toarray())\nprint('\\n\\nScores : \\n', scores)\n  \n#get top ranking features\nsums = ngrams.sum(axis = 0)\ndata1 = []\nfor col, term in enumerate(features):\n    data1.append( (term, sums[0,col] ))\nranking = pd.DataFrame(data1, columns = ['term','rank'])\nwords = (ranking.sort_values('rank', ascending = False))\nprint ('\\n\\nWords head : \\n', words.head(20))","f784d75c":"#count frequency of ngrams\ncount_values = ngrams.toarray().sum(axis = 0)\n\n#list of ngrams\nvocab = vectorizer.vocabulary_\ndf_bigram = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()], reverse = True)\n            ).rename(columns = {0: 'frequency', 1: 'bigram'})\n\nplt.figure(figsize = (20, 10))\nsns.lineplot(x = df_bigram['bigram'][:60], y = df_bigram['frequency'][:60])\nplt.xticks(rotation = 90, fontsize = 16)\nplt.xlabel('Bigram',fontsize = 20)\nplt.ylabel('Frequency',fontsize = 20)\nplt.title('Dataset Title Bigram',fontsize = 30)\nplt.show()\n\n#save\nplt.savefig('dataset_title_bigram.png')","b9cf1e26":"#get trigrams \nvectorizer = CountVectorizer(ngram_range = (3, 3))\n\n#matrix of ngrams\nngrams = vectorizer.fit_transform(train_df['dataset_title']) \nfeatures = (vectorizer.get_feature_names())\nprint('\\n\\nFeatures : \\n', features)\n\n#count frequency of ngrams\nprint('\\n\\nX1 : \\n', ngrams.toarray())\n  \n#apply TFIDF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(ngram_range = (3,3))\nngrams = vectorizer.fit_transform(train_df['dataset_title'])\nscores = (ngrams.toarray())\nprint('\\n\\nScores : \\n', scores)\n  \n#get top ranking features\nsums = ngrams.sum(axis = 0)\ndata1 = []\nfor col, term in enumerate(features):\n    data1.append( (term, sums[0,col] ))\nranking = pd.DataFrame(data1, columns = ['term','rank'])\nwords = (ranking.sort_values('rank', ascending = False))\nprint ('\\n\\nWords head : \\n', words.head(60))","af910fc3":"#count frequency of ngrams\ncount_values = ngrams.toarray().sum(axis = 0)\n\n#list of ngrams\nvocab = vectorizer.vocabulary_\ndf_trigram = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()], reverse = True)\n            ).rename(columns = {0: 'frequency', 1:'trigram'})\n\nplt.figure(figsize = (20, 10))\nsns.lineplot(x = df_trigram['trigram'][:60], y = df_trigram['frequency'][:60])\nplt.xticks(rotation = 90, fontsize = 16)\nplt.xlabel('Trigram',fontsize = 20)\nplt.ylabel('Frequency',fontsize = 20)\nplt.title('Dataset Title Trigram',fontsize = 30)\nplt.show()\n\n#save\nplt.savefig('dataset_title_trigram.png')","81c19758":"#find one-worded, two-worded, three-worded, four-worded, five-worded dataset title\none_worded_dataset_title = train_df[train_df['dataset_title'].str.split().apply(len) == 1]\ntwo_worded_dataset_title = train_df[train_df['dataset_title'].str.split().apply(len) == 2]\nthree_worded_dataset_title = train_df[train_df['dataset_title'].str.split().apply(len) == 3]\nfour_worded_dataset_title = train_df[train_df['dataset_title'].str.split().apply(len) == 4]\nfive_worded_dataset_title = train_df[train_df['dataset_title'].str.split().apply(len) == 5]\nsix_worded_dataset_title = train_df[train_df['dataset_title'].str.split().apply(len) == 6]\n\n#create a bar plot\nfig, ax = plt.subplots(figsize = (10, 6))\nax.bar([1, 2, 3, 4, 5, 6], [one_worded_dataset_title.size,\n                         two_worded_dataset_title.size,\n                         three_worded_dataset_title.size,\n                         four_worded_dataset_title.size,\n                         five_worded_dataset_title.size,\n                            six_worded_dataset_title.size])\n\n#label the x-axis instances\nax.set_xticks([1, 2, 3, 4, 5, 6])\nax.set_xticklabels([\"one\", \"two\", \"three\", 'four', 'five', 'six'])\n\n# set the title and the xy-axis labels\nplt.title(\"Number of Words in Dataset Title\")\nplt.xlabel(\"Number of Words\")\nplt.ylabel(\"Dataset Title\")\n\n# display the plot\nplt.show()","b4c9bef3":"train_df['cleaned_label'].unique()","5c129dad":"train_df['cleaned_label'].value_counts().head(20).to_frame()","0e5a605c":"#create a frequency distribution to see which words are used the most\nwords = list( train_df['cleaned_label'].values)\nstopwords = stopwords_list\nsplit_words = []\nfor word in words:\n    lo_w = []\n    list_of_words = str(word).split()\n    for w in list_of_words:\n        if w not in stopwords:\n            lo_w.append(w)\n    split_words.append(lo_w)\nallwords = []\nfor wordlist in split_words:\n    allwords += wordlist\n    \n#get 100 most common words\nmostcommon = FreqDist(allwords).most_common(100)\nmostcommon","9f8dcf3f":"#plot frequency distributions\nwordcloud = WordCloud(width = 1600, height = 800, \n                      background_color = 'black', \n                      colormap = 'Spectral', \n                      stopwords = stopwords_list).generate(str(mostcommon))\n\nfig = plt.figure(figsize = (20, 10), facecolor = 'white')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.title('Top 100 Most Common Words in cleaned_label', fontsize = 30)\nplt.tight_layout()\n\n#save\nplt.savefig('cleaned_label_wordcloud.png')","7e1a4111":"plt.figure(figsize = (30, 40)),\n\nsns.countplot(y = train_df['cleaned_label'], \n              order = train_df['cleaned_label'].value_counts().index, \n              palette = 'Spectral')\nplt.ylabel('Cleaned Label',fontsize = 30)\nplt.show()\n\n#save\nplt.savefig('cleaned_label.png')","bcab6ccc":"from sklearn.feature_extraction.text import CountVectorizer\n\n#get bigrams \nvectorizer = CountVectorizer(ngram_range = (2, 2))\n\n#matrix of ngrams\nngrams = vectorizer.fit_transform(train_df['cleaned_label']) \nfeatures = (vectorizer.get_feature_names())\nprint('\\n\\nFeatures : \\n', features)\n\n#count frequency of ngrams\nprint('\\n\\nX1 : \\n', ngrams.toarray())\n  \n#apply TFIDF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(ngram_range = (2, 2))\nngrams = vectorizer.fit_transform(train_df['cleaned_label'])\nscores = (ngrams.toarray())\nprint('\\n\\nScores : \\n', scores)\n  \n#get top ranking features\nsums = ngrams.sum(axis = 0)\ndata1 = []\nfor col, term in enumerate(features):\n    data1.append( (term, sums[0,col] ))\nranking = pd.DataFrame(data1, columns = ['term','rank'])\nwords = (ranking.sort_values('rank', ascending = False))\nprint ('\\n\\nWords head : \\n', words.head(20))","a225756e":"#count frequency of ngrams\ncount_values = ngrams.toarray().sum(axis = 0)\n\n#list of ngrams\nvocab = vectorizer.vocabulary_\ndf_bigram = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()], reverse = True)\n            ).rename(columns = {0: 'frequency', 1: 'bigram'})\n\nplt.figure(figsize = (20, 10))\nsns.lineplot(x = df_bigram['bigram'][:60], y = df_bigram['frequency'][:60])\nplt.xticks(rotation = 90, fontsize = 16)\nplt.xlabel('Bigram',fontsize = 20)\nplt.ylabel('Frequency',fontsize = 20)\nplt.title('Cleaned Label Bigram',fontsize = 30)\nplt.show()\n\n#save\nplt.savefig('cleaned_label bigram.png')","6423785a":"#get trigrams \nvectorizer = CountVectorizer(ngram_range = (3, 3))\n\n#matrix of ngrams\nngrams = vectorizer.fit_transform(train_df['cleaned_label']) \nfeatures = (vectorizer.get_feature_names())\nprint('\\n\\nFeatures : \\n', features)\n\n#count frequency of ngrams\nprint('\\n\\nX1 : \\n', ngrams.toarray())\n  \n#apply TFIDF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(ngram_range = (3, 3))\nngrams = vectorizer.fit_transform(train_df['cleaned_label'])\nscores = (ngrams.toarray())\nprint('\\n\\nScores : \\n', scores)\n  \n#get top ranking features\nsums = ngrams.sum(axis = 0)\ndata1 = []\nfor col, term in enumerate(features):\n    data1.append( (term, sums[0,col] ))\nranking = pd.DataFrame(data1, columns = ['term','rank'])\nwords = (ranking.sort_values('rank', ascending = False))\nprint ('\\n\\nWords head : \\n', words.head(20))","f51bfdbb":"#count frequency of ngrams\ncount_values = ngrams.toarray().sum(axis = 0)\n\n#list of ngrams\nvocab = vectorizer.vocabulary_\ndf_trigram = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()], reverse = True)\n            ).rename(columns = {0: 'frequency', 1:'trigram'})\n\nplt.figure(figsize = (20, 10))\nsns.lineplot(x = df_trigram['trigram'][:60], y = df_trigram['frequency'][:60])\nplt.xticks(rotation = 90, fontsize = 16)\nplt.xlabel('Trigram',fontsize = 20)\nplt.ylabel('Frequency',fontsize = 20)\nplt.title('Cleaned Label Trigram',fontsize = 30)\nplt.show()\n\n#save\nplt.savefig('cleaned_label trigram.png')","3cbacbc9":"#find one-worded, two-worded, three-worded, four-worded, five-worded dataset title\none_worded_cleaned_label = train_df[train_df['cleaned_label'].str.split().apply(len) == 1]\ntwo_worded_cleaned_label = train_df[train_df['cleaned_label'].str.split().apply(len) == 2]\nthree_worded_cleaned_label = train_df[train_df['cleaned_label'].str.split().apply(len) == 3]\nfour_worded_cleaned_label = train_df[train_df['cleaned_label'].str.split().apply(len) == 4]\nfive_worded_cleaned_label = train_df[train_df['cleaned_label'].str.split().apply(len) == 5]\n\n#create a bar plot\nfig, ax = plt.subplots(figsize = (10, 6))\nax.bar([1, 2, 3, 4, 5], [one_worded_cleaned_label.size,\n                         two_worded_cleaned_label.size,\n                         three_worded_cleaned_label.size,\n                         four_worded_cleaned_label.size,\n                         five_worded_cleaned_label.size])\n\n#label the x-axis instances\nax.set_xticks([1, 2, 3, 4, 5])\nax.set_xticklabels([\"one\", \"two\", \"three\", 'four', 'five'])\n\n# set the title and the xy-axis labels\nplt.title(\"Number of Words in Cleaned Label\")\nplt.xlabel(\"Number of Words\")\nplt.ylabel(\"Cleaned Label\")\n\n# display the plot\nplt.show()","cc9bd8e9":"from textblob import TextBlob\n\n#get texxt sentiment\ntrain_df['text_sentiment'] = train_df['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n\n#review\ntrain_df.head(5)","027ce03c":"from textblob import TextBlob\n\n#get text tag\ntrain_df['dataset_title_tag'] = train_df['dataset_title'].apply(lambda x: TextBlob(x).tags)\n\n#review\ntrain_df.head(5)","c3056d4c":"#save\ntrain_df.to_csv('train_df_tag.csv')","2b3a0f89":"Within the first section, this publication mentions that they used data from the National Education Longitudinal Study. So the task of this competition is to find string of 'dataset_title' within the 'text' body and return 'cleaned_label'.","1670fa12":"## TriGram\n\nTrigram is 3 consecutive words in a sentence. ","6d2df61e":"The same 'Id' and 'pub_title' of sample 14798 and 14799 are labeled differently as 'survey of earned doctorates' and 'national center for science and engineering' although they are the same dataset_title.","fdd2d9fd":"# SCRUB","1fa0fe26":"'survey of science and engineering research' dataset_title is labeled differently as 'survey of science and engineering research' and 'national center for science and engineering'","067397b5":"### Train Set","cffb3ab6":"### Basic Text Cleaning\n\nBefore we can create a bag of words or vectorize each document, we need to clean it up and split each document into an array of individual words. Computers are very particular about strings. If we tokenized our data in its current state, we would run into the following problems:\n\n* Counting things that aren't actually words. \n* Punctuation and capitalization would mess up our word counts. We need to remove punctuation and capitalization, so that all words will be counted correctly.","772bd0d9":"The classes are highly imbalanced.","2f64b3eb":"# Libraries","0c271587":"### Read Submission Data","f6efc003":"## Look At Each Feature Individually","eee5a9cb":"## TriGram","2a9e0769":"## BiGram\n\nAn n-gram means a sequence of n-words.\n\nSome English words occur together more frequently. So, in a text document we may need to identify such pair of words which will help in sentiment analysis. \n\nBigram is 2 consecutive words in a sentence.","b47a11a5":"## Look At Dataset Metrics","79a28a2f":"### Read Samples","64da5fc2":"### 'cleaned_label'","cbfebd8e":"# Notebooks that I Adapted:\n\n* https:\/\/www.kaggle.com\/dineshkumaranbalagan\/descriptive-analysis\n\n* https:\/\/www.kaggle.com\/mlconsult\/score-57ish-with-additional-govt-datasets\n\n* https:\/\/www.kaggle.com\/armandmorin\/show-us-data\n","4eaf406b":"# Introduction\n\n## Goal\n\n* The end goal is to do string matching of known datasets names in order to detect mentions of datasets in scientific publications.\n* To build a strong NLP model that can infer from context whether or not a piece of text in a publication is refering to the usage of a dataset or not.","c59bf8ba":"# OBTAIN","6ff9fe3d":"### Frequency Distributions","593170fe":"# SENTIMENT ANALYSIS","842e2143":"# EXPLORE","a81b993b":"### 'Id'","b3a43b92":"TextBlob is another excellent open-source library for performing NLP tasks with ease, including sentiment analysis. It also an a sentiment lexicon (in the form of an XML file) which it leverages to give both polarity and subjectivity scores. The polarity score is a float within the range [-1.0, 1.0]. The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.","cca99153":"### 'pub_title'","0e390ed1":"# Data Description\n\n         - train.csv- CSV file contains metadata of the publications\n         - train-JSON file contains publications that are referenced in train.csv\n         - test-CSV file contains publications for testing purpose\n         - sample_submission.csv-CSV file conatins publications IDs column and prediction columns\n\n**id** - publication id - note that there are multiple rows for some training documents, indicating multiple mentioned datasets.\n\n**pub_title** -title of the publication (a small number of publications have the same title).\n\n**dataset_title** -the title of the dataset that is mentioned within the publication.\n\n**dataset_label** -a portion of the text that indicates the dataset.\n\n**cleaned_label** -the dataset_label, as passed through the clean_text function from the Evaluation page.\n\n**PredictionString** -To be filled with equivalent of cleaned_label of train data","60060a45":"## BiGram","d3c83a7f":"The Train dataset has 19,661 counts but only 14,316 unique 'Id' in the dataset. This means some 'Id' are duplicates, meaning some 'Id' use multiple datasets.\n\nThe 'pub_title' has 19,661 counts but has only 14,271 unique titles. This means some 'pub_titles' are duplicates. There are less 'pub_title' counts than 'Id' counts, meaning some 'pub_title' has multiple 'Id'.\n\nThe 'dataset_title' has 19,661 counts but has only 45 unique titles. This means some 'dataset_title' are used many times by different publications.\n\nThe 'dataset_label' has 19,661 counts but has only 130 unique labels. This means some 'dataset_label' are duplicates. There are less 'dataset_title' counts than 'dataset_label', meaning some 'dataset_title' are labeled differently by different publications.","11a38736":"### 'dataset_title'","e178263c":"Here we see with samples 1450 and 12456, there are same 'text' with same 'Id' and same 'pub_title' but labeled different."}}