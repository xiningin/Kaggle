{"cell_type":{"c8531d9a":"code","dfe35df2":"code","73a96c4c":"code","5e88b661":"markdown"},"source":{"c8531d9a":"# refrence: https:\/\/www.kaggle.com\/dborkan\/benchmark-kernel\nclass SubmetricsAUC(object):\n    def __init__(self, valid_df, pred_y):\n        self.identity_columns = ['male', 'female', 'homosexual_gay_or_lesbian', 'christian',\n                                 'jewish', 'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n        self.SUBGROUP_AUC = 'subgroup_auc'\n        self.BPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\n        self.BNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n\n        self.TOXICITY_COLUMN = 'target'\n\n        self.valid_df = valid_df\n        self.pred_y = pred_y\n        self.model_name = 'pred'\n        self.valid_df[self.model_name] = self.pred_y\n        self.valid_df = self.convert_dataframe_to_bool(self.valid_df)\n\n    def compute_auc(self):\n\n        bias_metrics_df = self.compute_bias_metrics_for_model(self.identity_columns,\n                                                              self.model_name,\n                                                              self.TOXICITY_COLUMN).fillna(0)\n\n        final_score = self.get_final_metric(bias_metrics_df, self.calculate_overall_auc())\n\n        return final_score\n\n    @staticmethod\n    def power_mean(series, p):\n        total = sum(np.power(series, p))\n        return np.power(total \/ len(series), 1 \/ p)\n\n    @staticmethod\n    def calculate_auc(y_true, y_pred):\n        try:\n            return roc_auc_score(y_true, y_pred)\n        except ValueError:\n            return np.nan\n\n    @staticmethod\n    def convert_to_bool(df, col_name):\n        df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n\n    def convert_dataframe_to_bool(self, df):\n        bool_df = df.copy()\n        for col in ['target'] + self.identity_columns:\n            self.convert_to_bool(bool_df, col)\n        return bool_df\n\n    def compute_subgroup_auc(self, subgroup, label, model_name):\n        subgroup_examples = self.valid_df[self.valid_df[subgroup]]\n        return self.calculate_auc(subgroup_examples[label], subgroup_examples[model_name])\n\n    def compute_bpsn_auc(self, subgroup, label, model_name):\n        \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n        subgroup_negative_examples = self.valid_df[self.valid_df[subgroup] & ~self.valid_df[label]]\n        non_subgroup_positive_examples = self.valid_df[~self.valid_df[subgroup] & self.valid_df[label]]\n        examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n        return self.calculate_auc(examples[label], examples[model_name])\n\n    def compute_bnsp_auc(self, subgroup, label, model_name):\n        \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n        subgroup_positive_examples = self.valid_df[self.valid_df[subgroup] & self.valid_df[label]]\n        non_subgroup_negative_examples = self.valid_df[~self.valid_df[subgroup] & ~self.valid_df[label]]\n        examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n        return self.calculate_auc(examples[label], examples[model_name])\n\n    def compute_bias_metrics_for_model(self,\n                                       subgroups,\n                                       model,\n                                       label_col,\n                                       include_asegs=False):\n        \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n        records = []\n        for subgroup in subgroups:\n            record = {'subgroup': subgroup,\n                      'subgroup_size': len(self.valid_df[self.valid_df[subgroup]])\n                      }\n\n            record[self.SUBGROUP_AUC] = self.compute_subgroup_auc(subgroup, label_col, model)\n            record[self.BPSN_AUC] = self.compute_bpsn_auc(subgroup, label_col, model)\n            record[self.BNSP_AUC] = self.compute_bnsp_auc(subgroup, label_col, model)\n            records.append(record)\n        return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)\n\n    def calculate_overall_auc(self):\n        true_labels = self.valid_df[self.TOXICITY_COLUMN]\n        predicted_labels = self.valid_df[self.model_name]\n        return roc_auc_score(true_labels, predicted_labels)\n\n    def get_final_metric(self, bias_df, overall_auc, power=-5, weight=0.25):\n        bias_score = np.average([\n            self.power_mean(bias_df[self.SUBGROUP_AUC], power),\n            self.power_mean(bias_df[self.BPSN_AUC], power),\n            self.power_mean(bias_df[self.BNSP_AUC], power)\n        ])\n        return (weight * overall_auc) + ((1 - weight) * bias_score)","dfe35df2":"# # just like this\n# score = SubmetricsAUC(train_df, train_preds).compute_auc()","73a96c4c":"# # seems like this \n# auc = roc_auc_score(train_y, train_preds)","5e88b661":"## how to use"}}