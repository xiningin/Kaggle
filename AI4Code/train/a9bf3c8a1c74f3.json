{"cell_type":{"b732c8b6":"code","c6b755f6":"code","25908cd1":"code","2c70db58":"code","07299949":"code","0d6c7d4b":"code","984aed26":"code","bc4c6b92":"code","c8f254e8":"code","f6c1cdb0":"code","38383fc5":"code","99766b91":"code","9be86de3":"code","c05de9ff":"code","b152a63d":"code","2a4b9b66":"code","8dd00986":"code","78b3ec80":"code","6a23614e":"code","dfa5eb25":"code","a2f230f5":"code","7be7defa":"code","f165d241":"code","4771ad07":"code","57878e38":"code","b00086be":"code","bdeea3a0":"code","ceec9262":"markdown","36058f98":"markdown","46dbdf81":"markdown","265ed8db":"markdown","00b65b44":"markdown","e8089d42":"markdown","1d51096b":"markdown","89d6004e":"markdown","7b67f104":"markdown","bd9c2010":"markdown","d9e227b7":"markdown","b41a1e14":"markdown","04b39402":"markdown","fa2dfec4":"markdown","86ee194a":"markdown","9c6c9339":"markdown","954394f7":"markdown","91c87c84":"markdown","75cb87ef":"markdown","52639d4c":"markdown","642caad5":"markdown"},"source":{"b732c8b6":"#Libraries\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport os\nimport re\nimport json\nimport string\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom tqdm.autonotebook import tqdm\nfrom functools import partial\nfrom wordcloud import WordCloud, STOPWORDS\nimport nltk\nimport spacy\nnlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\nnlp.max_length = 4000000\nfrom nltk.probability import FreqDist\n","c6b755f6":"train = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/train.csv')\ntrain.head()","25908cd1":"train.columns","2c70db58":"train.info()","07299949":"for col in train.columns:\n    print(col + \":\" + str(len(train[col].unique())))","0d6c7d4b":"sample_sub = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv')\nsample_sub.head()","984aed26":"train_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\ntest_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/test'","bc4c6b92":"def json_to_text(filename, train_files_path=train_files_path, output='text'):\n    json_path = os.path.join(train_files_path, (filename+'.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data","c8f254e8":"tqdm.pandas()\ntrain['text'] = train['Id'].progress_apply(json_to_text)","f6c1cdb0":"train.head()","38383fc5":"tqdm.pandas()\nsample_sub['text'] = sample_sub['Id'].progress_apply(partial(json_to_text, train_files_path=test_files_path))","99766b91":"sample_sub.head()","9be86de3":"def text_cleaning(text):\n    text = ''.join([k for k in text if k not in string.punctuation])\n    text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n    text = re.sub(' +', ' ', text)\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    return text","c05de9ff":"tqdm.pandas()\ntrain['text'] = train['text'].progress_apply(text_cleaning)","b152a63d":"ul = train['cleaned_label'].unique()\nul[0:5]","2a4b9b66":"print('There are {} unique cleaned labels'.format(len(ul)))","8dd00986":"text = ' '.join(train['text'][train['cleaned_label']==ul[0]].sample(frac=0.3))\nwordcloud = WordCloud(background_color='white', stopwords=STOPWORDS, width=2000, height=1200).generate(text)\nbarplot_dim = (15, 15)\nax = plt.subplots(figsize=barplot_dim, facecolor='w')\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title('Top 100 Most Common Words in Publications Text', fontsize=50)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","78b3ec80":"text = ' '.join(train['text'][train['cleaned_label']==ul[10]].sample(frac=0.3))\nwordcloud = WordCloud(background_color='white', stopwords=STOPWORDS, width=2000, height=1200).generate(text)\nbarplot_dim = (15, 15)\nax = plt.subplots(figsize=barplot_dim, facecolor='w')\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title('Top 100 Most Common Words in Publications Text', fontsize=50)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","6a23614e":"text = ' '.join(train['text'][train['cleaned_label']==ul[100]].sample(frac=0.3))\nwordcloud = WordCloud(background_color='white', stopwords=STOPWORDS, width=2000, height=1200).generate(text)\nbarplot_dim = (15, 15)\nax = plt.subplots(figsize=barplot_dim, facecolor='w')\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.title('Top 100 Most Common Words in Publications Text', fontsize=50)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","dfa5eb25":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\ndef cosine(train, test):\n    \"\"\"\n    Enter text one from each train and test set for cosine similarity\n    \"\"\"\n    sw = stopwords.words('english')\n    X_list = word_tokenize(train)\n    Y_list = word_tokenize(test)\n    l1 =[];l2 =[]\n    # remove stop words from the string\n    X_set = {w for w in X_list if not w in sw} \n    Y_set = {w for w in Y_list if not w in sw}\n    # form a set containing keywords of both strings \n    rvector = X_set.union(Y_set) \n    for w in rvector:\n        if w in X_set: l1.append(1) # create a vector\n        else: l1.append(0)\n        if w in Y_set: l2.append(1)\n        else: l2.append(0)\n    c = 0\n        # cosine formula \n    for i in range(len(rvector)):\n        c+= l1[i]*l2[i]\n    cosine = c \/ float((sum(l1)*sum(l2))**0.5)\n    return cosine","a2f230f5":"for j in range(0,len(sample_sub['text'])):\n    print('Similarity of first train text with text {} from test data'.format(j))\n    print(cosine(train['text'][0], sample_sub.loc[j,'text']))","7be7defa":"for j in range(0,len(sample_sub['text'])):\n    print('Similarity of second train text with text {} from test data'.format(j))\n    print(cosine(train['text'][1], sample_sub.loc[j,'text']))","f165d241":"def jaccard_similarity(text_a, text_b):\n    word_set_a, word_set_b = [set(text.split())\n                              for text in [text_a, text_b]]\n    num_shared = len(word_set_a & word_set_b)\n    num_total = len(word_set_a | word_set_b)\n    return num_shared \/ num_total","4771ad07":"for j in range(0,len(sample_sub['text'])):\n    print('Similarity of first train text with text {} from test data'.format(j))\n    similarity = jaccard_similarity(sample_sub.loc[j,'text'], train['text'][0])\n    print(similarity)","57878e38":"for j in range(0,len(sample_sub['text'])):\n    print('Similarity of second train text with text {} from test data'.format(j))\n    similarity = jaccard_similarity(sample_sub.loc[j,'text'], train['text'][1])\n    print(similarity)","b00086be":"import difflib\n\nfor j in range(0,len(sample_sub['text'])):\n    print('Similarity of first train text with text {} from test data'.format(j))\n    d = difflib.SequenceMatcher(None, sample_sub.loc[j,'text'], train['text'][0])\n    similarity = d.ratio()*100\n    print(similarity)","bdeea3a0":"for j in range(0,len(sample_sub['text'])):\n    print('Similarity of second train text with text {} from test data'.format(j))\n    d = difflib.SequenceMatcher(None, sample_sub.loc[j,'text'], train['text'][1])\n    similarity = d.ratio()*100\n    print(similarity)","ceec9262":"# UPVOTE if you like this notebook.\n## Thanks","36058f98":"### Here is wordcloud based on texts from 10th unique label","46dbdf81":"<b>Let's see the Train Data now<\/b>","265ed8db":"### SequenceMatcher from difflib","00b65b44":"### Here is wordcloud based on texts from 100th unique label","e8089d42":"<h3><b>Data Description<\/b><\/h3>\n<p><\/p>\n<ul>\n    <li><b>id-<\/b> publication id - note that there are multiple rows for some training documents, indicating multiple mentioned datasets.<\/li>\n    <li><b>pub_title-<\/b>title of the publication (a small number of publications have the same title).<\/li>\n    <li><b>dataset_title-<\/b>the title of the dataset that is mentioned within the publication.<\/li>\n    <li><b>dataset_label-<\/b>a portion of the text that indicates the dataset.<\/li>\n    <li><b>cleaned_label-<\/b>the dataset_label, as passed through the clean_text function from the Evaluation page.<\/li>\n<\/ul>","1d51096b":"<b>Sample Submission<\/b>","89d6004e":"<h4>Inference<\/h4>\n\n\n- The Training Dataset has 19,661 samples but only 14,316 unique IDs in the dataset. This means that some publications include a multitude of datasets. \n\n\n- The pub_title unique count is also less than the Id unique counts. This points to the precense of several occurences of having 2 separate publications, each with a unique ID, but sharing the exact same title.\n\n\n- Also, there are a total of 45 unique dataset_title and 130 unique dataset_label. It means that a single dataset could have multible labels throughout different publications.","7b67f104":"<b>Now apply the function to submission Data<\/b>","bd9c2010":"<h3><b>Data Description<\/b><\/h3>\n<p><\/p>\n<ul>\n    <li><b>id-<\/b> publication id <\/li>\n    <li><b>PredictionString-<\/b>To be filled with equivalent of cleaned_label of train data..<\/li>\n    \n<\/ul>","d9e227b7":"### Cosine similarity function","b41a1e14":"<h3><center>EDA with Visualization<\/h3>","04b39402":"### Jaccard similarity","fa2dfec4":"<b>So we have no 'NULL' values in the train data<\/b>","86ee194a":"### Here is wordcloud based on texts from first unique label","9c6c9339":"<b>Create a function to Preprocess the data using Basic NLP Filters (all text to lower case, Removes special charecters, emojis and multiple spaces)<\/b>","954394f7":"There are several other distance\/similarity measures. Many of them available in package like sklean, spacy, nltk etc.","91c87c84":"<b>Now we will create a function to get the text from the JSON file and append it to the new column in table<\/b>","75cb87ef":"<h3><center>Some Similarity Measures<\/h3>","52639d4c":"<h3><b>Data Processing<\/b><\/h3>","642caad5":"<b>Train Data Exploration<\/b>"}}