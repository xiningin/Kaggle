{"cell_type":{"83b19b83":"code","46054363":"code","826eca01":"code","c4fa85f3":"code","6ccc7628":"code","8d2fba6c":"code","bdd29957":"code","e208c902":"code","fa212e15":"code","b504c472":"code","3dc4685d":"code","23cdcb7e":"code","e98e661d":"code","50516bfa":"code","2552078c":"code","609ec67c":"code","f997f963":"code","50404f6f":"code","b7cad229":"code","a08c07d5":"code","7cbcbf43":"code","e6e25dfc":"code","72fd51a9":"code","92582a83":"code","8375d561":"code","d5c2f34e":"code","68f5a799":"code","ed23f479":"code","bef1169f":"code","bf7f8a81":"code","2b649c73":"code","7c7b6e54":"code","6201b992":"code","c1661c25":"code","57144743":"markdown","b62859b0":"markdown","fdf159e4":"markdown","023c28a0":"markdown","b696dd54":"markdown","d607eb8b":"markdown","ca7ed321":"markdown","0239f1da":"markdown","07078eb4":"markdown","7284cd97":"markdown","6936fb01":"markdown","91666812":"markdown","6f0e8789":"markdown","c97b4685":"markdown","b9436d57":"markdown","543b8e78":"markdown","03832ce1":"markdown","04927ae4":"markdown"},"source":{"83b19b83":"# imports\nimport os\nimport shutil\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nfrom urllib.parse import urlparse\nfrom collections import defaultdict\n\n# data preparation\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\n# text cleaning and preprocessing\nimport re\nimport string\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk import pos_tag\nfrom sklearn.preprocessing import LabelEncoder\n\n# tensorflow imports\nfrom tensorflow import keras\nfrom kerastuner.tuners import RandomSearch\nfrom kerastuner.engine.hyperparameters import HyperParameters\nfrom keras.utils import np_utils\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow import keras\n\n# plotting\nimport matplotlib.pyplot as plt\n\n# evaluation\nfrom sklearn.metrics import accuracy_score\n\n\nRANDOM_STATE = 123\nTRAIN_SET_RATIO = 0.8\nTEST_SET_RATIO = 0.05\nVAL_SET_RATIO = 0.15","46054363":"news_data = pd.read_json('\/kaggle\/input\/news-category-dataset\/News_Category_Dataset_v2.json', lines=True)\nnews_data.info()","826eca01":"# sample data\nnews_data.sample(5, random_state=RANDOM_STATE)","c4fa85f3":"# number of unique elements \nnews_data.nunique()","6ccc7628":"print('Unique category: ', news_data.category.unique())","8d2fba6c":"news_data.category = news_data.category.map(lambda x: \"WORLDPOST\" if x == \"THE WORLDPOST\" else x)","bdd29957":"# news time period\nprint('News Time Period from {} to {}.'.format(min(news_data.date), max(news_data.date)))","e208c902":"# check where the news comes from\nparse_url = lambda url : urlparse(url).netloc\ndomain_series = news_data.link.apply(parse_url)\nprint('Unique domains: ', domain_series.unique())","fa212e15":"# check authors name\nnews_data.authors.value_counts()","b504c472":"# do specific authors write for a specific categories?\nt = news_data[['category', 'authors']].groupby(['authors'])\nt.head()","3dc4685d":"# make text corpus\ncorpus = pd.DataFrame()\ncorpus['text'] = news_data.headline\ncorpus['category'] = news_data.category","23cdcb7e":"# check for empty strings\nprint(corpus.text.apply(len).value_counts().sort_index())","e98e661d":"# delete rows with empty headline\ncorpus = corpus[corpus.text.apply(len) > 0]","50516bfa":"# check category distribution\ncorpus.category.value_counts()","2552078c":"# probability of category occurance\np = corpus.category.value_counts() \/ corpus.category.shape[0]\n\n# class labels\nlabels = corpus.category.value_counts().index\n\n# map class to number\nlabel_map = {}\nfor i in range(len(labels)):\n    label_map[labels[i]] = i\n\n# randomly sample labels and news data to calculate accuracy\ntest_set_size = int(corpus.shape[0] * TEST_SET_RATIO)\nbaseline_acc = []\n\n# simulate cross validation with for loop\nfor i in range(10):\n    # get some random test set\n    gt = corpus.category.sample(test_set_size).apply(lambda x : label_map[x])\n    # random prediction\n    prediction = np.random.choice(list(label_map.values()), test_set_size, p=p)\n    baseline_acc += [accuracy_score(gt, prediction)]\n    \n\nprint('Random Guess Accuracy CV 10: ', baseline_acc)\nprint('Random Guess Accuracy Mean: ', np.mean(baseline_acc))\nprint('Random Guess Accuracy Std: ', np.std(baseline_acc))","609ec67c":"# remove punctuation\nrm_punctuation = lambda x: x.translate(str.maketrans('', '', string.punctuation + \"\\'\\n\\r\\t\"))\n\n# remove stopwords for english\nstop_words = set(stopwords.words('english'))\nrm_stopwords = lambda x: ' '.join([word for word in x.split() if word not in stop_words])\n\n# preprocess text\ndef preprocess_text(text):\n    text = text.lower()\n    text = rm_punctuation(text)\n    text = rm_stopwords(text)\n    return text","f997f963":"# encode categories\ncategory_encoder = LabelEncoder()\ncategory_encoder.fit(corpus.category)\n\ncorpus['text_preprocessed'] = corpus.text.apply(preprocess_text)\ncorpus['category_enc'] = category_encoder.transform(corpus.category)","50404f6f":"corpus.sample(10, random_state=RANDOM_STATE)","b7cad229":"# make sure that after cleaning the text there are no empty strings\ncorpus = corpus[corpus.text_preprocessed.apply(len) > 0]","a08c07d5":"def lemmatize_text(text):\n        to_wordnet_tag = defaultdict(lambda : wordnet.NOUN)\n        # to_wordnet_tag['N'] = wordnet.NOUN\n        to_wordnet_tag['V'] = wordnet.VERB\n        to_wordnet_tag['J'] = wordnet.ADJ\n        to_wordnet_tag['R'] = wordnet.ADV\n        \n        lemmatizer = WordNetLemmatizer()\n        lemmata = [lemmatizer.lemmatize(word, pos=to_wordnet_tag[tag[0]]) for (word, tag) in text]\n        return lemmata\n    \ndef preprocessing_pipeline(document):\n    \"\"\"\n    @params\n    - document: (n,1) one dimensional shaped series were each entry is a string\n    \n    @return\n    - return document with preprocessed columns\n    \"\"\"\n    df = document.to_frame()\n    print('Tokenize Text...')\n    df['tokens'] = document.apply(word_tokenize)\n    \n    print('Build Part of Speech Tags...')\n    df['pos'] = df.tokens.apply(pos_tag)\n    \n    print('Build Lemmata...')\n    df['lemma'] = df.pos.apply(lemmatize_text)\n    \n    return df","7cbcbf43":"# preprocess text\nX_data = preprocessing_pipeline(corpus.text_preprocessed)","e6e25dfc":"# merge normalized text into corpus\ncorpus = pd.merge(X_data, corpus)\ncorpus.sample(5, random_state=RANDOM_STATE)","72fd51a9":"# split data into training and testing\n# use lemma\nX_train, X_test, y_train, y_test = train_test_split(\n    corpus.lemma,\n    corpus.category_enc,\n    train_size = TRAIN_SET_RATIO,\n    shuffle = True,\n    random_state = RANDOM_STATE\n)\n\n# One Hot encode the classes\ny_train_categorical = np_utils.to_categorical(y_train)\n\n# reshape X_train for further processing\nX_train = X_train.to_numpy().reshape(-1,)\n\nprint('X, y train shape: ', X_train.shape, y_train_categorical.shape)\nprint('X, y test shape: ', X_test.shape, y_test.shape)","92582a83":"# vector representation for lemmata\nkeras_tokenizer = Tokenizer()\nkeras_tokenizer.fit_on_texts(X_train)\nX_train_sequences = keras_tokenizer.texts_to_sequences(X_train)\n\n# padding: how long is the longest sentence?\nmax_sent_length = max([len(doc) for doc in corpus.lemma])\nprint('Maximum Sentence Length: ', max_sent_length)\n\n# pad data to be of uniform length\nX_train_padded = pad_sequences(X_train_sequences, max_sent_length, padding='post')\nprint(\"Training Data Shape: \", X_train_padded.shape, y_train_categorical.shape)","8375d561":"# Number of unique tokens\nword2index = keras_tokenizer.word_index\nprint(\"Number of unique tokens : \",len(word2index))\n\n# load embeddings\nprint('Loading word vectors...')\nword2vec = {}\n\n# choose dimension\n# 50 turned out to provide a better learning curve\ndimension = 50\n\nwith open(os.path.join('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.{}d.txt'.format(dimension)), encoding = \"utf-8\") as f:\n  # is just a space-separated text file in the format:\n  # word vec[0] vec[1] vec[2] ...\n    for line in f:\n        values = line.split() #split at space\n        word = values[0]\n        vec = np.asarray(values[1:], dtype='float32') #numpy.asarray()function is used when we want to convert input to an array.\n        word2vec[word] = vec\nprint('Found %s word vectors.' % len(word2vec))\n\n# create embedding matrix\nembedding_matrix = np.zeros((len(word2index)+1, dimension))\nembedding_vec=[]\nfor word, i in tqdm(word2index.items()):\n    embedding_vec = word2vec.get(word)\n    if embedding_vec is not None:\n        embedding_matrix[i] = embedding_vec","d5c2f34e":"# construct neural network model\ndef build_model():    \n    model = keras.models.Sequential([\n        keras.layers.Embedding(len(word2index)+1,dimension,weights=[embedding_matrix],input_length=max_sent_length, trainable=False),\n        keras.layers.Conv1D(\n            80,\n            activation = 'relu',\n            kernel_size = 3,\n            padding = 'same'\n        ),\n        keras.layers.MaxPool1D(pool_size=3),\n        keras.layers.Dropout(0.25),\n        keras.layers.Flatten(),\n        keras.layers.Dense(40, activation='softmax'),\n    ])\n    \n    optimizer = keras.optimizers.Adam(lr = 0.0001)\n    \n    model.compile(\n        loss='categorical_crossentropy',\n        optimizer=optimizer,\n        metrics=['accuracy']\n    )\n    \n    return model","68f5a799":"model = build_model()\n\nprint(model.summary())\n\nhist = model.fit(\n    X_train_padded,\n    y_train_categorical,\n    validation_split = VAL_SET_RATIO,\n    epochs = 500,\n    batch_size = 5,\n    shuffle = True,\n    callbacks = [keras.callbacks.EarlyStopping('val_accuracy', mode = 'max', patience = 2)]\n)","ed23f479":"# plot accuracy learning curve\nplt.plot(hist.history['accuracy'])\nplt.plot(hist.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='best')\nplt.show()","bef1169f":"# plot loss learning curve\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='best')\nplt.show()","bf7f8a81":"# use the tokenizer for the training data\n# because this is the data you know about\nX_test_sequence = keras_tokenizer.texts_to_sequences(X_test)\nX_test_padded = pad_sequences(X_test_sequence, max_sent_length, padding = 'post')\n\n# make predictions\npredictions = np.argmax(model.predict(X_test_padded), axis=1)\n\n# Accuracy of the model\nscore = accuracy_score(predictions, y_test.to_numpy())\nprint('Accuracy Score on test data: ', score)","2b649c73":"# delete logs\nproject_name = 'random_search_hyper_opt'\nwork_dir = '\/kaggle\/working\/'\nif os.path.exists(os.path.join(work_dir, project_name)):\n    shutil.rmtree(os.path.join(work_dir, project_name))","7c7b6e54":"def build_model_optimization():\n    model = keras.models.Sequential([\n        keras.layers.Embedding(len(word2index)+1,dimension,weights=[embedding_matrix],input_length=max_sent_length, trainable=False),\n        keras.layers.Conv1D(\n            hp.Int('Conv1d Units', 10, 510, 50, default = 210),\n            activation = 'relu',\n            kernel_size = hp.Choice('Conv1d Kernel Size', [3,5,7], default = 5),\n            padding = 'same'\n        ),\n        keras.layers.MaxPool1D(pool_size=1),\n        keras.layers.Dropout(hp.Float(\"Dropout Rate\", 0, 0.3, 0.1, default = 0)),\n        keras.layers.Flatten(),\n        keras.layers.Dense(40, activation='softmax'),\n    ])\n    \n    learning_rate = hp.Choice('Learning Rate', [0.0001, 0.001, 0.01], default = 0.001)\n    optimizer = keras.optimizers.Adam(lr = learning_rate)\n    \n    model.compile(\n        loss='categorical_crossentropy',\n        optimizer=optimizer,\n        metrics=['accuracy']\n    )\n    \n    return model","6201b992":"# Initialize Random Search\n# tuner = RandomSearch(\n#     build_model,\n#     objective = 'val_accuracy',\n#     max_trials = 200,\n#     executions_per_trial = 1,\n#     project_name = 'random_search_hyper_opt',\n#     seed = RANDOM_STATE\n# )\n\n# # Run Random Search\n# tuner.search(\n#     x = X_train_padded,\n#     y = y_train_categorical,\n#     epochs = 10,\n#     batch_size = 10,\n#     validation_data = (X_val_padded, y_val_categorical),\n#     shuffle=True,\n#     callbacks=[keras.callbacks.EarlyStopping('val_accuracy', mode='max')]\n# )","c1661c25":"# get the best performing model\n# model = tuner.get_best_models(1)[0]\n\n# show a summary result\n# print(tuner.results_summary())","57144743":"Note thate there is the category THE WORLDPOST and WORLDPOST. It is the same category, so it needs to be substituted.","b62859b0":"# Data Description\nThe data consists of a collection of news where each news entry is represented by the following columns:\n* category: Category of the news (string)\n* headline: Headline of the news (string)\n* author: Authors and Contributors or Associations (string)\n* short_description: Short description of the news' content (string)\n* date: Date of the published news. (date)\n\nThe goal is to classify the news category given only the headline. An accuracy of 90% on the test set is aimed.","fdf159e4":"# Data Inspection\nBefore startint to work towards a solution it is important to inspect the data. It is not only important to understand what the data represents and how it does so, but also to check for missing values, feature distribution and more.","023c28a0":"## Prepare Embeddings","b696dd54":"## Evaluate Model on Test Data\nThe model has been trained with the validation data. Lt's see how the model performs on data that it has not seen before. ","d607eb8b":"## Define Model\nNow that the data has been splitted and word embeddings has been prepared it is time to define and train a model.","ca7ed321":"There seems to be empty string that are not represented as null values.","0239f1da":"# Baseline Model - Random Guess\nCreating a Machine Learning model is all about simplicity. Generally you prefer a simpler model that generalizes better rather than a big complex model even if the simpler one performs slightly worse.  \nTherefore, a very simple baseline model works as comparison. The strategy will be simple. The model outputs a random category with the probability equal to the occurance of the category. It is just a random guess.","07078eb4":"## Text Preprocessing\nBefore text can be fed to a model the data needs to be preprocessed. This process involves e.g. tokenization and lemmatization. Tokens can be on different levels, like word or character. Let's consider each word to be a token.  \nIn languages there exist different word forms like plural or singualr. Stemming or Lemmatization takes care of this and normallizes the tokens.","7284cd97":"# Build Model\nAs said before try to keep it simple with models. I already tried a SVM Classifier. However, it performed so bad that I decided to not not proceed any further and swith to a Neural Network.  \nIn research there are various architectures for text classification that e.g. use LSTM or GRU Layers. Here, I choose an architecture with Word Embeddings and a Convolutional Layer. At the end of the model there is one or more fully connected layers with a SoftMax activation for the output with dimension (number of classes, 1). The SoftMax activation is responsible to provide a probability distribution over the classes. The highest value would be the final decision.","6936fb01":"Knowing the categories you can always guess a category randomly. But, you can see that a random guess performs really bad with an average accuracy (ACC) of around 6%.","91666812":"## Optimize Parameters (Optional)\nNo one can guess good parameters for a model. That is why you should search for them to optimize your model. You can use a Gaussican Process which should be preferred. For simplicity, let's stick to a Random Search. Plus, it is up to you which parameter you want to optimize. Just note, that it takes more time the more parameter you want to optimize.","6f0e8789":"The distribution of the categories is imbalanced. There are many more news with POLITICAL(32739) than e.g. EDUCATION (1004). The data might be resampled for training.","c97b4685":"Maybe there is some kind of relation between author and the category, e.g. Ron Dicker seem to write articles in the category ENTERTAINMENT.","b9436d57":"There are 6 rows with an empty string (length = 0). They can be removed.","543b8e78":"## Prepare Training, Validation and Testing Data\nIn order to have an idea how well the model works, the data is splitted into training, testing a validation set. The test set is not seen by the model at any point. The validation set is there to fine tune the model parameters. Therefore, it should be similiar to the test set. The training data is applied to train the model, thus it can be altered or augmented respectively.","03832ce1":"# Text Cleaning\nTo use a more sophisticated model the data needs to be cleaned. This involves preproccessing steps e.g. like the removal of stopwords (the, a, what, I, ...) or punctuation (.:,!).","04927ae4":"# Last Thoughts\nWith the current configuration the model achieves around 60% ACC on both train, validation and test set. Higher Learning rates and batch size let the model overfit quickly. Also, the complexity of the model plays a role, too. In general, a lower embedding dimension and filter performed better, meaning that it overfitted less. The ACC on the test set is still higher than the baseline, but there should be room for improvement.  \nOverfitting can be caused by various things. In general, the model is too complex in order to generalize well. One counter measurement is reducing the number of parameters in the model by reducing the size of the layers. Regularization with a dropout rate for the training process can help, too. Further, the training data can also be a cause. It can be imbalanced or not representing the validation\/test data. Increasing\/Decreasing or resampling the data accordingly might help."}}