{"cell_type":{"7245c532":"code","6e2da43d":"code","b9a5c990":"code","a7e4c002":"code","df3176f8":"code","073f1ff2":"code","668d1c19":"code","3ad3b440":"code","bfeadede":"code","17a10ffb":"code","4dfb3f54":"code","8bb3f3ab":"code","f27c4e9b":"code","c3423d6d":"code","a9324f1b":"code","cf468489":"code","7827660f":"code","5923116c":"code","5df1d254":"code","faf05fe5":"code","b811a322":"code","97fcfcee":"code","754de3fe":"code","178b10cf":"code","ae7571aa":"code","a26994aa":"code","31320072":"code","69f19d6c":"code","ed3e26a1":"code","305fbd8d":"code","33aa9731":"code","f038d5fd":"code","2254142d":"code","2c50ba0b":"code","cc0c9acf":"code","6b75899e":"code","097c79b1":"code","6503af65":"code","e37823db":"code","e7711e3f":"code","fe0deac2":"code","fbbc369b":"code","13601ff6":"code","9cf7d72f":"code","3e2cd119":"markdown","5162d77f":"markdown","9a269fc8":"markdown","d5f47e26":"markdown","32a711f1":"markdown","335278ec":"markdown","4fe3d728":"markdown","6f08275d":"markdown","74571131":"markdown","4b01dfbc":"markdown","68f91cd7":"markdown","4dde0fe7":"markdown","4d32c71f":"markdown","b985ce35":"markdown","f5d73155":"markdown","1208d5d5":"markdown","1e28bc3d":"markdown","58f6d7ee":"markdown","0c1f19ca":"markdown","3fe8a6ee":"markdown","c8dae120":"markdown","12b75e8a":"markdown","1ac05ae9":"markdown","0a02120f":"markdown","13c3178b":"markdown","929645d6":"markdown","e43c3696":"markdown","04d7c193":"markdown","20ad6603":"markdown","ef5b6082":"markdown","cad2aaa8":"markdown","2a3a9705":"markdown","2662c0b2":"markdown","de9273ca":"markdown","4af7e2d3":"markdown","9a99be4d":"markdown","1aaee39d":"markdown","827ae954":"markdown","771b8a3b":"markdown","2fe0c547":"markdown","273b2a04":"markdown","544a6279":"markdown","bffb3847":"markdown","c16c2050":"markdown","3095dd97":"markdown","7e95cf18":"markdown","cf1c2810":"markdown","3497623b":"markdown","2902f572":"markdown","67162d59":"markdown","ff2d20e7":"markdown","e997fad9":"markdown","9bbe6ca6":"markdown","c285ff39":"markdown","b85e8871":"markdown","66120264":"markdown","2ac0d990":"markdown","6606b5c3":"markdown","e0c2075b":"markdown","9d46fcac":"markdown","d807b105":"markdown","75cdc30b":"markdown","4b4618f7":"markdown","e5fd0907":"markdown","bb0e9a8b":"markdown","ab9ddcca":"markdown","6f4d58e6":"markdown","7c32beda":"markdown","da2e12c6":"markdown","543b4d7b":"markdown","fd97dabb":"markdown","9db8c597":"markdown","c1e26e1e":"markdown","2558b1c5":"markdown","c00e2142":"markdown"},"source":{"7245c532":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn \n%matplotlib inline","6e2da43d":"#Loading the single csv file to a variable named 'placement'\nplacement=pd.read_csv(\"..\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv\")","b9a5c990":"placement_copy=placement.copy()\nplacement_copy.head()","a7e4c002":"print (\"The shape of the  data is (row, column):\"+ str(placement.shape))\nprint (placement_copy.info())","df3176f8":"#Looking at the datatypes of each factor\nplacement_copy.dtypes","073f1ff2":"import missingno as msno \nmsno.matrix(placement)","668d1c19":"print('Data columns with null values:',placement_copy.isnull().sum(), sep = '\\n')","3ad3b440":"placement_copy['salary'].fillna(value=0, inplace=True)\nprint('Salary column with null values:',placement_copy['salary'].isnull().sum(), sep = '\\n')","bfeadede":"placement_copy.drop(['sl_no','ssc_b','hsc_b'], axis = 1,inplace=True) \nplacement_copy.head()","17a10ffb":"plt.figure(figsize = (15, 10))\nplt.style.use('seaborn-white')\nax=plt.subplot(221)\nplt.boxplot(placement_copy['ssc_p'])\nax.set_title('Secondary school percentage')\nax=plt.subplot(222)\nplt.boxplot(placement_copy['hsc_p'])\nax.set_title('Higher Secondary school percentage')\nax=plt.subplot(223)\nplt.boxplot(placement_copy['degree_p'])\nax.set_title('UG Degree percentage')\nax=plt.subplot(224)\nplt.boxplot(placement_copy['etest_p'])\nax.set_title('Employability percentage')","4dfb3f54":"Q1 = placement_copy['hsc_p'].quantile(0.25)\nQ3 = placement_copy['hsc_p'].quantile(0.75)\nIQR = Q3 - Q1    #IQR is interquartile range. \n\nfilter = (placement_copy['hsc_p'] >= Q1 - 1.5 * IQR) & (placement_copy['hsc_p'] <= Q3 + 1.5 *IQR)\nplacement_filtered=placement_copy.loc[filter]","8bb3f3ab":"plt.figure(figsize = (15, 5))\nplt.style.use('seaborn-white')\nax=plt.subplot(121)\nplt.boxplot(placement_copy['hsc_p'])\nax.set_title('Before removing outliers(hsc_p)')\nax=plt.subplot(122)\nplt.boxplot(placement_filtered['hsc_p'])\nax.set_title('After removing outliers(hsc_p)')","f27c4e9b":"plt.figure(figsize = (15, 7))\nplt.style.use('seaborn-white')\n\n#Specialisation\nplt.subplot(234)\nax=sns.countplot(x=\"specialisation\", data=placement_filtered, facecolor=(0, 0, 0, 0),\n                 linewidth=5,edgecolor=sns.color_palette(\"magma\", 3))\nfig = plt.gcf()\nfig.set_size_inches(10,10)\nax.set_xticklabels(ax.get_xticklabels(),fontsize=12)\n\n#Work experience\nplt.subplot(235)\nax=sns.countplot(x=\"workex\", data=placement_filtered, facecolor=(0, 0, 0, 0),\n                 linewidth=5,edgecolor=sns.color_palette(\"cividis\", 3))\nfig = plt.gcf()\nfig.set_size_inches(10,10)\nax.set_xticklabels(ax.get_xticklabels(),fontsize=12)\n\n#Degree type\nplt.subplot(233)\nax=sns.countplot(x=\"degree_t\", data=placement_filtered, facecolor=(0, 0, 0, 0),\n                 linewidth=5,edgecolor=sns.color_palette(\"viridis\", 3))\nfig = plt.gcf()\nfig.set_size_inches(10,10)\nax.set_xticklabels(ax.get_xticklabels(),fontsize=12,rotation=20)\n\n#Gender\nplt.subplot(231)\nax=sns.countplot(x=\"gender\", data=placement_filtered, facecolor=(0, 0, 0, 0),\n                 linewidth=5,edgecolor=sns.color_palette(\"hot\", 3))\nfig = plt.gcf()\nfig.set_size_inches(10,10)\nax.set_xticklabels(ax.get_xticklabels(),fontsize=12)\n\n#Higher secondary specialisation\nplt.subplot(232)\nax=sns.countplot(x=\"hsc_s\", data=placement_filtered, facecolor=(0, 0, 0, 0),\n                 linewidth=5,edgecolor=sns.color_palette(\"rocket\", 3))\nfig = plt.gcf()\nfig.set_size_inches(10,10)\nax.set_xticklabels(ax.get_xticklabels(),fontsize=12)\n\n#Status of recruitment\nplt.subplot(236)\nax=sns.countplot(x=\"status\", data=placement_filtered, facecolor=(0, 0, 0, 0),\n                 linewidth=5,edgecolor=sns.color_palette(\"copper\", 3))\nfig = plt.gcf()\nfig.set_size_inches(10,10)\nax.set_xticklabels(ax.get_xticklabels(),fontsize=12)","c3423d6d":"sns.set(rc={'figure.figsize':(12,8)})\nf, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw={\"height_ratios\": (.15, .85)})\n\nplacement_placed = placement_filtered[placement_filtered.salary != 0]\nsns.boxplot(placement_placed[\"salary\"], ax=ax_box)\nsns.distplot(placement_placed[\"salary\"], ax=ax_hist)\n \n# Remove x axis name for the boxplot\nax_box.set(xlabel='')\n","a9324f1b":"sns.set(rc={'figure.figsize':(12,8)})\nsns.set(style=\"white\", color_codes=True)\nsns.jointplot(x=placement_filtered[\"etest_p\"], y=placement_filtered[\"salary\"], kind='kde', color=\"skyblue\")","cf468489":"plt.figure(figsize = (15, 7))\nplt.style.use('seaborn-white')\nplt.subplot(231)\nsns.distplot(placement_filtered['ssc_p'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\nplt.subplot(232)\nsns.distplot(placement_filtered['hsc_p'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\nplt.subplot(233)\nsns.distplot(placement_filtered['degree_p'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\nplt.subplot(234)\nsns.distplot(placement_filtered['etest_p'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\nplt.subplot(235)\nsns.distplot(placement_filtered['mba_p'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)\n\nplt.subplot(236)\nsns.distplot(placement_placed['salary'])\nfig = plt.gcf()\nfig.set_size_inches(10,10)","7827660f":"#Code forked from-https:\/\/www.kaggle.com\/biphili\/hospitality-in-era-of-airbnb\nplt.style.use('seaborn-white')\nf,ax=plt.subplots(1,2,figsize=(18,8))\nplacement_filtered['workex'].value_counts().plot.pie(explode=[0,0.05],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Work experience')\nsns.countplot(x = 'workex',hue = \"status\",data = placement_filtered)\nax[1].set_title('Influence of experience on placement')\nplt.show()","5923116c":"g = sns.boxplot(y = \"status\",x = 'mba_p',data = placement_filtered, whis=np.inf)\ng = sns.swarmplot(y = \"status\",x = 'mba_p',data = placement_filtered, size = 7,color = 'black')\nsns.despine()\ng.figure.set_size_inches(12,8)\nplt.show()","5df1d254":"import plotly_express as px\ngapminder=px.data.gapminder()\npx.scatter(placement_filtered,x=\"mba_p\",y=\"etest_p\",color=\"status\",facet_col=\"workex\")","faf05fe5":"px.violin(placement_placed,y=\"salary\",x=\"specialisation\",color=\"gender\",box=True,points=\"all\")","b811a322":"sns.heatmap(placement_placed.corr(),annot=True,fmt='.1g',cmap='Greys')","97fcfcee":"sns.pairplot(placement_filtered,vars=['ssc_p','hsc_p','degree_p','mba_p','etest_p'],hue=\"status\")","754de3fe":"import warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.preprocessing import LabelEncoder\n\n# Make copy to avoid changing original data \nobject_cols=['gender','workex','specialisation','status']\n\n# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\nfor col in object_cols:\n    placement_filtered[col] = label_encoder.fit_transform(placement_filtered[col])\nplacement_filtered.head()","178b10cf":"dummy_hsc_s=pd.get_dummies(placement_filtered['hsc_s'], prefix='dummy')\ndummy_degree_t=pd.get_dummies(placement_filtered['degree_t'], prefix='dummy')\nplacement_coded = pd.concat([placement_filtered,dummy_hsc_s,dummy_degree_t],axis=1)\nplacement_coded.drop(['hsc_s','degree_t','salary'],axis=1, inplace=True)\nplacement_coded.head()","ae7571aa":"feature_cols=['gender','ssc_p','hsc_p','hsc_p','workex','etest_p','specialisation','mba_p',\n              'dummy_Arts','dummy_Commerce','dummy_Science','dummy_Comm&Mgmt','dummy_Others','dummy_Sci&Tech']\nlen(feature_cols)","a26994aa":"X=placement_coded.drop(['status'],axis=1)\ny=placement_coded.status","31320072":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.8,random_state=1)\nprint(\"Input Training:\",X_train.shape)\nprint(\"Input Test:\",X_test.shape)\nprint(\"Output Training:\",y_train.shape)\nprint(\"Output Test:\",y_test.shape)","69f19d6c":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))","ed3e26a1":"from sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\\n\",confusion_matrix)\nfrom sklearn.metrics import classification_report\nprint(\"Classification Report:\\n\",classification_report(y_test, y_pred))","305fbd8d":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nlogit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([-0.01, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","33aa9731":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(criterion=\"gini\", max_depth=3)\ndt = dt.fit(X_train,y_train)\ny_pred = dt.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","f038d5fd":"pip install pydotplus","2254142d":"from sklearn.externals.six import StringIO  \nfrom sklearn.tree import export_graphviz\nfrom IPython.display import Image  \nimport pydotplus\n\ndot_data = StringIO()\nexport_graphviz(dt, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True,feature_names = feature_cols,class_names=['0','1'], precision=1)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())","2c50ba0b":"from sklearn.ensemble import RandomForestClassifier\nrt=RandomForestClassifier(n_estimators=100)\nrt.fit(X_train,y_train)\ny_pred=rt.predict(X_test)\nfrom sklearn import metrics\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","cc0c9acf":"feature_imp = pd.Series(rt.feature_importances_,index=feature_cols).sort_values(ascending=False)\n# Creating a bar plot\nsns.barplot(x=feature_imp, y=feature_imp.index)\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")","6b75899e":"X=placement_coded.drop(['status','dummy_Comm&Mgmt','dummy_Sci&Tech','dummy_Science','dummy_Commerce',\n                        'dummy_Arts','dummy_Others'],axis=1)\ny=placement_coded.status\nX_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.8,random_state=1)\nrt2=RandomForestClassifier(n_estimators=100)\nrt2.fit(X_train,y_train)\ny_pred=rt2.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nroc_value = roc_auc_score(y_test, y_pred)\nroc_value\nprint(\"ROC Value:\",roc_value)","097c79b1":"from sklearn.neighbors import KNeighborsClassifier\nerror_rate = []\n\nfor i in range(1,40):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))","6503af65":"plt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","e37823db":"from sklearn.metrics import confusion_matrix\nknn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\\n\",confusion_matrix)\nprint(\"Classification Report:\\n\",classification_report(y_test, y_pred))","e7711e3f":"#Importing and fitting\nfrom sklearn.naive_bayes import BernoulliNB \nfrom sklearn.model_selection import cross_val_score\ngnb = BernoulliNB() \ngnb.fit(X_train, y_train) \n  \n#Applying and predicting \ny_pred = gnb.predict(X_test) \ncv_scores = cross_val_score(gnb, X, y, \n                            cv=10,\n                            scoring='precision')\nprint(\"Cross-validation precision: %f\" % cv_scores.mean())","fe0deac2":"from sklearn.metrics import confusion_matrix\nfrom sklearn.svm import SVC\nsvclassifier = SVC(kernel='linear')\nsvclassifier.fit(X_train, y_train)\ny_pred = svclassifier.predict(X_test)\nconfusion_matrix = confusion_matrix(y_test,y_pred)\nprint(\"Confusion Matrix:\\n\",confusion_matrix)\nprint(\"Classification Report:\\n\",classification_report(y_test,y_pred))","fbbc369b":"import xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nxg_reg = xgb.XGBClassifier(objective ='reg:logistic', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 10)\nxg_reg.fit(X_train,y_train)\n\npreds = xg_reg.predict(X_test)\nrmse = np.sqrt(mean_squared_error(y_test, preds))\nprint(\"RMSE: %f\" % (rmse))","13601ff6":"data_dmatrix = xgb.DMatrix(data=X,label=y)\nparams = {\"objective\":\"reg:logistic\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n                'max_depth': 5, 'alpha': 10}\n\ncv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,\n                    num_boost_round=50,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True, seed=123)\ncv_results.head()","9cf7d72f":"print((cv_results[\"test-rmse-mean\"]).tail(1))","3e2cd119":"## 7d. K Nearest Neighbours\nLet's try out a lazy supervised classification algorithm. Our beloved, KNN","5162d77f":"## 6b. One hot encoding\nWe have used dummies function for the category which has more than two types of classes","9a269fc8":"**Inference**\n* There is **no relation** between mba percentage and employability test\n* There are many candidates who **haven't got place**d when they don't have work experience\n* Most of the candidates who performed better in both tests **have got placed**","d5f47e26":"## 6d. Train and Test Split (80:20)","32a711f1":"**Inference**\n* We have **more male candidates** than female\n* We have candidates who did **commerce** as their hsc course and as well as undergrad\n* **Science background** candidates are the second highest in both the cases\n* Candidates from **Marketing and Finance** dual specialization are high \n* Most of our candidates from our dataset **don't have any work experience**\n* Most of our candidates from our dataset **got placed** in a company","335278ec":"## 3b.Extracting dataset","4fe3d728":"### Looking at Feature Importance\nLet's see which feature influences more on making the decision and we should cut it off to make our model accurate","6f08275d":"# 6. Preprocessing data for classficiation models\nNow let's welcome our data to the model.Before jumping onto creating models we have to prepare our dataset for the models. We dont have to perform imputation as we dont have any missing values but we have categorical variables which needs to be encoded.","74571131":"We have **215 candidate details** and there are mixed datatypes in each column. We have few missing values in the salary column as expected since those are the people who didn't get hired","4b01dfbc":"Woah **73% accurate** with using gini index as criterion. I have tried entropy which has high accuracy but considers less features for splitting,so I shifted to gini which considered more features for splitting","68f91cd7":"## 5i. Coorelation between academic percentages","4dde0fe7":"**Inference**\n* Many candidates who got placed received package between **2L-4L PA**\n* Only **one** candidate got around **10L PA**\n* The **average** of the salary is a little more than 2LPA\n","4d32c71f":"## 7f. Support Vector Machine\nLet's use SVM to classify our output feature","b985ce35":"### Error rate vs K-value","f5d73155":"## 7c. Random Forest\nSince one tree can't produce accurate results lets use random forest to create a aggregation of trees and produce accurate results","1208d5d5":"## 7e. Naive Bayes Classifier with Cross Validation\nLet's use Naive Bayes model for our dataset. Since our outcome feature has 1,0(placed, not placed) we can go with Bernoulli Naive bayes algorithm and also let's measure the accuracy with cross validation","1e28bc3d":"## 5f. MBA marks vs Placement Status- Does your academic score influence?","58f6d7ee":"## 4a. Handling mis_ing values\nFirst lets focus on the missing data in review features,if we drop the rows which has null values we might sabotage some potential information from the dataset. So we have to impute values into the NaN records which leads us to accurate models. Since it is a salary feature,it is best to impute the records with '0' for unhired candidates","0c1f19ca":"Our cross validation precision is approximately **73.5%** ","3fe8a6ee":"**Inference**\n* The **top salaries were given to male**\n* The **average salary** offered were also **higher for male**\n* **More male candidates were placed** compared to female candidates\n","c8dae120":"## 3c. Examining the dataset","12b75e8a":"We have **1 integer,5 float and 8 object** datatypes in our dataset","1ac05ae9":"**Inference**\n* Candidates who has **high score in higher secondary and undergrad got placed**\n* Whomever got **high scores in their schools got placed**\n* Comparing the number of students who got placed candidates who got **good mba percentage and employability percentage**  ","0a02120f":"As you see, we have very less number of outliers in our features. Especially we have majority of the outliers in **hsc percentage** Let's clear em up!","13c3178b":"**Insights:**\n* Our model has precisely classified **86%** of Not placed categories and **74%** of Placed categories\n* To talk in numbers **26+6** correct classifications and **1+9** false negative and false positive classification.\n* We should be considering the **precision value as our metric** because the possibility of commiting False Positive is very crucial in recuritment ","929645d6":"### ROC Curve\nLet's check out the performance of our model through ROC curve","e43c3696":"**Inference** <br>\nComparitively there's a slight difference between the percentage scores between both the groups, But still placed candidates still has an upper hand when it comes to numbers as you can see in the swarm. So as per the plot,percentage do influence the placement status","04d7c193":"Great. Now We have an **accuracy of 81%** and the **ROC value 73%** indicates the models have classified better without having much false positive predictions","20ad6603":"## Hit upvote if you like my work and also check out my [other notebooks](https:\/\/www.kaggle.com\/benroshan\/notebooks)","ef5b6082":"## 7g. XGBoost\nLet's try our the state of art ensemble model XGBoost. We have used RMSE metrics for model performance","cad2aaa8":"### Pruning out less important feature\nLet's cut off the less important feature and check for model accuracy.","2a3a9705":"Great. The error value of our model is just **0.577**. Now let's use cross validation and try to minimise further","2662c0b2":"# 3. Importing libraries and exploring Data\n## 3a.Importing Libraries\nPython is a fantastic language with a vibrant community that produces many amazing libraries. I am not a big fan of importing everything at once for the newcomers. So, I am going to introduce a few necessary libraries for now, and as we go on, we will keep unboxing new libraries when it seems appropriate","de9273ca":"**Inference**\n* We have **4** sets of placed and not placed students \n* It has been splitted based on ssc_p followed by hsc_p,mba_p and etest_p\n* We have minimised the depth to **3** to prevent it from overfitting\n* We still have **few gini value**(impurity) in classes of leaf node.\n* Pure classes show that they have been splitted under th criteria of **ssc_p<=63.7 and e_testp<=825.**\n\nSo the best splitting can be made possible through **etest_p** feature","4af7e2d3":"**Inference**\n* We have nearly **66.2%** of candidates who never had any work experience\n* Candidates who **never had work experience** have **got hired** more than the ones who had experience\n* We can conclude that **work experience doesn't influence** a candidate in the recruitment process","9a99be4d":"# 4. Data Cleaning","1aaee39d":"Nice. We have reduced our model error to **0.41** ","827ae954":"# 5.Data Visualizations\n## 5a. Count of categorical features- Count plot","771b8a3b":"From the ROC curve we can infer that our logistic model has classified the placed students correctly rather than predicting false positive. T**he more  the ROC curve(blue) lies towards the top left side the better our model** is. We can choose **0.8 or 0.9** for the threshold value which can reap us true positive results","2fe0c547":"As per our inference, we can visualize the null values in salary. Let's see the count","273b2a04":"We have an accuracy of **83%**. Not bad. But let's try check out important features and try to boost the precision","544a6279":"**Insights:**\n*  The Confusion matrix result is telling us that we have **9+26** correct predictions and **1+6** incorrect predictions.\n*  The Classification report reveals that we have **84%** precision which means the accuracy that the model classifier not to label an instance positive that is actually negative and it is important to consider precision value because when you are hiring, you want to **avoid Type I errors at all cost**. They are **culture killers**.In hiring, a false positive is when you THINK an employee is a good fit, but in actuality they\u2019re not.","bffb3847":"**Inference**\n* There are **67 null values** in our data, which means 67 unhired candidates. \n* We can't drop these values as this will provide a valuable information on why candidates failed to get hired.\n* We can't impute it with mean\/median values and it will go against the context of this dataset and it will show unhired candidates got salary.\n* Our best way to deal with these null values is to **impute it with '0'** which shows they don't have any income","c16c2050":"## 5d.Distribution of all percentages","3095dd97":"# 2.Kernel Goals\nThere are three primary goals of this kernel.\n\n* Do a **exploratory analysis** of the Recruitment dataset\n* Do an **visualization analysis** of the Recruitment dataset\n* **Prediction:** To predict whether a student got placed or not using **classification** models","7e95cf18":"# Report Summary\n![interview-questions.png](attachment:interview-questions.png)\nFrom the analysis report on Campus Recruitment dataset here are my following conclusions\n\n* **Educational percentages** are highly influential for a candidate to get placed\n* **Past work experience** doesn't influence much on your masters final placements\n* There are **no gender discrimination** while hiring, but higher packages were given to male\n* Academic percentages have **no relation** towards salary package.\n","cf1c2810":"## 6a. Label Encoding\nWe have used label encoder function for the category which has only two types of classes","3497623b":"### XGBoost with Cross Validation\nIn this algorithm we are using DMatrix to convert our dataset into a matrix and produce the output in dataframe. Algorithm inspired from [DataCamp](https:\/\/www.datacamp.com\/community\/tutorials\/xgboost-in-python#apply)","2902f572":"**Inference**\n* We have got **82% and 81%** precision in classifying our model.\n* **9+25** correctly classified and **2+6** wrongly classified( False Negative & False Positive)","67162d59":"## 7b. Decision Tree\nLet's checkout how the model makes the decision using Decision Tree Classifier","ff2d20e7":"**83% accurate**. That's really good. Let's check out confusion matrix and see the classification report","e997fad9":"As we see the **school and undergrad specialisations** have less influence in classifying the model. But it is really wierd to acknowledge **ssc_p** influencing more in classifying","9bbe6ca6":"## 7a.Logistic Regression\nLet's fit the model in logistic regression and figure out the accuracy of our model","c285ff39":"We have dropped **serial number** as we have index as default and we have **dropped the boards of school education** as I believe it doesn't matter for recruitment","b85e8871":"There are a lot of ups and downs in our graph. If we consider any value between 10-15 we may get an overfitted model. So let's stick onto the first trough. Our **K value is 5**","66120264":"## 5c. Employability score vs Salary- Joint plot","2ac0d990":"## 5b. Distribution Salary- Placed Students","6606b5c3":"# 1.Introduction\n![0fb047326b3c85e9d1e57b159d21935c531c-600x450.gif](attachment:0fb047326b3c85e9d1e57b159d21935c531c-600x450.gif)\nCampus recruitment is a strategy for **sourcing, engaging and hiring young talent for internship and entry-level positions**. College recruiting is typically a tactic for medium- to large-sized companies with high-volume recruiting needs, but can range from small efforts (like working with university career centers to source potential candidates) to large-scale operations (like visiting a wide array of colleges and attending recruiting events throughout the spring and fall semester). Campus recruitment often involves working with university career services centers and attending career fairs to meet in-person with college students and recent graduates.\n\n**Context of our Dataset:** Our dataset revolves around the placement season of a Business School in India. Where it has various factors on candidates getting hired such as work experience,exam percentage etc., Finally it contains the status of recruitment and remuneration details.","e0c2075b":"Voal\u00e1! We have removed the outliers","9d46fcac":"## 6c. Assigning the target(y) and predictor variable(X)\nOur Target is to find whether the candidate is placed or not. We use rest of the features except 'salary' as this won't contribute in prediction(i.e) In real world scenario, students gets salary after they get placed, so we can't use a future feature to predict something which happens in the present","d807b105":"**Inference**\n* Most of the candidates scored around **60 percentage** got a decent package of **around 3 lakhs PA**\n* **Not** many candidates received salary **more than 4 lakhs PA**\n* The bottom dense part shows the candidates who were **not placed**","75cdc30b":"## 4b. ....... Outliers\nOutliers are unusual values in your dataset, and they can distort statistical analyses and violate their assumptions. Unfortunately, all analysts will confront outliers and be forced to make decisions about what to do with them. Given the problems they can cause, you might think that it\u2019s best to remove them from your data. But, that\u2019s not always the case. Removing outliers is legitimate only for specific reasons.Outliers can be very informative about the subject-area and data collection process. It\u2019s essential to understand how outliers occur and whether they might happen again as a normal part of the process or study area. Unfortunately, resisting the temptation to remove outliers inappropriately can be difficult. Outliers increase the variability in your data, which decreases statistical power. Consequently, excluding outliers can cause your results to become statistically significant. In our case, **let's first visualize our data and decide on what to do with the outliers**","4b4618f7":"## 5h. Is there any gender bias while offering remuneration?","e5fd0907":"## Before we Begin:\nIf you liked my work, please upvote this kernel since it will keep me motivated to perform more in-depth reserach towards further datasets and produce more accurate models","bb0e9a8b":"**Inference**\n* Candidates who were good in their academics performed well throughout school,undergrad,mba and even employability test\n* These percentages **don't have any influence over their salary**","ab9ddcca":"Yayy ! we have cleared that Salary **with zero null values**. Now it's time to drop unwanted features !","6f4d58e6":"**Inference**\n* All the distributions follow **normal distribution** except salary feature\n* Most of the candidates **educational performances are between 60-80%**\n* **Salary distribution got outliers** where few have got salary of 7.5L and 10L PA","7c32beda":"## 5g.Does MBA percentage and Employability score correlate?","da2e12c6":"# 7. Machine Learning models \nNow let's feed the models with our data\n**Objective**: To predict whether a student got placed or not","543b4d7b":"### Confusion matrix and Classification report","fd97dabb":"## 5e.Work experience Vs Placement Status","9db8c597":"## 5j.Distribution of our data","c1e26e1e":"**Inference**\n* We have **Gender and Educational qualification** data\n* We have all the **educational performance(score)** data\n* We have the **status** of placement and salary details\n* We can expect **null values in salary** as candidates who weren't placed would have no salary\n* **Status** of placement is our target variable rest of them are independent variable except salary","2558b1c5":"### Choosing a K value \nLet's decide on the K value","c00e2142":"## 3d.Checking for missing data\nDatasets in the real world are often messy, However, this dataset is almost clean and simple. Lets analyze and see what we have here."}}