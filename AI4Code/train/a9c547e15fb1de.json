{"cell_type":{"ba622fc3":"code","2deda315":"code","36816621":"code","5630d86e":"code","7c4751be":"code","9ae877af":"code","922023b3":"code","7b99076d":"code","0b5de1da":"code","487f753e":"code","39ec4c04":"code","6a8927f6":"code","98277c2f":"code","b38dc751":"code","fa18733d":"code","a690b927":"code","6f33a34f":"code","7e757573":"code","8ec5dedc":"code","cc09f625":"code","4296efab":"code","677c5a4f":"code","92766211":"code","f141f903":"code","c9af17db":"code","07c6a899":"code","5a85c538":"code","ecd72602":"code","0053b833":"code","4ba1ebef":"code","c6f85e7b":"code","dfbbc81e":"code","85219d46":"code","0cfe510f":"code","49e9d11f":"code","3006c6c1":"code","636d5479":"code","b61f4b47":"code","10326e55":"code","720ea117":"code","eeb8b59b":"code","906c8b27":"code","bf6608f9":"code","e1f2b547":"code","8cbfc78e":"markdown","680fb296":"markdown","2dc5dedd":"markdown","92326bb5":"markdown","cecca018":"markdown","b2b5af04":"markdown","b3fd19ac":"markdown","8d3f2b97":"markdown","4ec4a3ae":"markdown","b88e589a":"markdown","f479b73d":"markdown","61bd47e0":"markdown","d6fdccf3":"markdown","f8901fcf":"markdown","eafc3cff":"markdown","3b693b44":"markdown","27a9a8ea":"markdown","5a3458b6":"markdown","26292add":"markdown","938e05b5":"markdown","f5669c49":"markdown","51bfe904":"markdown","bf95cdee":"markdown","44939559":"markdown","a88aa0ee":"markdown"},"source":{"ba622fc3":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport random\n\n%matplotlib inline\n\nrandom.seed(123) #making the results reproducible\n\nfilename = '..\/input\/creditcard.csv'\noriginal_data = pd.read_csv(filename)\ndata = original_data.copy() # create a copy for use leave the original in case we need it later.","2deda315":"data.head()","36816621":"amounts = data.loc[:, ['Amount']]\ndata['Amount'] = StandardScaler().fit_transform(amounts)\n\ndata.head()","5630d86e":"data = data.drop(columns=['Time'])\ndata.head()","7c4751be":"class_dist = pd.value_counts(data['Class'], sort = True)","9ae877af":"class_dist","922023b3":"print('%.2f%% of transactions are fraudulent.' % (class_dist[1]\/sum(class_dist)*100) )","7b99076d":"class_1 = data[data.Class == 1]\nclass_0 = data[data.Class == 0]","0b5de1da":"print(class_0.shape)\nprint(class_1.shape)","487f753e":"class_0_split = [round(x * class_0.shape[0]) for x in [0.7, 0.15, 0.15]]\nclass_1_split = [round(x * class_1.shape[0]) for x in [0.7, 0.15, 0.15]]\n\nprint('Non-fraudulent data split %s' % class_0_split)\nprint('Fraudulent data split %s' % class_1_split)\n\nprint('\\nCheck for rounding issues as we want to avoid increasing the total with rounding up: ')\nprint(sum(class_0_split) \/ class_0.shape[0])\nprint(sum(class_1_split) \/ class_1.shape[0])","39ec4c04":"class_0_train = class_0.sample(n = 344, random_state = 123,  replace = False) #make our random state reproducible\nclass_1_train = class_1.sample(n = 344, random_state = 234,  replace = False)","6a8927f6":"class_0_cv = class_0.drop(class_0_train.index).sample(n = 42647, random_state = 345,  replace = False)\nclass_0_test = class_0.drop(class_0_train.index).drop(class_0_cv.index).sample(n = 42647, random_state = 456,  replace = False)\n\nclass_1_cv = class_1.drop(class_1_train.index).sample(n = 74, random_state = 567, replace = False)\nclass_1_test = class_1.drop(class_1_train.index).drop(class_1_cv.index)","98277c2f":"train_set = pd.concat([class_0_train, class_1_train])\ncv_set = pd.concat([class_0_cv, class_1_cv])\ntest_set = pd.concat([class_0_test, class_1_test])","b38dc751":"#train_set.to_csv('training.csv')\n#cv_set.to_csv('cv.csv')\n#test_set.to_csv('test.csv')","fa18733d":"from sklearn.linear_model import LogisticRegression\n\nX_train = train_set.iloc[:, 0:29]\ny_train = train_set.iloc[:, 29]\n\nlr = LogisticRegression(penalty= 'l1', C=1) # default regularisation value for now.\n\nlr.fit(X_train, y_train)","a690b927":"lr.score(X_train, y_train)","6f33a34f":"X_cv = cv_set.iloc[:, 0:29]\ny_cv = cv_set.iloc[:, 29]","7e757573":"print(lr.score(X_cv, y_cv))","8ec5dedc":"y_cv_predict = lr.predict(X_cv)\n\ncv_set['Predicted'] = y_cv_predict\n\ncv_set.head()","cc09f625":"class_list = list(cv_set['Class'])\npred_list = list(cv_set['Predicted'])\n\ndef calc_prec_recall(class_list, pred_list):\n    \n    true_pos = 0\n    false_pos = 0\n    true_neg = 0\n    false_neg = 0\n\n\n\n    for i in range(cv_set.shape[0]):\n        if class_list[i] == 1:\n            if pred_list[i] == 1:\n                true_pos += 1\n            else:\n                false_neg += 1\n            \n        else:\n            if pred_list[i] == 0:\n                true_neg += 1\n            else:\n                false_pos += 1\n            \n    precision = true_pos \/ (true_pos + false_pos)\n    recall = true_pos \/ (true_pos + false_neg)\n    return precision, recall                                            \n                      \ncv_prec, cv_recall = calc_prec_recall(class_list, pred_list)                   ","4296efab":"print('Precision = %.2f%%' % (100 * cv_prec))\nprint('Recall = %.2f%%' % (100 * cv_recall))","677c5a4f":"def f1_score(precision, recall):\n    return (2 * precision * recall \/ (precision + recall))\n                                     \nfscore = f1_score(cv_prec, cv_recall)\nprint(fscore)","92766211":"C_set = [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300]     # A set of C values to try\n\ndef logistic_test(X_train, y_train, X_cv, y_cv, C_in):\n    \n    lr = LogisticRegression(penalty= 'l1', C=C_in)\n\n    lr.fit(X_train, y_train)\n    \n    train_score = lr.score(X_train, y_train)\n    cv_score = lr.score(X_cv, y_cv)\n    \n    class_list = y_cv.tolist()\n    pred_list = lr.predict(X_cv).tolist()\n    \n    cv_prec, cv_recall = calc_prec_recall(class_list, pred_list) \n\n    fscore = f1_score(cv_prec, cv_recall)\n    \n    return train_score, cv_score, fscore\n\ntraining_scores = []\ncv_scores = []\ncv_f1 = []\nbest_f1_score = 0\n\nfor C in C_set:\n    print('\\n-----------------------------------------------------------------------')\n    print('Fitting logistic regression with regularisation parameter %f' % C)\n    print('-----------------------------------------------------------------------')\n    a, b, c = logistic_test(X_train, y_train, X_cv, y_cv, C)    \n    print('Training score = %f' % a)\n    print('Cross-validation score = %f' % b)\n    print('F1 score = %f' %c)\n    training_scores.append(a)\n    cv_scores.append(b)\n    cv_f1.append(c)\n    if c > best_f1_score:\n        best_f1_score =c\n    ","f141f903":"plt.plot(C_set, training_scores, '-r', label='Training')\nplt.plot(C_set, cv_scores, '-b', label='Cross Validation')\nplt.xscale('log')\nplt.xlabel('C (inverse regularisation parameter)')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","c9af17db":"plt.plot(C_set, cv_f1, '-r', label='Cross Validation F1 Score')\nplt.xscale('log')\nplt.xlabel('C (inverse regularisation parameter)')\nplt.ylabel('F1 Score')\nplt.legend()\nplt.show()","07c6a899":"lr = LogisticRegression(penalty= 'l1', C=0.1)\nlr.fit(X_train, y_train)\nX_test = test_set.iloc[:, 0:29]\ny_test = test_set.iloc[:, 29]\n\nclass_list_test = list(test_set['Class'])\npred_list_test = lr.predict(X_test)\n\ntest_prec, test_recall = calc_prec_recall(class_list_test, pred_list_test) # pretty sure this is wrong - class list needs to be from test data\ntest_f1 = f1_score(test_prec, test_recall)\n\nprint('Accuracy on unseen test data: %.2f%%' % (100 * lr.score(X_test, y_test)))\nprint('Recall on unseen test data: %.2f%%' % (100*test_recall))\nprint('Precision on unseen test data: %.2f%%' % (100*test_prec))\nprint('F1 Score on unseen test data: %.2f%%' % (100 * test_f1))","5a85c538":"from sklearn.svm import SVC, LinearSVC #import both the Linear and SVM classifier\n\nsvm_lin = LinearSVC(C=1.0) # C is effectively a regularisation parameter, we will experiement with changing this later\n\nsvm_lin.fit(X_train, y_train)\n\nprint(\"Training set linear classifier score: %.2f%%\" % (100 * svm_lin.score(X_train, y_train)))\nprint(\"Cross-validation set linear classifier score: %.2f%%\" % (100 * svm_lin.score(X_cv, y_cv)))","ecd72602":"svm_lin_pred_list = svm_lin.predict(X_cv).tolist()\ncv_prec, cv_recall = calc_prec_recall(class_list, svm_lin_pred_list)\nfscore = f1_score(cv_prec, cv_recall)","0053b833":"print(\"The cross-validation precision is: %.2f%%\" % (100*cv_prec))\nprint(\"The cross-validation recall is: %.2f%%\" % (100*cv_recall))\nprint(\"The cross-validation F1 score is: %.2f%%\" % (100*fscore))","4ba1ebef":"svm_rbf = SVC(C=1.0, kernel='rbf', gamma='auto') # Using the default C for now and allow the algorithm to choose gamma","c6f85e7b":"svm_rbf.fit(X_train, y_train)\nprint(\"Training set rbf classifier score: %.2f%%\" % (100 * svm_rbf.score(X_train, y_train)))\nprint(\"Cross-validation set rbf classifier score: %.2f%%\" % (100 * svm_rbf.score(X_cv, y_cv)))","dfbbc81e":"svm_rbf_pred_list = svm_rbf.predict(X_cv).tolist()\ncv_prec, cv_recall = calc_prec_recall(class_list, svm_rbf_pred_list)\nfscore = f1_score(cv_prec, cv_recall)\nprint(\"The cross-validation precision is: %.2f%%\" % (100*cv_prec))\nprint(\"The cross-validation recall is: %.2f%%\" % (100*cv_recall))\nprint(\"The cross-validation F1 score is: %.2f%%\" % (100*fscore))","85219d46":"#C_set = [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300]     # A set of C values to try\n#gamma_set = [0.001, 0.003, 0.01, 0.03]     # A set of C values to try\n\nC_set = [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300]     # A set of C values to try\ngamma_set = [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3]     # A set of C values to try\n\ndef rbf_test(X_train, y_train, X_cv, y_cv, C_in, gamma_in):\n    \n    svm_rbf = SVC(C= C_in, kernel='rbf', gamma=gamma_in)\n\n    svm_rbf.fit(X_train, y_train)\n    \n    train_score = svm_rbf.score(X_train, y_train)\n    cv_score = svm_rbf.score(X_cv, y_cv)\n    \n    class_list = y_cv.tolist()\n    pred_list = svm_rbf.predict(X_cv).tolist()\n    \n    cv_prec, cv_recall = calc_prec_recall(class_list, pred_list) \n\n    fscore = f1_score(cv_prec, cv_recall)\n    \n    return train_score, cv_score, fscore\n\n\nIndex = ['gamma = %s' % g for g in gamma_set]\nColumns = ['C = %s' % C for C in C_set]\n\nrbf_training_scores_df = pd.DataFrame(index=Index, columns=Columns)\nrbf_cv_scores_df = pd.DataFrame(index=Index, columns=Columns)\nrbf_cv_f1_df = pd.DataFrame(index=Index, columns=Columns)\n\nrbf_best_f1_score = 0\n\nfor C in C_set:\n    for gamma in gamma_set:\n        print('\\n-----------------------------------------------------------------------')\n        print('Fitting Gaussian SVM with regularisation parameter %f and gamma, %f' % (C, gamma))\n        print('-----------------------------------------------------------------------')\n        a, b, c = rbf_test(X_train, y_train, X_cv, y_cv, C, gamma)    \n        #print('Training score = %f' % a)\n        #print('Cross-validation score = %f' % b)\n        #print('F1 score = %f' %c)\n        \n        rbf_training_scores_df.iloc[gamma_set.index(gamma), C_set.index(C)] = a\n        rbf_cv_scores_df.iloc[gamma_set.index(gamma), C_set.index(C)] = b\n        rbf_cv_f1_df.iloc[gamma_set.index(gamma), C_set.index(C)] = c\n        \n        \n        training_scores.append(a)\n        cv_scores.append(b)\n        cv_f1.append(c)\n        if c > best_f1_score:\n            best_f1_score =c","0cfe510f":"# For some reason when I looked at my data fram the first time, the plots didn't work as expected\n# I looked at the data type for each column using the dtype attribute and they were all objects\n# I'm not sure of the reason for this, but it could be that I initialised the arrays without any data\n# We can convert all the values to float to fix this.\n\n\nrbf_training_scores_df = rbf_training_scores_df.astype(float)\nrbf_cv_scores_df = rbf_cv_scores_df.astype(float)\nrbf_cv_f1_df = rbf_cv_f1_df.astype(float)\n\nplt.figure(figsize=(12,6))\nsns.heatmap(rbf_training_scores_df, annot=True, linewidths = 0.5, vmin=0.7, vmax=1)\nplt.yticks(rotation=0)\nplt.title('Training Data Accuracy', pad=10)\nplt.show()","49e9d11f":"plt.figure(figsize=(12,6))\nsns.heatmap(rbf_cv_scores_df, annot=True, linewidths = 0.5, vmin=0.7, vmax=1)\nplt.title('Cross-Validation Data Accuracy', pad=10)\nplt.yticks(rotation=0)\nplt.show()","3006c6c1":"plt.figure(figsize=(12,6))\nsns.heatmap(rbf_cv_f1_df, annot=True, linewidths = 0.5, vmin=0, vmax=0.7)\nplt.yticks(rotation=0)\nplt.title('F1 Score on Cross-Validation Data', pad=10)\nplt.show()","636d5479":"C_set = [0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1]     # A set of C values to try\ngamma_set = [0.0001, 0.0003, 0.001, 0.003]     # A set of C values to try\n\nIndex = ['gamma = %s' % g for g in gamma_set]\nColumns = ['C = %s' % C for C in C_set]\n\nrbf_training_scores_df = pd.DataFrame(index=Index, columns=Columns)\nrbf_cv_scores_df = pd.DataFrame(index=Index, columns=Columns)\nrbf_cv_f1_df = pd.DataFrame(index=Index, columns=Columns)\n\nrbf_best_f1_score = 0\n\nfor C in C_set:\n    for gamma in gamma_set:\n        #print('\\n-----------------------------------------------------------------------')\n        #print('Fitting Gaussian SVM with regularisation parameter %f and gamma, %f' % (C, gamma))\n        #print('-----------------------------------------------------------------------')\n        a, b, c = rbf_test(X_train, y_train, X_cv, y_cv, C, gamma)    \n        #print('Training score = %f' % a)\n        #print('Cross-validation score = %f' % b)\n        #print('F1 score = %f' %c)\n        \n        rbf_training_scores_df.iloc[gamma_set.index(gamma), C_set.index(C)] = a\n        rbf_cv_scores_df.iloc[gamma_set.index(gamma), C_set.index(C)] = b\n        rbf_cv_f1_df.iloc[gamma_set.index(gamma), C_set.index(C)] = c\n        \n        \n        training_scores.append(a)\n        cv_scores.append(b)\n        cv_f1.append(c)\n        if c > best_f1_score:\n            best_f1_score =c","b61f4b47":"rbf_training_scores_df = rbf_training_scores_df.astype(float)\nrbf_cv_scores_df = rbf_cv_scores_df.astype(float)\nrbf_cv_f1_df = rbf_cv_f1_df.astype(float)\n\nplt.figure(figsize=(12,6))\nsns.heatmap(rbf_training_scores_df, annot=True, linewidths = 0.5, vmin=0.7, vmax=1)\nplt.yticks(rotation=0)\nplt.title('Training Data Accuracy', pad=10)\nplt.show()","10326e55":"plt.figure(figsize=(12,6))\nsns.heatmap(rbf_cv_f1_df, annot=True, linewidths = 0.5, vmin=0, vmax=0.8)\nplt.yticks(rotation=0)\nplt.title('F1 Score on Cross-Validation Data', pad=10)\nplt.show()","720ea117":"C_set = [0.1, 0.2, 0.4, 0.8, 1.6, 3.2, 6.4, 12.8]     # A set of C values to try\ngamma_set = [3e-06, 0.00001, 0.00003, 0.0001, 0.0003]     # A set of C values to try\n\nIndex = ['gamma = %s' % g for g in gamma_set]\nColumns = ['C = %s' % C for C in C_set]\n\nrbf_training_scores_df = pd.DataFrame(index=Index, columns=Columns)\nrbf_cv_scores_df = pd.DataFrame(index=Index, columns=Columns)\nrbf_cv_f1_df = pd.DataFrame(index=Index, columns=Columns)\n\nrbf_best_f1_score = 0\n\nfor C in C_set:\n    for gamma in gamma_set:\n        print('\\n-----------------------------------------------------------------------')\n        print('Fitting Gaussian SVM with regularisation parameter %f and gamma, %f' % (C, gamma))\n        print('-----------------------------------------------------------------------')\n        a, b, c = rbf_test(X_train, y_train, X_cv, y_cv, C, gamma)    \n        #print('Training score = %f' % a)\n        #print('Cross-validation score = %f' % b)\n        #print('F1 score = %f' %c)\n        \n        rbf_training_scores_df.iloc[gamma_set.index(gamma), C_set.index(C)] = a\n        rbf_cv_scores_df.iloc[gamma_set.index(gamma), C_set.index(C)] = b\n        rbf_cv_f1_df.iloc[gamma_set.index(gamma), C_set.index(C)] = c\n        \n        \n        training_scores.append(a)\n        cv_scores.append(b)\n        cv_f1.append(c)\n        if c > best_f1_score:\n            best_f1_score =c","eeb8b59b":"rbf_training_scores_df = rbf_training_scores_df.astype(float)\nrbf_cv_scores_df = rbf_cv_scores_df.astype(float)\nrbf_cv_f1_df = rbf_cv_f1_df.astype(float)\n\nplt.figure(figsize=(12,6))\nsns.heatmap(rbf_training_scores_df, annot=True, linewidths = 0.5, vmin=0.7, vmax=1)\nplt.yticks(rotation=0)\nplt.title('Training Data Accuracy', pad=10)\nplt.show()","906c8b27":"plt.figure(figsize=(12,6))\nsns.heatmap(rbf_cv_f1_df, annot=True, linewidths = 0.5, vmin=0, vmax=0.9)\nplt.yticks(rotation=0)\nplt.title('F1 Score on Cross-Validation Data', pad=10)\nplt.show()","bf6608f9":"best_svm_rbf1 = SVC(C= 1.6, kernel='rbf', gamma=3e-05)\nbest_svm_rbf1.fit(X_train, y_train)\nbest_pred_list1 = best_svm_rbf1.predict(X_test).tolist()\nbest_precision1, best_recall1 = calc_prec_recall(y_test.tolist(), best_pred_list1)\n\nbest_svm_rbf2 = SVC(C= 0.4, kernel='rbf', gamma=1e-04)\nbest_svm_rbf2.fit(X_train, y_train)\nbest_pred_list2 = best_svm_rbf2.predict(X_test).tolist()\nbest_precision2, best_recall2 = calc_prec_recall(y_test.tolist(), best_pred_list2)\n\nbest_fscore1 = f1_score(best_precision1, best_recall1)\nbest_fscore2 = f1_score(best_precision2, best_recall2)","e1f2b547":"print(\"Model 1\")\nprint(\"F1 Score for 1st model on test data: %.2f%%\" % (100 * best_fscore1))\nprint(\"Precision: %.2f%%\" % (100* best_precision1))\nprint(\"Recall: %.2f%%\" % (100* best_recall1))\n\nprint(\"\\nModel 2\")\nprint(\"F1 Score for 2nd model on test data: %.2f%%\" % (100 * best_fscore2))\nprint(\"Precision: %.2f%%\" % (100* best_precision2))\nprint(\"Recall: %.2f%%\" % (100* best_recall2))","8cbfc78e":"In conclusion, these two models performed reasonably well on the test data set.\n\nThe SVM model with a Gaussian (RBF) kernel was able to identify approximately 80% of all of the fraudulent transations as fraudulent (recall). And of all of the transactions that the model selected as fraudulent, approximately 80% of them were indeed fraudulent (preicsion).\n\nI am pretty happy with this overall result and demonstration of how to avoid overfitting and how to handle skewed data sets in classification problems.\n\nI am sure it could be improved further with more data or perhaps a more complex model such as a neural network. ","680fb296":"Similar to the logistic regression, we got a higher score on the cross training data. This is due to highly skewed nature of the cross validation set compared to the training set.\n\nLet's see what the preicsion, recall and F1 score are like.","2dc5dedd":"The graph shows a clear under fit to our data when C is low, due to the high degree of reularisation.\n\nOn the other hand, when C is very high, there is less regularisation and the cross validation results worsen slightly. However in most cases, the accuracy for the cross-validation set is better than for the training set. This is likely due to the highly skewed nature of our Cross Validation set compared to the training set.","92326bb5":"# 1 - Logistic Regression Model\n## 1.1 - First Look at the Data\n\nThe data has 28 anonymised features, V1 to V28, which also appear to have been normalised. In addition, there is the time and the transaction amount, giving us 30 features in total.\n\nFor now, I am going to ignore the time feature. I will also normalise the Amount and add this as a new column.","cecca018":"Now, let's re-merge the data into three sets.","b2b5af04":"## 1.4 - Tuning the regularisation to improve cross-validation score\nNow, let's choose a range of values for our regularisation parameter, C (inverse of regularisation strength) to find the best overall F-Score","b3fd19ac":"These results seem promising. Let's take a look at more values of C from 0.1 to 10 and more values of gamma below 0.0001","8d3f2b97":"## 1.3 - Training the data with logistic regression\n\n","4ec4a3ae":"This gives us the required split of fraudulent and non-frudulent data.\n\nSince we have very skewed data, one method that can be used to create a useful classifier is to resample the data and to reduce the presence of the over-represented class in our training data.\n\nTo do this, let's create a training data set with 50:50 split between fraudulent and non-fraudulent training data. So we will only have 344 non-fraudulent examples in our training set instead of the prescribed 199,020 above","b88e589a":"This looks good, but remember our cross-validation data is highly skewed!\n\nWe need to look at the recall and precsision to have a true understanding of this data.\n\nLet's add the predicted classes to our cross-validation data so that we can compare it with the actual class and see where we have false\/true positives and negatives.","f479b73d":"Now we can drop the Time column to remove it","61bd47e0":"For the cross-validation, and test samples, we will maintain the normal skewness of the original data.\n\nWe have to take care here not to re-sample the trianing data (or the cross-validation data for the test data), so we use **drop** to remove these examples before sampling again.\n\nWe also have too much extra data from undersampling the training data, so we have to reduce the amount in the final test data to keep to ratio of fraudulent and non-fraudulent data.","d6fdccf3":"That was noticably slower to run than the logistic regression and the SVM with a linear kernel. Let's take a look at some plots to see how this performed.","f8901fcf":"## 1.5 - Checking unseen test data\n\nBased on our previous tuning to find the best C value via cross-validation, we can set C=0.1\n\nIt is important to note that using cross-validation data to tune our parameters is also a form of data fitting. To avoid overfitting to our cross validation set we should compare to an unseen data set. That is why we created a separate set of test data for the final result.\n\nLet's see how the model performs on the test set.","eafc3cff":"These datasets were examined in Excel and no duplicates were found.","3b693b44":"# A beginner's look at classification problems\n\nThis is my first kernel on Kaggle, so critique is very welcome.\n\nMy intention for this is to cover some of the more basic approaches to classification problems. My intention is also to make the code easy to follow for beginners, without abstracting too much of the work away through library functions. This is why you will see me doing things like manually calculating precision and recall. For this reason, I won't apologise for excessive use of print statements either. ;-)\n\nMy introduction to machine learning has been through Andrew Ng's excellent course on coursera.org. I really cannot recommend this course highly enough, even to those with some previous experience in machine learning. The majority of the work in this notebook covers some of the techniques and nuances that I have learned in this course.\n\nThere are many more advanced learning methods out there which are now very popular on Kaggle, such as various forms of Gradient Boosting, but the choice of ML algorithm is not the only important factor. I want to demonstrate good implementation of cross-validation and how to manage highly skewed data sets.","27a9a8ea":"## 2.4 - Choosing a final SVM model and testing it on the test \n\nBased on the F1 Score for the cross-validation data, there are two potential candidates for the best model (both shown with a F1 score of 0.82 on the figure above).\n\nLet's see how these two perform on the test data.","5a3458b6":"## 1.2 - Creating training, cross-validation and test data sets\nThe data have been split into fraudulent and non-fraudulent data. Now, let's create different data sets with the following split:\n\n1. 70% training data\n2. 15% cross-validation data\n3. 15% test data\n\nSo for our two classes these are approximately:","26292add":"# 2 Using Support Vector Machines (SVMs)\n\nA support vector machine is a useful way to generate a classifier, which fits a hyperplane to separate the data. A hyperplane is a subspace whose dimension is one less than that of its ambient space. So if we have:\n\n* Data in two dimensions, it will be separated by a line.\n* Data in three dimensions, the data will be separated by a two dimensional plane.\n* Etc.\n\nThe key thing to note is that, at it's heart an SVM is essentially a linear classifier. To generate more complex, non-linear classifiers, we can use various kernel functions. I won't go into the details, but the number of new features generated scales with the number of training examples. This can be problematic with very large training sets. With our relatively small training set size, this particular problem lends itself well to the use of SVMs.\n\nThese are the kernels we will test:\n\n* Linear (i.e. no kernel, just a basic SVM).\n* Radial Basis Functions (RBF) - This is a form of the Gaussian function.\n\nAlthough other kernels exist and we can even write our own kernels, these are the two most popular kernels. The RBF is capable of generating some very complex decision boundaries and should make an interesting comparison to the linear kernel.\n\nIt is very important to carry out feture scaling before using SVM's. Fortunately, our features are already scaled.\n\n## 2.1 - Trying the linear classifier\nScikit-learn has an inbuilt SVM classifier called SVC. This uses the RBF kernel by default. Although this has the option to use a linear kernel, there is also the option to use LinearSVC classifier. As far as I can tell these function similarly, but according to the documentation the LinearSVC is implemented slightly differently. This makes it less restrictive in the choice of penalties and loss functions. It should scale better to large numbers of samples, but that is not a concern as our training set is quite small.\n","938e05b5":"Two things immediately spring to mind.\n\n1. The best F1 score on the Cross-validation set is much better than the best for both our logistic regression, and our linear kernel SVM.\n\n\n2. Our selected values for gamma were, in general, too high. The automatic value for gamma is normally 1\/n_features, so in our case 1\/30. The higher values can deceptively look promising, with good training data and cross-validation accuracy, but the F score is much higher for the lower value of gamma. This suggests to me that the model is severly underfitting the data and predicitng every transaction as non-fraudulent. This shows how our highly skewed data set can be deceiving in cross validating the data. The choice to use F1 score has been validated in my opinion.\n\nLet's try a different range of values, focussing on lower gamma and C","f5669c49":"These data are highly skewed, which means that ordinary error metrics will not provide useful insight for our classifier.\n\nFor example, if we had a classifier that simply predicts that classes every transaction as non fraudulent (class 0), then it would be correct 99.83% of the time!\n\nWe need a better method of measuring the algorithm's error. In this case we should use recall and precision to evaluate the effectiveness of the algorithm.\n\nIn addition we need to take care when we separate our data into training, cross validation and test sets. There is a good chance we may have no fraudulent examples in one of these sets, which we want to avoid.\n\nSo first, we split our fraudulent examples (class 1) and our non-fraudulent examples (class 0).","51bfe904":"This is much worse than our first two attempts! The RBF kernel performed worse on the cross-validation set than the logistic regression and the SVM with the linear kernel. This should not be a concern as we only tuned our logistic regression model.\n\n## 2.3 - Tuning the Radial Basis Function SVM\n\nWe have two parameters that we can tune to try to improve the model. We can alter C - which acts as an inverse to regularisation and gamma, which increases or decreases the senstivity of our Gaussian function. I won't go into the full details, but it acts like another regularisation parameter. If you are interested to know more about the details, Andrew Ng's Coursera course is my recommended source.","bf95cdee":"The precision is bad, but the recall is good in this case. The low precision means that we have too many false positives and the algorithm often predicts a fraudulent transaction where there isn't one! This would be fine if we wanted to be very cautious about frudulent transactions, but customers would get very frustrated having their credit card wrongly declined.\n\nOn the other hand, we have high recall, which means we don't have many false negatives. This means that we don't miss many fraudulent transactions.\n\nWe can change the trade-off by between precision and recall by adjusting the threshold of our logistic regression classifier.\n\nWe may also be underfitting or overfitting to our training data. We simply used the defulat value for C, the inverse of the regularisation strength.\n\nSince we don't know whether we would prefer to maximise precision or recall, let's also introduce the F1 score as our metric for evaluating our cross-validation performance:\n\n\\begin{align}\nF_1 = 2 \\frac{Precision \\times Recall}{Precision + Recall}\n\\end{align}\n\nThis is also known as the harmonic mean of the precision and recall. Implmenting this:","44939559":"## 1.6 - Conclusion\nOverall the test data results are on par with the cross-validation set with an F1 score of 15.2% overall. The recall is good, but the precision is poor!\n\nThis is not surprising given that the model used was quite a simple one. We could generate a more complex logisitic regression model, by engineering new features and using these. In future I may come back to this in future, but for now I will move on to Support Vector Machines (SVMs).","a88aa0ee":"Overall these results are fairly similar to what we got with our first attempt when using the logistic regression. I will leave linear regression here for the sake of keeping this notebook brief. Normally the same process of tuning the hyperparameters (in this case, C) to improve the cross validation score is the next step.\n\n## 2.2 - Radial Basis Function Kernel (Gaussian Kernel)\n\nLet's move on to using the Gaussian kernel and see how this performs before tuning the model."}}