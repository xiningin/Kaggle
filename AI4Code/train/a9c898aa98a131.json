{"cell_type":{"5e9b1f50":"code","87049991":"code","d62bac07":"code","95d1bef9":"code","eafdc810":"code","959e5658":"code","f3a90cb3":"code","b9843021":"code","d1021179":"code","d9ecef83":"code","72b729f0":"code","21b20a85":"code","2fb46a52":"code","988afcbf":"code","41107ee0":"code","49b454b7":"code","fc8e8833":"code","956b7e98":"code","70c53aae":"code","201feb60":"code","28272d23":"code","372c031a":"code","26c78b4d":"code","04d701b7":"code","ad22e90c":"code","73fc58af":"code","70cbffd7":"code","74f80b0d":"code","be33e6c1":"code","f20ff057":"code","7f326aaa":"code","0b94078d":"code","b5674838":"code","ca4f738e":"code","8e20864a":"code","309544ac":"code","b21ad304":"code","6ba44055":"code","bc6921f4":"code","6cc551ae":"code","d772bf4a":"markdown","ba5761cd":"markdown","855381a1":"markdown","6685a5ed":"markdown","73307d0c":"markdown","9b7ce670":"markdown","36217c0b":"markdown","f0f060c8":"markdown","b1e361ea":"markdown","5d0e9121":"markdown","0736e8ed":"markdown","d0e2c769":"markdown","54326819":"markdown","f031f76b":"markdown","3cf6f936":"markdown","c1f41bf1":"markdown","73a91405":"markdown","4df4e287":"markdown","0d98dbec":"markdown","b4444138":"markdown"},"source":{"5e9b1f50":"import pandas as pd\nimport numpy as np\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport re\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.naive_bayes import GaussianNB\nimport warnings\nwarnings.filterwarnings('ignore') \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV, cross_val_score","87049991":"# Loading the test and training set\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\n\n# Combining both sets\nfull_data = [train, test]","d62bac07":"# Finding shape of each set\nprint('''Training set shape: {}\nTest set shape: {}'''.format(train.shape, test.shape))","95d1bef9":"# Exploring variables of each set\nfor dataset in full_data:\n    info = dataset.info()\n    print(info)","eafdc810":"# Creating a title column from the name variable\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\nfor dataset in full_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n    unique_title_count = pd.DataFrame(dataset['Title'].value_counts())\n    print(unique_title_count)","959e5658":"#\u00a0Some titles are the same so we will place them together\nfor dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\n# We have quite a few titles uncommon titles (<10) so we will replace them with 'Other'\nfor dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Dr', 'Rev', 'Major', 'Col', 'Lady', 'Capt', 'Countess', 'Jonkheer', 'Sir', 'Don', 'Dr', 'Dona'], 'Other')\n    print(dataset['Title'].value_counts())","f3a90cb3":"# Creating family size variable\nfor dataset in full_data:\n    dataset['FamilySize'] = 1 + dataset['SibSp'] + dataset['Parch']","b9843021":"pd.DataFrame(train)","d1021179":"corr_matrix = train.corr()\ncorr_matrix['Survived'].sort_values(ascending=False)","d9ecef83":"sns.heatmap(train[['Survived', 'Pclass','Sex',  'Age', 'SibSp', 'Parch', 'Fare', 'FamilySize']].corr(),annot=True, fmt = \".2f\", cmap = \"plasma\")","72b729f0":"# Probability of survival depending on sex\nsns.factorplot(data=train, x='Sex', y='Survived', kind='bar', palette='winter')","21b20a85":"train[[\"Sex\",\"Survived\"]].groupby('Sex').mean()","2fb46a52":"# Probability of survival depending on title\nsns.factorplot(data=train, x='Title', y='Survived', kind='bar', palette='winter')","988afcbf":"title_surv_df = train[[\"Title\",\"Survived\"]].groupby('Title').mean().sort_values(by='Survived', ascending=False)\ntitle_surv_df.style.background_gradient(subset=['Survived'], cmap='Greens')","41107ee0":"fare_under_150 = train[train['Fare'] < 150]\n\nsns.boxplot(y=fare_under_150['Fare'], x=train['Survived'], palette='winter')","49b454b7":"fare_surv_df = train[[\"Survived\",\"Fare\"]].groupby('Survived').mean().sort_values(by='Survived', ascending=False)\npd.DataFrame(fare_surv_df['Fare'].rename('Mean Fare Paid'))","fc8e8833":"# Probability of survival depending on location embarked from\nsns.factorplot(data=train, x='Embarked', y='Survived', kind='bar', palette='winter')","956b7e98":"emb_surv_df = train[[\"Embarked\",\"Survived\"]].groupby('Embarked').mean().sort_values(by='Survived', ascending=False)\nemb_surv_df","70c53aae":"# Probability of survival depending on family size\nsns.factorplot(data=train, x='FamilySize', y='Survived', kind='bar', palette='winter')","201feb60":"fam_surv_df = pd.crosstab(train['Survived'], train['FamilySize'])\nfam_surv_df","28272d23":"sns.factorplot(x='Survived', y = 'Age',data = train, kind='violin', palette='winter')","372c031a":"# Finding null values\ndef null_percentage(data):\n    total = data.isnull().sum().sort_values(ascending = False)\n    percent = round(data.isnull().sum().sort_values(ascending = False)\/len(data)*100,2)\n    return pd.concat([total, percent], axis=1, keys=['Total','Percent'])\n\nprint('''---Training Set Nulls--- \\n {} \\n\n---Test Set Nulls--- \\n {}'''.format(null_percentage(train), null_percentage(test)))","26c78b4d":"# Now we have checked for nulls we can split the training set into X and y\nX_train = train.drop(['Survived'], axis=1).copy()\ny_train = train['Survived']\n\nfull_data_X = [X_train, test]","04d701b7":"#\u00a0Making a copy for test set so that we have the passengerIDs for submission\ntest_copy = test.copy()\n\n# Dropping unwanted columns\nfor dataset in full_data_X:\n    dataset.drop(['PassengerId', 'SibSp', 'Parch', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)","ad22e90c":"# Splitting cols into numerical and categorical\nnumerical_cols = ['Age', 'Fare', 'FamilySize']\ncategorical_cols = ['Pclass', 'Sex', 'Embarked', 'Title']\n\nprint('Numerical Columns: {}\\n'.format(numerical_cols))\nprint('Categorical Columns: {}\\n'.format(categorical_cols))","73fc58af":"# Numerical column transformer\nnum_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\n# Categorical column transformer\ncat_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore')),\n])\n\n# Preprocessing pipeline\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', num_transformer, numerical_cols),\n        ('cat', cat_transformer, categorical_cols)\n    ])\n\n\n# Fitting the data and transforming the training & test set\nX_train_preprocessed = preprocessor.fit_transform(X_train)\ntest_preprocessed = preprocessor.fit_transform(test)\n\npd.DataFrame(X_train_preprocessed)","70cbffd7":"X_train = pd.DataFrame(X_train_preprocessed)","74f80b0d":"# Kfold cross validation to validate possible models\nk_fold = StratifiedKFold(n_splits=10)\nrandom_state = 0","be33e6c1":"# Logistic Regression\nlog_reg = LogisticRegression(random_state=random_state)\nlog_reg.fit(X_train, y_train)\n\n# Using cross validation\nlog_reg_cv = cross_val_score(log_reg, X_train, y = y_train, scoring = \"accuracy\", cv = k_fold, n_jobs=5)\nlog_reg_cv_mean = log_reg_cv.mean()\nlog_reg_cv_mean","f20ff057":"# K-NN\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\n\n# Using cross validation\nknn_cv = cross_val_score(knn, X_train, y = y_train, scoring = \"accuracy\", cv = k_fold, n_jobs=5)\nknn_cv_mean = knn_cv.mean()\nknn_cv_mean","7f326aaa":"# Kernel SVM\nk_svm = SVC(kernel='rbf', random_state=0)\nk_svm.fit(X_train, y_train)\n\n# Using cross validation\nk_svm_cv = cross_val_score(k_svm, X_train, y = y_train, scoring = \"accuracy\", cv = k_fold, n_jobs=5)\nk_svm_cv_mean = k_svm_cv.mean()\nk_svm_cv_mean","0b94078d":"# Naive Bayes\nnaive_bayes = GaussianNB()\nnaive_bayes.fit(X_train, y_train)\n\n# Using cross validation\nnaive_bayes_cv = cross_val_score(naive_bayes, X_train, y = y_train, scoring = \"accuracy\", cv = k_fold, n_jobs=5)\n\nnaive_bayes_cv_mean = naive_bayes_cv.mean()\nnaive_bayes_cv_mean","b5674838":"# Random Forest\nrf = RandomForestClassifier(n_estimators=100, random_state=0)\nrf.fit(X_train, y_train)\n\n# Using cross validation\nrf_cv = cross_val_score(rf, X_train, y = y_train, scoring = \"accuracy\", cv = k_fold, n_jobs=-1)\nprint(rf_cv.mean())\n\n# Grid Search\nparam_grid = [{'bootstrap': [True, False],\n 'max_depth': [10, 20, 30, 40, 50, None],\n 'max_features': ['auto', 'sqrt'],\n 'min_samples_leaf': [1, 2, 4],\n 'min_samples_split': [5, 10, 12],\n 'n_estimators': [100, 200, 300, 350]},\n]\n\nrf_2 = RandomForestClassifier(random_state=random_state)\n\ngrid_search = GridSearchCV(rf_2, param_grid, cv=5, scoring='neg_mean_squared_error')\n\ngrid_search.fit(X_train, y_train)","ca4f738e":"rf_best_params = grid_search.best_params_\nrf_best_params","8e20864a":"# Using best parameters for random forest\nrf3 = RandomForestClassifier(bootstrap= True,\n max_depth= 20,\n max_features= 'auto',\n min_samples_leaf= 1,\n min_samples_split= 10,\n n_estimators= 300,\nrandom_state=random_state)\nrf3.fit(X_train, y_train)\n\n# Using cross validation\nrf3_cv = cross_val_score(rf3, X_train, y = y_train, scoring = \"accuracy\", cv = k_fold, n_jobs=5)\nrf3_cv_mean = rf3_cv.mean()\nrf3_cv_mean","309544ac":"# XGBoost\nxgb = XGBClassifier()\nxgb.fit(X_train, y_train)\n\n# Using cross validation\nxgb_cv = cross_val_score(xgb, X_train, y = y_train, scoring = \"accuracy\", cv = k_fold, n_jobs=5)\nxgb_cv_mean = xgb_cv.mean()\nxgb_cv_mean","b21ad304":"# Creating dataframe showing accuracy scores of each model\ncompare = {'Model': ['Logistic Regression', 'K-NN', 'Kernel SVM', 'Naive Bayes', 'Random Forest', 'XGBoost'],\n          'Cross validation score': [log_reg_cv_mean, knn_cv_mean, k_svm_cv_mean, naive_bayes_cv_mean, rf3_cv_mean, xgb_cv_mean]}\ncompare_df = pd.DataFrame(data=compare)\n \n# Visualising best scores\ncompare_df.style.bar(subset=['Cross validation score'], color='#d65f5f')","6ba44055":"# Using our best model (Random Forest) to predict on test set\nrf3_test_predictions = rf3.predict(test_preprocessed)","bc6921f4":"submission = pd.DataFrame({'PassengerId':test_copy['PassengerId'],'Survived':rf3_test_predictions})\n\n#Visualising the first 5 rows\nsubmission.head()","6cc551ae":"#Converting DataFrame to a csv file\n#This is saved in the same directory as your notebook\ntitanic_predictions = 'Titanic Predictions 2.csv'\n\nsubmission.to_csv(titanic_predictions,index=False)\n\nprint('Saved file: ' + titanic_predictions)","d772bf4a":"#### 2. K-Nearest Neighbours","ba5761cd":"We can see that females are a lot more likely to survive than males. This is obviously a key factor in the probability of survival. Next we will delve deeper and see if there is any correlation between title and survival.","855381a1":"From this graph we can see that people that embarked from Cherbourg had the highest probability of survival, with Southampton having the lowest. Could this be due to the order in which they embarked, and hence location of stay on the boat?","6685a5ed":"### Comparing models","73307d0c":"### Selecting features and building preprocessing pipelines","9b7ce670":"### Searching data for null values","36217c0b":"## Feature Engineering","f0f060c8":"### Importing the data","b1e361ea":"#### 6. XGBoost","5d0e9121":"This box plot tells us that the fare paid generally affects the rate of survival, with a higher average fare for people that survived. However, we can see that this is not a perfect factor as many people who paid a high fare did not survive.","0736e8ed":"## Data Visualisation","d0e2c769":"#### 1. Logistic Regression Classifier","54326819":"#### 4. Naive Bayes","f031f76b":"### Importing Libaries","3cf6f936":"# Titanic data analysis and survival predictions","c1f41bf1":"As we can see, the group with the highest level of survival were married females (Mrs), with adult males (Mr) being the group least likely to survive. Younger males (Master) were a lot more likely to survive than adult males. As 'Other' contains many different titles, we have a higher standard deviation, possibly due to containing royalty. ","73a91405":"### Test set prediction","4df4e287":"## Model Building and predictions","0d98dbec":"#### 3. Kernel SVM","b4444138":"#### 5. Random Forest Classifier w\/ Grid Search"}}