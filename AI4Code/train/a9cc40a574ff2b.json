{"cell_type":{"ec51afd9":"code","cdd197bc":"code","71fae877":"code","f1409cad":"code","af999fcb":"code","0349aab3":"code","766b5441":"code","b52ece0d":"code","b0ed6475":"code","025c4a20":"code","160abbc6":"code","47bd0ab6":"code","dde64b54":"code","08114a23":"code","9f4fa06c":"code","3c32a155":"code","cb6c00fc":"code","af4ce4d0":"code","5e0dc6e7":"code","cb44a624":"code","5c8c3dbe":"code","79a9906f":"code","99617ada":"code","ee4c1862":"code","ffdf4c73":"code","ffeff1da":"code","b0dafa8d":"code","bb5c9478":"code","c472d392":"code","83e18cbd":"code","a33c117d":"code","5a63fe04":"code","85dc3f87":"code","7b2c1d8b":"code","63054886":"code","fcd4b8fb":"code","86d2e245":"code","014921ec":"code","a51815b2":"code","bd4133ca":"code","9793e18c":"code","43d75442":"code","f8b3848f":"code","b058e9bb":"code","a1bb58d7":"code","83599f37":"code","897cba25":"code","8cce31df":"code","4acfa30d":"code","01a54eac":"code","381a9cb0":"code","2bf4ac72":"code","fffb8790":"code","f9c40897":"code","e94749d2":"code","50bef577":"code","305c3a74":"code","5d46af48":"code","446655f9":"code","0094eccf":"markdown","240e9a92":"markdown"},"source":{"ec51afd9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cdd197bc":"import matplotlib.pyplot as plt\nimport seaborn as sns\ncolor=sns.color_palette()\nimport matplotlib as mpl","71fae877":"from sklearn import preprocessing as pp\nfrom scipy.stats import pearsonr\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import precision_recall_curve,average_precision_score\nfrom sklearn.metrics import roc_curve,auc,roc_auc_score\nfrom sklearn.metrics import confusion_matrix,classification_report","f1409cad":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nimport lightgbm as lgb","af999fcb":"#current_path=os.getcwd()\n#file=\"\"\ndata=pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")","0349aab3":"data.head()","766b5441":"data.describe()","b52ece0d":"data.columns","b0ed6475":"data['Class'].value_counts()","025c4a20":"data['Time']","160abbc6":"data.dtypes","47bd0ab6":"#Checking for null values\nnanCounter=np.isnan(data).sum()\nnanCounter","dde64b54":"distinctCounter=data.apply(lambda x: len(x.unique()))\ndistinctCounter","08114a23":"#Creating featureX and featureY\ndataX=data.drop(['Class'],axis=1).copy()\ndataY=data['Class'].copy()","9f4fa06c":"#Standardize the feature matrix X\nfeaturesToScale=dataX.drop(['Time'],axis=1).columns\nsX=pp.StandardScaler(copy=True)\ndataX.loc[:,featuresToScale]=sX.fit_transform(dataX[featuresToScale])","3c32a155":"dataX","cb6c00fc":"#Check correlation of features\ncorrelationMatrix= pd.DataFrame(data=[],index=dataX.columns,columns=dataX.columns)\nfor i in dataX.columns:\n    for j in dataX.columns:\n        correlationMatrix.loc[i,j]=np.round(pearsonr(dataX.loc[:,i],dataX.loc[:,j])[0],2)\n        \ncorrelationMatrix","af4ce4d0":"#Data Visualization\ncount_classes=pd.value_counts(data['Class'],sort=True).sort_index()\nax=sns.barplot(data=data,x=count_classes.index,y=(count_classes\/len(data)))\nax.set_title('Frequency Percentage by Class')\nax.set_xlabel('Class')\nax.set_ylabel('Frequency Percentage')","5e0dc6e7":"#Splitting training and test sets\nX_train,X_test,y_train,y_test=train_test_split(dataX,dataY,test_size=0.33,random_state=2018,stratify=dataY)","cb44a624":"#Crossvalidation\nk_fold=StratifiedKFold(n_splits=5,shuffle=True,random_state=2018)","5c8c3dbe":"#Machine learning models","79a9906f":"#Model 1: Logistic Regression\n#Set hyperparameters\npenalty=\"l2\" #As it is less sensitive to outliers\nC=1.0 #It is regularization strength #It addresses overfitting by penalizing complexity\n#Always positive floating number\n#The smaller the value, the smaller the regularization\n#Stronger the regularization,the greater penalty is applied to complexity\n\nclass_weight='balanced'\nrandom_state= 2018\nsolver='liblinear'\nn_jobs=-1\nlogReg=LogisticRegression(penalty=penalty,\n                         C=C,\n                         class_weight=class_weight,\n                         random_state=random_state,\n                         solver=solver,\n                         n_jobs=n_jobs)","99617ada":"#Training the model\ntrainingScores=[]\ncvScores=[]\npredictionsBasedOnKFolds=pd.DataFrame(data=[],index=y_train.index,columns=[0,1])\nmodel=logReg\n#The numpy module of Python provides a function called numpy.ravel, \n#which is used to change a 2-dimensional array or a multi-dimensional array into a contiguous flattened array.\nfor train_index,cv_index in k_fold.split(np.zeros(len(X_train)),y_train.ravel()):\n    \n    X_train_fold,X_cv_fold=X_train.iloc[train_index,:],X_train.iloc[cv_index,:]\n    y_train_fold,y_cv_fold=y_train.iloc[train_index],y_train.iloc[cv_index]\n    \n    model.fit(X_train_fold,y_train_fold)\n    \n    loglossTraining=log_loss(y_train_fold,model.predict_proba(X_train_fold)[:,1])\n    \n    trainingScores.append(loglossTraining)\n    \n    predictionsBasedOnKFolds.loc[X_cv_fold.index,:]=model.predict_proba(X_cv_fold)\n    loglossCV=log_loss(y_cv_fold,predictionsBasedOnKFolds.loc[X_cv_fold.index,1])\n    cvScores.append(loglossCV)\n    \n    print('Training Log Loss:',loglossTraining)\n    print('CV Log Loss:',loglossCV)\n    \nloglossLogisticRegression=log_loss(y_train,predictionsBasedOnKFolds.loc[:,1])\nprint('Logistic Regression Log Loss:',loglossLogisticRegression)","ee4c1862":"np.mean(trainingScores)","ffdf4c73":"np.mean(cvScores)","ffeff1da":"predictionsBasedOnKFolds.loc[:,1]","b0dafa8d":"#Evaluating the logistic regression model\npreds=pd.concat([y_train,predictionsBasedOnKFolds.loc[:,1]],axis=1)\npreds.columns=['trueLabel','prediction']\npredictionsBasedOnKFoldsLogisticRegression=preds.copy()\n\nprecision,recall,thresholds= precision_recall_curve(preds['trueLabel'],preds['prediction'])\n\naverage_precision= average_precision_score(preds['trueLabel'],preds['prediction'])\n\nplt.step(recall,precision,color='k',alpha=0.7,where='post')\nplt.fill_between(recall,precision,step='post',alpha=0.3,color='k')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0,1.05])\nplt.xlim([0.0,1.0])\n\nplt.title('Precision-Recall curve: Average Precision= {0:0.2f}'.format(average_precision))","bb5c9478":"#Area Under the Curve\n\nfpr,tpr,thresholds= roc_curve(preds['trueLabel'],preds['prediction'])\nareaUnderROC=auc(fpr,tpr)\n\nplt.figure()\nplt.plot(fpr,tpr,color='r',lw=2,label='ROC curve')\nplt.plot([0,1],[0,1],color='k',lw=2,linestyle='--')\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic:Area under the curve={0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","c472d392":"#Model 2: Random Forests\n#Setting the hyperparameters\nn_estimators=10 #We will build 10 trees and average the results across these 10 trees\nmax_features='auto'\nmax_depth=None\nmin_samples_split=2\nmin_samples_leaf=1\nmin_weight_fraction_leaf=0.0\nmax_leaf_nodes=None\nbootstrap=True\noob_score=False\nn_jobs=-1\nrandom_state=2018\nclass_weight='balanced'\n\nRFC=RandomForestClassifier(n_estimators=n_estimators,\n                          max_features=max_features,max_depth=max_depth,\n                          min_samples_split=min_samples_split,\n                          min_samples_leaf=min_samples_leaf,\n                          min_weight_fraction_leaf=min_weight_fraction_leaf,\n                          max_leaf_nodes=max_leaf_nodes,bootstrap=bootstrap,\n                          oob_score=oob_score,n_jobs=n_jobs,random_state=random_state,\n                          class_weight=class_weight)","83e18cbd":"trainingScores=[]\ncvScores=[]\npredictionsBasedOnKFolds=pd.DataFrame(data=[],index=y_train.index,columns=[0,1])\nmodel=RFC\n\nfor train_index,cv_index in k_fold.split(np.zeros(len(X_train)),y_train.ravel()):\n    X_train_fold,X_cv_fold=X_train.iloc[train_index,:],X_train.iloc[cv_index,:]\n    y_train_fold,y_cv_fold=y_train.iloc[train_index],y_train.iloc[cv_index]\n    \n    model.fit(X_train_fold,y_train_fold)\n    loglossTraining=log_loss(y_train_fold,model.predict_proba(X_train_fold)[:,1])\n    trainingScores.append(loglossTraining)\n    predictionsBasedOnKFolds.loc[X_cv_fold.index,:]=model.predict_proba(X_cv_fold)\n    loglossCV=log_loss(y_cv_fold,predictionsBasedOnKFolds.loc[X_cv_fold.index,1])\n    cvScores.append(loglossCV)\n    \n    print('Training Log Loss:',loglossTraining)\n    print('CV Log Loss:',loglossCV)\n    \nloglossRandomForestsClassifier=log_loss(y_train,predictionsBasedOnKFolds.loc[:,1])\nprint('Random Forests Log Loss:',loglossRandomForestsClassifier)\n    \n    \n    ","a33c117d":"#Evaluating the random forest model\npreds=pd.concat([y_train,predictionsBasedOnKFolds.loc[:,1]],axis=1)\npreds.columns=['trueLabel','prediction']\npredictionsBasedOnKFoldsRandomForests=preds.copy()\n\nprecision,recall,thresholds= precision_recall_curve(preds['trueLabel'],preds['prediction'])\n\naverage_precision= average_precision_score(preds['trueLabel'],preds['prediction'])\n\nplt.step(recall,precision,color='k',alpha=0.7,where='post')\nplt.fill_between(recall,precision,step='post',alpha=0.3,color='k')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0,1.05])\nplt.xlim([0.0,1.0])\n\nplt.title('Precision-Recall curve: Average Precision= {0:0.2f}'.format(average_precision))","5a63fe04":"#Area Under the Curve\n\nfpr,tpr,thresholds= roc_curve(preds['trueLabel'],preds['prediction'])\nareaUnderROC=auc(fpr,tpr)\n\nplt.figure()\nplt.plot(fpr,tpr,color='r',lw=2,label='ROC curve')\nplt.plot([0,1],[0,1],color='k',lw=2,linestyle='--')\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic:Area under the curve={0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","85dc3f87":"#Model 3: XGBoost\n#Setting the hyperparameters\n\nparams_xGB = {\n    'nthread':16, #number of cores\n    'learning_rate':0.3, #range 0 to 1, default 0.3\n    'gamma': 0, #range 0 to infinity, default 0\n# increase to reduce complexity (increase bias, reduce variance)\n    'max_depth': 6, #range 1 to infinity, default 6\n    'min_child_weight': 1, #range 0 to infinity, default 1\n    'max_delta_step': 0, #range 0 to infinity, default 0\n    'subsample': 1.0, #range 0 to 1, default 1\n    # subsample ratio of the training examples\n    'colsample_bytree': 1.0, #range 0 to 1, default 1\n    # subsample ratio of features\n    'objective':'binary:logistic',\n    'num_class':1,\n    'eval_metric':'logloss',\n    'seed':2018,\n    #'silent':1\n    \n    \n}","7b2c1d8b":"trainingScores=[]\ncvScores=[]\npredictionsBasedOnKFolds=pd.DataFrame(data=[],index=y_train.index,columns=['predictions'])\n\nfor train_index,cv_index in k_fold.split(np.zeros(len(X_train)),y_train.ravel()):\n    X_train_fold,X_cv_fold=X_train.iloc[train_index,:],X_train.iloc[cv_index,:]\n    y_train_fold,Y_cv_fold=y_train.iloc[train_index],y_train.iloc[cv_index]\n    dtrain=xgb.DMatrix(data=X_train_fold,label=y_train_fold)\n    dCV=xgb.DMatrix(data=X_cv_fold)\n    bst=xgb.cv(params_xGB,dtrain,num_boost_round=2000,nfold=5,early_stopping_rounds=200,verbose_eval=50)\n    best_rounds=np.argmin(bst['test-logloss-mean'])\n    bst=xgb.train(params_xGB,dtrain,best_rounds)\n    \n    loglossTraining=log_loss(y_train_fold,bst.predict(dtrain))\n    trainingScores.append(loglossTraining)\n    \n    predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction']=bst.predict(dCV)\n    loglossCV=log_loss(y_cv_fold,predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'])\n    cvScores.append(loglossCV)\n    \n    print('Training Log Loss:',loglossTraining)\n    print('CV Log Loss:',loglossCV)\n    \nloglossXGBoostGradientBoosting=log_loss(y_train,predictionsBasedOnKFolds.loc[:,'prediction'])\n\nprint('XGBoost Gradient Boosting Log Loss:',loglossXGBoostGradientBoosting)\n    \n    ","63054886":"preds = pd.concat([y_train,predictionsBasedOnKFolds.loc[:,'prediction']], axis=1)\npreds.columns = ['trueLabel','prediction']\npredictionsBasedOnKFoldsXGBoostGradientBoosting = preds.copy()\n\nprecision, recall, thresholds = \\\n    precision_recall_curve(preds['trueLabel'],preds['prediction'])\naverage_precision = \\\n    average_precision_score(preds['trueLabel'],preds['prediction'])\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = roc_curve(preds['trueLabel'],preds['prediction'])\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\n        Area under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","fcd4b8fb":"#Model 4: LightGBM\n#Setting the hyperparameters\nparams_lightGB = {\n'task': 'train',\n'application':'binary',\n'num_class':1,\n'boosting': 'gbdt',\n'objective': 'binary',\n'metric': 'binary_logloss',\n'metric_freq':50,\n'is_training_metric':False,\n'max_depth':4,\n'num_leaves': 31,\n'learning_rate': 0.01,\n'feature_fraction': 1.0,\n'bagging_fraction': 1.0,\n'bagging_freq': 0,\n'bagging_seed': 2018,\n'verbose': 0,\n'num_threads':16\n}","86d2e245":"trainingScores = []\ncvScores = []\npredictionsBasedOnKFolds = pd.DataFrame(data=[],\n                                index=y_train.index,columns=['prediction'])\n\nfor train_index, cv_index in k_fold.split(np.zeros(len(X_train)),\n                                          y_train.ravel()):\n    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], \\\n        X_train.iloc[cv_index,:]\n    y_train_fold, y_cv_fold = y_train.iloc[train_index], \\\n        y_train.iloc[cv_index]\n    \n    lgb_train = lgb.Dataset(X_train_fold, y_train_fold)\n    lgb_eval = lgb.Dataset(X_cv_fold, y_cv_fold, reference=lgb_train)\n    gbm = lgb.train(params_lightGB, lgb_train, num_boost_round=2000,\n                   valid_sets=lgb_eval, early_stopping_rounds=200)\n    \n    loglossTraining = log_loss(y_train_fold, \\\n                gbm.predict(X_train_fold, num_iteration=gbm.best_iteration))\n    trainingScores.append(loglossTraining)\n    \n    predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'] = \\\n        gbm.predict(X_cv_fold, num_iteration=gbm.best_iteration) \n    loglossCV = log_loss(y_cv_fold, \\\n        predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'])\n    cvScores.append(loglossCV)\n    \n    print('Training Log Loss: ', loglossTraining)\n    print('CV Log Loss: ', loglossCV)\n    \nloglossLightGBMGradientBoosting = \\\n    log_loss(y_train, predictionsBasedOnKFolds.loc[:,'prediction'])\nprint('LightGBM Gradient Boosting Log Loss: ', loglossLightGBMGradientBoosting)","014921ec":"trainingScores = []\ncvScores = []\npredictionsBasedOnKFolds = pd.DataFrame(data=[],\n                                index=y_train.index,columns=['prediction'])\n\nfor train_index, cv_index in k_fold.split(np.zeros(len(X_train)),\n                                          y_train.ravel()):\n    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], \\\n        X_train.iloc[cv_index,:]\n    y_train_fold, y_cv_fold = y_train.iloc[train_index], \\\n        y_train.iloc[cv_index]\n    \n    lgb_train = lgb.Dataset(X_train_fold, y_train_fold)\n    lgb_eval = lgb.Dataset(X_cv_fold, y_cv_fold, reference=lgb_train)\n    gbm = lgb.train(params_lightGB, lgb_train, num_boost_round=2000,\n                   valid_sets=lgb_eval, early_stopping_rounds=200)\n    \n    loglossTraining = log_loss(y_train_fold, \\\n                gbm.predict(X_train_fold, num_iteration=gbm.best_iteration))\n    trainingScores.append(loglossTraining)\n    \n    predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'] = \\\n        gbm.predict(X_cv_fold, num_iteration=gbm.best_iteration) \n    loglossCV = log_loss(y_cv_fold, \\\n        predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'])\n    cvScores.append(loglossCV)\n    \n    print('Training Log Loss: ', loglossTraining)\n    print('CV Log Loss: ', loglossCV)\n    \nloglossLightGBMGradientBoosting = \\\n    log_loss(y_train, predictionsBasedOnKFolds.loc[:,'prediction'])\nprint('LightGBM Gradient Boosting Log Loss: ', loglossLightGBMGradientBoosting)","a51815b2":"preds = pd.concat([y_train,predictionsBasedOnKFolds.loc[:,'prediction']], axis=1)\npreds.columns = ['trueLabel','prediction']\npredictionsBasedOnKFoldsLightGBMGradientBoosting = preds.copy()\n\nprecision, recall, thresholds = \\\n    precision_recall_curve(preds['trueLabel'],preds['prediction'])\naverage_precision = \\\n    average_precision_score(preds['trueLabel'],preds['prediction'])\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = roc_curve(preds['trueLabel'],preds['prediction'])\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\nArea under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","bd4133ca":"predictionsTestSetLogisticRegression = \\\n    pd.DataFrame(data=[],index=y_test.index,columns=['prediction'])\npredictionsTestSetLogisticRegression.loc[:,'prediction'] = \\\n    logReg.predict_proba(X_test)[:,1]\nlogLossTestSetLogisticRegression = \\\n    log_loss(y_test, predictionsTestSetLogisticRegression)","9793e18c":"predictionsTestSetRandomForests = \\\n    pd.DataFrame(data=[],index=y_test.index,columns=['prediction'])\npredictionsTestSetRandomForests.loc[:,'prediction'] = \\\n    RFC.predict_proba(X_test)[:,1]\nlogLossTestSetRandomForests = \\\n    log_loss(y_test, predictionsTestSetRandomForests)","43d75442":"predictionsTestSetXGBoostGradientBoosting = \\\n    pd.DataFrame(data=[],index=y_test.index,columns=['prediction'])\ndtest = xgb.DMatrix(data=X_test)\npredictionsTestSetXGBoostGradientBoosting.loc[:,'prediction'] = \\\n    bst.predict(dtest)\nlogLossTestSetXGBoostGradientBoosting = \\\n    log_loss(y_test, predictionsTestSetXGBoostGradientBoosting)","f8b3848f":"predictionsTestSetLightGBMGradientBoosting = \\\n    pd.DataFrame(data=[],index=y_test.index,columns=['prediction'])\npredictionsTestSetLightGBMGradientBoosting.loc[:,'prediction'] = \\\n    gbm.predict(X_test, num_iteration=gbm.best_iteration)\nlogLossTestSetLightGBMGradientBoosting = \\\n    log_loss(y_test, predictionsTestSetLightGBMGradientBoosting)","b058e9bb":"print(\"Log Loss of Logistic Regression on Test Set: \", \\\n          logLossTestSetLogisticRegression)\nprint(\"Log Loss of Random Forests on Test Set: \", \\\n          logLossTestSetRandomForests)\n# print(\"Log Loss of XGBoost Gradient Boosting on Test Set: \", \\\n#           logLossTestSetXGBoostGradientBoosting)\nprint(\"Log Loss of LightGBM Gradient Boosting on Test Set: \", \\\n          logLossTestSetLightGBMGradientBoosting)","a1bb58d7":"precision, recall, thresholds = \\\n    precision_recall_curve(y_test,predictionsTestSetLogisticRegression)\naverage_precision = \\\n    average_precision_score(y_test,predictionsTestSetLogisticRegression)\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = roc_curve(y_test,predictionsTestSetLogisticRegression)\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\nArea under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","83599f37":"precision, recall, thresholds = \\\n    precision_recall_curve(y_test,predictionsTestSetRandomForests)\naverage_precision = \\\n    average_precision_score(y_test,predictionsTestSetRandomForests)\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = roc_curve(y_test,predictionsTestSetRandomForests)\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\nArea under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","897cba25":"precision, recall, thresholds = \\\n    precision_recall_curve(y_test,predictionsTestSetXGBoostGradientBoosting)\naverage_precision = \\\n    average_precision_score(y_test,predictionsTestSetXGBoostGradientBoosting)\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = \\\n    roc_curve(y_test,predictionsTestSetXGBoostGradientBoosting)\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\nArea under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","8cce31df":"precision, recall, thresholds = \\\n    precision_recall_curve(y_test,predictionsTestSetLightGBMGradientBoosting)\naverage_precision = \\\n    average_precision_score(y_test,predictionsTestSetLightGBMGradientBoosting)\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = \\\n    roc_curve(y_test,predictionsTestSetLightGBMGradientBoosting)\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\nArea under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","4acfa30d":"#Ensemble\n# Without XGBoost\npredictionsBasedOnKFoldsFourModels = pd.DataFrame(data=[],index=y_train.index)\npredictionsBasedOnKFoldsFourModels = predictionsBasedOnKFoldsFourModels.join(\n    predictionsBasedOnKFoldsLogisticRegression['prediction'].astype(float), \\\n    how='left').join(predictionsBasedOnKFoldsRandomForests['prediction'] \\\n    .astype(float),how='left',rsuffix=\"2\").join( \\\n    predictionsBasedOnKFoldsLightGBMGradientBoosting['prediction'].astype(float), \\\n    how='left',rsuffix=\"4\")\npredictionsBasedOnKFoldsFourModels.columns = \\\n    ['predsLR','predsRF','predsLightGBM']","01a54eac":"predictionsBasedOnKFoldsFourModels['predsRF'].value_counts()","381a9cb0":"# predictionsBasedOnKFoldsFourModels = pd.DataFrame(data=[],index=y_train.index)\n# predictionsBasedOnKFoldsFourModels = predictionsBasedOnKFoldsFourModels.join(\n#     predictionsBasedOnKFoldsLogisticRegression['prediction'].astype(float), \\\n#     how='left').join(predictionsBasedOnKFoldsRandomForests['prediction'] \\\n#     .astype(float),how='left',rsuffix=\"2\").join( \\\n#     predictionsBasedOnKFoldsXGBoostGradientBoosting['prediction'].astype(float), \\\n#     how='left',rsuffix=\"3\").join( \\\n#     predictionsBasedOnKFoldsLightGBMGradientBoosting['prediction'].astype(float), \\\n#     how='left',rsuffix=\"4\")\n# predictionsBasedOnKFoldsFourModels.columns = \\\n#     ['predsLR','predsRF','predsXGB','predsLightGBM']","2bf4ac72":"X_trainWithPredictions = \\\n    X_train.merge(predictionsBasedOnKFoldsFourModels,\n                  left_index=True,right_index=True)\nX_trainWithPredictions","fffb8790":"params_lightGB = {\n    'task': 'train',\n    'num_class':1,\n    'boosting': 'gbdt',\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'metric_freq':50,\n    'is_training_metric':False,\n    'max_depth':4,\n    'num_leaves': 31,\n    'learning_rate': 0.01,\n    'feature_fraction': 1.0,\n    'bagging_fraction': 1.0,\n    'bagging_freq': 0,\n    'bagging_seed': 2018,\n    'verbose': -1,\n    'num_threads':16\n}","f9c40897":"trainingScores = []\ncvScores = []\npredictionsBasedOnKFoldsEnsemble = \\\n    pd.DataFrame(data=[],index=y_train.index,columns=['prediction'])\n\nfor train_index, cv_index in k_fold.split(np.zeros(len(X_train)), \\\n                                          y_train.ravel()):\n    X_train_fold, X_cv_fold = \\\n        X_trainWithPredictions.iloc[train_index,:], \\\n        X_trainWithPredictions.iloc[cv_index,:]\n    y_train_fold, y_cv_fold = y_train.iloc[train_index], y_train.iloc[cv_index]\n    \n    lgb_train = lgb.Dataset(X_train_fold, y_train_fold)\n    lgb_eval = lgb.Dataset(X_cv_fold, y_cv_fold, reference=lgb_train)\n    gbm = lgb.train(params_lightGB, lgb_train, num_boost_round=2000,\n                   valid_sets=lgb_eval, early_stopping_rounds=200)\n    \n    loglossTraining = log_loss(y_train_fold, \\\n        gbm.predict(X_train_fold, num_iteration=gbm.best_iteration))\n    trainingScores.append(loglossTraining)\n    \n    predictionsBasedOnKFoldsEnsemble.loc[X_cv_fold.index,'prediction'] = \\\n        gbm.predict(X_cv_fold, num_iteration=gbm.best_iteration) \n    loglossCV = log_loss(y_cv_fold, \\\n        predictionsBasedOnKFoldsEnsemble.loc[X_cv_fold.index,'prediction'])\n    cvScores.append(loglossCV)\n    \n    print('Training Log Loss: ', loglossTraining)\n    print('CV Log Loss: ', loglossCV)\n    \nloglossEnsemble = log_loss(y_train, \\\n        predictionsBasedOnKFoldsEnsemble.loc[:,'prediction'])\nprint('Ensemble Log Loss: ', loglossEnsemble)","e94749d2":"print('Feature importances:', list(gbm.feature_importance()))","50bef577":"preds = pd.concat([y_train,predictionsBasedOnKFoldsEnsemble.loc[:,'prediction']], axis=1)\npreds.columns = ['trueLabel','prediction']\n\nprecision, recall, thresholds = \\\n    precision_recall_curve(preds['trueLabel'],preds['prediction'])\naverage_precision = \\\n    average_precision_score(preds['trueLabel'],preds['prediction'])\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = roc_curve(preds['trueLabel'],preds['prediction'])\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\nArea under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","305c3a74":"scatterData = predictionsTestSetLightGBMGradientBoosting.join(y_test,how='left')\nscatterData.columns = ['Predicted Probability','True Label']\nax = sns.regplot(x=\"True Label\", y=\"Predicted Probability\", color='k', \n                 fit_reg=False, scatter_kws={'alpha':0.1},\n                 data=scatterData).set_title( \\\n                'Plot of Prediction Probabilities and the True Label')","5d46af48":"scatterDataMelted = pd.melt(scatterData, \"True Label\", \\\n                            var_name=\"Predicted Probability\")\nax = sns.stripplot(x=\"value\", y=\"Predicted Probability\", \\\n                   hue='True Label', jitter=0.4, \\\n                   data=scatterDataMelted).set_title( \\\n                   'Plot of Prediction Probabilities and the True Label')","446655f9":"# Pipeline for New Data\n# first, import new data into a dataframe called 'newData'\n# second, scale data\n# newData.loc[:,featuresToScale] = sX.transform(newData[featuresToScale])\n# third, predict using LightGBM\n# gbm.predict(newData, num_iteration=gbm.best_iteration)","0094eccf":"# **Test Set Evaluation**","240e9a92":"# **END**"}}