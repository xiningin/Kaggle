{"cell_type":{"1955a5f5":"code","9e9ae77c":"code","3f174e54":"code","595bc01b":"code","679dc7b2":"code","eee80072":"code","47d25ed4":"code","68b21179":"code","b9a0a087":"markdown","b6795071":"markdown","3ed65b87":"markdown","4ab16e00":"markdown","d59ce4f2":"markdown"},"source":{"1955a5f5":"from collections import defaultdict\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\nfrom sklearn.metrics import brier_score_loss, log_loss, average_precision_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\n\n%matplotlib inline","9e9ae77c":"# load dataset\ndf = pd.read_csv('..\/input\/creditcard.csv')\nX = df.copy().drop('Class', axis=1)\ny = df.copy()['Class']","3f174e54":"models = {\n    'logreg': {'model': LogisticRegression(), 'params': {'solver': ['saga'], 'penalty': ['l1', 'l2'], 'C': [0.1, 1, 10]}},\n    'rf': {'model': RandomForestClassifier(), 'params': {'n_estimators': [30], 'min_samples_split': [2, 4, 8, 12]}}\n}\n\ndef get_model(model_dict, X, y, sample_type):\n    if sample_type not in model_dict:\n        if model_dict['params'] is not None:\n            grid_model = GridSearchCV(model_dict['model'], model_dict['params'], cv=3)\n            grid_model.fit(X, y)\n            model_dict[sample_type] = grid_model.best_estimator_\n            print('%s: %s' % (sample_type, grid_model.best_params_))\n        else:\n            # if a param grid isn't provide, use the model provided as is\n            model_dict[sample_type] = model_dict['model']\n    return model_dict[sample_type]","595bc01b":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\nsm = SMOTE(random_state=42, sampling_strategy='minority') # used for downsampling\nnm = NearMiss(random_state=42) # used for upsampling\n\n# put all this in a method so we don't have to type it out three times\ndef train_test_score(model, model_name, sample_type, X_train, y_train, X_test, y_test, scores):\n    model.fit(X_train, y_train)\n    pred = model.predict_proba(X_test)\n    pred_df = pd.DataFrame({'pred': [p[1] for p in pred], 'actual': y_test}, index=X_test.index).sort_values('pred', ascending=False)\n    scores['logloss'][model_name][sample_type]['scores'].append(brier_score_loss(pred_df['actual'], pred_df['pred']))\n    scores['AP'][model_name][sample_type]['scores'].append(average_precision_score(pred_df['actual'], pred_df['pred']))\n    return scores\n\nscores = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(list))))\n\nfor train, test in kf.split(X):\n    X_train, X_test, y_train, y_test = X.iloc[train, :], X.iloc[test, :], y.iloc[train], y.iloc[test]\n    for model_name, model_dict in models.items():\n        model = get_model(model_dict, X_train, y_train, 'raw')\n        scores = train_test_score(model, model_name, 'raw', X_train, y_train, X_test, y_test, scores)\n        \n        X_up, y_up = sm.fit_resample(X_train, y_train)\n        model = get_model(model_dict, X_up, y_up, 'up')\n        scores = train_test_score(model, model_name, 'up', X_up, y_up, X_test, y_test, scores)\n        \n        X_down, y_down = nm.fit_resample(X_train, y_train)\n        model = get_model(model_dict, X_down, y_down, 'down')\n        scores = train_test_score(model, model_name, 'down', X_down, y_down, X_test, y_test, scores)","679dc7b2":"# put the results into a dataframe\nd = {'metric': [], 'model': [], 'sample': [], 'score': [], 'std': []}\nfor metric, scores1 in scores.items():\n    for m, scores2 in scores1.items():\n        for stype, scores3 in scores2.items():\n            if len(scores3['scores']) > 0:\n                d['metric'].append(metric)\n                d['model'].append(m)\n                d['sample'].append(stype)\n                d['score'].append(np.mean(scores3['scores']))\n                d['std'].append(np.std(scores3['scores']))\nresults_df = pd.DataFrame(d)","eee80072":"# plot average precision results\nplot_ap_df = results_df.loc[results_df['metric']=='AP', ['model', 'sample', 'score', 'std']]\nerror_df = plot_ap_df.pivot(index='sample', columns='model', values='std')\nplot_ap_df.pivot(index='sample', columns='model', values='score')\\\n    .plot(kind='bar', title='AP', yerr=error_df)","47d25ed4":"# plot logloss results\nplot_logloss_df = results_df.loc[results_df['metric']=='logloss', ['model', 'sample', 'score', 'std']]\nerror_logloss_df = plot_logloss_df.pivot(index='sample', columns='model', values='std')\nplot_logloss_df.pivot(index='sample', columns='model', values='score')\\\n    .plot(kind='bar', title='logloss', yerr=error_logloss_df)","68b21179":"# let's try plotting again without the downsampled results\nplot_logloss_no_down_df = results_df.loc[\n    (results_df['metric']=='logloss') & (results_df['sample']!='down'),\n    ['model', 'sample', 'score', 'std']\n]\nerror_logloss_df = plot_logloss_no_down_df.pivot(index='sample', columns='model', values='std')\nplot_logloss_no_down_df.pivot(index='sample', columns='model', values='score')\\\n    .plot(kind='bar', title='logloss', yerr=error_logloss_df)","b9a0a087":"For assigning class probabilities to transactions, training on the raw, unsampled, data beats both upsampling and downsampling for both classifiers. **Training on the unsampled data performs orders of magnitude better than on the downsampled data!**\n\n## Conclusions\nBalancing the training classes by downsampling decreases the model's perfromance for both  average precision and logloss. Balancing classes by upsampling provides no benefit to the average precision score and decreases the pefromance measured by logloss.\n\nIt's also worth pointing out that random forest performed better than logistic regression in all cases. I didn't bother normalizing data here and only made a small effort to tune the model parameters.","b6795071":"# Is balancing classes in an imbalanced dataset really a good idea?\nI'd say that the majority of advice I've read online about dealing with imbalanced datasets recommends balancing classes by either upsampling or downsampling before training a classification model. But both of these approaches contradict my intuition of how machine learning should work. In the case of upsampling you're fabricating data. Even worse in the case of downsampling, you're throwing information away! Despite an abundance of tutorials on _how_ to balance classes, I haven't found any that attempt to convince me that these approaches are a good idea _using actual data_.\n\nI'm not the only one confused by this. Many of the responses to [this stackexchange question asking \"when is unbalanced data really a problem\"](https:\/\/stats.stackexchange.com\/questions\/283170\/when-is-unbalanced-data-really-a-problem-in-machine-learning), say \"it's not!\". I also recently found a great blog post using data to address the question [\"does balancing classes improve classifier performance\"](http:\/\/www.win-vector.com\/blog\/2015\/02\/does-balancing-classes-improve-classifier-performance\/). There the author is looking at a very narrow speach to text classification problem. She concludes that balancing classes does not improve classifier performance, but she adds a few caveats. First, her conclusion applies to the specific problem she was solving (labeling spoken characters from the alphabet) and may not necesarily generalize to other data sets. Second, she looked at accuracy, precision, and recall for predicted labels after applying a somewhat arbitrary threshold. How might results differ if using different evaluation metrics that don't rely on specifying a decision threshold?\n\nIn this notebook I'll show that identifying fraud in credit card transaction data is another case where **balancing classes provides no benefit to classifier performance** (and that down sampling actually performance much worse).\n\n## Performance Metrics\nBefore we begin, let's talk about performance metrics. As many people have pointed out, accuracy is poor metric of performance for imbalanced datasets. To understand why this is the case, consider that many classification evaluation metrics consist of some combination of the four quadrants of the confusion matrix. That is counts of: True Negatives (TN), True Positives (TP), False Negatives (FN), and False Positives (FP). Accuracy is defined as\n```\nAccuracy = (TN + TP) \/ (TN + TP + FN + FP).\n```\nThe issue with imbalanced classifiers is that in most cases the TN value will be much larger than all of the other values and any metric that includes TN will be dominated by that term. I've seen people advise against using accuracy and recommend instead to use the area under the ROC curve. But the ROC curve is also a poor metric for imbalanced datasets. The x-axis of the ROC curve is the False Positive Rate (FPR) defined as:\n```\nFPR = FP \/ (FP + TN).\n```\nThere, again, is the TN.\n\nPrecision and Recall on the otherhand do not contain a TN term. There's typically a tradeoff between these metrics (better precision leads to worse recall and vice versa), and both metrics are usually reported when evaluating a classifier. \n\nMost classification models don't output classification labels directly. Instead, they give probabilities for each class label. The values in a confusion matrix are obtained after setting a decision threshold on these probabilities. For instance, a common decision threshold for binary classifiers is 0.5, i.e. if the classifier gives greater than a 50% chance of an instance being in the positive class, label that instance as positive.\n\nBut, not all binary classification problems require a binary classification label as their output. Fraud detection is a good example of this. Imagine that you had a fraud investigation team that could handle a maximum of 100 cases a day. In this case, the ideal output would be a list of suspicous transactions ordered by how likely they are to be fradulent. And the ideal performance metric would indicate how well this sorted list compared to actual cases of fraud (i.e. do instances of fraud appear higher in the list?).\n\nImagine further that you wanted to be able to suspend any accounts with transactions having greater than a 99% likelihood of being fraudulent. In that case it'd be important that your model give accurate probabilities and you'd want to performance metric that reflected that.\n\nIn the case of evaluating the performance of a sorted list, we can use the Average Precision (AP) metric. There are actually a few different metrics suitable for this task, but AP has the additional nice property of being equivalent to the area under the Precision-Recall curve. There are also a few metrics for evaluating the accuracy of a classifiers probabilities, including log loss and the brier score.\n\n**When comparing the performance of sampling techniques on classification of imbalanced datasets, we'll look at both average precision and log loss** (Note that higher average precision scores are better and lower log loss scores are better).","3ed65b87":"We'll try two different models, logistic regression and random forest. For each sampling technique we'll do a grid search to find the optimal set of parameters (specified by 'params' in the models dictionary below). We'll call the `get_model` method to perform the grid search for each model\/sample_type. The best model will be cached in the models dictionary for subsequent uses.","4ab16e00":"For ordering transaction by likelihood to be fraudulent, there's a clear disadvantage to training on downsampled data. This is especially true for logistic regression. Both logistic regression and random forest perform about the same when trained on unsampled vs upsampled data. There is no advantage to upsampling here, but the disadvantage is that training takes longer (because there's more data).","d59ce4f2":"Perform 5-fold cross validation. For each fold, try each of the three sampling methods on the training data: 'raw' (no sampling), 'up' (upsampling), and 'down' (downsampling). Train, predict, and score using average precision and log loss."}}