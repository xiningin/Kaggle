{"cell_type":{"66e40216":"code","c274c510":"code","fbb85e37":"code","2261c34c":"code","e72b11c8":"code","72df7660":"code","3da7ee20":"code","830964fe":"markdown","ef0f75fa":"markdown","4c840cd4":"markdown","21692d02":"markdown","a7d602f9":"markdown","0b9d6d02":"markdown"},"source":{"66e40216":"!pip install adjustText","c274c510":"import pandas as pd\nimport numpy as np\nfrom gensim.models.word2vec import Word2Vec\nfrom sklearn.manifold import TSNE\nfrom adjustText import adjust_text\n\n%matplotlib inline\nimport matplotlib.pyplot as plt","fbb85e37":"np.random.seed(42)","2261c34c":"data = pd.read_csv('..\/input\/groceries\/groceries - groceries.csv', header=0)\ndata.head(5)","e72b11c8":"sentences = []\nfor i, row in data.iterrows():\n    vals = row.values[1:].astype(str)\n    \n    # Remove the nans\n    vals = vals[vals != 'nan']\n    \n    # Order does not really matter in shopping baskets (unlike English sentences)\n    # so this is a form of augmentation\n    for _ in range(min(3, len(vals))):\n        np.random.shuffle(vals)\n        sentences.append(list(vals))\n        \nprint('\\n'.join([', '.join(x) for x in sentences[:10]]))","72df7660":"print('Embedding {} sentences...'.format(len(sentences)))\n    \nmodel = Word2Vec(\n    sentences,\n    size=10,\n    window=3,\n    workers=1,\n    sg=0,\n    iter=25,\n    negative=25,\n    min_count=1,\n    seed=42,\n    compute_loss=True\n)\n\nprint(model.get_latest_training_loss())","3da7ee20":"products = list(model.wv.vocab.keys())\nembeddings = []\nfor product in products:\n    embeddings.append(model.wv[product])\nembeddings = np.array(embeddings)\nprint(len(products), embeddings.shape)\n\ntsne = TSNE(random_state=42)\nX_tsne = tsne.fit_transform(embeddings)\n\nplt.figure(figsize=(10, 10))\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1])\n\ntexts = []\nfor x, y, lab in zip(X_tsne[:, 0], X_tsne[:, 1], products):\n    text = plt.text(x, y, lab)\n    texts.append(text)\n    \nadjust_text(texts, lim=5, arrowprops=dict(arrowstyle=\"->\", color='r', lw=0.5))\nplt.show()","830964fe":"This is only a simple proof-of-concept with a rather small dataset. Nevertheless, we can see that the Word2Vec already learned some cool things! Due to stochastic elements in Word2Vec, the embeddings can look different each time. On the command-line, you can prevent this by adding `PYTHONHASHSEED=42` as a prefix.\n\nIn the image below, we can cleary identify a cluster of alcohol, we can see how mustard and frankfurter have been placed together, and so on... Cool beans!\n\n![image.png](attachment:image.png)","ef0f75fa":"# Creating numerical representations for each of the products with Word2Vec\n\nIn this (rather short) notebook, I will demonstrate how simple it is to learn representations for each product in the shopping baskets with Word2Vec.","4c840cd4":"# We now feed these sentences to Word2Vec\n\nFeel free to play around with these hyper-parameters and see the impact on the embeddings!","21692d02":"# Create 2D t-SNE representation of embeddings\n\nLet's take a look at our embeddings in a 2D scatter plot! We will use `adjustText` to not clutter the plot with our labels.","a7d602f9":"# Load our data (shopping baskets)","0b9d6d02":"# Create \"sentences\"\n\nWord2Vec expects sentences as its input. For this dataset, each of the shopping baskets can be seen as a sentence and each of the products in the basket can be seen as a word. Word2Vec will then learn representations for each of these products by trying to either predict the other products in the basket based on 1 product (Skip-Gram) or a product based on the other products in the basket (Continuous Bag of Words)"}}