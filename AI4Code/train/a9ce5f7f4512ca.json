{"cell_type":{"39a94b97":"code","d9479262":"code","f5489079":"code","8abf7041":"code","a6672dd2":"code","653713f8":"code","11feb1ff":"code","d3f40b8a":"code","29cd268b":"code","fcbb50cc":"code","2591c590":"code","dd3ad744":"code","045d2ec7":"code","685b8562":"code","2df0f67f":"code","25f8917e":"code","ab7e9d70":"code","c9ef02cd":"code","0dce5f66":"code","7bef9074":"code","15be11a4":"code","3938814e":"code","2d4ada61":"code","626665c9":"code","c1b5193d":"code","3173fe19":"markdown","2430d2da":"markdown","351a0119":"markdown","64fba524":"markdown","6b552dc6":"markdown","3a75e444":"markdown","7a48c1e6":"markdown","86915bf6":"markdown","24eccfb2":"markdown","4579029d":"markdown","12873350":"markdown","86d36e60":"markdown","771c0204":"markdown","76cdbe8c":"markdown","574de191":"markdown","2a8862f7":"markdown","b360fad6":"markdown","fe5f2258":"markdown","2e0f9462":"markdown","44082706":"markdown"},"source":{"39a94b97":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d9479262":"#Load the set\ntrain_df=pd.read_csv('..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\ntrain_df.head()","f5489079":"import numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing,metrics,manifold\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV,cross_val_predict\nfrom imblearn.over_sampling import ADASYN,SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nimport collections\nimport matplotlib.patches as mpatches\nfrom sklearn.metrics import accuracy_score\n%matplotlib inline\nfrom sklearn.preprocessing import RobustScaler\nimport xgboost\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import classification_report,roc_auc_score,roc_curve,r2_score,recall_score,confusion_matrix,precision_recall_curve\nfrom collections import Counter\nfrom sklearn.model_selection import StratifiedKFold,KFold,StratifiedShuffleSplit\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD,SparsePCA\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom nltk.tokenize import word_tokenize\nfrom collections import defaultdict\nfrom collections import Counter\nimport seaborn as sns\nfrom wordcloud import WordCloud,STOPWORDS\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nimport xgboost\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import classification_report,roc_auc_score,roc_curve,r2_score,recall_score,confusion_matrix,precision_recall_curve\nfrom collections import Counter\nfrom sklearn.model_selection import StratifiedKFold,KFold,StratifiedShuffleSplit\nfrom xgboost import XGBClassifier as xg\nfrom lightgbm import LGBMClassifier as lg\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingClassifier,RandomForestClassifier,AdaBoostClassifier,BaggingClassifier,ExtraTreesClassifier","8abf7041":"%%time\n#Convert the labels into integers (numerics) for reference.\n\ntrain_li=[]\nfor i in range(len(train_df)):\n    if (train_df['sentiment'][i]=='positive'):\n        train_li.append(1)\n    else:\n        train_li.append(0)\ntrain_df['Binary']=train_li\ntrain_df.head()","a6672dd2":"%%time\n#Running the Preprocessing and cleaning phase as well as the TFIDF Vectorization\n\nimport re\n#Removes Punctuations\ndef remove_punctuations(data):\n    punct_tag=re.compile(r'[^\\w\\s]')\n    data=punct_tag.sub(r'',data)\n    return data\n\n#Removes HTML syntaxes\ndef remove_html(data):\n    html_tag=re.compile(r'<.*?>')\n    data=html_tag.sub(r'',data)\n    return data\n\n#Removes URL data\ndef remove_url(data):\n    url_clean= re.compile(r\"https:\/\/\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\n\n#Removes Emojis\ndef remove_emoji(data):\n    emoji_clean= re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    data=emoji_clean.sub(r'',data)\n    url_clean= re.compile(r\"https:\/\/\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\n#Lemmatize the corpus\ndef lemma_traincorpus(data):\n    lemmatizer=WordNetLemmatizer()\n    out_data=\"\"\n    for words in data:\n        out_data+= lemmatizer.lemmatize(words)\n    return out_data\n\ndef tfidf(data):\n    tfidfv = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), lowercase=True, max_features=150000)\n    fit_data_tfidf=tfidfv.fit_transform(data)\n    return fit_data_tfidf\n\n\ntrain_df['review']=train_df['review'].apply(lambda z: remove_punctuations(z))\ntrain_df['review']=train_df['review'].apply(lambda z: remove_html(z))\ntrain_df['review']=train_df['review'].apply(lambda z: remove_url(z))\ntrain_df['review']=train_df['review'].apply(lambda z: remove_emoji(z))\ncount_good=train_df[train_df['sentiment']=='positive']\ncount_bad=train_df[train_df['sentiment']=='negative']\ntrain_df['review']=train_df['review'].apply(lambda z: lemma_traincorpus(z))\n","653713f8":"%%time\n#TFIDF Vectorize the Data\n\ntrain_set=tfidf(train_df['review'])","11feb1ff":"%%time\n#Normal Train Test Split\n\ntrain_y=train_df['sentiment']\ntrain_x,test_x,train_y,test_y=train_test_split(train_set,train_y,test_size=0.2,random_state=42)\ntrain_x.shape,train_y.shape,test_x.shape,test_y.shape","d3f40b8a":"%%time\n#Stratified Train Test Split\n\ntrain_stratify_y=train_df['sentiment']\ntrain_stratified_x,test_stratified_x,train_stratified_y,test_stratified_y=train_test_split(train_set,train_stratify_y,test_size=0.2,random_state=42,stratify=train_stratify_y)\ntrain_stratified_x.shape,train_stratified_y.shape,test_stratified_x.shape,test_stratified_y.shape","29cd268b":"#Applying Logistic Regression on split tfidf baseline\nmodel=LogisticRegression()\nmodel.fit(train_x,train_y)\npred=model.predict(test_x)\nprint(\"Evaluate confusion matrix for LR\")\nprint(confusion_matrix(test_y,pred))\nprint(f\"Accuracy Score for LR with C=1.0  ={accuracy_score(test_y,pred)}\")","fcbb50cc":"#Applying MultiNomial Naive Bayes on split tfidf baseline\nmodel=MultinomialNB()\nmodel.fit(train_x,train_y)\npred=model.predict(test_x)\nprint(\"Evaluate confusion matrix for NB\")\nprint(confusion_matrix(test_y,pred))\nprint(f\"Accuracy Score for NB ={accuracy_score(test_y,pred)}\")","2591c590":"%%time\n#KFold and cross validation on tfidf baseline\nmodels=[]\nmodels.append(('LogisticRregression',LogisticRegression(C=1.0,penalty='l2')))\nmodels.append(('KNearestNeighbors',KNeighborsClassifier()))\nmodels.append(('DecisionTree',DecisionTreeClassifier(criterion='entropy')))\n#models.append(('RandomForestRegressor',RandomForestRegressor(n_estimators = 1000, random_state = 42)))\n# models.append(('RandomForestClassifier',RandomForestClassifier(n_estimators = 1000, criterion='gini')))\nmodels.append(('GradientBoostClassifier',GradientBoostingClassifier(learning_rate=1e-2, loss='deviance',n_estimators=100)))\nmodels.append(('AdaBoostClassifier',AdaBoostClassifier(learning_rate=1e-2,algorithm='SAMME.R',n_estimators=100)))\n#models.append(('ExtraTreesClassifier',ExtraTreesClassifier(n_estimators=10, max_depth=None,min_samples_split=2)))\nmodels.append(('BagClassifier',BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.5)))\n# models.append(('HistGradientBoostClassifier',HistGradientBoostingClassifier(max_iter=100)))\n# models.append(('SupportVectorClassifier',SVC(C=1.0,kernel='sigmoid')))\nmodel_result=[]\nscoring='accuracy'\nprint(\"Statistical Model TFIDF- Baseline Evaluation\")\nfor name,model in models:\n    kfold=KFold(n_splits=10,random_state=7)\n    results=cross_val_score(model,train_x,train_y,cv=kfold)\n    print(\"=======================\")\n    print(\"Classifiers: \",name, \"Has a training score of\", round(results.mean(), 2) * 100, \"% accuracy score\")\n    model_result.append(results.mean())","dd3ad744":"#Balancing The Sampple for TFIDF Baseline\n#SMOTE oversampling\nsmote=SMOTE(random_state=42,k_neighbors=2)\nsmote_train_x,smote_train_y=smote.fit_sample(train_x,train_y)\nsmote_train_x.shape,smote_train_y.shape","045d2ec7":"%%time\n#Applying SMOTE TFIDF Balanced Baseline with KFold\n\nmodels=[]\nmodels.append(('LogisticRregression',LogisticRegression(C=1.0,penalty='l2')))\nmodels.append(('KNearestNeighbors',KNeighborsClassifier()))\nmodels.append(('DecisionTree',DecisionTreeClassifier(criterion='entropy')))\n#models.append(('RandomForestRegressor',RandomForestRegressor(n_estimators = 1000, random_state = 42)))\n#models.append(('RandomForestClassifier',RandomForestClassifier(n_estimators = 1000, criterion='gini')))\nmodels.append(('GradientBoostClassifier',GradientBoostingClassifier(learning_rate=1e-2, loss='deviance',n_estimators=100)))\nmodels.append(('AdaBoostClassifier',AdaBoostClassifier(learning_rate=1e-2,algorithm='SAMME.R',n_estimators=100)))\n#models.append(('ExtraTreesClassifier',ExtraTreesClassifier(n_estimators=10, max_depth=None,min_samples_split=2)))\nmodels.append(('BagClassifier',BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.5)))\n# models.append(('HistGradientBoostClassifier',HistGradientBoostingClassifier(max_iter=100)))\n# models.append(('SupportVectorClassifier',SVC(C=1.0,kernel='sigmoid')))\nmodel_training_result,model_validation_result=[],[]\nscoring='accuracy'\nprint(\"Statistical Model SMOTE TFIDF- Baseline Evaluation\")\nfor name,model in models:\n    kfold=KFold(n_splits=10,random_state=7)\n    results=cross_val_score(model,smote_train_x,smote_train_y,cv=kfold)\n    print(\"=======================\")\n    print(\"Classifiers: \",name, \"Has a training score of\", round(results.mean(), 2) * 100, \"% accuracy score\")\n    predictions=cross_val_predict(model,test_x,test_y)\n    accuracy = accuracy_score(predictions,test_y)\n    model_training_result.append(results.mean())\n    model_validation_result.append(accuracy)\n\nfinal_outcomes=pd.DataFrame(columns=['Model','Training Acc','Validation Acc'])\nfinal_outcomes['Model']=models\nfinal_outcomes['Training Acc']=model_training_result\nfinal_outcomes['Validation Acc']=model_validation_result\nfinal_outcomes.to_csv('TFIDF-SMOTE-Baseline.csv',index=False)\nfinal_outcomes","685b8562":"%%time\n## Load word2vec algorithm from gensim and vectorize the words\nfrom gensim.models import Word2Vec,KeyedVectors\ncheck_df=list(train_df['review'].str.split())\nmodel=Word2Vec(check_df,min_count=1,iter=20)","2df0f67f":"#Label Encode the labels\nfrom sklearn.preprocessing import LabelEncoder\nlabel_y= LabelEncoder()\nlabels=label_y.fit_transform(train_df['sentiment'])\nlabels","25f8917e":"#Convert word vectors to sentence vectors\/sentence vectors and apply mean pooling\n\ndef convert_sentence(data):\n    vocab=[w for w in data if w in model.wv.vocab]\n    avg_pool=np.mean(model[vocab],axis=0)\n#     sum_pool=np.sum(model[vocab],axis=0)\n#     min_pool=np.min(model[vocab],axis=0)\n#     max_pool=np.max(model[vocab],axis=0)\n    return avg_pool\n\ntrain_df['Vectorized_Reviews']=train_df['review'].apply(convert_sentence)\n\n#Split the dataset into training and testing sets\ntrain_y=train_df['sentiment']\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['Vectorized_Reviews'],train_y,test_size=0.2,random_state=42)\ntrain_x.shape,train_y.shape,test_x.shape,test_y.shape\n    ","ab7e9d70":"test_x=list(test_x)\ntrain_x=list(train_x)","c9ef02cd":"%%time\n#Applying W2V Balanced Baseline with KFold\n\nmodels=[]\nmodels.append(('LogisticRregression',LogisticRegression(C=1.0,penalty='l2')))\nmodels.append(('KNearestNeighbors',KNeighborsClassifier()))\nmodels.append(('DecisionTree',DecisionTreeClassifier(criterion='entropy')))\n#models.append(('RandomForestRegressor',RandomForestRegressor(n_estimators = 1000, random_state = 42)))\n#models.append(('RandomForestClassifier',RandomForestClassifier(n_estimators = 1000, criterion='gini')))\nmodels.append(('GradientBoostClassifier',GradientBoostingClassifier(learning_rate=1e-2, loss='deviance',n_estimators=100)))\nmodels.append(('AdaBoostClassifier',AdaBoostClassifier(learning_rate=1e-2,algorithm='SAMME.R',n_estimators=100)))\nmodels.append(('ExtraTreesClassifier',ExtraTreesClassifier(n_estimators=10, max_depth=None,min_samples_split=2)))\nmodels.append(('BagClassifier',BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.5)))\n# models.append(('HistGradientBoostClassifier',HistGradientBoostingClassifier(max_iter=100)))\n#models.append(('SupportVectorClassifier',SVC(C=1.0,kernel='sigmoid')))\nw2v_model_training_result,w2v_model_validation_result=[],[]\nscoring='accuracy'\nprint(\"Statistical Model Word2Vec- Baseline Evaluation\")\nfor name,model in models:\n    kfold=KFold(n_splits=10,random_state=7)\n    results=cross_val_score(model,train_x,train_y,cv=kfold)\n    print(\"=======================\")\n    print(\"Classifiers: \",name, \"Has a training score of\", round(results.mean(), 2) * 100, \"% accuracy score\")\n    w2v_model_training_result.append(results.mean())\n    predictions=cross_val_predict(model,test_x,test_y)\n    accuracy = accuracy_score(predictions,test_y)\n    w2v_model_validation_result.append(accuracy)\n\nfinal_w2v_outcomes=pd.DataFrame(columns=['Model','Training Acc','Validation Acc'])\nfinal_w2v_outcomes['Model']=models\nfinal_w2v_outcomes['Training Acc']=w2v_model_training_result\nfinal_w2v_outcomes['Validation Acc']=w2v_model_validation_result\nfinal_w2v_outcomes.to_csv('W2V-Baseline.csv',index=False)\nfinal_w2v_outcomes","0dce5f66":"#Evaluating XGBoost & Light GBM on the dataset\nfrom xgboost import XGBClassifier as xg\nfrom lightgbm import LGBMClassifier as lg\nmodel_xgb= xg(n_estimators=100,random_state=42)\nmodel_xgb.fit(train_x,train_y)\ny_pred_xgb=model_xgb.predict(test_x)\nprint(accuracy_score(test_y,y_pred_lgbm.round()))\n# print(\"Confusion matrix\")\nmodel_lgbm= lg(n_estimators=100,random_state=42)\nmodel_lgbm.fit(train_x,train_y)\ny_pred_lgbm=model_lgbm.predict(test_x)\n# print(\"Confusion matrix\")\n# print(confusion_matrix(test_y,y_pred_lgbm))\nprint(accuracy_score(test_y,y_pred_lgbm.round()))","7bef9074":"from gensim.models import Word2Vec,KeyedVectors\nfrom gensim.scripts.glove2word2vec import glove2word2vec\n\n##Google News Vectors to word2vec format for mean pooling\ngoogle_news_embed='..\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin'\ngoogle_loaded_model=KeyedVectors.load_word2vec_format(google_news_embed,binary=True)\nprint(google_loaded_model)\n##Glove Vectors to word2vec format for mean pooling\nglove_file='..\/input\/glove-global-vectors-for-word-representation\/glove.6B.50d.txt'\nword2vec_output_file = 'glove.6B.100d.txt.word2vec'\nglove_loaded=glove2word2vec(glove_file, word2vec_output_file)\nglove_loaded = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\nprint(glove_loaded)\n##Fasttext to word2vec format for mean pooling\nfasttext_file=\"..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec\"\nfasttext_model = KeyedVectors.load_word2vec_format(fasttext_file, binary=False)\nprint(fasttext_model)","15be11a4":"def convert_sentence_embeddings(data,model):\n    vocab=[w for w in data if w in model.wv.vocab]\n    avg_pool=np.mean(model[vocab],axis=0)\n#     sum_pool=np.sum(model[vocab],axis=0)\n#     min_pool=np.min(model[vocab],axis=0)\n#     max_pool=np.max(model[vocab],axis=0)\n    return avg_pool\n\n#Google vectors\nprint('Google Vectors')\ntrain_google_df=train_df\ntrain_google_df['Google_News_Vectorized_Reviews']=train_df['review'].apply(lambda z: convert_sentence_embeddings(z,google_loaded_model) )\n#Split the dataset into training and testing sets\ntrain_google_y=train_df['sentiment']\ntrain_google_x,test_google_x,train_google_y,test_google_y=train_test_split(train_google_df['Google_News_Vectorized_Reviews'],train_google_y,test_size=0.2,random_state=42)\ntrain_google_x=list(train_google_x)\ntest_google_x=list(test_google_x)\n# train_google_x.shape,train_google_y.shape,test_google_x.shape,test_google_y.shape\n\n\n# #Glove Vectors\n# print('Glove Vectors')\n# train_glove_df=train_df\n# train_glove_df['Glove_Vectorized_Reviews']=train_df['review'].apply(lambda z: convert_sentence_embeddings(z,glove_loaded) )\n# #Split the dataset into training and testing sets\n# train_glove_y=train_df['sentiment']\n# train_glove_x,test_glove_x,train_glove_y,test_glove_y=train_test_split(train_glove_df['Glove_Vectorized_Reviews'],train_glove_y,test_size=0.2,random_state=42)\n# train_glove_x=list(train_glove_x)\n# test_glove_x=list(test_glove_x)\n# # train_glove_x.shape,train_glove_y.shape,test_glove_x.shape,test_glove_y.shape\n\n# #FastText Vectors\n# print('Fasttext Vectors')\n# train_fasttext_df=train_df\n# train_fasttext_df['Fasttext_Vectorized_Reviews']=train_df['review'].apply(lambda z: convert_sentence_embeddings(z,fasttext_model) )\n# #Split the dataset into training and testing sets\n# train_fasttext_y=train_df['sentiment']\n# train_fasttext_x,test_fasttext_x,train_fasttext_y,test_fasttext_y=train_test_split(train_fasttext_df['Fasttext_Vectorized_Reviews'],train_fasttext_y,test_size=0.2,random_state=42)\n# train_fasttext_x=list(train_fasttext_x)\n# test_fasttext_x=list(test_fasttext_x)\n# # train_fasttext_x.shape,train_fasttext_y.shape,test_fasttext_x.shape,test_fasttext_y.shape\n\n","3938814e":"#FastText Vectors\nprint('Fasttext Vectors')\ntrain_fasttext_df=train_df\ntrain_fasttext_df['Fasttext_Vectorized_Reviews']=train_df['review'].apply(lambda z: convert_sentence_embeddings(z,fasttext_model) )\n#Split the dataset into training and testing sets\ntrain_fasttext_y=train_df['sentiment']\ntrain_fasttext_x,test_fasttext_x,train_fasttext_y,test_fasttext_y=train_test_split(train_fasttext_df['Fasttext_Vectorized_Reviews'],train_fasttext_y,test_size=0.2,random_state=42)\ntrain_fasttext_x=list(train_fasttext_x)\ntest_fasttext_x=list(test_fasttext_x)\n# train_fasttext_x.shape,train_fasttext_y.shape,test_fasttext_x.shape,test_fasttext_y.shape\n","2d4ada61":"%%time\n#Applying Google Vectors Balanced Baseline with KFold\n\nmodels=[]\nmodels.append(('LogisticRregression',LogisticRegression(C=1.0,penalty='l2')))\nmodels.append(('KNearestNeighbors',KNeighborsClassifier()))\nmodels.append(('DecisionTree',DecisionTreeClassifier(criterion='entropy')))\n#models.append(('RandomForestRegressor',RandomForestRegressor(n_estimators = 1000, random_state = 42)))\n#models.append(('RandomForestClassifier',RandomForestClassifier(n_estimators = 1000, criterion='gini')))\nmodels.append(('GradientBoostClassifier',GradientBoostingClassifier(learning_rate=1e-2, loss='deviance',n_estimators=100)))\nmodels.append(('AdaBoostClassifier',AdaBoostClassifier(learning_rate=1e-2,algorithm='SAMME.R',n_estimators=100)))\nmodels.append(('ExtraTreesClassifier',ExtraTreesClassifier(n_estimators=10, max_depth=None,min_samples_split=2)))\nmodels.append(('BagClassifier',BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.5)))\n# models.append(('HistGradientBoostClassifier',HistGradientBoostingClassifier(max_iter=100)))\n#models.append(('SupportVectorClassifier',SVC(C=1.0,kernel='sigmoid')))\nmodels.append(('XGBoosting',xg(n_estimators=100,random_state=42)))\nmodels.append(('LightGBM',lg(n_estimators=100,random_state=42)))\ngoogle_model_training_result,google_model_validation_result=[],[]\nscoring='accuracy'\nprint(\"Statistical Model Google Vectors- Baseline Evaluation\")\nfor name,model in models:\n    kfold=KFold(n_splits=10,random_state=7)\n    results=cross_val_score(model,train_google_x,train_google_y,cv=kfold)\n    print(\"=======================\")\n    print(\"Classifiers: \",name, \"Has a training score of\", round(results.mean(), 2) * 100, \"% accuracy score\")\n    w2v_model_training_result.append(results.mean())\n    predictions=cross_val_predict(model,test_google_x,test_google_y)\n    accuracy = accuracy_score(predictions,test_google_y)\n    w2v_model_validation_result.append(accuracy)\n\nfinal_google_outcomes=pd.DataFrame(columns=['Model','Training Acc','Validation Acc'])\nfinal_google_outcomes['Model']=models\nfinal_google_outcomes['Training Acc']=google_model_training_result\nfinal_google_outcomes['Validation Acc']=google_model_validation_result\nfinal_google_outcomes.to_csv('GoogleNewsVectors-Baseline.csv',index=False)\nfinal_google_outcomes","626665c9":"%%time\n#Applying Glove Vectors Balanced Baseline with KFold\n\nmodels=[]\nmodels.append(('LogisticRregression',LogisticRegression(C=1.0,penalty='l2')))\nmodels.append(('KNearestNeighbors',KNeighborsClassifier()))\nmodels.append(('DecisionTree',DecisionTreeClassifier(criterion='entropy')))\n#models.append(('RandomForestRegressor',RandomForestRegressor(n_estimators = 1000, random_state = 42)))\n#models.append(('RandomForestClassifier',RandomForestClassifier(n_estimators = 1000, criterion='gini')))\nmodels.append(('GradientBoostClassifier',GradientBoostingClassifier(learning_rate=1e-2, loss='deviance',n_estimators=100)))\nmodels.append(('AdaBoostClassifier',AdaBoostClassifier(learning_rate=1e-2,algorithm='SAMME.R',n_estimators=100)))\nmodels.append(('ExtraTreesClassifier',ExtraTreesClassifier(n_estimators=10, max_depth=None,min_samples_split=2)))\nmodels.append(('BagClassifier',BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.5)))\n# models.append(('HistGradientBoostClassifier',HistGradientBoostingClassifier(max_iter=100)))\n#models.append(('SupportVectorClassifier',SVC(C=1.0,kernel='sigmoid')))\nmodels.append(('XGBoosting',xg(n_estimators=100,random_state=42)))\nmodels.append(('LightGBM',lg(n_estimators=100,random_state=42)))\nglove_model_training_result,glove_model_validation_result=[],[]\nscoring='accuracy'\nprint(\"Statistical Model Google Vectors- Baseline Evaluation\")\nfor name,model in models:\n    kfold=KFold(n_splits=10,random_state=7)\n    results=cross_val_score(model,train_glove_x,train_glove_y,cv=kfold)\n    print(\"=======================\")\n    print(\"Classifiers: \",name, \"Has a training score of\", round(results.mean(), 2) * 100, \"% accuracy score\")\n    w2v_model_training_result.append(results.mean())\n    predictions=cross_val_predict(model,test_glove_x,test_glove_y)\n    accuracy = accuracy_score(predictions,test_glove_y)\n    w2v_model_validation_result.append(accuracy)\n\nfinal_glove_outcomes=pd.DataFrame(columns=['Model','Training Acc','Validation Acc'])\nfinal_glove_outcomes['Model']=models\nfinal_glove_outcomes['Training Acc']=glove_model_training_result\nfinal_glove_outcomes['Validation Acc']=glove_model_validation_result\nfinal_glove_outcomes.to_csv('GoogleNewsVectors-Baseline.csv',index=False)\nfinal_glove_outcomes","c1b5193d":"%%time\n#Applying Fasttext Vectors Balanced Baseline with KFold\n\nmodels=[]\nmodels.append(('LogisticRregression',LogisticRegression(C=1.0,penalty='l2')))\nmodels.append(('KNearestNeighbors',KNeighborsClassifier()))\nmodels.append(('DecisionTree',DecisionTreeClassifier(criterion='entropy')))\n#models.append(('RandomForestRegressor',RandomForestRegressor(n_estimators = 1000, random_state = 42)))\n#models.append(('RandomForestClassifier',RandomForestClassifier(n_estimators = 1000, criterion='gini')))\nmodels.append(('GradientBoostClassifier',GradientBoostingClassifier(learning_rate=1e-2, loss='deviance',n_estimators=100)))\nmodels.append(('AdaBoostClassifier',AdaBoostClassifier(learning_rate=1e-2,algorithm='SAMME.R',n_estimators=100)))\nmodels.append(('ExtraTreesClassifier',ExtraTreesClassifier(n_estimators=10, max_depth=None,min_samples_split=2)))\nmodels.append(('BagClassifier',BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.5)))\n# models.append(('HistGradientBoostClassifier',HistGradientBoostingClassifier(max_iter=100)))\n#models.append(('SupportVectorClassifier',SVC(C=1.0,kernel='sigmoid')))\nmodels.append(('XGBoosting',xg(n_estimators=100,random_state=42)))\nmodels.append(('LightGBM',lg(n_estimators=100,random_state=42)))\nfasttext_model_training_result,fasttext_model_validation_result=[],[]\nscoring='accuracy'\nprint(\"Statistical Model Google Vectors- Baseline Evaluation\")\nfor name,model in models:\n    kfold=KFold(n_splits=10,random_state=7)\n    results=cross_val_score(model,train_fasttext_x,train_fasttext_y,cv=kfold)\n    print(\"=======================\")\n    print(\"Classifiers: \",name, \"Has a training score of\", round(results.mean(), 2) * 100, \"% accuracy score\")\n    w2v_model_training_result.append(results.mean())\n    predictions=cross_val_predict(model,test_fasttext_x,test_fasttext_y)\n    accuracy = accuracy_score(predictions,test_fasttext_y)\n    w2v_model_validation_result.append(accuracy)\n\nfinal_fasttext_outcomes=pd.DataFrame(columns=['Model','Training Acc','Validation Acc'])\nfinal_fasttext_outcomes['Model']=models\nfinal_fasttext_outcomes['Training Acc']=fasttext_model_training_result\nfinal_fasttext_outcomes['Validation Acc']=fasttext_model_validation_result\nfinal_fasttext_outcomes.to_csv('GoogleNewsVectors-Baseline.csv',index=False)\nfinal_fasttext_outcomes","3173fe19":"## MultiNomial Naive Bayes on TFIDF Baseline\n\n[MultiNomial NB](https:\/\/ogrisel.github.io\/scikit-learn.org\/sklearn-tutorial\/modules\/generated\/sklearn.naive_bayes.MultinomialNB.html) is a probabilistic statistical classification model which uses conditional probability to segregate or classify samples. This works well with discrete integer valued features (such as count vectorization) but can also be used with TFIDF vectors. Particularly, this uses the Bayes Theorem which tries to determine conditional probability using prior and posterior probabilities as shown in the figure:\n\n<img src=\"https:\/\/storage.googleapis.com\/coderzcolumn\/static\/tutorials\/machine_learning\/article_image\/Scikit-Learn%20-%20Naive%20Bayes.jpg\">\n\nThe major concept under this category is statistics of [Naive Bayes](https:\/\/scikit-learn.org\/stable\/modules\/naive_bayes.html). There are many variants:\n\n- Gaussian NB which relies on Gaussian distribution of the input features\n\n<img src=\"https:\/\/www.researchgate.net\/profile\/Yune_Lee\/publication\/255695722\/figure\/fig1\/AS:297967207632900@1448052327024\/Illustration-of-how-a-Gaussian-Naive-Bayes-GNB-classifier-works-For-each-data-point.png\">\n\n\n- Complement NB which is suited for unbalanced classes and relies on the statistics of complement of each class to generate the weights. It is better than Multinomial NB for textual classification as it has a normalization factor (and a smoothing hyperparameter alpha) which tends to capture information from longer sequences of text.\n\n\n<img src=\"https:\/\/www.researchgate.net\/profile\/Motaz_Saad\/publication\/231521157\/figure\/fig7\/AS:667829850345476@1536234452166\/Figure-31-Complement-Naive-Bayes-Algorithm-72.png\">\n\n- Bernoulli NB which relies on multivariate bernoulli distributions of the input features and also expects the data to be in binary format.\n\n<img src=\"https:\/\/www.astroml.org\/_images\/fig_simple_naivebayes_1.png\">\n\nOther variants include:\n\n- Categorical NB\n- Partial Fit of NB models\n\nResources:\n\n- [Blog](https:\/\/www.analyticsvidhya.com\/blog\/2017\/09\/naive-bayes-explained\/)\n- [Kernel](https:\/\/www.kaggle.com\/abhilash1910\/nlp-workshop-ml-india#Vectorization-and-Benchmarking)\n- [Jason's Blog](https:\/\/machinelearningmastery.com\/classification-as-conditional-probability-and-the-naive-bayes-algorithm\/)\n\n\n","2430d2da":"## Splitting the set\n\n\nOriginally we have the transformed \/vectorized data as we saw in previous notebook, the TSNE transformation of which looked like :\n\n<img src=\"https:\/\/i.imgur.com\/uuhL17b.png\">\n\n\nIn generic supervised learning, we are implying model training after splitting the tfidf vectorized data. This transformation is linear and just splits depending on the ratio\/test size provided by us. For this strategy, we will be using the sklearn split module.There are many versions of splitting the data based on the data, and some of them include:\n\n- [Test Train Split](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html)\n- [Stratified Split](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html)\n- [Stratified Shuffle Split](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedShuffleSplit.html)\n- [Shuffle Split](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.ShuffleSplit.html)\n- [Stratified K Fold](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedKFold.html)\n\nJason's [blog](https:\/\/machinelearningmastery.com\/train-test-split-for-evaluating-machine-learning-algorithms\/) provides a good idea about splitting techniques for generic classification problems.","351a0119":"## Converting Other Vectors (Glove,Fasttext) to Word2Vec \n\nIn this case, we will be using Glove,Fasttext by converting them to Word2Vec embeddings and then applying mean pooling on them. The method of conversion is taken from the [previous Notebook](https:\/\/www.kaggle.com\/colearninglounge\/nlp-end-to-end-cll-nlp-workshop). The steps involved are as follows:\n\n- Convert Glove,Fasttext,Google News to Word2Vec by using Gensim\n- Apply Mean Pooling on the Word Vectors to create Sentence Vectors\n- Apply Statistical Classifiers with Kfold Cross Validation\n\nThere are alternate strategies to apply bu the this is by far the simplest one with minimalistic code.\nSome resource:\n\n- [Good Alternate Script](https:\/\/www.kaggle.com\/eswarbabu88\/toxic-comment-glove-logistic-regression)\n- [Blog](https:\/\/towardsdatascience.com\/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa)\n\n","64fba524":"## Revisiting TFIDF for our Baseline\n\nTFIDF vectorization is non semantic frequency based algorithm which uses a logarithmic distribution over document frequencies to embed vectors based on normalized frequency of occurence of words in the corpus. A descriptive formulation is provided here:\n\n<img src=https:\/\/plumbr.io\/app\/uploads\/2016\/06\/tf-idf.png>\n\n\nThe logical inference for using TFIDF vectorization over other vectorization strategies to embed vectors in HD space is to capture rare words occuring across the corpus. This vectorized embeddings can be applied on a statistical model for training.\n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/876\/1*_OsV8gO2cjy9qcFhrtCdiw.jpeg\">","6b552dc6":"## Apply Statistical Models on Static Embeddings\n\nNow we move forward to apply the statsitical models on the Compressed Sentence Vectors . This will allow us to apply gradient boosting algorithms on the static embeddings computed by taking the mean of the word vectors.We have also compressed the train and test dataset for our purposes.\n\nSteps:\n\n- Apply Word2Vec on the Corpus\n- Create Sentence Vectors by Mean Pooling\n- Run the input sentence vectors with Kfold Cross Validation on Traditional and gradient boosting classifiers.\n\n\n<img src=\"https:\/\/media1.tenor.com\/images\/0e438477bb88b5683690bfe101cf1181\/tenor.gif?itemid=10724659\">","3a75e444":"### Convert the sentence vectors to List\n\nThis is done to ensure the dimensionality of the input sentence vectors is that of an array (list). This can be easily fed into any statistical classifier for our use case.\n\n\n","7a48c1e6":"## Applying the Classifiers\n\nHere in this sequence of codebases we apply the classifiers for our use case. ","86915bf6":"## Brief Introduction of Statistical Models\n\n\nThis is going to be a brief introduction of different statistical models which will be used simultaeneously with k fold and cross validation techniques for examining the accuracy of the models. In this case, we will be focussing on accuracy as the major KPI and later we will be running on different observations such as f1,ROC etc.\n\n\n### Decision Trees\n\n[Decision Trees](https:\/\/scikit-learn.org\/stable\/modules\/tree.html) is a supervised model for classification\/regression. This works on creating decision branches which evolves a criteria and is often acknowledged as a simplistic classification (white box) model as the stages of decision can be easily derived. A regression tree appears as follows:\n\n<img src=\"https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_tree_regression_0011.png\">\n\nThe [algorithms](https:\/\/scikit-learn.org\/stable\/modules\/tree.html) include ID3,C4.5\/C5.0,CART which can be analysed as follows:\n\n- ID3(Iterative Dichotomiser 3) was developed in 1986 by Ross Quinlan. The algorithm creates a multiway tree, finding for each node (i.e. in a greedy manner) the categorical feature that will yield the largest information gain for categorical targets. Trees are grown to their maximum size and then a pruning step is usually applied to improve the ability of the tree to generalise to unseen data.\n\n- C4.5 is the successor to ID3 and removed the restriction that features must be categorical by dynamically defining a discrete attribute (based on numerical variables) that partitions the continuous attribute value into a discrete set of intervals. C4.5 converts the trained trees (i.e. the output of the ID3 algorithm) into sets of if-then rules. These accuracy of each rule is then evaluated to determine the order in which they should be applied. Pruning is done by removing a rule\u2019s precondition if the accuracy of the rule improves without it.\n\n- C5.0 is Quinlan\u2019s latest version release under a proprietary license. It uses less memory and builds smaller rulesets than C4.5 while being more accurate.\n\n- CART (Classification and Regression Trees) is very similar to C4.5, but it differs in that it supports numerical target variables (regression) and does not compute rule sets. CART constructs binary trees using the feature and threshold that yield the largest information gain at each node.\n\n\nIn general, the major logics involved in Decision Trees involves computation of Entropy or Gini Index, which is as follows:\n\n<img src=\"https:\/\/qph.fs.quoracdn.net\/main-qimg-690a5cee77c5927cade25f26d1e53e77\">\n\nTypically a Gini Coefficient  is evaluated as the area between the ```y=x``` line and Lorentz curve\n\n<img src=\"https:\/\/i.stack.imgur.com\/iawuF.jpg\">\n\n\nMisclassification is another criteria:\n\n<img src=\"https:\/\/miro.medium.com\/max\/2180\/1*O5eXoV-SePhZ30AbCikXHw.png\">\n\nTypically a decision tree appears as follows:\n\n<img src=\"https:\/\/scikit-learn.org\/stable\/_images\/iris.png\">\n\nSome resources:\n\n- [Blog](https:\/\/towardsdatascience.com\/scikit-learn-decision-trees-explained-803f3812290d)\n- [Blog](https:\/\/machinelearningmastery.com\/classification-and-regression-trees-for-machine-learning\/)\n- [Blog](https:\/\/machinelearningmastery.com\/cost-sensitive-decision-trees-for-imbalanced-classification\/)\n\n\n\n### Random Forests \n\n\n[Random Forests](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html)  is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the ```max_samples``` parameter ```if bootstrap=True (default)```, otherwise the whole dataset is used to build each tree. When splitting each node during the construction of a tree, the best split is found either from all input features or a random subset of size ```max_features```. (See the parameter tuning guidelines for more details).The purpose of these two sources of randomness is to decrease the variance of the forest estimator. Indeed, individual decision trees typically exhibit high variance and tend to overfit. The injected randomness in forests yield decision trees with somewhat decoupled prediction errors. By taking an average of those predictions, some errors can cancel out. Random forests achieve a reduced variance by combining diverse trees, sometimes at the cost of a slight increase in bias. In practice the variance reduction is often significant hence yielding an overall better model.\n\n<img src=\"https:\/\/www.researchgate.net\/profile\/Hung_Cao12\/publication\/333438248\/figure\/fig6\/AS:763710377299970@1559094151459\/Random-Forest-model-with-majority-voting.ppm\">\n\n\n### Gradient Boosting Forests and Trees\n\n[Gradient Boosting](https:\/\/machinelearningmastery.com\/gentle-introduction-gradient-boosting-algorithm-machine-learning\/) is a central part of ensemble modelling in sklearn.\n\nThe goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability \/ robustness over a single estimator.\n\nTwo families of ensemble methods are usually distinguished:\n\n- In averaging methods, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.\n\n  Examples: Bagging methods, Forests of randomized trees\n\n- By contrast, in boosting methods, base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.\n\n  Examples: AdaBoost, Gradient Tree Boosting\n\n\nPictorially these can be represented as :\n\n<img src=\"https:\/\/miro.medium.com\/max\/3908\/1*FoOt85zXNCaNFzpEj7ucuA.png\">\n\n\nSeveral Boosting Models can be found under this criteria:\n\n#### [AdaBoosting](https:\/\/blog.paperspace.com\/adaboost-optimizer\/#:~:text=AdaBoost%20is%20an%20ensemble%20learning,turn%20them%20into%20strong%20ones.): \nThe core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consist of applying weights  to each of the training samples. Initially, those weights are all set to (1\/N), so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence\n\n<img src=\"https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_adaboost_hastie_10_2_0011.png\">\n\n\n### [LightGBM](https:\/\/lightgbm.readthedocs.io\/en\/latest\/)\n\nLight GBM is another gradient boosting strategy which relies on trees.It has the following advantages:\n\n- Faster training speed and higher efficiency.\n\n- Lower memory usage.\n\n- Better accuracy.\n\n- Support of parallel and GPU learning.\n\n- Capable of handling large-scale data.\n\nLightGBM grows leaf-best wise and will choose the leaf with maximum max delta loss to grow.\n\n<img src=\"https:\/\/lightgbm.readthedocs.io\/en\/latest\/_images\/leaf-wise.png\">\n\n\nSome resources:\n\n- [XGB](https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/model.html)\n- [Blogs](https:\/\/www.analyticsvidhya.com\/blog\/tag\/gradient-boosting\/)","24eccfb2":"### Applying SMOTE balancing on TFIDF Baseline\n\nHere, we will be applying SMOTE on TFIDF vectorized data for creating a different baseline.Since in our case, the data is balanced, applying SMOTE on balanced data would only reduce the efficiency of the models.\n\n","4579029d":"## Analysing TFIDF-LR Baseline with simple split \n\nIn this case, we want to evaluate the performance of a Logistic Regression classifier on the tfidf vectorized data sampled with normal train_test_split. Logistic Regression Classifier uses a sigmoid kernel for training. In a supervised learning mode , Logistic Regression is one of the standardised models under generalized linear models which tries a convex optimization by passing the cost function through the sigmoid kernel. The sigmoid function is denoted by the formulation:\n\n<img src=\"https:\/\/www.gstatic.com\/education\/formulas2\/-1\/en\/sigmoid_function.svg\">\n\n\nThis equation due to its convergence property (+\/- infinity) and due to its differentiability , the sigmoid kernel allows clamping of predicted values to binary labels. The sigmoid curve actually has optima at x=0 and x=1.Now in the case of supervised logistic regression, when we try to optimize the cost function (in this case a linear sum of weights & biases passed through sigmoid kernel), by applying stochastic gradient descent. Since by gradient descent, the steepest slope is considered, the change in derivatives (gradients) at each stage is computed and the weights of the cost function are updated. The effective loss function for logistic regression is E=(|y_predicted -y_actual|^2). This [blog](https:\/\/machinelearningmastery.com\/gradient-descent-for-machine-learning\/) provides an idea. \n\n\n\n<img src=\"https:\/\/res.cloudinary.com\/practicaldev\/image\/fetch\/s--xoKf0Xfi--\/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\/https:\/\/cdn-images-1.medium.com\/max\/2000\/1%2AXisqJi764DTxragiRFexSQ.png\">\n\nSome resources:\n\n- [Blog](https:\/\/www.analyticsvidhya.com\/blog\/2020\/10\/how-does-the-gradient-descent-algorithm-work-in-machine-learning\/)\n- [Sklearn documentation](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html)\n- [KDNuggets](https:\/\/www.kdnuggets.com\/2017\/04\/simple-understand-gradient-descent-algorithm.html)\n- [Blog](https:\/\/towardsdatascience.com\/machine-learning-101-an-intuitive-introduction-to-gradient-descent-366b77b52645)\n\n","12873350":"## Multiple Baseline computation using KFold and Cross Validation\n\n\nIn this concept, we will be training multiple statistical models based on KFold and Cross Validation Technique.\nKFold cross validators provide train\/test split indices for splitting the dataset into 'k' folds without shuffling. The general methodology for using Kfold and Cross Validation is provided below:\n\n- Shuffle the dataset randomly.\n- Split the dataset into k groups\n- For each unique group:\n   - Take the group as a hold out or test data set\n   - Take the remaining groups as a training data set\n   - Fit a model on the training set and evaluate it on the test set\n   - Retain the evaluation score and discard the model\n- Summarize the skill of the model using the sample of model evaluation scores\n\nThis technique has a following rule: The first ``` n_samples % n_splits``` folds have size ``` n_samples \/\/ n_splits + 1```, other folds have size ``` n_samples \/\/ n_splits```, where n_samples is the number of samples.\n\nA typical flowchart of cross validation is provided below:\n\n<img src=\"https:\/\/scikit-learn.org\/stable\/_images\/grid_search_workflow.png\">\n\nThis allows for better hyperparameter search using GridSearch CV algorithms which will be covered later.\nThe following procedure is followed for each of the k \u201cfolds\u201d:\n\n- A model is trained using  of the folds as training data;\n\n- The resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n\nThe performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small.\n\n<img src=\"https:\/\/scikit-learn.org\/stable\/_images\/grid_search_cross_validation.png\">\n\nSome resources:\n\n- [Blog](https:\/\/machinelearningmastery.com\/k-fold-cross-validation\/)\n- [Documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.KFold.html)\n- [Documentation](https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html)\n\n","86d36e60":"## Recap from Workshop-1\n\nIn the previous [Notebook](https:\/\/www.kaggle.com\/colearninglounge\/nlp-end-to-end-cll-nlp-workshop), we learned about cleaning,EDA, vectorization and embeddings. We saw how cleaning played a significant role in determining knowledge embeddings from the corpus and also determined the similarity between words and sentences. \n\nWe will be following a few steps from the elaborate Notebook:\n\n- Use the cleaning methods (regex) for redundancy removal\n- Lemmatization\n- Vectorization\n- Embeddings (Static,Dynamic,Transformer)\n\nSince we have already implemented the Regex cleaning method, we can apply the same here. In the first section of this notebook, we will be running statistical classifiers, with 3 different vectorized data.\n\n- Non semantic TFIDF Vectorized Data\n- Static Embedding Vectorized Data\n- Dynamic Embedding Vectorized Data\n\nFor the first use case of statistical models, we will be relying on TFIDF Baseline with Statistical classifiers.\n\n\n<img src=\"https:\/\/i.pinimg.com\/originals\/b0\/ec\/e4\/b0ece436f4244f1f97bab3facf4d6b8a.gif\">\n","771c0204":"## Creating Sentence Vectors from Word2Vec\n\nSince Word2Vec creates vector embeddings for individual words in a corpus by transforming them to a manifold, we need effective document \/sentence vectors from these individual vectorized words.  The concept of [pooling](https:\/\/medium.com\/technologymadeeasy\/the-best-explanation-of-convolutional-neural-networks-on-the-internet-fbb8b1ad5df8#:~:text=A%20pooling%20layer%20is%20another,in%20pooling%20is%20max%20pooling) is derived from Neural Networks particularly [Convolution Neural Network Architectures](https:\/\/analyticsindiamag.com\/max-pooling-in-convolutional-neural-network-and-its-features\/) where MaxPooling signifies taking the maximum from a range (particularly a kernel\/filter or a window of input features). A typical Maxpooling diagram is as follows:\n\n\n<img src=\"https:\/\/i.redd.it\/61tcfy2xy2u41.png\">\n\n\n\nBut in the case of creating document embeddings, a general notion is to use [Average pooling](https:\/\/i.redd.it\/61tcfy2xy2u41.png). Mean pooling is generally used to create document vectors by taking the average of all the vectors in the context. A schematic diagram of the same is provided:\n\n\n\n<img src=\"https:\/\/yashuseth.files.wordpress.com\/2018\/08\/5.jpg?w=834\">\n\n\nAs we move forward towards using complex embeddings, we will be using Mean Pooling to create sentence\/paragraph vectors from the individual word vectors. There are also other strategies involving Max Pooling and then applying Mean Pooling on the word Vectors to create complete vectors.\n\n<img src=\"https:\/\/www.researchgate.net\/profile\/Xingsheng_Yuan\/publication\/332810604\/figure\/fig2\/AS:754128875683841@1556809743129\/Simple-word-embedding-based-model-with-modified-hierarchical-pooling-strategy.png\">\n\n\nSome resources\n\n- [Research](https:\/\/www.researchgate.net\/figure\/Simple-word-embedding-based-model-with-modified-hierarchical-pooling-strategy_fig2_332810604)\n- [Some paper](https:\/\/www.cs.tau.ac.il\/~wolf\/papers\/qagg.pdf)\n- [Huggingface](https:\/\/medium.com\/huggingface\/universal-word-sentence-embeddings-ce48ddc8fc3a)","76cdbe8c":"## Applying XGBoost and LightGBM\n\nThis will allow us to further analyse the  results of XGBoost over LightGBM.\n\n<img src=\"https:\/\/miro.medium.com\/max\/1554\/1*FLshv-wVDfu-i54OqvZdHg.png\">","574de191":"## Splitting in Imbalanced Classes\n\nThere are several ways to strategize and split imbalanced classes. One of the ways is to use the \"stratify\" (Stratified Split) option during train_test_split. this splitting allows propertional splitting between the  classes, and also maintains the proportionality on the training and testing dataset.","2a8862f7":"## NLP Model Building: Traditional Classifiers\n\nAuthored by [abhilash1910](https:\/\/www.kaggle.com\/abhilash1910)\n\n### Movie Reviews !!\n\nThis is an extension of the original [Notebook](https:\/\/www.kaggle.com\/colearninglounge\/nlp-end-to-end-cll-nlp-workshop) which has been separately provided for a piecewise analysis of the NLP Model building with Transformers and sophiosticated Neural architectures. For more details other kernels are also provided:\n\n- [Kernel](https:\/\/www.kaggle.com\/colearninglounge\/nlp-end-to-end-cll-nlp-workshop-2)\n- [Kernel](https:\/\/www.kaggle.com\/abhilash1910\/nlp-workshop-ml-india)\nThe second and most interesting part of the curriculum is building different models - both statistical and deep learning so as to provide a proper classification model for our use case. In this case, we will create an initial baseline with statistical classification algorithms by using both non semantic vectors as well as semantic vectors. Later we will try to improvise on these baselines with standard neural models like Bidirectional LSTMs, Convolution Networks, Gated Recurrecnt Units. The improvements of these traditional neural models over the baselines would be further investigated when we will explore advanced architectures, particularly that of an encoder decoder . Further advancement would be made on attention based encoder-decoder modules like Transformers and using the different flavours from BERT to GPT.\n\nThe following is the workflow of this notebook:\n\n- Traditional NN models\n  - With Static Embeddings","b360fad6":"## Concluding Non Semantic Baseline Techniques\n\n\nWe have seen the effect of applying a statistical classifier on the basis of non semantic TFIDF vectorized data and also attained a parallel analysis of the accuracy of the different algorithms. The inference for using these statistical models is that it provides an initial benchmark which has to be improved further by trying different models as such. This provides a quick overview of how  a traditional classifier can be used for non semantic classification and in the next case we will be using semantic embeddings (vectors) with these traditional classifiers.\n\n\n<img src=\"https:\/\/media2.giphy.com\/media\/118u58QrLaLnDG\/giphy.gif\">","fe5f2258":"## Statistical Training Without Balancing\n\nWe have heard of balancing techniques in the previous [Notebook](https:\/\/www.kaggle.com\/colearninglounge\/nlp-end-to-end-cll-nlp-workshop), and approaches like [SMOTE](https:\/\/machinelearningmastery.com\/smote-oversampling-for-imbalanced-classification\/), [Adasyn](https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/generated\/imblearn.over_sampling.ADASYN.html),are used to balance the classes. In the first case, we will not be balancing the set and evaluate the preliminary training of the statistical models.This initial benchmark can be used for further improvement by balancing the dataset.\n\n\n","2e0f9462":"## Static Semantic Embedding Baseline\n\n\nIn this context, we will be applying the traditional ML classifiers on Word2Vec, Glove and Fasttext data to create a classification model. We have already realised the concept of these embeddings and some of the images of applying these embeddings on the corpus is provided in the Notebook [here](https:\/\/www.kaggle.com\/colearninglounge\/nlp-end-to-end-cll-nlp-workshop):\n\n### Word2Vec From [Gensim](https:\/\/www.analyticsvidhya.com\/blog\/tag\/word2vec\/)\n\n<img src=\"https:\/\/www.kaggleusercontent.com\/kf\/48903343\/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..C4q3IW8yHfXenn6fNzGSXQ.eHWK4_UfbdG6wh4wTDUBKfapys5hjPEyZqNdH-szO_domUFCC5uIpgEqg6IWleffop3sIT6lPyKsKPaMMNxUR9GtqbkZroidkwVVGL_4hdybmE0o6-yCbbAGxJfO44uwvSIc8ak_QqhFLLxhANMRRceiSBn7jK4mK9iUhnx4EBXhj9JQfuwlHDlCyWrL9FtyQV0a_iY0yRpJ39EvAjGAwYSnwk2FqjADkptTUaKA3liDy_ZohvtUGXZh4BEX0SCLLXpfKkleqq5sTeTLMU_h-AHH2z8AyVXFpSTMVAmXh2urgGjl1BbQjyf1fhxETZFj1eoCnpFddvNuK8hrIqdvuDyaybFnb_MTFScC3104AWu7sI7ke3-7fUFc0dGSzBzY2RE1s17MdaepXCNmDuBr40Yd2O4fN9VziTSgAZUKEhnbJe1C_MZtuYkUMZY7kRjiHstQQrS4lVYnALNlFlqzDy61-dl7MMgQlcK-EePn6lK4umB94lq1EC-AFLzFaoMeX4iC6z8LUPcSaQjjlkuOHBWpdfj3fYCQ_uaXeD7-vKuqTr-c1EAhrdXJjgOCzQjLmlvw1oisiTPpVDSjl2P4J6fL8WxEG4TmiBIHhQJywYr0lMN2kZdLDa3U98KtbFf5ed4_7upTP_IzQ2g8VzKvZa8W-9qRGFWLoW1hehO0DxQ.dFpEskCVbLZ4qvYaycKJcg\/__results___files\/__results___63_1.png\">\n\n<img src=\"https:\/\/www.kaggleusercontent.com\/kf\/48903343\/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..C4q3IW8yHfXenn6fNzGSXQ.eHWK4_UfbdG6wh4wTDUBKfapys5hjPEyZqNdH-szO_domUFCC5uIpgEqg6IWleffop3sIT6lPyKsKPaMMNxUR9GtqbkZroidkwVVGL_4hdybmE0o6-yCbbAGxJfO44uwvSIc8ak_QqhFLLxhANMRRceiSBn7jK4mK9iUhnx4EBXhj9JQfuwlHDlCyWrL9FtyQV0a_iY0yRpJ39EvAjGAwYSnwk2FqjADkptTUaKA3liDy_ZohvtUGXZh4BEX0SCLLXpfKkleqq5sTeTLMU_h-AHH2z8AyVXFpSTMVAmXh2urgGjl1BbQjyf1fhxETZFj1eoCnpFddvNuK8hrIqdvuDyaybFnb_MTFScC3104AWu7sI7ke3-7fUFc0dGSzBzY2RE1s17MdaepXCNmDuBr40Yd2O4fN9VziTSgAZUKEhnbJe1C_MZtuYkUMZY7kRjiHstQQrS4lVYnALNlFlqzDy61-dl7MMgQlcK-EePn6lK4umB94lq1EC-AFLzFaoMeX4iC6z8LUPcSaQjjlkuOHBWpdfj3fYCQ_uaXeD7-vKuqTr-c1EAhrdXJjgOCzQjLmlvw1oisiTPpVDSjl2P4J6fL8WxEG4TmiBIHhQJywYr0lMN2kZdLDa3U98KtbFf5ed4_7upTP_IzQ2g8VzKvZa8W-9qRGFWLoW1hehO0DxQ.dFpEskCVbLZ4qvYaycKJcg\/__results___files\/__results___59_1.png\">\n\n### Google News Vectors Variant of [Word2Vec](https:\/\/code.google.com\/archive\/p\/word2vec\/)\n\n\n<img src=\"https:\/\/www.kaggleusercontent.com\/kf\/48903343\/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..C4q3IW8yHfXenn6fNzGSXQ.eHWK4_UfbdG6wh4wTDUBKfapys5hjPEyZqNdH-szO_domUFCC5uIpgEqg6IWleffop3sIT6lPyKsKPaMMNxUR9GtqbkZroidkwVVGL_4hdybmE0o6-yCbbAGxJfO44uwvSIc8ak_QqhFLLxhANMRRceiSBn7jK4mK9iUhnx4EBXhj9JQfuwlHDlCyWrL9FtyQV0a_iY0yRpJ39EvAjGAwYSnwk2FqjADkptTUaKA3liDy_ZohvtUGXZh4BEX0SCLLXpfKkleqq5sTeTLMU_h-AHH2z8AyVXFpSTMVAmXh2urgGjl1BbQjyf1fhxETZFj1eoCnpFddvNuK8hrIqdvuDyaybFnb_MTFScC3104AWu7sI7ke3-7fUFc0dGSzBzY2RE1s17MdaepXCNmDuBr40Yd2O4fN9VziTSgAZUKEhnbJe1C_MZtuYkUMZY7kRjiHstQQrS4lVYnALNlFlqzDy61-dl7MMgQlcK-EePn6lK4umB94lq1EC-AFLzFaoMeX4iC6z8LUPcSaQjjlkuOHBWpdfj3fYCQ_uaXeD7-vKuqTr-c1EAhrdXJjgOCzQjLmlvw1oisiTPpVDSjl2P4J6fL8WxEG4TmiBIHhQJywYr0lMN2kZdLDa3U98KtbFf5ed4_7upTP_IzQ2g8VzKvZa8W-9qRGFWLoW1hehO0DxQ.dFpEskCVbLZ4qvYaycKJcg\/__results___files\/__results___66_0.png\">\n\n### [Glove](https:\/\/www.analyticsvidhya.com\/blog\/tag\/glove\/) [Embeddings](https:\/\/nlp.stanford.edu\/projects\/glove\/)\n\n<img src=\"https:\/\/www.kaggleusercontent.com\/kf\/48903343\/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..C4q3IW8yHfXenn6fNzGSXQ.eHWK4_UfbdG6wh4wTDUBKfapys5hjPEyZqNdH-szO_domUFCC5uIpgEqg6IWleffop3sIT6lPyKsKPaMMNxUR9GtqbkZroidkwVVGL_4hdybmE0o6-yCbbAGxJfO44uwvSIc8ak_QqhFLLxhANMRRceiSBn7jK4mK9iUhnx4EBXhj9JQfuwlHDlCyWrL9FtyQV0a_iY0yRpJ39EvAjGAwYSnwk2FqjADkptTUaKA3liDy_ZohvtUGXZh4BEX0SCLLXpfKkleqq5sTeTLMU_h-AHH2z8AyVXFpSTMVAmXh2urgGjl1BbQjyf1fhxETZFj1eoCnpFddvNuK8hrIqdvuDyaybFnb_MTFScC3104AWu7sI7ke3-7fUFc0dGSzBzY2RE1s17MdaepXCNmDuBr40Yd2O4fN9VziTSgAZUKEhnbJe1C_MZtuYkUMZY7kRjiHstQQrS4lVYnALNlFlqzDy61-dl7MMgQlcK-EePn6lK4umB94lq1EC-AFLzFaoMeX4iC6z8LUPcSaQjjlkuOHBWpdfj3fYCQ_uaXeD7-vKuqTr-c1EAhrdXJjgOCzQjLmlvw1oisiTPpVDSjl2P4J6fL8WxEG4TmiBIHhQJywYr0lMN2kZdLDa3U98KtbFf5ed4_7upTP_IzQ2g8VzKvZa8W-9qRGFWLoW1hehO0DxQ.dFpEskCVbLZ4qvYaycKJcg\/__results___files\/__results___70_0.png\">\n\n### [Fasttext Embeddings](https:\/\/fasttext.cc\/docs\/en\/supervised-tutorial.html)\n\n<img src=\"https:\/\/www.kaggleusercontent.com\/kf\/48903343\/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..C4q3IW8yHfXenn6fNzGSXQ.eHWK4_UfbdG6wh4wTDUBKfapys5hjPEyZqNdH-szO_domUFCC5uIpgEqg6IWleffop3sIT6lPyKsKPaMMNxUR9GtqbkZroidkwVVGL_4hdybmE0o6-yCbbAGxJfO44uwvSIc8ak_QqhFLLxhANMRRceiSBn7jK4mK9iUhnx4EBXhj9JQfuwlHDlCyWrL9FtyQV0a_iY0yRpJ39EvAjGAwYSnwk2FqjADkptTUaKA3liDy_ZohvtUGXZh4BEX0SCLLXpfKkleqq5sTeTLMU_h-AHH2z8AyVXFpSTMVAmXh2urgGjl1BbQjyf1fhxETZFj1eoCnpFddvNuK8hrIqdvuDyaybFnb_MTFScC3104AWu7sI7ke3-7fUFc0dGSzBzY2RE1s17MdaepXCNmDuBr40Yd2O4fN9VziTSgAZUKEhnbJe1C_MZtuYkUMZY7kRjiHstQQrS4lVYnALNlFlqzDy61-dl7MMgQlcK-EePn6lK4umB94lq1EC-AFLzFaoMeX4iC6z8LUPcSaQjjlkuOHBWpdfj3fYCQ_uaXeD7-vKuqTr-c1EAhrdXJjgOCzQjLmlvw1oisiTPpVDSjl2P4J6fL8WxEG4TmiBIHhQJywYr0lMN2kZdLDa3U98KtbFf5ed4_7upTP_IzQ2g8VzKvZa8W-9qRGFWLoW1hehO0DxQ.dFpEskCVbLZ4qvYaycKJcg\/__results___files\/__results___73_0.png\">\n","44082706":"## Conclusion\n\nThis concludes non semantic baseline classification along with static embeddings. The next part can be found in this [Notebook](https:\/\/www.kaggle.com\/colearninglounge\/nlp-model-building-transformers-attention-more)\n\n\n<img src=\"https:\/\/64.media.tumblr.com\/38d4b3b4455c6bf339f26cc3ab49e653\/tumblr_podm01SXsM1sc0ffqo2_540.gifv\">"}}