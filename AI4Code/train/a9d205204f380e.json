{"cell_type":{"c8b3c826":"code","f903d5db":"code","a6cc8a2f":"code","3650f5cf":"code","61df4dba":"code","07ddd993":"code","b2f1959a":"code","e1109509":"markdown","28c7088b":"markdown","b97e3e56":"markdown","c0e722e7":"markdown","da3277b6":"markdown"},"source":{"c8b3c826":"import pandas as pd\nimport numpy as np\nimport gc\n\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv\")\ntrain","f903d5db":"cat_features = [\"f22\", \"f43\"]\ncat_features.extend([\"f{}\".format(x) for x in range(242, 285)])\n\ncont_features = [\"f{}\".format(x) for x in range(242)]\ncont_features.remove(\"f22\")\ncont_features.remove(\"f43\")\n\ntarget = train[\"target\"]","a6cc8a2f":"# Reduce memory usage on our columns\nfor feature in cont_features:\n    train[feature] = train[feature].astype(np.float16)\n    test[feature] = test[feature].astype(np.float16)\n    \nfor feature in cat_features:\n    train[feature] = train[feature].astype(np.int8)\n    test[feature] = test[feature].astype(np.int8)\n    \n_ = gc.collect()","3650f5cf":"!pip install --force-reinstall xgboost==1.3.1","61df4dba":"import numpy\n\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nrandom_state = 2021\nn_folds = 3\nk_fold = StratifiedKFold(n_splits=n_folds)\n\nxgb_train_probas = numpy.zeros(len(train.index), )\nxgb_test_probas = numpy.zeros(len(test.index), )\n\ncb_train_probas = numpy.zeros(len(train.index), )\ncb_test_probas = numpy.zeros(len(test.index), )\n\nhgbc_train_probas = numpy.zeros(len(train.index), )\nhgbc_test_probas = numpy.zeros(len(test.index), )\n\nfeatures = cat_features + cont_features\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(train, target)):\n    print(\"--> Fold {}\".format(fold + 1))\n    y_train = target.iloc[train_index]\n    y_valid = target.iloc[test_index]\n\n    x_train = train[features].iloc[train_index]\n    x_valid = train[features].iloc[test_index]\n\n    xgb_model = XGBClassifier(\n        seed=random_state,\n        n_estimators=10000,\n        verbosity=1,\n        eval_metric=\"auc\",\n        tree_method=\"gpu_hist\",\n        gpu_id=0,\n        alpha=5.089629324639061,\n        colsample_bytree=0.9908475800809204,\n        gamma=2.7408840774631726,\n        reg_lambda=7.653094261603253,\n        learning_rate=0.07318975820906748,\n        max_bin=750,\n        max_depth=9,\n        min_child_weight=3.0472862580065305,\n        subsample=0.5607802273775566,\n        use_label_encoder=False,\n    )\n    xgb_model.fit(\n        x_train,\n        y_train,\n        eval_set=[(x_valid, y_valid)], \n        verbose=0,\n        early_stopping_rounds=50\n    )\n\n    train_oof_probas = xgb_model.predict_proba(x_valid)[:, -1]\n    test_oof_probas = xgb_model.predict_proba(test[features])[:, -1]\n\n    xgb_train_probas[test_index] = train_oof_probas\n    xgb_test_probas += test_oof_probas \/ n_folds\n    \n    print(\": XGB - ROC AUC Score = {}\".format(roc_auc_score(y_valid, train_oof_probas)))\n   \n    del(xgb_model)\n    _ = gc.collect()\n\n    \n    cb_model = CatBoostClassifier(\n        verbose=0,\n        eval_metric=\"AUC\",\n        random_state=random_state,\n        num_boost_round=20000,\n        od_type=\"Iter\",\n        od_wait=200,\n        task_type=\"GPU\",\n        devices=\"0\",\n        bootstrap_type=\"Bernoulli\",\n        grow_policy=\"Depthwise\",\n        l2_leaf_reg=6.177060577081939,\n        learning_rate=0.015198885894797058,\n        loss_function=\"CrossEntropy\",\n        max_depth=6,\n        min_data_in_leaf=2,\n        penalties_coefficient=0.8746786262649054,\n        cat_features=[x for x in range(len(cat_features))],\n    )\n    cb_model.fit(\n        x_train,\n        y_train,\n        eval_set=[(x_valid, y_valid)], \n        verbose=0,\n    )\n\n    train_oof_probas = cb_model.predict_proba(x_valid)[:, -1]\n    test_oof_probas = cb_model.predict_proba(test[features])[:, -1]\n\n    cb_train_probas[test_index] = train_oof_probas\n    cb_test_probas += test_oof_probas \/ n_folds\n    print(\": CB - ROC AUC Score = {}\".format(roc_auc_score(y_valid, train_oof_probas)))\n    \n    del(cb_model)\n    _ = gc.collect()\n\n    \n    hgbc_model = HistGradientBoostingClassifier(\n        random_state=2021,\n    )\n    hgbc_model.fit(\n        x_train,\n        y_train,\n    )\n\n    train_oof_probas = hgbc_model.predict_proba(x_valid)[:, -1]\n    test_oof_probas = hgbc_model.predict_proba(test[features])[:, -1]\n\n    hgbc_train_probas[test_index] = train_oof_probas\n    hgbc_test_probas += test_oof_probas \/ n_folds\n    print(\": HGBC - ROC AUC Score = {}\".format(roc_auc_score(y_valid, train_oof_probas)))\n\n    del(hgbc_model)\n    del(x_valid)\n    _ = gc.collect()\n\n    print(\"\")\n    \nprint(\"--> Overall metrics\")\nprint(\": XGB - ROC AUC Score = {}\".format(roc_auc_score(target, xgb_train_probas)))\nprint(\": CB - ROC AUC Score = {}\".format(roc_auc_score(target, cb_train_probas)))\nprint(\": HGBC - ROC AUC Score = {}\".format(roc_auc_score(target, hgbc_train_probas)))","07ddd993":"from sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.linear_model import RidgeClassifier\n\nrandom_state = 2021\nn_folds = 3\nk_fold = StratifiedKFold(n_splits=n_folds, random_state=random_state, shuffle=True)\n\nl1_train = pd.DataFrame(data={\n    \"xgb\": xgb_train_probas.tolist(),\n    \"cb\": cb_train_probas.tolist(),\n    \"hgbc\": hgbc_train_probas.tolist(),\n})\nl1_test = pd.DataFrame(data={\n    \"xgb\": xgb_test_probas.tolist(),\n    \"cb\": cb_test_probas.tolist(),\n    \"hgbc\": hgbc_test_probas.tolist(),\n})\n\ntrain_probas = numpy.zeros(len(l1_train.index), )\ntest_probas = numpy.zeros(len(l1_test.index), )\n\nfeatures = [\"xgb\", \"cb\", \"hgbc\"]\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(l1_train, target)):\n    print(\"--> Fold {}\".format(fold + 1))\n    y_train = target.iloc[train_index]\n    y_valid = target.iloc[test_index]\n\n    x_train = l1_train[features].iloc[train_index]\n    x_valid = l1_train[features].iloc[test_index]\n    \n    model = CalibratedClassifierCV(RidgeClassifier(random_state=random_state), cv=n_folds)\n    model.fit(\n        x_train,\n        y_train,\n    )\n\n    train_oof_probas = model.predict_proba(x_valid)[:, -1]\n    test_oof_probas = model.predict_proba(l1_test[features])[:, -1]\n\n    train_probas[test_index] = train_oof_probas\n    test_probas += test_oof_probas \/ n_folds\n\n    print(\": ROC AUC Score = {}\".format(roc_auc_score(y_valid, train_oof_probas)))\n    print(\"\")\n    \nprint(\"--> Overall metrics\")\nprint(\": ROC AUC Score = {}\".format(roc_auc_score(target, train_probas)))","b2f1959a":"submission[\"target\"] = test_probas.tolist()\nsubmission.to_csv(\"submission.csv\", index=False)","e1109509":"# Build Level 2 Models","28c7088b":"# Define Features\n\nThere are two types of features in the dataset, `cat_features` which are categorical, and `cont_features`, which are continuous. ","b97e3e56":"# Make Submission","c0e722e7":"# Build Level 1 Models","da3277b6":"# Introduction\n\nThis model demonstrates simple kernel stacking using XGBoost, Catboost, Hist Gradient Boosting Regression, and Ridge Regression. It uses 3-fold cross validation to build each model, and makes both test and training predictions out-of-fold. Those results are then fed into the level 2 Ridge model, where 3-fold cross validation is used again to make out-of-fold predictions for the submission result. No feature engineering is employed. Basic tuning is implemented. If you like the model, please consider upvoting!"}}