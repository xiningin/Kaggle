{"cell_type":{"b7d4d06f":"code","5b7b0bf9":"code","88114514":"code","1295e0c7":"code","fcba9e95":"code","56b14987":"code","9c42a877":"code","ed1a38d3":"code","04be3454":"code","f58987e5":"code","a8cd7bda":"code","31bc75f0":"code","2590326b":"code","fe9827dd":"code","8c406872":"code","638e475c":"code","3989733d":"code","9814de0a":"code","6ba2bb6d":"code","8ebe47ce":"code","ee02f43c":"code","5a69efa1":"code","ecb9bd16":"code","9712009b":"code","3cfe8d3e":"code","1fd310bb":"code","3e56afdf":"code","b51f00db":"code","f2499e0d":"code","9a8ed5f6":"code","b0fae269":"code","159b2b64":"code","eb93f8b4":"code","0b30059d":"code","353d04e1":"code","6d07618b":"code","dea3da52":"code","f60db5e0":"markdown","648d5010":"markdown","b9d572e7":"markdown","f5db209c":"markdown","113f46d8":"markdown","d3de9be5":"markdown","c8527361":"markdown","13904bb0":"markdown"},"source":{"b7d4d06f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5b7b0bf9":"import keras.backend as K\nfrom tensorflow.keras import models\nfrom numpy import array_equal\nimport numpy as np\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import LSTM, Bidirectional\nfrom tensorflow.keras.layers import Dense, Flatten,Embedding\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.layers import TimeDistributed\nfrom tensorflow.keras.layers import RepeatVector,Concatenate\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import load_model\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\nfrom tensorflow.keras.layers import Lambda, TimeDistributed\nfrom tensorflow.keras import backend as K","88114514":"# Loading Essential libraries \nimport warnings\nwarnings.filterwarnings('ignore')\n%config InlineBackend.figure_format = 'retina'\n%config Completer.use_jedi = False # this to force autocompletion ","1295e0c7":"### Reading the whole data \ndata_path  = '..\/input\/eng-hindi-translator\/hin.txt'\nwith open(data_path, 'r', encoding = 'utf-8') as f:\n    lines = f.read()\nlines[:100]   ","fcba9e95":"# now lets split the lines \ndef to_lines(text):\n    sents = text.strip().split('\\n')\n    sents = [text[0:2] for text in [i.split('\\t') for i in sents]]\n    return sents","56b14987":"df = to_lines(lines)\n","9c42a877":"df[:10]","ed1a38d3":"df = pd.DataFrame(df, columns = [ 'english', 'hindi'])\ndf = df.sample(frac = 1).reset_index(drop = True)\ndf.head()","04be3454":"# now lets do some preprocessing \n# lowercasing all the words\ndf['english'] = df.english.apply(lambda x:x.lower())\ndf['hindi'] = df.hindi.apply(lambda x:x.lower())","f58987e5":"!pip install text_hammer\nimport text_hammer as th","a8cd7bda":"# Remove quotes \nimport string\nexclude = set(string.punctuation)\nimport re\ndf['english'] = df.english.apply(lambda x:re.sub(\"'\",'',x))\ndf['hindi'] = df.hindi.apply(lambda x:re.sub(\"'\",'',x))\n# removing punctuation\ndf['english'] = df.english.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n                                 \ndf['hindi'] = df.hindi.apply(lambda x: ''.join(ch for ch in x if ch not in exclude ))\n# remove numericals number \ndf['english'] = df.english.apply(lambda x:re.sub('[0-9]+','',x))\ndf['hindi'] = df.hindi.apply(lambda x:re.sub('[0-9]+','',x))\ndf['hindi'] = df.hindi.apply(lambda x:re.sub('[\u0968\u0969\u0966\u096e\u0967\u096b\u096d\u096f\u096a\u096c]','',x))\n# Remove extra spaces\ndf['english']=df['english'].apply(lambda x: x.strip())\ndf['hindi']=df['hindi'].apply(lambda x: x.strip())\ndf['english']=df['english'].apply(lambda x: re.sub(\" +\", \" \", x))\ndf['hindi']=df['hindi'].apply(lambda x: re.sub(\" +\", \" \", x))\ndf['hindi'] = df.hindi.apply(lambda x: re.sub('[A-Za-z]+','',x))\ndf['english'] = df.english.progress_apply(lambda x:th.cont_exp(x))","31bc75f0":"# its time to prepare our data for training \ndf['hindi']= df['hindi'].apply(lambda x: 'START_ '+x+' _END') ","2590326b":"df['hindi_len'] = df.hindi.apply(lambda x:len(x.split()))\ndf['english_len'] = df.english.apply(lambda x:len(x.split()))","fe9827dd":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\ny = [len(x.split()) for x in df.english]\nplt.plot(y)\nplt.title('English words length')\nprint(np.max(y))","8c406872":"y = [len(x.split()) for x in df.hindi]\nplt.plot(y)\nplt.title('Hindi words length')\nprint(np.max(y))","638e475c":"df = df[(df.english_len<20)*(df.hindi_len<20)]\n","3989733d":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(df.english, df.hindi, test_size = 0.1, random_state= 42)","9814de0a":"def create_tokenizer(df):\n    tokenizer = Tokenizer(lower  = False, filters = '|!')\n    tokenizer.fit_on_texts(df)\n    return tokenizer","6ba2bb6d":"from keras.preprocessing.text import Tokenizer\n\nenglish_tokenizer = create_tokenizer(df.english)\nhindi_tokenizer = create_tokenizer(df.hindi)","8ebe47ce":"english_num_tokens = len(english_tokenizer.word_index) + 1\nhindi_num_tokens= len(hindi_tokenizer.word_index) + 1\nmax_english_len = df.english_len.max()\nmax_hindi_len = df.hindi_len.max()","ee02f43c":"hindi_num_tokens\n","5a69efa1":"def generate_batch(X = X_train, y = y_train, batch_size = 128):\n    ''' Generate a batch of data '''\n    while True:\n        for j in range(0, len(X), batch_size):\n            encoder_input_data = np.zeros((batch_size, max_english_len),dtype='float32')\n            decoder_input_data = np.zeros((batch_size, max_hindi_len),dtype='float32')\n            decoder_target_data = np.zeros((batch_size, max_hindi_len,hindi_num_tokens),dtype='float32')\n            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):\n                for t, word in enumerate(input_text.split()):\n                    encoder_input_data[i, t] = english_tokenizer.word_index[word] # encoder input seq\n                for t, word in enumerate(target_text.split()):\n                    if t<len(target_text.split())-1:\n                        decoder_input_data[i, t] = hindi_tokenizer.word_index[word] # decoder input seq\n                    if t>0: # here we will skip _START\n                        # decoder target sequence (one hot encoded) because it will be compared to the prediction \n                        # does not include the START_ token\n                        # Offset by one timestep because we are not going to include the START_\n                        decoder_target_data[i, t - 1, hindi_tokenizer.word_index[word]] = 1 # this is 3D\n            yield([encoder_input_data, decoder_input_data], decoder_target_data)\n            # decoder target data wiill be used to force our decoder to find right training","ecb9bd16":"import tensorflow as tf\nimport os\nfrom tensorflow.python.keras.layers import Layer\nfrom tensorflow.python.keras import backend as K\n\n\nclass AttentionLayer(Layer):\n    \"\"\"\n    This class implements Bahdanau attention (https:\/\/arxiv.org\/pdf\/1409.0473.pdf).\n    There are three sets of weights introduced W_a, U_a, and V_a\n     \"\"\"\n\n    def __init__(self, **kwargs):\n        super(AttentionLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert isinstance(input_shape, list)\n        # Create a trainable weight variable for this layer.\n\n        self.W_a = self.add_weight(name='W_a',\n                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n                                   initializer='uniform',\n                                   trainable=True)\n        self.U_a = self.add_weight(name='U_a',\n                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n                                   initializer='uniform',\n                                   trainable=True)\n        self.V_a = self.add_weight(name='V_a',\n                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n                                   initializer='uniform',\n                                   trainable=True)\n\n        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n\n    def call(self, inputs, verbose=False):\n        \"\"\"\n        inputs: [encoder_output_sequence, decoder_output_sequence]\n        \"\"\"\n        assert type(inputs) == list\n        encoder_out_seq, decoder_out_seq = inputs\n        if verbose:\n            print('encoder_out_seq>', encoder_out_seq.shape)\n            print('decoder_out_seq>', decoder_out_seq.shape)\n\n        def energy_step(inputs, states):\n            \"\"\" Step function for computing energy for a single decoder state\n            inputs: (batchsize * 1 * de_in_dim)\n            states: (batchsize * 1 * de_latent_dim)\n            \"\"\"\n\n            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n\n            \"\"\" Some parameters required for shaping tensors\"\"\"\n            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n            de_hidden = inputs.shape[-1]\n\n            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n            # <= batch size * en_seq_len * latent_dim\n            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n\n            \"\"\" Computing hj.Ua \"\"\"\n            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n            if verbose:\n                print('Ua.h>', U_a_dot_h.shape)\n\n            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n            # <= batch_size*en_seq_len, latent_dim\n            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n            if verbose:\n                print('Ws+Uh>', Ws_plus_Uh.shape)\n\n            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n            # <= batch_size, en_seq_len\n            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n            # <= batch_size, en_seq_len\n            e_i = K.softmax(e_i)\n\n            if verbose:\n                print('ei>', e_i.shape)\n\n            return e_i, [e_i]\n\n        def context_step(inputs, states):\n            \"\"\" Step function for computing ci using ei \"\"\"\n\n            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n\n            # <= batch_size, hidden_size\n            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n            if verbose:\n                print('ci>', c_i.shape)\n            return c_i, [c_i]\n\n        fake_state_c = K.sum(encoder_out_seq, axis=1)\n        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n\n        \"\"\" Computing energy outputs \"\"\"\n        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n        last_out, e_outputs, _ = K.rnn(\n            energy_step, decoder_out_seq, [fake_state_e],\n        )\n\n        \"\"\" Computing context vectors \"\"\"\n        last_out, c_outputs, _ = K.rnn(\n            context_step, e_outputs, [fake_state_c],\n        )\n\n        return c_outputs, e_outputs\n\n    def compute_output_shape(self, input_shape):\n        \"\"\" Outputs produced by the layer \"\"\"\n        return [\n            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n        ]","9712009b":"latent_dim = 1024 \n\n# Encoder \nencoder_inputs = Input(shape=(max_english_len,)) \nenc_emb = Embedding(english_num_tokens, latent_dim,trainable=True)(encoder_inputs) \n\n\nencoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True) \nencoder_outputs, state_h, state_c= encoder_lstm3(enc_emb) \n\n# Set up the decoder. \ndecoder_inputs = Input(shape=(None,)) \ndec_emb_layer = Embedding(hindi_num_tokens, latent_dim,trainable=True, mask_zero=True) \ndec_emb = dec_emb_layer(decoder_inputs) \n\n#LSTM using encoder_states as initial state\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \ndecoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c]) \n\n#Attention Layer\nattn_layer  = AttentionLayer(name='attention_layer') \nattn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs]) \n\n# Concat attention output and decoder LSTM output \ndecoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n\n#Dense layer\ndecoder_dense = TimeDistributed(Dense(hindi_num_tokens, activation='softmax')) \ndecoder_outputs = decoder_dense(decoder_concat_input) \n\n# Define the model\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs) \nmodel.summary()","3cfe8d3e":"# latent_dim = 1080\n# # TRAINING WITH TEACHER FORCING\n# # Define an input sequence and process it.\n# encoder_inputs= Input(shape=(None,))\n# enc_emb_layer =  Embedding(english_num_tokens, latent_dim, mask_zero = True,\n# #                            weights = [gensim_weight_matrix],trainable = False\n#                           )\n# enc_emb = enc_emb_layer(encoder_inputs)\n\n# #----------------------------------------------------------------#\n\n# encoder_l1 = LSTM(latent_dim, return_state=True, return_sequences = True)\n# encoder_outputs1, h1, c1 = encoder_l1(enc_emb) # here output consist of outputs, hiddenstates, cell_states \n\n# encoder_l2 = LSTM(latent_dim, return_state = True)\n# encoder_outputs2 , h2,c2 = encoder_l2(encoder_outputs1)\n\n# encoder_states = [h1, c1, h2, c2]\n\n\n# # Now lets design our decoder\n# decoder_inputs = Input(shape=(None,), name='decoder_inputs')\n# dec_emb_layer = Embedding(hindi_num_tokens, latent_dim, mask_zero = True)\n# dec_emb = dec_emb_layer(decoder_inputs)\n\n# #---------------------------------------------------------------------------#\n\n# out_layer1 = LSTM(latent_dim, return_sequences=True, return_state = True)\n\n# decoder_outputs1, dec_h1, dec_h2 = out_layer1(dec_emb,initial_state= [h1,c1])\n\n# out_layer2 = LSTM(latent_dim, return_sequences=True, return_state = True)\n# final, dec_h2, dec_c2 = out_layer2(decoder_outputs1, initial_state= [h2,c2])\n\n# output_layer = Dense(hindi_num_tokens, activation = 'softmax')\n# decoder_outputs2 = output_layer(final)\n\n\n# model = Model([encoder_inputs, decoder_inputs], decoder_outputs2) # here encoder_inputs, decoder inputs will be our model input \n\n","1fd310bb":"model.summary()","3e56afdf":"import keras\nes = keras.callbacks.EarlyStopping(monitor = 'val_loss',patience = 15, mode = 'min')\nlr = keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', patience = 3, mode = 'min', verbose = True)\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n","b51f00db":"train_samples = len(X_train)\nval_samples = len(X_test)\nbatch_size = 64\nepochs = 120","f2499e0d":"model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),\n                    steps_per_epoch = train_samples\/\/batch_size,\n                    epochs=epochs,\n                    validation_data = generate_batch(X_test, y_test, batch_size = batch_size),\n                    validation_steps = val_samples\/\/batch_size, callbacks = [lr, es])","9a8ed5f6":"# encoder inference\nencoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n\n# decoder inference\n# Below tensors will hold the states of the previous time step\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_hidden_state_input = Input(shape=(None,latent_dim)) # this is the encoders all hidden states \n\n# Get the embeddings of the decoder sequence\ndec_emb2= dec_emb_layer(decoder_inputs)\n\n# To predict the next word in the sequence, set the initial states to the states from the previous time step\ndecoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n\n#attention inference\nattn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\ndecoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n\n# A dense softmax layer to generate prob dist. over the target vocabulary\ndecoder_outputs2 = decoder_dense(decoder_inf_concat)\n\n# Final decoder model\ndecoder_model = Model(\n[decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n[decoder_outputs2] + [state_h2, state_c2])","b0fae269":"# now we need a function to provide all these inputs and outputs dynamically\ndef decode_sequence(input_seq):  # here input sequence will be our english sentence\n    # Encode the input as state vectors.\n    e_out, e_h, e_c = encoder_model.predict(input_seq)\n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1,1))\n    # Populate the first character of target sequence with the start character.\n    target_seq[0, 0] = hindi_tokenizer.word_index['START_']\n     # Sampling loop for a batch of sequences\n    # (to simplify, here we assume a batch of size 1).\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n        output_tokens,  h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c]) # here giving the input to decoder model \n         # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = hindi_tokenizer.index_word[sampled_token_index]\n        decoded_sentence += ' '+sampled_char\n        # Exit condition: either hit max length\n        # or find stop character.\n        if (sampled_char == '_END' or\n           len(decoded_sentence) > 80):\n            stop_condition = True\n        #Update the target sequence (of length 1).\n        target_seq = np.zeros((1,1))\n        target_seq[0, 0] = sampled_token_index\n        # Update states\n        e_h, e_c = h, c\n    return(decoded_sentence)\n    ","159b2b64":"# encoder_model = Model(encoder_inputs, encoder_states)\n\n\n\n# ###----------------------------------------------------####\n# decoder_state_input_h1 = Input(shape=(latent_dim,))\n# decoder_state_input_c1 = Input(shape=(latent_dim,))\n# decoder_state_input_h2 = Input(shape=(latent_dim,))\n# decoder_state_input_c2 = Input(shape=(latent_dim,))\n# decoder_states_inputs = [decoder_state_input_h1, decoder_state_input_c1, \n#                          decoder_state_input_h2, decoder_state_input_c2]\n\n\n# dec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence\n\n\n# d_o, dec_h1, dec_c1 = out_layer1(\n#     dec_emb2, initial_state=decoder_states_inputs[:2])\n\n# d_o, dec_h2, dec_c2  = out_layer2(\n#     d_o, initial_state=decoder_states_inputs[-2:])\n\n# decoder_states = [dec_h1, dec_c1, dec_h2, dec_c2]\n# decoder_outputs = output_layer(d_o)\n\n# decoder_model = Model(\n#     [decoder_inputs] + decoder_states_inputs,\n#     [decoder_outputs] + decoder_states)\n\n# decoder_model.summary()","eb93f8b4":"# # now we need a function to provide all these inputs and outputs dynamically\n# def decode_sequence(input_seq):  # here input sequence will be our english sentence\n#     # Encode the input as state vectors.\n#     states_value = encoder_model.predict(input_seq)\n#     # Generate empty target sequence of length 1.\n#     target_seq = np.zeros((1,1))\n#     # Populate the first character of target sequence with the start character.\n#     target_seq[0, 0] = hindi_tokenizer.word_index['START_']\n#      # Sampling loop for a batch of sequences\n#     # (to simplify, here we assume a batch of size 1).\n#     stop_condition = False\n#     decoded_sentence = ''\n#     while not stop_condition:\n#         output_tokens,  h1, c1,h2,c2 = decoder_model.predict([target_seq] + states_value) # here giving the input to decoder model \n#          # Sample a token\n#         sampled_token_index = np.argmax(output_tokens[0, -1, :])\n#         sampled_char = hindi_tokenizer.index_word[sampled_token_index]\n#         decoded_sentence += ' '+sampled_char\n#         # Exit condition: either hit max length\n#         # or find stop character.\n#         if (sampled_char == '_END' or\n#            len(decoded_sentence) > 80):\n#             stop_condition = True\n#         #Update the target sequence (of length 1).\n#         target_seq = np.zeros((1,1))\n#         target_seq[0, 0] = sampled_token_index\n#         # Update states\n#         states_value = [h1, c1,h2,c2]\n        \n#     return(decoded_sentence)\n    ","0b30059d":"train_gen = generate_batch(X_train, y_train, batch_size = 1)\nk=-1","353d04e1":"k+=1\n(input_seq, actual_output), _ = next(train_gen)\ndecoded_sentence = decode_sequence(input_seq)\nprint('Input English sentence:', X_train[k:k+1].values[0])\nprint('Actual Hindi Translation:', y_train[k:k+1].values[0][6:-4])\nprint('Predicted Hindi Translation:', decoded_sentence[:-4])","6d07618b":"test_gen = generate_batch(X_test, y_test, batch_size = 1)\nk=-1","dea3da52":"k+=1\n(input_seq, actual_output), _ = next(test_gen)\ndecoded_sentence = decode_sequence(input_seq)\nprint('Input English sentence:', X_test[k:k+1].values[0])\nprint('Actual Hindi Translation:', y_test[k:k+1].values[0][6:-4])\nprint('Predicted Hindi Translation:', decoded_sentence[:-4])","f60db5e0":"### Generator to create our data for training ","648d5010":"### Creating Encoder-Decoder model with stacked lstm layers \nhere we are not going to use word_embedding but we can try to improve our performance \n\nHERE WE HAVE TWO METHODS FOR STACKING LSTM FOR ENCODER DECODER MODEL \n\nFIRST WAY","b9d572e7":"### Design Inference Phase","f5db209c":"### Testing of train set ","113f46d8":"### ITs time to preprocess our data","d3de9be5":"### Model with using attention layer and stacked encoder decoder model","c8527361":"### This inference model if for only stacked encoder decoder without attention layer ","13904bb0":"### Inference pahse for attention layers encoder decoder model "}}