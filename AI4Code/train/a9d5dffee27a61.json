{"cell_type":{"99e802f7":"code","08b8c62d":"code","bc9bfab1":"code","45e2a745":"code","227fd699":"code","80ce571d":"code","204f99b2":"code","c2e8d772":"code","cf61b554":"code","105e2ddb":"code","149e1e17":"code","211bea85":"code","9477034c":"code","660b8b54":"code","f250164e":"code","bba1f7e0":"code","281ce5dd":"code","ea798809":"code","b7aa7846":"code","f6f147a8":"code","0c09184c":"code","cc2436d9":"code","d3e8f4ba":"code","7828d9ed":"code","566bdcba":"code","7ae9399a":"code","a270b4c1":"code","d21b006c":"code","8335748a":"code","08ae1a56":"code","13960caf":"code","12167faf":"code","d5ebe442":"code","05a00674":"code","3ac6dd12":"code","b1de97d3":"code","2ead7bd7":"code","9e6cf4fd":"code","0f5fed34":"markdown","236f61e5":"markdown","c54be7cc":"markdown","175eb171":"markdown","53205f7b":"markdown","a56d27f3":"markdown","45e5c17f":"markdown","70aa138b":"markdown","4ccfaf06":"markdown","07ad30bc":"markdown","71e14faa":"markdown","d53b2029":"markdown","dc2237b9":"markdown","b80a0e27":"markdown","87fb0cc3":"markdown","b15cb60b":"markdown","bff723a4":"markdown","56edfbce":"markdown","2cb7a8bc":"markdown","030dfd28":"markdown","10f0b4cf":"markdown"},"source":{"99e802f7":"import pandas as pd \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns \nimport tensorflow as tf \n%matplotlib inline \n","08b8c62d":"train_df=pd.read_csv('..\/input\/titanic\/train.csv',index_col='PassengerId')\ntrain_df","bc9bfab1":"train_df['Ticket']=train_df['Ticket'].apply(lambda x : x.split(' ')[-1] if len(x.split(' ')) >=2 else x)\ntrain_df['Ticket']=train_df['Ticket'].apply(lambda x : x if x[0].isdigit() else np.NAN)","45e2a745":"train_df=train_df.drop(['Name'],axis=1)\n# train_df=train_df.drop(['Name','Ticket'],axis=1)\ntrain_df","227fd699":"train_df.info()","80ce571d":"train_df=train_df.drop('Cabin',axis=1)","204f99b2":"train_df.info()","c2e8d772":"train_df.duplicated().sum()\n","cf61b554":"train_df.drop_duplicates(inplace=True)\ntrain_df.info()","105e2ddb":"from sklearn.impute import SimpleImputer\n\nnumirical_imputer=SimpleImputer(strategy='median')\ncategorical_imputer=SimpleImputer(strategy='most_frequent')\ntrain_df[['Age','Parch','Ticket']]=numirical_imputer.fit_transform(train_df[['Age','Parch','Ticket']])\ntrain_df[['Embarked','Sex','Fare']]=categorical_imputer.fit_transform(train_df[['Embarked','Sex','Fare']])\n# train_df['Age'].fillna(train_df['Age'].mean(),inplace=True)\n# mode_of_embardked=train_df['Embarked'].mode()[0]\n# print(mode_of_embardked)\n# train_df['Embarked'].fillna(mode_of_embardked,inplace=True)\ntrain_df.info()","149e1e17":"train_df","211bea85":"train_df.groupby('Embarked')['Survived'].value_counts().plot(kind='bar')","9477034c":"train_df.groupby('Pclass')['Survived'].value_counts().plot(kind='bar')","660b8b54":"train_df.groupby('Sex')['Survived'].value_counts().plot(kind='bar')","f250164e":"sns.scatterplot(x=train_df.Age,y=train_df.Survived)","bba1f7e0":"old_people=train_df[train_df['Age']>=train_df['Age'].mean()]\nyoung_people=train_df[train_df['Age']<train_df['Age'].mean()]\nold_people['Survived'].value_counts().plot(kind='bar')\n# plt.pie(x=old_people['Survived'].value_counts(),autopct='%1.0f%%',labels=['Die','Survived'])","281ce5dd":"young_people['Survived'].value_counts().plot(kind='bar')\n# plt.pie(x=young_people['Survived'].value_counts(),autopct='%1.0f%%',labels=['Die','Survived'])","ea798809":"sns.heatmap(train_df.corr(),annot=True)","b7aa7846":"train_df['IsAlone']=train_df['SibSp'] + train_df['Parch']\ntrain_df['IsAlone']=train_df['IsAlone'].apply(lambda x: 0 if x >0 else 1)\ntrain_df.groupby('IsAlone')['Survived'].value_counts()[1].plot(kind='bar')\nplt.title('Alone')\nplt.show()\ntrain_df.groupby('IsAlone')['Survived'].value_counts()[0].plot(kind='bar')\nplt.title('With Family')","f6f147a8":"continuos=['Age','SibSp','Parch','Fare','Ticket']\ntrain_df[continuos].plot(kind='box')","0c09184c":"for col in ['Ticket','Fare'] :\n    Q1 = train_df[col].quantile(0.25)\n    Q3 = train_df[col].quantile(0.75)\n    IQR = Q3-Q1\n    train_df=train_df[(Q1-1.5*IQR <= train_df[col]) & (train_df[col] <= Q3+1.5*IQR)]\ntrain_df[continuos].plot(kind='box')","cc2436d9":"x=train_df.drop('Survived',axis=1)\ny=train_df['Survived']","d3e8f4ba":"x.hist()","7828d9ed":"# x['SibSp']=np.log1p(x['SibSp'])\n# x['Fare']=np.log1p(x['Fare'])\n# x['Ticket']=np.log1p(x['Ticket'])\nx.hist()","566bdcba":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\ncol_to_standard=['Age','Fare','SibSp','Ticket']\nx[col_to_standard]=scaler.fit_transform(x[col_to_standard])\n# x=pd.DataFrame(scaler.fit_transform(x),index=x.index,columns=x.columns)\nx","7ae9399a":"x=pd.get_dummies(x)\nx","a270b4c1":"train_df['Survived'].value_counts().plot(kind='bar')","d21b006c":"# Ratio between target values\nsurvive_to_die_rario=train_df['Survived'].value_counts()[1]\/train_df['Survived'].value_counts()[0]\nsurvive_to_die_rario","8335748a":"from imblearn.over_sampling import SMOTE\n# Resampling the minority class. The strategy can be changed as required.\nsm = SMOTE(sampling_strategy='minority', random_state=42)\n# Fit the model to generate the data.\noversampled_X, oversampled_Y = sm.fit_resample(x, y)\ntrain_df = pd.concat([pd.DataFrame(oversampled_Y), pd.DataFrame(oversampled_X)], axis=1)\nx= oversampled_X\ny= oversampled_Y\noversampled_X","08ae1a56":"# Ratio between target values\nsurvive_to_die_rario=train_df['Survived'].value_counts()[1]\/train_df['Survived'].value_counts()[0]\nsurvive_to_die_rario","13960caf":"from sklearn.model_selection import train_test_split\nx_train,x_valid,y_train,y_valid = train_test_split(x,y,test_size=0.2)\n","12167faf":"from xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score,f1_score\nfrom sklearn.model_selection import cross_val_score,GridSearchCV\nparameters={'n_estimators':[200],'learning_rate':[0.1],'max_depth':[6],'reg_lambda':[5,10,20,50,100]}\nmodel=XGBClassifier()\ngrid_search=GridSearchCV(model,parameters)\ngrid_search.fit(x_train,y_train)\nprint(grid_search.best_estimator_)\n# model.fit(x_train,y_train)\n# y_pred=model.predict(x_valid)\n# scores=cross_val_score(grid_search,x_valid,y_valid,cv=5)\ny_pred=grid_search.predict(x_valid)\n\n# print(scores.mean())\n\nprint(f'Train Accuracy = {accuracy_score(y_train,grid_search.predict(x_train))}')\nprint(f'Train F1 Score = {f1_score(y_train,grid_search.predict(x_train))}')\n\nprint(f'Valid Accuracy = {accuracy_score(y_valid,y_pred)}')\nprint(f'Valid F1 Score = {f1_score(y_valid,y_pred)}')","d5ebe442":"from tensorflow.keras import layers,models,optimizers,losses\n\nnn_model =models.Sequential([\n                   layers.Dense(units=512,activation='relu'),\n                   layers.Dropout(0.2),\n                   layers.Dense(units=64,activation='relu'),\n                   layers.Dropout(0.5),\n                   layers.Dense(units=64,activation='leaky_relu'),\n                   layers.Dropout(0.2),\n                  #  layers.Dense(units=32,activation='leaky_relu'),\n                   layers.Dense(units=1,activation='sigmoid'),\n])\nnn_model.compile(optimizer=optimizers.Adam(learning_rate=0.0005),loss=losses.BinaryCrossentropy(from_logits=True),metrics=['accuracy'])\n# callback = tf.keras.callbacks.EarlyStopping(monitor='loss', baseline=1)\nnn_model.fit (x_train,y_train,epochs=200)\ny_pred_train=nn_model.predict(x_train)\ny_pred_train=np.apply_along_axis(lambda x : 1 if x >=0.5 else 0 ,axis=1,arr=y_pred_train)\n\ny_pred_valid=nn_model.predict(x_valid)\ny_pred_valid=np.apply_along_axis(lambda x : 1 if x >=0.5 else 0 ,axis=1,arr=y_pred_valid)\n\nprint(f'Train Accuracy = {accuracy_score(y_train,y_pred_train)}')\nprint(f'Train F1 Score = {f1_score(y_train,y_pred_train)}')\n\nprint(f'Valid Accuracy = {accuracy_score(y_valid,y_pred_valid)}')\nprint(f'Valid F1 Score = {f1_score(y_valid,y_pred_valid)}')","05a00674":"from sklearn import svm \nsvm_model=svm.NuSVC()\nsvm_model.fit(x_train,y_train)\n\ny_pred=svm_model.predict(x_valid)\n\nprint(f'Train Accuracy = {accuracy_score(y_train,svm_model.predict(x_train))}')\nprint(f'Train F1 Score = {f1_score(y_train,svm_model.predict(x_train))}')\n\nprint(f'Valid Accuracy = {accuracy_score(y_valid,y_pred)}')\nprint(f'Valid F1 Score = {f1_score(y_valid,y_pred)}')","3ac6dd12":"test_df=pd.read_csv('..\/input\/titanic\/test.csv',index_col='PassengerId')\n# test_df=test_df.drop(['Name','Ticket'],axis=1)\ntest_df=test_df.drop(['Name'],axis=1)\ntest_df=test_df.drop('Cabin',axis=1)\ntest_df['Ticket']=test_df['Ticket'].apply(lambda x : x.split(' ')[-1] if len(x.split(' ')) >=2 else x)\ntest_df['Ticket']=test_df['Ticket'].apply(lambda x : x if x[0].isdigit() else np.NAN)\n\ntest_df['IsAlone']=test_df['SibSp'] + test_df['Parch']\ntest_df['IsAlone']=test_df['IsAlone'].apply(lambda x: 0 if x >0 else 1)\n\ntest_df[['Age','Parch','Ticket']]=numirical_imputer.transform(test_df[['Age','Parch','Ticket']])\ntest_df[['Embarked','Sex','Fare']]=categorical_imputer.transform(test_df[['Embarked','Sex','Fare']])\n# test_df['SibSp']=np.log1p(test_df['SibSp'])\n# test_df['Fare']=np.log1p(test_df['Fare'])\n# test_df['Ticket']=np.log1p(test_df['Ticket'])\n\ncol_to_standard=['Age','Fare','SibSp','Ticket']\ntest_df[col_to_standard]=scaler.transform(test_df[col_to_standard])\n\ntest_df=pd.get_dummies(test_df)\ntest_df","b1de97d3":"test_df.info()","2ead7bd7":"test_preds_xbg=grid_search.predict(test_df)\ntest_preds_svm=svm_model.predict(test_df)\ny_test=nn_model.predict(test_df)\ny_test=np.apply_along_axis(lambda x : 1 if x >=0.5 else 0 ,axis=1,arr=y_test)\ntest_preds_nn=y_test","9e6cf4fd":"output_xgb = pd.DataFrame({'PassengerId': test_df.index,\n                       'Survived': test_preds_xbg})\noutput_xgb.to_csv('submission_xgb4.0.csv', index=False)\n\noutput_nn = pd.DataFrame({'PassengerId': test_df.index,\n                       'Survived': test_preds_svm})\noutput_nn.to_csv('submission_svm3.0.csv', index=False)\n\noutput_nn = pd.DataFrame({'PassengerId': test_df.index,\n                       'Survived': test_preds_nn})\noutput_nn.to_csv('submission_nn3.0.csv', index=False)","0f5fed34":"Remove outliers","236f61e5":"trying resampling with SMOTE","c54be7cc":"<h1>Dealing with missing values\nAge column will fill with its mean and Embarked column will fill with its mode value","175eb171":"Standardization","53205f7b":"There is 204 available information from Cabin column and about 700 null values ,so its exisance is unnessecary ","a56d27f3":"<h1>Normalization","45e5c17f":"From the result above it is balanced dataset because the ratio is more than 0.5","70aa138b":"<h1>Check outliers","4ccfaf06":"<h1>Invistigate dataset","07ad30bc":"<h1>Check duplication","71e14faa":"<h1>ML \/Models","d53b2029":"<h1>Dealing with categorical variables","dc2237b9":"Deep neural network","b80a0e27":"<h1>Features and target split","87fb0cc3":"#Check the balance of the dataset","b15cb60b":"<h1>Prepare test dataset","bff723a4":"Support Vector Machine","56edfbce":"Name ,Ticket columns are unnessecary ,so i will drop them","2cb7a8bc":"<h1>Feature Engineering","030dfd28":"Train validation split ","10f0b4cf":"<h1>Data visualization"}}