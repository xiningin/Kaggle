{"cell_type":{"0cde34fd":"code","d698719a":"code","3516f1e5":"code","490c34dc":"code","026612b9":"code","78c4277f":"code","f54ded85":"code","6318e154":"code","5f8bafc2":"code","dd10d000":"code","926a0dc8":"code","53686dee":"code","e4a81225":"code","18fef9f7":"code","15c6a347":"code","c05a66f3":"code","8668a1e5":"code","898c9b6d":"code","051d11eb":"code","97f7a1ed":"code","8676caf3":"code","e0fc2044":"code","87c707b9":"markdown","11435746":"markdown","6902b68e":"markdown","9bb95726":"markdown","7d4610a6":"markdown","fb6f05da":"markdown","b6c2de14":"markdown","47d74915":"markdown","d9868023":"markdown","0cf15a62":"markdown","71b33d75":"markdown","b83cfb5c":"markdown","73380f87":"markdown","0d0c1725":"markdown","42b9631a":"markdown","9ce4c38a":"markdown","a32eee19":"markdown"},"source":{"0cde34fd":"import os\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom tqdm.notebook import tqdm\nfrom collections import Counter\n\nwarnings.filterwarnings(\"ignore\")\nNUM_WORKERS = 4","d698719a":"DATA_PATH = \"\/kaggle\/input\/ventilator-pressure-prediction\/\"\n\nsub = pd.read_csv(DATA_PATH + 'sample_submission.csv')\ndf_train = pd.read_csv(DATA_PATH + 'train.csv')\ndf_test = pd.read_csv(DATA_PATH + 'test.csv')\n\n\ndf = df_train[df_train['breath_id'] < 5].reset_index(drop=True)","3516f1e5":"df.head()","490c34dc":"def plot_sample(sample_id, df):\n    df_breath = df[df['breath_id'] == sample_id]\n    r, c  = df_breath[['R', 'C']].values[0]\n\n    cols = ['u_in', 'u_out', 'pressure'] if 'pressure' in df.columns else ['u_in', 'u_out']\n    \n    plt.figure(figsize=(12, 4))\n    for col in ['u_in', 'u_out', 'pressure']:\n        plt.plot(df_breath['time_step'], df_breath[col], label=col)\n        \n    plt.legend()\n    plt.title(f'Sample {sample_id} - R={r}, C={c}')","026612b9":"for i in df['breath_id'].unique():\n    plot_sample(i, df_train)","78c4277f":"import torch\nfrom torch.utils.data import Dataset\n\nclass VentilatorDataset(Dataset):\n    def __init__(self, df):\n        if \"pressure\" not in df.columns:\n            df['pressure'] = 0\n\n        self.df = df.groupby('breath_id').agg(list).reset_index()\n        \n        self.prepare_data()\n                \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def prepare_data(self):\n        self.pressures = np.array(self.df['pressure'].values.tolist())\n        \n        rs = np.array(self.df['R'].values.tolist())\n        cs = np.array(self.df['C'].values.tolist())\n        u_ins = np.array(self.df['u_in'].values.tolist())\n        \n        self.u_outs = np.array(self.df['u_out'].values.tolist())\n        \n        self.inputs = np.concatenate([\n            rs[:, None], \n            cs[:, None], \n            u_ins[:, None], \n            np.cumsum(u_ins, 1)[:, None],\n            self.u_outs[:, None]\n        ], 1).transpose(0, 2, 1)\n\n    def __getitem__(self, idx):\n        data = {\n            \"input\": torch.tensor(self.inputs[idx], dtype=torch.float),\n            \"u_out\": torch.tensor(self.u_outs[idx], dtype=torch.float),\n            \"p\": torch.tensor(self.pressures[idx], dtype=torch.float),\n        }\n        \n        return data","f54ded85":"dataset = VentilatorDataset(df)\ndataset[0]","6318e154":"class Attention(torch.nn.Module):\n    def __init__(self, hidden_size, batch_first=False):\n        super(Attention, self).__init__()\n\n        self.hidden_size = hidden_size\n        self.batch_first = batch_first\n\n        self.att_weights = nn.Parameter(torch.Tensor(1, hidden_size), requires_grad=True)\n\n        stdv = 1.0 \/ np.sqrt(self.hidden_size)\n        for weight in self.att_weights:\n            nn.init.uniform_(weight, -stdv, stdv)\n\n    def get_mask(self):\n        pass\n\n    def forward(self, inputs, lengths):\n        if self.batch_first:\n            batch_size, max_len = inputs.size()[:2]\n        else:\n            max_len, batch_size = inputs.size()[:2]\n            \n        # apply attention layer\n        weights = torch.bmm(inputs,\n                            self.att_weights  # (1, hidden_size)\n                            .permute(1, 0)  # (hidden_size, 1)\n                            .unsqueeze(0)  # (1, hidden_size, 1)\n                            .repeat(batch_size, 1, 1) # (batch_size, hidden_size, 1)\n                            )\n    \n        attentions = torch.softmax(F.relu(weights.squeeze()), dim=-1)\n\n        # create mask based on the sentence lengths\n        mask = torch.ones(attentions.size(), requires_grad=True).cuda()\n        for i, l in enumerate(lengths):  # skip the first sentence\n            if l < max_len:\n                mask[i, l:] = 0\n\n        # apply mask and renormalize attention scores (weights)\n        masked = attentions * mask\n        _sums = masked.sum(-1).unsqueeze(-1)  # sums per row\n        \n        attentions = masked.div(_sums)\n\n        # apply attention weights\n        weighted = torch.mul(inputs, attentions.unsqueeze(-1).expand_as(inputs))\n\n        # get the final fixed vector representations of the sentences\n        representations = weighted.sum(1).squeeze()\n\n        return representations, attentions","5f8bafc2":"import torch\nimport torch.nn as nn\n\n\nclass RNNModel(nn.Module):\n    def __init__(\n        self,\n        input_dim=4,\n        lstm_dim=256,\n        dense_dim=256,\n        logit_dim=256,\n        num_classes=1,\n    ):\n        super().__init__()\n\n        self.mlp = nn.Sequential(\n            nn.Linear(input_dim, dense_dim \/\/ 2),\n            nn.ReLU(),\n            nn.Linear(dense_dim \/\/ 2, dense_dim),\n            nn.ReLU(),\n        )\n\n        self.gru = nn.LSTM(dense_dim, lstm_dim,dropout=0.2, batch_first=True, bidirectional=True)\n        self.gru = nn.LSTM(dense_dim, lstm_dim,dropout=0.2, batch_first=True, bidirectional=True)\n\n        self.logits = nn.Sequential(\n            nn.Linear(lstm_dim * 2, logit_dim),\n            nn.ReLU(),\n            nn.Linear(logit_dim, num_classes),\n        )\n\n    def forward(self, x):\n        features = self.mlp(x)\n        features, _ = self.gru(features)\n        pred = self.logits(features)\n        return pred","dd10d000":"import os\nimport torch\nimport random\nimport numpy as np\n\n\ndef seed_everything(seed):\n    \"\"\"\n    Seeds basic parameters for reproductibility of results.\n\n    Args:\n        seed (int): Number of the seed.\n    \"\"\"\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \n    \ndef count_parameters(model, all=False):\n    \"\"\"\n    Counts the parameters of a model.\n\n    Args:\n        model (torch model): Model to count the parameters of.\n        all (bool, optional):  Whether to count not trainable parameters. Defaults to False.\n\n    Returns:\n        int: Number of parameters.\n    \"\"\"\n    if all:\n        return sum(p.numel() for p in model.parameters())\n    else:\n        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    \ndef worker_init_fn(worker_id):\n    \"\"\"\n    Handles PyTorch x Numpy seeding issues.\n\n    Args:\n        worker_id (int): Id of the worker.\n    \"\"\"\n    np.random.seed(np.random.get_state()[1][0] + worker_id)\n    \n\ndef save_model_weights(model, filename, verbose=1, cp_folder=\"\"):\n    \"\"\"\n    Saves the weights of a PyTorch model.\n\n    Args:\n        model (torch model): Model to save the weights of.\n        filename (str): Name of the checkpoint.\n        verbose (int, optional): Whether to display infos. Defaults to 1.\n        cp_folder (str, optional): Folder to save to. Defaults to \"\".\n    \"\"\"\n    if verbose:\n        print(f\"\\n -> Saving weights to {os.path.join(cp_folder, filename)}\\n\")\n    torch.save(model.state_dict(), os.path.join(cp_folder, filename))","926a0dc8":"def compute_metric(df, preds):\n    \"\"\"\n    Metric for the problem, as I understood it.\n    \"\"\"\n    \n    y = np.array(df['pressure'].values.tolist())\n    w = 1 - np.array(df['u_out'].values.tolist())\n    \n    assert y.shape == preds.shape and w.shape == y.shape, (y.shape, preds.shape, w.shape)\n    \n    mae = w * np.abs(y - preds)\n    mae = mae.sum() \/ w.sum()\n    \n    return mae\n\n\nclass VentilatorLoss(nn.Module):\n    \"\"\"\n    Directly optimizes the competition metric\n    \"\"\"\n    def __call__(self, preds, y, u_out):\n        w = 1 - u_out\n        mae = w * (y - preds).abs()\n        mae = mae.sum(-1) \/ w.sum(-1)\n\n        return mae","53686dee":"import gc\nimport time\nimport torch\nimport numpy as np\nfrom torch.utils.data import DataLoader\nfrom transformers import get_linear_schedule_with_warmup\n\n\ndef fit(\n    model,\n    train_dataset,\n    val_dataset,\n    loss_name=\"L1Loss\",\n    optimizer=\"Adam\",\n    epochs=50,\n    batch_size=32,\n    val_bs=32,\n    warmup_prop=0.1,\n    lr=1e-3,\n    num_classes=1,\n    verbose=1,\n    first_epoch_eval=0,\n    device=\"cuda\"\n):\n    avg_val_loss = 0.\n\n    # Optimizer\n    optimizer = getattr(torch.optim, optimizer)(model.parameters(), lr=lr)\n\n    # Data loaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True,\n        num_workers=NUM_WORKERS,\n        pin_memory=True,\n        worker_init_fn=worker_init_fn\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=val_bs,\n        shuffle=False,\n        num_workers=NUM_WORKERS,\n        pin_memory=True,\n    )\n\n    # Loss\n#     loss_fct = getattr(torch.nn, loss_name)(reduction=\"none\")\n    loss_fct = VentilatorLoss()\n\n    # Scheduler\n    num_warmup_steps = int(warmup_prop * epochs * len(train_loader))\n    num_training_steps = int(epochs * len(train_loader))\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps, num_training_steps\n    )\n\n    for epoch in range(epochs):\n        model.train()\n        model.zero_grad()\n        start_time = time.time()\n\n        avg_loss = 0\n        for data in train_loader:\n            pred = model(data['input'].to(device)).squeeze(-1)\n\n            loss = loss_fct(\n                pred,\n                data['p'].to(device),\n                data['u_out'].to(device),\n            ).mean()\n            loss.backward()\n            avg_loss += loss.item() \/ len(train_loader)\n\n            optimizer.step()\n            scheduler.step()\n\n            for param in model.parameters():\n                param.grad = None\n\n        model.eval()\n        mae, avg_val_loss = 0, 0\n        preds = []\n\n        with torch.no_grad():\n            for data in val_loader:\n                pred = model(data['input'].to(device)).squeeze(-1)\n\n                loss = loss_fct(\n                    pred.detach(), \n                    data['p'].to(device),\n                    data['u_out'].to(device),\n                ).mean()\n                avg_val_loss += loss.item() \/ len(val_loader)\n\n                preds.append(pred.detach().cpu().numpy())\n        \n        preds = np.concatenate(preds, 0)\n        mae = compute_metric(val_dataset.df, preds)\n\n        elapsed_time = time.time() - start_time\n        if (epoch + 1) % verbose == 0:\n            elapsed_time = elapsed_time * verbose\n            lr = scheduler.get_last_lr()[0]\n            print(\n                f\"Epoch {epoch + 1:02d}\/{epochs:02d} \\t lr={lr:.1e}\\t t={elapsed_time:.0f}s \\t\"\n                f\"loss={avg_loss:.3f}\",\n                end=\"\\t\",\n            )\n\n            if (epoch + 1 >= first_epoch_eval) or (epoch + 1 == epochs):\n                print(f\"val_loss={avg_val_loss:.3f}\\tmae={mae:.3f}\")\n            else:\n                print(\"\")\n\n    del (val_loader, train_loader, loss, data, pred)\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    return preds\n","e4a81225":"def predict(\n    model,\n    dataset,\n    batch_size=64,\n    device=\"cuda\"\n):\n    \"\"\"\n    Usual torch predict function. Supports sigmoid and softmax activations.\n    Args:\n        model (torch model): Model to predict with.\n        dataset (PathologyDataset): Dataset to predict on.\n        batch_size (int, optional): Batch size. Defaults to 64.\n        device (str, optional): Device for torch. Defaults to \"cuda\".\n\n    Returns:\n        numpy array [len(dataset) x num_classes]: Predictions.\n    \"\"\"\n    model.eval()\n\n    loader = DataLoader(\n        dataset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS\n    )\n    \n    preds = []\n    with torch.no_grad():\n        for data in loader:\n            pred = model(data['input'].to(device)).squeeze(-1)\n            preds.append(pred.detach().cpu().numpy())\n\n    preds = np.concatenate(preds, 0)\n    return preds","18fef9f7":"def train(config, df_train, df_val, df_test, fold):\n    \"\"\"\n    Trains and validate a model.\n\n    Args:\n        config (Config): Parameters.\n        df_train (pandas dataframe): Training metadata.\n        df_val (pandas dataframe): Validation metadata.\n        df_test (pandas dataframe): Test metadata.\n        fold (int): Selected fold.\n\n    Returns:\n        np array: Study validation predictions.\n    \"\"\"\n\n    seed_everything(config.seed)\n\n    model = RNNModel(\n        input_dim=config.input_dim,\n        lstm_dim=config.lstm_dim,\n        dense_dim=config.dense_dim,\n        logit_dim=config.logit_dim,\n        num_classes=config.num_classes,\n    ).to(config.device)\n    model.zero_grad()\n\n    train_dataset = VentilatorDataset(df_train)\n    val_dataset = VentilatorDataset(df_val)\n    test_dataset = VentilatorDataset(df_test)\n\n    n_parameters = count_parameters(model)\n\n    print(f\"    -> {len(train_dataset)} training breathes\")\n    print(f\"    -> {len(val_dataset)} validation breathes\")\n    print(f\"    -> {n_parameters} trainable parameters\\n\")\n\n    pred_val = fit(\n        model,\n        train_dataset,\n        val_dataset,\n        loss_name=config.loss,\n        optimizer=config.optimizer,\n        epochs=config.epochs,\n        batch_size=config.batch_size,\n        val_bs=config.val_bs,\n        lr=config.lr,\n        warmup_prop=config.warmup_prop,\n        verbose=config.verbose,\n        first_epoch_eval=config.first_epoch_eval,\n        device=config.device,\n    )\n    \n    pred_test = predict(\n        model, \n        test_dataset, \n        batch_size=config.val_bs, \n        device=config.device\n    )\n\n    if config.save_weights:\n        save_model_weights(\n            model,\n            f\"{config.selected_model}_{fold}.pt\",\n            cp_folder=\"\",\n        )\n\n    del (model, train_dataset, val_dataset, test_dataset)\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    return pred_val, pred_test","15c6a347":"from sklearn.model_selection import GroupKFold\n\ndef k_fold(config, df, df_test):\n    \"\"\"\n    Performs a patient grouped k-fold cross validation.\n    \"\"\"\n\n    pred_oof = np.zeros(len(df))\n    preds_test = []\n    \n    gkf = GroupKFold(n_splits=config.k)\n    splits = list(gkf.split(X=df, y=df, groups=df[\"breath_id\"]))\n\n    for i, (train_idx, val_idx) in enumerate(splits):\n        if i in config.selected_folds:\n            print(f\"\\n-------------   Fold {i + 1} \/ {config.k}  -------------\\n\")\n\n            df_train = df.iloc[train_idx].copy().reset_index(drop=True)\n            df_val = df.iloc[val_idx].copy().reset_index(drop=True)\n\n            pred_val, pred_test = train(config, df_train, df_val, df_test, i)\n            \n            pred_oof[val_idx] = pred_val.flatten()\n            preds_test.append(pred_test.flatten())\n\n    print(f'\\n -> CV MAE : {compute_metric(df, pred_oof) :.3f}')\n\n    return pred_oof, np.mean(preds_test, 0)","c05a66f3":"class Config:\n    \"\"\"\n    Parameters used for training\n    \"\"\"\n    # General\n    seed = 42\n    verbose = 1\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    save_weights = True\n\n    # k-fold\n    k = 5\n    selected_folds = [0, 1, 2, 3, 4]\n    \n    # Model\n    selected_model = 'rnn'\n    input_dim = 5\n\n    dense_dim = 512\n    lstm_dim = 512\n    logit_dim = 512\n    num_classes = 1\n\n    # Training\n    loss = \"L1Loss\"  # not used\n    optimizer = \"Adam\"\n    batch_size = 128\n    epochs = 200\n\n    lr = 1e-3\n    warmup_prop = 0\n\n    val_bs = 256\n    first_epoch_eval = 0","8668a1e5":"pred_oof, pred_test = k_fold(\n    Config, \n    df_train,\n    df_test,\n)","898c9b6d":"def plot_prediction(sample_id, df):\n    df_breath = df[df['breath_id'] == sample_id]\n\n    cols = ['u_in', 'u_out', 'pressure'] if 'pressure' in df.columns else ['u_in', 'u_out']\n    \n    plt.figure(figsize=(12, 4))\n    for col in ['pred', 'pressure', 'u_out']:\n        plt.plot(df_breath['time_step'], df_breath[col], label=col)\n        \n    metric = compute_metric(df_breath, df_breath['pred'])\n        \n    plt.legend()\n    plt.title(f'Sample {sample_id} - MAE={metric:.3f}')","051d11eb":"df_train[\"pred\"] = pred_oof","97f7a1ed":"for i in df_train['breath_id'].unique()[:5]:\n    plot_prediction(i, df_train)","8676caf3":"df_test['pred'] = pred_test\n\nfor i in df_test['breath_id'].unique()[:5]:\n    plot_prediction(i, df_test)","e0fc2044":"sub['pressure'] = pred_test\nsub.to_csv('submission.csv', index=False)","87c707b9":"### Predictions","11435746":"### Metric & Loss\n> The competition will be scored as the mean absolute error between the predicted and actual pressures during the inspiratory phase of each breath. The expiratory phase is not scored.","6902b68e":"### Load","9bb95726":"### $k$-fold","7d4610a6":"### Fit","fb6f05da":"## Model\n- 2 Layer MLP\n- Bidirectional LSTM\n- Prediction dense layer","b6c2de14":"## Data","47d74915":"## Main","d9868023":"## Sub","0cf15a62":"# Deep Learning Starter : Simple LSTM\n\nThis notebook leverages the time series structure of the data.\n\nI expect sequential Deep Learning models to dominate in this competition, so here's a simple LSTM architecture.\n\nParameters were not really tweaked so the baseline is easily improvable.\n\nCode is taken from previous work, some functions are documented but the doc may be outdated.\n\n\n**Don't fork without upvoting ^^**","71b33d75":"**Thanks for reading !**","b83cfb5c":"## Train","73380f87":"### Predict","0d0c1725":"### Viz","42b9631a":"### Utils","9ce4c38a":"## Training","a32eee19":"### Dataset"}}