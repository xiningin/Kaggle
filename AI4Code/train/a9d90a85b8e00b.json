{"cell_type":{"56f9d789":"code","d885f5b1":"code","89ac9237":"code","18c32655":"code","abd93a76":"code","98a529b4":"code","201c7000":"code","94d18e8a":"code","10b637aa":"code","6a8eacf7":"code","4399b75d":"code","7788205c":"code","5921f047":"code","4250315b":"code","34f557ae":"code","185758be":"code","359d5880":"code","dcbe46ff":"code","c944174c":"code","00392ae4":"code","d10857d1":"markdown","835efade":"markdown","fc6e7926":"markdown","22c7782d":"markdown","4ad51ea5":"markdown","717bfdfe":"markdown","7cdf1953":"markdown","56ad052c":"markdown","9f9bda94":"markdown","793dead1":"markdown","857c4eb3":"markdown","9590e5ae":"markdown","031c6532":"markdown","7e07d74c":"markdown"},"source":{"56f9d789":"import torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom torchvision.datasets import MNIST\nimport torchvision.transforms as T\nimport matplotlib.pyplot as plt\nfrom IPython import display\nfrom tqdm import tqdm","d885f5b1":"plt.rcParams['figure.figsize'] = (11,11)\nplt.rcParams['image.cmap'] = 'gray'","89ac9237":"# Dataset\nmean = 0.1307\nstd = 0.3081\ndataset = MNIST('data', transform=T.Compose([T.ToTensor(), T.Normalize((mean,), (std,))]), download=True)","18c32655":"from copy import deepcopy\n\nclass Scaled_Act(nn.Module):\n    to_str = {'Sigmoid' : 'sigmoid', 'ReLU': 'relu', 'Tanh' : 'tanh', 'LeakyReLU': 'leaky_relu'}\n    def __init__(self, act, scale = None):\n        super().__init__()\n        self.act = act\n        act_name = Scaled_Act.to_str.get(act._get_name(), act._get_name())\n        param = getattr(act, 'negative_slope', None)\n        self.scale = scale if scale else torch.nn.init.calculate_gain(act_name, param)\n\n    def forward(self, input):\n        return self.scale*self.act(input)\n\nclass Equal_LR:\n    def __init__(self, name):\n        self.name = name\n\n    def compute_norm(module, weight):\n        mode = 'fan_in'\n        if hasattr(module, 'transposed') and module.transposed:\n            mode = 'fan_out'\n        return torch.nn.init._calculate_correct_fan(weight, mode)\n\n\n    def scale_weight(self, module, input):\n        setattr(module, self.name, module.scale*module.weight_orig)\n\n\n    def fn(self, module):\n        try:\n            weight = getattr(module, self.name)\n            module.scale = 1\/np.sqrt(Equal_LR.compute_norm(module, weight))\n            if isinstance(weight, torch.nn.Parameter):\n                # register new parameter -- unscaled weight\n                module.weight_orig = nn.Parameter(weight.clone()\/module.scale)\n                # delete old parameter\n                del module._parameters[self.name]\n            else:\n                # register new buffer -- unscaled weight\n                module.register_buffer('weight_orig', weight.clone()\/module.scale)\n                # delete old buffer\n                del module._buffers[self.name]\n            module.equalize = module.register_forward_pre_hook(self.scale_weight)\n        except:\n            pass\n\n    def __call__(self, module):\n        new_module = deepcopy(module)\n        new_module.apply(self.fn)\n        return new_module\n\n\n\ndef parameters_to_buffers(m):\n    params = m._parameters.copy()\n    m._parameters.clear()\n    for n,p in params.items():\n        m.register_buffer(n, p.data)\n\n\n\ndef grid(array, ncols=8):\n    array = np.pad(array, [(0,0),(1,1),(1,1),(0,0)], 'constant')\n    nindex, height, width, intensity = array.shape\n    ncols = min(nindex, ncols)\n    nrows = (nindex+ncols-1)\/\/ncols\n    r = nrows*ncols - nindex # remainder\n    # want result.shape = (height*nrows, width*ncols, intensity)\n    arr = np.concatenate([array]+[np.zeros([1,height,width,intensity])]*r)\n    result = (arr.reshape(nrows, ncols, height, width, intensity)\n              .swapaxes(1,2)\n              .reshape(height*nrows, width*ncols, intensity))\n    return np.pad(result, [(1,1),(1,1),(0,0)], 'constant')\n\n\n\nclass NextDataLoader(torch.utils.data.DataLoader):\n    def __next__(self):\n        try:\n            return next(self.iterator)\n        except:\n            self.iterator = self.__iter__()\n            return next(self.iterator)\n\n\n\ndef to_tensor(obj, device='cuda'):\n    if obj.shape[-1] != 3 and obj.shape[-1] != 1:\n        obj = np.expand_dims(obj,-1)\n    if obj.ndim < 4:\n        obj = np.expand_dims(obj,0)\n    t = torch.tensor(np.moveaxis(obj,-1,-3), dtype=torch.float, device=device)\n    return t\n\n\ndef to_img(obj):\n    array = np.moveaxis(obj.data.cpu().numpy(),-3,-1)\n    return array","abd93a76":"class Modulated_Conv2d(nn.Conv2d):\n    def __init__(self, in_channels, out_channels, kernel_size, latent_size,\n                 demodulate=True, bias=True, stride=1, padding=0, dilation=1, **kwargs):\n        super().__init__(in_channels, out_channels, kernel_size, stride,\n                         padding, dilation, groups=1,\n                         bias=bias, padding_mode='zeros')\n        self.demodulate = demodulate\n        # style mapping\n        self.style = nn.Linear(latent_size, in_channels)\n        # required shape might be different in transposed conv\n        self.s_broadcast_view = (-1,1,self.in_channels,1,1)\n        self.in_channels_dim = 2\n\n\n    def convolve(self,x,w,groups):\n        # bias would be added later\n        return F.conv2d(x, w, None, self.stride, self.padding, self.dilation, groups=groups)\n\n\n    def forward(self, x, v):\n        N, in_channels, H, W = x.shape\n\n        # new minibatch dim: (ch dims, K, K) -> (1, ch dims, K, K)\n        w = self.weight.unsqueeze(0)\n\n        # compute styles: (N, C_in)\n        s = self.style(v) + 1\n\n        # modulate: (N, ch dims, K, K)\n        w = s.view(self.s_broadcast_view)*w\n\n        # demodulate\n        if self.demodulate:\n            sigma = torch.sqrt((w**2).sum(dim=[self.in_channels_dim,3,4],keepdim=True) + 1e-8)\n            w = w\/sigma\n\n        # reshape x: (N, C_in, H, W) -> (1, N*C_in, H, W)\n        x = x.view(1, -1, H, W)\n\n        # reshape w: (N, C_out, C_in, K, K) -> (N*C_out, C_in, K, K) for common conv\n        #            (N, C_in, C_out, K, K) -> (N*C_in, C_out, K, K) for transposed conv\n        w = w.view(-1, w.shape[2], w.shape[3], w.shape[4])\n\n        # use groups so that each sample in minibatch has it's own conv,\n        # conv weights are concatenated along dim=0\n        out = self.convolve(x,w,N)\n\n        # reshape back to minibatch.\n        out = out.view(N,-1,out.shape[2],out.shape[3])\n\n        # add bias\n        if not self.bias is None:\n            out += self.bias.view(1, self.bias.shape[0], 1, 1)\n\n        return out\n\n\nclass Up_Mod_Conv(Modulated_Conv2d):\n    def __init__(self, in_channels, out_channels, kernel_size, latent_size,\n                 demodulate=True, bias=True, factor=2):\n        assert (kernel_size % 2 == 1)\n        padding = (max(kernel_size-factor,0)+1)\/\/2\n        super().__init__(in_channels, out_channels, kernel_size, latent_size, demodulate, bias,\n                         stride=factor, padding=padding)\n        self.output_padding = torch.nn.modules.utils._pair(2*padding - kernel_size + factor)\n        # transpose as expected in F.conv_transpose2d\n        self.weight = nn.Parameter(self.weight.transpose(0,1).contiguous())\n        self.transposed = True\n        # taking into account transposition\n        self.s_broadcast_view = (-1,self.in_channels,1,1,1)\n        self.in_channels_dim = 1\n\n    def convolve(self, x, w, groups):\n        return F.conv_transpose2d(x, w, None, self.stride, self.padding, self.output_padding, groups, self.dilation)\n\n\nclass Down_Mod_Conv(Modulated_Conv2d):\n    def __init__(self, in_channels, out_channels, kernel_size, latent_size,\n                 demodulate=True, bias=True, factor=2):\n        assert (kernel_size % 2 == 1)\n        padding = kernel_size\/\/2\n        super().__init__(in_channels, out_channels, kernel_size, latent_size, demodulate, bias,\n                         stride=factor, padding=padding)\n\n    def convolve(self, x, w, groups):\n        return F.conv2d(x, w, None, self.stride, self.padding, self.dilation, groups=groups)\n\n\nclass Down_Conv2d(nn.Conv2d):\n    def __init__(self, in_channels, out_channels, kernel_size,\n                 bias=True, factor=2):\n        assert (kernel_size % 2 == 1)\n        padding = kernel_size\/\/2\n        super().__init__(in_channels, out_channels, kernel_size, factor, padding, bias=True)\n\n    def convolve(self, x):\n        return F.conv2d(x, w, None, self.stride, self.padding, self.dilation, self.groups)\n\n\nclass Noise(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.noise_strength = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x, input_noise=None):\n        if input_noise is None:\n            input_noise = torch.randn(x.shape[0],1,x.shape[2],x.shape[3], device=x.device)\n        noise = self.noise_strength*input_noise\n        return x + noise\n\nclass Mapping(nn.Module):\n    def __init__(self, n_layers, latent_size, nonlinearity, normalize=True):\n        super().__init__()\n        self.normalize = normalize\n        self.layers = []\n        for idx in range(n_layers):\n            layer = nn.Linear(latent_size, latent_size)\n            self.add_module(str(idx), layer)\n            self.layers.append(layer)\n            self.layers.append(nonlinearity)\n\n    def forward(self, input):\n        if self.normalize:\n            input = input\/torch.sqrt((input**2).mean(dim=1, keepdim=True) + 1e-8)\n        for module in self.layers:\n            input = module(input)\n        return input\n\n\nclass G_Block(nn.Module):\n    def __init__(self, in_fmaps, out_fmaps, kernel_size, latent_size, nonlinearity, factor=2, img_channels=3):\n        super().__init__()\n        inter_fmaps = (in_fmaps + out_fmaps)\/\/2\n        self.upconv = Up_Mod_Conv(in_fmaps, inter_fmaps, kernel_size, latent_size,\n                                      factor=factor)\n        self.conv = Modulated_Conv2d(inter_fmaps, out_fmaps, kernel_size, latent_size,\n                                     padding=kernel_size\/\/2)\n        self.noise = Noise()\n        self.noise2 = Noise()\n        self.to_channels = Modulated_Conv2d(out_fmaps, img_channels, kernel_size=1,\n                                      latent_size=latent_size, demodulate = False)\n        self.upsample = nn.Upsample(scale_factor=factor, mode='bilinear', align_corners=False)\n        self.act = nonlinearity\n\n    def forward(self, x, v, y=None, input_noises=None):\n        x = self.noise(self.upconv(x,v), None if (input_noises is None) else input_noises[:,0])\n        x = self.act(x)\n        x = self.noise2(self.conv(x,v), None if (input_noises is None) else input_noises[:,1])\n        x = self.act(x)\n        if not y is None:\n            y = self.upsample(y)\n        else:\n            y = 0\n        y = y + self.to_channels(x,v)\n        return x, y\n\nclass D_Block(nn.Module):\n    def __init__(self, in_fmaps, out_fmaps, kernel_size, nonlinearity, factor=2):\n        super().__init__()\n        inter_fmaps = (in_fmaps + out_fmaps)\/\/2\n        self.conv = nn.Conv2d(in_fmaps, inter_fmaps, kernel_size, padding=kernel_size\/\/2)\n        self.downconv = Down_Conv2d(inter_fmaps, out_fmaps, kernel_size, factor=factor)\n        self.skip = Down_Conv2d(in_fmaps, out_fmaps, kernel_size=1, factor=factor)\n        self.act = nonlinearity\n\n    def forward(self, x):\n        t = x\n        x = self.conv(x)\n        x = self.act(x)\n        x = self.downconv(x)\n        x = self.act(x)\n        t = self.skip(t)\n        return (x + t)\/ np.sqrt(2)\n\n\nclass Minibatch_Stddev(nn.Module):\n    def __init__(self, group_size=4):\n        super().__init__()\n        self.group_size = group_size\n\n    def forward(self, x):\n        s = x.shape\n        t = x.view(self.group_size, -1, s[1], s[2], s[3])\n        t = t - t.mean(dim=0, keepdim=True)\n        t = torch.sqrt((t**2).mean(dim=0) + 1e-8)\n        t = t.mean(dim=[1,2,3], keepdim=True) # [N\/G,1,1,1]\n        t = t.repeat(self.group_size,1,1,1).expand(x.shape[0],1,*x.shape[2:])\n        return torch.cat((x,t),dim=1)","98a529b4":"def G_logistic_ns(fake_logits):\n    return -F.logsigmoid(fake_logits).mean() # -log(D(G(z)))\n\n\ndef D_logistic(real_logits, fake_logits):\n    return torch.mean(-F.logsigmoid(real_logits) + F.softplus(fake_logits)) # -log(D(x)) - log(1-D(G(z)))\n\ndef R1_reg(real_imgs, real_logits):\n    grads = torch.autograd.grad(real_logits.sum(), real_imgs, create_graph=True)[0]\n    return torch.mean((grads**2).sum(dim=[1,2,3]))\n\nclass Path_length_loss(nn.Module):\n    def __init__(self, decay=0.01):\n        super().__init__()\n        self.decay = decay\n        self.avg = 0\n\n    def forward(self, dlatent, gen_out):\n        # Compute |J*y|.\n        noise = torch.randn(gen_out.shape, device=gen_out.device)\/np.sqrt(np.prod(gen_out.shape[2:])) #[N,Channels,H,W]\n        grads = torch.autograd.grad((gen_out * noise).sum(), dlatent, create_graph=True)[0]  #[N, num_layers, dlatent_size]\n        lengths = torch.sqrt((grads**2).mean(2).sum(1)) #[N]\n        # Update exp average. Lengths are detached\n        self.avg = self.decay*torch.mean(lengths.detach()) + (1-self.decay)*self.avg\n        return torch.mean((lengths - self.avg)**2)\n\n\ndef Noise_reg(noise_maps, min_res=8):\n    loss = 0\n    for nmap in noise_maps:\n        res = nmap.shape[-1]\n        while res > 8:\n            loss += ( torch.mean(nmap * nmap.roll(shifts=1, dims=-1), dim=[-1,-2])**2\n                    + torch.mean(nmap * nmap.roll(shifts=1, dims=-2), dim=[-1,-2])**2 ).sum()\n            nmap = F.avg_pool2d(nmap.squeeze(), 2)\n            res = res\/\/2\n    return loss","201c7000":"class Generator(nn.Module):\n    def __init__(self, min_res, max_res, min_fmaps, max_fmaps, act, \n                 k_size, blocks, img_channels, latent_size, n_layers, style_mixing_prob = 0.8,\n                 dlatent_avg_beta = 0.995, weights_avg_beta=0.99, **kwargs):\n        super().__init__()\n        dres = min_res*2**blocks - max_res\n        assert dres >= 0\n        # building mapping net\n        self.latent_size = latent_size\n        self.mapping = Mapping(n_layers, latent_size, act)\n        # learnable const\n        self.const = nn.Parameter(torch.randn(max_fmaps, min_res, min_res))\n        # building main layers\n        fmaps = np.linspace(max_fmaps, min_fmaps, blocks+1).astype('int')\n        self.layers = []\n        for i in range(blocks):\n            layer = G_Block(fmaps[i],fmaps[i+1], k_size, latent_size, act, img_channels=img_channels)\n            self.add_module(str(i), layer)\n            self.layers.append(layer)\n        if dres > 0:\n            self.crop = torch.nn.ZeroPad2d(-dres\/\/2)\n        # style mixing\n        self.style_mixing_prob = style_mixing_prob \n        # running average of dlatents \n        self.dlatent_avg_beta = dlatent_avg_beta\n        self.register_buffer('dlatent_avg', torch.zeros(latent_size))\n        # running average of weights\n        self.weights_avg_beta = weights_avg_beta\n        self.Src_Net = deepcopy(self).apply(parameters_to_buffers)\n        self.Src_Net.train(False)      \n        \n        \n    # update running average of weights\n    def update_avg_weights(self):\n        params = dict(self.named_parameters())\n        buffers = dict(self.named_buffers())\n        for n,b in self.Src_Net.named_buffers():\n            try:\n                b.data.copy_(self.weights_avg_beta*b + (1-self.weights_avg_beta)*params[n])\n            except:\n                b.data.copy_(buffers[n])\n                \n    def load_avg_weights(self):\n        buffers = dict(self.Src_Net.named_buffers())\n        for n,p in self.named_parameters():\n            p.data.copy_(buffers[n])\n            \n            \n    # sample dlatents\n    def sample_dlatents(self, n):\n        v = self._sample_dlatents(n)\n        if self.training and self.style_mixing_prob > 0:\n            v = self._bcast_dlatents(v)\n            l = len(self.layers)\n            cut_off = torch.randint(l-1,())\n            v2 = self._bcast_dlatents(self._sample_dlatents(n))\n            mask = torch.empty(n, dtype=torch.bool).bernoulli_(self.style_mixing_prob).view(-1, 1) \\\n                   * (torch.arange(l)>cut_off)\n            v = torch.where(mask.unsqueeze(-1).to(device=v.device), v2, v)\n        return v\n    \n    def _sample_dlatents(self, n):\n        device = self.const.device\n        z = torch.randn(n, self.latent_size).to(device)\n        v = self.mapping(z)\n        # update dlatent average\n        if self.training:\n            self.dlatent_avg = self.dlatent_avg_beta*self.dlatent_avg + (1-self.dlatent_avg_beta)*v.data.mean(0)\n        return v\n    \n    def _bcast_dlatents(self, v):\n        # broadcast dlatents [N, dlatent_size] --> [N, num_layers, dlatent_size] \n        return v.unsqueeze(1).expand(-1, len(self.layers), -1)\n    \n    \n    # generate from dlatents and input noises (optionally)\n    def generate(self, v, input_noises=None):\n        x = self.const.expand(v.shape[0], *self.const.shape).contiguous()\n        input_noises = input_noises if input_noises else [None]*len(self.layers)\n        y = None\n        if v.ndim < 3:\n            v = self._bcast_dlatents(v)\n        for i,layer in enumerate(self.layers):\n            x, y = layer(x,v[:,i],y, input_noises[i])  \n        if hasattr(self, 'crop'):\n            y = self.crop(y)\n        return y\n    \n    \n    # for training\n    def sample(self, n):\n        dlatents = self.sample_dlatents(n)\n        x = self.generate(dlatents)\n        return x\n            \n    \n    # for evaluation\n    def sample_images(self,n, truncation_psi=1):\n        with torch.no_grad():\n            v = self.Src_Net.sample_dlatents(n)\n            # truncation trick\n            if truncation_psi < 1:\n                v = self.dlatent_avg + truncation_psi*(v-self.dlatent_avg)\n            images = to_img(self.Src_Net.generate(v))\n        return images","94d18e8a":"class Discriminator(nn.Module):\n    def __init__(self, min_res, max_res, min_fmaps, max_fmaps, act, \n                 k_size, blocks, img_channels, dense_size=128, **kwargs):\n        super().__init__()\n        assert max_res <= min_res*2**blocks and max_res >= (min_res-1)*2**blocks\n        # building layers\n        fmaps = np.linspace(min_fmaps, max_fmaps, blocks+1).astype('int')\n        self.from_channels = nn.Conv2d(img_channels, fmaps[0], 1)\n        self.layers = []\n        for i in range(blocks):\n            layer = D_Block(fmaps[i],fmaps[i+1], k_size, act)\n            self.add_module(str(i), layer)\n            self.layers.append(layer)\n        self.minibatch_sttdev = Minibatch_Stddev()\n        self.conv = nn.Conv2d(fmaps[-1]+1,fmaps[-1], 3)\n        self.dense = nn.Linear(fmaps[-1]*(min_res-2)**2, dense_size)\n        self.output = nn.Linear(dense_size, 1)\n        self.act = act\n    \n    \n    def get_score(self, imgs):\n        x = self.act(self.from_channels(imgs))\n        for layer in self.layers:\n            x = layer(x)\n        x = self.minibatch_sttdev(x)\n        x = self.act(self.conv(x))\n        x = x.view(x.shape[0],-1)\n        x = self.act(self.dense(x))\n        x = self.output(x)\n        return x","10b637aa":"def train(G, D, dataset, max_iter, batch_size, \n          G_opt_args, D_opt_args, mapping_opt_args,\n          D_steps, pl_weight, r1_weight,\n          r1_interval, pl_interval, val_interval, num_workers, pl_batch_part, checkpoint=None):\n    \n    pl_batch = int(pl_batch_part*batch_size)\n    device = next(D.parameters()).device\n    Path_length_reg = Path_length_loss()\n\n    # create dataloader\n    dataloader = NextDataLoader(dataset, batch_size, num_workers=num_workers)\n    mean = dataset.transforms.transform.transforms[1].mean[0]\n    std = dataset.transforms.transform.transforms[1].std[0]\n    \n    # load state\n    if checkpoint:\n        G.load_state_dict(checkpoint['G'])\n        D.load_state_dict(checkpoint['D'])\n        Path_length_reg.avg = checkpoint['pl_loss_avg']\n    \n    # create optimizer\n    G_params = []\n    for n,m in G.named_children():\n        if n != 'mapping':\n            G_params.extend(m.parameters())\n    gen_optimizer = torch.optim.Adam([{'params': G_params},\n                                  {'params': G.mapping.parameters(), **mapping_opt_args},\n                                  {'params': G.const, **mapping_opt_args},\n                                  ], **G_opt_args)\n    disc_optimizer = torch.optim.Adam(D.parameters(), **D_opt_args)\n    \n    G.train()\n    D.train()\n    \n    for i in tqdm(range(max_iter)):\n        # discriminator update\n        for j in range(D_steps):\n            real_imgs = next(dataloader)[0].to(device)\n            real_imgs.requires_grad = True\n            fake_imgs = G.sample(real_imgs.shape[0])\n            real_scores = D.get_score(real_imgs)\n            fake_scores = D.get_score(fake_imgs)\n            loss =  D_logistic(real_scores, fake_scores)\n            if i % r1_interval == 0 and j == D_steps-1:\n                loss += r1_weight*r1_interval*R1_reg(real_imgs, real_scores)\n            real_imgs.requires_grad = False\n            disc_optimizer.zero_grad()\n            loss.backward()\n            disc_optimizer.step()\n\n\n        # generator update\n        dlatent = G.sample_dlatents(batch_size)   \n        if i % pl_interval == 0:\n            # hack to compute path length loss with smaller minibatch (for reducing memory consumption)\n            dlatent_part1, dlatent_part_2 = dlatent[:pl_batch], dlatent[pl_batch:]\n            fake_imgs = G.generate(torch.cat((dlatent_part1, dlatent_part_2), 0))\n            fake_scores = D.get_score(fake_imgs)\n            loss = G_logistic_ns(fake_scores) \\\n                   + pl_weight*pl_interval*Path_length_reg(dlatent_part1, fake_imgs[:pl_batch])\n        else:\n            fake_imgs = G.generate(dlatent)\n            fake_scores = D.get_score(fake_imgs)\n            loss = G_logistic_ns(fake_scores)  \n        gen_optimizer.zero_grad()\n        loss.backward()\n        gen_optimizer.step()   \n        # updating running average   \n        G.update_avg_weights()\n        \n        if i % val_interval == 0:\n            display.clear_output(wait=True)\n            # print pictures\n            gen = G.sample_images(32)*std+mean\n            plt.imshow(grid(gen).squeeze())\n            plt.show()\n            # print prob distribution\n            plt.figure(figsize=(5,5))\n            plt.title('Generated vs real data')\n            plt.hist(torch.sigmoid(real_scores.data).cpu().numpy(), label='D(x)', alpha=0.5,range=[0,1])\n            plt.hist(torch.sigmoid(fake_scores.data).cpu().numpy(), label='D(G(z))',alpha=0.5,range=[0,1])\n            plt.legend(loc='best')\n            plt.show()\n\n\n        if i % (20*val_interval) == 0:\n            torch.save({\n                        'G': G.state_dict(),\n                        'D': D.state_dict(),\n                        'pl_loss_avg': Path_length_reg.avg.item()\n                        }, 'checkpoint.pt')","6a8eacf7":"img_channels = 1\nn_layers = 4  # number of layers in mapping from latents to dlatents\nlatent_size = 160 # for simplicity dim of latent space = dim of dlatent space","4399b75d":"min_res = 4 # resolution from which the synthesis starts\nmax_res = 28 # out resolution \nblocks = 3 # number of building blocks for both the generator and dicriminator\nk_size = 3 # convolutions kernel size\nmax_fmaps = 128 # number of feature maps at the beginning of generation\nmin_fmaps = 64  # number of feature maps before going to the number of channels","7788205c":"weights_avg_beta=0.995 # beta for running average of generator weights\nact = Scaled_Act(nn.LeakyReLU(0.2)) # activation function","5921f047":"device = 'cuda'\n\ntrain_params = {\n    'max_iter': 10000, \n    'batch_size' : 16,\n    'G_opt_args' : {'lr' : 0.001, 'betas' : (0.1, 0.99)}, \n    'D_opt_args' : {'lr' : 0.001, 'betas' : (0, 0.99), 'eps' : 1e-08}, \n    'mapping_opt_args' : {'lr' : 1e-5}, \n    'D_steps': 1, \n    'pl_weight': 2, \n    'r1_weight': 8, \n    'pl_batch_part': 0.5,\n    'pl_interval': 4, \n    'r1_interval': 16, \n    'num_workers': 2, \n    'val_interval': 20 }","4250315b":"G = Generator(min_res, max_res, min_fmaps, max_fmaps, act, \n              k_size, blocks, img_channels, latent_size, n_layers, weights_avg_beta=weights_avg_beta).to(device)\n\nD = Discriminator(min_res, max_res, min_fmaps, max_fmaps, act, \n                   k_size, blocks, img_channels).to(device)","34f557ae":"G = Equal_LR('weight')(G)\nD = Equal_LR('weight')(D)","185758be":"def init_weights(m):\n    if hasattr(m, 'weight_orig'):\n        torch.nn.init.normal_(m.weight_orig)\n    if hasattr(m, 'bias'):\n        torch.nn.init.zeros_(m.bias)\n        \nG.apply(init_weights)\nD.apply(init_weights)","359d5880":"train(G, D, dataset, **train_params)","dcbe46ff":"checkpoint = torch.load('checkpoint.pt')\nG.load_state_dict(checkpoint['G'])\nG.load_avg_weights()\nG.eval();","c944174c":"plt.title('Generated')\nplt.imshow(grid(G.sample_images(32, truncation_psi=0.9)*std+mean).squeeze())\nplt.show()\n\nplt.title('Real data')\ni = np.random.randint(50000)\nreal_imgs = dataset.data[i:32+i].unsqueeze(-1)\nplt.imshow(grid(real_imgs).squeeze())\nplt.show()","00392ae4":"plt.title('Generated')\n#Compare affects of truncation value\nplt.imshow(grid(G.sample_images(128, truncation_psi=0.9)*std+mean, ncols=12).squeeze())\nplt.show()","d10857d1":"### Discriminator architecture","835efade":"# **End-to-End training of StyleGAN on MNIST Dataset**","fc6e7926":"#### Parameters for building models. ","22c7782d":"## Network functions","4ad51ea5":"#### Equalized learning rate","717bfdfe":"### Generator architecture","7cdf1953":"### Training","56ad052c":"##Utility Functions","9f9bda94":"## Loading dataset","793dead1":"### Hyperparams","857c4eb3":"### Evaluation","9590e5ae":"### Define training loop","031c6532":"#### Generated with truncation trick  $ \\Psi = 0.9 $ and using running average weights","7e07d74c":"#### Initialization of weights"}}