{"cell_type":{"86bda6da":"code","5cefcea6":"code","55b0dafc":"code","185398eb":"code","a21a3111":"code","34241b8b":"code","2785ca79":"code","946ce170":"code","e9d7ebe3":"code","cb509119":"code","e7eaa7e8":"code","bea98f3a":"code","e9dd18ae":"code","338e4b42":"markdown","5fa144ad":"markdown","bfeb429e":"markdown","b34145b1":"markdown","870ff420":"markdown","4c60e253":"markdown"},"source":{"86bda6da":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","5cefcea6":"data = pd.read_csv('..\/input\/ex2data1.txt')\ndata.head(5)","55b0dafc":"X = np.array(data.iloc[:, 0:2]) #for plotting data we need array instead of dataframe slice\ny = np.array(data.iloc[:, 2]) #for plotting data we need array instead of series slice\n#type(y)","185398eb":"def PlotData(X,y):\n    \"\"\"\n    Plots the data points X and y.\n    n_samples = 99 can varify from (X.shape, y.shape)\n    \"\"\"\n    pos = np.argwhere(y == 1)#it will return np.ndarray instead of tuple if we use where.\n    neg = np.argwhere(y == 0)#it will return np.ndarray instead of tuple if we use where.\n  \n    plt.plot(X[pos, 0], X[pos, 1], linestyle='', marker='+', color='k')\n    plt.plot(X[neg, 0], X[neg, 1], linestyle='', marker='o', color='y')","a21a3111":"plt.figure()\nPlotData(X, y)\n# Labels and Legend\nplt.xlabel('Exam 1 score')\nplt.ylabel('Exam 2 score')\nplt.legend(['Admitted', 'Not admitted'], loc='upper right', numpoints=1)\nplt.show()","34241b8b":"#Initialze training parameters\nm, n =  X.shape\n\n# Add intercept term\nX = np.hstack((np.ones((m, 1)), X))\n\n# Initialize fitting parameters\ntheta = np.zeros(n + 1)  ","2785ca79":"#Compute sigmoid function.\ndef sigmoid(z):\n    g = 1 \/ (1 + np.exp(-z))\n    return g   \n\n#evaluate sigmoid(0), it should give you 0.5\n#sigmoid(0)","946ce170":"def h(theta,X):\n    return sigmoid(np.dot(X,theta))","e9d7ebe3":"def costFunction(X,y,theta):\n    J = (1\/m) * np.sum((-y * np.log(h(theta,X))) - ((1-y)*np.log(1-h(theta,X))))\n    \n    #grad = (1\/m) * ((h(theta,X)) - y)*X.T\n    grad = (1\/m) * (h(theta,X) - y).T.dot(X)\n    \n    return (J, grad)","cb509119":"cost, grad = costFunction(X,y,theta)\nprint('cost for intial theta value', cost)\nprint('gradient for initial theta value ', grad)","e7eaa7e8":"print(data.shape, X.shape, y.shape, theta.shape)\ny = y.reshape(m,1)\ntheta = np.zeros((n+1,1))\n","bea98f3a":"print(data.shape, X.shape, y.shape, theta.shape)","e9dd18ae":"import scipy.optimize as opt\n\ntheta, nfeval, rc = opt.fmin_tnc(func=costFunction, x0=theta, args=(X, y))\n\ncost, grad = costFunction(X,y,theta)\nprint('cost for intial theta value', cost)\nprint('gradient for initial theta value ', grad)","338e4b42":"**1.2 Implementation**\n\n**1.2.1 Warmup exercise: sigmoid function**\n\nBefore we start with the actual cost function, recall that the logistic regression\nhypothesis is defined as: $$ h_\\theta(x) = g(\\theta^T) $$,\n\nwhere function $g$ is the sigmoid function. The sigmoid function is defined as: $$g(z) = \\frac{1}{1+e^{-z}}$$ \n\n**Note:** Your first step is to implement this function sigmoid so it can be called by the rest of your program. When you are finished, try testing a few values by calling sigmoid(x) in a new cell. For large positive values of x, the sigmoid should be close to 1, while for large negative values, the sigmoid should be close to 0. Evaluating sigmoid(0) should give you exactly 0.5. Your code should also work with vectors and matrices. For a matrix, your function should perform the sigmoid function on every element.","5fa144ad":"**Note:**  These exercise series are from coursera machine learning course. Please keep Prof. Andrew Ng's each video and lecture note side by side for better understanding about this kernel. ","bfeb429e":"**1.2.2 Cost function and gradient**\n\nLet\u2019s implement the cost function for the Logistic Regression.\n\nthe cost function in logistic regression is\n\n$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\left[ -y^{(i)} \\log\\left(h_\\theta\\left( x^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - h_\\theta\\left( x^{(i)} \\right) \\right) \\right]$$ \n\nand the gradient of the cost is a vector of the same length as $\\theta$ where the $j^{th}$ element (for $j = 0, 1, \\cdots , n$) is defined as follows:\n$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left( x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)} $$\n\nNote that while this gradient looks identical to the linear regression gradient, the formula is actually different because linear and logistic regression have different definitions of $h_\\theta(x)$\n\n\n","b34145b1":"**1 Logistic Regression**\n\nIn this part of the exercise, we will build a logistic regression model to predict whether a student gets admitted into a university.\nSuppose that you are the administrator of a university department and you want to determine each applicant\u2019s chance of admission based on their\nresults on two exams. \n\nWe have historical data from previous applicants that we can use as a training set for logistic regression. For each training\nexample, we have the applicant\u2019s scores on two exams and the admissions decision.\n\nOur task is to build a classification model that estimates an applicant\u2019s\nprobability of admission based the scores from those two exams. \n","870ff420":"This kernel is not finished yet. Thanks for stopping by.","4c60e253":"**1.1 Visualizing the data**\n\nBefore starting to implement any learning algorithm, it is always good to visualize the data if possible.\n"}}