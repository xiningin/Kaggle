{"cell_type":{"306d0b1f":"code","b697953f":"code","a989c794":"code","d16de308":"code","9555d640":"code","f5c2a0c0":"code","2f275e91":"code","0594567c":"markdown","aef02cd2":"markdown","118b478f":"markdown","b54c9c36":"markdown","7b310785":"markdown","750ed4b0":"markdown","8554d5c8":"markdown","e5df73b6":"markdown","b3fe401e":"markdown"},"source":{"306d0b1f":"# Import Libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings('ignore')","b697953f":"# Make Custom Dataset\nX, y = make_classification(n_samples=500, n_features=2, n_redundant = 0, random_state=10)\n\n# Plot the data\nsns.scatterplot(x = X[ : , 0], y = X[ : , 1], hue=y)\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.title(\"Custom Dataset\")\nplt.show()","a989c794":"# Euclidean distance\n'''\nWe will be using Euclidean Distance as a metric to compute the distance between test point and the data points in the training set.\n'''\ndef euclidean_distance(x1, x2):\n    return np.sqrt(np.sum((x1-x2)**2, axis=1))\n\n\n# Create a custom KNN Class\nclass KNN_Custom:\n    \n    # Constructor\n    def __init__(self, k = 5):\n        self.k = k\n    \n    # Fit function \n    '''\n    Takes in the training data\n    '''\n    def fit(self, X, y):\n        self.X_train = X\n        self.y_train = y\n        \n    # Make Predictions\n    '''\n    Generates predictions on the test data using the training data\n    '''\n    def predict(self, test_points):\n        \n        \n        # Compute the distance between x and all other data points\n        euc_distances = [euclidean_distance(test_point, self.X_train) for test_point in test_points]\n        \n        # Sort the K Nearest Neighbors\n        k_indices = [np.argsort(distances)[ : self.k] for distances in euc_distances]\n        k_labels = [[self.y_train[i] for i in indices] for indices in k_indices]\n        \n        # Prepare the output and return the predictions\n        final_predictions = []\n        for label in k_labels:\n            predicted_label = Counter(label).most_common(1)[0][0]\n            final_predictions.append(predicted_label)\n            \n        return np.array(final_predictions)","d16de308":"# Split the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n\n# Test the algorithm\nknn = KNN_Custom()\nknn.fit(X_train, y_train)\n\n# Make Predictions\nknn_predictions = knn.predict(X_test)\n\n# Compute Accuracy\nknn_acc = np.sum(knn_predictions == y_test)\/X_test.shape[0] * 100\nprint(\"Accuracy of the custom KNN classifier is {} %\".format(knn_acc))","9555d640":"# Let's visualise the working of the algorithm\ntest_point = np.array([3, -2]).reshape(1,2)\n\n# Clasify the test point\ntest_prediction = knn.predict(test_point)\nprint(\"Predicted Class: \", test_prediction[0])\n\n# Plot the test point along with the data\nsns.scatterplot(X[ : , 0], X[ : , 1], hue=y)\nsns.scatterplot(test_point[ : , 0], test_point[ : , 1], color='green', marker= 'X', s=200, label = 'Test Point')\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.title(\"Predicted Class: {}\".format(test_prediction[0]))\nplt.legend()\nplt.show()","f5c2a0c0":"# Import KNN from sklearn\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Create the object and fit the model\nknn_2 = KNeighborsClassifier(metric='euclidean')\nknn_2.fit(X_train, y_train)","2f275e91":"# Make Predictions\nknn_predictions_2 = knn_2.predict(X_test)\n\n# Compute Accuracy\nknn_acc_2 = np.sum(knn_predictions_2 == y_test)\/X_test.shape[0] * 100\nprint(\"Accuracy of sklearn's KNN classiifier is {} %\".format(knn_acc_2))","0594567c":"\n<p style=\"font-size:25px ; color:crimson\"> Please do upvote the notebook if you find it informative. I will shortly be putting up a blog post on my website on implementing KNN from scratch where I will try to explain theoritical concepts used in this notebook in depth. You can visit my website using <a href=\"https:\/\/keepingupwithdatascience.wordpress.com\/\">the link<\/a> provided here.","aef02cd2":"# K-Nearest Neighbors from Scratch \n\n*** \nIn this notebook, I try to explain how you can code K-Nearest Neighbors Algorithm from scratch. K-Nearest Neighbors or KNN in-short is a lazy algorithm which doesn't require any training. Lazy Algorithms are type of algorithms which start generalizing the data only when the query is made. In simple language, it starts working only when the predict() function is called.\n***\n","118b478f":"***\nAs we can see, both the models perform exactly the same. Hence, we can say that our custom model matches the scikit learn's performance in this case.\n***","b54c9c36":"# Testing the Algorithm\n***\nIn this section, we split the dataset into train and test sets then we test our custom model to generate predictions on the test set using the train set.\n***","7b310785":"# KNN Class\n***\nIn this section, I will create a custom KNN Class to implement K Nearest Neighbor Classifier.\n***","750ed4b0":"# Comparing The Performance\n***\nIn this section, I use Scikit Learn's KNeighborsClassifier class to generate predictions on our test set and compare it's performance with our custom model.\n***","8554d5c8":"# Data Preparation\n***\nWe create a custom classification dataset using sklearn's make_classification function. This will be used to train and test our algorithm.\n***","e5df73b6":"***\nOur custom KNN classifier is giving an accuracy of 96% on the test data. We are also able to visualise the results using a random data point.\n***","b3fe401e":"* Data Points with orange color are associated with class 1.\n* Data Points with blue color are associated with class 0"}}