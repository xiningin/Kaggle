{"cell_type":{"ff2718c2":"code","53cc98c2":"code","5c5c0b15":"code","56a10144":"code","6ca9d40c":"code","21058e96":"code","776a44ac":"code","9bddb3ca":"code","ea58a56c":"code","06303d13":"code","2c1ea274":"code","30efc13b":"code","0fdbf009":"code","19f414a6":"code","90be123c":"code","a779c3e9":"markdown","89b92b9a":"markdown","72ecbd35":"markdown","a12a7678":"markdown","d52fb742":"markdown","3ede99ff":"markdown","055e248b":"markdown","24a4e868":"markdown","f0ce241d":"markdown","55e597e5":"markdown"},"source":{"ff2718c2":"import numpy as np\nimport pandas as pd\nimport os\nimport sys\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB, ComplementNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import confusion_matrix, f1_score","53cc98c2":"data = pd.read_csv(\"..\/input\/spam_ham_dataset.csv\", header=0)\ndata = data.drop('Unnamed: 0', axis=1)\ndata = data.drop('label_num', axis=1)\ndata.head()\n","5c5c0b15":"data.describe()","56a10144":"pipeline = Pipeline([\n    ('counts', CountVectorizer(ngram_range=(1,2))),\n    ('nb', MultinomialNB())\n])\n\npipeline1 = Pipeline([\n    ('tfid', TfidfVectorizer()),\n    ('lr', LogisticRegression())\n])\n\npipeline2 = Pipeline([\n    ('counts', CountVectorizer(ngram_range=(1,2))),\n    ('cnb', ComplementNB())\n])","6ca9d40c":"x_train, x_test, y_train, y_test = train_test_split(data['text'], data['label'],test_size=.20)","21058e96":"pipeline.fit(x_train, y_train)","776a44ac":"print(classification_report(y_test, pipeline.predict(x_test)))","9bddb3ca":"confusion_matrix(y_test, pipeline.predict(x_test))","ea58a56c":"pipeline1.fit(x_train, y_train)\nprint(classification_report(y_test, pipeline1.predict(x_test)))","06303d13":"confusion_matrix(y_test, pipeline1.predict(x_test))","2c1ea274":"pipeline2.fit(x_train, y_train)\nprint(classification_report(y_test, pipeline2.predict(x_test)))","30efc13b":"confusion_matrix(y_test, pipeline2.predict(x_test))","0fdbf009":"k_fold = KFold(n_splits=6)\nscores = []\nconfusion = np.array([[0, 0], [0, 0]])\nfor train_indices, test_indices in k_fold.split(data):\n    train_text = data.iloc[train_indices]['text'].values\n    train_y = data.iloc[train_indices]['label'].values\n\n    test_text = data.iloc[test_indices]['text'].values\n    test_y = data.iloc[test_indices]['label'].values\n\n    pipeline.fit(train_text, train_y)\n    predictions = pipeline.predict(test_text)\n\n    confusion += confusion_matrix(test_y, predictions)\n    score = f1_score(test_y, predictions, pos_label='spam')\n    scores.append(score)\n\nprint('Total emails classified:', len(data))\nprint('Score:', sum(scores)\/len(scores))\nprint('Confusion matrix:')\nprint(confusion)","19f414a6":"k_fold = KFold(n_splits=6)\nscores = []\nconfusion = np.array([[0, 0], [0, 0]])\nfor train_indices, test_indices in k_fold.split(data):\n    train_text = data.iloc[train_indices]['text'].values\n    train_y = data.iloc[train_indices]['label'].values\n\n    test_text = data.iloc[test_indices]['text'].values\n    test_y = data.iloc[test_indices]['label'].values\n\n    pipeline1.fit(train_text, train_y)\n    predictions = pipeline1.predict(test_text)\n\n    confusion += confusion_matrix(test_y, predictions)\n    score = f1_score(test_y, predictions, pos_label='spam')\n    scores.append(score)\n\nprint('Total emails classified:', len(data))\nprint('Score:', sum(scores)\/len(scores))\nprint('Confusion matrix:')\nprint(confusion)","90be123c":"k_fold = KFold(n_splits=6)\nscores = []\nconfusion = np.array([[0, 0], [0, 0]])\nfor train_indices, test_indices in k_fold.split(data):\n    train_text = data.iloc[train_indices]['text'].values\n    train_y = data.iloc[train_indices]['label'].values\n\n    test_text = data.iloc[test_indices]['text'].values\n    test_y = data.iloc[test_indices]['label'].values\n\n    pipeline2.fit(train_text, train_y)\n    predictions = pipeline2.predict(test_text)\n\n    confusion += confusion_matrix(test_y, predictions)\n    score = f1_score(test_y, predictions, pos_label='spam')\n    scores.append(score)\n\nprint('Total emails classified:', len(data))\nprint('Score:', sum(scores)\/len(scores))\nprint('Confusion matrix:')\nprint(confusion)","a779c3e9":"### Using Cross Validation: K-Fold Validation to pick the best model.","89b92b9a":"### Model Fit \n\n#### Pipeline 1 - CountVectorization and MultinomialNaiveBayes","72ecbd35":"### The dataset contains two categories of emails already classified for us - spam and ham. In this notebook, we will explore Classification algorithms and in the end, do a cross validation to choose the best model.","a12a7678":"#### Pipeline 3 - CountVectoriztion and ComplementNaiveBayes","d52fb742":"This completes this notebook.","3ede99ff":"#### Pipeline2 - TF-IDF and LogisticRegression","055e248b":"### Reading and Cleaning Data\n\nWe are not interested in two columns - Unnamed:0, and label_num. So we'll be dropping them.","24a4e868":"### Pipeline\n\nWe'll create three pipelines to look at three models as below: \n* pipeline to cater CountVectorization and MultinomialNaiveBayes\n* pipeline to cater TF-IDF and LogisticRegression\n* pipeline to cater CountVectoriztion and ComplementNaiveBayes\n","f0ce241d":"### Train\/Test Split","55e597e5":"### Importing Relevant Libraries"}}