{"cell_type":{"9df80af3":"code","c1390885":"code","2b05ab31":"code","eb8531fe":"code","5350de79":"code","1e7ae00f":"code","bd0ba69b":"code","5354348c":"code","d5636583":"code","f2fc29de":"code","12d0b3c3":"code","4c4c321b":"code","6dc58a4c":"code","30469d39":"code","98e5c8a9":"code","ef6b9fa4":"code","3c9a1b44":"code","64c0c524":"code","c2537214":"code","32c7ed02":"code","bd5c5d7d":"code","52b6120d":"code","8bc7548f":"code","4cbae658":"code","53e2bbd4":"code","c653925c":"code","4e087215":"code","ee1fa0bb":"code","c8133d42":"code","c2eaf1b7":"code","89338c7e":"code","28adda28":"code","85e7d692":"markdown","43582adc":"markdown","fe710446":"markdown","11714075":"markdown","019ecd7a":"markdown","99ed5997":"markdown","85d14bf5":"markdown","51db8b0c":"markdown","b74164fb":"markdown","d44a94a3":"markdown","91d38177":"markdown","13ac25a5":"markdown","73457b5f":"markdown","4d91fa27":"markdown","f16fe7f9":"markdown"},"source":{"9df80af3":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nimport spacy\nimport nltk","c1390885":"nltk.download('stopwords')","2b05ab31":"df = pd.read_csv('..\/input\/trip-advisor-hotel-reviews\/tripadvisor_hotel_reviews.csv')\ndf.head()","eb8531fe":"df.shape","5350de79":"df.isnull().sum()","1e7ae00f":"df[\"Rating\"].value_counts() #Checks the rating values in case there is a weird value","bd0ba69b":"df.loc[df[\"Review\"] == \"\"] #Checks for empty review strings","5354348c":"pos = [5]\nneg = [1, 2]\nneu = [3, 4]\n\ndef sentiment(rating):\n  if rating in pos:\n    return 2\n  elif rating in neg:\n    return 0\n  else:\n    return 1  \ndf['Sentiment'] = df['Rating'].apply(sentiment)\ndf.head()","d5636583":"fig = go.Figure([go.Bar(x=df.Sentiment.value_counts().index, y=df.Sentiment.value_counts().tolist())])\nfig.update_layout(\n    title=\"Values in each Sentiment\",\n    xaxis_title=\"Sentiment\",\n    yaxis_title=\"Values\")\nfig.show()","f2fc29de":"from nltk.corpus import stopwords\nstopwords_list = set(stopwords.words(\"english\"))\npunctuations = \"\"\"!()-![]{};:,+'\"\\,<>.\/?@#$%^&*_~\u00c2\"\"\" #List of punctuation to remove\n\ndef reviewParse(review):\n    splitReview = review.split() #Split the review into words\n    parsedReview = \" \".join([word.translate(str.maketrans('', '', punctuations)) + \" \" for word in splitReview]) #Takes the stubborn punctuation out\n    return parsedReview #Returns the parsed review\n  \ndef clean_review(review):\n    clean_words = []\n    splitReview = review.split()\n    for w in splitReview:\n        if w.isalpha() and w not in stopwords_list:\n            clean_words.append(w.lower())\n    clean_review = \" \".join(clean_words)\n    return clean_review\n\ndf[\"Review\"] = df[\"Review\"].apply(reviewParse).apply(clean_review) #Parse all the reviews for their punctuation and add it into a new column\n\ndf.head() #Take a peek at the dataset","12d0b3c3":"df.head()","4c4c321b":"docs = list(df['Review'])[:7000]","6dc58a4c":"from sklearn.feature_extraction.text import TfidfVectorizer \n \n# settings that you use for count vectorizer will go here \ntfidf_vectorizer=TfidfVectorizer(use_idf=True, max_features = 20000) \n \n# just send in all your docs here \ntfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(docs)","30469d39":"#tfidf_vectorizer.get_feature_names()","98e5c8a9":"X = tfidf_vectorizer_vectors.toarray()\nY = df['Sentiment'][:7000]","ef6b9fa4":"len(X[0])","3c9a1b44":"from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV \nfrom sklearn.metrics import mean_absolute_error, accuracy_score, confusion_matrix, classification_report, roc_auc_score,roc_curve,auc\nfrom sklearn.tree import DecisionTreeClassifier\n\nSEED=123\n\nX_train,X_test,y_train,y_test=train_test_split(X, Y, test_size=0.2, random_state=SEED, stratify=Y)","64c0c524":"fig = go.Figure([go.Bar(x=Y.value_counts().index, y=Y.value_counts().tolist())])\nfig.update_layout(\n    title=\"Values in each Sentiment\",\n    xaxis_title=\"Sentiment\",\n    yaxis_title=\"Values\")\nfig.show()","c2537214":"dt = DecisionTreeClassifier(random_state=SEED)\ndt.fit(X_train,y_train)\ny_pred_test = dt.predict(X_test)\nprint(\"Training Accuracy score: \"+str(round(accuracy_score(y_train,dt.predict(X_train)),4)))\nprint(\"Testing Accuracy score: \"+str(round(accuracy_score(y_test,dt.predict(X_test)),4)))","32c7ed02":"print(classification_report(y_test, y_pred_test, target_names=['positive', 'neutral', 'negative']))","bd5c5d7d":"cm = confusion_matrix(y_test, y_pred_test)\n#print('Confusion matrix\\n', cm)\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Negative', 'Actual Neutral', 'Actual Positive'], \n                        index=['Predict Negative', 'Predict Neutral', 'Predict Positive'])\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\nplt.show()","52b6120d":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\ny_pred_train = gnb.predict(X_train)\ny_pred_test = gnb.predict(X_test)\nprint(\"Training Accuracy score: \"+str(round(accuracy_score(y_train,gnb.predict(X_train)),4)))\nprint(\"Testing Accuracy score: \"+str(round(accuracy_score(y_test,gnb.predict(X_test)),4)))","8bc7548f":"print(classification_report(y_test, y_pred_test, target_names=['positive', 'neutral', 'negative']))","4cbae658":"cm = confusion_matrix(y_test, y_pred_test)\n#print('Confusion matrix\\n', cm)\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Negative', 'Actual Neutral', 'Actual Positive'], \n                        index=['Predict Negative', 'Predict Neutral', 'Predict Positive'])\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\nplt.show()","53e2bbd4":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(random_state=SEED).fit(X_train, y_train)\ny_pred_train = lr.predict(X_train)\ny_pred_test = lr.predict(X_test)\nprint(\"Training Accuracy score: \"+str(round(accuracy_score(y_train,lr.predict(X_train)),4)))\nprint(\"Testing Accuracy score: \"+str(round(accuracy_score(y_test,lr.predict(X_test)),4)))","c653925c":"print(classification_report(y_test, y_pred_test, target_names=['positive', 'neutral', 'negative']))","4e087215":"cm = confusion_matrix(y_test, y_pred_test)\n#print('Confusion matrix\\n', cm)\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Negative', 'Actual Neutral', 'Actual Positive'], \n                        index=['Predict Negative', 'Predict Neutral', 'Predict Positive'])\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\nplt.show()","ee1fa0bb":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\ny_pred_train = clf.predict(X_train)\ny_pred_test = clf.predict(X_test)\nprint(\"Training Accuracy score: \"+str(round(accuracy_score(y_train,clf.predict(X_train)),4)))\nprint(\"Testing Accuracy score: \"+str(round(accuracy_score(y_test,clf.predict(X_test)),4)))","c8133d42":"print(classification_report(y_test, y_pred_test, target_names=['positive', 'neutral', 'negative']))","c2eaf1b7":"cm = confusion_matrix(y_test, y_pred_test)\n#print('Confusion matrix\\n', cm)\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Negative', 'Actual Neutral', 'Actual Positive'], \n                        index=['Predict Negative', 'Predict Neutral', 'Predict Positive'])\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\nplt.show()","89338c7e":"from sklearn.ensemble import VotingClassifier\n\nclassifiers = [('Decision Tree', dt),\n               ('Logistic Regression', lr),\n                ('Naive Bayes', gnb)\n              ]\nvc = VotingClassifier(estimators=classifiers)\n# Fit 'vc' to the traing set and predict test set labels\nvc.fit(X_train, y_train)\n\nprint(\"Training Accuracy score: \"+str(round(accuracy_score(y_train,vc.predict(X_train)),4)))\nprint(\"Testing Accuracy score: \"+str(round(accuracy_score(y_test,vc.predict(X_test)),4)))","28adda28":"predictions = pd.Series(lr.predict(X), name=\"sentiment\")\nresults = pd.concat([predictions],axis=1)\nresults.to_csv(\"airbnb-review-sentiment.csv\",index=False)","85e7d692":"## Random Forest Classifier","43582adc":"## Text Preprocessing \n1. Tokenization\n2. Punctuation removal\n3. Stopword removal\n4. Removing HTML Tags\n5. Lower casing\n","fe710446":"## Read the dataset and clean it","11714075":"## Logistic Regression","019ecd7a":"## Segregating and Encoding Positive, Neutral and Negative labels","99ed5997":"### Divide the data into training and validation sets","85d14bf5":"## Import the necessary packages","51db8b0c":"## Ensembling ","b74164fb":"## Decision Tree Classifier","d44a94a3":"## Sentiment Analysis Pipeline\n![pipeline](https:\/\/cdn-images-1.medium.com\/max\/361\/0*ga5rNPmVYBsCm-lz.)","91d38177":"## Airbnb Reviews Sentiment Analysis\n\n![image](https:\/\/www.kdnuggets.com\/images\/sentiment-fig-1-689.jpg)\n\nSentiment analysis is the process of detecting positive or negative sentiment in text. It\u2019s often used by businesses to detect sentiment in social data, gauge brand reputation, and understand customers.\n\n### Why Is Sentiment Analysis Important?\nSentiment analysis is extremely important because it helps businesses quickly understand the overall opinions of their customers. By automatically sorting the sentiment behind reviews, social media conversations, and more, you can make faster and more accurate decisions.\n\nIt\u2019s estimated that 90% of the world\u2019s data is unstructured, in other words it\u2019s unorganized. Huge volumes of unstructured business data are created every day: emails, support tickets, chats, social media conversations, surveys, articles, documents, etc). But it\u2019s hard to analyze for sentiment in a timely and efficient manner.\n\n### Some Applications of Sentiment Analysis\nThe applications of sentiment analysis are endless and can be applied to any industry, from finance and retail to hospitality and technology. Below, we\u2019ve listed some of the most popular ways that sentiment analysis is being used in business:\n\n1. Social Media Monitoring\n1. Brand Monitoring\n1. Voice of customer (VoC)\n1. Customer Service\n1. Market Research","13ac25a5":"## Analysing Positive, Neutral and Negative Reviews","73457b5f":"## Create a TFIDF matrix out of it","4d91fa27":"## Naive Bayes Classifier","f16fe7f9":"## Conclusion\nHence we successfully studied various models like Decision Tree, SVM, Naive Bayes and Logistic Regression and implemented them for the given dataset as part of the experiment along with a comparative analysis of various metrics and made the following observations.\n\n1. Na\u00efve Bayes and Decision Tree are susceptible to noise if present in the dataset because when we reduced the number of features by considering only the most frequent words the accuracy and AUC score increased significantly.\n2. Logistic Regression and SVM performed almost same for the given dataset even with the initial number of features.\n3. We can increase Accuracy marginally by removing Named Entities using spacy and performing Lemmatization on top of that on all the models mentioned above.\n\n### Final Note\nSentiment analysis can be applied to countless aspects of business, from brand monitoring and product analytics, to customer service and market research. By incorporating it into their existing systems and analytics, leading brands (not to mention entire cities) are able to work faster, with more accuracy, toward more useful ends.\n\nSentiment analysis has moved beyond merely an interesting, high-tech whim, and will soon become an indispensable tool for all companies of the modern age. Ultimately, sentiment analysis enables us to glean new insights, better understand our customers, and empower our own teams more effectively so that they do better and more productive work."}}