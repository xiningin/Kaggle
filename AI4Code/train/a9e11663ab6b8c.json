{"cell_type":{"c3dff95a":"code","f829f405":"code","2c505bd7":"code","d3bdae85":"code","95b21f41":"code","78449c4b":"code","6f08d22b":"code","0a1abb3c":"code","5132b2d6":"code","eba94539":"code","5e2808c7":"code","1646a8d6":"code","6af59fd8":"code","b8cb1dfc":"code","a7f47bd0":"code","7024172e":"code","30b59718":"markdown"},"source":{"c3dff95a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f829f405":"df=pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/train.csv')","2c505bd7":"import sklearn","d3bdae85":"from sklearn import datasets\nfrom sklearn import model_selection\n\ndef create_folds(data):\n # we create a new column called kfold and fill it with -1\n data[\"kfold\"] = -1\n \n # the next step is to randomize the rows of the data\n data = data.sample(frac=1).reset_index(drop=True)\n # calculate the number of bins by Sturge's rule\n # I take the floor of the value, you can also\n # just round it\n#  num_bins = int(np.floor(1 + np.log2(len(data))))\n#  # bin targets\n#  data.loc[:, \"bins\"] = pd.cut(\n#  data[\"target\"], bins=num_bins, labels=False\n#  )\n \n # initiate the kfold class from model_selection module\n kf = model_selection.StratifiedKFold(n_splits=5)\n \n # fill the new kfold column\n # note that, instead of targets, we use bins!\n for f, (t_, v_) in enumerate(kf.split(X=data, y=data.target)):\n     data.loc[v_, 'kfold'] = f\n \n # drop the bins column\n#  data = data.drop(\"bins\", axis=1)\n # return dataframe with folds\n return data","95b21f41":"df=create_folds(df)","78449c4b":"import optuna\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xg\nimport joblib\nimport lightgbm as lgb ","6f08d22b":"def objective(trial):\n    \n    param = {\n        'tree_method':'gpu_hist',  # this parameter means using the GPU when training our model to speedup the training process\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]),\n        'n_estimators': 4000,\n        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20]),\n        'random_state': trial.suggest_categorical('random_state', [24, 48,2020]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n        'use_label_encoder':False\n    }\n    \n    \n    roc_auc_score=0\n    clf = xg.XGBClassifier(**param)\n    for fold in range(5):\n        \n        df_train=df[df.kfold!=fold]\n        df_test=df[df.kfold==fold]\n\n        x_train=df_train.drop(['target','id','kfold'],axis=1).values\n        y_train=df_train.target.values\n\n        x_test=df_test.drop(['target','id','kfold'],axis=1).values\n        y_test=df_test.target.values\n\n    #     clf=RandomForestClassifier()\n        clf.fit(x_train,y_train)\n\n        y_pred = clf.predict_proba(x_test)[:,1]\n        roc_auc_score = roc_auc_score+sklearn.metrics.roc_auc_score(y_test,y_pred)\n    \n    return roc_auc_score\/5\n#     return sklearn.model_selection.cross_val_score(clf, x, y, \n#        n_jobs=-1, cv=5).mean()","0a1abb3c":"study = optuna.create_study(direction='maximize')\n# optimization_function=partial(objective,x=x,y=y)\nstudy.optimize(objective, n_trials=12)","5132b2d6":"def run_folds(df,fold):\n    df_train=df[df.kfold!=fold]\n    df_test=df[df.kfold==fold]\n    \n    x_train=df_train.drop(['target','id','kfold'],axis=1).values\n    y_train=df_train.target.values\n    \n    x_test=df_test.drop(['target','id','kfold'],axis=1).values\n    y_test=df_test.target.values\n    #xgboost\n    param={'tree_method':'gpu_hist','lambda': 0.5033009770384954, 'alpha': 0.5585785710700676, 'colsample_bytree': 0.8, \n           'subsample': 0.5, 'learning_rate': 0.008, 'max_depth': 11, 'random_state': 2020, 'min_child_weight': 263,\n          'use_label_encoder':False,'n_estimators': 4000}\n\n    \n    clf= xg.XGBClassifier(**param)\n    clf.fit(x_train,y_train)\n    \n    y_pred = clf.predict_proba(x_test)[:,1]\n    roc_auc_score = sklearn.metrics.roc_auc_score(y_test,y_pred)\n    print(f\"Fold={fold}, roc_auc_score={roc_auc_score}\")\n    \n    File_name = 'model_lgbmclf' + str(fold)\n    joblib.dump(\n    clf,File_name)\n    \n    \nfor i in range(5):\n    run_folds(df,i)    ","eba94539":"# i didnot run the aboe code cell also just to save time","5e2808c7":"param={'tree_method':'gpu_hist','lambda': 0.5033009770384954, 'alpha': 0.5585785710700676, 'colsample_bytree': 0.8, \n           'subsample': 0.5, 'learning_rate': 0.008, 'max_depth': 11, 'random_state': 2020, 'min_child_weight': 263,\n          'use_label_encoder':False,'n_estimators': 4000}\nxgclf=xg.XGBClassifier(**param)\nxgclf.fit(df.iloc[:,1:-2],df.target)","1646a8d6":"submission = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv')","6af59fd8":"df_test = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/test.csv')\ndf_test = df_test.drop(columns = 'id')","b8cb1dfc":"y_final_avg=xgclf.predict_proba(df_test)","a7f47bd0":"submission['target'] = y_final_avg","7024172e":"submission.to_csv('pred_csv_xg_optuna_tuned2.csv',index = False)","30b59718":"i didnot run this here because i have done it already in my local "}}