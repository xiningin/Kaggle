{"cell_type":{"886d226d":"code","36bd7768":"code","4e256e44":"code","c145f186":"markdown","28ee96ae":"markdown"},"source":{"886d226d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","36bd7768":"# n: batch size\n# d_in: input dimension\n# h: hidden layer\n# d_out: output dimension\nn, d_in, h, d_out = 64, 1000, 100, 10\n\n# randomly initialize input, output & weights\nx = np.random.randn(n, d_in)\ny = np.random.randn(n, d_out)\nw1 = np.random.randn(d_in, h)\nw2 = np.random.rand(h, d_out)\n\nlr = 1e-6\n\nfor i in range(500):\n    # Forward pass: compute predicted y\n    h = x.dot(w1)\n    h_relu = np.maximum(h, 0)\n    y_pred = h_relu.dot(w2)\n    \n    # Compute loss\n    loss = np.square(y_pred - y).sum()\n    print(\"epoch: {} Loss: {}\".format(i, loss)) if i % 50 == 0 else None\n    \n    # Backprop to compute gradiens of w1 & w2 wrt loss\n    grad_y_pred = 2 * (y_pred -y)\n    grad_w2 = h_relu.T.dot(grad_y_pred)\n    grad_h_relu = grad_y_pred.dot(w2.T)\n    grad_h = grad_h_relu.copy()\n    grad_h[h<0] = 0\n    grad_w1 = x.T.dot(grad_h)\n    \n    # Update weights\n    w1 -= lr * grad_w1\n    w2 -= lr * grad_w2","4e256e44":"import torch\n\ndtype = torch.float\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n# n: batch size\n# d_in: input dimension\n# h: hidden layer\n# d_out: output dimension\nn, d_in, h, d_out = 64, 1000, 100, 10\n\n# randomly initialize input, output & weights\nx = torch.randn(n, d_in, device=device, dtype=dtype)\ny = torch.randn(n, d_out, device=device, dtype=dtype)\nw1 = torch.randn(d_in, h, device=device, dtype=dtype)\nw2 = torch.rand(h, d_out, device=device, dtype=dtype)\n\nlr = 1e-6\n\nfor i in range(500):\n    # Forward pass: compute predicted y\n    h = x.mm(w1)\n    h_relu = h.clamp(min=0)\n    y_pred = h_relu.mm(w2)\n    \n    # Compute loss\n    loss = (y_pred - y).pow(2).sum().item()\n    print(\"epoch: {} Loss: {}\".format(i, loss)) if i % 50 == 0 else None\n    \n    # Backprop to compute gradients of w1 & w2 wrt loss\n    grad_y_pred = 2 * (y_pred - y)\n    grad_w2 = h_relu.t().mm(grad_y_pred)\n    grad_h_relu = grad_y_pred.mm(w2.t())\n    grad_h = grad_h_relu.clone()\n    grad_h[h<0] = 0\n    grad_w1 = x.t().mm(grad_h)\n    \n    # Update weights using gradient descent\n    w1 -= lr * grad_w1\n    w2 -= lr * grad_w2","c145f186":"**Creating a two layer network using *Pytorch Tensor***","28ee96ae":"**Creating a two-layer network using *numpy***"}}