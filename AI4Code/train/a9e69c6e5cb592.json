{"cell_type":{"f0c8f715":"code","a7f60eb9":"code","0078d676":"code","1e5a229d":"code","ec5e8178":"code","589436d0":"code","d41c740c":"code","664a4f9c":"markdown","bb01d8c5":"markdown","8cce6ea3":"markdown","14acdda6":"markdown","de150ca9":"markdown","f831cddd":"markdown"},"source":{"f0c8f715":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a7f60eb9":"import random\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nimport json\nimport tensorflow as tf\nimport csv\nimport random\nimport numpy as np\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import regularizers","0078d676":"real_news = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/True.csv\")\nfake_news = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/Fake.csv\")\nreal_news['label'] = 1\nfake_news['label'] = 0\nwhole_dataset = pd.concat([real_news, fake_news])","1e5a229d":"X_train, X_test, y_train, y_test = train_test_split(whole_dataset['title'], whole_dataset['label'], test_size=0.2, random_state=10, stratify = whole_dataset['label'])\n","ec5e8178":"print(X_test.head())\nprint(y_test.head())","589436d0":"\nlengths = [len(x) for x in whole_dataset['title']]\nmax_length = max(lengths)\ntrunc_type = 'post'\npadding_type = 'post'\n\nembedding_dim = 100\noov_tok = \"<OOV>\"\n\ntokenizer = Tokenizer(oov_token=oov_tok)\ntokenizer.fit_on_texts(X_train)\n\nword_index = tokenizer.word_index\nvocab_size=len(word_index)\n\nsequences = tokenizer.texts_to_sequences(X_train)\npadded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\ntest_sequences = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\n\nprint(X_train.iloc[0], y_train[0])\nprint(X_train.iloc[1], y_train[1])\n#print(padded[0])","d41c740c":"from keras.callbacks import EarlyStopping\noverfitCallback = EarlyStopping(monitor='val_loss',\n                              min_delta=0,\n                              patience=5,\n                              verbose=0, mode='auto')\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1, 15, input_length=max_length),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Conv1D(64, 5, activation='relu'),\n    tf.keras.layers.MaxPooling1D(pool_size=4),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\n\nnum_epochs = 50\nhistory = model.fit(padded, y_train, epochs=num_epochs, validation_data=(test_sequences, y_test), verbose=2, callbacks=[overfitCallback])\n\nprint(\"Training Complete\")","664a4f9c":"I will tokenize the titles and pad them to have all the same length, which is set to the length of the longest title","bb01d8c5":"As a first baseline model, I will categorize real and fake news with a layers of Embeddings Biderectional LSTMs. For this approach I will use the news titles only, and disregards the rest of the information in the data set (i.e. text, date, category). ","8cce6ea3":"I load the data in the notebook and add labels (1 for real and 0 for fake news). These are the property I will predict). Once the label column is added, the two data sets are joined.","14acdda6":"The data is now reday for the modeling. I define an early stopping callback to prevent too much overfitting. The model is a sequential stacking of 6 layers.\nThe model is reaching a 97% validation accuracy and has an early stoppinf after 6 epochs.","de150ca9":"I split the data into training (80% of the data) and validation (20% of the data), I stratify the label column, to have the same amount of fake news in the training and validation set (and the same for the real news). ","f831cddd":"Next steps:\n- clean up of the titles\n- inclusion of news text in the prediction\n- plotting of the results and word clouds"}}