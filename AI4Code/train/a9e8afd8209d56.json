{"cell_type":{"141d5af5":"code","a3b0d017":"code","2d390c0d":"code","7d5857d0":"code","ba110361":"code","54821e89":"code","60514a7f":"code","3fceba04":"code","d44b13f8":"code","571310cf":"code","03908b86":"code","8433faa9":"code","b53e4268":"code","1da35610":"code","8e8c43a9":"code","5d6be24d":"code","d36027e3":"code","7247c082":"code","ed79022b":"code","507dfc9e":"code","47356c28":"code","bef61f7b":"markdown","d3042f9e":"markdown","64aa8252":"markdown","d721b052":"markdown","59da40b4":"markdown","1485fdfa":"markdown"},"source":{"141d5af5":"!pip install torchsummary","a3b0d017":"import torch\nfrom torchvision import transforms, datasets\nfrom torch import nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom torchsummary import summary\ntorch.manual_seed(42)\nimport base64","2d390c0d":"def to_base_64(string):\n    bytes_from_string = bytes(string, 'utf-8')\n    return(base64.b64encode(bytes_from_string))\n    \ndef from_base_64(bytes_):\n    encoded_string = base64.b64decode(bytes_)\n    return encoded_string.decode('utf-8')","7d5857d0":"X = torch.Tensor([[0., 0.],\n                  [0., 1.],\n                  [1., 0.],\n                  [1., 1.]])\n","ba110361":"y = torch.Tensor([[0.0], [1.0], [1.0], [0.0]])","54821e89":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","60514a7f":"X , y = X.to(device) , y.to(device)","3fceba04":"model = nn.Sequential(nn.Linear(2, 2),\n                      nn.Sigmoid(),\n                      nn.Linear(2, 1))","d44b13f8":"model = model.to(device)","571310cf":"criterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.03)","03908b86":"for i in range(10000):\n    optimizer.zero_grad()\n    y_hat = model(X)\n    loss = criterion(y_hat, y)\n    loss.backward()\n    optimizer.step()\n    if i % 1000 == 0:\n        print(loss.item())","8433faa9":"model = model.cpu()","b53e4268":"mnist_train = datasets.MNIST(\".\/data-mnist\",\n                        download=True,\n                        train=True,\n                        transform=transforms.ToTensor())\nmnist_test = datasets.MNIST(\".\/data-mnist\",\n                        download=True,\n                        train=False,\n                        transform=transforms.ToTensor())","1da35610":"## Tutaj umie\u015b\u0107 sw\u00f3j kod\n\n\n##","8e8c43a9":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","5d6be24d":"## Tutaj umie\u015b\u0107 sw\u00f3j kod\n\n\n\n\n##","d36027e3":"model","7247c082":"model = model.to(device)","ed79022b":"criterion = nn.NLLLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.03)","507dfc9e":"## Tutaj umie\u015b\u0107 sw\u00f3j kod\n\n\n\n\n\n\n\n##","47356c28":"plt.plot(all_accuracies_train)\nplt.plot(all_accuracies_test)\nplt.legend(['train','test'])\nplt.show()","bef61f7b":"## Modu\u0142 [sequential](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.Sequential.html) - uproszczone podej\u015bcie do sieci neuronowych\n\nNajprostszym sposobem na stworzenie prostej sieci neuronowej w Pytorchu jest u\u017cycie modu\u0142u sequential - Jest to modu\u0142 do tworzenia sieci neuronowych wzorowany na Kerasie. Polega on na \u0142\u0105czeniu standardowych dla Pytorcha warstw w jeden sekwencyjny model. Zalet\u0105 tego podej\u015bcia jest jego prostota. By stworzy\u0107 model, nie potrzeba zna\u0107 norm rz\u0105dz\u0105cych si\u0119 bibliotek\u0105 ani dzia\u0142ania poszczeg\u00f3lnych warstw. Niestety z powodu prostoty modu\u0142 ten ma znikome mo\u017cliwo\u015bci kustomizacji.\n\nPrzyk\u0142adow\u0105 si\u0119\u0107 u\u017cywajac tego modu\u0142u tworzy si\u0119 podaj\u0105c wszystkiego warstwy w kolejno\u015bci w kt\u00f3rej chcemy, by zosta\u0142y one policzone. Przyk\u0142ad dla warstw Linear i funcji aktywacyjne relu.\n```python\nmodel = nn.Sequential(nn.Linear(128, 16),\n                      nn.ReLU())\n```\n\nModel mo\u017cemy w ka\u017cdej chwili zwizualizowa\u0107 u\u017cywaj\u0105c funkcjin print\n``` \nprint(model)\n\nSequential(\n  (0): Linear(in_features=128, out_features=16, bias=True)\n  (1): ReLU()\n)\n```\n\nRozwi\u0105zanie w formie base_64: <br>\nCmlucHV0X3NpemUgPSAyOCoyOApoaWRkZW5fc2l6ZV8xID0gMTI4CmhpZGRlbl9zaXplXzIgPSA2NApvdXRwdXRfc2l6ZSA9IDEwCgptb2RlbCA9IG5uLlNlcXVlbnRpYWwobm4uRmxhdHRlbihzdGFydF9kaW09MSksCiAgICAgICAgICAgICAgICAgICAgICBubi5MaW5lYXIoaW5wdXRfc2l6ZSwgaGlkZGVuX3NpemVfMSksCiAgICAgICAgICAgICAgICAgICAgICBubi5SZUxVKCksCiAgICAgICAgICAgICAgICAgICAgICBubi5MaW5lYXIoaGlkZGVuX3NpemVfMSwgaGlkZGVuX3NpemVfMiksCiAgICAgICAgICAgICAgICAgICAgICBubi5SZUxVKCksCiAgICAgICAgICAgICAgICAgICAgICBubi5MaW5lYXIoaGlkZGVuX3NpemVfMiwgb3V0cHV0X3NpemUpLAogICAgICAgICAgICAgICAgICAgICAgbm4uTG9nU29mdG1heChkaW09MSkpCg==","d3042f9e":"## Czym s\u0105 sieci neuronowe - szybki wst\u0119p\n\nSieci neuronowe s\u0105 algorytmami wzorowanymi na biologicznych sieciach neuronowych. Sk\u0142adaj\u0105 si\u0119 one z podstawowego elementu, kt\u00f3rym jest neuron, kt\u00f3ry jest lu\u017ano opart na swoim biologicznym wzorcu. Ka\u017cdy neuron \u0142\u0105czony si\u0119 z innymi neuronami w s\u0105siednich warstwach poprzez sie\u0107 po\u0142\u0105cze\u0144 oraz odpowiadaj\u0105cych im wag. Wagi okre\u015blaj\u0105, jak wielki wp\u0142yw b\u0119dzie mia\u0142 sygna\u0142 z neuronu poprzedzaj\u0105cego na obecn\u0105 warto\u015b\u0107 neuronu. W zale\u017cno\u015bci od implementacji ka\u017cdy neuron posiada jeszcze dodatkowy, wolny argument zwany baiasem. Oznacza on warto\u015b\u0107, jak\u0105 przyjmie neuron, bez \u017cadnego wp\u0142ywu zewn\u0119trznego. Po zsumowaniu aktywacji neuron\u00f3w w poprzedniej warstwie oraz baiasu (zazwyczaj) nast\u0119puje u\u017cycie funkcji aktywacyjnej na tej\u017ce warto\u015bci.\nZadania funkcji aktywacyjnej:\n* Ograniczanie przedzia\u0142u warto\u015bci, kt\u00f3ry mo\u017cemy otrzyma\u0107 do tego, kt\u00f3ry jest adekwatny do problemu\n* Wprowadzanie nieliniowo\u015bci do naszego modelu (model jest w stanie odwzorowywa\u0107 bardziej skomplikowane funkcje)\n* Zapobieganiu problemom zwi\u0105zanych z gradientami.\n![Neuron](https:\/\/miro.medium.com\/max\/1838\/1*T4ARzySpEQvEnr_9pc78pg.jpeg)\n\nW zwi\u0105zku z faktem, i\u017c g\u0142\u00f3wne zastosowanie sieci neuronowych le\u017cy w modelowaniu problem\u00f3w, kt\u00f3re s\u0105 poza zasi\u0119giem innych algorytm\u00f3w, ich struktura zazwyczaj jest stosunkowo rozbudowana. Oznacza to, i\u017c w przeciwie\u0144stwie do wielu poprzednio om\u00f3wionych algorytm\u00f3w mog\u0105 si\u0119 one sk\u0142ada\u0107 z bardzo wielu blok\u00f3w, zwanych tak\u017ce warstwami. Warstwy te odpowiadaj\u0105 za pozwolenie sieci na wydobywanie coraz to bardziej skomplikowanych wzorc\u00f3w, gdy\u017c \u0142\u0105czy ona wzorce z poprzedniej warstwy. Przyk\u0142adowo, pierwsza warstwa mo\u017ce nauczy\u0107 si\u0119 rozpoznawa\u0107 kraw\u0119dzie, kolejna bardziej zaawansowane kszta\u0142ty a ostatnia twarze. Bez po\u015brednich warstw sie\u0107 musia\u0142aby nauczy\u0107 si\u0119 od razu rozpoznawa\u0107 twarze bez \u017cadnego kontekstu, co mog\u0142oby okaza\u0107 si\u0119 poza mo\u017cliwo\u015bciami tego\u017c algorytmu. \n\n<img src=\"https:\/\/miro.medium.com\/max\/453\/1*51D0MqtqHu3h2vTE5oJ-7g.png\" width=\"800\">\n\n\n","64aa8252":"### Zadanie trzecie\nBazuj\u0105c na poprzednich \u0107wiczeniach, skonstruuj p\u0119tle ucz\u0105c\u0105 i walidacyjn\u0105. W zwi\u0105zku z faktem, i\u017c przekazali\u015bmy wszystkie parametry sieci do optimizera, nie musimy zmienia\u0107 ich r\u0119cznie. Resetowanie gradientu dla wszystkich parametr\u00f3w odbywa si\u0119 metod\u0105 ```optimizer.step()``` poprzedzony metod\u0105 ```loss.backward()```, natomiast gradient zeruje si\u0119 u\u017cywaj\u0105c ```optimizer.zero_grad()```. By wykona\u0107 operacje, dla kt\u00f3rych nie chcemy zapisywa\u0107 gradientu takie jak inferencja, dan\u0105 cz\u0119\u015b\u0107 kodu nale\u017cy opakowa\u0107 w ```with torch.no_grad()```\nPrzydatne funkcje dla liczenia b\u0142\u0119du:\n* [torch.eq](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.eq.html)\n* [torch.argmax](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.argmax.html)\n* [Tensor.item()](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.Tensor.item.html)\n* [Tensor.cpu()](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.Tensor.cpu.html)\n\n<br>\n\nRozwi\u0105zanie w formie base_64:\n<br>\nCgphbGxfYWNjdXJhY2llc190cmFpbiA9IFtdCmFsbF9hY2N1cmFjaWVzX3Rlc3QgPSBbXQoKbnVtX2Vwb2NocyA9IDEwMAoKZm9yIGVwb2NoIGluIHJhbmdlKG51bV9lcG9jaHMpOgoKY29ycmVjdF9jb3VudCA9IDAKYWxsX2NvdW50ID0gMAoKZm9yIGxvY2FsX3RyYWluX2JhdGNoLCBsb2NhbF90cmFpbl9sYWJlbHMgaW4gdHJhaW46CiAgICBsb2NhbF90cmFpbl9iYXRjaCwgbG9jYWxfdHJhaW5fbGFiZWxzID0gbG9jYWxfdHJhaW5fYmF0Y2gudG8oZGV2aWNlKSwgbG9jYWxfdHJhaW5fbGFiZWxzLnRvKGRldmljZSkKCiAgICBvcHRpbWl6ZXIuemVyb19ncmFkKCkKICAgIHlfaGF0ID0gbW9kZWwobG9jYWxfdHJhaW5fYmF0Y2gpCiAgICBsb3NzID0gY3JpdGVyaW9uKHlfaGF0LCBsb2NhbF90cmFpbl9sYWJlbHMpCiAgICBsb3NzLmJhY2t3YXJkKCkKICAgIG9wdGltaXplci5zdGVwKCkKICAgIHlfaGF0ID0gdG9yY2guYXJnbWF4KHlfaGF0LGF4aXM9LTEpCiAgICBjb3JyZWN0X2NvdW50ICs9IHRvcmNoLnN1bSh0b3JjaC5lcSh5X2hhdCxsb2NhbF90cmFpbl9sYWJlbHMpKS5jcHUoKS5pdGVtKCkKICAgIGFsbF9jb3VudCArPSBsb2NhbF90cmFpbl9sYWJlbHMuc2hhcGVbLTFdCgoKcHJpbnQoIk1vZGVsIFRyYWluIEFjY3VyYWN5ID0iLCByb3VuZChjb3JyZWN0X2NvdW50L2FsbF9jb3VudCwzKSkKYWxsX2FjY3VyYWNpZXNfdHJhaW4uYXBwZW5kKGNvcnJlY3RfY291bnQvYWxsX2NvdW50KQogICAgCmNvcnJlY3RfY291bnQgPSAwCmFsbF9jb3VudCA9IDAKCgogICAgCmZvciBsb2NhbF90ZXN0X2JhdGNoLCBsb2NhbF90ZXN0X2xhYmVscyBpbiB0ZXN0OgogICAgbG9jYWxfdGVzdF9iYXRjaCwgbG9jYWxfdGVzdF9sYWJlbHMgPSBsb2NhbF90ZXN0X2JhdGNoLnRvKGRldmljZSksIGxvY2FsX3Rlc3RfbGFiZWxzLnRvKGRldmljZSkKICAgIHdpdGggdG9yY2gubm9fZ3JhZCgpOgogICAgICAgIHlfaGF0ID0gbW9kZWwobG9jYWxfdGVzdF9iYXRjaCkKICAgIHlfaGF0ID0gdG9yY2guYXJnbWF4KHlfaGF0LGF4aXM9LTEpCiAgICBjb3JyZWN0X2NvdW50ICs9IHRvcmNoLnN1bSh0b3JjaC5lcSh5X2hhdCxsb2NhbF90ZXN0X2xhYmVscykpLmNwdSgpLml0ZW0oKQogICAgYWxsX2NvdW50ICs9IGxvY2FsX3Rlc3RfbGFiZWxzLnNoYXBlWy0xXQoKcHJpbnQoIk1vZGVsIFRlc3QgQWNjdXJhY3kgPSIsIChjb3JyZWN0X2NvdW50L2FsbF9jb3VudCkpCmFsbF9hY2N1cmFjaWVzX3Rlc3QuYXBwZW5kKGNvcnJlY3RfY291bnQvYWxsX2NvdW50KQoK","d721b052":"### Zadanie pierwsze\nPrzekonwertuj obiekt Dataset an obiekt [Dataloader](https:\/\/pytorch.org\/docs\/stable\/data.html). Dataloader jest iteratorem pozwalaj\u0105cym przemieszcza\u0107 si\u0119 po Datasecie w spos\u00f3b cykliczny. \n\nRozwi\u0105zanie zakodowane w formie bytes64:\nCnRyYWluID0gdG9yY2gudXRpbHMuZGF0YS5EYXRhTG9hZGVyKG1uaXN0X3RyYWluLGJhdGNoX3NpemU9NjQpIAp0ZXN0ID0gdG9yY2gudXRpbHMuZGF0YS5EYXRhTG9hZGVyKG1uaXN0X3Rlc3QsYmF0Y2hfc2l6ZT0xMjgpIAo=\n<br>\nDla odkodowania zadania u\u017cyj funkcji:\n\n```python\nprint(from_base_64(zakodowany_string))\n```\n","59da40b4":"### Zadanie drugie\n\nStw\u00f3rz model o podstawowej architekturze podanej poni\u017cej. Warstwy konieczne do wykorzystania to:\n* [Warstwa Flatten](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.Flatten.html)\n* [Warstwa Linear](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.Linear.html)\n* [Warstwa Relu](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.ReLU.html)\n* [Warstwa LogSoftmax](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.LogSoftmax.html)\n\n\n```\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n           Flatten-1                  [-1, 784]               0\n            Linear-2                  [-1, 128]         100,480\n              ReLU-3                  [-1, 128]               0\n            Linear-4                   [-1, 64]           8,256\n              ReLU-5                   [-1, 64]               0\n            Linear-6                   [-1, 10]             650\n        LogSoftmax-7                   [-1, 10]               0\n================================================================\n```","1485fdfa":"## Czym s\u0105 obiekty [dataset](https:\/\/pytorch.org\/vision\/stable\/datasets.html) z modu\u0142u torchvision w pytorchu - szybkie wprowadzenie\n\nW zwi\u0105zku ze wzrostem zainteresowania uczeniem g\u0142\u0119bokim oraz w\u0142asnym typem zmiennych kt\u00f3ry u\u017cywany jest przez ka\u017cd\u0105 z bibliotek. Dlatego, by pom\u00f3c przy wdra\u017caniu si\u0119 w wybran\u0105 bibliotek\u0119 zar\u00f3wno tensorflow jak i pytorch stworzy\u0142 w\u0142asny interfejs do pobierania najpopularniejszych zbior\u00f3w danych z wybranej kategorii. Zbiory danych stworzone przy pomocy tej\u017ce biblioteki s\u0105 kompatybilne z rezt\u0105 narz\u0119dzi pozwalaj\u0105c na bezproblemowe zapoznanie si\u0119 z metodyk\u0105 pracy w wybranym frameworku.\n\nPodczas tworzenia sieci neuronowych w tym tutorialu u\u017cyjemy zbioru danych MNIST. Jest to zbi\u00f3r z 10 r\u00f3\u017cnymi kategoriami zdj\u0119\u0107 oraz od do zdj\u0119\u0107 na kategorie. Jest to jeden ze standardowych zbior\u00f3w benchmarkowych dla system\u00f3w klasyfikacji obraz\u00f3w."}}