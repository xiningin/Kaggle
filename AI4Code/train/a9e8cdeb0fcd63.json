{"cell_type":{"ce4b0707":"code","9b2d8616":"code","9afd008a":"code","e618f742":"code","c7a75982":"code","f2fdb297":"code","7913bba1":"code","827500c5":"code","d52e74d0":"code","35c20337":"code","95462a5d":"code","881c246e":"code","dc1f1ab8":"code","aed3056a":"code","2c4ba9c9":"code","ba8c3770":"code","28aa18f3":"markdown","6b9c4f91":"markdown","17be3f3b":"markdown"},"source":{"ce4b0707":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9b2d8616":"# importing all the relevant libraries which will be used \nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport matplotlib.colors\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, mean_squared_error, log_loss\nfrom tqdm import tqdm_notebook\nimport seaborn as sns\nimport time\nfrom IPython.display import HTML\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.datasets import make_blobs\n\nimport torch","9afd008a":"# Since we are using random function to create our dataset, setting seed so that results are reproducable. \ntorch.manual_seed(0)","e618f742":"# creating a color map beforhand so that it is easy to visualise data with various colors.\n\nmy_cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\",[\"red\", \"yellow\", \"green\"])","c7a75982":"# Using make_blobs function to create data with 4 centres(4 classes) with 2 features(columns) only with 1000 input rows\ndata, labels = make_blobs(n_samples= 1000, centers= 4, n_features= 2, random_state= 0)\nprint(data.shape,labels.shape)","f2fdb297":"# Visualising data that has been created\nplt.scatter(data[:,0], data[:,1], c= labels, cmap= my_cmap)\nplt.show()","7913bba1":"# Splitting the data into train and test set so that data can be trained on 1 part and tested on the other.\n#The propostion by default is 75% train and 25% test data set\nX_train, X_val, Y_train, Y_val = train_test_split(data,labels, stratify = labels, random_state = 0)\n\n# Visualising both train and test dataset\nprint(X_train.shape, X_val.shape, Y_train.shape, Y_val.shape, labels.shape)","827500c5":"# Since we will be using pytorch to create deep neural network, converting arrays to tensors.\n# using map function to apply same function on list of datasets.\n\nX_train, X_val, Y_train, Y_val = map(torch.tensor,(X_train, X_val, Y_train, Y_val))","d52e74d0":"# defining our model. a1 and a2 are activation functions of 1st and 2nd layer respectively.\n# h1 and h2 are hidden layers of activation function to bring non linearity using sigmoid\n\ndef model(x):\n  a1 = torch.matmul(x,weights1) + bias1 # (N,2) x (2,2) -> (N,2)\n  h1 = a1.sigmoid() # (N,2)\n  a2 = torch.matmul(h1,weights2) + bias2 # (N,2) x (2,4) -> (N,4)\n  h2 = a2.exp()\/a2.exp().sum(-1).unsqueeze(-1)\n  return h2","35c20337":"# defining cross entropy loss function\ndef loss_fn(y_hat,y):\n  return -(y_hat[range(y.shape[0]),y].log()).mean()","95462a5d":"# defining accuracy function\ndef accuracy(y_hat,y):\n  pred = torch.argmax(y_hat,dim =1)\n  return (pred== y).float().mean()","881c246e":"torch.manual_seed(0)\nweights1 = torch.randn(2,2)\/ math.sqrt(2)\nweights1.requires_grad_()\nbias1 = torch.zeros(2, requires_grad= True)\n\nweights2 = torch.randn(2,4) \/ math.sqrt(2)\nweights2.requires_grad_()\nbias2 = torch.zeros(4, requires_grad= True)\n\nlearning_rate = 0.2\nepochs = 1000\n\nX_train = X_train.float()\nY_train = Y_train.long()\n\nloss_arr = []\nacc_arr = []\n\nfor epoch in range(epochs) :\n  y_hat = model(X_train)\n  loss = loss_fn(y_hat, Y_train)\n  loss.backward()\n  loss_arr.append(loss.item())\n  acc_arr.append(accuracy(y_hat, Y_train))\n\n  with torch.no_grad():\n    weights1 -= weights1.grad * learning_rate\n    bias1 -= bias1.grad * learning_rate\n    weights2 -= weights2.grad * learning_rate\n    bias2 -= bias2.grad * learning_rate\n    weights1.grad.zero_()\n    bias1.grad.zero_()\n    weights2.grad.zero_()\n    bias2.grad.zero_()\n\nplt.plot(loss_arr,'r-')\nplt.plot(acc_arr,'b-')\nplt.show()\nprint(\"loss before training\", loss_arr[0])\nprint(\"loss after training\", loss_arr[-1])","dc1f1ab8":"import torch.nn.functional as F\nimport torch.nn as nn\nfrom torch import optim","aed3056a":"# Defining basic class to define the depth and nodes in neural network\n\nclass FirstNetwork(nn.Module):\n\n  def __init__(self):\n    super().__init__()\n    torch.manual_seed(0)\n    self.net = nn.Sequential(\n        nn.Linear(2,2),\n        nn.Sigmoid(),\n        nn.Linear(2,4),\n        nn.Softmax()\n    )\n    \n\n  def forward(self,x):\n    return self.net(x)","2c4ba9c9":"# Defining a fit function separately \ndef fit_v2(x,y,model,opt,loss_fn,epochs = 1000):\n  for epoch in range(epochs):\n    loss = F.cross_entropy(model(x), y)\n\n    loss.backward()\n\n    opt.step()\n    opt.zero_grad()\n  return loss.item()","ba8c3770":"# calling neural network class and invoking cross entropy function. \n#optim is being used to optimise the parameters defined in our class\n\nfn = FirstNetwork()\nloss_fn = F.cross_entropy\nopt = optim.SGD(fn.parameters(), lr =1)\nfit_v2(X_train, Y_train, fn, opt, loss_fn)","28aa18f3":"# **Creating a very basic deep neural network using Pytorch.**\n\n**Sharing a code for creating a very basic deep neural network. It is a very basic 2 layered deep neural network. It can be used to perform classfication model.Those who are new to deep learning models can use this model to create complex deep fully connected neural networks to perform multi class classfication**","6b9c4f91":"# **OPTION 1 Defining model, loss function, feedforward, backpropogation & accuracy functions explicitly**","17be3f3b":"# Option 2 : Using torch.nn module to simplify steps and create neural network"}}