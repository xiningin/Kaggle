{"cell_type":{"45dac068":"code","2fdbd060":"code","d82a1d03":"code","fcb91700":"code","0526c244":"code","6284b9db":"code","e1461b46":"code","2f92683a":"code","b187f76d":"code","be4f41c7":"code","0e7e1d16":"code","6471ffe8":"code","b722d177":"code","8c390c79":"code","910e93fe":"code","2f8705ab":"code","46c89f62":"code","695024e5":"code","ade3c907":"code","79602b1b":"markdown","0500c26d":"markdown","30c756c2":"markdown","1a64b9be":"markdown","c323e2af":"markdown","8bece30d":"markdown","3a9758b3":"markdown","397949f6":"markdown","539f1988":"markdown","b692cefc":"markdown","197e8a75":"markdown","61e5068a":"markdown","3964264b":"markdown","44c91876":"markdown","2ef94916":"markdown"},"source":{"45dac068":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA","2fdbd060":"dataset = pd.read_csv('..\/input\/diamondscsv')\ndataset = dataset.iloc[:,1:]\ndataset.head()","d82a1d03":"dataset.info()","fcb91700":"encoder = OrdinalEncoder()\nencoder.fit(dataset[['cut', 'color', 'clarity']])","0526c244":"dataset[['cut', 'color', 'clarity']] = encoder.transform(dataset[['cut', 'color', 'clarity']])\ndataset.head()","6284b9db":"Y = dataset['price']\nX = dataset.drop('price', axis=1)","e1461b46":"X","2f92683a":"Y","b187f76d":"X_train, X_val, Y_train, Y_val = train_test_split(X, Y, random_state=42, test_size=0.2)","be4f41c7":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)","0e7e1d16":"dataset = pd.DataFrame(X_train, columns=['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'x', 'y', 'z'])\ndataset['price'] = Y_train","6471ffe8":"dataset","b722d177":"corr = dataset.corr()\nplt.figure(figsize=(12, 10))\nsns.heatmap(corr, annot=True, cmap='YlGnBu', vmin=-1, vmax=1)\nplt.show()","8c390c79":"pca = PCA(n_components=X_train.shape[1])\npca_data = pca.fit_transform(X_train)\n\npercent_var_explained = pca.explained_variance_ \/ (np.sum(pca.explained_variance_))\ncum_var_explained = np.cumsum(percent_var_explained)","910e93fe":"plt.plot(cum_var_explained)\nplt.xlabel('n_components')\nplt.ylabel('% Variance Explained')\nplt.grid()\nplt.show()","2f8705ab":"cum_var_explained","46c89f62":"pca.explained_variance_","695024e5":"pca = PCA(n_components=6)\nX_train = pca.fit_transform(X_train)\nX_val = pca.transform(X_val)","ade3c907":"dataset = pd.DataFrame(X_train)\ndataset['price'] = Y_train\n\ncorr = dataset.corr()\nplt.figure(figsize=(12, 10))\nsns.heatmap(corr, annot=True, cmap='YlGnBu', vmin=-1, vmax=1)\nplt.show()","79602b1b":"Also , It can be seen only first 6 has significant values","0500c26d":"It can be seen after n_components =6 there is no % variance explained increase.<br>\nSo, n_components =6","30c756c2":"# Visualize Heatmap Again","1a64b9be":"# Import Dataset","c323e2af":"Now, Multi Collinearity is reduced","8bece30d":"# Train Test Split","3a9758b3":"# Find Input & Output Columns","397949f6":"# PCA","539f1988":"# Import Libraries","b692cefc":"From the heatmap, it can be seen that x,y,z are highly correlated with each other (>0.9).<br>\nTherefore, the data has multicollinearity.<br>\nRemoved by,\n- Drop (Information Loss)\n- PCA","197e8a75":"# Scaling","61e5068a":"# Visualize HeatMap","3964264b":"# Preprocessing","44c91876":"# Multi Collinearity Detection","2ef94916":"Also, cum_var_explained has only first 6 values with steep increase."}}