{"cell_type":{"b0a7cf9e":"code","de80fa4d":"code","cc569152":"code","416ce2f0":"code","34e1c083":"code","231b9f46":"code","4121be66":"code","2670e171":"code","01610c66":"code","1335ad5e":"code","fc575663":"code","a443388a":"code","3b770811":"code","93cce427":"code","4ced8c59":"code","53bce12e":"code","93d9c286":"code","26aa3d75":"code","7e4a1e34":"code","fc8b3a67":"code","2be59107":"code","c59f955b":"code","91706c81":"code","b8574767":"code","727268fb":"code","3e83fa35":"code","cfb985d9":"code","1f134ab4":"code","59024573":"code","6547e39c":"code","f5faa844":"code","b1bc0314":"markdown","3f95d56b":"markdown","3d93b1e0":"markdown","991db50a":"markdown","858057f0":"markdown","e920ff14":"markdown","f66a8cab":"markdown"},"source":{"b0a7cf9e":"import numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\nfrom sklearn.cluster import KMeans\nfrom sklearn import preprocessing\n\nimport scipy as sp;\nimport sklearn;\nimport sys;\nfrom nltk.corpus import stopwords;\nimport nltk;\nfrom gensim.models import ldamodel\nimport gensim.corpora;\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer;\nfrom sklearn.decomposition import NMF;\nfrom sklearn.preprocessing import normalize;\nimport pickle;\nfrom nltk.stem import PorterStemmer,SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom gensim import corpora, models\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","de80fa4d":"data = pd.read_csv(\"data.csv\")","cc569152":"df = data.copy()","416ce2f0":"stemmer = SnowballStemmer('english')","34e1c083":"df[\"clean_txt\"] = df[\"text\"].str.lower().apply(lambda x: re.sub(r\"[-.?!\\\/@#_*,:;()<>|0-9\\.]+\",'',x))","231b9f46":"def lemmatize_stemming(text):\n    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\ndef preprocess(text):\n    result=[]\n    for token in gensim.utils.simple_preprocess(text) :\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            result.append(lemmatize_stemming(token))            \n    return result","4121be66":"processed_docs = df['clean_txt'].map(preprocess)","2670e171":"dictionary = gensim.corpora.Dictionary(processed_docs)\nbow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]","01610c66":"#dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)","1335ad5e":"tfidf = models.TfidfModel(bow_corpus)","fc575663":"corpus_tfidf = tfidf[bow_corpus]","a443388a":"lda_model =  gensim.models.LdaMulticore(bow_corpus, \n                                   num_topics = 5, \n                                   id2word = dictionary,                                    \n                                   passes = 20,\n                                   workers = 2)","3b770811":"lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=5, id2word=dictionary, passes=2, workers=4)\n","93cce427":"for idx, topic in lda_model.print_topics(-1):\n    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n    print(\"\\n\")","4ced8c59":"def topic_assign(unseen_document):\n    bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n    topic = []\n    for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n        #print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 10)))\n        topic.append(lda_model.print_topic(index, 5))\n    return topic","53bce12e":"df[\"topic\"] = df.clean_txt.apply(lambda x: \"\".join(topic_assign(x))[:65])","93d9c286":"uniquelist = df[\"topic\"].unique().tolist()","26aa3d75":"uniquelist","7e4a1e34":"df.replace({'topic': {uniquelist[4]: \"glassdoor_reviews\", uniquelist[3]: \"sports_news\",\n                      uniquelist[2]: \"tech_news\",uniquelist[1]: \"room_rentals\",\n                      uniquelist[0]: \"Automobiles\"}}, inplace = True)","fc8b3a67":"df = df[[\"Id\",\"topic\"]]\n#df.to_csv(\"ketan_750_submission2.csv\", index=False)\ndf.to_csv(\"ketan_750_submission3.csv\", index=False)","2be59107":"for idx, topic in lda_model_tfidf.print_topics(-1):\n    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n    print(\"\\n\")","c59f955b":"def topic_assign_tfidf(unseen_document):\n    bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n    corpus_tfidf = tfidf[bow_vector]\n    topic = []\n    for index, score in sorted(lda_model_tfidf[corpus_tfidf], key=lambda tup: -1*tup[1]):\n        #print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 10)))\n        topic.append(lda_model_tfidf.print_topic(index, 5))\n    return topic\ndf[\"topic\"] = df.clean_txt.apply(lambda x: \"\".join(topic_assign(x))[:65])","91706c81":"df","b8574767":"lda_train2 = gensim.models.ldamulticore.LdaMulticore(\n                           corpus=bow_corpus,\n                           num_topics=5,\n                           id2word=dictionary,\n                           #chunksize=100,\n                           workers=7, # Num. Processing Cores - 1\n                           passes=50,\n                           eval_every = 1,\n                           per_word_topics=True)","727268fb":"for idx, topic in lda_train2.print_topics(-1):\n    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n    print(\"\\n\")","3e83fa35":"def get_topic(unseen_document):\n    bow_corpus = dictionary.doc2bow(unseen_document.split())\n    tpk= lda_train2.get_document_topics(bow_corpus)\n    tpk.sort(key=lambda x:x[1], reverse=True)\n    return tpk[0][0]","cfb985d9":"get_topic(df.clean_txt[0])","1f134ab4":"df[\"topic\"] = df.clean_txt.apply(lambda x: get_topic(x))","59024573":"df[\"topic\"] = df[\"topic\"].astype('str')","6547e39c":"df.replace({'topic': {\"0\": \"Automobiles\", \"2\": \"sports_news\",\n                      \"1\": \"tech_news\",\"4\": \"room_rentals\",\n                      \"3\": \"glassdoor_reviews\"}}, inplace = True)","f5faa844":"df[[\"Id\",\"topic\"]].to_csv(\"submission4_ketan_0785.csv\", index =False)","b1bc0314":"# Assign topics","3f95d56b":"# LDA tweaks","3d93b1e0":"# Data","991db50a":"# LDA","858057f0":"# Topics Printed","e920ff14":"# Ketan Dubbewar | Id_no:0785","f66a8cab":"# with tfidf"}}