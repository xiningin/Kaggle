{"cell_type":{"3a665a72":"code","9255042d":"code","15899977":"code","ba25911c":"code","ea7e1ab8":"code","748c9fd9":"code","ffa53fd4":"code","44f1b038":"code","a2a3997a":"code","5afa99e2":"code","aa175777":"code","d4fd6f6e":"code","f3c9c1e9":"code","45b8fc57":"code","8870779c":"code","f0a2e804":"code","f8468717":"code","d3243715":"code","edee4d97":"code","139e3765":"markdown","dbb68e16":"markdown","9d3515f8":"markdown","284eb5c8":"markdown","2f1c0cc4":"markdown","9ba5889c":"markdown","2169bb98":"markdown","e2cd1032":"markdown","cf01a559":"markdown","f9c995b6":"markdown","bcd6c238":"markdown","9b9e183b":"markdown","86296204":"markdown","76ecd3bb":"markdown","359d9683":"markdown","a122bd3b":"markdown","614df145":"markdown","c953a8ae":"markdown","00996bb7":"markdown","688f920b":"markdown","623018d3":"markdown","5c21ea6e":"markdown","179f4c33":"markdown","02fa259c":"markdown","60fdf9ee":"markdown","0c194892":"markdown","1d30cc3a":"markdown","5cbd2538":"markdown","0ecf6bba":"markdown","0cb4dac4":"markdown","bdbefa95":"markdown","e9515565":"markdown"},"source":{"3a665a72":"import pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nfrom lwoku import RANDOM_STATE, N_JOBS, VERBOSE, get_prediction\nfrom grid_search_utils import plot_grid_search, table_grid_search\n\nimport pickle","9255042d":"# Read training and test files\nX_train = pd.read_csv('..\/input\/learn-together\/train.csv', index_col='Id', engine='python')\nX_test = pd.read_csv('..\/input\/learn-together\/test.csv', index_col='Id', engine='python')\n\n# Define the dependent variable\ny_train = X_train['Cover_Type'].copy()\n\n# Define a training set\nX_train = X_train.drop(['Cover_Type'], axis='columns')","15899977":"xt_clf = ExtraTreesClassifier(verbose=VERBOSE,\n                              random_state=RANDOM_STATE,\n                              n_jobs=N_JOBS)","ba25911c":"parameters = {\n    'n_estimators': [10, 20, 50, 100, 200, 500, 1000, 1200, 1500, 1800, 1900, 2000, 2100, 3000]\n}\nclf = GridSearchCV(xt_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf, all_ranks=True)","ea7e1ab8":"parameters = {\n    'criterion': ['gini', 'entropy']\n}\nclf = GridSearchCV(xt_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","748c9fd9":"parameters = {\n    'max_depth': [1, 2, 5, 8, 13, 21, 34, 53, 54, 55, 89, None]\n}\nclf = GridSearchCV(xt_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","ffa53fd4":"parameters = {\n    'min_samples_split': [2, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377]\n}\nclf = GridSearchCV(xt_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","44f1b038":"parameters = {\n    'min_samples_leaf': [1, 2, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377]\n}\nclf = GridSearchCV(xt_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","a2a3997a":"parameters = {\n    'min_weight_fraction_leaf': [x \/ 10 for x in range(0, 6)]\n}\nclf = GridSearchCV(xt_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","5afa99e2":"parameters = {\n    'max_features': ['auto', 'sqrt', 'log2', 2, 5, 8, 13, 21, 34, None]\n}\nclf = GridSearchCV(xt_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf, all_ranks=True)","aa175777":"parameters = {\n    'max_leaf_nodes': [2, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, None]\n}\nclf = GridSearchCV(xt_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","d4fd6f6e":"parameters = {\n    'min_impurity_decrease': [x \/ 100 for x in range(0, 11)]\n}\nclf = GridSearchCV(xt_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","f3c9c1e9":"parameters = {\n    'bootstrap': [True, False]\n}\nclf = GridSearchCV(xt_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","45b8fc57":"parameters = {\n    'bootstrap': [True],\n    'oob_score': [True, False]\n}\nclf = GridSearchCV(xt_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","8870779c":"parameters = {\n    'warm_start': [True, False]\n}\nclf = GridSearchCV(xt_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","f0a2e804":"parameters = {\n    'class_weight': ['balanced', 'balanced_subsample', None]\n}\nclf = GridSearchCV(xt_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf, all_ranks=True)","f8468717":"xt_clf.max_features = None\nparameters = {\n    'n_estimators': range(1800, 2100, 10)\n}\nclf = GridSearchCV(xt_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","d3243715":"with open('clf.pickle', 'wb') as fp:\n    pickle.dump(clf, fp)","edee4d97":"clf.best_estimator_","139e3765":"More leaf nodes, more score.\nNone has unlimited leaf nodes and has the maximum score.","dbb68e16":"# Exhaustive search","9d3515f8":"# n_estimators\n##### : integer, optional (default=10)\n\nThe number of trees in the forest.","284eb5c8":"# Search over parameters","2f1c0cc4":"# oob_score\n##### : bool, optional (default=False)\n\nWhether to use out-of-bag samples to estimate\nthe generalization accuracy.","9ba5889c":"The best score is with the default value.","2169bb98":"The score increases when the `max_depth` also does.\nThe best score is from 54 or None.\nNone has less fit time.","e2cd1032":"## Export grid search results","cf01a559":"# min_weight_fraction_leaf\n##### : float, optional (default=0.)\n\nThe minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided.","f9c995b6":"# Introduction\n\nThe aim of this notebook is to optimize the Extra-trees model.\n\nFirst, all [Extra-trees classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier) parameters are analysed separately.\n\nThen, a grid search is carried out.\nThis is a search through all the combinations of parameters,\nwhich optimize the internal score in the train set.\n\nThe results are collected at [Tactic 03. Hyperparameter optimization](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization).","bcd6c238":"Gini criterior has a slightly better score.","9b9e183b":"The best score is with the default value.","86296204":"# min_samples_split\n##### : int, float, optional (default=2)\n\nThe minimum number of samples required to split an internal node:\n- If int, then consider `min_samples_split` as the minimum number.\n- If float, then `min_samples_split` is a fraction and\n  `ceil(min_samples_split * n_samples)` are the minimum\n  number of samples for each split.","76ecd3bb":"# warm_start\n##### : bool, optional (default=False)\n\nWhen set to ``True``, reuse the solution of the previous call to fit\nand add more estimators to the ensemble, otherwise, just fit a whole\nnew forest. See :term:`the Glossary <warm_start>`.","359d9683":"# bootstrap\n##### : boolean, optional (default=False)\n\nWhether bootstrap samples are used when building trees. If False, the\nwhole datset is used to build each tree.","a122bd3b":"# max_depth\n##### : integer or None, optional (default=None)\n\nThe maximum depth of the tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples.","614df145":"# criterion\n##### : string, optional (default=\"gini\")\n\nThe function to measure the quality of a split. Supported criteria are\n\"gini\" for the Gini impurity and \"entropy\" for the information gain.","c953a8ae":"As all the categories has the same number of samples,\n`balanced`, `balanced_subsample` and `None` options has the same result.","00996bb7":"The score is the same.\nThe fit time is greater when it's true.\nThe score is less than the score with bootstrap equal to true.","688f920b":"The only parameters, that changing their default values,\nmakes the score better are `max_features` and `n_estimators`.\nAll the others the default value is the right choice.","623018d3":"More features, more score.\nNone has all features and has the maximum score.","5c21ea6e":"This classifier has a good score with 10 estimators.\nIt estabilizes from 500 estimators.\nThen it has some chaotic behaviour and have a maximum score at 1900 estimators.","179f4c33":"# min_impurity_decrease\n##### : float, optional (default=0.)\n\nA node will be split if this split induces a decrease of the impurity\ngreater than or equal to this value.\n\nThe weighted impurity decrease equation is the following::\n\n    N_t \/ N * (impurity - N_t_R \/ N_t * right_impurity\n                        - N_t_L \/ N_t * left_impurity)\n                        \nwhere ``N`` is the total number of samples, ``N_t`` is the number of\nsamples at the current node, ``N_t_L`` is the number of samples in the\nleft child, and ``N_t_R`` is the number of samples in the right child.\n\n``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\nif ``sample_weight`` is passed.","02fa259c":"# max_leaf_nodes\n##### : int or None, optional (default=None)\n\nGrow trees with ``max_leaf_nodes`` in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nIf None then unlimited number of leaf nodes.","60fdf9ee":"# Prepare data","0c194892":"The best score is with the default value.","1d30cc3a":"The best score is with the default value.","5cbd2538":"The best score is with the default value.","0ecf6bba":"# min_samples_leaf\n##### : int, float, optional (default=1)\n\nThe minimum number of samples required to be at a leaf node.\nA split point at any depth will only be considered if it leaves at\nleast ``min_samples_leaf`` training samples in each of the left and\nright branches.  This may have the effect of smoothing the model,\nespecially in regression.\n\n- If int, then consider `min_samples_leaf` as the minimum number.\n- If float, then `min_samples_leaf` is a fraction and\n  `ceil(min_samples_leaf * n_samples)` are the minimum\n  number of samples for each node.","0cb4dac4":"# max_features\n##### : int, float, string or None, optional (default=\"auto\")\n\nThe number of features to consider when looking for the best split:\n- If int, then consider `max_features` features at each split.\n- If float, then `max_features` is a fraction and\n  `int(max_features * n_features)` features are considered at each\n  split.\n- If \"auto\", then `max_features=sqrt(n_features)`.\n- If \"sqrt\", then `max_features=sqrt(n_features)`.\n- If \"log2\", then `max_features=log2(n_features)`.\n- If None, then `max_features=n_features`.\n\n\nNote: the search for a split does not stop until at least one\nvalid partition of the node samples is found, even if it requires to\neffectively inspect more than ``max_features`` features.","bdbefa95":"# class_weight\n##### : dict, list of dicts, \"balanced\", \"balanced_subsample\" or None, optional (default=None)\n\nWeights associated with classes in the form ``{class_label: weight}``.\nIf not given, all classes are supposed to have weight one. For\nmulti-output problems, a list of dicts can be provided in the same\norder as the columns of y.\n\nNote that for multioutput (including multilabel) weights should be\ndefined for each class of every column in its own dict. For example,\nfor four-class multilabel classification weights should be\n[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n[{1:1}, {2:5}, {3:1}, {4:1}].\n\nThe \"balanced\" mode uses the values of y to automatically adjust\nweights inversely proportional to class frequencies in the input data\nas ``n_samples \/ (n_classes * np.bincount(y))``\n\nThe \"balanced_subsample\" mode is the same as \"balanced\" except that weights are\ncomputed based on the bootstrap sample for every tree grown.\n\nFor multi-output, the weights of each column of y will be multiplied.\n\nNote that these weights will be multiplied with sample_weight (passed\nthrough the fit method) if sample_weight is specified.","e9515565":"It has the same score."}}