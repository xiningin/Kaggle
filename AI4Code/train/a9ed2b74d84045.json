{"cell_type":{"1fc3724b":"code","8f779571":"code","94da59c6":"code","579959c9":"code","947edb51":"code","6398f3f6":"code","806cfd16":"code","cf403cd0":"markdown","0d1582b1":"markdown","6484d0e8":"markdown","97141a8f":"markdown"},"source":{"1fc3724b":"!pip uninstall -y shap","8f779571":"!pip install https:\/\/github.com\/ceshine\/shap\/archive\/master.zip","94da59c6":"import torch, torchvision\nfrom torchvision import datasets, transforms\nfrom torch import nn, optim\nfrom torch.nn import functional as F\n\nimport numpy as np\nimport shap","579959c9":"batch_size = 128\nnum_epochs = 2\ndevice = torch.device('cpu')\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(1, 10, kernel_size=5),\n            nn.MaxPool2d(2),\n            nn.ReLU(),\n            nn.Conv2d(10, 20, kernel_size=5),\n            nn.Dropout(),\n            nn.MaxPool2d(2),\n            nn.ReLU(),\n        )\n        self.fc_layers = nn.Sequential(\n            nn.Linear(320, 50),\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.Linear(50, 10),\n            nn.Softmax(dim=1)\n        )\n\n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = x.view(-1, 320)\n        x = self.fc_layers(x)\n        return x\n    \ndef train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output.log(), target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 100 == 0:\n            print('Train Epoch: {} [{}\/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx \/ len(train_loader), loss.item()))\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output.log(), target).item() # sum up batch loss\n            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss \/= len(test_loader.dataset)\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}\/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n    100. * correct \/ len(test_loader.dataset)))\n\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('mnist_data', train=True, download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor()\n                   ])),\n    batch_size=batch_size, shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('mnist_data', train=False, transform=transforms.Compose([\n                       transforms.ToTensor()\n                   ])),\n    batch_size=batch_size, shuffle=True)\n\nmodel = Net().to(device)\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n\nfor epoch in range(1, num_epochs + 1):\n    train(model, device, train_loader, optimizer, epoch)\n    test(model, device, test_loader)","947edb51":"# since shuffle=True, this is a random sample of test data\nbatch = next(iter(test_loader))\nimages, _ = batch\n\nbackground = images[:100]\ntest_images = images[100:103]\n\ne = shap.DeepExplainer(model, background)\nshap_values = e.shap_values(test_images)","6398f3f6":"shap_numpy = [np.swapaxes(np.swapaxes(s, 1, -1), 1, 2) for s in shap_values]\ntest_numpy = np.swapaxes(np.swapaxes(test_images.numpy(), 1, -1), 1, 2)","806cfd16":"# plot the feature attributions\nshap.image_plot(shap_numpy, -test_numpy)","cf403cd0":"The plot above shows the explanations for each class on four predictions. Note that the explanations are ordered for the classes 0-9 going left to right along the rows.","0d1582b1":"# PyTorch Deep Explainer MNIST example\n\nA simple example showing how to explain an MNIST CNN trained using PyTorch with Deep Explainer.","6484d0e8":"### Install the modified SHAP package","97141a8f":"### Proceed"}}