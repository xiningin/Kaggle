{"cell_type":{"188b566e":"code","889886fc":"code","527eb6cd":"code","4a1e8f72":"code","b3dac883":"code","df3fbd6b":"code","9bfae725":"code","c68f4853":"code","6c171841":"code","17263f08":"code","c911c92d":"code","00abbefd":"code","25ede0cf":"code","e664bab7":"code","5e4282e0":"markdown","bedc6b9a":"markdown","70d7ff73":"markdown","2e7c06c9":"markdown","5d52c4df":"markdown"},"source":{"188b566e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","889886fc":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport nltk\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords\n","527eb6cd":"reviews = pd.read_csv(\"\/kaggle\/input\/google-play-store-apps\/googleplaystore_user_reviews.csv\").dropna()\nreviews.info()","4a1e8f72":"reviews.loc[reviews['Sentiment_Polarity'] >0,'Positivity']=1\nreviews.loc[reviews['Sentiment_Polarity'] ==0,'Positivity']=0\nreviews.loc[reviews['Sentiment_Polarity'] <0,'Positivity']=-1\n\nreviews.head(5)","b3dac883":"data = pd.concat([reviews.Translated_Review,reviews.Positivity],axis=1)\ndata.head(3)","df3fbd6b":"def prepare(data):\n    CleanData=[]\n\n    for descr in data.Translated_Review:\n        descr=re.sub(\"[^a-zA-Z]\",\" \",descr) \n        descr=descr.lower()\n        descr = nltk.word_tokenize(descr)\n        descr = [word for word in descr if not word in set(stopwords.words(\"english\"))] \n        lemma = nltk.WordNetLemmatizer()\n        descr = [lemma.lemmatize(word) for word in descr]\n        descr = \" \".join(descr)\n        CleanData.append(descr)\n    return CleanData\n    \nCleanData=prepare(data)","9bfae725":"max_features= 300\n\ncount_vectorizer = CountVectorizer(max_features=max_features,stop_words=\"english\")\nsparce_matrix=count_vectorizer.fit_transform(CleanData).toarray()\nprint(\"Most used {} word: {}\".format(max_features,count_vectorizer.get_feature_names()))\n#%%\ny= data.iloc[:,1].values # Disaster or not\nx=sparce_matrix #Texts for training\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.01,random_state=42)\n","c68f4853":"#%%Naive bayes \nnb = GaussianNB()\nnb.fit(x_train,y_train)\n#%% Logistic Regression\nfrom sklearn import linear_model\nlogreg = linear_model.LogisticRegression(random_state=42,max_iter=100, C=10)\nlogreg.fit(x_train,y_train.T)","6c171841":"print(\"Test accuracy of naive bayes: \",nb.score(x_test,y_test))\nprint(\"Test accuracy of Logistic Regression:  \",logreg.score(x_test,y_test.T))","17263f08":"from keras.utils import to_categorical \ntrain=to_categorical(y_train+1)\ntest =to_categorical(y_test+1)\nprint(test)","c911c92d":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.layers.core import Dropout\nfrom keras.callbacks import ModelCheckpoint\nmodel = Sequential()\nmodel.add(Dense(input_dim=x_train.shape[1],\n                output_dim = train.shape[1],\n                init =   'uniform',\n                activation = 'relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(200, kernel_initializer='lecun_uniform'))\nmodel.add(Activation('relu'))\nmodel.add(Dense(train.shape[1], kernel_initializer='uniform'))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss = \"binary_crossentropy\", optimizer = 'adamax',metrics=[\"accuracy\"])\nmodel.fit(x_train,\n          train,\n          epochs = 200,\n          batch_size = 1000,\n          validation_data = (x_test,test),\n          verbose=1)","00abbefd":"print(\"Test loss and accuracy of Neural Network: \",model.evaluate(x_test,test))","25ede0cf":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10,10))\n\nplt.plot(model.history.history['loss'])\nplt.plot(model.history.history['val_loss'])\nplt.plot(model.history.history['accuracy'])\nplt.plot(model.history.history['val_accuracy'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train Loss', 'Test Loss','Train Accuracy','Test Accyracy'], loc='upper left')\nplt.show()","e664bab7":"y_axis=[nb.score(x_test,y_test),logreg.score(x_test,y_test.T),model.evaluate(x_test,test)[1]]\nx_axis=[\"Naive Bayes\",\"Logistic Regression\",\"Neural Network\"]\n\nfig,ax=plt.subplots(figsize=(10,6))\nax.bar(x_axis,y_axis)\nplt.show()","5e4282e0":"Comparing Naive Bayes, Logistic Regeression and Neural Network accuracies","bedc6b9a":"PLOT TRAINING","70d7ff73":"Label every text if it is positive, neutral or negative comment as 1, 0 and -1","2e7c06c9":"**Clean, Lemmatize Data**","5d52c4df":"**Neural Network reaches up to %87 test accuracy which is very nice. Adding layers don't change loss and without initializing first weights with lecun uniform model is limits itself by %80 accuracy. Model maybe improved using BERT model and other initializer-regularizer combinations.**"}}