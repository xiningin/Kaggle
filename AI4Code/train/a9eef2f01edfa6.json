{"cell_type":{"f41ddb4e":"code","cd8a2773":"code","a2f9e376":"code","a31c9135":"code","ade3ac63":"code","c1a46013":"code","059463e0":"code","e1fa2889":"code","adc5c94b":"code","b025ab2e":"code","fa65b0aa":"code","c704fcb4":"code","84de714a":"code","a0f49b49":"code","15a8966d":"code","1b659f9e":"code","5d2762fa":"code","f7d028b3":"code","16b5a7b7":"code","4a97e149":"code","9c31d0c1":"code","b3ca687f":"code","ed30618f":"code","dd7b1853":"code","335e5a92":"code","6c0b3367":"code","b53db96a":"code","a8f70caa":"code","ff442833":"code","3440035f":"code","4100a238":"code","784915ed":"markdown","5c491bae":"markdown","83aa0faa":"markdown","4a1e041a":"markdown","ac938ef2":"markdown","0f3a2fb2":"markdown","428e1d7c":"markdown","696bb853":"markdown","34d59b8c":"markdown"},"source":{"f41ddb4e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nsns.set()\n%matplotlib inline","cd8a2773":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a2f9e376":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","a31c9135":"train_data.Sex.unique()","ade3ac63":"train_data.Embarked.unique()","c1a46013":"train_data.info()","059463e0":"test_data.info()","e1fa2889":"#change the Sex data from \"male\" to 0 and from \"female\" to 1\ngender = {\"female\": 1, \"male\": 0}\ntrain_data['Sex']= train_data['Sex'].map(gender)\ntest_data['Sex']= test_data['Sex'].map(gender)","adc5c94b":"#change Embarked Value to numbers\ntrain_data[\"Embarked\"].fillna(\"S\", inplace=True)\nEmbark = {\"S\": 1, \"C\": 2, \"Q\":3}\ntrain_data['Embarked']= train_data['Embarked'].map(Embark)\ntest_data['Embarked']= test_data['Embarked'].map(Embark)","b025ab2e":"#create correlation\ncorr = train_data.corr(method ='pearson')\nprint(corr)","fa65b0aa":"#Plot on heatmap\nmask = np.triu(np.ones_like(corr, dtype=bool))\nf, ax = plt.subplots(figsize=(11, 9))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","c704fcb4":"#Examine the importance of Pclass first before building the model\nsns.set()\nplt.figure(figsize=(10, 8))\nax = sns.countplot(x = \"Pclass\",hue=\"Survived\", data=train_data)\nax.set_title(\"Survived by Passenger Class\")\nplt.xlabel(\"Class Accomodation\")\nplt.ylabel(\"Passengers\")\nplt.show()\nprint (\"Many Passengers did not survived in the lower deck\")","84de714a":"#Examine the importance of Siblings and Spouse first before building the model\nsns.set()\nplt.figure(figsize=(10, 8))\nax = sns.countplot(x = \"SibSp\",hue=\"Survived\", data=train_data)\nax.set_title(\"Survived by Siblings and Spouse\")\nplt.xlabel(\"Number of Siblings and Spouse\")\nplt.ylabel(\"Passengers\")\nplt.show()\nprint (\"Many Single Passengers survived\")","a0f49b49":"#Examine the importance of Age first before building the model\nsns.set()\nplt.figure(figsize=(20, 30))\nax = sns.countplot(y = \"Age\",hue=\"Survived\", data=train_data)\nax.set_title(\"Survived by Age\")\nplt.xlabel(\"Passengers\")\nplt.ylabel(\"Age\")\nplt.show()\nprint (\"Many infants but few old-aged survived. Passengers from teens to 50's have less importance in the survivability rate\")","15a8966d":"#Import various ML Algorithm Library and test accuracy each\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n#Import validators\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error\n#Import Hyperparameter Tuning\nfrom sklearn.model_selection import RandomizedSearchCV\n#import tarin and test split\nfrom sklearn.model_selection import train_test_split","1b659f9e":"print(train_data.isnull().sum())","5d2762fa":"print(test_data.isnull().sum())","f7d028b3":"tmp1 = test_data[\"Age\"].mean()\ntmp2 = train_data[\"Age\"].mean()","16b5a7b7":"train_data[\"Age\"].fillna((tmp1 + tmp2)\/2, inplace=True)","4a97e149":"test_data[\"Age\"].fillna((tmp1 + tmp2)\/2, inplace=True)\ntest_data[\"Fare\"].fillna(34, inplace=True)","9c31d0c1":"#Prepare X and Y data and the needed features\n#split dataset to evaluate the model\ntrain_cols = train_data[\"Survived\"]","b3ca687f":"#Identify Features needed for the model\nfeatures = [\"SibSp\", \"Sex\", \"Parch\", \"Embarked\", \"Age\", \"Fare\"]\ntrain_rows = pd.get_dummies(train_data[features])\nx_train, x_test, y_train, y_test = train_test_split(train_rows,train_cols, test_size=0.2, random_state=1)","ed30618f":"#final test data\ntest_rows = pd.get_dummies(test_data[features])","dd7b1853":"#Test on K Nearest Neighbor\nmodel = KNeighborsClassifier()\nmodel.fit(x_train, y_train)\npredictions = model.predict(x_test)\n\nprint(\"Accuracy Score:\",model.score(x_test,y_test))\n\ncross = cross_val_score(model, x_test, y_test, cv=5)\nprint(\"Cross Validation Score:\",cross.mean())\nprint(\"Mean Absolute Error :\",mean_absolute_error(y_test, predictions))","335e5a92":"#Test on Support Vector Classification\nmodel = SVC()\nmodel.fit(x_train, y_train)\npredictions = model.predict(x_test)\n\nprint(\"Accuracy Score:\",model.score(x_test,y_test))\n\ncross = cross_val_score(model, x_test, y_test, cv=5)\nprint(\"Cross Validation Score:\",cross.mean())\nprint(\"Mean Absolute Error :\",mean_absolute_error(y_test, predictions))","6c0b3367":"#Test on Decision Tree\nmodel = DecisionTreeClassifier(random_state=1)\nmodel.fit(x_train, y_train)\npredictions = model.predict(x_test)\n\nprint(\"Accuracy Score:\",model.score(x_test,y_test))\n\ncross = cross_val_score(model, x_test, y_test, cv=5)\nprint(\"Cross Validation Score:\",cross.mean())\nprint(\"Mean Absolute Error :\",mean_absolute_error(y_test, predictions))","b53db96a":"# Test on Logistic Regression\nmodel = LogisticRegression(random_state=1)\nmodel.fit(x_train, y_train)\npredictions = model.predict(x_test)\n\nprint(\"Accuracy Score:\",model.score(x_test,y_test))\n\ncross = cross_val_score(model, x_test, y_test, cv=5)\nprint(\"Cross Validation Score:\",cross.mean())\nprint(\"Mean Absolute Error :\",mean_absolute_error(y_test, predictions))","a8f70caa":"# Test on Random Forest Classifier \nmodel = RandomForestClassifier(random_state=1)\nmodel.fit(x_train, y_train)\npredictions = model.predict(x_test)\n\nprint(\"Accuracy Score:\",model.score(x_test,y_test))\n\ncross = cross_val_score(model, x_test, y_test, cv=5)\n\nprint(\"Cross Validation Score:\",cross.mean())\nprint(\"Mean Absolute Error :\",mean_absolute_error(y_test, predictions))","ff442833":"param_RFC={'max_depth': [2, 5, 7, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n 'max_features': ['auto', 'sqrt', 'log2'],\n 'class_weight':['balanced', 'balanced_subsample'],\n 'n_estimators': [50, 100, 200, 400, 600, 800, 1000,1400, 1600, 2000]}\n\nparam_LR={'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n 'penalty': ['none', 'l1', 'l2', 'elasticnet'],\n 'max_iter': [50, 100, 200, 400, 600, 800, 1000,1400, 1600, 2000],\n 'C': [100, 10, 1.0, 0.1, 0.01]}\n\nmodelRF = RandomForestClassifier(random_state=1)\ntuned_modelRF=RandomizedSearchCV(modelRF,param_RFC,cv=5,verbose=-1,random_state=1)\ntuned_modelRF.fit(x_train,y_train)\n\nmodelLR = LogisticRegression(random_state=1)\ntuned_modelLR=RandomizedSearchCV(modelLR,param_LR,cv=5,verbose=-1,random_state=1)\ntuned_modelLR.fit(x_train,y_train)\n\nprint('Random Forest Classifier: ', tuned_modelRF.best_params_)\nprint('Logisitic Regression: ', tuned_modelLR.best_params_)","3440035f":"# Now we will compare with tuned parameters into consideration\n\ntuned_model = LogisticRegression(solver='liblinear', penalty='l2', max_iter=400, C=100, random_state=1)\ntuned_model.fit(x_train, y_train)\ntuned_predictions = tuned_model.predict(x_test)\ntuned_cross = cross_val_score(tuned_model, x_test, y_test, cv=5)\nprint(\"Logistic Regression Cross Validation Score (tuned):\",tuned_cross.mean())\nprint(\"Logistic Regression  Mean Absolute Error (tuned) :\",mean_absolute_error(tuned_predictions, y_test))\n\ntuned_model = RandomForestClassifier( n_estimators=100, max_features='log2', max_depth=10, class_weight='balanced', random_state=1)\ntuned_model.fit(x_train, y_train)\ntuned_predictions = tuned_model.predict(x_test)\ntuned_cross = cross_val_score(tuned_model, x_test, y_test, cv=5)\nprint(\"Random Forest Cross Validation Score (tuned):\",tuned_cross.mean())\nprint(\"Random Forest Mean Absolute Error (tuned) :\",mean_absolute_error(tuned_predictions, y_test))","4100a238":"# Predict all test data using the fine tuned classifier\n\nfrom IPython.display import HTML\n\n#model = RandomForestClassifier( n_estimators=100, max_features='log2', max_depth=10, class_weight='balanced', random_state=1)\nmodel = LogisticRegression(solver='liblinear', penalty='l2', max_iter=400, C=100, random_state=1)\n\nmodel.fit(train_rows, train_cols)\nfinalpredict = model.predict(test_rows)\ncross = cross_val_score(model, test_rows, finalpredict, cv=5)\nprint(\"Cross Validation Score (tuned):\",cross.mean())\n\n#Save to CSV file\nresult = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': finalpredict})\nresult.to_csv('Titanic-Pred-LR.csv', index=False)\ndef create_download_link(title = \"Download CSV file\", filename = \"data.csv\"): \n    html = '<a href={filename}>{title}<\/a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\ncreate_download_link(filename='Titanic-Pred-LR.csv')","784915ed":"# Model Development and Evaluation","5c491bae":"# Method: Logistic Regression","83aa0faa":"# Method: Decision Tree","4a1e041a":"# MODEL TUNING","ac938ef2":"### We have noticed Logistic Regression and Random Forest methods have shown better accuracies.\n\nWe will now proceed with fine tuning of these two methods","0f3a2fb2":"# Method: Support Vector Machine (SVM)","428e1d7c":"# Method: k-Nearest Neighbor (kNN)","696bb853":"# Method: Random Forest","34d59b8c":"# Titanic Prediction"}}