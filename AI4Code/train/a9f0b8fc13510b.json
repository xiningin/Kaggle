{"cell_type":{"51e26f2e":"code","73606eb9":"code","d69f6238":"code","81b24602":"code","e48ea8cd":"code","f98f43c2":"code","040f5992":"code","8b573c9e":"code","7ef020f0":"code","2982e82f":"code","f4c7130b":"code","b039df6c":"code","7fc38b92":"code","e273bf45":"code","b05519e4":"code","cfef1dbe":"code","f09a3c7a":"code","cdc9b670":"code","8d8b446b":"code","ef42ff04":"code","ae367b87":"code","c19d9378":"code","353f75b7":"code","0044c524":"code","be410fce":"code","8e52fe88":"code","dc2aa3ef":"code","1132c50c":"code","fbe45f58":"code","65df8770":"code","ad09dc2c":"markdown","c263f5e2":"markdown","5e3848de":"markdown","dfcf1078":"markdown","4298df72":"markdown","c67c8428":"markdown","7c2b159d":"markdown","73c50acb":"markdown","0e7e179b":"markdown","abf84c13":"markdown","ec59bdf6":"markdown","1ee9de94":"markdown","c1f01859":"markdown","c74c1b6a":"markdown","d588ba78":"markdown","6938213e":"markdown","3a59607e":"markdown","530664fe":"markdown","9fcb45ec":"markdown","63a96249":"markdown","f1296a1a":"markdown","d897746b":"markdown","68798089":"markdown","4be87bf2":"markdown","e502caca":"markdown"},"source":{"51e26f2e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', 50)\npd.set_option('display.max_rows', 150)\nimport os\nimport gc\ngc.enable()\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom collections import Counter\nimport ast\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom textblob import TextBlob\nimport scipy.stats as stats\n\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport matplotlib.patches as mpatches\nimport time\n\nimport seaborn as sns #for making plots\nimport matplotlib.pyplot as plt # for plotting\nimport os  # for os commands\nfrom sklearn.manifold import TSNE\n","73606eb9":"## Common Variables for Notebook \nROOT = '\/kaggle\/input\/nlp-getting-started\/'\n\n## load the data \ndf_train = pd.read_csv(ROOT+'train.csv')\ndf_test = pd.read_csv(ROOT+'test.csv')\ndf_sub = pd.read_csv(ROOT+'sample_submission.csv')","d69f6238":"#Looking data format and types\nprint(df_train.info())\nprint(df_test.info())\nprint(df_sub.info())","81b24602":"#Some Statistics\ndf_train.describe()","e48ea8cd":"#Take a look at the data\ndf_train.head()","f98f43c2":"target = df_train['target']\nsns.set_style('whitegrid')\nplt.figure(figsize=(3,5))\nsns.countplot(target)","040f5992":"df_train[\"text\"].head()","8b573c9e":"#To check the text content we can use a list\ndf_train[\"text\"].tolist()[:5]","7ef020f0":"t = df_train[\"text\"].to_list()\nfor i in range(5):\n    print('Tweet Number '+str(i+1)+': '+t[i])","2982e82f":"l = df_train[\"location\"].to_list()\nprint('There is '+ str(len(set(l)))+ ' different loction')","f4c7130b":"df_train['location'].value_counts().head(n=20)","b039df6c":"# Plotting a bar graph of the number of tweets in each location, for the first ten locations listed\n# in the column 'location'\nlocation_count  = df_train['location'].value_counts()[:10,]\nplt.figure(figsize=(10,5))\nsns.barplot(location_count.index, location_count.values, alpha=0.9)\nplt.title('Top 10 locations')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('location', fontsize=12)\nplt.show()","7fc38b92":"# Plotting a bar graph of the number of tweets in each location, for the first ten locations listed\n# in the column 'location'\nlocation_count  = df_test['location'].value_counts()[:10,]\nplt.figure(figsize=(10,5))\nsns.barplot(location_count.index, location_count.values, alpha=0.8)\nplt.title('Top 10 locations')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('location', fontsize=12)\nplt.show()","e273bf45":"df = df_train[df_train['location'].notnull()]","b05519e4":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ndf['location'] = le.fit_transform(df.location.values)","cfef1dbe":"# tsne code from this great kernel: https:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets\n# New_df is from the random undersample data (fewer instances)\nX = df['location']\ny = df['target']\n\n\n# T-SNE Implementation\nt0 = time.time()\nX_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values.reshape(-1, 1))\nt1 = time.time()\nprint(\"T-SNE took {:.2} s\".format(t1 - t0))\n","f09a3c7a":"f, (ax1) = plt.subplots(1, 1, figsize=(12,8))\n# labels = ['Not Fake', 'Fake']\nf.suptitle('Clusters using Dimensionality Reduction', fontsize=14)\n\n\nblue_patch = mpatches.Patch(color='#0A0AFF', label='Not Fake')\nred_patch = mpatches.Patch(color='#AF0000', label='Fake')\n\n\n# t-SNE scatter plot\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 0), cmap='coolwarm', label='Not Fake', linewidths=2)\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 1), cmap='coolwarm', label='Fake', linewidths=2)\nax1.set_title('t-SNE', fontsize=14)\n\nax1.grid(True)\n\nax1.legend(handles=[blue_patch, red_patch])\n\n\nplt.show()","cdc9b670":"df_train['keyword'].value_counts().head(n=20)","8d8b446b":"# Plotting a bar graph of the number of tweets in each keyword, for the first ten keywords listed\nkeyword_count  = df_train['keyword'].value_counts()[:10,]\nplt.figure(figsize=(12,5))\nsns.barplot(keyword_count.index, keyword_count.values, alpha=0.9)\nplt.title('Top 10 keywords')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('keyword', fontsize=12)\nplt.show()","ef42ff04":"df_test['keyword'].value_counts().head(n=20)","ae367b87":"# Plotting a bar graph of the number of tweets in each keyword, for the first ten keywords listed\nkeyword_count  = df_test['keyword'].value_counts()[:10,]\nplt.figure(figsize=(12,5))\nsns.barplot(keyword_count.index, keyword_count.values, alpha=0.9)\nplt.title('Top 10 keywords')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('keyword', fontsize=12)\nplt.show()","c19d9378":"keyword_train  = list(set(df_train['keyword']))\nkeyword_test  = list(set(df_test['keyword']))\n\nprint(len(keyword_train))\nprint(len(keyword_test))","353f75b7":"def intersection(lst1, lst2): \n    lst3 = [value for value in lst1 if value in lst2] \n    return lst3 \n\nlen(intersection(keyword_train, keyword_test))","0044c524":"df = df_train[df_train['keyword'].notnull()]\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ndf['keyword'] = le.fit_transform(df.keyword.values)","be410fce":"# tsne code from this great kernel: https:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets\n# New_df is from the random undersample data (fewer instances)\nX = df['keyword']\ny = df['target']\n\n\n# T-SNE Implementation\nt0 = time.time()\nX_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values.reshape(-1, 1))\nt1 = time.time()\nprint(\"T-SNE took {:.2} s\".format(t1 - t0))","8e52fe88":"f, (ax1) = plt.subplots(1, 1, figsize=(12,8))\n# labels = ['Not Fake', 'Fake']\nf.suptitle('Clusters using Dimensionality Reduction', fontsize=14)\n\nX_reduced_tsne\nblue_patch = mpatches.Patch(color='#0A0AFF', label='Not Fake')\nred_patch = mpatches.Patch(color='#AF0000', label='Fake')\n\n\n# t-SNE scatter plot\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 0), cmap='coolwarm', label='Not Fake', linewidths=2)\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 1), cmap='coolwarm', label='Fake', linewidths=2)\nax1.set_title('t-SNE', fontsize=14)\n\nax1.grid(True)\n\nax1.legend(handles=[blue_patch, red_patch])\n\n\nplt.show()","dc2aa3ef":"df_train[\"text\"]","1132c50c":"len(set(df_train['text']))","fbe45f58":"df_test['text']","65df8770":"len(set(df_test['text']))","ad09dc2c":"# Intersection","c263f5e2":"# t-Distributed Stochastic Neighbor Embedding (tsne) to check Dimentionality Reduction (Keywors)","5e3848de":"# Exploring the data:\n# Let's check target distrubution\n# Check for Class Imbalance","dfcf1078":"I think there is not a big difference in location between train data and test data","4298df72":"# So again, what about test data?","c67c8428":"It's clear that the rest of the texts are hided because of the test length, I will put the column in a list to check the first 5 texts","7c2b159d":"We can see some clearly blue dots which represent real events","73c50acb":"# Let's do some text analysis","0e7e179b":"=> There is 3342 different loction from which the tweets have been posted","abf84c13":"=> There is a little difference between keywords in train set and test set, so I will check the intersection between the keywrds in train and test","ec59bdf6":"With a hard mathemitical computing we can know that there is 7613-7503=110 tweets which are duplicated :D","1ee9de94":"The intersection between the two datasets give us the same number of unique keywords, so the keywords used in both datasets with different occurrence","c1f01859":"# t-Distributed Stochastic Neighbor Embedding (tsne) to check Dimentionality Reduction (Location)","c74c1b6a":"Let's make it more perfect","d588ba78":"# Let's do the same for test set","6938213e":"# Tweets Locations","3a59607e":"# Count Locations","530664fe":"# Check Text Content","9fcb45ec":"3263-3243=20 tweets which are duplicated :D","63a96249":"# First look at the data","f1296a1a":"I want to check if there is some duplicated tweets, it could be a retweet. If we know that a tweet is fake or not so the other duplicated tweets will get the same class","d897746b":"# Now let's check keywords\n# Count keyword\n# Top 20 keywords","68798089":"There is 222 unique values in both train and test sets","4be87bf2":"# Label Encoding location column","e502caca":"# What about test data?"}}