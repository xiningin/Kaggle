{"cell_type":{"9de7cc27":"code","1884e77e":"code","78553441":"code","3f1e45d9":"code","4105e1bf":"code","f70cf4ab":"code","a7fa5f3d":"code","05c82fa3":"code","9fed0047":"code","f56ee132":"code","0ba45f3f":"code","75a410fc":"markdown","7350e67a":"markdown","d038fbbe":"markdown","74ce4440":"markdown","cd1a639e":"markdown","8c6d0037":"markdown","d9f08240":"markdown","4c177ca6":"markdown","2da156fe":"markdown"},"source":{"9de7cc27":"#IMPORTING REQUIRED MODULES\nimport pandas as pd\npd.options.display.max_columns = None\npd.set_option('display.float_format', lambda x: '%.6f' % x)\nfrom matplotlib import pyplot as plt\nplt.style.use('ggplot')\nfrom sklearn.cluster import KMeans\nimport seaborn as sns\nimport colorlover as cl\nimport plotly as py\nimport plotly.graph_objs as go\npy.offline.init_notebook_mode(connected = True)","1884e77e":"df = pd.read_csv('..\/input\/Mall_Customers.csv', index_col=0)\n#changing column names for better manipulability\ndf = df.rename(columns={'Gender': 'gender', 'Age': 'age', 'Annual Income (k$)': 'annual_income', 'Spending Score (1-100)': 'spending_score'})\ndf['gender'].replace(['Female','Male'], [0,1],inplace=True)\ndf.head()","78553441":"#SCALING\n#stocking mean and standard deviation in a dataframe (we will need them for unscaling)\ndfsp = pd.concat([df.mean().to_frame(), df.std().to_frame()], axis=1).transpose()\ndfsp.index = ['mean', 'std']\n#new dataframe with scaled values\ndf_scaled = pd.DataFrame()\nfor c in df.columns:\n    if(c=='gender'): df_scaled[c] = df[c]\n    else: df_scaled[c] = (df[c] - dfsp.loc['mean', c]) \/ dfsp.loc['std', c]\ndf_scaled.head()","3f1e45d9":"#the two \"intuitive\" clusters\ndff = df_scaled.loc[df_scaled.gender==0].iloc[:, 1:] #no need of gender column anymore\ndfm = df_scaled.loc[df_scaled.gender==1].iloc[:, 1:]","4105e1bf":"def number_of_clusters(df):\n\n    wcss = []\n    for i in range(1,20):\n        km=KMeans(n_clusters=i, random_state=0)\n        km.fit(df)\n        wcss.append(km.inertia_)\n\n    df_elbow = pd.DataFrame(wcss)\n    df_elbow = df_elbow.reset_index()\n    df_elbow.columns= ['n_clusters', 'within_cluster_sum_of_square']\n\n    return df_elbow\n\ndfm_elbow = number_of_clusters(dfm)\ndff_elbow = number_of_clusters(dff)\n\nfig, ax = plt.subplots(1, 2, figsize=(17,5))\n\nsns.lineplot(data=dff_elbow, x='n_clusters', y='within_cluster_sum_of_square', ax=ax[0])\nsns.scatterplot(data=dff_elbow[5:6], x='n_clusters', y='within_cluster_sum_of_square', color='black', ax=ax[0])\nax[0].set(xticks=dff_elbow.index)\nax[0].set_title('Female')\n\nsns.lineplot(data=dfm_elbow, x='n_clusters', y='within_cluster_sum_of_square', ax=ax[1])\nsns.scatterplot(data=dfm_elbow[5:6], x='n_clusters', y='within_cluster_sum_of_square', color='black', ax=ax[1])\nax[1].set(xticks=dfm_elbow.index)\nax[1].set_title('Male');","f70cf4ab":"def k_means(n_clusters, df, gender):\n\n    kmf = KMeans(n_clusters=n_clusters, random_state=0) #defining the algorithm\n    kmf.fit_predict(df) #fitting and predicting\n    centroids = kmf.cluster_centers_ #extracting the clusters' centroids\n    cdf = pd.DataFrame(centroids, columns=df.columns) #stocking in dataframe\n    cdf['gender'] = gender\n    return cdf\n\ndf1 = k_means(5, dff, 'female')\ndf2 = k_means(5, dfm, 'male')\ndfc_scaled = pd.concat([df1, df2])\ndfc_scaled.head()","a7fa5f3d":"#UNSCALING\n#using the mean and standard deviation of the original dataframe, stocked earlier\ndfc = pd.DataFrame()\nfor c in dfc_scaled.columns:\n    if(c=='gender'): dfc[c] = dfc_scaled[c]\n    else: \n        dfc[c] = (dfc_scaled[c] * dfsp.loc['std', c] + dfsp.loc['mean', c])\n        dfc[c] = dfc[c].astype(int)\n        \ndfc.head()","05c82fa3":"\ndef plot(dfs, names, colors, title):\n\n    data_to_plot = []\n    \n    for i, df in enumerate(dfs):\n\n        x = df['spending_score']\n        y = df['annual_income']\n        z = df['age']\n        data = go.Scatter3d(x=x , y=y , z=z , mode='markers', name=names[i], marker = colors[i])\n        data_to_plot.append(data)\n\n\n    layout = go.Layout(margin=dict(l=0,r=0,b=0,t=40),\n        title= title, scene = dict(xaxis = dict(title  = x.name,), \n        yaxis = dict(title  = y.name), zaxis = dict(title = z.name)))\n\n    fig = go.Figure(data=data_to_plot, layout=layout)\n    py.offline.iplot(fig)\n    \n\ndfcf = dfc[dfc.gender=='female']\ndfcm = dfc[dfc.gender=='male']\npurple = dict(color=cl.scales['9']['seq']['RdPu'][3:8])\nblue = dict(color=cl.scales['9']['seq']['Blues'][3:8])\nplot([dfcf, dfcm], names=['male', 'female'], colors=[purple, blue], title = 'Clusters - All Targets')\n","9fed0047":"dfc = dfc[(dfc.annual_income>40) & (dfc.spending_score>40)]\ndfc = dfc.sort_values('age').reset_index(drop=True)\ndfc","f56ee132":"dfcf = dfc[dfc.gender=='female']\ndfcm = dfc[dfc.gender=='male']\npurple = dict(color=cl.scales['9']['seq']['RdPu'][3:8])\nblue = dict(color=cl.scales['9']['seq']['Blues'][3:8])\nplot([dfcf, dfcm], names=['male', 'female'], colors=[purple, blue], title = 'Clusters - Primary Targets')","0ba45f3f":"df1 = dfc.iloc[[0], :]\ndf2 = dfc.iloc[[1,2], :]\ndf3 = dfc.iloc[[3,4], :]\n\nnames = ['younger women - moderated spenders', 'rich & independant young adults', 'parents - moderated spenders']\n\ncolors = []\nfor i in [1, 3, 5]: \n    colors.append(dict(color = cl.scales['11']['qual']['Paired'][i]))\n\nplot([df1, df2, df3], names=names, colors=colors, title = 'Marketing Clusters - Primary Targets')","75a410fc":"Now that we've got our centroids for both our clusters, we can plot them on a 3D plot.<br>\nWe apply different colors for female centroids vs. male centroids.","7350e67a":"Now that clusters are defined, we can \"unscale\" the data and the values of centroids.<br>\nThis will let us know the real values of age, income and spending score and thus have a better representation of the clusters.","d038fbbe":"Here are all our clusters.<br>\nNow, because we are big capitalistic firm that only wants profit, we'll keep only the interesting clusters.<br>\nThat means the clusters that either spend a lot, or have a high income that could allow them to spend a lot.<br>\nIn other words, we set aside the clusters that have low income <b> and <\/b> low spending score.<br>\nWe find ourselves with only the <b> primary targets <\/b>","74ce4440":"Right before jumping in though, we need to scale the data.<br>\nIndeed, creating clusters imply calculating distances between points.<br>\nIf a variable has a high scale (goes from 0 to 100 000), the distances between its points will be overweighted vs. a variable that goes from 1 to 10.<br>","cd1a639e":"It's interesting to see how four of the clusters go two by two, while one cluster stands alone.<br>\nWe therefore regroup the pairs, and find ourselves with three groups, which we describe, name and plot :\n\n- Female in their 30's, average spendings and average income. <b>Younger women - moderated spenders<\/b>.\n- Male & female in their 30's, high spendings and high income. <b>Rich & independant young adults<\/b>.\n- Male & female in their 50's, average spendings and average income. <b>Parents - moderated spenders<\/b>.","8c6d0037":"For both male and female, we want create 5 clusters.<br>\nWe'll extract their centroids (which is the gravity center of the cluster).<br>\nWe'll plot only the centroids for cleaner visualization.","d9f08240":"Within these two \"intuitevely\" created clusters, we'll create another n clusters.<br>\nHowever this time we won't intervene in the process : we'll let unsupervised learning algorithms search for interesting structures itself.<br>\nThe algorithm we'll use is called K-means clustering.<br>\nThe algorithm creates the clusters as followed : it maximizes the distances <b>between<\/b> groups, and minimzes the distances  <b>within<\/b> groups.\n\nFirst, we need to define the number of clusters we want to have within each group.<br>\nOn the one hand, the more clusters we'll have, the smaller the distances between points within a cluster will be.<br>\nOn the other hand, we don' want to find ourselves with 99 clusters that would be too hard understand for a human and unrelevant marketing-wise.<br><br>\nWe need to find just the right number, and this is done using the elbow method :<br>\nWhen the decrease in distance between points within a cluster gets too small, we consider generating more clusters is unsignificant.<br>\nOn the below graph, that would be when the line makes an elbow, also marked with a black dot.<br>","4c177ca6":"Segmentation is the practice of dividing a database into groups of observations that are similar in specific ways relevant to marketing.<br>\nEach groups contain individuals that are similar in-between themselves, and different from individuals from the other groups.<br>\nSegmentation is widely used as a marketing tool to create clusters of clients and adapt a relevant strategy for each of them.<br><br>\nIn this Kernel, we'll create our own segmentation using the mall customers dataset.<br>\nWe'll then see how we can use the segmentation marketing-wise.<br><br>\nNote that it gets particularly interesting for malls, as clusters will let us know which shops of the malls to promote to whom.<br>\nLet's first get a quick look at the dataset.","2da156fe":"Making clusters with a many variables is interesting in terms of precision.<br>\nHowever it becomes complicated to have a good vizualization of the clusters with more than 3 independant variables.<br>\n\nBut as all variables seem here interesting, we want use them all. <br>\nWe'll create clusters with all 4 variables of the dataset, and still vizualize them.<br><br>\nTo do this, we'll first create two clusters \"manually\" : male vs. female.<br>\nWe wouldn't have done that with any kind of dataset, but most mall shops are \"gender-oriented\".<br> \nThey don't have the same communication and promotion strategies whether they are targeting male or female.<br>\nTherefore it is safe enough to consider that a male will never share a cluster with a female.<br><br>\n"}}