{"cell_type":{"0ec053fa":"code","f221bde4":"code","a7705f1c":"code","97781f85":"code","ccc1ab81":"code","7fbf2634":"code","b90267bd":"code","ffdd3c2b":"code","9ac85632":"code","3b7582e6":"code","4b0e5bc1":"code","7b29b779":"code","f3edf389":"code","0d4b6826":"code","77788e5f":"code","3b589c1d":"code","018610b6":"code","8687a29d":"code","282eee4d":"code","abb88740":"code","c2d1d7f1":"code","0b4e5fc5":"code","f6754bba":"code","93699550":"code","81ad3b7b":"code","3e129731":"code","0bbd5035":"code","c5d49759":"code","ba66e3b6":"code","484d8c53":"code","f6bfd635":"code","2c0e81b6":"code","7298abb6":"code","ca731a6b":"code","d4a6dd65":"code","e75d6e3a":"code","4697fca8":"code","7c264916":"code","b14ce6f0":"code","5f27211a":"code","cb04f402":"code","d26fbf87":"code","fc212c36":"code","aa8286cb":"code","712e85f0":"code","38feae35":"code","bf1708a1":"code","7c2bf66c":"code","0b1afdc6":"code","98fa406d":"code","b62aae18":"code","8fdf43bf":"code","ce4a48f3":"code","094113a1":"code","d4a63cf0":"code","1d42b510":"code","9652aca4":"code","059c4506":"code","9d80549a":"code","da2571cc":"code","96273533":"code","b6c75461":"code","cafe9887":"code","dd8aed79":"code","055d675a":"code","93d2d047":"code","8fba2f43":"code","6b4b8c9d":"code","a34b2ac9":"code","4df0a57d":"code","5736a87e":"markdown","6d08137f":"markdown"},"source":{"0ec053fa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib\nimport warnings\nimport sklearn\nimport scipy\nimport numpy\nimport seaborn as sns\nimport matplotlib.pylab as pylab\nimport matplotlib.pyplot as plt\nfrom pandas import get_dummies\nimport matplotlib as mpl\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import make_scorer, accuracy_score \nfrom sklearn import metrics\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nkfold = StratifiedKFold(n_splits=10)\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f221bde4":"#import dataset\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","a7705f1c":"#Preview the train data\ntrain.head()","97781f85":"#Check rows and columns\ntrain.shape","ccc1ab81":"#Preview the test data\ntest.head()","7fbf2634":"#Check rows and columns\ntest.shape","b90267bd":"#Describe gives statistical information about numerical columns in the dataset\ntrain.describe()","ffdd3c2b":"#number of columns in train dataset\ntrain.columns","9ac85632":"# sum of missing values in columns\ntrain.isnull().sum()","3b7582e6":"# sum of missing values in columns\ntest.isnull().sum()","4b0e5bc1":"#We can see the relationship between two variables through the following plot\ntrain.plot(kind='scatter', x='Age', y='Fare',alpha = 0.5,color = 'red')\nplt.show()","7b29b779":"sns.heatmap(train.isnull(),yticklabels=False,cbar=False, cmap='viridis')\nplt.show()","f3edf389":"sns.heatmap(test.isnull(),yticklabels=False,cbar=False, cmap='viridis')\nplt.show()","0d4b6826":"#Survived passengers visualization\nsns.countplot(x='Survived', data=train)\nplt.show()","77788e5f":"#Survived passengers visualization as male & female\nsns.countplot(x='Survived', hue='Sex', data=train,)\nplt.show()","3b589c1d":"#Survived passengers visualization for Pclass\nsns.countplot(x='Survived', hue='Pclass', data=train)\nplt.show()","018610b6":"#Age distribution\ntrain[\"Age\"].hist(bins=30, color='red')\nplt.show()","8687a29d":"sns.countplot(x='SibSp', data=train)\nplt.show()","282eee4d":"#Boxplot visualization for Age & Pclass\nplt.figure(figsize=(10,7))\nsns.boxplot(x=\"Pclass\", y=\"Age\", data=train)\nplt.show()","abb88740":"# Age imputation for missing values in train\ndef impute_age(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    \n    if pd.isnull(Age):\n        \n        if Pclass==1:\n            return 38\n        \n        elif Pclass==2:\n            return 29\n        \n        else:\n            return 25\n    else:\n            return Age","c2d1d7f1":"train['Age'] = train[['Age','Pclass']].apply(impute_age,axis=1)","0b4e5fc5":"sns.heatmap(train.isnull(),yticklabels=False,cbar=False, cmap='viridis')\nplt.show()","f6754bba":"plt.figure(figsize=(10,7))\nsns.boxplot(x=\"Pclass\", y=\"Age\", data=test)\nplt.show()","93699550":"# Age imputation for missing values in test\ndef impute_age(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    \n    if pd.isnull(Age):\n        \n        if Pclass==1:\n            return 42\n        \n        elif Pclass==2:\n            return 27\n        \n        else:\n            return 24\n    else:\n            return Age","81ad3b7b":"test['Age'] = test[['Age','Pclass']].apply(impute_age,axis=1)","3e129731":"sns.heatmap(test.isnull(),yticklabels=False,cbar=False, cmap='viridis')\nplt.show()","0bbd5035":"#dropping Cabin feature due to high number of missing values\ntrain.drop('Cabin',axis=1,inplace=True)","c5d49759":"sns.heatmap(train.isnull(),yticklabels=False,cbar=False, cmap='viridis')\nplt.show()","ba66e3b6":"#dropping Cabin feature due to high number of missing values \ntest.drop('Cabin',axis=1,inplace=True)","484d8c53":"sns.heatmap(test.isnull(),yticklabels=False,cbar=False, cmap='viridis')\nplt.show()","f6bfd635":"train.isnull().sum()","2c0e81b6":"sns.countplot('Pclass', hue='Embarked', data=train)\nplt.show()","7298abb6":"#Filling missing values of Embarked with most frequent value 'S'\ntrain['Embarked']= train['Embarked'].fillna('S')","ca731a6b":"train.isnull().sum()","d4a6dd65":"#checking Embarked values as per Pclass\nsns.countplot('Pclass', hue='Embarked', data=test)\nplt.show()","e75d6e3a":"#Filling missing values of Embarked with most frequent value 'S'\ntest['Embarked']= test['Embarked'].fillna('S')","4697fca8":"test.head()","7c264916":"#Filling missing values in Fare column with 0\ntest['Fare']= test['Fare'].fillna(0)","b14ce6f0":"test.isnull().sum()","5f27211a":"# Create categorical values for Pclass\ntrain[\"Pclass\"] = train[\"Pclass\"].astype(\"category\")\ntrain = pd.get_dummies(train, columns = [\"Pclass\"],prefix=\"Pc\")","cb04f402":"# creating dummy variable for Sex in train dataset\nsx= pd.get_dummies(train['Sex'], prefix='Sex_')","d26fbf87":"# creating dummy variable for Embarked in train dataset\nembk= pd.get_dummies(train['Embarked'], prefix='Embarked_')","fc212c36":"# Dropping features Sex,Embarked,Name & Ticket\ntrain.drop(['Sex','Embarked','Name','Ticket'], axis=1, inplace=True)","aa8286cb":"train.head()","712e85f0":"# creating dummy variable for Sex in test dataset\nsx_1= pd.get_dummies(test['Sex'], prefix='Sex_')","38feae35":"# creating dummy variable for Embarked in test dataset\nembk_1= pd.get_dummies(test['Embarked'],prefix='Embarked_')","bf1708a1":"# concatinating new created features with train dataset\ntrain= pd.concat([train,sx,embk],axis=1)","7c2bf66c":"train.head()","0b1afdc6":"train.shape","98fa406d":"# Create categorical values for Pclass\ntest[\"Pclass\"] = test[\"Pclass\"].astype(\"category\")\ntest = pd.get_dummies(test, columns = [\"Pclass\"],prefix=\"Pc\")","b62aae18":"# Dropping features Sex,Embarked,Name & Ticket\ntest.drop(['Sex','Embarked','Name','Ticket'], axis=1, inplace=True)","8fdf43bf":"# concatinating new created features with test dataset\ntest= pd.concat([test,sx_1,embk_1],axis=1)","ce4a48f3":"test.head()","094113a1":"test.shape","d4a63cf0":"#Seperating dependent and independent variables\nX = train.drop(columns=['Survived'],axis=1)\ny = train['Survived']","1d42b510":"#train-test split\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=22)","9652aca4":"#Logistic Regression\nclf = LogisticRegression()\nclf.fit(train_X, train_y)\ny_pred = clf.predict(test_X)\naccuracy_score(test_y, y_pred)","059c4506":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\nclf_dt = DecisionTreeClassifier()\nclf_dt.fit(train_X, train_y)\ny_pred = clf_dt.predict(test_X)\naccuracy_score(test_y, y_pred)","9d80549a":"#Random Forest\nrf_clf = RandomForestClassifier(n_estimators=100, max_features=\"auto\",random_state=0)\nrf_clf.fit(train_X, train_y)\ny_pred = clf.predict(test_X)\naccuracy_score(test_y, y_pred)","da2571cc":"# RFC Parameters tunning \nrf_clf = RandomForestClassifier()\n\n\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [4,6],\n              \"max_features\": [5,10],\n              \"min_samples_split\": [2, 6],\n              \"min_samples_leaf\": [1, 5],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngs_rf_clf = GridSearchCV(rf_clf,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n\ngs_rf_clf.fit(train_X, train_y)\ny_pred = gs_rf_clf.predict(test_X)\naccuracy_score(test_y, y_pred)","96273533":"gs_rf_clf.best_params_","b6c75461":" #KNN\nknn = KNeighborsClassifier(n_neighbors = 3)\n\nknn.fit(train_X, train_y)\n\ny_pred = knn.predict(test_X)\n\naccuracy_score(test_y, y_pred)","cafe9887":"#BaggingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nmodel_bg=BaggingClassifier(n_estimators=100, bootstrap_features=True)\nmodel_bg.fit(train_X, train_y)\ny_pred = model_bg.predict(test_X)\nprint(accuracy_score(test_y, y_pred))","dd8aed79":"param_grid = {\n    'base_estimator__max_depth' : [3, 4, 5],\n    'max_samples' : [0.1, 0.2, 0.5]\n}\n\nclf = GridSearchCV(BaggingClassifier(DecisionTreeClassifier(),\n                                     n_estimators = 100, max_features = 0.5),\n                   param_grid, scoring = \"accuracy\")\nclf.fit(train_X, train_y)\ny_pred = clf.predict(test_X)\naccuracy_score(test_y, y_pred)","055d675a":"clf.best_params_","93d2d047":"model_gb= GradientBoostingClassifier()\nmodel_gb.fit(train_X,train_y)\ny_pred = model_gb.predict(test_X)\naccuracy_score(test_y, y_pred)","8fba2f43":"model_gb = GradientBoostingClassifier()\ngb_param_grid = {'n_estimators' : [100,300],\n              'learning_rate': [0.1, 0.05],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n               }\n\ngsmodel_gb = GridSearchCV(model_gb,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n\ngsmodel_gb.fit(train_X,train_y)\ny_pred = gsmodel_gb.predict(test_X)\naccuracy_score(test_y, y_pred)","6b4b8c9d":"gsmodel_gb.best_params_","a34b2ac9":"#Submission\nsubmission_predictions = model_bg.predict(test)","4df0a57d":"submission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": submission_predictions\n    })\nsubmission.to_csv('titanic.csv', index=False)","5736a87e":"Thanks a lot for having a look at this notebook. I am a beginner and this is my first Kaggle submission. For my next project I want to take a more purposeful approach to model selection, with a more extensive evaluation and diagnostics for parameter tuning.If you find this notebook useful, Please Do Upvote.","6d08137f":"The Titanic Disaster Dataset is based on the sinking of the RMS Titanic on April 14 1912, the largest passenger liner at the time. 1500 out of 2224 of the passengers died in the disaster. The dataset is a collection of the passenger\u2019s name, demographics, cabin class, ticket price, port boarded, and family information.\nOut of this 1237, only 891 of the passengers have their fate in the disaster declared. So, whether 418 passengers died or survived is an artificial mystery!\n\nCategorical data: Survived, Sex, and Embarked. Ordinal: Pclass.\n\nContinous\/numerical data: Age, Fare. Discrete: SibSp, Parch."}}