{"cell_type":{"7c6407ba":"code","0ceacee8":"code","0b3e31fa":"code","b16a9924":"code","fb1eb463":"code","da848c87":"code","204c858d":"code","dfabeabe":"code","999b1a3d":"code","0bbddeac":"code","cb366967":"code","22f6e812":"code","b7d06e22":"code","a3a44a30":"code","052be3c8":"code","a998e53a":"code","5ae777e7":"code","b4e37890":"code","5f257747":"code","77f481d6":"code","a11393de":"code","b3c9a1c2":"code","25b35997":"code","697b0e5a":"code","9734a1e3":"code","0848186b":"code","775b05ec":"code","50595d15":"code","23b8ae7d":"code","7aad30b6":"code","b3659f37":"code","b93692f0":"code","d869d263":"code","2db16812":"code","3f23c049":"code","2a7d8c8f":"code","ba9d25af":"code","94da218d":"code","be81c74c":"code","67b8ecc3":"code","1a704e59":"code","a1946df1":"code","2d61a76c":"code","b3950c10":"code","2d606961":"code","0688cb8b":"markdown","0adfdf15":"markdown","88eb0ba2":"markdown","c5ca0bbe":"markdown","ef7045ce":"markdown","f1c0352e":"markdown","f2b9aedf":"markdown","f6f195c5":"markdown","decd0284":"markdown","33a32417":"markdown","32215098":"markdown","97259190":"markdown","07960c03":"markdown","e37d267b":"markdown","3cb54e6c":"markdown","6f9c1aec":"markdown"},"source":{"7c6407ba":"# Importing the libraries that we will use throughout this project\nimport time\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport warnings \n\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 500)\nwarnings.filterwarnings('ignore') # For ignoring warnings","0ceacee8":"# This function will be used for exploring data\ndef check_df(dataframe, head=5):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(head))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)","0b3e31fa":"# Let's read datas that we are provided with\n# We use parse_dates=['date'] for converting 'date' column inti 'Date Format'\ntrain = pd.read_csv('..\/input\/demand-forecasting-kernels-only\/train.csv', parse_dates=['date'])\ntrain.info()\n# If we didn't use \"parse_dates=['date']\", the 'date' column would be \"object\" dtype\n# Since we want to use some functions of the 'Date Format' variable we had to convert it.","b16a9924":"test = pd.read_csv('..\/input\/demand-forecasting-kernels-only\/test.csv', parse_dates=['date']) # Reading test data\nsample_sub = pd.read_csv('..\/input\/demand-forecasting-kernels-only\/sample_submission.csv') # Reading the sample submission that is provided by Kaggle","fb1eb463":"# Quickly checking each set\ncheck_df(train)","da848c87":"check_df(test)","204c858d":"check_df(sample_sub)","dfabeabe":"# As I mentioned above we will generate some features, so this features have to generated both in test and train set\n# For that reason I am concatenating these sets under the name 'df'\ndf = pd.concat([train, test])\ndf.head()","999b1a3d":"df.isnull().sum() \n# As we see they are '45000' missing values in 'sales' column. It's because of test set, since our aim is to forecast that values \n# And theres is '913000' missing values in 'id' column and that is because we have 'id' column in test set but not in train set.Our final aim will be building \n# a model on train set and forecasting the 'sales' values of 'test' set due to 'id' column of the test set. Therefore, we have this column in test set but \n# not in train set \n# Since we created 'df' dataframe by concatenating 'train' and 'test' set, as a result we are facing such kind of missing values but it's not a problem\n# I just wanted to clarify what are they ","0bbddeac":"test['date'].min(), test['date'].max() # We want forecasting for first 3 months of 2018","cb366967":"train['date'].min(), train['date'].max() # We have the whole time period before the 'Timestamp('2018-01-01 00:00:00')' ==> which is begining of 'test' set\n# And it is quite reasonable, since we are dealing with time series data which ise \"sequential\"","22f6e812":"# Let's see minimum and maximum dates in the 'df'\ndf['date'].min(), df['date'].max()\n# We have data set for 5 years period in total","b7d06e22":"# For seeing the distribution of sales column\ndf[\"sales\"].describe([0.10, 0.30, 0.50, 0.70, 0.80, 0.90, 0.95, 0.99])","a3a44a30":"# How many unique store do we have ?\ndf[[\"store\"]].nunique()\n# As we were expecting kinda verfication questions","052be3c8":"# How many items do we have ?\ndf[[\"item\"]].nunique()","a998e53a":"# Do we have the same number of unique item in each store ?\ndf.groupby([\"store\"])[\"item\"].nunique()","5ae777e7":"# How many each unique item has been sold in each store for 5 years\ndf.groupby([\"store\", \"item\"]).agg({\"sales\": [\"sum\"]})","b4e37890":"# more descriptive statistics of the previous code\ndf.groupby([\"store\", \"item\"]).agg({\"sales\": [\"sum\", \"mean\", \"median\", \"std\"]})","5f257747":"# Since we have 'date' column in our hands we have to generate new features from that column as much as we can \n# Normally we shouldn't hesitate to generate a lot of features even we think that it will not have a point. \n# It's better to generate as much as we can and then by looking 'feature_importance' plot we can eleminate some of them which we see that they are not useful\n# features for the model.\n\n# So, before starting : We are aming to give some features that will represent some \"Time Series Pattern\". And here is the first place that we are starting\n# In that part by generating new features from \"date\" column we may catch some \"Time Series Pattern\" which called \"seasonality\" in time series terminology.\n# \"Seasonality\" like : \"Do we have patterns like : every month?, every year?, every weekend? and so on...\"\n# Since we expect some increase in demand for instance when it is weekend(Customers have time to visit stores), when it is beginning of the month(Customers\n# receives their salary) and so on. I have to give that kind of things to the model as a feature to make the model to catch that kind of patterns. \n\n# So let's generate new features from 'date' column\n# For that I will create function :\ndef create_date_features(df):\n    # Since we have converted 'date' column into 'Date Format' we are allowed to use the methods that we see below(like : date.dt.month,day,year etc.) \n    df['month'] = df.date.dt.month # Which month of the corresponding year\n    df['day_of_month'] = df.date.dt.day # Which day of the corresponding month\n    df['day_of_year'] = df.date.dt.dayofyear # Which day of the corresponding year\n    df['week_of_year'] = df.date.dt.weekofyear # Which week of the corresponding year\n    df['day_of_week'] = df.date.dt.dayofweek # Which day of the corresponding week of the each month\n    df['year'] = df.date.dt.year # Which year\n    df[\"is_wknd\"] = df.date.dt.weekday \/\/ 4 # df.date.dt.weekday => Starts from '0' means '0' = 'Monday'. So, '\/\/ 4' will give '1' when day number equals\n    # to '5'(which corresponds 'Saturday') and '6'(which corresponds 'Sunday') and '0' for rest of them. Consequently this column will represent whether \n    # the day is weekend or not\n    df['is_month_start'] = df.date.dt.is_month_start.astype(int) # Is it starting of the corresponding month\n    df['is_month_end'] = df.date.dt.is_month_end.astype(int) # Is it ending of the corresponding month\n    return df\n# As we notice this kind of features can help us to explain the variance or change of the sales column. We actually want to find what makes \"sales\" column\n# to change\n\n# Since this data is related to sales, by genereting the new features(as we have generated above from 'date' column) we can catch some patterns as \n# I mentioned above. For instance by generating  df['is_month_start'] column we allow our model to investigate this question :\n# 'Can beginning of month affect sales of item, or can help us to explain changes in sales column(because it is the data that all people receives their salary)'\n\n# And for isntance by generating df['year'] column we allow our model to investigate that question : 'Can we say that for each year we have different amount\n# of sales, can year will help us to explain changes in sales column'\n\n# The same thing for instance df[\"is_wknd\"] feature, as a human we can guess that maybe being end of the week can affect amount of sales beacuse people have\n# more time to buy new item. As I said we can guess it by human but we have to also allow for the model to investigate that effect by generating new feature \n# as for instance df[\"is_wknd\"]. So therfore we have generated that kind of features from 'date' column\n\n# Also you may add the features such as 'Is it 8 march?' that we think it can also affect amount of sales or days like 'Is it Black Friday?'\n\n# I haven't added that kind of features to my model but you can add and see the effect of them maybe they might boost your result\n# As I said don't hesitate to add new features to your model you can elimante them later by looking 'feature_importance' metric if it is useless","77f481d6":"# So let's use the function that we created above\ndf = create_date_features(df)\ncheck_df(df) # Now we have more features","a11393de":"# Since after generating new features now we have 'month' column in our hands let's see more detailed descriptive statistics\ndf.groupby([\"store\", \"item\", \"month\"]).agg({\"sales\": [\"sum\", \"mean\", \"median\", \"std\"]})\n# For instance if we can see some differences between months in terms of \"sales mean\" that might mean that \"month\" column might have distinctiveness property\n# That is why we generate new features like that one in order to catch why \"sales\" column changes.","b3c9a1c2":"df.groupby([\"store\", \"item\"])[['store', 'item', 'date', 'sales']].head()","25b35997":"# Before starting \"Lag\/Shifted Features\" part we have to be sure that all data have asending order due to the 'store', 'item' and 'time'. \n# Because in \"Lag\/Shifted Features\" part we will generate new features for each 'item' in each 'store' according to 'date' order\n# Simply means when we run this code 'df.groupby([\"store\", \"item\"])[['store', 'item', 'date', 'sales']].head()' we want to see sequnetial 'sales' according\n# to time for each 'item' in each store\n# So for being sure we have to write that code :\ndf.sort_values(by=['store', 'item', 'date'], axis=0, inplace=True)\n\n# What the above code does is : First sorts data due to 'store' value(which ranges between 1 and 10)\n# We may have this situation while sorting for 'store' value: ('store' value = 1, 'item' value = 1) and ('store' value = 1, 'item' value = 2) which one \n# has to come first? by writing -by=['store', 'item',- we say if the 'store' values are same sort for 'item' value\n\n# We solved one part of this problem, what if it faces that situation : \n# ('store' value = 1, 'item' value = 1, 'date' = 2013-01-01) and ('store' value = 1, 'item' value = 1, 'date' = 2013-01-02) which one has to come first?\n# by writing -by=['store', 'item', 'date']- we say when you see both 'store' value and 'item' value are same sort for 'date'\n\n# So, to conclude by writing above code we ensure that our data is sorted due to the 'store', 'item' and 'time'. ","697b0e5a":"# Let's first understand what is \"Lag\/Shifted Features\" first.\npd.DataFrame({\"sales\": df[\"sales\"].values[0:10],\n              \"lag1\": df[\"sales\"].shift(1).values[0:10],\n              \"lag2\": df[\"sales\"].shift(2).values[0:10],\n              \"lag3\": df[\"sales\"].shift(3).values[0:10],\n              \"lag4\": df[\"sales\"].shift(4).values[0:10]})\n# For instance if we generate 'lag' features for 'sales' column, 'lag1' column means 'the amount of 1 day before sale' for each 'sale'\n# 'lag1' : For '13.0' we don't have 1 day before sale, thus ==> NaN\n# 'lag1' : For '11.0' we do have 1 day before sale, thus ==> 13.0\n# 'lag1' : For '14.0' we do have 1 day before sale, thus ==> 11.0\n# ......... and so on.\n\n# The same principle is applied for other columns. 'lag2' column means 'the amount of 2 day before sale' for each 'sale'\n# 'lag2' : For '13.0' we don't have 2 day before sale, thus ==> NaN\n# 'lag2' : For '11.0' we don't have 2 day before sale, thus ==> NaN\n# 'lag2' : For '14.0' we do have 2 day before sale, thus ==> 13.0\n# 'lag2' : For '13.0' we do have 2 day before sale, thus ==> 11.0\n# ......... and so on.","9734a1e3":"# Now I hope we understand what are \"Lag\/Shifted Features\", so let's see one other example. Actually this example will be more appropriate for what we will \n# do in further steps for creating \"Lag\/Shifted Features\"\n\n# Now we will see \"Lag\/Shifted Features\"  specifically for each 'item' in each 'store' which we are caring in this work.\n# Because we want:  for instance in 'store' '1', how many 'item' '1' had been sold 1 day before (in'lag1' column) or how many 'item' '1' had been sold 2 day \n# before(in 'lag2' column) and so on. We have to focus on each unique 'item' in each 'store' separately, because we are seeking for some features that can help\n# to find some patterns that may help us to forecast. And each 'store' may have its own pattern for each 'item'. For that reason we have to generate features\n# for each unique 'item' in each 'store' separately\n\n# Let's see an example : With this code we generate 'lag1' feature for ecah unique 'item' in each 'store' separately.\ndf.groupby([\"store\", \"item\"])[['sales']].transform(lambda x: x.shift(1))\n# Let's do it by using function in next part \n\n# Before doing that I want to mention why do we need to generate that kind of feature ?","0848186b":"# Let's first create \"Random Noise\" function, for using when we want \ndef random_noise(dataframe):\n    return np.random.normal(scale=1.6, size=(len(dataframe),)) # Gaussian random noise","775b05ec":"# And let's create \"Lag\/Shifted Features\" by using this function\n# Since we will create more than 1 \"Lag\/Shifted Features\" I created that function.\ndef lag_features(dataframe, lags):\n    for lag in lags:\n        dataframe['sales_lag_' + str(lag)] = dataframe.groupby([\"store\", \"item\"])['sales'].transform(\n            lambda x: x.shift(lag)) + random_noise(dataframe) # Adding random noise to each value.\n    return dataframe","50595d15":"# As I said we are creating more than 1 \"Lag\/Shifted Features\". So this list represents all \"Lag\/Shifted Features\" that will be created\n# We started with '90' days gap, because we are aming to predict \"test\" set in the final stage. Our ultimate goal is to be successful in the \"test\" set\n# So if we don't start '90' days gap, majority of our new generated lag columns will be 'NaN'. \n# For instance if we intend to create 'lag1' feature, this feature will be almost 'NaN' for the 'test' data, and we probably will not be successful in the \n# test set, since this column will be almost 'NaN' for the 'test' column. Only one observation(the observation that comes after the last observation of 'train' set\n# because only for that observation we have 'lag1' feature) will not be 'NaN' others will be.\n\n# So for that reason we start with '90' days gap. In this case we will not have any 'NaN' in the \"Lag\/Shifted Features\" in the 'test' set.\n\n# We add more than 1 \"Lag\/Shifted Features\" because we will try which \"Lag\/Shifted Features\" makes sense for our data, means you can try other values in this\n# list and see the effect.\nlags_list = [91, 98, 105, 112, 119, 126, 182, 364, 546, 728] \ndf = lag_features(df, lags_list) # Let's create","23b8ae7d":"df.head()","7aad30b6":"df.tail()","b3659f37":"# Let's see an example of 'rolling mean features'\n# We have to first 'sift(1)' then take the previous 'n' variables. Because it's prediction of current variable, we have to leave out it.\npd.DataFrame({\"sales\": df[\"sales\"].values[0:10],\n              \"roll2\": df[\"sales\"].shift(1).rolling(window=2).mean().values[0:10],\n              \"roll3\": df[\"sales\"].shift(1).rolling(window=3).mean().values[0:10],\n              \"roll5\": df[\"sales\"].shift(1).rolling(window=5).mean().values[0:10]})\n# For instance \"roll2\" takes previous 2 variable and averages it\n# \"roll3\" takes previous 3 variable and averages it, you can examine by yourself.","b93692f0":"# Let's create \"rolling mean features\".\n\ndef roll_mean_features(dataframe, windows):\n    for window in windows:\n        dataframe['sales_roll_mean_' + str(window)] = dataframe.groupby([\"store\", \"item\"])['sales']. \\\n                                                          transform(\n            lambda x: x.shift(1).rolling(window=window, min_periods=10, win_type=\"triang\").mean()) + random_noise( # Again adding random noise\n            dataframe)\n    return dataframe\n\nroll_mean_list = [365, 546] # Again you can try various variables and see the effect. While I was working I saw that these [365, 546] have an effect.\ndf = roll_mean_features(df, roll_mean_list)\ndf.tail()","d869d263":"# You can examine by yourself from this example\npd.DataFrame({\"sales\": df[\"sales\"].values[0:10],\n              \"roll2\": df[\"sales\"].shift(1).rolling(window=2).mean().values[0:10], # It's 'rolling mean averages'\n              \"ewm099\": df[\"sales\"].shift(1).ewm(alpha=0.99).mean().values[0:10], # The rest is 'exponentially weighted mean averages' for different 'alpha'\n              \"ewm095\": df[\"sales\"].shift(1).ewm(alpha=0.95).mean().values[0:10],\n              \"ewm07\": df[\"sales\"].shift(1).ewm(alpha=0.7).mean().values[0:10],\n              \"ewm01\": df[\"sales\"].shift(1).ewm(alpha=0.1).mean().values[0:10]})","2db16812":"# Let's create 'Exponentially Weighted Mean Features' \n\ndef ewm_features(dataframe, alphas, lags):\n    for alpha in alphas:\n        for lag in lags:\n            dataframe['sales_ewm_alpha_' + str(alpha).replace(\".\", \"\") + \"_lag_\" + str(lag)] = \\\n                dataframe.groupby([\"store\", \"item\"])['sales'].transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())\n    return dataframe\n\n# In here we have two combinations : alphas and lags. Agian we give variety of variables for both and will se which one is best. \nalphas = [0.95, 0.9, 0.8, 0.7, 0.5]\nlags = [91, 98, 105, 112, 180, 270, 365, 546, 728]\n\ndf = ewm_features(df, alphas, lags)\ndf.head()","3f23c049":"# We have finished with generating new_features let's go to \"One-Hot encoding\" part","2a7d8c8f":"# We slected cotegoric features for  one-hot encoding\ndf = pd.get_dummies(df, columns=['store', 'item', 'day_of_week', 'month'])","ba9d25af":"df.head()","94da218d":"# We are standardizing by using logarithmic transformation\ndf['sales'] = np.log1p(df[\"sales\"].values)","be81c74c":"def smape(preds, target):\n    n = len(preds)\n    masked_arr = ~((preds == 0) & (target == 0))\n    preds, target = preds[masked_arr], target[masked_arr]\n    num = np.abs(preds - target)\n    denom = np.abs(preds) + np.abs(target)\n    smape_val = (200 * np.sum(num \/ denom)) \/ n\n    return smape_val\n\n# Calculating SMAPE for LightGBM output:\ndef lgbm_smape(preds, train_data):\n    labels = train_data.get_label()\n    smape_val = smape(np.expm1(preds), np.expm1(labels))\n    return 'SMAPE', smape_val, False","67b8ecc3":"# Let's define train and validation set.\ntrain = df.loc[(df[\"date\"] < \"2017-01-01\"), :] # Until beginning of 2017\nval = df.loc[(df[\"date\"] >= \"2017-01-01\") & (df[\"date\"] < \"2017-04-01\"), :] # First 3 months of 2017\n\ncols = [col for col in train.columns if col not in ['date', 'id', \"sales\", \"year\"]]\n\nY_train = train['sales']\nX_train = train[cols]\n\nY_val = val['sales']\nX_val = val[cols]\n\nY_train.shape, X_train.shape, Y_val.shape, X_val.shape","1a704e59":"# LightGBM parameters : Actually that parameters is pretrained parameters means this parameters have been found by GridSearch\/RandomizedSearch. \n#So I am writing using it directly, normally it should have been found by GridSearch\/RandomizedSearch methods. \nlgb_params = {'metric': {'mae'},\n              'num_leaves': 10,\n              'learning_rate': 0.02,\n              'feature_fraction': 0.8,\n              'max_depth': 5,\n              'verbose': 0,\n              'num_boost_round': 11000,\n              'early_stopping_rounds': 200,\n              'nthread': -1}","a1946df1":"lgbtrain = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\nlgbval = lgb.Dataset(data=X_val, label=Y_val, reference=lgbtrain, feature_name=cols)\n\nmodel = lgb.train(lgb_params, lgbtrain,\n                  valid_sets=[lgbtrain, lgbval],\n                  num_boost_round=lgb_params['num_boost_round'],\n                  early_stopping_rounds=lgb_params['early_stopping_rounds'],\n                  feval=lgbm_smape,\n                  verbose_eval=200)\n\ny_pred_val = model.predict(X_val, num_iteration=model.best_iteration)  \n\nsmape(np.expm1(y_pred_val), np.expm1(Y_val))","2d61a76c":"def plot_lgb_importances(model, plot=False, num=10):\n\n    gain = model.feature_importance('gain')\n    feat_imp = pd.DataFrame({'feature': model.feature_name(),\n                             'split': model.feature_importance('split'),\n                             'gain': 100 * gain \/ gain.sum()}).sort_values('gain', ascending=False)\n    if plot:\n        plt.figure(figsize=(10, 10))\n        sns.set(font_scale=1)\n        sns.barplot(x=\"gain\", y=\"feature\", data=feat_imp[0:25])\n        plt.title('feature')\n        plt.tight_layout()\n        plt.show()\n    else:\n        print(feat_imp.head(num))\n\n\n\nplot_lgb_importances(model, num=30, plot=True)\n","b3950c10":"train = df.loc[~df.sales.isna()]\nY_train = train['sales']\nX_train = train[cols]\n\ntest = df.loc[df.sales.isna()]\nX_test = test[cols]\n\nlgb_params = {'metric': {'mae'},\n              'num_leaves': 10,\n              'learning_rate': 0.02,\n              'feature_fraction': 0.8,\n              'max_depth': 5,\n              'verbose': 0,\n              'nthread': -1,\n              \"num_boost_round\": model.best_iteration}\n\n\n# LightGBM dataset\nlgbtrain_all = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\n\nmodel = lgb.train(lgb_params, lgbtrain_all, num_boost_round=model.best_iteration)\n\ntest_preds = model.predict(X_test, num_iteration=model.best_iteration)","2d606961":"# Submission part\nsubmission_df = test.loc[:, ['id', 'sales']]\nsubmission_df['sales'] = np.expm1(test_preds)\nsubmission_df['id'] = submission_df.id.astype(int)\n\nsubmission_df.to_csv('submission.csv', index=False)\nsubmission_df.head(20)","0688cb8b":"# **Final Model**","0adfdf15":"# **Adding \"Random Noise\" to the \"Lag\/Shifted Features\"**\n* We understand that using \"Lag\/Shifted Features\" really makes sense since statistical time seris models also use that approach. But since we are building machine learning model, there is a problem. We are generating these \"Lag\/Shifted Features\" from the target variable \"sales\", actually we are causing a problem which is called **data leakage** in data science literature. \n* The reason of that **data leakage** problem is that in our case, normally we shouldn't generate features by using target variable when we are working on ML project. Because it causes **overfitting** to the train data. Model notices target variable base features explains \"target\" column well, and focuses more to that columns. Consequently, it loses its \"generalization\" ability.\n* Since we feel obliged to generate features from target column in our case(beacuse we have only this feature in our hands and from statistical time series models like ARIMA uses that values for forcasting and we also have to use to represent time series patterns), for avoiding \"overfitting\" situation, as a solution we add \"Random, gaussian noise\" to \"Lag\/Shifted Features\" on purpose.\n* This will cause, model will not to learn the exact values of target variable and as a result we avoid \"overfitting\" situation","88eb0ba2":"# **LightGBM Model**","c5ca0bbe":"# **Modelling**","ef7045ce":"# **Exponentially Weighted Mean Features**\n* Another traditional \"Time Series Method\" is \"Exponentially Weighted Mean\" method. This method has parameter called _alpha_ used as smoothing factor. This parameter ranges between [0, 1]. If _alpha_ is close to 1 while taking average for last for instance 10 days(rolling mean features also was taking averages but without giving weight), it gives more _weight_ to the close days and decreases the _weight_ when going to more past days.  \n* You can read about this method more on internet, but briefly normally in time series forecatsing it's better to give more _weights_ to the more recent days rather tham giving same _weight_ to all past days.\n* Because more recent days have more influence to the current day. Therefore, giving more _weight_ to the more recent days makes sense.\n* This method uses that formula behind in its calculations(xt : past days values) : \n\n![image.png](attachment:cc95ea84-072b-4c41-aafd-b1e8590335fb.png)\n* As we see when it goes more past values it decreases the _weight_","f1c0352e":"# **Why do we need \"Lag\/Shifted Features\" features ?**\n* As I mentioned in the beginning of notebook, we have to generate some features that represents \"Time series\" patterns. By using \"Lag\/Shifted Features\" we actually add that kind of features to our data. Because if you are familiar with well-known \"Time Series\" statistical model like: ARIMA, Holt winters and etc, in almost all these models' formula we see that \"Lag\/Shifted Features\" actually is used.\n* For instance let's see \"ARIMA\" model's fomula : \n\n![image.png](attachment:ba3c7df6-a3b2-4154-a31c-982f1cc8b207.png)\n* As we see in the formula \"Lag\/Shifted Features\" are used as **yt-1**, **yt-2** ..... These are the previous term target variable values\n* From here we understand that using \"Lag\/Shifted Features\" as a feature for machine learning model really makes sense for representing \"Time Series\" pattern. \n","f2b9aedf":"# **Briefly EDA for verfication of data**","f6f195c5":"# **Custom Cost Function**\n* MAE: mean absolute error\n* MAPE: mean absolute percentage error\n* SMAPE: Symmetric mean absolute percentage error (adjusted MAPE). For this competition Kaggle wants 'SMAPE' metric from us. For that reason we will optimize our model for this metric.","decd0284":"# **Rolling Mean Features**\n* \"Moving Average Method\" is used for forcasting \"Time Series\" problems. This method simply takes \"n\" previous target variable and averages them and returns as a new value.\n* So since we know that, this kind of method is used for forcasting \"Time Series\" problems, again we generate new feature by using that method.\n* You may ask, why we use traditional \"Time Series Forecasting methods\" when we generate new features. Because, normally when we work on ML problems we try to generate features that we  think this new features have an ability of predicting target variable.\n* Since this kind of traditional  methods have been used for forecasting target variables, when we want to generate new features by looking these methods we become sure that  these new features will have predictive ability for predicting target variable. Because they have been used in traditional \"Time Series Method\", that means they have a predictive ability for target variable. \n* So since we said that while using ML approach we have to generate features that represent time series patterns, we actually get help from traditional methods for that purpose.\n* We actually try to add our **variety of predictions** for target varaible to the columns as a new feature by using traditional \"Time Series Methods\" approach.\n* So since these variety of predictions may cause overfitting to train data we add again **random, gaussian noise** to these new generated features on purpose.","33a32417":"# **Feature Importance**","32215098":"# **Satndardization of target variable**\n* Since we are in regression problem our target variable is continuous.\n* We will use LightGBM model, and this model uses 'Gradient Descent' method for optimization.\n* For diminishing optimization time we prefer to standardize target variable, but you prefer not to do it as well.","97259190":"# **One-Hot encoding**","07960c03":"# **Time-Based Validation Set**\n* We have to define validation set for optimizing our model. Normally in typical ML projects we define validation set either 'Hold-out validation set' approach or 'K-fold cross validation' approach. Since it's time series problem and order of the time matters 'K-fold cross validation' approach will not be useful.\n* Thus, we will continue with 'Hold-out validation set' approach.\n* But there is one point we have to think, which part has to be 'Hold-out validation set' ?. \n* As we know we optimze algorithm due to validation set, means our aim will be to fit validation set in a possible the best way. \n* Apart from that our ultimate goal will be assessed in 'test set'. Thus, we have to define validation set similar to 'test' set. We know that our 'test' set belongs to **first 3 months of 2018**.\n* For that reason we also define our 'hold-out validation' set as a **first 3 months of 2017**. Because we think that that part will be similar to 'test' set. Again, since we will optimize our model due to the 'hold-out validation' set we want this set to be similar to 'test' set.\n* So we decided **first 3 months of 2017** to be 'holf-out validation' set. We will remove that part from train set.\n* Since by removing that part from 'train' set we ruin the sequential order of  time for the **year 2017**, we will remove also rest part of the **2017** from the training set.(Because by removing first 3 months of 2017 for validation set, after 2016's last day, 2017's 4th month will come so as we see the order has been ruined) ","e37d267b":"# **Store Item Demand Forecasting with LightGBM**\n* We are given 5 years of store-item sales data and asked to predict 3 months of sales for 50 different items at 10 different stores.\n* It is actually a **\"Time Series problem\"**. The interesting part is that I will try to handle it with a **Machine Learning** approach by using the **LightGBM model**.\n* The challenge is here that normally machine learning algorithms can not be used for **\"Time Series\"** problems(because they are not time series algorithms) unless we are able to generate features that represent **\"Time Series\"** patterns for the Machine learning models. Once we are able to do that we actually convert the time series problem into **regression problem** which can be solved by various ML algorithms.\n* Thus, the key point is here that being able to generate appropriate features for the Machine Learning model in order to handle **\"Time Series problem\"** as a regression problem. Let's start !\n![image.png](attachment:aed30c55-b071-4099-bda7-cc0d9ae996b5.png)\n# **Business Problem**\n* There are 10 different stores and 50 distinct item in each. We want to forecast a number for each distinct 50 items that tell us how many each item will be sold on each day for 3 months period in every 10 stores. ","3cb54e6c":"# **Lag\/Shifted Features**","6f9c1aec":"# **Feature Engineering**\n* That section is the most crucial part of this work because as I mentioned at the very beginning of the work we have to generate features that represent \"Time Series\" patterns in order to apply the ML models to the time series problems."}}