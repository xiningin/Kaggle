{"cell_type":{"38640f08":"code","6a2522e6":"code","8c9314cb":"code","efc4a9e4":"code","68c2ee24":"code","e2500c0a":"code","89516300":"code","a068fabe":"code","8c70d33f":"code","375f9f82":"code","8a4123c7":"code","a6e6562d":"code","6f39f420":"code","903b22e2":"code","767281f7":"code","ce146ea3":"code","1f383dd8":"code","19508989":"code","b43f090d":"code","c3d08fff":"code","7225fdc5":"code","89856647":"code","766b00c3":"code","2d78d1a2":"code","9a902954":"code","d04a22cb":"code","b763eb8e":"markdown","cd5b6d3f":"markdown","109a9d05":"markdown","88741933":"markdown","43a56f3c":"markdown","b925cb85":"markdown","7468735f":"markdown","64e3056b":"markdown","2f96f370":"markdown","0bd6ad03":"markdown","3503b9a1":"markdown","1972ae66":"markdown","5350e63f":"markdown","638b2860":"markdown","339f3411":"markdown","4f980bed":"markdown","4bed2d08":"markdown","9052c475":"markdown","cdb13e02":"markdown","7fa556ae":"markdown","3137c4c3":"markdown","3e6db06f":"markdown","168647fd":"markdown","9b2ca259":"markdown","6533fc2b":"markdown","58cc94f1":"markdown","596074fb":"markdown","45b1b59a":"markdown","e9ce255b":"markdown","5193d6dd":"markdown","bc2bc9cf":"markdown","27122b30":"markdown","7c119a67":"markdown","482701ae":"markdown","f00ad613":"markdown","685b37b2":"markdown","f274f72a":"markdown","eb27614b":"markdown","6507ac2c":"markdown","59fab51a":"markdown","7ff4fe36":"markdown","1a775a97":"markdown","4923823b":"markdown"},"source":{"38640f08":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6a2522e6":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom IPython.display import Image\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms, models\n\nfrom torchvision.utils import make_grid\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nif torch.cuda.is_available():\n    torch.backends.cudnn.deterministic = True","8c9314cb":"### => train\/test\u306e\u9055\u3044\ntrain_df  = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\") \ntest_df = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\") ","efc4a9e4":"train_df.shape","68c2ee24":"train_df.head()","e2500c0a":"test_df.shape","89516300":"test_df.head()","a068fabe":"random_n = np.random.randint(len(train_df),size=8)\nrandom_n\n\n### => \u306a\u305cdataframe\u306bmax\u30922\u56de\u3082\ntrain_df.max().max()  ##255\n\n### => 255\u3067\u5272\u3063\u3066\u3044\u308b\u7406\u7531\n### => reshape\u3068-1\u306b\u3064\u3044\u3066\ntorch.Tensor((train_df.iloc[random_n, 1:].values\/255.))\n\n### => 255\u3067\u5272\u3063\u3066\u3044\u308b\u7406\u7531\n### => reshape\u3068-1\u306b\u3064\u3044\u3066\ntorch.Tensor((train_df.iloc[random_n, 1:].values\/255.).reshape((-1,28,28)))\n\n## 4\u6b21\u5143\u3078\u5909\u63db\n### => unsqueeze\u306b\u3064\u3044\u3066\ntorch.Tensor((train_df.iloc[random_n, 1:].values\/255.).reshape((-1,28,28))).unsqueeze(1)\n\n### => make_grid\ngrid = make_grid(torch.Tensor((train_df.iloc[random_n, 1:].values\/255.).reshape((-1, 28, 28))).unsqueeze(1), nrow=8)\nplt.rcParams['figure.figsize'] = (16, 2)\n\n### => transpose\u306b\u3064\u3044\u3066\nplt.imshow(grid.numpy().transpose((1,2,0)))\nplt.axis('off')\nprint(*list(train_df.iloc[random_n, 0].values), sep = ', ')","8c70d33f":"torch.Tensor((train_df.iloc[random_n, 1:].values\/255.).reshape((-1,28,28)))","375f9f82":"torch.Tensor((train_df.iloc[random_n, 1:].values\/255.).reshape((-1,28,28)))","8a4123c7":"train_df","a6e6562d":"train_df.iloc[:,1:].mean(axis=1).mean()","6f39f420":"train_df.iloc[:,1:]","903b22e2":"\n### => Dataset\u306b\u3064\u3044\u3066\nclass MNISTDataset(Dataset):\n    \"\"\"MNIST dtaa set\"\"\"\n    \n    def __init__(self, dataframe, \n                 ### => transforms.Compose\u306b\u3064\u3044\u3066\n                 transform = transforms.Compose([transforms.ToPILImage(),\n                                                 transforms.ToTensor(),\n                                                 ### => mean\u3068std\u306e\u78ba\u8a8d\n                                                 transforms.Normalize(mean=(0.5,), std=(0.5,))])\n                ):\n        df = dataframe\n        # for MNIST dataset n_pixels should be 784\n        self.n_pixels = 784\n        \n        if len(df.columns) == self.n_pixels:\n            # test data\u3000\u6b21\u306e\u65b9\u6cd5\u3067\u6b21\u5143\u3092\u5897\u3084\u3057\u3066\u3044\u308b\u3002\n            self.X = df.values.reshape((-1,28,28)).astype(np.uint8)[:,:,:,None]\n            self.y = None\n        else:\n            # training data\n            self.X = df.iloc[:,1:].values.reshape((-1,28,28)).astype(np.uint8)[:,:,:,None]\n            self.y = torch.from_numpy(df.iloc[:,0].values)\n            \n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        if self.y is not None:\n            return self.transform(self.X[idx]), self.y[idx]\n        else:\n            return self.transform(self.X[idx])","767281f7":"from torchvision.models.resnet import ResNet, BasicBlock, Bottleneck\n\n### => MNIST\u7528\u306bResNet\u3092\u4fee\u6b63\nclass MNISTResNet(ResNet):\n    def __init__(self):\n        #super(MNISTResNet, self).__init__(BasicBlock, [2, 2, 2, 2], num_classes=10) # Based on ResNet18\n        super().__init__(BasicBlock, [2, 2, 2, 2], num_classes=10) # Based on ResNet18\n        # super(MNISTResNet, self).__init__(BasicBlock, [3, 4, 6, 3], num_classes=10) # Based on ResNet34\n        # super(MNISTResNet, self).__init__(Bottleneck, [3, 4, 6, 3], num_classes=10) # Based on ResNet50\n        # Conv2d\u306eoutput_channel 64 \u306f\u30c7\u30d5\u30a9\u30eb\u30c8\n\n        ### => conv1\u306econv2d\u306b\u3064\u3044\u3066\n        ### => kernel\u306b\u3064\u3044\u3066\n        ### => stride\u306b\u3064\u3044\u3066\n        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=1, padding=3,bias=False)\n        #self.fc    = nn.Linear(784 * 1, 10)\n\nmodel = MNISTResNet()\nprint(model)","ce146ea3":"### => train\u306b\u3064\u3044\u3066\ndef train(train_loader, model, criterion, optimizer, epoch):\n    model.train()\n    loss_train = 0\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        # if GPU available, move data and target to GPU\n        if torch.cuda.is_available():\n            data = data.cuda()\n            target = target.cuda()\n        \n        # compute output and loss\n        output = model(data)\n        loss = criterion(output, target)\n        #loss_train += criterion(output, target).data.item()\n        \n        # TODO:\n        # 1. add batch metric (acc1, acc5)\n        # 2. add average metric top1=sum(acc1)\/batch_idx, top5 = sum(acc5)\/batch_idx\n        \n        # backward and update model\n        ### => zero_grad()\n        optimizer.zero_grad()\n        ### => loss.backward()\n        loss.backward()\n        ### => optimizer.step()\n        optimizer.step()\n        \n        if (batch_idx + 1)% 100 == 0:\n            print('Train Epoch: {} [{}\/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n                100. * (batch_idx + 1) \/ len(train_loader), loss.data.item()))\n    \n    #loss_train \/= len(val_loader.dataset)\n    #history['train_loss'].append(loss_train)","1f383dd8":"def validate(val_loader, model, criterion):\n    model.eval()\n    loss = 0\n    correct = 0\n    \n    for _, (data, target) in enumerate(val_loader):\n        if torch.cuda.is_available():\n            data = data.cuda()\n            target = target.cuda()\n        \n        ### => model\u306bdata\u3092\u5165\u529b\u3057\u305f\u3068\u304d\n        output = model(data)\n        \n        ### => validate\u6642\u306eloss\u306b\u3064\u3044\u3066\n        loss += criterion(output, target).data.item()\n\n        pred = output.data.max(1, keepdim=True)[1]\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n        \n        \n        \n    loss \/= len(val_loader.dataset)\n    #history['val_loss'].append(loss)\n        \n    print('\\nOn Val set Average loss: {:.4f}, Accuracy: {}\/{} ({:.3f}%)\\n'.format(\n        loss, correct, len(val_loader.dataset),\n        100.0 * float(correct) \/ len(val_loader.dataset)))\n","19508989":"train_transforms = transforms.Compose(\n    [transforms.ToPILImage(),\n### => RandAffine\u306b\u3064\u3044\u3066\n#     RandAffine,\n     transforms.ToTensor(),\n     transforms.Normalize(mean=(0.5,), std=(0.5,))])\n\nval_test_transforms = transforms.Compose(\n    [transforms.ToPILImage(),\n     transforms.ToTensor(),\n     transforms.Normalize(mean=(0.5,), std=(0.5,))])","b43f090d":"# example config, use the comments to get higher accuracy\ntotal_epoches = 20 # 50\nstep_size = 5     # 10\nbase_lr = 0.01    # 0.01\nbatch_size = 64\n\n\n### => optimizer\u306b\u3064\u3044\u3066\noptimizer = optim.Adam(model.parameters(), lr=base_lr)\n### => nn.CrossEntropyLoss\u306b\u3064\u3044\u3066\ncriterion = nn.CrossEntropyLoss()\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.1)\n\nif torch.cuda.is_available():\n    model = model.cuda()\n    criterion = criterion.cuda()","c3d08fff":"def split_dataframe(dataframe=None, fraction=0.9, rand_seed=1):\n    df_1 = dataframe.sample(frac=fraction, random_state=rand_seed)\n    df_2 = dataframe.drop(df_1.index)\n    return df_1, df_2","7225fdc5":"#history = {\n#    'train_loss': [],\n#    'val_loss': [],\n#    'test_acc': [],\n#}\n\n\nfor epoch in range(total_epoches):\n    print(\"\\nTrain Epoch {}: lr = {}\".format(epoch, exp_lr_scheduler.get_lr()[0]))\n\n    train_df_new, val_df = split_dataframe(dataframe=train_df, fraction=0.9, rand_seed=epoch)\n    \n    ## \u30aa\u30ea\u30b8\u30ca\u30eb\u3067\u306fget_dataset\u3067MNISTDataset\u3092wrapping\u3057\u3066\u3044\u308b\u304c\u3001\u308f\u304b\u308a\u306b\u304f\u3044\u306e\u3067\u5916\u3059\u3002\n    #train_dataset = get_dataset(train_df_new, transform=train_transforms)\n    #val_dataset = get_dataset(val_df, transform=val_test_transforms)\n    train_dataset = MNISTDataset(train_df_new, transform=train_transforms)\n    val_dataset = MNISTDataset(val_df, transform=val_test_transforms)\n    \n    \n    # CIFAR10\u306e\u5834\u5408\u3001\u6b21\u3002torchvision\u306b\u30c7\u30fc\u30bf\u304c\u6e96\u5099\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u305d\u308c\u3092\u5229\u7528\u3059\u308b\u3060\u3051\u3002\n    #train_dataset = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform_train)\n    #test_dataset = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform_test)\n\n    \n    \n    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                               batch_size=batch_size, shuffle=True)\n    val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n                                             batch_size=batch_size, shuffle=False)\n    \n    \n\n    train(train_loader=train_loader, model=model, criterion=criterion, optimizer=optimizer, epoch=epoch)\n    validate(val_loader=val_loader, model=model, criterion=criterion)\n    exp_lr_scheduler.step()\n\n#plt.figure()\n#plt.plot( range(1, total_epoches+1), history['train_loss'], label='train_loss' )\n#plt.plot( range(1, total_epoches+1), history['val_loss'], label='val_loss' )\n#plt.show()","89856647":"def prediciton(test_loader, model):\n    model.eval()\n    test_pred = torch.LongTensor()\n    \n    for i, data in enumerate(test_loader):\n        if torch.cuda.is_available():\n            data = data.cuda()\n            \n        output = model(data)\n        \n        pred = output.cpu().data.max(1, keepdim=True)[1]\n        test_pred = torch.cat((test_pred, pred), dim=0)\n        \n    return test_pred","766b00c3":"x = torch.tensor(2.5)\nx\nx.data\nx.cpu().data","2d78d1a2":"test_batch_size = 64\n## \u30aa\u30ea\u30b8\u30ca\u30eb\u3067\u306fget_dataset\u3067MNISTDataset\u3092wrapping\u3057\u3066\u3044\u308b\u304c\u3001\u308f\u304b\u308a\u306b\u304f\u3044\u306e\u3067\u5916\u3059\u3002\n#test_dataset = get_dataset(test_df, transform=val_test_transforms)\ntest_dataset = MNISTDataset(test_df, transform=val_test_transforms)\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                          batch_size=test_batch_size, shuffle=False)\n\n# tensor prediction\ntest_pred = prediciton(test_loader, model)\n\n# tensor -> numpy.ndarray -> pandas.DataFrame\ntest_pred_df = pd.DataFrame(np.c_[np.arange(1, len(test_dataset)+1), test_pred.numpy()], \n                      columns=['ImageId', 'Label'])\n\n# show part of prediction dataframe\nprint(test_pred_df.head())","9a902954":"# import the modules we'll need\nfrom IPython.display import HTML\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a link and click to download\ncreate_download_link(test_pred_df, filename=\"submission.csv\")","d04a22cb":"test_pred_df.to_csv('submission.csv', index=False)","b763eb8e":"### reshape\u3068-1\u306b\u3064\u3044\u3066\n\nhttps:\/\/qiita.com\/yosshi4486\/items\/deb49d5a433a2c8a8ed4\n\n\n```\nsource_shape.py\nz = np.array([[1, 2, 3, 4],\n         [5, 6, 7, 8],\n         [9, 10, 11, 12]])\nz.shape\n(3, 4)\n```\n-1\u3092\u6e21\u3057\u305f\u5834\u5408\u306f\u3001\u884c\u30d9\u30af\u30c8\u30eb\u306b\u306a\u308a\u307e\u3059\u3002\u3053\u306e\u5834\u5408\u306f\u3001\u5217\u30b5\u30a4\u30ba\u304c\u5143\u306e\u5f62\u72b6\u304b\u3089\u63a8\u6e2c\u3055\u308c\u3066\u6c7a\u5b9a\u3055\u308c\u307e\u3059\u3002\n\n```\nrow_vector.py\nz.reshape(-1)\narray([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n```\n\nreshape(-1, 2)\u3068\u3059\u308b\u3068\u5217\u6570\u3060\u3051\u304c2\u3067\u6c7a\u307e\u308a\u3001\u884c\u6570\u306f\u5143\u306e\u5f62\u72b6\u304b\u3089\u63a8\u6e2c\u3055\u308c\u3066\u6c7a\u5b9a\u3055\u308c\u307e\u3059\u3002\n\n```\nreshape.py\nz.reshape(-1, 2)\narray([[ 1,  2],\n   [ 3,  4],\n   [ 5,  6],\n   [ 7,  8],\n   [ 9, 10],\n   [11, 12]])\n   \n   \nz.reshape(-1,2,2)\narray([[[ 1,  2],\n        [ 3,  4]],\n\n       [[ 5,  6],\n        [ 7,  8]],\n\n       [[ 9, 10],\n        [11, 12]]])\n\n```\n\n","cd5b6d3f":"# MNIST\u7528Resnet\u6e96\u5099","109a9d05":"### zero_grad()\nbackward\u30e1\u30bd\u30c3\u30c9\u3067\u306f\u52fe\u914d\u304c\u7d2f\u7a4d\u3055\u308c\u308b\u3053\u3068\u306b\u6ce8\u610f\u3067\u3059\u3002\u30df\u30cb\u30d0\u30c3\u30c1\u306a\u3069\u3092\u8003\u616e\u3057\u3066\u3053\u306e\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u3067\u3059\u304c\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b3\u56de\u7d9a\u3051\u3066backward\u3092\u547c\u3073\u51fa\u3059\u3068\u52fe\u914d\u30823\u500d\u306b\u306a\u308a\u307e\u3059\u3002\n\u3053\u306e\u305f\u3081\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u66f4\u65b0\u304c\u7d42\u308f\u3063\u305f\u5f8c\u306b\u52fe\u914d\u3092\u30bc\u30ed\u30af\u30ea\u30a2\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002optim\u30d1\u30c3\u30b1\u30fc\u30b8\u306ezero_grad\u30e1\u30bd\u30c3\u30c9\u3067\u3084\u3063\u3066\u3044\u308b\u3053\u3068\u306f\u3053\u308c\u3068\u540c\u3058\u3067\u3059\u3002\n```\nfor i in range(3):\n    y_p = param_w * x + param_b\n    loss = torch.mean((y_p - y)**2)\n    loss.backward()\n    print(param_w.grad, param_b.grad)\n\n# tensor([-33.8909]) tensor([-20.4400])\n# tensor([-67.7818]) tensor([-40.8801])\n# tensor([-101.6727]) tensor([-61.3201])\n\nfor i in range(3):\n    if param_w.grad: param_w.grad.zero_()\n    if param_b.grad: param_b.grad.zero_()\n    y_p = param_w * x + param_b\n    loss = torch.mean((y_p - y)**2)\n    loss.backward()\n    print(param_w.grad, param_b.grad)\n\n# tensor([-33.8909]) tensor([-20.4400])\n# tensor([-33.8909]) tensor([-20.4400])\n# tensor([-33.8909]) tensor([-20.4400])\n```\n\nhttps:\/\/ohke.hateblo.jp\/entry\/2019\/12\/07\/230000\n\n","88741933":"**","43a56f3c":"### Train\u306b\u3064\u3044\u3066\n\noptimizer.zero_grad()\nloss.backward()\noptimizer.step()\n\u3092\u884c\u3044\u306a\u304c\u3089\u3001\u6b63\u89e3\u3068\u30e2\u30c7\u30eb\u51fa\u529b\u306e\u5dee\u304c\u5c0f\u3055\u304f\u306a\u308b\u3088\u3046\u306b\u30e2\u30c7\u30eb\u3092\u66f4\u65b0\u3057\u3066\u3044\u3063\u3066\u3044\u308b\u3002\n\n\n\u53c2\u8003URL<br>\nhttps:\/\/ohke.hateblo.jp\/entry\/2019\/12\/07\/230000","b925cb85":"\n\n## \u3010\u88dc\u8db3\u3011RGB\u306e\u53d6\u308a\u6271\u3044\u306b\u3064\u3044\u3066\n\n\u4eca\u56de\u306fgray\u30b9\u30b1\u30fc\u30eb\u3067\u3001RGB\u3067\u306f\u306a\u3044\u304c\u3001RGB\u306e\u5834\u5408\u306enumpy\u306e\u53d6\u308a\u6271\u3044\u306f\u3053\u3061\u3089\u3002<br>\nhttps:\/\/your-3d.com\/python-rgb\/\n\n","7468735f":"### mean\u3068std\u306e\u78ba\u8a8d\n\nhttps:\/\/deeplizard.com\/learn\/video\/lu7TCu7HeYc\n\n\n\u6b21\u306e\u65b9\u6cd5\u3067\u3001mean std\u3092\u8abf\u3079\u308b\u3053\u3068\u304c\u53ef\u80fd\u3002\n\u3072\u3068\u307e\u305a\u30010.5\u30680.5\u306b\u3057\u3066\u304a\u304f\n\n```\nloader = DataLoader(dataset=train_dataset, batch_size=len(train_set), num_workers=1)\ndata = next(iter(loader))\ndata[0].mean(), data[0].std()\n\n(tensor(0.2860), tensor(0.3530))\n```\n","64e3056b":"### validate\u6642\u306eloss\u306b\u3064\u3044\u3066\n\ntrain\u3068\u9055\u3063\u3066\u30011epoch\u3067\u8aad\u307f\u8fbc\u307f\u30c7\u30fc\u30bf\u306e\u5e73\u5747loss\u3092\u8a08\u7b97\u3059\u308b\u70ba\u3001loss += criterion\u306b\u306a\u3063\u3066\u3044\u308b\u3002\n\n","2f96f370":"### loss.backward()\n\n\u4e0b\u56f3\u306e\u9006\u4f1d\u642c\u304cbackword()<br>\n\u66f4\u65b0\u3059\u308b\u91cf\u306f\u3001\u52fe\u914dxlearning_rate\n\n <img src=\"https:\/\/thinkit.co.jp\/sites\/default\/files\/article_node\/AI_math03_01.png\" width=\"600\">","0bd6ad03":"# \u753b\u50cf\u51e6\u7406\u8a2d\u5b9a","3503b9a1":"### \u306a\u305cdataframe\u306bmax\u30922\u56de\u3082\n\ndataframe\u306bmax()\u3092\u4e00\u5ea6\u5b9f\u884c\u3059\u308b\u3068\u3001column\u3054\u3068\u306bmax\u3092\u7b97\u51fa\u3002\u51fa\u3066\u304d\u305f\u3082\u306e\u306b\u3055\u3089\u307fmax()\u3092\u3059\u308b\u3068\u3001column\u3068\u3054\u306e\u6700\u5927\u5024\u306e\u4e2d\u304b\u3089\u3055\u3089\u306b\u6700\u5927\u5024\u3092\u898b\u3064\u3051\u308b\u3053\u3068\u306b\u306a\u308a\u3001\u7d50\u679cdataframe\u5168\u4f53\u306e\u4e2d\u306e\u6700\u5927\u5024\u3092\u898b\u3064\u3051\u308b\u4e8b\u304c\u3067\u304d\u308b\u3002","1972ae66":"# Validate\u7528\u95a2\u6570","5350e63f":"# training\u5411\u3051\u306e\u8a2d\u5b9a","638b2860":"### numpy.transporse\n\u8ef8\u306e\u5165\u308c\u66ff\u3048<br>\npytorch\u3067\u306f\u3001\u753b\u50cf\u306fnChannels(RGB\u306e\u3053\u3068) x Height x Width\u3068Channel\u3092\u5148\u306b\u66f8\u304f\u3053\u3068\u3092\u671f\u5f85\u3057\u3066\u3044\u308b\u3002<br>\npytorch\u5411\u3051\u306breshape\u3057\u305f\u305f\u3081\u3001\u3053\u3053\u3067\u753b\u50cf\u5411\u3051\u306b\u3001Height x Width x nChannels\u306b\u5909\u63db\u3057\u3066\u3044\u308b\u3002\n\nhttps:\/\/deepage.net\/features\/numpy-transpose.html\n","339f3411":"### conv1\u306econv2d\u306b\u3064\u3044\u3066\n\n\u53c2\u8003URL<br>\nhttps:\/\/blog.shikoan.com\/pytorch-convtranspose2d\/\n\n\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u306eConv2D\u3067\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\n\u307e\u305a\u524d\u63d0\u3068\u3057\u3066\u3001MaxPooling\u306a\u3069\u306ePooling\u3092\u4f7f\u308f\u306a\u304f\u3066\u3082\u7573\u307f\u8fbc\u307f\uff08Conv2D\uff09\u3060\u3051\u3067\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u306f\u3067\u304d\u307e\u3059\u3002GAN\u3067\u4f7f\u308f\u308c\u308b\u624b\u6cd5\u3067\u3059\u304c\u3001CNN\u3067\u3082\u4f7f\u3048\u307e\u3059\u3002\u4f8b\u3048\u3070MNIST\u3067\u8003\u3048\u307e\u3057\u3087\u3046\u3002\n\n\u5165\u529b\uff1a(-1, 1, 28, 28)\uff0bkernel=3\u306e\u7573\u307f\u8fbc\u307f\n\u51fa\u529b\uff1a(-1, 32, 14, 14)\n\n\n\u3069\u3046\u3044\u3046\u51e6\u7406\u304b\u3068\u3044\u3046\u3068\u3001\u30e2\u30ce\u30af\u30ed\uff08\u30c1\u30e3\u30f3\u30cd\u30eb\u65701\uff09\u306e28\u00d728\u306e\u753b\u50cf\u3092\u300132\u30c1\u30e3\u30f3\u30cd\u30eb\u306e3\u00d73\u306e\u7573\u307f\u8fbc\u307f\u3092\u901a\u3057\u3066\u300114\u00d714\u306b\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u3068\u3044\u3046\u3082\u306e\u3067\u3059\u3002\u3053\u308c\u30921\u3064\u306eConv2D\u3067\u5b9a\u7fa9\u3059\u308b\u306b\u306f\u3069\u3046\u3057\u305f\u3089\u3088\u3044\u3067\u3057\u3087\u3046\u304b\uff1f\u7b54\u3048\u306f\u3053\u3046\u3067\u3059\u3002\n\n```\nimport torch.nn as nn\nnn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1)\n```\n\u3053\u308c\u306f\u308f\u304b\u308a\u3084\u3059\u3044\u3068\u601d\u3044\u307e\u3059\u3002PyTorch\u306epadding\u306f\u4e21\u5074\u306b\u4ed8\u4e0e\u3059\u308b\u30d4\u30af\u30bb\u30eb\u6570\u3001\u3064\u307e\u308apadding=1\u306a\u3089\u5de6\u53f3\u306b1\u30d4\u30af\u30bb\u30eb\u305a\u3064\u5165\u308c\u308b\u3068\u3044\u3046\u3053\u3068\u306b\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u516c\u5f0f\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306b\u3088\u308b\u3068\u3001\u51fa\u529b\u306e\u89e3\u50cf\u5ea6\u306e\u8a08\u7b97\u5f0f\u306f\u3001\n\n![image.png](attachment:image.png)\n\n\u3067\u8868\u3055\u308c\u307e\u3059\uff08\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u306b\u306fdilation\u304c\u3042\u308a\u307e\u3059\u304c\u3001\u81ea\u5206\u306f\u4f7f\u3063\u305f\u3053\u3068\u306a\u3044\u306e\u3067\u7701\u7565\u3057\u3066\u3044\u307e\u3059\uff09\u3002\u4e0a\u306e\u5f0f\u306e\u5834\u5408\u3001\u5206\u6bcd\u306f\u300c28+2\u00d71-3 = 27\u300d\u3001stride\u306e2\u3067\u5272\u308a1\u3092\u8db3\u3059\u306814.5\u3001\u5c0f\u6570\u70b9\u4ee5\u4e0b\u3092\u5207\u308a\u6368\u3066\u3066\u300c14\u300d\u3001\u3061\u3083\u3093\u3068\u6b63\u3057\u304f\u8a08\u7b97\u3067\u304d\u3066\u3044\u307e\u3059\u306d\u3002\n\n\u8a08\u7b97\u4e2d\u306b\u7aef\u6570\u304c\u3067\u304d\u3066\u3044\u308b\u3053\u3068\u306b\u3064\u3044\u3066\u3067\u3059\u304c\u3001\uff08\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u5074\u306e\uff09CNN\u306e\u5834\u5408\u306fkernel\u3092\u5947\u6570\u3068\u3059\u308b\u3053\u3068\u304c\u666e\u901a\u3067\u3059\u3002\u5947\u6570\u306b\u3059\u308b\u3068\u4e2d\u5fc3\u306e\u30d4\u30af\u30bb\u30eb\u304c\u751f\u307e\u308c\u3001\u7573\u8fbc\u307f\u306e\u8a08\u7b97\u304c\u6709\u52b9\u306b\u6a5f\u80fd\u3057\u3084\u3059\u3044\u3068\u8a00\u308f\u308c\u3066\u3044\u307e\u3059\u3002","4f980bed":"## \u53c2\u8003URL\n\ncnn-resnet18-mnist<br>\nhttps:\/\/github.com\/rasbt\/deeplearning-models\/blob\/master\/pytorch_ipynb\/cnn\/cnn-resnet18-mnist.ipynb\n\nNN\u3092pytorch\u3092\u4f7f\u3063\u3066\u4f5c\u6210<br>\nhttps:\/\/qiita.com\/fukuit\/items\/215ef75113d97560e599\n\ntransform\u30de\u30cb\u30e5\u30a2\u30eb<br>\nhttps:\/\/pytorch.org\/docs\/stable\/torchvision\/transforms.html\n\npyTorch\u306etransforms,Datasets,Dataloader\u306e\u8aac\u660e<br>\nhttps:\/\/qiita.com\/mathlive\/items\/2a512831878b8018db02\n\ntorchvision\u306etransforms<br>\nhttps:\/\/tasotasoso.hatenablog.com\/entry\/2020\/01\/12\/184130?utm_source=feed\n\nchumajin\u3055\u3093Training<br>\nhttps:\/\/www.kaggle.com\/chumajin\/day3-inference-resnet18\n\nBasic\u306apytorch<br>\nhttps:\/\/www.kaggle.com\/cauthur\/basic-deep-learning-with-pytorch\n\nresnet\u306e\u30bd\u30fc\u30b9<br>\nhttps:\/\/pytorch.org\/docs\/stable\/_modules\/torchvision\/models\/resnet.html\n\n\u56f3\u3067\u307e\u3068\u3081\u308bCNN(\u30bc\u30ed\u304b\u3089\u4f5c\u308bDeep learning\u304c\u3082\u3068\u30cd\u30bf\u3002pythorch\u3067\u8f09\u3063\u3066\u3044\u308b\u3002\uff09<br>\nhttps:\/\/note.com\/ryuwryyy\/n\/nfd0b8ff862aa\n\npytorch\u30c1\u30fc\u30c8\u30b7\u30fc\u30c8<br>\nhttps:\/\/qiita.com\/dokkozo\/items\/e173acded17a142e6d02\n\npytorch tensor\u30c1\u30fc\u30c8\u30b7\u30fc\u30c8<br>\nhttps:\/\/qiita.com\/dokkozo\/items\/e173acded17a142e6d02\n\npytorch\u306b\u3064\u3044\u3066\u7d50\u69cb\u307e\u3068\u307e\u3063\u3066\u3044\u308b<br>\nhttps:\/\/qiita.com\/miyamotok0105\/items\/1fd1d5c3532b174720cd\n\npytorch\u306b\u3064\u3044\u3066\u7d50\u69cb\u307e\u3068\u307e\u3063\u3066\u3044\u308b2<br>\nhttps:\/\/qiita.com\/mathlive\/items\/8e1f9a8467fff8dfd03c\n\nPyTorch transforms\/Dataset\/DataLoader\u306e\u57fa\u672c\u52d5\u4f5c\u3092\u78ba\u8a8d\u3059\u308b<br>\nhttps:\/\/qiita.com\/takurooo\/items\/e4c91c5d78059f92e76d\n\npytorch\u306eoptimizer\u4e00\u89a7<br>\nhttps:\/\/rightcode.co.jp\/blog\/information-technology\/torch-optim-optimizer-compare-and-verify-update-process-and-performance-of-optimization-methods\n\n\nPyTorch\u306f\u8aa4\u5dee\u9006\u4f1d\u64ad\u3068\u30d1\u30e9\u30e1\u30fc\u30bf\u66f4\u65b0\u3092\u3069\u3046\u3084\u3063\u3066\u884c\u3063\u3066\u3044\u308b\u306e\u304b\uff1f<br>\nhttps:\/\/ohke.hateblo.jp\/entry\/2019\/12\/07\/230000\n\n\n## resnet\u3092\u66f8\u3044\u3066\u3044\u308b\u3082\u306e\n\nCNN\u306e\u4ee3\u8868\u7684\u306a\u30e2\u30c7\u30ebResNet\u306e\u5b9f\u88c5\u3068\u305d\u308c\u3092\u7528\u3044\u305f\u753b\u50cf\u8a8d\u8b58<br>\nhttps:\/\/aizine.ai\/cnn-0801\/\n\nResNet (Residual Network) \u306e\u5b9f\u88c5<br>\nhttps:\/\/www.bigdata-navi.com\/aidrops\/2611\/\n\nResNet\u3092\u3044\u308d\u3093\u306a\u6a5f\u68b0\u5b66\u7fd2\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u5b9f\u88c5\u3057\u3066\u307f\u305f~TensorFlow\u7de8~<br>\nhttps:\/\/qiita.com\/gucchi0403\/items\/097d83f0e8a6091c1240\n\n\n\n## \u88dc\u8db3\n\n__getitem__\u306e\u6319\u52d5<br>\nhttps:\/\/pheromone.hatenablog.com\/entry\/20100829\/1283088398\n\n__getitem__\u306b\u4e0e\u3048\u308b\u5f15\u6570\u3084\u30b9\u30e9\u30a4\u30b9<br>\nhttps:\/\/qiita.com\/gyu-don\/items\/bde192b129a7b1b8c532\n\npython2\u30683\u306esuper\u306e\u4f7f\u3044\u65b9\u306e\u9055\u3044<br>\nhttps:\/\/www.lifewithpython.com\/2014\/01\/python-super-function.html\n\nResNet18\u3067CIAFR10\u3092\u753b\u50cf\u5206\u985e<br>\nhttps:\/\/testpy.hatenablog.com\/entry\/2020\/01\/12\/171347\n\npytorch\u3067CNN\u306eloss\u304c\u6bce\u56de\u5909\u308f\u308b\u554f\u984c\u306e\u5bfe\u51e6\u6cd5 (on gpu)<br>\nhttps:\/\/qiita.com\/chat-flip\/items\/c2e983b7f30ef10b91f6\n\npytorch\u3067CNN\u306eloss\u304c\u6bce\u56de\u5909\u308f\u308b\u554f\u984c\u306e\u5bfe\u51e6\u6cd5 (on cpu)<br>\nhttps:\/\/qiita.com\/chat-flip\/items\/4c0b71a7c0f5f6ae437f\n\n\n## Convolutional neural network\u306b\u3064\u3044\u3066\u308f\u304b\u308a\u3084\u3059\u3044\u8a18\u4e8b\n\n\u7573\u307f\u8fbc\u307f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u4ed5\u7d44\u307f\nhttps:\/\/postd.cc\/how-do-convolutional-neural-networks-work\/\n\nConvolutional Neural Network\u3068\u306f\u4f55\u306a\u306e\u304b\nhttps:\/\/qiita.com\/icoxfog417\/items\/5fd55fad152231d706c2\n","4bed2d08":"### 255\u3067\u5272\u3063\u3066\u3044\u308b\u7406\u7531\n\n\u753b\u50cf\u5316\u3059\u308b\u70ba\u3001train_df\u30920~1\u306b\u898f\u683c\u5316\u3059\u308b\u70ba\u306b\u3001255\u3067\u5272\u3063\u3066\u3044\u308b\u3002<br>\n**pytorch\u306emodel\u306b\u30c7\u30fc\u30bf\u3092\u8aad\u307e\u305b\u308b\u5834\u5408\u306f\u3001transforms.Normalize\u51e6\u7406\u306b\u898f\u683c\u5316\u304c\u5165\u3063\u3066\u3044\u308b\u306e\u3067\u3001255\u3067\u5272\u308b\u5fc5\u8981\u306a\u3057\u3002**","9052c475":"### \u3010\u88dc\u8db3] Resnet\u4ee5\u5916\u306emodel\u306b\u3064\u3044\u3066\n\nFirst, let's take a look at all available models in the torchvision package.\n\nThe models subpackage contains definitions for the following model architectures:\n- [AlexNet](https:\/\/arxiv.org\/abs\/1404.5997)\n- [VGG](https:\/\/arxiv.org\/abs\/1409.1556)\n- [ResNet](https:\/\/arxiv.org\/abs\/1512.03385)\n- [SqueezeNet](https:\/\/arxiv.org\/abs\/1602.07360)\n- [DenseNet](https:\/\/arxiv.org\/abs\/1608.06993)\n- [Inception](https:\/\/arxiv.org\/abs\/1512.00567) v3\n- [GoogLeNet](https:\/\/arxiv.org\/abs\/1409.4842)\n- [ShuffleNet](https:\/\/arxiv.org\/abs\/1807.11164) v2\n\nYou can construct a model with random weights by calling its constructor:\n```\n>>> resnet18 = models.resnet18()\n```\n\nIf you want to look closer to the source code of every model in torchvision package. You can find it on its official webpage or github repository. For example, the code of ResNet model is showed in the following page:\n- [torchvision.models.resnet](https:\/\/pytorch.org\/docs\/stable\/_modules\/torchvision\/models\/resnet.html)","cdb13e02":"### transforms.Compose\u306b\u3064\u3044\u3066\n\ntorchvision.transforms.Compose\u306f\u5f15\u6570\u3067\u6e21\u3055\u308c\u305flist\u578b\u306e[~~,~~,...]\u3068\u3044\u3046\u306e\u3092\u5148\u982d\u304b\u3089\u9806\u306b\u5b9f\u884c\u3057\u3066\u3044\u304f\u3082\u306e\u3067\u3042\u308b\u3002<br>\n\u305d\u306e\u305f\u3081list\u5185\u306e\u524d\u51e6\u7406\u306e\u9806\u756a\u306b\u306f\u5341\u5206\u6ce8\u610f\u3059\u308b\u3002<br>\nCompse\u306e\u3067\u884c\u3046\u51e6\u7406\u306e\u4e2d\u306b\u3001\u30c7\u30fc\u30bf\u306e\u6c34\u5897\u3057\u304c\u3042\u308b\u3002\n\n### \u30c7\u30fc\u30bf\u6c34\u5897\u3057\u306e\u7a2e\u985e\u306b\u3064\u3044\u3066\n\n\n\u4ee3\u8868\u7684\u306a\u51e6\u7406\u3068\u3057\u3066\u3001\u4ee5\u4e0b\u304c\u3042\u3052\u3089\u308c\u308b\u3002\n\n\n* \u56de\u8ee2\ntransforms.RandomRotation \u3067\u5909\u63db\u3067\u304d\u307e\u3059\u3002degrees \u5f15\u6570\u3067\u56de\u8ee2\u306e\u5ea6\u5408\u3044\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002\n```\ntransform = transforms.RandomRotation(degrees=30)\n```\n\n* \u6c34\u5e73\u79fb\u52d5\n\ntransforms.RandomAffine() \u3067\u5909\u63db\u3067\u304d\u307e\u3059\u3002translate \u5f15\u6570\u3067\u7e26\u6a2a\u65b9\u5411\u3078\u306e\u79fb\u52d5\u306e\u5e45\u306e\u6307\u5b9a\u304c\u3067\u304d\u307e\u3059\u3002\n\u4f8b\u3048\u3070 32 \u30d4\u30af\u30bb\u30eb\u306e\u6b63\u65b9\u5f62\u306e\u753b\u50cf\u3067\u3001translate=(0.5, 0) \u3068\u3057\u305f\u5834\u5408\u3001\u7e26\u65b9\u5411\u306b -16 ~ 16 \u306e\u5e45\u3067\u30e9\u30f3\u30c0\u30e0\u306b\u6c34\u5e73\u79fb\u52d5\u3057\u307e\u3059\n\n```\ntransform = transforms.RandomAffine(degrees=0, translate=(0.5, 0.5))\n```\n\n* \u305b\u3093\u65ad\n\ntransform = transforms.RandomAffine(degrees=0, translate=(0, 0), shear=(0, 30))\n\n```\ntransform = transforms.RandomAffine(degrees=0, translate=(0, 0), shear=(0, 30))\n\n```\n\n* \u62e1\u5927\n\ntransform = transforms.RandomCrop((16, 16))\n\n```\ntransforms.RandomCrop \u3067\u5909\u63db\u3057\u307e\u3059\u3002\u5f15\u6570\u306b\u306f\u30af\u30ed\u30c3\u30d7\u3059\u308b\u7e26\u6a2a\u306e\u753b\u7d20\u6570\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002\n```\n\n* \u6c34\u5e73\u53cd\u8ee2\n\ntransforms.RandomHorizontalFlip \u3067\u5909\u63db\u3057\u307e\u3059\u3002\u5f15\u6570 p \u306b\u306f\u3001\u53cd\u8ee2\u3092\u8d77\u3053\u3059\u78ba\u7387\u3092\u4e0e\u3048\u307e\u3059\u3002\n\u3082\u3057\u3082 1 \u3068\u3059\u308b\u3068\u3001100% \u6c34\u5e73\u306b\u53cd\u8ee2\u3059\u308b\u3068\u3044\u3046\u3053\u3068\u3092\u8868\u3057\u307e\u3059\u3002\n\n```\ntransform = transforms.RandomHorizontalFlip(p=1) \n```\n\n* \u5782\u76f4\u53cd\u8ee2\n\ntransforms.RandomVerticalFlip \u3067\u5909\u63db\u3057\u307e\u3059\u3002\u5f15\u6570 p \u306b\u306f\u3001\u53cd\u8ee2\u3092\u8d77\u3053\u3059\u78ba\u7387\u3092\u4e0e\u3048\u307e\u3059\u3002\n\u3082\u3057\u3082 1 \u3068\u3059\u308b\u3068\u3001100% \u5782\u76f4\u306b\u53cd\u8ee2\u3059\u308b\u3068\u3044\u3046\u3053\u3068\u3092\u8868\u3057\u307e\u3059\u3002\n\n```\ntransform = transforms.RandomVerticalFlip(p=1) \n```\n\n\n### \u88dc\u8db3\n\n\u56de\u8ee2\u3084\u6c34\u5e73\u79fb\u52d5\u3001\u305b\u3093\u65ad\u3067\u751f\u3058\u308b\u7a7a\u767d\u7b87\u6240\u306f\u3001\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f\u8f1d\u5ea6\u304c 0\uff08\u9ed2\u8272\uff09\u3067\u57cb\u3081\u3089\u308c\u307e\u3059\u3002\n\u7a7a\u767d\u7b87\u6240\u3092\u4efb\u610f\u306e\u8272\u3067\u5857\u308a\u3064\u3076\u3057\u305f\u3044\u5834\u5408\u306f\u3001fillcolor \u5f15\u6570\u3067\u6307\u5b9a\u3057\u307e\u3059\u3002\u30ab\u30e9\u30fc\u753b\u50cf\u306e\u5834\u5408\u3001RGB  \u306e\u8f1d\u5ea6\u3092\u30bf\u30d7\u30eb\u3067\u4e0e\u3048\u307e\u3059\u3002\n\n```\ntransform = transforms.RandomAffine(degrees=0, translate=(0.5, 0.5), fillcolor=(100, 100, 100))\n```","7fa556ae":"### Dataset\u306b\u3064\u3044\u3066\ndataset\u3092\u7d99\u627f\u3057\u3066\u4f7f\u3046\u3002<br>\n[torch.data.dataset](https:\/\/pytorch.org\/docs\/stable\/_modules\/torch\/utils\/data\/dataset.html#Dataset)\n\n\u4ee5\u4e0b3\u3064\u304c\u5fc5\u9808\u3002\n* __init__()\n* __getitem__()\n* __len__()\n\nA more detailed [DATA LOADING AND PROCESSING TUTORIAL](https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html) on PyTorch official website.\n\n\n\u4eca\u56de\u306fcsv\u304b\u3089dataset\u3092\u4f5c\u3063\u3066\u3044\u308b\u304c\u3001MNIST\u306f\u3082\u3068\u3082\u3068dataset\u304c\u6e96\u5099\u3055\u308c\u3066\u3044\u308b\u3002\n\n\n### Dataset\u3068dataloader\u306e\u95a2\u4fc2\n\n\u3053\u3061\u3089\u306e\u30b5\u30a4\u30c8\u304c\u8a73\u3057\u3044<br>\nhttps:\/\/qiita.com\/mathlive\/items\/2a512831878b8018db02\n\nhttps:\/\/qiita.com\/mathlive\/items\/8e1f9a8467fff8dfd03c\n\n#### Dataset\n\n```\ntrainset = torchvision.datasets.MNIST(root = 'path', train = True, download = True, transform = trans)\n```\n\n\u4e2d\u8eab\u306f\u3001\n\n\n```\nprint(trainset[0])\n\n------'''\u4ee5\u4e0b\u51fa\u529b\u7d50\u679c'''--------\n(tensor([data\u5185\u5bb9]), \u305d\u306edata\u306b\u5bfe\u5fdc\u3059\u308b\u6b63\u89e3label)\n```\n\n\n#### dataloader\n\n```\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size = 100, shuffle = True, num_workers = 2)\n```\n\u30c7\u30fc\u30bfset\u3092training\u306e\u305f\u3081\u306b\u3001\u5206\u5272\u3057\u305f\u308abatch\u5316\u3057\u305f\u308a\u3057\u3066\u3044\u308b\u3002<br>\nDataLoader\u306f\u914d\u5217\u3067\u306f\u306a\u304fiterator\u306a\u70ba\u3001\u6b21\u306e\u65b9\u6cd5\u3067\u4e2d\u8eab\u3092\u898b\u308b\u3002\n\n```\nfor data,label in trainloader:\n    break\nprint(data)\nprint(label)\n\n------'''\u4ee5\u4e0b\u51fa\u529b\u7d50\u679c'''--------\ntensor([[data1], [data2],..., [data100]])\ntensor([label1, label2,..., label100])\n```\n\n\u3084\u3063\u3066\u3044\u308b\u3053\u3068\u306f\u3053\u3061\u3089\u3002\n\n![image.png](attachment:image.png)","3137c4c3":"### kernel\u306b\u3064\u3044\u3066\n\n\u30d5\u30a3\u30eb\u30bf\u3068\u3082\u8a00\u308f\u308c\u308b\u3002\n\n1_qLvvyAA_IbXLff5mAZYbiA.png![image.png](attachment:image.png)","3e6db06f":"### lr_scheduler\n\nlearning\u30ec\u30fc\u30c8\u3092\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u3059\u308b\u6a5f\u80fd\u3002<br>\n\u30a8\u30dd\u30c3\u30af\u304cstep size\u9032\u3080\u3054\u3068\u306b\u3001learning rate\u306bgamma\u3092\u639b\u3051\u7b97\u3059\u308b\u3002<br>\nhttps:\/\/wonderfuru.com\/scheduler\/","168647fd":"\n### opitimizer.step()\n\n\u8a08\u7b97\u6e08\u307f\u306eloss\u306e\u52fe\u914d\u3092\u4f7f\u3063\u3066\u3001model\u3092\u66f4\u65b0\u3057\u307e\u3059\u3002<br>\n\n\ngrad\u30d7\u30ed\u30d1\u30c6\u30a3\u306b\u9069\u5f53\u306a\u5b66\u7fd2\u7387\u3092\u304b\u3051\u3066\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u66f4\u65b0\u3059\u308c\u3070\u3001\u7c21\u5358\u306a\u52fe\u914d\u964d\u4e0b\u6cd5\u3092\u5b9f\u88c5\u3067\u304d\u308b\u3053\u3068\u304c\u4e88\u60f3\u3067\u304d\u307e\u3059\u3002optim\u30d1\u30c3\u30b1\u30fc\u30b8\u306estep\u30e1\u30bd\u30c3\u30c9\u306f\u307e\u3055\u306b\u3053\u308c\u3092\u3084\u3063\u3066\u307e\u3059\u3002\n\n\u4ee5\u4e0b\u3067\u3001optimizer.step()\u3068\u540c\u3058\u4e8b\u3092\u66f8\u3044\u3066\u307f\u308b\u3002\n\n\nparam_w\u3068param_b\u306f\u30d5\u30a9\u30ef\u30fc\u30c9\u306b\u3066\u65e2\u306b\u8a08\u7b97\u30b0\u30e9\u30d5\u5185\u306b\u7d44\u307f\u8fbc\u307e\u308c\u3066\u3044\u307e\u3059\u306e\u3067\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u30ca\u30a4\u30fc\u30d6\u306b\u8a08\u7b97\u3059\u308b\u3068\u304a\u304b\u3057\u304f\u306a\u308a\u307e\u3059\u3002\n```\nlearning_rate = 0.01\n\nparam_w = param_w - param_w.grad * learning_rate\nparam_b = param_b - param_b.grad * learning_rate\n\nprint(param_w, param_b)\n\n```\n\nTensor\u306edetach\u30e1\u30bd\u30c3\u30c9\u3067\u8a08\u7b97\u30b0\u30e9\u30d5\u304b\u3089\u5207\u308a\u96e2\u3059\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\n\n\u3055\u3089\u306b\u52fe\u914d\u3092\u518d\u8a08\u7b97\u3059\u308b\u5834\u5408\u306f\u3001\u5fd8\u308c\u305arequires_grad\u30d7\u30ed\u30d1\u30c6\u30a3\u3092True\u306b\u30bb\u30c3\u30c8\u3057\u3066\u304f\u3060\u3055\u3044\n```\nlearning_rate = 0.01\n\nparam_w = (param_w - param_w.grad * learning_rate).detach().requires_grad_()\nparam_b = (param_b - param_b.grad * learning_rate).detach().requires_grad_()\n\nprint(param_w, param_b)\n```\n\n\u76f4\u306boptimizer(SGD)\u306e\u52fe\u914d\u66f4\u65b0\u3092\u66f8\u304f\u3068\u6b21\u306e\u901a\u308a\n\n```\nparam_w = torch.tensor([1.0], requires_grad=True)\nparam_b = torch.tensor([0.0], requires_grad=True)\n\nepochs = 300\nlearning_rate = 0.01\n\nfor epoch in range(1, epochs + 1):\n    y_p = param_w * x + param_b\n    loss = torch.mean((y_p - y)**2)\n    \n    loss.backward()\n    \n    param_w = (param_w - param_w.grad * learning_rate).detach().requires_grad_()\n    param_b = (param_b - param_b.grad * learning_rate).detach().requires_grad_()\n    \n    if epoch % 10 == 0:\n        print(f\"Epoch {epoch}: loss={loss}, param_w={float(param_w)}, param_b={float(param_b)}\")\n\n# Epoch 10: loss=52.31643295288086, param_w=3.4754106998443604, param_b=1.6848385334014893\n# Epoch 20: loss=23.30514144897461, param_w=4.616385459899902, param_b=2.8110599517822266\n# ...\n# Epoch 300: loss=0.5062416791915894, param_w=4.625252723693848, param_b=7.377382755279541\n```\n\n\nopitimizer.step()\u3092\u4f7f\u3046\u3068\u3001\n\n```\n# ... (\u7701\u7565) ...\n\noptimizer = optim.SGD([param_w, param_b], lr=learning_rate)\n\nfor epoch in range(1, epochs + 1):\n    y_p = param_w * x + param_b\n    loss = torch.mean((y_p - y)**2)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    \n    optimizer.step()\n        \n    if epoch % 10 == 0:\n        print(f\"Epoch {epoch}: loss={loss}, param_w={float(param_w)}, param_b={float(param_b)}\")\n        \n# Epoch 10: loss=52.31643295288086, param_w=3.4754106998443604, param_b=1.6848385334014893\n# Epoch 20: loss=23.30513572692871, param_w=4.616385459899902, param_b=2.8110601902008057\n# ...\n# Epoch 300: loss=0.5062416791915894, param_w=4.625252723693848, param_b=7.377382755279541\n```\n","9b2ca259":"## tensro\u3068numpy array\u306e\u9055\u3044\n\ntensor\u306f\u3001\u52fe\u914d\u60c5\u5831\u3092\u6301\u3064\u3053\u3068\u304c\u3067\u304d\u308b\u3002<br>\n\u30d0\u30c3\u30af\u30d7\u30ed\u30d1\u30b2\u30fc\u30b7\u30e7\u30f3\u5f8c\u306e\u7d50\u679c\u3082\u6301\u3066\u308b\u3068\u3044\u3046\u3053\u3068\u3002<br>\n\nhttps:\/\/qiita.com\/mathlive\/items\/241bfb42d852bb801b96#6-tensor%E5%9E%8B%E3%81%AE%E3%82%82%E3%81%861%E3%81%A4%E3%81%AE%E3%83%A1%E3%83%AA%E3%83%83%E3%83%88%E3%81%AEgrad\n\nmodel\u3092\u4fdd\u5b58\u3059\u308b\u3068\u304d\u306a\u3069\u3001\u52fe\u914d\u60c5\u5831\u3092\u6301\u3063\u305ftensor\u3092numpy\u306b\u3059\u308b\u3068\u304d\u306f\u3001\u4e0d\u8981\u306a\u52fe\u914d\u60c5\u5831\u3092\u524a\u9664\u3059\u308b\u305f\u3081\u3001detach\u304c\u5fc5\u8981\u306b\u306a\u3063\u3066\u304f\u308b\u3002<br>\nhttps:\/\/qiita.com\/mathlive\/items\/241bfb42d852bb801b96#6-tensor%E5%9E%8B%E3%81%AE%E3%82%82%E3%81%861%E3%81%A4%E3%81%AE%E3%83%A1%E3%83%AA%E3%83%83%E3%83%88%E3%81%AEgrad\n","6533fc2b":"## output.cpu().data\u306b\u3064\u3044\u3066\u8003\u5bdf\n\nx , x.data , x.cpu().data ,\u9055\u3044\u304c\u898b\u3048\u306a\u3044\u3002\u3002\u3002","58cc94f1":"### \u3010\u88dc\u8db3\u3011\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u306edownload\u306b\u3064\u3044\u3066\n\npytorch\u306e\u30bd\u30fc\u30b9\u306b\u6b21\u306e\u3088\u3046\u306b\u3042\u308b\u306e\u3067\u3001\u81ea\u52d5\u3067\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3055\u308c\u308b\u3002\n\n```\nmodel_urls = {\n    'resnet18': 'https:\/\/download.pytorch.org\/models\/resnet18-5c106cde.pth',\n    'resnet34': 'https:\/\/download.pytorch.org\/models\/resnet34-333f7ec4.pth',\n    'resnet50': 'https:\/\/download.pytorch.org\/models\/resnet50-19c8e357.pth',\n    'resnet101': 'https:\/\/download.pytorch.org\/models\/resnet101-5d3b4d8f.pth',\n    'resnet152': 'https:\/\/download.pytorch.org\/models\/resnet152-b121ed2d.pth',\n    'resnext50_32x4d': 'https:\/\/download.pytorch.org\/models\/resnext50_32x4d-7cdf4587.pth',\n    'resnext101_32x8d': 'https:\/\/download.pytorch.org\/models\/resnext101_32x8d-8ba56ff5.pth',\n    'wide_resnet50_2': 'https:\/\/download.pytorch.org\/models\/wide_resnet50_2-95faca4d.pth',\n    'wide_resnet101_2': 'https:\/\/download.pytorch.org\/models\/wide_resnet101_2-32ee1156.pth',\n}\n```","596074fb":"### optimizer\u306b\u3064\u3044\u3066\n\npytorch\u306eoptimizer\u4e00\u89a7<br>\nhttps:\/\/rightcode.co.jp\/blog\/information-technology\/torch-optim-optimizer-compare-and-verify-update-process-and-performance-of-optimization-methods","45b1b59a":"### make_grid\n\n\n\n\u753b\u50cf\u3092\u30a2\u30ec\u30a4\u8868\u793a\u3059\u308b\u305f\u3081\u306etorchvision.utils\u3002<br>\nhttps:\/\/blog.shikoan.com\/torchvision-image-tile\/\n","e9ce255b":"# Training\u5b9f\u884c","5193d6dd":"### stride\u306b\u3064\u3044\u3066\n\npicture_pc_0e10907a9e23465579518ac853c34a4b.png![image.png](attachment:image.png)","bc2bc9cf":"# \u30c7\u30fc\u30bf\u5206\u5272\u7528\u95a2\u6570","27122b30":"\n### nn.CrossEntropyLoss\u306b\u3064\u3044\u3066\n\n\u7b54\u3048\u3068\u30e2\u30c7\u30eb\u306e\u51fa\u529b\u3068\u306e\u30ba\u30ec\u306e\u8a55\u4fa1\u65b9\u6cd5\u306e\u4e00\u3064\u3002\n\n <img src=\"https:\/\/www.programmersought.com\/images\/567\/aaf7fc897c71a19520cdf8a935e06c87.JPEG\" width=\"500\">\n\nhttps:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.CrossEntropyLoss.html\n\n","7c119a67":"## detach\u306b\u3064\u3044\u3066\n\ndetach()\u306fTensor\u578b\u304b\u3089\u52fe\u914d\u60c5\u5831\u3092\u629c\u3044\u305f\u3082\u306e\u3092\u53d6\u5f97\u3059\u308b.<br>\nhttps:\/\/qiita.com\/mathlive\/items\/241bfb42d852bb801b96","482701ae":"## \u3010\u88dc\u8db3\u3011\u6b21\u5143\u306b\u3064\u3044\u3066\n\n\u753b\u50cf\u306f\u3001HightxWidthxChannel(R,G,B)\u3002\npytorch\u306f\u3001N\uff08BatchSize\uff09xCxHxW\u304c\u5fc5\u8981\u3002\ntranspose((1,2,0))\u3067\u3001HxWxC\u3078\u623b\u3057\u3066\u3044\u308b\u3002\n\n## openCV\u3067\u8aad\u307f\u8fbc\u3093\u3060\u30c7\u30fc\u30bf\u306b\u3064\u3044\u3066\u89e6\u308c\u3066\u307f\u308b\n\nopencv(cv2)\u306eimread()\u3067\u8aad\u307f\u8fbc\u3080\u3068\u3001H\uff08\u9ad8\u3055\uff09 x W\uff08\u5e45\uff09 x C\u8272\uff083\uff09\u306enumpy array\u3060\u304c\u3001\n\u8272\u306e\u9806\u756a\u306fBGR\uff08\u9752\u3001\u7dd1\u3001\u8d64\uff09\u3002\nOpenCV\u306e\u95a2\u6570imwrite()\u306fBGR\u306e\u9806\u756a\u3092\u524d\u63d0\u3068\u3057\u3066\u3044\u308b\u3002\n\nhttps:\/\/note.nkmk.me\/python-opencv-bgr-rgb-cvtcolor\/\n\n## Pillow\u306e\u5834\u5408\u306f\n\nPillow\u3067\u306f\u8272\u306e\u9806\u756a\u306fRGB\uff08\u8d64\u3001\u7dd1\u3001\u9752\uff09\u3092\u524d\u63d0\u3068\u3057\u3066\u3044\u308b\u3002\nOpenCV\u306e\u95a2\u6570cvtColor()\u3092\u4f7f\u3046\u3068RGB\u3084BGR\u3001HSV\u306a\u3069\u69d8\u3005\u306a\u8272\u7a7a\u9593\u3092\u76f8\u4e92\u306b\u5909\u63db\u3067\u304d\u308b\u3002\n```\ndst = cv2.cvtColor(src, code)\n```\n\n\u5f15\u6570code\u3092cv2.COLOR_BGR2RGB\u3068\u3059\u308b\u3068\u3001\u305d\u306e\u540d\u524d\u306e\u901a\u308aBGR\u304b\u3089RGB\u3078\u306e\u5909\u63db\u3068\u306a\u308b\u3002\n\nRGB\u306b\u5909\u63db\u3059\u308b\u3068\u3001PIL.Image\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306b\u5909\u63db\u3057\u4fdd\u5b58\u3057\u3066\u3082\u6b63\u3057\u3044\u753b\u50cf\u3068\u3057\u3066\u4fdd\u5b58\u3055\u308c\u308b\u3002\n\n```\nim_rgb = cv2.cvtColor(im_cv, cv2.COLOR_BGR2RGB)\n\nImage.fromarray(im_rgb).save('data\/dst\/lena_rgb_pillow.jpg')\n```\n\nRGB\u304b\u3089BGR\u3078\u5909\u63db\u3059\u308b\u5834\u5408\u306f\u3001\u5f15\u6570code\u3092cv2.COLOR_RGB2BGR\u3068\u3059\u308b\u3002<br>\nPIL.Image\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3067\u8aad\u307f\u8fbc\u3093\u3067ndarray\u306b\u5909\u63db\u5f8c\u3001OpenCV\u306eimwrite()\u3067\u4fdd\u5b58\u3059\u308b\u5834\u5408\u306f\u3053\u3061\u3089\u3092\u4f7f\u3046\u3002\n\n```\nim_pillow = np.array(Image.open('data\/src\/lena.jpg'))\n\nim_bgr = cv2.cvtColor(im_pillow, cv2.COLOR_RGB2BGR)\n\ncv2.imwrite('data\/dst\/lena_bgr_cv_2.jpg', im_bgr)\n```\n\nNumPy\u306e\u57fa\u672c\u6a5f\u80fd\u3067\u5b9f\u73fe\u3059\u308b\u5834\u5408\u3002\n\n```\nim_bgr = cv2.imread('data\/src\/lena.jpg')\n\nim_rgb = im_bgr[:, :, [2, 1, 0]]\nImage.fromarray(im_rgb).save('data\/dst\/lena_swap.jpg')\n\nim_rgb = im_bgr[:, :, ::-1]\nImage.fromarray(im_rgb).save('data\/dst\/lena_swap_2.jpg')\n```","f00ad613":"# train\u7528\u95a2\u6570","685b37b2":"# MNIST\u7528Dataset\u6e96\u5099","f274f72a":"# \u76ee\u7684\n\nMNIST\u3092resnet18\u3092\u4f7f\u3063\u3066\u591a\u5024\u5206\u985e\u3059\u308b\u30b3\u30fc\u30c9\u3092\u30d9\u30fc\u30b9\u306b\u3057\u3066\u3001\u4e2d\u3067\u4f7f\u308f\u308c\u3066\u3044\u308b\u6a5f\u80fd\u3084\u3001\u306a\u305c\u305d\u3093\u306a\u4e8b\u3092\u3057\u3066\u3044\u308b\u304b\u3092\n\u8efd\u304f\u6df1\u5800\u308a\u3057\u3001pytorch\u3092\u5c11\u3057\u306f\u81ea\u7531\u306b\u4f7f\u3048\u308b\u3088\u3046\u306b\u306a\u308b\u4e8b\u3092\u76ee\u6307\u3057\u307e\u3059\u3002\n\n## \u30d9\u30fc\u30b9\u306b\u3057\u305f\u30b3\u30fc\u30c9\n\nTony\u3055\u3093<br>\nMNIST with the new PyTorch 1.0.1<br>\nhttps:\/\/www.kaggle.com\/tonysun94\/pytorch-1-0-1-on-mnist-acc-99-8\n","eb27614b":"### RandAffine\u306b\u3064\u3044\u3066\n\n\u624b\u66f8\u304d\u6587\u5b57\u306a\u306e\u3067\u3001\u50be\u3044\u305f\u6587\u5b57\u306f\u5b58\u5728\u3057\u3066\u3044\u308b\u306f\u305a\u3002\n\u50be\u3051\u305f\u753b\u50cf\u3092\u4f5c\u3063\u3066\u5b66\u7fd2\u3057\u305f\u307b\u3046\u304c\u52b9\u679c\u304c\u9ad8\u3044\u3068\u4e88\u60f3\u3055\u308c\u308b\u304c\u3001\u4eca\u56de\u306fTAT\u512a\u5148\u3067\u3068\u308a\u3042\u3048\u305a\u4eca\u306foff","6507ac2c":"### train\/test\u306e\u9055\u3044\n\ntrain\u306b\u306f\u6b63\u89e3laber\u304c\u3042\u308a\u3001test\u306b\u306f\u7121\u3044\u3002\n\u6b63\u89e3label\u3092\u4f7f\u3063\u3066\u3001\u6b63\u3057\u304f\u6b63\u89e3\u3092\u5c0e\u304d\u51fa\u305b\u308bmodel\u3092\u4f5c\u308a\u3001\u4f5c\u3063\u305fmodel\u3068test\u306e\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u3001\u6b63\u89e3\u3092\u4e88\u60f3\u3059\u308b\u306e\u304c\u3001\u3053\u306e\u30b3\u30f3\u30da\u306e\u76ee\u7684\u306e\u305f\u3081\u3002","59fab51a":"### unsqueeze\n\u6b21\u5143\u3092\u8ffd\u52a0\u3002unsqueeze(1)\u306e1\u306f\u6b21\u5143\u3092\u8ffd\u52a0\u3059\u308b\u8ef8\uff08\u65b9\u5411\uff09<br>\nnn.Conv2d \u306f \u30b5\u30f3\u30d7\u30eb\u6570 x \u30c1\u30e3\u30cd\u30eb\u6570 x \u9ad8\u3055 x \u5e45 \u2013 nSamples x nChannels x Height x Width \u306e 4D Tensor \u3092\u53d6\u308b\u70ba\u3001\u4e00\u6b21\u5143\u8ffd\u52a0\u3057\u3066\u3044\u308b\u3002\n\u3082\u3057\u8cb4\u65b9\u304c\u5358\u4e00\u306e\u30b5\u30f3\u30d7\u30eb\u3092\u6301\u3064\u5834\u5408\u306b\u306f\u3001fake \u30d0\u30c3\u30c1\u6b21\u5143\u3092\u4ed8\u52a0\u3059\u308b\u305f\u3081\u306b input.unsqueeze(0) \u3092\u4f7f\u7528\u3057\u3066\u304f\u3060\u3055\u3044\u3002\nhttps:\/\/pytorch.org\/docs\/stable\/generated\/torch.unsqueeze.html\n\n\u3053\u308c\u3082\u6b21\u5143\u3092\u5897\u3084\u3059\u65b9\u6cd5\ndf.values.reshape((-1,28,28)).astype(np.uint8)[:,:,:,None]\n\n```\n>>> x = torch.tensor([1, 2, 3, 4])\n>>> torch.unsqueeze(x, 0)\ntensor([[ 1,  2,  3,  4]])\n>>> torch.unsqueeze(x, 1)\ntensor([[ 1],\n        [ 2],\n        [ 3],\n        [ 4]])\n```","7ff4fe36":"### model\u306bdata\u3092\u5165\u529b\u3057\u305f\u3068\u304d\nresnet\u306e\u7d99\u627f\u3082\u3068\u306enn.Module\u3092\u8aad\u3080\u3068\u3001forowrd\u304c\u547c\u3070\u308c\u3066\u3044\u308b\u3002<br>\nforward\u5b9f\u884c\u6642\u306b\u3001network\u3092\u5f62\u4f5c\u3063\u3066\u3044\u308b<br>\nhttps:\/\/pytorch.org\/docs\/stable\/_modules\/torch\/nn\/modules\/module.html#Module.eval\n\nforword\u304c\u547c\u3070\u308c\u308b\u3068\u3059\u308b\u3068\u3001resnet\u306e\u51fa\u529b\u304c\u5e30\u3063\u3066\u304d\u3066\u3044\u308b\u3002\u306a\u306e\u3067\u3001\u30b5\u30a4\u30ba10\u306e\u30d9\u30af\u30bf\u3002<br>\nhttps:\/\/pytorch.org\/docs\/stable\/_modules\/torchvision\/models\/resnet.html\n","1a775a97":"# Prediction on Test Set","4923823b":"### MNIST\u7528\u306bResNet\u3092\u4fee\u6b63\n\n\u30c7\u30d5\u30a9\u30eb\u30c8\u306eResNet\u306e\u5165\u529b\u30c1\u30e3\u30f3\u30cd\u30eb\u6570(RGB\u306e\u90e8\u5206)\u3068\u51fa\u529b\u30af\u30e9\u30b9\u6570\u3092\u3001MNIST\u7528\u306b\u8abf\u6574\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002\n\u5165\u529b\u30d9\u30af\u30bf\u30b5\u30a4\u30ba\u306f\u305f\u3057\u304b\u81ea\u52d5\u8abf\u6574\u3002\n\u4ee5\u4e0b\u306f\u305d\u306e\u8aac\u660e\u3002\n\n\n### Modify the official ResNet for MNIST\n\nThe official ResNet model is designed for the classification task on RGB image dataset, e.g. ImagNet. The main difference between ImageNet and MNIST is the following two points:\n\n| Input Image  |      ImageNet      |       MNIST       |\n| :---------:  | :----------------: | :---------------: |\n|  Channels    |      3    |     1       |\n| Resolution   | 224 x 224  | 28 x 28  |\n| Classes  | 1000 | 10 |\nSo we should better do some modifications on the original networks to adapt for MNIST dataset. Specifically, add the following modifications:\n\n* **Change the first convolutional layer to accept single channel input**\n* **Change the stride of the first convolutional layer from 2 to 1**\n* **Change the last fc layer's output features from 1000 to 10**\n\nBefore we start, in order to know which part of the code to modify, let's take a look how we get a model when calling the `torchvision.models.resnet18()` function.\n\nHere is the source code of `torchvision.models.resnet18()`. When we construct the ResNet18 architecture, we are running the following procedure."}}