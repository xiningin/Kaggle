{"cell_type":{"0ccabcd8":"code","c115b260":"code","db437b9e":"code","6efa741b":"code","d43878d9":"code","994abb47":"code","a372309c":"code","e50dc5e5":"code","628eff56":"code","39b881b6":"code","d2d630f2":"code","422e3977":"code","898f3c77":"code","e48ce4fb":"code","3803e508":"code","ec9c5b06":"code","12d6772c":"code","c530a8d1":"code","340f5cef":"code","c0148f67":"code","50b91c77":"code","d7c6aff4":"markdown","c22dc028":"markdown","beaccc78":"markdown","729d701a":"markdown","ed9e063e":"markdown","01449adc":"markdown","477be7da":"markdown","13ab0167":"markdown","2e118b3d":"markdown","eb846bb5":"markdown","ab230c8e":"markdown","55b1250f":"markdown","59294131":"markdown","271777d4":"markdown","98b3f7e9":"markdown","b62fe527":"markdown"},"source":{"0ccabcd8":"import numpy as np\nimport pandas as pd\nimport re\nimport lightgbm as lgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nfrom sklearn import tree\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom IPython.display import Image as PImage\nfrom subprocess import check_call\nfrom PIL import Image, ImageDraw, ImageFont\n\n# Loading the data\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = test['PassengerId']\n\n# Showing overview of the train dataset\ntrain.head()","c115b260":"test.head()","db437b9e":"def get_best_score(model):\n    \n    print(model.best_score_)\n    print(model.best_params_)\n    print(model.best_estimator_)\n    \n    return model.best_score_","6efa741b":"# Copy original dataset in case we need it later when digging into interesting features\n# WARNING: Beware of actually copying the dataframe instead of just referencing it\n# \"original_train = train\" will create a reference to the train variable (changes in 'train' will apply to 'original_train')\noriginal_train = train.copy() # Using 'copy()' allows to clone the dataset, creating a different object with the same values\n\n# Feature engineering steps taken from Sina and Anisotropic, with minor changes to avoid warnings\nfull_data = [train, test]\n\n# Feature that tells whether a passenger had a cabin on the Titanic\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\n# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n# Create new feature IsAlone from FamilySize\nfor dataset in full_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n# Remove all NULLS in the Embarked column\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n# Remove all NULLS in the Fare column\nfor dataset in full_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n\n# Define function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\nfor dataset in full_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n# Group all non-common titles into one single grouping \"Rare\"\nfor dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\nfor dataset in full_data:\n    # Mapping Sex\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n    \n    # Mapping titles\n    title_mapping = {\"Mr\": 1, \"Master\": 2, \"Mrs\": 3, \"Miss\": 4, \"Rare\": 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\n    # Mapping Embarked\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\n# Remove all NULLS in the Age column\nfor dataset in full_data:\n    for title in dataset.Title.unique():\n        dataset.loc[(dataset.Age.isnull())&(dataset.Title==title),'Age'] = train.Age[train.Title==title].mean()","d43878d9":"# Feature selection: remove variables no longer containing relevant information\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\ntrain = train.drop(drop_elements, axis = 1)\ntest  = test.drop(drop_elements, axis = 1)","994abb47":"train.head()","a372309c":"test.head()","e50dc5e5":"colormap = plt.cm.viridis\nplt.figure(figsize=(12,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True);","628eff56":"X_train = train.iloc[:, 1:]\ny_train = train.iloc[:, 0]\nX_test = test\n\nX_train.head()","39b881b6":"model_default = lgb.LGBMClassifier()\n\ncv_scores = cross_val_score(model_default, X_train, y_train, cv=5, scoring='accuracy')\n\nprint('Average accuracy: ', cv_scores.mean())\n\nmodel_default.fit(X_train, y_train);","d2d630f2":"from sklearn.model_selection import GridSearchCV\n\nmodel = lgb.LGBMClassifier()\n\n# Set list of values for each parameter\nparam_grid = {\n    'learning_rate': 10**np.linspace(-2, 0, 4), # from 0.01 to 1\n    'n_estimators': [100],\n    'num_leaves': np.linspace(4, 20, 4).astype(int),\n    'reg_alpha': np.linspace(0, 1, 4),\n}\n\nmodel_grid = GridSearchCV(model, param_grid, cv=5)\nmodel_grid.fit(X_train, y_train)\n\nget_best_score(model_grid);","422e3977":"from sklearn.model_selection import RandomizedSearchCV\nimport scipy.stats as sps\n\nmodel = lgb.LGBMClassifier()\n\n# Set random distribution for each parameter\nparam_grid = {\n    'learning_rate': sps.loguniform(0.01, 1),\n    'n_estimators': sps.randint(10, 200),\n    'num_leaves': sps.randint(2, 50),\n    'reg_alpha': sps.uniform(0, 1),\n    'reg_lambda': sps.uniform(0, 10),\n    'min_split_gain': sps.uniform(0, 1),\n}\n\nmodel_rand = RandomizedSearchCV(model, param_grid, cv=5, n_iter=100)\nmodel_rand.fit(X_train, y_train)\n\nget_best_score(model_rand);","898f3c77":"from sklearn.base import clone\nfrom sklearn.model_selection import cross_val_score\nfrom bayes_opt import BayesianOptimization\n\nclass BayesianSearchCV:\n    '''\n    Bayesian Search with cross validation score.\n    \n    Arguments:\n    \n    base_estimator: sklearn-like model\n    param_bounds: dict\n        hyperparameter upper and lower bounds\n        example: {\n            'param1': [0, 10],\n            'param2': [-1, 2],\n        }\n    scoring: string or callable\n        scoring argument for cross_val_score\n    cv: int\n        number of folds\n    n_iter: int\n        number of bayesian optimization iterations\n    init_points: int\n        number of random iterations before bayesian optimization\n    random_state: int\n        random_state for bayesian optimization\n    int_parameters: list\n        list of parameters which are required to be of integer type\n        example: ['param1', 'param3']\n    '''\n    \n    def __init__(\n        self,\n        base_estimator,\n        param_bounds,\n        scoring,\n        cv=5,\n        n_iter=50,\n        init_points=10,\n        random_state=1,\n        int_parameters=[],\n    ):\n        self.base_estimator = base_estimator\n        self.param_bounds = param_bounds\n        self.cv = cv\n        self.n_iter = n_iter\n        self.init_points = init_points\n        self.scoring = scoring\n        self.random_state = random_state\n        self.int_parameters = int_parameters\n    \n    def objective(self, **params):\n        '''\n        We will aim to maximize this function\n        '''\n        # Turn some parameters into ints\n        for key in self.int_parameters:\n            if key in params:\n                params[key] = int(params[key])\n        # Set hyperparameters\n        self.base_estimator.set_params(**params)\n        # Calculate the cross validation score\n        cv_scores = cross_val_score(\n            self.base_estimator,\n            self.X_data,\n            self.y_data,\n            cv=self.cv,\n            scoring=self.scoring)\n        score = cv_scores.mean()\n        return score\n    \n    def fit(self, X, y):\n        self.X_data = X\n        self.y_data = y\n        \n        # Create the optimizer\n        self.optimizer = BayesianOptimization(\n            f=self.objective,\n            pbounds=self.param_bounds,\n            random_state=self.random_state,\n        )\n        \n        # The optimization itself goes here:\n        self.optimizer.maximize(\n            init_points=self.init_points,\n            n_iter=self.n_iter,\n        )\n        \n        del self.X_data\n        del self.y_data\n        \n        # Save best score and best model\n        self.best_score_ = self.optimizer.max['target']\n        self.best_params_ = self.optimizer.max['params']\n        for key in self.int_parameters:\n            if key in self.best_params_:\n                self.best_params_[key] = int(self.best_params_[key])\n        \n        self.best_estimator_ = clone(self.base_estimator)\n        self.best_estimator_.set_params(**self.best_params_)\n        self.best_estimator_.fit(X, y)\n        \n        return self\n    \n    def predict(self, X):\n        return self.best_estimator_.predict(X)\n    \n    def predict_proba(self, X):\n        return self.best_estimator_.predict_proba(X)","e48ce4fb":"model = lgb.LGBMClassifier()\n\n# Set only upper and lower bounds for each parameter\nparam_grid = {\n    'learning_rate': (0.01, 1),\n    'n_estimators': (10, 200),\n    'num_leaves': (2, 50),\n    'reg_alpha': (0, 1),\n    'reg_lambda': (0, 10),\n    'min_split_gain': (0, 1),\n}\n\nmodel_bayes = BayesianSearchCV(\n    model, param_grid, cv=5, n_iter=60, scoring='accuracy',\n    int_parameters=['n_estimators', 'num_leaves'])\n\nmodel_bayes.fit(X_train, y_train)","3803e508":"get_best_score(model_bayes);","ec9c5b06":"predictions = model_default.predict(X_test)\n\nsubmission = pd.DataFrame()\nsubmission['PassengerId'] = PassengerId\nsubmission['Survived'] = predictions\nsubmission.to_csv('default.csv',index=False)\n\nsubmission.head()","12d6772c":"predictions = model_grid.predict(X_test)\n\nsubmission = pd.DataFrame()\nsubmission['PassengerId'] = PassengerId\nsubmission['Survived'] = predictions\nsubmission.to_csv('grid.csv',index=False)\n\nsubmission.head()","c530a8d1":"predictions = model_rand.predict(X_test)\n\nsubmission = pd.DataFrame()\nsubmission['PassengerId'] = PassengerId\nsubmission['Survived'] = predictions\nsubmission.to_csv('rand.csv',index=False)\n\nsubmission.head()","340f5cef":"predictions = model_bayes.predict(X_test)\n\nsubmission = pd.DataFrame()\nsubmission['PassengerId'] = PassengerId\nsubmission['Survived'] = predictions\nsubmission.to_csv('bayes.csv',index=False)\n\nsubmission.head()","c0148f67":"from sklearn.tree import DecisionTreeClassifier\n\nmodel = DecisionTreeClassifier()\n\ncv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n\nprint('Average accuracy: ', cv_scores.mean())","50b91c77":"model = DecisionTreeClassifier()\n\n# Set only upper and lower bounds for each parameter\nparam_grid = {\n    'max_depth': (2, 16),\n    'min_samples_split': (2, 20),\n    'min_samples_leaf': (1, 20),\n    'max_leaf_nodes': (2, 100),\n    'min_impurity_decrease': (0, 0.02),\n    'ccp_alpha': (0, 0.01),\n}\n\nmodel_bayes = BayesianSearchCV(\n    model, param_grid, cv=5, n_iter=60, scoring='accuracy',\n    int_parameters=['max_depth', 'min_samples_split', 'min_samples_leaf', 'max_leaf_nodes'])\n\nmodel_bayes.fit(X_train, y_train)\n\nget_best_score(model_bayes);","d7c6aff4":"Some useful functions:","c22dc028":"RandomSearch is easier to use then GridSearch.  \nHowever, it is still too stupid to work fast.  \nIt doesn't consider previous cross-validation results, when sampling new hyperparameters.  \nHence, it can choose bad hyperparameters over and over again with the same probability.  ","beaccc78":"There were 70 iterations and not 60, because bayesian optimisation also does some initial random steps.  \n\nWe can see that it took less iterations to find better hyperparameters than that of RandomizedSearchCV.","729d701a":"We cannot tune a lot of hyperparameters, since it will increase the amount of computation exponentially.","ed9e063e":"# Default Model\n\nLet's train the [LGBMClassifier](https:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.LGBMClassifier.html) model with all the default hyperparameters.","01449adc":"# 3 approaches to hyperparameter search\n\nIn this notebook we will look at 3 types of hyperparameter search:  \n1. **Grid search**\n2. **Random search**\n3. **Bayesian search**\n\nI will show you the simple `BayesianSearchCV` class written by me, based on https:\/\/github.com\/fmfn\/BayesianOptimization library.  \n\n### EDA\nIn this notebook there will be no EDA.  \nIf you are not familiar with Titanic dataset, here is a good notebook to start with:  \nhttps:\/\/www.kaggle.com\/dejavu23\/titanic-survival-seaborn-and-ensembles  \n\n### Feature engineering\nData preparation in this notebook is simple and taken from:  \nhttps:\/\/www.kaggle.com\/dmilla\/introduction-to-decision-trees-titanic-dataset  ","477be7da":"# Results on the test set\nLeaderboard results (from version 3 of this notebook):\n```\ndefault: 0.74880\ngrid: 0.75837\nrand: 0.75837\nbayes: 0.76076\n```","13ab0167":"# GridSearchCV\n\nGridSearch is a method based on bruteforce: just take all possible combinations of all hyperparameters, and crossvalidate on each of them.","2e118b3d":"# BayesianSearchCV\n\nBayesian search is like random search but much much smarter.  \nThe optimisation is based on [gaussian process](https:\/\/en.wikipedia.org\/wiki\/Gaussian_process).\n\nHere is a good intro into gaussian processes: https:\/\/katbailey.github.io\/post\/gaussian-processes-for-dummies\/\n\nThere is no `BayesianSearchCV` class in sklearn.  \nBut there is a good library that implements all of the bayesian optimization routines: https:\/\/github.com\/fmfn\/BayesianOptimization\n\n```\nfrom bayes_opt import BayesianOptimization\n```\nIf we import the `BayesianOptimization` class from this library, we can easily optimize any function we want with any hyperparameters.  \nOur function to optimize will be `cross_val_score`","eb846bb5":"# Preprocess data","ab230c8e":"# Writing submission files","55b1250f":"# Example with decision tree","59294131":"This model class supports sklearn API.  \nWe can use it in our sklearn-based pipelines.  ","271777d4":"# RandomizedSearchCV\n\nRandomSearchCV is based on random sampling of parameters.\n\nWith random search we can set more hyperparameters to tune.  \nAlso, we can stop worrying about the steps in the hyperparameter grid.  \nWe just set the number of iterations.","98b3f7e9":"# Loading the data","b62fe527":"Here I present the `BayesianSearchCV` class which is compatible with sklearn.  \n\nCheck out the repository: https:\/\/github.com\/hocop\/bayesian_search_cv\n\nSource code from the repository is copied here:"}}