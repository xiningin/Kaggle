{"cell_type":{"dfa89480":"code","bbd5d07f":"code","ee12e4f2":"code","cb7e882a":"code","fab06c6b":"code","c88b9e69":"code","ffa1ac0b":"code","7a938e9d":"code","18d9ef71":"code","8d1c0e55":"code","0995ad80":"code","19bb5949":"code","80c848df":"code","4c39ae44":"code","f679c8e9":"code","9780f8fe":"code","977ff6a3":"code","0169e3f3":"code","7264c4a9":"code","1ff06bbe":"code","41f171ca":"code","66ef339b":"code","d1be387e":"code","9b6e768a":"code","b7f197c9":"code","85f6feeb":"code","1ba55cff":"code","fac3fced":"code","1384423f":"code","4f442aae":"code","e133fd1f":"code","dfaf699b":"code","12eb6999":"code","8604d2e8":"code","7cf3a133":"code","12e406af":"code","0072d103":"code","454d9e84":"code","69581e4e":"code","5c106350":"code","fefc055f":"code","a98403ad":"code","858130f7":"code","e3b6400e":"code","83d39c87":"code","369adc6e":"code","7ec0a80e":"code","ce729370":"code","2d90e86c":"code","b036c5ef":"code","b60b473d":"code","6df8e4e9":"markdown","35fdc1e4":"markdown","9cb88716":"markdown","a9f12ee6":"markdown","6800d8e7":"markdown","8763089a":"markdown","16097828":"markdown","be0585fd":"markdown","da6cef87":"markdown","04e4c207":"markdown","95d707e3":"markdown","a1470b98":"markdown","d6bf822c":"markdown","f1212d49":"markdown","ba37724c":"markdown","e2f97802":"markdown","9fb095a8":"markdown","ae4257a9":"markdown","b7f3928c":"markdown","0cf65417":"markdown","b09404f2":"markdown","e2a3a3a0":"markdown","9ae49a2b":"markdown","fd62d0c7":"markdown","f02e011c":"markdown","88f70613":"markdown","08b72db5":"markdown","7b2719ef":"markdown","11d44529":"markdown","83f1a257":"markdown","fabf3b48":"markdown","92b25094":"markdown","246b9e2e":"markdown","3ecc3698":"markdown","2bf95dc5":"markdown","462a8600":"markdown","dbc76662":"markdown","acb3e650":"markdown","41d33f74":"markdown","77bb95a4":"markdown","7acfe464":"markdown","6d22fca1":"markdown","12ea6a95":"markdown","f4c35982":"markdown"},"source":{"dfa89480":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy.stats import norm\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\n%matplotlib inline\npd.plotting.register_matplotlib_converters()\nimport seaborn as sns\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","bbd5d07f":"path = '..\/input\/house-prices-advanced-regression-techniques\/train.csv'\ndf = pd.read_csv(path)\ndf.shape","ee12e4f2":"# df.columns","cb7e882a":"df.head(3)","fab06c6b":"# df.info() # types","c88b9e69":"df['SalePrice'].describe()","ffa1ac0b":"corrmat = df.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","7a938e9d":"k = 10 # number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10},\n                 yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","18d9ef71":"sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(df[cols], size = 2.5)\nplt.show();","8d1c0e55":"x = 'GrLivArea'\ny = 'SalePrice'\ndata = pd.concat([df[y], df[x]], axis=1)\ndata.plot.scatter(x=x, y=y, ylim=(0,800000))","0995ad80":"x = 'TotalBsmtSF'\ny = 'SalePrice'\ndata = pd.concat([df[y], df[x]], axis=1)\ndata.plot.scatter(x=x, y=y, ylim=(0,800000))","19bb5949":"x = 'OverallQual'\ny = 'SalePrice'\ndata = pd.concat([df[x], df[y]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=x, y=y, data=data)\nfig.axis(ymin=0, ymax=800000)","80c848df":"x = 'YearBuilt'\ny = 'SalePrice'\ndata = pd.concat([df[y], df[x]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=x, y=y, data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);","4c39ae44":"plt.figure(figsize=(22,8))\nsns.heatmap(df.isnull(), yticklabels = False, cbar = False) # missing values noise\nplt.show()","f679c8e9":"plt.figure(figsize=(20,8))\nplt.title('Percentage of Missing Data')\nplt.xlabel('Features')\nplt.xticks(rotation=90) \nplt.ylabel('Percentage')\nplt.ylim(0, 100)\nsns.barplot(x = df.columns, y = df.isnull().sum()\/len(df)*100)\nplt.show()","9780f8fe":"keys = ['Missing Values', 'Percentage']\ncount = len(df) # df.isnull().count()\ntotal = df.isnull().sum()\npercent = round(df.isnull().sum()\/count*100, 1)\nmissing_data = pd.concat([total, percent], axis=1, keys=keys)\nmissing_data.sort_values(by=keys[1], ascending=False).head(10)","977ff6a3":"outliers = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'LotFrontage']\ndf = df.drop(outliers, axis=1)\n# heuristic: removing columns based on my personal experience\ndrop_features = ['Id','Street','GarageYrBlt','YrSold','MoSold','LotShape','LotConfig',\n                 'LandContour','BldgType']\ndf = df.drop(drop_features, axis=1)\ndf.shape","0169e3f3":"df.head(3)","7264c4a9":"df['Age'] = 2020 - df['YearBuilt']\ndf['RemodelAge'] = 2020 - df['YearRemodAdd']\ndrop_engineered_features = ['YearBuilt', 'YearRemodAdd']\ndf = df.drop(drop_engineered_features, axis=1)\ndrop_features.extend(drop_engineered_features)\nplt.figure(figsize=(16,10))\nsns.heatmap(df.corr(), annot= False)\nplt.title('Correlation Matrix')","1ff06bbe":"numeric_imputer_features = ['MasVnrArea'] # Masonry veneer area in square feet\nprint(pd.isnull(df[numeric_imputer_features]).sum())","41f171ca":"numeric_imputer = SimpleImputer(strategy='constant', fill_value=0)\nnumeric_imputed_features = pd.DataFrame(numeric_imputer.fit_transform(df[numeric_imputer_features]), \n                                        columns = numeric_imputer_features)\ndf = df.drop(numeric_imputer_features, axis = 1)","66ef339b":"category_imputer_features = ['GarageCond', 'GarageType', 'GarageFinish', 'GarageQual', 'BsmtExposure',\n                             'BsmtFinType2', 'BsmtQual', 'BsmtCond', 'BsmtFinType1', 'MasVnrType']\nprint(pd.isnull(df[category_imputer_features]).sum())","d1be387e":"category_imputer = SimpleImputer(strategy='constant', fill_value='None')\ncategory_imputed_features = pd.DataFrame(category_imputer.fit_transform(df[category_imputer_features]), \n                                         columns = category_imputer_features)\ndf = df.drop(category_imputer_features, axis = 1)","9b6e768a":"df = df.reset_index() # creates index\ndf = df.drop('index', axis = 1) # new index\ndf[category_imputer_features] = category_imputed_features[category_imputer_features]\ndf[numeric_imputer_features] = numeric_imputed_features[numeric_imputer_features]","b7f197c9":"# Drop rows with missing prices\ndf = df.dropna(subset=['Electrical'])\n# Get list of Categorical Variables\nobject_columns = [column for column in df.columns if df[column].dtype == 'object']\n\n# Get number of unique entries in each column with categorical data\nobject_unique = list(map(lambda column: df[column].nunique(), object_columns))\ncategory_dictionary = dict(zip(object_columns, object_unique))\n# Print number of unique entries by column, in ascending order along with cardinality\nsorted(category_dictionary.items(), key=lambda x: x[1])","85f6feeb":"# Features to Inspect\ninspect_features = ['MSZoning', 'Utilities', 'CentralAir', 'Foundation', 'LandSlope',\n                    'HouseStyle'] # Street\n# Define Subplot Grid\nf, axes = plt.subplots(int(np.ceil(len(inspect_features)\/4)), 4,\n                       figsize=(22, 4*int(np.ceil(len(inspect_features)\/4))))\n# Render Plots\ncolumn_counter = 0\nfor index, feature in enumerate(inspect_features):\n    sns.scatterplot(x=df[feature], y=df['SalePrice'], ax=axes[int(np.floor(index\/4)), column_counter])\n    if column_counter == 3:\n        column_counter = 0\n    else:\n        column_counter += 1","1ba55cff":"sns.distplot(df['SalePrice']) # histogram","fac3fced":"print(\"Skewness: %f\" % df['SalePrice'].skew())","1384423f":"print(\"Kurtosis: %f\" % df['SalePrice'].kurt())","4f442aae":"fig = plt.figure()\nres = stats.probplot(df['SalePrice'], plot=plt) # probability plot","e133fd1f":"df['SalePrice'] = np.log(df['SalePrice']) # log transformation\nsns.distplot(df['SalePrice'], fit=norm) # transformed histogram","dfaf699b":"print(\"Skewness: %f\" % df['SalePrice'].skew()) # after log transformation","12eb6999":"print(\"Kurtosis: %f\" % df['SalePrice'].kurt()) # after log transformation","8604d2e8":"fig = plt.figure()\nres = stats.probplot(df['SalePrice'], plot=plt) # probability plot","7cf3a133":"# Features to drop\ndrop_high_cardinality_features = ['Neighborhood', 'Condition1', 'Condition2', 'HouseStyle',\n                                  'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd',\n                                  'ExterQual', 'ExterCond', 'Heating', 'HeatingQC', 'Electrical',\n                                  'KitchenQual', 'Functional', 'PavedDrive', 'SaleType',\n                                  'SaleCondition', 'GarageCond', 'GarageType', 'GarageFinish',\n                                  'GarageQual', 'BsmtExposure', 'BsmtFinType2', 'BsmtQual', 'BsmtCond',\n                                  'BsmtFinType1', 'MasVnrType']\n# Drop features\ndf = df.drop(drop_high_cardinality_features, axis=1)\n\n# Append list to drop_features, so that test set could be set up accordingly\ndrop_features.extend(drop_high_cardinality_features)\n\n# Features to Encode\nlabel_features = ['Utilities', 'CentralAir', 'Foundation', 'LandSlope'] # Street\n\n# Apply Label Encoder \nlabel_encoder = LabelEncoder()\nfor feature in label_features:\n    df[feature] = label_encoder.fit_transform(df[feature])\n\n# Features to Encode ('HouseStyle')\none_hot_features = ['MSZoning']\n\n# Initialize one-hot encoder\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\n# Apply one-hot encoder to each column with categorical data\nOH_features = pd.DataFrame(OH_encoder.fit_transform(df[one_hot_features]))\n\n# Remove categorical columns (will replace with one-hot encoding)\nnumeric_features = df.drop(one_hot_features, axis=1)\n\n# Add one-hot encoded columns to numerical features\ndf = pd.concat([numeric_features, OH_features], axis=1)\n\n# Drop NaN observations\ndf = df.dropna()\n\n# Final Shape of the training set\ndf.shape","12e406af":"X_train, X_test, y_train, y_test = train_test_split(df.drop(['SalePrice'], axis = 1), \n                                                    df['SalePrice'], test_size = .20, \n                                                    random_state= 0)\ntransformer = RobustScaler().fit(X_train)\nX_train_scaled = transformer.transform(X_train)\nX_test_scaled = transformer.transform(X_test)\nmodels = {\n    'LinearRegression': {\n        'model': LinearRegression()\n    },\n    'RandomForestRegressor': {\n        'model': RandomForestRegressor(n_estimators=80, max_depth=30, random_state = 0)\n    },\n    'XGBRegressor': {\n        'model': XGBRegressor(n_estimators = 150, max_depth = 30, learning_rate = 0.1, random_state = 2)\n    }\n}\n\n# Add dictionary attributes\nfor model in models:\n    models[model]['prediction'] = None\n    models[model]['errors'] = {\n        'mae': None,\n        'mse': None,\n        'rmse': None\n    }\n    models[model]['scores'] = {\n        'r2': None\n    }\n    \nfor model in models:\n    print('Running ', models[model]['model'])\n    models[model]['model'].fit(X_train_scaled, y_train)\n    models[model]['predictions'] = models[model]['model'].predict(X_test_scaled)\n    models[model]['errors']['mae'] = metrics.mean_absolute_error(y_test, models[model]['predictions'])\n    models[model]['errors']['mse'] = metrics.mean_squared_error(y_test, models[model]['predictions'])\n    models[model]['errors']['rmse'] = np.sqrt(models[model]['errors']['mse'])\n    models[model]['scores']['r2'] = metrics.r2_score(y_test, models[model]['predictions'])\n    print('MAE: ', models[model]['errors']['mae'])\n    print('MSE: ', models[model]['errors']['mse'])\n    print('RMSE: ', models[model]['errors']['rmse'])\n    print('R2: ', models[model]['scores']['r2'])\n    print('\\n')","0072d103":"for index, model in enumerate(models):\n    sns.scatterplot(models[model]['predictions'], y_test)\n    plt.title(model)\n    plt.show()\n    sns.distplot((y_test - models[model]['predictions']))\n    plt.title(model)\n    plt.show()","454d9e84":"df_test_original = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf_test_original.shape","69581e4e":"df_test_original.head(3)","5c106350":"df_test = df_test_original.copy()\ndf_test.shape","fefc055f":"df_test.head(3)","a98403ad":"df_test['Age'] = 2020 - df_test['YearBuilt']\ndf_test['RemodelAge'] = 2020 - df_test['YearRemodAdd']","858130f7":"category_imputed_features_test = pd.DataFrame(category_imputer.transform(df_test[category_imputer_features]), \n                                              columns = category_imputer_features)\nnumeric_imputed_features_test = pd.DataFrame(numeric_imputer.transform(df_test[numeric_imputer_features]), \n                                             columns = numeric_imputer_features)\ndf_test = df_test.drop(category_imputer_features, axis = 1)\ndf_test = df_test.drop(numeric_imputer_features, axis = 1)\ndf_test = df_test.reset_index()\ndf_test = df_test.drop('index', axis = 1)\ndf_test[category_imputer_features] = category_imputed_features_test[category_imputer_features]\ndf_test[numeric_imputer_features] = numeric_imputed_features_test[numeric_imputer_features]\ndf_test = df_test.drop(drop_features, axis=1)","e3b6400e":"# Fill 0 in case of NaN's in remaining features\ndf_test = df_test.fillna(0)","83d39c87":"# Unknown\/New Categories\nlabel_encoder.classes_ = np.append(label_encoder.classes_, '<unknown>')","369adc6e":"# Transform with Label Encoder\nfor feature in label_features:\n    \n    # Handle new categories in test set\n    df_test[feature] = df_test[feature].map(lambda s: '<unknown>' if s not in label_encoder.classes_ else s)\n    \n    # Transform\n    df_test[feature] = label_encoder.transform(df_test[feature])","7ec0a80e":"# Transform with One-Hot Encoder\nOH_test = pd.DataFrame(OH_encoder.transform(df_test[one_hot_features]))\nOH_test.index = df_test.index\ndf_test = df_test.drop(one_hot_features, axis=1)\ndf_test = pd.concat([df_test, OH_test], axis=1)","ce729370":"df_test.shape","2d90e86c":"outliers = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'LotFrontage']\ndf_test = df_test.drop(outliers, axis=1)\ndf_test.shape","b036c5ef":"df_test.head(3)","b60b473d":"# Apply Scaling\ndf_test_scaled = transformer.transform(df_test)\n\n# Generate Predictions\npredictions = models['XGBRegressor']['model'].predict(df_test_scaled)\n\n# Generate Output Frame\ndf_predictions = pd.DataFrame({'Id': df_test_original.Id, 'SalePrice': predictions})\n\n# Submit results\ndf_predictions.to_csv('submission.csv', index=False)","6df8e4e9":"<a id=\"changelog\"><\/a>\n# Change Log\n2020-11-25 First version","35fdc1e4":"## 1.4 Box Plots\nWhen the correlation is not too obvious then we choose the box plot instead...","9cb88716":"Let's take a look at the columns.","a9f12ee6":"Conclusion: The kurtosis is closer to zero now.","6800d8e7":"## 1.3 Scatter Plots\nLet's see look at the chosen features or variables on a scatter plot.","8763089a":"<a id=\"credits\"><\/a>\n# Credits\nThe [Ames Housing dataset](http:\/\/www.amstat.org\/publications\/jse\/v19n3\/decock.pdf) was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset.","16097828":"## 2.2 Feature Engineering\n\nLet's generate new columns or variables that can help us...","be0585fd":"Conclusion: The distribution for Sales Price looks a lot more closer to normal now.","da6cef87":"### 2.5 Log Transformation","04e4c207":"Let's describe the dependent variable (Y) that we want to predict.","95d707e3":"<a id=\"step3\"><\/a>\n# Step 3: Evaluate The Outcomes","a1470b98":"Conclusion: OverallQual and SalePrice have some correlation too.","d6bf822c":"## 1.2.1 Sale Price Correlation Matrix\nMore detailed.","f1212d49":">Hello! My name is [Mauricio Ruanova](https:\/\/mruanova.com) and I am following the [Decision Making](https:\/\/en.wikipedia.org\/wiki\/Decision-making) process.\nThis notebook is part of the competition [House Prices: Advanced Regression Techniques](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques): Predict sales prices and practice feature engineering, Random Forests, and gradient boosting.\n\n![Houses](https:\/\/mruanova.com\/houses.png)\n\nTable of Contents\n1. [Step 1 - Identify The Problem](#step1)\n1. [Step 2 - Explore Your Options](#step2)\n1. [Step 3 - Evaluate The Outcomes](#step3)\n1. [Step 4 - Decide And Act](#step4)\n1. [Change Log](#changelog)\n1. [Credits](#credits)\n\n<a id=\"step1\"><\/a>\n# Step 1: Identify The Problem\nIn this problem we have a clear goal: to predict the final price (SalePrice) of each home in the Ames, Iowa housing dataset based on 79 other variables,\nusing [Exploratory Data Analysis](https:\/\/en.wikipedia.org\/wiki\/Exploratory_data_analysis) \nand [Machine Learning](https:\/\/en.wikipedia.org\/wiki\/Machine_learning).\n\nPlease take a look at the [Data Dictionary](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data)\n\n## 1.1 Exploratory Data Analysis (EDA)","ba37724c":"## 1.3.2 Scatter Plot: TotalBsmtSF\nTotalBsmtSF: Total square feet of basement area.","e2f97802":"Conclusion: The training data has 1460 rows and 81 columns: Id + SalePrice + 79 variables.","9fb095a8":"Conclusion: YearBuilt and SalePrice have some correlation but not too strong.","ae4257a9":"Conclusion: The histogram looks better but let's review the numbers.","b7f3928c":"Reduced the number of columns from 81 to 66 because we dropped columns.","0cf65417":"### 2.4 Probability Plot","b09404f2":"Let's take a look at the data types.","e2a3a3a0":"## 1.3.1 Scatter Plot: GrLivArea\nGrLivArea: Above grade (ground) living area square feet.","9ae49a2b":"Conclusion: On average a house costs $ 180,921.19\n\nbut the most expensive ones is $ 755,000.00\n\nand the most affordable one is $ 34,000.00.","fd62d0c7":"## 1.4.1 Box Plot: OverallQual\nOverallQual: Rates the overall material and finish of the house.","f02e011c":"## 1.4.2 Box Plot: YearBuilt\nYearBuilt: Original construction date.","88f70613":"Conclusion: OverallQual and GrLivArea are strongly correlated. \n\nTotalBsmtSF and 1stFloor are the same but Total Basement Square Feet makes more sense. \n\nGarageCars and GarageArea indicate the similar ideas but Garage Cars is more correlated. \n\nFullBath is also a very important variable when buying a house obviously. \n\nTotRmsAbvGrd: Total rooms above grade (does not include bathrooms).\n\nGrLivArea: Above grade (ground) living area square feet. \n\nYearBuilt: Original construction date.","08b72db5":"Acceptable values of skewness fall between \u2212 3 and + 3.","7b2719ef":"Conclusion: The skewness is closer to zero now.","11d44529":"## 2.3 Skewness and Kurtosis\nRemove columns that we won't use. Remove rows that we won't use. Find missing values.\n\nA histogram will help us look for the skewness and kurtosis.\n\nSkewness refers to distortion or asymmetry in a symmetrical bell curve, or normal distribution, in a set of data. If the curve is shifted to the left or to the right, it is said to be skewed. \n\nKurtosis is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution.","83f1a257":"Kurtosis is appropriate from a range of \u2212 10 to + 10.","fabf3b48":"## 1.2 Heatmap or Correlation Matrix\nThe best way to start a Data Visualization.","92b25094":"<a id=\"step2\"><\/a>\n# Step 2: Explore Your Options\n## 2.1 Data Cleaning\nData cleaning is the process of fixing or removing incorrect, corrupted, incorrectly formatted, duplicate, or incomplete.\n### 2.1.1 Missing Data\nWe should delete the column or variable where 15% of the data or more is missing. ","246b9e2e":"Conclusion: We found many categorical missing values that we can impute.","3ecc3698":"We will need a more detailed view for: GrLivArea, TotalBsmtSF, OverallQual, YearBuilt.","2bf95dc5":"<a id=\"step4\"><\/a>\n# Step 4: Decide And Act","462a8600":"### 2.2.1 Imputations","dbc76662":"Conclusion: GrLivArea and SalePrice have a linear relationship.","acb3e650":"Conclusion: We found 8 numeric missing values that we can impute.","41d33f74":"Let's take a look at the first 3 rows.","77bb95a4":"- PoolQC 99% missing, \n- MiscFeature 96% missing, \n- Alley 93% missing, \n- Fence 80% missing, \n- FireplaceQu 47% missing, \n- LotFrontage 17% missing.\n\nOutliers are extreme values that deviate from other observations on data, they may indicate a variability in a measurement, experimental errors or a novelty. In other words, an outlier is an observation that diverges from an overall pattern on a sample.","7acfe464":"Conclusion: Data distribution should closely follow the diagonal that represents the normal distribution. We have a positive skewness or peakedness so we should apply log transformations to normalize the distribution.","6d22fca1":"Conclusion: Age and RemodelAge are going to be very useful in our SalePrice prediction.","12ea6a95":"Conclusion: Multicollinearity on TotalBsmtSF and 1stFlrSF; also on the garage variables; same thing with GrLivArea, TotalBsmtSF, and OverallQual.","f4c35982":"Conclusion: TotalBsmtSF and SalePrice have a strong linear almost exponential reaction."}}