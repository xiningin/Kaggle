{"cell_type":{"c3e90896":"code","32e67e62":"code","b6fe4b53":"code","a26a127e":"code","c7b99933":"code","c2871af1":"code","a6562008":"code","1152ef88":"code","94cc0f67":"code","01b48c58":"code","87880f46":"code","35a2f533":"code","b436f525":"code","e3d21130":"code","f4b96f2c":"code","24fb1d20":"code","43597e22":"code","e4e42985":"code","23cde95c":"code","ea45fff7":"code","eb6f5885":"code","ed0d6904":"code","bc4cd8af":"code","e2457497":"markdown","df964f4a":"markdown","2fc4f11b":"markdown","25e5a457":"markdown","8770a1b0":"markdown","783ed0ca":"markdown","5d63c7e0":"markdown","1bb17e2d":"markdown","5d466c71":"markdown","4396e7bd":"markdown","fad79bc5":"markdown","e54abe29":"markdown","764f1b1b":"markdown","3dbde68f":"markdown"},"source":{"c3e90896":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","32e67e62":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score","b6fe4b53":"data=pd.read_csv(\"..\/input\/student-marks\/marks.txt\",header=None)\ndata.columns = ['MID_SEM','END_SEM','RESULT']\ndata.sample(5)","a26a127e":"data.describe()","c7b99933":"data.info()","c2871af1":"rows,cols = data.shape\nprint(\"Rows:\",rows,\"Cols:\",cols)","a6562008":"no_pass_res = data.RESULT.value_counts()[1]\nno_fail_res = data.RESULT.value_counts()[0]\n\nprint(\"Number of Students who passed the semester:\",no_pass_res)\nprint(\"Number of Students who failed the semester:\",no_fail_res)","1152ef88":"print(\"Minimumm of MID_SEM:\",min(data.MID_SEM))\nprint(\"Maximum of MID_SEM:\",max(data.MID_SEM))","94cc0f67":"mid_sem = data['MID_SEM']\nmid_sem_mean = sum(mid_sem)\/len(mid_sem)\nprint(\"Mean of MID_SEM:\",mid_sem_mean)\n\n#using mean() function\nmid_mean = np.mean(mid_sem)\nprint(\"Mean of MID_SEM (using mean()):\",mid_mean)","01b48c58":"#From Scratch\nmid_sem_var = 0\nfor i in mid_sem:\n    mid_sem_var += (i - mid_mean)**2\nmid_sem_var = mid_sem_var \/ len(mid_sem)\nprint(\"Variance of MID_SEM:\",mid_sem_var)\n\n#using var() function\nmid_var = np.var(mid_sem)\nprint(\"Variance of MID_SEM (using var()):\",mid_var)","87880f46":"mid_sem_std = mid_sem_var**0.5\nprint(\"Std of MID_SEM:\",mid_sem_std)\n\n#Using std() function\nmid_std = np.std(mid_sem)\nprint(\"Std of MID_SEM (using std()):\",mid_std)","35a2f533":"mid_median = np.median(mid_sem)\nprint(\"Median of MID_SEM:\",mid_median)","b436f525":"mid_sem.quantile([0.25,0.75])","e3d21130":"sns.set_theme(style=\"darkgrid\")","f4b96f2c":"sns.boxplot(data.MID_SEM,color=\"#7E007B\")\nplt.show()","24fb1d20":"sns.boxplot(data.END_SEM,color=\"#7E007B\")\nplt.show()","43597e22":"sns.countplot(x=\"RESULT\",data=data,palette=\"prism\")\nplt.show()\n# 1 -> PASS\n# 0 -> FAIL","e4e42985":"#X and Y split\nX = data.iloc[:,:-1] #X = [MID_SEM,END_SEM]\nY = data.iloc[:,-1]  #Y = RESULT","23cde95c":"#Train and Test Data Split\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.10,random_state=1)\nprint(\"Number of entries in train_data:\",len(X_train))\nprint(\"Number of entries in test_data:\",len(X_test))","ea45fff7":"#Logistic Regression Model\nmodel=LogisticRegression()\nmodel=model.fit(X_train,Y_train)","eb6f5885":"#Predictions on Test Data\nY_pred = model.predict(X_test)","ed0d6904":"#Metrics: Confusion Matrix\nmatrix = confusion_matrix(Y_test,Y_pred)\nprint(matrix)","bc4cd8af":"#Metrics: Accuracy\nacc = accuracy_score(Y_test,Y_pred) * 100\nprint(\"Accuracy %:\",acc)","e2457497":"### **Min and Max of MID_SEM**","df964f4a":"### **Read Dataset**","2fc4f11b":"# **Stats, Graphs and Logistic Regression**\nIn this notebook, you will learn to calculate basic stats (like mean, median, variance, etc.), draw boxplot and count plot, and apply simple logistic regression on a Pandas data frame.\n\n### **Dataset**\nThe dataset contains the mid-semester, end-semester, and results of 100 students. \n\n### **Tasks**\n1. Calculate basic statistic measures(mean, median, variance, etc) using Python.\n2. Visualize the dataset using Matplotlib and Seaborn Libraries. \n3. ML model (Logistic Regression) could be used to predict the result of a student based on mid-semester and end-semester marks.","25e5a457":"### **Median(or Q2) of MID_SEM**\n\n**Median:** Median of a sorted array of size n is defined as the middle element when n is odd and average of middle two elements when n is even. It is also the 2nd Quartile (50%)","8770a1b0":"### **Variance of MID_SEM**\n\n**Variance**: It is the expectation of the squared deviation of a random variable from its mean","783ed0ca":"### **Mean of MID_SEM**\n\n**Mean:** It is the average of the feature MID_SEM.","5d63c7e0":"### **Simple Logistic Regression**","1bb17e2d":"### **Standard Deviation of MID_SEM**\n\n**Standard Deviation:** It is the square root of variance. A low standard deviation indicates that the values tend to be close to the mean of the set, while a high standard deviation indicates that the values are spread out over a wider range.","5d466c71":"### **Future Work**\n\n* Other ML techniques like Decision Tree, Random Forest, K-Nearest Neighbors, etc. applied to predict the result.\n* A larger dataset could be helpful while training a model.\n* I hope this helped you. Please post your feedback\/suggestions\/corrections in the comments section below if you have any. ","4396e7bd":"### **Observations**\n\n* It has 100 entries and 3 features. \n* There are no missing values.\n* Label encoding is not needed for the RESULT feature.\n* It has a mix of int64 and float64 features. ","fad79bc5":"### **Visualization**","e54abe29":"### **Dataset Overview**","764f1b1b":"### **Import Libraries**","3dbde68f":"### **1st Quartile and 3rd Quartile of MID_SEM**"}}