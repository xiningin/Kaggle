{"cell_type":{"113fa848":"code","b15ee93e":"code","b95412cb":"code","5caccec5":"code","ad4cde2b":"code","fba4b96c":"code","222f410c":"code","95fdc1b4":"code","448c8acb":"code","e9a3d469":"code","7d67a19a":"code","98f166aa":"code","82bcedcc":"code","f1412e7b":"code","1e9de39e":"code","fe4b149d":"code","279b77c3":"code","29e985fa":"markdown","ed01f9f9":"markdown","aeebb853":"markdown","e796590f":"markdown","e2a2e8da":"markdown","250bc37d":"markdown","c1fd58f1":"markdown","b7637d70":"markdown","f5184066":"markdown","f0155908":"markdown","a4461486":"markdown","87f4819d":"markdown"},"source":{"113fa848":"!pip install transformers\n\nimport torch\nfrom transformers import BertForQuestionAnswering\nfrom transformers import BertTokenizer\n\nbert_model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n#bert_model.save_weights(\"bert_model.h5\")\nimport pickle\npkl_filename = \"bert_model.pkl\"\nwith open(pkl_filename, 'wb') as file:\n    pickle.dump(bert_model, file)\n\nbert_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n#tokenizer.save_weights(\"tokenizer.h5\")\nimport pickle\npkl_filename = \"bert_tokenizer.pkl\"\nwith open(pkl_filename, 'wb') as file:\n    pickle.dump(bert_tokenizer, file)\n","b15ee93e":"\nimport os\n#%%capture\n!curl -O https:\/\/download.java.net\/java\/GA\/jdk11\/9\/GPL\/openjdk-11.0.2_linux-x64_bin.tar.gz\n!mv openjdk-11.0.2_linux-x64_bin.tar.gz \/usr\/lib\/jvm\/; cd \/usr\/lib\/jvm\/; tar -zxvf openjdk-11.0.2_linux-x64_bin.tar.gz\n!update-alternatives --install \/usr\/bin\/java java \/usr\/lib\/jvm\/jdk-11.0.2\/bin\/java 1\n!update-alternatives --set java \/usr\/lib\/jvm\/jdk-11.0.2\/bin\/java\nos.environ[\"JAVA_HOME\"] = \"\/usr\/lib\/jvm\/jdk-11.0.2\"","b95412cb":"#%%capture\n!pip install pyserini==0.8.1.0\nfrom pyserini.search import pysearch","5caccec5":"#%%capture\n!wget -O lucene.tar.gz https:\/\/www.dropbox.com\/s\/j55t617yhvmegy8\/lucene-index-covid-2020-04-10.tar.gz?dl=0\n!tar xvfz lucene.tar.gz\nminDate = '2020\/04\/10'  \nluceneDir = '\/kaggle\/working\/lucene-index-covid-2020-04-10\/'","ad4cde2b":"from IPython.core.display import display, HTML\nimport json\ndef show_query(query):\n    \"\"\"HTML print format for the searched query\"\"\"\n    return HTML('<br\/><div style=\"font-family: Times New Roman; font-size: 20px;'\n                'padding-bottom:12px\"><b>Query<\/b>: '+query+'<\/div>')\n\ndef show_document(idx, doc):\n    \"\"\"HTML print format for document fields\"\"\"\n    have_body_text = 'body_text' in json.loads(doc.raw)\n    body_text = ' Full text available.' if have_body_text else ''\n    return HTML('<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:10px\">' + \n               f'<b>Document {idx}:<\/b> {doc.docid} ({doc.score:1.2f}) -- ' +\n               f'{doc.lucene_document.get(\"authors\")} et al. ' +\n              f'{doc.lucene_document.get(\"journal\")}. ' +\n              f'{doc.lucene_document.get(\"publish_time\")}. ' +\n               f'{doc.lucene_document.get(\"title\")}. ' +\n               f'<a href=\"https:\/\/doi.org\/{doc.lucene_document.get(\"doi\")}\">{doc.lucene_document.get(\"doi\")}<\/a>.'\n               + f'{body_text}<\/div>')\n\ndef show_query_results(query, searcher, top_k=10):\n    \"\"\"HTML print format for the searched query\"\"\"\n    output_query = searcher.search(query)\n#    print(\"output_query\",output_query)\n    display(show_query(query))\n    for i, k in enumerate(output_query[:top_k]):\n        display(show_document(i+1, k))\n    return output_query[:top_k]   ","fba4b96c":"import json\ndef query_id(query_result):\n    #print(len(query_result))\n    #print(\"query_result\",query_result)\n    files_list=[]\n    for i in range(len(query_result)):\n      doc_json = json.loads(query_result[i].raw)\n      #print(\"doc_json\",doc_json)\n      paper_id = 'paper_id' in doc_json    \n      if paper_id :\n        #print(doc_json['paper_id'])\n        files_list.append(doc_json)\n        #print(doc_json)\n      #else:\n        #print(doc_json['sha'])\n    #print(\"files_list\",files_list)\n    return files_list","222f410c":"import glob\nimport json\nimport pandas as pd\nfrom tqdm import tqdm\nimport re\n#all_json=files_list\n#print(\"all_json type\", type(all_json))\n#print(\"length of json:\",len(all_json))\n\nclass FileReader:\n    def __init__(self, file):        \n #         print(\"file is=\",file)\n          content = file\n #         print(\"content is\", type(content))\n          self.paper_id = content['paper_id']\n          self.abstract = []\n          self.body_text = []\n          self.abstract_section=[]\n          self.body_section=[]\n          # Abstract\n          try:\n              for entry in content['abstract']:\n                  self.abstract.append(entry['text'])\n                  self.abstract_section.append(entry['section'])\n          except KeyError:pass    \n          # Body text\n          for entry in content['body_text']:\n              self.body_text.append(entry['text'])\n              self.body_section.append(entry['section'])\n          self.abstract = '\\n'.join(self.abstract)\n          self.body_text = '\\n'.join(self.body_text)\n          self.abstract_section = '\\n'.join(self.abstract_section)\n          self.body_section = '\\n'.join(self.body_section)\n            \n    def __repr__(self):\n        return f'{self.paper_id}: {self.abstract[:200]}++++++ {self.body_text[:200]}+++++'\n        \n#first_row = FileReader(all_json[0])\n#print(\"first_row===\",first_row)\n\ndef get_breaks(content, length):\n    data = \"\"\n    words = content.split(' ')\n    total_chars = 0\n    # add break every length characters\n    for i in range(len(words)):\n        total_chars += len(words[i])\n        if total_chars > length:\n            data = data + \"<br>\" + words[i]\n            total_chars = 0\n        else:\n            data = data + \" \" + words[i]\n    return data\n\ndef generate_clean_df(files_list):\n    all_json=files_list\n    dict_ = {'paper_id': [], 'abstract': [], 'body_text': [],'body_section':[],'abstract_section':[], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': [],'source_x':[],'publish_time':[]}\n\n    for idx, entry in enumerate(all_json):\n        content = FileReader(entry) \n        dict_['paper_id'].append(content.paper_id)\n        dict_['abstract'].append(content.abstract)\n        dict_['body_text'].append(content.body_text)\n        dict_['body_section'].append(content.body_section)\n        dict_['abstract_section'].append(content.abstract_section)\n    df_covid = pd.DataFrame(dict_, columns=['paper_id','abstract','abstract_section','body_section','body_text'])\n    #print(df_covid.head())\n    text_dict = df_covid.to_dict()\n    len_text = len(text_dict[\"paper_id\"])\n    paper_id_list  = []\n    body_text_list = []\n    body_section_list =[]\n    section_list =[]\n    for i in tqdm(range(0,len_text)):\n      paper_id = df_covid['paper_id'][i]\n      body_text = df_covid['body_text'][i].split(\"\\n\")\n      body_section = df_covid['body_section'][i].split('\\n')\n      for i in tqdm(range(0,len(body_text))):\n        paper_id_list.append(paper_id)\n        body_text_list.append(body_text[i])\n        body_section_list.append(body_section[i])\n        section_list.append(\"BODY\")\n\n    df_paragraph = pd.DataFrame({\"paper_id\":paper_id_list,\"section\":section_list,\"paragraph\":body_text_list,\"subsection\":body_section_list})\n    df_paragraph.to_csv(\"paragraph.csv\")\n    return df_paragraph\n","95fdc1b4":"#files_list=query_id(query_result)\n#cl_df=generate_clean_df(files_list)","448c8acb":"!wget https:\/\/github.com\/facebookresearch\/fastText\/archive\/v0.9.1.zip\n!unzip v0.9.1.zip\n!cd fastText-0.9.1\n","e9a3d469":"! ls -l\n! pip install \/kaggle\/working\/fastText-0.9.1\/.","7d67a19a":"! git clone https:\/\/github.com\/epfml\/sent2vec.git\n#! cd \/content\/sent2vec\/\n! pip install \/kaggle\/working\/sent2vec\/.","98f166aa":"pkl_filename = \"\/kaggle\/working\/bert_model.pkl\"\nwith open(pkl_filename, 'rb') as file:\n    bert_model = pickle.load(file)\n    \npkl_filename = \"\/kaggle\/working\/bert_tokenizer.pkl\"\nwith open(pkl_filename, 'rb') as file:\n    tokenizer = pickle.load(file)\n","82bcedcc":"import numpy as np\ndef bertsquadpred(bert_model, text, query):\n    input_ids = tokenizer.encode(query, text)\n    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n    sep_index = input_ids.index(tokenizer.sep_token_id)\n    num_seg_a = sep_index + 1\n    num_seg_b = len(input_ids) - num_seg_a\n    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n    assert len(segment_ids) == len(input_ids)\n    n_ids = len(segment_ids)\n    #print(\"n_ids\",n_ids)\n    #print(n_ids)\n    if n_ids < 512:\n        start_scores, end_scores = bert_model(torch.tensor([input_ids]), \n                                 token_type_ids=torch.tensor([segment_ids]))\n    else:        \n        start_scores, end_scores = bert_model(torch.tensor([input_ids[:512]]), \n                                 token_type_ids=torch.tensor([segment_ids[:512]]))\n    #print(\"start_scores\",start_scores)\n    #print(\"end_scores\",end_scores)\n    start_scores = start_scores[:,1:-1]\n    end_scores = end_scores[:,1:-1]\n    answer_start = torch.argmax(start_scores)\n    answer_end = torch.argmax(end_scores)\n    #print(\"answer_start, answer_end\",answer_start, answer_end)\n    answer = ''\n    \n    t_count = 0    \n    for i in range(answer_start, answer_end + 2):\n        if tokens[i] == '[SEP]' or tokens[i] == '[CLS]':\n            continue\n        if tokens[i][0:2] == '##':\n            answer += tokens[i][2:]\n        else:\n            if t_count == 0:\n                answer +=  tokens[i]\n            else:\n                answer += ' ' + tokens[i]\n        t_count+=1\n            \n    full_txt = ''\n    for t in tokens:\n        if t[0:2] == '##':\n            full_txt += t[2:]\n        else:\n            full_txt += ' ' + t\n            \n    abs_returned = full_txt.split('[SEP]')[1]\n            \n    #print(abs_returned)\n    ans={}\n    ans['answer'] = answer\n    #print(answer)\n    if answer.startswith('[CLS]') or answer_end.item() < sep_index or answer.endswith('[SEP]'):\n        ans['confidence'] = -1.0\n    else:\n        confidence = torch.max(start_scores) + torch.max(end_scores)\n        confidence = np.log(confidence.item())\n        ans['confidence'] = confidence\/(1.0+confidence)\n    ans['start'] = answer_start\n    ans['end'] = answer_end\n    ans['paragraph_bert'] = abs_returned\n    return ans","f1412e7b":"import sent2vec\n#from nltk import word_tokenize\n#from nltk.corpus import stopwords\nfrom string import punctuation\nfrom scipy.spatial import distance\n\n#model_path = \"\/content\/BioSentVec_PubMed_MIMICIII-bigram_d700.bin\"\n#model_path = \"\/kaggle\/input\/biosentvec\/BioSentVec_CORD19-bigram_d700.bin\"\nmodel_path = \"\/kaggle\/input\/covid-sent2vec-ver2\/BioSentVec_CORD19-bigram_d700_v2.bin\"\n#model_path = \"\/content\/BioSentVec_CORD19-bigram_d700.bin\"\nmodel = sent2vec.Sent2vecModel()\ntry:\n    model.load_model(model_path)\nexcept Exception as e:\n    print(e)\nprint('model successfully loaded')","1e9de39e":"import warnings\nwarnings.filterwarnings(\"ignore\")\n#from IPython.display import Markdown, display\nfrom IPython.display import Markdown\ndef test_query(cl_df):\n    para_vector_dict = {}\n    subsection_vector_dict={}\n    para_list = []\n    #data=[]\n    f_data=[]\n    count = 0\n    n_return = 2\n    for id, group in cl_df.groupby(['paper_id']):\n        para = group[\"paragraph\"].values\n        paper_id1=group[\"paper_id\"].values\n        paper_id=group[\"paper_id\"].values\n        paper_id=paper_id.tolist()       \n        #print(\"\\n paper_id\",set(paper_id)) \n        #display(Markdown(\"\"\"#Paper_id\"\"\"#Paper_id:\",set(paper_id)))\n        subsection = group[\"subsection\"].values\n        paras = [p for p in para if isinstance(p,str)]\n        subsections = [s for s in subsection if isinstance (s, str)]\n        \n        paras_count = len(paras)\n        #print(paras_count)    \n        if paras_count==0:\n            continue\n        #paras_text = \" \".join(paras)\n        paragraph_dict={}\n        paragraph_list=[]\n        k=0\n        for sub_para in paras:\n          paragraph_dict[k]=model.embed_sentence(sub_para)\n          paragraph_list.append(sub_para)\n          k+=1    \n        \n        keys = list(paragraph_dict.keys())\n        \n        vectors = np.array(list(paragraph_dict.values()))\n        #print(\"vectors are\",vectors)\n\n        nsamples, nx, ny = vectors.shape\n        para_vectors = vectors.reshape((nsamples,nx*ny))\n\n        from scipy.spatial import distance\n        from sklearn.metrics.pairwise import cosine_similarity\n\n        para_matrix_query = cosine_similarity(para_vectors, query_vector.reshape(1,-1))\n        para_indexes = np.argsort(para_matrix_query.reshape(1,-1)[0])[::-1][:5]        \n        short_listed_paras = [paragraph_list[i] for i in para_indexes]        \n        subsection_head=[subsections[i] for i in para_indexes]\n        \n        #display(Markdown(\"query:\",query))\n        \n        from itertools import chain \n        answer_squad_list=[]\n        answer_squad_dict={}\n        for sub_head,sub_para in zip(subsection_head,short_listed_paras):         \n            answer_vector = model.embed_sentence(sub_para)\n            cosine_sim = 1 - distance.cosine(query_vector, answer_vector)            \n            answer_squad = bertsquadpred(bert_model, sub_para, query)\n            answer_squad_list.append(answer_squad['answer'])\n        \n        display(Markdown(\"\"\"Paper_id:\"\"\"+str(set(paper_id))+\" \"))\n        #display(Markdown(\"\"\"Paper_id:\"\"\"+group['paper_id'].unique()+\" \"))\n        display(Markdown(\"\"\"Query   :\"\"\"+query))\n        display(Markdown(\"\"\"Answer  :\"\"\"+\" \".join(answer_squad_list)))\n        #print(\"-\"*90)\n        #data={'paper_id':group['paper_id'].unique(),'answer':\" \".join(answer_squad_list)}\n        #data={'paper_id':(group['paper_id'].nunique()),'subsection_consider':\",\".join(subsection_head),'answer':\" \".join(answer_squad_list)}\n        #data={'paper_id':set(paper_id),'subsection_consider':\",\".join(subsection_head),'answer':\" \".join(answer_squad_list)}\n        #f_data.append(data)dispgroup['paper_id'].unique()lay(Markdown(\"\"\"Paper_id:\"\"\"+str(set(paper_id))+\" \"))\n    #df = pd.DataFrame(f_data) \n    #print(df)\n    #display(Markdown(\"---Answer---\"))\n    #display(df[['paper_id', 'answer']])\n    #df.to_csv(\"\/kaggle\/working\/t.csv\", index=False)\n    #print(df)                                                                                                              \n                                                                                                                  \n        \n    ","fe4b149d":"query_list=[\n\"What has been published about information sharing and inter-sectoral collaboration\",\n\"What has been published about data standards and nomenclature\",\n\"What has been published about governmental public health\",\n\"Methods for coordinating data-gathering with standardized nomenclature\",\n\"Sharing response information among planners, providers, and others\",\n\"Understanding and mitigating barriers to information-sharing\",\n\"How to recruit, support, and coordinate local (non-Federal) expertise and capacity relevant to public health emergency response (public, private, commercial and non-profit, including academic)\",\n\"Integration of federal\/state\/local public health surveillance systems.\",\n\"Value of investments in baseline public health response infrastructure preparedness\",\n\"Modes of communicating with target high-risk populations (elderly, health care workers)\",\n\"Risk communication and guidelines that are easy to understand and follow (include targeting at risk populations\u2019 families too)\",\n\"Communication that indicates potential risk of disease to all population groups.\",\n\"Misunderstanding around containment and mitigation.\",\n\"Action plan to mitigate gaps and problems of inequity in the Nation\u2019s public health capability, capacity, and funding to ensure all citizens in need are supported and can access information, surveillance, and treatment.\",\n\"Measures to reach marginalized and disadvantaged populations.\"\n\"Data systems and research priorities and agendas incorporate attention to the needs and circumstances of disadvantaged populations and underrepresented minorities.\",\n\"Mitigating threats to incarcerated people from COVID-19, assuring access to information, prevention, diagnosis, and treatment.\",\n\"Understanding coverage policies (barriers and opportunities) related to testing, treatment, and care\"]","279b77c3":"for query in query_list:\n    from pyserini.search import pysearch\n    searcher = pysearch.SimpleSearcher(luceneDir)\n    query_result = show_query_results(query, searcher, top_k=10)\n    files_list=query_id(query_result)\n    cl_df=generate_clean_df(files_list)\n    query_vector = model.embed_sentence(query)\n    test_query(cl_df)","29e985fa":"Importing Sent2vec pre-trained model on Cord 19","ed01f9f9":"Extracting relevant pargraphs from the selected documents which are relevant to the question and extracting appropriate answer from the paragraphs. ","aeebb853":"Loaded Bert For Question and Answer pre-trained model and fucntion to extract answer","e796590f":"Note: This work is highly inspired from few other kaggle kernels , github sources and other data science resources. Any traces of replications, which may appear , is purely co-incidental. Due respect & credit to all my fellow kagglers. Thanks !!\n\nAnd this notebook is WIP","e2a2e8da":"Extracting the Paper_id of the information of the top searched results","250bc37d":"![Lucene.png](attachment:Lucene.png)","c1fd58f1":"Downloading pyserini for searching in Lucene database\n","b7637d70":"Setting up Java to use Lucene database","f5184066":"# **About Us:**\nWe are a group of AI and NLP scientists with experience across NLP, image processing and computer vision. Covid-19 Kaggle challenge has provided us with a unique opportunity to help humanity fight the corona virus pandemic collectively by utilizing benefits of AI and NLP. We have focused on creating NLP solution to enable users to ask questions and get the most accurate results from the vast corpus of medical journals.\n","f0155908":"Downloading Pre-trained model of Bert for Question Answering ","a4461486":"Extracting the data from json files for appropriate Top 10 documents from Lucene database","87f4819d":"Downloading latest Lucene index on Covid database"}}