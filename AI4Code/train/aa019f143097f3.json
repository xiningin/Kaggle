{"cell_type":{"1ea0a257":"code","b676c248":"code","2c534da8":"code","82651972":"code","4c63c2d7":"code","dfa4c1fe":"code","d9b2a16b":"code","ee3642dd":"code","83bd0eef":"code","9a8ebed1":"code","ba0a6c79":"code","6d799248":"code","0e739236":"code","dbacd1fb":"code","08b4f418":"code","54cdd582":"code","97dcac9b":"code","18998193":"code","b0313fa9":"code","ab2e0683":"code","e401ab69":"code","40ed464a":"code","33b90e1c":"code","4aafee59":"code","4cada120":"code","40e7fdaf":"code","e06a7e3b":"code","86c4678b":"code","2c8b6a6e":"code","4666b731":"code","2608c76a":"code","df75def3":"code","cd21bc38":"code","9454a91e":"code","e978907a":"code","a7f5f1be":"code","3d9f941a":"code","5966bb45":"code","7b17d27f":"code","03cf91e9":"code","eb6fd572":"code","90cf49cd":"code","1db104fb":"code","65711928":"code","46288623":"code","5ffd93f2":"code","e597e34f":"code","1c087855":"code","4e84b4b5":"code","c8eceffb":"code","f75af2f1":"code","2de5ba4a":"code","c2aa0875":"code","8f7a1d40":"code","7b28beda":"code","a1f3e1c8":"code","f6da2665":"markdown","3b413690":"markdown","627758ef":"markdown","53a3c3cf":"markdown","a0f54bed":"markdown","235ced63":"markdown","c3628369":"markdown","69fcd74d":"markdown","77d28372":"markdown","0fcd368a":"markdown","f41e1002":"markdown","45c4f40a":"markdown","2a65ea41":"markdown","9dfa34c4":"markdown","9b953ea4":"markdown","3325b0fa":"markdown","e1d02aab":"markdown","3b463017":"markdown","60b1644c":"markdown","fc213139":"markdown","ab35b266":"markdown","5b7b3106":"markdown","6c61cd71":"markdown","335110d5":"markdown","a4459ee3":"markdown"},"source":{"1ea0a257":"# Import libraries and set desired options\nimport os\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom scipy.sparse import hstack\n# !pip install eli5\nimport eli5\nimport time\nfrom tqdm import tqdm_notebook\nfrom scipy.sparse import csr_matrix\nfrom itertools import combinations\nfrom sklearn.preprocessing import OneHotEncoder, PolynomialFeatures\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.feature_selection import SelectFromModel\nfrom mlxtend.feature_selection import SequentialFeatureSelector\nfrom sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV\nfrom sklearn.model_selection import validation_curve, learning_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display_html\nfrom collections import Counter","b676c248":"PATH_TO_DATA = '..\/input'\nSEED = 42","2c534da8":"PATH_TO_TRAIN = os.path.join(PATH_TO_DATA, 'train_sessions.csv')\nPATH_TO_TEST = os.path.join(PATH_TO_DATA, 'test_sessions.csv')\nPATH_TO_DICT = os.path.join(PATH_TO_DATA, 'site_dic.pkl')","82651972":"times = ['time%s' % i for i in range(1, 11)]\ndf = pd.read_csv(PATH_TO_TRAIN, index_col='session_id', parse_dates=times)\ndf = df.sort_values(by='time1').reset_index(drop=True)\ntest_df = pd.read_csv(PATH_TO_TEST, index_col='session_id', parse_dates=times)","4c63c2d7":"yyyymmdd = 10000 * df.time1.dt.year + 100 * df.time1.dt.month + df.time1.dt.day\nfull_df = df.loc[yyyymmdd.isin(yyyymmdd[df.target == 1].unique())]","dfa4c1fe":"holdout_idx = int(len(full_df)*0.9)\nholdout_idx","d9b2a16b":"y_full = full_df['target'].astype('int')\ny_train = y_full.iloc[:holdout_idx]\ny_holdout = y_full.iloc[holdout_idx:]\n\nprint(y_train.shape, y_holdout.shape)","ee3642dd":"# read site -> id mapping provided by competition organizers \nwith open(PATH_TO_DICT, 'rb') as f:\n    site2id = pickle.load(f)\n# create an inverse id _> site mapping\nid2site = {v:k for (k, v) in site2id.items()}\n# we treat site with id 0 as \"unknown\"\nid2site[0] = 'unknown'\n\n# Map site integers to URLs\nsites = ['site%s' % i for i in range(1, 11)]\n# Sites used for creating test predictions\nX_full_sites = full_df[sites].fillna(0).astype('int').applymap(lambda x: id2site[x])\nX_test_sites = test_df[sites].fillna(0).astype('int').applymap(lambda x: id2site[x])\n# Subset of sites used for feature search\nX_train_sites = X_full_sites.iloc[:holdout_idx]\nX_holdout_sites = X_full_sites.iloc[holdout_idx:]\n\nprint(X_train_sites.shape, X_holdout_sites.shape, X_test_sites.shape)","83bd0eef":"def vectorize_sites(train_sites, test_sites, vectorizer_params):\n    # Join each row by space and apply an TF-IDF vectorizer to each row\n    site_vectorizer = TfidfVectorizer(**vectorizer_params)\n    \n    train_sessions = train_sites.apply(lambda row: ' '.join(row), axis=1).tolist()\n    train_sparse = site_vectorizer.fit_transform(train_sessions)\n    \n    test_sessions = test_sites.apply(lambda row: ' '.join(row), axis=1).tolist()\n    test_sparse = site_vectorizer.transform(test_sessions)\n    \n    return train_sparse, test_sparse, site_vectorizer","9a8ebed1":"%%time\nsite_vectorizer_params = {\n    'ngram_range': (1, 1), \n    'max_features': 50000, \n    'tokenizer': lambda s: s.split()\n}\nX_train_sites_sparse, X_holdout_sites_sparse, site_vectorizer = vectorize_sites(X_train_sites, X_holdout_sites, site_vectorizer_params)\nX_full_sites_sparse, X_test_sites_sparse, _ = vectorize_sites(X_full_sites, X_test_sites, site_vectorizer_params)\n\nprint(X_train_sites_sparse.shape, X_holdout_sites_sparse.shape, X_test_sites_sparse.shape)","ba0a6c79":"X_full_parts = X_full_sites.applymap(lambda x: ' '.join(x.split('.')))\nX_train_parts = X_train_sites.applymap(lambda x: ' '.join(x.split('.')))\nX_holdout_parts = X_holdout_sites.applymap(lambda x: ' '.join(x.split('.')))\nX_test_parts = X_test_sites.applymap(lambda x: ' '.join(x.split('.')))","6d799248":"%%time\npart_vectorizer_params = {\n    'ngram_range': (1, 1), \n    'max_features': 50000, \n    'tokenizer': lambda s: s.split()\n}\nX_train_parts_sparse, X_holdout_parts_sparse, part_vectorizer = vectorize_sites(X_train_parts, X_holdout_parts, part_vectorizer_params)\nX_full_parts_sparse, X_test_parts_sparse, _ = vectorize_sites(X_full_parts, X_test_parts, part_vectorizer_params)\n\nprint(X_train_parts_sparse.shape, X_holdout_parts_sparse.shape, X_test_parts_sparse.shape)","0e739236":"def stack_features(*matrices):\n    # Stack matrices into a single sparse matrix\n    return csr_matrix(hstack(matrices))","dbacd1fb":"X_full_sparse = stack_features(X_full_sites_sparse, X_full_parts_sparse)\nX_train_sparse = stack_features(X_train_sites_sparse, X_train_parts_sparse)\nX_holdout_sparse = stack_features(X_holdout_sites_sparse, X_holdout_parts_sparse)\nX_test_sparse = stack_features(X_test_sites_sparse, X_test_parts_sparse)","08b4f418":"X_full_times = full_df[times]\nX_train_times = X_full_times.iloc[:holdout_idx]\nX_holdout_times = X_full_times.iloc[holdout_idx:]\nX_test_times = test_df[times]\n\nprint(X_train_times.shape, X_holdout_times.shape, X_test_times.shape)","54cdd582":"def create_features(sites, times):\n    # Perform your feature engineering here\n    # For your features, check the following assumptions:\n    #   @ Linear relationship\n    #   @ Few outliers\n    #   @ Multivariate normality\n    #   @ No or little multicollinearity\n    #   @ No auto-correlation\n    #   @ Homoscedasticity\n    features = pd.DataFrame(index=sites.index)\n    \n    # Raw features here\n    # For cyclic features try to make them harmonic by mapping onto a circle and using two coordinates\n    features['year'] = year = times['time1'].apply(lambda t: t.year).astype(int)\n    features['yyyymm'] = times['time1'].apply(lambda t: 100 * t.year + t.month).astype('float64')\n    features['yyyymmdd'] = times['time1'].apply(lambda t: 10000 * t.year + 100 * t.month + t.day).astype('float64')\n    features['month'] = times['time1'].apply(lambda t: t.month).astype('float64')\n    features['day'] = times['time1'].apply(lambda t: t.day).astype('float64')\n    features['dayofyear'] = times['time1'].apply(lambda t: t.dayofyear).astype('float64')\n    features['dayofweek'] = dayofweek = times['time1'].apply(lambda t: t.dayofweek).astype('float64')\n    features['hour'] = hour = times['time1'].apply(lambda ts: ts.hour).astype('float64')\n    features['minute'] = times['time1'].apply(lambda ts: ts.minute).astype('float64')\n    features['seconds'] = ((times.max(axis=1) - times.min(axis=1)) \/ np.timedelta64(1, 's')).astype('float64')\n    \n    # Binary features\n    features['is_sunday'] = (dayofweek == 6).astype(int)\n    features['is_morning'] = ((hour >= 7) & (hour <= 11)).astype(int)\n    features['is_day'] = ((hour >= 12) & (hour <= 18)).astype(int)\n    features['is_evening'] = ((hour >= 19) & (hour <= 23)).astype(int)\n    features['is_night'] = ((hour >= 0) & (hour <= 6)).astype(int)\n    \n    # One-hot encoded features here\n    features = features.join(pd.get_dummies(features[['year']], columns=['year'], prefix='is_year'))\n    features['year'] = features['year'].astype('float64')\n    \n    return features","97dcac9b":"%%time\nX_full_feat = create_features(X_full_sites, X_full_times)\nX_train_feat = create_features(X_train_sites, X_train_times)\nX_holdout_feat = create_features(X_holdout_sites, X_holdout_times)\nX_test_feat = create_features(X_test_sites, X_test_times)\n\nprint(X_train_feat.shape, X_holdout_feat.shape, X_test_feat.shape)","18998193":"# Scale float columns\ncolumns_to_scale = X_full_feat.select_dtypes(include='float64').columns\n\nscaler = MinMaxScaler()\nX_train_feat[columns_to_scale] = scaler.fit_transform(X_train_feat[columns_to_scale])\nX_holdout_feat[columns_to_scale] = scaler.transform(X_holdout_feat[columns_to_scale])\nX_full_feat[columns_to_scale] = scaler.fit_transform(X_full_feat[columns_to_scale])\nX_test_feat[columns_to_scale] = scaler.transform(X_test_feat[columns_to_scale])","b0313fa9":"X_train_feat.describe().T['max']","ab2e0683":"if set(X_train_feat.columns) != set(X_holdout_feat.columns):\n    X_train_feat, X_holdout_feat = X_train_feat.align(X_holdout_feat, join='outer', axis=1, fill_value=0)\nif set(X_full_feat.columns) != set(X_test_feat.columns):\n    X_full_feat, X_test_feat = X_full_feat.align(X_test_feat, join='outer', axis=1, fill_value=0)","e401ab69":"time_split = TimeSeriesSplit(n_splits=5)\n\nprint([(el[0].shape, el[1].shape) for el in time_split.split(y_train)])","40ed464a":"print(X_train_feat.corrwith(y_train).abs().sort_values(ascending=False)[:20])","33b90e1c":"selector = SelectKBest(f_classif, k=5)\nselector.fit(X_train_feat, y_train)\n\nprint(X_train_feat.columns[selector.get_support(indices=True)])","4aafee59":"logit = LogisticRegression(C=1, random_state=SEED, solver='liblinear')","4cada120":"selector = SelectFromModel(estimator=logit)\nselector.fit(X_train_feat, y_train)\n\nprint(X_train_feat.columns[selector.get_support(indices=True)])","40e7fdaf":"%%time\nselectors = {}\nmodes = ['SFS', 'SBS', 'SFFS', 'SBFS']\nfor i in tqdm_notebook(range(len(modes))):\n    mode = modes[i]\n    forward = i in (0, 2)\n    floating = i in (2, 3)\n    selector = SequentialFeatureSelector(logit, k_features='best', forward=forward, floating=floating, \n                                         verbose=False, scoring='roc_auc', cv=time_split, n_jobs=-1)\n    selector.fit(X_train_feat, y_train, custom_feature_names=X_train_feat.columns)\n    selectors[mode] = selector","e06a7e3b":"feature_scores = {}\nfor selector in selectors.values():\n    for subset in selector.subsets_.values():\n        feature_names, cv_scores = subset['feature_names'], subset['cv_scores']\n        feature_scores[tuple(set(feature_names))] = cv_scores","86c4678b":"Counter([feature for features in feature_scores.keys() for feature in features])","2c8b6a6e":"def assess_feature_scores(feature_scores):\n    # Format scores\n    df = pd.DataFrame(feature_scores).T\n    split_cols = ['split_%s'%str(c+1) for c in df.columns]\n    df.columns = split_cols\n    df['mean'] = df[split_cols].mean(axis=1)\n    df['std'] = df[split_cols].std(axis=1)\n    df = df.sort_values(by='mean', ascending=False)\n    return df","4666b731":"assess_feature_scores(feature_scores)","2608c76a":"selected_features = ['is_evening', 'is_morning', 'seconds', 'hour']","df75def3":"def cross_validate(model, X_train, y_train, cv=time_split, scoring='roc_auc'):\n    # Cross validate on training set\n    return cross_val_score(model, X_train, y_train, cv=cv, scoring=scoring, n_jobs=-1)\n\ndef neg_cv_std(estimator, X, y):\n    return -np.std(cross_validate(estimator, X, y))","cd21bc38":"selector = SequentialFeatureSelector(logit, k_features='best', forward=False, floating=True, \n                                     verbose=1, scoring=neg_cv_std, cv=None, n_jobs=-1)\nselector.fit(X_train_feat[selected_features], y_train, custom_feature_names=selected_features)","9454a91e":"selector.k_feature_names_, selector.k_score_","e978907a":"selected_features = ['is_evening', 'is_morning', 'seconds']","a7f5f1be":"X_train = stack_features(X_train_sparse, X_train_feat[selected_features].values)\nX_holdout = stack_features(X_holdout_sparse, X_holdout_feat[selected_features].values)\n\nprint(X_train.shape, X_holdout.shape)","3d9f941a":"def plot_with_err(x, data, **kwargs):\n    mu, std = data.mean(1), data.std(1)\n    lines = plt.plot(x, mu, '-', **kwargs)\n    plt.fill_between(x, mu - std, mu + std, edgecolor='none',\n                     facecolor=lines[0].get_color(), alpha=0.2)","5966bb45":"def create_validation_curve(X, y):\n    Cs = np.logspace(-2, 1, 20)\n    time_cv = TimeSeriesSplit(n_splits=5)\n    logit = LogisticRegression(random_state=SEED, solver='liblinear')\n    train_scores, valid_scores = validation_curve(logit, X, y, 'C', Cs, cv=time_cv, scoring='roc_auc', n_jobs=-1, verbose=1)\n    \n    plot_with_err(Cs, train_scores, label='training scores')\n    plot_with_err(Cs, valid_scores, label='validation scores')\n    plt.xlabel('C'); plt.ylabel('ROC AUC')\n    plt.legend()\n    plt.show()","7b17d27f":"create_validation_curve(X_train, y_train)","03cf91e9":"def create_learning_curve(X, y, C=1):\n    train_sizes = np.linspace(0.3, 1, 10)\n    time_cv = TimeSeriesSplit(n_splits=5)\n    logit = LogisticRegression(C=C, random_state=SEED, solver='liblinear')\n    train_sizes, train_scores, valid_scores = learning_curve(\n        logit, X, y, train_sizes=train_sizes, cv=time_cv, scoring='roc_auc', n_jobs=-1, verbose=1)\n    \n    plot_with_err(train_sizes, train_scores, label='training scores')\n    plot_with_err(train_sizes, valid_scores, label='validation scores')\n    plt.xlabel('Training Set Size'); plt.ylabel('AUC')\n    plt.legend()\n    plt.show()","eb6fd572":"create_learning_curve(X_train, y_train, C=1)","90cf49cd":"logit.fit(X_train, y_train)","1db104fb":"def explain_model(model, site_feature_names, new_feature_names=None, top_n_features_to_show=10):\n    if new_feature_names is not None:\n        all_feature_names = site_feature_names + new_feature_names \n    else: \n        all_feature_names = site_feature_names\n    display_html(eli5.show_weights(estimator=model, feature_names=all_feature_names, top=top_n_features_to_show))\n    if new_feature_names:\n        print('New feature weights:')\n        print(pd.Series(model.coef_.flatten()[-len(new_feature_names):], index=new_feature_names).sort_values(ascending=False))","65711928":"site_feature_names = site_vectorizer.get_feature_names() + part_vectorizer.get_feature_names()\n\nexplain_model(logit, site_feature_names, new_feature_names=selected_features)","46288623":"def compute_score(model, X, y):\n    # Validate on validation set\n    y_pred = model.predict_proba(X)[:, 1]\n    return roc_auc_score(y, y_pred)","5ffd93f2":"compute_score(logit, X_holdout, y_holdout)","e597e34f":"# Prepare data for testing\nX_full = stack_features(X_full_sparse, X_full_feat[selected_features].values)\nX_test = stack_features(X_test_sparse, X_test_feat[selected_features].values)\n\nprint(X_full.shape, X_test.shape)","1c087855":"param_grid = {\n    'C': np.logspace(-2, 1, 20)\n}","4e84b4b5":"logit_gs = GridSearchCV(estimator=logit, param_grid=param_grid, scoring='roc_auc', \n                        iid=False, n_jobs=-1, cv=time_split, verbose=1, return_train_score=False)","c8eceffb":"logit_gs.fit(X_full, y_full);","f75af2f1":"logit_gs.best_score_, logit_gs.best_params_","2de5ba4a":"print(cross_validate(logit, X_full, y_full))\nprint(cross_validate(logit_gs.best_estimator_, X_full, y_full))","c2aa0875":"# Compare performance against regularization at different splits\nlogit_gs_scores = pd.DataFrame()\nfor i in range(5):\n    column = 'split%d_test_score'%i\n    logit_gs_scores[column] = logit_gs.cv_results_[column]\nax = logit_gs_scores.plot(figsize=(14, 6))\nax.set_xticks(range(len(param_grid['C'])))\nax.set_xticklabels(['%.2f'%c for c in param_grid['C']])\nax.legend(loc='center left', bbox_to_anchor=(1.01, 0.5));","8f7a1d40":"logit_gs.best_estimator_.fit(X_full, y_full)","7b28beda":"# A helper function for writing predictions to a file\ndef write_to_submission_file(predicted_labels, out_file,\n                             target='target', index_label=\"session_id\"):\n    predicted_df = pd.DataFrame(predicted_labels,\n                                index = np.arange(1, predicted_labels.shape[0] + 1),\n                                columns=[target])\n    predicted_df.to_csv(out_file, index_label=index_label)\n        \ndef predict_and_submit(model, X_test, file_name='submission.csv'):\n    test_pred = model.predict_proba(X_test)[:, 1]\n    write_to_submission_file(test_pred, file_name)","a1f3e1c8":"predict_and_submit(logit_gs.best_estimator_, X_test)","f6da2665":"## Evaluation","3b413690":"To avoid overfitting, we want not only to select features based on the highest CV score but also on *the stability of the model*. We see that the model's performance inversely correlates with the number of features, which means that the most of them are just noise.","627758ef":"Do not run it very often to not overfit though.","53a3c3cf":"## Feature selection","a0f54bed":"Split the dataset into training and holdout sets. We will perform CV on the training set, while at the end we will test our results on the holdout set. This way, we may see signs of overfitting locally and establish a good correlation between local score and LB. ","235ced63":"We will also vectorize site URL parts such as domains.","c3628369":"Finally, if you are confident enough in your features, run the model on the holdout set to get an estimation of the public LB score.","69fcd74d":"Correct cross-validation scheme for temporal data. \n\nWe will perform CV on the training data first, then perform validation on the holdout data to get better estimation of public LB score.","77d28372":"## Hyperparameter tuning","0fcd368a":" You may select the best features based on univariate statistical tests.","f41e1002":"The training set may have a different number of columns than the holdout set - align them.","45c4f40a":"You may also try to select features recursively based on the feature weight coefficients (e.g., linear models) or feature importance (tree-based algorithms).","2a65ea41":"After selecting the features of interest, let's remove them iteratively to get the most stable model.","9dfa34c4":"Correlation between features and target gives first hints.","9b953ea4":"Let's try something different: use SFSs to eliminate (or add) features based on a user-defined classifier\/regression performance metric.\n\nhttp:\/\/rasbt.github.io\/mlxtend\/user_guide\/feature_selection\/SequentialFeatureSelector\/#sequential-feature-selector","3325b0fa":"Try to improve the score a bit.","e1d02aab":"Prepare URLs for vectorization.","3b463017":"Now let's add some additional features. The more you have here the better, we will filter some of them out later. You can beat **both** baselines just by experimenting with this function.","60b1644c":"Select active days only to speed up the search process.","fc213139":"Now let's check the weights of the new features to be somewhat intuitively correct.\n\nIf you added some new features and your weights look totally different than for the baseline, this is usually a bad sign.","ab35b266":"## Submission","5b7b3106":"Split temporal data.","6c61cd71":"## Feature engineering","335110d5":"It's a good practice to visualize how the model behaves depending on the regularization parameter *C* and the training size.\n\nGood features lead to stable, narrow curves.","a4459ee3":"Stack both sparse matrices (URLs and URL parts) into one big sparse matrix."}}