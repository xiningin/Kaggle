{"cell_type":{"cd97a2b6":"code","87908b3b":"code","157ba500":"code","75034a38":"code","6c9dd94c":"code","6583e720":"code","e57a72c1":"code","ad83cd38":"code","00374dfa":"code","02aeaf93":"code","8058b1bb":"code","0ff1d00b":"code","5624e15a":"code","1bb3ceea":"code","512b298a":"code","0953001f":"code","d033df00":"code","5d4b68cf":"code","1e0a2818":"code","f07696d3":"markdown","a580b001":"markdown","7fa243b2":"markdown","0a74eed8":"markdown","aee0a581":"markdown"},"source":{"cd97a2b6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","87908b3b":"allTweets=pd.read_csv('\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv',header=None,encoding='cp1252')","157ba500":"allTweets=allTweets.reset_index()\nallTweets.head()","75034a38":"tweetsTXT=allTweets[5].tolist()\ntweetsIndex=allTweets['index'].tolist()","6c9dd94c":"import re\nimport nltk\n\nfrom string import punctuation\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\n\nstopword = stopwords.words('english')\nsnowball_stemmer = SnowballStemmer('english')\nwordnet_lemmatizer = WordNetLemmatizer()\n\ndef cleanTweet(tweet):\n    tweetNoAT= re.sub(r'\\@\\w+','', tweet)\n    tweetNoURL=re.sub(r'http\\S+','',tweetNoAT)\n    lower=tweetNoURL.lower()\n    lower=''.join(c for c in lower if c not in punctuation)\n    word_tokens = nltk.word_tokenize(lower)\n    word_tokens = [word for word in word_tokens if word not in stopword]\n    lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in word_tokens]\n    stemmed_word = [snowball_stemmer.stem(word) for word in lemmatized_word]\n    return stemmed_word","6583e720":"cleanTweet('@bestCATFRD check this out, the cat video, https:\/asdasd!')","e57a72c1":"masterRAW=list(map(lambda x:cleanTweet(x), tweetsTXT))","ad83cd38":"from collections import Counter\n## Create index2word and word2index\nmasterWord=[]\nfor indivList in masterRAW:\n    masterWord+=indivList\ncounterDict=Counter(masterWord)\ncommonWords=counterDict.most_common(2000)","00374dfa":"indexList=list(range(2000))\nword2idx={}\nidx2word={}\nfor key,val in zip(indexList,commonWords):\n    word2idx[val[0]]=key\n    idx2word[key]=val[0]","02aeaf93":"totLen=len(masterRAW)\ndef makeBatch(batchNum,batchSize):\n    currentX=[]\n    currentY=[]\n    currentIdx=[]\n    slips=masterRAW[batchNum*batchSize:(batchNum+1)*batchSize]\n    counter=0\n    for sent in slips:\n        if len(sent)<2:\n            continue\n        else: \n            for wordIdx in range(len(sent)-1):\n                if sent[wordIdx] in word2idx and sent[wordIdx+1] in word2idx:\n                    currentX.append(word2idx[sent[wordIdx]])\n                    currentIdx.append(batchNum*batchSize+counter)\n                    currentY.append([word2idx[sent[wordIdx+1]]])\n                    currentX.append(word2idx[sent[wordIdx+1]])\n                    currentIdx.append(batchNum*batchSize+counter)\n                    currentY.append([word2idx[sent[wordIdx]]])\n        counter+=1\n    return np.array(currentX),np.array(currentIdx),np.array(currentY)\n                    ","8058b1bb":"import math\nimport tensorflow as tf\nbatch_size = 160\nembedding_size = 32\ndoc_embedding_size=5\n\ntrain_inputs=tf.placeholder(tf.int32, shape=[None])\ntrain_docs=tf.placeholder(tf.int32,shape=[None])\ntrain_labels=tf.placeholder(tf.int32, shape=[None,1])\n\nembeddings = tf.Variable(tf.random_uniform((2000, embedding_size), -1, 1))\nembeddingDoc=tf.Variable(tf.random_uniform((1600000,doc_embedding_size),-1,1))\n\nembedWord = tf.nn.embedding_lookup(embeddings, train_inputs)\nembedDoc=tf.nn.embedding_lookup(embeddingDoc,train_docs)\n\nembed=tf.concat([embedWord,embedDoc],axis=1,name='concat')\n\n\nnce_weights = tf.Variable(tf.truncated_normal([2000, embedding_size+doc_embedding_size],\n                                              stddev=1.0 \/ math.sqrt(embedding_size+doc_embedding_size)))\nnce_biases = tf.Variable(tf.zeros([2000]))\n\nnce_loss = tf.reduce_mean(\n    tf.nn.nce_loss(weights=nce_weights,\n                   biases=nce_biases,\n                   labels=train_labels,\n                   inputs=embed,\n                   num_sampled=200,\n                   num_classes=2000))\n\noptimizer = tf.train.GradientDescentOptimizer(1.0).minimize(nce_loss)\n","0ff1d00b":"init=tf.global_variables_initializer()\nsess=tf.Session()\nsess.run(init)","5624e15a":"for epoch in range(10):\n    idx=0\n    tempLossTOT=0.0\n    for batchIndex in range(int(len(masterRAW)\/128)-1):\n        trainX,trainIndex,trainY=makeBatch(batchIndex,128)\n        loss,_ = sess.run([nce_loss,optimizer],feed_dict={train_inputs:trainX,train_docs:trainIndex,train_labels:trainY})\n        tempLossTOT+=loss\n        if batchIndex%1000==0:\n            print('Current Loss: '+str(tempLossTOT\/(batchIndex+1)*1.0 ))\n","1bb3ceea":"embeddingMat=sess.run(embeddings)","512b298a":"from sklearn.manifold import TSNE\nX_embedded = TSNE(n_components=2).fit_transform(embeddingMat)\nX_embedded.shape","0953001f":"\ncol1=[x[0] for x in X_embedded]\ncol2=[x[1] for x in X_embedded]\nkeys=word2idx.keys()","d033df00":"tsnedEmbedding=pd.DataFrame()\ntsnedEmbedding['word']=keys\ntsnedEmbedding['dim1']=col1\ntsnedEmbedding['dim2']=col2\ntsnedEmbedding.to_csv('2DEmbedding.csv')\n","5d4b68cf":"import plotly.express as px\nimport matplotlib.pyplot as plt\n\nx = tsnedEmbedding['dim1'].tolist()\ny = tsnedEmbedding['dim2'].tolist()\nn = tsnedEmbedding['word'].tolist()\n\nfig = px.scatter(tsnedEmbedding, x=\"dim1\", y=\"dim2\", text=\"word\", size_max=60)\nfig.update_traces(textposition='top center')\nfig.update_layout(\n    height=800,\n    title_text='Embedding Two-D Plot'\n)\nfig.show()\n","1e0a2818":"totLen\n","f07696d3":"## Downloading Embedding Matrix","a580b001":"## Make Word2Idx and Idx2Word ","7fa243b2":"## Generate Common Words","0a74eed8":"## **Preprocessing**","aee0a581":"## Text cleansing illustration"}}