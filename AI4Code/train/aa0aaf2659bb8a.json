{"cell_type":{"41604ca8":"code","1a069df2":"code","6a695b37":"code","3043b4ab":"code","3f21e610":"code","afb5eb05":"code","9477138b":"code","ada3756c":"code","489d5424":"code","53f338a9":"code","49ff6d4e":"code","6b812045":"code","6a6597a5":"code","0096d586":"code","ee478b8d":"code","9d5d01c4":"code","76d2360b":"code","b6b71750":"code","38a1a3fe":"code","30e6e2bf":"code","9a1e9f81":"code","ae7530fc":"code","5a8552d6":"code","5dc27a31":"code","681f1666":"code","cb946ed0":"code","d4d7db8e":"code","9a40eb57":"code","355e4a05":"code","b0b4e8aa":"code","75b88aab":"code","3b4e0803":"code","8ae2c90b":"code","36287dce":"code","0fa3b365":"code","69251a38":"code","5efb8332":"code","c50f9b55":"code","a62e45d1":"code","1234ba68":"code","9eb7c6b4":"code","09cd6bab":"code","9e7854c6":"code","bb09d794":"code","3942304b":"code","ba1ee861":"code","6d3524bf":"code","0ff036fc":"code","41d6232a":"code","64613e81":"code","1a3fe57b":"code","9261055c":"code","96b2cc54":"code","4b1e26e7":"code","ffce66b0":"code","b3a02041":"code","875a86e3":"code","24746a94":"code","82da6af4":"code","1452383a":"code","75d6be7f":"code","e5da5cf8":"code","25447a0d":"code","b439582f":"code","dcb6abec":"code","6ccc3c20":"code","e41de910":"code","56f0da29":"code","66706fa8":"code","96aef379":"code","cbc34cf6":"code","f2b0f9a7":"code","a8e85667":"code","5b431529":"code","e1a91b2a":"code","eee601f3":"code","8d318e01":"code","22c46cba":"code","dd00edd9":"code","dce99315":"code","e27f0499":"code","9c9e9844":"code","9acc7cc7":"code","20df0ab2":"code","a44d50d3":"code","7680d7d2":"code","becc8c27":"code","d69d6244":"code","abe85db4":"code","6639d0af":"code","9a45d883":"code","db656725":"code","788294c7":"code","ed473165":"code","a2764780":"code","80088aab":"code","41f92e52":"code","7ecb3798":"code","fd14f909":"code","230b2233":"code","01fa6d67":"code","ef5288cc":"code","0e7ccb7a":"code","ae83f954":"code","df5a43f0":"code","c0287b88":"code","da327198":"code","19c1a44b":"code","1cd54065":"code","7f30af3a":"code","4ffb848e":"code","673acd34":"code","9b520f52":"code","1f74739e":"code","51f35a0b":"code","7057f367":"code","7a6223a2":"code","91d7f0bf":"code","907f061b":"code","a7c8ac1b":"code","8bfb682f":"code","0efc95b7":"code","43cf187c":"code","e889cf38":"code","27d73543":"code","10167fcc":"code","3857da82":"code","52d76d4f":"code","0ac10961":"code","eee4816a":"code","cc87295a":"code","1c47d6ce":"code","22fdeb8a":"code","582fb755":"code","9d180563":"code","cb967d81":"code","19691a7c":"code","e2227124":"code","3f941a34":"code","ea2a9766":"code","e5c2119a":"code","22fe5db3":"code","b17f74ac":"code","cfb64919":"code","cc8965b2":"code","b5c54b01":"code","8cc9c31d":"code","ac277854":"code","dd71b8cd":"code","9b652fe3":"code","beaea983":"code","a88139f5":"code","0c091ec3":"code","b1dcae80":"code","9dfd9207":"code","d2dee4cc":"code","a332d697":"code","1664dcfb":"code","10f649b5":"code","06e906ee":"code","8ca3640a":"code","a378cef1":"code","60da098f":"code","b559b37d":"code","8b48163d":"code","5e2bfb7a":"code","ace4c36f":"code","db9383dd":"code","5fc34f4e":"code","57d94527":"code","1982c4eb":"code","c0c1cdc1":"code","fab0415e":"markdown","6939a455":"markdown","234d1a2b":"markdown","90f3d36f":"markdown","0451fbe4":"markdown","3fb117dd":"markdown","b8f1b8fe":"markdown","cd3445da":"markdown","c0647a11":"markdown","337dc6e4":"markdown","f92647d8":"markdown","565abbed":"markdown","d7f52f4b":"markdown","e0ff1b45":"markdown","f4630e73":"markdown","5b274c0e":"markdown","2ce34357":"markdown","544054f5":"markdown","37eff1fa":"markdown","434aa29b":"markdown","d370ac4f":"markdown","f568c266":"markdown","0c3ab7d6":"markdown","fb2377c4":"markdown","f1478c4f":"markdown","a54c913b":"markdown","0aec3e83":"markdown","2561e855":"markdown","dff301bb":"markdown","8d0ccbc8":"markdown","ec06a2d2":"markdown","39c40d61":"markdown","3db8bbc0":"markdown","af61f669":"markdown","60954cdf":"markdown","3901a387":"markdown","157bf79e":"markdown","f16d7b38":"markdown","ba49d436":"markdown","a99b102f":"markdown","98bb0d0d":"markdown","386135f7":"markdown","be38ac84":"markdown","04747a66":"markdown","7f0e5396":"markdown","244507e9":"markdown","68ee607e":"markdown","504fe866":"markdown","beecdb8f":"markdown","653e2980":"markdown","51f663c3":"markdown","e4bd85e7":"markdown","fe84442f":"markdown","5903df8a":"markdown","c73fa580":"markdown","992f9ab0":"markdown","ebba8f7a":"markdown","c132106a":"markdown","696c13e9":"markdown","9a71d961":"markdown","6f1a71a3":"markdown","ea56e5e5":"markdown","4fad6525":"markdown","a9972108":"markdown","66f4b0e1":"markdown","ff7041b3":"markdown"},"source":{"41604ca8":"\"\"\"\nWe perform the EDA here based on the early assumptions\nEDA includes:\nCleaning the data\nSplitting the data\nModel the data\nCheck for better accuracy\n\"\"\"\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport featuretools as ft\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\n","1a069df2":"# For plotting graphs\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# To ignore the warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","6a695b37":"df = pd.read_csv(\"..\/input\/loan-prediction-problem-dataset\/train_u6lujuX_CVtuZ9i.csv\")\ndf.head(5)","3043b4ab":"train,test = train_test_split(df, test_size =0.2)","3f21e610":"train.head()\ntrain.shape","afb5eb05":"train.describe()","9477138b":"train.head(5)","ada3756c":"test.shape","489d5424":"test.head()","53f338a9":"train['Loan_Status'].value_counts()","49ff6d4e":"gender_count = train['Gender'].value_counts()","6b812045":"train['Married'].value_counts()","6a6597a5":"df['Dependents'].value_counts()","0096d586":"edu_count = train['Education'].value_counts()\nprint(edu_count)","ee478b8d":"self_count = train['Self_Employed'].value_counts()\nprint(self_count)","9d5d01c4":"credit_count = train['Credit_History'].value_counts()\nprint(credit_count)","76d2360b":"prop_count = train['Property_Area'].value_counts()\nprint(prop_count)","b6b71750":"plt.figure(figsize=(3,5))\nsns.barplot(gender_count.index, gender_count.values\/491, alpha=0.9 )\nplt.title('Gender')","38a1a3fe":"plt.figure(figsize=(3,5))\nsns.barplot(train['Married'].value_counts().index, train['Married'].value_counts().values\/491, alpha=0.9 )\nplt.title('Married')","30e6e2bf":"plt.figure(figsize=(3,5))\nsns.barplot(self_count.index, self_count.values\/491, alpha=0.9 )\nplt.title('Self Employed')","9a1e9f81":"plt.figure(figsize=(3,5))\nsns.barplot(credit_count.index, credit_count.values\/491, alpha=0.9 )\nplt.title('Credit History')","ae7530fc":"plt.figure(figsize=(3,5))\nsns.barplot(train['Loan_Status'].value_counts().index, train['Loan_Status'].value_counts().values\/491, alpha=0.9 )\nplt.title('Loan Status')","5a8552d6":"plt.figure(figsize=(3,5))\nsns.barplot(train['Dependents'].value_counts().index, train['Dependents'].value_counts().values\/491, alpha=0.9 )\nplt.title('Dependents')","5dc27a31":"plt.figure(figsize=(3,5))\nsns.barplot(edu_count.index, edu_count.values\/491, alpha=0.9 )\nplt.title('Education')","681f1666":"plt.figure(figsize=(3,5))\nsns.barplot(prop_count.index, prop_count.values\/491, alpha=0.9 )\nplt.title('Property Area')","cb946ed0":"sns.set_style=(\"whitegrid\")\nsns.FacetGrid(train,hue=\"Loan_Status\",size=4).map(sns.distplot,\"ApplicantIncome\").add_legend();\nplt.show()","d4d7db8e":"sns.set_style=(\"whitegrid\")\nsns.FacetGrid(train,hue=\"Loan_Status\",size=4).map(sns.distplot,\"CoapplicantIncome\").add_legend();\nplt.show()","9a40eb57":"sns.set_style=(\"whitegrid\")\nsns.FacetGrid(train,hue=\"Loan_Status\",size=4).map(plt.scatter,\"Credit_History\",\"ApplicantIncome\").add_legend();\nplt.show()","355e4a05":"sns.set(style=\"whitegrid\")\nax = sns.boxplot(x=train[\"ApplicantIncome\"])","b0b4e8aa":"sns.set(style=\"whitegrid\")\nax = sns.boxplot(x=train[\"CoapplicantIncome\"])","75b88aab":"sns.set(style=\"whitegrid\")\nax = sns.boxplot(x=\"Education\", y=\"ApplicantIncome\", data=train)\n","3b4e0803":"sns.set_style=(\"whitegrid\")\nsns.FacetGrid(train,hue=\"Loan_Status\",size=4).map(sns.distplot,\"LoanAmount\").add_legend();\nplt.show()","8ae2c90b":"sns.set(style=\"whitegrid\")\nax = sns.boxplot(x=train[\"LoanAmount\"])","36287dce":"ct=pd.crosstab(train.Married, train.Loan_Status)\nprint(ct)","0fa3b365":"n = 1\nLoan_Status = np.random.choice([True,False], n)\nMarried = np.random.choice(['Yes','No', ], n)\nct.plot.bar(stacked=True)\nplt.legend(title='Loan_status')\n\nplt.show()","69251a38":"ct1=pd.crosstab(train.Dependents,train.Loan_Status)","5efb8332":"n = 1\nLoan_Status = np.random.choice([True,False], n)\nDependents= np.random.choice(['0','1', '2','3+'], n)\nct1.plot.bar(stacked=True)\nplt.legend(title='Loan_status')\n\nplt.show()","c50f9b55":"ct2=pd.crosstab(train.Education,train.Loan_Status)\nprint(ct2)","a62e45d1":"n = 1\nLoan_Status = np.random.choice([True,False], n)\nEducation= np.random.choice(['Yes','No'], n)\nct2.plot.bar(stacked=True)\nplt.legend(title='Loan_status')\n\nplt.show()","1234ba68":"ct3=pd.crosstab(train.Self_Employed,train.Loan_Status)\nprint(ct3)","9eb7c6b4":"n = 1\nLoan_Status = np.random.choice([True,False], n)\nSelf_Employed= np.random.choice(['Yes','No'], n)\nct3.plot.bar(stacked=True)\nplt.legend(title='Loan_status')\n\nplt.show()","09cd6bab":"ct4=pd.crosstab(train.Credit_History,train.Loan_Status)\nprint(ct4)","9e7854c6":"n = 1\nLoan_Status = np.random.choice([True,False], n)\nCredit_History= np.random.choice(['0.0','1.0'], n)\nct4.plot.bar(stacked=True)\nplt.legend(title='Loan_status')\n\nplt.show()","bb09d794":"ct5=pd.crosstab(train.Property_Area,train.Loan_Status)\nprint(ct5)","3942304b":"n = 1\nLoan_Status = np.random.choice([True,False], n)\nProperty_Area= np.random.choice(['Rural','Urban','Semiurban'], n)\nct5.plot.bar(stacked=True)\nplt.legend(title='Loan_status')\n\nplt.show()","ba1ee861":"train.groupby('Loan_Status')['ApplicantIncome'].mean().plot.bar()","6d3524bf":"train.shape","0ff036fc":"bins=[0,2500,4000,6000,81000]\ngroup=['Low','Average','High','Very High']\ntrain['Income_bin']=pd.cut(train['ApplicantIncome'],bins,labels=group)","41d6232a":"Income_bin=pd.crosstab(train['Income_bin'],train['Loan_Status'])\nIncome_bin.div(Income_bin.sum(1).astype(float),axis=0).plot(kind=\"bar\",stacked=True)\nplt.xlabel('ApplicantIncome') \nP=plt.ylabel('Percentage')","64613e81":"bins=[0,1000,3000,42000]\ngroup=['Low','Average','High']\ntrain['Coapplicant_Income_bin']=pd.cut(train['CoapplicantIncome'],bins,labels=group)","1a3fe57b":"Coapplicant_Income_bin=pd.crosstab(train['Coapplicant_Income_bin'],train['Loan_Status'])\nCoapplicant_Income_bin.div(Coapplicant_Income_bin.sum(1).astype(float),axis=0).plot(kind=\"bar\",stacked=True)\nplt.xlabel('CoapplicantIncome') \nP=plt.ylabel('Percentage')","9261055c":"train['Total_Income']=train['ApplicantIncome'] + train['CoapplicantIncome']","96b2cc54":"bins=[0,2500,4000,6000,81000]\ngroup=['Low','Average','High','Very High']\ntrain['Total_Income_bin']=pd.cut(train['Total_Income'],bins,labels=group)","4b1e26e7":"Total_Income_bin=pd.crosstab(train['Total_Income_bin'],train['Loan_Status'])\nTotal_Income_bin.div(Total_Income_bin.sum(1).astype(float),axis=0).plot(kind=\"bar\",stacked=True)\nplt.xlabel('TotalIncome') \nP=plt.ylabel('Percentage')","ffce66b0":"bins=[0,100,200,700]\ngroup=['Low','Average','High']\ntrain['LoanAmount_bin']=pd.cut(train['LoanAmount'],bins,labels=group)","b3a02041":"LoanAmount_bin=pd.crosstab(train['LoanAmount_bin'],train['Loan_Status'])\nLoanAmount_bin.div(LoanAmount_bin.sum(1).astype(float),axis=0).plot(kind=\"bar\",stacked=True)\nplt.xlabel('LoanAmount') \nP=plt.ylabel('Percentage')","875a86e3":"train=train.drop(['Income_bin','Coapplicant_Income_bin','LoanAmount_bin','Total_Income_bin','Total_Income'], axis=1);","24746a94":"train.shape\ntrain.head()","82da6af4":"train.isnull().sum()","1452383a":"cols = train.columns[:30] # first 30 columns\ncolours = ['#000099', '#ffff00'] # specify the colours - yellow is missing. blue is not missing.\nsns.heatmap(train[cols].isnull(), cmap=sns.color_palette(colours))","75d6be7f":"# We observe that we have many missing values in different columns. \n# Now we are going to replace the categorical values by most occuring value\n# and numerical values by median. Replacing missing values\n\ntrain['Gender'].fillna(train['Gender'].mode()[0],inplace = True)\ntrain['Dependents'].fillna(train['Dependents'].mode()[0],inplace = True)\ntrain['Married'].fillna(train['Married'].mode()[0],inplace = True)\ntrain['Self_Employed'].fillna(train['Self_Employed'].mode()[0],inplace = True)\ntrain['Credit_History'].fillna(train['Credit_History'].mode()[0],inplace = True)","e5da5cf8":"train['LoanAmount'].median()","25447a0d":"train['LoanAmount'].fillna(128, inplace =True)","b439582f":"train['Loan_Amount_Term'].median()","dcb6abec":"train['Loan_Amount_Term'].mode()[0]","6ccc3c20":"train['Loan_Amount_Term'].fillna(360, inplace =True)","e41de910":"train.isnull().sum()","56f0da29":"test.isnull().sum()","66706fa8":"cols = test.columns[:30] # first 30 columns\ncolours = ['#000099', '#ffff00'] # specify the colours - yellow is missing. blue is not missing.\nsns.heatmap(test[cols].isnull(), cmap=sns.color_palette(colours))","96aef379":"# We observe that we havefew missing values in different columns. \n# Now we are going to replace the categorical values by most occuring value\n# and numerical values by median. Replacing missing values\n\ntest['Gender'].fillna(test['Gender'].mode()[0],inplace = True)\ntest['Dependents'].fillna(test['Dependents'].mode()[0],inplace = True)\ntest['Married'].fillna(test['Married'].mode()[0],inplace = True)\ntest['Self_Employed'].fillna(test['Self_Employed'].mode()[0],inplace = True)\ntest['Credit_History'].fillna(test['Credit_History'].mode()[0],inplace = True)","cbc34cf6":"test['LoanAmount'].median()","f2b0f9a7":"test['LoanAmount'].fillna(122.5, inplace =True)","a8e85667":"test['Loan_Amount_Term'].median()","5b431529":"test['Loan_Amount_Term'].mode()[0]","e1a91b2a":"test['Loan_Amount_Term'].fillna(360, inplace =True)","eee601f3":"test.isnull().sum()","8d318e01":"train['Loan_Amount_Term'].hist(bins=100)","22c46cba":"test['Loan_Amount_Term'].hist(bins=100)","dd00edd9":"train['LoanAmount'].hist(bins=100)","dce99315":"test['LoanAmount'].hist(bins=100)","e27f0499":"sns.set(style=\"whitegrid\")\nax = sns.boxplot(x=train[\"LoanAmount\"])","9c9e9844":"train['LoanAmount_log'] = np.log(train['LoanAmount'])\ntrain['LoanAmount_log'].hist(bins =20)","9acc7cc7":"test['LoanAmount_log'] = np.log(test['LoanAmount'])","20df0ab2":"#Check for Unique","a44d50d3":"train.apply(lambda x: len(x.unique()))","7680d7d2":"test.apply(lambda x: len(x.unique()))","becc8c27":"train_1 = train.drop('Loan_ID', axis = 1)\ntest_1 = test.drop('Loan_ID', axis =1)","d69d6244":"X_train = train_1.drop('Loan_Status', 1)\ny_train = train_1.Loan_Status","abe85db4":"X_test = test_1.drop('Loan_Status', 1)\ny_test = test_1.Loan_Status","6639d0af":"X_train.shape","9a45d883":"X_test.shape","db656725":"y_train.head(10)","788294c7":"y_test.head(10)","ed473165":"X_train.head(5)","a2764780":"X_train1= pd.get_dummies(X_train)\ntrain_2 = pd.get_dummies(train_1)\ntest_2 = pd.get_dummies(test_1)\nX_test1 = pd.get_dummies(X_test)","80088aab":"approved_term = train_1[train_1['Loan_Status']=='Y']['Loan_Amount_Term'].value_counts()\nunapproved_term = train_1[train_1['Loan_Status']=='N']['Loan_Amount_Term'].value_counts()\ndf1 = pd.DataFrame([approved_term,unapproved_term])\ndf1.index = ['Approved','Unapproved']\ndf1.plot(kind='bar', figsize=(10,8))","41f92e52":"y_train.replace('N', 0,inplace=True)\ny_train.replace('Y', 1,inplace=True)","7ecb3798":"data_corr = pd.concat([X_train1, y_train], axis=1)\ncorr = data_corr.corr()\nplt.figure(figsize=(12,10))\nsns.heatmap(corr, annot=True);","fd14f909":"data_corr.corr()","230b2233":"y_t = train_1.Loan_Status","01fa6d67":"train_1['Loan_Status'].replace('N', 0,inplace=True)\ntrain_1['Loan_Status'].replace('Y', 1,inplace=True)","ef5288cc":"X_train1.head()","0e7ccb7a":"y_t.head()","ae83f954":"from sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier","df5a43f0":"classifier = []\nclassifier.append((\"LogisticReg\", LogisticRegression(solver='liblinear', multi_class='ovr')))\nclassifier.append((\"KNN\", KNeighborsClassifier()))\nclassifier.append((\"NaiveBayes\", GaussianNB()))","c0287b88":"seed = 0\nresults = []\nnames = []\nfor name, model in classifier:\n    kfold = model_selection.KFold(n_splits=10, random_state=seed ,shuffle=True)\n    cv_results = model_selection.cross_val_score(model, X_train1, y_t, cv=kfold)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","da327198":"logreg = LogisticRegression()\nlogreg.fit(X_train1,y_t)","19c1a44b":"y_pred = logreg.predict(X_test1)","1cd54065":"test_1['Loan_Status'].replace('N', 0,inplace=True)\ntest_1['Loan_Status'].replace('Y', 1,inplace=True)","7f30af3a":"y_te = test_1.Loan_Status","4ffb848e":"#accuracy_score = accuracy_score(y_te, y_pred)\n#print(accuracy_score)","673acd34":"import sklearn.metrics as metrics\n# calculate the fpr and tpr for all thresholds of the classification\nprobs = logreg.predict_proba(X_test1)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_te, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\n# method I: plt\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n","9b520f52":"cm = confusion_matrix(y_te, y_pred)\nprint(cm)","1f74739e":"report = classification_report(y_te, y_pred)\nprint(report)","51f35a0b":"\"\"\"Naive Bayes Classifier\"\"\"\nfrom sklearn.naive_bayes import GaussianNB\nNaiveB = GaussianNB()\nNaiveB.fit(X_train1, y_t)\ny_pred1 = NaiveB.predict(X_test1)\n# Creating confusion matrix and calculating the accuracy score\ncm_nb = confusion_matrix(y_te, y_pred1)\nprint(cm_nb)","7057f367":"as_nb = accuracy_score(y_te, y_pred1)\nprint(as_nb)","7a6223a2":"\"\"\" LOGISTIC REGRESSION \"\"\"\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 42)\nclassifier.fit(X_train1, y_t)\ny_pred = classifier.predict(X_test1)\n# Creating confusion matrix and calculating the accuracy score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm_logreg = confusion_matrix(y_te, y_pred)\nas_logreg=accuracy_score(y_te, y_pred)\n\n\"\"\" K-NEAREST NEIGHBORS \"\"\"\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 19, metric = 'minkowski', p = 2)\nclassifier.fit(X_train1, y_t)\ny_pred = classifier.predict(X_test1)\n# Creating confusion matrix and calculating the accuracy score\ncm_knn = confusion_matrix(y_te, y_pred)\nas_knn=accuracy_score(y_te, y_pred)","91d7f0bf":"print(cm_knn)","907f061b":"print(as_knn)","a7c8ac1b":"report = classification_report(y_te, y_pred)\nprint(report)","8bfb682f":"print(cm_logreg)","0efc95b7":"print(as_logreg)","43cf187c":"X_train1.head()","e889cf38":"X_train1['TotalIncome'] = X_train1['ApplicantIncome'] + X_train1['CoapplicantIncome']","27d73543":"sns.distplot(X_train1['TotalIncome'])","10167fcc":"X_train1['TotalIncomeLog'] = np.log(X_train1['TotalIncome'])\nsns.distplot(X_train1['TotalIncomeLog'])","3857da82":"X_test1['TotalIncome'] = X_test1['ApplicantIncome'] + X_test1['CoapplicantIncome']\nX_test1['TotalIncomeLog'] = np.log(X_test1['TotalIncome'])","52d76d4f":"X_test1['EMI'] = X_test1['LoanAmount']\/ X_test1['Loan_Amount_Term']","0ac10961":"X_train1['EMI'] = X_train1['LoanAmount']\/ X_train1['Loan_Amount_Term']","eee4816a":"sns.distplot(X_train1['EMI'])","cc87295a":"X_train1['BalanceIncome'] = X_train1['TotalIncome'] - (X_train1['EMI']*1000)\nsns.distplot(X_train1['BalanceIncome'])","1c47d6ce":"X_test1['BalanceIncome'] = X_test1['TotalIncome'] - (X_test1['EMI']*1000)","22fdeb8a":"X_train1 = X_train1.drop(['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term'], axis = 1)","582fb755":"X_train1.head()","9d180563":"X_test1 = X_test1.drop(['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term'], axis = 1)","cb967d81":"X_test1.head()","19691a7c":"classifier = []\nclassifier.append((\"LogisticReg\", LogisticRegression(solver='liblinear', multi_class='ovr')))\nclassifier.append((\"KNN\", KNeighborsClassifier()))\nclassifier.append((\"NaiveBayes\", GaussianNB()))\nclassifier.append((\"SVM Gaussian\", SVC(kernel = 'rbf', class_weight='balanced',random_state = 0)))\n#classifier.append((\"SVM No Kernel\", SVC(kernel = 'linear', random_state = 0)))\nclassifier.append((\"Decision Tree\", DecisionTreeClassifier(criterion = 'entropy', random_state = 0)))\nclassifier.append((\"Random Forest\", RandomForestClassifier(n_estimators = 500, criterion = 'entropy', random_state = 0)))","e2227124":"seed = 0\nresults = []\nnames = []\nfor name, model in classifier:\n    kfold = model_selection.KFold(n_splits=10, random_state=seed ,shuffle =True)\n    cv_results = model_selection.cross_val_score(model, X_train1, y_t, cv=kfold)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","3f941a34":"\"\"\" LOGISTIC REGRESSION \"\"\"\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(class_weight='balanced', random_state = 13)\nlogreg.fit(X_train1, y_t)\ny_pred = logreg.predict(X_test1)\n# Creating confusion matrix and calculating the accuracy score\nfrom sklearn.metrics import confusion_matrix, accuracy_score\ncm_logreg = confusion_matrix(y_te, y_pred)\nas_logreg=accuracy_score(y_te, y_pred)","ea2a9766":"print(as_logreg)","e5c2119a":"from numpy import sqrt\nfrom numpy import argmax\n# calculate the fpr and tpr for all thresholds of the classification\nprobs = logreg.predict_proba(X_test1)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_te, preds)\nroc_auc_logreg = metrics.auc(fpr, tpr)\n# calculate the g-mean for each threshold\ngmeans = sqrt(tpr * (1-fpr))\n# locate the index of the largest g-mean\nix = argmax(gmeans)\nprint('Best Threshold=%f, G-Mean=%.3f' % (threshold[ix], gmeans[ix]))\n\nplt.figure()\nplt.title('Receiver Operating Characterstics')\nplt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % (roc_auc_logreg))\nplt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\n \n# create the axis of thresholds (scores)\nax2 = plt.gca().twinx()\nax2.plot(fpr, threshold, markeredgecolor='r',linestyle='dashed', color='r')\nax2.scatter(fpr[ix], threshold[ix], marker='o', color='r', label='Best1')\nax2.set_ylabel('Threshold',color='r')\nax2.set_ylim([threshold[-1],threshold[0]])\nax2.set_xlim([fpr[0],fpr[-1]])\n \nplt.show()","22fe5db3":"\"\"\" K-NEAREST NEIGHBORS \"\"\"\nfrom sklearn.neighbors import KNeighborsClassifier\nKNN = KNeighborsClassifier(n_neighbors = 19, metric = 'minkowski', p = 2)\nKNN.fit(X_train1, y_t)\ny_pred = KNN.predict(X_test1)\n# Creating confusion matrix and calculating the accuracy score\ncm_knn = confusion_matrix(y_te, y_pred)\nas_knn=accuracy_score(y_te, y_pred)","b17f74ac":"print(as_knn)","cfb64919":"# calculate the fpr and tpr for all thresholds of the classification\nprobs = KNN.predict_proba(X_test1)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_te, preds)\nroc_auc_knn = metrics.auc(fpr, tpr)\n\n# method I: plt\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc_knn)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","cc8965b2":"\"\"\"Naive Bayes Classifier\"\"\"\nfrom sklearn.naive_bayes import GaussianNB\nNaiveB = GaussianNB()\nNaiveB.fit(X_train1, y_t)\ny_pred1 = NaiveB.predict(X_test1)\n# Creating confusion matrix and calculating the accuracy score\ncm_nb = confusion_matrix(y_te, y_pred1)\nas_nb = accuracy_score(y_te, y_pred1)\nprint(cm_nb)\nprint(as_nb)","b5c54b01":"from numpy import sqrt\nfrom numpy import argmax\n# calculate the fpr and tpr for all thresholds of the classification\nprobs = NaiveB.predict_proba(X_test1)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_te, preds)\nroc_auc_nb = metrics.auc(fpr, tpr)\n# calculate the g-mean for each threshold\ngmeans = sqrt(tpr * (1-fpr))\n# locate the index of the largest g-mean\nix = argmax(gmeans)\nprint('Best Threshold=%f, G-Mean=%.3f' % (threshold[ix], gmeans[ix]))\n\nplt.figure()\nplt.title('Receiver Operating Characterstics')\nplt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % (roc_auc_nb))\nplt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\n \n# create the axis of thresholds (scores)\nax2 = plt.gca().twinx()\nax2.plot(fpr, threshold, markeredgecolor='r',linestyle='dashed', color='r')\nax2.scatter(fpr[ix], threshold[ix], marker='o', color='r', label='Best1')\nax2.set_ylabel('Threshold',color='r')\nax2.set_ylim([threshold[-1],threshold[0]])\nax2.set_xlim([fpr[0],fpr[-1]])\n \nplt.show()","8cc9c31d":"\"\"\" SVM GAUSSIAN \"\"\"\nfrom sklearn.svm import SVC\nSVCG = SVC(kernel = 'rbf', class_weight='balanced',random_state = 42, probability = True)\nSVCG.fit(X_train1, y_t)\ny_pred = SVCG.predict(X_test1)\n# Creating confusion matrix and calculating the accuracy score\ncm_svm_gaussian = confusion_matrix(y_te, y_pred)\nas_svm_gaussian = accuracy_score(y_te, y_pred)\nprint(as_svm_gaussian)","ac277854":"# calculate the fpr and tpr for all thresholds of the classification\nprobs = SVCG.predict_proba(X_test1)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_te, preds)\nroc_auc_svg = metrics.auc(fpr, tpr)\n\n# method I: plt\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc_svg)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","dd71b8cd":"# \"\"\" SVM NO KERNEL \"\"\"\n# from sklearn.svm import SVC\n# classifier = SVC(kernel = 'linear', random_state = 0)\n# classifier.fit(X_train1, y_t)\n# y_pred = classifier.predict(X_test1)\n# # Creating confusion matrix and calculating the accuracy score\n# cm_svm_nokernel = confusion_matrix(y_te, y_pred)\n# as_svm_nokernel = accuracy_score(y_te, y_pred)\n# print(as_svm_nokernel)\n\"\"\" SVM GAUSSIAN \"\"\"\nfrom sklearn.svm import SVC\nSVCGI = SVC(kernel = 'rbf',random_state = 0, probability = True)\nSVCGI.fit(X_train1, y_t)\ny_pred = SVCGI.predict(X_test1)\n# Creating confusion matrix and calculating the accuracy score\ncm_svm_gaussian1 = confusion_matrix(y_te, y_pred)\nas_svm_gaussian1 = accuracy_score(y_te, y_pred)\nprint(as_svm_gaussian1)","9b652fe3":"# calculate the fpr and tpr for all thresholds of the classification\nprobs = SVCGI.predict_proba(X_test1)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_te, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\n# method I: plt\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","beaea983":"\"\"\" DECISION TREE CLASSIFICATION \"\"\"\nfrom sklearn.tree import DecisionTreeClassifier\nDTC = DecisionTreeClassifier(criterion = 'entropy', random_state = 42)\nDTC.fit(X_train1, y_t)\ny_pred = DTC.predict(X_test1)\n# Creating confusion matrix and calculating the accuracy score\ncm_dtc = confusion_matrix(y_te, y_pred)\nas_dtc = accuracy_score(y_te, y_pred)\nprint(as_dtc)","a88139f5":"# calculate the fpr and tpr for all thresholds of the classification\nprobs = DTC.predict_proba(X_test1)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_te, preds)\nroc_auc_dt = metrics.auc(fpr, tpr)\n\n# method I: plt\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc_dt)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","0c091ec3":"#Random FOrest without grid search and random values","b1dcae80":"\"\"\" RANDOM FOREST CLASSIFIER \"\"\"\nfrom sklearn.ensemble import RandomForestClassifier\nRanForest = RandomForestClassifier(n_estimators = 42, criterion = 'entropy', random_state = 42, max_depth = 3)\nRanForest.fit(X_train1, y_t)\ny_pred = RanForest.predict(X_test1)\n# Creating confusion matrix and calculating the accuracy score\ncm_rfc = confusion_matrix(y_te, y_pred)\nas_rfc = accuracy_score(y_te, y_pred)\nprint(as_rfc)","9dfd9207":"# Random Forest with best estimator and max depth","d2dee4cc":"from sklearn.model_selection import GridSearchCV\nparamgrid = {'max_depth': list(range(1,20,2)),'n_estimators':list(range(1,200,20))}\ngrid_search = GridSearchCV(RandomForestClassifier(random_state=1),paramgrid)\ngrid_search.fit(X_train1,y_t)\ngrid_search.best_estimator_","a332d697":"\"\"\" RANDOM FOREST CLASSIFIER \"\"\"\nfrom sklearn.ensemble import RandomForestClassifier\nRanForest = RandomForestClassifier(n_estimators = 141, criterion = 'entropy', random_state = 42, max_depth = 5)\nRanForest.fit(X_train1, y_t)\ny_pred = RanForest.predict(X_test1)\n# Creating confusion matrix and calculating the accuracy score\ncm_rfc = confusion_matrix(y_te, y_pred)\nas_rfc = accuracy_score(y_te, y_pred)\nprint(as_rfc)","1664dcfb":"from numpy import sqrt\nfrom numpy import argmax\n# calculate the fpr and tpr for all thresholds of the classification\nprobs = RanForest.predict_proba(X_test1)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_te, preds)\nroc_auc_rf = metrics.auc(fpr, tpr)\n# calculate the g-mean for each threshold\ngmeans = sqrt(tpr * (1-fpr))\n# locate the index of the largest g-mean\nix = argmax(gmeans)\nprint('Best Threshold=%f, G-Mean=%.3f' % (threshold[ix], gmeans[ix]))\n\nplt.figure()\nplt.title('Receiver Operating Characterstics')\nplt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % (roc_auc_rf))\nplt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\n \n# create the axis of thresholds (scores)\nax2 = plt.gca().twinx()\nax2.plot(fpr, threshold, markeredgecolor='r',linestyle='dashed', color='r')\nax2.scatter(fpr[ix], threshold[ix], marker='o', color='r', label='Best1')\nax2.set_ylabel('Threshold',color='r')\nax2.set_ylim([threshold[-1],threshold[0]])\nax2.set_xlim([fpr[0],fpr[-1]])\n \nplt.show()","10f649b5":"print(tpr)\nprint(fpr)\nprint(threshold)\nprint(gmeans)","06e906ee":"# Evaluating the best method to use in this loan prediction case\nscore={'auc_logreg':roc_auc_logreg, 'auc_knn':roc_auc_knn, 'auc_svm_gaussian':roc_auc_svg, 'auc_nb':roc_auc_nb, 'auc_dtc':roc_auc_dt, 'auc_rfc':roc_auc_rf}\nscore_list=[]\nfor i in score:\n    score_list.append(score[i])\n    u=max(score_list)\n    if score[i]==u:\n        v=i  \n    print(f\"{i}={score[i]}\");   \nprint(f\"The best AUROC score in this case is {v} with accuracy score {u}\")","8ca3640a":"# Evaluating the best method to use in this loan prediction case\nscore={'as_logreg':as_logreg, 'as_knn':as_knn, 'as_svm_gaussian':as_svm_gaussian, 'as_nb':as_nb, 'as_dtc':as_dtc, 'as_rfc':as_rfc}\nscore_list=[]\nfor i in score:\n    score_list.append(score[i])\n    u=max(score_list)\n    if score[i]==u:\n        v=i  \n    print(f\"{i}={score[i]}\");   \nprint(f\"The best accuracy score in this case is {v} with accuracy score {u}\")","a378cef1":"importances=pd.Series(RanForest.feature_importances_, index=X_train1.columns) \nimportances.plot(kind='barh', figsize=(12,8))","60da098f":"X_train1.head()","b559b37d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score, classification_report\nfrom sklearn.preprocessing import Normalizer, MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer, LabelEncoder\nfrom sklearn.pipeline import Pipeline","8b48163d":"pipe = Pipeline(steps=[\n    ('preprocess', StandardScaler()),\n    ('classification', MLPClassifier())\n])","5e2bfb7a":"random_state = 42\nmlp_activation = ['identity', 'tanh', 'relu']\nmlp_solver = ['sgd', 'adam']\nmlp_max_iter = [10, 100, 1000]\nmlp_alpha = [0.01, 0.1, 1]\npreprocess = [MinMaxScaler(), StandardScaler(), RobustScaler()]","ace4c36f":"mlp_param_grid = [\n    {\n        'preprocess': preprocess,\n        'classification__activation': mlp_activation,\n        'classification__solver': mlp_solver,\n        'classification__random_state': [random_state],\n        'classification__max_iter': mlp_max_iter,\n        'classification__alpha': mlp_alpha\n    }\n]","db9383dd":"print(X_train1.shape, y_t.shape)","5fc34f4e":"print(X_test1.shape, y_te.shape)","57d94527":"strat_k_fold = StratifiedKFold(\n    n_splits=10,\n    random_state=42, shuffle=True\n)\n\nmlp_grid = GridSearchCV(\n    pipe,\n    param_grid=mlp_param_grid,\n    cv=strat_k_fold,\n    scoring='f1',\n    n_jobs=-1,\n    verbose=2\n)\n\nmlp_grid.fit(X_train1, y_t)\n\n# Best MLPClassifier parameters\nprint(mlp_grid.best_params_)\n# Best score for MLPClassifier with best parameters\nprint('\\nBest F1 score for MLP: {:.2f}%'.format(mlp_grid.best_score_ * 100))\n\nbest_params = mlp_grid.best_params_","1982c4eb":"scaler = RobustScaler()\n\nprint('\\nData preprocessing with {scaler}\\n'.format(scaler=scaler))\n\nX_train_scaler = scaler.fit_transform(X_train1)\nX_test_scaler = scaler.transform(X_test1)\n\nmlp = MLPClassifier(\n    max_iter=1000,\n    alpha=1,\n    activation='identity',\n    solver='sgd',\n    random_state=42\n)\nmlp.fit(X_train_scaler, y_t)\n\nmlp_predict = mlp.predict(X_test_scaler)\nmlp_predict_proba = mlp.predict_proba(X_test_scaler)[:, 1]\n\nprint('MLP Accuracy: {:.2f}%'.format(accuracy_score(y_test, mlp_predict) * 100))\nprint('MLP AUC: {:.2f}%'.format(roc_auc_score(y_test, mlp_predict_proba) * 100))\nprint('MLP Classification report:\\n\\n', classification_report(y_test, mlp_predict))\nprint('MLP Training set score: {:.2f}%'.format(mlp.score(X_train_scaler, y_train) * 100))\nprint('MLP Testing set score: {:.2f}%'.format(mlp.score(X_test_scaler, y_test) * 100))","c0c1cdc1":"fpr, tpr, threshold = metrics.roc_curve(y_te, mlp_predict_proba)\nroc_auc = metrics.auc(fpr, tpr)\n# calculate the g-mean for each threshold\ngmeans = sqrt(tpr * (1-fpr))\n# locate the index of the largest g-mean\nix = argmax(gmeans)\nprint('Best Threshold=%f, G-Mean=%.3f' % (threshold[ix], gmeans[ix]))\n\nplt.figure()\nplt.title('Receiver Operating Characterstics')\nplt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % (roc_auc))\nplt.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\n \n# create the axis of thresholds (scores)\nax2 = plt.gca().twinx()\nax2.plot(fpr, threshold, markeredgecolor='r',linestyle='dashed', color='r')\nax2.scatter(fpr[ix], threshold[ix], marker='o', color='r', label='Best1')\nax2.set_ylabel('Threshold',color='r')\nax2.set_ylim([threshold[-1],threshold[0]])\nax2.set_xlim([fpr[0],fpr[-1]])\n \nplt.show()","fab0415e":"After observing the graph above we can say that the people having credit history as zero and with an income of less than\n20,000 can be segregated as NO. Although this is not very accurate.","6939a455":"<h4>Ordinal Feaures <\/h4>","234d1a2b":"But, here we get both the mode and median as 360. So we are choosing 360 to replace which just validates our point. ","90f3d36f":"As we can see, our data is <strong>imbalanced<\/strong> since there 70-30 distribution between approved and unapproved applicants.","0451fbe4":"Total_Income = Applicant Income + Co-applicant Income","3fb117dd":"<h2> Initial Model Building <\/h2>","b8f1b8fe":"<h3>Univariate Analysis<\/h3>","cd3445da":"The Loan_ID should not be same for two records, thus we check whether all the records have their unique Loan_ID variable. Since, we can see Loan_ID value is unique for both test and train, we proceed. ","c0647a11":"Instead of evaluating the applicant and co-applicant income separately, we combine it into a new column - Total_Income so that we get a better perspective.  ","337dc6e4":"<h2>Feature Engineering <\/h2>","f92647d8":"As observed from the graph above that people with bad credit history are  less likely to get a loan as compared to people\nwith good credit history.","565abbed":"Below are some of the factors which we think can affect Loan Approval: \n    <ol>\n    <li> Salary: Applicants with higher income have more chances of loan approval.<\/li>\n    <li> Previous History: Applicants who have repayed their earlier debts have greater changes of loan approval .<\/li>\n    <li> Loan Amount: If the loan amount is less , then the chances of loan approval is high. <\/li>\n    <li> Loan Term: Loan for less time period and less amount should have higher chances of approval . <\/li>\n    <li> EMI: Lesser the amount to be paid monthly to repay the loan , higher chances of approval . <\/li>","d7f52f4b":"In this section we will do univariate analysis. It is the simplest form of analysing data, where we examine each variable individually .","e0ff1b45":"The methods of handling outliers are somewhat similar to missing data. We either drop or adjust or keep them. We are trying to visualize them using log transformation.","f4630e73":"The Loan_Amount_Term is a continuous variable here. So instead of directly going with the median here, we would like to get an idea of what data represents by seeing the most occuring value.","5b274c0e":"The percentage of applicants who are graduates have got their loan approved rather than the one who are not graduates.","2ce34357":"<h6>Numerical Independent variable VS Target Variable <\/h6>","544054f5":"We observe a lot of outliers in this variable and the distribution is fairly normal. We will treat the outliers in the next section.\n","37eff1fa":"Here the y axis shows mean applicant income, no changes in the mean applicant income so we make bins for the applicant income varaiable based on the values in it and analyse the corresponding loan status for each bin.","434aa29b":"<h3>Data Cleaning<\/h3>","d370ac4f":"Checking for missing values in train dataset","f568c266":"<h6>Categorical Independent variable VS target Variable <\/h6>","0c3ab7d6":"For this graph we can consider, a value between 10  to 170  as an outlier .","fb2377c4":"Now the distribution looks much closer to normal and effect of extreme values has been significantly subsided.","f1478c4f":"\nWe can say that our data is cleaned and properly structured, we can move forward with out model building. ","a54c913b":"We observe that there are few missing values in test data set and now we will impute the new values , the same way we did for the train dataset above.","0aec3e83":"It can be inferred that Applicant Income does not affect the chances of loan approval which contradicts our hypothesis, in which the one with higher applicant income would have higher chance of approval. ","2561e855":" Now, we are going to upload or read the files\/data-sets using pandas. For this we used read_csv.","dff301bb":"We are visulaising different types of variables :\n<ol>\n    <li>Categorical Features<\/li>\n    <li>Ordinal Features<\/li>\n    <li>Numerical Features<\/li>\n<\/ol>","8d0ccbc8":"<h4>Hypothesis Generation <\/h4>","ec06a2d2":"This suggests that in our training dataset we have 12 missing values in Gender, 3 in married and so on.","39c40d61":"<h5>Checking for missing  values in test dataset<\/h5>","3db8bbc0":"Proportion of approved loans is higher for low and average Loan Amount as compared to that of High loan amount which supports our our initial hypothesis that approval chance is more for less loan amount. ","af61f669":"<ol>\n<li>Firstly, we can see that almost 70% of the applicants had their loan applications accepted.<\/li>\n<li>Most of the applicants have a good credit history.<\/li>\n<li>Very few who are self employed have applied for the loan.<\/li>\n<li>There is a large gender gap in the applied loan applicants. Most of them are male.<\/li>\n<li>Almost 65% of the applicants are Married.<\/li>\n<li>Less than 10% of the applicants have 3+ dependents.<\/li>\n<li>Graduates make up almost 78% of the applicants.<\/li>\n<\/ol>","60954cdf":"<h4>Getting new Data Columns<\/h4>","3901a387":"Thus, using the function above we have successfully divided our dataset. 80% of the total set is train and rest 20% is the test data","157bf79e":"<h4>Categorical Features<\/h4>","f16d7b38":"For this graph we can consider, a value above 600 as an outlier.","ba49d436":"After understaning the problem statement , we would now move on to hypothesis generation.","a99b102f":"<h2> Final Model Building <\/h2>","98bb0d0d":"As our main target is Loan Status Variable, we will try to find if Applicant income can exactly separate the Loan Status.\nSuppose if we can find that if applicant income is above some X amount then Loan Status is yes .Else it is No. Firstly I am trying to plot the distribution plot based on Loan Status.","386135f7":"We can see that there are graduates with very high incomes are outliers which would be taken care of later.","be38ac84":"<h5>Checking for outliers in our data<\/h5>","04747a66":"For this graph we can consider, a value between 50  to 170  as an outlier .","7f0e5396":"<h3>Bivariate Analysis<\/h3>","244507e9":"Unfortunately we cannot segregate based on Applicant Income alone. The same is the case with Co-applicant Income as shown in the graph plotted above. We try a different approach and do a scatter plot. Also, the distribution of both the graphs is not normal, we would try to make these distributions normal in the later section so that the algorithm can fit better.\nWe try a different approach and do a scatter plot.","68ee607e":"We can see that the proportion of loans getting approved for Low income is lower than when compared to Average, high and Very High total income.","504fe866":"It can be inferred from the above two graphs that most of the data in the distribution of applicant income and Coapplicant Income is towards the left which means it is not normally distributed and the box plot confirms the presence of a lot of outliers which can be attributed to income disparity in the society.","beecdb8f":"Out of the three plotted above, it can be infered that the people living in Semiurban places have a higher percentage to\nto get acquire a loan.","653e2980":"We have successfully imputed all the missing values and now no null values exist in the train dataset.","51f663c3":"We can infer that percentage of married people who have got their loan approved is higher when compared to non- married people.","e4bd85e7":"Since we have a lot of categorical variables which affect Loan Status, we need to convert each of them in to numeric data for modeling.\n\n","fe84442f":"With this, we have completed our Bivariate analysis.","5903df8a":"<h6>We have used a function for heatmap here. The heatmap is a way of representing the data in a 2-dimensional form. The data values are represented as colors in the graph. The goal of the heatmap is to provide a colored visual summary of information. Since we have less number of features here, we have used the heatmap data technique to gain a perspective on null data values.<\/h6>","c73fa580":"<h5>With this we have completed our exploratory data analysis on the training data and now we move on to data cleaning of both the train data and test data.<\/h5>","992f9ab0":"<h6>Now we would like to know how well each feature correlate with Loan Status, therefore we would look at the bivariate analysis.<\/h6>","ebba8f7a":"<h4>Numerical Features <\/h4>","c132106a":"Out of all the algorithms mentioned above , the best accuracy observed is with logistic regression after implementing the K-fold technique and the least accuracy is of K-nearest neighbours .","696c13e9":"We have used a histogram plot to observe any outliers if present in the data, as shown in the graphs above, there are  a few outliers.  \n","9a71d961":"It shows that if the coapplicant's income is less, then there is a higher chance of loan approval. But this does not look right . The possible explanation can be that not many applicants have a coapplicant , therefore the coapplicant income for them is 0 and hence the loan approval is not dependent on it.So we will make a new variable to see the combined effect on Loan status.","6f1a71a3":"There is hardly any correlation between Loan_Status and Self_Employed applicants.\nSo in short we can say that it doesn\u2019t matter whether the applicant is self employed or not.","ea56e5e5":"To determine the accuracy of our model after training it, we will test on the unseen data called as test data and thus using the function as given below we have split our dataset into train and test.","4fad6525":"Now that we have encountered our problem, there are three solutions which we can implement to counter it:\n\n<ol>\n    <li>Drop the observation.<\/li>\n    <li>Drop the feature.<\/li>\n    <li>Impute the missing data.<\/li>\n<\/ol>\n\nWe have implemented the third solution. ","a9972108":"Now we will drop the bins we made for the exploration part and move on to the next section of data cleaning .","66f4b0e1":"The percentage of applicants with either 0 or 2 dependents have got their loan approved is higher.","ff7041b3":"<h5> Here we will pause for conclusions from the data we plotted:<\/h5>"}}