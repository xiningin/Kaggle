{"cell_type":{"ee218245":"code","097fd371":"code","2276d148":"code","a1389198":"code","4c266d1a":"code","e58b432b":"code","a2a415cb":"code","310e090b":"code","4736f4aa":"code","79133e40":"code","3168fdd9":"code","cd4fa163":"code","3610b0e8":"code","4d685c3f":"code","e47500c2":"code","1b3e645f":"code","47a1a6b5":"code","0f0e56ce":"code","2e772aa1":"code","95829c0b":"code","05bf22f0":"code","05271246":"code","1528adea":"code","b314095d":"code","19d53188":"code","0b67f7c4":"code","d3885e3c":"code","a7acac60":"code","1e159fbe":"code","8b1265d7":"code","3bf0d584":"code","1ccc524c":"code","ddbd6acb":"markdown","23645226":"markdown","cab29c5f":"markdown","29b1a050":"markdown","bd3168eb":"markdown","c59cbc6a":"markdown","31dd3623":"markdown","0a743ff7":"markdown","89b6a150":"markdown","18464926":"markdown","e289a274":"markdown","c9c10f90":"markdown","b9918984":"markdown","353bfc40":"markdown","bc095e77":"markdown","44b74456":"markdown","fd3b0ff4":"markdown","be1e96ac":"markdown","54b71e67":"markdown"},"source":{"ee218245":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nfrom wordcloud import WordCloud,STOPWORDS\nimport spacy as sp\nimport string\nimport nltk\nimport re\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport plotly.offline as pyo\nfrom plotly.subplots import make_subplots\npyo.init_notebook_mode()\nnltk.download('vader_lexicon')\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nnlps = sp.load('en')\nfrom nltk.util import ngrams\nfrom nltk import word_tokenize\ndef RMSE(y,yh):\n    return np.sqrt(mean_squared_error(y,yh))\n\n\nplt.rc('figure',figsize=(18,11))","097fd371":"d_data =pd.read_csv('\/kaggle\/input\/drake-lyrics\/drake_data.csv',usecols=['album','lyrics_title','lyrics','track_views'])\nd_data.head(4)","2276d148":"preprocessed = d_data.copy()\nfor col in preprocessed.columns[:-1]:\n    preprocessed[col] = preprocessed[col].str.lower()\n\n    \npreprocessed.lyrics_title     = preprocessed.lyrics_title.apply(lambda x: x.replace('lyrics',''))\npreprocessed['is_demo']       = preprocessed.lyrics_title.apply(lambda x: 1 if x.find('(demo)') != -1 else 0)\npreprocessed.lyrics_title     = preprocessed.lyrics_title.apply(lambda x: x.replace('(demo)',''))\n\ndef extract_feat(s):\n    artist = s[s.find(\"(\")+1:s.find(\")\")]\n    if artist.find('ft.') != -1:\n        return artist.replace('ft. ','')\n    else:\n        return 'solo'\n    \ndef remove_artist(s):\n    artist = s[s.find(\"(\")+1:s.find(\")\")]\n    if artist.find('ft.') != -1:\n        return s[:s.find(\"(\")]\n    else:\n        return s\npreprocessed['featuring'] = preprocessed.lyrics_title.apply(extract_feat)\npreprocessed.lyrics_title = preprocessed.lyrics_title.apply(remove_artist)\npreprocessed.lyrics_title = preprocessed.lyrics_title.apply(remove_artist)\npreprocessed.lyrics_title = preprocessed.lyrics_title.apply(lambda x:re.sub(r'[^\\w\\s]', '', x) )\n\ndef view_preprocess(s):\n    if type(s)!= float:\n        if s[-1]=='K':\n            return float(s.replace('K',''))*1000\n        if s[-1]=='M':\n            return float(s.replace('M',''))*10**6\n\npreprocessed.track_views = preprocessed.track_views.apply(view_preprocess)\n\n\npreprocessed = preprocessed.loc[preprocessed.lyrics.dropna().index,:]\n\npreprocessed['number_of_verses']=0\npreprocessed['number_of_chorus']=0\npreprocessed.loc[preprocessed.lyrics.notna().index,'number_of_verses'] = preprocessed.lyrics[preprocessed.lyrics.notna()].apply(lambda x:len( re.findall(r'verse',x)))\npreprocessed.loc[preprocessed.lyrics.notna().index,'number_of_chorus'] = preprocessed.lyrics[preprocessed.lyrics.notna()].apply(lambda x:len( re.findall(r'chorus',x)))\n\npreprocessed.loc[preprocessed.lyrics.notna().index,'lyrics'] = preprocessed.lyrics[preprocessed.lyrics.notna()].apply(lambda x: re.sub(r'\\[([^]]*)]','',x))\npreprocessed.loc[preprocessed.lyrics.notna().index,'lyrics'] = preprocessed.lyrics[preprocessed.lyrics.notna()].apply(lambda x: x.replace('\\n',' '))\npreprocessed.loc[preprocessed.lyrics.notna().index,'lyrics'] = preprocessed.lyrics[preprocessed.lyrics.notna()].apply(lambda x:re.sub(r'[^\\w\\s]', '', x))\n\n\nsid = SIA()\npreprocessed['sentiments']           = preprocessed['lyrics'].apply(lambda x: sid.polarity_scores(x))\npreprocessed['Positive Sentiment']   = preprocessed['sentiments'].apply(lambda x: x['pos']) \npreprocessed['Neutral Sentiment']    = preprocessed['sentiments'].apply(lambda x: x['neu'])\npreprocessed['Negative Sentiment']   = preprocessed['sentiments'].apply(lambda x: x['neg'])\n\npreprocessed.drop(columns=['sentiments'],inplace=True)\n\npreprocessed['# Of Words']                 = preprocessed['lyrics'].apply(lambda x: len(x.split(' ')))\npreprocessed['# Of StopWords']             = preprocessed['lyrics'].apply(lambda x: len([word for word in x.split(' ') if word in list(STOPWORDS)]))\npreprocessed['Average Word Length']        = preprocessed['lyrics'].apply(lambda x: np.mean(np.array([len(va) for va in x.split(' ') if va not in list(STOPWORDS)])))\npreprocessed['Average Sentence Length']    = preprocessed['lyrics'].apply(lambda x: np.mean(np.array([len(va) for va in x.split('.')])))\n\n\n\n\n\n\n\n#Album Mean Statistics DF\nalbum_d = preprocessed.groupby(by='album').mean()\nalbum_d.drop(index='unreleased songs',inplace=True)\nalbum_d['Release_Year'] = 0\nalbum_d.loc['care package','Release_Year']                         =2019\nalbum_d.loc['certified lover boy','Release_Year']                  =2021\nalbum_d.loc['comeback season','Release_Year']                      =2007\nalbum_d.loc['dark lane demo tapes','Release_Year']                 =2020\nalbum_d.loc['drake demo disk','Release_Year']                      =2006\nalbum_d.loc['if you\u2019re reading this it\u2019s too late','Release_Year'] =2015\nalbum_d.loc['more life','Release_Year']                            =2017\nalbum_d.loc['nothing was the same','Release_Year']                 =2013\nalbum_d.loc['room for improvement','Release_Year']                 =2006\nalbum_d.loc['scary hours','Release_Year']                          =2018\nalbum_d.loc['scorpion','Release_Year']                             =2018\nalbum_d.loc['so far gone','Release_Year']                          =2009\nalbum_d.loc['so far gone (ep)','Release_Year']                     =2009\nalbum_d.loc['take care','Release_Year']                            =2011\nalbum_d.loc['thank me later','Release_Year']                       =2010\nalbum_d.loc['the best in the world pack','Release_Year']           =2019\nalbum_d.loc['views','Release_Year']                                =2016\n\n\n\npreprocessed.head(3)","a1389198":"plt.title('Amount Of Missing Values Per Feature',fontsize=19,fontweight='bold')\nsns.heatmap(preprocessed.isna().sum().to_frame(),annot=True,cmap='nipy_spectral')\nplt.show()","4c266d1a":"plt.title('Number Of Songs Associated With Each Album',fontsize=19,fontweight='bold')\nax = sns.barplot(y=preprocessed.album.value_counts().index,x=preprocessed.album.value_counts().values,palette='nipy_spectral')\nax.set_yticklabels(ax.get_yticklabels(),fontsize=15,fontweight='bold')\nplt.show()","e58b432b":"plt.title('Number Of Songs Labeled as Demo Songs',fontsize=19,fontweight='bold')\nax = sns.countplot(preprocessed.is_demo,palette='nipy_spectral')\nplt.show()","a2a415cb":"plt.title('Top 10 Most Featured Artist In Drakes Songs',fontsize=19,fontweight='bold')\nax = sns.barplot(y=preprocessed.featuring.value_counts()[1:11].index,x=preprocessed.featuring.value_counts()[1:11].values,palette='nipy_spectral')\nax.set_yticklabels(ax.get_yticklabels(),fontsize=15,fontweight='bold')\nplt.show()","310e090b":"plt.subplot(3,1,1)\nplt.title('Distribution Of Song Views Before Log Transformation',fontsize=19,fontweight='bold',color='b')\nsns.kdeplot(preprocessed.track_views)\nplt.subplot(3,1,2)\npreprocessed.track_views= np.log(preprocessed.track_views)\nplt.title('Distribution Of Song Views After Log Transformation',fontsize=19,fontweight='bold',color='r')\nsns.kdeplot(preprocessed.track_views,color='r')\nplt.subplot(3,1,3)\nplt.title('CDF Of Song Views After Log Transformation',fontsize=19,fontweight='bold',color='tab:red')\nsns.kdeplot(preprocessed.track_views,color='r',cumulative=True)\nplt.show()","4736f4aa":"plt.subplot(2,1,1)\nplt.title('Distribution Of Different Amounts of Verses In Drakes Songs',fontsize=19,fontweight='bold',color='g')\nsns.kdeplot(preprocessed.number_of_verses,color='g')\nplt.subplot(2,1,2)\nplt.title('Distribution Of Different Amounts of Choruses In Drakes Songs',fontsize=19,fontweight='bold',color='b')\nsns.kdeplot(preprocessed.number_of_chorus)","79133e40":"fig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Perason Correaltion',  'Spearman Correaltion'))\n\n\n\ns_val =preprocessed.corr('pearson')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,name='pearson',showscale=False,xgap=1,ygap=1),\n    row=1, col=1\n)\n\n\ns_val =preprocessed.corr('spearman')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,xgap=1,ygap=1),\n    row=2, col=1\n)\nfig.update_layout(\n    hoverlabel=dict(\n        bgcolor=\"white\",\n        font_size=16,\n        font_family=\"Rockwell\"\n    )\n)\nfig.update_layout(height=700, width=900, title_text=\"Correlations Between Our Different Numeric Features\")\nfig.show()","3168fdd9":"fig = go.Figure()\nalbum_d = album_d.sort_values(by='Release_Year')\nalbum_dm = album_d.groupby(by='Release_Year').mean().reset_index()\n\nfor column in album_dm.columns[:-1]:\n    fig.add_trace(\n        go.Scatter(\n            x = album_dm.Release_Year,\n            y = album_dm[column],\n            name = column,\n        )\n    )\n    \n\nbtns = []\nfor x,col in enumerate(album_dm.columns[:-1]):\n    bol = [False]*12\n    bol[x]=True\n    d = dict(label = col,\n                  method = 'update',\n                  args = [{'visible':bol},\n                          {'title': 'Distribution of [' +col+'] Over The Years',\n                           'showlegend':True}])\n    btns.append(d)\n    \n    \nfig.update_layout(title='Feautres Distribution Over The Years',\n    updatemenus=[go.layout.Updatemenu(\n        active=0,\n        showactive=True,\n        buttons=btns\n        )\n    ])\n\nfig.show()","cd4fa163":"album_mean = album_d.sort_values(by='Release_Year')\n\nalbum_year_index =[album + \"  \"+ str(year) for album,year in zip(album_mean.index,album_mean.Release_Year)]\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=album_year_index, y=album_mean['Positive Sentiment'],\n                    mode='lines+markers',\n                    name='Mean Positive Sentiment Value',hovertext=album_mean['Release_Year']))\nfig.add_trace(go.Scatter(x=album_year_index, y=album_mean['Negative Sentiment'],\n                    mode='lines+markers',\n                    name='Mean Negative Sentiment Value',hovertext=album_mean['Release_Year']))\n\nfig.update_layout(title='Album Sentiment Change Over The Years')\nfig.show()\n#album_mean","3610b0e8":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=album_year_index, y=album_mean['number_of_verses'],\n                    mode='lines+markers',\n                    name='Mean Verse Amount',hovertext=album_mean['Release_Year']))\nfig.add_trace(go.Scatter(x=album_year_index, y=album_mean['number_of_chorus'],\n                    mode='lines+markers',\n                    name='Mean Chorus Amount',hovertext=album_mean['Release_Year']))\n\nfig.update_layout(title='Album Chorus and Verse Count Change Over The Years')\nfig.show()","4d685c3f":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=album_year_index, y=album_mean['track_views'],\n                    mode='lines+markers',\n                    name='Mean Verse Amount',hovertext=album_mean['Release_Year']))\n\nfig.update_layout(title='Album Views Over The Years')\nfig.show()","e47500c2":"l_t = ' '.join(preprocessed.lyrics_title).strip().replace('drake','')\nl_t = l_t.replace('ft','')\nw_c = WordCloud(width=600,height=400,collocations = False,stopwords=STOPWORDS,colormap='nipy_spectral',background_color='white').generate(l_t)\n\nplt.title('Most Used Words By size In Drakes Song Titles',fontsize=19,fontweight='bold')\nplt.imshow(w_c)\nplt.axis('off')\nplt.show()","1b3e645f":"l_t = ' '.join(preprocessed.lyrics[preprocessed.lyrics.notna()]).strip()\nw_c = WordCloud(width=600,height=400,collocations = False,stopwords=STOPWORDS,colormap='nipy_spectral',background_color='white').generate(l_t)\n\nplt.title('Most Used Words By size In Drakes Lyrics',fontsize=19,fontweight='bold')\nplt.imshow(w_c)\nplt.axis('off')\nplt.show()","47a1a6b5":"#preprocessed.lyrics\nw1_dict = dict()\nfor word in l_t.split():\n    w= word.strip()\n    if w in STOPWORDS:\n        continue\n    else:\n        w1_dict[w] = w1_dict.get(w,0)+1\nw1_dict = {k: v for k, v in sorted(w1_dict.items(), key=lambda item: item[1],reverse=True)}\n\nw2_dict = dict()\n\ntop_10_w1 = list(w1_dict.keys())[:10]\ntoken=nltk.word_tokenize(l_t)\ntrigram =ngrams(token,3)\ntrigram = [k for k in trigram if k[0] in top_10_w1]","0f0e56ce":"w_c = WordCloud(width=600,height=400,collocations = False,colormap='nipy_spectral',background_color='white').generate(' '.join(top_10_w1))\nplt.title('Top 10 Words In Drakes Lyrics',fontsize=19,fontweight='bold')\nplt.imshow(w_c)\nplt.axis('off')\nplt.show()","2e772aa1":"token=nltk.word_tokenize(l_t)\nbigram=ngrams(token,2)\nbigram_dict = dict()\nfor i in bigram:\n    bigram_dict[i] = bigram_dict.get(i,0)+1\n    \ntrigram_dict = dict()\nfor i in trigram:\n    trigram_dict[i] = trigram_dict.get(i,0)+1","95829c0b":"tri_gram =pd.DataFrame(list(trigram_dict.keys())[:15],columns=['One Of Top 10 Words','Second Word','Third Word'])\n\ndef get_prob(sir):\n    key = (sir['One Of Top 10 Words'],sir['Second Word'],sir['Third Word'])\n    w3 = trigram_dict[key]\n    w2 = bigram_dict[(sir['One Of Top 10 Words'],sir['Second Word'])]\n    return w3\/w2\n\ntri_gram['Probabilty Of Sentence'] = tri_gram.apply(get_prob,axis=1)\n\ntri_gram.style.background_gradient(cmap='coolwarm')","05bf22f0":"NUMBER_OF_COMPONENTS=100\n\nCV = CountVectorizer()\nsvd = TruncatedSVD(NUMBER_OF_COMPONENTS)\n\nc_matrix = CV.fit_transform(preprocessed.lyrics)\n\ndec_matrix = svd.fit_transform(c_matrix)\ndec_df=pd.DataFrame(dec_matrix,columns=['PC_{}'.format(i) for i in range(1,NUMBER_OF_COMPONENTS+1)])\n\nex_var = svd.explained_variance_ratio_\nvariance_cum = np.cumsum(ex_var)\ndata = [go.Scatter(x=np.arange(0,len(variance_cum)),y=variance_cum,name='Cumulative Explained Variance',mode='lines+markers'),\n        go.Scatter(x=np.arange(0,len(variance_cum)),y=ex_var,name='Explained Variance',mode='lines+markers')]\nlayout = dict(title='Explained Variance Ratio Using {} Words'.format(NUMBER_OF_COMPONENTS),\n             xaxis_title='# Componenets',yaxis_title='Explained Variance',height=650,width=900)\nfig = go.Figure(data=data,layout=layout)\nfig.update_layout(template='seaborn')\nfig.show()","05271246":"dec_df.head(5)","1528adea":"preprocessed=preprocessed.reset_index()\nY = preprocessed.loc[preprocessed.track_views.notna(),'track_views']\nX = dec_df.iloc[Y.index,:]\n\nrf_pipe = Pipeline(steps =[  (\"RF\",RandomForestRegressor(random_state=42)) ])\nLR_pipe = Pipeline(steps =[  (\"LR\",LinearRegression()) ])\nRIDGE_pipe = Pipeline(steps =[ (\"R\",Ridge()) ])\n\n\nRF_f1_cross_val_scores = np.sqrt(-1*cross_val_score(rf_pipe,X,Y,cv=5,scoring='neg_mean_squared_error'))\nLR_f1_cross_val_scores= np.sqrt(-1*cross_val_score(LR_pipe,X,Y,cv=5,scoring='neg_mean_squared_error'))\nRIDGE_f1_cross_val_scores= np.sqrt(-1*cross_val_score(RIDGE_pipe,X,Y,cv=5,scoring='neg_mean_squared_error'))","b314095d":"fig = make_subplots(rows=3, cols=1,shared_xaxes=True,subplot_titles=('Random Forest Cross Val Scores',\n                                                                     'Linear Regression Cross Val Scores',\n                                                                    'Ridge Regression Cross Val Scores'))\n\nfig.add_trace(\n    go.Scatter(x=np.arange(0,len(RF_f1_cross_val_scores)),y=RF_f1_cross_val_scores,name='Random Forest'),\n    row=1, col=1\n)\nfig.add_trace(\n    go.Scatter(x=np.arange(0,len(LR_f1_cross_val_scores)),y=LR_f1_cross_val_scores,name='Linear Regression'),\n    row=2, col=1\n)\nfig.add_trace(\n    go.Scatter(x=np.arange(0,len(RIDGE_f1_cross_val_scores)),y=RIDGE_f1_cross_val_scores,name='Ridge Regression'),\n    row=3, col=1\n)\n\nfig.update_layout(height=700, width=900, title_text=\"Different Model 5 Fold Cross Validation\")\nfig.update_yaxes(title_text=\"RMSE\")\nfig.update_xaxes(title_text=\"Fold #\")\n\nfig.show()","19d53188":"def hyperparameter_estimators(X,Y,h_list):\n    r_s = []\n    for est in h_list:\n        model = Pipeline(steps =[  (\"RF\",RandomForestRegressor(random_state=42,n_estimators=est)) ])\n        model.fit(X,Y)\n        pred = model.predict(X)\n        r_s.append(r2_score(Y,pred))\n    return r_s\n        ","0b67f7c4":"estimators_under_test = [10,50,100,300,500,700,850]\nrs = hyperparameter_estimators(X,Y,estimators_under_test)\nplt.plot(estimators_under_test,rs,'go--')\nplt.title(r'Choosing The Number of Estimators That Maximizes $R^2$ of Prediction',fontsize=17,fontweight='bold')\nplt.xticks(estimators_under_test)\nplt.ylabel(r'$R^2$ Value',fontsize=17,fontweight='bold')\nplt.xlabel(r'Number Of Estimators',fontsize=17,fontweight='bold')\nplt.show()","d3885e3c":"def hyperparameter_estimators(X,Y,h_list):\n    r_s = []\n    for est in h_list:\n        model = Pipeline(steps =[  (\"RF\",RandomForestRegressor(random_state=42,n_estimators=500,max_leaf_nodes=est)) ])\n        model.fit(X,Y)\n        pred = model.predict(X)\n        r_s.append(r2_score(Y,pred))\n    return r_s\n\nnodes_under_test = [2,3,5,7,10,30,50]\nrs = hyperparameter_estimators(X,Y,nodes_under_test)\nplt.plot(nodes_under_test,rs,'ro--')\nplt.title(r'Choosing The Number of Leaf Nodes That Maximizes $R^2$ of Prediction',fontsize=17,fontweight='bold')\nplt.xticks(nodes_under_test)\nplt.ylabel(r'$R^2$ Value',fontsize=17,fontweight='bold')\nplt.xlabel(r'Number Of Leaf Nodes',fontsize=17,fontweight='bold')\nplt.show()","a7acac60":"rf_pipe = Pipeline(steps =[  (\"RF\",RandomForestRegressor(random_state=42,n_estimators=500,max_leaf_nodes=50)) ])\nrf_pipe.fit(X,Y)\npredictions = rf_pipe.predict(X)","1e159fbe":"print('Total RMSE For Prediction On Entire Dataset : {}'.format(RMSE(Y,predictions)))","8b1265d7":"plt.title(r'Validating That There Is No Visable Heteroscedasticity',fontsize=17,fontweight='bold')\nsns.residplot(predictions,Y)\nplt.show()","3bf0d584":"output = pd.DataFrame({'Prediction':predictions,'Actual':Y})\noutput.to_csv('view_prediction.csv',index=False)\n","1ccc524c":"fig = make_subplots(\n    rows=3, cols=2,subplot_titles=('','Actual','Predictions','Residuals'),\n    vertical_spacing=0.09,\n    specs=[[{\"type\": \"table\",\"rowspan\": 3}     ,{\"type\": \"scatter\"}] ,\n           [None                               ,{\"type\": \"scatter\"}]            ,           \n           [None                               ,{\"type\": \"scatter\"}]                           \n          ]\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=np.arange(0,len(output[\"Actual\"])),\n        y=output[\"Actual\"],\n        mode=\"markers\",\n    ),\n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=np.arange(0,len(output[\"Prediction\"])),\n        y=output[\"Prediction\"],\n        mode=\"markers\",\n    ),\n    row=2, col=2\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=np.arange(0,len(output[\"Prediction\"])),\n        y=output[\"Prediction\"]-output[\"Actual\"],\n        mode=\"markers\",\n    ),\n    row=3, col=2\n)\n\nfig.add_trace(\n    go.Table(\n        header=dict(\n            values=['Prediction','Actual'],\n            font=dict(size=10),\n            align=\"left\"\n        ),\n        cells=dict(\n            values=[output[k].tolist() for k in output.columns],\n            align = \"left\")\n    ),\n    row=1, col=1\n)\n\n\n\nfig.add_shape(type=\"line\",\n    x0=0, y0=(output[\"Prediction\"]-output[\"Actual\"]).mean(), x1=len(output[\"Prediction\"]), y1=(output[\"Prediction\"]-output[\"Actual\"]).mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x3', \n        yref='y3'\n)\n\nfig.update_layout(\n    height=800,\n    showlegend=False,\n    title_text=\"Prediction Evaluation\",\n)\n\nfig.show()","ddbd6acb":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;\">Vectorization And Decomposition <\/h3>\n","23645226":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Random Forest Tuning<\/h3>\n","cab29c5f":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">5 Fold Cross Validation<\/h3>\n","29b1a050":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;\">Model Selection And Evaluation<\/h3>\n","bd3168eb":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>We can observe that after performing log transformation on Drake's song views, we get a bimodal distribution,<\/span><\/p>\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>In other words, in log space, Drake's songs have 2 groups of which one is less dominant, but the presence of 2 underlaying groups is undeniable, and it is fascinating to understand what are those groups!<\/span><\/p>","c59cbc6a":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Above is a trigram of 15 sentences that start with one of the top 10 words and the probability that the sentence will appear in Drake's lyrics.<\/span><\/p>\n<p><br><\/p>","31dd3623":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Apparently, more than 90% of the variance in Drake's lyrics can be explained using only 100 words. We will use a Dataframe of decomposed count vectors for each word to predict the number of views a song might get.<\/span><\/p>","0a743ff7":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Libraries And Utilities<\/h3>\n","89b6a150":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>&quot;Scary Hours&quot; and &quot;Nothing was the Same&quot; definitely were of major significance to Drake&apos;s total success as they had the highest average views per song.<\/span><\/p>\n<p><br><\/p>\n<a href=\"https:\/\/ibb.co\/g4Z4RJK\"><img src=\"https:\/\/i.ibb.co\/z6V64Zv\/Screenshot-2021-01-09-154638.png\" alt=\"Screenshot-2021-01-09-154638\" border=\"0\"><\/a>\n<a href=\"https:\/\/ibb.co\/fM4z0Rk\"><img src=\"https:\/\/i.ibb.co\/6Yg7shH\/Screenshot-2021-01-09-173513.png\" alt=\"Screenshot-2021-01-09-173513\" border=\"0\"><\/a>","18464926":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Data Preprocessing And Feature Engineering<\/h3>\n","e289a274":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;\">Exploratory Data Analysis<\/h3>\n","c9c10f90":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Correlation Analysis<\/h3>\n","b9918984":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Text Based Analysis<\/h3>\n","353bfc40":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>So the number of verses and choruses also follow multimodal distributions.<\/span><\/p>\n<p style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\">The number of verses is more versatile and reminds us of a categorical value rather than a numeric one.<\/span><\/span><\/p>\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>On the other hand, the number of choruses follows a bimodal distribution, and it is hard not to wonder if there is any connection between the bimodal distribution of the number of views and this distribution.<\/span><\/p>","bc095e77":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Total Error And Heteroscedasticity Evaluation<\/h3>\n","44b74456":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Numeric Feature Analysis<\/h3>\n","fd3b0ff4":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>An interesting observation appears when we look into each album&apos;s mean values; when looking at the average value of choruses and verses in each album, we can see that in Drake&apos;s earlier years, the ratio between verses and choruses was opposite. Still, all the albums released between 2010 and 2017 share almost an identical ratio; what&apos;s even more fascinating is that it is clear that after 2017 Drake&apos;s albums resemble the ones in his early career stages.<\/span><\/p>\n<p><br><\/p>","be1e96ac":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:180%;text-align:center;border-radius: 15px 50px;\">Album Based Analysis<\/h3>\n","54b71e67":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Label Based Analysis<\/h3>\n"}}