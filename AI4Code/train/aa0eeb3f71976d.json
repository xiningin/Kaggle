{"cell_type":{"d5f7a8b6":"code","274506c1":"code","e5ccc717":"code","6ab98101":"code","dabe7743":"code","f740d395":"code","8700182f":"code","657584df":"code","35a5b1b7":"code","e560c44e":"code","5ee68d7a":"code","6ccfc584":"code","ffda1473":"code","06df7241":"code","3f420996":"code","4260f0af":"code","07148929":"code","bc13c84f":"code","775e8c2e":"code","18e9c3c5":"code","21dd7d94":"code","a100ab35":"code","e2129147":"code","9945fa0a":"code","7bcf8492":"code","f36a7e93":"code","4d1413b5":"code","dd67d6d7":"code","3c416fbf":"code","2fee215b":"code","e7a83334":"code","6862618f":"code","cfc9ed2e":"code","0f8b1b8a":"code","15417da3":"code","f3e7ccb7":"code","9bf46d61":"code","c2b1d02e":"code","7def4bed":"code","582a2f73":"code","a4dbae4f":"code","f312684f":"markdown","2bf69481":"markdown","a6a490e5":"markdown","4bbc28c9":"markdown","7a5d261f":"markdown","718b9038":"markdown","ac308a23":"markdown","f36926c8":"markdown","79c35c6d":"markdown","5be69129":"markdown","834d6e98":"markdown","35b435be":"markdown","2c4a921f":"markdown","6d124512":"markdown","21320740":"markdown","d5d60e74":"markdown","2f093eb0":"markdown","dac63e54":"markdown","4aa57724":"markdown","d0382a0c":"markdown","7526c0c5":"markdown","bfcf6421":"markdown","5bdd5472":"markdown","15dceb12":"markdown","15379171":"markdown","89ebe872":"markdown","3133a101":"markdown","2976f9f8":"markdown","a688e282":"markdown","0f6c2c35":"markdown","7e070a82":"markdown","38c55ed8":"markdown","ca0b7314":"markdown","455d1f8a":"markdown","b7afb513":"markdown","e90bc241":"markdown","8732bff8":"markdown","4fae89ae":"markdown","8f3067fe":"markdown","9bc2355a":"markdown","b3639e5c":"markdown","51168da8":"markdown","8c083c8d":"markdown","c6c77b8a":"markdown","06b28b65":"markdown","c19ce372":"markdown","42c75b3a":"markdown","825d0d35":"markdown","50392a9e":"markdown","874e2e5c":"markdown","5e61e7e3":"markdown","52e610dc":"markdown","df927bae":"markdown","85a34d56":"markdown","dcd9f684":"markdown","6733dbcf":"markdown","deac4e91":"markdown","599831b9":"markdown","7cd6af3e":"markdown","d9e77695":"markdown","dad65054":"markdown","b5054db6":"markdown","9c407eff":"markdown","6dbdb196":"markdown","b1307378":"markdown","d1c49c33":"markdown","b0602a37":"markdown"},"source":{"d5f7a8b6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder,StandardScaler,PowerTransformer, MinMaxScaler, RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_predict, train_test_split,GridSearchCV,cross_val_score\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge,ElasticNet\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\n\n\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.feature_selection import SelectKBest,SelectPercentile,f_classif,f_regression,mutual_info_regression,mutual_info_classif,SelectFromModel,RFE\n\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\n\nimport optuna\n\n#importing plotly and cufflinks in offline mode\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\nimport warnings\nwarnings.filterwarnings('ignore')","274506c1":"pd.set_option('max_columns',100)\npd.set_option('max_rows',900)\n\npd.set_option('max_colwidth',200)\n\ndf = pd.read_csv('..\/input\/car-price-prediction\/CarPrice_Assignment.csv')\ndf.head()","e5ccc717":"df.info()","6ab98101":"df.duplicated().sum()","dabe7743":"def missing (df):\n    missing_number = df.isnull().sum().sort_values(ascending=False)\n    missing_percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])\n    return missing_values\n\nmissing(df)","f740d395":"df.nunique()","8700182f":"df1= df.copy()","657584df":"df1['CarName'].sample(5)","35a5b1b7":"df1['CarName'].unique()","e560c44e":"df1['model'] = [x.split()[0] for x in df1['CarName']]\ndf1['model'] = df1['model'].replace({'maxda': 'Mazda','mazda': 'Mazda', \n                                     'nissan': 'Nissan', \n                                     'porcshce': 'Porsche','porsche':'Porsche', \n                                     'toyouta': 'Toyota', 'toyota':'Toyota',\n                            'vokswagen': 'Volkswagen', 'vw': 'Volkswagen', 'volkswagen':'Volkswagen'})\n","5ee68d7a":"df1= df1.drop(['car_ID','CarName'], axis=1)","6ccfc584":"print (f' We have {df1.shape[0]} instances with the {df1.shape[1]-1} features and 1 output variable')","ffda1473":"numerical= df1.drop(['price'], axis=1).select_dtypes('number').columns\n\ncategorical = df1.select_dtypes('object').columns\n\nprint(f'Numerical Columns:  {df1[numerical].columns}')\nprint('\\n')\nprint(f'Categorical Columns: {df1[categorical].columns}')","06df7241":"df1['price'].describe()","3f420996":"print( f\"Skewness: {df1['price'].skew()}\")","4260f0af":"df1['price'].iplot(kind='hist')","07148929":"df1[numerical].describe()","bc13c84f":"df1[numerical].iplot(kind='hist');","775e8c2e":"df1[numerical].iplot(kind='histogram',subplots=True,bins=50)\n","18e9c3c5":"skew_limit = 0.75 # This is our threshold-limit to evaluate skewness. Overall below abs(1) seems acceptable for the linear models. \nskew_vals = df1[numerical].skew()\nskew_cols= skew_vals[abs(skew_vals)> skew_limit].sort_values(ascending=False)\nskew_cols","21dd7d94":"df1[skew_cols.index].iplot(kind='hist');","a100ab35":"df1[skew_cols.index].iplot(kind='histogram',subplots=True,bins=50)\n","e2129147":"df_try = df1.copy()\n\nfor col in skew_cols.index.values:\n    df_try[col] = df_try[col].apply(np.log1p)\n\nprint(df_try[skew_cols.index].skew())\nprint()\n\ndf_try[skew_cols.index].iplot(kind='histogram',subplots=True,bins=50);","9945fa0a":"df_trans = df1[skew_cols.index].copy()\npt = PowerTransformer(method='yeo-johnson')\ntrans= pt.fit_transform(df_trans)\ndf_trans = pd.DataFrame(trans, columns =skew_cols.index )\nprint(df_trans.skew())\nprint()\ndf_trans.iplot(kind='histogram',subplots=True,bins=50);","7bcf8492":"numerical1= df1.select_dtypes('number').columns\n\n\nmatrix = np.triu(df1[numerical1].corr())\nfig, ax = plt.subplots(figsize=(14,10)) \nsns.heatmap (df1[numerical1].corr(), annot=True, fmt= '.2f', vmin=-1, vmax=1, center=0, cmap='coolwarm',mask=matrix, ax=ax);","f36a7e93":"df1 = df1.drop('citympg',axis=1)","4d1413b5":"df1[categorical].head()","dd67d6d7":"print(df1.groupby('fueltype')['price'].mean().sort_values())\nprint()\ndf1.groupby('fueltype')['price'].mean().iplot(kind='histogram',subplots=True,bins=50)","3c416fbf":"print(df1.groupby('aspiration')['price'].mean().sort_values())\nprint()\ndf1.groupby('aspiration')['price'].mean().iplot(kind='histogram',subplots=True,bins=50)","2fee215b":"print(df1.groupby('carbody')['price'].mean().sort_values())\nprint()\ndf1.groupby('carbody')['price'].mean().iplot(kind='histogram',subplots=True,bins=50)","e7a83334":"print(df1.groupby('drivewheel')['price'].mean().sort_values())\nprint()\ndf1.groupby('drivewheel')['price'].mean().iplot(kind='histogram',subplots=True,bins=50)","6862618f":"print(df1.groupby('enginelocation')['price'].mean().sort_values())\nprint()\ndf1.groupby('enginelocation')['price'].mean().iplot(kind='histogram',subplots=True,bins=50)","cfc9ed2e":"print(df1.groupby('enginetype')['price'].mean().sort_values())\nprint()\ndf1.groupby('enginetype')['price'].mean().iplot(kind='histogram',subplots=True,bins=50)","0f8b1b8a":"print(df1.groupby('fuelsystem')['price'].mean().sort_values())\nprint()\ndf1.groupby('fuelsystem')['price'].mean().iplot(kind='histogram',subplots=True,bins=50)","15417da3":"print(df1.groupby('model')['price'].mean().sort_values())\nprint()\ndf1.groupby('model')['price'].mean().iplot(kind='histogram',subplots=True,bins=50)","f3e7ccb7":"df2 = pd.get_dummies(df1, columns=categorical, drop_first=True)\ndf2.head()","9bf46d61":"X= df2.drop('price', axis=1)\ny= df2['price']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nmodel = LinearRegression()\n\n\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nprint (f'model : {model} and  rmse score is : {np.sqrt(mean_squared_error(y_test, y_pred))}, r2 score is {r2_score(y_test, y_pred)}')\n\n\n","c2b1d02e":"rmse_test =[]\nr2_test =[]\nmodel_names =[]\n\nnumerical2= df2.drop(['price'], axis=1).select_dtypes('number').columns\n\nX= df2.drop('price', axis=1)\ny= df2['price']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\ns = StandardScaler()\np= PowerTransformer(method='yeo-johnson', standardize=True)\n\nrr = Ridge()\nlas = Lasso()\nel= ElasticNet()\nknn = KNeighborsRegressor()\n\nmodels = [rr,las,el,knn]\n\nfor model in models:\n    ct = make_column_transformer((s,numerical2),(p,skew_cols.index),remainder='passthrough')  \n    pipe = make_pipeline(ct, model)\n    pipe.fit(X_train, y_train)\n    y_pred = pipe.predict(X_test)\n    rmse_test.append(round(np.sqrt(mean_squared_error(y_test, y_pred)),2))\n    r2_test.append(round(r2_score(y_test, y_pred),2))\n    print (f'model : {model} and  rmse score is : {round(np.sqrt(mean_squared_error(y_test, y_pred)),2)}, r2 score is {round(r2_score(y_test, y_pred),2)}')\n\nmodel_names = ['Ridge','Lasso','ElasticNet','KNeighbors']\nresult_df = pd.DataFrame({'RMSE':rmse_test,'R2_Test':r2_test}, index=model_names)\nresult_df","7def4bed":"rmse_test =[]\nr2_test =[]\nmodel_names =[]\n\n\n\nX= df2.drop('price', axis=1)\ny= df2['price']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n\nrf = RandomForestRegressor(random_state=42)\ngb = GradientBoostingRegressor(random_state=42)\net= ExtraTreesRegressor(random_state=42)\nxgb = XGBRegressor(random_state=42)\n\nmodels = [rf,gb,et,xgb]\n\nfor model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    rmse_test.append(round(np.sqrt(mean_squared_error(y_test, y_pred)),2))\n    r2_test.append(round(r2_score(y_test, y_pred),2))\n    print (f'model : {model} and  rmse score is : {round(np.sqrt(mean_squared_error(y_test, y_pred)),2)}, r2 score is {round(r2_score(y_test, y_pred),4)}')\n\nmodel_names = ['RandomForest','GradientBoost','ExtraTree','XGB']\nresult_df = pd.DataFrame({'RMSE':rmse_test,'R2_Test':r2_test}, index=model_names)\nresult_df","582a2f73":"X= df2.drop('price', axis=1)\ny= df2['price']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nrf = RandomForestRegressor(n_estimators= 220, random_state=42 )\n\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\nprint (f' rmse score is : {round(np.sqrt(mean_squared_error(y_test, y_pred)),4)}, r2 score is {round(r2_score(y_test, y_pred),4)}')\n\n\n","a4dbae4f":"importances = rf.feature_importances_\nfeature_names = [f'feature {i}' for i in range(X.shape[1])]\n\n# what are scores for the features\nfor i in range(len(rf.feature_importances_)):\n    if rf.feature_importances_[i] >0.001:\n        print(f'{X_train.columns[i]} : {round(rf.feature_importances_[i],3)}')\n\nprint()\n\nplt.bar([X_train.columns[i] for i in range(len(rf.feature_importances_))], rf.feature_importances_)\nplt.xticks(rotation=90)\nplt.rcParams[\"figure.figsize\"] = (24,12)\nplt.show()","f312684f":"### Drivewheel & Price","2bf69481":"### With Power Transformer","a6a490e5":"### Get the Dummies","4bbc28c9":"- Based on the Random Forest Regressor:\n   - **enginesize**\n   - **curbweight**\n   - **highway mpg**\n   - **horse power**\n- have biggest importance scores.\n","7a5d261f":"gif credit: https:\/\/giphy.com\/gifs\/girl-life-car-3ov9jWu7BuHufyLs7m","718b9038":"- We have 9 numerical features which have more than .5 correlation with the price variable.\n- Which is a good sign for the prediction capability of the model, but still we need to see in the practice.\n- From the threshold .9 perspective: Highwaympg and citympg has .97 correlation. We can drop one of them to avoid multicollinearity problems for the linear models.\n- I have observed several highly correlated features below the .9 level.","ac308a23":"- Since this is a beginner friendly notebook, let's see several options in the practice without touching our main dataset.\n- First I'll show np.log method.\n- Then I'll use transformation methods.","f36926c8":"### Engine location & Price","79c35c6d":"<a id=\"toc\"><\/a>\n\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Table of Contents<\/h3>\n    \n* [Data](#0)\n* [What Problem We Have and Which Metric to Use?](#1)\n\n* [Exploratory Data Analysis](#2)\n    * [Target Variable](#3)\n    * [Numerical Features](#4)\n    * [Categorical Features](#5)    \n    \n* [Model Selection](#6)    \n    * [Baseline Model](#7)\n    * [Models with Ridge & Lasso & ElasticNet and KNN](#8)\n    * [Models with Random Forest & Extra Trees & Gradient Boosting & XGBoost](#9)    \n    * [Best Model with Hyperparameter Tuning](#10)\n    * [Feature Importance](#11)    \n\n\n* [Conclusion](#12)\n\n* [References & Further Reading](#13)\n","5be69129":"- Overall data types seems ok. ","834d6e98":"- Based on the data and data dictionary, We have prediction \/ regression problem.\n- We wil make prediction on the target variable **PRICE**\n- And we will build a model to get best prediction on the price variable.\n- For that we will use RMSE(Root Mean Squared Error) and R2\n- [For the detailed info about the evaluation metrics](https:\/\/www.kaggle.com\/kaanboke\/the-most-common-evaluation-metrics-a-gentle-intro)","35b435be":"- With hyperparameter tuning we got a lift. \n- RMSE (from 1984.44 to 1975.8483)\n- R2 (from .9432 to .9437)","2c4a921f":"<a id=\"10\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Best Model with the Hyperparameter Tuning<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","6d124512":"<a id=\"0\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>Data<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","21320740":"- Let's make a copy of the dataset and start to work on it.","d5d60e74":"### With np.log","2f093eb0":"- Now we are talking.\n- Random forest, without any tuning got .94 R2 and lowest RMSE.\n- XGBoost also did a good job without any optimization \/ tuning.","dac63e54":"Turbo aspiration is more expensive than standard aspiration","4aa57724":"<a id=\"6\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>MODEL SELECTION<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","d0382a0c":"### Fuel system & Price","7526c0c5":"- We have developed model to predict car price problem.\n\n- First, we  made the detailed exploratory analysis.\n- We have decided which metric to use.\n- We analyzed both target and features in detail.\n- We transform categorical variables into numeric so we can use them in the model.\n- We transform numerical variables to reduce skewness and get close to normal  distribution.\n- We use pipeline to avoid data leakage.\n- We looked at the results of the each model and selected the best one for the problem in hand.\n- We made hyperparameter tuning of the best model see the improvement\n- We looked at the feature importance.\n\n\n\n- After this point it is up to you to develop and improve the models.  **Enjoy** \ud83e\udd18","bfcf6421":"- During the modelling process, we can use power transformer.","5bdd5472":"<a id=\"13\"><\/a>\n<font color=\"darkblue\" size=+1.5><b>References & Further Reading<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n\n\n[Machine Learning - Beginner &Intermediate-Friendly BOOKS](https:\/\/www.kaggle.com\/general\/255972)","15dceb12":"- Diesel cars are more expensive than cars with gas.","15379171":"### Aspiration & Price","89ebe872":"gif credit: https:\/\/giphy.com\/","3133a101":"- Based on the model, Porsche, Buick and Jaguar are the most expensive ones.\n- Chevroletis the least expensive model.","2976f9f8":"<a id=\"8\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Ridge &  Lasso  &  Elasticnet  &  KNN with Scaler and Transformer<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","a688e282":"<a id=\"7\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Baseline Model<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","0f6c2c35":"# Car Price Prediction Data","7e070a82":"- Based on the price, there are differences among the carbody.\n- While Wagon cars the leats expensive ones, hardtop and the convertibles are the most expensive ones.","38c55ed8":"![](https:\/\/media.giphy.com\/media\/3jVT4U5bilspG\/giphy.gif?cid=ecf05e47ijcazulbfateoqyazwbckrtfakr1olt4krmdycsd&rid=giphy.gif&ct=g)","ca0b7314":"- There is a quite difference based on the engine location. Rear engine location almost 3 times expensive than front ones.","455d1f8a":"### Model & Price","b7afb513":"- No missing values and no duplicates. Hurray!!!","e90bc241":"### Carbody & Price","8732bff8":"- I'll use linear regression model as a base model\n- And then I will use Ridge, Lasso, Elasticnet, KNeighborsRegressor and Support Vector MAchine Regressor\n- And then i will use ensemble models, like Randomforest, Gradient Boosting and Extra Trees\n-  Finally I will look at the XGBoost Regresson.\n- And after evaluating the algorithm, we will select our best model.\n- Let's start.","4fae89ae":"<a id=\"11\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Feature Importance<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","8f3067fe":"<a id=\"12\"><\/a>\n<font color=\"darkblue\" size=+1.5><b>Conclusion<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>\n","9bc2355a":"- Several features have gaussian-normal like ditsribution.\n- I have also observed skewness.\n- I'll look those in details.","b3639e5c":"- By using standard scaler and power transformer for the skewness;\n- For linear models we got .92 for the R2 and\n- 2307.47 RMSE which are better scores compare to the baseline model.","51168da8":"- Let's drop the 'citympg'","8c083c8d":"DATA DICTONARY\t\t\t\t\t\t\n\t\t\t\t\t\t\n1\t**Car_ID**: \t\t\tUnique id of each observation \t\t\n2\t**Symboling**:  \t\t\tIts assigned insurance risk rating, A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe. \t\t\n3\t**carCompany**: \t\t\tName of car company \t\t\n4\t**fueltype**:\t\t\tCar fuel type i.e gas or diesel \t\t\n5\t**aspiration**:\t\t\tAspiration used in a car \t\t\n6\t**doornumber**:\t\t\tNumber of doors in a car\t\t\n7\t**carbody**:\t\t\tbody of car \t\t\n8\t**drivewheel**:\t\t\ttype of drive wheel\t\t\n9\t**enginelocation**:\t\t\tLocation of car engine\t\t\n10\t**wheelbase**:\t\t\tWeelbase of car (\t\t\n11\t**carlength**:\t\t\tLength of car \t\t\n12\t**carwidth**:\t\t\tWidth of car \t\t\n13\t**carheight**:\t\t\theight of car\t\t\n14\t**curbweight**:\t\t\tThe weight of a car without occupants or baggage. \t\t\n15\t**enginetype**:\t\t\tType of engine. \t\t\n16\t**cylindernumber**:\t\t\tcylinder placed in the car \t\t\n17\t**enginesize**:\t\t\tSize of car \t\t\n18\t**fuelsystem**:\t\t\tFuel system of car \t\t\n19\t**boreratio**:\t\t\tBoreratio of car \t\t\n20\t**stroke**:\t\t\tStroke or volume inside the engine \t\t\n21\t**compressionratio**:\t\t\tcompression ratio of car \t\t\n22\t**horsepower**:\t\t\tHorsepower \t\t\n23\t**peakrpm**:\t\t\tcar peak rpm \t\n24\t**citympg**:\t\t\tMileage in city \t\t\n25\t**highwaympg**:\t\t\tMileage on highway \t\t\n26\t**price**: \t\t\tPrice of car \t\t\n\nReference: https:\/\/www.kaggle.com\/hellbuoy\/car-price-prediction","c6c77b8a":"- Let's observe the correlation among the numerical features\n- And also observe the correlation with the target variable","06b28b65":"![](https:\/\/media.giphy.com\/media\/3ov9jWu7BuHufyLs7m\/giphy.gif?cid=ecf05e47q8liqtbxy73738g13h2ofqf9nm9q82lm3py081io&rid=giphy.gif&ct=g)","c19ce372":"- I'll use only the brands\/make not the models.\n- I have seen several typos, I'll handle those.","42c75b3a":"- Our dataset has 8 different fuel system and price changes amongs them significantly.","825d0d35":"- There is no zero variance variable. \n- Car ID column is repetition of the index. So I'll drop it.\n- Carname has 147 different entity. I'll check it. And try to find a way to reduce the variance.\n- Other than that there is no problem.","50392a9e":"<a id=\"9\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Random Forest& Gradient Boosting & Extra Trees & XGBoost<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","874e2e5c":"#### Hi all.  \ud83d\ude4b\n\n#### We continue our **Beginner-Intermediate Friendly Machine Learning series**, which would help anyone who wants to learn or refresh the basics of ML.\n\n#### What we have covered: \n\n#### [Beginner Friendly Detailed Explained EDAs \u2013 For anyone at the beginnings of DS\/ML journey](https:\/\/www.kaggle.com\/general\/253911#1393015) \u2714\ufe0f\n\n#### [BIAS & VARIANCE TRADEOFF](https:\/\/www.kaggle.com\/kaanboke\/ml-basics-bias-variance-tradeoff) \u2714\ufe0f\n\n#### [LINEAR ALGORITHMS](https:\/\/www.kaggle.com\/kaanboke\/ml-basics-linear-algorithms)  \u2714\ufe0f\n\n#### [NONLINEAR ALGORITHMS](https:\/\/www.kaggle.com\/kaanboke\/nonlinear-algorithms)  \u2714\ufe0f\n\n#### [The Most Used Methods to Deal with MISSING VALUES](https:\/\/www.kaggle.com\/kaanboke\/the-most-used-methods-to-deal-with-missing-values)  \u2714\ufe0f\n\n#### [Beginner Friendly End to End ML Project- Classification with Imbalanced Data](https:\/\/www.kaggle.com\/kaanboke\/beginner-friendly-end-to-end-ml-project-enjoy)  \u2714\ufe0f\n\n#### [How to Prevent the Data Leakage ?](https:\/\/www.kaggle.com\/kaanboke\/how-to-prevent-the-data-leakage) \u2714\ufe0f\n\n#### [The Most Common EVALUATION METRICS- A Gentle Intro](https:\/\/www.kaggle.com\/kaanboke\/the-most-common-evaluation-metrics-a-gentle-intro) \u2714\ufe0f\n\n#### [Feature Selection-The Most Common Methods to Know](https:\/\/www.kaggle.com\/kaanboke\/feature-selection-the-most-common-methods-to-know) \u2714\ufe0f\n\n\n#### In this notebook we will  implement **End to End Prediction Model** by using different ML algorithms**\n#### Enjoy \ud83e\udd18","5e61e7e3":"### Fuel Type & Price","52e610dc":"- Let's drop the 'model' and 'carid' columns","df927bae":"- Let's see the numerical features","85a34d56":"#### By the way, when you like the topic, you can show it by supporting \ud83d\udc4d\n\n####  **Feel free to leave a comment**. \n\n#### All the best \ud83e\udd18","dcd9f684":"<a id=\"5\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Categorical Features<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","6733dbcf":"- Even though target variable has right skewness, I will not make any transformation on it.\n","deac4e91":"- It is important to note that Random Forest Regressor gave importance score bigger than 0 to 16 features.\n- Model used 16 out of 63 features to get best prediction.\n- For [For deatiled discussion on the Feature Selection-The Most Common Methods to Know](https:\/\/www.kaggle.com\/kaanboke\/feature-selection-the-most-common-methods-to-know)","599831b9":"-Before moving forward, I'll handle the 'carname'","7cd6af3e":"- Rear wheel drive cars are the most expensive ones.  Front wheel cars the least expensive ones.","d9e77695":"<a id=\"3\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Target Variable<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","dad65054":"<a id=\"4\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>Numerical Features<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","b5054db6":"- Let's import the libraries","9c407eff":"### Engine type & Price","6dbdb196":"- Baseline Model,in our case, Linear Regression model, without and scaling and transformation did a quite a good job.","b1307378":"- Our dataset has 7 different engine types and price changes amongs them significantly.","d1c49c33":"<a id=\"2\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>Exploratory Data Analysis<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>","b0602a37":"<a id=\"1\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>What Problem We Have and Which Metric to Use?<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Table of Contents<\/a>"}}