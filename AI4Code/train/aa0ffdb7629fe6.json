{"cell_type":{"c01d2b0b":"code","284780c7":"code","7e1f7b5a":"code","89b78339":"code","79b96910":"code","1be45040":"code","1090c21f":"code","ddb83f42":"code","4e9086a4":"code","a7315eb4":"code","35e4e308":"code","20b692f9":"code","bbf07564":"code","88e072a6":"code","89ef297c":"code","00b4aef4":"code","b2c6516d":"code","3c6ac46b":"code","b1ef1782":"code","b993dc96":"code","2f1c8f34":"code","d0f05ded":"code","aa6ca214":"code","fb3dfa63":"code","b6a17715":"code","750264ae":"code","a24d412e":"code","55a48e67":"markdown","b1c0e735":"markdown","1dfc359e":"markdown","e7ef6af7":"markdown","9c51fd0d":"markdown","3fe6eb3f":"markdown","2544fc9b":"markdown","9c324d5a":"markdown","acc23969":"markdown","62c662d9":"markdown","358c459b":"markdown","17e093fd":"markdown","3e0045ce":"markdown","f03d8fd9":"markdown"},"source":{"c01d2b0b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","284780c7":"import pandas as pd\nimport numpy as np","7e1f7b5a":"train_path = \"\/kaggle\/input\/twitter-sentiment-analysis-hatred-speech\/train.csv\"\ntrain_data = pd.read_csv(train_path)","89b78339":"train_data.head(3)","79b96910":"train_data = train_data.drop('id',axis=1)","1be45040":"size = train_data.shape[0]\nprint(size)","1090c21f":"import seaborn as sns\nsns.countplot(train_data['label'])","ddb83f42":"import nltk\nimport re\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer","4e9086a4":"def prepare_corpus(tweets):\n  corpus_tweets = []\n  size = tweets.shape[0]\n  ps = PorterStemmer()\n  for i in range(0,size):\n    tweet = re.sub(pattern='[^a-zA-Z]',repl=' ', string=tweets['tweet'][i])\n\n    tweet = re.sub(pattern='user' , repl='' , string = tweet)\n\n    tweet = tweet.lower()\n\n    words = tweet.split()\n\n    words = [ps.stem(word) for word in words if not word in stopwords.words('english')]\n\n    tweet = ' '.join(words)\n\n    corpus_tweets.append(tweet)\n  return corpus_tweets\n\ncorpus_tweets_train = prepare_corpus(train_data)","a7315eb4":"corpus_tweets_train[0:2]","35e4e308":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(max_features=7000)\nX_tfidf = tfidf.fit_transform(corpus_tweets_train).toarray()\ny_ifidf = train_data['label'].values","20b692f9":"X_tfidf[0:2]","bbf07564":"from sklearn.model_selection import train_test_split\ndef split_train_test(X,y):\n  X_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.20)\n  return X_train , X_test , y_train , y_test\n\nX_train_idf , X_test_idf , y_train_idf , y_test_idf = split_train_test(X_tfidf, y_ifidf)","88e072a6":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\ndef accuracy_check(model,data,label):\n  y_pred = model.predict(data)\n  print(classification_report(label , y_pred)) \n  accuracy = accuracy_score(label , y_pred)\n  return accuracy","89ef297c":"from sklearn.naive_bayes import MultinomialNB\nnb_idf = MultinomialNB()\nnb_idf.fit(X_train_idf , y_train_idf)\nnb_idf_accuracy = accuracy_check(nb_idf , X_test_idf , y_test_idf)\nprint(nb_idf_accuracy)","00b4aef4":"def optimization_idf(X_train_idf , X_test_idf , y_train_idf , y_test_idf):\n  best_accuracy = 0.0\n  alpha_val = 0.0\n  for i in np.arange(0.1,1.1,0.1):\n    temp_classifier = MultinomialNB(alpha=i)\n    temp_classifier.fit(X_train_idf, y_train_idf)\n    temp_y_pred = temp_classifier.predict(X_test_idf)\n    score = accuracy_score(y_test_idf, temp_y_pred)\n    print(\"Accuracy score for alpha={} is: {}%\".format(round(i,1), round(score*100,2)))\n    if score>best_accuracy:\n      best_accuracy = score\n      alpha_val = i\n  print('The best accuracy is {}% with alpha value as {}'.format(round(best_accuracy*100, 2), round(alpha_val,1)))\n  return alpha_val\n\noptimal_value_idf = optimization_idf(X_train_idf , X_test_idf , y_train_idf , y_test_idf)","b2c6516d":"ml_model_final = MultinomialNB(alpha = 0.1)\nml_model_final.fit(X_tfidf , y_ifidf)","3c6ac46b":"test_path = \"\/kaggle\/input\/twitter-sentiment-analysis-hatred-speech\/test.csv\"\ntest_data = pd.read_csv(test_path)","b1ef1782":"test_data.head(3)","b993dc96":"corpus_test = prepare_corpus(test_data)\nvectors = tfidf.transform(corpus_test).toarray()","2f1c8f34":"answer = ml_model_final.predict(vectors)","d0f05ded":"submission = test_data\nsubmission.head(3)","aa6ca214":"submission['Predicted Labels'] = answer","fb3dfa63":"submission.head()","b6a17715":"ones = [ans for ans in answer if ans==1]\nlen(ones)","750264ae":"import seaborn as sns\nsns.countplot(submission['Predicted Labels'])","a24d412e":"submission.to_csv('submission.csv' , index=False)","55a48e67":"### LIBRARIES FOR PREPROCESSING TEXT","b1c0e735":"#### A FUNCTION TO KNOW APPROPRIATE VALUE OF ALPHA(Hyperparameter)","1dfc359e":"### FINAL CHECKS","e7ef6af7":"### ACCURACY AND CLASSIFICATION REPORT FUNCTION","9c51fd0d":"### PANDAS TO READ CSV FILE NUMPY FOR ANY USE CASE","3fe6eb3f":"<br>The smae function use to convert into corpus . \n<br>The tfidf defined previously used for transformation . ","2544fc9b":"### DROPING AXIS\nThe id coloumn is no use to me so i am going to drop it.","9c324d5a":"### MAKING PREDICTIONS FOR THE TEST DATA","acc23969":"### PREPRAING CORPUS OUT OF THE TEXT\nI created a function corpus_tweets to create a corpus of sentences containing lametized words . \n<br> Creating the function would ease the work further .\n<br> The corpus will exactly have the tweets converted into list of tweets where the words are lametized. ","62c662d9":"##### HIT A UPVOTE IF YOU LIKED OR IT HELPED YOU . \nMy Special thanks to Krish Naik Sir for his youtube tutorials and His NLP playlist for awesome content .","358c459b":"Lets Know the size of our training","17e093fd":"### TFIDF Vectorizer\nIts is required to convert the corpus into meanigful sum of numbers . \n<br>TFIDF vectorizer performs well text preprocessing than count vectorizer . ","3e0045ce":"### MODEL PERFORMANCE\nThe accuracy hit is nearly 96% on the test data . \n<br> For the final model the complete data could be provided . ","f03d8fd9":"### SPLITTING THE DATA FOR TRAIN AND TEST\nUsing the train test split X and y are splitted to 80:20 ratio . "}}