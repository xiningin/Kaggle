{"cell_type":{"6e0fc0fc":"code","e03b6064":"code","aad78727":"code","0f5c78c5":"code","31590ecf":"code","a14fb219":"code","e6031114":"code","67377293":"code","73f77720":"code","fef55699":"code","18cfe4c6":"code","acd51006":"code","170a0694":"code","2e18cef6":"code","63945537":"code","58179ec7":"code","91c595ff":"code","5c462ef1":"code","352bc8b4":"code","42128a72":"code","ddac9753":"code","cfb63158":"code","39fd93a8":"code","48387f1d":"code","95f2ca2a":"code","7158fb10":"code","c4e60b4a":"markdown","dc23035a":"markdown","a83b0d2a":"markdown","6b98e6a4":"markdown","76da6b0a":"markdown","b51e07e6":"markdown","7bc816cf":"markdown"},"source":{"6e0fc0fc":"import os\nimport random\nimport cv2\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport time\nfrom PIL import Image\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import RandomSampler\nfrom torchvision import transforms, models\nfrom shutil import copy, rmtree\n\nSEED = 42\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\nseed_everything(SEED)","e03b6064":"num_classes = 107\nbatch_size = 10\n\noriginal_train_images_path = '\/kaggle\/input\/classification-with-limited-data\/food\/train\/images'\ntrain_images_path = '\/kaggle\/working\/train\/images'\nvalid_images_path = '\/kaggle\/working\/val\/images'\ntrain_labels_path = '\/kaggle\/input\/classification-with-limited-data\/food\/train\/train.csv'\ntest_images_path = '\/kaggle\/input\/classification-with-limited-data\/food\/test\/images'\nmodel_path = '\/models'\nsubmission_path = '\/submission'\n\n# is filled with good results (conf > 0.95) of the test set after the code is run\ntrain_images_path_ext = '\/kaggle\/working\/train_ext\/images'\n\n# load labels from csv\nlbls_df = pd.read_csv(train_labels_path)\n\npaths = [train_images_path, train_images_path_ext, valid_images_path, model_path, submission_path]\nfor path in paths:\n    if os.path.isdir(path):\n        rmtree(path)\n    os.makedirs(path)\n    \n# create validation data in corresponding folder (is only created once)\nfile_list = os.listdir(original_train_images_path)\n\n#valid_ct = int(len(file_list) * 0.1)\n# since we have no overfitting and achieve a good accuracy after 15 epochs, I will remove the validation set now. Set to one to avoid problems with validation code\nvalid_ct = 1\n\nif len(file_list) == 1070:\n    valid_list = random.sample(file_list, valid_ct)\n    for file in file_list:\n        path = ''\n        if file in valid_list:\n            path = valid_images_path\n        else:\n            path = train_images_path\n        \n        copy(os.path.join(original_train_images_path, file), os.path.join(path, file))","aad78727":"classes = sorted(lbls_df['Expected'].unique())\ncols = 5\nrows = len(classes) \/\/ cols + 1\n\nplt.figure(figsize=(3 * cols, 3 * rows))\nfor i, class_ in enumerate(classes):\n    files = lbls_df[lbls_df['Expected'] == class_]['Id'].values\n    file = random.choice(files)\n    img = cv2.imread(os.path.join(train_images_path, file))\n    if img is None:\n        img = cv2.imread(os.path.join(valid_images_path, file))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    plt.subplot(rows, cols, i + 1)\n    plt.imshow(img)\n    plt.axis('off')\n    plt.title(class_)\nplt.show()","0f5c78c5":"class FoodDataset(Dataset):\n    def __init__(self, path, labels_df, transform=None):\n        # load file names and the according labels\n        self.path = path\n        self.files = list(sorted(os.listdir(os.path.join(path))))\n        self.lbls = [labels_df[labels_df['Id'] == file]['Expected'].values[0] for file in self.files]\n        self.lbls = torch.LongTensor(np.array(self.lbls))\n        self.transform = transform\n\n    def __getitem__(self, index):\n        # load image and label\n        img = cv2.imread(os.path.join(self.path, self.files[index]))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n        lbl = self.lbls[index]\n\n        # apply transform\n        if self.transform:\n            # Convert numpy array to PIL image, including conversion from 0-255 to 0-1\n            img = Image.fromarray(img.astype(np.uint8))\n            # Apply transform\n            img = self.transform(img)\n\n        return img, lbl\n\n    def __len__(self):\n        return len(self.files)\n","31590ecf":"# define transforms\ntransform_train = transforms.Compose([transforms.RandomHorizontalFlip(0.25),\n                                      #transforms.RandomApply(\n                                      transforms.RandomRotation(180),\n                                      transforms.RandomHorizontalFlip(0.25),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                                     ])\n\ntransform_val = transforms.Compose([transforms.ToTensor(),\n                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n                                   ])\n\n# create datsets\ntrain_dataset = FoodDataset(train_images_path, lbls_df, transform=transform_train)\nvalid_dataset = FoodDataset(valid_images_path, lbls_df, transform=transform_val)\n\n# create dataloaders (apply batchsize and random sampling)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=RandomSampler(train_dataset))\nvalid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n\n# deactivate use of extended data (done after training)\nselect_ext = False","a14fb219":"# print an example for the data that is input into the network\nidx = random.randint(0, len(train_dataset))\nimages, labels = train_dataset[idx]\n\nplt.imshow(images.numpy().transpose(2,1,0))\nprint(labels.numpy())","e6031114":"import torch.nn as nn\n\n\n#net = models.resnext50_32x4d(pretrained=True)\nnet = models.resnext101_32x8d(pretrained=True)\n\nin_features = net.fc.in_features\nnet.fc = nn.Linear(in_features, num_classes)\n\n# For Classification dropout shouldn't be applied on the last layer but inbetween the net! \n# If you want to use dropout, modify this.\n#net.fc = nn.Sequential(\n#    nn.Dropout(0.5),\n#    nn.Linear(2048, num_classes)\n#)\n\n# Use GPU if available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nnet.to(device)","67377293":"# Load saved model\n#net.load_state_dict(torch.load(os.path.join(model_path, 'best_model.bin')))","73f77720":"# Configure loss, optimizer and learning rate scheduler\nimport torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.005, momentum=0.9)\n#optimizer = optim.AdamW(net.parameters(), lr=0.001)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.2, patience=1, verbose=True, min_lr=1e-8)","fef55699":"# Start with a validation. How good is your net currently?\nvalid_loss = 0.0\nk = 0\n\nfor data in valid_loader:\n    with torch.no_grad():\n        inputs, labels = data\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        valid_loss += loss.item()\n\n        for a in range(len(labels)):\n            prob = outputs.detach().cpu().numpy()[a]\n            predictedClass = np.argmax(prob)\n            if predictedClass == labels[a].item():\n                k += 1\n                \n\nprint('Current validation loss is: %0.3f You got %d of %d examples correctly classified.' % \n      (valid_loss\/len(valid_loader), k, len(valid_dataset)))","18cfe4c6":"# Initialize empty losses\nloss_train = []\nloss_val = []\nacc_train = []\nacc_val = []","acd51006":"# Amount of epochs\nn_epochs = 50\n\n# Extended Dataset selected? (can be disregarded for the first training)\nif select_ext:\n    train_loader_sel = train_loader_ext\n    train_dataset_sel = train_dataset_ext\n    if not train_loader_sel or not train_dataset_sel:\n        assert False\nelse:\n    train_loader_sel = train_loader\n    train_dataset_sel = train_dataset\n\n# initialize best model loss\nif valid_loss:\n    best_model_loss = valid_loss\n    last_best_model_loss = valid_loss + 1\nelse:\n    best_model_loss = float(\"inf\")\n    last_best_model_loss = float(\"inf\")\n\n# initialize some more variables for output display\nt = time.time()\nlen_train = len(train_loader_sel)\n\n# Set update interval of the Output. Increase the number in the end to decrease the update intervall\nupdate_interval = round(len_train \/ 10)\n\n# Start training!\nfor epoch in range(n_epochs):\n\n    net.train()\n    epoch_loss = 0.0\n    l = 0\n    \n    # Repeat over batches\n    for i, data in enumerate(train_loader_sel, 0):\n        # get the inputs and put them on the GPU if available\n        inputs, labels = data\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n\n        # increase l if the prediction was correct\n        for a in range(len(labels)):\n            prob = outputs.detach().cpu().numpy()[a]\n            predictedClass = np.argmax(prob)\n            if predictedClass == labels[a].item():\n                l += 1\n        \n        # print statistics\n        if i % update_interval == update_interval-1:\n            print('[%d, %5d] Loss: %.3f Time: %.1f min' %\n                  (epoch + 1, (i + 1) * batch_size, epoch_loss \/ (i + 1), (time.time() - t) \/ 60), end='\\r')   \n    \n    # Evaluation\n    net.eval()\n    valid_loss = 0.0\n    k = 0\n    \n    # Evaluate validation batches\n    for j, data in enumerate(valid_loader, 0):\n        with torch.no_grad():\n            # get the inputs\n            inputs, labels = data\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            # get outputs and loss\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            valid_loss += loss.item()\n            \n            # check if prediction is correct\n            for a in range(len(labels)):\n                prob = outputs.detach().cpu().numpy()[a]\n                predictedClass = np.argmax(prob)\n                if predictedClass == labels[a].item():\n                    k += 1\n                    \n    # Do a scheduler step. If loss doesn't decrease for some epochs, learning rate is reduced by half.\n    if len(valid_loader) == 1:\n        valid_loss = epoch_loss \/ len(train_dataset_sel)\n\n    scheduler.step(valid_loss)\n    \n    # Append Losses\n    i = len(train_dataset_sel)\n    j = len(valid_dataset)\n    loss_train.append(epoch_loss\/i)\n    loss_val.append(valid_loss\/j)\n    acc_train.append(l\/float(i))\n    acc_val.append(k\/float(j))\n    \n    # Display Current Status\n    time_remaining = (time.time() - t) \/ (epoch + 1) * (n_epochs - epoch) \/ 60\n    print('[%d \/ %d, %5d]  Time: %.1f min Time remaining: %.1f min \\n Train Loss: %.3f Train TP: %d \/ %d \\n Valid Loss: %.3f Valid TP: %d \/ %d' % \n          (epoch + 1, n_epochs, i, (time.time() - t) \/ 60, time_remaining, epoch_loss \/ i * batch_size, l, i, valid_loss \/ j * batch_size, k, j))\n    \n    # Save model if better than best\n    if best_model_loss > valid_loss:\n        last_best_model_loss = best_model_loss\n        best_model_loss = valid_loss\n        best_model_path = os.path.join(model_path, 'best_model.bin')\n        if os.path.isfile(best_model_path): \n            os.rename(best_model_path, os.path.join(model_path, 'last_best_model.bin'))\n        torch.save(net.state_dict(), os.path.join(model_path, 'best_model.bin'))\n        print('Model with loss of %0.3f saved.' % (best_model_loss\/j * batch_size))\n\nprint('Finished Training')\n","170a0694":"# maybe add a red line into the graphs for everytime training was restarted\n\nloss_range = np.arange(0, len(loss_train))\n\n# Cutoff early epochs to allow a better y axis scaling. Set to 0 to not cutoff anything. Not that a big problem with log scaling.\ncut = 0\n\n# Display losses\nplt.figure(figsize = (14,12))\nplt.subplot(211)\nplt.title(\"Loss\")\nplt.plot(loss_range, loss_train, 'r', label='Training')\nplt.plot(loss_range, loss_val, 'g', label='Validation')\nplt.legend()\nplt.xlim(left=cut)\nplt.yscale(\"log\")\nplt.grid(True, which='both')\n\n# Display accuracy\nplt.subplot(212)\nplt.title(\"Accuracy\")\nplt.plot(loss_range, acc_train, 'r', label='Training')\nplt.plot(loss_range, acc_val, 'g', label='Validation')\nplt.legend()\nplt.xlim(left=cut)\nplt.ylim(0, 1)\nplt.grid(True, which='both')\n\nplt.show()","2e18cef6":"#net.load_state_dict(torch.load(\"models\/best_model.bin\"))\nnet.eval()\n\n# Create DF for submission data\ndf = pd.DataFrame(columns=[\"Id\", \"Expected\"])\n\n# Initialize variables\namount = len(os.listdir(test_images_path))\ni = 0\nt = time.time()\nprobs = []\n\n# Evaluate all test images\nfor file in os.listdir(test_images_path):\n    # Progress display\n    i += 1\n    if i % int((amount \/ 100)) == int((amount \/ 100) - 1):\n        print('%.1f %% Progressed. (%d of %d) Time: %.0f s Time remaining: %.0f min' %\n             (i\/amount * 100, i, amount, time.time() - t, (time.time() - t) \/ i * (amount - i) \/ 60), end='\\r')\n    \n    # Load image\n    img = cv2.imread(os.path.join(test_images_path, file))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n    \n    # Apply necessary preprocessing\n    img = transform_val(Image.fromarray(img.astype(np.uint8)))\n    img = torch.unsqueeze(img, 0)\n    img = img.to(device)\n    \n    # Get probabilities from the model\n    prob = net(img).detach().cpu().numpy()[0]\n    \n    # Get best 3 predictions with probabilities\n    prob_log = 10 ** prob\n    prob_log_norm = prob_log \/ np.sum(prob_log)\n    sorted_idx = np.argsort(prob_log_norm)[::-1]\n    preds = sorted_idx[:3]\n    pred_probs = np.around(prob_log_norm[preds], 4)\n    \n    # Save probabilities and classes\n    probs.append((file, pred_probs, preds))\n    df.loc[len(df)] = [file, preds[0]]\n\n# Update progress display\nprint('%.1f %% Progressed. (%d of %d) Time: %.0f min                        ' % \n      (i\/amount * 100, i, amount, (time.time() - t) \/ 60), end='\\r')","63945537":"# Create extended dataset with all test examples that surpass a certain threshold confidence\n# Display the mean and max probability\nprobability = 0\nprobability2 = 0\nmax_prob = 0\nmax_prob2 = 0\nmin_prob = 1\n\nfor file, prob, predClass in probs:\n    if prob[0] > max_prob:\n        max_prob = prob[0]\n    if prob[1] > max_prob2:\n        max_prob2 = prob[1]\n    if prob[0] < min_prob:\n        min_prob = prob[0]\n    probability += prob[0]\n    probability2 += prob[1]\n\nprobability \/= len(probs)\nprobability2 \/= len(probs)\n\nprint('Mean Prob: %0.2f %% Max Prob: %0.2f %% Min Prob: %0.2f %% \\nMean Prob 2.: %0.2f %% Max Prob 2.: %0.2f %%' % \n      (probability * 100, max_prob * 100, min_prob * 100, probability2 * 100, max_prob2 * 100))","58179ec7":"# Threshold intervall for confidence. Confidence over 0.99 doesn't has to be retrained, since it doesn't provide much new information\nthreshold_max = 0.99\nthreshold_min = 0.75\n\n# Create df with labels for every available image\nlbls_df_ext = lbls_df.copy()\nlbls_df_ext = lbls_df_ext.append(df, ignore_index=True)\n\n# Delete existing extended data files\nif os.path.isdir(train_images_path_ext):\n    rmtree(train_images_path_ext)\n\n# Create directory for extanded training data and insert all basic training images\nos.makedirs(train_images_path_ext)\nfor file in os.listdir(train_images_path):\n    copy(os.path.join(train_images_path, file), os.path.join(train_images_path_ext, file))\n\ni = 0\n\n# Add all images that had a prediction above threshold to the extended directory\nfor file, prob, predClass in probs:\n    if prob[0] > threshold_min and prob[0] < threshold_max: \n        copy(os.path.join(test_images_path, file), os.path.join(train_images_path_ext, file))\n        i += 1\n\n# Create dataset and data loader\ntrain_dataset_ext = FoodDataset(train_images_path_ext, lbls_df_ext, transform=transform_train)\ntrain_loader_ext = DataLoader(train_dataset_ext, batch_size=batch_size, sampler=RandomSampler(train_dataset_ext))\n\n# Activate use of extended dataset in training\nselect_ext = True\n\nprint('Added %d instances from the training data with confidence between %0.2f and %0.2f. \\n' % (i, threshold_min, threshold_max) + \n      'Feel free to use the data for training. (To do that just start training again) \\n' +\n      'If you want to change the threshold you can simply change it and rerun this code block. \\n' + \n      'It might be a good idea to start with a reduced learning rate.')","91c595ff":"# Visualize your results\n\n# 0 for lowest confidence items\n# 1 for highest confidence items\n# 2 for random confidence items\nshow_order = 0\n# amount of items shown\nshow_items = 10\n# amount of comparisons from same class shown\n# for show_order 0 half of the examples are for the second best guess\nshow_comparison = 4\n\nitems = []\n\nif show_order == 0:\n    probs.sort(key=lambda x: x[1][0])\n    items = probs[0:show_items]\nelif show_order == 1:\n    probs.sort(key=lambda x: x[1][0], reverse=True)\n    items = probs[0:show_items]\nelif show_order == 2:\n    items = random.choices(probs, k=show_items)\nelse:\n    print('Choose value between 0 and 2 for show_order.')\n    assert False\n    \nplt.figure(figsize=(3 * (show_comparison + 1), 3*show_items))\nfor i, item in enumerate(items):\n    file, probabilities, predictions = item\n    prob = probabilities[0]\n    predClass = predictions[0]\n    \n    img = cv2.imread(os.path.join(test_images_path, file))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n    plt.subplot(show_items, show_comparison + 1, i * (show_comparison + 1) + 1)\n    plt.imshow(img)\n    plt.axis('off')\n    plt.title('Class: ' + str(predClass) + ' Probability: ' + str(round(prob, 3)))\n    \n    classFiles = lbls_df[lbls_df['Expected'] == predClass]['Id'].values\n    \n    show_len = show_comparison\n    if len(classFiles) < show_comparison:\n        show_len = len(classFiles)\n        \n    if show_order == 0:\n        classFiles2 = lbls_df[lbls_df['Expected'] == predictions[1]]['Id'].values\n        classFiles[show_len\/\/2:] = classFiles2[:len(classFiles) - show_len\/\/2]\n        plt.title('Classes: ' + str(predClass) + ', ' + str(predictions[1]) + ' Probs: ' + str(round(prob*100, 2)) + ', ' + str(round(probabilities[1]*100, 2)))\n    for j in range(show_len):\n        img = cv2.imread(os.path.join(train_images_path, classFiles[j]))\n        if img is None:\n            img = cv2.imread(os.path.join(valid_images_path, classFiles[j]))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        plt.subplot(show_items, show_comparison + 1, i * (show_comparison + 1) + 2 + j)\n        plt.imshow(img)\n        plt.axis('off')\nplt.show()","5c462ef1":"# Configure loss, optimizer and learning rate scheduler\nimport torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.0005, momentum=0.9)\n#optimizer = optim.AdamW(net.parameters(), lr=0.001)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.2, patience=2, verbose=True, min_lr=1e-8)","352bc8b4":"# Amount of epochs\nn_epochs = 30\n\n# Extended Dataset selected? (can be disregarded for the first training)\nif select_ext:\n    train_loader_sel = train_loader_ext\n    train_dataset_sel = train_dataset_ext\n    if not train_loader_sel or not train_dataset_sel:\n        assert False\nelse:\n    train_loader_sel = train_loader\n    train_dataset_sel = train_dataset\n\n# initialize best model loss\nif valid_loss:\n    best_model_loss = valid_loss\n    last_best_model_loss = valid_loss + 1\nelse:\n    best_model_loss = float(\"inf\")\n    last_best_model_loss = float(\"inf\")\n\n# initialize some more variables for output display\nt = time.time()\nlen_train = len(train_loader_sel)\n\n# Set update interval of the Output. Increase the number in the end to decrease the update intervall\nupdate_interval = round(len_train \/ 10)\n\n# Start training!\nfor epoch in range(n_epochs):\n\n    net.train()\n    epoch_loss = 0.0\n    l = 0\n    \n    # Repeat over batches\n    for i, data in enumerate(train_loader_sel, 0):\n        # get the inputs and put them on the GPU if available\n        inputs, labels = data\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n\n        # increase l if the prediction was correct\n        for a in range(len(labels)):\n            prob = outputs.detach().cpu().numpy()[a]\n            predictedClass = np.argmax(prob)\n            if predictedClass == labels[a].item():\n                l += 1\n        \n        # print statistics\n        if i % update_interval == update_interval-1:\n            print('[%d, %5d] Loss: %.3f Time: %.1f min' %\n                  (epoch + 1, (i + 1) * batch_size, epoch_loss \/ (i + 1), (time.time() - t) \/ 60), end='\\r')   \n    \n    # Evaluation\n    net.eval()\n    valid_loss = 0.0\n    k = 0\n    \n    # Evaluate validation batches\n    for j, data in enumerate(valid_loader, 0):\n        with torch.no_grad():\n            # get the inputs\n            inputs, labels = data\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            # get outputs and loss\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            valid_loss += loss.item()\n            \n            # check if prediction is correct\n            for a in range(len(labels)):\n                prob = outputs.detach().cpu().numpy()[a]\n                predictedClass = np.argmax(prob)\n                if predictedClass == labels[a].item():\n                    k += 1\n                    \n    # Do a scheduler step. If loss doesn't decrease for some epochs, learning rate is reduced by half.\n    if len(valid_loader) == 1:\n        valid_loss = epoch_loss \/ len(train_dataset_sel)\n\n    scheduler.step(valid_loss)\n    \n    # Append Losses\n    i = len(train_dataset_sel)\n    j = len(valid_dataset)\n    loss_train.append(epoch_loss\/i)\n    loss_val.append(valid_loss\/j)\n    acc_train.append(l\/float(i))\n    acc_val.append(k\/float(j))\n    \n    # Display Current Status\n    time_remaining = (time.time() - t) \/ (epoch + 1) * (n_epochs - epoch) \/ 60\n    print('[%d \/ %d, %5d]  Time: %.1f min Time remaining: %.1f min \\n Train Loss: %.3f Train TP: %d \/ %d \\n Valid Loss: %.3f Valid TP: %d \/ %d' % \n          (epoch + 1, n_epochs, i, (time.time() - t) \/ 60, time_remaining, epoch_loss \/ i * batch_size, l, i, valid_loss \/ j * batch_size, k, j))\n    \n    # Save model if better than best\n    if best_model_loss > valid_loss:\n        last_best_model_loss = best_model_loss\n        best_model_loss = valid_loss\n        best_model_path = os.path.join(model_path, 'best_model.bin')\n        if os.path.isfile(best_model_path): \n            os.rename(best_model_path, os.path.join(model_path, 'last_best_model.bin'))\n        torch.save(net.state_dict(), os.path.join(model_path, 'best_model.bin'))\n        print('Model with loss of %0.3f saved.' % (best_model_loss\/j * batch_size))\n\nprint('Finished Training')\n","42128a72":"# maybe add a red line into the graphs for everytime training was restarted\n\nloss_range = np.arange(0, len(loss_train))\n\n# Cutoff early epochs to allow a better y axis scaling. Set to 0 to not cutoff anything. Not that a big problem with log scaling.\ncut = 0\n\n# Display losses\nplt.figure(figsize = (14,12))\nplt.subplot(211)\nplt.title(\"Loss\")\nplt.plot(loss_range, loss_train, 'r', label='Training')\nplt.plot(loss_range, loss_val, 'g', label='Validation')\nplt.legend()\nplt.xlim(left=cut)\nplt.yscale(\"log\")\nplt.grid(True, which='both')\n\n# Display accuracy\nplt.subplot(212)\nplt.title(\"Accuracy\")\nplt.plot(loss_range, acc_train, 'r', label='Training')\nplt.plot(loss_range, acc_val, 'g', label='Validation')\nplt.legend()\nplt.xlim(left=cut)\nplt.ylim(0, 1)\nplt.grid(True, which='both')\n\nplt.show()","ddac9753":"#net.load_state_dict(torch.load(\"models\/best_model.bin\"))\nnet.eval()\n\n# Create DF for submission data\ndf = pd.DataFrame(columns=[\"Id\", \"Expected\"])\n\n# Initialize variables\namount = len(os.listdir(test_images_path))\ni = 0\nt = time.time()\nprobs = []\n\n# Evaluate all test images\nfor file in os.listdir(test_images_path):\n    # Progress display\n    i += 1\n    if i % int((amount \/ 100)) == int((amount \/ 100) - 1):\n        print('%.1f %% Progressed. (%d of %d) Time: %.0f s Time remaining: %.0f min' %\n             (i\/amount * 100, i, amount, time.time() - t, (time.time() - t) \/ i * (amount - i) \/ 60), end='\\r')\n    \n    # Load image\n    img = cv2.imread(os.path.join(test_images_path, file))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n    \n    # Apply necessary preprocessing\n    img = transform_val(Image.fromarray(img.astype(np.uint8)))\n    img = torch.unsqueeze(img, 0)\n    img = img.to(device)\n    \n    # Get probabilities from the model\n    prob = net(img).detach().cpu().numpy()[0]\n    \n    # Get best 3 predictions with probabilities\n    prob_log = 10 ** prob\n    prob_log_norm = prob_log \/ np.sum(prob_log)\n    sorted_idx = np.argsort(prob_log_norm)[::-1]\n    preds = sorted_idx[:3]\n    pred_probs = np.around(prob_log_norm[preds], 4)\n    \n    # Save probabilities and classes\n    probs.append((file, pred_probs, preds))\n    df.loc[len(df)] = [file, preds[0]]\n\n# Update progress display\nprint('%.1f %% Progressed. (%d of %d) Time: %.0f min                        ' % \n      (i\/amount * 100, i, amount, (time.time() - t) \/ 60), end='\\r')","cfb63158":"# Create extended dataset with all test examples that surpass a certain threshold confidence\n# Display the mean and max probability\nprobability = 0\nprobability2 = 0\nmax_prob = 0\nmax_prob2 = 0\nmin_prob = 1\n\nfor file, prob, predClass in probs:\n    if prob[0] > max_prob:\n        max_prob = prob[0]\n    if prob[1] > max_prob2:\n        max_prob2 = prob[1]\n    if prob[0] < min_prob:\n        min_prob = prob[0]\n    probability += prob[0]\n    probability2 += prob[1]\n\nprobability \/= len(probs)\nprobability2 \/= len(probs)\n\nprint('Mean Prob: %0.2f %% Max Prob: %0.2f %% Min Prob: %0.2f %% \\nMean Prob 2.: %0.2f %% Max Prob 2.: %0.2f %%' % \n      (probability * 100, max_prob * 100, min_prob * 100, probability2 * 100, max_prob2 * 100))","39fd93a8":"# Visualize your results\n\n# 0 for lowest confidence items\n# 1 for highest confidence items\n# 2 for random confidence items\nshow_order = 0\n# amount of items shown\nshow_items = 10\n# amount of comparisons from same class shown\n# for show_order 0 half of the examples are for the second best guess\nshow_comparison = 4\n\nitems = []\n\nif show_order == 0:\n    probs.sort(key=lambda x: x[1][0])\n    items = probs[0:show_items]\nelif show_order == 1:\n    probs.sort(key=lambda x: x[1][0], reverse=True)\n    items = probs[0:show_items]\nelif show_order == 2:\n    items = random.choices(probs, k=show_items)\nelse:\n    print('Choose value between 0 and 2 for show_order.')\n    assert False\n    \nplt.figure(figsize=(3 * (show_comparison + 1), 3*show_items))\nfor i, item in enumerate(items):\n    file, probabilities, predictions = item\n    prob = probabilities[0]\n    predClass = predictions[0]\n    \n    img = cv2.imread(os.path.join(test_images_path, file))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n    plt.subplot(show_items, show_comparison + 1, i * (show_comparison + 1) + 1)\n    plt.imshow(img)\n    plt.axis('off')\n    plt.title('Class: ' + str(predClass) + ' Probability: ' + str(round(prob, 3)))\n    \n    classFiles = lbls_df[lbls_df['Expected'] == predClass]['Id'].values\n    \n    show_len = show_comparison\n    if len(classFiles) < show_comparison:\n        show_len = len(classFiles)\n        \n    if show_order == 0:\n        classFiles2 = lbls_df[lbls_df['Expected'] == predictions[1]]['Id'].values\n        classFiles[show_len\/\/2:] = classFiles2[:len(classFiles) - show_len\/\/2]\n        plt.title('Classes: ' + str(predClass) + ', ' + str(predictions[1]) + ' Probs: ' + str(round(prob*100, 2)) + ', ' + str(round(probabilities[1]*100, 2)))\n    for j in range(show_len):\n        img = cv2.imread(os.path.join(train_images_path, classFiles[j]))\n        if img is None:\n            img = cv2.imread(os.path.join(valid_images_path, classFiles[j]))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        plt.subplot(show_items, show_comparison + 1, i * (show_comparison + 1) + 2 + j)\n        plt.imshow(img)\n        plt.axis('off')\nplt.show()","48387f1d":"df","95f2ca2a":"#df.to_csv(os.path.join(submission_path, 'submission.csv'), index = False)\ndf.to_csv('submission.csv', index = False)","7158fb10":"# Necessary so the output shows up after commit and run all in the output section of the notebook\n\npaths = [train_images_path, train_images_path_ext, valid_images_path]\nfor path in paths:\n    if os.path.isdir(path):\n        rmtree(path)\n\npatch = os.path.join(model_path, 'last_best_model.bin')\nif os.path.isfile(path):\n    os.remove(os.path.join(model_path, 'last_best_model.bin'))\n","c4e60b4a":"### 1) Set basic parameters and paths","dc23035a":"### 2) Show all different Classes","a83b0d2a":"### 3) Create Datasets","6b98e6a4":"# Classification with limited data\nApplication of Deep Learning - Kaggle Challenge 2 - Classification  \nhttps:\/\/www.kaggle.com\/c\/classification-with-limited-data\/leaderboard","76da6b0a":"### 5) Training\nChoose the amount of epochs and start training. Start it again to keep on training or continue (from epoch 0) after interrupting.","b51e07e6":"### 4) Load Model","7bc816cf":"### 6) Generating submission.csv file and EXTended training data"}}