{"cell_type":{"0bc78cb4":"code","5e21068e":"code","ee5daee5":"code","e070e274":"code","d6b1523d":"code","d6e0f511":"code","4394ce98":"code","918dd371":"code","e882ddbf":"code","f6053bfd":"code","8e068d85":"code","eb07a819":"code","48954ea9":"code","0c01cff8":"code","92dae3e9":"code","34e52caf":"code","3b6a78e0":"code","9f65b949":"code","14cd48b7":"code","9e063d33":"code","b99c6e2f":"code","008751b8":"code","a9b62aa4":"code","a6a677d7":"code","d844266c":"code","622c1b10":"code","92f094ad":"code","e5f850a9":"code","5ba833a1":"code","54d8fc22":"code","f8556be5":"code","c6315cdb":"code","7e7c55c1":"code","d4bf3b4b":"code","08a6fec3":"code","97df8c27":"code","7900eafa":"markdown","b83fcfb4":"markdown","7f658ae3":"markdown","0cef0167":"markdown","bbd8b0a3":"markdown","32f1e02f":"markdown","2d9b6de7":"markdown","e99776ca":"markdown","7d82da57":"markdown","54f01526":"markdown","2dfb5ae8":"markdown","5cee1d15":"markdown","bf0d904b":"markdown","2a36027e":"markdown","6251dd08":"markdown","7500b9e7":"markdown","a610a3d7":"markdown","795fd3d0":"markdown","955d1fcf":"markdown","cacbe30e":"markdown","22da832f":"markdown","0342067d":"markdown","1af7d0b7":"markdown","c502aa4d":"markdown","e9872e4c":"markdown","6f0eaaac":"markdown","91ee4eaf":"markdown","50fbe4d6":"markdown","b04fa7d1":"markdown"},"source":{"0bc78cb4":"# setting up the libraries that we will need \nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\n\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt","5e21068e":"# loading the data\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\nsub = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")","ee5daee5":"#data exploration\ntrain.head()","e070e274":"train.select_dtypes(include = ['object']).describe()","d6b1523d":"train.info()","d6e0f511":"test.info()","4394ce98":"print('Only {0:.0f}% of passengers have survived'.format(train[\"Survived\"].value_counts(1)[1]*100))","918dd371":"train_copy = train.copy()\ntest_copy = test.copy()","e882ddbf":"def corr(x):\n    corr_matrix = x.corr()\n    c= corr_matrix[\"Survived\"].sort_values(ascending=False)\n    print(c)\ncorr(train_copy)","f6053bfd":"print('{0:.2f}% of Parch values are 0'.format(train.Parch.value_counts(1)[0]*100))\nprint('{0:.2f}% of SibSp vales are 0'.format(train.SibSp.value_counts(1)[0]*100))","8e068d85":"print('{0:.0f}% of Cabin attribute in values are missing in training data'.format(train['Cabin'].isnull().value_counts(1)[1]*100))\nprint('{0:.0f}% of Cabin attribute in values are missing in test data'.format(test['Cabin'].isnull().value_counts(1)[1]*100))","eb07a819":"train_copy[\"deck\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in train_copy['Cabin'] ])\ntest_copy[\"deck\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in test_copy['Cabin'] ])","48954ea9":"train_copy.groupby(['deck','Pclass'])['deck'].count()","0c01cff8":"test_copy.groupby(['deck','Pclass'])['deck'].count()","92dae3e9":"g = sns.FacetGrid(train, col='Survived')\ng = g.map(sns.histplot, \"Age\")","34e52caf":"f=test.Fare.isnull().value_counts()[1]\ne=train.Embarked.isnull().value_counts()[1]\nprint('Only {0} value of Fare attribute is missing in test and only {1} values of Embarked attribute are missing in train'.format(f,e))","3b6a78e0":"test[test['Fare'].isnull()]","9f65b949":"g = sns.FacetGrid(train, col='Survived')\ng = g.map(sns.histplot, \"Fare\")","14cd48b7":"print('There is {0} unique values of Ticket attribute'.format(len(train.Ticket.unique())))","9e063d33":"train_copy['Title'] = train_copy['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]","b99c6e2f":"train_copy.Title.value_counts()","008751b8":"class Imputer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        self.med_fare_ = X.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\n        self.most_freq_embarked = X.Embarked.value_counts().index[0]\n        return self\n    def transform(self, X, y=None):\n        #replacing missing values of Age with median Age for each class. 1 value for each Sex.\n        X.Age = X.groupby(['Sex', 'Pclass'])['Age'].apply(lambda z: z.fillna(z.median()))\n        # 1 only missing value for Fare. A Man in the third class with no family\n        X.Fare = X.Fare.fillna(self.med_fare_)\n        # filling Embarked with the most frequent \n        X.Embarked = X.Embarked.fillna(self.most_freq_embarked)\n        \n        return X\n        \n    ","a9b62aa4":"class feature_engineering(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        # combining Sibsp and Parch to create relative on board then creating the family attribute\n        X[\"RelativesOnboard\"] = X[\"SibSp\"] + X[\"Parch\"]\n        X['Family'] = X['RelativesOnboard'].map({0: 'Alone', 1: 'Small', 2: 'Small', 3: 'Medium', 4: 'Medium', 5: 'Medium', 6: 'Large', 7: 'Large', 10: 'Large'})\n        # extracting the first letter of each cabin to create deck attribute\n        X[\"Deck\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in X['Cabin'] ])  \n        # regrouping deck catergories\n        X['Deck'] = X['Deck'].replace(['T', 'A', 'B', 'C'], 'ABC')\n        X['Deck'] = X['Deck'].replace(['D', 'E'], 'DE')\n        X['Deck'] = X['Deck'].replace(['F', 'G'], 'FG')\n        # we will transform the Age from continuous data into and attribute with 14 category.\n        X[\"Age\"] = pd.cut(X[\"Age\"],\n                               bins=[0., 5.0, 15.0, 25.0, 30.0, 40.0,50.0,60.0, np.inf],\n                               labels=[1, 2, 3, 4, 5,6,7,8])\n        # For Fare, Applying will reduce skewness distribution\n        X[\"Fare\"] = X[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\n        # Replace ticket ID by ticket frequency\n        X['Ticket_Frequency'] = X.groupby('Ticket')['Ticket'].transform('count')\n        # Extract title attribute from Name and create IsMarried Attribute\n        X['Title'] = X['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\n        X['Is_Married'] = 0\n        X['Is_Married'].loc[X['Title'] == 'Mrs'] = 1\n        X['Title'] = X['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss\/Mrs\/Ms')\n        X['Title'] = X['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr\/Military\/Noble\/Clergy')\n        return X","a6a677d7":"class encoding(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.ohe = OneHotEncoder()\n        self.le = LabelEncoder()\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        #label encoding\n        for i in ['Age', 'Fare', 'Embarked', 'Sex', 'Title', 'Family', 'Title', 'RelativesOnboard', 'Deck']:\n            X[i] = self.le.fit_transform(X[i])\n        #OneHotEncoding\n        encoded_features = []\n        for cat in ['Deck', 'Age','Pclass', 'Sex', 'Title','Embarked','Family','RelativesOnboard']:\n            encoded_feat = self.ohe.fit_transform(X[cat].values.reshape(-1, 1)).toarray()\n            n = X[cat].nunique()\n            cols = ['{}_{}'.format(cat, n) for n in range(1, n + 1)]\n            encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n            encoded_df.index = X.index\n            encoded_features.append(encoded_df)\n        X = pd.concat([X, *encoded_features[:8]], axis=1)\n        return X","d844266c":"# This class is used after cleaning and feature engineering to drop unnecessary columns\nclass Columns_drop(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X.drop(self.attribute_names, axis=1)","622c1b10":"dropped_columns = [ 'RelativesOnboard','Age', 'Sex','Deck', 'Family', 'Embarked', 'PassengerId', 'Pclass', 'Name', 'Ticket','Cabin','Embarked']\n","92f094ad":"processing_pipeline = Pipeline([\n    ('Filling missing values', Imputer()),\n    ('Feature Engineering', feature_engineering()),\n    ('Encoding', encoding()),\n    ('dropping useless columns', Columns_drop(dropped_columns) ),\n    \n])","e5f850a9":"X_train= train.copy()\nX_test = test.copy()\nX_train = processing_pipeline.fit_transform(X_train)\ncorr(X_train)\nX_train = X_train.drop('Survived',axis=1 )\ny_train = train['Survived']\nX_test = processing_pipeline.fit_transform(test)\n","5ba833a1":"from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\n","54d8fc22":"kfold = StratifiedKFold(n_splits=10)\nrandom_state = 42\nclassifiers = []\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier())\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, X_train, y = y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n    \ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"Algorithm\":[\"SVC\",\"DecisionTree\",\"RandomForest\",\"GradientBoosting\",\"ExtraTrees\"],\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std})\ncv_res","f8556be5":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 300, num = 3)]\n# Number of features to consider at every split\nmax_features = [ 20 ,'auto', 'log2']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 30, num = 3)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Create the random grid\nrf_param_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\n","c6315cdb":"# The number of boosting stages to perform\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 200, num = 3)]\n# Number of features to consider at every split\nmax_features = [ 20 ,'auto', 'log2']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 30, num = 3)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2,  10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# The function to measure the quality of a split\n# Create the random grid\ngb_param_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,}\n","7e7c55c1":"# Random forest Hyper parameter tuning\nrf = RandomForestClassifier()\n\n#fitting\nGrid_s_rf = GridSearchCV(rf, param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = True)\nGrid_s_rf.fit(X_train,y_train)\nRFC_best = Grid_s_rf.best_estimator_\n\n# Best score\nGrid_s_rf.best_score_","d4bf3b4b":"# Gradient boosting Hyper parameter tuning\ngbc = GradientBoostingClassifier()\n\n#fitting\nGrid_s_gb = GridSearchCV(gbc,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\nGrid_s_gb.fit(X_train,y_train)\nGBC_best = Grid_s_gb.best_estimator_\n\n# Best score\nGrid_s_gb.best_score_\n","08a6fec3":"#Combining 2 models\nvotingC = VotingClassifier(estimators=[('rfc', RFC_best),('gbc',GBC_best)], voting='soft', n_jobs=-1)\nvotingC = votingC.fit(X_train, y_train)","97df8c27":"predictions = votingC.predict(X_test)\nsubmission = pd.DataFrame({'PassengerId': test.PassengerId,\n                           'Survived': predictions})\nsubmission.to_csv('submission.csv', index = False)","7900eafa":"* Age distribution seems to be a gaussian distribution. We can notice that Age distribution are not the same in the survived and not survived subpopulation: Those who are between 60 and 80 are less likely to survive, and childrens and young passengers have higher chances of survival. \n* Transforming the age feature into categories seems better than using the age of each person because some there is some age categories that have high or less chance of surviving.","b83fcfb4":"#### 2.3 Data processing pipeline","7f658ae3":"### Cabin Attribute\n","0cef0167":"Most of Parch and SibSp values are 0 and they are not highly correlated with the target. On their own they don't give an important information. However, if we combine them we can create a new attribute called \"Family Size\" with 3 catergories: Small, Medium and Large. We can also add Alone category if the passenger has no family onboard the ship.","bbd8b0a3":"### Name","32f1e02f":"The attributes have the following meaning:\n* **Survived**: that's the target, 0 means the passenger did not survive, while 1 means he\/she survived.\n* **Pclass**: passenger class.\n* **Name**, **Sex**, **Age**: self-explanatory\n* **SibSp**: how many siblings & spouses of the passenger aboard the Titanic.\n* **Parch**: how many children & parents of the passenger aboard the Titanic.\n* **Ticket**: ticket id\n* **Fare**: price paid (in pounds)\n* **Cabin**: passenger's cabin number\n* **Embarked**: where the passenger embarked the Titanic","2d9b6de7":"### Ticket","e99776ca":"#### 2.2.2 Enconding\nIn this section we will label encode the following features:\n* Age, Fare, Embarked, Sex, Title, Family, Title, RelativesOnboard, Deck.\n\nAnd use OneHotEncoding on the folliwing featues:\n* Pclass, Sex, Deck, Family, Title, Embarked","7d82da57":"### Age","54f01526":"## **2. Data Processing**\n### 2.1 Data Cleaning: Dealing with missing value\nWe did notice in the previous section that some of the attributes are missing 1 or 2 values while other have more than 70% of missing values. So here we will deal with this missing data using diffirent approaches for each attribute.\n* One of the common approaches for filling missing numerical values is using the media. However, using the age might be a little bit tricky because the classes of passengers have different age ranges. So we will replace the missing values with the media age for each class, for both male and female passengers.\n* For the fare, we have only 1 missing value. We will fill it with the median 3rd class passangers that have no family with them\n* For Embarked we will use the most Frequent which is S\n* For the Cabin attribute we will deal with it in the feature engineering section because we are going to create a new attribute","2dfb5ae8":"* For the name attribute we can extract the title and analyse it better. We can extract family names also but that would take too much time so I am gonna stick to the title only","5cee1d15":"### 2.2 Feature Engineering:\n#### 2.2.1 Creating new features\nIn this section we will transform existing attribute and create new ones:\n* First we will combine the Parch and SibSp to create a Relative on board feature.\n* Using the Relative On board feature we will create a family attribute which consists of: \n - Alone if the passenger has no relatives on board\n - Small if he have 1 or 2 family members on board\n - Medium if he have 3, 4 or 5 family members\n - Large if he has more than 6\n* Using the cabin feature we will create a deack attribute by extracting the first letter of the cabin name for each passenger.\n* we will transform the Age from continuous data into and attribute with 8 categories.\n* For Fare, Applying will reduce skewness distribution\n* Replace ticket ID by ticket frequency\n* Extract title attribute from Name and create IsMarried Attribute\n","bf0d904b":"# **Introduction**\nHello, \nIn this notebook you will find a simple approach to tackle the famous Titanic disaster machine learning. It will include a full pipeline for data cleaning, feature engineering, modeling and paramaters tunning. I didn't focus on EDA because my main goal was to focus on feature engineering and classification task.","2a36027e":"* Training set have missing values in Age, Cabin and Embarked columns\n* Test set have missing values in Age, Cabin and Fare columns","6251dd08":"For each model we have 216 candidates, fiited for 10 folds resulting in 2160 fils in total. we will chose the best 2 models and then combine them using VotingClassifier.","7500b9e7":"* Its true that most of Cabin attribute are missing values but we can't drop this feature because some of the cabins have more survival rate than others, for example these who are close to surface.\n* After some research, tt turns out to be that the first letter of the Cabin values are the decks in which the cabins are located. So we will change the cabin attribute with a deck attribute and explore further this new feature\n","a610a3d7":"If you watched the movie then you know that decks were separated and some of them were meant to be used by 1 passenger class. However some of the decks were used by multiple class:\n* A, B and C decks: only for 1st class passengers\n* D and E: for all classes\n* F and G decks: for both 2nd and 3rd class passengers\n* Passengers labeled as X are the missing values in Cabin feature. I don't think it is possible to find those passengers' real Deck.","795fd3d0":"## **3. Modeling**\n#### 3.1 Evaluating models and making a choice\nIn this section we evaluate the performance of some well known models on the data and chose those who have a good accuracy. We will cross validation on 10 folds to meausure the performance on train data. ","955d1fcf":"# **Table of contents:** \n\n1. Loading data and some EDA\n\n2. Data Processing\n\n    2.1 Data cleaning: Dealing with null values\n\n    2.2 Feature engineering and encoding\n\n    2.3 Data Processing Pipeline\n\n3. Modeling\n\n    3.1 Evaluating models and making a choice\n\n    3.2 Hyper paramater tuning and combining models\n\n4. Submitting results","cacbe30e":"### Fare and Embarked","22da832f":"* We can notice that we have 4 major categories and that all the others can be of these 4. In the feature engineering section we will create 4 categories only for these attribute","0342067d":"### Parch and SibSp attribute","1af7d0b7":"* There is a large number of unique Tickets to analyse so we can create a ticket_freq attribute instead of Tickets.\n* Many passengers travelled along with groups. Those groups consist of friends, nannies, maids and etc. They weren't counted as family, but they used the same ticket.","c502aa4d":"#### 3.2 Hypter parameter tunning and combining models","e9872e4c":"## **4. Submitting results**\n","6f0eaaac":"* The passengers that has a missing value for fare is from class 3 and has 0 family members with him.","91ee4eaf":"## **1. Loading data and some EDA:**","50fbe4d6":"RandomForest and Gradient boosting are promissing models so we will continue with them.","b04fa7d1":"* Fare distribution is very skewed and might result in badbehaviour in our model. Transforming it into log seems like a good solution."}}