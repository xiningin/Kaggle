{"cell_type":{"9c5bdf04":"code","a12a3dc1":"code","5917baa0":"code","19a10aa3":"code","3107652a":"code","e767f4b6":"code","78ac24b2":"code","780d343c":"code","d50f0dfa":"code","a7cd804f":"code","68eb3b20":"code","e02c0a7d":"code","2ba156f7":"markdown","c33bfd07":"markdown","0b275da9":"markdown","57ef0ec5":"markdown","e12f78be":"markdown","beed1cf9":"markdown"},"source":{"9c5bdf04":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%pylab inline\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a12a3dc1":"df = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest_sample = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\n","5917baa0":"X = df.iloc[:,1:]  #image in pixel flatten formate\ny = df[\"label\"]    # label 0,1,2,3,4,5,6,7,8,9","19a10aa3":"X = X.T\ny = pd.DataFrame(np.array(y).reshape(1,len(y)))\n","3107652a":"fig = plt.figure()\nplt.figure()\nf, axarr = plt.subplots(2,2) \nfor i in range(2):\n    for j in range(2):\n        axarr[i][j].imshow(np.array(X.iloc[:,i+j]).reshape(28,28),cmap='gray')","e767f4b6":"shape_X = X.shape\nshape_Y = y.shape\nm = X.shape[1] \n\nprint ('The shape of X is: ' + str(shape_X))\nprint ('The shape of Y is: ' + str(shape_Y))\nprint ('I have m = %d training examples!' % (m))","78ac24b2":"# prediction = []\n\n# # for i in range(0,1000):\n# #     prediction.append([i+1,clf.predict(test_sample.iloc[i:i+1,:])[0]])\n# for i in tqdm (range (len(test_sample)),  \n#                desc=\"Loading\u2026\",  \n#                ascii=False, ncols=75):\n#     prediction.append([i+1,clf.predict(test_sample.iloc[i:i+1,:])[0]])\n#     time.sleep(0.01) \n\n# from tqdm import tqdm \n# import time \n  \n  \n","780d343c":"def sigmoid(z):\n    return 1\/(1+np.exp(-z))","d50f0dfa":"def layer_size(X,Y):\n    n_x = X.shape[0]\n    n_h = 4\n    n_y = y.shape[0]\n    return (n_x,n_h,n_y)","a7cd804f":"(n_x, n_h, n_y) = layer_size(X, y)\nprint(\"The size of the input layer is: n_x = \" + str(n_x))\nprint(\"The size of the hidden layer is: n_h = \" + str(n_h))\nprint(\"The size of the output layer is: n_y = \" + str(n_y))","68eb3b20":"def initialize_parameters(n_x, n_h, n_y):\n    \"\"\"\n    Argument:\n    n_x -- size of the input layer\n    n_h -- size of the hidden layer\n    n_y -- size of the output layer\n    \n    Returns:\n    params -- python dictionary containing your parameters:\n                    W1 -- weight matrix of shape (n_h, n_x)\n                    b1 -- bias vector of shape (n_h, 1)\n                    W2 -- weight matrix of shape (n_y, n_h)\n                    b2 -- bias vector of shape (n_y, 1)\n    \"\"\"\n    \n    np.random.seed(2)\n    \n    W1 = np.random.randn(n_h,n_x)*0.01\n    b1 = np.zeros((n_h,1))\n    W2 = np.random.randn(n_y,n_h)*0.01\n    b2 = np.zeros((n_y,1))\n    \n    assert (W1.shape == (n_h, n_x))\n    assert (b1.shape == (n_h, 1))\n    assert (W2.shape == (n_y, n_h))\n    assert (b2.shape == (n_y, 1))\n    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters","e02c0a7d":"n_x,n_h,n_y = layer_size(X,y)\nparameters = initialize_parameters(n_x, n_h, n_y)\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))","2ba156f7":"Mathematically:\n\nfor example $x^{(i)}$ :\n\n\n$z^{[1](i)}=W^{[1]}x^{(i)}+b^{[1]} \\ \\ \\ \\ \\ \\ \\ \\ \\ ...(1)$\n\n$a^{[1](i)}=tanh(z^{[1](i)}) \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  ...(2)$\n\n$z^{[2](i)}=W^{[2]}a^{[1](i)}+b^{[2]} \\ \\ \\ \\ \\  ...(3)$\n\n$\\hat y^{ (i)}=a^{[2](i)}= \\sigma (z^{[2](i)}) \\ \\ \\ \\ \\ \\ \\ \\ ...(4)$\n\n\n$ y^{(i)} _{prediction}= \\begin{cases} 1 & \\text{if } a^{[2](i) } > 0.5\\\\\n    0              & \\text{otherwise}\\end{cases} \\ \\ \\ ...(5)$\n    \n    \nGiven the predictions on all the examples, you can also compute the cost  J  as follows:\n\n#### $J=\u2212{1\\over m} \\sum _{i=0} ^{m}\\big(y^{(i)}log(a^{[2](i)})+(1\u2212y^{(i)})log(1\u2212a^{[2](i)})\\big) \\ \\ ...(6)$","c33bfd07":"## Let us now draw the architecture of Our Neural network\n\n\n![Screenshot%202021-03-05%20at%2011.05.43%20AM.png](attachment:Screenshot%202021-03-05%20at%2011.05.43%20AM.png)\n","0b275da9":"## Initialize the model's parameters\n\n- Make sure your parameters' sizes are right.\n- Refer to the neural network figure above if needed.\n- You will initialize the weights matrices with random values.\n- Use: `np.random.randn(a,b) * 0.01` to randomly initialize a matrix of shape (a,b).\n- You will initialize the bias vectors as zeros.\n- Use: `np.zeros((a,b))` to initialize a matrix of shape (a,b) with zeros.","57ef0ec5":"# Neural Network Model\n\nGeneral Methodology for Neural Network\n\n1. Define the neural network structure ( # of input units,  # of hidden units, etc). \n2. Initialize the model's parameters\n3. Loop:\n    - Implement forward propagation\n    - Compute loss\n    - Implement backward propagation to get the gradients\n    - Update parameters (gradient descent)","e12f78be":"## Defining the neural network structure\nDefine three variables:\n\n- n_x: the size of the input layer\n- n_h: the size of the hidden layer \n- n_y: the size of the output layer","beed1cf9":"### some utility function\n\nWe will be using Sigmoid as an activation function for final layer while for activation of hidden layer we will use tanh which can directly obtain from numpy  as `np.tanh()`\n\nWe know that\n  ### $\\sigma (z) = {1 \\over {1 + e^{-z}}} $"}}