{"cell_type":{"dd72557b":"code","50244177":"code","d8298e89":"code","df214463":"code","0728e32c":"code","7c35244f":"code","457cf655":"code","60e1b3af":"code","3ac754a5":"code","c0e68ff7":"code","8098a844":"code","0148e57a":"code","0f52cfa4":"code","bcc3f5a6":"markdown"},"source":{"dd72557b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","50244177":"train = pd.read_csv('\/kaggle\/input\/killer-shrimp-invasion\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/killer-shrimp-invasion\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/killer-shrimp-invasion\/temperature_submission.csv')\nprint('train: ', train.shape)\nprint('test: ', test.shape)\nprint('submission: ', submission.shape)","d8298e89":"train.query('Presence==0').sample(5).head()","df214463":"train.query('Presence==1').sample(5).head()","0728e32c":"train['is_test'] = 0\ntest['is_test'] = 1\ndata = pd.concat([train, test], sort=False)\ndata.reset_index(drop=True, inplace=True)\n\n# excluded pointid\nfeats = ['Salinity_today', 'Temperature_today', 'Substrate', 'Depth', 'Exposure'] \n\ndata[feats].head()","7c35244f":"def post_preparation_importance(feats, model, n_fold):\n    imp_df = pd.DataFrame()\n    imp_df[\"feature\"] = feats\n    imp_df[\"importance\"] = model.feature_importance(importance_type='gain', iteration=model.best_iteration)\n    imp_df[\"fold\"] = n_fold + 1\n    return imp_df\n\ndef post_display_importances(feature_importance_df_):\n    cols = feature_importance_df_[[\"feature\", \"importance\"]]\\\n        .groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n        \n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n\n    plt.figure(figsize=(8, 10))\n    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LightGBM Features (avg over folds)')\n    plt.show()","457cf655":"lgb_params = {\n    'boost': 'gbdt',\n    'objective': 'binary',\n    'metric': 'auc',\n    'verbosity': 1,\n}","60e1b3af":"n_split = 5\nkfold = KFold(n_splits=n_split, shuffle=True, random_state=42)\nfeature_importance_df = pd.DataFrame()\n\nfor i_fold, (train_idx, valid_idx) in enumerate(kfold.split(data)):\n    print(f\"--------fold {i_fold}-------\")\n    \n    ## train data\n    x_tr = data.loc[train_idx, feats]\n    y_tr = data.loc[train_idx, 'is_test']\n\n    ## valid data\n    x_va = data.loc[valid_idx, feats]\n    y_va = data.loc[valid_idx, 'is_test']\n    \n    lgb_train = lgb.Dataset(x_tr, label=y_tr)\n    lgb_test = lgb.Dataset(x_va, label=y_va)\n\n    model = lgb.train(\n        lgb_params,\n        lgb_train,\n        valid_sets=[lgb_train, lgb_test],\n        valid_names=['train', 'test'],\n        num_boost_round=1000,\n        early_stopping_rounds=100,\n        verbose_eval=100\n    )\n\n    valid_preds = model.predict(x_va, num_iteration=model.best_iteration)\n    valid_metric = roc_auc_score(y_va, valid_preds)\n    \n    fold_importance_df = post_preparation_importance(feats, model, n_split)\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    print('Valid Metric: {:.5f}'.format(valid_metric))","3ac754a5":"post_display_importances(feature_importance_df)","c0e68ff7":"data.query('is_test == 0')['Depth'].describe()","8098a844":"data.query('is_test == 1')['Depth'].describe()","0148e57a":"data.query('is_test == 1')['Exposure'].describe()","0f52cfa4":"data.query('is_test == 0')['Exposure'].describe()","bcc3f5a6":"# Overview\nI investigated the difference between train and test by Adversarial Validation.  \nWe can trust CV and public lb. Leaks are excluded..."}}