{"cell_type":{"ef76c328":"code","40b7694d":"code","94f0e7a1":"code","2de9d4ba":"code","0dc3c531":"code","8a427f09":"code","8705c882":"code","0a808ad9":"code","ec056273":"code","ad997a10":"code","e326e135":"code","3ca44922":"code","570b20ae":"code","7bfe85e9":"code","99ed503b":"code","a2f60304":"code","65b6ca8f":"code","8c6442bc":"code","a13e7af4":"markdown","2089cdd4":"markdown","400b380e":"markdown","649cf2c3":"markdown","562a69db":"markdown","974334f3":"markdown","9e4b81c6":"markdown"},"source":{"ef76c328":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.decomposition  import PCA,KernelPCA\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import train_test_split as tts\nimport sklearn\n\nimport warnings\nwarnings.filterwarnings('ignore')","40b7694d":"df = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')\nX= df.copy()\ny = X.pop('label')","94f0e7a1":"print('The dataset has ',X.shape[0],' tuples and ',X.shape[1],' features\\n Now we see a sample of the dataset')","2de9d4ba":"X.sample(5)","0dc3c531":"fig,axe = plt.subplots(2,5)\naxe = axe.flatten()\nfor i in range(10):\n    axe[i].axis('off')\n    abc = axe[i].imshow(X.iloc[i,:].to_numpy().reshape(28,28))\n_= fig.suptitle('First 10 images',fontsize=20)\nplt.tight_layout(pad= 0,h_pad= 0,w_pad= 0.5)","8a427f09":"print('Now we see the balance of dataset')\ny.value_counts()","8705c882":"print('It shows dataset is having equal values for each class thus is PERFECTLY balanced.\\n Now we see the variances in features')","0a808ad9":"X.var().sort_values(ascending=False)","ec056273":"pca = PCA(random_state=0)\npca.fit(X)","ad997a10":"pca.explained_variance_ratio_[:20]","e326e135":"c = np.cumsum(pca.explained_variance_ratio_)\nn = np.argmax(c>=0.95)+1\n_ = plt.figure(figsize=(16,8))\n_ = plt.axis([0, 400, 0, 1])\n_ = plt.plot(c,lw=1)\n_ = plt.plot([n,n],[0,1],\"k:\")\n_ = plt.xlabel('Number of Componets')\n_ = plt.ylabel('Explained Variance Ratio')\n_ = plt.suptitle('Explained Variance vs n_Components',fontsize=15)\n_ = plt.xticks(np.arange(0,400,10))\n_ = plt.yticks(np.arange(0,1.0,0.05))\n_ = plt.grid(True)\n_ = plt.annotate('n_Componets = '+str(n),xy=(n,0.95),xytext=(n+10,0.9),arrowprops=dict(arrowstyle=\"->\"))","3ca44922":"fig, ax = plt.subplots(1, figsize=(14, 10))\nplt.scatter(x = X.iloc[:,0], y= X.iloc[:,1],s=10, c=y.to_numpy(), cmap='Spectral', alpha=1.0)","570b20ae":"from sklearn.ensemble import RandomForestClassifier\nX=df.copy()\nX.pop('label')\npca = PCA(n_components=187,random_state=0)\nX=pca.fit_transform(X)\nrf = RandomForestClassifier(max_depth=50)\nrf.fit(X,y)","7bfe85e9":"print(classification_report(y,rf.predict(X)))","99ed503b":"X_test = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_test.csv')\ny_test=X_test.pop('label')\nX_= pca.transform(X_test)\ny_hat=rf.predict(X_)\nprint(classification_report(y_test,y_hat))","a2f60304":"classes = ['T-shirt\/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\ni = [(y_hat[i] != y_test[i]) for i in range(10000)]\nX_1 = X_[i]\ny_ = y_hat[i]\n\nfig,axe = plt.subplots(5,10)\naxe = axe.flatten()\nfor x in range(50):\n    axe[x].axis('off')\n    axe[x].set_title(classes[y_[x]])\n    abc = axe[x].imshow(pca.inverse_transform(X_1[x,:]).reshape(28,28))\n_= fig.suptitle('Some incorrectly classified images',fontsize=20)","65b6ca8f":"import umap\nX= df.copy()\ny = X.pop('label')\nmapper = umap.UMAP(random_state=0,n_neighbors=5).fit(X,y)\nX = mapper.transform(X)\nrf.fit(X,y)\nX_ = mapper.transform(X_test)\ny_hat= rf.predict(X_)\n\nprint(classification_report(y_test,y_hat))","8c6442bc":"i = [(y_hat[i] != y_test[i]) for i in range(10000)]\nX_1 = X_[i]\ny_ = y_hat[i]\nfig,axe = plt.subplots(5,10)\naxe = axe.flatten()\nfor x in range(50):\n    axe[x].axis('off')\n    axe[x].set_title(classes[y_[x]])\n    abc = axe[x].imshow(mapper.inverse_transform(X_1[x,:]).reshape(28,28))\n_= fig.suptitle('Some incorrectly classified images',fontsize=20)","a13e7af4":"Now we check with incorrectly classified values.","2089cdd4":"Here we can see there are numerous features with variance ranging from 0.009 to 10728. We can now either put Variance Threshold in place to select best features or can cocentrate the variance using PCA.\n# Principal Component Analysis\nIn short **Principal components can be defined as eigenvectors of covariance matrix of dataset**. In PCA a tuple(tensor) is projected on a sub-space in order to minimize eigen values so that the variance can be concentrated on lesser dimensions without harming the distance. The (linear PCA)algorithm uses translation and rotation to achieve the goal.<br>The sklearn function performs the algorithm and keeps dimensions stored along with explained variance so that it can be APPROXIMATELY reversed further. Here now we perform PCA over data and find out number of components for 95% variace explained.","400b380e":"# Purpose of kernel\nThis kernel is to explore basics of dimensionality reduction on the dataset. To do so first we look into dimensionality reduction and then will follow one of the most basic algorithm (linear) PCA. But, first thing first. We start with dimensionality reduction.\n## Dimensionality reduction\nThe real world datasets hold numerous features that may sometime go to the tune of thousands or even more. It actually poses problem of handling data and even worsens the speed of training. Point to ponder is that some of the features are either derived from some other singular feature or are having lesser important information. In that way we can opt to drop such features **in order to expedite the training process without harming quality of actual data to measureable extent**.<br> There are various methods for dimensionaliy reduction, but these can be vastly classified into two classes:-\n### Feature Selection\nIn this category fall all the method where we eliminate features on basis of their usefullness without altering other features. Some of the methods\/algorithms are:-\n - Missing Values Ratio\n - Variance Threshold\n - Correlation Filter\n - Variable neighborhood search\n - Scoreing based selection (SelectKBest)\n - Recursive Feature Elimination(RFE) etc\n \n### Feature Projection(Extraction)\nThis category digs deeper into dataset. Here data is represented as tensors in hyperspace and then **is projected on a relatively lower-dimensional space (generally) so that some of the features can be expansed.** The major methods\/algorithms here are:-\n - Principal Component Analysis(PCA)\n     - Lineear PCA\n     - Kernel PCA\n     - Sparse PCA\n     - Multilinear PCA\n     - Minibatch PCA\n - Independent Component Analysis (ICA)\n - IsoMap\n - Factor Analysis\n - UMAP etc\n \n Here we are focussing on PCA only.","649cf2c3":"# Fashion-MNIST: What is it?\nAn MNIST database is a Modified <a href='https:\/\/www.nist.gov\/'>National Institute of Standards and Technology<\/a> database - a large database used for training purpose. The prime MNIST database is digit database that has 60000 sample strong training dataset and 10000 sample strong test dataset. while each sample has a handwritten digit image of size 28\u00d728. Now we come to Fashion-MNIST. As per the official metadata:-<br>\n\n<b>Fashion-MNIST is a dataset of Zalando's article images\u2014consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. We intend Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.<\/b><br>\n<img src ='https:\/\/github.com\/zalandoresearch\/fashion-mnist\/raw\/master\/doc\/img\/embedding.gif'><br>\nTo work with the dataset ,alternatively, lateron the images'pixel were flattened to be stored in a tabular format thus forming csv file of 28\u00d728+1(for label),i.e,785 columns. The dataset has 10 labels(classes) as below:-\n\n\n    - 0 T-shirt\/top\n    - 1 Trouser\n    - 2 Pullover\n    - 3 Dress\n    - 4 Coat\n    - 5 Sandal\n    - 6 Shirt\n    - 7 Sneaker\n    - 8 Bag\n    - 9 Ankle boot\n\n\n\n","562a69db":"# Import required libraries","974334f3":"# This is just a beginner's exploration job, your comments for imploring the concepts better way are most welcome.","9e4b81c6":"To get 95% variance we can now plot cumulative explained variance ratio and check number of components there."}}