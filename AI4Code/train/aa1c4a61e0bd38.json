{"cell_type":{"dfd8b4a6":"code","50a56dbc":"code","73f1d2f9":"code","d2c7f065":"code","3ff12087":"code","b88ea1ec":"code","ea53337e":"code","49f7b951":"code","c52e3bd6":"code","6c0eefe9":"code","0dc9959d":"code","47b1a649":"code","89210478":"code","0467fd77":"code","0c2072f7":"code","f4cb0b16":"code","8b9ebe95":"code","4af1ce05":"code","86369b0f":"code","0b184f8b":"code","d9e36bf3":"code","0978c615":"code","f39cae6b":"code","8e91786e":"code","8ba07837":"code","1b004caa":"markdown","b333b1b6":"markdown","fb23029b":"markdown","6bbe19e7":"markdown","f3f7fd28":"markdown","5d2e2e45":"markdown","35009ff5":"markdown","b9177fb9":"markdown","dbf7599b":"markdown","79202390":"markdown","c578e8c0":"markdown","f3803ab6":"markdown","4116d2ff":"markdown","87c407f9":"markdown","f1947037":"markdown","fca56313":"markdown","d047b147":"markdown","ad2e5e9d":"markdown","e25f97cd":"markdown","2b8163c2":"markdown","2fc87664":"markdown","ce96cfc9":"markdown","e9f2ef8b":"markdown"},"source":{"dfd8b4a6":"import os.path\nfrom pathlib import Path\n","50a56dbc":"import subprocess\nversion = subprocess.check_output(['java', '-version'], stderr=subprocess.STDOUT)\nprint(version)","73f1d2f9":"%%capture\n!pip install pyserini==0.8.1.0\n!pip install transformers\n!pip install nltk\nimport json","d2c7f065":"if(not('11.0.2' in str(version))):\n    print('jdk upgrade required')\n    !curl -O https:\/\/download.java.net\/java\/GA\/jdk11\/9\/GPL\/openjdk-11.0.2_linux-x64_bin.tar.gz\n\n    !mv openjdk-11.0.2_linux-x64_bin.tar.gz \/usr\/lib\/jvm\/; cd \/usr\/lib\/jvm\/; tar -zxvf openjdk-11.0.2_linux-x64_bin.tar.gz\n    !update-alternatives --install \/usr\/bin\/java java \/usr\/lib\/jvm\/jdk-11.0.2\/bin\/java 1\n    !update-alternatives --set java \/usr\/lib\/jvm\/jdk-11.0.2\/bin\/java\nelse:\n    print('jdk level is Ok ')","3ff12087":"import json\nimport os\n#os.environ[\"JAVA_HOME\"] = \"\/usr\/lib\/jvm\/java-8-openjdk-amd64\"\nos.system(\"ls \/usr\/lib\/jvm\")\nos.environ[\"JAVA_HOME\"] = \"\/usr\/lib\/jvm\/jdk-11.0.2\"\n!ls '\/usr\/lib\/jvm'","b88ea1ec":"from IPython.core.display import display, HTML\nimport nltk\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tag import pos_tag\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nimport pandas as pd\nimport numpy as np\nimport string\nimport torch\nimport numpy\nfrom tqdm import tqdm\n#%tensorflow_version 1.x\n!pip install tensorflow==1.15.2\nimport tensorflow\nprint(tensorflow.__version__)","ea53337e":"%%capture\n\n!wget https:\/\/www.dropbox.com\/s\/j55t617yhvmegy8\/lucene-index-covid-2020-04-10.tar.gz\n!tar xvfz lucene-index-covid-2020-04-10.tar.gz\n!wget https:\/\/www.dropbox.com\/s\/szakwmvco88hp3m\/synonyms.csv?dl=0\n!mv synonyms.csv?dl=0 synonyms.csv","49f7b951":"!du -h lucene-index-covid-2020-04-10","c52e3bd6":"from transformers import *\n#let us try different BERT models, so far BERT model had better performance\n\n#dtokenizer = AutoTokenizer.from_pretrained('allenai\/scibert_scivocab_cased')\n#dmodel = AutoModelForQuestionAnswering.from_pretrained('allenai\/scibert_scivocab_cased')\n#dtokenizer = AutoTokenizer.from_pretrained('monologg\/biobert_v1.0_pubmed_pmc', do_lower_case=False)\n#dmodel = AutoModelForQuestionAnswering.from_pretrained('monologg\/biobert_v1.0_pubmed_pmc')\n#dtokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer\/Bio_ClinicalBERT\")\n#dmodel = AutoModelForQuestionAnswering.from_pretrained(\"emilyalsentzer\/Bio_ClinicalBERT\")\n\ndtokenizer= BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\ndmodel=BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\nscoredic={}\n\n\t\t\t","6c0eefe9":"# retrun synonyms for term \ndef getsynonym(term):\n    \n\n  df = pd.read_csv('..\/input\/synonymscsv\/synonyms.csv')\n\n  mylist=[]\n  for col in df.columns:\n    for index, rows in df.iterrows():\n      if rows[col]==term:\n        df2=rows[:]\n        df2=df2.dropna()\n        mylist = df2.values.tolist()\n        break;\n  return mylist","0dc9959d":"#return a string composed of the query and adding more synonynms.\ndef expandquery(query):\n  \n  searchquery=\"\"\n  querylist =query.split(\" \")\n  listofwords=[]\n  for term in querylist:\n    synonymlist = getsynonym(term)\n    if not synonymlist == []:\n      listofwords=listofwords+synonymlist\n    else:\n      searchquery=searchquery+\" \"+term\n  myset = set(listofwords)\n  mylist =list(myset)\n  searchquery2=\" \".join(str(item) for item in mylist)\n  searchquery = searchquery+\" \"+searchquery2\n  \n  return searchquery","47b1a649":"import unicodedata\n\ndef normalize_caseless(text):\n    return unicodedata.normalize(\"NFKD\", text.casefold())\n\ndef caseless_equal(left, right):\n    return normalize_caseless(left) == normalize_caseless(right)","89210478":"#return a string composed of the query after removing stop words\ndef removeCovidStopwords(query):\n  stop_wordsCovid =set(['what','how',\"which\",\"where\",\"virus\",\"viral\",\"viruses\",\"infection\",\"disease\",\"patients\",\"study\",\",\",\"?\"])\n  stop_words=set(stopwords.words(\"english\"))\n  searchquery=\"\"\n  word_tokens = word_tokenize(query)\n  print(type(stop_wordsCovid))\n  filtered_sentence = [w for w in word_tokens if ((not w in stop_words)and(not w in stop_wordsCovid))]\n  searchquery=\" \".join(str(item) for item in filtered_sentence)\n  return searchquery","0467fd77":"# return keywords to be used with pyserini\ndef extractquerysearch(query):\n  searchquery=\"\"\n  searchquery = normalize_caseless(query)\n  searchquery = removeCovidStopwords(searchquery)\n  searchquery=expandquery(searchquery)\n\n  return searchquery","0c2072f7":"# Clean some extra text in paper abstract for a better presentation of results\ndef cleantext(paragraph):\n  if paragraph.startswith('abstract')or paragraph.startswith('Abstract')or paragraph.startswith('ABSTRACT'):\n    paragraph =paragraph[8:]\n  \n  return paragraph","f4cb0b16":"query='What is known about covid-19 transmission, incubation, and environmental stability?'\nsearchquery=extractquerysearch(query)\nprint(\"keywords extracted are:\",searchquery)","8b9ebe95":"from pyserini.search import pysearch\n\nsearcher = pysearch.SimpleSearcher('lucene-index-covid-2020-04-10\/')\nhits = searcher.search(searchquery)\n\ndisplay(HTML('<div style=\"font-family: Times New Roman; font-size: 20px; padding-bottom:12px\"><b>Query<\/b>: '+query+'<\/div>'))\n\n\n# Prints the first 10 hits\nfor i in range(0, 10):\n  score=hits[i].score\n  scoredic.update({hits[i].lucene_document.get(\"title\") :score })\n  display(HTML('<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:10px\">' + \n               F'{i+1} {hits[i].docid} ({hits[i].score:1.2f}) -- ' +\n               F'{hits[i].lucene_document.get(\"authors\")} et al. --' + \n               F'<a href=\"https:\/\/doi.org\/{hits[i].lucene_document.get(\"doi\")}\">{hits[i].lucene_document.get(\"doi\")}<\/a>.'+\n               '<br>' +'<b> Paper Title: <\/b> '+\n               F'{hits[i].lucene_document.get(\"title\")}. '\n               \n               + '<\/div>'))","4af1ce05":"import matplotlib.pyplot as plt\nimport numpy as np\n\nplt.rcdefaults()\nfig, ax = plt.subplots()\n\ntitles = list(scoredic.keys())\ny_pos = np.arange(len(titles))\nscores = list(scoredic.values())\nerror = np.random.rand(len(titles))\n\nax.barh(y_pos, scores, xerr=error, align='center')\nax.set_yticks(y_pos)\nax.set_yticklabels(titles)\nax.invert_yaxis()  \nax.set_xlabel('Scores')\nax.set_title(query)\n\nplt.show()\n","86369b0f":"def answer_question(question, answer_text,dtokenizer,dmodel):\n    \n    answer = \"No highlight detected\"\n    if not question or not answer_text:\n      print(\"Empty question or Empty abstract\")\n      return answer\n    # ======== Tokenize ========\n    # Apply the tokenizer to the input text, treating them as a text-pair.\n    input_ids = dtokenizer.encode(question, answer_text,max_length=512)\n    # Report how long the input sequence is.\n    #print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n\n    # ======== Set Segment IDs ========\n    # Search the input_ids for the first instance of the `[SEP]` token.\n    sep_index = input_ids.index(dtokenizer.sep_token_id)\n\n    # The number of segment A tokens includes the [SEP] token istelf.\n    num_seg_a = sep_index + 1\n    \n    # The remainder are segment B.\n    num_seg_b = len(input_ids) - num_seg_a\n    \n    # Construct the list of 0s and 1s.\n    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n\n    # There should be a segment_id for every input token.\n    assert len(segment_ids) == len(input_ids)\n\n    # ======== Evaluate ========\n    # Run our example question through the model.\n    \n    start_scores, end_scores = dmodel(torch.tensor([input_ids]), # The tokens representing our input text.\n                                    token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text\n\n    # ======== Reconstruct Answer ========\n    # Find the tokens with the highest `start` and `end` scores.\n    answer_start = torch.argmax(start_scores)\n    answer_end = torch.argmax(end_scores)\n\n    # Get the string versions of the input tokens.\n    tokens = dtokenizer.convert_ids_to_tokens(input_ids)\n    \n    # Start with the first token.\n    answer = tokens[answer_start]\n    #if bert didn't get the tokens right, then the function retrun and highlight the keywords instead\n    if answer==dtokenizer.cls_token:\n      answer = \"No highlight detected\"\n      return answer\n    # if the first token is [sep] then skip and move forward  \n    if answer==dtokenizer.sep_token:\n      answer=\"\"\n\n    # Select the remaining answer tokens and join them with whitespace.\n    for i in range(answer_start + 1, answer_end + 1):\n        # If it's a subword token, then recombine it with the previous token.\n        if tokens[i][0:2] == '##':\n            answer += tokens[i][2:]\n        \n        # Otherwise, add a space then the token.\n        else:\n            if tokens[i-1]=='(' or tokens[i-1]  == '-':\n              answer += tokens[i]\n            elif tokens[i] == ')' or tokens[i]  == '-':\n              answer += tokens[i]\n            else:\n              answer += ' ' + tokens[i]\n\n    if answer==dtokenizer.sep_token:\n      answer='No highlight detected'\n    return answer","0b184f8b":"def highlightanswer(str,paragraph):\n  str_start=\"\"\n  str_end=\"\"\n  flag='none'\n  paragraph=normalize_caseless(paragraph)\n  str=normalize_caseless(str)\n  try:\n    indx = paragraph.index(str)\n  except:\n    return str_start, str, str_end,flag\n\n  if indx==-1:\n    return str_start, str, str_end,flag\n  str_start=paragraph[0:indx]\n  str_end=paragraph[indx+len(str):]\n  flag='done'\n  return str_start, str, str_end, flag","d9e36bf3":"def highlight_keywords(answer_text):\n\n  abstractwords= word_tokenize(answer_text)\n  searchquery_tokenized=word_tokenize(searchquery)\n  abstractpara=\"\"\n\n  for wrd in abstractwords:\n    if wrd in searchquery_tokenized:\n      abstractpara = abstractpara+\" \"+\"<font color='red'>\"+wrd+\"<\/font>\"\n    else:\n      abstractpara = abstractpara+\" \"+wrd\n  \n  return abstractpara","0978c615":"def display_marker_result():\n  display(HTML('<div style=\"font-family: Times New Roman; font-size: 20px; padding-bottom:12px; background:#e3e3e3\"><b>Query<\/b>: '+query+'<\/div>'))\n  # Prints the first 10 hits\n  for i in range(0, 10):\n    abstract=cleantext(hits[i].lucene_document.get(\"abstract\"))\n    answer =answer_question(query,abstract,dtokenizer,dmodel)\n    strstart, highlighted, strend, myflag= highlightanswer(answer,abstract)\n    if answer=='No highlight detected':\n      display(HTML('<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:10px\">' + '<b>'+\n               F'{i+1}'+') Score: <\/b>'+ F'{hits[i].score:1.2f}' +'-- <b>Authors: <\/b>'+\n               F'{hits[i].lucene_document.get(\"authors\")} et al. ' +'-- <b>DOI: <\/b>'+\n               F'<a href=\"https:\/\/doi.org\/{hits[i].lucene_document.get(\"doi\")}\">{hits[i].lucene_document.get(\"doi\")}<\/a>.'+\n               '<br> <b>Paper Title: <\/b>'+ F'{hits[i].lucene_document.get(\"title\")}. ' +\n               '<br> <b>Abstract: <\/b><br>'+\n               F'{highlight_keywords(abstract)}'\n               +'<font color=\"red\">'+\n               '<br><br><b>High Lights: <\/b> highlighting detected keywords <\/font><br>'+\n                '<\/div> --------------------------------------------------------------------------------------------------------------------------------------' ))\n    elif myflag=='none':\n      display(HTML('<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:10px\">' + '<b>'+\n               F'{i+1}'+') Score: <\/b>'+ F'{hits[i].score:1.2f}' +'-- <b>Authors: <\/b>'+\n               F'{hits[i].lucene_document.get(\"authors\")} et al. ' +'-- <b>DOI: <\/b>'+\n               F'<a href=\"https:\/\/doi.org\/{hits[i].lucene_document.get(\"doi\")}\">{hits[i].lucene_document.get(\"doi\")}<\/a>.'+\n               '<br> <b>Paper Title: <\/b>'+ F'{hits[i].lucene_document.get(\"title\")}. ' +\n               '<br> <b>Abstract: <\/b><br>'+\n               F'{abstract}'\n               +'<font color=\"red\">'+\n               '<br><br><b>High Lights: <\/b>'+F'{highlighted} '+'<\/font><br>'+\n                '<\/div> --------------------------------------------------------------------------------------------------------------------------------------' ))\n    else:\n        display(HTML('<div style=\"font-family: Times New Roman; font-size: 18px; padding-bottom:10px\">' + '<b>'+\n               F'{i+1}'+') Score: <\/b>'+ F'{hits[i].score:1.2f}' +'-- <b>Authors: <\/b>'+\n               F'{hits[i].lucene_document.get(\"authors\")} et al. ' +'-- <b>DOI: <\/b>'+\n               F'<a href=\"https:\/\/doi.org\/{hits[i].lucene_document.get(\"doi\")}\">{hits[i].lucene_document.get(\"doi\")}<\/a>.'+\n               '<br> <b>Paper Title: <\/b>'+ F'{hits[i].lucene_document.get(\"title\")}. ' +\n               '<br> <b>Abstract: <\/b><br>'+\n               F'{strstart} ' +'<font color=\"red\">'+F'{highlighted} '+'<\/font>'+F'{strend}'\n              \n               +'<font color=\"red\">'+\n               '<br><br><b>High Lights: <\/b>'+F'{highlighted} '+'<\/font><br>'+\n               '<\/div> ---------------------------------------------------------------------------------------------------------------------------------------' ))\n","f39cae6b":"display_marker_result()","8e91786e":"query ='what are the effectiveness of drugs being developed and tried to treat COVID-19 patients?'\nsearchquery=extractquerysearch(query)\nprint(\"keywords extracted is: \",searchquery)\nhits = searcher.search(searchquery)\ndisplay_marker_result()","8ba07837":"query=\"What do we know about COVID-19 risk factors?\"\nsearchquery=extractquerysearch(query)\nprint(\"keywords extracted is: \",searchquery)\nhits = searcher.search(searchquery)\ndisplay_marker_result()","1b004caa":"Function extractquerysearch is used to extract the keywords that we can use to fire search query. The result search query is only used for information retrieval and not with Bert model. In other words, we will use the original query as is with BERT.","b333b1b6":"Helper function getsynonym extract synonyms from synonyms.csv","fb23029b":"Helper function expandquery is used to expand the query","6bbe19e7":"Visualize the scores of relevance for each paper retrieved.","f3f7fd28":"Let us perform a new search now and see results","5d2e2e45":"# Highlight searching results for COVID-19 \n","35009ff5":"removeCovidStopwords function is used to remove stop words. \nOther than the default stop words extracted from wordnet, We have collected some stop words that are specific to COVID-19 data set. The customized stop words were selected by computing the term frequency of all papers abstract and manually selecting some of the words that are repeated in almost most of the papers.\n","b9177fb9":"Again, Let us perform a new search and see results","dbf7599b":"Perform the imports and downloads for prerequisites","79202390":"Download the pre-built index and download synonyms file. The synonym file is a preliminary version that was built manually to help in expanding the search query.","c578e8c0":"Helper function unicodedata is used to normalize the text.","f3803ab6":"Load BERT from HuggingFace Transformers","4116d2ff":"Using BERT, we will use answer_question function that will extract the answer using the query and absract. In case the answer is not found then we will highlight the keywords instead.","87c407f9":"Let us display the result.","f1947037":"Using the keywords exracted (i.e. searchquery ) Let us use pyserini to search for related publications. We will display the top 10 documents and their score.","fca56313":"Sanity check of index size (should be 1.3G):","d047b147":"Clean some extra text in retrieved paper abstract for a better results display. ","ad2e5e9d":"First, install Python dependencies","e25f97cd":"# Introduction:\nAbstract is one of the most important sections in any publication. It includes summary for the findings and results in a paper. It is the first thing that researchers view in order to decide whether to go deeper and read the whole publication or skip to another one. The abstract section includes a comprehensive outline of published paper contents, the intended purposes, the publication importance. Hence, we will exploit it to highlight relevant answers for user queries. \nThe solution objective is to ease the search for relevant topics asked about COVID-19. This is done by highlighting related answers from abstract publication extractions. In the following section we will describe the solution flow. \n\n\n# Methodology:\n**Data Preparation**\n\nData set used for searching is provided by Allen Institute for AI. Anserini team have provided already indexing for the data set covering title and abstract.\nReference: https:\/\/github.com\/castorini\/anserini\/blob\/master\/docs\/experiments-covid.md\n\nBuilding a customized stop words by compiling all the paper abstracts and computing the term frequency. By displaying the first 150 most frequent terms, we selected manually terms that not necessarily defined as a keyword. For example, a word like \u201cpatients\u201d or  \u201cdisease\u201d does not add much information as we know beforehand that the dataset is covering medical domain. \n\nBuilding a customized synonyms file for words that we want to expand. This is done by compiling all the queries and sub-quires published on Kaggle competition and computing the term frequency. By discarding the traditional English stop words like \u201cand\u201d, \u201cthe\u201d..etc, we selected manually terms that were interesting and added more synonym for it. For example, a word like \u201canimal\u201d, \u201cmonkey\u201d, \u201cmice\u201d and \u201cmouse\u201d will be probably used as a reference for clinical experiments on animals, and therefore we clustered them together as synonyms to be used in query expansions. \n\n\n**The solution works as depicted in figure 1:**\nStep 1: A user ask a query in natural language, for example: \u201cwhat are available vaccine for Covid 19\u201d\nStep 2: A keywords extraction module will process the query. The module aims to expand the query by synonyms, normalize the text, remove stop words. \nStep 3: Now the keywords are used to search in the indexed data set, we retrieve the top ten hits sorted in descending order of the search score. \nStep 4:  Using the abstract and user query we use Bert question and answer model to highlight the answer in the abstract\n![Solution Flow Diagram](..\/input\/diagram\/flow2.jpg)\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F687442%2F1be903a421119ac5add0beebff9846c1%2Fflow2.jpg?generation=1587076320936373&alt=media) \n# Discussion \n**Solution Pros:**\n\u2022\tSimple and straight forward solution with good results.  \n\u2022\tExploiting Anserini, a toolkit that is built on top of core Lucene libraries. Making it easy to retrieve related documents. [1][2]\n\u2022\tExploiting BERT, a pre-trained model for question and answering task [3]. Bert helped in boosting our result. It facilitated the focus on the search outcome by highlighting the answer of user query within the abstract of retrieved papers. Making it very simply and clear for the user to reach the desired information. \n\n**Solution Cons:**\n\u2022\tBert is unable to extract the answer from publication abstract that are more than 512 tokens. When this case occurs, we are highlighting interesting keywords instead using the extracted keywords.\n\u2022\tWe need to explore other models that are domain specific like scibert, biobert, Bio_ClinicalBERT\n\n\n# Acknowledgments:\nI would like to thank Anserini team for providing demo notebooks and indexed datasets from the Allen Institute for AI [github](https:\/\/github.com\/castorini\/anserini\/blob\/master\/docs\/experiments-covid.md) from the Allen Institute for AI.\n\nI would like to thank Chris McCormick for his Bert demos, articles and his notebook Question Answering with a Fine-Tuned BERT\n[here](https:\/\/colab.research.google.com\/drive\/1uSlWtJdZmLrI3FCNIlUHFxwAJiSu2J0-#scrollTo=W-1zl5XdYInf): \n\n\n# References\nIn this notebook, we'll perform data mining using Covid-19 publications title + abstract.The solution objective is to ease the search for relevant topics asked about COVID-19. This is done by highlighting related answers from abstract publication extractions. \n\n[1] Yang, Peilin, Hui Fang, and Jimmy Lin. \"Anserini: Enabling the use of Lucene for information retrieval research.\" Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2017.\n[2] https:\/\/github.com\/castorini\/anserini\n[3] Devlin, Jacob, et al. \"Bert: Pre-training of deep bidirectional transformers for language understanding.\" arXiv preprint arXiv:1810.04805 (2018).\n\n","2b8163c2":"highlight_keywords function will high light the keywords found in the abstract.","2fc87664":"function highlightanswer is used to highlight a string (str) in a paragraph.","ce96cfc9":"Using the user original query let us extract more keywords.","e9f2ef8b":"display_marker_result function will loop over the search hits sorted by documents score. It will highlight the answer and any kewords that would be interested to the user."}}