{"cell_type":{"4f4e457d":"code","7b36f80d":"code","01b723ae":"code","e5e0d7a6":"code","fc77b59b":"code","b274240c":"code","ada83fcd":"code","f1669546":"code","af141c03":"code","79f194e7":"code","c44f070e":"code","c22602e5":"code","5b024b5a":"code","8220ba18":"code","aba32329":"code","10727993":"code","0467c4c9":"code","150a17b7":"code","71805e66":"markdown","4c4aee8f":"markdown","7342b103":"markdown","2ebcf896":"markdown","1cbabb8b":"markdown","c87b8d23":"markdown","a862f067":"markdown","e08a6faa":"markdown","3e630e56":"markdown"},"source":{"4f4e457d":"from tqdm.auto import tqdm\nimport os\n\nimport numpy as np\nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split , StratifiedKFold\n\n\nimport tensorflow as tf \nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model, load_model, save_model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.layers import Input,Dense, LSTM, RNN, Bidirectional, GlobalAveragePooling2D , Dropout\n\nfrom transformers import TFAutoModel , AutoTokenizer\n","7b36f80d":"class config:\n  seed = 43\n  test_path= \"..\/input\/iiitd-abuse-detection-challenge\/eam2021-test-set-public\/eam2021-test-set-public.csv\"\n  train_path = \"..\/input\/iiitd-abuse-detection-challenge\/eam2021-train-set\/bq-results-20210825-203004-swh711l21gv2.csv\"\n  langs = {'Hindi':'hi','Telugu':'te','Marathi':'mr','Tamil':'ta','Malayalam':'ml','Bengali':'bn','Kannada':'kn','Odia':'or','Gujarati':'gu',}\n  save_dir = \".\/result\"\n  AUTOTUNE = tf.data.AUTOTUNE\n  \n  #model params\n  epochs = 12\n  max_len = 64\n  batch_size = 128\n  hf_path = \"google\/muril-base-cased\"\n  tokenizer_path = \"..\/input\/murilweightssqlen64\/result\/muril_tokenizer\"  \n  model_weights = \"..\/input\/murilweightssqlen64\/result\"\n\n  \ndef seed_everything(seed = config.seed):\n  print(f\"seeded everything to seed {seed}\")\n  os.environ['PYTHONHASHSEED'] = str(seed)\n  np.random.seed(seed)\n  tf.random.set_seed(seed)\n\nif not os.path.exists(config.save_dir):\n  os.makedirs(config.save_dir)\nseed_everything()","01b723ae":"tokenizer = AutoTokenizer.from_pretrained(config.hf_path)","e5e0d7a6":"def fast_encode(texts, tokenizer, chunk_size=512, maxlen=config.max_len):\n    \n    input_ids = []\n    tt_ids = []\n    at_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size]\n        encs = tokenizer(\n                    text_chunk,\n                    max_length = config.max_len,\n                    padding='max_length',\n                    truncation=True\n                    )\n        \n        input_ids.extend(encs['input_ids'])\n        tt_ids.extend(encs['token_type_ids'])\n        at_ids.extend(encs['attention_mask'])\n    \n    return {'input_ids': input_ids, 'token_type_ids': tt_ids, 'attention_mask':at_ids}\n","fc77b59b":"df_test = pd.read_csv(config.test_path)","b274240c":"df_test","ada83fcd":"token_data = fast_encode(list(df_test['commentText'].values), tokenizer)\ntoken_data['Id'] = list(df_test['Id'].values)\n","f1669546":"df_tokenized = pd.DataFrame(token_data)","af141c03":"df_tokenized","79f194e7":"del token_data ","c44f070e":"@tf.function\ndef test_prep_function(embeddings):\n  input_ids = embeddings['input_ids']\n  attention_mask = embeddings['attention_mask']\n\n  return {'input_ids': input_ids , 'attention_mask': attention_mask}\n","c22602e5":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","5b024b5a":"def create_model(transformer_model):\n  input_id_layer = Input(shape=(config.max_len,) ,dtype = tf.int32 , name = 'input_ids')\n  attention_mask_layer = Input(shape=(config.max_len,) , dtype = tf.int32 , name = 'attention_mask')\n\n  transformer = transformer_model(input_ids = input_id_layer , attention_mask = attention_mask_layer)[0]\n  transformer_output = transformer[:,0,:]\n\n  x = Dropout(0.2)(transformer_output)\n  predictions = Dense(1, activation = \"sigmoid\")(x)\n\n  model = Model(inputs=[input_id_layer , attention_mask_layer], outputs = predictions)\n  model.compile(\n      optimizer = Adam(learning_rate= 1e-5),\n      metrics = 'AUC',\n      loss = 'binary_crossentropy'\n  )\n\n  return model\n","8220ba18":"with strategy.scope():\n  transformer_model = TFAutoModel.from_pretrained(config.hf_path)\n  model = create_model(transformer_model)\nmodel.summary()","aba32329":"predictions = {}\nfor k in range(1,6):\n    print(f\"********** Fold Running {k} **********\")\n    test_embeddings = {'input_ids': df_tokenized['input_ids'].tolist() ,\"attention_mask\":df_tokenized['attention_mask'].tolist()}\n    test_steps = len(test_embeddings['input_ids'])\/\/config.batch_size\n    \n    print(f\"Predicting on {test_steps} steps\")\n    \n    #dataset creation\n    test_dataset = tf.data.Dataset.from_tensor_slices((test_embeddings))\n    test_dataset = (\n      test_dataset\n      .map(test_prep_function , num_parallel_calls = config.AUTOTUNE)\n      .batch(config.batch_size)\n      .prefetch(config.AUTOTUNE)\n    )\n    \n    #Clearing backend session\n    K.clear_session()\n    print(\"Backend Cleared\")\n    \n    #prediction\n    print(\"creating model\")\n    with strategy.scope():\n        model1 = create_model(transformer_model)\n    \n    print(\"loading model\")\n    model1.load_weights(f'{config.model_weights}\/muril_fold{k}.h5')\n    y_predict = model1.predict(test_dataset , verbose = 1)\n    predictions[k] = y_predict\n    del model1","10727993":"final_predictions = np.mean([predictions[1],predictions[2],predictions[3],predictions[4],predictions[5]] , axis = 0)","0467c4c9":"preds = []\nfor i in tqdm(final_predictions):\n    if i > 0.5:\n        preds.append(1)\n    else:\n        preds.append(0)","150a17b7":"df_submission = pd.DataFrame({'Id': df_test['Id'], 'Expected': preds})\ndf_submission.to_csv('submission.csv' , index = False)\ndf_submission","71805e66":"\n# \ud83d\ude9a Imports","4c4aee8f":"# \ud83d\udcdd Abstract\nTrain and prototype your models quickly by using TPUs. This notebook shows easy and quick way to inference \ud83e\udd17Transformers on TPUs.\n\n# Versions\n**Note:- Made a mistake in [training notebook](https:\/\/www.kaggle.com\/bharadwajvedula\/tpu-muril-training) used ROC AUC score as metrics instead of F1 Score will correct this in future versions**\n\nVersion 1: Max length 64 LB: **0.**  CV: **0.9496**  [CV Score is ROC AUC Score]\n\n\n**All the references are mentioned below**","7342b103":"# \ud83d\udcaf Submission","2ebcf896":"# \ud83e\udde0 Modelling","1cbabb8b":"# \ud83d\udcca Dataset Preperation Function","c87b8d23":"# \ud83d\udd04 KFold Prediciton","a862f067":"# Tokenizer","e08a6faa":"# \ud83d\udccc References\nThank You Harveen for providing such good kernels\n1. [Harveen's baseline notebook](https:\/\/www.kaggle.com\/harveenchadha\/iiitd-muril-hf-tf-w-b-baseline\/)\n2. [Harveen's Tokenization notebook](https:\/\/www.kaggle.com\/harveenchadha\/tokenize-train-data-using-bert-tokenizer)\n\n\nThanks for viewing, drop your suggestions down in the comments below. \ud83d\ude42","3e630e56":"# \u2699\ufe0f Configs"}}