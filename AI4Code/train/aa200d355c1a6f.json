{"cell_type":{"88a4566c":"code","2670775a":"code","e248591a":"code","143dce26":"code","9b9f1add":"code","cc203c62":"code","84af1a92":"code","f0b91a2c":"code","a2af7cea":"code","d89edf91":"code","cf88577b":"code","39914da8":"code","7d7f9e10":"code","4e6ce40a":"code","e76ec37c":"code","eb804d96":"code","6d375d95":"code","8caa99d8":"code","f093f624":"code","9ba5e6e6":"code","b6e9accf":"code","125036ed":"code","75cf81e1":"code","a6b0d7f1":"code","183f5260":"code","c505d398":"code","1da51fff":"code","cffe6630":"code","6aeec1db":"code","b46c4a61":"code","1765c9cb":"code","a1e60183":"markdown","e1f535fd":"markdown","4160cedc":"markdown","26ed1b64":"markdown","3b4ff4b2":"markdown","9cda5bec":"markdown","6aaea7e6":"markdown","ec2cfa76":"markdown","8f005b3a":"markdown","0cf59c6d":"markdown","50ac2761":"markdown"},"source":{"88a4566c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n# import nltk\n# nltk.download('stopwords')\n# nltk.download('punkt')\nfrom nltk.corpus import stopwords\n# from nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom xgboost import XGBClassifier\nimport eli5\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2670775a":"# Loading the dataset\ndata = pd.read_csv('..\/input\/bank-complaint\/complaints.csv')","e248591a":"print(data.head())","143dce26":"print(\"\\nNumber of dimensions in dataset : \", data.ndim)\nprint(\"\\nDimensions of dataset : \", data.shape)\nprint(\"\\nNumber of Features in the dataset: \", len(data.columns))\nprint(\"\\nList of features in the dataset: \")\n\nfor i in range(len(data.columns)):\n    print(end = \"\\t\")\n    print(str(i + 1)+\". \"+data.columns[i])\n    \nprint(\"\\nNumber of elements in each feature of dataset:\\n\", data.count()) #To get an idea if there are any missing values.\nprint(\"\\n\\nCount of NaN values in each column:\\n \",data.isna().sum())  #Counting number of missing values in each column","9b9f1add":"# Renaming Columns\n\ndata.columns = [col.strip() for col in data.columns]\ndata.columns = [col.replace('-',\"_\") for col in data.columns]\ndata.columns = [col.replace(' ',\"_\") for col in data.columns]\ndata.columns = [col.title() for col in data.columns]\n\ndata.rename(columns = {'Consumer_Complaint_Narrative':'Complaint', 'Company_Public_Response':'Response'}, inplace = True)\nprint(data.columns)\n","cc203c62":"# Counting number of missing values in each column\n\nprint(\"\\n\\nCount of NaN values in each column:\\n \",data.isna().sum())","84af1a92":"# Selecting records having registered complaints only\n\ndata = data[data['Complaint'].notnull()]\nprint(data.shape)","f0b91a2c":"# Converting registered Complaint into lowercase string\n\ndata['Complaint'] = data['Complaint'].astype(str)\n\ndata['Complaint'] = data['Complaint'].str.lower()","a2af7cea":"# Identifying unique Products\n\nunique_ele = data['Product'].unique()\nprint(\"Number of unique elements in Product : \", len(unique_ele))\nprint(\"\\n\\nUnique elements in Product : \", unique_ele)","d89edf91":"# Number of Complaints in each Product\n\ntemp_series = data.groupby('Product').size()\nprint(temp_series)\n\ntemp_series.sort_values(ascending = False, inplace = True)\nprint(\"\\nAfter sorting in Descending Order :\\n\\n\",temp_series)","cf88577b":"# Selecting the Product for Analysis\n\nproduct_list = list(temp_series[2:7].index)  # Considering only 5 products with comparable numbers.\nprint(\"\\n The Product list is : \", product_list)\n\ndata = data.loc[data['Product'].isin(product_list)]\nlength = len(data['Product'])\ndata.index = [np.arange(0,length)]\nprint(\"\\n The dimensions of dataset after filering out the Product list : \", data.shape)\n# print(\"\\n\\n The Dataset after filering out the Product list\\n\\n\", data.head())\n\ndata.to_csv('bank_dataset.csv')","39914da8":"# Creating LabelEncoder object \n\nencoder = LabelEncoder()\ndata['Product_Encoding'] = encoder.fit_transform(data['Product'])\nprint(data.head())\n# We will use encoder.inverse_transform(df['Product_Encoding'] to get back corresponding Product\n# We can use encoder.classes_ to get list of products.","7d7f9e10":"# Tokenizing the complaints & performing lemmatization on each token\n\nlemmatizer = WordNetLemmatizer()\n\nstop_words = set(stopwords.words('english'))  # Creating stopwords object\n\ndata['Filtered_Text'] = data['Complaint'].apply(lambda x: \" \".join([lemmatizer.lemmatize(word) for word in x.split() if word not in stop_words]))\nprint(data.head())","4e6ce40a":"# Dividing the Dataset into Testing and Training set.\n\ntarget = data['Product_Encoding']\nX = data.drop(columns = ['Product_Encoding'])\nprint(\"\\n List of Input : \",X.columns)\n# print(\"\\n Target : \",target.name)\n\nX_train, X_test, target_train, target_test = train_test_split(X, target, test_size = 0.30, random_state = 25)\nprint(\"\\nOriginal Number of records in processed dataset:\", len(X))\nprint(\"\\nNumber of records in training features for training and testing purpose resp. are :\",len(X_train), len(X_test), sep = ' ')\nprint(\"\\nNumber of records in target feature for training and testing purpose resp. are :\",len(target_train), len(X_test), sep = ' ')","e76ec37c":"# Initializing vectorizer & transforming the text\n\nvectorizer = TfidfVectorizer(max_features = 5000)  # Selecting top 5000 uncommon words only\nY = vectorizer.fit_transform(X_train['Filtered_Text'])\n\nprint(\"Shape : \", Y.shape)\n\n# vocab = vectorizer.get_feature_names() \n# print(vocab)\n\n","eb804d96":"Z = vectorizer.transform(X_test['Filtered_Text'].T) # Transposing the transformed vectorizer\nprint(\"Shape : \", Z.shape)","6d375d95":"X_test","8caa99d8":"# Training decision tree classifier\nmodel_type_1 = DecisionTreeClassifier(criterion = 'gini', max_depth = 10, random_state = 20) #There are 2 criteria : 'gini' or 'entropy'\nmodel = model_type_1.fit(Y,target_train)\noutput = model.predict(Z)\nprint(confusion_matrix(target_test, output))\n# accuracy_score(target_test,output)\nprint(\"\\n Accuracy Score : \",accuracy_score(target_test,output))\n","f093f624":"pd.Series(target_test).value_counts()","9ba5e6e6":"# Training Random Forest classifier\nmodel_type_2 = RandomForestClassifier(n_estimators = 10, random_state = 20)\nmodel = model_type_2.fit(Y,target_train)\noutput = model.predict(Z)\nprint(confusion_matrix(target_test, output))\n# accuracy_score(target_test,output)\nprint(\"\\n Accuracy Score : \",accuracy_score(target_test,output))\n","b6e9accf":"# Training using Naive Bayes classifier\n\nmodel_type_3= GaussianNB()\nmodel = model_type_3.fit(Y.toarray(),target_train)\noutput = model.predict(Z.toarray())\nprint(confusion_matrix(target_test, output))\n# accuracy_score(target_test,output)\nprint(\"\\n Accuracy Score : \",accuracy_score(target_test,output))\n","125036ed":"# Training using Passive Aggressive classifier\n\n\nmodel_type_4= PassiveAggressiveClassifier()\nmodel = model_type_4.fit(Y,target_train)\noutput = model.predict(Z)\nprint(confusion_matrix(target_test,output))\n# accuracy_score(target_test,output)\nprint(\"\\n Accuracy Score : \",accuracy_score(target_test,output))\n","75cf81e1":"# Training using XGBoost classifier\n\n\nmodel_type_5= XGBClassifier(max_depth = 7, n_estimators = 50)\nmodel = model_type_5.fit(Y,target_train)\noutput = model.predict(Z)\nprint(confusion_matrix(target_test,output))\nprint(\"\\n Accuracy Score : \",accuracy_score(target_test,output))\n\n","a6b0d7f1":"# Creating a new dataframe with testing set having predicted values.\n\nNew_output = pd.DataFrame(X_test.copy(deep = True))\nNew_output['Actual_Class'] = target_test\nNew_output['Predicted_Class'] = output\n\nNew_output.index = np.arange(1, New_output.shape[0] + 1)\n\n# print(New_output.head())\nprint(\"Shape : \",New_output.shape)","183f5260":"# Selecting those records having differing actual and predicted output\n\ndata_2 = New_output.loc[New_output['Actual_Class'] != New_output['Predicted_Class']]\nprint(\"\\nShape of data whose Actual & predicted class differs = \" + str(data_2.shape))\n\n\ndata_2 = data_2.groupby('Actual_Class').head()\nprint(\"\\n\\nTaking 5 samples of each wrongly predicted complaint\" + str(data_2.shape))\n\nprint(data_2[['Actual_Class', 'Predicted_Class']])\n# print(data_2[['Complaint','Actual_Class', 'Predicted_Class']])","c505d398":"# Analysing top 5 complaints to understand why our model is making incorrect predictions\n\ns1 = data_2['Complaint'].head(5)\nfor i in range(len(s1)):\n    print(s1.iloc[i], end = \"\\n\"*4)\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', 5000)","1da51fff":"# Analysing top 5 complaints to understand how our model is making correct predictions\n\ncorrect_predicted_complaint = New_output.loc[ New_output['Actual_Class'] == New_output['Predicted_Class'] & (New_output['Actual_Class'] == 0) ]\ndata_3 = correct_predicted_complaint['Complaint'][0:5]\ndata_3.index = np.arange(0, len(data_3))\nprint(data_3)\n","cffe6630":"# Visualizing our models predictions\n\neli5.show_weights(model,vec = vectorizer, top = 10)  #This is used to inspect model parameters\n# print(eli5.show_weights(model,vec = vectorizer, top = 10)  #This is used to inspect model parameters)\n\n","6aeec1db":"eli5.show_prediction(model, vec = vectorizer, doc =  s1.iloc[0]) #This explains how the model makes individual predictions.\n","b46c4a61":"encoder.inverse_transform([0,1,2,3,4]) # To understand what the 'Predicted_Class' code represents.","1765c9cb":"eli5.show_prediction(model, vec = vectorizer, doc = data_3.iloc[0])  #Visualizing how our model predicts for 1st complaint instance.","a1e60183":"### **Cleaning the data**\n\n___","e1f535fd":"> - ### TF-IDF Vectorizer\n>It stands for Term Frequency * Inverse Document Frequency Vectorizer. It is used to create a vector of words ( based on ***Bag of Words*** Model)  where low value of TF-IDF for a word indicates it occurs very commonly whereas a high value indicates a rare word. \n>\n>>**Term Frequency** is defined as the count of a term in a document.\n>>\n>>**Inverse Document Frequency** is defined as the total number of documents divided by the number of documents containing the word.\n>","4160cedc":"### **Preprocessing the data**\n\n***","26ed1b64":"#### There are 3 approaches to convert Categorical values to Numerical figures\n>   **1. Label-Encoding** (We are using this here.) : It replaces each categorical value with a unique number. It can only be applied to target variable otherwise when used on training variables , it may cause the algorithm to identify some relation between the different categorical values. \n>\n>   **2. One-Hot-Encoding :** It can only be applied to training variables to create new feature vector for each categorical value and similar functinality is provided by *pandas.get_dummies*. Its advantage is that the algorithm doesn't form any relatioship between categorical values.\n>\n>   **3. Find & Replace** (Find Categorical Values & replace them with a Numerical value.)","3b4ff4b2":">## Some Important Terms of Natural Language Processing (NLP)\n> - **Lemma :** The base form of a word is referred to as lemma. Eg: \"playing\", \"played\", \"play\" have the common lemma \"play\".\n>\n> - **Token :**  A token is a string of contiguous characters between two spaces, or between a space and punctuation marks.\n>\n> - **Document :** Document refers to a body of text. A collection of documents make up a corpus. Eg. Here in our dataset, each record in \"Complaint\" field is a document\n>\n> - **Corpus :** A corpus refers to a collection of text.Eg. Here in our dataset, the entire \"Complaint\" field is corpus. \n>\n> - **Stopwords :**  Stopwords are those words which contribute little to overall meaning of the document. They are generally the most common words in a language. For instance, \"the\", \"and\", and \"a\" are stopwords of English language.\n>\n> - **Vocabulary :** The entire set of terms used in a body of text.\n>\n> ### Normalization Techniques\n>>1.  **Lemmatization :** Lemmatization is the process of reducing a word to its base form using \n>>2.  **Stemmimg :** Stemming is the process of reducing a word to its word stem (i.e. base form or lemma) by removing affixes (suffixes and prefixes).","9cda5bec":"># **Different Stages of Data Analysis**\n>\n>***\n>\n> ## **1. Data Exploration**\nThis is the first step for creating any model. In this, we perform initial analysis to check the number of features available. Among given features which are important for creating model and which we can ignore.     \n>\n>## **2. Data Cleaning** \nIn this step, we either remove or apply correction to record's data that have NULL values or some inconsistent value. \n>\n>## **3. Data Preprocessing**\nIn this step, we process the records in such a way that they can be used as input by the model creation algorithm.\n>\n> Eg. Converting non-numeric data into numeric data.\n>\n>## **4. Model Preparation** \n>   In this step, we select a model and train our data on it. \n>\n>## **5. Model Evaluation** \nIn this step, we make predicions using the trained model and use confusion matrix for evaluating its performance. We choose the model with the best evaluation stats.\n\n","6aaea7e6":"> - ### Train-Test Splitting :\n>      It is used for dividing the dataset into training and testing sets.","ec2cfa76":"### **Training models & evaluating their performance**\n\n***","8f005b3a":"# import pickle\n# saved_model = pickle.dumps(model_type_5)\n# pickle.dump(model_type_5,open('saved_model','wb' ))","0cf59c6d":" ### **Exploring the data**\n \n ***","50ac2761":"> ### Analysing the Predictions"}}