{"cell_type":{"2bf1e76e":"code","93c2ee11":"code","1484448d":"code","edb9765b":"code","c06cd9f3":"code","f760b056":"code","f46e1e0a":"code","752f4532":"code","4fcc4a51":"code","49b0c4a3":"code","85dca296":"code","83e98eca":"code","da34e7f8":"code","585da00b":"code","a37325ec":"code","f1a21907":"code","6bf9035b":"code","37801122":"code","9e1dca74":"code","0b11af11":"code","85b4a97d":"code","9bf408c9":"code","2e1791d9":"code","fc75ba6f":"code","c692428a":"markdown","c17293d2":"markdown","5a74e833":"markdown","6b46e0b7":"markdown","5a6e8aea":"markdown","ced5a444":"markdown","95034597":"markdown","eaf3a841":"markdown","390f276e":"markdown","0e8e71d9":"markdown"},"source":{"2bf1e76e":"import os\nimport datetime\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","93c2ee11":"df = pd.read_csv('..\/input\/jena-climate-2009-2016\/jena_climate_2009_2016.csv', index_col=None)\n\ndisplay(df.head())\ndisplay(df.tail())","1484448d":"df.shape","edb9765b":"df.describe().transpose()","c06cd9f3":"corr = df.corr()\nax = sns.heatmap(corr, \n                 vmin=-1, vmax=1, center=0,\n                 cmap=sns.diverging_palette(20, 220, n=200),\n                 square=True)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')","f760b056":"# indices of point of split\nprint(df[df[\"Date Time\"]=='31.12.2014 23:50:00'].index.values)\nprint(df[df[\"Date Time\"]=='31.12.2015 23:50:00'].index.values)","f46e1e0a":"for col in ['wv (m\/s)', 'max. wv (m\/s)']:\n    df[col] = df[col].replace(-9999.00, 0)","752f4532":"features = [list(df.columns)[c] for c in [1,2,6,8,9,11,12]]\nfeatures","4fcc4a51":"df_filt = df[['Date Time']+features].copy()\ndf_filt.head()","49b0c4a3":"date_time = pd.to_datetime(df.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')","85dca296":"timestamp_s = date_time.map(datetime.datetime.timestamp)\nday = 24*60*60\nyear = (365.2425)*day\n\ndf['Day sin'] = np.sin(timestamp_s * (2 * np.pi \/ day))\ndf['Day cos'] = np.cos(timestamp_s * (2 * np.pi \/ day))\ndf['Year sin'] = np.sin(timestamp_s * (2 * np.pi \/ year))\ndf['Year cos'] = np.cos(timestamp_s * (2 * np.pi \/ year))","83e98eca":"df.head()","da34e7f8":"split_fraction = 0.75075\ntrain_split = int(split_fraction * int(df.shape[0]))\n\nstep = 6\npast = 720\nfuture = 72\nbatch_size = 256\n\ndata_mean = df[:train_split].mean(axis=0)\ndata_std = df[:train_split].std(axis=0)\n\ndf = (df-data_mean) \/ data_std","585da00b":"#df = normalize(df.values, train_split)\ndf = df.values\ndf = pd.DataFrame(df)\ndf.head()\n\ntrain_data = df.loc[:train_split-1]\nval_data = df.loc[train_split:]","a37325ec":"train_data.head()","f1a21907":"start = past + future\nend = start + train_split\n\nx_train = train_data[[i for i in range(11)]].values\ny_train = df.iloc[start:end][[1]]\n\nsequence_length = int(past \/ step)","6bf9035b":"print('X_train shape == {}.'.format(x_train.shape))\nprint('y_train shape == {}.'.format(y_train.shape))","37801122":"dataset_train = keras.preprocessing.timeseries_dataset_from_array(\n    x_train.astype(np.float32),\n    y_train.astype(np.float32),\n    sequence_length=sequence_length,\n    sampling_rate=step,\n    batch_size=batch_size,\n)","9e1dca74":"x_end = len(val_data) - past - future\n\nlabel_start = train_split + past + future\n\nx_val = val_data.iloc[:x_end][[i for i in range(11)]].values\ny_val = df.iloc[label_start:][[1]]\n\ndataset_val = keras.preprocessing.timeseries_dataset_from_array(\n    x_val.astype(np.float32),\n    y_val.astype(np.float32),\n    sequence_length=sequence_length,\n    sampling_rate=step,\n    batch_size=batch_size,\n)\n\n\nfor batch in dataset_train.take(1):\n    inputs, targets = batch\n\nprint(\"Input shape:\", inputs.numpy().shape)\nprint(\"Target shape:\", targets.numpy().shape)","0b11af11":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard","85b4a97d":"learning_rate = 0.001\ninputs = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]))\nlstm_out = keras.layers.LSTM(48)(inputs)\noutputs = keras.layers.Dense(1)(lstm_out)\n\nmodel = keras.Model(inputs=inputs, outputs=outputs)\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"mse\")\nmodel.summary()","9bf408c9":"%%time\nearly_stopper = EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=7, restore_best_weights=True)\ncheckpointer = ModelCheckpoint(monitor=\"val_loss\", save_freq=\"epoch\",filepath=\"{epoch:03d}-{val_loss:.5f}.h5\", verbose=1, save_weights_only=True, save_best_only=True)\nlr_reducer = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.169, patience=3, verbose=1)\n\nhistory = model.fit(\n    dataset_train,\n    epochs=50,\n    validation_data=dataset_val,\n    callbacks=[early_stopper, checkpointer, lr_reducer],\n)","2e1791d9":"def visualize_loss(history, title):\n    loss = history.history[\"loss\"]\n    val_loss = history.history[\"val_loss\"]\n    epochs = range(len(loss))\n    plt.figure()\n    \n    plt.plot(epochs, loss, \"b\", label=\"Training loss\")\n    plt.plot(epochs, val_loss, \"r\", label=\"Validation loss\")\n    plt.title(title)\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()\n\n\nvisualize_loss(history, \"Training and Validation Loss\")","fc75ba6f":"std = data_std[1]\navg = data_mean[1]\n   \ndef show_plot(plot_data, delta, title):\n    labels = [\"History\", \"True Future\", \"Model Prediction\"]\n    marker = [\".-\", \"rx\", \"go\"]\n    time_steps = list(range(-(plot_data[0].shape[0]), 0))\n    if delta:\n        future = delta\n    else:\n        future = 0\n\n    plt.title(title)\n    for i, val in enumerate(plot_data):\n        if i:\n            plt.plot(future, plot_data[i], marker[i], markersize=10, label=labels[i])\n        else:\n            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n    plt.legend()\n    plt.xlim([time_steps[0], (future + 5) * 2])\n    plt.xlabel(\"Time-Step\")\n    plt.show()\n    return\n\n\nfor x, y in dataset_val.take(20):\n    p = x[0][:, 1].numpy()\n    q = y[0].numpy()\n    p = p*std + avg\n    q = q*std + avg\n    pred = model.predict(x)[0]\n    pred = (pred*std + avg)\n    \n    show_plot([p, q, pred], 12, \"Single Step Prediction\")","c692428a":"## **Replace default empty value**","c17293d2":"**Training Loss is lower than Validation Loss, it means the model is slightly over-fitting**","5a74e833":"**Sin-Cos Extraction**","6b46e0b7":"**Removing the redundant features**","5a6e8aea":"**Split fraction to use 6 yrs of training data**","ced5a444":"# **Data Analysis**","95034597":"# **Data Loading**","eaf3a841":"**Extract Date-Time column**","390f276e":"# **Data Manipulation**","0e8e71d9":"**Normalising the Data**"}}