{"cell_type":{"6610dbbe":"code","1b2a586d":"code","d29feab3":"code","576c9ca3":"code","d0223719":"code","faab2475":"code","c5b0c262":"code","5f0ce529":"code","73b9c9f4":"code","ea9dc853":"code","88fda2b8":"code","13fd42a0":"code","f940495e":"code","65fbbe7b":"code","1d346984":"code","5a298496":"code","8d6b819a":"code","a370536a":"code","3dc74e9e":"code","6db40c37":"code","47583d50":"markdown","c89028f6":"markdown","58b86f84":"markdown","26d11dae":"markdown","0f1a3cf5":"markdown","ef792245":"markdown","d474d49d":"markdown","fadbb715":"markdown","d26a35ea":"markdown","6191e6a5":"markdown","c097dfd9":"markdown","62e2cfd0":"markdown","249e2e4d":"markdown"},"source":{"6610dbbe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1b2a586d":"import string\nimport re\nfrom pickle import dump, load\nimport json\nimport time\nfrom nltk.corpus import stopwords","d29feab3":"from keras.layers.embeddings import Embedding\nfrom keras.preprocessing.text import text_to_word_sequence, Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom numpy import asarray\nfrom numpy import zeros","576c9ca3":"train = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/train.csv')","d0223719":"def merge(filename, test=False):\n    if test:\n        df = pd.read_json(f'..\/input\/coleridgeinitiative-show-us-the-data\/test\/{filename}.json')\n    else:\n        df = pd.read_json(f'..\/input\/coleridgeinitiative-show-us-the-data\/train\/{filename}.json')\n    text = \" \".join(list(df['text']))\n    return text","faab2475":"start = time.time()\ntrain['text'] = train['Id'].apply(merge)\nend = time.time()\nprint(f'This cell executes in {end - start:.2f} seconds.') # usually 1.5mn - but sometimes longer","c5b0c262":"train.head()","5f0ce529":"# lowercasing the text\ntrain['text'] = train['text'].str.lower()","73b9c9f4":"# removing punctuation\nre_punc = re.compile('[%s]' % re.escape(string.punctuation))\n\ndef remove_punctuation(text):\n    return ' '.join([re_punc.sub('', word) for word in str(text).split()])\n\ntrain['text'] = train['text'].apply(lambda text: remove_punctuation(text))","ea9dc853":"# removing stopwords\nstop_words = list(stopwords.words('english'))\n\ndef remove_stopwords(text):\n    return ' '.join([word for word in str(text).split() if word not in stop_words])\n\ntrain['text'] = train['text'].apply(lambda text: remove_stopwords(text))","88fda2b8":"# removing nonalphabetic tokens\ndef remove_nonalpha(text):\n    return ' '.join([word for word in str(text).split() if word.isalpha()])\n\ntrain['text'] = train['text'].apply(lambda text: remove_nonalpha(text))","13fd42a0":"# removing short words\ndef remove_short(text):\n    return ' '.join([word for word in str(text).split() if len(word) > 1])\n\ntrain['text'] = train['text'].apply(lambda text: remove_short(text))\ntrain.head()","f940495e":"test_filenames = os.listdir('..\/input\/coleridgeinitiative-show-us-the-data\/test')","65fbbe7b":"# turn it into a dataframe: let's first get the Id column\ntest = pd.DataFrame({'Id':test_filenames})\ntest['Id'] = test['Id'].apply(lambda x : x.split('.')[0])\n\n# we now add the text column (with the article text)\ntest['text'] = test['Id'].apply(merge, test=True)","1d346984":"test","5a298496":"title_words = [x.lower() for x in set(train['dataset_title'].unique()).union(set(train['dataset_label'].unique()))]\n\ndef clean_articles(text):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()","8d6b819a":"submission = pd.read_csv(\"..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv\")","a370536a":"article_titles = []\n\nfor idx in submission['Id']:\n    article_text = test[test['Id'] == idx].text.str.cat(sep = '\\n').lower()\n    label = []\n    \n    for data_title in title_words:\n        if data_title in article_text:\n            label.append(clean_articles(data_title))\n            \n    article_titles.append(\"|\".join(label))","3dc74e9e":"submission['PredictionString'] = article_titles\nsubmission","6db40c37":"submission.to_csv('submission.csv', index = False)","47583d50":"Thanks for reading!\n\nThis notebook was inspired by the following resources:\n* https:\/\/neptune.ai\/blog\/exploratory-data-analysis-natural-language-processing-tools\n* https:\/\/www.kaggle.com\/anthokalel\/coleridge-complete-eda\n* https:\/\/www.kaggle.com\/ajaypawar123\/eda-text-processing-string-matching-beginners\/notebook#String-Matching","c89028f6":"Then we proceed to perform all our preprocessing steps:\n* lowercasing all text\n* removing all punctuation\n* removing stopwords\n* removing nonalphabetic tokens\n* removing words shorter than one character","58b86f84":"Now we apply this function to our ``train.csv`` file to concatenate all our training data.","26d11dae":"Finally we save our submission.","0f1a3cf5":"Then we load the submission template to fill.","ef792245":"Now we must clean this text. But an easy way to have a first submission would be to search within the text column if we find dataset names that are already in our train set. We will perform that by keeping only words in the ``train[dataset_label]`` and ``train[dataset_title]`` columns (we refer to columns not cleaned because we have not cleaned the test articles' text).\n\nWe first extract the set of words from these 2 columns and define how we will clean our text (we keep alphanumeric characters, split the text into tokens, lowercase it and strip trailing and leading spaces). The set of title words consists of the union of the sets of (unique) words from the ``train[dataset_label]`` and ``train[dataset_title]`` columns, that we then lowercase.","d474d49d":"Now we loop over the 4 rows of the submission dataframe to perform our string matching operation. More precisely, we get the 4 rows to concatenate the text of each article, and then we filter this text based on our ``title_words`` list of words. Finally we build a string a title words separated by ``|``.","fadbb715":"# Merging ``train.csv`` with the rights articles","d26a35ea":"# Preprocessing","6191e6a5":"Let's add this to our submission dataframe and check it.","c097dfd9":"The following function will be used to perform compile all paragraphs from a given article (using the ``Id`` column from the ``train.csv`` file) and then concatenate it with our ``train`` dataframe (``train.csv``). ","62e2cfd0":"# String matching and how to make predictions","249e2e4d":"We first get the list of the test files' names (we should have 4 of them). We will then create a similar dataframe to ``train.csv`` with only an ``Id`` column and the article's text."}}